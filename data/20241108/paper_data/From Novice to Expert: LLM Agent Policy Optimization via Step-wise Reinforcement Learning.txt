From Novice to Expert: LLM Agent Policy Optimization via
Step-wise Reinforcement Learning
ZhiruiDeng YutaoZhu RuibinXiong
ZhichengDou Ji-RongWen MangWang
zrdeng@ruc.edu.cn yutaozhu94@gmail.com WeipengChen
dou@ruc.edu.cn jrwen@ruc.edu.cn xiongruibin18@mails.ucas.ac.cn
GaolingSchoolofArtificial GaolingSchoolofArtificial songmu@baichuan-inc.com
Intelligence Intelligence chenweipeng@baichuan-inc.com
RenminUniversityofChina RenminUniversityofChina BaichuanIntelligentTechnology
Beijing,China Beijing,China Beijing,China
Abstract OptimizationviaStep-wiseReinforcementLearning.InProceedingsofThe
Theoutstandingcapabilitiesoflargelanguagemodels(LLMs)ren- WebConference(WWWâ€™25).ACM,NewYork,NY,USA,12pages.https:
//doi.org/XXXXXXX.XXXXXXX
derthemacrucialcomponentinvariousautonomousagentsystems.
Whiletraditionalmethodsdependontheinherentknowledgeof
LLMswithoutfine-tuning,morerecentapproacheshaveshiftedto-
1 Introduction
wardthereinforcementlearningstrategytofurtherenhanceagentsâ€™
abilitytosolvecomplexinteractivetaskswithenvironmentsand Largelanguagemodels(LLMs)havebegunarevolutionaryerain
tools.However,previousapproachesareconstrainedbythesparse artificialgeneralintelligence(AGI),duetotheirremarkablecapa-
rewardissue,whereexistingdatasetssolelyprovideafinalscalar bilitiesinhandlingcomplexinteractivetaskswithenvironments
rewardforeachmulti-stepreasoningchain,potentiallyleadingto andtools[42,46].Thetasksinvolvemultipleareasincludingweb
ineffectivenessandinefficiencyinpolicylearning.Inthispaper, browsing[12],webshopping[48],householding[35],andcomplex
weintroduceStepAgent,whichutilizesstep-wiserewardtoop- questionanswering[15,40,47].Althoughthesemodels(e.g.,Chat-
timizetheagentâ€™sreinforcementlearningprocess.Inheritingthe GPT[24]andGPT-4[25])areendowedwithextensiveknowledge
spiritofnovice-to-experttheory,wefirstcomparetheactionsof duringpre-trainingonalarge-scalecorpus,theydemonstratea
theexpertandtheagenttoautomaticallygenerateintermediate tendencytogeneratehallucinatedcontent[21,54].Totacklethis
rewardsforfine-grainedoptimization.Additionally,wepropose issueandfurtheralignwithhumanpreferences,researchershave
implicit-rewardandinversereinforcementlearningtechniquesto introducedtrainingLLMagentswithreinforcementlearning(RL)to
facilitateagentreflectionandpolicyadjustment.Furthertheoretical enhancetheirabilityforcomplicatedtaskplanningandresolving.
analysisdemonstratesthattheactiondistributionoftheagentcan InitialeffortsindevelopingLLMagents[26,28,38,55]concen-
convergetowardtheexpertactiondistributionovermultipletrain- trated on maximizing the token-level generation probability of
ingcycles.Experimentalresultsacrossvariousdatasetsindicate theexpertactions,denotedinFigure1(a).Thesemethods,while
thatStepAgentoutperformsexistingbaselinemethods. straightforwardandreward-free,fallshortwhenconfrontedwith
thetrainingdatashortagesituationandstruggletogeneralizebe-
yondthetrainingdatadistribution.Recognizingtheseconstraints,
CCSConcepts
researchers[2,11,36,37,57]haveshiftedtowardsleveragingman-
â€¢ Computing methodologies â†’ Planning and scheduling;
uallyannotatedpreferencesorthefinalenvironmentfeedbackas
Reinforcementlearning;Inversereinforcementlearning.
additionalrewardsignalsandconductingreinforcementlearning
trainingonthebasisofthesupervisedfine-tuning(SFT)model.
Keywords
Nevertheless,thesemethodsarerestrictedbythesparsityandde-
LLMAgentPlanning,ReinforcementLearning,Process-Reward
layoftherewardsignals.AsshowninFigure1(b),existingreward
Optimization
signalsarerepresentedasasinglescalarrewardforeachgener-
atedobservation-actiontrajectory.Suchsparsefeedbackrenders
ACMReferenceFormat:
ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,Mang itchallengingforthemodeltodiscernthequalityofeachaction,
Wang,andWeipengChen.2024.FromNovicetoExpert:LLMAgentPolicy particularlyfortaskswithlongreasoningchain.Consequently,the
modelstrugglestopreciselyrefinelow-qualityactions,resulting
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor inlowlearningefficiency.Thedelayedrewardfeedbackprevents
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation themodelfrommakingtimelycorrections,potentiallyleadingto
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe sub-optimalresponses.
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Process-supervisedreinforcementlearning[27,41]presentsa
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. promisingsolutiontothesechallengesbyprovidingsupervision
WWWâ€™25,April28-May02,2025,Sydney,Australia ateachintermediatereasoningstep.Throughouttheprocessof
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. agentreasoning,therewardofeachintermediatestepcanassist
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX inidentifyingunderperformedpoliciestimely,allowingforagent
4202
voN
6
]IA.sc[
1v71830.1142:viXraWWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
agentandtheexpert.Webeginbyobservingexpertbehaviorpat-
ternsandthenforcetheagenttopracticeindependentlyateach
Expert SFT Observation step.Thisfacilitatesadeeperandfine-grainedcomprehensionof
LLM Agent
theexpertâ€™sdecision-makingprocesses,spontaneouslyproviding
(a) Reward-Free Method Action step-wiserewardfeedback.Next,wedeviseareflectionmoduleto
effectivelyadjustandimproveagentpolicybasedonthepractice
Reward: 1.0 results.Wedevisetwostrategiesforagentreflection,including
Expert
implicit-rewardreinforcementlearningandinversereinforcement
learning.Tovalidatetheeffectivenessofourmodel,weconduct
Agent Reward: 0.2 RL
extensiveexperimentsonthreedifferentscenariosofagentinter-
LLM Agent
activetasks.Experimentalresultsconsistentlydemonstratethat
(b) Sparse Reward Method ourmodelStepAgentoutperformsthestate-of-the-artLLMagent
models.Thisclearlyindicatesthesuperiorityofapplyingstep-wise
rewardreinforcementlearningtoLLMagentpolicylearning.
Expert
Ourmaincontributionsarethree-fold:
Agent Agent Step-wise RL (1)Weproposeastep-wisereinforcementlearningframework
StepAgentthatautomaticallyconstructsintermediatefeedbackto
LLM Agent
progressivelyandefficientlyoptimizetheagentpolicytoeventually
(c) Our Method
alignwiththeexpertpolicy.
(2)Weintroducetwostagesencompassinginspectionandre-
Figure1:Thecomparisonbetweenourstep-wisefeedback
flection,andconstructprocess-supervisedtrainingdatawithout
LLMagentframeworkandpreviousapproaches.
humanannotationtofacilitatethenovicesbecomingexperts.
(3)Wedevisetworeflectionstrategiesforstep-wiseoptimization,
capabilityrapidimprovements.Inlightofthis,weproposetoop- includingimplicit-rewardandinversereinforcementlearning.
timizeagentpolicybyincorporatingstep-wisesupervision
intoreinforcementlearning.However,directlyapplyingstep-
2 RelatedWork
wisesupervisiontoLLMagentsintroducesitsownsetofchallenges.
First,valueassessmentsforindividualstepsareoftenabsentfrom 2.1 LLMsasAgent
thecurrentmulti-stepagentinteractiondatasets,leavingonlya Recently, the outstanding capabilities of large language models
finalevaluation.Evenforhumanannotators,fullyunderstanding (LLMs)haveledresearcherstoexploreadoptingthesemodelsas
thecontributionofeachsteptotheultimateoutcomepresentsa agentcorecontrollersandconstructingartificialintelligence(AI)
significantchallengethatcanbebothcostlyandlabor-intensive. agents.Thedevelopmentofexistingagentsystemscanberoughly
Furthermore,thenecessityforagentstointeractwiththedynami- dividedintotwoprimarycategories:prompt-basedmethodsand
callychangingenvironmentmakesthesituationevenmorecompli- fine-tuning-basedmethods.
cated.SamplingrewarddistributionsbasedonMCTS[7]requires Prompt-basedMethods.Prompt-basedmethods[22,30]fo-
theagenttointeractwiththeenvironmentuntilobtainingthefinal cused on carefully designing the prompt and directly utilizing
rewardwhichisnon-parallelizableandinefficient. closed-source large language models, such as ChatGPT [24] or
Consideringtheaforementionedconcerns,weaimtoefficiently GPT-4[25],fortaskplanningandreasoning.Chain-of-Thought
constructstep-wiserewardsupervisionwithoutadditionalhuman (CoT)prompting[45]wasthefundamentalofmostprompt-based
annotationtoaddresstheabilitygapbetweentheLLMagentand methodswhichintroducedintermediatereasoningstepsindemon-
theexpert.WetakeinspirationfromBennerâ€™snovice-to-expertthe- strationstoenhancethecapacitytodosophisticatedreasoning.
ory[3,4]â€”novicescangraduallyalignwithexpertpolicythrough InheritthespiritofCoTprompting,ReAct[50]devisedathink-
repeatedlyobservingexpertbehaviorswithautonomouspractic- and-actformatprompttoinspireLLMstogeneratebothreasoning
ingandreflectionoftheircurrentpolicy[3,4].Intriguingly,even tracesandtask-specificactionsinaninterleavedmanner.ToT[49]
lackingexplicitprocess-supervisedrewardsignals,novicescanstill furthergeneralizedtotree-structureensuringtoexplorevarious
progressivelyapproximateexpertpolicyandrespondswiftlyto reasoningpathsandmakeglobaldecisionsbylookingaheador
externalstimuli.Thiscognitiveproficiencymirrorsthechallenge backtrackingwhennecessary.Drivenbyhumanrevisionbehavior,
ofadaptingstep-wisereinforcementlearninginagentinteraction SELF-REFINE[20]utilizedasingleLLMasthegenerator,refiner,
tasksâ€”lackingstep-wisesupervisionandflexibility. andfeedbackprovider.Inaddition,Reflexion[34]leveragedlinguis-
Drawing on the above motivations, we propose a step-wise ticfeedbackmaintainedinamemorybuffertoreinforceagentsand
LLMAgentlearningframework(StepAgent),whichemulatesthe inducebetterdecision-making.
novice-to-expertlearningprocessbyautomaticallyconstructing Fine-tuning-basedMethods.Althoughprompt-basedmeth-
supervisionsignalsforstep-wisereinforcementlearning,thereby odscouldachievepromisingperformanceswithouttraining,they
approachingtheexpertpolicy.Wedelineatethenovice-to-expert heavilyrelyonwell-designedpromptsandadvancedclosed-source
processintotwodistinctstepsinthecontextofagenttasks,in- models(e.g.,ChatGPTandGPT-4)leadingtohighusagecosts.To
cludinginspectionandreflection.Specifically,fortheinspection addressthesechallenges,recentstudies[8,10,51,53]constructed
stage,ourtargetistorecognizethepolicydistinctionbetweenthe experttrajectorydatawithteacheragents(e.g.,GPT-4orhumans)FromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWWâ€™25,April28-May02,2025,Sydney,Australia
andperformedsupervisedfine-tuningonopen-sourceLLMs(e.g., policyğœ‹ ğœƒ(Â·|ğ‘  ğ‘¡), whereğ‘  ğ‘¡ = (Promptsys,ğ‘œ1,ğ‘1,Â·Â·Â·,ğ‘ ğ‘¡âˆ’1,ğ‘œ ğ‘¡) âˆˆ S
LLaMA[39]andMistral[16]).Takingastepfurther,NAT[44]and isthecurrentstateoftheenvironment.Theinteractionprocess
ETO[36]introducednegativesamplesduringthefine-tuningto repeatsuntilthetaskcompletesorexceedsthemaximumsteps.
reducemodelhallucinationsandenhancerobustness.Furthermore, A reward ğ‘Ÿ âˆˆ [0,1] is then computed for the final trajectory
RejectionsamplingFine-Tuning(RFT)[52]collectedcorrectreason- (Promptsys,ğ‘œ1,ğ‘1,Â·Â·Â·,ğ‘œ ğ‘›,ğ‘ ğ‘›), whereğ‘Ÿ = 1 indicates the task is
ingpathsgeneratedbythesupervisedmodeltoenrichfine-tuning successand0meansfailure.1Theconditionalprobabilitydistribu-
datasetswhileSPIN[9]empoweredaweakAIagentleveragingits tionfortheoverallprocessğœ‹ ğœƒ(ğ‘ ğ‘›|ğ‘œ1) canbedenotedthrougha
generateddatafortrainingwithoutadditionalhumanannotation. decompositionasfollows:
Inthispaper,wefocusonfine-tuningLLMswithreinforcement
ğ‘›
learninganddeviseastep-wiselearningstrategytoalignthecapa- (cid:214)
ğœ‹ ğœƒ(ğ‘ ğ‘›|ğ‘œ1)= ğœ‹ ğœƒ(ğ‘ ğ‘¡|ğ‘  ğ‘¡). (1)
bilitiesoftheagentwiththeexpert.
ğ‘¡=1
2.2 ReinforcementLearningforLLMs 3.2 SupervisedFine-tuning
WiththedevelopmentoftheLLMs,reinforcementlearning(RL)[11, Supervisedfine-tuning(SFT)entailsleveragingrelativelysmaller
57]playsavitalroleinimprovingthecapabilitiesofLLMs.Actor- labeledexpertdatatobetteradaptthepre-trainedLLMstospecific
Critic[17]wasthebasisofmanyadvancedRLalgorithmswhich domainsordownstreamtasks[26,55],providingasolidfoundation
leveragedtheactorpolicynetworktointeractwiththeenviron- forcreatingapowerfulagent.
mentandperformpolicyupdatesundertheguidanceofthecritic Givenanexpertinteractiontrajectoryğ‘¡ ğ‘’ =(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘›,ğ‘Ë† ğ‘›)in
valuefunction.Basedontheactor-criticalgorithm,TrustRegion theexperttrajectorysetT,weleveragetheauto-regressivelossto
PolicyOptimization(TRPO)[32]introducedtrustregiontoensure fine-tunetheinitialLLMandobtainthebaseagentğœ‹ asfollows:
ğœƒ0
monotonicperformanceofpolicylearningwhileProximalPolicy
Optimization(PPO)[33]furtherproposedpenaltyandclipstrate-
ğ¿ SFT=âˆ’E ğ‘¡ğ‘’âˆ¼T[ğœ‹ ğœƒ(ğ‘Ë† ğ‘›|ğ‘œË† 1)]. (2)
g oi fe is nt so tas bim ilip tl yify duth rie na glg Ro er ii nth fom rcim empl ee nm te Ln et aa rt nio in n. gT fo roso mlv Het uh me ap nro Fb ele em d- F (ğ‘œo Ë† 1ll ,o ğ‘Ë†w 1,i .n ..g ,ğ‘œË†E ğ‘¡)q .u Wat eio fin rs( t1 c), oğœ‹ nğœƒ ca( tğ‘Ë† eğ‘› n| ağ‘œË† t1 e) t= he(cid:206) inğ‘› ğ‘¡ s= t1 ruğœ‹ cğœƒ ti( oğ‘Ë† nğ‘¡| pğ‘ Ë† ğ‘¡ r) o, mw ph t,e are ctğ‘  iË† oğ‘¡ n= s
back(RLHF)training,DirectPreferenceOptimization(DPO)[29] andobservationsintrajectoryğ‘¡ ğ‘’asatokensequenceğ‘¤ =(ğ‘¤1,...,ğ‘¤ ğ‘™)
adoptedasimpleclassificationlosstofine-tuningLLMsandachieve withlengthğ‘™.Then,theprobabilityğœ‹ ğœƒ(ğ‘Ë† ğ‘›|ğ‘œË† 1)inEquation(2)can
higherefficiencyandbetterperformances.Sincetherewardsignal beformulatedasfollows:
isuncertainorsparseinreal-worldscenarios,researchersproposed
âˆ‘ï¸
behaviorcloning(BC)[38]toimitatethebehaviorsofexperts.Fur- ğœ‹ ğœƒ(ğ‘Ë† ğ‘›|ğ‘œË† 1)=âˆ’ logğœ‹ ğœƒ(ğ‘¤ ğ‘˜|ğ‘¤ <ğ‘˜)Ã—1ğ‘¤ğ‘˜âˆˆA, (3)
thermore,GenerativeAdversarialImitationLearning(GAIL)[14] ğ‘˜
d agev enis te td oa fin ti tt he era et xiv pe err tew daa tr ad df iu sn trc it bi uo tn iol nea .rningstrategyforcingthe w inh de icr ae tğ‘¤ or< fğ‘˜ unin cd tii oca nte ins dt io ck ae tn ins gb wef ho ere tht eh re ğ‘¤ğ‘˜-th ist aok toe kn ea nn od f1 ağ‘¤ cğ‘˜ tiâˆˆ oA nsi gs ea nn
-
ğ‘˜
Therewardfunctioninpreviousagentapproacheswaseither eratedbytheagent.Wemasktheobservationtokensandcompute
manuallyannotated[2,11,37]orlimitedtothefinalrewardfeed- theprobabilitysolelyfortheactiontokens.
backfromtheenvironment[35,43,48].Inthiswork,weproposea
step-wisereinforcementlearningmethodandautomaticallygener-
4 FromNovicetoExpert
aterewardsforeachstep.
Largelanguagemodel(LLM)agentshavedemonstratedsuperior
capabilitiesintacklingcomplexinteractivetasks,byleveraging
3 Preliminaries
reinforcementlearningstrategytoaligntheagentpolicywithhu-
Inthissection,wefirstformulatetheagenttaskandthenreview manpreferences.However,existingresearchonLLMagents[9,36]
supervisedfine-tuningforLLMs,acrucialstepbeforereinforcement encountersignificantchallengesstemmingfromrewardsignalspar-
learningthatpreparesthemodelforspecifictasks. sityandthecomplexitiesassociatedwithreasoningprocess.To
addresstheselimitations,inthissection,weintroduceastep-wise
3.1 ProblemFormulation reinforcementlearningframeworktooptimizetheagentpolicy
Theprocessofanagentinteractingwiththeenvironmentfortask withoutmanuallyannotatingtheproceduralrewards.Ourapproach
solvingcanbeformalizedasapartiallyobservableMarkovdecision isinspiredbytheprinciplesofBennerâ€™snovicetoexpert[3,4],
process(POMDP)withthestatesetS,actionsetA,observation facilitatingprogressivelyself-iterativeexperienceacquisition.By
setO,transitionfunctionF :SÃ—A â†’S,andrewardfunction constantlymonitoringtheexpertâ€™sbehaviorsandpracticesponta-
R :SÃ—Aâ†’ [0,1].Initially,theenvironmentprovidesageneral neously,theLLMagentcanaccumulateexperienceandeventually
taskinstructionPromptsysasthesystemprompt,alongwiththe advancefromnovicetoexpertproficiency.
agentâ€™sinitialobservationğ‘œ1 âˆˆ Oasthespecifictaskinput,and The overall framework of StepAgent is depicted in Figure 2.
theagentneedstointeractwiththeenvironmentmultipletimes StepAgent comprises two major phases: (1) Inspection and (2)
forcompletingthetaskandgeneratingresponses. Reflection.Thedetailsofthetwostagesareintroducedinthe
Specifically,atthetimestepğ‘¡,thelargelanguagemodelagent followingsections.
parameterizedbyğœƒ receivesanobservationğ‘œ ğ‘¡ âˆˆ O fromtheen-
vironmentanddecidestotakeanactionğ‘ ğ‘¡ âˆˆAaccordingtothe 1WeomitPromptsysforsimplificationinthefollowingexpressions.WWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Inspection LLM Agent Observe E shn ev lfi r so tan bm lee .n itt : n I e w edo su tl od bli ek e k ea t ob ,u gn ld ul te e no f f rc er ea ,c ak ne drs p, rs ip ci ec y lo b we ee rf ta hn ad n c 4h 0e .e 0s 0e dw oh lli ac rh s .i s
Expert Expert: Thought: I need to find ...
Inspection Inspection Practice Action: search[bundle of crackers, spicy beef and cheese]
Agent: Thought: The Spicy Beef Backpacking Bundle Environment: B0978Q1KK9, The Spicy Beef Back-packing Bundle â€¦, $14.49;
(B0978P4L31) seems to match my criteria and is well B0978P4L31, â€¦
Step-wise within my budget. Action: click[b0978p4l31] Expert: Thought: The Spicy Beef Backpacking Bundle (B0978Q1KK9) seems
Training Data Observe to match my criteria and is well within my budget. Action: click[b0978q1kk9]
LLM Agent Environment: flavor name: original beef backpack bundle; spicy beef backpack
Reflection Implicit Pair- bundleâ€¦. Price: $14.49. Rating: N.A. Description. Features. Reviews. Buy Now.
wise Feedback Reward Func Practice Expert: Now that I've clicked on the Spicy Beef Backpacking Bundle, I need to
PPO Agent: Thought: This is a spicy beef and cheese confirm that it's the spicy beef and cheese flavor, shelf stable, keto, and gluten-
bundle that's shelf stable. I should verify that it's keto free as required. Action: click[spicy beef backpack bundle]
LLM Agent LLM Agent and gluten-free, but since the price is affordable and it â€¦
seems to match my needs, I'll go ahead and purchase
Implicit-Reward RL Inverse RL it. Action: click[buy now] Expert: Thought: â€¦, I'll proceed with the purchase. Action: click[buy now]
Figure2:ThearchitectureofourproposedframeworkStepAgentcontainingtwostages:inspectionandreflection.Bluesnowfake
indicatesfrozenparameterswhileredflamemeanstrainableparameters.TheexamplecomesfromtheWebShopdataset.
4.1 Inspection:RecognizingCapabilityGaps thisspontaneousexerciseisthatthenovicegeneratesactionsbased
Inspection,inaccordancewithBennerâ€™snovicetoexperttheory[3, onthepreviouslyestablishedlearningtargets.Specifically,foreach
4],involvesthenoviceinitiallyobservingexpertbehaviorsandat- learningobjectiveinTsample,wetreatthestateğ‘ Ë† ğ‘– =(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘–)
temptingtoreplicatethesebehaviorsindependentlyunderthesame asthepromptandlettheagentğœ‹ ğœƒ parameterizedbyğœƒ togenerate
circumstance.Thiscomparativepracticeaimstorecognizetheca- theappropriateactionasEquation(5)andobtainthecorresponding
pabilitygapbetweenthenoviceandtheexpert,therebyfacilitating agenttrajectory(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘–,ğ‘ğœƒ ğ‘–) âˆˆT sağœƒ mple.
subsequentlynovicepolicyimprovements.Previousmethodsfor
constructingLLMagents[9,36]focusonobservingandimitating ğ‘ğœƒ ğ‘– âˆ¼ğœ‹ ğœƒ(ğ‘|ğ‘ ). (5)
thecompletebehaviortrajectoryoftheexpertwiththefinalenvi-
ronmentalrewardfeedbackforoptimization.However,duetothe 4.2 Reflection:StrategizingPolicyRefinement
complexityoftheagenttasks,LLMagentsneedtoconstantlyinter- Innovice-to-experttheory,progressiontowardexpert-levelperfor-
actwiththeenvironmentandengageintrial-and-errortoarriveat mancerequiresnovicestoreflectontheirinteractiontrajectories.
theultimatereasoningoutcome.Theinherentmulti-stepreasoning Thisintrospectionisintendedtosummarizeandinternalizeexpe-
characteristicsofagenttasksbringdualchallengesofefficiencyand riences,ultimatelyleadingtothedevelopmentofindividualized
effectivenessforthenoviceâ€™sself-attemptsofthecompletetrajec- behaviorpatternsandpolicies.Therefore,inthissection,welever-
tory.First,emulatingthefulltrajectoryoftheexpertandacquiring ageinteractionsconstructedinSection4.1anddevisetwodistinct
thefinalenvironmentalfeedbackrequiretheagenttoconstantly reflectionstrategies,includingimplicit-rewardreinforcementlearn-
interactwiththeenvironment.Thisinteractionissequentialand ingandinversereinforcementlearning.
cannotbeparallelized,resultinginthesignificantconsumptionof
computationaltimeandresources.Besides,thenecessityforthe 4.2.1 Implicit-Reward Reinforcement Learning. We begin by di-
rectlycomparingtheactionsoftheexpertandthenoviceagent
novicetocomprehendeveryexpertactionsimultaneouslycanlead
withoutintroducingexplicitrewardestimation.Givenatrajectory
toinformationoverload.Thisoverloadcomplicatesthenoviceto
digestandmasterthespecificsofeachbehavior,oftenresultingin pair (ğ‘¡ sample,ğ‘¡ ğœƒ) whereğ‘¡ sample = (ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘–,ğ‘Ë† ğ‘–) istheexpert
inefficientlearningprocesses.Consequently,novicesmayrequire trajectorywhileğ‘¡ ğœƒ =(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘–,ğ‘ğœƒ ğ‘–)isthecorrespondingagent
additionaltrainingdataoriterationstofullygrasptheinsights trajectory.Inheritingthespiritofpreviousworks[9,36],weutilize
derivedfromtheexpertâ€™sexperiences. thedirectpreferenceoptimizationloss[29],definedasfollows:
Tat ht ieT sno et nia v ad e bd l lyr ee sos tbs hst eeh r ne v os ee va icl ni em d toi it ma idt ii t eo a nn t tes if, t yhit se hi es ox re p ts ces ore mtn â€™s it ni aa gcl stf i ioo nnr ts hth est ie repn b- eo b hv y ai -c vse it oet rpo s. ğ¿ implicit(ğœ‹ ğœƒ,ğœ‹ ğ‘’)=âˆ’E[logğœ(ğ›½logğœ‹ ğœ‹ğœƒ ğ‘’(( ğ‘ğ‘ Ë†Ë† ğ‘–ğ‘– || ğ‘ ğ‘  Ë†Ë† ğ‘–ğ‘– )) âˆ’ğ›½logğœ‹ ğœ‹ğœƒ ğ‘’(( ğ‘ğ‘
ğœƒ
ğ‘–ğœƒ ğ‘– || ğ‘ ğ‘  Ë†Ë† ğ‘–ğ‘– )) )],
andfacilitatethemasteryofcriticalskills.Specifically,considering (6)
an expert trajectory ğ‘¡ ğ‘’ = (ğ‘œË† 1, ğ‘Ë† 1, Â·Â·Â·,ğ‘œË† ğ‘›,ğ‘Ë† ğ‘›) withğ‘›-steps, we whereğœ‹ ğœƒ isthecurrentagentpolicyneededtobeoptimized,ğœ‹ ğ‘’ is
segmentthistrajectoryaftereachaction,treatingeachactionasa theexpertpolicy,ğœ‹ refisthereferencemodelinitializedwiththe
short-termlearningobjectiveforthenovice: agentpolicyandğ›½isahyper-parameter.
(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘–,ğ‘Ë† ğ‘–) âˆˆTsample, ğ‘– =1,2,Â·Â·Â·,ğ‘›. (4) 4.2.2 InverseReinforcementLearning. Consideringthelackofre-
wardsignalsforeachreasoningstepinexistingdatasets,weintro-
Whenthenoviceestablisheslearningtargets,ittriggerstheprac- duceaninversereinforcementlearning(IRL)method[1,14,23,31].
ticestageintheexpert-novicelearningprocess.Thisspontaneous Thismethodfirstinfersthestep-wiserewardfunctionbasedon
exerciseisgearedtowardsidentifyingthebehavioraldiscrepan- theexpertâ€™sandagentâ€™sbehaviorsandthenleveragesthereward
cies between the novice agent and the expert, allowing for the functiontofine-grainedoptimizestheagentpolicy.
accumulationofexperienceandthegradualdevelopmentofthe WefirstdefinetheoccupancymeasureğœŒ forapolicyğœ‹,indi-
ğœ‹
noviceâ€™sbehavioralpatternsthroughrepeatedpractice.Centralto catingthenormalizeddistributionofstate-actionpairswhentheFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWWâ€™25,April28-May02,2025,Sydney,Australia
agentadoptspolicyğœ‹ toexploretheenvironment: Algorithm1StepAgentwithInverseReinforcementLearning
ğœŒ ğœ‹(ğ‘ ,ğ‘)=(1âˆ’ğ›¾)âˆ‘ï¸ ğ‘¡âˆ =0ğ›¾ğ‘¡ğ‘ƒ ğœ‹(ğ‘  ğ‘¡ =ğ‘ )ğœ‹(ğ‘|ğ‘ ), (7) 1 2 3: :
:
I On up tu pt u: taE :gx Fep in ne t art lpt aor gla i ecje nyc tit pno oir ti lie ia cs l yi( zğ‘œ ğœ‹eË† 1 ğœƒd,ğ‘ bË† 1 y, ğœ‹... ğœƒ, 0ğ‘œË† ğ‘›âˆ’1,ğ‘Ë† ğ‘›) âˆˆT,
w prh oe br ae bi1 liâˆ’ tyğ›¾ oi fs thth ee an geo nrm tia nli sz ta at ti eo ğ‘ n af tac tit mor e,ğ‘ƒ ğ‘¡ğœ‹ w( hğ‘  ğ‘¡ en= ağ‘  d) or pe tp inre gs pe on lt is cyth ğœ‹e . 4 5: : I fn oi rti ia tl ei rz ae tiğœ‹ oğœƒ n1 ğ‘–â† =ğœ‹ 1,ğœƒ 20 ,...do
Toaccuratelyimitatetheexpertpolicy,itisessentialtoensure 6: //InspectionStage.
thatthepolicydistributiongeneratedbytheagentisassimilaras 7: Foreachsampledstep-wiseexperttrajectory(ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,
possibletothatgeneratedbytheexpert.Thiscanbeachievedby ğ‘œË† ğ‘¡,ğ‘Ë† ğ‘¡) âˆˆTsamplegeneratethecorrespondingagenttrajectory
m asa pin ot sa si in bi ln eg toth ta ht at th oe fa tg he en et xâ€™s po erc tcu ğœŒpan .Wcy em ae da os pu tre JeğœŒ nğœ‹ sğœƒ eni -s Sa hs ancl no os ne (ğ‘œË† 1,ğ‘Ë† 1,Â·Â·Â·,ğ‘œË† ğ‘¡,ğ‘ğœƒ ğ‘¡) âˆˆT sağœƒ mplewithpolicyğœ‹ ğœƒğ‘–
ğœ‹ğ‘’ 8: //ReflectionStage.
divergence(JS)tomeasurethedistancebetweentwodistributions.
9: fordatain(Tsample,T sağœƒ mple)do
m ğœ‹inJS(ğœŒ ğœ‹ğœƒ,ğœŒ ğœ‹ğ‘’)âˆ’ğœ†ğ»(ğœ‹ ğœƒ), (8)
10: trainthediscriminatorwiththefollowingloss:
whereğœ†isthehyper-parameter,ğ»(ğœ‹ ğœƒ)=â–³E ğœ‹ğœƒ[âˆ’logğœ‹ ğœƒ(ğ‘|ğ‘ )]isthe E ğœ‹ğœƒ[log(ğ·ğ‘¤(ğ‘ ,ğ‘))]+E ğœ‹ğ‘’[log(1âˆ’ğ·ğ‘¤(ğ‘ ,ğ‘)] (11)
ğ›¾-discountedcausalentropy[5]oftheagentpolicy. 11: Updatetheparameterofthediscriminatorğ· ğ‘¤ â†’ğ· ğ‘¤â€²
FollowingGAIL[14],theJensen-ShannondivergenceJS(ğœŒ ğœ‹ğœƒ,ğœŒ ğœ‹ğ‘’) 12: TakeapolicystepwithPPOruleandrewardfunction
andberepresentedbyaconvexcostfunctionregularizerğœ›(ğœŒ ğœ‹ğœƒ âˆ’ log(ğ· ğ‘¤â€²(ğ‘ ,ğ‘))andupdatepolicyğœ‹ ğœƒğ‘– â†’ğœ‹ ğœƒğ‘–â€².
ğœŒ ğœ‹ğ‘’),uptoaconstantshiftandscaling.Thedefinitionoftheconvex
costfunctionregularizerğœ›:RSÃ—A â†’Râˆª{âˆ}isdefinedas:
ğœ›(ğ‘) â‰œ
(cid:26) E +âˆğœ‹ğ‘’[âˆ’ğ‘(ğ‘ ,ğ‘)âˆ’log(1âˆ’ğ‘’ğ‘(ğ‘ ,ğ‘))] ğ‘
ğ‘
< â‰¥0 0;
.
(9)
tribP ur to ioo nf. oT fh ste ato ec -c au cp tia on ncy pam ire sa .s Cu ore nsr ee qp ure es ne tn lyt ,s tt hh ee dn io scrm rea pl aiz ne cd yd bi es --
tweenğœŒ andğœŒ canbemeasuredusingtheKullback-Leibleror
Accordingto[14],theoptimalsolutionoftheaboveregularizer
ğœ‹ğœƒ ğœ‹ğ‘’
ğœ›(ğœŒ
ğœ‹ğœƒ
âˆ’ğœŒ ğœ‹ğ‘’)isdenotedasfollows: J se itn iose nn 5-S .1h ca an nno bn ed reiv fe or rg me un lc ae teK dL i/ nJS to(ğœŒ Pğœ‹ rğœƒ o, pğœŒ oğœ‹ sğ‘’ i) ti. oIn nt 5h .2is .context,Propo â–¡-
sup E ğœ‹ğœƒ[log(ğ·(ğ‘ ,ğ‘))]+E ğœ‹ğ‘’[log(1âˆ’ğ·(ğ‘ ,ğ‘))].
ğ·âˆˆ(0,1)SÃ—A Proposition5.2. Provingthatoptimizingthelossfunctioncan
Therefore,theoptimizationproblemofEquation(8)canbetrans- beultimatelyequivalenttotheminimizedKL/JSdivergence.
formedintofindingasaddlepoint(ğœ‹,ğ·)ofthebelowEquation:
Intheremainingsection,wedemonstratethatthisproposition
E ğœ‹ğœƒ[log(ğ·(ğ‘ ,ğ‘))]+E ğœ‹ğ‘’[log(1âˆ’ğ·(ğ‘ ,ğ‘)]âˆ’ğœ†ğ»(ğœ‹ ğœƒ). (10) isvalidforbothreflectionmechanisms,includingimplicit-reward
reinforcementlearningandinversereinforcementlearning.
Wedirectlytrainadiscriminatornetworkğ· :SÃ—Aâ†’(0,1),
utilizingdatasampledfromtheexpertandagenttrajectories.The
Proof. Inthefollowingparts,wefirstprovethatProposition5.2
primaryobjectiveofğ·istodifferentiatebetweenthedistribution
holdsforimplicit-rewardreinforcementlearning,andthenprove
ofdatageneratedbytheagentpolicyğœ‹ andtheexpertpolicyğœ‹ .
ğœƒ ğ‘’ forinversereinforcementlearningoptimization.
Whenğ·cannotdistinguishdatageneratedbytheagentfromthe
STEP1.AccordingtoRafailovetal.[29],theoptimalsolution
expert,thentheoccupancymeasureoftheagentğœŒ hassuccessfully
ğœ‹ oftheKL-constrainedrewardmaximizationobjectivecanberear-
matchedthatoftheexpertğœŒ .Thediscriminatornetworkğ·can
ğœ‹ğ‘’ rangedsothattherewardfunctioncanbeexpressedas
beinterpretedasanimplicitrewardmodelprovidingstep-wise
learningsignalstotheagentpolicy.Thecompletelearningprocess
ğ‘Ÿ(ğ‘ ,ğ‘)=ğ›½log
ğœ‹ ğœƒ(ğ‘|ğ‘ )
+ğ›½logğ‘(ğ‘ ),
ofStepAgent-inverseisintroducedinAlgorithm1. ğœ‹ ref(ğ‘|ğ‘ )
whereğ‘(ğ‘ ) isthepartitionfunction[13,18].FollowingBradley-
5 TheoreticalAnalysis Terrymodel[6],wehave:
Inthissection,weprovideatheoreticalanalysistoprovethatthe
distributionofactionsgeneratedbytheagentcanconvergetoward
ğ‘(ğ‘1 >ğ‘2|ğ‘ )=ğœ(ğ‘Ÿ(ğ‘ ,ğ‘1)âˆ’ğ‘Ÿ(ğ‘ ,ğ‘2)).
theexpertactiondistributionovermultipletrainingcycles. Then,thepolicyobjectivecanbeformulatedasEquation(6)which
isequivalenttominimizingtheKLdivergence.
Assumption 1. The loss function of Equation (6) and (10) is
STEP2.Inversereinforcementlearningfirsttrainsadiscrimina-
boundedandLipschitzcontinuous.
tornetwork,whichsubsequentlygeneratesscoresthatserveasthe
Sinceourpolicyupdatemethodemploysgradientdescent,under rewardfunctionforoptimizingthepolicynetwork.Itsoptimization
Assumption1,thepolicyğœ‹ willconvergetoalocalminimumas targetcanbedenotedas:
ğœƒ
theiterationsincrease.Thefollowinganalysesareconductedunder
Assumption1.
ğ½(ğœƒ)=âˆ’E (ğ‘ ,ğ‘)âˆ¼ğœ‹ğœƒ[logğ·(ğ‘ ,ğ‘)].
Accordingtothepolicygradienttheorem,thegradientofğ½(ğœƒ)
Proposition5.1. TheoccupancymeasureğœŒ ğœ‹ğœƒ fortheagentpolicy canbeexpressedas:
canconvergetocloselyapproximatetheexpertâ€™soccupancymeasure
ğœŒ ğœ‹ğ‘’,afterseveraliterations. âˆ‡ğœƒğ½(ğœƒ)=E (ğ‘ ,ğ‘)âˆ¼ğœ‹ğœƒ [âˆ‡ğœƒlogğœ‹ ğœƒ(ğ‘|ğ‘ )ğ‘…(ğ‘ ,ğ‘)].WWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Table1:Statisticofdatasetsinourexperiments. Wecomparethetwovariants(i.e.,implicitandinverse)ofour
methodStepAgentwithseveralbaselinesincluding(1)Supervised
Type Dataset #Train #Dev #Test #Turns
Fine-Tuning(SFT)[8,53]conductsbehavioralcloningonexperttra-
jectories,whichisthebaseagentforStepAgentandotherbaselines.
WebShop 1,938 - 200 4.9
Web
Mind2Web 1,009 - 912 7.3
(2)ProximalPolicyOptimization(PPO)[33]andDirectPreference
Optimization (DPO) [29] are two representative reinforcement
Agent ScienceWorld 1,483 194 241 14.4 learningmethods.Weutilizethefinaltaskrewardfromtheenvi-
ALFWorld 3,321 140 134 10.1
ronmentastherewardfeedbackforPPO.AsforDPO,weadoptthe
HotpotQA 90,447 7,405 7,405 7.0 trajectoriesgeneratedbytheagentasnegativesamples.(3)Rejec-
Multihop
QA
2WikiMultihopQA 167,454 12,576 12,576 8.2 tionsamplingFine-Tuning(RFT)[52]andSPIN[9]incorporatethe
MuSiQue 19,938 2,417 2,417 7.8 successtrajectoriesoftheagenttotheexperttrajectorydatasetand
trainstheagentonnewaugmentedtrajectories.(4)NAT[44]and
ETO[36]introducerejectedtrajectoriesintothetrainingprocess,
Weutilizetheoutputofthediscriminatorastherewardandthe
allowingtheagenttolearnfromitsfailureexperiences.Wealso
gradientofthepolicybecomes:
compareStepAgentwithClosed-SourceLLMsincludingGPT-3.5
âˆ‡ğœƒğ½(ğœƒ)=E (ğ‘ ,ğ‘)âˆ¼ğœ‹ğœƒ [âˆ‡ğœƒlogğœ‹ ğœƒ(ğ‘|ğ‘ )ğ·(ğ‘ ,ğ‘)]. (GPT-3.5-turbo-1106)[24]andGPT-4(GPT-4-0125-preview)[25].
Theexpectedreturnofthepolicyupdate(i.e.,theoutputofthe
discriminator)isrelatedtothegradientofthepolicyparameters. 6.3 EvaluationMetrics
AccordingtoEquation(8-10),optimizingthelossfunctionofthe To align with previous methods [9, 36], we report the average
discriminatornetworkisequivalenttoreducingtheJSdivergence resultsofthetestset.ForWebShopandScienceWorld,weemploy
betweenthetwooccupancymeasures.Theapplicationofthepolicy thefinalrewardautomaticallyassessedbytheenvironmentasthe
gradienttheoremenablestheagenttooptimizeitsstrategyusing evaluationmetricwhileforALFWorld,weutilizethesuccessrate
feedbackfromthediscriminator.Thisprocessensuresthetrajectory forjudgement.IntermsofMind2Web,wereportmacroelement
generatedbytheagenttograduallyapproachtheexpertâ€™strajectory accuracy.Additionally,forthethreemulti-hopquestion-answering
distributionbymaximizingtheoutputofthediscriminator. tasks,weleverageExactMatch(EM)forevaluation.
â–¡
6.4 ImplementationDetails
6 ExperimentalSettings Consistentwithexistingworks[19,36],weemployReAct-form[50]
6.1 Datasets togeneratetheinteractiontrajectory,whichadditionallygenerates
Chain-of-Thought(CoT)rationales[45]beforeeachaction.Foreach
TothoroughlyevaluatetheabilityofourproposedmodelStepAgent,
task,aone-shotin-contextexampleisemployedintheinstruction
weutilizerepresentativetasksfromthreeaspects,includingweb
prompt.ThedetailsofpromptsaredescribedinAppendixA.For
tasks,agenttasks,andmulti-hopquestion-answeringtasks.The
thethreemulti-hopquestionansweringtasks,duetothelackof
statisticsofthesedatasetsaredelineatedinTable1.
intermediatereasoningstepsinthedatasets,weemployGPT-4[25]
WebtasksconsistofWebShop[48]foronlineshoppingand
astheexperttogeneratetrajectoriesandselecttrajectorieswiththe
Mind2Web[12]forcomplextasksonvariouswebsites.Rewardsin
exactmatchscoreequallingoneastheexperttrajectories.Welever-
thetwodatasetsaredensevariableandrangefrom0to1.
agegreedygenerationforourmethodandallbaselineapproaches.
AgenttaskscontainScienceWorld[43]forscienceexperiments,
IntheSFTstage,wesetthelearningrateas1e-5andthebatchsize
andALFWorld[35]forembodiedhousework.Theformercontains
as64.wechoosethecosineschedulerwitha0.03warmup.Wetrain
continuous final rewards from zero to one while the latter has
themodelforfourepochsonalldatasets.Forthereflectionstage,
binaryrewardsdemonstratingthecompletionofthetask.Forboth
thelearningrateis5e-7andthebatchsizeis16.Thetrainingepoch
datasets,wetreatthein-distributiontestsetsasthevalidationset
issetasone.WeleveragetheAdamWoptimizerinbothstages.All
andtheout-of-distributionunseenvariationswhichaimtoassess
experimentsarecarriedouton8NVIDIAA10080GGPUs.
thegeneralizationcapabilitiesofagentsasthetestset.
Multi-hopquestion-answeringtasksincludeHotpotQA[47],
7 ResultsandAnalysis
2WikiMultihopQA[15],andMuSiQue[40].Foreachdataset,we
leveragetheirassociatedWikipediaarticlescontextsasourretrieval 7.1 OverallResults
corpustoconductmulti-stepreasoning.Consideringtherestric- TheoverallperformanceofourproposedmethodsStepAgentand
tionsofexperimentalcosts,followingpreviousapproaches[50,56], allbaselinesareshowninTable2.Wecanobservethat:
weutilizeasubsetoftheentiredataset,selecting5,000samples (1)BothvariantsofStepAgentconsistentlyoutperformallbase-
fortrainingfromthetrainingsetand500sampleseachforthe linemethodsacrossthreedistincttaskcategoriesbyasignificant
validationandtestsetsfromthedevelopmentset. margin.IncomparisonwithETOandSPIN,whichintroducethe
entiretrajectoryfortraining,StepAgentachievesasignificantedge
6.2 BackboneModelsandBaselines withimprovementsoftheresultsoveralltasks.Thisperformance
WeverifytheeffectivenessandrobustnessofourStepAgentontwo demonstratestheeffectivenessofutilizingthestep-wiserewardsig-
widely-usedopen-sourcemodels:Mistral-7B(Mistral-7B-Instruct- nalstoemulatetheexpertpolicy.Evenwithouthuman-annotated
v0.1)andLlama-3-8B(Meta-Llama-3-8B-Instruct). step-wisepreferencedata,StepAgentstillcangraduallyalignwithFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWWâ€™25,April28-May02,2025,Sydney,Australia
Table2:Performancecomparisonofallmethods.Bothvariantscanoutperformallbaselinesbasedonopen-sourcedmodels.
WebTasks AgentTasks Question-AnsweringTasks
Backbone Methods
WebShop Mind2Web ScienceWorld ALFWorld HotpotQA 2WikiMultihopQA MuSiQue
GPT-3.5 Base 40.2 2.0 19.9 2.2 13.0 17.6 4.6
GPT-4 Base 58.0 26.7 53.6 36.6 39.4 64.8 28.2
Base 2.7 17.8 4.2 0.0 4.2 10.6 1.4
SFT 60.1 48.7 52.0 68.5 24.8 40.4 22.9
PPO 60.8 49.5 53.3 69.1 25.4 41.5 23.2
DPO 62.4 50.9 54.1 70.6 26.9 42.7 24.9
RFT 61.5 49.8 53.2 69.8 26.0 42.2 23.5
Mistral
7B SPIN 63.6 51.7 55.0 71.4 27.6 43.1 25.0
NAT 61.3 50.4 52.9 69.3 26.1 41.9 24.1
ETO 64.1 52.4 56.5 72.8 28.2 43.8 25.4
StepAgent-Implicit 66.2 53.3 59.6 74.2 31.0 46.8 27.7
StepAgent-Inverse 66.5 53.6 59.7 74.9 30.8 46.6 27.5
Base 7.2 23.6 32.3 0.0 15.6 13.8 9.0
SFT 62.6 50.3 54.5 67.8 33.0 47.8 30.6
PPO 63.2 51.0 55.0 67.9 33.2 47.6 30.4
DPO 64.0 52.6 56.9 70.3 35.1 48.5 31.6
RFT 63.6 50.8 54.7 68.0 33.5 47.9 30.8
Llama3
8B SPIN 65.4 53.9 60.3 71.9 34.8 48.9 31.9
NAT 63.2 50.9 55.6 68.3 33.4 48.0 31.0
ETO 65.7 54.0 62.5 73.4 35.2 49.4 32.3
StepAgent-Implicit 67.2 55.8 63.6 75.5 38.1 51.3 34.4
StepAgent-Inverse 67.6 55.9 64.1 76.1 37.8 52.0 34.1
Table3:AblationstudieswithdifferentrewardtypesbasedonLlama38Bandinversereinforcementlearning.
RewardType WebTasks AgentTasks Question-AnsweringTasks
Method
Step Final WebShop Mind2Web ScienceWorld ALFWorld HotpotQA 2WikiMultihopQA MuSiQue
StepAgent-inverse Ã— âœ“ 65.7 54.0 62.5 73.4 35.2 49.4 32.3
StepAgent-inverse âœ“ Ã— 67.6 55.9 64.1 76.1 37.8 52.0 34.1
StepAgent-inverse âœ“ âœ“ 68.0 56.4 64.8 76.2 38.9 52.0 34.8
theexpertpolicydistribution,leadingtosubstantialenhancements step-wiserewardscanfacilitatetheagentâ€™sdeeperunderstanding
intheresponsequality. andinternalizationoftheexpertpolicyâ€™sunderlyinglogic.
(2)InversereinforcementlearningstrategyStepAgent-Inverse Inthefollowingsections,weconductseveraladditionalexperi-
withexplicitrewardsdemonstratesaslightperformanceimprove- mentstoinvestigateStepAgentindepth.
mentcomparedtoimplicit-rewardreinforcementlearningmethods
StepAgent-Implicitonmostdatasets.Thisindicatesthatexplicit
7.2 AblationStudiesonRewardType
rewardscanprovidethemodelwithmuchcleareroptimizationob-
Inthissection,weconductablationstudiestoanalyzetheinfluence
jectives,therebyfacilitatingmoreeffectiveadjustmentsinbehavior.
ofdifferentrewardtypesonourStepAgentmodel.Weinvestigate
Consequently,theclarityoftheoptimizationtargetsenablesthe
ourmodelStepAgentwiththreevariants[27,41]:(1)Step-wise
noviceagenttomoreeffectivelyapproachexpert-levelperformance.
reward,whichconstructsstep-wiserewardbyobservingandim-
(3)Interestingly,StepAgenthasachievedmoresignificantim-
itating the expert behaviors and optimizes agent strategy with
provementsonthethreemulti-hopquestion-answeringtasks.Con-
step-wise rewards, as introduced in Section 4. (2) Final reward,
cretely,StepAgentcansurpassthestate-of-the-artmodelETObyan
whichutilizesthefinalenvironmentalfeedbackastherewardfor
absolutevalueimprovementof2.9%ontheHotpotQAdataset.Since
optimization.(3)Wealsoexplorethecombinationofthetworeward
thereasoningstepsinmulti-hopquestion-answeringtasksdemon-
typestoevaluatetheirimpactontheperformanceofStepAgent.
stratecomplexsemanticrelationships(i.e.,parallelorhierarchical),
ExperimentsareconductedbasedonLlama3 andinverseRLand
itischallengingfortheagenttoeffectivelyimitatesuchcomplex 8B
wecanobtainsimilarconclusionswithothersettings.
expertpolicybasedsolelyonfinalrewardsignals(e.g.,tasksuccess
FromtheresultsinTable3,wecanobservethatoptimizingthe
orfailureorexactmatchwiththecorrectanswer).Introducing
reinforcementlearningprocesssolelywiththefinalenvironmen-
talfeedbackasrewardsresultsinperformancedegradationonallWWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
80 68 44 68 44
Mistral-7B-Implicit
70 Mistral-7B-Inverse 66 42 66 WS Implicit 42
60 M Mi is st tr ra al l- -1 13 3B B- -I Im nvp el ric seit 64 W WS S I Im nvp el ric seit 40 64 W HPS II mnv pe lir cs ie t 40
50 H HP P I Im nvp el ric seit HP Inverse
62 38 62 38
40
60 36 60 36
30
20 WebShop Mind2WebSci-World ALFWorldHotpotQA 2WikiQA MuSiQue 58 1 3 5 7 9 34 58 1 2 3 4 5 34
(a) Training Iteration (b) Practice Number
Figure3:Performancewithdifferentbackbonemodelparam-
etersonalldatasets. Figure4:Performancewithdifferenttrainingiterationsand
practicenumbers.â€œWSâ€isWebShopwhileâ€œHPâ€isHotpotQA.
tasks.Concretely,eliminatingstep-wiserewardcausestheobvi-
ousdroponalltasks(e.g.,WebShop:68.0â†’âˆ’67.2andScienceWorld: TrainingIteration.Toidentifytheoptimaliterationnumber,
64.8â†’âˆ’63.6).Thisindicatesthatthestep-wiserewardcanfacilitate weincreasethetrainingiterationnumberfromonetonine,while
theagentâ€™scapabilitytoalignwiththeexpert.Meanwhile,thefinal closelymonitoringtheperformancechangesassociatedwiththe
environmentalfeedbackalsocontributestothefinalresultswhich tworeflectionmechanisms.AsdepictedinFigure4(a),theperfor-
verifiesthatacombinationofthestep-wiseandthefinalreward manceofStepAgentimprovesprogressivelyasthetrainingiteration
supervisionisbeneficial.Step-wiserewardsupervisionprovidesim- numberincreasesforbothimplicit-rewardandinversereinforce-
mediatefeedback,boostingoptimizationefficiency,whilethefinal mentlearningstrategies.However,thepeakperformanceofthetwo
supervisionofferscleardirectionfortheoveralllearningobjectives. methodsdiffers.Specifically,ontheWebShopdataset,theimplicit-
Althoughthecombinationofthetworewardtypescanleadtobet- rewardstrategyreachesthepeakafterthreeiterationswhereas
terresults,obtainingthefinalrewardnecessitatesinteractionwith theinversereinforcementlearningmethodachievesitsbestperfor-
theenvironmentwhichcannotbeparallelized.Consequently,in manceattheseveniteration.Thisindicatesthatmoreiterationsare
thispaper,weexclusivelyfocusonadoptingthestep-wisereward requiredforthemodeltocorrectlylearntheexplicitrewardfunc-
tostrikeabalancebetweenefficiencyandeffectiveness. tion,whichleadstoslowerconvergence.Besides,theperformance
startstodegradewhentheiterationsexceedthepeak.Thisphenom-
7.3 PerformancewithDifferentModelSize enoncanbeattributedtothefactthatastheagentâ€™scapabilities
TofurtherillustratetherobustnessofStepAgent,weconductexper- improve,ourself-playmethodforgeneratingstep-wisefine-tuning
imentswithdifferentbackbonemodelparametersizes.Weutilize data may struggle to provide contrasting positive and negative
Mistral andMistral forthisanalysis.Theresultsaredepicted samples.Theabsenceofcleardistinctionsbetweensuccessfuland
7B 13B
inFigure3.â€œ-Implicitâ€indicatesStepAgentwithimplicit-reward unsuccessfulbehaviorsdisruptsthelearningprocess.
reinforcementlearningstrategywhileâ€œ-Inverseâ€representsinverse PracticeNumber.Inthispart,weconductexperimentstoinves-
reinforcementlearningmethod.WeabbreviateScienceWorldand tigatewhetherintroducingdiverseagenttrajectoriesisbeneficial
2WikiMultihopQAasSci-Worldand2WikiQAforlimitedspace. forperformanceimprovement.Toachievethis,weforcethenovice
First,wecanobservethatStepAgentdemonstratesconsistentand agenttopracticemultipletimesforeachlearningobjectiveduring
robustefficacyacrossmodelswithdifferentparameterscales.This theinspectionphase.Figure4(b)showstheresultsoftworeflection
performancestabilityhighlightsourmodelâ€™sadaptabilitytodiffer- variants.Wecanobservethattheresultsofbothvariantsaregrad-
entconfigurations,ensuringthatitsreliabilityinachievingeffective uallyincreasingasthepracticenumbergrowsfromonetothree.
resultsregardlessoftheparameterscaleemployed.Second,com- Thisimpliesthatintroducingmorediversetrainingsamplescan
paredwithMistral ,Mistral achievessuperiorperformances. acceleratethenoviceâ€™sacquisitionoftheexpertpolicy.However,
7B 13B
Thisindicatestheimportanceofthebackbonemodelâ€™scapability, theperformancedoesnotincreasewhenthepracticenumberex-
asitsignificantlyinfluencestheeffectivenessofpost-imitatedlearn- ceedsthree.Apotentialexplanationisthat,atthesamecognitive
ing.TheenhancedcapacityofMistral allowsformoreeffective level,thediversityofthesamplesremainslimiteddespitemultiple
13B
learningandadaptation,contributingtoimprovedperformances. attempts.Consequently,incorporatingmoresamplesmayleadto
informationredundancy,whichcanhinderlearningefficiencyand
7.4 ExplorationofParametersSettings alsoincreasecomputationalcosts.
In StepAgent, two important hyper-parameters will impact the
experimentalperformanceâ€“thenumberoftrainingiterationsin 8 ConclusionandFutureWork
Algorithm1andthepracticenumberoftheagentduringtheinspec- Reinforcementlearninghasbecomeaneffectiveapproachforalign-
tionstageofeachiteration.Inthissection,weconductexperiments ingagentbehaviorswithhumanpreferences.However,existing
toinvestigatetheirinfluences.Werandomlyselectedtworepresen- reinforcementlearningmethodsprimarilyadoptthefinalenviron-
tativedatasetsWebShopandHotpotQAforthisexperimentandwe mentalfeedbacktooptimizetheagentstrategy.Inthispaper,in-
candrawsimilarconclusionsonotherdatasets. spiredbyBennerâ€™snovice-to-experttheory,weproposedStepAgent,
xirteM
noitaulavEFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWWâ€™25,April28-May02,2025,Sydney,Australia
astep-wisereinforcementlearningframeworkwithoutstep-wise [16] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De-
humanannotation.Intheinspectionstage,thenoviceagentfirst vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,
observesthebehaviorsoftheexpertandthenrehearsesthedemon- GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint
arXiv:2310.06825(2023).
stratedactions.Duringthereflectionstage,theagentcomparesits [17] VijayR.KondaandJohnN.Tsitsiklis.1999.Actor-CriticAlgorithms.InAdvances
actionswiththoseoftheexpertandadjustsitspolicytobetteralign inNeuralInformationProcessingSystems12,[NIPSConference,Denver,Colorado,
USA,November29-December4,1999],SaraA.Solla,ToddK.Leen,andKlaus-
withtheexpertâ€™spolicydistribution.Experimentalresultsacross RobertMÃ¼ller(Eds.).TheMITPress,1008â€“1014.http://papers.nips.cc/paper/1786-
threetypesoftasksconsistentlydemonstratethesuperiorityof actor-critic-algorithms
StepAgentoverexistingbaselines.Besides,weconductadditional [18] TomaszKorbak,HadyElsahar,GermÃ¡nKruszewski,andMarcDymetman.2022.
Onreinforcementlearninganddistributionmatchingforfine-tuninglanguage
experimentstofurtherillustratetheeffectivenessandefficiencyof modelswithnocatastrophicforgetting.AdvancesinNeuralInformationProcessing
StepAgent.Inthefuture,weaimtoenhanceLLMagentsbyinte- Systems35(2022),16203â€“16220.
[19] XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,
gratingmoreadvancedcognitivecapabilitiestobettersatisfyuser
HangliangDing,KaiwenMen,KejuanYang,etal.2023.Agentbench:Evaluating
demandsandrespondtodynamicenvironments. llmsasagents.arXivpreprintarXiv:2308.03688(2023).
[20] AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,Sarah
Wiegreffe,UriAlon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,etal.
2024. Self-refine:Iterativerefinementwithself-feedback. AdvancesinNeural
References
InformationProcessingSystems36(2024).
[21] JoshuaMaynez,ShashiNarayan,BerndBohnet,andRyanMcDonald.2020.
[1] SaurabhAroraandPrashantDoshi.2021. Asurveyofinversereinforcement Onfaithfulnessandfactualityinabstractivesummarization. arXivpreprint
learning:Challenges,methodsandprogress. ArtificialIntelligence297(2021), arXiv:2005.00661(2020).
103500. [22] YoheiNakajima.2023. https://github.com/yoheinakajima/babyagi
[2] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,Nova [23] AndrewYNg,StuartRussell,etal.2000.Algorithmsforinversereinforcement
DasSarma,DawnDrain,StanislavFort,DeepGanguli,TomHenighan,etal.2022. learning..InIcml,Vol.1.2.
Trainingahelpfulandharmlessassistantwithreinforcementlearningfrom [24] OpenAI.2022.GPT-3.5. https://openai.com/index/chatgpt/
humanfeedback.arXivpreprintarXiv:2204.05862(2022). [25] OpenAI.2024.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL] https://arxiv.
[3] PatriciaBenner.1982. Fromnovicetoexpert. AJNTheAmericanJournalof org/abs/2303.08774
Nursing82,3(1982),402â€“407. [26] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,Pamela
[4] PatriciaBenneretal.1984.Fromnovicetoexpert.MenloPark84,1480(1984), Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022.
10â€“1097. Traininglanguagemodelstofollowinstructionswithhumanfeedback.Advances
[5] MichaelBloemandNicholasBambos.2014. Infinitetimehorizonmaximum inneuralinformationprocessingsystems35(2022),27730â€“27744.
causalentropyinversereinforcementlearning.In53rdIEEEconferenceondecision [27] SarahPan,VladislavLialin,SherinMuckatira,andAnnaRumshisky.2023.Letâ€™s
andcontrol.IEEE,4911â€“4916. ReinforceStepbyStep.arXivpreprintarXiv:2311.05821(2023).
[6] HeejongBongandAlessandroRinaldo.2022.Generalizedresultsfortheexistence [28] DeanAPomerleau.1991. Efficienttrainingofartificialneuralnetworksfor
andconsistencyoftheMLEintheBradley-Terry-Lucemodel.InInternational autonomousnavigation.Neuralcomputation3,1(1991),88â€“97.
ConferenceonMachineLearning.PMLR,2160â€“2177. [29] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherD.Manning,Ste-
[7] CameronBrowne,EdwardJackPowley,DanielWhitehouse,SimonM.Lucas, fanoErmon,andChelseaFinn.2023. DirectPreferenceOptimization:Your
PeterI.Cowling,PhilippRohlfshagen,StephenTavener,DiegoPerezLiebana, LanguageModelisSecretlyaRewardModel.InAdvancesinNeuralInfor-
SpyridonSamothrakis,andSimonColton.2012.ASurveyofMonteCarloTree mationProcessingSystems36:AnnualConferenceonNeuralInformationPro-
SearchMethods.IEEETrans.Comput.Intell.AIGames4,1(2012),1â€“43. https: cessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,
//doi.org/10.1109/TCIAIG.2012.2186810 2023,AliceOh,TristanNaumann,AmirGloberson,KateSaenko,MoritzHardt,
[8] BaianChen,ChangShu,EhsanShareghi,NigelCollier,KarthikNarasimhan, andSergeyLevine(Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/
and Shunyu Yao. 2023. FireAct: Toward Language Agent Fine-tuning. a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html
CoRR abs/2310.05915 (2023). https://doi.org/10.48550/ARXIV.2310.05915 [30] ToranBruceRichards.2023. https://github.com/Significant-Gravitas/AutoGPT
arXiv:2310.05915 [31] StuartRussell.1998.Learningagentsforuncertainenvironments.InProceedings
[9] ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi,andQuanquanGu.2024. oftheeleventhannualconferenceonComputationallearningtheory.101â€“103.
Self-PlayFine-TuningConvertsWeakLanguageModelstoStrongLanguage [32] JohnSchulman,SergeyLevine,PieterAbbeel,MichaelI.Jordan,andPhilipp
Models. arXiv:2401.01335[cs.LG] https://arxiv.org/abs/2401.01335 Moritz.2015. TrustRegionPolicyOptimization.InProceedingsofthe32nd
[10] ZehuiChen,KuikunLiu,QiuchenWang,WenweiZhang,JiangningLiu,Dahua InternationalConferenceonMachineLearning,ICML2015,Lille,France,6-11July
Lin,KaiChen,andFengZhao.2024.Agent-FLAN:DesigningDataandMeth- 2015(JMLRWorkshopandConferenceProceedings,Vol.37),FrancisR.Bachand
odsofEffectiveAgentTuningforLargeLanguageModels. arXivpreprint DavidM.Blei(Eds.).JMLR.org,1889â€“1897. http://proceedings.mlr.press/v37/
arXiv:2403.12881(2024). schulman15.html
[11] PaulF.Christiano,JanLeike,TomB.Brown,MiljanMartic,ShaneLegg,and [33] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov.
DarioAmodei.2017.DeepReinforcementLearningfromHumanPreferences.In 2017. ProximalPolicyOptimizationAlgorithms. CoRRabs/1707.06347(2017).
AdvancesinNeuralInformationProcessingSystems30:AnnualConferenceonNeu- arXiv:1707.06347 http://arxiv.org/abs/1707.06347
ralInformationProcessingSystems2017,December4-9,2017,LongBeach,CA,USA, [34] NoahShinn,FedericoCassano,AshwinGopinath,KarthikNarasimhan,and
IsabelleGuyon,UlrikevonLuxburg,SamyBengio,HannaM.Wallach,RobFergus, ShunyuYao.2024.Reflexion:Languageagentswithverbalreinforcementlearning.
S.V.N.Vishwanathan,andRomanGarnett(Eds.).4299â€“4307.https://proceedings. AdvancesinNeuralInformationProcessingSystems36(2024).
neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html [35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam
[12] XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang, Trischler,andMatthewJ.Hausknecht.2021. ALFWorld:AligningTextand
HuanSun,andYuSu.2024.Mind2web:Towardsageneralistagentfortheweb. EmbodiedEnvironmentsforInteractiveLearning.In9thInternationalConference
AdvancesinNeuralInformationProcessingSystems36(2024). onLearningRepresentations,ICLR2021,VirtualEvent,Austria,May3-7,2021.
[13] DongyoungGo,TomaszKorbak,GermÃ¡nKruszewski,JosRozen,NahyeonRyu, OpenReview.net. https://openreview.net/forum?id=0IOX0YcCdTn
andMarcDymetman.2023.Aligninglanguagemodelswithpreferencesthrough [36] YifanSong,DaYin,XiangYue,JieHuang,SujianLi,andBillYuchenLin.2024.
f-divergenceminimization.arXivpreprintarXiv:2302.08215(2023). TrialandError:Exploration-BasedTrajectoryOptimizationofLLMAgents.In
[14] JonathanHoandStefanoErmon.2016.GenerativeAdversarialImitationLearning. Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputational
InAdvancesinNeuralInformationProcessingSystems29:AnnualConferenceon Linguistics(Volume1:LongPapers),Lun-WeiKu,AndreMartins,andVivek
NeuralInformationProcessingSystems2016,December5-10,2016,Barcelona,Spain, Srikumar(Eds.).AssociationforComputationalLinguistics,Bangkok,Thailand,
DanielD.Lee,MasashiSugiyama,UlrikevonLuxburg,IsabelleGuyon,and 7584â€“7600. https://aclanthology.org/2024.acl-long.409
RomanGarnett(Eds.).4565â€“4573. https://proceedings.neurips.cc/paper/2016/ [37] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,Chelsea
hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html Voss,AlecRadford,DarioAmodei,andPaulFChristiano.2020. Learningto
[15] XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,andAkikoAizawa.2020. summarizewithhumanfeedback. AdvancesinNeuralInformationProcessing
ConstructingAMulti-hopQADatasetforComprehensiveEvaluationofReason- Systems33(2020),3008â€“3021.
ingSteps.InProceedingsofthe28thInternationalConferenceonComputationalLin- [38] UmarSyed,MichaelH.Bowling,andRobertE.Schapire.2008.Apprenticeship
guistics.InternationalCommitteeonComputationalLinguistics,Barcelona,Spain learningusinglinearprogramming.InMachineLearning,Proceedingsofthe
(Online),6609â€“6625. https://www.aclweb.org/anthology/2020.coling-main.580WWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
Twenty-FifthInternationalConference(ICML2008),Helsinki,Finland,June5- conditionalneuralsequencegeneration.arXivpreprintarXiv:2011.02593(2020).
9,2008(ACMInternationalConferenceProceedingSeries,Vol.307),WilliamW. [55] YujiaZhou,ZhichengDou,andJi-RongWen.2023.Enhancinggenerativeretrieval
Cohen,AndrewMcCallum,andSamT.Roweis(Eds.).ACM,1032â€“1039. https: withreinforcementlearningfromrelevancefeedback.InProceedingsofthe2023
//doi.org/10.1145/1390156.1390286 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.12481â€“12490.
[39] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas- [56] YujiaZhou,ZhengLiu,JiajieJin,Jian-YunNie,andZhichengDou.2024.Metacog-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos- nitiveretrieval-augmentedlargelanguagemodels.InProceedingsoftheACMon
ale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. arXiv WebConference2024.1453â€“1463.
preprintarXiv:2307.09288(2023). [57] DanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.Brown,AlecRadford,Dario
[40] HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal. Amodei,PaulF.Christiano,andGeoffreyIrving.2019.Fine-TuningLanguage
2022. MuSiQue:MultihopQuestionsviaSingle-hopQuestionComposition. ModelsfromHumanPreferences.CoRRabs/1909.08593(2019).arXiv:1909.08593
TransactionsoftheAssociationforComputationalLinguistics(2022). http://arxiv.org/abs/1909.08593
[41] JonathanUesato,NateKushman,RamanaKumar,FrancisSong,NoahSiegel,
LisaWang,AntoniaCreswell,GeoffreyIrving,andIrinaHiggins.2022.Solving
mathwordproblemswithprocess-andoutcome-basedfeedback.arXivpreprint
arXiv:2211.14275(2022).
[42] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,
A Prompts
ZhiyuanChen,JiakaiTang,XuChen,YankaiLin,WayneXinZhao,ZheweiWei,
andJirongWen.2024. Asurveyonlargelanguagemodelbasedautonomous
A.1 WebShop
agents.FrontiersofComputerScience18,6(March2024). https://doi.org/10.1007/
s11704-024-40231-1
[43] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre CÃ´tÃ©, and Prithviraj Am- TaskInstructionforWebShop
manabrolu.2022. ScienceWorld:IsyourAgentSmarterthana5thGrader?.
InProceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguage Youarewebshopping.Iwillgiveyouinstructionsabout
Processing,EMNLP2022,AbuDhabi,UnitedArabEmirates,December7-11,2022,
what to do. You have to follow the instructions. Every
YoavGoldberg,ZornitsaKozareva,andYueZhang(Eds.).AssociationforCom-
putationalLinguistics,11279â€“11298. https://doi.org/10.18653/V1/2022.EMNLP- roundIwillgiveyouanobservationandalistofavailable
MAIN.775 actions,youhavetorespondanactionbasedonthestate
[44] RenxiWang,HaonanLi,XudongHan,YixuanZhang,andTimothyBaldwin.
2024.LearningFromFailure:IntegratingNegativeExampleswhenFine-tuning and instruction. You can use search action if search is
LargeLanguageModelsasAgents.arXivpreprintarXiv:2402.11651(2024). available.Youcanclickoneofthebuttonsinclickables.
[45] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
Anactionshouldbeofthefollowingstructure:
QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
inlargelanguagemodels.Advancesinneuralinformationprocessingsystems35 search[keywords]
(2022),24824â€“24837. click[value]
[46] ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,
Iftheactionisnotvalid,performnothing.Keywordsin
MingZhang,JunzheWang,SenjieJin,EnyuZhou,RuiZheng,XiaoranFan,Xiao
Wang,LimaoXiong,YuhaoZhou,WeiranWang,ChanghaoJiang,YichengZou, search are up to you, but the value in click must be a
XiangyangLiu,ZhangyueYin,ShihanDou,RongxiangWeng,WensenCheng, valueinthelistofavailableactions.Rememberthatyour
QiZhang,WenjuanQin,YongyanZheng,XipengQiu,XuanjingHuang,andTao
Gui.2023. TheRiseandPotentialofLargeLanguageModelBasedAgents:A keywordsinsearchshouldbecarefullydesigned.
Survey. arXiv:2309.07864[cs.AI] https://arxiv.org/abs/2309.07864 Yourresponseshouldusethefollowingformat:
[47] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,Ruslan
Thought:Ithink...
Salakhutdinov,andChristopherD.Manning.2018. HotpotQA:ADatasetfor
Diverse,ExplainableMulti-hopQuestionAnswering.InProceedingsofthe2018 Action:search[something]
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,
October31-November4,2018,EllenRiloff,DavidChiang,JuliaHockenmaier,
andJunâ€™ichiTsujii(Eds.).AssociationforComputationalLinguistics,2369â€“2380.
https://doi.org/10.18653/V1/D18-1259
[48] ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan.2022.WebShop: A.2 Mind2Web
TowardsScalableReal-WorldWebInteractionwithGroundedLanguageAgents.
InAdvancesinNeuralInformationProcessingSystems35:AnnualConferenceon TaskInstructionforMind2Web
NeuralInformationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,
November28-December9,2022,SanmiKoyejo,S.Mohamed,A.Agarwal,Danielle Youareahelpfulassistantthatisgreatatwebsitedesign,
Belgrave,K.Cho,andA.Oh(Eds.). http://papers.nips.cc/paper_files/paper/2022/
navigation,andexecutingtasksfortheuser.
hash/82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html
[49] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,and User:"<html><div><div><atockhomepage/><but-
KarthikNarasimhan.2024.Treeofthoughts:Deliberateproblemsolvingwith tonid=0bookareservation.toggleopen><span>Book
largelanguagemodels. AdvancesinNeuralInformationProcessingSystems36
areservation</span></button><buttonbookareserva-
(2024).
[50] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikR.Narasimhan, tion.toggleopen></button></div><div><selectid=1
andYuanCao.2023. ReAct:SynergizingReasoningandActinginLanguage type><optionreservationstrue>Dinein</option><op-
Models.InTheEleventhInternationalConferenceonLearningRepresentations,
ICLR2023,Kigali,Rwanda,May1-5,2023.OpenReview.net. https://openreview. tionpickup>Pickup</option><optiondelivery>Deliv-
net/pdf?id=WE_vluYUL-X ery </option> <option events> Events </option> <op-
[51] DaYin,FaezeBrahman,AbhilashaRavichander,KhyathiChandu,Kai-WeiChang,
tion wineries> Wineries </option> <option all> Every-
YejinChoi,andBillYuchenLin.2024. AgentLumos:UnifiedandModular
TrainingforOpen-SourceLanguageAgents. arXiv:2311.05657[cs.AI] https: thing </option> </select> <div id=2> <p> Celebrating
//arxiv.org/abs/2311.05657 andsupportingleadingwomenshakinguptheindustry.
[52] ZhengYuan,HongyiYuan,ChengpengLi,GuantingDong,ChuanqiTan,and
</p><span>Explorenow</span></div></div></div>
ChangZhou.2023.ScalingRelationshiponLearningMathematicalReasoning
withLargeLanguageModels.CoRRabs/2308.01825(2023). https://doi.org/10. </html>"BasedontheHTMLwebpageabove,trytocom-
48550/ARXIV.2308.01825arXiv:2308.01825 pletethefollowingtask:
[53] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong,
andJieTang.2023. AgentTuning:EnablingGeneralizedAgentAbilitiesfor Task:CheckforpickuprestaurantavailableinBoston,NY
LLMs.CoRRabs/2310.12823(2023). https://doi.org/10.48550/ARXIV.2310.12823 onMarch18,5pmwithjustoneguest
arXiv:2310.12823
Previousactions:None
[54] ChuntingZhou,GrahamNeubig,JiataoGu,MonaDiab,PacoGuzman,Luke
Zettlemoyer,andMarjanGhazvininejad.2020.DetectinghallucinatedcontentinFromNovicetoExpert:LLMAgentPolicyOptimizationviaStep-wiseReinforcementLearning WWWâ€™25,April28-May02,2025,Sydney,Australia
Whatshouldbethenextaction?Pleaseselectfromthe </div></div></li><divid=1>EnterpriseFleetManage-
followingchoices(Ifthecorrectactionisnotinthepage ment</div></ul></nav><divregion><buttonid=2se-
above,pleaseselectA.â€™Noneoftheaboveâ€™): lectedpick-update03/19/2023><span><span>19</span>
A.Noneoftheabove <div><span>Mar</span><span>2023</span></div>
B.<buttonid=0bookareservation.toggleopen><span> </span></button></div></div></html>"Basedonthe
Booka HTMLwebpageabove,trytocompletethefollowingtask:
C.<selectid=1type><optionreservationstrue>Dinein Task:FindaminivanatBrooklynCityfromApril5thto
</option><option April8thfora22yearoldrenter.
D. <div id=2> <p> Celebrating and supporting leading Previousactions:[searchbox]Pick-up&ReturnLocation
womenshakingup (ZIP,CityorAirport)(...->TYPE:Brooklyn[option]Brook-
Assistant:Answer:C.Action:SELECT,Value:Pickup lyn,NY,USSelect->CLICK
User: "<html> <div> <main main> <section tabpanel> Whatshouldbethenextaction?Pleaseselectfromthe
<div><ultablist><litabheadinglevel3searchand></li> followingchoices(Ifthecorrectactionisnotinthepage
<li id=0 tab heading level 3 search and> <span> Hotel above,pleaseselectA.â€™Noneoftheaboveâ€™):
</span> </li> <li tab heading level 3 search and> </li> A.Noneoftheabove
<litabheadinglevel3searchand></li></ul><divtab- B.<divid=0><div><div><div>BuyACar</div><div>
panel><divid=1><div><span>Dates*</span><button C.<divid=1>EnterpriseFleetManagement</div>
buttoncleardates/></div><div><label>Travelers</la- D.<buttonid=2selectedpick-update03/19/2023><span>
bel> <div> <p> 1 Adult </p> <button button> 1 Adult <span>19</span>
</button><divdialog><buttonbuttontravelwithapet. Assistant:Answer:D.Action:CLICK
this><span>Travelwithapet</span></button><div>
<buttonbuttonclearallfields>Clearall</button><but-
tonbutton></button></div></div></div></div></div>
</div></div></section></main><footercontentinfo> A.3 ScienceWorld
<div><h3>StayConnected</h3><ulid=2><amobile TaskInstructionforScienceWorld
tools></a><aopenunitedâ€™stiktokfeedin></a><aopen
Youareahelpfulassistanttodosomescientificexperiment
unitedâ€™sfacebookpagein></a><aopenunitedâ€™stwitter
inanenvironment.Intheenvironment,thereareseveral
feedin></a><aopenunitedâ€™syoutubepagein></a><a
rooms: kitchen, foundry, workshop, bathroom, outside,
openunitedâ€™sinstagramfeedin></a><aopenunitedâ€™s
livingroom,bedroom,greenhouse,artstudio,hallwayYou
linkedinprofilein></a></ul></div></footer></div>
shouldexploretheenvironmentandfindtheitemsyou
</html>"BasedontheHTMLwebpageabove,trytocom-
needtocompletetheexperiment.Youcanteleporttoany
pletethefollowingtask:
roominonestep.Allcontainersintheenvironmenthave
Task:Comparethefaretypestobooka1-adultticketfrom
alreadybeenopened,youcandirectlygetitemsfromthe
Springfiels,ILtoAustin,TXforApril29th2023
containers.Theavailableactionsare:
Previousactions:[combobox]Enteryourdepartingcity,
openOBJ:openacontainer
airportname,orairpor...->TYPE:SPRINGFIELD[button]
closeOBJ:closeacontainer
Springfield,IL,US(SPI)->CLICK[combobox]Enteryour
activateOBJ:activateadevice
destinationcity,airportname,orairp...->TYPE:AUSTIN
deactivateOBJ:deactivateadevice
[button]Austin,TX,US(AUS)->CLICK
connectOBJtoOBJ:connectelectricalcomponents
Whatshouldbethenextaction?Pleaseselectfromthe
disconnectOBJ:disconnectelectricalcomponents
followingchoices(Ifthecorrectactionisnotinthepage
useOBJ[onOBJ]:useadevice/item
above,pleaseselectA.â€™Noneoftheaboveâ€™):
lookaround:describethecurrentroom
A.Noneoftheabove
examineOBJ:describeanobjectindetail
B.<liid=0tabheadinglevel3searchand><span>Hotel
lookatOBJ:describeacontainerâ€™scontents
C.<divid=1><div><span>Dates*</span><buttonbutton
readOBJ:readanoteorbook
cleardates
moveOBJtoOBJ:moveanobjecttoacontainer
D.<ulid=2><amobiletools></a><aopenunitedâ€™stiktok"
pickupOBJ:moveanobjecttotheinventory
Assistant:Answer:A.
pourOBJintoOBJ:pouraliquidintoacontainer
User:"<html><div><navmainmenu><ul><li><div
mixOBJ:chemicallymixacontainer
button>CarSales</div><divid=0><div><div><div>
teleporttoLOC:teleporttoaspecificroom
BuyACar</div><div>PlanYourPurchase</div></div>
focusonOBJ:signalintentonataskobject
<div><h4>ItsTaxRefundTime.TreatYourselftoanUp-
wait:tasknoactionfor10steps
grade.</h4><p>Withavarietyofoptions,investyour
wait1:tasknoactionforastep
refundinwhatyoureallywant-aquality,usedvehicle
Yourresponseshouldusethefollowingformat:
from Enterprise. </p> <a> View Inventory </a> </div>WWWâ€™25,April28-May02,2025,Sydney,Australia ZhiruiDeng,ZhichengDou,YutaoZhu,Ji-RongWen,RuibinXiong,MangWang,andWeipengChen
A.5 HotpotQA,2WikimultihopQAandMusique
Thought:Ithink...
Action:openOBJ TaskInstructionforMultihop-QADatasets
Youareanexpertinthisfield.Pleaseanswerthequestion
assimplyandconciselyaspossible.EveryroundIwillgive
A.4 ALFWorld youanobservation,youhavetorespondwithinterleaving
ThoughtandActionsteps.Thoughtcanreasonaboutthe
TaskInstructionforALFWorld
currentsituation,andActioncanbetwotypes:
Interact with a household to solve a task. Imagine you (1) Search[entity], which searches the exact entity on
areanintelligentagentinahouseholdenvironmentand Wikipediaandreturnsthefirstparagraphifitexists.If
yourtargetistoperformactionstocompletethetaskgoal. not,itwillreturnsomesimilarentitiestosearch.
Atthebeginningofyourinteractions,youwillbegiven (2)Finish[answer],whichreturnstheanswerandfinishes
thedetaileddescriptionofthecurrentenvironmentand thetask.
yourgoaltoaccomplish.Foreachofyourturn,youwill Yourresponseshouldusethefollowingformat:
begiventheobservationofthelastturn.Youshouldfirst Thought:Ithink...
thinkaboutthecurrentconditionandplanforyourfuture Action:...
actions,andthenoutputyouractioninthisturn.
Theavailableactionsare:
1.gotorecep
2.taskobjfromrecep
3.putobjin/onrecep
4.openrecep
5.closerecep
6.toggleobjrecep
7.cleanobjwithrecep
8.heatobjwithrecep
9.coolobjwithrecep
whereobjandrecepcorrespondtoobjectsandreceptacles.
Afteryoureachturn,theenvironmentwillgiveyouim-
mediatefeedbackbasedonwhichyouplanyournextfew
steps.iftheenvrionmentoutput"Nothinghappened",that
meansthepreviousactionisinvalidandyoushouldtry
moreoptions.
Yourresponseshouldusethefollowingformat:
Thought:<yourthoughts>
Action:<yournextaction>