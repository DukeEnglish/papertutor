Analogist: Out-of-the-box Visual In-Context Learning with Image
Diffusion Model
ZHENGGU,CityUniversityofHongKongandStateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
SHIYUANYANG,CityUniversityofHongKongandTianjinUniversity,China
JINGLIAOâˆ—,CityUniversityofHongKong,China
JINGHUOâˆ—,StateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
YANGGAO,StateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China
ImageColorization ImageDeblurring ImageDenoising Low-lightEnhancement
ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€²
ğ‘¨â€² ğ‘¨â€² ğ‘¨â€² ğ‘¨â€²
ImageEditing ImageTranslation StyleTransfer MotionTransfer
ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€²
ğ‘¨â€² ğ‘¨â€² ğ‘¨â€² ğ‘¨â€²
Skeleton-to-imageGeneration Mask-to-imageGeneration ImageInpainting Object Multiplication
ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€² ğ‘¨ ğ‘© ğ‘©â€²
ğ‘¨â€² ğ‘¨â€² ğ‘¨â€² ğ‘¨â€²
Fig.1. Examplesofin-contextvisualgenerationbyourmethodusingapretrainedStableDiffusionInpaintingmodelaredemonstrated.Withanexampleimage
pairğ´andğ´â€²,illustratingavisualtransformation,andaqueryimageğµ,ourmethodenhancesthemodelâ€™scapacityforvisualin-contextcomprehension,pro-
ducingareasonableoutputğµâ€²thatfollowsthesamevisualpattern.Sourceimages:ImageNet[Dengetal.2009],LOL[Chenetal.2018],InstructPix2Pix[Brooks
etal.2023],TongYiQianWenAPP,UBC-Fashion[Zablotskaiaetal.2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016],DALLE-3[Betkeretal.2023].
VisualIn-ContextLearning(ICL)hasemergedasapromisingresearcharea bytextprompts.Ourmethodisout-of-the-boxanddoesnotrequirefine-
duetoitscapabilitytoaccomplishvarioustaskswithlimitedexamplepairs tuningoroptimization.Itisalsogenericandflexible,enablingawiderange
throughanalogicalreasoning.However,training-basedvisualICLhaslim- ofvisualtaskstobeperformedinanin-contextmanner.Extensiveexperi-
itationsinitsabilitytogeneralizetounseentasksandrequiresthecollec- mentsdemonstratethesuperiorityofourmethodoverexistingapproaches,
tionofadiversetaskdataset.Ontheotherhand,existingmethodsinthe bothqualitativelyandquantitatively.Ourprojectwebpageisavailableat
inference-basedvisualICLcategorysolelyrelyontextualprompts,which https://analogist2d.github.io.
failtocapturefine-grainedcontextualinformationfromgivenexamplesand
CCSConcepts:â€¢Computingmethodologiesâ†’Imageprocessing.
canbetime-consumingwhenconvertingfromimagestotextprompts.To
addressthesechallenges,weproposeAnalogist,anovelinference-based AdditionalKeyWordsandPhrases:VisualIn-ContextLearning,Diffusion
visualICLapproachthatexploitsbothvisualandtextualpromptingtech- Models,ImageTransformation
niquesusingatext-to-imagediffusionmodelpretrainedforimageinpainting.
Forvisualprompting,weproposeaself-attentioncloning(SAC)methodto 1 INTRODUCTION
guidethefine-grainedstructural-levelanalogybetweenimageexamples.
Asoneofthemostpopularresearchtopicsintherecentfieldof
Fortextualprompting,weleverageGPT-4Vâ€™svisualreasoningcapabilityto
naturallanguageprocessing(NLP),in-contextlearning(ICL)rep-
efficientlygeneratetextpromptsandintroduceacross-attentionmasking
resentsaparadigmwhereinlargelanguagemodels(LLMs)acquire
(CAM)operationtoenhancetheaccuracyofsemantic-levelanalogyguided
theabilitytolearntasksbasedonalimitedsetofdemonstrative
âˆ—JingLiaoandJingHuoaretheco-correspondingauthors. examples[Dongetal.2022].Unlikesupervisedlearning,ICLdirectly
generatespredictionsusingpretrainedLLMs[Brownetal.2020].
Authorsâ€™addresses:ZhengGu,guzheng@smail.nju.edu.cn,CityUniversityofHong Thisparadigmoffersaninterpretableinterfaceforinteractingwith
KongandStateKeyLabforNovelSoftwareTechnology,NanjingUniversity,China;
ShiyuanYang,s.y.yang@my.cityu.edu.hk,CityUniversityofHongKongandTianjin LLMsthroughlanguagedemonstrations,mirroringhumandecision-
University,China;JingLiao,jingliao@cityu.edu.hk,CityUniversityofHongKong, makingbylearningthroughanalogiesandsimilarexperiences.ICL
China;JingHuo,huojing@nju.edu.cn,StateKeyLabforNovelSoftwareTechnology,
significantlylowerscomputationalcostsforadaptingmodelstonew
NanjingUniversity,China;YangGao,gaoy@nju.edu.cn,StateKeyLabforNovelSoft-
wareTechnology,NanjingUniversity,China. tasks,makinglanguage-model-as-a-servicefeasibleandenabling
4202
yaM
61
]VC.sc[
1v61301.5042:viXra2 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
practicalapplicationsinlarge-scale,real-worldtaskssuchasma- 2023]thatrelyontime-consumingtextualinversionoptimization,
chinetranslation[Xuetal.2023],informationextraction[Heetal. weproposeutilizingGPT-4Vâ€™svisualreasoningcapabilitytoana-
2023],andcomplexityreasoning[Weietal.2022]. lyzethesemantictransformationbetweenğ´andğ´â€² andapplyit
FollowingthesuccessofNLP,researchinvisualIn-ContextLearn- analogicallytoğµtogenerateatextualdescriptionofğµâ€².Thisisfa-
inghasentereditsembryonicstageofexploration[Baietal.2023; cilitatedbyourwell-designedgraphicalandtextualinstructionsfed
Yangetal.2023a].Specifically,whenthedemonstrationisapairof intoGPT-4V.Furthermore,weintroduceacross-attentionmasking
imagesğ´andğ´â€²,visualin-contextlearningcanbeconsideredasan (CAM)operationtorestricttheinteractionbetweentextandimage
imageanalogyproblem[Hertzmannetal.2001].Thisinvolvesanalo- totheğµâ€²regiononly,whichensuresthatthetextualpromptmore
gizingtheobservedtransformationfromğ´toğ´â€² andapplyingit accuratelyguidesthegenerationofğµâ€².
ontoaqueryimageğµ,resultinginğµâ€².Thisanalogycapabilityholds Withbothsemantic-level(coarse-grained)andstructural-level
significantpotentialincomputergraphicsandvisiontasks[Caoetal. (fine-grained)contextualinformationrespectivelyprovidedbytex-
2023;Parmaretal.2023;Å ubrtovÃ¡etal.2023].Forexample,asshown tualandvisualpromptingtechniques,ourapproachiscapableof
inFigure1,withjustasinglepairofexampleswithouttrainingona performingawiderangeofvisualtasksinanin-contextmanner,
largedataset,thepretrainedmodelcanperformtasksrangingfrom asillustratedinFigure1.Ourapproachisanout-of-the-boxsolu-
low-leveltaskssuchascolorization,deblurring,denoising,etc.,to tionthatonlyrequiresoneforwardstepofapretraineddiffusion
high-leveltaskssuchasimageediting,imagetranslation,motion model,withouttheneedforfine-tuningoroptimization.Extensive
transfer,etc.VisualICLalsoofferssignificantpotentialinenhancing experimentsandcomparisonsacrossdifferenttaskshaveconfirmed
creativeworkflows.Designerscanleverageamodeltolearndesign thatourmethodoutperformsexistingtraining-basedandinference-
ideassuchascolorthemes,typography,andvisualmotifsfroman basedvisualICLmethods,bothqualitativelyandquantitatively.Our
examplepairandadaptthemanalogouslytodifferentcontents. methodisprimarilydesignedforapplicationswheretheinputğ´and
ExistingvisualICLworksfallintotwocategories:training-based ğ´â€²arespatiallyaligned.Nonetheless,weshowthatitholdspromise
andinference-based.Training-basedmethodstrainthegenerative forapplicationsinmisalignedscenariosaswell.Insummary,our
modelondiversein-contexttasks[Najdenkoskaetal.2023;Wang contributionscanbesummarizedasfollows:
etal.2023a].TheICLcapabilitiesprimarilyexhibittaskssimilarto
â€¢ We introduce Analogist, an out-of-the-box approach for
theirtrainingtasksandhavelimitationswhenappliedtounseen
visualin-contextlearningthatutilizesapretraineddiffusion
tasks.Moreover,collectingandorganizingthedataintoin-context
inpainting model along with effective visual and textual
taskformatislaborious.Inference-basedmethodsconductICLvia
promptingtechniques.
appropriatepromptingthemodelduringinference,possessingbetter
â€¢ Invisualprompting,weproposeaSelf-AttentionCloning
generalizability.However,existingmethods[Nguyenetal.2023;
(SAC)methodthateffectivelyguidestheimageinpainting
Å ubrtovÃ¡etal.2023]convertthegivenimagesintotextualprompts,
modeltoexploitfine-grainedcontextualinformationinthe
fallingshortintwoaspects.First,thetextualpromptingiscoarse-
2Ã—2gridvisualprompt.
grainedandcannotcoverthedetailedinformationpresentedinthe
â€¢ In textual prompting, we propose to efficiently generate
imageexamples.Second,textualinversionfromimagesrequires
textualpromptsusingGPT-4Vandenhancetheaccuracyof
iterativeoptimization,whichisstilltime-consuming.
textualguidancebyintroducingaCross-AttentionMasking
Inthiswork,weproposeAnalogist,anovelinference-basedvisual
(CAM)operation.
ICLapproach,toaddresstheaforementionedchallenges.Weintro-
ducebothvisualandtextualpromptingtechniquesonapretrained
2 RELATEDWORK
text-to-imagediffusionmodel.
2.1 VisualIn-contextLearning
Firstly,weintroduceanovelvisualpromptingtechniquetoover-
comethecoarse-granularityissueintextualprompting.Inspired InspiredbythetaxonomyinDongetal.[2022],wecategorizecur-
byMAEVQGAN[Baretal.2022],weformulatetheICLtaskasan rentvisualin-contextlearningintotwogroups,training-basedand
imageinpaintingtaskbyarrangingtheexemplaryimagepairğ´and
inference-based, based on the criterion of whether the model is
ğ´â€²,thequeryimageğµ,andtheunknownimageğµâ€²ina2Ã—2grid.
trainedonin-contexttasks.
Then,weutilizeapretraineddiffusioninpaintingmodeltofillin
theregionofğµâ€².Toguidetheinpaintingprocesswithfine-grained Training-basedMethods. Training-basedmethodstrain(orfine-
visualcontextualinformation,weproposeaself-attentioncloning tune)themodelondiversein-contexttasks.Painter[Wangetal.
(SAC)method.Thismethodclonestheself-attentionmapsbetween 2023b]usespairedinputandoutputimagesasvisualpromptsto
ğ´andğµtotheself-attentionmapsbetweenğ´â€²andğµâ€²duringthe trainaVisionTransformer[Dosovitskiyetal.2020],whichenables
forwardpropagationofthediffusioninpaintingmodel.Sincethe themodeltolearnandperformawiderangeofvisiontasks.The
self-attentionmapsrepresentsimilaritybetweenpixels,theSAC follow-upworkSegGPT[Wangetal.2023c]extendsthein-context
methodeffectivelyhelpslearnstructural-levelrelationshipsbetween learningcapabilitiesofPainterspecificallyforpreciseandadapt-
ğ´andğµ,whicharethenappliedtoğ´â€²togenerateğµâ€²analogically. ablesegmentationacrossvariousdomains.Morerecently,several
Inadditiontovisualpromptingofferingstructural-levelguidance, workprogressivelyexhibitstheICLabilityofstate-of-the-artdif-
weincorporatetextualpromptingtooffersemantic-levelguidance fusionmodels[Rombachetal.2022].PromptDiffusion[Wangetal.
byprovidingappropriatetextpromptstotheinpaintingmodel.How- 2023a] introduces ControlNet [Zhang et al. 2023] to tune a pre-
ever,unlikepreviousmethods[Nguyenetal.2023;Å ubrtovÃ¡etal. trainedStableDiffusiononsixmanuallydesignedvision-languageAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 3
tasks.Theproposedmethodisabletogeneralizetosimilar,contex- Ourworkaimstotackletheproblemofimageanalogiesinthe
tuallyrelatedunseentasks.However,itposeschallengeforusersto paradigmofvisualin-contextlearning.Differentfromtraditional
offerdetailedandprecisetextdescriptions.ImageBrush[SUNetal. texture synthesis approaches [Hertzmann et al. 2001; Liao et al.
2023]introducesanovelframeworkforimagemanipulationusing 2017],theanalogyisachievedbypromptingapre-trainedtext-to-
in-contextvisualinstructions,ratherthannaturallanguage.Anaddi- imagediffusionmodelandcanbeappliedtomoreapplicationssuch
tionalpromptencoderisintroducedtotranslatethevisualchanges aslow-leveltasks,manipulationtasks,andvisiontasks.
depictedintheexampleimagesintotextfeaturestoguidethein-
2.3 Prompt-basedImageEditing
paintingmodel.ImageBrushisbuiltonadiffusion-basedinpainting
modelandtrainedonseveralvisiondatasets.Theabovetraining- Recentmultimodalapproacheshavedemonstratedsuperiortext-
basedmethodsnecessitatetheconstructionofhigh-qualityanddi- imagefeaturealignmentcapabilities[Lietal.2022;Radfordetal.
versetasks,makingthepipelinelaboriousandinflexible.Meanwhile, 2021],leadingtoaseriesofworksonprompt-basedimageediting.
thetesttasksshouldideallybearsomesimilaritytothetraining PreviousGAN-basedmethodsperformmanipulationinthelatent
tasks,suggestingopportunitiesforimprovinggeneralizability. spaceviaGANinversion[Baykaletal.2023;Patashniketal.2021;
Xiaetal.2022].Morerecentmethodsutilizetext-to-imagediffusion
Inference-basedMethods. Insteadoftuningthemodelparameters, modelstoattainleadingoutcomes[Brooksetal.2023;Caoetal.2023;
inference-basedmethodsinspirethemodelâ€™sunderstandingonthe Parmaretal.2023].However,thesemethodsstruggletodoimage
givendemonstrationsduringinferencetime.Amongthem,MAEVQ- analogytasksincetheytaketextualdescriptionsasinput,which
GAN[Baretal.2022]innovativelyproposesavisualprompting isnotsufficientlyintuitiveandaccuratetodepictdetailsrelatedto
formatofinpaintingthemissingpatchina2Ã—2grid-likeimage. theimagestructure.Incontrast,ourworktakesapairofimages
Themodelispre-trainedonfiguresfromcomputervisionpapers asdemonstrationinput,utilizesself-attentiontoprovidestructure-
whicharetypicallyinaregulargridpatternandemergeswithICLca- relatedinformation,andautomaticallyacquiresthecorresponding
pability.However,thegenerationeffectsarenotentirelysatisfactory textualdescriptionthroughGPT-4V.
duetolimitationsindatasetsizeandmodelcapacityincomparison
3 PRELIMINARY
withthelatestdiffusionmodels.VISII[Nguyenetal.2023]considers
thedemonstrationasimagesbeforeandafterimageediting.This SinceourapproachutilizesapretrainedStableDiffusioninpainting
approachestimatestheeditinginstructionbasedonapretrained model,webrieflyreviewlatentStableDiffusioninSection3.1as
text-basedimageeditingmodel[Brooksetal.2023],producingre- wellastheStableDiffusioninpaintingmodelinSection3.2.
sultswithhigherquality.However,reverse-engineeringthetextual
descriptionofthedifferencesbetweentwoimagesthroughoptimiza- 3.1 LatentDiffusionModels.
tionremainstime-consuming.Whatâ€™smore,bytransferringvisual
DenoisingDiffusionProbabilisticModels(DDPM)[Hoetal.2020]
informationtocoarse-grainedtext,thegenerationprocessismerely
areaclassofgenerativemodelsthatgraduallyconvertrandomnoise
drivenbytextualdescriptions.Theroleofvisualpromptingisnot
intostructureddatathroughaseriesofreversediffusionstepsbased
fullyleveraged,leadingtoinaccuratecontextualunderstanding.
onaMarkovchain.LatentDiffusionModels(LDM)likeStableDif-
Ourworkfallsintothecategoryofinference-basedmethodsand,
fusion(SD)[Rombachetal.2022]enhancesDDPMbyemployingan
notably,eliminatestheneedforadditionaloptimizationsteps.In- encoderğ¸tomaphigh-dimensionaldatağ‘¥ intolower-dimensional
steadofsolelyrelyingontextualprompts,ourapproachleverages latentspaceğ‘§ = ğ¸(ğ‘¥).ThegenerationofStableDiffusioncanbe
bothtextualandvisualprompting.Thisallowsustorespectivelyun- guidedbyanadditionaltextembeddingğ‘(ğ‘¦)encodedbyCLIP[Rad-
derstandsemantic-levelandstructural-levelcontextualinformation fordetal.2021]andatextpromptğ‘¦.Duringtraining,anUNetmodel,
forvisualICL.Besides,ourmethodutilizesGPT-4Vtogettextual parameterizedbyğœƒ,isoptimizedtoeliminatethenoiseğœ–introduced
promptsinsteadoftextualinversion. intoğ‘§ ğ‘¡:
2.2 ImageAnalogies L=E ğ‘§âˆ¼ğ¸(ğ‘¥),ğ‘¦,ğœ–âˆ¼N(0,1),ğ‘¡ (cid:2) âˆ¥ğœ–âˆ’ğœ– ğœƒ(ğ‘§ ğ‘¡,ğ‘¡,ğ‘(ğ‘¦))âˆ¥2 2(cid:3). (1)
Definedbyğ´:ğ´â€² ::ğµ:ğµâ€²,thegoalofimageanalogies[Hertzmann
Duringinference,arandomlysampledlatentğ‘§
ğ‘‡
âˆ¼N(0,1)ispro-
etal.2001]istofindanâ€œanalogousâ€imageğµâ€²thatrelatestoğµinthe gressivelydenoisedthroughthemodeltoproduceacleanlatent
samewayasğ´â€²relatestoğ´.Suchideacanbeextendedinvarious representationğ‘§ 0by
waysofimagesynthesis[Diamantietal.2015;JamriÅ¡kaetal.2019; 1 (cid:20) 1âˆ’ğ›¼ ğ‘¡ (cid:21)
Liaoetal.2017;Yuanetal.2024].Recently,DIA[Å ubrtovÃ¡etal. ğ‘§ ğ‘¡âˆ’1= âˆš ğ›¼
ğ‘¡
ğ‘§ ğ‘¡ âˆ’ 1âˆ’âˆš ğ›¼Â¯ğ‘¡ğœ– ğœƒ (ğ‘§ ğ‘¡,ğ‘¡,ğ‘(ğ‘¦)) , (2)
2023]investigatestheimageanalogiestaskwithDiffusionmodel.
ThismethodestimatestheCLIPfeaturesofthegivenimages.The whereğ›¼Â¯ğ‘¡ =(cid:206) ğ‘–ğ‘¡ =1ğ›¼ ğ‘¡.Subsequently,thecleanlatentisfedintothe
CLIPfeaturesareinjectedintoapretrainedtext-to-imagediffusion decodertoobtainthegeneratedimageğ·(ğ‘§ 0).
modeltoprovidein-contextguidance.DIAiscapableofexecuting
3.2 StableDiffusionInpaintingModel
example-basedimageeditingthatencompassescomplex,higher-
levelcontextualorstructuralrelationships.However,sincethegoal WeapplyourmethodoverthepretrainedStableDiffusioninpainting
ofCLIPistoalignimageandtextspaces,theestimatedfeaturesare model,whichisfine-tunedtoboastsanadditionalfeatureofimage
highlevelandstruggletocapturedetailedimageinformation. inpainting. The forward process of the inpainting pipeline is as4 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
Visual Prompting Self-Attention Cloning (SAC) Cross-Attention Masking (CAM) Textual Prompting
ğ‘¨ ğ‘¨â€² ğ‘¨â€²
ğ‘¨ ğ‘¨â€² âŒ ğ‘¨ ğ‘¨â€²
addmarks
âŒ add arrows ğ‘© ğ‘©â€²
ğ¸(ğ¼) Close-up of a
ğ‘© Input ğ¼ tigerâ€™s face
concat Please help me with the image analogy
ğ‘¨ ğ‘¨â€² âŒ task: take an image A and its transfor-
ğ‘© clone result ğ‘©â€² mation Aâ€™, and provide any image B to
produce an output Bâ€™ that is analogous
: Self-attention Relations âœ” to Aâ€™. Or, more succinctly: A : Aâ€™ :: B :
Bâ€™. You should give me the text prompt
of image Bâ€™ with no more than 5 words.
Mask
ğ‘© ğ¼â€² ğ‘© Stable Diffusion Inpainting
Based on the provided image transforma-
ğ‘¨ ğ‘¨â€² tion from A to Aâ€™, where a domestic cat
Encoder in image A is transformed into a tiger in
image Aâ€™, the analogy for image B which
fD orD wI aM rd ï¼Ÿ concat ğ‘¥ " SAC CAM SAC CAM ğ‘¥ "#$ ğ‘¥ % f i cme ha aat ru g ar e ce ts eB râ€™a i s f td e icao stm u tore i s n tt hgic e a cc a a ttt i ig ns eh r Bo .u wld it hr e ss iu mlt i lain r
Thus, the text prompt for image Bâ€™ would
UNet ğ‘© Output ğ‘©â€² be: â€œClose-up of a tiger's faceâ€
ğ¸(ğ¼â€²) ğ‘¥
!
Fig.2. OverviewoftheproposedAnalogist.Avisualdemonstrationisdefinedbyanexamplepairğ´(womanholdingacat)andğ´â€²(thesamewomanholdinga
tiger).Givenanewimageğµ(anothercat),weformatthesethreeimagesintoa2Ã—2gridandtacklethisproblembyfillthemissingimageviaapretrained
StableDiffusioninpaintingmodel.WeemployGPT-4Vtoprovideapropertextdescription(i.e.,â€œclose-upofatigerâ€™sfaceâ€)tofurtherguidetheinpainting
process.Duringtheprocessofmodelinference,Self-AttentionCloning(SAC)andCross-AttentionMasking(CAM)areintroducedtoencouragethemodel
concentrateonthevisualandtextualprompts,thusenhanceitsin-contextlearningcapacities.Sourceimage:InstructPix2Pix[Brooksetal.2023].
follows:
1 (cid:20) 1âˆ’ğ›¼ ğ‘¡ (cid:21)
ğ‘§ ğ‘¡âˆ’1= âˆš ğ›¼
ğ‘¡
ğ‘§ ğ‘¡ âˆ’ 1âˆ’âˆš ğ›¼Â¯ğ‘¡ğœ– ğœƒ (ğ‘§ ğ‘¡,ğ‘¡,ğ‘(ğ‘¦),ğ¸(ğ¼ ğ‘š),ğ‘€) , (3)
TheUNetisupdatedtoincludefiveextrainputchannelsâ€“four
dedicatedtotheencodedmasked-imageğ¸(ğ¼ ğ‘š)andoneforthemask
ğ‘€itself.Thesetwoextrainputsareconcatedwithğ‘§ ğ‘¡ tofedintothe
UNettopredictthenoiseateachtimestep.
4 METHOD ğ‘¨ ğ‘©
ThegoalofICListoencouragepretrainedmodeltolearntasks Fig.3. Visualizationoftheattentionrelationships.Givenananchorpoint
givenonlyafewexamplesintheformofdemonstration[Dong onimageğ´(showninred,green,andbluecolors),wecalculatetheatten-
et al. 2022]. Specific to the image domain, the demonstration is tionvaluesbetweenthispointandallregionsofimageğµ.Soucreimage:
definedasanexampleimagepairğ´andğ´â€²,whereğ´â€²istheresult InstructPix2Pix[Brooksetal.2023].
obtainedbyapplyingacertainvisualeffectortransformationto
ğ´.Givenanewqueryimageğµ,themodelisexpectedtoapplythe
sameeffecttoğµ,thuscreatinganewimageğµâ€²,sothatğ´:ğ´â€² ::ğµ: 4.1 VisualPrompting
ğµâ€²[Hertzmannetal.2001].Thisprocessdemonstratesthemodelâ€™s
Tointroducefine-grainedstructural-levelvisualguidanceinthein-
understanding and replication of visual transformations from a
contextinferenceprocess,weconstructavisualpromptintheform
givendemonstrationtoanewcontext,exhibitingtheICLability.
ofa2Ã—2grid-likeimageforthepretrainedinpaintingmodel,and
AsillustratedinFigure2,toaddressthisissue,weapproachit
providevisualcontextualinformationbycloningtheself-attention
frombothvisualstructural-level(Section4.1)andtextualsemantic-
associationsbetweenthegivenimages.
level(Section4.2)perspectives.Forvisualprompting(redregion
inFigure2),weformulatetheinputimagesintoa2x2gridimage, 4.1.1 2Ã—2-gridPrompting. Imageinpaintingmodelsfillinunknown
utilizingapretraineddiffusioninpaintingmodeltofillinthemissing areasofanimagebasedonitsknownregions,whichnaturallyaligns
regioninSection4.1.1.Tointroducemorefine-grainedvisualinfor- withtheconceptofICL.AsshowninFigure2,totakeadvantage
mation,weproposeSelf-AttentionCloning(SAC)inSection4.1.2. ofthisproperty,wefirstrearrangetheinputimagesğ´,ğ´â€²,andğµ
Fortextualprompting(blueregioninFigure2),GPT-4Viselabo- intoasingle2Ã—2grid-likeimage,denotedasğ¼.Imageğµispasted
ratedtoprovidesemantic-levelguidancetothegenerationprocess tothebottomrightcornerofthegridimage,gettingimageğ¼â€².We
inSection4.2.1.Tofostersemanticcorrespondencebetweenthe extractthefeaturesofthepastedimage,ğ¸(ğ¼â€²),andaddnoisetoitvia
inpaintedimageandthetextprompt,weproposeCross-Attention diffusionforwardprocess,gettingtheinitialğ‘¥ ğ‘‡.Toalignwiththe
Masking(CAM)inSection4.2.2. interfaceofthepretrainedmodel,amaskimageğ‘€issimultaneously
Encoder
redoceDAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 5
â„ğ‘¤Ã—ğ‘ Self-AttentionCloning(SAC) â„ğ‘¤Ã—ğ‘ Cross-AttentionMasking(CAM)
ğ‘„ â„³ ğ´",ğµ" â‰”â„³(ğ´,ğµ)(ğ‘  ğ‘„ â„³ ğ´ â‰”0; â„³ ğ´" â‰”0; â„³ (ğµ)â‰”0
! ! ! ! !
â„ğ‘¤Ã—â„ğ‘¤ â„ğ‘¤Ã—ğ¿
â„ğ‘¤ â„ğ‘¤ â„ğ‘¤ â„ğ‘¤ â„ğ‘¤ â„ğ‘¤ â„ğ‘¤
â„ğ‘¤Ã—ğ‘ Ã— Ã— ğ¿Ã—ğ‘ Ã—ğ¿ Ã—ğ¿ Ã—ğ¿
4 4 4 4 4 4 4
ğ¾ ğ‘  Softmax ğ¾ 0 0 0
â„³!(ğ´,ğµ) â„³!(ğ´â€²,ğµâ€²) â„³! ğ´ â„³! (ğ´â€²) â„³! (ğµ)
â„ğ‘¤Ã—ğ‘ â„³ ! ğ¿Ã—ğ‘ â„³ !
ğ‘‰ ğ‘ ğ‘‰ ğ‘
: Assignment operation : Assignment operation
Fig.4. Detailedillustrationofself-attentioncloning(SAC).Thesubself- Fig.5. Detailedillustrationofcross-attentionmasking(CAM).Thesub
attentionmapMğ‘ (ğ´â€²,ğµâ€²)issetasthevalueofMğ‘ (ğ´,ğµ),denotingcloning cross-attentionmapbetweentextembeddingandregionsğ´,ğ´â€²,andğµare
therelationbetweenğ´andğµtothatofğ´â€²andğµâ€². settozero,makingthesemanticguidancemorefocusedonregionğµâ€².
generated.Inthismask,thebottomrightregionisentirelyones, whereğ‘ isacoefficientusedtobalancethedegreeofpreservingthe
whiletheremainingregionsarezeros.Ateachtimestepğ‘¡,thelatent structureofimageğµandthedegreeofapplyingtransformations.
ğ‘¥ ğ‘¡ âˆˆRğ‘Ã—4Ã—â„Ã—ğ‘¤ isconcatenatedwiththefeatureğ¸(ğ¼) âˆˆRğ‘Ã—4Ã—â„Ã—ğ‘¤ Weperformtheself-attentioncloningoperationbeforesoftmaxto
andmaskğ‘€ âˆˆRğ‘Ã—1Ã—â„Ã—ğ‘¤ ,constructingtheinputoftheUNet.By preventtheoriginalself-attentionresultsbeingexcessivelyaffected.
establishingsucha2Ã—2-gridprompt,weencouragethemodelto
fillinthecontentofunknownarea(ğµâ€²)basedonthecontextual 4.2 TextualPrompting
regions(ğ´,ğ´â€²,andğµ)intheimage.
Cloningself-attentioneffectivelymanagesbasicin-contextvisual
guidance,yetthediffusionmodelâ€™scelebratedtext-to-imagefeature
4.1.2 Self-AttentionCloning. Thekeyofin-contextlearningisto
remainsunderutilizedtoprovidesemantic-levelguidance.Toad-
recognizetaskinstructionfromthegivendemonstration.Previous
dressthis,weutilizeGPT-4Vâ€™svisualreasoningabilities[Yangetal.
inference-basedworkextractthevisualinstructionsthroughcross-
2023a]toprovidesemanticguidancetotheinpaintingmodel.
attentioninjection,whichcouldonlyprovidescoarseandimprecise
guidance.Differently,weintroducefine-grainedstructural-aware 4.2.1 GPT-4VPrompting. WepromptGPT-4Vtogenerateacoher-
contextualinformationviaself-attention. enttextdescriptiontoaidtheinpaintingprocess.Consideringthe
OurmotivationcomesfromanobservationthattheDiffusion consistencyoftheentirepipeline,wefeedthewhole2ğ‘¥2grid-like
modelaccuratelyconstructsassociationsbetweendifferentpositions imagedirectlyintoGPT-4Vwithapre-designedproblemDescrip-
intheknownareasthroughself-attention.Weshowthevisualiza- tion,asdepictedinFigure2.Weemploytwocarefully-designed
tionofself-attentionrelationsinFigure3.Wecalculatetheattention graphicalinstructionstomakeiteasierforGPT-4Vtounderstand
valuesbetweenkeysemanticpositions(e.g.,theeyes,mouth,and thetask.Firstly,inspiredby[Yangetal.2023b],weplacealetter
flowerinthefirstrowandthespire,building,andthebackground mark(ğ´,ğ´â€²,ğµ,ğµâ€²)inthetop-leftcornerofeachgridcell.Secondly,
grasslandinthesecondrow)inğ´andallregionsinğµ.Theresults weaddprominentarrowmarkers(â†’)betweenğ´andğ´â€²,aswell
demonstratethatthevisualassociationsbetweenimagescanbe asbetweenğµandğµâ€²,toindicatetherelationshipbetweenthetwo
accuratelyidentifiedthroughself-attention,whichcouldbemore images.Theseapproachesintroducestructured,easilyidentifiable
accuratethanabstractsemantictextpromptsasguidance.Basedon referencepoints,facilitatingmoreeffectiveandaccurateresponses
thisobservation,weproposetouseself-attentionasastructural- toqueriesinvolvingvisualcontent.Then,GPT-4Visaskedtoper-
levelpriortoguidethein-contextgenerationprocedurebymodu- formananalogyandoutputthetextdescriptionforğµâ€².Finally,we
latingself-attentioninUNet.WeshowanexampleinFigure2of useGPT-4Vâ€™sanswerasthesemantic-levelpositivetextpromptto
translatingacatintoatiger.Therelativepositionalrelationship reinforcethemodelâ€™sICLcapabilities.Wealsoemploynegativetext
ofthetigerinğµâ€²andthetigerinğ´â€²shouldbeconsistentwiththe prompts(i.e.,â€œMessy,Disordered,Chaotic,Cluttered,Haphazard,
relativepositionalrelationshipofthetwocatsinğµandğ´. Unkempt,Scattered,Disheveled,Tangled,Randomâ€)topreventthe
Wepresentdetailedillustrationoftheproposedself-attention diffusionmodelfromgeneratingirregularandillogicalresults.These
cloning(SAC)inFigure4.Denotetheimagefeaturebeforeself- twopromptsworkcooperativelytoinjectsemantic-levelguidance
attentionasğ¹ ğ‘– âˆˆRâ„Ã—ğ‘¤Ã—ğ‘ .Theself-attentionmapMğ‘  âˆˆRâ„ğ‘¤Ã—â„ğ‘¤ intothemodel.
recordsthesimilarityofeachpositionontheentireimagewith
otherpositions,whichalsoincludesthesimilaritiesbetweenğ´and 4.2.2 Cross-AttentionMasking. Notethatthepromptobtainedfrom
ğµ,aswellasbetweenğ´â€²andğµâ€².Weextractthesubself-attention
GPT-4Visspecificallytailoredforğµâ€²,yetthetextualguidanceim-
mapMğ‘ (ğ´,ğµ) âˆˆ Râ„ 4ğ‘¤Ã—â„ 4ğ‘¤ andassignitsvaluetoMğ‘ (ğ´â€²,ğµâ€²) âˆˆ pactstheentireimagethroughcross-attentionintheUNet.Toad-
dress this issue, we propose cross-attention masking (CAM): in
Râ„ 4ğ‘¤Ã—â„ 4ğ‘¤
: cross-attentionlayers,werestrictthetextinteractsonlywiththe
Mğ‘ (ğ´â€²,ğµâ€²):=Mğ‘ (ğ´,ğµ)Â·ğ‘ , (4) regioncorrespondingtoğµâ€².Specifically,supposethecross-attention
xamtfoS6 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
ğ‘¨ ğ‘¨â€² ğ‘© GPT-4VPrompt MAEVQGAN PromptDiffusion DIA VISII Analogist
Black
maracas to
wooden
Red chair
detailed
painting
Lemonwith
black
background
Illuminated
closet interior
Woman with
drawn
mustache
Old man's
skull
Abstract
mountain
watercolor
painting
Person in
dress standing
Classroom
desk and
chair
Soccer ball in
forest
Fig.6. Comparisonwithotherbaselinemethods,eachrowindicatesonetask,giventheinputimagepairğ´,ğ´â€²andqueryimageğµ.SinceMAEVQGAN[Bar
etal.2022]doesnottaketextasinputandDIA[Å ubrtovÃ¡etal.2023]andVISII[Nguyenetal.2023]estimatethetextpromptsbyextraoptimization,thetext
promptsgeneratedbyGPT-4VpromptingareonlyusedbyPromptDiffusion[Wangetal.2023a]andAnalogist.Sourceimages:ImageNet[Dengetal.2009],
LOL[Chenetal.2018],InstructPix2Pix[Brooksetal.2023],UBC-Fashion[Zablotskaiaetal.2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016].
noitaziroloC
rulbeD
esioneD
tnemecnahnE
gnitidE
noitalsnarT
refsnarTelytS
egami-ot-notelekS
egami-ot-ksaM
gnitniapnIAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 7
ImageColorization Deblur
â€œDog in colorâ€ â€œColorized lakeside houseâ€ â€œEagle perched clearlyâ€ â€œMannequin wearing trench
Denoise ImageEditing coatâ€
â€œDog with longer coatâ€ â€œBlurryImageSharpenedâ€ â€œHippoinsunsetpaintingâ€ â€œSmallcabinbyriverâ€
ImageTranslation StyleTransfer
â€œMotorcycle silhouette at sunsetâ€ â€œMonochrome close-up catâ€ â€œMill sketch styleâ€ â€œCaricatured man smilingâ€
Mask-to-imageGeneration ImageInpainting
â€œRealistic room cornerâ€ â€œColored shapes to kitchenâ€ â€œDuckenteringwaterâ€ â€œJeepmovedforwardâ€
Fig.7. ExamplesofresultsgeneratedbytheproposedAnalogistondifferenttasks.Ineachexample,theimageğ´andğ´â€²areshowninthefirstcolumn,the
imageğµandgeneratedimageğµâ€²isshowninthesecondandthirdcolumn.ThetextpromptgeneratedviaGPT-4Visshownbeloweachexample.Source
ImageNet[Dengetal.2009],InstructPix2Pix[Brooksetal.2023],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016].
mapasMğ‘ âˆˆ Râ„ğ‘¤Ã—ğ¿ ,whereğ¿denotesthelengthoftextembed- missingareasspecifiedbyamask.TheUNetarchitecturecontains
ding.Werepurposetheindicesofdifferentregionsidentifiedinthe 16blocks,eachconsistsofonecross-attentionandoneself-attention.
previousSACprocessandsettheattentionvaluesbetweenthetext WeperformSACandCAMfromlayer3to10atalltimestepsinthe
andregionsotherthanğµâ€²(i.e.,ğ´,ğ´â€²,andğµ)tozero: diffusionprocess.Thescaleforclassifier-freeguidanceissetat15.
Mğ‘(ğ´):=0;Mğ‘(ğ´â€²):=0;Mğ‘(ğµ):=0. (5)
Thecoefficientforself-attentioncloningğ‘  =1.3inallexperiments
exceptforskeleton-to-imagewhereğ‘  = 1.4.Allexperimentsare
AsillustratedinFigure5,weutilizetheattentionmappost-softmax, conductedonanRTX3090GPU.
aswearecompletelyobstructingtherelationshipbetweenthetext
andregionsoutsideofğµâ€².
5.2 EvaluationSetup
AsfortheattentionmapindexinginSACandCAM,duetothe
Dataset. Weemploythefollowingthreemajorcategories,totaling
fixedpositionsofeachimage,weareabletopre-calculatethein-
tentaskstoevaluatetheeffectivenessoftheproposedmethodquan-
dicesrequiredforextractingthenecessarysub-attentionmaps(e.g.,
Mğ‘ (ğ´,ğµ) and Mğ‘(ğ´)) from the entire attention map. This pre- titatively:low-leveltasks,manipulationtasks,andmorechallenging
visiontasks.
determinationstreamlinestheentirepipeline,enhancingitssim-
plicityandefficiency. â€¢ Low-level tasks. We test out method on four low-level
tasks,i.e.,imagecolorization,imagedeblurring,imagede-
5 EXPERIMENTS
noising,andimageenhancement.Forthefirstthreetasks,we
5.1 ImplementationDetails samplein-the-wildimagesfromImageNet[Dengetal.2009]
and apply corresponding transformations (i.e., grayscale,
WeimplementourworkinPyTorch[Paszkeetal.2019].Theinput
imagesğ´,ğ´â€²,ğµareresizedto256Ã—256andspatiallycombinedto gaussianblurry,addingnoise).Forimageenhancement,we
usetheLOLdataset[Chenetal.2018],whichconsistsof
forma512Ã—512grid-likeimage.WeusedapubliclyavailableStable
Diffusioninpaintingmodel1.ThemodelisinitializedwithSD1.2 low/normal-lightimagepairs.Wecollect100samplesfor
eachlow-leveltask.
andtrainedoninpaintingtask,thereforecapableofinpaintingthe
â€¢ Manipulationtasks.Weselectthreekindofimagema-
1https://huggingface.co/runwayml/stable-diffusion-inpainting nipulationtasks(i.e.,imageediting,imagetranslation,and8 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
ğ‘¨ ğ‘¨â€² ğ‘© ImageBrush Analogist nothavepairedtextdescriptions,weinputthesametextprompts
asoursthatobtainedfromGPT-4VintoPromptDiffusion[Wang
etal.2023a]toensureafaircomparison.
EvaluationMetrics. Weevaluatethemodelâ€™sICLcapacityviathe
CLIPdirectionsimilaritybetweenthedemonstrationandthepro-
ducedresults.WeutilizetheImageEncoderfromCLIPtoextractthe
imagefeaturesofğ´,ğ´â€²,ğµ,andthegeneratedğµâ€².Then,wecalculate
thecosinesimilaritybetweenthedirectionalchangesfromğ´toğ´â€²
andfromğµtoğµâ€².Thehigherthesimilarity,themoreconsistentthe
inferredğµâ€²iswiththetransformationeffectsappliedtoğ´.Dueto
thegenerationdiversityofdiffusionmodels,wedonotcompare
pixel-levelmetricslikeSSIMandPSNR.Instead,wecalculateFID
betweenthegeneratedğµâ€²imagesandthegroundtruthimages.In
ordertoobtainmoreaccurateresult,wemergeallthedataineach
majorcategorytocalculatetheFIDvaluesforcomparison.
5.3 QualitativeResults
Figure6presentscomparisonofourmethodwiththebaselineson
allofthetentasks.ForMAEVQGAN[Baretal.2022],duetothelack
ofspecificstructuringoftrainingdataintotheformoftasksand
theabsenceoftextualguidance,thequalityofthegeneratedoutput
isrelativelypoor,especiallyforhigh-leveltaskslikemanipulation.
ForPromptDiffusion[Wangetal.2023a],thebiasintrainingtask
(i.e.,image-to-HED,HED-to-image)significantlyimpactstheICL
Fig.8. ComparisonwithImageBrush[SUNetal.2023].TheresultofIm-
generalizabilityofthemodel.Asshownintheexampleofdeblurand
ageBrushinthefirstthreetasksarefromtheoriginalpaperandtheresult
translation,theresultstendtoproducelinedrawingssimilarwith
ofthelastthreetasksareprovidedbytheauthorsofImageBrush.Source
images:InstructPix2Pix[Brooksetal.2023],UBC-Fashion[Zablotskaiaetal. edgedetectionresults.Fortheothertwoinference-basedmethods
2019],ScanNet[Daietal.2017],DAVIS[Perazzietal.2016]. DIA[Å ubrtovÃ¡etal.2023]andVISII[Nguyenetal.2023],theycon-
ductin-contextlearningthroughtheestimatedtextsolely,making
itdifficultprovidesufficientlyaccuratepromptinformationtogen-
styletransfer)fromtheCLIP-filteredsubsetprocessedby eratethecorrectresults.Ourmethodtakesintoaccountguidanceat
InstructPix2Pix[Brooksetal.2023].Sincethedatasetiscon- boththevisualandsemanticlevels,whichcanproduceaccurateand
structedforgeneralimageediting,wesplitthesamplesinto reasonablein-contextoutputs.NoticethatGPT-4Vpromptingmay
threetasksbasedonthekeywords.Instructionscontaining strugglewithvisiontasks,givingcoarsedescriptions.Forexample,
â€œaddâ€,â€œremoveâ€areconsideredasimageeditingtasks,those â€œpersonindressstandingâ€intheskeleton-to-imageexampledoes
withâ€œmake,turn,changeâ€areimagetranslationtasks.Each notgivethedetaileddescriptionthatwhatposethewomanshould
manipulationtaskcontains200samples. bestandingin.However,thankstotheproposedSACoperation,
â€¢ Visiontasks.Weselectthreemorechallengingvisiontasks thesestructure-awarein-contextinformationcanbestillcaptured
forevaluation:skeleton-to-imagegenerationfromUBC-Fas- andutilizedtoproducethecorrectresults.Figure7showsfurther
hion[Zablotskaiaetal.2019],mask-to-imagegeneration resultsofAnalogistonthesetasks,demonstratingtheICLcapabil-
fromScanNet[Daietal.2017],andimageinpaintingfrom itiesofourproposedmethod.Morerandomlyselectedresultsare
DAVISdataset[Perazzietal.2016].Eachtaskcontains200 showninsupplementarymaterials.
samples. Additionally,weconductedacomparisonwithImageBrush[SUN
etal.2023].SinceImageBrushhasnotreleasedthecode,thecompar-
Bydevelopingthesethreemajorcategories,wecanevaluateifthe
isonismadeintherangeoftrainingtasksofImageBrush.Asshown
pretrainedmodeliscapableofunderstanding,processing,andutiliz-
inFigure8,itisworthnotingthatourmethodismoreeffectiveat
ingvisualinformationacrossvariouslevels,whilealsoevaluating
preservingthedetailsinImageğµ.Especiallyinmanipulationtasks,
itsabilitytogeneralizeeffectivelyacrossthesetasks.
thecoloroftheaurora,thecontourstructureoftheanimals,and
Baselinemethods. Wetakefourmethods,MAEVQGAN[Baretal. thetextureontheclothingarebetterpreserved.Thisisbecauseour
2022],PromptDiffusion[Wangetal.2023a],DIA[Å ubrtovÃ¡etal. proposedvisualandtextualpromptingcontainmoredetailedin-
2023]andVISII[Nguyenetal.2023]asourbaseline.Allbaseline contextinformation.Onthethreevisiontasks,weachievecompeti-
methodsutilizetheofficialimplementationsandcheckpointspro- tiveresultswithImageBrush.Notethatourmodelisnotfine-tuned
vided.SincePromptDiffusion[Wangetal.2023a]requirestextas specificallyforthesetasks,whichdemonstrateoursuperiorityof
partofitsinput,butmostofthetestdatasets(suchaslow-level)do in-contextgeneralizabilityasaninference-basedmethod.
gnitidE
noitalsnarT
refsnarTelytS
egami-ot-notelekS
egami-ot-ksaM
gnitniapnI
revo
aroruAâ€œ
dliw
detaminAâ€œ
eutats
eznorBâ€œ
ni
namoWâ€œ
gniteem
lanigirOâ€œ
derotseRâ€œ
â€sniatnuom
ywons
â€sdinac
â€tceffe
â€sserd
tnagele
â€enecs
moor
â€egami
ognimalfAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 9
Table1. QuantitativecomparisonondifferentcategoryoftaskswithpreviousICLapproaches.WereportthecosinesimilaritybetweenCLIPdirectionfromğ´
toğ´â€²andfromğµtoğµâ€².Highersimilarityrepresentsmorecontextuallyappropriategeneratedresults.Thebestresultsarehighlighted.
Category Task MAEVQGAN PromptDiffusion DIA VISII Analogist
Colorization 0.0558 0.1283 0.0066 0.1061 0.1797
Deblur -0.0961 0.0251 -0.1337 0.0081 0.0608
Lowleveltasks
Denoise -0.0389 0.1612 0.1212 0.1098 0.2391
Enhancement 0.1120 0.1551 -0.1443 0.2181 0.2251
ImageEditing 0.1600 0.1768 0.0922 0.2181 0.1800
Manipulationtasks ImageTranslation 0.2526 0.2426 0.1617 0.2965 0.3136
StyleTransfer 0.2274 0.2336 0.1515 0.2687 0.2455
Skeleton-to-image 0.4452 0.6150 0.2874 0.5201 0.7334
Visiontasks Mask-to-image 0.4467 0.3984 0.1590 0.3071 0.5531
Inpainting -0.0357 0.0014 -0.0511 0.0619 0.1013
Average 0.1529 0.2137 0.0650 0.2104 0.2832
Table2. ComparisonofFIDbetweenthegeneratedğµâ€²sandtheground-
isbecauseVISIIleveragesanInstructPix2Pix[Brooksetal.2023]
truthimages.Thebestresultsarehighlighted.Ourmethodoutperforms
modelwhichispretrainedonthesamedataset,makingitmore
previousmethodsintermsofallthethreetaskcategories.
familiarwithgeneratingdataofsimilarquality.
Method Low-level Manipulation Vision UserStudy. Weconductauserstudytoevaluatetheperceptual
performanceofourmethod.Theuserstudyconsistedof50ques-
MAEVQGAN 181.48 143.19 169.74
tions,with42participantsinvolved,containingallofthe10kind
PromptDiffusion 180.39 111.79 159.02
oftasks.Ineachquestion,first,wepresentedtheparticipantswith
DIA 173.10 103.39 191.51 imagesğ´andğ´â€²,askingthemtoanalyzethechangesbetweenthem.
VISII 140.39 88.36 138.44 Then,weprovidedimageğµandtaskedthemwithpredictingthe
Analogist 114.15 85.67 96.67 expectedtransformationofğµ followingthesamepattern.Subse-
quently,wedisplayedtheoutputsgeneratedbydifferentmethods
Table3. Userstudyresults.Ineachtask,wereporttheaveragepercentageof forthistask,andtheparticipantswererequiredtoselecttheone
selectedresultbytheusers.Thebestresultsarehighlighted.Ourapproach theydeemedmostconsistentwiththeidentifiedpatternandofthe
garneredthehighestnumberofselections.
highestgenerativequality.Wereporttheaverageselectionresult
forthethreemajortasks:low-leveltasks,manipulationtasks,and
Method Low-level Manipulation Vision visiontasksinTable3.Ourproposedmethodexhibitedthehighest
rateofbeingchosenamongallofthethreetasks.
MAEVQGAN 3.51% 3.45% 0.87%
PromptDiffusion 5.33% 14.99% 9.09% 5.5 AblationStudy
DIA 4.88% 3.32% 0.43%
VISII 20.18% 18.30% 15.58% Effectivenessofproposedcomponents. Toevaluatetheeffective-
Analogist 66.10% 59.95% 74.03% nessoftheproposedcomponents,weconductaseriesofablation
studies.TheablationresultsarepresentedinFigure9.(a)Thebase-
line model of pretrained inpainting model generates rough and
5.4 QuantitativeComparisons low-qualityresults.(b)Bypastingğµtothebottomrightcornerof
thegridimage,theoutputsaremorestructurallyconsistentwith
CLIPDirection. WecomputethefollowingCLIPdirectionsimilar-
ity,ğ‘ğ‘œğ‘ [(E(ğµâ€²)âˆ’E(ğµ)),(E(ğ´â€²)âˆ’E(ğ´))],toevaluatehowfaithfully
ğµ.(c)Addingnegativepromptshelpstostabilizethegeneration
processandavoidmessyresults.(d-1)Crucially,whenoperating
thetransformationsprovidedbythemodeladheretothetransfor-
self-attentioncloningbyMğ‘ (ğµ,ğµâ€²):=Mğ‘ (ğ´,ğ´â€²),themodelretains
mationscontainedinthegivenexamples.Theresultsareshownin
theinformationfromğµ,butisunabletoextractaccuratecontext
inTable1.NotethatVISII[Nguyenetal.2023]achievesacceptable
fromğ´â€² toinferthesametransformationresult.(d-2)Whenex-
resultsinmanipulationtaskssincethemodelitutilizesispretrained
ecuting SAC by Mğ‘ (ğ´â€²,ğµâ€²) := Mğ‘ (ğ´,ğµ), the model is required
onthisip2pdataset[Brooksetal.2023].Overall,ourmethoddemon-
tokeepthestructuralrelationbetweenğ´andğµ consistent,after
stratessuperiorICLcapabilitiesacrossallthesetasks.
theyhavebeentransformedintoğ´â€²andğµâ€².Thus,weuse(d-2)in-
FrÃ©chetinceptiondistance(FID). WecalculateFIDbetweengen- steadof(d-1).(e)WhenaddingtextualpromptsfromGPT-4Vinthe
eratedimagesandgroundtruthontheentiremajorcategory.The wholegridimage,themodelrarelyfocusesthetextguidanceonthe
resultsareshowninTable2.TheproposedAnalogistoutperforms targetinpaintingareağµâ€².(f)Finally,withtheproposedCAM,our
allbaselinesacrossthethreemajortasks.NoticethatVISII[Nguyen fullapproachnotonlymaintainedrespectablegenerationquality
etal.2023]outperformsotherbaselinesonmanipulationtasks.This butalsosuccessfullyidentifiedthenecessaryvisualediting(adding10 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
Cat with sunglasses
Cubist art transformation
Cartoonish mosque illustration
Input (a) (b) (c) (d-1) (d-2) (e) (f)
Fig.9. Ablationontheproposedcomponents.Aninput2Ã—2imagegridisinpaintedby:(a)pretrainedSDInpaintingmodelwithrandomnoiseasinput,
(b)initializingğµâ€² asnoisedğµ,(c)addingnegativeprompt,(d-1)addingself-attentioncloning(SAC)byMğ‘ (ğµ,ğµâ€²) := Mğ‘ (ğ´,ğ´â€²),(d-2)addingSACby
Mğ‘ (ğ´â€²,ğµâ€²):=Mğ‘ (ğ´,ğµ),(e)addingGPT-4Vpromptingwithoutcross-attentionmasking(CAM),and(f)addingCAM(thefullapproach).Sourceimages:The
1ğ‘ ğ‘¡ rowaregeneratedbyDALLE-3[Betkeretal.2023]andallothersarefromInstructPix2Pix[Brooksetal.2023].
Withoutgraphicalinstructions
Pleasehelpmewiththeimageanalogytask:takean
imageAand its transformation Aâ€™, and provideany
imageBtoproduceanoutputBâ€™thatisanalogoustoAâ€™. ğ‘ =0.5 ğ‘ =1.0 ğ’”=ğŸ.ğŸ‘ ğ‘ =1.5 ğ‘ =1.8
Or,moresuccinctly:A:Aâ€™::B:Bâ€™.Youshouldgiveme
thetextpromptofimageBâ€™withnomorethan5words.
Toproceedwiththeimageanalogytask,lneedtounderstandthetransformation
f trr ao nm sfoim rmag ae tioA nt ho asim bea eg ne aA p. pC lieo dul fd roy mou imp al gea es Aec tola ir mify ago er Ad ?es Tc hr ii sbe ww illh ha et lpki mnd eo tof Input Only Only BothE&D BothE&D BothE&D
Encoder Decoder Deep Mid Shallow
createananalogoustransformationforimageBtogenerateimageBâ€™.
Addinggraphicalinstructions Fig.11. Ablationonhyper-parameters.Inthefirstrow,lowercoefficientğ‘ 
producesresultsmorelikeğµ,whilehigherğ‘ transfersmorefeatureofğ´â€².In
ğ‘¨ ğ‘¨â€² Pleasehelpmewiththeimageanalogytask:takean
imageAand its transformation Aâ€™, and provideany
thesecondrow,performingSACandCAMatmiddlelayers(16Ã—16)ofthe
imageBtoproduceanoutputBâ€™thatisanalogoustoAâ€™. UNetachievesbalancebetweenstructurepreservingandtransformation
ğ‘© ğ‘©â€² Or,moresuccinctly:A:Aâ€™::B:Bâ€™.Youshouldgiveme applying.Sourceimages:InstructPix2Pix[Brooksetal.2023].
thetextpromptofimageBâ€™withnomorethan5words.
TocreateananalogoustransformationfromBtoBâ€˜,similartoAtoAâ€™,we
observethatAisawintryforestscene,andAâ€˜addsanEastAsianpagodawith
redaccentsandaslightlywarmerambiance,whilemaintainingthesnowytheme.
GiventhatBisalsoasnowyforestscene,tomakeananalogoustransformationto GPT-4VPrompting. Weablateonthedesignedgraphicalinstruc-
Bâ€™,wewouldintroduceanelementthatisculturallysignificantorarchitecturally tionsthatusedtohintGPT-4VinFigure10.Withoutaddingthe
distinctwhileadjustingtheambianceinasimilarway.
ThetextpromptforBâ€˜wouldbe:â€œPagodainsnowyforestâ€ visualmarksonthegridimage,GPT-4Vmaynotknowthecor-
respondingrelationshipofthegivenimages,thereforeisunable
Fig.10. AblationonthegraphicalinstructionsinGPT-4Vprompting.By tocorrectlyanalyzethecontentaccordingtotheinstructions.By
addingmarksandarrows,theidentityandrelationofthetaskbecomes explicitlymarkingthepositionsofimages(ğ´,ğ´â€²,ğµ,andğµâ€²)onthe
moreobvious,makingiteasierforGPT-4Vtoproducepropertextprompt. constructedgridimage,GPT-4Vconvenientlyunderstandstheinfor-
Sourceimages:InstructPix2Pix[Brooksetal.2023].
mationcontainedinthepictures.Meanwhile,theintroducedarrows
fromğ´toğ´â€²andğµtoğµâ€²successfullydemonstratethetransforma-
tionrelations,makingitmoreacceptableforGPT-4Vtoproduce
theidealresponseofaddingaâ€œpagodainthesnowyforestâ€.This
sunglasses),effects(applyingacubiststyle),andtransformations textpromptwillintroducesemanticcontextualinformationforthe
(changingchurchintomosque)fortheICLtask. pretrainedmodeltounderstandthetask.NotethatourmethodisAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 11
PhototoCaricature SketchtoPortrait
ğ‘¨ ğ‘¨â€² ğ‘¨ ğ‘¨â€²
ğ‘¨ ğ‘¨ğŸ" ğ‘¨ğŸ" ğ‘¨ğŸ‘" ğ‘¨ğŸ’"
ğ‘© ğ‘©
ğ‘© ğ‘©ğŸ" ğ‘©ğŸ" ğ‘©ğŸ‘" ğ‘©ğŸ’"
Fig.12. Giventhesameimageğ´andğµinthefirstcolumn,anddifferent â€œSmilingwomanexaggeratedfeaturesâ€ â€œColor realistic woman's portraitâ€
ğ´â€²s,ourmethodisabletorecognizethecontextualrelationbetweenğ´and NormaltoRGB IcontoImage
ğ´â€²andproducetheoutputğµâ€²imagesaccordingly.Sourceimage:ğ´and ğ‘¨ ğ‘¨â€² ğ‘¨ ğ‘¨â€²
ğµarefromImageBrush[SUNetal.2023].{ğ´â€²,ğ´â€²,ğ´â€²,ğ´â€²}aregenerated
1 2 3 4
usingMasaCtrl[Caoetal.2023].
Table4. ComparisonofinferencetimetakentoperformoneICLtaskfor
differentmethods.Comparedtoexistingmethods,ourmethoddoesnot ğ‘© ğ‘©
requiretrainingonaspecifictaskandadditionaloptimization.
Method Inferencetime
MAEVQGAN[Baretal.2022] 0.4s
â€œRGBshiftfacialimageâ€ â€œDetailedcrabrenderingâ€
PromptDiffusion[Wangetal.2023a] 4s
DIA[Å ubrtovÃ¡etal.2023] 258s
Fig.13. Examplesofapplicationfortaskswhereğ´andğ´â€²arealigned.The
VISII[Nguyenetal.2023] 685s textpromptsgeneratedbyGPT-4Visshownbeloweachexample.utput
Analogist(ours) 4s imagesarehighlighted.Sourceimage:Photo-to-caricatureimagesarefrom
CariMe[Guetal.2021].Sketch-to-portraitimagesarefromDeepFace-
Drawing[Chenetal.2020].Normal-to-RGBimagesarefromTrevithicket
al.[2024].IconimagesarefromIconShop[Wuetal.2023].
genericandsupportsothervision-languagemodels[Zhuetal.2023]
aswell.
Hyper-parameters. Wepresentablationontheparametersensitiv-
ityofourproposedmethodinFigure11.AsfortheSACcoefficientğ‘ , experimentasshowninFigure12.Giventhesameimageğ´asan
utilizingasmallerğ‘ value(ğ‘  =0.5)resultsinanoutputmoreclosely imageofwolves,wefirsttranslateğ´intodifferentexampleoutputs
resemblingtheoriginalImageğµ,whereasalargervalue(ğ‘  =1.3) (cid:8)ğ´â€²,ğ´â€²,ğ´â€²,ğ´â€²(cid:9)
usingMasaCtrl[Caoetal.2023],obtainingdifferent
tendstoimbuetheresultwithcharacteristicsofğ´â€².However,ex- ani1 mal2 slik3 eli4
on,tiger,dog,andpanda.WeconstructdifferentICL
cessivelylargecoefficients(ğ‘  =1.8)leadstoanoverlyunbalanced tasks,keepingtheimageğ´andğµ beingthesame,whilevarying
attention map, which in turn reduces the quality of generation. theimageğ´â€²s.Ourmethodisabletorecognizethetranslationfrom
WealsoablatetheselectionofUNetlayersinwhichweperform ğ´toğ´â€²accordinglyandgeneratethecorrespondinganimalsinğµâ€²,
SACandCAM.Theresultsindicatethatitisnecessarytoperform demonstratingtheICLcapacityofourAnalogist.
operationssimultaneouslyinboththeencoderandthedecoder.Fur-
thermore,iftheoperationsareperformedatashallowlevel(high InferenceRuntime. Inthissection,wecomparetheexecutiontime
resolution),theoutcomeismerelyasimplereplicationofsomecol- fordifferentICLmethodsperformedonce.Ourexperimentiscon-
orsandcoarsetextures,leadingtopoorquality.Iftheoperations ductedonanRTX3090GPU,andwecalculatedthetimetakento
areperformedatadeeperlevel(lowresolution),theexcessivecom- generateoneimage.TheresultisshowninTab4.MAEVQGAN[Bar
pressionofinformationleadstothegeneratedresultbeingsimilar etal.2022]istheleasttime-consuming,taking0.4seconds,since
totheoriginalimageğµ.Inourexperiments,weperformSACand
itisgeneratingveryfewtokenswithouttheneedofiterativelyde-
CAMatamiddleleveloftheUNetlayers. noising.OurmethodAnalogisttakesabout4second,thesameas
PromptDiffusion[Wangetal.2023a],whichistypicallythestandard
5.6 Analysis
samplingtimeforDiffusionmodels,butdoesnotrequirespecific
DifferentIn-contextexamples. Amodelwithcontextualreasoning fine-tuning.Asforthepreviousinference-baesdmethodsDIA[Å ubr-
abilitiesshouldbeabletoproducedifferentresultsbasedondifferent tovÃ¡etal.2023]andVISII[Nguyenetal.2023],ittakesratherlong
in-contextexamples,whengiventhesameinput.Toverifythat time(i.e.,258secondsand685seconds)forthesetwomethodsto
ourapproachhassuchcapabilities,weconductedthefollowing estimatetheCLIPfeatureandeditinginstructionrespectively.12 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
ShapeChange Resize
ğ‘¨â€²
ğ‘©
MAEVQGAN Analogist MAEVQGAN Analogist
Fig.14. Illustrationofthepipelinefortasksinwhichğ´isalignedwith
Shape&ColorChange NumberExtrapolation
ğµ insteadofğ´â€².Weswapthepositionsofğ´â€² andğµ inthegridimage.
Throughthisway,wesimplifytheproblemintoalignedtasks.Sourceimages:
generatedbyDALLE-3[Betkeretal.2023].
MotionTransfer ObjectMultiplication
ğ‘¨ ğ‘¨â€² ğ‘¨ ğ‘¨â€²
MAEVQGAN Analogist MAEVQGAN Analogist
LetterExtrapolation Letter&StyleExtrapolation
ğ‘© ğ‘©
MAEVQGAN Analogist MAEVQGAN Analogist
Fig.16. Examplesofapplicationfortaskswhereğ´,ğ´â€²andğµareallmis-
â€œhigh contrast animecharacterâ€ â€œgoldbricksstackedâ€
aligned.WetestourmethodwithoutSAC,onlyCAMisapplied.Output
MotionEditing&StyleTransfer ObjectMultiplication&Editing
imagesarehighlighted.Sourceimages:MAEVQGAN[Baretal.2022].
ğ‘¨ ğ‘¨â€² ğ‘¨ ğ‘¨â€²
forphoto-to-caricatureandicon-to-image.However,ourmethodis
stillrobusttotheseminorissuessinceweareprovidingin-context
informationfrombothstructuralandsemanticlevels.
ğ‘© ğ‘©
6.2 ğ´andğµarealigned
Wemakeitpossibletoaddresstaskswhereğ´ isalignedwithğµ
insteadofğ´â€².WegiveanexampleofobjectmultiplicationinFig-
ure14,whereğ´containsonebrickandğ´â€²containsabrickstack.
â€œstylized corgi in grassâ€ â€œwatermelon jack-o'-lanternâ€
Thisproblemcannotbedonethroughouroriginalpipeline.To
tacklethisproblem,weswapthepositionsofğ´â€²andğµinthegrid
Fig.15. Examplesofapplicationfortaskswhereğ´andğµarealigned.The
textpromptsofGPT-4Vareshownbeloweachexample.Outputimages
image,constructinganewgridimagewhereğ´â€²containsonebrick
arehighlighted.Sourceimages:Theexampleimagesofthefirstmotion
andğµcontainsastackofbricks.Inthisway,wesimplifythetask
transfercasearefromChangetal.[2023].Theotherthreeexampleimages
intoonewhereğ´andğ´â€²arealignedagain,i.e.,changingthetask
aregeneratedbyDALLE-3[Betkeretal.2023]. ofturningonebrickintobrickstackintothetaskofchangingbricks
intogoldenbricks.Thisstrategycanbeappliedtotaskslikemo-
tiontransferandimageanalogywhereğ´andğ´â€²aremisalignedin
6 APPLICATION
figure15.Wealsodemonstrateourmethodâ€™sabilityofaddressing
Inthissection,weextendAnalogisttothreecategoriesofapplica- taskswithmultipletranslationslikebothmotioneditingandstyle
tions:(a)ğ´andğ´â€²arealigned,(b)ğ´andğµarealigned,and(c)ğ´, transfer,andobjectmultiplicationwithediting.
ğ´â€²,andğµareallmisaligned.For(b)and(c),wemakeadjustments
6.3 ğ´,ğ´â€²,andğµareallmisaligned
toourmethodaccordingly.
We extend our method on tasks whereğ´,ğ´â€², andğµ are all mis-
6.1 ğ´andğ´â€² arealigned
alignedinFigure16,suchaschangingacircletoasquare,resizing
Undertheconditionthatğ´andğ´â€²arealigned,weshowexample abigcircletoasmallerone,extrapolatingnewcontentofnumbers
of applications in Figure 13, e.g., photo-to-caricature, sketch-to- andletters.WetestourmethodwithoutSACtopreventincorrect
portrait,normal-to-RGB,andicon-to-imagetasks.Theresultsshow structureguidance.Analogistproducesreasonableresultsandout-
thatourmethodisabletogeneratereasonableresultsonthesetasks. performsMAEVQGAN.Itshouldbepointedoutthatthequalityof
Noticethatthereareslightstructuralchangesbetweenğ´andğ´â€² longsequencelettergenerationstillhaveroomtoimproveduetoAnalogist:Out-of-the-boxVisualIn-ContextLearningwithImageDiffusionModel â€¢ 13
Secondly,themodelstruggleswithproducingdatathatitseldom
Addingapolar bear Sketchofelephant
seesduringthetrainingstage.AsshowninFigure17(b),whenasked
toproduceunnaturalimageslikenormalmapandline-drawing
icons,themodelfailstogenerateaccurateresultssincemostofits
trainingdataarenaturalRGBimages.Ontheotherhand,itexplains
ourmethodâ€™smediocreperformanceonvisiontaskscomparedto
ImageBrush[SUNetal.2023].Webelievethiscouldpotentiallybe
:â€œMountainlakeattwilightâ€ :â€œLionsketch in savannaâ€ achievedbydemandingamorepowerfulpretrainedbasemodel.
Finally,theproposedself-attentioncloningmaystrugglewith
(a)ExampleofinaccuratepromptbyGPT-4V.Theexpectedrightpromptisshownabove scenarioinwhichğ´,ğ´â€²,andğµareallmisalignedasshowninFig-
theimagewiththecriticalwordsmarkedgreen.ThepromptgivenbyGPT-4Visshown
belowwiththewrongwordsinred. ure17(c).Thestructural-levelinformationisnotapplicableinthis
case.Onepossiblesolutionistorelyonsemantic-levelinformation
toproducethetransformationasdiscussedinSection6.3.
8 CONCLUSION
Addressingthelimitationsofinaccurateinstructionandtedious
optimizationofexistinginference-basedmethods,weintroduced
â€œThermal image filterâ€ â€œOutlined glasses iconâ€ Analogist,anovelapproachforvisualIn-ContextLearning(ICL)
combining visual and textual prompting. The proposed method
(b)Failureexamplesofgeneratingunnaturalimagesonwhichthemodelisrarelyseen
duringthepretrainingstage,forexample,normalmapsandabstracticons. utilizesatext-to-imagediffusionmodelpretrainedforimagein-
painting,makingitanout-of-the-boxsolutionforawiderangeof
visualtasks.WeinnovatewithSelf-AttentionCloning(SAC)forvi-
sualprompting,enablingfine-grainedstructural-levelanalogy,and
leverageGPT-4Vâ€™svisualreasoningforefficienttextualprompting,
supplementedbyCross-AttentionMasking(CAM)forenhanced
semantic-levelanalogyaccuracy.Ourapproach,withouttheneed
forextratrainingoroptimization,demonstratessuperiorperfor-
â€œTwoOrangesâ€,Analogist â€œTwoOrangesâ€,AnalogistwithoutSAC
manceinbothqualitativeandquantitativemeasures,showcasing
(c)Exampleofğ´,ğ´â€²,andğµareallmisaligned,whereSACisnotapplicable. robustICLcapabilities.
ACKNOWLEDGMENTS
Fig.17. Exampleoffailurecases.(a)GPT-4Vfailstoaccuratelydeducethe
correcttextualpromptfromthegivengridimageswhenthetransformation
ThisworkwassupportedinpartbytheNationalNaturalScience
(addingapolarbear)orcategory(elephant,insteadoflion)isambiguous.
FoundationofChinaunderGrant62276128,Grant62192783inpart
(b)Themodelfailstogenerateunnaturalimageslikenormalmapsoricons
bytheCollaborativeInnovationCenterofNovelSoftwareTech-
eventhoughgiventherighttextprompt.(c)TheproposedSACstruggles
nologyandIndustrialization,andaGRFgrantfromtheResearch
withtaskswhereğ´,ğ´â€²,andğµareallmisaligned.Sourceimage:Trevithick
etal.[2024],IconShop[Wuetal.2023],andDALLE-3[Betkeretal.2023]. GrantsCouncil(RGC)oftheHongKongSpecialAdministrative
Region,China[ProjectNo.CityU11216122].
REFERENCES
notorioustendencyofdiffusionmodelstostrugglewithgenerating YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,
high-quality text. Nevertheless, we believe these results demon- JitendraMalik,andAlexeiAEfros.2023. SequentialModelingEnablesScalable
LearningforLargeVisionModels.arXivpreprintarXiv:2312.00785(2023).
stratethepre-trainedgenerativemodelshaveamplepotentialof
AmirBar,YossiGandelsman,TrevorDarrell,AmirGloberson,andAlexeiEfros.2022.
in-contextabilitytobefurthertapped. Visualpromptingviaimageinpainting.AdvancesinNeuralInformationProcessing
Systems35(2022),25005â€“25017.
7 LIMITATION AhmetCanberkBaykal,AbdulBasitAnees,DuyguCeylan,ErkutErdem,AykutErdem,
andDenizYuret.2023.CLIP-guidedStyleGANInversionforText-drivenRealImage
Editing.ACMTransactionsonGraphics42,5(2023),1â€“18.
Althoughourapproachenhancesin-contextlearningabilities,itâ€™s
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,
importanttoconsidertwopossiblelimitations.Firstly,theinpainting JuntangZhuang,JoyceLee,YufeiGuo,etal.2023. Improvingimagegeneration
modelmightbemisledbyincorrecttextdescriptions.InFigure17(a), withbettercaptions.ComputerScience.https://cdn.openai.com/papers/dall-e-3.pdf
whenthetransformationfromğ´toğ´â€²isminor(i.e.,theaddedobject 2(2023),3.
TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023.Instructpix2pix:Learning
inthefirstcaseissmallandeasilyoverlooked),GPT-4Vfailsto tofollowimageeditinginstructions.InProceedingsoftheIEEE/CVFConferenceon
recognizeit.Thesecondcaseshowsanstyletransfertaskofdrawing ComputerVisionandPatternRecognition.18392â€“18402.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,Prafulla
â€œasketchofelephantâ€.However,GPT-4Vrecognizestheobjectas Dhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.
alioninsteadofanelephant,leadingtoinaccurateguidance.The 2020. Languagemodelsarefew-shotlearners. AdvancesinNeuralInformation
ProcessingSystems33(2020),1877â€“1901.
potentialsolutioncouldbeleavinganinterfaceforuserstomonitor
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang
andcustomizethetextpromptsinrealtime. Zheng.2023. Masactrl:Tuning-freemutualself-attentioncontrolforconsistent14 â€¢ ZhengGu,ShiyuanYang,JingLiao,JingHuo,andYangGao
imagesynthesisandediting.InProceedingsoftheIEEE/CVFInternationalConference theIEEE/CVFConferenceonComputerVisionandPatternRecognition.10684â€“10695.
onComputerVision.22560â€“22570. AdÃ©laÅ ubrtovÃ¡,MichalLukÃ¡Ä,JanÄŒech,DavidFutschik,EliShechtman,andDaniel
DiChang,YichunShi,QuankaiGao,JessicaFu,HongyiXu,GuoxianSong,QingYan, Sy`kora.2023. DiffusionImageAnalogies.InACMSIGGRAPH2023Conference
XiaoYang,andMohammadSoleymani.2023.MagicDance:RealisticHumanDance Proceedings.1â€“10.
VideoGenerationwithMotions&FacialExpressionsTransfer. arXivpreprint YashengSUN,YifanYang,HouwenPeng,YifeiShen,YuqingYang,HanHu,LiliQiu,
arXiv:2311.12052(2023). andHidekiKoike.2023. ImageBrush:LearningVisualIn-ContextInstructions
Shu-YuChen,WanchaoSu,LinGao,ShihongXia,andHongboFu.2020.DeepFace- forExemplar-BasedImageManipulation.InThirty-seventhConferenceonNeural
Drawing:Deepgenerationoffaceimagesfromsketches. ACMTransactionson InformationProcessingSystems. https://openreview.net/forum?id=EmOIP3t9nk
Graphics39,4(2020),72â€“1. AlexTrevithick,MatthewChan,TowakiTakikawa,UmarIqbal,ShaliniDeMello,
WeiChen,WangWenjing,YangWenhan,andLiuJiaying.2018.DeepRetinexDecom- ManmohanChandraker,RaviRamamoorthi,andKokiNagano.2024.WhatYouSee
positionforLow-LightEnhancement.InBritishMachineVisionConference.British isWhatYouGAN:RenderingEveryPixelforHigh-FidelityGeometryin3DGANs.
MachineVisionAssociation. arXivpreprintarXiv:2401.02411(2024).
AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,ThomasFunkhouser,and XinlongWang,WenWang,YueCao,ChunhuaShen,andTiejunHuang.2023b.Images
MatthiasNieÃŸner.2017. Scannet:Richly-annotated3dreconstructionsofindoor speakinimages:Ageneralistpainterforin-contextvisuallearning.InProceedings
scenes.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog- oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.6830â€“6839.
nition.5828â€“5839. XinlongWang,XiaosongZhang,YueCao,WenWang,ChunhuaShen,andTiejun
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet:A Huang.2023c.SegGPT:TowardsSegmentingEverythinginContext.InProceedings
large-scalehierarchicalimagedatabase.In2009IEEEConferenceonComputerVision oftheIEEE/CVFInternationalConferenceonComputerVision.1130â€“1140.
andPatternRecognition.Ieee,248â€“255. ZhendongWang,YifanJiang,YadongLu,yelongshen,PengchengHe,WeizhuChen,
OlgaDiamanti,ConnellyBarnes,SylvainParis,EliShechtman,andOlgaSorkine- ZhangyangWang,andMingyuanZhou.2023a.In-ContextLearningUnlockedfor
Hornung.2015.SynthesisofComplexImageAppearancefromLimitedExemplars. DiffusionModels.InThirty-seventhConferenceonNeuralInformationProcessing
ACMTransactionsonGraphics(Mar2015),1â€“14. https://doi.org/10.1145/2699641 Systems. https://openreview.net/forum?id=6BZS2EAkns
QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun, JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocV
JingjingXu,andZhifangSui.2022.Asurveyforin-contextlearning.arXivpreprint Le,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoninginlarge
arXiv:2301.00234(2022). languagemodels. AdvancesinNeuralInformationProcessingSystems35(2022),
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xiaohua 24824â€“24837.
Zhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold, RonghuanWu,WanchaoSu,KedeMa,andJingLiao.2023. IconShop:Text-Guided
SylvainGelly,etal.2020.Animageisworth16x16words:Transformersforimage VectorIconSynthesiswithAutoregressiveTransformers. ACMTransactionson
recognitionatscale.arXivpreprintarXiv:2010.11929(2020). Graphics42,6(2023),1â€“14.
ZhengGu,ChuanqiDong,JingHuo,WenbinLi,andYangGao.2021.CariMe:Unpaired WeihaoXia,YulunZhang,YujiuYang,Jing-HaoXue,BoleiZhou,andMing-Hsuan
caricaturegenerationwithmultipleexaggerations.IEEETransactionsonMultimedia Yang.2022. Ganinversion:Asurvey. IEEETransactionsonPatternAnalysisand
24(2021),2673â€“2686. MachineIntelligence45,3(2022),3121â€“3138.
JiabangHe,LeiWang,YiHu,NingLiu,HuiLiu,XingXu,andHengTaoShen.2023.ICL- CanwenXu,YichongXu,ShuohangWang,YangLiu,ChenguangZhu,andJulian
D3IE:In-ContextLearningwithDiverseDemonstrationsUpdatingforDocument McAuley.2023.Smallmodelsarevaluableplug-insforlargelanguagemodels.arXiv
InformationExtraction.InProceedingsoftheIEEE/CVFInternationalConferenceon preprintarXiv:2305.08848(2023).
ComputerVision.19485â€“19494. JianweiYang,HaoZhang,FengLi,XueyanZou,ChunyuanLi,andJianfengGao.2023b.
AaronHertzmann,CharlesE.Jacobs,NuriaOliver,BrianCurless,andDavidH.Salesin. Set-of-markpromptingunleashesextraordinaryvisualgroundingingpt-4v.arXiv
2001. Imageanalogies.InProceedingsofthe28thannualconferenceonComputer preprintarXiv:2310.11441(2023).
graphicsandinteractivetechniques. https://doi.org/10.1145/383259.383295 ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,
JonathanHo,AjayJain,andPieterAbbeel.2020. Denoisingdiffusionprobabilistic andLijuanWang.2023a.Thedawnoflmms:Preliminaryexplorationswithgpt-4v
models.AdvancesinNeuralInformationProcessingSystems33(2020),6840â€“6851. (ision).arXivpreprintarXiv:2309.174219,1(2023).
OndÅ™ejJamriÅ¡ka,Å Ã¡rkaSochorovÃ¡,OndÅ™ejTexler,MichalLukÃ¡Ä,JakubFiÅ¡er,Jingwan LiangYuan,DingkunYan,SuguruSaito,andIsseiFujishiro.2024. DiffMat:Latent
Lu,EliShechtman,andDanielSÃ½kora.2019. Stylizingvideobyexample. ACM diffusionmodelsforimage-guidedmaterialgeneration.VisualInformatics(2024).
TransactionsonGraphics(Aug2019),1â€“11. https://doi.org/10.1145/3306346.3323006 PolinaZablotskaia,AliaksandrSiarohin,BoZhao,andLeonidSigal.2019. Dwnet:
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022. Blip:Bootstrapping Densewarp-basednetworkforpose-guidedhumanvideogeneration.arXivpreprint
language-imagepre-trainingforunifiedvision-languageunderstandingandgenera- arXiv:1910.09139(2019).
tion.InInternationalConferenceonMachineLearning.PMLR,12888â€“12900. LvminZhang,AnyiRao,andManeeshAgrawala.2023. Addingconditionalcontrol
JingLiao,YuanYao,LuYuan,GangHua,andSingBingKang.2017.Visualatribute totext-to-imagediffusionmodels.InProceedingsoftheIEEE/CVFInternational
transferthroughdeepimageanalogy.ACMTransactionsonGraphics36,4(2017), ConferenceonComputerVision.3836â€“3847.
120. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.
IvonaNajdenkoska,AnimeshSinha,AbhimanyuDubey,DhruvMahajan,Vignesh MiniGPT-4:EnhancingVision-LanguageUnderstandingwithAdvancedLargeLan-
Ramanathan,andFilipRadenovic.2023.ContextDiffusion:In-ContextAwareImage guageModels.InTheTwelfthInternationalConferenceonLearningRepresentations.
Generation.arXivpreprintarXiv:2312.03584(2023).
ThaoNguyen,YuhengLi,UtkarshOjha,andYongJaeLee.2023.VisualInstructionIn-
version:ImageEditingviaImagePrompting.InThirty-seventhConferenceonNeural
InformationProcessingSystems. https://openreview.net/forum?id=l9BsCh8ikK
GauravParmar,KrishnaKumarSingh,RichardZhang,YijunLi,JingwanLu,andJun-
YanZhu.2023. Zero-shotimage-to-imagetranslation.InACMSIGGRAPH2023
ConferenceProceedings.1â€“11.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advancesin
NeuralInformationProcessingSystems32(2019).
OrPatashnik,ZongzeWu,EliShechtman,DanielCohen-Or,andDaniLischinski.
2021.Styleclip:Text-drivenmanipulationofstyleganimagery.InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision.2085â€“2094.
FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,LucVanGool,MarkusGross,and
AlexanderSorkine-Hornung.2016.Abenchmarkdatasetandevaluationmethodol-
ogyforvideoobjectsegmentation.InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition.724â€“732.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021.
Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInterna-
tionalConferenceonMachineLearning.PMLR,8748â€“8763.
RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.InProceedingsof