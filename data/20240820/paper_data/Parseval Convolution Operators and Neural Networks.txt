Parseval Convolution Operators
and Neural Networks
MichaelUnser[0000âˆ’0003âˆ’1248âˆ’2513] and
StanislasDucotterd[0009âˆ’0006âˆ’2047âˆ’5179]
Abstract We first establish a kernel theorem that characterizes all linear shift-
invariant (LSI) operators acting on discrete multicomponent signals. This result
naturally leads to the identification of the Parseval convolution operators as the
classofenergy-preservingfilterbanks.Wethenpresentaconstructiveapproachfor
thedesign/specificationofsuchfilterbanksviathechainingofelementaryParseval
modules, each of which being parameterized by an orthogonal matrix or a 1-tight
frame.OuranalysisiscomplementedwithexplicitformulasfortheLipschitzconstant
of all the components of a convolutional neural network (CNN), which gives us a
handle on their stability. Finally, we demonstrate the usage of those tools with the
design of a CNN-based algorithm for the iterative reconstruction of biomedical
images. Our algorithm falls within the plug-and-play framework for the resolution
ofinverseproblems.Ityieldsbetter-qualityresultsthanthesparsity-basedmethods
used in compressed sensing, while offering essentially the same convergence and
robustnessguarantees.
1 Introduction
The goal of this chapter is twofold. The first objective is to characterize a special
typeofconvolutionaloperatorsthatarerobustandinherentlystablebecauseoftheir
Parseval property. The second objective is to showcase the use of these operators
inthedesignofthrustworthyneural-network-basedalgorithmsforsignalandimage
processing. Our approach is deductive, in that it relies on the higher-level tools of
functional analysis to identify the relevant operators based on their fundamental
MichaelUnser
EPFL,Lausanne,Switzerland,e-mail:michael.unser@epfl.ch
StanislasDucotterd
EPFL,Lausanne,Switzerland,e-mail:stanislas.Ducotterd@epfl.ch
1
4202
guA
91
]PS.ssee[
1v18990.8042:viXra2 MichaelUnserandStanislasDucotterd
properties; namely, linearity, shift-invariance (LSI), and energy conservation (Par-
seval).
ThestudyofLSIoperators(a.k.a.filters)reliesheavilyontheFouriertransform
and is a central topic in linear-systems theory and signal processing [25, 39, 54].
Hence, the first step of our investigation is to extend the classic framework to
accommodate the kind of processing performed in convolutional neural networks
(CNNs), where the convolutional layers have multichanel inputs and outputs. We
do so by adopting an operator-based formalism with appropriate Hilbert spaces,
whichthenalsomakesthedescriptionofCNNsmathematicallyprecise.Asonemay
expect,thecorrespondingLSIoperatorsarecharacterizedbytheirimpulseresponse
or,equivalently,bytheirfrequencyresponse,theextensiontotheclassicsettingof
signalprocessingbeingthattheseentitiesnowbothhappentobematrix-valued(see
Theorem2).
OurfocusonParsevaloperatorsismotivatedbythedesiretocontrolthestabilityof
thecomponentsofCNNs,whichcanbequantifiedmathematicallybytheirLipschitz
constant (see Section 2.3). Indeed, it is known that the stability of conventional
deep neural networks degrades (almost) exponentially with their depth [59]. This
lack of stability partly explains why CNNs can occasionally hallucinate, which is
unacceptableforcriticalapplicationssuchas,forinstance,diagnosticimaging.Our
proposedremedyistoconstraintheLipschitzconstantofeachlayer,withtheâ€œultra-
stableâ€ configuration being the one where each component is non-expansive; i.e.,
withaLipschitzconstantnogreaterthan1.Parsevaloperatorsareexemplarinthis
respectsincetheypreserveenergy,which,ineffect,turnsthe(worst-case)Lipschitz
boundintoanequality.Additionalfeaturesthatmotivatetheirusageareasfollows:
1. Parsevalconvolutionoperatorshavearemarkablysimpletheoreticaldescription,
whichisgiveninProposition3;
2. theyadmitconvenientparametricrepresentations(seeSection4)thataredirectly
amenabletoanoptimizationinstandardcomputationalframeworksformachine
learning,suchasPyTorch.
However,onemustacknowledgethatthereisnofreelunch.Anyattempttostabilize
aneuralnetworkbyconstrainingtheLipschitzconstantofeachlayerwillnecessarily
reduceitsexpressivity,asdocumentedin[23,18].Thegoodnewsisthatthiseffect
islesspronouncedwhenthelinearlayershavetheParsevalproperty,asconfirmedin
ourexperiments(Parsevalvs.spectralnormalization).Infact,weshalldemonstrate
that the use of Parseval CNNs (as substitute for the classic proximity operator of
convex optimization) results in a substantial improvement in the quality of image
reconstruction over that of the traditional sparsity-based methods of compressed
sensing,whileitoffersessentiallythesametheoreticalguarantees(consistencyand
stability).ParsevalConvolutionOperatorsandNeuralNetworks 3
1.1 RelatedWorksandConcepts
Conceptually, Parseval operators are the infinite-dimensional generalization of or-
thogonalmatrices(one-to-onescenario)and,moregenerally,of1-tightframes(one-
to-many scenario) [10]. The latter involve rectangular matrices A âˆˆ Rğ‘€Ã—ğ‘ with
ğ‘€ â‰¥ ğ‘ andthepropertythatATA = Iğ‘ (identity).WhentheoperatorisLSI,then
itisdiagonalizedbytheFouriertransformâ€”apropertythatcanbeexploitedforthe
designofParsevalfilterbanks.
Thespecificationoffiltersfortheorthogonalwavelettransform[16,33,36]isa
specialinstanceoftheone-to-onescenario.Infact,thereisacomprehensivetheory
for the design of perfect-reconstruction filterbanks [42, 53], the orthogonal ones
beingsometimesreferredtoaslosslesssystems[51].Itincludesgeneralfactoriza-
tion results for paraunitary matrices associated with finite impulse response (FIR)
filterbanksofagivenMcMillandegree[51,Theorem14.4.1,p.736]orofagiven
size[48],withthecaveatthattheseonlyholdintheone-dimensionalsetting.There
arealsoadaptationsofthoseresultsforlinear-phasefilters[41,45,47].
The one-to-many scenario (wavelet frames) caught the interest of researchers
in the late â€™90s, motivated by an early application in texture analysis [49] that
involved a computational architecture that is a â€œhandcraftedâ€ form of CNN. Such
redundant wavelet designs are less constrained than the orthogonal ones. They go
under the name of oversampled filterbanks [15], oversampled wavelet transforms
[6],undecimatedwavelettransform[32],lappedtransforms[9],or,moregenerally,
tight(wavelet)frames[1,11,27,28,29].
TheuseofParsevaloperatorsinthecontextofneuralnetworksismorerecent.The
newtwistbroughtforthbymachinelearningisthatthefilterscannowbelearnedto
providethebestperformanceforagivencomputationaltask,whichisfeasibleunder
theavailabilityofsufficienttrainingdataandcomputationalpower.Thefirstattempts
toorthogonalizethelinearlayersofaneuralnetworkweremotivatedbythedesire
to avoid vanishing gradients and to improve robustness against adversarial attacks
[2,13,20,56].Severalresearchteams[22,30,43]thenproposedsolutionsforthe
training of orthogonal convolution layers that are inspired by the one-dimensional
factorizationtheoremsuncoveredduringthedevelopmentofthewavelettransform.
TherearealsoapproachesthatoperatedirectlyintheFourierdomain[46].
1.2 RoadMap
Thischapterisorganizedasfollows.
WestartwithapresentationofbackgroundmaterialinSection2.First,wesetthe
notation and introduce the Hilbert spaces for the representation of ğ‘‘-dimensional
vector-valued signals, such as â„“ğ‘(Zğ‘‘), whose elements are ğ‘-component discrete
2
signals or images. We then move on to the discrete Fourier transform in Section
2.2. This is complemented with a discussion of fundamental continuity/stability4 MichaelUnserandStanislasDucotterd
properties of operators in the general context of Banach/Hilbert spaces in Section
2.3.
InSection3,wefocusonthediscreteLSIsettingandidentifythecompletefamily
ofcontinuousLSIoperatorsT : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘€(Zğ‘‘) (Theorem2),includingthe
LSI 2 2
determination of their Lipschitz constant. These operators are multichannel filters
withğ‘-channelinputsand ğ‘€-channeloutputs.Theyareuniquelyspecifiedbytheir
matrix-valued impulse response or, equivalently, by their matrix-valued frequency
response.AnimportantsubclassaretheLSI-Parsevaloperators;theseareidentified
inProposition3asconvolutionoperatorswithaparaunitaryfrequencyresponse.
InSection4,wedevelopaconstructiveapproachforthedesign/specificationof
Parseval filterbanks. The leading idea is to generate higher-complexity filterbanks
throughthechainingofelementaryParsevalmodules,eachbeingparameterizedby
aunitarymatrixor,eventually,a1-tightframe(seeTable1).
Section 5 is devoted to the application of our framework to the problem of
biomedical image reconstruction. Our approach revolves around the design of a
robust1-LipschitzCNNforimagedenoisingthatmimicsthearchitectureofDnCNN
[58]â€”a very popular image denoiser. The important twist is that, unlike DnCNN,
theconvolutionlayersofournetworkareconstrainedtobeParseval,whichmakes
our denoiser compatible with the powerful plug-and-play (PnP) paradigm for the
resolutionoflinearinverseproblems[8,26,44,52].Wefirstprovidemathematical
supportforthisprocedureintheformofconvergenceguaranteesandstabilitybounds.
WethendemonstratethefeasibilityoftheapproachforMRIreconstructionandreport
experimentalresultswhereitsignificantlyoutperformsthestandardtechnique(total-
variation-regularizedreconstruction)usedincompressedsensing.
2 MathematicalBackground
2.1 Notation
Weuseboldfaceloweranduppercaseletterstodenotevectorsandmatrices,respec-
tively(e.g.,u âˆˆ Rğ‘ andU âˆˆ Cğ‘€Ã—ğ‘ ).Specificinstancesareeğ‘› (theğ‘›thelementof
thecanonicalbasisinRğ‘ )andIğ‘ = [e 1...eğ‘] (theunitmatrixofsizeğ‘).
Adiscretemultidimensionalscalarsignal(e.g.,theinputoroutputofaconvolu-
tionalneuralnetwork)isasequence(ğ‘¥[ğ’Œ]) ğ’ŒâˆˆZğ‘‘ ofrealnumbersthat,dependingon
thecontext,willbedenotedasğ‘¥ âˆˆ â„“ (Zğ‘‘) (i.e.,asamemberofaHilbertspace),or
2
ğ‘¥[Â·],whereâ€œÂ·â€isaplaceholderfortheindexingvariable.Ouruseofsquarebrackets
follows the convention of signal processing, as reminder of the discrete nature of
theobjects.Avector-valuedsignalx[Â·] = (ğ‘¥ 1[Â·],...,ğ‘¥ ğ‘[Â·])isanindexedsequence
of vectors x[ğ’Œ] = (ğ‘¥ 1[ğ’Œ],...,ğ‘¥ ğ‘[ğ’Œ]) âˆˆ Rğ‘ with ğ’Œ ranging over Zğ‘‘ . Likewise,
(cid:2) (cid:3)
X[Â·] = x 1[Â·] Â·Â·Â· xğ‘€[Â·] is a matrix-valued signal or sequence. An alternative
representationofsuchsequencesisParsevalConvolutionOperatorsandNeuralNetworks 5
âˆ‘ï¸
X[Â·] = X[ğ’]ğ›¿[Â·âˆ’ğ’] (1)
ğ’âˆˆZğ‘‘
whereğ›¿[Â·âˆ’ğ’] denotesthe(scalar)Kroneckerimpulseshiftedby ğ’,withğ›¿[0] = 1
andğ›¿[ğ’Œ] =0for ğ’Œ âˆˆZğ‘‘\{0}.
In the same spirit, we use the notation ğ‘“(Â·), f(Â·), F(Â·) to designate objects that
arerespectivelyscalar,vector-valued,andmatrix-valuedfunctionsofacontinuously
varyingindexsuchasğ’™ âˆˆRğ‘‘ orğ âˆˆTğ‘‘ = [âˆ’ğœ‹,+ğœ‹]ğ‘‘ (thefrequencyvariable).
Thesymbolâˆ¨ denotestheflippingoperatorwithxâˆ¨[ğ’Œ] =â–³ x[âˆ’ğ’Œ] forall ğ’Œ âˆˆ Zğ‘‘ ,
while UH âˆˆ Cğ‘Ã—ğ‘€ is the Hermitian transpose of the complex matrix U âˆˆ Cğ‘€Ã—ğ‘
â–³
with [UH]ğ‘›,ğ‘š = [U]ğ‘š,ğ‘› (transposewithcomplexconjugation).
OurprimaryHilbertspaceofvector-valuedsignalsisâ„“ğ‘(Zğ‘‘) =â„“ ({1,...,ğ‘}Ã—
2 2
Zğ‘‘) =â„“ (Zğ‘‘)Ã—Â·Â·Â·Ã—â„“ (Zğ‘‘) (cid:0)ğ‘ occurrencesofâ„“ (Zğ‘‘)(cid:1) ,whichisthedirect-product
2 2 2
extensionofâ„“ (Zğ‘‘).Specifically,
2
(cid:110) (cid:111)
â„“ 2ğ‘(Zğ‘‘) = x[Â·] :Zğ‘‘ â†’Rğ‘ s.t.âˆ¥x[Â·]âˆ¥ â„“ğ‘(Zğ‘‘) <âˆ
2
with
(cid:32) ğ‘ (cid:33)1/2
â–³ âˆ‘ï¸ âˆ‘ï¸
âˆ¥x[Â·]âˆ¥ â„“ğ‘(Zğ‘‘) = |ğ‘¥ ğ‘›[ğ’Œ]|2 . (2)
2
ğ‘›=1ğ’ŒâˆˆZğ‘‘
Byinvokingadensityargument,wecaninterchangetheorderofsummationin(2)
sothat
âˆ¥x[Â·]âˆ¥ â„“ 2ğ‘(Zğ‘‘) =(cid:13) (cid:13)(âˆ¥ğ‘¥ 1âˆ¥ â„“ 2(Zğ‘‘),...,âˆ¥ğ‘¥ ğ‘âˆ¥ â„“ 2(Zğ‘‘))(cid:13) (cid:13) 2 =(cid:13) (cid:13)âˆ¥x[Â·]âˆ¥ 2(cid:13) (cid:13) â„“ 2(Zğ‘‘),
where
(cid:32) ğ‘ (cid:33)1/2
â–³ âˆ‘ï¸
âˆ¥uâˆ¥
2
= |ğ‘¢ ğ‘›|2
ğ‘›=1
istheconventionalEuclideannormofthevectoru= (ğ‘¢ 1,...,ğ‘¢ ğ‘) âˆˆCğ‘ .
2.2 TheDiscreteFourierTransformandPlancherelâ€™sIsomorphism
ThediscreteFouriertransformofasignalğ‘¥[Â·] âˆˆâ„“ (Zğ‘‘) âŠ‚ â„“ (Zğ‘‘)isdefinedas
1 2
ğ‘¥Ë†(ğ) =â–³ F {ğ‘¥[Â·]}(ğ) = âˆ‘ï¸ ğ‘¥[ğ’Œ]eâˆ’jâŸ¨ğ,ğ’ŒâŸ©, ğ âˆˆRğ‘‘. (3)
d
ğ’ŒâˆˆZğ‘‘
The function ğ‘¥Ë† : Rğ‘‘ â†’ C is continuous, bounded and 2ğœ‹-periodic. It is therefore
entirely specified by its main period Tğ‘‘ = [âˆ’ğœ‹,ğœ‹]ğ‘‘ . The original signal can be
recoveredbyinverseFouriertransformationas6 MichaelUnserandStanislasDucotterd
âˆ« dğ
ğ‘¥[ğ’Œ] = Fâˆ’1{ğ‘Ë†}[ğ’Œ] = ğ‘¥Ë†(ğ)ejâŸ¨ğ,ğ’ŒâŸ© , ğ’Œ âˆˆZğ‘‘. (4)
d
Tğ‘‘
(2ğœ‹)ğ‘‘
Byinterpretingtheinfinitesumin(3)asanappropriatelimit,onethenextendsthe
definitionoftheFouriertransformtoencompassallsquare-summablesignals.This
yieldstheextendedoperator F : â„“ (Zğ‘‘) â†’ ğ¿ (Tğ‘‘),where ğ¿ (Tğ‘‘) isthespaceof
d 2 2 2
measurable complex Hermitian-symmetric and square-integrable functions on
Tğ‘‘
.
ThelatterisaHilbertspaceequippedwiththeHermitianinnerproduct
â–³ âˆ« dğ
âŸ¨ğ‘¥Ë†,ğ‘¦Ë†âŸ© ğ¿ 2(Tğ‘‘) = Tğ‘‘ğ‘¥Ë†(ğ)ğ‘¦Ë†(ğ) (2ğœ‹)ğ‘‘. (5)
TheFouriertransform F : â„“ (Zğ‘‘) â†’ ğ¿ (Tğ‘‘) isabÄ³ectiveisometry(unitarymap
d 2 2
between two Hilbert spaces) with Fâˆ’1 : ğ¿ (Tğ‘‘) â†’ â„“ (Zğ‘‘), where the inverse
d 2 2
transform is still specified by (4) with an extended Lebesgue interpretation of the
integral.Indeed,byinvokingtheCauchy-Schwartzinequality,wegetthat
âˆ« Tğ‘‘(cid:12)
(cid:12)
(cid:12)ğ‘¥Ë†(ğ)ejâŸ¨ğ,ğ’ŒâŸ©(cid:12)
(cid:12)
(cid:12)
(2d ğœ‹ğ
)ğ‘‘
=âˆ«
Tğ‘‘
|ğ‘¥Ë†(ğ)|Ã—1
(2d ğœ‹ğ
)ğ‘‘
â‰¤ âˆ¥ğ‘¥Ë†âˆ¥ğ¿
2
(cid:18)âˆ«
Tğ‘‘1
(2d ğœ‹ğ )ğ‘‘(cid:19)1 2
= âˆ¥ğ‘¥Ë†âˆ¥ğ¿ 2,
whichensuresthewell-posednessedof(4)forallğ‘¥Ë† âˆˆ ğ¿ (Tğ‘‘).
2
Thecornerstoneoftheâ„“ theoryoftheFouriertransformisthePlancherel-Parseval
2
identity
âˆ€ğ‘¥,ğ‘¦ âˆˆâ„“ 2(Zğ‘‘) : âŸ¨ğ‘¥,ğ‘¦âŸ© â„“ (Zğ‘‘) =â–³ âˆ‘ï¸ ğ‘¥[ğ’Œ]ğ‘¦[ğ’Œ] = âŸ¨ğ‘¥Ë†,ğ‘¦Ë†âŸ© ğ¿ (Tğ‘‘). (6)
2 2
ğ’ŒâˆˆZğ‘‘
ItensuresthattheinnerproductispreservedintheFourierdomain.
The vector-valued extension of these relations is immediate if one defines the
Fouriertransformofavector-valuedsignalx[Â·] âˆˆâ„“ğ‘(Zğ‘‘)as
2
âˆ€ğ âˆˆRğ‘‘ : F d{x[Â·]}(ğ) = F
dï£±ï£´ï£´ï£´ï£²ï£®
ï£¯ ï£¯
ï£¯
ğ‘¥ 1
. .
.[Â·] ï£¹
ï£º ï£º
ï£ºï£¼ï£´ï£´ï£´ï£½
(ğ) =â–³
ï£®
ï£¯ ï£¯
ï£¯
ğ‘¥Ë† 1(
. .
.ğ) ï£¹
ï£º ï£º
ï£º
= (cid:98)x(ğ) (7)
ï£´ï£´ï£´ï£¯ ï£¯ğ‘¥ ğ‘[Â·]ï£º ï£ºï£´ï£´ï£´ ï£¯ ï£¯ğ‘¥Ë†ğ‘(ğ)ï£º
ï£º
ï£³ï£° ï£»ï£¾ ï£° ï£»
anditsinverseas
F dâˆ’1{ (cid:98)x} = F
dâˆ’1ï£±ï£´ï£´ï£´ï£²ï£®
ï£¯
ï£¯
ï£¯
ğ‘¥Ë†
.
.
.1 ï£¹
ï£º
ï£º
ï£ºï£¼ï£´ï£´ï£´ï£½
=
ï£®
ï£¯
ï£¯
ï£¯
F dâˆ’1
.
.
.{ğ‘¥Ë† 1} ï£¹
ï£º
ï£º
ï£º
=
ï£®
ï£¯
ï£¯
ï£¯
ğ‘¥ 1
.
.
.[Â·] ï£¹
ï£º
ï£º ï£º. (8)
ï£´ï£´ï£´ï£¯ ï£¯ğ‘¥Ë†ğ‘ï£º ï£ºï£´ï£´ï£´ ï£¯ ï£¯Fâˆ’1{ğ‘¥Ë†ğ‘}ï£º
ï£º
ï£¯ ï£¯ğ‘¥ ğ‘[Â·]ï£º
ï£º
ï£³ï£° ï£»ï£¾ ï£° d ï£» ï£° ï£»
Thecorrespondingvector-valuedversionofPlancherelâ€™sidentityreadsParsevalConvolutionOperatorsandNeuralNetworks 7
ğ‘
âˆ€x,yâˆˆâ„“ 2ğ‘(Zğ‘‘) : âŸ¨x,yâŸ© â„“ğ‘(Zğ‘‘) =â–³ âˆ‘ï¸ âˆ‘ï¸ ğ‘¥ ğ‘›[ğ’Œ]ğ‘¦ ğ‘›[ğ’Œ]
2
ğ‘›=1ğ’ŒâˆˆZğ‘‘
â–³ âˆ‘ï¸ğ‘ âˆ« dğ
= âŸ¨ (cid:98)x, (cid:98)yâŸ© ğ¿ 2ğ‘(Tğ‘‘) =
ğ‘›=1
Tğ‘‘ğ‘¥Ë†ğ‘›(ğ)ğ‘¦Ë†ğ‘›(ğ) (2ğœ‹)ğ‘‘.
(9)
ThePlancherel-FourierisomorphismisthenexpressedasF :â„“ğ‘(Zğ‘‘) â†’ ğ¿ğ‘(Tğ‘‘)
d 2 2
and Fâˆ’1 : ğ¿ğ‘(Tğ‘‘) â†’ â„“ğ‘(Zğ‘‘) where ğ¿ğ‘(Tğ‘‘) is the Hilbert space of complex-
d 2 2 2
valued Hermitian-symmetric functions associated with the inner product (9) for
(2ğœ‹)-periodicvector-valuedfunctions.
2.3 1-LipandParsevalOperators
The transformations that occur in a neural network can be described through the
actionofsomeoperatorsTthatmapanymemberğ‘¥ofavectorspaceX(forinstance,
a specific input of the network or of one of its layers) into some element ğ‘¦ =
T{ğ‘¥} of another vector space Yğ‘– (e.g., the output of the network or any of its
intermediate layers). These operators T: X â†’ Y can be linear (as in the case of
a convolution layer) or, more generally, nonlinear. A minimal requirement is that
the T be continuous, which is a mathematical precondition tied to the underlying
topologies.
Definition1 Consider the (possibly nonlinear) mapping T: X â†’ Y, where X =
(X,âˆ¥ Â· âˆ¥ ) andY = (Y,âˆ¥ Â· âˆ¥ ) aretwocompletenormedspaces(e.g.,Banachor
X Y
Hilbertspaces).Then,Tcanexhibitthefollowingformsofcontinuity.
1. Continuityatğ‘¥ âˆˆ X:Foranyğœ– > 0,thereexistssome ğœ‡ > 0suchthat,forany
0
ğ‘¥ âˆˆXwithâˆ¥ğ‘¥âˆ’ğ‘¥ âˆ¥ < ğœ‡,itholdsthatâˆ¥T{ğ‘¥}âˆ’T{ğ‘¥ }âˆ¥ < ğœ–.
0 X 0 Y
2. UniformcontinuityonX:Foranyğœ– > 0,thereexistssome ğœ‡ > 0suchthat,for
anyğ‘¥,ğ‘¥ âˆˆXwithâˆ¥ğ‘¥âˆ’ğ‘¥ âˆ¥ < ğœ‡,itholdsthatâˆ¥T{ğ‘¥}âˆ’T{ğ‘¥ }âˆ¥ < ğœ–.
0 0 X 0 Y
3. Lipschitzcontinuity:Thereexistssomeconstantğ¿ >0suchthat
âˆ€ğ‘¥,ğ‘¥ âˆˆX : âˆ¥T{ğ‘¥}âˆ’T{ğ‘¥ }âˆ¥ â‰¤ ğ¿âˆ¥ğ‘¥âˆ’ğ‘¥ âˆ¥ . (10)
0 0 Y 0 X
Thethirdform(Lipschitz)isobviouslyalsothestrongestwith3 â‡’ 2 â‡’ 1.The
smallestğ¿forwhich(10)holdsiscalledtheLipschitzconstantofTwith
âˆ¥T{ğ‘¥}âˆ’T{ğ‘¥ }âˆ¥
Lip(T) = sup 0 Y. (11)
âˆ¥ğ‘¥âˆ’ğ‘¥ âˆ¥
âˆ€ğ‘¥,ğ‘¥ 0âˆˆX, ğ‘¥â‰ ğ‘¥ 0 0 X
Definition2 AnoperatorT:X â†’Y issaidtobeof1-LiptypeifLip(T) =1.
The1-Lipoperatorsareofspecialinteresttousbecausetheyareinherentlystable:
asmallperturbationoftheirinputcanonlyinduceasmalldeviationoftheiroutput.8 MichaelUnserandStanislasDucotterd
Moreover, they can be chained at will without any degradation in overall stability
becauseLip(T â—¦T ) â‰¤ Lip(T )Lip(T ) =1.
2 1 2 1
For linear operators, the graded forms of continuity in Definition 1 can all be
related to one overarching simplifying concept: the boundedness of the operator.
The two key ideas there are: (i) a linear operator is (locally) continuous at any
ğ‘¥ âˆˆ X ifandonlyifitiscontinuousat0;and,(ii)itisuniformlycontinuousifand
0
onlyifitisbounded[12,Theorem2.9-2,p.84].Finally,thereisoneveryattractive
form of 1-Lip linear operators for which (10) holds as an equality, rather than a
â€œworst-caseâ€inequality.Tomakethisexplicit,wenowrecallsomebasicproperties
of linear operators acting on Hilbert spaces and identify the subclass of Parseval
operators,whicharenorm-aswellasinner-product(angle)preserving.
Definition3 LetXandYbetwoHilbertspaces.ThemostbasicHilbertianproper-
tiesofalinearoperatorT:X â†’Y areasfollows.
1. Boundedness(continuity):Thereexistsaconstantğµ <âˆsuchthat
âˆ€ğ‘¥ âˆˆX : âˆ¥T{ğ‘¥}âˆ¥ â‰¤ ğµâˆ¥ğ‘¥âˆ¥ , (12)
Y X
withthesmallestğµin(12)beingthenormoftheoperatordenotedbyâˆ¥Tâˆ¥.
2. Boundednessfrombelow(injectivity):Thereexistsaconstant0< ğ´suchthat
âˆ€ğ‘¥ âˆˆX : ğ´ âˆ¥ğ‘¥âˆ¥ â‰¤ âˆ¥T{ğ‘¥}âˆ¥ . (13)
X Y
3. Isometry:Tisnorm-preserving(orParseval),meaningthatboth(12)and(13)
holdwith ğ´= ğµ=1.
ToidentifythecriticalboundsinDefinition3,weobservethat,foranyğ‘¥ âˆˆX\{0},
âˆ¥T{ğ‘¥}âˆ¥ ğ‘¥
ğ´ â‰¤ Y = âˆ¥T{ğ‘§}âˆ¥ â‰¤ ğµ withğ‘§ = . (14)
âˆ¥ğ‘¥âˆ¥ Y âˆ¥ğ‘¥âˆ¥
X X
ThisholdsbyvirtueofthelinearityofTandthehomogeneitypropertyofthenorm.
Inparticular,thisallowsustospecifytheinducednormoftheoperatoras
âˆ¥Tâˆ¥ =â–³ sup
âˆ¥T{ğ‘¥}âˆ¥
Y = sup âˆ¥T{ğ‘§}âˆ¥ . (15)
âˆ¥ğ‘¥âˆ¥ Y
ğ‘¥âˆˆX\{0} X ğ‘§âˆˆX: âˆ¥ğ‘§âˆ¥X=1
Notethat(15)canbeobtainedbyrestricting(11)toğ‘¥ = 0,whichthenalsoyields
0
âˆ¥Tâˆ¥ =Lip(T) duetothelinearityofT.Theisometryproperty(Item3)isbyfarthe
mostconstraining,asitimpliesthetwootherswith âˆ¥Tâˆ¥ = 1.Asitturnsout,ithas
otherremarkableconsequences,whichyieldsomealternativecharacterization(s).
Proposition1(Properties of Parseval operators) Let X and Y be two Hilbert
spaces. Then, the linear operator T: X â†’ Y is a Parseval operator if any of the
followingequivalentconditionsholds.
1. Isometry
âˆ€ğ‘¥ âˆˆX : âˆ¥ğ‘¥âˆ¥ = âˆ¥T{ğ‘¥}âˆ¥ . (16)
X YParsevalConvolutionOperatorsandNeuralNetworks 9
2. Preservationofinnerproducts
âˆ€ğ‘¥ ,ğ‘¥ âˆˆX, âŸ¨T{ğ‘¥ },T{ğ‘¥ }âŸ© = âŸ¨ğ‘¥ ,ğ‘¥ âŸ© . (17)
1 2 1 2 Y 1 2 X
3. Pseudo-inversionviatheadjointsothatTâˆ—â—¦T=Id:X â†’Y â†’X,wherethe
HermitianadjointTâˆ— :Y â†’Xistheuniquelinearoperatorsuchthat
âˆ€(ğ‘¥,ğ‘¦) âˆˆXÃ—Y : âŸ¨T{ğ‘¥},ğ‘¦âŸ© = âŸ¨ğ‘¥,Tâˆ—{ğ‘¦}âŸ© . (18)
Y X
Proof.
(i)1â‡”2:Fromthebasicpropertiesof(real-valued)innerproductsandthelinearity
ofT,wehavethat
âˆ¥ğ‘¥ âˆ’ğ‘¥ âˆ¥2 = âŸ¨ğ‘¥ âˆ’ğ‘¥ ,ğ‘¥ âˆ’ğ‘¥ âŸ© = âˆ¥ğ‘¥ âˆ¥2 âˆ’2âŸ¨ğ‘¥ ,ğ‘¥ âŸ© +âˆ¥ğ‘¥ âˆ¥2
2 1 X 2 1 2 1 X 2 X 1 2 X 1 X
âˆ¥T{ğ‘¥ âˆ’ğ‘¥ }âˆ¥2 = âˆ¥T{ğ‘¥ }âˆ’T{ğ‘¥ }âˆ¥2 = âˆ¥T{ğ‘¥ }âˆ¥2 âˆ’2âŸ¨T{ğ‘¥ },T{ğ‘¥ }âŸ© +âˆ¥T{ğ‘¥ }âˆ¥2 .
2 1 Y 2 1 Y 2 Y 1 2 Y 1 Y
Byequatingthesetwoexpressions,wereadilydeducethat(16)implies(17).Like-
wise,intheextendedcomplexsetting,wefindthatRe(âŸ¨ğ‘¥ ,ğ‘¥ âŸ© ) =Re(âŸ¨T{ğ‘¥ },T{ğ‘¥ }âŸ© ),
1 2 X 1 2 Y
whichultimatelyalsoyields(17).Conversely,bysettingğ‘¥ =ğ‘¥ in(17),wedirectly
1 2
get(16).
(ii)2â‡”3:TheexistenceandunicityoftheadjointoperatorTâˆ—in(18)isastandard
resultinthetheoryoflinearoperatorsonHilbert/Banachspaces.Bysettingğ‘¥ =ğ‘¥ ,
1
ğ‘¦ =T{ğ‘¥ },andapplying(18),werewrite(17)as
2
âˆ€ğ‘¥ ,ğ‘¥ âˆˆX : âŸ¨T{ğ‘¥ },T{ğ‘¥ }âŸ© = âŸ¨Tâˆ—T{ğ‘¥ },ğ‘¥ âŸ© = âŸ¨ğ‘¥ ,ğ‘¥ âŸ© . (19)
1 2 1 2 Y 1 2 X 1 2 X
SincetheinnerproductseparatesallpointsintheHilbertspace(Hausdorffproperty),
the right-hand side of (19) is equivalent to Tâˆ—T{ğ‘¥ } = ğ‘¥ for all ğ‘¥ âˆˆ X, which
1 1 1
translatesintoTâˆ—T=Tâˆ—â—¦T=IdonX. âŠ“âŠ”
The classic example of a Parseval operator is the discrete Fourier transform
Fğ‘‘ : â„“ 2(Zğ‘‘) â†’ ğ¿ 2(Tğ‘‘) withtheHilbertiantopologyspecifiedinSection2.2.The
fundamentalpropertythereisthattheHilbertspacesX = â„“ (Zğ‘‘) andY = ğ¿ (Tğ‘‘)
2 2
areisomorphicwithF ğ‘‘âˆ’1 = F ğ‘‘âˆ—beingatrueinverseofFğ‘‘ (bÄ³ection),meaningthat,
inadditiontoItem3inProposition1,wealsohavethatFğ‘‘â—¦F ğ‘‘âˆ— =IdonY = ğ¿ 2(Tğ‘‘)
(right-inverseproperty).
By contrast, the Parseval convolution operators investigated in this paper will
typically not be invertible from the right, the reason being that the effective range
spaceY(cid:101)=T(X)isonlya(closed)subspaceofY.
An important observation is that, in addition to linearity and continuity, all the
operatorpropertiesinDefinition3areconservedthroughcomposition.
Proposition2 Let X, X , and X be three Hilbert spaces. If the linear operators
1 2
T : X â†’ X and T : X â†’ X are both bounded (resp, bounded below with
1 1 2 1 2
constants ğ´ ,ğ´ or of Parseval type), then the same holds true for the composed
1 210 MichaelUnserandStanislasDucotterd
operator T = T â—¦T : X â†’ X â†’ X with âˆ¥Tâˆ¥ â‰¤ âˆ¥T âˆ¥ âˆ¥T âˆ¥ (resp., with lower
2 1 1 2 1 2
bound ğ´= ğ´ ğ´ ).
1 2
Forinstance,ifT andT arebothboundedbelow,then,forallğ‘¥ âˆˆX,
1 2
âˆ¥T T {ğ‘¥}âˆ¥ â‰¥ ğ´ âˆ¥T {ğ‘¥}âˆ¥ â‰¥ ğ´ ğ´ âˆ¥ğ‘¥âˆ¥ (20)
2 1 X 2 2 1 X 1 2 1 X
withT {ğ‘¥} âˆˆX .
1 1
3 Vector-ValuedLSIOperatorsonâ„“ğ‘µ(Zğ’…)
2
Inthissection,weshallidentifyandcharacterizethespecialclassoflinearoperators
thatoperateondiscretevector-valuedsignalsandcommutewiththeshiftoperation.
Definition4 A discrete operator T is linear-shift-invariant (LSI) if it is linear
LSI
andif,foranydiscretevector-valuedsignalx[Â·] initsdomainandany ğ’Œ âˆˆZğ‘‘ ,
0
T {x[Â·âˆ’ğ’Œ ]} =T {x}[Â·âˆ’ğ’Œ ].
LSI 0 LSI 0
WeobservethattheLSIpropertyisconservedthroughlinearcombinationsand
composition.Moreover,weshallseethatallâ„“ -stableLSIoperatorsactingondiscrete
2
vector-valued signals can be identified as (multichannel) convolution operators, as
statedinTheorem2.
3.1 Refresher:ScalarConvolutionOperators
Tosetthecontext,wefirstpresentaclassicresultonthecharacterizationofscalar
LSI operators, together with a self-contained proof that will serve as model for
subsequentderivations.
Theorem1(KerneltheoremfordiscreteLSIoperatorsonâ„“ (Zğ‘‘))Foranygiven
2
â„ âˆˆâ„“ 2(Zğ‘‘),theoperatorTâ„ :ğ‘¥[Â·] â†¦â†’ (â„âˆ—ğ‘¥)[Â·] withğ‘¥[Â·] âˆˆâ„“ 2(Zğ‘‘)and
(â„âˆ—ğ‘¥)[ğ’Œ] =â–³ âŸ¨â„,ğ‘¥[ğ’Œâˆ’Â·]âŸ© â„“ (Zğ‘‘) = âˆ‘ï¸ â„[ğ’]ğ‘¥[ğ’Œâˆ’ğ’], ğ’Œ âˆˆZğ‘‘ (21)
2
ğ’âˆˆZğ‘‘
islinear-shift-invariant.Moreover,Tâ„ continuouslymapsâ„“ 2(Zğ‘‘) â†’ â„“ 2(Zğ‘‘) ifand
only if âˆ¥â„Ë†âˆ¥ğ¿
âˆ
= esssup ğâˆˆ[âˆ’ğœ‹,+ğœ‹]ğ‘‘(cid:12) (cid:12)â„Ë†(ğ)(cid:12) (cid:12) < âˆ. Conversely, for every continuous
LSI operator T : â„“ (Zğ‘‘) â†’ â„“ (Zğ‘‘), there is one and only one â„ âˆˆ â„“ (Zğ‘‘) with
LSI 2 2 2
âˆ¥â„Ë†||ğ¿
âˆ
= âˆ¥T LSIâˆ¥suchthatT
LSI
:ğ‘¥[Â·] â†¦â†’ (â„âˆ—ğ‘¥)[Â·]wheretheconvolutionisspecified
by(21).
Proof.ParsevalConvolutionOperatorsandNeuralNetworks 11
Direct part. The assumption (â„,ğ‘¥) âˆˆ â„“ (Zğ‘‘) Ã—â„“ (Zğ‘‘) ensures that (21) is well-
2 2
defined for any ğ’Œ âˆˆ Zğ‘‘ . The shift-invariance is then an obvious consequence of
Definition4,as
Tâ„{ğ‘¥}[ğ’Œâˆ’ğ’Œ 0] = âŸ¨â„,ğ‘¥[ğ’Œâˆ’ğ’Œ 0âˆ’Â·]âŸ©â„“
2
= âŸ¨â„,ğ‘¥[(ğ’Œâˆ’Â·)âˆ’ğ’Œ 0]âŸ©â„“
2
=Tâ„{ğ‘¥[Â·âˆ’ğ’Œ 0]}[ğ’Œ].
ByobservingthattheFouriertransformofğ‘¥[ğ’Œâˆ’Â·] âˆˆâ„“ (Zğ‘‘)isğ‘¥Ë†(ğ)ejâŸ¨ğ,ğ’ŒâŸ©,wethen
2
invokePlancherelâ€™sidentity(6)toshowthat
(â„âˆ—ğ‘¥)[ğ’Œ] = âŸ¨â„,ğ‘¥[ğ’Œâˆ’Â·]âŸ©â„“
2
=âˆ«
Tğ‘‘
â„Ë†(ğ)ğ‘¥Ë†(ğ)ejâŸ¨ğ,ğ’ŒâŸ© (2d ğœ‹ğ
)ğ‘‘
= F dâˆ’1(cid:8)â„Ë† Ã—ğ‘¥Ë†(cid:9) [ğ’Œ],
wheretheidentificationoftheinverseFourieroperatorislegitimatesincethebound-
ednessofâ„Ë† impliesthatâ„Ë†Ã—ğ‘¥Ë† âˆˆ ğ¿ (Tğ‘‘).Consequently,weareinthepositionwhere
2
wecaninvokeParsevalâ€™srelation
âˆ« dğ âˆ« dğ
âˆ¥â„âˆ—ğ‘¥âˆ¥2 = |â„Ë†(ğ)|2|ğ‘¥Ë†(ğ)|2 â‰¤ âˆ¥â„Ë†âˆ¥2 |ğ‘¥Ë†(ğ)|2 .
â„“ 2 Tğ‘‘ (2ğœ‹)ğ‘‘ ğ¿ âˆ Tğ‘‘ (2ğœ‹)ğ‘‘
Thisyieldsthestabilityboundâˆ¥â„âˆ—ğ‘¥âˆ¥â„“ â‰¤ âˆ¥â„Ë†âˆ¥ğ¿ âˆ¥ğ‘¥âˆ¥â„“ ,whichimpliesthecontinuity
of Tâ„ : â„“ 2(Zğ‘‘) â†’ â„“ 2(Zğ‘‘). To show th2 at the latâˆ ter bo2 und is sharp (â€œif and only ifâ€
part of the statement), we refer to the central, more technical part of the proof of
Theorem2.
â–³
Indirect Part. We define the linear functional â„ : ğ‘¥ â†¦â†’ âŸ¨â„,ğ‘¥âŸ© = T {ğ‘¥âˆ¨}[0]. The
LSI
continuity of T
LSI
: â„“ 2(Zğ‘‘) â†’ â„“ 2(Zğ‘‘) implies that T LSI{ğ‘¥ ğ‘–âˆ¨}[0] = âŸ¨â„,ğ‘¥ ğ‘–âŸ© â†’ 0
foranysequenceofsignals (ğ‘¥ ğ‘–)ğ‘–âˆˆN inâ„“ 2(Zğ‘‘) thatconvergesto0(or,equivalently,
ğ‘¥âˆ¨ â†’ 0).Thisensuresthatthefunctional â„ : ğ‘¥ â†¦â†’ âŸ¨â„,ğ‘¥âŸ© iscontinuousonâ„“ (Zğ‘‘),
ğ‘– 2
meaningthatâ„ âˆˆ (cid:0)â„“ 2(Zğ‘‘)(cid:1)â€² =â„“ 2(Zğ‘‘),whichallowsustowritethatâŸ¨â„,ğ‘¥âŸ© = âŸ¨â„,ğ‘¥âŸ©â„“ 2.
Wethenmakeuseoftheshift-invariancepropertytoshowthat
T LSI{ğ‘¥}[ğ’Œ] =T LSI{ğ‘¥[Â·+ğ’Œ]}(0) = âŸ¨â„,ğ‘¥[Â·+ğ’Œ]âˆ¨âŸ©â„“
2
= âŸ¨â„,ğ‘¥[ğ’Œâˆ’Â·]âŸ©â„“
2
forany ğ’Œ âˆˆZğ‘‘ ,fromwhichwealsodeducethatT {ğ›¿} = â„. âŠ“âŠ”
LSI
Theorem1tellsusthatanLSIoperatorT : â„“ (Zğ‘‘) â†’ â„“ (Zğ‘‘) canalwaysbe
LSI 2 2
implementedasadiscreteconvolutionwithitsimpulseresponse â„ = T {ğ›¿[Â·]}.It
LSI
also provides the Lipschitz constant of the operator, as Lip(T ) = âˆ¥â„Ë†âˆ¥ (supre-
LSI âˆ
mumofitsfrequencyresponse).(Werecallthat,foralinearoperator,theLipschitz
constantispreciselythenormoftheoperator.)Wealsonotethattheclassiccondi-
tionforstabilityfromlinear-systemstheory, â„ âˆˆ â„“ (Zğ‘‘),issufficienttoensurethe
1
continuityoftheoperatorbecause |â„Ë†(ğ)| â‰¤ âˆ¥â„âˆ¥â„“ .However,thelatterconditionis
1
notnecessary;forinstance,theâ„“ -Lipschitzconstantofanideallowpasspassfilter
2
is1bydesign,whileits(sinc-like)impulseresponseisnotincludedinâ„“ (Zğ‘‘).
112 MichaelUnserandStanislasDucotterd
3.2 MultichannelConvolutionOperators
We now show that the concept carries over to vector-valued signals. To that end,
weconsideragenericmultichannelconvolutionoperatorthatactsonan ğ‘-channel
inputsignalx[Â·] âˆˆâ„“ğ‘(Zğ‘‘) andreturnsan ğ‘€-channeloutputy[Â·].Suchanoperator
2
(cid:2) (cid:3)
ischaracterizedthroughitsmatrix-valuedimpulseresponseH[Â·] with H[Â·] =
ğ‘š,ğ‘›
â„ ğ‘š,ğ‘›[Â·] âˆˆâ„“ 2(Zğ‘‘)andH[ğ’Œ] âˆˆRğ‘€Ã—ğ‘ foranyğ’Œ âˆˆZğ‘‘ .Fromnowon,weshalldenote
suchaconvolutionoperatorbyT andrefertoitasamultichannelfilter.
H
Tobenefitfromthetoolsandtheorydevelopedforthescalarcase,itisusefulto
express the multichannel convolution as the matrix-vector combination of a series
ofcomponent-wisescalarconvolutions (â„ ğ‘š,ğ‘›âˆ—ğ‘¥ ğ‘›)[Â·] with (ğ‘š,ğ‘›) âˆˆ {1,...,ğ‘€}Ã—
{1,...,ğ‘}.Thisiswrittenas
T H :x[Â·] â†¦â†’ (Hâˆ—x)[ğ’Œ] =â–³
ï£®
ï£¯ ï£¯ ï£¯
(cid:205) ğ‘›ğ‘ =1(â„ 1,ğ‘›
. . .
âˆ—ğ‘¥ ğ‘›)[ğ’Œ] ï£¹
ï£º ï£º ï£º, (22)
ï£¯ ï£¯ ï£°(cid:205) ğ‘›ğ‘ =1(â„ ğ‘€,ğ‘›âˆ—ğ‘¥ ğ‘›)[ğ’Œ]ï£º ï£º
ï£»
where
ï£® â„ 1,1[Â·] Â·Â·Â· â„ 1,ğ‘[Â·] ï£¹
H[Â·] = ï£¯ ï£¯
ï£¯
. . . ... . . . ï£º ï£º
ï£º
= [h 1[Â·] Â·Â·Â· hğ‘[Â·]] (23)
ï£¯ ï£¯â„ ğ‘€,1[Â·] Â·Â·Â· â„ ğ‘€,ğ‘[Â·]ï£º ï£º
ï£° ï£»
withtheğ‘›thcolumnoftheimpulseresponsebeingidentifiedas
ï£® â„ 1,ğ‘›[Â·] ï£¹
ï£¯ . ï£º
hğ‘›[Â·] = ï£¯
ï£¯
. . ï£º
ï£º
=T H{eğ‘›ğ›¿[Â·]}.
ï£¯ ï£¯â„ ğ‘€,ğ‘›[Â·]ï£º
ï£º
ï£° ï£»
We also note that the convolution in (22) has an explicit representation, given by
(25),whichisthematrix-vectorcounterpartofthescalarformula(21).
Asinthescalarscenario,themultichannelconvolutioncanbeimplementedbya
multiplicationintheFourierdomain,withthefrequencyresponseofthefilternow
havingtheformofamatrix.Specifically,foranyx[Â·] âˆˆ â„“ğ‘(Zğ‘‘) withvector-valued
2
Fouriertransformx= F {x[Â·]} âˆˆ ğ¿ğ‘(Tğ‘‘),wehavethat
(cid:98) d 2
(cid:8) (cid:9) (cid:104) (cid:105)
Fğ‘‘ (Hâˆ—x)[Â·] (ğ) =H(cid:98)(ğ) (cid:98)x(ğ) = (cid:98)h 1(ğ) Â·Â·Â· (cid:98)hğ‘(ğ) (cid:98)x(ğ)
ï£® â„Ë† 1,1(ğ) Â·Â·Â· â„Ë† 1,ğ‘(ğ) ï£¹ ï£® ğ‘¥Ë† 1(ğ) ï£¹
= ï£¯ ï£¯ . . . ... . . . ï£º ï£º ï£¯ ï£¯ . . . ï£º ï£º, (24)
ï£¯ ï£º ï£¯ ï£º
ï£¯
ï£¯â„Ë† ğ‘€,1(ğ) Â·Â·Â· â„Ë†
ğ‘€,ğ‘(ğ)ï£º
ï£º
ï£¯ ï£¯ğ‘¥Ë†ğ‘(ğ)ï£º
ï£º
ï£° ï£» ï£° ï£»ParsevalConvolutionOperatorsandNeuralNetworks 13
where the matrix-valued function H(cid:98) : [âˆ’ğœ‹,ğœ‹]ğ‘‘ â†’ Cğ‘Ã—ğ‘€ , with [H(cid:98)]ğ‘š,ğ‘› = â„Ë† ğ‘š,ğ‘› =
F d{â„ ğ‘š,ğ‘›}, is the component-by-component Fourier transform of the matrix filter
H[Â·].
3.3 KernelTheoremforMultichannelLSIOperators
The matrix-vector convolution specified by (22) is well-defined for any x[Â·] âˆˆ
â„“ğ‘(Zğ‘‘) under the assumption that H[Â·] âˆˆ â„“ (Zğ‘‘)ğ‘€Ã—ğ‘ . Yet, we need to be a bit
2 2
moreselectivetoensurethattheoperatoris(Lipschitz-)continuouswithrespectto
theâ„“ -norm.WeshowinTheorem2thatthereisanequivalencebetweencontinuous
2
multi-channel LSI operators and bounded multichannel filters (convolution opera-
tors), while we also give an explicit formula for the norm of the operator. As one
may expect, the Schwartz kernel of the LSI operator is the matrix-valued impulse
responseofthemultichannelfilter.
Theorem2(Kernel theorem for LSI operators â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘€(Zğ‘‘)) For any
2 2
given H[Â·] âˆˆ â„“ (Zğ‘‘)ğ‘€Ã—ğ‘, the convolution operator T : x[Â·] â†¦â†’ (Hâˆ—x)[Â·] with
2 H
ğ‘-vector-valuedinputx[Â·] âˆˆâ„“ğ‘(Zğ‘‘)andğ‘€-vector-valuedoutput
2
(Hâˆ—x)[ğ’Œ] = âˆ‘ï¸ H[â„“]x[ğ’Œâˆ’â„“], ğ’Œ âˆˆZğ‘‘ (25)
â„“âˆˆZğ‘‘
islinear-shift-invariantandcharacterizedbyitsmatrix-valuedfrequencyresponse
F d{H[Â·]} = H(cid:98)(Â·) âˆˆ ğ¿ 2(Tğ‘‘)ğ‘€Ã—ğ‘. Moreover, T
H
continuously maps â„“ 2ğ‘(Zğ‘‘) â†’
â„“ğ‘€(Zğ‘‘)ifandonlyif
2
âˆ¥T Hâˆ¥ =ğœ sup,H = esssup ğœ max(cid:0) H(cid:98)(ğ)(cid:1) <âˆ, (26)
ğâˆˆ[âˆ’ğœ‹,ğœ‹]ğ‘‘
whereğœ max(cid:0) H(cid:98)(ğ)(cid:1) withğfixedisthemaximalsingularvalueofthematrixH(cid:98)(ğ) âˆˆ
Cğ‘€Ã—ğ‘.
Conversely,foreverycontinuousLSIoperatorT :â„“ğ‘(Zğ‘‘) âˆ’â†’c. â„“ğ‘€(Zğ‘‘),there
LSI 2 2
isoneandonlyoneH[Â·] âˆˆâ„“ (Zğ‘‘)ğ‘€Ã—ğ‘ (thematrix-valuedimpulseresponseofT )
2 LSI
suchthatT LSI =T H :x[Â·] â†¦â†’ (Hâˆ—x)[Â·] andâˆ¥T LSIâˆ¥ â„“ğ‘â†’â„“ğ‘€ =ğœ sup,H <âˆ.
2 2
Proof.
Direct Part. The ğ‘šth entry of (Hâˆ—x)[ğ’Œ] can be identified as (cid:2) (Hâˆ—x)[ğ’Œ](cid:3) =
ğ‘š
âŸ¨gğ‘š,x[ğ’Œâˆ’Â·]âŸ© â„“ğ‘(Zğ‘‘) withgğ‘š[Â·] = (â„ ğ‘š,1[Â·],...,â„ ğ‘š,ğ‘[Â·]) âˆˆ â„“ 2ğ‘(Zğ‘‘) beingtheğ‘šth
2
rowofthematrix-valuedimpulseresponseH[Â·].TheLSIproperty(seeDefinition
4)thenfollowsfromtheobservationthat
(cid:2) (Hâˆ—x)[ğ’Œâˆ’ğ’Œ 0](cid:3) ğ‘š = âŸ¨gğ‘š,x[(ğ’Œâˆ’ğ’Œ 0)âˆ’Â·)]âŸ© â„“ğ‘(Zğ‘‘)
2
= âŸ¨gğ‘š,x[(ğ’Œâˆ’Â·)âˆ’ğ’Œ 0]âŸ© â„“ğ‘(Zğ‘‘) = (cid:2) (Hâˆ—x[Â·âˆ’ğ’Œ 0])[ğ’Œ](cid:3) ğ‘š,
214 MichaelUnserandStanislasDucotterd
forğ‘š =1,...,ğ‘€ andany ğ’Œ,ğ’Œ âˆˆZğ‘‘ .
0
The Fourier-domain equivalent of the hypothesis x âˆˆ â„“ğ‘(Zğ‘‘) (resp. H[Â·] âˆˆ
2
â„“ 2(Zğ‘‘)ğ‘€Ã—ğ‘ )is (cid:98)x âˆˆ ğ¿ 2ğ‘(Tğ‘‘) (resp.,H(cid:98)(Â·) âˆˆ ğ¿ 2(Tğ‘‘)ğ‘€Ã—ğ‘ ).Thekeyforthisequiva-
lenceisthevector-valuedversionofParsevalâ€™sidentitygivenby
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ‘ âˆ« dğ âˆ« dğ
âˆ¥xâˆ¥ â„“2
2ğ‘(Zğ‘‘)
= ğ‘›=1âˆ¥ğ‘¥ ğ‘›âˆ¥ â„“2
2
=
ğ‘›=1
Tğ‘‘
|ğ‘¥Ë†ğ‘›(ğ)|2
(2ğœ‹)ğ‘‘
=
Tğ‘‘
âˆ¥ (cid:98)x(ğ)âˆ¥2
2(2ğœ‹)ğ‘‘
= âˆ¥ (cid:98)xâˆ¥2
ğ¿
2ğ‘(Tğ‘‘).
Likewise, under the assumption that H(cid:98)(Â·) (cid:98)x(Â·) âˆˆ ğ¿ 2ğ‘€(Tğ‘‘), we can evaluate the â„“ 2-
normoftheconvolvedsignalas
(cid:18)âˆ«
dğ
(cid:19)1
2
âˆ¥Hâˆ—xâˆ¥ â„“ 2ğ‘€(Zğ‘‘) =
Tğ‘‘
âˆ¥H(cid:98)(ğ) (cid:98)x(ğ)âˆ¥2 2(2ğœ‹)ğ‘‘ = âˆ¥H(cid:98)(cid:98)xâˆ¥ ğ¿ 2ğ‘€(Tğ‘‘), (27)
wherewearerelyingonthepropertythattheconvolutioncorrespondstoapointwise
multiplicationintheFourierdomain.
NormoftheOperator.Implicitinthespecificationofğœ sup,Hin(26)istherequirement
thatthematrix-valuedfrequencyresponseH(cid:98)(Â·) : Tğ‘‘ â†’ Cğ‘€Ã—ğ‘ bemeasurableand
boundedalmosteverywhere.ThismeansthatH(cid:98)(ğ) withğ fixedisawell-defined
matrix in Cğ‘€Ã—ğ‘ for almost any ğ âˆˆ Tğ‘‘ . In that case, we can specify its maximal
singularvaluesby
ğœ max(cid:0) H(cid:98)(ğ)(cid:1) = sup
âˆ¥H(cid:98)( âˆ¥ğ uâˆ¥)uâˆ¥
2
uâˆˆCğ‘\{0} 2
Consequently,foranyx(Â·) âˆˆ ğ¿ğ‘(Tğ‘‘),wehavethat
(cid:98) 2
âˆ¥H(cid:98)(ğ) (cid:98)x(ğ)âˆ¥ 2 â‰¤ âˆ¥ (cid:98)x(ğ)âˆ¥ 2Â·ğœ max(cid:0) H(cid:98)(ğ)(cid:1) â‰¤ âˆ¥ (cid:98)x(ğ)âˆ¥ 2Â·ğœ sup,H
foralmostanyğ âˆˆTğ‘‘ .Thisimpliesthat
(cid:18)âˆ«
dğ
(cid:19)1
2
âˆ¥H(cid:98)(cid:98)xâˆ¥ ğ¿ 2ğ‘€(Tğ‘‘) =
Tğ‘‘
âˆ¥H(cid:98)(ğ) (cid:98)x(ğ)âˆ¥2 2(2ğœ‹)ğ‘‘
(cid:18)âˆ«
dğ
(cid:19)1
2
â‰¤ ğœ sup,H
Tğ‘‘
âˆ¥ (cid:98)x(ğ)âˆ¥2 2(2ğœ‹)ğ‘‘ =ğœ sup,HÂ·âˆ¥ (cid:98)xâˆ¥ ğ¿ 2ğ‘(Tğ‘‘) (28)
which,duetotheFourierisometry,yieldstheupperboundâˆ¥T Hâˆ¥ â‰¤ ğœ sup,H.
Likewise, (27) implies that âˆ¥T Hâˆ¥ = âˆ¥H(cid:98)âˆ¥, which is the norm of the pointwise
multiplication operator (cid:98)x â†¦â†’ H(cid:98)(cid:98)x and is equal to âˆ¥ğœ max(cid:0) H(cid:98)(Â·)(cid:1) âˆ¥ğ¿ âˆ. Indeed, for any
(cid:98)x(Â·) âˆˆ S(Tğ‘‘)ğ‘ âŠ‚ ğ¿ 2ğ‘(Tğ‘‘), the boundedness of H(cid:98) : ğ¿ 2ğ‘(Tğ‘‘) â†’ ğ¿ 2ğ‘€(Tğ‘‘) implies
thatParsevalConvolutionOperatorsandNeuralNetworks 15
âˆ« dğ âˆ« dğ
[âˆ’ğœ‹,ğœ‹]ğ‘‘(cid:98)xH(ğ)H(cid:98)H(ğ)H(cid:98)(ğ) (cid:98)x(ğ)
(2ğœ‹)ğ‘‘
â‰¤ âˆ¥H(cid:98)âˆ¥2
[âˆ’ğœ‹,ğœ‹]ğ‘‘
âˆ¥ (cid:98)x(ğ)âˆ¥2 2(2ğœ‹)ğ‘‘,
whichisequivalentto
âˆ« (cid:16) (cid:17) dğ
[âˆ’ğœ‹,ğœ‹]ğ‘‘(cid:98)xH(ğ) âˆ¥H(cid:98)âˆ¥2Iğ‘ âˆ’H(cid:98)H(ğ)H(cid:98)(ğ) (cid:98)x(ğ)
(2ğœ‹)ğ‘‘
â‰¥ 0.
(cid:16) (cid:17)
ThisrelationimpliesthattheHermitian-symmetricmatrix âˆ¥H(cid:98)âˆ¥2Iğ‘ âˆ’H(cid:98)H(ğ)H(cid:98)(ğ)
isnonnegative-definiteforalmostanyğ âˆˆ Tğ‘‘ .Onthesideoftheeigenvalues,this
translatesinto
(cid:16) (cid:17) (cid:16) (cid:17)
âˆ¥H(cid:98)âˆ¥2âˆ’ğœ†
max
H(cid:98)H(ğ)H(cid:98)(ğ) = âˆ¥H(cid:98)âˆ¥2âˆ’ğœ m2
ax
H(cid:98)(ğ) â‰¥ 0 ğ‘.ğ‘’.
(cid:16) (cid:17)
leadingto âˆ¥ğœ max H(cid:98)(Â·) âˆ¥ğ¿ âˆ = ğœ sup,H â‰¤ âˆ¥H(cid:98)âˆ¥ = âˆ¥T Hâˆ¥.Sincewealreadyknowthat
âˆ¥T Hâˆ¥ â‰¤ ğœ sup,H,wededucethatâˆ¥T Hâˆ¥ =ğœ sup,H.
(cid:104) (cid:105)
IndirectPart.Wedefinethelinearfunctionalsgğ‘š :x[Â·] â†¦â†’ âŸ¨gğ‘š,xâŸ© = T LSI{xâˆ¨}[0]
ğ‘š
with ğ‘š âˆˆ {1,...,ğ‘€}. The continuity of T : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘€(Zğ‘‘) implies that
LSI 2 2
(cid:104) (cid:105)
T LSI{xâˆ¨
ğ‘–
}[0] = âŸ¨gğ‘š,xğ‘–âŸ© â†’0foranyconvergingsequencexğ‘–[Â·] â†’0(or,equiv-
ğ‘š
alently, xâˆ¨
ğ‘–
[Â·] â†’ 0) inâ„“ 2ğ‘(Zğ‘‘). This ensures that the functional gğ‘š : x â†¦â†’ âŸ¨gğ‘š,xâŸ©
iscontinuousonâ„“ 2ğ‘(Zğ‘‘),whichisequivalenttogğ‘š = (ğ‘” ğ‘›,ğ‘š[ğ’Œ]) (ğ‘›,ğ’Œ)âˆˆ{1,...,ğ‘}Ã—Zğ‘‘ âˆˆ
(cid:0)â„“ 2ğ‘(Zğ‘‘)(cid:1)â€² = â„“ 2ğ‘(Zğ‘‘). This then allows us to write âŸ¨gğ‘š,xâŸ© = âŸ¨gğ‘š,xâŸ© â„“ğ‘ for all
ğ‘š âˆˆ {1,...,ğ‘€}.Wethenmakeuseoftheshift-invariancepropertytosho2 wthat
âŸ¨g 1,xâˆ¨[Â·+ğ’Œ]âŸ©
â„“ğ‘
(cid:169) . 2 (cid:170)
T LSI{x}[ğ’Œ] =T LSI{x[Â·+ğ’Œ]}[0] =(cid:173)
(cid:173)
. . (cid:174)
(cid:174)
(cid:173) âŸ¨gğ‘€,xâˆ¨[Â·+ğ’Œ]âŸ© â„“ğ‘(cid:174)
(cid:171) 2 (cid:172)
(cid:205) ğ‘›ğ‘ =1(cid:205) ğ’âˆˆZğ‘‘ğ‘” ğ‘›,1[ğ’]ğ‘¥ ğ‘›[ğ’Œâˆ’ğ’]
=(cid:169)
(cid:173)
. .
.
(cid:170)
(cid:174)
(cid:173) (cid:174)
(cid:171)(cid:205) ğ‘›ğ‘ =1(cid:205) ğ’âˆˆZğ‘‘ğ‘” ğ‘›,ğ‘€[ğ’]ğ‘¥ ğ‘›[ğ’Œâˆ’ğ’]
(cid:172)
(cid:205) ğ‘›ğ‘ =1(ğ‘” ğ‘›,1âˆ—ğ‘¥ ğ‘›)[ğ’Œ]
=(cid:169) (cid:173) . . . (cid:170) (cid:174)= (Hâˆ—x)[ğ’Œ]
(cid:173) (cid:174)
(cid:171)(cid:205) ğ‘›ğ‘ =1(ğ‘” ğ‘›,ğ‘€ âˆ—ğ‘¥ ğ‘›)[ğ’Œ]
(cid:172)
forany ğ’Œ âˆˆ Zğ‘‘ ,fromwhichwededucethatT = T withmatrix-valuedimpulse
LSI H
response H[Â·] whose entries are â„ ğ‘š,ğ‘›[ğ’Œ] = (cid:2) gğ‘š[ğ’Œ](cid:3)
ğ‘›
= ğ‘” ğ‘›,ğ‘š[ğ’Œ] with ğ‘” ğ‘›,ğ‘š[Â·] âˆˆ
â„“ (Zğ‘‘). âŠ“âŠ”
2
An immediate consequence is that the composition of the two continuous LSI
operatorsT : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘ 2(Zğ‘‘) andT : â„“ğ‘ 2(Zğ‘‘) â†’ â„“ğ‘€(Zğ‘‘) yieldsastable
H1 2 2 H2 2 216 MichaelUnserandStanislasDucotterd
multi-filter T
H
= T
H2âˆ—H1
: â„“ 2ğ‘(Zğ‘‘) â†’ â„“ 2ğ‘€(Zğ‘‘) with âˆ¥T Hâˆ¥ â‰¤ ğœ sup,H1ğœ sup,H2. The
frequency response of the composed filter is the product H(cid:98)(ğ) = H(cid:98)1(ğ)H(cid:98)2(ğ)
of the individual responses, as expected. On the side of the impulse response, this
translatesintothematrix-to-matrixconvolution
(H âˆ—H )[ğ’Œ] =â–³ âˆ‘ï¸ H [ğ’]H [ğ’Œâˆ’ğ’], ğ’Œ âˆˆZğ‘‘, (29)
2 1 2 1
ğ’âˆˆZğ‘‘
which is the matrix counterpart of (21). Beside the fact that the inner dimension
(ğ‘ )ofthematricesmustmatch,animportantdifferencewiththescalarsettingis
2
thatmatrixconvolutionsaregenerallynotcommutative.
3.4 ParsevalFilterbanks
We now proceed with the characterization of the complete family of Parseval LSI
operatorsfromâ„“ğ‘(Zğ‘‘) â†’â„“ğ‘€(Zğ‘‘).WeknowfromTheorem2thattheseareneces-
2 2
sarilyfilterbanksoftheformT :x[Â·] â†¦â†’ (Hâˆ—x)[Â·],whichcanalsobespecifiedby
H
theirmatrix-valuedfrequencyresponseH(cid:98)(Â·).Moreover,Proposition1tellsusthat
theParsevalconditionisequivalenttoTâˆ— â—¦T =Id.
H H
Consequently, the only remaining part is to identify the adjoint operator Tâˆ— :
H
â„“ğ‘€(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘),whichisdonethroughthemanipulation
2 2
âˆ€(x,y) âˆˆâ„“ğ‘(Zğ‘‘)Ã—â„“ğ‘€(Zğ‘‘) :
2 2
âŸ¨y,(Hâˆ—x)[Â·]âŸ© â„“ğ‘€(Zğ‘‘) = âŸ¨ (cid:98)y,H(cid:98)(cid:98)xâŸ© ğ¿ğ‘€(Tğ‘‘) = âŸ¨H(cid:98)H (cid:98)y, (cid:98)xâŸ© ğ¿ğ‘(Tğ‘‘) = âŸ¨(HTâˆ¨âˆ—y)[Â·],x)âŸ© â„“ğ‘(Zğ‘‘),
2 2 2 2
whereweusedtheFourier-Plancherelisometry,apointwiseHermitiantransposition
to move the frequency-response matrix on the other side of the inner product, and
the property that a complex conjugation of the frequency response translates into
theflippingoftheimpulseresponse.Basedon(18),wecanthenidentifyTâˆ— :yâ†¦â†’
H
(HTâˆ¨ âˆ—y)[Â·]. This shows that the adjoint of T is the convolution operator whose
H
matriximpulseresponseisHTâˆ¨[Â·] (theflippedandtransposedversionofH[Â·])and
whosefrequencyresponseisF d{HTâˆ¨[Â·]} =H(cid:98)H(Â·).
Proposition3(Characterization of Parseval-LSI operators) A linear operator
T:â„“ğ‘(Zğ‘‘) â†’â„“ğ‘€(Zğ‘‘)withğ‘€ â‰¥ ğ‘ isLSIandenergy-preserving(Parseval)ifand
2 2
onlyifitcanberepresentedasamultichannelfilterbankT=T :x[Â·] â†¦â†’ (Hâˆ—x)[Â·]
H
whosematrix-valuedimpulseresponseH[ğ’Œ] âˆˆ Rğ‘€Ã—ğ‘ with ğ’Œ rangingoverZğ‘‘ has
anyofthefollowingequivalentproperties.
1. Invertibilitybyflip-transposition:
(HTâˆ¨âˆ—H)[Â·] =Iğ‘ğ›¿[Â·],
whichisequivalenttoTâˆ— â—¦T =Idonâ„“ğ‘(Zğ‘‘).
H H 2ParsevalConvolutionOperatorsandNeuralNetworks 17
2. Paraunitaryfrequencyresponse:
H(cid:98)H(ğ)H(cid:98)(ğ) =Iğ‘ forallğ âˆˆTğ‘‘,
whereH(cid:98) = F d{H[Â·]} âˆˆ ğ¿ 2(Tğ‘‘)ğ‘€Ã—ğ‘ isthediscreteFouriertransformofH[Â·].
3. Preservationofinnerproducts:
âˆ€x,yâˆˆâ„“ 2ğ‘(Zğ‘‘) : âŸ¨x,yâŸ© â„“ğ‘(Zğ‘‘) = âŸ¨(Hâˆ—x)[Â·],(Hâˆ—y)[Â·]}âŸ© â„“ğ‘€(Zğ‘‘).
2 2
We also note that the LSI-Parseval property implies that âˆ¥H[Â·]âˆ¥ â„“ğ‘€Ã—ğ‘(Zğ‘‘) = ğ‘
2
andâˆ¥T âˆ¥ =1,althoughthoseconditionsareobviouslynotsufficient.
H
WhileItem1suggeststhattheadjointTâˆ— = T : â„“ğ‘€(Zğ‘‘) â†’ â„“ğ‘(Zğ‘‘) actsas
H HTâˆ¨ 2 2
the inverse of T , this is only true for signals y[Â·] âˆˆ T (cid:0)â„“ğ‘(Zğ‘‘)(cid:1) âŠ‚ â„“ğ‘€(Zğ‘‘) that
H H 2 2
areintherangeoftheoperator.Inotherwords,Tâˆ— isonlyaleftinverseofTâˆ—,while
H H
isfailstobearightinverseingeneral,unless ğ‘€ = ğ‘.ThisisdenotedbyTâˆ— = T+
H H
(generalizedinverse).
Whiletheğ‘€-to-ğ‘filterTâˆ— isgenerallynotaParsevalfilter,itis1-Lipschitz(since
H
âˆ¥Tâˆ— âˆ¥ = âˆ¥T âˆ¥ = 1) with its Gram operator (Tâˆ—âˆ— â—¦Tâˆ—) = (T â—¦Tâˆ—) : â„“ğ‘€(Zğ‘‘) â†’
H H H H H H 2
â„“ğ‘€(Zğ‘‘)
beingtheorthogonalprojectorontherangeofT ,ratherthantheidentity.
2 H
Correspondingly, from the properties of the singular value decomposition (SVD),
wecaninferthatH(cid:98)H(ğ)withğfixedhasthesamenonzerosingularvaluesasH(cid:98)(ğ)
(ğ‘ singular values equal to one) and that these are complemented with (ğ‘€ âˆ’ ğ‘)
additionalzerostomakeupforthefactthatğ‘€ > ğ‘.
ThefilterbanksusedinconvolutionalneuralnetworkaregenerallyFIR,meaning
thattheirmatriximpulseresponseisfinitelysupported.Thisisthereasonwhythe
reminderofthechapterisdevotedtotheinvestigationofFIR-Parsevalconvolution
operators.Tosetthestage,westartwiththesingle-channelcaseğ‘ = ğ‘€ =1,which
hasthefewestdegreesoffreedom.
Proposition4 Thereal-valuedLSIoperatorTâ„ :â„“ 2(Zğ‘‘) â†’â„“ 2(Zğ‘‘)isFIR-Parseval
if and only if â„ = Â±ğ›¿[Â·âˆ’ ğ’Œ 0] for some ğ’Œ
0
âˆˆ Zğ‘‘. Equivalently, Tâ„ = Â±Sğ’Œ0 where
Sğ’Œ0 :ğ‘¥[Â·] â†¦â†’ğ‘¥[Â·âˆ’ğ’Œ 0].
Proof. FromProposition3,weknowthattheLSI-Parsevalpropertyisequivalentto
(â„âˆ¨ âˆ— â„)[ğ’Œ] = (cid:205) ğ’âˆˆZğ‘‘ â„[âˆ’ğ’]â„[ğ’Œ âˆ’ ğ’] = (cid:205) ğ’âˆˆZğ‘‘ â„[ğ’]â„[ğ’ + ğ’Œ] = ğ›¿[ğ’Œ], which
is obviously met for â„ = Â±ğ›¿[Â·âˆ’ ğ’Œ ]. Now, if supp(â„) = {ğ’ âˆˆ Zğ‘‘ : â„[ğ’] â‰  0} is
0
finiteandincludesatleasttwodistinctpoints,thentherealwaysexistssomecritical
offset ğ’Œ â‰  0 such that supp(â„) âˆ©supp(â„[Â·+ ğ’Œ ]) = {ğ’ }; in other words, such
0 0 0
thattheintersectionofthesupportanditsshiftedversionbyâˆ’ğ’Œ consistsofasingle
0
point. Consequently, (cid:205) ğ’âˆˆZğ‘‘ â„[ğ’]â„[ğ’ + ğ’Œ 0] = â„[ğ’ 0]â„[ğ’ 0 + ğ’Œ 0] â‰  0, which is
incompatiblewiththedefinitionoftheKroneckerdelta. âŠ“âŠ”
Proposition4identifiestheshiftoperatorsasfundamentalLSI-Parsevalelements,
butthefamilyisactuallylargerifwerelaxtheFIRcondition.Thefrequency-domain
conditionforParsevalis |â„Ë†(ğ)| = 1,whichtranslatesintothefilterbeingall-pass.18 MichaelUnserandStanislasDucotterd
Besideanypoweroftheshiftoperator,aclassicexampleforğ‘‘ =1isâ„Ë†(ğœ”) = eâˆ’jğœ”âˆ’ğ‘§ 0 ,
1âˆ’ğ‘§ 0eâˆ’jğœ”
withthecaveatthattheimpulseresponseofthelatterisinfinitelysupported.
4 ParametrizationofParsevalFilterbanks
Whilethedesignoptionsfor(univariate)FIRParsevalfiltersarefairlylimited(see
Proposition 4), we now show that the possibilities open up considerably in the
multichannelsetting.Thisisgoodnewsforapplications.
Our approach to construct trainable FIR Parseval filterbanks is based on the
definition of basic 1-to-ğ‘, ğ‘-to-ğ‘, and ğ‘-to-(ğ‘ğ‘) Parseval filters that can then
be chained, in the spirit of neural networks, to produce more complex structures.
Specifically, let T : â„“ğ‘ğ‘–(Zğ‘‘) â†’ â„“ğ‘ğ‘–+1(Zğ‘‘),ğ‘– = 1,...,ğ¼ be a series of Parseval
Hğ‘– 2 2
filters with ğ‘ 1 = ğ‘ â‰¤ ğ‘ ğ‘– â‰¤ Â·Â·Â· â‰¤ ğ‘ ğ¼+1 = ğ‘€ and Tâˆ— Hğ‘– â—¦T Hğ‘– = Id on â„“ 2ğ‘ğ‘–(Zğ‘‘).
Because the LSI and Parseval properties are preserved through composition, one
immediatelydeducesthatthecomposedoperator
T =T â—¦Â·Â·Â·â—¦T :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘€(Zğ‘‘) (30)
H Hğ¼ H1 2 2
isParseval-LSIwithimpulseresponseH[Â·] = (Hğ¼âˆ—Â·Â·Â·âˆ—H 2âˆ—H 1)[Â·] âˆˆâ„“ 2(Zğ‘‘)ğ‘€Ã—ğ‘ .
Thisfilterisinvertiblefromtheleftwithitsgeneralizedinversebeing
T+ =Tâˆ— =Tâˆ— â—¦Â·Â·Â·â—¦Tâˆ— :â„“ğ‘€(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘), (31)
H H H1 Hğ¼ 2 2
which means that the inverse filtering can be achieved via a simple flow-graph
transpositionoftheoriginalfilterarchitecture.
Thus, our design concept is to rely on simple elementary modules, each being
parameterized by an orthogonal matrix Uğ‘– âˆˆ Cğ‘€Ã—ğ‘€ where ğ‘€ is typically the
numberofoutputchannels.ThelistofourprimarymodulesissummarizedinTable
1.Additionaldetaileddescriptionsandexplanationsaregivenintheremainderof
thissection.
4.1 Normalizedpatchoperator
Ourfirsttoolisasimplemechanismtoaugmentthenumberofoutputchannelsof
thefilterbank.Itinvolvesapatchofsizeğ‘€ specifiedbyalistK ğ‘€ = {ğ’Œ 1,Â·Â·Â·ğ’Œğ‘€}of
indices,whichwillthereafterbeusedtodescribethesupportoffiltersactingoneach
featurechannel.Ournormalizedpatchoperatorextractsthesignalvalueswithinthe
patchinrunningfashionasParsevalConvolutionOperatorsandNeuralNetworks 19
LSI-ParsevalOperators ImpulseResponse
PatchdescriptorK ğ‘€ ={ğ’Œ 1,...,ğ’Œğ‘€}
Iğ‘ğ›¿[Â·âˆ’ğ’Œ 1]
PatchKğ‘€ :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘€Ã—ğ‘(Zğ‘‘) âˆš1 ğ‘€(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174)
(cid:174)
Iğ‘ğ›¿[Â·âˆ’ğ’Œğ‘€]
UnitarymatrixU=[u 1...uğ‘] âˆˆCğ‘Ã—ğ‘ (cid:171) (cid:172)
MultU:â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) Uğ›¿[Â·]
ğ‘
Th=UPatchKğ‘ :â„“ 2(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) h[Â·]= âˆš1
ğ‘
âˆ‘ï¸ uğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›]
ğ‘›=1
LargeunitarymatrixU=[U 1Â·Â·Â·Uğ‘] âˆˆCğ‘ğ‘Ã—ğ‘ğ‘
ğ‘
UPatchKğ‘ :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘ğ‘ (Zğ‘‘) âˆš1 ğ‘âˆ‘ï¸ Uğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›]
ğ‘›=1
Generalizedshiftwithğ‘² = (ğ’Œ 1,...,ğ’Œğ‘) âˆˆZğ‘‘Ã—ğ‘
Sğ‘² :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) diag(ğ›¿[Â·âˆ’ğ’Œ 1],...,ğ›¿[Â·âˆ’ğ’Œğ‘])
FramematrixAâˆˆCğ‘€Ã—ğ‘ s.t.AHA=Iğ‘
ğ‘
ASğ‘² :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘€(Zğ‘‘) âˆ‘ï¸ ağ‘›eH ğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›]
ğ‘›=1
UnitarymatricesU,VâˆˆCğ‘Ã—ğ‘
ğ‘
USğ‘² :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) âˆ‘ï¸ uğ‘›eH ğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›]
ğ‘›=1
ğ‘
USğ‘²VH:â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) âˆ‘ï¸ uğ‘›vH ğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›]
ğ‘›=1
Rank-ğ‘˜projectorPğ‘˜ =Uğ‘˜UH
ğ‘˜
âˆˆRğ‘Ã—ğ‘ withP2
ğ‘˜
=Pğ‘˜ =PT
ğ‘˜
PPğ‘˜,ğ‘›:â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) (Iğ‘ âˆ’Pğ‘˜)ğ›¿[Â·]+Pğ‘˜ğ›¿[Â·âˆ’eğ‘›]
HouseholderelementwithuâˆˆCğ‘ s.t.âˆ¥uâˆ¥ =1
2
Hu,ğ‘›:â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) (Iğ‘ âˆ’uuH)ğ›¿[Â·]+uuHğ›¿[Â·âˆ’eğ‘›]
Table1:ElementaryparametricParsevalmulti-filters.There,mostfiltersareparam-
eterized by a unitary matrix/frame and a list of neighborhood indices ğ’Œ 1,...,ğ’Œğ‘
(notnecessarilydistinct).Thevectoreğ‘› with [eğ‘›]ğ‘š = ğ›¿ ğ‘›âˆ’ğ‘š istheğ‘›thelementofa
canonicalbasis.
x[Â·âˆ’ğ’Œ ]
1
PatchKğ‘€ :x[Â·] â†¦â†’ âˆš1
ğ‘€
(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174) (cid:174). (32)
x[Â·âˆ’ğ’Œğ‘€]
(cid:171) (cid:172)
One easily checks that PatchKğ‘€ : â„“ 2ğ‘(Zğ‘‘) â†’ â„“ 2ğ‘€Ã—ğ‘(Zğ‘‘) is LSI and Parseval,
because the â„“ 2-norm is invariant to a shift and conserved in eacâˆšh of the output
componentsâ€”the very reason why the output is normalized by ğ‘€. Its adjoint,
Patchâˆ— :â„“ğ‘€Ã—ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘),isthesignalrecompositionoperator
Kğ‘€ 2 220 MichaelUnserandStanislasDucotterd
y [Â·]
1 ğ‘€
Patchâˆ— Kğ‘€ :(cid:169) (cid:173)
(cid:173)
. . . (cid:170) (cid:174) (cid:174)â†¦â†’ âˆš1
ğ‘€
âˆ‘ï¸ yğ‘š[Â·+ğ’Œğ‘š], (33)
yğ‘€[Â·] ğ‘š=1
(cid:171) (cid:172)
where the yğ‘š[Â·] are ğ‘-vector-valued signals. The fundamental property for this
construction is Patchâˆ—
Kğ‘€
â—¦PatchKğ‘€ = Id on â„“ 2ğ‘(Zğ‘‘), as direct consequence of the
isometricnatureoftheoperator.
For ğ‘ = 1, the impulse response of Patchâˆ—
Kğ‘€
is âˆš1
ğ‘€
(cid:2)ğ›¿(Â·+ğ’Œ 1) Â·Â·Â· ğ›¿(Â·+ğ’Œğ‘€)(cid:3)
whosevector-valuedFouriertransformis âˆš1 (cid:2) ejâŸ¨ğ,ğ’Œ1âŸ© Â·Â·Â· ejâŸ¨ğ,ğ’Œğ‘€âŸ©(cid:3) .Theparauni-
ğ‘€
tarynatureofthissystemisrevealedinthebasicrelation
ï£®eâˆ’jâŸ¨ğ,ğ’Œ1âŸ©
ï£¹
âˆš1 (cid:2) ejâŸ¨ğ,ğ’Œ1âŸ© Â·Â·Â· ejâŸ¨ğ,ğ’Œğ‘€âŸ©(cid:3) âˆš1
ï£¯
ï£¯ ï£¯
.
. .
ï£º
ï£º ï£º =
(cid:205) ğ‘šğ‘€ =1|ejâŸ¨ğ,ğ’Œğ‘šâŸ©|2
=1, (34)
ğ‘€ ğ‘€ ï£¯ ï£º ğ‘€
ï£¯ ï£¯eâˆ’jâŸ¨ğ,ğ’Œğ‘€âŸ©ï£º
ï£º
ï£° ï£»
whichholdsforanychoiceofthe ğ’Œğ‘š.
4.2 Parametric1-to-ğ‘µ ParsevalModule
Thenecessaryandsufficientconditionfora1-to-ğ‘operatorT :â„“ (Zğ‘‘) â†’â„“ğ‘(Zğ‘‘)
h 2 2
tohavetheParsevalpropertyis
ğ‘
âˆ‘ï¸
(hTâˆ¨âˆ—h)[Â·] = (â„âˆ¨
ğ‘›
âˆ—â„ ğ‘›)[Â·] =ğ›¿[Â·],
ğ‘›=1
which,oncestatedininthefrequencydomain,is
ğ‘
âˆ€ğ âˆˆTğ‘‘ : âˆ¥(cid:98)h(ğ)âˆ¥2 =âˆ‘ï¸ |â„Ë† ğ‘›(ğ)|2 =1. (35)
ğ‘›=1
This indicates that the frequency responses of the component filters â„ ğ‘› should be
power complementary. This is a standard requirement in wavelet theory and the
constructionoftightframes,whichhasbeenthebasisforvariousparametrizations
[53,42].
Whatweproposehereisasimplematrix-basedconstructionofsuchfilterswith
the support of each filter also being of size ğ‘. The filtering window, which is
common to all channels and assimilated to a patch, is specified by the index set
K ğ‘ = {ğ’Œ 1,...,ğ’Œğ‘}.Theseindicesareusuallychosentobecontiguousandcentered
around the origin. For instance, K = {âˆ’1,0,1} specifies centered filters of size 3
3
indimensionğ‘‘ =1.GivensomeorthogonalmatrixU= (cid:2) u
1
Â·Â·Â· uğ‘(cid:3) âˆˆ Rğ‘Ã—ğ‘ ,our
basicparametric1-to-ğ‘ filteringoperatoristhengivenbyParsevalConvolutionOperatorsandNeuralNetworks 21
T
h
=Mult Uâ—¦PatchKğ‘ :â„“ 2(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘), (36)
where Mult : x[Â·] â†¦â†’ Ux[Â·] is the pointwise matrix-multiplication operator.
U
This succession of operations yields the vector-valued impulse response h[Â·] =
âˆš1
ğ‘
(cid:205) ğ‘›ğ‘ =1uğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›].ThisfilterisParsevalbyconstructionbecauseitisthecom-
positionoftwoParsevaloperators.
ğ‘
0A <s ğ‘va ,r wia hn ict, hw the enm ra ey sua ll ts so inco an ss hi od re tr era Pr ae rd su evc ae ld fip lta et rch h[K Â·]ğ‘
=0
âˆš=
1
ğ‘{ğ’Œ (cid:205)1,
ğ‘›ğ‘
=.
0
1.. u, ğ‘›ğ’Œ ğ›¿ğ‘ [Â·0} âˆ’w ğ’Œğ‘›it ]h
.
Thelatterisparameterizedbytheâ€œtruncatedâ€matrixU
0
= (cid:2) u
1
0 Â·Â·Â· uğ‘ 0(cid:3) âˆˆ Rğ‘Ã—ğ‘ 0,
whichissuchthatUT 0U
0
=Iğ‘
0
(1-tightframeproperty).
4.3 Parametric ğ‘µ-to-ğ’‘ğ‘µ ParsevalModule
TheconcepthereisessentiallythesameasinSection4.2,exceptthatwenowhaveto
usealargerortho-matrixUâˆˆRğ‘ğ‘Ã—ğ‘ğ‘ andapatchneighorhoodK ğ‘ = {ğ’Œ 1,...,ğ’Œğ‘}.
Thisthenyieldsthemulti-filter
T
H
=Mult Uâ—¦PatchKğ‘ :â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘ğ‘ (Zğ‘‘), (37)
whichisguaranteedtohavetheParsevalproperty,basedonthesameargumentsas
before.Itsadjointis
Tâˆ— =T+ =Patchâˆ— â—¦Mult :â„“ğ‘ğ‘ (Zğ‘‘) â†’â„“ğ‘(Zğ‘‘). (38)
H H Kğ‘ UT 2 2
(cid:2) (cid:3)
ToidentifytheimpulseresponseofMult Uâ—¦PatchKğ‘,wepartitionU= U
1
Â·Â·Â·Uğ‘
into ğ‘submatricesUğ‘– âˆˆRğ‘ğ‘Ã—ğ‘ ,eachassociatedwithitsshift ğ’Œğ‘–,whichyields
ğ‘
1 âˆ‘ï¸
H[Â·] = âˆš
ğ‘
Uğ‘–ğ›¿[Â·âˆ’ğ’Œğ‘–]. (39)
ğ‘–=1
BytheorthonormalityofthecolumnvectorsofU,wethenexplicitlyevaluate
ğ‘ ğ‘
1 âˆ‘ï¸âˆ‘ï¸
(HTâˆ¨âˆ—H)[Â·] =
ğ‘
UT ğ‘šUğ‘›ğ›¿[Â·+ğ’Œğ‘šâˆ’ğ’Œğ‘›]
ğ‘š=1ğ‘›=1
ğ‘ ğ‘
1 âˆ‘ï¸ 1 âˆ‘ï¸
=
ğ‘
UT ğ‘›Uğ‘›ğ›¿[Â·] =
ğ‘
Iğ‘ğ›¿[Â·] =Iğ‘ğ›¿[Â·], (40)
ğ‘›=1 ğ‘›=1
whichconfirmsthatTâˆ— =T+.
H H22 MichaelUnserandStanislasDucotterd
4.4 GeneralizedShiftComposedwithaTightFrame
Withtheviewofextending(36)tovector-valuedsignals,weintroducethegeneralized
shift(orscrambling)operatorSğ‘² : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘(Zğ‘‘),withtranslationparameter
2 2
ğ‘² = (ğ’Œ 1,...,ğ’Œğ‘) âˆˆZğ‘‘Ã—ğ‘ ,as
Sğ’Œ1{ğ‘¥ 1[Â·]} ğ‘¥ 1[Â·âˆ’ğ’Œ 1]
Sğ‘²{x[Â·]} =(cid:169) (cid:173) . . . (cid:170) (cid:174)=(cid:169) (cid:173) . . . (cid:170) (cid:174). (41)
(cid:173) (cid:174) (cid:173) (cid:174)
Sğ’Œğ‘{ğ‘¥ ğ‘[Â·]} ğ‘¥ ğ‘[Â·âˆ’ğ’Œğ‘]
(cid:171) (cid:172) (cid:171) (cid:172)
Itisthemultichannelextensionofthescalarshiftbyğ’Œ
0
âˆˆZğ‘‘ denotedbySğ’Œ0 :ğ‘¥[Â·] â†¦â†’
ğ‘¥[Â·âˆ’ ğ’Œ ]. The generalized shift is obviously LSI-Parseval and has the convenient
0
semigrouppropertySğ‘²0Sğ‘² =S(ğ‘²0+ğ‘²) :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘)withSğ‘²âˆ’ğ‘² =S0ğ‘ =Id,
2 2
foranyğ‘²,ğ‘² âˆˆZğ‘‘Ã—ğ‘ .ThisisparalleltothescalarsettingwherewehavethatS0 =Id
0
andSğ’Œ0Sğ’Œ =S(ğ’Œ0+ğ’Œ) :â„“ 2(Zğ‘‘) â†’â„“ 2(Zğ‘‘) forany ğ’Œ,ğ’Œ
0
âˆˆ Zğ‘‘ ,sothat (Sğ’Œ0)âˆ’1 =Sâˆ’ğ’Œ0
withallshiftoperatorsbeingunitary.
Now,letA= (cid:2) a
1
Â·Â·Â· ağ‘(cid:3) = (cid:2) b
1
Â·Â·Â· bğ‘€(cid:3)T âˆˆRğ‘€Ã—ğ‘ withğ‘€ â‰¥ ğ‘ bearectangu-
larmatrixsuchthatATA=Iğ‘ (tight-frameproperty).Thegeometryissuchthatthe
columnvectors {ağ‘›} ğ‘›ğ‘
=1
formanorthonormalfamilyinRğ‘€ (butnotabasisunless
ğ‘€ = ğ‘), while the row vectors {bğ‘š} ğ‘šğ‘€
=1
form a 1-tight frame of Rğ‘ that is the
redundant counterpart of an ortho-basis. Here too, the defining property is energy
conservation: (cid:205) ğ‘š=1|âŸ¨bğ‘š,xâŸ©|2 = âŸ¨Ax,AxâŸ© = âŸ¨ATAx,xâŸ© = âˆ¥xâˆ¥2
2
for all x âˆˆ Rğ‘
(Parseval),albeitinthesimplerfinite-dimensionalsetting.
Given such a tight-frame matrix A âˆˆ Rğ‘€Ã—ğ‘ and a set of shift indices ğ‘² =
(ğ’Œ 1,...,ğ’Œğ‘) âˆˆZğ‘‘Ã—ğ‘ ,wethenspecifytheoperator
T =Mult â—¦Sğ‘² =ASğ‘² :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘€(Zğ‘‘). (42)
H A 2 2
The matrix-valued frequency response of this filter is H(cid:98)(ğ) = Adiag(eâˆ’jâŸ¨ğ,ğ’Œ1âŸ©,
...,eâˆ’jâŸ¨ğ,ğ’Œğ‘âŸ©),whichisparaunitary,irrespectivelyofthechoiceoftheshifts ğ’Œğ‘š.
4.5 ğ‘µ-to-ğ‘µ ParsevalFilters
WeknowfromProposition3thatT
:â„“ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘)isaParsevalmulti-filterif
H 2 2
andonlyifH(cid:98)H(ğ)H(cid:98)(ğ) =Iğ‘forallğ âˆˆTğ‘‘ .Bytakinginspirationfromthesingular-
valuedecomposition,thissuggeststheconsiderationofparaunitaryelementsofthe
form:Uğš²(cid:98)(ğ),ğš²(cid:98)(ğ)VH,orUğš²(cid:98)(ğ)VH,whereU = (cid:2) u
1
Â·Â·Â· uğ‘(cid:3) âˆˆ Cğ‘Ã—ğ‘ andV =
(cid:0) v
1
... vğ‘(cid:1) âˆˆ Cğ‘Ã—ğ‘ are unitary matrices, and ğš²(cid:98)(ğ) = diag(cid:0)ğœ†Ë† 1(ğ),...,ğœ†Ë† ğ‘(ğ)(cid:1)
wheretheğœ†Ë† ğ‘›(ğ)areall-passfilterswith|ğœ†Ë† ğ‘›(ğ)| =1forallğ âˆˆTğ‘‘ .
Since our focus is on FIR filters, we invoke Proposition 4 to deduce that the
only acceptable form of diagonal matrix is ğš²(cid:98)(ğ) = diag(eâˆ’jâŸ¨ğ,ğ’Œ1âŸ©,...,eâˆ’jâŸ¨ğ,ğ’Œğ‘âŸ©)ParsevalConvolutionOperatorsandNeuralNetworks 23
with shift parameter (ğ’Œ 1,...,ğ’Œğ‘) = K âˆˆ Zğ‘‘Ã—ğ‘ , which is precisely the frequency
response of the generalized shift operator Sğ‘². The resulting parametric Parseval
operatorsareUSğ‘²,Sğ‘²VH,andUSğ‘²VH.Theimpulseresponsesofthesefiltersare
sumsofrank-1elementswiththeirsupportbeingspecifiedbythe ğ’Œğ‘š,whichneed
notbedistinct.Specifically,wehavethat
ğ‘
USğ‘²VH =T
H
:â„“ 2ğ‘(Zğ‘‘) â†’â„“ 2ğ‘(Zğ‘‘) with H[Â·] =âˆ‘ï¸ uğ‘›vH ğ‘›ğ›¿[Â·âˆ’ğ’Œğ‘›], (43)
ğ‘›=1
which encompasses the two lighter filter variants by taking U or V equal to Iğ‘ =
(cid:2) (cid:3)
e
1
Â·Â·Â· eğ‘ .Anothercanonicalconfigurationof (43)isobtainedbytakingU = V
which,asweshallsee,makesaninterestingconnectionwithtwoclassicfactorization
ofparaunitarysystems.WhiletheoperatorUSğ‘²VH isobviouslyageneralizationof
USğ‘²,thereiscomputationalmeritwiththelighterversion,especiallyinthecontext
ofcomposition.
Proposition5 Let W 1,...,Wğ‘€+1 âˆˆ Cğ‘Ã—ğ‘ and U 1,...,Uğ‘€+1 âˆˆ Cğ‘Ã—ğ‘ be two
series of orthogonal matrices, and ğ‘² 1,...,ğ‘²ğ‘€ âˆˆ Zğ‘‘Ã—ğ‘ some corresponding shift
indices.Then,thecomposedparametricoperators
Wğ‘€+1Sğ‘²ğ‘€Wğ‘€Â·Â·Â·W 3Sğ‘²2W 2Sğ‘²1W
1
(44)
and
UH ğ‘€+1(Uğ‘€Sğ‘²ğ‘€UH ğ‘€)Â·Â·Â·(U 2Sğ‘²2UH 2)(U 1Sğ‘²1UH 1) (45)
spanthesamefamilyofğ‘-to-ğ‘ Parsevalmulti-filters.
Indeed,bysettingW
1
= UH 1,W
2
= UH 2U 1,...,Wğ‘€ = UH ğ‘€Uğ‘€âˆ’1 andWğ‘€+1 =
UH ğ‘€+1Uğ‘€,wecanuse(44)toreplicate(45).Again,thekeyisthatthemultiplication
(composition) of two unitary matrices yields another unitary matrix. Conversely,
(45) reproduces (44) if we set UH = W , UH = W UH = W W ,..., UH =
1 1 2 2 1 2 1 ğ‘€
Wğ‘€âˆ’1UH
ğ‘âˆ’1
=Wğ‘€âˆ’1Wğ‘€âˆ’2Â·Â·Â·W 1,andUH
ğ‘€+1
=Wğ‘€UH
ğ‘
=Wğ‘€Â·Â·Â·W 1.
4.6 Projection-basedParsevalFilterbanks
These ğ‘-to-ğ‘ filterbanks are parameterized by a projection matrix P. They are
multi-dimensional adaptations of classic canonical structures that were introduced
by Vaidyanathan and others for the factorization of paraunitry matrices for ğ‘‘ = 1
[41].Toexplaintheconcept,werecallthatamatrixPisamemberofP(ğ‘,ğ‘˜) (the
setofallorthogonalprojectionmatricesinRğ‘ ofrankğ‘˜)ifandonlyifitfulfilsthe
followingconditions:
1. Rank:PâˆˆRğ‘Ã—ğ‘ withrank(P) = ğ‘˜.
2. Idempotence:PP=P.24 MichaelUnserandStanislasDucotterd
3. Symmetry: PT = P, which together with Item 2, implies that P is an ortho-
projector.
AnyPâˆˆP(ğ‘,ğ‘˜)canbeparameterizedasP=(cid:205) ğ‘›ğ‘˜ =1uğ‘›uT
ğ‘›
=Proj span(u1,...,uğ‘˜),where
u 1,...,uğ‘˜ is a set of orthogonal vectors in Rğ‘ . Since P is an ortho-projector, it
inducesthedirect-sumdecompositionRğ‘ =Ran(P)âŠ•Ker(P),wherethemembers
ofRan(P) areeigenvectorsofPwitheigenvalue1(projectionproperty),whilethe
membersofKer(P) =Ran(P)âŠ¥areeigenvectorswitheigenvalue0.Theparametriza-
tionofPthensimplyfollowsfromtheSVD,withthevectorsu 1,...,uğ‘˜ beingany
setoforthogonalmembersofRan(P).Inparticular,therank-1ortho-projectorsare
parameterizedbyasingleunitvector,withP(ğ‘,1) = {uuT âˆˆ Rğ‘Ã—ğ‘ s.t.âˆ¥uâˆ¥ = 1}.
2
Finally, two projection matrices P âˆˆ P(ğ‘,ğ‘˜) and(cid:101)P âˆˆ P(ğ‘,ğ‘ âˆ’ ğ‘˜) are said to be
complementary if P+(cid:101)P = Iğ‘. In fact, Pğ‘˜ âˆˆ P(ğ‘,ğ‘˜) has a single complementary
projectorthatisgivenbyIğ‘ âˆ’PâˆˆP(ğ‘,ğ‘ âˆ’ğ‘˜).
A basic FIR-Parseval projection element is characterized by a matrix impulse
responseoftheform
H P,ğ’Œ1[Â·] = (Iğ‘ âˆ’P)ğ›¿[Â·]+Pğ›¿[Â·âˆ’ğ’Œ 1], (46)
whereP âˆˆ Rğ‘Ã—ğ‘ isaprojectionmatrixand ğ’Œ âˆˆ [âˆ’1,1]ğ‘‘\{0}issomeelementary
1
(multidimensional)unitshift.WeshallrefertosuchastructurebyPROJ-ğ‘˜,with ğ‘˜
beingtheranktheprojector.TheimpulseresponseofaPROJ-1elementis
H u,ğ’Œ1[Â·] = (Iğ‘ âˆ’uuT)ğ›¿[Â·]+uuTğ›¿[Â·âˆ’ğ’Œ 1] (47)
which, similarly to a Householder matrix, can be parameterized by a single unit
vectoru.MoregenerallyforPROJ-ğ‘˜,theconditionP âˆˆ P(ğ‘,ğ‘˜) translatesintothe
existenceofanortho-matrixU= [u 1Â·Â·Â·uğ‘] âˆˆRğ‘Ã—ğ‘ suchthatP=(cid:205) ğ‘›ğ‘˜ =1uğ‘›uT ğ‘›and
(Iğ‘ âˆ’P) = (cid:205) ğ‘›ğ‘ =ğ‘˜+1uğ‘›uT ğ‘›. This then allows us to express the convolution operator
x co[Â· n] sisâ†¦â†’
ts
o( fH tP w,ğ’Œ o1 sâˆ— hx if) t[ sÂ·] ona ls y:T ğ’ŒHP,ğ’Œ r1 ep= eatU edSğ‘² ğ‘˜1U tiH mew si ,th ana ds 0cr (a zm erb ol )in fg orm that eri rx emğ‘² a1 int ih na gt
1
entries.Consequently,(46)and(47)aretwospecialcasesof (43),whichconfirms
theirParsevalproperty.
Whilethe ğ‘-to-ğ‘ schemewepresentedinSection4.5ismoregeneralandalso
suggests some natural computational streamlining (see Proposition 5), arguments
canbemadeinfavoroftheuseofPROJ-ğ‘˜ filteringcomponents,eachofwhichhas
aminimalsupportofsize2.
Thestrongestargumentistheoreticalbutonlyholdsforğ‘‘ = 1[19].Specifically,
ithasbeenshownthatallParsevalfiltersofafixedMcMillandegree(i.e.,thetotal
number of delays required to implement the filterbank) admit a factorization in
termsofProj-1elements[41].Likewise,anyfilterbankwithfiltersoffixedsupport
ğ‘€ admits a factorization in terms of Proj-ğ‘˜ elements, which ensures that such a
parametrizationiscomplete[48].Thetrickypartinthislattertypeoffactorization
isthatitalsorequirestheadjustmentoftherankğ‘˜ ğ‘– ofeachcomponent.
Unfortunately,suchresultsdonotgeneralizetohigherdimensionsbecauseofthe
lack of a general polynomial factorization theorem for ğ‘‘ > 1. Simply stated, thisParsevalConvolutionOperatorsandNeuralNetworks 25
means that there are many multidimensional filters that cannot be realized from a
compositionofelementaryfiltersofsize2.For ğ‘‘ = 1,theelementaryshiftin(46)
and(47)issetto ğ‘˜ = 1,butitisnotclearhowtoproceedsystematicallyinhigher
1
dimensions.
Inthecontextofaconvolutionalneuralnetworkwheremanydesignchoicesare
adhoc,thelackofguaranteeofcompletenessamongallParsevalfiltersofsize ğ‘€
(onearbitraryfamilyoffiltersamongmanyothers)isnotparticularlytroublesome.
Themoreimportantissueistobeabletoexploittheavailabledegreesoffreedomby
adjustingtheparametersforbestperformanceduringthetrainingprocedure.Thisis
achievedeffectivelyfor ğ‘‘ = 2intheblock-convolutionorthogonalparametrization
(BCOP) framework [30], which relies on the composition of PROJ-ğ‘˜ with ğ’Œ âˆˆ
1
{(0,1),(1,0)} (inalternation).Byformulatingthetrainingproblemwithtwicethe
numberofchannels(halfofwhicharedummyandconstrainedtohavezerooutput)
with P âˆˆ P(2ğ‘,ğ‘), the authors are also able to optimally adjust the parameter ğ‘˜
(rankoftheprojector)foreachunit.
5 ApplicationtoDenoisingandImageReconstruction
WenowdiscusstheapplicationofParsevalfilterbankstobiomedicalimagerecon-
struction.Specifically,weshallrelyon1-LipschitzneuralnetworksthatuseParseval
convolution layers and that are trained for the denoising of a representative set of
images.
Dependingonthecontext,theimagetobereconstructedisdescribedasasignal
ğ‘ [Â·] âˆˆ â„“ (Zğ‘‘) or as the vector s = (ğ‘ [ğ’Œ]) âˆˆ Rğ¾ , where Î© âŠ‚ Zğ‘‘ is a region
2 ğ’ŒâˆˆÎ©
of interest composed of a finite number ğ¾ of pixels. Our computational task is to
recoversâˆˆRğ¾
fromthenoisymeasurementvector
y=As+nâˆˆRğ‘€,
(48)
wherenissome(unknown)noisecomponentandwhereA âˆˆ Rğ‘€Ã—ğ¾ isthesystem
matrix that models the physics of the acquisition process. A simplified version of
(48)withğ‘€ =ğ¾andA=I(identity)isthebasicdenoisingproblem,wherethetask
istorecoversfromthenoisysignal
z=s+nâˆˆRğ¾.
(49)
5.1 FromVariationaltoIterativePlug-and-PlayReconstruction
Tomakesignal-recoveryproblemswell-posedmathematically,oneusuallyincorpo-
ratespriorknowledgeabouttheunknownimagesbyimposingregularityconstraints
onthesolution.Thisleadstothevariationalreconstruction26 MichaelUnserandStanislasDucotterd
sâˆ— =argmin(ğ½(y,As)+ğ‘…(s)), (50)
sâˆˆRğ¾
whereğ½: Rğ‘€Ã—Rğ‘€ â†’R+isadata-fidelitytermandğ‘…: Rğ¾ â†’R+isaregularization
functional that penalizes â€œnon-regularâ€ solutions. If s â†¦â†’ ğ½(y,As) is differentiable
andğ‘…isconvex,then(50)canbesolvedbytheiterativeforward-backwardsplitting
(FBS)algorithm[14]with
sğ‘˜+1 =prox(cid:8) sğ‘˜ âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(y,Asğ‘˜)(cid:9).
(51)
ğ›¼ğ‘…
Here,âˆ‡âˆ‡âˆ‡ğ½(y,As) isthegradientof ğ½ withrespecttos, ğ›¼ âˆˆ Risthestepsizeofthe
update,andprox istheproximaloperatorofğ‘…definedas
ğ‘…
(cid:16) (cid:17)
prox{z} =argmin 1âˆ¥zâˆ’sâˆ¥2+ğ‘…(s) . (52)
2
ğ‘… sâˆˆRğ¾
Animportantobservationisthatprox
:Rğ¾ â†’Rğ¾
actuallyreturnsthesolutionof
ğ‘…
thedenoisingproblemwithavariationalformulationthatisaparticularcaseof(50)
withA=Iandthequadraticdatatermğ½(z,s) = 1âˆ¥zâˆ’sâˆ¥2.
2
The philosophy of PnP algorithms [52] is to replace prox with an off-the-
ğ›¼ğ‘…
shelf denoiser D: Rğ¾ â†’ Rğ¾ . While not necessarily corresponding to an explicit
regularizer ğ‘…,thisapproachhasledtoimprovedresultsinimagereconstruction,as
shownin[40,44,57].TheconvergenceofthePnP-FBSiterations
sğ‘˜+1 =D(cid:8) sğ‘˜ âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(y,Asğ‘˜)(cid:9)
(53)
canbeguaranteed[21,Proposition15]if
â€¢ thedenoiserDisaveraged,whichmeansthatistakestheformD= ğ›½R+(1âˆ’ğ›½)Id
with ğ›½ âˆˆ (0,1)andanoperatorR:Rğ¾ â†’Rğ¾ suchthatLip(R) â‰¤ 1;
â€¢ the data term ğ½(y,HÂ·) is convex, differentiable with ğ¿-Lipschitz gradient, and
ğ›¼ âˆˆ (0,2/ğ¿).
Moreover, it is possible to prove that the solution(s) of the PnP algorithm satisfies
thepropertiesexpectedofafaithfulreconstruction.Thefirstsuchpropertyisajoint
formofconsistencybetweenthereconstructedimagesâˆ—(outcomeofthealgorithm)
andthemeasurementy(input).
Proposition6 Let sâˆ— and sâˆ— be fixed points of (53) for measurements y and y ,
1 2 1 2
respectively.IftheoperatorDisaveragedwithğ›½ â‰¤ 1/2andğ½(y,As) = 1âˆ¥yâˆ’Asâˆ¥2,
2 2
thenitholdsthat
âˆ¥Asâˆ—âˆ’Asâˆ—âˆ¥ â‰¤ âˆ¥y âˆ’y âˆ¥. (54)
1 2 1 2
Proof. IfDis ğ›½-averagedwith ğ›½ â‰¤ 1/2,then(2Dâˆ’Id)is1-Lipschitzsince
âˆ¥(2Dâˆ’Id){z âˆ’z }âˆ¥ = âˆ¥2ğ›½(R{z }âˆ’R{z }+(1âˆ’2ğ›½)(z âˆ’z )âˆ¥
1 2 1 2 1 2
â‰¤ 2ğ›½âˆ¥R{z }âˆ’R{z }âˆ¥+(1âˆ’2ğ›½)âˆ¥z âˆ’z âˆ¥
1 2 1 2
â‰¤ âˆ¥z âˆ’z âˆ¥, âˆ€z ,z âˆˆRğ¾. (55)
1 2 1 2ParsevalConvolutionOperatorsandNeuralNetworks 27
Usingthisproperty,wegetthat
âˆ¥(2Dâˆ’Id){sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y )}âˆ’(2Dâˆ’Id){sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y )}âˆ¥
1 1 1 2 2 2
â‰¤ âˆ¥(sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))âˆ’(sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))âˆ¥. (56)
1 1 1 2 2 2
Fromthefixed-pointpropertyofsâˆ— andsâˆ—,itfollowsthat
1 2
âˆ¥2(sâˆ—âˆ’sâˆ—)âˆ’(sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))+(sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))âˆ¥
1 2 1 1 1 2 2 2
â‰¤ âˆ¥(xâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))âˆ’(sâˆ—âˆ’ğ›¼âˆ‡âˆ‡âˆ‡ğ½(Asâˆ—,y ))âˆ¥. (57)
1 1 1 2 2 2
Next,weusethefactthatâˆ‡âˆ‡âˆ‡ğ½(As,y) =AT(Asâˆ’y)anddevelopbothsidesas
âŸ¨sâˆ—âˆ’sâˆ—,AT(Asâˆ—âˆ’y )âˆ’AT(Asâˆ—âˆ’y )âŸ© â‰¥ 0. (58)
1 2 2 2 1 1
Finally,wemoveAT totheothersideoftheinnerproductandinvoketheCauchy-
Schwartzinequalitytogetthat
âˆ¥A(sâˆ—âˆ’sâˆ—)âˆ¥âˆ¥y âˆ’y âˆ¥ â‰¥ âŸ¨A(sâˆ—âˆ’sâˆ—),y âˆ’y âŸ© â‰¥ âˆ¥A(sâˆ—âˆ’sâˆ—)âˆ¥2, (59)
1 2 1 2 1 2 1 2 1 2
whichisequivalentto(54). âŠ“âŠ”
WhenAisinvertible,(54)yieldsthedirectrelation
1
âˆ¥sâˆ—âˆ’sâˆ—âˆ¥ â‰¤ âˆ¥y âˆ’y âˆ¥. (60)
1 2 ğœ (ATA) 1 2
min
ItensuresthattheiterativereconstructionalgorithmisitselfgloballyLipschitzstable.
Inotherwords,asmalldeviationoftheinputcanonlyresultinalimiteddeviation
oftheoutput,whichintrinsicallyprovidesprotectionagainsthallucinations.Under
slightlystrongerconstraintsonD,wehaveacomparableresultfornon-invertibleA.
Proposition7 InthesettingofProposition6andforağ¿ -LipschitzdenoiserDwith
0
ğ¿ <1,itholdsthat
0
ğ›¼âˆ¥Aâˆ¥ğ¿
âˆ¥sâˆ—âˆ’sâˆ—âˆ¥ â‰¤ 0âˆ¥y âˆ’y âˆ¥. (61)
1 2 1âˆ’ğ¿ 1 2
0
Proof.
âˆ¥sğ‘˜ âˆ’sğ‘˜âˆ¥ = âˆ¥D{sğ‘˜âˆ’1âˆ’ğ›¼AT(Asğ‘˜âˆ’1âˆ’y )}âˆ’D{sğ‘˜âˆ’1âˆ’ğ›¼AT(Asğ‘˜âˆ’1âˆ’y )}âˆ¥
1 2 1 1 1 2 2 2
â‰¤ ğ¿ âˆ¥(Iâˆ’ğ›¼ATA)(sğ‘˜âˆ’1âˆ’sğ‘˜âˆ’1)âˆ’ğ›¼AT(y âˆ’y )âˆ¥
0 1 2 1 2
â‰¤ ğ¿ âˆ¥sğ‘˜âˆ’1âˆ’sğ‘˜âˆ’1âˆ¥+ğ›¼ğ¿ âˆ¥Aâˆ¥âˆ¥y âˆ’y âˆ¥
0 1 2 0 1 2
â‰¤ ğ¿2âˆ¥sğ‘˜âˆ’2âˆ’sğ‘˜âˆ’2âˆ¥+ğ›¼âˆ¥Aâˆ¥(ğ¿ +ğ¿2)âˆ¥y âˆ’y âˆ¥
0 1 2 0 0 1 2
ğ‘˜
â‰¤ ğ¿ğ‘˜âˆ¥s0âˆ’s0âˆ¥+ğ›¼âˆ¥Aâˆ¥âˆ¥y âˆ’y âˆ¥âˆ‘ï¸ ğ¿ğ‘›. (62)
0 1 2 1 2 0
ğ‘›=128 MichaelUnserandStanislasDucotterd
Takingthelimitğ‘˜ â†’âˆ,wegetthatâˆ¥sâˆ—âˆ’sâˆ—âˆ¥ â‰¤ ğ›¼âˆ¥Aâˆ¥ğ¿ 0âˆ¥y âˆ’y âˆ¥. âŠ“âŠ”
1 2 1âˆ’ğ¿ 0 1 2
Sinceitisformulatedasadatafittingproblem,thereconstruction(50)generally
has better data consistency than the one provided by end-to-end neural-network
frameworksthatdirectlyreconstructsfromy[24,34,55,31].Thoselatterapproaches
arealsoknowntosufferfromstabilityissues[3].Moreimportantly,theyhavebeen
foundtoremoveorhallucinatestructure[38,37],whichisunacceptableindiagnostic
imaging.TheusageofempiricalPnPmethodswithoutstrictLipschitzcontrolwithin
the loop is also subject to caution, as they do not offer any guarantee of stability.
By contrast, the PnP approach (53) with averagedness constraints comes with the
stability bounds (54), (60) and (61). This is a step toward reliable deep-learning-
based image reconstruction as it intrinsically limits the ability of the method to
overfitandtohallucinate.
5.2 LearninganAveragedDenoiserforPnP
Ourapproachtoimproveuponclassicimagereconstructionistolearntheoperator
D= ğ›½R+(1âˆ’ğ›½)Idin(53).Wepretrainitforthebestperformanceinthedenoising
scenario(49).Tothatend,weimposethestructureofthe1-LipLSIoperatorRasan
ğ¿-layer convolutional neural network with all intermediate layers being composed
of the same number (ğ‘) of feature channels. Specifically, by reverting back to the
notationofSection3,wehavethatR:â„“ (Zğ‘‘) â†’â„“ (Zğ‘‘)with
2 2
R=T
Hğ¿
â—¦ğˆğ¿ â—¦T
Hğ¿âˆ’1
â—¦ğˆğ¿âˆ’1â—¦...â—¦T
H2
â—¦ğˆ 2â—¦T H1, (63)
whereT
Hğ‘˜
areLSIoperatorswithmatrix-valuedimpulseresponseHğ‘˜[Â·] andğˆğ‘˜ =
(ğœ ğ‘˜,1,...,ğœ ğ‘˜,ğ‘)arepointwisenonlinearitieswiththesharedactivationprofileğœ ğ‘˜,ğ‘› :
R â†’ Rwithineachfeaturechannel.Asforthedomainandrangeoftheoperators,
we have that T : â„“ (Zğ‘‘) â†’ â„“ğ‘(Zğ‘‘) andğ‘‡ : â„“ğ‘(Zğ‘‘) â†’ â„“ (Zğ‘‘) for the input
H1 2 2 Hğ¿ 2 2
andoutputlayers,whileğ‘‡ :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘)forğ‘˜ =2,...,(ğ¿âˆ’1).Likewise,
Hğ‘˜ 2 2
ğˆğ‘˜ :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘),withtheeffectofthenonlinearlayerbeingdescribedby
2 2
âˆ€ğ’Œ âˆˆZğ‘‘ :
ğˆğ‘˜ï£±ï£´ï£´ï£´ï£²(cid:169) (cid:173)ğ‘¥ 1
. .
.[Â·]
(cid:170)
(cid:174)ï£¼ï£´ï£´ï£´ï£½
[ğ’Œ] =(cid:169) (cid:173)
ğœ ğ‘˜,1(ğ‘¥
. .
.1[ğ’Œ])
(cid:170) (cid:174) (64)
ï£´ï£´ï£´(cid:173)
ğ‘¥
ğ‘[Â·](cid:174)ï£´ï£´ï£´ (cid:173)
ğœ ğ‘˜,ğ‘(ğ‘¥
ğ‘[ğ’Œ])(cid:174)
ï£³(cid:171) (cid:172)ï£¾ (cid:171) (cid:172)
withactivationfunctionsğœ
ğ‘˜,ğ‘›
:Râ†’Rforğ‘›=1,...,ğ‘.
SincetheLipschitzconstantofthecompositionoftwooperatorsisboundedby
theproductoftheirindividualLipschitzconstant,wehavethat
ğ¿
(cid:214)
Lip(R) â‰¤ Lip(T H1) Lip(ğˆğ‘˜)Lip(T Hğ‘˜), (65)
ğ‘˜=2ParsevalConvolutionOperatorsandNeuralNetworks 29
whichmeansthatwecanensurethatLip(R) â‰¤ 1byconstrainingeachğˆğ‘˜ andT
Hğ‘˜
tobe1-Lipschitz.
5.2.1 Specificationof1-LipConvolutionLayers
WeconsidertwowaysofenforcingLip(T ) =1.Botharesupportedbyourtheory.
Hğ‘˜
â€¢ Spectral normalization (SN) [40]: During the learning process, we repeatedly
renormalize the denoising filters Hğ‘˜ by dividing them by their spectral norm
Lip(T Hğ‘˜) = âˆ¥T Hğ‘˜âˆ¥ =ğœ sup,Hğ‘˜ (seeTheorem2).
â€¢ BCOP[30]:TheParsevalfiltersT areparameterizedexplicitlyusingorthogonal
Hğ‘˜
matricesUğ‘˜ âˆˆ Rğ‘Ã—ğ‘ ,asdescribedinSections4.5-4.6.Weusetheimplementa-
tionprovidedbytheBCOPframeworkofLietal.Asforthelastğ‘-to-1multifilter
T ,itisnotliterallyParseval,butrathertheadjointofaParsevaloperator,which
Hğ¿
preservesthe1-Lippropertyaswell.
5.2.2 Specificationof1-LipActivationFunctions
TheLipschitzconstantofanonlinearscalaractivation ğ‘“ :Râ†’Risgivenby
dğ‘“(ğ‘¡)|
Lip(ğ‘“) =sup| |. (66)
ğ‘¡âˆˆR
dğ‘¡
Thisresultcanthenbeappliedtothefullnonlinearlayerğˆğ‘˜ : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘(Zğ‘‘)
2 2
throughthepoolingformula(68).
Proposition8 Let ğ’‡ : â„“ğ‘(Zğ‘‘) â†’ â„“ğ‘(Zğ‘‘) beagenericpointwisenonlinearmap-
2 2
pingspecifiedby
ğ‘“ ğ’Œ,1(ğ‘¥ 1[ğ’Œ])
ğ’‡(cid:8) x[Â·](cid:9) [ğ’Œ] =(cid:169) (cid:173) . . . (cid:170) (cid:174), ğ’Œ âˆˆZğ‘‘ (67)
(cid:173) (cid:174)
ğ‘“ ğ’Œ,ğ‘(ğ‘¥ ğ‘[ğ’Œ])
(cid:171) (cid:172)
Then, ğ’‡ :â„“ğ‘(Zğ‘‘) â†’â„“ğ‘(Zğ‘‘)isLipschitzcontinuousifandonlyifallthecomponent-
2 2
wise transformations ğ‘“ ğ’Œ,ğ‘› : R â†’ R, with (ğ’Œ,ğ‘›) âˆˆ Zğ‘‘ Ã—{1,...,ğ‘} are Lipschitz-
continuous.ItsLipschitzconstantisthengivenby
Lip(ğ’‡) = ğ¿
ğ’‡
= sup Lip(ğ‘“ ğ’Œ,ğ‘›) <âˆ. (68)
(ğ’Œ,ğ‘›)âˆˆZğ‘‘Ã—{1,...,ğ‘}
Proof. Undertheassumptionthatthe ğ‘“ ğ’Œ,ğ‘› areLipschitzcontinuous,foranyx,y âˆˆ
â„“ğ‘(Zğ‘‘)wehavethat
230 MichaelUnserandStanislasDucotterd
ğ‘
âˆ¥ğ’‡{y}âˆ’ ğ’‡{x}âˆ¥ â„“2
ğ‘
=âˆ‘ï¸ âˆ‘ï¸ (cid:12) (cid:12)ğ‘“ ğ’Œ,ğ‘›(ğ‘¦ ğ‘›[ğ’Œ])âˆ’ ğ‘“ ğ’Œ,ğ‘›(ğ‘¥ ğ‘›[ğ’Œ])(cid:12) (cid:12)2
2 ğ‘›=1ğ’ŒâˆˆZğ‘‘
ğ‘
â‰¤ âˆ‘ï¸ âˆ‘ï¸ Lip(ğ‘“ ğ’Œ,ğ‘›)2(cid:12) (cid:12)ğ‘¦ ğ‘›[ğ’Œ]âˆ’ğ‘¥ ğ‘›[ğ’Œ](cid:12) (cid:12)2
ğ‘›=1ğ’ŒâˆˆZğ‘‘
(cid:32) (cid:33)2
â‰¤ sup Lip(ğ‘“ ğ’Œ,ğ‘›) âˆ¥yâˆ’xâˆ¥ â„“2 ğ‘,
(ğ’Œ,ğ‘›)âˆˆZğ‘‘Ã—{1,...,ğ‘} 2
which proves that Lip(ğ’‡) â‰¤ ğ¿ . From the definition of the supremum, for any
ğ’‡
ğœ– > 0, there exists some (ğ’Œ ,ğ‘› ) âˆˆ Zğ‘‘ Ã— {1,...,ğ‘} such that Lip(ğ’‡) â‰¤ ğ¿ â‰¤
0 0 ğ’‡
(1 + ğœ–)ğ¿ 0 with ğ¿ 0 = Lip(ğ‘“ ğ’Œ0,ğ‘› 0). Likewise, since ğ‘“ = ğ‘“ ğ’Œ0,ğ‘› 0 : R â†’ R is ğ¿ 0-
Lipschitz continuous, for any ğœ–â€² > 0 there exist some ğ‘¥,ğ‘¦ âˆˆ R with ğ‘¥ â‰  ğ‘¦ such
that(1+ğœ–â€²)|ğ‘“(ğ‘¦)âˆ’ ğ‘“(ğ‘¥)| â‰¥ ğ¿ |ğ‘¦âˆ’ğ‘¥|.Wethenconsiderthecorresponding(worst-
0
case) signals xËœ = ğ‘¥eğ‘› 0ğ›¿[Â· âˆ’ ğ’Œ 0] and yËœ = ğ‘¦ ğ¿eğ‘› 0ğ›¿[Â· âˆ’ ğ’Œ 0], for which we have that
Lip(ğ’‡)âˆ¥yËœâˆ’xËœâˆ¥
â„“ğ‘
â‰¥ âˆ¥ğ’‡(yËœ)âˆ’ ğ’‡(xËœ)âˆ¥
â„“ğ‘
â‰¥ (1+ğœ–)(ğ’‡ 1+ğœ–â€²)âˆ¥yËœâˆ’xËœâˆ¥ â„“ğ‘.Sinceğœ–â€²andğœ–canbe
chosenarbitrari2 lysmall,theLipschi2 tzboundissharpwithL2 ip(ğ’‡) = ğ¿ .Thesame
ğ’‡
kind of worst-case signals can also be used to show the necessity of the Lipschitz
continuityofeach ğ‘“ ğ’Œ,ğ‘› :Râ†’R. âŠ“âŠ”
Accordingly,inourexperiments,wehaveconsideredtwoconfigurations.
â€¢ Fixedactivationasarectifiedlinearunit(ReLU)withLip(ReLU) = âˆ¥1 +âˆ¥ğ¿
âˆ
=1.
â€¢ Learnable linear spline (LLS) [18], with learned activations ğœ ğ‘˜,ğ‘› : R â†’ R s.t.
Lip(ğœ ğ‘˜,ğ‘›) = 1.Thesenonlinearitiesaresharedwithineachconvolutionchannel
(ğ‘˜,ğ‘›) âˆˆ {2,...,ğ¿}Ã—{1,...,ğ‘}.TheyareparameterizedusinglinearB-splines
subjecttoasecond-ordertotal-variationregularizationthatpromotescontinuous
piecewise-linearsolutionswiththefewestlinearsegments[5,50].
5.2.3 ImageDenoisingExperiments
Wetrain1-Lipdenoiserswithğ¿ =8,ğ‘ =64,andfiltersofsize(3Ã—3).Thetraining
datasetconsistsof238400patchesofsize (40Ã—40) takenfromtheBSD500image
dataset[4].Allnoise-freeimagessin(49)arenormalizedtotakevaluesin [0,1].
TheyarethencorruptedwithadditiveGaussiannoiseofstandarddeviationğœtotrain
the denoiser D for the regression task D{z} â‰ˆ s. The performance on the BSD68
test set is provided in Table 2 for ğœ = 5/255,10/255. The general trend for each
experimental condition is the same: The Parseval filters parameterized by BCOP
consistentlyoutperformthe1-Lipfiltersobtainedbysimplespectralnormalization.
Thereisalsoasystematicbenefitintheutilizationoflearned1-Lipsplineactivations
(LLS),ascomparedtothestandardReLUdesign.ParsevalConvolutionOperatorsandNeuralNetworks 31
Table2:PSNRandSSIMonBSD68fortwonoiselevels.
Noiselevel ğœ=5/255 ğœ=10/255
Metric PSNR SSIM PSNR SSIM
ReLU-SN 35.78 0.9297 31.48 0.8533
ReLU-BCOP 36.10 0.9386 31.92 0.8735
LLS-SN 36.68 0.9504 32.36 0.8883
LLS-BCOP 36.86 0.9546 32.55 0.8962
5.3 NumericalResultsforPnP-FBS
We now demonstrate the deployment of our learned denoisers in the PnP-FBS
algorithmforimagereconstruction.Tothatend,weselectthedata-fidelitytermas
ğ½(y,As) = 1âˆ¥yâˆ’Asâˆ¥2, where the matrix A simulates the physics of biomedical
2 2
imageacquisitions[35].Toensuretheconvergenceof (53),weset ğ›¼ = 1/âˆ¥ATAâˆ¥.
Our denoiser is defined as D = ğ›½R+ (1âˆ’ ğ›½)Id, while the constant ğ›½ âˆˆ [0,1) and
the training noise level ğœ âˆˆ {5/255,10/255} are tuned for best performance. In
ourexperiments,wenoticedthatthebest ğ›½ isalwayslowerthan1/2,whichmeans
thatthemathematicalassumptionsforProposition6aremet.Wealsocompareour
reconstructionalgorithmswiththeclassictotal-variation(TV)method[7].
InourMRIexperiment,thegoalisrecoversfromy=MFs+nâˆˆCğ‘€ ,whereM
isasubsamplingmask(identitymatrixwithsomemissingentries),Fisthediscrete
Fourier-transformmatrix,andnisarealizationofacomplex-valuedGaussiannoise
characterizedbyğœ fortherealandimaginaryparts.Weinvestigatedthree ğ‘˜-space
n
sampling schemes (random, radial, and Cartesian=uniform along the horizontal
direction),eachgivingrisetoaspecificsub-samplingmask.
Thereconstructionperformanceforvariousğ‘˜-spacesamplingconfigurationsand
designchoicesisreportedinTable3.Similarlytothedenoisingexperiment,BCOP
alwaysoutperformsSN,whileLLSbringsadditionalimprovements.OurCNN-based
methodsgenerallyperformbetterthanTV(standardreconstructionalgorithm),while
they essentially offer the same theoretical guarantees (consistency and stability)
[17].TheonlynotableexceptionistheTV-regularizedreconstructionofBrainwith
Cartesiansampling,whichisofbetterqualitythantheoneobtainedwithSN-ReLU.
The results for Brain and Bust with the Cartesian mask are shown in Figures 1
and2,respectively.InthelowerpanelofFigure1,weobservestripe-likestructures
in the zero-fill reconstruction. These are typical aliasing artifacts that result from
thesubsamplinginthehorizontaldirectioninFourierspace.Theyaresignificantly
reducedwiththehelpofTV(whichisroutinelyusedforthatpurpose)aswellasin
theLLS-BCOPreconstruction,whichoverallyieldsthebestvisualquality.32 MichaelUnserandStanislasDucotterd
Ground Truth Zero-filling TV ReLU-SN ReLU-BCOP LLS-SN LLS-BCOP
21.57 24.43 24.14 24.42 25.09 25.18
22.88 27.06 26.01 26.50 27.79 28.07
Fig.1:Groundtruth,zero-fillreconstructionHTy,andPnP-FBSreconstructionusing
severalnetworkparameterizationsontheBrainimagewiththeCartesianmask.
Lowerpanel:zoomofaregionofinterest.TheSNRisevaluatedwithrespecttothe
groundtruth(leftimage)andisoverlaidinwhite.
Ground Truth Zero-filling TV ReLU-SN ReLU-BCOP LLS-SN LLS-BCOP
23.44 27.69 27.77 28.02 28.48 28.86
21.73 26.58 26.31 26.55 27.09 27.61
Fig.2:Groundtruth,zero-fillreconstructionHTy,andPnP-FBSreconstructionusing
severalnetworkparameterizationsontheBustimagewiththeCartesianmask.
Lowerpanel:zoomofaregionofinterest.TheSNRisevaluatedwithrespecttothe
groundtruth(leftimage)andisoverlaidinwhite.
Table3:PSNRandSSIMfortheMRIreconstructionexperiment.
Subsamplingmask Random Radial Cartesian
Imagetype Brain Bust Brain Bust Brain Bust
Zero-filling 24.6827.3123.8525.1321.5723.44
TV 30.3732.2929.4631.5824.4327.69
ReLU-SN 32.4533.3630.9232.3324.1427.77
ReLU-BCOP 32.5333.6730.9332.7224.4228.02
LLS-SN 33.3434.3231.8233.3525.0928.48
LLS-BCOP 33.6134.6732.0933.7225.1828.86ParsevalConvolutionOperatorsandNeuralNetworks 33
6 Conclusion
Inthischapter,wehaveconductedasystematicinvestigationofmultichannelconvo-
lutionoperatorswithaspecialemphasisontheclassofLSIParsevaloperators.What
setstheParsevaloperatorsapartfromstandardfilterbanksistheirlosslessnature(en-
ergyconservation).Thismakesthemultra-stableandparticularlyeasytoinvertby
mere flow-graph transposition of the computational architecture. The other impor-
tantfeatureisthattheParsevalpropertyispreservedthroughcomposition.Formally,
thismeansthattheParsevalfilterbanksforma(non-commutative)operatoralgebra.
Onthemorepracticalside,thisenablestheconstructionofhigher-complexityfilters
through the chaining of elementary parametric modules, as exemplified in Section
4.
ThesepropertiesmakeParsevalfilterbanksespeciallyattractiveforthedesignof
robust(e.g,1-Lip)convolutionalnetworks.Wehavedemonstratedtheapplicationof
such Parseval CNNs for the reconstruction of biomedical images. We have shown
that the use of pre-trained Parseval filterbanks generally improves the quality of
iterative image reconstruction, while it offers the same mathematical guarantees
as the conventional â€œhandcraftedâ€ reconstruction schemes. The training of such
structures is straightforwardâ€”it is done before hand on a basic denoising task.
Furthertopicsofresearchinclude(i)theinvestigationandcomparisonofdifferent
factorizationschemeswiththeviewofidentifyingthemosteffectiveones,and(ii)
the determination of the performance limits of CNN-based approaches under the
mathematicalconstraintofstability/trustworthiness.
References
1. A. Aldroubi. Portraits of frames. Proceedings of the American Mathematical Society,
123(6):1661â€“1668,1995.
2. C.Anil,J.Lucas,andR.Grosse.SortingoutLipschitzfunctionapproximation.InProceedings
ofthe36thInternationalConferenceonMachineLearning,pages291â€“301.PMLR,May2019.
3. V.Antun,F.Renna,C.Poon,B.Adcock,andA.C.Hansen. Oninstabilitiesofdeeplearning
inimagereconstructionandthepotentialcostsofAI.ProceedingsoftheNationalAcademyof
Sciences,117(48):30088â€“30095,May2020.
4. P.ArbelÃ¡ez,M.Maire,C.Fowlkes,andJ.Malik. Contourdetectionandhierarchicalimage
segmentation. IEEETransactionsonPatternAnalysisandMachineIntelligence,33(5):898â€“
916,2011.
5. P.Bohra,J.Campos,H.Gupta,S.Aziznejad,andM.Unser. Learningactivationfunctions
indeep(spline)neuralnetworks. IEEEOpenJournalofSignalProcessing,1:295â€“309,Nov.
2020.
6. H.Bolcskei,F.Hlawatsch,andH.Feichtinger. Frame-theoreticanalysisofoversampledfilter
banks. IEEETransactionsonSignalProcessing,46:3256â€“3268,1998.
7. A.Chambolle. Analgorithmfortotalvariationminimizationandapplications. Journalof
MathematicalImagingandVision,20(1-2):89â€“97,2004.
8. S. H. Chan, X. Wang, and O. A. Elgendy. Plug-and-play ADMM for image restoration:
Fixed-point convergence and applications. IEEE Transactions on Computational Imaging,
3(1):84â€“98,2016.34 MichaelUnserandStanislasDucotterd
9. A.ChebiraandJ.Kovacevic. Lappedtightframetransforms. InProc.IEEEInternational
ConferenceonAcoustics,SpeechandSignalProcessing,volume3,pages857â€“860,Honolulu,
HI,USA,2007.
10. O.Christensen.Framesandpseudo-inverses.JournalofMathematicalAnalysisandApplica-
tions,195(2):401â€“414,1995.
11. O.Christensen. AnIntroductiontoFramesandRieszBases. Birkhauser,2003.
12. P.G.Ciarlet.LinearandNonlinearFunctionalAnalysiswithApplications,volume130.SIAM,
2013.
13. M.Cisse,P.Bojanowski,E.Grave,Y.Dauphin,andN.Usunier.Parsevalnetworks:Improving
robustnesstoadversarialexamples. InProceedingsofthe34thInternationalConferenceon
MachineLearning,pages854â€“863.PMLR,July2017.
14. P.CombettesandV.Wajs.Signalrecoverybyproximalforward-backwardsplitting.Multiscale
Modeling&Simulation,4:1168â€“1200,2005.
15. Z.CvetkovicandM.Vetterli. Oversampledfilterbanks. IEEETransactionsonSignalPro-
cessing,46(5):1245â€“1255,May1998.
16. I.Daubechies. TenLecturesonWavelets. SocietyforIndustrialandAppliedMathematics,
Philadelphia,PA,1992.
17. P.delAguilaPla,S.Neumayer,andM.Unser. Stabilityofimage-reconstructionalgorithms.
IEEETransactionsonComputationalImaging,9:1â€“12,2023.
18. S. Ducotterd, A. Goujon, P. Bohra, D. Perdios, S. Neumayer, and M. Unser. Improving
Lipschitz-constrainedneuralnetworksbylearningactivationfunctions. JournalofMachine
LearningResearch,25(65):1â€“30,2024.
19. X. Gao, T. Nguyen, and G. Strang. On factorization of M-channel paraunitary filterbanks.
IEEETransactionsonSignalProcessing,49(7):1433â€“1446,July2001.
20. M.Hasannasab,J.Hertrich,S.Neumayer,G.Plonka,S.Setzer,andG.Steidl.Parsevalproximal
neuralnetworks.JournalofFourierAnalysisandApplications,26(4):PaperNo.59,31,2020.
21. J.Hertrich,S.Neumayer,andG.Steidl. ConvolutionalproximalneuralnetworksandPlug-
and-Playalgorithms. LinearAlgebraandItsApplications,631:203â€“234,2021.
22. L.Huang,L.Liu,F.Zhu,D.Wan,Z.Yuan,B.Li,andL.Shao.Controllableorthogonalization
intrainingDNNs.In2020IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages6428â€“6437,Seattle,WA,USA,June2020.IEEE.
23. T.Huster,C.-Y.J.Chiang,andR.Chadha. LimitationsoftheLipschitzconstantasadefense
againstadversarialexamples. InECMLPKDD2018Workshops,LectureNotesinComputer
Science,pages16â€“29,Cham,2019.SpringerInternationalPublishing.
24. K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep convolutional neural network
forinverseproblemsinimaging. IEEETransactionsonImageProcessing,26(9):4509â€“4522,
Sept.2017.
25. T.Kailath. LinearSystems,volume156. Prentice-HallEnglewoodCliffs,NJ,1980.
26. U.S.Kamilov,C.A.Bouman,G.T.Buzzard,andB.Wohlberg. Plug-and-playmethodsfor
integratingphysicalandlearnedmodelsincomputationalimaging:Theory,algorithms,and
applications. IEEESignalProcessingMagazine,40(1):85â€“97,Jan.2023.
27. J.KovacevicandA.Chebira. Anintroductiontoframes. FoundationsandTrendsinSignal
Processing,2(1):1â€“94,2007.
28. J.KovacevicandA.Chebira. Lifebeyondbases:Theadventofframes(PartI). IEEESignal
ProcessingMagazine,24:86â€“104,2007.
29. J.KovacevicandA.Chebira. Lifebeyondbases:Theadventofframes(PartII). IEEESignal
ProcessingMagazine,24:115â€“125,2007.
30. Q. Li, S. Haque, C. Anil, J. Lucas, R. Grosse, and J.-H. Jacobsen. Preventing gradient
attenuationinLipschitzconstrainedconvolutionalnetworks. AdvancesinNeuralInformation
ProcessingSystems,32:15390â€“15402,Dec.2019.
31. D.J.Lin,P.M.Johnson,F.Knoll,andY.W.Lui. ArtificialintelligenceforMRimagerecon-
struction:anoverviewforclinicians. JournalofMagneticResonanceImaging,53(4):1015â€“
1028,2021.
32. F.Luisier,T.Blu,andM.Unser. ImagedenoisinginmixedPoisson-Gaussiannoise. IEEE
TransactionsonImageProcessing,20(3):696â€“708,Mar.2011.ParsevalConvolutionOperatorsandNeuralNetworks 35
33. S.Mallat. AWaveletTourofSignalProcessing. AcademicPress,SanDiego,1998.
34. M.McCann,K.Jin,andM.Unser. Convolutionalneuralnetworksforinverseproblemsin
imagingâ€”areview. IEEESignalProcessingMagazine,34(6):85â€“95,Nov.2017.
35. M.McCannandM.Unser. Biomedicalimagereconstruction:Fromthefoundationstodeep
neuralnetworks. FoundationsandTrendsinSignalProcessing,13(3):280â€“359,Dec.2019.
36. Y.Meyer. OndelettesetopÃ©rateursI:Ondelettes. Hermann,Paris,France,1990.
37. M.J.Muckley,B.Riemenschneider,A.Radmanesh,S.Kim,G.Jeong,J.Ko,Y.Jun,H.Shin,
D.Hwang,M.Mostapha,S.Arberet,D.Nickel,Z.Ramzi,P.Ciuciu,J.-L.Starck,J.Teuwen,
D. Karkalousos, C. Zhang, A. Sriram, Z. Huang, N. Yakubova, Y. W. Lui, and F. Knoll.
Resultsofthe2020fastMRIchallengeformachinelearningMRimagereconstruction. IEEE
TransactionsonMedicalImaging,40(9):2306â€“2317,Sept.2021.
38. G.NatarajandR.Otazo.Model-freedeepMRIreconstruction:Arobustnessstudy.InISMRM
WorkshoponDataSamplingandImage,2020.
39. A.V.Oppenheim,R.W.Schafer,andJ.R.Buck. Discrete-timeSignalProcessing. Prentice
Hall,UpperSaddleRiver,2ndedition,1999.
40. E.Ryu,J.Liu,S.Wang,X.Chen,Z.Wang,andW.Yin. Plug-and-playmethodsprovably
convergewithproperlytraineddenoisers. InInternationalConferenceonMachineLearning,
pages5546â€“5557.PMLR,2019.
41. A. Soman, P. Vaidyanathan, and T. Nguyen. Linear phase paraunitary filter banks: theory,
factorizations and designs. IEEE Transactions on Signal Processing, 41(12):3480â€“3496,
1993.
42. G.StrangandT.Nguyen. WaveletsandFilterBanks. Wellesley-Cambridge,Wellesley,MA,
1996.
43. J. Su, W. Byeon, and F. Huang. Scaling-up diverse orthogonal convolutional networks by
aparaunitaryframework. InProceedingsofthe39thInternationalConferenceonMachine
Learning,pages20546â€“20579.PMLR,June2022. ISSN:2640-3498.
44. Y.Sun,Z.Wu,X.Xu,B.Wohlberg,andU.S.Kamilov. Scalableplug-and-playADMMwith
convergenceguarantees. IEEETransactionsonComputationalImaging,7:849â€“863,2021.
45. T.Tran,R.deQueiroz,andT.Nguyen.Linear-phaseperfectreconstructionfilterbank:Lattice
structure,design,andapplicationinimagecoding. IEEETransactionsonSignalProcessing,
48:133â€“147,2000.
46. A.TrockmanandJ.Z.Kolter.OrthogonalizingconvolutionallayerswiththeCayleytransform.
InICLR,May2021.
47. R. TurcajovÃ¡. Factorizations and construction of linear phase paraunitary filter banks and
highermultiplicitywavelets. NumericalAlgorithms,8(1):1â€“25,1994.
48. R.TurcajovÃ¡andJ.Kautsky.Shiftproductsandfactorizationsofwaveletmatrices.Numerical
Algorithms,8(1):27â€“45,1994.
49. M.Unser. Textureclassificationandsegmentationusingwaveletframes. IEEETransactions
onImageProcessing,4(11):1549â€“1560,1995.
50. M.Unser. Arepresentertheoremfordeepneuralnetworks. JournalofMachineLearning
Research,20(110):1â€“30,2019.
51. P.P.Vaidyanathan. MultirateSystemsandFilterBanks. Prentice-Hall,EnglewoodCliffs,NJ,
1993.
52. S.V.Venkatakrishnan,C.A.Bouman,andB.Wohlberg.Plug-and-playpriorsformodelbased
reconstruction.In2013IEEEGlobalConferenceonSignalandInformationProcessing,pages
945â€“948,2013.
53. M.VetterliandJ.Kovacevic.WaveletsandSubbandCoding.PrenticeHall,EnglewoodCliffs,
NJ,1995.
54. M.Vetterli,J.KovaÄeviÄ‡,andV.K.Goyal. FoundationsofSignalProcessing. Cambridge
UniversityPress,Cambridge,UK,2014.
55. G. Wang, J. C. Ye, and B. De Man. Deep learning for tomographic image reconstruction.
NatureMachineIntelligence,2(12):737â€“748,Dec.2020.
56. L.Xiao,Y.Bahri,J.Sohl-Dickstein,S.Schoenholz,andJ.Pennington. Dynamicalisometry
and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural
networks. InProceedingsofthe35thInternationalConferenceonMachineLearning,pages
5393â€“5402.PMLR,July2018.36 MichaelUnserandStanislasDucotterd
57. D.H.Ye,S.Srivastava,J.-B.Thibault,K.Sauer,andC.Bouman. Deepresiduallearningfor
model-basediterativeCTreconstructionusingPlug-and-Playframework. InIEEEInterna-
tionalConferenceonAcoustics,SpeechandSignalProcessing,pages6668â€“6672,2018.
58. K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang. BeyondaGaussiandenoiser:Resid-
ual learning of deep CNN for image denoising. IEEE Transactions on Image Processing,
26(7):3142â€“3155,2017.
59. D.Zou,R.Balan,andM.Singh.OnLipschitzboundsofgeneralconvolutionalneuralnetworks.
IEEETransactionsonInformationTheory,66(3):1738â€“1759,2019.