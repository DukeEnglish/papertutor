Computers&OperationsResearch135(2021)105456
ContentslistsavailableatScienceDirect
ComputersandOperationsResearch
journalhomepage:www.elsevier.com/locate/cor
VariableselectionforNaÃ¯veBayesclassification
RafaelBlanqueroa,c,EmilioCarrizosaa,c,PepaRamÃ­rez-Cobob,c,
M.RemediosSillero-Denamiela,c,âˆ—
aDepartamentodeEstadÃ­sticaeInvestigaciÃ³nOperativa,UniversidaddeSevilla,Seville,Spain
bDepartamentodeEstadÃ­sticaeInvestigaciÃ³nOperativa,UniversidaddeCÃ¡diz,CÃ¡diz,Spain
cIMUS,InstitutodeMatemÃ¡ticasdelaUniversidaddeSevilla,Seville,Spain
A R T I C L E I N F O A B S T R A C T
MSC: TheNaÃ¯veBayeshasproventobeatractableandefficientmethodforclassificationinmultivariateanalysis.
62H30 However, features are usually correlated, a fact that violates the NaÃ¯ve Bayesâ€™ assumption of conditional
62H20 independence,andmaydeterioratethemethodâ€™sperformance.Moreover,datasetsareoftencharacterizedby
Keywords: alargenumberoffeatures,whichmaycomplicatetheinterpretationoftheresultsaswellasslowdownthe
Clustering methodâ€™sexecution.
Conditionalindependence In this paper we propose a sparse version of the NaÃ¯ve Bayes classifier that is characterized by three
Dependencemeasures properties.First,thesparsityisachievedtakingintoaccountthecorrelationstructureofthecovariates.Second,
Heuristics
differentperformancemeasurescanbeusedtoguidetheselectionoffeatures.Third,performanceconstraints
Probabilisticclassification
ongroupsofhigherinterestcanbeincluded.Ourproposalleadstoasmartsearch,whichyieldscompetitive
Cost-sensitiveclassification
running times, whereas the flexibility in terms of performance measure for classification is integrated. Our
findingsshowthat,whencomparedagainstwell-referencedfeatureselectionapproaches,theproposedsparse
NaÃ¯veBayesobtainscompetitiveresultsregardingaccuracy,sparsityandrunningtimesforbalanceddatasets.
Inthecaseofdatasetswithunbalanced(orwithdifferentimportance)classes,abettercompromisebetween
classificationratesforthedifferentclassesisachieved.
1. Introduction othermultivariatecontextssuchasregression(Caietal.,2009;Carri-
zosaetal.,2021;Linetal.,2011),clustering(BenatiandGarcÃ­a,2014;
Among the assortment of current classification techniques, the Maldonado et al., 2015), time series analysis (Blanquero et al., 2020;
NaÃ¯veBayes(NB)classifierhasplayedaprominentrolebecauseofits Carrizosaetal.,2017)orvisualization(CarrizosaandGuerrero,2014),
simplicity,tractabilityandefficiency,seeHandandYu(2001).Theim- hasledtothedevelopmentofsparsemultivariatetechniques,seeHastie
plicitassumptionofindependentfeaturesconditionedtotheclasseases etal.(2015).Sparsityinclassificationiscloselylinkedtotheconcepts
theNBimplementationsignificantly,sinceitallowsthedecomposition ofVariableSelectionandFeatureSelection(Carrizosaetal.,2016;George
ofasamplelikelihoodintoaproductofunivariatemarginals.Inaddi- andMcCulloch,1993;Linetal.,2011;ZouandHastie,2005),whose
tion,theNBusuallyestimatesfewerparametersthanotherrenowned
aimistoidentifytherelevantvariableswithinasetofmanypredictors
classifiers, so it is less prone to overfitting (Domingos and Pazzani,
sothatclassificationaccuracyisnotreduced.
1997;HandandYu,2001).Asaconsequence,anumberofapplications
Inthispaper,weproposeanalternativesparsemethodfordatabases
oftheNBinrealcontextscanbefound,forexample,inmedicine(Wolf-
withdependentfeatures.Inparticular,weembedavariablereduction
son et al., 2015), genetics (Minnier et al., 2015), reliability (Turhan
algorithm within the NBâ€™s scheme to produce a sparse version of the
and Bener, 2009), risk (Minnier et al., 2015) or document analy-
classifier.Ouraimistwo-fold:ononehand,sparsityispursuedinthe
sis(Guanetal.,2014),amongothers.Nowadays,datasetsareusually
sensethatonlyasubsetofpredictivefeaturesisusedbytheclassifierâ€™s
characterized by a large number of features and, although such high
dimensionality does not represent a major computational drawback construction,makingtheso-obtainedclassifiermoreinterpretable,and,
whenrunningtheNB,itmayhavenegativeconsequencesintermsof ontheotherhand,wehaveaflexibleframeworktochosetheaccuracy
thecomprehensibilityofitssolutions(CarrizosaandRomeroMorales, measure to be optimized so that the classifierâ€™s performance does not
2013). The search for more interpretable solutions, also common in worsenwithrespecttotheclassicNB.
âˆ— Correspondingauthor.
E-mailaddress: rsillero@us.es(M.R.Sillero-Denamiel).
https://doi.org/10.1016/j.cor.2021.105456
Received16June2020;Receivedinrevisedform5April2021;Accepted28June2021
Availableonline2July2021
0305-0548/Â© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license
(http://creativecommons.org/licenses/by-nc-nd/4.0/).R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Someworkshaveaddresseddifferentstrategiesforvariablereduc- Zhangetal.(2020),twoclass-specificattributeweightedNaÃ¯veBayes
tion for the NB. For example, Feng et al. (2015) and McCallum and versionsaredefined.
Nigam(1998)basetheirfeatureselectionapproachesontheunivariate Notonlyourmethodestablishesthesparsityintermsofthecorre-
correlations between features and the class. In this sense, Chen et al. lationamongthecovariatesandisflexiblesothatthemostconvenient
(2020), Tang et al. (2016a) and Tang et al. (2016b) aim to rank the classification measure can be used, but also it is a cost-sensitive clas-
featuresaccordingtotheircapacityforclassificationoraspecificfea- sifier. When dealing with real-world applications where there exist
tureselectioncriterion.InZhangetal.(2009),theuseoftheprincipal groups at risk (as it happens in medical contexts, risk management,
componentstechniqueandgeneticalgorithmstoremoveirrelevantand creditcardfrauddetectionorwhenfairclassificationisarequirement
redundantfeaturesareexamined. TheEvolutionalNaÃ¯veBayes(Jiang asasocialcriterion),cost-sensitivelearningapproachesthatassigndif-
etal.,2005)isawrapperwhichalsoperformsageneticsearchtoselect ferentimportancetothedifferentgroupsshouldbeconsidered(Leevy
asubsetfromthewholeset,althoughitissensitivetomanyparameters, etal.,2018).Moreover,thesemethodsturnouttobeveryconvenient
which is disadvantageous in practice. Other studies which are also for unbalanced datasets, where the minority class may be the worst
focused on hard variable selection approaches to reduce the number classified (and the most critical one). In particular, the inclusion of
of redundant predictors are Bermejo et al. (2014) and (Mukherjee constraintsontheproportionsofcorrectlyclassifiedinstancesofsuch
and Sharma, 2012). In this sense, Langley and Sage (1994) define groupsmaybeconvenientforhavingdirectcontrolovertheirmisclas-
theselectiveNaÃ¯veBayes(SNB)classifier,whichisbasedonawrapper sificationratesandobtainingadequateresultsforthem(BenÃ­tez-PeÃ±a
approach(KohaviandJohn,1997).However,duetothecomplexityof et al., 2019; Blanquero et al., 2021a,b). That is, whereas the global
theinvolvedsearchalgorithmanditstendencytomakeoverfitting,the performance criterion is optimized, further control can be added via
SNBdoesnotperformwellonlargedatasets(BoullÃ©,2007).Therefore, performance constraints on the groups of interest in each case. As it
aBayesianapproachâ€“definedasSNB(MAP)â€“isconsideredinBoullÃ© willbedetailed,thesparseNBdefinedinthisworkisabletointegrate
(2007) to improve the performance of the SNB so that a compromise suchperformanceconstraints.
betweentheperformanceoftheclassifierandthesparsityisfound.An- This paper is organized as follows. In Section 2, a brief review
otherexamplecanbefoundinâ€˜â€˜annâ€™â€™RatanamahatanaandGunopulos of the NB is done, the notation is introduced, and some performance
(2003),whichproposesamethodthatcombinesNBanddecisiontrees. measures typically used in classification are reviewed. A numerical
However,aspointedoutbyBoullÃ©(2007),itisimportanttoâ€˜â€˜exploit examplemotivatingourapproachforasparseNBispresentednext.In
multivariatepreprocessingmethodsinordertocircumventtheNaÃ¯veBayes Section3,theproposedversionofsparseNBisdescribed.Section4il-
assumptionâ€™â€™. In this paper, we adopt this scheme and propose a hard lustratesthenewsparseclassifier.Syntheticdatasetsaswellastenwell
variableselectionprocesswhichismotivatedbytheconditionalindepen- documentedrealdatabaseswithdifferentpropertieswillbethoroughly
denceassumptionoftheNB.ItisknownthattheNBisBayes-optimal analyzed, considering different performance measures and/or adding
(that is, it guarantees the minimum classification error), when the performance constraints in groups of interest. A complete discussion
predictorsareindependentconditionedtotheclass(Kuncheva,2006). concerningtheperformanceresults,sparsityandrunningtimesofthe
On the other hand, it is also well documented in the literature that proposedmethodologyincomparisonwithbenchmarkapproacheswill
conditionalindependenceisasufficientconditionbutnotnecessaryto be given. Finally, some conclusions to this work and further related
gettheoptimalNB(DomingosandPazzani,1997;HandandYu,2001; research are described in Section 5. Further information concerning
Hastie et al., 2001). Even if the fact that features are conditionally thepropertiesoftheconsidereddatasetsandthechoiceofthetuning
independentmightnotmakeasignificantdifferencewithrespecttothe parameterswillbedescribedattheSupplementaryMaterial.
situationwherefeaturesarecorrelated,suchslightdifferenceintheNB
performance may be crucial for some real contexts (cancer diagnosis,
2. Preliminaries
for example). The sparse version of the NB proposed in this paper,
which is suitable for dealing with correlated patterns in datasets, is
2.1. TheNaÃ¯veBayesclassifierandperformancemeasures
obtained by integrating a variable reduction method in such a way
that only certain combinations of features, chosen according to their (
Consideraclassificationproblemwithasetofð‘features ð‘‹ ,â€¦,
degree of dependence, are considered. Other papers have considered ) 1
ð‘‹ and ð¾ possible classes. Given a new observation ð± = (ð‘¥ ,â€¦,ð‘¥ ),
before correlations among the features as is the case of Hall (2000), ð‘ 1 ð‘
the aim is to assign ð± to one of the ð¾ classes. The NB computes the
Jiangetal.(2019)andRezaeietal.(2018).ThefilterCorrelationbased
conditionalprobabilitiesð‘(ð¶ âˆ£ð±)forð‘˜=1,â€¦,ð¾ andð± isassignedto
FeatureSelection(CFS)(Hall,2000)isbasedontheassumptionthata ð‘˜
theclassð‘¦Ì‚âˆˆ{1,â€¦,ð¾}satisfying
goodsubsetofattributesshouldbehighlycorrelatedwiththeresponse
variable but, on the other hand, there should exist few dependencies ð‘¦Ì‚= argmax ð‘(ð¶ âˆ£ð±).
ð‘˜
among them. This hypothesis is also used in Jiang et al. (2019), ð‘˜âˆˆ{1,â€¦,ð¾}
whereacorrelation-basedfeatureweightingfilterforNBisdeveloped. Thecomputationofð‘(ð¶ âˆ£ð±)maybecumbersomeifthenumberof
ð‘˜
InRezaeietal.(2018),clusteringisusedtodetectgroupsofcorrelated features ð‘ is large. However, the use of the Bayes theorem eases the
featuresandselectonlyasmallnumberofattributes.Inparticular,the previouscomputationsince
optimalnumberofclustersstemsfromthemeansilhouettescore,which
ðœ‹(ð¶ )ð‘(ð±âˆ£ð¶ )
measureshowsimilaravariableistoitsownclustercomparedtoother ð‘(ð¶ ð‘˜âˆ£ð±)= ð‘˜
ð‘(ð±)
ð‘˜ ,
clusters.
Additionally,thenovelstrategycanbeimplementedusingthemost where ðœ‹(ð¶ ð‘˜) is the prior distribution for the class, ð‘(ð± âˆ£ ð¶ ð‘˜) is the
adequate performance measure given the properties of the datasets.
likelihoodfunctionofthedataandð‘(ð±)istheso-calledevidence.Since
Minimizingtheoverallmisclassificationrateisalwaysanoption,but, theevidenceisthesameforalltheclasses,inpractice,theinterestis
forexample,ifdatasetsareunbalanced,theAUC(areaundertheROC incomputingthenumerator.
curve) may be preferred, since it is sensitive to class imbalance and, ThekeyassumptionoftheNBisthe independenceofthefeatures
therefore, achieves a better compromise among the correct classifi- conditionedtotheclass,whichimpliesthat
cation rates for the different classes. Recent works have considered
different alternative performance measures, Jiang et al. (2012, 2019)
ð‘(ð±|ð¶ ð‘˜)=ð‘(ð‘¥ 1,â€¦,ð‘¥ ð‘|ð¶ ð‘˜)=ð‘(ð‘¥ 1âˆ£ð¶ ð‘˜)â€¦ð‘(ð‘¥ ð‘âˆ£ð¶ ð‘˜) (1)
and Zhang et al. (2020). For instance, the Randomly Selected NaÃ¯ve and therefore, the probabilities of interest ð‘(ð¶ âˆ£ ð±) are computed in
ð‘˜
Bayes(Jiangetal.,2012)considerstheclassificationaccuracy(ACC), a straightforward manner as proportional to (1). Note that, in (1), a
AUCorconditionalloglikelihood;whereasinJiangetal.(2019)and probabilitydistributionforthefeaturesconditionedtotheclassð‘‹ âˆ£ð¶
ð‘– ð‘˜
2R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
needstobechosenbytheuserandestimatedbysomestatisticalmethod 3.1. Descriptionofthemethod
as,forexample,amaximumlikelihoodcriterion.
Severalmeasurescanbeusedtostudyaclassifierâ€™sperformance,see The variable reduction strategy proposed in this section is based
for example (Sokolova and Lapalme, 2009). In real contexts, besides on a clustering of features made in terms of their dependencies. As
commented in Section 2.1, the key assumption of the NaÃ¯ve Bayes is
good overall classification rates, high classification rates for specific
the independence of the features conditioned to the class. The novel
classesmaybesought.Forthisreason,throughoutthiswork,weshall
method presented in this work aims to preserve the independence
considertheclassicRecallofeachclassð‘˜(Recall )forð‘˜=1,â€¦,ð¾,and
ð‘˜ assumptionwithoutdamagingthepredictivepoweroftheclassicNB.
also,theaccuracy(ACC)andtheprecision,whicharedefinedasfollows,
In other words, our methodology helps to select variables that are
(ð‘‡ð‘Ÿð‘¢ð‘’ð¶ð‘™ð‘Žð‘ ð‘ ð‘˜)Ã—100 as independent as possible while provides good classification accu-
ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ = , (2)
ð‘˜ ð‘ð‘¢ð‘šð‘ð‘’ð‘Ÿð‘œð‘“ ð‘–ð‘›ð‘‘ð‘–ð‘£ð‘–ð‘‘ð‘¢ð‘Žð‘™ð‘ ð‘–ð‘›ð‘ð‘™ð‘Žð‘ ð‘ ð‘˜ racy.Todothat,weconsideradependencemeasurebetweenrandom
(âˆ‘ ð‘‡ð‘Ÿð‘¢ð‘’ð¶ð‘™ð‘Žð‘ ð‘ ð‘˜)
Ã—100 variables ð‘‹ and ð‘Œ, which increases with the degree of dependence
ACC= ð‘˜ , (3)
ð‘‡ð‘œð‘¡ð‘Žð‘™ð‘›ð‘¢ð‘šð‘ð‘’ð‘Ÿð‘œð‘“ ð‘–ð‘›ð‘‘ð‘–ð‘£ð‘–ð‘‘ð‘¢ð‘Žð‘™ð‘  between the variables. First, consider for ð‘–,ð‘— âˆˆ {1,â€¦,ð‘} and ð‘˜ =
ð‘ð‘Ÿð‘’ð‘ð‘–ð‘ ð‘–ð‘œð‘› =
ð‘‡ð‘Ÿð‘¢ð‘’ð¶ð‘™ð‘Žð‘ ð‘ ð‘˜
, (4)
1,â€¦,ð¾,thedependencebetweenfeatureð‘‹ ð‘–andfeatureð‘‹ ð‘—conditioned
ð‘˜ (ð‘‡ð‘Ÿð‘¢ð‘’ð¶ð‘™ð‘Žð‘ ð‘ ð‘˜)+(ð¹ð‘Žð‘™ð‘ ð‘’ð¶ð‘™ð‘Žð‘ ð‘ ð‘˜) on class ð¶ ð‘˜. In order to have a unique, summarized measure of de-
pendence between ð‘‹ and ð‘‹ , let ð‘€ be the matrix whose elements
aswellastheAUC. ð‘– ð‘—
(ð‘€(ð‘–,ð‘—)) represent the maximum dependence among all classes, be-
tween ð‘‹ and ð‘‹ . Note that such a choice represents the worst case
ð‘– ð‘—
scenario.Anumberofdependencemeasuresproposedintheliterature
2.2. Theindependenceassumption:anumericalexample
canbeselected:Pearsoncorrelationcoefficient,Spearmanâ€™srank-order
correlation coefficient, Hoeffding ð· statistic (see Hoeffding, 1948),
The effect of the independence assumption over the performance the mutual information coefficient (MI) (Linfoot, 1957), the Maximal
of the NB when correlated features are analyzed, has been studied Information coefficient (MIC) (Reshef et al., 2011) or the distance
in the literature, see Domingos and Pazzani (1996, 1997), Hand and correlation coefficient (SzÃ©kely et al., 2007), among others. We tried
Yu (2001), Hastie et al. (2001) and Zhang (2004). As commented using these different measures and similar results were obtained (see
in Section 1, the conclusion is that, even though the independence Section4andtheSupplementaryMaterial).Therefore,sincethemutual
assumption is not satisfied, the classifierâ€™s performance may not be information measure enables us to work with both continuous and
considerably altered.However, using justa properlychosen subsetof categoricalvariablesandhasbeenwidelyusedintheliterature(Kinney
the variables may make the independence assumption less violated, et al., 2010; Sharpee et al., 2004), we will select this measure. This
coefficient quantifies the information about one variable ð‘‹ provided
andtheaccuracyimproved(ontopofthefactthatamodelwithless
byadifferentvariableð‘Œ,anditisdefinedas
variablesismoreexplainable).
( )
Inordertoillustratehowtheviolationoftheindependenceassump- ð‘(ð‘¥,ð‘¦)
ð¼(ð‘‹,ð‘Œ)= ð‘(ð‘¥,ð‘¦)log ð‘‘ð‘¥ð‘‘ð‘¦,
tionmayaffecttheperformanceoftheNB,considerthenextnumerical âˆ« ð‘Œ âˆ« ð‘‹ ð‘(ð‘¥)ð‘(ð‘¦)
example.Asampleofsize2000ofarandomvector(ð‘‹ 1,ð‘‹ 2,ð‘‹ 3,ð‘‹ 4)is inthecaseofcontinuousvariables.Inthecategoricalcase,theprevious
simulated for two classes from a multivariate Normal distribution in formula can be rewritten in terms of sums. The previous dependence
suchawaythattherandomvariablesareindependentconditionedto measure can be computed by the function mutinformation from
the classes except for ð‘‹ and ð‘‹ which are correlated according to the infotheo package of the Statistical software environment R (R
1 2
a Pearson coefficient of 0.95. A Gaussian NB classifier (that is, ð‘‹ âˆ£ CoreTeam,2017).Anillustrationofthematrixð‘€ fortherealdataset
( ) ð‘–
ð¶ âˆ¼ð‘ ðœ‡ ,ðœŽ2 forð‘–=1,2,3,4,ð‘˜=1,2,whereðœ‡ wererandomly Statlog (Australian Credit Approval) from the UCI Machine Learning
ð‘˜ ð‘–,ð‘˜ ð‘–,ð‘˜ ð‘–,ð‘˜
selected in the interval [1,7]) was run using all possible subsets of Repository(Lichman,2013)isrepresentedbyFig.1.Thedatasetcon-
cernscreditcardapplications,andisformedby14variablesandtwo
features and the results are shown in Table 1. The accuracy when all
classes (+/-). Moreover, to visualize the different correlation patterns
thevariablesareusedisequalto78.28,avaluethatisimprovedifthe
of the real-life datasets used throughout this work, in Section 3 of
set{ð‘‹ ,ð‘‹ ,ð‘‹ }isconsidered(accuracyequalto79.94).
1 3 4 the Supplementary Material, the associated matrices ð‘€ using the MI
Having illustrated that using just a subset of the features may
measurearerepresentedviaheatmaps.
improve accuracy, we face the combinatorial problem of finding the
Next,withtheaimofperformingaclusteranalysisintermsofthe
adequatesetoffeaturestobeused.Thepreviousbruteforceprocedure, degree of association among the features, a dissimilarity matrix ð» of
where all possible combinations of features are examined, turns out dimensionð‘Ã—ð‘isdefinedintermsofmatrixð‘€ bytheelements
infeasible in practice, especially for large databases. Instead, in this
ð‘€âˆ—âˆ’ð‘€(ð‘–,ð‘—)
paper we propose a variable reduction method in which only certain ð»(ð‘–,ð‘—)= , (5)
ð‘€âˆ—
combinations of features are sampled and evaluated. Such combina-
where
tions,aswillbeseeninSection3,shallbechosenbyconsideringthe
dependenciesamongthefeatures. ð‘€âˆ—= max ð‘€(ð‘–,ð‘—).
ð‘–,ð‘—âˆˆ{1,â€¦,ð‘}
Notethat,underthepreviousdefinition,theelementsofð»arebounded
3. AsparseNaÃ¯veBayes below by zero, where this value represents the maximum degree of
dependence.Moreover,theupper-boundoftheelementsofð» isone,
which represents the minimum degree of dependence. Therefore, the
AscommentedinSection2.2,consideringallpossiblecombinations
higherthevaluesofð»(ð‘–,ð‘—)are,thelessdependenceexistsbetweenð‘‹
ð‘–
of features to determine the best one is hard from a computational andð‘‹ ,accordingtotheselecteddependencemeasure.
ð‘—
point of view, especially for large datasets since a total of 2ð‘âˆ’1 sets Notethat,asdescribedinSection1oftheSupplementaryMaterial,
shouldbeevaluated.Theaimofthissectionistodescribeanefficient theresultsobtainedareratherrobustregardingthedependencemea-
methodologytoguidethesearchofthesubsetoffeatures,byinspecting sure.Oncethedependencemeasureisset,theclassifierâ€™sperformance
onlysomesubsetsselectedintermsofthedependenceamongfeatures. measuretobemaximizedintheembeddedVariableSelectionstrategy
Asaresult,asparse,computationallytractableNBisobtained. canbechosen,amongthepreviouslydescribedmeasuresinSection2.1,
3R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Table1
Performancerateforallpossiblecombinationsoffeaturesinamultivariatenormalsimulatedexample.
Combination ð‘‹ ð‘‹ ð‘‹ ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹ ð‘‹,ð‘‹,ð‘‹ ð‘‹,ð‘‹,ð‘‹ ð‘‹,ð‘‹,ð‘‹ ð‘‹,ð‘‹,ð‘‹ ð‘‹,ð‘‹,ð‘‹,ð‘‹
1 2 3 4 1 2 1 3 1 4 2 3 2 4 3 4 1 2 3 1 2 4 1 3 4 2 3 4 1 2 3 4
of variables
ACC 68.08 68.51 68.55 69.01 68.38 74.99 75.08 74.86 75.25 75.68 73.58 73.85 79.94 79.84 78.28
)
ð‘½ .Notethatthehigher(lower)thevalueofthecut,themorelikely
ðŸ0
wearetochooseindependent(dependent)variables.
Although the previous strategy reduces the computational cost of
the brute force approach, it still may be costly for large datasets that
originateacomplexdendrogramwithmanycombinationsperthresh-
old.Inaddition,removingsomeofthefeaturesfromthecombinations
may lead up to sparser and more accurate models, since it might
happen that the (independent) variables selected in the combinations
have a very low predictive power. In order to strive to avoid such
inconveniences, we propose a refinement of the strategy as follows.
First, a maximum number ð‘† of combinations per threshold is set (if
thetotalnumberofpossibilitiesforagiventhresholdð‘,ð‘›ð‘(ð‘),doesnot
exceedð‘†,thenallofthemwillbeconsidered).Inthepreviousexample,
ð‘›ð‘(0.74) = 4. We should point out that parameters ð¶ and ð‘† are used
toalleviatethecomputationalburden,since,ascommentedbefore,ð¶
fixes the number of cuts along the height in the dendrogram, and ð‘†
themaximumnumberofcombinationsperthresholdtobeevaluated.
Therefore,thehigherð¶andð‘†are,thehigherthecomputertimeis.For
thisreason,wewillfixreasonablevaluesfortheseparametersinSec-
tion4.3.Second,foreachclusterofvariablestobeexamined,avalue
ð‘ž representing the probability of selecting this cluster for extracting
randomlyavariabletobeincludedinthecombinationisalsoset.Ifwe
( )
Fig.1. Heatmapassociatedtomatrixð‘€ (basedonMIcorrelation)correspondingto f (ixð‘ž=0.4,thepreviousf )ou (rcomb )ination (sbecome ð‘‰ 1, )ð‘‰ 7,ð‘‰ 8,ð‘‰ 13,ð‘‰ 14 ,
theAustraliandataset. ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ , ð‘‰ ,ð‘‰ and ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ , respectively.
1 4 6 8 9 12 14 2 8 4 6 10 11
The parameter ð‘ž is directly related to the sparsity degree: the lower
thevalueofð‘žis,thelessvariablesareinspected(theexpectednumber
accordingtotheuserâ€™sconvenienceandthepropertiesofthedataset. variablestobeconsideredisequaltoð‘žÃ—ð‘).Thechoiceofthevalues
Generally,theACCisselected,butinsomecases,e.g.forunbalanced {ð¶,ð‘†,ð‘ž}willbediscussedinSection4.
datasetsorwhenthereexistcriticalclasses,ourproposalreplaceACC Oncethesetofcombinationsoffeaturestobeevaluatedisreduced,
with AUC, precision or a certain Recall. Thus, the novel method turns the NB would be implemented and, its performance and feasibility
out specially advisable for datasets where the classes are unbalanced on the constrains considered, evaluated for each combination. This is
and/or of different importance. The selection of the dependence and summarizedinstep3oftheAlgorithm1.
theclassifierâ€™sperformancemeasuresisthefirststepofouralgorithm Finally,thefeasiblecombinationyieldingthehighestperformance
(seeAlgorithm1). measure(accuracy,AUCorwhateverchosenmeasure)wouldbecon-
Oncewe havechosena dependencemeasure, andthe elements of sidered the best, taking also into account the whole set of variables
thematrixð» arecomputed,weperformahierarchicalclusteranalysis in this comparison (step 4). For the Australian database example,
of features according to the dissimilarity matrix ð» (step 2 of the if no constraint is imposed, the features selected by our model are
(ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ),whichachieveanACCof86.76,whereasthe
algorithm). 1 4 6 8 9 12 14
wholesetofvariablesreturns85.29.Accordingtotheresults,itcanbe
Intheobtaineddendrogram,theverticalaxisrepresentsthedegree
deducedthatourmodelhaskepttheimportantfeatures,usingonlya
ofdissimilarity.Thehigherthevalueoftheheightis,thelessdependent
half of the total set. However, in this dataset, the positive class (the
thevariablesare,accordingtothedependencemeasure.Forexample,
loadisgranted)isthemostrisky.Then,ifweimposee.g.thatRecall+
the dendrogram corresponding to the Australian dataset is given by ( )
>92,thecombinationoffeatures ð‘‰ ,ð‘‰ wouldbetheselectedone.
Fig.2.Suchadendrogramhasbeenobtainedusingtheroutinehclust 2 8
A summary concerning the strategy for the sparse NB is given by
of the Statistical software environment R. In this case, ð‘‰ and ð‘‰ are
5 6 Algorithm1.
highlydependent,aswellasð‘‰ andð‘‰ .However,therestofvariables
9 10
arealmostindependent,sincetheyclusteratheightsbetween0.9and1.
4. Numericalillustrations
Once the dendrogram is built, a (not necessarily regular) grid of
a specified number ð¶ of cuts along the height is fixed. The basic
In this section, the behavior and performance of our approach is
idea underlying the variable reduction strategy is to examine at each illustrated throughout an extensive empirical study, using both simu-
cut (or threshold) of the grid several combinations of features, in latedandrealdatasets.Inthefirstcase,asyntheticdatasetissimulated
such a way that only one feature is selected per cluster since all in order to test how the performance and level of sparsity of the
elements in a cluster are assumed to be strongly dependent. As an proposed sparse NB changes with the level of dependence among the
example, consider Fig. 2 and assume that one of the ð¶ cuts is ð‘ = features. Second, ten real datasets from the UCI Machine Learning
0.74 (horizontal line). Then, we consider that there are 12 clusters: Repository(Lichman,2013),presentingdifferentcorrelationpatterns,
10 clusters formed by only one feature and the clusters {ð‘‰ 5,ð‘‰ 6} and differentdegreesofunbalancednessandsomeofthemcombiningboth
{ð‘‰ 9,ð‘‰ 10}. And, therefore, four independent combinations would be continuousandcategoricalvariables,willbeanalyzedunderthesparse
( )
selectedatthisthreshold: ð‘‰ 1,ð‘‰ 7,ð‘‰ 11,ð‘‰ 3,ð‘‰ 13,ð‘‰ 2,ð‘½ ðŸ“,ð‘‰ 4,ð‘‰ 14,ð‘‰ 8,ð‘‰ 12,ð‘½ ðŸ— , NBdescribedinSection3.Intheexperiments,theperformanceratesof
( ) (
ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘½ ,ð‘‰ ,ð‘‰ , ð‘‰ ,ð‘‰ ,ð‘½ , ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ , theclassifiershallbeestimatedaccordingtoanð‘ runsð‘âˆ’foldcross
1 7 11 3 13 2 ðŸ“ 4 14 8 12 ðŸ0 1 7 11 3 13 2
) (
ð‘½ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘½ and ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ , ð‘‰ ,ð‘‰ ,ð‘½ ,ð‘‰ ,ð‘‰ ,ð‘‰ ,ð‘‰ , validationprocedure,withð‘=10.Ateachfold,thedatasetissplitinto
ðŸ” 4 14 8 12 ðŸ— 1 7 11 3 13 2 ðŸ” 4 14 8 12
4R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Fig.2. Clusterdendrogram(basedonMIcorrelation)correspondingtotheAustraliandataset.
Algorithm1:Pseudo-codeofthesparseNB Now, we discuss the choice of the parameters {ð¶,ð‘†,ð‘ž} and the
performancecriterion.
1.Selectthedependenceandtheclassifierâ€™sperformancemeasures.
2.Performclusteranalysisandbuildthedendrogram.
3.Variablereductionstrategy:setspecificvaluesfortheparameters{ð¶,ð‘†,ð‘ž}
Choiceoftheparameterð¶
andinitializeîˆ²=âˆ….
forð‘=1,â€¦,ð¶ do Thevalueofð¶,whichrepresentsthenumberofcutsinthevertical
forð‘ =1,â€¦,min{ð‘›ð‘(ð‘),ð‘†}do axisofthedendrogram,iscriticalforapropersampling.Asadefault
(a)Obtaintheð‘ thcombinationoffeatures.Foreachclusteronlyone
value, we propose to select the points of the grid where features are
variableisrandomlyselectedwithprobabilityð‘ž,andnonewith
probability1âˆ’ð‘ž. clustered. When the routine hclust of R is used to generate the
(b)Constructtheclassifierfortheð‘ thcombinationoffeatures. dendrogram (as in this work), one has ð¶ = ð‘ âˆ’ 1, where ð‘ is the
(c)Evaluatetheselectedclassifierâ€™sperformancemeasureandif numberoffeatures.Inaddition,hclustspecifieswheretomakethe
feasible,addittoîˆ². cuts.However,alargevalueofð¶ mayslowdowntheexecutionofthe
end algorithm notably and, on the other hand, it may lead to overfitting.
end For this reason, ð¶ will be defined as min{ð‘âˆ’1,100}. As will be seen
4.VariableSelection:selectthecombinationofvariablesleadingtothebest
next,inSection4.2,suchachoiceyieldsarightbalancebetweenthe
performance,amongthoseinîˆ².
performanceandthecomputationaltimefortheconsidereddatasets.
Choiceoftheparameterð‘†
threesets,theso-calledtraining,validationandtestingsets.Atenthof
the dataset is used as testing set, and the remaining nine tenths are Regarding the value of ð‘†, which represents the maximum total of
( ) ( )
for trainingset 2Ã— 9 andvalidation set 1Ã— 9 . Steps2,3(a) combinations for each cut, we tested several possible values for this
and 3(b) of Alg3 orith10 m 1 are implemented o3 n th10 e training set. The parameter(seeSupplementaryMaterial),andsettledonthefinalchoice
ð‘† = 25. Note that under the previous choices of ð¶ and ð‘† a total of
different classifiers built in this way are compared according to their
performance results (step 3(c)) on the validation set. The classifier max{25Ã—(ð‘âˆ’1),25Ã—100} combinations of features will be evaluated
undertheproposedsparseNBincontrasttothetotalnumberofpossible
(combination of features) with the highest performance measure on
combinations,equalto2ð‘âˆ’1.
the validation set is chosen, and its average performance rates on
the testing set are reported. Special emphasis will be made on the
performance behavior and sparsity of the solutions of the proposed Choiceoftheparameterð‘ž
method.
Small values of ð‘ž are associated with more sparsity (since fewer
4.1. Parameterssetting variables would be included in the combinations to be examined).
Therefore, ð‘ž should be selected in such a way that it provides a
The probability distribution for the features conditioned to the compromise between the classifierâ€™s performance and the sparsity of
class ð‘‹ âˆ£ ð¶ , for ð‘– = 1,â€¦,ð‘, ð‘˜ = 1,â€¦,ð¾, needs to be selected. the solution. In particular, in the Supplementary Material, different
ð‘– ð‘˜
It is well-known in the literature that the performance of the NB experiments to evaluate how the choice of this parameter affects the
classifierimproveswhenfeaturesarecategorizedusinganydiscretiza- results can be found. Here, the selection of this parameter has been
tionmethod(Liuetal.,2002;BoullÃ©,2004;BoullÃ©,2006).Therefore, addressedaccordingtothedependencematrixð‘€,whichisdefinedin
insteadofimposingaspecificprobabilitydistribution(suchastheGaus- Section 3. In particular, when 20% of the matrix elements are higher
sian),weadoptedthediscretizationmethodbasedonanentropycrite- than 0.1 (that is, from moderate to strong dependence cases), we fix
rion(seeDoughertyetal.,1995)andusedthemdlproutine(Fayyad ð‘ž = 0.4 (which implies a sparser solution). Otherwise (few dependent
andIrani,1993)fromthediscretizationpackageofR. features),ð‘ž=0.6willbeset.
5R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
4.3. Datasetsandbenchmarkapproaches
The so-called Breast Cancer Wisconsin (Diagnostic) Data Set, Wine
Data Set, Mushroom, Waveform Database Generator Data Set (version
2),ISOLETDataSet,MultipleFeaturesDataSet,SPECTFHeartDataSet,
GermanCredit,PageBlocksClassificationDataSet andStatlog(Australian
Credit Approval) shall be considered. They are described in Table 3,
whose first three columns report the dataset name, the number of
instances and the class split. The number of continuous variables (ð¿)
and categorical variables (ð¿â€²) are presented in the last two columns.
Three of the ten datasets, SPECTF Heart Data Set, German Credit and
PageBlocksClassificationDataSet,areunbalanceddatasets,duetothe
verydifferentsizesoftheclasses.
Weaimtocomparethenovelmethodwithalternative,well-known
strategies for feature selection. In this study, we focus on techniques
which perform hard variable selection and, in consequence, feature
weightingapproachesasinJiangetal.(2019)havenotbeenconsidered
here. There exist two main groups of methods that select features:
filters (Guyon et al., 2006; Saeys et al., 2007) and wrappers (Kohavi
Fig.3. Scalability.
and John, 1997; Saeys et al., 2007). We selected one filter and one
wrapperthatarewellreferencedintheliteratureandthatcanbeeasily
adaptedtotheNBclassifiertomakeafaircomparison.Ourchoicewas
4.2. Simulationstudy
thefilterCFSintroducedinSection1,seeHall(2000),andthewrapper
Boruta (Kursa and Rudnicki, 2010). The wrapper Boruta, which is in
In this section we analyze how the performance of the sparse principledesignedusingaRandomForeststrategy,canbemodifiedand
NB varies as dependence among features increases. In particular, we adapted to any classifier, in particular, to the NB. These methods are
simulatedatafollowingWittenetal.(2014)andaccordingtothemodel
widelyspreadandcanbecomputedbytheroutinescfsandBoruta,
from R packages FSelector and Boruta, respectively. In order to
ð² = ð—ðœ·+ðœº with ð‘ âˆˆ {100,200,300,400,500}. The errors ðœ€ ,â€¦,ðœ€ are
1 ð‘ adaptthewrapperBorutatotheNBclassifier,wehaveusedthefunction
iidfromað‘(0,2.52)distribution.Theobservations(rowsofð—)areiid
filterVarImp in R package caret as the function that returns
fromað‘ (0,ðœ®)distribution,whereðœ® isað‘Ã—ð‘blockdiagonalmatrix,
ð‘ the importance of the attributes, instead of the default getImpRfZ,
withelementsasfollows:
which is based on the Random Forest classifier. It is important to
âŽ§1 ifð‘–=ð‘—,
highlightthatthetimelimitisnotaninputparameterofthecfsand
ðœ®
ð‘–ð‘—
=âŽª âŽª âŽ¨ðœŒ
ðœŒ if
ð‘+1i â‰¤fð‘– ð‘–â‰¤ â‰¤4ð‘ ð‘, ,ð‘— ð‘â‰¤ +14ð‘, â‰¤ð‘–â‰  ð‘—â‰¤ð‘—,
ð‘,ð‘–â‰ ð‘—,
B wo er reut foa unro dut (i tn oes bean dd ist ch ue sr se ef dore la, td ei rf )f .er Ae pn ac re ts fi rn omthe thco em pp ru evta ioti uo sna fel ac to us rt es
âŽª 4 4 selectionmethods,thatcanbeappliedtoanyclassifier,thereareworks
âŽª0 otherwise
that specifically deal with variable reduction for the NB. In particu-
âŽ©
lar, BoullÃ© (2007) proposes a straightforward Bayesian modern-style
WeexploredvariousvaluesofðœŒ,rangingfrom0.1to0.9.Furthermore,
approach,theMAPApproachforVariableSelection(notedSNB(MAP)),
ð›½ ð‘– âˆ¼ Unif[0.9, 1.1] for 1 â‰¤ ð‘– â‰¤ âŒŠ 4ð‘ âŒ‹ and ð›½ ð‘– âˆ¼ Unif[-1 3 âˆ’0.1, âˆ’1 3 +0.1] wheretheconditionalprobabilitiesareformulatedaccordingto
otherwise. In other words, there are two sets of ð‘ and 3ð‘ correlated
4 4 âˆð‘
features, respectively, and all the features are associated with the ð‘(ð¶ ð‘˜|ð±)âˆðœ‹(ð¶ ð‘˜) ð‘(ð‘¥ ð‘–|ð¶ ð‘˜)ð‘Žð‘–, ð‘˜=1,â€¦,ð¾. (6)
response. Finally, two classes are defined according to the sign of ð‘¦ ð‘›, ð‘–=1
ð‘›=1,â€¦,2000. InEq.(6),thevalues{ð‘Ž}ð‘ areeither1or0,dependingonwhetheror
ð‘– ð‘–=1
TheresultsinTable2havebeenobtainedusingtheMutualInfor- notfeatureð‘–isincludedinthemodel.Then,theposteriordistribution
mation dependence measure (MI), ð‘† = 25 and values of ð‘ž fixed as in of the different models (resulting from different choices of {ð‘Ž ð‘–}ð‘ ð‘–=1) is
evaluated using a shrinkage prior so that parsimonious models are
Section 4.3. Moreover, the performance measure considered for these
favored. In the same paper, a search heuristic that performs a fast
simulated experiments is the accuracy, and its average performance
forward backward selection is described and therefore, it has been
ratesasin(3)onthetestingsetarereportedinTable2.
implementedinthispapertorunthedifferentexperiments.However,
Some conclusions can be drawn. On the one hand, in terms of
when the number of variables increases, note that the time required
sparsity levels, the sparse NB returns better results in the presence of to run this method is excessive. For that reason, a time limit of eight
moderate to strong dependence cases. For datasets where the depen- hoursforthetwobiggestconsideredrealdatasets(ISOLET andMultiple
dences among features are weak, ðœŒ = 0.1, our sparse strategy is able Features) was fixed. Finally, we have also compared with the Lasso
toremovearoundonethirdofthetotalnumberofvariableswhereas, approachforclassification(seeVincentandHansen,2014),whosegoal
in some cases, the ACC is slightly reduced with regards to the classic is precisely the same: obtain good classification performance while
NB.WhileðœŒincreases,ourproposalisabletosignificantlyreducethe selecting few features. The routine fit in R package msgl has been
used.
numberofvariablesconsidered,achievingbetterACCresultsthanthe
Next, we will break the results down depending on the datasets
classicNB,asthecurseofdependencyisalleviated.Ontheotherhand,
arebalancedorunbalanced.Aswewillshowbelow,ifnecessaryand
Fig.3reportsthelogarithmoftheaverageusertimes(inseconds)when
motivatedbythepropertiesofthedataset,ourproposalcanbeeasily
thesparseNBisrunonIntel(R)Core(TM)i7-7500UCPUat2.70GHz
adaptedintermsoftheperformancecriteriontobeoptimizedandthe
2.90GHzwith8.0GBofRAM.Theð‘‹-axisshowstheðœŒvalueswhereas
requiredconstraintsongroupsofinterest.Tomakeafaircomparison,
eachlinerepresentsthenumberofvariablesofthedataset(ð‘).Overall, wedonotimposeanyadditionalconstraintsandthereforeonlytheper-
forweakdependencesamongfeatures,thebehaviorofrunningtimeis formancecriterionwillchangeaccordinglythroughoutthesesections.
monotonousrespecttothenumberofvariables,butthischangeswhen However,anillustrativeexamplewhereconstraintsareimposedisalso
ðœŒincreases. includedattheendofSection4.5.
6R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Table2
Averageaccuracyandsparsity(10runs10-foldCV)forsimulateddatasets.
ð‘ Method ðœŒ=0.1 ðœŒ=0.5 ðœŒ=0.7 ðœŒ=0.9
ACC Sparsity ACC Sparsity ACC Sparsity ACC Sparsity
SparseNB 88.20 66 93.30 37 93.20 24 95.05 21.30
100
ClassicNB 89.50 100 87.05 100 87.10 100 86.95 100
SparseNB 90.10 113.5 93.40 52.50 95.70 30.50 96.55 20.10
200
ClassicNB 90.10 200 87.30 200 87.65 200 86.80 200
SparseNB 89.85 168.60 92.40 79.70 94.70 37.30 95.95 18.20
300
ClassicNB 90.15 300 87.00 300 87.15 300 87.85 300
SparseNB 91.90 216.30 91.45 94 92.35 46.80 93.65 17
400
ClassicNB 89.65 400 87.25 400 87.25 400 87.50 400
SparseNB 91.90 283.20 90.70 102.70 93.85 29.60 91.90 5.90
500
ClassicNB 90.15 500 87.05 500 87.45 500 87.55 500
Table3
Datasetsdescription.
Name Instances Classsplitin% L Lâ€™
BreastCancerWisconsin 569 63(Benign)/37(Malignant) 30 0
WineDataSet 178 33(Class1)/40(Class2)/27(Class3) 13 0
Mushroom 8124 51.8(edible)/48.2(poisonous) 0 22
WaveformDatabaseGenerator 5000 33.33(Class0)/33.33(Class1)/33.33(Class2) 40 0
ISOLETDataSet 7797 26equiprobableclasses(0.04) 617 0
MultipleFeatures 2000 9equiprobableclasses(0.11) 649 0
SPECTFHeart 267 79(Abnormal)/21(Normal) 44 0
GermanCredit 1000 70(Class1)/30(Class2) 7 13
PageBlocksDataSet 5473 90(Negative)/10(Positive) 10 0
Australian 690 55.5(Negative)/44.5(Positive) 6 8
4.4. Resultsforbalanceddatasets whichareunbalancedaccordingtoclasses.Itimpliesthattheuseofthe
ACC,definedby(3),astheperformancecriterionmaynotbeasensible
Forcomparisonpurposes,considerthesameparameterssettingthan choice because of the difference between the classes sizes. Therefore,
in Section 4.2, where for Waveform dataset, ð‘ž is equal to 0.6 and, forthesecases,theareaunderthecurve(AUC)aswellastheprecisionof
for the remaining balanced databases, ð‘ž = 0.4. We next analyze the themajorityclass(Class1),calculatedby(4),willbethemeasurestobe
performanceandsparsityofthemethod,aswellastherunningtimes. maximized.Theformermeasure,ð‘ð‘Ÿð‘’ð‘ð‘–ð‘ ð‘–ð‘œð‘› ,leadstogoodð‘…ð‘’ð‘ð‘Žð‘™ð‘™ ,since
1 2
Theaverageaccuracy,numberofvariablesintheselectedcombinations will minimize the False Class 1. In addition, the performance at each
and the CPU time in seconds for 1 fold-CV execution are shown by class,willbeinspectedviatheso-calledRecall.Wehaveconsideredthe
Fig.4.Moreover,acomparisonbetweenthesparseNBwiththeabove- previoustwoperformancemeasureswhenselectingthesetofvariables
mentioned feature selection methods is made. The results under the via sparse NB, and the obtained results are shown in red and blue
classicNB,CFS,Boruta,SNB(MAP)andLassomethodsarealsoshown. (respectively) in Fig. 5. Finally, ð‘ž = 0.6 for German Credit and Page
Several conclusions can be drawn at this point. Note that the Blocks,whereasisequalto0.4inthecaseofSPECTFHeartDataSet.
performanceratesunderthesparseNBarecomparabletotheclassicNB
Again,theperformanceresults,thesparsityresultsandtherunning
usingbetweenahalfandonethirdofthevariables,exceptforWaveform
timesarereportedinFig.5.Foreachdataset,twographicsareshown.
Database. As commented before, the novel approach is intended to
The images on the left represent the AUC versus the sparsity, while
address databases with correlated patterns and, for this reason, the
the Recall of the majority and minority classes (ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ and ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ ,
outperformanceofthesparseNBimproveswiththedependenceamong 1 2
respectively)aredrawnontherightside.NotethattheBorutaresults
the features. Therefore, since the variables of the Waveform Database
forSPECTF databasearenotreportedsincethisdatasetdoesnotsatisfy
are almost independent, it is expected that the novel sparse strategy
thetechnicalrequirementsoftheimplementationofthatmethod.
doesnotyieldasignificantenhancementinthissense,asFig.4shows.
TheperformanceratesunderthesparseNBarecomparabletothe
With regards to the five feature selection methods considered in
results obtained with all the features, since the AUC (respectively,
thisstudy,thenextconclusionscanbedrawnfromthefigure.Whereas
the precision) has been used as performance criterion and the novel
theproposedmethodachievescompetitiveACCandsparsityresults,it
approachkeepsatleasttheareaunderthecurve(orprecision)obtained
performs in between the other methods in these two measures. Also,
bytheclassicNB.ThesparseNBisabletoreducetolessthanhalfthe
itcanbeconcludedthatSNB(MAP)iscomputationallyslowerthanthe
numberofvariablesinthecaseofSPECTFHeartdataset;itremovesone
sparseNB.Inaddition,BorutaandCFSarelesscomputationallycostly
fourthofthevariablesofGermandatasetandonethirdinPageBlocks
thanthesparseNB,butwhenthenumberoffeaturesincrease,itturns
DataSet.Now,ifwecomparetoCFS,Boruta,SNB(MAP)andtheLasso,
outtobeexceptionallylow.
itcanbeobservedhow,althoughtheytendtobesparser,theyincrease
In summary, it can be deduced that, for balanced datasets with
dependencies among the features, the proposed sparse NB leads to significantlythemisclassificationrateontheminorityclass(ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ 2),
a significant reduction in the number of features while keeping the since,ingeneral,theytendtoincreasethecorrectclassificationforthe
power prediction. Also, it can be concluded that in general, for this majority class (ð‘…ð‘’ð‘ð‘Žð‘™ð‘™ 1) and to decrease the minority one. The latest
kind of datasets, our method and the Lasso seem to achieve the best resultsasserttheneedtochooseanappropriateperformancemeasure
compromisebetweenaccuracy,sparsityandrunningtimes. accordingtothepropertiesofthedataset.
Therefore,withregardstotheunbalanceddatabases,thesparseNB
4.5. Resultsforunbalanceddatasets providesmorebalancedRecallvalues,inthesensethattheperformance
of the least frequent class is not so reduced. Another illustration is
Inthissectionwedealwiththreeunbalanceddatasets.TheSPECTF givenbyTable4,wheretheAustralianCreditApprovalisconsidered.As
Heart Data Set, German Credit and Page Blocks Classification Data Set, commentedbefore,inthiscase,thepositiveclass(theloadisgranted)
7R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Fig.4. Averageaccuracy,sparsityandCPUtime(10runs10-foldCV)forBreastCancer (BC),Wine,Mushroom,Waveform,ISOLET andMultipleFeatures(Mult.Feat.)datasets.
Table4 is the most risky. For these cases, the sparse NB would be the most
Averageperformanceandsparsity(10runs10-foldCV)forAustraliandatasetusingthe suitablechoice,notonlybecausetheperformancecriteriontobeused
sparseNBwithdifferentperformancemeasurestoselectthesetofvariables.
canbeeasilyadaptedbutalsobecausewhileoptimizingsuchcriterion,
Method Recall- Recall+ ACC Sparsity constraintsonacceptableperformancemeasurescanbeincluded.The
ClassicNB 91.10 78.78 85.61 14 secondrowofTable4showstheresultsfortheSparseNBiftheACCis
SparseNB(ACC) 84.59 85.83 85.15 5.78
consideredasperformancecriterionandnoadditionalconstraintsare
SparseNB(ACC);Recall+>85 84.14 86.48 85.19 5.53
imposed.However,theACCcanbeoptimizedwhereasaperformance
SparseNB(Recall+);Recall->60 79.93 92.35 85.46 1.4
constraintontheRecallofthepositiveclassisconsidered(Recall+>
85), as can be seen in the third row of Table 4. As a final example,
we are interested in maximizing the Recall + instead. Note that the
improvement in the positive class will be at the expense of reducing
8R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Fig.5. Averageperformance,sparsityandCPUtime(10runs10-foldCV)forSPECTF,GermanandPageBlocksdatasets.
the Recall - and therefore admissible values for it have been imposed sizes and properties. The numerical results show that not only sparse
viaathresholdvaluetoavoidworseningit,sayRecall->60(lastrow). solutionsareattained,butalsotheperformanceratesarecomparable
To sum up, for unbalanced datasets with dependent variables, the or better than those achieved under the classic version of the NB,
considered benchmark methods tend to be sparser than our approach where all features are taken into account for classifying. In addition,
but at the cost of damaging unpredictably the performance of the when compared with benchmark approaches, the novel method turns
classifierand,inparticular,theRecalloftheleastfrequentclass.Incon- outespeciallyadvisablefordatasetswheretheclassesareunbalanced
trast,thenovelmethodallowstheusertosettheperformancemeasure and/or of different importance. This fact stems from the flexibility of
thatbestsuitsitaswellasadmissiblevaluesforspecificperformance ourmethodintheselectionoftheperformancemeasureandtheability
measures,whichturnsoutadvantageousforunbalanceddatasetsorfor to include constraints on certain performance measures for feature
casesinwhichmisclassificationcostsarestronglyclass-dependent. selection,whichdoesnotoccurwiththefeatureselectionapproaches
proposedintheliterature.
5. Conclusionsandextensions In this work, sparsity has been explored in the case of the NB
because of its tractability and good performance, but other classifiers
In this paper, a new version of the NaÃ¯ve Bayes (NB) classifier for couldhavealsobeentestedinstead,asforexamplethesupportvector
dealingwithdatasetswithcorrelatedpatternsisproposedwiththeaim machines.Workontheseissuesisunderway.
ofimprovingthesparsityofthesolution.Inordertoachievesparsity,
avariablereductiontechniqueisembeddedintotheclassifier.Sucha CRediTauthorshipcontributionstatement
variablereductionstrategyisbasedonclusteringthefeaturesinterms
oftheirdependencedegree,anditselectscombinationsoffeaturesthat, RafaelBlanquero:Conceptualization,Methodology,Writing-orig-
being as independent as possible, lead to a good performance rate. inaldraft,Writing-review&editing,Supervision.EmilioCarrizosa:
The performance measure used in the algorithm can be given by the Conceptualization, Methodology, Writing - original draft, Writing -
out-of-sampleaccuracy,ormoregenerally,anestimateoftheexpected review & editing, Supervision. Pepa RamÃ­rez-Cobo: Conceptualiza-
misclassification cost, among others. The proposed methodology has tion, Methodology, Writing - original draft, Writing - review & edit-
been tested on synthetic datasets and ten real datasets of different ing, Supervision. M. Remedios Sillero-Denamiel: Conceptualization,
9R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Methodology, Software, Validation, Writing - original draft, Writing - Hall, M.A., 2000. Correlation-based feature selection for discrete and numeric class
review&editing. machinelearning.In:ProceedingsoftheSeventeenthInternationalConferenceon
MachineLearning.ICMLâ€™00,MorganKaufmannPublishersInc.,SanFrancisco,CA,
USA,pp.359â€“366.
Acknowledgments
Hand,D.J.,Yu,K.,2001.Idiotâ€™sBayes-notsostupidafterall?Internat.Statist.Rev.
69(3),385â€“398.
Thisresearchispartiallysupportedbyresearchgrantsandprojects Hastie, T., Tibshirani, R., Friedman, J., 2001. The Elements of Statistical Learning.
MTM2015-65915-R(MinisteriodeEconomÃ­ayCompetitividad,Spain) Springer,NY.
andPID2019-110886RB-I00(MinisteriodeCiencia,InnovaciÃ³nyUni- Hastie,T.,Tibshirani,R.,Wainwright,M.,2015.StatisticalLearningwithSparsity.The
LassoandGeneralizations.CRCPress.
versidades, Spain), FQM-329 and P18-FR-2369 (Junta de AndalucÃ­a,
Hoeffding,W.,1948.Anon-parametrictestofindependence.Ann.Math.Stat.19(4),
Spain), PR2019-029 (Universidad de CÃ¡diz, Spain), FundaciÃ³n 546â€“557.
BBVAandECH2020MSCARISENeEDSProject(GrantagreementID: Jiang,L.,Cai,Z.,Zhang,H.,Wang,D.,2012.Notsogreedy:Randomlyselectednaive
822214).Thissupportisgratefullyacknowledged. Bayes.ExpertSyst.Appl.39(12),11022â€“11028.
Jiang,L.,Zhang,H.,Cai,Z.,Su,J.,2005.EvolutionalnaiveBayes.In:Proceedingsof
the1stInternationalSymposiumonIntelligentComputationandItsApplications.
AppendixA. Supplementarymaterial
ISICA2005,ChinaUniversityofGeosciencesPress,pp.344â€“350.
Jiang,L.,Zhang,L.,Li,C.,Wu,J.,2019.Acorrelation-basedfeatureweightingfilter
Supplementarymaterialrelatedtothisarticlecanbefoundonline fornaiveBayes.IEEETrans.Knowl.DataEng.31(2),201â€“213.
athttps://doi.org/10.1016/j.cor.2021.105456. Jiang, L., Zhang, L., Yu, L., Wang, D., 2019. Class-specific attribute weighted naive
Bayes.PatternRecognit.88,321â€“330.
Kinney, J.B., Murugan, A., Callan, C.G., Cox, E.C., 2010. Using deep sequencing to
References
characterize the biophysical mechanism of a transcriptional regulatory sequence.
Proc.Natl.Acad.Sci.107(20),9158â€“9163.
â€˜â€˜annâ€™â€™ Ratanamahatana, C., Gunopulos, D., 2003. Feature selection for the naive Kohavi, R., John, G.H., 1997. Wrappers for feature subset selection. Artificial
bayesianclassifierusingdecisiontrees.Appl.Artif.Intell.17(5â€“6),475â€“487. Intelligence97(1â€“2),273â€“324.
Benati,S.,GarcÃ­a,S.,2014.Amixedintegerlinearmodelforclusteringwithvariable Kuncheva,L.I.,2006.OntheoptimalityofNaÃ¯veBayeswithdependentbinaryfeatures.
selection.Comput.Oper.Res.43,280â€“285. PatternRecognit.Lett.27(7),830â€“837.
BenÃ­tez-PeÃ±a, S., Blanquero, R., Carrizosa, E., RamÃ­rez-Cobo, P., 2019. On support Kursa,M.B.,Rudnicki,W.R.,2010.Featureselectionwiththeborutapackage.J.Stat.
vector machines under a multiple-cost scenario. Adv. Data Anal. Classif. 13 (3), Softw.36(11),1â€“13.
663â€“682. Langley,P.,Sage,S.,1994.InductionofselectiveBayesianclassifiers.In:Proceedings
Bermejo,P.,GÃ¡mez,J.A.,Puerta,J.M.,2014.Speedingupincrementalwrapperfeature oftheTenthInternationalConferenceonUncertaintyinArtificialIntelligence.pp.
subsetselectionwithNaiveBayesclassifier.Knowl.-BasedSyst.55,140â€“147. 399â€“406.
Blanquero,R.,Carrizosa,E.,JimÃ©nez-Cordero,A.,MartÃ­n-BarragÃ¡n,B.,2020.Selection Leevy,J.L.,Khoshgoftaar,T.M.,Bauder,R.A.,Seliya,N.,2018.Asurveyonaddressing
of time instants and intervals with support vector regression for multivariate high-class imbalance in big data. J. Big Data 5 (42), http://dx.doi.org/10.1186/
functionaldata.Comput.Oper.Res.123,105050. S40537-018-0151-6.
Blanquero, R., Carrizosa, E., Molero-RÃ­o, C., Romero Morales, D., 2021a. Optimal Lichman,M.,2013.UCIMachineLearningRepository.UniversityofCalifornia,School
randomizedclassificationtrees.Comput.Oper.Res.132,105281. ofInformationandComputerSciences,Irvine.
Blanquero, R., Carrizosa, E., RamÃ­rez-Cobo, P., Sillero-Denamiel, M.R., 2021b. A
Lin,D.,Foster,D.P.,Ungar,L.H.,2011.VIFregression:Afastregressionalgorithmfor
cost-sensitiveconstrainedlasso.Adv.DataAnal.Classif.15,121â€“158.
largedata.J.Amer.Statist.Assoc.106(493),232â€“247.
BoullÃ©,M.,2004.Khiops:Astatisticaldiscretizationmethodofcontinuousattributes.
Linfoot,E.,1957.Aninformationalmeasureofcorrelation.Inf.Control1(1),85â€“89.
Mach.Learn.55(1),53â€“69.
Liu,H.,Hussain,F.,Tan,C.L.,Dash,M.,2002.Discretization:Anenablingtechnique.
BoullÃ©, M., 2006. MODL: A Bayes optimal discretization method for continuous
DataMin.Knowl.Discov.6(4),393â€“423.
attributes.Mach.Learn.65(1),131â€“165.
Maldonado, S., Carrizosa, E., Weber, R., 2015. Kernel Penalized K-means: A feature
BoullÃ©,M.,2007.Compression-basedaveragingofselectivenaiveBayesclassifiers.J.
selectionmethodbasedonKernelK-means.Inform.Sci.322,150â€“160.
Mach.Learn.Res.8,1659â€“1685.
McCallum, A., Nigam, K., 1998. A comparison of event models for naive bayes text
Cai, A., Tsay, R., Chen, R., 2009. Variable selection in linear regression with many
classification.In:AAAI-98WorkshoponLearningforTextCategorization,vol.752.
predictors.J.Comput.Graph.Statist.18(3),573â€“591.
pp.41â€“48.
Carrizosa, E., Guerrero, V., 2014. Biobjective sparse principal component analysis. J.
Minnier,J.,Yuan,M.,Liu,J.S.,Cai,T.,2015.Riskclassificationwithanadaptivenaive
MultivariateAnal.132,151â€“159.
Bayeskernelmachinemodel.J.Amer.Statist.Assoc.110(509),393â€“404.
Carrizosa, E., Molero-RÃ­o, C., Romero Morales, D., 2021. Mathematical optimization
Mukherjee,S.,Sharma,N.,2012.IntrusiondetectionusingnaiveBayesclassifierwith
in classification and regression trees. TOP 29, 5â€“33. http://dx.doi.org/10.1007/
featurereduction.Proc.Technol.4,119â€“128.
s11750-021-00594-1.
R Core Team, 2017. R: A Language and Environment for Statistical Computing. R
Carrizosa,E.,Nogales-GÃ³mez,A.,RomeroMorales,D.,2016.Stronglyagreeorstrongly
FoundationforStatisticalComputing,Vienna,Austria.
disagree?:Ratingfeaturesinsupportvectormachines.Inform.Sci.329,256â€“273.
Reshef,D.N.,Reshef,Y.A.,Finucane,H.K.,Grossman,S.R.,McVean,G.,Turnbaugh,P.J.,
Carrizosa,E.,Olivares-Nadal,A.V.,RamÃ­rez-Cobo,P.,2017.Asparsity-controlledvector
Lander,E.S.,Mitzenmacher,M.,Sabeti,P.C.,2011.Detectingnovelassociationsin
autoregressivemodel.Biostatistics18(2),244â€“259.
largedatasets.Science334(6062),1518â€“1524.
Carrizosa, E., Romero Morales, D., 2013. Supervised classification and mathematical
optimization.Comput.Oper.Res.40(1),150â€“165. Rezaei,M.,Cribben,I.,Samorani,M.,2018.Aclustering-basedfeatureselectionmethod
Chen,S.,Webb,G.I.,Liu,L.,Ma,X.,2020.AnovelselectivenaÃ¯veBayesalgorithm. forautomaticallygeneratedrelationalattributes.Ann.Oper.Res.http://dx.doi.org/
Knowl.-BasedSyst.192,105361. 10.1007/s10479-018-2830-2.
Domingos,P.,Pazzani,M.,1996.Beyondindependence:Conditionsfortheoptimality Saeys, Y., Inza, I., LarraÃ±aga, P., 2007. A review of feature selection techniques in
of the simple Bayesian classifier. In: Proceedings of the Thirteenth International bioinformatics.Bioinformatics23(19),2507â€“2517.
ConferenceonMachineLearning.MorganKaufmann,pp.105â€“112. Sharpee,T.,Rust,N.C.,Bialek,W.,2004.Analyzingneuralresponsestonaturalsignals:
Domingos, P., Pazzani, M., 1997. On the optimality of the simple Bayesian classifier Maximallyinformativedimensions.NeuralComput.16(2),223â€“250.
underzero-oneloss.Mach.Learn.29(2â€“3),103â€“130. Sokolova, M., Lapalme, G., 2009. A systematic analysis of performance measures for
Dougherty,J.,Kohavi,R.,Sahami,M.,1995.Supervisedandunsuperviseddiscretization classificationtasks.Inf.Process.Manage.45(4),427â€“437.
of continuous features. In: Prieditis, A., Russell, S. (Eds.), Machine Learning SzÃ©kely,G.J.,Rizzo,M.L.,Bakirov,N.K.,2007.Measuringandtestingdependenceby
Proceedings1995.pp.194â€“202. correlationofdistances.Ann.Statist.35(6),2769â€“2794.
Fayyad, U.M., Irani, K.B., 1993. Multi-interval discretization of continuous valued Tang,B.,He,H.,Baggenstoss,P.M.,Kay,S.,2016a.ABayesianclassificationapproach
attributesforclassificationlearning.In:Proceedingsofthe13thInternationalJoint usingclass-specificfeaturesfortextcategorization.IEEETrans.Knowl.DataEng.
ConferenceonArtificialIntelligence.Morgan-Kaufmann,pp.1022â€“1029. 28(6),1602â€“1606.
Feng,G.,Guo,J.,Jing,B.-Y.,Sun,T.,2015.FeaturesubsetselectionusingnaiveBayes Tang,B.,Kay,S.,He,H.,2016b.TowardoptimalfeatureselectioninnaiveBayesfor
fortextclassification.PatternRecognit.Lett.65,109â€“115. textcategorization.IEEETrans.Knowl.DataEng.28(9),2508â€“2521.
George,E.,McCulloch,R.,1993.Variableselectionviagibbssampling.J.Amer.Statist. Turhan, B., Bener, A., 2009. Analysis of Naive Bayesâ€™ assumptions on software fault
Assoc.88(423),881â€“889. data:Anempiricalstudy.DataKnowl.Eng.68(2),278â€“290.
Guan,G.,Guo,J.,Wang,H.,2014.VaryingNaÃ¯veBayesmodelswithapplicationsto Vincent,M.,Hansen,N.R.,2014.Sparsegrouplassoandhighdimensionalmultinomial
classificationofchinesetextdocuments.J.Bus.Econom.Statist.32(3),445â€“456. classification.Comput.Statist.DataAnal.71,771â€“786.
Guyon,I.,Gunn,S.,Nikravesh,M.,Zadeh,L.A.,2006.FeatureExtraction.Foundations Witten,D.M.,Shojaie,A.,Zhang,F.,2014.Theclusterelasticnetforhigh-dimensional
andApplications.In:StudiesinFuzzinesandSoftComputing,vol.207,Springer. regressionwithunknownvariablegrouping.Technometrics56(1),112â€“122.
10R.Blanqueroetal. ComputersandOperationsResearch135(2021)105456
Wolfson, J., Bandyopadhyay, S., Elidrisi, M., Vazquez-Benitez, G., Vock, D.M., Mus- Zhang, H., Jiang, L., Yu, L., 2020. Class-specific attribute value weighting for Naive
grove, D., Adomavicius, G., Johnson, P.E., Oâ€™Connor, P.J., 2015. A Naive Bayes Bayes.Inform.Sci.508,260â€“274.
machine learning approach to risk prediction using censored, time-to-event data. Zhang, M., PeÃ±a, J., Robles, V., 2009. Feature selection for multi-label naive Bayes
Stat.Med.34(21),2941â€“2957. classification.Inform.Sci.179(456),3218â€“3229.
Zhang, H., 2004. The optimality of Naive Bayes. In: Barr, V., Markov, Z. (Eds.), Zou,H.,Hastie,T.,2005.Regularizationandvariableselectionviatheelasticnet.J.
ProceedingsoftheSeventeenthInternationalFloridaArticialIntelligenceResearch R.Stat.Soc.Ser.BStat.Methodol.67(2),301â€“320.
SocietyConference.pp.562â€“567.
11