LongAlign: A Recipe for Long Context Alignment
of Large Language Models
YushiBai‚Ä°‚Ä†,XinLv¬ß,JiajieZhang‚Ä°‚Ä†,YuzeHe‚Ä°,JiQi‚Ä°‚Ä†,
LeiHou‚Ä°,JieTang‚Ä°,YuxiaoDong‚Ä°,JuanziLi‚Ä°
‚Ä°TsinghuaUniversity ¬ßZhipu.AI
Abstract
Extendinglargelanguagemodelstoeffectively
handlelongcontextsrequiresinstructionfine-
tuningoninputsequencesofsimilarlength. To
addressthis,wepresentLongAlign‚Äîarecipe
of the instruction data, training, and evalua-
tionforlongcontextalignment. First,wecon-
structalonginstruction-followingdatasetus-
ingSelf-Instruct. Toensurethedatadiversity,
itcoversabroadrangeoftasksfromvarious
long context sources. Second, we adopt the
packingandsortedbatchingstrategiestospeed
upsupervisedfine-tuningondatawithvaried
lengthdistributions. Additionally,wedevelop Figure1: TestresultsonLongBench-Chat,whichcon-
alossweightingmethodtobalancethecontri- tainsreal-worldqueriesof10k-100kinlength1.
bution to the loss across different sequences
duringpackingtraining. Third, weintroduce
theLongBench-Chatbenchmarkforevaluating However, several challenges are required to ad-
instruction-followingcapabilitiesonqueriesof
dress. First,thereisanabsenceoflonginstruction-
10k-100k in length. Experiments show that
followingdatasetsforsupervisedfine-tuning(SFT),
LongAlign outperforms existing recipes for
andbyextensionthelackofmethodsforconstruct-
LLMsinlongcontexttasksbyupto30%,while
ingsuchdata. Second, thevariedlengthdistribu-
alsomaintainingtheirproficiencyinhandling
short,generictasks. Thecode,data,andlong- tion of long-context data drastically reduces the
aligned models are open-sourced at https: trainingefficiencyoftraditionalbatchingmethods
//github.com/THUDM/LongAlign. inamulti-GPUsetup,asGPUsprocessingshorter
inputshavetostayidleuntilthosehandlinglonger
1 Introduction
inputscompletetheirtasks. Third,thereisacrucial
Largelanguagemodels(LLMs)withlargecontext need for a robust benchmark to evaluate LLMs‚Äô
windows facilitate tasks such as summarization, long-contextcapacitiesagainstreal-worldqueries.
questionansweringonlongtextandcode(Baietal., To address them, we present the LongAlign
2023a). Importantly, they may form the founda- recipe, covering data, efficient training, and eval-
tionalsupportforlife-longconversationsandcom- uation,respectively. Data-wise,toconstructadi-
plexagentscenarios(Xiaoetal.,2023;Liuetal., verselonginstruction-followingdataset,wecollect
2023). Existingworkstobuildlong-contextLLMs long sequences from nine sources and use Self-
predominantly focus on context extension (Chen Instruct(Wangetal.,2022)togenerate10kinstruc-
etal.,2023a;Xiongetal.,2023;Pengetal.,2023), tiondataof8k-64klength.
thatis,positionencodingextensionandcontinual Training-wise, to address the inefficiency un-
trainingonlongtext. der uneven batching, we adopt the packing strat-
In this work, we instead focus on the perspec- egy (Krell et al., 2021) that packs sequences to-
tive of long context alignment, i.e., instruction
fine-tuning LLMs to handle long user prompts. 1LongAlign-6B-64k,LongAlign-7B-64kandLongAlign-
13B-64karetrainedbasedonChatGLM3-6B,Llama-2-7B
‚Ä†WorkdonewhenYB,JZ,andJQinternedatZhipu.AI. andLlama-2-13B,respectively.
1
4202
naJ
13
]LC.sc[
1v85081.1042:viXragetheruptothemaximumlengthbeforedispatch- not require fine-tuning often employ techniques
ingthemtoGPUs. However,weidentifiedabias suchasslidingwindowattention(Hanetal.,2023;
in loss averaging during this packing training, as Xiao et al., 2023) or neighboring token compres-
packs containing different numbers of sequences sion(Jiangetal.,2023;Zhangetal.,2024;Jinetal.,
are assigned equal weight in the final loss calcu- 2024)tohandlethepositionalO.O.D.problemin
lation. To mitigate this bias, we propose a loss attention computation for long contexts. These
weightingstrategytobalancecontributionstothe methods,althoughcapableofextendingthecontext
loss across different sequences. In addition, we length of LLMs in a plug-and-play manner, still
introduceasortedbatchingmethodthatgroupsse- cannot match the performance of the fine-tuned
quencesofsimilarlengthstoreducetheintra-batch approaches. Prominentfine-tunedapproachesfor
idletime. longcontextscaling(Chenetal.,2023a;Pengetal.,
Evaluation-wise,wedevelopLongBench-Chat, 2023;Xiongetal.,2023;Chenetal.,2023b;Zhu
abenchmarkcompromisingopen-endedquestions etal.,2023;Fuetal.,2023)typicallyinvolveposi-
of 10k-100k length annotated by Ph.D. students. tionencodingextensionandcontinualpretraining
It covers diverse aspects of instruction-following onlongersequences.
abilitiessuchasreasoning,coding,summarization, LLM Alignment. Following the previous steps
and multilingual translation over long contexts. oflongcontextscaling,itisvitaltoalsoalignthe
GPT-4(OpenAI,2023b)isemployedtoscorethe model with instruction-following data to ensure
machine-generated responses based on our anno- thatitcaninteractwithvarioususerrequestsina
tatedgroundtruthsandfew-shotscoringexamples. chatinterface(Wangetal.,2023). Thisphase,often
ExtensiveexperimentsshowthatLongAlignef- referredtoassupervisedfine-tuningorinstruction-
fectivelyalignsmodelstohandlecontextsofupto tuning,hasbeenextensivelystudiedinshortcon-
64k tokens in length while maintaining their per- textscenarios(Wangetal.,2022;Taorietal.,2023;
formanceongeneraltaskswithoutdegradation. In Wang et al., 2023; Tunstall et al., 2023). How-
addition,wehavethefollowingfindings: ever, the introduction of long sequences presents
uniquechallengesintermsofdata,trainingmeth-
‚Ä¢ ImpactofDataQuantityandDiversity: Both
ods, and evaluation for alignment. Xiong et al.
the quantity and the diversity of the long in-
(2023) proposes generating long instruction data
structiondatasignificantlyinfluencethealigned
by concatenating short instruction data, yet their
model‚Äôsabilitytohandlelongcontexts,impact-
dataset and model weight are not open-sourced.
ingfinalperformancebyupto30%.
Ontheotherhand, whileChenetal.(2023b)has
‚Ä¢ BenefitsofLongInstructionData: Theamount madetheirlonginstructiondata,LongAlpaca-12k,
oflonginstructiondatapositivelyaffectstheper- availableandemployedLoRA(Huetal.,2022)for
formance on long-context tasks while does not efficient fine-tuning, it lacks in-depth discussion
hurtthemodels‚Äôshort-contexthandlingcapaci- andcomparativeanalysisoftheinfluenceofdata
ties. andtrainingmethodologies. Ourworkaimstofind
anoptimalsolutionforsupervised(fullparameter)
‚Ä¢ EffectivenessofTrainingStrategies: Thepack-
fine-tuningonlongcontextwithfullattention,by
ing and sorted batching strategies adopted can
tuningdata, trainingmethods, andevaluatingthe
acceleratetrainingbyover100%withoutperfor-
alignedmodelsonawiderangeoftasks.
mancecompromise. Furthermore,theproposed
lossweightingtechniqueimproveslongcontext
3 LongAlign
performanceby10%.
Inthissection,wediscussthemethodologyinLon-
2 RelatedWork
gAlign, involving the data construction process,
LongContextScaling. Longcontextscalingaims trainingmethod,andevaluationbenchmark.
to expand the limited context length of existing
3.1 Preliminary
LLMstosupportlongcontexttasks(Xiongetal.,
2023). Thecurrentmethodsforlongcontextscal- Largelanguagemodelscanlearnalignmentbysu-
ingcanbedividedintotwocategories: thosethat pervised fine-tuning on high-quality pairs of in-
requirefine-tuningorcontinualtrainingonlonger struction x and response y (Ouyang et al., 2022;
sequencesandthosethatdonot. Methodsthatdo Chungetal.,2022). Duringtraining,theinstruction
2relativelysmalleramountoflonginstructiondata
User:
In my younger and more vulnerable years my father gave me Long Doc results in a long-tail data length distribution. As
some advice that I've been turning over in my mind ever since.
‚Ä¶ showninFigure3left,themajorityofthedatafalls
G rei qv ue in r eth se u mab mo av re i zte atx it o, np l oe ra is ne t ep gr ro ap tio os ne f5 ro E mn g mli us lh ti q pu lee s pt aio rtn ss , that Task type withinthe0-8klengthrange,whiletheremaining
make sure they are diverse and cover all parts of the text, in (summary) dataisfairlyevenlydistributedinthe8k-64klength
the following format: ‚Äú1: ‚Äù, ‚Äú2: ‚Äù, ...
interval. Under this distribution, during training,
Assistant: Generated
1. Summarize the plots between Gatsby and Daisy‚Ä¶ Task& Ans a data batch typically contains mostly short data,
[{‚Äúrole‚Äù: ‚Äúuser‚Äù, ‚Äúcontent‚Äù: Long Doc + Task}, yet these batches also include a few longer texts
{‚Äúrole‚Äù: ‚Äúassistant‚Äù, ‚Äúcontent‚Äù: Answer}]
whichnecessitatemuchmorecomputationtimes,
Figure2: Dataconstructionexample. resultinginconsiderableidletimes. Tominimize
theseidletimes,themosteffectiveapproachisto
concatenate or sort the data in a manner that en-
andresponsearetypicallyconcatenatedtoforma
sures a more uniform length and computational
sequence[x,y],whichisthenprocessedthroughan
timewithineachbatch. Bearingthisinmind,we
auto-regressivelanguagemodelœÄ tomaximizethe
exploretwotrainingmethods,namelypackingand
probability P (y|x). The loss is similar to a lan-
œÄ
sortedbatching.
guagemodelingloss,whileonlyaccountingforthe
lossassociatedwiththetokensiny (targettokens):
Packing. It involves concatenating data of vary-
|y| ing lengths together until reaching the maxi-
(cid:88)
L([x,y]) = ‚àí logP (y |[x,y ]). (1) mum length. The resulting packed data, whose
œÄ i <i
i=1 lengthsaregenerallyclosetothemaximumlength,
are then batched and processed on multi-GPUs.
3.2 DatasetConstruction
Thisapproacheffectivelyminimizestheidletime
Longinstructiondatatypicallyinvolvesalongcon- within each batch, as depicted in the upper right
text material, such as a book, an extensive docu- of Figure 3. Additionally, to prevent cross-
ment, or a lengthy code, accompanied by a task contaminationbetweendifferentsequenceswithin
query that requires summarizing, reasoning, or the same pack during self-attention calculation,
computingbasedonthematerial. Duringconstruc- we pass a list containing the starting and ending
tion, we first collect long articles and documents positions of different sequences and utilize the
from9variedsources,coveringbooks,encyclope- flash_attn_varlen_funcfromFlashAtten-
dias,academicpapers,codes,etc. Wethenemploy tion2(Daoetal.,2022;Dao,2023),whichsupports
Claude2.1(Anthropic,2023)togeneratetasksand efficient computation of block diagonal attention
answersaccordingtoagivenlongcontext,asillus- (seeAppendixBformoredetails). Itrequiresless
tratedinFigure2. Tofosteradiverserangeofgen- computation and IO time compared to the tradi-
eratedtasks,weincorporatetasktypedescriptions tionaluseofa2Dattentionmask.
into the prompts, such as queries for summaries,
informationextraction,reasoning,etc. Usingthis However, we notice that the packing strategy
methodology,wecreatetasksandanswersfor10k leads to a bias towards longer sequences and se-
lengthy texts, yielding a totalof 10k instances of quences containing more target tokens. This is
superviseddata,ofwhich10%isinChinese. The becausedifferentpacks,eachcontributingequally
length of these data ranges from 8k to 64k, mea- to the final loss, contain varying numbers of se-
sured by ChatGLM tokenizer (Zeng et al., 2023) quences with different numbers of target tokens.
duetoitshighercompressionrateforChinesechar- Consequently,whencalculatingthemeanlossfor
acters. Detailsregardingthepromptsandthedata each batch, sequences in packs with fewer se-
constructionprocesscanbefoundinAppendixA. quences (typically the longer ones) or those con-
tainingmoretargettokens,haveagreaterinfluence
3.3 EfficientLong-ContextTraining
onthefinalloss. Formally,considerM sequences
Toensurethatthemodelretainstheabilitytohan- packedintoabatchofK packswherethei-thpack
dle both long and short texts (general capability) consistsofthesequenceswithindicesin[P ,P ),
i‚àí1 i
afterSFT,wemixthelonginstructiondatawitha thus it holds that P = 1,P = M +1. Let L
0 K i
general instruction dataset for training. The mix- denotethetotalsummationoflossoverN target
i
tureofalargeamountofgeneralshortdatawitha tokens in the i-th sequence. If we weigh each se-
3Packing
Length distribution block diagonal attention mask
Na√Øve batching Idle time
ùëµùíä ùë≤ # packs in the batch
√ó
Device1 ‚Ä¶ loss weighting ùëµùíäùë¥ # sequences in the batch
# target tokens in
current sequence ùëñ
Device2
Sorted batching
Device‚Ä¶ ‚Ä¶ Batch 1 Batch 2
‚Ä¶
Training time
Sequence Length
‚Ä¶ ‚Ä¶
Figure3: Underalong-taileddatalengthdistribution,packingorsortedbatchingcanreduceidletimeandspeedup
thetrainingprocess. Lossweightingisrequiredduringpackingtobalancethelosscontributionacrosssequences.
quenceequally,thelossshouldbe 3.4 LongBench-Chat
1 (cid:88)M L
i
Althoughthereareexistingbenchmarksforevalu-
L = , (2)
atingLLMs‚Äôlongcontextunderstanding(Anetal.,
M N
i
i=1 2023;Baietal.,2023a;Lietal.,2023b), theydo
whilethelosscalculatedunderpackingis
notfocusonassessingtheirinstruction-following
1
(cid:88)K P (cid:88)k‚àí1 P (cid:88)k‚àí1 capabilityunderlongcontext. Furthermore,their
L‚Ä≤ = ( L / N ) Ã∏= L. (3) relianceonautomaticmetricsforevaluationlimits
i i
K
k=1 i=P k‚àí1 i=P k‚àí1 theassessmentofalignedmodels‚Äôlongerandmore
ComparedwithEq.2,thisequatestoassigninga diverseoutputstoreal-worldqueries,andhowtheir
weightof(N /(cid:80)P k‚àí1 N )tosequencej inthe responsesalignwithhumanpreference.
j i=P i
k‚àí1
loss, i.e., in favor of sequences with more target Tothisend,weproposeLongBench-Chat,which
tokensandsequencesinsmallerpacks. Toaddress includes50longcontextreal-worldqueriesrang-
thisinequality,weproposetoscalethelossinthe ing from 10k to 100k in length, covering various
i-th sequence by K/(N M) and instead take the keyuser-intensivescenariossuchasdocumentQA,
i
sumofthescaledlossoneachpack,whichresults summarization,andcoding. Itconsistsof40tasks
inanequallosstoEq.2: inEnglishand10inChinese. Toensuretheeval-
uation truly reflects the model‚Äôs ability to follow
L‚Ä≤ =
1
(cid:88)K
(
P (cid:88)k‚àí1
L iK
) =
1
(cid:88)M
L iK
= L. longcontextinstructions,weavoidusingpopular
K N iM K N iM long texts that are likely to have been seen and
k=1 i=P k‚àí1 i=1
(4) memorized by the model during pretraining. We
Asdemonstratedinourexperimentalsection,the also avoid posing questions that the model could
lossweightingstrategyresultsina10%improve- answerwithoutreadingthelongtext.
mentindownstreamtasks. For evaluation, following previous works that
Sorted batching. We also consider an efficient haveshowntheeffectivenessofusingLLMasan
sorted batching strategy for training (lower right evaluator(Baietal.,2023b;Zhengetal.,2023;Ke
ofFigure3). Toensurethatthesequenceswithin etal.,2023),weemployGPT-4(OpenAI,2023b)
eachbatchareofsimilarlengths,wesortthedata to score the model‚Äôs response in 1-10 based on a
bylengthandselectarandomconsecutivegroup givenhuman-annotatedreferencedanswerandfew-
ofdataforeachbatch,withnorepetition. However, shotscoringexamplesforeachquestion. Weonly
thisstrategyinevitablyintroducesabiasinthedata passtheshortquery(withoutthelongdocument)
distributionacrossdifferentbatches,wherebatches to the evaluator, as currently there is no model
consist either of all long sequences or all short capableofevaluatingthequalityofresponsesunder
sequences. This can be potentially disastrous for long context inputs. To ensure that the evaluator
SGDoptimization. Inourexperiments,weobserve canmakeinformedjudgmentsbasedsolelyonthe
that sorted batching significantly accelerates the groundtruth and few-shot scoring examples, we
process without a noticeable negative impact on steerclearofoverlyopen-endedquestions,suchas
performance. This might be attributed to our use ‚ÄúWriteapoembasedontheprecedingtext‚Äù.
oflargegradientaccumulationstepsandthestrong TovalidatethereliabilityofusingGPT-4asan
adaptabilityoftheoptimizer. evaluator on LongBench-Chat, we conduct a hu-
4
rebmuNHuman GPT-4 GPT-4+Few-shot 4 Experiments
Spearman(œÅ) 0.817 0.788 0.844 In this section, we aim to answer the following
Kendall(œÑ) 0.694 0.656 0.716
researchquestionsthroughaseriesofexperiments:
RQ1. DuringSFT,howdoesthequantityanddi-
Table 1: Inter-annotator correlations; correlations be-
versity of the long instruction data influence the
tweenGPT-4(w/andw/oFew-shot)andhuman.
model‚Äôsperformanceindownstreamtasks.
RQ2. Whetherincorporatinglonginstructiondata
manevaluationstudy(moredetailsinAppendixC). duringtrainingaffectsthemodel‚Äôsgeneralcapabili-
InTable1,wepresentthecorrelationbetweenGPT- tiesandtheirinstruction-following/conversational
4‚Äôsassessmentsusingzero-shotprompting,which abilitiesinshortcontextscenarios.
involvesonlythereferencedanswer,anditsevalu- RQ3. Theimpactthatthepackingandsortedbatch-
ationswithadditionalfew-shotscoringexamples, ingtrainingmethodshaveonthetrainingefficiency
comparedtocrowdsourcedhumanjudgments. We andthefinalperformanceofthemodels.
alsoshowtheinter-annotatorcorrelationinthefirst Wealsoincorporatediscussionsonthescalability
column. We find that with few-shot prompting, of LongAlign on model size and context length,
GPT-4‚Äôs correlation with human annotations not andthelearningcurveinlongcontextalignment.
only aligns but also surpasses the level of agree-
mentamonghumanannotators,provingtherelia- 4.1 ExperimentalSetup
bility of such a metric on LongBench-Chat. We Data. Tomaintainthemodel‚Äôsgeneralcapabilities
furtherdiscoverthattheoverallaveragescores(1-
anditsproficiencyinfollowingshortinstructions,
10) obtained using GPT-4+Few-shot differ by an
weutilizeShareGPT(Chiangetal.,2023)(empty
average of 0.1 or less from the scores given by
assistant responses are filtered out) as the source
human experts. Additionally, we do not observe
of short instruction data in our training data. To
a significant bias in GPT-4‚Äôs scoring towards the
compare the impact of different aspects of long
length of responses ‚Äî in fact, it even penalizes
instructiondataonmodeltraining,weincorporate
excessivelylengthyresponses.
the following four suites of long instruction data
Leaderboard. Figure1reportsthetestresultsof in our experiment. ‚ÄòLongAlign-0k‚Äô, ‚ÄòLongAlign-
currentlongcontext(16k+)instructionfine-tuned 5k‚Äô,and‚ÄòLongAlign-10k‚Äô: 0,5k,and10kinstances
models (chat models) and our most competent
of LongAlign data, constructed according to the
models trained with LongAlign on LongBench- procedureinSec3.2;‚ÄòLongAlpaca-12k‚Äô: 12kdata
Chat. We include API-based Commercial mod-
fromtheLongAlpacadataset(Chenetal.,2023b).
els: GPT-4-1106-preview(OpenAI,2023a)(GPT-
LongAlpacaincludes9klongQAdataand3kshort
4 Turbo), GLM-4-128k2, and Claude-2.1 (An-
QA data, where the long QA data is generated
thropic, 2023); as well as open-sourced models:
based only on academic papers and books, offer-
InternLM2-7b-200k,InternLM2-20b-200k(Team,
inglessdiversitycomparedtoourLongAligndata.
2023),ChatGLM3-6B-32k(Duetal.,2022;Zeng
We use this dataset to compare the impact of the
et al., 2023), Vicuna-7b-v1.5-16k (Zheng et al.,
diversityoflonginstructiondataonmodeltraining.
2023), Orion-14b-LongChat (Chen et al., 2024),
Model. Weincludethreemodelvariants,namely
LongChat-7b-v1.5-32k (Li et al., 2023a), and
ChatGLM3-6B(Duetal.,2022;Zengetal.,2023),
Mixtral-8x7b-Instruct-v0.2 (Jiang et al., 2024).
Llama-2-7B, and Llama-2-13B (Touvron et al.,
Notethatweemploymiddletruncationforinputs
2023)(allbasemodels). Giventheir8kand4kcon-
surpassingthemodel‚Äôscontextwindow. Ourevalu-
textwindows, wefirstperformcontextextension
ationresultrevealsthattheperformanceofcurrent
toextendtheircontextwindowto64k,resultingin
open-sourcedmodelsstillsignificantlylagsbehind
ChatGLM3-6B-64k,Llama-2-7B-64k,andLlama-
commercial models, which partially attributed to
2-13B-64k. Thisinvolvesexpandingthebasefre-
thescaledifferencebetweenthesemodels. Addi-
quencyboftheRoPEpositionencoding(Suetal.,
tionally, we observe that models with a context
2024)by200times(from10,000to2,000,000)and
length of 32k or less tend to underperform on
continualtrainingonpretrainingdatawithlengths
LongBench-Chat,indicatingthatalongercontext under64k,foratotalof10billiontokens3.
windowisnecessarytocompletetheselongtasks.
3Continualtrainingon10Btokensissufficientforcontext
2https://open.bigmodel.cn/pricing extension,assuggestedinFuetal.(2023).
5TrainingData LongTasks ShortTasks
(Long) LongBench-Chat S-DocQA M-DocQA Summ MT-Bench ARC HellaSwag TruthfulQA MMLU
LongAlign-0k 3.73 58.7 41.1 38.4 5.34 50.3 74.7 51.6 45.5
LongAlign-5k 5.97 61.8 42.1 42.0 5.51 50.3 75.1 52.5 46.6
LongAlign-10k 6.21 64.0 44.4 44.2 5.5 50.5 74.9 52.5 45.5
LongAlpaca-12k 4.46 65.8 45.6 44.1 4.93 51.5 75.4 53.2 47.1
Table2: PerformanceofChatGLM3-6B-64kaftertrainingondifferentquantitiesandtypesoflonginstructiondata.
SFT on 0k long data SFT on 5k long data Benchisabilingual,multi-tasklongcontextbench-
mark. We conduct evaluations on three types of
tasks within it: Single-Doc QA, Multi-Doc QA,
andSummarization. Sincethealignedmodelstypi-
callyproducelongerresponses,insteadofusingthe
originalmetrics(ROUGE,F1)toscorethemodels‚Äô
replies,weuseGPT-4toratethemodel‚Äôsoutputs
SFT on 10k long data SFT on 12k longalpacadata basedontheiralignmentwiththegroundtruthan-
swersonLongBench. Forshortcontexttasks,we
use MT-Bench (Zheng et al., 2023), a multi-turn
chatbenchmark,tomeasurethemodels‚Äôabilityto
followshortinstructions. Wealsoevaluateonthe
generaltasksonOpenLLMLeaderboard(Beech-
ing et al., 2023), including ARC (Clark et al.,
2018), HellaSwag (Zellers et al., 2019), Truthful
Figure 4: 1k-60k Needle test performance of Chat- QA (Lin et al., 2022), and MMLU (Hendrycks
GLM3-6B-64ktrainedondifferentsuitesoflongdata et al., 2021). We follow the evaluation settings
mixedwithShareGPT.
in the Open LLM Leaderboard and utilize lm-
evaluation-harness framework (Gao et al., 2023)
forevaluationonthesetasks. Toensurethemost
Training. All models are trained with 8xA800
stable evaluation results, we use GPT-4 to score
80GGPUsandDeepSpeed+ZeRO3+CPUoffload-
twiceonLongBench-ChatandMT-Bench,andav-
ing(Rasleyetal.,2020). Themodelscanbetrained
eragethesescorestoobtainthefinalscore.
withamaximumlengthof64ktokenswithoutGPU
memoryoverflow. Consequently,wesetthemax-
4.2 InfluenceofData
imumlengthofthetrainingdatato64k,withany
We conduct SFT on ChatGLM3-6B-64k using
dataexceedingthislengthbeingtruncatedfromthe
ShareGPTdatamixedwithdifferentsuitesoflong
right. Forpackingtraining,eachpackconsistsof
instructiondata. AllmodelsexceptLongAlign-0k
12sequencesonaverage,wesetthetotalbatchsize
aretrainedusingthemoreefficientpackingstrat-
to 8, resulting in a global batch size of 96. For a
egywithlossweighting. Theevaluationresultsare
fair comparison, we set the batch size to 8, with
reportedinTable2. ForLongBench-ChatandMT-
a gradient accumulation step of 12 for other non-
Bench,thereportedresultsareaveragedoverGPT-
packing training methods. We train 2 epochs on
4‚Äôs rating (1-10) across all test instances, while
thetrainingdata(approximately1500-2000steps).
resultsonotherdatasetsarenormalizedbetween0-
Evaluation. We involve both long context tasks
100. Wealsoconductthe‚ÄúNeedleinAHayStack‚Äù
andshortcontexttasksinevaluation. Inbothlong
experiment4(resultvisualizationinFigure4)totest
and short scenarios, we consider tasks that eval-
themodel‚Äôsabilitytoutilizeinformationfrom10
uatetheinstruction-followingandconversational
differentpositionswithinlongcontextsofvarying
abilities, as well as tasks that assess general ca-
lengthsbetween1k-60k. Specifically,thistaskasks
pabilities. For long context tasks, we use our
forthemodeltoretrieveapieceoffact(the‚Äònee-
proposed LongBench-Chat to evaluate the mod-
dle‚Äô)thatisinsertedinthemiddle(positionedata
els‚Äôlongcontextalignmentproficiencyandemploy
specifieddepthpercent)ofalongcontextwindow
LongBench(Baietal.,2023a)totestthemodel‚Äôs
generallongcontextunderstandingabilities. Long- 4https://github.com/gkamradt/LLMTest_NeedleInAHaystack
6LongTasks ShortTasks
TrainingMethod
LongBench-Chat S-DocQA M-DocQA Summ MT-Bench ARC HellaSwag TruthfulQA MMLU
ChatGLM3-6B-64k
Na√Øvebatching 5.87 65.4 45.0 44.8 5.61 50.7 74.7 52.8 46.0
Sortedbatching 5.4 66.2 46.3 43.7 5.76 51.3 74.8 51.9 46.3
Packing 5.76 65.0 45.1 42.8 5.64 50.9 74.8 50.5 47.2
Packing+lossweighting 6.21 64.0 44.4 44.2 5.5 50.5 74.9 52.5 45.5
Llama-2-7B-64k
Na√Øvebatching 5.95 62.8 42.7 41.6 5.52 48.9 74.8 45.3 43.6
Sortedbatching 6.38 63.4 42.2 41.3 5.51 49.5 74.8 48.0 44.3
Packing 5.89 61.7 40.4 42.0 5.58 48.1 74.9 46.1 43.9
Packing+lossweighting 6.10 60.8 41.3 43.1 5.60 48.4 74.5 47.4 43.3
Table3: PerformanceofChatGLM3-6B-64kandLlama-2-7B-64kunderdifferenttrainingmethods.
(the‚Äòhaystack‚Äô). Wesummarizeourkeyfindings
ontheinfluenceofdataasfollows.
120 Na√Øve batching 117.2
Packing
100 Sorted batching
1. Morelonginstructiondataenhancestheper-
80
formance in long tasks, and without compro- 67.2
60
misingtheperformanceinshorttasks. Compar- 45.4 41.244.5
40
ingtheperformanceofLongAlign-0k,LongAlign-
20.519.1 23.423.3
20
5k, and LongAlign-10k, we observe that as the
amount of long instruction data increases, there 0
ChatGLM3-6B-64k Llama-2-7B-64k Llama-2-13B-64k
is a consistent improvement in the model‚Äôs per-
Figure 5: Training time (hrs) on 8xA800 80G GPUs
formance across all long tasks. Meanwhile, in-
underdifferenttrainingmethods.
triguingly,itsperformanceonshorttasksremains
comparable to when it is trained solely on short
instructions. Additionally,giventheinferiorperfor- 4.3 ImpactofTrainingMethods
manceofLongAlign-0kinlongtasks(especiallyon
We compare different training methods on
LongBench-Chat),thisalsoindicatesthatmerely
ChatGLM3-6B-64kandLlama-2-6B-64k,includ-
performingcontextextensiononthebasemodelis
ing na√Øve batching, packing (w/ and w/o loss
insufficienttoensuregoodperformanceondown-
weighting), and sorted batching, to assess their
streamlongtasks. Itisnecessarytoincorporatea
impact on training efficiency, as well as their in-
substantial amount of long data covering various
fluence on downstream task performance.5 All
lengths during SFT. Moreover, the needle test re-
models are trained on LongAlign-10k. Figure 5
sultalsosuggeststhatmorelongdataenhancesthe
displaysacomparisonofthetrainingtimerequired
model‚Äôsabilitytoutilizeinformationfromdifferent
foreachmethod. Table3presentstheperformance
positionswithinlongtexts,resultinginadecrease
ondownstreamtasks. Ourfindingsareasfollows.
ofthemodel‚Äôsretrievalerror.
1. Packing and sorted batching double the
training efficiency while exhibiting good per-
2. Diversity of long instruction data is benefi-
formance. From Figure 5, we can see that the
cial for the model‚Äôs instruction-following abil-
training efficiency of packing and sorted batch-
ities. LongAlign-10k shows significantly better
ing is comparable, both requiring less than half
results in long and short instruction-following
the time needed under na√Øve batching. Addition-
tasks(LongBench-ChatandMTBench),compared
ally,accordingtotable3,modelstrainedwiththe
to LongAlpaca-12k. Meanwhile, LongAlpaca-
twoefficientmethodsperformcomparablytothose
12kslightlyoutperformsLongAlign-10konLong-
trainedwithna√Øvebatchingonbothlongandshort
Bench. This is primarily due to its superior per-
tasks. Wealsofindthattheeffectivenessofthese
formanceonthe2WikiMQA(Hoetal.,2020)and
twotrainingmethodsvarieswithdifferentmodels.
NarrativeQA(KocÀáisky` etal.,2018)datasets,which
arebasedonWikipediaandnovels,bearingmore 5Na√ØvebatchingandsortedbatchingconsumemoreGPU
memorycomparedtopacking, duetotheiruseofgradient
resemble to the source of the instruction data in
accumulation.Wetruncatealldatato56klengthforChatGLM
LongAlpaca-12k. withthesetwomethodstoensurenoGPUmemoryoverflow.
7
)h(
emit
gniniarTLlama-2-13B-64k LongBench-Chat S-DocQA M-DocQA Summ MT-Bench
Packing+lossweighting 6.79 68.0 40.3 43.6 6.12
Sortedbatching 7.02 66.1 43.9 45.3 6.02
Table4: Scalingup: LongAlignonLLama-2-13B.
Forinstance,themodeltrainedonChatGLM3-6B
1.0
usingpacking+lossweightingshowssignificantly
betterperformanceonLongBench-Chat,whereas 0.8
sortedbatchingperformsthebestforLlama-2-7B.
0.6
2. Loss weighting significantly improves per-
formanceonlonginstructiontaskforpacking 0.4
LongBench-Chat
training. Bycomparingtheperformanceofmod-
0.2 MT-Bench
elswithandwithoutlossweightingstrategyduring
0 250 500 750 1000 1250 1500 1750
packingtraining,it‚Äôsevidentthatincorporatingthe Training steps
loss weighting strategy greatly improves the ca-
Figure6: Relativeperformanceonlongandshorttasks
pabilityinLongBench-Chat(byabout5%‚àº10%), throughoutthetrainingprocessofChatGLM3-6B-64k.
whilehavingaminimalandvariableimpactonthe
performanceofothertasks. Webelievethatthisis
training,illustratinghowperformancevarieswith
primarilybecause,withoutlossweightinginSFT
the number of training steps. We use exponen-
data,differentlonginstructiondatacontributevari-
tialmovingaveragetosmooththeoriginalperfor-
ably to the loss ‚Äî longer data tend to contribute
mance curves (dotted lines), and display them as
more to the loss (refer to Eq. 3). Such an unnat-
solidlines. Weobservethatthetrendsofthetwo
ural weighting bias is often detrimental to model
learningcurvesarestrikinglysimilar‚Äîbothshow
training,potentiallyleadingtotraininginstability,
rapidimprovementbetween0-500steps,followed
deviatingitfromtheoptimallearningtrajectory.
byaslowrise,andstabilizeafter1000steps. This
4.4 Discussion mayimplyadeeperconnectionbetweenlongand
shortalignment. Theymightbejointlydetermined
ScalabilityofLongAlign. Weexploretwoscaling
bysharedlatentfactors,whichareoptimizeddur-
directions on our LongAlign framework: larger
ing training to help the model align to both long
modelsizeandlongercontextwindow. Todoso,
andshortinstructionssimultaneously.
we fine-tune Llama-2-13B-64k using LongAlign-
In Appendix D, we provide case analyses
10k dataset with the two efficient training meth-
of different LongAlign-tuned models on out-of-
ods, and the evaluation results are shown in Ta-
distribution (OOD) long context query, that is,
ble 4. Compared to the 7B-scale model, the 13B
querythatthemodelshavenotencounteredinthe
modelshowsa10%improvementonLongBench-
longcontextSFTdata. Wefindthatmodelstrained
Chat, setting a new record among open-sourced
withLongAligncangeneralizetoOODlongcon-
models (LongAlign-13B-64k in Figure 1). This
textqueries,suchaswritingareviewforaresearch
indicates that our alignment method scales effec-
paper, andthatlarger-scalemodelshavestronger
tively to larger-scale models. We also construct
generalizationcapabilities.
SFTdataupto128kinlengthwithhumanannota-
tion and successfully align ChatGLM3-6B under
5 Conclusion
128kcontextwindowusingpackingtrainingwith
loss weighting, resulting in ChatGLM3-6B-128k This paper aims to find the best practice for long
(performanceshowninFigure1). context alignment in the scope of data, training
Learningcurveonlongtaskv.s. shorttask. To method, and evaluation. Our proposed solution,
comparethelearningprocessesofalignmentunder namelyLongAlign,usesSelf-Instructtoconstruct
longcontextandshortcontext,wepresentinFig- diverse long instruction data, and efficiently fine-
ure6therelativeperformancecurvesonlongand tune the model with packing combined with loss
shortinstruction-followingtasks(onLongBench- weighting or sorted batching. Moreover, we in-
Chat and MT-Bench, respectively) during model troduce LongBench-Chat to facilitate reliable as-
8
erocs
laniF
/
erocSsessment of LLM‚Äôs instruction-following ability PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
on practical long context interactions. Through AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord.2018. Thinkyouhavesolvedquestionan-
controlled experiments, we find that the amount,
swering? tryarc,theai2reasoningchallenge. arXiv
diversity of data, as well as the correct training
preprintarXiv:1803.05457.
method,arecrucialtothefinalperformance.
TriDao.2023. FlashAttention-2: Fasterattentionwith
betterparallelismandworkpartitioning.
References
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, andChristopherR√©.2022. FlashAttention: Fastand
JunZhang,LingpengKong,andXipengQiu.2023. memory-efficientexactattentionwithIO-awareness.
L-eval: Institutingstandardizedevaluationforlong InAdvancesinNeuralInformationProcessingSys-
contextlanguagemodels. tems.
Anthropic.2023. Anthropic: Introducingclaude2.1. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
JiezhongQiu,ZhilinYang,andJieTang.2022. Glm:
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Generallanguagemodelpretrainingwithautoregres-
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao siveblankinfilling. InProceedingsofthe60thAn-
Liu,AohanZeng,LeiHou,YuxiaoDong,JieTang, nualMeetingoftheAssociationforComputational
and Juanzi Li. 2023a. Longbench: A bilingual, Linguistics(Volume1: LongPapers),pages320‚Äì335.
multitaskbenchmarkforlongcontextunderstanding.
arXivpreprintarXiv:2308.14508. Yao Fu, Xinyao Niu, Xiang Yue, Rameswar Panda,
YoonKim,andHaoPeng.2023. Understandingdata
YushiBai, JiahaoYing, YixinCao, XinLv, YuzeHe, influenceoncontextscaling. YaoFu‚ÄôsNotion.
XiaozhiWang,JifanYu,KaishengZeng,YijiaXiao,
HaozheLyu,etal.2023b. Benchmarkingfoundation LeoGao,StellaBiderman,SidBlack,LaurenceGold-
modelswithlanguage-model-as-an-examiner. arXiv ing,TravisHoppe,CharlesFoster,JasonPhang,Ho-
preprintarXiv:2306.04181. raceHe, AnishThite, NoaNabeshima, etal.2020.
The pile: An 800gb dataset of diverse text for lan-
Edward Beeching, Cl√©mentine Fourrier, Nathan guagemodeling. arXivpreprintarXiv:2101.00027.
Habib, Sheon Han, Nathan Lambert, Nazneen
Rajani, Omar Sanseviero, Lewis Tunstall, and LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,
Thomas Wolf. 2023. Open LLM leader- SidBlack,AnthonyDiPofi,CharlesFoster,Laurence
board. https://huggingface.co/spaces/ Golding,JeffreyHsu,AlainLeNoac‚Äôh,HaonanLi,
HuggingFaceH4/open_llm_leaderboard. KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Aviya Skowron, Lintang Sutawika, Eric Tang, An-
YongqiangLiu, HaihuiPan, LeichaoXu, Dacheng ishThite, BenWang, KevinWang, andAndyZou.
Zhang,ZhipengZhang,andKunHan.2024. Orion- 2023. A framework for few-shot language model
14b: Open-sourcemultilinguallargelanguagemod- evaluation.
els. arXivpreprintarXiv:2401.12246.
ChiHan,QifanWang,WenhanXiong,YuChen,Heng
ShouyuanChen,ShermanWong,LiangjianChen,and Ji, and Sinong Wang. 2023. Lm-infinite: Simple
YuandongTian.2023a. Extendingcontextwindow on-the-fly length generalization for large language
oflargelanguagemodelsviapositionalinterpolation. models. arXivpreprintarXiv:2308.16137.
arXivpreprintarXiv:2306.15595.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, MantasMazeika,DawnSong,andJacobSteinhardt.
ZhijianLiu, SongHan, andJiayaJia.2023b. Lon- 2021. Measuringmassivemultitasklanguageunder-
glora: Efficientfine-tuningoflong-contextlargelan- standing. InInternationalConferenceonLearning
guagemodels. arXivpreprintarXiv:2309.12307. Representations.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, XanhHo,Anh-KhoaDuongNguyen,SakuSugawara,
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan andAkikoAizawa.2020. Constructingamulti-hop
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion qadatasetforcomprehensiveevaluationofreasoning
Stoica, and Eric P. Xing. 2023. Vicuna: An open- steps. InProceedingsofthe28thInternationalCon-
sourcechatbotimpressinggpt-4with90%*chatgpt ferenceonComputationalLinguistics,pages6609‚Äì
quality. 6625.
HyungWonChung,LeHou,ShayneLongpre,Barret Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
Wang,MostafaDehghani,SiddharthaBrahma,etal. WeizhuChen.2022. LoRA:Low-rankadaptationof
2022. Scalinginstruction-finetunedlanguagemodels. largelanguagemodels. InInternationalConference
arXivpreprintarXiv:2210.11416. onLearningRepresentations.
9Albert Q Jiang, Alexandre Sablayrolles, Antoine 2022. Training languagemodelsto followinstruc-
Roux,ArthurMensch,BlancheSavary,ChrisBam- tions with human feedback. Advances in Neural
ford,DevendraSinghChaplot,DiegodelasCasas, InformationProcessingSystems,35:27730‚Äì27744.
Emma Bou Hanna, Florian Bressand, et al. 2024.
Mixtralofexperts. arXivpreprintarXiv:2401.04088. BowenPeng,JeffreyQuesnelle,HongluFan,andEn-
ricoShippole.2023. Yarn: Efficientcontextwindow
HuiqiangJiang,QianhuiWu,XufangLuo,Dongsheng extensionoflargelanguagemodels. arXivpreprint
Li,Chin-YewLin,YuqingYang,andLiliQiu.2023. arXiv:2309.00071.
Longllmlingua: Accelerating and enhancing llms
in long context scenarios via prompt compression. JeffRasley,SamyamRajbhandari,OlatunjiRuwase,and
arXivpreprintarXiv:2310.06839. Yuxiong He. 2020. Deepspeed: System optimiza-
tionsenabletrainingdeeplearningmodelswithover
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng 100billionparameters. InProceedingsofthe26th
Jiang,ZiruiLiu,Chia-YuanChang,HuiyuanChen, ACMSIGKDDInternationalConferenceonKnowl-
andXiaHu.2024. Llmmaybelonglm: Self-extend edgeDiscovery&DataMining,pages3505‚Äì3506.
llmcontextwindowwithouttuning. arXivpreprint
arXiv:2401.01325. Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
PatrickLeGresley, JaredCasper, andBryanCatan-
PeiKe,BosiWen,ZhuoerFeng,XiaoLiu,XuanyuLei, zaro. 2019. Megatron-lm: Training multi-billion
JialeCheng,ShengyuanWang,AohanZeng,Yuxiao parameterlanguagemodelsusingmodelparallelism.
Dong, Hongning Wang, et al. 2023. Critiquellm: arXivpreprintarXiv:1909.08053.
Scaling llm-as-critic for effective and explainable
JianlinSu, MurtadhaAhmed, YuLu, ShengfengPan,
evaluationoflargelanguagemodelgeneration. arXiv
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
preprintarXiv:2311.18702.
hancedtransformerwithrotarypositionembedding.
Tom√°≈°KocÀáisky`,JonathanSchwarz,PhilBlunsom,Chris Neurocomputing,568:127063.
Dyer,KarlMoritzHermann,G√°borMelis,andEd-
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
ward Grefenstette. 2018. The narrativeqa reading
Dubois, Xuechen Li, Carlos Guestrin, Percy
comprehensionchallenge. TransactionsoftheAsso-
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ciationforComputationalLinguistics,6:317‚Äì328.
ford alpaca: An instruction-following llama
MarioMichaelKrell,MatejKosec,SergioPPerez,and model. https://github.com/tatsu-lab/
AndrewFitzgibbon.2021. Efficientsequencepack- stanford_alpaca.
ingwithoutcross-contamination: Acceleratinglarge
InternLM Team. 2023. Internlm: A multilingual
language models without impacting performance.
languagemodelwithprogressivelyenhancedcapa-
arXivpreprintarXiv:2107.02027.
bilities. https://github.com/InternLM/
DachengLi,RulinShao,AnzeXie,YingSheng,Lian-
InternLM.
minZheng,JosephE.Gonzalez,IonStoica,Xuezhe
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Ma, and Hao Zhang. 2023a. How long can open-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
sourcellmstrulypromiseoncontextlength?
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
JiaqiLi,MengmengWang,ZilongZheng,andMuhan Bhosale, et al. 2023. Llama 2: Open founda-
Zhang.2023b. Loogle: Canlong-contextlanguage tion and fine-tuned chat models. arXiv preprint
models understand long contexts? arXiv preprint arXiv:2307.09288.
arXiv:2311.04939.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
StephanieLin,JacobHilton,andOwainEvans.2022.
Shengyi Huang, Leandro von Werra, Cl√©mentine
Truthfulqa: Measuring how models mimic human
falsehoods. InProceedingsofthe60thAnnualMeet- Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
ingoftheAssociationforComputationalLinguistics rect distillation of lm alignment. arXiv preprint
(Volume1: LongPapers),pages3214‚Äì3252.
arXiv:2310.16944.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
XiaoLiu,HaoYu,HanchenZhang,YifanXu,Xuanyu
Hessel,TusharKhot,KhyathiChandu,DavidWad-
Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen
den,KelseyMacMillan,NoahA.Smith,IzBeltagy,
Men,KejuanYang,etal.2023. Agentbench:Evaluat-
andHannanehHajishirzi.2023. Howfarcancamels
ingllmsasagents. arXivpreprintarXiv:2308.03688.
go? exploringthestateofinstructiontuningonopen
OpenAI.2023a. Newmodelsanddeveloperproducts resources. InThirty-seventhConferenceonNeural
announcedatdevday. InformationProcessingSystemsDatasetsandBench-
marksTrack.
OpenAI.2023b. Openai: Gpt-4.
YizhongWang,YeganehKordi,SwaroopMishra,Alisa
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida, Liu,NoahA.Smith,DanielKhashabi,andHannaneh
CarrollWainwright,PamelaMishkin,ChongZhang, Hajishirzi. 2022. Self-instruct: Aligning language
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. modelwithselfgeneratedinstructions.
10Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient streaming
languagemodelswithattentionsinks. arXivpreprint
arXiv:2309.17453.
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
Rungta,KarthikAbinavSankararaman,BarlasOguz,
etal.2023. Effectivelong-contextscalingoffounda-
tionmodels. arXivpreprintarXiv:2309.16039.
Liang Xu, Xuanwei Zhang, and Qianqian Dong.
2020. Cluecorpus2020: A large-scale chinese cor-
pusforpre-traininglanguagemodel. arXivpreprint
arXiv:2003.01355.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machinereallyfinishyoursentence? InProceedings
of the 57th Annual Meeting of the Association for
ComputationalLinguistics,pages4791‚Äì4800.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
WendiZheng,XiaoXia,etal.2023. Glm-130b: An
openbilingualpre-trainedmodel. InTheEleventhIn-
ternationalConferenceonLearningRepresentations.
PeitianZhang, ZhengLiu, ShitaoXiao, NingluShao,
QiweiYe,andZhichengDou.2024. Soaringfrom
4kto400k: Extendingllm‚Äôscontextwithactivation
beacon. arXivpreprintarXiv:2401.03462.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXivpreprintarXiv:2306.05685.
DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
haoWu,FuruWei,andSujianLi.2023. Pose: Effi-
cientcontextwindowextensionofllmsviapositional
skip-wisetraining.
11A DatasetConstructionDetails
Datasources. The9sourcesofthedocumentsinourconstructedLongAligndatasetarelistedbelow6,
alongwiththeircopyrightinformation:
‚Ä¢ Arxiv(Academicpapers): Open-accessedandcanbedownloadedfreelybyanyone.
‚Ä¢ Books3(Books): FromThePile,currentlyitisnotlicensedtobedownloaded.
‚Ä¢ C4Dataset(Varioustypesofarticles): PubliclyavailabledatasetwithODC-BYlicense.
‚Ä¢ CLUECorpus2020(VarioustypesofChinesearticles): ExtractedChineseinstancesfromtheCommon-
CrawlcorpusbyXuetal.(2020).
‚Ä¢ CommonCrawlcorpus(Varioustypesofarticles): Publiclyavailabledatasetandcanbedownloaded
freelybyanyone.
‚Ä¢ Github(Coderepositories): Open-accessedandcanbedownloadedfreelybyanyone.
‚Ä¢ StackExchange(Question-and-answerwebsites): FreelydownloadableandlicensedunderCCBY-SA.
‚Ä¢ Wikipedia(Encyclopedias): GrantfreeaccessandlicensedunderCCBY-SA.
‚Ä¢ WuDaoCorpora(Varioustypesofarticles): open-accesseddataset.
Wesamplearticleswithlengthsunder64k(measuredbyChatGLM3-6Btokenizer)fromthesedatasets.
Notethatweupsamplelongerarticlestoensureourdatasetcoversmorelongtexts.
Promptsfordatageneration. Duringthedatagenerationprocess,weemployfourtypesoftaskprompts
toencourageClaudetoproduceamorediversesetofinstructiondata:
‚Ä¢ Generaltypetask
{LongDoc}
Given the above text, please propose 5 English questions that are diverse and cover all
partsofthetext,inthefollowingformat: "1: ","2: ",...
‚Ä¢ Summarytypetask
{LongDoc}
Given the above text, please propose 5 English questions that require summarization or
integrationfrommultipleparts,makesuretheyarediverseandcoverallpartsofthetext,inthe
followingformat: "1: ","2: ",...
‚Ä¢ Reasoningtypetask
{LongDoc}
Given the above text, please propose 5 English questions that require multi-hop reason-
ing,makesuretheyarediverseandcoverallpartsofthetext,inthefollowingformat: "1: ","2: ",
...
‚Ä¢ Informationextractiontypetask
{LongDoc}
Given the above text, please propose 5 English information-seeking questions, make sure
theyarediversedandcoverallpartsofthetext,inthefollowingformat: "1: ","2: ",...
6Arxiv,Books3,CC,Github,StackExchange,andWikipediaaresampledfromThePile(Gaoetal.,2020).
12Foreachlongarticle,werandomlyselectoneofthefourtaskpromptsandhaveClaudegeneratefive
questionstoensurethatthequestionscovercontentfrommultiplespanswithinthelongtext. Wethen
randomlychooseoneofthesequestionsandrequestClaudeforitsanswer,resultingininstructiondataas
illustratedinFigure2. ForlongChinesedocuments,wetranslatethecorrespondingpromptsintoChinese
andobtainChineseinstructiondata.
B TrainingMethodDetails
Hereweprovidedetailsregardingtheimplementationofthepackingstrategyandlossweighting. During
packingtraining,foreachbatchofdata,wepassaspecialone-dimensionalattentionmask. Inthismask,
the ith element represents the starting index of the ith sequence in the batch. The first element of the
maskis0,andthelastelementisequaltobatch_size√óseq_len. Duringtheattentioncomputation,we
usetheflash_attn_varlen_funcfunctionfromFlashAttention2andpasstheattentionmaskto
the function‚Äôs cu_seqlens_q and cu_seqlens_k parameters. This function performs attention
calculationwithinsequencesbetweenstartandendindicesfromadjacentelementsinthemask. Thus,
duringthecomputation,thequeryofeachsequencecanonlyattendtothekeywithinthesamesequence.
Forimplementationofthelossweightingstrategy,wefirstpreprocessthetrainingdatatoproducea
weighted1Dmaskforeachpackofsequences,wheretheweightissetto1/N (N isthenumberoftarget
tokensincurrentsequence)onthepositioncorrespondingtotargettokens,otherwise0. Duringtraining,
wesetM andK,i.e.,thenumberofsequencesandpacksinthecurrentbatch,ontheflyaccordingtoits
configuration. Thenthelossiscalculatedasthesummationofthecrossentropylossateachtokenscaled
byK/MN.
C EvaluationDetails
C.1 LongBench-Chat
Evaluationdata. 30questiondatainLongBench-Chatareproposedbyourauthorteamtobestmimic
real user queries, these include 20 English and 10 Chinese questions. The remaining 20 questions in
LongBench-ChatareselectedfromlongdependencyQAtasksintheLooGLEdataset(Lietal.,2023b).
ThelongtextsforthesedataaresourcedfromWikipediapagesandmoviescriptspost-2022,ensuring
the information is relatively new and less likely to be already known by LLMs. We aim to select
questionsthatresemblerealuserinquiries,canbeansweredfromthetext,andensureadiversetypeof
questions(includingComprehension&Reasoning,MultipleInformationRetrieval,TimelineReorder,and
Computationtypes). ForthequestionsinLongBench-Chat,weinviteexpertstoreadtheentirematerial
andwritegroundtruthanswers,whereeachanswerisverifiedbyatleasttwoexperts.
Evaluation prompts. For each question, we manually score on three responses as few-shot scoring
examples,shuffletheirorderineachevaluationrunandusethefollowingprompttogetGPT-4‚Äôsevaluation:
[Instructions]YouareaskedtoevaluatethequalityoftheAIassistant‚Äôsanswerstouserquestionsas
animpartialjudge,andyourevaluationshouldtakeintoaccountfactorsincludingcorrectness(high
priority),helpfulness,accuracy,andrelevance. Thescoringprinciplesareasfollows: 1. Readthe
AIassistant‚Äôsanswerandcomparetheassistant‚Äôsanswerwiththereferenceanswer. 2. Identifyall
errorsintheAIAssistant‚Äôsanswersandconsiderhowmuchtheyaffecttheanswertothequestion. 3.
EvaluatehowhelpfultheAIassistant‚Äôsanswersareindirectlyansweringtheuser‚Äôsquestionsand
providingtheinformationtheuserneeds. 4. ExamineanyadditionalinformationintheAIassistant‚Äôs
answertoensurethatitiscorrectandcloselyrelatedtothequestion. Ifthisinformationisincorrect
ornotrelevanttothequestion,pointsshouldbedeductedfromtheoverallscore.
Please give an overall integer rating from 1 to 10 based on the above principles, strictly in the
followingformat:"[[rating]]",e.g. "[[5]]".
[Question]{}
[Referenceanswerbegins]{}[Referenceanswerends]
Belowareseveralassistants‚Äôanswersandtheirratings:
[Assistant‚Äôsanswerbegins]{}[Assistant‚Äôsanswerends]
13Rating: [[{}]]
[Assistant‚Äôsanswerbegins]{}[Assistant‚Äôsanswerends]
Rating: [[{}]]
[Assistant‚Äôsanswerbegins]{}[Assistant‚Äôsanswerends]
Rating: [[{}]]
Pleaseratethefollowingassistantanswersbasedonthescoringprinciplesandexamplesabove:
[Assistant‚Äôsanswerbegins]{}[Assistant‚Äôsanswerends]
Rating:
Hereisthezero-shotpromptusedasthebaselineinourmetricevaluationstudy:
[Instructions]YouareaskedtoevaluatethequalityoftheAIassistant‚Äôsanswerstouserquestionsas
animpartialjudge,andyourevaluationshouldtakeintoaccountfactorsincludingcorrectness(high
priority),helpfulness,accuracy,andrelevance. Thescoringprinciplesareasfollows: 1. Readthe
AIassistant‚Äôsanswerandcomparetheassistant‚Äôsanswerwiththereferenceanswer. 2. Identifyall
errorsintheAIAssistant‚Äôsanswersandconsiderhowmuchtheyaffecttheanswertothequestion. 3.
EvaluatehowhelpfultheAIassistant‚Äôsanswersareindirectlyansweringtheuser‚Äôsquestionsand
providingtheinformationtheuserneeds. 4. ExamineanyadditionalinformationintheAIassistant‚Äôs
answertoensurethatitiscorrectandcloselyrelatedtothequestion. Ifthisinformationisincorrect
ornotrelevanttothequestion,pointsshouldbedeductedfromtheoverallscore.
Please give an overall integer rating from 1 to 10 based on the above principles, strictly in the
followingformat:"[[rating]]",e.g. "[[5]]".
[Question]{}
[Referenceanswer]{}
[Assistant‚Äôsanswer]{}
Rating:
Humanevaluation. HereweprovidemoredetailsforthehumanevaluationstudyonLongBench-Chat.
Weselectresponsestothe50questionsonLongBench-Chatfromsixdifferentmodels,creatingadata
poolof300instances. Weinvitetwohumanexperts(botharePh.D.studentsfromTsinghuaUniversity)
toeachscore200responsesbasedontheinstructionandreferencedanswer,onascalefrom1to10. The
scoringcriteriaprovidedtothehumanexpertsareasfollows:
Pleasescoretheassistant‚Äôsresponsebasedonthequestionandthereferenceanswer,with1
beingthelowestand10thehighest. Theannotationmustadheretothefollowingrequirements:
1. Focusprimarilyonwhethertheresponsecoversthekeypointsinthereferenceanswer.
2. Forreferenceanswerscontainingmultiplekeypoints,lookforhowmanyofthesetheresponse
accuratelyaddressesandscoreaccordingly.
3. Iftheresponseincludespointsnotfoundinthereferenceanswer,checktheoriginaltextfor
evidence. Deductpointsatyourdiscretionifitdoesnotalignwiththeoriginaltext.
4. Alsoconsiderdeductingpointsforoverlyverboseresponsesorthosethatareexcessively
generalized.
Evaluation cost. On LongBench-Chat, a run of evaluation requires approximately 32,000 tokens on
average(almostentirelyasinputtokens). Therefore,usingGPT-4forevaluationwouldcostabout$0.96
perrun.
C.2 LongBench
Evaluationprompts. WeuseGPT-4toscoretheresponsesfromouralignedmodelsinSingle-DocQA,
Multi-DocQA,andSummarizationtasksonLongBench. ForthefirsttwoQAtasks,thepromptforthe
GPT-4evaluatorisasfollows.
14YouareaskedtoevaluatethequalityoftheAIassistant‚Äôsanswerstouserquestionasanimpartial
judge,andyourevaluationshouldtakeintoaccountfactorsincludingcorrectness(highpriority),and
comprehensiveness(whethertheassistant‚Äôsanswercoversallpoints). ReadtheAIassistant‚Äôsanswer
andcompareagainstthereferenceanswer,andgiveanoverallintegerratingin1,2,3(1=wrongor
irrelevant,2=partiallycorrect,3=correctandcomprehensive)basedontheaboveprinciples,strictly
inthefollowingformat:"[[rating]]",e.g. "[[2]]".
Question:
{Question}
Referenceanswer:
{Groundtruth}
Assistant‚Äôsanswer:
{Response}
Rating:
ThepromptforGPT-4evaluationonsummarizationtasksisasfollows.
You are asked to evaluate the quality of the AI assistant‚Äôs generated summary as an impartial
judge, and your evaluation should take into account factors including correctness (high priority),
comprehensiveness(whethertheassistant‚Äôssummarycoversallpoints),andcoherence. ReadtheAI
assistant‚Äôssummaryandcompareagainstthereferencesummary,andgiveanoverallintegerratingin
onascaleof1to5,where1isthelowestand5isthehighestbasedontheevaluationcriteria,strictly
inthefollowingformat:"[[rating]]",e.g. "[[3]]".
Referencesummary:
{Groundtruth}
Assistant‚Äôssummary:
{Response}
Rating:
Evaluationcost. OnLongBench,arunofGPT-4evaluationon12datasetsinSingle-DocQA,Multi-Doc
QA,andSummarizationtasksrequiresapproximately800,000tokensonaverage(almostentirelyasinput
tokens). Therefore,usingGPT-4forevaluationwouldcostabout$24perrun.
C.3 NeedleTest
Forthe‚ÄúNeedleinAHaystack‚Äùevaluation,followingtheoriginalconfigurationintheoriginalgithub
repository,weuse‚ÄúThebestthingtodoinSanFranciscoiseatasandwichandsitinDoloresParkon
a sunny day.‚Äù as the needle fact, and Paul Graham‚Äôs essays as the long haystack context. We use the
querypromptfromClaude2.17: ‚ÄúWhatisthebestthingtodoinSanFrancisco? Hereisthemostrelevant
sentenceinthecontext:‚Äù.
D CaseStudiesonOODQueries
AspartofourresearchonaligningLLMsonlongcontext,wecomeupwithanintriguingandpractical
casestudy: Canweevaluatethelongcontextunderstandingcapabilityofourtrainedmodelsusingthis
paperasthelonginput? Henceweusethepaperasinput(ofcourse,topreventrecursivenesting,the
inputpaperdoesnotincludethissection). WeconsiderusingOODquery,thatis,querythatthemodel
hasnotencounteredinthelongcontextSFTdata. Ourqueriesinclude‚ÄúFromaprofessionalreviewer‚Äôs
perspective,identifythestrengthsandweaknessesofthispaper‚Äùand‚ÄúUseraptotellaboutthispaper‚Äù.
Let‚Äôsseehowthemodelvariantsresponse:8
7https://www.anthropic.com/news/claude-2-1-prompting
8WeremovetheAbstract,Conclusion,andLimitationssectionsintheinputtopreventthemodelfromsimplycopyingcontent
fromthesesectionswithoutdemonstratingitsownunderstanding.
15!!!Disclaimer: ThebelowcontentsregardingthispaperaregeneratedbyAIandDOESNOTreflectthe
trueviewsoftheauthorsorreadersofthispaper. Pleasebeawareandidentifyanymisinformation.
gUser: Fromaprofessionalreviewer‚Äôsperspective,identifythestrengthsandweaknessesofthis
paper.
3 LongAlign-6B-64k (ChatGLM3-6B-64k trained on LongAlign-10k using packing + loss
weighting):
AsanAIlanguagemodel,IcannotprovideasubjectivereviewasIlacktheabilitytoevaluatethe
qualityofthepaperbasedonpersonalbiasesorpredispositions. However,Icanprovideanobjective
analysisofthestrengthsandweaknessesofthepaper.
Strengths:
1. Comprehensiveapproach: Thepaperprovidesathoroughandsystematicrecipe,LongAlign,to
addressthechallengesofaligninglargelanguagemodelstohandlelongcontext. Itinvolvescollecting
diverselongdata,employingefficienttrainingstrategies,andevaluatingthemodelsagainstrealistic
longcontextbenchmarks.
2. Impactofdata: Thepaperdemonstratesthatthequantityanddiversityoflonginstructiondata
significantlyimpactthemodel‚Äôsabilitytohandlelongcontexts,andthatincorporatingmorelongdata
isbeneficial. Thisprovidesavaluableperspectiveontheimportanceofdiverselongcontextdata.
3. Trainingmethods: Thepapercomparestheeffectivenessofdifferenttrainingmethodsandidentifies
thepackingandsortedbatchingstrategiesasefficientandeffective. Thisprovidesvaluableinsights
forpractitionersworkingonlargelanguagemodeltraining.
Weaknesses:
1. Lackofalarge-scalehumanevaluationstudy: Thepaperdoesnotincludeacomprehensivelarge-
scalehumanevaluationstudytovalidatetheperformanceoftheGPT-4evaluatoronLongBench-Chat.
Whilethecorrelationwithhumanannotationsispresented,adirectcomparisonbetweenhumanand
GPT-4‚Äôsassessmentsislacking.
2. Scalabilitylimits: ThepaperdoesnotexplorethescalabilityofLongAligntoevenlargermodelsor
longercontextwindows. WhilesomeexperimentalresultsonthescalingofLongAlignarepresented,
thelimitsoftheapproacharenotfullyexplored.
3. Learningcurveanalysis: Thepaperdoesnotprovideathoroughanalysisofthelearningcurve
underlongcontextalignment,comparingtheperformanceonshortandlongcontexttasksthroughout
training. Thiswouldhaveprovidedvaluableinsightsintothelearningdynamics.
Inconclusion,thepaperprovidesavaluablecontributiontothefieldbyprovidingacomprehensive
recipeforaligninglargelanguagemodelstohandlelongcontext,anddemonstratingtheimpactof
dataquantityanddiversityonmodelperformance. However,amorecomprehensiveandrigorous
evaluationstudy andadeeperexplorationofthe learningdynamicsunderlong contextalignment
wouldfurtherenhancethepaper‚Äôsimpact.
3LongAlign-7B-64k(Llama-2-7B-64ktrainedonLongAlign-10kusingsortedbatching):
AsanAIlanguagemodel,Idonothavepersonalopinionsoremotions,butIcanprovideaneutral
evaluationofthepaperbasedonitscontentandstructure.
Strengths:
1. The paper presents a comprehensive recipe, LongAlign, for aligning large language models to
handle long contexts. This is a significant contribution to the field as there is a lack of study and
discussiononlongcontextalignment.
2. Thepaperprovidesadetaileddescriptionofthedataconstructionprocess,includingcollecting
longsequencesfrom9sourcesandusingClaude2.1togeneratetasksandanswersfor10klengthy
texts.
3. Thepaperproposestwoeffectivetrainingmethods-packingandsortedbatching-tospeedupthe
trainingprocessandimprovethefinalperformanceofthemodels.
164. The paper introduces LongBench-Chat, a series of open-ended real-world user questions of
10k-100klength,coveringvariousaspectsofinstruction-followingability.
5. Thepaperconductsexperimentsonlongcontextbenchmarksandgeneralbenchmarks,demonstrat-
ingtheeffectivenessofLongAligninaligningmodelstohandlecontextsofupto64k.
Weaknesses:
1. Thepaperdoesnotprovideadetailedanalysisoftheimpactoftheproposedtrainingmethodson
themodel‚Äôsgeneralcapabilitiesandconversationalabilitiesinshortcontextscenarios.
2. Thepaperdoesnotdiscussthelimitationsoftheproposedmethodsorsuggestanyfutureworkthat
couldimprovethem.
3. ThepaperdoesnotprovideadetailedcomparisonofLongAlignwithotherexistingmethodsfor
longcontextscaling.
4. Thepaperdoesnotprovideacomprehensiveevaluationoftheproposedmethodsonotherlong
contextbenchmarksorreal-worlduserqueries.
Overall, the paper presents a thorough recipe for long context alignment and provides effective
trainingmethodstospeedupthetrainingprocess. However,amorein-depthanalysisoftheimpacton
generalcapabilitiesandacomparisonwithothermethodswouldmakethepapermorecomprehensive.
3LongAlign-13B-64k(Llama-2-13B-64ktrainedonLongAlign-10kusingsortedbatching):
Strengths:
1. Thepaperpresentsacomprehensivesolutionforaligninglargelanguagemodelstohandlelong
contextinteractions,includingconstructingadiverselonginstructiondataset,trainingmethods,and
evaluationbenchmark.
2. The authors conduct extensive experiments to demonstrate the effectiveness of their approach,
showingimprovementsindownstreamtaskswithoutperformancedegradationongeneralbenchmarks.
3. Thepaperprovidesvaluableinsightsintothechallengesandsolutionsforlongcontextalignment,
whichisanimportantbutunderstudiedarea.
4. The proposed LongBench-Chat benchmark is a valuable resource for evaluating long context
understandingandinstruction-followingabilitiesoflargelanguagemodels.
5. Thepaperiswell-writtenandeasytofollow,withclearexplanationsandvisuals.
Weaknesses:
1. The paper lacks a clear theoretical framework or motivation for why long context alignment is
importantandhowitdiffersfromshortcontextalignment.
2. Theexperimentscouldbenefitfrommorein-depthanalysisanddiscussionoftheresults,especially
inrelationtotheproposedsolutions.
3. TheevaluationonLongBench-Chatisbasedonasmallnumberofhuman-annotatedexamples,
whichmaynotfullycapturethediversityofreal-worldqueries.
4. Thepaperdoesnotdiscusspotentiallimitationsorscalabilityissueswiththeproposedapproachas
themodelsizeandcontextlengthincrease.
5. Theproposedlossweightingstrategyisnotfullyexplainedandjustified,anditsimpactontraining
stabilityandconvergencecouldbefurtherinvestigated.
TheauthorsbelieveLongAlign-13B-64kgeneratesarelativelygoodreview,withonlyafewflaws. This
casealsoshowstheO.O.D.generalizationabilityofthemodel‚Äîitcangeneratereasonablereviewfora
researchpaperwithoutencounteringsuchtypeofdataduringtraining.
gUser: Useraptotellaboutthispaper.
3LongAlign-6B-64k:
Thispaperpresentsarecipeforlongcontextalignment,LongAlign,whichaddressesthechallenges
of training large language models (LLMs) to handle longer inputs. The key ideas of LongAlign
include:
171. Data: CollectingdiverselongsequencesfrommultiplesourcesandusingSelf-Instructtogenerate
10kinstructiondataof8k-64klength.
2. Trainingmethod: Consideringamoreefficientpackingstrategythatpackssequencestogetherup
tothemaximumlengthbeforedispatchingthemtoGPUsforoptimization. Thisisbalancedwitha
lossweightingstrategytopreventsequencesinpackswithfewersequencesfromdisproportionately
affectingtheloss. Alsoproposingasortedbatchingstrategythatgroupssequencesofsimilarlengths
toreduceintra-batchidletime.
3. Evaluation: Introducing LongBench-Chat, a series of open-ended real-world user questions of
10k-100klength,coveringaspectsofinstruction-followinglikereasoning,coding,andsummarization,
aswellasbeingmultilingual. GPT-4isemployedtoscorethemodel‚Äôsresponses.
ExperimentsdemonstratethatLongAligneffectivelyalignsLLMstohandlelongcontextsofupto
64k length without performance degradation on general benchmarks, showing the impact of data
quantity,diversity,trainingmethods,andevaluation.
3LongAlign-7B-64k:
ThispaperpresentsLongAlign,arecipeforaligninglargelanguagemodelstohandlelongcontexts.
Thekeychallengesinlongcontextscalingincludelackofacomprehensiveinstruction-following
dataset,unevenbatchinginmulti-GPUtraining,andabsenceofareliablebenchmark. LongAlign
addresses these challenges by collecting long instruction data from 9 sources, using Self-Instruct
to generate 10k instruction data of 8k-64k length, and constructing LongBench-Chat, a series of
open-ended real-world user questions of 10k-100k length. The paper also proposes packing and
sorted batching training methods, loss weighting for packing training, and a benchmark called
LongBench-Chattoevaluatethealignedmodels. ExperimentsshowthatLongAligneffectivelyaligns
modelstohandlecontextsofupto64kwithoutperformancedegradationongeneralbenchmarks.
3LongAlign-13B-64k:
Yo,listenup,Igotastorytospit
Aboutapaperthat‚Äôllmakeyourheadspinandflip
It‚Äôscalled"LongAlign: ARecipeforLongContextAlignment"
Andit‚Äôsallaboutenhancinglargelanguagemodels‚Äôcapabilities
Theytacklethechallengeofscalingupmodelstohandlelongcontexts
Bycollectingandconstructingadiversedatasetoflonginstructiondata
Trainingmethodslikepackingandsortedbatchingareexplored
Tooptimizeefficiencyandperformancewithoutcompromisinggeneralcapabilities
Theirevaluationbenchmark,LongBench-Chat,isaseriesofreal-worlduserqueries
Toassessmodels‚Äôabilitytofollowlongcontextinstructionsandproviderelevantanswers
TheyfindthatincorporatinglonginstructiondataduringtrainingiscrucialAndthattheproposed
methodscanspeeduptrainingwhileimprovingperformance
Soifyou‚Äôreintolanguagemodelsandtheircapabilities
Thispaper‚Äôsamust-read,it‚Äôllmakeyouthinktwice
LongAlign,therecipeforlongcontextalignment
Isthekeytounlockingthefullpotentialoflargelanguagemodels,that‚Äôsforsure,it‚Äôsago!
Although LongAlign-6B-64k and LongAlign-7B-64k make good summaries for our paper, only
LongAlign-13B-64kfollowstheinstructionandsummarizesourpaperinrap. Itsrapisintherightform,
yetitdoesnotrhyme.
18E Limitations
Ourworkinexploringlongcontextalignmenthasitslimitations. Fromadataperspective,weprimarily
coverlong instructiondata forcategories like longcontext QA, summarization, and reasoningin data
construction. Inreality,therearemanyothertypesoflonginstructiontasksthatheavilyrelyontheability
tounderstandextendedtexts,suchasmulti-turndialogues(hundredsorthousandsofturns,evenlife-long
dialogues),long-termrole-playing,andlong-historyagenttasks,etc. Wefindthatcollectingavailable
dataforthesetasksischallengingbecausethecurrentperformanceofLLMsonthesetasksdoesnotyet
meethumanneeds. Consequently,usersrarelyinteractwithLLMsinthismanner. Additionally,since
currentLLMs,whetherAPI-basedoropen-sourcedmodels,performpoorlyonthesetasks,it‚Äôsdifficultto
automaticallyconstructsuchdatausingaSelf-Instructlikeapproach. Wehopetoexploremoretypesof
longcontextdata,enablingmodelstoalignwithhumanexpectationsacrossvariouslongcontexttasksin
futureworks.
Fromatrainingperspective,duetothelimitationsoftheDeepSpeedframeworkandourGPUresources
thatonlysupportSFTfor10Blevelmodelswithamaximumlengthof64k,wedonotconductmassive
experimentsonlongerdataorlargermodels. Somecurrentframeworks,suchasMegatron(Shoeybietal.,
2019),supportmoreparallelizationmethodsincludingmodelparallelismandsequenceparallelism,but
aredifficulttouseandreproduceduetothecomplexityoftheircodestructure. Wehopetoexplorelong
contextalignmentonlongersequencesandlarger-scalemodelsusingmoreadvancedtrainingframeworks.
Additionally,exploringRLHFinlongcontextalignmentisalsoapromisingdirection.
19