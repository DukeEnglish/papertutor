Scalable Offline Reinforcement Learning for Mean Field Games
AxelBrunnbauer JulianLemmel ZahraBabaiee
TUWien TUWien TUWien
Vienna,Austria Vienna,Austria Vienna,Austria
axel.brunnbauer@tuwien.ac.at julian.lemmel@tuwien.ac.at zahra.babaiee@tuwien.ac.at
SophieNeubauer RaduGrosu
DatenVorsprungGmbH TUWien
Vienna,Austria Vienna,Austria
sophie@datenvorsprung.at radu.grosu@tuwien.ac.at
ABSTRACT andLions[23]andHuangetal.[17],provideascalableapproxima-
Reinforcementlearningalgorithmsformean-fieldgamesoffera tiontolarge-Nplayergamesbymodelingtheinteractionsbetween
scalableframeworkforoptimizingpoliciesinlargepopulationsof individualagentsandastatisticalrepresentationofthepopulation,
interactingagents.Existingmethodsoftendependononlineinter- termedthemean-field.Thisapproachhasshowngreatpromise
actionsoraccesstosystemdynamics,limitingtheirpracticality inreducingthecomplexityofmulti-agentinteractions,allowing
inreal-worldscenarioswheresuchinteractionsareinfeasibleor forthedevelopmentofmoretractablesolutionsinenvironments
difficulttomodel.Inthispaper,wepresentOfflineMunchausen withmanyagents.EarlyworksonMFGstackledproblemsofrela-
MirrorDescent(Off-MMD),anovelmean-fieldRLalgorithmthat tivelysmallscale,oftenunderrestrictiveassumptionssuchaslinear
approximatesequilibriumpoliciesinmean-fieldgamesusingpurely dynamicsandquadraticcostfunctions.Thesesimplifiedmodels,
offlinedata.Byleveragingiterativemirrordescentandimportance whilemathematicallyelegant,limittheapplicabilityofMFGstoreal-
samplingtechniques,Off-MMDestimatesthemean-fielddistribu- worldsystemsthatexhibitcomplex,nonlinearbehavior.However,
tionfromstaticdatasetswithoutrelyingonsimulationorenvi- recentadvancesinthefieldhavefocusedonscalingMFGsolutions
ronmentdynamics.Additionally,weincorporatetechniquesfrom byleveragingdeepreinforcementlearning(DRL)techniques.These
offlinereinforcementlearningtoaddresscommonissueslikeQ- methodsuseneuralnetworkfunctionapproximatorstocompute
valueoverestimation,ensuringrobustpolicylearningevenwith equilibriumpoliciesinMFGs,asdemonstratedin[24,32,33].Such
limiteddatacoverage.Ouralgorithmscalestocomplexenviron- approaches have enabled significant progress in applying MFG
mentsanddemonstratesstrongperformanceonbenchmarktasks theorytomorepracticalandlarge-scaleenvironments.
likecrowdexplorationornavigation,highlightingitsapplicability Despitetheseadvances,acriticalissueremains:mostexisting
toreal-worldmulti-agentsystemswhereonlineexperimentationis MFG-basedRLalgorithmsrelyononlineinteractionwiththeen-
infeasible.WeempiricallydemonstratetherobustnessofOff-MMD vironment.Inmanyreal-worldapplications,particularlythosein-
tolow-qualitydatasetsandconductexperimentstoinvestigateits volvinglargepopulationsofagentsorhumaninteractions(e.g.,
sensitivitytohyperparameterchoices. trafficrouting,crowddynamics,orrecommendationsystems),on-
lineinteractioniseitherimpracticalorethicallyunjustifiable.For
example,insystemswithhumanagents,itisoftencostlyorintru-
KEYWORDS
sivetocollectreal-timedata,andcontinuousexplorationcouldlead
Mean-FieldGames,DeepReinforcementLearning,OfflineRein- tounintendedconsequencessuchasuserdissatisfactionorsafety
forcementLearning risks.Furthermore,environmentswithmanyagentscanbedifficult
tomodelaccurately,andreal-timeexperimentationinsuchsystems
maynotbepossible.
1 INTRODUCTION Insingle-agentRL,offlinelearningisawellresearchedareaand
ReinforcementLearning(RL)hasemergedasafoundationaltoolfor allowstosolvethisproblembyenablingthelearningofpolicies
solvingsequentialdecision-makingproblemsacrossadiverserange frompre-collected,fixeddatasets,eliminatingtheneedforonline
ofdomains,includingrobotics,healthcare,autonomoussystems, interactions.Theseofflinemethodshaveprovenhighlyeffectivein
andgametheory.However,whileRLtechniqueshaveseensignifi- settingswherereal-timeinteractionislimitedorexpensive.How-
cantsuccessinsingle-agentsettings,thetransitiontomulti-agent ever, the application of offline RL techniques to MFGs remains
reinforcementlearning(MARL)presentsuniquechallenges,suchas underexplored.CurrentMFGmethodshavelargelyoverlookedthe
theexponentialgrowthinthestateandactionspacesasthenumber offlinesetting,wherenoonlineinteractionwiththeenvironment
ofinteractingagentsincreases,makingtheproblemsignificantly isavailableduringlearning.
morecomplexbothintermsofcomputationandcoordination. Tobridgethisgap,weproposeOfflineMunchausenMirror
TraditionalMARLmethodsoftendonotscaletomany-agent Descent(Off-MMD),anofflinemean-fieldRLalgorithm.Off-MMD
settingsandrapidlybecomeinfeasibleinenvironmentswithlarge extendstherecentlyintroducedDeepMunchausenOnlineMirror
populationsofagents.Asthenumberofagentsgrows,learning Descent(D-MOMD)methodbyLauriÃ¨reetal.[24]andcombines
effectivestrategiescanbecomecomputationallyprohibitive.To thescalabilityofMFGapproximationswiththedataefficiencyof
addressthisissue,Mean-FieldGames(MFGs),introducedbyLasry offlineRL.Tothebestofourknowledge,Off-MMDisthefirstdeep
4202
tcO
32
]GL.sc[
1v89871.0142:viXraoffline RL algorithm specifically designed for MFGs, capable of dynamics,rewards,andevenpolicies.However,inthiswork,we
handlingarbitrarilysizeddatasets.Thisinnovationopensthedoor focusonthecasewhereonlytherewardsdependonthemean-field:
forapplyingMFGtheoryinreal-worldsystemswhereonlinedata
collectionisprohibitive.TheprimarychallengeinadaptingMFGs
ğ‘Ÿ
ğ‘¡
:SÃ—AÃ—Î”
S
â†¦â†’R. (1)
Givenapolicyğœ‹,themean-fieldflowğœ‡ğœ‹ isdefinedbythefol-
totheofflinesettingstemsfromthefactthatthesesystemstypi-
lowingrecursiverelation,denotedbythemean-fieldevaluation
callyrequireestimatingthedistributionofagents(themean-field),
whichcomplicatesthedirectadaptationofonlinealgorithms.To
operatorğœ™(ğœ‹)=ğœ‡ ğœ‹:
addressthis,werepurposeideasfromoff-policypolicyevaluation ğœ‡ ğ‘¡ğœ‹ +1(ğ‘ â€²)= âˆ‘ï¸ âˆ‘ï¸ ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)ğœ‹(ğ‘|ğ‘ )ğœ‡ ğ‘¡ğœ‹ (ğ‘ ), (2)
(OPE)toapproximatethemean-fielddistributionusingofflinedata.
ğ‘ âˆˆSğ‘âˆˆA
Additionally,weapplyarobustregularizationmechanismtomiti- ğœ‡ 0ğœ‹ =ğ‘š0, (3)
gatetheeffectsofdistributionalshift,awell-knownissueinoffline
RLwherethelearningpolicyencountersstatesoractionsnotsuffi-
whereğ‘(ğ‘ â€²|ğ‘ ,ğ‘)representsthetransitiondynamicsoftheenviron-
cientlyrepresentedinthedataset.Thisregularizationstabilizesthe
ment,andğ‘š0 istheinitialdistributionoverstates.Thegoalfor
policylearningprocess,ensuringmorereliableperformanceeven anagentistofindapolicyğœ‹ : S â†¦â†’ Î” A whichmaximizesthe
expectedsumofrewardswithrespecttoagivenmean-fieldflowğœ‡:
inunderrepresentedareasofthestatespace.
WeempiricallyvalidateOff-MMDonasuiteofbenchmarktasks ğ‘‡
todemonstrateitsefficacy.Specifically,weevaluateitsperformance max ğ½(ğœ‹,ğœ‡)=E ğœ‹(cid:104)âˆ‘ï¸ ğ›¾ğ‘¡ğ‘Ÿ(ğ‘  ğ‘¡,ğ‘ ğ‘¡,ğœ‡ ğ‘¡)(cid:105)
ğœ‹
ontasksinvolvingbothexplorationandcrowdnavigation,two ğ‘¡=0
commonchallengesinthefieldofMFGs.Additionally,weexplore subjectto: ğ‘ 0âˆ¼ğœ‡0
thesensitivityofOff-MMDtothequalityofthedatasetsusedfor ğ‘ ğ‘¡ âˆ¼ğœ‹(Â·|ğ‘  ğ‘¡)
training, assessing how variations in state-action coverage and
ğ‘  ğ‘¡+1âˆ¼ğ‘(Â·|ğ‘  ğ‘¡,ğ‘ ğ‘¡).
trajectoryqualityimpactperformance.Wefurtherinvestigatethe
Bymakingtherewarddependonlyonotheragentsviathemean-
importanceoftheproposedregularizationterm,designedtoprevent
field,insteadoftheindividualstatesandactionsofallotheragents,
theoverestimationofğ‘„-valuesâ€”awell-documentedissueinoffline
weobtainamuchsmalleroptimizationproblemtosolve.
RLalgorithms.Insummary,ourcontributionsare:
(1) OfflineMunchausenMirrorDescent,anoveldeepRLalgo- LearninginMFGs. Incontrasttosingle-agentRL,wherewe
optimizeastationaryrewardsignal,algorithmsforMFGstypically
rithmforofflinelearningforMFGs.
aim to find policies that are close to some equilibrium concept
(2) Extensiveevaluationoftheperformanceandablationstud-
becausetheirperformancelargelydependsonotheragents.The
ieswithrespecttoperformanceanddatasetquality.
conceptofNash-Equilibria(NE),acommonsolutionconceptin
game theory, has been extended to MFGs [23] and is the main
2 BACKGROUND
optimizationtargetforalgorithmssolvingnon-cooperativeMFGs.
Sequentialdecisionmakingproblemscommonlymakeuseoffinite
horizonMarkovdecisionprocess(MDP).AfinitehorizonMDPis Definition2.1. Thebestresponse(BR)toamean-fieldflowğœ‡is
thesolutionoftheoptimizationproblem
atupleâŸ¨S,A,ğ‘Ÿ,ğ‘,ğ›¾,ğ»,ğœ‡0âŸ©consistingofasetofstatesS,asetof
actionsA,arewardfunctionğ‘Ÿ :SÃ—A â†¦â†’R,stochasticdynam- ğœ‹âˆ—=argmaxğ½(ğœ‹,ğœ‡)=ğµğ‘…(ğœ‡).
icsğ‘ : SÃ—A â†¦â†’ Î” ,discountfactorğ›¾ âˆˆ (0,1),horizonğ» âˆˆ N ğœ‹
S
andinitialstatedistributionğœ‡0 âˆˆÎ” S.Problemsinvolvingalarge Definition2.2(ğœ–-MFNE). Ağœ–-Mean-FieldNashEquilibriumwith
numberofinteractingagentsbecomeintractableasthesizeofthe ğœ– â‰¥0isdefinedasatuple(ğœ‹,ğœ‡ğœ‹)forwhichthefollowingholds:
stateandactionspacegrowsexponentiallywiththenumberof supğ½(ğœ‹â€²,ğœ‡ğœ‹ ) â‰¤ ğ½(ğœ‹,ğœ‡ğœ‹ )+ğœ–.
agents.Inmany-agentgames,whereagentsareanonymousand ğœ‹â€²âˆˆÎ 
identical,Mean-FieldGames(MFGs)offeraneffectiveframework
EarlymethodstosolveMFGsprimarilyinvolvedsolvingcoupled
toapproximateNashequilibriabysimplifyingtheinteractionsbe-
partialdifferentialequations,typicallyaforward-backwardsys-
tweenagents.FirstintroducedbyLasryandLions[23]andHuang
temofHamilton-Jacobi-BellmanandFokker-Planck-Kolmogorov
etal.[17],MFGsaddressthiscomplexitybymodelinginteractions
equations,tocomputethevaluefunctionanddistributionflowof
throughthedistributionofagentstates,ratherthantrackingindivid-
agents[1,2].However,thesemethodsstrugglewithscalabilityin
ualagents.Thisreducesthedimensionalityoftheproblem,making
high-dimensionalstate-actionspacesandcomplexenvironments.
itmoretractableandallowstoapproximatefinite,ğ‘-playergames.
AlgorithmsforsolvingMFGsusingRLcommonlyrelyonsome
InMFGs,arepresentativeagentinteractswiththemean-field(i.e.,
formoffixedpointiteration,alternatingbetweenpolicyupdates
thedistributionofallagents),ratherthandirectlyinteractingwith
andthemean-fielddistributioncomputation.Thereby,theyusea
eachindividualagent.Consequently,theproblembecomesopti-
bestresponsecomputationstepbeforecomputingthemean-field.
mizingasinglepolicywithrespecttothispopulationdistribution,
Onconvergence,thefixedpointiteration
whichleadstomorecomputationallyefficientalgorithms.
Inthiswork,weconsiderstochastic,finite-horizonMFGswitha
ğœ™(ğœ‹âˆ—)=ğœ™(ğµğ‘…(ğœ‡âˆ—))=ğœ‡âˆ—
finitesetofstatesSandactionsA.Themean-field,whichisthe yields a MFNE. However, generally convergence is not guaran-
distributionofagentsoverstatesattimeğ‘¡,isdenotedbyğœ‡
ğ‘¡
âˆˆÎ” S. teed [12] and methods from algorithmic game theory, such as
Inthemostgeneralform,MFGsallowformean-field-dependent Fictitious Play (FP) [4], are used to stabilize training. In recentyears,machinelearningapproaches,particularlyRL,havebeen AlsoCuiandKoeppl[12]andPerrin[31]introducedeepRLbased
exploredasapromisingalternativeforsolvingMFGs[8,14,40]. approachesonMFGs.Otherworksaddresstheassumptionofiden-
OneofthecentralchallengesinusingRLtosolveMFGsliesin ticalagentsandextendMFGstomulti-populationgames[7,13].
thenon-stationarityintroducedbymulti-agentinteractions,which Subramanianetal.[35]introducedecentralizedMFGs,allowingto
complicatesthelearningprocess.DeepLearningvariantsofFP[15] lifttheassumptionofindistinguishableagents.Inverse-RL(IRL)
wereadaptedtotheMFGsettingstoscaletolargerstateandaction methodshavebeenappliedtotheMFGsettingtoinferunknown
spaces[6,32,33,37]. rewardsignals[10,11].Yangetal.[39]introduceanIRLapproach
Inthiswork,wefocusonaclassofalgorithmsthatevaluatea thatlearnsthedynamicsandtherewardmodelfromdata.How-
policyinsteadofcomputingaBRateachiteration[5].Specifically, ever,theirworkfocusesonbehaviorpredictionratherthanfinding
weadapttheOnlineMirrorDescentalgorithm(OMD)[24,30]tothe MFNE.Recentworkonmodel-basedalgorithmsinthemean-field
offlinesetting.OMDalternatesbetweenpolicyevluationandmean- control(MFC)setting,asubclassofMFGsinwhichagentsfully
fieldupdates,asoutlinedinAlgorithm1.ThemaindifferencetoBR cooperate,canlearnamodeloftheenvironmentandusethatto
algorithmsisthatOMDtracksthesumofpreviousğ‘„-valuesinstead optimizeaMFCpolicywithbettersampleefficiency[16,29].Jusup
ofpolicies.ThepolicyupdateinOMDisasoftmaxoverthesumof etal.[18]extendthistosafety-constrainedproblems.However,
previousğ‘„-values.However,itisnotstraightforwardtosumupğ‘„ althoughthoseapproacheslearnamodeloftheenvironment,they
functionsinthecaseofnonlinearfunctionapproximators,suchas stillassumeaccesstotheenvironmenttogeneratenewdatafor
neuralnetworks.ThiscanbeavoidedbyapplyingtheMunchausen exploration.
Trick,asitwasproposedin[24]: DespitemanyadvancesinthefieldofMFGs,thedirectionof
purelyofflinelearningremainsanunderexploredtopic.SAFARI[9]
ğ‘–
ğœ‹ğ‘– =softmax(cid:16)1âˆ‘ï¸ ğ‘„ğ‘—(cid:17) (4) is,toourknowledge,theonlyapproachspecificallydesignedfor
ğœ
ğ‘—=0 offlinemean-fieldRL,whichisnotdirectlycomparabletoourap-
=argmax
ğœ‹âˆˆÎ”
A(cid:16) âŸ¨ğœ‹,ğ‘„ğ‘– âŸ©âˆ’ğœKL(cid:0)ğœ‹||ğœ‹ğ‘–âˆ’1(cid:1)(cid:17) (5) p inro ua sc inh gas RKit Hd Soe (s Ren po rt oa dp up cr io nx gim Kea rt ne eM lHFN ilbE e. rI tts Sm paa ci en )i en mn bo ev da dti io nn gslie tos
=argmax ğœ‹âˆˆÎ” A(cid:16) âŸ¨ğœ‹,ğ‘„ (cid:32)ğ‘– (cid:32)(cid:32)(cid:32)(cid:32)+ (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)ğœ (cid:32) ln (cid:32)(cid:32)(cid:32)ğœ‹ (cid:32)(cid:32)(cid:32)(cid:32)ğ‘– (cid:32)(cid:32)âˆ’ (cid:32)(cid:32)1 âŸ©âˆ’ğœ âŸ¨ğœ‹ (cid:32)(cid:32)(cid:32),ln (cid:32)ğœ‹ (cid:32)(cid:32)âŸ©(cid:17) (6) m reo gude lal rt ih zee dm ve aa lun e-fi ie teld rad tii ost nri bb au st eio dn o, nco fim xeb din te rd ajew ci tt oh ria en s.u Wnc he ir leta ti hn ety o- -
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ğ‘„Ëœğ‘– H(ğœ‹) reticallyrobust,withdataset-dependentperformancebounds,SA-
=softmax(cid:16)1 ğ‘„Ëœğ‘–(cid:17)
. (7)
F reA qR uI irf ea ic ne vs es ri tg inn gifi aca Gn rt amsca mla ab tri il xity thi as tsu ge ros. wT sh qe uaR dK raH tS icae lm lyb wed itd hin thg es
ğœ
datasetsize,leadingtosubstantialmemorydemandsandcomputa-
This insight allows us to apply policy evaluation directly onğ‘„Ëœ
tionalbottlenecks.
toavoidthesummationofğ‘„-functions.InEquation(6), âŸ¨ğœ‹,ğ‘„ğ‘–âŸ©
denotestheshorthandnotationfor(cid:205) ğ‘ğœ‹(ğ‘|ğ‘ )ğ‘„ğ‘–(ğ‘ ,ğ‘).Themodified
4 METHOD
BellmanOperatoristhendefinedas:
In this section we discuss the foundational components of Off-
(Bğœ‡ğœ‹ğ‘„Ëœ )(ğ‘ ,ğ‘)=ğ‘ŸËœ+ğ›¾E ğ‘ â€²,ğ‘â€²[ğ‘„Ëœ (ğ‘ â€²,ğ‘â€²)âˆ’ğœ lnğœ‹ğ‘–âˆ’1 (ğ‘â€²|ğ‘ â€²)] (8)
MMD.Inparticular,weelaboratehowwecanleveragemethods
ğ‘ŸËœ(ğ‘ ,ğ‘,ğœ‡)=ğ‘Ÿ(ğ‘ ,ğ‘,ğœ‡)+ğœğ›¼lnğœ‹ğ‘–âˆ’1 (ğ‘|ğ‘ ), (9) fromofflinepolicyevaluationtoestimatethemean-fieldğœ‡ from
staticdatasetsandprovideamodifiedversionoftheD-MOMD
withmodifiedrewardğ‘ŸËœ,whichpenalizesdeviationsfromtheprevi-
algorithmtoadaptittotheofflinelearningregime.
ouspolicyğœ‹ğ‘–âˆ’1.Thehyperparameterğœ actsasatemperatureand
scalesthesumofğ‘„-valuestoavoidprematureconvergencewhereas
4.1 OfflineMean-FieldEstimation
ğ›¼ isaregularizationparametertocontrolhowfaranewpolicy
Fixedpointalgorithmsforsolvingmean-fieldgames,asdiscussedin
canbefromthepreviouspolicy.Foramoredetailedderivation,we
Section2,iteratebetween(1)evaluatingthemean-fielddistribution
referto[24].
ofagentsand(2)best-responsecomputationorpolicyevaluation.
Thefirststep,mean-fieldestimation,canbedoneviadirectcompu-
Algorithm1MunchausenOnlineMirrorDescentforMFGs[24]
tationifonehasaccesstothetransitionmodelorviaMonte-Carlo
1: forğ‘–=1...ğ¿do samplesifasimulatorisprovided.Inscenarioswhereonlyastatic
2:
Mean-FieldUpdate:ğœ‡ğ‘– â†ğœ™(ğœ‹ğ‘–)
datasetofpreviouslycollectedenvironmentinteractionsareavail-
3:
RegularizedPolicyEvaluation:ğ‘„Ëœğ‘–+1â†Bğœ‹ğ‘–ğ‘„Ëœğ‘–
able,onlinealgorithmslikeD-MOMD[24]arenotapplicable.
ğœ‡ğ‘–
4:
PolicyUpdate:ğœ‹ğ‘–+1(Â·|ğ‘ )â†softmax( ğœ1ğ‘„Ëœğ‘–+1(ğ‘ ,Â·)) Therefore,weseektoapproximatethemean-fieldflow
5: endfor ğœ‡ ğ‘¡ğœ‹ +1(ğ‘ â€²)=âˆ‘ï¸âˆ‘ï¸ ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)ğœ‹(ğ‘|ğ‘ )ğœ‡ ğ‘¡ğœ‹ (ğ‘ ) (10)
ğ‘ âˆˆğ‘†ğ‘âˆˆğ´
withouthavingaccesstothetransitionmodelğ‘(ğ‘ â€²|ğ‘ ,ğ‘).Oneap-
3 RELATEDWORK proach to mitigatethis is toleveragethe data tolearn a model
Recentyearsbroughtupmanyworksthataddressvariouslimi- ofthedynamics.However,themodelsmaybeinaccurateinlarge
tationsofMFGs.LauriÃ¨reetal.[24]introducedthebasisofour actionspaces,wherenotallactionsarefrequentlyvisited.More-
work,DeepMunchausenMirrorDescent,adeepneuralnetwork over,approximatingtheenvironmentdynamicswithneuralnet-
based variant of OMD [30] with strong empirical performance. worksmightcauseadditionalbiasesfromcovariateshiftsduetothechangeofpolicies[38].Inthiswork,weleveragethefactthat 4.2 OfflineMunchausenMirrorDescent
thisproblemcanbeequivalentlytreatedasanoff-policystateden- InSection4.1,weintroducedanofflinemethodforestimatingthe
sityestimationproblem.OPEmethodsaredesignedtoestimate mean-fielddistributionofapolicy.Thismethodcan,inprinciple,
quantitiessuchasrewardsorvaluefunctionsfromoff-policysam- bedirectlyappliedtoD-MOMDtoadaptittotheofflinelearning
ples.Werepurposethisideatoestimatethestatedistributionunder setting.TheupdateruleinouralgorithmfollowstheclassicTD-
a new policy. In particular, we are interested to estimate ğœ‡ ğœ‹(ğ‘ ) errorminimizationapproach,asusedinDQN[27],wherethetarget
fromsamplesthatarenotcollectedfromğœ‹ butsomeother,pos- ğ‘„-valuesareparameterizedbyğœƒÂ¯.Specifically,theobjectiveisto
siblyunknown,behaviorpolicyğœ‹ ğ›½ andwithouthavingaccessto minimizethetemporal-differenceerrorwherethepolicyevaluation
environmentdynamicsğ‘(ğ‘ â€²|ğ‘ ,ğ‘). operatorisdefinedasinEquation(8):
Letğ‘‘ ğœ‡ğœ‹(ğ‘ ,ğ‘)=ğœ‹(ğ‘|ğ‘ )ğœ‡ ğœ‹(ğ‘ )bethejointstate-actiondistribution
givenamean-fieldğœ‡ ğœ‹ andpolicyğœ‹.Werestatethemean-fieldflow minE (ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼D(cid:104) (cid:0)ğ‘„ ğœƒ(ğ‘ ,ğ‘)âˆ’(Bğœ‡ğœ‹ğ‘„ ğœƒÂ¯)(ğ‘ ,ğ‘)(cid:1)2(cid:105) . (15)
asanexpectationoverğ‘‘ğœ‹: ğœƒ
ğœ‡
However,naivelyapplyingoff-policyalgorithmstoofflineRL
ğœ‡ ğœ‹ğ‘¡+1 (ğ‘ â€²)=E (ğ‘ ,ğ‘)âˆ¼ğ‘‘ ğœ‡ğœ‹ ğ‘¡(cid:104) ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)(cid:105) . (11) t na os tk wst ey lp li rc ea pl rly esl ee nad tes dto inth the eov de ar te as st ei tm .Tat oio an ddo rf eğ‘„ ss-v ta hl iu se is ssfo ur e,a wct eio in ns
-
corporatearegularizationtermfollowingtheideaofConservative
Ingeneral,wecouldapplyanyOPEmethodcapableofestimating
Q-Learning(CQL)[21],whichisdesignedtolearnaconservative
density(ratios)suchasmodel-basedestimators[19,41]orDICE-
lowerboundofthetrueğ‘„-function.CQLintroducesaregularized
styleapproachesforestimatingstationarydistributions[26,36],as
versionoftheBellmanequation,wheretheobjectivebalancesthe
Off-MMDisagnostictotheestimationmethod.Inthisworkwe
maximizationoftheğ‘„-valuesoverthedatasetandtheminimiza-
decideforusingimportancesamplingtoestimateğœ‡ğ‘¡+1.Inparticu-
ğœ‹ tionofthetemporal-differenceerror.ThegeneralCQLoptimization
lar,wemakeuseofthemarginalizedimportancesampling(MIS)
problemisgivenas:
estimator of Xie et al. [38] because of its theoretical properties
a ren qd uii rts easi dm dip til oic nit ay lsc to epm sp ,sa ure chd ato sso ot lh ve inr ga ap npr ino nac eh rs o, pw timhi ic zh att iy op ni pca roll by
-
m ğ‘„inm ğœ‹a ËœxE (ğ‘ ,ğ‘)âˆ¼D,ğœ‹Ëœ(cid:2)ğ‘„(ğ‘ ,ğ‘)(cid:3) âˆ’E (ğ‘ ,ğ‘)âˆ¼D,ğœ‹ğ›½(cid:2)ğ‘„(ğ‘ ,ğ‘)(cid:3)
(16)
lem[28,42]orfittinganothermodel[19,41]. +(cid:12) (cid:12)ğ‘„âˆ’Bâˆ—ğ‘„(cid:12) (cid:12)2 +R(ğœ‹Ëœ),
Letğ‘‘(ğ‘ ,ğ‘)=ğœ‹ ğ›½(ğ‘|ğ‘ )ğ‘‘(ğ‘ )bethejointstate-actiondistributionof
thedatasetcollectedunderbehaviorpolicyğœ‹ .Inpractice,ğœ‹ isof- whereğœ‹Ëœ representsapolicyusedtodefinethejoint-actiondistribu-
ğ›½ ğ›½
tenunknownandcanbeapproximatedusingthestate-conditional tionoverwhichweminimizethestate-actionvalues.Thesecond
empiricaldistributionoveractionsinD[21].Wecanapplyimpor- termencouragestighterboundsbymaximizingğ‘„-valuesunderthe
tancesamplingtoreformulateEquation(11)as datasetdistribution.ThelasttermistheclassicBellmanequation
minimizingtheTDerrorwitharegularizationtermRappliedto
(cid:34)ğ‘‘ğœ‹ (ğ‘ ,ğ‘) (cid:35) ğœ‹Ëœ.ForspecificchoicesofR,theinnermaximizationproblemcan
ğœ‡ ğœ‹ğ‘¡+1 (ğ‘ â€²)=E (ğ‘ ,ğ‘)âˆ¼ğ‘‘ ğ‘‘ğœ‡ğ‘¡ (ğ‘ ,ğ‘) ğ‘(ğ‘ â€²|ğ‘ ,ğ‘) (12) besolvedinclosedform.Acommonchoicefor R istousethe
KLdivergencetosomeprioractiondistributionğœŒ.IfwechoseğœŒ
=E
(ğ‘ ,ğ‘)âˆ¼ğ‘‘(cid:104)ğœ‹
ğœ‹
ğ›½(ğ‘ (ğ‘|ğ‘  |) ğ‘ )ğœ‡ ğ‘‘ğœ‹ğ‘¡ (( ğ‘ ğ‘  )) ğ‘(ğ‘ â€²|ğ‘ ,ğ‘)(cid:105)
. (13)
t ro egb ue lat rh ie zeu dn ,i cf lo or sm edd -fis ot rr mibu lot sio sn fuo nv ce tr ioa nct fi oo rn Os, ffw -Meo Mb Dta :inanentropy
Thisfactorizationofthestate-actiondistributionallowsustoapply L(ğœƒ)=ğœ‚E (ğ‘ ,ğ‘)âˆ¼D(cid:104) logâˆ‘ï¸ expğ‘„ ğœƒ(ğ‘ ,ğ‘â€²)âˆ’ğ‘„ ğœƒ(ğ‘ ,ğ‘)(cid:105)
MIStoapproximateEquation(13)usingsamples(ğ‘ ğ‘–,ğ‘ğ‘–,ğ‘ ğ‘– )from ğ‘â€² (17)
ğ‘¡ ğ‘¡ ğ‘¡+1
finite dataset D. Letğ‘‘Ë† (ğ‘  ğ‘¡) = |D1
|
(cid:205) ğ‘–|D|1[ğ‘  ğ‘¡(ğ‘–) = ğ‘  ğ‘¡] denote the +E (ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼D(cid:104) (cid:0)ğ‘„ ğœƒ(ğ‘ ,ğ‘)âˆ’(Bğœ‡ğœ‹ğ‘„ ğœƒÂ¯)(ğ‘ ,ğ‘)(cid:1)2(cid:105) ,
empiricalstatedistributionattimeğ‘¡,thenthemarginalizedstate
whereğœ‚isahyperparametertocontroltheimportanceoftheregu-
distributioncanbeestimatedrecursivelyby
larization.
ğœ‡ ğœ‹ğ‘¡+1 (ğ‘ )â‰ˆ |D1
|
âˆ‘ï¸| ğ‘–D =0| ğœ‡ ğ‘‘ğœ‹ğ‘¡
Ë†
(( ğ‘ ğ‘  ğ‘¡(ğ‘¡ ğ‘–(ğ‘– )) )) ğœ‹ğœ‹ ğ›½( (ğ‘ ğ‘ğ‘¡( ğ‘¡(ğ‘– ğ‘–) )|ğ‘  |ğ‘ ğ‘¡( ğ‘¡(ğ‘– ğ‘–) ))
)
1[ğ‘  ğ‘¡( +ğ‘–) 1=ğ‘ ]
(14)
E toqu cT oah mte io pp n use t( eu 14d t) ho t e- oc lo ocd soe smf fo p ur u ntO ce tff it o- h nM e aM o nfflD di ui ns pes dmh ao tew ean tn h-i fi en e pA l adl rg a ao n mr di et E th eqm ru sa2 ğœƒt. iW vo in ae ( su 1 ts 7 oe )
-
ğœ‡ ğœ‹0 (ğ‘ )=ğ‘‘Ë† (ğ‘ 0). c sch ha est mic ag ar sad Ai le gn ot rd ite hs mcen 1.t.Off-MMDthusfollowsthesameiterative
This yields an unbiased estimator of ğœ‡ with polynomial error
ğœ‹
boundwithrespecttotimehorizonğ»,whichreducestoO(ğ»)in 5 EXPERIMENTALEVALUATION
somecases,suchasboundedmaximumexpectedreturns[38].Note WeevaluatetheOff-MOMDalgorithmontwogrid-worldproblems
that,unlikeintypicalsingle-agentofflineRLscenarios,wecan introducedbyLauriÃ¨reetal.[24]andcompareitsperformance
notdirectlyestimaterewardğ‘Ÿ ,asitdependsnonlinearlyonğœ‡ in againsttheonlinevariant.Wealsoconductexperimentstoinvesti-
ğ‘¡ ğ‘¡
general.Thus,forthegeneralcase,werequireaccesstothereward gatethesensitivitytothequalityofthedatasetandinvestigatethe
function.Forspecialcases,suchasrewardfunctionsmonotonicin importanceoftheconservativeconstraintonthelossfunction.The
ğœ‡,wecouldinprincipleapproximateğ‘Ÿ directly. algorithmsandtheenvironmentsareimplementedinJAX[3]and
ğ‘¡Algorithm2OfflineMunchausenMirrorDescent(Off-MMD) ExplorationTask. Inthistask,agentsstartintheleftupper
1:
Input:DatasetD,initialparametersğœƒ1 cornerandmustspreadevenlyacrossallfourrooms.Thereward
functionisdefinedas
2: forğ‘–=1...ğ¿do
3: Estimatemean-fieldğœ‡ğ‘– usingeq.(14) ğ‘Ÿ(ğ‘  ğ‘¡,ğ‘ ğ‘¡,ğœ‡ ğ‘¡)=âˆ’logğœ‡ ğ‘¡(ğ‘  ğ‘¡),
4: forğ‘— =1...ğµdo
5: SamplebatchB:{(ğ‘  ğ‘¡ğ‘˜,ğ‘ğ‘˜ ğ‘¡,ğ‘  ğ‘¡ğ‘˜ +1)} ğ‘˜ğ‘ =1âˆ¼D w hih gi hc eh ri rn ec wen at ri dv siz wes ha eg ne ln ot ws -t do eo nc sc iu typy stl ae ts es sc ar ro ew rd ee ad chs eta dt .e Ts, hl eea od pi tn ig mt ao
l
6: Relabelrewardusingğœ‡ ğ‘¡ğ‘–:ğ‘Ÿ ğ‘¡ğ‘˜ =ğ‘Ÿ(ğ‘  ğ‘¡ğ‘˜,ğ‘ğ‘˜ ğ‘¡,ğœ‡ ğ‘¡ğ‘–)
policyforthistaskspreadsevenlyacrossthewholestate-space.
7: Update:ğœƒ ğ‘– â†ğœƒ ğ‘– âˆ’âˆ‡ğœƒL(ğœƒ ğ‘–)usingeq.(17)
InFigure1a,wepresenttheexploitabilityover100iterationsof
8: endfor
ouralgorithm.Theonlinevariantrapidlyconvergestotheexpected
9:
ğœƒğ‘–+1â†ğœƒğ‘–
10:
Updatepolicy:ğœ‹(ğ‘|ğ‘ )=softmax(cid:0) ğœ1ğ‘„Â¯ ğœƒğ‘–+1(ğ‘ ,ğ‘)(cid:1) o Mu Mtc Dom ,ee a. cW hte rae iv na el dua ot ne ath de ap tae sr ef to orm fda in ffc ee reo nf tt qh ure ae liti yn .s Wtan hc ee ns tro af inO eff d-
11: endfor
onsufficientlyhigh-qualitydatasets,Off-MMDconsistentlylearns
policiesthatperformwell.Figure1billustratestheevolutionofthe
buildoncodebyKostrikov[20]andLanctotetal.[22].Thecode mean-fieldovertime,supportingthisobservation.Asexpected,the
fortheexperimentsisavailableonGitHub.1 policytrainedondatageneratedbyauniformrandompolicyfails
tospreadevenlyacrossallrooms,particularlyinthelower-right
5.1 ExperimentSetup room,duetoinsufficientcoverageofthisregioninthedataset.
Tomakerunscomparablewitheachother,weemployExploitability CrowdModellingwithCongestion. Inthistask,agentsalso
asanevaluationcriteria(alsooftenreferredtoasRegret): startintheupperleftcorner.Differentlytotheexplorationtask,
E(ğœ‹,ğœ‡)=maxğ½(ğœ‹â€²,ğœ‡)âˆ’ğ½(ğœ‹,ğœ‡). agentsmustnavigatetothetargetpositioninthelowerrightcor-
ğœ‹â€² nerwhileavoidinghigh-densityareas.Furthermore,wesimulate
ItdirectlymeasureshowfaralearnedpolicyisfromaMFNEby congestioneffectsbypenalizingmovementswhenagentsarein
quantifyingthepotentialutilityanagentcangainbydeviatingfrom crowdedareas.Therewardfunctionisdefinedas
itspolicy,withlowerexploitabilityindicatingbetterequilibrium
approximation.Weusethesameevaluationprotocolasin[24]and
ğ‘Ÿ(ğ‘  ğ‘¡,ğ‘ ğ‘¡,ğœ‡ ğ‘¡)=âˆ’||ğ‘ 
ğ‘¡
âˆ’ğ‘ target||âˆ’ğœ‡ ğ‘¡(ğ‘  ğ‘¡)||ğ‘||âˆ’logğœ‡ ğ‘¡(ğ‘  ğ‘¡).
computethegroundtruthmean-fieldandexploitability. Thisisamorecomplexrewardfunction,asitposesatrade-offof
Both, Off-MMD and D-MOMD, optimize ağ‘„ function repre- conflictinggoalsfortheagents.Theresults,showninFigure2a,
sentedasaneuralnetworkwith3layersof128parametersand demonstrateconvergenceofOff-MMD(Exp)andOff-MMD(Int)
ReLUactivations.Thehyperparamersettingsarethesameforall towardsthebaseline.Notably,thepolicytrainedontherandom
instancesofOff-MMDoveralltasks,excepttheablationstudies. datasetalsoperformsreasonablywell.Wehypothesizethatthe
Formoredetails,werefertoAppendixA. distancepenaltyprovideseffectiveguidance,enablingthepolicy
tosolvethetaskevenwithlimitedstate-actioncoverageincertain
5.2 PerformanceEvaluation partsofthestatespace.
Weevaluatetheperformanceofouralgorithmontwodistincttasks
withinagridworldenvironmentconsistingoffourseparatedrooms 5.3 ImpactofDatasetQuality
connectedbynarrowcorridors,asdescribedin[24,25].Agentscan
InofflineRL,weareinterestedintherobustnessofpolicyperfor-
choosefromfiveactions:moveup,down,left,right,orstayinplace.
mancestodatasetquality.Inthisexperiment,weaimtoinvestigate
Ifanactionresultsinacollisionwithawall,theagentremainsin
thesensitivityofOff-MMDtochangesinthequalityofthetrajecto-
itscurrentposition.Thetimehorizonforeachepisodeissetto40
riesinthedatasetandthecoverageofthestatespace.Buildingon
timesteps.Thetwotasksweevaluateareexplorationandcrowd
themethodologyofSchweighoferetal.[34],wecategorizedatasets
navigation.Foreachtask,wetrainOff-MMDonthreedatasetsof
basedontwocriteria:state-actioncoverageandtrajectoryquality.
varyingqualityandcompareitsexploitabilityagainstthebaseline.
ThefollowingvariantsofOff-MMDareincludedintheevaluation: Definition5.1(State-ActionCoverage). Letğ‘¢ ğ‘ ,ğ‘(D) denotethe
â€¢ D-MOMD:Theonlinebaselinealgorithm. numberofuniquestate-actionpairsinadatasetD,thenthestate-
â€¢ Off-MMD(Exp):Trainedondatacollectedbyafullytrained actioncoverageofthisdatasetisdefinedas
D-MOMDpolicy. ğ‘¢ ğ‘ ,ğ‘(D)
â€¢ Off-MMD(Int):Trainedondatacollectedfromaninter- Coverage(D)= |ğ‘†||ğ´| . (18)
mediatecheckpoint.
â€¢ Off-MMD(Rand):Trainedondatacollectedfromauni- Definition5.2(TrajectoryQuality). Letğ‘”(D)denotetheaverage
formrandompolicy. episodereturnofadataset.Furthermore,letDminandDexpertbe
referencedatasets,collectedbyasuboptimalandanexpertpolicy,
Alldatasetscontain100Kepisodeswith40timestepseach.Thesub-
sequentsectionspresenttheevaluationresultsfortheexploration
respectively.ThetrajectoryqualityofanotherdatasetDisdefined
as
andcrowdnavigationtasks.
Quality(D)=
ğ‘”(D)âˆ’ğ‘”(Dmin)
. (19)
1https://github.com/axelbr/offline-mmd ğ‘”(Dexpert)âˆ’ğ‘”(Dmin)Exploration Navigation
300
D-MOMD 600 D-MOMD
250 Off-MMD(Rand) Off-MMD(Rand)
Off-MMD(Int) 500 Off-MMD(Int)
200 Off-MMD(Expert) Off-MMD(Expert)
400
150
300
100 200
50 100
0
0 20 40 60 80 100 0 20 40 60 80 100
Iteration Iteration
(a)Exploitability (a)Exploitability
t=1 t=5 t=10 t=20 t=40 t=1 t=5 t=10 t=20 t=40
10âˆ’3 10âˆ’2 10âˆ’1 100 10âˆ’3 10âˆ’2 10âˆ’1 100
(b)Mean-FieldEvolution (b)Mean-FieldEvolution
Figure 1: (a) Off-MMD can approximate the performance Figure2:Off-MMDperformsbestwithintermediateandex-
of D-MOMD on the Exploration task when being trained pertqualitydatasets.Comparedtotheexplorationtask,the
onreasonablygooddatasets.Trainingrunswereconducted policytrainedontherandombehaviordatasetperformsbet-
over10seedsfor100iterationsofOff-MMDandD-MOMD. ter.ExperimentsettingsarethesameasinFigure1.
Wereportthemeanexploitabilityandthe95%confidence
interval. (b) Evolution of the mean-field over timestepsğ‘¡.
behaviorcanbeexplainedbythechallengesinherentinmulti-agent
Darkerareasindicatehigherstatedensity.
systems:thepolicyreturnisstronglyinfluencedbythebehavior
ofotheragents.Therefore,performanceachievedunderonemean-
Inourexperiments,weusedatasetscollectedbytheexpertpolicy fieldsettingmaynotbecomparabletoanother.
andtherandompolicyforcomputingthenormalizationboundsin InFigure4,wevisualizetheapproximationofthemean-field
Equation(19).Usingthethreedatasetsintroducedpreviouslyfor underdatasetsofvaryingqualityforafixedpolicy.Theleftmostcol-
thenavigationtask,wegenerate100syntheticdatasetsbyrandomly umnshowstheempiricalstatedistributionofthedatasetswhereas
subsamplingepisodes.Thedatasetsizesrangefrom1,000to100,000 thecenterandrightcolumnshowsthemean-fieldundersomefixed
episodes.WethentrainpoliciesusingOff-MMDonthesedatasets policy.Thegroundtruthmean-fieldservesasareferenceforthe
toevaluatetheeffectofdatasetqualityonperformance. approximationintheright-mostcolumnandisthereforeindepen-
Figure3presentstheexploitabilityofOff-MMDwhentrainedon dentofthedatasetquality.Figure4showshowtheapproximation
datasetswithvaryingstate-actioncoverageandtrajectoryquality. ofthemean-fieldchangeswiththestatecoverageinthedataset.
Theresultsindicateastrongcorrelationbetweenstate-actioncov- Thisisparticularlyprevalentinthefirstrow,wherewechosethe
erageandperformance,whereastrajectoryqualityappearstobea datasetwiththeloweststatecoveragefromthesetofsynthetic
weakerpredictor,exceptinextremecasessuchasexpertdemon- datasets.Theapproximationcannotprovideestimatesforunvis-
strationsorfullyrandomdatasets(highlightedinFigure3).This itedpartsofthestatespace.However,forthestatesthatareinthe
ytilibatiolpxE
enilnO
trepxE
tnI
dnaR
ytilibatiolpxE
enilnO
trepxE
tnI
dnaRExploitabilityvs. DatasetQuality Empirical GroundTruth Approximation
1.0
160
0.8
140
0.6
120
0.4
100
0.2 80
0.0 60
0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45
State-ActionCoverage
Expert Intermediate Random
Figure3:Exploitabilityvs.Data-Quality:Eachpointrepre-
sentsatrainingrunofOff-MMDonadatasetwithspecific
state-actioncoverageandtrajectoryquality.Thecolorindi-
catestheexploitabilityofthepolicyafter100iterationswith
darkercolorsindicatinghigherexploitability.Forreference,
wealsomarkthedatasetsusedinpreviousexperiments. Figure4:Theleftcolumnshowstheempiricalstatedistri-
butionofdatasetscollectedbybehaviorpolicyğœ‹ğ›½
(fromthe
navigationtask)withdifferentstatecoverages.Thecenter
dataset,itproducescorrectestimationsofthemean-fielddistribu- columnshowstheground-truthmean-fieldthatisgenerated
tion.Inscenarioswithhigherqualitydatasets,weobserveaccurate bythenewpolicytoevaluateandtherightcolumnshows
approximationsoftheground-truthdistribution. theapproximatedmean-fieldofthatpolicyusingjustthe
dataset.Themean-fieldsarepickedatğ‘¡ = 15.Whitespots
5.4 EffectofRegularization indicatenostate-actioncoverageinthisarea.
Inthefollowing,weinvestigatetheimportanceoftheCQLregu-
RegularizationStrength
larizationtermwithrespecttotherobustnesstovaryinglevelsof
600
state-coverageinthedatasetandthetrainingstabilityofOff-MMD. Î·=0.0
Thefirstexperiment,showninFigure5,aimstoexaminethe Î·=0.5
500
effect of the regularization hyperparameterğœ‚ on the quality of Î·=1.0
thepolicy.Weconducttrainingrunswithvaryingregularization Î·=2.0
400
strengths,rangingfrom0(e.g.noregularization)to5acrossdatasets Î·=5.0
D-MOMD
withdifferentlevelsofstate-actioncoverage.Specifically,weselect 300
fivedatasetsclosesttoeachstate-actioncoveragebin,withcover-
agevaluesrangingfrom0.15to0.45.Off-MMDistrainedfor100 200
iterationsoneachofthesedatasets,allowingustoanalyzetheinflu-
enceofregularizationunderdiversecoverageconditions.Figure5 100
showsthatmoderateregularizationallowstoreachcomparable
exploitabilityasD-MOMDonhigherstate-actioncoverageswhile 0.15 0.20 0.25 0.30 0.35 0.40 0.45
beingsignificantlymorerobustthanOff-MMDwithoutregulariza- DatasetCoverage
tion.Thebestfinalperformanceisachievedwithğœ‚ =2.0,which
coincideswiththerecommendedsettingforCQL[21].Furthermore, Figure5:Wereporttheexploitabilityofpolicieswithdif-
wecanseethatlargervaluesfortheregularizationhyperparameter ferentvaluesofğœ‚ ineq.(17).Wetraineachconfiguration
ğœ‚ dampenthedifferenceinperformanceoverdifferentcoverage on5datasetsthathavestate-actioncoveragesclosetoaspe-
levels,buthurtthemaximumachievableperformance. cificvalue,rangingfrom0.15to0.45.Wereportthemean
Inanotherablationexperiment,weinvestigatethetrainingstabil- exploitabilityofthepoliciesafter100iterations.Forrefer-
ityofOff-MMD,specificallyfocusingonmonotonicimprovement, ence,wealsoplottheperformanceoftheonlinebaseline
underdifferentvaluesoftheregularizationparameterğœ‚.Figure6 after100iterations.
presentstheresultsoffivetrainingrunsontheexpertdatasetofthe
navigationtask,eachwithadifferentvalueofğœ‚.Theresultsshow
thattheregularizationtermplaysacrucialroleinstabilizingthe
ytilauQyrotcejarT
ytilibatiolpxE
ytilauQwoL
ytilauQmuideM
ytilauQhgiH
ytilibatiolpxEOptimizationStability agents.Addressingthislimitationoffersapromisingavenueforfu-
tureresearch.Onepotentialsolutioncouldinvolveadaptingmodel-
600 Î·=0.0
basedalgorithmsspecificallydesignedforofflineRLsettings,to
Î·=0.5
500 Î·=1.0 handlemean-fielddependenciesindynamics.Suchadvancements
Î·=2.0 wouldbroadenthescopeofOff-MMD,makingitapplicabletoa
400 Î·=5.0 widerrangeofmulti-agentsystemswhereinteractionsbetween
agentsandtheenvironmentaremoreintertwined.
300
200 REFERENCES
[1] YvesAchdou,FabioCamilli,andItaloCapuzzo-Dolcetta.2012. MeanField
100 Games:NumericalMethodsforthePlanningProblem. SIAMJournalonCon-
trolandOptimization50,1(2012),77â€“109. https://doi.org/10.1137/100790069
arXiv:https://doi.org/10.1137/100790069
0 20 40 60 80 100 [2] YvesAchdouandItaloCapuzzo-Dolcetta.2010.MeanFieldGames:Numerical
Iteration Methods.SIAMJ.Numer.Anal.48,3(2010),1136â€“1162. https://doi.org/10.1137/
090758477arXiv:https://doi.org/10.1137/090758477
[3] JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,Chris
Figure6:Weoptimizepolicieswithdifferentvaluesofğœ‚on Leary,DougalMaclaurin,GeorgeNecula,AdamPaszke,JakeVanderPlas,Skye
Wanderman-Milne,andQiaoZhang.2018.JAX:composabletransformationsof
thesameexpertdatasetofthenavigationtasksandplotthe Python+NumPyprograms. http://github.com/jax-ml/jax
exploitabilityovertrainingiterations.Weshowthemean [4] GeorgeW.Brown.1951.IterativeSolutionofGamesbyFictitiousPlay.InActivity
and95%confidenceintervalover10seeds.
AnalysisofProductionandAllocation,T.C.Koopmans(Ed.).Wiley,NewYork.
[5] Cacace,Simone,Camilli,Fabio,andGoffi,Alessandro.2021.Apolicyiteration
methodformeanfieldgames.ESAIM:COCV27(2021),85. https://doi.org/10.
1051/cocv/2021081
trainingdynamics.Lowervaluesofğœ‚leadtooscillationsinpolicy [6] PierreCardaliaguetandSaeedHadikhanloo.2017.Learninginmeanfieldgames:
Thefictitiousplay.ESAIM:Control,OptimisationandCalculusofVariations23,2
performanceand,insomecases,evenresultindivergence,asseen (2017),569â€“591. https://doi.org/10.1051/cocv/2016004
intheunregularizedcase.Incontrast,highervaluesofğœ‚ reduce [7] ReneCarmona,DanielCooney,ChristyGraves,andMathieuLauriere.2019.
StochasticGraphonGames:I.TheStaticCase. arXiv:1911.10664[math.OC]
performancefluctuationsbetweeniterations,contributingtomore
https://arxiv.org/abs/1911.10664
stableandconsistentlearningprogress. [8] RenÃ©Carmona,MathieuLauriÃ¨re,andZongjunTan.2021.Model-FreeMean-
FieldReinforcementLearning:Mean-FieldMDPandMean-FieldQ-Learning.
arXiv:1910.12802[math.OC] https://arxiv.org/abs/1910.12802
6 CONCLUSION
[9] MinshuoChen,YanLi,EthanWang,ZhuoranYang,ZhaoranWang,andTuoZhao.
We present Offline Munchausen Mirror Descent (Off-MMD), a 2021.PessimismMeetsInvariance:ProvablyEfficientOfflineMean-FieldMulti-
AgentRL.InAdvancesinNeuralInformationProcessingSystems,M.Ranzato,
novelalgorithmdesignedforlearningequilibriumpoliciesinmean- A.Beygelzimer,Y.Dauphin,P.S.Liang,andJ.WortmanVaughan(Eds.),Vol.34.
fieldgamesusingonlyofflinedata.Thisapproachaddressesthe CurranAssociates,Inc.,17913â€“17926. https://proceedings.neurips.cc/paper_
files/paper/2021/file/9559fc73b13fa721a816958488a5b449-Paper.pdf
limitationsofexistingmethodsthatrelyoncostlyandoftenim-
[10] YangChen,LiboZhang,JiamouLiu,andShuyueHu.2022. Individual-Level
practicalonlineinteractions.Byleveragingimportancesampling InverseReinforcementLearningforMeanFieldGames.InProceedingsofthe21st
andğ‘„-valueregularizationtechniques,Off-MMDprovidesaneffi- InternationalConferenceonAutonomousAgentsandMultiagentSystems(Virtual
cientmeanstoapproximatethemean-fielddistributionfromstatic
Event,NewZealand)(AAMASâ€™22).InternationalFoundationforAutonomous
AgentsandMultiagentSystems,Richland,SC,253â€“262.
datasets,ensuringscalabilityandrobustnessincomplexenviron- [11] YangChen,LiboZhang,JiamouLiu,andMichaelWitbrock.2023.Adversarial
ments.Ourempiricalevaluationsdemonstratedthealgorithmâ€™s InverseReinforcementLearningforMeanFieldGames.InProceedingsofthe2023
InternationalConferenceonAutonomousAgentsandMultiagentSystems(London,
strongperformanceacrosstwocommonbenchmarksforMFGs, UnitedKingdom)(AAMASâ€™23).InternationalFoundationforAutonomousAgents
eveninscenarioswithlimiteddatacoverageorsub-optimaldatasets. andMultiagentSystems,Richland,SC,1088â€“1096.
[12] KaiCuiandHeinzKoeppl.2021.ApproximatelySolvingMeanFieldGamesvia
Withitsabilitytoscaleandadapttoreal-worldmulti-agentsystems,
Entropy-RegularizedDeepReinforcementLearning.InProceedingsofThe24th
Off-MMDopensnewavenuesforapplyingRLbasedalgorithms
InternationalConferenceonArtificialIntelligenceandStatistics(Proceedingsof
forMFGstosettingswhereonlineexperimentationisinfeasible, MachineLearningResearch,Vol.130),ArindamBanerjeeandKenjiFukumizu
(Eds.).PMLR,1909â€“1917. https://proceedings.mlr.press/v130/cui21a.html
irresponsibleordifficulttomodel.Webelievethatthisworklays
[13] ChristianFabian,KaiCui,andHeinzKoeppl.2023.LearningSparseGraphon
thefoundationforfutureresearchintoofflinelearningmethods MeanFieldGames.InProceedingsofThe26thInternationalConferenceonArtificial
forcomplex,large-scalemulti-agentinteractions,bridgingthegap IntelligenceandStatistics(ProceedingsofMachineLearningResearch,Vol.206),
FranciscoRuiz,JenniferDy,andJan-WillemvandeMeent(Eds.).PMLR,4486â€“
betweenofflineRLandMFGs.Futureresearchdirectionsinclude 4514. https://proceedings.mlr.press/v206/fabian23a.html
applicationstoreal-worlduse-casessuchaslocationrecommen- [14] XinGuo,AnranHu,RenyuanXu,andJunziZhang.2019. LearningMean-
dationsystemsortrafficrouting,twoproblemdomainssuffering
FieldGames.InAdvancesinNeuralInformationProcessingSystems,H.Wallach,
H.Larochelle,A.Beygelzimer,F.d'AlchÃ©-Buc,E.Fox,andR.Garnett(Eds.),Vol.32.
fromovercrowdingeffectsduetoselfishagents. CurranAssociates,Inc. https://proceedings.neurips.cc/paper_files/paper/2019/
file/030e65da2b1c944090548d36b244b28d-Paper.pdf
[15] JohannesHeinrich,MarcLanctot,andDavidSilver.2015.FictitiousSelf-Playin
6.1 Limitations
Extensive-FormGames.InProceedingsofthe32ndInternationalConferenceon
WhileOff-MMDmarksafirststeptowardsscalableofflineRLal- MachineLearning(ProceedingsofMachineLearningResearch,Vol.37),Francis
BachandDavidBlei(Eds.).PMLR,Lille,France,805â€“813. https://proceedings.
gorithmsforMFGs,itiscurrentlylimitedtoenvironmentswhere mlr.press/v37/heinrich15.html
thedynamicsareindependentofthemean-field.Thisassumption [16] JiaweiHuang,BatuhanYardim,andNiaoHe.2024.OntheStatisticalEfficiency
restrictsitsapplicabilitytoscenarioswheretheenvironmentdy- ofMean-FieldReinforcementLearningwithGeneralFunctionApproximation.
InProceedingsofThe27thInternationalConferenceonArtificialIntelligenceand
namicsarenotarbitrarilyinfluencedbythecollectivebehaviorof Statistics(ProceedingsofMachineLearningResearch,Vol.238),SanjoyDasgupta,
ytilibatiolpxEStephanMandt,andYingzhenLi(Eds.).PMLR,289â€“297. https://proceedings. Intelligence,IJCAI-21,Zhi-HuaZhou(Ed.).InternationalJointConferenceson
mlr.press/v238/huang24a.html ArtificialIntelligenceOrganization,356â€“362. https://doi.org/10.24963/ijcai.2021/
[17] MinyiHuang,RolandMalhame,andPeterCaines.2006.Largepopulationsto- 50MainTrack.
chasticdynamicgames:Closed-loopMcKean-VlasovsystemsandtheNash [33] SarahPerrin,JulienPerolat,MathieuLauriere,MatthieuGeist,RomualdElie,
certainty equivalence principle. Commun. Inf. Syst. 6 (01 2006). https: andOlivierPietquin.2020.FictitiousPlayforMeanFieldGames:Continuous
//doi.org/10.4310/CIS.2006.v6.n3.a5 TimeAnalysisandApplications.InAdvancesinNeuralInformationProcessing
[18] MatejJusup,BarnaPÃ¡sztor,TadeuszJanik,KenanZhang,FrancescoCorman, Systems,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),
AndreasKrause,andIlijaBogunovic.2024.SafeModel-BasedMulti-AgentMean- Vol.33.CurranAssociates,Inc.,13199â€“13213. https://proceedings.neurips.cc/
FieldReinforcementLearning.InProceedingsofthe23rdInternationalConference paper_files/paper/2020/file/995ca733e3657ff9f5f3c823d73371e1-Paper.pdf
onAutonomousAgentsandMultiagentSystems(Auckland,NewZealand)(AAMAS [34] KajetanSchweighofer,Marius-constantinDinu,AndreasRadler,MarkusHof-
â€™24).InternationalFoundationforAutonomousAgentsandMultiagentSystems, marcher,VihangPrakashPatil,AngelaBitto-nemling,HamidEghbal-zadeh,
Richland,SC,973â€“982. andSeppHochreiter.2022. ADatasetPerspectiveonOfflineReinforcement
[19] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Learning.InProceedingsofThe1stConferenceonLifelongLearningAgents(Pro-
Joachims.2020. MOReL:Model-BasedOfflineReinforcementLearning.In ceedingsofMachineLearningResearch,Vol.199),SarathChandar,RazvanPascanu,
Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- andDoinaPrecup(Eds.).PMLR,470â€“517. https://proceedings.mlr.press/v199/
zato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates, schweighofer22a.html
Inc.,21810â€“21823. https://proceedings.neurips.cc/paper_files/paper/2020/file/ [35] SriramGanapathiSubramanian,MatthewE.Taylor,MarkCrowley,andPascal
f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf Poupart.2022. DecentralizedMeanFieldGames. ProceedingsoftheAAAI
[20] IlyaKostrikov.2021.JAXRL:ImplementationsofReinforcementLearningalgo- ConferenceonArtificialIntelligence36,9(Jun.2022),9439â€“9447. https://doi.org/
rithmsinJAX. https://doi.org/10.5281/zenodo.5535154 10.1609/aaai.v36i9.21176
[21] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020. [36] JunfengWen,BoDai,LihongLi,andDaleSchuurmans.2020.Batchstationary
ConservativeQ-LearningforOfflineReinforcementLearning.InAdvances distributionestimation.InProceedingsofthe37thInternationalConferenceon
in Neural Information Processing Systems, H. Larochelle, M. Ranzato, MachineLearning(ICMLâ€™20).JMLR.org,Article945,11pages.
R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, [37] QiaominXie,ZhuoranYang,ZhaoranWang,andAndreeaMinca.2021.Learning
Inc.,1179â€“1191. https://proceedings.neurips.cc/paper_files/paper/2020/file/ WhilePlayinginMean-FieldGames:ConvergenceandOptimality.InProceedings
0d2b2061826a5df3221116a5085a6052-Paper.pdf ofthe38thInternationalConferenceonMachineLearning(ProceedingsofMachine
[22] Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, LearningResearch,Vol.139),MarinaMeilaandTongZhang(Eds.).PMLR,11436â€“
SatyakiUpadhyay,JulienPÃ©rolat,SriramSrinivasan,FinbarrTimbers,KarlTuyls, 11447. https://proceedings.mlr.press/v139/xie21g.html
ShayeganOmidshafiei,DanielHennes,DustinMorrill,PaulMuller,TimoEwalds, [38] Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. 2019. Towards Opti-
RyanFaulkner,JÃ¡nosKramÃ¡r,BartDeVylder,BrennanSaeta,JamesBradbury, mal Off-Policy Evaluation for Reinforcement Learning with Marginalized
DavidDing,SebastianBorgeaud,MatthewLai,JulianSchrittwieser,ThomasAn- Importance Sampling. In Advances in Neural Information Processing Sys-
thony,EdwardHughes,IvoDanihelka,andJonahRyan-Davis.2019.OpenSpiel: tems, Vol. 32. https://proceedings.neurips.cc/paper_files/paper/2019/file/
AFrameworkforReinforcementLearninginGames.CoRRabs/1908.09453(2019). 4ffb0d2ba92f664c2281970110a2e071-Paper.pdf
arXiv:1908.09453[cs.LG] http://arxiv.org/abs/1908.09453 [39] JiachenYang,XiaojingYe,RakshitTrivedi,HuanXu,andHongyuanZha.
[23] J.M.LasryandPierre-LouisLions.2007.Meanfieldgames.JapaneseJournal 2018.DeepMeanFieldGamesforLearningOptimalBehaviorPolicyofLarge
ofMathematics2(2007),229â€“260. https://api.semanticscholar.org/CorpusID: Populations.InInternationalConferenceonLearningRepresentations. https:
1963678 //openreview.net/forum?id=HktK4BeCZ
[24] MathieuLauriÃ¨re,SarahPerrin,SertanGirgin,PaulMuller,AyushJain,Theophile [40] YaodongYang,RuiLuo,MinneLi,MingZhou,WeinanZhang,andJunWang.
Cabannes,GeorgiosPiliouras,JulienPÃ©rolat,RomualdElie,OlivierPietquin,etal. 2018.MeanFieldMulti-AgentReinforcementLearning.InProceedingsofthe35th
2022.Scalabledeepreinforcementlearningalgorithmsformeanfieldgames.In InternationalConferenceonMachineLearning(ProceedingsofMachineLearning
InternationalConferenceonMachineLearning.PMLR,12078â€“12095. Research,Vol.80),JenniferDyandAndreasKrause(Eds.).PMLR,5571â€“5580.
[25] MathieuLauriÃ¨re,SarahPerrin,JulienPÃ©rolat,SertanGirgin,PaulMuller,Ro- https://proceedings.mlr.press/v80/yang18d.html
mualdÃ‰lie,MatthieuGeist,andOlivierPietquin.2024.LearninginMeanField [41] TianheYu,GarrettThomas,LantaoYu,StefanoErmon,JamesYZou,Sergey
Games:ASurvey. arXiv:2205.12944[cs.LG] https://arxiv.org/abs/2205.12944 Levine,ChelseaFinn,andTengyuMa.2020.MOPO:Model-basedOfflinePolicy
[26] JongminLee,WonseokJeon,ByungjunLee,JoellePineau,andKee-EungKim. Optimization.InAdvancesinNeuralInformationProcessingSystems,H.Larochelle,
2021.OptiDICE:OfflinePolicyOptimizationviaStationaryDistributionCorrec- M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates,
tionEstimation.InProceedingsofthe38thInternationalConferenceonMachine Inc.,14129â€“14142. https://proceedings.neurips.cc/paper_files/paper/2020/file/
Learning(ProceedingsofMachineLearningResearch,Vol.139),MarinaMeilaand a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf
TongZhang(Eds.).PMLR,6120â€“6130. https://proceedings.mlr.press/v139/lee21f. [42] RuiyiZhang*,BoDai*,LihongLi,andDaleSchuurmans.2020.GenDICE:Gen-
html eralizedOfflineEstimationofStationaryValues.InInternationalConferenceon
[27] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness, LearningRepresentations. https://openreview.net/forum?id=HkxlcnVFwB
MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasKirkebyFidjeland,
GeorgOstrovski,StigPetersen,CharlieBeattie,AmirSadik,IoannisAntonoglou,
HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.
2015.Human-levelcontrolthroughdeepreinforcementlearning.Nature518
(2015),529â€“533. https://api.semanticscholar.org/CorpusID:205242740
[28] OfirNachum,YinlamChow,BoDai,andLihongLi.2019.DualDICE:Behavior-
AgnosticEstimationofDiscountedStationaryDistributionCorrections.InAd-
vancesinNeuralInformationProcessingSystems,H.Wallach,H.Larochelle,
A.Beygelzimer,F.d'AlchÃ©-Buc,E.Fox,andR.Garnett(Eds.),Vol.32.Cur-
ranAssociates,Inc. https://proceedings.neurips.cc/paper_files/paper/2019/file/
cf9a242b70f45317ffd281241fa66502-Paper.pdf
[29] BarnaPÃ¡sztor,AndreasKrause,andIlijaBogunovic.2023.EfficientModel-Based
Multi-AgentMean-FieldReinforcementLearning. TransactionsonMachine
LearningResearch(2023). https://openreview.net/forum?id=gvcDSDYUZx
[30] JulienPÃ©rolat,SarahPerrin,RomualdElie,MathieuLauriÃ¨re,GeorgiosPiliouras,
MatthieuGeist,KarlTuyls,andOlivierPietquin.2022.ScalingMeanFieldGames
byOnlineMirrorDescent.InProceedingsofthe21stInternationalConference
onAutonomousAgentsandMultiagentSystems(VirtualEvent,NewZealand)
(AAMASâ€™22).InternationalFoundationforAutonomousAgentsandMultiagent
Systems,Richland,SC,1028â€“1037.
[31] SarahPerrin.2022.ScalingupMulti-agentReinforcementLearningwithMeanField
GamesandVice-versa.Theses.UniversitÃ©deLille. https://theses.hal.science/tel-
04284876
[32] SarahPerrin,MathieuLauriÃ¨re,JulienPÃ©rolat,MatthieuGeist,RomualdÃ‰lie,and
OlivierPietquin.2021.MeanFieldGamesFlock!TheReinforcementLearning
Way.InProceedingsoftheThirtiethInternationalJointConferenceonArtificialA HYPERPARAMETERS
ThehyperparametersfortheexperimentsontheperformanceevaluationareshowninTable1.ForD-MOMD,weprovidehyperparameters
inTable2.
Hyperparameter Value Description
LearningRate 0.001 Stepsizeforgradientupdates
ğœ 20.0 Temperature
ğ›¼ 0.99 Weightformomentuminupdates
ğ›¾ 0.99 Discountfactor
ğœ‚ 3.0 RegularizationtermforCQL
B 2000 Numberofbatches
N 512 Batchsize
Table1:HyperparametersforOff-MMD.
Hyperparameter Value Description
BufferSize 100,000 Sizeofthereplaybuffer
N 256 Numberofsamplesdrawnfromthebufferperupdate
Updatesteps 4000 Gradientupdatesperiteration
LearningRate 0.001 Stepsizeforgradientupdates
ğœ–(start) 1.0 Initialvalueofğœ–-greedyexploration
ğœ–(finish) 0.1 Finalvalueofğœ–
ğœ–annealtime 1,000,000 Numberofstepstoannealepsilonfromstarttofinish
ğœ 20.0 Temperature
ğ›¼ 0.99 Weightformomentuminupdates
ğ›¾ 0.99 DiscountFactor
TargetUpdateInterval 200 Stepsbetweentargetnetworkupdates
TrainingInterval 10 Numberofenvironmentstepsperupdate
Table2:HyperparametersforD-MOMD