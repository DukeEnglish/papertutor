TENSOR NETWORK-CONSTRAINED KERNEL MACHINES AS
GAUSSIAN PROCESSES
APREPRINT
FrederiekWesel KimBatselier
DelftCenterforSystemsandControl DelftCenterforSystemsandControl
DelftUniversityofTechnology DelftUniversityofTechnology
TheNetherlands TheNetherlands
f.wesel@tudelft.nl k.batselier@tudelft.nl
March29,2024
ABSTRACT
TensorNetworks(TNs)haverecentlybeenusedtospeedupkernelmachinesbyconstrainingthe
modelweights,yieldingexponentialcomputationalandstoragesavings. Inthispaperweprovethat
theoutputsofCanonicalPolyadicDecomposition(CPD)andTensorTrain(TT)-constrainedkernel
machinesrecoveraGaussianProcess(GP),whichwefullycharacterize,whenplacingi.i.d. priors
overtheirparameters. Weanalyzetheconvergenceofboth CPD and TT-constrainedmodels,and
showhowTTyieldsmodelsexhibitingmoreGPbehaviorcomparedtoCPD,forthesamenumber
ofmodelparameters. Weempiricallyobservethisbehaviorintwonumericalexperimentswhere
werespectivelyanalyzetheconvergencetotheGPandtheperformanceatprediction. Wethereby
establishaconnectionbetweenTN-constrainedkernelmachinesandGPs.
1 Introduction
TensorNetworks[TNs,Cichocki,2014;Cichockietal.,2016,2017],atoolfrommultilinearalgebra,extendtheconcept
ofrankfrommatricestotensorsallowingtorepresentanexponentiallylargeobjectwithalinearnumberofparameters.
Assuch,TNshavebeenusedtoreducethestorageandcomputationalcomplexitiesbycompressingthemodelparameters
ofarangeofmodelssuchasDeepNeuralNetworks(DNNs)[Novikovetal.,2015],ConvolutionalNeuralNetworks
(CNNs)[Jaderbergetal.,2014;Lebedevetal.,2015], RecurrentNeuralNetworks(RNNs)[Yeetal.,2018], Graph
NeuralNetworks(GNNs)[Huaetal.,2022]andtransformers[Maetal.,2019].
Similarly,TNshavealsofoundapplicationinthecontextofkernelmachines[StoudenmireandSchwab,2016;Novikov
etal.,2018;WeselandBatselier,2021]. Suchmodelslearnamultilinear(i.e. nonlinear)data-dependentrepresentation
from an exponentially large number of fixed features by means of a linear number of parameters, and are as such
characterizedbyanimplicitsourceofregularization. Furthermore,storageandtheevaluationofthemodelandits
gradientrequirealinearcomplexityinthenumberofparameters,renderingthesemethodspromisingcandidatesfor
applicationsrequiringbothgoodgeneralizationandscalability. Howevertheirmultilinearityprecludesclosed-form
Bayesianinference,andhasrestrictedthetrainingofthesemodelstothemaximumlikelihood(ML)andmaximuma
posteriori(MAP)framework.
Incontrast,GaussianProcesses[GPs,RasmussenandWilliams,2006]areanestablishedframeworkformodeling
functions which naturally allow the practitioner to incorporate prior knowledge. Importantly, when considering
i.i.d. observationsandGaussianlikelihoods,GPsallowforthedeterminationoftheposteriorinclosed-form,which
considerablyfacilitatestaskssuchasinference,samplingandtheconstructionofsparseapproximationsamongmany
others. Themaindrawbackofhavingaclosed-formposteriorlieshoweverintheirinabilitytoautonomouslylearn
features,whichhasarguablyfavoredtheuseofdeeplearningmodels.
InthispaperweestablishaconnectionbetweenTN-constrainedkernelmachinesandGPs,thussolvinganopenproblem
consideredbyWeselandBatselier[2021,2023]. WeprovethattheoutputsofCanonicalPolyadicDecomposition
4202
raM
82
]GL.sc[
1v00591.3042:viXraTensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
(CPD)andTensorTrain(TT)-constrainedkernelmachinesconvergetoafullycharacterizedGPwhenspecifyingi.i.d.
priorsacrosstheircomponents. Thisresultallowsustoderivethatthatforthesamenumberofmodelparameters,
TT-constrainedmodelsachievefasterconvergencetotheGPcomparedtotheirCPDcounterpartsandthusaremore
pronetoexhibit GP behavior. Weanalyzetheconsequencesofthesefindingsinthecontextof MAP estimationand
finallyempiricallyobserveGPconvergenceandGPbehaviorintwonumericalexperiments.
Therestofthispaperisorganizedasfollows.Insection2weprovideabriefintroductiontoGPsandtheirapproximations,
TNsand TN-constrainedkernelmachines. Insection3wepresentourmainresult,i.e. theequivalenceinthelimit
betweenTN-constrainedkernelmachinesandGPs. Insection4wediscussournumericalresultswhichillustratethe
foundresults. Wethenprovideareviewofrelatedwork(section5)andaconclusion(section6). Wediscussthenotation
usedthroughoutthepaperinappendixA.
2 Background
GPs are a collection of random variables such that any finite subset has a joint Gaussian distribution [Rasmussen
and Williams, 2006]. They provide a flexible formalism for modeling functions which inherently allows for the
incorporationofpriorknowledgeandtheproductionofuncertaintyestimatesintheformofapredictivedistribution.
Morespecifically,aGPisfullyspecifiedbyameanfunctionÂµâˆˆR,typicallychosenaszero,andacovarianceorkernel
functionk(Â·,Â·):RDÃ—RD â†’R:
f(x)âˆ¼GP(Âµ,k(x,Â·)).
Givenalabeleddataset{(x n,y n)}N n=1consistingofinputsx
n
âˆˆRD andi.i.d. noisyobservationsy
n
âˆˆR,GPscanbe
usedformodelingtheunderlyingfunctionf inclassificationorregressiontasksbyspecifyingalikelihoodfunction.
Forexamplethelikelihood
p(y |f(x ))=N(f(x ),Ïƒ2), (1)
n n n
yieldsa GP posteriorwhichcanbeobtainedinclosed-formbyconditioningtheprior GP onthenoisyobservations.
Calculatingthemeanandcovarianceofsuchaposteriorcruciallyrequiresinstantiatingandformallyinvertingthe
kernelmatrixK suchthatk :=k(x ,x ). TheseoperationsrespectivelyincuracomputationalcostofO(N2)and
n,m n m
O(N3)andthereforeprohibittheprocessingoflarge-sampleddatasets.
2.1 BasisFunctionApproximation
TheprevailingapproachinliteraturetocircumventtheO(N3)computationalbottleneckistoprojecttheGPontoa
finitenumberofBasisFunctions(BFs)[e.g.,RasmussenandWilliams,2006;QuinËœonero-CandelaandRasmussen,
2005]. Thisisachievedbyapproximatingthekernelas
k(x,xâ€²)â‰ˆÏ†(x)TÎ›Ï†(xâ€²), (2)
where here Ï†(x) : RD â†’ RM are (nonlinear) basis functions and Î› âˆˆ RMÃ—M are the BF weights. This finite-
dimensionalkernelapproximationensuresadegeneratekernel[RasmussenandWilliams,2006],asitcharacterizedby
afinitenumberofnon-zeroeigenvalues. ItsassociatedGPcanbecharacterizedequivalentlyas
f(x)=âŸ¨Ï†(x),wâŸ©, w âˆ¼N(0,Î›), (3)
whereinw âˆˆRM arethemodelweightsandÎ›istheassociatedpriorcovariance. OncemoreconsideringaGaussian
likelihood(equation(1))yieldsaclosed-formposteriorGPwhosemeanandcovariancerequireonlytheinversionof
thematrix(cid:80)N Ï†(x )Ï†(x )T. ThisyieldsacomputationalcomplexityofO(NM2+M3),whichallowstotackle
n=1 n n
large-sampleddatawhenN â‰«M.
2.2 ProductKernels
IntheremainderofthispaperweconsiderGPswithproductkernels.
Definition2.1(Productkernel[RasmussenandWilliams,2006]). Akernelk(x,xâ€²)isaproductkernelif
D
(cid:89)
k(x,xâ€²)= k(d)(x ,xâ€²), (4)
d d
d=1
whereeachk(d)(Â·,Â·):RÃ—Râ†’Risavalidkernel.
2TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Whilemanycommonlyusedkernelsareproductkernelse.g. theGaussiankernelandthepolynomialskernel,product
kernelsprovideastraightforwardstrategytoextendone-dimensionalkernelstothehigher-dimensionalcase[Rasmussen
andWilliams,2006;Hensmanetal.,2017]. Thebasisfunctionsandpriorcovarianceofproductkernelscanthenbe
determinedbasedonthebasisfunctionexpansionoftheirconstituentsasfollows.
Lemma2.2(Basisfunctionsandpriorcovariancesofproductkernels). Considertheproductkernelofdefinition2.1.
Denotethebasisfunctionsandpriorcovarianceofeachfactork(d)(x d,xâ€² d)asÏ†(d)(x d)âˆˆRMd andÎ›(d) âˆˆRMdÃ—Md
respectively,thenthebasisfunctionsandpriorcovarianceofk(x,xâ€²)are
Ï†(x)=âŠ—D Ï†(d)(x ), (5)
d=1 d
and
Î›=âŠ—D Î›(d), (6)
d=1
The inherent challenge in this approach stems from the exponential increase of the number of basis functions M
andthusofmodelparametersasafunctionofthedimensionalityoftheinputdata,therebyrestrictingtheirutilityto
low-dimensionaldatasets.
SuchstructurearisesforinstancewhendealingwithMercerexpansionsofproductkernels,inthestructuredkernel
interpolation framework [Wilson and Nickisch, 2015; Yadav et al., 2021] variational Fourier features framework
[Hensmanetal.,2017]andHilbert-GPframework[SolinandSaÂ¨rkkaÂ¨,2020]. Alternativeimportantapproximation
strategieswhichavoidthisexponentialscalingarerandomfeatures[RahimiandRecht,2007;LaÂ´zaro-Gredillaetal.,
2010], inducing features [CsatoÂ´ and Opper, 2002; Seeger et al., 2003; QuinËœonero-Candela and Rasmussen, 2005;
SnelsonandGhahramani,2006;Hensmanetal.,2013,2015]andadditiveGPs[Duvenaudetal.,2011;Luetal.,2022]
whichcircumventtheoutlinedcomputationalissue. AllthoseapproachescanbeinterpretedasprojectingtheGPona
setofBFs.
The performance of these methods however tends to deteriorate in higher dimensions, as they need to cover an
exponentiallylargedomainwithalinearnumberofrandomsamplesorinducingpoints. Theseissuesaresomeofthe
computationalaspectsofthecurseofdimensionality,whichrendersitdifficulttooperateinhigh-dimensionalfeature
spaces[Hastieetal.,2001].
2.3 TensorNetworks
Arecentalternativeapproachtoremedysaidcurseofdimensionalityaffectingtheexponentiallyincreasingweightsof
thelinearmodelinequation(3)consistsinconstrainingthemodelsweightswtobealow-ranktensornetwork. TNs
expressaD-dimensionaltensorW asamulti-linearfunctionofanumberofcoretensors. TwocommonlyusedTNsare
theCPDandTT,definedasfollows.
Definition2.3(CPD[Hitchcock,1927]). AD-dimensionaltensorW âˆˆRM1Ã—M2Ã—Â·Â·Â·Ã—MD hasarank-RCPDif
R D
(cid:88)(cid:89)
w = w(d) . (7)
m1,m2,...,mD md,r
r=1d=1
ThecoresofaCPDarethematricesW(d) âˆˆRMdÃ—R. SinceaCPDtensorcanbeexpressedsolelyintermsofitscores,
itsstoragerequiresP
=R(cid:80)D
M
parametersasopposedto(cid:81)D
M .
CPD d=1 d d=1 d
Definition 2.4 (TT [Oseledets, 2011]). A D-dimensional tensor W âˆˆ RM1Ã—M2Ã—Â·Â·Â·Ã—MD admits a rank-
(R :=1,R ,...,R :=1)tensortrainif
0 1 D
(cid:88)R0 (cid:88)R1 (cid:88)RD (cid:89)D
w = Â·Â·Â· w(d) . (8)
m1,m2,...,mD rdâˆ’1,md,rd
r0=1r1=1 rD=1d=1
ThecoresofatensortrainareD3-dimensionaltensorsW(d) âˆˆRRdâˆ’1Ã—MÃ—Rd whichyieldP
TT
=(cid:80)D d=1R Dâˆ’1M DR
D
parameters.
InthefollowingwedenotebyTN(W)atensorwhichadmitsageneralTNformat,byCPD(W)atensorwhichadmitsa
rank-RCPformandbyTT(W)atensorinrank-(R 0:=1,R 1,...,R D:=1)TTform. Lastly,wedenotebyR 1(W)a
tensorwhichisinrank-1CPformorrank-(1,1,...,1)TT,asbothareequivalent.
Importantly,werefertoatensoringeneral TN formatTN(W) âˆˆ RM1Ã—M2Ã—Â·Ã—MD asunderparametrized ifitsrank
hyperparameters,e.g.
RincaseofCPD,arechosensuchthatitsstoragecostislessthan(cid:81)D
d=1M d. Thisiscrucialin
ordertoobtainstorage,andaswewillsee,computationalbenefits.
3TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
2.4 TensorNetwork-ConstrainedKernelMachines
TNshavebeenusedtoreducethenumberofmodelparametersinkernelmachines(equation(3))bytensorizingtheBFs
Ï†(x)andmodelweightswandbyconstrainingbothtobeunderparameterizedTNs. Thisapproachlaysitsfoundations
onthefactthattheFrobeniusinnerproductofatensorizedvectorisisometricwithrespecttotheEuclideaninner
product,i.e.
f(x)=âŸ¨Ï†(x),wâŸ©=âŸ¨ten(Ï†(x)),ten(w)âŸ© . (9)
F
ThisisometryallowsthentoconstraintheBFsandthemodelweightstobeanunderparameterizedTN. Sinceproduct
kernelsyieldanexpansionintermsofKronecker-productBFs(equation(5)),theyarearank-1TNbydefinitionafter
tensorization. Embeddingtheserelationsyieldsanapproximatemodel
f(x)â‰ˆf (x):=âŸ¨R (ten(Ï†(x))),TN(ten(w))âŸ© , (10)
TN 1 F
characterizedbylowerstorageandcomputationalcomplexities. Thisapproachhasbeenproposedmostlyforweights
modeledasCPD[KargasandSidiropoulos,2021;WeselandBatselier,2021,2023]orTT[Wahlsetal.,2014;Stouden-
mireandSchwab,2016;Batselieretal.,2017;Novikovetal.,2018;Chenetal.,2018]astheyarguablyintroducefewer
hyperparameters(onlyoneincaseofCPD)andthusareinpracticeeasiertoworkwithcomparedtootherTNssuchas
theMulti-ScaleEntanglementRenormalizationAnsatz(MERA)[ReyesandStoudenmire,2021].
Wedefinesuchmodelsaswewillneedthemindetailinthenextsection,wherewepresentourmaincontribution.
Definition2.5(CPD-constrainedkernelmachine). TheCPD-constrainedkernelmachineisdefinedas
f (x):=âŸ¨R (ten(Ï†(x))),CPD(ten(w))âŸ© (11)
CPD 1 F
R
(cid:88)
= h (x), (12)
r
r=1
wheretheintermediatevariablesh âˆˆRaredefinedas
r
D
h (x):=(cid:89) Ï†(d)(x )T w(d) . (13)
r d :,r
d=1
Similarly,weprovideadefinitionfortheTT-constrainedkernelmachine.
Definition2.6(TT-constrainedkernelmachine). TheTT-constrainedkernelmachineisdefinedas
f (x):=âŸ¨R (ten(Ï†(x))),TT(ten(w))âŸ© (14)
TT 1 F
(cid:88)RD R (cid:88)Dâˆ’1 (cid:88)R0 (cid:89)D
= Â·Â·Â· z(d) (x ), (15)
rdâˆ’1,rd d
rD=1rDâˆ’1=1 r0=1d=1
wheretheintermediatevariablesZ(d) âˆˆRRdâˆ’1Ã—Rd aredefinedelement-wiseas
(cid:88)Md
z(d) (x ):= Ï†(d)(x )w(d) . (16)
rdâˆ’1,rd d md d rdâˆ’1,md,rd
md=1
Evaluating CPD and TT-constrained kernel machines (equation (11), equation (14)) and their gradients can be ac-
complished with O(P ) and O(P ) computations, respectively. This allows the practitioner to tune the rank
CPD TT
hyperparameter in order to achieve a model that fits in the computational budget at hand and that learns from the
specifiedBFs.
Fromanoptimizationpoint-of-view,modelsintheformofequation(10)havebeentrainedbothintheML[Stoudenmire
andSchwab,2016;Batselieretal.,2017]andintheMAPsetting[Wahlsetal.,2014;Novikovetal.,2018;Chenetal.,
2018;KargasandSidiropoulos,2021;WeselandBatselier,2021,2023]andinthecontextofGPvariationalinference
[Izmailovetal.,2018]whereTTsareusedtoparameterizethevariationaldistribution. Itishowevernotclearifandhow
thesemodelsrelatetotheweight-spaceGPequation(3).
Inthefollowingsectionwepresentthemaincontributionofourwork: weshowhowwhenplacingi.i.d. priorsonthe
coresoftheseapproximatemodels,theyconvergetoaGPwhichwefullycharacterize.
4TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
P =1600 P =1600000 P =1320 P =1605000
CPD CPD TT TT
Figure1: Histogramsoftheempirical PDF of CPD (blue)and TT (orange)modelsspecifiedintheorem3.1and3.2
evaluatedatarandompointasafunctionofmodelparametersP forD = 16. Theblacklineisthe PDF ofthe GP.
NoticehowTTconvergesfastertotheGPforthesamenumberofmodelparametersP.
3 TN-ConstrainedKernelMachinesas GPs
WecommencetooutlinethecorrespondencebetweenTN-constrainedkernelmachineandGPs,whichmakesuseofthe
CentralLimitTheorem(CLT). Webeginbyelucidatingthesimplestcase,i.e. theCPD.
Theorem3.1(GPlimitofCPD-constrainedkernelmachine). ConsidertheCPD-constrainedkernelmachine
f (x):=âŸ¨R (ten(Ï†(x))),CPD(ten(w))âŸ© .
CPD 1 F
IfeachoftheRcolumnsw(d)
:,r
âˆˆRMd ofeachCPDcoreisani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E w(d) =0,
:,r
E(cid:104)
w :( ,d r)w :( ,d
r)T(cid:105)
=Râˆ’ D1Î›(d),
thenf (x)convergesindistributionasRâ†’âˆžtotheGP
CPD
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
CPD d
d=1
Proof. SeeappendixB.1.
AsimilarresultcanbeconstructedfortheTTcase.
Theorem3.2(GPlimitofTT-constrainedkernelmachine). ConsidertheTT-constrainedkernelmachine
f (x):=âŸ¨R (ten(Ï†(x))),TT(ten(w))âŸ©
TT 1 F
IfeachoftheR dâˆ’1R dfibersW(d)
rdâˆ’1,:,rd
âˆˆRMd ofeachTTcoreisani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E W(d) =0,
rdâˆ’1,:,rd
E(cid:104)
W(d) W(d)
T(cid:105)
=
1
Î›(d),
rdâˆ’1,:,rd rdâˆ’1,:,rd (cid:112)
R R
dâˆ’1 d
thenf (x)convergesindistributionasR â†’âˆž,R â†’âˆž,...,R â†’âˆžtotheGaussianprocess
TT 1 2 Dâˆ’1
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
TT d
d=1
Proof. SeeappendixB.2.
5TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Theorem3.2guaranteestheconvergenceindistributionoff (x)totheGPofequation(3)bytakingsuccessivelimits
TT
ofeachTTrank. Importantly,thesameconvergenceresultsalsoholdstrueiftheTTranksgrowsimultaneously,see
appendixB.3.
Boththeorem3.1and3.2areremarkable,astheyimplythatadegenerateGPwhichcanbedefinedwithanexponential
numberof(cid:81)D
d=1M dweightswcanbealsoobtainedwithaninfinitenumberofmodelparametersP usingtheCPD-
constrainedmodelofdefinition2.5ortheTT-constrainedmodelofdefinition2.6. Further,theorem3.1and3.2suggest
thatCPDandTT-basedmodelsexhibitGPbehaviorintheoverparameterizedregime. GPbehaviorischaracterizedby
afixedlearningrepresentation,whichincaseofthekernelintheorem3.1and3.2isfullydefinedbytheBFsandis
hencedata-independent. Onthecontrary,inthefiniterankregime,bothCPDandTTmodelsareabletocraftnonlinear
featuresfromtheprovidedBFs,potentiallylearningmorelatentpatternsinthedata.
3.1 ConvergenceRatestotheGP
Whileboththeorem3.1andtheorem3.2guaranteeconvergenceindistributiontotheGPofequation(3),theydosoat
ratesthatdifferintermsofthenumberofmodelparameters. Letusassume,forsimplicity,thatthenumberofbasis
functionsisthesamealongeachdimension,i.e.,M,andthattheDâˆ’1TTranksequalR.Itfollowsthenthatthenumber
ofCPDmodelparametersP =MDR andthenumberofTTmodelparametersP =M(Dâˆ’2)R2 +2MR =
CPD CPD TT TT TT
O(MDR2 ). Given the convergence rate of the CLT for the expression in equation (11) to the GP in equation (3),
TT
denotedasO(1/(cid:113) R CPD)withrespecttothevariableP CPD,wecanestablishthefollowingcorollarybysubstitutingR CPD
asafunctionofP .
CPD
Corollary3.3(ConvergencerateforCPD). Undertheconditionsoftheorem3.1,thefunctionf (x)convergesin
CPD
distributiontotheGPdefinedbyequation(3). Theconvergencerateisgivenby:
(cid:32)(cid:18) (cid:19)1(cid:33)
MD 2
f (x)â†’O .
CPD P
CPD
Due to their hierarchical structure, TT models are a composition of RDâˆ’1 variables, but can be represented in a
TT
quadraticnumberofmodelparametersinR ,sinceP =O(MDR2 ). ExpressingthentheCLTconvergencerateof
TT TT TT
O(1/(cid:113) R TTDâˆ’1)asafunctionofP TTyieldsthefollowingcorollary.
Corollary 3.4 (Convergence rate for TT). Under the conditions of theorem 3.2, the function f (x) converges in
TT
distributiontotheGPdefinedbyequation(3). Theconvergencerateisgivenby:
(cid:32)(cid:18) (cid:19)Dâˆ’1(cid:33)
MD 4
f (x)â†’O .
TT P
TT
Therefore,whendealingwithidenticalmodelsintermsofthenumberofbasisfunctions(M),dimensionalityofthe
inputs(D),andthenumberofmodelparameters(P =P ),f (x)willconvergeatapolynomiallyfasterratethan
CPD TT TT
f (x),thusexhibitingGPbehaviorwithareducednumberofmodelparameters. Inparticular,basedoncorollaries3.3
CPD
and3.4weexpecttheGPconvergencerateofTTmodelstobefasterforD â‰¥3.
TheseinsightsarerelevantforpractitionersengagedwithTN-constrainedkernelmachines,astheyshedlightonthe
balancebetweentheGPand(deep)neuralnetworkbehaviorinherentinthesemodels. Notably,CPDandTT-constrained
models,akintoshallowandDNNsrespectively,havethecapacitytocraftadditionalnonlinearitiesbeyondtheprovided
basis functions. This characteristic can result in superior generalization when dealing with a limited number of
parameters. However,astheparametercountincreases,weexpectthesemodelstotransitiontowards GP behavior,
characterizedbyafixedfeaturerepresentationandstaticincomparison.
3.2 ConsequencesforMAPEstimation
Asdiscussedinsection2.4, TN-constrainedkernelmachinesaretypicallytrainedinthe ML or MAP frameworkby
constraining the weights w in the log-likelihood or log-posterior to be a TN. In said MAP context, and e.g. when
specifyinganormalprioronthemodelweightsw âˆ¼N(0,Î›),theresultingregularizationtermâ„¦isapproximatedby
â„¦ as
TN
(cid:12)(cid:12) (cid:12)(cid:12)2 (cid:12)(cid:12) (cid:16) (cid:17) (cid:12)(cid:12)2
â„¦:=(cid:12) (cid:12)(cid:12) (cid:12)Î›âˆ’1 2w(cid:12) (cid:12)(cid:12)
(cid:12)
â‰ˆâ„¦ TN:=(cid:12) (cid:12)(cid:12) (cid:12)TN(ten Î›âˆ’ 21w )(cid:12) (cid:12)(cid:12)
(cid:12)
,
F F
6TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
 '     '     '     '    
 & 3 '       Ã®   
     7 7
  Ã®   
          Ã®   
   
   
  Ã®   
   
                                                             
 3  3  3  3
Figure2: MeanandstandarddeviationoftheCrameÂ´râ€“vonMisesstatisticW2 evaluatedbetweentheempiricalCDF
ofCPDandTTmodelsspecifiedintheorem3.1and3.2evaluatedatN = 10randompointsasafunctionofmodel
parametersP forD =2,4,8,16. ThetwomodelsareequivalentforD =2. NoticehowTTconvergesfastertotheGP
asthedimensionalityoftheinputsDincreases.
whereÎ›=âŠ—D Î›(d). Forexample,incaseofCPD-constrainedmodelswehave
d=1
â„¦ =(cid:12) (cid:12)(cid:12) (cid:12)âŠ™D (cid:16) W(d)T Î›(d)âˆ’1 W(d)(cid:17)(cid:12) (cid:12)(cid:12) (cid:12)2 . (17)
CPD (cid:12)(cid:12) d=1 (cid:12)(cid:12)
F
ThisformofregularizationisconsideredforTTbyWahlsetal.[2014];Novikovetal.[2018];Chenetal.[2018]andfor
CPDbyWeselandBatselier[2021,2023]. ItprovidesaFrobeniusnormapproximationoftheregularizationtermwhich
recoverstheoriginalMAPestimateasthehyperparametersofTN(ten(Î›w))arechosensuchthatTN(ten(Î›w))=
ten(Î›w). Ifwenowconsidertheregularizationterminthelog-posterioroftheorem3.1weendupwith
D (cid:12)(cid:12) (cid:12)(cid:12)2
â„¦ CPD:=RD1 (cid:88)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)Î›(d)âˆ’1 2W(d)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)
. (18)
d=1 F
ThisregularizationhasbeenappliedwithoutthescalingfactorRD1 andwithÎ›(d) = I Md,asobservedinthework
ofKargasandSidiropoulos[2021],whomaynothavebeenawareoftheunderlyingconnectionatthattime. Itdoes
notaccountforthemodelinteractionsacrossthedimensionalityduetothei.d.d. assumptionsonthecores. Contrary
totheregularizationâ„¦ inequation(17),itprovidesanapproximationwhichrecoversthelog-priorâ„¦andthusthe
TN
MAP inthelimit. Theseconsiderationspointtothefactthatifthepractitionerisinterestedonlyina MAP estimate
whichgivesweighttothefullpriorw âˆ¼N(0,Î›)aswellaspossible,hemightbemoreinterestedintheregularization
ofequation(17). Furthermore,thepriorsintheorem3.1and3.2provideasensibleinitialguessforgradient-based
optimizationwhichisinvariantw.r.t. thedimensionalityoftheinputsandthechoiceofrankhyperparameters. Wehence
directlyaddresstheinitializationissuesaffectingTN-constrainedkernelmachines[Barrattetal.,2021]byprovidinga
sensibleinitializationstrategyandregularizationwhichdoesnotsufferfromvanishingorexplodinggradients.
4 NumericalExperiments
Wesetuptwonumericalexperimentsinordertorespectivelyempiricallyobservetheclaimsintheorem3.1and3.2by
evaluatingtheconvergencetothepriorGPinequation(3),andtoevaluatetheGPbehaviorofsuchmodelsatpredictionin
thefiniterankcase.InallexperimentswemadeuseoftheHilbert-GPSolinandSaÂ¨rkkaÂ¨[2020]toprovideaBFexpansion
fortheGaussiankernelandoptforM =10basisfunctionsperdimension. Whensamplingthecoresofthemodelsin
theorem3.1and3.2weresortedtonormallydistributedrandomvariables. WeimplementedbothmodelsinPyMC
[Abril-Plaetal.,2023]. ThefullyanonymizedPythonimplementationisavailableatgithub.com/fwesel/tensorGP.
4.1 GPConvergence
In order to empirically verify the convergence to the GP of equation (3) we sample 10000 instances of the CPD
andTTmodelsspecifiedintheorem3.1and3.2forincreasingCPDandTTranksyieldinguptoP = 10000model
parameters. SincethetargetdistributionisGaussianwithknownmoments,werecordtheCrameÂ´râ€“vonMisesstatistic
W2[Dâ€™AgostinoandStephens,1986]whichgivesametricofclosenessbetweenthetargetandoursampledempirical
CDF. We repeat this for N = 10 randomly sampled data points and for D = 2,4,8,16 and report the mean and
standarddeviationoftheresultsinfigure2. Thereinitcanbeobservedthatforthesamenumberofmodelparameters,
7
   :TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
 \ D F K W  H Q H U J \
 1        '     1        '   
      & 3 '
    
 7 7
    
 * 3
    
    
         
    
    
    
                                                                         
 3  3
Figure 3: Mean and standard deviation of the test RMSE evaluated of CPD and TT models as a function of model
parametersP aswellastheirtargetGP. NoticehowintheyachtdatasetTTexhibitsmoreGPbehaviorcomparedtoCPD
asP increases. OntheenergydatasetbothmethodsexhibitGPbehavioralreadyforranksdifferentthanone,which
explainswhyTTappearstobeslower.
TT converges more rapidly than CPD as the dimensionality of the inputs grows. Both approaches however need
exponentiallymoreparameterstoconvergeatthesamerateforincreasingdimensionalityoftheinputs. Notethatfor
D =4CPD,contrarytowhatstatedinsection3.1stillconvergesfasterduetotheapproximationmadewhenconsidering
P =DMR2. HistogramsoftheempiricalCDFforonedatapointareshowninfigure1. Thisbehaviorstemsfrom
TT
thefactthatforafixedcombinationofD,M andP,TTcapturesanexponentialRDâˆ’1rangeofmodelinteractionsin
contrasttotheRlinearinteractionsexhibitedbyCPD. ThisfactrendersthechoiceofTTmoresuitablewithrespect
toCPDifonewishesthemodeltoexhibitmoreGPbehavior,whichischaracterizedbyaweight-independentfeature
representationandislesslikelytooverfit.
4.2 GPBehavioratPrediction
Toinvestigatewhether CPD and TT-constrainedkernel machinesexhibit GP behavior asthenumber ofparameters
increaseswetackletwosmallUCIregressionproblems,yachtandenergy[DuaandGraff,2017]. Weconsider70%
of the data for training an the remaining for test and model our observations as having i.i.d. Gaussian likelihood
(equation(1)).WethenconsidertheGPintheorem3.1and3.2whichwetrainbymaximizingthemarginalizedlikelihood
(10randominitializations). InordertocomparemodelswiththetargetGP,wefixtheobtainedhyperparameters{Ïƒ,Î›}
andsample4chainsof2000instancesfromtheposteriorsp(CPD(ten(w))|y)andp(TT(ten(w))|y)forarange
of model parameters P using the No U-Turn Sampling Hamiltonian Monte Carlo scheme with default parameters.
After discarding the first 1000 samples (burn-in), we obtain posterior predictive distributions p(f (x) | y) and
CPD
p(f (x) | y). WethencomparethetwomodelsintermsoftheRootMeanSquaredError(RMSE)oftheposterior
TT
meanonthetestdata. WeplotthemeanandstandarddeviationoftheRMSEoverthechainsinfigure3.
Infigure3onecanobservethatthepredictionofbothCPDandTTmodelstendstowardstheGPasthenumberofmodel
parametersincreases. Incaseoftheyachtdatasetthishappenswithsmallererrors,i.e. bothmodelsgeneralizebetter.
Asexpected,theTT-constrainedkernelmachineexhibitsmoreGPbehaviorandinthiscaseworsegeneralization. In
caseoftheenergydatasetbothmethodshaveverysimilarperformancecomparedtotheGPandconvergewithlarger
testerrorsassoonastheirranksaredifferentthanone. Whilethebehavioronbothdatasetscanbeexplainedintermof
theGPunderfittingontheyachtdataset,itisworthnotingthatbothTTandCPD-basedmodelsperformatleastaswell
astheGPforrankshigherthanone,renderingthemcomputationallyadvantageousalternativestotheGPinthiscontext.
5 RelatedWork
OurcontributioniscloselytiedtothelinksbetweenBayesianneuralnetworksandGPs,firstestablishedforsingle-layer
single-outputneuralnetworks[Neal,1996a,b]havingsigmoidal[Williams,1996], Gaussian[Williams,1997]and
rectifiedlinearunit[ChoandSaul,2009]asactivationfunction.
ThisideawasextendedtoDNNsbyLeeetal.[2018]andMatthewsetal.[2018]fortheallthemostcommonactivation
functions. ItisimportanttonotethatthederivationofLeeetal.[2018]makesuseofrecursiveapplicationoftheCLT
8
 ( 6 0 5TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
toeachlayerofthenetwork,whileMatthewsetal.[2018]considersthecasewherethewidthofeachlayergrows
simultaneouslytotheothers,whichisarguablymorevaluableinpractice.
FurtherextensionshavebeenproposedtoCNNswherethenumberofchannelstendstoinfinity[Novaketal.,2018;
Garriga-Alonsoetal.,2018],toRNNs[Sunetal.,2022]andtoDNNshavinglow-rankconstraintsontheweightmatrices
[Nait-Saadaetal.,2023].
Inparticulartheorem3.1resemblestheresultsofNeal[1996a,b];Williams[1997]whichrelateinfinite-widthsingle
layer neural networks to GPs. The CPD rank corresponds exactly to the width of the neural network. The crucial
differencelieshoweverintheproductstructure,whichisnotpresentinneuralnetworksandintroducesanonlinearityof
differentkindthantheoneinducedbytheactivationfunction,whichislinearintheCPDandmoreingeneralforany
TN. TTsontheotherhandresembleDNNsastheymaptheoutputofeachcoretothenextone. However,incontrastto
DNNs,theinputsareprocessedoverthedepthofthenetwork. Foramoreindepthdiscussionwereferthereaderto
[Cohenetal.,2016].
Likewisetheorem3.2istheTNcounterparttotheworksofLeeetal.[2018];Matthewsetal.[2018]whichrelatefinite
depthneuralnetworkstoGPs. TheTTranksarethenequivalenttothewidthofeachlayerandtheactivationfunctionis
linear. IncontrasttotheDNNscase,theinducedGPsaredegenerate.
6 Conclusion
In this paper we proved that CPD and TT-constrained kernel machines are GPs in the limit or large TN ranks. We
characterizedthetarget GP, analyzedtheconvergencebehaviorofbothmodelsandshowedthatcomparedto CPD,
TT-basedmodelsconvergefastertotheGPwhendealingwithhigher-dimensionalinputs. Weempiricallydemonstrated
thesepropertiesbymeansofnumericalexperiments.
WhiletheGPconvergenceistooslowtowarrantthesemodelsasGPpriorapproximations,theinsightswederivedare
usefulinpracticeastheyshedlightontheeffectsofthechoiceofTNandregularizationinthesemodels.
References
O.Abril-Pla,V.Andreani,C.Carroll,L.Dong,C.J.Fonnesbeck,M.Kochurov,R.Kumar,J.Lao,C.C.Luhmann,
O.A.Martin,M.Osthege,R.Vieira,T.Wiecki,andR.Zinkov. PyMC:Amodern,andcomprehensiveprobabilistic
programming framework in Python. PeerJ Computer Science, 9:e1516, Sept. 2023. ISSN 2376-5992. doi:
10.7717/peerj-cs.1516.
F.Barratt,J.Dborin,andL.Wright. ImprovementstoGradientDescentMethodsforQuantumTensorNetworkMachine
Learning. InSecondWorkshoponQuantumTensorNetworksinMachineLearning,May2021.
K.Batselier,Z.Chen,andN.Wong. TensorNetworkalternatinglinearschemeforMIMOVolterrasystemidentification.
Automatica,84:26â€“35,Oct.2017. ISSN0005-1098. doi: 10.1016/j.automatica.2017.06.033.
Z.Chen,K.Batselier,J.A.K.Suykens,andN.Wong. ParallelizedTensorTrainLearningofPolynomialClassifiers.
IEEETransactionsonNeuralNetworksandLearningSystems,29(10):4621â€“4632,Oct.2018. ISSN2162-2388. doi:
10.1109/TNNLS.2017.2771264.
Y. Cho and L. Saul. Kernel Methods for Deep Learning. In Advances in Neural Information Processing Systems,
volume22.CurranAssociates,Inc.,2009.
A. Cichocki. Era of Big Data Processing: A New Approach via Tensor Networks and Tensor Decompositions.
arXiv:1403.2048[cs],Aug.2014.
A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, and D. P. Mandic. Tensor Networks for Dimensionality
ReductionandLarge-ScaleOptimization: Part1Low-RankTensorDecompositions. FoundationsandTrendsÂ®in
MachineLearning,9(4-5):249â€“429,2016. ISSN1935-8237,1935-8245. doi: 10.1561/2200000059.
A. Cichocki, A.-H. Phan, Q. Zhao, N. Lee, I. V. Oseledets, M. Sugiyama, and D. Mandic. Tensor Networks for
DimensionalityReductionandLarge-ScaleOptimizations.Part2ApplicationsandFuturePerspectives. Foundations
andTrendsÂ®inMachineLearning,9(6):249â€“429,2017. ISSN1935-8237,1935-8245. doi: 10.1561/2200000067.
N.Cohen,O.Sharir,andA.Shashua. OntheExpressivePowerofDeepLearning: ATensorAnalysis. InConference
onLearningTheory,pages698â€“728.PMLR,June2016.
L.CsatoÂ´ andM.Opper. SparseOn-LineGaussianProcesses. NeuralComputation,14(3):641â€“668,Mar.2002. ISSN
0899-7667. doi: 10.1162/089976602317250933.
R.B.Dâ€™AgostinoandM.A.Stephens. Goodness-of-FitTechniques. CRCPress,Jan.1986.
9TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
D.DuaandC.Graff. UCIMachineLearningRepository,2017.
D.K.Duvenaud, H.Nickisch, andC.Rasmussen. AdditiveGaussianProcesses. AdvancesinNeuralInformation
ProcessingSystems,24:226â€“234,2011.
A.Garriga-Alonso,C.E.Rasmussen,andL.Aitchison. DeepConvolutionalNetworksasshallowGaussianProcesses.
InInternationalConferenceonLearningRepresentations,Sept.2018.
T.Hastie,J.Friedman,andR.Tibshirani. TheElementsofStatisticalLearning. SpringerSeriesinStatistics.Springer,
NewYork,NY,2001. ISBN978-1-4899-0519-2978-0-387-21606-5. doi: 10.1007/978-0-387-21606-5.
J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for Big data. In Proceedings of the Twenty-Ninth
ConferenceonUncertaintyinArtificialIntelligence,UAIâ€™13,pages282â€“290,Arlington,Virginia,USA,Aug.2013.
AUAIPress.
J.Hensman, A.Matthews, andZ.Ghahramani. ScalableVariationalGaussianProcessClassification. InArtificial
IntelligenceandStatistics,pages351â€“360.PMLR,Feb.2015.
J.Hensman,N.Durrande,andA.Solin. VariationalFourierfeaturesforGaussianprocesses. TheJournalofMachine
LearningResearch,18(1):5537â€“5588,Jan.2017. ISSN1532-4435.
F.L.Hitchcock. TheExpressionofaTensororaPolyadicasaSumofProducts. JournalofMathematicsandPhysics,
6(1-4):164â€“189,1927. ISSN1467-9590. doi: 10.1002/sapm192761164.
C. Hua, G. Rabusseau, and J. Tang. High-Order Pooling for Graph Neural Networks with Tensor Decomposition.
AdvancesinNeuralInformationProcessingSystems,35:6021â€“6033,Dec.2022.
P.Izmailov,A.Novikov,andD.Kropotov. ScalableGaussianProcesseswithBillionsofInducingInputsviaTensor
TrainDecomposition. InInternationalConferenceonArtificialIntelligenceandStatistics,pages726â€“735.PMLR,
Mar.2018.
M.Jaderberg,A.Vedaldi,andA.Zisserman. SpeedingupConvolutionalNeuralNetworkswithLowRankExpansions.
ProceedingsoftheBritishMachineVisionConference2014,2014. doi: 10.5244/c.28.88.
N.KargasandN.D.Sidiropoulos. SupervisedLearningandCanonicalDecompositionofMultivariateFunctions. IEEE
TransactionsonSignalProcessing,pages1â€“1,2021. ISSN1941-0476. doi: 10.1109/TSP.2021.3055000.
M.LaÂ´zaro-Gredilla,J.QuinËœnero-Candela,C.E.Rasmussen,andb.R.Figueiras-Vidal. SparseSpectrumGaussian
ProcessRegression. JournalofMachineLearningResearch,11(63):1865â€“1881,2010. ISSN1533-7928.
V.Lebedev,Y.Ganin,M.Rakhuba,I.V.Oseledets,andV.S.Lempitsky. Speeding-upConvolutionalNeuralNetworks
UsingFine-tunedCP-Decomposition. InInternationalConferenceonLearningRepresentations,Jan.2015.
J.Lee,Y.Bahri,R.Novak,S.S.Schoenholz,J.Pennington,andJ.Sohl-Dickstein. DeepNeuralNetworksasGaussian
Processes. InInternationalConferenceonLearningRepresentations,Feb.2018.
X. Lu, A. Boukouvalas, and J. Hensman. Additive Gaussian Processes Revisited. In Proceedings of the 39th
InternationalConferenceonMachineLearning,pages14358â€“14383.PMLR,June2022.
X.Ma,P.Zhang,S.Zhang,N.Duan,Y.Hou,M.Zhou,andD.Song. ATensorizedTransformerforLanguageModeling.
InAdvancesinNeuralInformationProcessingSystems,volume32.CurranAssociates,Inc.,2019.
A.G.d.G.Matthews,M.Rowland,J.Hron,R.E.Turner,andZ.Ghahramani. GaussianProcessBehaviourinWide
DeepNeuralNetworks,Aug.2018.
T. Nait-Saada, A. Naderi, and J. Tanner. Beyond IID weights: Sparse and low-rank deep Neural Networks are
alsoGaussianProcesses. InTheTwelfthInternationalConferenceonLearningRepresentations,Oct.2023. doi:
10.48550/arXiv.2310.16597.
R. M. Neal. Bayesian Learning for Neural Networks. Springer Science & Business Media, Jan. 1996a. ISBN
978-1-4612-0745-0.
R.M.Neal. PriorsforInfiniteNetworks. InR.M.Neal, editor, BayesianLearningforNeuralNetworks, Lecture
Notes in Statistics, pages 29â€“53. Springer, New York, NY, 1996b. ISBN 978-1-4612-0745-0. doi: 10.1007/
978-1-4612-0745-0 2.
R.Novak,L.Xiao,Y.Bahri,J.Lee,G.Yang,J.Hron,D.A.Abolafia,J.Pennington,andJ.Sohl-dickstein. Bayesian
DeepConvolutionalNetworkswithManyChannelsareGaussianProcesses. InInternationalConferenceonLearning
Representations,Sept.2018.
A.Novikov,D.Podoprikhin,A.Osokin,andD.P.Vetrov. Tensorizingneuralnetworks. InC.Cortes,N.Lawrence,
D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems, volume28.
CurranAssociates,Inc.,2015.
10TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
A. Novikov, I. Oseledets, and M. Trofimov. Exponential machines. Bulletin of the Polish Academy of Sciences:
TechnicalSciences;2018;66;No6(SpecialSectiononDeepLearning: TheoryandPractice);789-797,2018. ISSN
2300-1917.
I.V.Oseledets. Tensor-TrainDecomposition. SIAMJournalonScientificComputing,33(5):2295â€“2317,Jan.2011.
ISSN1064-8275,1095-7197. doi: 10.1137/090752286.
J.QuinËœonero-CandelaandC.E.Rasmussen. AUnifyingViewofSparseApproximateGaussianProcessRegression.
JournalofMachineLearningResearch,6(65):1939â€“1959,2005. ISSN1533-7928.
A.RahimiandB.Recht. Randomfeaturesforlarge-scalekernelmachines. InProceedingsofthe20thInternational
ConferenceonNeuralInformationProcessingSystems,NIPSâ€™07,pages1177â€“1184,RedHook,NY,USA,Dec.2007.
CurranAssociatesInc. ISBN978-1-60560-352-0.
C.E.RasmussenandC.K.I.Williams.GaussianProcessesforMachineLearning.AdaptiveComputationandMachine
Learning.MITPress,Cambridge,Mass,2006. ISBN978-0-262-18253-9.
J.A.ReyesandE.M.Stoudenmire. Multi-scaletensornetworkarchitectureformachinelearning. MachineLearning:
ScienceandTechnology,2(3):035036,July2021. ISSN2632-2153. doi: 10.1088/2632-2153/abffe8.
M.W.Seeger,C.K.I.Williams,andN.D.Lawrence. FastForwardSelectiontoSpeedUpSparseGaussianProcess
Regression. InInternationalWorkshoponArtificialIntelligenceandStatistics,pages254â€“261.PMLR,Jan.2003.
E.SnelsonandZ.Ghahramani. SparseGaussianProcessesusingPseudo-inputs. InAdvancesinNeuralInformation
ProcessingSystems,volume18.MITPress,2006.
A.SolinandS.SaÂ¨rkkaÂ¨. Hilbertspacemethodsforreduced-rankGaussianprocessregression. StatisticsandComputing,
30(2):419â€“446,Mar.2020. ISSN1573-1375. doi: 10.1007/s11222-019-09886-w.
E.M.StoudenmireandD.J.Schwab.Supervisedlearningwithtensornetworks.InProceedingsofthe30thInternational
ConferenceonNeuralInformationProcessingSystems,NIPSâ€™16,pages4806â€“4814,RedHook,NY,USA,Dec.2016.
CurranAssociatesInc. ISBN978-1-5108-3881-9.
X.Sun,S.Kim,andJ.-I.Choi. Recurrentneuralnetwork-inducedGaussianprocess. Neurocomputing,509:75â€“84,Oct.
2022. ISSN0925-2312. doi: 10.1016/j.neucom.2022.07.066.
S.Wahls,V.Koivunen,H.V.Poor,andM.Verhaegen. LearningmultidimensionalFourierserieswithtensortrains. In
2014IEEEGlobalConferenceonSignalandInformationProcessing(GlobalSIP),pages394â€“398,Dec.2014. doi:
10.1109/GlobalSIP.2014.7032146.
F.WeselandK.Batselier. Large-ScaleLearningwithFourierFeaturesandTensorDecompositions. InAdvancesin
NeuralInformationProcessingSystems,May2021.
F. Wesel and K. Batselier. Tensor-based Kernel Machines with Structured Inducing Points for Large and High-
DimensionalData. InProceedingsofThe26thInternationalConferenceonArtificialIntelligenceandStatistics,
pages8308â€“8320.PMLR,Apr.2023.
C.Williams. ComputingwithInfiniteNetworks. InAdvancesinNeuralInformationProcessingSystems,volume9.
MITPress,1996.
C.Williams. ComputingwithInfiniteNetworks. AdvancesinNeuralInformationProcessingSystems9,pages295â€“301,
1997.
A.WilsonandH.Nickisch.KernelInterpolationforScalableStructuredGaussianProcesses(KISS-GP).InProceedings
ofthe32ndInternationalConferenceonMachineLearning,pages1775â€“1784.PMLR,June2015.
M.Yadav,D.Sheldon,andC.Musco. FasterKernelInterpolationforGaussianProcesses. InProceedingsofThe24th
InternationalConferenceonArtificialIntelligenceandStatistics,pages2971â€“2979.PMLR,Mar.2021.
J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu. Learning Compact Recurrent Neural Networks With
Block-Term Tensor Decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition,pages9378â€“9387,2018.
11TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
A Notation
Throughout this paper we denote scalars in both capital and non-capital italics w,W, vectors in non-capital bold
w, matrices in capital bold W and tensors in capital italic bold font W. The m-th entry of a vector w âˆˆ RM is
indicatedasw
m
andthem 1m 2...m D-thentryofaD-dimensionaltensorW âˆˆRM1Ã—M2Ã—Â·Â·Â·Ã—MD asw m1,m2,...mD.
We employ the column notation to indicate a set of elements of tensor given a set of indices, e.g. W and
m1,:,m2
W represent respectively all elements and the first three elements along the second dimension of tensor
m1,1:3,m2
W withfixedindicesm andm . TheKroneckerproductisdenotedbyâŠ—andtheHadamard(elementwise)byâŠ™.
1 2
We employ one-based indexing for all tensors. The Frobenius inner product between two D-dimensional tensors
V,W âˆˆRM1Ã—M2Ã—Â·Â·Â·Ã—MD is
(cid:88)M1 (cid:88)M2 (cid:88)MD
âŸ¨V,WâŸ© := Â·Â·Â· v w ,
F m1,m2,...,mD m1,m2,...,mD
m1=1m2=1 mD=1
andtheFrobeniusnormofW âˆˆRM1Ã—M2Ã—Â·Â·Â·Ã—MD isdenotedanddefinedas
||W||2 :=âŸ¨W,WâŸ© .
F F
Wedefinethevectorizationoperatorasvec(Â·):RM1Ã—M2Ã—Â·Â·Â·Ã—MD â†’RM1M2Â·Â·Â·MD suchthat
vec(W) =w ,
m m1,m2,...,mD
withm=m 1+(cid:80)D d=2(m dâˆ’1)(cid:81)d kâˆ’ =1 1M k. Likewise,itsinverse,thetensorizationoperatorten(Â·):RM1M2Â·Â·Â·MD â†’
CM1Ã—M2Ã—...MD isdefinedsuchthat
ten(w) =w .
m1,m2,Â·Â·Â·,mD m
B Proofs
B.1 GPofCPD-ConstrainedKernelMachine
TheoremB.1(GPlimitofCPD-constrainedkernelmachine). ConsidertheCPD-constrainedkernelmachine
f (x):=âŸ¨R (ten(Ï†(x))),CPD(ten(w))âŸ© .
CPD 1 F
IfeachoftheRcolumnsofeachCPDcorew(d)
:,r
âˆˆRMd isani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E w(d) =0,
:,r
E(cid:104) w(d)w(d)T(cid:105)
=
1
Î›(d),
:,r :,r RD1
thenf (x)convergesindistributionasRâ†’âˆžtotheGaussianprocess
CPD
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
CPD d
d=1
Proof. Consider the R intermediate functions h
r
of equation (13) which constitute the CPD-constrained model of
equation(11). Duetothei.i.d. assumptiononw(d) eachaddendisthesamefunctionofi.i.d. randomvariablesand
:,r
thusisitselfi.i.d.. Themeanofeachaddendis
(cid:34) D (cid:35)
E[h (x)]=E (cid:89) Ï†(d)(x )T w(d) =0, (19)
r d :,r
d=1
12TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
duetothei.i.dassumptionandthelinearityofexpectation. Itscovarianceis
E[h (x)h (xâ€²)] (20a)
r r
(cid:34) D D (cid:35)
=E (cid:89) Ï†(d)(x )T w(d) (cid:89) Ï†(d)(xâ€²)T w(d) (20b)
d :,r d :,r
d=1 d=1
(cid:34) D (cid:35)
=E (cid:89) Ï†(d)(x )T w(d) w(d) T Ï†(d)(xâ€²) (20c)
d :,r :,r d
d=1
D
=(cid:89)
Ï†(d)(x
)TE(cid:104)
w(d) w(d)
T(cid:105)
Ï†(d)(xâ€²) (20d)
d :,r :,r d
d=1
D
=1 (cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(xâ€²).
R d d
d=1
Herethestepfromequation(20b)toequation(20c)exploitsthefactthatthetransposeofascalarisequaltoitself,the
stepfromequation(20c)toequation(20d)isduetothelinearityofexpectation. Asthevariancesofeachintermediate
functionh areappropriatelyscaled,bythemultivariatecentrallimittheoremf (x)convergesindistributiontoa
r CPD
multivariatenormaldistribution,whichisfullyspecifiedbyitsfirsttwomoments
E(cid:2)
f
(x)(cid:3)
=0,
CPD
D
E(cid:2) f (x)f (xâ€²)(cid:3) = (cid:89) Ï†(x )TÎ›(d)Ï†(x â€²).
CPD CPD d d
d=1
Since any finite collection of {f (x),...,f (xâ€²)} will have a joint multivariate normal distribution with the
CPD CPD
aforementionedfirsttwomoments,weconcludethatf (x)istheGaussianprocess
CPD
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
CPD d
d=1
B.2 GPofTT-ConstrainedKernelMachineintheSequentialLimitoftheTTRanks
TheoremB.2(GPinthelimitofTT-constrainedkernelmachine). ConsidertheTT-constrainedkernelmachine
f (x):=âŸ¨R (ten(Ï†(x))),TT(ten(w))âŸ©
TT 1 F
IfeachoftheR dâˆ’1R dfibersofeachTTcoreW(d)
rdâˆ’1,:,rd
âˆˆRMd isani.i.d. randomvariablesuchthat
(cid:104) (cid:105)
E W(d) =0,
rdâˆ’1,:,rd
E(cid:104)
W(d) W(d)
T(cid:105)
=
1
Î›(d),
rdâˆ’1,:,rd rdâˆ’1,:,rd (cid:112)
R R
dâˆ’1 d
thenf (x)convergesindistributionasR â†’âˆž,R â†’âˆž,...,R â†’âˆžtotheGaussianprocess
TT 1 2 Dâˆ’1
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
TT d
d=1
Proof. Definethevectorofintermediatefunctionh(d+1) âˆˆRRd+1 recursivelyas
(cid:88)Rd
h(d+1):= z(d+1) (x )h(d),
rd+1 rd,rd+1 d+1 rd
rd=1
13TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
withh(0):=1. Notethatthefirsttwomomentsofintermediatevariablez(d+1) (x )are
rd,rd+1 d+1
(cid:104) (cid:105)
E z(d+1) (x ) =0,
rd,rd+1 d+1
(cid:104) (cid:105)
E z(d+1) (x )z(d+1) (xâ€² )
rd,rd+1 d+1 rd,rd+1 d+1
= 1 Ï†(d)(x )T Î›(d)Ï†(d)(xâ€²)
(cid:112) d d
R R
d d+1
Weproceedbyinduction. Fortheinductionstepsupposethath r(d d)isaGP,identicalandindependentforeveryr dsuch
that
(cid:32) d (cid:33)
h(d) âˆ¼GP 0,âˆš1 (cid:89) Ï†(p)(x )T Î›(p)Ï†(p)(Â·) .
rd R p
d p=1
Thescalarh(d+1)isthesumofR i.i.d. termshavingmean
rd+1 d
(cid:104) (cid:105) (cid:104) (cid:105)
E h(d+1) =E z(d+1) (x )h(d) =0,
rd+1 rd,rd+1 d+1 rd
andcovariance
(cid:104) (cid:105)
E h(d+1)h(d+1)
rd+1 rd+1
(cid:104) (cid:105)
=E z(d+1) (x )h(d)z(d+1) (xâ€² )h(d)
rd,rd+1 d+1 rd rd,rd+1 d+1 rd
(cid:104) (cid:105) (cid:104) (cid:105)
=E z(d+1) (x )z(d+1) (xâ€² ) E h(d)h(d)
rd,rd+1 d+1 rd,rd+1 d+1 rd rd
d+1
= 1 (cid:89) Ï†(p)(x )T Î›(p)Ï†(p)(xâ€²).
(cid:112) p p
R
d+1 p=1
SincetheassumptionsoftheCLTaresatisfiedh r(d d+ +11)convergesindistributiontothenormaldistribution,fullyspecified
by the above mentioned first two moments. Since any finite collection of {h(d+1)(x ),...,h(d+1)(xâ€² )}
rd+1 1:d+1 rd+1 1:d+1
will have a joint multivariate normal distribution with the aforementioned first two moments, we conclude that
h( rd d+ +11)(x 1:d+1)istheGP
(cid:32) d+1 (cid:33)
h(d+1) âˆ¼GP 0, 1 (cid:89) Ï†(p)(x )T Î›(p)Ï†(p)(Â·) .
rd+1 (cid:112)
R
p
d+1 p=1
Forthebasecase,considertheR outputsofthefirsthiddenfunctionh(1). Theyarei.i.d. withmean
1 r1
(cid:104) (cid:105)
E h(1)(x ) =0.
r1 1
andcovariance
E(cid:104) h(1)(x )h(1)(xâ€²)(cid:105) = âˆš1 Ï†(1)(x )T Î›(1)Ï†(1)(x ).
r1 1 r1 1 R 1 1
1
WenowconsidertheR outputsofthesecondhiddenfunctionh(2)
2 r2
(cid:88)R1
h(2) = z(2) (x )h(1),
r2 r1,r2 2 r1
r1=1
whicharei.i.d. astheyarethesamefunctionoftheR i.i.d. outputsofh(1)(x ). Morespecifically,theirmeanand
1 r1 1
covarianceare
(cid:104) (cid:105)
E h(2) =0,
r2
(cid:104) (cid:105)
E h(2)(x )h(2)(xâ€²)
r2 2 r2 2
2
=âˆš1 (cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(x ).
d d
R
2
d=1
14TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
Once more by the CLT h( r2 2) converges in distribution to the normal distribution with the above first two moments.
Sinceanyfinitecollectionof{h(2)(x ),...,h(2)(xâ€² }willhaveajointmultivariatenormaldistributionwiththe
r2 1:2 r2 1:2
aforementionedfirsttwomoments,weconcludethath( 22)(x 1:2)istheGP
(cid:32) 2 (cid:33)
1 (cid:89)
h(2) âˆ¼GP 0,âˆš Ï†(d)(x )TÎ›(d)Ï†(d)(Â·) ,
r2 R d
2
d=1
which is our base case. Hence by induction f (x) = h(D) converges in distribution as R â†’ âˆž, R â†’ âˆž, ...,
TT 1 2
R
Dâˆ’1
â†’âˆžtotheGP
(cid:32) D (cid:33)
f (x)âˆ¼GP 0,(cid:89) Ï†(d)(x )T Î›(d)Ï†(d)(Â·) .
TT d
d=1
B.3 GPofTT-ConstrainedKernelMachineintheSimultaneousLimitoftheTTRanks
Intheorem3.2weprovebyinductionthattheTT-constrainedkernelmachineconvergestoaGPbytakingsuccessive
limitsoftheTTranks. ThisresultisanalogoustotheworkofLeeetal.[2018],whoprovethatfortheDNNs,taking
sequentiallythelimitofeachlayer. Amorepracticallyusefulresultconsistsintheconvergenceinthesimultaneous
limitofTTranks.
IndeeplearningMatthewsetal.[2018,theorem4]proveconvergenceinthecontextofDNNsoverthewidthsofall
layerssimultaneously. SaidtheoremhasbeenemployedtoproveGPconvergenceinthecontextofconvolutionalneural
networks[Garriga-Alonsoetal.,2018]andinthecontextofDNNswhereeachweightmatrixisoflowrank[Nait-Saada
etal.,2023].
SeeingthesimilaritybetweenTT-constrainedkernelmachines(equation(14))andDNNsandthetechnicalityofthe
proof, similarly to [Garriga-Alonso et al., 2018; Nait-Saada et al., 2023] we draw a one-to-one map between the
TT-constrainedkernelmachinesandtheDNNsconsideredinMatthewsetal.[2018,theorem4]. Convergenceinthe
simultaneouslimitisthenguaranteedbyMatthewsetal.[2018,theorem4].
Webeginbyrestatingthedefinitionsoflinearenvelopeproperty,DNNs,linearenvelopepropertyandnormalrecursion
asfoundinMatthewsetal.[2018]. Tomakethecomparisoneasierforthereader,wechangetheindexingnotationto
matchtheoneinthispaper.
DefinitionB.3(Linearenvelopepropertyfornonlinearities[Matthewsetal.,2018]). Anonlinearityt:Râ†’Rissaid
toobeythelinearenvelopepropertyifthereexistc,lâ‰¥0suchthatthefollowinginequalityholds
|t(u)|<c+l|u|âˆ€uâˆˆR. (25)
DefinitionB.4(FullyconnectedDNN[Matthewsetal.,2018]). Afullyconnecteddeepneuralwithone-dimensional
outputandinputsxâˆˆRR0 isdefinedrecursivelysuchthattheinitialstepis
(cid:88)R0
h(1)(x)= z(1) x +b(1), (26)
r1 r1,r0 r0 r1
r0=1
theactivationstepbynonlinearactivationfunctiontisgivenby
g(d) =t(f(d)), (27)
rd rd
andthesubsequentlayersaredefinedbytherecursion
(cid:88)Rd
h(d+1) = z(d+1) g(d)+bd+1, (28)
rd+1 rd+1,rd rd rd+1
rd=1
sothath(D)istheoutputofthenetwork. Intheabove,Z(d) âˆˆRRdâˆ’1Ã—Rd andb(d) âˆˆRRd arerespectivelytheweights
andbiasesofthed-thlayer.
DefinitionB.5(WidthfunctionMatthewsetal.[2018]). ForagivenfixedinputnâˆˆN,awidthfunctionv(d) :Nâ†’N
atdepthdspecifiesthenumberofhiddenunitsR atdepthd.
d
15TensorNetwork-ConstrainedKernelMachinesasGaussianProcesses APREPRINT
LemmaB.6(Normalrecursion[Matthewsetal.,2018]). Considerz(d) âˆ¼N(0,C(d))andb(d) âˆ¼N(0,C(d)). If
rdâˆ’1,rd w rd b
theactivationsofthed-thlayerarenormallydistributedwithmoments
(cid:104) (cid:105)
E h(d) =0 (29)
rd
(cid:104) (cid:105)
E h(d)h(d) =K(x,xâ€²), (30)
rd rd
thenunderrecursionequations(27)and(28),asR â†’âˆž,theactivationsofthenextlayerconvergeindistribution
dâˆ’1
toanormaldistributionwithmoments
(cid:104) (cid:105)
E h(d+1) =0 (31)
rd+1
(cid:104) (cid:105)
E h(d+1)h(d+1) =C(d+1)E [t(Ïµ )t(Ïµ )]+C(d+1). (32)
rd+1 rd+1 w (Ïµ1,Ïµ2)âˆ¼N(0,K) 1 2 b
WecannowstatethemajorresultinMatthewsetal.[2018].
TheoremB.7(GPinthesimultaneouslimitoffullyconnectedDNNs[Matthewsetal.,2018]). Considerarandom
DNNoftheformofdefinitionB.4obeyingthelinearenvelopeconditionofdefinitionB.3. Thenforallsetsofstrictly
increasingwidthfunctionsv(d)andforanycountableinputset{x,...,xâ€²},thedistributionoftheoutputofthenetwork
convergesindistributiontoa GP asn â†’ âˆž. The GP hasmeanandcovariancefunctionsgivenbytherecursionin
lemmaB.6.
CorollaryB.8(GPinthesimultaneouslimitofTT-constrainedkernelmachines). ConsiderarandomTT-constrained
kernelmachineoftheformofdefinition2.6obeyingthelinearenvelopeconditionofdefinitionB.3. Thenforallsetsof
strictlyincreasingwidthfunctionsv(d)andforanycountableinputset{x,...,xâ€²},thedistributionoftheoutputof
thenetworkconvergesindistributiontoaGPasP â†’ âˆž. TheGPhasmeanandcovariancefunctionsgivenbythe
recursioninlemmaB.6andstatedintheorem3.2.
Proof. When examining definition B.4 and comparing it with definition 2.6 it becomes clear that both models are
similar. Inthespecialcaseofinvolvinglinearactivationfunctionandzerobiases,themodelsarestructurallyidenticalif
oneconsidersunitinputsx=1inequation(26). ThenormalrecursioninlemmaB.6issatisfiedbyTT-constrained
kernelmachines,aswehavethat
t(u):=uâˆ€uâˆˆR,
C(d+1):=0,
b
C(d+1):= 1 Ï†(d)(x )T Î›(d)Ï†(d)(xâ€²),
(cid:112) d d
R R
d d+1
d
K:=âˆš1 (cid:89) Ï†(p)(x )T Î›(p)Ï†(p)(xâ€²)
R p p
d p=1
E [t(Ïµ )t(Ïµ )]:=K.
(Ïµ1,Ïµ2)âˆ¼N(0,K) 1 2
HencebytheoremB.7,forallsetsofstrictlyincreasingwidthfunctionsv(d)andforanycountableinputset{x,...,xâ€²},
thedistributionoftheoutputofthenetworkconvergesindistributiontoaGP,fullyspecifiedbytheoutputofthenormal
recurisioninlemmaB.6,whichequalstheGPintheorem3.2.
16