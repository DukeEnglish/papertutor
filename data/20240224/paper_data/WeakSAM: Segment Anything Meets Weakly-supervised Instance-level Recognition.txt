WeakSAM: Segment Anything Meets Weakly-supervised Instance-level
Recognition
LianghuiZhu*1 JunweiZhou*1 YanLiu2 XinHao2 WenyuLiu1 XinggangWang1
Abstract WSOD VOC 2007
test (W2N)
Weakly supervised visual recognition using in-
exact supervision is a critical yet challenging 73.4
learningproblem. Itsignificantlyreduceshuman WSIS COCO 2017 WSOD VOC 2012
test (CIM) test (SoS-WSOD)
labeling costs and traditionally relies on multi-
25.2 69.9
instancelearningandpseudo-labeling. Thispa- 65.4
perintroducesWeakSAMandsolvestheweakly- 17.2 61.9
supervisedobjectdetection(WSOD)andsegmen-
tationbyutilizingthepre-learnedworldknowl- 17.0 16.6
edge contained in a vision foundation model,
55.9
25.0 24.6
i.e.,theSegmentAnythingModel(SAM).Weak-
WSIS COCO 2017
SAM addresses two critical limitations in tra-
val (CIM) WSOD COCO 2014
ditional WSOD retraining, i.e., pseudo ground 63.9 val (SoS-WSOD)
truth (PGT) incompleteness and noisy PGT in-
Previous SOTA
WSIS VOC 2012
stances, through adaptive PGT generation and
WeakSAM
test (CIM)
Region of Interest (RoI) drop regularization. It
alsoaddressestheSAMâ€™sproblemsofrequiring
Figure1.QuantitativecomparisonsbetweenWeakSAMandpre-
promptsandcategoryunawarenessforautomatic
viousSOTAmethodsunderdifferenttasksandbenchmarks.The
object detection and segmentation. Our results
scaleofeachaxisintheradarchartisnormalizedbytheperfor-
indicate that WeakSAM significantly surpasses
manceofthepreviousSOTAmethods(markedinparentheses),
previousstate-of-the-artmethodsinWSODand
andthestrideofeachaxisisthesame.
WSISbenchmarkswithlargemargins,i.e. aver-
ageimprovementsof7.4%and8.5%,respectively. exactsupervision,suchasimage-levellabels. Subsequently,
Codeisavailableathttps://github.com/ thetrainedWSLnetworkisemployedtogeneratepseudo
hustvl/WeakSAM. groundtruth(PGT),whichservesasaformofrefined,al-
beitstillinaccuratesupervision. Finally,thePGTisusedas
inaccuratesupervisiontolaunchWSLretraining. Although
1.Introduction theiterativeWSLprocessachievessignificantprogress,it
is still limited by the lack of external knowledge, which
Weakly-supervised learning (WSL) (Zhou, 2018; Wang
restrictstheperformanceofWSLandhindersitfrommatch-
et al., 2013; Xu et al., 2014) is a crucial component of
ingfully-supervisedlearning(FSL).
machinelearning. Itisparticularlyvaluableintaskswhere
strong supervision is difficult to annotate due to the high Nowadays,foundationmodelsaregainingincreasingatten-
costofdatalabeling(Locatelloetal.,2020;Schroeteretal., tionbecauseoftheirtransferablepre-learnedworldknowl-
2019; Fu et al., 2020). Due to the massive demand for edge, whichcanberegardedaspowerfulexternalknowl-
annotated data in visual perception, WSL is essential in edgeforWSL.Asavisionfoundationmodel,SAM(Kirillov
developingalabel-efficientrecognitionsystem. Inthestan- etal.,2023)achievesoutstandingperformanceininterac-
dardweakly-supervisedvisualperceptionparadigm(Tang tive, class-agnostic segmentation. SAM owes its success
etal.,2018a;Suietal.,2022),trainingcommenceswithin- topromptabletrainingonalarge-scaledataset. However,
therearetwomaindrawbackstoSAM:First,SAMrequires
*Equalcontribution 1SchoolofEIC,HuazhongUniversityof
interactiveoperationsasinput,whichmeansitcannotwork
Science&Technology2AlipayTianQianSecurityLab. Corre-
automaticallywithouthumanprompts. Second,SAMpro-
spondenceto:XinggangWang<xgwang@hust.edu.cn>.
ducesclass-agnosticsegmentsandcannotassignclasslabels.
ThesedrawbacksseverelyrestricttheapplicationofSAM
1
4202
beF
22
]VC.sc[
1v21841.2042:viXraWeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
as a generic visual framework. As a strong complement, significantly surpassing previous SOTA methods as
WSLisgoodatminingclassificationcluesthroughinexact showninFig.1.
supervision,whichcanprovideautomaticpromptsforSAM.
Subsequently, WSL with SAMâ€™s knowledge can further
2.RelatedWork
bringclass-awareperception.
2.1.SegmentAnythingModel
This motivates us to assimilate SAM within the WSL
paradigm.TheWeakSAMframeworkisdesignedtoharness TherecentSegmentAnythingModel(SAM)(Kirillovetal.,
transferableknowledgefromSAM,therebyenrichingthe 2023)drawsgreatattentionfromresearchers. TheSAMis
WSL process. Simultaneously, it offers the capability to trainedonSA-1Bwithover1billionmasks,followingthe
deliver automatic classification clues to SAM. This bidi- model-in-the-loopmanner. Besides,SAMperformssupe-
rectionalenhancementconstructsapromisingfoundation- riorzero-shottransfercapabilitiesandisappliedinmany
model-based weakly-supervised visual perception frame- visual tasks, e.g., FGVP (Yang et al., 2023) incorporates
work. Specifically,inaweakly-supervisedobjectdetection SAM to achieve zero-shot fine-grained visual prompting,
(WSOD) setting, WeakSAM uses classification clues as MedSAM (Ma & Wang, 2023) adapts SAM into a large
SAMpromptstoproduceproposalsautomatically. These scalemedicaldatasettobuildamedicalfoundationmodel,
proposalsarethenusedinWSODtrainingforclass-aware andsomemethods(Sunetal.,2023;Jiang&Yang,2023;
perception. Chen et al., 2023) utilize SAM to deal with the weakly-
supervisedsemanticsegmentationproblem. However,SAM
WithinthescopeoftheWeakSAMframework,ouranalysis
isaninteractivesegmentationmethod,whichheavilyrelies
identifiestwoprevailinglimitationsintheiterativeWSOD
onhumanprompts.
retrainingapproach: theissueofpseudogroundtruth(PGT)
incompletenessandthepresenceofnoisyPGTinstances. Inourapproach,weinnovativelyproposetoautomatically
Theformer,PGTincompleteness,referstothetendencyof promptSAMusingclassificationcluesforextractingregion
WSOD-generatedPGTtoomitsomeobjectsorcategories, proposals. Thismethodresultsinhigh-recallproposalsthat
leading to insufficient training for these categories. The surpasstraditionalmethodslikeSelectiveSearchinterms
latter, noisy PGT instances, pertain to the prevalent pres- ofbothefficiencyandeffectiveness. Thisadvancementrep-
enceofnoisewithinthePGT,whichadverselyimpactsthe resentsasignificantimprovementinthedomainofproposal
retrainingprocess. Toeffectivelymitigatethesechallenges, generationwithintheWSODframework.
weintroducetwokeystrategies: adaptivePGTgeneration
2.2.Weakly-supervisedObjectDetection
toaddressthePGTincompletenessproblem,andRegionof
Interest(RoI)dropregularizationtocounteractthenoisein Weakly-supervisedobjectdetection(WSOD)withimage-
PGTinstances. Moreover,WeakSAMâ€™scapabilityenables level labels (Laptev et al.; Diba et al., 2017; Tang et al.,
the extension in the realm of weakly-supervised instance 2018b; Gao et al., 2018; Wan et al., 2018; Zhang et al.,
segmentation(WSIS).Inthiscontext,SAMisemployedto 2018a;Liuetal.,2019;Lietal.,2019;Arunetal.,2019;
furtherrefineWeakSAM-PGT,enablingthegenerationof Sunetal.,2020;Arunetal.,2020;Jiaetal.,2021;Wanetal.,
pseudoinstancesegmentationlabels. Thisapproachexem- 2019)isimportantforreducingthehumanannotationbur-
plifiesWeakSAMispromisingtobuildaunifiedweakly- den. Thepreviousworks,i.e.,WSDDN(Bilen&Vedaldi,
supervisedinstance-levelrecognitionframework. 2016b)andOICR(Tangetal.,2017),proposedtheMultiple
InstanceLearningandonlinerefinementparadigms. The
Themaincontributionsofthispapercanbesummarizedas
laterworksaimedtoimprovetheWSODperformancefrom
follows: differentperspectives. SuchasWSOD2(Zengetal.,2019)
â€¢ Weproposeaweakly-supervisedinstance-levelrecog- introduced bottom-up object evidence, PCL (Tang et al.,
nition framework (WeakSAM), which automatically 2018a) proposed to cluster proposals, MIST (Ren et al.,
prompts SAM by classification clues for proposals. 2020)utilizedaself-trainingalgorithm,etc. Besides,some
TheWeakSAM-proposalsimproveboththeeffective- methods(Tangetal.,2018a;Jieetal.,2017;Lietal.,2016;
nessandefficiencyofWSOD. Suietal.,2022;Zhangetal.,2018b;Huangetal.,2022)also
retrainedafully-supervisedobjectdetectionnetworkwith
â€¢ We analyze the weaknesses in traditional WSOD re- generated pseudo ground truth (PGT). However, most of
training,andproposeadaptivePGTgenerationandRoI themusedtheproposalsgeneratedfromlow-levelmethods,
dropregularizationtoaddressthem,respectively. Af- i.e.,SelectiveSearch(Uijlingsetal.,2013),EdgeBox(Zit-
tertheWeakSAM-WSODiscomplete,theproposed nick&DollaÂ´r,2014),andMCG(Pont-Tusetetal.,2016),
WeakSAMcanbeeasilyappliedtoWSISfurther. whichcontainagreatnumberofredundantproposalsand
bringanoptimizationchallenge.
â€¢ The proposed WeakSAM achieves state-of-the-art
(SOTA)resultsontheWSODandWSISbenchmarks, Differentfrompreviousmethods,ourWeakSAM-proposals
2WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
ClassificationCluesGeneration WeakSAMProposalsGen. WSODPipeline WSIS
FineCAM
PlainTransformer Weakly-supervised
Generation SAM
Detector
Adaptive PGT
CrossAttention CoarseCAM FineCAM
KÃ—HÃ—NÃ—NÃ—C NÃ—NÃ—C NÃ—NÃ—C SAM Generation
Dense Spatial PseudoInstanceLabels
Sampling
PeakPoints Extraction
RoIDrop Instance
Retraining Segmentation
PromptPointsGeneration Proposals & Masks
Figure2.AnoverviewoftheproposedWeakSAMframework.WefirstintroduceclassificationcluesandspatialpointsasautomaticSAM
prompts,whichaddresstheproblemofSAMrequiringinteractiveprompts.Next,weusetheWeakSAM-proposalsintheWSODpipeline,
inwhichtheweakly-superviseddetectorperformsclass-awareperceptiontoannotatepseudogroundtruth(PGT).Then,weanalyze
theincompletenessandnoiseproblemexistinginPGTandproposeadaptivePGTgeneration,RoIdropregularizationtoaddressthem,
respectively.Finally,weuseWeakSAM-PGTtopromptSAMforWSISextension.Thesnowflakemarkmeansthemodelisfrozen.
havefewernumbersandhigherrecall,whichreducesthe 3.Method
difficultyoffindingthecorrectproposalsforWSODmeth-
WepresenttheWeakSAMframeworkasshowninFig.2.
ods. ForthekeyproblemofPGTincompletenessandnoisy
Atfirst,WeakSAMautomaticallygeneratespromptsfrom
PGTinstances,weproposeadaptivePGTgenerationand
classificationcluesandspatialsamples. Next,WeakSAM
RegionofInterest(RoI)dropregularizationtoaddressthem,
sendthepromptstoSAMforWeakSAM-proposals. Then,
respectively.
welaunchtheweakly-supervisedobjectdetection(WSOD)
2.3.Weakly-supervisedInstanceSegmentation pipeline,whichisenhancedbyWeakSAM-proposals,adap-
tivepseudogroundtruth(PGT)generation,andRoIdrop
Weakly-supervisedinstancesegmentation(WSIS)aimsto
regularization. Last,weusethePGTboxesgeneratedbythe
achieveinstancesegmentationthroughweaksupervision,
WSODpipelinetolaunchtheweakly-supervisedinstance
suchasbox-levelsupervision(Tianetal.,2021;Wangetal.,
segmentationextension.
2021;Chengetal.,2023;Hsuetal.,2019;Liaoetal.,2019;
Leeetal.,2021;Khorevaetal.,2017;Zhangetal.,2023;
3.1.ClassificationCluesasAutomaticPrompt
Zhuetal.,2023b;Lietal.,2022),andimage-levelsuper-
vision (Ge et al., 2019; Ou et al., 2021; Zhu et al., 2019; Previous WSOD methods face an optimization prob-
Liu et al., 2020; Hwang et al., 2021; Zhang et al., 2021; lem caused by the redundant proposals, e.g., Selective
Huetal.,2020;Hsiehetal.,2023;Laradji12etal.). The Search (Uijlings et al., 2013) and EdgeBox (Zitnick &
WSISwithimage-levelsupervisionischallengingbecauseit DollaÂ´r,2014), becausetheseproposalsareonlybasedon
lacksaccurateinstancelocations. Someimage-levelWSIS low-levelfeatures. Toaddressthisproblem,weproposeto
methodsuseCAMtoextractcoarseobjectlocations,such transferknowledgeinthefoundationmodel,i.e.,SAM,for
as PRM (Zhou et al., 2018), IAM (Zhu et al., 2019), IR- proposalgeneration. Weuseclassificationcluestoprompt
Net (Ahn et al., 2019), BESTIE (Kim et al., 2022), etc. SAMautomatically,whichalsosolvestheshortcomingof
Someotherimage-levelWSISmethodstrytoincorporate SAMrequiringinteractiveprompts
instancecluesfromextrapriors,suchasFanetal.(Fanetal.,
2018b),LIID(Liuetal.,2020),CIM(Lietal.,2023),etc.
ClassificationCluesGeneration AsshowninFig.2,we
However,theyalwaysneedcomplicatednetworksandlack
extractclassificationcluesfromaclassificationViT.Specif-
high-qualityinstancesegments.
ically,wechoosethepre-trainedweakly-supervisedseman-
DifferentfrompreviousWSISmethods,theproposedWSIS ticsegmentationnetwork, WeakTr(Zhuetal.,2023a), to
extensionusingWeakSAM-PGTandSAMâ€™spredictionis provide classification clues because of its superior local-
concise and effective. The generated pseudo instance la- ization ability. At first, we extract cross-attention maps
belscanfurtherbeappliedtoanyfully-supervisedinstance CA âˆˆ RKÃ—HÃ—NÃ—NÃ—C from the self-attention maps,
segmentationmethod. whereK isthenumberoftransformerencodinglayers,H
isthenumberofattentionheadsineachlayer,NÃ—N isthe
spatialsizeofthevisualtokens,andC representsthetotal
3WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
numberofclassificationcategories. Then,weobtaincoarse V ,andmaxpoolingoperation. Next,wereshapethe
delete
CAM, CAM âˆˆ RNÃ—NÃ—C, from the convolutional inputmapsandensurethelasttwodimensionscorrespondto
coarse
CAM head, which takes visual tokens at the final trans- theoriginalimagesizeandtheothersasthefirstdimension.
formerlayerasinputandproducescoarseCAM.Last,we Then,weapplymaxpoolingontheinputmapsM,andsort
useWeakTrtoproducefineCAM,CAM âˆˆRNÃ—NÃ—C. V andP indescendingorderbasedonV. Last,weremove
fine
points with low activation values or close to high-score
PromptPointsGeneration AsshowninFig.2,weex- points.
tractpromptsfromdensesamplingpoints,cross-attention
maps, and CAMs. At first, the dense sampling requires WeakSAMProposalsGeneration AttheWeakSAMpro-
splittingtheimageintoSÃ—S patchesandtakingthecen- posalgenerationstage,weusethethreekindsofprompts
terpointsasprompts. Notably,thedensesamplingpoints topromptSAMautomatically. Wedirectlyaddsemantic-
provide spatial-aware prompts but lackexplicitreference awarepromptsandspatial-warepromptstothepromptlist,
to objects and semantics, which means increasing the S becausetheyusuallyhaveclearlocalizationtoforeground
usuallyleadstoagreatnumberofinvalidsamplingpoints. objectsandspatialpositions,respectively. Fortheinstance-
Then,wegetpeakpointsfromthecross-attentionmapsas awarepromptsthathavesomeredundancy,weclusterthem
prompts. We observe that these maps do not solely con- tofiltertheduplicatedonesandthenaddthemtotheprompt
centrateonobjectsfromtheircorrespondingcategoriesbut list.Finally,allpromptsinthepromptlistareusedtoprompt
alsogiveattentiontoobjectsfromdifferentcategories. So, SAMforproposals.
wemarkthesepromptsasinstance-awareones. Last, we
extract peak points from coarse CAM and fine CAM as
3.2.WeakSAMWSODPipeline
semantic-awareprompts,whicharemorepreciseandfocus
onareasofforegroundobjects. Tobetterdescribetheproposedweakly-supervisedobject
detection (WSOD) pipeline, we first present the weakly-
Algorithm1PeakPointsExtraction supervised detector training with WeakSAM-proposals.
Require: mapsM(eitherCAorCAM),kernelsizek,activa- Then, we identify the PGT incompleteness problem and
tionthresholdÏ„ introducetheproposedadaptivePGTgenerationtoaddress
Ensure: peak points coordinates list P = [p 0,p 1,...,p nâˆ’1], it. Last, we analyze the noise problem existing in the re-
correspondingpeakvalueslistV =[v ,v ,...,v ]
0 1 nâˆ’1 trainingphase,andproposeRegionofInterest(RoI)drop
1: M=M.view(-1,N,N)//reshape
regularizationtoalleviatetheeffectofnoise.
2: InitializeP,V asemptylist
3: InitializeMaxpool()operationwithkernelsizek
4: P,V =Maxpool(M)//getcoordinatesandvalues Weakly-supervisedDetectorTraining Aprimarychal-
5: SortV indescendingorderofnumericalvalue,andrearrange
lengeintraditionalWSODmethodsisthelowtrainingef-
P accordingly
ficiency,largelyattributedtotheredundancyofproposals.
6: InitializelistP ,V tomarkpointsfordeletion
delete delete
7: foreachindexifrom0tolength(P)do TraditionalapproachesofteninvolvetheRegionofInterest
8: //skipfurtherchecksforpointsmarkedfordeletion poolinglayerprocessingthousandsofproposalsperimage,
9: ifp iinP deletethen which impairs both effectiveness and efficiency. To ad-
10: Continue
dressthisissue,ourWeakSAM-proposalsadopttransferred
11: endif
knowledge from SAM and classification clues. The pro-
12: //markactivationpointswithlowscore
13: ifv <Ï„ then posedmethodfocusesongeneratingasmallerquantityof
i
14: Appendp i,v itoP delete,V
delete
proposalswhilemaintaininghighrecall,therebyenhancing
15: Continue theoverallefficiencyandefficacyofthedetectionprocessin
16: endif
aWSODcontext.WemainlyapplytheproposedWeakSAM
17: //marklower-scorepointsnearthecurrentpoint
onsomeconvincingWSODmethods,includingOICR(Tang
18: foreachindexj =i+1tolength(P)do
19: if||p âˆ’p ||â‰¤k/2then et al., 2017) and MIST (Ren et al., 2020), which receive
j i
20: Appendp j,v j toP delete,V delete significantimprovements. AsshowninTable1,quantitative
21: endif resultsshowthatWeakSAM-enhancedWSODcanannotate
22: endfor
boundingboxesforobjectsmoreprecisely.
23: endfor
24: RemoveallpointsinP andV fromP andV
delete delete
25: returnP,V Adaptive PGT Generation Generating high-quality
pseudo ground truth (PGT) is the key to the WSOD
Specifically, we extract peak points from cross-attention paradigm. Traditional WSOD methods often encounter
mapsandCAMs, asshowninAlgorithm1. Givencross- theissueofPGTincompleteness. Thisoccursbecausethese
attention maps or CAMs as input, we first initialize the methodstypicallyselecttop-scoringproposalsasPGTor
peakpointslistP,peakvalueslistV,deletedlistsP , applyauniformthresholdtofilterproposalsacrossallcate-
delete
4WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
gories. Suchapproachescanleadtotheomissionofobjects
or entire categories, especially when proposals in certain
         
categoriesscorelow. Toaddresstheseproblems, wepro-
poseanadaptivePGTgenerationmethodtonormalizethe          
scoredistributionofproposals,ensuringtheyfallwithina
        
similarrange,asshowninAlgorithm.2.
ForboxlistB âˆˆ RNÃ—5 andcorrespondingscorelistS âˆˆ         
RNÃ—1, we first select them with a specific classification         
 1 X P E H U  R I  5 R , V
labelandthennormalizethescores. TheN isthenumber
ofpredictedboxes,andtheseconddimensionofB isthe       ( U U R U  5 D W H    
combinationofacategorylabelandfourcoordinatevalues.  
                                           
Next,wekeepboxeswithscoreshigherthanthethreshold  1 R U P D O L ] H G  & O D V V L I L F D W L R Q  / R V V
Ï„ . Pleasenotethatthenormalizationenablesthethreshold
s
toworkforallcategoriesadaptively,sowewouldnotlosea Figure3.Therelationshipbetweenthenormalizedclassification
groundtruthcategoryevenifallboxesinthiscategoryhave loss,thecorrespondingnumberofRoIs,andthecorresponding
low scores. Then, we select the boxes whose main parts errorrate.TheresultsareobtainedfromtrainingtheFaster-RCNN
usingPGTinthepreliminarytrainingstage.
arenotcontainedinsomebiggerboxes. Becausetheboxes
thathavemoreoverlapareoftenlocalcomponentsofsome
Intuitively, we propose a method, named RoI drop regu-
objects. Last,wereturntheboxlistBâ€²asthefinalPGT.
larization, to adaptively drop the RoIs with larger losses.
Notably,theproposedmethodiseasytoimplementandcan
Algorithm2AdaptivePseudoGroundTruthGeneration
furtherhelpthequery-baseddetectorstoalleviatethenoisy
Require: boxes list B of an image, corresponding scores list PGTproblembyitsvariant,querydropregularization. For
S,correspondingclassificationlabelsY,scorethresholdÏ„ s, anchor-basedFSODmethods,e.g.,Faster-RCNN(Renetal.,
overlapthresholdÏ„
o 2015), we first determine the thresholds Ï„ and Ï„ for
Ensure: pseudogroundtruthboxesBâ€² cls reg
1: initializeBâ€²asemptylist classificationlossandregressionloss,respectively. Then,
2: foreachy iinY do wecomputethedropsignald iforiâˆ’thRoI.
3: //getboxesâ€™indiceswithlabely
4: idx =where(B[:,0]==y ) i (cid:26) 1, lcls â‰¤Ï„ , andlreg â‰¤Ï„
i i d = i cls i reg , (1)
5: S i=S[idx i,:] i 0, others
6: B =B[idx ,:]
i i
7: S inorm= maS x(i Sâˆ’ im )âˆ’in m(S ini ()
Si)
//normalizescores wherethel iclsandl ireg representtheclassificationlossand
8: //keepboxeswithhighscore regression loss for each RoI, respectively. When the two
9: idx ={j|s âˆˆSnorm, s >Ï„ }
keep j i j s losses of a RoI are all below their thresholds, we set its
10: B =B [idx ,:]
11:
Sni ormi =Snok re mep
[idx ,:]
drop signal d
i
as 1. Finally, we integrate the d
i
into the
i i keep
12: //selectboxeswithlessoverlap computationoffinallossL.
13: foreachboxb inB do
j i
14: overlaps={|bjâˆ©bk| |b âˆˆB , kÌ¸=j}
|bj| k i
(cid:88) (cid:88)
1 15 6:
:
ifa All po pv ee nr dl bap t< oBÏ„ o
â€²
inoverlapsthen L= d il icls+Î» pâˆ— id il ireg, (2)
j i i
17: endif
18: endfor
19: endfor where pâˆ— i is 1 if the box is positive, and 0 if the box is
20: returnBâ€² negative. TheÎ»isabalancingweight.
RoIDropRegularization Arecognizedissueinthere- Forquery-basedFSODmethods,e.g.,DINO(Zhangetal.,
training phase of WSOD is noisy PGT instances. These 2022),sincequeriescanberegardedasdynamicRoIs,we
noisyinstancesresultinPGTactingastheinaccuratesuper- applyquerydropregularizationonthem. Becauseonlya
vision. Alleviatingthisproblemiscriticalforenhancingthe few matched queries need to calculate box loss lbox and
performanceofWSODretraining. Toanalyzethisproblem
IoUlossliou,weonlysetapercentilethresholdbasedon
indepth,wefirstdividetheRoIsintodifferentlossintervals. classification loss lcls. Only when the iâˆ’th queryâ€™s loss
Then,wemarktheRoIswhosecorrespondingPGTsdonot l icls islessthanthelossatÏ„%percentile,i.e.,l Ï„cls,willits
haveatleast70%IoUwiththegroundtruthboxesaserror correspondingd ibesetto1.
ones. Last, we present the statistics as shown in Fig. 3,
(cid:26) 1, lcls â‰¤lcls
whichdemonstratesthattheRoIswithlargerlossesareina d = i Ï„ . (3)
i 0, others
smallamountandhaveahigherrorrate.
5
 V , R 5  I R  U H E P X 1
 H W D 5  U R U U (WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table1.ComparisonsoftheWSODperformanceintermsofAPmetricsonthreebenchmarks: PASCALVOC2007,PASCALVOC
2012,andCOCO2014.TheSup.columndenotesthetypeofsupervisionusedfortrainingincludingfullsupervision(F),point-level
labels(P),image-levellabels(I).â€œ*â€meanstheresultsrelyonMCG(Pont-Tusetetal.,2016)proposals.â€œâ€¡â€meansthismethodusethe
aheavyRN50-WS-MRRP(Shenetal.,2020)backbone(1.76Ã—parametersthanVGG16and10.10Ã—parametersthanRN50).Wemark
thebestWSODresultsinbold.
VOC07 VOC12 COCO14
Methods Proposal Sup. Retrain
AP50 AP50 AP50:95 AP50 AP75
Fully-supervisedobjectdetectionmethods.
FasterR-CNN(Renetal.,2015) RPN F â€“ 69.9 â€“ 21.2 41.5 â€“
WSODmethodswithpointsupervision.
P2BNet(Chenetal.,2022) RPN P â€“ 60.2 â€“ 19.4 43.5 â€“
WSODmethodswithimage-levelsupervision.
C-MIDN (Gaoetal.,2019) SS,MCG â€“ 52.6 50.2 9.6âˆ— 21.4âˆ— â€“
WSOD2 (Zengetal.,2019) SS â€“ 53.6 47.2 10.8 22.7 â€“
SLV (Chenetal.,2020) SS â€“ 53.5 49.2 â€“ â€“ â€“
CASD (Huangetal.,2020) SS â€“ 56.8 53.6 12.8 26.4 â€“
IM-CFB (Yinetal.,2021) SS I â€“ 54.3 49.4 â€“ â€“ â€“
OD-WSCL (Seoetal.,2022) SS,MCG â€“ 56.4 54.6 13.7âˆ— 27.7âˆ— 11.9âˆ—
WSOD-CBL (Yinetal.,2023) SS â€“ 57.4 53.5 13.6 27.6 â€“
WSOVOD (Linetal.,2024) LO-WSRPN+SAM â€“ 59.1 59.8 18.8 27.1 19.7
WSOVODâ€¡ LO-WSRPN+SAM â€“ 63.4 62.1 20.5 29.1 21.4
Baselineandours.
OICR(Tangetal.,2017) SS,MCG â€“ 41.2 37.9 8.0âˆ— 18.9âˆ— 7.0âˆ—
I
WeakSAM(OICR) WeakSAM â€“ 58.9+17.7 58.4+20.5 19.9+11.9 32.1+13.2 20.6+13.6
Baselineandours.
MIST (Renetal.,2020) SS,MCG â€“ 54.9 52.1 11.4âˆ— 24.3âˆ— 9.4âˆ—
I
WeakSAM(MIST) WeakSAM â€“ 67.4+12.5 66.9+14.8 22.9+11.5 35.2+10.9 24.6+15.2
WSODmethodswithimage-levelsupervision.+Retrain
W2F(Zhangetal.,2018b) RPN FasterR-CNN 52.4 47.8 â€“ â€“ â€“
SoS-WSOD(Suietal.,2022) RPN I FasterR-CNN 64.4 61.9 16.6 32.8 15.2
W2N(Huangetal.,2022) RPN FasterR-CNN 65.4 60.8 15.9 33.3 13.4
Ours.+Retrain
WeakSAM(OICR) RPN FasterR-CNN 65.7 62.9 22.3 36.5 23.0
WeakSAM(MIST) RPN FasterR-CNN 71.8 69.2 23.8 38.5 25.1
I
WeakSAM(OICR) â€“ DINO 66.1 63.7 24.9 36.9 26.8
WeakSAM(MIST) â€“ DINO 73.4 70.2 26.6 39.3 29.0
may have different settings. For WSOD, we use three
datasets,i.e.,PASCALVOC2007(Everinghametal.,2015),
(cid:88)
L Hungarian = d i[l icls+pâˆ— il ibox+pâˆ— il iiou]. (4) PASCALVOC2012(Everinghametal.,2015),andCOCO
i 2014(Linetal.,2014). PASCALVOC2007has2501im-
ages for training, 2510 images for evaluation, and 4592
3.3.WeakSAMforWSIS imagesfortesting. PASCALVOC2012contains5717train-
ingimages,5823validationimages,and10991testimages.
Thankstothehigh-qualityWeakSAM-PGT,wecandirectly
COCO2014includesaround80,000imagesfortrainingand
usethemtopromptSAMforprecisesegmentsaspseudo
40,000imagesforvalidation. FollowingpreviousWSOD
instancelabels. FollowingthepracticesintheWeakSAM-
methods, we train WeakSAM on train and val sets and
WSODpipeline,weevaluatethequalityofWeakSAM-PGT
evaluateWeakSAMonthetestsetforPASCALVOC2007
usingR-CNN-basedandquery-basedinstancesegmentation
and2012. ForCOCO2014,weusethetrainsetfortrain-
methods,respectively. Notably,wedonotintroducemore
ing and the val set for evaluating. PASCAL VOC 2007
techniquesintheWeakSAM-WSIS,becausetheWeakSAM
and2012datasetscomprise20objectcategoriesandCOCO
pseudoinstancelabelsareaccurateenough.
2014comprises80ones. Wereporttheaverageprecision
APmetricsforthesebenchmarks. ForWSIS,weusetwo
4.Experiment
datasets,i.e.,PASCALVOC2012,andCOCO2017. The
4.1.ExperimentalSetup PASCAL VOC 2012 dataset includes 10582 images for
training, and 1449 images for evaluation, comprising 20
DatasetsandMetrics WeevaluatetheproposedWeak-
objectcategories. TheCOCO2017datasetincludes115K
SAMonbothweakly-supervisedobjectdetection(WSOD)
trainingimages,5Kvalidationimages,and20Ktestingim-
and weakly-supervised instance segmentation (WSIS)
ages,comprising80objectcategories. Followingprevious
benchmarks. Notably,thesamedatasetsfordifferenttasks
6WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table2.ComparisonsoftheWSISperformanceintermsofAPmetricsonPASCALVOC2012.TheSup.columndenotesthetypeof
supervisionusedfortrainingincludingmasksupervision(M),saliencymaps(S),image-levellabels(I),andSAMmodels(A).Wemark
thebestWSISresultsinbold.
VOC12
Methods Backbone Sup. Retrain
AP AP AP AP
25 50 70 75
Fully-supervisedinstancesegmentationmethods.
MaskR-CNN(Heetal.,2017) ResNet-101 M â€“ 76.7 67.9 52.5 44.9
WSISmethodswithimage-levelsupervision.+Retrain
WISE(Laradjietal.,2019) ResNet-50 I MaskR-CNN 49.2 41.7 â€“ 23.7
IRNet(Ahnetal.,2019) ResNet-50 I MaskR-CNN â€“ 46.7 23.5 â€“
LIID(Liuetal.,2020) ResNet-50 I+S MaskR-CNN â€“ 48.4 â€“ 24.9
Arunetal.(Arunetal.,2020) ResNet-50 I MaskR-CNN 59.7 50.9 30.2 28.5
WS-RCNN(Ouetal.,2021) VGG-16 I MaskR-CNN 62.2 47.3 â€“ 19.8
BESTIE(Kimetal.,2022) HRNet-W48 I MaskR-CNN 61.2 51.0 31.9 26.6
CIM(Lietal.,2023) ResNet-50 I MaskR-CNN 68.7 55.9 37.1 30.9
Ours.
WeakSAM ResNet-50 I+A MaskR-CNN 70.3 59.6 43.1 36.2
WeakSAM ResNet-50 I+A Mask2Former 73.4 64.4 49.7 45.3
Table3.ComparisonsoftheWSISperformanceintermsofAPmetricsonCOCO2017.TheSup.columndenotesthetypeofsupervision
usedfortrainingincludingmasksupervision(M),saliencymaps(S),image-levellabels(I),andSAMmodels(A).Wemarkthebest
WSISresultsinbold.
COCOval2017 COCOtest-dev
Methods Backbone Sup. Retrain
AP AP AP AP AP AP
50:95 50 75 50:95 50 75
Fully-supervisedinstancesegmentationmethods.
MaskR-CNN(Heetal.,2017) ResNet-50 M â€“ 34.4 55.1 36.7 33.6 55.2 35.3
WSISmethodswithimage-levelsupervision.
WS-JDS(Shenetal.,2019) VGG-16 I â€“ 6.1 11.7 5.5 â€“ â€“ â€“
PDSL(Shenetal.,2021) ResNet18-WS I â€“ 6.3 13.1 5.0 â€“ â€“ â€“
Fanetal.(Fanetal.,2018a) ResNet-101 I+S MaskR-CNN â€“ â€“ â€“ 13.7 25.5 13.5
LIID(Liuetal.,2020) ResNet-50 I+S MaskR-CNN â€“ â€“ â€“ 16.0 27.1 16.5
BESTIE(Kimetal.,2022) HRNet-W48 I MaskR-CNN 14.3 28.0 13.2 14.4 28.0 13.5
CIM(Lietal.,2023) ResNet-50 I MaskR-CNN 17.0 29.4 17.0 17.2 29.7 17.3
Ours.
WeakSAM ResNet-50 I+A MaskR-CNN 20.6 33.9 22.0 21.0 34.5 22.2
WeakSAM ResNet-50 I+A Mask2Former 25.2 38.4 27.0 25.9 39.9 27.9
methods,wereporttheaverageprecisionAPmetricswith 4.2.ComparisonswithState-of-the-artMethods
differentIntersection-over-Union(IoU)thresholds.
Weakly-supervised object detection We present the
ImplementationDetails ForWeakSAMproposalsgener- quantitative WSOD results in Table. 1. Compared with
ation,weadopttheWeakTr(Zhuetal.,2023a)withDeiT- our WSOD baseline methods, i.e., OICR and MIST, the
S(Touvronetal.,2021)modelforgeneratingclassification proposedWeakSAMachievesover10%improvementson
clues,theSAM(Kirillovetal.,2023)withViT-H(Dosovit- allmetrics. TheresultsofWeakSAM(MIST)surpassall
skiyetal.,2020)modeltogenerateproposals. ForWeak- WSODmethodsonallmetrics,whichdemonstratetheeffec-
SAM WSOD pipeline, we use the WSOD networks, i.e., tivenessofWeakSAM-proposals. ComparedwithWSOD
OICR (Tang et al., 2017), and MIST (Ren et al., 2020), methodsretrainedbypseudogroundtruth(PGT),theWeak-
withtheVGG-16(Hanetal.,2021)backbonetogenerate SAM (MIST) with Faster R-CNN retraining still outper-
pseudogroundtruth(PGT),andFSODnetworks,i.e.,Faster formstheSoS-WSOD(Suietal.,2022)andW2N(Huang
R-CNN(Renetal.,2015)andDINO(Zhangetal.,2022), et al., 2022) on all metrics, and the WeakSAM (MIST)
with the ResNet-50 (He et al., 2016) backbone to retrain. with DINO retraining even has comparable performance
AsfortheWeakSAM-WSIS,weuseSAM-ViT-Htogener- with fully-supervised Faster R-CNN. The retraining re-
atepseudoinstancelabelsandtraintheR-CNN-basedand sultsdemonstratetheeffectivenessoftheproposedWSOD
query-basedmethods,i.e.,MaskR-CNN(Heetal.,2017) pipeline,whichincludestheadaptivePGTgenerationand
and Mask2former (Cheng et al., 2022), respectively. All RoI drop retraining. Compared with concurrent work,
hyper-parameters in Alg. 1 and Alg. 2 are following the WSOVOD(Linetal.,2024),whichalsoincorporatesSAM,
defaultmannersas Zhuetal.(2023a)and Suietal.(2022).
7WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table4.AblationstudiesforWeakSAMpromptsonPASCALVOC2007. Weevaluatetheaveragenumberofproposals,recall,and
WSODperformancebyMIST(Renetal.,2020).
Recall
SS DenseSample CAM CAM CrossAttn. Num. AP
fine coarse IoU=0.50 IoU=0.75 IoU=0.90 50
(cid:33) 2001 92.6 57.7 19.2 54.9
(cid:33) 129 79.6 50.7 24.3 45.2
(cid:33) (cid:33) 151 88.9 67.0 37.2 63.3+18.1
(cid:33) (cid:33) (cid:33) 174 90.6 70.1 40.1 65.5+20.3
(cid:33) (cid:33) (cid:33) (cid:33) 213 95.6 75.0 42.1 67.4+22.2
Duetothelimitationofpages,weleavemoreablationstud-
Table5.Ablation studies for adaptive PGT generation and RoI
iesinthesupplementarymaterial,includingefficiencyanal-
dropregularization.WepresenttheresultsonthePASCALVOC
ysis,sensitivityanalysis,qualitativeanalysis,etc.
2007testset.
(a)Ablationstudiesfortheanchor-baseddetector,i.e.,FasterR- ImprovementsofWeakSAMPrompts Tofurtherana-
CNN(Renetal.,2015). lyzetheimprovementsbroughtbytheproposedWeakSAM
prompts, we conduct ablation experiments for different
Top-1PGT AdaptivePGT RoIDrop AP
50 promptsinTable4. Here,weusetheSelectiveSearch(Ui-
(cid:33) 68.4 jlingsetal.,2013)asthebaselinemethod. Whenonlyusing
(cid:33) 70.7+2.3 the densely sampled points, the generated proposals can
(cid:33) (cid:33) 71.8+3.4 achieve 5.1% higher Recall (IoU=0.90), and 9.7% lower
AP forMIST.AfteraddingpeakCAMpointsandpeak
50
(b) Ablation studies for the query-based detector, i.e., cross attention points as prompts, we can achieve higher
DINO(Zhangetal.,2022).
recallandAP throughonly213proposalsonaverage.
50
Top-1PGT AdaptivePGT QueryDrop AP 50 ImprovementsofWSODPipeline Tofurtheranalyzethe
(cid:33) 71.1 improvementsbroughtbytheproposedWeakSAM-WSOD
pipeline,weconductablationexperimentsforadaptivePGT
(cid:33) 72.8+1.7 generation and RoI drop regularization in Table 5. Here,
(cid:33) (cid:33) 73.4+2.3 wesetabaselinethatusesthepredictedboxeswiththetop-
1scoreasPGTandplainFasterR-CNNastheretraining
ourWeakSAM(MIST)alsoachievesbetterperformance.
network. Itshowsthat adaptive PGTgenerationandRoI
drop can both improve the retraining results of Faster R-
Weakly-supervised instance segmentation We first
CNN (Ren et al., 2015) and DINO (Zhang et al., 2022),
presentthequantitativeWSISresultsofthePASCALVOC
respectively.
2012valsetinTable2.TheproposedWeakSAMwithMask
R-CNN retraining achieves the best performance, which
5.Conclusion
demonstratestheWeakSAMcanbenefitWSISeffectively.
Furthermore,thepseudoinstancelabelsgeneratedbyWeak- In this paper, we introduce WeakSAM, a novel frame-
SAMcanalsobeusedbythemodernquery-basedmethods, work utilizing the Segment Anything Model (SAM) for
e.g.,Mask2Former(Chengetal.,2022),whichachievesthe weakly-supervisedinstance-levelrecognition,demonstrat-
bestresults. ingleadingperformanceinWSODandWSISbenchmarks.
Different from the original SAM, which requires interac-
WethenshowthequantitativeWSISresultsonCOCO2017
tionandcannotbeawareofcategories,WeakSAMrepre-
valandtestsets. Onthesemorechallengingbenchmarks,
sentsaninnovativefusionofSAMwithweakly-supervised
WeakSAM with Mask R-CNN retraining achieves better
learning (WSL), overcoming the redundancy problem of
resultsthanCIM(Lietal.,2023). Besides,theWeakSAM
WSODproposals. TofurtheraddressWSODissuessuch
withMask2Formeralsopresentsthebestresults.
as pseudo ground truth (PGT) incompleteness and noisy
PGTinstances, ourapproachincludesadaptivePGTgen-
4.3.AblationStudies
erationandaRegionofInterest(RoI)dropregularization.
Inthissection,wepresenttheablationstudiestoevaluate TheadaptabilityofWeakSAMisfurthershowcasedthrough
the improvements brought by the proposed methods, i.e., itsextensiontoweakly-supervisedinstancesegmentation
WeakSAMprompts,adaptivePGTgeneration,andRoIdrop (WSIS). Our work aims to inspire further research with
retraining. SAMandWSL,contributingsignificantlytothedevelop-
8WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
mentofauniversalframeworkforweakly-supervisedrecog- Diba, A., Sharma, V., Pazandeh, A., Pirsiavash, H., and
nition. VanGool,L. Weaklysupervisedcascadedconvolutional
networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 914â€“922,
References
2017.
Ahn,J.,Cho,S.,andKwak,S. Weaklysupervisedlearning
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
of instance segmentation with inter-pixel relations. In
D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M.,
ProceedingsoftheIEEE/CVFConferenceonComputer
Heigold,G.,Gelly,S.,etal. Animageisworth16x16
VisionandPatternRecognition(CVPR),June2019.
words:Transformersforimagerecognitionatscale.arXiv
Arun, A., Jawahar, C., and Kumar, M. P. Dissimilarity preprintarXiv:2010.11929,2020.
coefficientbasedweaklysupervisedobjectdetection. In
Everingham, M., Eslami, S. A., Van Gool, L., Williams,
ProceedingsoftheIEEE/CVFConferenceonComputer
C. K., Winn, J., and Zisserman, A. The pascal visual
VisionandPatternRecognition(CVPR),June2019.
objectclasseschallenge: Aretrospective. International
Arun,A.,Jawahar,C.,andKumar,M.P. Weaklysupervised journalofcomputervision,111:98â€“136,2015.
instancesegmentationbylearningannotationconsistent
Fan, R., Hou, Q., Cheng, M.-M., Yu, G., Martin, R. R.,
instances. InEuropeanConferenceonComputerVision,
andHu,S.-M. Associatinginter-imagesalientinstances
pp.254â€“270.Springer,2020.
for weakly supervised semantic segmentation. In Pro-
Bilen,H.andVedaldi,A. Weaklysuperviseddeepdetection ceedingsoftheEuropeanconferenceoncomputervision
networks. In Proceedings of the IEEE conference on (ECCV),pp.367â€“383,2018a.
computervisionandpatternrecognition,pp.2846â€“2854,
Fan, R., Hou, Q., Cheng, M.-M., Yu, G., Martin, R. R.,
2016a.
andHu,S.-M. Associatinginter-imagesalientinstances
Bilen,H.andVedaldi,A. Weaklysuperviseddeepdetection for weakly supervised semantic segmentation. In Pro-
networks. In Proceedings of the IEEE conference on ceedingsoftheEuropeanconferenceoncomputervision
computervisionandpatternrecognition,pp.2846â€“2854, (ECCV),pp.367â€“383,2018b.
2016b.
Fu,D.,Chen,M.,Sala,F.,Hooper,S.,Fatahalian,K.,and
Chen,P.,Yu,X.,Han,X.,Hassan,N.,Wang,K.,Li,J.,Zhao, Re, C. Fast and three-rious: Speeding up weak super-
J.,Shi,H.,Han,Z.,andYe,Q. Point-to-boxnetworkfor visionwithtripletmethods. InIII,H.D.andSingh,A.
accurateobjectdetectionviasinglepointsupervision. In (eds.),Proceedingsofthe37thInternationalConference
European Conference on Computer Vision, pp. 51â€“67. on Machine Learning, volume 119 of Proceedings of
Springer,2022. MachineLearningResearch,pp.3280â€“3291.PMLR,13â€“
18 Jul 2020. URL https://proceedings.mlr.
Chen, T., Mai, Z., Li, R., and Chao, W.-l. Segment any-
press/v119/fu20a.html.
thing model (sam) enhanced pseudo labels for weakly
supervised semantic segmentation. arXiv preprint Gao, M., Li, A., Yu, R., Morariu, V. I., and Davis, L. S.
arXiv:2305.05803,2023. C-wsl: Count-guidedweaklysupervisedlocalization. In
Proceedings of the European conference on computer
Chen,Z.,Fu,Z.,Jiang,R.,Chen,Y.,andHua,X.-S. Slv:
vision(ECCV),pp.152â€“168,2018.
Spatial likelihood voting for weakly supervised object
detection. InProceedingsoftheIEEE/CVFConference Gao,Y.,Liu,B.,Guo,N.,Ye,X.,Wan,F.,You,H.,andFan,
on Computer Vision and Pattern Recognition (CVPR), D. C-midn: Coupledmultipleinstancedetectionnetwork
June2020. withsegmentationguidanceforweaklysupervisedobject
detection. InProceedingsoftheIEEE/CVFInternational
Cheng,B.,Misra,I.,Schwing,A.G.,Kirillov,A.,andGird-
ConferenceonComputerVision,pp.9834â€“9843,2019.
har,R. Masked-attentionmasktransformerforuniversal
imagesegmentation. InProceedingsoftheIEEE/CVF Ge,W.,Guo,S.,Huang,W.,andScott,M.R. Label-penet:
conferenceoncomputervisionandpatternrecognition, Sequentiallabelpropagationandenhancementnetworks
pp.1290â€“1299,2022. for weakly supervised instance segmentation. In Pro-
ceedingsoftheIEEE/CVFInternationalConferenceon
Cheng, T., Wang, X., Chen, S., Zhang, Q., and Liu, W.
ComputerVision,pp.3345â€“3354,2019.
Boxteacher: Exploring high-quality pseudo labels for
weaklysupervisedinstancesegmentation.InProceedings Han, K., Xiao,A., Wu, E., Guo, J., Xu,C., andWang, Y.
of the IEEE/CVF Conference on Computer Vision and Transformerintransformer. AdvancesinNeuralInforma-
PatternRecognition,pp.3145â€“3154,2023. tionProcessingSystems,34:15908â€“15919,2021.
9WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn- Khoreva, A., Benenson, R., Hosang, J., Hein, M., and
ingforimagerecognition. InProceedingsoftheIEEE Schiele,B. Simpledoesit: Weaklysupervisedinstance
conferenceoncomputervisionandpatternrecognition, andsemanticsegmentation. InProceedingsoftheIEEE
pp.770â€“778,2016. ConferenceonComputerVisionandPatternRecognition
(CVPR),July2017.
He,K.,Gkioxari,G.,DollaÂ´r,P.,andGirshick,R. Maskr-
cnn. InProceedingsoftheIEEEinternationalconference Kim,B.,Yoo,Y.,Rhee,C.E.,andKim,J. Beyondsemantic
oncomputervision,pp.2961â€“2969,2017. to instance segmentation: Weakly-supervised instance
segmentationviasemanticknowledgetransferandself-
Hsieh,Y.-H.,Chen,G.-S.,Cai,S.-X.,Wei,T.-Y.,Yang,H.- refinement. InProceedingsoftheIEEE/CVFConference
F.,andChen,C.-S. Class-incrementalcontinuallearning onComputerVisionandPatternRecognition(CVPR),pp.
forinstancesegmentationwithimage-levelweaksuper- 4278â€“4287,June2022.
vision. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision(ICCV),pp.1250â€“1261, Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,
October2023. Gustafson,L.,Xiao,T.,Whitehead,S.,Berg,A.C.,Lo,
W.-Y., DollaÂ´r, P., and Girshick, R. Segment anything.
Hsu,C.-C.,Hsu,K.-J.,Tsai,C.-C.,Lin,Y.-Y.,andChuang, arXiv:2304.02643,2023.
Y.-Y. Weakly supervised instance segmentation using
the bounding box tightness prior. Advances in Neural Laptev,I.,Kantorov,V.,Oquab,M.,andCho,M. Context-
InformationProcessingSystems,32,2019. locnet: Context-awaredeepnetworkmodelsforweakly
supervisedlocalization.
Hu, Z., Liu, Z., Li, G., Ye, L., Zhou, L., and Wang, Y.
Weakly supervised instance segmentation using multi- Laradji,I.H.,Vazquez,D.,andSchmidt,M. Wherearethe
stageerasingrefinementandsaliency-guidedproposals masks: Instancesegmentationwithimage-levelsupervi-
ordering. JournalofVisualCommunicationandImage sion. arXivpreprintarXiv:1907.01430,2019.
Representation,73:102957,2020.
Laradji12, I.H., Vazquez, D., Schmidt,M., andElement,
Huang, Z., Zou, Y., Kumar, B., and Huang, D. Compre- A. Where are the masks: Instance segmentation with
hensiveattentionself-distillationforweakly-supervised image-levelsupervision.
objectdetection. Advancesinneuralinformationprocess-
ingsystems,33:16797â€“16807,2020. Lee, J., Yi, J., Shin, C., and Yoon, S. Bbam: Bounding
boxattributionmapforweaklysupervisedsemanticand
Huang, Z., Bao, Y., Dong, B., Zhou, E., and Zuo, W. instancesegmentation. InProceedingsoftheIEEE/CVF
W2n:switching from weak supervision to noisy super- ConferenceonComputerVisionandPatternRecognition
visionforobjectdetection,2022. (CVPR),pp.2643â€“2652,June2021.
Hwang,J.,Kim,S.,Son,J.,andHan,B. Weaklysupervised Li, D., Huang, J.-B., Li, Y., Wang, S., and Yang, M.-H.
instancesegmentationbydeepcommunitylearning. In Weaklysupervisedobjectlocalizationwithprogressive
ProceedingsoftheIEEE/CVFWinterConferenceonAp- domainadaptation. InProceedingsoftheIEEEConfer-
plicationsofComputerVision(WACV),pp.1020â€“1029, ence on Computer Vision and Pattern Recognition, pp.
January2021. 3512â€“3520,2016.
Jia,Q.,Wei,S.,Ruan,T.,Zhao,Y.,andZhao,Y.Gradingnet: Li, W., Liu, W., Zhu, J., Cui, M., Hua, X.-S., andZhang,
Towardsprovidingreliablesupervisionsforweaklysu- L. Box-supervisedinstancesegmentationwithlevelset
pervisedobjectdetectionbygradingtheboxcandidates. evolution. InEuropeanconferenceoncomputervision,
InProceedingsoftheAAAIConferenceonArtificialIn- pp.1â€“18.Springer,2022.
telligence,volume35,pp.1682â€“1690,2021.
Li,X.,Kan,M.,Shan,S.,andChen,X. Weaklysupervised
Jiang,P.-T.andYang,Y.Segmentanythingisagoodpseudo- objectdetectionwithsegmentationcollaboration. InPro-
labelgeneratorforweaklysupervisedsemanticsegmen- ceedingsoftheIEEE/CVFInternationalConferenceon
tation. arXivpreprintarXiv:2305.01275,2023. ComputerVision(ICCV),October2019.
Jie, Z., Wei, Y., Jin, X., Feng, J., and Liu, W. Deep self- Li,Z.,Zeng,Z.,Liang,Y.,andYu,J.-G.Completeinstances
taughtlearningforweaklysupervisedobjectlocalization. miningforweaklysupervisedinstancesegmentation. In
InProceedingsoftheIEEEconferenceoncomputervi- InternationalJointConferenceonArtificialIntelligence,
sionandpatternrecognition,pp.1377â€“1385,2017. 2023.
10WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Liao,S.,Sun,Y.,Gao,C.,KP,P.S.,Mu,S.,Shimamura,J., segmentationandobjectproposalgeneration. IEEEtrans-
andSagata,A. Weaklysupervisedinstancesegmentation actionsonpatternanalysisandmachineintelligence,39
usinghybridnetworks. InICASSP2019-2019IEEEIn- (1):128â€“140,2016.
ternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pp.1917â€“1921.IEEE,2019. Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn:
Towardsreal-timeobjectdetectionwithregionproposal
Lin, J., Shen, Y., Wang, B., Lin, S., Li, K., and Cao, L.
networks. Advances in neural information processing
Weaklysupervisedopen-vocabularyobjectdetection. In
systems,28,2015.
ProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,2024.
Ren,Z.,Yu,Z.,Yang,X.,Liu,M.-Y.,Lee,Y.J.,Schwing,
A.G.,andKautz,J. Instance-aware,context-focused,and
Lin,T.-Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ra-
memory-efficientweaklysupervisedobjectdetection. In
manan,D.,DollaÂ´r,P.,andZitnick,C.L. Microsoftcoco:
ProceedingsoftheIEEE/CVFconferenceoncomputer
Commonobjectsincontext. InComputerVisionâ€“ECCV
visionandpatternrecognition,pp.10598â€“10607,2020.
2014: 13thEuropeanConference,Zurich,Switzerland,
September6-12,2014,Proceedings,PartV13,pp.740â€“
Schroeter, J., Sidorov, K., and Marshall, D. Weakly-
755.Springer,2014.
supervised temporal localization via occurrence count
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and DollaÂ´r, P. learning. InInternationalConferenceonMachineLearn-
Focallossfordenseobjectdetection. InProceedingsof ing,pp.5649â€“5659.PMLR,2019.
the IEEE international conference on computer vision,
pp.2980â€“2988,2017. Seo, J., Bae, W., Sutherland, D. J., Noh, J., and Kim, D.
Objectdiscoveryviacontrastivelearningforweaklysu-
Lin,Y.,Chen,M.,Wang,W.,Wu,B.,Li,K.,Lin,B.,Liu, pervised objectdetection. InEuropeanConference on
H., and He, X. Clip is also an efficient segmenter: A ComputerVision,pp.312â€“329.Springer,2022.
text-drivenapproachforweaklysupervisedsemanticseg-
mentation. InProceedingsoftheIEEE/CVFConference Shen, Y., Ji, R., Wang, Y., Wu, Y., and Cao, L. Cyclic
onComputerVisionandPatternRecognition,pp.15305â€“ guidanceforweaklysupervisedjointdetectionandseg-
15314,2023. mentation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.697â€“
Liu,B.,Gao,Y.,Guo,N.,Ye,X.,Wan,F.,You,H.,andFan,
707,2019.
D. Utilizingtheinstabilityinweaklysupervisedobject
detection. InCVPRWorkshops,2019.
Shen,Y.,Ji,R.,Wang,Y.,Chen,Z.,Zheng,F.,Huang,F.,
andWu,Y. Enablingdeepresidualnetworksforweakly
Liu, Y., Wu, Y.-H., Wen, P., Shi, Y., Qiu, Y., and Cheng,
supervisedobjectdetection. InComputerVisionâ€“ECCV
M.-M. Leveraginginstance-,image-anddataset-levelin-
2020: 16thEuropeanConference,Glasgow,UK,August
formationforweaklysupervisedinstancesegmentation.
23â€“28, 2020, Proceedings, Part VIII 16, pp. 118â€“136.
IEEETransactionsonPatternAnalysisandMachineIn-
Springer,2020.
telligence,44(3):1415â€“1428,2020.
Locatello,F.,Poole,B.,RaÂ¨tsch,G.,SchoÂ¨lkopf,B.,Bachem, Shen, Y., Cao, L., Chen, Z., Zhang, B., Su, C., Wu, Y.,
O.,andTschannen,M. Weakly-superviseddisentangle- Huang,F.,andJi,R. Paralleldetection-and-segmentation
mentwithoutcompromises. InInternationalConference learningforweaklysupervisedinstancesegmentation. In
onMachineLearning,pp.6348â€“6359.PMLR,2020. ProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.8198â€“8208,2021.
Loshchilov,I.andHutter,F. Decoupledweightdecayregu-
larization. arXivpreprintarXiv:1711.05101,2017. Sui,L.,Zhang,C.-L.,andWu,J. Salvageofsupervisionin
weaklysupervisedobjectdetection. InProceedingsofthe
Ma,J.andWang,B. Segmentanythinginmedicalimages.
IEEE/CVFConferenceonComputerVisionandPattern
arXivpreprintarXiv:2304.12306,2023.
Recognition,pp.14227â€“14236,2022.
Ou,J.-R.,Deng,S.-L.,andYu,J.-G. Ws-rcnn: Learningto
scoreproposalsforweaklysupervisedinstancesegmenta- Sun, G., Wang, W., Dai, J., and Van Gool, L. Mining
tion. Sensors,21(10):3475,2021. cross-imagesemanticsforweaklysupervisedsemantic
segmentation. In Computer Visionâ€“ECCV 2020: 16th
Pont-Tuset,J.,Arbelaez,P.,Barron,J.T.,Marques,F.,and EuropeanConference,Glasgow,UK,August23â€“28,2020,
Malik,J. Multiscalecombinatorialgroupingforimage Proceedings,PartII16,pp.347â€“365.Springer,2020.
11WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Sun,W.,Liu,Z.,Zhang,Y.,Zhong,Y.,andBarnes,N.Anal- Wang,X.,Feng,J.,Hu,B.,Ding,Q.,Ran,L.,Chen,X.,and
ternativetowsss? anempiricalstudyofthesegmentany- Liu, W. Weakly-supervised instance segmentation via
thingmodel(sam)onweakly-supervisedsemanticseg- class-agnosticlearningwithsalientimages. InProceed-
mentationproblems. arXivpreprintarXiv:2305.01586, ingsoftheIEEE/CVFConferenceonComputerVision
2023. andPatternRecognition,pp.10225â€“10235,2021.
Tang,P.,Wang,X.,Bai,X.,andLiu,W. Multipleinstance Xu,C.,Tao,D.,Xu,C.,andRui,Y. Large-marginweakly
detectionnetworkwithonlineinstanceclassifierrefine- supervised dimensionality reduction. In International
ment.InProceedingsoftheIEEEconferenceoncomputer conference on machine learning, pp. 865â€“873. PMLR,
visionandpatternrecognition,pp.2843â€“2851,2017. 2014.
Xu,L.,Ouyang,W.,Bennamoun,M.,Boussaid,F.,andXu,
Tang, P., Wang, X., Bai, S., Shen, W., Bai, X., Liu, W.,
D. Multi-classtokentransformerforweaklysupervised
andYuille,A. Pcl: Proposalclusterlearningforweakly
semanticsegmentation. InProceedingsoftheIEEE/CVF
supervisedobjectdetection.IEEEtransactionsonpattern
ConferenceonComputerVisionandPatternRecognition,
analysisandmachineintelligence,42(1):176â€“191,2018a.
pp.4310â€“4319,2022.
Tang, P., Wang, X., Wang, A., Yan, Y., Liu, W., Huang,
Yang,K.,Li,D.,andDou,Y. Towardspreciseend-to-end
J., and Yuille, A. Weakly supervised region proposal
weaklysupervisedobjectdetectionnetwork. InProceed-
network and object detection. In Proceedings of the
ingsoftheIEEE/CVFInternationalConferenceonCom-
European conference on computer vision (ECCV), pp.
puterVision,pp.8372â€“8381,2019.
352â€“368,2018b.
Yang, L., Wang, Y., Li, X., Wang, X., and Yang,
Tian,Z.,Shen,C.,Wang,X.,andChen,H. Boxinst: High- J. Fine-grained visual prompting. arXiv preprint
performanceinstancesegmentationwithboxannotations. arXiv:2306.04356,2023.
In Proceedings of the IEEE/CVF Conference on Com-
Yin, Y., Deng, J., Zhou, W., and Li, H. Instance mining
puter Vision and Pattern Recognition, pp. 5443â€“5452,
with class feature banks for weakly supervised object
2021.
detection. In Proceedings of the AAAI Conference on
Touvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles, ArtificialIntelligence,volume35,pp.3190â€“3198,2021.
A.,andJeÂ´gou,H.Trainingdata-efficientimagetransform-
Yin, Y., Deng, J., Zhou, W., Li, L., and Li, H. Cyclic-
ers&distillationthroughattention. InInternationalcon-
bootstraplabelingforweaklysupervisedobjectdetection.
ferenceonmachinelearning,pp.10347â€“10357.PMLR,
InProceedingsoftheIEEE/CVFInternationalConfer-
2021.
enceonComputerVision,pp.7008â€“7018,2023.
Uijlings, J. R. R., van de Sande, K. E. A., Gevers, Zeng,Z.,Liu,B.,Fu,J.,Chao,H.,andZhang,L. Wsod2:
T., and Smeulders, A. W. M. Selective search for Learningbottom-upandtop-downobjectnessdistillation
object recognition. International Journal of Com- forweakly-supervisedobjectdetection. InProceedings
puter Vision, 104:154 â€“ 171, 2013. URL https: oftheIEEE/CVFinternationalconferenceoncomputer
//api.semanticscholar.org/CorpusID: vision,pp.8292â€“8300,2019.
216077384.
Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni,
Wan,F.,Wei,P.,Jiao,J.,Han,Z.,andYe,Q. Min-entropy L.M.,andShum,H.-Y.Dino:Detrwithimproveddenois-
latentmodelforweaklysupervisedobjectdetection. In inganchorboxesforend-to-endobjectdetection,2022.
ProceedingsoftheIEEEconferenceoncomputervision
Zhang,J.,Su,H.,He,Y.,andZou,W.Weaklysupervisedin-
andpatternrecognition,pp.1297â€“1306,2018.
stancesegmentationviacategory-awarecenternesslearn-
ingwithlocalizationsupervision. PatternRecognition,
Wan,F.,Liu,C.,Ke,W.,Ji,X.,Jiao,J.,andYe,Q. C-mil:
136:109165,2023.
Continuationmultipleinstancelearningforweaklysuper-
visedobjectdetection. InProceedingsoftheIEEE/CVF Zhang,K.,Yuan,C.,Zhu,Y.,Jiang,Y.,andLuo,L. Weakly
ConferenceonComputerVisionandPatternRecognition,
supervisedinstancesegmentationbyexploringentireob-
pp.2199â€“2208,2019. jectregions. IEEETransactionsonMultimedia,2021.
Wang, X., Wang, B., Bai, X., Liu, W., and Tu, Z. Max- Zhang,X.,Feng,J.,Xiong,H.,andTian,Q.Zigzaglearning
margin multiple-instance dictionary learning. In Inter- forweaklysupervisedobjectdetection. InProceedings
nationalconferenceonmachinelearning,pp.846â€“854. oftheIEEEconferenceoncomputervisionandpattern
PMLR,2013. recognition,pp.4262â€“4270,2018a.
12WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Zhang,Y.,Bai,Y.,Ding,M.,Li,Y.,andGhanem,B. W2f:
Aweakly-supervisedtofully-supervisedframeworkfor
objectdetection. InProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,pp.928â€“936,
2018b.
Zhou,Y.,Zhu,Y.,Ye,Q.,Qiu,Q.,andJiao,J.Weaklysuper-
visedinstancesegmentationusingclasspeakresponse. In
ProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pp.3791â€“3800,2018.
Zhou, Z.-H. A brief introduction to weakly supervised
learning. Nationalsciencereview,5(1):44â€“53,2018.
Zhu, L., Li, Y., Fang, J., Liu, Y., Xin, H., Liu, W., and
Wang,X. Weaktr: Exploringplainvisiontransformerfor
weakly-supervisedsemanticsegmentation.arXivpreprint
arXiv:2304.01184,2023a.
Zhu, L., Peng, L., Ding, S., and Liu, Z. An encoder-
decoderframeworkwithdynamicconvolutionforweakly
supervisedinstancesegmentation. IETComputerVision,
2023b.
Zhu,Y.,Zhou,Y.,Xu,H.,Ye,Q.,Doermann,D.,andJiao,J.
Learninginstanceactivationmapsforweaklysupervised
instancesegmentation. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition
(CVPR),June2019.
Zitnick,C.L.andDollaÂ´r,P. Edgeboxes: Locatingobject
proposalsfromedges. InComputerVisionâ€“ECCV2014:
13thEuropeanConference,Zurich,Switzerland,Septem-
ber 6-12, 2014, Proceedings, Part V 13, pp. 391â€“405.
Springer,2014.
13WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
A.MoreDetailsofWeakSAMProposals regression losses of RoIs have different number distribu-
tionscomparedtotheclassificationlosses. However,they
            exhibitsimilarerrorratecurves. Thisobservationfurther
demonstratesthenecessityofRoIdropregularizationwith
           
aregressionthresholdÏ„ .
reg
           
          C.MoreDetailsofQueryDropRegularization
         
BecauseDINO(Zhangetal.,2022)employsFocalloss(Lin
        et al., 2017) as the classification loss, queries associated
                                       
withbackgroundclassestendtohavehigherpredictedprob-
Figure4.Thecosinesimilarityamongthefeaturesofproposals, abilities and lower losses. This results in the inadvertent
i.e.,LeftforSelectiveSearchproposalsandRightforWeakSAM- omissionofmostforegroundcategoryquerieswhendirectly
proposals. For a single image from PASCAL VOC 2007, we droppingqueries. To mitigatethisissue, ourfirst stepin-
randomlysampled200proposalfeaturestocalculatetheirsimilar- volvesnormalizingtheunweightedFocalloss,whichises-
ity. sentiallythebinarycross-entropyloss,forbothforeground
and background queries within each training batch. Nor-
Wefurtheranalyzetheproposalsimilarityinthedifferent
malizing at the batch level broadens the sampling scope
weakly-supervisedobjectdetection(WSOD)proposals,as
from a single image to the size of the batch. In the sec-
showninFig.4. Werandomlysample200proposalfeatures
ondstep,queriesaredroppedbasedontheirlossranking
eachfromSelectiveSearch(Uijlingsetal.,2013)proposals
post-normalization.Thisapproachavoidsmakingthemodel
andWeakSAM-proposals, andthencomputetheircosine
convergeslowlyduetothedroppingofthemostforeground
similarity,respectively. Pleasenotethatallthefeaturesare
queries.
output by the RoI pooling layer. It can be seen that the
featuresfromSelectiveSearchtendtohavehighersimilarity
withotherones. Incontrast,thefeaturesfromWeakSAM- D.EfficiencyComparison
proposalsshowlowersimilarity,whichusuallymeansithas
Tofurtheranalyzetheefficiencyimprovementbroughtby
lessoverlapandredundancy.
the WeakSAM, we present the efficiency comparison be-
tweenSelectiveSearch(Uijlingsetal.,2013)andourWeak-
B.MoreDetailsofRoIDropRegularization
SAMonamachinewith4GPUcards,asshowninTable6.
OurWeakSAMreducesthenumberofproposalsby89.4%,
the proposal generation time by 65.5%, the WSOD net-
worktrainingtimeby43.8%,andtheGPUmemorycostby
    68.2%. Theresultsdemonstratethesignificantefficiency
    
improvementbroughtbytheproposedWeakSAM.
   
        
E.AdditionalQuantitativeResults
   
   
We present the comparison on PASCAL VOC 2007
   
    trainvalsetintermsofCorLoc,asshowninTable7.Itcan
   
 1 X P E H U  R I  5 R , V beseenthattheWeakSAMachievesthe13.9%and14.1%
     ( U U R U  5 D W H     CorLoc improvements on OICR and MIST, respectively.
TheWeakSAM(OICR)outperformstheWSOD-CBL(Yin
   
 
                                            et al., 2023) by 11.1% CorLoc. The results demonstrate
 1 R U P D O L ] H G  5 H J U H V V L R Q  / R V V
thesignificanteffectivenessimprovementbroughtbyour
WeakSAM.
Figure5.Therelationshipbetweenthenormalizedregressionloss,
thecorrespondingnumberofRoIs,andthecorrespondingerror
rate.TheresultsareobtainedfromtrainingtheFaster-RCNN(Ren F.AdditionalAblationStudies
etal.,2015)usingPASCALVOC2007pseudogroundtruth(PGT)
F.1.ImprovementsofClassificationMethods
inthepreliminarytrainingstage.
Tofurtheranalyzetheimpactofmethodsthatgenerateclas-
Wefurtherpresenttherelationshipbetweenthenormalized sification clues, we replaced WeakTr in WeakSAM with
regression loss, the corresponding number of RoIs, and MCTformerandCLIP-ES.AsindicatedinTable8,Weak-
the corresponding error rate in Fig. 5. It shows that the
14
 V , R 5  I R  U H E P X 1
 H W D 5  U R U U (WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table6.EfficiencycomparisonbetweenSelectiveSearchandourWeakSAMduringthetrainingonthePASCALVOC2007.â€˜Num.â€™is
thenumberofproposals,â€˜T â€™isthetimeconsumptionforgeneratingproposals,â€˜T â€™isthetimeconsumptionfortrainingthe
Proposals WSOD
WSODnetwork,i.e.,MIST(Renetal.,2020),andâ€˜M â€™istheGPUmemorycostforeachGPUcard.
WSOD
Num. T T M
Proposals WSOD WSOD
SS(Uijlingsetal.,2013) 2001 11.6hrs 16hrs 17810MiB
Ours 213-89.4% 4hrs-65.5% 9hrs-43.8% 5667MiB-68.2%
Table7. ComparisononPASCALVOC2007trainvalsetintermsofCorLoc(%)withmulti-scaletesting.
Methods Sup. Proposal CorLoc
WSDDN(Bilen&Vedaldi,2016a) EB(Zitnick&DollaÂ´r,2014) 53.5
Yangetal.(Yangetal.,2019) SS(Uijlingsetal.,2013) 68.0
C-MIL (Wanetal.,2019) SS 65.0
C-MIDN(Gaoetal.,2019) SS 53.5
WSOD2(Zengetal.,2019) SS 69.5
I
CASD(Huangetal.,2020) SS 70.4
OD-WSCL(Seoetal.,2022) SS 69.8
WSOD-CBL(Yinetal.,2023) SS 71.8
WSOVOD(Linetal.,2024) LO-WSRPN+SAM 77.2
WSOVODâ€¡ LO-WSRPN+SAM 80.1
OICR(Tangetal.,2017) SS 60.6
I
WeakSAM(OICR) WeakSAM 74.5+13.9
MIST(Renetal.,2020) SS 68.8
I
WeakSAM(MIST) WeakSAM 82.9+14.1
SAM(MCTformer)achievesa1.6%higherRecall(IoU=90) mance,whereasdroppingbothtypesofqueriesresultsina
thanWeakSAM(WeakTr). Furthermore,WeakSAM(CLIP- slightdecreaseinperformance. Wemaintaintheviewpoint
ES) records increases of 0.6% and 2.4% in Recall over that dropping more background queries may also lead to
WeakSAM(WeakTr)atIoUthresholdsof75and90,respec- slowerconvergence. Consequently,wechoosetodroponly
tively. TheseresultsdemonstratetheversatilityoftheWeak- Query toachievebetterperformance.
things
SAMproposal-generatingmethodacrossdifferentclassifi-
WefurtheranalyzetheimpactofclassificationthresholdÏ„
cationmethods. Pleasenotethatallclassificationmethods
inquerydropregularization,asshowninTable11. Quan-
employed in this study are CAM networks from weakly-
titative results demonstrate that 90 is the best percentile
supervisedsemanticsegmentation(WSSS)methods. Since
classificationthreshold.
thesenetworksaretypicallywell-tunedonspecificdatasets,
such as PASCAL VOC 2012 and COCO 2014, they are
adeptatprovidingrichclassificationclues. G.AdditionalVisualizationResults
Fig.6comparestheSelectiveSearchproposalswiththose
F.2.AblationStudiesforRoIDropRegularization
generatedbyWeakSAM.TheWeakSAM-proposalsexhibit
To further analyze the impact of the regression threshold lessredundancythanSelectiveSearchproposals. Fig.7con-
andclassificationthresholdinRoIdropregularization,we trasts the Top-1 PGT with adaptive PGT, demonstrating
conduct experiments as shown in Table 9. It is observed that adaptive PGT generation captures a greater number
that the best regression threshold Ï„ and classification ofobjects,whichmightbemissedbytheTop-1approach.
reg
threshold Ï„ for RoI drop regularization is 1.0 and 4.0, Additionally, adaptive PGT can be seamlessly integrated
cls
respectively. togeneratepseudoinstancelabels. Fig.8presentstheob-
jectdetectionresultsusingWeakSAM(MIST),showingits
F.3.AblationStudiesforQueryDropRegularization capabilitytoaccuratelycaptureentireobjectswithoutgener-
atingexcessivenoisyboundingboxes. InFig.9,theinstance
Toanalyzetheimpactofdroppingqueriescorresponding segmentationresultsofWeakSAM-Mask2Formerretraining
to foreground categories Query things and background cat- areshowcased. Theresultsindicateeffectivesegmentation
egories Query bkg, we conduct ablations as shown in Ta- ofentireinstanceswithanotablereductioninoverlapping
ble 10. Experimental results indicate that dropping only segments.
theforegroundqueries(Query )leadstothebestperfor-
things
15WeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Table8.AblationstudiesforclassificationmethodsthatgenerateWeakSAMqueriesonPASCALVOC2007trainvalset.Weevaluate
theaveragenumberofproposalsandRecall.WepresenttheresultsofSelectiveSearch(Uijlingsetal.,2013)atthefirstrowasabaseline.
Recall
CLSMethods Num.
IoU=0.50 IoU=0.75 IoU=0.90
None 2001 92.6 57.7 19.2
WeakTr(Zhuetal.,2023a) 213 95.6 75.0 42.1
MCTformer(Xuetal.,2022) 173 93.2 74.8 43.7
CLIP-ES(Linetal.,2023) 205 93.8 75.6 44.5
Figure6. VisualizationoftheproposalsboxesonthePASCALVOC2007trainvalset.
H.MoreImplementationDetails
Table9.Ablationstudyfortheregressionthresholdandclassifica- ForAlgorithm1,wesetthekernelsizekto128andactiva-
tionthresholdinRoIdropregularizationintermsofAP onthe tionthresholdÏ„ to0.9followingdefaultparametersfrom
50
PASCALVOC2007testset. WeakTr(Zhuetal.,2023a). AndforAlgorithm2,wefol-
lowthedefaultmannerssimilartoSoS-WSOD(Suietal.,
(a)Ablationstudyforthere- (b)Ablationstudyfortheclas-
2022),inwhichscorethresholdÏ„ issetto0.3,andoverlap
gressionthreshold. sificationthreshold. s
thresholdÏ„ issetto0.85.
o
Ï„ 0.8 1.0 1.2 Ï„ 3.0 4.0 5.0
reg cls ForFasterR-CNN(Renetal.,2015)retraining,weadopt
AP 71.0 71.8 71.3 AP 71.2 71.8 71.1
50 50 thesametrainingstrategyandhyper-parametersastheFully-
supervisedones. ForDINO(Zhangetal.,2022)retraining,
weusealearningrateof9e-5withtheAdamW(Loshchilov
Table10.Ablation studies for query drop regularization on the &Hutter,2017)optimizer,andamaxepochof14. More-
PASCALVOC2007testset. over,weapplymulti-scaleaugmentationandhorizontalflips
inbothtrainingandtesting.
Baseline Query Query AP
things bkg 50
ForimplementationsofMaskR-CNN(Heetal.,2017)and
(cid:33) 72.8
Mask2Former(Chengetal.,2022),wefollowtheirdefault
(cid:33) 73.4+0.6 hyper-parameters.
(cid:33) (cid:33) 73.3+0.5
Table11.Ablationstudyfortheclassificationthresholdinquery
dropregularizationintermsofAP onthePASCALVOC2007
50
testset.
Ï„ (%) 100 90 80
AP 72.8 73.4 71.8
50
16
egamIlanigirO
SS
MASkaeWWeakSAM:SegmentAnythingMeetsWeakly-supervisedInstance-levelRecognition
Figure7.VisualizationofthepseudogroundtruthboxesandpseudoinstancelabelsonthePASCALVOC2012trainaugset.
Figure8. Visualizationoftheweakly-supervisedobjectdetectiononthePASCALVOC2007testset.
Figure9. Visualizationoftheweakly-supervisedinstancesegmentationonthePASCALVOC2012valset.
17
TGP
1-poT
TGP
evitpadA
.tsnI
oduesP
hturTdnuorG
TSIM
MASkaeW
hturTdnuorG
MIC
MASkaeW