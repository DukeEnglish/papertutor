GazeTrak: Exploring Acoustic-based Eye Tracking on a
Glass Frame
Ke Li Ruidong Zhang Boao Chen
CornellUniversity CornellUniversity CornellUniversity
Ithaca,USA Ithaca,USA Ithaca,USA
kl975@cornell.edu rz379@cornell.edu bc526@cornell.edu
Siyuan Chen Sicheng Yin Saif Mahmud
CornellUniversity UniversityofEdinburgh CornellUniversity
Ithaca,USA Edinburgh,UnitedKingdom Ithaca,USA
sc2489@cornell.edu yinsicheng1999@outlook.com sm2446@cornell.edu
Qikang Liang FranÃ§ois GuimbretiÃ¨re Cheng Zhang
CornellUniversity CornellUniversity CornellUniversity
Ithaca,USA Ithaca,USA Ithaca,USA
ql75@cornell.edu fvg3@cornell.edu chengzhang@cornell.edu
ABSTRACT CCSCONCEPTS
Inthispaper,wepresentGazeTrak,thefirstacoustic-based â€¢Human-centeredcomputingâ†’Ubiquitousandmo-
eyetrackingsystemonglasses.Oursystemonlyneedsone biledevices;â€¢Hardwareâ†’Powerandenergy.
speakerandfourmicrophonesattachedtoeachsideofthe
glasses. These acoustic sensors capture the formations of KEYWORDS
theeyeballsandthesurroundingareasbyemittingencoded
EyeTracking,AcousticSensing,SmartGlasses,Low-power
inaudiblesoundtowardseyeballsandreceivingthereflected
signals.Thesereflectedsignalsarefurtherprocessedtocal-
ACMReferenceFormat:
culatetheechoprofiles,whicharefedtoacustomizeddeep KeLi,RuidongZhang,BoaoChen,SiyuanChen,SichengYin,Saif
learning pipeline to continuously infer the gaze position. Mahmud,QikangLiang,FranÃ§oisGuimbretiÃ¨re,andChengZhang.
Inauserstudywith20participants,GazeTrakachievesan 2024.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlass
accuracyof3.6Â°withinthesameremountingsessionand4.9Â° Frame.InThe30thAnnualInternationalConferenceonMobileCom-
acrossdifferentsessionswitharefreshingrateof83.3Hzand putingandNetworking(ACMMobiComâ€™24),September30-October4,
apowersignatureof287.9mW.Furthermore,wereportthe 2024,WashingtonD.C.,DC,USA.ACM,NewYork,NY,USA,16pages.
https://doi.org/10.1145/3636534.3649376
performanceofourgazetrackingsystemfullyimplemented
onanMCUwithalow-powerCNNaccelerator(MAX78002).
Inthisconfiguration,thesystemrunsatupto83.3Hzand
1 INTRODUCTION
hasatotalpowersignatureof95.4mWwitha30HzFPS.
Currently,state-of-the-arteyetrackingtechnologiesutilize
camerastocapturegazepoints.However,cameras-basedeye
trackingsolutionsareknowntohavearelativelyhighpower
Permissiontomakedigitalorhardcopiesofallorpartofthisworkfor signature,whichmaynotworkwellforsmartglasseswitha
personalorclassroomuseisgrantedwithoutfeeprovidedthatcopies relativelysmallbatterycapacity.Forinstance,TobiiProGlass
arenotmadeordistributedforprofitorcommercialadvantageandthat
3 [2], which is considered as one of the best eye tracking
copiesbearthisnoticeandthefullcitationonthefirstpage.Copyrights
glasses,canonlylastfor1.75hourswithanextendedbattery
forcomponentsofthisworkownedbyothersthantheauthor(s)must
behonored.Abstractingwithcreditispermitted.Tocopyotherwise,or capacityof3400mAh.WhenusingthebatteryofaGoogle
republish,topostonserversortoredistributetolists,requirespriorspecific Glass(570mAh),thiseyetrackingsystemcanonlylast18
permissionand/orafee.Requestpermissionsfrompermissions@acm.org. minutes.Thelimitedtrackingtimehashindereditsabilityto
ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA collectgazepointdataineverydaylife,whichcanbehighly
Â©2024Copyrightheldbytheowner/author(s).Publicationrightslicensed
informativeformanyapplications,suchas,monitoringusersâ€™
toACM.
mental or physical health conditions [48, 58], gaze-based
ACMISBN979-8-4007-0489-5/24/09...$15.00
https://doi.org/10.1145/3636534.3649376 input,andattentionandinterestanalysis[15].
4202
beF
22
]CH.sc[
1v43641.2042:viXraACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Figure1:EchoProfilesofDifferentMicrophoneswhenMovingGazetoDifferentRegionsofTheScreen.
Toovercomethischallenge,weintroduceGazeTrak,which anaveragetrackingaccuracyof4.9Â°forcross-sessionscenar-
exploresutilizingacousticsensing(knownforrelativelylow iosand3.6Â°forin-sessionscenarioswitharefreshingrateof
power, lightweight, and affordable) to continuously track 83.3Hz.Wemadeademovideo1todemonstratethetracking
gazepointsonaglassframe.Itssensingprincipleisbased performanceandreal-worldapplicationsofoursystem.
onthefactthateyeballsarenotperfectlysphericalandro- Althoughthecurrentaccuracyofoursystemwasworse
tatingthemwouldexposedifferentshapesandstretchthe thancommercialeyetrackerssuchasTobiiProGlasses3[2]
skinaroundthemwithuniqueformations.Thiscanprovide andPupilLabsGlasses[28],itisstillcomparabletosome
highlyvaluableinformationforinferringgazepoints.Gaze- webcam-basedeye-trackingsystems[21,53].Furthermore,
Trakusesonespeakerandfourmicrophonesoneachside duetothelow-powerfeatureofacousticsensors,GazeTrak,
oftheglassframe.Thespeakeremitsfrequency-modulated including the data collection system, has a relatively low
continuous-wave(FMCW)acousticsignalswiththefrequency powersignatureof287.9mW.Comparedtocamera-based
above18kHztowardstheeyeballs.Themicrophonescapture wearableeyetrackingsystems,ourproposedsystemreduces
thesignalsreflectedbytheeyeballsandtheirsurroundingar- thepowerconsumptionbyover95%.Ifusingabatterywith
eas,whichareusedtoprocessandcalculatetheechoprofiles. thecapacitysimilartoTobiiProGlasses3,oursystemcanex-
Theseechoprofilesarefedtoacustomizeddeeplearning tendtheusagetimefrom1.75hoursto38.5hours.Itcaneven
algorithmbasedonResNet-18topredictthegazepoint. last6.4hoursonthebatteryofnormalsmartglasses,suchas
Weconductedtworoundsofuserstudiestoevaluatethe GoogleGlass.Thepowersignatureofoursystemcanbefur-
performanceofGazeTrak.Duringthestudies,eachpartici- therimprovedusingarecentlyintroducedmicro-controller
pantwasaskedtolookatandfollowtheinstructionpoints withalow-powerCNNaccelerator(MAX78002).Hence,we
on the screen. In the first round of the study, 12 partici- implementedourgazetrackingpipelinefullyonMAX78002.
pantsevaluatedourfirsthardwareprototype(showninFig.3 Withtherefreshratesetas30Hz,thepowerconsumption
(f)),wherethemicrophonesandspeakersweregluedona ofthewholesystemincludingthedatapreprocessingand
glassframe.Theaveragecross-sessiontrackingaccuracywas modelinferenceismeasuredas95.4mW.
4.9Â°.Itconfirmedtheoptimalsettingsofthesensingsystem, Insummary,thecontributionsofourpaperareasfollows:
whichhelpedusdesignthefinalprototype.Thefinalproto-
â€¢ Wedesignedandimplementedthefirstacoustic-based
type(asshowninFig.3(g))featuresamorecompactform
continuouseyetrackingsystemonglasses.
factor,significantlylowersignalstrength,andimprovedenvi-
â€¢ Auserstudywith20participantsshowedanaverage
ronmentalsustainability.Toensureconsistentperformance
cross-sessionaccuracyof4.9Â°witharefreshingrate
betweenthetwoprototypes,weconductedasecondroundof
upto83.3Hzandapowersignatureof287.9mW.
studywith10participants,includingsomenewparticipants,
evaluatingthefinalprototype.Thefinalprototypeachieved
1https://youtu.be/XvNLNkfQY7QGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
â€¢ Theperformanceofthesystemremainedrobustunder Theaforementionedfrontalcamera-basedtechnologies
differentnoisyenvironmentsandwithdifferentstyles aremostlylocatedatfixedpositionsanddonotworkwell
ofglassframes. whileusersmovetoanotherpositionorarewalkingaround.
â€¢ Areal-timepipelinewasimplementedonMAX78002 In order to allow some mobility for users while they are
to make inferences on the board with a power con- usingtheeyetrackingtechnologies,manyresearchersin-
sumptionof95.4mWat30Hz. vestigatedutilizingthecamerasonmobiledevicestotrack
eyemovements,suchasmobilephones[25,30,67]ortablets
2 RELATEDWORK [7,25].However,theseeyetrackingtechnologiesbasedon
2.1 Webcam-basedEyeTrackingSystems mobiledevicesstillrequireuserstoholdthemobiledevicesin
frontoftheirfaceallthetimeandcannotprovidecompletely
Webcamshavebeenwidelyusedtoimplementeyetracking
hands-freeandmotion-freeexperiencesforusers.
technologiesbecauseofitsubiquityoncomputersandits
advantageoflow-cost.Researchershavedonemanyworkto
explorethepotentialofwebcamsforeyetracking[46,47,52, 2.3 WearableEyeTrackingTechnologies
62].Currently,thereareplentyofwebcam-basedeyetrack-
Toovercomethechallengesthatnon-wearableeyetracking
ingplatformsthatareavailableonline,suchasRealEye.io
technologiesfaceasdescribedinthelasttwosubsections,
[53],GazeRecorder[19],andWebGazer.js[21].
manywearableeyetrackingtechnologiesbasedoncameras
Thesewebcam-basedeyetrackingplatformsprovideaf-
[12,24,29,39,40,42,51,66],opticalsensors[8,34,35,56,57],
fordablesolutionsforeyetrackingwithacceptabletracking
acousticsensors[20],magneticsensors[61],Electrooculog-
performanceforeverydayusers.However,thepositionof
raphy(EOG)sensors[9,10],orinertialmeasurementunits
webcams are usually fixed and they have a relatively low
(IMU)[29]havebeendeployedondifferentkindsofwear-
resolution.Therefore,theirperformancecanbemoreeas-
ablesincludingglasses[8,12,20,35,39,40,51,56,57,66],
ilyimpactedbyfactorslikelightingconditions,occlusions,
goggles[9,10],hat[29],andhead-mounteddevices[24,34,
cameraorientations,etc.
42,61].Amongallthesewearableeyetrackingtechnologies,
camera-basedonesusuallyoutperformothersintermsof
2.2 OtherNon-wearableEyeTracking
trackingperformanceanddonotrequirelotsofcalibration
Technologies
datafromnewusers.Manywearableeyetrackersusingcam-
Inordertoprovidemoreaccurateandreliablesolutionsto eras,especiallyonglasses,havebecomecommercialandcan
eyetrackingandmakethemmoreapplicabletoassortedsce- beusedasareliablewaytotrackeyemovementscontinu-
narios,researchershaveputlotsofeffortsinimplementing ously,suchasTobiiProGlasses3[2],PupilLabs(Invisible,
other non-wearable eye tracking technologies other than Core,VR/ARadd-ons)[28],DikablisGlasses3[16],andSMI
webcam-based systems, most of which are based on cam- EyeTrackingGlasses[23].Withthesetechnologies,various
eraswithahigherresolutionthanwebcams.Frontalcamera- novel gaze-based applications have been enabled, includ-
basedeyetrackingtechnologiesbasedoncomputervision ingdetectionofeyecontacts[63],interactionwithdevices
techniques can take full advantage of the whole facial in- [26,38,45],monitoringmentalhealth[31,58],etc.
formationoftheuserforeyetracking,whichusuallyleads Despiteofthepromisingtrackingperformance,current
tohightrackingaccuracy.Differentkindsofcamerashave solutionstowearableeyetrackingsystemsstillhavesome
beenusedintrackingeyemovements,suchasRGBcameras limitations. First of all, many eye tracking systems above
[3], infrared (IR) cameras [44], and thermal cameras [59]. canonlyrecognizediscretegestures[9,10,56,57,66],limit-
Beyondusingjustonecamera,manytechnologiesadopted ingtheirperformanceinapplicationsthatneedcontinuous
multiplecamerasintheireyetrackingsystemsinorderto trackingoftheeyes.Camera-basedwearableeyetrackerscan
improvetheperformanceindifferentperspectivesincluding providehighaccuracyincontinuouseyetracking,butcam-
providinglargertrackingcoverage[4],allowingforusermo- erasareusuallypower-hungry,whichmakesthemrelatively
tion[22]andtrackingeyemovementsofmultipleusers[36]. impracticalwhiledeployedinwearablesthatneedtobeworn
Becauseofthereliabletrackingperformanceandreasonable ineverydaysettings.Toaddressthisissue,[39]and[40]pro-
calibrationtimeneeded,frontalcamera-basedeyetracking posedlow-powersolutionstotrackinggazepositionswith
technologieshavebeenwellcommercialized,amongwhich camerasonglasseswhilemaintainingpromisingaccuracies.
TobiiProFusion[1]isoneofthebestdesktopeyetrackers Despiteoftheimpressiveperformance,changinglighting
becauseitonlyrequiressecondsofcalibrationprocessfor conditionscanstillbeaproblemforcamera-basedsystems,
newusersandcanprovideatrackingaccuracyaslowas0.3Â° as[40]demonstratedthattheperformancebecameworse
inoptimalconditions.Asaresult,thisproducthasbeenused inanoutdoorsetting.Besides,commercialeyetrackersare
asreferenceinmanyresearchprojects. usuallyexpensiveanddonotprovideopen-sourcesoftwareACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Table1:GazeTrakandOtherContinuousEyeTrackingTechniques.ThepowerofGazeTrak(Teensy4.1)doesnot
includedatapreprocessinganddeeplearninginference.Thereportedaccuracyistestedwithinthesamesession
withoutusersremountingthedevice.Bothweightandcostincludetherecordingunit.NS=NotSpecified.
Reference FormFactor Sensors Power Accuracy RefreshRate Weight Cost
Choetal.[12] Glasses Cameras >7W 0.79Â° NS NS NS
Ryanetal.[51] Glasses Cameras >1.6W 2Â° NS NS ~$700
iShadow[39] Glasses Cameras 0.07W 3Â° 30Hz NS NS
CIDER[40] Glasses Cameras 0.032W 0.6Â° 250Hz NS NS
PupilLabsGlasses[28] Glasses Cameras 8.6W 0.6Â° 30/60/120Hz 202.75g $2,849
TobiiProGlasses3[2] Glasses Cameras 10.7W 0.6Â° 50/100Hz 388.5g $16,055
SMIGlasses[23] Glasses Cameras NS 0.5Â° 60/120Hz NS $41,000
Lietal.[35] Glasses NIRLED&Photodiodes 395ÂµW <2Â° 120Hz <25g NS
Lietal.[34] Head-mounted Photodiodes 791ÂµW 6.3Â° 10Hz NS NS
GazeRecorder[19] Webcam Camera / 1.05Â° 30Hz / $500/month
WebGazer.js[21] Webcam Camera / 4.17Â° NS / Free
RealEye.io[53] Webcam Camera / ~5Â° 60Hz / $600/month
GazeTrak(Teensy4.1) Glasses AcousticSensors 0.288W 3.6Â° 83.3Hz 44.2g ~$75
GazeTrak(MAX78002) Glasses AcousticSensors 0.095W 4.2Â° 30Hz / /
forusers,preventingthemfrombeingeasilyaccessedand 3 PRINCIPLEANDALGORITHMS
adaptedbygeneralusers. Activeacousticsensingisbasedonaffordablesensors(speak-
Recently,Lietal.[35]proposedalow-costandbattery- ersandmicrophones),thesizesofwhicharerelativelysmall.
freesolutiontocontinuouseyetrackingusingnearinfrared Previousresearchworkhasprovedthatitisabletoprovide
emitters and receivers on glasses. It achieves competitive enoughinformationtotracksubtleskindeformationssuch
performancebuttheystatedinthepaperthatthissystem asfacialexpressions[18,33].Inthissection,wediscusshow
canbeimpactedbydirectsunlightandglassesmovement,i.e. thisapproachcanbeadaptedtoeyetracking.
theremountingoftheglasses.Besides,thisworktracksthe
positionandsizeofthepupilsowecannotdirectlycompare
3.1 FMCW-basedActiveAcousticSensing
ittooursystem.Afterconversion,itstrackingaccuracyof
Inordertocapturetheformationaroundeyeballs,weuse
gazepositionsissmallerthan2Â°inangularerror.Another
FMCW-basedacousticsensing,whichhasbeenwidelyproven
systemusingsimilartechnologyfromthesamegroup[34]
effectivetoestimatedistanceandmovementsfromcomplex
tracksgazepositionswithanaccuracyof6.3Â°,worsethan
environments[41,60].
thetrackingaccuracyofoursystemat4.9Â°.Golardetal.[20]
conductedamodelingandempiricalstudytoprovethatul-
3.1.1 EncodedFMCWSignals. WhilecustomizingtheFMCW
trasoundcanprovidealow-power,fastandlight-insensitive
signalsforoursystem,threemainfeaturesaretakenintoac-
alternativeforcamera-basedeyetrackingtechnologies.How-
count:1)Operatingfrequencyrange:Thedeviceisexpectedto
ever,itwasevaluatedonaphysical3Dmodelofahuman
bewornbyusersforalongperiodoftimeintheireveryday
eyeandusedtime-of-flightestimatedfromacousticsignals
lives.Asaresult,theFMCWsignalsneedtobetransmittedin
andnotclearhowitcanapplyonarealuser.
theinaudiblefrequencyrange.Besides,toensuretheencoded
Tothebestofourknowledge,GazeTrakisthefirstwear-
signalsareminimallyimpactedbythenoiseintheenviron-
able sensing technology based on active acoustic sensing
ment,theoperatingfrequencyrangewepickshouldalsobe
thatcantrackgazepointscontinuously.Wesummarizedand
uncommonindailysettings;2)Samplingrate:Toachievea
compared GazeTrak with some aforementioned wearable
reasonablespatialandtemporalresolutionoftrackingeye
andwebcam-basedeyetrackingtechniquesthatcancontin-
movements, the sampling rate of FMCW signals must be
uouslytrackgazepositionsinTab.1.Thesetechniquesare
highenough;3)Gain:Aspowersignatureincreaseswiththe
thosethataremostrelatedtooursystem.Pleasenotethat
signalgain,thesignalgainshouldbeproperlydetermined
commercialeyetrackingwearables[2,23,28]usuallyhave
tobalancesignalstrengthandpowerconsumption.
camera(s)recordingthevideooftheenvironmentaswellso
Considering all the factors above, we set the operating
wecanonlyroughlycomparethemtoourdeviceinterms
frequencyrangeoftheFMCWsignalsthatweemitinthe
ofpowerandweight.
GazeTraksystemabove18kHz,becausethisrangeisnear-
inaudibleanduncommoninthesoundsgeneratedbynormal
human activities. Because both eyes contain informationGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
Figure2:OverviewoftheGazeTrakSystem:UsetheSpeakerontheRightSide(18-21kHz)forIllustration.
while moving, we placed one speaker on each side of the 3.2 MachineLearningAlgorithms
glassframe.Wesetthespeakerontherightsidetooperateat
3.2.1 GroundTruthAcquisitionwithoutusingEyeTrackers.
18-21kHzwhiletheoneontheleftsideoperatesat21.5-24.5 Aprofessionaleye-tracker(e.g.,TobiiProFusion)canpro-
kHztomakesuretheydonotinterferewitheachother.To videhighlyaccurategroundtruth,butitisexpensive.Ifour
guaranteethatthesystemworksreliablyinthesefrequency systemneedsaprofessionaleye-trackertotrainthesystem,
ranges,wesettheADCsamplingrateas50kHzwiththe itwillmakeoureye-trackingsystemlessaccessible.
framelengthofFMCWsignalsas600samples.Thisgives Therefore,wedevelopedanewgroundtruthacquisition
thesystemarefreshrateofeyetrackingat83.3Hz(50000 andcalibrationsystemthatonlyneedsaprogramrunning
samples/sÃ·600samples).Webelievearefreshrateof83.3 onalaptop.Theprogramgeneratesinstructionpointson
Hzissufficienttoprovidecontinuousgazetrackingsince thescreenasthegroundtruth.Duringdatacollection,the
the frame rate of most videos are 30 Hz or 60 Hz. Lastly, usersonlyneedtolookatandfollowthemovementsofthe
thegainwasexperimentallyadjustedtomakesurethatthe instructionpoints.Thesegroundtruthdataalongwiththe
signaldoesnotsaturatethemicrophoneswhilethepower echoprofilesarefedintothemachinelearningmodelfor
consumptionisrelativelylow. training.Thismethodisgenerallyapplicableonanydevice
withascreen.Fordetailsabouthowtheinstructionpoints
aregenerated,pleaserefertoSec.5.Tobettercompareour
systemwithcommercialeyetrackers,wealsouseaTobii
3.1.2 AcousticPatternsforContinuousEyeTracking. After Pro Fusion (120 Hz) [1] to record the eye movements to
receivingthereflectedFMCWsignals,wefirstapplyaBut-
demonstratetheeffectivenessofourtrainingmethods.
terworthband-passfilterwithacut-offfrequencyrangeof
18-21kHzor21.5-24.5kHzonthesignaltoremovethesig- 3.2.2 Deep Learning Model. We developed a customized
nalsinthefrequencyrangethatwearenotinterestedin.It deep-learningpipelinetolearntheechoprofilescalculated
alsohelpsprotecttheprivacyofusersbecauseweremove on the received signals. Because in the echo profiles (See
theaudiblerangeofthesignals.Thenwefurtherprocessthe Fig.1),thetemporalinformationhasbeenconvertedtothe
filteredsignaltoobtainuniqueacousticpatterns.According spatialinformationonanimage,wedecidedtouseResNet-
topriorresearchwork[32,33,37,54,60,64,65],EchoProfile 18astheencoderofourdeeplearningmodelbecauseCNN
providesanaccuratedepictionofthestatusandmovements networksareknowntobegoodatextractingfeaturesfrom
ofthereflectingobjectsintheenvironment.Asaresult,in images.Thenafully-connectednetworkisusedasadecoder
thispaper,wealsouseechoprofilesastheacousticpatterns topredictgazepositionsbasedonthefeaturesextractedfrom
thatoursystemmonitors.AsshowninFig.2(a)-(c),echo theimages.
profile is obtained by continuously calculating the cross- Becauseofthelimiteddistancebetweenthesensorson
correlation between the received signals and transmitted theglassesandtheeyes,weareonlyinterestedinacertain
signals.Fig.1demonstratesthatdifferenteyefixationsand rangeoftheechoprofiles(Fig.2(c)).Asaresult,wecropthe
movements are correlated with different patterns in echo echoprofilesofeachchanneltogetthecenter70pixels(23.8
profiles.Basedontheseobservationsabove,webelievethat cm)vertically.Thenwerandomlycrop60pixels(20.4cm)
ourGazeTraksystemutilizingFMCW-basedactiveacoustic outofthese70pixelsfordataaugmentationpurposetomake
sensingisabletotrackeyemovementscontinuouslywith surethesystemwillnotbeseverelyimpactedbythevertical
highaccuracy. shiftingcausedbyremountingthedevice.TocontinuouslyACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Figure 3: Hardware and Form Factor for GazeTrak: (a) Speaker board; (b) Microphone board (front view); (c)
Microphoneboard(backview);(d)CustomizedPCBboardfortheaudiochipNXPSGTL5000;(e)Teensy4.1;(f)
Glassesformfactorwithspeakersandmicrophonesattached(M1-8:microphones,S1-2:speakers);(g)Attachable
andmorecompactprototype;(h)MAX78002EvaluationKit.
trackthegazepositions,weapplyaslidingwindowof0.3 4 DESIGNANDIMPLEMENTATION
secondsontheechoprofiles.Asaresult,thedimensionof 4.1 HardwareDesign
theechoprofilethatweinputintothedeeplearningmodel
InordertoimplementtheFMCW-basedactiveacousticsens-
foronechannelis26(0.3sÃ—50000HzÃ·600samples+1)Ã—
ing technique mentioned in the section above, we chose
60(pixels).Becauseweuse2speakersand8microphonesin
Teensy4.1[50]asthemicro-controllertoprovidereliable
oursystem,whichwillbeillustratedinSubsec.4.2,wecrop
FMCWsignalgenerationandreceiving.WedesignedaPCB
outthesamedimensionofechoprofilesforall2Ã—8=16
boardtosupporttwoSGTL5000chipswhicharethesameas
channels,makingthedimensionoftheinputvectortothe
theoneontheTeensyaudioshield[49].Withthiscustomized
deeplearningmodelas26Ã—60Ã—16.
PCBboardpluggedontoTeensy,itcansupportasmanyas
Weusetheinstructionpointsasthelabels(seeSubsubsec.
8microphonesand2speakers.Wechosethespeakercalled
3.2.1)andthemeansquarederror(MSE)asthelossfunction.
OWR-05049T-38D[14]andtheMEMSmicrophonecalled
WechoseAdamoptimizerandsetthelearningrateas0.01.
ICS-43434[55]tosupportsignaltransmissionandreception.
Themodelistrainedfor30epochstogettheestimationof
WealsobuiltcustomizedPCBboardsforthespeakerandthe
thetwogazecoordinates(x,y).
microphonetomakethemassmallaspossible.Weusedthe
Inter-ICSound(I2S)busesontheTeensy4.1totransmitdata
3.2.3 EvaluationMetrics. Thepredictionofoursystemis
betweentheTeensy4.1andtheSGTL5000chips,speakers
thecoordinate(x,y)ofourestimatedgazepositiononthe
andmicrophones.ThecollecteddataisstoredintheSDcard
screeninpixels.ToevaluatetheaccuracyofGazeTrak,we
onTeensy4.1.Fig.3(a)-(e)showthesecomponents.
adoptedtheaccuracydefinedinCOGAINeyetrackeraccu-
racytermsanddefinitions[11].Theevaluationmetricwe
useinoursystemisthemeangazeangularerror(MGAE)
4.2 FormFactorDesign
betweenthecoordinateofourprediction(x,y)andthatof
thegroundtruth(xâ€™,yâ€™).TocalculateMGAEindegreesfrom Wedesignedthefirstformfactorusingacommodityglass
thecoordinates,wefirstneedtogettheangularerrorðœƒ be- frame.Weglued1speakerand4microphonestoeachinner
tweenthepredictionandthegroundtruthofeachdatapoint. sideofapairoflight-weightglasses.Thespeakersandmi-
ðœƒ canbecalculatedusingthelawofcosinesinatriangleas crophonesaresymmetricallyplacedontheglasses,asshown
follows: inFig.3(f).
Basedontheexperiencewelearnedduringtheiteration
process,therearethreekeyfactorswetookintoconsider-
ð‘‘2 +ð‘‘2 âˆ’ð‘‘2 ationwhiledesigningthefinalformfactorofGazeTrak:1)
ð‘’ð‘” ð‘’ð‘ ð‘”ð‘
ðœƒ =arccos( )Ã—180Ã·ðœ‹ (1) Type of glass frame: We started designing the form factor
2Ã—ð‘‘ ð‘’ð‘”Ã—ð‘‘
ð‘’ð‘ withalargeglassframebecausewebelieveithasmoreroom
forustoplacesensors.However,thelargertheglassframeis,
whereð‘‘ ,ð‘‘ andð‘‘ arethedistancebetweenuserâ€™seyes theeasieritwillbefortheframetotouchtheskin,blocking
ð‘’ð‘” ð‘’ð‘ ð‘”ð‘
andgroundtruth,thedistancebetweenuserâ€™seyesandpre- thesignaltransmissionandreception.Asaresult,wefinally
diction,andthedistancebetweengroundtruthandpredic- pickedarelativelysmallglassframewithanosepadthat
tionrespectively.MGAEisobtainedbyaveragingðœƒ overall can support the glass frame to a higher position. Besides,
thedatapointsinthetestingdataset. thelight-weightglassesminimizethepressureattachedonGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
theuserâ€™snose,makingitmorecomfortabletowear;2)Sen- toreachthisgoal,wecarefullydesignedtheinstructionvideo
sor position: The speakers and microphones on two sides forparticipantsâ€™gazetofollow.Basically,onthewhitescreen,
are symmetric because we believe the movements of two therewouldbeonereddotmovingaroundandweaskedpar-
eyesareusuallysynchronized.Oneachside,weplacethe ticipantstostareatthepointandfollowitwiththeireyes.We
speakerontheframeoftheglassesnexttotheoutercanthi dividedthescreeninto100regions.Foreachdatapoint,the
becauseitiseasierforthespeakerstotouchtheskinifthey instructionpointappearedatarandompositionwithinone
areplacedabovethecheekbonesornexttotheeyebrows, randomregion.Theinstructionpointwouldmovequickly
consideringtheirheight.Themicrophonesarescatteredon tothatrandompositionandstaystaticatthatpositionfora
theframeasfarawayfromeachotheraspossibletocapture certainperiodoftimebecausewemainlywouldliketotest
moreinformationbyreceivingsignalstravellingindifferent howGazeTrakperformstotrackthefixationofparticipants.
paths.Thesensorsareattachedasfarawayfromthecenter Wesuccessfullyrecruited20participants(10femalesand
ofthelensesaspossibleinordertoavoidblockingtheview 10males,22yearsoldonaverage).Notethatsomepartic-
oftheuser;3)Stability:Wefoundthatthestabilityofthe ipants participated in the study for multiple times to test
deviceseverelyimpactstheperformanceofoursystemes- different settings. The study was conducted in an experi-
peciallywhenusersneedtoremountthedevicefrequently. mentroomonauniversitycampus.Duringthestudy,the
Theanti-slipperynosepadpreventstheglassesfromsliding participantssitonachairandputontheglassesformfactor
downtheuserâ€™snose.Furthermore,weaddedtwoearloops with our GazeTrak system. For each participant, we pro-
attheendofthelegsoftheglassframe.Theygreatlyhelps duced12sessionsofinstructionpoints.Duringtheinterval
fixtheglassespositionfrombehindearsandimprovesthe between sessions, participants were instructed to remove
performanceofthesystem.Finally,wemadetheformfactor thedevice,placeitonthetable,andthenputitbackon.This
asshowninFig.3(f). stepwastakentodemonstratethatoursystemcontinuedto
functioncorrectlyevenafterthedevicewasremounted.In
4.3 FinalHardwarePrototype eachsession,theinstructionpointmovedtoallthe100pre-
definedregionsinarandomorder.Thedurationforwhich
Theprototypeaboveissuitableforinitialtestingandcompar-
theinstructionpointremainedateachpositionvariedfrom
isonofdifferentconfigurations.However,oncethedesignof
0.5to3.5seconds,withanaverageof2seconds.Asaresult,
theprototypeisfinalized,weaimtocreateamorecompact
theaveragelengthofeachinstructionsessionwas200sec-
andlessobtrusiveformfactorthatissuitableforeveryday
onds.Beforeeachsession,therewasa15-secondcalibration
use by users. To achieve this, we have designed two PCB
processwiththeinstructionpointmovingtothefourcorners
boards,eachcontainingonespeakerandfourmicrophones
ofthescreenandthecenterofthescreen.
onboard,whichcanbeattachedtoonesideoftheglasses.
Thefullstudytooknomorethan1.5hoursforeachpartic-
WehavealsodeployedtheTeensy4.1andthePCBboard
ipant,duringwhichwecollectedapproximately40minutes
with SGTL5000 chips directly onto one leg of the glasses.
of data (200 seconds Ã— 12 sessions). Upon completing the
To connect the micro-controller and the customized PCB
studytasks,theparticipantwasaskedtocompleteaques-
boards,wehaveusedflexibleprintedcircuit(FPC)cables.
tionnairetocollecttheirdemographicinformationandtheir
Thesystemhasaninterfacethatallowsittobepoweredby
feedbackusingthissystem.
aLi-Pobattery.ThecompactprototypeisshowninFig.3(g),
andFig.1showsauserwearingtheprototype.Webelieve
6 EVALUATIONRESULTS
that this prototype can be easily adapted and attached to
differenttypesofglasses. Inthissection,wefirstevaluatedtheperformanceofGaze-
We have measured the weight of the prototype, and it Trakwiththeinitialprototype,comparingdifferentground
carriesatotalweightof44.2grams,includingtheglasses, truthacquisitionmethods,sensorconfigurationsandamounts
Teensy4.1,PCBboards,andtheLi-Pobattery.Comparedto oftrainingdata.Thenwetestedoursystemundernoisyenvi-
camera-basedeyetrackingglasses,ourGazeTrakdeviceis ronmentsandonglassesofvariousframestyles.Finally,we
muchlighter.Forexample,TobiiProGlasses3[2]weigh76.5 optimizedthesystemonthefinalprototypeandevaluatedit
gramsfortheglassesand312gramsfortherecordingunit. withanotherstudy,withpowerconsumptionmeasured.
Ourdevicehasasignificantadvantageovercamera-based
eyetrackingglassesintermsofweight. 6.1 User-dependentModel
WefirsttestedoursystemusingthefirstprototypeinFig.3(f)
5 USERSTUDYPROCEDURE
with12participants.Auser-dependentmodelwasapplied
Theobjectiveofouruserstudyistovalidatetheperformance to train a separate model for each participant. Among 12
ofGazeTrakoncontinuouslytrackinggazepoints.Inorder sessions we collected for each participant, we conductedACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
Table2:StudyResultsforDifferentMicConfigurations.
MicConfiguration M1+M5 M2+M6 M3+M7 M4+M8 Best4Mics(M2,4,6,8) Best6Mics(M1,2,4,5,6,8) All8Mics
MGAE 7.7Â° 7.2Â° 8.5Â° 6.9Â° 5.9Â° 5.5Â° 4.9Â°
a 6-fold cross validation to test the tracking performance thatTobiiProFusioncantrackthegazepointswithanav-
of our system by using 10 sessions (33.3 minutes) of data erageaccuracyof1.9Â°duringthecalibrationprocessforall
fortrainingand2sessions(6.7minutes)ofdatafortesting. participants.Weplottedthetrackingperformanceofboth
UsingtheevaluationmetricsdefinedinSubsubsec.3.2.3,we GazeTrakandTobiiforallparticipantsinFig.4.
calculatedthemeangazeangularerror(MGAE)indegree
forallparticipants,andtheresultweobtainedwas5.9Â°.To 6.2 ImpactofSensorConfigurations
furtherimprovetheperformance,weadoptedthe15-second
Inthissubsection,weevaluatedtheimpactofthenumber
calibrationdatabeforeeachsessiontofine-tunethemodel,
andplacementofmicrophonesontrackingperformanceto
which resulted in an improved performance of 4.9Â°. It is
determinetheoptimalsensorpositionforthebestresults.
worthmentioningthatasimilarcalibrationprocessisalso
Weassessedfourdifferentsettings:1)onemicrophoneon
requiredforcommodityeyetrackers(e.g.,TobiiPro).The
eachside(leftandright);2)twomicrophonesoneachside;
distancebetweentheparticipantsâ€™eyesandthescreencenter
3) three microphones on each side and 4) all four micro-
ismeasuredtobearound60cmsowehaveafieldofview
phonesoneachside.Inthefirstsetting,wecomparedthe
of60Â°(thelargestpossibleangularerror)inthisstudy.We
performanceusingdatafromfoursetsofmicrophoneset-
madeademovideoshowinghowourpredictionlookslike
tings(M1+M5,M2+M6,M3+M7,andM4+M8inFig.3(f)),
visuallywiththislevelofaccuracy.
whichispresentedinTab.2.Thefindingsdemonstratethat
theM4+M8pairofmicrophonesprovidesthebesttracking
performanceamongthefourpairstested.Weconducteda
one-wayrepeatedmeasuresANOVAtestontheresultsof
thefoursettingsandidentifiedastatisticallysignificantdif-
ference (ð¹(3,44) = 6.74,ð‘ = 0.001 < 0.05). These results
indicatethatmicrophoneplacementcanaffectgazetracking
performance,possiblyduetodifferencesinsignalreflection
beforearrivingatdifferentmicrophones.
We further conducted experiments to evaluate perfor-
manceusingdifferentcombinationsofmicrophonesunder
settings 2 and 3. The results showed that the best perfor-
mance was 5.9 degrees and 5.5 degrees, respectively. We
Figure4:MGAEDistributionacrossParticipants. alsoranaone-wayrepeatedmeasuresANOVAtestamong
theresultsofthesefoursettingsusingdatafrom12partici-
pants.Theresultsshowedastatisticallysignificantdifference
Next, we aimed to compare the impact on gaze track-
(ð¹(3,44) = 51.61,ð‘ = 0.00001 < 0.05).Thesefindingssug-
ingperformanceofusingdifferentgroundtruthacquisition
gestthatoursystemrequiresfourmicrophonesoneachside
methods:acommodityeyetracker(TobiiProFusion)versus
(eightmicrophonesintotal)toachievethebestperformance.
ourmethod(usinginstructionpointsonthescreen).Weused
theeyetrackingdatarecordedbyTobiiProFusionasthe
6.3 ImpactofBlinking
groundtruthtotrainthemodel,andtheMGAEafterfine-
tuningwas4.9Â°.Weconductedarepeatedmeasurest-test Blinkingcanintroducenoiseinourhighly-sensitiveacoustic
betweentheresultsusingTobiidataasthegroundtruthand sensingsystemasitcanleadtorelativelylargemovements
thoseusinginstructionpointsasthegroundtruthforall12 aroundtheeye.Weconductedanevaluationtodetermine
participants,anddidnotfindastatisticallysignificantdiffer- whetherblinkingaffectsthetrackingperformanceofoursys-
ence(ð‘ =0.92>0.05).Thissuggeststhatusinginstruction tem.Forthisevaluation,weselecteddatafromthreepartici-
points on a screen monitor as the ground truth can be as pantswiththebest,worst,andaveragetrackingperformance
effectiveasusingTobiidata. (P1, P2, P10). We removed the data where the participant
Apartfromthat,wealsorecordedtheeyetrackingaccu- blinks(about10%oftotaldata)basedonthegroundtruth
racyofTobiiProFusionitselfwhichwasreportedafterthe dataobtainedfromTobiiEyeTracker.Wethenusedthepro-
calibrationprocessoftheTobiiplatform.Theresultsshowed cessed data to retrain the user-dependent model for eachGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
participant.Ourresultsshowedthattheperformancedidnot 4)drivingnoise(65.6dB(A))recordedwhiledrivingavehi-
improveafterremovingtheblinkingdata.Onepossiblerea- cle.Afteroverlayingeachofthesefournoises,thetracking
sonforthisresultisthattheblinkingpatternsareconsistent performanceremainedunchangedforeveryparticipant.
andcanbelearnedbythemachinelearningmodel.There-
fore,ourfindingssuggestthatblinkingdoesnotsignificantly 6.5.2 Real-worldNoisyEnvironments. Inthesecondexperi-
ment,weinvitedeightparticipantsfromtheprevioususer
impacttheperformanceofoursystem.
studyandrecruitedtwonewparticipants(P13andP14)to
testourdeviceindifferentreal-worldnoisyenvironments.
6.4 User-adaptiveModel Sincethisstudyrequiredustomovetodifferentenviron-
To reduce the need of providing lots of training data for ments,thestudydesigndifferedslightlyfromtheprevious
a new user, we employed a three-step process to train a studydescribedinSec.5.
user-adaptivemodel.Firstly,wetrainedalargebasemodel Inthisstudy,weusedanAppleMacBookProwitha13.3-
usingdatafromallparticipantsexcepttheonebeingtested. inchdisplaytoplaytheinstructionvideos.Weusedthein-
Secondly,wefine-tunedthemodelusingthetrainingdata structionpointsasthegroundtruth.TheMacBookProwas
collectedfromthecurrentparticipant.Notably,theuseronly placedonamovabletable,andparticipantswereinstructed
needstoprovidetrainingdataonceduringtheinitialsystem tositinfrontofthetabletoconductthestudy.Additionally,
use.Finally,atthebeginningofeachsession,wefurtherfine- accordingtoSubsec.6.4,6sessionsoftrainingdataaresuffi-
tunedthemodelusingcalibrationdatacollectedfromthe cienttoprovideacceptabletrackingperformance.Therefore,
participantbeforetestingorusingthesystem.Todetermine foreachparticipant,wecollectedatotalof8sessionsofdata
theamountofdatarequiredtoachievecompetitivetracking inaquietexperimentroom,with6sessionsfortrainingand
performance,wereservedtwosessionsofdatafortestingand 2sessionsfortesting.Wethencollectedadditionaltesting
usedvaryingamountsoftrainingdatafromtheparticipant data under two different noisy environments. In the first
tofine-tunethelargemodel. environment,participantsusedoursystemwhileweplayed
Theresultsshowthatanewuseronlyneedstoprovide randommusicfor2sessions.Inthesecondenvironment,we
sixsessionsoftrainingdata(approximately20minutes)to collected2sessionsoftestingdataatacampuscafewhere
achievegoodperformance.Collectingmoredatadoesnot staffandpeopleweretalkingaroundduringbusinesshours.
necessarilyresultinbetterperformance.Additionally,with The noise levels under each environment were measured
onlytwoorthreesessionsofdata(approximately6minutes), using the CDC NIOSH app: 1) quiet room (33.8 dB(A)); 2)
thesystemcanachieveaperformanceof6.7Â°and6.1Â°,respec- playmusic(64.0dB(A));3)inthecafe(56.6dB(A)).Thisstudy
tively.Ifnouserdataiscollected,theperformanceis11.3Â°. designledtoatotalof12sessionsofdatacollectionforeach
Thisislikelybecausedifferentpeoplehaveuniquehead,face, participant,whichisthesameasthepreviousstudy.
andeyeshapes.Therefore,tofurtherreducetheamountof Wetrainedapersonalizedmodelforeachparticipantusing
trainingdatarequiredfromeachnewuser,wemayneedto 6sessionsofdatacollectedinthequietroom.Then,the2
collectasignificantlylargeramountoftrainingdatafroma testingsessionscollectedineachscenariowereusedtotest
morediversesetofparticipants. theperformance ofoursystem indifferent environments.
Theaveragegazetrackingperformanceofoursystemacross
10participantsremainedconsistentat3.8Â°and4.8Â°undertwo
6.5 ImpactofEnvironmentNoise
noisyenvironments,playingmusicandinthecafe,whilethe
Toensurethatouracousticsensingsystemisresistantto performanceinthequietroomwas4.6Â°.Overall,theaverage
differenttypesofenvironmentalnoise,weconductedtwo accuracyofgazetrackingdidnotchangesignificantlywith
experimentsasdescribedinthissubsection. thepresenceofnoiseintheenvironment.Weconducteda
one-wayrepeatedmeasuresANOVAtestamongtheresults
ofthesethreescenariosforall10participantsanddidnot
6.5.1 NoiseInjection. Inthefirstexperiment,werecorded
noisesindifferentenvironmentsusingthemicrophoneson findastatisticallysignificantdifference(ð¹(2,27) =2.46,ð‘ =
0.11 > 0.05). This again validates that our system is not
ourglassframe.Wethenoverlaidthenoiseontothedata
easilyaffectedbyenvironmentalnoise.
collectedintheuserstudytosimulatedifferentnoisyenvi-
ronments.Werecordedthenoiseinfourdifferentenviron-
6.6 ImpactofDifferentGlassFrames
mentsandmeasuredtheaveragenoiselevelsusingasound
levelmeterappcalledNIOSHprovidedbyCDC[17]:1)street Inouruserstudy,weonlytestedoursystemononeglass
noise(70.8dB(A)) recordedonthestreetnearacrossroad; frame(F1).However,webelieveourGazeTraksystemcanbe
2) music noise (64.5 dB(A)) recorded while playing music easilyappliedtoglasseswithdifferentframestyles.Inorder
onacomputer;3)cafenoise(54.5dB(A))recordedinacafe; tovalidatethisassumption,wedeployedoursystemontwoACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
otherpairsofglassesasshowninFig.5.Theoriginalglass 6.7 EvaluationontheFinalPrototype
frame in Fig. 5 (a) is frameless and relatively lightweight. Intheprevioususerstudies,weevaluatedGazeTrakwith
In this study, we applied our system on two new glasses variousconfigurationsunderdifferentscenarios,usingthe
with different styles, size and weight. The first new glass initialprototypethatwehaddeveloped.Theresultsofthese
frame (small glasses, F2) has a smaller size than F1 but a studieshelpedusconfirmtheprototypesettingsanddevelop
largerweightduetotheframearoundthelens(Fig.5(b)). anoptimizedsystemprototype,whichfeaturesamorecom-
Thesecondnewglassframe(largeglasses,F3)withaframe pactformfactorasshowninFig.3(g).Inthissubsection,
aroundlens(Fig.5(c))hasamuchlargersizeandweight ourobjectivewastoassesstheperformanceandpowercon-
comparedtoF1andF2.Toevaluateoursystemonthesenew sumptionofthisfinalprototype.
glassframes,threeparticipantsfromtheoriginalstudy(P1,
P5andP7)agreedtoparticipateinthisadditionalstudy.The 6.7.1 GazeTrackingAccuracy. Toevaluatethefinalproto-
type,werecruited10participants(fourofwhomparticipated
studysetupsandprocedureswereexactlythesameasthe
inthepreviousstudy).Thestudydesignwassimilartothe
previousstudydescribedinSec.5.
previousstudy,exceptthatweonlyusedinstructionpoints
asthegroundtruthacquisitionmethod.Eachparticipantcol-
lectedeightsessionsofdata(sixsessionsfortrainingandtwo
fortesting).Wereducedthesignalstrengthfromthespeaker
to20%oftheoriginalsetup,aswefoundthatevenwith2%of
theoriginalstrength,theperformancewassimilarinthepi-
lotstudy.Hence,thisfinalprototypehassignificantlylower
signalstrengthandimprovedenvironmentalsustainability.
Additionally,wesettheCPUspeedoftheTeensy4.1to150
(a)OriginalGlasses(F1) (b)SmallGlasses(F2) MHzinthisstudy(standardspeed:600MHz)tolowerpower
consumption.Withthissetting,thesystemexperienceda
datalossrateof0.002%,andtheperformanceofoursystem
wasnotaffectedbythisloss,asshowninTab.3.Apartfrom
thecross-sessionperformance,wealsoconductedatestof
thein-sessiontrackingaccuracyinwhichthetrainingdata
andtestingdataweresplitfromthesamesessionswithout
remountingthedevicetoshowtheoptimalperformanceof
(c)LargeGlasses(F3) oursystem.
Figure5:GazeTrakDeployedonGlasseswithVarious Table3:GazeTrackingPerformanceinMGAEwiththe
FrameStyles. FinalPrototype.
Settings P1 P2 P7 P10 P15 P16 P17 P18 P19 P20 Avg
Wecollected12sessionsofdataforeachparticipanttesting
Cross-session 4.4Â° 4.9Â° 5.9Â° 8.0Â° 3.6Â° 3.6Â° 3.9Â° 4.8Â° 4.7Â° 5.7Â° 4.9Â°
eachglassframe.SinceSubsec.6.4indicatesthat6sessionsof
In-session 3.9Â° 3.6Â° 4.9Â° 5.3Â° 1.8Â° 2.6Â° 2.6Â° 3.6Â° 3.1Â° 4.9Â° 3.6Â°
trainingdataissufficient,wedumpedthefirst4sessionsfor
eachglassframeandusedthelast8sessionstoruna4-fold AsshowninTab.3,themeangazeangularerror(MGAE)
crossvalidationinordertotestthetrackingperformanceof is4.9Â°forthecross-sessionevaluation,whichissimilarto
oursystemondifferentglasses.Inthiscase,wecanmake the previous study. When evaluating the performance of
sure that participants are familiar with the wearing of all GazeTrakwithinthesamesessions,theaccuracyimproves
theglassframesandeliminatetheimpactofsomerandom to3.6Â°.Wedidnotaddearloopstothisprototypebecausethe
factors.Theevaluationresultshowsthatthesmallglasses legsoftheglasseswerewiderthantheearloopswehad.For
(F2)yieldedasimilaraverageperformancetotheoriginal mostparticipants,theglassesfitwellontheirears,butone
glasses(F1)(bothat5.3Â°),whilethelargeglasses(F3)resulted participant(P10)reportedthattheglasseskeptslidingdown
inarelativelypooreraverageperformance(at6.1Â°),witha duringthestudy,whichmayhaveaffectedtheirperformance.
dropinperformanceof15%.Onepossiblereasonfortheper- Basedonthequestionnaires,noparticipantreportedbeing
formancedifferenceisthatthesensorsonthelargerglasses able to hear the signal emitted from our system. We also
weremuchclosertotheskin.Sometimes,thesensorsmay measuredthesignallevelfromoursystemusingtheNIOSH
directlytouchtheskin,whichcouldblockthetransmission app.Weplacedthephonerunningtheappcloseenoughto
andreceptionofsignals,asweexplainedinSubsec.4.2. thespeakersinoursystemandtheappgaveusanaverageGazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
signallevelof43.1dB(A).Thisisbelowthemaximumallow- 7.1 MLModels
abledailynoiserecommendedbyCDC,whichis85dB(A) Toachievethisgoal,thedeeplearningmodelsweretrained
overeighthoursintheworkspace[43]. andsynthesizedinadvance,usingtheai8xlibraries[5].We
implementedtwomodelswithai8x,whichwereResNet-18
6.7.2 Power Consumption. We measured the power con- (usedinthepreviousstudy)andMobileNetforcomparison.
sumption of our system with a current ranger [27]. The
DuetothehardwarelimitofMAX78002,wemodifiedthe
averagecurrentflowingthroughthesystemwasmeasured
models to be compatible with the chip. Specifically, for a
as88.3mA@3.26V,whichgivesusapowerconsumption
Conv2dlayer,thekernelsizecouldonlybesetto1x1or3x3
of287.9mW.Thisvaluewastestedwithall8microphones
andthestrideisfixedto[1,1].Inaddition,someconvolu-
and 2 speakers working, and with the data being written
tionlayersofResNet-18weresubstitutedwithdepthwise
intotheSDcard.Oursystemcanlastupto38.5hourswitha
separable convolution layers to avoid exceeding the limit
batteryofsimilarcapacitytoTobiiProGlasses3(3400mAh),
of the number of parameters in the model. Furthermore,
whiletheworkingtimeofTobiiProGlasses3isonly1.75
wequantizedtheinputandtheweightsofthemodelswith
hours. If applied to non-eye-tracking glasses, like Google
ai8x,whichconvertedthemallinto8-bitdataformattosave
Glass,oursystemcanrunfor6.4hours.Itisworthnoting
memoryforstorageandincreasethespeedofinference.
thattheseestimatesdonotincludethepowerconsumption
ofdatapreprocessinganddeeplearninginferencerunning
onalocalserver.Wemeasuredthepowerconsumptionof
differentcomponentsinoursystem(Tab.4).Teensy4.1has 7.2 DataPreprocessing
ahighbasepowerconsumption,whilethesensors(speakers Before the deep learning model, we also need to apply a
andmicrophones)consumemuchlesspower. band-passfilteronthereceivedsignalsandperformcross-
correlationbetweenreceivedsignalsandtransmittedsignals
toobtainechoprofilesasdescribedinSubsubsec.3.1.2.Inthe
Table4:PowerConsumptionofDifferentComponents
implementationonMAX78002,weremovedtheband-pass
onTeensy4.1.Powerofdatapreprocessinganddeep
filtersinceallthecomputationsaredoneontheMCUandwe
learninginferenceisNOTincluded.
donothavetouseaband-passfiltertofilterouttheaudible
rangeofsignalstoprotectusersâ€™privacy.Besides,thishelps
Total Speakers&Mics SDcardwriting Otheroperations
reducetheprocessingtime.
287.9mW 16.4mW 72.7mW 198.8mW Thenweexperimentedtwodifferentmethodstorealize
thecross-correlation:(1)bruteforcetocalculateechoprofiles
pointbypoint;(2)thedotproductfunctionintheCMSIS-DSP
library.Resultsofstandardtests[6]revealedthatittookthe
6.7.3 Usability. Aftertheuserstudy,wedistributedaques-
system178.3msand45.4mstocomputeoneechoframeand
tionnaire to every participant to ask for feedback on our
makeoneinferencewiththesetwomethodsutilizedrespec-
prototype.First,theparticipantsevaluatedtheoverallcom-
tively.Consideringthatoneframeofouraudiodatacomes
fortablenessandtheweightoftheprototypewitharating
from0to5.Acrossall10participants,theaveragescoresthey
every12msinoursystem(600samplesÃ·50000samples/s),
thisprocessingtimeistoolongtokeepoursystemrunning
gavetothesetwoaspectsare4.5(std=0.7)and4.2(std=0.8),
in real-time with an FPS of 83.3 Hz. Finally, we explored
indicatingthatGazeTrakisoverallcomfortabletowearand
method(3)aConv2dlayer(kernelsize1x1)withtransmitted
easytouse.Furthermore,all10participantsanswered"No"
signalsastheuntrainedweightsandreceivedsignalsplaced
tothequestion"Canyouhearthesoundemittedfromour
alongthechannelaxisoftheinput.Thiscanincreasethe
system?",verifyingtheinaudibilityoftheacousticsignals
speedofechoprofilecalculationbecauseitusestheCNN
fromtheGazeTraksystem.
acceleratoronMAX78002.Wecompressedthesamplesused
forcross-correlationfrom600x600to34x34andthepixels
7 DATAPREPROCESSINGANDMODEL
ofinterestfrom60pixels(20.4cm)to30pixels(10.2cm)in
INFERENCEONMAX78002
thiscasetofurtherdecreasetheprocessingtime.
In the previous evaluation, we recorded audio data with WiththisConv2dlayeraddedontopofthedeeplearning
Teensy 4.1 first and run the signal processing and deep model,themodeldirectlytakestherawaudiodataasinputin
learningpipelineonalocalserveroffline.Toenablepredic- instanceswiththesizeof64(34+30samples)x26(frames)x
tionsofgazepositionsinreal-timeonanMCU,wedeployed 8(microphones).Thismethodallowsthesystemtomakeone
thewholepipelineonamicro-controllerwithanultra-low- inferencewithin10.3ms,whichisenoughforthereal-time
powerCNNaccelerator(MAX78002[13]). pipelinewithadouble-buffermethodapplied(DMAmovesACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
thecurrentframeinonebufferwhiletheCPUprocessesthe IfwecanuseMAX78002todirectlycontrolspeakersand
previousframeinanotherbuffer). microphonesinfuture,wewillbeabletooptimizethepower
efficiencyandkeeptheoverallpowerconsumptionofour
7.3 AccuracyandRefreshRate real-timesystemaround95.4mW,i.e.,79.0mW(MAX78002
Tovalidatethesemodificationsandcompression,weeval- withResNet-18runningat30HZ)+16.4mW(2speakersand
uatedthein-sessionperformanceofdifferentmodelswith 8microphones).Weadmitthatthisisjustanestimateofthe
differentsettingsusingdatacollectedwiththefinalproto- powerofthisreal-timesystemandthepowerconsumption
typeinSubsec.6.7andshowedtheresultsinTab.5. ofMAX78002mightincreaseifitdoesneedtocontrolthe
sensorsbutwedonotexpectittobeveryhighbecausethe
current power of MAX78002 already includes that of the
Table5:AverageIn-sessionPerformanceacross10Par-
CPUandtheCNNacceleratorrunningatfullspeed.
ticipantswithDifferentModelsandSettings.
Table6:PowerConsumptionofMAX78002withDif-
Models MLLibraries Compressed? Quantized? MGAE ferentModelsRunning.
pytorch âœ— âœ— 3.6Â°
ai8x âœ— âœ— 4.0Â°
ResNet-18 Models ResNet-18 MobileNet
ai8x âœ“ âœ— 4.0Â°
ai8x âœ“ âœ“ 4.2Â° FPS(Hz) 83.3 30 83.3 30
ai8x âœ“ âœ— 4.2Â°
MobileNet Power(mW) 96.9 79.0 86.0 75.7
ai8x âœ“ âœ“ 4.3Â°
Asshowninthetable,thesamemodeltrainedwithai8x 8 DISCUSSION
isslightlyworsethanthattrainedwithPyTorchgiventhe
8.1 EvaluatingSimplerRegressionModels
constraintsoftheconvolutionlayersdiscussedabove.Com-
pressingthesizeofinputdatadoesnotaffecttheaccuracy. We adopted two traditional regression models, which are
WhileMobileNetyieldscomparableaccuracytoResNet-18, linearregression(LR)andgradientboostedregressiontrees
bothmodelssufferaslightperformancedropafterquantiza- (GBRT),topredictgazepositionsusingthedatacollectedin
tionsincetheprecisionofdataisdecreased. Subsec.6.7andtheresultsshowedthattheaveragein-session
GiventhelimitationoftheI2SinterfacesonMAX78002, trackingaccuracyforthesetwomodelsacross10participants
totestoursysteminamorerealisticcondition,westilluse are11.6Â°and6.8Â°respectively.Comparedtotheresultsin
Teensy 4.1 to control the speakers and microphones and Tab.3,thetraditionalregressionmodelsoutputmuchworse
transfer the received audio data to MAX78002 via the se- accuraciesthanResNet-18(3.6Â°).Weconductedananalysis
rial port. This generates a steady stream of audio data to oftheimpurity-basedfeatureimportancewithGBRT,com-
MAX78002similartothestreamitwouldreceiveifitcould paringthefeaturesindifferentchannelsofmicrophonesin
accommodate8microphonesbyitself.Infuture,wewillex- Tab.7.Itturnsoutthatthechannelsreceivingsignalsfrom
ploreconnectingmicrophonesdirectlytoMAX78002using 18-21kHz(S1)aregenerallymoreimportantthanchannels
multi-channelaudioprotocolssuchasTime-divisionMulti- receivingsignalsfrom21.5-24.5kHz(S2).Furthermore,the
plexing(TDM).EvaluationresultsshowedthatforResNet- microphonesthatareclosertotheinnercornersoftheeyes
18andMobileNet,MAX78002spent124.1msand41.6ms (M1,M4,M5M8)aremoreimportantthanthosecloserto
respectivelyloadingtheweightsofthemodel.Thisisaone- thetailsoftheeyes(M2,M3,M6,M7).
time effort and can be done before running the real-time
Table7:FeatureImportanceAnalysisforDifferentMi-
pipeline.Thenittook12mstoloadoneinstanceandmake
crophonesusingGBRT(Scaledto0-100).
aninferencebasedonitforbothResNet-18andMobileNet,
givingarefreshrateof83.3Hz.
Microphones M1 M2 M3 M4 M5 M6 M7 M8
Importance(S1/S2) 100/27 57/23 33/12 96/29 66/81 42/17 28/15 85/79
7.4 PowerConsumption
WemeasuredthepowerconsumptionoftheMAX78002eval-
8.2 ImpactofReal-worldFactors
uationkitwhileitmadeinferences.Tab.6demonstratesthat
MAX78002consumes96.9mWand86.0mWrespectively 8.2.1 HeadMovements. Intheuserstudy,wedidnotuse
whenmakinginferenceswithResNet-18andMobileNetat achinresttofixtheparticipantsâ€™headsotheycouldturn
83.3Hz.Therefreshratecanbereducedto30Hztosave theirheadfreely.However,webelievethatathorougheval-
power,whichisenoughformanyapplications.Inthiscase, uationonhowheadmovementsaffectsystemperformance
thepowerbecomes79.0mWand75.7mWrespectively. ispreferredinfuturework.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
8.2.2 Near-andFar-sighted. Intheuserstudy,wecollected forusers.However,Subsec.6.1showsthatthetrackingaccu-
participantsâ€™degreesofmyopiainthequestionnaires,which racywithoutfine-tuningisstillacceptable,at5.9Â°,compared
showednoconnectiontothegazetrackingperformance. totheaccuracyachievedwithfine-tuning(4.9Â°).
8.2.3 UserSpeaking. Oneresearcherevaluatedoursystem 8.4.3 Reducing Training Effort. Subsec. 6.4 suggests that
whenkeepingsilentandkeepingtalkingtohimself.Thegaze GazeTrakachievessatisfactoryperformanceonnewusers
trackingperformanceofthesilentsessionsandthetalking withapproximately20minutesoftrainingdatausingthe
sessionsisthesameat3.9Â°. user-adaptivemodel.Thistrainingeffortcanbefurtherre-
ducedbyconstructingalargerandmorediversedatasetfrom
muchmoreparticipantstotrainthebasedmodel.Moreover,
8.3 PotentialApplications
dataaugmentationmethods,suchasincludingsimulation
The goal of this paper is to demonstrate the feasibility of datatotrainthemodel,canbeexploredaswell.
our new acoustic-based gaze tracking system on glasses.
While our eye tracking accuracy of 4.9Â° is comparable to 8.4.4 TowardsaMoreIntegratedSystem. InSec.7,westill
somewebcam-basedmethods,itislowerthancommercial usedTeensy4.1tocontrolthespeakersandmicrophones,
eyetrackers(1.9Â°inourstudy).Therefore,oursystemmay andtransferaudiodatatotheMCUMAX78002.Infuture,
notbeimmediatelyapplicabletosomeapplicationsrequiring weplantofurthercustomizeourownPCBsforMAX78002
highlypreciseeyetracking.However,oursystemcanstillbe toallowittodirectlycontrolspeakersandmicrophones.We
usedinmanyapplications,suchasinteractionwithinterface believethatthepowerconsumptionofourreal-timesystem
elementslikebuttonsinAR,thatdonotrequireveryhigh canbefurtherreducedinthiscasebecauseTeensy4.1with
accuracyeyetrackers. ahighbasepowercanberemoved.Furthermore,wedonot
Oursystemcanalsobepotentiallyusedintrackingirreg- expectthatthepowerconsumptionofMAX78002willbe
ular eye movements, enabling healthcare applications for significantlyincreasedsincetheon-boardCPUandCNNac-
monitoringusersâ€™healthconditionsineverydaylife.This celeratorofMAX78002werealreadyoperatingatmaximum
requiresmonitoringthegazemovementsthroughouttheday speedinourcurrentevaluation.Withasolidsystemimple-
foranalysisineverydaylife,insteadofjusttrackingtheirac- mentation,weplantocarryoutanextensiveevaluationof
curategazepositionsforafewhoursinacontrolledsettings. thismoreintegratedsysteminfutureworktovalidateour
The low-power and lightweight features of our GazeTrak speculation.
systemmakeitagoodcandidatesolutiontoenablingava-
rietyofapplicationsthatcamera-basedeyetrackerscannot 9 CONCLUSION
realize,bycontinuouslyunderstandingusergazemovements
Inthispaper,wepresentthefirstacoustic-basedeyetrack-
inthewildforextendedperiods.Furthermore,oursystem
ingglassescapableofcontinuousgazetracking.Thestudy
canalleviatetheprivacyconcernfromusersascomparedto
involving20participantsconfirmsthatoursystemcanac-
camera-basedmethods.
curatelytrackgazepointscontinuously,achievinganaccu-
racyof3.6Â°withinthesamesessionand4.9Â°acrossdifferent
8.4 LimitationandFutureWork sessions.Whencomparedtocommercialcamera-basedeye
trackingglassessuchasTobiiProGlasses3,oursystemre-
8.4.1 ImprovingthePerformance. Thereisroomforfurther
ducespowerconsumptionby95%.Areal-timepipelineis
improvements of the performance of our system. For in-
implementedonMAX78002tomakeinferenceswithapower
stance,wecanapplycalibrationprocessontheoutputofthe
signatureof95.4mWat30Hz.
systemtofurtherenhanceperformance.Weexperimented
with affine transformation and projective transformation
totransformtheoutputbuttheydidnotimmediatelyim- ACKNOWLEDGMENTS
provetheperformance.Accordingtotheanalysisoftheerror ThisworkissupportedbyNationalScienceFoundation(NSF)
distributionoftheeyetrackingresults,webelievethisisbe- underGrantNo.2239569,NSFâ€™sInnovationCorps(I-Corps)
causetheerrordistributionisnotlinearinoursystemsowe underGrantNo.2346817,theIgniteProgramandtheAnn
needtoexploremorenon-lineartransformationmethodsto S.BowersCollegeofComputingandInformationScience
improvetheperformance. atCornellUniversity.TheauthorswouldliketothankProf.
SusanFussellandherlabforsharingtheaccesstothecom-
8.4.2 CalibrationProcessforFine-tuning. Oursystemcur- mercial eye tracker Tobii Pro Fusion with us. We also ap-
rentlyrequiresa15-secondcalibrationprocessbeforeeach preciatethehelpofaCornellstudent,VipinGunda,onthe
sessiontofine-tunethemodel,whichmaybeinconvenient developmentofthepipelineonMAX78002.ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
REFERENCES
[17] CentersforDiseaseControlandPrevention(CDC).2023. NIOSH
[1] TobiiAB.2022. TobiiProFusion. RetrievedSept.13,2022from SoundLevelMeterApp. RetrievedMar13,2023fromhttps://www.
https://www.tobiipro.com/product-listing/fusion/ cdc.gov/niosh/topics/noise/app.html
[2] TobiiAB.2022. TobiiProGlasses3. RetrievedSept.13,2022from [18] YangGao,YinchengJin,SeokminChoi,JiyangLi,JunjiePan,LinShu,
https://www.tobiipro.com/product-listing/tobii-pro-glasses-3/ ChiZhou,andZhanpengJin.2022.SonicFace:TrackingFacialExpres-
[3] Artsiom Ablavatski, Andrey Vakunov, Ivan Grishchenko, Karthik sionsUsingaCommodityMicrophoneArray.Proc.ACMInteract.Mob.
Raveendran,andMatsveiZhdanovich.2020.Real-timePupilTracking WearableUbiquitousTechnol.5,4,Article156(dec2022),33pages.
fromMonocularVideoforDigitalPuppetry. CoRRabs/2006.11341 https://doi.org/10.1145/3494988
(2020).arXiv:2006.11341 https://arxiv.org/abs/2006.11341 [19] GazeRecorder.2022.GazeRecorderWebcamEyeTracking. Retrieved
[4] ChristerAhlstromandTaniaDukic.2010.ComparisonofEyeTrack- Sept.13,2022fromhttps://gazerecorder.com/webcam-eye-tracking-
ing Systems with One and Three Cameras. In Proceedings of the accuracy/
InternationalConferenceonMethodsandTechniquesinBehavioral [20] AndreGolardandSachinSTalathi.2021. UltrasoundforGazeEs-
Research(MB).Article3,4pages. https://doi.org/10.1145/1931344. timationâ€”AModelingandEmpiricalStudy. Sensors21,13(2021),
1931347 4502.
[5] AnalogDevicesAI.2022.MaximIntegratedAI. RetrievedAug23,2023 [21] BrownHCIGroup.2021. WebGazer.js:DemocratizingWebcamEye
fromhttps://github.com/MaximIntegratedAI Tracking on the Browser. Retrieved Sept. 13, 2022 from https:
[6] Analog Devices AI. 2023. MAX7800x Power Monitor and //webgazer.cs.brown.edu/#publication
Energy Benchmarking Guide. Retrieved Aug 23, 2023 from [22] CraigHennesseyandJacobFiset.2012. Longrangeeyetracking:
https://github.com/MaximIntegratedAI/MaximAI_Documentation/ bringing eye tracking into the living room. In Proceedings of the
blob/master/Guides/MAX7800x%20Power%20Monitor%20and% SymposiumonEyeTrackingResearchandApplications.249â€“252.
20Energy%20Benchmarking%20Guide.md [23] iMotions.2022. SMIEyeTrackingGlasses. RetrievedSept.13,2022
[7] TanyaBafna,PerBÃ¦kgaard,andJohnPaulinPaulinHansen.2021.Eye- fromhttps://imotions.com/hardware/smi-eye-tracking-glasses/
Tell:Tablet-basedCalibration-freeEye-typingusingSmooth-pursuit [24] MoritzKassner,WilliamPatera,andAndreasBulling.2014.Pupil:an
movements. In ACM Symposium on Eye Tracking Research and opensourceplatformforpervasiveeyetrackingandmobilegaze-
Applications.1â€“6. based interaction. In Proceedings of the 2014 ACM international
[8] FrankHBorsatoandCarlosHMorimoto.2016.Episcleralsurfacetrack- joint conference on pervasive and ubiquitous computing: Adjunct
ing:challengesandpossibilitiesforusingmicesensorsforwearable publication.1151â€“1160.
eyetracking.InProceedingsoftheNinthBiennialACMSymposium [25] KyleKrafka,AdityaKhosla,PetrKellnhofer,HariniKannan,Suchendra
onEyeTrackingResearch&Applications.39â€“46. Bhandarkar,WojciechMatusik,andAntonioTorralba.2016.EyeTrack-
[9] AndreasBulling,DanielRoggen,andGerhardTrÃ¶ster.2008. Itâ€™sin ingforEveryone.InProceedingsoftheIEEEConferenceonComputer
youreyes:Towardscontext-awarenessandmobileHCIusingwearable VisionandPatternRecognition(CVPR).
EOGgoggles.InProceedingsofthe10thinternationalconferenceon [26] MikkoKytÃ¶,BarrettEns,ThammathipPiumsomboon,GunA.Lee,and
Ubiquitouscomputing.84â€“93. MarkBillinghurst.2018. Pinpointing:PreciseHead-andEye-Based
[10] AndreasBulling,DanielRoggen,andGerhardTrÃ¶ster.2009.Wearable TargetSelectionforAugmentedReality.InProceedingsoftheCHI
EOGGoggles:Eye-BasedInteractioninEverydayEnvironments.In ConferenceonHumanFactorsinComputingSystems.1â€“14. https:
CHIExtendedAbstractsonHumanFactorsinComputingSystems. //doi.org/10.1145/3173574.3173655
3259â€“3264. https://doi.org/10.1145/1520340.1520468 [27] LowPowerLab.2018.IntroductiontoCurrentRanger. RetrievedMar
[11] Communication by Gaze Interaction Association. 2010. 13,2023fromhttps://lowpowerlab.com/guide/currentranger/
Woking copy of definitions and terminology for Eye Tracker [28] PupilLabs.2022.PupilInvisible. RetrievedSept.13,2022fromhttps:
accuracy and precision. Retrieved Sept. 13, 2022 from //pupil-labs.com/products/
http://old.cogain.org/forums/eye-tracker-accuracy-and-precision- [29] AntonioLanata,GaetanoValenza,AlbertoGreco,andEnzoPasquale
general-discussion/eye-tracker-accuracy-terms-and-definiti.html Scilingo.2015.Robustheadmountedwearableeyetrackingsystemfor
[12] ChulWooCho,JiWooLee,KwangYongShin,EuiChulLee,KangRy- dynamicalcalibration.JournalofEyeMovementResearch8,5(2015).
oungPark,HeekyungLee,andJihunCha.2012. GazeDetectionby [30] YaxiongLei.2021. EyeTrackingCalibrationonMobileDevices.In
WearableEye-TrackingandNIRLED-BasedHead-TrackingDevice ACMSymposiumonEyeTrackingResearchandApplications(ETRA
BasedonSVR.EtriJournal34,4(2012),542â€“552. Adjunct).Article4,4pages. https://doi.org/10.1145/3450341.3457989
[13] Analog Devices. 2022. MAX78002 Evaluation Kit. Retrieved [31] JueLi,HengLi,WaleedUmer,HongweiWang,XuejiaoXing,Shukai
Aug 23, 2023 from https://www.analog.com/media/en/technical- Zhao,andJunHou.2020.Identificationandclassificationofconstruc-
documentation/data-sheets/MAX78002EVKIT.pdf tionequipmentoperatorsâ€™mentalfatigueusingwearableeye-tracking
[14] DigiKey. 2023. OWR-05049T-38D. Retrieved Mar 13, 2023 from technology.AutomationinConstruction109(2020),103000.
https://www.digikey.com/en/products/detail/ole-wolff-electronics- [32] KeLi,RuidongZhang,SiyuanChen,BoaoChen,MoseSakashita,
inc/OWR-05049T-38D/13683703 FranÃ§ois GuimbretiÃ¨re, and Cheng Zhang. 2024. EyeEcho: Con-
[15] SiboDong,JustinGoldstein,andGraceHuiYang.2022.GazBy:Gaze- tinuous and Low-power Facial Expression Tracking on Glasses.
BasedBERTModeltoIncorporateHumanAttentioninNeuralInfor- arXiv:2402.12388[cs.HC]
mationRetrieval.182â€“192. https://doi.org/10.1145/3539813.3545129 [33] KeLi,RuidongZhang,BoLiang,FranÃ§oisGuimbretiÃ¨re,andCheng
[16] Ergoneers. 2022. Dikablis Glasses 3. Re- Zhang.2022.EarIO:ALow-powerAcousticSensingEarableforCon-
trieved Sept. 13, 2022 from https://www.ergoneers. tinuouslyTrackingDetailedFacialMovements. Proceedingsofthe
com/en/mobile-eye-tracker-dikablis-glasses-3/?gclid= ACMonInteractive,Mobile,WearableandUbiquitousTechnologies
CjwKCAiA9tyQBhAIEiwA6tdCrFd7F7xBwNa4XVP09wRHlBATh_ 6,2(2022),1â€“24.
jafRuH5ErUVdhJt5WVLK2_FdVs7RoCpK4QAvD_BwE [34] TianxingLi,QiangLiu,andXiaZhou.2017.Ultra-LowPowerGaze
TrackingforVirtualReality.InProceedingsoftheACMConference
onEmbeddedNetworkSensorSystems(SenSys).Article25,14pages.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame ACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA
https://doi.org/10.1145/3131672.3131682 andforthediagnosisofdevelopmentaldisordersinyoungchildren.
[35] TianxingLiandXiaZhou.2018.Battery-FreeEyeTrackeronGlasses. InIEEEInternationalSymposiumonRobotandHumanInteractive
In Proceedings of the Annual International Conference on Mobile Communication (RO-MAN). 594â€“598. https://doi.org/10.1109/
ComputingandNetworking(MobiCom).67â€“82. https://doi.org/10. ROMAN.2007.4415154
1145/3241539.3241578 [49] PJRC.2023. AudioAdaptorBoardsforTeensy3.xandTeensy4.x.
[36] BhanukaMahanama.2022.Multi-UserEye-Tracking.InSymposium RetrievedMar13,2023fromhttps://www.pjrc.com/store/teensy3_
on Eye Tracking Research and Applications (ETRA). Article 36, audio.html
3pages. https://doi.org/10.1145/3517031.3532197 [50] PJRC.2023.Teensy4.1DevelopmentBoard. RetrievedMar13,2023
[37] SaifMahmud,KeLi,GuilinHu,HaoChen,RichardJin,RuidongZhang, fromhttps://www.pjrc.com/store/teensy41.html
FranÃ§oisGuimbretiÃ¨re,andChengZhang.2023.PoseSonic:3DUpper [51] WayneJRyan,AndrewTDuchowski,andStanTBirchfield.2008.Lim-
BodyPoseEstimationThroughEgocentricAcousticSensingonSmart- bus/pupilswitchingforwearableeyetrackingundervariablelighting
glasses.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.7,3, conditions.InProceedingsofthe2008symposiumonEyetracking
Article111(sep2023),28pages. https://doi.org/10.1145/3610895 research&applications.61â€“64.
[38] MizukiMatsubara,JoachimFolz,TakumiToyama,MarcusLiwicki, [52] ShreshthSaxena,ElkeLange,andLaurenFink.2022. TowardsEffi-
Andreas Dengel, and Koichi Kise. 2015. Extraction of read text cientCalibrationforWebcamEye-TrackinginOnlineExperiments.
using a wearable eye tracker for automatic video annotation. In InSymposiumonEyeTrackingResearchandApplications(ETRA).
AdjunctProceedingsofthe2015ACMInternationalJointConference Article27,7pages. https://doi.org/10.1145/3517031.3529645
onPervasiveandUbiquitousComputingandProceedingsofthe2015 [53] RealEyesp.zo.o.2022. RealEyeWebcamEye-Tracking. Retrieved
ACMInternationalSymposiumonWearableComputers.849â€“854. Sept.13,2022fromhttps://www.realeye.io/
[39] Addison Mayberry, Pan Hu, Benjamin Marlin, Christopher Salt- [54] RujiaSun,XiaoheZhou,BenjaminSteeper,RuidongZhang,Sicheng
house, and Deepak Ganesan. 2014. IShadow: Design of a Wear- Yin,KeLi,ShengzhangWu,SamTilsen,FrancoisGuimbretiere,and
able, Real-Time Mobile Gaze Tracker. In Proceedings of the 12th ChengZhang.2023.EchoNose:SensingMouth,BreathingandTongue
Annual International Conference on Mobile Systems, Applications, GesturesinsideOralCavityusingaNon-contactNoseInterface.In
andServices(MobiSysâ€™14).82â€“94. https://doi.org/10.1145/2594368. Proceedingsofthe2023ACMInternationalSymposiumonWearable
2594388 Computers(Cancun,QuintanaRoo,Mexico)(ISWCâ€™23).Association
[40] AddisonMayberry,YaminTun,PanHu,DuncanSmith-Freedman, forComputingMachinery,NewYork,NY,USA,22â€“26. https://doi.
DeepakGanesan,BenjaminM.Marlin,andChristopherSalthouse. org/10.1145/3594738.3611358
2015. CIDER:EnablingRobustness-PowerTradeoffsonaCompu- [55] TDK.2023.ICS-43434. RetrievedMar13,2023fromhttps://invensense.
tational Eyeglass. In Proceedings of the 21st Annual International tdk.com/products/ics-43434/
Conference on Mobile Computing and Networking (MobiComâ€™15). [56] CihanTopal,AtakanDogan,andOmerNezihGerek.2008.Awearable
400â€“412. https://doi.org/10.1145/2789168.2790096 head-mountedsensor-basedapparatusforeyetrackingapplications.
[41] RajalakshmiNandakumar,ShyamnathGollakota,andNathanielWat- In2008IEEEConferenceonVirtualEnvironments,Human-Computer
son.2015. Contactlesssleepapneadetectiononsmartphones.In InterfacesandMeasurementSystems.IEEE,136â€“139.
Proceedingsofthe13thannualinternationalconferenceonmobile [57] CihanTopal,Ã–merNezihGerek,andAtakanDogË‡an.2008. Ahead-
systems,applications,andservices.45â€“57. mounted sensor-based eye tracking device: eye touch system. In
[42] BasilioNoris,Jean-BaptisteKeller,andAudeBillard.2011.Awearable Proceedings of the 2008 symposium on Eye tracking research &
gazetrackingsystemforchildreninunconstrainedenvironments. applications.87â€“90.
ComputerVisionandImageUnderstanding115,4(2011),476â€“486. [58] MÃ©LodieVidal,JaysonTurner,AndreasBulling,andHansGellersen.
[43] U.S.DepartmentofHealthandHumanServices.1998.Criteriafora 2012.WearableEyeTrackingforMentalHealthMonitoring.Computer
recommendedstandard:occupationalnoiseexposure.DHHS(NIOSH) Communications35,11(jun2012),1306â€“1311. https://doi.org/10.1016/
PublicationNo.98â€“126(1998). https://www.cdc.gov/niosh/docs/98- j.comcom.2011.11.002
126/ [59] QuanWang,LauraBoccanfuso,BeibinLi,AmyYeo-jinAhn,ClaireE.
[44] TakehikoOhno,NaokiMukawa,andShinjiroKawato.2003. Just Foster,MargaretP.Orr,BrianScassellati,andFrederickShic.2016.
blinkyoureyes:Ahead-freegazetrackingsystem.InCHIâ€™03extended ThermographicEyeTracking.InProceedingsoftheBiennialACM
abstractsonHumanfactorsincomputingsystems.950â€“957. SymposiumonEyeTrackingResearch&Applications.307â€“310. https:
[45] LucasPaletta,HelmutNeuschmied,MichaelSchwarz,GeraldLodron, //doi.org/10.1145/2857491.2857543
MartinPszeida,StefanLadstÃ¤tter,andPatrickLuley.2014. Smart- [60] TianbenWang,DaqingZhang,YuanqingZheng,TaoGu,Xingshe
phoneEyeTrackingToolbox:AccurateGazeRecoveryonMobileDis- Zhou,andBernadetteDorizzi.2018.C-FMCWbasedcontactlessres-
plays.InProceedingsoftheSymposiumonEyeTrackingResearch pirationdetectionusingacousticsignal.ProceedingsoftheACMon
andApplications(ETRA).367â€“68. https://doi.org/10.1145/2578153. Interactive,Mobile,WearableandUbiquitousTechnologies1,4(2018),
2628813 1â€“20.
[46] Alexandra Papoutsaki. 2015. Scalable Webcam Eye Tracking by [61] EricWhitmire,LauraTrutoiu,RobertCavin,DavidPerek,BrianScally,
LearningfromUserInteractions.InProceedingsoftheAnnualACM James Phillips, and Shwetak Patel. 2016. EyeContact: scleral coil
Conference Extended Abstracts on Human Factors in Computing eye tracking for virtual reality. In Proceedings of the 2016 ACM
Systems(CHIEA).219â€“222. https://doi.org/10.1145/2702613.2702627 InternationalSymposiumonWearableComputers.184â€“191.
[47] Alexandra Papoutsaki, James Laskey, and Jeff Huang. 2017. [62] KatarzynaWisiecka,KrzysztofKrejtz,IzabelaKrejtz,DamianSromek,
Searchgazer:Webcameyetrackingforremotestudiesofwebsearch.In AdamCellary,BeataLewandowska,andAndrewDuchowski.2022.
Proceedingsofthe2017conferenceonconferencehumaninformation ComparisonofWebcamandRemoteEyeTracking.InSymposiumon
interactionandretrieval.17â€“26. EyeTrackingResearchandApplications(ETRA).Article32,7pages.
[48] LorenzoPiccardi,BasilioNoris,OlivierBarbey,AudeBillard,Giusep- https://doi.org/10.1145/3517031.3529615
pinaSchiavone,FlavioKeller,andClaesvonHofsten.2007.WearCam: [63] ZhefanYe,YinLi,AlirezaFathi,YiHan,AgataRozga,GregoryD
A head mounted wireless camera for monitoring gaze attention Abowd,andJamesMRehg.2012.DetectingeyecontactusingwearableACMMobiComâ€™24,September30-October4,2024,WashingtonD.C.,DC,USA Lietal.
eye-trackingglasses.InProceedingsofthe2012ACMconferenceon on Human Factors in Computing Systems (Hamburg, Germany)
ubiquitouscomputing.699â€“704. (CHIâ€™23).AssociationforComputingMachinery,NewYork,NY,USA,
[64] RuidongZhang,HaoChen,DevanshAgarwal,RichardJin,KeLi, Article852,18pages. https://doi.org/10.1145/3544548.3580801
FranÃ§oisGuimbretiÃ¨re,andChengZhang.2023. HPSpeech:Silent [66] Yanxia Zhang, Andreas Bulling, and Hans Gellersen. 2011. Dis-
SpeechInterfaceforCommodityHeadphones.InProceedingsofthe criminationofgazedirectionsusinglow-leveleyeimagefeatures.
2023ACMInternationalSymposiumonWearableComputers(Can- InProceedingsofthe1stinternationalworkshoponpervasiveeye
cun,QuintanaRoo,Mexico)(ISWCâ€™23).AssociationforComputing tracking&mobileeye-basedinteraction.9â€“14.
Machinery, New York, NY, USA, 60â€“65. https://doi.org/10.1145/ [67] AnjieZhu,QianjingWei,YilinHu,ZhangweiZhang,andShiweiCheng.
3594738.3611365 2018. MobiET:ANewApproachtoEyeTrackingforMobileDe-
[65] Ruidong Zhang, Ke Li, Yihong Hao, Yufan Wang, Zhengnan Lai, vice.InProceedingsoftheACMInternationalJointConferenceand
FranÃ§oisGuimbretiÃ¨re,andChengZhang.2023.EchoSpeech:Continu- International Symposium on Pervasive and Ubiquitous Computing
ousSilentSpeechRecognitiononMinimally-obtrusiveEyewearPow- andWearableComputers(UbiComp).862â€“869. https://doi.org/10.
eredbyAcousticSensing.InProceedingsofthe2023CHIConference 1145/3267305.3274174