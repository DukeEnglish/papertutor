Knowledge Probing for Graph Representation Learning
MingyuZhaoâˆ— XingyuHuangâˆ— ZiyuLyuâ€ 
my.zhao1@siat.ac.cn xingyuhuang1998@outlook.com luziyucrystal@163.com
SchoolofCyberScienceand SchoolofCyberScienceand SchoolofCyberScienceand
Technology,ShenzhenCampusofSun Technology,ShenzhenCampusofSun Technology,SunYat-senUniversity
Yat-senUniversity.ShenzhenInstitute Yat-senUniversity.ShenzhenInstitute Shenzhen,Guangdong,China
ofAdvancedTechnology,Chinese ofAdvancedTechnology,Chinese
AcademyofSciences,Universityof AcademyofSciences.
ChineseAcademyofSciences. Shenzhen,Guangdong,China
Shenzhen,Guangdong,China
YanlinWang LixinCui LuBai
yanlin-wang@outlook.com cuilixin@cufe.edu.cn bailu@bnu.edu.cn
SunYat-senUniversity CentralUniversityofFinanceand BeijingNormalUniversity
Guangzhou,Guangdong,China Economics Beijing,China
Beijing,China
ABSTRACT KEYWORDS
Graphlearningmethodshavebeenextensivelyappliedindiverse GraphRepresentationLearning,KnowledgeProbing,Evaluation
applicationareas.However,whatkindofinherentgraphproper-
tiese.g.graphproximity,graphstructuralinformationhasbeen ACMReferenceFormat:
MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai.
encodedintographrepresentationlearningfordownstreamtasks
2018.KnowledgeProbingforGraphRepresentationLearning.InProceedings
isstillunder-explored.Inthispaper,weproposeanovelgraphprob-
ofMakesuretoenterthecorrectconferencetitlefromyourrightsconfirmation
ingframework(GraphProbe)toinvestigateandinterpretwhether
emai(Conferenceacronymâ€™XX).ACM,NewYork,NY,USA,11pages.https:
thefamilyofgraphlearningmethodshasencodeddifferentlevels
//doi.org/XXXXXXX.XXXXXXX
ofknowledgeingraphrepresentationlearning.Basedontheintrin-
sicpropertiesofgraphs,wedesignthreeprobestosystematically
investigatethegraphrepresentationlearningprocessfromdiffer- 1 INTRODUCTION
entperspectives,respectivelythenode-wiselevel,thepath-wise Graphsareaprevalentdatastructureandhavebeenbroadlyin
level,andthestructurallevel.Weconstructathoroughevaluation multiplefields[16].Forexample,socialnetworks[13,23],molecular
benchmarkwithninerepresentativegraphlearningmethodsfrom graphstructures,andbiologicalproteinnetworksareuniversally
randomwalkbasedapproaches,basicgraphneuralnetworksand modeledasgraphs[5].Inrecentdecades,alotofgraphrepresen-
self-supervisedgraphmethods,andprobethemonsixbenchmark tationslearningmethodshavebeendevised,rangingfrommatrix
datasetsfornodeclassification,linkpredictionandgraphclassifi- factorizationmethods[1,6],random-walkbasedalgorithms[14,24],
cation.TheexperimentalevaluationverifythatGraphProbecan tothepopularfamilyofgraphneuralnetworks(GNN)[10,15,18,
estimatethecapabilityofgraphrepresentationlearning.Remaking 22,33,35,39].Thegraphrepresentationlearningmethodshave
resultshavebeenconcluded:GCNandWeightedGCNmethodsare demonstrateddifferentperformanceontheclassicaldownstream
relativelyversatilemethodsachievingbetterresultswithrespect tasks,e.g.nodeclassification,linkpredictionandgraphclassifica-
todifferenttasks. tion.Andthediversegraphrepresentationlearningmethodshave
beenextensivelyappliedinmultipleapplicationareas,e.g.social
CCSCONCEPTS
networkanalysis[13,23],recommendersystem[17,18,29],and
â€¢Informationsystems;â€¢Computingmethodologiesâ†’Knowl- proteinclassification[5,9].
edgerepresentationandreasoning; Thosegraphrepresentationlearningmethodstendtolearna
mappingwhichembednodesor(sub)graphsintoalow-dimensional
âˆ—Bothauthorscontributedequallytothisresearch.
â€ Correspondingauthor. vectorsbyencodingrelationalinformationandstructuralinforma-
tion,andthelearnedembeddingsareusedforfurtherdownstream
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
tasks[27].However,thereisnostudytoinvestigateandexplain
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation whatkindsofgraphpropertieshavebeenactuallycodedinthe
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe learnedembeddingthroughdifferentgraphrepresentationlearning
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
methods.Itlacksasystematicalevaluationtoprobewhetherthe
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. graphinherentproperties(e.g.graphproximity,graphstructural
Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY information)areencodedintothelearnednodeandgraphrepre-
Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
sentationswiththepopularbutblack-boxgraphrepresentation
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/XXXXXXX.XXXXXXX learningmethods.
4202
guA
7
]GL.sc[
1v77830.8042:viXraConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Inthispaper,wedeviseasystematicgraphprobingbenchmark approximatewithnegativesampling.However,matrixfactorization
(GraphProbe)toinvestigatewhattypesofknowledgeareencoded andrandomwalkmethodsareshallowembeddingapproaches[16]
intographrepresentationlearningfor9representativegraphlearn- andtheycannotcapturestructuralsimilarity.Inrecentdecades,
ingmethodsfromdiversecategoriesincludingrandomwalkbased graphneuralnetworkshavebeenextensivelyproposedforgraph
methods,classicalgraphneuralnetworkslikegraphconvolution representationlearning[16].Thefamilyofgraphneuralnetworks
networks(GCN)andgraphattentionnetworks(GAT),unsupervised relyontheneighborhoodaggregationstrategy,andsolvethemain
graphlearningmethodsandweightedgraphlearningmethods.We limitationsofthedirectencodingmethodslikematrixfactorization
devisethreetypesofprobesatthreedifferentlevels,respectively andrandomwalkapproaches.Forexample,Kipfetal.[22]pro-
thenode-wise,path-wiseandstructure-wiselevels.First,the posedascalableapproach(GraphconvolutionNetwork,GCN)for
node-wiseprobeisproposedtoinvestigatewhetherthenode-wise semi-supervisedlearningongraph-structureddata,byutilizingan
influencesareencodedingraphrepresentationlearning,andtwo efficientlayer-wisepropagationrulethatisbasedonafirst-order
intrinsicnodecentralitymetrics(e.g.eigenvectorcentralityand approximationofspectralconvolutionsongraphs.GraphSAGE[15]
betweennesscentrality)areadoptedtomeasurenode-wiseinflu- wasageneralinductivegraphrepresentationlearningframework,
ences.Second,adistanceprobeisdesignedtoexplorewhetherthe andlearnedafunctionthatgeneratesembeddingsbysamplingand
path-wiseinformation(distancebetweentwonodes)canbeem- leveragednodefeatureinformationtoefficientlygeneratenode
beddedintorepresentationlearningofnodes,basedontheshortest embeddingsforpreviouslyunseendata.Inaddition,someunsuper-
pathsofapairofnodes.Third,weleverageWeisfeiler-Lehman visedgraphlearningframeworkshavebeendevised.Forexample,
kernelalgorithm[31]asstructuralevaluationmetricsanddevisea VariationalGraphAuto-Encoders(VGAE)proposedanunsuper-
parameter-freestructuralprobetointerpretwhetherthestructural visedlearningframeworkbasedonthevariationalauto-encoder.
informationisenoverviecodedintothelearnedgraphrepresenta- Basedontherecentcontrastivelearningtechniques,Youetal.[36]
tions.Weconductextensiveexperimentstoprobetheperformance proposedagraphcontrastivelearningframeworkforunsupervised
ofgraphrepresentationlearningon6benchmarkdatasetsfrom representationlearningofgraphdataandexploredfourtypesof
diversedomainsforthreetraditionaldownstreamtasks.Oursys- graphaugmentationstoincorporatevariouspriors.Althoughex-
tematicevaluationhasconcludedsomeremarkedresults.Themain tensivegraphrepresentationlearningmethodshavebeenproposed
contributionsofthispaperaresummarizedasfollows: andworkedonthetraditionaldownstreamtasks,nostudieshave
â€¢ Toourbestknowledge,ourproposedmethodisthefirsttime investigatedwhethertheinherentgraphstructuralandtopological
informationhasbeenencodedintographrepresentationlearning.
toexploreknowledge probingon varioustypesof graph
Inthispaper,weproposeagraphknowledgeprobingframeworkto
learning models across all classical downstream tasks. A
probethelatentknowledgewithingraphrepresentationlearning,
systematicgraphprobingframeworkisnovellyintroduced,
andanswerwhatkindofgraphinformationhavebeenencoded
in which three types of probes based on graph intrinsic
whenperformingdownstreamtasks.
propertiesaredevisedrespectivelyfromthenode-wise,path-
wiseandstructure-wiselevels.
â€¢ Anevaluationbenchmarkforgraphrepresentationlearning
2.2 KnowledgeProbe
onnodeclassification,linkpredictionandgraphclassifica-
Recently,knowledgeprobeshavebeenproposedtoprobeknowl-
tion,broadlyincludingninerepresentativegraphlearning
edgeinpre-trainedlanguagemodels(PLMs)suchasELMo[25]
modelsfromfourdifferentcategoriesandsixbenchmarks
andBERT[11].Probingmethodsaredesignedtounderstandand
datasetsfromthreedomainsincludingcitationnetworks,
interpretwhatknowledgehavebeenlearnedinthepre-trainedlan-
socialnetworksandBio-chemicalnetworks.
â€¢ Ourexperimentalresultsconcluderemarkedfindings.Espe- guagemodels,andtheyprobespecificknowledgeincludinglinguis-
ticknowledgeConneauetal.[7],HewittandManning[19],Hou
cially,ourdevisedknowledgeprobesareverifiedtoreflect
andSachan[20],Shietal.[32],andfactualknowledgePetronietal.
thecapabilityofgraphrepresentationlearning,andhave
[26].Forexample,Hewittetal.[19]proposedastructuralprobeto
competitiveandconsistentresultswiththetraditionaleval-
evaluatewhethersyntaxtreeshavebeenencodedinalineartrans-
uationmetricssuchasaccuracy,AUCandF1scores.
formationofaneuralnetworkâ€™swordrepresentationspace.The
2 RELATEDWORK probingresultsdemonstratedthatthetransformsexistforthetwo
PLMsELMoandBERT.Petronietal.proposedaLAMAbenchmark
2.1 GraphRepresentationLearning
toprobefactualknowledgeinPLMsusingpromt-basedretrieval.
GraphrepresentationLearningmethodsmapstructuralgraphdata Themostsimilarworkis[2]inwhichaprobingframeworkhas
intolow-dimensionaldensevectors,bycapturingthegraphtopol- beenproposedforquantifythechemicalknowledgeandmolec-
ogystructure,node-to-noderelationshipsandotherrelevantinfor- ularpropertiesingraphrepresentationsforgraphbasedneural
mation.Earlymethodsforlearningrepresentationsfornodeson networks.Differentfrompreviousstudies,weproposeaholistic
graph-structureddataweremainlymatrixfactorizationmethods graphprobingbenchmarktounderstandandinterpretwhether
basedondimensionreduction[1,6],andrandomwalkapproaches differenttypesofinherentgraphpropertieshavebeenencodedinto
basedonrandomwalkstatistics(e.g.DeepWalk[24]andNode2Vec graphrepresentationlearning,fromthenode-wise,path-wiseand
[14]).Forexample,DeepWalk[24]wasthefirsttoinputrandom structural-wiselevels.Thestudiedgraphmodelscoveringabroad
walkpathsintoaskip-grammodeltolearngraphnodeembeddings. rangeofgraphlearningmethods,rangingfromrandomwalkbased
Node2vec[14]combinedbothbreadth-firstanddepth-firstwalksto methodstographneuralnetworks.Inaddition,webenchmarkourKnowledgeProbingforGraphRepresentationLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
knowledgeprobesonthreeclassicaldownstreamtaskswithnine Wedeviseasupervisedprobetocomparethenodecentrality
representativegraphlearningmethods. oftwodifferentnodesğ‘£ ğ‘– andğ‘£ ğ‘—.Wecalculatethenodecentrality
valuesğ¶(ğ‘£ ğ‘–)andğ¶(ğ‘£ ğ‘—)basedonsomegraphnodecentralitymetrics
3 THEGRAPHEMBEDDINGPROBES (e.g.eigenvectorcentralityandbetweenesscentrality),andmap
thepair-wisecentralitycomparisonintoabinaryvalueğ‘™ ğ‘–ğ‘— asin
Inordertoinvestigatewhethertheinherentgraphpropertieshave
Equation2.Thebinaryvalueğ‘™isusedasthecentralitylabeltotrain
beenencodedintographrepresentationlearningandrevealwhy
thenodecentralityprobeğ‘ƒğ‘(hğ‘˜ ,ğ‘™,ğ‘‡).
differentgraphlearningmethodshavedifferentperformanceon 1:|ğ‘‰|
downstream tasks, we we propose a knowledge probing frame- (cid:26)1 ğ¶(ğ‘£ ğ‘–) â‰¥ğ¶(ğ‘£ ğ‘—)
work(GraphProbe)toprobegraphrepresentationlearning,and ğ‘™ ğ‘–ğ‘— = 0 ğ¶(ğ‘£ ğ‘–) <ğ¶(ğ‘£ ğ‘—). (2)
devisethreeknowledgeprobesfromdifferentlevels,respectively
node-wise,path-wise,andstructure-wiselevels.Figure1shows FollowingtheprobearchitectureinPimenteletal.[27]whichhas
the overall architecture of our proposed GraphProbe. From the beendesignedtoreducetheinformationloss,weadoptasimple
node-wiselevel,wedevisethenodecentralityprobestoinvestigate
learningnetworktwo-layerperception(MLP)1tolearnthesuper-
whethertheinfluencesofnodepropertiesandthelocalneighbour- visedprobe:ğ‘ƒ ğ‘ğ‘˜(hğ‘˜ ğ‘–,hğ‘˜ ğ‘—)=ğ‘€ğ¿ğ‘ƒ(hğ‘˜ ğ‘– âˆ¥hğ‘˜ ğ‘—),andoutputtheprobability
hoodinformationcanbeencodedintorepresentationlearningof ğ‘ thatthepreviousnodeğ‘£ ğ‘– hasalargercentralitythannodeğ‘£ ğ‘—.
nodesfordownstreamtaskssuchasnodeclassificationandlink Cross-entropyisusedasthelossfunctiontotrainthesupervised
prediction.Fromthepath-wiselevel,weleveragethedistancemet- probe.Finally,theprobescoresofthenodecentralityprobeğ‘† ğ‘ğ‘˜
ricsofpathsbetweentwonodestoexplorewhetherthepath-wise formodelğ‘€ğ‘˜ aretheevaluationmeasurescorestomeasurethe
topologicalinformationcanbeencodedintographrepresentation predictionfromtheprobebasedonthelearnedrepresentationsof
learning.Fromthestructure-wiselevel,agraphstructuralprobe themodelğ‘€ğ‘˜ asfollows:
i ss ubd -e gv ri as pe hd it no foin rmve as tt ii og nat ie sew nh coet dh ee dr inth toe gst rr au pc htu rera pl rein sef nor tam tia ot nio ln eae r. ng -. ğ‘† ğ‘ğ‘˜ =ğ¸ğ‘£ğ‘ğ‘™(ğ‘ƒ ğ‘ğ‘˜ (hğ‘˜ ğ‘–,hğ‘˜ ğ‘—),ğ‘™ ğ‘–ğ‘—). (3)
ingwhenperformingtheclassicalgraphclassificationtask.Inthe Classicalevaluationmetricscanbeusedforğ¸ğ‘£ğ‘ğ‘™(e.g.F1-score,AUC,
followingparts,wefirstlyhaveaformalproblemdefinitionfor Accuracy).WereportresultswithAccuracyandF1-score[28]in
theknowledgeprobingongraphrepresentationleaningandthen experimentsection.Ahigherprobescoreğ‘† ğ‘ğ‘˜ meansthegraph-based
illustratedifferentprobesindetails.
modelğ‘€ğ‘˜
hasgreaterabilitytoencodecentralityinformationinto
thenoderepresentations.Weexploretwonodecentralitymetrics
ProblemDefinition. Wegiveaformaldefinitionfortheknowl- forğ¶(Â·),respectivelyeigenvectorcentrality[4]andbetweeness
edgeprobingproblemongraphrepresentationlearning.Giventhe
centrality[30].
constructedgraphdatağº ={ğ‘‰,ğ¸},ğ‘‰ isthesetofnodesinğº,and
ğ¸isthesetoftheedges.hğ‘– representsthefeaturerepresentation EigenvectorCentrality. Eigenvectorcentrality[4]measuresthe
ofeachnodeğ‘£ ğ‘–,andthedimensionofthefeaturerepresentation importance of nodes in a network by exploiting adjacency and
isğ‘‘.ThenoderepresentationXcanberandomlyinitializedorini- eigenvectormatrices.Eigenvectorcentralityisauniquemeasure
tializedwithmetafeatures.Withagraph-basedmodelğ‘€ğ‘˜ ,wecan thatsatisfiescertainnaturalprinciplesforarankingalgorithm[3].
obtainthelearnednodefeaturerepresentationhğ‘˜ =ğ‘€(ğº,X,ğœƒğ‘˜). AndWangetal.[34]showthatseveralrecommendationalgorithms
Thegraphprobeğ‘ƒ isafunctiontoestimatew1 h:| eğ‘‰ th| erthelearned b tia os ned ofon eign eo nd ve ei cm top ror ct ea nn tc re alh ita yv .e Abe âˆˆen Rğ‘›e Ã—n ğ‘›ha in sc te hd ew adit jh act eh ne ci ynt mro ad tru ic x-
representations encode the specified properties I, as defined in
suchthatğ‘ ğ‘–ğ‘— = 1ifnodeğ‘– isconnectedtonode ğ‘— andğ‘ ğ‘–ğ‘— = 0if
Equation1.
not.Theformaldefinitionoftheeigenvalueğœ†andtheeigenvector
ğ‘†ğ‘˜ =ğ‘ƒ(hğ‘˜ 1:|ğ‘‰|,ğ¼,ğ‘‡). (1) xisğ´x =ğœ†xAndtheprincipaleigenvectorxğ‘ = (ğ‘¥ 1ğ‘ ,Â·Â·Â·,ğ‘¥ ğ‘›ğ‘ )is
theeigenvectorcorrespondingtotheeigenvaluewiththelargest
inwhichğ¼ denotestheinvestigatedmetricsusedinthedevised
modulus.Theeigenvectorcentralityofnodeğ‘–canbecomputedas
probes,andğ‘‡ denotestheapplieddownstreamtask.Theprobe
scoreğ‘†ğ‘˜
estimateshowwellthelearnedrepresentationsfromthe
inEquation4:
graph-basedmodelğ‘€ğ‘˜ encodeinformationforthedownstream ğ¸ğ¶(ğ‘–)=ğ‘¥ ğ‘–ğ‘ ,ğ¸ğ¶(ğ‘› ğ‘–)= ğœ†1 âˆ‘ï¸ ğ¸ğ¶(ğ‘› ğ‘—),ğ‘¥ ğ‘– = ğœ†1 ğ´ğ‘‡ ğ‘–,ğ‘—ğ‘¥ ğ‘—. (4)
taskğ‘‡ withrespecttoğ¼.Inthefollowingsections,wewilldescribe ğ‘›ğ‘—âˆˆğ‘(ğ‘›ğ‘–)
thedifferentprobesindetails. whereğ¸ğ¶(ğ‘› ğ‘–),ğ¸ğ¶(ğ‘› ğ‘—)istheamountofinfluencethatnodeğ‘› ğ‘–,ğ‘›
ğ‘—
carries,ğ‘(ğ‘› ğ‘–)isthesetofdirectneighborsofnodeğ‘› ğ‘–,andğœ†isa
3.1 TheNodeCentralityProbe
constant.
Fromthenode-wiseperspective,weleveragethenodecentrality
propertiesofgraphdataastheestimatedmetricsğ¼ becausethe BetweennessCentrality. Ingraphtheory,betweennesscentrality[12]
isameasureofcentralityinagraph.Foreverypairofverticesina
nodecentralityreflectstheinfluenceandimportanceofanode
connectedgraph,thereexistsatleastoneshortestpathbetweenthe
andtotheextentcapturestheneighborhoodinformationofanode
verticessuchthateitherthenumberofedgesthatthepathpasses
[8].Wedeviseanodecentralityprobetoinvestigatewhetherthe
through(forunweightedgraphs).Thebetweennesscentralityfor
learnedrepresentationshaveencodedthenodecentralitywhen
eachvertexisthenumberoftheseshortestpathsthatpassthrough
performingclassicaldownstreamtasks,e.g,nodeclassificationand
linkprediction. 1Weuseatwo-layerMLPasthelearningnetwork,andtheactivationfunctionisReLU.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Figure1:TheoverallarchitectureofourproposedGraphProbe.
thevertex.Itappliestoawiderangeofproblemsinnetworktheory, pairsinthetraininggraph.Specifically,weapproximatethrough
includingproblemsrelatedtosocialnetworks,biology,transport gradientdescent:
andscientificcooperation[12].Thebetweenesscentralityisdefined âˆ‘ï¸ (cid:12) (cid:12)
asinEquation5:
ğ‘š ğµğ‘–ğ‘› (cid:12) (cid:12)ğ‘‘ ğº(ğ‘› ğ‘–,ğ‘› ğ‘—)âˆ’ğ‘‘ ğµ(hğ‘–,hğ‘—)2(cid:12) (cid:12). (7)
ğ‘–,ğ‘—âˆˆğº
âˆ‘ï¸ ğœ ğ‘—ğ‘¡(ğ‘£ ğ‘–)
ğµğ¶(ğ‘£ ğ‘–)= ğœ . (5) whereğ‘‘ ğº(ğ‘› ğ‘–,ğ‘› ğ‘—)denotesthedistanceoftheshortestpathoftwo
ğ‘—ğ‘¡
ğ‘£ğ‘–â‰ ğ‘£ğ‘—â‰ ğ‘£ğ‘¡âˆˆğ‘‰ nodesğ‘£ ğ‘–,ğ‘£
ğ‘—
inğº2.
ğœ ğ‘—ğ‘¡ isnumberoftheshortestpathsbetweennodeğ‘£ ğ‘— andğ‘£ ğ‘¡,and
ğœ ğ‘—ğ‘¡(ğ‘£ ğ‘–)isthenumberofshortestpathpassingthroughnodeğ‘£ ğ‘–. ğ‘† ğ‘‘ğ‘˜ = (cid:12) 1 (cid:12). (8)
(cid:205) ğ‘–,ğ‘—âˆˆğº(cid:12) (cid:12)ğ‘‘ ğº(ğ‘› ğ‘–,ğ‘› ğ‘—)âˆ’ğ‘‘ ğµ(hğ‘˜ ğ‘–,hğ‘˜ ğ‘—)2(cid:12)
(cid:12)
3.2 TheDistanceProbe
Theprobescoreğ‘†ğ‘˜
representstheperformanceoftheprobeto
Ingraphdata,thedistancebetweentwonodescanbeestimated ğ‘‘
bytheshortestpath.Fromthepath-wiseperspective,wedevise recreatethegraphdistance.Therefore,Biggerğ‘† ğ‘‘ğ‘˜ indicatesbetter
thedistanceprobetoinvestigatewhetherthenoderepresentations performance.
encodethepath-leveldistanceinformationofgraphstructure.Fol-
lowingHewittandManning[19],wedevisethedistanceprobeğ‘ƒ
ğ‘‘
3.3 TheGraphStructuralProbe
astoestimatethedifferencesbetweenthegroundedshortestpaths Weproposeastructuralprobetoestimatewhetherthestructural
oftwonodesandthevectordistanceoftwonodesâ€™representations. informationhasbeenencodedintotheembeddingoftheentire
Firstly,wedefineafamilyofinnerproducts,hğ‘‡ğ‘Šhparameterized graph.ThegraphrepresentationHğ‘˜
isconstructedbyaggregating
byanypositivesemi-definite,thesymmetricmatrixğ‘Š âˆˆ ğ‘†ğ‘š +Ã—ğ‘š . thenoderepresentationsthroughthereadoutoperation:
E suq cu hiv ta hle an tt ğ‘Šly, =we ğµc ğ‘‡a ğµn .v Tie hw eit nh nis ea rs pa roli dn uea cr tt hr ğ‘‡an ğ‘Šsf hor im sta hti eo nn eğµ quâˆˆ ivR ağ‘˜ lÃ— eğ‘š nt, Hğ‘˜ =ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘œğ‘¢ğ‘¡(ğº,hğ‘˜ 1:|ğ‘‰|). (9)
to(ğµh)ğ‘‡(ğµh),thenormofhoncetransformedbyğµ.Everyinner
Thereadoutoperationcanobtaingraph-levelrepresentation,e.g.
productcorrespondstoadistancemetric.Therefore,thedefinition sum, mean and max pooling3. We report the results with sum
ofthedistancebetweentwonodesâ€™embeddinghğ‘˜ ğ‘–,hğ‘˜
ğ‘— is: operation.
Inordertoextracttheinherentstructuralinformationofgraphs,
ğ‘‘ ğµ(hğ‘˜ ğ‘–,hğ‘˜ ğ‘—)2=(ğµ(hğ‘˜ ğ‘– âˆ’hğ‘˜ ğ‘—))ğ‘‡ (ğµ(hğ‘˜ ğ‘– âˆ’hğ‘˜ ğ‘—)). (6) weusetheWeisfeiler-Lehman(WL)isomorphismtest[31]asevalu-
Thedistanceprobeğ‘ƒ ğ‘‘ istrainedtorecreatethegraphdistanceof ationmetricstomeasurethesimilaritybetweengraphstructures.
nodepairsinthetraininggraph,andoptimizedthroughgradient
2Consideringthecomputationalcomplexity,weonlykeepthenodepairswithdistance
descentinEquation7.Theparametersofourprobeareexactlythe
lessthanorequalto3.
matrixğµ,whichwetraintorecreatethegraphdistanceofnode 3ThethreeoperationshavebeenimplementedinthebenchmarkKnowledgeProbingforGraphRepresentationLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Table1:Benchmarkdatasets.TheClassesindicatethenode 4 EXPERIMENT
classnumberfortheCoraandFlickrdataset,andgraphclass
4.1 RepresentativeGraphLearningMethods
numberforENZYMESandMUTAG.
Inordertoevaluateourprobingmethodsondifferentcategories,we
Dataset Graphs Classes Nodes Edges NodeFeatures usesomerepresentativegraphlearningmethodsforexperimental
Citation evaluationandreporttheirresultsinSection55.Ingeneral,we
Cora 1 7 2,708 5,429 1,433
networks selectgraphlearningmethodsfrom4categories,includingrandom
Flickr 1 7 89,250 899,756 500
Social walk based graph embedding methods (e.g. Node2Vec [14] and
Yelp 1 - 69,716 1,561,406 -
networks MovieLens 1 - 10,352 100,836 404 DeepWalk[24]),basicgraphneuralnetworks(e.g.GCN[22]and
Bio- ENZYMES 600 6 33 124 21 GAT[33]),self-supervisedgraphlearningmethods(e.g.GCL[36]
networks MUTAG 188 2 18 20 7
andVGAE[21])andweightedgraphlearningmethods(WGCN[38]).
Inaddition,weaddthecontroltask,i.e.anaivetwo-layerMLP
methodtoavoidpotentialperformancebiasduetothelearning
performanceofourprobes6.
TheWLsubtreekernelalgorithmcollectstheinformationofneigh-
â€¢ DeepWalk[24]:isoneofrandomwalkbasedgraphembed-
bornodes,andusehashtoaggregatethemtogeneratethelabelof
dingmethod.Itadoptsthelocalinformationobtainedfrom
thenodeinoneiteration.
truncatedrandomwalkstolearnthelatentrepresentation
Wedeviseastructuralprobetoexplorewhetherthesegraph
ofnodesviaskip-Gramwithhierarchicalsoftmax.
structuralinformationarepreservedinthegraphembeddingrep-
ğ‘˜ â€¢ Node2Vec[14]:isalsoaclassicgraphembeddingmethod.
resentationH .Thebottom-rightfigure(c)inFigure1showsthe
â€¢ Chebyshev[10]:generalizesconvolutionalneuralnetworks
workflowofthestructuralprobe.Thestructuralprobeistoestimate
(CNNs)inthecontextofspectralgraphtheoryanddesign
whetherthegraphembeddinghasencodedthegraphstructural
fastlocalizedconvolutionalfiltersongraphsforgraphlearn-
informatione.g.theWeisfeiler-Lehman(WL)isomorphismtestin-
ing.
formation.Fortheinputgraphs{ğº 1,ğº 2,Â·,ğº ğ‘›},weobtainthegraph
â€¢ GCN [22]: performs semi-supervised learning on graph-
representationğ· ={Hğ‘˜ 1,Hğ‘˜ 2,Â·Â·Â·,Hğ‘˜ ğ‘›}fromthegraph-basedmodel
structureddataandintroducesasimpleandwell-behaved
4,andcomputethepair-wisesimilarityofgraphembeddings.Co-
layer-wisepropagationruleforneuralnetworkmodelsvia
sinesimilarityisused,andobtainthepairwisesimilarityofgraph the localized first-order approximation of spectral graph
representationsğ‘† ğ‘ğ‘˜ ğ‘œğ‘ (ğº ğ‘š,ğº ğ‘›)forapairofgraphsğº ğ‘š andğº ğ‘›: convolutions.
â€¢ GAT[22]:incorporatesmaskedself-attentionlayersontop
Hğ‘˜ Â·Hğ‘˜
ofGCN-stylemethods.
ğ‘† ğ‘ğ‘˜ ğ‘œğ‘ (ğº ğ‘š,ğº ğ‘›)= âˆ¥Hğ‘˜ğºğ‘š âˆ¥âˆ¥Hğº ğ‘˜ğ‘› âˆ¥. â€¢ GraphSAGE[15]:isaninductiveframeworkforrepresen-
ğºğ‘š ğºğ‘› tationlearningonlargegraphswhichleveragesnodefeature
informationtoefficientlygeneratenodeembeddingsforun-
Andwealsocomputethepair-wisesimilarityoftheWLoutputs
seendata.
foreachpairofgraphs.AstheoutputoftheWLalgorithmisa â€¢ VGAE[21]:isunsupervisedlearningframeworkbasedon
setofeachnodelabels,weuseJaccardsimilaritytocomputethe
thevariationalauto-encoder.
structural-levelgraphsimilarityğ‘† ğ‘ ğ‘˜ ğ‘¡ğ‘Ÿ(ğº ğ‘š,ğº ğ‘›)forapairofgraphs â€¢ GCL[36]:isagraphcontrastivelearningframeworkforun-
ğº ğ‘š andğº ğ‘›: supervisedrepresentationlearningofgraphdataanddevises
fourtypesofgraphaugmentationstoincorporatevarious
ğ‘† ğ‘ ğ‘˜ ğ‘¡ğ‘Ÿ(ğº ğ‘š,ğº ğ‘›)=ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘(ğ‘Šğ¿(ğº ğ‘š),ğ‘Šğ¿(ğº ğ‘›)). priors
â€¢ WGCN[38]:considersthedirectionalstructuralinformation
ğ‘Šğ¿(ğº ğ‘š) isthegraphlabeloutputfromtheWLsub-treekernel fordifferentnodesandproposesaGCNmodelwithweighted
algorithm.Basedonthepair-wisesimilarityscores,wecanhave structuralfeatures.
thepair-wisesimilaritymatrixfromthegraphembeddinglevel â€¢ Controlmethod(MLP):weuseasimpletwolayerMLP
Sğºğ¸ andthestructurallevelSğ‘ ğ‘¡ğ‘Ÿ .Spearmancorrelationcoefficient modelwithReLuafterthefirstlayerasthecontrolmethod.
isusedtoestimatethestructuralscoresğ‘† ğ‘ ğ‘˜ ,bycomparingthetwo We perform probing evaluation on the representative graph
similaritymatrixes,computedasinEquation10. learningmethodsforthreeclassicaldownstreamtasksofgraph
learningmethods.Inadditiontolearningwithrandominitialization,
ğ‘† ğ‘ ğ‘˜ = ğ‘›1 âˆ‘ï¸ 1âˆ’ 6(cid:205) ğ‘›ğº (ğ‘›ğ‘› 2âˆˆ âˆ’ğ· 1ğœ ğ‘š )2 ğ‘›. (10) w pee rff ou rr mthe tr host ru od uy ghgr aa np ah lyle sa isrn inin Sg ew cti it oh nm 5e .ta-featuresinitialized,and
(ğºğ‘šâˆˆğ·
4Ifthegraphlearningalgorithmcanlearntherepresentationoftheentiregraph,we
ğœ rğ‘š anğ‘› ki es dth ğ‘…e (So ğ‘šğ‘ r ğ‘¡d ğ‘Ÿe )r .i ğ‘…ng (Â·)di is sta rn ac ne kib ne gtw oe pe en rat th ioe nra on nke ad nğ‘… i( nS pğ‘šğº uğ¸ t) aa rn rad yt .h Ae d
g
5Orir
a
ue pc rht pl ry
re
opu brs iee
ns
geit n. mtO
a
ett tih hoe onr d.w ii sse fl, ew xie blu ys ue st eh de fr oe ra ad no yut gro ap pe hra lt ei ao rn nio nf gn mod ee ths oto d.r Dep ur ee ts oen st path ce
e
higher structure probe scoreğ‘† ğ‘ ğ‘˜ indicates that the graph repre- limitation,wechosedifferentrepresentativegraphlearningmethodsfromdiverse
categories.
sentationshavethegreaterabilitytocapturethegraphstructural 6OurdevisedprobesarebasedonMLPandweuseMLPasthecontrolexperiment
information. variableforcomparison.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
4.2 DownstreamTasksandDatasets neighborfeaturesforeachnode.Also,weaggregatetheweighted
Weconductperformanceevaluationontheclassicdownstream neighborfeaturesforeachnode.Finally,weuseReLUactivation
tasksofgraphlearningmethods,includingnodeclassification[22], function.Inordertokeepperformanceofit,weusethesameen-
linkprediction[29]andgraphclassification[35]. coderGCNasVGAE[21].ForGCL,weusethesameaugmentation
astheGCL,werandomlydropedgesandswapsomenodeinorder
â€¢ Nodeclassification:Thistaskisoneofthemostpopular
tomakedifferentgraphs.Weusethecosinesimilaritiestocalculate
andwidelyusedapplicationsofgraphlearningmodels.The
thetwoaugmentinggraphsandcalculatethecontrastiveloss.The
graphlearningmethodslearnthenoderepresentationsand
activationfunctionELUisusedforGATandtheactivationfunction
classifynodesintodifferentgroups.
ReLUisusedfortheothermodels.Thedropoutratioforallmodels
â€¢ Linkprediction:Itistopredicatewhetherthereexista
is0.5.Theoutputdimensionsizeforallmodelsis64.Thelearning
linkbetweentwonodes.Forexample,therecommendation
rateissetas0.001.
probleminrecommendersystemscenarioscanbeformu-
latedasonelinkpredictiontaskandconstructauser-item
5 EXPERIMENTALRESULTS
interactiongraphtopredicttheprobabilityoflinkingauser
toaitem. 5.1 PerformanceofGraphLearningMethodson
â€¢ Graphclassification:Itsgoalistoclassifyawholegraph NodeClassification
intodifferentcategories.Themainapplicationsofgraphclas-
Inordertovalidatetheknowledgeprobingperformanceofour
sificationareproteinclassificationandchemicalcompound
methodswithrespecttothenodeclassificationtask,wecompare
classification.
theprobingscoresoftherepresentativegraphlearningmethods
Weadoptsomebenchmarkdatasetsfromdifferentdomainsin-
withreferencetocommonlyusedmetricsinnodeclassification
cludingcitationnetworks,socialnetworks,andBio-chemicalNet-
includingAccuracy(ACC)andF1scores.Table2showstheper-
works.Table1showsthestatisticsandproprietressoftheused
formancecomparisonfornodeclassification.Inadditionaltothe
benchmarks.
absoluteperformancenumbers,weaddtheoverallranknumbers
â€¢ CitationNetworks:Cora[23]isadatasetcontainingscien- amongthecomparedmethodswithbracketsastherelativeperfor-
tificpaperscategorizedintosevenclasses.Itcommonlybe mance.Forexample,thehighlightednumber79.9(1)indicatesGAT
usedfornodeclassification(transductive).Thenumberof obtains0.799accuracyonCoradataset,andrankfirstamongall
nodeclassesis7,andthenumberofnodefeaturesis1,433. comparedmethods.Inalltables,weuseboldfonttohighlightthe
â€¢ SocialNetworks:Flickr[37]isaimagehostingandvideo bestperformanceandunderlinestoindicatetheworstperformance
hostingplatform.Flickrdatasetisusedfornodeclassification 7.
(inductive).Thenumberofnodeclassesis7andthenumber Fornodeclassification,wereporttransductiveperformancefrom
of node features is 500. In addition, we adopt two social CoraandinductiveperformancefromFlickr.Wecanseethatthe
networksdatasetsYelp[29]andMovieLens[17]forLink bestmethodonCora(transductive)isGAT,andthebestmethodis
prediction.Yelpincludes69,716usersanditemsasnodes,and GCLonFlick(inductive)withrespecttobothACCandF1scores.
1,561,406interactionrecords.MovieLensdatasetincludes Ourcentralityprobes(ECandBC)haveconsistentevaluationre-
9,742moviesasnodesinthegraph.Italsoincludesover sultswiththecommonlyusedgoldenmetrics,namelyGATranks
100,836ratingsprovidedbyaround610users.Thenumber first.Fortheworstcases,ourcentralityproberhasthesameresults
ofmeta-featuresforitemsis404. (DeepWalkorMLP)withACCandF1onFlickdataset.OnCora,
â€¢ Bio-ChemicalNetworks:Twodatsetsareusedforgraph wefindaninterestingphenomenonthatthesimpleMLPmethods
classification,respectivelyEnzyme[5]andMutag[9].En- isbetterthanGCL,Node2VecandDeepWalkwithrespecttoAcc
zymedatasetcontainstheproteinsinformationwhichcon- andF1whileourcentralityprobingmethoddemonstratetheworst
tributetocatalyzingchemicalreactionsinthebody.Mutag methodsarerespectivelyMLPandNode2Vec.Itmightraisethe
datasetisawidelyusedtoxicitydatasetwhichhelpsassess questionwhetherthefinalstatisticalmetricslikeaccuracyandF1
the potential risks of exposure to various chemicals and actuallyreflectthecapabilityofthegraphrepresentationlearning
compounds. Incomparisonwiththecentralityprobes,ourdistanceprobeare
notveryconsistentwithourcentralityprobesandthetraditional
4.3 ExperimentalSetting
metricsalthoughtheycanfindtheworsecases.Itisbecausethe
AlldatasetsexpectYelpdatasetfollowusethesplittingrulesas thedistanceprobeisapath-wiseprobewhilethenodeclassifica-
previousstudies.FortheYelpdataset,thedatasplittingratiofor tionmightemphasizetoencodemoretopologicalinformationinto
thetrainingset,validationsetandtestsetissetas7:2:1. graphrepresentationlearning.Thepath-wiseprobemethode.g.the
ForChebyshev,weuseonelayerstructure.ForGCN,GAT,Graph- distanceprobemightbeinappropriateforknowledgeprobinginthe
SAGEandMLP,weadoptthetwolayerstructure.Thelengthof nodeclassificationtaskasonlyencodingthepath-wiseinformation
walkforNode2Vecissetas20.Thesizeofthecontextwindow ingraphrepresentationlearningcannotworkswell,e.g.random
fortheskip-gramandthewalkspernodeareboth10.Inorder walkbasedmethodslikeDeepWalkandNode2Vec.
tomakethewalkunbiased,wealsosetthepwhichcontrolshow Wheninitializingwithmeta-features,wehavethesimilarresults
likelythewalkistogobacktothepreviousnode,itissetas1.0.For onFlickdataset,andthebestmethodisGCLwithrespecttoboth
WGCNmethod,weusethesamesetas[38],threelayersstructure,
computetheneighborsnumberforeachnodeandusetheweighted 7Statisticalsignificancetestshavebeenconducted.AllreportedresultsaresignificantKnowledgeProbingforGraphRepresentationLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Table2:ResultsofthegraphlearningmodelsonNodeClassification.Highlightthebestperformancewithboldfontandthe
worstoneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
ACC 52.9(6) 78.6(3) 79.9(1) 77.3(4) 79.7(2) 45(8) 76.13(5) 14.1(10) 21.24(9) 52.4(7)
F1 56.94(6) 77.96(3) 79.18(1) 76.64(4) 78.84(2) 45.7(8) 74.32(5) 12.88(10) 21.19(9) 51.93(7)
Cora EC 60.45(7) 73.6(2) 75.12(1) 72.42(4) 72.83(3) 62.38(6) 67.17(5) 57.5(9) 58.13(8) 57.35(10)
BC 59.24(6) 70.24(3) 76.12(1) 58.01(7) 71.24(2) 61.13(5) 64.13(4) 54.1(9) 54.12(8) 53.8(10)
Distance 8.05(10) 11.24(7) 12.98(3) 11.93(6) 24.16(1) 10.08(9) 12.91(4) 14.68(2) 11.98(5) 11.24(7)
ACC 77.5(5) 81.14(4) 72.05(6) 57.05(7) 91.41(2) 92.8(1) 85.12(3) 50.49(8) 50.12(9) 47.94(10)
F1 77.5(5) 81.14(4) 72.05(6) 57.05(7) 91.41(2) 92.8(1) 85.12(3) 50.49(8) 50.12(9) 47.94(10)
Flickr EC 49.7(7) 50(6) 50.05(4) 50.05(4) 71.28(2) 76.12(1) 69.12(3) 32.18(8) 31.24(9) 23.13(10)
BC 68.17(4) 65.72(7) 66.12(6) 67.12(5) 70.13(2) 75.13(1) 70.1(3) 30.13(8) 28.17(10) 29.38(9)
Distance 13.45(6) 13.82(2) 13.82(2) 12.54(7) 13.62(5) 13.82(2) 19.52(1) 11.98(8) 10.44(9) 9.68(10)
Dataset-meta Initializationwithmeta-features
ACC 62.3(7) 79.4(2) 79.9(1) 77.3(5) 78.5(3) 55.14(9) 78.12(4) 55.18(8) 68.91(6) 53.4(10)
F1 63.9(6) 78.57(2) 79.02(1) 76.52(5) 77.83(4) 55.18(9) 78.01(3) 58.13(8) 61.13(7) 52.8(10)
Cora EC 65.79(6) 72.37(1) 72.14(2) 66.18(4) 70.24(3) 41.23(8) 66.01(5) 52.37(7) 32.44(9) 30.12(10)
BC 63.71(6) 70.12(1) 69.27(2) 66.04(4) 66.13(3) 51.28(8) 64.78(5) 57.89(7) 41.28(9) 39.38(10)
Distance 8.03(10) 11.18(8) 13.2(4) 12.03(5) 11.47(6) 11.04(9) 30.91(1) 14.59(2) 13.82(3) 11.19(7)
ACC 77.9(5) 82.12(4) 74.13(6) 59.38(7) 92.42(2) 93.12(1) 86.13(3) 52.48(8) 52.34(9) 50.13(10)
F1 77.9(5) 82.12(4) 74.13(6) 59.38(7) 92.42(2) 93.12(1) 86.13(3) 52.48(8) 52.34(9) 50.13(10)
Flickr EC 48.37(7) 81.38(5) 68.73(6) 86.81(3) 90.38(2) 91.24(1) 85.12(4) 48.37(7) 46.37(9) 42.41(10)
BC 38.14(6) 39.14(4) 37.89(7) 38.84(5) 67.4(2) 68.38(1) 59.24(3) 32.32(8) 29.38(9) 29.04(10)
Distance 24.25(1) 24.25(1) 21.48(5) 19.09(7) 23.01(4) 18.33(8) 23.62(3) 18.02(9) 19.1(6) 16.04(10)
Table3:ResultsofthegraphlearningmodelsonLinkPrediction.Highlightthebestperformancewithboldfontandtheworst
oneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
AUC 61.24(7) 78.14(1) 63.12(6) 63.71(5) 73.14(3) 67.12(4) 76.12(2) 55.12(8) 52.37(9) 42.32(10)
F1 52.24(8) 71.50(1) 66.15(5) 65.23(6) 71.09(2) 70.01(3) 68.57(4) 53.17(7) 51.95(9) 51.05(10)
Yelp Distance 31.90(3) 32.02(1) 24.25(5) 22.05(8) 24.25(5) 28.32(4) 32.01(2) 24.18(7) 16.33(9) 14.04(10)
EC 46.28(9) 71.21(2) 60.71(5) 64.28(4) 72.16(1) 56.73(6) 70.12(3) 49.82(7) 48.12(8) 45.39(10)
BC 45.13(7) 72.12(1) 56.78(6) 63.47(4) 71.97(2) 58.12(5) 71.12(3) 39.02(9) 40.18(8) 36.85(10)
AUC 68.21(5) 73.75(2) 47.91(10) 71.21(3) 74.29(1) 68.14(6) 70.12(4) 56.43(7) 51.24(8) 50.12(9)
F1 50.75(9) 67.94(3) 61.25(4) 58.64(5) 70.48(2) 58.60(6) 70.87(1) 51.84(7) 51.06(8) 48.82(10)
MovieLens Distance 19.50(8) 19.52(7) 29.78(2) 24.25(4) 24.17(6) 28.77(3) 31.82(1) 24.20(5) 15.75(9) 12.31(10)
EC 50.52(7) 70.85(2) 61.23(5) 63.41(4) 72.16(1) 59.13(6) 70.12(3) 49.82(9) 50.12(8) 41.26(10)
BC 40.97(8) 72.30(1) 57.55(6) 63.77(4) 71.97(2) 62.12(5) 69.13(3) 39.02(9) 42.12(7) 37.24(10)
Dataset-meta Initializationwithmeta-features
AUC 63.85(7) 81.14(2) 78.03(4) 70.37(5) 78.24(3) 70.13(6) 82.34(1) 59.12(8) 55.37(9) 45.32(10)
F1 53.44(9) 75.35(1) 67.45(5) 66.83(6) 73.19(4) 74.60(2) 73.89(3) 57.12(7) 54.82(8) 52.44(10)
Yelp Distance 28.96(4) 33.20(2) 24.18(8) 24.47(7) 24.92(6) 25.70(5) 49.06(1) 29.87(3) 19.47(9) 13.82(10)
EC 47.21(8) 72.43(2) 61.24(5) 65.12(4) 73.13(1) 57.14(6) 71.21(3) 50.12(7) 47.12(9) 42.31(10)
BC 46.14(7) 73.13(3) 57.38(6) 64.18(4) 73.59(1) 59.12(5) 73.24(2) 41.31(9) 45.89(8) 38.13(10)
AUC 70.57(4) 73.56(3) 48.37(10) 74.92(2) 74.96(1) 52.38(8) 70.18(5) 56.66(7) 57.17(6) 50.12(9)
F1 62.45(5) 66.16(3) 45.63(8) 71.86(1) 67.97(2) 44.22(9) 64.51(4) 51.65(6) 50.13(7) 43.59(10)
MovieLens Distance 27.15(5) 29.77(2) 22.43(7) 28.02(4) 22.84(6) 28.68(3) 32.28(1) 20.51(8) 19.52(9) 16.33(10)
EC 60.73(5) 72.63(1) 71.88(2) 60.36(6) 71.61(3) 31.35(10) 65.35(4) 55.14(8) 60.12(7) 42.31(9)
BC 53.29(6) 71.10(2) 75.79(1) 50.47(8) 71.04(3) 43.13(9) 67.12(4) 53.17(7) 58.38(5) 40.13(10)
ofourcentralityprobesandtheclassicalmetrics.OnCora,the Inordertofurtherdiscusstherelationsbetweenthenode-wise
top-2methodwithrespecttoACCandF1areGATandGCNwhile propertiesandthegraphrepresentationlearning,wecalculatethe
top-2methodswithrespecttoourcentralityprobesareGATand homophily[40]ofdifferentgraphsfromdifferentdatasets.The
CCN.OnCoraandFlickdatasets,theworstcasesareallMLPwith homophilyratioofisabout0.8forCoranad0.32forFlickr.Itmight
respecttobothofourcentralityprobesandtheclassicalmetrics. explainwhydifferentgraphmethodsshowdifferentperformance,
Ingeneral,ithasslightperturbationintherelativeperformance duetotheinherentgraphproperties.GNNmethodslikeGATcan
wheninitializedwithmeta-features. havebetterperformanceonhighlyhomophilousgraphs.Ourprobe
alsohaveconsistentresultswiththehomophilyanalysis.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Table4:ResultsofthegraphlearningmodelsonGraphClassification.Highlightthebestperformancewithboldfontandthe
worstoneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
ACC 82.05(5) 92.31(1) 87.18(2) 84.62(4) 74.36(6) 74.36(6) 87.13(3) 72.71(9) 73.13(8) 71.49(10)
MUTAG F1 82.05(5) 92.31(1) 87.18(2) 84.62(4) 74.36(6) 74.36(6) 87.13(3) 72.71(9) 73.13(8) 71.49(10)
Structure 11.96(4) 15.69(1) 13.39(2) 8.29(5) 3.89(7) 3.79(8) 12.23(3) 5.12(6) 3.13(9) 2.62(10)
ACC 45(4) 44.17(5) 45.83(3) 54.17(2) 43.33(6) 43.33(6) 59.38(1) 42.5(8) 41.5(9) 37.5(10)
ENZYMES F1 15(10) 22.5(3) 20.83(4) 18.33(5) 26.67(1) 16.67(8) 25.81(2) 18.27(6) 17.93(7) 16.67(8)
Structure 3.44(7) 11.82(3) 8.37(4) 12.83(2) 7.26(6) 8.36(5) 15.48(1) 2.19(8) 2.04(9) 1.84(10)
Dataset-meta Initializationwithmeta-features
ACC 94.87(1) 82.05(6) 84.62(4) 92.31(2) 79.49(7) 84.62(4) 90.24(3) 77.31(8) 75.13(9) 74.62(10)
MUTAG F1 94.87(1) 82.05(6) 84.62(4) 92.31(2) 79.49(7) 84.62(4) 90.24(3) 77.31(8) 75.13(9) 74.62(10)
Structure 18.19(1) 4.78(7) 6.02(5) 16.48(3) 5.98(6) 15.8(4) 17.29(2) 2.38(9) 2.41(8) 1.73(10)
ACC 43.33(5) 47.5(3) 39.17(10) 57.5(2) 43.33(5) 46.67(4) 60.35(1) 41.32(8) 40.13(9) 43.33(5)
ENZYMES F1 22.5(3) 21.67(4) 15(10) 20(5) 19.17(6) 25.83(1) 25.23(2) 17.99(8) 18.12(7) 17.5(9)
Structure 7.49(3) 5.12(4) 3.48(6) 8.89(2) 3.24(7) 4.56(5) 10.23(1) 1.31(9) 1.78(8) 0.39(10)
Figure2:Radarchartcomparisonofgraphembeddingmodelsfordifferentinformationembeddingcapabilitie
â€¢ Ourcentralityprobesiseffectiveforknowledgeprob- 5.2 PerformanceofGraphLearningMethodson
ingofgraphrepresentationlearninginthenodeclas- LinkPrediction
sificationtask.Theyhasconsistentresultswiththetradi-
Toevaluatetheknowledgeprobingperformanceofourmethods
tionalevaluationmetricsaccuracyandF1,validatingtheir
withrespecttothelinkpredictiontask,wecomparetheprobing
effectivenessofourcentralityprobes.
scoresoftherepresentativegraphlearningmethodswithreference
â€¢ Thepath-wiseprobemethode.g.thedistanceprobemight
tocommonlyusedmetricsinthelinkpredictiontaskonYelpand
beinappropriateforknowledgeprobinginthenodeclassifi-
Movielensdataset,includingAUCandF1scores.Weevaluateboth
cationtask.
thecentralityprobesandthedistanceprobes,andtheperformance
â€¢ GATissuperiortootherrepresentativemethods.The
comparisonresultsonYelpandMovieLensforlinkpredictionare
top-2methodsareGATandGCN.Initializingwithmeta-
reportedinTable3.OnYelpdataset,thebestmethodsisGCNand
measuresresultsintosmallperturbationamongthetopper-
theworstmethodisDeepWalkexceptMLP,withrespecttoboth
centileinperformanceranking.
AUCandF1scores.ThedistanceprobehasconsistentresultswithKnowledgeProbingforGraphRepresentationLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Skyline Plot on different Downstream Tasks Skyline Plot on different Downstream Tasks Skyline Plot on different Downstream Tasks
5 Data MLP 5 Data MLP 5 Data MLP
Dominate DeepWalk Dominate Node2Vec Dominate Node2Vec
4 Line Chebyshev 4 Line DeepWalk 4 Line DeepWalk
Node2Vec
3 GraphSAGE 3 VGAE GCL 3 VGAEGCL
GAT Chebyshev Chebyshev
2 WGCN 2 GraphSAGE 2 GraphSAGE
GCL WGCN WGCN
1 VGAE 1 GAT 1 GAT
GCN GCN GCN
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Performance of Node Classification Performance of Node Classification Performance of Link Prediction
Figure3:SkylinePlotondifferentDownstreamTasksbasedonrandominitialization
bothAUCandF1scores.Althoughthecentralityprobescannotbe bestmethodisGCNwithwithbothACCandF1andourstructural
totallythesamewiththetraditionalscores,wecanseethatthey( probingresultisconsistentwiththetraditionalgoldenmetrics.On
especiallythecentralityprobewithbetweeness)havesimilarresults ENZYMESdataset,ourstructuralprobingresultsareconsistent
byrankingGCN,VGAEattop-3positionsinthecasethatthetwo withACCscores(WGCNisbest).Forthecaseswithmeta-features,
traditionalmetricshavesimilarresultsratherthantheconsistent ourstructuralprobingresultsareconsistentwithACCandF1scores
results.OnMoivelensdataset,thetraditionalmetricshaveshown onMUTAGdatasetandwithACCscores(Chebyshevisbest)on
considerabledifferentresultsintheperformanceranking,inwhich ENZYMESdataset(WGCNisbest).
thebestmethodisVGAEwithAUCandthatisWGCNwithF1. â€¢ Ourstructureprobeiseffectiveforknowledgeprobing
Thisphenomenontosomeextentindicatesthatdifferentevaluation
ofgraphrepresentationlearninginthegraphclassifica-
metricsmighthavesomedifferentrelativeresultsduetodifferent
tiontask.Theyhaveconsistentresultswiththetraditional
measuremechanisms.OurdistanceprobeisconsistentwithF1for
evaluationmetricsACCandF1fordifferentinitialization
thebestmethod,andourcentralityprobesaremoresimilarwith
setting.
AUCscores. â€¢ Ingeneral,nosinglemethodcandominateothermeth-
Wheninitializingwithmeta-features,theperformanceoflink
odsonthetwodatasets.WGCNhasrelativelyrobustpe-
predictiononYelpandMovielenshavesomedifferentresults.The
formance,rankingattop-3positionsonthetwodatasets.In
bestmethodonYelpisWGCN(AUC)andGCN(F1),incontrast
somecases,some"out-of-date"methodse.g.Chebyshevcan
withrandominitialization(GCN).OnMoivelensdataset,thebest
havebetterperformance.
ones are VGAE (AUC) and GraphSAGE (F1) while the random
initializationresultsareVGAE(AUC)andWGCN(F1).Ourdistance 5.4 VisualizationAnalysis
probealsocapturethedifferencesduetodifferentinitialization
Inordertofurthercomparetheoverallperformanceofdifferent
settingandisstillconsistentwiththetraditionalmetrics,ranking
methodsfordifferentinformationembeddingcapabilities,wecom-
WGCNandGCNatthetop-2positions.
putetherankingofthe9representativemethodsandtheMLPbase-
â€¢ Thedistanceprobeiseffectiveforknowledgeprobing
lineforeachprobeanduseradarchartstovisualizetheircapacities
ofgraphrepresentationlearninginthelinkprediction
inFigure2(IindicatestheinductiveandTindicatestransductive,
task.Theyhaveconsistentresultswiththetraditionaleval-
andMtheMeta).GCNandWGCNhavebetterperformance
uationmetricsAUCandF1fordifferentinitializationsetting.
inmostprobingaspectsincomparisonwithothermethods.
Thereasonmightbethatthepath-wiseprobeisdevisedto
Althoughhavingthesamecapacitiesofaggregatingtheneighbor
probethepath-wiseinformationwithingraphrepresenta-
information,VGAEhasthesub-optimalperformance.Furthermore,
tionlearningwhichactsthecorerolesinlinkprediction.
theyallhardlyrelyonmetainformations.GCLhasbetterperfor-
â€¢ Ingeneral,nosinglemethodcanbetotallysuperiortoother
manceonNodeClassification(Inductive),GAThasbetterperfor-
methodswithrespecttoallevaluationmetricsonthetwo
manceonNodeClassification(transductive).Chebyshevmightbe
datasets.GCN,VGAEandWGCNcanberankedatthetop
betterusedwithmetainformation.Node2Vechastheworstperfor-
positions.
manceonallaspects.
Wealsodrawtheskylineplotforfindingthemethodsondif-
5.3 PerformanceofGraphLearningMethodson
ferentdownstreamtaskswhichcannotbedominatedbyother
GraphClassification
methodsinFigure3.Wecaninvestigatethejoint-abilitiesofgraph
We compare the structure probing scores of the representative learningmethodsondifferentdownstreamtasks.InLinkPrediction-
graphlearningmethodswithreferencetocommonlyusedmetrics NodeClassificationtasks,GAT,VGAEandGCNhasbetterperfor-
inthegraphclassificationtaskonMUTAGandENZYMESdataset, mancethatcannotbedominated.InGraphClassification-Node
includingaccuracy(ACC)andF1scores.Table3demonstratesthe Classification,GATandGCNperformswell.GAThasbettergraph
performanceresultsongraphclassification.OnMUTAGdataset,the classificationcapabilitiesandGCNhasbetternodeclassification
noitciderP
kniL
fo
ecnamrofreP
noitacifissalC
hparG
fo
ecnamrofreP
noitacifissalC
hparG
fo
ecnamrofrePConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
abilities.InGraphClassification-LinkPredictions,onlyGCNcannot forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Long
bedominatedbyothers.Fromtheskylineresults,wecanseethat andShortPapers),pages4171â€“4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics.
GCNandGAThasbetterjoint-abillitesfordownstreamtasks.
[12] LintonFreeman.1977. Asetofmeasuresofcentralitybasedonbetweenness.
Sociometry,40:35â€“41.
5.5 EffectsofParameters [13] CLeeGiles,KurtDBollacker,andSteveLawrence.1998.Citeseer:Anautomatic
citationindexingsystem.InProceedingsofthethirdACMconferenceonDigital
Theonlyhyperparametersusedinourprobesisthepathparameter. libraries,pages89â€“98.
[14] AdityaGroverandJureLeskovec.2016.node2vec:Scalablefeaturelearningfor
Itcontrolstheshortestpaththatourprobecandetectinthegraph.
networks.InProceedingsofthe22ndACMSIGKDDinternationalconferenceon
WecalculatetheCorrelationswiththeF1scoresindifferentpath, Knowledgediscoveryanddatamining,pages855â€“864.
itshowsthatmostofourbestpatharebetween3and4.Weuse [15] WillHamilton,ZhitaoYing,andJureLeskovec.2017.Inductiverepresentation
learningonlargegraphs.Advancesinneuralinformationprocessingsystems,30.
themaxscoreofthepathforeachdatasets [16] WilliamL.Hamilton,RexYing,andJureLeskovec.2017.Representationlearning
ongraphs:Methodsandapplications.IEEEDataEng.Bull.,40:52â€“74.
[17] F.MaxwellHarperandJosephA.Konstan.2015.Themovielensdatasets:History
6 CONCLUSION
andcontext.ACMTrans.Interact.Intell.Syst.,5(4).
In this paper, we proposed a graph probing benchmark for the [18] XiangnanHe,KuanDeng,XiangWang,YanLi,YongDongZhang,andMeng
Wang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor
representativegraphlearningmethods.Diverseprobesatthree recommendation.InProceedingsofthe43rdInternationalACMSIGIRConference
differentlevels(node-wise,path-wiseandstructure-wise)arede- onResearchandDevelopmentinInformationRetrieval,SIGIRâ€™20,page639â€“648,
NewYork,NY,USA.AssociationforComputingMachinery.
vised to investigate and interpret weather the graph properties
[19] JohnHewittandChristopherD.Manning.2019.Astructuralprobeforfinding
fromdifferentlevelsareencodedintorepresentationlearningof syntaxinwordrepresentations. InProceedingsofthe2019Conferenceofthe
thesevenrepresentativegraphneuralnetworksbasedmethods.We NorthAmericanChapteroftheAssociationforComputationalLinguistics:Hu-
manLanguageTechnologies,Volume1(LongandShortPapers),pages4129â€“4138,
conductsystemevaluationandthoroughanalysistoinvestigate
Minneapolis,Minnesota.AssociationforComputationalLinguistics.
whatkindofinformationhavebeenencodedandwhichmethods [20] YifanHouandMrinmayaSachan.2021.Birdâ€™seye:Probingforlinguisticgraph
havecompetitiveperformancewithdifferenttargeteddownstream structureswithasimpleinformation-theoreticapproach.InProceedingsofthe
59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th
tasks.Theexperimentalevaluationvalidatetheeffectivenessof InternationalJointConferenceonNaturalLanguageProcessing(Volume1:Long
GraphProbe.Furthermore,Weconcludesomeremarkingfindings: Papers),pages1844â€“1859,Online.AssociationforComputationalLinguistics.
[21] ThomasNKipfandMaxWelling.2016.Variationalgraphauto-encoders.arXiv
GATissuperiorinthenodeclassification;GCNandWGCNare
preprintarXiv:1611.07308.
relativelyversatilemethodsachievingbetterresultswithrespectto [22] ThomasN.KipfandMaxWelling.2017.Semi-supervisedclassificationwithgraph
differenttasks.Thebenchmarkcodesandresourceswillbepublic convolutionalnetworks.InInternationalConferenceonLearningRepresentations.
[23] AndrewKachitesMcCallum,KamalNigam,JasonRennie,andKristieSeymore.
afteracceptance.
2000.Automatingtheconstructionofinternetportalswithmachinelearning.
InformationRetrieval,3:127â€“163.
REFERENCES [24] BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014.Deepwalk:Onlinelearning
ofsocialrepresentations.InProceedingsofthe20thACMSIGKDDinternational
[1] AmrAhmed,NinoShervashidze,ShravanNarayanamurthy,VanjaJosifovski, conferenceonKnowledgediscoveryanddatamining,pages701â€“710.
andAlexanderJ.Smola.2013.Distributedlarge-scalenaturalgraphfactorization. [25] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,Christopher
InProceedingsofthe22ndInternationalConferenceonWorldWideWeb,WWW Clark,KentonLee,andLukeZettlemoyer.2018.Deepcontextualizedwordrepre-
â€™13,page37â€“48,NewYork,NY,USA.AssociationforComputingMachinery. sentations.InProceedingsofthe2018ConferenceoftheNorthAmericanChapter
[2] MohammadSadeghAkhondzadeh,VijayLingam,andAleksandarBojchevski. oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
2023. Probinggraphrepresentations. InInternationalConferenceonArtificial Volume1(LongPapers),pages2227â€“2237,NewOrleans,Louisiana.Association
IntelligenceandStatistics,pages11630â€“11649.PMLR. forComputationalLinguistics.
[3] AlonAltmanandMosheTennenholtz.2005. Rankingsystems:Thepagerank [26] FabioPetroni,TimRocktÃ¤schel,SebastianRiedel,PatrickLewis,AntonBakhtin,
axioms.InProceedingsofthe6thACMConferenceonElectronicCommerce,EC YuxiangWu,andAlexanderMiller.2019.Languagemodelsasknowledgebases?
â€™05,page1â€“8,NewYork,NY,USA.AssociationforComputingMachinery. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
[4] PhillipBonacich.1987.Powerandcentrality:Afamilyofmeasures.American Processingandthe9thInternationalJointConferenceonNaturalLanguagePro-
journalofsociology,92(5):1170â€“1182. cessing(EMNLP-IJCNLP),pages2463â€“2473,HongKong,China.Associationfor
[5] KarstenM.Borgwardt,ChengSoonOng,StefanSchÃ¶nauer,S.V.N.Vishwanathan, ComputationalLinguistics.
AlexJ.Smola,andHans-PeterKriegel.2005. Proteinfunctionpredictionvia [27] TiagoPimentel,JosefValvoda,RowanHallMaudslay,RanZmigrod,Adina
graphkernels.Bioinformatics,21(1):47â€“56. Williams,andRyanCotterell.2020.Information-theoreticprobingforlinguistic
[6] ShaoshengCao,WeiLu,andQiongkaiXu.2015.Grarep:Learninggraphrepre- structure.InProceedingsofthe58thAnnualMeetingoftheAssociationforCom-
sentationswithglobalstructuralinformation.InProceedingsofthe24thACM putationalLinguistics,pages4609â€“4622,Online.AssociationforComputational
InternationalonConferenceonInformationandKnowledgeManagement,CIKM Linguistics.
â€™15,page891â€“900,NewYork,NY,USA.AssociationforComputingMachinery. [28] C.J.VanRijsbergen.1979. InformationRetrieval,2ndedition. Butterworth-
[7] AlexisConneau,GermanKruszewski,GuillaumeLample,LoÃ¯cBarrault,and Heinemann,USA.
MarcoBaroni.2018. Whatyoucancramintoasingle$&!#*vector:Probing [29] PrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,and
sentenceembeddingsforlinguisticproperties. InProceedingsofthe56thAn- TinaEliassi-Rad.2008.Collectiveclassificationinnetworkdata.AImagazine,
nualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long 29(3):93â€“93.
Papers),pages2126â€“2136,Melbourne,Australia.AssociationforComputational [30] MarvinE.Shaw.1954.Groupstructureandthebehaviorofindividualsinsmall
Linguistics. groups.TheJournalofPsychology,38(1):139â€“149.
[8] KousikDas,SovanSamanta,andMadhumangalPal.2018.Studyoncentrality [31] NinoShervashidze,PascalSchweitzer,ErikJanvanLeeuwen,KurtMehlhorn,
measuresinsocialnetworks:asurvey.Socialnetworkanalysisandmining,8:1â€“11. andKarstenM.Borgwardt.2011. Weisfeiler-lehmangraphkernels. J.Mach.
[9] AsimKumarDebnath,RosaL.LopezdeCompadre,GargiDebnath,AlanJ.Shus- Learn.Res.,12(null):2539â€“2561.
terman,andCorwinHansch.1991.Structure-activityrelationshipofmutagenic [32] XingShi,InkitPadhi,andKevinKnight.2016.Doesstring-basedneuralMTlearn
aromaticandheteroaromaticnitrocompounds.correlationwithmolecularorbital sourcesyntax? InProceedingsofthe2016ConferenceonEmpiricalMethodsin
energiesandhydrophobicity.JournalofMedicinalChemistry,34(2):786â€“797. NaturalLanguageProcessing,pages1526â€“1534,Austin,Texas.Associationfor
[10] MichaÃ«lDefferrard,XavierBresson,andPierreVandergheynst.2016.Convolu- ComputationalLinguistics.
tionalneuralnetworksongraphswithfastlocalizedspectralfiltering.Advances [33] PetarVelickovic,GuillemCucurull,ArantxaCasanova,AdrianaRomero,Pietro
inneuralinformationprocessingsystems,29. Lio,YoshuaBengio,etal.2017.Graphattentionnetworks.stat,1050(20):10â€“48550.
[11] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: [34] L.Wang,C.Chen,andH.Li.2022.Linkpredictionofcomplexnetworkbased
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.In oneigenvectorcentrality.InJournalofPhysics:ConferenceSeries,volume2337,
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation page012018(8pp.).2022WorkshoponPatternRecognitionandDataMiningKnowledgeProbingforGraphRepresentationLearning Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
(WPRDM2022),24-26June2022,Wuhan,China. [38] YunxiangZhao,JianzhongQi,QingweiLiu,andRuiZhang.2021.Wgcn:graph
[35] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.Howpowerful convolutionalnetworkswithweightedstructuralfeatures. InProceedingsof
aregraphneuralnetworks?InInternationalConferenceonLearningRepresenta- the44thInternationalACMSIGIRConferenceonResearchandDevelopmentin
tions. InformationRetrieval,pages624â€“633.
[36] YuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang,and [39] HaoZhuandPiotrKoniusz.2021.Simplespectralgraphconvolution.InInterna-
YangShen.2020.Graphcontrastivelearningwithaugmentations.Advancesin tionalConferenceonLearningRepresentations.
neuralinformationprocessingsystems,33:5812â€“5823. [40] JiongZhu,YujunYan,LingxiaoZhao,MarkHeimann,LemanAkoglu,andDanai
[37] HanqingZeng,HongkuanZhou,AjiteshSrivastava,RajgopalKannan,andViktor Koutra.2020.Beyondhomophilyingraphneuralnetworks:Currentlimitations
Prasanna.2020.GraphSAINT:Graphsamplingbasedinductivelearningmethod. andeffectivedesigns.Advancesinneuralinformationprocessingsystems,33:7793â€“
InInternationalConferenceonLearningRepresentations. 7804.