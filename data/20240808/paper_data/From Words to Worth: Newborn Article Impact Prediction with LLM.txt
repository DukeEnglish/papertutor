From Words to Worth: Newborn Article Impact Prediction with LLM
PenghaiZhao1,QinghuaXing1,KairanDou1,JinyuTian1,YingTai3,JianYang1,
Ming-MingCheng12,XiangLi*12
1VCIP,CS,NankaiUniversity 2NKIARI,ShenzhenFutian 3PCALab,NanjingUniversity
zhaopenghai@mail.nankai.edu.cn,xiang.li.implus@nankai.edu.cn
Abstract Lv. I -Unrestricted AIP (previous)
As the academic landscape expands, the challenge of Input Features Method
efficiently identifying potentially high-impact articles
among the vast number of newly published works be-
Citation & Venue Characteristic
comes critical. This paper introduces a promising ap- Citation Counts
proach,leveragingthecapabilitiesoffine-tunedLLMs
SVM
to predict the future impact of newborn articles solely
Is highly-cited?
based on titles and abstracts. Moving beyond tradi-
‚Ä¶
tionalmethodsheavilyreliantonexternalinformation, Article Content ‚Ä¶
theproposedmethoddiscernsthesharedsemanticfea-
Rely on External Info Fair Value
tures of highly impactful papers from a large collec-
tion of title-abstract and potential impact pairs. These Lv. II -Newborn AIP (ours)
semanticfeaturesarefurtherutilizedtoregressanim- Method Target
proved metric, TNCSI , which has been endowed
SP 0.26
with value, field, and time normalization properties.
LLM
Additionally, a comprehensive dataset has been con-
structedandreleasedforfine-tuningtheLLM,contain- Internal Info is Sufficient Decent Value
ing over 12,000 entries with corresponding titles, ab-
stracts, and TNCSI . The quantitative results, with Figure 1: A taxonomy of article impact prediction (AIP):
SP
anNDCG@20of0.901,demonstratethattheproposed sincetherearevirtuallynootherLv.IImethods,the‚Äúnew-
approach achieves state-of-the-art performance in pre- bornAIP‚Äùsegmentrepresentstheproposedapproach,which
dictingtheimpactofnewbornarticleswhencompared predicts future academic impact in a ‚Äúdouble-blind peer-
to competitive counterparts. Finally, we demonstrate review‚Äùmanner.
a real-world application for predicting the impact of
newbornjournalarticlestodemonstrateitsnoteworthy
practicalvalue.Overall,ourfindingschallengeexisting
paradigmsandproposeashifttowardsamorecontent- Recently, as the field of Large Language Model (LLM)
focusedpredictionofacademicimpact,offeringnewin- agent-based automated scientific research systems rapidly
sightsforassessingnewbornarticleimpact. evolves (de la Torre-Lo¬¥pez, Ram¬¥ƒ±rez, and Romero 2023;
Wangetal.2023),articleimpactpredictionhasneverbeen
Introduction more important than it is today. Imitating human experts,
Theemergingfieldofarticleimpactpredictionisbecoming these autonomous systems typically start with identifying
increasingly critical in advancing scientific research. Gen- the most relevant and valuable research literature from ex-
erally,itfocusesonforecastingthepotentialfuturecitation tensive academic articles. Only then do these systems ex-
counts of academic publications by exploiting the external tractandsynthesizeknowledgefromtheretrievedliterature,
datarelatedtothearticle(Xia,Li,andLi2023),suchasearly therebyenablingpracticalapplicationssuchasideagenera-
citationfeature,venuecharacteristics,andauthorreputation, tion (Baek et al. 2024) and compound discovery (M. Bran
etc.Unliketraditionalbibliometricevaluationsthatmeasure etal.2024),etc.Asthesayinggoes,Youcan‚Äôtmakebricks
establishedinfluence,articleimpactpredictiontypicallyen- without straw, article impact prediction has thus become a
compasses a broader range of applications. Large institu- centralcomponentofautomatedresearchsystems.
tionsutilizeitforresearchfundingdecisionsandacademic However, almost all existing impact prediction ap-
promotions. Individuals may also benefit from impact pre- proaches rely on external historical data (Vergoulis et al.
dictions, which help them efficiently identify cutting-edge 2020; Wang et al. 2021; Zhao and Feng 2022; Abbas et al.
articles and remain leading in their fields, especially given 2023;ZhangandWu2024),whichlimitsthepracticalvalue
thehundredsofdailyarXivsubmissionsacrossvariousaca- ofthesemethods.Particularly,forthosenewlyuploadedpa-
demicdisciplines. pers on pre-print websites (e.g., arXiv), the absence of his-toricalcitationdataandpublicationvenueinformationposes ‚Ä¢ New Method: Tailored improvements have been made
challenges for existing methods to make accurate predic- totheTNCSI,andforthefirsttime,wedemonstratethat
tions. Inaddition, althoughmost academic efforts prefer to LLMsarecapableofpredictingthefutureimpactofnew-
predictcitationcounts,thevalidityofcitationcountsthem- bornarticlesina‚Äúdouble-blindreview‚Äùsetup.
selves remains debatable. As the Leiden Manifesto (Hicks ‚Ä¢ NewDataset:Accordingly,wehaveconstructedandre-
et al. 2015) and the DORA Declaration (San Francisco leased the TKPD and NAID datasets. They are used to
2018) indicate, citation count is not well suited for cross- guide ChatGPT in generating topic key phrases and to
disciplinarycomparableevaluationsandshouldnotbeused train state-of-the-art LLMs for accurate article impact
asthesolemetricforassessingresearchimpact.Forexam- predictions,respectively.
ple,apaper(Lietal.2023)withonehundredcitationsmight
‚Ä¢ Application: Finally, we discuss and present an exam-
beunsurprisingintheboomingfieldofartificialintelligence,
pleoftheproposedmethod‚Äôsapplicationinareal-world
butinarelativelynarrowyetequallyimportantfieldsuchas
scenario,specificallyinpredictingtheimpactofjournal
paleontology, the paper with one hundred citations (Turner
articlespublishedin2024,withthehopeofinspiringfur-
2011) could be considered a cornerstone. Since automated
theradvancementsinthebroaderresearchfield.
scientificresearchsystemsmostlystartfromestimatingthe
valueofarticles,suchlimitationsundoubtedlyweakentheir Ourcodeframework,datasetsarereleasedatsway.cloud.
abilitytogatherknowledgefromotherfields,therebyreduc- microsoft/KOH09sPR21Ubojbc.
ingtheefficiencyofknowledgesynthesis.
RelatedWork
Toaddressthepotentialissuesofregardingcitationcounts
as the regression target, we first draw inspiration from the Bibliometrics is a research field that utilizes quantitative
designprinciplesoftheTopicNormalizedCitationSuccess analysis and statistical methods to assess the impact of
Index (TNCSI) (Zhao et al. 2024) and make tailored im- scholarly publications. Typically, bibliometrics can be di-
provements to adapt it for predicting the impact of new- videdintotwomajorcategories:metricsforevaluatingjour-
born papers across various fields. The improved metric is nals and metrics for evaluating individual articles. As the
named TNCSI , where SP stands for the Same Period, LeidenManifesto(Hicksetal.2015)andtheDORADecla-
SP
to highlight that the proposed metric is capable of compar- ration(SanFrancisco2018)recommend,donotusejournal-
ing papers across varying time frames. In response to the basedmetricstomeasurethequalityofindividualresearch
problem of over-reliance on external information, we con- articles.Therefore,inthispaper,wedonotintendtouseany
tendthatthe‚Äúworth‚Äùofanarticlecantypicallybeassessed journal-level bibliometric indicators (such as JIF (Garfield
by ‚Äúwords‚Äù, since key elements such as contribution, nov- 1955)) as inputs or prediction targets. Instead, we focus
elty,andinsightsareoftenreflectedinthetitleandabstract. on bibliometric indicators for individual articles. Tab. 1 il-
Therefore, we try to regress the TNCSI by feeding only lustrates the differences among them. Although FWCI and
SP
the title and abstract of the article to fine-tuning the LLMs RCRareexcellentmetrics,theirnon-normalizednumerical
forreliableimpactpredictions. properties may impair the convergence of neural networks.
Forbetterclarityonthecurrentstateofimpactprediction ProposedbyZhaoetat.,TNCSI(Zhaoetal.2024)features
methods,wesummarizeandintroduceataxonomybasedon aclearphysicalmeaningandfavorablemathematicalprop-
the information required for the prediction (See in Fig. 1). erties,representingtheprobability(rangesbetween0and1)
Thefirstleveliscalledunrestrictedarticleimpactprediction, thatanarticle‚Äôsimpactsurpassesthatofotherarticlesinthe
where predictions are permitted to rely on external histori- samefield.However,TNCSIisinitiallydesignedtoevalu-
cal information, and authors‚Äô reputation; this is the level at atereviewpapersacrossdifferentfieldsandisthereforenot
whichmostcurrentmethodsaresituated.Thesecondlevelis suitableforassessingnormalresearchpapers.Furthermore,
namednewbornarticleimpactprediction,whichparticularly TNCSI primarily focuses on assessing the existing impact
emphasizesmakingpredictionsabouttheimpactonlybased ofareviewpaperanddoesnotnormalizetheimpactofpa-
on the article itself. This task is similar to a double-blind pers published in different years. This may lead to poten-
reviewprocess,wherethemodelpredictsthefutureimpact tialunfaircomparisonsinnewbornarticleimpactprediction
without any author and affiliation information, publication tasks.Therefore,weproposeanimprovedversiontoaddress
details, or early citation data. Such an approach is partic- thelimitationsofTNCSI.Moredetailscanbefoundinthe
ularly valuable for screening newly uploaded manuscripts, Approachsection.
such as arXiv pre-prints and conference papers, as it may ArticleImpactPredictionapproachestypicallyadoptma-
helpresearcherseffectivelyidentifythemostpromisingar- chine learning methods to forecast the future impact of ar-
ticles. In this paper, we focus on the most challenging yet ticles. Most existing methods tend to exploit article statis-
mostvaluabletask:newbornarticleimpactprediction. tical features, author characteristics, journal attributes, and
historical citation data to aid decision trees, LSTM, MLP,
Tosummarize,thecorecontributionsofthisworkareas
and other machine learning algorithms in making predic-
follows:
tions(FuandAliferis2008;Wang,Yu,andYu2011;Qiuand
‚Ä¢ NewTask:Weintroduceataxonomyanddefineanovel Han 2024; Kousha and Thelwall 2024). Ruan et al. (Ruan
task entitled newborn article impact prediction, which etal.2020)aimstoenhancethepredictionaccuracyoffive-
aimstoaccuratelypredictthescholarlyimpactofnewly year citation counts using a four-layer Back Propagation
publishedarticleswithoutexternalinformation. (BP)neuralnetworkbyleveragingmultiplefeaturesrelatedBibliometric Value Field Time Approach
Cites √ó √ó √ó TailoredImprovementtotheTNCSI
FWCI(Colledge2014) √ó ‚úì √ó
As mentioned in the Related Work section, TNCSI suffers
RCR(Hutchinsetal.2016) √ó ‚úì √ó fromcertainlimitations,suchasbeingrestrictedtoevaluat-
TNCSI(Zhaoetal.2024) ‚úì ‚úì √ó ing review papers and only taking into account the cumu-
TNCSI (Ours) ‚úì ‚úì ‚úì lative impact of articles. We conducted a detailed analysis
SP
of its computational process and identified the reasons be-
hind these limitations. First, TNCSI requires a predefined
Table 1: Several article-level bibliometrics for evaluating
prompt template to guide ChatGPT in generating a corre-
scholar Impact: value, field, and time respectively indicate
sponding review research area from the given title and ab-
whetherthemetricisavaluebetween0and1,whetherital-
stract. The original prompt is specifically designed for re-
lows for cross-field comparisons, and whether it is suitable
view papers rather than normal research papers. Therefore,
forthecomparisonofpaperspublishedatdifferenttimes.
usingtheirpromptdirectlyonregularpapersresultsinpoor
performance. Second, TNCSI primarily considers the cu-
mulative impact of an article since its publication. How-
to papers, journals, authors, references, and early citations.
ever, for constructing datasets of the article impact predic-
Ma et al. (Ma et al. 2021) propose a citation count pre-
tion task, this approach may lead to potential issues of un-
diction model that uses early citations and paper semantic
faircomparison.Specifically,paperspublishedearliertypi-
features as input and employs Bi-LSTM for final predic-
cally exhibit higher TNCSI values than recently published
tions. Another notable citation-based machine learning ap-
ones.Thiscouldpotentiallyconfusethenetwork‚Äôslearning
proach exploits static features and time-dependent citation
process,makingLLMdifficulttomodeltherelationshipbe-
featurestopredictpotentiallyexcellentpapers(Hu,Cui,and
tweentextfeaturesanditsimpactvalues.
Lin2023).InABBAS‚Äôswork(Abbasetal.2023),anMLP-
Based on the discussion above, we make tailored im-
based method leveraging only time-related features is pro-
provements to the TNCSI and name the improved metric
posed to make prediction of future citation counts, achiev-
as TNCSI . Similar to the computational procedure of
ingadecentperformancewithanNDCGof0.95.Zhanget SP
TNCSI,theprocedureoftheproposedTNCSI isdivided
al.(ZhangandWu2024)discoverthatemployingdifferent SP
into three steps. In the first step, a well-designed prompt is
modelsforpapersinvariousdomainssignificantlyenhances
utilizedtoguideChatGPT(currentlyrefertogpt-3.5-turbo-
theaccuracyofpredictionbyleveragingearlycitationdata.
0125)toidentifythetopickeyphraseofanarticle.Wehave
De(deWinter2024)attemptstoguideChatGPT-4inscoring
designedandtestedavarietyofprompttemplatesforiden-
over2,000paperabstractsfrommultipleperspectives,find-
tifyingthearticlekeyphrasefromdifferentperspectives.To
ing that the scores have Spearman correlation coefficients
furthermitigateindividualcognitivebiases,weenlistedthe
greaterthan0.4withMendeleyreadership,andacorrelation
help of numerous researchers in the prompt creation pro-
of 0.18 with citation counts. To the best of our knowledge,
cess.Allprompttemplatesaretestedonahuman-annotated
thereiscurrentlynomethodcapableofaccuratelypredicting
datasettoevaluatethecorrespondingperformanceinthekey
theimpactofanarticlebasedsolelyontheinternalcontent.
phrase identification task. The second step involves using
Large Language Models have demonstrated powerful
ChatGPT-generatedkeyphrasestoretrieve1,000relatedpa-
long-form text modeling capabilities and have been widely
persandtheirmata-info(e.g.,citationcounts)fromtheSe-
appliedtovariousNLPtasksoverthepastfewyears,includ-
mantic Scholar API. Unlike TNCSI, which considers cita-
ing dialogue systems, machine translation, sentiment anal-
tioncountsovertheentiretimeframe,TNCSI focuseson
ysis, etc. (Zhao et al. 2023) Many commercial large lan- SP
theconcurrentpaperswithina6-monthwindowbeforeand
guage models (OpenAI 2022, 2023; Google 2024; Kimi.ai
after the publication date. This approach ensures that each
2024) are not openly accessible, which prevents us from
paperiscomparedonlytootherspublishedwithinasimilar
fine-tuningorinstruction-tuningthem.Therefore,weturned
timeframe, thereby minimizing the citation advantage that
ourattentiontoseveralexcellentopen-accesslargelanguage
olderpapersaccumulateduetotheirextendedpresence.As
models. LLaMA series (Touvron et al. 2023; AI 2024) are
a result, this method endows TNCSI with the ability to
advanced language models created by Meta AI, available SP
normalizecitationimpactacrossdifferentpublicationtimes.
in multiple versions ranging from 7B to 70B parameters.
ThefinalstepremainsconsistentwithTNCSI.Thesimpli-
It demonstrates decent performance on most tasks and has
fiedmathematicalexpressionsareshownasfollows:
been widely adopted for various applications. Apart from
LLaMA, there are several other notable open-source large
P(X =x)= Count(px). (1)
language models, such as Qwen (Bai et al. 2023), Mis- C
tral (Jiang et al. 2023), Falcon (Almazrouei et al. 2023), Here, C = 1000 refers to the total number of retrieved
etc. Regardless of the specific large language model, they papers.Count(p )representsthenumberofpaperpwithx
x
were originally developed for autoregressive text genera- citations. P(X = x) is a discrete probability distribution
tion.Inthisstudy,weuseonlythefirstgeneratedtokenfor that describes the probability of a paper having exactly x
numericalregression.Detaileddescriptionsandcomprehen- citationsamongtheretrievedC papers.
siveevaluationsofthesemodelswillbeprovidedintheAp- In their work (Zhao et al. 2024), P(X = x) has been
proachandExperimentsections. thoroughly discussed and is shown to follow an exponen-M
L
L
Key Phrase
S2 Database
Meta-info Retrieval
Figure 2: Flowchart for calculating TNCSI : TNCSI ‚àà [0,1] represents the probability that a paper‚Äôs citation count
SP SP
outperformsotherpapersinthesamefieldandtimeperiod.‚ÄúS2‚ÄùreferstoSemanticScholar.
tiallydecayingdistribution.Therefore,itcouldbeconverted
intoaprobabilitydensityfunctionusingthemaximumlike- w =LLM(w ,w ,...,w ), (3)
t+1 1 2 t
lihood estimation. As shown in Eq. (2), we may derive the
where LLM(¬∑) represents a large language model that pre-
finalTNCSI ‚àà [0,1]bycalculatingthevalueofthecor-
SP dictsthenexttokeninthesequencebasedontheinputtoken
respondingdefiniteintegral:
sequence{w ,w ,...,w }.
1 2 t
(cid:90) cites To facilitate the LLM‚Äôs prediction of a single numerical
TNCSI SP = Œªe‚àíŒªxdx,x‚â•0, (2) value,Weemployasimplemulti-layerperceptron(MLP)to
0 transformw ‚ààRB√ó1√óDintoarealnumberv ‚ààR.Then,
t+1
wherecitesrepresentsthenumberofcitationsthatthepaper thevaluevisfedtoaSigmoidfunction,resultinginthepre-
beingevaluatedhasreceived. dictedTNÀÜCSI ‚àà[0,1].Here,Brepresentsthebatchsize,
SP
D isthedimension,andRdenotesthesetofrealnumbers.
LLMforNewbornArticleImpactPrediction
Theprocesscanberepresentedbythefollowingequations:
Theautoregressivemechanismoflargelanguagemodelshas
beenwell-documented(Zhaoetal.2023).Essentially,these TNÀÜCSI =œÉ(MLP(w )), (4)
SP t+1
decoder-only models generate text in a sequential manner,
with each token prediction relying on the context provided where TNÀÜCSI SP is obtained by passing w t+1 through a
bythepreviouslygeneratedtokens.Suchaparadigmallows MLPandthenapplyingtheSigmoidfunctionœÉtotheMLP
ittofullyleverageunlabeleddataforself-supervisedlearn- output.
ing. Finally,weaimtominimizethemeansquareerror(MSE)
losstoalignthepredictedoutputTNÀÜCSI ofthenetwork
SP
totheTNCSI obtainedfrompreviousstatisticalcalcula-
‚Ä¶ Title ‚Ä¶ Abstract ‚Ä¶ ‚Ä¶ SP
tions.
In practice, the immense number of parameters in large
Tokenizer & Positional Embedding language models requires substantial computational re-
sources for training, which exceeds our practical capac-
ity. Therefore, we adopted low-rank matrix decomposition
(LoRA) (Hu et al. 2021) and model quantization tech-
LoRA niques (Dettmers et al. 2022) to reduce computational re-
sourceconsumptionandacceleratenetworktrainingandin-
ferenceprocesses.Werecommendreadersrefertotheorig-
inalpapersforfurtherdetails.
‚Ä¶
Datasets
Sigmoid MLP
We have constructed a total of two datasets, the Topic Key
Phrase Dataset (TKPD) and Normalized Article Influence
Figure 3: LLM as scholar impact predictor: overall frame- Dataset(NAID).Eachofthesedatasetsservesdifferentpur-
workoftheproposedapproach. poses,whichwillbedescribedinmoredetailbelow.
TopicKeyPhraseDataset:TKPDincludes251entriesen-
In this paper, we maintain the autoregressive generation compassingtitles,abstracts,andcoretaskorfieldnamesof
scheme of the large language model unchanged. However, randomarticlesacrossvariousfieldsinartificialintelligence.
unlike conventional text generation, we focus solely on the To mitigate subjectivity in our study and ensure consistent
first token that the model generates autoregressively in re- annotations, we employ manual labeling of key phrases by
sponse to user input. Specifically, assume the current input aseasonedAIresearcher.Duetothespecializedknowledge
sequenceis{w ,w ,...,w }.Therelationshipbetweenthe required for data annotation, this paper is precluded from
1 2 t
LLMandthegenerationofthenexttokenw canbeex- annotatingarticlesfromnon-AIfields.Nevertheless,webe-
t+1
pressedas: lievethattheinconsistenciesbetweenthedifferentsubfieldswithintheAIfieldaresufficienttosimulatethedifferences ComparisonwithPreviousMethods
betweenthedistinctdisciplines.
This subsection compares the proposed method with pre-
NormalizedArticleImpactDataset:NAIDisusedtotrain
vious state-of-the-art methods. Due to the different predic-
LLMs to predict the impact of articles. It comprises the ti-
tiontargetsofvariousmethods,weemployNDCGtoeval-
tle,abstract,andthecorrespondingTNCSI ,etc.Intotal,
SP uatetheeffectivenessofthesemethodsinidentifyingpoten-
NAID includes more than 12,000 data entries from differ-
tially high-impact papers. As previously mentioned, nearly
ent AI fields. We selected papers with category IDs con-
all methods heavily rely on external data from the papers.
taining ‚Äúcs.CV‚Äù, ‚Äúcs.CL‚Äù, and ‚Äúcs.AI‚Äù that were uploaded
Therefore,toensureafaircomparison,weremovedtheex-
toarXivbetween2020and2022.NAIDisauniformlydis-
ternalinformationrelieduponbypreviousmethods.Details
tributeddataset,meaningthatthesourcesofthepapers,the
ofthereproductionprocessareprovidedintheAppendix.
originalpublicationyearofthepapers,andthecorrespond-
Tab. 2 clearly illustrates the performance differences be-
ingTNCSI valuesareevenlydistributed.Formoredetails
SP tween our proposed method and previous SOTA methods
onhowweconstructthedataset,pleaserefertoourproject
inidentifyingpotentiallyhigh-impactpapers.Theproposed
webpage.
method demonstrates a notable superiority in the newborn
BothTKPDandNAIDarereleasedatoutprojectpage.
article impact prediction setting compared to earlier repre-
sentative works. Most level I methods underperform with-
Experiments
out external information. For example, the LSTM-based
Metrics method (Ma et al. 2021) reports an NDCG of 0.84 when
leveraging external information, but its performance drops
MeanAbsoluteError:MAEisemployedtoassessthepre-
significantly to 0.196 when relying solely on the title and
diction accuracy. It is a measure used to evaluate the dis-
abstract, suggesting a limited capacity to effectively map
crepancy between the predicted value and the ground truth
semantic features to the target TNCSI . The less favor-
y ,whichisdefinedasfollows: SP
i able performance of ChatGPT-generated and LLaMA-3-
n generatedapproachesinidentifyinghigh-impactpaperssug-
1 (cid:88)
MAE= |y ‚àíyÀÜ|, (5) geststhatzero-shotLLM-generatedapproachesstillrequire
n i i
i=1 furtherexploration.
wherenrepresentsthenumberofsamplesinthetestset,y
i PerformanceofVariousLLMs
denotestheactualoutput(e.g.,TNCSI ),yÀÜ standsforthe
SP i
predictedvalue(e.g.,TÀÜNCSI ),|y ‚àíyÀÜ|istheabsolute Asthecoretaskofthispaper,wehaveextensivelymeasured
SP i i
the performance of various large language models on the
difference between the actual and predicted values. Gener-
NAIDtestset.ThedatainTab.3showsthatLLama-3with
ally,alowerMAEindicatesahigheraccuracyofthemodel‚Äôs
its8Bparametersachievedthebestperformance.Addition-
predictions.
ally,weobservethatMAEvaluesarenotalwaysnegatively
Normalized Discounted Cumulative Gain (Ja¬®rvelin and
correlatedwithNDCGvalues.AlowerMAEmightnotnec-
Keka¬®la¬®inen 2000): NDCG is another metric to evaluate the
essarilyleadtoahigherNDCG.TakingFalconandPhi-3as
effectivenessoftheprediction.NDCG,originallydeveloped
an example, we find that although Falcon achieves a better
for recommendation systems to measure the gain of a doc-
MAE, its NDCG is slightly reduced when compared to the
umentbasedonitspositionintherecommendedlist,iscal-
Phi-3.Thismayindicatethatthemodelismoreaccuratein
culatedasfollows:
predicting papers with lower influence but less effective in
DCG@K predictingthosewithhigherinfluence.Consideringthatour
NDCG@K= , (6)
IDCG@K primaryfocusisonpredictinghigh-impactpapers,ahigher
NDCG value is generally more advantageous than a lower
where DCG@K = (cid:80)K i=1(2yÀÜi ‚àí 1)/log 2(i + 1), and
MAEinmostscenarios.
IDCG@K = (cid:80)K (2yi ‚àí1)/log (i+1). K = 20 repre- TheQwenfamilyisselectedtofurtherexploretheeffects
i=1 2
sentsthepositioncutofffortherecommendedlist.NDCGis of model size on the performance of article impact predic-
ametricthatrangesfrom0to1,withscorescloserto1re- tiontasks.ComparedtotheLLaMAseries,theQwenseries
flecting that more influential documents are ranked higher, featuresmoreofficial modelswithsmallerparameter sizes,
signifyingbetterperformance. specifically0.5B,1.5B,and7B.Wetraineachofthesemod-
NormalizedEditDistance(YujianandBo2007):NEDisa elsontheNAIDtrainset,andthetestresultsareillustrated
metrictomeasurethesimilaritybetweentwostringsbynor- inFig.4.Itcanbeobservedthatasthemodelparametersize
malizingtheeditdistancebythelengthofthelongerstring. increases,theperformancecorrespondinglyimproves.
Itisdefinedas:
EffectivenessofPromptEngineering
ED(A,B)
NED(A,B)= , (7) Thispaperconductspromptengineeringontwotasks:iden-
max(|A|,|B|)
tifying the topic key phrase for calculating TNCSI , and
SP
whereED(A,B)istheeditdistancebetweenstringsAand guidingtheLLMtomakepredictions.
B,andmax(|A|,|B|)isthelengthofthelongerstring.The ForIdentifyingKeyPhrase:Wecarriedoutnumerousex-
lowertheNEDvalue,themoresimilarthetwostringsare. periments to test the performance of different models andMethods Ori.Lv. InputFeatureforFairComparison Ori.Target NDCG‚Üë
MLP-based(Ruanetal.2020) I paperlength,referencenumbers,etc. Cites 0.147
LSTM-based(Maetal.2021) I title,andabstract Cites 0.196
ModelEnsemble(ZhangandWu2024) I (Ruanetal.2020)+researchfiled Cites 0.201
MLP-based(Hu,Cui,andLin2023) I thesameto(Ruanetal.2020) IsTop5% 0.464
ChatGPT-generated(deWinter2024) II title,andabstract Score 0.597
LLaMA-3-generated II title,andabstract TNCSI 0.674
SP
Fine-tunedLLaMA-3-based(ours) II title,andabstract TNCSI 0.901
SP
Table 2: Comparison with previous approaches: an upward arrow indicates that a higher value is better, and vice versa. Bold
fontdenotesthebestperformanceamongallmethods.‚ÄòOri.Lv.‚Äôreferstothetaxonomyleveloftheoriginalstudy,while‚ÄòOri.
Target‚Äôdenotesthepredictedtargettypeintheoriginalresearch.SeetheAppendixformorereproductiondetails.
LLMs Size‚Üì MAE‚Üì NDCG‚Üë Memory‚Üì      
Phi-3 3.8B 0.226 0.742 6.2GB      
                 
Falcon 7B 0.231 0.740 8.9GB
     
Qwen-2 7B 0.223 0.774 12.6GB
     
Mistral 7B 0.220 0.850 15.4GB            
LLama-3 8B 0.216 0.901 9.4GB
     
     
     
     
Table3:PerformancecomparisonofdifferentLLMsonthe
NAID test set: size refers to the number of model param-                  
     
eters, and memory stands for the minimum memory usage
     
duringinference.
    %     %   %
 4 Z H Q
promptsontheTPKD.Tab.4reportstheNEDfor4repre- Figure 4: The impact of various model parameters on per-
sentativepromptsontheTPKD.Ultimately,weemploythe formance: the larger the number of model parameters, the
prompttemplatefromthelastrow,alongwithgpt-3.5-turbo- bettertheperformance.
0125,togeneratetopickeyphrases.Extendedexperimental
recordscanbefoundintheSupplementaryMaterialandour
projectpage. descriptionsoftenleadtobetterresults.However,overlyde-
tailed prompts may sometimes cause a slight NDCG de-
UserPromptTemplate NED‚Üì crease.
Identify the research field from the given ti- 0.30 ComparativeAnalysisoftheTNCSI
SP
tleandabstract.YouMUSTrespondwiththe
We have trained LLaMA-3 on articles from different years
keywordONLYinthisformat:xxx
andwithvariousregressiontargetstodemonstratethesupe-
Basedonthetitleandabstract,determinethe 0.29
riorityoftheproposedTNCSI .AsshowninFig.5,when
main area of study for the paper, focusing SP
targetingtheimprovedTNCSI ,themodelprovidesmore
on a keyword that accurately represents the SP
stablepredictionsregardingarticlesfromdifferentyears.Ta-
field. You MUST respond with the keyword
ble6furtherdemonstratesthegeneralizabilityofTNCSI .
ONLYinthisformat:xxx. SP
It enables various types of models to better resist the bias
Giventhetitleandabstractbelow,determine 0.26
accumulated over time. This suggests that TNCSI em-
thespecificresearchfieldbyfocusingonthe SP
powers the model to identify semantic features shared by
main application area and the key technol-
high-impact articles across different years, thereby achiev-
ogy. You MUST respond with the keyword
ingsignificantimprovementsinoveralltaskperformance.
ONLYinthisformat:xxx.
Applications
Table4:Effectivenessofpromptengineering:comparisonof
In this section, we present an intriguing example, journal
varioususerpromptsforidentifyingkeyphrase.
averageimpactprediction,tofurtherdemonstratetheeffec-
tivenessofourmethodinreal-worldapplications.
For Guiding LLM: As shown in Tab. 5, We test several Theoretically,journalsindifferentquartilesareexpected
prompt templates to wrap the title and abstract before in- to exhibit varying average impacts. Therefore, we guide
puttingthemintothefine-tunedLLM.DespitePEFT,varia- the LLaMA-3 to predict the average TNCSI of articles
SP
tionsinprompttemplatesaffectperformance;moredetailed published in 2024 across several journals from different
  ( $ 0
  * & ' 1PromptTemplate NDCG‚Üë Methods Target NDCG‚Üë
Title:{title}\nAbstract:{abstract}. 0.849 MLP-based TNCSI 0.464
Given the provided title and abstract, pre- 0.869 MLP-based TNCSI 0.634
SP
dict the future normalized academic impact
LSTM-based TNCSI 0.373
onascalefrom0(lowestimpact)to1(high-
LSTM-based TNCSI 0.646
est impact). You may consider factors such SP
Fine-tunedLLama-3-based TNCSI 0.865
as the language clarity, novelty of the re-
Fine-tunedLLama-3-based TNCSI 0.901
search, or the claim of state-of-the-art, etc. SP
Title:{title}\nAbstract:{abstract}
Givenacertainpaperentitled{title},andits 0.889 Table 6: Performance comparison using the TNCSI
SP
abstract: {abstract}. Predict its normalized metric: all methods show improvements when targeting
scholarimpact: TNCSISP.Theinputdataofeachmethodisconsistentwith
Givenacertainpaperentitled{title},andits 0.901 Tab.2.
abstract: {abstract}. Predict its normalized
scholarimpact(between0and1):
sions,theproposedapproachmayalsohelpefficientlyiden-
tifyhigh-qualityresearchworthcloserexamination. It may
Table 5: Effectiveness of prompt engineering: comparison
significantly reduce the time researchers spend reviewing
of various prompts for guilding LLMs to predict the future
largevolumesofpapersonarXiv,therebyenhancingoverall
impact.
researchefficiency.
     
 $ O O  \ H D U V        1 R W D E O H  7 R S         
    
 1 R W D E O H  7 R S    
    
                      
               
                                   
               
                 
     
          
     
      4   4   4   4 
TNCSI TNCSISP  - & 5  4 X D U W L O H V
Figure 5: Impact of different prediction targets on perfor- Figure 6: Predicted TNCSI values for journals in differ-
SP
mance: TNCSI SP demonstrates superior performance over ent JCR quartiles: higher quartiles show higher predicted
TNCSIwithtrainingdatafromdifferentyears.
values. To avoid potential conflicts of interest, we denote
Q1, Q2, Q3, and Q4 to represent articles from journals in
JCRquartiles1,2,3,and4,respectively.
JCR quartiles within the field of computer science. Since
LLaMA-3‚Äôs training data only extends up to early 2023, it
is highly unlikely that the model has encountered these ar- Conclusion
ticles, significantly reducing the risk of data leakage. It is Inthispaper,wedemonstratethepotentialofLLMsforpre-
also worth noting that a journal‚Äôs impact factor would be dicting the tailored TNCSI of newborn papers with ti-
SP
significantly influenced by a small number of highly cited tlesandabstractsonly.TheNAIDdataset,comprisingover
papers (Lei and Sun 2020; Leydesdorff 2012). To this end, 12,000 entries, is constructed and utilized to fine-tune var-
weanalyzedover500randomlyselectedarticlesfromvari- ious advanced LLMs. Empirical evaluations demonstrate
ousjournalsacrossdifferentquartilesforimpactprediction, that the LLaMA-3 model, with an MAE of 0.216 and an
focusingontheaveragepredictedTNCSI SP ofthetop5% NDCG@20 of 0.901, significantly surpasses the perfor-
and 25% of notable papers within each quartile. In Fig. 6, manceofpriormethodswhensolelyrelyingoninternalin-
weobserveaclearpositivecorrelationbetweenthepredicted formation.Furthermore,theimpactvaluespredictedbyour
impactofthenotabletop5%ofarticlesandtheirrespective method show a strong positive correlation with the quar-
quartiles. Although the predicted impact of the top 25% of tile rankings of journals for articles published in 2024, il-
articlesintheQ2quartileisslightlyhigherthanthatofQ1, lustratingthepracticalapplicabilityofourapproachinreal-
itisstillconsideredareasonablephenomenon. world settings. Overall, the proposed approach effectively
Beyond journal impact prediction, our system holds estimates a future impact score from 0 to 1 for newly pub-
promise for a variety of other real-world applications. For lishedpapers,presentingconsiderablebenefitsforindividu-
instance, given the vast number of daily pre-print submis- als,institutions,andautomatedscientificresearchsystems.
 ( $ 0
PSISCNT  H J D U H Y $Appendices manner, where the pre-trained LLaMA-3 model generates
autoregressiveoutputstopredictinfluence.
ImplementationDetails
Due to the specialized nature of the field, most papers
In this paper, We partition the NAID into training, valida- are designed to serve specific institutions, resulting in lim-
tion,andtestsetsinaratioof8:1:1.Weconductagridsearch itedavailabilityofopen-sourcecode.Additionally,thehigh
onthevalidationsetandidentifyoptimalsettingsforseveral costofaccessingcertaindatabaseshasfurtherhinderedour
key hyperparameters. The initial learning rate is set to 5e- ability to reproduce experiments from some papers. Con-
5 and is dynamically adjusted based on the effective batch sequently, despite our best efforts to replicate the reported
sizeandtrainingsteps.ThemaximumlengthfortheLLMis methods, there may still be minor differences in the details
1024.TherankrforLoRAissetto16andisonlyappliedto comparedtotheoriginalauthors‚Äôapproaches.Nevertheless,
theqandvmatricesintheself-attentionmechanism.Tofur- we maintain that these discrepancies are unlikely to have a
ther reduce memory consumption, we employ 8-bit model significantimpactontheoverallexperimentaloutcomes.
quantizationtechniques.Allexperimentsarecarriedouton
the server with 8 √ó NVIDIA A40 GPUs. Unless otherwise PreviewoftheTKPDandNAID
stated, all models are trained for 5 epochs. All the metrics
We provide a preview of the TKPD and NAID datasets in
mentioned in this paper are calculated in July 2024. Please
Tab.7andTab.8,respectively.TheTKPDdatasetincludes
note that the citation counts provided by Semantic Scholar
over200entries,consistingoftitles,abstracts,andmanually
aretypicallylowerthanthosedisplayedbyGoogleScholar.
annotated topic key phrases. The NAID dataset comprises
More experiment settings could be found in the provided
over12,000entries,featuringfieldssuchastitles,abstracts,
codeframework.
TNCSI,TNCSI ,open-sourcestatus,andRQMvalues,etc.
SP
Both datasets are available in our Supplementary Material
ReproductionDetailsofPreviousMethods
andprojectpage.
Todemonstratethesuperiorityofourproposedmethod,we
compare it with previous SOTA methods in the Approach Title Topic
section.Inthissubsection,wefurtherexplainhowwerepli- Oracle-MNIST:aDatasetofOracle oracle charac-
catethesemethods. Characters for Benchmarking Ma- terrecognition
Aspreviouslymentioned,articleimpactpredictionmeth- chineLearningAlgorithms
ods at different levels rely on varying data and exhibit dif- Bridging Cross-Lingual Gaps Dur- cross-lingual
ferenttaskcomplexities.Directlycomparingmethodsacross ing Leveraging the Multilingual textgeneration
alllevelsmayleadtopotentialunfairness.Therefore,weex- Sequence-to-Sequence Pretraining
cludedexternalfeaturessuchasjournalscharacteristics,ci- for Text Generation and Under-
tation features, as well as author and affiliation reputation standing
whenmakingpredictionsforlevelImethods. UniSAr:AUnifiedStructure-Aware text-to-SQL
To mitigate potential issues such as gradient explosion, AutoregressiveLanguageModelfor
we normalize the inputs for all MLP-based methods to en- Text-to-SQL
surethatdifferentfeaturesareonaconsistentscale.Specif- Inpainting at Modern Camera Res- image inpaint-
ically, for the work by Hu et al. (Hu, Cui, and Lin 2023), olutionbyGuidedPatchMatchwith ing
which primarily focuses on classifying whether an article Auto-Curation
belongs to the notable top 5%, we convert their approach
into a regression task that represents cumulative impact
Table7:ApreviewoftheTKPD:forclarityofpresentation,
(TNCSI).FortheapproachproposedbyZhangetal.(Zhang
wehaveexcludedtheabstractfield.
andWu2024),wedividethetrainingdataintothreegroups
basedonarXivcategoryIDs:cs.CV,cs.CL,andcs.AI.Then,
wetrainseparatemodelsforeachgroupandutilizethecor-
WillAdditionalInformationBoosting
responding model weights for inference based on the cat-
Performance?
egory of the data during testing. For the ChatGPT-based
method (de Winter 2024), we replicated the approach us- WehavealreadydemonstratedthatLLMsarecapableofpre-
ing the exact prompts provided by the authors. Surpris- dicting future impact using only titles and abstracts. This
ingly, the original performance of ChatGPT in predicting raises the question of whether additional information not
article impact is unsatisfactory (about an NDCG of 0.05). included in the abstract could further enhance prediction
This may be due to the fact that the original method is not performance. Therefore, we design multiple experiments
specifically designed for impact prediction but rather aims to quantitatively analyze whether the availability of a pa-
toinvestigatethecorrelationbetweenChatGPT‚Äôsscoresand per‚Äôsopen-sourcecode,achievementofSOTAperformance,
various factors, and is not specifically designed to predict contribution of a new dataset, and the quality of its refer-
scores that reflect academic impact. Therefore, we emulate ences (Zhao et al. 2024) may boost the performance. The
the prompt used by LLaMA3 to guide ChatGPT (gpt-3.5- additional information is extracted by gpt-3.5-turbo-0125,
turbo-0125)ingeneratingresponses,whichsignificantlyin- whichreadsasmuchofthearticletextaspossible.
creases the corresponding NDCG value to 0.597. LLaMA- Theprompttemplatedisslightlymodifiedfromtheorig-
3-generated refers to employing LLaMA-3 in a chat-based inal one: ‚ÄúGiven a certain paper, Title: {title} Abstract:Title Abstract Cites TNCSI
SP
LoRA: Low-Rank Adaptation of Animportant paradigm ofnaturallanguageprocess- 4421 1
LargeLanguageModels ingconsistsof...
HuBERT: Self-Supervised Speech Self-supervisedapproachesforspeechrepresentation 1793 1
Representation Learning by Masked learningarechallengedbythreeuniqueproblems:(1)
PredictionofHiddenUnits therearemultiple...
YOLOX: Exceeding YOLO Series in Inthisreport,wepresentsomeexperiencedimprove- 2651 0.906
2021 mentstoYOLOseries...
MobileBERT: a Compact Task- Natural Language Processing (NLP) has recently 665 0.963
AgnosticBERTforResource-Limited achieved great success by using huge pre-trained
Devices modelswith...
A Time Series is Worth 64 Words: WeproposeanefficientdesignofTransformer-based 298 0.825
Long-term Forecasting with Trans- models for multivariate time series forecasting and
formers self-supervisedrepresentationlearning.It...
XLM-T:MultilingualLanguageMod- LanguagemodelsareubiquitousincurrentNLP,and 152 0.426
els in Twitter for Sentiment Analysis theirmultilingualcapacityhasrecentlyattractedcon-
andBeyond siderableattention.However,...
Confidence Score for Source-Free Source-free unsupervised domain adaptation 42 0.299
UnsupervisedDomainAdaptation (SFUDA)aimstoobtainhighperformance...
An Improved Dilated Convolutional Crowdmanagementtechnologiesthatleveragecom- 2 0.004
Network for Herd Counting in puter vision are widespread in contemporary times.
CrowdedScenes Thereexistsmany...
Table8:ApreviewoftheNAID:displayingonlykeyfieldsforclarity.
InputType MAE‚Üì NDCG‚Üë ImpactofDifferentTrainingSchemeson
Title&Abstract 0.216 0.901 PredictivePerformance
+OpenAccessStatus 0.214 0.878
In our paper, we adopt one of the most common methods
+ReleaseDataset 0.212 0.915
for fine-tuning LLMs, LoRA (Hu et al. 2021). This sec-
+SOTAClaim 0.215 0.881
tion explores how other fine-tuning approaches may influ-
+RQM(Zhaoetal.2024) 0.211 0.931
ence performance. As shown in Tab. 10, we also experi-
+Allabove 0.209 0.917
ment with several fine-tuning methods, including freezing
the backbone and fine-tuning only the classification head
Table 9: Ablation study on additional information: addi- MLP, as well as employing rsLoRA (Kalajdzievski 2023),
tionalinformationboostspredictionperformance. andDoRA(Liuetal.2024)tofine-tunethemodel.
Trainingschemes MAE‚Üì NDCG‚Üë
LoRA(Huetal.2021) 0.216 0.901
{abstract}. State-of-the-Art Performance: {‚ÄôYes‚Äô or ‚ÄôNo‚Äô}. ClassificationHeadOnly 0.237 0.839
Released a New Dataset: {‚ÄôYes‚Äô or ‚ÄôNo‚Äô}. Code Open Ac- rsLoRA(Kalajdzievski2023) 0.213 0.897
cess: {‚ÄôYes‚Äô or ‚ÄôNo‚Äô}. Reference Quality Metric(on a scale DoRA(Liuetal.2024) 0.217 0.902
fromlowest0tohighest1):{RQM}.Predictitsnormalized
academicimpact(between0and1):‚Äù. Table 10: Comparison of Various Training Schemes: fine-
tuningwithLoRAyieldsbetterresultsthanfine-tuningonly
As observed in Tab. 9, nearly all additional information
theclassificationhead.
contributestoanimprovementintheMAEmetric.However,
onlytheavailabilityofanopen-sourcedatasetandthequal-
ity of references positively impact the NDCG metric. This
ImpactoftheVariousLossFunctiononPredictive
effectmaybeattributedtothefactthatabstractstypicallyal-
Performance
readyincludekeyinformation,suchasSOTAperformance,
andaddingredundantdatamaybringunnecessarycomplex- We investigate the impact of different loss functions on
ity to the LLM. Nevertheless, when the model is provided modelperformance.InadditiontoMSEloss,wetesttheper-
withalloftheinformation,boththeMAEandNDCGmet- formance when using L1, SmoothL1, and BCELoss as the
rics exhibit improvements compared to scenarios where no lossfunctions.ItiscrucialtonotethatforBCELoss,weem-
additionalinformationisincluded.Thissuggeststhatthein- ployPyTorch‚Äôsbuilt-inBCEWithLogitsLossduringtraining
corporation of additional information may enhance overall to enhance numerical stability. The sigmoid function is ap-
performance. pliedseparatelyduringtheinferencephasetonormalizetheoutputs. AI, M. 2024. Meta LLaMA 3: Advancements in Open-
TheexperimentalresultsarepresentedinTab.11,show- SourceLargeLanguageModels. https://ai.meta.com/blog/
ingthatthemodel‚Äôsabilitytoidentifyhigh-impactpapersis meta-llama-3/. Accessed:2024-07-04.
strongest when MSE is used as the loss function. The per- Almazrouei,E.;Alobeidli,H.;Alshamsi,A.;Cappelli,A.;
formanceofL1andSmoothL1issimilar,whichmaybedue Cojocaru,R.;Debbah,M.;Goffinet,E¬¥.;Hesslow,D.;Lau-
to the balanced nature of the NAID dataset. BCELoss per- nay,J.;Malartic,Q.;etal.2023. Thefalconseriesofopen
formsslightlyworsethanSmoothL1,withanNDCGscore languagemodels. arXivpreprintarXiv:2311.16867.
of0.762. Baek,J.;Jauhar,S.K.;Cucerzan,S.;andHwang,S.J.2024.
Researchagent:Iterativeresearchideagenerationoversci-
LossFunction NDCG‚Üë entificliteraturewithlargelanguagemodels.arXivpreprint
arXiv:2404.07738.
MSELoss 0.901
Bai,J.;Bai,S.;Chu,Y.;Cui,Z.;Dang,K.;Deng,X.;Fan,
L1Loss 0.831
Y.;Ge,W.;Han,Y.;Huang,F.;etal.2023. Qwentechnical
SmoothL1Loss 0.787
report. arXivpreprintarXiv:2309.16609.
BCELoss 0.762
Colledge,L.2014. Snowballmetricsrecipebook. Amster-
dam:SnowballMetricsProgramPartners,110:82.
Table 11: Comparison of adopting various loss functions:
delaTorre-Lo¬¥pez,J.;Ram¬¥ƒ±rez,A.;andRomero,J.R.2023.
MSElossdeliversthebestperformanceintermsofNDCG. Artificialintelligencetoautomatethesystematicreviewof
scientificliterature. Computing,105(10):2171‚Äì2194.
deWinter,J.2024.CanChatGPTbeusedtopredictcitation
counts,readership,andsocialmediainteraction?Anexplo-
EthicalStatement
rationamong2222scientificabstracts. Scientometrics,1‚Äì
We are aware of the potential for manipulation through 19.
excessive optimization of titles and abstracts. Researchers Dettmers,T.;Lewis,M.;Belkada,Y.;andZettlemoyer,L.
must refrain from excessively embellishing titles and ab- 2022. Gpt3. int8 (): 8-bit matrix multiplication for trans-
stracts, particularly by making false claims about un- formersatscale. AdvancesinNeuralInformationProcess-
ingSystems,35:30318‚Äì30332.
achieved performance or overly exaggerating the signifi-
cance of their methods, in an attempt to manipulate pre- Fu,L.D.;andAliferis,C.2008. Modelsforpredictingand
dictedimpactvalues. explaining citation count of biomedical articles. In AMIA
Annualsymposiumproceedings,volume2008,222.Ameri-
Due to constraints such as the access frequency limits
canMedicalInformaticsAssociation.
of the Semantic Scholar API, we are unable to construct a
Garfield, E. 1955. Citation indexes for science: A new
larger dataset. Therefore, our proposed method only serves
dimension in documentation through association of ideas.
asapreliminaryexploratoryapproach.Thepredictionsgen-
Science,122(3159):108‚Äì111.
eratedbythismethodareprobabilisticestimatesandshould
Google.2024. Gemini:Google‚ÄôsAIModel. https://www.
never be considered definitive assessments of an article‚Äôs
google.com/gemini. Accessed:2024-07-04.
quality. The method is intended to provide additional in-
sights and must not replace the existing peer-review pro- Hicks, D.; Wouters, P.; Waltman, L.; De Rijcke, S.; and
Rafols, I. 2015. Bibliometrics: the Leiden Manifesto for
cess, which remains essential for maintaining the integrity
researchmetrics. Nature,520(7548):429‚Äì431.
andrigorofacademicresearch.Theauthorsarenotrespon-
sibleforanydecisionsmadebasedonthepredictions. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.;
Wang, S.; Wang, L.; and Chen, W. 2021. Lora: Low-
rank adaptation of large language models. arXiv preprint
Acknowledgments
arXiv:2106.09685.
This research was supported by the Young Scientists Fund Hu,Z.;Cui,J.;andLin,A.2023.Identifyingpotentiallyex-
oftheNationalNaturalScienceFoundationofChina(Grant cellentpublicationsusingacitation-basedmachinelearning
approach. InformationProcessing&Management,60(3):
No.62206134), the Fundamental Research Funds for the
103323.
Central Universities 070-63233084, and the Tianjin Key
LaboratoryofVisualComputingandIntelligentPerception Hutchins,B.I.;Yuan,X.;Anderson,J.M.;andSantangelo,
G.M.2016.Relativecitationratio(RCR):anewmetricthat
(VCIP). Computation is supported by the Supercomputing
usescitationratestomeasureinfluenceatthearticlelevel.
Center of Nankai University (NKSC). This work was sup-
PLoSbiology,14(9):e1002541.
ported by the National Science Fund of China under Grant
Ja¬®rvelin,K.;andKeka¬®la¬®inen,J.2000. IRevaluationmeth-
No.62361166670.
odsforretrievinghighlyrelevantdocuments.InACMSIGIR
Forum,volume51,243‚Äì250.ACMNewYork,NY,USA.
References
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;
Abbas, K.; Hasan, M. K.; Abbasi, A.; Mokhtar, U. A.; Chaplot, D. S.; Casas, D. d. l.; Bressand, F.; Lengyel, G.;
Khan,A.;Abdullah,S.N.H.S.;Dong,S.;Islam,S.;Alboa- Lample, G.; Saulnier, L.; et al. 2023. Mistral 7B. arXiv
neen,D.;andAhmed,F.R.A.2023. Predictingthefuture
preprintarXiv:2310.06825.
popularityofacademicpublicationsusingdeeplearningby Kalajdzievski,D.2023. Arankstabilizationscalingfactor
consideringitastemporalcitationnetworks. IEEEAccess. forfine-tuningwithlora. arXivpreprintarXiv:2312.03732.Kimi.ai. 2024. Kimi.ai. https://kimi.moonshot.cn/. Ac- Xia,W.;Li,T.;andLi,C.2023. Areviewofscientificim-
cessed:2024-07-04. pact prediction: tasks, features and methods. Scientomet-
Kousha, K.; and Thelwall, M. 2024. Factors associating rics,128(1):543‚Äì585.
withorpredictingmorecitedorhigherqualityjournalarti- Yujian,L.;andBo,L.2007.AnormalizedLevenshteindis-
cles:AnAnnualReviewofInformationScienceandTech- tance metric. IEEE transactions on pattern analysis and
nology(ARIST)paper. JournaloftheAssociationforIn- machineintelligence,29(6):1091‚Äì1095.
formationScienceandTechnology,75(3):215‚Äì244. Zhang,F.;andWu,S.2024. Predictingcitationimpactof
Lei, L.; and Sun, Y. 2020. Should highly cited items be academicpapersacrossresearchareasusingmultiplemod-
excludedinimpactfactorcalculation?Theeffectofreview elsandearlycitations. Scientometrics,1‚Äì30.
articles on journal impact factor. Scientometrics, 122(3): Zhao, P.; Zhang, X.; Cheng, M.-M.; Yang, J.; and Li, X.
1697‚Äì1706. 2024. A Literature Review of Literature Reviews in Pat-
Leydesdorff, L. 2012. Alternatives to the journal impact tern Analysis and Machine Intelligence. arXiv preprint
factor:I3andthetop-10%(ortop-25%?)ofthemost-highly arXiv:2402.12928.
citedpapers. Scientometrics,92(2):355‚Äì365.
Zhao, Q.; and Feng, X. 2022. Utilizing citation network
Li,Y.;Hou,Q.;Zheng,Z.;Cheng,M.-M.;Yang,J.;andLi, structuretopredictpapercitationcounts:Adeeplearning
X.2023. Largeselectivekernelnetworkforremotesensing approach. JournalofInformetrics,16(1):101235.
objectdetection. InProceedingsoftheIEEE/CVFInterna-
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou,
tionalConferenceonComputerVision,16794‚Äì16805.
Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023.
Liu, S.-Y.; Wang, C.-Y.; Yin, H.; Molchanov, P.; Wang, A survey of large language models. arXiv preprint
Y.-C. F.; Cheng, K.-T.; and Chen, M.-H. 2024. Dora: arXiv:2303.18223.
Weight-decomposed low-rank adaptation. arXiv preprint
arXiv:2402.09353.
M. Bran, A.; Cox, S.; Schilter, O.; Baldassari, C.; White,
A.D.;andSchwaller,P.2024. Augmentinglargelanguage
modelswithchemistrytools. NatureMachineIntelligence,
1‚Äì11.
Ma,A.;Liu,Y.;Xu,X.;andDong,T.2021.Adeep-learning
basedcitationcountpredictionmodelwithpapermetadata
semanticfeatures. Scientometrics,126(8):6803‚Äì6823.
OpenAI.2022. ChatGPT:optimizinglanguagemodelsfor
dialogue. https://openai.com/blog/chatgpt/. Accessed 25
December2023.
OpenAI.2023.GenerativePre-trainedTransformer4(GPT-
4).https://openai.com/gpt-4/.Accessed25December2023.
Qiu,J.;andHan,X.2024.AnEarlyEvaluationoftheLong-
Term Influence of Academic Papers Based on Machine
LearningAlgorithms. IEEEAccess,12:41773‚Äì41786.
Ruan,X.;Zhu,Y.;Li,J.;andCheng,Y.2020.Predictingthe
citationcountsofindividualpapersviaaBPneuralnetwork.
JournalofInformetrics,14(3):101039.
SanFrancisco,D.2018. SanFranciscodeclarationonre-
searchassessment.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
A.;Babaei,Y.;Bashlykov,N.;Batra,S.;Bhargava,P.;Bhos-
ale, S.; et al. 2023. Llama 2: Open foundation and fine-
tunedchatmodels. arXivpreprintarXiv:2307.09288.
Turner, D. 2011. Paleontology: a philosophical introduc-
tion. CambridgeUniversityPress.
Vergoulis,T.;Kanellos,I.;Giannopoulos,G.;andDalama-
gas,T.2020. SimplifyingImpactPredictionforScientific
Articles. ArXiv,abs/2012.15192.
Wang, H.; Fu, T.; Du, Y.; Gao, W.; Huang, K.; Liu, Z.;
Chandak,P.;Liu,S.;VanKatwyk,P.;Deac,A.;etal.2023.
Scientificdiscoveryintheageofartificialintelligence. Na-
ture,620(7972):47‚Äì60.
Wang, K.; Shi, W.; Bai, J.; Zhao, X.; and Zhang, L.
2021. Prediction and application of article potential ci-
tations based on nonlinear citation-forecasting combined
model. Scientometrics,126:6533‚Äì6550.
Wang,M.;Yu,G.;andYu,D.2011.Miningtypicalfeatures
forhighlycitedpapers. Scientometrics,87(3):695‚Äì706.