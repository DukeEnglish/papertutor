Hard to Explain: On the Computational Hardness
of In-Distribution Model Interpretation
GuyAmira,*,1,ShahafBassana,1 andGuyKatza
aTheHebrewUniversityofJerusalem,Jerusalem,Israel
Abstract. The ability to interpret Machine Learning (ML) mod- Tobridgethisgap,workbyBarceloetal.[12]proposesassessing
elsisbecomingincreasinglyessential.However,despitesignificant theinterpretabilityofanMLmodelbyexaminingthecomputational
progressinthefield,thereremainsalackofrigorouscharacteriza- complexityinvolvedingeneratingvarioustypesofexplanationsforit.
tionregardingtheinnateinterpretabilityofdifferentmodels.Inan TheideaisthatifexplanationscanbeefficientlyobtainedforanML
attempttobridgethisgap,recentworkhasdemonstratedthatitis model,itcanbeconsideredinterpretable.Conversely,ifobtaining
possibletoformallyassessinterpretabilitybystudyingthecomputa- explanations is computationally intractable, the model is deemed
tionalcomplexityofexplainingthedecisionsofvariousmodels.In uninterpretable. For example, while obtaining certain explanation
thissetting,ifexplanationsforaparticularmodelcanbeobtained formsfordecisiontreescanbecomputedinpolynomialorevenlinear
efficiently,themodelisconsideredinterpretable(sinceitcanbeex- time, these same tasks become NP-hard for neural networks [12,
plained‚Äúeasily‚Äù).However,ifgeneratingexplanationsoveranML 47,44].Thisprovidesrigorousmathematicalevidencethatneural
modeliscomputationallyintractable,itisconsidereduninterpretable. networksareindeedlessinterpretablethandecisiontreesinthese
Priorresearchidentifiedtwokeyfactorsthatinfluencethecomplexity contexts.
of interpreting an ML model: (i) the type of the model (e.g., neu- Thecomputationalcomplexityofobtainingexplanationswasstud-
ralnetworks,decisiontrees,etc.);and(ii)theformofexplanation iedinavarietyofdifferentsettings[12,89,15],inwhichthecom-
(e.g.,contrastiveexplanations,Shapleyvalues,etc.).Inthiswork,we putational complexity is typically analyzed along two main axes:
claimthatathird,importantfactormustalsobeconsideredforthis (i)themodeltypeand(ii)theexplanationform.Forexample,comput-
analysis‚Äîtheunderlyingdistributionoverwhichtheexplanationis ingShapleyvalueexplanationsfordecisiontreescanbeobtainedin
obtained.Consideringtheunderlyingdistributioniskeyinavoiding polynomialtime[8,88],whileobtainingminimumsizecontrastive
explanationsthataresociallymisaligned,i.e.,conveyinformation explanationsforneuralnetworksisNP-complete[12].
thatisbiasedandunhelpfultousers.Wedemonstratethesignificant
TheDistributionComponent.Inmanyexplainabilitymethods,un-
influenceoftheunderlyingdistributionontheresultingoverallinter-
derstanding the rationale behind a specific input prediction often
pretationcomplexity,intwosettings:(i)predictionmodelspairedwith
involvesdefininganexplanationthatsatisfiescertainpropertiesin anexternalout-of-distribution(OOD)detector;and(ii)prediction
inputssimilartotheonebeinginterpreted.Forinstance,inputsthat
modelsdesignedtoinherentlygeneratesociallyalignedexplanations.
areidenticaltotheoriginaloneinmostfeatures,withdifferences
Our findings prove that the expressiveness of the distribution can
inonlyafew.Thisapproachcanbeproblematicbecausethesenew
significantlyinfluencetheoverallcomplexityofinterpretation,and
inputsmightbeout-of-distribution(OOD),andmaydeviatesubstan-
identifyessentialprerequisitesthatamodelmustpossesstogenerate
tiallyfrominputsofinterest.Hence,theOODinputsmayaffectthe
sociallyalignedexplanations.Weregardthisworkasasteptowardsa
explanationinunexpectedways,andconveyunintuitiveinformation
rigorouscharacterizationofthecomplexityofgeneratingexplanations
tousers.Haseetal.[38]refertoexplanationsthatdisregardtheinput
forMLmodels,andtowardsgainingamathematicalunderstanding
distributionassociallymisaligned,i.e.,conveyinformationthatis
oftheirinterpretability.
biasedandunhelpfultousers.
This general OOD phenomenon in explanations is termed ‚Äúthe
1 Introduction OODproblemofexplainability"[38]andisencounteredinnumer-
ousexplanationforms,includingcounterfactualexplanations[74],
EnsuringtheinterpretabilityofMLmodelsisbecomingincreasingly contrastiveexplanations[96,36],sufficientexplanations[38,96,36],
vital,asitenhancestheirtrustworthiness,particularlywhendeployed andShapleyvalues[83].Therefore,manypracticalexplanationtech-
insafety-criticalsystems[43].However,despitesignificantadvance- niquesaimtomitigatetheimpactofOODinstances,makingthisa
mentsinthefield,thereremainsanotablelackofmathematicalrigor crucialaspectofcomputingmorepreciseexplanations[59,20,95,
inunderstandingtheinherentinterpretabilityofvariousMLmodels. 38,85,100,84].
Forinstance,manyfundamentalclaimswithininterpretability,such Inthiswork,wearguethatevaluatingthecomputationalcomplexity
as‚Äúdecisiontreesaremoreinterpretablethanneuralnetworks",are ofexplainingthedecisionofamodel,shouldnotrelysolelyonthe
oftenregardedasfolkloreandlacksufficientmathematicalrigor. model type and the explanation form, but also on the underlying
distributionoverwhichtheexplanationiscomputed.Thedistribution
‚àóCorrespondingAuthor.Email:guy.amir2@mail.huji.ac.il. componentiscrucialforensuringthatthecomputedexplanationsare
1Equalcontribution. sociallyalignedandmeaningful.Inthispaper,weillustratetheimpact
4202
guA
7
]GL.sc[
1v51930.8042:viXraof this factor on the overall interpretation complexity, in various taskofobtaininganalignedinterpretationofthemodelmaybesub-
settingsandscenarios. stantiallymorecomplexthanthemisalignedform.
InSec.5westudythespecificcaseofself-alignedexplanations.
A Running Example. Consider the task of classifying low-
Here,ourfocusshiftsfromrelyingonanexternalOOD-detection
dimensional images as either ‚Äú0" or ‚Äú1". Due to the simplicity of
modeltothepossibilityofutilizingasinglemodelthatderivesaligned
thistask,letusassumethatitcanbeeffectivelylearnedusingasim-
explanations.Specifically,wefocusonthecaseofefficientlyproduc-
pledecisiontreeclassifier.Givenanimageclassifiedas‚Äú0",wecan
ingasinglemodelthatservesbothasaclassifierandasanOOD
interpretthepredictionofthedecisiontreeusingalocal,post-hoc
detector,giventhateachoftheseisrealizedseparatelybythesame
explainabilitymethod.Forinstance,wecanobtainasufficientreason
modelclass.Asweprove,thiscapabilitycorrelatestothedegreeof
S [47,25,13]:asubsetoffeatures(inthiscase,pixels)that,when
expressivenessinherentinvariousMLmodels‚Äîwhilesomemodel
fixed,ensuretheimageremainsclassifiedas‚Äú0‚Äù,regardlessofthe
typespossesstherequiredlevelofexpressiveness,othersdonot.We
assignmentoftheadditionalfeaturesS.Fortunately,sincethistask
provetheseinsightsforspecificmodeltypesandshowthat,assuming
waslearnedbyadecisiontreeclassifier,obtainingalocallyminimal
PÃ∏=NP,bothneuralnetworksanddecisiontreeshavethecapabilityto
sufficientreasoncanbeachievedinpolynomialtime[44].
deriveself-alignedexplanations,whilelinearclassifiersdonot.
However,despitetheirappeal,sufficientreasons,similarlytoother
Furthermore,relatedworkiscoveredinSec.6.Weconcludein
explanationforms,sufferfromtheOODproblemofexplainability[38,
Sec.7,anddiscussthelimitationsofourtheoreticalframework,as
96,36].Inthisparticularcase,thesufficientreasonSmaytakeinto
wellaspotentialfutureworkinSec.8.
accountOODassignmentsoverS.Inotherwords,settingthepixels
Duetospacelimitations,weprovideonlyconciseoverviewsofthe
ofStopartialimagesthatareOOD(e.g.,imagesfeaturingunrelated
proofsofourvariousclaims,andreferthereadertotheappendixfor
digits,orcats)mightresultintheimagebeingclassifiedas‚Äú1‚Äù.This
thecomprehensiveandmoredetailedproofs.
willprecludeSfrombeingasufficientreason‚Äîevenifitisonewhen
takingintoaccountonlythecontextofinterest(i.e.,allin-distribution
imagesofthedigits‚Äú0‚Äùor‚Äú1‚Äù). 2 Preliminaries
Acommonsolutionforbridgingthisgapistotrainanothermodel
Domain
todetectOODinputs,andthenuseittodismantletheeffectofany
misleadingassignment[59,20,95,38,85,100].However,thetaskof
Weassumeasetofnfeaturesx=(x ,...,x ),wherethedomain
1 n
OODdetectionisconsideredverychallenging,bothintheory[32,73]
ofeachfeatureisx ‚àà{0,1}.Theentirefeaturespaceisdenotedas
i
andinpractice[41,80,16]‚Äîasmodelingthefeaturedistribution F={0,1}n.Weseektolocallyinterpretthepredictionofabinary
isoftenharderthantheoriginalpredictiontask[83].Hence,obtain- classifierf : F ‚Üí {0,1},i.e.,givenaninputx ‚àà F,toexplainthe
inganOODclassifiermayrequiretrainingaveryexpressivemodel,
predictionf(x)oftheclassifieroverthisspecificinput.Wefollow
suchasagenerativemodelthatapproximatesthedomaindistribution
commonpracticeinthefield,andconcentrateonBooleaninputand
p (x).Forourrunningexample,forinstance,learningtodistinguish
œï outputvalues,tomakethepresentationclearer[7,89,12].However,
betweenin-distributionimages(‚Äú0‚Äùor‚Äú1‚Äù)andOODimages(any
manyofourfindingsarealsoapplicabletoscenariosinvolvingreal-
otherpossibleimage)maybeasubstantiallyhardertaskthanlearning
valueddata.
toclassifyimagesof‚Äú0‚Äùand‚Äú1‚Äù.Suchataskmayrequiretheuseof
amuchmoreexpressivemodel,suchasadeepgenerativeneuralnet-
work.ThecomplexityofobtainingasufficientreasonSthatignores ComplexityClassesandSecond-OrderLogic(SOL)
theeffectofanyOODassignmentmaythusbemuchgreaterthanthat
Thepaperassumesbasicfamiliaritywiththecommoncomplexity
ofsimplyexplainingthedecisiontreeclassifier,withoutconsidering
classesofpolynomialtime(PTIME)andnondeterministicpolynomial
thedistribution.Revisitingourrunningexample,thefindingsinthis
time(NP,co-NP).Thesecondorderofpolynomialhierarchy,i.e.,Œ£P,
studydemonstratethatperformingthistaskisindeedNP-hard,de- 2
whichisbrieflymentionedinthepaper,isthesetofproblemsthat
spitethefactthatcomputingsuchanexplanationwithoutdistribution
becomemembersofNPgivenanoraclethatsolvesco-NPproblems
alignmentcanbedoneinpolynomialtime.
inO(1).Wealsodiscusstheclass#P,whichcorrespondstothetotal
PaperStructure.InSec.2,westartbycoveringtherelevantback- numberofacceptingpathsofapolynomial-timenondeterministicTur-
groundforthiswork.Next,inSec.3,weexamineawidevarietyof ingmachine.ItiswidelybelievedthatPTIME‚ääNP‚ääŒ£P‚ää#P[10].
2
explanationforms,suchassufficiency-based,contrastive-based,and WeusethecommonconventionL ‚â§ L todenoteapolynomial-
1 p 2
counting-basedexplanations,andstudyhowtheycanbeformalized timereductionfromlanguageL toL ,andL = L toindicate
1 2 1 p 2
tomaintainsocialalignment.Specifically,wedelveintothecommon thatsuchareductionexistsinbothdirections.
scenariowheretheclassificationmodeliscoupledwithanadditional Thepaperalsomakesuseofsecond-orderlogic(SOL)formulas
component‚ÄîanOODdetector.Thisdetectorplaysacrucialrolein ‚Äî a generalization of the first-order predicate logic. In both logic
mitigatingtheimpactofOODcounterfactualsinexplanations,and forms,existentialoruniversalquantifiersareappliedtoeachvariable
canbeusedtoalignvariousexplanationformswithadistributionof orsubsetthereof,sothattheformulaevaluatestoeithertrueorfalse.
interest.Weproceedtodemonstratethatdiverseexplanationformscan However,wechoseSOLformulasforourabstractionduetotheirhigh
beunifiedthroughasingleframework,whichcapturestheirshared expressivity(incontrasttofirst-orderlogicqueriessuggestedin[7]),
structure. Given an OOD detector, this framework can be used to astheycanalsoencodeanexplanationsize,whichisinfeasiblewith
preservethealignmentofeachoftheseexplanations;aswellasto FOL.Assuch,SOL-basedqueriesarerigorousenoughtoenablethe
studythecomputationalcomplexityofobtainingthem. formulationofgeneralproofsthatholdforanyexplanationwithin
InSec.4weprovethatforanyexplanationmatchingourabstract this framework. For each SOL formula Q, we define #Q as the
form, the complexity of interpreting a model is dominated by the correspondingcountingproblemoverthatformula‚Äîwhichcounts
complexity of interpreting an OOD detector for the same type of thenumberofsatisfyingassignmentsforQ.Givenafinitenumber
explanation.SinceOODdetectioniscomputationallyhard[32],the ofinputs,implyingafinitelogic-basedmodel,eachSOLformulaisassociatedwithaspecificcomplexityclasswithinthepolynomial often,andalignswithcommonlyusedexplainabilitytechniques[77].
hierarchy,andwithacorrespondingcountingclass. Formallyput:
‚àÄ(z‚ààF). [f(x S;z S¬Ø)=f(x)] (1)
ExplainabilityQueries
where(x S;z S¬Ø)denotesanassignmentinwhichthevaluesofSare
We follow prior work [12, 15] and define an explainability query, takenfromxand,theremainingvalues(i.e.,fromS),aretakenfrom
denotedQ,whichrepresentssomeformofinterpretation.Qtakes z.
both f and x as inputs, and it outputs information regarding the GivenacontextC,indicatedbyœÄ,asociallyalignedsufficient
interpretationoff(x).Inlinewithpreviouswork[69,89,7,9,15], reasonisdefinedasfollows[96,36]:
ouremphasisisonexplainabilityqueriesthatoutputananswertoa
decisionproblem‚Äîprovidingadefiniteyes/noansweror,inthecase ‚àÄ(z‚ààF). [œÄ(x S;z S¬Ø)=1‚Üíf(x S;z S¬Ø)=f(x)] (2)
ofQbeingacountingproblem,anumericalvalue.Forexample,Q
Awidelyobservedconventionintheliteratureisthatsmallersuf-
canprovideayes/noanswertothequestionisaspecificsubsetof
ficientreasons(relativetothesizeof|S|)aremoremeaningfulthan
featuresasufficientreason?Itcanalsocountthenumberofpossible
largerones[47,12,37].Consequently,itisinterestingtoconsidercar-
assignmentsinwhichthepredictionisaltered,ormaintained.
dinallyminimalsufficientreasons.Clearly,thesecanalsobeobtained
withrespecttoœÄ.Thisleadsustoourfirstexplainabilityquery:
Models
MSR(MinimumSufficientReason):
Thetechniquespresentedinthisworkareapplicabletoadiversesetof Input:Modelf,inputx,contextindicatorœÄ,andintegerk.
modelclasses.Still,wefocusourattentiononafewpopularmodels, Output:Yes,ifthereexistsasufficientreasonSforf(x)withrespect
locatedattheextremitiesoftheinterpretabilityspectrum:decision toœÄsuchthat|S|‚â§k,andNootherwise.
trees,linearclassifiers,andneuralnetworks.Specifically,weaddress
Wenotethatwecanconsiderthecaseofsociallymisalignedqueries
FreeBinaryDecisionDiagrams(FBDDs),whichserveasanextension
asatrivialcaseofthisdefinition,inwhichthecontextindicatoris
ofdecisiontrees,alongwithPerceptronsandMulti-LayerPerceptrons
the constant function œÄ := 1, indicating the entire input space as
(MLPs)employingReLUactivations.Anexactformalizationofthese
in-distribution.
modelsappearsintheappendix.
Contrastive/Counterfactual-BasedExplanations.Adifferentap-
proachtointerpretingamodelisbyobservingsubsetsoffeatures
3 SociallyAlignedExplainabilityQueries
that, when altered, may cause the classification of the model to
ContextIndicator change[47,12].Thesearereferredtoascontrastiveexplanations
orcontrastivereasons,andthecorrespondingvaluesarereferredtoas
TocopewiththeundesiredeffectsofOODinputassignments,we counterfactualexplanations.WedefineasubsetS ‚äÜ{1,...,n}as
considersomecontext C ‚äÜ Foverwhichanexplanationistobe contrastiveifalteringitsvaluesmaycausetheoriginalclassification
provided.Intuitively,contextCdenotestheentirepotentialsetof f(x)tochange:
in-distributioninputsthatwetakeintoconsiderationwhenproviding
anexplanation,whiledisregardingtheeffectofanyOODassignment ‚àÉz‚ààF. [f(x S¬Ø;z S)Ã∏=f(x)] (3)
fromF\C.BecausedescribingthecontextCexplicitlyisclearly
ToavoidcounterfactualOODassignments,acontrastivesubset
non-trivial,inourframework,weinsteadassumetheexistenceofa
contextindicatorœÄ :F‚Üí{0,1}:abinaryclassifieroveraspecific S can be obtained with respect to a context indicator œÄ [96], by
encoding:
contextC,i.e.,œÄ(x)=1 .
{x‚ààC}
Naturally, assuming the existence of a context indicator œÄ that ‚àÉz‚ààF. [œÄ(x S¬Ø;z S)=1‚àßf(x S¬Ø;z S)Ã∏=f(x)] (4)
perfectlycapturesthedesiredcontextCisnon-trivialaswell.For
instance, in our running example, this requires œÄ to identify any Similarly to sufficient reasons, smaller contrastive reasons tend
possibleimageofeither‚Äú0‚Äùor‚Äú1‚Äù.Nevertheless,practicaltoolswere tobemoremeaningful.Here,too,itisusuallymoreinformativeto
showntobeabletoapproximatesuchdomains,forexample,byusing focusoncardinallyminimalcontrastivereasons,asexpressedinthe
generative-model-based OOD classifiers, trained to learn the data followingexplainabilityquery:
distributionp (x)[92,63].Intheseparticularscenarios,theindicated
œï
Ccanbeseenasamereapproximationofthetrue,intendedcontext. MCR(MinimumChangeRequired):
Input:Modelf,inputx,contextindicatorœÄ,andintegerk.
Output: Yes, if there exists some contrastive reason S such that
SociallyAligningExplainabilityQueries
|S|‚â§kforf(x)withrespecttoœÄ,andNootherwise.
Modelinterpretabilityissubjective,andthishasledtothedesignof
Counting-BasedExplanations.Finally,anothercommonexplainabil-
multipleformsofexplanationsinrecentyears.Wefocushereona
ityformisbasedonexploringthenumberofassignmentcompletions
fewwidelyusedexplanationforms,andanalyzethemrigorously.
formaintaining(oraltering)aspecificclassification[52,28,89].As
Sufficiency-Based Explanations. A common definition of an ex- withpreviousexplanationforms,weredefinetheproblemtoavoid
planationforamodelf‚Äôsdecisionwithrespecttoaninputxisthat countingOODcompletions,whichmaycausethesocialmisalignment
ofasufficientreason[47,25,13].Asufficientreasonisasubsetof ofthecorrespondinginterpretation.Inordertodoso,wedefinethe
featuresS ‚äÜ{1,...,n}suchthat,whenfixedtothecorresponding completioncountcofSwithrespecttoœÄas:
valuesinx,determinethatthepredictionremainsf(x),regardlessof
theotherfeatures‚Äôassignments[12,69].Thisnotationisusedquite c(S):=|{z‚àà{0,1}|S|,œÄ(x S¬Ø;z S)=1,f(x S¬Ø;z S)Ã∏=f(x)}| (5)appendix.Inessence,thisabstractformcapturesvarious(logically
CC(CountCompletions): expressible)explanationformulations,overwhichwecandismantle
Input:Modelf,inputx,contextindicatorœÄ,andsubsetoffeatures theeffectofOODcounterfactuals.
S. By using this single, broader form of Q, we are able to prove
Output:Thecompletioncountc(S)off(x)withrespecttoœÄ. generalpropertiesregardingsociallyalignedexplainabilityqueries,
anddeducethecomplexityofinterpretingthesequeriesinvarious
ThewidelyusedShapleyvalues[83],whichserveasacommon
settings.
formofexplanation[64,84],canalsobecharacterizedasatypeof
countingproblem[88,8].
4 TheComplexityofObtainingSociallyAligned
Explanations
AbstractQueryForm
AGeneralFramework
Manyoftheexplanationformsstudiedintheliterature,includingthe
aforementionedones,becomemoremeaningfulwhentheeffectof Toevaluatethecomputationalcomplexityofinterpretingaspecific
OODcounterfactualsarereduced.Foranalyzinghowdistributions classofmodels,denotedasC ,itisusefultodefineQ(C )asthe
M M
affect the complexity of obtaining explanations not only for one computationalproblemrepresentedbyinterpretingasetofmodels
specificexplanation,butforawidearrayofexplanationforms,we withintheclassC withrespecttoanexplainabilityqueryQ[12,
M
proceedtodefineabstractexplainabilityqueries.Wethenprovide 15].Toillustrate,letusconsidertheclassofmulti-layerperceptrons
generalresultsregardingthecomputationalcomplexityofobtaining denotedasC .MSR(C )isthenthecomputationalproblemof
MLP MLP
thisabstractformofexplanation. obtainingcardinallyminimalsufficientreasonsforanMLP,givenan
Thetaskofobtainingeachoftheexplanationtypesdiscussedso inputx.
farcanbeachievedbyinvokingadecisionprocedurefordetermining Whilethisformalizationishelpfulforassessingtheinterpretability
whetherornotf(x S¬Ø;z S)=f(x)(orforsolvingthecorresponding ofaspecificmodeltype,itdoesnotconsidertheunderlyingcontext
countingproblem).Thesedecisionproceduresreceiveapartialas- andthus,itmayproducesociallymisalignedexplanations.
signment(x S¬Ø;z S)ofagiveninputx,whichfixessomefeaturesofx Werevisitourrunningexample,whereourmodelf representsa
whileallowingtheresttochangeaccordingtoanarbitraryz;andtheir decisiontree.Wefurtherassumethatthedecisionofwhetherx‚ààC
goalistodeterminewhethertheseassignmentspreserve,oralter,the (orequivalently,whetherxisin-distribution)islearnedbyanother
classificationoutcome. model,e.g.,adeepneuralnetwork.Inthisscenario,thecontextindi-
The task of deciding whether or not f(x S¬Ø;z S) = f(x) can, in catorœÄbelongstoadifferentclassthanf (whichisinC M).Insuch
turn,beformulatedasanSOLformula,SOL ¬¨f,whichencodesthat acase,weshouldposeadifferenttypeofquestionthatassessesthe
f(x S¬Ø;z S) Ã∏= f(x) (or, again, the counting problem over that for- computationalcomplexityofprovidingasociallyalignedexplanation
mula).IfSOL ¬¨f isfalse,thentheanswertotheoriginalproblemis foraninstanceclassifiedbyf.Specifically,weneedtodetermine
affirmative;andotherwise,itisnegative. thecomputationalcomplexityofinterpretingamodelf ‚ààC while
M
Therelevantformulaisfullyquantified,inamannerthatrepresents ensuringitsalignmentwithacontextindicatorfunctionœÄ‚ààC .As
œÄ
aspecificexplanationform.Forexample,contrastivereasonqueries mentionedearlier,inmanyinstances(includingourexample),œÄcor-
checkwhetherthereexistsanyassignmentleadingtoamisclassifica- respondstoamoreexpressivefunctionthanf,potentiallydominating
tion,whereassufficientreasonqueriesaskwhethertheclassification theoverallcomplexity.Therefore,weintroducethefollowingnotion
stays constant for all possible completions. The goal is to eventu- thatenablesustoassessthecomputationalcomplexityofmodelsin
allydeterminewhethertheformulaistrueornot,andequivalently‚Äî C withrespecttoaclassofcontextindicatorsC .
M œÄ
whethertheexplanationiscorrect.
Intheappendix,weshowhoweachofthepredefinedexplainability Definition 3. Given an explainability query Q, a class of predic-
queriescanbeformalizedusingthisnotion,inwhichMSR(Minimum tion models C M, and a class of context indicators C œÄ, we define
SufficientReason)andMCR(MinimumChangeRequired)arepos- Q(C M,C œÄ)asthecomputationalproblemofQdefinedbytheset
siblesolutionstoanunderlyingsatisfiabilityqueryoverSOL ¬¨f,and offunctionswithinC M,withrespecttothecontextsinducedbythe
CC(CountCompletions)isthecountingsolutionof#SOL ¬¨f.This functionsofC œÄ.
canalsobeextendedtoadditionalexplanationforms.
Forourrunningexample,Q(C ,C )denotesthecomputational
DT MLP
Definition 1. Let SOL be an SOL formula encoding the query complexityofsomeexplainabilityqueryQ,giventhatourclassifica-
¬¨f
f(x S¬Ø;z S) Ã∏= f(x).Wedefineanabstractquery,Q,thatreceivesf tionmodelisadecisiontreeandtheOODdetectorisamulti-layer
andxasinputs,andanswerswhetherSOL istrue.Forthecounting perceptron.Wenotethat,similarlytothepreviouslystudiedevalu-
¬¨f
case,Qreturnsthecountingof#SOL ¬¨f. ationofQ(C M)[12],theformalizationofQ(C M,C œÄ)considersa
‚Äúworst-case‚Äùscenarioofthecorrespondingalignment,andnotany
Next,weadjustthisabstractqueryformtoprovideonlysocially
parameter-specificconfiguration.Thisiscapturedbyassessingthe
aligned explanations. This is performed by incorporating into the
correspondingcomplexitywithrespecttoaclassofpredictionmodels
formulatheadditionalconstraintœÄ(x S¬Ø;z S)=1,whichguarantees
andaclassofdistributionindicators.
thatanyexplanationthatsatisfiesthequeryisalsoin-distribution.
Definition2. LetSOL beanSOLformulaencodingthequery TheComplexityofQ(C ,C )
¬¨f,œÄ M œÄ
f(x S¬Ø;z S)Ã∏=f(x)‚àßœÄ(x S¬Ø;z S)=1.Therespectivealignedquery,Q,
We prove a connection between the complexity of calculating an
receivesasinputsf,x,andœÄ,andanswerswhetherSOL istrue.
¬¨f,œÄ
alignedexplanationQ(C ,C ),tothecomplexityofobtainingmis-
Forthecountingcase,Qreturnsthecountingof#SOL . M œÄ
¬¨f,œÄ
alignedexplanationsofeitherQ(C )orQ(C ).Thisrelationholds
M œÄ
Anyofthealignedqueryformsmentionedintheprevioussection inabroadsense,asweproveitforourabstractqueryformQ,defined
canbedescribedasanabstractnotionofthisqueryasweshowinthe inSec.3.First,clearly,if1 ‚àà C (1isatrivialfunctionthatacceptsany whereKisacomplexityclassofthepolynomialhierarchy(orthe
œÄ
possible inputas in-context), then Q(C ,C ) is polynomiallyre- classassociatedwithitscountingproblem),thenQ(C ,C )isalso
M œÄ M œÄ
duciblefromQ(C ).Wenotethat1 ‚àà C isatrivialrequestfor K-complete.
M œÄ
anyexpressiveclassofcontextindicators,forexample,assumingthe
existenceofaneuralnetworkthatalwaysoutputs1. The‚Äúhardness‚ÄùpartofTheorem3isadirectconsequenceofTheo-
rems1and2.However,whenspecificallyconsideringMLPs,com-
Theorem1. If1‚ààC œÄthenQ(C M)‚â§ p Q(C M,C œÄ). pletenessalsoholds.Theproofofthisclaimisrelegatedtotheap-
pendix, and is a result of the fact that any Boolean circuit can be
This result is, of course, not surprising and a more interesting
polynomiallyreducedtoanMLP[12].Thisrelationimpliesthatthe
connection to explore is the less straightforward relation between
‚Äúhardest‚Äùpossiblecomplexityclassinthepolynomialhierarchyis
Q(C ,C )andQ(C ).Weshowthatasimilarresulttotheformer
M œÄ œÄ alwaysassociatedwiththeoneforinterpretinganMLPoverQ.Fig.1
canbeobtainedinthiscaseaswell,providedthatC issymmetrically
œÄ depictstherelationsamongdifferentcomplexityclasses,asderived
constructible(givensomef ‚ààC ,wecanconstructinpolynomial
œÄ fromTheorems1,2,and3.
time¬¨f ‚àà C );andthatC isnaivelyconstructible(givensome
œÄ M
x‚ààF,itholdsthatwecanconstructinpolynomialtime1 ‚ààC ).
{x} M
Afullformalizationoftheseconditionsisprovidedintheappendix. ùë∏(ùë™ )
ùë¥ùë≥ùë∑
Laterinthissection,wealsodemonstratethattheseconstructions
alsoholdforpopularfunctionclasses,andprovidemodel-specific
ùë™ |ùë™ =ùë™
instantiationsofourframework. ùë¥ œÄ ùë¥ùë≥ùë∑
Theorem2. IfC MissymmetricallyconstructibleandC œÄisnaively ùë∏(ùë™ ,ùë™ )
constructible,thenQ(C )‚â§ Q(C ,C ). ùë¥ œÄ
œÄ p M œÄ
Theorem2indicatesthat,givenbasicassumptionsregardingthe
expressivityofC andC ,itholdsthatthecomplexityofevaluating
M œÄ
Q(C ,C ), i.e., interpreting a model from C with respect to a
M œÄ M
modelfromC ,forsomeexplainabilityqueryQ,isatleastashard
œÄ ùë∏(ùë™ ) ùë∏(ùë™ )
asinterpretingQ(C ),i.e.,interpretingtheOODdetectorœÄ.Thisis ùë¥ œÄ
œÄ
significant‚ÄîasinmanycasesC œÄ,theclassassociatedwiththeinput Figure1:AvisualillustrationofTheorems1,2,and3.Dashedlines
distribution,ismuchmoreexpressivethanC ,theclassassociated depictthatbothqueriesareinthesamecomplexityclass,andarehard
M
withthepredictionmodel,andhencemaybemuchhardertointerpret. forthatclass.Arrowsaredirectedfromthequerywiththe‚Äúeasier‚Äù
complexityclasstothequerywiththe‚Äúharder‚Äùcomplexityclass.
Proofsketch.ThereductionexploitsthenaiveconstructibilityofC ,
M
withtheaimofrenderingobsoletetheconjunctresponsibleforvali-
InTable1,weexemplifytheaforementionedexplainabilityqueries
datingwhetherasubsetiscontrastive.Thereductiontakesadvantage
(MCR,MSR,andCC)andaspecificscenariowhereC issettoeither
of thefact that œÄ ‚àà C is symmetrically constructible inorder to M
œÄ C orC ,whereasthecontextindicatorC issettoC
transformœÄtovalidatethemodelinsteadoftheindicatedcontext.By FBDD Perceptron œÄ MLP
(thisisthecaseofourrunningexample,inwhichtheOODdetectionis
employingthisapproach,itbecomesfeasibletopolynomiallyreduce
performedusingamoreexpressivemodelthantheoriginalclassifier).
anySOLformularepresentingQ(C )toanequivalentSOLformula
œÄ Hence,Theorem3impliesthatthecomplexityofsolvingthealigned
undertheformulationofQ(C ,C ).Consequently,anydecisionor
M œÄ queryisprimarilydeterminedbythecomplexityinvolvedinusingan
countingsolutionfortheoriginalSOLformulawillbetantamountto
MLP,assummarizedinTable1.
solvinganequivalentSOLformulacorrespondingtoaqueryseeking
sociallyalignedexplanations.
5 ‚ÄúSelf-Alignment‚Äù:IncorporatingSocial
AlignmentwithinaSingleModel
Model-SpecificFrameworkInstantiations
Untilnow,wefocusedonthegeneralscenarioinwhichf andœÄare
Next,wepresentspecificresultswhenfocusingonFBDDs,Percep-
chosenfromtwodifferentmodelclasses(forinstancef isadecision
trons,andMLPs.Itisstraightforwardtoshowthattheseclassesof
tree,andœÄ isaneuralnetwork).However,insomecases,f andœÄ
modelsmatchourtheoreticalframework,asthefollowingholds(and
canbetwomodelsofthesametype,i.e.,fromthesameclass.Inthis
proveninourappendix):
scenario,givenaclassifierandanOODdetector,bothfromthesame
Proposition1. FBDDs,Perceptrons,andMLPsareallsymmetrically class,practitionersmightdecidetotrainasinglemodelthatlearns
constructibleandnaivelyconstructible. boththepredictiontaskandthealignmenttask.Moreformally,we
saythatasinglemodelclassCis‚Äúself-aligned‚Äùwhenitisexpressive
Dominance of Interpreting MLPs. We prove that when dealing enoughtoincorporatethisdualprocedure.Thisisdemonstratedby
withcomplexityclassesofexplainabilityqueriesthatarefromthe thefactthatgivenamodelf andacontextindicatorœÄ,anewmodelg
polynomialhierarchy(suchasNP,Œ£P,etc.),thecomplexityclass canbeefficientlyconstructedtoshowthealignmentoff withrespect
2
associatedwiththeMLPalwaysdominatestheoverallcomplexity. tothedistributionindicatedbyœÄ:
Hence,theexactcomplexityclassofQ(C ,C )whenC =C
M œÄ M MLP
Definition4. AclassofmodelsCisself-alignedifforanyf,œÄ‚ààC,
and/orC =C ,isequivalenttothatofQ(C ).Thisclaimholds
œÄ MLP MLP
and any inputs x and I, there exists a polynomially constructible
foranyclassofpolynomiallycomputablefunctions.
functiong‚ààC,suchthat:
Theorem3. LetC ,C beclassesofpolynomiallycomputablefunc-
M œÄ
tionssuchthatC =C orC =C .IfQ(C )isK-complete, ‚ü®f,œÄ,x,I‚ü©‚ààQ(C,C) ‚áê‚áí ‚ü®g,x,I‚ü©‚ààQ(C) (6)
M MLP œÄ MLP MLPTable1:ThecomputationalcomplexityofQ(C M)andQ(C M,C œÄ)withrespecttovariousexplainabilityqueries.
C =C C =C
M FBDD M Perceptron
Q(C ) Q(C ,C ) Q(C ) Q(C ,C )
M M MLP M M MLP
MCR PTIME NP-complete PTIME NP-complete
MSR NP-complete Œ£P-complete PTIME Œ£P-complete
2 2
CC PTIME #P-complete #P-complete #P-complete
Intuitively,foranypossibleexplainabilityquerywithinQ(decision BuildinguponProposition4,wecandeducethefollowingcorollary
orcounting),explanationsoff,alignedbyœÄ,canbeexpressedbya (provedintheappendix):
singleaggregatedfunctiong.Clearly,gmustbeatleastasexpressive
as the original models f and œÄ. This raises the question of how Theorem 5. Assuming that P Ã∏= NP, the class C Perceptron is not
expressiveaclassofmodelsCshouldbe,forittobeself-aligned. self-aligned.
Theorem4. GivenaclassofmodelsC,ifforanyf 1,f 2 ‚ààC,wecan Thesefindingsunderscoreacrucialaspectconcerningtheinter-
polynomiallyconstructg:=f 1[op]f 2 ‚ààC,for[op]‚àà{‚àß,‚Üí},then pretabilityofPerceptrons.Whileproducingexplanationspertainingto
Cisself-aligned. themcanbeachievedwithlowcomputationalcomplexity(providing
furtherevidenceoftheirinterpretability),theyarenotself-aligned.
Intuitively,classesofmodelsthatarecapableofexpressingthe
Consequently,obtainingalignedexplanationsusingPerceptronsne-
logicaloperators‚Üíand‚àßarecapableof‚Äúcapturing‚Äùthatagivenex-
cessitatestheadoptionofamoresophisticatedmodel,thatisexpres-
planationformisdeterminedbyitsunderlyingdistribution.Theproof
siveenoughtoincorporatesocialalignment‚Äîandthis,inturn,can
ofthistheoremisrelegatedtotheappendix,andcanbeobtainedby
significantlyincreasetheoverallcomplexityoftheirinterpretation.
showinganequivalencebetweenthetwounderlyingformalizations.
Ifself-alignmentimpliesthat,givenapredictionmodelf anda
contextindicatorœÄ,wecanattainasingleaggregatedmodelg‚Äîthen 6 RelatedWork
clearly the computational complexity of interpreting f ‚àà C with
respecttoœÄ‚ààC(i.e.,thecomplexityofQ(C,C))iscorrelatedtothe This work continues a line of research that focuses on Formal
complexityofinterpretingg‚ààC(i.e.,thecomplexityofQ(C)).This XAI [46, 89, 9, 7, 48, 13, 14]. Prior studies have already in-
canbedemonstratedbythesubsequentproposition: vestigated the explanation forms that were analyzed within our
work[7,89,9,7],includingsufficiency-basedexplainabilityqueries
Proposition2. IftheconditionsinTheorem4holdforaclassof
(MSR)[47,69],contrastive/counterfactual-basedqueries(MCR)[81,
modelsC,thenQ(C,C)= Q(C).
P 49],andcounting-basedqueries(CC)[28].Otherwork[36]defined
formalnotionsofsufficientandcontrastivereasonsunderspecific
Model-SpecificResults
contextsandsuggestedwaystocomputethemonawiderangeof
models[96].However,theseexplanationformswerenotanalyzed
Wemoveontoanalyzewhichoftheaforementionedmodelclasses
withrespecttotheiroverarchingcomputationalcomplexity.Closerto
incorporateself-alignment.First,weshowthatbothFBDDsandMLPs
oursistheworkofCooperetal.[21]whichanalyzesdifferentprop-
areself-aligned,whichisaresultoftheircapabilitytopolynomially
erties(includingthecomputationalcomplexity)ofsufficiency-based
express‚Üíand‚àßrelationswithintheirclass:
explanationsunderlogicalconstraints.Wealsoacknowledgethework
Proposition3. FBDDsandMLPsareself-aligned,andhence,itfol- ofArenasetal.[7],whichdescribesagenerallogic-basedexplanation
lowsthat:Q(C FBDD,C FBDD)= P Q(C FBDD)andQ(C MLP,C MLP)= P form,similartoourabstractqueryform.Whiletheirworkfocuses
Q(C MLP). onexplanationsoffirst-orderlogicformsfordecisionqueries,our
approachismoreexpressive,encompassingsecond-orderlogicforms
However,incontrasttodecisiontreesandneuralnetworks,linear
thatincorporatebothdecision-basedandcounting-basedexplanations.
classifierslacktheabilitytocapturethenotionofself-alignment.Itis
Anotherlineofresearchexaminesthecomputationalcomplexity
importanttonotethatasinglePerceptroncannotinherentlyrepresent
of obtaining Shapley value-based explanations [8, 88, 72], where
the‚Üíand‚àßrelationsovertwootherPerceptrons.Thatsaid,itis
alignmentwithrespecttoagivendistributionisvital[83].Specifically,
worthemphasizingthatthisobservationalonedoesnotconclusively
VandenBroecketal.[88]identifyacomplexitygapininterpreting
establishtheirlackofself-alignment,asthisconditionissufficient
Shapley valueswhen consideringfullyfactorized orNaiveBayes-
butnotnecessary.TorigorouslyprovetheinabilityofPerceptronsto
modeleddistributions.
beself-aligned,weprovethesubsequentproposition:
Insomecases,theterm‚Äúsufficientreason‚Äùisalsodefinedasan
Proposition 4. While the query MCR(C ) can be solved in abductiveexplanation[47]andcorrelateswiththenotionofaprime
Perceptron
polynomialtime,thequeryMCR(C ,C )isNP-complete. implicantforaBooleanclassifier[28].TheCCqueryisassociated
Perceptron Perceptron
withprobabilisticnotionsofexplainability,bycorrelatingthepre-
Proofsketch.Membershipresultsfromthefactthatwecanguessa cision of the explanation with the number of possible input com-
subsetoffeaturesSandvalidatewhetheritiscontrastiveforf and pletions[77,89].Asimilarnotion,formallyknownasaŒ¥-relevant
whetheritisalsoin-distribution(byfeedingittoœÄ).Forhardness, set[52,89],focusesonboundingthisspecificportion.
wereducefromSSP(thek-subset-sumproblem),whichisaclassic ThedependencyofexplanationsonOODassignmentshasbeen
NP-completeproblem.ThereductionexploitstherangesofthePer- studiedextensively[97,84,33,39,59,40,95,83].Specifically,many
ceptronsofbothf andœÄ inordertobindthetargetsumT ofthe heuristic-basedtoolsandframeworkshavebeenproposedfordeal-
subset,bothfromaboveandfrombelow. ingwiththeOODcounterfactualprobleminmodelexplainability.Theseincludemarginalizingthepredictionofthemodeloverpos- sharedcharacteristicsamongdifferenttypesofexplainabilityqueries
siblecounterfactualassignments[100,59,95],samplingpointsin canbeutilizedtooffermoregeneralizedassessments.
theproximityoftheoriginalinput[20,79,77],aswellascounter- Finally, we highlight that our study primarily concentrates on
factualtraining[38,87]‚Äîamethodthat,similarlytoadversarial an OOD detector œÄ(x), which classifies each input as either in-
training[99],seekstorobustifymodelstoOODcounterfactuals.Other distributionorOOD,ratherthanontheinputdistributionp (x)itself.
Œ∏
workfocusesonmitigatingtheeffectofOODassignmentsonthe Thisapproachisduetothestrictlyformalnatureoftheexplanations
computationofShapleyvalues[83,56,85].Inspiteofthesenotable we investigate; an explanation is either valid or not, necessitating
accomplishments,thetheoreticalanalysisoftheOODcounterfactual a definitive categorization of the presence or absence of each in-
problemwithrespecttoitscomputationalcomplexityhasyettobe put.Incontrast,probabilisticexplanationforms,suchasŒ¥-relevant
thoroughlyexamined. sets[89,52]orShapleyvalues[64,83],aredefinedinrelationtothe
distributionitselfandcanalsobeassessedbasedonthecomputational
complexityofobtainingthem.Forinstance,arecentstudybyMar-
7 Conclusion
zouketal.[72]exploresthecomputationalcomplexityofcalculating
Computationalcomplexitytheorystandsasapotentialavenuetofor- ShapleyvalueswithinMarkoviandistributions.Futureresearchcan
mallyassesstheinterpretabilityofvariousMLmodels.Priorresearch focusonexpandingthestrictlyformalexplanationframeworkdis-
examinedthisbyconsideringtwomainfactors:themodeltypeandthe cussedheretoincludeprobabilisticexplanationformsaswell,where
explanationform.Weclaimthatathirdandimportantfactorshouldbe complexityassessmentswouldfocusdirectlyontheinputdistribution
takenintoconsideration‚Äîtheunderlyingdistributionoverwhichthe p Œ∏(x)ratherthanontheOODdetectorœÄ(x).Other,broaderfuture
explanationiscomputed.Toachievethisgoal,wegeneralizeexisting workcanexploretherelationbetweenthecomputationalcomplexity
explainabilityqueriesandshowhowaunifiedformcandescribethe ofgeneratingexplanations(ourcurrentfocus)andthecomplexityof
desiredsocialalignmentrequirementforanyexplanationformunder theexplanationsthemselves.Thiscanbeachievedusingvarioustools,
oursecond-orderlogicformalization.Moreover,wepresentaframe- suchasKolmogorovcomplexity.Wealsocoveradditionalextensions
workforassessingthecomputationalcomplexityofthesequeriesand ofourframeworkintheappendix.
demonstratethat,forabroadrangeofmodeltypesandqueryforms,
providingsociallyalignedexplanationsisashardasinterpretinga Acknowledgments
modeldesignedtodetectOODinputs.AsOODdetectionisknown
This work was partially funded by the European Union (ERC,
tobesubstantiallydifficult,suchmodelsmayoftenrequiremoreex-
VeriDeL,101112713).Viewsandopinionsexpressedarehowever
pressivecapacitythantheoriginalclassificationmodels,significantly
thoseoftheauthor(s)onlyanddonotnecessarilyreflectthoseofthe
impactingtheoverallcomplexityofmodelinterpretation.Finally,we
EuropeanUnionortheEuropeanResearchCouncilExecutiveAgency.
provideananalysisoftherequiredcapacityofmodelstoinherently
NeithertheEuropeanUnionnorthegrantingauthoritycanbeheld
producealignedexplanationswithoutusinganexternalOODdetector.
responsibleforthem.TheworkofAmirwasfurthersupportedbya
Wehopethatourworkservesasafoundationforadeepermathemat-
scholarshipfromtheCloreIsraelFoundation.
icalunderstandingoftheinterpretabilitypertainingtovariousML
models.
References
8 LimitationsandFutureWork [1] G.Amir,M.Schapira,andG.Katz. TowardsScalableVerification
ofDeepReinforcementLearning.InProc.21stInt.Conf.onFormal
MethodsinComputer-AidedDesign(FMCAD),pages193‚Äì203,2021.
Ourframeworkcanbeextendedalongseveraldifferentaxes.Firstand
[2] G.Amir,H.Wu,C.Barrett,andG.Katz.AnSMT-BasedApproachfor
foremost,wenotethatassumingtheexistenceofacontextindicatorœÄ VerifyingBinarizedNeuralNetworks.InProc.27thInt.Conf.onTools
foridentifyingOODinputsishighlynon-trivial.Previouswork,both andAlgorithmsfortheConstructionandAnalysisofSystems(TACAS),
theoreticalandpractical,hashighlightedthechallengesassociated pages203‚Äì222,2021.
[3] G.Amir,T.Zelazny,G.Katz,andM.Schapira. Verification-Aided
withobtainingsuchanOODdetector[32,73,41,80,16].However,
DeepEnsembleSelection.InProc.22ndInt.Conf.onFormalMethods
itisimportanttoemphasizethatourframeworkdoesnotnecessarily inComputer-AidedDesign(FMCAD),pages27‚Äì37,2022.
assumethecompleteaccuracyorcorrectnessofsuchaclassifier.In- [4] G.Amir,D.Corsi,R.Yerushalmi,L.Marzari,D.Harel,A.Farinelli,
andG.Katz.VerifyingLearning-BasedRoboticNavigationSystems.
stead,œÄcanbeviewedasafunctionthatprovidesanapproximation
InProc.29thInt.Conf.onToolsandAlgorithmsfortheConstruction
oftheunderlyingcontextC.Therefore,futureresearchendeavors andAnalysisofSystems(TACAS),pages607‚Äì627,2023.
couldcenteraroundevaluatingthecomputationalcomplexityofspe- [5] G.Amir,Z.Freund,G.Katz,E.Mandelbaum,andI.Refaeli.veriFIRE:
cificapproximationstailoredtoparticularcontextsofinterest.While VerifyinganIndustrial,Learning-BasedWildfireDetectionSystem.In
Proc.25thInt.SymposiumonFormalMethods(FM),pages648‚Äì656,
theseapproximationsmayonlyofferapartiallyguaranteedsolution
2023.
tothealignmentissue,theymaystillexhibitanimprovedcomplexity [6] G.Amir,O.Maayan,T.Zelazny,G.Katz,andM.Schapira.Verifying
overall. GeneralizationinDeepLearning.InProc.35thInt.Conf.onComputer
AidedVerification(CAV),pages438‚Äì455,2023.
Otherlimitationscorrespondtosimilar(non-aligned)approaches
[7] M.Arenas,D.Baez,P.Barcel√≥,J.P√©rez,andB.Subercaseaux.Foun-
for analyzing the computational complexity of obtaining explana- dationsofSymbolicLanguagesforModelInterpretability. InProc.
tions[12,89,15].Firstly,ouranalysisconsidersonlyaworst-case 34thInt.Conf.onAdvancesinNeuralInformationProcessingSystems
scenariothatmaychangeundervariousparameter-specificconfigu- (NeurIPS),pages11690‚Äì11701,2021.
[8] M.Arenas,P.Barcel√≥,L.Bertossi,andM.Monet.TheTractabilityof
rations.Secondly,thenaturalsubjectivityofinterpretabilitymakesit
SHAP-Score-BasedExplanationsforClassificationoverDeterministic
challengingtoanalyzethecomputationalcomplexityofinterpreting andDecomposableBooleanCircuits. InProc.35thAAAIConf.on
amodelinasingle‚Äúcorrect‚Äùway.Toaddressthisissue,theoretical ArtificialIntelligence,pages6670‚Äì6678,2021.
frameworksdefinevariousexplainabilityqueriesandevaluatethem [9] M.Arenas,P.Barcel√≥,M.RomeroOrth,andB.Subercaseaux. On
ComputingProbabilisticExplanationsforDecisionTrees. InProc.
separately.Weregardourproofforawiderangeofexplainability
35thInt.Conf.onAdvancesinNeuralInformationProcessingSystems
queriesQ(theabstractqueryform)aspotentialevidencethatthe (NeurIPS),pages28695‚Äì28707,2022.[10] S.AroraandB.Barak.ComputationalComplexity:AModernApproach. Perceptron)‚ÄîaReviewofApplicationsintheAtmosphericSciences.
CambridgeUniversityPress,2009. AtmosphericEnvironment,32(14-15):2627‚Äì2636,1998.
[11] G.Audemard,J.Lagniez,P.Marquis,andN.Szczepanski.Computing [35] T.Gehr,M.Mirman,D.Drachsler-Cohen,E.Tsankov,S.Chaudhuri,
AbductiveExplanationsforBoostedTrees.InProc.26thInt.Conf.on andM.Vechev. AI2:SafetyandRobustnessCertificationofNeural
ArtificialIntelligenceandStatistics(AISTATS),2023. NetworkswithAbstractInterpretation.InProc.39thIEEESymposium
[12] P.Barcel√≥,M.Monet,J.P√©rez,andB.Subercaseaux. ModelInter- onSecurityandPrivacy(S&P),2018.
pretabilitythroughtheLensofComputationalComplexity. InProc. [36] N.GorjiandS.Rubin. SufficientReasonsforClassifierDecisions
33rdInt.Conf.onAdvancesinNeuralInformationProcessingSystems inthePresenceofDomainConstraints. InProc.36thAAAIConf.on
(NeurIPS),pages15487‚Äì15498,2020. ArtificialIntelligence,pages5660‚Äì5667,2022.
[13] S.BassanandG.Katz. TowardsFormalApproximatedMinimalEx- [37] J.HalpernandJ.Pearl.CausesandExplanations:AStructural-Model
planationsofNeuralNetworks.InProc.29thInt.Conf.onToolsand Approach.PartI:Causes. TheBritishJournalforthePhilosophyof
AlgorithmsfortheConstructionandAnalysisofSystems(TACAS), Science,2005.
pages187‚Äì207,2023. [38] P.Hase,H.Xie,andM.Bansal.TheOut-of-DistributionProbleminEx-
[14] S.Bassan,G.Amir,D.Corsi,I.Refaeli,andG.Katz. FormallyEx- plainabilityandSearchMethodsforFeatureImportanceExplanations.
plainingNeuralNetworkswithinReactiveSystems.InProc.23rdInt. InProc.34thInt.Conf.onAdvancesinNeuralInformationProcessing
Conf.onFormalMethodsinComputer-AidedDesign(FMCAD),pages Systems(NeurIPS),pages3650‚Äì3666,2021.
10‚Äì22,2023. [39] S.Hooker,D.Erhan,P.Kindermans,andB.Kim. ABenchmarkfor
[15] S.Bassan,G.Amir,andG.Katz.Localvs.GlobalInterpretability:A InterpretabilityMethodsinDeepNeuralNetworks. InProc.32nd
ComputationalComplexityPerspective. InProc.41stInt.Conf.on Int. Conf. on Advances in Neural Information Processing Systems
MachineLearning(ICML),2024. (NeurIPS),2019.
[16] D.Berend,X.Xie,L.Ma,L.Zhou,Y.Liu,C.Xu,andJ.Zhao.Catsare [40] C.Hsieh,C.Yeh,X.Liu,P.Ravikumar,S.Kim,S.Kumar,andC.Hsieh.
notFish:DeepLearningTestingCallsforOut-of-DistributionAware- EvaluationsandMethodsforExplanationthroughRobustnessAnalysis.
ness. In Proc. 35th IEEE/ACM Int. Conf. on Automated Software InProc.9thInt.Conf.onLearningRepresentations(ICLR),2021.
Engineering(ASE),pages1041‚Äì1052,2020. [41] Y.Hsu,Y.Shen,H.Jin,andZ.Kira.GeneralizedODIN:DetectingOut-
[17] R.Boumazouza,F.Cheikh-Alili,B.Mazure,andK.Tabia.ASTERYX: of-DistributionImageWithoutLearningFromOut-of-DistributionData.
AModel-AgnosticSAT-BasedApproachforSymbolicandScore-Based InProc.IEEE/CVFConf.onComputerVisionandPatternRecognition
Explanations.InProc.30thACMInt.Conf.onInformation&Knowl- (CVPR),2020.
edgeManagement(CIKM),pages120‚Äì129,2021. [42] X.HuangandJ.Marques-Silva.FromRobustnesstoExplainabilityand
[18] R.Bunel,I.Turkaslan,P.Torr,P.Kohli,andP.Mudigonda.AUnified BackAgain,2023.TechnicalReport.https://arxiv.org/abs/2306.03048.
ViewofPiecewiseLinearNeuralNetworkVerification.InProc.32nd [43] X.Huang,D.Kroening,W.Ruan,J.Sharp,Y.Sun,E.Thamo,M.Wu,
Conf.onNeuralInformationProcessingSystems(NeurIPS),pages andX.Yi. ASurveyofSafetyandTrustworthinessofDeepNeural
4795‚Äì4804,2018. Networks:Verification,Testing,AdversarialattackandDefence,and
[19] M. Casadio, E. Komendantskaya, M. Daggitt, W. Kokke, G. Katz, Interpretability.ComputerScienceReview,37:100270,2020.
G.Amir,andI.Refaeli. NeuralNetworkRobustnessasaVerifica- [44] X.Huang,Y.Izza,A.Ignatiev,andJ.Marques-Silva. OnEfficiently
tionProperty:APrincipledCaseStudy. InProc.34thInt.Conf.on ExplainingGraph-BasedClassifiers,2021. TechnicalReport.https:
ComputerAidedVerification(CAV),pages219‚Äì231,2022. //arxiv.org/abs/2106.01350.
[20] C.Chang,E.Creager,A.Goldenberg,andD.Duvenaud.Explaining [45] X.Huang,M.Cooper,A.Morgado,J.Planes,andJ.Marques-Silva.
ImageClassifiersbyCounterfactualGeneration.InProc.7thInt.Conf. FeatureNecessity&RelevancyinMLClassifierExplanations. In
onLearningRepresentations(ICLR),2019. Proc.29thInt.Conf.onToolsandAlgorithmsfortheConstructionand
[21] M.CooperandL.Amgoud. AbductiveExplanationsofClassifiers AnalysisofSystems(TACAS),pages167‚Äì186,2023.
underConstraints:ComplexityandProperties.In26thEuropeanConf. [46] A.Ignatiev.TowardsTrustableExplainableAI.InProc.29thInt.Joint
onArtificialIntelligence(ECAI),2023. Conf.onArtificialIntelligence(IJCAI),pages5154‚Äì5158,2020.
[22] D.Corsi,R.Yerushalmi,G.Amir,A.Farinelli,D.Harel,andG.Katz. [47] A.Ignatiev,N.Narodytska,andJ.Marques-Silva. Abduction-Based
ConstrainedReinforcementLearningforRoboticsviaScenario-Based ExplanationsforMachineLearningModels.InProc.33rdAAAIConf.
Programming, 2022. Technical Report. https://arxiv.org/abs/2206. onArtificialIntelligence,pages1511‚Äì1519,2019.
09603. [48] A.Ignatiev,N.Narodytska,andJ.Marques-Silva.OnRelatingExplana-
[23] D.Corsi,G.Amir,G.Katz,andA.Farinelli. AnalyzingAdversarial tionsandAdversarialExamples.InProc.32ndInt.Conf.onAdvances
InputsinDeepReinforcementLearning,2024.TechnicalReport.https: inNeuralInformationProcessingSystems(NeurIPS),2019.
//arxiv.org/abs/2402.05284. [49] A.Ignatiev,N.Narodytska,N.Asher,andJ.Marques-Silva. From
[24] D.Corsi,G.Amir,A.Rodr√≠guez,C.S√°nchez,G.Katz,andR.Fox. ContrastivetoAbductiveExplanationsandBackAgain.InProc.Int.
Verification-GuidedShieldingforDeepReinforcementLearning. In Conf.ItalianAssociationforArtificialIntelligence,2020.
Proc.1stInt.ReinforcementLearningConf.(RLC),2024. [50] A.Ignatiev,Y.Izza,P.Stuckey,andJ.Marques-Silva.UsingMaxSAT
[25] A.DarwicheandA.Hirth.OntheReasonsBehindDecisions.InProc. forEfficientExplanationsofTreeEnsembles.InProc.36thAAAIConf.
23rdEuropeanConf.onArtificialIntelligence(ECAI),pages712‚Äì720, onArtificialIntelligence,pages3776‚Äì3785,2022.
2020. [51] Y.IzzaandJ.Marques-Silva. OnExplainingRandomForestswith
[26] A.DarwicheandA.Hirth. Onthe(Complete)ReasonsBehindDe- SAT. InProc.30thInt.JointConf.onArtificialIntelligence(IJCAI),
cisions. JournalofLogic,LanguageandInformation,32(1):63‚Äì88, 2021.
2023. [52] Y.Izza,A.Ignatiev,N.Narodytska,M.Cooper,andJ.Marques-Silva.
[27] A.DarwicheandC.Ji.OntheComputationofNecessaryandSufficient EfficientExplanationswithRelevantSets,2021. TechnicalReport.
Explanations.InProc.36thAAAIConf.onArtificialIntelligence,pages https://arxiv.org/abs/2106.00546.
5582‚Äì5591,2022. [53] Y.Izza,A.Ignatiev,andJ.Marques-Silva.OnTacklingExplanationRe-
[28] A.DarwicheandP.Marquis.AKnowledgeCompilationMap.Journal dundancyinDecisionTrees.JournalofArtificialIntelligenceResearch
ofArtificialIntelligenceResearch(JAIR),17:229‚Äì264,2002. (JAIR),75:261‚Äì321,2022.
[29] R.Ehlers. FormalVerificationofPiece-WiseLinearFeed-Forward [54] Y.Izza,X.Huang,A.Morgado,J.Planes,A.Ignatiev,andJ.Marques-
NeuralNetworks.InProc.15thInt.Symp.onAutomatedTechnology Silva. Distance-RestrictedExplanations:TheoreticalUnderpinnings
forVerificationandAnalysis(ATVA),pages269‚Äì286,2017. &EfficientImplementation,2024.TechnicalReport.https://arxiv.org/
[30] Y.Elboher,J.Gottschlich,andG.Katz.AnAbstraction-BasedFrame- abs/2405.08297.
workforNeuralNetworkVerification. InProc.32ndInt.Conf.on [55] Y.Jacoby,C.Barrett,andG.Katz. VerifyingRecurrentNeuralNet-
ComputerAidedVerification(CAV),pages43‚Äì65,2020. worksusingInvariantInference. InProc.18thInt.Symposiumon
[31] R.Fagin.GeneralizedFirst-OrderSpectraandPolynomial-TimeRec- AutomatedTechnologyforVerificationandAnalysis(ATVA),pages
ognizableSets.ComplexityofComputation,7:43‚Äì73,1974. 57‚Äì74,2020.
[32] Z.Fang,Y.Li,J.Lu,J.Dong,B.Han,andF.Liu.IsOut-of-Distribution [56] D.Janzing,L.Minorics,andP.Bl√∂baum.FeatureRelevanceQuantifi-
DetectionLearnable?InProc.36thInt.Conf.onAdvancesinNeural cationinExplainableAI:ACausalProblem.InProc.23rdInt.Conf.
InformationProcessingSystems(NeurIPS),2022. onArtificialIntelligenceandStatistics(AISTATS),pages2907‚Äì2916,
[33] R.FongandA.Vedaldi.InterpretableExplanationsofBlackBoxesby 2020.
MeaningfulPerturbation.InProc.IEEEInt.Conf.onComputerVision [57] G.Katz,C.Barrett,D.Dill,K.Julian,andM.Kochenderfer.Reluplex:
(ICCV),pages3429‚Äì3437,2017. AnEfficientSMTSolverforVerifyingDeepNeuralNetworks.InProc.
[34] M.GardnerandS.Dorling.ArtificialNeuralNetworks(theMultilayer 29thInt.Conf.onComputerAidedVerification(CAV),pages97‚Äì117,2017. [80] J.Serr√†,D.√Ålvarez,V.G√≥mez,O.Slizovskaia,J.N√∫√±ez,andJ.Luque.
[58] G.Katz,D.Huang,D.Ibeling,K.Julian,C.Lazarus,R.Lim,P.Shah, InputComplexityandOut-of-DistributionDetectionwithLikelihood-
S.Thakoor,H.Wu,A.Zeljic¬¥,D.Dill,M.Kochenderfer,andC.Barrett. BasedGenerativeModels.InProc.7thInt.Conf.onLearningRepre-
TheMarabouFrameworkforVerificationandAnalysisofDeepNeural sentations(ICLR),2019.
Networks. InProc.31stInt.Conf.onComputerAidedVerification [81] A.Shih,A.Choi,andA.Darwiche.FormalVerificationofBayesian
(CAV),pages443‚Äì452,2019. NetworkClassifiers. InProc.Int.Conf.onProbabilisticGraphical
[59] S.Kim,J.Yi,E.Kim,andS.Yoon. InterpretationofNLPModels Models(PGM),pages427‚Äì438,2018.
ThroughInputMarginalization.InProc.Conf.onEmpiricalMethods [82] X.Sun,H.Khedr,andY.Shoukry. FormalVerificationofNeural
inNaturalLanguageProcessing(EMNLP),2020. NetworkControlledAutonomousSystems. InProc.22ndACMInt.
[60] B.K√∂nighofer,F.Lorber,N.Jansen,andR.Bloem.ShieldSynthesis Conf.onHybridSystems:ComputationandControl(HSCC),2019.
forReinforcementLearning.InProc.Int.SymposiumonLeveraging [83] M.SundararajanandA.Najmi.TheManyShapleyValuesforModel
ApplicationsofFormalMethods,VerificationandValidation(ISoLA), Explanation. InProc.37thInt.Conf.onMachineLearning(ICML),
pages290‚Äì306,2020. pages9269‚Äì9278,2020.
[61] E. La Malfa, A. Zbrzezny, R. Michelmore, N. Paoletti, and [84] M.Sundararajan,A.Taly,andQ.Yan.AxiomaticAttributionforDeep
M.Kwiatkowska. OnGuaranteedOptimalRobustExplanationsfor Networks.InProc.34thInt.Conf.onMachineLearning(ICML),2017.
NLPModels. InProc.30thInt.JointConf.onArtificialIntelligence [85] M.Taufiq,P.Bl√∂baum,andL.Minorics.ManifoldRestrictedInterven-
(IJCAI),2021. tionalShapleyValues.InProc.26thInt.Conf.onArtificialIntelligence
[62] C. Lee. Representation of Switching Circuits by Binary-Decision andStatistics(AISTATS),2023.
Programs.TheBellSystemTechnicalJournal,38(4):985‚Äì999,1959. [86] H.Tran,S.Bak,andT.Johnson. VerificationofDeepConvolutional
[63] C.Liang,P.Huang,W.Lai,andZ.Ruan.GAN-BasedOut-of-Domain NeuralNetworksUsingImageStars.InProc.32ndInt.Conf.onCom-
DetectionUsingBothIn-DomainandOut-of-DomainSamples.InProc. puterAidedVerification(CAV),pages18‚Äì42,2020.
IEEEInt.Conf.onAcoustics,SpeechandSignalProcessing(ICASSP), [87] K.Vafa,Y.Deng,D.Blei,andA.Rush. RationalesforSequential
pages7663‚Äì7667,2021. Predictions.InProc.Conf.onEmpiricalMethodsinNaturalLanguage
[64] S.LundbergandS.Lee. AUnifiedApproachtoInterpretingModel Processing(EMNLP),2021.
Predictions.InProc.30thInt.Conf.onAdvancesinNeuralInformation [88] G.VandenBroeck,A.Lykov,M.Schleich,andD.Suciu. Onthe
ProcessingSystems(NeurIPS),2017. TractabilityofSHAPExplanations.JournalofArtificialIntelligence
[65] Z.Lyu,C.Ko,Z.Kong,N.Wong,D.Lin,andL.Daniel. Fastened Research(JAIR),74:851‚Äì886,2022.
Crown:TightenedNeuralNetworkRobustnessCertificates. InProc. [89] S.W√§ldchen,J.Macdonald,S.Hauch,andG.Kutyniok. TheCom-
34thAAAIConf.onArtificialIntelligence(AAAI),pages5037‚Äì5044, putationalComplexityofUderstandingBinaryClassifierDecisions.
2020. JournalofArtificialIntelligenceResearch(JAIR),70:351‚Äì387,2021.
[66] U.Mandal,G.Amir,H.Wu,I.Daukantas,F.Newell,U.Ravaioli, [90] H.Wu,O.Isac,A.Zeljic¬¥,T.Tagomori,M.Daggitt,W.Kokke,I.Refaeli,
B.Meng,M.Durling,M.Ganai,T.Shim,G.Katz,andC.Barrett. G.Amir,K.Julian,S.Bassan,P.Huang,O.Lahav,M.Wu,M.Zhang,
FormallyVerifyingDeepReinforcementLearningControllerswith E.Komendantskaya,G.Katz,andC.Barrett.Marabou2.0:AVersatile
LyapunovBarrierCertificates. InProc.24thInt.Conf.onFormal FormalAnalyzerofNeuralNetworks. InProc.36thInt.Conf.on
MethodsinComputer-AidedDesign(FMCAD),2024. ComputerAidedVerification(CAV),2024.
[67] U.Mandal,G.Amir,H.Wu,I.Daukantas,F.Newell,U.Ravaioli, [91] M.Wu,H.Wu,andC.Barrett.Verix:TowardsVerifiedExplainability
B.Meng,M.Durling,K.Hobbs,M.Ganai,T.Shim,G.Katz,and ofDeepNeuralNetworks. InProc.36thInt.Conf.onAdvancesin
C.Barrett.SafeandReliableTrainingofLearning-BasedAerospace NeuralInformationProcessingSystems(NeurIPS),2024.
Controllers. InProc.43rdDigitalAvionicsSystemsConf.(DASC), [92] X.Xuan,P.Xizhou,L.Nan,H.Xing,M.Lin,Z.Xiaoguang,and
2024. D.Ning.GAN-BasedAnomalyDetection:AReview.Neurocomputing,
[68] J.Marques-SilvaandA.Ignatiev.DeliveringTrustworthyAIthrough 493,2022.
formalXAI.InProc.36thAAAIConf.onArtificialIntelligence,pages [93] R.Yerushalmi,G.Amir,A.Elyasaf,D.Harel,G.Katz,andA.Mar-
3806‚Äì3814,2022. ron.Scenario-AssistedDeepReinforcementLearning.InProc.10th
[69] J.Marques-Silva,T.Gerspacher,M.Cooper,A.Ignatiev,andN.Nar- Int.Conf.onModel-DrivenEngineeringandSoftwareDevelopment
odytska. ExplainingNaiveBayesandOtherLinearClassifierswith (MODELSWARD),pages310‚Äì319,2022.
PolynomialTimeandDelay.InProc.33rdInt.Conf.onAdvancesin [94] R.Yerushalmi,G.Amir,A.Elyasaf,D.Harel,G.Katz,andA.Mar-
NeuralInformationProcessingSystems(NeurIPS),pages20590‚Äì20600, ron. EnhancingDeepReinforcementLearningwithScenario-Based
2020. Modeling.SNComputerScience,4(2):156,2023.
[70] J.Marques-Silva,T.Gerspacher,M.Cooper,A.Ignatiev,andN.Naro- [95] J.Yi,E.Kim,S.Kim,andS.Yoon. Information-TheoreticVisual
dytska.ExplanationsforMonotonicClassifiers.InProc.38thInt.Conf. ExplanationforBlack-BoxClassifiers,2020.TechnicalReport.https:
onMachineLearning(ICML),2021. //arxiv.org/abs/2009.11150.
[71] L. Marzari, D. Corsi, F. Cicalese, and A. Farinelli. The #DNN- [96] J.Yu,A.Ignatiev,P.Stuckey,N.Narodytska,andJ.Marques-Silva.
Verificationproblem:CountingUnsafeInputsforDeepNeuralNet- EliminatingTheImpossible,WhateverRemainsMustBeTrue,2022.
works.InProc.32ndInt.JointConf.onArtificialIntelligence(IJCAI), TechnicalReport.https://arxiv.org/abs/2206.09551.
2023. [97] O.Zaidan,J.Eisner,andC.Piatko.Using‚ÄúAnnotatorRationales‚ÄùtoIm-
[72] R. Marzouk and C. de La Higuera. On the Tractability of SHAP proveMachineLearningforTextCategorization.InProc.Conf.North
ExplanationsunderMarkovianDistributions,2024.TechnicalReport. AmericanChapteroftheAssociationforComputationalLinguistics
http://arxiv.org/abs/2405.02936. (NAACL),pages260‚Äì267,2007.
[73] P.MortezaandY.Li.ProvableGuaranteesforUnderstandingOut-of- [98] H.Zhang,M.Shinn,A.Gupta,A.Gurfinkel,N.Le,andN.Narodytska.
DistributionDetection.InProc.36thAAAIConf.onArtificialIntelli- VerificationofRecurrentNeuralNetworksforCognitiveTasksvia
gence,pages7831‚Äì7840,2022. ReachabilityAnalysis. InProc.24thEuropeanConf.onArtificial
[74] R.Poyiadzi,K.Sokol,R.Santos-Rodriguez,T.DeBie,andP.Flach. Intelligence(ECAI),pages1690‚Äì1697,2020.
FACE:FeasibleandActionableCounterfactualExplanations.InProc. [99] W.Zhao,S.Alwidian,andQ.Mahmoud.AdversarialTrainingMethods
AAAI/ACMConf.onAI,Ethics,andSociety(AIES),2020. forDeepLearning:ASystematicReview.Algorithms,15(8):283,2022.
[75] A.Ralston,E.Reilly,andD.Hemmendinger. EncyclopediaofCom- [100] L.Zintgraf,T.Cohen,T.Adel,andM.Welling. VisualizingDeep
puterScience.JohnWileyandSonsLtd.,2003. NeuralNetworkDecisions:PredictionDifferenceAnalysis. InProc.
[76] H.Ramchoun,Y.Ghanou,M.Ettaouil,andM.AmineJanatiIdrissi. 7thInt.Conf.onLearningRepresentations(ICLR),2017.
MultilayerPerceptron:ArchitectureOptimizationandTraining. Int.
JournalofInteractiveMultimediaandArtificialIntelligence,2016.
[77] M.Ribeiro,S.Singh,andC.Guestrin.Anchors:High-PrecisionModel-
AgnosticExplanations.InProc.32ndAAAIConf.onArtificialIntelli-
gence,2018.
[78] A.Rodriguez,G.Amir,D.Corsi,C.Sanchez,andG.Katz. Shield
SynthesisforLTLModuloTheories,2024. TechnicalReport.https:
//arxiv.org/abs/2406.04184.
[79] S.SanyalandX.Ren.DiscretizedIntegratedGradientsforExplaining
LanguageModels. InProc.Conf.onEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),2021.Appendix
(Misaligned)CC(CountCompletions):
Input:Modelf,inputx,andinputI :=‚ü®S‚ü©.
Theappendixcontainsdefinitions,formalizations,andproofsthat
Output: The number of assignments of SOL := ‚àÉ(z ‚àà
werementionedthroughoutthepaper: ¬¨f
F)œà (x,S,z).
¬¨f
AppendixAdescribestheabstractqueryform.
AppendixBdescribesthespecificmodeltypes,andtheuniversal Onceagain,itisworthnotingthatintheseparticularscenarios,
modelproperties. the value of S is derived from a subset of the input I for the CC
AppendixCcontainsananalysisofthemodel-specificproperties. query,whileforMCR,itisdefinedwithinSOL ¬¨f.Now,weproceed
AppendixDincludestheproofsforthetheoremsandpropositions todemonstratehowthesufficiency-basedexplainabilityquery(MSR)
mentionedinthepaper. canalsobeacquired.Inthiscase,SOL ¬¨f correspondstothenegation
AppendixEincludespossibleextensionsofourtheoreticalframe- ofœà ¬¨f:
work.
(Misaligned)MSR(MinimumSufficientReason):
Input:Modelf,inputx,andinputI :=‚ü®k‚ü©.
A AbstractQueryForm
Output: Yes, if SOL := ‚àÉS ‚äÜ (1,...,n) ¬¨‚àÉ(z ‚àà
¬¨f
WepresentacomprehensiveanalysisoftheabstractqueryformQ F)œà (x,S¬Ø,z)‚àß|S|‚â§kissatisfiable,andNootherwise.
¬¨f
discussedinourpaper,providingamoredetailedexplanationofour
process. It is straightforward to show that this formalization of MSR is
equivalenttoitspredefinedversions,since:
TheMisalignedCase
‚àÄ(z‚ààF) [f(x S;z S¬Ø)=f(x)] ‚áê‚áí
(8)
I fn eri rti ia nl gly t, ow the ein at bro std ru acc te qth ue erq yu fe or ry mfo Qrm thf ao tr dth oe es‚Äú nm oi tsa cl oig nn sie dd e‚Äù rc tha ese c, or ne-
-
¬¨‚àÉ(z‚ààF) [f(x S¬Ø;z S)Ã∏=f(x)]
textindicatorœÄasaninput.Weformulatethisspecificscenarioand Inotherwords,asubsetSissufficienttodetermineaprediction
demonstrateitsapplicabilityingeneralizingthevariousdiscussed f(x)ifandonlyiftheredoesnotexistanyassignmenttothecomple-
explainabilityqueries:MSR,CC,andMCR,allintheirmisaligned
mentaryS¬Øthatiscontrastive.Thisleadsustothefactthatthereexists
versions.Subsequently,weproceedtooutlinetheconsequentialquery asufficientreasonofsizekifandonlyifthereexistssomesubsetS
forthealignedscenario,whereœÄistakenintoaccount,dismantling
ofsizeksuchthatnopossibleassignmenttoS¬Øiscontrastive.
theeffectofanyOODcounterfactual.Wethenreiteratehowtheex- Wenotethatwhenevaluatingthecomplexityofthemisaligned
plainabilityqueriesofMSR,CC,andMCR,whenconsideredintheir abstractexplainabilityqueryQ,weconsideritinrelationtoasingle
alignedforms,areallspecificinstancesoftheabstractqueryQ. classofmodels.Forinstance,Q(C M)describesthecomplexityof
Letœà ¬¨f(x,S,z)denotethefollowingconjunct: obtaininga(misaligned)explainabilityqueryforamodelf ‚àà C C
using the (misaligned) abstract query form Q. Unlike the aligned
œà ¬¨f :=[f(x S¬Ø;z S)Ã∏=f(x)] (7) version,wedonotinputtwofamiliesoffunctionssincethecontext
indicatorœÄisnotincludedaspartoftheinputforthesequeries.
LetSOL denoteanSOLformulathatincludesœà ,wherethe
¬¨f ¬¨f
variable f is exclusively present in œà . In other words, f is not
¬¨f
foundinanyotherconjunctofSOL ¬¨f apartfromœà ¬¨f.Wedenotethe TheAlignedCase
(misaligned)versionofQasanyexplainabilityquerythattakesf,x,
andIasinputs,whereIrepresentsasetofadditionalarbitraryinputs. Wenowproceedtodescribethealignedversionoftheabstractquery
TheoutputofQisasatisfyingsolutiontoSOL ¬¨f orthecounting form.Inthiscase,weaimtoconstructasimilarabstractqueryform
of#SOL ¬¨f.Wearenowabletoformulatetheabstract(misaligned) withtheadditionalrequirementofneutralizingtheinfluenceofany
queryformQ: OODcounterfactuals.Thisisobtainedbyaddinganadditionalcon-
straint,namely[œÄ(x S¬Ø;z S)=1].Toformallydefinethis,weintroduce
(Misaligned)Q(AbstractQueryForm): œà (x,S,z)asfollows:
¬¨f,œÄ
Input:Modelf,inputx,andinputI.
Output:aYesorNoanswer,towhetherSOL ¬¨f holds,orthenumber œà ¬¨f,œÄ :=[f(x S¬Ø;z S)Ã∏=f(x)]‚àß[œÄ(x S¬Ø;z S)=1] (9)
ofassignmentsofSOL .
¬¨f
Similarly,wedefineSOL asanySOLformulathatincludes
¬¨f,œÄ
œà ,wheref andœÄareexclusivelyincludedwithinœà .Inother
ItisimportanttohighlightthatthevaluesofSandzareimplicitly ¬¨f,œÄ ¬¨f,œÄ
words,f andœÄ arenotpresentinanyconjunctofSOL except
presentinœà .Thesevaluescaneitherbeincludedaspartofthe ¬¨f,œÄ
¬¨f
forœà .Wedenotethe(aligned)versionofQasanexplainability
inputI,ortheycanbeexplicitlydefinedwithinSOL .Now,we ¬¨f,œÄ
¬¨f
query that takes f, x, œÄ, and I as inputs, where I represents an
demonstratehowtheaforementionedexplainabilityqueries(intheir
arbitrarysetofadditionalinputs.TheoutputofQisasolution(either
misalignedform)canbepreciselyformulatedasspecificinstancesof
decisionorcounting),overSOL .Therefore,theabstractquery
Q.Westartbyillustratingthisfortherelativelysimplerscenariosof ¬¨f,œÄ
form,inthiscase,canbeexpressedasfollows:
MCRandCC:
(Misaligned)MCR(MinimumChangeRequired): Q(AbstractQueryForm):
Input:Modelf,inputx,andinputI :=‚ü®k‚ü©. Input:Modelf,inputx,contextindicatorœÄ,andinputI.
Output:Yes,ifSOL
¬¨f
:=‚àÉS ‚äÜ(1,...,n) ‚àÉ(z‚ààF)œà ¬¨f(x,S,z)‚àß Output:aYesorNoanswer,towhetherSOL ¬¨f,œÄholds,orthenumber
|S|‚â§kissatisfiable,andNootherwise. ofassignmentsofSOL ¬¨f,œÄ.Webrieflyillustratehowthisabstractqueryformencompassesall Multi-LayerPerceptron(MLP).Givenasetoftweightmatrices
thepreviouslydefinedexplainabilityqueries,includingthealigned W(1),...,W(t),tbiasvectorsb(1),...,b(t) andtactivationfunc-
versionsofMSR,MCR,andCC.Onceagain,itisstraightforwardto tionsf(1),...,f(t),aMulti-LayerPerceptron(MLP)[34,76]f,with
showthatMCRandCCareinstancesofthisabstractqueryform(this t‚àí1hiddenlayers(hj forj ‚àà{1,...,t‚àí1})andasingleoutput
time,inthealignedversion): layer (ht), is recursively defined based on the following series of
functions:
MCR(MinimumChangeRequired):
Input:Modelf,inputx,contextindicatorœÄ,andinputI :=‚ü®k‚ü©.
Output: Yes, if SOL := ‚àÉS ‚äÜ (1,...,n) ‚àÉ(z ‚àà h(j) :=œÉ(j)(h(j‚àí1)W(j)+b(j)) (j ‚àà{1,...,t}) (11)
¬¨f,œÄ
F)œà ¬¨f,œÄ(x,S,z)‚àß|S|‚â§kissatisfiable,andNootherwise. f outputsthevalueofthefunctionf := h(t),andh(0) := x ‚àà
{0,1}n corresponds to the input of the model. The weight matri-
cesandbiasesaredefinedbyaseriesofpositivevaluesd ,...,d
CC(CountCompletions): 0 t
representingthedimensionsoftheirinputs.Inaddition,weassume
Input:Modelf,inputx,contextindicatorœÄ,andinputI :=‚ü®S‚ü©.
thatalltheweightsandbiases(learnedduringtraining)haveratio-
Output: The number of assignments of SOL := ‚àÉ(z ‚àà
¬¨f,œÄ nal values, i.e., W(j) ‚àà Qdj‚àí1√ódj and b(j) ‚àà Qdj. Notice that
F)œà (x,S,z).
¬¨f,œÄ due to our focus on binary classifiers over {0,1}n, then it holds
that: d = n and d = 1. Furthermore, we consider the popular
Similarly to the misaligned case, the aligned version of the 0 t
ReLU(x) = max(0,x)activationfunction.Thelastactivationof
sufficiency-basedquery(MSR)canbeobtainedasfollows:
MLPsistypicallyasigmoidfunction,butsinceweareonlyinter-
estedinpost-hocinterpretations,wecanequivalently,withoutloss
MSR(MinimumSufficientReason):
ofgenerality,considerthelastactivationtocorrespondtothestep
Input:Modelf,inputx,contextindicatorœÄ,andinputI :=‚ü®k‚ü©.
function:
Output: Yes, if SOL := ‚àÉS ‚äÜ (1,...,n) ¬¨‚àÉ(z ‚àà (cid:40)
¬¨f,œÄ 1, y>0
F)œà (x,S¬Ø,z)‚àß|S|‚â§kissatisfiable,andNootherwise. step(y)= (12)
¬¨f,œÄ
0, y‚â§0
Theequivalencebetweenthesespecificinstancesandthepredefined
Perceptron.APerceptron[75]isanMLPwithasinglelayer(i.e.,
explainabilityqueriesholdsintheseparticularscenarios.Thisisdue
t=1):f(x)=œÉ(‚ü®W,x‚ü©+b),forW ‚ààQn√ód1 andb‚ààQ.Hence,
tothefollowing:
withoutlossofgenerality,foraPerceptronf itholdsthat:
‚àÄ(z‚ààF) [œÄ(x S;z S¬Ø)=1‚Üíf(x S;z S¬Ø)=f(x)] ‚áê‚áí
(10)
f(x)=1 ‚áê‚áí ‚ü®W,x‚ü©+b>0 (13)
¬¨‚àÉ(z‚ààF) [f(x S¬Ø;z S)Ã∏=f(x)]‚àß[œÄ(x S¬Ø;z S)=1]
UniversalProperties
RecallthatinthecaseofthealignedversionofQ,theunderlying
computationalcomplexityisevaluatedbyconsideringtwoclasses Next,weprovidethepreciseformalizationfortheuniversalproper-
ofmodels:C fortheclassificationmodelandC forthecontext tiesoverC andC thatwerementionedwithinourstudy.These
M œÄ M œÄ
indicator.Forinstance,Q(C ,C )representsthecomputationalcom- arethatC issymmetricallyconstructible,whereasC isnaively
M œÄ œÄ M
plexityofobtaininganexplainabilityqueryformodelsf ‚ààC with construcatble.
M
respecttothecontextindicatorsœÄ‚ààC andanabstractqueryform
œÄ
Definition1. AclassoffunctionsCissymmetricallyconstructibleif
Q.
givenamodelf ‚ààC,then¬¨f ‚ààCcanbeconstructedinpolynomial
time.
B ModelTypesandUniversalProperties
Definition2. AclassoffunctionsCisnaivelyconstructibleifforany
ModelTypes
valuex‚ààF,then1 ‚ààCcanbeconstructedinpolynomialtime.
{x}
Next,weprovideafulldescriptionofthemodelsthataretakeninto
accountwithinourwork. C Model-SpecificProperties
BinaryDecisionDiagram(BDD).ABDD[62]isagraphicalrepre-
Asmentionedabove,ourpropositionsandtheoremsarebasedonthe
sentationofaBooleanfunctionf :F‚Üí{0,1},realizedbyadirected,
universalpropertiesformulatedinSectionBoftheappendix.These
acyclicgraph,forwhich:(i)eachinternalnodev(i.e.,non-sinknodes)
qualitiesincludesymmetricconstructabilityandnaiveconstructability.
correspondstoasinglefeature(1,...,n);(ii)eachinternalnodev
Inthissection,weillustratehowthesepropertiesindeedholdforthe
has precisely two output edges, representing the values {0,1} as-
particularmodelsdiscussedinourwork,namelyFBDDs,Perceptrons,
signedtov;(iii)eachleafcorrespondstoeitheratrue,orfalse,label;
andMLPs.Weemphasizethattheseareonlyparticularillustrations,
and(iv)eachvariableappearsatmostonce,alonganygivenpathŒ±
andthatthesepropertiescanbeproventoholdforabroaderrangeof
withintheBDD.
hypothesisclasses.First,werecallProposition1:
Hence,everypathŒ±fromtherootnodetoaleaf,correspondsto
aspecificinputassignmentx‚ààF,withf(x)matchingthevalueof Proposition1. FBDDs,Perceptrons,andMLPsareallsymmetrically
theleafoftherelevantpathŒ±.Followingpreviousconventions[12, constructibleandnaivelyconstructible.
44,45,9],weregardthesize|f|oftheBDDtobethetotalnumber
Tothisend,weprovethefollowinglemmas.
ofedges.Wefocusonthepopularhypothesisclassof‚ÄúFreeBDDs‚Äù
(FBDDs),inwhichdifferentpathsmayhavevariousorderingsofthe Lemma1. TheclassC isnaivelyconstructibleandsymmetrically
FBDD
inputvariables{1,...,n}. constructible.Itisstraightforwardtoshowthisinthefollowingmanner:(i)given
anFBDDf wecanconstruct¬¨f byduplicatingf andnegatingall
leafnodesvintheduplicateddiagram;and(ii)givenaninputx‚ààF 1 ùë•
1 ùë§+
wecansimplyconstructanFBDDf withasingleacceptingpathŒ±
matchingtheassignmentofx. 1 ùë• 2 ùë§+
‚àíœÉ (‚Ñé1 ùë•)+0.5
ùë§‚àí 1‚â§ùëñ‚â§ùëõ ùëñ ùëñ
Lemma2. TheclassC MLPisnaivelyconstructibleandsymmetrically 0 ùë•
3
ùë†ùë°ùëíùëù
constructible. ùë§+
1 ùë•
4 ùë§‚àí
MLPs can also be constructed symmetrically and naively in a
straightforwardmanner.First,westatethatforeveryMLPf,wecan 0 ùë•
5
construct,inlineartime,anequivalentMLPf‚Ä≤,suchthattheweights
andbiasesareintegers(thiscanbeachievedbymultiplyingthevalues
Figure2:AnillustrationofthenaiveconstructabilityofaPerceptron
bythelowestcommondenominator,asdonein[12]).Next,forthe
model,indicatingthevalue[1,1,0,1,0],andw+ =1,w‚àí =(‚àí1).
biasinthelastlayer,wealsoadd‚àí0.5.Thisprocedureguaranteesthat: Thebiastermis[‚àí(cid:80) (h1¬∑x )]+0.5=(‚àí3)+0.5=(‚àí2.5).
(i)foreveryinputx‚àà{0,1}n,itholdsthatf(x)=f‚Ä≤(x),i.e.,the 1‚â§i‚â§n i i
newMLPf‚Ä≤isequivalenttof;and(ii)thereisnobinaryinputxsuch
Theorem1. If1‚ààC thenQ(C )‚â§ Q(C ,C ).
thatforf‚Ä≤ =step(h‚Ä≤(t‚àí1)W‚Ä≤(j)+b‚Ä≤(t))itholdsthat(h‚Ä≤(t‚àí1)W‚Ä≤(j)+ œÄ M p M œÄ
b‚Ä≤(t))(x)=0,i.e.,noinputxisexactlyonthedecisionboundaryof Proof. Theproofisstraightforwardsincegivensome‚ü®f,x,I‚ü©the
f‚Ä≤(asalllinearcombinationsofintegers‚Äîremainintegers,andthe reductioncansimplyencodeandreturn‚ü®f,x,1,I‚ü©.Clearly,itholds
singlebiasisnotaninteger).Next,symmetricconstructabilityforf‚Ä≤ that:‚ü®f,x,I‚ü© ‚àà Q(C M) ‚áê‚áí ‚ü®f,x,1,I‚ü© ‚àà Q(C M,C œÄ),which
(andhence,forf)isacquiredasfollows.Wecanconstruct¬¨f‚Ä≤ by concludesthecorrectnessofthereduction.
negatingtheweightsofthelastlayerh‚Ä≤t(settingh‚Ä≤t =‚àíh‚Ä≤tforall
i)andnegatingthebiasoftheoutputlayerb‚Ä≤t.Sini cethelai
stlayer
Theorem2. IfC MissymmetricallyconstructibleandC œÄisnaively
constructible,thenQ(C )‚â§ Q(C ,C ).
containsasinglestepfunction,negatingthecorrespondingweights œÄ p M œÄ
(andbias)willresultinaflippedclassification. Givensome‚ü®f ,x,I‚ü©:thereductioncheckswhetherf isavalid
1 1
To show naive constructability, we make use of the following encodingofafunctioninC .Ifnot,itreturnsaninvalidencoding.If
œÄ
Lemma[12]: so,itconstructsthenegationfunction¬¨f (basedonourassumptions,
1
thiscanbecomputedinpolynomialtime).Then,thereductioncom-
Lemma3. GivenaBooleancircuitB,wecanconstruct,inpolyno-
putesf (x)andconstructs1 ‚ààC (alsoinpolynomialtime).If
mialtime,anMLPf ,whichinducesanequivalentBooleanfunction 1 {x} M
B f (x)=1,thereductionreturns‚ü®f =1 ,œÄ =¬¨f ,x,I‚ü©,andif
relativetoB. 1 2 {x} 2 1
f (x)=0,itreturns‚ü®f =1 ,œÄ =f ,x,I‚ü©.
1 2 {x} 2 1
Hence,asadirectcorollary,itispossibletopolynomiallyconstruct LetQ 1denotetheSOLformulathatcorrespondstothesolutionof
anMLPthatcorrespondstotheBooleancircuitrepresenting:x 1‚àß Q(C œÄ)andletQ
2
denotetheSOLformulathatcorrespondstothe
x 2...‚àßx n. solutionofQ(C M,C œÄ).Letusdenoteœà ¬¨f1 astheaforementioned
conjunct(seeSec.Aoftheappendix)thatcorrespondstoQ ,andby
1
Lemma4. TheclassC isnaivelyconstructibleandsymmetri-
Perceptron œà theconjunctthatcorrespondstoQ .
callyconstructible.
¬¨f2,œÄ 2
Assume‚ü®f ,x,I‚ü©‚ààQ(C ).Sinceinthiscaseitholdsthatf ‚àà
1 œÄ 1
ThesymmetricconstructionprovedforMLPsalsoholdsdirectly C œÄ,then:
forPerceptrons(bynegatingtheweightsofh1 aswellasthebias
b1).Givenaninputx‚ààF,naiveconstructabilitycanbeachievedby
œà
¬¨f1
=[f 1(x S;z S¬Ø)Ã∏=f 1(x)]
constructingamodelwherethecorrespondingsinglehiddenlayer Forœï
2
:=[œÄ 2(x S;z S¬Ø)=1]itholdsthat:
h1 isweightedsuchthath1 := w+ forx = 1andh1 := w‚àí for
i i i
x = 0,forsomeuser-definedvaluew.Thesinglebiastermb1 is
i
settob1 := ‚àí[(cid:80) 1‚â§i‚â§n(h1
i
¬∑x i)]+0.5.Theintuitionbehindthis œà ¬¨f2,œÄ =[f 2(x S;z S¬Ø)Ã∏=f 2(x)]‚àß[œÄ 2(x S;z S¬Ø)=1]
constructionisthatitmaximizesthecontributionoftheparticular =[f 2(x S;z S¬Ø)Ã∏=f 2(x)]‚àßœï 2
inputxwhilerenderingnegativevaluesforanyotherinputinF\x.
=[1 {x}(x S;z S¬Ø)Ã∏=1 {x}(x)]‚àßœï
2
AnillustrationofthisconstructionisprovidedinFig.2.Also,we
observethatthisconstructionclearlyservesasvalidproofforthe
=[1 {x}(x S;z S¬Ø)Ã∏=1]‚àßœï
2
naiveconstructabilityofMLPs,butthiswasalreadytriviallyderived
fromthepropertiesdiscussedintheprevioussection. Assumethatf (x)=1.Inthiscase,thereductionsetsœÄ to¬¨f ,
1 2 1
andthus:
D MainTheoremProofs
Inthissection,weproveallthetheoremsandpropositionspresented œï
2
=[œÄ 2(x S;z S¬Ø)=1]=[¬¨f 1(x S;z S¬Ø)=1]=
inthemaintext. [f 1(x S;z S¬Ø)Ã∏=1]=[f 1(x S;z S¬Ø)Ã∏=1]‚àß[f 1(x)=1]
Wherethelastencodedconjunctf (x) = 1isatautologyunder
TheComplexityofObtainingSociallyAligned 1
thisscenario.Overall,wegetthat:
Explanations
First,weprovidetheproofsforTheorems1and2,asdiscussedinthe
œà
¬¨f2,œÄ
=[1 {x}(x S;z S¬Ø)Ã∏=1]‚àß[f 1(x S;z S¬Ø)Ã∏=1]
maintext.Morespecifically: ‚àß[f 1(x)=1]=[f 1(x S;z S¬Ø)Ã∏=1]=œà ¬¨f1ThismeansthatQ 1andQ 2areequivalentandhenceanysolution RecallthateachqueryQ(C M)isassociatedwithanSOLformula
forSOLor#SOLwillbeequivalenttoQ(C M,C œÄ)andQ(C œÄ).Thus, SOL
¬¨f
and each aligned query Q(C M,C œÄ) is associated with an
‚ü®f 1,x,I‚ü©‚ààQ(C œÄ) ‚áê‚áí ‚ü®f 2,x,œÄ 2,I‚ü©‚ààQ(C M,C œÄ). SOLformulaSOL ¬¨f,œÄ.Bothoftheseformulascanbewrittenintheir
Assumethatf 1(x) = 0.Inthiscase,thereductionsetsœÄ 2 tof 1 PrenexNormalFormSOLk ¬¨f andSOLk ¬¨f,œÄ.Sincetheprefixesof
andthus: boththeseformulasareequivalent,thenbothofthemareassociated
withsomecomplexityclassK‚Ä≤ inthepolynomialhierarchy(orits
correspondingcountingclass),duetotheextensionofFagin‚ÄôsTheo-
œï 2 =[œÄ 2(x S;z S¬Ø)=1]=[f 1(x S;z S¬Ø)=1] rem,whichasmentioned,holdsforourcase(aswefocusondiscrete
=[f 1(x S;z S¬Ø)Ã∏=0]=[f 1(x S;z S¬Ø)Ã∏=0]‚àß[f 1(x)=0] inputsofafinitesize).
Clearly,sinceC andC areclassesofpolynomiallycomputable
M œÄ
Overall,weagainobtainthat: functions,thenbydefinitionQ(C ,C ) ‚àà K‚Ä≤ andQ(C ) ‚àà K‚Ä≤.
M œÄ M
Morespecifically,thismeansthatQ(C ) ‚àà K‚Ä≤ aswell.Next,to
MLP
œà ¬¨f2,œÄ =[1 {x}(x S;z S¬Ø)Ã∏=1]‚àß[f 1(x S;z S¬Ø)Ã∏=0] provehardness,wewillmakeuseofLemma3,aswellasKarp‚Äôs
‚àß[f 1(x)=0]=[f 1(x S;z S¬Ø)Ã∏=0]=œà
¬¨f1
reduction.
Karp‚Äôs reduction implies that any quantified propositional for-
Hence, again it holds that Q 1 and Q 2 are equivalent, and from mula‚àÉX 1‚àÄX 2,...(‚àÉ/‚àÄ)X k,œà(X 1,...,X k),foraquantifier-free
the same reason stated above, it thus holds that ‚ü®f 2,x,œÄ 2,I‚ü© ‚àà Booleanformulaœà(over(X 1,...,X k))isŒ£p k-complete.Inaddition,
Q(C M,C œÄ). asLemma3indicatesthatanarbitraryBooleanformulaœà canbe
Now,assume‚ü®f 1,x,I‚ü© Ã∏‚àà Q(C œÄ).Inthiscase,thereductionini- translated to an equivalent MLP in polynomial time, it holds that
tially checks the validity of the encoding, which includes that of Q(C )isK‚Ä≤-hard.AswehavealsoshownthatQ(C )isinK‚Ä≤,
MLP MLP
f ‚àà C œÄ.Hence,weareonlylefttocheckthecaseswheref ‚àà C œÄ wededucethatQ(C MLP)isK‚Ä≤-complete.
but‚ü®f,x,œÄ,I‚ü©Ã∏‚ààQ(C œÄ).ThisimpliesthattheSOLwasunsatisfiable Overall,wegetthatQ(C MLP)isK-completeforsomeclassinthe
or,ifQisacountingquery,that#SOLreturnedanincorrectcount. polynomialhierarchy,andalsoK‚Ä≤-complete.Eachlanguageiscom-
SincethepreviousresultdemonstratedthatQ 1andQ 2areequivalent pleteonlyforoneclassinthehierarchy(orotherwise,thehierarchy
undertheassumptionthatf ‚ààC œÄ,anyassignmenttoQ 1willhold collapses),andthusK=K‚Ä≤.
if and only if it holds to Q 2. Consequently, we can conclude that Now, we get that Q(C M,C œÄ) ‚àà K‚Ä≤ = K. Since we know that
‚ü®f 2,x,œÄ 2,I‚ü©Ã∏‚ààQ(C M,C œÄ). C
M
=C MLPorC
œÄ
=C MLP,andsinceMLPsarebothsymmetrically
constructibleandnaivelyconstructible(Lemma2)thenasaconse-
Theorem3. LetC ,C beclassesofpolynomiallycomputablefunc-
M œÄ quenceofTheorems1and2wegetthatQ(C ,C )isalsoK-hard.
M œÄ
tionssuchthatC =C orC =C .IfQ(C )isK-complete,
M MLP œÄ MLP MLP WededucethatQ(C ,C )isK-complete.
M œÄ
where K is a complexity class of the polynomial hierarchy (or its
Note.InordertorelyonFagin‚Äôsthorem[31],wemustassumea
associatedcountingclass),thenQ(C ,C )isalsoK-complete.
M œÄ finitestructure.Thisholdsinthecaseofdiscreteinputs,butnotfor
anygeneralSOLencoding(whichinfact,mayevenbeundecidable).
Proof.Thecomplexityclasseswithinthepolynomialhierarchyconsist
oftheclassesŒ£p k andŒ†p k forallk.TheclassŒ£p k isdefinedasall ‚ÄúSelf-Alignment‚Äù:IncorporatingSocialAlignment
languagesLsuchthatthereexistsapolynomialtimeTuringmachine withinaSingleModel
M andpolynomialsq ,...,q suchthat:
1 k
Inthenextsubsection,weelaborateonthegeneral,andmodel-specific,
x‚ààL ‚áê‚áí ‚àÉy ‚àÄy ,...(‚àÉ/‚àÄ)y ,|y |‚â§q (|X|) proofsforourfindingsregardingtheself-alignmentpropertyofagiven
1 2 k i i
(14)
‚àß M(x,y ,...,y )=1 modelclassC.First,wereiteratethedefinitionofself-alignment:
1 k
Ontheotherhand,Œ†pisdefinedrespectivelybyalternatingquanti- Definition4. AclassofmodelsCisself-alignedifforanyf,œÄ‚ààC,
fiers‚àÄ‚àÉ,...,andhencek Œ†p ={L|L¬Ø ‚ààŒ£p}.InthecontextofSOL, and any inputs x and I, there exists a polynomially constructible
wedefineSOLkasaformk ulaoftheform:k functiong‚ààC,suchthat:
‚ü®f,œÄ,x,I‚ü©‚ààQ(C,C) ‚áê‚áí ‚ü®g,x,I‚ü©‚ààQ(C) (16)
‚àÉX ‚àÄX ,...(‚àÉ/‚àÄ)X ,œï(X ,...,X ) (15)
1 2 k 1 k
Theorem4. GivenaclassofmodelsC,ifforanyf ,f ‚ààC,wecan
1 2
whereœïisaquantifier-freeFOLformulaover(X 1,...,X k).Extend- polynomiallyconstructg:=f 1[op]f
2
‚ààC,for[op]‚àà{‚àß,‚Üí},then
ingFagin‚Äôstheorem[31]showsthatasolutiontoanSOLformula Cisself-aligned.
withkalternatingquantifiers(startingwith‚àÉ)isŒ£p-complete.We
k
notethatthisholdsinthecaseoffinitestructures,whichisoursetting, Proof.BasedontheassumptionsonC,givenanytwomodelsf,œÄ‚àà
asforanynwehaveafinitenumberofinputs.SinceanySOLformula C,wecanconstruct,inpolynomialtime,afunctiongthatencodes
canbewrittenasanSOLformulaconsistingofalternatingquantifiers alogicalrelationbetweentheoriginalclassifierf andthecontext
‚àÉ,‚àÄ,...or‚àÄ,‚àÉ,...,thenbyextendingFagin‚Äôstheorem[31],wecan indicatorœÄ.Wedefineg‚ààCbasedontheoriginalclassificationf(x).
concludethateachSOLformulaisassociatedwithaclassinthepoly- Ourreductiondefinesslightlydifferentfunctionsg‚ààC,depending
nomialhierarchy(thisholdsinthecaseoffinitestructures,whichis onwhethertheoriginalclassificationisf(x)=1orf(x)=0.
indeedourcase,aswefocusondiscreteinputs).Forexample‚àÉ‚àÄSOL Inthecaseoff(x)=1,wedefineg‚ààCasfollows:
isŒ£p-completeand‚àÄ‚àÉSOLisŒ†p-complete(again,asinourcasethe
form2 ulaincludesmodelsthatare2 restrictedtofiniteinputs).Wenote g:=[f ‚à®¬¨œÄ]‚ààC. (17)
thatforthecountingcase,eachoneofthesecomplexityclasseshasa
Next,wenotethatforthegiveninputxitholdsthat:
correspondingassociatedcountingclass,forexample,thenumberof
satisfyingassignmentsfor‚àÉSOLor‚àÉ‚àÄSOL. g(x)=[f ‚à®¬¨œÄ](x)=f(x)‚à®œÄ(x)=1‚à®¬¨œÄ(x)=1 (18)Italsoholdsthat: Lemma5. TheclassC isself-aligned.
FBDD
œà ‚áê‚áí
¬¨f,œÄ
Proof.WerelyonProposition4andshowthecorrespondingencodings
[f(x S;z S¬Ø)Ã∏=f(x)=1]‚àß[œÄ(x S;z S¬Ø)=1] ‚áê‚áí
of‚àßand‚ÜíforC (asufficientconditionfortheself-alignmentof
FBDD
[f(x S;z S¬Ø)=0]‚àß[œÄ(x S;z S¬Ø)=1] ‚áê‚áí theclass).Morespecifically,giventwofunctionsf t,f
s
‚ààC FBDD,
[¬¨f(x S;z S¬Ø)=1]‚àß[œÄ(x S;z S¬Ø)=1] ‚áê‚áí
(19)
over{0,1}n,wecanpolynomiallyconstructanewfunctionf
k
‚àà
[(¬¨f ‚àßœÄ)(x S;z S¬Ø)=1] ‚áê‚áí C FBDD,suchthatf k =f t‚àßf s.
Asafirststep,weshowhow,givenf andf ,wecanconstruct
[¬¨(¬¨f ‚àßœÄ)(x S;z S¬Ø)=0] ‚áê‚áí
somegeneraldecisiondiagramf‚Ä≤ :=f
t
‚Üíf
os
rf‚Ä≤ :=f ‚àßf .In
t s t s
[(f ‚à®¬¨œÄ)(x S;z S¬Ø)=0] ‚áê‚áí thesecondstepofthisprocess,weexplainhowwecanreducef‚Ä≤to
[g(x S;z S¬Ø)=0Ã∏=1=g(x)] ‚áê‚áí œà ¬¨g somef k ‚ààC FBDD.
Inthecaseoff(x)=0,wedefineg‚ààCasfollows: Lemma6. Givensomef t,f
s
‚ààC FBDD,itispossibletopolynomially
constructtheBooleanfunctionsf‚Ä≤ :=f ‚Üíf andf‚Ä≤ :=f ‚àßf .
t s t s
g:=[f ‚àßœÄ]‚ààC. (20)
Andhence,iff(x)=1: Proof.Noticethatwedonotrequirethatf‚Ä≤ ‚ààC FBDD,aswelatershow,
f‚Ä≤ willbeageneralformofadecisiondiagram,andtheencoding
‚ü®f,x,œÄ,I‚ü©‚ààQ(C,C) ‚áê‚áí ‚ü®g,x,I‚ü©‚ààQ(C) (21) off‚Ä≤ willbeofsizeO(f ¬∑f ).Forf‚Ä≤ := f ‚àßf weperformthe
s t s t
followingsteps:
Inthecasethatforthegiveninputxitholdsthat:
g(x)=[f ‚àßœÄ](x)=f(x)‚àßœÄ(x)=0‚àßœÄ(x)=0 (22) 1. Createasinglecopyoff t.
2. Foreachleafnodelabeled‚Äú1‚Äùinf ,deletetheleafandconnectits
t
Italsoholdsthat: predecessortotherootofacopyoff .
s
3. Wenotethatallleafnodeslabeled‚Äú0‚Äùinf areleftunchanged.
œà ‚áê‚áí t
¬¨f,œÄ
[f(x S;z S¬Ø)Ã∏=f(x)=0]‚àß[œÄ(x S;z S¬Ø)=1] ‚áê‚áí Itishencestraightforwardtoshowthat‚àÄz‚ààF:
[f(x S;z S¬Ø)=1]‚àß[œÄ(x S;z S¬Ø)=1] ‚áê‚áí (23)
[(f ‚àßœÄ)(x S;z S¬Ø)=1] ‚áê‚áí
f‚Ä≤(z)=1 ‚áê‚áí f (z)=1‚àßf (z)=1
t s
[g(x S;z S¬Ø)=1Ã∏=0=g(x)] ‚áê‚áí œà
¬¨g
Next,wedemonstratehow,giventwofunctionsf ,f ‚àà C
Hence,forallcases,itholdsthat: t s FBDD
over{0,1}n,wecanpolynomiallyconstructafunctionf‚Ä≤,satisfying
‚ü®f,x,œÄ,I‚ü©‚ààQ(C,C) ‚áê‚áí ‚ü®g,x,I‚ü©‚ààQ(C) (24) f k =f t ‚Üíf s,againusinganencodingofsizeO(f t¬∑f s).
Specifically,ourconstructionincludesthefollowingsteps:
Thus,weconcludethatCisself-aligned,andhenceTheorem4is
proven. 1. Since the class C is symmetrically constructible (Proposi-
FBDD
tion1),wecanpolynomiallyconstructafunctionf‚Ä≤ := ¬¨f ‚àà
Proposition2. IftheconditionsinTheorem4holdforaclassof t
C .
modelsC,thenQ(C,C)= Q(C). FBDD
P 2. Weconstructanewfunctionf ‚ààC bydeletingall‚Äú0‚Äùleaf
k FBDD
nodesoff‚Ä≤,andaddingedgesbetweenthepredecessorsofthe‚Äú0‚Äù
Proof.Basedonthepreviousreduction,wecandeducethatifthe
leaves,andtherootofacopyoff .
s
conditions in Theorem 4 hold for a model class C, then C is self-
aligned,i.e.,givenf,œÄ‚ààCwecanpolynomiallyconstructafunction Itisnowstraightforwardtoshowthat‚àÄz‚ààF:
g‚ààC,suchthat:
f (z)=0 ‚áê‚áí
‚ü®f,œÄ,x,I‚ü©‚ààQ(C,C) ‚áê‚áí ‚ü®g,x,I‚ü©‚ààQ(C) k
¬¨f (z)=0‚àßf (z)=0 ‚áê‚áí
t s
Hence:
f (z)=1‚àßf (z)=0
t s
Q(C,C)‚â§ Q(C)
p Hence,f‚Ä≤ = f ‚Üí f .Fig.3depictsthisconstruction.Weem-
t s
Inaddition,fromTheorems1and2,itholdsthat: phasize that the decision diagram f‚Ä≤ is not necessarily an FBDD.
Thisisbecausethesamevariablescanrepeatthemselvesmorethan
oncewithinf‚Ä≤.Welaterprovehowanyf‚Ä≤canbereducedtosome
Q(C)‚â§ p Q(C,C) f k ‚ààC FBDD.
Finally,itisstraightforwardtoconcludethat: Lemma7. Giventheencodingoff‚Ä≤fromLemma6,wecanpolyno-
miallyreducef‚Ä≤tosomef ‚ààC .
Q(C,C)= Q(C) k FBDD
P
Proposition3. FBDDsandMLPsareself-aligned,andhence,itfol- Proof.Asmentionedearlier,f‚Ä≤isnotnecessarilyanFBDDsinceit
lowsthat:Q(C FBDD,C FBDD)= P Q(C FBDD)andQ(C MLP,C MLP)= P maycontainvariousrepeatedvariableswithinitsdiagram.Wenow
Q(C MLP). showhoweachf‚Ä≤canbereducedtosomef
k
‚ààC FBDD.ùíá
ùíï
ùíá ‚Üíùíá
ùíï ùíîss
ùüè
ùüè ùüè ùüè ùüè ùíá ùüè ùíÇ ùíá ùíî ùüè ùíÇ ùíá ùíî
ùíá ùíî ùíá ùíî ùüè ùíâ ùíá ùíî ùüè ùíâ
ùíá ùíî ùüè ùíÇ ùüè ùíÉ ùüè
ùíî ùüè
ùüè ùüè ùíÉ ùüè ùüè ùíì
ùüè ùüè ùüè ùüè ùüè ùüè ùíì ùüè ùüè
ùüè ùüè ùüè
ùüè ùüè
ùüè ùüè
Figure3:Anillustrationofthepolynomialconstructionf‚Ä≤ :=f
t
‚Üí Figure4:Anillustrationofthepolynomialconstructionoff
k
‚ààC FBDD,
f ,relyingonf ,f ‚ààC .Forf andf thedashedlinesrepresent givenf‚Ä≤.Theblueboxesrepresentanareaprunedduringourrecursive
s t s FBDD s t
pathsthatendwitha‚Äú0‚Äùleafnode,whilesolidlinesrepresentpaths procedure,inordertoconstructavalidFBDD,withoutarepetitionof
thatendwitha‚Äú1‚Äùleafnode. features.
WedefineeachpathŒ±asaconcatenationoftwosubpathsŒ± := oftheclass).Westartbyshowinghow,giventwofunctionsf ,f ‚àà
t s
[Œ± ;Œ± ],eachcorrespondingthetherelevantpathinf (orf )ac- C over{0,1}n,wecanpolynomiallyconstructanewfunction
t s s t MLP
cordingly. Since each node v corresponds to some input feature f ‚ààC ,suchthatf =f ‚à®f .Thiswilllaterimplytheexistence
k MLP k t s
i‚àà(1,...,n),wedenotex (v)asafunctionthatmapsvtoitscor- oftheaforementionedlogicrelations.Moreformally:
v
respondingfeature.Weusethecommonconventionsofparent(v),
Lemma9. Letf ,f ‚àà C ,thenf = f ‚à®f ‚àà C canbe
left(v),andright(v). t s MLP k t s MLP
Wenowdescribethefollowingrecursivealgorithmforreducingf‚Ä≤ constructedinpolynomialtime.
tosomef ‚ààC .
k FBDD
Proof.Weassumethatf consistsofthiddenlayers,whilef consists
t s
1. WetraverseonallcorrespondingpathsŒ±,startingfromtheroot ofshiddenlayers.WerecallthatourdefinitionofanMLPisdefined
nodeandtraversingdownwards.Thealgorithmdoesnotchangef‚Ä≤ recursively,orinotherwords,f :=h(t)isdefinedas:
t 1
aslongasweareontheŒ± partofthetraversion.
t
2. Reachinganodev ‚ààŒ± ,ifforallv ‚ààŒ± itholdsthatx (v )Ã∏=
s s i t v i
x v(v s),thenwerecursivelycontinuetraversingbothleftandright. h 1(j) :=œÉ 1(j)(h( 1j‚àí1)W 1(j)+b( 1j)) (j ‚àà{1,...,t}) (25)
Intuitively,thismeansthatthefeaturecorrespondingtov wasnot
partofthedecisionpathofŒ± t.
s andf
s
:=h( 2s)isdefinedas:
3. Otherwise,wereachsomenodev ‚àà Œ± suchthatthereexists
s s
av i ‚àà Œ± t inwhichx v(v i) = x v(v s).Inthiscase,wedeletev s h(j) :=œÉ(j)(h(j‚àí1)W(j)+b(j)) (j ‚àà{1,...,s}) (26)
fromf‚Ä≤.Wenowneedtoconnecttheparentofv witheitherthe 2 2 2 2 2
s
leftorrightchildofv s withinf‚Ä≤.Assumethatforx v(v i+1) = Wenowconstructf :=h(k)wherek:=max(s,t)+1.Forfully
right(x v(v i)),orinotherwordsv ileadstoarightturninthepath formulatingh wenek edtof3 ormulatetheassociatedweightsW(i)
Œ±.Inthiscase,weconnecttheparentofv withtherightchildof 3 3
s andbiastermsb(i) foreachlayeri.Noticethati = 0istheinput
v .Iftheoppositeholds,i.e.,v leadstoaleftturninthepathŒ±, 3
s i layersonocorrespondingbiasorweightisdefinedforthem,andthe
thenweconnecttheparentofv withtheleftchildofv .
s s dimensionsoftheinputlayersofbothh andh areequal(forMLPs
1 2
Intuitively,whentraversingoverthesecondpartofthepathŒ± thatreceiveinputsfromthesamedomain).
s
wecanpotentiallycomeacrosstwoscenarios.Inthefirst,wereach Assuming that s = t, we construct b(i) := b(i) ¬∑ b(i) for all
3 1 2
afeaturewithinthepaththatdidnotparticipateinŒ± t.Inthiscase, 1 ‚â§ i ‚â§ s. In other words, b 3 is a concatenation of b 1 and b 2.
wewanttocontinuetraversingbothpossiblescenarios(leavingthe Notice that given that the dimensions of the hidden layers corre-
corresponding feature in the tree). In the second case, we reach a spondingtoh 1ared1 1,...,ds 1andthecorrespondingdimensionsof
featurethatalreadyparticipatedinŒ± t.Inthisscenario,wedeletethe thehiddenlayerscorrespondingtoh 2 ared1 2,...,ds 2,thenthecor-
correspondingnodefromŒ± sandconnectitinthesamedirectionin respondingdimensionsofthehiddenlayersofh 3 inthiscaseare:
whichthecorrespondingfeatureisconnectedinŒ± s. (d1 1+d1 2),...,(ds 1+ds 2).
Hence, at the end of this recursive process, we are left with an Now,assumingthats Ã∏= t,withoutlossofgenerality,wecanas-
equivalentdiagramwhereforeachpathŒ±,notwonodesv i,v j ‚ààŒ± sumethatt > s.Foranylayer1 ‚â§ i ‚â§ sweconstructb 3 inthe
exist,suchthatx (v )=x (v ).Figure4depictsanillustrationof sameway,i.e.,b(i) := b(i)¬∑b(i).Foranys < i ‚â§ tweconstruct:
v i v j 3 1 2
thisrecursiveprocess. b(i) :=b(i)¬∑(0).Inotherwords,weconcatenatethevectorb witha
3 1 1
Thisconcludestheproofthatgiventwofunctionsf t,f s ‚ààC FBDD singlebiasterm0.Inthisparticularcase,thecorrespondingdimen-
wecanpolynomiallyconstructsomef k ‚ààC FBDD,whichissufficient sionsofh 3are:(d1 1+d1 2),...,(ds 1+ds 2),(ds 1+1+1),...,(dt 1+1).
toshowthatC FBDDisself-aligned. Finally,weconstructtheweightvectorW 3(i) ‚ààQdi 3‚àí1√ódi 3.Again,
forthecasewheres=t,weconstructitsuchthatforany1‚â§i‚â§s:
Lemma8. TheclassC isself-aligned.
MLP
(cid:32) (cid:33)
W(i) 0
Proof.WerelyonProposition4andshowthecorrespondingencodings W(i) = 1 (27)
of‚àßand‚ÜíforC
MLP
(asufficientconditionfortheself-alignment 3 0 W 2(i)Assuming,withoutlossofgenerality,thats < t,weconstructit know that MLPs are symmetrically constructible. In other words,
suchthatforany1‚â§i‚â§sthenW 3(i)isformalizedasinEquation27 givenf t ‚ààC MLPwecanpolynomiallyconstructf k :=¬¨f t ‚ààC MLP.
andforanys<i‚â§tthen: Since any logic gate can be encoded using the universal NOR
gate,wecannowpolynomiallyconstructbothf := f ‚Üí f ‚àà
k s t
W(i) =(cid:18) W 1(i) 0(cid:19) (28) C MLP andf k := f s‚àßf t ‚àà C MLP.Thiscanbedonebyrecursively
3 0 1 building the corresponding MLPs representing either the f ‚à®f
s t
or ¬¨f constructions in a polynomial number of steps. Each one
Intuitively,theconstructionofthebiastermsandtheweightmatrix s
ofthesestepsrunsinpolynomialtimeandoutputsanewMLPof
Wi layerscapturesasituationwherewe‚Äústack‚Äùthehiddenlayers
3 size O(f +f ) at each step. Hence, we conclude that MLPs are
of h and h . This is a result of the concatenated bias vectors at s t
1 2 self-aligned.
eachstep,aswellthefactthatwezeroouttheeffectoftheweights
correspondingtoh 1withthoseofh 2,andviceversa. Next,wepresenttheformalizationoftheSubsetSumproblem,used
Now,wedescribetheconstructionforthelastlayerkofh 3(recall forprovingProposition4.
thatk := max(s,t)+1).Sincewefocusonbinaryclassification,
thesingleactivationfunctionœÉofthelastlayercanbeconsidered, SSP(SubsetSumProblem):
withoutlossofgenerality,asastepfunction.Wealsodefinethebias Input:(z ,z ,...,z )setofpositiveintegers,integerk(suchthat
1 2 n
tobezero,i.e.,bk 3+1 =0.Now,weareleftwithdefiningtheweight k‚â§n)anda(target)integerT.
matrixofthelastlayer.Formally,wedefineW 3(k) ‚àà Qdk 3‚àí1√ódk 3‚àí1 , Outp (cid:80)ut:Yes,ifthereexistsasubsetS ‚äÜ(1,2,...,n)ofsizeksuch
whereeachweightinW 3(k)issomestrictlypositiveweightw 3+. that i‚ààSz i =T,andNootherwise.
Wenowprovethattheaboveencodingoff satisfiesthatf =
k k
f ‚à®f foranyvaluez‚ààF:
t s Proposition 4. While the query MCR(C ) can be solved in
Perceptron
polynomialtime,thequeryMCR(C ,C )isNP-complete.
f (z)=h(k)(z)=0 ‚áê‚áí Perceptron Perceptron
k 3
(‚àó)
step (w+¬∑ReLU(h(t))+w+¬∑ReLU(h(s)))(z)=0 ‚áê‚áí Proof.First,webrieflyexplainhowitispossibletocheckwhethera
3 3 1 3 2
(‚àó‚àó) subsetoffeaturesiscontrastiveforaPerceptronmodel,withinthe
(w+¬∑ReLU(h(t))+w+¬∑ReLU(h(s)))(z)‚â§0 ‚áê‚áí misalignedconfiguration,inpolynomialtime[12].
3 1 3 2
APerceptronisdefinedbyf = ‚ü®w,b‚ü©,wherew istheweight
w+¬∑(ReLU(h(t))+ReLU(h(s)))(z)‚â§0 ‚áê‚áí
3 1 2 vectorcorrespondingtotheinputx,andbisthebiasterm.Wecan
(‚àó‚àó‚àó)
(cid:80)
obtaintheexactvalueof x ¬∑w .Then,forthefeaturesinS,
(ReLU(h(t))+ReLU(h(s)))(z)‚â§0 ‚áê‚áí i‚ààS¬Ø i i
1 2 itispossibletolinearlyfindtheyassignmentscorrespondingtothe
(ReLU(h( 1t))+ReLU(h( 2s)))(z)=0 ‚áê‚áí maximalandminimalvaluesof(cid:80) i‚ààSy i¬∑w i.Themaximalvalueis
(‚àó‚àó‚àó‚àó) obtainedbysettingy :=1whenw ‚â•0andy :=0whenw =0.
i i i i
ReLU(h(t)(z))=0‚àßReLU(h(s)(z))=0 ‚áê‚áí Theminimalvalueisobtainedrespectively(settingy := 1when
1 2 i
h(t)(z)‚â§0‚àßh(s)(z)‚â§0 ‚áê‚áí
w
i
<0andy
i
:=0whenw
i
‚â•0).Now,wecancalculatetheentire
1 2 rangeofpossiblevaluesthatcanbeobtainedbysettingthevalues
step 1(ht 1‚àí1(z))=0‚àßstep 2(hs 2‚àí1(z))=0 ‚áê‚áí ofS¬Øtox.Iftheminimalpossiblevalueisnegativeandthemaximal
f (z)=0‚àßf (z)=0 possiblevalueispositivethenitmeansthatSisindeedcontrastive,as
s k
thereexistsasubsetoffeaturesthatcanaltertheclassification.Ifnot,
Where(*)holds,sincef isdirectlyconnectedtotheoutputsof i.e.,theentirerangeiseitherstrictlypositiveornegative,thismeans
k
bothf andf (inwhichtheirstepfunctionwasreplacedbyReLU thatSisnotcontrastive.Moreformally,Siscontrastiveifandonly
s t
activations).Thisisthecasebothforwhens = tor,withoutloss if:
ofgenerality,whens < t,replacingeachcorrespondingweightin
W 1i withasingleneuron.Equivalence(**)holdsdirectlyfromthe (cid:88) x i¬∑w i+max{(cid:88) y i¬∑w i+b|y‚ààF}>0 ‚àß
definitionofthestepfunction(seeEquation12),whileequivalence i‚ààS i‚ààS¬Ø
(***)holdsfromthefactthatw+ >0.Equivalence(****)follows (cid:88) (cid:88) (29)
3 x ¬∑w +min{ y ¬∑w +b|y‚ààF}‚â§0
fromthefactthatReLUisanon-negativefunction. i i i i
WeprovideavisualillustrationofthisconstructioninFig.5.Note
i‚ààS i‚ààS¬Ø
thatthefiguredoesnotexplicitlystatetheinnerconnectionsbetween Inotherwords,itholdsthat:
f andf thatarewithzeroweights.Now,basedonLemma2,we
t s (cid:88) (cid:88)
‚àímax{ y ¬∑w +b|y‚ààF}< x ¬∑w
i i i i
i‚ààS¬Ø i‚ààS
(30)
(cid:88)
‚â§‚àímin{ y ¬∑w +b|y‚ààF}
i i
ùíá ùíï ùë†ùë°ùëíùëù1 i‚ààS¬Ø
ùëÖùëíùêøùëà
ùëì ùë° ùë§ 3+
ùë†ùë°ùëíùëù3 Membership.Giventheaforementioneddescriptionregardingthe
ùëì
ùë†
ùëÖùëíùêøùëà ùë§ 3+ p beo rl sy hn io pm inia Nlv Pa il sid sa trti ao ign ho tff oc ro wn at rr da ,st si iv ne cere oa nso en cs anfo gr uP ee sr sc aep st ur bo sn es t, Sm ae nm d-
ùíá ùíî ùë†ùë°ùëíùëù2 validatewhetheritholdsthat|S|<k,Siscontrastivewithrespect
tof(x),aswellasthefactthatœÄ(x ;z )=1.Iftheseholdthenwe
S S
knowthat‚ü®f,œÄ,x,k‚ü©‚ààMCR(C ,C ).
Perceptron Perceptron
Figure5:Anillustrationoftheeffectiveconstructionf t‚à®f
s
‚ààC MLP,
relyingonf ,f ‚ààC .
t s MLPHardness.WereduceMCR(C Perceptron,C Perceptron)fromSSP(theSub- min{(cid:88) y ¬∑w2+b |y‚ààF}=b
setSumproblem),aclassicNP-completeproblem,previouslyformal- i i 2 2 (37)
i‚ààS
ized.Givensome‚ü®(z ,z ,...,z ),k,T‚ü©thereductionfirstchecks
1 2 n
Now,if‚ü®(z ,z ,...,z ),T‚ü©‚ààSSP,thenthereexistsasubsetof
thespecificcasewherek=n.Inthisscenario,wewanttoconstruct 1 2 n
(cid:80)
a‚Äúdummy‚Äùresult.Inotherwords,if(cid:80)n
i=1z
i
=T,wecanconstruct
f ine dat iu care tes sS tha‚äÜ t:(1,2,...,n)ofsizeksuchthat i‚ààSz
i
= T.This
a‚Äúdummy‚Äùinstanceof‚ü®f ,œÄ := f ,x := (1,1),k := 2‚ü©.Wede-
1 2
finef := ‚ü®w1,b ‚ü©wherew1 := (1,‚àí1)andb := 0.Wedefine
f 2 :=1 ‚ü®w2,b 2‚ü© su1 ch that w1 := (1,1) and b 1 1 := 1. In case that (cid:88) x i¬∑w i1 =‚àíT =‚àíb 1+ 1 4 >‚àíb 1 =
(cid:80)n i=1z
i
Ã∏=T,wecansimplyconstructafalseencoding. i‚ààS¬Ø
(38)
Ifthisisnotthecase(meaningthatk Ã∏= n),thereductioncon- ‚àímax{(cid:88) y ¬∑w1+b |y‚ààF}
structs the two following Perceptrons f := ‚ü®w1,b ‚ü© and f := i i 1
1 1 2 i‚ààS
‚ü®w2,b ‚ü©, where w1 := (‚àíz ,‚àíz ,...,‚àíz ), b := T + 1,
2 1 2 n 1 4 aswellas:
w2 := (z ,z ,...,z ),andb := ‚àíT.Thereductionconstructs
1 2 n 2
‚ü®f :=f 1,œÄ :=f 2,x:=1 n,k :=k‚ü©,and1 ndenotesaunitvector (cid:88) x ¬∑w1 =‚àíT =‚àíb + 1 <‚àíb + 1 <
ofsizen. i i 1 4 1 2
aliF gnir es dt, bn yot fice ifth aa nt dt oh ner lye ie fx :istsacontrastivereasonofsizekforf 1
i‚ààS¬Ø
‚àímin{(cid:88) y ¬∑w1+b |y‚ààF} (39)
2 i i 1
i‚ààS
‚àÉS ‚àà(1,...n),z‚ààF. |S|‚â§k‚àß
(31) Regardingw2,itholdsthat:
[f 2(x S¬Ø;z S)=1‚àßf 1(x S¬Ø;z S)Ã∏=f 1(x)]
(cid:88) x ¬∑w2 =T =‚àíb
ThismeansthatthereexistsasubsetSofsizeksuchthat: i i 2
i‚ààS¬Ø
(40)
[‚àímax{(cid:88) y i¬∑w i1+b 1|y‚ààF}<(cid:88) x i¬∑w i1 =‚àímin{(cid:88) y i¬∑w i2+b 2|y‚ààF}
i‚ààS i‚ààS¬Ø i‚ààS
‚â§‚àímin{(cid:88) y ¬∑w1+b |y‚ààF}]‚àß andhence:
i i 1 (32)
i‚ààS
[(cid:88) x ¬∑w2 ‚â§‚àímin{(cid:88) y ¬∑w2+b |y‚ààF}] ‚ü®f,œÄ,x,k‚ü©‚ààMCR(C Perceptron,C Perceptron) (41)
i i i i 2
i‚ààS¬Ø i‚ààS Giventhat‚ü®(z 1,z 2,...,z n),T‚ü©Ã∏‚ààSSP,thentheredoesnotexist
(cid:80)
Weassumeavalidencoding(sincethiscantriviallybevalidatedin asubsetoffeaturesS ‚äÜ(1,2,...,n)ofsizeksuchthat i‚ààSz i =
polynomialtime).Thefirst‚Äúdummy‚Äùvalidationcheckswhetherk= T.Thisisequivalenttosayingthattheredoesnotexistasubsetof
nand(cid:80)n z =T.Insuchacase,itholdsthat‚ü®(z ,z ),k,T‚ü©‚àà featuresS ‚äÜ(1,2,...,n)ofsizeksuchthat:
i=1 i 1 2
SSP.WealsonotethatS,whichisofsizek =2(theentireinput
domain)isanalignedcontrastivereason.Thisisduetothefactthat ((cid:88) x i¬∑w i1 ‚â§‚àíT)‚àß((cid:88) x i¬∑w i2 ‚â•T) (42)
f ((1,1))=1andf ((0,1))=0.Additionally,itholdsthatforany i‚ààS¬Ø i‚ààS¬Ø
1 1
zitsatisfiesthatf 2(z)=1.Inotherwords: meaningthattheredoesnotexistasubsetSsuchthat:
‚àÉS ‚àà(1,...n),z‚ààF. |S|‚â§k‚àß [‚àímax{(cid:88) y ¬∑w1+b|y‚ààF}<(cid:88) x ¬∑w1]‚àß
(33) i i i i
[f 2(x S¬Ø;z S)=1‚àßf 1(x S¬Ø;z S)Ã∏=f 1(x)] i‚ààS i‚ààS¬Ø
(43)
Hence‚ü®f,œÄ,x,k‚ü©‚ààMCR(C Perceptron,C Perceptron). [(cid:88) x i¬∑w i2 ‚â§‚àímin{(cid:88) y i¬∑w i2+b|y‚ààF}]
Thus,wecannowassumethatk<n.Sinceallvaluesinw1are i‚ààS¬Ø i‚ààS
negative,thenitholdsthatforanysubsetoffeaturesS: andhence:‚ü®f,œÄ,x,k‚ü© Ã∏‚àà MCR(C ,C ),concluding
Perceptron Perceptron
thereduction.
max{(cid:88) y ¬∑w1+b |y‚ààF}=b (34)
i i 1 1
Theorem 5. Assuming that P Ã∏= NP, the class C is not
i‚ààS Perceptron
self-aligned.
ItalsoclearlyholdsthatforanyS:
b ‚â•min{(cid:88) y ¬∑w1+b |y‚ààF} (35) Proof. Assume, by negation, that the model class C Perceptron is self-
1 i i 1 aligned. Hence, we deduce that given some f,œÄ ‚àà C we
Perceptron
i‚ààS can polynomially construct a function g such that ‚ü®f,œÄ,x,I‚ü© ‚àà
Sinceweknowthatk<n,andsincethesearenegativeintegers, Q(C ,C ) if and only if ‚ü®g,x,I‚ü© ‚àà Q(C ). As
Perceptron Perceptron Perceptron
thenthereexistsatleastoneintegerinthecomplementarysetS,and weknowfromProposition4thatdecidingQ(C ,C )is
Perceptron Perceptron
henceitalsoholdsthat: NP-hardandthatQ(C )isinPTIME,thefollowingclaimholds
Perceptron
max{(cid:88) y ¬∑w1+b |y‚ààF}=b > onlyifPTIME=NP.Hence,C Perceptronisnotself-aligned.
i i 1 1
i‚ààS
(36) E FrameworkExtenstions
min{(cid:88) y ¬∑w1+b |y‚ààF}+ 1
i i 1 2 Ourframeworkcanbeextendedinmultipleaxes.First,althoughwe
i‚ààS
followcommonconventions(e.g.,[9,12,89,15])andfocusonbinary
Regardingw2,sinceallofitsvaluesarepositive,thenitholdsthat: inputandoutputdomainstosimplifyourpresentation,someofourfindingsareapplicabletoanydiscreteinputoroutputdomains.Addi-
tionally,ratherthanconsideringindividualfeatures,wecanconsider
‚Äúhigh-level"features,e.g.,bygroupingmultiplefeaturestogether.This
allowsdefiningexplanations,forexample,intermsofsuper-pixels
andalsoRGBimagesinvariouspracticalsettings.
Morebroadly,thetypesofexplanationsanalyzedinthisworkare
explanationswithformalandmathematicalguarantees,commonly
discussedwithinasub-fieldofinterestknownasFormalXAI[68].One
benefitofexplanationswithformalguaranteesisthat,unlikeheuristic-
basedexplanations,theyenableamorerigorousandmathematical
analysis,allowingthestudyofcomputationalcomplexityaspectsof
obtainingexplanations[12,13].
However,thereexistsabodyofworkinFormalXAIthatfocuses
onthepracticalaspectofcomputingexplanationswithformalguaran-
tees[25,69,13,14,47,26,27,44,51].Initialeffortstocomputesuch
explanationsweredemonstratedonsimpleMLmodels,whichallow
tractablecomputationsofexplanations.Thesemodelsincludedecision
trees[53,44],linearmodels[69],monotonicclassifiers[70],andtree
ensembles[50,51,11,17].Morerecently,variousmethodshavebeen
proposedtoobtainexplanationswithformalguaranteesforneural
networks[61,91,14,13,42,54].Thistaskisconsideredacomputa-
tionallychallengingone[12,15].However,thedevelopmentofsuch
methodsisfacilitatedbytherapidadvancementofneuralnetwork
verificationtools[57,58,90,82,65,35,98,55,60,1,22,93,94,4],
whicharebeingdevelopedmorebroadlytoformallycertifyadiverse
setofprovableproperties[23,24,66,67,78,29,18,86,30,19,2,
3,5,6,71].Webelievethatourworkcanserveasasteptowards
developingamorerigorousunderstandingofthepotentialcapabilities
andlimitationsofcomputingexplanationswithformalguarantees
concerningagivendistributionofinterest.