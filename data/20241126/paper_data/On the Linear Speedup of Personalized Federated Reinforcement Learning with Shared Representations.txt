ON THE LINEAR SPEEDUP OF PERSONALIZED FED-
ERATED REINFORCEMENT LEARNING WITH SHARED
REPRESENTATIONS
GuojunXiong1‚àó, ShufanWang2,DanielJiang3,JianLi2
1HarvardUniversity,2StonyBrookUniversity,3AppliedReinforcementLearningTeam,AIatMeta
ABSTRACT
Federated reinforcement learning (FedRL) enables multiple agents to collabora-
tivelylearnapolicywithoutsharingtheirlocaltrajectoriescollectedduringagent-
environment interactions. However, in practice, the environments faced by dif-
ferentagentsareoftenheterogeneous,leadingtopoorperformancebythesingle
policylearnedbyexistingFedRLalgorithmsonindividualagents. Inthispaper,
wetakeafurtherstepandintroduceapersonalizedFedRLframework(PFEDRL)
bytakingadvantageofpossiblysharedcommonstructureamongagentsinhetero-
geneous environments. Specifically, we develop a class of PFEDRL algorithms
named PFEDRL-REP that learns (1) a shared feature representation collabora-
tively among all agents, and (2) an agent-specific weight vector personalized to
its local environment. We analyze the convergence of PFEDTD-REP, a particu-
lar instance of the framework with temporal difference (TD) learning and linear
representations. To the best of our knowledge, we are the first to prove a linear
convergencespeedupwithrespecttothenumberofagentsinthePFEDRLsetting.
Toachievethis,weshowthatPFEDTD-REPisanexampleofthefederatedtwo-
timescale stochastic approximation with Markovian noise. Experimental results
demonstrate that PFEDTD-REP, along with an extension to the control setting
based on deep Q-networks (DQN), not only improve learning in heterogeneous
settings,butalsoprovidebettergeneralizationtonewenvironments.
1 INTRODUCTION
Federated reinforcement learning (FedRL) (Nadiger et al., 2019; Liu et al., 2019; Xu et al., 2021;
Zhang et al., 2022a; Jin et al., 2022; Khodadadian et al., 2022; Yuan et al., 2023; Salgia & Chi,
2024; Woo et al., 2024; Zheng et al., 2024; Lan et al., 2024) has recently emerged as a promising
framework that blends the distributed nature of federated learning (FL) (McMahan et al., 2017)
withreinforcementlearning‚Äôs(RL)abilitytomakesequentialdecisionsovertime(Sutton&Barto,
2018). In FedRL, multiple agents collaboratively learn a single policy without sharing individual
trajectoriesthatarecollectedduringagent-environmentinteractions,protectingeachagent‚Äôsprivacy.
OnekeychallengefacingFedRLisenvironmentheterogeneity,wherethecollectedtrajectoriesmay
vary to a large extent across agents. To illustrate, consider a few existing applications of FL: on-
deviceNLPapplications(e.g., nextwordprediction, sentencecompletion, webquerysuggestions,
andspeechrecognition)fromInternetcompanies(Hardetal.,2018;Yangetal.,2018;Wangetal.,
2023b),on-devicerecommenderoradpredictionsystems(Maengetal.,2022;Kricheneetal.,2023),
andInternetofThingsapplicationslikesmarthealthcareorsmartthermostats(Nguyenetal.,2021;
Imteajetal.,2022;Zhangetal.,2022b;Boubouhetal.,2023). Notethatalloftheabove(1)existin
settingswithenvironmentheterogeneity(heterogeneoususers,devices,patients,orhomes)and(2)
couldpotentiallybenefitfromanRLproblemformulation.
Asaresult,ifallagentscollaborativelylearnasinglepolicy,whichmostexistingFedRLframeworks
do, the learned policy might perform poorly on individual agents. This calls for the design of
a personalized FedRL (PFEDRL) framework that can provide personalized policies for agents in
differentenvironments. Nevertheless,despitetherecentadvancesinFedRL,thedesignofPFEDRL
‚àóThisworkwasdonewhenG.XiongwasaPhDstudentatStonyBrookUniversity.
1
4202
voN
22
]GL.sc[
1v41051.1142:viXraTable1: ComparisonofexistingFedRLframeworksintermsofnoise; environments(Homo: Ho-
mogeneous,Hetero:Heterogeneous);usingrepresentations(Rep.L)ornot;timescale(TS),singleor
two-TS(two)updates;multiplelocalupdatesornot;personalizationacrossagentsornot;andwith
orwithoutlinearconvergencespeedupguarantee.
Method Noise Env. Rep.L TS Localupdates Personalized Linearspeedup
FedTD&FedQ(Khodadadianetal.,2022) Markov Homo. ‚úó Single ‚úì ‚úó ‚úì
FedTD(DalFabbroetal.,2023) Markov Homo. ‚úó Single ‚úó ‚úó ‚úì
FedTD(Wangetal.,2023a) Markov Hetero. ‚úó Single ‚úì ‚úó ‚úì
QAvg&PAvg(Jinetal.,2022) i.i.d. Hetero. ‚úó Single ‚úó ‚úó ‚úó
FedQ(Wooetal.,2023) Markov Hetero. ‚úó Single ‚úì ‚úó ‚úì
A3C(Shenetal.,2023) Markov Homo. ‚úó Two ‚úó ‚úó ‚úì
FedSARSA(Zhangetal.,2024) Markov Hetero. ‚úó Single ‚úì ‚úó ‚úì
PFEDRL-REP Markov Hetero. ‚úì Two ‚úì ‚úì ‚úì
anditsperformanceanalysisremains,toalargeextent,anopenquestion. Motivatedbythis,thefirst
inquiryweaimtoanswerinthispaperis:
Can we design a PFEDRL framework for agents in heterogeneous environments that not
only collaboratively learns a useful global model without sharing local trajectories, but
alsolearnsapersonalizedpolicyforeachagent?
WeaddressthisquestionbyviewingthePFEDRLprobleminheterogeneousenvironmentsasN par-
allelRLtaskswithpossiblysharedcommonstructure.Thisisinspiredbyobservationsincentralized
learning(Bengioetal.,2013;LeCunetal.,2015)andfederatedordecentralizedlearning(Collins
et al., 2021; Tziotis et al., 2023; Xiong et al., 2024), where leveraging shared (low-dimensional)
representationscanimproveperformance. Atheoreticalunderstandingofusingsharedrepresenta-
tionsamongstheterogeneousagentshasreceivedrecentemphasisinthestandardsupervisedFL(or
decentralizedlearning)setting(Collinsetal.,2021;Tziotisetal.,2023;Xiongetal.,2024).
However, a theoretical analysis of PFEDRL with shared representations is more subtle because
eachagentin PFEDRL collectsdatabyfollowingitsownpolicy(therebygeneratingaMarkovian
trajectory)andsimultaneouslyupdatesitsmodelparameters.Thisisinstarkcontrasttothestandard
FLparadigm,wheredataistypicallycollectedinani.i.d. fashion. Oursecondresearchquestionis:
How do the shared representations affect the convergence of PFEDRL under Markovian
noise,andisitpossibletoachieveanN-foldlinearconvergencespeedup?
DespitetherecentprogressinthestandardsupervisedFLsetting(Collinsetal.,2021;Tziotisetal.,
2023;Xiongetal.,2024),tothebestofourknowledge,thisquestionisstillopeninthecontextof
learningpersonalizedpoliciesinFedRLunderMarkoviannoise(seeTable1). Motivatedbythese
openquestions,ourmaincontributionsare:
‚Ä¢PFEDRL-REPframework. WeproposePFEDRL-REP,anewPFEDRLframeworkwithshared
representations. PFEDRL-REPlearnsaglobalsharedfeaturerepresentationcollaborativelyamong
agentsthroughtheaidofacentralserver,alongwithagent-specificparametersforpersonalizingto
eachagent‚Äôslocalenvironment. The PFEDRL-REP frameworkcanbepairedwithawiderangeof
RLalgorithms, includingbothvalue-basedandpolicy-basedmethodswitharbitraryfeaturerepre-
sentations.
‚Ä¢ Linear speedup for TD learning. We then introduce PFEDTD-REP, an instantiation of the
above PFEDRL-REP framework for TD learning (Sutton & Barto, 2018). We analyze its con-
vergence in a linear representation setting, proving the convergence rate of PFEDTD-REP to be
OÀú(cid:0) N‚àí2/3(T +2)‚àí2/3(cid:1) ,whereN isthenumberofagentsandT isthenumberofcommunication
rounds. This implies a linear convergence speedup for PFEDTD-REP with respect to the number
ofagents,ahighlydesirablepropertythatallowsformassiveparallelisminlarge-scalesystems. To
ourknowledge,thisisthefirstlinearspeedupresultforPFEDRLwithsharedrepresentationsunder
Markoviannoise,providingatheoreticalanswertotheempiricalobservationsinMnihetal.(2016)
thatfederatedversionsofRLalgorithmsyieldfasterconvergence. Toshowthisresult,wemakeuse
of two-timescale stochastic approximation theory and address the challenges of Markovian noise
throughaLyapunovdriftapproach.
22 PROBLEM FORMULATION
In this section, we first review the standard FedRL framework and then introduce our proposed
PFEDRL-REP framework, which incorporates personalization and shared representations. Let N
and T be thenumber of agentsand communicationrounds, respectively. Denote [N] as theset of
integers{1,...,N}and‚à•¬∑‚à•asthel -norm. Weuseboldfacetodenotematricesandvectors.
2
2.1 PRELIMINARIES: FEDERATEDREINFORCEMENTLEARNING
A FedRL system with N agents interacting with N independent heterogeneous environments is
modeledasfollows. Theenvironmentofagenti‚àà[N]isaMarkovdecisionprocess(MDP)Mi =
‚ü®S,A,Ri,Pi,Œ≥‚ü©,whereS andAarefinitestateandactionsets,Riistherewardfunction,Piisthe
transition kernel, and Œ≥ ‚àà (0,1) is the discount factor. Suppose agent i is equipped with a policy
œÄi : S ‚Üí ‚àÜ(A)(amappingfromstatestoprobabilitydistributionsoverA). Ateachtimestepk,
agent i is in state si and takes action ai according œÄi(¬∑|si), resulting in reward Ri(si,ai). The
k k k k k
environmentthentransitionstoanewstatesi accordingtoPi(¬∑|si,ai). Thissequenceofstates
k+1 k k
andactionsformsaMarkovchain,thesourceoftheaforementionedMarkoviannoise. Inthispaper,
thisMarkovchainisassumedtobeunichain,whichisknowntoasymptoticallyconvergetoasteady
state. Wedenotethestationarydistributionas¬µi,œÄi.
ThevalueofœÄi inenvironmentMi isdefinedasVi,œÄi(s)=E (cid:2)(cid:80)‚àû Œ≥kRi(si,ai)|si =s(cid:3) . In
œÄi k=0 k k 0
realisticproblemswithlargestatespaces,itisinfeasibletostoreVi,œÄi(s)forallstates,sofunction
approximationisoftenused.OneexampleisVi,œÄi(s)‚âàŒ¶Œ¶Œ¶(s)Œ∏Œ∏Œ∏,whereŒ¶Œ¶Œ¶‚ààR|S|√ódisastatefeature
representationandŒ∏Œ∏Œ∏ ‚ààRdisanunknownlow-dimensionalweightvector.
One intermediate goal in RL is to estimate the value function corresponding to a policy œÄ using
trajectories collected from the environment. This task is called policy evaluation, and one widely
used approach is temporal difference (TD) learning (Sutton, 1988). The FedRL version of TD
learning is called FedTD (Khodadadian et al., 2022; Dal Fabbro et al., 2023; Wang et al., 2023a),
whereN agentscollaborativelyevaluateasinglepolicyœÄbylearningacommon(non-personalized)
weight vector Œ∏Œ∏Œ∏, using trajectories collected from N different environments. More precisely, we
haveœÄi ‚â°œÄandŒ∏Œ∏Œ∏i ‚â°Œ∏Œ∏Œ∏,‚àÄi‚àà[N]. GivenafeaturerepresentationŒ¶Œ¶Œ¶(s),‚àÄs,thiscanbeformulated
asthefollowingoptimizationproblem:
N
m Œ∏Œ∏Œ∏in N1 (cid:88) E s‚àº¬µi,œÄ(cid:13) (cid:13)Œ¶Œ¶Œ¶(s)Œ∏Œ∏Œ∏‚àíVi,œÄ(s)(cid:13) (cid:13)2 . (1)
i=1
Due to space constraints, we focus our presentation on the policy evaluation problem. Note that
policy evaluation is an important part of RL and control, since it is a critical step for methods
basedonpolicyimprovement. OurproposedPFEDRLframework(seeAlgorithm1)canbedirectly
appliedtocontrolproblemsaswell,butwerelegatethesediscussionstoSection5andAppendixC.
2.2 PERSONALIZEDFEDRLWITHSHAREDREPRESENTATIONS
Since the local environments are heterogeneous across the N agents, the aforementioned FedRL
methods (in Section 2.1) that aim to learn a common weight vectorŒ∏Œ∏Œ∏ may perform poorly on in-
dividual agents. This necessitates the search for personalized local weight vectorsŒ∏Œ∏Œ∏i that can be
learned collaboratively among N agents in N heterogeneous environments (without sharing their
locally collected trajectories). As alluded to earlier, we view the personalized FedRL (PFEDRL)
problem as N parallel RL tasks with possibly shared common structure, and we propose that the
agents collaboratively learn a common features representationŒ¶Œ¶Œ¶ in addition to a personalized lo-
cal weights Œ∏Œ∏Œ∏i. Specifically, the value function of agent i is approximated as Vi,œÄi ‚âà fi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶),
where fi(¬∑,¬∑) is a general function parameterized by these two unknown parameters.1 The policy
1Theapproximationfi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶)isgeneralandcantakeonvariousforms,includingaslinearapproximations
orneuralnetworks. Forinstance,itcanberepresentedasalinearcombinationofŒ¶Œ¶Œ¶andŒ∏Œ∏Œ∏i,i.e.,fi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶) :=
Œ¶Œ¶Œ¶Œ∏Œ∏Œ∏i inTDwithlinearfunctionapproximation(Bhandarietal.,2018). Inaddition,fi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶)canrepresenta
deepneuralnetwork;see,e.g.,ourextensionofPFEDRLtocontrolproblemsanditsinstantiationwithDQN
(deepQ-networks)(Mnihetal.,2015)inSection5andAppendixC.
3Algorithm1PFEDRL-REP: AGeneralDescription
Input: SamplingpolicyœÄi,‚àÄi‚àà[N];
1: InitializetheglobalfeaturerepresentationŒ¶Œ¶Œ¶ 0andlocalweightvectorŒ∏Œ∏Œ∏i 0,‚àÄi‚àà[N]randomly;
2: forroundt=0,1,...,T ‚àí1do
3: foragent1,...,N do
4: Œ∏Œ∏Œ∏i
t+1
= WEIGHT UPDATE(Œ¶Œ¶Œ¶ t,Œ∏Œ∏Œ∏i t,Œ± t,K);
5: Œ¶Œ¶Œ¶i
t+1/2
= FEATURE UPDATE(Œ¶Œ¶Œ¶ t,Œ∏Œ∏Œ∏i t+1,Œ≤ t);
6: endfor
7: ServercomputesthenewglobalfeaturerepresentationŒ¶Œ¶Œ¶ t+1 = N1 (cid:80)N i=1Œ¶Œ¶Œ¶i t+1/2.
8: endfor
evaluationproblemof(1)canbeupdatedforthisnewsettingas:
min
1 (cid:88)N
min E
(cid:13)
(cid:13)fi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶(s))‚àíVi,œÄi
(s)(cid:13) (cid:13)2
, (2)
Œ¶Œ¶Œ¶ N {Œ∏Œ∏Œ∏i,‚àÄi} s‚àº¬µi,œÄi (cid:13) (cid:13)
i=1
where N agents collaboratively learn a shared feature representationŒ¶Œ¶Œ¶ via a server, along with a
personalizedlocalweightvector{Œ∏Œ∏Œ∏i,‚àÄi}usinglocaltrajectoriesateachagent.
Remark2.1. ThelearningofasharedfeaturerepresentationŒ¶Œ¶Œ¶inPFEDRLisrelatedtoideasfrom
representation learning theory (Agarwal et al., 2020; 2023), and this is believed to achieve better
generalizationperformancewithrelativelysmalltrainingdata. InconventionalFedRL,thefeature
representationŒ¶Œ¶Œ¶isgivenandfixed. Indeed,aswenumericallyverifyinSection4.3,our PFEDRL
presentsbettergeneralizationperformancetonewenvironments.
3 PFEDRL-REP ALGORITHMS
Wenowproposeaclassofalgorithmscalled PFEDRL-REP thatrealize PFEDRL withsharedrep-
resentations. PFEDRL-REPalternatescomprisesofthreemainstepsforeachagentateachcommu-
nicationround: (1)alocalweightvectorupdate;(2)alocalfeaturerepresentationupdate;and(3)a
globalfeaturerepresentationupdateviatheserver.
Steps1and2: Localweightandfeaturerepresentationupdates. Atroundt,agentiperformsan
updateonitslocalweightvectorgivenitscurrentglobalfeaturerepresentationŒ¶Œ¶Œ¶ andlocalweight
t
vectorŒ∏Œ∏Œ∏i.WealloweachagenttoperformKstepsoflocalweightvectorupdates.Oncetheupdated
t
local weight vectorŒ∏Œ∏Œ∏i is obtained, each agent i executes a one-step local update on its feature
t+1
representationtoobtainŒ¶Œ¶Œ¶i . Werepresenttheseupdatesusingthefollowinggenericnotation:
t+1/2
Œ∏Œ∏Œ∏i
t+1
= WEIGHT UPDATE(Œ¶Œ¶Œ¶ t,Œ∏Œ∏Œ∏i t,Œ± t,K) and Œ¶Œ¶Œ¶i
t+1/2
= FEATURE UPDATE(Œ¶Œ¶Œ¶ t,Œ∏Œ∏Œ∏i t+1,Œ≤ t), (3)
where Œ± and Œ≤ are learning rates for the weight and feature updates, respectively. The generic
t t
functions WEIGHT UPDATE and FEATURE UPDATE willbespecializedtotheparticularsoftheun-
derlyingRLalgorithm: inSection4wediscussthecaseofTDwithlinearfunctionapproximation
indetail,andinAppendixC,weshowinstantiationsofQ-learningandDQNinourframework.
Step3: Server-basedglobalfeaturerepresentationupdate.
N
The server computes an average of the received local feature 1 (cid:88)
Œ¶Œ¶Œ¶ = Œ¶Œ¶Œ¶i . (4)
representation updates Œ¶Œ¶Œ¶i from all agents to obtain the t+1 N t+1/2
t+1/2
i=1
nextglobalfeaturerepresentationŒ¶Œ¶Œ¶ asin(4).
t+1
The PFEDRL-REP procedurerepeats(3)and(4)andissummarizedinAlgorithm1andFigure1.
We emphasize that because PFEDRL-REP operates in an RL setting, there is no ground truth for
the value function and learning occurs through interactions with an MDP environment, resulting
in non-i.i.d. data. In contrast, in the standard FL setting (where shared representations have been
investigated), there exists a known ground truth and training data are sampled in an i.i.d. fashion
(Collinsetal.,2021;Tziotisetal.,2023;Xiongetal.,2024). Thenon-i.i.d. (Markovian)dataisthe
maintechnicalchallengethatweneedtoovercome.
4Server Server Server Œ¶ùë°+1=1 3‡∑ç3 Œ¶ùë°ùëñ
+1/2
Œ¶ùë°+1 Œ¶1 ùë°+Œ¶ 1/ùë°2 2+1/2 Œ¶ùë° Œ¶+1 ùë°3 +1/2Œ¶ùë°+1 ùëñ=1
Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3
(Œ¶ùë°,ùõâ1 ùë°) (Œ¶ùë°,ùõâùë°2) (Œ¶ùë°,ùõâùë°3) (Œ¶ùë°,ùõâ1 ùë°+1) (Œ¶ùë°,ùõâùë°2 +1) (Œ¶ùë°,ùõâùë°3 +1) (Œ¶ùë°+1,ùõâ1 ùë°+1) (Œ¶ùë°+1,ùõâùë°2 +1) (Œ¶t+1,ùõâùë°3 +1)
Env. 1 Env. 2 Env. 3 Env. 1 Env. 2 Env. 3 Env. 1 Env. 2 Env. 3
At the beginning of round t Local Weight Update Local and Global Representation Update
Figure 1: An illustrative example of PFEDRL-REP for 3 agents. (a) At the beginning of round
t, each agent i = 1,2,3 has a local weight vectorŒ∏Œ∏Œ∏i and a global feature representationŒ¶Œ¶Œ¶ . (b)
t t
Using(Œ¶Œ¶Œ¶ ,Œ∏Œ∏Œ∏i),eachagentiperformsaK-stepupdatetoobtainŒ∏Œ∏Œ∏i asin(3). NotethatŒ¶Œ¶Œ¶ remains
t t t+1 t
unchangedatthisstep.(c)Agentiupdatesthefeaturerepresentationbyexecutingaone-stepupdate
toobtainŒ¶Œ¶Œ¶i asin(3),whichdependsonbothŒ∏Œ∏Œ∏i andŒ¶Œ¶Œ¶ . Finally,eachagentisharesŒ¶Œ¶Œ¶i
t+1/2 t+1 t t+1/2
withtheserver,whichthenexecutesanaveragingstepasin(4)toproduceŒ¶Œ¶Œ¶ .Updatedparameters
t+1
arehighlightedinred,whilesharedparameters(theglobalfeaturerepresentation)areinblue.
4 PFEDTD-REP WITH LINEAR REPRESENTATION
We present PFEDTD-REP, an instance of PFEDRL-REP paired with TD learning and analyze its
convergenceinalinearrepresentationsetting.
4.1 PFEDTD-REP: ALGORITHMDESCRIPTION
Here,thegoalofN agentsistocollaborativelysolveproblem(2)whentheunderlyingRLalgorithm
is TD learning. We first need to specify WEIGHT UPDATE and FEATURE UPDATE of Algorithm 1
forthecaseofTD.Attimestepk,thestateofagentiissi,anditsvaluefunctioncanbedenoted
k
as V(si) = Œ¶Œ¶Œ¶(si)Œ∏Œ∏Œ∏i in a linear representation setting. By the standard one-step Monte Carlo
k k
approximationusedinTD,wecomputeVÀÜ(si)=ri +Œ≥Œ¶Œ¶Œ¶(si )Œ∏Œ∏Œ∏i. TheTDerrorisdefinedas
k k k+1
Œ¥i :=VÀÜ(si)‚àíV(si)=ri +Œ≥Œ¶Œ¶Œ¶(si )Œ∏Œ∏Œ∏i‚àíŒ¶Œ¶Œ¶(si)Œ∏Œ∏Œ∏i. (5)
k k k k k+1 k
Thegoalofagentiistominimizethefollowinglossfunctionforeverysi ‚ààS
k
Li(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)= 1(cid:13) (cid:13)V(si)‚àíVÀÜ(si)(cid:13) (cid:13)2 , (6)
k 2 k k
with VÀÜ(si) treated as a constant. We now denote the Markovian observations of agent i at the
k
k-th time step of communication round t as Xi := (si ,ri ,si ). Note that the observa-
t,k t,k t,k t,k+1
tion sequences {Xi ,‚àÄt,k} differ across agents in heterogeneous environments. We assume that
t,k
{Xi ,‚àÄt,k}arestatisticallyindependentacrossallagents.
t,k
Localweightvectorupdate. Asinline4ofAlgorithm1,giventhecurrentglobalfeaturerepresen-
tationŒ¶Œ¶Œ¶ ,eachagentitakesK localupdatestepsonitslocalweightvectorŒ∏Œ∏Œ∏i as
t t
Œ∏Œ∏Œ∏i =Œ∏Œ∏Œ∏i +Œ± g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,Xi ), (7)
t,k t,k‚àí1 t t,k‚àí1 t t,k‚àí1
for k ‚àà [K], where g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,Xi ) is the negative stochastic gradient of the loss function
t,k‚àí1 t t,k‚àí1
Li(Œ¶Œ¶Œ¶ (si ),Œ∏Œ∏Œ∏i )withrespecttoŒ∏Œ∏Œ∏,giventhecurrentfeaturerepresentationŒ¶Œ¶Œ¶ :
t t,k‚àí1 t,k‚àí1 t
g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,Xi ):=‚àí‚àá Li(Œ¶Œ¶Œ¶ (si ),Œ∏Œ∏Œ∏i )=Œ¥i Œ¶Œ¶Œ¶ (si )‚ä∫ . (8)
t,k‚àí1 t t,k‚àí1 Œ∏Œ∏Œ∏ t t,k‚àí1 t,k‚àí1 t,k‚àí1 t t,k‚àí1
Sincethereare K stepsoflocalupdates, wedenoteŒ∏Œ∏Œ∏i := Œ∏Œ∏Œ∏i . Wefurtheradda norm-scaling
t+1 t,K
(i.e.,clipping)stepfortheupdatedweightvectorsŒ∏Œ∏Œ∏i ,i.e.,enforcing‚à•Œ∏Œ∏Œ∏i ‚à•‚â§B,tostabilizethe
t+1 t+1
update. Thisisessentialforthefinite-timeconvergenceanalysisinSection4.2,andthistechniqueis
widelyusedinconventionalTDlearningwithlinearfunctionapproximation(Bhandarietal.,2018).
Localfeaturerepresentationupdate. Asinline5ofAlgorithm1,giventheupdatedlocalweight
vectorŒ∏Œ∏Œ∏i ,agentithenexecutesaone-steplocalupdateontheglobalfeaturerepresentation:
t+1
Œ¶Œ¶Œ¶i =Œ¶Œ¶Œ¶ +Œ≤ h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,{Xi }K ), (9)
t+1/2 t t t+1 t t,k‚àí1 k=1
5whereh(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,{Xi }K )isthenegativestochasticgradientofthelossLi(Œ¶Œ¶Œ¶ (si ),Œ∏Œ∏Œ∏i )
t+1 t t,k‚àí1 k=1 t t,k‚àí1 t+1
withrespecttothecurrentglobalfeaturerepresentationŒ¶Œ¶Œ¶ ,satisfying
t
‚ä∫
h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,Xi ):=‚àí‚àá Li(Œ¶Œ¶Œ¶ (si ),Œ∏Œ∏Œ∏i )=Œ¥i Œ∏Œ∏Œ∏i . (10)
t+1 t t,k‚àí1 Œ¶Œ¶Œ¶ t t,k‚àí1 t+1 t,k‚àí1 t+1
Server-basedglobalfeaturerepresentationupdate. Asinline7ofAlgorithm1,theserverthen
averages the received local feature representation updates in (9) to obtain the next global feature
representation:
N
1 (cid:88)
Œ¶Œ¶Œ¶ =Œ¶Œ¶Œ¶ +Œ≤ ¬∑ h(Œ∏Œ∏Œ∏j ,Œ¶Œ¶Œ¶ ,{Xi }K ). (11)
t+1 t t N t+1 t t,k‚àí1 k=1
j=1
Thefullpseudo-codeofPFEDTD-REPisgiveninAppendixB.
4.2 CONVERGENCEANALYSIS
Thecoupledupdatesin(7)and(11)canbeviewedasafederatednonlineartwo-timescalestochastic
approximation(2TSA)(Doan,2021)withMarkoviannoise,withŒ∏Œ∏Œ∏i updatingonafastertimescale
t
andŒ¶Œ¶Œ¶ on a slower timescale. We aim to establish the finite-time convergence rate of the 2TSA
t
coupledupdates(7)and(11). Thisisequivalenttofindingasolutionpair(Œ¶Œ¶Œ¶‚àó,{Œ∏Œ∏Œ∏i,‚àó,‚àÄi})suchthat2
E [g(Œ∏Œ∏Œ∏i,‚àó,Œ¶Œ¶Œ¶‚àó,Xi)]=0 and E [h(Œ∏Œ∏Œ∏i,‚àó,Œ¶Œ¶Œ¶‚àó,Xi)]=0 (12)
si‚àº¬µi,si ‚àºPi (¬∑|si) t si‚àº¬µi,si ‚àºPi (¬∑|si) t
t t+1 œÄi t t t+1 œÄi t
holdforallMarkovianobservationsXi. Here,¬µi istheunknownstationarydistributionofstatesi
t t
ofagentiatt,andPi isthetransitionkernelofagentiunderpolicyœÄi.
œÄi
Althoughtheroot(Œ¶Œ¶Œ¶‚àó,{Œ∏Œ∏Œ∏i,‚àó,‚àÄi})ofthenonlinear2TSAin(7)and(11)isnotuniqueduetosimple
permutations (rotations), it is proved in Tsitsiklis & Van Roy (1996) that the standard TD iterates
convergeasymptoticallytoavectorŒ∏Œ∏Œ∏‚àógivenafixedfeaturerepresentationŒ¶Œ¶Œ¶almostsurely,whereŒ∏Œ∏Œ∏‚àó
istheuniquesolutionofacertainprojectedBellmanequation. Hence,foragenti,inordertostudy
the stability ofŒ∏Œ∏Œ∏i when the feature representationŒ¶Œ¶Œ¶ is fixed, we note that there exists a mapping
Œ∏Œ∏Œ∏i =yi(Œ¶Œ¶Œ¶)thatmapsŒ¶Œ¶Œ¶totheuniquesolutionofE [g(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,Xi)]=0.
si‚àº¬µi,si ‚àºPi (¬∑|si) t
t t+1 œÄi t
Inspired by Doan (2020), the finite-time analysis of a 2TSA boils down to the choice of two step
sizes {Œ± ,Œ≤ ,‚àÄt} and a Lyapunov function that couples the two iterates in (7) and (11). We first
t t
definethefollowingtwoerrorterms:
Œ¶Œ¶Œ¶Àú =Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó and Œ∏Œ∏ÀúŒ∏i =Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), ‚àÄi‚àà[N], (13)
t t t t t
which together characterizes the coupling between {Œ∏Œ∏Œ∏i ,‚àÄi} andŒ¶Œ¶Œ¶ . If {Œ∏Œ∏ÀúŒ∏i ,‚àÄi} andŒ¶Œ¶Œ¶Àú go to
t+1 t t+1 t
zero simultaneously, the convergence of ({Œ∏Œ∏Œ∏i ,‚àÄi},Œ¶Œ¶Œ¶ ) to ({Œ∏Œ∏Œ∏i,‚àó,‚àÄi},Œ¶Œ¶Œ¶‚àó) can be established.
t+1 t
Thus, to prove the convergence of ({Œ∏Œ∏Œ∏i ,‚àÄi},Œ¶Œ¶Œ¶ ) of the 2TSA in (7) and (11) to its true value
t+1 t
(Œ¶Œ¶Œ¶‚àó,{Œ∏Œ∏Œ∏i,‚àó,‚àÄi}), we define the following weighted Lyapunov function to explicitly couple the fast
andslowiterates
N
M({Œ∏Œ∏Œ∏i ,‚àÄi},Œ¶Œ¶Œ¶ ):=‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2+ Œ≤ t‚àí1 1 (cid:88) ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2. (14)
t+1 t t Œ± N t+1 t
t
i=1
Remark4.1. NotethattheLyapunovfunction(14)for2TSAdoesnotinherentlyrequirethesolution
tobeunique. Ifmultiplesolutionsorequilibriaexist,theLyapunovfunctionshouldstillbeableto
show that the system will converge to one of these possible equilibria, ensuring that the system‚Äôs
statedoesnotdivergeandeventuallystabilizesatsomeequilibriumpoint,whichhighlydependson
the initialization ofŒ¶Œ¶Œ¶ . To clarify this, in the rest of this paper, we useŒ¶Œ¶Œ¶‚àó to clearly indicate the
0 0
dependenceoftheinitializationofŒ¶Œ¶Œ¶,andŒ¶Œ¶Œ¶‚àó in(14)isinterchangeablewithŒ¶Œ¶Œ¶‚àó,whichdenotesthe
0
optimumclosetotheinitialpoint.
2The root (Œ¶Œ¶Œ¶‚àó,{Œ∏Œ∏Œ∏i,‚àó,‚àÄi}) of the nonlinear 2TSA in (7) and (11) can be established by using the ODE
methodfollowingthesolutionofsuitablydefineddifferentialequations(Doan,2021;2020;Chenetal.,2019)
asin(12).
6Our goal is to characterize the finite-time convergence of E[M({Œ∏Œ∏Œ∏i ,‚àÄi},Œ¶Œ¶Œ¶ )], the Lyapunov
t+1 t
functionin(14). Westartwithsomestandardassumptionsfirst.
Assumption4.2. ThelearningratesŒ± andŒ≤ satisfythefollowingconditions:
(i)(cid:80)‚àû
Œ± = ‚àû,
t t t=0 t
(ii)(cid:80)‚àû Œ±2 < ‚àû, (iii)(cid:80)‚àû Œ≤ = ‚àû, (iv)(cid:80)‚àû Œ≤2 < ‚àû, (v)Œ≤ /Œ± isnon-increasingint, and
t=0 t t=0 t t=0 t t t
(vi)lim Œ≤ /Œ± =0.
t‚Üí‚àû t t
Assumption4.3. Agenti‚ÄôsMarkovchain{Xi}isirreducibleandaperiodic. Hence,thereexistsa
t
unique stationary distribution ¬µi (Levin & Peres, 2017) and constants C > 0 and œÅ ‚àà (0,1) such
thatd (P(Xi|Xi = x),¬µi) ‚â§ CœÅk,‚àÄk ‚â• 0,x ‚àà X,whered (¬∑,¬∑)isthetotal-variation(TV)
TV k 0 TV
distance(Levin&Peres,2017).
Remark4.4. Assumption4.3impliesthattheMarkovchaininducedbyœÄi admitsauniquestation-
ary distribution ¬µi. This assumption is commonly used in the asymptotic convergence analysis of
stochasticapproximationunderMarkoviannoise(Borkar,2009;Chenetal.,2019).
Wecandefinethesteady-statelocalTDupdatedirectionas
g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶):=E [g(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,Xi)],
si‚àº¬µi,si ‚àºPi (¬∑|si) t
t t+1 œÄi t
h¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶):=E [h(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,Xi)]. (15)
si‚àº¬µi,si ‚àºPi (¬∑|si) t
t t+1 œÄi t
Definition4.5(Mixingtime,similartoChenetal.(2019)). First,definethediscrepancyterm
Œæ (Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,x)=max(cid:8) ‚à•E[g(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,Xi)|X =x]‚àíg¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶)‚à•, ‚à•E[h(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,Xi)|X =x]‚àíh¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶)‚à•(cid:9) .
t t 0 t 0
ForŒ¥ >0,themixingtimeisdefinedas
œÑ = maxmin(cid:8) t‚â•1:Œæ (Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,x)‚â§Œ¥(‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+‚à•Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+1),‚àÄk ‚â•t,‚àÄ(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶,x)(cid:9) ,
Œ¥ k
i‚àà[N]
whichdescribesthetimeittakesforallagents‚Äôtrajectories(Markovchains)tobewell-represented
bytheirstationarydistributions.
Lemma4.6. g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)in(8)isgloballyLipschitzcontinuous w.r.tŒ∏Œ∏Œ∏ andŒ¶Œ¶Œ¶ uniformlyinX, i.e.,
‚à•g(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚àíg(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚à•‚â§L (‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•),‚àÄX ‚ààX.
1 1 2 2 g 1 2 1 2
Lemma4.7. h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)in(10)isgloballyLipschitzcontinuousw.r.tŒ∏Œ∏Œ∏andŒ¶Œ¶Œ¶uniformlyinX,i.e.,
‚à•h(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚àíh(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚à•‚â§L (‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•),‚àÄX ‚ààX.
1 1 2 2 h 1 2 1 2
Lemma4.8. yi(Œ¶Œ¶Œ¶),‚àÄiisLipschitzcontinuousinŒ¶Œ¶Œ¶,i.e.,‚à•yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ )‚à•‚â§L ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•.
1 2 y 1 2
Fornotationalsimplicity,weletL:=max{L ,L ,L }andassumethatListhecommonLipschitz
g h y
constantinLemmas4.6-4.8inthefollows.
Remark4.9. TheLipschitzcontinuityofhguaranteestheexistenceofasolutionŒ¶Œ¶Œ¶totheequilib-
rium(12)forafixedŒ∏Œ∏Œ∏,whiletheLipschitzcontinuityofgandyiensurestheexistenceofasolution
Œ∏Œ∏Œ∏iof(12)whenŒ¶Œ¶Œ¶isfixed.
Lemma4.10. Thereexistsaœâ >0suchthat‚àÄŒ¶Œ¶Œ¶,Œ∏Œ∏Œ∏and‚àÄi:
‚ü®Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó,h¬Ø(yi(Œ¶Œ¶Œ¶),Œ¶Œ¶Œ¶)‚ü©‚â§‚àíœâ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶‚à•2, (cid:10) Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ),g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶ )(cid:11) ‚â§‚àíœâ‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶)‚à•2.
0 0 t t‚àí1 t t‚àí1
Remark4.11. Lemma4.10guaranteesthestabilityofthetwo-timescaleupdatein(7)and(11),and
can be viewed as the monotone property of nonlinear mappings leveraged in Doan (2020); Chen
etal.(2019).
Lemma 4.12. Under Assumption 4.3, and Lemma 4.6 and 4.7, there exist constants C > 0,
œÅ ‚àà (0,1) and L = max(L ,L ,max g(Œ∏Œ∏Œ∏‚àó,Œ¶Œ¶Œ¶‚àó,X),max h(Œ∏Œ∏Œ∏‚àó,Œ¶Œ¶Œ¶‚àó,X)) such that œÑ ‚â§
1 g h X X Œ¥
log(1/Œ¥)+log(2L1Cd),andlim Œ¥œÑ =0.
log(1/œÅ) Œ¥‚Üí0 Œ¥
4.2.1 MAINRESULTS
Wenowpresentourmaintheoreticalresultsinthiswork.
Theorem4.13. LetT ‚â•2œÑ forsomeŒ¥ >0. Supposethatthelearningratesarechosenas
Œ¥
Œ± =Œ± /(t+2)5/6 and Œ≤ =Œ≤ /(t+2),
t 0 t 0
7(cid:112)
whereŒ± ‚â§1/(2L 2(1+L2)),Œ≤ ‚â§œâ/2,andL=max{L ,L ,L }. Wehave
0 0 g h y
M({Œ∏Œ∏Œ∏i},Œ¶Œ¶Œ¶ ) C
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§ 1 0 + 2
T+2 T+1 (T +2)2 (T +2)2/3
(cid:18) N (cid:19)
+ C 1 E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ 1 E(cid:88) ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 , (16)
(T +2)2/3 0 0 N 1 0
i=1
withC =(4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2(3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2)
2 0 0 0 0 0
andC =(144œÑ2K2L2Œ¥2+4L2/N)Œ± Œ≤ .
1 Œ¥ 0 0
Thefirsttermoftheright-handsideof(16)correspondstothebiasduetoinitialization,whichgoes
tozeroatarateO(1/T2). ThesecondtermisduetothevarianceoftheMarkoviannoise. Thethird
termcorrespondstotheaccumulatedestimationerrorofthetwo-timescaleupdate. Thesecondand
thirdtermsdecayatarateO(1/T2/3),andhencedominatetheoverallconvergenceratein(16).
Remark4.14. Doan(2020)providedthefirstfinite-timeanalysisforgeneralnonlinear2TSAunder
i.i.d noise, and then extended it to the Markovian noise setting under the assumptions that both
g¬Ø and h¬Ø functions are monotone in both parameters (Doan, 2021). Since Doan (2021) leverages
the methods from Doan (2020), it needs a detailed characterization of the covariance between the
error induced by Markovian noise and the residual error of the parameters in (13), rendering the
convergence analysis much more intricate. To address this and inspired by the single-timescale
SA (Srikant & Ying, 2019), we use a Lyapunov drift approach to capture the evolution of two
coupled parameters under Markovian noise, and the characterization of impacts of a norm-scaling
stepfurtherdistinguishesourwork.
Corollary4.15. SupposethatŒ≤ =o(N‚àí2/3)andthatT2 >N. Then,wehave
0
(cid:18) (cid:19)
1
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§O .
t+2 t+1 N2/3(T +2)2/3
Remark 4.16.
Corollary4.15indicatesthattoattainanœµaccuracy,ittakesO(cid:0) œµ‚àí3/2(cid:1)
stepswitha
convergencerateO(cid:0) T‚àí2/3(cid:1) ,whileO(cid:0) N‚àí1œµ‚àí3/2(cid:1) stepswithaconvergencerateO(cid:0) N‚àí2/3T‚àí2/3(cid:1)
(the hidden constants in O(¬∑) are the same). In this sense, we prove that PFEDTD-REP achieves
a linear convergence speedup with respect to the number of agents N, i.e., we can proportionally
decreaseT asN increaseswhilekeepingthesameconvergencerate. Toourbestknowledge,thisis
thefirstlinearspeedupresultforpersonalizedFedRLwithsharedrepresentationsunderMarkovian
noise,andishighlydesirablesinceitimpliesthatonecanefficientlyleveragethemassiveparallelism
inlarge-scalesystems.Recently,Shenetal.(2023)considereda2TSAinafederatedRLsettingand
achievedaconvergencerateofO(cid:0) T‚àí2/5(cid:1) andthusasamplecomplexityofO(cid:0) œµ‚àí5/2(cid:1)
. Incontrast,
ourmethodcanconvergequickerandenjoysalowersamplecomplexity,andtheconvergencespeed
matchesthebest-knownconvergencespeedfornon-linear2TSAundereveni.i.dnoise(Doan,2020).
Inaddition,wenotethatsingle-timescale(SA)methodsmayenjoyafasterconvergencespeedanda
lowersamplecomplexity.However,itisknownthatthe2TSAsettingismuchmoreinvolvedthanthe
SAsetting,astherearetwoparameterstobeupdatedinacoupledandasynchronousmanner.Toour
bestknowledge,therearenoexistingworksinthe2TSAsettingsthatachievethesameconvergence
rateorsamplecomplexityasthoseintheSAsettings.Itmaybeaninterestingdirectiontoinvestigate
forthecommunity. Finally,similartoFLsettings(Collinsetal.,2021),thelocalstepdoesnothurt
theglobalconvergencewithaproperlearningratechoice.
Remark4.17. ThegradienttrackingtechniquediscussedinZeng&Doan(2024)couldpotentially
beeffectiveinhandlingMarkoviansamplingandimprovingthecurrentconvergenceratetoO(T‚àí1).
However, it is unsure if it can be applied to PFEDRL-REP, i.e., personalized federated reinforce-
mentlearningsettingwithsharedrepresentations,wheremultipleagentsarecoupledthroughacen-
tral server and each agent performs multiple local updates for their personalized weights. In our
PFEDRL-REP framework, multiple agents are coupled through a central server, and each agent
performsmultiplelocalupdatesfortheirpersonalizedweights,whereasZeng&Doan(2024)only
considers single-agent settings under i.i.d. noise. Furthermore, Zeng & Doan (2024) assumes a
second-order variance bound of the stochastic function, while our analysis does not include such
anassumption. Investigatingtheseisoutofthescopeofthiswork,whichalreadyconsidersavery
challengingsetting.
810 10 10
5 5 5
TD
PFedTD-Rep
FedTD
0 0 0
0 100 200 0 100 200 0 100 200
Episode
(a)Valuefunctionestimates. (b)Convergencespeedofvaluefunctionlearning.
Figure2: ComparisonsinaCliffWalkingEnvironmentwith3agents.
4.2.2 INTUITIONSANDPROOFSKETCH
We highlight the key ideas and challenges behind the convergence rate analysis of PFEDTD-REP
withtwocoupledparameters,whichisanexampleofafederatednonlinear2TSA.Withthedefined
Lyapunovfunctionin(14), thekeyistofindthedriftbetweenM({Œ∏Œ∏Œ∏i ,‚àÄi},Œ¶Œ¶Œ¶ )inthet-thcom-
t+1 t
municationroundandM({Œ∏Œ∏Œ∏i,‚àÄi},Œ¶Œ¶Œ¶ )inthe(t‚àí1)-thcommunicationround. Toachievethis,
t t‚àí1
we separately characterize the drift betweenŒ¶Œ¶Œ¶ andŒ¶Œ¶Œ¶ , and the drift betweenŒ∏Œ∏Œ∏i andŒ∏Œ∏Œ∏i,‚àÄi.
t+1 t t+1 t
Weemphasizethethreemainchallengesincharacterizingthedrift: (i)howtoboundthestochastic
gradientwithMarkoviansamples;(ii)howtoleveragemixingtimeœÑ tohandlethebiasedparame-
terupdatesduetoMarkoviannoise; and(iii)howtodealwithmultiplelocalupdatesforthelocal
weightvectorŒ∏Œ∏Œ∏i.
We introduce the mixing time property of MDPs and thus we have that the gap between the bi-
ased gradient at each time step and the true gradient can be bounded when the time step exceeds
the mixing time œÑ, as defined in Definition 4.5. To characterize the effect of local updates, the
key philosophy is to bound the gradient at the initial local step and the gradient at the final local
steps, whichcanbedonebyleveragingtheLipschitzpropertyofthosegradientfunctionsinLem-
mas 4.6, 4.7 and 4.8. See Appendix F.1.1 and Appendix F.1.2 for details. Once we establish the
driftoftheLyapunovfunction,theremainingtaskistoselectsuitabledynamictwo-timescalelearn-
ing rates {Œ± ,‚àÄt} and {Œ≤ ,‚àÄt} for the weight vector update in (7) and the feature representation
t t
updatein(9),respectively. SeeAppendixF.1.3fordetails. Insummary,the2TSAunder PFEDRL
setting with multiple agents, Markovian samples, and multiple local updates, highly differentiates
ourworkfromexistingworks,e.g.,Doan(2020;2021);Srikant&Ying(2019)(seediscussionsin
Remark4.14).
4.3 NUMERICALEVALUATION
We empirically evaluate the performance of PFEDTD-REP. We consider a tabular CliffWalking
environment(Brockmanetal.,2016)witha4√ó12gridworld,where3agentsevaluate3different
policies. Thedimensionforthefeaturerepresentationandweightvectorissettobe6. Wecompare
PFEDTD-REPwith(i)‚ÄúTD‚Äù:eachagentindependentlyleveragestheconventionalTDwithoutcom-
munication;and(ii)‚ÄúFedTD‚Äùwithoutpersonalization(Khodadadianetal.,2022;DalFabbroetal.,
2023) as listed in Table 1. As shown in Figure 2a, PFEDTD-REP ensures personalization among
allagentswhileFedTDtendstoconvergeuniformlyamongallagents. Further, PFEDTD-REP at-
tains values much closer to the ground-truth achieved by TD for each agent compared to FedTD;
andPFEDTD-REPconvergesmuchfasterthanTD.Forinstance,agent1onlyneeds50episodesto
convergeunderPFEDTD-REP,whileittakesmorethan150episodestoconvergeunderTD,asillus-
tratedinFigure2b. TheimprovedconvergenceperformanceofPFEDTD-REPfurthersupportsour
theoreticalfindingsthatleveragingsharedrepresentationsnotonlyprovidespersonalizationamong
agentsinheterogeneousenvironmentsbutyieldfasterconvergence.
5 APPLICATION TO CONTROL PROBLEMS
Inthissection,webrieflydiscusshowourproposedPFEDRL-REPframeworkcanbeappliedtothe
controlproblemsinRL.MoredetailsareprovidedinAppendicesC.3(i.e.,Algorithm4)andG.
9
eulaV
detcepxE
evitageNPFedDQN-Rep DQN FedDQN FedQ-K PFedDQN-Rep DQN FedDQN FedQ-K
LFRL PerDQNAvg FedAsynQ-ImAvg LFRL PerDQNAvg FedAsynQ-ImAvg
2
102
2
102
0
103
0
103
1.5 1.5
nruteR
1
nruteR
1
nruteR-0.5 nruteR-0.5
0.5 0.5
0 0 -1 -1
0 100 200 300 400 500 0 100 200 300 400 500 0 50 100 0 50 100
Episode Episode Episode Episode
(a) Return pole length 0.54 (b) Generalization pole length 0.82 (a) Return pole length 0.3 (b) Generalization pole length 0.36
(a)Cartpoleenvironment. (b)Acrobotenvironment.
Figure3: Comparisonsincontrolproblems.
PFEDDQN-REP(aninstanceofPFEDRL-REPpairedwithDQN)leveragessharedrepresenta-
tionstolearnacommonfeaturespacethatcapturestheunderlyingdynamicsandfeaturesrelevant
acrossdifferentbutrelatedtasksencounteredbyvariousagents. InPFEDDQN-REP,thetargetnet-
workisacriticalcomponentthatprovidesstabilitytothelearningprocessbyservingasarelatively
static benchmark for calculating the loss during training updates (Mnih et al., 2015). The target
network‚Äôsarchitecturemirrorsthatofthemainnetwork,includingthesharedrepresentationmodel.
However,itsparametersareupdatedlessfrequently.Thissetupensuresthatthecalculatedtargetval-
ues, whichguidethepolicyupdates, arebasedonaconsistentrepresentationoftheenvironment‚Äôs
state,asencodedbythesharedrepresentationmodel. Thesynergybetweenthetargetnetworkand
the representation model is thus central to achieving stable and convergent learning. In Line 13
of Algorithm 4, the algorithm performs a scheduled update of the shared representationŒ¶Œ¶Œ¶ of the
mainnetwork‚Äôsparameterswiththeguidanceofthetargetnetwork. InLine18ofAlgorithm4,for
everyT times, thealgorithmperformsascheduledupdateofthetargetnetwork‚Äôsparameters
target
by copying over the parameters from the main network. This step is essential for maintaining the
stabilityofthelearningprocess,asitensuresthatthetargetvaluesagainstwhichthepolicyupdates
arecomputedremainconsistentandreflectiveofthemostrecentknowledgeencodedintheshared
representation. The update frequency is carefully chosen to balance learning stability with model
adaptiveness.
NumericalEvaluation. WeconsideramodifiedCartPoleenvironment(Brockmanetal.,2016)by
changingthelengthofpoletocreatedifferentenvironments(Jinetal.,2022). Specifically,wecon-
sider 10 agents with varying pole length from 0.38 to 0.74 with a step size of 0.04. We compare
PFEDDQN-REP with (i) a conventional DQN that each agent learns its own environment inde-
pendently;(ii)afederatedversionDQN(FedDQN)thatallowsallagentstocollaborativelylearna
singlepolicy(withoutpersonalization);(iii)twofederatedalgorithmswithoutpersonalizingFedQ-K
(Khodadadianetal.,2022),LFRL(Liuetal.,2019); and(iv)twopersonalizedalgorithmsPerDQ-
NAvg(Jinetal.,2022)andFedAsynQ-ImAvg(Wooetal.,2023). Werandomlychooseoneagent
andpresentitsperformanceinFigure3a.Again,weobservethatourPFEDDQN-REPobtainslarger
rewardthanbenchmarkswithoutpersonalization,thankstoourpersonalizedpolicy;andachievesthe
maximizedreturnmuchfasterthanexistingpersonlalizedalgorithmsduetoleveragingonlypartial
information among agents. We further evaluate the effectiveness of shared representation learned
by PFEDDQN-REP whengeneralizesittoanewagent. AsshowninFigure3a, PFEDDQN-REP
generalizesquicklytothenewenvironment.SimilarobservationscanbemadefromFigure3busing
Acrobotenvironments(seedetailsinAppendixG).Insummary,thesignificanceofour PFEDRL-
REPframeworkliesinitssuperiorperformanceinheterogeneousenvironmentscomparedtoexisting
algorithmsthatdonotincorporatepersonalization. Additionally,ourPFEDRL-REPframeworkalso
enablesquickadaptationtonew,previouslyunobservedenvironments.
LimitationsandOpenProblems.Inthispaper,wecharacterizethefinite-timeconvergenceratefor
PFEDTD-REP with linear feature representation. However, our analysis is not directly applicable
to the control problems (e.g., PFEDQ-REP, an instance of PFEDRL-REP with Q-learning) since
Q-learning is not a linear operation with respect to the shared representation and local weights.
Additionally,Q-learningistypicallycombinedwithdeepneuralnetworks,wheretheQ-functionis
approximatedbyaneuralnetworkasin PFEDDQN-REP. Itisimportanttonotethatevenforthe
single-agentQ-learningwithneuralnetworkfunctionapproximation,thefinite-timeconvergenceof
neuralweightsisnotwell-studied. Thecomplexityisfurthercompoundedinpersonalizedfederated
RLframeworks,wheremultipleagentsshareacommonrepresentationwhilemaintainpersonalized
localweights.Giventhepromisingexperimentalresultsoncontrol,whetherwecanprovideabound
forPFEDQ-REPeitherwithlinearorneuralfeaturerepresentationsremainsanopenproblem.
10REFERENCES
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural com-
plexityandrepresentationlearningoflowrankmdps. Advancesinneuralinformationprocessing
systems,33:20095‚Äì20107,2020.
Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Prov-
able benefits of representational transfer in reinforcement learning. In The Thirty Sixth Annual
ConferenceonLearningTheory,pp.2114‚Äì2187.PMLR,2023.
ManojGhuhanArivazhagan,VinayAggarwal,AadityaKumarSingh,andSunavChoudhary. Fed-
eratedlearningwithpersonalizationlayers. arXivpreprintarXiv:1912.00818,2019.
YoshuaBengio,AaronCourville,andPascalVincent. Representationlearning: Areviewandnew
perspectives. IEEEtransactionsonpatternanalysisandmachineintelligence,35(8):1798‚Äì1828,
2013.
ElHoucineBergou,KonstantinBurlachenko,AritraDutta,andPeterRichta¬¥rik. Personalizedfeder-
atedlearningwithcommunicationcompression. arXivpreprintarXiv:2209.05148,2022.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learningwithlinearfunctionapproximation. InConferenceonlearningtheory,pp.1691‚Äì1692.
PMLR,2018.
VivekSBorkar. Stochasticapproximation: adynamicalsystemsviewpoint, volume48. Springer,
2009.
KarimBoubouh,RobertBasmadjian,OmidArdakanian,AlexandreMaurer,andRachidGuerraoui.
Efficacy of temporal and spatial abstraction for training accurate machine learning models: A
casestudyinsmartthermostats. EnergyandBuildings,296:113377,2023.
GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and
WojciechZaremba. Openaigym. arXivpreprintarXiv:1606.01540,2016.
Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated meta-learning with
fastconvergenceandefficientcommunication. arXivpreprintarXiv:1802.07876,2018.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Perfor-
manceofq-learningwithlinearfunctionapproximation: Stabilityandfinite-timeanalysis. arXiv
preprintarXiv:1905.11425,pp. 4,2019.
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared repre-
sentationsforpersonalizedfederatedlearning. InInternationalconferenceonmachinelearning,
pp.2089‚Äì2099.PMLR,2021.
Nicolo` DalFabbro,AritraMitra,andGeorgeJPappas. Federatedtdlearningoverfinite-rateerasure
channels: Linearspeedupundermarkoviansampling. IEEEControlSystemsLetters,2023.
Thinh T Doan. Nonlinear two-time-scale stochastic approximation: Convergence and finite-time
performance. arXivpreprintarXiv:2011.01868,2020.
ThinhTDoan. Finite-timeconvergenceratesofnonlineartwo-time-scalestochasticapproximation
undermarkoviannoise. arXivpreprintarXiv:2104.01627,2021.
AlirezaFallah, AryanMokhtari, andAsumanOzdaglar. Personalizedfederatedlearning: Ameta-
learningapproach. arXivpreprintarXiv:2002.07948,2020.
Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low.
Fault-tolerant federated reinforcement learning with theoretical guarantee. Advances in Neural
InformationProcessingSystems,34:1007‚Äì1021,2021.
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franc¬∏oise Beaufays, Sean
Augenstein,HubertEichner,Chloe¬¥ Kiddon,andDanielRamage. Federatedlearningformobile
keyboardprediction. arXivpreprintarXiv:1811.03604,2018.
11AhmedImteaj,KhandakerMamunAhmed,UrmishThakker,ShiqiangWang,JianLi,andMHadi
Amini. Federatedlearningforresource-constrainedIoTdevices: Panoramasandstateoftheart.
FederatedandTransferLearning,pp.7‚Äì27,2022.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learningwithlinearfunctionapproximation. InConferenceonLearningTheory,pp.2137‚Äì2143.
PMLR,2020.
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement
learning with environment heterogeneity. In International Conference on Artificial Intelligence
andStatistics,pp.18‚Äì37.PMLR,2022.
SajadKhodadadian,PranaySharma,GauriJoshi,andSivaThejaMaguluri.Federatedreinforcement
learning: Linear speedup under markovian sampling. In International Conference on Machine
Learning,pp.10997‚Äì11057.PMLR,2022.
Walid Krichene, Nicolas Mayoraz, Steffen Rendle, Shuang Song, Abhradeep Thakurta, and
LiZhang. Privatelearningwithpublicfeatures. arXivpreprintarXiv:2310.15454,2023.
GuangchenLan,Dong-JunHan,AbolfazlHashemi,VaneetAggarwal,andChristopherGBrinton.
Asynchronousfederatedreinforcementlearningwithpolicygradientupdates: Algorithmdesign
andconvergenceanalysis. arXivpreprintarXiv:2404.08003,2024.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436‚Äì444,
2015.
DavidALevinandYuvalPeres. Markovchainsandmixingtimes,volume107. AmericanMathe-
maticalSoc.,2017.
Boyi Liu, Lujia Wang, and Ming Liu. Lifelong federated reinforcement learning: a learning ar-
chitecturefornavigationincloudroboticsystems. IEEERoboticsandAutomationLetters,4(4):
4555‚Äì4562,2019.
KiwanMaeng,HaiyuLu,LucaMelis,JohnNguyen,MikeRabbat,andCarole-JeanWu. Towards
fairfederatedrecommendationlearning: Characterizingtheinter-dependenceofsystemanddata
heterogeneity. InProceedingsofthe16thACMConferenceonRecommenderSystems,pp.156‚Äì
167,2022.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InArtificialintelli-
genceandstatistics,pp.1273‚Äì1282.PMLR,2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare,AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,etal.Human-level
controlthroughdeepreinforcementlearning. nature,518(7540):529‚Äì533,2015.
VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. InInternationalconferenceonmachinelearning,pp.1928‚Äì1937.PMLR,2016.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free
representation learning and exploration in low-rank mdps. arXiv preprint arXiv:2102.07035,
2021.
Chetan Nadiger, Anil Kumar, and Sherine Abdelhak. Federated reinforcement learning for fast
personalization. In 2019 IEEE Second International Conference on Artificial Intelligence and
KnowledgeEngineering(AIKE),pp.123‚Äì127.IEEE,2019.
DinhCNguyen,MingDing,PubuduNPathirana,ArunaSeneviratne,JunLi,andHVincentPoor.
FederatedlearningforInternetofThings: Acomprehensivesurvey. IEEECommunicationsSur-
veys&Tutorials,23(3):1622‚Äì1658,2021.
Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: Techniques,
applications,andopenchallenges. arXivpreprintarXiv:2108.11887,2021.
12Sudeep Salgia and Yuejie Chi. The sample-communication complexity trade-off in federated q-
learning. arXivpreprintarXiv:2408.16981,2024.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
HanShen, KaiqingZhang, MingyiHong, andTianyiChen. Towardsunderstandingasynchronous
advantageactor-critic:Convergenceandlinearspeedup.IEEETransactionsonSignalProcessing,
2023.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387‚Äì395.Pmlr,2014.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. Advancesinneuralinformationprocessingsystems,30,2017.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtdlearning. InConferenceonLearningTheory,pp.2803‚Äì2830.PMLR,2019.
RichardSSutton. Learningtopredictbythemethodsoftemporaldifferences. Machinelearning,
3:9‚Äì44,1988.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function
approximation. Advancesinneuralinformationprocessingsystems,9,1996.
IsidorosTziotis,ZebangShen,RamtinPedarsani,HamedHassani,andAryanMokhtari. Straggler-
resilientpersonalizedfederatedlearning. TransactionsonMachineLearningResearch,2023.
HanWang, AritraMitra, HamedHassani, GeorgeJPappas, andJamesAnderson. Federatedtem-
poraldifferencelearningwithlinearfunctionapproximationunderenvironmentalheterogeneity.
arXivpreprintarXiv:2302.02212,2023a.
Sid Wang, Ashish Shenoy, Pierce Chuang, and John Nguyen. Now it sounds like you: Learning
personalizedvocabularyondevice. arXivpreprintarXiv:2305.03584,2023b.
ChristopherJCHWatkinsandPeterDayan. Q-learning. Machinelearning,8:279‚Äì292,1992.
JiinWoo,GauriJoshi,andYuejieChi. Theblessingofheterogeneityinfederatedq-learning:Linear
speedupandbeyond. InInternationalConferenceonMachineLearning.PMLR,2023.
Jiin Woo, Laixi Shi, Gauri Joshi, and Yuejie Chi. Federated offline reinforcement learning: Col-
laborative single-policy coverage suffices. In Forty-first International Conference on Machine
Learning,2024.
Guojun Xiong, Gang Yan, Shiqiang Wang, and Jian Li. Deprl: Achieving linear convergence
speedup in personalized decentralized learning with shared representations. In Proceedings of
theAAAIConferenceonArtificialIntelligence,volume38,pp.16103‚Äì16111,2024.
MinruiXu,JialiangPeng,BBGupta,JiawenKang,ZehuiXiong,ZhenniLi,andAhmedAAbdEl-
Latif. Multiagentfederatedreinforcementlearningforsecureincentivemechanisminintelligent
cyber‚Äìphysicalsystems. IEEEInternetofThingsJournal,9(22):22095‚Äì22108,2021.
TimothyYang,GalenAndrew,HubertEichner,HaichengSun,WeiLi,NicholasKong,DanielRa-
mage, and Franc¬∏oise Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXivpreprintarXiv:1812.02903,2018.
ZhenyuanYuan,SiyuanXu,andMinghuiZhu. Federatedreinforcementlearningforgeneralizable
motionplanning. In2023AmericanControlConference(ACC),pp.78‚Äì83.IEEE,2023.
Sihan Zeng and Thinh Doan. Fast two-time-scale stochastic gradient method with applications in
reinforcementlearning. InTheThirtySeventhAnnualConferenceonLearningTheory,pp.5166‚Äì
5212.PMLR,2024.
13Chenyu Zhang, Han Wang, Aritra Mitra, and James Anderson. Finite-time analysis of on-policy
heterogeneousfederatedreinforcementlearning. arXivpreprintarXiv:2401.15273,2024.
KaiqingZhang,ZhuoranYang,andTamerBas¬∏ar. Multi-agentreinforcementlearning: Aselective
overviewoftheoriesandalgorithms. Handbookofreinforcementlearningandcontrol,pp.321‚Äì
384,2021.
Sai Qian Zhang, Jieyu Lin, and Qi Zhang. A multi-agent reinforcement learning approach for
efficientclientselectioninfederatedlearning.InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume36,pp.9091‚Äì9099,2022a.
TuoZhang,LeiGao,ChaoyangHe,MiZhang,BhaskarKrishnamachari,andASalmanAvestimehr.
FederatedlearningfortheInternetofThings: Applications,challenges,andopportunities. IEEE
InternetofThingsMagazine,5(1):24‚Äì29,2022b.
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun.
Efficientreinforcementlearninginblockmdps: Amodel-freerepresentationlearningapproach.
InInternationalConferenceonMachineLearning,pp.26517‚Äì26547.PMLR,2022c.
Zhong Zheng, Fengyu Gao, Lingzhou Xue, and Jing Yang. Federated q-learning: Linear regret
speedup with low communication cost. In The Twelfth International Conference on Learning
Representations,2024.
A RELATED WORK
Single-Agent Reinforcement Learning. RL is a machine learning paradigm that trains agents to
makesequencesofdecisionsbyrewardingdesiredbehaviorsand/orpenalizingundesiredonesina
givenenvironment(Sutton&Barto,2018). StartingfromTemporalDifference(TD)Learning(Sut-
ton, 1988), which introduced the concept of learning from the discrepancy between predicted and
actual rewards through episodes, the widely used Q-Learning (Watkins & Dayan, 1992) emerged,
advancingthefieldwithanoff-policyalgorithmthatlearnsaction-valuefunctionsandenablespol-
icyimprovementwithoutneedingamodeloftheenvironment. Lateron, theintroductionofDeep
Q-Networks(DQN)(Mnihetal.,2015)markedasignificantleap,integratingdeepneuralnetworks
withQ-Learningtohandlehigh-dimensionalstatespaces,thusenablingRLtotacklecomplexprob-
lems. Subsequently, policy-basedalgorithmssuchasProximalPolicyOptimization(PPO)(Schul-
man et al., 2017) and deep Deterministic Policy Gradients (DDPG) (Silver et al., 2014), leverage
theActor-Criticframeworktoprovidemorestableandrobustwaystodirectlyoptimizethepolicy,
overcomingchallengesrelatedtoactionspaceandvariance.
Federated Reinforcement Learning. Jin et al. (2022) introduced a FedRL framework with N
agentscollaborativelylearningapolicybyaveragingtheirQ-valuesorpolicygradients. Khodada-
dianetal.(2022)providedaconvergenceanalysisoffederatedTD(FedTD)andQ-learning(FedQ)
when N agents interact with homogeneous environments. A similar FedTD was considered in
DalFabbroetal.(2023),andexpandedtoheterogeneousenvironmentsinWangetal.(2023a). Woo
et al. (2023) analyzed (a)synchronous variants of FedQ in heterogeneous settings, and an asyn-
chronous actor-critic method was considered in Shen et al. (2023) with linear speedup guarantee
only under i.i.d. samples. Zhang et al. (2024) provided a finite-time analysis of FedSARSA with
linearfunctionapproximation(i.e.,fixedfeaturerepresentation).Tofacilitatepersonalizationinhet-
erogeneoussettings,Jinetal.(2022)proposedaheuristicpersonalizedFedRLmethodwhereagents
shareacommonmodel,butmakeuseofindividualenvironmentembeddings. Noticethatthereisan
earlyworkFanetal.(2021)whichconsideredaspecialsettingwhereeachagentcanbeByzantine
andsuffersrandomfailureineveryroundandconvergencewasestablishedbasedoni.i.dnoise.
Personalized Federated Learning (PFL). In contrast to standard FL where a single model is
learned,PFLaimstolearnN modelsspecializedforN localdatasets.ManyPFLmethodshavebeen
developed,includingbutnotlimitedtomulti-tasklearning(Smithetal.,2017),meta-learning(Chen
etal.,2018), andvariouspersonalizationtechniquessuchaslocalfine-tuning(Fallahetal.,2020),
layerpersonalization(Arivazhaganetal.,2019),andmodelcompression(Bergouetal.,2022). An-
other line of work (Collins et al., 2021; Xiong et al., 2024) leveraged the common representation
14Algorithm2PFEDTD-REP
1: Input: SamplingpolicyœÄi,‚àÄi‚àà[N];
2: Initialize Œ∏Œ∏Œ∏i = 000, Si,‚àÄi ‚àà [N], and randomly generate Œ¶Œ¶Œ¶ ‚àà R|S|√ód with each row being
0 0
unit-normvector;
3: fort=0,1,...,T ‚àí1do
4: fori=1,...,N do
5: fork =1,...,K do
6: SampleobservationsXi ;
t,k‚àí1
7: SetŒ∏Œ∏Œ∏i
t,k
=Œ∏Œ∏Œ∏i t,k‚àí1+Œ± tg(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t,X ti ,k‚àí1);
8: endfor
9: Scale‚à•Œ∏Œ∏Œ∏i ‚à•toBif‚à•Œ∏Œ∏Œ∏i ‚à•>B,otherwisekeepitunchanged;
t+1 t+1
10: SetŒ¶Œ¶Œ¶i
t+1/2
=Œ¶Œ¶Œ¶ t+Œ≤ th(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t,{X ti ,k‚àí1}K k=1);
11: NormalizeŒ¶Œ¶Œ¶i asŒ¶Œ¶Œ¶i ‚Üê
Œ¶Œ¶Œ¶i
t+1/2 ;
t+1/2 t+1/2 ‚à•Œ¶Œ¶Œ¶i ‚à•
t+1/2
12: endfor
13: Œ¶Œ¶Œ¶ t+1 = N1 (cid:80)N i=1Œ¶Œ¶Œ¶i t+1/2.
14: endfor
amongagentsinheterogeneousenvironmentstoguaranteepersonalizedmodelsforfederatedsuper-
visedlearning.
Representation Learning in MDP. Representation learning aims to transform high-dimensional
observation to low-dimensional embedding to enable efficient learning, and has received increas-
ing attention in Markov decision processs (MDP) settings, such as linear MDPs (Jin et al., 2020),
low-rank MDPs (Modi et al., 2021; Agarwal et al., 2020) and block MDPs (Zhang et al., 2022c).
However, it is open in the context of leveraging representation learning in PFedFL. In this work,
we prove that representation augmented PFedFL forms a general framework as a federated two-
timescalestochasticapproximationwithMarkoviannoise,whichdifferssignificantlyfromexisting
works,andhencenecessitatesdifferentprooftechniques.
Multi-Agent Reinforcement Learning vs. Federated Reinforcement Learning. The advent
of Multi-Agent Reinforcement Learning (MARL) expanded RL‚Äôs applications, allowing multiple
agentstolearnfrominteractionsincooperative,competitive,ormixedsettings,openingnewavenues
for complex applications and research (Zhang et al., 2021). Multi-agent Reinforcement Learning
(MARL)addressesscenarioswheremultipleagentsoperatewithinasharedorinterrelatedenviron-
ment, potentially engaging in both cooperative and competitive behaviors. The complexity arises
fromeachagentneedingtoconsiderthestrategiesandactionsofothers, makingthelearningpro-
cess highly dynamic. Federated Reinforcement Learning (FedRL)(Qi et al., 2021), contrasts with
MARLbyfocusingonprivacy-preserving,distributedlearningacrossagentsthatdonotsharetheir
rawdata. Instead,theseagentsmightcontributetowardsacentralizedlearningmodelwithoutcom-
promisingindividualdataprivacy,addressingtheuniquechallengesoflearningfromdecentralized
datasources.
B PSEUDOCODE OF PFEDTD-REP
Inthissection,wepresentthepseudocodeofPFEDQ-REPassummarizedinAlgorithm2.
C APPLICATION TO CONTROL TASKS IN RL
The Q-function of agent i in environment Mi under policy œÄi is defined as Qi,œÄi(s,a) =
E (cid:2)(cid:80)‚àû Œ≥kRi(si,ai)|si =s,ai =a(cid:3) . When the state and action spaces are large, it is com-
œÄi k=0 k k 0 0
putationally infeasible to store Qi,œÄi(s,a) for all state-action pairs. One way to deal with is to
approximate the Q-function as Qi,œÄi(s,a) ‚âà Œ¶Œ¶Œ¶(s,a)Œ∏Œ∏Œ∏, whereŒ¶Œ¶Œ¶ ‚àà R|S|√ó|A|√ód is a feature repre-
sentationcorrespondingtostate-actions,andŒ∏Œ∏Œ∏ ‚àà Rd isalow-dimensionalunknownweightvector.
15WhenŒ¶Œ¶Œ¶isgivenandknown,thisfallsundertheparadigmofRLorFedRLwithfunctionapproxi-
mation.
C.1 PRELIMINARIES: CONTROLINFEDERATEDREINFORCEMENTLEARNING
Another task in RL is to search for an optimal policy, which is called a control problem, and one
commonly used approach is Q-learning (Watkins & Dayan, 1992). Under the FedRL framework,
the goal of a control problem is to let N agents collaboratively learn a policy œÄ‚àó that performs
uniformlywellacrossN differentenvironments,i.e.,œÄ‚àó =argmax 1 (cid:80)N E (cid:2) Vi,œÄi(si)|si ‚àº
œÄ N i=1 œÄi 0 0
(cid:3)
d , where d is the common initial state distribution in these N environments. Similar to (1),
0 0
thiscanbeformulatedastheoptimizationproblemin(17)tocollaborativelylearnacommon(non-
personalized) weight vector Œ∏Œ∏Œ∏ ‚â° Œ∏Œ∏Œ∏i,‚àÄi ‚àà [N] when the feature representation Œ¶Œ¶Œ¶(s,a),‚àÄs,a are
given.
L(Œ∏Œ∏Œ∏):=m Œ∏Œ∏Œ∏in
N1 (cid:88)N
E s‚àº¬µi,œÄ‚àó
(cid:13)
(cid:13) (cid:13)Œ¶Œ¶Œ¶(s,a)Œ∏Œ∏Œ∏‚àíQi,œÄ‚àó
(s,a)(cid:13)
(cid:13)
(cid:13)2
. (17)
i=1 a‚àºœÄ‚àó(¬∑|s)
Again,weusethesuperscriptitohighlightheterogeneousenvironmentsPiamongagents.
C.2 CONTROLINPERSONALIZEDFEDRLWITHSHAREDREPRESENTATIONS
The control problem in (17) aims to learnŒ¶Œ¶Œ¶ and {Œ∏Œ∏Œ∏i,‚àÄi} simultaneously among all N agents via
solvingthefollowingoptimizationproblem:
L(Œ¶Œ¶Œ¶,{Œ∏Œ∏Œ∏i,‚àÄi}):=min
1 (cid:88)N
min E
(cid:13)
(cid:13)fi(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶(s,a))‚àíQi,œÄi,‚àó
(s,a)(cid:13) (cid:13)2
. (18)
Œ¶Œ¶Œ¶ N {Œ∏Œ∏Œ∏i,‚àÄi} s‚àº¬µi,œÄi,‚àó (cid:13) (cid:13)
i=1 a‚àºœÄi,‚àó(¬∑|s)
C.3 ALGORITHMS
Inthissubsection,wepresenttworealizationsofourproposedPFEDRL-REPinAlgorithm1,oneis
PFEDQ-REPassummarizedinAlgorithm3,federatedQ-learningwithsharedrepresentations,and
theotherisPFEDDQN-REPasoutlinedinAlgorithm4,federatedDQNwithsharedrepresentations.
D FIGURE ILLUSTRATIONS
We present some figures to further highlight the proposed personalized FedRL (PFEDRL) frame-
workwithsharedrepresentations.
SchematicframeworkofconventionalFedRL.WebeginbyintroducingtheconventionalFedRL
framework(Khodadadianetal.,2022),whereN agentscollaborativelylearnacommonpolicy(or
optimalvaluefunctions)viaaserverwhileengagingwithhomogeneousenvironments. Eachagent
generatesindependentMarkoviantrajectories,asdepictedinFigure4.
Schematic framework for our proposed PFEDRL with shared representations. We introduce
our proposed personalized FedRL (PFEDRL) framework with shared representations in Figure 5.
In PFEDRL, N agents independently interact with their own environments and execute actions
accordingtotheirindividualRLcomponentparameterizedbyŒ¶Œ¶Œ¶andŒ∏Œ∏Œ∏i. Eachagentiperformslocal
updateonitslocalweightvectorŒ∏Œ∏Œ∏ ,whilejointlyupdatingtheglobalsharedfeaturerepresentation
i
Œ¶Œ¶Œ¶throughtheserver. Similarly,theupdatefollowstheMarkoviantrajectories.
Motivation of Personalized FedRL. In the following, we also want to provide some examples
showing that the conventional FedRL framework may fail, as depicted in Figure 6. In Figure 6a,
we provide an example where three agents assess distinct policies within the same environment.
In the traditional FedRL framework, agents exchange the evaluated value functions via a central
server,leadingtoaunifiedconsensusonvaluefunctionsforthreedifferentpolicies. Thisenforced
consensus on value functions, despite the diversity in policies, is not optimal. In another scenario
depicted in Figure 6b, three agents each interact with their unique environments. The objective
for each agent is to learn an optimal policy tailored to its specific environment. However, within
16Algorithm3PFEDQ-REP
Input: SamplingpolicyœÄi,‚àÄi‚àà[N].
1: InitializeŒ∏Œ∏Œ∏i =000,andsi,‚àÄi‚àà[N],andrandomlygenerateŒ¶Œ¶Œ¶‚ààR|S||A|√ód witheachrowbeing
0 0
unit-normvector.
2: fort=0,1,...,T ‚àí1do
3: fori=1,...,N do
4: fork =1,...,K do
5: SampleobservationsXi =(si ,si ,ai );
t,k‚àí1 t,k t,k‚àí1 t,k‚àí1
6: With fixed Œ¶Œ¶Œ¶ t, update Œ∏Œ∏Œ∏i t,k ‚Üê Œ∏Œ∏Œ∏i t,k‚àí1 +Œ± t ¬∑(r ti ,k‚àí1 +Œ≥max aŒ¶Œ¶Œ¶ t(si t,k+1,a)Œ∏Œ∏Œ∏i t,k‚àí1 ‚àí
Œ¶Œ¶Œ¶ (si )Œ∏Œ∏Œ∏i )¬∑Œ¶Œ¶Œ¶ (si ,ai );
t t,k‚àí1 t,k‚àí1 t t,k‚àí1 t,k‚àí1
7: endfor
8: Scale‚à•Œ∏Œ∏Œ∏i ‚à•toBif‚à•Œ∏Œ∏Œ∏i ‚à•>B,otherwisekeepitunchanged.
t+1 t+1
9: if(s,a)‚ààXi ,‚àÉk ‚àà{0,...,K‚àí1}then
t,k
10: UpdateŒ¶Œ¶Œ¶i t+1/2(s,a)=Œ¶Œ¶Œ¶i t(s,a)+Œ≤ t(r(s,a)+Œ≥max aŒ¶Œ¶Œ¶ t(s‚Ä≤,a)Œ∏Œ∏Œ∏i t+1‚àíŒ¶Œ¶Œ¶ t(s,a)‚ä∫Œ∏Œ∏Œ∏i t+1)¬∑
Œ∏Œ∏Œ∏i ;
t+1
11: else
12: Œ¶Œ¶Œ¶i (s,a)=Œ¶Œ¶Œ¶i(s,a);
t+1/2 t
13: endif
14: NormalizeŒ¶Œ¶Œ¶i asŒ¶Œ¶Œ¶i ‚Üê
Œ¶Œ¶Œ¶i
t+1/2 ;
t+1/2 t+1/2 ‚à•Œ¶Œ¶Œ¶i ‚à•
t+1/2
15: endfor
16: Œ¶Œ¶Œ¶ t+1 ‚Üê N1 (cid:80)N i=1Œ¶Œ¶Œ¶i t+1/2,‚àÄi‚àà[N].
17: endfor
Figure4: SchematicrepresentationofFedRL,whereN agentsinteractwithhomogeneousenviron-
ments.
the traditional FedRL framework, the central server mandates a uniform policy across all three
agents,whichclearlycontradictstheintendedgoalofachievingenvironment-specificoptimization.
This highlights the necessity for personalized decision-making, a feature that conventional FedRL
frameworksdonotaccommodate.
ExampleofRLcomponentsthatfittheproposedPFEDRLwithsharedrepresentations. Inthe
following,weaimtoshowcaseexamplesofRLcomponentsthatarecompatiblewithourproposed
PFEDRLframeworkfeaturingsharedrepresentations. Anillustrativeexampleofthisframeworkis
presented in Figure 7. It is important to note that both the DQN architecture in Figure 7a and the
policygradient(PG)approachinFigure7bseamlesslyintegrateintoourproposedframework. This
integrationisachievedbydesignatingtheparametersofthefeatureextractionnetworkastheshared
feature representationŒ¶Œ¶Œ¶, and the parameters of the fully connected network, which either predict
theQ-valuesordeterminethepolicy,asthelocalweightvectorŒ∏Œ∏Œ∏. Thisarrangementunderscoresthe
adaptabilityofourframeworktovariousRLmethodologies,facilitatingpersonalizedlearningwhile
maintainingacommonfoundationofsharedrepresentations.
17Algorithm4PFEDDQN-REP
Initialize: The parameters (Œ¶Œ¶Œ¶,Œ∏Œ∏Œ∏i) for each Q network Qi(s,a), the replay buffer Ri, and copy
the same parameter from Q network to initialize the target Q network Qi,‚Ä≤(s,a) for agent i,‚àÄi ‚àà
[N];
1: forepisodee=1,...,E do
2: Gettheinitialstateoftheenvironment;
3: fort=0,1,...,T ‚àí1do
4: fori=1,...,N do
5: fork =1,...,K do
6: Select action a t,k‚àí1 according to œµ-greedy policy with the current network
Qi(s ,a);
t,k‚àí1
7: Executeactiona t,k‚àí1,receivetherewardr(s t,k‚àí1,a t,k‚àí1),andtheenvironmentstate
transitstos ;
t,k
8: Storethetuple(s t,k‚àí1,a t,k‚àí1,r(s t,k‚àí1,a t,k‚àí1,s t,k)intothereplaybufferRi;
9: SampleN datatuplesfromthereplaybufferRi;
10: Update the local weight Œ∏Œ∏Œ∏i(t,k) by minimizing the loss compared with the target
networkQi,‚Ä≤withfixedrepresentationŒ¶Œ¶Œ¶ ;
t
11: endfor
12: SampleN datatuplesfromreplaybufferRi;
13: Update representation model locally by minimizing the loss compared with the target
networkQi,‚Ä≤withfixedweightsŒ∏Œ∏Œ∏ ,andyieldŒ¶Œ¶Œ¶i ;
t+1 t+1/2
14: endfor
15: Averagetherepresentationmodelfromallagents,i.e.,Œ¶Œ¶Œ¶ t+1 := N1 (cid:80)N i=1Œ¶Œ¶Œ¶i t+1/2;
16: endfor
17: ifmod(t,T target)=0then
18: updatethetargetnetworkQi,‚àó becopytheup-to-dateparametersofQnetworkQi, ‚àÄi ‚àà
[N];
19: endif
20: endfor
Figure 5: Our proposed PFEDRL-REP framework where N agents independently interact with
theirownenvironmentsandtakeactionsaccordingtotheirindividualRLcomponentparameterized
by Œ¶Œ¶Œ¶ and Œ∏Œ∏Œ∏i. Agent i locally update weight vector Œ∏Œ∏Œ∏ while jointly updating the shared feature
i
representationŒ¶Œ¶Œ¶throughthecentralserver. TheupdatefollowstheMarkoviantrajectories.
18(b) Agents learn optimal policies for heterogeneous
(a)Agentsevaluatedifferencepoliciesinthesame
environments.
environment.
Figure6:AnillustrativeexamplewiththreeagentsthatdemonstratestheconventionalFedRLframe-
workfailstowork.
(a)WhenDQNmeetstheproposedframework. (b)WhenPGmeetstheproposedframework.
Figure7: Anillustrativeexamplefortheproposedframework. NoticethatboththeDQNin(a)and
policygradient(PG)in(b)canbefittedintotheproposedframeworkbytreatingtheparametersof
thefeatureextractionnetworkasthesharedfeaturerepresentationŒ¶Œ¶Œ¶andtheparameterofthefully
connectednetworkwhichmapstotheQvalueofpolicyasthelocalweightvectorŒ∏Œ∏Œ∏.
E PROOF OF LEMMAS IN SECTION 4.2
E.1 PROOFOFLEMMA4.6
Proof. Recall that for any observation X = (s,a,s‚Ä≤), the function g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X) defined in (8) is
expressedas
g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X):=(r(s,a)+Œ≥Œ¶Œ¶Œ¶(s‚Ä≤)Œ∏Œ∏Œ∏‚àíŒ¶Œ¶Œ¶(s)Œ∏Œ∏Œ∏)¬∑Œ¶Œ¶Œ¶(s)‚ä∫
,
and hence we have the following inequality for any parameter pairs (Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ) and (Œ∏Œ∏Œ∏ ,ŒªŒªŒª ) with
1 1 2 2
X =(s,a,s‚Ä≤),
‚à•g(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚àíg(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚à•
1 1 2 2
=‚à•(r(s,a)+Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(r(s,a)+Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 1 1 1 1 2 2 2 2 2
(a ‚â§1)
‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 1 1 1 1 1 2 1 2 1
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 2 1 2 1 2 2 2 2 2
(a ‚â§2)
‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚à•¬∑‚à•Œ¶Œ¶Œ¶ (s)‚à•
1 1 1 1 1 2 1 2 1
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 2 1 2 1 2 2 2 2 2
(a ‚â§3)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 2 1 2 1 2 1 2 2 2 2 2
(a ‚â§4)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 2 1 2 1 2 1 1 2 1 2 2
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ¶Œ¶Œ¶ (s)‚ä∫ ‚à•
1 2 1 2 2 2 2 2 2 2
(a ‚â§5)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏
‚à•+(cid:13)
(cid:13)(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏
)(cid:13) (cid:13)¬∑(cid:13)
(cid:13)Œ¶Œ¶Œ¶ (s)‚àíŒ¶Œ¶Œ¶
(s)(cid:13)
(cid:13)
1 2 (cid:13) 1 2 1 2 (cid:13) (cid:13) 1 2 (cid:13)
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚à•¬∑‚à•Œ¶Œ¶Œ¶ (s)‚à•
1 2 1 2 2 2 2 2 2
(a ‚â§6) (1+Œ≥)(cid:13)
(cid:13)Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏
(cid:13) (cid:13)+(cid:13)
(cid:13)(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏
)(cid:13)
(cid:13)¬∑‚à•Œ¶Œ¶Œ¶ (s)‚àíŒ¶Œ¶Œ¶ (s)‚à•
(cid:13) 1 2(cid:13) (cid:13) 1 2 1 2 (cid:13) 1 2
19+‚à•Œ¶Œ¶Œ¶ (s‚Ä≤)‚àíŒ¶Œ¶Œ¶ (s‚Ä≤)‚à•¬∑‚à•Œ≥Œ∏Œ∏Œ∏ ‚à•+‚à•Œ¶Œ¶Œ¶ (s)‚àíŒ¶Œ¶Œ¶ (s)‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚à•
1 2 2 1 2 2
‚â§(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+(2+2Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•
1 2 2 1 2
(a7)
‚â§ L (‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•),
g 1 2 1 2
(a ) is due to the fact that ‚à•x + y‚à• ‚â§ ‚à•x‚à• + ‚à•y‚à•,‚àÄx,y ‚àà Rd, (a ) holds due to ‚à•x ¬∑ y‚à• ‚â§
1 2
‚à•x‚à•¬∑‚à•y‚à•,‚àÄx,y ‚àà Rd, (a )comesfromthefactand‚à•Œ¶Œ¶Œ¶ (s)‚à• ‚â§ 1,‚à•Œ¶Œ¶Œ¶ (s)‚à• ‚â§ 1‚àÄs.(a )‚àí(a )
3 1 2 4 6
holds for the same reason as (a )‚àí(a ). The last inequalty (a ) comes from the fact that Œ∏Œ∏Œ∏ is
1 3 7
boundedbynormBandbysettingL :=max(1+Œ≥,(2+2Œ≥)B).
g
E.2 PROOFOFLEMMA4.7
Proof. Recall that for any observation X = (s,a,s‚Ä≤), the function h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X) defined in (10) is
expressedas
h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X):=(r(s,a)+Œ≥Œ¶Œ¶Œ¶(s‚Ä≤)Œ∏Œ∏Œ∏‚àíŒ¶Œ¶Œ¶(s)Œ∏Œ∏Œ∏)¬∑Œ∏Œ∏Œ∏‚ä∫
,
and hence we have the following inequality for any parameter pairs (Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ) and (Œ∏Œ∏Œ∏ ,ŒªŒªŒª ) with
1 1 2 2
X =(s,a,s‚Ä≤),
‚à•h(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚àíh(Œ∏Œ∏Œ∏ ,Œ¶Œ¶Œ¶ ,X)‚à•
1 1 2 2
=‚à•(r(s,a)+Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(r(s,a)+Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
1 1 1 1 1 2 2 2 2 2
( ‚â§b1)
‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
1 1 1 1 1 2 1 2 1 1
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
2 1 2 1 1 2 2 2 2 2
( ‚â§b2)
‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚à•
1 1 1 1 2 1 2 1 1
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
2 1 2 1 1 2 2 2 2 2
( ‚â§b3)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•2¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
1 1 2 2 1 2 1 1 2 2 2 2 2
( ‚â§b4)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•2¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
1 1 2 2 1 2 1 1 2 1 2 1 2
+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )¬∑Œ∏Œ∏Œ∏‚ä∫ ‚à•
2 1 2 1 2 2 2 2 2 2
( ‚â§b5)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•2¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•+‚à•(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•
1 1 2 2 1 2 1 1 2
+‚à•(Œ≥œïœïœï (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚àí(Œ≥Œ¶Œ¶Œ¶ (s‚Ä≤)Œ∏Œ∏Œ∏ ‚àíŒ¶Œ¶Œ¶ (s)Œ∏Œ∏Œ∏ )‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚à•
2 1 2 1 2 2 2 2 2
( ‚â§b6)
(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•2¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•+(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•¬∑‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•
1 1 2 1 1 2 2 1 2
‚â§(1+Œ≥)‚à•Œ∏Œ∏Œ∏ ‚à•2¬∑‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•+(1+Œ≥)(‚à•Œ∏Œ∏Œ∏ ‚à•+‚à•Œ∏Œ∏Œ∏ ‚à•)¬∑‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•
1 1 2 1 2 1 2
(b7)
‚â§ L (‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•+‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•),
h 1 2 1 2
(b ) is due to the fact that ‚à•x + y‚à• ‚â§ ‚à•x‚à• + ‚à•y‚à•,‚àÄx,y ‚àà Rd, (b ) holds due to ‚à•x ¬∑ y‚à• ‚â§
1 2
‚à•x‚à•¬∑‚à•y‚à•,‚àÄx,y ‚àà Rd, (b ) comes from the fact and ‚à•Œ¶Œ¶Œ¶ (s)‚à• ‚â§ 1,‚à•Œ¶Œ¶Œ¶ (s)‚à• ‚â§ 1‚àÄs. (b )‚àí(b )
3 1 2 4 6
holds for the same reason as (b )‚àí(b ). The last inequalty (b ) comes from by setting L :=
1 3 7 h
max((1+Œ≥)B2,(2+2Œ≥)B).
E.3 PROOFOFLEMMA4.8
Proof. Duetothenorm-scalestep(step9)inAlgorithm2,wehave
‚à•yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ )‚à•‚â§ max ‚à•Œ∏Œ∏Œ∏‚àíŒ∏Œ∏Œ∏‚Ä≤‚à•‚â§2B. (19)
1 2
(‚à•Œ∏Œ∏Œ∏‚à•‚â§B,‚à•Œ∏Œ∏Œ∏‚Ä≤‚à•‚â§B)
Since the representation matricesŒ¶Œ¶Œ¶ andŒ¶Œ¶Œ¶ are of unit-norm in each row, there exists a positive
1 2
constantL suchthat
y
‚à•yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ )‚à•‚â§L ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•. (20)
1 2 y 1 2
20E.4 PROOFOFLEMMA4.10
Proof. In the TD learning setting for our PFEDTD-REP, at time step k, the state of agent i is si,
k
anditsvaluefunctioncanbedenotedasV(si) =Œ¶Œ¶Œ¶(si)Œ∏Œ∏Œ∏i inalinearrepresentation,whereŒ¶Œ¶Œ¶(si)
k k k
isafeaturevectorandŒ∏Œ∏Œ∏i isaweightvector. Thegoalofagentiistominimizethefollowingloss
functionforeverysi ‚ààS:
k
1(cid:12) (cid:12)2
Li(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)= (cid:12)V(si)‚àíVÀÜ(si)(cid:12) ,
k 2(cid:12) k k (cid:12)
with VÀÜ(si) = ri +Œ≥Œ¶(si )Œ∏i being a constant. Therefore, to updateŒ¶Œ¶Œ¶(s) andŒ∏Œ∏Œ∏, we just take
k k k+1
thenaturalgradientdescent. Specifically,weupdateŒ∏Œ∏Œ∏accordingto(7)bytakingagradientdescent
stepwithrespecttoŒ∏Œ∏Œ∏,withfixedŒ¶Œ¶Œ¶. Similarly,weupdateŒ¶Œ¶Œ¶(s)accordingto(9)bytakingagradient
descentstepwithrespecttoŒ¶Œ¶Œ¶(s),withfixedŒ∏Œ∏Œ∏.
Next,weshowtheconvexityofthelossfunctionLi(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)withrespecttothefeaturerepresenta-
k
tionŒ¶Œ¶Œ¶(si)underafixedŒ∏Œ∏Œ∏i.SincetheestimatedvaluefunctionisapproximatedasV(si)=Œ¶Œ¶Œ¶(si)Œ∏Œ∏Œ∏i,
k k k
whereŒ∏Œ∏Œ∏i isafixedparameter. Takingthesecond-orderderivativeofLi(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)w.r.t.Œ¶Œ¶Œ¶(si)will
‚ä∫ k k
involveŒ∏Œ∏Œ∏iŒ∏Œ∏Œ∏i ,whichisapositivesemi-definitematrixaslongasŒ∏Œ∏Œ∏i Ã∏=000. Positivesemi-definiteness
oftheHessianimpliesconvexity. Hence, Li(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)isconvexonŒ¶Œ¶Œ¶(si)underafixedŒ∏i. This
k k
propertyholdsviceversa,i.e.,Li(Œ¶Œ¶Œ¶(si),Œ∏Œ∏Œ∏i)isconvexonŒ∏Œ∏Œ∏iunderafixedŒ¶Œ¶Œ¶(si).
k k
Recall that the optimal solution Œ¶Œ¶Œ¶‚àó and Œ∏Œ∏Œ∏‚àó is defined as the set of possible values that make the
0
expectation of stochastic gradient g and h tends to be 0, as defined in (12), which is analogy to
makethefirst-ordergradientoflossfunctionbe0andachievethelocalminima. Theinequalitiesin
Lemma4.10denotethattheupdatesmadetothefeaturematrixŒ¶Œ¶Œ¶forfixedŒ∏Œ∏Œ∏inthefirstequationand
theparametersŒ∏Œ∏Œ∏forfixedŒ¶Œ¶Œ¶inthesecondequationisdirectedtowardsreducingthedeviationfrom
the optimal solutions close to initial point. As we only care about the solution to make stochastic
gradients be 0, for a fixedŒ∏Œ∏Œ∏, the loss function L is convex w.r.t. Œ¶Œ¶Œ¶, the learning process ofŒ¶Œ¶Œ¶ is
guaranteedtomovetowardsdecreasingthedifferencefromanoptimalpoint. Thisalsoholdsforthe
updateofŒ∏Œ∏Œ∏.
E.5 PROOFOFLEMMA4.12
Proof. UnderLemma4.6,wehave
‚à•g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)‚àíg(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó,X)‚à•‚â§L(‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•),‚àÄi‚àà[N]. (21)
Similarly,underLemma4.7,wehave
‚à•h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)‚àíh(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó,X)‚à•‚â§L(‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•),‚àÄi‚àà[N]. (22)
LetL =max(L,max g(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó,X),max h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó,X)),thenaccordingto(21)-(22),
1 X X
wehave
‚à•g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶)‚à•‚â§L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1),
1
and
‚à•h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶)‚à•‚â§L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1).
1
Denotehj(Œ∏Œ∏Œ∏,œïœïœï,X)asthej-thelementofh(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X). FollowingChenetal.(2019),wecanshow
thatŒ∏Œ∏Œ∏ ‚ààRd,Œ¶Œ¶Œ¶‚ààR|S|√ód,andx‚ààX,
‚à•E[h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)|X =x]‚àíE [h(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)]‚à•
0 ¬µ
d
(cid:88)
‚â§ |E[hj(Œ∏Œ∏Œ∏,ŒªŒªŒª,X)|X =x]‚àíE [hj(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)]|
0 ¬µ
j=1
(cid:12)
(cid:88)d (cid:12) (cid:20) hj(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X) (cid:12) (cid:21)
‚â§2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1) (cid:12)E (cid:12)X =x
1 (cid:12) 2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œª‚àíŒª‚àó‚à•+1)(cid:12) 0
(cid:12) 1
j=1
(cid:12)
(cid:20) hj(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X) (cid:21)(cid:12)
‚àíE (cid:12)
¬µ 2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œª‚àíŒª‚àó‚à•+1) (cid:12)
1 (cid:12)
‚â§2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1)dC œÅk,
1 1 1
21wherethelastinequalityholdsduetoAssumption4.3withconstantsC > 0andœÅ ‚àà (0,1). To
1 1
guarantee2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1)dC œÅk ‚â§Œ¥(‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1),wehave
1 1 1
log(1/Œ¥)+log(2L C d)
œÑ ‚â§ 1 1 . (23)
Œ¥ log(1/œÅ )
1
Usingthesameprocedureswecanshowthat
‚à•E[g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)|X =x]‚àíE [g(Œ∏Œ∏Œ∏,Œ¶Œ¶Œ¶,X)]‚à•‚â§2L (‚à•Œ∏Œ∏Œ∏‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•+‚à•Œ¶Œ¶Œ¶‚àíŒ¶Œ¶Œ¶‚àó‚à•+1)dC œÅk,
0 ¬µ 1 2 2
hencewehave
log(1/Œ¥)+log(2L C d)
œÑ ‚â§ 1 2 . (24)
Œ¥ log(1/œÅ )
2
BysettingœÑ asthelargestvaluein(23)and(24),wearriveatthefinalresultinLemma4.12.
Œ¥
F PROOFS OF MAIN RESULTS
F.1 PROOFOFTHEOREM4.13
For notational simplicity, in the proofs, we use h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ) to denote h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,{Xi }K ),
t+1 t t+1 t t,k‚àí1 k=1
andg(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )todenoteg(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ,Xi ). Inthefollowing,wefirstfocusontheupdateof
t,k‚àí1 t t,k‚àí1 t t,k‚àí1
theglobalrepresentationŒ¶Œ¶Œ¶ andcharacterizethedriftofit.
t
F.1.1 DRIFTOFŒ¶Œ¶Œ¶
t
ThedriftofŒ¶Œ¶Œ¶ isgiveninthefollowinglemma.
t
LemmaF.1. ThedriftbetweenŒ¶Œ¶Œ¶ andŒ¶Œ¶Œ¶ isgivenby
t+1 t
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t+1
=E[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+
NŒ≤ t2 2EÔ£Æ Ô£∞(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:88)N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)(cid:13) (cid:13)
(cid:13)
(cid:13)2Ô£π
Ô£ª+2Œ≤
tE(cid:34)
‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶
t,‚àí N1(cid:88)N
h¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)‚ü©(cid:35)
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Term1 Term2
(cid:34) N (cid:35)
+2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü© . (25)
t t N t+1 t t+1 t
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term3
Proof. BasedontheupdateofŒ¶Œ¶Œ¶ in(11),Wehavethefollowingequation
t
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]‚àíE[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t+1 t
=E[‚à•Œ¶Œ¶Œ¶‚àó‚à•2+‚à•Œ¶Œ¶Œ¶ ‚à•2‚àí2‚ü®Œ¶Œ¶Œ¶‚àó,Œ¶Œ¶Œ¶ ‚ü©]‚àíE[‚à•Œ¶Œ¶Œ¶‚àó‚à•2+‚à•Œ¶Œ¶Œ¶ ‚à•2‚àí2‚ü®Œ¶Œ¶Œ¶‚àó,Œ¶Œ¶Œ¶ ‚ü©]
t+1 t+1 t t
=E[‚à•Œ¶Œ¶Œ¶ ‚à•2]‚àíE[‚à•Œ¶Œ¶Œ¶ ‚à•2]‚àí2‚ü®Œ¶Œ¶Œ¶‚àó,Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚ü©]
t+1 t t+1 t
=E[‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ,Œ¶Œ¶Œ¶ +Œ¶Œ¶Œ¶ ‚ü©]‚àí2‚ü®Œ¶Œ¶Œ¶‚àó,Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚ü©]
t+1 t t+1 t t+1 t
=E[‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ,Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚ü©]+2E[‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ,Œ¶Œ¶Œ¶ ‚ü©]‚àí2‚ü®Œ¶Œ¶Œ¶‚àó,Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚ü©]
t+1 t t+1 t t+1 t t t+1 t
Œ≤2 Ô£Æ(cid:13) (cid:13)(cid:88)N (cid:13) (cid:13)2Ô£π (cid:34) 1 (cid:88)N (cid:35)
= Nt 2E Ô£∞(cid:13)
(cid:13)
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)(cid:13)
(cid:13)
Ô£ª‚àí2Œ≤ tE ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ t,
N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚ü© , (26)
(cid:13) (cid:13)
i=1 i=1
whichdirectlyleadsto
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t+1
Œ≤2 Ô£Æ(cid:13) (cid:13)(cid:88)N (cid:13) (cid:13)2Ô£π (cid:34) 1 (cid:88)N (cid:35)
=E[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ Nt 2E Ô£∞(cid:13)
(cid:13)
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)(cid:13)
(cid:13)
Ô£ª‚àí2Œ≤ tE ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ t,
N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚ü© .
(cid:13) (cid:13)
i=1 i=1
(27)
Rearrangingthelasttermyieldsthedesiredresult.
22Inthefollowing,weseparatelyboundTerm toTerm .
1 3
WefirstboundTerm asfollows.
1
LemmaF.2. Foranyt‚â•œÑ,wehave
4Œ≤2L2 (cid:34) (cid:88)N (cid:35)
Term ‚â§4Œ≤2(L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4Œ≤2Œ¥2 (28)
1 t t N t+1 t t
i=1
Proof. Notethat
Ô£Æ(cid:13) (cid:13)2Ô£π
Œ≤2 (cid:13)(cid:88)N (cid:88)N (cid:88)N (cid:13)
Term 1 = Nt 2E Ô£∞(cid:13) (cid:13) h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚àí h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)+ h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1 i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
triangularinequality 2Œ≤2 (cid:13)(cid:88)N (cid:88)N (cid:13)
‚â§ N2t E Ô£∞(cid:13) (cid:13) h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚àí h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzofh
Ô£Æ(cid:13) (cid:13)2Ô£π
2Œ≤2 (cid:13)(cid:88)N (cid:13)
+ N2t E Ô£∞(cid:13) (cid:13) h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
(a ‚â§1) 2Œ≤ Nt2L 22 E(cid:34) 2N(cid:88)N (cid:13) (cid:13)(Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t))(cid:13) (cid:13)2 +2N2‚à•(Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó)‚à•2(cid:35)
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
2Œ≤2 (cid:13)(cid:88)N (cid:88)N (cid:88)N (cid:13)
+ N2t E Ô£∞(cid:13) (cid:13) h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)‚àí h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)+ h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1 i=1
4Œ≤2L2 (cid:34) (cid:88)N (cid:35)
‚â§4Œ≤2L2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t E ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
t t N t+1 t
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
4Œ≤2 (cid:13)(cid:88)N (cid:88)N (cid:13)
+ N2t E Ô£∞(cid:13) (cid:13) h(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶‚àó)‚àí h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzofh,yi
Ô£Æ(cid:13) (cid:13)2Ô£π
4Œ≤2 (cid:13)(cid:88)N (cid:13)
+ N2t E Ô£∞(cid:13) (cid:13) h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
(a ‚â§2)
4Œ≤2L2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
4Œ≤ t2L2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
t t N t+1 t
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
+4Œ≤
t2L4E(cid:104)
‚à•Œ¶Œ¶Œ¶
t‚àíŒ¶Œ¶Œ¶‚àó‚à•2(cid:105)
+
4 NŒ≤ 2t2
E
Ô£∞(cid:13)
(cid:13)
(cid:13)(cid:88)N h(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)‚àí(cid:88)N h¬Ø(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1
(a ‚â§3)
4Œ≤2(L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
4Œ≤ t2L2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+4Œ≤2Œ¥2,
t t N t+1 t t
i=1
wherethe(a )isdueto‚à•(cid:80)N x ‚à•2 ‚â§N(cid:80)N ‚à•x ‚à•2,(a )isduetotheLipschitzoffunctionsh
1 i=1 i i=1 i 2
andyi,and(a )holdsbasedonthemixingtimepropertyinDefinition4.3.
3
Next,weboundTerm inthefollowinglemma.
2
23LemmaF.3. Wehave
(cid:34) N (cid:35)
Term ‚â§Œ≤ (L/Œ± ‚àí2œâ)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
Œ≤ tŒ± tL
E
(cid:88)
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 . (29)
2 t t t N t+1 t
i=1
Proof. Wehave
(cid:34) N (cid:35)
Term =2Œ≤ E ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶
,‚àí1(cid:88)
h¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
2 t t N t+1 t
i=1
(cid:34) N (cid:35)
=2Œ≤ E ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶
,‚àí1(cid:88)
h¬Ø(yi(Œ¶Œ¶Œ¶ ),Œ¶Œ¶Œ¶ )‚ü©
t t N t t
i=1
Ô£Æ Ô£π
Ô£Ø N Ô£∫
+2Œ≤ tEÔ£Ø Ô£Ø Ô£Ø‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ t, N1 (cid:88) h¬Ø(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶ t)‚àíh¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚ü©Ô£∫ Ô£∫
Ô£∫
Ô£∞ i=1 Ô£ª
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitz ofh
(cid:34) N (cid:35) (cid:34) N (cid:35)
‚â§2Œ≤ E ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ,‚àí1(cid:88) h¬Ø(yi(Œ¶Œ¶Œ¶ ),Œ¶Œ¶Œ¶ )‚ü© +2Œ≤ LE ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ , 1 (cid:88) (yi(Œ¶Œ¶Œ¶ )‚àíŒ∏Œ∏Œ∏i )‚ü©
t t N t t t t N t t+1
i=1 i=1
(cid:34) N (cid:35)
( ‚â§b1)
2Œ≤ E ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶
,‚àí1(cid:88)
h¬Ø(yi(Œ¶Œ¶Œ¶ ),Œ¶Œ¶Œ¶ )‚ü© +Œ≤ L/Œ± E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t N t t t t t
i=1
(cid:34) N (cid:35)
+
Œ≤ tŒ± tL
E
‚à•(cid:88)
(Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ ))‚à•2
N2 t+1 t
i=1
(cid:34) N (cid:35)
( ‚â§b2) 2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h¬Ø(yi(Œ¶Œ¶Œ¶ ),Œ¶Œ¶Œ¶ )‚ü© +Œ≤ L/Œ± E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t N t t t t t
i=1
(cid:34) N (cid:35)
+
Œ≤ tŒ± tL
E
(cid:88)
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
N t+1 t
i=1
(cid:34) N (cid:35)
‚â§Œ≤ (L/Œ± ‚àí2œâ)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
Œ≤ tŒ± tL
E
(cid:88)
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 ,
t t t N t+1 t
i=1
where (b ) holds because 2xTy ‚â§ Œ≤‚à•x‚à•2 + 1/Œ≤‚à•y‚à•2,‚àÄŒ≤ > 0, (b ) is due to ‚à•(cid:80)N x ‚à•2 ‚â§
1 2 i=1 i
N(cid:80)N ‚à•x ‚à•2,andthelastinequalityisduetoAssumption4.10.
i=1 i
Next,weboundTerm inthefollowinglemmas.
3
LemmaF.4. Forallt‚â•œÑ wehave
Term ‚â§(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
3 t t t t t t t‚àíœÑ t
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t
+
16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i,‚àó‚àíŒ∏Œ∏Œ∏
‚à•2(cid:35)
+11Œ≤ Œ± Œ¥2. (30)
N t+1 t t
i=1
Proof. WefirstdecomposeTerm asfollows
3
(cid:34) N (cid:35)
Term =2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
3 t t N t+1 t t+1 t
i=1
24(cid:34) N (cid:35)
=2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ , 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t t t‚àíœÑ N t+1 t t+1 t
i=1
(cid:34) N (cid:35)
+2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t t‚àíœÑ N t+1 t t+1 t
i=1
(cid:34) N (cid:35)
=2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ , 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t t t‚àíœÑ N t+1 t t+1 t
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C1
(cid:34) N N (cid:35)
1 (cid:88) 1 (cid:88)
+2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t t‚àíœÑ N t+1 t N t+1 t‚àíœÑ
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C2
(cid:34) N N (cid:35)
+2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí 1 (cid:88) h¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t t‚àíœÑ N t+1 t‚àíœÑ N t+1 t‚àíœÑ
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C3
(cid:34) N N (cid:35)
+2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí 1 (cid:88) h¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü© .
t t‚àíœÑ N t+1 t‚àíœÑ N t+1 t
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
C4
Next,weboundC as
1
(cid:34) N (cid:35)
C =2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ , 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àíh¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
1 t t t‚àíœÑ N t+1 t t+1 t
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
‚â§Œ≤ t/Œ± tE[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶ t‚àíœÑ‚à•2]+Œ≤ tŒ± tE
Ô£∞(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚àíh¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)+h¬Ø(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13) 1 (cid:88)N (cid:13)
‚â§Œ≤ t/Œ± tE[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶ t‚àíœÑ‚à•2]+2Œ≤ tŒ± tE Ô£∞(cid:13) (cid:13)N h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
+2Œ≤ tŒ± tE
Ô£∞(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¬Ø(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)‚àíh¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
=Œ≤ t/Œ± tE[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶ t‚àíœÑ‚à•2]+
2Œ≤ NtŒ±
2tE
Ô£∞(cid:13)
(cid:13)
(cid:13)(cid:88)N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
+2Œ≤ tŒ± tE
Ô£∞(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¬Ø(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)‚àíh¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
LemmaF.2
‚â§ Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t t
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
+2Œ≤ tŒ± tE
Ô£∞(cid:13)
(cid:13)
(cid:13)N1 (cid:88)N
h¬Ø(yi(Œ¶Œ¶Œ¶‚àó),Œ¶Œ¶Œ¶‚àó)‚àíh¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzof h
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
25+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t t
i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13) 1 (cid:88)N (cid:13)
+2Œ≤ tŒ± tL2E Ô£∞(cid:13) (cid:13)N 2(Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ t)+2(Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶‚àó))(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t t
i=1
+4Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
4Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i
‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•2(cid:35)
t t t N t+1
i=1
=Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2+4Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
N t+1 t t t t t t
i=1
+
4Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )+yi(Œ¶Œ¶Œ¶
)‚àíyi(Œ¶Œ¶Œ¶‚àó)‚à•2(cid:35)
N t+1 t t
i=1
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t t
i=1
+4Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
8Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
t t t N t+1 t
i=1
+8Œ≤ Œ± L4E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t
=Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤ Œ± L2(3+4L2)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
16Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2,
N t+1 t t t
i=1
wherethelastinequalityisduetotheLipschitzofthefunctionyi.
Next,weboundC asfollows.
2
(cid:34) N N (cid:35)
1 (cid:88) 1 (cid:88)
C =2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
2 t t‚àíœÑ N t+1 t N t+1 t‚àíœÑ
i=1 i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13) 1 (cid:88)N 1 (cid:88)N (cid:13)
‚â§Œ≤ t/Œ± tE[‚à•Œ¶Œ¶Œ¶ t‚àíœÑ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+Œ≤ tŒ± tE Ô£∞(cid:13) (cid:13)N h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t)‚àí N h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶ t‚àíœÑ)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1
(cid:124) (cid:123)(cid:122) (cid:125)
Lipschitzof h
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àíœÑ t t t t‚àíœÑ
=Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ +Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àíœÑ t t t t t t‚àíœÑ
‚â§2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+Œ≤ Œ± L2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àíœÑ t t t t t t t t‚àíœÑ
=(2Œ≤ /Œ± +Œ≤ Œ± L2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2].
t t t t t‚àíœÑ t t t t
Similarly,C isboundedexactlysameasC ,i.e.,
4 2
C ‚â§(2Œ≤ /Œ± +Œ≤ Œ± L2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2].
4 t t t t t‚àíœÑ t t t t
26Next,weboundC asfollows.
3
(cid:34) N N (cid:35)
C =2Œ≤ E ‚ü®Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó, 1 (cid:88) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí 1 (cid:88) h¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
3 t t‚àíœÑ N t+1 t‚àíœÑ N t+1 t‚àíœÑ
i=1 i=1
Ô£Æ(cid:13) (cid:13)2Ô£π
‚â§Œ≤ t/Œ± tE[‚à•Œ¶Œ¶Œ¶ t‚àíœÑ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+Œ≤ tŒ±
tN1
2E
Ô£∞(cid:13)
(cid:13)
(cid:13)(cid:88)N
h(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t‚àíœÑ)‚àí(cid:88)N
h¬Ø(Œ∏Œ∏Œ∏i t+1,Œ¶Œ¶Œ¶
t‚àíœÑ)(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1 i=1
Definition4.3
‚â§ Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t‚àíœÑ
Ô£Æ(cid:32)
N
(cid:33)2Ô£π
+Œ≤ tŒ± tN1 2E Ô£∞ NŒ¥‚à•Œ¶Œ¶Œ¶ t‚àíœÑ ‚àíŒ¶Œ¶Œ¶‚àó‚à•+NŒ¥+Œ¥(cid:88)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13) Ô£ª
i=1
(cid:104) (cid:105)
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+3Œ≤ Œ± Œ¥2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2 +3Œ≤ Œ± Œ¥2
t t t‚àíœÑ t t t‚àíœÑ t t
+ 3Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N (cid:13) (cid:13)Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13)2(cid:35)
N t+1
i=1
(cid:104) (cid:105)
=Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+3Œ≤ Œ± Œ¥2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2 +3Œ≤ Œ± Œ¥2
t t t‚àíœÑ t t t‚àíœÑ t t
+ 3Œ≤ t NŒ± tŒ¥2 E(cid:34) (cid:88)N (cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)+yi(Œ¶Œ¶Œ¶ t)‚àíyi(Œ¶Œ¶Œ¶‚àó)(cid:13) (cid:13)2(cid:35)
i=1
(cid:104) (cid:105)
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+3Œ≤ Œ± Œ¥2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2 +3Œ≤ Œ± Œ¥2
t t t‚àíœÑ t t t‚àíœÑ t t
+ 6Œ≤ t NŒ± tŒ¥2 E(cid:34) (cid:88)N (cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:35) +6Œ≤ tŒ± tL2Œ¥2E(cid:104) ‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2(cid:105)
i=1
‚â§(2Œ≤ /Œ± +6Œ≤ Œ± Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+(2Œ≤ /Œ± +6Œ≤ Œ± Œ¥2+6Œ≤ Œ± L2Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t‚àíœÑ t t t t t t t t
+3Œ≤ tŒ± tŒ¥2+ 6Œ≤ t NŒ± tŒ¥2 E(cid:34) (cid:88)N (cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:35) ,
i=1
wherethelastinequalitycomesfromE[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]‚â§2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2].
t‚àíœÑ t‚àíœÑ t t
Hence,wehaveTerm asfollows
3
Term =C +C +C +C
3 1 2 3 4
‚â§Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤ Œ± L2(3+4L2)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t‚àíœÑ t t t
+
16Œ≤ tŒ± tL2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t t
i=1
+(2Œ≤ /Œ± +Œ≤ Œ± L2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t‚àíœÑ t t t t
+(2Œ≤ /Œ± +6Œ≤ Œ± Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t t‚àíœÑ t
+(2Œ≤ /Œ± +6Œ≤ Œ± Œ¥2+6Œ≤ Œ± L2Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t
+3Œ≤ tŒ± tŒ¥2+ 6Œ≤ t NŒ± tŒ¥2 E(cid:34) (cid:88)N (cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:35)
i=1
+(2Œ≤ /Œ± +Œ≤ Œ± L2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+2Œ≤ /Œ± E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t‚àíœÑ t t t t
‚â§(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t t t t t‚àíœÑ t
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t
+
16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•yi(Œ¶Œ¶Œ¶ )‚àíŒ∏Œ∏Œ∏i
‚à•2(cid:35)
+11Œ≤ Œ± Œ¥2,
N t t+1 t t
i=1
whichcompletestheproof.
27ToboundTerm ,weneedtoboundE[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2],whichisshowninthefollowinglemma.
3 t t‚àíœÑ
LemmaF.5. wehave‚àÄt‚â•2œÑ
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]‚â§4œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2. (31)
t t‚àíœÑ 0 0 t 0 0
Proof. The proof follows similar procedures of proof for Lemma 3 in Dal Fabbro et al. (2023).
Startingwith
(cid:13) (cid:13)2
Œ≤2 (cid:13)(cid:88)N (cid:13) 1 (cid:88)N
‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2 =‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2+ t (cid:13) h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )(cid:13) ‚àí2Œ≤ ‚ü®Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ , h(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚ü©
t+1 t N2 (cid:13) t+1 t (cid:13) t t N t+1 t
(cid:13) (cid:13)
i=1 i=1
(cid:13) (cid:13)2
‚â§(1+Œ≤ /Œ± )‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2+
(Œ≤ tŒ± 0+Œ≤ t2)(cid:13) (cid:13)(cid:88)N
hi(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶
)(cid:13)
(cid:13)
t 0 t N2 (cid:13) t t+1 t (cid:13)
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
‚â§(1+Œ≤ /Œ± )‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2+
2Œ≤ tŒ±
0
(cid:13) (cid:13)(cid:88)N
hi(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶
)(cid:13)
(cid:13) , (32)
t 0 t N2 (cid:13) t t+1 t (cid:13)
(cid:13) (cid:13)
i=1
wherethefirstinequalityholdsdueto2xTy‚â§Œ≥‚à•x‚à•2+1/Œ≥‚à•y‚à•2,‚àÄŒ≥ >0,andthesecondinequality
holdssinceŒ≤ Œ± ‚â•Œ≤2. WethenhavethefollowinginequalityaccordingtoLemmaF.2,
t 0 t
E(cid:2) ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:3) ‚â§(1+Œ≤ /Œ± +8Œ≤ Œ± L2(1+L2))E(cid:2) ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:3)
t+1 t 0 t 0 t
+
8Œ≤ tŒ± 0L2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
+8Œ≤ Œ± Œ¥2
N t+1 t t 0
i=1
‚â§(1+Œ≤ /Œ± +8Œ≤ Œ± L2(1+L2))E(cid:2) ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:3) +8Œ≤ Œ± (L2B2+Œ¥2).
t 0 t 0 t t 0
(33)
BylettingŒ± ‚â§ ‚àö 1 ,wehaveŒ≤ /Œ± ‚â•8Œ≤ Œ± L2(1+L2),andhence
0 t 0 t 0
2L 2(1+L2)
E(cid:2) ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:3) ‚â§(1+2Œ≤ /Œ± )E(cid:2) ‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:3) +8Œ≤ Œ± (L2B2+Œ¥2). (34)
t+1 0 0 t 0 0
Therefore,forallt‚Ä≤suchthatt‚àíœÑ ‚â§t‚Ä≤ ‚â§t,
œÑ‚àí1
(cid:88)
E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]‚â§(1+2Œ≤ /Œ± )œÑE[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤ Œ± (L2B2+Œ¥2) (1+2Œ≤ /Œ± )‚Ñì.
t‚Ä≤ 0 0 t‚àíœÑ 0 0 0 0
‚Ñì=0
(35)
Usingthefactthat(1+x)‚â§ex(DalFabbroetal.,2023),ifweletŒ≤ /Œ± ‚â§ 1 ,wehave
0 0 8œÑ
(1+2Œ≤ /Œ± )‚Ñì ‚â§(1+2Œ≤ /Œ± )œÑ ‚â§e0.25 ‚â§2,
0 0 0 0
and
œÑ‚àí1
(cid:88)
(1+32Œ≤2)‚Ñì ‚â§2œÑ.
‚Ñì=0
Hence,wehave
E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]‚â§2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+16Œ≤ Œ± œÑ(L2B2+Œ¥2).
t‚Ä≤ t‚àíœÑ 0 0
Since ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2 ‚â§ œÑ(cid:80)t‚àí1 ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2 = œÑ Œ≤2 (cid:80)t‚àí1 ‚à•(cid:80)N hi(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚à•2, when
t t‚àíœÑ ‚Ñì=t‚àíœÑ ‚Ñì+1 ‚Ñì N2 ‚Ñì=t‚àíœÑ i=1 ‚Ñì ‚Ñì+1 ‚Ñì
t‚â•2œÑ,wehave‚Ñì‚â•œÑ andthus
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àíœÑ
Œ≤2 (cid:88)t‚àí1 (cid:88)N
‚â§œÑ ‚à• hi(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚à•2
N2 ‚Ñì ‚Ñì+1 ‚Ñì
‚Ñì=t‚àíœÑ i=1
t‚àí1
(cid:88)
‚â§œÑ ((4Œ≤2(L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤2L2B2œÑ2+4Œ≤2Œ¥2œÑ2
0 ‚Ñì 0 0
‚Ñì=t‚àíœÑ
28‚â§4Œ≤2(L2+L4)œÑ2(2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+16Œ≤ Œ± œÑ(L2B2+Œ¥2))+4Œ≤2L2B2œÑ2+4Œ≤2Œ¥2œÑ2
0 t‚àíœÑ 0 0 0 0
=8Œ≤2(L2+L4)œÑ2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤2L2B2œÑ2+4Œ≤2Œ¥2œÑ2
0 t‚àíœÑ 0 0
‚â§œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤2L2B2œÑ2+4Œ≤2Œ¥2œÑ2
0 0 t‚àíœÑ 0 0
‚â§2œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+2œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤2L2B2œÑ2+4Œ≤2Œ¥2œÑ2.
0 0 t 0 0 t t‚àíœÑ 0 0
Since2œÑ2Œ≤2/Œ±2 ‚â§1/2whenŒ≤ /Œ± ‚â§ 1 ,wehave
0 0 0 0 8œÑ
E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]‚â§4œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2.
t t‚àíœÑ 0 0 t 0 0
Thiscompletestheproof.
LemmaF.6. Term isboundedasfollows
3
Term ‚â§(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)(4œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2)
3 t t t t t t 0 0 t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t
+
16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i,‚àó‚àíŒ∏Œ∏Œ∏
‚à•2(cid:35)
+11Œ≤ Œ± Œ¥2.
N t+1 t t
i=1
Proof. SubstitutingtheboundofE[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]in(31)intoTerm inLemmaF.4yieldthefinal
t t‚àíœÑ 3
results.
Provided Term in Lemma F.2, Term in Lemma F.3, and Term in Lemma F.6, we have the
1 2 3
followinglemmatocharacterizethedriftbetweenŒ¶Œ¶Œ¶ andŒ¶Œ¶Œ¶ .
t+1 t
LemmaF.7. Fort‚â•2œÑ,thefollowingholds
E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t+1
‚â§(1+4Œ≤2(L2+L4)+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)4œÑ2Œ≤2/Œ±2
t t t t t t t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))+Œ≤ (L/Œ± ‚àí2œâ))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t t t
+
4Œ≤ t2L2+Œ≤ tŒ± tL+16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i,‚àó‚àíŒ∏Œ∏Œ∏
‚à•2(cid:35)
N t+1
i=1
+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)(8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2)+4Œ≤2Œ¥2+11Œ≤ Œ± Œ¥2.
t t t t t t 0 0 t t t
Proof. SubstitutingTerm ,Term andTerm backintoLemmaF.1,wehave
1 2 3
E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t+1
4Œ≤2L2 (cid:34) (cid:88)N (cid:35)
‚â§E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+4Œ≤2(L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4Œ≤2Œ¥2
t t t N t+1 t t
i=1
(cid:34) N (cid:35)
+Œ≤ (L/Œ± ‚àí2œâ)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+
Œ≤ tŒ± tL
E
(cid:88)
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
t t t N t+1 t
i=1
+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)(4œÑ2Œ≤2/Œ±2E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2)
t t t t t t 0 0 t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t
+
16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i,‚àó‚àíŒ∏Œ∏Œ∏
‚à•2(cid:35)
+11Œ≤ Œ± Œ¥2
N t+1 t t
i=1
=(1+4Œ≤2(L2+L4)+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)4œÑ2Œ≤2/Œ±2
t t t t t t t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))+Œ≤ (L/Œ± ‚àí2œâ))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t t t
+
4Œ≤ t2L2+Œ≤ tŒ± tL+16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i,‚àó‚àíŒ∏Œ∏Œ∏
‚à•2(cid:35)
N t+1
i=1
+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)(8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2)+4Œ≤2Œ¥2+11Œ≤ Œ± Œ¥2.
t t t t t t 0 0 t t t
Thiscompletestheproof.
29F.1.2 DRIFTOFŒ∏Œ∏Œ∏i,‚àÄi.
t
Next,wecharacterizethedriftbetweenŒ∏Œ∏Œ∏i andŒ∏Œ∏Œ∏i.
t+1 t
LemmaF.8. ThedriftbetweenŒ∏Œ∏Œ∏i andŒ∏Œ∏Œ∏i,‚àÄiisgivenby
t+1 t
Ô£Æ(cid:13) (cid:13)2Ô£π
E[‚à•Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)‚à•2]=E Ô£∞(cid:13) (cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)+Œ± t(cid:88)K g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)
(cid:13)
Ô£ª+E(cid:104)(cid:13) (cid:13)yi(Œ¶Œ¶Œ¶ t‚àí1)‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ )+Œ± g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ),yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ ) .
t t‚àí1 t t,k‚àí1 t t‚àí1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term6
(36)
Proof. AccordingtotheupdateofŒ∏Œ∏Œ∏i in(7),wehave
t
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13) (cid:88)K (cid:13)
E[‚à•Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)‚à•2]=E Ô£∞(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)+Œ± t g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)+yi(Œ¶Œ¶Œ¶ t‚àí1)‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1
Ô£Æ(cid:13) (cid:13)2Ô£π
=E Ô£∞(cid:13) (cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)+Œ± t(cid:88)K g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)
(cid:13)
Ô£ª+E(cid:104)(cid:13) (cid:13)yi(Œ¶Œ¶Œ¶ t‚àí1)‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ )+Œ± g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ),yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ ) ,
t t‚àí1 t t,k‚àí1 t t‚àí1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term6
(37)
wherethesecondinequalityholdsdueto‚à•x+y‚à•2 =‚à•x‚à•2+‚à•y‚à•2+2‚ü®x,y‚ü©.
Wenextanalyzeeachtermin(37). First,weboundTerm inthefollowinglemma.
4
LemmaF.9. Witht‚â•œÑ,wehaveTerm boundedas
4
Term
4
‚â§(1+2Œ≤ t‚àí1/Œ± t‚àí2Œ± tKœâ)E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
+(12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t‚àí1 t‚àí1
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àí1 t t‚àí1 t t‚àí1
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤ +Œ±3/Œ≤ (3K2B2+3K2Œ¥2).
t t t t‚àí1 t t‚àí1
(38)
Proof. AccordingtothedefinitionofTerm ,wehave
4
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13) (cid:88)K (cid:13)
Term 4 =E Ô£∞(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)+Œ± t g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1
Ô£Æ(cid:13) (cid:13)2Ô£π
=E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +Œ± t2E Ô£∞(cid:13) (cid:13) (cid:13)(cid:88)K g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1
(cid:42) K (cid:43)
(cid:88)
+2Œ± Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )
t t t‚àí1 t,k‚àí1 t
k=1
30(cid:34)(cid:42) K (cid:43)(cid:35)
=E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +2Œ± tE Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1),(cid:88) g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)
k=1
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13)(cid:88)K (cid:88)K (cid:88)K (cid:88)K (cid:13)
+Œ± t2E Ô£∞(cid:13) (cid:13) g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)+ g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí g¬Ø(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1 k=1 k=1 k=1
Ô£Æ(cid:13) (cid:13)2Ô£π
‚â§E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +2Œ± t2E Ô£∞(cid:13) (cid:13) (cid:13)(cid:88)K g(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí(cid:88)K g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
MixingtimepropertyinDefinition4.3
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13)(cid:88)K (cid:88)K (cid:13)
+2Œ± t2E Ô£∞(cid:13) (cid:13) g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí g¬Ø(yi(Œ¶Œ¶Œ¶ t),Œ¶Œ¶Œ¶ t)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1 k=1
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )
t t t‚àí1 t,k‚àí1 t
k=1
‚â§E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +6Œ± t2Œ¥2K2E(cid:104) ‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2(cid:105) +6Œ± t2Œ¥2K2(1+B2)+2Œ± t2K2L2B2
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
+2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )
t t t‚àí1 t,k‚àí1 t
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term41
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ) , (39)
t t t‚àí1 t,k‚àí1 t t,k‚àí1 t
k=1 k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Term42
where the first inequality holds due to the fact that ‚à•x+y‚à•2 ‚â§ 2‚à•x‚à•2 +2‚à•y‚à•2, and the second
inequalityisduetothemixingtimepropertyoffunctiongasinDefinition4.3.
Next,weboundTerm as
41
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
Term =2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )
41 t t t‚àí1 t,k‚àí1 t
k=1
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
=2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶ )
t t t‚àí1 t t‚àí1
k=1
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶ )
t t t‚àí1 t,k‚àí1 t t t‚àí1
k=1 k=1
‚â§‚àí2Œ± KœâE(cid:2) ‚à•Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ )‚à•2(cid:3)
t t t‚àí1
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
+2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶ )
t t t‚àí1 t,k‚àí1 t t t‚àí1
k=1 k=1
‚â§‚àí2Œ± tKœâE(cid:2) ‚à•Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)‚à•2(cid:3) +Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13)(cid:88)K (cid:88)K (cid:13)
+Œ± t3/Œ≤ t‚àí1E Ô£∞(cid:13)
(cid:13)
g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí g¬Ø(Œ∏Œ∏Œ∏i t,Œ¶Œ¶Œ¶ t‚àí1)(cid:13)
(cid:13)
Ô£ª. (40)
(cid:13) (cid:13)
k=1 k=1
(cid:20)(cid:13) (cid:13)2(cid:21)
Inparticular,wecanboundE (cid:13)(cid:80)K g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí(cid:80)K g¬Ø(Œ∏Œ∏Œ∏i,Œ¶Œ¶Œ¶ )(cid:13) as
(cid:13) k=1 t,k‚àí1 t k=1 t t‚àí1 (cid:13)
Ô£Æ(cid:13) (cid:13)2Ô£π
(cid:13)(cid:88)K (cid:88)K (cid:13)
E Ô£∞(cid:13) (cid:13) g¬Ø(Œ∏Œ∏Œ∏i t,k‚àí1,Œ¶Œ¶Œ¶ t)‚àí g¬Ø(Œ∏Œ∏Œ∏i t,Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1 k=1
31Ô£Æ(cid:13) (cid:13)2Ô£π
‚â§2L2E(cid:104)
‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶
t‚àí1‚à•2(cid:105)
+2L2E
Ô£∞(cid:13)
(cid:13)
(cid:13)(cid:88)K
Œ∏Œ∏Œ∏ t,k‚àí1‚àíŒ∏Œ∏Œ∏
t(cid:13)
(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
k=1
(cid:34) K (cid:35)
‚â§2L2E(cid:104) ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2(cid:105) +2L2KE (cid:88) ‚à•Œ∏Œ∏Œ∏ ‚àíŒ∏Œ∏Œ∏ ‚à•2
t t‚àí1 t,k‚àí1 t
k=1
(cid:104) (cid:105)
‚â§2L2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2 +2L2K2B2. (41)
t t‚àí1
Substituting(41)backinto(40),wehaveTerm boundedas
41
Term
41
‚â§‚àí2Œ± tKœâE(cid:2) ‚à•Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)‚à•2(cid:3) +Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
(cid:104) (cid:105)
+Œ±3/Œ≤ (2L2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2 +2L2K2B2). (42)
t t‚àí1 t t‚àí1
WenextboundTerm as
42
(cid:34)(cid:42) K K (cid:43)(cid:35)
(cid:88) (cid:88)
Term =2Œ± E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ ), g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )‚àí g¬Ø(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ )
42 t t t‚àí1 t,k‚àí1 t t,k‚àí1 t
k=1 k=1
‚â§Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +Œ± t3/Œ≤ t‚àí1(3K2B2+3K2Œ¥2+3K2Œ¥2E[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2])
‚â§Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +Œ± t3/Œ≤ t‚àí1(3K2B2+3K2Œ¥2)
+6K2Œ¥2Œ±3/Œ≤ E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+6K2Œ¥2Œ±3/Œ≤ E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2] (43)
t t‚àí1 t‚àí1 t t‚àí1 t t‚àí1
SubstitutingTerm andTerm backinto(39),wegetthefinalresult
41 42
Term
4
‚â§E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +6Œ± t2Œ¥2K2E(cid:104) ‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2(cid:105) +6Œ± t2Œ¥2K2(1+B2)+2Œ± t2K2L2B2
+Term +Term
41 42
=E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +6Œ± t2Œ¥2K2E(cid:104) ‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2(cid:105) +6Œ± t2Œ¥2K2(1+B2)+2Œ± t2K2L2B2
‚àí2Œ± tKœâE(cid:2) ‚à•Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)‚à•2(cid:3) +Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
(cid:104) (cid:105)
+Œ±3/Œ≤ (2L2E ‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2 +2L2K2B2)
t t‚àí1 t t‚àí1
+Œ≤ t‚àí1/Œ± tE(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105) +Œ± t3/Œ≤ t‚àí1(3K2B2+3K2Œ¥2)
+6K2Œ¥2Œ±3/Œ≤ E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+6K2Œ¥2Œ±3/Œ≤ E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àí1 t‚àí1 t t‚àí1 t t‚àí1
‚â§(1+2Œ≤ t‚àí1/Œ± t‚àí2Œ± tKœâ)E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
+(12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t‚àí1 t‚àí1
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àí1 t t‚àí1 t t‚àí1
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤
t t t t‚àí1
+Œ±3/Œ≤ (3K2B2+3K2Œ¥2) (44)
t t‚àí1
Thiscompletestheproof.
Next,weboundTerm inthefollowinglemma.
5
LemmaF.10. Witht‚â•œÑ,wehaveTerm boundedas
5
4Œ≤2 L4 (cid:34) (cid:88)N (cid:35)
Term ‚â§4Œ≤2 (L4+L6)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t‚àí1 E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4L2Œ≤2 Œ¥2.
5 t‚àí1 t‚àí1 N t t‚àí1 t‚àí1
i=1
(45)
32Proof. Wehave
Term
5
=E(cid:104)(cid:13) (cid:13)yi(Œ¶Œ¶Œ¶ t‚àí1)‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:105) =L2E(cid:2) ‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶ t‚àí1‚à•2(cid:3)
Ô£Æ(cid:13) (cid:13)2Ô£π
L2Œ≤2 (cid:13)(cid:88)N (cid:13)
= Nt 2‚àí1E Ô£∞(cid:13) (cid:13) h(Œ∏Œ∏Œ∏i t,Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13) Ô£ª
(cid:13) (cid:13)
i=1
4Œ≤2 L4 (cid:34) (cid:88)N (cid:35)
‚â§4Œ≤2 (L4+L6)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t‚àí1 E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4L2Œ≤2 Œ¥2,
t‚àí1 t‚àí1 N t t‚àí1 t‚àí1
i=1
(46)
wherethelastinequalityholdsduetoLemmaF.2.
Next,weboundTerm inthefollowinglemma.
6
LemmaF.11. WehaveTerm boundedas
6
Term ‚â§Œ≤ /Œ± Term +Œ± /Œ≤ Term . (47)
6 t‚àí1 t 4 t t‚àí1 5
Proof.
(cid:34)(cid:42) K (cid:43)(cid:35)
(cid:88)
Term =2E Œ∏Œ∏Œ∏i‚àíyi(Œ¶Œ¶Œ¶ )+Œ± g(Œ∏Œ∏Œ∏i ,Œ¶Œ¶Œ¶ ),yi(Œ¶Œ¶Œ¶ )‚àíyi(Œ¶Œ¶Œ¶ )
6 t t‚àí1 t t,k‚àí1 t t‚àí1 t
k=1
Ô£Æ(cid:13) (cid:13)2Ô£π
‚â§Œ≤ t‚àí1/Œ± tE Ô£∞(cid:13) (cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)+Œ± t(cid:88)K g(Œ∏Œ∏Œ∏i t,k‚àí1(cid:13) (cid:13)
(cid:13)
Ô£ª+Œ± t/Œ≤ t‚àí1E(cid:104)(cid:13) (cid:13)yi(Œ¶Œ¶Œ¶ t‚àí1)‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
k=1 (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) Term5
Term4
(48)
ProvidingTerm inLemmaF.9,Term inLemmaF.10,andTerm inLemmaF.11,wehave
4 5 6
thefollowingresult.
LemmaF.12. Fort‚â•œÑ,thefollowingholds
E[‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2]
t+1 t
(cid:34) (cid:32)
‚â§ (1+Œ≤ /Œ± ) (1+2Œ≤ /Œ± ‚àí2Œ± Kœâ)
t‚àí1 t t‚àí1 t t
(cid:33) (cid:35)
4Œ≤2 L2 4Œ≤2 L4
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ ) t‚àí1 +(1+Œ± /Œ≤ ) t‚àí1
t t t‚àí1 t t‚àí1 N t t‚àí1 N
¬∑E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
(cid:34) (cid:32)
+ (1+Œ≤ /Œ± ) (12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )
t‚àí1 t t t t‚àí1
(cid:33)
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )(4Œ≤ (L2+L4))
t t t‚àí1 t t‚àí1 t‚àí1
(cid:35)
+(1+Œ± /Œ≤ )4Œ≤2 (L4+L6) ¬∑E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àí1 t‚àí1 t‚àí1
(cid:16)
+(1+Œ≤ /Œ± ) (12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )4Œ≤2 Œ¥2
t‚àí1 t t t t‚àí1 t t‚àí1 t‚àí1
(cid:17)
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤ +Œ±3/Œ≤ (3K2B2+3K2Œ¥2)
t t t t‚àí1 t t‚àí1
+(1‚àíŒ± /Œ≤ )¬∑4L2Œ≤2 Œ¥2. (49)
t t+1 t‚àí1
33Proof. Accordingto(36),wehave
E[‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2]=Term +Term +Term
t+1 t 4 5 6
LemmaF.11
‚â§ (1+Œ≤ /Œ± )Term +(1+Œ± /Œ≤ )Term
t‚àí1 t 4 t t‚àí1 5
(cid:32)
‚â§(1+Œ≤ t‚àí1/Œ± t) (1+2Œ≤ t‚àí1/Œ± t‚àí2Œ± tKœâ)E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
+(12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t‚àí1 t‚àí1
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t t‚àí1 t t‚àí1 t t‚àí1
(cid:33)
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤ +Œ±3/Œ≤ (3K2B2+3K2Œ¥2)
t t t t‚àí1 t t‚àí1
(cid:32)
+(1+Œ± /Œ≤ ) 4Œ≤2 (L4+L6)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àí1 t‚àí1 t‚àí1
4Œ≤2 L4 (cid:34) (cid:88)N (cid:35) (cid:33)
+ t‚àí1 E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4L2Œ≤2 Œ¥2
N t t‚àí1 t‚àí1
i=1
(cid:32)
Lemm ‚â§aF.10 (1+Œ≤ t‚àí1/Œ± t) (1+2Œ≤ t‚àí1/Œ± t‚àí2Œ± tKœâ)E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
+(12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t‚àí1 t‚àí1
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )
t t t‚àí1 t t‚àí1
(cid:16) 4Œ≤2 L2 (cid:34) (cid:88)N (cid:35) (cid:17)
¬∑ 4Œ≤2 (L2+L4)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]+ t‚àí1 E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4Œ≤2 Œ¥2
t‚àí1 t‚àí1 N t t‚àí1 t‚àí1
i=1
(cid:33)
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤ +Œ±3/Œ≤ (3K2B2+3K2Œ¥2)
t t t t‚àí1 t t‚àí1
(cid:32)
+(1+Œ± /Œ≤ ) 4Œ≤2 (L4+L6)E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àí1 t‚àí1 t‚àí1
4Œ≤2 L4 (cid:34) (cid:88)N (cid:35) (cid:33)
+ t‚àí1 E ‚à•Œ∏Œ∏Œ∏ ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2 +4L2Œ≤2 Œ¥2
N t t‚àí1 t‚àí1
i=1
(cid:34) (cid:32)
= (1+Œ≤ /Œ± ) (1+2Œ≤ /Œ± ‚àí2Œ± Kœâ)
t‚àí1 t t‚àí1 t t
(cid:33) (cid:35)
4Œ≤2 L2 4Œ≤2 L4
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ ) t‚àí1 +(1+Œ± /Œ≤ ) t‚àí1
t t t‚àí1 t t‚àí1 N t t‚àí1 N
¬∑E(cid:104)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t‚àíyi(Œ¶Œ¶Œ¶ t‚àí1)(cid:13) (cid:13)2(cid:105)
(cid:34) (cid:32)
+ (1+Œ≤ /Œ± ) (12Œ±2Œ¥2K2+6K2Œ¥2Œ±3/Œ≤ )
t‚àí1 t t t t‚àí1
(cid:33)
+(12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )(4Œ≤ (L2+L4))
t t t‚àí1 t t‚àí1 t‚àí1
(cid:35)
+(1+Œ± /Œ≤ )4Œ≤2 (L4+L6) ¬∑E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t t‚àí1 t‚àí1 t‚àí1
(cid:16)
+(1+Œ≤ /Œ± ) (12Œ±2Œ¥2K2+2L2Œ±3/Œ≤ +6K2Œ¥2Œ±3/Œ≤ )4Œ≤2 Œ¥2
t‚àí1 t t t t‚àí1 t t‚àí1 t‚àí1
34(cid:17)
+6Œ±2Œ¥2K2(1+B2)+2Œ±2K2L2B2+2L2K2B2Œ±3/Œ≤ +Œ±3/Œ≤ (3K2B2+3K2Œ¥2)
t t t t‚àí1 t t‚àí1
+(1+Œ± /Œ≤ )¬∑4L2Œ≤2 Œ¥2. (50)
t t‚àí1 t‚àí1
Thiscompletestheproof.
F.1.3 FINALSTEPOFPROOFFORTHEOREM4.13
Now,wearereadytoproofthedesiredresultinTheorem4.13.
AccordingtothedefinitionofLyapunovfunctionin(14),Wehave
N
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )=‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2+ Œ≤ t ¬∑ 1 (cid:88) ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
t+2 t+1 t+1 Œ± N t+2 t+1
t+1
i=1
‚â§(1+4Œ≤2(L2+L4)+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)4œÑ2Œ≤2/Œ±2
t t t t t t t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))+Œ≤ (L/Œ± ‚àí2œâ))E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]
t t t t t t t t t
+
4Œ≤ t2L2+Œ≤ tŒ± tL+16Œ≤ tŒ± tL2+6Œ≤ tŒ± tŒ¥2 E(cid:34) (cid:88)N
‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶
)‚à•2(cid:35)
N t+1 t
i=1
+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)(8Œ≤2L2B2œÑ2+8Œ≤2Œ¥2œÑ2)+4Œ≤2Œ¥2+11Œ≤ Œ± Œ¥2
t t t t t t 0 0 t t t
(cid:34) (cid:32)
Œ≤
+ t ¬∑ (1+Œ≤ /Œ± ) (1+2Œ≤ /Œ± ‚àí2Œ± Kœâ)
Œ± t t+1 t t+1 t+1
t+1
(cid:33) (cid:35)
4Œ≤2L2 4Œ≤2L4
+(12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ ) t +(1+Œ± /Œ≤ ) t
t+1 t+1 t t+1 t N t+1 t N
(cid:34) N (cid:35)
¬∑ N1 E (cid:88)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2
i=1
(cid:34) (cid:32)
+ (1+Œ≤ /Œ± ) (12Œ±2 Œ¥2K2+6K2Œ¥2Œ±3 /Œ≤ )
t t+1 t+1 t+1 t
(cid:33)
+(12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ )(4Œ≤ (L2+L4))
t+1 t+1 t t+1 t t
(cid:35)
+(1+Œ± /Œ≤ )4Œ≤2(L4+L6) ¬∑E[‚à•Œ¶Œ¶Œ¶‚àó‚àíŒ¶Œ¶Œ¶ ‚à•2]
t+1 t t t
(cid:16)
+(1+Œ≤ /Œ± ) (12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ )4Œ≤2Œ¥2
t t+1 t+1 t+1 t t+1 t t
(cid:17)
+6Œ±2 Œ¥2K2(1+B2)+2Œ±2 K2L2B2+2L2K2B2Œ±3 /Œ≤ +Œ±3 /Œ≤ (3K2B2+3K2Œ¥2)
t+1 t+1 t+1 t t+1 t
(cid:35)
+(1+Œ± /Œ≤ )¬∑4L2Œ≤2Œ¥2 . (51)
t+1 t t
Tosimplifythenotations,wedefine
D :=(4Œ≤2(L2+L4)+(7Œ≤ /Œ± +2Œ≤ Œ± L2+6Œ≤ Œ± Œ¥2)4œÑ2Œ≤2/Œ±2
1 t t t t t t t 0 0
+(6Œ≤ /Œ± +6Œ≤ Œ± Œ¥2(1+L2)+4Œ≤ Œ± L2(3+4L2))+Œ≤ L/Œ± )
t t t t t t t t
(cid:34) (cid:32)
Œ≤
+ t (1+Œ≤ /Œ± ) (12Œ±2 Œ¥2K2+6K2Œ¥2Œ±3 /Œ≤ )
Œ± t t+1 t+1 t+1 t
t+1
(cid:33)
+(12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ )(4Œ≤ (L2+L4))
t+1 t+1 t t+1 t t
(cid:35)
+(1+Œ± /Œ≤ )4Œ≤2(L4+L6) , (52)
t+1 t t
35and
D :=4Œ≤3/Œ± L2+Œ±2L+16Œ±2L2+6Œ± Œ± Œ¥2
2 t t+1 t t t t
(cid:34)(cid:32) (cid:33) (cid:35)
4Œ≤2L2 4Œ≤2L4
+ (2Œ≤ /Œ± )+(12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ ) t +(1+Œ± /Œ≤ ) t
t t+1 t+1 t+1 t t+1 t N t+1 t N
(cid:34) (cid:32)
+ Œ≤ /Œ± (1+2Œ≤ /Œ± ‚àí2Œ± Kœâ)
t t+1 t t+1 t+1
(cid:33) (cid:35)
4Œ≤2L2 4Œ≤2L4
+(12Œ±2 Œ¥2K2+2L2Œ±3 /Œ≤ +6K2Œ¥2Œ±3 /Œ≤ ) t +(1+Œ± /Œ≤ ) t . (53)
t+1 t+1 t t+1 t N t+1 t N
SinceD isofhigherordersofo(Œ≤ )andD isofhigherorderofo(Œ± ), wecanletD ‚â§ œâŒ≤
1 t 2 t+1 1 t
andD ‚â§KœâŒ± . Therefore,wehave
2 t+1
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§(1‚àíœâŒ≤ )M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )
t+2 t+1 t t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
+(144œÑ2K2L2Œ¥2+4L4/N)Œ≤ tŒ±
t+1
E[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ N1 E (cid:88)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2
i=1
+4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2 (3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2
t+1 t t+1 t+1 t
‚â§(1‚àíœâŒ≤ )M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )
t t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
+(144œÑ2K2L2Œ¥2+4L2/N)Œ≤ tŒ±
t
E[‚à•Œ¶Œ¶Œ¶ t‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ N1 E (cid:88)(cid:13) (cid:13)Œ∏Œ∏Œ∏i t+1‚àíyi(Œ¶Œ¶Œ¶ t)(cid:13) (cid:13)2
i=1
+4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2(3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2, (54)
t t t t t
where the first inequality holds by omitting the higher order of learning rates, and the second in-
equalityholdsduetothedecreasinglearningratesofŒ± .
t
Wenowsettheproperdecayinglearningrates. LetŒ± = Œ± /(t+2)5/6 andŒ≤ = Œ≤ /(t+2). We
t 0 t 0
thenhave
(t+2)2¬∑(1‚àíœâŒ≤ )=(t+2)2(1‚àíœâŒ≤ )/(t+2)‚â§(t+1)2, (55)
t 0
ifœâŒ≤ <2. Inaddition,wehavethefollowinginequalities
o
(t+2)2¬∑Œ± Œ≤ ‚â§Œ± Œ≤ (t+2)1/3,
t t 0 0
(t+2)2¬∑Œ±2 =Œ±2(t+2)2.
t 0
Hence,multiplyingbothsideswith(t+2)2,wehave
(t+2)2M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§(t+1)2M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )
t+2 t+1 t+1 t
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144œÑ2K2L2Œ¥2+4L2/N)Œ± Œ≤0(t+2)1/3 E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ E ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
0 t N t+1 t
i=1
+(4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2(3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2)(t+2)1/3.
0 0 0 0 0
Summingtheaboveequationfromt=0,...,T,wehave
(T +2)2M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§M({Œ∏Œ∏Œ∏i},Œ¶Œ¶Œ¶ )
t+2 t+1 1 0
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144œÑ2K2L2Œ¥2+4L2/N)Œ± Œ≤0(T +2)4/3 E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ E ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
0 0 N 1 0
i=1
+(4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2(3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2)(T +2)4/3.
0 0 0 0 0
Dividingbothsidesby(T +2)2,wehave
M({Œ∏Œ∏Œ∏i},Œ¶Œ¶Œ¶ )
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§ 1 0
t+2 t+1 (T +2)2
36Table2: Parametersetting
Parameter Description
Inputsize 4
Hiddensize 128√ó128√ó128
Outputsize 2
Activationfunction ReLu
Numberofepisodes 500
Batchsize 64
Discountfactor 0.98
œµgreedyparameter 0.01
Targetupdate 30
Buffersize 10000
Minimalsize 500
Learningrate 0.002,decaysevery100episodes
(cid:34) (cid:34) N (cid:35)(cid:35)
1 (cid:88)
+(144œÑ2K2L2Œ¥2+4L2/N)Œ± Œ≤ (T +2)‚àí2/3 E[‚à•Œ¶Œ¶Œ¶ ‚àíŒ¶Œ¶Œ¶‚àó‚à•2]+ E ‚à•Œ∏Œ∏Œ∏i ‚àíyi(Œ¶Œ¶Œ¶ )‚à•2
0 0 0 N 1 0
i=1
+(4Œ± Œ≤ K2(3Œ¥2(1+B2)+L2B2)+2Œ±2(3K2B2+3K2Œ¥2+2L2K2B2)+8Œ± Œ≤ Œ¥2)(T +2)‚àí2/3.
0 0 0 0 0
Thiscompletestheproof.
F.2 PROOFOFCOROLLARY4.15
IfŒ± =Œ≤ =o(N‚àí1/3K‚àí1/2),wehave
0 0
(cid:18) (cid:19)
1 1 1 1
M({Œ∏Œ∏Œ∏i },Œ¶Œ¶Œ¶ )‚â§O + + + ,
t+2 t+1 (T +2)2 N2/3(T +2)2/3 K2N5/3(T +2)2/3 K2N2/3(T +2)2/3
(cid:16) (cid:17)
whichisdominatedbyO 1 ifT2 >N.
N2/3(T+2)2/3
G ADDITIONAL EXPERIMENT DETAILS
Compute resources. The experiments are performed on a computer with Intel 14900k CPU with
48GBofRAM.NoGPUisinvolved.
PFEDDQN-REP inCartPoleEnvironment. Weevaluatetheperformance PFEDDQN-REP ina
modifiedCartPoleenvironment(Brockmanetal.,2016). SimilartoJinetal.(2022),wechangethe
length of pole to create different environments. Specifically, we consider 10 agents with varying
pole length from 0.38 to 0.74 with a step size of 0.04. We compare PFEDDQN-REP with (i) a
conventional DQN that each agent learns its own environment independently; and (ii) a federated
versionDQN(FedDQN)thatallowsallagentstocollaborativelylearnasinglepolicy(withoutper-
sonalization). WerandomlychooseoneagentandpresentitsperformanceinFigure3(top)(a). The
resultsoftheotheragentsarepresentedinFigure8. Again, weobservethatour PFEDDQN-REP
achieves the maximized return much faster than the conventional DQN due to leveraging shared
representationsamongagents;andobtainslargerrewardthanFedDQN,thankstoourpersonalized
policy. We further evaluate the effectiveness of shared representation learned by PFEDDQN-REP
whengeneralizesittoanewagent. AsshowninFigure3(top)(b),ourPFEDDQN-REPgeneralizes
quicklytothenewenvironment. DetailedparametersettingscanbefoundinTable2.
PFEDDQN-REP in Acrobot Environment. We further evaluate FEDDQN-REP in a modified
Acrobot environment (Brockman et al., 2016). The pole length is adjusted with [-0.3, 0.3] with
a step size of 0.06, and the pole mass with be adjusted accordingly (Jin et al., 2022). The same
twobenchmarksarecomparedasinFigure3(top). Theparametersettingremainsthesameexcept
numberofepisodesdecreasesto100. SimilarobservationscanbemadefromFigure3(bottom)and
Figure9asthosefortheCartpoleenviroments.
Hopper Environment. We further consider an environment Hopper from gym, whose state and
actionspacearebothcontinuous. Wevarythelengthoflegstobe0.02+0.001‚àói,whereiisthe
37PFedDQN-Rep DQN FedDQN FedQ-K LFRL PerDQNAvg FedAsynQ-ImAvg
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.38 0.42 0.46
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.5 0.58 0.62
102 102 102
2 2 2
n n n
ru
te1
ru
te1
ru
te1
R R R
0 0 0
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Episode Episode Episode
0.66 0.7 0.74
Figure8:ComparisonofcontrolbyDQN,FedDQNandPFEDDQN-REPinCartpoleEnvironments.
PFedDQN-Rep DQN FedDQN FedQ-K LFRL PerDQNAvg FedAsynQ-ImAvg
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
-0.24 -0.18 -0.12
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
-0.06 0 0.06
102 102 102
0 0 0
n n n
ru
te -5
ru
te -5
ru
te -5
R R R
-10 -10 -10
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Episode Episode Episode
0.12 0.18 0.24
Figure9:ComparisonofcontrolbyDQN,FedDQNandPFEDDQN-REPinAcrobotEnvironments.
i-thindexedagent,whilekeepingthesameparameterssuchashealthyreward,forwardrewardand
ctrl cost (l2 cost function to penalize large actions). We increase the number of agents to 20, and
plotthereturnwithrespecttoframes. Wegenerateanewsampledtransitiontovalidatethegener-
alization nature of the algorithms. In order to fit the algorithm to continuous setting, we modified
theproposedalgorithmtoaDDPGbasedalgorithm, similartoanyDQNrelatedbenchmarks. For
FedQ-K,LFRLandFedAsynQ-Imavg,wediscretizethestateandactionspace. SimilartoCartpole
andAcrobotenvironment,ourproposedPFedDDPG-Repachievesthebestrewardandgeneralizeto
newenvironmentsquicklyasinFigure10.
Linear Speedup. We now verify the main theoretical result of the linear speedup. We vary the
number of agents from 2 to 10. We compute the convergence time and compare it to the non-
38PFedDDPG-Rep DDPG FedDDPG FedQ-K
LFRL PerDDPGAvg FedAsynQ-ImAvg
103 103
0.6 0.6
n0.4 n0.4
ru ru
te te
R0.2 R0.2
0 0
0 500 1000 1500 2000 0 500 1000 1500 2000
Frame Frame
(a) Average return for 20 agents (b) Generalization parameter 0.05
Figure10: Hopperenvironment.
15
Cartpole
Acrobot
10
p
u
d
e
e
p
S 5
0
2 4 6 8 10
Number of agents
Figure11: Speedupwithdifferentnumberofagents.
personalization counterpart. From Figure 11, we observe that the speedup (convergence time) is
almostlinearlyincreasing(decreasing)asthenumberofclientsincreases.
Statistical Significance. We report the return average, variance average, return median and total
runningtimefor10environmentsforCartpoleandAcrobotenvironments.BycomparingwithDQN
algorithmwithoutpersonalization,wecanvalidatethelinearspeedupinrunningtime. Amongall
algorithms,ourPFedDQN-Repachievesthebestreturnaverageandmedian,withtopvarianceand
runningtime,assummarizedinTables3and4. Wealsoprovideazoom-inshortenedplotforboth
environmentstoshowthequickadaptationspeedwhensharingrepresentationsasinFigure12.
Personalization. To validate that the personalization is reached, we first compute the cosine sim-
ilarity matrix of transition probabilities in both environments. After the algorithm converges, we
computethecosinesimilaritymatrixofpolicylayer(lastlayer)intheneuronnetworkinFigures13
and14. Wenoticethatwhilethenearbyagentsmightsharesimilarityintheirpolicy, personaliza-
tion is reached corresponds to their transition probabilities. The shared representation layer stay
identical.
Table3: StatisticsforCartpoleenvironment.
Algorithm Returnaverage Varianceaverage Returnmedian Totalrunningtime(s)
PFedDQN-Rep 143 43 154 466
DQN 135 54 127 3840
FedDQN 101 67 88 387
FedQ-K 112 34 107 490
LFRL 117 47 99 434
PerDQNAvg 127 48 131 520
FedAsynQ-ImAvg 119 51 117 501
39Table4: StatisticsforAcrobotenvironment.
Algorithm Returnaverage Varianceaverage Returnmedian Totalrunningtime(s)
PFedDQN-Rep -42 37 -29 618
DQN -63 67 -57 5854
FedDQN -714 162 -625 571
FedQ-K -213 41 -202 621
LFRL -207 58 -194 676
PerDQNAvg -295 64 -277 602
FedAsynQ-ImAvg -191 36 -186 664
PFedDQN-Rep DQN FedDQN FedQ-K PFedDQN-Rep DQN FedDQN FedQ-K
LFRL PerDQNAvg FedAsynQ-ImAvg LFRL PerDQNAvg FedAsynQ-ImAvg
2
102
2
102
0
103
0
103
1.5 1.5
nruteR
1
nruteR
1
nruteR-0.5 nruteR-0.5
0.5 0.5
0 0 -1 -1
0 50 100 150 200 0 50 100 150 200 0 10 20 30 0 10 20 30
Episode Episode Episode Episode
(a) Return pole length 0.54 (b) Generalization pole length 0.82 (a) Return pole length 0.3 (b) Generalization pole length 0.36
(a)Cartpoleenvironment. (b)Acrobotenvironment.
Figure12: Shortenedplotforcartpoleandacrobotenvironment.
1.0 1.0 1.0
0.280.350.31 0.2 0.370.350.360.440.36 1 0.3 0.350.330.210.380.340.360.450.39 1 1 1 1 1 1 1 1 1 1 1
0.390.690.530.340.550.450.660.38 1 0.36 0.43 0.7 0.560.390.56 0.5 0.680.41 1 0.39 1 1 1 1 1 1 1 1 1 1
0.8 0.8 0.8
0.480.540.480.660.730.750.46 1 0.380.44 0.520.560.480.710.750.770.48 1 0.410.45 1 1 1 1 1 1 1 1 1 1
0.460.710.680.390.640.56 1 0.460.660.36 0.5 0.72 0.7 0.440.660.58 1 0.480.680.36 1 1 1 1 1 1 1 1 1 1
0.6 0.6 0.6
0.570.630.650.780.81 1 0.560.750.450.35 0.590.660.650.810.83 1 0.580.77 0.5 0.34 1 1 1 1 1 1 1 1 1 1
0.650.720.760.77 1 0.810.640.730.550.37 0.680.720.760.81 1 0.830.660.750.560.38 1 1 1 1 1 1 1 1 1 1
0.4 0.4 0.4
0.520.530.58 1 0.770.780.390.660.34 0.2 0.560.570.58 1 0.810.810.440.710.390.21 1 1 1 1 1 1 1 1 1 1
0.560.61 1 0.580.760.650.680.480.530.31 0.570.63 1 0.580.760.65 0.7 0.480.560.33 1 1 1 1 1 1 1 1 1 1
0.2 0.2 0.2
0.73 1 0.610.530.720.630.710.540.690.35 0.74 1 0.630.570.720.660.720.56 0.7 0.35 1 1 1 1 1 1 1 1 1 1
1 0.730.560.520.650.570.460.480.390.28 1 0.740.570.560.680.59 0.5 0.520.43 0.3 1 1 1 1 1 1 1 1 1 1
1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0
Agent Agent Agent
(a)Transitionprobabilityheatmap (b)Personalizationlayerheatmap. (c)Representationlayerheatmap.
Figure13: HeatmapofCartpoleenvironment.
1.0 1.0 1.0
0.15 0.11 0.120.0540.0670.12-0.098-0.150.032 1 0.0470.0320.061-0.15-0.067-0.140.0073-0.24-0.16 1 1 1 1 1 1 1 1 1 1 1
0.17 0.160.0210.24 0.19 0.310.0160.056 1 0.032 -0.026-0.036-0.0950.015-0.00530.0160.030.014 1 -0.16 1 1 1 1 1 1 1 1 1 1
0.8 0.8 0.8
0.180.0460.0560.15 0.1 0.25-0.055 1 0.056-0.15 0.081-0.0570.0270.01-0.0680.0990.037 1 0.014-0.24 1 1 1 1 1 1 1 1 1 1
-0.074-0.11-0.14-0.170.061-0.18 1 -0.0550.016-0.098 -0.023-0.049-0.069-0.230.0084-0.28 1 0.0370.030.0073 1 1 1 1 1 1 1 1 1 1
0.6 0.6 0.6
0.38 0.23 0.18 0.52 0.44 1 -0.18 0.25 0.31 0.12 0.1 -0.0140.00440.34 0.26 1 -0.280.0990.016-0.14 1 1 1 1 1 1 1 1 1 1
0.27 0.24 0.1 0.26 1 0.440.061 0.1 0.190.067 0.0870.11 -0.05-0.0063 1 0.260.0084-0.068-0.0053-0.067 1 1 1 1 1 1 1 1 1 1
0.4 0.4 0.4
0.33 0.28 0.18 1 0.26 0.52 -0.17 0.15 0.240.054 0.0990.0620.052 1 -0.00630.34 -0.23 0.010.015-0.15 1 1 1 1 1 1 1 1 1 1
0.0015-0.0061 1 0.18 0.1 0.18 -0.140.0560.0210.12 -0.18-0.098 1 0.052-0.050.0044-0.0690.027-0.0950.061 1 1 1 1 1 1 1 1 1 1
0.2 0.2 0.2
0.2 1 -0.00610.28 0.24 0.23 -0.110.0460.16 0.11 0.0016 1 -0.0980.0620.11-0.014-0.049-0.057-0.0360.032 1 1 1 1 1 1 1 1 1 1
1 0.20.00150.33 0.27 0.38-0.0740.18 0.17 0.15 1 0.0016-0.180.0990.087 0.1 -0.0230.081-0.0260.047 1 1 1 1 1 1 1 1 1 1
1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0 1 2 3 4 5 6 7 8 9 10 0.0
Agent Agent Agent
(a)Transitionprobabilityheatmap (b)Personalizationlayerheatmap. (c)Representationlayerheatmap.
Figure14: HeatmapofAcrobotenvironment.
40
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1
tnegA
tnegA
01
9
8
7
6
5
4
3
2
1
01
9
8
7
6
5
4
3
2
1