WildLMa: Long Horizon Loco-Manipulation in the Wild
Ri-Zhao Qiu‚àó1, Yuchen Song‚àó1, Xuanbin Peng‚àó1, Sai Aneesh Suryadevara1, Ge Yang2, Minghuan Liu1
Mazeyu Ji1, Chengzhe Jia1, Ruihan Yang1, Xueyan Zou1, Xiaolong Wang1,3
‚àóequal contribution
1UC San Diego 2MIT 3NVIDIA
https://wildlma.github.io
Fig. 1: WildLMa implements a framework for in-the-wild manipulation with a quadruped robot, which combines a whole-
body controller and imitation learning for effective single-skill learning. (a) Long Horizon Loco-Manipulation in indoor
as well as outdoor settings. (b) Teleoperation demonstration for collecting training data for imitation learning. (c) The
constructed skill library with various skills, which can be composed by LLM planner to complete complex tasks.
Abstract‚Äî‚ÄòIn-the-wild‚Äô mobile manipulation aims to deploy for human. To accomplish these tasks, the robot needs skills
robots in diverse real-world environments, which requires the that generalize to unseen objects and a planner capable of
robot to (1) have skills that generalize across object config-
compositing skills over a long horizon.
urations; (2) be capable of long-horizon task execution in
Existing methods [17, 20, 31, 32, 44, 61, 71] have ap-
diverse environments; and (3) perform complex manipulation
beyond pick-and-place. Quadruped robots with manipulators proached mobile manipulation from two primary directions.
holdpromiseforextendingtheworkspaceandenablingrobust Modular methods [32, 44, 71] aim at designing decoupled
locomotion, but existing results do not investigate such a perception-planning modules. With advances in large-scale
capability. This paper proposes WildLMa with three compo-
vision models [28, 34, 45], recent modular methods [32,
nents to address these issues: (1) adaptation of learned low-
44] exhibit strong generalizability in perception to an open
level controller for VR-enabled whole-body teleoperation and
traversability; (2) WildLMa-Skill ‚Äî a library of generalizable set of language-specified objects. However, their planning
visuomotor skills acquired via imitation learning or heuristics modules [6, 20, 32, 44] often rely on heuristic-based motion
and (3) WildLMa-Planner ‚Äî an interface of learned skills planning, limiting tasks to mostly simple pick-and-place.
that allow LLM planners to coordinate skills for long-horizon
End-to-end approaches [11, 12, 17, 22, 31, 69], on the other
tasks. We demonstrate the importance of high-quality training
hand, use learned policies to enable robot with complex
data by achieving higher grasping success rate over existing
RL baselines using only tens of demonstrations. WildLMa actions. They, however, often hold a strong assumption of
exploits CLIP for language-conditioned imitation learning that thesmalltraining-testingdistributiongap(e.g.,sim2real[31]
empiricallygeneralizestoobjectsunseenintrainingdemonstra- or intra-class variation [17]) and thus do not show strong
tions.Besidesextensivequantitativeevaluation,wequalitatively
generalizability. In addition, policies learned via imitation
demonstrate practical robot applications, such as cleaning up
learning are prone to compounding error [26, 69] over
trash in university hallways or outdoor terrains, operating
articulated objects, and rearranging items on a bookshelf. long-horizon execution. Thus, these learned skills should be
designedtobeasatomicaspossibleforbothgeneralizability
I. INTRODUCTION and accuracy.
Practicalrobotmobilemanipulationrequiresgeneralizable This paper investigates in-the-wild mobile manipulation
skills and long-horizon task execution. Consider a scenario thataddressestheseissuesforreal-worlddeployment.Specif-
where a mobile robot is deployed out-of-box at a family ically, in-the-wild manipulation requires the robot to have
house. The robot is tasked with daily chores including skills that (1) generalize across texture, lighting, and diverse
collectingthetrasharoundthehouseandgrabbingsomething environments;(2)arecapableoflong-horizonexecution;and
4202
voN
22
]OR.sc[
1v13151.1142:viXra(3) perform complex manipulation beyond pick-and-place. other than simple pick-and-place. WildLMa uses imitation
To this end, we propose WildLMa. For generalizability, learning to learn diverse skills with generalizability, task
WildLMa enables language-conditioned imitation learning complexity, and long-horizon run for in-the-wild execution.
(WildLMa-Skill). Building upon ACT [17, 69], WildLMa- b) Long-horizon Mobile Manipulation: For robots to
Skill improves generalizability via pre-trained CLIP and assist with real-world tasks such as cleaning up home, they
composableskills.InsteadofsimplyusingCLIPfeatures[12, need to be capable of dealing with long-horizon mobile
22], we apply a reparameterization trick [73] to CLIP to manipulation, where independent skills are planned and
computeprobabilitymapsgivenobjecttextqueryasanaux- triggered to complete given goals. Existing methods rely
iliary input. We then use VR teleoperation [9, 13] to collect on sampling-based planning [18, 53], RL [19, 30, 65, 66],
humandemonstrationstoacquirecomplexskillssuchasnon- and Large Language Models (LLMs) [20, 24, 46, 51] to
prehensile manipulation. We adapt a learned low-level con- coordinate skill primitives for long-horizon task execution.
troller [31] for VR-based whole-body teleoperation, which Recent work [20, 24, 46, 51] have found that LLM-based
significantly increases the robot workspace and reduces the methods, especially Large-Mutlimodal Models (LMMs) [8,
demonstration cost by 26.9% compared to the decoupled 37],arepromisingtoserveaseffectiveplannersforembodied
strategy.Finally,basedonalibraryofacquiredgeneralizable agents,wheretheresearcheffortsarecenteredaroundhierar-
and atomic skills, WildLMa provides a language interface chicalsearch[46]andre-planning[71].WildLMaisintended
(WildLMa-Planner) that allows interfacing with LLMs to to be orthogonal to these existing work in LLM planner.
composite skills for long-horizon execution. Instead of studying the planning capability, we investigate
In summary, our contributions are: the potential of interfacing LMMs with skills acquired via
‚Ä¢ A generic framework with techniques that allow imitation learning for practical applications.
generalizable language-condition imitation learning c) Imitation Learning: Imitation learning has demon-
(WildLMa-Skill) with interfacing to the LLM planner strated promising results through learning from real-world
(WildLMa-Planner). expert demonstrations [9, 11, 13, 17, 22, 48, 57, 64]. Inves-
‚Ä¢ Demonstrations of in-the-wild mobile manipulation tigated for decades since the 80s [41], behavior cloning [5,
tasks with full-stack and systematic deployment of the 41]isoneofthemostcommonlyusedimitationlearningap-
proposed framework. proach that learns an end-to-end mapping from observations
‚Ä¢ Comprehensiveevaluationandablationfortheproposed to actions. Recently, researchers have shown that this classic
technique, which paves the way for future study. approach not only allows complex manipulations [9, 13, 17,
70], but also holds the potential that scaling up training data
II. RELATEDWORK
with low-cost hardware [12, 17, 22, 48, 59, 64, 69] will
a) Mobile Manipulation: Mobile manipulation has lead to generalizable policies. Similar to existing work [9,
gainedincreasingattentionforitsvisionofenablingrobotsto 13, 23, 43, 49], we also use VR devices to collect expert
performdiversepracticaltasks.Intermsofhardwareconfig- demonstrations that minimize the expert-agent observation
urations,wheeledrobotshavemadesubstantialstrides[1,29, gap [69]. To reduce the cross-embodiment gap between the
32,55,58,61,71]foritsreliablebasemovement[54],while human operator and the quadruped robot, we combine the
recently, legged robots have also gained more interest for its VR demonstration with learned whole-body controller. In
robust locomotion [10, 63] and the extended workspace via addition,wealsoimprovethevanillaACTmodel[17,69]to
whole-body arm-base coordination [15, 22, 31, 42]. supportlanguage-conditionedimitationlearningthatismore
Categorized by methodology, existing methods can be di- generalizable with autonomous termination.
videdintomodularmethodsandend-to-endmethods.Recent d) Whole-bodyControl: QuadrupedWhole-BodyCon-
modular approaches [2, 27, 32, 33, 44, 66, 67, 68, 71] trol (WBC) draws inspiration from the natural motions of
design decoupled perception-planning strategy. In particular, animals to extend the robot workspace via arm-base coordi-
perception [32, 44, 71] are often done by applications of nation. The WBC capability is usually achieved via model-
vision foundation models [21, 28, 34, 45]; whereas grasp- based hierarchical trajectory optimization [3, 35, 50, 72, 74]
ing are done by off-the-shelf pose prediction models (e.g., or sim2real RL [15, 22, 31]. Our work is based on the low-
GraspNet[14])andIKsolver[47].Despitestrongperception levelcontrollerproposedbyVBC[31],whichdesignedabi-
designs,modularmethodsaremostlylimitedtosimplepick- level RL paradigm with a low-level whole-body controller.
and-place tasks. On the other hand, end-to-end approaches Notably, some existing work has also attempted to com-
use Reinforcement Learning (RL) [19, 31, 40, 60, 61] or bine teleoperation with whole-body control for quadruped
ImitationLearning(IL)[17,22,48]toenablecomplextasks robots [42, 72] but does not investigate learning skills from
beyond pick-and-place such as articulated manipulation [4, teleoperation. Most related to our work, Ha et al. [22]
61] and non-prehensile manipulation [17, 22]. However, demonstrated whole-body imitation learning with handheld
theseworkoftenfallshortwhentraining-testingdistributions data collection hardware. The main differences between our
mismatch. work and Ha et al. [22] are (1) the data collected without
Most closely related to our work, Yokoyama et al. [66] teleoperating the robot can include only wrist camera obser-
proposed to use sim2real RL for in-the-wild mobile manip- vation, which may lead to worse performance than multi-
ulation. However, they do not investigate manipulation tasks camera setup as we empirically verify and (2) Ha et al. [22]CLIP Features BaseVelocity,EEPose
Aggregation (50 Hz) Gripper w/ LivoxMid-
RealsenseD405 360
‚Ä¶
Cross Attention Flatten+MLP
door Action Chunking
Transformers 50 Hz MLP
CLIP 10 Hz Action
ADA
Task-specific button Linear Linear
Robot Azure Kinect
Texts 10 Hz ‚Ä¶ Robot ùìè State
State
(a)WildLMa-Skill (b)Whole-body Controller
Fig. 2: Overview of WildLMa models and robot setups. (a) WildLMa takes a frozen CLIP model to encode task-specific
texts and visual observations; (b) Our robot platform is a Unitree B1 quadruped combined with a Unitree Z1 arm and a
3D-printed gripper, with two RGBD cameras and one lidar mounted on.
focusonexecutionofshorttasks;whileweinvestigatein-the- minimize the expert-agent observation gap [69], the tele-
wild mobile manipulation with long horizon task execution. operator gets real-time streams of the robot‚Äôs head camera
views and wrist camera views.
To translate tele-operator movement to robot movement,
III. METHOD
welinearlytransformtheoperator‚Äôsrightwristpose(relative
to their initial hand pose) T ‚àà SE(3) into the relative
WildLMa designs three components to address challenges right
endeffectorposeT ‚ààSE(3).Wescalethetranslationswith
for in-the-wild mobile manipulation. Sec. III-A describes ee
a constant s , as we find that the workspace of the Z1 arm
adapting a whole-body controller to support efficient teleop- c
isslightlylargerthanaveragehumanarms.Moreconcretely,
eration and more diverse real-world tasks. In Sec. III-B, we
let R be the rotational component and t be the
proposeWildLMa-Skill,whichmodifiesthepre-trainedCLIP right right
translational component of T , T is given by,
model [45] for generalizable imitation learning. WildLMa- right ee
Skillthenconstructsaskilllibraryconsistingoflearnedskills (cid:20) R s ¬∑t (cid:21)
and analytical skills (e.g., navigation). Finally, WildLMa- T ee = 0ri ‚ä∫ght c 1right . (1)
Planner (Sec. III-C) interfaces WildLMa-Skill with an LLM
The gripper open-close actions are then naturally mapped
planner to carry out long-horizon execution.
fromthepinchingofthethumband theindexfinger(via3D
keypoints).Thewhole-bodycontrollerautomaticallycontrols
A. Whole-body VR Teleoperation
thebaserotationtocoordinatewiththearm.Inturn,thetele-
Recent imitation learning methods have benefited from oeprator‚Äôs left wrist governs planar base movements (e.g.,
improved data collection methods via VR/AR-based teleop- angularandlinearvelocities).Whenthetele-operatorpinches
eration [9, 13, 23, 43]. However, though human operators their left fingers, VR tracks the pose T as a virtual
left
can naturally tele-operate bipedal humanoid robots [9, 16, joystick with deadzone (x = 5cm). We find this simple
th
23], it is non-trivial to tele-operate quadruped robots due to base command mapping sufficient for the tasks involved.
the embodiment gap [39] between two-legged human and
quadruped robots inspired by four-legged animals. B. WildLMa-Skill
To reduce the need for the tele-operator to consider both WildLMa-Skill contains skills from two categories: skills
the base movement and the arm movement, we propose to acquired via imitation learning and with analytical planners.
use a whole-body controller [31] that allows smooth arm- a) WildLMa-Skill - Imitation Learning: The collected
basecoordinationfortherobot.Inparticular,weusethelow- real-world demonstrations can be turned into autonomous
levelwhole-bodypolicydevelopedbyLiuetal.[31].Trained skills with existing behavior cloning methods [9, 17]. Many
with RL, the learned whole-body controller takes in base existing methods, however, struggle to generalize to novel
commands (linear velocity and angular velocity) and 6DOF environments[17].Toimprovethegeneralizabilityoflearned
end effector pose w.r.t. the arm base. The policy outputs skills without expensive demonstration collection cost, we
armandbasejointcommandsforcoordinatedmovementthat propose to adapt pre-trained CLIP [45] to ACT [69] for
extends the workspace (illustrated in Fig. 1). imitation learning of individual skills.
Based on the pre-trained low-level controller, we then ImprovingGeneralizabilitywithCLIP.Weencodecamera
design an interface for human users to teleoperate the robot. observationswithafrozenCLIPvisualbackbone.Insteadof
We use the OpenTV framework [9] with Apple Vision Pro, simply using intermediate CLIP features as in [12, 22], we
which allows real-time video streaming, tracking of 6DOF applyMaskCLIP[73],areparameterizationtricktogenerate
poses of head and hands, and 3D gesture keypoints. To image-text cross attention map. More concretely, let ‚Ñ¶ beFine-grained Planner
Please clean the trash in the Input:
Skillset:
hallway. ‚Ä¢ Individual Task ‚Ä¢ GraspFloor(obj)
‚Ä¢ Pre-built Nodes ‚Ä¢ GraspTable(obj)
1. Go to Table in the Hallway ‚Ä¢ Skillset ‚Ä¢ Press(obj)
2. Pick up the trash ‚Ä¢ Place(obj)
3. Place trash in the trash Output: ‚Ä¢ Nav(obj)
bin Coarse Planner ‚Ä¢ Actionable Skill ‚Ä¢ PushDoor
‚Ä¢ Node Description
...
‚Ä¢ Node Information
1. Nav(Table)
2. GraspTable(Bottle)
3. Nav(TrashBin) GPT4 Agent
4. Place(TrashBin)
Level1 ‚Ä¢ Floor2RoboticLab ‚Ä¢ Floor1Hallway Pre-built Nodes
Fine-grained Planner
Level2 ‚Ä¢ TrashBin ‚Ä¢ BottleontheTable ‚Ä¢ Backpack
Fig. 3: Overview of WildLMa-planner. Given a constructed hierarchical scene graph, WildLMa-planner adopts a coarse-to-
fine searching mechanism to determine node traversal and structured actions to take.
the space of RGB images. The original CLIP [45] is a location to another known location), we implement it with
mapping function f (¬∑) : ‚Ñ¶ (cid:55)‚Üí RC, where RC is analytical planning.
visual
the image-text embedding space learned from contrastive
C. WildLMa-Planner
learning [45]. MaskCLIP modifies the network architecture
to a new mapping function g (¬∑):‚Ñ¶(cid:55)‚ÜíH√óW√óRC, The WildLMa-Skill module provides skills that can be
visual
composed for long-horizon execution, which is intentionally
whichisafeaturemapalignedtotheCLIPembeddingspace
RC (illustrated in Fig. 2). designed to be agnostic of the high-level planner. Here, we
propose WildLMa-Planner, a simple LLM-based planner to
Image-text Cross Attention. Consistent with findings by
show how learned skills can be composed.
Chi et al. [12], we found that the adaptation of CLIP [45]
Initial Mapping. We implement a LiDAR-based SLAM
improves the performance. However, when tested with ob-
system using FAST-LIO [62] and DLO [7] to obtain consis-
jectsunseeninthetrainingdemonstrations,thesuccessrateis
tent robot pose estimation in the world frame. We manually
still unsatisfactory. Thus, we propose to make the acquired
annotate pose-level waypoints (e.g., stand in front of recep-
skills language-conditioned by introducing cross-attention.
tacles) and connectivity for task execution. The robot stands
We provide task-specific texts during both the training and
at every waypoint to capture images with its head camera.
testing time (e.g., for the ADA door button-pressing task,
To automatically annotate the semantics of each waypoint,
weuse‚Äòdoor‚Äôand‚ÄòADAbutton‚Äô)withCLIPtextembedding
f (¬∑):Text(cid:55)‚ÜíRC. With slight abuse of notation, the text GPT4-V[37]provideshigh-leveldescriptionsofimagesand
text
listsofobjectsofeachwaypoint.Wemanuallycreateabstract
vector can then be compared with the CLIP feature map via
nodes (e.g., a room with multiple pose-level waypoints) to
cosine similarity
construct a hierarchical graph for searching. Note that off-
g (¬∑)f (¬∑) the-shelf scene graph construction methods [20, 25, 36] can
CROSSATT(¬∑,¬∑)= visual text , (2)
||g (¬∑)||||f (¬∑)|| potentially replace this step.
visual text
Hierarchical Long-horizon Planning. We adopt a hier-
where the comparison is done independently on the pixel archical coarse-to-fine approach to translate template-free
level. The resulting similarity is comparable to the proba- commands into detailed, actionable robot skills.
bility map of text queries. We apply dropout [52] to cross- Coarse Planner. Using CoT [56], the coarse planner
attention during training to avoid over-reliance on attention. receivestemplate-freeinstructionsanddecomposestheminto
Autonomous Termination. To autonomously terminate individual tasks. For instance, the command ‚Äòclean the trash
skills, in order to hand control back to high-level planners, in the hallway‚Äô can be decomposed into tasks ‚Äònavigate to
we add a virtual ‚Äòend‚Äô action signal prediction. Empirically, hallway‚Äô, ‚Äòpick up the trash‚Äô, and ‚Äòplace trash in the trash
addingtheendsignaltoonlytheendoftheepisodedoesnot bin‚Äô. We will release the detailed prompts.
work,asthesupervisionistoosparse.Ourproposedsolution Fine-grained Planner. The fine-grained planner invokes
istoimplementabufferofendsignalforeveryskillsuchthat actionable skills at particular nodes given individual tasks
the last n = 10 frames of the demonstrations carry the end generatedbythecoarseplanner.Thefine-grainedplannerhas
signal.Duringdeployment,weuseaslidingwindowdetector priorknowledgeoftherobot‚Äôsskilllibrary(showninFig.3)
to terminate task execution if the end signal is greater than and nodes constructed in the initial mapping stage. For each
œÑ =0.8 for 10 consecutive predictions. task, the agent uses a breadth-first search (BFS) approach to
b) WildLMa-Skill - Analytical Planning: In this paper, searchnodesandidentifytheoptimalgoalnode.Duringthis
we learn all manipulation-related skills with imitation learn- stage the LLM acts as a heuristic evaluator, estimating the
ing. For the base-only skill (i.e., navigating from a known likelihoodofanodebeingthemostlikelylocationrelatedtoTabletopGrasping ButtonPressing GroundGrasping
Method I.D. O.O.D. I.D. O.O.D I.D. O.O.D Avg.Succ.
WildLMa(Ours) 94.4% 75% 80% 57.5% 60% 60% 71.2%
ACT(MobileALOHA)[17] 77.8% 19.4% 55% 25% 60% 30% 40.8%
OpenTV[9] 88.9% 77.8% 75% 25% 50% 50% 64.4%
VBC[31] 50%‚àó 50%‚àó NA‚Ä† NA‚Ä† 43.8%‚àó 43.8%‚àó 46.9%
GeFF[44] 55.6%‚àó 55.6%‚àó NA‚Ä† NA‚Ä† NA‚Ä† NA‚Ä† 55.6%
TABLE I: Success rate of autonomous skill execution. Imitation learning methods outperform RL [31] and zero-shot
method [44] on comparable tasks. Both OpenTV and WildLMa achieve noticeably higher success rates in the challenging
O.O.D. setting. ‚Ä†: methods involve learned/manual policies that are not trivially applicable to the task settings. ‚àó: Method
does not differentiate object sets and success rates are averaged on I.D. and O.O.D. object sets.
Pipeline Collect&DropTrash ShelfRearrangement Backbone InDist. OutofDist. Avg.Succ.
WildLMa(Ours) 7/10 3/10 CLIP[45] 83.3% 69.4% 76.4%
ACT[17,69] 0/10 0/10 ResNet[69]‚ãÜ 77.8% 19.4% 48.6%
DinoV2[38] 88.9% 77.8% 83.3%
TABLE II: Evaluation of long-horizon execution. Given a
few training demonstrations (10), WildLMa improves long- TABLE III: Ablation of different visual encoders pre-
horizon task success rate via (1) improved generalizability trained with different objectives. The evaluation is done on
of single skill and (2) divide-and-conquer. the object-grasping tasks. ‚ãÜ: we followed ACT [17, 69] to
use ImageNet-pretrained ResNet-18 as the encoder, which
has fewer parameters.
the task, based on the semantic context and objects present
at the node. Once the target node is identified, the planner
eveninI.D.settings.TheOut-of-distribution(O.O.D.)setting
constructs a plan detailing the navigation and manipulation
permutesthetestingobjects(placement/texture),receptacles,
sequence drawn from the pre-defined skill library.
andbackgroundenvironmentsforlearnedskills.Illustrations
of the differences between these two settings can be found
IV. EXPERIMENTS
on the website.
Hardware Platforms. We use the Unitree B1 quadruped
Baseline Implementation. Besides ablating design
robotwithaUnitreeZ1arm.Wereplacethebeak-likedefault
choices of our components, we implement several base-
Z1endeffectorwitha3D-printedparallelsoftgripper,which
lines to validate the efficacy of WildLMa. To compare
was adapted from UMI-Gripper [12] to directly operate
with existing imitation learning methods, we choose Mobile
the gripper with gear rotations. For perception, an Azure
ALOHA [17] which uses ACT [69] with ResNet-18 and
Kinect camera is mounted on the robot‚Äôs head, and an Intel
OpenTV [9] using ACT and DinoV2 [38]. Unless specif-
Realsense D405 is used as the in-wrist camera. A LIVOX
ically noticed, these baselines use the same training data
MID-360 LiDAR is installed at the robot‚Äôs tail for enhanced
as WildLMa. In addition, we compare two recent works in
localization during navigation.
quadruped loco-manipulation [31, 44] to compare WildLMa
Implementation Details. The WildLMa-Skill module in-
against RL-based and zero-shot grasping methods. Note that
dependentlytrainsweightsforeachskill(with30-60demon-
both VBC [31] and GeFF [44] were designed for grasping,
strations each acquired via tele-operation). The head/wrist
so they are not trivially applicable to non-prehensile manip-
RGB observations are processed through a CLIP [45]
ulation such as button pressing.
ViT-B/16 encoder with MaskCLIP [73] re-parameterization.
Task-specific texts are then compared with the feature maps A. Evaluation
togeneratecross-attention,wheretextsmaydifferintraining
Weaddressimportantresearchquestionsinourevaluation:
sequences and testing run. For navigation between given
waypoints, we implement a PD-based waypoint follower. ‚Ä¢ What advantages does WildLMa-Skill have compared
WildLMa-Planner requires geometric annotations of nodes to existing baselines in quadruped manipulation? [A1,
and edges. For efficiency, the spatial locations of nodes are A2]
annotated by operating the robot to turn 360 degrees during ‚Ä¢ How does WildLMa-Planner perform in long-horizon
the initial scene scanning, and the edges are made between execution? [A3]
physically adjacent nodes with no obstacle in between. ‚Ä¢ Arethedesignchoices(e.g.,visualbackboneandcross-
Experimental Protocol. We define two experiment set- attention) effective? [A4, A5]
tings to investigate the generalizability of skills learned via ‚Ä¢ Does whole-body control improve teleoperation? [A6]
imitation learning [9, 13, 17]. The in-distribution (I.D.) set- ‚Ä¢ Whatarethereal-worldapplicationsofWildLMa?[A7]
ting tests the learned skills with backgrounds and object ar- A1. WildLMa outperforms recent imitation learning
rangementsapproximatelysimilartothetrainingdemonstra- baselines. From Tab. I, we can see that WildLMa achieves
tions.Notethat,tomakethesettingmorerealistic,wedonot bestoverallsuccessrate.ComparedtovanillaACT[17,69],
enforce identical robot positioning and lighting conditions WildLMaachievesslightlybettersuccessratesonI.D.settingWhole-body (Ours) Decoupled Control W/o Whole-body (Arm Only)
Metric Ground Grasping Rearrange Shelf Ground Grasping Rearrange Shelf Ground Grasping Rearrange Shelf
Average Time 21.87s 27.25s 37.35s 29.81s - 27.88s
Success Rate 95% 70% 80% 40% 0% 70%
TABLE IV: Comparison of success rate and completion time for our whole-body controller, decoupled control with
manual base pitching and arm control implemented via Unitree SDK, and arm-only policies. Four teleoperators are tasked
to manipulate objects at various heights for three trials in each task.
Camera TabletopGrasping ButtonPressing DoorOpening
Head+Wrist 94.4% 80% 70%
HeadOnly 27.8% 75% 30%
WristOnly 83.3% 85% 10%
TABLE V: Ablation of input visual modality. Tasks that (a) Tabletop Grasping (b) Ground Grasping
involveocclusionsignificantlybenefitfrommulti-viewsetup.
Backbone InDist. OutofDist. Avg.Succ.
w/cross-attention(Ours) 94.4% 75% 84.7%
w/ocross-attention 83.3% 69.4% 76.4%
(a) Door Button Pressing (a) Shelf Rearrangement
TABLE VI: Ablation of cross-attention on the object-
Fig. 4: Qualitative illustrations of some evaluated tasks.
grasping tasks. Cross-attention improves both I.D. and
O.O.D. setting by using additional task-specific information.
models [38, 45] perform much better in the O.O.D. setting.
A5. Cross-attention significantly improves O.O.D. imi-
and significantly better success rate on the O.O.D. setting. tation learning. Tab. VI shows the proposed cross-attention
We reason this is because ResNet is vulnerable to changes
improvesboththeI.D.andO.O.D.performanceofCLIP[45]
in lighting and texture. OpenTV [9], on the other hand,
module by introducing additional task-specific text prompts.
shows more robustness to these adversarial conditions due A6. Whole-body controllers enable efficient VR teleoper-
to its use of the recent DinoV2 backbone [38], but slightly ation of quadruped robots. The motivation for combining
underperforms our method. whole-bodycontrolandteleoperationistoimproveteleoper-
A2. WildLMa outperforms RL and zero-shot baselines.
ationefficiency.Tovalidatethispoint,wereportthestatistics
Due to less reliance on real-world demonstrations, RL and of teleoperation in Tab. IV, which shows our learning-based
zero-shot baselines demonstrate less performance gap be- controller outperforms the decoupled analytical controller
tween I.D. and the O.O.D. settings in Tab. I. As an RL- fromUnitreeSDK.Sincethesetasksrequirereachingobjects
based method, VBC [31] suffers from sim2real gaps such as at various heights (toys and books at different levels of
inaccuratecontactmodelingandcumulativesensorlatencies. storage), teleoperation without whole-body control fails to
Therefore, VBC performs worse in real-world settings than grasp from the ground due to the limited workspace.
its simulation counterpart. On the other hand, zero-shot A7. WildLMa allows the robot to learn diverse tasks.
modularmethodssuchasGeFF[44]donotnaturallyexhibit BesidesqualitativesamplesinFig.1,weprovidemorevideos
corrective behavior like learning-based methods, which are of our robot working on different practical tasks in the
vulnerable to errors compounding from different modules. supplementary video and the website.
A3.WildLMaiscapableofhandlinglong-horizonmanip-
ulation under perturbations. Tab. V validates the efficacy V. CONCLUSION
of WildLMa in handling long-horizon tasks under certain In this paper, we present WildLMa, a modular framework
perturbations. We include videos on the website. Our ex- that includes (1) WildLMa-Skill, which implements a library
periments include 20 training sequences with variations in of generalizable visuomotor skills that improve ACT [69]
robot positioning, lighting, and object placement in both the for learning generalizable imitation learning skills; and (2)
trainingandtestingtime.ACTfailsentirelyforlong-horizon WildLMa-Planner, an interface that enables interactions be-
tasks when trained directly on a few sequences of demon- tween imitation learning skills and LLM planner to support
strations. On the other hand, WildLMa successfully learns long-horizon task execution. Furthermore, we deploy this
generalizableskillsfromalimitednumberofdemonstrations frameworkonaquadrupedrobotcontrolledbyawhole-body
to achieve better success rates for long-horizon execution. controller, which allows us to efficiently collect demonstra-
A4. Pre-trained Visual Backbones improve skill gen- tion data and support extended workspace for diverse tasks.
eralizability. We ablate the choice of visual backbones In summary, WildLMa implements practical, generalizable
in Tab. III. CLIP [45] is the simple application of CLIP skills, and long-horizon manipulation, which we hope will
features without cross-attentions. While different backbones motivate future research toward in-the-wild mobile manipu-
performsimilarlyintheI.D.setting,weseethatfrozenlarge lation that facilitates real-world deployment of robots.REFERENCES [19] J. Gu, D. S. Chaplot, H. Su, and J. Malik, ‚ÄúMulti-
[1] M. Ahn et al., ‚ÄúDo as i can, not as i say: Ground- skill mobile manipulation for object rearrangement,‚Äù
ing language in robotic affordances,‚Äù arXiv preprint
inTheEleventhInternationalConferenceonLearning
arXiv:2204.01691, 2022. Representations, 2023.
[2] E. Arcari et al., ‚ÄúBayesian multi-task learning mpc [20] Q. Gu et al., ‚ÄúConceptgraphs: Open-vocabulary 3d
for robotic mobile manipulation,‚Äù IEEE Robotics and scene graphs for perception and planning,‚Äù in ICRA,
Automation Letters, 2023. 2024.
[3] C. D. Bellicoso et al., ‚ÄúAlma-articulated locomotion [21] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, ‚ÄúOpen-
vocabulary object detection via vision and
and manipulation for a torque-controllable robot,‚Äù in
2019 International conference on robotics and au- language knowledge distillation,‚Äù arXiv preprint
tomation (ICRA), 2019. arXiv:2104.13921, 2021.
[22] H. Ha, Y. Gao, Z. Fu, J. Tan, and S. Song,
[4] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tul-
‚ÄúUmi on legs: Making manipulation policies mobile
siani,‚ÄúTrack2act:Predictingpointtracksfrominternet
with manipulation-centric whole-body controllers,‚Äù in
videos enables diverse zero-shot robot manipulation,‚Äù
in ECCV, 2024. CoRL, 2024.
[23] T. He et al., ‚ÄúOmnih2o: Universal and dexterous
[5] M. Bojarski, ‚ÄúEnd to end learning for self-driving
cars,‚Äù in arXiv preprint arXiv:1604.07316, 2016. human-to-humanoid whole-body teleoperation and
[6] M. Chang et al., ‚ÄúGoat: Go to any thing,‚Äù in RSS, learning,‚Äù in CoRL, 2024.
[24] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch,
2024.
‚ÄúLanguage models as zero-shot planners: Extracting
[7] K.Chen,B.T.Lopez,A.-a.Agha-mohammadi,andA.
actionable knowledge for embodied agents,‚Äù in Inter-
Mehta, ‚ÄúDirect lidar odometry: Fast localization with
dense point clouds,‚Äù IEEE Robotics and Automation national conference on machine learning, 2022.
Letters, 2022. [25] N. Hughes, Y. Chang, and L. Carlone, ‚ÄúHydra:
[8] A.-C. Cheng et al., ‚ÄúSpatialrgpt: Grounded spatial A real-time spatial perception system for 3d scene
reasoning in vision language model,‚Äù arXiv preprint graph construction and optimization,‚Äù arXiv preprint
arXiv:2406.01584, 2024. arXiv:2201.13360, 2022.
[26] A. Iyer et al., ‚ÄúOpen teach: A versatile teleopera-
[9] X. Cheng, J. Li, S. Yang, G. Yang, and X. Wang,
tion system for robotic manipulation,‚Äù arXiv preprint
‚ÄúOpen-television:Teleoperationwithimmersiveactive
visual feedback,‚Äù in CoRL, 2024. arXiv:2403.07870, 2024.
[27] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, ‚ÄúGraspsplats:
[10] X. Cheng, K. Shi, A. Agarwal, and D. Pathak, ‚ÄúEx-
treme parkour with legged robots,‚Äù in ICRA, 2024. Efficient manipulation with 3d feature splatting,‚Äù in
[11] C. Chi et al., ‚ÄúDiffusion policy: Visuomotor policy CoRL, 2024.
learning via action diffusion,‚Äù in RSS, 2023. [28] A.Kirillovetal.,‚ÄúSegmentanything,‚ÄùinProceedings
of the IEEE/CVF International Conference on Com-
[12] C. Chi et al., ‚ÄúUniversal manipulation interface: In-
puter Vision, 2023.
the-wild robot teaching without in-the-wild robots,‚Äù
in RSS, 2024. [29] T.Lewetal.,‚ÄúRobotictablewipingviareinforcement
[13] R. Ding et al., ‚ÄúBunny-visionpro: Real-time bimanual learning and whole-body trajectory optimization,‚Äù in
2023 IEEE International Conference on Robotics and
dexterous teleoperation for imitation learning,‚Äù arXiv
preprint arXiv:2407.03162, 2024. Automation (ICRA), 2023.
[30] Z. Liang, Y. Mu, H. Ma, M. Tomizuka, M. Ding,
[14] H.-S. Fang, C. Wang, M. Gou, and C. Lu, ‚ÄúGraspnet-
and P. Luo, ‚ÄúSkilldiffuser: Interpretable hierarchical
1billion: A large-scale benchmark for general object
grasping,‚Äù in CVPR, 2020. planning via skill abstractions in diffusion-based task
execution,‚Äù in CVPR, 2024.
[15] Z. Fu, X. Cheng, and D. Pathak, ‚ÄúDeep whole-body
[31] M. Liu et al., ‚ÄúVisual whole-body control for legged
control: Learning a unified policy for manipulation
and locomotion,‚Äù in Conference on Robot Learning, loco-manipulation,‚Äù in CoRL, 2024.
[32] P. Liu, Y. Orru, C. Paxton, N. M. M. Shafiullah, and
2023.
L.Pinto,‚ÄúOk-robot:Whatreallymattersinintegrating
[16] Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C.
open-knowledge models for robotics,‚Äù arXiv preprint
Finn, ‚ÄúHumanplus: Humanoid shadowing and imita-
tion from humans,‚Äù in CoRL, 2024. arXiv:2401.12202, 2024.
[33] P. Liu et al., ‚ÄúDynamem: Online dynamic spatio-
[17] Z. Fu, T. Z. Zhao, and C. Finn, ‚ÄúMobile aloha:
semantic memory for open world mobile manipula-
Learningbimanualmobilemanipulationwithlow-cost
whole-body teleoperation,‚Äù arXiv, 2024. tion,‚Äù arXiv preprint arXiv:2411.04999, 2024.
[34] S. Liu et al., ‚ÄúGrounding dino: Marrying dino with
[18] C. R. Garrett, T. Lozano-Pe¬¥rez, and L. P. Kaelbling,
grounded pre-training for open-set object detection,‚Äù
‚ÄúPddlstream:Integratingsymbolicplannersandblack-
boxsamplersviaoptimisticadaptiveplanning,‚ÄùarXiv, arXiv preprint arXiv:2303.05499, 2023.
[35] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter,
2020.
‚ÄúCombining learning-based locomotion policy withmodel-based manipulation for legged mobile manipu- [54] Stretch open source mobile manipulator - hello robot,
lators,‚Äù IEEE Robotics and Automation Letters, 2022. https://hello-robot.com/stretch-3-
[36] D. Maggio et al., ‚ÄúClio: Real-time task-driven product, Accessed: 2024-09-01.
open-set 3d scene graphs,‚Äù arXiv preprint [55] C.Sunetal.,‚ÄúFullyautonomousreal-worldreinforce-
arXiv:2404.13696, 2024. ment learning with applications to mobile manipula-
[37] OpenAI, ‚ÄúGpt-4 technical report,‚Äù OpenAI, Tech. tion,‚Äù in Conference on Robot Learning, 2022.
Rep., 2023. [56] J. Wei et al., ‚ÄúChain-of-thought prompting elicits
[38] M. Oquab et al., ‚ÄúDinov2: Learning robust vi- reasoning in large language models,‚Äù NeurIPS, 2022.
sual features without supervision,‚Äù arXiv preprint [57] J. Wong et al., ‚ÄúError-aware imitation learning from
arXiv:2304.07193, 2023. teleoperation data for mobile manipulation,‚Äù in Con-
[39] A. Padalkar et al., ‚ÄúOpen x-embodiment: Robotic ference on Robot Learning, 2022.
learning datasets and rt-x models,‚Äù arXiv preprint [58] J. Wu et al., ‚ÄúTidybot: Personalized robot assistance
arXiv:2310.08864, 2023. with large language models,‚Äù Autonomous Robots,
[40] G. Pan et al., ‚ÄúRoboduet: A framework affording 2023.
mobile-manipulation and cross-embodiment,‚Äù arXiv [59] P. Wu, Y. Shentu, Z. Yi, X. Lin, and P. Abbeel,
preprint arXiv:2403.17367, 2024. ‚ÄúGello:Ageneral,low-cost,andintuitiveteleoperation
[41] D. A. Pomerleau, ‚ÄúAlvinn: An autonomous land ve- framework for robot manipulators,‚Äù arXiv preprint
hicle in a neural network,‚Äù in Advances in neural arXiv:2309.13037, 2023.
information processing systems, 1988. [60] F. Xia, C. Li, R. Mart¬¥ƒ±n-Mart¬¥ƒ±n, O. Litany, A. To-
[42] T. Portela, G. B. Margolis, Y. Ji, and P. Agrawal, shev, and S. Savarese, ‚ÄúRelmogen: Integrating motion
‚ÄúLearning force control for legged manipulation,‚Äù in generation in reinforcement learning for mobile ma-
ICRA, 2024. nipulation,‚Äù in 2021 IEEE International Conference
[43] Y. Qin et al., ‚ÄúAnyteleop: A general vision-based on Robotics and Automation (ICRA), 2021.
dexterousrobotarm-handteleoperationsystem,‚ÄùarXiv [61] H. Xiong, R. Mendonca, K. Shaw, and D. Pathak,
preprint arXiv:2307.04577, 2023. ‚ÄúAdaptive mobile manipulation for articulated objects
[44] R.-Z. Qiu et al., ‚ÄúLearning generalizable feature in the open world,‚Äù arXiv preprint arXiv:2401.14403,
fields for mobile manipulation,‚Äù arXiv preprint 2024.
arXiv:2403.07563, 2024. [62] W. Xu and F. Zhang, ‚ÄúFast-lio: A fast, robust lidar-
[45] A. Radford et al., ‚ÄúLearning transferable visual mod- inertial odometry package by tightly-coupled iterated
els from natural language supervision,‚Äù in ICML, kalmanfilter,‚ÄùIEEERoboticsandAutomationLetters,
PMLR, 2021. 2021.
[46] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. [63] R.Yangetal.,‚ÄúGeneralizedanimalimitator:Agilelo-
Reid,andN.Suenderhauf,‚ÄúSayplan:Groundinglarge comotion with versatile motion prior,‚Äù arXiv preprint
language models using 3d scene graphs for scalable arXiv:2310.01408, 2023.
robot task planning,‚Äù in CoRL, 2023. [64] S. Yang et al., ‚ÄúAce: A cross-platform visual-
[47] Ros moveit motion planning framework, https:// exoskeletons system for low-cost dexterous teleoper-
moveit.ros.org/, Accessed: 2024-09-13. ation,‚Äù in CoRL, 2024.
[48] N.M.M.Shafiullahetal.,‚ÄúOnbringingrobotshome,‚Äù [65] S. Yenamandra et al., ‚ÄúHomerobot: Open-
arXiv preprint arXiv:2311.16098, 2023. vocabulary mobile manipulation,‚Äù arXiv preprint
[49] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, arXiv:2306.11565, 2023.
and P. Isola, ‚ÄúDistilled feature fields enable few-shot [66] N.Yokoyamaetal.,‚ÄúAsc:Adaptiveskillcoordination
language-guided manipulation,‚Äù in CoRL, 2023. for robotic mobile manipulation,‚Äù IEEE Robotics and
[50] J.-P. Sleiman, F. Farshidian, and M. Hutter, ‚ÄúVersatile Automation Letters, 2023.
multicontact planning and control for legged loco- [67] J. Zhang et al., ‚ÄúGamma: Graspability-aware mobile
manipulation,‚Äù Science Robotics, 2023. manipulationpolicylearningbasedononlinegrasping
[51] C.H.Song,J.Wu,C.Washington,B.M.Sadler,W.-L. pose fusion,‚Äù arXiv preprint arXiv:2309.15459, 2023.
Chao, and Y. Su, ‚ÄúLlm-planner: Few-shot grounded [68] K. Zhang, B. Li, K. Hauser, and Y. Li, ‚ÄúAdaptigraph:
planning for embodied agents with large language Material-adaptive graph-based neural dynamics for
models,‚Äù in ICCV, 2023. robotic manipulation,‚Äù in RSS, 2024.
[52] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever, [69] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn,‚ÄúLearn-
and R. Salakhutdinov, ‚ÄúDropout: A simple way to ing fine-grained bimanual manipulation with low-cost
preventneuralnetworksfromoverfitting,‚ÄùThejournal hardware,‚Äù in arXiv preprint arXiv:2304.13705, 2023.
of machine learning research, 2014. [70] T. Z. Zhao et al., ‚ÄúAloha unleashed: A simple recipe
[53] S.Srivastava,E.Fang,L.Riano,R.Chitnis,S.Russell, for robot dexterity,‚Äù in CoRL, 2024.
and P. Abbeel, ‚ÄúCombined task and motion planning [71] P. Zhi et al., ‚ÄúClosed-loop open-vocabulary mo-
through an extensible planner-independent interface bile manipulation with gpt-4v,‚Äù arXiv preprint
layer,‚Äù in ICRA, 2014. arXiv:2404.10220, 2024.[72] C. Zhou, C. Peers, Y. Wan, R. Richardson, and D.
Kanoulas, ‚ÄúTeleman: Teleoperation for legged robot
loco-manipulation using wearable imu-based motion
capture,‚Äù arXiv preprint arXiv:2209.10314, 2022.
[73] C. Zhou, C. C. Loy, and B. Dai, ‚ÄúExtract free dense
labels from clip,‚Äù in ECCV, 2022.
[74] S. Zimmermann, R. Poranne, and S. Coros, ‚ÄúGo
fetch!-dynamic grasps using boston dynamics spot
withexternalroboticarm,‚Äùin2021IEEEInternational
Conference on Robotics and Automation (ICRA),
2021.