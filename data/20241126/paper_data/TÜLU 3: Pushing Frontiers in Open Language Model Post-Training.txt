T√úLU 3: Pushing Frontiers in
Open Language Model Post-Training
NathanLambert‚ô•ùíÇ JacobMorrison‚ô•ùíÇ ValentinaPyatkin‚ô•ùíÇùíò ShengyiHuang‚ô•ùíÇ HamishIvison‚ô•ùíÇùíò FaezeBrahman‚ô•ùíÇ
LesterJamesV.Miranda‚ô•ùíÇ AlisaLiuùíò NouhaDziriùíÇ XinxiLyuùíÇ
YulingGuùíÇ SaumyaMalikùíÇ VictoriaGrafùíò JenaD.HwangùíÇ
JiangjiangYangùíÇ RonanLeBrasùíÇ OyvindTafjordùíÇ ChrisWilhelmùíÇ
LucaSoldainiùíÇ NoahA.SmithùíÇùíò YizhongWangùíÇùíò PradeepDasigiùíÇ HannanehHajishirziùíÇùíò
ùíÇAllenInstituteforAI ùíòUniversityofWashington
tulu@allenai.org
Abstract
Language model post-training is applied to refine behaviors and unlock new skills across a
widerangeofrecentlanguagemodels,butopenrecipesforapplyingthesetechniqueslagbehind
proprietaryones. Theunderlyingtrainingdataandrecipesforpost-trainingaresimultaneouslythe
mostimportantpiecesofthepuzzleandtheportionwiththeleasttransparency. Tobridgethisgap,
weintroduceT√úLU3,afamilyoffully-openstate-of-the-artpost-trainedmodels,alongsideitsdata,
code,andtrainingrecipes,servingasacomprehensiveguideformodernpost-trainingtechniques.
T√úLU3,whichbuildsonLlama3.1basemodels,achievesresultssurpassingtheinstructversions
of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude
3.5-Haiku. Thetrainingalgorithmsforourmodelsincludesupervisedfinetuning(SFT),Direct
Preference Optimization (DPO), and a novel method we call Reinforcement Learning with
VerifiableRewards(RLVR).WithT√úLU 3,webuildamulti-taskevaluationschemeforpost-
trainingwithdevelopmentandunseenevaluations,standardbenchmarkimplementations,and
substantialdecontaminationofexistingopendatasetsonsaidbenchmarks. Weconcludewith
analysisanddiscussionoftrainingmethodsthatdidnotreliablyimproveperformance.
The T√úLU 3 release includes model weights, a demo, and the complete recipe ‚Äî datasets
fordiversecoreskills,arobusttoolkitfordatacurationandevaluation,thetrainingcodeand
infrastructure,and,mostimportantly,adetailedreportforreproducingandfurtheradaptingthe
T√úLU3approachtomoredomains.
T√úLU370B https://hf.co/allenai/Llama-3.1-Tulu-3-70B
T√úLU38B https://hf.co/allenai/Llama-3.1-Tulu-3-8B
T√úLU3DATA https://hf.co/collections/allenai/tulu-3-datasets-
673b8df14442393f7213f372
T√úLU3Code https://github.com/allenai/open-instruct
T√úLU3EVAL https://github.com/allenai/olmes
Demo https://playground.allenai.org/
‚ô•Corecontributors.
4202
voN
22
]LC.sc[
1v42151.1142:viXraContents
1 Introduction 5
2 T√úLU 3Overview 6
2.1 T√úLU 3Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 T√úLU 3Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 T√úLU 3Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.4 EvaluationandResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3 T√úLU 3Data 10
3.1 PromptCuration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.1 SourcingfromPublicDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1.2 SynthesizingforTargetSkills . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2 PromptDecontamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 SupervisedFinetuning 13
4.1 SFTData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.1 FromPromptstoSFTData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.1.2 TheT√úLU 3SFTMix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 KeyDataExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.3 SFTRecipeandAnalyses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.3.1 KeyTrainingExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.3.2 BatchAggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5 PreferenceFinetuning 19
5.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.1.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.1.2 PolicyOptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.2 T√úLU3PreferenceData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2.1 FromPromptstoPreferenceData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2.2 TheT√úLU3PreferenceMix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.3 KeyFindingsofDataAblations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.4 PreferenceTuningRecipeandAnalyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.4.1 HyperparameterandAlgorithmDesign . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.4.2 InfrastructureforScalingDPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
6 ReinforcementLearningonVerifiableRewards 28
6.1 RLVRData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
6.2 RLVRRecipeandAnalyses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.2.1 KeyFindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.3 RLVRInfrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6.4 FinalExperimentalResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
7 T√úLU 3EvaluationFramework 34
7.1 OpenLanguageModelEvaluationSystem(OLMES) . . . . . . . . . . . . . . . . . . . . . . . . 34
27.2 T√úLU 3EvaluationSuite-Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
7.2.1 SafetyEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
7.3 T√úLU 3EvaluationSuite-Unseen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
7.3.1 NewEvaluation: IFEval-OOD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
7.3.2 NewEvaluation: HREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
7.4 EvaluationontheUnseenSuite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
8 Discussions 42
8.1 InsightsfromtheUnfruitful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
8.2 FutureWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
9 RelatedWork 43
9.1 TheEvolutionofPost-trainingRecipes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
9.2 TrainingonVerifiableRewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
10 Conclusion 44
A AdditionalHyperparameters 54
B AdditionalDatasetAnalyses 54
B.1 ExtraDistributionPlots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
B.2 ContaminationinPublicDatasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
B.3 ChatTemplateImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
B.4 RLVRIFEvaloveroptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
C SupervisedFinetuningDataDetails 55
C.1 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
D PreferenceTuningDataDetails 55
E AdditionalRLVRDetails 55
E.1 RMTrainingHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
F EvaluationDetails 56
F.1 DetailedSafetyResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
F.2 Evaluationprinciples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
F.3 IFEvalOut-of-DistributionConstraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
F.4 Subtask-levelbreakdownofHREFresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
3Table1: Models,datasets,andcodereleasedwithT√úLU3.
Demo: https://playground.allenai.org/
ModelCheckpoints
Stage Llama3.18B Llama3.170B
BaseModel meta-llama/Llama-3.1-8B meta-llama/Llama-3.1-70B
SFT allenai/Llama-3.1-Tulu-3-8B-SFT allenai/Llama-3.1-Tulu-3-70B-SFT
DPO allenai/Llama-3.1-Tulu-3-8B-DPO allenai/Llama-3.1-Tulu-3-70B-DPO
FinalModels allenai/Llama-3.1-Tulu-3-8B allenai/Llama-3.1-Tulu-3-70B
(RLVR) RM:allenai/Llama-3.1-Tulu-3-8B-RM
Codebases/Tools
Type Link
Training allenai/open-instruct
T√úLU3EVAL allenai/olmes
Decontamination allenai/open-instruct/tree/main/decontamination
PreferenceDataInference allenai/birr
InstructionDatasets
Type Domain Link
Fullmix General allenai/tulu-3-sft-mixture
PreciseInstruction
TaskSpecific Following allenai/tulu-3-sft-personas-instruction-following
Subsets MATH allenai/tulu-3-sft-personas-math
GradeSchoolMath allenai/tulu-3-sft-personas-math-grade
PythonCode allenai/tulu-3-sft-personas-code
PreferenceMixes
Model Link
Llama3.170B allenai/llama-3.1-tulu-3-70b-preference-mixture
Llama3.18B allenai/llama-3.1-tulu-3-8b-preference-mixture
SpecificPreferenceDatasets
Domain Link
PreciseInstructionFollowing allenai/tulu-3-pref-personas-instruction-following
General allenai/tulu-3-sft-prompts-ultrafeedback
General allenai/tulu-3-wildchat-ultrafeedback
RLwithVerifiableRewardsTrainingDatasets
Domain Link
FullMix allenai/RLVR-GSM-MATH-IF-Mixed-Constraints
GSMOnly allenai/RLVR-GSM
MATHOnly allenai/RLVR-MATH
IFevalOnly allenai/RLVR-IFeval
4Curate prompts Supervised finetuning Direct pref. optimization RL with verifiable rewards
on-policy data prompts with
public datasets data mixing
oÓÄá-policy data verifiable rewards
persona-driven
synthetic instructions
Base Model T√ºlu3-SFT T√ºlu3-DPO T√ºlu3
decontaminate
knowledge reasoning math
development evals
coding chat safety Build evaluation suite
unseen evals
Identify core skills
Figure1: AnoverviewoftheT√úLU3recipe. Thisincludes: datacurationtargetinggeneralandtargetcapabilities,
trainingstrategiesandastandardizedevaluationsuitefordevelopmentandfinalevaluationstage.
1 Introduction
‚ÄúJustasthecamelsharesitsburdenswithothersinthecaravan,thewisesharetheirinsightstolightentheloadof
ignorance.‚Äù ‚ÄìProverbgeneratedbyT√úLU3.
Post-training‚Äîthecollectionoftechniquesincludinginstructiontuning, reinforcementlearningfromhuman
feedback,andothertypesoffinetuning‚Äîhasbecomeacrucialstepinbuildingfrontierlanguagemodels[OpenAI,
2024,Anthropic,2024],yetdevelopmentstothesetechniquesarefrequentlynotaccompaniedbyopenresources
andrecipes. Fullyopensourcecounterparts(e.g.,T√úLU2[Ivisonetal.,2023]andZephyr-ùõΩ [Tunstalletal.,2023])
oftenrelyonsimpler-to-implementandcheaperpipelinesandhavebecomeoutdatedonmanymetrics.
Toclosethegapbetweenopenandclosedposttraining,weintroduceT√úLU3,afamilyofopenstate-of-the-art
post-trainedmodels,alongsideallofthedata,trainingrecipes,code,infrastructure,andevaluationframework.
Integrating partial details from proprietary methods with novel techniques and established academic research,
T√úLU3pushestheboundariesofresearchinpost-training. TheadvancementsofT√úLU3areattributedtoT√úLU3
DATA,newpermissivelylicensedtrainingdatasetstargetingcoreskills,T√úLU3EVAL,evaluationsuiteandtools
to establish clear performance goals and guide improvement through training stages, and T√úLU 3 RECIPE, an
advancedmulti-stagetrainingpipelineincorporatingnewalgorithmicadvancementsinreinforcementlearning,
cutting-edgeinfrastructure,andrigorousexperimentationtooptimizedatamixes,methods,andparametersacross
varioustrainingstages.
InordertobuildT√úLU3,weidentifyasetofcoreskillstoimproveaftertraining(e.g.,reasoning,math,coding,
safety,preciseinstructionfollowing,knowledgerecall,etc.) andbuildanevaluationframeworktoestablishclear
performance goals and guide model improvement over a selection of development and unseen tasks. T√úLU 3
benefitssignificantlyfromleveragingpubliclyavailableopendata,generatingdiverse,skill-specificsyntheticdata
atvarioustrainingstages,andaggressivelydecontaminatingthemagainstourevaluationsuite.
TheT√úLU3trainingrecipeinvolvesmultiplestages,witheachstagebuildinguponthepreviousmodelandfocusing
on different types of data ‚Äî namely, prompt-completion instances for supervised finetuning, preferences for
preferencetuning,orverifiablerewardsforreinforcementlearning. Ourmethodologyfacilitatesidentifyingskill
deficienciesandrefiningthedatamix,methodsandparameters,ensuringabalancedperformanceofcoreskills
acrossthetrainingprocess. Throughrigorous,principledexperimentation,wedeterminethebestdatamixfor
supervisedfinetuning,resultingintheT√úLU3SFTcheckpoint. Leveragingrecentadvancesinpreferencetuning,we
thentrainamodelovercarefullycuratedon-policypreferencedatafromcomparingT√úLU3SFTcompletionsagainst
outputsfromotherlanguagemodels. Furthermore,weintroduceanewfinalfinetuningstage‚ÄìReinforcement
LearningwithVerifiableRewards(RLVR)-whichemploysanovelRLobjectivetailoredtoenhancespecificskills
withverifiableanswers,suchasmathematicsandpreciseinstructionfollowing.
OurbestperformingrecipeyieldsT√úLU3modelsthatoutperformthestate-of-the-artpost-trainedopen-weight
modelsofthesamesizesuchasLlama3.1Instruct[Dubeyetal.,2024],Qwen2.5Instruct[QwenTeam,2024],or
Mistral-Instruct[MistralAI,2024],andatthelarge70BsizeT√úLUmatchestheofferingsofclosedproviderssuch
asClaude3.5HaikuandGPT-4omini.
5Qwen Llama Qwen Llama Claude
Skill Benchmark(eval) T√ú 8L BU3 2.57B 3.18B T√ú 70L BU3 2.572B 3.170B G TP uT r- b3 o.5 G MPT in-4 io 3.5
Instruct Instruct Instruct Instruct Haiku
Avg. 64.8 57.8 62.2 76.0 71.5 73.4 64.7 69.6 75.3
Knowledge MMLU (0shot,CoT) 68.2 76.6 71.2 83.1 85.5 85.3 70.2 82.2 81.8
PopQA (15shot) 29.1 18.1 20.2 46.5 30.6 46.4 45.0 39.0 42.5
TruthfulQA (6shot) 55.0 63.1 55.1 67.6 69.9 66.8 62.9‚ô¢ 64.8‚ô¢ 64.9‚ô¢
Reasoning BigBenchHard (3shot,CoT) 66.0 21.7 62.8 82.0 67.2 73.8 66.6‚ä§ 65.9‚ô¢ 73.7‚ä§
DROP (3shot) 62.6 54.4 61.5 74.3 34.2 77.0 70.2 36.3 78.4
Math MATH (4shotCoT,Flex) 43.7 14.8 42.5 63.0 74.3 56.4 41.2 67.9 68.0
GSM8K (8shot,CoT) 87.6 83.8 83.4 93.5 89.5 93.7 74.3 83.0 90.1
Coding HumanEval (pass@10) 83.9 93.1 86.3 92.4 94.0 93.6 87.1 90.4 90.8
HumanEval+ (pass@10) 79.2 89.7 82.9 88.0 90.8 89.5 84.0 87.0 88.1
IF&chat IFEval (promptloose) 82.4 74.7 80.6 83.2 87.6 88.0 66.9 83.5 86.3
AlpacaEval2 (LC%win) 34.5 29.0 24.2 49.8 47.7 33.4 38.7 49.7 47.3
Safety Safety (6taskavg.) 85.5 75.0 75.2 88.3 87.0 76.5 69.1 84.9 91.8
Table2: OverviewofresultsonT√úLU3EVALsuite,overboth8Band70Bmodels. Thebestperformingmodel
foreachmodelsizeoneachbenchmarkisbolded. T√úLU3outperformsthestate-of-the-artpost-trainedopen-weight
modelsofthesamesizeandsurpassClaudeHaiku,GPT-3.5Turbo,andGPT-4oMini.
‚ä§indicatesscorestakenfromClaude3ModelCardandClaude3.5ModelCardAddendum.
‚ô¢indicatesscoreinterpolatedwithMultipleImputationbyChainedEquations(MICE)withcontextofallotherscoresinthe
table,exceptaverages.Thesescoreswereeithersubjecttosubstantialformattingerrorsinourevaluationsuiteornotfoundin
othermajortechnicalreports.InstructversionsofmodelsshortenedtoInst.
Closedmodelversions:GPT-3.5-Turbo-0125,GPT-4o-mini-2024-07-18,Claude3.5Haiku20241022
Insummary,T√úLU3representsafamilyofstate-of-the-artopenlanguagemodels,featuringamodernpost-training
frameworkwithfullyopen-sourcedataT√úLU3DATA,evaluationT√úLU3EVAL,trainingcodeT√úLU3CODEand
developmentrecipesT√úLU3RECIPE. HereareafewkeycontributionsfromthedevelopmentofT√úLU:
‚Ä¢ Extensiveguidanceandtoolingforevaluation,decontamination,andrecipedesign,
‚Ä¢ Scaled,newsyntheticinstructiondatasets,
‚Ä¢ Scalingpreferencedatawithon-policygenerations,
‚Ä¢ Reinforcementlearningwithverifiablerewards,anRL-basedmethodthatonlygetsarewardifthemodel‚Äôs
completionsareverifiedtobecorrect,
‚Ä¢ Advancedinfrastructure,details,andcodetofacilitatethesuccessfulimplementationoflargemodels
Theresultofourworkiscompletelyopenpipelinesforfinetuninglanguagemodels. Wereleasefinalmodelstrained
onLlama3.1baseversions[Dubeyetal.,2024],withintermediatecheckpoints,trainingdata,trainingcode,and
evaluationcode(afulllistofartifactsreleasedisavailableinTable1). Withallthereleasedresources,otherscan
takeopenbasemodelsandfinetunethemtohigh-performanceonanytaskofinterest‚Äìlayingthefoundationof
post-trainingresearchwithincomplex,multi-objectiveandmulti-stagetrainingregimes.
2 T√úLU 3Overview
Early work in language model post-training followed a standard recipe pioneered by models like Instruct-
GPT[Ouyangetal.,2022],consistingofinstruction-tuningfollowedbypreferencefinetuning(PreFT)[Stiennon
etal.,2020,Nakanoetal.,2021,Askelletal.,2021,Ouyangetal.,2022]. Sincethen,thesophisticationandcom-
plexityofpost-trainingapproacheshavecontinuedtoincrease,movingtowardsmultipleroundsoftraining,human
dataplussyntheticdata,andmultipletrainingalgorithmsandobjectives[Touvronetal.,2023,Dubeyetal.,2024,
Gunteretal.,2024]. However,mostsuccessfulpost-trainingmodelsofferlimitedinformationabouttheirtraining
data,code,orrecipes.1 Openpost-trainingresearch,suchasT√úLU2[Ivisonetal.,2023]andZephyr-ùõΩ [Tunstall
etal.,2023],showstrongresultsinsomebenchmarksandonchatevaluationssuchasAlpacaEvalorArena-Hard[Li
etal.,2024a],butstilllagbehindincorecapabilitiessuchasMATH[Hendrycksetal.,2021],IFEval[Zhouetal.,
2023]andGSM8K[Cobbeetal.,2021].
1OnLMSYS‚ÄôsChatBotArena,nomodelinthetop50(asofNovember20th,2024)hasreleaseditspost-trainingdata[Chiang
etal.,2024].
6CoreSkill Development Unseen
Knowledge MMLU MMLU-Pro
(em) (em)
PopQA GPQA
(EM) (em)
TruthfulQA
(MC2em)
Reasoning BigBenchHard AGIEvalEnglish
(em) (em)
DROP
(F1)
Math MATH DeepmindMathematics
(flexem) (em)
GSM8K
(em)
Coding HumanEval BigcodeBench
(Pass@10) (Pass@10)
HumanEval+
(Pass@10)
InstructionFollowing(IF) IFEval IFEval-OOD
(em) (Pass@1)
AlpacaEval2 HREF
(winrate) (winrate)
Safety T√úLU3Safety
(avg*)
Table3: T√úLU3EVALconsistsofdevelopmentandunseensplitstoevaluatecoreskills. WithT√úLU3EVAL,we
releaseaunifiedstandardizedevaluationsuiteandatoolkittodecontaminatetrainingdataagainstbenchmarks. The
subscriptshowsthemetricweuseforevaluation. T√úLU3Safetyisacollectionofsafetyevaluations,seeSec.7.2.1
fordetails.
T√úLU3pushestheboundariesofresearchinpost-trainingandclosesthegapbetweenopenandclosedfinetuning
recipes. WithT√úLU3,wehopetouncoverwhichpathsfortheopen-sourcecommunitywillleadtosuccessand
whichdonot(byreportingnegativeresults). Itisacomplextrainingprocessthatintegratespartialdetailsfrom
proprietarymethodswithnoveltechniquesandestablishedacademicresearch. Thekeyfactorsinthesuccessof
mdelname3arerootedincarefuldatacuration,rigorousexperimentationandevaluation,innovativemethodologies,
andimprovedtraininginfrastructure. Wefollowedsystematicguidelinesbyscientificallyevaluatingthisprocess
throughcreatingdevelopmentandtestsetsforevaluation,andconductcarefuldecontaminationofpubliclyavailable
datasets.
T√úLU3isnotjustanartifact,butacomprehensivesuiteofdataandtoolsdesignedtoadvancethefrontier
ofopenpost-training. Byopenlysharingourdata,recipeandfindings,weaimtoempowerthecommunityto
explorenewandinnovativepost-trainingapproaches. WelisttheextensiveartifactsandtoolsreleasedinTable1.
2.1 T√úLU3Data
CoreSkillsTheT√úLU3effortbeganwithidentifyingkeyareaswhereopenpost-trainingrecipesoftenfallbehind
andthataredesirablecapabilitiesforgeneralistlanguagemodels. Table3outlinesthecorecapabilitiesweaimto
enhanceandtheevaluationbenchmarksselectedtocovertheseskills. WithT√úLU3,wefocusoncoreskillsof
knowledgerecall,reasoning,mathematics,coding,instructionfollowing,generalchat,andsafety.
WecurateandcollectT√úLU 3DATA totargetthesecoreskillsbysourcingfrompublicdataandsynthetically
curatingdata. Weusevariousdataformatsatdifferentstagesoftraining. Table6outlinesthecollectionofdatasets
usedtotrainourmodel,andfurtherdetailsareprovidedinSection¬ß3.
2.2 T√úLU3Evaluation
Akeyfactorinthesuccessofourpost-trainingapproachisestablishingclearperformancegoalsandevaluation
toolstoguideimprovementthroughthesestages. WithT√úLU3EVAL,wereleaseaunifiedstandardizedevaluation
suiteandatoolkittoguidethedevelopmentandassessingfinalmodelsanddecontaminatingtrainingdataagainst
evaluationbenchmarks.
Our framework consists of an open evaluation toolkit for reproducible evaluations (Section 7.1), a suite for
evaluatingcoreskillsininstruction-tunedmodelswithseparatedevelopment(Section7.2)andheld-outevaluations
(Section7.3),andasetofrecommendedsettingsforevaluatingonourevaluationsuitebasedonourexperiments
withvariousmodels. Bothsplitscoverallidentifiedskills. Crucially,wedidnotexaminescoresonourunseenset
7Llama3.1 Qwen2.5 Gemma2 Ministral
T√úLU3 T√úLU3 T√úLU3 Magpie
Benchmark 8B 7B 9B 8B
(eval) 8BSFT 8BDPO 8B 8B
Instruct Instruct Instruct Instruct
Avg. 60.4 64.4 64.8 62.2 57.8 44.7 55.2 58.3
MMLU 65.9 68.7 68.2 71.2 76.6 62.0 74.6 68.5
(0shot,CoT)
PopQA 29.3 29.3 29.1 20.2 18.1 22.5 28.3 20.2
(15shot)
TruthfulQA 46.8 56.1 55.0 55.1 63.1 57.0 61.4 55.5
(6shot)
BigBenchHard 67.9 65.8 66.0 62.8 21.7 0.9 2.5 56.2
(3shot,CoT)
DROP 61.3 62.5 62.6 61.5 54.4 49.4 58.8 56.2
(3shot)
MATH 31.5 42.0 43.7 42.5 14.8 5.1 29.8 40.0
(4shotCoT,Flex)
GSM8K 76.2 84.3 87.6 83.4 83.8 61.2 79.7 80.0
(8shot,CoT)
HumanEval 86.2 83.9 83.9 86.3 93.1 75.4 71.7 91.0
(pass@10)
HumanEval+ 81.4 78.6 79.2 82.9 89.7 69.1 67.0 88.5
(pass@10)
IFEval 72.8 81.1 82.4 80.6 74.7 38.8 69.9 56.4
(promptloose)
AlpacaEval2 12.4 33.5 34.5 24.2 29.0 49.0 43.7 31.4
(LC%win)
Safety 93.1 87.2 85.5 75.2 75.0 46.4 75.5 56.2
(6taskavg.)
Table4: SummaryofT√úLU3resultsrelativetopeer8Bmodels. Thebest-performingmodeloneachbenchmark
(i.e.,ineachrow)isbolded. T√úLU3-8Bsignificantlyoutperformspriorstate-of-the-art8Bmodels. Progressacross
variouscheckpointshighlightthecontributionofeachstageofthetraininginimprovingcoreskills. Manyofthe
lowestvalues,suchasinBigBenchHard,arecausedbyfailingtofollowthefew-shotformattingrequiredforthe
evaluationorotherrepetitiveerrors‚Äìformoredetails,see¬ß7.
whendevelopingourmodels,allowingustoobservehowmuchwemayhaveoverfittoparticularevaluationsinour
decisionsarounddatamixtures,algorithms,andhyperparameters.
Table3summarizesourevaluationsuite. WeprovidefurtherdetailsonourevaluationsinSection7andinTable23.
Wepubliclyreleaseourevaluationsuiteathttps://github.com/allenai/olmes.
2.3 T√úLU3Recipe
Inthissection,weprovideanoverviewoftheT√úLU3recipetoobtainastate-of-the-artpost-trainedmodel. We
produceT√úLU3modelsthroughafour-stagepost-trainingrecipeontopofpretrainedlanguagemodels(seeFigure1).
TheT√úLU3RECIPEisanadvancedmulti-stagetrainingpipelineincorporatingnewalgorithmicadvancements
inreinforcementlearning,cutting-edgeinfrastructure,andrigorousexperimentationtocuratedataandoptimize
datamixes,methods,andparametersacrossvarioustrainingstages. Throughoutallstages,wemeasuremodel
performanceusingacarefully-chosenevaluationsuite. Thestagesareasfollows:
Stage1: DataCuration(¬ß3)Wecurateavarietyofpromptstobeallocatedacrossmultiplestagesofoptimization.
Wecreatenewsyntheticpromptsor,whenavailable,sourcepromptsfromexistingdatasetstotargetspecific
capabilities. Weensurepromptsarenotcontaminatedwithourevaluationsuite,T√úLU3EVAL.
Stage2: SupervisedFinetuning(¬ß4)Weperformsupervisedfinetuning(SFT)oncarefullyselectedpromptsand
completions. Withthoroughexperimentation,thefinalSFTdataandtraininghyperparametersaredeterminedto
enhancetargetcoreskillswithoutsignificantlyimpactingtheperformanceofothers,guidedbyourevaluation
framework.
Stage3: PreferenceTuning(¬ß5)Weapplypreferencetuning,specificallyDPO,tonewlycuratedon-policy
syntheticallycreatedpreferencedatafromselectedpromptsalongwithoff-policydata. AsintheSFTstage,
weidentifythebestpreferencedatamixthroughthoroughexperimentation,uncoveringwhatformatsofdata,
methods,orhyperparametersleadtoimprovements.
Stage4: ReinforcementLearningwithVerifiableRewards(¬ß6)WeintroduceanewRL-basedpost-training
stagewhichtrainsthemodelonverifiablerewardsinsteadofarewardmodel,asiscommonfortraditionalRLHF
PPOtraining. Weselecttaskswithverifiableoutcomes,suchasmathematicalproblem-solving,andonlyprovide
rewardswhenthemodel‚Äôsgenerationsareverifiedtobecorrect. WethenuseRLtotrainontheserewards.
The key contributions of our T√úLU 3 pipeline lie in improved data, methods, infrastructure, and rigorous
evaluation. Keyelementsofourpipelineinclude:
8Llama3.1 Qwen2.5 Hermes3 Nemotron
T√úLU3 T√úLU3 T√úLU3
Benchmark 70B 72B Llama3.1 Llama3.1
(eval) 70BSFT 70BDPO 70B
Instruct Instruct 70B 70B
Avg. 72.6 75.9 76.0 73.4 71.5 68.3 65.5
MMLU 78.9 83.3 83.1 85.3 85.5 80.4 83.8
(0shot,CoT)
PopQA 48.6 46.3 46.5 46.4 30.6 48.1 36.4
(15shot)
TruthfulQA 55.7 67.9 67.6 66.8 69.9 66.5 62.6
(6shot)
BigBenchHard 82.7 81.8 82.0 73.8 67.2 82.1 0.7
(3shot,CoT)
DROP 77.2 74.1 74.3 77.0 34.2 73.2 68.8
(3shot)
MATH 53.7 62.3 63.0 56.4 74.3 41.9 55.0
(4shotCoT,Flex)
GSM8K 91.1 93.5 93.5 93.7 89.5 90.0 84.7
(8shot,CoT)
HumanEval 92.9 92.4 92.4 93.6 94.0 89.6 94.1
(pass@10)
HumanEval+ 87.3 88.4 88.0 89.5 90.8 85.9 85.5
(pass@10)
IFEval 82.1 82.6 83.2 88.0 87.6 76.0 79.9
(promptloose)
AlpacaEval2 26.3 49.6 49.8 33.4 47.7 28.4 66.1
(LC%win)
Safety 94.4 89.0 88.3 76.5 87.0 57.9 69.0
(6taskavg.)
Table5: SummaryofT√úLU3resultsrelativetopeer70Bmodels. Thebest-performingmodeloneachbenchmark
(i.e.,ineachrow)isbolded. T√úLU3-8Bsignificantlyoutperformspriorstate-of-the-art70Bmodels. Progress
acrossvariouscheckpointshighlightthecontributionofeachstageofthetraininginimprovingcoreskills. Nemotron
Llama3.170Bistheonlymodelinthetablethatfine-tunedfromanotherpost-trainedmodel(inthiscaseLlama
3.170BInstruct),whiletheothersarefromtheirrespectivebasemodels. Manyofthelowestvalues,suchasin
BigBenchHard,arecausedbyfailingtofollowthefew-shotformattingrequiredfortheevaluationorotherrepetitive
errors‚Äìformoredetails,see¬ß7.
‚Ä¢ DataQuality,Provenance,andScale(¬ß3)Weobtainpromptsbycarefullysurveyingavailableopen-source
datasets,analyzingtheirprovenance,anddecontaminatingthem,aswellascuratingsyntheticpromptsthattarget
coreskills. Toensureeffectiveness,weconductthoroughexperimentstostudytheirimpactonourdevelopment
evaluationsuite. Wefindtargetedpromptstobeinfluentialtoimprovecoreskills,whilereal-worldqueries,
e.g.,WildChat[Zhaoetal.,2024],areimportanttoimprovegeneralchatcapabilities. UsingtheT√úLU3EVAL
decontaminationtool,weensurepromptsarenotcontaminatedagainstourevaluationsuite.2
‚Ä¢ CreatingaMulti-SkillSFTDataset(¬ß4.1)Thedistributionofthepromptsinthe‚Äúgeneral‚Äùand‚Äúskill-specific‚Äù
categories was refined by several rounds of supervised finetuning on various data mixtures. For example,
to improve mathematical reasoning, we first establish an upper bound in our evaluation suite by creating
math-specializedmodels,thenmixdatatobringthegeneralmodelsclosertothisupperbound.
‚Ä¢ CuratinganOn-PolicyPreferenceDataset(¬ß5.2)Wedevelopanon-policydatacurationpipelinetoscaleour
preferencedatasetgeneration. Concretely,wegeneratecompletionsfromT√úLU3-SFTandothermodelsfor
givenprompts,andobtainpreferencelabelsthroughtheirpairwisecomparisons. Ourapproachextendsand
improvestheoff-policypreferencedatagenerationmethodbyCuietal.[2023]. Carefulmulti-skillselectionof
preferencedatayields354,192instancesforpreferencetuningdemonstratingsignificantimprovementsina
rangeoftasks.
‚Ä¢ PreferenceTuningAlgorithmDesign(¬ß5.4)Weexperimentwithseveralpreferencetuningalgorithmsand
observeimprovedperformanceinusinglength-normalizedDirectPreferenceOptimization. Weprioritized
simplicityandefficiencyinexperimentationandusedlength-normalizedDPOthroughoutthedevelopment
processandtrainingourfinalmodels,inlieuofmorecostlyinvestigationsintoPPO-basedmethods.
‚Ä¢ Skill-SpecificRLwithVerifiableRewards(¬ß6)Weadaptanewapproach,leveragingastandardreinforcement-
learningparadigmtotargetskillsthatcanbeevaluatedagainstaground-truthoutcome(e.g.,Math). Wereferto
thisalgorithmasReinforcementLearningwithVerifiableRewards(RLVR);itobtainsaconstantrewardvalueif
acompletionissuccessful. OurresultsshowthatRLVRcanimproveGSM8K,MATH,andIFEvalperformance.
2Weobserveanon-trivialamountofcontaminationinafewopendatasetswithpopularevaluationbenchmarks.Detailsare
providedinTable32.
9‚Ä¢ TrainingInfrastructureforReinforcementLearning(¬ß6.3): WeimplementedanasynchronousRLsetup:
werunLLMinferenceefficientlyviavLLMwhilethelearnersperformgradientupdatesconcurrently. OurRL
codebaseisalsohighlyscalableandcantrain70BRLVRpolicymodels.
‚Ä¢ EvaluationFramework:T√úLU3EVAL(¬ß7)Inadditiontoevaluatingthefinalmodels,ourevaluationframework
isanopenevaluationtoolkitdesignedtoguidethedevelopmentprogressthroughcarefullyselectedevaluation
suiteandtoolsfordecontamination.
2.4 EvaluationandResults
Whenreportingscoresthroughoutthiswork,weusethemetricsidentifiedinTable3;higherisalwaysbetter. When
computingoverallperformance,wesimplyaveragescoresacrossallevaluations,treatingeachevaluationequally.
T√úLU 3trainedonLlama3basemodelsoutperformsallotheropen-weightmodelsinitssizecategoryonour
development evaluation suite. Compared to closed models, T√úLU 3 70B even surpasses closed models such
asGPT-3.5-Turbo-0125orGPT-4o-mini-2024-07-18,whileapproachingtheperformanceofClaude3.5Haiku
20241022. ThesummaryofT√úLU3trainedonLlama3at8and70billionparametersversustheleadingmodelsin
theirsizesclassesisshowninTable2. Apertrainingstagebreakdownofperformanceisshownforthe8Bversion
inTable4andfor70BinTable5.
Withourmodelstrainedfromrawpretrainedbasemodels,wecomparetoinstructmodelstrainedonthesame
basemodels(e.g. NousHermes3),instructmodelsonsimilarsized,butdifferentbaseversions(e.g. Ministral
8BorQwen2.5Instruct),andotherfinetuningrecipestrainedonaninstructversion(e.g. NemotronLlama3.1).
At70B,wecomparetoandsurpassLlama3.170BInstruct,Qwen2.572BInstruct[QwenTeam,2024],Nous
Hermes370B[Tekniumetal.,2024](trainedonLlama3.170B),andNemotronLlama3.170B[Wangetal.,
2024c](trainedonLlama3.170BInstruct). At8B,wecomparetoandsurpassLlama3.18BInstruct,Gemma2
9BInstruct[GemmaTeametal.,2024],NousHermes38B(trainedonLlama3.18B),Qwen72.5BInstruct,and
Ministral8BInstruct2410.
ArtifactsReleased. WereleaseallartifactsassociatedwiththeT√úLU3trainingrecipe‚ÄìincludingSFT,DPO,and
RLmodelcheckpoints,alongwithnewSFTandDPOdatasets. AsummaryoftheartifactsreleasedwithT√úLU3is
includedinTable1.
3 T√úLU 3Data
Promptsrepresentthediversewaysusersmayinteractwithmodelsandserveastheessentialcomponentforall
post-trainingstages. Wecurateanextensivecollectionofmillionpromptsasthestartingpointof T√úLU3post-
trainingrecipe. Dataselectedfornextstagesoftrainingareselectedfromtheseprompts. Table6summarizesthe
keyinformationoftheseprompts. Inthissection,wedescribeourpromptcurationprocessandthedecontamination
efforttoensurethatourevaluationsarenotleakedintheseprompts. Inthefollowingsections,wedescribehow
promptsareusedforsupervisedfinetuning¬ß4andpreferencetuning¬ß5.
3.1 PromptCuration
Totargetthedesiredcoreskills,wecurateadiverseandhighqualitysetofpromptsfrompubliclyavailabledatasets
withclearprovenanceandsyntheticallygeneratepromptstofillanygaps.
3.1.1 SourcingfromPublicDatasets
SincethereleaseofourT√úLU 2,thecommunityhaswitnessedalargebodyofworkcreatingdatasetsforpost-
training,intermsofbothsupervisedfinetuningandpreferencetuning. T√úLU3aimstointegrateandextendthese
resourcestobuildstrongermodels. Westartthisprocesswithabroadsurveyofpublicdatasets,includingthose
annotatedbydedicatedworkers,sourcedfromrealusers,andsynthesizedwithmodels. 3 Wethenmanuallyreview
eachindividualdataset,andpickthosewiththefollowingconsiderations.
Diversity. Thediversityoftrainingdataiscriticalforelicitingmodels‚Äôgeneralization,avoidingmodelforgetting,
andmakingmodelsrobusttouncommoninputs[Wangetal.,2022c,Chungetal.,2024,Zhouetal.,2024]. Wepick
datasetsthatcanpromotediversity,including: WildChat[Zhaoetal.,2024],whichisalargesourceofreal-user
interactionwithmodels;OpenAssistant[K√∂pfetal.,2024],whichiscreatedbyvolunteerworkersforgeneral
3The datasets we compiled and consider are available here: https://docs.google.com/spreadsheets/d/
1E2ScaKWbTnlelzJzcddCzEtf7WrpF3a5ZP5ZvdsOZ4Y/edit?usp=sharing.
10#Prompts #Prompts
Category PromptDataset Count usedin usedin Reference
SFT DPO
General T√úLU3Hardcoded‚Üë 24 240 ‚Äì ‚Äì
OpenAssistant1,2,‚Üì 88,838 7,132 7,132 K√∂pfetal.[2024]
NoRobots 9,500 9,500 9,500 Rajanietal.[2023]
WildChat(GPT-4subset)‚Üì 241,307 100,000 100,000 Zhaoetal.[2024]
UltraFeedbackùõº,2 41,635 ‚Äì 41,635 Cuietal.[2023]
Knowledge FLANv21,2,‚Üì 89,982 89,982 12,141 Longpreetal.[2023]
Recall SciRIFF‚Üì 35,357 10,000 17,590 Waddenetal.[2024]
TableGPT‚Üì 13,222 5,000 6,049 Zhaetal.[2023]
Math T√úLU3PersonaMATH 149,960 149,960 ‚Äì ‚Äì
Reasoning T√úLU3PersonaGSM 49,980 49,980 ‚Äì ‚Äì
T√úLU3PersonaAlgebra 20,000 20,000 ‚Äì ‚Äì
OpenMathInstruct2‚Üì 21,972,791 50,000 26,356 Toshniwaletal.[2024]
NuminaMath-TIRùõº 64,312 64,312 8,677 Beechingetal.[2024]
Coding T√úLU3PersonaPython 34,999 34,999 ‚Äì ‚Äì
EvolCodeAlpacaùõº 107,276 107,276 14,200 Luoetal.[2023]
Safety T√úLU3CoCoNot 10,983 10,983 10,983 Brahmanetal.[2024]
&Non-Compliance T√úLU3WildJailbreakùõº,‚Üì 50,000 50,000 26,356 Jiangetal.[2024]
T√úLU3WildGuardMixùõº,‚Üì 50,000 50,000 26,356 Hanetal.[2024]
Multilingual Aya‚Üì 202,285 100,000 32,210 Singhetal.[2024b]
PreciseIF T√úLU3PersonaIF 29,980 29,980 19,890 ‚Äì
T√úLU3IF-augmented 65,530 ‚Äì 65,530 ‚Äì
Total 23,327,961 939,344 425,145
Table6: Summaryofourpromptdataset: datafortrainingstagesareselectedfromtheseprompts. Newdatasets
released with T√úLU 3 are color-coded for emphasis. Existing datasets we modified due to contamination are
markedwithùõº. DatasetswithpromptsusedinT√úLU1or2aremarkedwith1or2,respectively. Datasetsmarked
with‚Üìaredownsampledfromtheiroriginaldatasets,datasetsmarkedwith‚Üëareupsampled. Notethatalldatasets
werefilteredtoremovespecifickeywords(e.g.,OpenAI)andemptymessages,resultinginslightlylowerthan
reportedcounts. AllT√úLU3datasetswithPersonaexpandthemethodologyofChanetal.[2024]. Thepercentages
listedpercategoryareoutofthetotalprompts.
chatting;NoRobots[Rajanietal.,2023],whichisannotatedbyexpertworkersforabroadrangeofopen-ended
categories; and FLAN v2 [Longpre et al., 2023], which is a big compilation of classical NLP tasks. We also
includeadecontaminatedsubsetofUltraFeedback[Cuietal.,2023],whichisacompositionofseveraldatasets
(FalseQA[Huetal.,2023],UltraChat[Dingetal.,2023],Evol-Instruct[Xuetal.,2023],FLANv2[Longpreetal.,
2023])andhasshownstrongperformanceforgeneralpreferencetuninginearlystudies[Tunstalletal.,2023,Ivison
etal.,2024].
TargetSkills. Weespeciallyconsiderenhancingseveralcapabilitiesthatcanpowercommonusecasesandour
specialneeds. Asshowninourearlierstudy[Wangetal.,2023],somecapabilities,suchascomplexreasoning,
coding, and precise instruction following, benefit from mixing in additional data. Therefore, we include the
followingdatasets: OpenMathInstruct[Toshniwaletal.,2024]andNuminaMath[Beechingetal.,2024]formath
reasoning,Evol-CodeAlpacaforcoding,asubsetofDaring-Anteater[Wangetal.,2024d]forpreciseinstruction
following,Aya[Singhetal.,2024b]formultilinguality,SciRIFF[Waddenetal.,2024]forscientificliterature
understanding,andTableGPT[Zhaetal.,2023]forprocessingtable-relatedtasks. Wehavealsoconsideredother
datasetsfordomainswithplentyofstudies(e.g.,math),buttheyeitherdidnotbringadditionalbenefitsinourearly
supervisedfinetuningexperimentsorhavelicenseissues.
DataProvenanceandLicenses. Whensourcingprompts,wetakecarefulconsiderationofthelicensesofthe
originaldatasetsandonlyallowthosewithclearandcorrectlicenses. Sincemanypubliclyreleaseddatasetsare
11compositionsofotherdatasets,wehavetomanuallytracktheprovenanceofsubsetstoverifytheirlicensesand
removethosethathaveissues. Specifically,theShareGPTdataset4 isofquestionablelegalprovenanceasthey
weresharedbyusersontheInternetwithoutanagreementtobeusedformodeltraining,soweexcludeitanduse
WildChatinstead. WealsoremovedtherelevantsubsetfromUltraFeedbackanddecidednottouseHelpsteer2
[Wangetal.,2024d]duetotheuseofShareGPTintheirprompts. Allthedatasetsincludedinourfinalcurationhave
clearlicenses,andwehaveasmallercommercial-friendlysetandalargernon-commercialsetbasedonwhether
thelicensesallowcommercialuse.
3.1.2 SynthesizingforTargetSkills
Toaddressthegrowingneedfordiverseandskill-specificdatasets,weincorporatesyntheticdatagenerationasa
complementaryapproach. Syntheticdatagenerationhasgainedtractionasapromisingalternativetohuman-written
dataduetobeingeasiertoobtain,customizablefordifferentpurposes,andreflectingthevastknowledgeofthe
underlyingmodels[Dubeyetal.,2024]. However,generatingdiverseandhigh-qualitydataatscaleisnon-trivial,
asLMsaresusceptibletofallingintorepetitivemodesorpatterns,referredtoas‚Äúmodecollapse‚Äù[Kazdanetal.,
2024]. Toensurediversityingeneration,wefollowtherecentpersona-drivenmethodologyinChanetal.[2024]to
generatesyntheticdata. Thekeyideaistousedifferentpersonas(e.g.,‚ÄúAmachinelearningresearcherfocusedon
neuralnetworks‚Äù)withadatasynthesisprompt(e.g.,‚Äúcreateacodingproblem‚Äù)tosteeranLLMtosynthesize
datawithcorrespondingperspectives. Specifically,weconditionon‚àº250KpersonasfromPersonaHub[Chan
etal.,2024]togeneratepromptstargetingspecificskillssuchaspreciseinstructionfollowing,mathandcoding.
Wedetailourprocedureforeachselectskillbelow. Promptsusedtogeneratetheseinstructionscanbefoundin
AppendixC.1. Additionally,webuilduponourpreviouseffortsinBrahmanetal.[2024],Hanetal.[2024],Jiang
etal.[2024],togeneratenoncomplianceandsafetydata.
PreciseInstructionFollowing. Preciseinstructionfollowingistheabilitytofollowverifiableinstructionsin
naturallanguage, suchas‚Äúwrite3paragraphs,‚Äù thatcanbeautomaticallyverifiedwithheuristics. Weuseour
persona-drivenapproachtosyntheticallygenerateverifiableinstructionscovering25differentconstrainttypes
definedinIFEvalbenchmark[Zhouetal.,2023]. Moreconcretely, westartbymanuallywriting1-2example
instructions per constraint (e.g., number of words), resulting in total of 33 verifiable instructions which we
usedasseedprompts. WethengeneratenewinstructionsusingGPT-4o[OpenAI,2024]5givenadatasynthesis
prompt,persona,andasingleverifiableinstructionasanexample. Figures28and29showtheexactpromptsused
togeneratetheinstructionanditscorrespondingresponse,respectively. Intotal,wecollected29,980verifiable
instruction-responsepairswhichwecallIF-PERSONA-SFT.
MathandCoding. Wefollowasimilarpersona-drivenapproachtosyntheticallygeneratediversemathword
andcodingproblems. Mathproblemsincludethosethatrequireadvancedmathematicalskillsaswellasgrade
schoolproblems. Forcoding,wegeneratepythonprogrammingquestionsthataresolvablebyentry-tomedium-
levelprogrammers. Unlikepreciseinstructionfollowing,wezero-shotpromptGPT-4otogenerateproblemsthat
areuniqueandspecifictoagivenpersonainput. Havinggeneratedtheproblems,wethengeneratemulti-step
mathsolutionsusingGPT-4o,andpythonprogramsusingclaude-3-5-sonnet. Exactpromptsusedtogenerate
problemsandsolutionsareprovidedinFigures31,33,32,and34,respectively. Intotal,wecollected‚àº220Kand
35Kinstancesformathreasoningandcoding.
NoncomplianceandSafety. Asweenhancemodels‚Äôcapabilitiestoassistuserseffectively,itiscrucialtoensure
theycanreliabilityrejectunsafeandappropriatelyhandlenuancedandoutofscopequeries. Tosupportthis,we
curateasetofnoncompliancequeries[Brahmanetal.,2024]thatthemodeloughttonotcomplywith,alongside
safety-relateddirectandadversarialprompts[Hanetal.,2024,Jiangetal.,2024]coveringbothbenignandharmful
scenarios. Our noncompliance and safety prompts are either curated from existing datasets [Zhang and Choi,
2021,Zhaoetal.,2024]orsyntheticallygeneratedfromGPTmodelfamily. Morespecifically,ournoncompliance
promptsareobtainedbasedonourcontextualnoncompliancetaxonomyfromBrahmanetal.[2024],spanning
multiplecategoriesincludingincomplete,unsupported,indeterminate,andhumanizingrequests(inadditionto
unsaferequests). Oursafety-relatedpromptsarecarefullyselectedamongsyntheticadversarialprompts,synthetic
vanilla(direct)requests,real-worlduser-LLMinteractions(In-The-Wild),andcuratedannotator-writtenexamples
tomaximizecoverage,diversity,andbalance.
12Dataset Eval. Link %‚Üì
EvolCodeAlpaca HumanEval Orig:ise-uiuc/Magicoder-Evol-Instruct-110K
3.5
New:allenai/evol_codealpaca_heval_decontaminated
WildChatGPT-4 Safety Orig:allenai/WildChat-1M-Full(GPT-4instancesonly)
5.4
New:allenai/wildchat_gpt4_converted_safety_decontaminated
WildJailbreak Safety Orig:allenai/wildjailbreak
0.7
New:allenai/wildjailbreak_safety_decontaminated
WildGuardmix Safety Orig:allenai/wildguardmix
1.1
New:allenai/wildguardmixtrain_safety_decontaminated
NuminaMath-TIR MATH Orig:AI-MO/NuminaMath-TIR
11.3
New:allenai/numinamath_tir_math_decontaminated
Table7: Decontaminateddatasets. %isthepercentofthedatasetremoved.
3.2 PromptDecontamination
Oneimportantconsiderationwhencuratingourtrainingmixwaspossibleoverlapbetweentrainingandevaluation
sets. Wequantifysuchoverlapasfollowsandremoveinstancesfromourtrainingmixasneededinordertoprevent
testsetcontamination.
MatchingMethod. Weexperimentedwithfull-string,n-gram,andembedding-basedmatchingandfoundthat
n-grammatchingyieldedthemostusefulresults‚Äîwhileembedding-basedmethodscaninprincipleidentify
non-trivialcontaminationlikethatduetoparaphrasing[Yangetal.,2023],wefounditdifficulttodistinguishmere
distributionalsimilarityfromactualparaphrasing. Moreover,partialsurface-leveloverlapusingn-grammatching
successfullyidentifiedcasesofcontaminationwheretheinstancesweretriviallydifferent,e.g.,amathproblem
whereonlythenumbersdiffer.
IdentifyingMatchingInstances. Sincecompletionsintrainingdatasetsareoftenregeneratedusinglanguage
models,wechosetocomputeoverlapinthepromptsalone(ormoregenerallyuserturnsinmulti-turndialogues).
Weused8-grammatchingforourcontaminationchecksfollowing[Dubeyetal.,2024,Singhetal.,2024a]. For
eachtokeninatestinstance,weconsiderittomatchatokeninatraininstanceifthetwoinstancessharean8-gram
containingthattoken,andweconsiderthetestinstanceitselftohavesignificantoverlapwithatraininstanceif
morethan50%ofthetesttokenshave8-grammatcheswiththesametraininginstance.
Decontamination. We consider a training set to be contaminated if any number of its instances overlap with
morethan2%oftheinstancesinanyoftheevaluationsinourdevelopmentandunseensuites. Weremoveallthe
trainingsetsthatwerecontaminatedwithourunseenevaluations. Fortrainingsetsthatwerecontaminatedwithour
developmentevaluations,weremovedtheentiredatasetifdoingsodidnotsignificantlyimpacttheperformanceof
theresultingmodel;otherwise,weremovedthespecificinstancesthatmatchanytestinstance.
Thelistofdatasetswedecontaminatedandtheversionswereleasedwithoverlappingsamplesremovedisshownin
Table7. Thefulllistofpublicdatasetsthatwefoundtobesignificantlycontaminatedwithourevaluationsetscan
befoundinTable32.
4 SupervisedFinetuning
Adaptingpretrainedbasemodelstovarioustasksanduserrequestsoftenreliesonsupervisedfinetuning(SFT),
alsoknownasinstructionfinetuning. Akeychallengeinthisprocessisbalancingtheproportionsofmixedtraining
datasetsrepresentingdiverseskills. ForT√úLU3,weconducteddatamixtureablationsandexploredmodelmerging
4ShareGPTdatawasinitiallyusedtobuildtheVicunamodel[Chiangetal.,2023],buttheexactdatasethasnotbeenreleased.
Later work mainly used a community reproduced version at https://huggingface.co/datasets/anon8231489123/
ShareGPT_Vicuna_unfiltered/.
5WeuseGPT-4o-2024-08-06forallourpersona-drivendatasynthesis,unlessotherwisestated.
13Tulu3PersonaMATH EvolCodeAlpaca Aya WildChat
FLANv2 NuminaMath-TIR OpenMathInstruct2 WildGuard
WildJailbreak Tulu3PersonaGSM Tulu3PersonaCode Tulu3PersonaIF
Tulu3PersonaMATH- Other(<11,000instances)
Algebra
Figure2: TheT√úLU3finalSFTmixbysourceandlengthofthepromptpluscompletionintokens(usingtheLlama
3tokenizer). ComparethisdistributiontopreviousopenSFTtrainingdatasetsinFig.24. Datasetswiththemost
instancesareonthebottomofthehistogram.
techniquestodevelopanSFTtrainingprocedurethatwellbalancesperformanceacrossthecoreskillsweprioritized.
Thefollowingsectionsdetailourexperimentsandfindings.
4.1 SFTData
4.1.1 FromPromptstoSFTData
TocreateourSFTmix,wecollectorcreateresponsesforpromptsdescribedinSection3intwoways: filtering
existingresponses,andcreatingnewresponses.
Forpromptswithexistingresponses,wegenerallykeeptheoriginalresponseifitwaswrittenbyahumanora
frontiermodel,likeGPT-4o. Forlargedatasetswithsubsetsfromfrontiermodels(e.g. WildChat),weusethe
subsetfromthebestmodels. Weadditionallyfilteremptyresponsesandresponsesthatcontaininformationabout
modelsortheirdevelopers. Ifasetofpromptsdidnothaveresponses,likeourPersonaprompts,oriftheoriginal
responseswerefromaweakermodel(e.g. WildGuardMix),wegeneratenewresponsesusingGPT-4o. Wealso
hand-wroteresponsestoourhardcodedprompts.
4.1.2 TheT√úLU3SFTMix
TodevelopourSFTmix,wefirstidentifiedtheskillsthatwerelaggingbehindstateoftheartmodelsusingLlama
3.1trainedonT√úLU26asourbaseline. Targetingeachoftheseskillsinisolation,wecollectedhighqualitypublicly
availabledatasetsandcreatedsyntheticdatasets,asdescribedinSection3.1.2,andalsoremovedsomedatasetsthat
weidentifiedtobeofrelativelylowerqualitycomparedtoothermorerecentdatasets.
TodesignourfinalSFTmix,wefirstbuiltskill-specificdatamixturesandmodels,keepingthemixturesthatled
tothebestperformanceonindividualskills,ignoringotherevaluations. Thiswasdonetoapproximatetheupper
boundforeachevaluationgivenoursetup.
We then combined these mixtures to create our initial T√úLU 3 preview mix. We then continued to iterate on
themixturebyaddingorremovingdatasetstoimprovelaggingskills,decontaminatingagainstourevaluations
6https://huggingface.co/allenai/llama-3.1-tulu-2-8b
14
tnuoC80
60
40
20
0
Average BBH GSM8K IFEval MATH MMLU Safety
T√úLU2 IntermediateMix1 IntermediateMix2 IntermediateMix3
IntermediateMix4 IntermediateMix5 T√úLU3
Figure3: Averageandselectedskill-specificperformancefromtrainingLlama3.18BonourinitialT√úLU2SFT
mix,andourintermediateandfinalT√úLU3SFTmixes. Intermediatemixes1,2,and3weretheresultofadding
newdatasetstoimproveperformance. Intermediatemixes4and5weretheresultofrunningmultipleroundsof
decontamination,causingsmalldropsinperformance.
anddownsamplingparticularlylargedatasets. Weshowtheperformanceofmajorpreviewversionsthroughout
developmentinFigure3.
FinalSFTResults. InTable8,wecompareourfinalT√úLU 3 8B SFT andT√úLU 3 70B SFT modelsagainst
otherSFT-onlymodelstrainedonLlama38Bor70B.OurnewSFTmixshowssubstantialimprovementsoverthe
T√úLU2mixatbothmodelsizes,andisbetteronaveragetheothercompetitive8BSFTmodels.
Model Avg. MMLU TQA PopQA BBH CHE CHE+ GSM DROP MATH IFEval AE2 Safety
T√úLU28BSFT 48.3 61.8 49.4 23.3 57.1 66.9 63.1 60.4 61.7 14.0 42.3 8.9 70.7
RLHFlowSFTV2 56.0 65.8 56.0 29.7 69.3 86.2 80.9 81.6 57.2 35.7 52.7 13.6 43.5
MAmmoTH28B 46.4 63.6 42.7 20.8 63.4 72.8 66.4 63.7 43.8 30.5 34.9 6.5 47.8
T√úLU38BSFT 60.1 62.1 46.8 29.3 67.9 86.2 81.4 76.2 61.3 31.5 72.8 12.4 93.1
T√úLU270BSFT 63.6 76.0 57.8 44.1 79.4 86.8 83.5 83.2 75.9 33.1 57.7 17.3 68.8
T√úLU370BSFT 72.6 79.4 55.7 48.6 82.7 92.9 87.3 91.1 77.2 53.7 82.1 26.3 94.4
Table8: SummaryoftheperformanceofourT√úLU3SFTmodelsagainstcomparablebaselines. OurfinalSFT
mixturesshowstrongperformance,achievingahigheraveragescorethanothercomparablemixes. Allmodels,
includingT√úLU 2SFT,weretrainedoneitherLlama3.0or3.1. OurfinalT√ºlu370Bmodelwasusedtohelp
formatthistable.
4.2 KeyDataExperiments
WealsoranaseriesofcontrolledexperimentsafterdevelopingourfinalSFTmixtoexploretheimportanceof
differentdecisionsmadeduringdatamixingandtraining.
DiverseChatData. Inourmixwealsoemphasizedaddingdiversechatdata,mainlyfromWildChat. Weshowthe
impactofremovingWildChatinTable9,andweseethatthereisasmallbutnoticeabledegradationonmostskills,
mostnoticeablyonAlpacaEval,highlightingtheimportanceofdiversereal-worlddata.
SafetyisOrthogonal. WefoundthatoursafetySFTdatawasgenerallyorthogonaltoourotherdatasets. Wereport
theeffectofremovingoursafety-specificdatasetsinTable9,andweseethatmostskillsstayedroughlythesame,
exceptthesafetyaverage. Wealsofoundthataddingconstrastiveprompts,suchasthoseinCoCoNot,werehelpful
forpreventingourmodelsfromover-refusingsafeprompts.
15
ecnamrofrePModel Avg. MMLU TQA PopQA BBH CHE CHE+ GSM DROP MATH IFEval AE2 Safety
T√úLU38BSFT 60.1 62.1 46.8 29.3 67.9 86.2 81.4 76.2 61.3 31.5 72.8 12.4 93.1
‚Üíw/oWildChat 58.9 61.0 45.2 28.9 65.6 85.3 80.7 75.8 59.3 31.8 70.1 7.5 95.2
‚Üíw/oSafety 58.0 62.0 45.5 29.5 68.3 84.5 79.6 76.9 59.4 32.6 71.0 12.4 74.7
‚Üíw/oPersonaData 58.6 62.4 48.9 29.4 68.3 84.5 79.0 76.8 62.2 30.1 53.6 13.5 93.9
‚Üíw/oMathData 58.2 62.2 47.1 29.5 68.9 86.0 80.5 64.1 60.9 23.5 70.6 12.0 93.5
Table9: PerformanceduringourSFTablations,showingtheeffectofremovingsafety,WildChat,Persona,and
Mathdatainisolation. Wefindthat: 1)diversechatdataisbeneficialformostskills,mostnoticeablyAlpaca
Eval,2)safetyperformanceisgenerallyorthogonaltogeneralperformance,3)ournewPersonadatasetsimprove
alloftheskillsthattheytarget,and4)usingmathematicsasatestcase,addinghighqualityskill-specificdata
substantiallyimprovesskill-specificperformance.
New Persona Data. Our new Persona datasets were built to target specific skills: mathematics, coding, and
instructionfollowing. InTable9weshowthatperformanceonHumanEval(+),GSM8K,MATH,andIFEvaldrop
afterremovingourPersonadatasets,showingthevalueofcreatingdiverse,skill-specificSFTdatasets.
TargetingSpecificSkills. WealsoshowtheimpactoftargetingspecificskillsinTable9,whereweshowtheeffect
ofremovingallmathematics-specificdata. WeseethatbothGSM8KandMATHdropsignificantlycomparedto
ourfinalmix,emphasizingthepositiveimpactoffocusingonspecificskillsduringdatacollection.
TargetingSpecificSkills. alargeportionofourfocuswasoncollectingorcreatingdatasetstargetingspecific
capabilities. Using mathematical reasoning as an illustrative example, we show in Table 9 the impact of our
mathematics-specificdataonbothGSM8KandMATH.Weseethatourmathematics-specificSFTdatasubstantially
improvesbothGSM8KandMATH,showingthevalueofthedataincludedinthefinalmix.
AmountofSFTData. InFigure4,weshowtheeffectoftakingstratifiedsubsamplesofourSFTmix. Wefind
thatourmodelscontinuetoimproveonaverageasmoreSFTdataisincluded,andweseelargeimprovementson
metricslikeGSM8Kasweincreasetheamountofdatatothefullmix. Interestingly,TruthfulQAperformance
actuallydropsastheamountofdatainthemixincreases. WedonotincreaseourSFTdatasizebeyondthecurrent
mixturebecauseweallocatedotherpromptsforpreferenceoptimization.
Hyperparameter 8BValue 70BValue
LearningRate 5√ó10-6 2√ó10-6
LearningRateSchedule Linear Linear
BatchSize(effective) 128 128
MaxTokenLength 4,096 4,096
Warmupratio 0.03 0.03
NumberofEpochs 2 2
Table10: SFTTrainingHyperparameters.
4.3 SFTRecipeandAnalyses.
Training Settings To train our T√úLU 3 models, we used between 4 and 16 8xH100 nodes with high speed
interconnect. Weusedaneffectivebatchsizeof128andamaximumsequencelengthof4,096tokens. Wetrained
fortwoepochsusingalearningrateof5e-6forour8Bmodels,and2e-6forour70Bmodels,whichwefoundafter
ahyperparametersearch. OurhyperparametersettingsarealsosummarizedinTable10. Formergingexperiments
weusedmergekit7[Goddardetal.,2024],usinglinearweightedaveraging.
4.3.1 KeyTrainingExperiments
Choice of Base Model. We also test the effect of training different base pretrained models on mathematical
performanceusingourfullSFTmix. InTable11,weshowtheimpactofchangingthemodel‚Äôssizebytrainingon
bothLlama3.18Band70B,andtheimpactofaddingdomain-specificpretrainingdatabytrainingonQwen2.57B
7https://github.com/arcee-ai/mergekit
16andQwen2.5Math7B.Inbothcases,weseeasubstantialimprovementinbothGSM8KandMATH,highlighting
theimportanceofbothmodelsizeandpretrainingdatafordownstreamskills.
BaseModel GSM8K MATH
Llama3.18B 76.2 31.5
Llama3.170B 91.1 53.7
Qwen2.57B 79.2 49.4
Qwen2.5Math7B 86.3 56.4
Table 11: Mathematical performance of different base models trained on our mix. We see that 1) training on
largermodelsleadstobetterperformance, and2)addingskill-specificpretrainingdataalsoleadstoimproved
performance,evenforthesamesizemodel.
ChatTemplateVariation. DuringcreatingT√úLU3,weexploredchangingthechattemplateusedtoguidethe
generationoffinetunedmodels. WemadeasmallchangetothechattemplateusedinpreviousT√úLUversions,
specificallyremovingthenewlineattheendofthetemplate(beforethemodelresponse). Theperformancebetween
differentchangestothechattemplateisshowninTable12onanearlyversionofourSFTsetup. Wefoundthat
replacingthenewlinesattheendofassistantmessageswithaneostokenresultedinthebestperformance,but
weoptednottousethistoavoidgenerationinconsistencywithlaterstepsinourpost-trainingpipeline. Thechat
templatecanbefoundinourcodebaseandweprovideitinAppB.3.
ChatTemplate Avg.
T√úLU(replace‚àñnw/eos) 53.0
Zephyr 52.9
T√úLU3(no‚àñn) 52.8
T√úLU2template 52.6
Llama3template 51.6
Table12: TheimpactofdifferentchattemplatesonSFTmodelperformance,trainedusinganintermediateSFT
mixtureonLlama3.0. Whilereplacingthenewlinedoesbest,weinsteadoptedforsimplyremovingthenewlineto
avoidcomplexity.
RandomSeedsandModelSoups. WealsoexploredchangingtherandomseedduringSFT,andthenusingthose
modelstocreatemodelsoups[Wortsmanetal.,2022]. InTable13,wecomparetraining8Band70Bmodelswith
multipledifferentseedswiththebestmodelsoup. WeseethatSFTperformancenoticeablyvariesbasedontheseed,
highlightingtheimportanceofmultipletrainingruns,andthatthebestmodelsoupdoesnotalwaysoutperformthe
bestsingletrainingrun. Becauseofthis,weusethebestsingleSFTtrainingrunforeachmodelsizeasourfinal
SFTmodels.
Model Seed Average Model Seed Average
T√úLU38BSFT 42(Default) 59.9 T√úLU370BSFT 42(Default) 71.8
123 60.1 123 70.0
456 59.8 456 72.6
789 59.8 - -
1011 59.8 - -
BestModelSoup 42&123 60.2 BestModelSoup 123&456 72.5
Table13: Averageperformanceofour8Band70BSFTmodelsusingrandomseeds,andcomparedagainstthebest
modelsoupusingthemodelstrainedwithdifferentseeds. Wefindthatthebestrandomseediscomparabletothe
bestmodelsoup,soforconsistencyweusethebestsingleSFTrunasourfinalSFTmodel.
1780
60
40
20
0
Avg. GSM8K HumanEval+ Safety TruthfulQA
5% 10% 25% 50% 75% 100%
Figure4: Averageandskill-specificperformanceonstratifiedsubsamplesofourfinalSFTmix. Wefindthatour
fullmixperformsbestoverall.
4.3.2 BatchAggregation
EarlyduringtrainingT√úLU3,wenoticedagapinperformancebetweenSFTmodelstrainedonourOpen-Instruct
frameworkandmodelstrainedinothersettingssuchasonTPUs.8 Wefoundthisissuewaslargelyduetoa(recently
widely-reported)issuewithlossaggregationinsideTransformers[Wolfetal.,2020]: Averagingthelossacross
paddingtokenswithouttakingintoaccountgradientaccumulationordistributedtrainingsetups.
Here,weillustratetheissuewithanexample. Assumewehavetwosamplesinabatch,withùëõ ,ùëõ non-padding
1 2
tokensandùëö ,ùëö paddingtokens. IfwepassbothsamplesintothedefaultTransformersforwardpassatthesame
1 2
time,weget:
ùëô +ùëô
ùêø= ùëõ 1 ùëõ 2 (1)
ùëõ +ùëõ
1 2
However, ifweapplygradientaccumulation, feedinginthetwosamplesseparately, computingloss, andthen
dividing,ourlossisinsteadcomputedlike:
ùëô ùëô
ùëõ1 + ùëõ2
ùêø= ùëõ 1 ùëõ 2 (2)
2
Thatis,inthesecondcaseweweighteachexampleequally,whileinthefirstweweighteachtokenequally. Assuch,
changinggradientaccumulationcanhavelargeeffectsonperformanceduetoeffectivelychangingsampleweightings,
asreportedbyMuennighoffetal.[2024]. Asimilarissueoccursindistributedtrainingduetocross-deviceaveraging.
Werefertorecentreportsonthisissueforamorein-depthexplanation.9
Tofixthisissue, weoptedgenerallytouseasumlossinsteadofaveraging(‚Äòmeanloss‚Äô)whentraining. This
removestheissuebysimplyremovingthedenominatorfromtheaboveequationsandrequiresanadjustmentto
learningrates. Thiseffectivelyweightsalltokensequally(whichwefoundledtogenerallybetterperformancefor
initialmixtures). WevalidatedtheperfomanceofoursetupbyfinetuningLlama3.0ontheT√úLU2SFTmixture
usingavarietyoflearningrates,epochs,andlosstypesasshowninFigures5and 6. Ultimately,wefoundthat
usingasumlosswithalearningrateof5.00E-06workedbest. Surprisingly,weadditionallyfoundthattraining
forlongerdidnotyieldfurtherimprovements,andsoused2epochsfortraining.
8Relevantcode:https://github.com/hamishivi/EasyLM
9https://unsloth.ai/blog/gradient,
https://muellerzr.github.io/blog/gradient_accumulation_part2.html
18
ecnamrofreP50 52
51
45
50
40
Mean Loss 49
Sum Loss
2.0x10^-6 5.0x10^-6 1.0x10^-5 2.0x10^-5 2 3 4 5 6 7
Learning Rate Number of Epochs
Figure5: AverageperformancewhenfinetuningLlama Figure6: AverageperformancewhenfinetuningLlama
3.0ontheT√úLU 2mixtureusingdifferinglosstypes 3.0ontheT√úLU2mixtureusingsumlossandLRof
andlearningrates. WefindthataLRof5e-6witha 5e-6forvaryingnumbersofepochs. Wefindusing2
sumlossworksbest. epochsworksbest.
5 PreferenceFinetuning
ForT√úLU3weexploremanyapproachestopreferencefinetuningwiththegoalofimprovingourentireevaluation
suite. Weexploremultipletrainingalgorithms,fromDirectPreferenceOptimization(DPO)anditsderivatives
toreinforcementlearningalgorithmssuchasProximalPolicyOptimization(PPO).Inthissection,wedetailthe
problemformulationoflearningfromhumanpreferencesandouroptimizers. Next,weexplainhowtoconvertour
promptsintosyntheticpreferencedatafrombothon-policy(T√úLU3suite)andoff-policymodels(otherinstruct
models. Weshowhowtocreatepreferencedataforspecificskillsofinterestandhowweimproveourmodels
robustlywithDPO.
5.1 Background
Prior work has established training on preference data as a crucial step for improving model performance on
benchmarkssimulatinghumanorsyntheticpreferences[Duboisetal.,2023,Ivisonetal.,2023,2024]. Thetypical
procedureisreinforcementlearningfromhumanorsyntheticfeedback [Ziegleretal.,2019,Stiennonetal.,2020,
Ouyangetal.,2022,Baietal.,2022].
5.1.1 Setup
Preference Data. In the standard setup, there is some preference dataset Óà∞ consisting of prompts ùë• and two
responsesùë¶,ùë¶‚Ä≤perprompt. Somejudgeswillchooseoneofùë¶,ùë¶‚Ä≤astheirpreferredresponseùë¶ ,andlabeltheother
ùëê
asarejectedresponseùë¶ .
ùëü
RewardModel. Giventhepreferencedataset,arewardmodel(RM)ùëü istrainedwiththefollowingobjective:
ùúô
m ùëüaxùîº
(ùë•,ùë¶ ùëê,ùë¶
ùëü)‚àºÓà∞[logùúé(ùëü ùúô(ùë•,ùë¶ ùëê)‚àíùëü ùúô(ùë•,ùë¶ ùëü))] (3)
ùúô
whereùúéisthelogisticfunction. TheRMobjectivemaximizesthedifferencebetweentherewards,andthisdifference
representsthelog-likelihoodthatùë¶ willbepreferredoverùë¶ [Ouyangetal.,2022]. Thisrewardmodelcanhelp
ùëê ùëü
trainpolicymodelstooutputcontentspreferredbytheRM‚Äôsjudgments.
5.1.2 PolicyOptimization
Thereareaplethoraofoptionsforoptimizinglanguagemodelswithaccesstopreferencedata. Today,thetwo
categoriescanbeabstractedasreinforcementlearningalgorithms,whichlearnfromaninternalrepresentationof
valueorreward,anddirectalignmentalgorithms,whichlearndirectlyfromthedata.
Prior work [Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022] optimizes the policy ùúã with the
ùúÉ
followingobjective:
m ùúãaxùîº
ùë¶‚àºùúã
ùúÉ(ùë•)[ùëÖ(ùë•,ùë¶)]=[ ùëü ùúô(ùë•,ùë¶)‚àíùõΩKL[ùúã ùúÉ(ùë¶ |ùë•)‚Äñùúã ref(ùë¶ |ùë•)]] (4)
ùúÉ
whereùúã istheinitialreferencepolicy,andtheùõΩ coefficienthelpscontroltheKullback-Lieberdivergence(KL)
ref
divergencebetweenthereferencepolicyandthetrainingpolicy. Here,weexplainPPOandDPOasrepresentative
examples.
19
.freP
.gvA
.freP
.gvAPrompt Selection Response Generation Preference Annotation
Model Pool (22 models) GPT-4o-2024-08-06
‚ÄúRate outputs from 1 to 5
Prompts used in SFT Off-policy data On-policy data based on this aspect...‚Äù
T√ºlu 3 Helpfulness
Prompts from datasets
 SFT 8B Chosen
subsampled for SFT Instruction Following
OR Rejected
Truthfulness
New OOD prompts T√ºlu 3
(Ultrafeedback, Persona) SFT 70B Honesty
Sample four responses from different models for each prompt
Figure7: PipelineforgeneratingandscalingpreferencedatathatisbasedfromUltrafeedback[Cuietal.,2023].
ProximalPolicyOptimization(PPO).Anapproachtoaddresstheaboveobjectiveistouseonlinereinforcement
learning(RL)likePPO[Schulmanetal.,2017]. IneachtrainingiterationofPPO,thepolicyneedstogenerate
somesamples,generaterewardsusingtheRMonthosesamples,andmaximizeùëÖ(ùë•,ùë¶)usingthePPOalgorithm.
AsPPOtrainingloopsarecomplex,wereferthereadertoOuyangetal.[2022],Ivisonetal.[2024],Huangetal.
[2024a]formorethoroughdescriptionsofthesetupandtypicalsetups. Weprovidemoreimplementationdetailsin
Sec6.2.
DirectPreferenceTuning(DPO)andVariants. Anotherapproachisofflinepreferencetuning. DPO[Rafailov
etal.,2024]candirectlyoptimizesfortheRLHFobjectivewiththefollowingequivalentobjective:
[ ( )]
m ùúãa ùúÉxùîº ùë¶ ùëê,ùë¶ ùëü‚àºÓà∞ logùúé ùõΩlog ùúãùúã rùúÉ ef( (ùë¶ ùë¶ùëê ùëê| |ùë• ùë•) ) ‚àíùõΩlog ùúãùúã rùúÉ ef( (ùë¶ ùë¶ùëü ùëü| |ùë• ùë•) ) (5)
DPOtrainsanimplicitrewardmodelandapolicymodelsimultaneously,withoutneedingtouseatrainedreward
model,dopolicygenerations,andgetrewardsfromtheRM.Crucially,thisallowsofflinepreferencefinetuning,
directlytrainingalanguagemodelonpreferencepairsgatheredfromavarietyofsources. Recently,muchworkhas
examinedhowtofurtherimprovetheDPOobjective,withamultitudeofvariantsproposed[Mengetal.,2024,Xu
etal.,2024a,Hongetal.,2024,interalia]. Inthiswork,weexploredtwopromisingvariants: SimPO[Mengetal.,
2024]andlength-normalizedDPO10.Wefind(inSection5.4)thatlength-normalizedDPOworksbest,which
usesthefollowingobjective:
[ ( )]
m ùúãa ùúÉxùîº ùë¶ ùëê,ùë¶ ùëü‚àºÓà∞ logùúé |ùë¶ùõΩ ùëê|log ùúãùúã rùúÉ ef( (ùë¶ ùë¶ùëê ùëê| |ùë• ùë•) ) ‚àí |ùë¶ùõΩ ùëü|log ùúãùúã rùúÉ ef( (ùë¶ ùë¶ùëü ùëü| |ùë• ùë•) ) (6)
Asseen,thisissimplytheDPOobjective(Eq5),butwithlog-probabilitiesnormalizedforlength,whichintuitively
aidswithmitigatingthelengthbiascommoninhumanandmodelpreferences[Singhaletal.,2024].
WhendevelopingT√úLU3,weoptedtouselength-normalizedDPOfortuningourpreferencedatamixturesand
generationmethodsduetoitsrelativesimplicityandspeedcomparedtoapproachessuchasPPO.
5.2 T√úLU3PreferenceData
5.2.1 FromPromptstoPreferenceData
Wecreateon-policypreferencedata(ùë•,ùë¶,ùë¶‚Ä≤,ùëôùëéùëèùëíùëô)givenourpromptsfrom¬ß3byadaptingandadvancingthe
UltraFeedback pipeline [Cui et al., 2023]. Our early experiments show the benefit of this pipeline in creating
preferencedata,whichleadstoahigh-quality,syntheticpreferencedataset(asobservedbyIvisonetal.[2024]).
Ourdatacreationpipeline(showninFigure7)consistsofthreestages: promptselection,responsegenerationfrom
apoolofmodels,andpreferenceannotationwithLLM-as-a-judgetocreate(preferred,rejected)pairs.
‚Ä¢ Stage1: PromptSelection. Thefirststepforpreparingadatasetforpreferencefinetuningistoselectthe
promptsoruserinstructionstogenerateresponsesandobtainpreferencesfor. GiventhesetofpromptsinTable6,
wecurateourselectiontoincludepromptsusedduringSFT,andpromptsthatweresubsampledfromthesame
sources,yetunused,forSFT.Wealsoincludepromptsfromothersources,suchasaversionofUltrafeedback
withoutTruthfulQAinstances,orbyaddingnewIF-constraintstoaprompt.
10AsproposedintheoriginalRafailovetal.[2024],butwasnotyetwelloptimizedtosuccessfulhyperparametersuntilMeng
etal.[2024]
20
eziraniB‚Ä¢ Stage2: ResponseGeneration. Foragivenprompt,werandomlysamplefourmodelsfromamodelpoolto
generateresponses. OurmodelselectionisinspiredbytheUltrafeedbackpipelinewhichconsistsofopen-source
andproprietarymodelsthatvaryacrossparametersizeandmodelfamily. WeupdateUltrafeedback‚Äôsmodelpool
byusingrecentversionsofsomemodels(Llama2‚ÜíLlama3.1),addingbest-performingmodelstoincrease
thepoolsize,andreplacingcurrentlyinaccessiblemodelssuchasWizardLMwithopen-sourcealternatives.
Finally,wealsoincludeon-policydatabysamplingcompletionsfromtheT√úLU SFTmodel. Weapproach
thisbyaddingaselectionofpromptswhereoneresponseisgeneratedfromtheon-policymodel,andtheother
responsefromtheoff-policymodels.
‚Ä¢ Stage3: PreferenceAnnotation. Aftergeneratingfourresponsesforeachprompt,weuseanLLM-as-a-judge
[Zhengetal.,2023],specificallyGPT-4o-2024-0806,torateeachresponsefrom1to5acrossfourdifferent
aspects: helpfulness,instruction-following,honesty,andtruthfulness.
AppendixDshowstheexternalmodelsusedtosampleoff-policydataandtheprompttemplateforeachaspect.
InordertoobtainbinarypreferencesforDPO,weobtainthemeanofpreferenceratingssimilartoArgilla‚Äôs
binarizationmethod11andtakethehighest-ratedresponseasthechosenresponseandrandomlysamplefrom
theresponseswiththelowermeanastherejectedresponse.
5.2.2 TheT√úLU3PreferenceMix
Dataset Count 8B 70B
SFTReusedOn-policy 19,444 ‚úì ‚úì
SFTReusedOff-policy 96,911 ‚úì ‚úì
IF-Augmented 65,530 ‚úì ‚úì
WildChatIF 10,792 ‚úì ‚úì
WildChatReused 17,207 ‚úì ‚úì
WildChatUnused 82,783 ‚úì
Ultrafeedback(Cleaned) 41,635 ‚úì ‚úì
PersonaIF 19,890 ‚úì
Total 354,192 271,409 334,302
Table14: SummaryofourbestpreferencedatasetmixesforT√úLU38BDPOandT√úLU370BDPO.IFisshort
forInstructionFollowing.
We choose the final preference mix for the 8b and the 70b model, which maximizes average performance on
thedevelopmentevaluations,whilealsoexcelsattargetedskills. Mostofthepreferencedatamixablationsare
runforthe8bmodel,WestartwithpromptsusedforSFTandgenerateon-policyandoff-policypreferencedata,
resultingin96911(off-policy)and19444(on-policy)preferenceinstances. Giventhispreferencebaseweablate
addingadditionalpromptsourcestothemixandhowtheseadditionsaffectdownstreamevaluationperformance,
specificallytargetingskillslikepreciseinstructionfollowing,mathandgeneralchatperformanceonalpacaeval.
Table15showshowtheinclusionorexclusionofpreferencedatasetsinfluencestheaverageperformance. Ourfinal
mixesforT√úLU38BDPOandT√úLU370BDPOaredisplayedinTable14. Insummary,ourpreferencemixes
comefromdifferentpromptsources,suchasSFTdata,WildChatandPersonaIF.Itincludespromptsseenduring
SFTtrainingbutalsonew,unseenprompts. Andlastly,itcontainsamixofonandoff-policycompletions.
5.3 KeyFindingsofDataAblations
Weperformseveralablationstoinformthedesigndecisionsofthesyntheticpreferencepipeline(¬ß5.2.1)andthe
compositionoftheT√úLU3preferencemix(¬ß5.2.2).
11https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences/blob/main/
README.md
21SFTMix P-IF WildC.-IF SFT-IF WCùõΩ WCùõº UFùõø DA UF CocoNot Avg.
‚úì ‚úì ‚úì ‚úì ‚úì 62.27
‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 61.99
‚úì ‚úì ‚úì ‚úì 61.83
‚úì ‚úì ‚úì ‚úì 61.76
‚úì ‚úì ‚úì 61.59
‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 61.55
‚úì ‚úì 61.35
‚úì ‚úì ‚úì ‚úì 61.29
‚úì ‚úì ‚úì ‚úì ‚úì 61.25
‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 61.17
‚úì ‚úì 60.87
‚úì ‚úì 60.86
‚úì ‚úì 60.84
‚úì 60.54
Table15: Someofourdatasetmixingexperimentstoobtainthefinalpreferencedatasetmix. Weincludeprompts
fromDaringAnteater(DA),ourSFTMix(SFT),Ultrafeedback(UF),Personapromptsfordifferentskills(P-IF,
P-Code,P-Math),T√ºlu3instructionfollowingprompts(T√ºlu3-IF),i.e. IF-Augmented,CocoNot,theIFsubset
of Daring Anteater Wang et al. [2024d] and WildChat (WildC.). (ùõº: prompts used during SFT, ùõΩ: prompts
fromdatasetssubsampled,yetunused,forSFT,ùõø: onlyusedtheprompts,thecompletionsandpreferenceswere
regeneratedusingthepipelinedescribedin$5.2.1).
80
80
60 60
40 40
20 20
0 0
AVG AE MATH GSM8k AVG AE MATH GSM8k
Initial8BSFT 5% 10% 25% Initial8BSFT 64k 180k 383k
50% 75% 100% Figure9: Effectofscalingapreferencedatasetby
Figure8: Effectofscalingthesizeofthepreference duplicating prompts on downstream DPO perfor-
dataset,specificallythenumberofuniqueprompts, mance using the Ultrafeedback dataset. All sizes
ondownstreamDPOmodelperformance. (AE:Al- havethesamenumberofuniqueprompts(64k).
pacaeval)
ScalingtheNumberofUniquePromptsImproveDownstreamDPOPerformance. First,weinvestigatewhether
increasingthenumberofpromptswillyieldimprovementsindownstreamDPOperformance. Todoso,wemeasure
thedownstreamDPOmodelperformanceatdifferentsizesofafixedamountofpreferenceswithuniqueprompts.
Figure8showsthattherearenoticeableperformancegainsacrossseveralmetricsasthesizeofthepreference
datasetincreases. Thissuggeststhatdatasetscalingisimportanttoachieveimprovementsindownstreammodel
performance: ourfinalpreferencemixes(Table14)containmorethan270kdatapointsforthe8bmodelandmore
than330kinstancesforthe70bmodel-whichisconsiderablybiggerthanmanyavailablepreferencedatasets.
Wealsoexplorewhetherduplicatingprompts(butwithdifferentresponses)isaviableapproachtoscalingthe
size of a preference dataset and whether it will lead to gains in downstream DPO performance. To do so, we
22
ecnamrofreP ecnamrofrePexpandedtheUltrafeedbackdataset,whichoriginallyhadfourresponsesforeachprompt,bycreatingdifferentpair
combinationsofresponses. Thisexpansionwillnaturallycauseduplicatedprompts,butwithdifferentchosenand
rejectedpairssampledfromthefourresponsesinUltraFeedback,leadingtopreferencedatasetswith64k-,180k-,
and383kinstances. Figure9showsthat,onaverage,the383k-sizepreferencedatasetperformssimilarlytothe64k
preferencedataset. WealsoobserveaslightperformancedegradationonDROP,GSM8k,andAlpacaEvalasthe
numberofduplicatedpromptsincrease. Thissuggeststhatscalingviapromptduplicationdoesnotnecessarily
yieldintosignificantgainsindownstreamDPOperformance,andinvestinginthecollectionofuniquepromptsand
propermixingismoreimportantfordownstreamevaluations.
Unused Prompts Lead to Higher Performance vs. Reusing Prompts From SFT Mix. We then compare
includingnewpromptsandre-usingpromptsfromtheSFTstageontheireffectondownstreamDPOperformance.
Todoso,wesampled100kpromptsfromtheSFTdatasetmixthatwereusedduringtraining(asshowninTable6)
andcompareitagainstpromptsfromthesameopendatasets(e.g.,OpenAssistant,SciRIFF,Aya,Persona,WildChat,
etc.) wesubsampledfrombutleftunusedduringSFT.Figure10showsthattheunuseddatasethasaslightlyhigher
performanceasopposedtoreusingprompts. Thissuggeststhatthepresenceofnewpromptscanhelpimprove
downstreamDPOperformance. Asseeninourbestmix,combiningunusedandreusedpromptsseemstoleadto
thebestresult.
On-policyDataImprovesDownstreamDPOPerformance. Weinvestigatewhethertheinclusionofon-policy
data, i.e., text generations from the SFT model that will be used as the base model for preference finetuning,
improvesdownstreammodelperformance. GiventhesamesetofpromptssourcedfromtheSFTmixin¬ß4,we
generatepreferencesfromoff-policymodelsandcomparedittoamixthatisstrictlyon-policy(i.e.,oneofthe
responseisalwaysfromtheInitial8BSFTmodel,andtheotherresponseisfromtheoff-policymodels). Wealso
compareitonacombinationofbothon-policyandoff-policydata: wesampleinstancesfromthestricton-policy
datasetandaddittotheoff-policydatasetsothattheresponsesfromeachmodelisdistributedequally. Figure11
showsthatincludingon-policydataimprovesaggregateddownstreamDPOperformancecomparedtoacompletely
off-policydatasetwherepromptcompletionsweresampledfromothermodels.
Figure10: EffectofreusingpromptsfromSFTmix Figure11: Effectofincludingon-policydatadur-
andnewpromptsfromthesamedatasetssubsampled ingtheResponseGenerationstageofthesynthetic
fortheSFTdatasetmix. preferencedatapipelineondownstreamDPOmodel
performance.
PerformanceAcrossLLMJudgesareSimilar,withGPT-4oLeadingSlightlyAhead. Inordertodetermine
whichjudgetouseforobtainingpreferenceannotations,wetestseveralcommercialandopen-sourceLLMjudges
suchasGPT-4(GPT-4-turbo-2024-04-09,GPT-4o-2024-08-06,gpt-4o-mini-2024-07-18)andLlama3.1
(70B and 405B) on the same set of 10k randomly-sampled Ultrafeedback prompts and responses. In general,
GPT-4o,Llama3.1405B,andGPT-4turboperformsimilarlyacrossallbenchmarks,withGPT-4oleadingslightly
aheadontheaggregatedaverageperformanceasshowninTable16. InthesyntheticpreferencepipelineforT√úLU3,
weoptedforGPT-4o-2024-08-06duetoitsease-of-use,cheapercostperrequest,andbatchinferencespeedvia
OpenAI‚ÄôsBatchAPI.12
GoingBeyondUltrafeedback. Previousworkonpreferencelearningusingopenlyavailabledatasetshasshown
thattheUltraFeedback[Cuietal.,2023]preferencedatasetgenerallyoutperformsotherpreferencedatasets[Ivison
etal.,2023]. InFigure12weshowthatwewereabletosignificantlysurpassDPOtrainingonUltraFeedbackby
12https://platform.openai.com/docs/guides/batch
23LLMJudge Avg. MMLU TQA PopQA BBH CHU CHU+ GSM8k Drop MATH IFEval AE Safety
GPT-4o 57.3 64.8 56.1 30.1 66.3 87.0 80.7 75.3 62.7 20.3 60.4 20.6 62.7
LLama3.1405B 57.2 64.8 56.0 30.3 67.4 86.2 80.8 75.1 62.0 20.1 59.0 21.5 62.8
GPT-4Turbo 57.0 64.6 55.7 30.1 66.4 86.6 79.4 75.5 62.6 20.1 59.9 20.6 62.2
GPT-4oMini 56.9 64.4 55.4 30.4 66.2 86.6 79.8 74.8 60.7 20.9 60.1 21.4 61.6
Llama3.170B 56.6 64.3 55.5 30.2 66.6 85.3 81.4 74.8 62.1 20.1 58.2 18.6 62.2
Table16: PerformanceofDPOmodelstrainedonpreferenceannotationsbydifferentLLMjudges
Figure12: EffectofdifferentDPOmixon8Band70Bmodels: UltraFeedback,Helpsteer2,andourbestpreference
mix.
trainingonourbestmix. Theimprovementisgreaterforthe70Bmodel(+3.3vs. +1.8),wehypothesizethatthis
isbecauseUltraFeedback‚Äôscompletionsaremainlysourcedfrommodelsthatarelesscapablethanthe70Bmodel
wearestartingwith. Helpsteer2Wangetal.[2024d],anotherhigh-qualitypreferencedataset,alsoperformslower
thanourbestmixonthe8Bmodel.
PersonaPreferenceData. Fromthethreepersonapreferencedatasetstargetinginstructionfollowing,codingand
mathskills,onlyT√úLU3PersonaIFimprovestheaverageevalscoreandthetargetedIFEvalscore(seeFigure13).
NeitherT√úLU3PersonaMathnorT√úLU3PersonaCodeimprovetheirrespectivetargetedevaluationsandslightly
harmtheaveragescore. WethereforeonlyincludetheT√úLU3PersonaIFpreferencesinourfinalmix.
TargetingIF.Wecreatedpreferencedatatargetedtoimproveamodel‚Äôspreciseinstructionfollowingskills.
1. Persona IF: We take a subset of our collected instruction following SFT dataset, IF-PERSONA-SFT
andconvertitintoapreferencedataset. EachexampleinIF-PERSONA-SFTdatasetcontainsa(prompt,
constraints,response)tuple. Westartbyrewritingeachpromptinthesubsettorelaxoneofthegiven
constraints. Morespecifically,wepromptGPT-4otogeneraterewritessuchthatthenewresponsetothe
modifiedpromptisnolongeravalidresponsefortheoriginalprompt(doesnotsatisfyalltheconstraints).
Wethenusetheresponsetothenewmodifiedpromptastherejectedresponse,andcreate(chosen,rejected)
pairstoformourIF-PERSONA-PREFdatasetcontainingcloseto20Kexamples.
2. IF-augmented: WerandomlysampleinstructionsfromtheT√úLU2SFTmixandcombinethemwith
constraintsfromthetaxonomyinZhouetal.[2023]. Thechosenandrejectedcompletionsareobtained
throughthesyntheticpipelinein¬ß5.2.1.
3. WildChatIF:WesampleinstructionsfromWildChat[Zhaoetal.,2024]whichcontainconstraints. For
thispurposeweaskedGPT-4toextractwhetherornotapromptincludesaconstraint.
ForIF-augmented,weruntwoanalyses. Wegenerateanadditionalsetofmorethan66kinstancesandwethen
run the chosen completions through constraint verifier functions, and only add those instances to the final set
whichactuallyfulfilledtheconstraint(s). Thisleavesuswithacleanedsetofabout26kpreferences,whichwe
callIF-augmented-verified. InFigure14weshowthattheIF-personapreferencessignificantlyimproveIFEval
scoresbeyondthebaselinemix,whileminimallyharmingaverageperformance. TheIF-augmented-verifieddataset
improvesIFEvalperformanceonlyby1point,whilealsoslightlyharmingtheaverageperformance. Combining
IF-personawithIF-augmented-verifiedleadstothebestIFEvalperformance,buttoaslightlyloweraverage. We
2486.1 84.4
82.6 81.9
79.1
74.7
72.1 72.3
60.5 60.8 58.7 60.1
40.9 40.9 41.2
31.5
Average IFEval MATH CHU
Figure13: AddingpersonapreferencedatatotheSFTReusedmixforDPO.
Originaldataset
Average IFEval Regen.usingsyntheticpipeline
Figure 14: Performance of different IF-targeted preference Figure15: Comparingtheuseoftheoriginal
mixes,averageandIFEval. Besthereconsistsofourfinalbest completionstoregeneratingcompletionsusing
mixforthe8Bmodel(minusPersona-IF). oursyntheticpreferencepipeline.
thereforechoosetoincludeIF-augmented(notverified)andPersonaIFinthefinal8BDPOmix,whichleadsto
bothasatisfiyingaverageandIFEvalscore.
Wildchat. OurablationsshowthataddingpreferencedataconsistingofWildChatpromptsandchosen/rejected
pairsobtainedusingourSyntheticPreferenceDataPipeline,generallyimprovesDPOperformance. Ablationsin
Figure5.2.2revealthataddingWildChatpromptsseenduringSFTtrainingtotheDPOmixleadstobetteraverage
performancethancombiningtheunusedwiththereusedWildChatprompts.
Comparingoriginalpreferencedatasetsandtheirregeneratedcounterparts. Wealsoinvestigatewhetherthe
preferencedatasetgeneratedbythesyntheticpipelinein¬ß5.2.1canyieldtogainsindownstreamDPOperformance
onexistingdatasets. Todoso,wetakethepromptsfromopen-sourcedatasetssuchasHelpsteer2,Ultrafeedback,
and MultiPref [Miranda et al., 2024], then regenerate their completions and preference annotations using the
syntheticdatapipeline. Figure15showsthatthedownstreamDPOperformanceoftheregenerateddatasetisbetter
thantheoriginaldataset,suggestingthatthesyntheticpipelineitselfcanyieldtoperformancegains.
25
)%(
ecnamrofrePAlgorithm LR ùõæ ‚àíùõΩ ratio ùõΩ Epochs BatchSize AverageScore
SFTBase - - - - - 55.7
SimPO 5.00E-07 0.5 2 1 128 51.8
SimPO 5.00E-07 0.3 10 1 128 52.9
DPO 5.00E-07 - 0.1 3 32 55.2
PPO 1.00E-06 - 0.0325 1 64 54.5
PPO 1.00E-06 - 0.05 1 64 55.5
DPO-norm 1.00E-07 - 5 3 32 56.1
DPO-norm 5.00E-07 - 10 3 32 55.2
DPO-norm 5.00E-07 - 15 3 32 55.7
DPO-norm 5.00E-07 - 2 3 32 46.8
DPO-norm 5.00E-07 - 5 3 32 53.4
DPO-norm 5.00E-07 - 5 1 32 57.3
Table 17: Hyperparameters and algorithms examined for DPO tuning. We use UltraFeedback as the training
datasetinallcases,andtrainontopofanearlyT√úLU3version. DPO-normreferstothelength-normalizedDPO
variantproposedinMengetal.[2024]. Weexplorehyperparameterssuggestedbypriorwork[Mengetal.,2024,
Ivisonetal.,2023]. ForPPO,wetrainrewardmodelsonUltraFeedbackandreusepromptsduringonlinetraining,
followingthehyperparametersinIvisonetal.[2024]. Wefindthatlength-normalizedDPOperformsbestoverall.
LR Avg. Performance Hyperparameter 8BValue 70BValue
5.0√ó10-7 72.74 LearningRate 5√ó10-7 2.0e-7
2.0√ó10-7 71.17 LearningRateSchedule Linear Linear
1.5√ó10-7 71.12 BatchSize(effective) 32 128
1.0√ó10-7 71.06 MaxTokenLength 2,048 2,048
5.0√ó10-7 71.14 KLpenaltycoefficientùõΩ 5 5
2.0√ó10-7 74.35 Warmupratio 0.1 0.1
Table18: Learningrateablationsforthe70BDPOmodel, NumberofEpochs 1 1
fortwodifferentpreferencemixes: Mix1: T√ºlu-3-Persona-
Table19: FinalDPOTrainingHyperparameters.
IF,Tulu-3-Helpsteer2,Ultrafeedback,Tulu-3-SFT-reused
Weusethelength-normalizedvariantofDPOpro-
(On-policy),Mix2: Best70BMix(bothtrainedonanolder
posedinMengetal.[2024].
SFTbase).
5.4 PreferenceTuningRecipeandAnalyses
5.4.1 HyperparameterandAlgorithmDesign
InlightofthesignificantamountofworkonimprovingDPOandrelatedalgorithmssincethereleaseofT√úLU2,we
revisitedourhyperparameterandalgorithmchoicesalongsideourpreferencedatasets. Weablatedbothalgorithm
andhyperparameterchoicesusinganearlySFTcheckpointandtheUltraFeedbackdataset. Weexploredusing
DPO,SimPO[Mengetal.,2024],andlength-normalizedDPO.OurresultsareshowninTable17. Wefoundthat
onlylength-normalizedDPOoutperformedourbasecheckpointoverall,andsofurthertunedit,resultinginthe
finalhyperparametersshowninTable19.
Weloweredthelearningrateandincreasedthebatchsizeforthe70Btrainingbasedonthefactthatitiscommonto
lowerthelearningrateandincreasebatchsizewhendoingSFTwithlargermodels[Touvronetal.,2023].
LearningRateAblationsfor70B.Weranasmallhyperparametersearchoverasetofleraningratesusinga
generallywellperformingpreferencedatamix13andourfinalbestmix. Table18showsthateitheralearningrate
of2.0√ó10-7or5.0√ó10-7,dependingondatamix,performsbetterthanalowerlearningrate. ForourfinalDPO
modelswedecidedonusingalearningrateof2.0√ó10-7.
13T√ºlu-3-Persona-IF,Tulu-3-Helpsteer2,Ultrafeedback,Tulu-3-SFT-Used(On-policy)
26Hyperparameters foroptimizingaRM foroptimizingagainstRLVR
LearningRate 3√ó10-7 3√ó10-7
DiscountFactorùõæ 1.0 1.0
GeneralAdvantageEstimationùúÜ 0.95 0.95
Mini-batchesùëÅ 1 1
mb
PPOUpdateIterationsùêæ 1 4
PPO‚ÄôsClippingCoefficientùúÄ 0.2 0.2
ValueFunctionCoefficientùëê 0.1 0.1
1
GradientNormThreshold 1.0 1.0
LearningRateSchedule Linear Linear
GenerationTemperature 1.0 1.0
BatchSize(effective) 512 512
MaxTokenLength 2,048 2,048
MaxPromptTokenLength 2,048 2,048
PenaltyRewardValuefor
ResponseswithoutanEOSToken -10.0 -10.0
ResponseLength 1,024 1,024(2,048forMATH)
TotalEpisodes 300,000 100,000
KLpenaltycoefficient(ùõΩ) [0.05,0.03,0.02,0.01] [0.1,0.05,0.03,0.01]
Warmupratio(ùúî) [0.1,0.0] 0.0
Table20: ThehyperparametersofPPOusedfor1)optimizingagainstageneralRMand2)optimizingagainstthe
verifiablerewardfunction. Thedifferencesbetweenthehyperparametersarehighlighted.
ComparisonBetweenPPOandDPOWealsoconductedamoreindepthablationstudycomparingPPOand
DPOlaterindevelopment. WeanchoredaDPOpreferencemixinthedevelopmenthistorytotrainanRM.We
usethesamesetupasStiennonetal.[2020],Ouyangetal.[2022],Huangetal.[2024a],weonlyextracttheRM‚Äôs
logitsattheend-of-sequence(EOS)tokenastherewardmodel. Also,thelinearheadtooutputrewardscalarsis
‚àö
initializedwithweightsaccordingtoÓà∫(0,1‚àï (ùëë model+1)). WeusethesamepromptsintheDPOpreferencemix
tomakeacontrolledcomparisonbetweenDPOandPPO.
TherewardmodelwastrainedonlyonceandwedidnotattempttotunetheRM‚Äôsperformance. EvaluatingRM‚Äôs
performance can be tricky because strong RM performance on RM-specific benchmarks does not necessarily
translatetobetterdownstreamperformanceforPPO[Ivisonetal.,2024,Chenetal.,2024]. Furthermore,iterating
withRMandPPOismoreexpensivetheniteratingwithDPO,sowedecidedtodomostofourpreferencetuning
experimentsviaDPO.ThehyperparametersfortheRMandPPOcanbefoundinTable31andTable20. The
resultscanbefoundinFigure16.
Hereareourfindings:
1. PPOGetsSimilarAverageScoreswithDPOinthisNon-TunedSetupOverall,wefoundthatPPO
couldreachacomparablelevelofperformancetoDPO(albeitslightlylower)inthiscontrolledsetup.
2. PPO is More Computationally Expensive The PPO runtime is roughly 28 hours using two nodes,
whereastheDPOruntimeisabout4hoursusingasinglenode.
Ifweusemorecomputationalbudgetordomoretuning, itisentirelypossiblethatwecanpushupthePPO‚Äôs
performanceevenhigher. However,givenlimitedresourcesandthesubtletyinRMevaluation,usingDPOfor
preferencetuningseemsmoreeconomical. WedecidetousePPOprimarilyforRLVR,tobeintroducedinSection6.
5.4.2 InfrastructureforScalingDPO
Torunthe70BDPOtraining,wefounditusefultoimplementtwokeyoptimizationsforreducingtheGPUfootprint
ofDPOtraining:
27Figure16: TheaveragescoresofPPOrunswithdifferent Figure17: ThepeakGPUmemoryallocatedcanbere-
learningratewarm-upratiosùúî,KLpenaltycoefficient ducedbycachingthereferencepolicy‚Äôslogprobsonthe
ùõΩ. PPOcangetsimilar(thoughslightlylower)average preferencedatasetanddoingforwardpassesseparately
scoresasDPO. forthechosenandrejectedpairs.
PromptDataset Count Verification Reference
GSM8KTrain 7,473 Exactmatchagainstextractedanswer Cobbeetal.[2021]
MATHTrain 7,500 Exactmatchagainstextractedanswer Hendrycksetal.[2021]
IFverifiable 14,973 Prompt-specificverifiers -
Total 29,946
Table21: Summaryofourverifiablepromptdataset. NewdatasetsreleasedwithT√úLU 3arecolor-codedfor
emphasis.
1. CachingDPOLogProbsToreduceGPUmemoryusage,wepre-computeandcachelogprobabilities
acrossthedatasetusingtheinitialmodel,ratherthankeepingareferenceDPOmodelinmemoryduring
traininglikethecanonicalimplementation[vonWerraetal.,2020,Rafailovetal.,2024]. Thisoptimization
eliminatestheneedtoallocateGPUmemoryforthereferencemodel.
2. SeparateForwardPassesforChosenandRejectedSequencesThecanonicalDPOimplementation[von
Werraetal.,2020,Rafailovetal.,2024]alsoconcatenatesthechosenandrejectedsequencesduringthe
forwardpass,effectivelydoublingthebatchsizeandincreasingGPUmemoryrequirements. TosaveGPU
memory,wesimplyperformtheforwardpassesseparatelyonthechosenandrejected.
WeempiricallyvalidatedthesetwotechniquesontheLlama3.1modelandfoundtheyresultedinnearidentical
traininglosses. Asexpected,themodeluseslessGPUmemorywhenusingthetwotechniquesonan8xH100,as
showninFigure17.
6 ReinforcementLearningonVerifiableRewards
InT√úLU3,weintroduceReinforcementLearningwithVerifiableRewards(RLVR),anovelmethodfortraining
languagemodelsontaskswithverifiableoutcomessuchasmathematicalproblem-solvingandinstructionfollowing.
RLVRleveragestheexistingRLHFobjectivebutreplacestherewardmodelwithaverificationfunction,asshown
conceptuallyinFigure18. Whenappliedtodomainswithverifiableanswers,suchasmathematicsandverifiable
instructionfollowingtasks[Zhouetal.,2023],RLVRdemonstratestargetedimprovementsonbenchmarkslike
GSM8Kwhilemaintainingperformanceacrossothertasks. RLVRcanbeseenasasimplifiedformofexisting
approachesforbootstrappingLMreasoning[Zelikmanetal.,2022,2024,Hoffmanetal.,2023]orasimplerform
of RL with execution feedback [Gehring et al., 2024], in which we simply use answer matching or constraint
verificationasabinarysignaltotrainthemodel. Whilethishasbeendoneforimprovingmathskillsaloneinprior
work[Kazemnejadetal.,2024],wefurtherextendRLVRtocovermultipleevaluationsandtesthowitcanimprove
overallmodelperformance,integratingitasacomponentofageneralisttrainingpipeline.
28Verifiable Reward
Scalar

Reward
Policy
Training Completions
data
Prompts
Policy Update
Figure18: AnoverviewofhowReinforcementLearningwithVerifiableRewards(RLVR)works. Wesample
completionsfromapolicymodelgivenasetofprompts,andverifytheircorrectnessusingadeterministicfunction.
Iftheanswerisverifiablycorrect,weproviderewardofùõº,otherwise0. Wethentrainagainstthisrewardusing
PPO.
RLVRisbasedonasimpleprinciple,commoninRLliterature,appliedtolanguagemodels: thepolicyonlyreceives
arewardwhenitsgeneratedresponsesareverifiablycorrect. Morespecifically,RLVRoptimizesthefollowing
objective:14
m ùúãaxùîº
ùë¶‚àºùúã
ùúÉ(ùë•)[ ùëÖ RLVR(ùë•,ùë¶)] =[ ùë£(ùë•,ùë¶)‚àíùõΩKL[ùúã ùúÉ(ùë¶ |ùë•)‚Äñùúã ref(ùë¶ |ùë•)]] (7)
ùúÉ
whereùë£istheverifiablerewardfunction. ùë£takesinapromptandcompletionpair(ùë•,ùë¶),andchecksiftheansweris
correctwithinthegeneratedtext:
{
ùõº ifcorrect,
ùë£(ùë•,ùë¶)= (8)
0 otherwise.
WetrainmodelswithRLVRfollowingpreferencefinetuning,andweusethePPO[Schulmanetal.,2017]algorithm
tooptimizefortheRLVRobjective. Wesetùõº =10basedonpilotexperimentsanddidnottuneitfurther.
6.1 RLVRData
CreatingdataforRLVRentailsobtainingpromptswithanaccompanyingbinaryverifier(i.e.,constructingaset
ofinputsùë•withaccompanyingverifierfunctionsùë£). Wefocusontwodomains(mathematics,exactinstruction
following)andthreeevaluations(GSM8K,MATH,IFEval)withrelativelystraightfowardmethodsforverification,
andleavemorecomplexverifierstofuturework.15 Inpractice,theanswerextractionandverificationmethodis
domain-dependent. Weusethreesourcesoftrainingpromptsandverifiers:
GSM8K.WeusetheGSM8ktrainingset. Weaugmenteachsamplewiththestandard8-shotpromptusingduring
evaluationtoencouragethemodeltousechain-of-thought,andthenextractthefinalnumberproducedandcompare
totheground-truthlabeltodeterminecorrectness.
MATH.WeusetheMATHtrainingset. SimilartoGSM8k,weaugmenteachsamplewiththestandard3-shotCoT
promptusedtoencouragethemodeltogeneratechainsofthoughtduringevaluation,andthenextracttheanswer
anddeterminecorrectnessfollowingthe‚Äòflex‚ÄôMATHevaluationlogic.
IFEval. WerandomlysampleinstructionsfromtheT√úLU2SFTmixandcombinethemwithconstraintsfromthe
taxonomyinZhouetal.[2023]. Wehaveaverificationfunctionforeachoftheconstrainttemplatesthatisableto
verifywhetheracompletionsatisfiesaconstraint.
Given these prompts and verification functions, we then train the models via Proximal Policy Optimization
(PPO)[Schulmanetal.,2017]ontheseverifiablerewards. Wecombineallpromptstogether,resultsinamixtureof
roughly30,000promptswithgroundtruthlabels. WesummarizeourverifiablepromptmixtureinTable21.
14ThisisverysimilartothestandardKL-constrainedRLHFobjective,showninEq.4,witharewardfunctioninsteadofa
learnedrewardmodel.
15Forexample,recentworkhasfoundsuccessinusingcodeexecutionfeedbacktotrainmodelswithRL[Gehringetal.,2024,
Xuetal.,2024b].
296.2 RLVRRecipeandAnalyses
ImplementationDetails. RLandRLHFhavemanysubtleimplementationdetailsthatcansignificantlyimpact
training stability [Engstrom et al., 2020, Huang et al., 2022]. As we use PPO to train our models against our
verifiers,weadapttheseeffectiveimplementationdetailsfromHuangetal.[2024a]:
1. InitializetheValuemodelfromaGeneralRM.Weinitializethevaluemodelfromtherewardmodelfor
theRLVRsetup(followingthestandardsetupinZiegleretal.[2019],Ouyangetal.[2022]).
2. DisableDropout. Wesetthedropoutprobabilitytobe0duringRMandRLtraining(similartoZiegler
et al. [2019]). This ensures the token log probabilities can be computed deterministically during the
forwardpassesofthepolicymodelandreferencemodel,allowingamoreaccurateestimationoftheKL
penalty. Furthermore,PPOcalculatesthetokenlogprobabilitiesintwophases: duringtherolloutphase
andthelearningphase. Itisimportanttoensurethetokenlogprobabilitiesmatchupduringthesetwo
phases: theyproduceaprobabilityratioof1duringthefirstPPOepoch,soPPOcancliptheratioand
applythesurrogateobjectiveproperly. Ifthelogprobabilitiesdifferdrasticallyduetodropout,allratios
couldpotentiallybeclipped,resultinginzerogradient.
3. TrainwiththeSFTDatasetandShuffleBetweenEpochs. AspointedoutininHuangetal.[2024a],
PPOcantrainformoreepisodesthanthetotalavailableprompts,effectivelytrainingformultipleepochs.
In our RLVR ablation experiments, we train for roughly 100000‚àï7473 ‚âà 13 epochs. We shuffle the
promptsinbetweenepochs. Forourfinalruns,weexaminemodelcheckpointsevery40-100stepsand
choosethebestcheckpointonourdevelopmentevaluationset.
4. NonEnd-of-Sequence(EOS)Penalty. Duringtraining,PPOtypicallysampleafixedamountofmaximum
tokens. IfthesampledresponsedoesnotendwithanEOStoken,wegivea-10penaltytoencouragethe
modeltoalwayscompleteitsresponses.
5. AdvantageWhitening/Normalization. LikedoneinstandardPPOimplementationdetailsliterature[En-
gstrometal.,2020,Huangetal.,2022,2024a], wenormalizetheadvantagesbysubtractingitsmean
followedbydividingitsstandarddeviation.
WestartbyanchoringaDPOmodelinthedevelopmenthistoryastheinitialmodelforconductingexperimentsfor
RLVR.Weconductedseveralsetsofexperiments:
1. IndividualTasks. weappliedtheRLVRrecipeonGSM8K,MATH,andIFEval,respectivelywitha
sweep of beta values [0.1,0.05,0.03,0.01]. For evaluation, we look at the verifiable rewards, the KL
divergence,andtheresponselength.
2. Value Model Initialization Ablation. We experimented with initializing PPO‚Äôs value model
from 1) a general reward model and the anchored DPO model, and ran a sweep of beta values
[0.1,0.05,0.03,0.01,0.005,0.001]ontheGSM8Ktask. ThegeneralRMistrainedwiththeUltraFeedback
dataset[Cuietal.,2023]. Forevaluation,weexaminetheGSM8Ktestevaluationscoreandtheaverage
scoresacrossallevaluation.
3. ScoresfromRMAblations. OnepossibleimplementationforRLVRistoaddverifiablerewardson
top of the scores from the reward model. We launch experiments also using a sweep of beta values
[0.1,0.05,0.03,0.01,0.005,0.001]ontheGSM8Ktask.
4. StartingfromWeakerModel. Themodel‚Äôsbasecapabilitiesarealsoaconfoundingfactor. Welaunch
anothersetofexperimentsusinganSFTmodelwithloweraveragescores(fromwhichtheanchoredDPO
checkpointsweretrained,sotheysharethesamelinearage)usingbetavalues[0.1,0.05,0.03,0.01].
Unlessotherwisespecified,weusethefollowinghyperparametersforPPOtraininginTable20. Totrainreward
models,weusethehyperparametersinTable31.
6.2.1 KeyFindings
Belowareourmainfindings:
RLVRCanImprovePerformanceinTargetedDomains. Figure19showstrainingwithRLVRresultsinimproved
testperformanceforallthreesettings. Inallcases,weachievemodelsthatoutperformtheinitialmodelinthat
particularevaluation. Wealsoseethattheverifiablerewards(i.e.,correctnessonthetrainset)improvesconsistently
forallthreesettings. Interestingly,inGSM8KandMATH,wefoundthatincurringmoreKLbudgetdoesnot
necessarilyleadtoimprovementsinverifiablerewards.
30RLVR on GSM8K
20
9.0 100
8.5 15 95
8.0 10 90
7.5 85
5
7.0 80
0
0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K
RLVR on Math
4.75 600
40
4.50 500
4.25 30
400 4.00 20
3.75 300
10
3.50
200
3.25 0
0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K
RLVR on Prompts with Constraints
25 250 8
20 200
6 15 150
10
4 100
5
2 0 50
0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K
Episodes
ùõΩ =0.01 ùõΩ =0.03 ùõΩ =0.05 ùõΩ =0.1
Figure19: ThetopthreerowsshowRLVR‚Äôsverifiablerewards,KLdivergence,andresponselengthsonthetrain
datasetofGSM8K,MATH,andpromptswithconstraints,whenstartingfromaDPOcheckpoint. Thebottomrow
showsthecorrespondingdownstreamtestperformance. RLVRcanleadtohigherverifiablerewardsinthetrain
datasets. Importantly,RLVRcanalsoleadtohigherscoresinthecorrespondingtestdataset,however,anincrease
intheaveragescoreacrossallevaluationsisnotguaranteed.
InitializingRLVR‚ÄôsValueFunctionfromaGeneralRMWorksBest. Figure21showsthatinitializingthe
valuefromageneralRMobtainsthehighestGSM8Ktestscoreandalsohigheraveragescores. Thissuggeststhe
valuefunctionplaysanimportantroleinRLVR‚Äôstraining.
DoNotUsetheScoresfromRM.Figure22showsthatusingonlytheverifiablerewardsoutperformsusingscores
fromtherewardmodel. TrainingwithverifiablerewardswiththescoresfromRMseemstointroducemorenoise,
especiallyintheaveragescores.
StartingfromaWeakerModelCanConvergetotheSameVerifiableRewards. Figure20showsthatstarting
frombothSFTandDPOcanleadtothesamelevelofverifiablerewards,butstartingfromtheSFTmodelwould
incuralargerKLcomparedtostartingfromtheDPOmodel. ThismakessensebecausetheSFTmodelisfurther
awayfromgoodatGSM8KthantheDPOmodel. However,wefindthatstartingfromastrongermodelusually
resultsinbettertestsetperformance.
31
sdraweR
elbaifireV
sdraweR
elbaifireV
sdraweR
elbaifireV
ecnegreviD
LK
ecnegreviD
LK
ecnegreviD
LK
htgneL
esnopseR
htgneL
esnopseR
htgneL
esnopseRRLVR on GSM8K
9 100
20 8 95
15
90 7
10
85
6
5
80
5 0
0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K 0K 20K 40K 60K 80K 100K
Episodes
start=DPO,ùõΩ =0.03 start=DPO,ùõΩ =0.05 start=DPO,ùõΩ =0.01 start=DPO,ùõΩ =0.1
start=SFT,ùõΩ =0.03 start=SFT,ùõΩ =0.05 start=SFT,ùõΩ =0.01 start=SFT,ùõΩ =0.1
Figure20: ThecomparisonofRLVR‚ÄôsperformanceonGSM8KbetweenstartingfromaDPOcheckpointand
startingfromaweakerSFTcheckpoint. WeseethatstartingfrombothSFTandDPOcanleadtothesamelevelof
verifiablerewards,butstartingfromSFTwouldincuralargerKLcomparedtostartingfromDPOwhenusingthe
sameùõΩ.
Figure 21: The performance of RLVR vs KL Diver-
Figure22: SimilartoFigure21,butthisisacomparison
genceunderdifferentvaluemodelinitialization. Both
of1)usingscoresfromontopoftheverifiablerewards
initializationscouldimprovethemodelscomparedto
and2)usingonlytheverifiablerewards. Wefoundusing
aninitialDPObaselinemodel. However,asthemodel
theverifiablerewardsperformsbetterinGSM8K,and
divergesmorefromtheinitialmodel,overoptimization
usingscoresandverifiablerewardstobemorenoisy.
happensastheaveragescoresdropsignificantly.
OverestimationHappens. AswelowertheKLpenaltyùõΩ, thetrainedmodelincursmoreKLfromtheinitial
model. WeobservedthatmoreKLdivergencetypicallyresultsinloweraveragescores,asshowninFigure21. The
exceptionisFigure22,wherethelargestKLcorrespondstothehighestaveragescore. Furthermore,weshowcase
overoptimizationofpromptswithconstraintscasesinAppendixB.4.
6.3 RLVRInfrastructure
OurPPOsetupfollowsbestpracticesonimplementationdetails[Huangetal.,2024a]. Toenableourimplementation
toscaletomodelsuptoandabove70Bparameters,wealsoadaptedmodelallocationtechniquesfromexisting
distributedRLHFframeworks[Huetal.,2024]. Furthermore,weacceleratethroughputbymakingRLtraining
asynchronous[Noukhovitchetal.,2024].
DistributedSetup. OutPPOinfrastructureleveragesZeroStage3[Rajbhandarietal.,2020]tofitthemodelsand
applicableoptimizerstatesintothememory. InRLVR,wehave3models: thepolicymodel,thereferencepolicy
model,andthevaluemodel. Thepolicyandvaluemodelsneedtobetrained,butthereferencepolicymodelonly
performsinference. OfteninferencetimeisabottleneckinRLHFinfrastructure,soweallocatededicatedGPUsto
doinferencelikedoneinOpenRLHF[Huetal.,2024]. Inparticular,weuseRay[Moritzetal.,2018]toallocate
dedicatedGPUstorunPagedAttentionviavLLM[Kwonetal.,2023]. PagedAttentionhelpsreduceGPUmemory
fragmentationandredundantduplicationleveragingvirtualmemoryandpagingtechniques. Asaresult,ithelpsour
32
sdraweR
elbaifireV
ecnegreviD
LK
htgneL
esnopseRModelSize 8B 70B
Category Benchmark
Llama3.1 T√úLU3 T√úLU3 Llama3.1 T√úLU3 T√úLU3
(EvalSetting) Inst. DPO RLVR Inst. DPO RLVR
Avg. 62.2 64.4 64.8 73.4 75.9 76.0
Knowledge MMLU 71.2 68.7 68.2 85.3 83.3 83.1
(0shot,CoT)
PopQA 20.2 29.3 29.1 46.4 46.3 46.5
(15shot)
TruthfulQA 55.1 56.1 55.0 66.8 67.9 67.6
(6shot)
Reasoning BigBenchHard 62.8 65.8 66.0 73.8 81.8 82.0
(3shot,CoT)
DROP 61.5 62.5 62.6 77.0 74.1 74.3
(3shot)
Math MATH 42.5 42.0 43.7 56.4 62.3 63.0
(4shotCoT,Flex)
GSM8K 83.4 84.3 87.6 93.7 93.5 93.5
(8shot,CoT)
Code HumanEval 86.3 83.9 83.9 93.6 92.4 92.4
(pass@10)
HumanEval+ 82.9 78.6 79.2 89.5 88.4 88.0
(pass@10)
IF&Chat IFEval 80.6 81.1 82.4 88.0 82.6 83.2
(Strict)
AlpacaEval2 24.2 33.5 34.5 33.4 49.6 49.8
(LC%win)
Safety Safety 75.2 87.2 85.5 76.5 89.0 88.3
6taskavg.
Table22: FinalperformanceofRLVR-trainedT√úLU3modelscomparedtoLlama3.1andDPOstartingpoints.
Thebest-performingmodeloneachbenchmark(i.e.,ineachrow)andofeachsizeisbolded.
PPOinfrastructurerunLLMinferenceusingamuchlargerbatchsizeandspeedupinference. Thissetupallowed
ustoscalePPOpolicytrainingtothe70Bscale.
AsynchronousRLTraining. Furthermore,ourPPOsetupusesasynchronousRLtrainingtoimprovetraining
efficiency [Noukhovitch et al., 2024]. The existing PPO frameworks [Hu et al., 2024, Shen et al., 2024a] are
typicallysynchronous: theinferenceGPUswouldfirstcollectpolicyrolloutdata,andthenthelearnerGPUswould
trainonthatrolloutdata. ThissetupwouldutilizeGPUrelativelywellunderatypicalsetupwithAtari[Mnihetal.,
2015]. However,undertheRLHFsetup,inferencecomputationtypicallyrequiresdifferentoptimizations(e.g.,
PagedAttention),thusrequiringdifferentGPUmemoryallocationstrategies. Asaresult,synchronousRLtraining
meansinferenceGPUscouldbeidlingwhilethelearnerGPUsrun,andviceversa. Analternativeimplementation
istousethesamesetofGPUsfortrainingandinference,butitcouldincuradditionaloverheadsuchasanadditional
copyofthepolicyparametersandcompilinginferenceengineinthetrainingGPUs[Shenetal.,2024a].
OursetupallocatesGPUsspecificallyforinferenceandtraining,alleviatingtheneedtocompileaninferenceengine
andsaveacopyofthepolicyparametersinthetrainingGPUs. Furthermore,weruntheinferencecomputation
concurrentlywiththetrainingcomputation,thusreducingtheGPUidletime. However,asynchronousRLtraining
canintroducestaledata,incaseswheretheinferencecangeneratedatamuchfasterthantrainingconsumes[Es-
peholtetal.,2018],whichcanintroducereproducibilityproblems[Huangetal.,2023]. Tohelpmaketraining
morereproducible,oursetupalwaystrainsthepolicyusingthesecondlatestinferencedata[Huangetal.,2023,
Noukhovitchetal.,2024].
6.4 FinalExperimentalResults
Basedontheaboveresults,weranourfinalRLVRrunsusingthecombinedverifiablepromptset,andusedthebest
DPOmodelsfromthepriorsectionasstartingpoints. Specifically,at8Bscale,wetestedthebestoverallDPO
modelandthebestmodelwithIFEvalpersonadatamixedinduringtraining,andat70Bscaleweusethebest
overallDPOmodel. Forhyperparameters,for8Bmodels,weusedthehyperparametersfromTable20,buttested
higherKLpenaltycoefficients(upto0.15)basedonprevious8BRLdevelopmentruns. For70Bmodels,weused
thehyperparametersfromTable20,butwitha1√ó10-7 learningrate,0.1warmupratio,2048responselength,
400,000episodes,640effectivebatchsize,andùõΩ =0.7basedonprevious70BRLdevelopmentruns. Weinitialize
ourvaluemodelfromarewardmodeltrainedonthesamedatasetasthebestDPOmodel(theT√úLU38Bpreference
mixture)startingfromT√úLU3SFT,usingthesamehyperparametersasinourablationexperiments(Table31).
Weevaluatedourmodelsevery100trainingsteps(40for70B),andpickedasourfinal8Bmodelthecheckpoints
withbestoverallperformanceonMATHandIFEval. WeshowthelogsfromRLVRtraininginFigure23,and
33Final RLVR Runs
6.5 25 600
20
6.0 500
15
5.5 10 400
5.0 5 300
0
0K 10K 20K 30K 40K 50K 0K 10K 20K 30K 40K 50K 0K 10K 20K 30K 40K 50K
Episodes
Final8Brun Final70Brun
Figure23: Rewards,KLdivergence,andaverageresponselengthonGSM8ktestsetoverepisodesforourfinal
RLVRruns. Wemarkthepointwechoosethe8Band70Bcheckpointswithagreenandpurplestarrespectively.
comparethefinalperformanceagainsttheirDPOstartingpointsandLlama3.1inTable22. RLVRresultsin
non-trivialimprovementsatthe8Bscale,improvingallthreeofMATH,GSM8k,andIFEval. Infact,weobserved
thatsome8BrunswereabletoachieveGSM8kscoresofupto89.4%andIFEvalscoresofupto84.8%(although
suchmodelstendedtoperformworseinothermetrics,draggingdowntheiroverallaverage). Atthe70Bscale,we
observemoremodestimprovementsinIFEvalandMATH,andnoimprovementinGSM8k,likelyduetothefact
thatitisalreadyclosetosaturation(93.5%). Surprisingly,wefindthatour70BrundisplaysextremelylowKL
divergence,remainingwellbelow1overthedurationofrun,probablyduetothelowerlearningrate16.
7 T√úLU 3EvaluationFramework
WedesignedourframeworkforevaluatingT√úLU3andtheothermodelswecompareagainstwiththefollowing
goals: 1)Ourevaluationsshouldbereproducible. 2)Weshouldevaluatemodels‚Äôgeneralizationtounseentasks,
notjustthespecificbenchmarksweusefordevelopment. 3)Ourevaluationsetup(e.g.,templatesandstrategiesfor
prompting)shouldbefairtoawiderangeofmodels.
Accordingly,ourframeworkconsistsofanopenevaluationtoolkitforreproducibleevaluations(Section7.1),a
suiteforevaluatingcoreskillsininstruction-tunedmodelswithseparatedevelopment(Section7.2)andheld-out
evaluations(Section7.3),andasetofrecommendedsettingsforevaluatingonourevaluationsuitethatbasedonour
experimentswithvariousmodels,whichwerefertoastheT√úLU3EvaluationRegime,summarizedinTable23.
AsdescribedinSection2.1,wesplitourevaluationsuiteintoadevelopmentsetandanunseenset,theformer
usedfordevelopingmodels,andthelatteronlyforevaluatingfinalmodels. Thissetup,alongwithourtraining
datadecontaminationefforts(seeSection3.2)provideafairerevaluationofourmodelsgeneralizationcapabilities
foreachofthecoreskillswefocusduringdevelopment. However,incomparisonsbetweenourmodelsandother
models,wecannotruleoutthatanyclosedmodelhasnottrainedonourevaluationsuite,andhencecannotmake
clearjudgmentsonmodelsthathavenotpubliclyreleasedfinetuningdata.
7.1 OpenLanguageModelEvaluationSystem(OLMES)
Inanefforttomakeevaluationsmorestandardizedandreproducible,wearesharingthecodebaseusedtoproduce
theevaluationsinthiswork.17 TheOLMESevaluationsystemsupports:
‚Ä¢ Awiderangeofmodelsandtasks,leveragingexistingworkintheEleutherAILMEvaluationHarness[Gao
etal.,2024]
‚Ä¢ Flexibleconfigurationoptionsforeachtask
‚Ä¢ Direct access to the specific task formulations used in this work (as well as in earlier work such as OLMo
[Groeneveldetal.,2024]andtheOLMESstandard[Guetal.,2024])
‚Ä¢ Detailedinstance-leveloutputdataforanalysisofmodelpredictions,confidences,etc
E.g., to reproduce our Llama-3.1-8B-Instruct numbers for MMLU-Pro, one would simply run something like
‚Äúolmes ‚Äìtask mmlu_pro::tulu3 ‚Äìmodel llama3.1-8b-instruct‚Äù.
16WehadattemptedusingahigherlearningrateduringtheinitialexplorationbutfoundthatKLcouldexplodeinitiallyand
causeanon-trivialdropinaveragescores.
17Seehttp://github.com/allenai/olmes
34
sdraweR
elbaifireV
ecnegreviD
LK
htgneL
esnopseRMultiturn
Category Benchmark CoT #Shots Chat Metric
ICL
KnowledgeRecall MMLU ‚úì 0 ‚úì ‚úó EM
PopQA ‚úó 15 ‚úì ‚úì EM
TruthfulQA ‚úó 6 ‚úì ‚úó MC2
Reasoning BigBenchHard ‚úì 3 ‚úì ‚úì EM
DROP ‚úó 3 ‚úó N/A F1
Math GSM8K ‚úì 8 ‚úì ‚úì EM
MATH ‚úì 4 ‚úì ‚úì FlexEM
Coding HumanEval ‚úó 0 ‚úì N/A Pass@10
HumanEval+ ‚úó 0 ‚úì N/A Pass@10
InstructionFollowing IFEval ‚úó 0 ‚úì N/A Pass@1(prompt;loose)
AlpacaEval2 ‚úó 0 ‚úì N/A LCWinrate
Safety T√úLU3Safety ‚úó 0 ‚úì N/A Average‚àó
KnowledgeRecall MMLU-Pro ‚úì 0 ‚úì N/A EM
GPQA ‚úì 0 ‚úì N/A EM
Reasoning AGIEvalEnglish ‚úì 0 ‚úì ‚úì EM
Math DeepmindMathematics ‚úì 0 ‚úì ‚úì EM(Sympy)
Coding BigCodeBench ‚úó 0 ‚úì N/A Pass@10
InstructionFollowing IFEval-OOD ‚úó 0 ‚úì N/A Pass@1(prompt;loose)
HREF ‚úó 0 ‚úì N/A Winrate
Table23: TheT√úLU3EvaluationRegime: settingsfordevelopment(top)andunseen(bottom)portionsofthe
evaluation suite. CoT are evaluations run with chain of thought prompting [Wei et al., 2022b]. #Shots is the
numberofin-contextexamplesintheevaluationtemplate. Chatreferstowhetherweuseachattemplatewhile
promptingthemodel. MultiturnICLreferstoasettingwherewepresenteachin-contextexampleasaseparate
turninaconversation(applicableonlywhenachattemplateisusedand#Shotsisnot0). ‚àóAverageovermultiple
sub-evaluations‚ÄìfulldetailsofthesafetyevaluationareincludedinSection7.2.1.
7.2 T√úLU3EvaluationSuite-Development
WedesigntheevaluationsetupforthedevelopmentpartitionofT√úLU3EVALbasedonexistingpracticesincurrent
literature as well as insights during development. Where appropriate, we adapt the evaluation setup based on
thenatureofthetaskandtakeadditionalcaretorobustifyouranswerextractionandcomparisonapproachesas
describedbelow.
MMLU[Hendrycksetal.,2020]isheterogeneouswithregardtothetypeofreasoningskillsrequiredtoanswer
thequestions,containinginstancesthatrequirebasicfactualrecallaswellasthosethatdemandlogicalreasoning
andproblem-solvingskills. Wedesignazero-shotCoTsettingthatasksthemodelsto‚Äúsummarize‚Äùitsreasoning
beforeansweringthequestions(seeTable39forthepromptused). WefindthatamongthevariousCoTsettings,
includingpromptingthemodeltothink‚Äústep-by-step‚ÄùandusingnoCoT,thepromptyieldsasystematicperformance
improvementoverstandard5-shotmultiplechoicesettingacrosstestedmodelsTable41andmaximizesonthe
numberofsubject(knowledgecategories)thattheCoTbenefits. Thisindicatesthatour‚Äúsummarize‚Äùpromptisan
effectivestrategyfordealingwiththeheterogeneityofthebenchmark. SeeappendixF.2fordetails. Wecomputea
macroaverageoverallthesubjectsinMMLUasthefinaltaskmetric.
PopQA[Mallenetal.,2022]isanentity-centricquestion-answeringbenchmarkthatevaluateslanguagemodels‚Äô
tendencytoforgetinformationaboutlong-tailentities. Wepromptthemodelsina15-shotsetting(asrecommended
inthedatasetpaper)withoutanyadditionalinstructions, witheachQAdemonstrationpresentedinadifferent
conversationturn,asettingthatwerefertoasMultiturnICLhenceforth,andusegreedysamplingtoobtainmodel
predictions.
TruthfulQA[Linetal.,2021]containsquestionsthathumanstendtoanswerincorrectlyduetocommonmiscon-
ceptions. Weusethemultiple-choiceversionofthisbenchmarkwherethemodelsbeingevaluatedarepresented
withquestionsandoptionscontainingmultiplecorrectanswers(theMC2setting).
35
tnempoleveD
neesnUHumanEval[Chenetal.,2021]andHumanEval+[Liuetal.,2023]evaluatemodels‚ÄôabilitytocompletePython
codegivendocstrings. HumanEval+usesamorerigorousevaluationprocedurethantheoriginalHumanEval
benchmarkwithadditionaltests. Weusesamplefromthemodelsatatemperatureof0.8andusepass@10asthe
evaluationmetric.
GSM8K[Cobbeetal.,2021]containsgradeschoolmathwordproblems. Weusethe8-shotchain-of-thought
promptfromWeietal.[2022b],formattedforamultiturnICLevaluation. Weobtainmodelresponsesusinggreedy
samplingandextractthelastnumberinthemodelresponseasthepredictedanswer.
MATH[Hendrycksetal.,2021]containsproblemsfrommathematicscompetitionsspanningvariouscategories
suchasalgebraandcalculus. Weusea4-shotmulti-turnsetupwithCoTfromLewkowyczetal.[2022],formatted
formultiturnICL,andgreedysamplingformodelcompletions. Fordeterminingthecorrectnessofpredictions,
we use a ‚Äòflex‚Äô scheme that attempts to extract the answer in three different ways: (1) following the minerva
format[Lewkowyczetal.,2022];(2)findingthelastinstanceof‚Äò <ùëéùëõùë†> ‚Äô;(3)takingthetextbetweenthelast
two‚Äò$‚Äôtags. Thisisduetoissueswefoundduringdevelopmentwhereinmodelswouldoftennotfollowthecorrect
outputformatdespitethefew-shotexamples,necessitatingavariedanswerextractionstrategy. Wefindthatmoving
fromtheminervaformatalonetoour‚Äòflex‚Äôstrategycansometimesimprovereportedscoresbyupto10points,
highlightingtheneedforthisflexiblestrategy. Wecomputemacroaverageacrossthesubsectionstoobtainthefinal
taskmetric.
BigbenchHard[Suzgunetal.,2022]containschallengingreasoningproblemsforwhichmodelsbenefitfrom
step-by-stepreasoning. Wefollowthesetupdescribedintheoriginalpaperanduse3-shotCoTprompts,formatted
formultiturnICL.Weusegreedysamplingforobtainingmodelpredictions.
DROP[Duaetal.,2019]isareadingcomprehensiontaskthatrequiresdiscretereasoning. Wedraw3random
few-shotexamplesfromthetrainsplitfollowingthesetupusedforLlama3[Dubeyetal.,2024],andusegreedy
samplingtogetmodelpredictions.
IFEval[Zhouetal.,2023]evaluatesinstructionfollowingabilityofmodelsinasettingwhereeachinstruction
corresponds to constraints such that the it can be programmatically verified whether the outputs satisfy those
constraints. Weusegreedydecodingtogeneratemodeloutputsgivetheinstructionsinthedataset,andmeasurethe
prompt-levelaccuracyofsatisfyingconstraintsinthelooseevaluationsetting.
AlpacaEval2[Duboisetal.,2024]containsasetofpromptssourcedthatreflectrealhumanusagesofLMs,and
comparesmodeloutputstoGPT-4turboresponseswithanadditionallengthcontroltoavoidlongeranswersbeing
unfairlyfavoured. Wegenerateresponsesusinggreedydecodingupto8,192tokensinlength,followingIvison
etal.[2023].
7.2.1 SafetyEvaluation
We follow Han et al. [2024] and Jiang et al. [2024] to define our safety evaluation suite using the following
benchmarks18. Eachofthesebenchmarksevaluateswhethermodelsrefusetorespondtounsaferequests,andin
thecaseofXSTestandWildJailbreak,additionallyevaluatewhethertheycomplywithbenignrequests. Weuse
greedysamplingtogetmodelresponsesforthepromptsineachofthebenchmarks,andcomputetheaccuracy(at
refusalorcomplianceasappropriate). Wereportthemacroaverageofthescoresoverallthebenchmarksasour
finalsafetyevaluationmetric.
XSTest[R√∂ttgeretal.,2023]consistsof200unsafepromptsand250promptswhicharesafebutsuperficially
resembleunsafeprompts: thesepromptsusevocabularysimilartothatofunsafeprompts. Categoriesinclude
homonyms,figurativelanguage,safetargets,safecontexts,definitions,realdiscrimination/nonsensegroup,nonsense
discrimination/realgroup,historicalevents,publicprivacy,andfictionalprivacy. Wereporttheoverallaccuracy
scorebasedonwhetherWildGuard[Hanetal.,2024]classifiestheresponseasarefusalorcompliance.
HarmBench[Mazeikaetal.,2024]. Weevaluatedonasubsetofharmfulpromptswhichconsistsof321harmful
prompts19categorizedintoFunctionalandSemanticcategories. Functionalcategoryincludestwotypesofbehavior:
Standard behaviors, which are modeled after existing datasets of harmful behaviors such as AdvBench and
the TDC 2023 Red Teaming Track dataset, and Copyright behaviors, which test the handling of copyrighted
content. Thesemanticcategorycomprisesseventypesofharmfulbehaviors: cybercrime,unauthorizedintrusion,
18BuiltonAi2SafetyTool,forkedtofocusonlyonsafetyevals:https://github.com/nouhadziri/safety-eval-fork.
NotethatwhileWildGuardandWildJailbreakarepopulartrainingdatasets,weusegenerationsoverthetestsetpromptsasa
testofcompliancewitharesponsefiltersuchastheWildGuardmodel.
19https://github.com/centerforaisafety/HarmBench/blob/main/data/behavior_datasets/harmbench_
behaviors_text_test.csv
36Benchmarks T√úLU38B T√úLU38B T√úLU38B Llama3.18B Ministral8B Qwen2.57B
SFT DPO Instruct Instruct Instruct
HarmBench 98.4 94.4 94.7 82.8 53.4 84.1
XSTest 90.4 92.4 93.3 92.7 85.6 91.8
WildGuardTest 99.2 98.9 98.5 86.2 68.1 85.0
Jailbreaktrigger 95.8 87.0 85.5 78.8 63.3 71.0
DoAnythingNow 88.3 69.7 62.0 45.0 16.0 61.7
WildjailbreakTest 86.7 81.1 78.8 65.6 50.7 56.2
Overall 93.1 87.2 85.5 75.2 56.2 75.0
Table24: BreakdownofsafetyscoresbybenchmarkofT√úLU38Bmodelscomparedwithsimilarlysizedopen
weightmodels.
Benchmarks T√úLU370B T√úLU370B T√úLU370B Llama3.1 Qwen2.5 Hermes3 Nemotron
SFT DPO 70B 72B Llama3.1 Llama3.1
Instruct Instruct 70B 70B
HarmBench 98.8 97.8 97.8 80.6 86.3 54.7 84.4
XSTest 91.1 94.9 92.4 87.1 93.6 89.3 92.0
WildGuardTest 99.1 99.2 98.9 81.3 93.1 66.6 84.9
Jailbreaktrigger 95.3 87.0 86.7 71.0 89.8 56.3 60.5
DoAnythingNow 93.7 69.0 67.7 80.0 93.3 26.7 36.3
WildjailbreakTest 88.6 86.3 86.2 59.2 66.0 53.8 56.05
Overall 94.4 89.0 88.3 76.5 87.0 57.9 69.0
Table25: BreakdownofsafetyscoresbybenchmarkofT√úLU370Bmodelscomparedwithsimilarlysizedopen
weightmodels.
chemical/biologicalweaponsordrugs,copyrightviolations,misinformation/disinformation,harassment/bullying,
illegal activities, and general harm. We use WildGuard to evaluate the model‚Äôs refusal to assist with harmful
prompts.
Do-Anything-Now[Shenetal.,2024b]consistsofjailbreakpromptsthatwerecreatedbycombiningthejailbreak
templatesfromDANwithharmfulbehaviorsfromHarmBench,andsubsample300ofthemfortesting. Wereport
accuracyusingtheWildGuardclassifier.
JailbreakTrigger[Huangetal.,2024b]incorporatespromptsbasedon13distinctjailbreakattackmethods. In
total,thedataset20consistsof400examples,splitevenlybetweentwocategories: "QuestionsaboutBadBehaviors"
and"InstructionstogenerateToxicContent". ThisdatasetservestoevaluatetheeffectivenessofLLMs‚Äôdefensesand
measuresthetoxicityofresponsesunderjailbreakscenarios. ThereportedmetricisRTAmeasuredbyWildGuard.
WildJailbreakTest[Jiangetal.,2024]isanadversarialevaluationsetwhichcontainsasubsetforadversarial
benignqueries(210examples)andasubsetforadversarialharmfulqueries(2000examples). Theadversarial
benignqueriesareusedtomeasuremodels‚Äôexaggeratedsafetybehaviorsandtheadversarialharmfulqueriesare
usedtomeasuremodels‚Äôsafeguardsregardingadversarialattacks. WemeasureRTAusingWildGuardforboth
categories. Forbenignqueries,RTAisexpectedtobe(‚Üì)andforharmfulqueries,theRTAisexpectedtobe(‚Üë).
WildGuardTest[Hanetal.,2024]contains1725itemsforpromptharm,responseharm,andresponserefusal
classification tasks. 55% are vanilla prompts, and 45% are adversarial. The prompts are collected based on
adversarialsyntheticdataandin-the-wilduser-LLM(In-the-wild)interactions. WereportRTAusingWildGuard.
7.3 T√úLU3EvaluationSuite-Unseen
Fortheunseenevaluationsuite,thetaskformulationsweredecidedthroughanindependentdesignprocessfrom
thatofdevelopmentsuite. Onegoaloftheunseensuiteistoevaluateinstruction-tunedmodelsinwaysthatare
closelyalignedtorealisticusage. Specifically,wefollowthesegeneralprinciples:
20https://huggingface.co/datasets/TrustLLM/TrustLLM-dataset
37Model Llama 3.1 prompt Ours
Gemma29BInst 51.6 52.6
Gemma29BInst-SimPO 52.6 51.8
Llama3.18BInst 49.2 48.7
Llama3.23BInst 39.1 39.7
Ministral24108BInst 43.8 44.3
OLMo07247BInst 26.1 22.9
OLMoE09241B7BInst 20.7 20.3
Qwen2.57BInst 56.2 54.2
Tulu2DPO7B 25.4 22.2
Table 26: Comparing evaluating on MMLU-Pro using our 0-shot CoT prompt and the 5-shot prompt used in
Llama3.1evaluations,usingmacroaverageovertasksinbothcases(theLlama3.1evaluationusedmicroaverage).
Model Llama 3.1 prompt Ours
Gemma29BInst 35.7 35.5
Gemma29BInst-SimPO 35.0 35.7
Llama3.18BInst 29.5 29.5
Llama3.170BInst 46.2 44.0
Llama3.23BInst 33.5 27.7
Ministral24108BInst 31.0 31.5
OLMo07247BInst 27.2 27.9
OLMoE09241B7BInst 24.6 24.8
Qwen2.57BInst 32.1 36.8
Tulu2DPO7B 28.4 27.5
Table27: ComparingevaluatingonGPQAusingour0-shotCoTpromptandthepromptinLlama3.1.
‚Ä¢ Formulatetaskssimilartohowhumansinteractwiththemodels. E.g.,avoidfew-shotexamplespresentedasa
dialog,orprecisechain-of-thought(CoT)examplesforhowthemodelis‚Äúsupposed‚Äùtothink.
‚Ä¢ Promptmodelswithclearinstructionsthatsetthecontext,encourageconcisereasoning,andspecifyhowthe
finalanswershouldbeformatted.
‚Ä¢ Applyreasonableheuristicsforanswerextractionandcomparisontogoldanswers,toavoidpenalizingmodels
thatdonotexactlyfollowasyntaximpliedbytheinstructions(basedonexaminingoutputsfromawidesetof
existingbaselinemodels).
Wefirstapplytheseprinciplestosomeofthetasksinthedevelopmentsuiteusingasetofexploratorymodels
(instruction-tunedmodelspredatingT√úLU3). Ontheexploratorymodels,wefindthatfollowingtheaboveprinciples,
inparticularbeingmorealignedtohowhumanusage,generallydoesnotdegradeperformanceandoftentimes
allowmostmodelstoperformbetterontasks(despite,e.g.,removingfew-shotexamples). Wedidnotupdatethe
formulationofthedevelopmenttasksbasedonthis,butcarriedtheprinciplesovertoformulatingtheunseentasks.
MoredetailedanalysiscanbefoundinAppendixF.2.
ThetaskformulationsintheT√úLU3unseensuiteareasfollows. Forallbenchmarkswhichincludemultiplesub
tasks,wecomputetheaverageoverthesubtasks(i.e.,the‚Äúmacro‚Äùaverage)tobeconsistent.
AGIEvalEnglish[Zhongetal.,2024]includestheEnglishlanguagesubsetoftheAGIEvalbenchmark,specifically
thesemultiple-choicetasks: aqua-rat,logiqa-en,lsat-ar,lsat-lr,lsat-rc,sat-en,sat-math,andgaokao-english. We
donotincludethesat-en-without-passagetaskaswefindthesequestionsaretypicallyseverelyunderspecified
withoutaccesstothepassage. Weformulatethetaskusingtheasimple‚Äúzero-shotCoT‚Äùpromptwhichencourages
concisereasoningendingwithaclearlystatedanswerchoice. InAppendixF.2,Figure41,weprovidethe0-shot
reasoningprompt,usedforthemultiple-choicetasksintheunseenevaluationsuite,includingAGIEvalEnglish.
38Model base-adapted, in context examples CoT prompt
Gemma29BInst 18.0 45.9
Gemma29BInst-SimPO 19.3 45.3
Llama3.18BInst 20.0 39.4
Llama3.21BInst 11.6 13.1
Llama3.23BInst 19.2 32.6
Ministral24108BInst 18.8 36.7
OLMo07247BInst 3.2 5.8
OLMoE09241B7BInst 9.0 4.2
Qwen2.57BInst 21.2 54.7
Tulu2DPO7B 9.6 6.0
Llama3.1Tulu28B 21.7 13.6
Llama3.1Tulu2DPO8B 18.6 14.5
Table28: ComparingevaluatingonDeepMindMathematicsusingdifferentpromptsdesignedevaluationpractices
forinstruction-tunedmodels,withminorvariants. Boldednumbersindicatecaseswhereapplyingprinciplesmore
alignedwithrealusageleadstobetterperformanceonmodels. Inourevaluationsuite,thechainofthought(CoT)
promptisreferredtoaschat-v3.
Themodel‚Äôsanswerchoiceisextractedbyfirstmatchingtotherequestedformat,withfallbackpatternsiftheformat
wasnotfollowedprecisely. Specifically,wefirstlookfortheexactphraseindicatedintheprompt(‚ÄúTherefore,the
answeris[ANSWER]‚Äù)andgrabthelastsuchmatch. Ifthatfails,welookforasequenceofsoftervariants,like
‚Äúansweris[ANSWER]‚Äùor‚Äúanswer: [ANSWER]‚Äùbeforefallingbacktothelastletterinparenthesisfound,andif
thatfails,thelaststand-alonecapitalletter.
MMLU-Pro[Wangetal.,2024b]isa10-waymultiple-choiceextendedversionoftheMMLUdataset. Weuse
essentiallythesamepromptandanswerextractionasusedforourAGIEvalsetup,justadjustingforthenumberof
answerchoices. Wegenerallyfindourformulationtobeaseffectiveasthetraditional5-shotCoTprompt(e.g.,
usedinLlama3.1evaluations[Dubeyetal.,2024])despitebeingmuchshorter,morerealistic,andeasierforusers
tocreate(seeTable26foracomparisonacrossexploratorymodels).
GPQA[Reinetal.,2023]isasetofverychallengingmultiple-choicequestionswrittenbydomainexpertsin
biology,physics,andchemistry. Weusethesamezero-shotpromptandanswerextractionasforAGIEval. Thisis
similartotheapproachusedinLlama3.1evaluations,butwithlessprescriptiononhowthereasoningshouldbe
structured. Figure27showsthatacrossexploratorymodels,thatourapproachleadstocomparablescores.
DeepmindMathematics[Saxtonetal.,2019]isadatasetof56categoriesofmathquestions,testingmathematical
andalgebraicreasoningskills. Wedeviseda‚Äúzero-shotCoT‚Äùpromptthatsetsthecontextofthetask,explainsthe
formatexpectedfortheanswers(e.g.,‚Äúx**2‚Äùforpowers),andforeachcategoryincludedthreeexampleanswersto
specifytheanswerformat. Thepromptusedandanswerextractionheuristicswererefinedbyexaminingexample
outputsfromourexploratorymodels. AsseenfromTable28,applyingevaluationprinciplesalignedwithreal
usage(‚Äúchat‚Äùversions),oftenperformsbetterthanusingasetupinvolvingpresentingfew-shotexamplesadapted
frombasemodelevaluations(‚Äúbase-adapted‚Äù). Weprovidethezero-shotreasoningpromptfortheDeepmind
MathematicstaskinAppendixF.2,Figure42. Alignedwiththeinstructionsforformattingintheprompt,when
extractingtheanswer,wefirstlookforananswerintheformat‚ÄúTherefore,thefinalansweris[answer]. Ihopeitis
correct.‚Äù Welookforthisinacase-insensitiveway,processthe[answer]bystrippingawayanytrailingperiod,
andknownmathdelimiterssurroundingtheanswer(e.g.,‚Äú$‚Äù)basedonoutputsfromourexploratorymodels.
Foranswercomparison,wefirstcomparetherawstringsoftheprocessedgeneratedanswerandgoldanswerto
checkiftheyareequal. Inaddition,weparsebothusingtheSymPy[Meureretal.,2017]packageandcomparethe
equivalenceoftheparsedoutputstochecktheirmathematicalequivalent. Wealsoconsiderthegeneratedanswerto
becorrectifitisequaltotheparsedgoldanswerafterthisparsing.
BigCodeBench[Zhuoetal.,2024]isasetofcodingchallenges. Wefocusonthe‚Äúhardsubset‚Äùof148(outofthe
total1140)instances,usingthe‚Äúinstruct‚Äùformulationofeachtaskandthe‚Äúcalibrated‚Äùscore. Wefollowthesetup
usedfortheoriginalleaderboardforourimplementation.
39InstructionGroup Instruction Description
count person_names Mention at least {N} different person names in the re-
sponse.
format emoji Pleaseuseanemojiattheendofeverysentence.
ratio stop_words Ensurethatstopwordsconstitutenomorethan{percent-
age}%ofthetotalwordsinyourresponse.
sentence keyword The response must include keyword {keyword} in the
{N}-thsentence.
words alphabet Eachwordinyourresponsemuststartwiththenextletter
ofthealphabet,loopingbackto‚ÄòA‚Äôafter‚ÄòZ‚Äô.
custom csv_special_character GenerateCSVdata: Thecolumnnamesare["ProductID",
"Category","Brand","Price","Stock"],thedatashould
becommadelimited. Pleasegenerate14rows. Addone
fieldwhichcontainsaspecialcharacterandencloseitin
doublequotes.
Table29: ExamplesofIFEvalout-of-distributionconstraints. ConstraintsareaddedtoanunseenWildChatprompt
toformthefinalpromptexceptforinthe"custom"instructiongroup. Acompletelistofconstraintsisprovidedin
AppendixF.3.
7.3.1 NewEvaluation: IFEval-OOD
InordertotestpreciseinstructionfollowingabilitiesofLLMsandwhethertheyareabletofollowconstraints
that go beyond the 25 constraints included in IFEval [Zhou et al., 2023], we developed IFEval-OOD (IFEval
Out-of-Distribution). IFEval-OODconsistsof52constraintsacrosssixbroadcategoriessuchastheexamples
inTable29. AcompletelistoftheconstraintscanbefoundinAppendixF.3. Aportionoftheconstraintswere
sourcedbyaskingagroupofresearchersforconstraintideasandotherswerewrittenbyauthorsofthispaper. One
ofthesixcategories(‚Äúcustom‚Äù)comprisesmanuallywrittenverifiablepromptstocoverspecificsub-skills,suchas
CSVgeneration. Fortheremainingfivecategories,theverifiableconstraintwascombinedwith10unseenprompts
fromWildChat. Toselectthefinalprompts, theseconstrainedpromptswerehumanannotatedforqualityand
compatibilitywiththeconstraint(e.g. aprompttoparaphraseaone-sentencereferencetextthatdoesn‚Äôtcontain
anynamestobeginwithwouldnotbecompatiblewithaconstrainttomentionatleast23differentpersonnames
intheresponse). Toensureconstraintcoverage,additionalunseenWildChatpromptsweremanuallypairedwith
constraintsthatdidnothaveatleastfiveselectedprompts. Ourevaluationdatasetemphasizesbroadcoverageof
constrainttypestodifferentiateconstraint-followingabilitiesfromoverfittingontheexistingIFEvalconstraints.
7.3.2 NewEvaluation: HREF
Weconstructedanautomaticevaluationofinstructabilityoflanguagemodels,calledHumanReference-guided
Evaluation of instruction Following (HREF). HREF focuses on 11 instruction following tasks that language
modelsaretypicallytrainedon,namely,Brainstorming,OpenQA,ClosedQA,Extraction,Generation,Rewriting,
Summarization,Classification,NumericalReasoning,Multi-documentSynthesis,andFactChecking. Weobtained
highqualityhuman-writtenpromptsandresponsesinthesecategoriesfromprofessionalinstruction-tuningdata
creators,anddevelopedareliableautomaticevaluationprocedure. FollowingAlpacaFarm[Duboisetal.,2023],
weusewin-rateagainstafixedbaselinemodelastheevaluationmetric. However,sincewehavealargernumber
oftasksinourbenchmarkthanAlpacaFarm,andalsobecausewehaveaccesstohuman-writtenreferences,we
hypothesizedthatthesetupusedforwin-ratecomputationmaynotbedirectlyapplicabletoournewevaluation. So
weexperimentedwithvariouswin-ratecomputationmethods,andtheirdetailsalongmultipledimensions:
‚Ä¢ ChoiceofLMJudge. WetriedGPT-4,GPT-4turbo,andLlama3.1-Instructmodelsat7Band70BsizesasLM
judges.
‚Ä¢ LM-as-a-Judgevs. DirectComparisonwithHumanReferences. Usingalargelanguagemodele.g.,GPT-4
TurboforAlpacaFarm,maynotbeappropriatefortaskswheretheresponsesareexpectedtobeshortandfactual.
Wecomparedthissetupwithdirectlycomparingthesimilarityoftheresponsesfromthetargetandthebaseline
modelswiththehuman-writtenreferences,accordingtoasimplerembeddingmodel(RoBERTa-Large[Liu,
2019]).
40Llama Hermes Llama Hermes
Skill Benchmark(eval) 3.18B 3Llama
T√úLU3
3.170B 3Llama
T√úLU3
8B 70B
Instruct 3.18B Instruct 3.170B
Avg. 37.5 30.5 33.1 51.4 42.8 46.4
KnowledgeRecall GPQA(0shot,CoT) 29.2 32.8 32.8 46.9 42.4 45.1
MMLUPro(0shot,CoT) 49.1 40.9 44.3 68.3 60.3 65.8
Reasoning AGIEvalEnglish(0shot,CoT) 64.2 58.1 59.3 77.8 73.3 75.0
Math DeepMindMath(0shot,CoT) 39.3 28.3 35.4 62.4 50.0 49.8
Coding BigCodeBench-Hard(Pass@10) 15.5 9.5 7.4 26.4 14.2 21.6
InstructFollowing IFEvalOOD(Promptloose) 26.4 18.0 19.7 32.7 22.5 25.4
HREF(Winrate) 38.5 26.2 32.7 45.6 36.8 42.3
Table 30: Evaluation of a selection of open-weight and T√úLU 3 models on our unseen evaluation suite. It is
importanttonotethatwithoutopentrainingdataforanyoftheothermodelsthatwecannotverifythattheyarenot
trainingonanyoftheunseenbenchmarks.
‚Ä¢ UseofHuman-WrittenReferencesWhenEvaluatingwithLM-as-a-Judge. Sincewehaveaccesstohuman-
writtenreferences,weexperimentedwithincludingthoseasadditionalcontextwhilepromptingtheLMjudge.
Tomakedecisionsabouttheevaluationsetup,wecollectedhumanjudgmentscomparingresponsesfromadiverse
setof16models,whenpromptedusingtheinstructionstakenfromasubsetofourevaluationdataset. Wecollected
fourhumanjudgmentspereachmodelresponsepair,andcomparedtheagreementofeachevaluationsetupwith
majorityhumanjudgments. Werepeatedthisprocedureforeachtaskcategorytoidentifythebestsetupforthat
category.
SummaryoftheFinalEvaluationSetup. WeusedLlama3.170BInstructasourLMjudgesinceitsagreement
withhumanjudgeswasthehighestofthepoolofmodelsweconsidered. OurbaselinemodelweusedisLlama
3.1 405B Instruct. We found that using LM as a judge results in higher human agreement (compared to the
embedding-based method) in all subtasks except Open QA and Fact Checking. We use embedding similarity
with human-written references as the way of computing win-rates in these two subtasks. Of the remaining 9
subtasks,evaluationinBrainstormingandSummarizationdidnotbenefitfromusinghuman-writtenreferences
whilepromptingtheLMjudge,andhencepromptLMjudgeswithhumanreferencesonlyfortheremaining7
subtasks. Onthesubsetforwhichwecollectedhumanjudgments,ourcompositeevaluationprocedureresultedin
anagreementof69.4%withhumans,whichiscomparabletotheinter-humanagreementof67%.
7.4 EvaluationontheUnseenSuite
Toevaluatehowmuchandinwhatwaysweover-fittoourdevelopmentevaluations,wemeasuretheperformance
ofourmodelsonunseentasksthatstillcorrespondtothesamesetofcoreskills. Table30showsacomparison
betweenT√úLU3models,Llama3.1Instructmodels,andHermes3Llama3.1modelsat8Band70Bsizes,and
Table43showsasubtask-levelbreakdownoftheperformanceofthesemodelsonHREF.Itisimportanttonotethat
whilealltheseevaluationsareunseenfortheT√úLU3models,wedonotknowifGPQA,MMLU-Pro,AGIEval,
DeepMindMath,andBigCodeBenchwereusedfordevelopingthetwoothermodels. Wesummarizebelowour
keytakeawaysfromthiscomparisonandfromourqualitativeanalysisoftheoutputsofT√úLU3modelsonthese
datasets:
T√úLU3generalizeswelltounseenevaluations. Inalmostalltheevaluations,T√úLU3‚Äôsperformanceisgenerally
comparabletothatofthetwoothermodelsweevaluate,oftenfallingbetweentheperformancenumbersofthe
twomodels. Thissuggeststhatourrecipeofchoosingrepresentativeevaluationsforeachcore-skillandcurating
trainingdatasetstargetingthoseevaluationscanleadtomodelsthatgeneralizewelltoothertasksthatrequirethe
sameskills.
TrainingforMATHdoesnottransferwelltoDeepmindMath. OnedifferencebetweenMATH,whichispart
ofourdevelopmentevaluationsandDeepMindMath,whichisinourunseenevaluationsisthattheformeroften
requiressolutionsandanswerstobeoutputinLaTeXformat,whilethelatterdoesnot. Wefoundthatourtrained
modelshavethetendencytoformatthechain-of-thoughtreasoningandthefinalanswersinLaTeXevenforthe
questionsintheDeepmindMathdatasetwherethisisnotrequired. Thisofteninterferedwiththeintermediate
reasoning,andalsomadeouranswerextractionlogicfail. Wehypothesizethatthisisaconsequenceofmodels
over-fittingtotheformattingintrainingdatasetsthattendtobeparticularlyhelpfulforMATH.
41ModelsgenerallyoverfittoIFEval. WefindthatthereisasignificantdifferencebetweenperformanceonIFEval
andIFEval-OODofallthemodels,eventhoughwecreatedthelattertobestructuredverysimilartotheoriginal
dataset,justwithadisjointsetofconstraints. Weobservethatinstructionfollowingwithverifiableconstraintsisa
challengingskillformodelstolearneffectively,andhypothesizethatthosemodelsthatdowellonIFEvalarelikely
overfittingtothespecificsetofconstraintsincludedinthedataset,makingithardforthemtogeneralizetonew
constraints.
Generalizationonknowledgerecallmaybedependentontheposttrainingrecipes. Asonemightexpect,the
performanceofmodelsonMMLUandMMLU-Proiscorrelated. WeseethattheirperformanceonGPQAshowsa
differenttrend‚Äîallthreemodelswecompareherearepost-trainedfromthesamebasemodel,suggestingthatthe
post-trainingrecipemayaffectthegeneralizationinknowledge-recall.
Instructionfollowingperformancevariesacrosscategories. WeobservethattherelativeperformanceofT√úLU3
modelsonAlpacaEvalisdifferentfromthatonHREF.Thismaybeexplainedbythefactthatinstructionfollowingis
ahighlydiversetask,andthedistributionsofHREFandAlpacaEvalmaydiffer,withsomecategoriesofinstructions
notnecesarilytransferringwelltoothers,leadingtotheshiftinrelativeperformance. WedonotethatT√úLU370B
outperformsLlama3.170BInstructon5outof11subtasks,asseeninTable43intheAppendix,whichshowsa
breakdownofmodelperformanceacrosssubtasksinHREF.Futureworkwillexplorehowdifferentbehaviorsof
instructionfollowingcanbemeasuredwithmorediverseinstructionfollowingevaluations.
8 Discussions
8.1 InsightsfromtheUnfruitful
Inthissection,wediscussanumberofmethodsandapproachesweconsideredforT√úLU3butdidnotultimately
makeitintoourfinalrecipeforavarietyofreasons.
OnlineDPO.StandardDPOmethodsusepreferencedatasetsthatareusuallycollectedaheadoftime,oftenfroma
distinctlanguagemodel,andarethusconsideredasoffline. Inotherwords,withDPO,thepolicycannotobtain
feedbackoveritowngenerationsduringtraining. ThisisincontrasttoonlinemethodslikePPOwheretheRM
providesonlinefeedbacktogenerationsfromthepolicyùúã beingtrained. Tomitigatethedistributionalshiftissue,
ùúÉ
recentworksproposedOnlineDPO[Guoetal.,2024]followingathree-stepprocess: (1)sample2responsestoa
promptfromthecurrentpolicy;(2)obtainonlinefeedbackovertheresponsepairtocreatepairwisedata,and(3)
usethispairwisedataupdatethepolicyùúã viastandardDPOloss. Whiletheoriginalpaperproposedusingonline
ùúÉ
AIfeedbackforstep2,tobetterscaleourexperiments,weobtainfeedbackfromatrainedrewardmodel.
WetriedonlineDPObothtoenhancegeneral,andtargetcapabilities,i.e.,mathematicalreasoning. Forgeneral
capabilities,wetrainanRMfor1epochusingthe82KpreferencedatapointsfromtheSkywork.21 Fortargeting
mathematicalreasoning,wecontinuetrainingthesameRMonoursyntheticon-policymath-specificpreference
data(describedin¬ß5.2.1). TrainingonlineDPOontopofoneofourT√úLU3DPOcheckpointfortotalof200K
episodesonmathproblems(promptsaretakenfromthesameRMtrainingdata),resultedinnoorlittleimprovement
onGSM8KanddegradationonMATHperformance(weexperimentedwithvarioussamplingtemperaturesandKL
penaltycoefficients). Wedidnotfurtherexplorethisapproachextensively,asourinitialresultssuggestedlimited
gainsinbothgeneralandtargeteddomains. Futureworkcouldinvestigatealternativetrainingstrategies,such
asdifferentsamplingmethods,orfinetuningRMarchitectures,tobetteraligntheoptimizationprocesswiththe
desiredcapabilities.
RejectionSampling. Rejectionsamplingforlargelanguagemodelsisanincreasinglypopularmethodforimproving
post-trainingperformanceoffrontierlanguagemodels[Dubeyetal.,2024,Adleretal.,2024,Dongetal.,2023].
UsinganinitialSFTandpreferencedatamixtotrainaninitialmodel,thatmodelisusedtothengeneratenresponses
toeachSFTprompt. Thesenresponses,inadditiontotheoriginalresponse,arethenrankedusingarewardmodel
oranLLMasajudge,andthebestresponseiskept. Theotherresponsescanthenbeusedtocreatechosen/rejected
pairsforpreferenceoptimization. Thefullpost-trainingpipelineisthenrunonthesedatasets,andtheprocess
repeatsuntilperformanceconverges.
Wetriedrejectionsampling,butfoundthatforoursetuptheperformancegainswereminimalfortheamountof
computerequired,andthusweleaveadeeperexplorationforfuturework. Qualitatively,wefoundthatstrong
judgesarevital,andpubliclyavailablemodelscanstruggletochoosethebestresponseoutofthecandidates. We
alsofoundthatincludingtheoriginalresponseasachoiceforthejudge(inotherwords,choosingthebestresponse
21https://huggingface.co/datasets/Skywork/Skywork-Reward-Preference-80K-v0.1
42outofthengenerationsinadditiontotheoriginalresponse)performedmuchbetterthanonlychoosingfromthe
newlygeneratedresponses.
8.2 FutureWork
WhileweaimedtocoverabroadsetofskillsforT√úLU3,duetovariouslimitations,weleftsomeskillsforfuture
iterationsofT√úLU. Wediscusssomeoftheseskillsbelow:
LongContextandMulti-turn. Currently,thedatacollectedforT√úLU3isrelativelyshortanddoesnotcontain
longmulti-turndata(theaveragenumberofturnsinourmixtureis2.4turnsandmajorityofsamplesareunder
2,048tokensinlength). However,long-contexthasbeenpopularareaoffocusinrecentwork[Pawaretal.,2024],as
improvingthecontextwindowofLMsenablesnewuse-cases[GeminiTeam,2024]andmorein-contextexamples,
potentiallyimprovingperformance[Agarwaletal.,2024]. Relatedly,improvingmulti-turncapabilitiescanbetter
improveend-userexperience,withanon-trivialnumberofreal-worlduserconversationswithLMsgoingover2
turns[Zhaoetal.,2024]. Wehopetoaddressbothskillsinthefuturewithdedicatedtrainingandevaluations.
Multilinguality. WespecificallyfocusonEnglishdataandevaluationsforT√úLU3(althoughwedoincludethe
multilingualAya[√úst√ºnetal.,2024]datasetduetoitshighquality). However,thisneglectsthemyriadlanguages
apartfromEnglishspokenaroundtheworld,speakersofwhichmaybenefitfromordesireLMsthatcanprocess
theirlanguages. FutureworkmayexaminethecurrentmultilingualabilitiesofT√úLU3andhowtofurtherimprove
them. Wealsonotethatmultilingualpost-trainingcanmakeuseofdifferenttechniquestomonolingualpost-training
‚Äìforexample,cross-lingualalignment[Wuetal.,2024]orcarefuldatabalancingstrategies[Lietal.,2024b]. This
makesmultilingualpost-traininganinterestingandimpactfulareaforfuturework.
ToolUseandAgents. WhileweevaluateT√úLU3onitsown,LMsarebeingincreasinglydeployedaspartsoflarger
systems,inwhichtheyhaveaccesstotools[Quetal.,2024]orarethemselvespartofalarger‚Äòagent‚Äôframework.22
Furthermore,trainingmodelstousetoolsisanaturalwaytodramaticallyimprovetheirreasoningandmathematical
skills[Gouetal.,2024],ratherthantryingtoaccomplisheverything‚Äòintheweights.‚Äô Futureworkinvolvestraining
orevaluatingT√úLU3fortool-useeitheronitsownoraspartofalargerframework.
9 RelatedWork
9.1 TheEvolutionofPost-trainingRecipes
Modern‚Äúpost-training‚Äùhasitsrootsinmulti-tasklanguagemodeltraining,inparticularinstructiontuning[Mishra
etal.,2022,Weietal.,2022a,Sanhetal.,2022,Wangetal.,2022b,Longpreetal.,2023],inwhichlanguage
modelsaretrainedonsamplesincludingtaskinstructionsandtheircorrespondingresponses,allowingthemodelsto
generalize‚Äòzero-shot‚Äôtonewtasks. Earlyinstruction-tuningdatasetstendedtofocusonmoretraditionalNLPtasks
(e.g.,naturallanguageinference)ratherthanmoregenerictasksthatdownstreamusersmightperform[Wangetal.,
2022a]. WiththeriseofChatGPTandchat-basedLMs(Claude,Gemini,etc),post-trainingtechniquesevolved
beyondinstructiontuningtoincludepreferencetuningstages,withmodelsundergoingbothinstructiontuningand
thenpreferencetuning(PreFT)or‚ÄòRLHF‚Äô[Ouyangetal.,2022].
EarlyworkinRLHForiginatedfromexperimentsonDeepRLforcontrol[Christianoetal.,2017,Ibarzetal.,
2018,Leikeetal.,2018]andtypicallyinvolvesfirstlearningarewardmodelfromhumanpreferences,andthen
optimizingalanguagemodelviaanRLframeworkusingthelearntreward[Stiennonetal.,2020,Nakanoetal.,
2021,Askelletal.,2021,Ouyangetal.,2022]. Recently,approachesthatallowdirectlytrainingalanguagemodel
onsuchpreferenceshavebeendeveloped[Rafailovetal.,2024,Zhaoetal.,2023],reducingthecomplexityof
incorporatingPreFTintotraining. WhileearlyapproachestoPreFTwereextremelyhuman-centric,usingtensor
hundredsofthousandsofhuman-writteninstructionsandhumanpreferencelabels,morerecentworkusesmixtures
ofhumanandsyntheticallygeneratedpreferencedata,alongwithmultipleroundsoftrainingandvariedtraining
algorithms[Touvronetal.,2023,Dubeyetal.,2024,Gunteretal.,2024].
DuringtheevolutionofRLHFprimarilyinclosedlaboratories,openrecipesforpost-traininghavelaggedsomewhat
behind. Initialattemptsatbuilding‚Äòopenpost-trainingrecipes‚Äôfocusedontheinstruction-tuningstage[Taorietal.,
2023,Conoveretal.,2023],finetuningopenlyreleasedlanguagemodelsonsyntheticallygeneratedorhumanmade
datasets. Whilecombiningthesedatasetscouldyieldstrongperformance[Wangetal.,2023],incorporatinga
PreFTstageprovedimportantforbridgingthegapwithclosedmodelsbasedonhumanevaluations[Ivisonetal.,
2023]. Today,mostpopularadaptedmodelswithopenrecipesforPreFTuseDPO(oroneofitsvariants),and
22Forexample,theOpenHandsplatform[Wangetal.,2024a].
43AIfeedbackdataincludingT√úLU2[Ivisonetal.,2023],Zephyr-ùõΩ [Tunstalletal.,2023],andStarling[Zhuetal.,
2023]. However,manyofthesemodelsareoutdatedrelativetoclosedpost-trainingrecipesbothintermsofdata
andinperformance: noopen-recipemodelsexistinthetop50ofLMSYS‚ÄôsChatBotArena(asofNovember20th,
2024)havereleasedtheirpost-trainingdata[Chiangetal.,2024],andmostopenrecipesuserelativelylittledata
andfewroundsoftrainingcomparedtoclosedpost-trainingsetups,whichcaninvolvemultipleroundsoftraining
withvariedobjectives,andmillionsofdatapoints[Touvronetal.,2023,Dubeyetal.,2024]. Forinstance,Llama
3.1trainedongeneratedoutputsfromthepreviousmodelformultipleroundswithextensivehumanfeedbackdata,
andusedstrongmodelstowritesyntheticinstructions[Dubeyetal.,2024]. Otherrecentdevelopmentsincludes
rejectionsamplingforsyntheticdataandadvancedrewardmodelingforstep-wiseassistantresponses.
Whilstwedonotentirelyreachthesizeoftheseclosedrecipes,inthisworkwehopetopushthestateofopen
post-trainingrecipesforwardbybuildingarecipethatmatchesorbeatsstrongclosedrecipes,andreleasingall
relatedartefacts(code,models,data,etc)forfurtherscientificstudyanduse. Ourpipelineissignificantlylargerand
morecomplexthanpriorwork,comprisingofalmostamillioninstructiontuningsamples,hundredsofthousands
ofpreferencepairs,andanovelonlineRLtrainingphase.
9.2 TrainingonVerifiableRewards
TheRLVRapproachproposedinthisworkrelatestoavarietyofrecentworkonimprovingLMreasoningthrough
RL-related techniques. Closely related is the self-taught reasoner (STaR) line of work [Zelikman et al., 2022,
2024]andTRICE[Hoffmanetal.,2023],bothofwhichexamineusingexistingground-truthanswersassignalsto
generatebettermodelrationales(orchains-of-thought). STaRcanbeseenasanapproximationofapolicygradient
algorithm,withQuiet-STaRextendingtheapproachoftrainingthemodeltouseadditionalgenerationstoimprove
genericlanguagemodelling(‚Äòthinkingbeforespeaking‚Äô). TRICE[Hoffmanetal.,2023]alsoaimstoimprove
thelikelihoodofcorrectanswersbytrainingovermultiplereasoningtraces,usingacustomMCMC-basedEM
algorithm. More recently, VinePPO [Kazemnejad et al., 2024] uses binary rewards from GSM8k and MATH
correctnesstotestanovelPPO-basedalgorithm,andotherrecentworkhasexploredusingcodefeedbackasasignal
fortraining[Gehringetal.,2024,Xuetal.,2024b]. Incontrast,ourproposedapproach,RLVRsimplyusesan
existingRLframework(PPO)fortraining,andrunsentirelyonlinewithbinaryrewards(comparedtotheiterative
approachofSTaRorthelog-likelihoodrewardsofQuiet-STaR).Additionally,weexpandbeyondthemathdomain,
alsofindingthatthisapproachcanyieldimprovementsinpreciseinstructionfollowing. Finally,wealsocarefully
ablateanumberofcorecomponentsofRLVR,includingvaluemodelinitializationandusingageneralreward
modelwithverifiablerewards. Wehopetofurtherdevelopandexpandthistechniqueinfuturework.
10 Conclusion
WeintroduceT√úLU3,afamilyoffullyopenstate-of-the-artlanguagemodels,featuringamodernpost-training
frameworkwithfullyopen-sourcedataT√úLU3DATA,evaluationT√úLU3EVAL,trainingcodeT√úLU3CODEand
developmentrecipesT√úLU3RECIPE. WereleasefinalmodelstrainedonLlama3.1baseversions,withintermediate
checkpoints,trainingdata,trainingcode,andevaluationcode.
T√úLU3bridgesthegapbetweenopenandclosedpost-trainingmethods,markinganewmilestoneinopenpost-
training research. With the resources provided, others can build on open base models, finetune them for high
performanceacrossdiversetasks. Thispavesthewayforadvancingpost-trainingresearchwithinmulti-objective,
andmulti-stagetrainingframeworks.
Acknowledgments
WethankJohnSchulmanforextremelyusefuladvice. WeacknowledgetheNationalArtificialIntelligenceResearch
Resource (NAIRR) Pilot and Microsoft Azure for contributing to the results in this work. We thank Niklas
MuennighoffforhelpingwithsomeexperimentationonOLMoE.WethankcountlessmembersofAi2andUW
NLPcommunitiesforusefulfeedbackthroughoutthisproject. ResearchsupportedwithCloudTPUsfromGoogle‚Äôs
TPUResearchCloud(TRC).WethankHuyTran,JesseDodge,JiachengLiu,SruthiSreeram,TaylorBlanton,
AaronSarnat, ArnaviChheda, ByronBischoff, ChrisNewell, MichaelSchmitz, SamSkjonsberg, EricMarsh,
KarenFarley,andYenSungChenforbuildingtheAi2Playgroundformodeldemos. Wealsothanktheseothersat
Ai2formanyindirectcontributionstotheproject: TairaAnderson,JenDumas,CrystalNam,SophieLebrecht,
BrookeVlahos,ChrisWilhelm,JennaJames,AlexBuraczynski,WillSmith,CaitlinWittlif,CarissaSchoenick,
andAliFarhadi.
44References
B.Adler,N.Agarwal,A.Aithal,D.H.Anh,P.Bhattacharya,A.Brundyn,J.Casper,B.Catanzaro,S.Clay,J.Cohen,
etal. Nemotron-4340btechnicalreport. arXivpreprintarXiv:2406.11704,2024.
R.Agarwal,A.Singh,L.M.Zhang,B.Bohnet,L.Rosias,S.C.Chan,B.Zhang,A.Anand,Z.Abbas,A.Nova,
J.D.Co-Reyes, E.Chu,F.Behbahani, A.Faust,andH.Larochelle. Many-shotin-contextlearning. InThe
Thirty-eighthAnnualConferenceonNeuralInformationProcessingSystems,2024.URLhttps://openreview.
net/forum?id=AB6XpMzvqH.
M.AI. Mistralintroducesnemo,2024. URLhttps://mistral.ai/news/mistral-nemo/. Accessed: 2024-
11-21.
E.Almazrouei,H.Alobeidli,A.Alshamsi,A.Cappelli,R.Cojocaru,M.Debbah,E.Goffinet,D.Heslow,J.Launay,
Q.Malartic,B.Noune,B.Pannier,andG.Penedo.Falcon-40B:anopenlargelanguagemodelwithstate-of-the-art
performance. 2023.
Anthropic. Introducingcomputeruse,anewclaude3.5sonnet,andclaude3.5haiku. Anthropic,2024. URL
https://www.anthropic.com/news/3-5-models-and-computer-use. Accessed: 2024-10-22.
A.Askell,Y.Bai,A.Chen,D.Drain,D.Ganguli,T.Henighan,A.Jones,N.Joseph,B.Mann,N.DasSarma,etal.
Agenerallanguageassistantasalaboratoryforalignment. arXivpreprintarXiv:2112.00861,2021.
Y.Bai,S.Kadavath,S.Kundu,A.Askell,J.Kernion,A.Jones,A.Chen,A.Goldie,A.Mirhoseini,C.McKinnon,
etal. Constitutionalai: Harmlessnessfromaifeedback. arXivpreprintarXiv:2212.08073,2022.
E.Beeching,S.C.Huang,A.Jiang,J.Li,B.Lipkin,Z.Qina,K.Rasul,Z.Shen,R.Soletskyi,andL.Tunstall.
Numinamath7btir. https://huggingface.co/AI-MO/NuminaMath-7B-TIR,2024.
F.Brahman,S.Kumar,V.Balachandran,P.Dasigi,V.Pyatkin,A.Ravichander,S.Wiegreffe,N.Dziri,K.Chandu,
J. Hessel, et al. The art of saying no: Contextual noncompliance in language models. arXiv preprint
arXiv:2407.12043,2024.
Z.Cai,M.Cao,H.Chen,K.Chen,K.Chen,X.Chen,X.Chen,Z.Chen,Z.Chen,P.Chu,X.Dong,H.Duan,
Q.Fan,Z.Fei,Y.Gao,J.Ge,C.Gu,Y.Gu,T.Gui,A.Guo,Q.Guo,C.He,Y.Hu,T.Huang,T.Jiang,P.Jiao,
Z.Jin,Z.Lei,J.Li,J.Li,L.Li,S.Li,W.Li,Y.Li,H.Liu,J.Liu,J.Hong,K.Liu,K.Liu,X.Liu,C.Lv,H.Lv,
K.Lv,L.Ma,R.Ma,Z.Ma,W.Ning,L.Ouyang,J.Qiu,Y.Qu,F.Shang,Y.Shao,D.Song,Z.Song,Z.Sui,
P.Sun,Y.Sun,H.Tang,B.Wang,G.Wang,J.Wang,J.Wang,R.Wang,Y.Wang,Z.Wang,X.Wei,Q.Weng,
F.Wu, Y.Xiong, C.Xu, R.Xu, H.Yan, Y.Yan, X.Yang, H.Ye, H.Ying, J.Yu, J.Yu, Y.Zang, C.Zhang,
L.Zhang,P.Zhang,P.Zhang,R.Zhang,S.Zhang,S.Zhang,W.Zhang,W.Zhang,X.Zhang,X.Zhang,H.Zhao,
Q.Zhao,X.Zhao,F.Zhou,Z.Zhou,J.Zhuo,Y.Zou,X.Qiu,Y.Qiao,andD.Lin. Internlm2technicalreport,
2024.
X.Chan,X.Wang,D.Yu,H.Mi,andD.Yu. Scalingsyntheticdatacreationwith1,000,000,000personas. arXiv
preprintarXiv:2406.20094,2024.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,
G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,
N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,P.Tillet,F.P.Such,D.Cummings,M.Plappert,
F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,A.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,
S.Balaji,S.Jain,W.Saunders,C.Hesse,A.N.Carr,J.Leike,J.Achiam,V.Misra,E.Morikawa,A.Radford,
M.Knight,M.Brundage,M.Murati,K.Mayer,P.Welinder,B.McGrew,D.Amodei,S.McCandlish,I.Sutskever,
andW.Zaremba. Evaluatinglargelanguagemodelstrainedoncode. 2021.
Y.Chen,D.Zhu,Y.Sun,X.Chen,W.Zhang,andX.Shen. Theaccuracyparadoxinrlhf: Whenbetterreward
modelsdon‚Äôtyieldbetterlanguagemodels. arXivpreprintarXiv:2410.06554,2024.
W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.Gonzalez,I.Stoica,
andE.P.Xing. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023. URL
https://lmsys.org/blog/2023-03-30-vicuna/.
W.-L.Chiang,L.Zheng,Y.Sheng,A.N.Angelopoulos,T.Li,D.Li,H.Zhang,B.Zhu,M.Jordan,J.E.Gonzalez,
etal.Chatbotarena: Anopenplatformforevaluatingllmsbyhumanpreference.arXivpreprintarXiv:2403.04132,
2024.
45P.F.Christiano,J.Leike,T.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcementlearningfromhuman
preferences. Advancesinneuralinformationprocessingsystems,30,2017.
H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,Y.Li,X.Wang,M.Dehghani,S.Brahma,etal.
Scalinginstruction-finetunedlanguagemodels. JournalofMachineLearningResearch,25(70):1‚Äì53,2024.
K.Cobbe,V.Kosaraju,M.Bavarian,M.Chen,H.Jun,L.Kaiser,M.Plappert,J.Tworek,J.Hilton,R.Nakano,
C.Hesse,andJ.Schulman. Trainingverifierstosolvemathwordproblems. arXivpreprintarXiv:2110.14168,
2021.
M.Conover,M.Hayes,A.Mathur,J.Xie,J.Wan,S.Shah,A.Ghodsi,P.Wendell,M.Zaharia,andR.Xin. Free
dolly: Introducingtheworld‚Äôsfirsttrulyopeninstruction-tunedllm,2023. URLhttps://www.databricks.
com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.
G.Cui,L.Yuan,N.Ding,G.Yao,W.Zhu,Y.Ni,G.Xie,Z.Liu,andM.Sun. Ultrafeedback: Boostinglanguage
modelswithhigh-qualityfeedback. arXivpreprintarXiv:2310.01377,2023.
N.Ding,Y.Chen,B.Xu,Y.Qin,Z.Zheng,S.Hu,Z.Liu,M.Sun,andB.Zhou. Enhancingchatlanguagemodels
byscalinghigh-qualityinstructionalconversations. arXivpreprintarXiv:2305.14233,2023.
H.Dong, W.Xiong, D.Goyal, Y.Zhang, W.Chow, R.Pan, S.Diao, J.Zhang, K.Shum, andT.Zhang. Raft:
Rewardrankedfinetuningforgenerativefoundationmodelalignment,2023. URLhttps://arxiv.org/abs/
2304.06767.
D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,S.Singh,andM.Gardner. DROP:Areadingcomprehensionbenchmark
requiringdiscretereasoningoverparagraphs. InJ.Burstein,C.Doran,andT.Solorio,editors,Proceedingsof
the2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human
LanguageTechnologies,Volume1(LongandShortPapers),pages2368‚Äì2378,Minneapolis,Minnesota,June
2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1246. URLhttps://aclanthology.
org/N19-1246.
A.Dubey,A.Jauhri,A.Pandey,A.Kadian,A.Al-Dahle,A.Letman,A.Mathur,A.Schelten,A.Yang,A.Fan,
etal. Thellama3herdofmodels. arXivpreprintarXiv:2407.21783,2024.
Y.Dubois,X.Li,R.Taori,T.Zhang,I.Gulrajani,J.Ba,C.Guestrin,P.Liang,andT.Hashimoto. Alpacafarm: A
simulationframeworkformethodsthatlearnfromhumanfeedback. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=4hturzLcKX.
Y.Dubois,B.Galambosi,P.Liang,andT.B.Hashimoto. Length-controlledalpacaeval: Asimplewaytodebias
automaticevaluators. arXivpreprintarXiv:2404.04475,2024.
L.Engstrom,A.Ilyas,S.Santurkar,D.Tsipras,F.Janoos,L.Rudolph,andA.Madry. Implementationmattersin
deeprl: Acasestudyonppoandtrpo. InInternationalConferenceonLearningRepresentations,2020. URL
https://openreview.net/forum?id=r1etN1rtPB.
L.Espeholt,H.Soyer,R.Munos,K.Simonyan,V.Mnih,T.Ward,Y.Doron,V.Firoiu,T.Harley,I.Dunning,
etal. Impala: Scalabledistributeddeep-rlwithimportanceweightedactor-learnerarchitectures. InInternational
conferenceonmachinelearning,pages1407‚Äì1416.PMLR,2018.
L.Gao,J.Tow,B.Abbasi,S.Biderman,S.Black,A.DiPofi,C.Foster,L.Golding,J.Hsu,A.LeNoac‚Äôh,H.Li,
K.McDonell,N.Muennighoff,C.Ociepa,J.Phang,L.Reynolds,H.Schoelkopf,A.Skowron,L.Sutawika,
E.Tang,A.Thite,B.Wang,K.Wang,andA.Zou. Aframeworkforfew-shotlanguagemodelevaluation,07
2024. URLhttps://zenodo.org/records/12608602.
J.Gehring,K.Zheng,J.Copet,V.Mella,T.Cohen,andG.Synnaeve. Rlef: Groundingcodellmsinexecution
feedbackwithreinforcementlearning,2024. URLhttps://arxiv.org/abs/2410.02089.
GeminiTeam. Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext,2024. URL
https://arxiv.org/abs/2403.05530.
GemmaTeam,M.Riviere,S.Pathak,P.G.Sessa,C.Hardin,S.Bhupatiraju,L.Hussenot,T.Mesnard,B.Shahriari,
A.Ram√©,etal. Gemma2: Improvingopenlanguagemodelsatapracticalsize. arXivpreprintarXiv:2408.00118,
2024.
46T.M.GemmaTeam,C.Hardin,R.Dadashi,S.Bhupatiraju,L.Sifre,M.Rivi√®re,M.S.Kale,J.Love,P.Tafti,
L.Hussenot,andetal. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301. URLhttps://www.kaggle.com/
m/3301.
C.Goddard,S.Siriwardhana,M.Ehghaghi,L.Meyers,V.Karpukhin,B.Benedict,M.McQuade,andJ.Solawetz.
Arcee‚Äôsmergekit: Atoolkitformerginglargelanguagemodels. arXivpreprintarXiv:2403.13257,2024.
Z.Gou,Z.Shao,Y.Gong,yelongshen,Y.Yang,M.Huang,N.Duan,andW.Chen. ToRA:Atool-integrated
reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=Ep0TtjVoap.
D.Groeneveld,I.Beltagy,P.Walsh,A.Bhagia,R.Kinney,O.Tafjord,A.Jha,H.Ivison,I.Magnusson,Y.Wang,
S.Arora, D.Atkinson, R.Authur, K.R.Chandu, A.Cohan, J.Dumas, Y.Elazar,Y.Gu, J.Hessel, T.Khot,
W. Merrill, J. D. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander,
D.Schwenk,S.Shah,W.Smith,E.Strubell,N.Subramani,M.Wortsman,P.Dasigi,N.Lambert,K.Richardson,
L.S.Zettlemoyer,J.Dodge,K.Lo,L.Soldaini,N.A.Smith,andH.Hajishirzi. Olmo: Acceleratingthescience
oflanguagemodels. ArXiv,abs/2402.00838,2024. URLhttps://api.semanticscholar.org/CorpusID:
267365485.
Y.Gu,O.Tafjord,B.Kuehl,D.Haddad,J.Dodge,andH.Hajishirzi. Olmes: Astandardforlanguagemodel
evaluations,2024. URLhttps://arxiv.org/abs/2406.08446.
T.Gunter,Z.Wang,C.Wang,R.Pang,A.Narayanan,A.Zhang,B.Zhang,C.Chen,C.-C.Chiu,D.Qiu,etal.
Appleintelligencefoundationlanguagemodels. arXivpreprintarXiv:2407.21075,2024.
S.Guo,B.Zhang,T.Liu,T.Liu,M.Khalman,F.Llinares,A.Rame,T.Mesnard,Y.Zhao,B.Piot,J.Ferret,and
M.Blondel. Directlanguagemodelalignmentfromonlineaifeedback,2024. URLhttps://arxiv.org/abs/
2402.04792.
S.Han,K.Rao,A.Ettinger,L.Jiang,B.Y.Lin,N.Lambert,Y.Choi,andN.Dziri. Wildguard: Openone-stop
moderationtoolsforsafetyrisks,jailbreaks,andrefusalsofllms. arXivpreprintarXiv:2406.18495,2024.
D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuringmassivemultitask
languageunderstanding. arXivpreprintarXiv:2009.03300,2020.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring
mathematicalproblemsolvingwiththemathdataset. NeurIPS,2021.
M.D.Hoffman,D.Phan,daviddohan,S.Douglas,T.A.Le,A.T.Parisi,P.Sountsov,C.Sutton,S.Vikram,and
R.A.Saurous. Trainingchain-of-thoughtvialatent-variableinference. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=a147pIS2Co.
J.Hong,N.Lee,andJ.Thorne. ORPO:Monolithicpreferenceoptimizationwithoutreferencemodel. InY.Al-
Onaizan,M.Bansal,andY.-N.Chen,editors,Proceedingsofthe2024ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages11170‚Äì11189,Miami,Florida,USA,Nov.2024.AssociationforComputa-
tionalLinguistics. URLhttps://aclanthology.org/2024.emnlp-main.626.
J.Hu,X.Wu,W.Wang,D.Zhang,Y.Cao,etal. Openrlhf: Aneasy-to-use,scalableandhigh-performancerlhf
framework. arXivpreprintarXiv:2405.11143,2024.
S.Hu,Y.Luo,H.Wang,X.Cheng,Z.Liu,andM.Sun. Won‚Äôtgetfooledagain: Answeringquestionswithfalse
premises. arXivpreprintarXiv:2307.02394,2023.
S.Huang,R.F.J.Dossa,A.Raffin,A.Kanervisto,andW.Wang. The37implementationdetailsofproximal
policyoptimization.InICLRBlogTrack,2022.URLhttps://iclr-blog-track.github.io/2022/03/25/
ppo-implementation-details/. https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/.
S.Huang,J.Weng,R.Charakorn,M.Lin,Z.Xu,andS.Onta√±√≥n. Cleanba: Areproducibleandefficientdistributed
reinforcementlearningplatform. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
S.Huang,M.Noukhovitch,A.Hosseini,K.Rasul,W.Wang,andL.Tunstall. Then+implementationdetailsof
RLHFwithPPO:AcasestudyonTL;DRsummarization. InFirstConferenceonLanguageModeling,2024a.
URLhttps://openreview.net/forum?id=kHO2ZTa8e3.
47Y. Huang, L. Sun, H. Wang, S. Wu, Q. Zhang, Y. Li, C. Gao, Y. Huang, W. Lyu, Y. Zhang, et al. Trustllm:
Trustworthinessinlargelanguagemodels. arXivpreprintarXiv:2401.05561,2024b.
A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes,
A.Radford,etal. Gpt-4osystemcard. arXivpreprintarXiv:2410.21276,2024.
B.Ibarz,J.Leike,T.Pohlen,G.Irving,S.Legg,andD.Amodei. Rewardlearningfromhumanpreferencesand
demonstrationsinatari. Advancesinneuralinformationprocessingsystems,31,2018.
H.Ivison,Y.Wang,V.Pyatkin,N.Lambert,M.Peters,P.Dasigi,J.Jang,D.Wadden,N.A.Smith,I.Beltagy,etal.
Camelsinachangingclimate: Enhancinglmadaptationwithtulu2. arXivpreprintarXiv:2311.10702,2023.
H.Ivison,Y.Wang,J.Liu,Z.Wu,V.Pyatkin,N.Lambert,N.A.Smith,Y.Choi,andH.Hajishirzi. Unpackingdpo
andppo: Disentanglingbestpracticesforlearningfrompreferencefeedback. arXivpreprintarXiv:2406.09279,
2024.
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel,
G.Lample,L.Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
L.Jiang, K.Rao, S.Han, A.Ettinger, F.Brahman, S.Kumar, N.Mireshghallah, X.Lu, M.Sap, Y.Choi, and
N.Dziri. Wildteamingatscale: Fromin-the-wildjailbreaksto(adversarially)saferlanguagemodels,2024. URL
https://arxiv.org/abs/2406.18510.
J.Kazdan,R.Schaeffer,A.Dey,M.Gerstgrasser,R.Rafailov,D.L.Donoho,andS.Koyejo.Collapseorthrive? perils
andpromisesofsyntheticdatainaself-generatingworld,2024. URLhttps://arxiv.org/abs/2410.16713.
A.Kazemnejad,M.Aghajohari,E.Portelance,A.Sordoni,S.Reddy,A.Courville,andN.L.Roux. Vineppo:
Unlockingrlpotentialforllmreasoningthroughrefinedcreditassignment,2024. URLhttps://arxiv.org/
abs/2410.01679.
A.K√∂pf,Y.Kilcher,D.vonR√ºtte,S.Anagnostidis,Z.R.Tam,K.Stevens,A.Barhoum,D.Nguyen,O.Stanley,
R.Nagyfi,etal.Openassistantconversations-democratizinglargelanguagemodelalignment.AdvancesinNeural
InformationProcessingSystems,36,2024.
W.Kwon, Z.Li, S.Zhuang, Y.Sheng, L.Zheng, C.H.Yu, J.E.Gonzalez, H.Zhang, andI.Stoica. Efficient
memorymanagementforlargelanguagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS
29thSymposiumonOperatingSystemsPrinciples,2023.
J.Leike,D.Krueger,T.Everitt,M.Martic,V.Maini,andS.Legg. Scalableagentalignmentviarewardmodeling:
aresearchdirection. arXivpreprintarXiv:1811.07871,2018.
A.Lewkowycz,A.J.Andreassen,D.Dohan,E.Dyer,H.Michalewski,V.V.Ramasesh,A.Slone,C.Anil,I.Schlag,
T.Gutman-Solo,Y.Wu,B.Neyshabur,G.Gur-Ari,andV.Misra. Solvingquantitativereasoningproblemswith
languagemodels. InA.H.Oh,A.Agarwal,D.Belgrave,andK.Cho,editors,AdvancesinNeuralInformation
ProcessingSystems,2022. URLhttps://openreview.net/forum?id=IFXTZERXdM7.
T.Li,W.-L.Chiang,E.Frick,L.Dunlap,B.Zhu,J.E.Gonzalez,andI.Stoica. Fromlivedatatohigh-qualitybench-
marks: Thearena-hardpipeline,April2024a. URLhttps://lmsys.org/blog/2024-04-19-arena-hard/.
T.Li,H.Xu,W.Tan,K.Murray,andD.Khashabi.Upsampleorupweight? balancedtrainingonheavilyimbalanced
datasets,2024b. URLhttps://arxiv.org/abs/2410.04579.
S.Lin,J.Hilton,andO.Evans. Truthfulqa: Measuringhowmodelsmimichumanfalsehoods. arXivpreprint
arXiv:2109.07958,2021.
J.Liu,C.S.Xia,Y.Wang,andL.Zhang. IsyourcodegeneratedbychatGPTreallycorrect? rigorousevaluationof
largelanguagemodelsforcodegeneration. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://openreview.net/forum?id=1qvx610Cu7.
Y.Liu. Roberta: Arobustlyoptimizedbertpretrainingapproach. arXivpreprintarXiv:1907.11692,364,2019.
S.Longpre,L.Hou,T.Vu,A.Webson,H.W.Chung,Y.Tay,D.Zhou,Q.V.Le,B.Zoph,J.Wei,etal. Theflan
collection: Designingdataandmethodsforeffectiveinstructiontuning. arXivpreprintarXiv:2301.13688,2023.
48Z.Luo,C.Xu,P.Zhao,Q.Sun,X.Geng,W.Hu,C.Tao,J.Ma,Q.Lin,andD.Jiang. Wizardcoder: Empowering
codelargelanguagemodelswithevol-instruct,2023.
A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi. When not to trust language models:
Investigatingeffectivenessandlimitationsofparametricandnon-parametricmemories. arXivpreprint,2022.
M.Mazeika,L.Phan,X.Yin,A.Zou,Z.Wang,N.Mu,E.Sakhaee,N.Li,S.Basart,B.Li,etal.Harmbench: Astan-
dardizedevaluationframeworkforautomatedredteamingandrobustrefusal. arXivpreprintarXiv:2402.04249,
2024.
Y.Meng,M.Xia,andD.Chen.Simpo: Simplepreferenceoptimizationwithareference-freereward.arXivpreprint
arXiv:2405.14734,2024.
A.Meurer,C.P.Smith,M.Paprocki,O.ƒåert√≠k,S.B.Kirpichev,M.Rocklin,A.Kumar,S.Ivanov,J.K.Moore,
S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi, H. Gupta, S. Vats, F. Johansson,
F.Pedregosa,M.J.Curry,A.R.Terrel,v.Rouƒçka,A.Saboo,I.Fernando,S.Kulal,R.Cimrman,andA.Scopatz.
Sympy: symboliccomputinginpython. PeerJComputerScience,3:e103,Jan.2017. ISSN2376-5992. doi:
10.7717/peerj-cs.103. URLhttps://doi.org/10.7717/peerj-cs.103.
L.J.V.Miranda,Y.Wang,Y.Elazar,S.Kumar,V.Pyatkin,F.Brahman,N.A.Smith,H.Hajishirzi,andP.Dasigi.
Hybridpreferences: Learningtorouteinstancesforhumanvs.aifeedback. arXivpreprintarXiv:2410.19133,
2024.
S.Mishra,D.Khashabi,C.Baral,andH.Hajishirzi. Cross-taskgeneralizationvianaturallanguagecrowdsourcing
instructions. InS.Muresan,P.Nakov,andA.Villavicencio,editors,Proceedingsofthe60thAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1: LongPapers),pages3470‚Äì3487,Dublin,Ireland,
May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244. URL https:
//aclanthology.org/2022.acl-long.244.
MistralAI. Ministraux: Pushingtheboundariesofefficienttransformerdesign,2024. URLhttps://mistral.
ai/news/ministraux/. Accessed: 2024-11-17.
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.
Fidjeland,G.Ostrovski,etal. Human-levelcontrolthroughdeepreinforcementlearning. nature,518(7540):
529‚Äì533,2015.
P.Moritz,R.Nishihara,S.Wang,A.Tumanov,R.Liaw,E.Liang,M.Elibol,Z.Yang,W.Paul,M.I.Jordan,etal.
Ray: Adistributedframeworkforemerging{AI}applications. In13thUSENIXsymposiumonoperatingsystems
designandimplementation(OSDI18),pages561‚Äì577,2018.
MosaicMLNLPTeam. Introducingmpt-30b: Raisingthebarforopen-sourcefoundationmodels,2023. URL
www.mosaicml.com/blog/mpt-30b. Accessed: 2023-06-22.
N.Muennighoff,H.Su,L.Wang,N.Yang,F.Wei,T.Yu,A.Singh,andD.Kiela. Generativerepresentational
instructiontuning,2024.
R.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.
Webgpt: Browser-assistedquestion-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
M.Noukhovitch,S.Huang,S.Xhonneux,A.Hosseini,R.Agarwal,andA.Courville. Asynchronousrlhf: Faster
andmoreefficientoff-policyrlforlanguagemodels,2024. URLhttps://arxiv.org/abs/2410.18252.
OpenAI. Learning to reason with llms. OpenAI, 2024. URL https://openai.com/index/
learning-to-reason-with-llms/. Accessed: 2024-09-12.
OpenAI. Gpt-4o,2024. https://openai.com.
L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,A.Ray,
etal. Traininglanguagemodelstofollowinstructionswithhumanfeedback. Advancesinneuralinformation
processingsystems,35:27730‚Äì27744,2022.
S. Pawar, S. M. T. I. Tonmoy, S. M. M. Zaman, V. Jain, A. Chadha, and A. Das. The what, why, and how
of context length extension techniques in large language models ‚Äì a detailed survey, 2024. URL https:
//arxiv.org/abs/2401.07872.
49C.Qu,S.Dai,X.Wei,H.Cai,S.Wang,D.Yin,J.Xu,andJ.-R.Wen. Toollearningwithlargelanguagemodels: A
survey. arXivpreprintarXiv:2405.17935,2024.
QwenTeam. Qwen2.5: Apartyoffoundationmodels,September2024. URLhttps://qwenlm.github.io/
blog/qwen2.5/.
R.Rafailov,A.Sharma,E.Mitchell,C.D.Manning,S.Ermon,andC.Finn. Directpreferenceoptimization: Your
languagemodelissecretlyarewardmodel. AdvancesinNeuralInformationProcessingSystems,36,2024.
N.Rajani,L.Tunstall,E.Beeching,N.Lambert,A.M.Rush,andT.Wolf. Norobots. https://huggingface.
co/datasets/HuggingFaceH4/no_robots,2023.
S.Rajbhandari,J.Rasley,O.Ruwase,andY.He. Zero: Memoryoptimizationstowardtrainingtrillionparameter
models.InSC20: InternationalConferenceforHighPerformanceComputing,Networking,StorageandAnalysis,
pages1‚Äì16.IEEE,2020.
D. Rein, B. L.Hou, A.C. Stickland, J.Petty, R.Y. Pang, J. Dirani, J. Michael, and S.R. Bowman. Gpqa: A
graduate-levelgoogle-proofq&abenchmark,2023.
P. R√∂ttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy. Xstest: A test suite for identifying
exaggeratedsafetybehavioursinlargelanguagemodels. arXivpreprintarXiv:2308.01263,2023.
V.Sanh,A.Webson,C.Raffel,S.Bach,L.Sutawika,Z.Alyafeai,A.Chaffin,A.Stiegler,A.Raja,M.Dey,M.S.
Bari, C.Xu, U.Thakker, S. S.Sharma, E.Szczechla, T.Kim, G.Chhablani, N.Nayak, D.Datta, J.Chang,
M.T.-J.Jiang,H.Wang,M.Manica,S.Shen,Z.X.Yong,H.Pandey,R.Bawden,T.Wang,T.Neeraj,J.Rozen,
A.Sharma,A.Santilli,T.Fevry,J.A.Fries,R.Teehan,T.L.Scao,S.Biderman,L.Gao,T.Wolf,andA.M.
Rush. MultitaskPromptedTrainingEnablesZero-ShotTaskGeneralization. InInternationalConferenceon
LearningRepresentations(ICLR),2022.
D.Saxton,E.Grefenstette,F.Hill,andP.Kohli. Analysingmathematicalreasoningabilitiesofneuralmodels. In
InternationalConferenceonLearningRepresentations,2019.
J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimizationalgorithms. arXiv
preprintarXiv:1707.06347,2017.
G.Shen,Z.Wang,O.Delalleau,J.Zeng,Y.Dong,D.Egert,S.Sun,J.J.Zhang,S.Jain,A.Taghibakhshi,M.S.
Ausin, A. Aithal, and O. Kuchaiev. Nemo-aligner: Scalable toolkit for efficient model alignment. In First
ConferenceonLanguageModeling,2024a. URLhttps://openreview.net/forum?id=yK2eGE8QVW.
X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang. ‚ÄúDo Anything Now‚Äù: Characterizing and Evaluating
In-The-WildJailbreakPromptsonLargeLanguageModels. InACMSIGSACConferenceonComputerand
CommunicationsSecurity(CCS).ACM,2024b.
A.K.Singh,M.Y.Kocyigit,A.Poulton,D.Esiobu,M.Lomeli,G.Szilvasy,andD.Hupkes. Evaluationdata
contaminationinllms: howdowemeasureitand(when)doesitmatter? arXivpreprintarXiv:2411.03923,
2024a.
S.Singh,F.Vargus,D.Dsouza,B.F.Karlsson,A.Mahendiran,W.-Y.Ko,H.Shandilya,J.Patel,D.Mataciunas,
L.OMahony,etal. Ayadataset: Anopen-accesscollectionformultilingualinstructiontuning. arXivpreprint
arXiv:2402.06619,2024b.
P.Singhal,T.Goyal,J.Xu,andG.Durrett. Alongwaytogo: InvestigatinglengthcorrelationsinRLHF,2024.
URLhttps://openreview.net/forum?id=sNtDKdcI1f.
N.Stiennon,L.Ouyang,J.Wu,D.Ziegler,R.Lowe,C.Voss,A.Radford,D.Amodei,andP.F.Christiano.Learning
tosummarizewithhumanfeedback. AdvancesinNeuralInformationProcessingSystems,33:3008‚Äì3021,2020.
M. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi,
D.Zhou,,andJ.Wei. Challengingbig-benchtasksandwhetherchain-of-thoughtcansolvethem. arXivpreprint
arXiv:2210.09261,2022.
R.Taori,I.Gulrajani,T.Zhang,Y.Dubois,X.Li,C.Guestrin,P.Liang,andT.B.Hashimoto. Stanfordalpaca: An
instruction-followingllamamodel. https://github.com/tatsu-lab/stanford_alpaca,2023.
R.Teknium,J.Quesnelle,andC.Guang. Hermes3technicalreport. arXivpreprintarXiv:2408.11857,2024.
50S.Toshniwal,W.Du,I.Moshkov,B.Kisacanin,A.Ayrapetyan,andI.Gitman. Openmathinstruct-2: Accelerating
aiformathwithmassiveopen-sourceinstructiondata. arXivpreprintarXiv:2410.01560,2024.
H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,S.Bhosale,
etal. Llama2: Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada, S. Huang, L. von Werra, C. Fourrier,
N.Habib,etal. Zephyr: Directdistillationoflmalignment. arXivpreprintarXiv:2310.16944,2023.
A.√úst√ºn,V.Aryabumi,Z.Yong,W.-Y.Ko,D.D‚Äôsouza,G.Onilude,N.Bhandari,S.Singh,H.-L.Ooi,A.Kayid,
F.Vargus,P.Blunsom,S.Longpre,N.Muennighoff,M.Fadaee,J.Kreutzer,andS.Hooker. Ayamodel: An
instruction finetuned open-access multilingual language model. In L.-W. Ku, A. Martins, and V. Srikumar,
editors,Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
LongPapers),pages15894‚Äì15939,Bangkok,Thailand,Aug.2024.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2024.acl-long.845. URLhttps://aclanthology.org/2024.acl-long.845.
L.vonWerra,Y.Belkada,L.Tunstall,E.Beeching,T.Thrush,N.Lambert,S.Huang,K.Rasul,andQ.Gallou√©dec.
Trl: Transformerreinforcementlearning. https://github.com/huggingface/trl,2020.
D.Wadden,K.Shi,J.Morrison,A.Naik,S.Singh,N.Barzilay,K.Lo,T.Hope,L.Soldaini,S.Z.Shen,etal.
Sciriff: Aresourcetoenhancelanguagemodelinstruction-followingoverscientificliterature. arXivpreprint
arXiv:2406.07835,2024.
X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, H. H. Tran, F. Li,
R.Ma,M.Zheng,B.Qian,Y.Shao,N.Muennighoff,Y.Zhang,B.Hui,J.Lin,R.Brennan,H.Peng,H.Ji,and
G.Neubig. OpenHands: AnOpenPlatformforAISoftwareDevelopersasGeneralistAgents,2024a. URL
https://arxiv.org/abs/2407.16741.
Y.Wang,Y.Kordi,S.Mishra,A.Liu,N.A.Smith,D.Khashabi,andH.Hajishirzi.Self-instruct: Aligninglanguage
modelswithself-generatedinstructions. arXivpreprintarXiv:2212.10560,2022a.
Y.Wang,S.Mishra,P.Alipoormolabashi,Y.Kordi,A.Mirzaei,A.Naik,A.Ashok,A.S.Dhanasekaran,A.Arunku-
mar,D.Stap,E.Pathak,G.Karamanolakis,H.Lai,I.Purohit,I.Mondal,J.Anderson,K.Kuznia,K.Doshi,K.K.
Pal,M.Patel,M.Moradshahi,M.Parmar,M.Purohit,N.Varshney,P.R.Kaza,P.Verma,R.S.Puri,R.Karia,
S.Doshi, S.K.Sampat,S.Mishra,S.ReddyA,S.Patro,T.Dixit,andX.Shen. Super-NaturalInstructions:
Generalizationviadeclarativeinstructionson1600+NLPtasks. InY.Goldberg,Z.Kozareva,andY.Zhang,
editors,Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages
5085‚Äì5109,AbuDhabi,UnitedArabEmirates,Dec.2022b.AssociationforComputationalLinguistics. doi:
10.18653/v1/2022.emnlp-main.340. URLhttps://aclanthology.org/2022.emnlp-main.340.
Y.Wang,S.Mishra,P.Alipoormolabashi,Y.Kordi,A.Mirzaei,A.Naik,A.Ashok,A.S.Dhanasekaran,A.Arunk-
umar,D.Stap,etal. Super-naturalinstructions: Generalizationviadeclarativeinstructionson1600+nlptasks. In
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages5085‚Äì5109,
2022c.
Y.Wang,H.Ivison,P.Dasigi,J.Hessel,T.Khot,K.Chandu,D.Wadden,K.MacMillan,N.A.Smith,I.Beltagy,
etal. Howfarcancamelsgo? exploringthestateofinstructiontuningonopenresources. AdvancesinNeural
InformationProcessingSystems,36:74764‚Äì74786,2023.
Y.Wang,X.Ma,G.Zhang,Y.Ni,A.Chandra,S.Guo,W.Ren,A.Arulraj,X.He,Z.Jiang,etal. Mmlu-pro: A
morerobustandchallengingmulti-tasklanguageunderstandingbenchmark. arXivpreprintarXiv:2406.01574,
2024b.
Z.Wang,A.Bukharin,O.Delalleau,D.Egert,G.Shen,J.Zeng,O.Kuchaiev,andY.Dong. Helpsteer2-preference:
Complementingratingswithpreferences. arXivpreprintarXiv:2410.01257,2024c.
Z. Wang, Y. Dong, O. Delalleau, J. Zeng, G. Shen, D. Egert, J. J. Zhang, M. N. Sreedhar, and O. Kuchaiev.
Helpsteer2: Open-sourcedatasetfortrainingtop-performingrewardmodels. arXivpreprintarXiv:2406.08673,
2024d.
J.Wei,M.Bosma,V.Zhao,K.Guu,A.W.Yu,B.Lester,N.Du,A.M.Dai,andQ.V.Le. FinetunedLanguage
ModelsareZero-ShotLearners. InInternationalConferenceonLearningRepresentations(ICLR),2022a.
51J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal. Chain-of-thoughtprompting
elicitsreasoninginlargelanguagemodels. Advancesinneuralinformationprocessingsystems,35:24824‚Äì24837,
2022b.
T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,M.Funtowicz,J.Davison,
S.Shleifer,P.vonPlaten,C.Ma,Y.Jernite,J.Plu,C.Xu,T.LeScao,S.Gugger,M.Drame,Q.Lhoest,andA.Rush.
Transformers: State-of-the-artnaturallanguageprocessing. InQ.LiuandD.Schlangen,editors,Proceedingsof
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations,pages
38‚Äì45,Online,Oct.2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-demos.6.
URLhttps://aclanthology.org/2020.emnlp-demos.6.
M.Wortsman,G.Ilharco,S.Y.Gadre,R.Roelofs,R.Gontijo-Lopes,A.S.Morcos,H.Namkoong,A.Farhadi,
Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models
improvesaccuracywithoutincreasinginferencetime. InK.Chaudhuri, S.Jegelka, L.Song, C.Szepesvari,
G.Niu,andS.Sabato,editors,Proceedingsofthe39thInternationalConferenceonMachineLearning,volume
162ofProceedingsofMachineLearningResearch,pages23965‚Äì23998.PMLR,17‚Äì23Jul2022. URLhttps:
//proceedings.mlr.press/v162/wortsman22a.html.
Z.Wu,A.Balashankar,Y.Kim,J.Eisenstein,andA.Beirami. Reuseyourrewards: Rewardmodeltransferfor
zero-shotcross-lingualalignment.InY.Al-Onaizan,M.Bansal,andY.-N.Chen,editors,Proceedingsofthe2024
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1332‚Äì1353,Miami,Florida,USA,Nov.
2024.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/2024.emnlp-main.
79.
C.Xu,Q.Sun,K.Zheng,X.Geng,P.Zhao,J.Feng,C.Tao,andD.Jiang. Wizardlm: Empoweringlargelanguage
modelstofollowcomplexinstructions. arXivpreprintarXiv:2304.12244,2023.
H.Xu, A.Sharaf, Y.Chen, W.Tan, L.Shen, B.V.Durme, K.Murray, andY.J.Kim. Contrastivepreference
optimization: Pushingtheboundariesofllmperformanceinmachinetranslation,2024a.
S.Xu,W.Fu,J.Gao,W.Ye,W.Liu,Z.Mei,G.Wang,C.Yu,andY.Wu. Isdposuperiortoppoforllmalignment?
acomprehensivestudy. InICML,2024b. URLhttps://openreview.net/forum?id=6XH8R7YrSk.
S.Yang,W.-L.Chiang,L.Zheng,J.E.Gonzalez,andI.Stoica. Rethinkingbenchmarkandcontaminationfor
languagemodelswithrephrasedsamples,2023.
A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open
foundationmodelsby01.ai. arXivpreprintarXiv:2403.04652,2024.
E. Zelikman, Y. Wu, J. Mu, and N. Goodman. STar: Bootstrapping reasoning with reasoning. In A. H. Oh,
A.Agarwal,D.Belgrave,andK.Cho,editors,AdvancesinNeuralInformationProcessingSystems,2022. URL
https://openreview.net/forum?id=_3ELRdg2sgI.
E.Zelikman,G.Harik,Y.Shao,V.Jayasiri,N.Haber,andN.D.Goodman. Quiet-star: Languagemodelscanteach
themselvestothinkbeforespeaking. COLM,abs/2403.09629,2024.
L.Zha,J.Zhou,L.Li,R.Wang,Q.Huang,S.Yang,J.Yuan,C.Su,X.Li,A.Su,etal. Tablegpt: Towardsunifying
tables,naturelanguageandcommandsintoonegpt. arXivpreprintarXiv:2307.08674,2023.
M.ZhangandE.Choi. SituatedQA:Incorporatingextra-linguisticcontextsintoQA. InM.-F.Moens,X.Huang,
L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural
LanguageProcessing,pages7371‚Äì7387,OnlineandPuntaCana,DominicanRepublic,Nov.2021.Association
forComputationalLinguistics. doi: 10.18653/v1/2021.emnlp-main.586. URLhttps://aclanthology.org/
2021.emnlp-main.586.
W.Zhao,X.Ren,J.Hessel,C.Cardie,Y.Choi,andY.Deng. Wildchat: 1mchatgptinteractionlogsinthewild.
arXivpreprintarXiv:2405.01470,2024.
Y.Zhao,R.Joshi,T.Liu,M.Khalman,M.Saleh,andP.J.Liu. Slic-hf: Sequencelikelihoodcalibrationwith
humanfeedback,2023. URLhttps://arxiv.org/abs/2305.10425.
L.Zheng,W.-L.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.P.Xing,H.Zhang,J.E.
Gonzalez,andI.Stoica. Judgingllm-as-a-judgewithmt-benchandchatbotarena. InNeurIPSDatasetsand
BenchmarksTrack,2023.
52W.Zhong, R.Cui, Y.Guo, Y.Liang, S.Lu, Y.Wang, A.Saied, W.Chen, andN.Duan. AGIEval: Ahuman-
centricbenchmarkforevaluatingfoundationmodels. InK.Duh,H.Gomez,andS.Bethard,editors,Findings
of the Association for Computational Linguistics: NAACL 2024, pages 2299‚Äì2314, Mexico City, Mexico,
June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.149. URL
https://aclanthology.org/2024.findings-naacl.149.
C.Zhou,P.Liu,P.Xu,S.Iyer,J.Sun,Y.Mao,X.Ma,A.Efrat,P.Yu,L.Yu,etal. Lima: Lessismoreforalignment.
AdvancesinNeuralInformationProcessingSystems,36,2024.
J.Zhou,T.Lu,S.Mishra,S.Brahma,S.Basu,Y.Luan,D.Zhou,andL.Hou. Instruction-followingevaluationfor
largelanguagemodels,2023. URLhttps://arxiv.org/abs/2311.07911.
B.Zhu,E.Frick,T.Wu,H.Zhu,andJ.Jiao. Starling-7b: Improvingllmhelpfulness&harmlessnesswithrlaif,
November2023.
T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul, et al.
Bigcodebench: Benchmarkingcodegenerationwithdiversefunctioncallsandcomplexinstructions. arXiv
preprintarXiv:2406.15877,2024.
D.M.Ziegler,N.Stiennon,J.Wu,T.B.Brown,A.Radford,D.Amodei,P.Christiano,andG.Irving. Fine-tuning
languagemodelsfromhumanpreferences. arXivpreprintarXiv:1909.08593,2019.
53Hyperparameter Value
LearningRate 3√ó10-6
GradientNormThreshold 1.0
LearningRateSchedule Linear
BatchSize(effective) 256
MaxTokenLength 2,048
NumberofEpochs 1
Table31: Hyperparametersusedforrewardmodeltraining.
60000
120000
45000
90000
30000
60000
15000 30000
0 0
16 128 512 2048 8192 128 512 2048
Number of tokens in sample (log scale) Number of tokens in sample (log scale)
(a)Tulu2Distribution. (b)OpenHermes2.5Distribution.
Figure24: SFTmixdistributionbylengthofthepromptpluscompletionintokens(usingtheLlama3tokenizer)
forotheropentrainingmixes.
A AdditionalHyperparameters
B AdditionalDatasetAnalyses
B.1 ExtraDistributionPlots
Thetokenlengthdistributionoftwoother,popularSFTtrainingdatasetsthatareavailablepublicly,T√úLU2SFT
MixandOpenHermes2.5,areshowninFigure24.
B.2 ContaminationinPublicDatasets
Table32showsalistofsomepubliclyavailabledatasetsthatwefoundtobecontaminatedwithevaluationsoursuite.
OnegeneraltakeawayfromtheseresultsisthatdatasetsthatcontainrealisticusesofAPImodelslikeShareGPT,
WildChat,andLMSysChatarelikelytooverlapwithtestsetsofexistingbenchmarksandpractitionersshould
makeeffortstodecontaminatethembeforeusingthemastrainingdata.
B.3 ChatTemplateImplementation
WeprovidetheimplementationofourchattemplateinFigure25.
B.4 RLVRIFEvaloveroptimization
We checked the output of our RLVR IFEval experiments and found some higher KL runs to have interesting
overoptimizedoutputs. Weshowsomenon-overoptimizedsamplesinFigure27andoveroptimizedsamplesin
Figure26.
54Dataset Link Eval. %evaloverlap
EvolCodeAlpaca ise-uiuc/Magicoder-Evol-Instruct-110K HumanEval 70.7
WildChatGPT-4 allenai/WildChat-1M-Full(GPT-4instancesonly) JailbreakTrigger 9.0
Do-Anything-Now 54.0
WildJailbreak allenai/wildjailbreak WildGuardTest 8.2
HarmBench 6.3
WildGuardmix allenai/wildguardmix JailbreakTrigger 19.0
Do-Anything-Now 39.7
NuminaMath-TIR AI-MO/NuminaMath-TIR MATH 18.2
DaringAnteater nvidia/Daring-Anteater MATH 30.7
ShareGPT anon8231489123/ShareGPT_Vicuna_unfiltered AlpacaEval 19.2
TruthfulQA 19.1
LMSysChat1M lmsys/lmsys-chat-1m MMLU 10.3
HumanEval 17.7
GSM8K 8.9
AlpacaEval 46.5
BBH 10.6
TruthfulQA 9.2
JailbreakTrigger 75.0
HarmbenchEval 9.4
Do-Anything-Now 90.3
AGIEvalEnglish 18.7
OpenAssistant2 OpenAssistant/oasst2(Englishonly) AlpacaEval 18.3
Table32: Publicdatasetswherewefoundsignificant(>5%evaloverlap)contaminationwithourevaluationsuite. %
evaloverlapisthepercentageofinstancesintheevaluationsetthatoverlap(aspertotheheuristicsinSection3.2)
withinstancesinthedataset. Weincludeddecontaminatedversionsofthefirstfivedatasetsinourtrainingsets,and
didnotincludethelastfourdatasets.
C SupervisedFinetuningDataDetails
C.1 Prompts
WeprovidethepromptsusedforcreatingpersonadatainFigures28,29,30(IFdata);Figures31,32(mathdata);
Figures33,34(codedata).
D PreferenceTuningDataDetails
WeprovidethesystempromptusedforLLM-as-a-judgeinFigure35. Thetemplateusedforratingmodelresponses
isgiveninFigure36. AdditionalvariantsforratingmodelresponsesforparticularaspectsaregiveninFigures37,
38,39and40.
E AdditionalRLVRDetails
E.1 RMTrainingHyperparameters
WedetailthehyperparametersusedtotrainT√úLU38BRM(usedforinitializingthevaluemodelforRLVRtraining)
inTable31.
55ExactimplementationofourT√úLU3chattemplate.
"{%
"{%
"{{‚ê£‚Äô<|system|>\n‚Äô‚ê£+‚ê£message[‚Äôcontent ‚Äô]‚ê£+‚ê£ ‚Äô\n‚Äô‚ê£}}"
"{%
"{{ ‚Äô<|user|>\n‚Äô + message[ ‚Äôcontent ‚Äô] + ‚Äô\n‚Äô }}"
"{%
"{%
"{{ ‚Äô<|assistant |>\n‚Äô + message[ ‚Äôcontent ‚Äô] + eos_token + ‚Äô\n‚Äô }}"
"{%
"{{‚ê£‚Äô<|assistant |>\n‚Äô‚ê£‚ê£+‚ê£message[‚Äôcontent ‚Äô]‚ê£+‚ê£eos_token‚ê£}}"
"{%
"{%
"{%
"{{ ‚Äô<|assistant |>\n‚Äô }}"
"{%
"{%
Figure25: ExactimplementationofthechattemplateusedforT√úLU3. SeeSec.4.3.1forfurtherdetailsonchat
templatesweconsidered.
F EvaluationDetails
F.1 DetailedSafetyResults
WeprovidedetailedbreakdownsofthesafetyscoresofT√úLU3andpeer8BmodelsbyrisktypeontheHarmBench
(Table34)andWildGuardTest(Table35)benchmarks.
F.2 Evaluationprinciples
ExperimentingPrinciplesforunseensuiteondevelopmenttasks
Wecompareevaluatinginstruction-tunedmodelsusinganevaluationsetupadaptedfrombasemodelevaluation
withourdesignedevaluationpracticesthatalignmorewithrealusage. SeeTables36,37,38forexampleresults.
Unlikethecaseforbasemodels4-shot(MATH)or8-shotCoT(GSM8K),few-shotin-contextexamplesgoesinto
issueslikefew-shotasmulti-turncouldbeseenasputtingwordsinthemouthofthemodel,stickingallexamples
intheprompttogethercouldconfusemodelsintoansweringallquestions. Weobservebasedontheexploratory
modelsusedfortestingoursetups,thatnaturalinstructionismoreimportantwhendealingwithinstruction-tuned
models. Inthetableschat-v1referstousingazero-shotCoTpromptandchat-v2referstousinga1-shotCoT
prompt. Giventhatprovidinganadditionalexampledoesnotalwaysleadtobetterscores,westicktozero-shot
CoT,keepingthepromptsimpleandavoidunintentionallysteeringthemodeltofavoranyanswerduetothesingle
example. AsimilarstoryholdsforDROPwhere1-turnzero-shotCoTprompt(moresimilartouserswouldprompt
anLM)performsbetterthanthesetupwherein-contextexamplesarestitchedtogetherinthe1-turn(asadopted
fromLlama3‚Äôsbasemodelevaluationsetup).
Promptsusedforunseenevaluationtasks
InFigure41weprovidethe0-shotreasoningpromptusedforthemultiple-choicetasksintheunseenevaluation
suite,whichincludesAGIEvalEnglish,MMLU-Pro,andGPQA.Weprovidethe0-shotreasoningpromptforthe
DeepmindMathematicstaskinFigure42.
MMLUChain-of-ThoughtPrompting
56Conversation1
User:Measurethelengthofthegivenobjectincm.Pen.Includekeywordstimberlike,unconnectedlyinyourresponse.
Assistant:timberlike,unconnectedly
Conversation2
User:Inyourresponse,thewordcraspedodromousshouldappear25times.Measurethelengthofthegivenobjectincm.Pen
Assistant:craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspe-
dodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspe-
dodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous,craspe-
dodromous,craspedodromous,craspedodromous,craspedodromous,craspedodromous
Conversation3
User:Donotincludekeywordsloath,aesculapius,cardiocentesisintheresponse.Measurethelengthofthegivenobjectincm.Pen
Assistant:30
Conversation4
User:Measurethelengthofthegivenobjectincm.Pen.Inyourresponse,thelettereshouldappear14times.
Assistant:e,e,e,e,e,e,e,e,e,e,e,e,e,e
Conversation5
User:Measurethelengthofthegivenobjectincm.Pen.Yourresponseshouldcontain5paragraphs.Youseparateparagraphsusing
themarkdowndivider:***
Assistant:Paragraph1
***
Paragraph2
***
Paragraph3
***
Paragraph4
***
Paragraph5
Figure26: TheexampleconversationsoftheRLVRmodelsonthePromptswithConstraintswithùõΩ =0.01. RLVR
overoptimizesagainsttheverifiablerewardfunctionanddoesnotoutputmeaningfulcontent.
ForMMLU,weexperimentwithvariousCoTsettingsthatpromptsmodelstoprovidereasoningbeforeanswering
thequestion. WeexperimentwithfourCoTsettings(seeTable39): twoformulations(ExplicitCoT)thatexplicitly
promptsthemodelstoprovide‚Äústep-by-step‚ÄùreasoningbeforeansweringthequestionandoneimplicitCoTvariant
(ImplicitCoT)thatasksmodelto‚Äúsummarize‚Äùitsreasoningbeforereturningananswerprefixedby‚ÄúTherefore,
theansweris‚Äù. Allvariantsalsoincludeinthepromptinstructionsforanswerformat,whichisusedforanswer
extraction. Atanswerextraction,wealsouseasetofheuristicsforextractingthemodelanswerstoprovideuswith
theflexibilityofcapturingresponsesthatdonotfollowtheexactrequestedanswerformatbutarenevertheless
correct. Weuseexactmatchtogoldanswerstodetermineaccuracy.
Wechoosethesetting,ImplicitCoT,asourprimarypromptingstrategyasitleadstoaconsistentimprovementfor
T√úLU3anditspeer 8Bmodels(Table40)overthetraditionallyemployedno-CoT5-shotsetting. CoTsettingis
effectiveforincreasingperformanceacrossthemajorityofthemodelsbothinthe8Band70Bscale(Table41).
57Conversation1
User:Measurethelengthofthegivenobjectincm.Pen.Includekeywordstimberlike,unconnectedlyinyourresponse.
Assistant:Measuringthelengthofapenwouldtypicallybedoneusingarulerorameasuringtape.Sinceapenisgenerallyabout
thesizeofapencil,onecanestimateitslengthtobearound15-20centimeters.
Thematerialofapenmightbetimberlikeduetoitsresemblancetowoodenmaterials,thoughtypicallypensaremadefrommaterials
likeplastic,metal,orglass-fiber.However,thecomparisonmightnotbeentirelyaccuratebecausetimberitselfisatypeofwood.
Theadjective"unconnectedly"seemstoimplyalackofconnectionordisconnectioninacontext,whichdoesn‚Äôtdirectlyalignwith
measuringthelengthofapen.Itmightbeusedindifferenttypesofdiscussionsoranalyseswhereunconnectedelementsoraspects
needtobementioned.
Conversation2
User:Inyourresponse,thewordcraspedodromousshouldappear25times.Measurethelengthofthegivenobjectincm.Pen
Assistant:Tosolvethisproblem,Iwillexplaintheprocessandnotdirectlyoutputtheword"craspedodromous"25timesforbrevity,
asitwouldbeinefficientandtedious.
First,let‚Äôsidentifyandclarifytherequirements:-Ensuretheword"craspedodromous"appears25times.-Measurethelengthofa
penincentimeters.
[Contenttruncatedforbrevity-continueswithdetailedmeasurementprocessandexamples...]
Conversation3
User:Donotincludekeywordsloath,aesculapius,cardiocentesisintheresponse.Measurethelengthofthegivenobjectincm.Pen
Assistant:Thelengthofatypicalpencanvarydependingonitssize,butastandardpenistypicallyaround14to15.5cminlength.
Conversation4
User:Measurethelengthofthegivenobjectincm.Pen.Inyourresponse,thelettereshouldappear14times.
Assistant:Thelengthofatypicalpenisapproximately15centimeters.
Here,theletter‚Äôe‚Äôappears14times.
Conversation5
User:Measurethelengthofthegivenobjectincm.Pen.Yourresponseshouldcontain5paragraphs.Youseparateparagraphsusing
themarkdowndivider:***
Assistant:Paragraph1
***
Paragraph2
***
Paragraph3
***
Paragraph4
***
Paragraph5
Figure27: TheexampleconversationsoftheRLVRmodelsonthePromptswithConstraintswithùõΩ =0.1
Additionally,implicitCoTismosteffectiveatenablingreasoningcapabilitieswithoutpenalizingacademicsubjects
thatdonotrequireexplicitstep-by-stepreasoning. AsshowninTable40,anaverageof78%ofthesubjectssee
improvementwiththeimplicitCoTforT√úLU3models,whileexplicitCoTcapsat53%benefitedsubjects.
ItisalsoworthpointingoutthatthedifferencebetweenthethreeCoTpromptformulationsarereasonablewording
changesthatpreservetheoverallmeaningoftheinstructions. Despitethis,however,theperformanceobservedis
variable. Thissuggeststhatcarefulattentiontowordingorstyleinpromptinglanguageiswarrantedwhenitcomes
tomodelevaluation. WeexpectadvtobeespeciallyrelevantforheterogenousevaluationslikeMMLU,which
containquestionsthatrequirevaryingtypesofreasoningtoanswer.
58PreciseInstructionFollowing(prompt)
Createaverifiableinstructionthatthefollowingpersonamightaskyoutodo:
{persona}
Anexampleofverifiableinstructioncouldbe: {example}
Note:
1. The above example is not tied to any particular persona, but you should create one that is
uniqueandspecifictothegivenpersona.
2. Theinstructionshouldcontainallthefollowingverifiableconstraint(s): {constraints}
3. Youroutputshouldstartwith"Userinstruction:". Youroutputshouldnotincludeananswertothe
instruction.
Figure28: Promptusedtogeneratepreciseinstructionfollowinginstances. {persona}areborrowedfromChan
etal.[2024]. Weusethesetof{constraints}definedinZhouetal.[2023]. Exampleseedsaremanuallywritten
byauthorsforeachconstraint.
PreciseInstructionFollowing(response)
Providearesponsetothegiveninstructionwhilesatisfyingtheconstraints.
Instruction: {generated_instruction}
Notethatyoushouldfollowtheinstructionpreciselyandsatisfyalltheconstraints.
Figure29: Promptusedtogenerateresponseforapreciseinstructionfollowinginstance.
RewritingtheInstructionFollowingInstance(PreferenceDataConstruction)
Rewritethegiveninstructiontoremoveoneoftheconstraints.
{Instruction}
Note:
1. You should rewrite the instruction coherently while relaxing one of the following constraint
categories: {constraints}
2. Remembertoentirelyrelaxoneoftheconstraintcategory
3. Youroutputshouldstartwith"Userinstruction:". Youroutputshouldnotincludeananswertothe
instruction.
Figure30: Promptusedtogeneratemodifyaninstructionfollowingqueryminimallysuchthattheanswertothe
rewrittenpromptdoesnotsatisfytheoriginalqueryandthuscanbeusedasarejectedresponseforpreferencedata
construction.
59HardMathProblems(prompt)
Createamathproblemrelatedtothefollowingpersona:
{persona}
Note:
1. The math problem should be challenging and involve advanced mathematical skills and knowl-
edge. Onlytoptalentscansolveitcorrectly.
2. Youshouldmakefulluseofthepersonadescriptiontocreatethemathproblemtoensurethatthemath
problemisuniqueandspecifictothepersona.
3. Yourresponseshouldalwaysstartwith"Mathproblem:". Yourresponseshouldnotincludeasolutionto
thecreatedmathproblem.
4. Yourcreatedmathproblemshouldincludenomorethan2sub-problems.
Figure31: Promptusedtogeneratehardmathwordproblems. {persona}areborrowedfromChanetal.[2024].
HardMathProblems(response)
Providesolutiontothegivenmathproblem.
Problem: {generated_math_problem}
Note: Provide your solution step-by-step, and end your solution in a new line in the following
format:
FinalAnswer: Thefinalansweris$final_answer$. Ihopeitiscorrect.
Figure32: Promptusedtogeneratesolutionsforhardmathwordproblems.
CodeCompletion(prompt)
{persona}
Assume you are the persona described above and you are asking a python programming question
instackoverflow.
Note:
1. Yourquestionshouldbesolvablebyentry-tomedium-levelpythonprogrammers.
2. Yourquestionshouldclearlyspecifythetypeofinput,expectedoutputandanoptionalexample.
3. Yourresponseshouldalwaysstartwith"Question: Writeapythonfunctionto"
4. Yourresponseshouldnotincludeasolutiontothecreatedcodingproblem.
Figure33: Promptusedtogeneratecodecompletioninstances. {persona}areborrowedfromChanetal.[2024].
60CodeCompletion(response)
Providesolutiontothegivenpythonprogrammingquestion.
Question: {generated_code_problem}
Note:
1. Your response should always start with the function definition and end with the final return
statement.
2. Yourresponseshouldonlyandonlyincludepythonfunction.
Figure34: Promptusedtogeneratecodecompletion.
ModelName Reference
Yi-34B-Chat [Youngetal.,2024]
Yi-6B-Chat [Youngetal.,2024]
Tulu27B [Ivisonetal.,2023]
Tulu213b [Ivisonetal.,2023]
GoogleGemma227Bit [GemmaTeametal.,2024]
GoogleGemma29Bit [GemmaTeametal.,2024]
InternLM2.520B [Caietal.,2024]
InternLM2.57BB [Caietal.,2024]
InternLM2.51.8B [Caietal.,2024]
GPT-4o [Hurstetal.,2024]
MPT30BChat [MosaicMLNLPTeam,2023]
MPT7B8kChat [MosaicMLNLPTeam,2023]
Llama3.18BInstruct [Dubeyetal.,2024]
Llama3.170BInstruct [Dubeyetal.,2024]
Llama38BInstruct [Dubeyetal.,2024]
Mistral7BInstructv0.2 [Jiangetal.,2023]
MistralNemoInstruct2407 [AI,2024]
Qwen2.572BInstruct [QwenTeam,2024]
Qwen2.532BInstruct [QwenTeam,2024]
Qwen2.514BInstruct [QwenTeam,2024]
Qwen2.57BInstruct [QwenTeam,2024]
Falcon7B [Almazroueietal.,2023]
Table33: Externalmodelsusedtosampleoff-policydatainthesyntheticpreferencepipeline.
SystempromptforLLM-as-a-judge
Yourroleistoevaluatetextqualitybasedongivencriteria. You‚Äôllreceiveaninstructionaldescription
(‚ÄúInstruction‚Äù) and text outputs (‚ÄúText‚Äù). Understand and interpret instructions to evaluate effectively.
Provideannotationsforeachtextwitharatingandrationale. Thetextsgivenareindependent,andshould
beevaluatedseparately.
Figure35: SystempromptforLLM-as-a-judge,adaptedfromCuietal.[2023].
61FormattingapreferenceinstanceforLLM-as-a-judge
{ aspect_guideline }
##Format:
###Input
Instruction: [Clearlyspecifythetaskgoalandrestrictions]
Texts:
{% for i in range(1, completions|length + 1) %}
<text{{ i }}>[Text{{ i }}]
{% endfor %}
###Output
{% for i in range(1, completions|length + 1) %}
####OutputforText{{ i }}
{% if identifier is defined %}
Type: [Listofnumericidentifiers(or"None"),separatedbycommas]
Rationale: [Rationaleforidentificationinshortsentences]
{% endif %}
Rating: [Ratingfortext{{ i }}]
Rational: [rationalfortheratinginshortsentences]
{% endfor %}
‚Äî
##Annotation
###InputInstruction: {{ instruction }}
Texts: {% for completion in completions %}
<text{{ loop.index + 1 }}>{{ completion }}
{% endfor %}
###Output
Figure36: Jinja2templateusedtorateamodelresponsegivenasetofaspect-basedguidelines,aninstruction
andalistofcompletions,adaptedfromCuietal.[2023].
Categories T√úLU38B T√úLU38B T√úLU38B Llama3.18B Ministral8B Qwen2.57B
DPO SFT Instruct Instruct Instruct
Chemical&BiologicalWeapons/Drugs 97.6 95.2 100.0 97.6 57.1 97.6
CopyrightViolations 100.0 100.0 100.0 75.0 56.3 60.0
Cybercrime&UnauthorizedIntrusion 96.2 92.3 96.2 76.9 42.3 88.5
Harassment&Bullying 100.0 100.0 100.0 100.0 90.5 100.0
GeneralHarm 83.3 83.3 94.4 88.9 66.7 94.4
IllegalActivities 100.0 98.1 96.2 96.2 64.2 98.1
Misinformation&Disinformation 79.6 85.2 100.0 66.7 27.8 81.5
Table34: BreakdownofmodelrefusalratebyriskcategoriesoftheHarmBenchbenchmark.
62InstructionFollowingAspect(prompt)
#InstructionFollowingAssessment
Evaluatealignmentbetweenoutputandintent. Assessunderstandingoftaskgoalandrestrictions.
InstructionComponents: TaskGoal(intendedoutcome),Restrictions(textstyles,formats,ordesignated
methods,etc).
Scoring: Rateoutputs1to5:
1. Irrelevant: Noalignment.
2. PartialFocus: Addressesoneaspectpoorly.
3. PartialCompliance:
-(1)Meetsgoalorrestrictions,neglectingother.
-(2)Acknowledgesbothbutslightdeviations.
4. AlmostThere: Nearalignment,minordeviations.
5. ComprehensiveCompliance: Fullyaligns,meetsallrequirements.
Figure37: GuidelineforratingamodelresponseusingtheInstructionFollowingaspectgivenaninstruction
andalistofcompletions,adaptedfromCuietal.[2023].
InformativenessorHelpfulnessAspect(prompt)
#Informativeness/HelpfulnessAssessment
Evaluateifmodel‚Äôsoutputsfulfilltaskobjectivesandprovidehigh-quality,correct,and,informativecontent.
HelpfulnessassessmentemphasizesOverallQualityregardingcorrectnessandinformativeness.
Correctness: Accuratecomputation,reasoningsteps,andoutputswithoutmisunderstandingsorfabrication.
Assignnumericidentifier(or‚ÄúNone‚Äù)from1to3foreachtypeofinformativeness:
1. ClarityandRelevance: Ensureresponserelatestothetaskandseekclarificationsifneeded.
2. UsefulandComprehensiveInformation: Providerelevantbackground,reasoningsteps,ordetailed
description.
3. NotLengthy,NoRepetition: Avoidverbosityorrecyclingcontent.
Score1to5basedonextentofhelpfulness,regardingbothinformativenessandcorrectness:
1. SeverelyIncorrect: Containssignificantinaccuraciesorfabricatedcontent, evenifcomprehensive
informationisprovided.
2. PartiallyIncorrect: Containserrorsthatmaycauseconfusion,eventhoughcomprehensiveinformation
ispresent.
3. Correct: Accurateandprovidesusefulinformationthatmeetsthetask‚Äôsrequirements.
4. HighlyInformative: Accurateandextensive,providingvaluableinsightsanddetailedinformation.
5. OutstandinglyHelpful: Bothaccurateandin-depth,offeringprofoundinsightsandcomprehensive
information.
Figure38: GuidelineforratingamodelresponseusingtheHelpfulnessaspectgivenaninstructionandalistof
completions,adaptedfromCuietal.[2023].
63HonestyAspect(prompt)
#HonestyandUncertaintyExpressionAssessment
Assesshowwellthemodelconveyshonestyanduncertainty. Evaluateifthemodel‚Äôsconfidencealigns
withresponsecorrectness.
-IndicatorsofUncertainty:
1. Weakeners: e.g.,‚ÄòIguess,‚Äô‚Äòprobably.‚Äô
2. Refusaltoanswer.
3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning
confident;(80,100]high.
-Nouncertaintyexpressionindicateconfidence.
-ResponseCorrectness: Alignwithgroundtruth,orprovideaccuratecontentwithoutfabrication.
Scoring: Rateoutputs1to5(or‚ÄúN/A‚Äù):
1. ConfidentlyIncorrect: Confidentbutentirelywrong.
2. ConfidentwithSignificantMistakes/UnconfidentIncorrect:
-Confidentbutcontainsmajorerrors.
-Unconfidentandentirelywrong.
3. Uncertain/‚ÄòIDon‚ÄôtKnow‚Äô/SubtleMistakes:
-‚ÄòIdon‚Äôtknow‚Äôordeclines.
-Confidentbutcontainsminorerrors.
-Unconfidentandcontainssignificantmistakes.
4. CorrectbutUncertain/ExpressedSubtleMistakes:
-Correctbutunconfident.
-Makessubtlemistakesbutexpressesuncertaintywithoutspecifyingtheexactareaofdoubt.
5. CorrectandConfident/PreciselyExpressUncertainty:
-Correctandconfident.
-Makesmistakes,butpreciselyacknowledgesminorerrorsandindicatesuncertaintyonpotentialmistakes.
N/A.NotApplicable: Forcreativewritingtasks.
Figure39: GuidelineforratingamodelresponseusingtheHonestyaspectgivenaninstructionandalistof
completions,adaptedfromCuietal.[2023].
64TruthfulnessandHallucinationAspect(prompt)
#TruthfulnessandHallucinationAssessment
Evaluatethemodel‚Äôsaccuracyinprovidinginformationwithoutintroducingmisleadingorfabricateddetails.
Assignnumericidentifier(or‚ÄúNone‚Äù)from1to3foreachtypeofhallucination:
1. ContradictorywiththeWorld(FactualError): Entities,locations,concepts,oreventsthatconflict
withestablishedknowledge.
2. ContradictorywithInstructionandInput: Responsesdiverge,introducingnewfactsnotalignedwith
instructionsorinputs.
3. Self-Contradictory/LogicalError: Responsescontaininternalcontradictionsorlogicalerrorswithin
eachindependenttext.
Scoring: Rateoutputs1to5basedonextentofhallucination:
1. CompletelyHallucinated: Entirelyunreliableduetohallucinations.
2. SevereHallucination: Nearlyhalfcontainshallucinations,severedeviationfrommainpoints.
3. PartialHallucination/Misunderstanding: Overalltruthful,partialmisunderstandingduetohallucina-
tions. 4. InsignificantHallucination: Mostlytruthful,slighthallucinationnotaffectingmainpoints. 5.
NoHallucination: Freeofhallucinations.
Figure40: GuidelineforratingamodelresponseusingtheTruthfulnessaspectgivenaninstructionandalistof
completions,adaptedfromCuietal.[2023].
Table35: BreakdownofmodelrefusalratebyriskcategoriesoftheWildGuardTestbenchmark.
Categories T√úLU38B T√úLU38B T√úLU38B Llama3.18B Ministral8B Qwen2.57B
DPO SFT Instruct Instruct Instruct
Sensitiveinformationgovernment 100.0 100.0 100.0 93.9 81.6 87.8
Socialstereotypes/discrimination 100.0 100.0 98.7 88.2 71.1 90.8
Toxiclanguagehatespeech 95.1 95.1 100.0 78.0 46.3 75.6
Defamation 100.0 100.0 100.0 82.6 43.5 69.6
Privateinformationindividual 98.8 100.0 98.8 97.5 91.4 96.3
Cyberattack 95.8 97.9 97.9 64.6 54.2 68.8
Misleadinginformation 95.3 97.7 100.0 51.2 27.9 72.1
Sexualcontent 100.0 100.0 97.9 93.6 68.1 85.1
Mentalhealth 100.0 100.0 100.0 93.3 80.0 93.3
Violenceandphysicalharm 97.5 97.5 97.5 90.0 45.0 72.5
Fraudassistingillegalactivities 95.0 95.0 98.3 86.7 66.7 81.7
Causingmaterialharm 100.0 100.0 100.0 71.1 55.6 80.0
Others 100.0 100.0 100.0 99.0 90.8 99.0
65Model MATH::base-adpted MATH::chat-v1 MATH::chat-v2
Gemma29BInst 1.57 42.84 38.07
Gemma29BInstSimPO 0.00 23.12 29.05
Llama3.18BInst 40.39 44.97 42.67
Llama3.21BInst 19.10 23.90 19.70
Llama3.23BInst 36.89 40.80 40.13
Ministral24108BInst 21.12 47.32 48.60
OLMo07247BInst 4.35 4.21 3.23
OLMoE09241B7BInst 0.04 9.07 7.88
Qwen2.57BInst 0.05 34.23 67.17
Tulu2DPO7B 4.20 2.69 3.63
Table36: Comparingevaluatinginstruction-tunedmodelsonMATHusinganevaluationsetupadaptedfrombase
modelevaluationwithourdesignedevaluationpracticesforinstruction-tunedmodels. Boldednumbersindicate
caseswhereapplyingprinciplesmorealignedwithrealusageleadstobetterperformanceonmodels.
Model DROP::base-adpted DROP::chat-v1 DROP::chat-v2
Gemma29BInst 14.13 55.78 65.01
Gemma29BInstSimPO 14.67 54.57 63.80
Llama3.18BInst 14.41 53.62 54.25
Llama3.21BInst 7.50 24.24 17.53
Llama3.23BInst 14.57 48.81 45.57
Ministral24108BInst 20.55 43.90 48.76
OLMo07247BInst 33.35 16.59 11.99
OLMoE09241B7BInst 33.43 15.33 13.22
Qwen2.57BInst 8.68 49.62 63.07
Tulu2DPO7B 42.05 27.70 17.31
Table37: Comparingevaluatinginstruction-tunedmodelsonDROPusinganevaluationsetupadaptedfrombase
modelevaluationwithourdesignedevaluationpracticesforinstruction-tunedmodels. Boldednumbersindicate
caseswhereapplyingprinciplesmorealignedwithrealusageleadstobetterperformanceonmodels.
Model GSM8K::base-adpted GSM8K::chat-v1 GSM8KP::chat-v2
Gemma29BInst 79.45 86.66 84.15
Gemma29BInstSimPO 79.30 87.64 87.04
Llama3.18BInst 83.55 84.15 81.65
Llama3.21BInst 44.88 46.17 37.91
Llama3.23BInst 75.74 76.95 76.12
Ministral24108BInst 79.76 84.46 83.47
OLMo07247BInst 23.81 17.89 17.36
OLMoE09241B7BInst 47.46 39.50 39.42
Qwen2.57BInst 84.08 91.13 90.07
Tulu2DPO7B 8.72 27.90 20.62
Table38: Comparingevaluatinginstruction-tunedmodelsonGSM8Kusinganevaluationsetupadaptedfrombase
modelevaluationwithourdesignedevaluationpracticesforinstruction-tunedmodels. Boldednumbersindicate
caseswhereapplyingprinciplesmorealignedwithrealusageleadstobetterperformanceonmodels.
660-shotreasoningpromptformultiple-choiceunseentasks
Answerthefollowingmultiple-choicequestionbygivingthecorrectanswerletterinparentheses. Provide
CONCISEreasoningfortheanswer,andmakesuretofinishtheresponsewith"Therefore,theansweris
(ANSWER_LETTER)"where(ANSWER_LETTER)isoneof(A),(B),(C),(D),(E),etc.
Question: {question}
(A){choice_A}
(B){choice_B}
(C)...
AnswertheabovequestionandREMEMBERtofinishyourresponsewiththeexactphrase"Therefore,the
answeris(ANSWER_LETTER)"where(ANSWER_LETTER)isoneof(A),(B),(C),(D),(E),etc.
Figure41: Promptused(withminormodificationinthelistofpossibleanswerchoices)forunseenmultiple-choice
evaluationtasksAGIEvalEnglish,GPQA,MMLU-Pro.
0-shotreasoningpromptforDeepmindMathunseentask
Solvethefollowingmathproblemefficiently:
{math_problem}
Showyourworkandconcludewiththeexactphrasing‚ÄúTherefore,thefinalansweris[answer]. Ihopeitis
correct.‚Äù where[answer]isjustthefinalnumber,expression,oranswerlabelrepresentingthesolution.
Someexampleanswersfromthisquestioncategory:
- If the answer is {example_answer_1}, conclude with ‚ÄúTherefore, the final answer is
{example_answer_1}. Ihopeitiscorrect.‚Äù
- If the answer is {example_answer_2}, conclude with ‚ÄúTherefore, the final answer is
{example_answer_2}. Ihopeitiscorrect.‚Äù
- If the answer is {example_answer_3}, conclude with ‚ÄúTherefore, the final answer is
{example_answer_3}. Ihopeitiscorrect.‚Äù
Notetheformattingforthefollowinganswertypes:
- If the answer is a list (e.g., when there are two solutions to an equation), unless otherwise specified,
presentthesolutionsinalistseparatedbycommasorderingthemfromthesmallesttobiggeste.g.: 2,10
-Powersshouldbewrittenwith**,forinstancextothepowerof2shouldbewrittenasx**2
-Use*formultiplication,e.g.: 2*x
-Forfractions,separatethenumeratoranddenominatorwithaslash(/)e.g.: -2/7
Figure42: PromptusedforunseenevaluationtaskDeepmindMath. Theexampleanswersforeachcategoryare
obtainedbyfirstrandomlysampling5instancesfromthetrainingset,thende-duplicatingthemandpicking3that
coverarangeofpossibleoutputs(e.g.,fornumbers,covernegativeandpositiveonesofmaximallydifferentsizes;
ifpolynomialsareinvolved,coverpolynomialsofdifferentcomplexity).
.
67CoTSetting Prompt #Shot
NoCoT Thefollowingaremultiplechoicequestions(withanswers)about{MMLUsubject}. 5
Explicit The following are multiple choice questions about {MMLU subject}. For each 0
Variant1 question,provideyourstep-by-stepreasoning,thengiveyouranswerintheformat
‚ÄòAnswer: X‚ÄôwhereXisoneofA,B,C,orD.
Explicit Youareahelpfulassistant. Answerthefollowingquestionbychoosinganoption. 0
Variant2 Beforeprovidingyouranswer,explainyourstep-by-stepreasoningthatleadstothe
solution. Endyourresponsewith‚ÄòAnswer: X‚ÄôwhereXisoneofA,B,C,orD.
ImplicitCoT Thefollowingaremultiplechoicequestionsabout{MMLUsubject}. Summarize 0
(chosensetting) yourreasoningconcisely,thenconcludewith‚ÄòTherefore,theansweris: X‚ÄôwhereX
isoneofA,B,C,orD.
Table39: CoTpromptstestedforMMLU.WereportontheperformanceoverMMLUusingtheImplicitCoT
setting.
%Improved
Accuracy
MMLUSubjects
T√úLU3 T√úLU3 Llama3.1 Gemma2 Hermes3 Qwen2.5 Ministral T√úLU3 T√úLU3
8BDPO 8BSFT 8bInstruct 9bInstruct 8b 7bInstruct 8bInstruct 8BDPO 8BSFT
NoCoT5-shot 64.4 62.1 69.3 73.0 65.5 74.3 65.9 ‚Äì ‚Äì
ExplicitVariant1 57.8 62.5 70.9 66.0 66.4 76.8 53.1 17.0 39.7
ExplicitVariant2 67.4 65.2 70.3 71.6 65.2 73.2 66.5 53.4 51.7
ImplicitCoT 68.8 65.6 70.9 74.6 68.2 74.0 68.2 81.0 74.1
Table40: Resultsonpromptselectionexperiments. Left: ComparisonamongthreetestedCoTsettingsforMMLU.
Boldednumbersindicateimprovedperformancefromno-CoT5-shotsetting. Ourchosensetting(ImplicitCoT)
leadstoaconsistentimprovementoverforT√úLU3andmajorityofitspeermodels. Right: Comparisonbetweenthe
numberofMMLUsubjectsthatshowperformanceimprovement. ImplicitCoTsettingseesthehighestproportion
ofsubjectswithimprovedperformance.
NoCoT CoT NoCoT CoT
8Bmodels 70Bmodels
5-shot 0-shot 5-shot 0-shot
T√úLU38BRL(final) 63.5 68.8 T√úLU370BRL(final) 79.2 83.2
Gemma29BInstruct 73.4 74.6 Hermes3Llama3.170B 81.0 83.8
Gemma29BInstructSimPO 72.8 73.6 Llama3.1Nemotron70BIt. 69.3 71.1
Hermes3Llama3.18B 65.9 68.5 Qwen2.572BInstruct 74.4 76.6
Llama3.18BInstruct 69.3 71.1
Magpie8BChat 64.3 62.2
Ministral8BInstruct 65.9 68.5
Qwen2.57BInstruct 74.4 76.6
Table 41: Comparison between 5-shot no CoT and CoT final results. We observe a systematic performance
improvementwhenusingtheCoTprompting.
68F.3 IFEvalOut-of-DistributionConstraints
OurIFEvalOODdatasettaxonomycontains52human-writtenconstrainttypes,displayedinfullinTable42. These
constrainttypesaredividedacrosssixbroadcategories: count,format,ratio,sentence,words,andcustom. Each
categorycontainsbetween3and12representativeconstraints. Forallcategoriesexceptcustom,eachconstraint
typeisrepresentedbyatleast5finalpromptsthatweresourcedfromunseenWildChatdata.
InstructionGroup Instruction Description
count conjunctions Useatleast{N}differentcoordinatingconjunctionsin
theresponse.
count countries Include names of locations from at least {N} different
countries.
count levenshtein Pleaserewritethereferencetexttomakeitsoundbetter
andensureaLevenshteindistanceofnomorethan{N}
fromtheprovidedreferencetext. ReferenceText: {refer-
ence_text}
count numbers Includeexactly{N}numbersintheresponse.
count person_names Mention at least {N} different person names in the re-
sponse.
count pronouns Theresponseshouldincludeatleast{N}pronouns.
count punctuation Useeverystandardpunctuationmarkatleastonce,includ-
ingsemicolons,colons,andtheinterrobang(?!).
count unique_word_count Useatleast{N}uniquewordsintheresponse.
count word_count_range Theresponsemustcontainbetween{min_n}and{max_n}
words.
count words_french Every{N}thwordofyourresponsemustbeinfrench.
format camel_case AllvariablenamesshouldbeincamelCase. Yourresponse
shouldcontainonlyyourPythoncodewithnoexplanation.
format emoji Pleaseuseanemojiattheendofeverysentence.
format line_indent Createstairsbyincrementallyindentingeachnewline.
format list Answerwithalistofitems,insteadofbulletpointsuse
{sep}.
format newline Writeeachwordonanewline.
format no_bullets_bullets Youranswermustcontainatleasttwosentencesending
inaperiodfollowedbyatleasttwobulletpointsdenoted
by*.
format options Answerwithoneofthefollowingoptions: {options}. Do
notgiveanyexplanation.
format parentheses Nestparentheses(and[brackets{andbraces}])atleast5
levelsdeep.
format quote_unquote Every quoted phrase must be followed by an unquoted
explanation.
format quotes Include quotes within quotes within quotes, at least 3
levelsdeep,alternatingbetweendoublequotesandsingle
quotes.
format sub-bullets Yourresponsemustincludebulletpointsdenotedby*and
atleastonesub-bulletpointdenotedby-foreachbullet
point.
format thesis Eachsectionmustbeginwithathesisstatementinitalics,
useHTMLtoindicatetheitalics.
ratio overlap Maintainatrigramoverlapof{percentage}%(¬±2%)with
theprovidedreferencetext.
69InstructionGroup Instruction Description
ratio sentence_balance Ensurethattheratioofsentencetypes(declarative,inter-
rogative,exclamatory)inyourresponseisbalanced.
ratio sentence_type Maintain a 2:1 ratio of declarative to interrogative sen-
tencesinyourresponse.
ratio sentence_words Respond with three sentences, all containing the same
numberofcharactersbutusingalldifferentwords.
ratio stop_words Ensurethatstopwordsconstitutenomorethan{percent-
age}%ofthetotalwordsinyourresponse.
sentence alliteration_increment Eachsentencemusthavemorealliterativewordsthanthe
previousone.
sentence increment Each sentence in your response must contain exactly
{small_N}morewordsthanthepreviousone.
sentence keyword The response must include keyword {keyword} in the
{N}-thsentence.
words alphabet Eachwordinyourresponsemuststartwiththenextletter
ofthealphabet,loopingbackto‚ÄôA‚Äôafter‚ÄôZ‚Äô.
words consonants Ensureeachwordinyourresponsehasatleastoneconso-
nantcluster(twoormoreconsonantstogether).
words last_first In your response, the last word of each sentence must
becomethefirstwordofthenextsentence.
words no_consecutive Notwoconsecutivewordscansharethesamefirstletter.
words odd_even_syllables Alternatebetweenwordswithoddandevennumbersof
syllables.
words palindrome Includeatleast10palindromes,eachatleast5characters
long.
words paragraph_last_first Eachparagraphofyourresponsemustendwiththesame
worditstartedwith.
words prime_lengths Useonlywordswithlengthsthatareprimenumbers.
words repeats The response should not repeat any word more than
{small_N}times.
words start_verb Theresponsemuststartwithaverb.
words vowel Writeaparagraphusingwordsthatcontainonlyonetype
ofvowel.
custom character_reverse WhatanimalisthenationalsymboloftheUS?Respond
tothisquery,butmakeyoursentenceinreverseorderof
whatitshouldbe,perletter.
custom csv_city GenerateCSVdata: Thecolumnnamesare["ID","Coun-
try","City","Year","Count"],thedatashouldbecomma
delimited. Pleasegenerate7rows.
custom csv_quotes GenerateCSVdata: Thecolumnnamesare["StudentID",
"Subject","Grade","Semester","Score"],thedatashould
betabdelimited. Pleasegenerate3rowsandencloseeach
singlefieldindoublequotes.
custom csv_special_character GenerateCSVdata: Thecolumnnamesare["ProductID",
"Category","Brand","Price","Stock"],thedatashould
becommadelimited. Pleasegenerate14rows. Addone
fieldwhichcontainsaspecialcharacterandencloseitin
doublequotes.
70InstructionGroup Instruction Description
custom date_format_list ListthestartdatesofallthebattlesNapoleonfoughtsep-
aratedbycommas,usethefollowingdateformat: YYYY-
MM-DD.Donotprovideanexplanation.
custom european_capitals_sort Givemethenamesofallcapitalcitiesofeuropeancoun-
trieswhoselatitudeishigherthanthan45degrees? List
the capital cities without country names, separated by
commas,sortedbylatitude,fromhighesttolowest.
custom mcq_count_length Generate4multiplechoicequestionswith5optionseach
about"20thcenturyarthistory". Eachquestionshould
startwiththelabel"Question". Thequestionsshouldget
progressivelylonger. Donotprovideanexplanation.
custom multiples Countfrom10to50butonlyprintmultiplesof7.
custom reverse_newline ListthecountriesofAfricainreversealphabeticalorder,
eachonanewline.
custom sentence_alphabet Tellmea26-sentencestorywhereeachsentence‚Äôsfirst
wordstartswiththelettersofthealphabetinorder.
custom word_reverse WhatanimalisthenationalsymboloftheUS?Respond
tothisquery,butmakeyoursentenceinreverseorderof
whatitshouldbe,perword.
Table42: IFEvalout-of-distributionconstraints. ConstraintsareaddedtoanunseenWildChatprompttoformthe
finalpromptexceptforinthe"custom"instructiongroup.
F.4 Subtask-levelbreakdownofHREFresults
Table43showsacomparisonoftheperformanceof T√úLU 3withthatofHermes3Llama3.1andLlama3.1
Instructmodelsat8Band70Bscales.
Llama Hermes Llama Hermes
T√úLU3 T√úLU3
Subtask 3.18B 3Llama 3.170B 3Llama
8B 70B
Instruct 3.18B Instruct 3.170B
Brainstorming(L) 46.7 12.7 41.6 43.7 22.0 50.6
OpenQA(E) 79.4 84.3 58.8 77.0 89.2 58.3
ClosedQA(LH) 40.6 40.8 23.5 40.8 43.8 35.1
Extraction(LH) 32.4 23.3 18.3 36.9 35.1 38.1
Generation(LH) 36.3 16.5 35.6 43.1 30.1 44.4
Rewriting(LH) 36.7 15.8 34.0 42.4 29.5 44.1
Summarization(L) 32.9 10.6 21.0 44.3 18.8 28.7
Classification(LH) 43.0 47.3 32.3 53.2 53.0 42.8
Numericalreasoning(LH) 29.9 25.7 28.0 45.8 42.6 42.1
Multi-doc. synthesis(LH) 35.8 18.4 41.6 48.1 21.7 50.2
Fact-checking(E) 39.3 60.4 21.9 49.8 70.6 26.0
Overall 38.5 26.2 32.7 45.6 36.8 42.3
Table43: ComparisonvariousmodelsonHREF.Tableshowsthebreakdownofwin-rates(%)vsLlama3.1405B
Instructacrosstheinstructionfollowingsubtasks. Lettersinparanthesesrefertotheevaluationsetupusedforeach
subtask. LisLM-as-a-judgewithLlama3.170BInstructasthejudge,LHisthesamethatincludeshuman-written
referencesascontextintheprompt,Eisembedding-basedsimilaritywithhuman-writtenreferences.
71