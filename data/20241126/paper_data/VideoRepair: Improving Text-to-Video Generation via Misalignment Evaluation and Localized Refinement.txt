VIDEOREPAIR: Improving Text-to-Video Generation
via Misalignment Evaluation and Localized Refinement
DaeunLee JaehongYoon JaeminCho MohitBansal
UNCChapelHill
{daeun,jhyoon,jmincho,mbansal}@cs.unc.edu
https://video-repair.github.io
1bear and 2 people making pizza A camel lounging in front of a snowman
Mis-aligned video
A dog sitting under a umbrella A group of two artists collaborate on a painting,
on a sunny beach each adding their own unique touch to the canvas.
VideoRepair
1. Video Evaluation
2. Refinement Planning
A blue car parked next to Five colorful parrots perch on a branch,
a red fire hydrant on the street. 3. Region Decomposition squawking loudly at each other.
4. Localized Refinement
Refined video
Initial video + VideoRepair (Ours) Initial video + VideoRepair (Ours)
Figure1. VIDEOREPAIRisamodel-agnostic,training-free,automaticrefinementframeworkforimprovingalignmentsintext-to-video
generation.Givenaninitialvideofromatext-to-videogenerationmodel,VIDEOREPAIRrefinesvideoinfourstages:(1)videoevaluation,
(2)refinementplanning,(3)regiondecomposition,and(4)localizedrefinement.Theblack-whitemaskinthebottomleftofeachexample
indicatesthelocalizedrefinementplan(black:regionstopreserve/white:regionstorefine).
Abstract generatingfine-grainedevaluationquestionsandanswering
thosequestionswithMLLM.In(2)refinementplanning,we
identifyaccuratelygeneratedobjectsandthencreatelocal-
Recenttext-to-video(T2V)diffusionmodelshavedemon-
izedpromptstorefineotherareasinthevideo. Next,in(3)
strated impressive generation capabilities across various
regiondecomposition,wesegmentthecorrectlygenerated
domains. However,thesemodelsoftengeneratevideosthat
areausingacombinedgroundingmodule. Weregenerate
havemisalignmentswithtextprompts,especiallywhenthe
the video by adjusting the misaligned regions while pre-
promptsdescribecomplexsceneswithmultipleobjectsand
servingthecorrectregionsin(4)localizedrefinement. On
attributes. Toaddressthis,weintroduceVIDEOREPAIR,a
twopopularvideogenerationbenchmarks(EvalCrafterand
novelmodel-agnostic,training-freevideorefinementframe-
work that automatically identifies fine-grained text-video
T2V-CompBench),VIDEOREPAIRsubstantiallyoutperforms
recentbaselinesacrossvarioustext-videoalignmentmetrics.
misalignments and generates explicit spatial and textual
feedback, enabling a T2V diffusion model to perform tar-
Weprovideacomprehensiveanalysisof VIDEOREPAIRcom-
ponentsandqualitativeexamples.
geted,localizedrefinements. VIDEOREPAIRconsistsoffour
stages: In(1)videoevaluation,wedetectmisalignmentsby
1
4202
voN
22
]VC.sc[
1v51151.1142:viXraInitial Video
Original Object / Attribute Misalignment
T2V model Training-free
Three llamas Misaligned! Refinement âœ“Missing object âœ“Semantic Leakage
on a high plateau
(a)Prompt Optimization Refined Video
Prompt N Iteration Original
Rewriting T2V model
âœ“No visual guidance âœ“Expensive search
(b)Localized Feedback with External Generator Refined Video
The man had been pruning 2 Dog and a whale,
Localized External the roses in hisbackyard ocean adventure
Feedback Generator
âœ“Incorrect count âœ“Incorrect Attribute
âœ“Poor Integration âœ“Require external generator
(c) VideoRepair (Ours) Refined Video
Localized Original
Feedback T2V model
âœ“Fine-grained localized visual guidance
Figure2.Comparisonofdifferentrefinementmethodsforalign- Four children play tag in a park A brown bird and a blue bear
ment.(a)Promptoptimization(e.g.,OPT2I[29])byLLM-based
rewritingwithoutvisual/fine-grainedfeedback,makingthesearch
expensive(e.g.,30iterations).(b)Recentworkonlocalizedfeed- Figure3.ExamplemisalignmentsinT2Vgeneration.Left:object
back(e.g., SLD[52])providesvisualfeedbackbutrequiresthe misalignment(e.g.,manismissing/5childreninsteadof4).Right:
useofanexternallytrainedlayout-guidedgenerationmoduleand attributemisalignment(e.g.,dogâ€™scolorisleakedfromdolphinâ€™s
result in unnatural refined output. Our (c) VIDEOREPAIR is a color/bearisbrowninsteadofblue). Videosaregeneratedwith
training-free,model-agnosticrefinementframeworkforT2Valign- T2V-turbo[18].
mentthatprovidesfine-grainedlocalizedvisualguidanceanduses
theoriginalT2Vmodel.
generator(e.g.,GLIGEN[19]),andtheaddedobjectoften
poorlyharmonizeswiththeoriginalimage(seeFig.5).
1.Introduction
To address these limitations, we introduce VIDEORE-
Recenttext-to-video(T2V)diffusionmodels[3,8,11,14,41, PAIR, a new framework that automatically detects fine-
47,54]haveshownimpressivephotorealismandversatility grainedtextmisalignmentingeneratedvideosandperforms
acrossdiversedomains. However,thesemodelsoftenstrug- localizedrefinements(Fig.2(c)). VIDEOREPAIRperforms
gletogeneratevideosthataccuratelyfollowtextprompts, a four-stage process: (1) video evaluation, (2) refinement
particularlywhenthepromptincludesmultipleobjectsand planning,(3)regiondecomposition,and(4)localizedrefine-
attributes,suchasincorrectnumberofobjectsorattribute ment,asillustratedinFig.4. In(1)videoevaluation,wefirst
bindings(seeFig.3). Suchmisalignmentproblemslargely generatealistoffine-grained,object-centricevaluationques-
discouragepracticalapplications. tionsfrom theinitialpromptwith anLLM(e.g., GPT-4o)
Several recent work has studied enhancing text align- and establish corresponding answer, including evaluation
mentsofdiffusionmodelsviatraining-freeiterativerefine- score(i.e.,0or1),usinganMLLM(e.g.,GPT-4o[31])to
ments[29,52].Promptoptimization[29]iterativelysearches identify errors in the generated videos. In (2) refinement
forbetterprompts,bycreatingmultiplevariationsofprompts planning, we identify accurately generated objects using
withLLMandchoosingvideoswiththehighestscore(e.g., obtainedquestion-answerpairs. Here,wealsocreatealo-
DSG[6]),asdescribedinFig.2(a). However,asthereisno calized prompt for the refinement region, specifying the
explicitfeedbackformisalignment,suchmethodissensitive objects,attributes,andconceptstoberefinedandhowthey
toinitialnoise[1,30,37,45]andthusrequiresmanyitera- shouldbeadjusted. Thispromptwillguidetherefinement
tions(e.g.,30)toreachapromptthatimprovesalignment.In processinthelaststage. In(3)regiondecomposition,we
adifferentapproach,SLD[52]proposesarefinementframe- adoptMolmo[7]andSemantic-SAM[15]tosegmentthe
work with more explicit guidance, as described in Fig. 2 regionstokeepinthevideoduringrefinement. Finally,in
(b). SLDfirstgeneratesabounding-boxlevelplanwithan (4)localizedrefinement,wegenerateanewvideobasedon
LLM,thenrunsasetofrefinementoperations(e.g.,object MultiDiffusion[2],bymaintainingthecorrectly-generated
addition,deletion,reposition)followingtheplan. However, regionswiththeoriginalnoises/promptsandupdatingthe
theobjectadditionrequiresanexternallayout-guidedobject remainingregionswithrefinementprompt+resamplednoise.
2We demonstrate the effectiveness of VIDEOREPAIR in whereanLLMprovidesabounding-boxlevelplan,followed
two popular text-to-video generation benchmarks: Eval- byasetofoperations(e.g.,objectaddition,deletion,repo-
Crafter[25]andT2V-CompBench[44],whereVIDEORE- sition). However,SLDrequiresanexternallayout-guided
PAIR significantlyoutperformsrecentrefinementmethods object generator (e.g., GLIGEN [19]) to add objects, and
acrossvarioustypesofcompositionalpromptswithdifferent theaddedobjectoftenpoorlyharmonizeswiththeoriginal
objectnumber,attributes,andlocations. Weprovidecom- image. VIDEOREPAIRisatraining-freerefinementframe-
prehensiveanalysisonVIDEOREPAIRcomponents. Lastly, workthatgivesfine-grainedlocalizedfeedback,workswith
we provide qualitative examples where VIDEOREPAIR is anyT2Vdiffusionmodel,anddoesnotrequireadditional
moreeffectiveinimprovingtext-videoalignmentsthanbase- generator.
lines. Wehopeourstudyencouragesfutureadvancementsin
automaticrefinementframeworksinvisualgenerationtasks. 3. VIDEOREPAIR: Improving Text-to-Video
Generation via Misalignment Evaluation
2.RelatedWorks
andLocalizedRefinement
Text-to-videogenerationwithdiffusionmodels. Text-to-
video(T2V)diffusionmodels[3,8,11,12,14,26,41,47,
WeproposeVIDEOREPAIR,anovelautomaticrefinement
49,51,54,56]aimtoproducevideosdescribinggiventext
frameworkforT2Vgeneration. VIDEOREPAIRisdesigned
withthreekeyquestionsinmind: (i)whichpromptelements
prompts. Thesemethodstrainadenoisingmodelthatcan
aremisalignedinthevideo?;(ii)whichobjectsarevaluable
graduallygenerateclearvideosfromnoisyvideos,wherethe
tobepreserved?;(iii)whichvideoareasshouldbemodified?;
noisesareaddedviadiffusionprocess[10]. Thedenoisingis
commonlyperformedinacompactlatentspaceofanautoen-
(iv)howcanweupdatethevideo? In VIDEOREPAIR, we
explicitlydetect,localize,andaddressthefine-grainederrors
coder[39]forcomputationalefficiency. VideoCrafter2[4]
ofT2Vgenerationmodelsinafour-stageprocess: (1)video
synthesizes low-quality videos with high-quality images
evaluation(Sec.3.1),(2)refinementplanning(Sec.3.2),(3)
throughajointtrainingdesignofspatialandtemporalmod-
regiondecomposition(Sec.3.3),and(4)localizedrefinement
ules,obtaininghigh-qualityvideos. T2V-turbo[18]presents
(Sec.3.4). Below,wedescribetheproblemdefinitionand
adistilledvideo consistencymodel[42, 48]for improved
detailsofeachstage.
and rapid video generation. A line of recent work also
studiesLLM-guidedplanningframeworks,whereanLLM
3.1.VideoEvaluationwithMisalignmentDetection
firstgeneratesanoverallplan(e.g.,listofboundingboxes)
thenvideodiffusionmodelsrenderthescenefollowingthe Generatingevaluationquestions. Ourprimarygoalisto
plan[21,22,27]. However,eventherecentT2Vdiffusion generateavideothatachievesimprovedalignmentwithtext
modelssufferfrommisalignmentproblems.Inthefollowing, prompts,usingapre-trainedT2Vdiffusionmodelwithout
wediscusstheresearchdirectionofrefiningtheimage/video requiringadditionalfine-tuning. Givenaninputtextprompt
diffusionmodels,includingVIDEOREPAIR. pandinitialnoiseÏµ
0
âˆ¼ N (0,I), wefirstgenerateanini-
tialvideoV =f(p,Ïµ ). Toidentifywhichelementsofthe
0 0
Automaticrefinementforimage/videodiffusionmodels. promptaremisalignedwiththegeneratedvideo,wecreate
Recentworksproposerefinementframeworksthatautomati- alistofevaluationquestionsdesignedtobeansweredwith
callyimprovediffusionmodelsâ€™textalignment[17,29,43, â€œyesâ€orâ€œno.â€ Specifically,wegenerateobject-centricques-
52]. Alineofworkstudiestraining-basedrefinement,where tionsbyprovidingmanuallywrittenin-contextexamplesto
theydetecterrorsofadiffusionmodel,generatetrainingdata, theLLM.
andthenfinetunethemodeltoimprovealignment[17,43]. Following DSG [6], we first define semantic cate-
However,training-basedmethodsareexpensiveandcanof- gories consisting of entities, attributes, and relationships.
tenmakethemodeloverfittospecificdomainsofgenerated Each element is represented as a semantic tuple T: at-
trainingdata. Anotherlineofworkproposestraining-free tributesareexpressedin2-tuples(entity,itsattribute,(e.g.,
refinement[29,52]. OPT2I[29]presentsiterativeprompt {bed, blue}), and relationships are in 3-tuples (sub-
optimization,whereanLLMprovidesvariousvariationsof ject entity, object entity, and their relationship, (e.g.,
text prompts, T2I diffusion models generate images from {people, pizza, make}). Based on T, which cov-
the prompts, and the images are ranked with a T2I align- ersallscene-relevantinformation,wegeneratequestionsQ
mentscore(e.g.,DSG[6])toprovidethefinalimage. Since usingtheLLM(e.g.,GPT-4).
no explicit feedback is given to the backbone generation NotethatalthoughDSGincludesâ€œcountâ€questions,they
model,itusuallytakeslongiterations(e.g.,30LLMcalls) areonlygeneratedwhenthereismorethanoneobjectclass;
tofindapromptthatprovidesimprovedalignment,making i.e.,thereisnopenaltyaboutobjectcountswhenthereisa
theframeworkexpensivetouseinpractice. SLD[52]pro- singleobjectintheprompt. Forexample, givenaprompt
posesarefinementframeworkwithmoreexplicitguidance, â€˜thereisabearâ€™,DSGonlygeneratesanevaluationquestion
3Misaligned!
1 bear and 2 people + T2V model VideoRepair Improved
making pizza Text-video Alignment!
Initial Prompt ğ‘ Initial Noise ğœ–0 Initial Video ğ‘‰0 Refined Video ğ‘‰1
Step 1.
1
V bi
e
md ae
ar
kao
in
n
E
d
g
v
2
pa
ip
zl
e
zu aoa pt leio n
LLM
1
2 3
â€¦..
.
( ((b ppe eeoa or
p
p,
l l
1
e
e)
, , 2 p) izza, make) ğ‘„
ğ‘„ğ‘„
ğ‘
ğ‘ğ‘
ğ‘
ğ‘ğ‘ğ‘’
ğ‘
ğ‘ğ‘
ğ‘™
ğ‘™ğ‘Ÿ :
:
:
â€œ
â€œ
â€œI
A
As
r
r
t
e
eh
t
te
h
hr
e
ee
r
po
e
en
t
owe
p
ob
le
e
p
a mer o? apâ€
kl ie n? gâ€
pizza?â€
MLLM ğ´
ğ´ğ´
ğ‘ ğ‘ğ‘
ğ‘ğ‘ ğ‘ğ‘’
ğ‘
ğ‘ğ‘
ğ‘™
ğ‘™ğ‘Ÿ :
:
:
{
{
{
YN
N
eo
o
s,
,
}1 2,
,
2
3
}
}
ğ‘«ğ‘ºğ‘®ğ’ğ’ƒğ’‹: Fine-grained Semantic Tuples & Generating Evaluation Question Evaluation Results
Step 2. Refinement Planning
G wi hv ae tn
i
sth te
h
e{e mva ol su ta ati co cn
u
r re as teu ll yts
g
f er no em
ra
s tt ee dp o1 b},
ject?
MLLM ğ‘¶ ğ‘µâˆ— âˆ—:
:
â€œ 1bearâ€ G froiv men s t th ee p { 1e }v , a wl hua at ti so hn o r ue ls du blt es in LLM maâ€œ k2
i
np ge o pp izl ze
a â€
the other region?
Choose which object(s) to preserve ğ‘‚ ğ‘âˆ— âˆ—:
:
o hb oj wec mt t ao
n
k ye oe fp
ğ‘‚âˆ— to keep
Plan how to refine the other region R Pe rf oin me pm
t
e ğ‘n ğ‘Ÿt
Step 3. Region Decomposition
â€œPoint {ğ‘µâˆ—} {ğ‘¶âˆ—}â€ Molmo Semantic
= â€œPoint 1 bearâ€ SAM
Pointing Prompt ğ‘ğ‘ Pointing ğ‘‚âˆ— Region to keep Region to refine
Step 4. Localized Refinement
In Init ii ta ial lN Po rois me pğğŸ
t
ğ’‘&
+
R Re es fia nm empl ee nd
t
N Po rois mes
p
tğ ğ’‘0â€²&
ğ’“ T2V model
1 bear and 2 people â€œ2 people Ranking
making pizza making pizzaâ€ (reusing ğ·ğ‘†ğºğ‘œğ‘ğ‘— ) Refined Video ğ‘‰1
Figure4. IllustrationofVIDEOREPAIR. VIDEOREPAIRrefinesthegeneratedvideoinfourstages: (1)videoevaluation(Sec.3.1),(2)
refinementplanning(Sec.3.2),(3)regiondecomposition(Sec.3.3),and(4)localizedrefinement(Sec.3.4).Fromtheinitialpromptp,we
firstgenerateafine-grainedevaluationquestionsetQoandaskMLLMtoanswerit.Next,weidentifyOâˆ—byMLLMandplanhowtorefine
theotherregionbyLLM.UsingOâˆ—,wedecomposetheregionbyMolmoandSemanticSAMtogettheregiontokeep. Intheend,we
conductlocalizedrefinementbytheoriginalfrozenT2Vmodel.
â€œisthereabear?â€,whichonlychecksthebearâ€™sexistence, â€˜Isthereonebear?â€™ correspondstoQo(count-relatedques-
c
butdoesnotpenalizewhenmorethanonebearisgenerated. tion). Specifically, for attribute-related questions Qo, we
a
Toaddressthis,weeditin-contextexamplesofDSGsothat promptGPT-4otoanalyzetheinitialvideoV andprovidea
0
it can generate count-related questions for a single object binaryanswer(yes=1andno=0),denotedasAo ={bo},to
a a
(e.g., â€˜Isthereonebear?â€™) aswell, asvisualizedinFig.4 evaluatethealignmentbetweentheobjectsinV andthose
0
step 1. We denote our modified DSG as DSGObj, Q as specifiedinthepromptp. Forcount-relatedquestionsQo,
c c
count-related questions, and attribute/relationship-related wegeneratetheexactnumberofobjectsandabinaryanswer
questionsasQ . Ourablationstudy(seeTable3)showsthat intheformofatripletAo ={bo,no,no},whereno andno
a c c p v p v
evaluationquestionsofDSGObj aremoreeffectivethanthe arethenumbersoftheobjectOinpandV . Thatis,bo =1
0 c
originalDSGquestionsinguidingvideorefinements. See ifno = no,otherwisebo = 0. Forexample,inFig.4,for
p v c
appendixformoredetails. thequestionâ€œIsthereonebear?â€ bbear = 0whennbear = 1
c p
andnbear =2(whenthevideoV showstwobearsandthe
v 0
promptdescribesonlyonebear,itismarkedincorrect).
Answering to identify video errors. We now evaluate
Inthisway,wecanquantitativelyassessboththepresence
the generated video V to identify elements that are ac-
0 and count of essential objects, enabling us to detect any
curately generated and those that require refinement. We
discrepancies between the objects specified in V and p.
first define the set of key object names in p from entity 0
Here,weusethefirstframeofV asinputforGPT-4o.
tuplesandgroupquestionsinQcorrespondingtoeachob- 0
ject. Let Qo = {Qo,Qo} âŠ‚ Q be the set of questions re-
a c 3.2. RefinementPlanning: whattokeepandrefine
latedtoobjectO(e.g.,bear),includingbothattribute-related
(Qo)andcount-relatedquestions(Qo),eachrequiringadis- Identifyingvisualcontenttoberetained. Asmentioned
a c
tinctanswerformat. Forexample,â€˜Arethepeoplemaking earlier,theinitiallygeneratedvideomaysufferfrominsuf-
pizza?â€™ correspondstoQo (attribute-relatedquestion),and ficient text-video alignment due to incorrect or distorted
a
4generationofobjectsandtheirattributes. However,thisdoes TolocalizethecorrectlygeneratedOâˆ—area,wegenerate
notmeanthatallcomponentsaremis-generated;somekey alocalmaskfortheareatoberefinedusingMolmo[7],a
conceptsorobjectsmayhavebeengeneratedpreciselyin VLMthatcanlocalizespecificobjectsbyreferring2dpoints
certainareas. VIDEOREPAIRaimstoretaintheseaccurately in an image given a text-pointing prompt, and Semantic-
generated portions while focusing on correcting only the SAM[15].Asillustratedinstep3ofFig.4,wefirstconstruct
mis-generatedregionstoensureimprovedtext-videoalign- apointingpromptpp usingpredefinedNâˆ— andOâˆ—,â€œPoint
ment. Tothisend,wefirstidentifythekeyobjectOâˆ— and {Nâˆ—}{Oâˆ—}â€(e.g.,â€œPoint1bearâ€)andobtainthereferring
determinethenumberofitsinstancestobepreserved. First, 2DcoordinatesforOâˆ—withMolmo. Then,usingthispoint
toselectwhichobject(i.e.,objectname)shouldberetained, asinput,weemploySemantic-SAMtosegmentthespecified
wepromptGPT-4owithquestion-answerpairsandtheinitial areaandfinallyrepresentitasabinarysegmentationmask
videoasinput,allowingittoidentifythemostsuitableobject MâˆˆRHÃ—W,whereH andW denotestheheightandwidth
topreserve. Next,todeterminethenumberofinstancesof ofthevideoframe,respectively.
Oâˆ—toretain,wedefinethecountofOâˆ—asNâˆ—basedonthe
previoustripletAoâˆ— ={boâˆ—,noâˆ—,noâˆ—}asfollows: 3.4.LocalizedRefinement
c c p v
Localized noise re-initialization. At this stage, we re-
(cid:40)
noâˆ— ifnoâˆ— â‰¤noâˆ— finethevideotoachieveamoreaccuratelyalignedoutput
Nâˆ— = nop âˆ— othep rwise,v (1) video. Inspired by region-based text-to-image generation
v framework [2, 19], we adopt a mask-based segmentation
approach to control specific regions within the video gen-
where noâˆ— < noâˆ— indicates the need to remove excess in-
p v eration process. Given the region to refine M (obtained
stancesoftheobject,andnoâˆ— >noâˆ— suggeststhatadditional
p v inSec.3.3),weintroduceaselectivenoisere-samplingpro-
instancesarerequired.Forexample,instep2ofFig.4,ifOâˆ—
cess to enable controlled regeneration of specific regions.
representsabearwithnoâˆ— =1andnoâˆ— =2,wesetNâˆ— =1,
p v Specifically,differentfromMultiDiffusion[2],wepreserve
preservingonlyonebearbecausethereisanextrabearin thepartialregionoftheinitialnoisemapÏµ detectedbyM
the generated content (noâˆ— < noâˆ—), requiring the deletion 0
p v andre-initializetherestareaswithanewlysamplednoise
ofanextrainstance. Thisapproacheffectivelyremovesthe Ïµâ€² âˆ¼N(0,I)toexpectdifferentgenerationtendencyfrom
0
additionalinstance,ensuringaccurateobjectrepresentation.
initialvideo.Thisaimstokeepthemaskedareasinthevideo
consistentwhileallowingtheunmaskedareastoberefined
basedontheupdatedprompt.
Prompt regeneration for regions requiring refinement.
To process the pixel-level mask M in the latent space,
Weadditionallygeneratealocalpromptforrefinementtoen-
we transform M from pixel space to latent space through
abledistinctcontroloverdifferentregionsduringgeneration.
block averaging (i.e., pooling). Specifically, we take the
To this end, we prompt an LLM to produce a refinement-
orientedprompt,pr,basedonQbutexcludinganyquestions meanofeach|H/c|Ã—|W/c|submatrixwithinM,wherec
related to Oâˆ—. As illustrated in Fig. 4, steps 2 and 4, this denotesthedownsamplingscalefactor,effectivelyreducing
thespatialresolutionofthemask. Weapplythistransformed
regeneratedlocalpromptwillbeusedtoguidethedenoising
maskconsistentlyacrosstheentiretemporaldomain. The
processforspecificareastoberefinedduringvideogenera-
combinednoisemapÏµâˆ—isthencomputedasfollows:
tioninalaterstage(willbediscussedinSec.3.4). 0
Ïµâˆ— =(Ïµ âŠ—pool(M,c))+(Ïµâ€² âŠ—(1âˆ’pool(M,c))), (2)
3.3.RegionDecomposition 0 0 0
Given the errors identified in earlier stages, we localize where pool(Â·,c) : RHÃ—W â†’ R|H/c|Ã—|W/c| denotes the
thevideoregionscorrespondingtotheerrorstocreatecon- blockaveraging(i.e., pooling)operationwithscalefactor
creteguidanceinthefollowingrefinementstage. Similarly, c,andâŠ—representselement-wisemultiplication,whichpre-
SLD[52]usesanopen-vocabularyobjectdetectortodetect servestheinitialnoisemapstructureinthemaskedregions.
localizationerrorsthroughboundingboxes.However,unlike UsingthishybridnoisemapÏµâˆ— 0 asinputtoafrozenvideo
T2Igeneration,T2Vgenerationoftenintroducescomplex diffusionmodelwithcorrespondinglocalizedprompts,we
distortions(e.g.,attributemixing)acrossscenes,makingit achieve targeted refinement within designated regions, re-
difficulttocaptureallcasesofdistortionusingonlyamodel- sulting in high-fidelity video with controlled updates that
baseddetector. Additionally,segmentingeachobjectarea alignwiththeintendedmodifications.
withseparateboundingboxescomplicatesinteractionsbe-
tweenobjectsinthescene. Toaddressthesechallenges,we Localizedtextguidance. Weapplydistincttextprompts
proposeanalternativeapproach: identifyingandretaining toregionsbasedontheirnoisere-initializationstatus,using
correctlygeneratedobjectswhileregeneratingmisaligned 1âˆ’Mforre-initializedareasandMforpreservedregions.
objectsintheremainingareas. For the re-initialized regions, we guide generation in the
5latent space using regenerated prompts pr (See Sec. 3.2) tionâ€™promptsamongtheirattributes,eachcontainingspecific
tailoredtothoseareas. Meanwhile,motivatedbythepres- descriptions: objectcounts(e.g.,â€˜2Dogandawhale,ocean
ence of noise bias [1, 37, 45] (or referred to as trigger adventureâ€™), object colors (e.g., â€˜A green umbrella with a
patches[30])tiedtospecifictextpromptsindiffusionmod- yellowbirdperchedontopâ€™),andactions(e.g.,â€˜Hawaiian
els, we reuse the initial prompt p to ensure that features womanplayingaukuleleâ€™). Generalpromptswithoutspe-
associatedwithOâˆ—arepreservedintheregionsdesignated cificattributeswerealsoincludedintheâ€˜othersâ€™section(e.g.,
forretention(Fig.4,step4). Ourproposedregionalizedde- â€˜goldfishinglassâ€™).
compositionoftheinitialpromptallowsforthecreationof ForevaluationmetricsinEvalCrafter,wemainlyadopt
neworadditionalobjectsintheunmaskedareaswhilemain- theoverallText-VideoAlignmentandvideoqualityscores.
tainingthedesignatedcontentwithinthemaskedregions. In Here,theText-VideoAlignmentscoreisdefinedasanav-
theend,VIDEOREPAIRenablestheexistingvideodiffusion erageofCLIP-Score[38],SD-Score[36],BLIP-BLEU[16,
modeltogenerateenhancedvideocontentthatisbothcon- 33], Detection-Score, Count-Score, and Color-Score [5].
sistentandmorecloselyalignedwiththeinputprompt. By Thevideoqualityscoreindicatestheaveragescoreofthe
leveragingadvancedlocalpromptsandmaskconfigurations, Video Quality Assessment score [50] and the Inception
VIDEOREPAIReffectivelyresolvesdiscrepanciesbetween Score[40]. Afulltablewithmetric-wiseresultsisprovided
the query and the generated video, all without requiring intheappendix.
additionaltraining. ForT2V-CompBench,weevaluategenerationcapabili-
tiesfromthecomposition-centricsceneprompts,including
Video ranking. To further ensure the quality of refined spatial relationships, generative numeracy, and consistent
videos,weimplementasimplevideorankingstrategy. Sim- attribute binding, where each of these categories includes
ilar to generating multiple candidate prompts in [29], we 100prompts. WeevaluateeachcategoryusingImageGrid-
produceK refinedvideosusingdifferentrandomseedsand LLaVA [23] for consistent attribute binding and Ground-
select the best one based on their DSGobj scores, as ob- ingDINO[24]fortheothertwodimensions. Seeappendix
tainedinSec.3.1,thusavoidingadditionalcomputationsor formoredetails.
resource burdens. If multiple videos receive tied DSGobj
(sinceDSGobj providesdiscretescoresbasedonthenumber Implementation Details. We implement VIDEORE-
of questions), we select the video with the highest BLIP- PAIR on two recent T2V models (T2V-turbo [18] and
BLEUscore[25]amongthem. VideoCrafter2[4]). Inthevideoevaluationstage(Sec.3.1),
wefollowtheDSG[6]framework,whereweemployGPT-4-
Iterative refinement. Users can iteratively perform the 0125[32]togenerateevaluationquestionsandGPT-4o[31]
four-stepVIDEOREPAIRcycleforfurtherimprovementin toprovidecorrespondinganswers. Intheregiondecomposi-
text-videoalignment. Bydefault,weusesingleiterationas tionstage(Sec.3.3),weemployMolmoE-1BandSemantic-
wefindsingleiterationscouldalreadyprovidemeaningful SAM(L).WeuseK = 5and1iterationforexperiments.
improvement(seeFig.7). OncetheDSGscorereaches1.0(themaximumscore),we
do â€˜early-stopâ€™ and skip further refinements. We use two
4.Experiments NVIDIARTXA6000GPUs(40GB)forexperiments. See
appendixformoredetails.
WecompareVIDEOREPAIRandrecentrefinementmethods
on different text-to-video generation benchmarks. Below
Baselines. WecompareVIDEOREPAIRwithrecentrefine-
we provide the experiment setups (Sec. 4.1), quantitative
ment methods, OPT2I [29]) and SLD [52], on the same
evaluation with baselines (Sec. 4.2), qualitative examples
T2Vmodels(T2V-turbo[18]andVideoCrafter2[4]). We
(Sec.4.3),andadditionalanalysisofVIDEOREPAIRcompo-
unified random seeds in all experiments, ensuring that all
nents(Sec.4.4).
methodsrefinethesameinitialvideo. NotethatbothOPT2I
4.1.ExperimentSetups andSLDareoriginallyintroducedfortext-to-imagerefine-
ment. Following ManËœasetal.[29],wealsoincludeâ€˜LLM
Benchmarksandevaluationmetrics. Weadopttwotext-
paraphrasingâ€™ as a baseline - using GPT-4 to generate di-
to-videogenerationbenchmarks: EvalCrafter[25]andT2V-
verseparaphrasesoftheinitialprompt. ForOPT2I,wescore
CompBench[44],whichextensivelyevaluatetext-to-video
the videos by the original DSG [6] (unmodified version)
alignmentwithvarioustypesofprompts. ForEvalCrafter,
on the first frame, using GPT-3.5 for question generation
wesplitpromptsbyattributesaccordingtotheirofficialmeta-
andGPT-4oforVQA.WesetOPT2Itoiterativelygenerate
data.1 Inourexperiments,weuseâ€˜countâ€™,â€˜colorâ€™,andâ€˜ac-
fivepromptcandidatesandperform10and5iterationsteps
1https://github.com/evalcrafter/EvalCrafter/ for T2V-turbo and VideoCrafter2, considering the slower
blob/master/metadata.json inferencetimeofVideoCrafter2. ForSLD,sinceitsrefine-
6Table1.EvaluationresultsonEvalCrafter[25]. Wereporttheresultswithfourpromptsplits,includingCount,Color,Action,andOthers.
Averagerepresentstheaveragescoreofallsplits.Theotherssectioncontainsgeneralpromptswithoutspecificattributes.Thebestnumbers
withineachblockarebolded.
Text-VideoAlignment
Method VisualQuality
Count Color Action Others Average
VideoCrafter2 47.52 46.28 44.07 46.20 46.02 61.89
+LLMparaphrasing 45.87 47.81 44.41 45.16 45.81 62.53
+SLD[52] 44.47 46.45 39.89 44.06 43.72 52.53
+OPT2I[29] 47.69 47.67 45.04 44.65 46.26 62.13
+VIDEOREPAIR(Ours) 48.67 48.80 45.47 46.35 47.32 62.15
T2V-turbo 46.14 45.05 41.42 43.16 43.94 63.54
+LLMparaphrasing 49.49 43.16 41.32 44.75 44.68 62.98
+SLD[52] 47.39 43.99 42.13 43.28 44.20 56.67
+OPT2I[29] 47.44 45.00 44.64 45.54 45.66 63.35
+VIDEOREPAIR(Ours) 52.51 51.17 45.79 46.11 48.89 63.28
Blue apple bouncing near a pink tree Fourchildrenand threedogshavinga picnicina park A child building a sandcastleon the left of a beach umbrella
o
b
ru
V-t
2
T
I2)01
TP Oret=
(I
D
LSr)1
et=
(I
ria
p
eR
o
e
dr)1
et=
(I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure5.VideosgeneratedwithT2V-turboandrefinementframeworks(OPT2I/SLD/VIDEOREPAIR)onT2V-turbo.VIDEOREPAIR
successfullyaddressesobjectandattributemisalignmentissues(e.g.,numeracy,spatialrelationship,attributeblending)comparedtoT2V-
turboandotherrefinementmethods.MorevisualizationexampleswithT2V-turboandVideoCrafter2areprovidedintheappendix.
mentmodelisbasedonLMD+[20],weapplySLDtoeach andcountcategorieswhenappliedtoVideoCrafter2. Weex-
frameofinitialvideosfromT2Vmodels. Seeappendixfor pectthatthisisbecauseSLDisdesignedtomergedenoised
moredetails. Inaddition,wealsocompareVIDEOREPAIR latentfeaturesofeachobjectforeachframe,resultinginchal-
withstate-of-the-art(SoTA)T2Vmodels: ModelScope[47], lengesinmaintainingconsistentobjectcountsandlocations
ZeroScope[13],Latte[28],Show-1[55],Open-Sora-Plan forvideogenerationtasks. OPT2Iimprovesthealignment
v1.1.0[35],VideoTetris[46],andVico[53]. onactioncategorybyextensivelysearchingforoptimized
promptsthroughsequentialprocessesofpromptcandidate
4.2.QuantitativeResults generation, rankingviaDSG,andselection. However, its
overallimprovementsarelimitedasitcannotprovidefine-
EvalCrafter: VIDEOREPAIRimprovesT2Valignments, grainedlocalizedguidancebeyondthetextspace.
outperforming existing refinement methods. Tab. 1
showstheevaluationresultsonEvalCrafter,measuredwith Alternatively, VIDEOREPAIRsurpassesallbaselinesin
text-video alignment and visual quality. We observe that the text-video alignment metric (evaluated using CLIP,
SLDandOPT2Ionlyimprovethebaselinediffusionback- BLIP2, andSAM-Track)acrossallfoursplitsbyasignif-
bonesminimallyorevenhurttheperformance. Specifically, icantmargin,achievingrelativeimprovementsof+2.87%
SLDSLDsignificantlydeterioratesthealignmentofaction and+11.09%overVideoCrafter2andT2V-turbooninitial
7Table2.EvaluationresultsonT2V-Compbench[44].Wereport Table 3. Ablations of different VIDEOREPAIR components,
theresultswiththreepromptsplits(Consist-Attr/Spatial/Numeracy) evaluatedonthreesplits(Count/Color/Action)ofEvalCrafter.Our
Thebestnumberswithineachblockarebolded. defaultsetupishighlightedwithabluebackground.
Method Consist-Attr Spatial Numeracy Avg. Eval.Question ObjectSelection RankingMetric Text-Video
(OtherT2Vmodels) (Sec.3.1) (Sec.3.2/Sec.3.3) (Sec.3.4) Alignment
ModelScope[47] 0.5483 0.4220 0.2066 0.3923 DSG random DSGObj 46.73
ZeroScope[13] 0.4495 0.4073 0.2378 0.3648 DSGObj random DSGObj 47.11
Latte[28] 0.5325 0.4476 0.2187 0.3996 DSGObj GPT-4o DSGObj 49.82
Show-1[55] 0.6388 0.4649 0.1644 0.4227 DSGObj GPT-4o CLIPScore 46.30
Open-Sora-Plan[35] 0.7413 0.5587 0.2928 0.5309 DSGObj GPT-4o BLIP-BLEU 49.56
VideoTetris[46] 0.7125 0.5148 0.2609 0.4961
VideoCrafter2 0.6868 0.5174 0.2962 0.5001
+Vico[53] 0.6470 0.5425 0.2762 0.4886
+VIDEOREPAIR(Ours) 0.7275 0.5440 0.3603 0.5439 formanceinthespatialrelationshipdimension,itstruggles
T2V-turbo 0.7162 0.5585 0.2821 0.5189 withconsistentattributebinding,whichrequiresmanagingat
+LLMparaphrasing 0.6582 0.5380 0.2512 0.4825 leasttwodynamicobjectswithdistinctattributes,resulting
+SLD[52] 0.6850 0.6163 0.2455 0.5156
ina-0.64%decreaseinaverageperformancefromtheorig-
+OPT2I[29] 0.7452 0.5972 0.2909 0.5444
+VIDEOREPAIR(Ours) 0.7475 0.6000 0.2931 0.5469 inalT2V-turbo. Vico[53]isbuiltupontheVideoCrafter2.
Itimprovesspatialrelationshipsofobjectsinvideos. How-
Refinement Iteration ever,itnotablydegradesotheraspectsofcompositionality
invideogeneration,suchasnumeracyandattributebinding.
4.3.QualitativeResults
Fig.5visualizesvideosgeneratedbytheoriginalT2V-turbo
A family of four set up a tent and build a campfire, andrefinementframeworks(OPT2I,SLD,andVIDEORE-
enjoying a night of camping under the stars
PAIR)onT2V-turbo. Theseexamplesclearlyillustratehow
effectively VIDEOREPAIR addresses object and attribute
misalignment issues (as discussed in Fig. 3) compared to
T2V-turbo and other refinement methods. In the leftmost
example, VIDEOREPAIR preciselygeneratesthespecified
Sevenlively puppies playing and tumbling together. colorattribute(blueapple),whileothermethodsincorrectly
producepinkapplesblendedwiththepinktree.Inthemiddle
Figure6. TheiterativerefinementofVIDEOREPAIR. Videos
in each column represent the outputs of successive refinement
example, VIDEOREPAIR improvesonT2V-turbobyaccu-
rately generating three distinct three dogs, whereas other
iterations,wheretheoutputfromthepreviousstepservesasthe
inputforthecurrentstep. Thetextatthebottomofeachvideo baselineseitherfailtodo(OPT2I)soorintroduceartificial
rowindicatesthecorrespondingtextprompt. Morevisualization distortions(SLD).Intherightmostexample,VIDEOREPAIR
examplesareprovidedintheappendix. successfullycapturesspatialrelationshipsamongdifferent
objects(sandcastleontheleftofabeachumbrella)without
compromisingmulti-objectgeneration.
video generations. Note that VIDEOREPAIR performs a
Inaddition,wevalidatethepotentialofVIDEOREPAIR
refinementprocessthatpreservesthevisualqualityofthe
foriterativerefinement. Whileasinglerefinementstepof
backbone models. This shows the strong effectiveness of
VIDEOREPAIRmaynotfullyachieveprecisealignmentwith
VIDEOREPAIRinimprovingT2Valignmentwithoutsacri-
the initial prompt, we explore an iterative refinement pro-
ficingvisualquality.
cess to progressively enhance alignment and address any
residual discrepancies. As shown in Fig. 6, the initial re-
T2V-Compbench: VIDEOREPAIRimprovesT2Valign- finementprocesspartiallyresolvesmisalignmentsbetween
ments,alsooutperformingstrongT2Vbaselines. Tab.2 the video and the prompt (generating a scene depicting a
shows the evaluation results on T2V-Compbenchâ€™s three nightofcampingunderthestars)butmissesthefamily. but
dimensions: consistent attribute binding, spatial relation- omits the presence of the family. Through subsequent re-
ship, and generative numeracy. We observe that VIDE- finement iterations, VIDEOREPAIR successfully achieves
OREPAIR improves initial videos from both T2V models precisealignmentwiththetextprompt. Similarly,theexam-
(VideoCrafter2 and T2V-turbo) in all three splits, achiev- pleatthebottomofFig.6demonstratesthegenerationof
ingrelativeimprovementsof+8.76%and+5.40%,respec- sevencutepuppiesafteriterativerefinements.
tively. WhileSLDwithT2V-turbodemonstratesstrongper- Please also see the appendix for additional qualitative
8Table 4. Ablations of # Videos candidates (K) evaluated on ğ·ğ‘†ğºğ‘œğ‘ğ‘— score per iteration Text-video alignment score per iteration
EvalCrafterCount,Color,andActionpromptsusingT2V-turbo.
Avg.representstheaverageperformanceofthesethreecategories.
Ourdefaultsetupishighlightedwithabluebackground.
Text-VideoAlignment
#Videocandidates(K)
Count Color Action Avg. Initial video 1 Itera2 tion 3 4 Initial video 1 Itera2 tion 3 4
1 46.88 46.36 44.79 46.01
Figure7. Impactofiterativerefinement. Iterativerefinement
5 52.51 51.17 45.79 49.82
graduallyimprovesDSGObjandtext-videoalignmentscoreonall
threepromptcategories(count/color/action)ofEvalCrafter. The
â€˜initial videoâ€™ refers to a video from T2V-turbo. We use video
exampleswherewecompareVIDEOREPAIRwithbaselines, rankingwithK=5candidates.
iterativerefinements,andstep-by-stepillustrationsofVIDE-
OREPAIR.
5.Conclusion
4.4.AdditionalAnalysis
We introduce VIDEOREPAIR, a novel model-agnostic,
VIDEOREPAIRcomponents. Wecomparedifferentcom- training-freevideorefinementframeworkthatautomatically
ponents of VIDEOREPAIR using T2V-turbo backbone, on identifiesfine-grainedtext-videomisalignmentsandgener-
threesplits(Count+Color+Action)ofEvalCrafter. Foreval- ates explicit spatial and textual feedback, enabling a T2V
uationquestions(Sec.3.1),wecomparetheoriginalDSG diffusionmodeltoperformtargeted,localizedrefinements.
question and our modified DSGObj. For the selection of VIDEOREPAIR consists of four stages: In (1) video eval-
themostprominentobjectOâˆ—(Sec.3.2/Sec.3.3),wecom- uation, wedetectmisalignmentsbygeneratingevaluation
pare selecting via GPT-4o to randomly selecting from all questionsandansweringthequestionswithMLLM.In(2)
objectsfromtheDSGObjsemantictuples. Forscoringmeth- refinementplanning, weidentifyaccuratelygeneratedob-
odsforvideoranking(Sec.3.4),wecompareDSGObjwith jectsandcreatealocalpromptforrefinement. In(3)region
CLIPScore[9]andBLIP-BLEU[16,34],whicharepartsof decomposition,wesegmentareasinavideotopreserveand
metricsusedinEvalCrafter. Tab.3showsthatthecombina- refineusingacombinedgroundingmodule. Finally,in(4)
tionofDSGObj forevaluationquestion,GPT-4oforobject localizedrefinement,weregeneratethevideobyadjusting
selection,andDSGObj forvideorankingachievesthebest themisalignedregionswhilepreservingthecorrectregions.
performanceoverall. Weusethesecomponentsforthede- Ontwopopularvideogenerationbenchmarks(EvalCrafter
faultsettingofVIDEOREPAIR. andT2V-CompBench),VIDEOREPAIRsubstantiallyoutper-
formsrecentbaselinesacrossvarioustext-videoalignment
metrics. WeprovidecomprehensiveanalysisonVIDEORE-
Ablationson#Videoscandidates. Wequantitativelyana- PAIRcomponents. Wehopethatourworkencouragesfuture
lyzetheimpactofthevideorankinginVIDEOREPAIRusing advancementsinautomaticrefinementframeworksinvisual
asubsetofEvalCrafter(Count,Color,andActioncategories). generationtasks.
AsshowninTab.4,VIDEOREPAIRalreadyobtainsuperior
performancewithoutvideoranking(i.e.,K =1),compared Acknowledgments
to strong baselines (LLM paraphrasing: 44.7, SLD: 44.5,
This work was supported by DARPA ECOLE Program
OPT2I:45.7inaverage),highlightingtheeffectivenessof
No. HR00112390060, NSF-AI Engage Institute DRL-
VIDEOREPAIR refinement process. Furthermore, we en-
2112635, DARPA Machine Commonsense (MCS) Grant
hancetext-videoalignmentbyincorporatingvideoranking
N66001-19-2-4031,AROAwardW911NF2110220,ONR
basedonDSGObj.
GrantN00014-23-1-2356, AccelerateFoundationModels
Researchprogram,andaBloombergDataSciencePhDFel-
lowship. Theviewscontainedinthisarticlearethoseofthe
ImpactofIterativeRefinement. Weexperimentwithit-
authorsandnotofthefundingagency.
erativelyperformingVIDEOREPAIRtofurtherimprovethe
text-video alignments. We monitor the DSGObj score and
References
terminatetheiterativerefinementwhentheDSGObjreaches
1.0(maxscore),andusevideorankingwithK=5candidates.
[1] YuanhaoBan,RuochenWang,TianyiZhou,BoqingGong,
AsillustratedinFig.7,iterativerefinementbenefitsallthree Cho-JuiHsieh,andMinhaoCheng. Thecrystalballhypoth-
promptsplits(count/color/action)ofEvalCrafter. Addi- esisindiffusionmodels:Anticipatingobjectpositionsfrom
tionaliterativerefinementexamplesareprovidedinFig.17. initialnoise. arXivpreprintarXiv:2406.01970,2024. 2,6
9[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. [15] FengLi,HaoZhang,PeizeSun,XueyanZou,ShilongLiu,
Multidiffusion:Fusingdiffusionpathsforcontrolledimage JianweiYang,ChunyuanLi,LeiZhang,andJianfengGao.
generation. InProceedingsoftheInternationalConference Semantic-sam:Segmentandrecognizeanythingatanygranu-
onMachineLearning(ICML),2023. 2,5 larity. arXivpreprintarXiv:2307.04767,2023. 2,5
[3] AndreasBlattmann, TimDockhorn, SumithKulal, Daniel [16] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-
Mendelevitch, MaciejKilian, DominikLorenz, YamLevi, 2: Bootstrappinglanguage-imagepre-trainingwithfrozen
Zion English, Vikram Voleti, Adam Letts, et al. Stable imageencodersandlargelanguagemodels. InInternational
video diffusion: Scaling latent video diffusion models to conferenceonmachinelearning,pages19730â€“19742.PMLR,
largedatasets. arXivpreprintarXiv:2311.15127,2023. 2,3 2023. 6,9
[4] HaoxinChen,YongZhang,XiaodongCun,MenghanXia, [17] JialuLi,JaeminCho,Yi-LinSung,JaehongYoon,andMohit
XintaoWang,ChaoWeng,andYingShan. Videocrafter2: Bansal. Selma: Learning and merging skill-specific text-
Overcomingdatalimitationsforhigh-qualityvideodiffusion to-imageexpertswithauto-generateddata. InAdvancesin
models. arXivpreprintarXiv:2401.09047,2024. 3,6 NeuralInformationProcessingSystems(NeurIPS),2024. 3
[5] YangmingCheng,LiuleiLi,YuanyouXu,XiaodiLi,Zongxin [18] JiachenLi,WeixiFeng,Tsu-JuiFu,XinyiWang,SugatoBasu,
Yang, Wenguan Wang, and Yi Yang. Segment and track WenhuChen,andWilliamYangWang. T2v-turbo: Break-
anything. arXivpreprintarXiv:2305.06558,2023. 6 ingthequalitybottleneckofvideoconsistencymodelwith
[6] JaeminCho,YushiHu,RoopalGarg,PeterAnderson,Ranjay mixedrewardfeedback. InAdvancesinNeuralInformation
Krishna, JasonBaldridge, MohitBansal, JordiPont-Tuset, ProcessingSystems(NeurIPS),2024. 2,3,6,14
andSuWang. Davidsonianscenegraph: Improvingrelia-
[19] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,Jian-
bility in fine-grained evaluation for text-image generation.
weiYang, JianfengGao, ChunyuanLi, andYongJaeLee.
InProceedingsoftheInternationalConferenceonLearning
Gligen:Open-setgroundedtext-to-imagegeneration. InPro-
Representations(ICLR),2024. 2,3,6,13
ceedingsoftheIEEEInternationalConferenceonComputer
[7] MattDeitke,ChristopherClark,SanghoLee,RohunTripathi,
VisionandPatternRecognition(CVPR),2023. 2,3,5,13
Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas
[20] LongLian,BoyiLi,AdamYala,andTrevorDarrell. Llm-
Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and
groundeddiffusion:Enhancingpromptunderstandingoftext-
pixmo:Openweightsandopendataforstate-of-the-artmul-
to-imagediffusionmodelswithlargelanguagemodels. arXiv
timodalmodels. arXivpreprintarXiv:2409.17146,2024. 2,
preprintarXiv:2305.13655,2023. 7
5
[21] LongLian,BaifengShi,AdamYala,TrevorDarrell,andBoyi
[8] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Li. Llm-groundedvideodiffusionmodels. InProceedings
Jonathan Granskog, and Anastasis Germanidis. Structure
oftheInternationalConferenceonLearningRepresentations
andcontent-guidedvideosynthesiswithdiffusionmodels. In
(ICLR),2024. 3
ProceedingsoftheInternationalConferenceonComputer
[22] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.
Vision(ICCV),2023. 2,3
Videodirectorgpt: Consistentmulti-scenevideogeneration
[9] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,
viallm-guidedplanning. arXivpreprintarXiv:2309.15091,
and Yejin Choi. CLIPScore: A reference-free evaluation
2023. 3
metricforimagecaptioning. InProceedingsofthe2021Con-
[23] HaotianLiu, ChunyuanLi, YuhengLi, andYongJaeLee.
ferenceonEmpiricalMethodsinNaturalLanguageProcess-
Improvedbaselineswithvisualinstructiontuning. InPro-
ing,pages7514â€“7528,OnlineandPuntaCana,Dominican
ceedingsoftheIEEE/CVFConferenceonComputerVision
Republic,2021.AssociationforComputationalLinguistics.
andPatternRecognition,pages26296â€“26306,2024. 6
9
[10] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- [24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
sionprobabilisticmodels. Advancesinneuralinformation Zhang, JieYang, QingJiang, ChunyuanLi, JianweiYang,
processingsystems,33:6840â€“6851,2020. 3 HangSu,etal.Groundingdino:Marryingdinowithgrounded
[11] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan, pre-training for open-set object detection. arXiv preprint
MohammadNorouzi,andDavidJFleet.Videodiffusionmod- arXiv:2303.05499,2023. 6
els. InAdvancesinNeuralInformationProcessingSystems [25] YaofangLiu,XiaodongCun,XueboLiu,XintaoWang,Yong
(NeurIPS),2022. 2,3 Zhang, HaoxinChen, YangLiu, TieyongZeng, Raymond
[12] WenyiHong,MingDing,WendiZheng,XinghanLiu,andJie Chan,andYingShan. Evalcrafter:Benchmarkingandeval-
Tang.Cogvideo:Large-scalepretrainingfortext-to-videogen- uating large video generation models. In Proceedings of
erationviatransformers. arXivpreprintarXiv:2205.15868, theIEEEInternationalConferenceonComputerVisionand
2022. 3 PatternRecognition(CVPR),2024. 3,6,7,14
[13] huggingface. Zeroscope,2023. 7,8 [26] ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,
[14] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- LiangWang,YujunShen,DeliZhao,JingrenZhou,andTie-
vosyan, Roberto Henschel, Zhangyang Wang, Shant niuTan.Videofusion:Decomposeddiffusionmodelsforhigh-
Navasardyan, and Humphrey Shi. Text2video-zero: Text- qualityvideogeneration. arXivpreprintarXiv:2303.08320,
to-imagediffusionmodelsarezero-shotvideogenerators. In 2023. 3
ProceedingsoftheInternationalConferenceonComputer [27] Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang,
Vision(ICCV),2023. 2,3 JianzhuangLiu,YifanLiu,YafeiWen,XiaoxinChen,and
10ShifengChen. Gpt4motion: Scriptingphysicalmotionsin [42] YangSong,PrafullaDhariwal,MarkChen,andIlyaSutskever.
text-to-videogenerationviablender-orientedgptplanning. In Consistencymodels. arXivpreprintarXiv:2303.01469,2023.
ProceedingsoftheIEEEInternationalConferenceonCom- 3
puterVisionandPatternRecognition(CVPR),2024. 3 [43] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin,
[28] XinMa,YaohuiWang,GengyunJia,XinyuanChen,Ziwei Da-ChengJuan,DanaAlon,CharlesHerrmann,Sjoerdvan
Liu,Yuan-FangLi,CunjianChen,andYuQiao.Latte:Latent Steenkiste,RanjayKrishna,etal. Dreamsync:Aligningtext-
diffusiontransformerforvideogeneration. arXivpreprint to-imagegenerationwithimageunderstandingfeedback. In
arXiv:2401.03048,2024. 7,8 SyntheticDataforComputerVisionWorkshop@CVPR2024,
[29] Oscar ManËœas, Pietro Astolfi, Melissa Hall, Candace Ross, 2023. 3
JackUrbanek,AdinaWilliams,AishwaryaAgrawal,Adriana [44] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu,
Romero-Soriano,andMichalDrozdzal. Improvingtext-to- ZhenguoLi,andXihuiLiu. T2v-compbench:Acomprehen-
imageconsistencyviaautomaticpromptoptimization. arXiv sivebenchmarkforcompositionaltext-to-videogeneration.
preprintarXiv:2403.17804,2024. 2,3,6,7,8,13,14,15,22 arXivpreprintarXiv:2407.14505,2024. 3,6,8,14
[30] JiafengMao,XuetingWang,andKiyoharuAizawa. Guided [45] WenqiangSun,TengLi,ZehongLin,andJunZhang. Spatial-
imagesynthesisviainitialimageeditingindiffusionmodel. awarelatentinitializationforcontrollableimagegeneration.
InProceedingsofthe31stACMInternationalConferenceon arXivpreprintarXiv:2401.16157,2024. 2,6
Multimedia,pages5321â€“5329,2023. 2,6
[46] YeTian,LingYang,HaotianYang,YuanGao,YufanDeng,
[31] OpenAI. Hellogpt-4o,2024. 2,6
JingminChen,XintaoWang,ZhaochenYu,XinTao,Pengfei
[32] OpenAI. GPT-4technicalreport,2024. 6
Wan,etal. Videotetris:Towardscompositionaltext-to-video
[33] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
generation. InAdvancesinNeuralInformationProcessing
Zhu. Bleu: amethodforautomaticevaluationofmachine
Systems(NeurIPS),2024. 7,8
translation. InProceedingsofthe40thannualmeetingofthe
[47] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
AssociationforComputationalLinguistics,pages311â€“318,
XiangWang,andShiweiZhang. Modelscopetext-to-video
2002. 6
technicalreport. arXivpreprintarXiv:2308.06571,2023. 2,
[34] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
3,7,8
Zhu. Bleu: amethodforautomaticevaluationofmachine
[48] Xiang Wang, Shiwei Zhang, Han Zhang, Yu Liu, Yingya
translation. InProceedingsofthe40thannualmeetingofthe
Zhang,ChangxinGao,andNongSang. Videolcm: Video
AssociationforComputationalLinguistics,pages311â€“318,
latentconsistencymodel,2023. 3
2002. 9
[49] XiangWang,ShiweiZhang,HangjieYuan,ZhiwuQing,Biao
[35] PKU-YuanLabandTuzhanAIetc. Open-sora-plan,2023. 7,
Gong,YingyaZhang,YujunShen,ChangxinGao,andNong
8
Sang. Arecipeforscalinguptext-to-videogenerationwith
[36] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,
text-freevideos.InProceedingsoftheIEEE/CVFConference
TimDockhorn,JonasMuÂ¨ller,JoePenna,andRobinRombach.
onComputerVisionandPatternRecognition,pages6572â€“
Sdxl:Improvinglatentdiffusionmodelsforhigh-resolution
6582,2024. 3
imagesynthesis. arXivpreprintarXiv:2307.01952,2023. 6
[50] HaoningWu,ErliZhang,LiangLiao,ChaofengChen,Jing-
[37] ZipengQi,LichenBai,HaoyiXiong,etal. Notallnoisesare
wenHou,AnnanWang,WenxiuSun,QiongYan,andWeisi
createdequally:Diffusionnoiseselectionandoptimization.
Lin. Exploringvideoqualityassessmentonusergenerated
arXivpreprintarXiv:2407.14041,2024. 2,6
contentsfromaestheticandtechnicalperspectives. InPro-
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
ceedingsoftheIEEE/CVFInternationalConferenceonCom-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
puterVision,pages20144â€“20154,2023. 6,14
AmandaAskell,PamelaMishkin,JackClark,etal. Learning
transferable visual models from natural language supervi- [51] JayZhangjieWu, YixiaoGe, XintaoWang, StanWeixian
sion. InInternationalconferenceonmachinelearning,pages Lei,YuchaoGu,YufeiShi,WynneHsu,YingShan,Xiaohu
8748â€“8763.PMLR,2021. 6 Qie,andMikeZhengShou. Tune-a-video:One-shottuning
ofimagediffusionmodelsfortext-to-videogeneration. In
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
ProceedingsoftheInternationalConferenceonComputer
Patrick Esser, and BjoÂ¨rn Ommer. High-resolution image
Vision(ICCV),2023. 3
synthesis with latent diffusion models. In Proceedings of
theIEEEInternationalConferenceonComputerVisionand [52] Tsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li,
PatternRecognition(CVPR),2022. 3 andTrevorDarrell. Self-correctingllm-controlleddiffusion
[40] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki models.InProceedingsoftheIEEEInternationalConference
Cheung,AlecRadford,andXiChen. Improvedtechniques onComputerVisionandPatternRecognition(CVPR),2024.
fortraininggans. Advancesinneuralinformationprocessing 2,3,5,6,7,8,13,14,15
systems,29,2016. 6,14 [53] XingyiYangandXinchaoWang. Compositionalvideogener-
[41] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn, ationasflowequalization. arXivpreprintarXiv:2407.06182,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, 2024. 7,8,15
OranGafni,etal. Make-a-video: Text-to-videogeneration [54] ZhuoyiYang,JiayanTeng,WendiZheng,MingDing,Shiyu
withouttext-videodata. arXivpreprintarXiv:2209.14792, Huang,JiazhengXu,YuanmingYang,WenyiHong,Xiao-
2022. 2,3 hanZhang,GuanyuFeng,etal. Cogvideox: Text-to-video
11diffusionmodelswithanexperttransformer. arXivpreprint
arXiv:2408.06072,2024. 2,3
[55] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui
Zhao,LingminRan,YuchaoGu,DifeiGao,andMikeZheng
Shou.Show-1:Marryingpixelandlatentdiffusionmodelsfor
text-to-videogeneration. InternationalJournalofComputer
Vision,pages1â€“15,2024. 7,8
[56] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
Zhang,WangmengZuo,andQiTian.Controlvideo:Training-
free controllable text-to-video generation. arXiv preprint
arXiv:2305.13077,2023. 3
12Appendix 1.0(indicatingaperfectscore),theVIDEOREPAIRprocess
isterminated.
A.VIDEOREPAIRImplementationDetails 13 A.3.KeyObjectExtraction
A.1.QuestionGeneration . . . . . . . . . . . . . 13
To extract the key concept Oâˆ— from initial videos V , we
A.2.VisualQuestionAnswering . . . . . . . . . 13 0
providethefirstframeofV andthelistofquestion-answer
A.3.KeyObjectExtraction . . . . . . . . . . . . 13 0
pairsforeachobjecttoGPT4oasshowninFig.21. Here,
A.4.RefinementPromptGeneration . . . . . . . 13
weprioritizeselectingobjectswithahighernumberof1.0
B.AdditionalBaselineDetails 13 scores. Moreover,weforceGPT4otoselectâ€˜objectâ€™instead
ofâ€˜backgroundâ€™elementstoimprovetheaccuracyofregion
C.AdditionalEvaluationDetails 14 decompositionbypointing.
D.AdditionalQuantitativeAnalysis 14 A.4.RefinementPromptGeneration
D.1.InferenceTime . . . . . . . . . . . . . . . . 14
Toproducearefinementpromptpr,weuseGPT4within-
D.2.Increasing#ofVideoCandidates . . . . . . 14
struction as shown in Fig. 22. After getting Oâˆ—, we can
decompose the whole question set Q as Qoâˆ— and others
E.AdditionalQualitativeExamples 15
depending on whether the Oâˆ— keyword is included in the
E.1.ComparisonwithBaselines . . . . . . . . . 15
question. Togeneratepr fromspecificquestionsets,weuti-
E.2.IterativeRefinement . . . . . . . . . . . . . 15
lizefivemanuallycraftedin-contextexamplestoensurethe
E.3.Step-by-stepIllustrationofVIDEOREPAIR . 15
accuracyofthegenerationprocess. IftheDSGscoreis0.0
A. VIDEOREPAIR ImplementationDetails (indicatingacompletefailurefromVQA)andthekeyobject
Oâˆ—cannotbeidentified,weconsidertheT2Vmodeltohave
A.1.QuestionGeneration failedingeneratinganyobjectcorrectly. Insuchcases,we
ForgeneratingDSGObj,wefollowDSG[6]mainlybutre- paraphraseQdirectlyintopr usingalargelanguagemodel
(LLM).
visein-contextexamples. GiventhelimitationofDSGas
describedinFig.8,wechangeallâ€˜entity-wholeâ€™tuplesof
B.AdditionalBaselineDetails
DSGtoâ€˜countâ€™attributetuplestocapturetheexactnumber
ofobjects.
LLMParaphrasing. Following[29],wecompareVIDE-
OREPAIRwithparaphrasingpromptsfromLLM.Here,we
Init Prompt: 1 bearand 2 peoplemakingpizza
askGPT4togeneratediverseparaphrasesofeachprompt,
obj withoutanycontextabouttheconsistencyoftheimagesgen-
DSG DSG
erated from it. The prompt used to obtain paraphrases is
Is there a bear? Is there one bear?
providedinFig.23.
Yes No
Figure8.ComparisonofDSGandDSGObj.ComparedtoDSGObj
OPT2I. SinceOPT2I[29]aimstoimprovetext-imagecon-
(ours), DSG does not penalize the video even if more than one
sistency for T2I models, we reimplement OPT2I for T2V
object(e.g.,1bearinthiscase)isgeneratedwhenthetargetobject
setup. Specifically,wereplacetheoriginalT2Imodelpart
count=1inthetextprompt.
withT2Vmodels(T2V-TurboandVideoCrafter2)togener-
ateoutputs. UsingGPT-4o,wethenposeDSGquestionsto
A.2.VisualQuestionAnswering theseoutputs. Forprompts,wedirectlyadopttheonespro-
videdintheoriginalOPT2Ipaper. ForLLM,weuseGPT4
Toevaluatethegeneratedvideos,weutilizeGPT-4otoan-
swerbothcount-related(Qo)andattribute-related(Qo)ques- asVIDEOREPAIR. Finally,weperformiterativerefinement,
c a
tions, asillustratedinFig.20. ForQo prompts, weguide running 10 iterations for T2V-Turbo and 5 iterations for
c
VideoCrafter2,withfivevideocandidatesperiteration.
GPT-4o through four steps: reasoning, answering, count-
ing the predicted number of objects (no), and verifying
p
the true count (no). These steps yield an answer triplet SLD. ToadaptSLD[52]totheT2Vsetup,weapplytheir
v
Ao = {bo,no,no}. Toensurevalidresponses,weaccount officialcodetoindividualvideoframesandmaintaintheir
c c p v
fordependenciesamongquestions,followingthemethod- default setup. Note that SLD is a GLIGEN [19]-based
ology of DSG [6]. Each question is posed to GPT-4o se- T2Imodel,whichposeschallengesfordirectextensionto
quentially,andaDSGscoreiscalculatedafterprocessingall videogeneration. SinceSLDoperatesusingDDIMinver-
VQAtasks. ThisDSGscoredetermineswhethertheVIDEO- sion,weusetheinitialvideosgeneratedbyT2V-Turboand
REPAIRprocessshouldcontinue. IftheDSGscorereaches VideoCrafter2 as inputs, enabling the implementation of
13theirnoisecompositionmethod. Here,weuseoneiteration Table5.Inferencetimeandtext-videoalignmentofVIDEORE-
forSLDandGPT4forLLM. PAIRandbaselines.MeasuredwithasingleNVIDIAA10080GB
GPU,onEvalCraftercountsplitwith50prompts.
C.AdditionalEvaluationDetails
Text-Video
Timepervideo(â†“) Totaltime(â†“) (â†‘)
EvalCrafter. To evaluate the effectiveness of VIDEO- Alignment
T2V-turbo[18] 3.55s 3m12s 46.14
REPAIR across different prompt dimensions, we decom-
OPT2I[29](k=5,iter=5) 185.86s 2h34m53s 47.44
poseEvalCrafter[25]usingtheofficialmetadata.json.
SLD[52](iter=1) 365.30s 5h4m25s 47.39
Specifically, we utilize the attributes key for each Ours(k=5,iter=1) 59.61s 49m40s 52.51
promptandcategorizethedatasetintoâ€˜countâ€™,â€˜colorâ€™,â€˜ac-
tionâ€™, â€˜textâ€™, â€˜faceâ€™, and â€˜amp (camera motion)â€™. Prompts
inferencetimecomparedtootherbaselines,whilealsohigh-
withoutexplicitattributesaregroupedintoanâ€˜othersâ€™cat-
lightingtheimpactofiterativerefinementandtheeffectof
egory. Among these dimensions, we focus on â€˜countâ€™,
varyingthenumberofvideocandidates.
â€˜colorâ€™,â€˜actionâ€™,andâ€˜othersâ€™,excludingâ€˜textâ€™,â€˜faceâ€™,and
â€˜ampâ€™. Thisdecisionisbasedonourobservationthatvideo D.1.InferenceTime
errors related to text prompts (e.g., â€œthe words â€˜KEEP
OFFTHEGRASSâ€),faceprompts(e.g.,â€œKanyeWesteat-
To validate the efficiency of VIDEOREPAIR, we compare
its inference time against other baselines. The evaluation
ing spaghettiâ€), and amp prompts (e.g., â€œA Vietnam map,
isconductedusingasingleNVIDIAA10080GBGPUon
largemotionâ€)cannotbereliablydetectedthroughGPT-4o
the â€˜countâ€™ section of EvalCrafter. We report the average
question-answering, therefore hard to proceed VIDEORE-
inference time per video, the total inference time for 50
PAIR.
videos, and the text-video alignment score. As shown in
For evaluation metrics, we mainly adopt the aver-
age text-video alignment score they proposed. Among
Table5,VIDEOREPAIRdemonstratesthehighestefficiency
amongallbaselineswhilealsoachievingsuperiortext-video
their all text-video alignment scores (CLIP-Score, SD-
alignment scores. Notably, even with just one iteration,
Score, BLIP-BLEU, Detection-Score, Count-Score,
Color-Score, Celebrity ID Score, and OCR-Score)
VIDEOREPAIRcanrefineasinglevideoinonly59seconds.
we exclude Celebrity ID Score and OCR-Score
since they are related to â€˜faceâ€™ and â€˜textâ€™ categories. Test-video alignment score per k
Therefore, we calculate text-video alignment score as
Avg(CLIP-Score,SD-Score,BLIP-BLEU,DetectionScore,
CountScore,ColorScore). For overall video quality, we
directlyadopttheirmetricsincludingInceptionScore[40]
andVideoQualityAssessment(VQA ,VQA )[50].
A T
T2V-Compbench. SinceVIDEOREPAIRhasstrengthin
compositionalgeneration,weadoptT2V-Compbench[44]
andevaluatethreedimensions: spatialrelationships,gener-
ative numeracy, and consistent attribute binding. â€˜Spatial # of video candidates (K)
relationshipsâ€™ requires the model to generate at least two
Figure9. Impactofthenumberofvideocandidates. Wevary
objectswhilemaintainingaccuratespatialrelationships(e.g.
thenumberofvideocandidatesKas1,5,10,and20forranking.
â€˜totheleftofâ€™,â€˜totherightofâ€™,â€˜aboveâ€™,â€˜belowâ€™,â€˜infront
ofâ€™)throughoutthedynamicvideo. â€˜Generativenumeracyâ€™
specifies one or two object types, with quantities ranging D.2.Increasing#ofVideoCandidates
from one to eight. â€˜Consistent attribute bindingâ€™ contains
Toevaluatetheimpactofvideoranking,wevarythenumber
color,shape,andtextureattributesamongtwoobjects.
ofvideocandidatesasK =1,5,10,and20duringtherank-
Following[44],weadoptVideoLLM-basedmetricsfor
ingprocess. Thevariationamongvideocandidatesarises
consistentattributebindinganddetection-basedmetricsfor
from different random seeds used to initialize Ïµâ€². For ex-
spatialRelationshipsandnumeracy. 0
ample,videorankingisnotappliedwhenK =1,andonly
onerefinementisproducedusingasinglerandomseednoise
D.AdditionalQuantitativeAnalysis
Ïµâ€². Forranking metrics, we relyonDSGObj acrossall ab-
0
Inthissection,wepresentadditionalquantitativeresultsto lation studies. As depicted in Fig. 9, higher K values (5,
provide a deeper understanding. Specifically, we demon- 10,and20)consistentlyyieldhigherscoresacrossallcate-
strate that VIDEOREPAIR achieves superior efficiency in goriesthanK =1. Thistrendisparticularlyprominentin
14theâ€˜countâ€™category,whereincreasingK leadstonoticeable
performanceimprovements,highlightingtheimportanceof
consideringmultiplecandidatesforranking.
E.AdditionalQualitativeExamples
E.1.ComparisonwithBaselines
Wepresentadditionalqualitativecomparisonswithbaseline
methods(OPT2I[29],SLD[52],andVico[53])inFigs.10
to 16. These examples address a variety of failure cases
commonlyobservedinT2Vmodels,includinginaccuracies
in object count and attribute depiction, as highlighted in
ourmainpaper. Figs.10to13correspondtoresultsfrom
T2V-Turbo,whileFigs.14to16showcaseexamplesfrom
VideoCrafter2. Additionally, we provide binary segmen-
tation masks that identify preserved areas (in black) and
updatedareas(inwhite).
Acrosstheseexamples, VIDEOREPAIR effectivelypre-
servestheOâˆ—areaswhilerefiningtheremainingregionsus-
ingpr. Forinstance,inFig.10,thecamelfromtheoriginal
T2V-Turbovideoispreserved,andasnowmanissuccess-
fullyadded. Incontrast,whileSLDalsoleveragesDDIM
inversiontopreserveobjects,itoftenfailstointegratenew
objectsseamlessly.
E.2.IterativeRefinement
We also demonstrate the results of iterative refinement in
Fig. 17, showing the initial video alongside the first and
second refinements generated from T2V-Turbo. Overall,
VIDEOREPAIRprogressivelyenhancestext-videoalignment
witheachrefinementstep.
For numeracy-related cases (e.g., six dancers and five
cows),VIDEOREPAIRiterativelyaddsorremovesspecific
objects,ensuringalignmentwiththegivenprompts. Incases
ofmissingobjects(e.g.,biologistsandducks), VIDEORE-
PAIRsuccessfullygeneratesadditionalbiologistsandmulti-
pleduckswhilepreservingthecontextoftheinitialvideo.
Additionally,forattribute-relatedprompts(e.g.,yellowum-
brellaandbluecup),VIDEOREPAIReffectivelyrefinesob-
jectattributes,suchasaddingawoodenhandletotheum-
brella and enhancing the cupâ€™s blue color. These results
demonstratetheabilityofVIDEOREPAIRtoiterativelyim-
provebothobjectcountandattributealignmentwithhigh
fidelity.
E.3.Step-by-stepIllustrationof VIDEOREPAIR
InFigs.18and19,weprovidedetailedillustrationsofall
fourVIDEOREPAIRsteps.
15Pink motorcycle weaving through orange traffic cones A camel lounging in front of a snowman
o
b
ru
V-t
2
T
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure10.QualitativeexamplesfromT2V-turbo.
1 bear and 2 people making pizza Teddy bear and 3 real bear
o
b
ru
V-t
2
T
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure11.QualitativeexamplesfromT2V-turbo.
16Yellow rose swaying near a green bench A blue car parked next to a red fire hydrant on the street.
o
b
ru
V-t
2
T
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure12.QualitativeexamplesfromT2V-turbo.
five aliens in a forest Five colorful parrots perch on a branch, squawking loudly at each other.
o
b
ru
V-t
2
T
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure13.QualitativeexamplesfromT2V-turbo.
17A silver cell phone lays next to a red fire hydrant. A penguin standing on the right side of a cactus in a desert
2
re
tfa
rC
o
e
d
iV
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure14.QualitativeexamplesfromVideoCrafter2.
A dog sitting under a umbrella on a sunny beach With the style of pointilism, A green apple and a black backpack.
2
re
tfa
rC
o
e
d
iV
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure15.QualitativeexamplesfromVideoCrafter2.
18three foxes in a snowy forest A basket placed below a television
2
re
tfa
rC
o
e
d
iV
I2)0
1
T P Ore=
t (I
D)1
=
L Sre
t (I
o
c
iV
ria
p
e R
or)1
e=
e dt (I
iV
Frame 1 Frame 8 Frame 16 Frame 1 Frame 8 Frame 16
Figure16.QualitativeexamplesfromVideoCrafter2.
Iteration1~3 Iteration1~3
A team of two marine biologists study a colony of penguins, A group of six dancers perform a ballet on stage,
monitoring their breeding habits. their movements synchronized and graceful.
A bright yellow umbrella with a wooden handle. A mother and her child feed ducks at a pond.
It's compact and easy to carry.
Five cows graze lazily in a green meadow on a perfect spring day. A blue cup and a green cell phone,with the style of pencil drawing
Figure17. VideosgeneratedusingiterativerefinementwithVIDEOREPAIR. Wedepictiterativerefinementresultsgeneratedfrom
T2V-Turbo.Overall,VIDEOREPAIRprogressivelyenhancestext-videoalignmentwitheachrefinementstep.
19Step1. Video Evaluation
{'Q': 'Is there one camel?', 'A': 1.0,
'reasoning': 'There is one visible camel in the image.', 'obj_in_prompt': 1, 'obj_in_img': 1}
{'Q': 'Is there one snowman?', 'A': 0.0, 'reasoning': 'There are no snowmen in the image.',
'obj_in_prompt': 1, 'obj_in_img': 0}
{'Q': 'Is the camel lounging?', 'A': 1.0}
{'Q': 'Is the camel in front of the snowman?', 'A': 0.0}
Step2. Refinement Planning
[Object decision] Preserved object : camel | Preserved num : 1
Regenerating prompt : One snowman.
Step3. Region Decomposition Step4. Localized Refinement
1 camel Top-ranked video
Figure18.OutputfromeachstepofVIDEOREPAIR.WeillustratewholeoutputsfromeachstepofVIDEOREPAIR.
Step1. Video Evaluation
{'Q': 'Are there four children?', 'A': 1.0, 'reasoning': 'There are four visible children in the
image.', 'obj_in_prompt': 4, 'obj_in_img': 4}
{'Q': 'Are there three dogs?', 'A': 0.0, 'reasoning': 'There is only one dog visible in the image.',
'obj_in_prompt': 3, 'obj_in_img': 1}
{'Q': 'Is there a picnic?', 'A': 1.0}
{'Q': 'Is there a park?', 'A': 1.0}
{'Q': 'Are the children having a picnic?', 'A': 1.0}
{'Q': 'Are the children in the picnic?', 'A': 1.0}
{'Q': 'Are the dogs in the picnic?', 'A': 0.0}
{'Q': 'Is the picnic in the park?', 'A': 1.0}
Step2. Refinement Planning
[Object decision] Preserved object : children | Preserved num : 4
Regenerating prompt : Three dogs at a picnic in the park.
Step3. Region Decomposition Step4. Localized Refinement
4 children Top-ranked video
Figure19.OutputfromeachstepofVIDEOREPAIR.WeillustratewholeoutputsfromeachstepofVIDEOREPAIR.
201. Given the question: "{cur_question}", provide a brief reasoning (up to two sentences) to determine the
accurate answer.
2. Respond to the question using binary values: 1.0 for "Yes" and 0.0 for "No". If the answer is uncertain
or unnatural due to image distortion or other issues, respond with 0.0 ("No").
3. Return the number of "{key_objects}" (as an integer) mentioned in the initial prompt "{cur_question}".
4. Return the number of "{key_objects}" (as an integer) in the provided image.
Return the result as a dictionary in the following format (not in JSON format):
{{"Q": "<question>",
"A": <binary answer>,
"reasoning": "<brief reasoning>",
"obj_in_prompt": <number of key object mentioned in the initial prompt>,
"obj_in_img": <number of key object in the image>}}
Example:
{{"Q": "Is there one robot?",
"A": 0.0,
"reasoning": "There are two visible robots in the image.",
"obj_in_prompt": 1,
"obj_in_img": 2}}
Please provide only the dictionary as the output without any additional text or explanation.
Respond to "{cur_question}" using binary values: 1.0 for Yes and 0.0 for No.
If the answer is uncertain due to image distortion or other issues, respond with 0.0 (No). \
Return the result as a dictionary in the following format (not in JSON format): \
{{"Q": "<question>", "A": <binary answer>}} \
(e.g., {{"Q": "Is there one robot?", "A": 0.0}}) \
Provide only the dictionary as the output, without any additional text or explanations.
Figure20.Promptstoperformvisualquestionansweringinvideoevaluationsteps.Top:ThepromptforQo(count-relatedquestion),
c
Bottom:promptforQo (attribute-relatedquestion).cur questionmeanseachDSGObjquestionandkey objectsmeansentityword
a
ineachquestion.
Given the generated image and the list of question-answer pairs for each object, represented as
{object_wise_dict}, choose the most accurately or visibly generated object from the list
{key_objects_from_Q}.
Prioritize selecting objects with a high number of answers rated 1.0 for each question.
Select the object that is both large and clearly visible, prioritizing prominent objects (such as
animals, humans, or specific items) over background elements (like ocean or city).
Return only the name of the best object to keep from the list, without additional explanation
(e.g., 'dog').
Figure21.Prompttochoosewhichobject(s)topreserve.WeaskGPT4otoselectobjectstopreserveinthescene.
21Given the following list of questions {question_list}, create a single descriptive sentence that combines
the meaning of each question into a natural, affirmative statement that provides a full, concise summary.
Examples:
- Example 1
Question list: ['Is there a bed?', 'Is the bed blue?', 'Are the pillows beige?', 'Are the pillows with the
bed?']
Answer: "Blue bed with beige pillows."
- Example 2
Question list: [Are there three real bears?]
Answer: "Three real bears."
- Example 3
Question list: [Are there two people?, Are the people making pizza?]
Answer: "Two people making pizza.
- Example 4
Question list: [Is there a family?, Is there one cat?, Is there a park?, Is the family taking a walk?, Is
the cat walking?, Is the family enjoying?, Is the family breathing fresh air?, Is the family exercising?]
Answer: "A family and a cat are walking in the park."
- Example 5
Question list: [Is there a green bench?, Is there an orange tree?, Is the bench green?, Is the tree
orange?]
Answer: "Green bench and orange tree."
Your Current Task: Your response should be a concise 1 phrase, without additional explanation
(e.g., "a small bear")
Figure22.Prompttoplanhowtorefinetheotherregions.Weusefivein-contextexamplestocreatetherefinementpromptfromthe
questionrelatedtootherobjects.
Generate 1 paraphrase of the following image description
while keeping the semantic meaning: "{init_prompt}".
Provide your response as a single phrase without any explanation.
Format it as: <PROMPT> ... </PROMPT>.
(e.g., <PROMPT>Two dogs and a whale embark on a sea adventure.</PROMPT>)
Figure23.PromptforLLMparaphrasing.FollowingOPT2I[29],weaskGPT4togeneratediverseparaphrasesofeachpromptforLLM
paraphrasingbaselineexperiments.
22