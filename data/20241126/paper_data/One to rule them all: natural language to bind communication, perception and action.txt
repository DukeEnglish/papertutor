One to rule them all: natural language to bind
communication, perception and action
SimoneColombani1,3, DimitriOgnibene2 and GiuseppeBoccignone1
2UniversityofMilan,Italy
1UniversityofMilano-Bicocca,Milan,Italy
3OversonicRobotics,CarateBrianza,Italy
Abstract
Inrecentyears,researchintheareaofhuman-robotinteractionhasfocusedondevelopingrobotscapableof
understandingcomplexhumaninstructionsandperformingtasksindynamicanddiverseenvironments.These
systemshaveawiderangeofapplications,frompersonalassistancetoindustrialrobotics,emphasizingthe
importanceofrobotsinteractingflexibly,naturallyandsafelywithhumans.
Thispaperpresentsanadvancedarchitectureforroboticactionplanningthatintegratescommunication,percep-
tion,andplanningwithLargeLanguageModels(LLMs).Oursystemisdesignedtotranslatecommandsexpressed
innaturallanguageintoexecutablerobotactions,incorporatingenvironmentalinformationanddynamically
updatingplansbasedonreal-timefeedback.
ThePlannerModuleisthecoreofthesystemwhereLLMsembeddedinamodifiedReActframeworkareemployed
tointerpretandcarryoutusercommandslikeâ€˜Gotothekitchenandpickupthebluebottleonthetableâ€™. By
leveragingtheirextensivepre-trainedknowledge,LLMscaneffectivelyprocessuserrequestswithouttheneed
tointroducenewknowledgeonthechangingenvironment.ThemodifiedReActframeworkfurtherenhances
theexecutionspacebyprovidingreal-timeenvironmentalperceptionandtheoutcomesofphysicalactions.By
combiningrobustanddynamicsemanticmaprepresentationsasgraphswithcontrolcomponentsandfailure
explanations,thisarchitectureenhancesarobotâ€™sadaptability,taskexecutionefficiency,andseamlesscollabora-
tionwithhumanusersinsharedanddynamicenvironments.Throughtheintegrationofcontinuousfeedback
loopswiththeenvironmentthesystemcandynamicallyadjuststheplantoaccommodateunexpectedchanges,
optimizingtherobotâ€™sabilitytoperformtasks. Usingadatasetofpreviousexperienceispossibletoprovide
detailedfeedbackaboutthefailure.UpdatingtheLLMscontextofthenextiterationwithsuggestiononhowto
overcametheissue.
ThissystemhasbeenimplementedonRoBee,thecognitivehumanoidrobotdevelopedbyOversonicRobotics,
showcasingitsadaptabilityandpotentialforintegrationacrossdiverseenvironments.ByleveragingLLMsand
semanticmapping,thearchitectureenablesRoBeetonavigateandrespondtoreal-timechanges.
Keywords
Human-Robotinteraction,Robottaskplanning,LargeLanguageModels,Automatedplanning
1. Introduction
TheintegrationofLLMsinroboticsystemshasopenednewavenuesforautonomoustaskplanning
and execution [2, 3]. These models demonstrate exceptional natural language understanding and
commonsensereasoningcapabilities,enhancingarobotâ€™sabilitytocomprehendcontextsandexecute
commands[4,5]. HoweverLLMsarenotbeabletoplanautonomously,theyneedtobeintegratedin
architecturesthatenablethemtounderstandtheenvironment,therobotcapabilitiesandstate[6]. This
researchaimstoempowerrobotstocomprehenduserrequestsandautonomouslygenerateactionable
plansindiverseenvironments.
Theefficacyoftheseplansreliesontherobotâ€™sunderstandingofitsoperatingenvironment[7]. To
bridgethisgap,ourworkemploysscenegraphs[8]asasemanticmappingtool,offeringastructured
representationofspatialandsemanticinformationwithinascene.
AI4CC-IPS-RCRA-SPIRIT2024:InternationalWorkshoponArtificialIntelligenceforClimateChange,ItalianWorkshoponPlanning
andScheduling,RCRAWorkshoponExperimentalevaluationofalgorithmsforsolvingproblemswithcombinatorialexplosion,
andSPIRITWorkshoponStrategies,Prediction,Interaction,andReasoninginItaly.November25-28th,2024,Bolzano,Italy[1].
$simone.colombani@studenti.unimi.it(S.Colombani);dimitri.ognibene@unimib.it(D.Ognibene);
giuseppe.boccignone@unimi.it(G.Boccignone)
Â©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
voN
22
]OR.sc[
1v33051.1142:viXraInourapproach,weleverageLLMsthroughin-context[9],whichenablesthemodelstolearnandadapt
based on the information provided in the context. Our work implements a modified version of the
ReAct[10]frameworkthatexpandthecontextofLLMswithenvironmentalinformationandexecution
feedback,allowingthemodeltoplanandexecuteskills[11]translatingthemintophysicalactions.
Motivation The primary focus of our work is to enable robot to interact flexibly and robustly in
dynamicanddiverseenvironmentswithlimitedhumanintervention.Traditionalroboticsystemsusually
relyonstatic,pre-programmedinstructionsorclosedworldpredefinedknowledgeandsettings,limiting
theiradaptabilitytodynamicenvironments. Interactingwithhumansindailytaskswithincomplex
environments disrupts these assumptions. LLMs and VLM can provide open-domain knowledge to
representnovelconditionswithouthumanintervention. However,thesemodelsarenotinformedof
thespecificrobot,taskandsettingsathand,thatdefinewhatinformationcanberelevantandnecessary
tofindandreasonabout[12]. Exceedinginthelevelofdetailmayleadtoimpracticalcomputational
requirements and response time. Discarding crucial information, spatial or semantic, may lead to
repeatedfailuresduetotheintroducednon-managedpartialobservability[13]. Tofindtherelevant
informationmaybetooslow[14]. LLMscanstillproduceoutputsthatarelogicallyinconsistentor
impractical[15],expeciallyiftheyarenotintegratedintosystemsthatallowthemtoadapttochanges
in the environment and the physical capabilities of the robots. Finally task execution, robots may
encounter unexpected situations, such as unanticipated obstacles, sensor errors, or changes in the
environmentthatwerenotaccountedforintheinitialplan. Suchscenariosnecessitaterobusterror
handlingmechanismsandadaptiveplanningstrategiesthatenablethesystemtoreassessandmodify
itsactionsin real-time[16]. Byintroducing executioncontrollingandfailuremanagementintothe
planning process at different levels as well as retrieval of previous successful plans, we propose a
solutiontoenhancetherobustnessandflexibilityofLLM-basedroboticsystems. Thisapproachensures
thattherobotcaneffectivelyperceivechangesintheenvironmentandthefailuresthatmayarisefrom
them,allowingittoadaptstrategiesinresponsetonewchallenges.
Proposedapproach Oursystemaddressesthechallengesofdynamicenvironmentsthroughareal-
timeperceptionmoduleandaPlannermodulethatintegratesexecutioncontrol,andfailuremanagement.
ItcompriseaControllerthatmonitorstheexecutionoftasksanddetectserrors,whiletheExplainer
analyzes failures and suggests adjustments based on past experiences. This feedback loop enables
adaptivere-planning,allowingthesystemtomodifyitsactionsasneeded. Specifically,weproposethe
useoftheReAct[10]framework,expandingitsoperationalspacewithskills,physicalactionsofthe
robotandwithperceptionaction,toaccessinformationfromtheenvironment. ByleveragingLLMsfor
naturallanguageunderstandingandaperceptionsystem,thearchitecturesupportsautonomoustask
executionindynamicscenarios.
2. Related works
AsubstantialbodyofliteratureexplorestheutilizationofLLMsforrobotictaskplanning[4,5].
LLMforrobotplanning RecentworkshighlightthepotentialofLargeLanguageModels(LLMs)
in robotic planning [17, 18, 19]. DEPS [20] introduces an iterative planning approach for agents in
open-worldenvironments,suchasMinecraft. ItutilizesLLMstoanalyzeerrorsduringexecutionand
refineplans,improvingbothreasoningandgoalselectionprocesses. However,thisapproachhasbeen
primarily developed and tested in virtual environments, with notable differences in comparison to
real-worldsettingsduetothedynamicandunpredictablenatureofphysicalenvironments. Additionally,
DEPSdoesnotleveragepreviousissuesandsolutionsbutreliessolelyonfeedbackfromhumansand
vision-languagemodels(VLMs).Figure1:Architectureofthesystem.
Scene graph as environemental representation The use of scene graphs [21] as a means to
represent the robotâ€™s environment has gained traction. [22] employs 3D scene graphs to represent
environmentsandusesLLMstogeneratePlanningDomainDefinitionLanguage(PDDL)files. This
methoddecomposeslong-termgoalsintonaturallanguageinstructionsandenhancescomputational
efficiencybyaddressingsub-goals. However,itlacksamechanismforreplanningbasedonfeedback
during execution, which could limit its adaptability in dynamic scenarios. SayPlan [23] integrates
semanticsearchwithscenegraphsandpathplanningtoaidrobotsinnavigatingcomplexenvironments
throughnaturallanguage. Bycombiningthesetechniques,SayPlansimulatesvariousscenariostorefine
tasksequences,whichhelpsimproveoveralltaskperformanceincomplexenvironments. However,itis
relianceonstaticpre-built3Dscenegraphs,hinderingadaptabilitytodynamicreal-worldenvironments.
Replanning Replanningenableslong-termautonomoustaskexecutioninrobotics[24]. DROC[25]
empowers robots to process natural language corrections and generalize that information to new
tasks. It introduces a mechanism to distinguish between high-level and low-level errors, allowing
moreflexibleplancorrections. However,DROCdoesnotaddressthetypesoffailuresthatmayoccur
during plan execution, focusing instead on high-level corrections provided by users. [26] supports
autonomouslong-termtaskexecutionbyintegratingLLMsforplanningandVLMsforfeedback. This
approachadaptstochangesintheenvironmentthroughastructuredcomponentsystemthatverifies
and corrects plans as needed. Yet, the feedback is limited to what is visible to the robotâ€™s camera,
potentiallyoverlookingothersignificantenvironmentalchanges.
3. Architecture
Oursystemisbasedontwocomponents:
â€¢ PerceptionModule: itisresponsibleforsensingandinterpretingtheenvironment. Itbuilds
andmantainsasemanticmapintheformofadirectedgraphthatintegratesbothgeometricand
semanticinformation.
â€¢ PlannerModule: ittakestheinformationprovidedbythePerceptionModuletoformulateplans
andactionsthatallowtherobottoperformspecifictasks.
Figure 1 show how these components interact to allow the robot to understand its environment
andactaccordinglytosatisfyuserrequests. ThePerceptionmoduleusesdataprovidedbytherobotâ€™s
sensors to supply the semantic map to the Planner module, which in turn processes it to generate
specificactionplans. InwhatfollowswepreciselyaddressthePlannerModulewhiledetailsontheFigure2:Architectureoftheplannermodule.
PerceptionModulewillbeprovidedinaseparatearticle.
3.1. Plannermodule
The architecture of the Planner module is designed to translate user requests, expressed in natural
language, into specific actions executableby a robot. Thismodule isresponsible for understanding
instructions,planningappropriateactions,andmanagingtheexecutionofthoseactionsinadynamic
environment. ThePlanningmoduleiscomposedbyfivesub-modules:
â€¢ Task Planner: Translates user requests, expressed in natural language, into a sequence of
high-levelskills.
â€¢ SkillPlanner: Translateshigh-levelskillsintospecific,low-levelexecutablecommands.
â€¢ Executor: Executesthelow-levelactionsgeneratedbytheSkillPlanner.
â€¢ Controller: Monitorstheexecutionofactionsandmanagesanyerrorsorunexpectedevents
duringtheprocess.
â€¢ Explainer: Interprets the causes of execution failures by analyzing data received from the
ControllerandprovidessuggestionstotheTaskPlanneronhowtoadjusttheplan.
ThearchitectureoftheplannermoduleisshowninFigure2. Themaincomponentofthesystemis
the Task Planner, which receives the userâ€™s request and translates it into a list of high-level "skills"
thatrepresenttherobotâ€™scapabilities. Theseskillsincludeactionssuchas"PICK"(graspanobject),
"PLACE"(placeanobject),and"GOTO"(movetoaposition).3.1.1. TaskPlanner
The decision-making process of the Task Planner is driven by a policy, which is implemented as a
LLM.Apolicyisastrategyorrulethatdefineshowactionsareselectedbasedonthecurrentstateor
context,[27].
TaskPlannerisimplementedusingtheReActframework[10], whichalternatesbetweenreasoning
and action phases during the process. In the reasoning phase, the Task Planner can access various
"perception"actionstogatherinformationfromtheenvironment,suchasthesemanticmapandthe
currentstateoftherobot,andcanexecuteoneormore"skill"actionstoperformphysicalactions.
The classical idea of ReAct is to augment the agentâ€™s action space to ğ´Ë† = ğ´ âˆª ğ¿, where ğ¿ is the
spaceoflanguage-basedreasoningactions. Anactionğ‘Ë† âˆˆ ğ¿,referredtoasa"thought" orreasoning
ğ‘¡
trace,doesnotdirectlyaffecttheexternalenvironmentbutinsteadupdatesthecurrentcontextğ‘ =
ğ‘¡+1
(ğ‘ ,ğ‘Ë† ) by adding useful information to support future decision-making [10]. In the classical idea
ğ‘¡ ğ‘¡
therecouldbevarioustypesofusefulthoughts,suchasdecomposingtaskgoalsandcreatingaction
plans, injecting commonsense knowledge relevant to task solving, extracting important parts from
observations,trackingprogressandtransitioningactionplans,handlingexceptionsandadjustingaction
plans,andsoon,butalwayswithoutmodifyingthephysicalenvironment,onlyembeddingitwithin
thecontext. Interestingly,thisapproachmixesreasoningandactioninaflexiblemanner. Inthefuture,
we will analyse the potential of this approach also connecting to the planning-to-plan [28, 29] and
meta-reasoning[30,31,32]concepts.
Inourwork,weaugmenttheagentâ€™s[33]actionspacewithtwotypesofactions:
â€¢ A skill action ğ‘
ğ‘¡
âˆˆ ğ´skill, which involves physically interacting with the environment, such
asmanipulatingobjectsornavigating. Theresultofaskillactionprovidesnewfeedbackthat
updatesthecurrentcontext.
â€¢ Aperceptionactionğ‘
ğ‘¡
âˆˆ ğ´perception,whichinvolvesaccessinginformationfromtheenvironment,
suchasqueryingthesemanticmaporsensors,andintegratingthatinformationintothecontext.
Theaugmentedactionspaceisdefinedas:
ğ´Ë† = ğ´skillâˆªğ´perceptionâˆªğ¿
Thus,theLLMservesasthepolicyğœ‹ thatselectsdifferenttypesofactionsfromtheaugmentedaction
spaceanddynamicallyadaptingthecurrentcontextğ‘ usedtoplanbasedonreal-timeinformationand
ğ‘¡
reasoning.
FormalDescription: TheTaskPlannerâ€™spolicyğœ‹,representedbytheLLM,canbeformalizedasa
functionthatmapsthecurrentcontextğ‘ toanactionğ‘Ë† fromtheaugmentedactionspaceğ´Ë†:
ğ‘¡ ğ‘¡
ğœ‹ : ğ¶ â†’ ğ´Ë†, ğœ‹(ğ‘ ) = ğ‘Ë†
ğ‘¡ ğ‘¡
Where:
â€¢ ğ¶ isthesetofallpossiblecontexts.
â€¢ ğ´Ë† istheaugmentedactionspaceğ´Ë† = ğ´skillâˆªğ´perceptionâˆªğ¿.
â€¢ ğ‘ representsthecurrentcontextattimeğ‘¡,whichincludesthestateoftherobot,theenvironment,
ğ‘¡
andanypastactionsorthoughts.
â€¢ ğ‘Ë†
ğ‘¡
âˆˆ ğ´Ë† istheactionchosenbythepolicy,whichcanbeaskillactionğ‘
ğ‘¡
âˆˆ ğ´skill,aperception
actionğ‘
ğ‘¡
âˆˆ ğ´perception,orareasoningtraceğ‘Ë†
ğ‘¡
âˆˆ ğ¿.
Thecontextğ‘ isupdatedbasedonthechosenaction:
ğ‘¡
â€¢ Ifğ‘Ë† âˆˆ ğ¿(areasoningaction),thecontextupdatesto:
ğ‘¡
ğ‘ = (ğ‘ ,ğ‘Ë† )
ğ‘¡+1 ğ‘¡ ğ‘¡
This represents the thought process, where reasoning contributes new information without
affectingtheexternalenvironment.â€¢ If ğ‘Ë†
ğ‘¡
âˆˆ ğ´perception (a perception action), the result of querying the environment updates the
context:
ğ‘
ğ‘¡+1
= (ğ‘ ğ‘¡,ğ‘“perception(ğ‘Ë† ğ‘¡))
Here,ğ‘“perception representsthefunctionthatgathersinformationandmodifiesthecontextbased
ontheperceptionactionâ€™soutcome.
â€¢ Ifğ‘Ë†
ğ‘¡
âˆˆ ğ´skill (askillaction),therobotinteractswiththeenvironment,andthecontextupdates
basedonfeedbackfromthephysicalaction:
ğ‘
ğ‘¡+1
= (ğ‘ ğ‘¡,ğ‘“skill(ğ‘Ë† ğ‘¡))
Whereğ‘“skill isthefunctionthatcapturestheresultofexecutingaphysicalskill,suchasmanipu-
latinganobjectormovingtoalocation.
3.1.2. SkillPlanner
Once a high-level request for the execution of a skill is made, the Skill Planner is responsible for
translatingthehigh-levelskills,providedbytheTaskPlanner,intosequencesoflow-levelcommands
executablebytherobot.WhiletheTaskPlannerfocusesonunderstandingnaturallanguageandcreating
ageneralplan,theSkillPlannerdealswiththespecificdetailsofhoweachskillshouldbeexecuted,
consideringtherobotâ€™sstateandtheenvironment.
Let a skill be represented in the following general form, defined by the Task Planner with specific
syntax:
ğ‘†ğ¾ğ¼ğ¿ğ¿_ğ‘ğ´ğ‘€ğ¸(ğ‘ğ‘ğ‘Ÿğ‘ğ‘š 1,ğ‘ğ‘ğ‘Ÿğ‘ğ‘š 2,...,ğ‘ğ‘ğ‘Ÿğ‘ğ‘š ğ‘)
Where:
â€¢ isthenameoftheskilltobeexecuted(e.g., , , ).
SKILL_NAME PICK PLACE GOTO
â€¢ are parameters for the skill, such as the object to
param_1, param_2, ..., param_N
manipulateorthedestinationtonavigateto.
UsingastrictsyntaxensuresthattheSkillPlannercancorrectlyinterpretthehigh-levelcommands
withoutambiguity. Forinstance,anaturallanguagecommandlike"Movenearthetableandgrabthe
bottle" wouldlackprecision. TheSkillPlannerneedsconcreteparametersfortherobottoacteffectively.
SkillPlannerworkflow: TheSkillPlanneroperatesbyperformingthreefunctions:
1. PreconditionVerification: Beforetranslatingaskillintolow-levelcommands,theSkillPlanner
verifiesthatthenecessarypreconditionsforexecutionaremet. Letğ‘  representthecurrentstateofthe
ğ‘¡
robotandtheenvironmentattimeğ‘¡,andğ‘ƒ(skill,ğ‘  )denoteafunctionforeveryskillthatevaluatesthe
ğ‘¡
preconditionsforagivenskill. Thepreconditioncheckcanbeexpressedas:
{ï¸ƒ
1, ifallpreconditionsaremet
ğ‘ƒ(skill,ğ‘  ) =
ğ‘¡ 0, otherwise
Forexample,beforeexecutingthe skill,thefollowingchecksmaybeperformed:
PICK
â€¢ Theobjectisvisiblebytherobot.
â€¢ Theobjectisreachablefortheroboticarm.
â€¢ Theroboticarmisfree.
Ifanyoftheseconditionsarenotmet(ğ‘ƒ(skill,ğ‘  ) = 0),theSkillPlannerreportsafailuretotheTask
ğ‘¡
Planner.
2. Target nodes extraction: Based on the parameters of the skill, the Skill Planner extracts the
targetnodesfromthesemanticmapâ„³,whichcontainsgeometricandsemanticinformationabouttheenvironment. Everynodeprovidesgeometricinformationsuchasobjectâ€™spositionandrelevant
context,whichisthenusedtogeneratelow-levelcommands.
3.GenerationofLow-LevelCommands:Whenğ‘ƒ(skill,ğ‘  ) = 1,theSkillPlannertranslatetheskill
ğ‘¡
intoasequenceoflow-levelcommandstocontroltherobotbehavior. Inthissystem,werepresentskill
decompositionincommandsasHierarchicalTaskNetworks(HTNs)thatcontainslow-levelcommands
executablebytherobot. Letğ¶ğ‘€(skill,node,ğ‘  )denotethefunctionthattranslatesthegivenskillinto
ğ‘¡
low-levelcommandsbasedonthetargetnodesextractedfromthesemanticmapandcurrentstate. The
outputisasequenceofpre-modeledcommandsparameterizedwiththeinformationoftherobotstate
andthetargetnodes,{ğ‘ğ‘š ,ğ‘ğ‘š ,...,ğ‘ğ‘š },whereeachcommandğ‘ğ‘š directsspecificcomponentsof
1 2 ğ‘˜ ğ‘–
therobot. OurimplementationuseHTNssolelyonthebreakdownofskillsintocommandswithout
usingthemwithadvancedfeatureslikere-planningorerrorrecoveryofthecommands. Inthiscase,if
anycommandfails,theentireskillfails,withnoattemptatre-planningattheskillplannerlevel. The
processcanberepresentedas:
{ğ‘ğ‘š ,ğ‘ğ‘š ,...,ğ‘ğ‘š } = ğ¶ğ‘€(skill,node,ğ‘  )
1 2 ğ‘˜ ğ‘¡
TheSkillPlannerisdesignedtobeflexibleandextendable. Theskillfunctionsğ‘ƒ,andğ¶ğ‘€ canbe
adaptedorextendedtoaccommodatenewskills,hardware,orenvironments.
3.1.3. Executor
TheExecutorisresponsiblefordirectlyinteractingwiththerobotâ€™shardwaretoexecutethecommands
providedbytheSkillPlanner. Ittranslatesthelow-levelcommandsintophysicalactionsbycontrolling
varioushardwareelementssuchasmotors,roboticarmgrippers,andotheractuatorsrequiredfortask
execution.
Let the set of low-level commands generated by the Skill Planner be represented as above, i.e.,
ğ‘ğ‘š ,ğ‘ğ‘š ,...,ğ‘ğ‘š = ğ¶ğ‘€(skill,node,ğ‘  ), where ğ¶ğ‘€(skill,node,ğ‘  ) defines the sequence of com-
1 2 ğ‘˜ ğ‘¡ ğ‘¡
mandsbasedontheskill,thetargetnode,andthecurrentstateoftherobotandtheenvironment.
TheExecutoristaskedwithexecutingthesecommandsonthephysicalrobot. Letthestateofthe
robotattimeğ‘¡bedenotedbyâ„ ,andthefunctionthatmapsalow-levelcommandğ‘ toaneffectonthe
ğ‘¡ ğ‘–
robotâ€™sstatebedenotedasğ»(ğ‘ğ‘š ,â„ ). Theexecutionofacommandattimeğ‘¡canbedescribedas:
ğ‘– ğ‘¡
â„ = ğ»(ğ‘ğ‘š ,â„ )
ğ‘¡+1 ğ‘– ğ‘¡
whereâ„ istheupdatedstateafterexecutingthecommandğ‘ğ‘š . Thisprocessisrepeatedforeach
ğ‘¡+1 ğ‘–
commandinthesequence{ğ‘ğ‘š ,ğ‘ğ‘š ,...,ğ‘ğ‘š }untiltheentireskillisexecuted.
1 2 ğ‘˜
Executorworkflow:
â€¢ Commandreception:TheExecutorreceivesasetoflow-levelcommands{ğ‘ğ‘š ,ğ‘ğ‘š ,...,ğ‘ğ‘š }
1 2 ğ‘˜
fromtheSkillPlanner. Eachcommandspecifiesaconcreteactiontobeperformedbytherobotâ€™s
hardwarecomponents.
â€¢ Hardwareinteraction:Foreachcommandğ‘ğ‘š ,theExecutorinteractswiththerobotâ€™shardware,
ğ‘–
adjustingthemotors,grippers,andotheractuators. Thisinteractioncanberepresentedbythe
functionğ»(ğ‘ğ‘š ,â„ )thatdeterminestheeffectofacommandontherobotâ€™sstateâ„ .
ğ‘– ğ‘¡ ğ‘¡
â€¢ Commandexecution: TheExecutorexecuteseachcommandğ‘ğ‘š inthesequence,ensuring
ğ‘–
thattherobotâ€™sstatetransitionsfromstateâ„ toâ„ . Formally:
ğ‘¡ ğ‘¡+1
â„_ğ‘¡+1 = ğ»(ğ‘ğ‘š_ğ‘–,â„_ğ‘¡), âˆ€ğ‘– = 1,2,...,ğ‘˜
After executing all commands, the robotâ€™s reaches the final state â„ , corresponding to the
ğ‘¡+ğ‘˜
completionoftheskill.â€¢ Real-Timefeedback: Duringexecution,therobotâ€™sprovidesfeedbackonitscurrentstate. Let
ğ‘“ denotethefeedbackattimeğ‘¡,andğ‘“ betheupdatedfeedbackafterexecutingcommandğ‘ğ‘š :
ğ‘¡ ğ‘¡+1 ğ‘–
ğ‘“ = ğ¹(ğ‘ğ‘š ,â„ )
ğ‘¡+1 ğ‘– ğ‘¡
where ğ¹ is the feedback function. If unexpected feedback ğ‘“ is received, the Executor can
ğ‘¡+1
triggeradjustmentstotheplanorinformtheSkillPlannerofapotentialissue.
Differentrobotsmayusedifferentcommunicationprotocols,andhardwareconfigurations. Therefore,
theExecutormustbeadaptedforeachspecificrobotsystem,ensuringthatitcorrectlyinteractswith
therobotâ€™shardware.
3.1.4. Controller
TheControllerisresponsibleformonitoringtherobotâ€™sstatusandtheenvironmentduringcommand
execution,ensuringthattheyarecarriedoutasplanned. Aftereachcommandisexecuted,theExecutor
sendsfeedbackindicatingeithersuccessorfailure. Ifafailureoccurs,itresultsinthefailureoftheentire
skill. Uponthecompletionofallcommands,asuccessfeedbackwillindicatethesuccessfulexecutionof
theskill.
Denote ğ‘“ the feedback from the Executor at time ğ‘¡. The Controller processes ğ‘“ to determine the
ğ‘¡ ğ‘¡
outcomeoftheexecutedskills. Thefeedbackcanbeclassifiedintotwocategories: successandfailure.
Feedbackprocessing:
â€¢ Success: Ifthefeedbackğ‘“ indicatessuccessfulexecutionofacommandanditisthelastcom-
ğ‘¡
mandtoexecute,theskillisconsideredsuccessfullycompleted,theControllersendsapositive
acknowledgmenttotheTaskPlannertocontinuetheplanningprocess. However,ifthefeedback
indicatessuccessbutthecommandisnotthelastone,theControllerwaitsfortheexecutionof
thenextcommand:
ifğ‘“ = Success =â‡’ TaskPlannercontinues
ğ‘¡
â€¢ Failure: Ifafailureoccursduringtheexecutionofanycommand,theplannedskillfailsandthe
Controllergeneratesafailuremessageğ‘š thatincludesthereasonforthefailure. Thismessage
ğ‘“
issenttotheExplainer. Letğ‘’ representthespecificerrordetectedattimeğ‘¡. Thefailuremessage
ğ‘¡
canberepresentedas:
ğ‘š = Failure(ğ‘’ )
ğ‘“ ğ‘¡
whereğ‘’ canincludevariouserrorreasonssuchasobstaclesdetected,non-executabletrajectories,
ğ‘¡
orenvironmentalchanges.
TheControllerâ€™soperationishighlydependentonthespecificrobotsysteminuse,asitreliesonthe
characteristicsoftherobotandtheemployedsoftwaresystem. InaROSenvironment,forexample,the
ControllerinteractswithROSnodesthatcontroltherobotâ€™shardware. Inourwork,RoBee,described
insection5,hasasystemthatallowstoobtainfeedbackontheexecutionofcommands.
3.1.5. Explainer
TheExplainercomponentplaysacriticalroleinenhancingtheplanningprocessbyprovidinginsights
totheTaskPlannerwhenfailuresoccurduringtheexecutionphase. Afterreceivingthefailurereason,
theExplainersearchesadatasetğ’Ÿ forpreviousinstancesofsimilarfailures. Thisdatasetcomprises
recordsoffailuresassociatedwithspecificskillsanduserrequests. Letğ’Ÿ denotethesubsetofthe
ğ‘Ÿ
ğ‘“
datasetcontainingrecordsoffailuresandsolutionsrelatedtothesameskillanderrormessage. The
datasethasbeenmanuallybuiltbasedonpreviousexperiences,desiredbehaviors,andexpectedfailures.
Thesearchcanbeexpressedas:
ğ’Ÿ = {(ğ‘  ,ğ‘¢ ,ğ‘Ÿ ) âˆˆ ğ’Ÿ|ğ‘  = skill_name, ğ‘’ = ğ‘Ÿ , ğ‘¢ âˆ¼ user_request}
ğ‘Ÿ ğ‘“ ğ‘˜ ğ‘Ÿ ğ‘“ ğ‘˜ ğ‘Ÿ ğ‘“ ğ‘Ÿ
where:â€¢ ğ‘  istheskillbeingexecuted(e.g.,PICK).
ğ‘˜
â€¢ ğ‘¢ representsthespecificuserrequestassociatedwiththefailure.
ğ‘Ÿ
â€¢ ğ‘Ÿ isthefailurereasonprovidedbytheController
ğ‘“
â€¢ ğ‘¢ âˆ¼ user_request indicates that the user request in the dataset is similar to the current user
ğ‘Ÿ
request.
Ratherthansearchingforanexactmatchtotheuserâ€™srequest,theExplainerassessesthesimilarityof
theuserâ€™srequest(ğ‘¢ )totheinstancesinthedatasetlinkedtothesuggestion,usingcosinesimilarityin
ğ‘Ÿ
ourapproach[34]. Thismethodenablesthesystemtoidentifythemostrelevantpastinstances,even
whentheuserâ€™srequestsarenotidentical.
Oncerelevantinstancesareidentified,theExplaineranalyzesthesecasestogenerateasuggestionğ‘ 
ğ‘”
fortheTaskPlanneronhowtoproceed. Thesuggestionisstructuredasfollows:
ğ‘  = Suggest(ğ’Ÿ )
ğ‘” ğ‘Ÿ
ğ‘“
Forinstance,iftheControllerreportsthefailurereason:
ğ‘Ÿ = "CannotexecutetheapproachmovementforthePICKskill,objecttoofar"
ğ‘“
The Explainer analyzes this failure and may find a previous instance where the robot successfully
resolvedasimilarissue. ItcouldrecommendacommandtotheTaskPlanner:
ğ‘  = "UsetheGOTOskilltomoveneartheobjecttopick"
ğ‘”
ThissuggestionenablestheTaskPlannertoadjustitsstrategyeffectively,movingtherobotcloserto
theobjectbeforeattemptingthePICKactionagain.
ThesuggestionsprovidedbytheExplainercanbetailoredtoaccommodatespecificbehaviorsofthe
robot. Thisadaptabilitycanbeachievedbymodifyingtheparametersofthedatautilizedtogenerate
thesuggestions. Thus, theExplainerenhancestheresilienceofthesystem, facilitatingitsabilityto
adapttochangingconditionsandrecoverfromerrorsefficiently.
4. Behavior example of the system
Toillustratetheproposedsystemâ€™sbehaviorinadynamicenvironment,considerascenariowherethe
robotistaskedwithpickingupabottlefromatableinaroomandplacingitinadesignatedareain
otherroom. Inthisexample,thesystemmustreacttounexpectedchanges,suchasthebottlebeing
movedbyanexternalagentduringthetask.
Example User Request: The user provides the instruction: "Go to the table in the kitchen, pick
up the bottle, and place it on the table in the bedroom." Perception Module: The robot senses the
environment and generateds a semantic map, which includes the pose and types of objects. Task
Planner: Therequestisinterpreted,andtheTaskPlannerbeginstheplanningprocedure. Itstartsby
callingperceptionactionstogatherinformationabouttheenvironmentandtherobotâ€™sstate. Using
the results of these perception actions, the Task Planner determines the next steps in its execution
bycallingskillactionsandincorporatingtheoutcomesofeachskillintotheplanningprocess. LLM
interpretstherequestandreasonsasfollows.UserRequest: "Gotothetableinthekitchen,pickupthebottle,andplaceitonthetableinthe
bedroom."
RobotState: Therobotiscurrentlyinthebedroomandhasboththerightandleftarmsempty.
Thought:Thetaskistofetchthebottlefromthekitchenandplaceitonthetableinthebedroom.
Ineedtoknowalltheroomswheretherobotcangoto.
Perceptionaction: GetMapRooms()
Observation: Therobotidentifiestherooms: [kitchen, bedroom].
Thought: Now,Iwillnavigatetothekitchentofindthebottle.
Skillaction: GOTO(kitchen)
Observation: Therobotmovestothekitchen.
Thought: Ishouldidentifythespecificlocationofthebottlewithinthekitchen.
Perceptionaction: GetObjectInRoom(kitchen)
Observation: Therobotfindsthefollowingobjectsinthekitchen: [sink on the table_1,
.
stove on the table_1, bottle, table_1, table_2]
Thought: Iwillnowmovetothelocationofthebottle.
Skillaction: GOTO(bottle)
Observation: Therobotmovestothebottleâ€™slocationinthekitchen.
Thought: Iwillpickupthebottlenow.
Skillaction: PICK(bottle, right)
Observation: Therobotcanâ€™tseethebottletopick,findabottleinthesemanticmapandtry
againtopickit.
Thought: Thebottleisnotvisiblewhereexpected,Ineedtocheckthelocationofthebottle
again.
Perceptionaction: GetObjectInRoom(kitchen)
Observation: Therobotfindsthefollowingobjectsinthekitchen: [sink on the table_1,
.
stove on the table_1, bottle on the table_2, table_1, table_2 ]
Thought: Iwillgotothetabletopickupthebottleinstead.
Skillaction: GOTO(table_2)
Observation: Therobotmovestothetable.
Thought: Iwillpickupthebottlefromthetablenow.
Skillaction: PICK(bottle, right)
Observation: Therobotsuccessfullypicksupthebottle.
Thought: Next,Ineedtonavigatetothebedroomtoplacethebottle.
Skillaction: GOTO(bedroom)
Observation: Therobotmovestothebedroom.
Thought: IneedtolocatethetableinthebedroomwhereIwillplacethebottle.
Perceptionaction: GetObjectInRoom(bedroom)
Observation: Therobotsees: [table, bed, lamp on the table]inthebedroom.
Thought: Iwillapproachthetabletoplacethebottle.
Skillaction: GOTO(table)
Observation: Therobotapproachesthetable.
Thought: Finally,Iwillplacethebottleonthetable.
Skillaction: PLACE(bottle, right)
Observation: Therobotplacesthebottleonthetablesuccessfully.
SkillPlanner: Foreachhigh-levelskill,oncethepreconditionsaremet,theSkillPlannertranslates
thechoosenskillinasequenceoflow-levelcommands,suchasmotormovementsfornavigation,armarticulationforpicking,andplacingactions. Forexampleoncethe skillis
PICK(bottle, right)
planned,itcanbetranslatedanddividedintothefollowingphaseswithrelativecommands:
â€¢ Approach:Therobotarmmovestowardstheobjectâ€™sposition,makinganynecessaryadjustments
toaligncorrectly,andopensthegripper.
â€¢ Grasp: The robot activates the gripping mechanisms to seize the object. This phase includes
closingthegripperandverifyingthegrasp.
â€¢ Lifting: Therobotliftstheobjectfromthesurfaceitison.
Execution: TheExecutorbeginsexecutingtheplannedskill,whichiscomposedofasequenceof
commandsbytheSkillPlanner. TheExecutorfollowstheorderedstepstoachievethegoal. Forexample
withtheskill ,theExecutorreceivethelistofcommandandexecute:
PICK(bottle, right)
â€¢ Executeapproach: Therobotarmmovestowardstheobjectâ€™spositionandopenthegripper.
â€¢ Executegrasp: Thisphaseincludesclosingthegripperandverifyingthegrasp.
â€¢ Executelifting: Therobotliftstheobjectfromthesurfaceitison.
Thus,whenanunexpectedeventoccurs,suchasthebottlebeingmovedorisnotreachabletheexecutor
mayraiseafailuremessage.
ControllerandExplainerinteraction:
â€¢ TheControllerdetectsthattheobjectisnolongerintheexpectedlocationandsendsafailure
messagetotheExplainer.
â€¢ TheExplaineranalyzesthefailure,referencingpreviousinstanceswhereobjectsweremoved
unexpectedly. ItsuggeststheTaskPlannertoreanalysethesemanticmapandupdatetheobjectâ€™s
location.
Re-planning: Basedonthesuggestion,theTaskPlannerissuesanewplan:
â€¢ Execute togoneartheidentifiedbottle.
GOTO(table)
â€¢ Afterlocatingthebottleonthetable,therobotupdatesitsactionsandproceedstoexecutethe
remainingtasks.
Thisexampledemonstrateshowthesystemadaptsinreal-time,allowingforcontinuoustaskexecution
evenindynamicandunpredictableenvironments.
Planningalgorithm Wenowformalizethisprocessintheformofanadaptiveplanningalgorithm.
Inthisalgorithm,theusedLLMisageneralistmodelsuchasLlama370BInstruct [35],whosebehavior
weinfluencethroughin-contextlearning[9].Figure3:Robee,humaniodrobotdevelopedbyOversonicRobotics.
Algorithm1PlanningwithextedendReActFramework
1: Input: Userrequestğ‘Ÿ,Robotstateğ‘…
ğ‘ 
2: Output: Executionofuserrequest
3: procedurePlanning(ğ‘Ÿ,ğ‘€)
4: ğ¶ â† InitializeLLMContext(ğ‘Ÿ, ğ‘€, ğ‘… )
0 ğ‘ 
5: whilenotgoalachieveddo
6: ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† TaskPlanner(ğ‘Ÿ,ğ¶ ) â—Getfirstskill
0
7: if ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = "Skill" then
8: ğ‘ğ‘œğ‘šğ‘šğ‘ğ‘›ğ‘‘ğ‘  â† SkillPlanner(ğ‘ ğ‘˜ğ‘–ğ‘™ğ‘™,ğ¶ ) â—Translateskillintolow-levelcommands
ğ‘¡
9: ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘  â† Executor(ğ‘ğ‘œğ‘šğ‘šğ‘ğ‘›ğ‘‘ğ‘ ) â—Executecommands
10: if ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘  = Falsethen
11: ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’ğ‘€ğ‘ ğ‘” â† Controller(ğ¶ ) â—Detectfailure
ğ‘¡
12: ğ‘ â† Explainer(ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’ğ‘€ğ‘ ğ‘”) â—Generatesuggestion
ğ‘¡
13: else
14: ğ‘ â† Skillsuccesfullyexecuted
ğ‘¡
15: endif
16: else
17: ğ‘ â† CallPerceptionAction() â—ReadingsemanticmapfromPerceptionModule
ğ‘¡
18: endif
19: ğ¶ â† UpdateContext(ğ¶ ) â—Updatecontext
ğ‘¡+1 ğ‘¡
20: ğ‘ ğ‘˜ğ‘–ğ‘™ğ‘™ â† TaskPlanner(ğ‘Ÿ,ğ¶ ) â—Getnextskillbasedonupdatedcontext
ğ‘¡+1
21: endwhile
22: endprocedure
This algorithm shows the adaptive behavior of the system by incorporating feedback loops that
facilitate real-time re-planning. By alternating between action and reasoning phases, the robot can
continuouslyadapttochanges,ensuringtasksuccesseveninunpredictableenvironments.Figure4:Environmentusedduringtheexecutionofexperiments.
5. Robot Hardware
ThesystemwasimplementedusingRoBee,acognitivehumanoidrobotdevelopedbyOversonicRobotics.
RoBee measures 160 cm in height and weighs 60 kg. It has 32 degrees of freedom, enabling highly
flexiblemovement. Therobotisequippedwithmultiplesensors,includingcameras,microphones,and
forcesensors.
Thecamerasprovidereal-timevisualdata,supportingnavigationandobjectrecognitiontasks. The
microphonesfacilitateaudioinput,enablingspeechrecognitionandinteractionthroughnaturallan-
guageprocessing. Theforcesensorsareusedforhandlingobjects,allowingRoBeetoadjustgripforce
based on the characteristics of the item being manipulated, enhancing precision and safety during
interactions.
RoBeeâ€™smechanicalstructureincludestwoarmscapableofbimanualmanipulation,eachcapableof
handlingobjectsweighingupto5kg. Thesystemincludesatorsoandlegsystemdesignedforbal-
ance and mobility. RoBee is equipped with LIDAR sensors for real-time environment mapping and
obstacledetection. TheseLIDARsensorsenabletherobottonavigateautonomouslythroughcomplex
environments,ensuringsafeoperationinsharedspaces. Thecombinationofautonomousnavigation
technologiesandLIDAR-baseddetectionenhancestheabilityofRoBeetomoveefficientlyandavoid
collisionsindynamicindustrialenvironments.
Inadditiontoitsphysicalcapabilities,RoBeeintegrateswithcloud-basedsystems,allowingforremote
monitoring,taskscheduling,anddataanalytics.
ThePlanner-moduletakesintoaccountRoBeeâ€™sembodiment,ensuringthatthesystemisalignedwith
therobotâ€™scapabilitiessuchasitsdegreesoffreedom,sensorsuite,andabilitytoperformmanipulation
andnavigation.
6. Preliminary results
Preliminaryexperimentswereconductedinasimulatedenvironmentreplicatingtwomainrooms: a
kitchenandabedroom,asillustratedinFigure6.
Duringtheexperiments,threetypesofrequestsweretested,eachvaryingincomplexity:
â€¢ Simplerequests: directcommandsthatinvolveonlyoneskill. Forexample,"Pickupthebottle
infrontofyou",wherethetaskplannerneedsonlytoidentifytheparametersandactivatethe
appropriateskill.
â€¢ Moderately complex requests: tasks that require the robot to perform multiple skills in
sequence,asexplicitlydescribedinthecommand. Anexampleis"Gotothekitchen,pickupthe
bottle,andbringittothetableinthebedroom",whichinvolvesmultipleskills. Thesetasksrequire
ahigherlevelofcomplexity,withplanningacrossseveralstepsandhandlingpotentialfailures.â€¢ Complexrequests: suchas"Iâ€™mthirsty,canyouhelpme?",whichweremoreopen-endedand
requiredtherobottointerpretthetaskandbreakitdownintomultiplesteps.
The results in table 1 showed that the system performed well with simple requests, followed by
moderately complex ones. However, the success rate for complex requests was significantly lower,
withonly25%ofthetaskscompletedcorrectly. Thislowerperformancewasattributedtothesystemâ€™s
difficultyinunderstandingandmanagingambiguousorunder-specifiedinstructions.
Itisimportanttonotethatthesearepreliminaryresults,andfurtheranalysisisongoing. Athorough
evaluationofthedataiscurrentlyunderway,includingacomparisonwiththestateoftheartinrobot
taskexecutionandnaturallanguageunderstanding. Thiswillallowforadeeperunderstandingofthe
systemâ€™sstrengthsandareasforimprovement.
Requesttype Numberofattempts Successrate
Simplerequests 30 90%
Moderatelycomplexrequests 20 75%
Complexrequests 10 25%
Table1
Numberofattemptsandsuccessrateforeachrequesttype
7. Conclusions
Theproposedplanningsystemexhibitsnotablestrengths,particularlyitsadaptabilityandseamlesswith
therobotâ€™sdiversesetofskillforexecutingcomplextasks. Thesystemâ€™scoreadvantageliesinitsability
to interpret user commands through natural language processing, converting them into high-level
actionsthatarefurtherrefinedintolow-level,executabletasks. Byintegratingreal-timeenvironmental
feedbackfromthePerceptionModulethroughanextendedversionofReActframework,thesystemcan
dynamicallyadjusttounexpectedsituations,suchasobstaclesorexecutionfailures. Thisadaptability
is supported by an architecture, where the Task Planner, Skill Planner, Controller, and Explainer
componentsworkinharmonytoensuresmoothtaskexecutioneveninchangingenvironments.
One of the systemâ€™s key strengths is its ability to manage error recovery through feedback loops,
allowingtherobottoadaptquicklytofailuresduringtaskexecution. TheExplainermoduleprovideson
theflysuggestionstomodifytheplanbasedonpasterrors,enhancingthesystemâ€™svalidity. Theuseof
semanticmapsandscenegraphsprovidestherobotwithastructuredunderstandingofitsenvironment,
ensuringthatactionsarecontextuallyaccurateandresponsivetoreal-worldconditions.
TheintegrationofLLMs,perceptualfeedback,andflexibletaskplanningmechanismsmakesthesystem
highlyversatileforcomplex,dynamicenvironments. ItsimplementationonRoBee,thehumanoidrobot
developedbyOversonicRobotics,hasdemonstrateditspracticalpotential,positioningitasavaluable
toolforapplicationsrequiringadvancedhuman-robotinteractionandadaptabilityinunpredictable
settings.
Inthefuture,otherthanextendingthelowlevelskillsetavailable,wewillinvestigatethepossibilityto
autonomouslyexpandtheExplainerdatasetaswellasprovidingsimilarinformationdirectlytotheTask
Planner,increasingflexibilityandreliabilityandreducingthenumberofre-planningevents. Wewill
alsostudycapabilityofthesystemtoproactivelyacquireinformationabouttheenvironment[14]and
humanpartnersboththroughsensors[36]andcommunicationstrategies,leveragingthepotentialfor
proactiveinformationgatheringbehavioursofLLMs[37,38,39]. Moreover,itwillbecrucialtoassess
thereliabilityofthesystembothattheplanninglevelaswellasthecommunicationlevel,considering
theintroductionofembodimentandenvironmentwhilethelimitationinpragmaticunderstandingof
LLMarestilltobeunderstood[39,40,41].Acknowledgments
Special thanks to Oversonic Robotics for enabling the implementation of this project using their
humanoidrobot,RoBee.
References
[1] D.Aineto,R.DeBenedictis,M.Maratea,M.Mittelmann,G.Monaco,E.Scala,L.Serafini,I.Serina,
F.Spegni,E.Tosello, A.Umbrico,M.Vallati(Eds.),ProceedingsoftheInternationalWorkshop
onArtificialIntelligenceforClimateChange,theItalianworkshoponPlanningandScheduling,
theRCRAWorkshoponExperimentalevaluationofalgorithmsforsolvingproblemswithcom-
binatorialexplosion,andtheWorkshoponStrategies,Prediction,Interaction,andReasoningin
Italy(AI4CC-IPS-RCRA-SPIRIT2024),co-locatedwith23rdInternationalConferenceoftheItalian
AssociationforArtificialIntelligence(AIxIA2024),CEURWorkshopProceedings,CEUR-WS.org,
2024.
[2] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,Q.Vuong,
T. Yu, et al., Palm-e: an embodied multimodal language model, in: Proceedings of the 40th
InternationalConferenceonMachineLearning,2023,pp.8469â€“8488.
[3] M.Ahn,A.Brohan,N.Brown,Y.Chebotar,O.Cortes,B.David,C.Finn,C.Fu,K.Gopalakrishnan,
K.Hausman,etal., Doasican,notasisay: Groundinglanguageinroboticaffordances, arXiv
e-prints(2022)arXivâ€“2204.
[4] J.Wang,Z.Wu,Y.Li,H.Jiang,P.Shu,E.Shi,H.Hu,C.Ma,Y.Liu,X.Wang,etal., Largelanguage
modelsforrobotics: Opportunities,challenges,andperspectives, arXivpreprintarXiv:2401.04334
(2024).
[5] F.Zeng,W.Gan,Y.Wang,N.Liu,P.S.Yu, Largelanguagemodelsforrobotics: Asurvey, arXiv
e-prints(2023)arXivâ€“2311.
[6] S.Kambhampati,K.Valmeekam,L.Guan,K.Stechly,M.Verma,S.Bhambri,L.Saldyt,A.Murthy,
Llmscanâ€™tplan,butcanhelpplanninginllm-moduloframeworks, arXivpreprintarXiv:2402.01817
(2024).
[7] S.Tellex,N.Gopalan,H.Kress-Gazit,C.Matuszek, Robotsthatuselanguage, AnnualReviewof
Control,Robotics,andAutonomousSystems3(2020)25â€“55.
[8] G.Zhu, L.Zhang, Y.Jiang, Y.Dang, H.Hou, P.Shen, M.Feng, X.Zhao, Q.Miao, S.A.A.Shah,
etal., Scenegraphgeneration: Acomprehensivesurvey, arXive-prints(2022)arXivâ€“2201.
[9] Q.Dong,L.Li,D.Dai,C.Zheng,Z.Wu,B.Chang,X.Sun,J.Xu,Z.Sui, Asurveyonin-context
learning, arXivpreprintarXiv:2301.00234(2022).
[10] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,Y.Cao, React: Synergizingreasoningand
actinginlanguagemodels, in: InternationalConferenceonLearningRepresentations(ICLR),2023.
[11] L. Heuss, D. Gebauer, G. Reinhart, Concept for the automated adaption of abstract planning
domainsforspecificapplicationcasesinskills-basedindustrialrobotics, JournalofIntelligent
Manufacturing(2023)1â€“26.
[12] M.Shanahan, Frameproblem,the, EncyclopediaofCognitiveScience(2006).
[13] L. P. Kaelbling, M. L. Littman, A. R. Cassandra, Planning and acting in partially observable
stochasticdomains, Artificialintelligence101(1998)99â€“134.
[14] D. Ognibene, G. Baldassare, Ecological active vision: four bioinspired principles to integrate
bottomâ€“up and adaptive topâ€“down attention tested with a simple camera-arm robot, IEEE
transactionsonautonomousmentaldevelopment7(2014)3â€“25.
[15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, P. Fung, Survey of
hallucinationinnaturallanguagegeneration, ACMComputingSurveys55(2023)1â€“38.
[16] O.Ruiz,J.Rosell,M.Diab, Reasoningandstatemonitoringfortherobustexecutionofrobotic
manipulationtasks, in: 2022IEEE27thInternationalConferenceonEmergingTechnologiesand
FactoryAutomation(ETFA),IEEE,2022,pp.1â€“4.[17] J.Liang,W.Huang,F.Xia,P.Xu,K.Hausman,B.Ichter,P.Florence,A.Zeng, Codeaspolicies:
Language model programs for embodied control, in: 2023 IEEE International Conference on
RoboticsandAutomation(ICRA),IEEE,2023,pp.9493â€“9500.
[18] B.Liu,Y.Jiang,X.Zhang,Q.Liu,S.Zhang,J.Biswas,P.Stone, Llm+p:Empoweringlargelanguage
modelswithoptimalplanningproficiency, arXive-prints(2023)arXivâ€“2304.
[19] C.H.Song,J.Wu,C.Washington,B.M.Sadler,W.-L.Chao,Y.Su, Llm-planner:Few-shotgrounded
planning for embodied agents with large language models, in: Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,2023,pp.2998â€“3009.
[20] Z.Wang,S.Cai,G.Chen,A.Liu,X.Ma,Y.Liang, Describe,explain,planandselect: Interactive
planningwithlargelanguagemodelsenablesopen-worldmulti-taskagents, arXive-prints(2023)
arXivâ€“2302.
[21] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, S. Savarese, 3d scene graph: A
structureforunifiedsemantics,3dspace,andcamera,in:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,2019,pp.5664â€“5673.
[22] Y.Liu,L.Palmieri,S.Koch,I.Georgievski,M.Aiello, Delta: Decomposedefficientlong-termrobot
taskplanningusinglargelanguagemodels, arXive-prints(2024)arXivâ€“2404.
[23] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,N.Suenderhauf, Sayplan: Groundinglarge
languagemodelsusing3dscenegraphsforscalablerobottaskplanning, in:7thAnnualConference
onRobotLearning,2023.
[24] M.Cashmore, A.Coles, B.Cserna, E.Karpas, D.Magazzeni, W.Ruml, Replanningforsituated
robots, in: ProceedingsoftheInternationalConferenceonAutomatedPlanningandScheduling,
volume29,2019,pp.665â€“673.
[25] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, D. Sadigh, Distilling and
retrievinggeneralizableknowledgeforrobotmanipulationvialanguagecorrections, in: 2024IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2024,pp.15172â€“15179.
[26] M.Skreta,Z.Zhou,J.L.Yuan,K.Darvish,A.Aspuru-Guzik,A.Garg, Replan: Roboticreplanning
withperceptionandlanguagemodels, arXive-prints(2024)arXivâ€“2401.
[27] H.Geffner, Non-classicalplanningwithaclassicalplanner: Thepoweroftransformations, in:
EuropeanWorkshoponLogicsinArtificialIntelligence,Springer,2014,pp.33â€“47.
[28] D. Ognibene, G. Pezzulo, H. Dindo, Resources allocation in a bayesian, schema-based model
ofdistributedactioncontrol, in: NIPS-WorkshoponProbabilisticApproachesforRoboticsand
Control,2009.
[29] M.Ho,D.Abel,J.Cohen,M.Littman,T.Griffiths, Peopledonotjustplan,theyplantoplan, in:
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,2020,pp.1300â€“1307.
[30] S.Russell,E.Wefald, Principlesofmetareasoning, Artificialintelligence49(1991)361â€“395.
[31] S.Zilberstein,S.J.Russell, Anytimesensing,planningandaction: Apracticalmodelforrobot
control, in: IJCAI,volume93,1993,pp.1402â€“1407.
[32] R.Ackerman,V.A.Thompson, Meta-reasoning:Monitoringandcontrolofthinkingandreasoning,
Trendsincognitivesciences21(2017)607â€“617.
[33] S.J.Russell,P.Norvig,Artificialintelligence: amodernapproach,Pearson,2016.
[34] F.Rahutomo,T.Kitasuka,M.Aritsugi,etal., Semanticcosinesimilarity, in: The7thinternational
studentconferenceonadvancedscienceandtechnologyICAST,volume4,UniversityofSeoul
SouthKorea,2012,p.1.
[35] A.Dubey,A.Jauhri,A.Pandey,A.Kadian,A.Al-Dahle,A.Letman,A.Mathur,A.Schelten,A.Yang,
A.Fan,etal., Thellama3herdofmodels, arXiv.org(????).
[36] D.Ognibene,Y.Demiris, Towardsactiveeventrecognition., in: IJCAI,2013,pp.2495â€“2501.
[37] S.Patania,E.Masiero,L.Brini,G.Donabauer,U.Kruschwitz,V.Piskovskyi,D.Ognibene, Large
languagemodelsasanactivebayesianfilter: informationacquisitionandintegration, in: Proceed-
ingsofthe28thWorkshopontheSemanticsandPragmaticsofDialogue-FullPapers,SEMDIAL,
Trento,Italy,2024.URL:http://semdial.org/anthology/Z24-Patania_semdial_0006.pdf.
[38] A.Z.Ren,A.Dixit,A.Bodrova,S.Singh,S.Tu,N.Brown,P.Xu,L.Takayama,F.Xia,J.Varley,etal.,
Robotsthataskforhelp: Uncertaintyalignmentforlargelanguagemodelplanners, ProceedingsofMachineLearningResearch229(2023).
[39] B.Magnini, Towardcollaborativellms: Investigatingproactivityintask-orienteddialogues, in:
Proceedingsofthe28thWorkshopontheSemanticsandPragmaticsofDialogue-InvitedTalks,
SEMDIAL,Trento,Italy,2024.URL:http://semdial.org/anthology/Z24-Magninini_semdial_0003a.
pdf.
[40] A.Martinenghi,C.Koyuturk,S.Amenta,M.Ruskov,G.Donabauer,U.Kruschwitz,D.Ognibene,
Vonneumidas: Enhancedannotationschemaforhuman-llminteractionscombiningmidaswith
vonneumanninspiredsemantics, in: Proceedingsofthe28thWorkshopontheSemanticsand
PragmaticsofDialogue-PosterAbstracts,SEMDIAL,Trento,Italy,2024.URL:http://semdial.org/
anthology/Z24-Martinenghi_semdial_0045.pdf.
[41] A. Martinenghi, G. Donabauer, S. Amenta, S. Bursic, M. Giudici, U. Kruschwitz, F. Garzotto,
D. Ognibene, Llms of catan: Exploring pragmatic capabilities of generative chatbots through
predictionandclassificationofdialogueactsinboardgamesâ€™multi-partydialogues, in:Proceedings
ofthe10thWorkshoponGamesandNaturalLanguageProcessing@LREC-COLING2024,2024,
pp.107â€“118.
8. Online Resources
MoreinformationaboutRoBeeandOversonicRoboticsareavailable:
â€¢ RoBee,
â€¢ OversonicRobotics