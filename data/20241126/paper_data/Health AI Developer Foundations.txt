2024-11-25
Health AI Developer Foundations
Atilla P. Kiraly‚àó,1, Sebastien Baur‚àó,1, Kenneth Philbrick1, Fereshteh Mahvar1, Liron Yatziv1,
Tiffany Chen1, Bram Sterling1, Nick George1, Fayaz Jamil1, Jing Tang1, Kai Bailey1, Faruk
Ahmed1, Akshay Goel1, Abbi Ward1, Lin Yang1, Andrew Sellergren1, Yossi Matias1,
Avinatan Hassidim1, Shravya Shetty1, Daniel Golden1, Shekoofeh Azizi2, David F. Steiner1,
Yun Liu1, Tim Thelin1, Rory Pilgrim1 and Can Kirmizibayrak1
1GoogleResearch,2GoogleDeepMind,‚àóEqualcontributions., Correspondence:health-ai-foundations@google.com
RobustmedicalMachineLearning(ML)modelshavethepotentialtorevolutionizehealthcarebyacceler-
atingclinicalresearch,improvingworkflowsandoutcomes,andproducingnovelinsightsorcapabilities.
DevelopingsuchMLmodelsfromscratchiscostprohibitiveandrequiressubstantialcompute,data,and
time(e.g.,expertlabeling). Toaddressthesechallenges,weintroduceHealthAIDeveloperFoundations
(HAI-DEF),asuiteofpre-trained,domain-specificfoundationmodels,tools,andrecipestoaccelerate
buildingMLforhealthapplications. Themodelscovervariousmodalitiesanddomains,includingradi-
ology(X-raysandcomputedtomography),histopathology,dermatologicalimaging,andaudio. These
modelsprovidedomainspecificembeddingsthatfacilitateAIdevelopmentwithlesslabeleddata,shorter
trainingtimes,andreducedcomputationalcostscomparedtotraditionalapproaches. Inaddition,we
utilizeacommoninterfaceandstyleacrossthesemodels,andprioritizeusabilitytoenabledevelopers
tointegrateHAI-DEFefficiently. Wepresentmodelevaluationsacrossvarioustasksandconcludewitha
discussionoftheirapplicationandevaluation,coveringtheimportanceofensuringefficacy,fairness,
andequity. Finally,whileHAI-DEFandspecificallythefoundationmodelslowerthebarriertoentryfor
ML in healthcare, we emphasize the importance of validation with problem- and population-specific
dataforeachdesiredusagesetting. Thistechnicalreportwillbeupdatedovertimeasmoremodalities
andfeaturesareadded.
1. Introduction
Machine learning (ML) models, trained on diverse data ranging from genomic sequences to clinical
images, have the potential to transform healthcare in applications ranging from accelerating drug
discovery to enabling personalized diagnoses. In day-to-day clinical workflows, ML models have also
been developed to help automate manual processes, assist with triage, diagnosis, or prognosis, and
more, with the goal of helping improve quality of care or efficiency.
However, building robust ML models for these domains entails challenges. Development often
requires large, labeled datasets, which are expensive and time-consuming to create and curate.
Beyond cost, sharing these datasets across institutions is often restricted due to privacy and other
considerations. Datascarcity,especiallyforrareconditionsandunderrepresentedpopulations,further
hinders dataset curation and limits generalizability. Finally, significant compute resources are often
necessary to train large models or when utilizing large datasets, and clinical and technical modality-
specific expertise, especially in the use of DICOMs for pathology and radiology, is often needed to
correctly prepare data for ML models.
To help address these challenges, we present Health AI Developer Foundations (HAI-DEF), with
thegoalofcatalyzingthedevelopmentandadoptionofAIinhealthcare. HAI-DEFincludesfoundation
models, tooling, recipes, and ready-to-use research endpoints. These resources were created to
enable researchers and developers to both iterate on research ideas quickly and have a lower-friction
path to incorporating AI in real-world use settings. In the initial phase, we offer research endpoints
and open-weight models, enabling generation of high-quality embeddings from medical images
(chest X-rays (CXR), histopathology patches, skin images, computed tomography (CT) images) and
¬© 2024Google.Allrightsreserved
4202
voN
22
]GL.sc[
1v82151.1142:viXraHealthAIDeveloperFoundations
audio recordings (health acoustics like coughs and breaths). These embeddings offer a compact
yet information rich representation of the given data. By leveraging these embeddings and tooling,
researchers and developers can build robust models that perform well across diverse clinical settings
with significantly less labeled data, shorten model training times, and reduce computational costs.
Table 1 provides an overview of the modalities and features offered per modality. Documentation
and additional information is available at the HAI-DEF developer site.
Table1 | HAI-DEF:AMultimodalPlatformforacceleratinghealthAI.HAI-DEFsupportsresearchacrossaudiosignals,
radiology,histopathology,anddermatology,withmoremodalitiesplanned.Foundationmodelsforeachmodalitycanbe
accessedviaresearchendpointsordeployedusingopenweightmodelsinopen-sourcecontainers. Per-modalitytools,
includingacustomhistopathologylibraryandacode-freeX-rayembeddinginterface,simplifyintegrationandaccelerate
research.
Modalities Research Endpoint Open Model Weights Open Source Container
Audio ‚úì
Computed Tomography ‚úì
Histopathology ‚úì ‚úì ‚úì
Dermatology ‚úì ‚úì ‚úì
X-ray (CXR) ‚úì ‚úì ‚úì
2. Models
HAI-DEF encompasses multiple distinct models, each tailored to a specific use case and trained using
advancedtechniquesonlarge,diversedatasets. Thefollowinglistisasnapshotofourcurrentprogress,
and our goal is to expand this model set in the near future.
HAI-DEF encompasses multiple distinct models, each tailored to a specific data modality and
trained using various techniques on large, diverse datasets. The following list snapshots our current
available models, which we expect to expand in the near future. More information about how to
accessthesemodelscanbefoundattheHAI-DEFdevelopersite(forCXR,PathandDermFoundation)
and respective GitHub repositories for the research endpoints (HeAR and CT Foundation).
2.1. CXR Foundation
CXR Foundation (Sellergren et al., 2022) is a set of 3 models, all using an EfficientNet-L2 (Xie et al.
(2020)) image encoder backbone. The three models learned representations of CXRs by leveraging
both the image data and the clinically relevant information available in corresponding radiology
reports.
1. The original CXR Foundation is trained using supervised contrastive learning (SupCon, Khosla
et al. (2020)). This model is used for legacy support and comparison purposes.
2. ELIXR-C (Xu et al., 2023) is an image/text encoder trained with the CLIP method (Radford
etal.,2021). Thismodeltendstoperformbetteratzero-shottasks,i.e. thosethatdonotrequire
further data for training.
3. ELIXR-B (Xu et al., 2023) is trained with the BLIP-2 method (Li et al., 2022) on a dataset of
over 1,000,000 chest radiographs from 5 hospitals in India, and 4 hospitals in the USA. This
model tends to perform better on downstream classification tasks.
2HealthAIDeveloperFoundations
2.2. Path Foundation
PathFoundation(Laietal.,2023)isaVisionTransformer(ViT)(Dosovitskiyetal.,2020)encoderfor
histopathologyimagepatchestrainedwithself-supervisedlearning(MaskedSiameseNetworks,Assran
et al. (2022)). It incorporates pathology-specific optimizations, including approaches to help learn
stain-agnostic features and to generalize across patch magnifications. Training for this model utilized
hematoxylin and eosin (H&E) stained whole slide images (WSIs) from The Cancer Genome Atlas
(TCGA) (National Cancer Institute (NCI), 2024).
2.3. Derm Foundation
DermFoundation(Rikhyeetal.,2024)isaBiTResNet-101x3(Kolesnikovetal.,2019)imageencoder
trained using a two-stage approach on over 16K natural and dermatology images (Liu et al., 2020).
First,contrastivelearning(ConVIRT,Zhangetal.(2020))wasappliedonalargenumberofimage-text
pairsfromtheinternet(Sunetal.(2017)). Second,themodelwasfine-tunedtoidentifydermatology
conditions on a mix of datasets, including tele-dermatology and skin cancer datasets.
2.4. HeAR
HeAR (Baur et al., 2024) is a ViT audio encoder trained using a Masked Autoencoder (MAE) ap-
proach (He et al., 2021) on a large dataset of 313 million unlabelled, non-medical audio clips. The
model learns to reconstruct masked spectrogram patches, capturing rich acoustic representations of
health-related sounds like coughs and breathing patterns.
2.5. CT Foundation
CT Foundation provides embeddings suitable for downstream classification tasks. The underlying
model is VideoCoCa (Yan et al., 2023), a video-text model designed for efficient transfer learning
from 2D Contrastive Captioners (CoCa) (Yu et al., 2022). CT Foundation was trained on CT volumes
and radiology reports from over 500,000 cases across different anatomic regions, using a similar
approach to Yang et al. (2024).
3. Evaluations
We evaluated the efficacy of our domain-specific foundation models on a suite of data-efficient
classification tasks, benchmarking their performance against generic models when possible. Across
various levels of training data subsampling, classifiers leveraging foundation model embeddings
consistently outperformed those using generic embeddings, demonstrating superior data efficiency
(Figure 1).
Beyond data efficiency, our foundation models demonstrate strong generalization capabilities
across diverse tasks within their respective domains. The CXR foundation model (ELIXR-B) achieved
strong performance on tasks spanning classification, semantic search, visual question answering,
and report quality assurance (Xu et al., 2023). The Derm foundation model effectively handles data
covering419skinconditions,withsubgroupanalysisrevealingnostatisticallysignificantperformance
difference across Fitzpatrick skin types (Rikhye et al., 2024). HeAR, our health acoustics model,
generalizes better when tested on audio recordings from unseen devices, compared to other strong
audioencoders(CLAP(Elizaldeetal.,2023),TRILL(Shoretal.,2020),FRILL(Peplinskietal.,2020),
BigSSL-12 (Zhang et al., 2022)), signifying its robust generalization capabilities (Baur et al., 2024).
Finally,thePathfoundationmodelexhibitsstrongperformanceonawiderangeoftasksencompassing
17uniquetissuetypesand12cancertypes,includingtumordetection,grading,subtyping,andtissue
type classification (Lai et al., 2023).
3HealthAIDeveloperFoundations
Figure1 | Dataefficiencycomparisonoffourofourfoundationmodelsagainstestablishedapproaches.CXR(upperleft)
showstheaverageperformanceacrosssixbinaryclassificationtaskswiththeoriginalCXRFoundationmodelandthenew
ELIXR-Bmodel;forPathology(upperright),itfocusesona4-classprostatecancerGleasongradingtaskforneedlecore
biopsiesandthedetectionofmetastaticbreastcancerinlymphnodes. ForDerm(lowerleft),theevaluationcentered
onskinconditioncategory(28-way)identification,whileforHeAR(lowerright),itinvolvedidentifyingCOVID-19from
coughsounds.Notably,thefoundationmodelsachievecomparableresultswhileusingsubstantiallylesstrainingdataand
compute.
4HealthAIDeveloperFoundations
These results highlight the potential of our domain-specific foundation models to substantially
reduce the data, compute, and technical expertise required for developing task-specific deep learning
models in their respective domains, while achieving comparable or superior performance to existing
approaches. Further details on each foundational model are presented below.
3.1. CXR Foundation
Area under the curve (ROC AUC) metrics with both linear and non-linear models applied to CXR
embeddingswereevaluated. Onpublicdatasets,suchasChestX-ray14andCheXpert,resultsimproved
the data-accuracy trade-off for models developed across a range of training dataset sizes and several
findings. Figure 2 shows the comparison for all three models using a linear probe. When evaluating
the original CXR Foundation‚Äôs ability to develop tuberculosis models, data efficiency gains were more
striking: modelstrainedontheembeddingsofjust45imagesachievednon-inferioritytoradiologistsin
detectingtuberculosisonanexternalvalidationdataset(Sellergrenetal.,2022). Forbothtuberculosis
andsevereCOVID-19outcomes,wehaveshownthatnon-linearclassifierstrainedonCXRFoundation
embeddingsoutperformedamodelthatwasfine-tunedontheentiredataset(Sellergrenetal.,2022).
Figure2 | PerformanceofELIXR-C,ELIXR-B,andtheoriginalCXRFoundationembeddingsfordata-efficientclassification.
TheROCAUCresultsofalinearprobeareshownaveragedacross2datasets(CheXpertandChestX-ray14)forsevenfindings:
atelectasis,cardiomegaly,airspaceopacity,fracture,pneumothorax,consolidation,pleuraleffusion,andpulmonaryedema.
BothELIXR-CandELIXR-BdemonstratesuperiorperformancecomparedtotheoriginalCXRFoundationatmatching
datasetsizes.
We have also developed a no-code interface to demonstrate the feasibility and performance of
binary classification tasks based on CXR Foundation. This demo builds a classifier model using the
embeddings from the DICOM images and labels from the ChestX-ray14 dataset, and uses the CXR
Foundationtogenerateimageembeddingsandtrainasimpleperformbinaryclassification. Thedemo
can also accept DICOMs directly from the browser as input and can be found on GitHub.
5HealthAIDeveloperFoundations
Table2 | EstimatedcostperembeddingforthePathFoundationmodelifVertexAIembeddingendpointisexecutedat
capacity.Endpointestimatedtocost$3.12perhour.
External Data Source Google Cloud Storage DICOM Store
Embeddings per Hour 960,869 243,779 988,217
‚àºPrice ($) / Embedding $0.000003 $0.00001 $0.000003
‚àºEmbedding / $ 307,970 78,134 316,736
3.2. Path Foundation
Wehave evaluated PathFoundation for 11histopathology tasks vialinear probing, shown inFigure 3.
ROC AUC scores are above 0.8 in all but one task.
Figure3 | Performance,measuredinROCAUC,ofthePathFoundationon11histopathologyclassificationtasksvialinear
probing.ThePathFoundationembeddingsdemonstratesuperiorperformancecomparedtoImageNetfeaturesacrossall
tasks.
Besides performance, efficiency is also an important evaluation factor as whole slide digital
pathology images are some of the largest medical images (by size in bytes). Recognizing that the
efficiency,timeandtotalcost,ofembeddinggenerationisabarriertothedevelopmentanddeployment
ofpathologyML,wedevelopedaVertexAIendpointthatgeneratesembeddingsforpathologyimaging
stored both in the Cloud and externally.
Allperformancemeasuresreportedherewereperformedintriplicateandthemeantimerequired
to complete the task is reported. Measurements were done using a Vertex AI endpoint running on
n1-highmem-4 virtual machines with an attached NVIDIA Tesla V100 16GB . The endpoint was run
as a single node with horizontal scaling disabled. More details can be found at Section A.1.
Embedding generation from the DICOM store and externaldata were very similar and faster than
embeddinggenerationfromGoogleCloudStorage. ThecostperembeddingwasestimatedviaVertex
AI pricing at $3.12/hour. Table 2 illustrates estimates for the cost of embedding with the endpoint
operated at near capacity. Figure 4 shows the throughput (embeddings per second) for each of the
data sources.
6HealthAIDeveloperFoundations
Figure4 | PathFoundationpredictionperformancebasedondatasource.(maximumembeddings/second);errorbars
representSD.
3.3. Derm Foundation
To evaluate Derm Foundation, we trained and evaluated classifiers for ten tasks. We used the SCIN
(SkinConditionImageNetwork)Wardetal.(2024)datasettotrainandtesttheseclassifiers. SCINis
anopenaccessdatasetcreatedbygeneratingrepresentativeimagesfrominternetuserswiththegoal
of providing an accessible diverse dermatology dataset. The results are presented in Figure 5.
3.4. HeAR
HeARhasbeentrainedonalargecollectionofshort(2seconds,sampledat16kHz,mono-audio)clips
of sounds made by humans, such as coughing, breathing, or speaking. No segmentation or padding
was used. The audio typically contains background noise, which helps make the encoder robust to
such noise.
By reconstructing masked sounds, it learns meaningful representations that capture underlying
patterns useful for downstream tasks. This embedding model can be used to explore the possibility
of developing classifiers for health-related non-semantic acoustics. Examples of such tasks include
detecting tuberculosis using cough sounds, detecting dementia using non-semantic voice patterns,
and detecting air exhalation volume from exhalation sounds.
We observed in Baur et al. (2024) that HeAR tends to perform better than available competing
audio encoders for such tasks, while typically needing less data, as shown in Figure 6. Deployment
and test-time scenarios are typically very different from lab tests. For health acoustics, it typically
meansthattherecordingdevicesanddatacollectionprotocolscanbedifferent. Deeplearningmodels
can be brittle to such distribution shifts, and yet we observed that HeAR is more robust to such
changes.
7HealthAIDeveloperFoundations
Figure5 | ROCAUCcurvesofdataefficientclassifiersusingSCINdatasetforarepresentativesampleofdermatologytasks.
(top)showsresultsofalogisticregressionclassifierand(bottom)showsresultsusingasimple(two-layer)neuralnetwork.
Figure6 | ComparisonoftheperformanceofHeARtootheraudioencodersoncoughandspirometrytasks.They-axis
showstheROCAUCperformancewhilethex-axisshowsthepercentageoftrainingdataused.HeARperformsfavorably
acrossdifferentdataregimesandtasks,demonstratingitsdataefficiency.
8HealthAIDeveloperFoundations
3.5. CT Foundation
CT Foundation was trained using three-dimensional computed tomography (CT) scans and corre-
sponding radiology reports across different study types, including Head/Neck, Neck, Spine, Heart,
Angiography, Chest, Abdomen and Pelvis, and Extremities. To test CT Foundation‚Äôs utility and gener-
alizability, we evaluated the embeddings across seven classification tasks using multilayer perceptron
models with increasing amounts of training data. Tasks were diverse, spanning head, chest, and
abdominopelvic regions, each involving the detection of abnormalities. These tasks were related
to classifying intracranial hemorrhage, calcifications in the chest and heart, lung cancer prediction
in the chest, suspicious abdominal lesions, nephrolithiasis, and abdominal aortic aneurysm in ab-
dominopelvicCTs. Allbinarylabelswiththeexceptionofthelungcancerandhemorrhagetaskswere
automatically extracted from the clinical radiology reports, which were written by board certified
radiologists. The lung cancer prediction task was drawn from NLST (National Lung Screening Trial
Research Team, 2011) and used pathology-confirmed cancer outcomes within 2 years of the lung
screening task for cancer positive labels. The hemorrhage task was further verified and labeled
by board certified radiologists. The evaluation set to measure the performance was held constant.
Figure 7 shows the results in ROC AUC and accuracy for these tasks. All but one of the ROC AUC
measured tasks achieved a score of 0.8 or greater.
Figure7 | CTFoundationembeddingsperformance(ROC-AUCandaccuracy)acrosseachtask.Eachmodelwastrained
onlyusingCPUswithdifferentsizesoftrainingdata,andtheevaluationsubsetwasheldconstant.Nreferstothetotalsize
oftheevaluationsetusedtomeasureperformance. Throughouteachtrainingrunthepositiveratiointhetrainingset
approximatelymatchestheevaluationset.Thelungcancertrainingandevaluationsplitsmatchthoseofthetrainingand
tunesetsusedinArdilaetal.(2019).
4. Discussion
The ML models within HAI-DEF have been made available for research use over the last 3 years as
researchendpointswhereuserscanuploadimagesandreceiveembeddings. HAI-DEFnowexpandsthis
9HealthAIDeveloperFoundations
collection of research endpoints with open-weight models and containerized per-modality solutions
thatcanbedeployedasendpointsinuser-managedenvironmentsforCXR,PathandDermFoundation.
This unlocks use cases beyond research, and enables use on datasets that cannot be processed by a
Cloud system due to privacy, institutional policies, or other considerations.
Thesefoundationmodelresearchendpointshavebeenadoptedbymanyresearchers,withmillions
ofAPIcallsasofthiswriting. Users,includingbothmachinelearningresearchersandclinicians,have
utilized these research endpoints to explore a diverse range of applications and have found them
valuable for improving performance and building models quickly for research purposes. Use cases
advancedbyresearchersinclude:usingPathFoundationtohelpdistinguishdifferenttypesofsarcoma
at University College London; and using CXR Foundation for identifying necrotizing enterocolitis on
neonatal radiology images at Guys St. Thomas Trust.
Asweexpandtheroutesbywhichfoundationmodelscanbeconsumed,itisimportanttoconsider
various trade-offs. Using model weights directly is the most flexible way to consume the models; for
example,themodelscanbeleveragedaspartofexistingMLsoftwareinfrastructureorasaconstituent
model in an ensemble for real time use. However, this requires developers to process the data into
a format that models expect (which might differ between different models and modalities). In this
regard, consuming the models via endpoints that provide additional preprocessing logic may make
it easier to fetch and process data for inference. Deploying endpoints on cloud infrastructure (for
instance as a Google Cloud Vertex AI endpoint) also offers scalability without needing researchers or
infrastructure staff to manage the complexity of ensuring robustness to fluctuating usage volumes.
Data use restrictions are another factor that drives differences between the open-weights versus
research endpoint approaches. Google-maintained research endpoints can only be used for research
scenarios and with de-identified data. This may render it unsuitable for use cases with strict data
locality requirements. On the other hand, certain datasets have usage terms that prohibit direct
availability of the downstream trained models, or are associated with other sensitivities that might
make releasing model weights infeasible. The endpoint approach can respect these restrictions while
providinganalternativeroutetoenablingdownstreamusebyotherresearchers. Finally,anotherroute
combines some elements of both open-weights and endpoint approaches: user-deployed endpoints
(using the ready-to-deploy containers on Google Cloud‚Äôs Model Garden) have the scalability and
preprocessing benefits of research endpoints, in addition to meeting the data locality and sensitivity
requirements for some users.
With regards to downstream use cases, even though embedding models have been trained on
a large diverse datasets (a compute- and data-intensive process), making them useful for specific
tasks should always be done via validation and/or tuning on data specific to the problem and patient
population of interest. This fine-tuning can be especially important for rarer examples that the
embedding models may not have ‚Äòseen‚Äô many of during development. For many applications, we and
other researchers observe that using a foundation model can reduce the amount of data needed (i.e.,
inthelow-dataregime). Insomeinstances,weadditionallyfindbenefits(suchasgeneralization)ina
higher data regime. For example, we have found HeAR to display more robust performance when
testing on a new audio collection smartphone.
Reducing bias and promoting fairness and equity are important when using ML in healthcare
applications. Fine-tuning with local data specific to the use case, patient population, data acquisition
protocol or devices is likely essential to achieving higher performance. Embedding models, by
reducing barriers to entry can democratize use of AI in low resource settings. Crucially, Weng et al.
(2024) emphasizes that regardless of the model‚Äôs origin (a foundation model or otherwise), rigorous
evaluation of the final downstream model is necessary to guarantee fairness and mitigate potential
biases. We are also looking forward to community feedback and lessons learnt applying these models
in a diverse range of tasks. One interesting but challenging goal is to incorporate feedback or
additional datasets into the foundation models for improvements and further use by the community.
10HealthAIDeveloperFoundations
By establishing a positive feedback loop, these models could improve over time and become even
more useful for all users.
Like all ML models, models provided in HAI-DEF have limitations to consider. Though individual
detailsdiffer,eachmodelinHAI-DEFhasbeentrainedonvaryingamountsofdatabasedonavailability;
please refer to each associated manuscript for more details. As discussed above, we emphasize the
importance of rigorous evaluation per use case and population, to improve overall performance and
address issues of bias, fairness, and equity. Finally, while we strove to make the models useful for
a variety of use cases, some applications may need further development. For example, the models
were developed with a focus on classification tasks, and prognosis tasks will need to be further
evaluated. Image segmentation and generation tasks are also currently not supported. Further,
specific requirements such as smaller models (e.g. for on-device applications on a mobile device) or
lower latency will need other techniques such as distillation to the target model size of interest.
5. Future Work
We hope to expand and improve the HAI-DEF program to make it useful for an even wider range of
applications and to accelerate the adoption of AI in healthcare. One such example is the adoption
of open language models in applications working with health guidelines. We plan to release a
health guidelines toolkit containing code recipes to help developers evaluate model performance and
incorporate them into applications such as training and education.
We look forward to receiving feedback and success stories from the community, both in terms of
how existing models can be improved but also whether they can be expanded to support use cases
we may not have considered yet.
6. Acknowledgements
HAI-DEFistheresultofextensivecollaborativeeffortsinvolvingnumerousindividualsandinstitutions.
WeextendoursinceregratitudetotheNIHClinicalCenter,StanfordMedicalCenter,ProjectCoswara,
CoughVID, and the Center for Infectious Disease Research in Zambia for their collaboration and for
making their datasets available to the research community. We also thank DeepHealth and Apollo
Radiologyfortheirexpertiseanddatasets. Additionally,themodelsandresultsareinpartbasedupon
datageneratedbytheTCGAResearchNetworkandtheNationalLungScreeningTrial(NLST)National
Lung Screening Trial Research Team (2011).
WegratefullyacknowledgethecontributionsoftheGoogleResearchandDeepMindteams,partic-
ularly their work on software infrastructure and modeling. We further acknowledge Google Cloud
for making these research endpoints available to the research community, enabling broader access
and facilitating further advancements. We are indebted to all the clinicians who have contributed
their time and expertise in labeling data and evaluating these models. Their contributions have been
crucial to the development and validation of our work. Finally, the authors thank Jonathan Krause
and Dale Webster for their review and feedback.
11HealthAIDeveloperFoundations
References
DiegoArdila,AtillaP.Kiraly,SujeethBharadwaj,BokyungChoi,JoshuaJ.Reicher,LilyPeng,DanielTse,
Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich, and Shravya Shetty. End-to-end
lung cancer screening with three-dimensional deep learning on low-dose chest computed tomogra-
phy. Nature Medicine, 25(6):954‚Äì961, 2019. ISSN 1546-170X. doi: 10.1038/s41591-019-0447-x.
URL https://doi.org/10.1038/s41591-019-0447-x.
Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent,
Armand Joulin, Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient
learning. arXiv preprint arXiv:2204.07141, 2022. URL https://arxiv.org/abs/2204.07141.
Sebastien Baur, Zaid Nabulsi, Wei-Hung Weng, Jake Garrison, Louis Blankemeier, Sam Fishman,
Christina Chen, et al. Hear ‚Äì health acoustic representations. arXiv preprint arXiv:2403.02522,
2024. URL http://arxiv.org/abs/2403.02522.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, et al. An image is worth 16x16 words: Transformers for image
recognition at scale. arXiv preprint arXiv:2010.11929, 2020. URL http://arxiv.org/abs/
2010.11929.
BenjaminElizalde,SohamDeshmukh,MahmoudAlIsmail,andHuamingWang. Clap: Learningaudio
concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 1‚Äì5. IEEE, 2023.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked au-
toencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021. URL http:
//arxiv.org/abs/2111.06377.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020. URL http://arxiv.org/abs/2004.11362.
AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,JoanPuigcerver,JessicaYung,SylvainGelly,andNeil
Houlsby. Bigtransfer(bit): Generalvisualrepresentationlearning. arXivpreprintarXiv:1912.11370,
2019. URL http://arxiv.org/abs/1912.11370.
Jeremy Lai, Faruk Ahmed, Supriya Vijay, Tiam Jaroensri, Jessica Loo, Saurabh Vyawahare, Saloni
Agarwal, et al. Domain-specific optimization and diverse evaluation of self-supervised models for
histopathology. arXiv preprint arXiv:2310.13259, 2023. URL http://arxiv.org/abs/2310.
13259.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-
trainingforunifiedvision-languageunderstandingandgeneration.arXivpreprintarXiv:2201.12086,
2022. URL http://arxiv.org/abs/2201.12086.
Yuan Liu, Ayush Jain, Clara Eng, David H. Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme
deOliveiraMarinho,JessicaGallegos,SaraGabriele,VishakhaGupta,NaliniSingh,VivekNatarajan,
Rainer Hofmann-Wellenhof, Greg S. Corrado, Lily H. Peng, Dale R. Webster, Dennis Ai, Susan J.
Huang, Yun Liu, R. Carter Dunn, and David Coz. A deep learning system for differential diagnosis
of skin diseases. Nature Medicine, 26(6):900‚Äì908, jun 2020. ISSN 1546-170X. doi: 10.1038/
s41591-020-0842-3. URL https://doi.org/10.1038/s41591-020-0842-3.
National Cancer Institute (NCI). GDC Data Portal Homepage, 2024. URL https://portal.gdc.
cancer.gov/. Accessed October 4, 2024.
12HealthAIDeveloperFoundations
National Lung Screening Trial Research Team. The National Lung Screening Trial: overview and
study design. Radiology, 258(1):243‚Äì253, 2011. doi: 10.1148/radiol.10091808.
Jacob Peplinski, Joel Shor, Sachin Joglekar, Jake Garrison, and Shwetak Patel. Frill: A non-semantic
speech embedding for mobile devices. arXiv preprint arXiv:2011.04609, 2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, et al. Learning transferable visual models from natural language supervision. arXiv preprint
arXiv:2103.00020, 2021. URL http://arxiv.org/abs/2103.00020.
Rajeev V. Rikhye, Aaron Loh, Grace Eunhae Hong, Preeti Singh, Margaret Ann Smith, Vijaytha
Muralidharan, Doris Wong, et al. Closing the ai generalization gap by adjusting for dermatology
condition distribution differences across clinical settings. arXiv preprint arXiv:2402.15566, 2024.
URL http://arxiv.org/abs/2402.15566.
Andrew B. Sellergren, Christina Chen, Zaid Nabulsi, Yuanzhen Li, Aaron Maschinot, Aaron Sarna,
Jenny Huang, et al. Simplified transfer learning for chest radiography models using less data.
Radiology, 305(2):454‚Äì65, 2022.
Joel Shor, Aren Jansen, Ronnie Maor, Oran Lang, Omry Tuval, Felix de Chaumont Quitry, Marco
Tagliasacchi, Ira Shavitt, Dotan Emanuel, and Yinnon Haviv. Towards learning a universal non-
semantic representation of speech. arXiv preprint arXiv:2002.12764, 2020.
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. arXiv preprint arXiv:1707.02968, 2017. URL
http://arxiv.org/abs/1707.02968.
Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana,
Jay Hartford, Pradeep Kumar S, Tiya Tiyasirichokchai, Sunny Virmani, Renee Wong, Yossi Matias,
Greg S. Corrado, Dale R. Webster, Dawn Siegel, Steven Lin, Justin Ko, Alan Karthikesalingam,
Christopher Semturs, and Pooja Rao. Crowdsourcing dermatology images with google search ads:
Creating a real-world skin condition dataset, 2024.
Wei-Hung Weng, Andrew Sellergen, Atilla P Kiraly, Alexander D‚ÄôAmour, Jungyeon Park, Rory Pilgrim,
Stephen Pfohl, Charles Lau, Vivek Natarajan, Shekoofeh Azizi, Alan Karthikesalingam, Heather
Cole-Lewis,YossiMatias,GregSCorrado,DaleRWebster,ShravyaShetty,ShruthiPrabhakara,Krish
Eswaran, Leo A G Celi, and Yun Liu. An intentional approach to managing bias in general purpose
embedding models. The Lancet Digital Health, 6(2):e126‚Äìe130, February 2024. ISSN 2589-7500.
doi: 10.1016/S2589-7500(23)00227-3. URL https://doi.org/10.1016/S2589-7500(23)
00227-3.
QizheXie,Minh-ThangLuong,EduardHovy,andQuocV.Le. Self-trainingwithnoisystudentimproves
imagenet classification, 2020. URL https://arxiv.org/abs/1911.04252.
Shawn Xu, Lin Yang, Christopher Kelly, Marcin Sieniek, Timo Kohlberger, Martin Ma, Wei-Hung
Weng, Atilla P. Kiraly, Sahar Kazemzadeh, Zakkai Melamed, Jungyeon Park, Patricia Strachan, Yun
Liu, Chuck Lau, Preeti Singh, Christina Chen, Mozziyar Etemadi, Sreenivasa Raju Kalidindi, Yossi
Matias, Katherine Chou, Greg S. Corrado, Shravya Shetty, Daniel Tse, Shruthi Prabhakara, Daniel
Golden, Rory Pilgrim, Krish Eswaran, and Andrew Sellergren. Elixr: Towards a general purpose
x-rayartificialintelligencesystemthroughalignmentoflargelanguagemodelsandradiologyvision
encoders. arXivpreprintarXiv:2308.01317,2023. URLhttps://arxiv.org/abs/2308.01317.
Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu.
Videococa: Video-text modeling with zero-shot transfer from contrastive captioners, 2023. URL
https://arxiv.org/abs/2212.04979.
13HealthAIDeveloperFoundations
LinYang,ShawnXu,AndrewSellergren,TimoKohlberger,YuchenZhou,IraKtena,AtillaKiraly,Faruk
Ahmed, Farhad Hormozdiari, Tiam Jaroensri, et al. Advancing multimodal medical capabilities of
gemini. arXiv preprint arXiv:2405.03162, 2024.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models, 2022. URL https://arxiv.org/abs/
2205.01917.
Yu Zhang, Daniel S Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu,
Yanping Huang, Shibo Wang, et al. Bigssl: Exploring the frontier of large-scale semi-supervised
learning for automatic speech recognition. IEEE Journal of Selected Topics in Signal Processing, 16
(6):1519‚Äì1532, 2022.
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Con-
trastive learning of medical visual representations from paired images and text. arXiv preprint
arXiv:2010.00747, 2020. URL http://arxiv.org/abs/2010.00747.
14HealthAIDeveloperFoundations
A. Appendix
A.1. Background and Methodology For Path Foundation Evaluations
At its highest magnification, digital pathology images are gigapixel-sized, measuring in the tens to
hundreds of thousands of pixels per slide. The Path embeddings model has been trained to produce
high quality machine learning embeddings for image patches of size 224 √ó 224 pixels cropped
from pathology images. At high magnification, these image patches represent a relatively small
portionoftheslide,andenabletherepresentationofintra-slideheterogeneityindifferentpartsofthe
tissue specimen. However, computing patch embeddings across such high magnification pathology
images can require the generation of tens of thousands of embeddings for a single whole slide image
(Figure 8).
Figure 8 | Illustrationofpatchembeddingscomputedacrossahistopathologyimage. Areasoftheimagewithsame
coloringexhibitedsimilarùëò-meansclusteringfrompatchlevelembeddings.
The endpoint that was used for evaluations runs as a deployable online prediction REST service
withinVertexAIandcangenerateembeddingsfordatastoredbothwithinCloud(CloudDICOMstore
orGoogleCloudStorage)andfromexternaldatasources(i.e.,sendingthedataaspartoftherequest).
The REST service supports requests that may contain multiple patch embeddings per-data-source
and/or multiple data sources. As with all Vertex AI prediction services the service can be configured
to horizontally scale to meet demand. To generate embeddings, the endpoint will: 1) retrieve the
imaging necessary to complete the embedding generation, 2) generate the embedding, and 3) return
the embeddings to the caller. For Cloud data sources, it is optimal to co-locate the Vertex AI endpoint
hostingembeddinggenerationinthesamedatacenterthathoststheimagesinordertoimprovedata
transport efficiency.
When embeddings are generated from data not stored in Cloud, data are sent directly within the
request as a base64 encoded compressed image. This increases the amount of imaging data that can
fit within the request. The endpoint supports decoding commonly used compression methods (e.g.,
lossy JPEG, PNG for lossless data transmission, etc.). Regardless of exactly how imaging is encoded,
on receipt, the endpoint extracts the pixel data directly from the request and then generates and
returns the embedding result. Multiple factors affect the time required to generate embeddings and
the associated cost, including: the compute (CPU and GPU) that backs the Vertex AI embedding
generation service, the data source (e.g., DICOM store, Google Cloud Storage, or external data), the
image compression format, and the total size of the embedding request.
DICOM embeddings were generated from non-overlapping regions of a DICOM VL Whole Slide
Microscopy Image. The DICOM instance was encoded using: 256 √ó 256 pixel frames that were
encoded using JPEG baseline transfer syntax and TILED_FULL organization. Google Cloud Storage
embeddings were generated from JPEG images saved to Google Cloud Storage. Image dimensions
matched the embedding model patch dimensions (224 √ó 224 pixels) to enable stored images to be
15HealthAIDeveloperFoundations
Table3 | VertexAIendpointperformancemetricssubjectedto10concurrentrequestsfor5,000imageembeddings.
External Data Source Google Cloud Storage DICOM Store
CPU Utilization (%) 123 122 93
GPU Duty Factory (%) 27 7 25
used directly as input to the embedding model. For the external data, the images stored in Google
Cloud Storage were read in their entirety and used as the source images for the tests.
To estimate the total maximum capacity of an embedding endpoint for the various imaging data
sourcesitwasempiricallydeterminedthatperformancemetrics,CPUutilization,andGPUdutyfactor
plateauedwhentheembeddingendpointwassubjectedto10parallelrequestsfor5,000embeddings;
metrics shown in Table 3. The total time required to generate 50,000 embeddings by 10 parallel
processes was quantified. Total time was defined as the time between sending the first request from
the client to reception of embeddings.
16