Time is on my sight: scene graph filtering for dynamic
environment perception in an LLM-driven robotâ‹†
SimoneColombani1,3, LucaBrini2, DimitriOgnibene2 and GiuseppeBoccignone1
1UniversityofMilan,Italy
2UniversityofMilano-Bicocca,Milan,Italy
3OversonicRobotics,CarateBrianza,Italy
Abstract
Robotsareincreasinglybeingusedindynamicenvironmentslikeworkplaces,hospitals,andhomes.Asaresult,
interactionswithrobotsmustbesimpleandintuitive,withrobotsâ€™perceptionadaptingefficientlytohuman-
inducedchanges.
Thispaperpresentsarobotcontrolarchitecturethataddresseskeychallengesinhuman-robotinteraction,with
a particular focus on the dynamic creation and continuous update of the robotâ€™s state representation. The
architectureusesLargeLanguageModelstointegratediverseinformationsources,includingnaturallanguage
commands,roboticskillsrepresentation,real-timedynamicsemanticmappingoftheperceivedscene. This
enablesflexibleandadaptiveroboticbehaviorincomplex,dynamicenvironments.
Traditionalroboticsystemsoftenrelyonstatic,pre-programmedinstructionsandsettings,limitingtheiradapt-
abilitytodynamicenvironmentsandreal-timecollaboration.Incontrast,thisarchitectureusesLLMstointerpret
complex,high-levelinstructionsandgenerateactionableplansthatenhancehuman-robotcollaboration.
Atitscore,thesystemâ€™sPerceptionModulegeneratesandcontinuouslyupdatesasemanticscenegraphusing
RGB-Dsensordata,providingadetailedandstructuredrepresentationoftheenvironment.Aparticlefilteris
employedtoensureaccurateobjectlocalizationindynamic,real-worldsettings.
ThePlannerModuleleveragesthisup-to-datesemanticmaptobreakdownhigh-leveltasksintosub-tasksand
linkthemtoroboticskillssuchasnavigation,objectmanipulation(e.g.,PICKandPLACE),andmovement(e.g.,
GOTO).
By combining real-time perception, state tracking, and LLM-driven communication and task planning, the
architectureenhancesadaptability,taskefficiency,andhuman-robotcollaborationindynamicenvironments.
Keywords
Human-Robotinteraction,Robottaskplanning,Largelanguagemodels,Scenegraphs
1. Introduction
Immediacy is crucial in assistive robotics [1, 2, 3]. In a typical human-robot interaction scenario,
usersmayprovidecommandsinnaturallanguage,suchasâ€œPickthebluebottleonthetableandbring
it to meâ€. To such aim, the use of Large Language Models (LLM) allows robots to interpret natural
languagerequestsandâ€œtranslateâ€instructionsintoplanstoachievespecificgoals;yet,thesemodels
needtoknowtheenvironmentinwhichtheyoperatesotogenerateaccurateplans[4]. Theneedfor
translationarisesfromthecomplexityofhumanlanguageandthevariabilityininstructions. Usersmay
expresscommandsdifferentlyorexploitambiguoustermsthattherobotmustcomprehend. Toaddress
thesechallenges,roboticarchitecturesmustintegratenaturallanguageprocessingwithenvironmental
understanding.
The chief concern of the work is to exploit scene graphs as semantic maps providing a structured
representation of spatial and semantic information of robotâ€™s environment. This enables LLMs to
generateplansbasedonthisinformation. Indeed,viascenegraphsrobotscanmaptherelationships
betweenobjects,theirproperties,andtheirspatialarrangements.
Hereweaddresssuchlimitationsbyrepresentingtheenvironmentasagraphendowedwithupdatable
WorkshoponAdvancedAIMethodsandInterfacesforHuman-CenteredAssistiveandRehabilitationRobotics(aFit4MedRob
event)-AIxIA2024,November25â€“28,2024,Bolzano,Italy
$simone.colombani@studenti.unimi.it(S.Colombani);l.brini@campus.unimib.it(L.Brini);dimitri.ognibene@unimib.it
(D.Ognibene);giuseppe.boccignone@unimi.it(G.Boccignone)
Â©2024Copyrightforthispaperbyitsauthors.UsepermittedunderCreativeCommonsLicenseAttribution4.0International(CCBY4.0).
4202
voN
22
]OR.sc[
1v72051.1142:viXrasemanticsthatlanguagemodelscaninterpret. Moreprecisely,thedynamicsoftheupdateisachieved
viaparticlefilteringtoenhancethereliabilityandprecisionofreal-timesemanticmapping. Themodel
adopted(PSGTR)islightweightandcanbeeasilyutilized,makingitsuitableforliveapplicationsand
accessibleevenonlesspowerfulhardware. UsingRoBee,thecognitivehumanoidrobotdevelopedby
Oversonic Robotics, the system dynamically updates the environment graph and replans in case of
failure,overcomingchallengesinlong-termtaskplanning.
2. Related works
Ascenegraphcapturesdetailedscenesemanticsbyexplicitlymodelingobjects,theirattributes,andthe
relationshipsbetweenpairedobjects(e.g.,â€œbluebottleonthetableâ€)[5]. 3Dscenegraphs[6]extendthis
concepttothree-dimensionalspaces,representingenvironmentslikehousesoroffices,whereeachpiece
offurniture,room,andobjectisanode. Theedgesbetweenthesenodesdescribetheirrelationships,
suchasavaseonatableorachairinfrontof asofa.
Recent works, such as [7] and [8] have proposed to generate 3D scene graphs from RGB-D images,
combininggeometricandsemanticinformationtocreatedetailedenvironmentalrepresentations. Scene
graphshavebeenwidelyusedincomputervisionandroboticstoimprovesceneunderstanding,object
detection,andtaskplanning. Forexample,SayPlan[9]integrates3DscenegraphsandLLMsfortask
navigationandplanning,performingsemanticsearchesonthesceneandinstructionstocreateaccurate
plans,furtherrefinedthroughscenariosimulations. DELTA[10]utilizes3Dscenegraphstogenerate
PDDL files, employing multiple phases to prune irrelevant nodes and decompose long-term goals
intomanageablesub-goals,enhancingcomputationalefficiencyforexecutionwithclassicalplanners.
SayNav[11]constructsscenegraphsincrementallyfornavigationinnewenvironments,allowingthe
robottogeneratedynamicandappropriatenavigationplansinunexploredspacesbypassingthescene
graphtoaLLM,thusfacilitatingeffectivemovementandexecutionofuserrequests.
Inacrudesummary,themainlimitationsoftheabovementionedapproachestobuildenvironment
representations lie in their reliance on computationally heavy vision-language models (VLMs) and
computer vision models. Such models are not designed for precision and often demand significant
resources,whilelackingtheabilitytobeupdatedinrealtime,andthuslimitingtheirpracticalapplication.
3. Architecture
Oursystemisbasedontwocomponents:
â€¢ PerceptionModule: itisresponsibleforsensingandinterpretingtheenvironmentandbuilding
a semantic map in the form of a directed graph that integrates both geometric and semantic
information. Itsarchitectureisexplainedindetailbelow.
â€¢ PlannerModule: ittakestheinformationprovidedbythePerceptionModuletoformulateplans
andactionsthatallowtherobottoperformspecifictasks. Itiscomposedbythefollowing:
â€“ TaskPlanner: Translatesuserrequests,expressedinnaturallanguage,intohigh-levelskills.
â€“ SkillPlanner: Translateshigh-levelskillsintospecific,low-levelexecutableactions.
â€“ Executor: Executesthelow-levelactionsgeneratedbytheSkillPlanner.
â€“ Controller: Monitorstheexecutionofactionsandmanagesanyerrorsorunexpectedevents
duringtheprocess.
â€“ Explainer: Interpretsthereasonsofexecutionfailuresbyanalyzingdatareceivedfromthe
ControllerandprovidessuggestionstotheTaskPlanneronhowtoadjusttheplan.
Thesecomponentsinteracttoallowtherobottounderstanditsenvironmentandactaccordinglyto
satisfyuserrequests. InwhatfollowswespecificallyaddressthePerceptionModulewhiledetailson
theplannerwillbeprovidedinaseparatearticle.RobotHardware. ThesystemwasimplementedusingRoBee,thecognitivehumanoidrobotdeveloped
byOversonicRobotics. RoBee,showninFigure3.1,stands160cmtallandweighs60kg. Itfeatures32
degreesoffreedom,andisequippedwithcameras,microphones,andforcesensors.
3.1. Perceptionmodule
ThePerceptionModuleisthecomponentresponsibleforbuildingarepresentationoftheenvironment,
which the robot can use for task planning. The representation takes the form of a semantic map, a
graphthatintegratesbothgeometricandsemanticinformationabouttheenvironment. Togenerate
thesemanticmap,theperceptionmoduleusesdatafromvarioussensors. ItrequiresRGB-Dframes
obtainedfromthecamerawhicharethenprocessedusingascenegraphgenerationmodel,suchas
PSGTR[12]toextractobjectsmasks,labelandrelationships. Alsoitusesdataonthecameraposition
relativetothegeometricmaptodeterminethelocationoftheobjectsidentifiedbythemodel. More
formally,aSemanticMapisrepresentedasadirectedgraphğº = (ğ‘‰ ,ğ¸ )where:
ğ‘š ğ‘š ğ‘š
â€¢ Anodeğ‘£ âˆˆ ğ‘‰ canbeoneofthefollowingtypes:
ğ‘š
â€“ Room node: Defines the different semantic areas of the environment, such as â€œkitchen,â€
â€œliving room,â€ or â€œbedroom.â€ Each room node contains information about its geometric
boundariesandtheobjectnodesitcontains;
â€“ Object node: Represents physical objects in the environment, such as â€œtable,â€ â€œchair,â€ or
â€œbottle.â€ Eachobjectnodecontainsinformationaboutits3Dposition,semanticcategory,
dimensions,andotherrelevantproperties:
â€¢ Anedgeğ‘’ âˆˆ ğ¸ canrepresent:
ğ‘š
â€“ Therelationshipbetweentwoobjects;
â€“ Theconnectionbetweentworooms;
â€“ Thebelongingofanobjecttooneandonlyoneroom.
Thepresenceofroomnodesisimportantbecauseitfacilitatesthecategorizationofobjectsbasedon
theirrespectiverooms,whichhelpsdistinguishbetweenobjectswiththesamenameandenhances
thenaturallanguagedescriptionofthetask,whileroomnodesenabletheapplicationofgraphsearch
algorithmsforplanningpathstoobjects. Roomnodesarecreatedbasedonthegeometricmap,while
objectnodesaregeneratedfollowingthestepsexplainedbelow.
Astoedges,morespecifically:
â€¢ Edgesbetweenroomsdirectlyconnecttworoomsandfacilitatenavigationbetweenthem.
â€¢ Edgesbetweenobjectsrepresenttherelationshipsbetweenobjectsandaredirected,thedirection
capturingtheinfluenceofoneobjectonanother;thelabelassociatedwitheachedgeisderived
fromtheinferencesmadebythePSGTRmodel.
Figure3.1showsanexampleofasemanticmapofanoffice,builtwiththeroomnodeâ€™Officeâ€™(italian,
â€™Ufficioâ€™)andtheobjectnodesconnectedtoeachotherbyrelationshipsandlinkedtotheroomnode.
Generatingandupdatingthesemanticmap Thescenegraphgenerationprocessisbasedonthe
PSGTRmodel,asingle-stagemodelbuiltontheTransformerarchitecture[13]. Thismodelgeneratesa
graphrepresentationofascenegivenitspanopticsegmentation. PSGTRdoesnotachievethehighest
qualityinpanopticsegmentationcomparedtobettermodels,butitprovidesreasonableinferencetimes
forreal-timeapplications,takingabout400mstoprocessa480pimageonamachinewithaccesstoan
NVIDIAT4GPU.
ThePerceptionModuleusestheresultofPSTGRandbuildsthesemanticmapfollowingthestepsbelow:
1. ReadingRGB-Dframes: Thevideoframesfromtherobotâ€™scamerasaresenttothemodeltobe
analyzedandusedtogeneratethescenegraph.Figure1:Thefigureontheleftshowcasesanexampleofasemanticmapinanofficeenvironment,whilethe
imageontherightshowsRoBee,thehumanoidrobotdevelopedbyOversonicRobotics.
2. Reading robot poses: To generate the scene and semantic map, it is necessary to know the
robotâ€™spositionrelativetothegeometricmap,thecameraâ€™spositionrelativetothemap,andthe
cameraâ€™smountingpositionontherobot.
3. Inference:Eachreceivedframeisprocessedbythemodel. Resultsareinformationaboutdetected
objects,suchaslabelsandmasks,andtherelationshipsbetweenthem,suchasrelationshiplabels
andassociatedprobabilities.
4. Graphconstruction: Thisstepinvolvesextractingdatafromtheobjectreturnedbythemodel
andcomputingvaluesdependentontherobotsystem,suchasthepositionofobjects. Atafiner
levelitconsistsofthreesub-steps:
a) Node construction: Classes and masks of detected objects are extracted. Next, the 3D
positionofeachobjectiscomputed,startinginthepixelcoordinatesystem,thentransform-
ingtothecamerasystem,andfinallytotherobotâ€™smapcoordinatesystem. Nodesforthe
semanticsceneandthesemanticmapareinstantiatedusingtheappropriate3Dcoordinates.
Adistance-basedfilterisappliedtopruneobjectsthataretoofarfromtherobottoavoid
issueswithobjectdetectionandtracking.
b) Edge construction: Data about relationships between objects are extracted. For each
relationship,thesourceandtargetobjectindicesareidentified. Ifbothobjectsmeetdistance
constraintsandtherelationshipprobabilityexceedsadefinedthreshold,anedgeiscreated
betweenthecorrespondingnodes.
c) Inference improvement through Particle Filter (PF): As the modelâ€™s output is not
accurateregardingmaskinference,thisleadstoerrorsincalculatingtheobjectâ€™scentroid
forobtainingitspositionrelativetothemap. APFbasedonpreviousobservationsisapplied
toimprovetheaccuracyoftheresult.
Attheendoftheprocess,thesemanticmapisupdatedwiththenewinformation,andthesemantic
sceneisgeneratedandprovidedtotheplannermodule.
ThePFisusedtotracktheobjectmasksinreal-time,providedasoutputbythePSGTRmodel,andto
improvetheestimationoftheirpositioninspace. Duringtheupdateprocess,thefilterusesinformation
fromframesacquiredtorefinethepositionestimateoftheobjects. Thelastobjectmasksidentifiedby
thePSGTRmodelarecomparedwithpreviousonesusingtheIntersectionoverUnion(IoU)metrics
andbyapplyingthemotionmodel,whichcanbedefinedasatransformationofthecameraposition
relativetothemapbetweentwotimeinstances. Denotethetransformationmatricesdescribingthe
camerapositionattimeğ‘¡âˆ’1andatsubsequenttimeğ‘¡,T andT ,respectively;then,thechangein
ğ‘¡âˆ’1 ğ‘¡
positionandorientationcanbeexpressedbythetransformationmatrixâˆ†T = T Tâˆ’1 . Toassociate
ğ‘¡ ğ‘¡âˆ’1
objectsbetweensuccessiveframes,weuseanIoUmatrixcomputedoversegmentationmasks. Fortwo
masksğ´andğµ,IoUisdefinedasIoU(ğ´,ğµ) = |ğ´âˆ©ğµ|,where|ğ´âˆ©ğµ|representstheareaofintersection
|ğ´âˆªğµ|
betweenmasksğ´andğµ,and|ğ´âˆªğµ|representstheareaoftheirunion. TocomparesegmentationTable1
Comparisonofpositiondata
Property NoParticle Particle
Realposition[m] (0.67,0.10,0.95) (0.67,0.10,0.95)
Meanposition[m] (0.74,-0.08,0.93) (0.65,0.08,0.94)
Meanofabsoluteerror[m] (0.07,0.18,0.02) (0.02,0.02,0.01)
Errorstandarddeviation[m] (0.35,0.24,0.03) (0.17,0.12.0.02)
masksbetweentwosuccessiveframes,wedenotethesegmentationmaskattimeğ‘¡âˆ’1asğ‘€ andat
ğ‘¡âˆ’1
timeğ‘¡asğ‘€ . Thetransformationmatrixâˆ†Tisappliedtothepreviousmasktoobtainatransformed
ğ‘¡
maskğ‘€â€² suchthatğ‘€â€² = âˆ†TÂ·ğ‘€ . TheIntersectionoverUnion(IoU)isthencomputedbetween
ğ‘¡âˆ’1 ğ‘¡âˆ’1 ğ‘¡âˆ’1
thetransformedmaskğ‘€â€² andthecurrentmaskğ‘€ asfollows: IoU(ğ‘€â€² ,ğ‘€ ) = |ğ‘€ ğ‘¡â€² âˆ’1âˆ©ğ‘€ğ‘¡|. This
ğ‘¡âˆ’1 ğ‘¡ ğ‘¡âˆ’1 ğ‘¡ |ğ‘€ ğ‘¡â€² âˆ’1âˆªğ‘€ğ‘¡|
allowsustoidentifythesameobjectacrosssuccessiveframesbasedontheirmasks.
Moreformally,eachobjectisrepresentedbyasetofğ‘ particles,whereeachparticleğ‘ ğ‘¡ attimeğ‘¡isa
ğ‘–
3Dvectorrepresentingahypothesisabouttheobjectâ€™sposition: ğ‘ ğ‘¡ = [ğ‘¥ ,ğ‘¦ ,ğ‘§ ]ğ‘‡,whereğ‘– = 1,...,ğ‘.
ğ‘– ğ‘– ğ‘– ğ‘–
The particles are initialized with a normal distribution around the initially observed position ğœ‡ =
0
[ğ‘¥ ,ğ‘¦ ,ğ‘§ ]ğ‘‡: ğ‘ 0 âˆ¼ ğ’©(ğœ‡ ,Î£ ), where Î£ = diag(ğœ2,ğœ2,ğœ2) is the initial covariance matrix. Initial
0 0 0 ğ‘– 0 0 0 ğ‘¥ ğ‘¦ ğ‘§
weightsareuniform: ğ‘¤0 = 1,whereğ‘– = 1,...,ğ‘. Predictiontakesintoaccountthecameramotion.
ğ‘– ğ‘
If ğ‘‡ is the transformation matrix from frame ğ‘¡âˆ’1 to frame ğ‘¡, each particle is updated as ğ‘ ğ‘¡ =
ğ‘¡âˆ’1,ğ‘¡ ğ‘–
ğ‘‡ Â·ğ‘ ğ‘¡âˆ’1+ğ‘ 0,whereğ‘ 0representsthenoiseaddedtoaccountforuncertaintiesinmotion,maintaining
ğ‘¡âˆ’1,ğ‘¡ ğ‘– ğ‘– ğ‘–
thesamedistributionstructureusedforinitialparticleinitialization. Givenanewobservationğ‘  ,the
ğ‘›ğ‘’ğ‘¤
particleweightsareupdatedbasedontheEuclideandistancebetweenthepredictedpositionandthe
observedone: ğ‘‘ğ‘¡ = â€–ğ‘ ğ‘¡ âˆ’ğ‘  â€– andğ‘¤ğ‘¡ = 1 . Weightsarethennormalized: ğ‘¤ğ‘¡ = ğ‘¤ ğ‘–ğ‘¡ . The
ğ‘– ğ‘– ğ‘›ğ‘’ğ‘¤ 2 ğ‘– 1+ğ‘‘ğ‘¡ ğ‘– âˆ‘ï¸€ğ‘ ğ‘¤ğ‘¡
ğ‘– ğ‘—=1 ğ‘—
finalpositionoftheobjectË†ğ‘  isestimatedastheweightedmeanofalltheparticles: Ë†ğ‘  = âˆ‘ï¸€ğ‘ ğ‘¤ğ‘¡ğ‘ ğ‘¡.
ğ‘¡ ğ‘¡ ğ‘–=1 ğ‘– ğ‘–
Table1showstheimprovementobtainedover30measurementsusingparticlefilter.
Theoverallprocessforupdatingthesemanticmapusingtheparticlefiltercanbesummarizedbythe
algorithm1.
Algorithm1SemanticMapupdateusingParticleFilter
1: foreachframeğ‘¡do
2: foreachobjectğ‘˜do
3: Applytransformation:ğ‘€â€² =Î”TÂ·ğ‘€ â—Transformpreviousmasks
ğ‘¡âˆ’1 ğ‘¡âˆ’1
4: endfor
5: ComputeIoU(ğ‘€â€² ,ğ‘€ )= |ğ‘€ğ‘¡â€² âˆ’1âˆ©ğ‘€ğ‘¡| â—ComputeIoUbetweennodesandinferenceresults
ğ‘¡âˆ’1 ğ‘¡ |ğ‘€ ğ‘¡â€² âˆ’1âˆªğ‘€ğ‘¡|
6: foreachobjectğ‘˜do
7: ifIoU>ğœ†IoUthen
8: Updateweights:ğ‘‘ğ‘¡
ğ‘–
=â€–ğ‘ ğ‘¡ ğ‘–âˆ’ğ‘ newâ€– 2,ğ‘¤ ğ‘–ğ‘¡ = 1+1
ğ‘‘ğ‘¡
ğ‘–
9: Normalize:ğ‘¤ğ‘¡ = ğ‘¤ğ‘–ğ‘¡
ğ‘– âˆ‘ï¸€ğ‘ ğ‘¤ğ‘¡
ğ‘—=1 ğ‘—
10: Estimate:^ğ‘  =âˆ‘ï¸€ğ‘ ğ‘¤ğ‘¡ğ‘ ğ‘¡
ğ‘¡ ğ‘–=1 ğ‘– ğ‘–
11: endif
12: endfor
13: foreachunmatchedobservationdo
14: Initnewobject:ğ‘ 0 âˆ¼ğ’©(ğœ‡ ,Î£ )
ğ‘– 0 0
15: endfor
16: Updatesemanticmapwith^ğ‘ 
ğ‘¡
17: endfor
4. Conclusions
Scenegraphsprovideastructuredrepresentationthatcapturesgeometricandsemanticinformation
abouttheenvironment. Thiscomprehensiveunderstandingenablesimprovedtaskplanningwithlargelanguagemodels,allowingrobotstoexecutecommands.
Inthisarticlewehaveshownhowtousereal-timesensordatatodynamicallyupdatesemanticmaps,
thusenablingtherobottoadapttoongoingchangesintheirenvironment,particularlyincollaborative
settings influenced by human actions. Here, particle filtering is applied to improve geometric data
precisionandsemanticmapaccuracy. Thiscanbeparticularlyimportantalsoforsocialinteractionand
intentionprediction[14,15]otherthanphysicalinteractionwiththeenvironment.
Theissuesaddressedinthisworkarecogent. Indeed, theeffectivenessofplannersintranslating
complexinstructionsintoactionableplansreliesonarobuststaterepresentation. Withoutanaccurate
semanticmap,plannersriskgeneratingplansthatmisalignwiththeactualenvironment,potentially
leadingtotaskfailures. Theintegrationofsemanticandgeometricinsightspermitsrobotstoreason
abouttheirenvironmentinamoreinformedandadaptiveway,ensuringthattheycanoperateeffectively
andresponsivelyindynamicenvironments.
TheadoptionofasemanticmapcontainingrichspatialinformationcombinedwithaflexibleLLM
basedplannercaneasilyallowtoexploreinthefuturetheintroductionofnewspatialrelationships,e.g.
wrapped,stuckunder,surrounding,aligned,thatcouldsupportspecificnovelrobotskills[16].
Acknowledgments
SpecialthankstoOversonicRoboticsforenablingtheimplementationofthesystemusingtheirhu-
manoidrobot,RoBee.
References
[1] C. Di Napoli, G. Ercolano, S. Rossi, Personalized home-care support for the elderly: a field
experiencewithasocialrobotathome, UserModelingandUser-AdaptedInteraction33(2023)
405â€“440.
[2] L.Lucignano,F.Cutugno,S.Rossi,A.Finzi, Adialoguesystemformultimodalhuman-robotinter-
action, in: Proceedingsofthe15thACMonInternationalconferenceonmultimodalinteraction,
2013,pp.197â€“204.
[3] D.Ognibene,L.Mirante,L.Marchegiani, Proactiveintentionrecognitionforjointhuman-robot
search and rescue missions through monte-carlo planning in pomdp environments, in: Social
Robotics: 11thInternationalConference,ICSR2019,Madrid,Spain,November26â€“29,2019,Pro-
ceedings11,Springer,2019,pp.332â€“343.
[4] C.Galindo,J.-A.FernÃ¡ndez-Madrigal,J.GonzÃ¡lez,A.Saffiotti, Robottaskplanningusingsemantic
maps, Roboticsandautonomoussystems56(2008)955â€“966.
[5] G.Zhu, L.Zhang, Y.Jiang, Y.Dang, H.Hou, P.Shen, M.Feng, X.Zhao, Q.Miao, S.A.A.Shah,
etal., Scenegraphgeneration: Acomprehensivesurvey, arXive-prints(2022)arXivâ€“2201.
[6] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, S. Savarese, 3d scene graph: A
structureforunifiedsemantics,3dspace,andcamera,in:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,2019,pp.5664â€“5673.
[7] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera, W. Paul,
K.Ellis,R.Chellappa,etal., Conceptgraphs: Open-vocabulary3dscenegraphsforperceptionand
planning, in: 2024IEEEInternationalConferenceonRoboticsandAutomation(ICRA),IEEE,2024,
pp.5021â€“5028.
[8] H.Chang,K.Boyalakuntla,S.Lu,S.Cai,E.Jing,S.Keskar,S.Geng,A.Abbas,L.Zhou,K.Bekris,
etal., Context-awareentitygroundingwithopen-vocabulary3dscenegraphs, arXivpreprint
arXiv:2309.15940(2023).
[9] K.Rana,J.Haviland,S.Garg,J.Abou-Chakra,I.Reid,N.Suenderhauf, Sayplan: Groundinglarge
languagemodelsusing3dscenegraphsforscalablerobottaskplanning, in:7thAnnualConference
onRobotLearning,2023.[10] Y.Liu,L.Palmieri,S.Koch,I.Georgievski,M.Aiello, Delta: Decomposedefficientlong-termrobot
taskplanningusinglargelanguagemodels, arXive-prints(2024)arXivâ€“2404.
[11] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Saynav: Grounding large lan-
guagemodelsfordynamicplanningtonavigationinnewenvironments, in: Proceedingsofthe
InternationalConferenceonAutomatedPlanningandScheduling,volume34,2024,pp.464â€“474.
[12] J. Yang, Y. Z. Ang, Z. Guo, K. Zhou, W. Zhang, Z. Liu, Panoptic scene graph generation, in:
EuropeanConferenceonComputerVision,Springer,2022,pp.178â€“196.
[13] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,I.Polosukhin, At-
tentionisallyouneed, in:Proceedingsofthe31stInternationalConferenceonNeuralInformation
ProcessingSystems,NIPSâ€™17,CurranAssociatesInc.,RedHook,NY,USA,2017,p.6000â€“6010.
[14] D.Ognibene,E.Chinellato,M.Sarabia,Y.Demiris, Contextualactionrecognitionandtargetlocal-
izationwithanactiveallocationofattentiononahumanoidrobot, Bioinspiration&biomimetics
8(2013)035002.
[15] S.Rossi,M.Staffa,L.Bove,R.Capasso,G.Ercolano, Userâ€™spersonalityandactivityinfluenceon
hricomfortabledistances, in: SocialRobotics: 9thInternationalConference,ICSR2017,Tsukuba,
Japan,November22-24,2017,Proceedings9,Springer,2017,pp.167â€“177.
[16] D.Marocco,A.Cangelosi,K.Fischer,T.Belpaeme, Groundingactionwordsinthesensorimotor
interaction with the world: experiments with a simulated icub humanoid robot, Frontiers in
neurorobotics4(2010)1308.
5. Online Resources
MoreinformationaboutRoBeeandOversonicRoboticsareavailable:
â€¢ RoBee,
â€¢ OversonicRobotics