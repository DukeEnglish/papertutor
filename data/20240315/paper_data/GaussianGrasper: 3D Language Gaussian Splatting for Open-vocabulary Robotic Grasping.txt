GaussianGrasper: 3D Language Gaussian Splatting for
Open-vocabulary Robotic Grasping
Yuhang Zheng, Xiangyu Chen, Yupeng Zheng, Songen Gu, Runyi Yang, Bu Jin, Pengfei Li, Chengliang Zhong,
Zengmao Wang, Lina Liu, Chao Yang, Dawei Wang, Zhen Chen, Xiaoxiao Long‚àó, Meiqing Wang‚àó
Abstract‚ÄîConstructing a 3D scene capable of accommodating
open-ended language queries, is a pivotal pursuit, particularly
within the domain of robotics. Such technology facilitates robots
in executing object manipulations based on human language
directives. To tackle this challenge, some research efforts have
beendedicatedtothedevelopmentoflanguage-embeddedimplicit
View 1
fields. However, implicit fields (e.g. NeRF) encounter limitations Language query: Hamburger
due to the necessity of processing a large number of input views
for reconstruction, coupled with their inherent inefficiencies in
inference.Thus,wepresenttheGaussianGrasper,whichutilizes3D
GaussianSplattingtoexplicitlyrepresentthesceneasacollection
ofGaussianprimitives.OurapproachtakesalimitedsetofRGB-
View 2
D views and employs a tile-based splatting technique to create
2D Feature LERF Ours
a feature field. In particular, we propose an Efficient Feature
Distillation (EFD) module that employs contrastive learning to 3D
efficiently and accurately distill language embeddings derived Localization Ours
fromfoundationalmodels.Withthereconstructedgeometryofthe
Gaussianfield,ourmethodenablesthepre-trainedgraspingmodel
LERF
to generate collision-free grasp pose candidates. Furthermore, we
propose a normal-guided grasp module to select the best grasp
pose. Through comprehensive real-world experiments, we demon-
2D Feature
strate that GaussianGrasper enables robots to accurately query Fusion
and grasp objects with language instructions, providing a new
solution for language-guided manipulation tasks. Data and codes
canbeavailableathttps://github.com/MrSecant/GaussianGrasper. Fig.1. Wepresentacomparisonbetweenourmethod,2Dfeaturefusion,and
Index Terms‚ÄîLanguage-guided Robotic Manipulation, 3D LERF.Whengiventhelanguagequery"hamburger",thefeaturesextracted
bythe2Dfoundationmodelsexhibitinconsistenciesbetweentwoviewpoints,
Gaussian Splatting, Language Feature Field
andLERFlacksclearsegmentationboundaries.Consequently,theybothsuffer
from imprecise 3D localization, as depicted by the yellow and purple 3D
I. INTRODUCTION
boundingboxes.Incontrast,ourmethodreconstructsaconsistentfeaturefield
Recently, there has been an increasing scholarly focus on andachievesmoreprecise3Dlocalization.
language-guided robotic manipulation due to its vast potential
to afford language-guided manipulation. Most existing works
in facilitating human-robot interaction, enabling robotic home
are based on 2D images [1], [2], [3], [4] which are efficient
services, and enhancing flexible manufacturing. Imagine that
but have limitations for robotic manipulation as robots can not
a robot is asked to pick up the water cup in a cluttered,
easily infer visual occlusion and spatial relation from multi-
unstructured environment, it needs to (1) locate the water
view misaligned images.
cup via responding to language description; (2) be aware
To obtain precise 3D positions for robotic manipulation,
of the geometry to execute a stable grasp. In this process,
recent works have focused on 3D representations. A straight-
understanding the diverse objects with different shapes and
forward approach is leveraging 2D visual models [5], [6], [7]
material properties in the open world is the pivotal challenge.
to extract semantics and then fuse the 2D semantics into 3D
Although many works have proposed various solutions,
points or volume. However, this fusion strategy suffers from
existing capabilities of scene understanding are insufficient
semantic inconsistency in 3D, as the semantics provided by
Yuhang Zheng is with the School of Mechanical Engineering and Au- the visual model are not consistent across multi-views. Other
tomation,BeihangUniversityandEncoSmart.XiangyuChenandZhenChen methods [8], [9], [10], [11], [12], [13] that use 3D backbone
are with the EncoSmart. Yupeng Zheng and Bu Jin are with the Institute
to extract features and are supervised by 3D annotation or
ofAutomation,ChineseAcademyofSciences.SongenGu,PengfeiLiand
ChengliangZhongarewiththeAIR,TsinghuaUniversity.RunyiYangiswith manipulation feedback can effectively make robots explicitly
theImperialCollegeLondon.LinaLiuiswiththeChinaMobileResearch understand 3D scenes and learn skills but are difficult to apply
Institute. Zengmao Wang is with the School of Computer Science, Wuhan
to the real world due to the challenges in data acquisition
University.ChaoYangiswiththeShanghaiAILaboratory.DaweiWangand
XiaoxiaoLongarewiththeDepartmentofComputerScience,theUniversity and annotation. Recently, distilled feature fields (DFFs) [14],
ofHongKong.MeiqingWangiswiththeSchoolofMechanicalEngineering [15], [16] which reconstruct 3D feature fields from 2D images
andAutomation,BeihangUniversity.
via implicit representation were introduced. Based on 2D-to-
‚àó XiaoxiaoLongandMeiqingWangarethecorrespondingauthors.
Email:xxlong@connect.hku.hk,wangmq@buaa.edu.cn 3D distillation, recent works [17], [18] have made impressive
4202
raM
41
]OR.sc[
1v73690.3042:viXraprogress in improving 3D scene understanding and enabling methods treated the grasping task as 2D pose detection [20],
robots to interact with the physical world according to natural [21], [22], [23], [24]. They define the grasp pose as a fixed-
language. However, DFFs can not be inflexibly applied for height oriented rectangle and predict the orientation and the
robotics manipulation as most of these methods suffer from width of the rectangle. However, their predicted grasp poses
(1) imprecise localization as these methods extract patch-level are limited to 3-DoF due to the lack of 3D geometry. To
features, resulting in ambiguous boundaries; (2) high costs of allow robots to plan higher dexterous grasps, extensive works
collecting dense training views (e.g. 50 views in F3RM [17]); focuses on 6-DoF grasping which use depth to augment the
(3) slow inference speed, hindering robots from responding grasp pose detection [25], [26] or leverage point cloud as input
to language instructions in time and (4) weak ability to cope to provide local geometry [27], [28], [29], [30], [31]. These
with scene changes caused by manipulation. methods exhibit high success rate when the depth is accurate
To tackle problems, we introduce GaussianGrasper, an but suffering from performance drops when encountering
open-worldroboticmanipulationsystembasedon3DGaussian photometrically challenging objects such as transparent objects.
Splatting (3DGS) [19], which models the 3D scene as a To further improve the performance, some work fuses RGB
set of 3D Gaussian primitives. Our insight is that we (1) with depth [32], [33], [34] as RGB can alleviate the depth-
reconstruct a 3D feature field via efficient feature distillation missing problem.
to support language-guided localization; (2) render depth and In this paper, we use the RGB-D based method to generate
surface normal to provide detailed local geometry, enabling graspposes.Toachievestablegrasp,wefurtherexplicitlyutilize
the generation of feasible grasping poses; (3) operate Gaussian Force-closuretheory[35]toenhancegraspposedetectionwhere
primitives and fine-tune 3DGS to update the changed scene. estimated normal is used to filter out unfeasible grasp poses
More specifically, our method enables language-guided (more details in Normal-guided grasp of Sec. III-C3).
manipulation via the following steps: (1) Initialization: we
B. Reconstructing 3D Feature Field For Manipulation
scan RGB-D images of a few viewpoints to initialize the
3DGS, reducing the cost of data collection. (2) Feature field A number of recent work integrate 2D foundation mod-
reconstruction: we propose an efficient feature distillation els with 3D feature fields in contexts other than robotic
(EFD) module that employs SAM [5] and CLIP [6] to extract manipulation [36], [14], [37], [38], [16], [39]. Based on
dense and shape-aware 2D descriptors and leverage contrastive the implicit representation, methods [14], [39], [16] leverage
learning to efficiently optimize the distilled features. (3) featuredistillationvianeuralrenderingtoreconstruct3Dfeature
Localization and grasp: we use open-vocabulary queries to field.Whileasexplicitrepresentations,methods[36],[37],[38]
locate the target object and use a pre-trained grasping module re-project 2D features and optimize the feature field in 3D.
to provide grasp poses where rendered normal is used to filter For robot manipulation, recent works like F3RM [17] and
out unfeasible proposals based on Force-closure theory. (4) LERF-TOGO [18] leverage feature distillation to improve the
Scene updating: After executing manipulation, we update the robot understanding of 3D scenes and enable language-guided
scene by operating corresponding Gaussian primitives and grasping. However, these methods need images from many
fine-tuning 3DGS with images from fewer views. viewpoints as input and can not quickly update the scene to
In summary, the contributions of this paper are as follows: cope with changes (movement or rotation) of objects. As an
explicit representation, SparseDFF [40] re-projects 2D features
‚Ä¢ We introduce GaussianGrasper, a robot manipulation
to 3D and leverages fusion strategy to optimize the feature
system implemented by a 3D Gaussian field endowed
field, leading to a significant reduction in usage of the number
with open-vocabulary semantics and accurate geometry
of viewpoints. However, explicit representation is not efficient
that is capable of rapid updates to support open-world
for carrying high-dimension language features due to memory
manipulation tasks guided by language.
usage and computation time.
‚Ä¢ We propose EFD that leverages contrastive learning to
Different from the methods above, we propose an efficient
efficiently distill CLIP features and augment feature fields
feature distillation module based on explicit 3D Gaussians
with SAM segmentation prior, addressing computational
representation which uses segmentation priors provided by
expense and boundary ambiguity challenges.
SAM to speed up feature field reconstruction and reduce the
‚Ä¢ We propose a normal-guided grasp module that uses
usage of memory (more details in Sec. III-B).
rendered normal to filter out unfeasible grasp poses.
‚Ä¢ We demonstrate the system‚Äôs zero-shot generalization
III. METHODOLOGY
capability for manipulation tasks in multiple real-world
In general terms, our method aims to pick up objects or
household tabletop scenes and common objects.
place objects in specified locations according to the language
II. RELATEDWORK instructions. The pipeline is shown in Fig. 2 (a) where our
method (1) collects multi-view RGB-D images as input to
A. Grasp Pose Detection
initialize3DGaussianfield;(2)reconstructs3Dfeaturefieldvia
Grasp pose detection is the pivotal part of robot grasping, efficientfeaturedistillationmoduleand(3)achieveslanguaged-
which plays a critical role in enabling the robot to interact guided manipulation. Specifically, we first introduce how to
with objects in the physical world. Previous 3-DoF Grasping initialize 3DGS and the differentiable rasterizer of 3DGS in1. Scan Scene and Initialize 3D Gaussian Field 2. Reconstruct 3D Gaussian Field 3. Languaged-guided Manipulation
Multi-view RGB-D Query
EFD: Efficient Feature Distillation
Pick up the hamburger
Locate
Re-projected Point Cloud
3D Localization
Feature Field (PCA shown) Grasping model
Reconstruct Generate Grasp
Pose Candidates
Normal-guided Grasp
Initialize
Filter Execute
Depth Field Normal Field Grasp Poses Grasping
3D Gaussian Field
(a) Our Proposed Pipeline
Render
Contrastive Loss
n1 n1
Œ±1
MLP
Œ±1 ùú∂ùú∂ùüèùüè+ùú∂ùú∂ùüêùüê‚â§ùúΩùúΩùíïùíïùíïùíïùíïùíïùíïùíï ùú∂ùú∂ùüèùüè+ùú∂ùú∂ùüêùüê>ùúΩùúΩùíïùíïùíïùíïùíïùíïùíïùíï
‚àö Feasible Grasp Pose √óUnfeasible Grasp Pose
Latent Feature Recovered Feature
Œ±2
Sample PixelPairs
ùêøùêø
Sample Pixels
n2
n2
Œ±2
Contact points
Distillation Loss Grasping line
SAM CLIP
Estimated Normal
Image Segmentation Map CLIP Feature
(b) EFD: Efficient Feature Distillation ùêπùêπ (c) Normal-guided Grasp
Fig.2. Thearchitectureofourproposedmethod.(a)isourproposedpipelinewherewescanmulti-viewRGBDimagesforinitializationandreconstruct3D
Gaussianfieldviafeaturedistillationandgeometryreconstruction.Subsequently,givenalanguageinstruction,welocatethetargetobjectviaopen-vocabulary
querying.Graspposecandidatesforgraspingthetargetobjectarethengeneratedbyapre-trainedgraspingmodel.Finally,anormal-guidedmodulethatuses
surfacenormaltofilteroutunfeasiblecandidatesisproposedtoselectthebestgrasppose.(b)elaboratesonEFDwhereweleveragecontrastivelearningto
constrainrenderedlatentfeatureLandonlysampleafewpixelstorecoverfeaturestotheCLIPspaceviaanMLP.Then,therecoveredfeaturesareusedto
calculatedistillationlosswiththeCLIPfeatures.(c)showsthenormal-guidedgraspthatutilizesForce-closuretheorytofilteroutunfeasiblegraspposes.
section III-A. Next, we elaborate on the EFD module in 2) Differentiable Rasterizer for 3D Gaussians: 3DGS ren-
section III-B. Finally, we introduce how to achieve language- ders the Gaussian primitives into images in a differentiable
guided manipulation in detail in section III-C, including mannertooptimizetheparameters.Givenasetof3DGaussian
locating objects through language queries, using a pre-trained primitives G = {g | i = 1,2,3,..,n}, each 3D Gaussian
i
grasping model to generate grasp poses, proposing a normal- primitive g is first projected onto the corresponding 2D plane
i
guided grasp strategy to select feasible poses and updating the and is then sorted based on its depth d from the viewpoint
i
scene after manipulation. plane. By tile-based rasterization, we sum up the pixel color
C(u) after sorting each primitive:
A. Preliminaries: 3D Gaussian Splatting
1) Gaussian Primitive Initialization: 3DGS uses Structure i‚àí1
(cid:88) (cid:89)
from Motion (SfM) [41] as the initialization part, which takes C(u)= c iŒ± i (1‚àíŒ± j) (1)
acollectionofRGBimagesasinputandoutputsasparsepoint i j=1
cloud.ThesepointsconstructasetofGaussianprimitives,each
where c is a feature vector represented by spherical harmonics
definedbymean¬µanda3DcovariancematrixŒ£=RSSTRT, i
(SH) and Œ± is obtained by multiplying Gaussian weight with
i
where R and S represent the rotation matrix and the scaling
opacity Œ± associated to Gaussian primitives.
matrix.Toreducetheviewpointsandspeedupinitialization,we
employ multi-view depth to re-project pixels of RGB images
B. Efficient Feature Distillation
into the world coordinate to initialize Gaussian field. Each
point is the center of a Gaussian primitive and the rotation We reconstruct 3D open-vocabulary feature field via extract-
matrix and scaling matrix are initialized randomly. ing dense CLIP features and efficiently distilling them into 3D,based on 3DGS. The open-vocabulary reconstruction enables where u and v are pixels in the ith sampled pair. As the
i i
the scene to respond to natural language instructions. contrastive loss homogenizes the features within each mask,
1) Instance-level Segmentation Prior and Open-vocabulary we only need to recover latent features of per mask to the
Features: To exact dense and shape-aware open-vocabulary CLIP space and subsequently minimize the distance between
features, we first use SAM to produce a set of instance-level the recovered feature and the CLIP feature. In practice, we
masks. Then, we leverage CLIP to obtain open-vocabulary randomly sample the same number of pixels within each mask,
features for each mask. Concretely, we process the input whoselatentfeaturesarethenrecoveredviaatrainabledecoder
images through SAM to obtain a set of mask proposals Œ® composed of two fully connected layers. We calculate the
and corresponding scores. Based on these scores, a non- distillation loss between the recovered feature and the CLIP
maximum suppression strategy [42] is then implemented to feature of all sampled k pixels, defined as:
filter superfluous masks. The resultant filtered set of masks
k
constitutes a segmentation map of the image, representing 1 (cid:88)
L =1‚àí Œ®(L(i))¬∑F(i) (4)
instance-level priors. After filtration, we process each valid distill k
i=1
mask-alignedimageregionintotheCLIPmodeltoextractopen-
By enhancing the low-dimension open-vocabulary feature
vocabulary features. Finally, we incorporate CLIP features of
embedding of 3D Gaussian and using contrastive learning
all masks into a feature map, referred to as F.
which leverages the segmentation prior derived from SAM,
2) Open-vocabulary Feature Distillation: We propose a
our method provides a powerful and efficient solution for
novel and efficient open-vocabulary feature distillation method
reconstructing 3D open-vocabulary representation.
thatstartswithenhancingeach3DGaussianprimitivewithem-
bedded open-vocabulary feature. As an explicit representation, C. Language-guided Robotic Manipulation
a 3D Gaussian field can be composed of millions of primitives.
We use the reconstructed feature field to conduct robotic
Directly incorporating high-dimensional CLIP features (over
manipulation. Given a language instruction, our method begins
500 dimensions) into all primitives will result in unacceptable
with employing open-vocabulary queries to locate the target
memory costs and computation time. Therefore, we compress
object. Subsequently, we render the depth and normals of 3D
the embedded open-vocabulary feature of Gaussian primitives
Gaussian primitives to obtain the object‚Äôs detailed geometry.
fromhigh-dimensionCLIPspacetolow-dimensionlatentspace.
Then, a point cloud-based grasping module is used to generate
After initializing 3D Gaussian primitives with low-dimension
grasp poses and the rendered normal is used to filter out
latent feature l, we employ a feature rasterizer to render the
unfeasible ones. After manipulating objects, we quickly update
feature map L:
the scene using observations from fewer viewpoints.
i‚àí1 1) Open-vocabularyQuerying: Asourreconstructedfeature
(cid:88) (cid:89)
L(u)= l iŒ± i (1‚àíŒ± j) (2) field is aligned with natural language, we can locate the
i j=1 object described by language instructions via open-vocabulary
querying.WefirstfollowtheapproachofLERF[16]tocompute
where l is the open-vocabulary feature embedding of ith 3D
i the relevance score s for each textual query:
gaussian primitives and L(u) represents the rendered open-
vocabulary feature embedding at pixel u. exp(Œ®(L)¬∑Tq)
s=min (5)
To distill the 2D open-vocabulary feature to 3D field, we i exp(Œ®(L)¬∑Tq)+exp(Œ®(L)¬∑Tcanon)
i
need to (1) recover the dimension of L to that of F and
whereTq istheCLIPembeddingofthetextqueryandTcanon
(2) minimize the feature distance between the recovered L i
is a set of canonical phrase embeddings Tcanon selected from
and the F. However, high-dimensional vector computation i
"object", "things", "stuff", and "texture".
for dense feature maps leads to a catastrophic increase in
As a result, for each textual query, we obtain a relevance
computation time and memory usage. To tackle this problem,
heatmap where the points with relevance scores below a pre-
we propose a contrastive-learning-based distillation strategy,
determined threshold will be filtered out. Thus, the remaining
as shown in Fig. 2 (b). Specifically, having instance-level
region forms a mask for predicting the queried object. After
segmentation masks extracted by SAM, we impose constraints
obtaining the mask of the queried object, we locate the object
for the consistency of each pixel‚Äôs rendered feature within the
with a bounding box and convex hull.
same mask. To ensure runtime efficiency, we randomly sample
2) Geometry Restruction: To obtain dense point cloud
a few pixel pairs within each mask and only minimize the
representations of objects and surface normal which is closely
distance of latent features between pixels of per sampled pair.
relatedtoroboticgrasping,werenderdepthandsurfacenormal
The number of pairs for each mask is proportion to the mask‚Äôs
to multiple viewpoints.
area and the number of total pairs n is fixed. We calculate the
Depthrendering:SimilartotherenderingofRGB,wecompute
average feature distances between all pairs as contrastive loss,
the depth value for each pixel using the rasterizer:
written as:
n i‚àí1
1 (cid:88) (cid:88) (cid:89)
L contr. =1‚àí n L(u i)¬∑L(v i) (3) D(u)= d iŒ± i (1‚àíŒ± j) (6)
i=1 i j=1where D(u) indicates the rendered depth map at pixel u. highestscoremaynotbethesuitableoneforstablegrasp.Thus,
We use the depth map obtained from the depth camera for we explicitly utilize the force closure theory as a screening
the corresponding viewpoint to supervise the rendered depth mechanism. For each grasp pose proposal with two contact
map where we calculate the L1 loss: points, we calculate the angle between the grasping line and
the surface normal of each contact point and get the sum of
m
L = 1 (cid:88) |DÀÜ(i)‚àíD(i)| (7) two angles. If the sum of the two angles is less than or equal
depth m to a pre-defined threshold Œ∏ , we regard the grasp pose as a
i=1 thre
feasible proposal, otherwise it is unfeasible, as shown in Fig. 2
where DÀÜ is the depth map obtained from the depth camera
(c). We choose the pose with the highest "grasping score" in
and m is the number of pixels with valid depth value.
feasible proposals as the final grasp pose.
Normal rendering: As surface normals are directional vectors
4) Gaussian Field Updating: As an explicit representation
that should exhibit rotational equivariance, normals cannot
composed of 3D Gaussian primitives, it is easy to operate
be rendered as semantic features. Therefore, we follow [43],
Gaussian primitives. Therefore, after manipulating an object,
[44] that use the shortest axis direction of the 3D Gaussian
we will operate all Gaussian primitives within this object‚Äôs
primitives to serve as surface normal. As a result, the normals
convex hull with the same rotation and translation, which can
aregeometricpropertiesofGaussianprimitives,relatedtotheir
be calculated from the transform of the end-effector of the
orientations. We render the normal map by using the rasterizer:
robot arm. After operating 3D Gaussian primitives to a new
i‚àí1 position,weusefewerviewsandtimetofine-tune3DGaussian
(cid:88) (cid:89)
N(u)= n Œ± (1‚àíŒ± ) (8) field to update the scene.
i i j
i j=1
IV. EXPERIMENT
where N(u) indicates the rendered surface normal map at pixel
In this section, we first introduce the setup of the experi-
u and n is the normal of the ith 3D Gaussian primitive.
i mental environment. Next, we conduct experiments to validate
The rendered normal map represents the per-pixel surface
our proposed EFD module where we report both quantitative
normalintherobotbasecoordinate.WeuseaSobel-likeopera-
results and qualitative results. Subsequently, we show the
tortocomputethenormalsforeachpixelintheacquireddepth
resultsofgeometryreconstructionandconductablationstudyto
map and transform the calculated normals from the camera
demonstrate the effectiveness of our proposed normal-guided
coordinate to the robot base coordinate. After normalizing all
grasp. Finally, we conduct grasp-update-grasp experiments
normalmapstounitvectors,wesupervisetherenderednormals
on scenes to verify the effectiveness and efficiency of our
within the valid depth region:
scene update module. These experiments fully demonstrate the
L =
1 (cid:88)m (cid:18)(cid:16) NÀÜ(i)‚àíN(i)(cid:17)2 +1‚àíNÀÜ(i)¬∑N(i)(cid:19)
(9)
performance of our method in open-scene understanding and
normal m language-guided grasping.
i=1
A. Experimental Setup
3) Feasible Grasp Pose Generation: After obtaining the
localization and geometry of the object, we (1) employ a grasp 1) Scenes,ObjectsandDevices: Webuilta140√ó70√ó30cm3
detection method to propose initial grasp poses and (2) utilize desktop scene with common objects in the kitchen including
our rendered normal to augment the grasp pose proposals various food and tableware as well as office supplies including
according to the force closure theory. staplers,mouseanddecorativeornaments.WeuseaUR5robot
Grasp pose generation: In our work, we employ the Any- arm equipped with a ROBOTIQ gripper to execute robotic
Grasp [34], the state-of-the-art grasp detection network, which manipulation. We set up our system in 10 open desktop scenes
takes colorful point cloud as input and generates a set of with a total of 44 objects (40 are graspable) where we execute
collision-free grasp proposals for parallel two-finger grippers. language-guidedmanipulation120times.Intermsofcomputing
Each grasp proposal is represented by grasp position, width, resources, we use an NVIDIA RTX-3090 GPU to reconstruct
height, depth and a grasping score. We generate the grasp- the feature field and reconstruct geometry. The reconstruction
pose proposals for the object queried by language with the process only requires approximately 6GB of memory in total.
following steps: (1) Re-project the rendered depth of the 2) Data Collection and Processing: We first use the robot
queried object from each viewpoint to produce a dense point arm equipped with a Realsense D455 to scan the desktop
cloud representation. (2) Combine the object‚Äôs point cloud scene from 16 viewpoints. At the same time, we will also
with the point cloud derived from 3D Gaussian primitives record the camera extrinsic parameter, calculated through the
because the derived point cloud depicts the localization and transformation of the end effector to the base. After obtaining
approximate shape of other objects, which is beneficial for images and depth maps, we re-project the depth map and
generating collision-free grasp poses. (3) Employ AnyGrasp to convert all re-projected points to the base coordinate. We
generategraspposes,whicharerestrictedintheaforementioned downsample the number of these points to about 300k to
bounding box, obtained from steps in III-C1. initialize the 3D Gaussian primitives. The collected images are
Normal-guided grasp: Although AnyGrasp provides a "grasp- processed by SAM and CLIP to generate segmentation maps
ing score" for each grasp proposal, the grasp pose with the and open-vocabulary feature maps.Image SAM+CLIP Image SAM+CLIP Image SAM+CLIP Image SAM+CLIP
LERF Ours LERF Ours LERF Ours LERF Ours
‚ÄòUmbrella‚Äô ‚ÄòWooden fork‚Äô ‚ÄòTurkey‚Äô ‚ÄòFish‚Äô
Image SAM+CLIP Image SAM+CLIP Image SAM+CLIP Image SAM+CLIP
LERF Ours LERF Ours LERF Ours LERF Ours
‚ÄòCamera model‚Äô ‚ÄòRoasted chicken wing‚Äô ‚ÄòFork‚Äô ‚ÄòBanana‚Äô
Fig.3. Relevancemapofthegivenlanguageinstructions.OurmethodexhibitsclearersegmentationboundariescomparedtoLERF,whichcanbeusedto
obtainmoreaccuratelocalization.ComparedwithSAM+CLIP,ourapproachexhibitsmoreconsistentopen-vocabularyfeaturesacrossmulti-views.For
instance,in‚ÄôRoastedchickenwing‚Äô,theresponseofSAM+CLIPisthechickenwingandtheforkwhileourmethodmakesthecorrectresponse.
B. Results of Efficient Feature Distillation TABLEI
QUANTITATIVECOMPARISONSOFSEMANTICSEGMENTATIONAND
We show both qualitative results and quantitative results to
LOCALIZATIONACCURACYONOURSCENARIOS
demonstrate the effectiveness and efficiency of our proposed Method mIoU(%)‚Üë Accuracy(%)‚Üë Timeperquery(s)‚Üì
EFD module. Our baselines are Lseg [45] and LERF [16] (All LSeg[45] 26.4 40.6 -
mention of LERF in our experiments includes an extra depth LERF*[16] 41.3 65.1 40.27
Ours 58.2 87.5 0.22
supervision to ensure a fair comparison with our method.)
"*"representsLERFwithanextradepthsupervision
In qualitative results, we compare our method with SAM +
CLIP (our 2D sudo labels) and LERF and show the relevance Besides,wetestthequeryspeedandreportresultsinTableI.
mapofeachgivenlanguageinstruction.AsshowninFig.3,the The metric is the time usage (s) per text query at a resolution
purpleboxesdemonstratethatthefeaturesextractedfromSAM of 640√ó480. It can be seen that our method achieves an
and CLIP are not accurate such as the incomplete wooden fork approximate180√óspeedupoverLERF.Wealsodirectlydistill
in ‚ÄôWooden fork‚Äô and the pot and turkey with similar semantic CLIP features into 3D Gaussian field, which takes over 70 GB
features in ‚ÄôTurkey‚Äô. In comparison, our relevance map is more of memory, making it hard to be applied to robots.
accurate, proving that our reconstructed feature field solves the
problemoffeatureinconsistencyacrossmultipleviews.Besides, C. Results of Geometry Reconstruction
compared with LERF, our method exhibits better segmentation We show the visualization of our rendered depth and normal
boundaries. Therefore, our method can help robots reduce the compared with ground truth (scanned by D455 camera), as
ambiguity of object perception. shown in Fig. 4. The black region represents the invalid value
We report the quantitative results of two tasks including of ground truth. It can be seen that the surface normal we
segmentation and localization. In the segmentation task, as rendered is smoother than the ground truth, as shown in red
described in III-C1 we filter out the region whose relevance boxes. Furthermore, even in areas where the ground truth is
score is below 0.85 to form a predicted segmentation map. invalid, we can still render accurate depth and surface normal.
We calculate the mIoU metric between predicted segmentation For example, in the third row, the camera can not capture the
maps and our manually annotated ground truth. In the localiza- depth of the silver eyeglass case in this view because of the
tion task, following LERF, given a language instruction, if the specular reflection of the case but our method renders accurate
point with the highest relevance score is in the target object, it depth and normal of it.
is a successful localization. We calculate the average accuracy
D. Effectiveness of Normal-guided Grasp
ofallresponsesasthemetrics.Theresultsofsegmentationand
localizationareshowninTableIwhereourmethodsignificantly In this subsection, we aim to validate the effectiveness of
outperforms other approaches. our proposed normal-guided grasp. We first give the qualitativeInitial Scene
After Updating
Image GT Depth Rendered Depth GT Normal Rendered Normal
Ground Truth
Fig.4. Comparedwithscanneddepthandsurfacenormal,ourrendereddepth
andsurfacenormalissmoother.Ourmethodrendersaccuratedepthandsurface
normaleveninareaswherethegroundtruthisinvalid.
TABLEII RGB Depth Normal Segmentation
GRASPINGRESULTS:RESULTSAREREPORTEDACROSS40DIFFERENT
Fig. 6. Results of scene update. We show the RGB, depth, normal, and
OBJECTSOF10SCENARIOS.EACHOBJECTISGRASPEDTHREETIMES.
segmentationbeforeandafterthesceneupdatebasedonthelanguagequery
Method GraspingSuccessRate(%) "orange".Asindicatedbytheyellowboxes,oursceneupdatingsuccessfully
LSeg+Depth[45] 26.7 moves the orange to the plate and restores the region that was previously
LERF+AnyGrasp[16] 55.8 obscuredbytheorange.Asindicatedbytheredboxes,theupdatedorange
Oursw/o.NormalFilter 78.3 stillmaintainsaccurategeometryandsemanticfeatures.
Oursw/NormalFilter 85.0
TABLEIII
resulttovalidatethatthesurfacenormalcanfilteroutunfeasible EFFIECIENCYCOMPARISONBETWEENLERFANDOURMETHOD.
grasp poses. As shown in Fig. 5, the original top-ranked Method Viewpoints Memory Time
proposal (red) is filtered out as the angles between its grasping LERF[16] 16 15GB 30min
Ours 5 4GB 1min
line and surface normal of contact points are too large. In
contrast, the original second-ranked proposal is feasible. Thus,
we execute a grasp based on this pose. 1) Successful rate of manipulation: In this subsection, we
Besides, we report the quantitative results of the grasping show the result of languaged-guided grasping where we tested
success rate with and without the normal filter, as shown in 120 times on 40 objects. We compare our method with (1)
TableII.Leveragingthenormalfiltersignificantlyincreasesthe Lseg + AnyGrasp and (2) LERF + AnyGrasp. To obtain the
success rate by 7.7%, further demonstrating the effectiveness 3D point cloud, we use the rendered depth to re-project the
of our proposed normal-guided grasp. segmentation masks of LERF and use scanned depth to re-
projectthesegmentationmasksofLSeg.Wedefineasuccessful
grasp as stably picking up the corresponding object according
Right View
to the language instruction and raising it to a height of more
than 10cm over 3 seconds. The result is shown in Table II,
where our method far exceeds other methods in success rate.
2) Scene updating: To validate the effectiveness of our
Final Grasp Pose proposed efficient scene updating, we execute an experiment
Left View
whose process is (1) picking up the object and placing it to
the target position according to the language instruction, (2)
capturing RGB-D images from 5 viewpoints to update the
scene, (3) executing another manipulation on this object. The
Grasp Proposals Surface Normal Execution result is shown in Fig. 6, where our updated scene remains
high quality RGB, geometry and semantic features, proving
Fig.5. Effectivenessofourproposednormal-guidedgrasp.Theleftcolumn the effectiveness of the proposed scene updating. Besides, we
showsthetop5graspproposalsprovidedbyAnyGrasp.Theredderthecolor, also show the efficiency comparison between LERF and ours
thehigherthegraspingscore.Themiddlecolumndisplaysthesurfacenormal
includingviewpointnumbers,memoryusageandreconstruction
oftheobject,withpurplearrowsindicatingthenormalofthecontactpoints.
Therightcolumndemonstratesthesuccessfulexecutionofgraspingtheknife time for updating, as shown in Table IV-E2. Our scene update
utilizingthefinalgraspposeafterfilteringoutunreasonableproposals. capability makes the reconstructed scene more capable of
handling continuous grasping.
E. Results of Language-guided Manipulation
V. LIMITATION
Inthissubsection,wefirstgivetheresultoflanguage-guided
grasping. Then, we validate our proposed scene updating via One limitation is that our reconstructed scene remains static.
continuous language-guided picking and placing. Althoughwehaveproposedascene-updatingmoduletohandlecontinuous robotic grasping, we are unable to account for [19] B.Kerbl,G.Kopanas,T.Leimk√ºhler,andG.Drettakis,‚Äú3dgaussian
unrecordable scene changes, such as objects shifting positions splattingforreal-timeradiancefieldrendering,‚ÄùACMTransactionson
Graphics,2023.
causedbycollisionsorvibrations.Anotherlimitationisthatour
[20] J.RedmonandA.Angelova,‚ÄúReal-timegraspdetectionusingconvolu-
method fails to estimate the depth and normal of transparent tionalneuralnetworks,‚ÄùinICRA,2015.
objects due to the lack of ground truth. [21] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,
andK.Goldberg,‚ÄúDex-net2.0:Deeplearningtoplanrobustgraspswith
VI. CONCLUSION syntheticpointcloudsandanalyticgraspmetrics,‚ÄùRSS,2017.
[22] D.Guo,F.Sun,H.Liu,T.Kong,B.Fang,andN.Xi,‚ÄúAhybriddeep
In this paper, we introduce GaussianGrasper, a novel architectureforroboticgraspdetection,‚ÄùinICRA,2017.
approach for open-world robotic grasping guided by natural [23] X.Zhou,X.Lan,H.Zhang,Z.Tian,Y.Zhang,andN.Zheng,‚ÄúFully
convolutional grasp detection network with oriented anchor box,‚Äù in
language instructions from RGB-D inputs. Taking multi-view
IROS,2018.
RGB-D images as input, our method efficiently reconstructs [24] Y. Zheng, Q. Wang, C. Zhong, H. Liang, Z. Han, and Y. Zheng,
consistent feature fields through our proposed EFD module. ‚ÄúEnhancingdailylifethroughaninteractivedesktoproboticssystem,‚Äùin
CICAI,2023.
Thefeaturefieldenablestherobottounderstandtheopenworld
[25] X. Zhu, L. Sun, Y. Fan, and M. Tomizuka, ‚Äú6-dof contrastive grasp
and make precise localization based on language instructions. proposalnetwork,‚ÄùinICRA,2021.
Besides, we estimate the geometry and propose the normal- [26] G.Zhai,D.Huang,S.-C.Wu,H.Jung,Y.Di,F.Manhardt,F.Tombari,
N.Navab,andB.Busam,‚ÄúMonograspnet:6-dofgraspingwithasingle
guided grasp to augment the robotic grasping. Furthermore,
rgbimage,‚ÄùinICRA,2023.
our scene can also be quickly updated to support continuous [27] H. Liang, X. Ma, S. Li, M. G√∂rner, S. Tang, B. Fang, F. Sun, and
grasping. Sufficient real-world experiments have demonstrated J.Zhang,‚ÄúPointnetgpd:Detectinggraspconfigurationsfrompointsets,‚Äù
inICRA,2019.
the effectiveness of our method.
[28] C.Wu,J.Chen,Q.Cao,J.Zhang,Y.Tai,L.Sun,andK.Jia,‚ÄúGrasp
proposalnetworks:Anend-to-endsolutionforvisuallearningofrobotic
REFERENCES
grasps,‚ÄùNeurIPS,2020.
[1] C. Lynch and P. Sermanet, ‚ÄúLanguage conditioned imitation learning [29] M. Sundermeyer, A. Mousavian, R. Triebel, and D. Fox, ‚ÄúContact-
overunstructureddata,‚ÄùRSS,2020. graspnet:Efficient6-dofgraspgenerationinclutteredscenes,‚ÄùinICRA,
[2] M.Shridhar,L.Manuelli,andD.Fox,‚ÄúCliport:Whatandwherepathways 2021.
forroboticmanipulation,‚ÄùinCoRL,2022. [30] B.Zhao,H.Zhang,X.Lan,H.Wang,Z.Tian,andN.Zheng,‚ÄúRegnet:
[3] X.Lin,J.So,S.Mahalingam,F.Liu,andP.Abbeel,‚ÄúSpawnnet:Learning Region-based grasp network for end-to-end grasp detection in point
generalizablevisuomotorskillsfrompre-trainednetworks,‚ÄùarXivpreprint clouds,‚ÄùinICRA,2021.
arXiv:2307.03567,2023. [31] A. Alliegro, M. Rudorfer, F. Frattin, A. Leonardis, and T. Tommasi,
[4] P.-L.Guhur,S.Chen,R.G.Pinel,M.Tapaswi,I.Laptev,andC.Schmid, ‚ÄúEnd-to-endlearningtograspviasamplingfromobjectpointclouds,‚Äù
‚ÄúInstruction-drivenhistory-awarepoliciesforroboticmanipulations,‚Äùin
RA-L,2022.
CoRL,2022. [32] H.-S.Fang,C.Wang,M.Gou,andC.Lu,‚ÄúGraspnet-1billion:Alarge-
[5] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
scalebenchmarkforgeneralobjectgrasping,‚ÄùinCVPR,2020.
T.Xiao,S.Whitehead,A.C.Berg,W.-Y.Lo,etal.,‚ÄúSegmentanything,‚Äù [33] H. Fang, H.-S. Fang, S. Xu, and C. Lu, ‚ÄúTranscg: A large-scale real-
inICCV,2023. world dataset for transparent object depth completion and a grasping
[6] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,
baseline,‚ÄùRA-L,2022.
G.Sastry,A.Askell,P.Mishkin,J.Clark,etal.,‚ÄúLearningtransferable [34] H.-S.Fang,C.Wang,H.Fang,M.Gou,J.Liu,H.Yan,W.Liu,Y.Xie,
visualmodelsfromnaturallanguagesupervision,‚ÄùinICML,2021. andC.Lu,‚ÄúAnygrasp:Robustandefficientgraspperceptioninspatial
[7] M.Caron,H.Touvron,I.Misra,H.J√©gou,J.Mairal,P.Bojanowski,and
andtemporaldomains,‚ÄùT-RO,2023.
A.Joulin,‚ÄúEmergingpropertiesinself-supervisedvisiontransformers,‚Äù [35] A.TenPasandR.Platt,‚ÄúUsinggeometrytodetectgraspposesin3d
inICCV,2021. pointclouds,‚ÄùRoboticsResearch:Volume1,2018.
[8] D. Z. Chen, A. X. Chang, and M. Nie√üner, ‚ÄúScanrefer: 3d object [36] H.-Y.F.Tung,R.Cheng,andK.Fragkiadaki,‚ÄúLearningspatialcommon
localizationinrgb-dscansusingnaturallanguage,‚ÄùinECCV,2020. sensewithgeometry-awarerecurrentnetworks,‚ÄùinCVPR,2019.
[9] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu, ‚ÄúSynergies [37] H.-Y. F. Tung, Z. Xian, M. Prabhudesai, S. Lal, and K. Fragkiadaki,
between affordance and geometry: 6-dof grasp detection via implicit ‚Äú3d-oes:Viewpoint-invariantobject-factorizedenvironmentsimulators,‚Äù
representations,‚ÄùRSS,2021. arXivpreprintarXiv:2011.06464,2020.
[10] S. Chen, R. G. Pinel, C. Schmid, and I. Laptev, ‚ÄúPolarnet: 3d point [38] S. Peng, K. Genova, C. Jiang, A. Tagliasacchi, M. Pollefeys,
cloudsforlanguage-guidedroboticmanipulation,‚ÄùArXiv,2023. T.Funkhouser,etal.,‚ÄúOpenscene:3dsceneunderstandingwithopen
[11] M. Shridhar, L. Manuelli, and D. Fox, ‚ÄúPerceiver-actor: A multi-task
vocabularies,‚ÄùinCVPR,2023.
transformerforroboticmanipulation,‚ÄùinCoRL,2022. [39] C.Wang,M.Chai,M.He,D.Chen,andJ.Liao,‚ÄúClip-nerf:Text-and-
[12] Y.Ze,G.Yan,Y.-H.Wu,A.Macaluso,Y.Ge,J.Ye,N.Hansen,L.E.Li,
imagedrivenmanipulationofneuralradiancefields,‚ÄùinCVPR,2022.
andX.Wang,‚ÄúGnfactor:Multi-taskrealrobotlearningwithgeneralizable [40] Q.Wang,H.Zhang,C.Deng,Y.You,H.Dong,Y.Zhu,andL.Guibas,
neuralfeaturefields,‚ÄùinCoRL,2023. ‚ÄúSparsedff:Sparse-viewfeaturedistillationforone-shotdexterousmanip-
[13] C.Zhong,Y.Zheng,Y.Zheng,H.Zhao,L.Yi,X.Mu,L.Wang,P.Li,
ulation,‚ÄùarXivpreprintarXiv:2310.16838,2023.
G.Zhou,C.Yang,X.Zhang,andJ.Zhao,‚Äú3dimplicittransporterfor [41] J.L.SchonbergerandJ.-M.Frahm,‚ÄúStructure-from-motionrevisited,‚Äù
temporallyconsistentkeypointdiscovery,‚ÄùinICCV,2023. inCVPR,2016.
[14] S.Kobayashi,E.Matsumoto,andV.Sitzmann,‚ÄúDecomposingnerffor [42] A.NeubeckandL.VanGool,‚ÄúEfficientnon-maximumsuppression,‚Äùin
editingviafeaturefielddistillation,‚ÄùNeurIPS,2022. ICPR,2006.
[15] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi, ‚ÄúNeural feature [43] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma,
fusionfields:3ddistillationofself-supervised2dimagerepresentations,‚Äù ‚ÄúGaussianshader: 3d gaussian splatting with shading functions for
in3DV,2022. reflectivesurfaces,‚ÄùarXivpreprintarXiv:2311.17977,2023.
[16] J.Kerr,C.M.Kim,K.Goldberg,A.Kanazawa,andM.Tancik,‚ÄúLerf: [44] X. Long, Y. Zheng, Y. Zheng, B. Tian, C. Lin, L. Liu, H. Zhao,
Languageembeddedradiancefields,‚ÄùinICCV,2023. G.Zhou,andW.Wang,‚ÄúAdaptivesurfacenormalconstraintforgeometric
[17] W.Shen,G.Yang,A.Yu,J.Wong,L.P.Kaelbling,andP.Isola,‚ÄúDistilled
estimationfrommonocularimages,‚ÄùarXivpreprintarXiv:2402.05869,
featurefieldsenablefew-shotlanguage-guidedmanipulation,‚ÄùinCoRL, 2024.
2023. [45] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl,
[18] A.Rashid,S.Sharma,C.M.Kim,J.Kerr,L.Y.Chen,A.Kanazawa,
‚ÄúLanguage-drivensemanticsegmentation,‚ÄùinICLR,2022.
and K. Goldberg, ‚ÄúLanguage embedded radiance fields for zero-shot
task-orientedgrasping,‚ÄùinCoRL,2023.