AlphaDou: High-Performance End-to-End Doudizhu AI Integrating
Bidding
Chang Lei1, Huan Lei1‚àó,
1Independent Researcher
lcrichard@alumni.sjtu.edu.cn, lei-h21@tsinghua.org.cn
Abstract However, AI has not performed perfectly in cer-
tain gambling games that require bidding. NukkiAI
Artificial intelligence for card games has long been a only outperformed professional human players in non-
popular topic in AI research. In recent years, com- bidding 1v1 Bridge, and Douzero did not consider the
plexcardgameslikeMahjongandTexasHold‚Äôemhave
biddingphaseduringtraining.TheseAIsfunctionmore
been solved, with corresponding AI programs reach- as playing machines rather than proficient gamblers.
ing the level of human experts. However, the game of
The bidding phase contains rich strategic information
Dou Di Zhu presents significant challenges due to its
that significantly influences player strategies. When an
vast state/action space and unique characteristics in-
volving reasoning about competition and cooperation, opponent has a strong hand, they tend to bid high for
making the game extremely difficult to solve.The RL higher potential rewards, while players should adopt
model DouZero, trained using the Deep Monte Carlo a conservative strategy to minimize losses. Conversely,
algorithmframework,hasshownexcellentperformance when opponents bid low, players can employ more ag-
in DouDiZhu. However, there are differences between gressive strategies to increase their gains.
itssimplifiedgameenvironmentandtheactualDouDi
This work aims to develop a high-performance end-
Zhuenvironment,anditsperformanceisstillaconsid-
to-end Doudizhu AI model that incorporates bidding.
erabledistancefromthatofhumanexperts.Thispaper
Doudizhu, also known as Fighting the Landlord, is the
modifies the Deep Monte Carlo algorithm framework
byusingreinforcementlearningtoobtainaneuralnet- most popular card game in China. Doudizhu is a 3-
work that simultaneously estimates win rates and ex- player IIG where players bid based on their hands, and
pectations. The action space is pruned using expecta- thewinningbidderbecomestheLandlord.Theremain-
tions,andstrategiesaregeneratedbasedonwinrates. ingplayersformthePeasantsteamtoopposetheLand-
This RL model is trained in a realistic DouDiZhu en- lord. If any Peasant player wins, the entire team wins.
vironment and achieves a state-of-the-art level among The Landlord wins double rewards, while each Peasant
publicly available models.
player receives a single reward if the team wins, and
viceversa.Rewardsarerelatedtothebidscoreandthe
Introduction occurrence of "bombs" (four cards of the same rank)
or "rockets" during the game. Players with good hands
Games can be broadly classified into two categories:
tend to bid high to become the Landlord for higher
perfect-information games (PIGs) and imperfect-
returns. Moreover, Doudizhu has a large, flexible, and
informationgames(IIGs).InPIGs,playerscanobserve
diverse action space with thousands of possible states
allgamestates,suchasinShogi,Go,andChess.Incon-
(1083) and actions (27,472) due to card combinations
trast, IIGs involve scenarios where participants cannot
and complex rules (Zha et al. 2019). Additionally, re-
access complete information about other players, such
wards in Doudizhu are sparse and highly variable, only
as in heads-up Texas Hold‚Äôem. Reinforcement learning
awarded at the end of the game and influenced by the
(RL) has been successfully applied to create numer-
biddingphase,thenumberof"bombs"duringthegame,
ous game AIs. RL algorithms have achieved remark-
and"spring"rewards.Thesecharacteristicsmaketrain-
able success in both PIGs and IIGs, exemplified by Al-
ing a Doudizhu AI extremely challenging, and existing
phaGo (Silver et al. 2016) and AlphaZero (Silver et al.
Doudizhu AIs exhibit certain issues.
2017) in Go, AlphaStar (Vinyals et al. 2019) in Star-
Craft II, OpenAI Five (OpenAI et al. 2019) in Dota Previous research on Doudizhu AI has primarily fo-
2, Suphx (Li et al. 2020) in Mahjong, Douzero (Zha cused on the playing phase, neglecting the bidding
et al. 2021) in Doudizhu, NukkiAI (Bouzy, Rimbaud, phase or employing completely random bid strategies.
and Ventos 2020) in Contract Bridge, and AlphaHol- DeltaDou (Jiang et al. 2019) is the first AI program
dem (Zhao et al. 2022a) in Hold‚Äôem. to achieve human-level performance compared to top
humanplayers,usinganAlphaZero-likealgorithmwith
‚àóCorresponding Author Bayesianmethodstoinferhiddeninformationandsam-
4202
luJ
41
]IA.sc[
1v97201.7042:viXraple other players‚Äô actions based on their policy net- The Game of Doudizhu
works.However,thevastactionspaceinDoudizhulim-
Doudizhu is a three-player card game that is extremely
its DeltaDou‚Äôs effectiveness. Douzero introduced Deep
popular in China and is considered a typical gambling
Monte Carlo (DMC), which combines the conventional
game. Among the three players, two are Peasants who
Monte Carlo method with deep neural networks. In
need to cooperate to compete against the third player,
a Monte Carlo self-play framework, deep neural net-
the Landlord. The game comprises two phases: 1) Bid-
works first estimate the value of each action (Q-value),
ding and 2) Cardplay.
then select the action with the highest Q-value as
a training label or final move. DMC addresses the
Bidding
challenge of Doudizhu‚Äôs large action space, making
training more stable. Doudizhu with DMC successfully The Bidding Phase determines the roles of the players.
outperformed other RL algorithms, including Deep-Q- Atthestartofthegame,eachplayerreceivesseventeen
Learning (DQN) (Mnih et al. 2015; You et al. 2019), cards from a shuffled deck in a counterclockwise man-
Combination Q-Network (CQN) (You et al. 2019), and ner, with three cards left in the middle of the table. In
A3C (Mnih et al. 2016; You et al. 2019). Subsequently, the Bidding Phase, a randomly selected player begins
(Wang, Wu, and Lai 2023) noted that the score distri- thebiddingprocess,followedbytheothersinsequence.
bution in gambling games is a combination of winning Each player can only bid once, with options to bid 1
and losing score distributions, with risk-averse strate- point, 2 points, 3 points, or pass. A subsequent player
gies resulting in a significant gap between them. Pre- musteitherchooseahigherbidorpass.Thefirstplayer
vious value-based methods directly predicted this com- to bid 3 points becomes the Landlord, or if all players
bined distribution, leading to high variance and unsta- completetheirbids,theplayerwiththehighestbidbe-
bletraining.TheyproposedWagerWin(Wang,Wu,and comestheLandlord,whiletheothertwoplayersbecome
Lai 2023), which introduces probability and value fac- Peasants. The Landlord has the privilege to reveal the
torization, enabling individual updates of the winning threeremainingcardsforallplayerstoseeandthenin-
probability, losing Q-value, and winning Q-value. This corporatesthesecardsintohis/herhand.Notably,ifall
method stabilizes the training of gambling game AIs. threeplayerschoosetopass,thegameresultsinadraw,
However, WagerWin primarily accelerated AI conver- and a new game starts with a fresh deal. The bid score
gence without significantly improving the optimal pol- impacts the final game rewards: if the Landlord wins,
icy and winning rate. In addition, several variants of he/she gains points equal to twice the bid score from
DouZerohavebeenproposed,suchasDouZero+(Zhao both Peasants. Conversely, if any Peasant wins, both
et al. 2022b) and Full DouZero+ (Zhao et al. 2024), Peasants receive points equal to the bid score from the
which incorporate predictions of the opponents‚Äô hands Landlord,wholosesdoublethepoints.Thisscenarioas-
based on the original DouZero model. However, these sumestheabsenceofBombs,Rockets,andSpring(refer
variantsdonotprovidequantifiableimprovementsover to the following section).
the original DouZero. Mdou (Luo et al. 2024), on the
other hand, consolidates the three models of DouZero Cardplay
into a single model for training, resulting in faster con- During the Cardplay phase, players take turns playing
vergence. Despite this, the overall performance of the cards. Each game consists of multiple rounds, starting
modeldoesnotshowsignificantimprovementcompared with a player playing a valid card combination (e.g.,
toDouZero.RARSMSDou(LuoandTan2024)utilized solo, pair). The first round is initiated by the Land-
the PPO framework (Schulman et al. 2017) to enhance lord. Subsequent players must either pass or defeat the
Doudizhu AI, addressing the large action space by ab- previous hand by playing a higher-ranked combination
stracting actions into several major categories, training (anactionhasarank,refertoAppendixA).Theround
a PPO model to select categories, and then training a continues until two consecutive players pass. Then, the
DMC model to choose actions within the selected cat- player who played the last hand starts the next round.
egory. RARSMSDou outperformed DouZero. The objective is to clear all cards from one‚Äôs hand to
win. Each "Bomb" and "Rocket" can double the game‚Äôs
stakes.IftheLandlordwins,theyreceivedoublethere-
In this paper, we propose AlphaDou, an end-to-end wards, whereas if the Peasant team wins, each Peasant
Doudizhu AI system that incorporates bidding. Our player receives single rewards. Rewards are influenced
model does not require abstract state/action spaces bythebidscoreandthepresenceofBombsorRockets.
or any human knowledge. By utilizing more informa- Bombs surpass any action. The only way to defeat a
tion, our model can adjust its playing strategy based Bomb is with a higher-ranked Bomb or a Rocket. The
on bidding results. We conducted several experiments, Rocket is the highest action in the game and can beat
andourresultsindicatethatourbidstrategysurpasses anyBomboraction.WhenaBomborRocketisplayed,
the bid network using supervised learning with hu- the points at stake double. For instance, if the winning
manexpertdata.Also,Ourmodeloutperformsexisting bidis3pointsatthestart,itbecomes6pointsifaBomb
Doudizhu AIs regardless of whether the bid strategy is isplayedand12pointsifanotherBombisplayed.With
applied. two Bombs played, the Landlord stands to win/lose 24points, and each Peasant stands to win/lose 12 points.
Observation Shape Description
Thegameconcludeswhenaplayerclearsalltheircards.
Actions from {0, 1, 2, 3}
To encourage more aggressive play, Doudizhu includes actions (1, 54)
repeated 54 times
a"Spring"reward:ifthroughoutthegame,thePeasant
team makes no plays other than passing, or the Land- my_handcards (1, 54) Player‚Äôs current hand
lord only passes once, it is termed as Spring or Anti-
The bid called by the 1st player,
Spring,respectively,doublingthereward(equivalentto 1st bid* (1, 54)
repeated 54 times
playing an additional Bomb).
For more information, readers may also refer to the The bid called by the 2nd player,
2nd bid* (1, 54)
Wikipedia page on Doudizhu. repeated 54 times
The bid called by the 3rd player,
3rd bid* (1, 54)
AlphaDou repeated 54 times
The goal of AlphaDou is to incorporate the Bid Phase
* if the score has not been called yet, repeat ‚Äú-1‚Äù 54 times
in the training and testing stages of the Doudizhu AI,
enabling the AI to fully engage in a complete gambling Table 1: Observation data during the bidding phase.
game."End-to-end"heremeansthatthisframeworkdi-
rectly accepts game state information and outputs ac-
tions, without requiring handcrafted feature encoding plicate inputs in the bid data, the information in data
as input or iterative reasoning during decision-making. part b directly affects the final game score and is thus
AlphaDou uses a reinforcement learning (RL) frame- included separately as input.
work to achieve this goal, driven solely by game re- Weusesixneuralnetworkstomodelthesixpositions:
wards. The Bid Phase introduces significant variance "first", "second", "third", "landlord", "landlord_down",
in game rewards, so we implemented a series of mea- and "landlord_up". The "first", "second", and "third"
sures to reduce reward variance and make the model‚Äôs models form the Bid Model, representing the first,
strategy more flexible. second, and third players in the bidding process, re-
spectively.The"landlord","landlord_down",and"land-
Card Representation and Neural lord_up" models form the Card Model, representing
the Landlord, the player to the left of the Landlord,
Architecture
and the player to the right of the Landlord during the
For any card combination, excluding jokers, we encode
card-playing phase, respectively. The Bid Model and
the remaining card combination into a one-hot 4√ó13
the Card Model have similar network structures. The
matrix, with 13 columns representing the cards 3, 4,
threeneuralnetworksusedforbiddingintheBidModel
5, 6, 7, 8, 9, T, J, Q, K, A, 2. The i-th row (where
share the same structure, and the three neural net-
i‚àà{0,1,2,3,4}) indicates whether the number of that
works used for playing cards in the Card Model share
card type is greater than i; if true, it is 1, otherwise it
the same structure. Figure 1(b) illustrates the neural
is0.Thisisthenflattenedintoa1√ó52vector,withan
network structure. To ensure the network gives more
additional 1 √ó 2 matrix indicating the presence of the
importancetoinputsthathaveagreaterimpactonthe
Black and Red Jokers. Figure 1(a) demonstrates this
final score, we repeat the input data part b four times
encoding process.
and concatenate it with the residual part of the input.
During the bidding phase, we record the observed
The bidding network input is not divided into parts a
data as shown in Table 1 and generate a 5 √ó 54 obser-
and b, so the yellow box region is not present in its
vation matrix for each possible move. All observation
structure. The rest of the structure is similar. The pa-
matricesarecombinedintoabatch√ó5√ó54matrixas
rameters of each layer of the neural network are shown
input data, where the batch size is the number of valid
in the Table 3.
moves.
In the card-playing phase, our recorded data is di- Deep Monte-Carlo
videdintopartsaandb.TheobservationdatainTable
Monte Carlo (MC) methods are a class of methods
2 is used to generate a 72 √ó 54 observation matrix for
that estimate strategies and value functions by mod-
each possible move. All observation matrices are com-
eling sample paths. Monte Carlo methods are very ef-
binedintoabatch√ó5√ó54matrixasinputdataparta,
fective in episodic tasks to estimate the value function
where the batch size is the number of valid moves. We
by taking every-visit MC approach (Sutton 1998).
also encode the number of bombs played in the game
as a one-hot 1 √ó 15 matrix, indicating the number of 1. Generating sample trajectories using a speci-
bombs from 0 to 14 that have been played. A 1 √ó 3 fied policy œÄ: Starting from an initial state, simu-
matrix records the bid scores of the first, second, and lateusingthecurrentpolicyuntilaterminalstateis
third players (with -1 if they did not participate in the reached, generating a complete state-action-reward
bidding). The bid data and the bomb count are con- sequence.
catenated and repeated batch times to form a batch √ó 2. Calculating returns and updating Q(s,a) val-
18 matrix as data part b. Although there might be du- ues: For each state-action pair (s,a) in every tra-(a)
JOKER
3 4 5 6 7 8 9 T J Q K A 2 B R
1 1 0 0 0 0 0 0 0 1 0 0 0 1 0
1 1 0 0 0 0 0 0 0 0 0 0 0
1 1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0
REKOJ
(b) Residual block 1
Residual block 2
Residual block 3 ùë∏
ùíò
1 2 3
C C C
F F F ùíë
ùë∏
ùíç
Bomb_num (1, 15)
Bid_info (1, 3)
√óùüí
Figure 1: Encoding process of card combinations.
jectory, calculate the cumulative return and add it When updating the Q-Net, we do not directly min-
to the return list for that state-action pair. Use the imize the Mean Square Error, MSE(reward, predicted
averageofthesereturnstoupdatetheQ(s,a)value. Q-value), to update the Q-Net. Instead, we simultane-
3. Policy improvement: For each state, update the ouslyoptimizethewinningprobabilityp w(s,a),andthe
policy to select the action with the highest Q value Q-values Q (s,a) and Q (s,a) for winning and losing.
w l
in that state. We divide the training data D into two mutually ex-
clusive datasets for winning and losing. For outcome
œÄ(s)‚ÜêargmaxQ(s,a) u:
a
TheDMCmethodhasbeendemonstratedinDouzero D =D ‚à™D
w l
to achieve superior results in Doudizhu.
D ={(s,a,R ,u)|u=1}
w w
DMC with Probability and Value D ={(s,a,R ,u)|u=‚àí1}
l l
Factorization Q (s,a)istrainedusingD ,whileQ (s,a)istrained
w w l
Considering the significant gap between the distribu- using D . The winning probability p (s,a) is trained
l w
tion of winning scores and losing scores, we perform using {u} in D.
ValueFactorizationontheQ-value(Wang,Wu,andLai The final loss function is:
2023).GiventhattherearenotiesinDoudizhuandthe
outcomes of the game (win or lose) are mutually exclu- L=Œ± L +Œ± L
1 p 2 q
sive, we have: whereŒ± andŒ± aretwohyperparameterscontrolling
1 2
theweights.Thewinningprobabilityp (s,a)isderived
w
from the neural network output p(s,a):
Q(s,a)=p (s,a)Q (s,a)+(1‚àíp (s,a))Q (s,a)
w w w l
where p w(s,a) is the winning probability given (s,a), p (s,a)= p(s,a)+1
and Q (s,a) and Q (s,a) are the Q-values for winning w 2
w l
and losing, respectively. The loss function for the probability is:Observation Shape Description
action (1, 54) Actions to be computed by the neural network
The three one-hot vectors spliced together represent
num_cards_left (1, 54) the number of cards remaining with the landlord
(1, 20), landlord_up (1, 17), and landlord_down (1, 17)
my_handcards (1, 54) Player‚Äôs current hand
other_handcards (1, 54) The sum of the remaining two players‚Äô current hands
three_landlord_cards (1, 54) Landlord cards not yet played
landlord_played_cards (1, 54) Cards played by the landlord
landlord_up_played_cards (1, 54) Cards played by landlord_up
landlord_down_played_cards (1, 54) Cards played by landlord_down
The score called by the first player, divided by 3,
1st_bid (1, 54)
repeated 54 times
The score called by the second player, divided by 3,
2nd_bid (1, 54)
repeated 54 times
The score called by the third player, divided by 3,
3rd_bid (1, 54)
repeated 54 times
spring (1, 54) Whether spring bonuses can still be earned
card_play_action_seq (60, 54) History of cards played (contains 60 historical actions)
Table 2: Observation data during the card-playing phase.
CardModel BidModel
Layer
Input Out #blocks Input Out #blocks
Residual block 1 72√ó54 72√ó27 3 5√ó54 5√ó27 3
Residual block 2 72√ó27 144√ó14 3 5√ó27 10√ó14 3
Residual block 3 144√ó14 288√ó7 3 10√ó14 20√ó7 3
FC 1 2088 2048 / 140 256 /
FC 2 2048 512 / 256 256 /
FC 3 512 128 / 256 128 /
Out layer 128 3 / 128 3 /
Table 3: Parameters of the neural network.
outputs are not always equal, which introduces errors
L =MSE(p(s,a),u) in calculating Q(s,a). In this case, we directly choose
p
The loss function for the Q-value is:
themovewiththehighestwinningprobabilityp w(s,a).
If factors affecting the final reward still exist, we
prune the moves based on Q(s,a). We consider moves
L =|D w| MSE (Q (s,a),R )+ whose difference from maxQ(s,a) is within a certain
q |D| Dw w w percentage range œÅ = 0.05 as selectable moves, forming
the pruned set of selectable moves:
|D |
l MSE (Q (s,a),R )
|D| Dl l l (cid:12) (cid:12)Q(s,a)‚àímaxQ(s,a)(cid:12)
(cid:12)
When generating a strategy, we calculate Q(s,a) = A cut ‚àà{a|(cid:12) (cid:12) maxQ(s,a) (cid:12) (cid:12)<œÅ}
p (s,a)Q (s,a)+(1‚àíp (s,a))Q (s,a), and consider
w w w l Then, we choose the move with the highest winning
whether factors affecting the final reward still exist:
probability p (s,a) within A :
whether the spring bonus can no longer be obtained, w cut
and whether there are no bomb cards left in this game.
If these factors are excluded, theoretically Q (s,a) = a best =maxp w(s,a),a‚ààA cut
w
|Q (s,a)|,buttheabsolutevaluesoftheneuralnetwork We use the epsilon-greedy method to introduce ex-
lploration into the strategy œÄ(s) used to generate data. ‚Ä¢ ADP1 (Average Difference in Points 1): The
InappendixB,weutilizedthewinratemodeldouzero- averagedifferenceofpointsscoredpergamebetween
wpandtheexpectedvaluemodeldouzero-adpprovided AandB.Thebasepointis1.Eachbombwilldouble
byDouZerotoverifythatourproposedstrategygener- the score.
ationmethodoutperformsthe"choosingthemovewith ‚Ä¢ ADP2 (Average Difference in Points 2): The
the highest expectation" strategy. averagedifferenceofpointsscoredpergamebetween
AandB.Thebasescoreis1to3points,determined
EXPERIMENTS by the highest bid during the bidding phase. Each
bomb will double the score. Spring bonuses will also
In this chapter, we compare the performance of the Al-
double the score.
phaDoucard-playingmodel(CardModel)withDouzero
and Douzero Resnet. Douzero Resnet is the state-of- Additionally, we evaluate each position‚Äôs:
the-art Dou Dizhu AI based on the Douzero algorithm, ‚Ä¢ LP (Landlord Percentage): The number of
replacing the LSTM neural network in Douzero with games in which A became the Landlord Player di-
ResNet, significantly improving performance compared vided by the total number of games.
to Douzero. The weights and code for Douzero Resnet
‚Ä¢ DR (Draw Rate): The number of draw games di-
are open-sourced at https://github.com/Vincentzyx/
vided by the total number of games.
Douzero_Resnet. We also compare the AlphaDou bid
The test results are shown in Table 4.
model (Bid Model) with a supervised learning bid
For each test group, the Bid Model shows signifi-
model (Douzero Resnet Bid). The Douzero Resnet Bid
cant improvement in WP, ADP2, and LP compared
weusedisderivedfromDouzeroResnet:fixingtheland-
to Douzero Resnet Bid, while also achieving a lower
lord player‚Äôs hand, randomly distributing 1000 sets of
Draw Rate. In Dou Dizhu, the Landlord Player wins
farmer hands and landlord hands, loading the Douzero
double the rewards, so accurately determining whether
Resnet model for games, obtaining a mean score from
a player should become the Landlord Player is crucial
the results of 1000 games, using the hand as input,
for scoring. The Bid Model is more aggressive, tend-
and the mean score as the label for supervised learn-
ingtobecometheLandlordPlayermoreoften,whereas
ing. When applying the model, a threshold is set for
Douzero Resnet Bid is more conservative. One reason
bidding: a model output greater than -0.1 bids 1 point,
is that the Bid Model adjusts its bids by considering
greater than 0 bids 2 points, and greater than 0.1 bids
the bids of other players; when opponents bid low, the
3 points. The Douzero demonstration website https:
Bid Model may bid high even if the player‚Äôs hand is
//www.douzero.org/bid also has a bidding model, but
not exceptionally good but relatively better than the
its bidding method is not based on a 3-point system,
opponents‚Äô hands.
and it does not have an interface reserved for testing.
When two positions are replaced with Bid Models,
Our AI system is trained on a server with 4 Intel(R)
the ADP and LP of the position still using Douzero
Xeon(R) Gold 6330 CPUs @ 2.10GHz and a GeForce
Resnet Bid significantly decrease, indicating that the
RTX4090GPUintheUbuntu20.04operatingsystem.
moreaccuratejudgmentoftheBidModelsexploitsthe
The code is available in the supplement.
Douzero Resnet Bid.
Compare Bid Model to Douzero Resnet
Compare Card Model to Benchmarks with
Bid
Random Bidding Phase
Dou Dizhu has three players, and we catego-
ToevaluatetheperformanceoftheCardModel,wefol-
rize them into three positions‚Äîfirst, second, and
lowed the approach of (Jiang et al. 2019) and douzero
third‚Äîaccording to the order of bidding. To evalu-
(Zha et al. 2021), initiating a competition between the
ate the performance of the Bid model, we initially set
LandlordandthePeasants.Wereducevariancebyplay-
all three positions to a combination of Douzero and
ing each deck twice. Specifically, for two competing al-
DouzeroResnetBid,recordingthescoresforeachposi-
gorithms A and B, they will first play with A as the
tionafter4000games(controlgroup).Next,wesucces-
LandlordandBasthePeasantsforagivendeck.Then,
sively replace the Douzero Resnet Bid at each position
they swap roles, with A as the Peasants and B as the
with the Bid Model and conduct the same 4000 games
Landlord,andplaythesamedeckagain.Atotalof4,000
to observe whether the scores at each position improve
games were conducted. Considering that the Bid result
compared to the control group. Since Douzero‚Äôs card
is random, we set the initial score of the game to 2
playingisunaffectedbytheBidmodel,weuseDouzero
points for the Landlord‚Äôs win and 1 point for the Peas-
as the card-playing model in this experiment.
ants‚Äô win, with each bomb doubling the final score (we
Metrics. Following (Jiang et al. 2019), given an al- define this scoring method as ADP1). The Card Model
gorithm A and an opponent B, we use two metrics to
needs to decide the playing strategy based on the bid-
compare the performance of A and B:
ding process, and the random bidding process will lead
‚Ä¢ WP (Winning Percentage): The number of to a decline in model performance because the random
games won by A divided by the total number of testingdeckdistributiondeviatesfromthetrainingpro-
games. cess.1st position 2nd position 3rd position Draw
WP ADP2 LP WP ADP2 LP WP ADP2 LP DR
Control 0.361 -0.119 0.324 0.389 0.184 0.299 0.369 -0.065 0.255 0.123
1st 0.411 0.014 0.423 0.420 0.115 0.285 0.401 -0.129 0.243 0.049
2nd 0.387 -0.116 0.321 0.416 0.266 0.377 0.386 -0.150 0.224 0.078
3rd 0.387 -0.207 0.315 0.415 0.099 0.290 0.415 0.108 0.342 0.054
1st & 2nd 0.424 0.014 0.416 0.435 0.193 0.331 0.404 -0.207 0.223 0.030
1st & 3rd 0.418 -0.048 0.387 0.423 0.031 0.281 0.424 0.018 0.311 0.021
2nd & 3rd 0.400 -0.198 0.314 0.432 0.191 0.360 0.419 0.007 0.294 0.032
all 0.426 -0.035 0.384 0.436 0.127 0.324 0.421 -0.090 0.282 0.010
Table 4: Performance of Bid Model against Douzero Resnet Bid by playing 4,000 randomly sampled decks. The
ControlgroupmeansuseDouzeroResnetBidmodelsinallthe3positions.Foreachexperimentalgroup,wechanged
some of the positions to the Bid Model. Results where the experimental group outperforms the control group are
highlighted in boldface. The sum of WP is not 1 because two players win when the Peasants Team wins.
Table5showstheresults.Asofthecompletionofthis lord),thestrengthofDouzeroResnet(Peasant)ismuch
paper, RARSMSDou is the strongest publicly available weaker than that of the Card Model (Peasant). This
DouDiZhu model. In a 1000-game test with douzero may be because the Landlord side only needs to con-
using random bidding, it achieved a win rate of 0.582 sider confrontation, while the Peasant side needs to
and an ADP1 of 0.414. Although we did not directly consider cooperation, making it easier for the Landlord
testAlphaDouagainstRARSMSDou,bothweretested model to converge.
againstthebaselinedouzero.AlphaDouperformedbet-
ter than RARSMSDou in the random bidding test.
Random Bidding
WP ADP1 WP ADP2
Compare Card Model to benchmarks with
Card Model 0.5145 -0.0483 0.7850 4.6451
Bidding Phase Douzero Resnet 0.4903 -0.2088 0.7709 4.2965
We conducted another 4,000 matches and with the bid
model set to Bid Models. The game scores are divided Table 7: Performance of Card Model (Landlord) and
intoADP1andADP2.ADP1isconsistentwiththeone Douzero Resnet (Landlord) against Douzero (Peasant)
mentionedabove,andADP2iscalculatedbasedonthe by playing 4,000 randomly sampled decks.
results of the Bid Models. For example, if the landlord
wins with 3 points, the landlord scores 6 points, the
Case Study: Bid Model vs Douzero Resnet
peasants score 3 points, and each bomb will double the
final score. The results are shown in the table 6. Bid
TheCardModelstilldominatesallotheralgorithms. In these cases, we use the following abbreviations: ‚ÄúP‚Äù
Compared to the random Bidding Phase, the WP of for ‚ÄúPass‚Äù, ‚ÄúT‚Äù for card ‚Äú10‚Äù, ‚ÄúJ‚Äù for Jack, ‚ÄúQ‚Äù for
the Card Model against Douzero Resnet and Douzero Queen,‚ÄúK‚ÄùforKing,‚ÄúA‚ÄùforAce,‚ÄúB‚ÄùforBlackJoker,
hassignificantlyimproved.ThisindicatesthattheCard and ‚ÄúR‚Äù for Red Joker. Each action is represented as
Model can adjust its playing strategy based on the Bid ‚Äúposition: action,‚Äù where ‚Äúposition‚Äù can be ‚ÄúL‚Äù for
resultstoachievehigherreturns.Notably,withtheBid- Landlord, ‚ÄúD‚Äù for Peasant-Down, or ‚ÄúU‚Äù for Peasant-
dingPhase,theWPofDouzeroResnetagainstDouzero Up. For example, ‚ÄúL:TT‚Äù denotes the Landlord play-
also increased (0.5809 > 0.5702), but Douzero Resnet ingaPair(1010),and‚ÄúD:22‚ÄùindicatesPeasant-Down
does not adjust its playing strategy based on the bid playing a Pair (22). The actions are separated by com-
results. We believe this is because the bid results pro- mas (e.g., ‚ÄúL:J,D:Q,U:Pass‚Äù).
vided by the Bid Model are favorable to the landlord, Comparedtothresholdbidding,reinforcementlearn-
and if a model‚Äôs landlord strength is very strong, its ing bidding has a more flexible handling of different
overall win rate will correspondingly increase. bidding situations. Here, we analyze a hand with the
Table 7 shows the WP and ADP of the Card Model cards 333444569TTJJQKK2. The hand score given by
(Landlord) and Douzero Resnet (Landlord) against DouzeroResnetBidis-0.987,indicatingthatthishand
Douzero (Peasant). It can be seen that the strength is very weak. Firstly, the only high card is 2, and the
of Douzero Resnet (Landlord) is quite similar to that absence of 7, 8, A, B, and R means that there is a high
of the Card Model (Landlord). Therefore, after adjust- probability that other players‚Äô hands form bombs. Us-
ing the bid strategy, the overall win rate of Douzero ingDouzeroResnetBid,thechoicewouldbe‚Äú0points‚Äù.
Resnet against Douzero will increase. Correspondingly, For the same hand, the Bid Model gives different bid-
we find that although the strength of Douzero Resnet ding scores based on the bidding order. In different
(Landlord) is similar to that of the Card Model (Land- situations, the Bid Model‚Äôs bidding strategy varies, asB Card Model Douzero Resnet Douzero
A WP ADP1 WP ADP1 WP ADP1
Card Model - - 0.5224 0.1032 0.5970 0.4343
Douzero Resnet -0.4776 -0.1032 - - 0.5702 0.2688
Douzero 0.4030 -0.4343 0.4298 -0.2688 - -
Table 5: Performance of Card Model against Douzero Resnet and Douzero by playing 10,000 randomly sampled
decks with random bidding phase. Algorithm A outperforms B if WP is larger than 0.5 or ADP is larger than 0
(highlighted in boldface).
Card Model Douzero Resnet Douzero
WP ADP1 ADP2 WP ADP1 ADP2 WP ADP1 ADP2
Card Model - - - 0.5442 0.3150 0.7376 0.6167 0.5759 1.5852
Douzero Resnet 0.4558 -0.3150 -0.7376 - - - 0.5809 0.3140 0.9371
Douzero 0.3833 -0.5759 -1.5852 0.4191 -0.3140 -0.9371 - - -
Table6:PerformanceofCardModelagainstDouzeroResnetandDouzerobyplaying4,000randomlysampleddecks
with bidding phase. Algorithm A outperforms B if WP is larger than 0.5 or ADP is larger than 0 (highlighted in
boldface).
shown in Table 8. The Bid Model tends to bid 2 points hand strength. If the first bidder bids 2 points, it indi-
inthefirstposition,0pointsinthesecondposition,and catessomecardpower,reducing|Q |andincreasingthe
l
3 points in the third position (only if 2 points were bid winrate.Still,biddingisnotadvisableastheLandlord
in the first position). will likely lose to the first bidder. This analysis for the
Inthefirstposition,themodelbids2points,butthe second position is based on the rule that the Landlord
predicted winning and losing points differ significantly loses double the points compared to the Peasant. If the
from the results in the last column of the table. In the losspointsfortheLandlordandPeasantwerethesame,
table, the Landlord wins with 2 points. However, when the strategy could be to bid 3 points and become the
bidding2pointsinthefirstposition,themodeltendsto Landlord, especially if the first bidder bids 0 points.
predict that the Landlord will eventually be outbid by This strategy is common in professional JJ DouDiZhu
otherplayerswith3points.Althoughthehandisweak, tournaments where the Landlord and Peasant lose the
thepresenceoftheairplane333444makesthecardtype same points upon failure.
looktidy.Thisimpliesthatotherplayerswillhavefewer Inthethirdposition,themodeltendstobid3points,
smallcards.Whenaplayerhasveryfewsmallcardsand withQ approaching9points.Themodelbelievesthat
w
manyhighcards,bidding3pointscanyieldthegreatest highcardpower(Rocket)isintheLandlord‚Äôshand,and
value.Evenifthefirstpositionbids2points,theywon‚Äôt the remaining players‚Äô card types are not neat, making
become the final Landlord. If other players don‚Äôt feel it difficult to handle the airplane card type (333444)
confidentenoughtobid3points,itmeansthatthehigh andpotentialconsecutivepairs(99TTJJ,TTJJQQKK,
cardsareeitherwiththeLandlordorwithotherplayers etc.).Inthiscase,bidding3pointstobecometheLand-
but not in a neat formation, requiring many turns to lordcouldresultingainingabomborrocketalongwith
play out. In this case, a 2-point Landlord can still win, aSpring,cleverlyutilizingthebiddingpositiontoallow
preventingadrawandretainingthepotentialtoexploit weak card power to exploit even weaker card power.
weaker hands with weak card power.
Case Study: Card Model vs Douzero
Inthesecondposition,ifthefirstbidderdoesn‚Äôtshow
strong card power (less than 2 points) and the strong Resnet vs Douzero
cardpowerisn‚Äôtintheirownhand,itmustbewiththe Figure 2 shows the cards held by the landlord, the
third bidder. The model then evaluates the win rate as landlord‚Äôs next player, and the landlord‚Äôs previous
extremely low (‚âà 0.1). The model predicts |Q | ‚âà 6 player.Thelandlorddoesnothaveanyjokercards,but
l
and Q ‚âà 3, indicating that the third bidder is very their hand is well-structured and strong. The landlord
w
likely to have a bomb and will bid 3 points to become chooses to play 44, which brings the game to the first
the Landlord. With the bomb in the Landlord‚Äôs hand, critical point: the landlord‚Äôs next player Douzero plays
even if they sense defeat, they won‚Äôt use the bomb to KK, while Card Model and Douzero Resnet choose to
avoid doubling the loss. Thus, even winning yields only playAA.PlayingKKallowsthelandlordtoregaincard
3 points. Bidding 3 points to become the Landlord in rightswithAA,whereasplayingAApreventstheland-
this situation results in a significant negative return, lord from taking card rights.
while bidding 0 points or any other score could lead First, let‚Äôs analyze the scenario where the farmer
to a misjudgment by the Peasant teammate about the plays KK and the landlord regains card rights withPosition First Second Agent Win rate Q Q Q
w l
First / / 2 0.2724 4.4712 -5.7912 -2.9976
Second 0 / 0 0.1122 3.7488 -5.8030 -4.7328
Second 1 / 0 0.1203 3.8616 -6.0600 -4.8672
Second 2 / 0 0.2407 3.3096 -4.3200 -2.4816
Third 0 0 3 0.5154 9.3168 -7.7808 1.0296
Third 0 1 3 0.4742 9.4392 -7.8432 0.3528
Third 0 2 3 0.4378 8.5080 -7.5552 -0.5232
Third 1 0 3 0.4907 9.4536 -7.6920 0.7200
Third 1 2 3 0.4281 7.9008 -7.1016 -0.6792
Third 2 0 0 0.2477 2.2656 -2.7432 -1.5024
Table 8: The Bid Model‚Äôs strategy for bidding points in different situations
LAST HAND LAST HAND
LÔºö D LÔºö D: PASS U:PASS
L nw L nw
pU
droldna
Card Model Douzero Resnet Douzero
oD
droldnaL
pU
droldna
Card Model Douzero Resnet Douzero
oD
droldnaL
Landlord Landlord
Figure2:Landlord‚Äôs,landlord‚Äôsnextplayer‚Äôs,andland- Figure 3: Play scenarios after the landlord regains card
lord‚Äôs previous player‚Äôs hands rights
AA. In this situation, the landlord has the advantage, From the analysis above, it is clear that the farmer‚Äôs
with Douzero Resnet and Douzero opting to play 5557, choice to play KK leads to quick failure. Only by play-
while Card Model chooses to play 888999TJ, as shown ing AA can the farmer have a chance to win. Card
in Figure 3. In this scenario, if the opponent plays Model and Douzero Resnet are more sensitive to po-
QQQ5 after 5557, the only way to regain card rights tential dangers. After the landlord‚Äôs next player plays
isbyplaying222K.Thelandlord‚Äôspreviousplayerthen AA and gains card rights, the game reaches the second
usestherockettoobtainforcedcardrights,leavingthe critical point, as shown in Figure 4. Douzero Resnet
landlord with 7888999K, resulting in failure and bomb opts to play 89TJQ to gain card rights before playing
penaltyforthelandlord.Conversely,CardModel‚Äôsplay 3336, while Card Model directly plays 3336.
of 888999TJ, an airplane type, is a rare hand. The Analyzing Douzero Resnet‚Äôs play, playing 89TJQ re-
opponent has a low probability of suppressing it. If duces hand complexity. When 3336 is played next,
the farmer uses the rocket to gain forced card rights, the landlord plays 555T, leaving the landlord‚Äôs next
thelandlord‚Äôsremaininghandis5557K222.Withthree player with 777KK2. If they play 7772, leaving KK,
2s being the highest cards, the landlord can still win the landlord can play 2227. Even if the landlord‚Äôs pre-
and receive a bomb reward. If the farmer opts to pass vious player uses the rocket for forced card rights, the
against the airplane type, the landlord can play 5557 landlord‚Äôs remaining AA will be the highest cards and
and have 222K left. The farmer can only prevent the win, earning the bomb reward. Returning to 777KK2,
landlord from playing all their cards in the next hand if 777K is played, leaving K and 2, it can secure a
by using the rocket. However, using the rocket leaves win. However, leaving two single cards is unwise, es-
the landlord with three 2s, and the farmer cannot win. pecially when neither K nor 2 is the highest card (the
Thefarmercanonlyavoidusingtherockettoevadethe joker hasn‚Äôt appeared yet). This incomplete informa-
bomb penalty. The Card Model landlord values card tion game makes the 777K strategy unlikely, leading to
rights more and plays more conservatively, leading to a the farmer‚Äôs almost certain failure.
higher win rate. Now, consider Card Model‚Äôs play of directly playing3336.Afterthelandlordplays555T,thelandlord‚Äôsnext models trained in complex environments can also per-
player is inclined to play 777K. This is because the re- form excellently in more simplified environments.
maining K can combine with 89TJQ to form 89TJQK,
retainingthesingle2,ensuringthefarmer‚Äôsvictory.The
CardModelfarmercanmaintainhanddiversityincom-
plex situations, keeping more possibilities open, which
also results in a higher win rate.
LAST HAND
DÔºö U: PASS L:PASS
L nw
pU
droldna
Card Model Douzero Resnet
oD
droldnaL
Landlord
Figure 4: Second critical point in the game
Conclusion
The game of DouDiZhu is an extremely challenging in-
complete information game. It has a vast state/action
space and unique characteristics involving reasoning
about competition and cooperation, making the game
particularly difficult to solve. Research on DouDiZhu
typicallysimplifiesthegamebynotconsideringthebid-
ding phase and the "spring" bonus, as including these
factors increases the variance in rewards, making the
model harder to converge. Additionally, the inclusion
ofbiddingcancausedeviationsinthecarddistribution
compared to when bidding is not included.
Thispaperfirstincorporatesfactorslikebiddingand
the spring bonus to make the research environment
more closely resemble the actual DouDiZhu game en-
vironment. Secondly, it modifies the Deep Monte Carlo
algorithm framework, using reinforcement learning to
obtain a neural network that simultaneously estimates
win rates and expectations. The action space is pruned
using expectations, and strategies are generated based
on win rates. This modification allows the DMC algo-
rithm to produce strategies that are not solely depen-
dentonvalue(expectation)butalsoconsiderwinrates,
resulting in a state-of-the-art (SOTA) DouDiZhu rein-
forcement learning model, which we named AlphaDou.
We compared AlphaDou with the baseline program
DouZero, achieving a win rate of 0.6167 in an envi-
ronment with bidding. Even when there are differences
betweenthetrainingenvironmentwithbiddingandthe
testing environment without bidding, AlphaDou still
achieved a win rate of 0.5970 and an average score per
game of 0.4343, making it the SOTA RL model. RLA.The Combinations and Ranks of Cards
One of the challenges in the game of DouDiZhu is the vast state/action space, which includes numerous card combi-
nations. For certain categories, players can choose a "kick-out" card, which can be any card from their hand, directly
leading to a large action space. For the landlord player, the winning condition is to play all their cards, while farmer
players do not always need to play all their cards; their teammate clearing their hand also signifies victory. This re-
quiresconsideringusinglargercardsaskick-outcardsandretainingsmallercardstocoordinatewiththeteammate‚Äôs
plays.Playersneedtocarefullystrategizetheirmovestowinthegame.TheclassificationofcardtypesinDouDiZhu
isshownintheTable9.Notethat"Bombs"and"Rockets"breakcategoryrulesandcandominateallothercategories.
Category of Actions Description Num
Pass Not play cards 1
Any single card.
Solo (F) 15
3<4<5<6<7<...<K<A<2<B<R
Two same cards.
Pair (F) 13
33<44<55<66<...<KK<AA<22
Three same cards.
Trio (F) 13
333<444<555<...<KKK<AAA<222
A Trio and a Solo.
Trio-Solo (F) 182
333? <444?<555?<...<AAA? <222?
A Trio and a Pair.
Trio-Pair (F) 156
333* <444*<555*<...<AAA* <222*
Four same cards.
Bomb (F) 13
3333<4444<5555<...<AAAA <2222
Rocket (F) Black and Red Jokers 1
Bomb with 2 additional Solos.
Quad-Solo (F) 1326
3333??<4444??<...AAAA??< <2222??
Bomb with 2 additional Pairs.
Quad-Pair (F) 856
3333**<4444**<...AAAA**< <2222**
Least 5 consecutive cards
Chain-Solo (V) 34567<45678<...<9TJQK<TJQKA
36
345678<456789<...<89TJQK<9TJQKA
Least 3 consecutive cards
Chain-Pair (V) 334455<445566<...<QQKKAA 52
33445566<44556677<...<JJQQKKAA
Least 2 consecutive Trios
Plane (V) 333444<444555<...<KKKAAA 45
333444555<444555666<...<QQQKKKAAA
Plane with each Trio has a distinct Solo.
Plane-Solo (V) 333?444?<444?555?<...<KKK?AAA? 21822
333?444?555?<444?555?666?<...<QQQ?KKK?AAA?
Plane with each Trio has a distinct Pair.
Plane-Pair (V) 333*444*<444*555*<...<KKK*AAA* 2939
333*444*555*<444*555*666*<...<QQQ*KKK*AAA*
Table 9: Actions and Their Ranks in Doudizhu. Doudizhu uses a 54-card deck, which includes 3, 4, 5, 6, 7, 8, 9,
10 (T), Jack (J), Queen (Q), King (K), Ace (A), 2, Black Joker (B) and Red Joker (R). Suits are irrelevant. ‚Äú?‚Äù
and ‚Äú*‚Äù denote any Solo or Pair, respectively. ‚ÄúF‚Äù and ‚ÄúV‚Äù denote fixed-length action and variable-length action,
respectively. This table is cited from (Luo and Tan 2024).B.Mixture of the Policy Makes the AI Stronger
Douzero has open-sourced two model weights: douzero-wp, which uses win rate as the reward, and douzero-adp,
which uses expectation as the reward. Douzero generates strategies based on the maximum output of douzero-adp.
We propose the following two methods to consider both douzero-wp and douzero-adp models simultaneously to
generate strategies:
1. Bomb check: Check for factors that influence the final reward. If none exist, choose the move with the highest
win rate based on douzero-wp output. Factors influencing the final reward include spring reward and bomb reward.
Douzero does not consider the spring reward, so this step is to determine the presence of a bomb.
2. Mixed strategy (Mix): Prune moves based on the expectation derived from douzero-adp. We consider moves
withanexpectationdifferencewithinacertainpercentagerange(œÅ=0.05)fromthemaximumexpectationasviable
moves. Then, select the move with the highest win rate among the viable moves.
We can derive four different RL models for generating strategies: douzero, douzero with only Bomb check (Bomb
check), douzero with only mixed strategy (Mix), and douzero with Bomb check followed by mixed strategy (Bomb
check & Mix). We tested the performance of these four models against douzero in fixed 4000 game scenarios at
different positions (landlord, farmer). The specific results are displayed in the Table 10. It can be seen that both
Bomb Check and Mixed strategy yield better strategies than the standalone douzero-adp. Bomb Check followed by
Mixed strategy achieves the best strategy.
Landlord Farmer Overall
WP ADP1 WP ADP1 WP ADP1
Douzero 0.434 -0.391 0.566 0.391 0.5 0.0
Bomb check 0.442 -0.362 0.578 0.449 0.509 0.043
Mix 0.446 -0.331 0.581 0.460 0.513 0.064
Bomb check & mix 0.452 -0.306 0.586 0.482 0.519 0.088
Table 10: Performance of Different StrategiesAcknowledgment and Hassabis, D. 2016. Mastering the game of Go
Thanks to juidxsrs15746 (GITHUB ID) for com-
with deep neural networks and tree search. Nature,
529(7587): 484‚Äì489.
putational resource support. Thanks to Vincentzyx
(GITHUB ID), the training framework was modified Silver,D.;Hubert,T.;Schrittwieser,J.;Antonoglou,I.;
based on his open source project https://github.com/ Lai,M.;Guez,A.;Lanctot,M.;Sifre,L.;Kumaran,D.;
Vincentzyx/Douzero_Resnet. Graepel, T.; Lillicrap, T.; Simonyan, K.; and Hassabis,
D. 2017. Mastering Chess and Shogi by Self-Play with
References a General Reinforcement Learning Algorithm.
Bouzy,B.;Rimbaud,A.;andVentos,V.2020.Recursive Sutton, R. S. 1998. Reinforcement learning. Adaptive
computation and machine learning series. Cambridge,
MonteCarloSearchforBridgeCardPlay.In2020IEEE
Massachusetts: MIT Press. ISBN 9780262257053. In-
Conference on Games (CoG). IEEE.
cludes bibliographical references (p. [291]-312) and in-
Jiang, Q.; Li, K.; Du, B.; Chen, H.; and Fang, H. 2019.
dex. - Description based on PDF viewed 12/23/2015.
DeltaDou:Expert-levelDoudizhuAIthroughSelf-play.
Vinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Math-
InProceedingsoftheTwenty-EighthInternationalJoint
ieu, M.; Dudzik, A.; Chung, J.; Choi, D. H.; Pow-
Conference on Artificial Intelligence, IJCAI-2019. In-
ell, R.; Ewalds, T.; Georgiev, P.; Oh, J.; Horgan, D.;
ternational Joint Conferences on Artificial Intelligence
Kroiss, M.; Danihelka, I.; Huang, A.; Sifre, L.; Cai,
Organization.
T.; Agapiou, J. P.; Jaderberg, M.; Vezhnevets, A. S.;
Li, J.; Koyamada, S.; Ye, Q.; Liu, G.; Wang, C.; Yang,
Leblond,R.;Pohlen,T.;Dalibard,V.;Budden,D.;Sul-
R.;Zhao,L.;Qin,T.;Liu,T.-Y.;andHon,H.-W.2020.
sky, Y.; Molloy, J.; Paine, T. L.; Gulcehre, C.; Wang,
Suphx: Mastering Mahjong with Deep Reinforcement
Z.; Pfaff, T.; Wu, Y.; Ring, R.; Yogatama, D.; W√ºn-
Learning.
sch,D.;McKinney,K.;Smith,O.;Schaul,T.;Lillicrap,
Luo, Q.; and Tan, T.-P. 2024. RARSMSDou: Mas- T.; Kavukcuoglu, K.; Hassabis, D.; Apps, C.; and Sil-
ter the Game of DouDiZhu With Deep Reinforcement ver, D. 2019. Grandmaster level in StarCraft II using
Learning Algorithms. IEEE Transactions on Emerging multi-agent reinforcement learning. Nature, 575(7782):
Topics in Computational Intelligence, 8(1): 427‚Äì439. 350‚Äì354.
Luo,Q.;Tan,T.-P.;Su,Y.;andJin,Z.2024.MDou:Ac- Wang, H.; Wu, H.; and Lai, G. 2023. WagerWin: An
celerating DouDiZhu Self-Play Learning Using Monte- Efficient Reinforcement Learning Framework for Gam-
CarloMethodWithMinimumSplitPruningandaSin- bling Games. IEEE Transactions on Games, 15(3):
gle Q-Network. IEEE Transactions on Games, 16(1): 483‚Äì491.
90‚Äì101. You, Y.; Li, L.; Guo, B.; Wang, W.; and Lu, C. 2019.
Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lilli- Combinational Q-Learning for Dou Di Zhu.
crap,T.P.;Harley,T.;Silver,D.;andKavukcuoglu,K. Zha, D.; Lai, K.-H.; Cao, Y.; Huang, S.; Wei, R.; Guo,
2016. Asynchronous Methods for Deep Reinforcement J.; and Hu, X. 2019. RLCard: A Toolkit for Reinforce-
Learning. ICML 2016. ment Learning in Card Games.
Mnih,V.;Kavukcuoglu,K.;Silver,D.;Rusu,A.A.;Ve- Zha, D.; Xie, J.; Ma, W.; Zhang, S.; Lian, X.; Hu, X.;
ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; and Liu, J. 2021. DouZero: Mastering DouDizhu with
Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beattie, Self-Play Deep Reinforcement Learning.
C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.;
Zhao, E.; Yan, R.; Li, J.; Li, K.; and Xing, J. 2022a.
Wierstra, D.; Legg, S.; and Hassabis, D. 2015. Human-
AlphaHoldem: High-Performance Artificial Intelligence
level control through deep reinforcement learning. Na- for Heads-Up No-Limit Poker via End-to-End Rein-
ture, 518(7540): 529‚Äì533.
forcement Learning. Proceedings of the AAAI Confer-
OpenAI; :; Berner, C.; Brockman, G.; Chan, B.; Che- ence on Artificial Intelligence, 36(4): 4689‚Äì4697.
ung,V.;Dƒôbiak,P.;Dennison,C.;Farhi,D.;Fischer,Q.;
Zhao,Y.;Zhao,J.;Hu,X.;Zhou,W.;andLi,H.2022b.
Hashme,S.;Hesse,C.;J√≥zefowicz,R.;Gray,S.;Olsson,
DouZero+:ImprovingDouDizhuAIbyOpponentMod-
C.;Pachocki,J.;Petrov,M.;d.O.Pinto,H.P.;Raiman,
eling and Coach-guided Learning.
J.; Salimans, T.; Schlatter, J.; Schneider, J.; Sidor, S.;
Zhao, Y.; Zhao, J.; Hu, X.; Zhou, W.; and Li, H. 2024.
Sutskever, I.; Tang, J.; Wolski, F.; and Zhang, S. 2019.
Full DouZero+: Improving DouDizhu AI by Opponent
Dota2withLargeScaleDeepReinforcementLearning.
Modeling, Coach-guided Training and Bidding Learn-
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.;
ing. IEEE Transactions on Games, 1‚Äì13.
and Klimov, O. 2017. Proximal Policy Optimization
Algorithms.
Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre,
L.;vandenDriessche,G.;Schrittwieser,J.;Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S.;
Grewe, D.; Nham, J.; Kalchbrenner, N.; Sutskever, I.;
Lillicrap, T.; Leach, M.; Kavukcuoglu, K.; Graepel, T.;