BECAUSE:
Bilinear Causal Representation for Generalizable
Offline Model-based Reinforcement Learning
HaohongLin1,WenhaoDing1,JianChen1,LaixiShi2,JiachengZhu3,BoLi4,DingZhao1
1CMU, 2Caltech, 3MIT, 4UChicago&UIUC
{haohongl, wenhaod, dingzhao}@andrew.cmu.edu
Abstract
Offlinemodel-basedreinforcementlearning(MBRL)enhancesdataefficiencyby
utilizingpre-collecteddatasetstolearnmodelsandpolicies,especiallyinscenarios
where exploration is costly or infeasible. Nevertheless, its performance often
suffersfromtheobjectivemismatchbetweenmodelandpolicylearning,resultingin
inferiorperformancedespiteaccuratemodelpredictions. Thispaperfirstidentifies
the primary source of this mismatch comes from the underlying confounders
presentinofflinedataforMBRL.Subsequently,weintroduceBilinEarCAUSal
rEpresentation(BECAUSE),analgorithmtocapturecausalrepresentationforboth
statesandactionstoreducetheinfluenceofthedistributionshift,thusmitigating
theobjectivemismatchproblem. Comprehensiveevaluationson18tasksthatvary
indataqualityandenvironmentcontextdemonstratethesuperiorperformanceof
BECAUSEoverexistingofflineRLalgorithms. Weshowthegeneralizabilityand
robustnessofBECAUSEunderfewersamplesorlargernumbersofconfounders.
Additionally,weoffertheoreticalanalysisofBECAUSEtoproveitserrorbound
andsampleefficiencywhenintegratingcausalrepresentationintoofflineMBRL.
1 Introduction
OfflineReinforcementLearning(RL)hasshowngreatpromiseinlearningdirectlyfromhistorically
collected datasets, especially in scenarios where active interaction is expensive or infeasible [1].
Specifically,offlinemodel-basedreinforcementlearning(MBRL)[2,3,4],learningpolicieswithan
estimatedworldmodel,generallyperformbetterthantheirmodel-freecounterpartsinlong-horizon
taskssuchasself-drivingvehicles[5],robotics[6],andhealthcare[7]. However,offlineRLsuffers
fromdistributionshiftsincetherolloutdatacouldsamplefromsomeunknownbehaviorpoliciesthat
aresub-optimalorfromslightlydifferentenvironmentscomparedtothedeploymenttime[8].
Althoughidentifyingdistributionshiftissues,many Datacollectionwithbehavior policy
ofthecurrentofflineMBRLworksfailtomodelthe (Unknown)
shiftinenvironmentdynamics,whichisubiquitous DataBuffer Model Policy test Environment
andcouldcausecatastrophicfailureoftrainedpolicy {ğ‘ ,ğ‘,ğ‘ !,ğ‘Ÿ} ğ‘‡((ğ‘ !|ğ‘ ,ğ‘) ğœ‹(ğ‘|ğ‘ ) ğ‘‡(ğ‘ â€²|ğ‘ ,ğ‘)
ataslightlydifferentdeploymentstage. Furthermore,
Objective Mismatch
sincethelearningobjectivesoftheworldmodelsand
lowmodellossâ‰  success lowmodellossâ‰ˆ success
policies are isolated from each other, a significant Success Ourmethod
challengeinofflineMBRLisobjectivemismatch[9, Failure
10]problem(showninFigure1):modelsthatachieve Model Loss Model Loss
a lower training loss are not necessarily better for
controlperformance. Forexample,inlong-horizon Figure1: Theobjectivemismatchproblem.
planningtasks,therewardissparseyetthepredictionaccuracyofthemodelmaydecayasthehorizon
Preprint.Underreview.
4202
luJ
51
]GL.sc[
1v76901.7042:viXraenlargesandcompoundingerroraccumulates. Previousworks[9,10]haveattemptedtoreducesuch
objectivemismatchbyjointlylearningthemodelandpolicy. However,theperformanceissuboptimal
intheabsenceoftheunderlyingcauseofobjectivemismatch[8].
Inthiswork,weidentifythattheobjectivemismatchbetweenmodelestimationandpolicylearning
comesfromtwosourcesofdistributionshiftinofflineMBRL:(1)shiftbetweentheonlineoptimal
policyandofflinesub-optimalbehaviorpolicies,and(2)shiftbetweenthedatacollectionenvironment
andonlinetestingenvironments. Unlikehumans,whomakedecisionsbasedonreasoningovertask-
relevant factors, models in offline RL memorize correlations without learning the causality. The
sub-optimalbehaviorpoliciesintroducespuriouscorrelations[11]betweenactionsandstates,making
themodelmemorizespecificactions.Whenonlinetestingenvironmentsdifferfromthedatacollection
environment,themodelcouldoverfitspuriouscorrelationsinthestateandfailtogeneralizetounseen
states. Basedontheanalysisoftheabovemismatch,ourworkdiffersfrompreviousworkofcausal
model-basedRL[12,13]inthatwemodelcausalityinbothmodelandpolicylearning. Weaimto
avoidspuriouscorrelationbydiscoveringunderlyingstructuresbetweenabstractedstatesandactions.
Toalleviateobjectivemismatchandgeneralizewell,weintroducetheBilinEarCAUSalrEpresenta-
tion(BECAUSE)thatintegratesthecausalrepresentationinbothworldmodellearningandplanning
ofMBRLagents. InspiredbypreliminaryworksthatusebilinearMDPstocapturethestructural
representationinMBRL[14],wefirstapproximatethecausalrepresentationtocapturethelow-rank
structureintheworldmodel,thenusethislearnedrepresentationtofacilitateplanningbyquantifying
theuncertaintyofsampledtransitionpairs. Consequently,wefactorizethespuriouscorrelationsand
learnaunifiedrepresentationforboththeworldmodelandplanner.
Insummary,thecontributionofthispaperisthreefold:
â€¢ WeformulateofflineMBRLintothecausalrepresentationlearningproblem,highlightingthetight
connection between structural causal models and low-rank structures in MDPs. To the best of
ourknowledge, thisisthefirstworkthatsystematicallyrevealstheconnectionbetweencausal
representationlearningandBilinearMDPs.
â€¢ WeproposeBECAUSE,anempiricalcausalrepresentationframework,basedontheaboveformu-
lation. BECAUSEfirstlearnsacausalworldmodel,thenfostersthegeneralizabilityofofflineRL
agentsbyquantifyingtheuncertaintyofthestatetransition,whichfacilitatesconservativeplanning
tomitigatetheobjectivemismatch.
â€¢ Weprovideextensiveempiricalstudiesandperformanceanalysisintasksofmultipledomainsto
demonstratethesuperiorityofBECAUSEoverexistingbaselines,whichillustratesitspotentialto
improvethegeneralizabilityandrobustnessofofflineMBRLalgorithms.
2 ProblemFormulation
Toalleviatetheobjectivemismatchproblemandthedegradedperformancecausedbythespurious
correlation,wefirstprovideournovelformulationoflearningtheunderlyingcausalstructuresof
MarkovDecisionProcess(MDP)underthebilinearMDPsetting,thenintroducethecausaldiscovery
forMDPwithconfounders.
2.1 Preliminary: MDPandBilnearMDP
(cid:8) (cid:9)
Wedenoteanepisodicfinite-horizonMDPbyM = S,A,T,H,r ,whichiscomposedofstate
spaceS,actionspaceA,asetoftransitionfunctionsT,planninghorizonH andrewardfunctionr
associatedwithtaskpreferences. Withoutlossofgeneralityinmanyreal-worldpractices,weassume
thattherewardfunctionisboundedbyr âˆˆ [0,1],âˆ€h âˆˆ [H]. Specifically,weareinterestedina
h
goal-conditionedrewardsetting,whereâˆ€g âˆˆS,r(s,a;g)=1ifandonlyifs=g.
Given a policy Ï€ and the state-action pair (s,a) âˆˆ S Ã—A, we then define the state-action value
(cid:104) (cid:105)
functioninthetimestephasQÏ€(s,a)=E (cid:80)H r (s ,a )|s =s,a =a ,andthevaluefunc-
h Ï€ i=h i i i h h
(cid:104) (cid:105)
tionVÏ€(s)=E (cid:80)H r (s ,a )|s =s . TheexpectationE hereisintegratedintorandomness
h Ï€ i=h i i i h Ï€
throughoutthetrajectory,whichisessentiallyinducedbytherandomactionofthepolicya âˆ¼Ï€(Â·|s )
i i
andthetime-homogeneoustransitiondynamicsoftheenvironments âˆ¼T(Â·|s ,a ),âˆ€iâˆˆ[h,H].
i+1 i i
Intheofflinedataset,thedatarolloutscanbeseenasgeneratedbysome(mixed)behaviorpolicyÏ€ ,
Î²
resultinginadatasetDwithintotalnsamples{s ,a ,sâ€²,r } .
i i i i 1â‰¤iâ‰¤n
2Definition1(BilinearMDP[14]). Foreach(s,a) âˆˆ S Ã—A,sâ€² âˆˆ S, wehavethecorresponding
featurevectorÏ•(Â·,Â·):R|S|Ã—R|A| â†’Rd,Âµ(Â·):R|S| â†’Rdâ€². WithsomecorematrixM âˆˆRdÃ—dâ€²,
wecanrepresentthetransitionfunctionkernelT(Â·|Â·,Â·)as
âˆ€s,a,sâ€² âˆˆSÃ—AÃ—S, T(sâ€²|s,a)=Ï•(s,a)TMÂµ(sâ€²), (1)
whereÏ•(s,a)andÂµ(sâ€²)areembeddingfunctionsthatmaptheoriginalstateandactiontothelatent
space,M isthecorematrixthatmodelsthetransitionrelationshipbetweentheprevioustimestep
andnexttimestepinthelatentspace. Suchalineardecompositioninthetransitiondynamicsallows
ustoembedstructuresofthetransitionmodelwithoutthelossofgeneralfunctionapproximation
capabilitiestoderivestateandactionrepresentations.
2.2 ActionStateConfoundedMDP
We consider the existence of confounders in Causal Path State Nodes Confounders
the MDP to represent the offline data collec- Confounder Path Action Nodes
tionprocess,anddefineaction-stateconfounded ğ‘  ğ‘ â€² ğ‘¢ ğ‘¢ !
MDP(ASC-MDP):
ğ‘  ğ‘ â€² ğ‘  ğ‘ â€²
Definition2(ASC-MDP). Besidesthecompo-
(cid:8) (cid:9)
nentsinstandardMDPsM= S,A,T,H,r ,
ğ‘ ğ‘¢ ğ‘ ğ‘¢ ğ‘ ğ‘¢ â€²
" !
weintroduceasetofunobservedconfounders
u. InASP-MDP,confoundersarefactorizedas (a) Confounded MDP (b) SC-MDP (c) ASC-MDP (Ours)
u = {u ,u } , where u âˆˆ U denotes Figure2: ComparisonofourASC-MDPwithtwoex-
Ï€ c 1â‰¤hâ‰¤H Ï€
the confounders between s and a âˆ¼ Ï€ (s) in- istingformulations.
Î²
ducedbybehaviorpolicies,andu âˆˆU denotestheconfounderswithinthestate-actionpairsofthe
c
environmenttransition,thatis,theinherentstructurebetween(s,a)andsâ€². Hereweassumeatime-
invariantconfounderdistributionuâˆ¼P (Â·),âˆ€hâˆˆ[H],whichisacommonassumption[15,16,17]
u
The resulting causal relationship of ASC-MDP is demonstrated in Figure 2. Originating from
theoriginalMDP,ASC-MDPisdifferentfromtheConfoundedMDP[18]andState-Confounded
MDP(SC-MDP)[19]inthatitmodelsboththespuriouscorrelationbetweenthecurrentstatesand
thecurrentactiona,aswellasthosebetweenthenextstatesâ€²and(s,a). Yet,confoundedMDPand
SC-MDPonlymodelpartofthepossibleconfoundersbetweenstatesandactions. Thefactorization
oftheconfounderinASC-MDPalignswiththesourceofspuriouscorrelationinofflineMBRL.
3 ProposedMethod: BECAUSE
We propose BECAUSE, our core methodology for modeling, learning, and applying our causal
representations for generalizable offline MBRL. Section 3.1 models the basic format of causal
representationsandanalyzestheirproperties. Section3.2givesacompactwaytolearnthecausal
representationÏ•(s,a)andÂµ(sâ€²),aswellasthecoremaskestimationM. Section3.3utilizesthese
learnedcausalrepresentationsinbothworldmodellearningandMBRLplanningfromofflinedatasets.
3.1 CausalRepresentationforASC-MDP
Inthepresenceofahiddenconfounderu,wemodeltheconfounderbehindthetransitiondynamics
asalinearconfoundedMDP[18]:
T(sâ€²|s,a,u)=Ï•(cid:101)(s,a,u)TÂµ(sâ€²), (2)
whereu âˆ¼ P u(Â·). InspiredbytheBilinearMDPinDefinition1, wedecomposeÏ•(cid:101)(s,a,u)intoa
confounder-awarecorematrixM(u)andafeaturemappingÏ•(s,a),whichfactorizetheinfluenceof
theconfounders. Giventhefactorizationofconfounderu={u ,u }inDefinition2,wederivevia
c Ï€
d-separationinthegraphicalmodelinFigure2thatsâ€² âŠ¥âŠ¥u |{s,a,u }. Asaresult,weonlyneedto
Ï€ c
considertheconfounderu fromtheenvironmentwhendecomposingthetransitionmodel:
c
T(sâ€²|s,a,u)=T(sâ€²|s,a,u )=Ï•(s,a)TM(u )Âµ(sâ€²). (3)
c c
(cid:20) 0dÃ—d M (cid:21)
Definition3(ConstructionofcausalgraphG). InASC-MDP,G= . forall(sparse)
0dâ€²Ã—d 0dâ€²Ã—dâ€²
corematrixM,thecausalgraphGisbipartite,thusâˆ€G,GâˆˆDAG.
3Definition3revealstheconnectionbetweenthecorematrixM andcausalgraphG,asisformulated
in the ASC-MDP. To reduce the influence of u and estimate the unconfounded transition model
T(sâ€²|s,a),onewayistoidentifythecausalstructuresinducedbytheconfounderu forthetransition
c
dynamics[20]. Existingmethodsindifferentiablecausaldiscovery[21,22,23]transformcausal
discoveryonsomecausalgraphG,intoamaximumlikelihoodestimation(MLE)withregularization:
G(cid:98) =argmaxlogp(D;Ï•,Âµ,G)âˆ’Î»|G| =â‡’ M(cid:99)=argmaxlogp(D;Ï•,Âµ,M)âˆ’Î»|M|, (4)
GâˆˆDAG M
Sinceinourcase,M isasub-matrixofthecausalgraphG. GiventheDefinition3,Gautomatically
satisfies the formulation of ASC-MDP, thus discovering G is essentially estimating the sparse
submatrixM withoutDAGconstraints: M(cid:99)=argmax logp(D;Ï•,Âµ,M)âˆ’Î»|M|. Weelaborate
M
Definition3andshowtherelationshipbetweencorematrixM andcausalgraphGinAppendixA.2.
WealsomaketheassumptionthatthesparseGandM remaininvariantwithdifferentenvironment
confoundersintheofflinetrainingandonlinetesting.
Assumption1(Invariantcausalgraph). WedenotethecausalgraphGunderconfounderuasG(u).
ThegeneralizationproblemthatweaimtosolvesatisfiestheinvarianceinthecausalgraphG(u ),
c
whereG(u )=G(uâ€²),M(u )=M(uâ€²),u anduâ€² aretheconfoundersintrainingandtesting.
c c c c c c
Remark1. Theassumption1canalsobeinterpretedastaskindependencein [24],invariantstate
representation[25],orinvariantactioneffectin[26].
3.2 LearningCausalRepresentationfromOfflineData
WefirstlearnthecausalworldmodelT(sâ€²|s,a) Uncertainty
start
Â·Â·Â·
inthepresenceofconfoundersuintheoffline
datasets. AsformulatedinASC-MDP2,there Breakcorrelation Planning â€¦ Â·Â·Â·
are two sets of confounders: u and u . To Â·Â·Â· goal
Ï€ c ğ‘ ,ğ‘
estimateanunconfoundedtransitionmodeland ğœ™(â‹…,â‹…)
removetheeffectofconfounder,wefirstremove
ğ‘¢
theimpactofu cwhichcomesfromthedynam- O Buffl fi fn ee
r
ğ‘€)
ics shift by estimating a batch-wise transition ğ‘‡"(ğ‘ !|ğ‘ ,ğ‘) ğ¸!(s,a)
matrixM(u ),thenweapplyareweightingfor- ğ‘ â€² World Model UncertaintyQuantifier
c ğœ‡(â‹…)
mulatodeconfoundu inducedbythebehavior
Ï€
policiesandmitigatethemodelobjectivemis- Figure 3: BECAUSElearnsacausality-awarerepre-
match. sentationfromthebufferandusesitinboththeworld
model and uncertainty quantification to obtain a pes-
As discussed in Definition 3, we only need to
simisticplanningpolicy.
optimizethepartoftheparametersofthecausal
graph G, i.e. M. Thus, we can remove the constraints in (4), then transform the original causal
discoveryproblemintoaregularizedMLEproblemasfollows:
minL (M)=min(âˆ’logp(D;Ï•,Âµ,M)+Î»|M|)
mask
M M
=min(cid:0)E âˆ¥ÂµT(sâ€²)Kâˆ’1âˆ’Ï•T(s,a)Mâˆ¥2 + Î»âˆ¥Mâˆ¥ (cid:1) . (5)
(s,a,sâ€²)âˆˆD Âµ 2 0
M (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
WorldModelLearning SparsityRegularization
whereK :=(cid:80) Âµ(sâ€²)Âµ(sâ€²)T isaninvertiblematrix. Thederivationofequation(5)iselaborated
Âµ sâ€²âˆˆS
in Appendix A.3. In practice, we use the Ï‡2-test for discrete state and action space and the fast
ConditionalIndependentTest(CIT)[27]forcontinuousvariablestoestimateeachentryinthecore
matrixM. WeregularizethesparsityofM bycontrollingthep-valuethresholdinCITandprovidea
moredetailedimplementationinAppendixC.1.
Estimatingthecoremaskprovidesamoreaccuraterelationshipbetweenstateandactionrepresen-
tations,andwefurtherrefinethestateactionrepresentationfunctionÏ•andÂµtohelpcapturemore
accuratetransitiondynamics. Weoptimizethembysolvingthefollowingproblem,accordingtothe
transitionmodellossandspectralnormregularization[28]tosatisfytheregularityconstraintsofthe
featureinAssumption3:
minL rep(Ï•,Âµ)=minE (s,a,sâ€²)âˆˆD âˆ¥ÂµT(sâ€²)K Âµâˆ’1âˆ’Ï•T(s,a)Mâˆ¥2 2+Î» Ï•âˆ¥Ï•âˆ¥ 2+Î» Âµâˆ¥Âµâˆ¥ 2. (6)
Ï•,Âµ Ï•,Âµ
TheworldmodellearningprocessisillustratedinFigure3. Theestimationofindividual M(u )
c
mitigatesthespuriouscorrelationbroughtbyu . Tofurtherdealwiththespuriouscorrelationin
c
4u inducedbythebehaviorpolicyÏ€ (a|s,u ),weutilizetheconditionalindependencepropertyin
Ï€ Î² Ï€
theASC-MDPshowninequation(3). Thefollowingequationshowsthatthetrueunconfounded
transitionT canberewritteninthereweightingformulas. ThisreweightingprocessinmaskM serves
asthesoftinterventionapproach[18,29]toestimatethetreatmenteffectinthetransitionfunctionT
inanunconfoundedway:
E [Ï•(s,a)TM(u )Âµ(sâ€²)Â·Ï€ (a|s,u )]
T(sâ€²|s,a)= pu c Î² Ï€
E Ï€ (a|s,u )
pu Î² Ï€
(7)
(cid:20)E
[M(u )Ï€ (a|s,u
)](cid:21)
=Ï•(s,a)T pu c Î² Ï€ Âµ(sâ€²)â‰œÏ•(s,a)T M(u)Âµ(sâ€²).
E Ï€ (a|s,u )
pu Î² Ï€
The derivation of equation (7) is illustrated in Appendix A.4. equation (7) basically shows
a re-weighting process given the empirical estimation of M(u ) in every batch of trajectories:
c
M(u)= E pu[M(uc)Ï€Î²(a|s,uÏ€)]. ComparedtothegeneralreweightingstrategiesinpreviousMBRL
E puÏ€Î²(a|s,uÏ€)
literatures[18,29]whichreweightstheentirevaluefunction,thisre-weightingprocessisconducted
onlyontheestimatedmatrix,whiletherepresentationÏ•(s,a)andÂµ(sâ€²)aresubsequentlyregularized
by weighted estimation of M. The pipeline of causal world model learning is described in the
first part of the Algorithm 1. We discuss more details of the implementation and experiment in
AppendixC.
3.3 CausalRepresentationforUncertaintyQuantification
ToavoidenteringOODstatesinthe
Algorithm1:BECAUSETrainingandPlanning
onlinedeployment,wefurtherdesign
Input: OfflinedatasetD,causaldiscoveryfrequencyk
apessimisticplanneraccordingtothe
uncertaintyofthepredictedtrajecto- Output: CausalmaskM,featurefunctionÏ•(cid:98),Âµ (cid:98),policyÏ€ (cid:98)
riesintheimaginationrolloutstepto // Causal world model learning
mitigateobjectivemismatch. M â†[1]dâ€²Ã—d
0
foriâˆˆ[K]do
We use the feature embedding from
bilinearcausalrepresentationtohelp UpdateÏ•(cid:98)n,Âµ (cid:98)nbyL rep(Ï•,Âµ)in(6)
quantify the uncertainty, denoted as ifi mod k=0then
E Î¸(s,a). As we have access to the UpdateM(cid:99)nwithL mask(M)in(5)
offline dataset, we learn an Energy- WeightedaverageM with(7)
n
based Model (EBM) [30, 31] based // Uncertainty quantifier learning
ontheabstractedstaterepresentation FitE (s,a)withL (8)
Î¸ EBM
Ï•andcorematrixM. Ahigherout-
InitializeV(cid:98)H+1(s)=0,âˆ€(s,a)
putoftheenergyfunctionE (Â·,Â·)in-
Î¸ // Pessimistic planning
dicatesahigheruncertaintyinthecur-
whileh<H do
rentstateastheyarevisitedbythebe-
EstimatetheuncertaintywithscoreE (s,a)
Î¸
haviorpoliciesÏ€ lessfrequently. In
Î² ComputeQ (s,a)with(9)
practice,theenergy-basedmodelusu- h
allysuffersfromahigh-dimensional
V(cid:98)h(s,a)=max aQ(s,a)
dataspace[32]. Tomitigatethisover- aâ†argmax aQ(s,a)
head of training a good uncertainty sâ€²,r â†env.step(a,g)
quantifier, we first embed the state
samplesthroughtheabstractrepresentationÂµ(sâ€²),andthestateactionpairviaÏ•(s,a).
L (Î¸)=E E [Âµ(s+)|Ï•(s,a)]âˆ’E E [Âµ(sâˆ’)|Ï•(s,a)]+Î» âˆ¥Î¸âˆ¥ , (8)
EBM T(cid:98)(Â·|s,a) Î¸ q(s,a) Î¸ EBM 2
where Âµ(s)+ refers to the positive samples from the approximated transition dynamics T(cid:98)(Â·|s,a),
andÂµ(sâˆ’)referstothelatentnegativesamplesviatheLangevindynamics[30]. Additionally,we
regularizetheparametersofEBMtoavoidoverfittingissues. Weattachmoretrainingdetailsand
results of EBMs in Appendix C.2 The learned energy function E (s,a) is used to quantify the
Î¸
uncertaintybasedontheofflinedata.
Duringtheonlineplanningstage,weusethelearnedEBMtoadjusttherewardestimationbasedon
ModelPredictiveControl(MPC)[33]. Attimesteph,webasicallysubtracttheoriginalstepreturn
estimationr (s,a)byitsuncertaintyE (s,a):
h Î¸
(cid:88)
Q h(s,a)=Q(cid:98)h(s,a)âˆ’E Î¸(s,a)=r h(s,a)âˆ’E Î¸(s,a)+ T(cid:98)(sâ€²|s,a)V(cid:98)h+1(sâ€²).
(9)
(cid:124) (cid:123)(cid:122) (cid:125)
sâ€²âˆˆS
AdjustedReturn
53.4 TheoreticalAnalysisofBECAUSE
ThenwemoveontodevelopthetheoreticalanalysisfortheproposedmethodBECAUSE.Based
on two standard Assumption 2 and 3 on the featureâ€™s existence and regularity, we achieve the
finite-samplecomplexityguaranteeâ€”anupperboundofthesuboptimalitygapasfollows,whose
proofispostponedtoAppendixB.
Theorem1(Performanceguarantee). Considerany0<Î´ <1andanyinitialstatesâˆˆS. Under
(cid:101)
theAssumption2, 3andthatthetransitionmodelT isanSCM(definedin4),foranyaccuracylevel
0â‰¤Î¾ â‰¤1,withprobabilityatleast1âˆ’Î´,theoutputpolicyÏ€ofBECAUSE(Algorithm1)basedon
(cid:80)
thehistoricaldatasetDwithn= n(s,a)samplesgeneratedfromabehaviorpolicyÏ€
(s,a)âˆˆSÃ—A Î²
satisfies:
H (cid:34)(cid:115) (cid:35)
Vâˆ—(s)âˆ’VÏ€(s)â‰²min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9)(cid:88) E log(1/Î´) |s =s ,
1 (cid:101) 1 (cid:101) 1 Î¾ s 0 Ï€âˆ— n(s ,a ) 1 (cid:101)
h h
h=1
whereC ,C aresomeuniversalconstants,ÏƒisSCMâ€™snoiselevel(seeDefinition4),andM âˆˆRdÃ—dâ€²
1 s
istheoptimalgroundtruthsparsetransitionmatrixtobeestimated.
Theerrorboundshrinksastheofflinesamplesizenoverallstate-actionpairsincrease. Italsogrows
proportionallytotheplanninghorizonH,SCMâ€™snoiselevelÏƒ,andtheâ„“ normofthegroundtrue
0
causalmaskM,whichdescribestheintrinsiccomplexityoftheworldmodel.
Consequently,withProposition1intheAppendix,wecanachieveÎ¾-optimalpolicy(Vâˆ—(s)âˆ’VÏ€(s)â‰¤
1 (cid:101) 1 (cid:101)
Î¾)aslongasthehistoricaldatasetsatisfiesthefollowingconditions: âˆ€0â‰¤Î¾ â‰¤1,
min(cid:8) C2log2(cid:0)âˆ¥Mâˆ¥0(cid:1) |S|,C2Ïƒ2âˆ¥Mâˆ¥ (cid:9) Â·H2log(1/Î´)
min E (cid:2) n(s ,a )|s =s(cid:3)â‰³ 1 Î¾ s 0 .
(s,a,h)âˆˆSÃ—AÃ—[H] Ï€â‹† h h 1 (cid:101) Î¾2
4 ExperimentResults
Inthissection, weconductacomprehensiveempiricalevaluationofBECAUSEâ€™sgeneralization
performanceinadiversesetofenvironments,coveringdifferentdecision-makingproblemsinthe
gridworld,manipulation,andautonomousdrivingdomains,showninAppendixFigure8.
4.1 ExperimentSetting
EnvironmentDesign Wedesign18tasksin3representativeRLenvironments. Agentsneedto
acquirereasoningcapabilitiestoreceivehigherrewardsandachievegoals.
â€¢ Lift: ObjectmanipulationenvironmentinRoboSuite[34]. Wedesignedthisenvironmentforthe
agenttoliftanobjectwithaspecificcolorconfigurationonthetabletoadesiredheight. Inthe
OODenvironmentLift-O,thereisaninjectedspuriouscorrelationbetweenthecolorofthecube
andthepositionofthecubeinthetrainingphase. Duringthetestingphase,thecorrelationbetween
colorandpositionisdifferentfromtraining.
â€¢ Unlock: WedesignedthisenvironmentfortheagenttocollectakeytoopendoorsinMinigrid[35].
IntheOODenvironmentUnlock-O,therewillbeadifferentnumberofgoals(doorstobeopened)
inthetestingenvironmentsfromthetrainingenvironments.
â€¢ Crash: Safety is critical in autonomous driving, which is reflected by the collision avoidance
capability. WeconsiderariskyscenariowhereanAVcollideswithajaywalkerbecauseitsviewis
blockedbyanothercar[36]. Wedesignsuchacrashscenariobasedonhighway-env[37],where
thegoalistocreatecrashesbetweenapedestrianandAVs. IntheOODenvironmentCrash-O,the
distributionofreward(numberofpedestrians)isdifferentinonlinetestingenvironments.
Forallthreedifferentenvironments,wesetaspecificsubsetofthestatespaceasthegoalg âˆˆS,and
therewardisdefinedasthegoal-reachingrewardr(s,a,g)=I(r =g). Whentheepisodeendsin
thegoalstatewithinthetaskhorizonH,theepisodeisconsideredasuccess. Wethenusetheaverage
successrateasthegeneralevaluationmetricsforourBECAUSEandallbaselines.
Ineachenvironment,wecollectthreetypesofofflinedata: random,medium,andexpertbasedon
thedifferentlevelsofu inthebehaviorpolicies. InUnlockenvironments,wecollect200episodes
Ï€
6Lift-Random Lift-Medium Lift-Expert Unlock-Random Unlock-Random
Unlock-Random Unlock-Medium Unlock-Expert Unlock-Medium Unlock-Medium
Crash-Random Crash-Medium Crash-Expert Unlock-Expert Unlock-Expert
(a) (b) (c)
Figure4: ResultsofBECAUSEandbaselinesindifferenttasks. (a)Averagesuccessrateindistri-
butionandoutofdistribution. (b)Averagesuccessratew.r.t. ratioofofflinesamples. (c)Average
successratew.r.t. spuriouslevelintheenvironments. Weevaluatethemeanandstandarddeviationof
thebestperformanceamong10randomseedsandreporttask-wiseresultsinAppendixTable3.
fromeachlevelofbehaviorpoliciesastheofflinedemonstrationdata,andthenumberofepisodesis
1,000intheenvironmentsLiftandCrash,whichallhavecontinuousstateandactionspace. Amore
detailedviewoftheenvironmenthyperparametersandbehaviorpolicydesignisinAppendixC.5.
Baselines WecompareourproposedBECAUSEwithseveralofflinecausalRLorMBRLbaselines.
ICIL[38]learnsadynamic-awareinvariantcausalrepresentationlearningtoassistageneralizable
policylearningfromofflinedatasets. CCIL[39]conductsasoftinterventioninourofflinesetting
byjointlyoptimizingpolicyparametersandmasksoverthestate. MnM[9]unifiestheobjectiveof
jointlytrainingthemodelandpolicy,whichallocateslargerweightsinthestatepredictionlossinthe
high-rewardregion.Delphic[40]introducesdelphicuncertaintytodifferentiatebetweenuncertainties
causedbyhiddenconfoundersandtraditionalepistemicandaleatoricuncertainties. TD3+BC[41]
isanofflinemodel-freeRLapproachthatcombinestheTwinDelayedDeepDeterministicPolicy
Gradient (TD3) algorithm with Behavior Cloning (BC) to adopt both the actor-critic framework
andsupervisedlearningfromexpertdemonstrations. MOPO[2]isanofflineMBRLapproachthat
usesflatlatentspaceandcount-baseduncertaintyquantificationtomaintainconservatisminonline
deployment. GNN[42]isaGNN-basedbaselineusingaRelationalGraphConvolutionalNetworkto
modelthetemporaldependencyofstate-actionpairsinthedynamicmodelwithmessagepassing.
CDL[24]usescausaldiscoverytolearnatask-independentworldmodel. DenoisedMDP[12]and
IFactor[13]conductcausalstateabstractionbasedontheircontrollabilityandrewardrelevance.
Thelastthreemethodsaredesignedforonlinesettings,soweonlyimplementtheirmodellearning
objectives. WeattachmoredetailsofthebaselineimplementationinAppendixC.6.
4.2 ExperimentResultsAnalysis
Weempiricallyanswerthefollowingresearchquestions.
â€¢ RQ1:HowisthegeneralizabilityofBECAUSEintheonlineenvironments(whichmaybeunseen)?
Specifically,howdoesBECAUSEperformunderdiversequalitiesofdemonstrationdata(different
levelofu ),anddifferentenvironmentcontexts(differentu )?
Ï€ c
â€¢ RQ2: HowdoesthedesigninBECAUSEcontributetotherobustnessofitsfinalperformance
underdifferentsamplesizesorspuriouslevels?
â€¢ RQ3:HowdoesBECAUSEachievetheaforementionedgeneralizabilitybymitigatingtheobjective
mismatchprobleminofflineMBRL?
7Unlock-random Unlock-medium Unlock-expert
Negative Reward
Positive Reward
Example Policies
Figure5: Evaluationofthedifferencebetweenthedistributionofepisodicmodellossforsuccess
andfailuretrajectories. Thehigherdifferenceindicatesareductioninmodelmismatchissues. An
exampleoffailuremodeistryingtoopenthedoorwithouthavingthekey.
ForRQ1,inFigure4(a),weevaluatethesuccessrateintheonlineenvironmentagainstdifferent
baselines. Theresultshowsthatunderdifferentenvironmentsanddifferentqualitiesofbehavior
policiesÏ€ (differentu ),BECAUSEconsistentlyachievesthebestperformancein8outof9for
Î² Ï€
boththein-distribution(I)andout-of-distribution(O)forallthedemonstrationdataquality(different
levelofu ). WhereOhereindicatesthetasksunderunseenenvironmentwithconfounderuâ€² Ì¸=u
Ï€ c c
differentfromofflinetraining. Anotherfindingisthatmodel-basedapproachesgenerallyperform
betterthanmodel-freeapproachesatvariouslevelsofofflinedata,whichshowstheimportanceof
worldmodellearningforgeneralizableofflineRL.WeattachthedetailedresultsintheAppendix
Table3,4andthecausalmasksdiscoveredineachenvironmentinAppendixFigure9forreference.
ForRQ2,wecomparedifferentaspectsof Figure6: TheablationstudiesbetweenBECAUSEand
BECAUSEâ€™srobustnesswithMOPOwith- itsvariants. WereporttheoverallSuccessrate(%)over
outcausalstructures[2]. Wecomparetheir 9in-distribution(I)and9out-of-distribution(O)tasks,
performancewithdifferentratiosoftheen- respectively. Boldisthebest.
tireofflinedatasetandillustratethesuccess
ratesinFigure4(b). Theresultshowsthat, Variants BECAUSE Optimism Linear Full
for any selected number of samples, BE-
Overall-I 73.3Â±4.5 64.4Â±6.4 57.9Â±6.1 39.3Â±6.3
CAUSEconsistentlyoutperformsMOPO
Overall-O 43.0Â±4.9 32.4Â±3.3 33.2Â±5.2 25.2Â±3.9
withaclearmargin. WealsoevaluateBE-
CAUSEperformanceathigherspuriouslevelsinFigure4(c). Weaddupto8Ã—oftheoriginalnumber
ofconfoundersintheenvironmentstotesttherobustnessoftheagentâ€™sperformance. BECAUSE
consistentlyoutperformsMOPOandthemarginenlargesasthespuriouslevelgrowshigher.
ForRQ3,weaimtounderstandwhetherBECAUSEachieveshigherperformancebyresolvingthe
objectivemismatchproblem. Wefirstcollecttwogroupsoftrajectories: Ï„ andÏ„ ,eachwith
pos neg
positive reward (success) and negative reward (failure) in Unlock task with sparse goal-reaching
reward. We want to have a model whose loss is informative for discriminating control results,
that is, we wish L (Ï„ ) < L (Ï„ ). According to our visualization in Figure 5, in
model pos model neg
Unlock-Expert andUnlock-Medium, theratioofÏ„ ismuchhigherinBECAUSEthanMOPO
pos
amongthetrajectorieswithlowmodelloss. InUnlock-Random, themismatchofthemodeland
controlobjectiveismoresignificant,sincethedemonstrationispoorinstatecoverage. MOPOcannot
succeedevenwhenthemodellossislow,whereasourmethodscan. Weperformahypothesistest
withH :L (Ï„ )<L (Ï„ ). InBECAUSE,thisdesiredpropertyismoresignificant
0 model pos model neg
than MOPO attributed to the causal representation we learn, indicating a reduction of objective
mismatch. WeattachdetaileddiscussionsforthemismatchevaluationinAppendixC.3andTable2.
4.3 AblationStudies
We conducted ablation studies with three variants of BECAUSE and report the average success
rateacrossninein-distributionandnineout-of-distributiontasksinTable6. TheOptimismvariant
conductsoptimisticplanninginsteadofpessimisticplanninginequation(9),whichusesuniform
samplingintheplannermodule. TheLinearvariantassumesafullconnectiontothecausalmatrix
M,thendirectlyuseslinearMDPtoparameterizethedynamicsmodelT,whichremovesthecausal
discoverymoduleinBECAUSE.TheFullvariantlearnsfromthefullbatchofdatatoestimatethe
8
OPOM
ESUACEBcausalmaskwithoutiterativeupdate. Wereporttheresultsofthetask-wiseablationwithconfidence
intervalandsignificanceinAppendixTable5and6.
5 RelatedWorks
ObjectiveMismatchinMBRL TheobjectivemismatchinMBRL[43,44]referstothefactthat
pureMLEestimationoftheworldmodeldoesnotalignwellwiththecontrolobjective. Previous
works[29,45]proposereweightingduringmodeltrainingtoalleviatethismismatch, [46]proposes
agoal-awarepredictionbyredistributingmodelerroraccordingtotheirtaskrelevance. Theseworks
essentiallyreweightlossfortheentiremodeltraining, whileourworkconductsreweightingjust
overtheestimatedcausalmaskmoreefficiently. Morerecently, [9,10]proposedajointtraining
betweentheworldmodelandpolicies. Althoughjointoptimizationimprovesperformance,theydo
notaddressthegeneralizabilityofthelearnedmodelunderthedistributionshiftsetting. Intheoffline
setting,Model-basedRL[2,3,4]employsmodelensemble,pessimisticpolicyoptimizationorvalue
iteration[47,48],andanenergy-basedmodelforplanning[49]toquantifyuncertaintyandimprove
testperformance. Tothebestofourknowledge,nopreviousworkexploredormodeledtheimpactof
distributionshiftontheobjectivemismatchprobleminMBRL.
Causal Discovery with Confounder Most of the existing causal discovery methods [50] can
be categorized into constraint-based and score-based. Constraint-based methods [51] start from
a complete graph and iteratively remove edges with statistical hypothesis testing [52, 53]. This
type of method is highly data-efficient but not robust to noisy data. As a remedy, score-based
methods[54,55]usemetricssuchasthelikelihoodorBIC[56]asscorestomanipulateedgesin
the causal graph. Recently, researchers have extended score-based methods with RL [57], order
learning [58] or differentiable discovery [22, 59, 60]. To alleviate the non-identifiability under
hiddenconfounders,activeinterventionmethodshavebeenexplored[61],aimingtobreakspurious
correlationsinanonlinefashion. Withextraassumptionsonconfounders,somerecentworksdetect
suchcorrelations[62,63,64]sothatmodelscaneffectivelyidentifyelusiveconfounders.
Causal Reinforcement Learning Recently, many RL algorithms have incorporated causality
to improve reasoning capability [65] and generalizability. For instance, [66] and [67] explicitly
estimatecausalstructureswiththeinterventionaldataobtainedfromtheenvironmentinanonline
setting. These structures can be used to constrain the output space [19] or to adjust the buffer
priority[68]. Buildingdynamicmodelsinmodel-basedRL[24,69,70]basedoncausalgraphsis
widelystudied. MostexistingcausalMBRLworksfocusonestimatingthecausalworldmodelby
predicting transition dynamics and rewards. Existing methods learn this causal world model via
sparsityregularization[23,71],conditionalindependencetest[24,69,72],variationalinference[73,
12],counterfactualdataaugmentation[74,75],hierarchicalskillabstraction[76,77],uncertainty
quantification[40],rewardredistribution[24,78],causalcontextmodeling[79,80]andstructure-
awarestateabstraction[12,13,81,82]basedonthecontrollabilityandtaskorrewardrelevance.
However,thepresenceofconfoundersduringdatacollectioncanskewthelearnedpolicy,makingit
susceptibletospuriouscorrelations. Deconfoundingsolutionshavebeenproposedeitherbetween
actionsandstates[39,83,84]oramongdifferentdimensionsofstatevariables[19,85].
6 Conclusion
Inthispaper,westudyhowtomitigatetheobjectivemismatchprobleminMBRL,especiallyunder
the offline settings where distribution shift occurs. We first propose ASC-MDP and the bilinear
causalrepresentationassociatedwithit. Basedontheformulation,weproposedhowtolearnthis
causalabstractionbyalternatingbetweencausalmasklearningandfeaturelearninginfittingthe
worlddynamics. Intheplanningstage,weappliedthelearnedcausalrepresentationtoanuncertainty
quantification module based on EBM, which improves the robustness under uncertainty in the
online planning stage. We theoretically justify BECAUSEâ€™s sub-optimality bound induced by
thesparsematrixestimationproblemandofflineRL.Comprehensiveexperimentson18different
tasksshowthatgivenadiverselevelofdemonstrationastheofflinedataset,BECAUSEhasbetter
generalizabilitythanbaselinesindifferentonlineenvironments,anditrobustlyoutperformsbaselines
underdifferentspuriouslevelsorsamplesizes. WeempiricallyshowthatBECAUSEmitigatesthe
objectivemismatchwithcausalawarenesslearnedfromofflinedata. OnelimitationofBECAUSE
liesinitssimplifiedassumptionoftime-homogeneouscausalstructure,whichmaynotalwaysholdin
long-horizonornon-stationarysettings. Besides,thecurrentimplementationisstillbasedonvector
observations. Itwillbeinterestingtoscaleupthecausalreasoningframeworkintohigh-dimensional
observationstodiscoverconceptfactorsinlong-horizonvisualRLsettings.
9References
[1] SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning:
Tutorial,review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
[2] TianheYu,GarrettThomas,LantaoYu,StefanoErmon,JamesYZou,SergeyLevine,Chelsea
Finn,andTengyuMa. Mopo: Model-basedofflinepolicyoptimization. AdvancesinNeural
InformationProcessingSystems,33:14129â€“14142,2020.
[3] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:
Model-basedofflinereinforcementlearning. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.
Balcan,andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,
pages21810â€“21823.CurranAssociates,Inc.,2020.
[4] TianheYu,AviralKumar,RafaelRafailov,AravindRajeswaran,SergeyLevine,andChelsea
Finn. Combo: Conservative offline model-based policy optimization. Advances in neural
informationprocessingsystems,34:28954â€“28967,2021.
[5] ZeyuZhuandHuijingZhao. Asurveyofdeeprlandilforautonomousdrivingpolicylearning.
IEEETransactionsonIntelligentTransportationSystems,23(9):14043â€“14065,2021.
[6] AviralKumar,AnikaitSingh,FrederikEbert,MitsuhikoNakamoto,YanlaiYang,ChelseaFinn,
andSergeyLevine. Pre-trainingforrobots: Offlinerlenableslearningnewtasksfromahandful
oftrials. arXivpreprintarXiv:2210.05178,2022.
[7] ShengpuTangandJennaWiens. Modelselectionforofflinereinforcementlearning: Practical
considerationsforhealthcaresettings. InMachineLearningforHealthcareConference,pages
2â€“35.PMLR,2021.
[8] LaixiShiandYuejieChi. Distributionallyrobustmodel-basedofflinereinforcementlearning
withnear-optimalsamplecomplexity. arXivpreprintarXiv:2208.05767,2022.
[9] BenjaminEysenbach,AlexanderKhazatsky,SergeyLevine,andRussRSalakhutdinov. Mis-
matchednomore: Jointmodel-policyoptimizationformodel-basedrl. AdvancesinNeural
InformationProcessingSystems,35:23230â€“23243,2022.
[10] ShentaoYang,ShujianZhang,YihaoFeng,andMingyuanZhou. Aunifiedframeworkforalter-
natingofflinemodeltrainingandpolicylearning. AdvancesinNeuralInformationProcessing
Systems,35:17216â€“17232,2022.
[11] JonasPeters,DominikJanzing,andBernhardSchÃ¶lkopf. Elementsofcausalinference: founda-
tionsandlearningalgorithms. TheMITPress,2017.
[12] TongzhouWang, SimonS Du, AntonioTorralba, PhillipIsola, Amy Zhang, andYuandong
Tian. Denoised mdps: Learning world models better than the world itself. arXiv preprint
arXiv:2206.15477,2022.
[13] Yu-RenLiu,BiweiHuang,ZhengmaoZhu,HonglongTian,MingmingGong,YangYu,andKun
Zhang. Learningworldmodelswithidentifiablefactorization. arXivpreprintarXiv:2306.06561,
2023.
[14] LinYangandMengdiWang. Reinforcementlearninginfeaturespace: Matrixbandit,kernels,
and regret bound. In International Conference on Machine Learning, pages 10746â€“10756.
PMLR,2020.
[15] WeitongZhang,JiafanHe,DongruoZhou,QGu,andAZhang.Provablyefficientrepresentation
selectioninlow-rankmarkovdecisionprocesses: fromonlinetoofflinerl. InUncertaintyin
ArtificialIntelligence,pages2488â€“2497.PMLR,2023.
[16] FanFengandSaraMagliacane. Learningdynamicattribute-factoredworldmodelsforefficient
multi-objectreinforcementlearning. arXivpreprintarXiv:2307.09205,2023.
[17] AngelaZhou. Reward-relevance-filteredlinearofflinereinforcementlearning. arXivpreprint
arXiv:2401.12934,2024.
[18] LingxiaoWang,ZhuoranYang,andZhaoranWang. Provablyefficientcausalreinforcement
learning with confounded observational data. Advances in Neural Information Processing
Systems,34:21164â€“21175,2021.
[19] WenhaoDing,LaixiShi,YuejieChi,andDingZhao. Seeingisnotbelieving: Robustreinforce-
mentlearningagainstspuriouscorrelation. arXivpreprintarXiv:2307.07907,2023.
10[20] JunzheZhangandEliasBareinboim. Markovdecisionprocesseswithunobservedconfounders:
Acausalapproach. Technicalreport,Technicalreport,TechnicalReportR-23,PurdueAILab,
2016.
[21] KarrenYang,AbigailKatcoff,andCarolineUhler. Characterizingandlearningequivalence
classesofcausaldagsunderinterventions. InInternationalConferenceonMachineLearning,
pages5541â€“5550.PMLR,2018.
[22] Philippe Brouillard, SÃ©bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and
AlexandreDrouin. Differentiablecausaldiscoveryfrominterventionaldata. AdvancesinNeural
InformationProcessingSystems,33:21865â€“21877,2020.
[23] ZizhaoWang,XuesuXiao,YukeZhu,andPeterStone.Task-independentcausalstateabstraction.
WorkshoponRobotLearning: Self-SupervisedandLifelongLearning,NeurIPS,2021.
[24] ZizhaoWang,XuesuXiao,ZifanXu,YukeZhu,andPeterStone. Causaldynamicslearningfor
task-independentstateabstraction. arXivpreprintarXiv:2206.13452,2022.
[25] AmyZhang,ClareLyle,ShagunSodhani,AngelosFilos,MartaKwiatkowska,JoellePineau,
Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In International
ConferenceonMachineLearning,pages11214â€“11224.PMLR,2020.
[26] WenxuanZhu,ChaoYu,andQiangZhang. Causaldeepreinforcementlearningusingobserva-
tionaldata. arXivpreprintarXiv:2211.15355,2022.
[27] KrzysztofChalupka,PietroPerona,andFrederickEberhardt. Fastconditionalindependence
testforvectorvariableswithlargesamplesizes. arXivpreprintarXiv:1804.02747,2018.
[28] YuichiYoshidaandTakeruMiyato. Spectralnormregularizationforimprovingthegeneraliz-
abilityofdeeplearning. arXivpreprintarXiv:1705.10941,2017.
[29] NathanLambert,BrandonAmos,OmryYadan,andRobertoCalandra. Objectivemismatchin
model-basedreinforcementlearning. arXivpreprintarXiv:2002.04523,2020.
[30] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models.
AdvancesinNeuralInformationProcessingSystems,32,2019.
[31] YezhenWang,BoLi,TongChe,KaiyangZhou,ZiweiLiu,andDongshengLi. Energy-based
open-worlduncertaintymodelingforconfidencecalibration. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages9302â€“9311,2021.
[32] BoPang,TianHan,ErikNijkamp,Song-ChunZhu,andYingNianWu. Learninglatentspace
energy-basedpriormodel. AdvancesinNeuralInformationProcessingSystems,33:21994â€“
22008,2020.
[33] EduardoFCamachoandCarlosBordonsAlba. Modelpredictivecontrol. Springerscience&
businessmedia,2013.
[34] YukeZhu,JosiahWong,AjayMandlekar,RobertoMartÃ­n-MartÃ­n,AbhishekJoshi,Soroush
Nasiriany,andYifengZhu. robosuite: Amodularsimulationframeworkandbenchmarkfor
robotlearning. InarXivpreprintarXiv:2009.12293,2020.
[35] MaximeChevalier-Boisvert,LucasWillems,andSumanPal. Minimalisticgridworldenviron-
mentforopenaigym. https://github.com/maximecb/gym-minigrid,2018.
[36] ZennaTavares,JamesKoppel,XinZhang,RiaDas,andArmandoSolar-Lezama. Alanguage
forcounterfactualgenerativemodels. InInternationalConferenceonMachineLearning,pages
10173â€“10182.PMLR,2021.
[37] EdouardLeurent.Anenvironmentforautonomousdrivingdecision-making.https://github.
com/eleurent/highway-env,2018.
[38] IoanaBica,DanielJarrett,andMihaelavanderSchaar. Invariantcausalimitationlearningfor
generalizablepolicies. AdvancesinNeuralInformationProcessingSystems,34:3952â€“3964,
2021.
[39] PimDeHaan,DineshJayaraman,andSergeyLevine. Causalconfusioninimitationlearning.
AdvancesinNeuralInformationProcessingSystems,32,2019.
[40] AlizÃ©ePace,HugoYÃ¨che,BernhardSchÃ¶lkopf,GunnarRatsch,andGuyTennenholtz. Delphic
offline reinforcement learning under nonidentifiable hidden confounding. In The Twelfth
InternationalConferenceonLearningRepresentations,2023.
11[41] ScottFujimotoandShixiangShaneGu.Aminimalistapproachtoofflinereinforcementlearning.
Advancesinneuralinformationprocessingsystems,34:20132â€“20145,2021.
[42] MichaelSchlichtkrull,ThomasNKipf,PeterBloem,RiannevandenBerg,IvanTitov,andMax
Welling. Modelingrelationaldatawithgraphconvolutionalnetworks. InEuropeansemantic
webconference,pages593â€“607.Springer,2018.
[43] Amir-massoudFarahmand,AndreBarreto,andDanielNikovski. Value-awarelossfunctionfor
model-basedreinforcementlearning. InArtificialIntelligenceandStatistics,pages1486â€“1494.
PMLR,2017.
[44] YupingLuo,HuazheXu,YuanzhiLi,YuandongTian,TrevorDarrell,andTengyuMa. Algo-
rithmicframeworkformodel-baseddeepreinforcementlearningwiththeoreticalguarantees.
arXivpreprintarXiv:1807.03858,2018.
[45] PierlucaDâ€™Oro,AlbertoMariaMetelli,AndreaTirinzoni,MatteoPapini,andMarcelloRestelli.
Gradient-awaremodel-basedpolicysearch. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pages3801â€“3808,2020.
[46] SurajNair,SilvioSavarese,andChelseaFinn. Goal-awareprediction: Learningtomodelwhat
matters. InInternationalConferenceonMachineLearning,pages7207â€“7219.PMLR,2020.
[47] ChiJin,ZhuoranYang,ZhaoranWang,andMichaelIJordan. Provablyefficientreinforcement
learningwithlinearfunctionapproximation. InConferenceonLearningTheory,pages2137â€“
2143.PMLR,2020.
[48] MasatoshiUeharaandWenSun. Pessimisticmodel-basedofflinereinforcementlearningunder
partialcoverage. arXivpreprintarXiv:2107.06226,2021.
[49] YilunDu, ShuangLi, JoshuaTenenbaum, andIgorMordatch. Learningiterativereasoning
throughenergyminimization. InInternationalConferenceonMachineLearning,pages5570â€“
5582.PMLR,2022.
[50] ClarkGlymour,KunZhang,andPeterSpirtes. Reviewofcausaldiscoverymethodsbasedon
graphicalmodels. Frontiersingenetics,10:524,2019.
[51] PeterSpirtes,ClarkNGlymour,RichardScheines,andDavidHeckerman.Causation,prediction,
andsearch. MITpress,2000.
[52] KarlPearson. X.onthecriterionthatagivensystemofdeviationsfromtheprobableinthecase
ofacorrelatedsystemofvariablesissuchthatitcanbereasonablysupposedtohavearisenfrom
randomsampling. TheLondon,Edinburgh,andDublinPhilosophicalMagazineandJournalof
Science,50(302):157â€“175,1900.
[53] KunZhang,JonasPeters,DominikJanzing,andBernhardSchÃ¶lkopf. Kernel-basedconditional
independencetestandapplicationincausaldiscovery. arXivpreprintarXiv:1202.3775,2012.
[54] DavidMaxwellChickering. Optimalstructureidentificationwithgreedysearch. Journalof
machinelearningresearch,3(Nov):507â€“554,2002.
[55] Alain Hauser and Peter BÃ¼hlmann. Characterization and greedy learning of interventional
markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning
Research,13(1):2409â€“2464,2012.
[56] AndrewANeathandJosephECavanaugh. Thebayesianinformationcriterion: background,
derivation, and applications. Wiley Interdisciplinary Reviews: Computational Statistics,
4(2):199â€“203,2012.
[57] ShengyuZhu,IgnavierNg,andZhitangChen. Causaldiscoverywithreinforcementlearning.
arXivpreprintarXiv:1906.04477,2019.
[58] DezhiYang,GuoxianYu,JunWang,ZhengtianWu,andMaozuGuo. Reinforcementcausal
structurelearningonordergraph. InProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume37,pages10737â€“10744,2023.
[59] NanRosemaryKe,OlexaBilaniuk,AnirudhGoyal,StefanBauer,HugoLarochelle,Bernhard
SchÃ¶lkopf,MichaelCMozer,ChrisPal,andYoshuaBengio. Learningneuralcausalmodels
fromunknowninterventions. arXivpreprintarXiv:1910.01075,2019.
[60] YunzhuLi,AntonioTorralba,AnimaAnandkumar,DieterFox,andAnimeshGarg. Causal
discoveryinphysicalsystemsfromvideos.AdvancesinNeuralInformationProcessingSystems,
33:9180â€“9192,2020.
12[61] NinoScherrer,OlexaBilaniuk,YashasAnnadani,AnirudhGoyal,PatrickSchwab,Bernhard
SchÃ¶lkopf,MichaelCMozer,YoshuaBengio,StefanBauer,andNanRosemaryKe. Learning
neuralcausalmodelswithactiveinterventions. arXivpreprintarXiv:2109.02429,2021.
[62] ChristopherClark,MarkYatskar,andLukeZettlemoyer.Donâ€™ttaketheeasywayout:Ensemble-
basedmethodsforavoidingknowndatasetbiases. arXivpreprintarXiv:1909.03683,2019.
[63] DivyanshKaushik,EduardHovy,andZacharyCLipton. Learningthedifferencethatmakesa
differencewithcounterfactually-augmenteddata. arXivpreprintarXiv:1909.12434,2019.
[64] MeikeNauta,RickyWalsh,AdamDubowski,andChristinSeifert. Uncoveringandcorrecting
shortcutlearninginmachinelearningmodelsforskincancerdiagnosis. Diagnostics,12(1):40,
2021.
[65] PrashanMadumal,TimMiller,LizSonenberg,andFrankVetere. Explainablereinforcement
learningthroughacausallens. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume34,pages2493â€“2500,2020.
[66] SurajNair,YukeZhu,SilvioSavarese,andLiFei-Fei.Causalinductionfromvisualobservations
forgoaldirectedtasks. arXivpreprintarXiv:1910.01751,2019.
[67] SergeiVolodin,NevanWichers,andJeremyNixon. Resolvingspuriouscorrelationsincausal
modelsofenvironmentsviainterventions. arXivpreprintarXiv:2002.05217,2020.
[68] MaximilianSeitzer,BernhardSchÃ¶lkopf,andGeorgMartius. Causalinfluencedetectionfor
improvingefficiencyinreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,34,2021.
[69] Zheng-Mao Zhu, Xiong-Hui Chen, Hong-Long Tian, Kun Zhang, and Yang Yu. Offline
reinforcementlearningwithcausalstructuredworldmodels. arXivpreprintarXiv:2206.01474,
2022.
[70] MircoMutti,RiccardoDeSanti,EmanueleRossi,JuanFelipeCalderon,MichaelBronstein,and
MarcelloRestelli. Provablyefficientcausalmodel-basedreinforcementlearningforsystematic
generalization. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,
pages9251â€“9259,2023.
[71] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-
stationary reinforcement learning. Advances in Neural Information Processing Systems,
35:31957â€“31971,2022.
[72] JiahengHu,ZizhaoWang,PeterStone,andRobertoMartin-Martin. Elden: Explorationvia
localdependencies. arXivpreprintarXiv:2310.08702,2023.
[73] WenhaoDing,HaohongLin,BoLi,andDingZhao. Generalizinggoal-conditionedreinforce-
mentlearningwithvariationalcausalreasoning. AdvancesinNeuralInformationProcessing
Systems,35:26532â€“26548,2022.
[74] LarsBuesing,TheophaneWeber,YoriZwols,NicolasHeess,SebastienRacaniere,ArthurGuez,
andJean-BaptisteLespiau. Woulda,coulda,shoulda: Counterfactually-guidedpolicysearch. In
InternationalConferenceonLearningRepresentations,2018.
[75] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based
counterfactualdataaugmentation. arXivpreprintarXiv:2210.11287,2022.
[76] TabithaELee,JialiangAlanZhao,AmritaSSawhney,SiddharthGirdhar,andOliverKroemer.
Causalreasoninginsimulationforstructureandtransferlearningofrobotmanipulationpolicies.
In2021IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages4776â€“4782.
IEEE,2021.
[77] TabithaEdithLee,ShivamVats,SiddharthGirdhar,andOliverKroemer. Scale: Causallearning
anddiscoveryofrobotmanipulationskillsusingsimulation. InConferenceonRobotLearning,
pages2229â€“2256.PMLR,2023.
[78] YudiZhang,YaliDu,BiweiHuang,ZiyanWang,JunWang,MengFang,andMykolaPech-
enizkiy. Interpretable reward redistribution in reinforcement learning: A causal approach.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[79] BiweiHuang,FanFeng,ChaochaoLu,SaraMagliacane,andKunZhang. Adarl: What,where,
andhowtoadaptintransferreinforcementlearning. InInternationalConferenceonLearning
Representations,2021.
13[80] PeideHuang,XilunZhang,ZiangCao,ShiqiLiu,MengdiXu,WenhaoDing,JonathanFrancis,
BingqingChen,andDingZhao.Whatwentwrong?closingthesim-to-realgapviadifferentiable
causaldiscovery. InConferenceonRobotLearning,pages734â€“760.PMLR,2023.
[81] XiangFu,GeYang,PulkitAgrawal,andTommiJaakkola. Learningtaskinformedabstractions.
InInternationalConferenceonMachineLearning,pages3480â€“3491.PMLR,2021.
[82] BiweiHuang,ChaochaoLu,LiuLeqi,JosÃ©MiguelHernÃ¡ndez-Lobato,ClarkGlymour,Bern-
hardSchÃ¶lkopf,andKunZhang. Action-sufficientstaterepresentationlearningforcontrolwith
structuralconstraints. InInternationalConferenceonMachineLearning,pages9260â€“9279.
PMLR,2022.
[83] ZhihongDeng,ZuyueFu,LingxiaoWang,ZhuoranYang,ChenjiaBai,ZhaoranWang,andJing
Jiang. Score: Spuriouscorrelationreductionforofflinereinforcementlearning. arXivpreprint
arXiv:2110.12468,2021.
[84] MaximeGasse,DamienGrasset,GuillaumeGaudron,andPierre-YvesOudeyer. Usingcon-
foundeddatainlatentmodel-basedreinforcementlearning. TransactionsonMachineLearning
Research,2023.
[85] JunzheZhang,DanielKumor,andEliasBareinboim. Causalimitationlearningwithunobserved
confounders. Advancesinneuralinformationprocessingsystems,33:12263â€“12274,2020.
[86] YingJin,ZhuoranYang,andZhaoranWang. Ispessimismprovablyefficientforofflinerl? In
InternationalConferenceonMachineLearning,pages5084â€“5096.PMLR,2021.
[87] ThomasBlumensathandMikeEDavies. Iterativehardthresholdingforcompressedsensing.
Appliedandcomputationalharmonicanalysis,27(3):265â€“274,2009.
[88] AmirBeckandMarcTeboulle. Afastiterativeshrinkage-thresholdingalgorithmforlinear
inverseproblems. SIAMjournalonimagingsciences,2(1):183â€“202,2009.
[89] PeterAuer,ThomasJaksch,andRonaldOrtner. Near-optimalregretboundsforreinforcement
learning. Advancesinneuralinformationprocessingsystems,21,2008.
[90] RyanTibshiraniandLarryWasserman. Sparsity,thelasso,andfriends. Lecturenotesfrom
â€œStatisticalMachineLearning,â€CarnegieMellonUniversity,Spring,2017.
[91] Patrick E McKnight and Julius Najab. Mann-whitney u test. The Corsini encyclopedia of
psychology,pages1â€“1,2010.
14A AuxiliaryDetailsofBECAUSEFramework
A.1 NotationSummary
WeillustrateallthenotationsusedinthemainpaperandappendixinTable1.
Table1: Notationsusedinthispaperandtheircorrespondingmeanings.
Notation Explanation
A,a Actionspace,action
S,s Statespace,state
r Reward
Î³ Discountfactor
T(Â·|Â·,Â·) Transitiondynamics
h,H Timesteph,HorizonH
Ï€âˆ— Optimalpolicyintheonlineenvironments
Ï€ Behaviorpolicygeneratingtheofflinedatasets
Î²
V(Â·) Statevaluefunction
Q(Â·,Â·) Action-statevaluefunction
D Offlinedatasets
n(s,a) Numberofsamplesfor(s,a)pairsinofflinedatasets
B Bellmanoperator
h
Ï•(Â·,Â·) Featurerepresentationofstateandaction
Ï•(cid:101)(Â·,Â·) Featureofstate,actionandconfounderinequation(2)
Âµ(Â·) Featurerepresentationofnextstate
d,dâ€² DimensionsoffeaturesforÏ•(Â·,Â·)andÂµ(Â·)
K FeaturematrixexpandedfromÂµinequation(5)
Âµ
C ,C ,C Featureregularity
Ï• Âµ Î²
C Sparsity-relatedconstant
s
Îº Restrictiveeigenvalueconstant
X FeatureKroneckerproduct
u,u ,u Confounders
c Ï€
M,M(u) Binarytransitionmatrix(undercertainconfounders)
G Causalgraph
M(cid:99),M(u) Estimatedcausalmatrix
Î²M,Î²(cid:98)M Optimal/estimatedparametersincausalmatrix
PAG(Â·) ParentalnodeinthecausalgraphG
Ïµ ExogenousnoiseinSCMbyDefinition4
Î»,Î» ,Î» Spectrumregularizerweightinequation(6)
Ï• Âµ
Ïƒ StandarddeviationofexogenousnoiseinSCM
Î¾ Accuracylevelofthepolicy
K IterativeupdatestepsinBECAUSE
E Energy-basedmodel
Î¸
Î´ Levelofâ€˜highprobabilityâ€™
Î“(Â·,Â·) Uncertaintyquantificationfunction
E Î´-uncertaintyquantifierset
Î» Regularizerweightfortheâ„“ norminEBM
EBM 2
A.2 DerivationofDefinition3
(cid:20) 0dÃ—d M (cid:21)
ThenodeofthiscausalgraphG = containstwogroupsofentities: (1)Thestate
0dÃ—dâ€² 0dÃ—d
actionabstractionÏ•(s,a), and(ii)thenextstateabstractionÂµ(sâ€²). WedenoteÏ•(Â·,Â·)(i) astheith
factorintheabstractedstateactionrepresentations,andÂµ(Â·)(j) forthejth factorintheabstracted
staterepresentations.
15ThesourcenodeofallthenodesGisÏ•(s,a)(i),whichistheabstractedstate-actionrepresentation,
andthesinknodeofalltheedgesinGisthetoÂµ(s)â€²(i).
T(sâ€²|s,a)=(cid:2) Ï•(s,a)T Âµ(sâ€²)T(cid:3)(cid:20) 0dÃ—d M (cid:21)(cid:20) Ï•(s,a)(cid:21) =Ï•(s,a)TMÂµ(sâ€²), (10)
0dÃ—dâ€² 0dÃ—d Âµ(sâ€²)
Therefore, G is a bipartite graph, since there will be no edges between Ï•(s,a)(i),Ï•(s,a)(j), or
Âµ(s)(i),Âµ(s)(j).
Consequently,weshowthatGâˆˆDAG.
A.3 Derivationofequation(5)
Definition 4 (Structured Causal Model). An SCM Î¸ := (S,E) consists of a collection S of d
functions[11],
s :=f (PAG(s ),Ïµ ), j âˆˆ[d], (11)
j j j j
wherePAG âŠ‚{s ,...,s }\{s }arecalledparentsofx intheDirectedAcyclicGraph(DAG)G,
j 1 d j j
andE = {Ïµ }d arejointlyindependent. Forinstance, incontinuousstateandactionspace, we
i i=1
parameterizetheworldmodelwithjointGaussianDistribution,i.e. Ïµâˆ¼N(0,ÏƒI ).
ddâ€²
WethenusebilinearMDPtoapproximatetheoriginallikelihoodfunctioninequation(4),i.e.
(cid:89)
p(D;Ï•,Âµ,M)âˆ exp(âˆ’âˆ¥ÂµT(sâ€²)Kâˆ’1âˆ’Ï•T(s,a)Mâˆ¥2),
Âµ 2 (12)
(s,a,sâ€²)âˆˆD
whereK :=(cid:80) Âµ(sâ€²)Âµ(sâ€²)T isaninvertiblematrix. ThenwecanapplyanMLEinequation(5).
Âµ sâ€²âˆˆS
InourBECAUSEalgorithm,theoptimizationofthecausalworldmodelisconductedbysolving
theregularizedMLEprobleminequation(13). ThebiggestdifferencebetweenBECAUSEandthe
offlineversionof[14,15]isthatitaimstoapplyâ„“ regressioninsteadofridgeregressiontoestimate
0
matrixM:
M =argmax[logp(D;Ï•,Âµ,M)âˆ’Î»|M|]
n
M
(cid:88)
=argmin âˆ¥ÂµT(sâ€²)K Âµâˆ’1âˆ’Ï•T(s,a)Mâˆ¥2 2+ Î»âˆ¥Mâˆ¥ 0 . (13)
M (cid:124) (cid:123)(cid:122) (cid:125)
(s,a,sâ€²)âˆˆD
SparsityRegularization
(cid:124) (cid:123)(cid:122) (cid:125)
WorldModelLearning
A.4 Proofofequation(7)
Thederivationdependsonthefollowingre-weightingformulain[18]:
E T(sâ€²|s,a,u)Ï€ (a|s,u )
T(sâ€²|s,a)= pu Î² Ï€ . (14)
E Ï€ (a|s,u )
pu Î² Ï€
Thenweapplyequation(14)tothedecompositioninequation(2)andequation(3),whichyields
E (cid:2) T(sâ€²|s,a,u)Ï€ (a|s,u )(cid:3)
T(sâ€²|s,a)= pu Î² Ï€
E Ï€ (a|s,u )
pu Î² Ï€
E (cid:2) Ï•(s,a)TM(u )Âµ(sâ€²)Ï€ (a|s,u )(cid:3)
=
pu c Î² Ï€
E [Ï€ (a|s,u )]
pu Î² Ï€ (15)
(cid:34) E (cid:2) M(u )Ï€ (a|s,u )(cid:3)(cid:35)
=Ï•(s,a)T pu c Î² Ï€ Âµ(sâ€²)
E [Ï€ (a|s,u )]
pu Î² Ï€
=Ï•(s,a)T M(u)Âµ(sâ€²),
wherethelastequalityholdsbylettingM(u):= E pu[M(uc)Ï€Î²(a|s,uÏ€)].
E pu[Ï€Î²(a|s,uÏ€)]
16B ProofofTheorem1
Inthissection,weprovidetheproofofthesub-optimalityupperboundinTheorem1. Wefirstshow
someusefuldefinitionsandlemmasinSectionB.1. Armedwiththem,weprovidethetheoretical
resultstailoredforthecausaldiscoverysettinginSectionB.2. Furthermore,wegiveadetailedproof
oftheuncertaintysetforminourcausaldiscoveryproblemsinSectionB.3.
B.1 Preliminary
Inthissubsection,wefirstdefinetheÎ´-uncertaintyquantifierÎ“,thenwerefertothelemmasinthe
previousliteraturetoconstructasuboptimalityboundbasedonthedefineduncertaintyquantifierÎ“.
First,wedefinetheBellmanoperatorB ,forsomevaluefunctionV :S (cid:55)â†’R,theBellmanoperator
h
canbedefinedas:
(B V)(s,a)=E[r (s ,a )+V(s )|s =s,a =a]. (16)
h h h h h+1 h h
Similarly,wedenotetheapproximateBellmanoperatoroftheempiricalMDPconstructedfromthe
offlinedatasetDasB (cid:98)hforanyhâˆˆ[H].
Definition5(Î´-UncertaintyQuantifier). Welet{Î“ }H ,Î“ : S Ã—A (cid:55)â†’ RtobeaÎ´-uncertainty
h h=1 h
quantifierwithrespecttodatadistributionP iftheevent:
D
(cid:110) (cid:111)
E = |(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|â‰¤Î“ h(s,a),âˆ€(s,a,h)âˆˆSÃ—AÃ—[H]
satisfiesP (E)â‰¥1âˆ’Î´.
D
Asweconsidertheofflinemodellearningandplanning,wedefinethemodelevaluationerrorateach
stephâˆˆ[H]as
âˆ€(s,a)âˆˆSÃ—A: Î¹ h(s,a)=(B hV(cid:98)h+1)(s,a)âˆ’Q(cid:98)h(s,a), (17)
whereÎ¹ istheerrorinducedbytheapproximateBellmanoperator, especiallythetransitionker-
h
nel based on D. We then identify the source of sub-optimality in our offline MBRL setting by
decomposingthesub-optimalityerrorinLemma1.
Lemma1(DecompositionofSuboptimality[86]).
H H
(cid:88) (cid:88)
âˆ€sâˆˆS : Vâˆ—(s)âˆ’VÏ€(s)=âˆ’ E [Î¹ (s ,a )|s =s]+ E [Î¹ (s ,a )|s =s]
h h Ï€ h hâ€² hâ€² h Ï€âˆ— hâ€² hâ€² hâ€² h
hâ€²=h hâ€²=h
H
(cid:88)
+ E Ï€âˆ—[âŸ¨Q(cid:98)hâ€²(s hâ€²,Â·),Ï€âˆ—(Â·,s hâ€²)âˆ’Ï€ (cid:98)(Â·,s hâ€²)âŸ© A|s
h
=s],
hâ€²=h
(18)
whereÏ€isanylearnedpolicy,Ï€âˆ—istheoptimalpolicythatmaximizesthecumulativereturnasbelow:
H
Ï€âˆ— =argmaxE (cid:104) (cid:88) Î³hâ€² r(s ,a )|s (cid:105) .
Ï€ hâ€² hâ€² h
Ï€
hâ€²=1
Basedonthisdecomposition,wewillgetthebasicformofsub-optimalityerrorboundforgeneral
offlineRLsettingsinLemma2:
Lemma 2 (Suboptimality in standard MDP [86]). Suppose we have {Î“ }H as Î´-uncertainty
h h=1
quantifier. UnderE definedinequation(5),thesuboptimalityerrorboundbyconservativeplanning
satisfies:
H
(cid:88)
âˆ€sâˆˆS : Vâˆ—(s)âˆ’VÏ€(s)â‰¤2 E [Î“ (s ,a )|s =s].
h h Ï€âˆ— hâ€² hâ€² hâ€² 1
hâ€²=h
Thebasicformofsub-optimalityboundinLemma2involvesanuncertaintyquantifierÎ“ ,whichin
h
ourcasewillbefurtherreplacedbyanexactboundinoursparsematrixestimationproblemofcausal
discoveryalgorithms.
17B.2 ProofofTheorem1
Themainresultsholdunderthefollowingtwoassumptions:
Assumption2(Existenceofacorematrixgiventhefeatureembedding). Foreach(s,a)âˆˆSÃ—A,
featurevectorsÏ•(s,a)âˆˆRd,Âµ(s)âˆˆRdâ€² areapproximatedasapriori. Givenaspecificconfounder
setu,thereexistsanunknownmatrixM(u)âˆ— âˆˆRdâ€²Ã—dsuchthat,
T(sâ€²|s,a,u)=Ï•(s,a)TM(u)Âµ(sâ€²). (19)
Assumption3(Featureregularity). Weassumefeatureregularity[14,15]forthefollowingcompo-
nentsoftheconfoundedbilinearMDP:
â€¢ âˆ€u,âˆ¥M(u)âˆ¥2 â‰¤C d,
F M
â€¢ âˆ€(s,a)âˆˆSÃ—A,âˆ¥Ï•(s,a)âˆ¥2 â‰¤C d,
2 Ï•
â€¢ âˆ€sâ€² âˆˆR|S|,âˆ¥ÂµTsâ€²âˆ¥ â‰¤C âˆ¥sâ€²âˆ¥ ,âˆ¥ÂµKâˆ’1âˆ¥ â‰¤Câ€²,
2 Âµ âˆ Âµ 2,âˆ Âµ
â€¢ âˆ€s,a,sâ€² âˆˆSÃ—AÃ—S,âˆ¥Ï•(s,a)Âµ(sâ€²)Tâˆ¥ â‰¤C .
1 Âµ
whereC ,C ,C ,Câ€² aresomeuniversalconstants.
M Ï• Âµ Âµ
(cid:113)
Here,foranymatrixX,âˆ¥Xâˆ¥ :=max (cid:80) X2 representstheoperator2(cid:55)â†’âˆnorm.
2,âˆ i j ij
Proofpipeline. Armedwiththeaboveassumptions,weturntothebilinearMDPsetting,whichthis
workfocuseson. Weshalldevelopthefinite-sampleanalysisbyspecifyingthemainerrortermâ€”-
Î´-uncertaintyquantifierÎ“(seeLemma2)forourtime-homogeneouscorematrixestimationproblem
inthefollowinglemma.
Lemma3(UncertaintyboundforBilinearCausalRepresentation). UndertheAssumption2, 3and
thatT isanSCM(definedin4),fortheBECAUSEalgorithm,fortheÎ¾-optimalpolicy(Vâˆ—(s)âˆ’
1 (cid:101)
VÏ€(s)â‰¤Î¾),âˆ€0â‰¤Î¾ â‰¤1,wehavetheÎ´-uncertaintysetas:
1 (cid:101)
(cid:110)
E
BECAUSE
= |(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|
(cid:115)
â‰²min(cid:8)
C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112)
|S|,C
Ïƒ(cid:112)
âˆ¥Mâˆ¥
)(cid:9) log(1/Î´) ,âˆ€(s,a,h)âˆˆAÃ—SÃ—[H](cid:111)
,
1 Î¾ s 0 n(s,a)
whereC issomeuniversalconstants.
1
Armedwiththeabovelemma,wecompletetheproofofTheorem1byshowingthat
H
(cid:88)
Vâˆ—(s)âˆ’VÏ€(s)â‰¤2 E [Î“ (s ,a )|s =s]
1 (cid:101) 1 (cid:101) Ï€âˆ— hâ€² hâ€² hâ€² 1 (cid:101)
hâ€²=1
H (cid:34) (cid:115) (cid:35)
â‰²2(cid:88) E min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9) log(1/Î´) |s =s
Ï€âˆ— 1 Î¾ s 0 n(s ,a ) 1 (cid:101)
h h
h=1
(20)
ThisconcludestheproofofTheorem1.
B.3 ProofofLemma3
ThekeytoprovingTheorem1istoproveLemma3. TheproofpipelineofLemma3isillustrated
below. In Step 1, we derive the estimation of the causal transition matrix M in BECAUSE as a
sparsityregressionproblem. InStep2,wedecomposetheerrortermswithinÎ´-uncertaintysetinto
two parts: (a) error due to the under-explored dataset, (b) error due to optimization error in the
structuredcausalmodel. ThenweboundbotherrortermsinStep3andStep4,respectively. Finally,
inStep5,wesumupalltheresultsandderivetheformofÎ´-uncertaintyquantifierwhichwillleadto
ourfinalresultsinTheorem1.
18Step1: derivingtheoutputmodelofBECAUSE. Recallingtheoriginaloptimizationproblemin
equation(13)toestimatethecorematrix:
M(cid:99)=argmax[logp(Ï•,Âµ,M)âˆ’Î»|M|]
M
(cid:88)
=argmin âˆ¥Âµ(sâ€²)TK Âµâˆ’1âˆ’Ï•(s,a)TMâˆ¥2 2+ Î»âˆ¥Mâˆ¥ 0 . (21)
M (cid:124) (cid:123)(cid:122) (cid:125)
(s,a,sâ€²)âˆˆD
SparsityRegularization
(cid:124) (cid:123)(cid:122) (cid:125)
ModelLearning
This part of derivation aims to transform the above estimation problem into a linear regression
problem, withtheregressiondatapairs(X,T)andsomeunknownparametersÎ² associatedwith
maskM tobeestimated. Eventually,weâ€™llderivetherepresentationofeachpartofÎ²M,X,T,and
eventuallyreachthefollowingform:
(cid:88)
min [âˆ¥T (sâ€² |s ,a )âˆ’X Î²Mâˆ¥2+Î»âˆ¥Î²Mâˆ¥ ]. (22)
Î²M
Ï€Î² i i i i 2 0
(si,ai,sâ€² i)âˆˆD
Wedefineeachcomponentofthistargetformofâ„“ regressionasfollows:
0
â€¢ ForunknownparametersÎ²M: WefirstdefineÎ²M âˆˆ[0,1]ddâ€² asacolumndimensionalvector
consistingofalltheentriesintime-homogenouscausalmatrixM,whereÎ²M denotesthei-th
i
entryofÎ²M. Besides,wedefineÎ²M asthetruecorematrixgivensomeofflinedatasetD and
D
correspondingdatapairsT ,X thatsatisfiesT =Î²MX +Ïµ.
data data data D data
â€¢ FordatasetD: RecallthetransitionpairsintheofflinedatasetD ={s ,a ,sâ€²} . Here,n
i i i 1â‰¤iâ‰¤n
representsthesamplesizeovercertainstate-actionpairsintherolloutdatabysomebehavior
policyÏ€ . Forsimplicity,wedenotenâ‰œn(s,a)inthefollowingderivation,whichismentioned
Î²
inSection2.1.
â€¢ ForregressiontargetT : Then,weintroducethefollowingtransitiontargetsT inducedby
Ï€Î² Ï€Î²
theofflinedatasetDsampledwithbehaviorpolicyÏ€ :
Î²
(cid:80) 1(s =s,a =a,sâ€² =sâ€²)
T (sâ€²|s,a):= (si,ai,sâ€² i)âˆˆD i i i
Ï€Î² (cid:80) 1(s =s,a =a)
(si,ai,sâ€² i)âˆˆD i i
(23)
1 (cid:88)
= 1(s =s,a =a,sâ€² =sâ€²).
n(s,a) i i i
(si,ai,sâ€² i)âˆˆD
Underthenfinitesamplesintheofflinedataset,weassumethatT âˆ¼N(E[T ],Ïƒ2I ). The
Ï€Î² Ï€Î² n
above definition specifies the regression target in the â„“ regression problem, and we denote
0
T =[T (sâ€²|s ,a ),Â·Â·Â· ,T (sâ€² |s ,a )]T âˆˆRn astheempiricaltransitionprobabilitiesof
Ï€Î² Ï€Î² 1 1 1 Ï€Î² n n n
certaintransitionpairsintheofflinedataD ={s ,a ,sâ€²} .
i i i 1â‰¤iâ‰¤n
â€¢ ForregressiondataX: Next, weneedtospecifythedataX intheregressionproblem. We
denote the i-th row of X as the i-th sample in the offline transition pairs X âˆˆ D, which is a
i
vectorofKroneckerproductbetweenÏ•(s i,a i)âˆˆRdandnormalized Âµ C(s Âµâ€² i) âˆˆRdâ€² (withoutloss
ofgenerality,weassumeC =1andonlyneedtonormalizeÂµ(sâ€²)byC ):
Ï• i Âµ
Âµ(sâ€²)
X =Ï•(s ,a )âŠ— i
i i i C
Âµ
(24)
1
= [Ï•(s ,a )(1)Âµ(sâ€²)(1),Ï•(s ,a )(1)Âµ(sâ€²)(2),Â·Â·Â· ,Ï•(s ,a )(d)Âµ(sâ€²)(dâ€²)]T
C i i i i i i i i i
Âµ
Asaresult, X âˆˆ Rddâ€², sincethereareinallnsamplesinofflinedataset, X âˆˆ RnÃ—ddâ€² isthe
i
dataset-dependent matrix with all n rows of samples, and d and dâ€² are the latent dimension
of Ï• and Âµ, respectively. Based on the feature regularity criteria in Assumption 3, we have
âˆ¥X âˆ¥ â‰¤âˆ¥X âˆ¥ â‰¤1,âˆ¥Xâˆ¥ â‰¤1.
i 2 i 1 âˆ
The prior work [14] estimate the transition kernel of a bilinear MDP using the following ridge
regression:
minE âˆ¥Âµ(sâ€²)TKâˆ’1âˆ’Ï•(s,a)TMâˆ¥2+Î»âˆ¥Mâˆ¥ . (25)
(s,a,sâ€²)âˆˆD Âµ 2 2
M
19Inthispaper,inordertopromotethesparsityofthematrixM,weintroducetheâ„“ regularization
0
termandarriveatthefollowingoptimizationproblem:
(cid:88)
min [âˆ¥Âµ(sâ€²)TKâˆ’1Âµ(sâ€²)âˆ’Ï•(s ,a )TMÂµ(sâ€²)âˆ¥2+Î»âˆ¥Î²Mâˆ¥ ]
i Âµ i i i i 2 0
Î²M
(si,ai,sâ€² i)âˆˆD
(26)
(cid:88)
â†’m Î²Min [âˆ¥T Ï€Î²(sâ€²
i
|s i,a i)âˆ’X iÎ²Mâˆ¥2 2+Î»âˆ¥Î²Mâˆ¥ 0]=:Î²(cid:98) DM,
(si,ai,sâ€² i)âˆˆD
wherewedenotethesolutionassociatedwiththeofflinedatasetDasÎ²(cid:98)M. Here,weusetheempirical
D
versionconstructedbythefinitesamplesinofflinedatasetD.
Giventhegoal-conditionedrewardsetting,forasingleepisodesâˆ¼Ï„,r(s,a;g)=1ifandonlyif
s=g,otherwiser(s,a;g)=0,asisspecifiedinSection2.1. Sinceweareessentiallypredictingthe
(cid:80)
probabilities(normalizedtoasumof1)ofwhetherthenextstateisthegoalstate,i.e. V(cid:98)(s)=1.
sâˆˆS
Therefore,wehaveâˆ¥V(cid:98)(Â·)âˆ¥
1
â‰¤1.
Asisdenotedbyequation(23),forthespecificofflinedatasetcollectedbysomebehaviorpoliciesÏ€ ,
Î²
wehavetheregressiontargetT Ï€Î²(sâ€²|s,a)= (cid:80) ( (cid:80)si, (a si i, ,s aâ€² i i) ,âˆˆ sD
â€²
i)âˆˆ1 D(s 1i= (ss i, =ai s= ,aa i, =sâ€² i a= )sâ€²) ,andthecorresponding
featuresinducedbythedatasetX
i
=Ï•(s i,a i)âŠ— Âµ C(s Âµâ€² i). Wehavethefollowingequationshold:
T =XÎ²M +Ïµ, (27)
Ï€Î² D
whereÎ² isthetrueunderlyingtransitionmaskgiventheofflinedatasetD,Ïµâˆ¼N(0,ÏƒÂ·I )issome
D n
exogenousnoiseofthetransitionmodel.
Specifically,fortheregressionproblem,wetransformtheoriginaltrajectorydatasetDas[X,T ],
Ï€Î²
as the representation of transition pairs rolled out by the behavior policy Ï€ . Similarly, we can
Î²
definesomeâ€™well-exploreddatasetâ€™Dâˆ—,whichisaninfinitedatasetrolloutbythebehaviorpolicy
Ï€ , Dâˆ— = {s ,a ,r }âˆ , similar to the definition of regression target T in equation (23) and
Î² i i i i=0
representationdataX inequation(24),wehave[X,E[T ]]astheregressionpairswithaccesstothe
Ï€Î²
truetransitionprobabilitydistributionforallthestateactionpairs(s,a). HereX âˆˆ RnÃ—ddâ€² isthe
regressiondatadefinedinequation(22). Recallingthedefinitioninequation(23),wecanfurther
buildtheregressiontargetunderwell-exploreddatasetDâˆ—as:
(cid:80) 1(s =s,a =a,sâ€² =sâ€²)
E[T (sâ€²|s,a)]= (si,ai,sâ€² i)âˆˆDâˆ— i i i
Ï€Î² (cid:80) (si,ai,sâ€² i)âˆˆDâˆ—1(s i =s,a i =a) (28)
=E [1(s =s,a =a,sâ€² =sâ€²)]
sâ€²âˆ¼T(Â·|s,a) i i i
We denote E[T ] = (cid:2)E[T (sâ€²|s ,a )],Â·Â·Â·E[T (sâ€² |s ,a )](cid:3)T âˆˆ Rn. In practice, with finite
Ï€Î² Ï€Î² 1 1 1 Ï€Î² n n n
samplesizen,wehaveE[T (sâ€²|s,a)]=T(sâ€²|s,a)+Ïµ=XÎ²M +Ïµ. Inaddition,wealsointroduce
Ï€Î² Dâˆ—
avectorformTâˆˆR|S||A|Ã—|S|sothatâˆ€(s,a)âˆˆS Ã—A,wehave
T(Â·|s,a)=(cid:2) T(sâ€²|s,a) T(sâ€²|s,a) Â·Â·Â· T(sâ€² |s,a)(cid:3)T âˆˆR|S|,
1 2 |S|
where the state space is denoted as S = {sâ€²,Â·Â·Â· ,s } are all possible states. Similar to the
1 |S|
KroneckerproductwedefineforX inequation(26),wedefineXinamatrixformforanystate-action
pair(s,a):
X(Â·|s,a)=(cid:2)
Ï•(s,a)âŠ—
Âµ(sâ€² 1)
,Â·Â·Â·Ï•(s,a)âŠ—
Âµ(sâ€² |S|) (cid:3)T âˆˆR|S|Ã—ddâ€²
.
C C
Âµ Âµ
Inaddition,weletX(sâ€²|s,a) âˆˆ R1Ã—ddâ€² denotethesâ€²-throwofX(Â·|s,a)associatedwiththestate
sâ€² âˆˆS. Consequently,theestimatedtransitionkernelcanbeexpressedasfollows:
T(cid:98)(Â·|s,a)=Ï•T(s,a)M(cid:99)Âµ(Â·)=X(Â·|s,a)Î²(cid:98)M.
D
Step 2: decomposing the term of interest. To begin with, recalling the definition of Bellman
operatorB inequation(16)andapplyingHÃ¶lderâ€™sinequality,thetermofinterestforanytimestep
h
201â‰¤hâ‰¤H andstate-actionpair(s,a)âˆˆSÃ—Acanbecontrolledas
|(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|â‰¤|âŸ¨T(cid:98)(Â·|s,a)âˆ’T(Â·|s,a),V(cid:98)h+1âŸ©|
â‰¤âˆ¥T(cid:98)(Â·|s,a)âˆ’T(Â·|s,a)âˆ¥ âˆâˆ¥V(cid:98)h+1âˆ¥
1
(29)
â‰¤âˆ¥T(cid:98)(Â·|s,a)âˆ’T(Â·|s,a)âˆ¥ âˆ,
wherethefirstinequalityisheldgivenourgoal-conditionedrewardformulationinsection2.1. To
continue,wehave
|(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|â‰¤âˆ¥T(cid:98)(Â·|s,a)âˆ’T(Â·|s,a)âˆ¥
âˆ
(30)
=âˆ¥X(Â·|s,a)Î²(cid:98) DM âˆ’X(Â·|s,a)Î²Mâˆ¥
âˆ
Here,werecallÎ²(cid:98)M representstheparametervectorintheestimatedcausalmasksbasedontheoffline
D
datasetD sampledbyÏ€ Î². Similarly,wedenoteÎ²(cid:98) DM
âˆ—
astheestimatedcausalmaskoutputtedfrom
equation(26)basedontheinfinitedatasetDâˆ— generatedbythebehaviorpolicyÏ€ . Then,wecan
Î²
furthercontrolequation(30)as
âˆ¥X(Â·|s,a)Î²(cid:98) DM âˆ’X(Â·|s,a)Î²Mâˆ¥
âˆ
=âˆ¥X(Â·|s,a)Î²(cid:98) DM âˆ’X(Â·|s,a)Î²(cid:98) DM
âˆ—
+X(Â·|s,a)Î²(cid:98) DM
âˆ—
âˆ’X(Â·|s,a)Î²Mâˆ¥
âˆ
â‰¤âˆ¥X(Â·|s,a)[Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—]âˆ¥ âˆ+âˆ¥X(Â·|s,a)[Î²(cid:98) DM
âˆ—
âˆ’Î²M]âˆ¥
âˆ
â‰¤âˆ¥X(Â·|s,a)âˆ¥ âˆâˆ¥Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—âˆ¥ âˆ+âˆ¥X(Â·|s,a)âˆ¥ âˆâˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥
âˆ
â‰¤âˆ¥Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—âˆ¥ âˆ+âˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥
âˆ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(a) (b)
(31)
Herethelastinequalitycomesfromthefactthat
(cid:88)
âˆ¥X(Â·|s,a)âˆ¥ =max |X(Â·|s,a) |=maxâˆ¥X(Â·|s,a) âˆ¥
âˆ ij i 1
iâˆˆ|S| iâˆˆ|S|
jâˆˆ[ddâ€²]
C
=maxâˆ¥Ï•(s,a)Âµ(sâ€²)Tâˆ¥ â‰¤ Âµ =1
iâˆˆ|S| i 1 C Âµ
basedonthedefinitionofX inequation(24)andassumption3. Here(a)comesfromthemismatch
errorbetweenthedemonstratedofflinedatasetandsomeoptimalrolloutdatasets. And(b)comes
from the error of the â„“ optimization of causal masks given the existence of exogenous noise Ïƒ
0
definedbySCMinDefinition4. Wewillcontrolthemseparatelyinthefollowing.
Step 3: Controlling term (a). We need to consider the optimization process in the original
regressionprobleminequation(22)tofullyunderstandthedifferencebetweenÎ²(cid:98)M andÎ²(cid:98)M,where
D Dâˆ—
theonlydifferenceisthatthelatterusesaperfectdatasetwithinfinitesamples. Theoptimization
problemwetarget(cf.equation(26))canbesolvedbytheiterativehardthresholdingalgorithm(IHT)
proposedby[87]IHToffersaniterativesolutionfortheâ„“ regressionproblem,armedwithahard
0
thresholdingoperatorasbelow:
(cid:26)
max{0,Î² âˆ’Î»} ifÎ² >Î»
[g (Î²)] = j j (32)
Î» j 0 ifÎ² â‰¤Î», j =1,Â·Â·Â· ,ddâ€².
j
We denote Î²(cid:98)M(i) as the estimated causal mask parameters after i-th iterations with dataset D.
D
Similarly,wedenoteÎ²(cid:98)M(i)astheestimationafteri-thiterationswithdatasetDâˆ—. Weinitializethe
Dâˆ—
graphtobeafullgraphregardlessofthedatasets(Dâˆ—orD)usedintheoptimizationprocess,leading
toÎ²(cid:98)M(0)=Î²(cid:98)M(0)=1âˆˆRddâ€².
Dâˆ— D
RecallthatX âˆˆRnÃ—ddâ€²,T
Ï€Î²
âˆˆRn,E[T Ï€Î²]âˆˆRnandÎ²(cid:98) DM is[0,1]ddâ€² basedonthedefinitioninthe
originalâ„“ optimizationprobleminequation(22),equation(23)andequation(28). Theupdaterules
0
ofusingeitherthedatasetDorDâˆ—canbewrittenas:
Î²(cid:98) DM(i)=g Î»(Î²(cid:98) DM(iâˆ’1)+Î·XT[T
Ï€Î²
âˆ’XÎ²(cid:98) DM(iâˆ’1)])
(33)
Î² DM âˆ—(i)=g Î»(Î²(cid:98) DM âˆ—(iâˆ’1)+Î·XT[E[T Ï€Î²]âˆ’XÎ²(cid:98) DM âˆ—(iâˆ’1)]).
21NotethatthedifferencebetweenthetwoparametersÎ²(cid:98)M andÎ²(cid:98)M essentiallyreliesonthedifference
D Dâˆ—
between two pairs of transition kernel estimation datasets [X,E[T ]] and [X,T ]. It is easily
Ï€Î² Ï€Î²
verified that the hard-thresholding operator [g (Â·)] defined in equation (32) is L = 1-Lipschitz.
Î» i
Accordingto[88],wecansetthelearningrateÎ· â‰¤ 1 =1here. UsingtheLipschitzproperty,ateach
L
iterativeupdatestepi,wecancontrolthedifferencebetweentheestimatedparametersobtainedby
usingDorDâˆ—as
âˆ¥Î²(cid:98) DM(i)âˆ’Î²(cid:98) DM âˆ—(i)âˆ¥
2
=âˆ¥g Î»(cid:0) Î²(cid:98) DM(iâˆ’1)+Î·XT[T
Ï€Î²
âˆ’XÎ²(cid:98) DM(iâˆ’1)](cid:1)
âˆ’g Î»(cid:0) Î²(cid:98) DM âˆ—(iâˆ’1)+Î·XT[E[T Ï€Î²]âˆ’XÎ²(cid:98) DM âˆ—(iâˆ’1)](cid:1) âˆ¥
2
â‰¤âˆ¥(cid:0) Î²(cid:98) DM(iâˆ’1)+Î·XT[T
Ï€Î²
âˆ’XÎ²(cid:98) DM(iâˆ’1)](cid:1) âˆ’(cid:0) Î²(cid:98) DM âˆ—(iâˆ’1)+Î·XT[E[T Ï€Î²]âˆ’XÎ²(cid:98) DM âˆ—(iâˆ’1)](cid:1) âˆ¥
2
â‰¤âˆ¥(I
ddâ€²
âˆ’Î·XTX)[Î²(cid:98)D(iâˆ’1)âˆ’Î²(cid:98)Dâˆ—(iâˆ’1)]+Î·XT[T
Ï€Î²
âˆ’E[T Ï€Î²]]âˆ¥
2
â‰¤âˆ¥I
ddâ€²
âˆ’Î·XTXâˆ¥ 2âˆ¥Î²(cid:98)D(iâˆ’1)âˆ’Î²(cid:98)Dâˆ—(iâˆ’1)âˆ¥ 2+Î·âˆ¥Xâˆ¥ 2âˆ¥T
Ï€Î²
âˆ’E[T Ï€Î²]âˆ¥
2
â‰¤âˆ¥Î²(cid:98)D(iâˆ’1)âˆ’Î²(cid:98)Dâˆ—(iâˆ’1)âˆ¥ 2+Î·âˆ¥T
Ï€Î²
âˆ’E[T Ï€Î²]âˆ¥ 2,
(34)
Here, I represent the identity matrix of size ddâ€² Ã—ddâ€², the last inequality holds based on the
ddâ€²
factthatâˆ¥Xâˆ¥ â‰¤ 1definedinequation(24). SinceÎ· â‰¤ 1 = 1,asaresult,âˆ¥I âˆ’Î·XTXâˆ¥ =
2 L ddâ€² 2
max(1âˆ’Î·Î»(XTX))â‰¤1. WeperformIHTforsufficientK >0iterationsandoutputthelaststep
estimation as our solutions for either the offline dataset D (we use for practical optimization) or
theperfectdatasetDâˆ—. Inpractice, foranydatasetsuchasD andanyaccuracylevel0 â‰¤ Î¾ â‰¤ 1,
we have the output of IHT gradually converges to the optimal solution Î²(cid:98)M of the problem in
D
(cid:16) (cid:17)
equation(26). Namely,afteratmostK â‰ƒlog âˆ¥Mâˆ¥0 steps[87,Corollary1],theoutputsatisfies
Î¾
(cid:13) (cid:13)
(cid:13)Î²(cid:98)M âˆ’Î²(cid:98)M(K)(cid:13) â‰¤ Î¾. Similarly, based on the perfect dataset Dâˆ—, the output of IHT gradually
(cid:13) D D (cid:13) 4
2
convergestoÎ²(cid:98)M atthesamerate. Consequently,thetermofinterest(a)canbeboundedrecursively
Dâˆ—
as:
(cid:13) (cid:16) (cid:17) (cid:16) (cid:17)(cid:13)
âˆ¥Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—âˆ¥
âˆ
=(cid:13) (cid:13)Î²(cid:98) DM(K)âˆ’Î²(cid:98) DM âˆ—(K)+ Î²(cid:98) DM âˆ’Î²(cid:98) DM(K) + Î²(cid:98) DM
âˆ—
âˆ’Î²(cid:98) DM âˆ—(K) (cid:13)
(cid:13)
âˆ
(cid:13) (cid:13) Î¾ Î¾
â‰¤(cid:13)Î²(cid:98)M(K)âˆ’Î²(cid:98)M(K)(cid:13) + +
(cid:13) D Dâˆ— (cid:13) âˆ 4 4
(cid:13) (cid:13) Î¾ Î¾
â‰¤(cid:13)Î²(cid:98)M(K)âˆ’Î²(cid:98)M(K)(cid:13) + +
(cid:13) D Dâˆ— (cid:13) 2 4 4
(35)
Î¾
â‰¤âˆ¥Î²(cid:98) DM(0)âˆ’Î²(cid:98) DM âˆ—(0)âˆ¥ 2+KÎ·âˆ¥T
Ï€Î²
âˆ’E[T Ï€Î²]âˆ¥ 2+
2
Î¾
=KÎ·âˆ¥T âˆ’E[T ]âˆ¥ +
Ï€Î² Ï€Î² 2 2
â‰²Î·log(cid:0)âˆ¥Mâˆ¥ 0(cid:1) âˆ¥T âˆ’E[T ]âˆ¥ + Î¾ ,
Î¾ Ï€Î² Ï€Î² 2 2
wherethesecondinequalityholdsbyrecursivelyapplyingequation(34)toK,Kâˆ’1,Â·Â·Â· ,0iterations,
the last equality is due to the fact that Î²(cid:98)M(0) = Î²(cid:98)M(0), and the last inequality holds by setting
D Dâˆ—
K =log(âˆ¥Mâˆ¥2).
Î¾
Nowtheremainingoftheproofwillfocusoncontrollingâˆ¥T âˆ’E[T ]âˆ¥ . Recallthatwehave
Ï€Î² Ï€Î² âˆ
definedtheregressiontargetinequation(23)andequation(28):
n(s,a)
1 (cid:88)
T (sâ€²|s,a)= 1(s =s,a =a,sâ€² =sâ€²),
Ï€Î² n(s,a) i i i
i=1
E[T (sâ€²|s,a)]=E [1(s =s,a =a,sâ€² =sâ€²)].
Ï€Î² sâ€²âˆ¼T(Â·|s,a) i i i
Proposition1(Well-exploreddataset). WiththeofflinedatasetDofintotalnsampleswithn(s,a)
samplesgeneratedindependentlyconditionedonany(s,a). Forany0<Î´ <1,withprobabilityat
22least1âˆ’Î´,onehas
(cid:115)
(cid:18) (cid:19)
|S| |S||A|
âˆ€(s,a)âˆˆSÃ—A: âˆ¥T (Â·|s,a)âˆ’E[T (Â·|s,a)]âˆ¥ â‰¤C log ,
Ï€Î² Ï€Î² 2 Î² n(s,a) Î´
forsomeuniversalconstantC .
Î²
Theabovepropositioncanbedirectlyprovedbyapplying[89,Lemma17]overall(s,a)âˆˆSÃ—A:
(cid:115)
(cid:18) (cid:19)
14|S| 2|S||A|
max âˆ¥T (Â·|s,a)âˆ’E[T (Â·|s,a)]âˆ¥ â‰¤ log . (36)
(s,a)âˆˆSÃ—A Ï€Î² Ï€Î² 1 n(s,a) Î´
Asaresult,wecanfurtherextendtheresultsinequation(35)toboundtheterm(a)asfollows:
Î¾
âˆ¥Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—âˆ¥
âˆ
â‰¤KÎ·âˆ¥T
Ï€Î²
âˆ’E[T Ï€Î²]âˆ¥ 2+
2
(cid:115)
(cid:18) (cid:19)
â‰²Î·C C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1) |S| log |S||A| + Î¾
Î² Âµ Î¾ n(s,a) Î´ 2
(cid:115) (37)
(cid:18) (cid:19)
â‰¤C C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1) |S|
log
|S||A|
+
Î¾
Î² Âµ Î¾ n(s,a) Î´ 2
(cid:115)
(cid:18) (cid:19)
â‰œC log(cid:0)âˆ¥Mâˆ¥ 0(cid:1) |S| log |S||A| + Î¾ ,
1 Î¾ n(s,a) Î´ 2
whereC = C C issomeconstantthatisrelatedtothefeatureregularity,Î¾ istheleveloferror
1 Î² Âµ
toleranceinthecumulativevaluereturns,inoursetting,0â‰¤Î¾ â‰¤1.
Step4: Controllingterm(b). For(b)inequation(31),whichisâˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥ âˆ,weareinterested
inwhatistheoptimizationerrorgivenfinitewell-exploredofflinedatasetDâˆ—. Heretheoptimization
errormainlyoriginatesfromtheGaussiannoiseintheSCMformulationinDefinition4. Yetour
causaldiscoverymodule,i.e. anâ„“ estimator,willalwaysencountersomeestimationerrorinduced
0
bytheexogenousnoise.
Firstly,wehavetheboundedrelationshipbetweenâ„“ andâ„“ norm:
2 âˆ
âˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥
âˆ
â‰¤âˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥ 2,
thenwecananalyzetheerrorboundforâ„“ regressioninthesenseofâ„“ norm.
0 2
Thederivationbelowgenerallyfollowstheâ„“ regularizedlinearregressionboundin[90]. Basedon
0
Assumption2and3,thereexistsM,wedenotethisoptimalsolutioninthevectorformasÎ²M.
Besidestheaforementionedoptimizationerrorintheiterativethresholdingupdateprocess,according
totheSCMinDefinition4andAssumption2andProposition1,wedenoteafinitesubsetofobserved
transitionprobabilitiesofthewell-exploreddataE[T ]withsizen: T âˆˆRn. Bydefinitionabove,
Ï€Î² obv
wecanthenassumethefinite-sampleregressiontargetT isgeneratedbycausalfeaturesoftransition
pairs(denotedbyX =X =X ,X âˆˆRnÃ—ddâ€²)andtheground-truthcausalmask(representedby
Ï€âˆ— Ï€Î²
Î²M âˆˆRddâ€²)withthefollowingequation:
T â‰œE[T ]=T +Ïµ=XÎ²M +Ïµ. (38)
obv Ï€Î²
HereÏµâˆ¼N(0,ÏƒI )istheindependentexogenousnoisedefinedinDefinition4. Weâ€™llthenusethe
n
aboveequationtoboundterm(b)inequation(31).
Specifically,wesolvedthisâ„“ regressionproblemwithitsboundedformasfollows:
0
minâˆ¥T âˆ’XÎ²Mâˆ¥2, s.t.âˆ¥Î²Mâˆ¥ â‰¤s. (39)
obv 2 0
Î²M
InBECAUSE,weselectthesparsitylevelsâ‰ˆâˆ¥Î²Mâˆ¥ =âˆ¥Mâˆ¥ â‰¤ddâ€².
0 0
23Forsimplicity,wedenotetheapproximatesolutionÎ²(cid:98)M inequation(39)asÎ²(cid:98)M. Bythevirtueof
Dâˆ—
optimalityofthesolutionÎ²M inequation(39),wefindthat
âˆ¥T obvâˆ’XÎ²Mâˆ¥2
2
â‰¤âˆ¥T obvâˆ’XÎ²(cid:98)Mâˆ¥2 2, (40)
thenbyexpandingandshiftingtheterms, wecanderivethebasicinequalityfortheâ„“ estimator
0
above:
âˆ¥T obvâˆ’XÎ²Mâˆ¥2
2
â‰¤âˆ¥(T obvâˆ’XÎ²M)+(XÎ²M âˆ’XÎ²(cid:98)M)âˆ¥2
2
=( =38 â‡’) âˆ¥Ïµâˆ¥2 â‰¤âˆ¥Ïµ+(XÎ²M âˆ’XÎ²(cid:98)M)âˆ¥2
2 2
=âˆ¥Ïµâˆ¥2+âˆ¥XÎ²(cid:98)M âˆ’XÎ²Mâˆ¥2+2âŸ¨Ïµ,XÎ²Î²âˆ— âˆ’XÎ²(cid:98)MâŸ©
2 2
SincebothÎ²(cid:98)M andÎ²M ares-sparse,thevectorÎ²(cid:98)M âˆ’Î²M isatmost2s-sparse. Wedenotetheset
of all 2s-sparse ddâ€²-dimensional vector set as Tddâ€²(2s), we denote v = I(Î²(cid:98)M âˆ’Î²M = 0) as an
indicatorvector,v
i
=0ifandonlyifÎ²(cid:98) iM âˆ’Î² iM,âˆ€iâˆˆ[ddâ€²].
Byshiftingtheterms,weusethesub-GaussianassumptionandfurtheruseHÃ¶lderâ€™sinequalitytoget
thefollowingresults:
1 2
âˆ¥XÎ²(cid:98)M âˆ’XÎ²Mâˆ¥2 â‰¤ âŸ¨XTÏµ,Î²(cid:98)M âˆ’Î²MâŸ©
n(s,a) 2 n(s,a)
2
â‰¤ n(s,a)âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥
2
sup âŸ¨v,XTÏµâŸ©
vâˆˆTddâ€²(2s)
(XTÏµ) (41)
â‰¤2âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥
2
sup âˆ¥ n(s,a)Sâˆ¥
2
|S|=2s
(cid:115)
2slog(ddâ€²/Î´)
â‰¤2âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥ 2Ïƒ
n(s,a)
,
whereÏµâˆ¼N(0,Ïƒ2I )istheexogenousnoisevariableintheSCMinDefinition4.
n
Thenbysimplyapplyingrestrictedeigenvalue(RE)condition,forsomeÎº(S)>0,wehave
(cid:115)
1 2slog(ddâ€²/Î´)
Îº(S)âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥2
2
â‰¤ n(s,a)âˆ¥X(Î²(cid:98)M âˆ’Î²M)âˆ¥2
2
â‰¤2âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥ 2Ïƒ
n(s,a)
Therefore,wehave:
âˆš (cid:114) (cid:115)
2Ïƒ 2s log(ddâ€²/Î´) âˆ¥Mâˆ¥ log(ddâ€²/Î´)
âˆ¥Î²(cid:98)M âˆ’Î²Mâˆ¥2
2
â‰¤
Îº(S) n
â‰œC sÏƒ 0
n(s,a)
(42)
withprobabilityatleast1âˆ’Î´,whichboundsterm(b)inequation(31).
Step5: Summinguptheresults Summarizingbothboundsforterms(a)and(b)inequation(31),
wewillgetthefollowingboundswithprobability1âˆ’Î´:
|(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|â‰¤âˆ¥Î²(cid:98) DM
âˆ—
âˆ’Î²Mâˆ¥ âˆ+âˆ¥Î²(cid:98) DM âˆ’Î²(cid:98) DM âˆ—âˆ¥
âˆ
(cid:115) (cid:115)
â‰¤C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1) |S| log(cid:18) |S||A|(cid:19)
+C
Ïƒ(cid:112)
âˆ¥Mâˆ¥
log(ddâ€²/Î´)
+
Î¾
1 Î¾ n(s,a) Î´ s 0 n(s,a) 2
(cid:115)
â‰²min(cid:8)
C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112)
|S|,C
Ïƒ(cid:112)
âˆ¥Mâˆ¥
)(cid:9) log(1/Î´)
1 Î¾ s 0 n(s,a)
(43)
Therefore,wecompletetheproofofLemma3byshowingthatforall(s,a)âˆˆAÃ—A,hâˆˆ[H],
(cid:40)
E
BECAUSE
|(B (cid:98)hV(cid:98)h+1)(s,a)âˆ’(B hV(cid:98)h+1)(s,a)|
(44)
(cid:115) (cid:41)
â‰²min(cid:8)
C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112)
|S|,C
Ïƒ(cid:112)
âˆ¥Mâˆ¥
)(cid:9) log(1/Î´)
1 Î¾ s 0 n(s,a)
24Thefinalboundoftheterm(b)includesadependencyofO(âˆš1 )andlogarithmofdimensionalityddâ€²
n
intheestimatedtransitionmatrix. ItalsoincursadependencyoferrortoleranceÎ¾orSCMâ€™snoise
âˆš (cid:112)
levelÏƒsquarerootofsparsitylevel s= âˆ¥Mâˆ¥ .
0
Sofar,weprovethefollowingboundintheorem1:
H (cid:34)(cid:115) (cid:35)
Vâˆ—(s)âˆ’VÏ€(s)â‰¤min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9)(cid:88) E log(1/Î´) |s =s ,
1 (cid:101) 1 (cid:101) 1 Î¾ s 0 Ï€âˆ— n(s ,a ) 1 (cid:101)
h h
h=1
(45)
InordertoachieveÎ¾-optimalpolicysuchthatVâˆ—(s)âˆ’VÏ€(s)â‰²Î¾,theRHSneedstosatisfy:
1 (cid:101) 1 (cid:101)
H (cid:34)(cid:115) (cid:35)
min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9)(cid:88) E log(1/Î´) |s =s â‰²Î¾ (46)
1 Î¾ s 0 Ï€âˆ— n(s ,a ) 1 (cid:101)
h h
h=1
(cid:113)
Wefirstmultiply min E (cid:2) n(s ,a )|s =s(cid:3) ,andthentakethesquareonboth
(s,a,h)âˆˆSÃ—AÃ—[H] Ï€â‹† h h 1 (cid:101)
sides. Wehave:
ï£®(cid:115) ï£¹
min(cid:8) C 1log(cid:0)âˆ¥M Î¾âˆ¥ 0(cid:1)(cid:112) |S|,C sÏƒ(cid:112) âˆ¥Mâˆ¥ 0)(cid:9)(cid:88)H E Ï€âˆ—ï£° log(1/Î´)min (s, na, (h s)âˆˆ ,S aÃ—A )Ã—[H]n(s h,a h)(cid:3) |s 1 =s (cid:101)ï£»
h h
h=1
(cid:114)
â‰²Î¾ min E (cid:2) n(s ,a )|s =s(cid:3)
Ï€â‹† h h 1 (cid:101)
(s,a,h)âˆˆSÃ—AÃ—[H]
(47)
Giventhedefinition,LHSinequation(47)satisfies:
H
LHS â‰²min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9)(cid:88) E (cid:104)(cid:112) log(1/Î´)|s =s(cid:105) (48)
1 Î¾ s 0 Ï€âˆ— 1 (cid:101)
h=1
ToensurethesatisfactionofÎ¾-optimalpolicy,wethuswouldliketheRHSofequation(47)satisfies:
(cid:114)
RHS =Î¾ min E (cid:2) n(s ,a )|s =s(cid:3)
Ï€â‹† h h 1 (cid:101)
(s,a,h)âˆˆSÃ—AÃ—[H]
H
â‰³min(cid:8) C log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112) |S|,C Ïƒ(cid:112) âˆ¥Mâˆ¥ )(cid:9)(cid:88) E (cid:104)(cid:112) log(1/Î´)|s =s(cid:105) (49)
1 Î¾ s 0 Ï€âˆ— 1 (cid:101)
h=1
=min(cid:8)
C
log(cid:0)âˆ¥Mâˆ¥ 0(cid:1)(cid:112)
|S|,C
Ïƒ(cid:112)
âˆ¥Mâˆ¥
)(cid:9) H(cid:112)
log(1/Î´)
1 Î¾ s 0
Consequently,wecanshiftthetermsandgetthesamplecomplexityboundfortheÎ¾-optimalpolicy,
âˆ€0â‰¤Î¾ â‰¤1:
min(cid:8) C2log2(cid:0)âˆ¥Mâˆ¥0(cid:1) |S|,C2Ïƒ2âˆ¥Mâˆ¥ (cid:9) Â·H2log(1/Î´)
min E (cid:2) n(s ,a )|s =s(cid:3)â‰³ 1 Î¾ s 0 ,
(s,a,h)âˆˆSÃ—AÃ—[H] Ï€â‹† h h 1 (cid:101) Î¾2
(50)
C AdditionalExperimentsDetails
Inthissection,weprovideadditionalexperimentresultsandalgorithmimplementationdetails.
C.1 ImplementationofCausalDiscovery
Weimplementthecausaldiscoveryprimarilybasedonequation(5). However,inpractice,howto
controlthecoefficientbeforethesparsityregularizationtermsiscrucialtothefinalperformance. In
25practice,insteadofcontrollingÎ»,weusep-valueasathresholdtodeterminethefollowingconditional
independence:
Ï•(s,a)(i) âŠ¥âŠ¥Âµ(sâ€²)(j)|Ï•(s,a)âˆ’(i). (51)
HereÏ•(Â·,Â·)(i)meansthatthiselementistheithfactorintheabstractedstateactionrepresentation,
similartoÂµ(Â·,Â·)(j),Ï•(s,a)âˆ’(i) meansalltheotherfactorsintherepresentationexceptfortheith
factor.
If the p-value based on the above conditional independence test is less than a threshold, we can
remove the edge by setting M = 0. Please refer to the Appendix Table 10 for the selection of
ij
thresholdineachenvironment.
C.2 TrainingDetailsofEnergy-basedModel
WetraintheEBMaccordingtothemarginlossinequation(8). Inpractice,weattachTanh()tothe
outputlayertocliptheunnormalizedscorebetween-1and+1. Theenergynetworkstakeinboth
conditionsandsamples,thenconcatenatethemtogetherandsentitintoMLPencoders. Thedetailed
hyperparametersofEBMarelistedinTable9.
InthevanillaEBM,peoplefollowtheLangevindynamicstoeffectivelysamplethenegativesamples.
Here,aswediscoverthecausalmaskandidentifythecausalrepresentationinthemodellearning
stage,wefindthatwecanusetheserepresentationsinboththeenergynetworksandthesampling
processtogetsomeeffectivenegativesamples,whichissimilartothepracticeofaugmentationof
causality-guidedcounterfactualdata[75].
Theinterestingtrickweemployhereisthewaytogetournegativesamplesbymixingthelatent
factorsfromofflinedata. Forexample,forapositivesamplearray
ï£®
Âµ(sâ€²)(1) Âµ(sâ€²)(2) Â·Â·Â·
Âµ(sâ€²)(dâ€²)ï£¹
1 1 1
x+ =ï£¯ ï£¯Âµ(sâ€² 2)(1) Âµ(sâ€² 2)(2) Â·Â·Â· Âµ(sâ€² 2)(dâ€²)ï£º ï£º, (52)
ï£° Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· ï£»
Âµ(sâ€² )(1) Âµ(sâ€² )(2) Â·Â·Â· Âµ(sâ€² )(dâ€²)
n n n
withconditions:
ï£® ï£¹
Ï•(s ,a )(1) Ï•(s ,a )(2) Â·Â·Â· Ï•(s ,a )(d)
1 1 1 1 1 1
ï£¯Ï•(s ,a )(1) Ï•(s ,a )(2) Â·Â·Â· Ï•(s ,a )(d)ï£º
y =ï£¯ 1 1 1 1 1 1 ï£º. (53)
ï£° Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· ï£»
Ï•(s ,a )(1) Ï•(s ,a )(2) Â·Â·Â· Ï•(s ,a )(d)
1 1 1 1 1 1
Here,s ,a ,sâ€² denotesthetimestepoftheofflinesamples. Ï•(Â·,Â·)(i) meansthiselementistheith
i i i
factorintheabstractedstateactionrepresentation,similartotheÂµ(Â·,Â·)(i)
aswealreadygetthecorrespondingcausalrepresentationÂµ(sâ€²)thatissemanticallymeaningful,
wecanmixthecolumnstocreateusefulcounterfactualnegativesamples
ï£® Âµ(sâ€²)(1) Âµ(sâ€²)(2) Â·Â·Â· Âµ(sâ€² )(dâ€²)ï£¹
1 2 nâˆ’1
xâˆ’ =ï£¯ ï£¯Âµ(sâ€² 2)(1) Âµ(sâ€² 1)(2) Â·Â·Â· Âµ(sâ€² n)(dâ€²) ï£º ï£º. (54)
countefactual ï£° Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· ï£»
Âµ(sâ€² )(1) Âµ(sâ€²)(2) Â·Â·Â· Âµ(sâ€²)(dâ€²)
n 2 1
Thesecounterfactualnegativesamplesseemtoeffectivelyusethecausalrepresentationandspeedup
thetraining,asweshowinFigure7.
C.3 AdditionalMismatchAnalysis
Toevaluatethesignificanceoftheobjectivemismatcheffectatthreedifferentlevelsofofflinedatasets
inUnlock environments, wecollect5,000episodesineachoftheUnlockenvironmentsforeach
method. Thenweevaluatethemismatchviathefollowingtwometrics.
26T=0 T=100 T=200 T=300 T=400
T=500 T=600 T=700 T=800 T=900
Next State
GT
Figure7: ComparisonoftheconvergencespeedinEBMtraining. Comparedtorandomnegative
samples,ourapproachenjoysahigherrateofconvergenceempirically.
â€¢ WeconducthypothesistestingviaMann-WhitneyUTest[91],withNullhypothesis
H :L (Ï„ )<L (Ï„ ),
0 model pos model neg
â€¢ Tounderstandtheexactdifferenceinmodellossbetweentwogroupsofsamples,wecompute
theirWasserstein-1distanceintheepisodicmodellossbetweenthetrajectorieswithpositiveand
negativerewards,i.e. W (Ï„ âˆ¥Ï„ ).
1 pos neg
Wereporttheresultsofp-valueandtheW distanceoftwogroupsofmodellosssamplesinthe
1
followingtable.
Table2: Thecomparisonresultsofthep-valueandtheW distance(Ã—10âˆ’4)betweenMOPOand
1
BECAUSE.Boldmeansthebetter.
Unlock-Expert Unlock-Medium Unlock-Random
Methods
p-value(â†“) W Dist(â†‘) p-value(â†“) W Dist(â†‘) p-value(â†“) W Dist(â†‘)
1 1 1
MOPO 6.5Ã—10âˆ’5 0.7 1.0 1.1 1.0 N.A.
Ours â‰ˆ0 3.1 â‰ˆ0 3.0 0.9 <0.1
C.4 AdditionalExperimentResults
Wereporttheresultsofthetask-wiseperformanceofallbaselinesinthemainexperimentandvariants
intheablationstudiesinTable3,4,5,and 6.
27
modnaR
MBE
lasuaC
modnaR
MBE
lasuaCTable3: Successrate(%)for18tasksinthreedifferentenvironments. Weevaluatethemeanand
95%confidenceintervalgivenbythet-testofthebestperformanceamong10randomseeds,aswell
asthep-valuebetweentheoverallperformance. Boldisthebest.
Env ICIL CCIL TD3+BC MOPO GNN CDL Denoised IFactor MnM Delphic Ours
Lift-I-R 14.3Â±9.9 10.0Â±11.4 9.7Â±5.4 24.3Â±2.9 22.1Â±3.6 33.8Â±5.0 20.0Â±4.0 24.0Â±2.8 16.3Â±2.8 20.2Â±3.1 19.2Â±0.6
Lift-O-R 8.5Â±4.7 0.0Â±0.0 1.3Â±1.0 10.2Â±1.6 13.3Â±2.3 16.0Â±4.7 15.5Â±3.8 21.2Â±3.0 14.2Â±2.2 17.7Â±2.8 21.4Â±3.9
Unlock-I-R 3.4Â±1.1 2.2Â±0.8 4.39Â±0.9 21.5Â±1.9 11.7Â±2.1 6.6Â±0.5 6.9Â±0.7 8.1Â±1.1 8.6Â±1.3 14.4Â±1.1 32.7Â±2.8
Unlock-O-R 11.6Â±4.0 13.5Â±3.7 13.3Â±3.0 16.6Â±1.3 12.1Â±1.5 7.6Â±1.0 7.0Â±0.8 8.0Â±1.1 9.0Â±1.0 12.2Â±1.1 27.6Â±2.0
Crash-I-R 11.4Â±4.3 19.5Â±10.5 9.1Â±6.6 32.7Â±4.9 11.7Â±0.8 39.7Â±4.6 32.0Â±2.8 31.3Â±4.4 16.0Â±2.2 17.4Â±3.6 59.4Â±6.1
Crash-O-R 2.8Â±1.8 5.7Â±3.0 0.9Â±0.6 10.0Â±2.0 3.7Â±0.4 10.8Â±1.9 10.8Â±2.1 11.0Â±3.6 4.1Â±0.6 5.4Â±1.1 19.7Â±1.4
Lift-I-M 54.0Â±13.3 44.0Â±20.8 26.6Â±14.8 39.8Â±5.3 27.5Â±5.3 32.9Â±5.7 35.3Â±4.8 41.0Â±5.7 28.7Â±1.8 24.0Â±2.3 59.5Â±4.4
Lift-O-M 46.8Â±15.2 20.0Â±21.9 13.0Â±1.1 31.9Â±2.8 25.8Â±1.8 26.0Â±4.2 27.0Â±3.2 30.2Â±2.7 24.3Â±2.3 18.3Â±2.6 32.3Â±4.9
Unlock-I-M 4.8Â±0.9 4.7Â±1.3 5.4Â±1.0 84.8Â±5.1 17.5Â±2.6 29.7Â±4.4 12.9Â±1.5 37.7Â±5.5 37.6Â±4.9 74.1Â±1.5 98.0Â±4.9
Unlock-O-M 12.9Â±2.8 14.1Â±2.0 19.2Â±2.5 39.5Â±4.7 16.2Â±1.8 20.5Â±3.9 10.8Â±0.8 21.5Â±2.7 27.0Â±3.7 51.7Â±2.3 68.8Â±1.5
Crash-I-M 35.5Â±9.9 24.5Â±12.2 16.3Â±9.0 47.7Â±7.3 11.5Â±1.1 63.5Â±4.0 63.8Â±4.0 45.5Â±5.0 18.4Â±2.9 58.2Â±2.2 90.4Â±1.8
Crash-O-M 11.7Â±3.5 7.9Â±4.4 5.6Â±3.1 17.3Â±2.3 3.8Â±0.4 20.0Â±2.3 20.3Â±1.6 16.0Â±2.0 6.3Â±1.9 22.2Â±1.7 20.3Â±1.9
Lift-I-E 73.8Â±17.0 86.7Â±15.7 44.3Â±12.2 82.4Â±6.7 63.3Â±0.0 71.0Â±5.5 74.2Â±5.5 98.0Â±3.7 33.3Â±3.0 53.5Â±6.3 92.8Â±1.2
Lift-O-E 54.1Â±26.1 80.0Â±18.9 41.6Â±16.6 72.4Â±1.9 60.8Â±0.6 63.1Â±5.1 64.0Â±4.4 91.7Â±5.7 29.0Â±4.1 49.5Â±3.1 93.7Â±5.9
Unlock-I-E 11.6Â±3.2 13.7Â±3.1 15.6Â±2.2 88.8Â±4.6 15.3Â±1.6 73.2Â±2.8 50.3Â±3.0 59.3Â±2.6 60.7Â±2.2 83.2Â±1.2 97.4Â±1.0
Unlock-O-E 22.4Â±5.0 35.4Â±6.0 41.6Â±6.2 39.9Â±4.4 13.8Â±1.4 41.3Â±4.1 35.7Â±2.3 29.5Â±3.5 38.7Â±2.0 54.4Â±1.9 82.1Â±6.5
Crash-I-E 35.7Â±9.8 26.8Â±7.2 26.0Â±14.4 58.5Â±4.7 11.2Â±0.9 63.7Â±2.8 69.3Â±3.9 52.0Â±5.3 10.2Â±1.1 57.0Â±1.2 95.3Â±1.3
Crash-O-E 11.3Â±3.8 12.3Â±3.5 6.2Â±1.8 14.8Â±2.1 3.5Â±0.9 18.8Â±1.8 20.7Â±2.2 15.6Â±2.3 3.9Â±0.6 16.5Â±1.7 20.7Â±3.2
Overall-I 27.2 25.8 17.5 53.4 21.0 44.7 40.5 44.1 25.5 44.7 73.3
Overall-O 20.2 19.9 14.6 28.1 17.0 24.9 23.5 27.2 17.4 27.5 43.0
Table4: p-valuesofdifferentmethods(eachhas10randomtrials)againstBECAUSEinvarious
environments. Underthesignificancelevel0.05,wemarkallthebaselineresultsthataresignificantly
lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly
outperforms 10 baselines in 18 tasks in 91.1% of the experiments (164 out of total 180 pairs of
experiments).
Env ICIL TD3+BC MOPO GNN CDL Denoised IFactor CCIL MnM Delphic
Lift-I-random 0.001 0.000 0.001 0.000 0.000 0.000 0.001 0.001 0.000 0.000
Lift-O-random 0.000 0.000 0.000 0.001 0.033 0.013 0.464 0.000 0.001 0.052
Unlock-I-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-random 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Lift-I-medium 0.198 0.000 0.000 0.000 0.000 0.000 0.000 0.067 0.000 0.000
Lift-O-medium 0.966 0.000 0.438 0.009 0.022 0.032 0.209 0.125 0.003 0.000
Unlock-I-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-medium 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-medium 0.000 0.000 0.018 0.000 0.412 0.500 0.001 0.000 0.000 0.943
Lift-I-expert 0.017 0.000 0.004 0.000 0.000 0.000 0.993 0.203 0.000 0.000
Lift-O-expert 0.004 0.000 0.000 0.000 0.000 0.000 0.297 0.027 0.000 0.000
Unlock-I-expert 0.000 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Unlock-O-expert 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-I-expert 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
Crash-O-expert 0.000 0.000 0.002 0.000 0.133 0.500 0.005 0.000 0.000 0.011
28Table5: Successrate(%)for18tasksinthreedifferentenvironments. Weevaluatethemeanand
95%confidenceintervalofthetestperformanceamong10randomseeds. Boldmeansthebest.
Env BECAUSE BECAUSE-Optimism BECAUSE-Linear BECAUSE-Full
Lift-I-random 33.8Â±5.0 23.2Â±3.1 16.5Â±1.6 22.2Â±6.6
Lift-O-random 21.4Â±3.9 15.3Â±1.5 8.9Â±4.8 18.2Â±5.3
Unlock-I-random 32.7Â±2.8 31.2Â±2.4 20.7Â±1.7 10.7Â±0.8
Unlock-O-random 27.6Â±2.1 26.3Â±1.7 24.0Â±2.7 9.3Â±0.6
Crash-I-random 59.4Â±6.2 49.9Â±9.2 54.3Â±5.4 36.1Â±7.3
Crash-O-random 19.7Â±1.4 14.2Â±0.5 14.8Â±1.5 10.0Â±2.2
Lift-I-medium 59.5Â±4.5 46.8Â±2.1 24.4Â±5.3 36.4Â±6.7
Lift-O-medium 32.3Â±5.0 24.5Â±2.5 16.4Â±2.3 28.9Â±4.3
Unlock-I-medium 98.0Â±4.9 92.7Â±5.8 91.0Â±1.8 29.9Â±1.9
Unlock-O-medium 68.8Â±1.5 58.7Â±2.2 60.0Â±2.0 18.7Â±1.3
Crash-I-medium 90.4Â±1.8 82.8Â±9.9 66.7Â±7.4 60.8Â±1.8
Crash-O-medium 20.3Â±1.9 17.5Â±0.0 15.9Â±2.8 24.6Â±0.9
Lift-I-expert 92.8Â±1.2 68.6Â±4.7 75.6Â±10.7 78.1Â±6.1
Lift-O-expert 93.7Â±6.0 58.3Â±7.9 66.1Â±8.0 71.9Â±6.9
Unlock-I-expert 97.4Â±1.0 93.1Â±1.9 94.0Â±2.0 29.3Â±1.3
Unlock-O-expert 82.1Â±6.6 64.6Â±3.1 65.8Â±3.2 20.2Â±1.7
Crash-I-expert 95.3Â±1.4 91.0Â±2.2 77.9Â±2.8 50.3Â±7.6
Crash-O-expert 20.7Â±3.2 12.5Â±0.0 26.9Â±6.1 25.0Â±1.8
Overall-I 73.3 64.4 57.9 39.3
Overall-O 43.0 32.4 33.2 25.2
Table6: p-valuesofdifferentmethods(eachhas10randomtrials)againstBECAUSEinvarious
environments. Underthesignificancelevel0.05,wemarkallthebaselineresultsthataresignificantly
lower than BECAUSE as green, and the rest as red. We can see that BECAUSE significantly
outperforms3variantsin18tasks83.3%oftheexperiments(45outoftotal54pairsofexperiments).
Env BECAUSE-Optimism BECAUSE-Linear BECAUSE-Full
Lift-I-random 0.001 0.000 0.003
Lift-O-random 0.003 0.000 0.146
Unlock-I-random 0.189 0.000 0.000
Unlock-O-random 0.145 0.015 0.000
Crash-I-random 0.036 0.090 0.000
Crash-O-random 0.000 0.000 0.000
Lift-I-medium 0.000 0.000 0.000
Lift-O-medium 0.004 0.000 0.130
Unlock-I-medium 0.067 0.006 0.000
Unlock-O-medium 0.000 0.000 0.000
Crash-I-medium 0.061 0.000 0.000
Crash-O-medium 0.005 0.005 1.000
Lift-I-expert 0.000 0.003 0.000
Lift-O-expert 0.000 0.000 0.000
Unlock-I-expert 0.000 0.002 0.000
Unlock-O-expert 0.000 0.000 0.000
Crash-I-expert 0.001 0.000 0.000
Crash-O-expert 0.000 0.969 0.990
29C.5 AdditionalEnvironmentDescription
Weprovideamoredetaileddescriptionoftheenvironmentsweuseintheexperiment,asshownin
Table7.
Lift The Lift environment based on the robosuite [34] contains 33 dimensions of state space,
includingtheendeffectorpose,jointpose,jointvelocity,cubeposeaswellasitsrelativeposition,
cubecolor,andacontactflag. Itcontains4dimensionsofhybridactionspacethatusesOperation
SpaceControl(OSC)tocontrolthe3Dpositionandthe1Dgrippermovement. Thetaskiscounted
asasuccesswhentheassignedblockisliftedfromthetableover0.1m. Thegeneralizationsetting
intheLiftenvironmentistouseanunseencombinationofpositionandcolorduringonlinetesting.
Thisenvironmentcanbeabstractedinto15dimensionsoffactorizablestatespaceand4dimensions
offactorizableactionspace. ThecausalgraphofthisenvironmentisrecordedinFigure9(a).
Unlock TheUnlockenvironmentsbasedontheMiniGridworld[35]contain110dimensionsof
discretestatespace,with3of36-dimensionalvectorinputsrepresentingthecurrentpositionofthe
agent,key,anddoorina6x6gridworld. Therest2dimensionsinthestatespacememorizethestate
ofwhethertheagenthasthekeyinhand. Theactionspaceisalsodiscrete(witheightdimensions)
todeterminethemovement(up/down/left/right)andthepick-key,open-dooractions. Anepisode
willbecountedasasuccesswhentheagentholdsthekeyandusesittoopenthedoorintheright
position. ThegeneralizationsettingintheUnlockenvironmentistochangethepositionofthedoor
andincreasethenumberoftotalgoalsintheenvironment. Theagentwillonlysuccessfullyfinishone
episodebyopeningallthedoors. ThecausalgraphofthisenvironmentisrecordedinFigure9(b).
Crash TheCrashenvironmentsarebasedontheHighwayenvironment[37]whichcontains22
dimensions of continuous state space, with four vector inputs representing the current position,
velocity, and orientation of the surrounding vehicles and ego vehicles. There are two additional
dimensionsofstatememorizingthecollisiontypebetweentheegovehiclesandsurroundingvehicles
orpedestrians. The8-dimensionalactionspaceiscontinuoustodeterminetheaccelerationinthe
xâˆ’ydirectionsoftheegoandsurroundingagents. ThegeneralizationoftheCrashenvironmentis
toadddifferentnumbersofpedestriansthatmaycausethecrash. Anepisodewillonlyendwhenthe
egovehicleshaveanear-misswithbothofthepedestriansatthescene. Wevisualizethecausalgraph
ofthisenvironmentinFigure9(c).
AllthreeenvironmentsarevisualizedinFigure8. WelisttheirbasicconfigurationsinTable7.
Table7: Environmentconfigurationsusedinexperiments
Environment
Parameters
Lift Unlock Crash
Maxstepsize 30 15 30
Statedimension 33 110 22
Actiondimension 4 8 8
Actiontype Hybrid Discrete Hybrid
Intrinsicstaterank 15 4 6
Intrinsicactionrank 4 3 4
Figure8: Threeenvironmentsusedinthispaper.
30(a) Lift Environment (b) Unlock Environment (c) Crash Environment
Figure9: UnderlyingcausalgraphGinall3environmentswithexpertdemonstration.
C.6 AdditionalBaselineInformation
Wecollectdataontheabove3differentenvironments,thusforming9groupsofofflinedatasets.
Table8: Bahaviorpoliciesusedtocollectofflinedataindifferentenvironments.
Environment Behavior #Episodes SuccessRate AdditionalDescription
Random 1000 0.24 Randomactionsafterafewstepsofinitialization.
Lift Medium 1000 0.60 Randomactionsbeforethegoal-reachingexpert.
Expert 1000 1.00 Queryexpertpolicyforalltimesteps.
Random 200 0.21 Randomnavigationwithhighrandomness
Unlock Medium 200 0.46 Targetedsearchingingoaldirections
Expert 200 0.87 ShortestpathplanningviaAâˆ—
Random 1000 0.14 Fixedego,randompedestrians
Crash Medium 1000 0.35 Plannedego,randompedestrians
Expert 1000 0.66 Planninginbothegoandpedestrians
Aftercollectingthedatausingscriptedpoliciesindifferentenvironments,wetrainallagentsaswell
asBECAUSEunder10differentrandomseeds. Thenwereportthebestperformanceofeachtrial
andcomputethemeanandstandarddeviationover10seedsforeachtaskintheAppendix3.
Werefertothefollowingcodebasetoimplementallthebaselinesweuse:
â€¢ InvariantCausalImitationLearning(ICIL,[38]):https://github.com/ioanabica/Invariant-
Causal-Imitation-Learning,MITLicense.
â€¢ CausalConfusionImitationLearning(CCIL,[39]):referencelinktothepaper.
â€¢ TD3 with Behavior Cloning (TD3+BC, [41]): https://github.com/sfujim/TD3_BC, MIT
License.
â€¢ Model-based Offline Policy Otimization (MOPO, [2]): https://github.com/junming-
yang/mopo.git,MITLicense.
â€¢ RelationalGraphNeuralNetwork(GNN,[42]):https://github.com/MichSchli/RelationPrediction.git,
MITLicense.
â€¢ CausalDynamicsLearning(CDL,[24]):https://github.com/wangzizhao/robosuite/tree/cdl,
MITLicense.
â€¢ DenoisedMDP(Denoised,[12]): https://github.com/facebookresearch/denoised_mdp.git,
CCBY-NC4.0.
â€¢ MismatchNoMore(MnM,[9]):referencelinktothepaper.
â€¢ Worldmodelwithidentifiablefactorization(IFactor,[13]), referencelinktothepaper.
â€¢ DelphicOfflineRL(Delphic,[40]): referencelinktothepaper.
31ThedetailedhyperparametersweuseinBECAUSEandotherbaselinesarelistedinTable9and
Table10:
C.7 ExperimentSupport
Ourcodeisavailableattheanonymousrepo:https://anonymous.4open.science/r/BECAUSE-NeurIPS
Computingresources Theexperimentsarerunonaserverwith2Ã—AMDEPYC754232-Core
ProcessorCPU,2Ã—NVIDIARTX3090graphicsand2Ã—NVIDIARTXA6000graphics,and252GB
memory. Foronesingleexperiment,ittakesBECAUSEandotherbaselinesabout1.5hourswith
100,000iterationstotraintheworldmodeland1,000,000stepstotraintheenergy-basedmodels.
C.8 BroaderImpact
Thisworkincorporatescausalityintoreinforcementlearningmethods,whichhelpshumansunder-
standtheunderlyingmechanismofalgorithmsandcheckthesourceoffailures. However,thelearned
causalworldmodelmaycontainhuman-readableprivateinformationabouttheenvironmentandthe
dataset. Tomitigatethispotentialnegativesocietalimpact,thecausalworldmodelshouldonlybe
accessibletotrustworthyusers.
Table9: Hyper-parametersofmodelsusedinexperimentsofBECAUSEandbaselines(PartI)
Environment
Models Parameters
Lift Unlock Crash
Learningrate 0.0001 0.001 0.0001
SizeofdataD 15000 4000 15000
Epochperiteration 20 5 10
Batchsize 256 256 256
PlanninghorizonH 15 10 20
Planningpopulation 1500 100 1000
BECAUSE
RewarddiscountÎ³ 0.99 0.99 0.99
SpectralnormregularizerÎ» 10âˆ’4 10âˆ’4 10âˆ’4
Ï•
SpectralnormregularizerÎ» 10âˆ’4 10âˆ’4 10âˆ’4
Âµ
Causaldiscoveryp 10âˆ’8 10âˆ’4 10âˆ’6
thres
Encoderhiddens 256 64 128
EBMhidden 256 64 128
EBMnegativebuffer 5000 1000 5000
EBMtrainingsteps 1000 1000 1000
EBMregularizerÎ» 10âˆ’4 10âˆ’4 10âˆ’4
EBM
MLPhiddens 256 64 128
MOPO* MLPlayers 2 2 2
Ensemblenumber 5 5 5
Initializedmaskcoef. 1.0 1.0 1.0
CDL* MLPhiddens 256 64 128
Sparsityregularizer 0.001 0.001 0.001
GNNhiddens 256 64 128
GNN*
GNNlayers 3 1 3
* UsethesameplanningparametersasBECAUSE.
32Table10: Hyper-parametersofmodelsusedinexperimentsofbaselines(Continued)
Environment
Models Parameters
Lift Unlock Crash
LearningrateofMINE 0.0001 0.0001 0.0001
MINEhiddens 256 64 128
MLPhiddens 256 64 128
LearningrateofEBM 0.01 0.01 0.01
ICIL
SizeofbufferofEBM 1000 1000 1000
EBMtrainingsteps 1000 1000 1000
EBMhiddens 256 64 128
Koflangevinrollout 60 60 60
Î» oflangevinrollout 0.01 0.01 0.01
Var
LearningrateofCritic 0.0003 0.003 0.0003
Critichiddens 256 64 128
LearningrateofActor 0.0003 0.001 0.0001
Actorhiddens 256 64 128
TD3+BC
Targetupdaterate 0.005 0.001 0.0001
Policynoise 0.2 0.2 0.2
BalancecoefficientÎ± 1.0 2.5 2.5
xbeliefsize 256 64 128
ybeliefsize 256 64 128
zbeliefsize 0 0 0
xstatesize 33 110 22
DenoisedMDP
ystatesize 33 110 22
zstatesize 0 0 0
embeddingsize 256 64 128
Learningrate 0.0001 0.001 0.0001
Allhiddendim 256 64 128
IFactor Disentangledprioroutputsize 19 7 10
Learningrate 0.0001 0.001 0.0001
Allhiddendim 256 64 128
MnM Discriminatorlearningrate 0.0001 0.001 0.0001
Discriminatorclipnorm 0.25 0.25 0.25
Regweight 0.0001 0.001 0.0001
CCIL Initialmaskprobability 0.95 0.95 0.95
Learningrate 0.0001 0.001 0.0001
Ensemblemodelsize 5 5 5
Uncertaintypenaltyweight 0.0001 0.0001 0.00005
Delphic
KLweight 0.0001 0.0001 0.00005
33