LLaVolta: Efficient Multi-modal Models
via Stage-wise Visual Context Compression
JienengChen*,LuoxinYe*,JuHe,Zhao-YangWang,DanielKhashabiâ€ ,AlanYuilleâ€ 
Johns Hopkins University
Abstract
Whilesignificantadvancementshavebeenmadeincompressedrepresentationsfortextembeddingsin
largelanguagemodels(LLMs),thecompressionofvisualtokensinlargemulti-modalmodels(LMMs)
hasremainedalargelyoverlookedarea. Inthiswork,wepresentthestudyontheanalysisofredundancy
concerningvisualtokensandefficienttrainingwithinthesemodels. Ourinitialexperimentsshowthat
eliminating up to 70% of visual tokens at the testing stage by simply average pooling only leads to
a minimal 3% reduction in visual question answering accuracy on the GQA benchmark, indicating
significant redundancy in visual context. Addressing this, we introduce Visual Context Compressor,
which reduces the number of visual tokens during training to enhance training efficiency without
sacrificingperformance. Tominimizeinformationlosscausedbythecompressiononvisualtokenswhile
maintainingtrainingefficiency,wedevelopLLaVoltaasalitetrainingscheme. LLaVoltaincorporates
stage-wisevisualcontextcompressiontoprogressivelycompressthevisualtokensfromheavilytolightly,
andfinallynocompressionattheendoftraining,yieldingnolossofinformationwhentesting. Extensive
experimentsdemonstratethatourapproachenhancestheperformanceofMLLMsinbothimage-language
andvideo-languageunderstanding,whilealsosignificantlycuttingtrainingcosts.
Website https://beckschen.github.io/llavolta.html
Code https://github.com/Beckschen/LLaVolta
1. Introduction
TheadventofLLMs[31,32,40]hasmarkedanewerainthefieldofartificialintelligenceandnatural
languageprocessing. LLMscanplayaroleasauniversalinterfaceforageneral-purposeassistant,where
varioustaskinstructionscanbeexplicitlyrepresentedinlanguageandguidetheend-to-endtrainedneural
assistanttosolveataskofinterest. Forexample,therecentsuccessofChatGPT[31]andGPT-4[32]
have demonstrated the power of aligned LLMs in following human instructions, and have stimulated
tremendous interest in developing open-source LLMs [39, 41]. As the horizon of LLM applications
broadens and the availability of open-source LLMs increases, the integration of multi-modality into
thesemodelspresentsanewfrontierinexpandingtheircapabilities. Multi-modalLLMs[1,26,38,51]
(MLLMs),whichcanprocessandunderstandnotjusttextbutalsovisualinformation,standatthecutting
edgeofthisevolution.
WhileMLLMshavemadesignificantstrides,acrucialaspectthatremainsrelativelyunexploredisthe
efficientrepresentationandprocessingofvisualinformationwithinthesemodels. Substantialefforts[17,
*Equallycontributed
â€ Equallyadvised
4202
nuJ
82
]VC.sc[
1v29002.6042:viXra(a) Performance vs. Visual Token Compression Rate (b) Attention to Visual Tokens vs. System Prompt Tokens
Figure1 | VisualtokensareredundantinMLLMs. Left: TheaccuracyoftheLLaVA-1.5-7B[26]modelon
theGQA[19]benchmarksvarieswithdifferentpercentagesofretainedvisualtokens. Theğ‘¥-axisrepresentsthe
percentageoforiginalvisualtokenspreservedafterapplying1Daveragepoolingwithvaryingstridesizesğ‘†applied
inğ‘–-thTransformerlayer. Right: Visualtokensreceivelessattentionfromthe[ANS]tokenaswegodeeperinto
itslayersofLLaVA-1.5-7Bmodel. Thesefindingscollectivelysuggestasignificantredundancywithinthevisual
tokensoftheMLLMs.
33, 50] have been dedicated to optimizing the efficient representation of text tokens through various
compressiontechniques[17,33,50],aimedatenhancinginferenceefficiencybyattentivelyselecting
importanttokens. However,theefficientlearningofvisualtokensinMLLMhasnotgarneredcomparable
attention. Naturally,thisraisesquestionsaboutthepotentialredundancypresentinvisualtokensandits
implicationsfortheoverallcomputationalefficiencyofMLLMs.
Westartourworkbyaddressingthequestion: Arevisualtokensredundantinmulti-modalLLMs?
Toexplorethis,wefirstexperimentwithsimplyreducingthenumberofvisualtokensinapre-trained
LLaVA-1.5-7B[26]attheinferencestageviaaveragepooling(Â§3). AsshowninFig.1(left),ourinitial
resultsdemonstratethateliminatingupto70%ofvisualtokensbypoolingthemwithastrideof4starting
fromTransformerlayer2incursonlyaminimalperformancelossontheGQAbenchmark,specifically
a3%accuracyreduction. Additionally,wecomputeandpresenttheaverageattentionvaluesfromthe
[ANS] token to visual tokens and system prompt tokens across different Transformer layers in the
pre-trainedLLaVA-1.5-7B[26]. AsrevealedinFig.1(right;bluetrends),thevisualtokensaregenerally
lessattendedto,measuredbasedonaverageattentionfromthe[ANS]token,asthelayersgetdeeper.
Thesetwoearlyexplorationsindicatesignificantredundancyinvisualtokens.
Addressingthis,inthisworkwedevelopaneffectiveVisualContextCompressorthatcanbeintegrated
into the training of MLLMs. Surprisingly, a simple average pooler nested in LLMs stands out as the
mosteffectivecompressor,outperformingtheattention-based[17,50]andparametric[22]counterparts.
Weattributethistotworeasons: (1)Thesimplepoolingoperationmakestrainingstable,whereasprior
attention-based approaches [17, 50] are specifically designed for accelerating inference rather than
training. (2) Visual tokens in the deeper Transformer layers are less attended to (see Fig. 1 (right))
andparticularlyredundant,makingasimplecompressorplacedinadeeperTransformerlayereffective
enough. At a lower training cost, the LLaVA-1.5-7B [26] trained with the proposed Visual Context
Compressoriscompetitivewiththenon-compressedbaselineacrossvariousmulti-modalbenchmarks
(e.g., GQA [19] and MM-Vet [47]). This dual achievement highlights Visual Context Compressorâ€™s
roleasapivotaladvancementinenhancingtheefficiencyandperformanceofMLLMsacrossvarious
multi-modalquestion-answeringbenchmarks.
To further mitigate the information loss caused by compressing visual tokens, especially under
a large compression ratio (CR), we have devised a LLaVA-powered lite training scheme, dubbed
2LLaVolta, which progressively employs Visual Context Compressor at multiple training stages with
differentcompressionratios(Â§3.3). Specifically,LLaVoltaprogressesthroughseveralstages,beginning
withahighlevelofvisualtokencompressionandgraduallyreducingthecompressionratiountilthefinal
stages,wherefullvisualtokensareutilized. Thismulti-stageapproachallowsforadaptivecompression
levelsthatensuretrainingefficiencywithoutlosinginformationattesting,thusmaintainingtheoverall
effectivenessofthemodel.
Extensive experimental evaluations of LLaVolta have been conducted on thirteen widely-adopted
MLLMbenchmarksforbothimage-languageunderstandingandvideo-languageunderstanding,showing
promisingresults. WeobservethatLLaVoltanotonlyenhancestheperformanceofMLLMs,butalso
achievesasubstantialreductionintrainingcosts. Theseexperimentsvalidatetheeffectivenessofour
method,demonstratingitscapabilitytooptimizeresourceutilizationwhilemaintainingorevenimproving
modelperformance.
Insummary,ourpapermakesthefollowingcontributions:
â€¢ WepresenttwoinitialstudiestoverifytheredundancyofvisualtokensinMLLMs.
â€¢ We propose the Visual Context Compressor, a simple yet effective compression technique that
utilizesanaveragepooler,enhancingtheefficiencyofmulti-modalmodels.
â€¢ WeproposetheLLaVoltaasanefficienttrainingschemebyleveragingVisualContextCompressor
atmultipletrainingstageswithaprogressivelydecreasingcompressionratio. Tothebestofour
knowledge,weareamongthefirsttoexploreefficienttrainingofMLLMs.
â€¢ ExtensiveexperimentsshowthatourapproachnotonlyimprovestheperformanceofMLLMsin
image-languageandvideo-languageunderstandingacrossvariousbenchmarksbutalsoshowcases
efficiencygainsbyreducingtrainingcostsby16%.
2. Related Works
Multi-modalLLMs. Theevolutionoflargelanguagemodels[9,31,32]intotheirmulti-modalcounter-
parts[26,38]representsasignificantleapintheirabilitytofollowinstructionsandgeneralizeacrosstasks.
ThistransitionhasbeenmarkedbyseminalworkssuchasFlamingo[1],BLIP-2[22]andLLaVA[26],
whichhaveextendedLLMcapabilitiestoencompassvisualtasks,demonstratingimpressivezero-shot
generalization and in-context learning abilities. Progress in multi-modal LLMs has primarily been
drivenbyadvancementsinvisualinstructiontuning[26,51],leveragingvision-languagedatasetsand
refiningvisualinstruction-followingdata. Additionally,effortshavebeenmadetoenhancethegrounding
capabilitiesofmulti-modalLLMsthroughtheuseofspecializeddatasetsaimedatimprovingtask-specific
performance. Despitetheseadvancements,theexplorationofvisualcompressionwithinmulti-modal
LLMsremainsrelativelyunderdeveloped. Thedesignandoptimizationofcompressionstrategiesare
crucialformaximizingtheeffectivenessandefficiencyofmulti-modalLLMs,suggestingapotentialarea
forfutureresearchanddevelopment.
VisualRedundancy. Incomputervision,reducingredundancyiscrucialforcreatingefficientyet
effective models without losing accuracy [4]. Redundancy in images often arises from the inherent
characteristicsofnaturalscenes,includingrepetitivepatterns,textures,andareasofuniformcolor. These
features,whilecontributingtotherichnessanddetailofvisualperception,canleadtoinefficienciesin
bothstorageandprocessingwhennotadequatelyaddressed. Imagecompressionalgorithms[43]can
reducefilesizebyeliminatingorefficientlyencodingredundantdata. Thesemethodstakeadvantage
of human visual perceptionâ€™s tolerances to subtly reduce data without significantly impacting image
quality. Advancedmachinelearningmodels,particularlyCNNsandautoencoders[3],offersophisticated
approachestominimizingredundancy. Transformers[42],asafundamentalarchitectureforLLMs[9,
32],applyself-attentionmechanismstodynamicallybindthemostinformativepartsoftokents. Vision
Transformers[6,7,11,15]trainedwithCLIPobjective[7,34]encodeanimagetoasequenceofvisual
features for multi-modal LLMs [26]. Nevertheless, visual tokens receive less attention in LLMs due
3to attention shrinkage [44], resulting a waste of computation. In this work, we focus on reducing the
redundancyofvisualtokensinMLLMs.
Efficient LLMs. Efficient inference and training for LLMs are important. Compressing input
sequencesforefficiencyreasonsinTransformersisnotanewideaforNLP.Muchworkisbeingdoneto
acceleratetheinferenceofLMs. Forexample,PyramidTransformervariants[10]and [18]areproposed
inEncoder-DecoderLMsthatprogressivelycompressthesequenceasthelayersgrowdeeperviapooling
or core-set selection. Nawrot et al. [30] propose adaptively compressing the sequence based on the
predictedsemanticboundarieswithinthesequence. Raeetal.[35]proposecompressingthefine-grained
pastactivationstocoarsermemories. VCC[50]compressthesequenceintoamuchsmallerrepresentation
ateachlayerbyprioritizingimportanttokens. Besidesefficientinference,acceleratingtrainingforLLMs
attracts attention as well. A staged training setup [36] is proposed which begins with a small model
andincrementallyincreasestheamountofcomputeusedfortrainingbyapplyingagrowthoperatorto
increasethemodeldepthandwidth. However,efficienttrainingforLLMsinmulti-modalscenariosis
rarelyexplored.
3. Method
Inthissection,wefirstintroduceanoverviewofmulti-modalLLMsinÂ§3.1. Then,wedefinetheproblem
ofvisualredundancyandintroduceVisualContextCompressorinÂ§3.2. Finally,wepresentourproposed
LLaVoltainÂ§3.3.
3.1. Preliminaries: AMulti-modalLLM
WestartbyreviewingthedesignoftheLLaVAfamily[25,26]. ForprocessinganinputimageXğ‘£,we
utilizethepre-trainedCLIPvisualencoderViT-L/14, asdetailedby[34], toextractthevisualfeature
Zğ‘£ = ğ‘”(Xğ‘£), where ğ‘”(.) indicates the visual encoder. To bridge the gap between visual and linguistic
modalities, the LLaVA [25, 26] framework as an MLLM implements a straightforward linear/MLP
transformation. ThisinvolvesatrainableprojectionmatrixW,whichmapsthevisualfeaturesZğ‘£ into
thelinguisticembeddingspace, producinglanguageembeddingtokensHğ‘£ = WZğ‘£. Thesetokensare
designedtomatchthedimensionalityofthewordembeddingswithintheLLM.
ForeachimageXğ‘£,onecangeneratemulti-turnconversationdata (X1 ğ‘,X1 ğ‘,Â·Â·Â· ,Xğ‘‡ ğ‘,Xğ‘‡ ğ‘) withğ‘‡ asthe
numberofturns. Onecanorganizethemasasequence,bytreatingallanswersastheassistantâ€™sresponse
andtheinstructionXğ‘¡ attheğ‘¡-thturnas:
instruct
Xğ‘¡ =
(cid:26) RandomChoose[X1 ğ‘,Xğ‘£] or [Xğ‘£,X1 ğ‘], ğ‘¡ = 1
(1)
instruct Xğ‘¡ , ğ‘¡ > 1
ğ‘
Thisapproachestablishesastandardizedformatforthemulti-modalinstruction-followingsequence.
Itallowsfortheinstruction-basedtuningoftheLLMtobeappliedtothepredictiontokens,utilizingthe
modelâ€™snativeauto-regressivetrainingobjective. Specifically,forasequencewithlengthğ¿,thelikelihood
ofthetargetresponsesXğ‘ iscalculatedas:
ğ¿
(cid:214)
ğ‘(Xğ‘|Xğ‘£,Xinstruct) = ğ‘ ğœƒ(ğ‘¥ ğ‘–|Xğ‘£,Xinstruct,<ğ‘–,Xğ‘,<ğ‘–), (2)
ğ‘–=1
3.2. VisualContextCompressor
ProblemFormulation: Theredundancyobservedinimagesoftenarisesfrominherenttraitsofnatural
scenes,includingrepetitivepatterns,textures,andregionswithuniformcolor. Whilethesetraitsenrich
4visualperceptionbyofferingdetailanddepth,theycanalsopresentchallengesintermsofstorageand
processingefficiency. ConsideringtheinherentlimitationsofTransformersinhandlinglongsequences[2,
27,46],itiscriticaltominimizeanylengthredundanciestoobtainamoreeffectiveaccuracy/efficiency
trade-off.
TheobjectiveofthisstudyistodecreasethelengthofvisualtokensXğ‘£ (i.e.,itshiddenstatesHğ‘£ ifin-
sideLLMs),whilesimultaneouslymaximizingtheprobabilityofthetargetresponse ğ‘(Xğ‘|Xğ‘£,Xinstruct)
asdescribedinEquation(2).
Visual Context Compressor: A key design change that we introduce is a compressor layer that
compresses the dimensions of the visual inputs by reducing the effective number of visual tokens.
As depicted in Fig. 2, the compressor is simply an average pooler in our setting. It is applied to
the visual tokens in ğ‘˜-th Transformer layer of an LLM. Formally, given the hidden visual tokens at
ğ‘˜-th Transformer layer Hğ‘˜ âˆˆ RğµÃ—ğ¶Ã—ğ¿, the compressor is expected to fulfill the following projection:
ğ‘“ : RğµÃ—ğ¶Ã—ğ¿ â†¦â†’ RğµÃ—ğ¶Ã—ğ¿ out,whichresultsincompressedvisualtokens
Hğ‘˜ âˆˆ RğµÃ—ğ¶Ã—ğ¿ out, where ğ¿
out
= ğ‘†ğ¿ with ğ‘  as the compression stride. In Â§4, we explore multiple
variantsofcompressor ğ‘“ toreducethetokenlength,includingrandomtokendropping[16]withdropping
ratio 1âˆ’ ğ‘†1, K-Means [20] with number of centroids set to ğ‘ ğ¶ = ğ‘†ğ¿ , attention-based token-centric
compression [50], attention-based token dropping [8, 17], and average pooling with stride ğ‘ . To our
surprise,wefindthatthesimpleaveragepooleristhemosteffectivecompressorforvisiontokenswithin
MLLMs, due to its stability during training detailed in Â§ 4.4. Thus, we choose average pooler as the
compressor.
Note that the proposed Visual Context Compressor can be directly applied to any off-the-shelf
MLLMstoassessthevisualredundancy,asconductedinÂ§4.2. OnecanalsotrainanMLLMwithVisual
ContextCompressortoreducethenumberofvisualtokenswhilemaintainingcompetitivemulti-modal
performance.
Compression Ratio (CR)â€¡. For an LLM with ğ‘ "What is the
Transformer decoder layers, the compression ratio for dog up to?"
visualtokenscanbecalculatedas:
visual token + text token
ğ‘ Â·ğ¿
CR = (ğ‘âˆ’ğ¾)Â·ğ¿ +ğ¾Â·ğ¿ , (3) layer 1
out
where ğ¾ isthe ğ¾-thTransformerlayerofamulti-modal LLM layer 2
â€¦â€¦
LLM;ğ¿isthethelengthofvisualtokensinputintoVisual
layer K
Context Compressor; ğ¿ is the compressed length of ğ¿
out
Compression Ratio
visual tokens generated by Visual Context Compressor,
ğ‘"ğ¿ Visual Compressor
asillustratedinFig.2. ğ‘âˆ’ğ¾ "ğ¿!"#+ğ¾"ğ¿ ğ¿
!"#
Ourarchitecturemodificationsthusfarmostlyimpacts layer K+1
theinferenceefficiencyofMLLM,however,itsimpacton
â€¦â€¦
performance-compressiontrade-offremainsunclear. We
willstudythisquestioninthecontextoftrainingMLLMs layer N
withagoalofenhancingefficiencywithoutcompromising
ANS
performance. WethenmoveontofurtherutilizeVisual
ContextCompressortodesignanefficienttrainingscheme
Figure2 | ExampleofVisualContextCompres-
to incorporates Visual Context Compressor at various
sorinamulti-modalLLM.
stagesofthetrainingprocess.
â€¡DefinitionofcompressionratiofromWikipedia
53.3. LLaVoltaasaLiteTrainingScheme
Training with Visual Context Compressor not only facilitates efficient inference but also enhances
trainingefficiency. However,devisinganeffectivetrainingschemeposeschallengeswhenensuringfair
comparisonswiththeoriginalLLaVA[25],primarilyduetodifferencesinthenumberoftokensinvolved
ininference. Thisdiscrepancymayleadtoinformationloss,particularlywhenoperatingunderascenario
withahighcompressionratio. Totacklethisissue,wehavedevelopedalitetrainingschemeforLLaVA,
dubbedasLLaVolta,whichemploysstage-wisevisualcontextcompression. Generally,assumingthere
are ğ‘ ğ‘  totalstages,stageğ‘–involves ğ‘1
ğ‘ 
ofthetotaltrainingepochswithacompressionratioofğ‘Ÿ ğ‘–,andthe
finalstageproceedswithoutanycompression. Essentially, astrainingprogresses, ğ‘– increaseswhileğ‘Ÿ ğ‘–
decreases.
Inthiswork,asdepictedinFig.3,weprimarilyexploreathree-stagetrainingpipelinethatprogres-
sivelyreducesthecompressionratio,asdetailedbelow:
TrainingStageI:HeavyCompression. TheMLLMtrainingatthefirstone-thirdofthetotaltraining
iterationscommenceswithaheavycompressionratio(>500%),whereVisualContextCompressor is
appliedinanearlylayeroftheLLMwithalargepoolingstride. Thissetupenablesaveryfasttraining
speed.
TrainingStageII:LightCompression. TheMLLMcontinuestrainingwithanotherone-thirdofthe
totaltrainingepochs. Atthisstage,VisualContextCompressorisappliedatonlythedeeperlayersofthe
LLMwithasmallerpoolingstridecomparedtoTrainingStageI.
TrainingStageIII:NoCompression. TheMLLMcontinuestrainingwiththefinalone-thirdofthe
totaltrainingepochs,followingthestandardMLLMtrainingprotocolwithoutcompression. Disabling
compression in the final stage ensures that the number of tokens remains consistent with the original
MLLMduringinference,avoidingthelossofinformationcausedbythereductionofvisualtokens.
Giventheabovemetaframework,wecaninstantiateafamilyoftrainingschemes,asdemonstrated
inTab.1. Thesingle-stage(non-compression)schemeisequivalenttotheMLLMbaseline. Formulti-
stagetraining,thecompressionstagecaneithergodeeperorwider. â€œdeeperâ€impliesanincreasein ğ¾
(Transformerlayer),whileâ€œwiderâ€meansadecreaseinthestrideofthepooler.
#Stages Scheme Stage Layer Stride CR #Epoch #Stages Scheme Stage Layer Stride CR #Epoch
Single nocompression ğ‘†1 / / 100% 1 ğ‘†1 2 8 557% 0.25
Two compression ğ‘†1 2 8 557% 0.5 ğ‘†2 2 2 188% 0.25
ğ‘†2 / / 100% 0.5 Four widerthendeeper
ğ‘†3 16 2 133% 0.25
ğ‘†1 2 8 557% 0.33
ğ‘†4 / / 100% 0.25
Three compr.deeper ğ‘†2 16 8 178% 0.33
ğ‘†1 2 8 557% 0.25
ğ‘†3 / / 100% 0.33
ğ‘†1 2 8 557% 0.33 Four deeperthenwider ğ‘†2 16 8 178% 0.25
Three compr.wider ğ‘†2 2 2 188% 0.33 ğ‘†3 16 2 133% 0.25
ğ‘†3 / / 100% 0.33 ğ‘†4 / / 100% 0.25
Table1 | InstantiationsofLLaVoltaschemes. deeperindicatesthatthecompressorâ€™spositionintheLLMshifts
fromtheshallowlayer(e.g.,2)toadeeperlayer(e.g.,16). widerindicatesthatthecompressorâ€™sstridedecreases
whilethenumberofvisualtokensincreases.
Notethatalltrainingschemeswillbestandardizedtocompletejustoneepoch. Thus,inthethree-stage
training,eachstagewillreceiveonethirdofanepoch,whileinthefour-stagetraining,eachstagewill
receiveonefourthofanepoch. Effectsofnon-uniformstagesplittingarepresentedintheAppendix.
6Stage I Stage II Stage III
first 1/3 of iterations second 1/3 of iterations last 1/3 of iterations
heavy compression light compression no compression
â€How many "Are the napkin â€How many
cats are in and the cup the yellow taxis are
this picture?" same color?" on the street?"
visual token + text token visual token + text token visual token + text token
layer 1 layer 1 layer 1
LLM Visual Compressor LLM layer 2 LLM layer 2
CR=800% layer 3 layer 3
layer 2
Visual Compressor layer 4
layer 3
CR=200% layer 5
layer 4 layer 4 â€¦â€¦
â€¦â€¦ â€¦â€¦ layer N-1
layer N layer N layer N
ANS wider ANS ANS
Figure 3 | Meta framework of LLaVolta, consisting with multiple training stages: Stage I with heavy visual
compression; Stage II with light visual compression in deeper layer with wider token window; Stage III with
standardMLLMtraining(asthereisalsonocompressioninstandardinference). Thiscanacceleratethetrainingby
16+%whilemaintainingperformance.
4. Experiments
Inthissection,webeginbydetailingtheexperimentalsetupinÂ§4.1. Next,weelaborateontheproof-of-
conceptinSectionÂ§4.2. Followingthis,wevalidatetheproposedLLaVoltainÂ§4.3withanablation
studyinÂ§4.4. Finally,weassesstheextensibilitytovideo-languageinÂ§4.5.
4.1. ExperimentalSetup
WeadopttheVicuna-v1.5-7B[9]asthelanguagemodel,leveragingtheLLaMA2codebase[41]. We
leveragethepre-trainedCLIPViT-L/14[11,34]withaninputresolutionof336Ã—336,resultingin576
visualtokens. WeemploytheLLaVAframework [25]toconnectthefrozenCLIPvisionencoderand
the Vicuna LLMs. Along with the projector, we train the entire LLM instead of parameter-efficient
finetuning. WefollowLLaVA-1.5 [25]toperformdatapreparationandtrainingscheduleforpretraining
andinstructiontuning. Weconductalltheexperimentswiththemachineof8Ã—NvidiaRTX6000Ada.
Due to multiple invalid image links in the dataset of instruction tuning stage, the scores of LLaVA-
1.5 reported in our analysis are reproduced by ourselves to ensure a fair comparison under the same
experimentalenvironment.
It is worth mentioning that assessing visual token redundancy only necessitates the inference of
existingoff-the-shelfmodels,whereastheotherexperimentsinvolvethetrainingofmulti-modalLLMs,
specificallyprojectorsandLLMs.
BenchmarksandMetrics: WeadoptthirteenbenchmarksspecificallydesignedforMLLMevalua-
tion,includingGQA[19],MM-Vet[47],ScienceQA(SQA)[29],MME[12],TextVQA[37],POPE[23],
MMBench[28],MMBench-CN[28],VQA-v2[13],LLaVA-Bench-in-the-Wild(LLaVAğ‘Š)[26],VisWiz[14],
SEED-Image[21]andMMMU[49]. GQAandVQA-v2evaluatethemodelâ€™svisualperceptioncapabil-
itiesonopen-endedshortanswers. MME-Perceptionevaluatesmodelâ€™svisualperceptionwithyes/no
questions. ScienceQAwithmultiplechoiceareusedtoevaluatethezero-shotgeneralizationonscien-
tific question answering. TextVQA contains text-rich visual question answering. MMBench and the
7
repeedCNversionevaluateamodelâ€™sanswerrobustnesswithall-roundshufflingonmultiplechoiceanswers.
MM-Vetevaluatesamodelâ€™scapabilitiesinengaginginvisualconversations. Additionally,weextend
LLaVolta to video-language understanding, and follow Video-LLaVA [24] to evaluate the models on
MSVD-QA[5],MSRVTT-QA[45]andActivityNet-QA[48],wheretheaccuracyandscoreareassessed
usingGPT-Assistant.
Wereporttheofficialmetricscalculatedusingthestandardimplementationsprovidedforeachbenchmark
forafaircomparison. Latencyisreportedasthetimetakenduringinferenceuntilthefirstanswertoken
is produced. When reporting average performance in Table 2, the score of MME is divided by 2000,
as its range is from 800 to 2000. TFLOPs are profiled via DeepSpeed. For total number of tokens,
#Tokens = (cid:205)ğ‘ #Tokenğ‘– . The training time is reported for one epoch of training during the LLaVA
ğ‘–
instruction-tuningstage. TheCompressionRatio(CR)isdefinedasinEquation3.
4.2. ProofofConcept: VisualContextRedundancy
Toassesstheredundancyofvisualtokens,weperformaveragepoolingwithinanoff-the-shelfLLaVA-
1.5-7Bcheckpointatthetestingstage,usingdifferentpoolingstridesizesğ‘†acrossvariousTransformer
layers ğ¾. AsshowninFig.1,themodelstillexhibitsstrongperformanceevenwhenretainingonly62.5%
ofthevisualtokens(ğ‘† = 4,ğ¾ = 16)intheMM-Vetbenchmark,withouttheneedforadditionaltraining.
Whenadoptingthesamesetting(ğ‘† = 4,ğ¾ = 16),asimilartrendcanbeobservedintheGQAbenchmark
aswell,wherethecompressedmodelonlyhas1%performancedropthantheuncompressedcounterpart.
Surprisingly,intheGQAbenchmark,eliminatingupto70%ofvisualtokens(ğ‘† = 4,ğ¾ = 16)resultsin
amere3%decreaseinperformance. Thisproof-of-conceptshowsacertainlevelofredundancyinthe
visualtokenswithinMLLMs.
4.3. MainResults: LLaVolta
In this section, we present the main results of LLaVolta schemes instantiated in Â§ 3.3. We conduct
a thorough evaluation of the multi-modal capability across 13 benchmarks. Tab. 2 demonstrates that
ourproposedLLaVoltanotonlyconsistentlylowerstrainingcostsby16%(15.3hoursvs.12.8hours)
but also surpasses the non-compression baseline. The four-stage training schemes achieves the best
performanceinnineoutofthethirteenbenchmarksandobtains61.9%averageperformance,improving
LLaVA-v1.5-7B[25]withmuchlessoverallTFLOPsandtrainingtime. Thisindicatesthenecessityof
designinganoptimallylitetrainingscheme.
Test Train
#Stages Scheme #Tokensâ€  CRâ€  TFLOPsâ€  Time GQA MMVet SQA MME VQAğ‘‡ POPE MMB MMBğ¶ğ‘ VQAğ‘£2 LLaVAğ‘¤ VisWiz SEEDğ¼ MMMU Avg.
Single nocompression 18432 - 8.26 15.3h 62.6.49 31.91 70.8.59 146713 58.3.15 86.1.24 65.3.93 59.4.92 78.9.37 65.5.56 49.8.6 66.7.25 35.1.86 61.8.32
Two compression 10062 183% 5.20 12.8h 61.9.23 31.71.5 70.9.34 148023 58.3.46 86.5.33 64.8.23 59.01.1 78.5.20 67.3.91 47.21.8 64.9.17 34.9.11 61.5.40
Three compr.deeper 10597 174% 5.13 12.8h 62.1.01 30.5.40 70.5.23 147713 58.4.07 86.6.14 65.6.26 59.9.27 78.5.22 67.51.4 49.2.56 65.9.17 35.0.19 61.8.10
Three compr.wider 10407 177% 3.93 12.8h 61.11.6 31.8.61 71.0.28 143412 58.5.04 86.6.06 64.8.23 59.1.83 78.7.02 64.34.8 49.81.1 65.3.04 34.3.75 61.3.28
Four widerthendeeper 11088 166% 5.39 12.9h 62.1.09 31.6.58 71.4.36 144415 58.7.24 86.8.21 65.3.30 59.3.26 78.8.05 67.73.1 50.1.21 65.6.15 33.8.78 61.8.35
Four deeperthenwider 10863 170% 5.45 12.8h 62.1.07 31.5.20 70.5.16 147216 58.7.08 86.3.33 65.6.52 59.9.61 78.8.03 68.22.1 48.31.3 66.1.20 35.1.02 61.9.47
Table2 | PerformanceofLLaVolta. SeethedefinitionofeachtrainingschemeinTab.1. â€ : averageacrossstages.
Thederivedfivetrainingschemesachievecompetitiveresultswhilereducing16%trainingtime. Wereportthe
averageresultsacrossthreeruns,withthestandarddeviationwrittenatthebottomrightoftheaverageresult. The
four-stagetrainingachievesthehighestperformanceinnineofthirteenbenchmarks,outperformingthebaseline
(LLaVA-v1.5-7B)whilerequiringsignificantlyfewerTFLOPsandlesstrainingtime.
4.4. AblationStudy
Inthissection,weperformanablationstudyonthechoiceofvisualcompressorsbycomparingdifferent
compression methods. Additionally, we examine the effects of varying the stride and LLM layer in
trainingVisualContextCompressor.
8Compressor #Tokens CR GQA MM-Vet SQA MME VQAğ‘‡ POPE MMB MMBğ¶ğ‘ VQAğ‘£2 LLaVAğ‘Š VisWiz SEEDğ¼ MMMU Avg.
Trainwithoutcompression;Testingwithcompression
RandomDropping 3312 556% 50.6 21.4 69.3 1142 46.5 55.8 39.7 33.3 59.3 47.6 47.2 52.2 34.3 47.3
K-Means 3312 556% 54.4 25.9 69.7 1155 49.0 78.6 55.3 46.1 69.3 57.6 48.9 56.1 32.9 54.0
FastV[8] 3312 556% 52.1 30.6 69.4 1298 53.4 65.6 60.1 53.0 68.6 54.8 50.0 56.3 34.9 54.9
VCC[50] 3582 514% 54.7 26.9 69.2 1246 49.2 72.3 60.8 52.0 68.1 55.6 47.8 57.0 34.8 54.7
AveragePooling 3312 556% 53.7 25.6 69.4 1150 47.7 70.1 56.4 46.5 67.0 55.6 50.0 55.7 34.3 53.0
Trainwithcompression;Testingwithcompression
RandomDropping 3312 556% 53.4 25.0 69.4 1186 49.4 64.9 52.0 41.1 59.7 51.5 47.9 52.6 34.6 50.8
K-Means 3312 556% 57.5 25.9 55.6 1279 51.4 79.4 62.6 54.6 75.7 59 46.1 59.2 34.1 57.9
FastV[8] 3312 556% 55.9 27.9 70.4 1327 49.7 79.8 62.9 55.9 69.5 61.7 49.6 56.8 35.1 57.0
VCC[50] 3582 514% 57.7 29.3 70.7 1398 53.0 83.6 65.0 55.8 74.1 58.0 48.2 60.1 35.0 58.5
AveragePooling 3312 556% 60.0 30.7 70.8 1450 55.1 85.5 65.0 59.5 75.9 66.9 46.4 62.6 33.8 60.4
Table3 | Comparisonamongdifferentvisualcompressors. Highervaluesarepreferred. Allmethodsexcept
VCCaresettothecompressionratioof556%toapproximateVCCâ€™s514%[50]forafaircomparison. Thebest
scoresaremarkedasgrayandthesecondbestareunderlined. Attention-basedcompressors(i.e.,FastVandVCC)
excelduringtheinferencephase,yettheirapplicationtothetrainingphaseproveschallenging. Averagepooling
showsamorestableperformanceduringthetrainingphase.
ChoiceofVisualCompressors. Thedesignchoicesinclude(1)randomtokendropping,(2)K-Means
clustering,(3)averagepooling,(4)FastV[8],(5)VCC[17],(6)parametricpre-trainedQ-Former[22].
We have the following three observations. Firstly, Tab. 3 shows that the attention-based methods,
includingFastVandVCCwin9/13bestandsecondbestscores,showcasingthehighperformancewhen
compressingvisualtokensininference. However,theyareineffectivewhenappliedtotrainingbecause
thein-trainingattentionscoresareunstable. Secondly,andsurprisingly,theaveragepoolingobtainsthe
highest scores on eleven out of thirteen benchmarks when it is used to train MLLMs with a high CR.
Thirdly,Tab.4showsthatbothQ-Formerandaveragepoolingcanobtainreasonablygoodperformance
whentrainedwithextremelyhighCRs,andtheaveragepoolingperformsbetterwithlesstrainingcost.
ThereasoncouldbethattheQ-FormerresamplestokensoutsidetheLLM,potentiallycausingtheLLM
to overlook crucial information relevant to the response. In contrast, our approach employs average
poolingsubsequenttoTransformerlayer ğ¾,allowingtheinitial ğ¾ layersoftheLLMtoeffectivelyretain
importantinformationfromuncompressedtokens. Giventhesethreeinsights,weselectaveragepooling
asourfavoredapproachforvisualcompression.
Train
Method #Param#Tokens CR TimeGQAMMVetSQAMMEVQAğ‘‡POPEMMBMMBğ¶ğ‘VQAğ‘£2LLaVAğ‘¤VisWizSEEDğ¼MMMUAvg.
Q-Former[22] 105M 1024 1800%10.4h 55.7 26.4 69.3 1217 49.2 83.0 57.7 50.7 71.4 64.6 52.6 55.1 34.0 56.2
Ours 0 855 2156% 9.2h 55.9 26.3 71.0 1321 51.6 82.5 63.3 55.9 74.5 63.1 47.8 57.3 35.7 57.8
Table 4 | Parametric vs. nonparametric visual compressor. We follow miniGPT-4 [51] that uses
Q-Formerpre-trainedfromBLIP-2[22]astheparametriccompressor(Allotheraspectsaremaintained
as in LLaVA to ensure a fair comparison). Ours: pooling with stride 64 on LLM layer 1 to ensure
comparable CRs. Our nonparametric compressor outshines the parametric Q-Former counterpart in
termsofbothperformanceandtrainingefficiency.
PerformanceAcrossCompressionRatios. Herein,wetrainthemulti-modalLLMwithourVisual
ContextCompressorinvarioussettings. AsdemonstratedinTab.5,theproposedmethodofferscertain
improvements and trade-offs compared to the state-of-the-art method, LLaVA-1.5-7B. We have the
following two observations. Firstly, in the heavy compression level, the performance of MLLM is
inverselyproportionaltothecompressionratio(linearlyscalingtothenumberofvisualtokens). Secondly,
theperformanceofMLLMsatthelightcompressionleveldoesnotcorrelatedirectlywiththenumberof
visualtokens, makingthisobservationsomewhatunexpected. WeattributethistotheMLLMsatthis
9levelofcompressionbeingrelativelyinsensitivetochangesinthecompressionratio. Thisindicatesthat
MLLMs trained at a light compression level will not hurt the model performance at all. For instance,
thesettingofstride16inlightcompressionlevelattainsa188%CRandalsooutperformsthebaseline
LLaVA-v1.5-7B across all four metrics. The above observations pave the way for developing a more
systematictrainingscheme.
Train
Stride #Tokens CR Latency TFLOPs time GQA MMVet SQA MME VQAğ‘‡ POPE MMB MMBğ¶ğ‘ VQAğ‘£2 LLaVAğ‘¤ VisWiz SEEDğ¼ MMMU Avg.
HeavycompressioninLLMlayer2
8 3312 557% 37.9ms 2.14 12.0 59.9.13 30.1.92 70.9.17 144311 55.3.3 85.3.21 65.2.25 59.5.06 76.0.09 65.92.0 46.6.2 62.6.0 34.2.54 60.3.2
2 9792 188% 48.6ms 4.77 12.6 61.9.43 30.91.1 71.6.69 145018 57.6.08 86.3.22 67.2.05 59.9.4 78.0.17 66.4.85 48.7.25 65.9.49 34.1.34 61.6.08
LightcompressioninLLMlayer16
8 10368 178% 51.3ms 5.00 12.8 62.6.03 30.4.54 71.1.27 14629 58.2.01 86.0.09 65.3.52 58.9.57 78.8.12 63.91.1 51.4.15 66.8.23 35.81.4 61.8.04
2 13824 133% 58.8ms 6.40 14.2 61.9.45 31.51.0 70.8.49 146224 58.5.02 86.4.12 66.4.33 59.6.47 78.9.02 65.3.46 49.5.97 66.7.23 35.1.87 61.8.01
Base[25] 18432 100% 68.5ms 8.26 15.3h 62.6.49 31.91.0 70.8.59 146713 58.3.15 86.1.24 65.3.93 59.4.92 78.9.37 65.5.56 49.8.6 66.7.25 35.1.86 61.8.32
Table 5 | TrainingMLLMswithVisualContextCompressorinvariouscompressionlevels. Wereportthe
averageresultsacrossthreeruns,withthestandarddeviationwrittenatthebottomrightoftheaverageresult. In
the heavy compression range, the performance is inversely proportional to the compression ratio. In the light
compressionrange,theperformanceisnotsensitivetocompression. Performanceremainshighformodelsatthe
lightcompressionlevel.
Furthermore,weconductanablationstudyonthenumberofiterationsindifferentstages(uniformvs.
non-uniformstagesplitting),whichisdetailedintheAppendix.
4.5. ExtensibilitytoVideoMLLMs
WeextendourtrainingschemetoVideoLLaVA[24]andtheresultsinTab.6revealsimilarfindingsas
before: theproposedtrainingschemeachievecompetitiveresultswhilereducing9%trainingtime. Itis
worthmentioningVideoLLaVAdoesnotsupportDeepSpeedZeRO-3,unlikeLLaVA,whichresultsin
differentrelativeefficiencygains.
MSVD-QA MSRVTT-QA ActivityNet-QA Average
#Stages Scheme #Tokensâ€  CRâ€  TFLOPsâ€  Train-time
Score Acc Score Acc Score Acc Score Acc
Single nocompression 147456 - 29.68 40.7h 3.69 69.1 3.48 56.8 3.28 47.5 3.48 57.8
Two compression 80496 183% 17.73 37.1h 3.71 69.0 3.50 56.9 3.29 47.9 3.50 57.9
Three compr.deeper 84776 174% 17.29 37.1h 3.73 69.3 3.51 57.2 3.28 47.4 3.51 58.0
Three compr.wider 83256 177% 16.86 37.0h 3.72 69.0 3.51 57.2 3.29 47.7 3.51 58.0
Four widerthendeeper 88704 166% 18.32 37.2h 3.72 69.1 3.51 57.2 3.27 48.0 3.50 58.1
Four deeperthenwider 86904 170% 18.64 37.1h 3.74 69.8 3.49 56.9 3.27 47.8 3.50 58.2
Table6 | PerformanceofLLaVoltaonVideoLLaVA[24]. SeethedefinitionofeachtrainingschemeinTab.1. â€ :
averageacrossstages. Toimplementourmulti-stagetraining,weapplythesamecompressionprocessingtothe8
framesrepresentingthevideorespectively. Thederivedfivetrainingschemesachievecompetitiveresultswhile
reducing9%trainingtime.
5. Conclusion
In this work, we conduct two initial studies to investigate and verify the redundancy of visual tokens
inmulti-modalLLMs. Toaddressthis,weproposeVisualContextCompressor,astraightforwardyet
effectivecompressiontechniquethatemploysasimpleaveragepooler,seamlesslyintegratingintothe
training of MLLMs. This approach enhances training efficiency without compromising performance.
10To further mitigate the information loss brought by the token compression, we introduce LLaVolta, a
multi-stage training scheme that utilizes Visual Context Compressor with a progressively decreasing
compression rate. Experimental results on various visual question answering benchmarks verify the
effectivenessofLLaVoltainboostingperformancewhilealsodemonstratingefficiencygainsbyreducing
trainingcostsby16%. Tothebestofourknowledge,wearethefirsttoacceleratethetrainingofmulti-
modalLLMfromthecompressionperspective. WehopethattheproposedVisualContextCompressor
andLLaVoltawillinspiremorein-depthanalysisofvisualredundancyexistingincurrentMLLMsand
callforfuturedesignsofefficienttrainingforMLLMs.
References
[1] J.-B.Alayracetal.â€œFlamingo:avisuallanguagemodelforfew-shotlearningâ€.In:Advancesin
neuralinformationprocessingsystems35(2022),pp.23716â€“23736.
[2] C. Anil et al. â€œExploring length generalization in large language modelsâ€. In: arXiv preprint
arXiv:2207.04901(2022).
[3] P.Baldi.â€œAutoencoders,unsupervisedlearning,anddeeparchitecturesâ€.In:ProceedingsofICML
workshoponunsupervisedandtransferlearning.JMLRWorkshopandConferenceProceedings.
2012,pp.37â€“49.
[4] H.Barlow.â€œRedundancyreductionrevisitedâ€.In:Network:computationinneuralsystems12.3
(2001),p.241.
[5] D.ChenandW.B.Dolan.â€œCollectinghighlyparalleldataforparaphraseevaluationâ€.In:Proceed-
ingsofthe49thannualmeetingoftheassociationforcomputationallinguistics:humanlanguage
technologies.2011,pp.190â€“200.
[6] J.-N. Chen et al. â€œTransmix: Attend to mix for vision transformersâ€. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.2022,pp.12135â€“12144.
[7] J. Chen et al. â€œViTamin: Designing Scalable Vision Models in the Vision-language Eraâ€. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.2024.
[8] L.Chenetal.â€œAnimageisworth1/2tokensafterlayer2:Plug-and-playinferenceacceleration
forlargevision-languagemodelsâ€.In:arXivpreprintarXiv:2403.06764(2024).
[9] W.-L.Chiangetal.â€œVicuna:Anopen-sourcechatbotimpressinggpt-4with90%*chatgptqualityâ€.
In:Seehttps://vicuna.lmsys.org(accessed14April2023)2.3(2023),p.6.
[10] Z. Dai et al. â€œFunnel-transformer: Filtering out sequential redundancy for efficient language
processingâ€.In:Advancesinneuralinformationprocessingsystems33(2020),pp.4271â€“4282.
[11] A. Dosovitskiy et al. â€œAn image is worth 16x16 words: Transformers for image recognition at
scaleâ€.In:arXivpreprintarXiv:2010.11929(2020).
[12] C.Fuetal.â€œMME:AComprehensiveEvaluationBenchmarkforMultimodalLargeLanguage
Modelsâ€.In:arXivpreprintarXiv:2306.13394(2023).
[13] Y.Goyaletal.â€œMakingthevinvqamatter:Elevatingtheroleofimageunderstandinginvisual
questionansweringâ€.In:CVPR.2017.
[14] D. Gurari et al. â€œVizwiz grand challenge: Answering visual questions from blind peopleâ€. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2018,pp.3608â€“
3617.
[15] J.Heetal.â€œTransfg:Atransformerarchitectureforfine-grainedrecognitionâ€.In:Proceedingsof
theAAAIconferenceonartificialintelligence.Vol.36.1.2022,pp.852â€“860.
[16] K.Heetal.â€œMaskedautoencodersarescalablevisionlearnersâ€.In:ProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition.2022,pp.16000â€“16009.
11[17] L.Houetal.â€œTokendroppingforefficientbertpretrainingâ€.In:Proceedingsofthe60thAnnual
MeetingoftheAssociationforComputationalLinguistics.2022.
[18] X.Huangetal.â€œPyramid-BERT:Reducingcomplexityviasuccessivecore-setbasedtokenselec-
tionâ€.In:arXivpreprintarXiv:2203.14380(2022).
[19] D. A. Hudson and C. D. Manning. â€œGqa: A new dataset for real-world visual reasoning and
compositionalquestionansweringâ€.In:ProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition.2019,pp.6700â€“6709.
[20] T.Kanungoetal.â€œAnefficientk-meansclusteringalgorithm:Analysisandimplementationâ€.In:
IEEEtransactionsonpatternanalysisandmachineintelligence24.7(2002),pp.881â€“892.
[21] B. Li et al. â€œSeed-bench: Benchmarking multimodal llms with generative comprehensionâ€. In:
arXivpreprintarXiv:2307.16125(2023).
[22] J. Li et al. â€œBlip-2: Bootstrapping language-image pre-training with frozen image encoders
and large language modelsâ€. In: International conference on machine learning. PMLR. 2023,
pp.19730â€“19742.
[23] Y.Lietal.â€œEvaluatingobjecthallucinationinlargevision-languagemodelsâ€.In:arXivpreprint
arXiv:2305.10355(2023).
[24] B.Linetal.â€œVideo-llava:Learningunitedvisualrepresentationbyalignmentbeforeprojectionâ€.
In:arXivpreprintarXiv:2311.10122(2023).
[25] H.Liuetal.ImprovedBaselineswithVisualInstructionTuning.2023.
[26] H.Liuetal.â€œVisualinstructiontuningâ€.In:Advancesinneuralinformationprocessingsystems36
(2024).
[27] N.F.Liuetal.â€œLostinthemiddle:Howlanguagemodelsuselongcontextsâ€.In:arXivpreprint
arXiv:2307.03172(2023).
[28] Y.Liuetal.â€œMMBench:IsYourMulti-modalModelanAll-aroundPlayer?â€In:arXivpreprint
arXiv:2307.06281(2023).
[29] P. Lu et al. â€œLearn to explain: Multimodal reasoning via thought chains for science question
answeringâ€.In:AdvancesinNeuralInformationProcessingSystems(2022).
[30] P.Nawrotetal.â€œEfficienttransformerswithdynamictokenpoolingâ€.In:arXivpreprintarXiv:2211.09761
(2022).
[31] OpenAI.ChatGPT.https://openai.com/blog/chatgpt/.2022.
[32] OpenAI.GPT-4TechnicalReport.2023.arXiv:2303.08774[cs.CL].
[33] G.QinandB.VanDurme.â€œNugget:Neuralagglomerativeembeddingsoftextâ€.In:International
ConferenceonMachineLearning.PMLR.2023,pp.28337â€“28350.
[34] A. Radford et al. â€œLearning transferable visual models from natural language supervisionâ€. In:
Internationalconferenceonmachinelearning.PMLR.2021,pp.8748â€“8763.
[35] J.W.Raeetal.â€œCompressivetransformersforlong-rangesequencemodellingâ€.In:arXivpreprint
arXiv:1911.05507 (2019).
[36] S.Shenetal.â€œStagedtrainingfortransformerlanguagemodelsâ€.In:InternationalConferenceon
MachineLearning.PMLR.2022,pp.19893â€“19908.
[37] A.Singhetal.â€œTowardsvqamodelsthatcanreadâ€.In:CVPR.2019.
[38] G. Team et al. â€œGemini: a family of highly capable multimodal modelsâ€. In: arXiv preprint
arXiv:2312.11805(2023).
[39] G. Team et al. â€œGemma: Open models based on gemini research and technologyâ€. In: arXiv
preprintarXiv:2403.08295(2024).
12[40] H.Touvronetal.â€œLlama2:OpenFoundationandFine-TunedChatModelsâ€.In:arXivpreprint
arXiv:2307.09288(2023).
[41] H. Touvron et al. â€œLlama: Open and efficient foundation language modelsâ€. In: arXiv preprint
arXiv:2302.13971(2023).
[42] A. Vaswani et al. â€œAttention is all you needâ€. In: Advances in neural information processing
systems30(2017).
[43] G.K.Wallace.â€œTheJPEGstillpicturecompressionstandardâ€.In:IEEEtransactionsonconsumer
electronics38.1(1992),pp.xviiiâ€“xxxiv.
[44] G. Xiao et al. â€œEfficient streaming language models with attention sinksâ€. In: arXiv preprint
arXiv:2309.17453(2023).
[45] J. Xu et al. â€œMsr-vtt: A large video description dataset for bridging video and languageâ€. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.2016,pp.5288â€“
5296.
[46] X.Yeetal.â€œAnaloBench:BenchmarkingtheIdentificationofAbstractandLong-contextAnalo-
giesâ€.In:arXivpreprintarXiv:2402.12370(2024).
[47] W.Yuetal.â€œMm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilitiesâ€.In:arXiv
preprintarXiv:2308.02490(2023).
[48] Z. Yu et al. â€œActivitynet-qa: A dataset for understanding complex web videos via question an-
sweringâ€. In: Proceedings ofthe AAAI Conference on Artificial Intelligence. Vol.33. 01. 2019,
pp.9127â€“9134.
[49] X. Yue et al. â€œMmmu: A massive multi-discipline multimodal understanding and reasoning
benchmarkforexpertagiâ€.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition.2024,pp.9556â€“9567.
[50] Z. Zeng et al. â€œVcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important
Tokensâ€.In:AdvancesinNeuralInformationProcessingSystems36(2024).
[51] D.Zhuetal.â€œMinigpt-4:Enhancingvision-languageunderstandingwithadvancedlargelanguage
modelsâ€.In:arXivpreprintarXiv:2304.10592(2023).
13Appendix
Intheappendix,weprovideadditionalinformationaslistedbelow:
â€¢ Â§Aprovidestheadditionalexperimentalresults.
A. Additional Experimental Results
A.1. Non-uniformStageSplitting
Bydefault,thetrainingtimeisevenlydividedacrosseachstage. Toexplorehowthecompressionstage
affectstotaltrainingtime,wemodifytherelativeproportionofdifferentstages. Thisvariationistestedin
thetwo-stagesetupreferencedinTab.1,adjustingfromthestandard50%inStage1and50%inStage2
todifferentdistributions. Tab.7belowdisplaystheresultsoftheseexperiments.
Stage1 Stage2 #Tokens CR GQA MMVet SQA MME VQAğ‘‡ POPE MMB MMBğ¶ğ‘
0% 100% 18432 - 62.0 31.1 70.1 1453.0 58.2 85.9 64.3 58.3
25% 75% 11088 166% 62.1 31.7 70.6 1474.5 58.8 86.4 65.1 59.6
50% 50% 10863 170% 62.2 30.0 70.3 1443.5 57.5 85.8 64.8 59.7
75% 25% 10597 174% 61.6 32.2 70.8 1471.5 57.5 86.6 65.2 58.9
90% 10% 10407 177% 61.2 31.0 70.5 1447.5 56.3 86.4 64.4 56.9
100% 0% 10062 183% 55.9 29.5 64.1 1257.8 49.1 86.6 47.4 29.2
Table7 | Effectsofnon-uniformstagesplittingatthetwo-stageset-up. Performancedecreasesasthe
proportionofStage2decreases,albeitattheexpenseoflowercompressionratios.
WeobservethatastheStage2increasesfrom0%to100%,thereisagradualdecreaseinthemodelâ€™s
performance across various metrics (such as GQA, MMVet, SQA, MME, VQA, POPE, MMB, and
MMBğ¶ğ‘). Althoughthereisadeclineinperformance,itisrelativelyminorwhenthecompressionstage
makesupto50%ofthetrainingduration. However, whentheproportionofthecompressionstageis
reducedbelow50%,thedeclineinperformancebecomesmoresignificant. Inconclusion,keepingthe
compressionstagebetween0-50%ofthetrainingtimeminimizesperformancelosswhilestillachieving
significantcompressionratios.
14