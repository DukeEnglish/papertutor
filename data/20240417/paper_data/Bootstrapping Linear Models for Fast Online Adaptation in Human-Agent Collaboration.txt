Bootstrapping Linear Models for Fast Online Adaptation in
Human-Agent Collaboration
BenjaminA.Newman ChrisPaxton
CarnegieMellonUniversity,Meta Meta
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
newmanba@cmu.edu cpaxton@meta.com
KrisKitani HennyAdmoni
CarnegieMellonUniversity,Meta CarnegieMellonUniversity
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
kkitani@cmu.edu henny@cmu.edu
ABSTRACT 1 INTRODUCTION
Agentsthatassistpeopleneedtohavewell-initializedpoliciesthat Agentsthatcollaboratewithpeopletocompleteaperson‚Äôspre-
canadaptquicklytoalignwiththeirpartners‚Äôrewardfunctions.Ini- ferredgoalcannotalwaysknowthispreferenceinadvanceofan
tializingpoliciestomaximizeperformancewithunknownpartners interaction.Thoughpeoplemayinitiallystatethesepreferences,
canbeachievedbybootstrappingnonlinearmodelsusingimitation theymaydrift,sometimeschangingentirely,overthecourseof
learningoverlarge,offlinedatasets.Suchpoliciescanrequirepro- multipleinteractionepisodes.Whiletheremaybenocontinued
hibitivecomputationtofine-tunein-situandthereforemaymiss explicitcommunicationbetweencollaborativepartners,people‚Äôs
criticalrun-timeinformationaboutapartner‚Äôsrewardfunctionas in-situbehaviorsaregoal-drivenandthuscanrevealtheup-to-date
expressedthroughtheirimmediatebehavior.Incontrast,online preference.Thismeansthatupdatingagentpoliciesbasedonin-situ
logisticregressionusinglow-capacitymodelsperformsrapidinfer- behaviorsiscriticalforassistingpeopleduringcollaborations,i.e.
enceandfine-tuningupdatesandthuscanmakeeffectiveuseofim- ensuringthatrobotactionsaredeferentialtousergoals[21].
mediatein-taskbehaviorforrewardfunctionalignment.However, Muchcurrentresearchinhuman-agentcollaborationaimsto
theselow-capacitymodelscannotbebootstrappedaseffectively learnzero-shotcollaborationpoliciesfromofflinedatasetsthatare
byofflinedatasetsandthushavepoorinitializations.Wepropose eithercollectedfromhuman-humandemonstrations[8]orgen-
BLR-HAC,BootstrappedLogisticRegressionforHumanAgentCol- eratedsynthetically[28].Insteadofusinganindividual‚Äôsin-situ
laboration,whichbootstrapslargenonlinearmodelstolearnthe behaviortoupdateamodelonlinetoimproveperformancewithre-
parametersofalow-capacitymodelwhichthenusesonlinelogistic specttothatindividual‚Äôspreference,theseapproachestrainagents
regressionforupdatesduringcollaboration.WetestBLR-HACin offlineincollaborationwiththepopulationofpartneragentsrepre-
asimulatedsurfacerearrangementtaskanddemonstratethatit sentedbythetrainingdataset.Theythentargetgoodperformance
achieveshigherzero-shotaccuracythanshallowmethodsandtakes inaggregateontaskmetrics.Attesttime,theseapproachesassume
farlesscomputationtoadaptonlinewhilestillachievingsimilar thepreferencesandbehaviorofanewhumancollaboratorwill
performancetofine-tuned,largenonlinearmodels.Forcode,please fallwithinthedistributionofthecollaboratorsrepresentedbythe
seeourprojectpagehttps://sites.google.com/view/blr-hac trainingdata.Whiletheseapproacheshavebeenshowntobeef-
fectiveontaskmetricsingeneralcollaborationsettings,theydo
KEYWORDS notnecessarilytransfertothestrictercriteriaofassistivecollabo-
rationswheresuccessinataskisdictatedbyapersonalpreference
AssistiveRobotics;OnlineAssistance;Human-RobotInteraction;
andpeople‚Äôsgoalsandbehaviorscandriftawayfromthetraining
CollaborativeAssistance
distribution.
Furthermore,thepopulationofpersonalpreferencesissubstan-
ACMReferenceFormat:
tialanddiverse,makingitdifficulttoensuresufficientcoverage
BenjaminA.Newman,ChrisPaxton,KrisKitani,andHennyAdmoni.2024.
duringtrainingtime.Collectinglargedatasetsofhuman-human
BootstrappingLinearModelsforFastOnlineAdaptationinHuman-Agent
Collaboration.InProc.ofthe23rdInternationalConferenceonAutonomous dataistime-consumingandexpensive,whilecollaborationamong
AgentsandMultiagentSystems(AAMAS2024),Auckland,NewZealand,May populationsofprocedurallygeneratedagentscanyielddatathat
6‚Äì10,2024,IFAAMAS,10pages. donottightlymatchthedistributionofthehumanpopulation.Fur-
thermore,aspeoplerepeatedlyexecuteacollaborativetask,they
maydevelopnewpreferencesthatareunlikelytobecapturedby
thedistributionofcollaborationdatarepresentedinofflinedatasets.
ThisworkislicensedunderaCreativeCommonsAttribution
Weproposeamethodthattakesadvantageoftheseadvance-
International4.0License.
mentsinzero-shotcoordinationandappliesthemtoalgorithms
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems forfast,onlineadaptationfromin-situbehavior.Inthisway,we
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6‚Äì10,2024, hopetoachievebothgoodinitialperformancewhenassistinga
Auckland,NewZealand.¬©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org).
4202
rpA
61
]IA.sc[
1v33701.4042:viXraH A H A
State
Action Action Corrective Action Action Action
Actions
select sd place on top place on bottom select sd place on bottom
Figure1:Onestepofanexamplesurfacerearrangementtask:cupboardorganization.Fromlefttoright:aperson(H)picksan
objecttoplaceinthedishwasher;theagent(A)initiallyplacesthisincorrectly;thepersoncorrectstheplacement.Fromthis,
theagentlearnsthattheuserlikestoplaceblueobjectsonthebottomshelfandcanplacethenext,similarobjectcorrectly.
newpartner,butalsotocontinuetoadapttotheirpreferenceover Whilesomepriorapproachestodevelopingautonomousassis-
continuedexposure. tantsforhouseholdtasksrelyonpeopleprovidingfulltaskdemon-
Decipheringpeople‚Äôsexactpreferencescanbedifficult,however, strationsinadvanceofacollaboration,BLR-HACaimstooperate
asthesepreferencesareoftennotexplicitlystatedandcanchange inrealtime,utilizinginformationfromeachactionasitistakenby
overthecourseofaninteraction.Fortunately,in-situbehaviorsare aperson.Furthermore,approachesrelyingonfulltaskdemonstra-
goal-directedandcanimplicitlyrevealinformationaboutaperson‚Äôs tionscanintroduceadditionalburdenonapersonandberedundant
currentpreferenceorgoal,evenwhenitisnotexpresslycommuni- tothegoal-directedbehaviorpeopleexhibitwhencompletingtasks
cated.Wesuggestthatagentsengaginginassistivecollaborations [6].Incontrast,trainingshallow,low-capacitymodelswithlogistic
utilize these goal-directed behaviors to infer and act towards a regressionthroughMaxEntIRLtoutilizein-situbehaviorhasbeen
person‚Äôscurrentgoal,therebyenablingpersonalizedassistance. showntoeffectivelyandquicklyadapttopeople‚Äôsobjectivesin
Tododevelopamodelthatcanutilizethesegoal-directedbe- areassuchasrobotteleoperation[16]andmotionplanning[18].
haviorsforcollaborativeassistance,weintroduceBLR-HAC:Boot- We test BLR-HAC in a simulated version of our surface re-
strappedLogisticRegressionforHumanAgentCollaboration.This arrangementtask.WefindthatBLR-HACoutperformsbaseline
model is trained using a two-stage approach: first, we pretrain low-capacitymodelsandlarge,nonlinearmodelstrainedwithbe-
atransformer[30]tolearntoproducetheparametersofashal- haviorcloninginzero-shotcoordination.WealsofindthatBLR-
low,parameterizedpolicythatsecond,isupdatedthroughouta HACachievessimilarperformancebutrequiresafractionofthe
human-agentcollaborationusingonlinelogisticregression.Totest computeofatransformerthatisfine-tunedonline.Thisfinding
BLR-HAC,wefirstintroduceaformalizationofaspecificinstance holdstruewhenconsideringbothpreferencesthatremainthesame
ofarearrangementtask,whichwecallassistivesurfacerearrange- overtime,i.e.arestationary,andthosethatdrift,i.e.arenonsta-
ment.WethencompareBLR-HAC‚Äôsperformanceinasimulated tionary.Takentogether,theseresultsshowhowBLR-HACisable
versionofthistaskagainsttwobaselines:1)atraditionaltrans- totakeadvantageofthestrengthsofbothzero-shotandfastonline
formertrainedwithbehaviorcloningand2)atraditionalshallow adaptationmethods.Itdoesthisbypretrainingalarge,nonlinear
policytrainedwithonlinelogisticregression. modeltolearntheparametersofashallowpolicythatcanbeup-
Ourchosendomainofsurfacerearrangementmodelshousehold datedwithonlinelogisticregression.Thisresultsinacollaborative
tasks, like dishwasher loading, which have complex, long-term agentthatisbothwell-initializedandhighlyadaptable.
dependenciesdeterminedbyacombinationofaperson‚Äôsenviron- Inthispaperwemakethefollowingcontributions:
mentandtheirstronglyheldpreferences.Forexample,aperson
mayprefertoplacelargedishesbeforesmallonestomaximize
capacity.Suchhighdimensionalstateandpreferencespaceslead
toanalmostinfinitenumberofdiverseandequallyvalidsolutions ‚Ä¢ aformalizationofcommonhouseholdtasksascollaborative
forcompletinganygivenhouseholdchores.Forexample,choosing IRLtasks,whichwecallsurfacerearrangement,
toloadadishwasherbasedondishmaterialisjustasvalidasload- ‚Ä¢ anovelmodel,BLR-HAC,thatcombinesthestrengthsof
ingbasedondishsize;itisamatterofpersonalpreference.Given pretrainedlarge,nonlinearmodelswithlow-capacitymodels
thisdiversity,householdtasksmakeespeciallygoodtestbedsfor trainedonlinevialogisticregressionforefficientlearningin
studyingalgorithmsthatrequirealigningrobotpolicieswithpeo- human-robotcollaborations,and
ple‚Äôsrewardfunctions,thusmimickingmanyusecasesforassistive ‚Ä¢ evidence from experiments in simulation that BLR-HAC
robotics. outperformsitscomponentmodels.2 RELATEDWORK device,suchasajoystick,arobotcanobserveuserinputcommands
First,wepresentworkinstateandaction-conditionedmodelsthat andinfertheuser‚Äôsmostlikelygoalfromasetofpredetermined
donotexplicitlylearnabouttheirhumanpartnerduringtaskexecu- goals. The robot then assists the user by moving along a path
tion.Wefollowthisbyreviewingworkinadaptivecollaborations. towardsthepredictedgoal[16].MaxEntIRLcanalsobeusedto
interpret less direct forms of user behavior, such as physically
2.1 StateandAction-ConditionedCollaboration pushingarobotoutofthewaytodeterminewhichpaththeuser
preferstherobottotake,forexampletocarryacoffeemugaround
Priorapproachestosolvinglong-horizontaskswithcomplextem-
alaptopcomputerinsteadofoverit[18],usingnaturalisticeye
poraldependenciesandunderspecifiedsolutions,suchasthose
gazeincombinationwithjoysticksignalstocontrolarobotarm
presentinoursurfacerearrangementdomain,canrelyonresolv-
[4, 5, 22], or using corrective actions to learn about features of
ingambiguitiesthroughacombinationofteleoperationandpre-
theenvironmentthatrelatetoaperson‚Äôspreferencetoincrease
programmedroutines[12],orbysuggestingoptimal,predetermined
generalizabilityandsampleefficiency[24].Weareinterestedin
solutions[20].Solutionsfollowingtheformermethodcanplaceun-
adaptingonlineMaxEntIRLfordetermininghigh-leveltaskplans
dueburdenonapersontoexplicitlyexpresstheirpreferences,and
consistentwithuserpreferencesinhouseholdcollaborationsfrom
renderrobotactionredundantwhenademonstrationcompletes.
in-taskcorrectivebehavior.
Theyalsorequirepeopletocontinuallydemonstratetheirdesired
IRLhasalsobeenappliedtolearnrobotpoliciesinothertypes
solutionastheconstraintsofthetask,suchasaperson‚Äôspreference
ofhuman-robotinteractions.Forexample,tolearnpeople‚Äôsprefer-
ortheenvironment,vary.Methodsfollowingthelatterexampledo
encesfromobservationsofindependenttaskdemonstrations[31],
notallowforfullfreedomofexpressionfromtheuserandassume
orbylearningassistivesocialactionsfortherapybycombininga
allusershavethesame‚Äúoptimal‚Äùsolution.
therapists‚Äôexpertisewithexpertdemonstrations[3],orforsocial
Zero-shotcoordinationisarecentfieldofresearchaimingto
health,suchasarobotreceptionistlearningtogivehygienead-
develop models that can successfully and immediately interact
viceinashoppingmall[11].Ourformulationlearnspreferences
withnovelpartners.Thiscanbedonebypretrainingmodelsin
fromin-situ,collaborativebehaviorforcollaborativerearrangement
simulationagainstagentsdesignedtomimichumanbehavior[8]
tasks.
oroveradiversepopulationofsimulatedagents[28].Usingthese
Finally,anotherimportantaspectofmaintainingassistivehuman-
methods,though,canleadtooverlyspecificsolutions.Othershave
agentcollaborationsistomaintaincollaborativefluency[15].Main-
usedlargelanguagemodelstrainedwithweb-scaledatatopropose
tainingprinciplesofcollaborativefluency,suchasminimizingagent
taskplansthatarethenexecutedbyrobots[2].Thesetaskplans
andhumanidletime,allowshuman-agentcollaborationstofunc-
arenotadaptedtoanindividualuser,whoserewardfunctionmay
tionsimilarlytohuman-humancollaborations,therebyreducing
ormaynotfitwellwithinthedistributionseenduringtraining.
frictiononpeopletointeractwithautonomousagents.Further-
Thesemethodsplacetheburdenonthepersontoeitheraccepta
more,robotsassistingpeopletocompletecollaborativetaskshas
lesspreferredrobotbehaviororcontinuetoprovideactionsthat
beenshowntoaffectaperson‚Äôsultimatedecision[23],makingit
increasethelikelihoodoftherobotbehaviorexhibitingbehavior
importanttocontinuallymonitorandassesspeople‚Äôsgoalsduring
inlinewiththeperson‚Äôspreferences.Inthiswork,wefocuson
collaboration.Inthiswork,wewillusetheseideasasjustification
combiningthesegoodinitializationswithonlineadaptation.
forourdesiretodevelopanalgorithmthatadaptstouserprefer-
Often,approachesrelyingsolelyonlarge,pretraineddeepneural
encesinreal-time.
networksrequirepeopletogenerateexplicitdescriptionsoftheir
preferenceswhichcanbedecodedbythemodelintorobotaction
[2].Actionsproducedfromthisprocessarenotguaranteedtoalign 3 METHODS
withaperson‚Äôstaskobjective.Whiledeepnetworkscanpotentially
Weformalizethetaskofsurfacerearrangement,aspecificinstance
be adapted to meet individual preferences through fine-tuning
ofrearrangementproblems[7,29],asadecentralizedpartiallyob-
[10,14],doingsowithlargemodelscanleadtochallenging,unstable
servableMarkovdecisionproblem(DEC-POMDP).
learningthatresultsinvariableperformance[19].Inthiswork,we
focusondevelopinganalgorithmthatcanquicklyadapttopeople‚Äôs
naturallyexpressed,task-orientedbehavior. 3.1 DefiningSurfaceRearrangement
To study assistive collaborations, we introduce assistive surface
2.2 AdaptiveCollaborations
rearrangement,acollaborativepickandplacetaskwheretwoagents
Using IRL for robot control can be difficult, in part, due to the worktogethertoarrangeasetofobjectsùëÇintoasetoflocations
ambiguitythatarisesfromtraditionalIRL[1].Maximumentropy ùêø.Inthistask,theassistiveagentaimstohelpapersonrearrange
IRLfacilitatesthisbyusingtheprincipleofmaximumentropyto objectsintolocations.Importantly,theagent‚Äôsgoalistoachievethe
ordersolutionsaccordingtohowwelltheymatchobserveduser finalstatethatisdesiredbytheperson,whichisinitiallyunknown
behavior[32].Thissolutionhasalsobeenusedinbehavioralscience totheagent.
tomodelpeople‚Äôsabilitytoinferothers‚Äôgoalsfromtheirbehavior Asingleepisodeofthistaskconsistsofanobjectrepositorycon-
asexhibitedduringgoal-directedplans[6]. tainingobjectsùëú ‚ààùëÇ.Theinitialstateoftheepisodeisùêørandomly
Theseinsightshavebeenappliedtorobottrajectoryoptimization chosenobjectsfromùëÇ,andùêøvacantlocations.Eachlocationhasa
for shared control. In the difficult task of teleoperating a high- capacityforasingleobject.Progressinthetaskismadebyplacing
degreeoffreedomrobotarmwithalow-degreeoffreedominput objectsùëúintolocationsùëô ‚ààùêø.AtaskiscompletedwhenallobjectsAlgorithm1SurfaceRearrangement estimateandmaximizetheperson‚Äôsrewardfunction.We
R 1e :q ùë†u 0ir ‚Üêe: eùúã nùúÉ v, .rùúã eùúÉÀÜ s, ee tn ()v,ùëÇ,ùêø
‚Ä¢
ùõæth ,e are df io scr oe ua nss tiu nm ge faa cl tl oa rg .entshavethesamerewardfunction.
2: ùúâ ‚Üê (cid:2)ùë†0(cid:3)
3:
whileùúâ.length<ùêødo
54 :: ùëéùëé ‚Ñé ùëüùë°ùë° ‚Üê‚Üê ùúãùúã ùúÉùúÉ ÀÜ(cid:16)(cid:0) ¬∑¬∑ || ùëéùë†ùë° ‚Ñéùë°‚àí ,1 ùë†ùë°(cid:1) ‚àí1(cid:17) won eeG h(i abv vee ecn a nt u oh sa cet otw nh te e ra oos lt )s h ,u e tm hr iie sst paw rs oo s bua lmg eme en d rt ts eo da ubn ced eat sh p ta oet r aw so se in nar goe lv eo e arn gl wy eh no o tp s pt ei rm op bi oz ll ei in c mg y
,
6:
ùë†ùë°,ùëé ùëêùë° ‚Üêenv.step(cid:16) ùëé ‚Ñéùë°,ùëé ùëüùë°,ùë†ùë°‚àí1(cid:17) allowing it to be decomposed to a POMDP. Since POMDPs are
computationallyintractabletosolveexactly,weusetheQMDP
7: ùúâ.append(cid:16)(cid:104) ùëé ‚Ñéùë°,ùëé ùëüùë°,ùëé ùëêùë°,ùë†ùë°(cid:105)(cid:17) approximation[17].Priorworkinonlinehumanrobotcollaboration
8: endwhile [18]hasshownhowaQMDPcanbesolvedonlineusingonline
gradientdescent,adaptedforourpurposeinAlg.3.
ùëúhavebeenplacedintoalocationùëô.Forsimplicity,weassumethat
ùëÅ ‚â§ |ùêø|andthatplacingùëúinùëô occursinstantaneously. 4 APPROACH
Twoagentsinteractinanepisodeinthefollowingway.The
Ourultimategoalistolearnanassistivepolicythatcollaborates
humanagentùúã
ùúÉ
firstpicksanobjectgiventhecurrentstateùë†ùë°‚àí1.
with a person during a surface rearrangement task. Given that
Then,therobotagentùúã placesthisobjectintoalocation.The
ùúÉÀÜ wewantourpolicytobeassistive,itshouldtakeactionsthatare
environmentthenreturnsthenextstateùë†ùë°
andthehumancorrects alignedwiththeperson‚Äôsunderlyingpreferenceforcompletingthe
therobot‚Äôsaction,returningùëé ùëêùë° .Anepisodeùúâcanberepresented
task.Weinterpretthisasaregretminimizationproblem,wherethe
asthefollowingtuple:(cid:16) ùë†0,ùëé ‚Ñé1,ùëé ùëü1,ùëé ùëê1,ùë†1,...ùëé ‚Ñéùêø,ùëé ùëüùêø,ùëé ùëêùêø,ùë†ùêø(cid:17) . policyaimstominimizetheregretofitsactionswithrespecttothe
actionsthatwouldbeexhibitedundertheperson‚Äôstruepreference
3.2 FormalizingSurfaceRearrangement forcompletingthetask.Importantly,weassumethatthepolicy
does not have prior knowledge of this preference and that the
Giventhisdescription,wecanmodelassistivesurfacerearrange-
persondoesnotimmediatelyorexplicitlyrevealit.Additionally,
mentasadecentralizedpartiallyobservableMarkovdecisionprob-
weassumethatthespaceofpossiblepreferencesthepersoncould
lem(DEC-POMDP)whichisatupleof (ùëÜ,Œ†,ùê¥,ùëá,ùëç,ùëÇ,ùëü,ùõæ).Our
holdtobeextremelylarge,makingdisambiguationfromlimited
objectiveistotrainapolicyùúã ùëü thatsolvesthisDEC-POMDP:
interactionwiththepersondifficult.
‚Ä¢ Sisthesetofallpossiblestates.Asinpriorwork[18],we Under these conditions, we have two main ways to perform
assumethataparticularstateùë† ‚ààùëÜisatupleofobservable regretminimization.First,wecanensureourpolicytakesgood
andunobservablefeatures:ùë† = (ùë•,{ùúÉ ùëñ}).Observablestate initialactionsthatarelikelytoalignwiththeperson‚Äôspreference,
featuresarerepresentedasatupleofallpossiblelocations oftenreferredtoaszero-shotperformance.Second,wecanadapt
andallpossibleobjects.Locationsarerepresentedbytheir thepolicyonlineasahistoryofbehaviorisaccumulated.
IDandtheircurrentoccupancy.Objectsarerepresentedby Actioninferenceandpolicyadaptationdonotoperatewithina
theirIDandthelocationtheycurrentlyoccupy,ifany.The vacuum,butratherwithinthecourseoftheinteraction.Thecom-
unobservable portion of the state,ùúÉ ùëñ describes the learn- monmetricinhuman-robotinteractionofcollaborativefluency
ableparametersoftherewardfunctionconsistentwiththe [15],forexample,iscriticaltopeopleconsideringaninteraction
human‚Äôspreferenceinthetask. witharobottobe‚Äúgood.‚ÄùAnimportantfacetofthismetricisre-
‚Ä¢ Œ†isthesetofagents.Inourinitialversionofthisproblem, latedtotheamountoftimetherobotsitsidleduringtaskexecution.
weassumetwoagents:ahumanagentandanassistiveagent. Thismakesfrequentlyupdatinglargemodelsduringaninteraction
‚Ä¢ ùê¥ ùëñ isthesetofactionsforaparticularagentùëé ùëñ.Weassume challenging,asbothactioninferenceandpolicyupdatingrequire
thatthepersonbothselectsobjectsandcorrectsobjectplace- largeamountsofcomputation,leadingtohighrobotidletimes.We
ments,whiletherobotcanonlymakeobjectplacements. aimtodevelopamethodthatcantakeadvantageofthegoodperfor-
‚Ä¢ ùëç ùëñ isthesetofobservationsusedtoinferùúÉ.Theassistive manceoflargenonlinearmodelswhilebeingabletoquicklyadapt
agent‚Äôsobservationspaceistheperson‚Äôsactionspace.Inthis touserpreferences,asexpressedthroughtheirin-situbehavior,
workweassumethatthehumandoesnotinfertherobot‚Äôs withoutcausingtherobottoidle.
preference. TolearnanassistivepolicythatsolvestheDEC-POMDPdis-
‚Ä¢ ùëá(ùë†ùë°‚àí1,aùë°‚àí1,ùë†ùë°)denotesthetransitiondynamicsthatmodel cussedinSec.3.2,wefirstgenerateasimulateddatasetofdiverse,
theprobabilityofenteringaparticularstategiventhecurrent high-levelpreferences(Sec.4.1.1).Usingthesepreferences,wecol-
stateandbothagents‚Äôactions.Asinpriorwork[18],changes lectadatasetofcollaborativedemonstrationsinasimulatedsur-
inùëá aredictatedbyùúÉ.Weassumethistobeconstantand facerearrangementtaskoverarangeofdifficulties(Sec.4.1.2).We
deterministicwithinasingleepisode. thentrainourtwo-stagealgorithmbyfirst,learningtomimicthe
‚Ä¢ ùëÇ ùëñ(ùë†ùë°+1,ùë¢ ùëñùë°,ùëßùë°+1),theobservationdistributionforagentùúã ùëñ. collectedexpertdemonstrations(Sec.4.2)andsecond,usingthe
‚Ä¢ ùëü ùëñ(ùë†ùë°,{ùëé ùëñ}ùë°)istherewardfunctionfortheeachagent.We preferencerepresentationslearnedinSec.4.3toperformfast,online
assume an assistive setting where the agent is trying to adaptation.Input Feature Embedding Preference Estimation Action Estimation
Human Action Object Features
at-k-1:t ùùì
h h
R ao cb to -t k A -2c :t ti -o 1n Locatio ùùìn F reatures TrTarnasnf Pso rfr Beom fr Ble ome r lcer oe kn ùüÅcErc kn eE cnMocododedere lr Œ∏ ùùÖ a rt
State State Features
st-k-2:t-1 ùùì
s
Figure2:BLR-HACOverviewFromlefttoright,wefirstembedtheinputstateandactionsusingùúô.Thesearethenconcatenated
andfedintothepreferenceestimatorùúì.Thislearnstooutputrewardparameters,ùúÉ whichareusedtoinitializeanonline
learningpolicyusingthepolicyùúã,whichdeterminestherobot‚Äôsactionùëé ùëü.
Algorithm2ExpertDemonstrationCollection Algorithm3LearningPriorsforOnlineLinearRegression
Require: Œò,ùúã,env,ùëÇ,ùêø Require: ùê∑,M,ùúô ‚Ñé,ùúô ùëü,ùúô ùë†
1: ùê∑ = [] 1: whiletrainingdo
2:
forùúÉ inŒòdo
2:
for(ùë†,ùëé ‚Ñé,ùëò)inùê∑do
3: ùúâ ‚ÜêsurfaceRearrangement(ùúã ùúÉ,env,O,L) 3: ùúÉÀÜ‚ÜêM(ùúô ùë†(ùë†),ùúô ‚Ñé(ùëé ‚Ñé),ùúô ùëü (ùëé ùëê))
4: ùê∑.append(ùúâ) 4: ùëé ùëü ‚Üêargmaxùëéùëü‚ààùê¥ùëüùúô ‚Ñé(ùëé ‚Ñéùë°)¬∑ùúÉÀÜ¬∑ùúô ùëü(ùê¥ ùëü)
5: endfor 5: loss‚Üêùëù(ùëé ùëê)logùëû(ùëé ùëü)
6:
training‚ÜêM.update(loss)
7: endfor
8: endwhile
4.1 Datasets
4.1.1 ModelingaDiverseUserPopulation. Thetwokeyideasof
ourmethodtodevelopassistiverobotsforhouseholdcollaborations wecollect100demonstrationsfromeachpreferencegeneratedin
isthatthemethodshouldbeabletobotheffectivelyusealarge Sec.4.1.1.
populationofpreferencedatatopretraingoodinitializationsand
beabletoquicklyadapttoaparticularpreferencewhenpresented 4.2 LearningPreferencesinaDiverseUser
withinformationaboutthatpreference.
Population
Tocapturetheseideasinourexperiments,wedevelopasimu-
lateddatasetofpreferences.First,wesamplealargesetofpref- Thefirststepofourproposedalgorithmaimstominimizeregret
erences,representingapopulation,asencodedbyùúÉ.Weassume by achieving good zero-shot performance. Ultimately, we want
preferencesfromwithinthispopulationaredrawnnormallyfrom
tomodelùëù(ùëé ùëü|ùë†,ùëé ‚Ñé).Thisproblem,however,isill-posed,astwo
oneofseveralmodes,eachofwhichindicatesasubpopulationof policiesparameterizedbydifferentpreferenceswillcorrectlytake
similarpreferences.Wesamplethreepreferencedatasets:train,, twodifferentactionsùëé ùëü giventhesamestateandhumanaction.
andtest.Fromeachsetofpreferences,wesampleepisodesofsur-
Toaccountforthisambiguity,weincludeahistoryofùëòpriorstate
facerearrangementepisoderollouts,thuscreatingthreedatasets: and action pairs taken under the current preference and maxi-
ùê∑ ùë°ùëüùëéùëñùëõ, ùê∑ ùëíùë£ùëéùëô, andùê∑ ùë°ùëíùë†ùë°.ùê∑ ùë°ùëüùëéùëñùëõconsistsof1000simulatedprefer- mizeùëù(ùëé ùëü|ùë†ùë°‚àíùëò‚àí2:ùë°‚àí1,ùëé ‚Ñéùë°‚àíùëò‚àí1:ùë°,ùëé ùëêùë°‚àíùëò‚àí2:ùë°‚àí1).Forthesakeofbrevity,
ences,sampledfromfourmodes,with1000episodesperpreference. we will slightly abuse notation and refer to this distribution as
ùê∑ ùëíùë£ùëéùëô, andùê∑ ùë°ùëíùë†ùë° eachcontain100simulatedpreferences,with20 ùëù(ùëé ùëü|ùë†,ùëé ‚Ñé,ùëò).
episodesperpreference. Again,whentrainingassistiveagents,achievinglowzero-shot
performanceisnotouronlyobjective.Wealsoneedanagentthat
4.1.2 EnvironmentsforSurfaceRearrangement. Totesttheefficacy adaptsonlinetoincominguserbehaviorwhilemaintainingcollabo-
ofourapproachatvaryingdifficulties,wedevelopthreeenviron- rativefluency.Thismeansdevelopingalightweight,low-parameter
ments.Eachenvironmentscalesproblemdifficultybyincreasing modelcapableofperformingactioninferenceandpolicyadaptation
thesizeofthestatespace.Wehaveasmallenvironment,withfive inrealtime.
possibleobjectsandfivelocations,amediumenvironment,with To do this, instead of learningùëù(ùëé ùëü|ùë†,ùëé ‚Ñé,ùëò) directly, we first
tenobjectsandtenlocations,andfinallyalargeenvironment,with learnalatentspacethatcorrespondstotheweightsofalogistic
25objectsand25locations. regressionproblem.Theseweightsserveastheinputtothesecond
Tocollectademonstrationdatasetforeachenvironment,weuse stepofouralgorithm,Sec.4.3.Thus,wetrainourmodeltomaximize
Alg.2.Importantly,tocollectexpertdemonstrations,wesetùúÉ =ùúÉÀÜ ùëÄ ùúô,ùúì(ùë†,ùëé ‚Ñé,ùëò,ùë°)=ùëù(ùúÉ|ùë†,ùëé ‚Ñé,ùëò,ùë°).Inthisway,weplaceaninductive
andusealinearpolicyùúã = ùúô ‚Ñé(ùëé ‚Ñé) ¬∑ùúÉ ¬∑ùúô ùëü(ùê¥ ùëü),whereallùúô are biasoverthelatentspaceofthemodel,enticingittolearnamatrix
implementedasone-hotembeddinglayers.Foreachenvironment ofsizeùëÇ√óùêø,thatcanbeusedastheweightsofanonlinelogisticregressionproblem.Wetreatthisasaclassificationproblemand intheinitialmodelweights.Ourintuition,though,isthat
minimizethecrossentropylossbetweenourmodel‚Äôspredictions sincedemonstrationsaredrawnfromalarge,diversepopu-
andthecollectedexpertdemonstrations:ùêø=ùëù(ùëé ùëê)¬∑logùëû(ùëé ùëü)where lationofpreferences,andthattherelationsbetweenprefer-
ùëû(ùëé ùëü)=ùúô(ùëé ‚Ñé)¬∑ùëÄ(ùë†,ùëé ‚Ñé,ùëò,ùë°)¬∑ùúô(ùê¥ ùëü),asshowninAlg.3. encesandpeoplearenotknownapriori,thisdisambigua-
tionwillbenefitfromanonlinearfunctionapproximator.We
4.3 BootstrappingShallowLinearModelsfor expectnonlinear,high-capacitymodelstooutperformthis
Fast,OnlineAdaptation baseline.
‚Ä¢ DeepLinear.Sincethespaceofpreferencesisverylarge,
Thesecondstepofourproposedalgorithmaimstominimizeregret
itcouldsimplybethatincreasingmodelcapacitywithout
throughonlineadaptation.Usingtheoutputofthemodellearned
introducingnonlinearitymaycapturethepreferencedistri-
inSec.4.2,wecanemployonlinelogisticregression,whichhas
bution.Totestthis,weintroduceDeepLinear,whichsimply
beenshowntoworkwellforteachinghumanpreferencestoagents
addsadditionalmodelparametersinbothwidthanddepth.
throughcorrectivefeedbackinrobotcontroltasks.Importantly,
WeexpectthismodeltooutperformaShallowLinearmodel
sinceonlinelogisticregressionhasaverysimpleupdateruleto
estimateùúÉthatoperatesoveramuchsmallernumberofparameters butunderperformnonlinearmethods.
‚Ä¢ Multi-LayerPerceptron.Totesttheimportanceofmodel-
thanalarge,nonlinearnetwork,wecanadaptthisinitialestimate
ingthepreferencedistributionwithanonlinearmodel,we
oftheperson‚Äôspreferencein-situwithoutriskinglargehumanor
introduceamulti-layerperceptronbaseline.Weexpectthis
robotidletime,therebymaintainingcollaborativefluency.
ToupdateourestimateofùúÉ,weusealinearapproximationofthe modeltooutperformbothlinearmethodsbutunderperform
attention-basedmechanisms.
QMDPsolutiontotheDEC-POMDPinSection3.2andstochastic
‚Ä¢ CausalTransformer.Finally,sincewearepassingahis-
gradientdescent,resultinginthefollowingupdaterule:
toryofbehaviortothemodelateverytimestep,wecan
ùúÉÀÜ=ùúÉÀÜ‚àíùõº(ùúô ‚Ñé(ùëé ‚Ñé)¬∑ùúô ùëü(ùëé ùëü)‚àíùúô ‚Ñé(ùëé ‚Ñé)¬∑ùúô ùëü(ùëé ùëê)) inferthecurrentpreferencefromthissequenceofbehaviors.
whereùõº isthelearningrate. Attention-basedmechanisms,specificallycausaltransform-
ers,havebeenshowntoexcelatmodelingsequentialdata.
5 EXPERIMENTALDESIGN Totestthisweimplementùúì asatransformer,andexpectit
tooutperformallothermethods.
Totestouralgorithm,wedesignseveralexperiments.First,wevali-
datetheneedforlarge,nonlinearmodelstolearnthedistributionof Thesecondaxisofbaselineswedevelopcomparestheimpor-
preferencesembeddedinthedemonstrationdataset,Sec.5.1.Then tanceofintroducinganinductivebiasoverthelatentspaceinorder
weexplorehowouralgorithmfaresinitsintendedusecase:fast, tolearnùúÉ.Wecompareanimplementationoftheabovemodels
onlineadaptation.Wetestthisintwoscenarios.Sec.5.2.1analyzes inwhicheachmodelminimizesùêø=ùëù(ùëé ùëü)¬∑logM(ùë†,ùëé ‚Ñé,ùëò)toour
adaptationtoasinglepreferenceovertime,whileSec.5.2.2explores proposedinductivebias,whichminimizesùêø =ùëù(ùëéùëü)¬∑logùúô(ùëé ‚Ñé)¬∑
howwellouralgorithmfareswhenthepreferencegeneratingthe M(ùë†,ùëé ‚Ñé,ùëò)¬∑ùúô(ùê¥ ùëü).
behaviorchangeswithoutexplicitcommunicationtotherobot.
5.1.1 ImplementationDetails. Toimplementourmodelswemake
5.1 Zero-ShotCoordination thefollowingdecisions.Weperformaseparateparametersweep
foreachmodelandenvironmentforthefollowingparametersand
Weevaluateourmodelineachenvironmentoverthetestsetusing
ranges:learningrate (1ùëí‚àí3,1ùëí‚àí6),thedimensionalityofhidden
Alg.3.Whileweareinsearchofanalgorithmthatperformsregret
layers(25,28),andthenumberoflayersinùúì (3,5,7,10,12).Weset
minimization,thismetricisrelativetoaspecificpreference.To
thesizeoftheinputhistorytobe50,paddingwhennecessary.For
understandmodelperformanceinanabsolutesenseandcompare
eachmodel,weimplementallùúô asasingle,one-hotembedding
acrossenvironments,wereportaccuracyintermsofthenumberof
spaceofvocabularysize208,where0-7arespecialcharacters,8-107
correctrobotactionpredictions.Thismetricisinverselycorrelated
arelocationindices,and108-207areobjectindices.Toimplement
withregret.
ùúì,weusePyTorch[25]andbaseourimplementationofacausal
Wechooseourbaselinestoexaminetwokeyquestions:1)are
transformeronDecisionTransformer[9].Weimplementùúã asa
high-capacity,nonlinearmodelsnecessaryfordisambiguationbe-
simplelinearmodelforinductivebiasandasanMLPfornoin-
tweenpreferencesinahighlydiversepreferencespace,and2)how
ductivebias.Allmodelsaretrainedusingtheappropriatetraining
doesinducinganinductiveprioroverthelatentspaceaffectzero-
andevaluationsets,whichdonotoverlapwiththetestset,with10
shotperformance?
epochsofearlystopping.
Toanswerthesequestionsweintroducebaselinesacrosstwo
axes:modelcomplexityandmodelbias.Todeterminetheeffectof
high-capacitynonlinearmodelsonzero-shotperformancewecom- 5.2 Test-TimeAdaptation
parefourlevelsofmodelcomplexityintermsofhowweimplement Developingassistivepoliciesisnotonlyaboutachievinggoodzero-
ùúì inFig.2: shotperformance,however.Thespaceofactualhumanpreferences
‚Ä¢ ShallowLinear.TypicalonlineIRLsettingslearnashallow isalmostboundlessandlikelyimpossibletocaptureinadvance
modelfromscratchusingMaxEntIRL.Tobootstrapthispro- ofaninteraction.Therefore,itisimportanttodevelopalgorithms
cess,onecouldperformthesameprocessovertheoffline thatcanrapidlyalignthemselveswithpreferencesassociatedwith
dataset,therebyencodingthediversepreferencepopulation aperson‚Äôsin-situ behavior.Westudythisintwosettings.First,Small Medium Large
NoPrior Prior(ours) NoPrior Prior(ours) NoPrior Prior(ours)
ShallowLinear 0.413 0.665 0.215 0.518 0.096 0.289
DeepLinear 0.425 0.680 0.199 0.504 0.101 0.303
MLP 0.605 0.759 0.361 0.653 0.120 0.358
Transformer 0.729 0.771 0.603 0.673 0.160 0.412
Table1:Wecomparezero-shotperformanceonthetestsetofeachenvironment.Wehavetwoaxesofcomparison:model
complexityintherows,andinductivepriorinthecolumns.Resultsarereportedintermsofaccuracy.Wecanseethatthe
highestcapacity,attentionbasedmodeltrainedwithaninductiveprioroutperformsallothermodelsineveryenvironment.
weanalyzeouralgorithm‚Äôsabilitytoadapttoastationaryprefer- attention-based method trained with an inductive prior outper-
enceoverthecourseofmultipleepisodes.Then,weanalyzeour formsallothermethods,achieving77.1%,67.3%,and41.2%accu-
algorithm‚Äôsabilitytoadaptinscenarioswherepreferencesarenon- racyonthesmall,medium,andlargeenvironments,respectively.
stationary.Here,weareinterestedinanalgorithm‚Äôsabilityto1) Weseethatthedifferenceinperformancebetweenmodelstrained
maintaindecentperformanceinthefaceofthepreferencechange, withandwithouttheinductivepriorincreasesasthedifficultyof
and2)rapidlyrecoverafterthechangeinpreference. theproblemincreases.Additionally,weseethegeneraltrendthat
highercapacity,nonlinearmodelsoutperformlowercapacitylinear
5.2.1 Stationary Preferences. To test our algorithm‚Äôs ability to
models.Theseresultsempiricallyjustifyourdesiretouseahigh-
adapttostationarypreferences,weaveragetheperformanceof
capacitynonlinearmodeltobootstrapalinearmodelinanonline
ourbootstrappedonlineIRLalgorithmoverallpreferencesinthe
logisticregressionproblem.
testingsetover20episodesineachtestingenvironment.
OursecondsetofresultsisshowninFig.3.Here,weplotthetest-
Wecompareagainstalinearmodelthatlearnsfromscratchand
timeadaptationaccuracyforthreemodels:linear(inred),BLR-HAC
amethodthatoptimizesoveralltransformerparametersbetween
(ingreen),andanonlinetransformer(inyellow).Fromthesegraphs,
episodesbutkeepsinferencecomputationconstant.Wemeasure
wecanseesupportforourhypothesisthatbootstrapped,shallow
computationcostintermsofFLOPSandcalculatethesevalues
linearmodelstrainedwithIRLachievegoodaccuracywithlow
empiricallyusingFVCore.Weexpecttoseethatthebootstrapped
computation.WecanseethatBLR-HACandTransformerbothstart
onlineIRLalgorithmachievessimilarperformancetotheonline
withhigheraccuracythanLinearinallcasesandthatthisdifference
transformermethodbutatafractionofthecompute.
increasesastheproblemcomplexityincreases.Furthermore,wesee
5.2.2 Nonstationary Preferences. Similar to the stationary pref- howBLR-HACachievessimilarperformanceoverepisodesasthe
erencesexperiment,werunIRLover20episodes.Inthisanaly- transformermethod,butatafractionofthecomputation.While
sis,however,weswitchtoadifferentrandomobjectiveafter10
bothmethodshavesimilarinferencecompute,ofùëÇùë•ùêøFLOPS,BLR-
episodes.Again,wecompareagainstalinearmodellearningfrom
HACusesonly2ùë•ùëÇùë•ùêøFLOPS,whiletheTransformermethoduses
scratchandanonlinetransformerimplementation.Weexpectto
‚àº400ùëÄFLOPSduringupdates.
seethatthelinearmethodstartswithpoorperformancebutadapts Finally,weseeinFig.4resultsfromtest-timeadaptionwith
quicklywhenexposedtoincomingbehavior.Weexpecttoseethat nonstationarypreferences.Theseresultsshowmixedsupportfor
thetransformermethodstartswithgoodperformanceandadapts ourhypothesisthatbootstrapped,shallowlinearmodelstrained
moreslowlyasbehaviordataisaccumulated.Finally,weexpect withIRLrecoverwellfromunexpectedshiftsinuserbehavior.In
ourmethodtoachievethebenefitsofboththelinearfromscratch eachgraph,episodes1-10showsimilarresultstothepreviousset
andthetransformermethods:itshouldstartoffwithreasonable ofexperiments.Atepisode10,however,thepreferenceshifts,and
performanceandadaptquicklyasdataisaggregated. allmodelssufferadropinperformance.Interestingly,inallcases,
BLR-HACsuffersthesmallestdropinperformance.Whilethisisa
5.2.3 ImplementationDetails. Forbothexperiments,wedoahy- positiveresult,wealsoseethatastheenvironmentbecomesmore
perparametersweepoverthelearningrateintherange(1ùëí‚àí2,1ùëí‚àí5)
complex, BLR-HAC suffers in its adaptation rate from episodes
forthetransformerand(1,5,10)forthelinearmodels.Inbothcases,
10-20.Whileitadaptsonparwiththelinearmethod(thoughstill
weusethemaximumlearningrateforallexperiments.Additionally, achieveshigherperformanceduetoitsbetterinitialperformance)
weusestochasticgradientdescentforoptimizationinbothcases. itadaptsslowerthanthetransformer-basedmethod.Thisislikely
Totrainthetransformermethod,weperformfivestepsofgradient duetothefactthatthetransformerisabletomakebetteruseof
descentbetweeneachepisode. thelargeramountsofdatathatarebeingaggregatedinthelarge
environment.
6 RESULTS
FromrunningtheexperimentsoutlinedinSec.5,wehavethree
mainresults.First,wefindsupportforourhypothesisthatnonlin-
ear,high-capacitymodelstrainedwithinductivebiasescanlearn
a diverse population of user preferences. In Tab. 1, we see theStationaryAdaptationinùëÜùëöùëéùëôùëô StationaryAdaptationinùëÄùëíùëëùëñùë¢ùëö StationaryAdaptationinùêøùëéùëüùëîùëí
1 1 1
0.5
Linear
0.5 0.5
BLR-HAC
Transf
0 0 0
1 5 10 15 20 1 5 10 15 20 1 5 10 15 20
EpisodeNumber EpisodeNumber EpisodeNumber
Figure3:StationaryTest-TimeAdaptation.Learningcurvesforeachtestenvironmentforeachalgorithm.Wereporttheaverage
accuracyovereachepisode.BLR-HACisabletoachievethelowzero-shotperformanceofthetransformermethod,andthefast
adaptationofthelinearmethod.Additionally,wecanseethatastheepisodelengthincreases,thesedifferencesinperformance
aremorenotable,withthelinearmethodfailingtocatchuptotheothertwomethodsoverthecourseof20episodes.
NonstationaryAdaptationinùëÜùëöùëéùëôùëô NonstationaryAdaptationinùëÄùëíùëëùëñùë¢ùëö NonstationaryAdaptationinùêøùëéùëüùëîùëí
1 1 1
0.5 0.5 0.5
Linear
BLR-HAC
Transf
0 0 0
1 5 10 15 20 1 5 10 15 20 1 5 10 15 20
EpisodeNumber EpisodeNumber EpisodeNumber
Figure4:NonstationaryTest-TimeAdaptation.Learningcurvesovereachtestenvironmentforeachalgorithm.Wereport
averageaccuracyoverepisodes.BLR-HACisabletoperformonparwiththetransformermethodinthesmallandmedium
environmentsandpartofthelargeenvironment.BLR-HACoutperformsallmethodsinallenvironmentsimmediatelyafter
thepreferenceswitch.Inthelargeenvironment,though,thetransformerrecoversmorequicklyasithasaccesstomoredata.
7 DISCUSSION,LIMITATIONS,ANDFUTURE MechanicalTurk[13]orProlific[26]wouldallowustopretrain
WORK BLR-HACwithrealdata.
Finally,ourmethodalsoassumesasingle,synchronizedmodal-
Wedeveloppoliciesforassistiveagentsthatarebothwell-initialized
ityofcorrectiveactions:directstatecorrections.Thismakesour
andhighly-adaptable.Throughsimulatedexperiments,ourmethod
learningproblemeasierbymaximizingthecorrelationbetweenthe
achievesboththegoodinitializationsoflarge,nonlinearmodels
leader‚Äôscorrectionsandtheirrewardfunction.Wewouldliketo
trainedwithbehaviorcloningandthefastadaptationtouserbehav-
extendourapproachtoaccountforothermodalitiesofcorrections
iorpresentinlow-capacitymodelstrainedwithonlineMaxEntIRL.
issuedasynchronously,suchasthoseexpressedinrealtimethrough
Importantly,BLR-HACinitializesbetterthanShallowLinearontest
verbalornonverbalcommunication.
datathatisfarfromtheinitialdistribution,meaningthatourap-
proachshouldideallyallowforfasteradaptationtopopulationsfor
8 CONCLUSION
whomitisdifficulttocollectdataforofflinepretraining.
FutureworkshouldexploreapplyingBLR-HACtouserstudies Inthiswork,welaidoutanargumentforwhyassistiveagents
withrealpeopletodeterminewhetherthebetterinitializationsand shouldbebothwell-initializedandhighly-adaptable.Weintroduced
fasteradaptationsofourmethodholdoutsideofsimulationand anovelformulationofassistivehuman-agentcollaborationascol-
arepreferred.Itisalsoimportanttostudyhowtheeffectofthesize laborativeinversereinforcementlearningandintroducedanalgo-
ofthesurfacerearrangementproblemontheseresults. rithmBLR-HACthattakesadvantageofsophisticatedpopulation-
Userstudiesalsoprovideanopportunitytoimproveourmethod. levelmodelingfoundindeepneuralnetworkswiththefastadapta-
Collectinginteractiondatathroughinteractivesimulators,such tionofshallow,low-capacityinversereinforcementlearningmeth-
as AI Habitat [27, 29], deployed on platforms such as Amazon ods. Finally, we verified these claims through simulated experi-
ments.
)tcerroc%(ycaruccA
)tcerroc%(ycaruccA9 ETHICSSTATEMENT
[10] SeanChen,JensenGao,SiddharthReddy,GlenBerseth,AncaD.Dragan,and
SergeyLevine.2022.ASHA:AssistiveTeleoperationviaHuman-in-the-LoopRein-
Weshowwecanuseofflinedatasetstobootstrapassistivecollab-
forcementLearning.In2022InternationalConferenceonRoboticsandAutomation
orations by pretraining assistive agents. This method, however, (ICRA).7505‚Äì7512. https://doi.org/10.1109/ICRA46639.2022.9812442
necessitatesusingspecificsubpopulationsofthelargerhumanpop- [11] ZhichaoChen,YutakaNakamura,andHiroshiIshiguro.2022. Androidasa
ReceptionistinaShoppingMallUsingInverseReinforcementLearning.IEEE
ulation,i.e.thoserepresentedbythedataset.Thisleadstoethical RoboticsandAutomationLetters7,3(2022),7091‚Äì7098. https://doi.org/10.1109/
questionssuchas:Arethepreferencespresentinthedatasetrepre- LRA.2022.3180042
[12] MateiCiocarlie,KaijenHsiao,AdamLeeper,andDavidGossow.2012.Mobile
sentativeofthelargerpopulation?Howdoesthisaffectpeoplewho
manipulationthroughanassistivehomerobot.In2012IEEE/RSJInternational
holdpreferencesoutsidethissubpopulation?Thesequestionsare ConferenceonIntelligentRobotsandSystems.5313‚Äì5320. https://doi.org/10.1109/
especiallypertinentinassistivesettings,whereagentsarelikelyto IROS.2012.6385907
[13] KevinCrowston.2012.AmazonMechanicalTurk:AResearchToolforOrganiza-
encounterout-of-distributionphenomenaattest-time.Itisques-
tionsandInformationSystemsScholars.InShapingtheFutureofICTResearch.
tionssuchasthesethatmotivatethiswork. MethodsandApproaches,AnolBhattacherjeeandBrianFitzgerald(Eds.).Springer
Weassumeacriticalpartofprovidingassistanceistoreduce BerlinHeidelberg,Berlin,Heidelberg,210‚Äì221.
[14] JerryZhi-YangHe,ZackoryErickson,DanielS.Brown,AditiRaghunathan,and
unnecessaryburdenplacedonindividualswhileactinginalign- AncaDragan.2022. LearningRepresentationsthatEnableGeneralizationin
mentwiththeirpreference.Whenaperson‚Äôspreferencesarewell AssistiveTasks.In6thAnnualConferenceonRobotLearning. https://openreview.
net/forum?id=b88HF4vd_ej
representedbythedataset,pretrainingnecessarilyminimizesa
[15] GuyHoffman.2019.EvaluatingFluencyinHuman‚ÄìRobotCollaboration.IEEE
person‚Äôsburdentobringtheagentintoalignmentwiththeirprefer- TransactionsonHuman-MachineSystems49,3(2019),209‚Äì218. https://doi.org/
ence.Whenaperson‚Äôspreferencesarenotwellrepresentedbythe 10.1109/THMS.2019.2904558
[16] ShervinJavdani,HennyAdmoni,StefaniaPellegrinelli,SiddharthaS.Srini-
dataset,ourmethodalignstotheperson‚Äôspreferencequicklyby
vasa, and J. Andrew Bagnell. 2018. Shared autonomy via hindsight opti-
usingtheirin-situ,goal-directedbehavior.Thus,whilethemodel mizationforteleoperationandteaming. TheInternationalJournalofRobot-
doesnothaveaninitialrepresentationoftheseout-of-domainpref- icsResearch37,7(2018),717‚Äì742. https://doi.org/10.1177/0278364918776060
arXiv:https://doi.org/10.1177/0278364918776060
erences,itdoesknowhowtointerpretgoal-directedbehaviorsin [17] MichaelLLittman,AnthonyRCassandra,andLesliePackKaelbling.1995.Learn-
ordertolearnsucharepresentation. ingpoliciesforpartiallyobservableenvironments:Scalingup.InMachineLearn-
ingProceedings1995.Elsevier,362‚Äì370.
Webelievethereisampleopportunityforfutureworktocon-
[18] DylanPLosey,AndreaBajcsy,MarciaKO‚ÄôMalley,andAncaDDragan.2022.
tinuetoexploresolutionstotheseethicaldilemmas,suchasto Physicalinteractionascommunication:Learningrobotobjectivesonlinefrom
learnmoregeneralizablefeaturesofpreferencesthatallowforbet- humancorrections.TheInternationalJournalofRoboticsResearch41,1(2022),
20‚Äì44.
terrepresentationsofhumanpreferences,orbyteachingagents
[19] MariusMosbach,MaksymAndriushchenko,andDietrichKlakow.2021. On
tolearntolearnpreferences,whichwouldimproveanassistive theStabilityofFine-tuning{BERT}:Misconceptions,Explanations,andStrong
agentsabilitytoadapttoout-of-distributionpreferences. Baselines. In International Conference on Learning Representations. https:
//openreview.net/forum?id=nzpLWnVAyah
[20] BenjaminNewman,KevinCarlberg,andRutaDesai.2020.OptimalAssistancefor
Object-RearrangementTasksinAugmentedReality. arXiv:2010.07358[cs.HC]
[21] BenjaminA.Newman,ReubenM.Aronson,KrisKitani,andHennyAdmoni.
REFERENCES 2022. HelpingPeopleThroughSpaceandTime:AssistanceasaPerspective
onHuman-RobotInteraction. FrontiersinRoboticsandAI 8(2022). https:
[1] PieterAbbeelandAndrewYNg.2004. Apprenticeshiplearningviainverse //doi.org/10.3389/frobt.2021.720319
reinforcementlearning.InProceedingsofthetwenty-firstinternationalconference [22] Benjamin A. Newman, Reuben M. Aronson, Siddhartha S. Srinivasa, Kris
onMachinelearning.1. Kitani, and Henny Admoni. 2022. HARMONIC: A multimodal dataset of
[2] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes, assistive human‚Äìrobot collaboration. The International Journal of Robot-
ByronDavid,ChelseaFinn,KeerthanaGopalakrishnan,KarolHausman,Alex icsResearch41,1(2022),3‚Äì11. https://doi.org/10.1177/02783649211050677
Herzog,etal.2022. Doasican,notasisay:Groundinglanguageinrobotic arXiv:https://doi.org/10.1177/02783649211050677
affordances.arXivpreprintarXiv:2204.01691(2022). [23] BenjaminA.Newman,AbhijatBiswas,SarthakAhuja,SiddharthGirdhar,KrisK.
[3] AntonioAndriella,CarmeTorras,CarlaAbdelnour,andGuillemAleny√†.2022. Kitani,andHennyAdmoni.2020.ExaminingtheEffectsofAnticipatoryRobot
IntroducingCARESSER:Aframeworkforinsitulearningrobotsocialassistance AssistanceonHumanDecisionMaking.InSocialRobotics,AlanR.Wagner,David
fromexpertknowledgeanddemonstrations.UserModelingandUser-Adapted Feil-Seifer,KerstinS.Haring,SilviaRossi,ThomasWilliams,HongshengHe,and
Interaction(032022). https://doi.org/10.1007/s11257-021-09316-5 ShuzhiSamGe(Eds.).SpringerInternationalPublishing,Cham,590‚Äì603.
[4] ReubenM.AronsonandHennyAdmoni.2022.GazeComplementsControlInput [24] BenjaminA.Newman,ChristopherJasonPaxton,KrisKitani,andHennyAd-
forGoalPredictionDuringAssistedTeleoperation.Roboticsscienceandsystems moni.2023.TowardsOnlineAdaptationforAutonomousHouseholdAssistants.
(2022). https://par.nsf.gov/biblio/10327640 InCompanionofthe2023ACM/IEEEInternationalConferenceonHuman-Robot
[5] ReubenM.Aronson,ThiagoSantini,ThomasC.K√ºbler,EnkelejdaKasneci,Sid- Interaction(Stockholm,Sweden)(HRI‚Äô23).AssociationforComputingMachinery,
dharthaSrinivasa,andHennyAdmoni.2018. Eye-HandBehaviorinHuman- NewYork,NY,USA,506‚Äì510. https://doi.org/10.1145/3568294.3580136
RobotSharedManipulation.InProceedingsofthe2018ACM/IEEEInternational [25] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
ConferenceonHuman-RobotInteraction(Chicago,IL,USA)(HRI‚Äô18).Association Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDes-
forComputingMachinery,NewYork,NY,USA,4‚Äì13. https://doi.org/10.1145/ maison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,Alykhan
3171221.3171287 Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
[6] ChrisLBaker,JoshuaBTenenbaum,andRebeccaRSaxe.2007.Goalinference Chintala.2019.PyTorch:AnImperativeStyle,High-PerformanceDeepLearn-
asinverseplanning.InProceedingsoftheAnnualMeetingoftheCognitiveScience ingLibrary. InAdvancesinNeuralInformationProcessingSystems32.Curran
Society,Vol.29. Associates,Inc.,8024‚Äì8035. http://papers.neurips.cc/paper/9015-pytorch-an-
[7] DhruvBatra,AngelXChang,SoniaChernova,AndrewJDavison,JiaDeng, imperative-style-high-performance-deep-learning-library.pdf
VladlenKoltun,SergeyLevine,JitendraMalik,IgorMordatch,RoozbehMot- [26] Prolific.2014Online.Prolific. https://www.prolific.co
taghi,etal.2020.Rearrangement:Achallengeforembodiedai.arXivpreprint [27] ManolisSavva,AbhishekKadian,OleksandrMaksymets,YiliZhao,ErikWijmans,
arXiv:2011.01975(2020). BhavanaJain,JulianStraub,JiaLiu,VladlenKoltun,JitendraMalik,etal.2019.
[8] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel, Habitat:Aplatformforembodiedairesearch.InProceedingsoftheIEEE/CVF
andAncaDragan.2019.Ontheutilityoflearningabouthumansforhuman-ai InternationalConferenceonComputerVision.9339‚Äì9347.
coordination.Advancesinneuralinformationprocessingsystems32(2019). [28] DJStrouse,KevinMcKee,MattBotvinick,EdwardHughes,andRichardEverett.
[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, 2021. Collaboratingwithhumanswithouthumandata. AdvancesinNeural
Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. InformationProcessingSystems34(2021),14502‚Äì14515.
Decision Transformer: Reinforcement Learning via Sequence Modeling.
arXiv:2106.01345[cs.LG][29] AndrewSzot,AlexanderClegg,EricUndersander,ErikWijmans,YiliZhao, Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),
JohnMTurner,NoahDMaestre,MustafaMukadam,DevendraSinghChap- Vol.30.CurranAssociates,Inc. https://proceedings.neurips.cc/paper/2017/file/
lot,OleksandrMaksymets,AaronGokaslan,Vladim√≠rVondru≈°,SameerDharur, 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
FranziskaMeier,WojciechGaluba,AngelXChang,ZsoltKira,VladlenKoltun, [31] BryceWoodworth,FrancescoFerrari,TeofiloE.Zosa,andLaurelD.Riek.2018.
JitendraMalik,ManolisSavva,andDhruvBatra.2021. Habitat2.0:Training PreferenceLearninginAssistiveRobotics:ObservationalRepeatedInverseRein-
HomeAssistantstoRearrangetheirHabitat.InAdvancesinNeuralInforma- forcementLearning.InProceedingsofthe3rdMachineLearningforHealthcareCon-
tionProcessingSystems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.Wortman ference(ProceedingsofMachineLearningResearch,Vol.85),FinaleDoshi-Velez,Jim
Vaughan(Eds.). https://openreview.net/forum?id=DPHsCQ8OpA Fackler,KenJung,DavidKale,RajeshRanganath,ByronWallace,andJennaWiens
[30] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones, (Eds.).PMLR,420‚Äì439. https://proceedings.mlr.press/v85/woodworth18a.html
AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll [32] BrianD.Ziebart,AndrewMaas,J.AndrewBagnell,andAnindK.Dey.2008.
youNeed.InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.Von MaximumEntropyInverseReinforcementLearning.InProc.AAAI.1433‚Äì1438.