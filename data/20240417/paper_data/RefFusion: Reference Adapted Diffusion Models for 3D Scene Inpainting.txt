RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting
ASHKANMIRZAEI,NVIDIA,UniversityofToronto,Canada
RICCARDODELUTIO,NVIDIA,USA
SEUNGWOOKKIM,NVIDIA,SouthKorea
DAVIDACUNA,NVIDIA,Canada
JONATHANKELLY,UniversityofToronto,Canada
SANJAFIDLER,NVIDIA,UniversityofToronto,Canada
IGORGILITSCHENSKI,UniversityofToronto,Canada
ZANGOJCIC,NVIDIA,Switzerland
Input Gaussians Novel Views
Scene
Inpainting
Object
Insertion
Inpainted Gaussians Novel Views
Scene
Outpainting
Sparse View
Reconstruction
Fig.1. Thisworkpresentsanapproachfor3Dinpaintingbasedonthedistillationofthe2Dgenerativepriorsfromreferenceadapteddiffusionmodels.Left
Givenascenerepresentedusing3DGaussiansplattingandtheuser-definedmask,weusethescoredistillationobjectivebasedonapersonalizeddiffusion
modeltoinpaintthemissingcontentin3D.RightOurmethodisgeneralandcanbe,withoutanychanges,appliedtoothereditingtaskssuchasobject
insertion,outpainting,andsparseviewreconstruction.Pleasevisitourprojectpage.
Neuralreconstructionapproachesarerapidlyemergingasthepreferred generalityofourformulationonotherdownstreamtaskssuchasobject
representationfor3Dscenes,buttheirlimitededitabilityisstillposinga insertion,sceneoutpainting,andsparseviewreconstruction.
challenge.Inthiswork,weproposeanapproachfor3Dsceneinpaintingâ€”the
taskofcoherentlyreplacingpartsofthereconstructedscenewithdesired
content.Sceneinpaintingisaninherentlyill-posedtaskasthereexistmany
1 INTRODUCTION
solutionsthatplausiblyreplacethemissingcontent.Agoodinpainting
methodshouldthereforenotonlyenablehigh-qualitysynthesisbutalso Neuralreconstructionmethodsenableseamlessreconstructionof
ahighdegreeofcontrol.Basedonthisobservation,wefocusonenabling 3Dscenesfromasetofposedimages.Theirsimpleformulation,
explicitcontrolovertheinpaintedcontentandleverageareferenceimageas highvisualfidelity,andincreasinglyfastrenderingmakethemthe
anefficientmeanstoachievethisgoal.Specifically,weintroduceRefFusion, preferred representation for a variety of use cases from AR/VR
anovel3Dinpaintingmethodbasedonamulti-scalepersonalizationofan
applicationstoroboticssimulation.Whilethesemethodsenable
imageinpaintingdiffusionmodeltothegivenreferenceview.Thepersonal-
novel-viewsynthesis,theyarestillinherentlylimitedtothecontent
izationeffectivelyadaptsthepriordistributiontothetargetscene,resulting
capturedinthetrainingdata.Yet,tomakethemusefulinpractice,
inalowervarianceofscoredistillationobjectiveandhencesignificantly
itiscriticaltoimpartthemwitheditability.Oneofthekeydesirable
sharperdetails.Ourframeworkachievesstate-of-the-artresultsforobject
removalwhilemaintaininghighcontrollability.Wefurtherdemonstratethe manipulationoperationsistheabilitytoremovepartsofthescene
andsubstitutethemwithsomedesiredcontent.Thistask,generally
knownas3Dinpainting,involvessynthesizingplausiblecontent
Authorsâ€™addresses:AshkanMirzaei,NVIDIA,UniversityofToronto,Canada,ashkan@ thatcoherentlyblendswiththerestofthescenewhenviewedfrom
cs.toronto.edu;RiccardodeLutio,NVIDIA,USA,rdelutio@nvidia.com;SeungWook
Kim,NVIDIA,SouthKorea,seungwookk@nvidia.com;DavidAcuna,NVIDIA,Canada, anyangle.Inpaintingisaninherentlyill-posedproblemasthere
dacunamarrer@nvidia.com;JonathanKelly,UniversityofToronto,Canada,jkelly@ oftenexistmultipleviableapproachestocompletethescene.Agood
utias.utoronto.ca;SanjaFidler,NVIDIA,UniversityofToronto,Canada,sfidler@nvidia.
inpaintingmodelshouldthereforealsobecontrollable,suchthat
com;IgorGilitschenski,UniversityofToronto,Canada,igor@gilitschenski.org;Zan
Gojcic,NVIDIA,Switzerland,zan.gojcic@gmail.com. userscanchoosethesolutionthatbestfitstheirneeds.
4202
rpA
61
]VC.sc[
1v56701.4042:viXra2 â€¢ AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked Image LaMa SDXL Inpainting Ours #1 Ours #2
Fig.2. Comparisonof2Dimageinpaintingmethodsonmultipleviewsofthesamescene.LaMa[Suvorovetal.2022]yieldsrelativelyconsistentinpaintings
butlacksdetails.SDXL[Podelletal.2023]synthesizescontentwithhigh-quality,butlowmulti-viewconsistencyduetothehighdiversityofitsgenerations.
Bypersonalizingthediffusionmodeltothereferenceview,ourmethodachieveshigh-qualitygenerationswithsuperiormulti-viewconsistency.Ours#1and
Ours#2areadaptedtoSDXLoutputsshowninthesecondandthirdrowrespectively.
Withtheadventoflargediffusionmodels[Balajietal.2022;Rom- personalizationmethodforinpaintingdiffusionmodelsthatadapts
bachetal.2022a;Sahariaetal.2022a,b],2Dimageinpaintinghas themodeltothegivenreferenceview.Thiscontributionaloneen-
beengettingincreasinglyclosetoachievingtheseproperties.Unfor- ablesexplicitcontrolovertheinpaintedcontentandreducesthe
tunately,large-scale3Dgenerativemethodsstilllagfarbehind.This varianceofthescoredistillationobjective.ii)Weleveragetheex-
canbeaccreditedtothelackoflarge-scale3Ddatasetsandefficient plicitnatureof3DGaussiansplattingtoconsolidatenoisy2Dmasks
high-resolution3Ddatastructures.Asaconsequence,muchofthe anddirectthegradientsofdifferentlosstermstothepertinentre-
recentandconcurrentworkon3Dinpaintinghasresortedtolifting gions.iii)Weproposeacombinationofobjectivetermsthatenables
thepriorsof2Dinpaintingmodelsto3D.Suchliftingisperformed usingtheSDSoptimizationprocedureatthescenelevel.iv)We
eitherexplicitly,byindependentlyinpaintingoneormultiple2D proposeanewdatasetdesignedforevaluatingobjectremovaland
viewsandconsolidatingtheirmulti-viewinconsistenciesin3D[Liu 3Dinpainting,comprisingninesceneswithlargecameramotion.
etal.2022;Mirzaeietal.2023a,b;Wangetal.2023b].Alternatively, TheexperimentsinSection4demonstratethehighvisualquality,
itcanbeformulatedasacontinuousdistillationprocess[Pooleetal. controllability,anddiversityofourapproach.Inaddition,theappli-
2023]thatseeks3Dinpaintingsusinga2Ddiffusionmodelasa cationsinSection5showcasethegeneralityofourformulation.
priorforoptimization[Prabhuetal.2023].
Thesemethodsachievepromisingresultsbutmanychallenges 2 RELATEDWORK
remain:a)Trade-offbetweendiversityandmulti-viewconsistency.
2Dand3DInpaintingInpaintingistheprocessofreplacingmiss-
Deterministic2Dinpaintingmodels,e.g.[Suvorovetal.2022],yield
ingregionswithrealisticcontent.Forexample,in2Dthisinvolves
relatively multi-viewconsistentinpaintings thatcandirectly be
producingplausiblevaluesformissingpixelsofanimage.Asan
liftedto3Dusingperceptuallosses[Mirzaeietal.2023b;Wangetal.
inherentlygenerativetask,advancementsofgenerativemodelshave
2023b].Yet,thiscomesatthecostofdiversityandlimitedvisual
ledtoincreasedperformanceof2Dinpainting.GenerativeAdversar-
quality.Prompt-basedinpaintingdiffusionmodelscansynthesize
ialNetwork(GAN)[Goodfellowetal.2014]-basedapproaches[Li
high-quality,diverse,andcontrollableinpaintings,butduetothe
etal.2022;Liuetal.2021;Suvorovetal.2022;Yuetal.2019;Zhao
looseconstraintoftextguidance,theirinpaintingsarehighlymulti-
etal.2021;Zhengetal.2022]learntohallucinatethemissingpixels
viewinconsistentandthusdifficulttoliftto3D(seeFigure2).b)
byplayinganadversarialgamebetweenanimageinpainterand
Maintainingfidelitytotheobservedcontent.Byinpaintingthe3D
a discriminator. Recently, diffusion model (DM) [Ho et al. 2020;
contentusing(inconsistent)2Dmasks,thesemethodsignorethat
Sohl-Dicksteinetal.2015;Songetal.2021]-basedinpaintingmod-
themissingcontentin3Dispotentiallysmallerthanthatnaively
els[Avrahamietal.2022;Lugmayretal.2022;Mengetal.2022;
definedbythe2Dmasks.Forexample,inthesceneshowninFigure3,
Rombachetal.2022a;Xieetal.2023]haveachievedstate-of-the-art
thecontentbehindthevaseisobservedinotherviewsandtherefore
results.Theseoperatebygraduallyperturbingacleanimagetowards
doesnotneedtobesynthesized.Finally,c)Conflictinggradients.The
randomnoisewhiletrainingadenoisingnetworktoreconstructthe
inconsistenciesacrosstheinpaintedviewsmayleadtoconflicting
image,conditionedonitsmaskedversion.
gradients,whichinturnresultinsmoothed-outinpaintings.
In3Dinpainting,thegoalistosynthesizeplausiblecontentfor
Toaddresstheabovechallenges,weproposeRefFusion,anovel
themissingregionsin3Dscenes.Thisisasignificantlyhardertask
3Dinpaintingmethodbasedoncontinuousdistillationofareference
asthegeneratedcontentneedstobeconsistentacrossvariousviews.
adapteddiffusionmodel.Inparticular:i)Weproposeamulti-scale
Although3DgenerativemodelshaverecentlygatheredincreasedRefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting â€¢ 3
3 METHOD
interest[Bautistaetal.2022;Kalischeketal.2022;Kimetal.2023;
Nicholetal.2022;Zengetal.2022],theirqualityisstilllimitedby Ourmethodimprovesthedistillationof2DDMpriorsfor3Din-
thescarcityof3Dtrainingdataandthedifficultyofselectingan paintingofscenesrepresentedusingGaussiansplatting[Kerbletal.
appropriateunderlyingrepresentation.Therefore,mostexisting3D 2023].Tothisend,weadapta2DinpaintingDMbyemploying
inpaintingmodels[Haqueetal.2023;Liuetal.2022;Mirzaeietal. multi-scalecropsderivedfromareferenceimage(Section3.3).The
2023a,b;Prabhuetal.2023;Wangetal.2023b;Weberetal.2023; adaptationlargelyreducesthevarianceofthescoredistillationob-
Wederetal.2023]stillrelyonliftingthepriorsfrom2Dinpainting jectiveandremovestheneedfortextguidance.Moreover,itallows
modelsto3D.Inparticular,theyfirstinpaintmaskedinputimages ustointroduceamulti-scalescoredistillationobjective(Section3.4)
andcomplementthereconstructionobjectivewithregularization thatencompassesbothglobalcontextandlocaldetails.Toguidethe
techniquestolessenthemulti-viewinconsistencies[Liuetal.2022; supervisionofthereconstructiontothepertinentregions,welever-
Mirzaeietal.2023a,b;Wangetal.2023b;Wederetal.2023].The agetheexplicitnatureoftheGaussianrepresentation(Section3.5).
problemofinconsistencyisamplifiedwhenusingrecentDM-based ImportantpreliminariesaresummarizedinSection3.1,whilethe
inpaintingmodelswithincreaseddiversity.Weaddressthechallenge implementationdetailsareprovidedinSection3.7.
withthescoredistillationobjective[Pooleetal.2023]asfollows.
Distilling2DDiffusionModelPriorsScoreDistillationSampling 3.1 Preliminaries
(SDS),firstintroducedby[Pooleetal.2023],hasrecentlybeenused
DiffusionModelsareafamilyofgenerativemodelsknownfor
togeneraterealistic3Dobjects[Chenetal.2023;Liangetal.2023;
theirabilitytotransformsamplesfromatractabledistribution,typi-
Linetal.2023;Wangetal.2023a]andeven4Dscenes[Lingetal. callyaGaussian,towardsthetargetdatadistributionğ‘(x)[Hoetal.
2023;Renetal.2023;Zhengetal.2023]bydistillingthepriorsof
2020;Sohl-Dicksteinetal.2015].Thesemodelsarebuiltaround
text-to-imageDMs[Balajietal.2022;Daietal.2023;Rombachetal.
twokeyprocesses.Aforwardprocess,whichgraduallyremoves
2022a;Sahariaetal.2022b].TheSDSformulationfor3Dscenes thestructurefromthedatasamplesxâˆ¼ğ‘(x)byaddingnoise.And,
worksbybackpropagatinggradientsfromaDMâ€™sdenoisertothe
a reverse process, which slowly removes this noise and reintro-
underlyingscenerepresentationsothattherenderingslookrealistic.
ducesthestructureintoanintermediatelatentvariabledenotedas
Byrunningthisoptimizationprocessovermanycameraviewsthis xğ‘¡ =ğ›¼ ğ‘¡x+ğœ ğ‘¡ğ,ğ âˆ¼N(0,ğ‘°).Inthiscontext,ğ›¼ ğ‘¡ andğœ ğ‘¡ representpre-
resultsina3Dconsistentscenerepresentation. determinednoiseschedules,andthevariableğ‘¡ denotesthetimestep
Inthispaper,weproposetofollowthisparadigmanduseapre-
withahighervalueindicatingagreateramountofnoise.There-
trained2DinpaintingDMasapriortoguidetheoptimizationpro-
verseprocessistypicallyparameterizedbyaconditionalneural
cessusingSDS.ConcurrentworksInpaint3D[Prabhuetal.2023]
networkğ thatistrainedtopredictthenoiseğusingthefollowing
ğœ½
andNeRFiller[Weberetal.2023]operateonasimilarscheme,either
simplifiedobjective[Hoetal.2020]:
byusingtheSDSobjectivedirectlyoritsiterativedatasetupdate
formulationproposedbyIN2N[Haqueetal.2023].Wefindthat E xâˆ¼ğ‘(x),ğâˆ¼N(0,ğ‘°),ğ‘¡âˆ¼ğ‘‡[ğ‘¤(ğ‘¡)||ğ ğœ½(xğ‘¡,ğ‘¡,c)âˆ’ğ||2 2], (1)
personalizingtheDMtothetargetsceneandreferenceimageiskey whereğ’„representsacondition(e.g.text,image,etc.)thatallowscon-
toachievingsharpresultsandusercontrollability. trollingthegenerationprocess,ğ‘¤(ğ‘¡)representsatime-conditional
Personalizing Diffusion Models Adapting large-scale text-to- weighting,andğ‘‡ isasetcontainingaselectionoftimesteps.
imageDMstogenerateuser-specifiedcontentoffersanefficientway Latent Diffusion Models (LDMs), improve computational and
forobtainingahigh-qualitypersonalizedgenerativemodel.This memoryefficiencyovertraditionalDMsbyperformingthediffusion
principlehasbeensuccessfullyappliedtomanyapplications,suchas processinalowerdimensionallatentspace[Rombachetal.2022a].
preservingtheidentityofanobject[Ruizetal.2023]orrenderinga Thisdimensionalityreductionisachievedbyemployingapretrained
chromeballtoestimatelighting[Phongthaweeetal.2023].Different encoderâ€“decoderarchitecture,wheretheencoderEmapssamples
personalizationapproacheshavebeenproposed;DreamBooth[Ruiz from the data distribution x âˆ¼ ğ‘(x) into a latent space Z. The
etal.2023]finetunesaDMandTextualinversion[Galetal.2022] decoderDperformstheinverseoperation,suchthatD(E(ğ’™))â‰ˆğ’™.
optimizesanewwordembeddingthatcangeneratetargetsthrough InLDMs,theDMoperatesonZ,thereforeğ’™inEquation1isreplaced
apretrainedDM.LoRA[Huetal.2022;Shahetal.2023],originally byitslatentrepresentationğ’›:=E(ğ’™).
proposedforlanguagemodels,injectstrainablerank-decomposed ScoreDistillationSamplingoriginallyproposedby[Pooleetal.
matricesintofrozenDMsforparameter-efficientpersonalization. 2023],leveragesapretrainedDMtoguidetheoptimizationofadif-
RealFill[Tangetal.2023]and[Charietal.2023]personalizeboth ferentiable,parametricimage-renderingfunctionğ‘” ğ“(ğ…):=ğ’™,where
thecontextencoderandDMfortargetimages. ğ… denotesthecameraposefromwhichtheimageğ’™ isrendered.
PersonalizedDMshavebeenpreviouslyusedfor2Dto3Ddistil- Specifically,theparametersğ“areupdatedusingthegradient:
l ea tti ao ln .. 2P 02ro 3l ]ifi uc sD er Le oam RAer t[ oW la en arg net anal. e2 v0 o2 l3 va in] gan md oD dr ee la am nC dr ca oft m3D pu[ tS eun a âˆ‡ ğ“L SDS(ğ“,ğœ½):=E ğâˆ¼N(0,ğ‘°),ğ‘¡âˆ¼ğ‘‡[ğ‘¤(ğ‘¡)(ğË† ğœ½(ğ’›ğ‘¡,ğ‘¡,ğ’„)âˆ’ğ)ğœ• ğœ•ğ’› ğ“ğ‘¡ ], (2)
modifiedSDSobjectivefortext-to-3Dgeneration.Similarly,weper-
sonalize a pretrained DM using LoRA, but we propose a robust
whereğ’›=E(ğ‘” ğ“(ğ…))and
pipelinetailoredfor3DinpaintingtoefficientlypersonalizetheDM
onaninpaintedreferenceviewanduseittoinpaintasceneby ğË† ğœ½(ğ’›ğ‘¡,ğ‘¡,ğ’„):=(1+ğ›¼)ğ ğœ½(ğ’›ğ‘¡,ğ‘¡,ğ’„)âˆ’ğ›¼ğ ğœ½(ğ’›ğ‘¡,ğ‘¡,âˆ…). (3)
optimizinganSDSobjective. Here,ğË† denotestheclassifierfreeguidance(CFG)version[Ho
ğœ½
andSalimans2021]ofğ usedintext-conditionedDMstoenable
ğœ½4 â€¢ AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Inputs LoRA Adaptation 3D Inpainting
Training Views
Dataset of
Refer Cen roc pe s View skâ€œ sA [, p ch roo pto p eo df ] â€ R Tra an indo inm g InR pu an in 2 tiD n g Monodepth
View
Masks
Inpainting
DM
LoRA Adapted
Global Crops Inpainting DM
(Pretrained)
Reference View Noise â€œA photo of
sks[, cropped]â€
Masked
Gaussians Discriminator
Local Crops
Fig.3. Overviewoftheproposedapproach.RefFusiontakestrainingviews,masks,andthereferenceviewasinput(left).WeadapttheinpaintingLDMon
boththeglobalandlocalcropsofthereferenceview(middle).Then,wedistillthepriorsoftheadaptedLDMtothescene(right)byminimizingtheSDS
objective.Additionally,weuseadiscriminatorlosstomitigatepotentialartifactsinappearanceandadepthlosstoenhancegeometry.WetrackGaussians
representingthemaskedandunmaskedregions,andbackpropagatethegradientsofindividualtermsonlytothepertinentregions.
higherqualitygenerationviaaguidancescaleparameterğ›¼.Intu- optimizationproblem.Asaresult,theoutputsproducedbythis
itively, CFG aims to trade diversity with quality and is a linear procedureoftenexhibitalackofdetailanddiversity.Inaddition,
combinationoftwoğ terms,withtheconditionomittedinthe DMsaretypicallyguidedusingtextprompts,whicharenotsuited
ğœ½
latter(representedherebythenullsymbolâˆ…). todescribelarge-scalescenesandthereforeonlyprovideweakreg-
PersonalizationandParameterEfficientFinetuningDMscan ularization.Tomitigatetheseproblems,weproposetopersonalize
bepersonalizedtogenerateimagesalignedwithaparticularconcept, theDMbasedonasingle(ormultiple)referenceimageofthescene
subject,orstyle[Ruizetal.2023;Shahetal.2023],byfinetuning beforedistillingitspriorsto3D.Thispersonalizationadaptsthe
theirweightsonaselectnumberofimages.However,updatingall DMtowardsthereferenceimageandalleviatestheneedfortext
theparametersofapretrainedDMiscomputationallyexpensive. guidanceduringthedistillationprocess.
Instead,parameter-efficientfinetuningmethodslikeLoRA[Huetal. LetIğ‘Ÿ
âˆˆRâ„Ã—ğ‘¤Ã—3denoteareferenceimage,whichcanbegener-
2022],injecttrainablelow-rankdecompositionmatricesandaimto atedbyaninpaintingmodeloranactualimageofthescene.Weuse
learnonlythevariationsfromthepretrainedweights. LoRAfinetuningtopersonalizeapretrainedinpaintingLDMtoIğ‘Ÿ by
GaussianSplattingparameterizesthescenewithasetof3DGauss- minimizingtheobjectiveinEquation1.Specifically,ateachiteration,
ianparticlesG,whereeachparticleisrepresentedbyitsposition wefirstaugmentIğ‘Ÿ bycroppingoutarandomthinborderaroundit
ğ âˆˆR3,scaleğ’” âˆˆR3,rotationğ’“ âˆˆR4,opacityğœ âˆˆR,andspherical toobtainIğ‘ ğ‘Ÿ âˆˆRâ„â€²Ã—ğ‘¤â€²Ã—3,whereâ„â€² â‰¤â„andğ‘¤â€² â‰¤ğ‘¤.Wethensample
harmonicscoefficientsğœ· âˆˆR48.Gaussianparticlescanbeefficiently arandomrectangularmaskğ’âˆˆ{0,1}â„â€²Ã—ğ‘¤â€² thatmasksoutpartof
renderedusingthedifferentiablesplattingformulationproposed theimageandtasktheLDMtoinpaintthemissingcontent.The
in[Kerbletal.2023],andhence,optimizedfromasetofposedimages conditioningğ’„inEquation1isobtainedbyconcatenatingthelatent
usingareconstructionloss.Consequently,thesetofdifferentiable representationofthemaskedreferenceviewğ’›=E(ğ’â—¦Iğ‘ ğ‘Ÿ)andthe
parametersisdefinedasğ“ :={ğğ‘–,ğ’”ğ‘–,ğ’“ğ‘–,ğœ ğ‘–,ğœ·ğ‘–} ğ‘–| =G 1| . maskğ’,whereâ—¦denotestheelement-wiseproduct.Forthetextem-
3.2 ProblemSetup beddingğ’š=E text(ğ¶ ğ‘‡)weuseafixedpromptğ¶ ğ‘‡ ="Aphotoofsks".
WeapplyLoRAtotheattentionlayersofboththetextencoderE
text
Givena3Dreconstructionofasceneandthecorrespondingposed andtheU-Netdenoiserğ .
ğœ½
imagesthatgeneratedit,weinvestigate3Deditingtechniquesthat Inpractice,thenativeimageresolutionofourLDMis512Ã—512
enableausertoperformcertainmanipulations.Inthefollowing,we pixelswitha64Ã—64dimensionallatentspace.Suchlowresolution
detailourcontributionsonthetaskof3Dinpainting,beforeshow- is often too coarse to capture the high-frequency details of the
ingresultsonotherapplicationsthatcanbetackledwiththesame scene.Especially,asthegradientsoftheSDSlossarecomputed
formulationSection5.Inalltheexperiments,weadoptGaussian inthelow-resolutionlatentspaceandthenbackpropagatedtothe
splatting[Kerbletal.2023]asourunderlying3Drepresentation. input(cf.Equation2).Toaddressthischallenge,weproposeamulti-
However,ourapproachcanalsobeappliedtoother3Drepresenta- scalepersonalizationstrategywhereinthediffusioninpainteris
tionssuchasNeRFs[Mildenhalletal.2020]. additionallyfinetunedonlocal512Ã—512cropssampledaroundthe
3.3 Multi-ScalePersonalization maskedregionofIğ‘Ÿ.ToenabletheDMtodiscernvariationsbetween
localandglobalcropsweuseadistincttextprompt,ğ¶â€² ="Aphoto
Ourapproachinvolvesdistillingthelearnedpriorsfrom2Dgen- ğ‘‡
ofsks,cropped"fortheselocalcrops.
erativemodelsintothe3Ddomaintoprovideeffectivegenerative
guidance.Distillingpriorsfrom2DimageDMsinto3Dusingascore
distillationobjectiveiscommonlyrecognizedasamode-seeking
slexiP
deksamnURefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting â€¢ 5
3.4 Multi-ScaleSDSObjective
geometryandappearanceofthesynthesizedregion.Theoverall
Buildingonthemulti-scalepersonalization,weformulateamulti- objectiveis:
scaleSDSobjective: L:=L rec+ğœ† SDSL SDS+ğœ† depthL depth+ğœ† advL adv, (5)
L SDS:=L Sg Dlo Sbal+L Slo Dc Sal. (4) whereğœ† âˆ— >0balancetherelativecontributionofeachlosstermand
L andL denotetheadversarialanddepthloss,respectively.
adv depth
Specifically,consideranimageË†Iğ‘–renderedfromarandomcamera
Depth Regularization We regularize the depth using the out-
posefromthetrainingsetalongwithitsmask,ğ’ğ‘–.Then,âˆ‡Lglobal is putsofamonoculardepthestimationmodel.Inparticular,after
SDS
computedusingthewholeË†Iğ‘– andthetextguidance"Aphotoofsks", everyğ‘ depth iterations,werenderanimageË†Iğ‘– fromarandomly
whileL Slo Dc SalconsidersonlyarandompatchË†I ğ‘–ğ‘ aroundthemaskedre- selectedcameraposeinthetrainingset.WethenmaskË†Iğ‘– withits
gionofË†Iğ‘–andisguidedusingthetextprompt"Aphotoofsks,cropped". correspondingmaskğ’ğ‘– andinpaintitusingouradaptedinpaint-
Suchmulti-scaleformulationallowsustobalanceglobalcontext
ingmodelstartingfromarandomtimestepğ‘¡
depth
âˆˆğ‘‡.Thisprocess
withlocaldetailsevenatthelowresolutionoftheDMâ€™slatentspace.
yieldsaninpaintedviewË†Iinpaintthatwefeedintoamonoculardepth
estimationmodeltoobtainthedepthmapğ‘‘Ëœ.
3.5 SplittingtheGaussiansintoMasked/UnmaskedSets Note thatğ‘‘Ëœ is only relative and ambiguous in terms of scale
Weassumethattheregionofthescenethatshouldberemovedand andoffset.Tofixtheambiguity,weoptimizeascaleğ‘ andoffsetğ‘œ
inpaintediseithermaskeddirectlyin3Dorprovidedintheformof tominimizetheğ¿2errorbetweentherendereddepth,ğ‘‘Ë†,andthe
2Dmasksforeachtrainingview.Thelattercanbegeneratedusing aligneddepthğ‘‘Â¯=ğ‘ ğ‘‘Ëœ+ğ‘œforthesetofunmaskedpixelsğ‘ƒ .
unmasked
anoff-the-shelf2Dsegmentationmethodorprovidedbytheuser. Finally,weemploythefastbilateralsolver[BarronandPoole2016]
Assuch2Dmaskscanbeinconsistentin3D,weproposeasimple toreducepotentialremainingmisalignmentsandcalculateL
depth
heuristicthatconsolidatesthem. onthemaskedpixels,ğ‘ƒ ,as:
masked
3DConsistentMasksToconsolidateinconsistent2Dmasks,we
1 âˆ‘ï¸
firstdetermineifeachGaussianparticleshouldbemaskedorun- L depth:= |ğ‘ƒ | âˆ¥ğ‘‘Ë†(ğ‘)âˆ’ğ‘‘Â¯(ğ‘)âˆ¥2 2. (6)
masked,bycountinghowoftenitcontributestothevolumerender- masked ğ‘âˆˆğ‘ƒ masked
ingofmaskedandunmaskedpixelsacrossthetrainingviews.Ifthe AdverserialObjectiveTomitigateanycolormismatchesandar-
ratiomasked/unmaskedisaboveathresholdğœ maskthe3Dparticle tifactsontheboundaryofthehallucinatedregion,weemploya
islabeledasmasked.Afterparticlelabelling,werasterizethemback discriminatorD ğƒ parameterizedbyğƒ.Thisdiscriminatoristrained
tothetrainingviewsandthresholdtherenderedsemanticvalues todifferentiatebetweenrealpatchesthataresampledaroundthe
usingğœ mâ€² asked,producingasetof3Dconsistent2Dbinarymasks. maskedregionofthereferenceviewIğ‘Ÿandthefakepatchesrendered
ConfiningtheLossFunctionstoPertinentRegionsDuringthe fromğ‘” ğ“(ğ…)aroundthemaskofthetrainingviews.Wepropagate
optimization,weusetheper-Gaussianmaskstodirectthegradients thegradientsoftheadversariallosstoallsphericalharmonicscoeffi-
ofindividuallossterms.Specifically,thereconstructionlossfromun- cientsğœ·ğ‘– âˆˆğ“,whilekeepingotherparametersfixed.FollowingGaN-
maskedpixelsonlyupdatestheparametersofunmaskedGaussians, eRF[Roessleetal.2023],weemploythefollowingregularizedver-
whereasthegradientsfromotherlosstermsareonlypropagated sionoftheGANlossL advwithanğ‘… 1gradientpenalty[Mescheder
tothemaskedGaussians.Thisdifferentiationintomaskedandun- etal.2018]onD ğƒ,controlledbyabalancingscalarğœ† gp:
maskedGaussiansiscrucialtopreventguidancelosses(e.g.,SDS
loss)fromundesirablyinfluencingtheregionsthatdonotrequire minğœ·maxğœ‰E(cid:104) ğ‘“(Dğœ‰(Ë†Iğ‘ƒ fake))+ğ‘“(âˆ’Dğœ‰(Iğ‘ƒ real))âˆ’ğœ† gpâˆ¥âˆ‡Dğœ‰(Ë†Iğ‘ƒ fake)âˆ¥2 2(cid:105) , (7)
hallucination.But,italsorequiresustoadaptthedensificationand
whereğ‘“(ğ‘¥):=âˆ’log(1+exp(âˆ’ğ‘¥)),andË†Iğ‘ƒ andIğ‘ƒ
correspondto
pruningheuristicsproposedin[Kerbletal.2023].Specifically,if fake real
thesampledfakeandrealpatches,respectively.
aGaussianissplitorclonedwetransferitsmaskvaluetoitschil-
dren.Additionally,ifaGaussiantransitionsbetweentheregions,
3.7 ImplementationDetails
wepruneittomaintainregion-specificfidelity.
Reference-GuidedInitializationAtthestartoftheinpainting Forunprojectionandreprojectionofthemasks,wesetğœ maskedand
process, we remove Gaussians in the masked scene region and
ğœâ€² to1and0.3,respectively.Ineachiteration,wecompute
masked
replacethemusingthereferenceviewIğ‘Ÿ.Specifically,wefirstpredict Lglobal usinganimagerenderedfromarandomcameraposefrom
thedepthğ‘‘Ëœ ğ‘Ÿ ofthereferenceimageIğ‘Ÿ.Wethencompensatethe thS eD tS rainingset,whileaveragingtheLlocal acrosstwo512Ã—512
scaleandoffsetambiguitiestoderivethealigneddepthğ‘‘Ë† ğ‘Ÿ (similar patchessampledfromtheboundingboxS oD fS themaskedregion.Fol-
towhatwillfollowinourdepthregularization)andunprojectit lowingGaussiansplatting[Kerbletal.2023],weresizeeachtraining
to3D.Weempiricallyobservethatreference-guidedinitialization viewtohavethelargersideoftheimagesbe1600pixels.Thedepth
outperformsrandominitializationTable3. lossiscalculatedevery8thiteration.Fortheadversarialloss,we
sample64realand64fake64Ã—64patcheseveryiteration.Thearchi-
3.6 LossesandTraining
tectureofthediscriminatorfollowsStyleGAN2[Karrasetal.2020],
WetrainourmethodusingthecombinationofthemultiscaleSDSob- andthehyperparametersoftheadversariallossareconsistentwith
jectiveSection3.4andthereconstructionlossL rec(ğ¿1andD-SSIM). GANeRF[Roessleetal.2023].Thelossweightsofindividualloss
Additionally,weemploytworegularizationtermsthatimprovethe termsaresettoğœ† SDS := 0.001,ğœ† depth := 0.0625,andğœ† adv := 0.03,6 â€¢ AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Table1. QuantitativeevaluationofobjectremovalonSPIn-NeRFdataset.
respectively.WeadopttheGaussianoptimizationparametersfrom
[Kerbletal.2023],butchangethedensificationandcloningheuristic Method LPIPSâ†“
topreventoverdensificationofGaussiansduetolargergradientsof
NeRF+LaMa(2D) 0.5369
theSDSloss.TogeneratetherefenceviewsweuseSDXL[Podell
ObjectNeRF[Yangetal.2021] 0.6829
etal.2023]inpaintingmodelduetoitshigherresolutionandqual-
MaskedNeRF[Mildenhalletal.2020] 0.6030
ity.However,tospeedupthedistillation,wepersonalizeStable-
NeRF-In[Liuetal.2022] 0.4884
Diffusion-2-Inpainting[Rombachetal.2022b]astheadaptedLDM.
SPIn-NeRF-SD[Mirzaeietal.2023b] 0.5701
ThesameLoRAoptimizationparametersasRealFill[Tangetal.
SPIn-NeRF-LaMa[Mirzaeietal.2023b] 0.4654
2023]areemployed;wetraintheDMfor2000iterations,withLoRA
Inpaint3D[Prabhuetal.2023] 0.5150
ranksetto8,ğ›¼ setto16,resolutionsetto512,andlearningrates
Reference-guidedNeRF[Mirzaeietal.2023a](SDV2) 0.4532
2ğ‘’âˆ’4and4ğ‘’âˆ’5fortheU-Netandthetextencoder,respectively.We
Reference-guidedNeRF[Mirzaeietal.2023a](SDXL) 0.4453
settheLoRAdropoutto0.1.NotethattheadaptedLDMexpects
Ours 0.4283
512Ã—512images;thus,forcalculatingtheglobalSDSloss,wefirst
bilinearlydownsampletherenderedviewsto512Ã—512. Table2. UserstudyofobjectremovalonSPIn-NeRFdataset.Foreach
methodwereportthepercentageofratersthatpereferreditoverours.
4 EXPERIMENTS
Method Quality(%) Removal(%)
DatasetandMetricsFollowingrelatedwork,weperformmost
SPIn-NeRF-LaMa 18.38 29.32
experiments on the SPIn-NeRF [Mirzaei et al. 2023b] dataset. It
Inpaint3D 11.87 11.52
consistsof10scenesoriginallydesignedforobjectremovalevalua-
Reference-guidedNeRF(SDXL) 23.64 43.65
tion.Eachsceneinthedatasetincludes60imageswithanunwanted
object(trainingviews)and40imageswithoutit(testviews).Human-
annotatedmasksoftheobjectregionareavailableforbothtraining BaselinesWecompareRefFusiontotwonaivebaselines,Masked
andtestviews.Formoreinformationaboutthedatasetpleaserefer NeRFandNeRF+LaMa.Intheformer,thereconstructionlossisonly
to[Mirzaeietal.2023b].Toperformaquantitativecomparisonofour calculatedontheunmaskedpixels,whilethelatterusesLaMa[Su-
methodtotheselectedbaselines,weuse40ground-truthimagesof vorov et al. 2022] to independently inpaint rendered views. We
eachscenewithouttheobject.Specifically,werendercorresponding furthercomparetoawiderangeofexisting3Dinpaintingmeth-
viewsfromeachmodelandcomputetheaveragelearnedperceptual ods:ObjectNeRF[Yangetal.2021],NeRF-In[Liuetal.2022],SPIn-
imagepatchsimilarity(LPIPS)[Zhangetal.2018]1.Inlinewiththe NeRF[Mirzaeietal.2023b],andconcurrentworkInpaint3D[Prabhu
relatedwork[Mirzaeietal.2023b]wecalculatethemetricsaround etal.2023].Forallthebaselines,weusedthesourcecodeorthe
themaskedregionbyconsideringtheboundingboxofthemask renderedimagesprovidedbytheauthorsoftherespectiveworks.
anddilatingtheboxineverydirectionby10%.Toaddressthelimi- ObjectRemovalWefirstprovideevaluationsbasedonthestan-
tationsofSPIn-NeRFdatasetindemonstratingmethodbehaviors dard3Dinpaintingbenchmark,theSPIn-NeRFdataset[Mirzaeietal.
onsceneswithmoresignificantcameramotionforinpainting,we 2023b].Toensureafaircomparison,weusethehuman-annotated
introduceourdatasetcomprisingninescenes.Thisdatasetencom- masks from the SPIn-NeRFdataset for all methods. As depicted
passeswide-baselineforward-facingscenesaswellas360-degree inTable1ourmethodsurpassesallbaselinesintermsofLPIPS.Fur-
scenes.Foreachscene,ourdatasetincludesimagesbothwithand thermore,wealsocomparefavourablyinaqualitativecomparison
withoutunwantedobjects,alongwithhuman-annotatedmasksfor asseeninFigure4.NotethattheperceptuallossusedinSPIn-NeRF
everyscene. doesnâ€™tfullyresolvethemulti-viewinconsistenciesintroducedby
WefurtherperformedauserstudyusingAmazonMechanical inpaintingeachtrainingviewseparately,thereforeresultinginvi-
Turk.Specifically,raterswereshownvideosdepictingtheresultsof sualartifacts.Reference-guidedNeRFperformswellintheremoval
twomethodsrenderedfromthesamenoveltrajectories(ourmethod region,howeverthequalityoftheoverallreconstructionquality
andabaselineinrandomorder),alongwiththeinputvideoand suffers.Finally,Inpaint3Dalsodistillsapretrained2Dinpainting
imageshighlightingtheunwantedobject.Raterswerethenaskedto DMusinganSDSobjective,howeverthelackofpersonalization
voteforthemethodthatproducedthebestvideointermsofoverall resultsinafuzzyinpainting.Wesummarizetheresultsoftheuser
qualityandbestobjectremoval.Toimprovethequalityofresponses, studyinTable2,RefFusionoutperformsallbaselinesintermsof
weintroducedanattentionchecktask.Foreachscene,weadded overallqualityandobjectremoval.
aquestionwherethevideosdisplayedforbothmethodswerethe TheSPIn-NeRFdataset[Mirzaeietal.2023b]exclusivelycom-
same(outputsofourmodel)andaddedanoptionsuggestingthat prisessceneswithminimalcameramovements.Toremedythiscon-
bothofthevideosareidentical.Wesetathresholdof50%accuracy straint,weintroduceourdatasetfeaturingninedistinctsceneswith
ontheattentioncheckquestionstostrikeabalancebetweenthe broadercamerabaselines,includingwide-baselineforward-facing
quantityandqualityoftheresponses.Intotal,thisresultedin32 scenesand360-degreescenes.InFigure5,wepresentqualitative
userscomparingourmethodtothreebaselinesacross10scenesand comparisonsbetweentheoutcomesofourapproachandReference-
answering2questionsperexample. guidedNeRFappliedtoscenesfromtheMipNeRF360dataset[Barron
etal.2022](depictedinthefirstandsecondscenesoftheleftcol-
1Priorworkalsoreportsper-sceneFIDscore,butduetothesmallsizeofthedataset umn),aswellasourownscenes.Theresultsnotablyillustratethat
(40images)FIDvaluesareunrepresentative.WethusomitthemandonlyreportLPIPS. Reference-guidedNeRF,reliantsolelyonasingleinpaintedviewRefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting â€¢ 7
Masked Image SPIn-NeRF Reference-guided NeRF Inpaint3D Ours
Fig.4. QualitativeobjectremovalresultsontheSPIn-NeRFdataset.RefFusionconsistentlyoutperformsthebaselines,yieldingsharperreconstructionand
moreplausibleinpainting.
Table3. AblationstudyofobjectremovalonSPIn-NeRFdataset.
additionallyusetheGTviewsforsupervisionusingreconstruction
Method LPIPSâ†“ lossâ€”Ours(LoRA+recon).Figure6showsthatgenerativepriorscan
guidethereconstructioninasparseviewsetting2,especiallywhen
Oursw/opersonalization 0.5719
onlyasmallnumberofviewsareavailable.Indeed,Ours(LoRA+
Oursw/osplittingtheGaussians 0.5128
recon)consistentlyoutperforms3DGSintermsofLPIPSandPSNR.
Oursw/olocalSDS 0.5093
Figure11depictsaqualitativecomparisonwith3DGS.
Oursw/oreference-guidedinitialization 0.4680
ObjectInsertionFigure7illustratesthecapacityof RefFusionto
Oursw/oadversarialloss 0.4326
insertobjectsintoascene.Wedemonstratethatbyusingareference
Oursw/odepthloss 0.4299
viewwithanaddedobjectinthemaskedregion,obtainedusinga
Ours 0.4283
text-to-imageinpaintingdiffusionmodel.Ourmethodsucceedsin
distillingthespecifiedobjectintothescenewithhighvisualfidelity.
forprojectioninto3Dspace,encounterschallengesinextrapolating SceneOutpaintingInFigure8,wefollowtheprocedureproposed
informationfromthereferencetofurtherviews.Conversely,our in[Prabhuetal.2023]andgenerateinversemasksbyplacinga
methodconsistentlyyieldssharperandmoreplausibleinpainting sphereatafixeddistancealongtheopticalaxisandcheckingfor
resultswithoutvisualartifacts.Notethatinthethirdexample,we ray-sphereintersection.ExamplemaskisshowninFigure8(left).
removetwodistinctobjectsbyconcurrentlyincorporatingbothinto Giventhismask,wetaskourmethodtooutpaintthesceneusing
themasks. thesameformulationandhyperparametersusedforobjectremoval.
AblationonDesignChoicesWeperformablationstudiesonour Ourmethodcompletesthesceneinaplausiblemanner,butthe
keydesignchoicestohighlighttheirsignificance.Table3andFig- outpaintedregionslackvisualfidelityandhigh-frequencydetails.
ure10depictquantitativeandqualitativeresults.Thelargestdrop This hints that special treatment is required to obtain the same
inperformanceisobservedwhenremovingpersonalizationfrom qualityofresultswhenperformingoutpainting.
ourmethod.Thisconfirmsourintuitionthatreferenceadaptation
iscrucialforunlockingthepotentialofSDS-basedoptimizationat 6 CONCLUSIONS,LIMITATIONSANDFUTUREWORK
thescenelevel.Otherablationsshowthatremovinganycompo-
WeintroducedRefFusion,a3Dinpaintingframeworkthatachieves
nentsleadstoadverseeffectsonthefinaloutcomes.Whilewefound
state-of-the-artresults,whileofferingimprovedcontrollabilityover
thedepthlossnottobecrucialfortheobjectremovaltask,itstill
theinpaintedcontent.Weachievethisbypersonalizinganinpait-
contributestoasubtleimprovement.Moreover,itisparticularly
ningLDMtothegivenreferenceimageanddistillingtheadapted
beneficialforotherapplicationssuchasobjectinsertion.
diffusionpriortothe3DsceneWedemonstratethegeneralityofour
5 APPLICATIONS methodacrossadiversesetofdownstreamapplicationsincluding
objectinsertion,3Doutpainting,andsparseviewreconstruction.
In this section, we demonstrate the general applicability of our Despiteachievingnotableresults,therearemanyavenuestoim-
methodtovariousdownstream3Dapplications. proveRefFusion.Theprimarychallengeliesinremovinglarge
SparseViewReconstructionToinvestigatethebenefitsofour objectsthatsubstantiallycoverpartsofthereferenceimage.Fine-
methodforguidingthesparseviewreconstruction,weconsidera tuningthediffusionmodelonthetargetdatasetcouldhelpbridge
scenewithanunwantedoccluderwhereonlyasmallsetofcleanim- thisgap.Additionally,althoughLoRAimprovestheefficiencyofthe
agesfromthescene(GTviews)isavailable.Specifically,weconsider adaptationprocesstothetargetscene,thisprocessstillrequiresa
twosettings:a)weonlyusetheGTviewsforLoRAfinetuningand
donotusethemduringthereconstructionâ€”Ours(LoRA),orb)we 2Asimilarobservationwasconcurrentlyshownby[Wuetal.2023].8 â€¢ AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked Image Reference-guided NeRF Ours Masked Image Reference-guided NeRF Ours
Fig.5. Qualitativeobjectremovalresultsonsceneswithlargercameramovements(MipNeRF360dataset[Barronetal.2022]andscenesfromourproposed
dataset).RefFusionconsistentlyoutperformstheReference-guidedNeRF.
considerableamountoftime.Furtheradvancementsinparameter-
efficientfinetuningandpersonalizationofdiffusionmodelsareex-
pectedtofurtherspeedupandenhanceourapproach.
Finally,recentadvancesinmulti-viewawareandvideodiffusion
modelscouldsignificantlyenhancethemulti-viewqualityofour
results.Especially,astheremainingartifactsaremostlyvisiblewhen
thecameramoves.
REFERENCES
Fig.6. ResultsofthesparseviewreconstructiononSPIn-NeRFdataset.
OmriAvrahami,DaniLischinski,andOhadFried.2022.Blendeddiffusionfortext-driven
UsingthesparseGTviewsonlyforpersonalizationOurs(LoRA)already editingofnaturalimages.InCVPR.
yieldscompetitiveresults.Whencombinedwiththereconstructionloss YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,Qinsheng
Ours(LoRA+recon)consistentlyoutperforms3DGS[Kerbletal.2023], Zhang,KarstenKreis,MiikaAittala,TimoAila,SamuliLaine,BryanCatanzaro,
TeroKarras,andMing-YuLiu.2022.eDiff-I:Text-to-ImageDiffusionModelswith
showcasingthepotentialofgenerativepriorstoguide3Dreconstruction.
EnsembleofExpertDenoisers.arXiv(2022).
JonathanT.Barron,BenMildenhall,DorVerbin,PratulP.Srinivasan,andPeterHedman.
Masked View Reference View Novel View #1 Novel View #2 2022.Mip-NeRF360:UnboundedAnti-AliasedNeuralRadianceFields.CVPR(2022).
JonathanTBarronandBenPoole.2016.Thefastbilateralsolver.InECCV.
MiguelÃngelBautista,PengshengGuo,SamiraAbnar,WalterTalbott,AlexanderT
Toshev,ZhuoyuanChen,LaurentDinh,ShuangfeiZhai,HanlinGoh,DanielUlbricht,
AfshinDehghan,andJoshuaM.Susskind.2022. GAUDI:ANeuralArchitectfor
Immersive3DSceneGeneration.InNeurIPS.
PradyumnaChari,SizhuoMa,DaniilOstashev,AchutaKadambi,GurunandanKrishnan,
JianWang,andKfirAberman.2023.PersonalizedRestorationviaDual-PivotTuning.
arXiv(2023).
RuiChen,YongweiChen,NingxinJiao,andKuiJia.2023.Fantasia3D:Disentangling
GeometryandAppearanceforHigh-qualityText-to-3DContentCreation.InICCV.
XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,Peizhao
Zhang,SimonVandenhende,XiaofangWang,AbhimanyuDubey,MatthewYu,
Fig.7. Sampleobjectinsertionresults. AbhishekKadian,FilipRadenovic,DhruvMahajan,KunpengLi,YueZhao,Vladan
Petrovic,MiteshKumarSingh,SimranMotwani,YiWen,YiwenSong,Roshan
Masked View Reference View Novel View
Sumbaly,VigneshRamanathan,ZijianHe,PeterVajda,andDeviParikh.2023.Emu:
EnhancingImageGenerationModelsUsingPhotogenicNeedlesinaHaystack.arXiv
(2023).
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,
andDanielCohen-Or.2022. Animageisworthoneword:Personalizingtext-to-
imagegenerationusingtextualinversion.arXiv(2022).
Fig.8. Ourapproachiscapableofoutpaintingscenesbyinvertedmasks.RefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting â€¢ 9
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,Sherjil RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
Ozair,AaronCourville,andYoshuaBengio.2014. Generativeadversarialnets. 2022b.High-ResolutionImageSynthesisWithLatentDiffusionModels.InCVPR.
NeurIPS(2014). NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Aberman.2023. DreamBooth:FineTuningText-to-imageDiffusionModelsfor
Kanazawa.2023. Instruct-NeRF2NeRF:Editing3DSceneswithInstructions.In Subject-DrivenGeneration.(2023).
ICCV. ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,
JonathanHo,AjayJain,andPieterAbbeel.2020. DenoisingDiffusionProbabilistic DavidFleet,andMohammadNorouzi.2022a. Palette:Image-to-imagediffusion
Models.InNeurIPS. models.InSIGGRAPH.
JonathanHoandTimSalimans.2021. Classifier-freediffusionguidance. NeurIPS ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,
Workshops(2021). SeyedKamyarSeyedGhasemipour,RaphaelGontijo-Lopes,BurcuKaragolAyan,
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, TimSalimans,JonathanHo,DavidJ.Fleet,andMohammadNorouzi.2022b.Photo-
LuWang,andWeizhuChen.2022.LoRA:Low-rankadaptationoflargelanguage realisticText-to-ImageDiffusionModelswithDeepLanguageUnderstanding.In
models.ICLR(2022). NeurIPS.
NikolaiKalischek,TorbenPeters,JanD.Wegner,andKonradSchindler.2022.Tetrahe- VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,and
dralDiffusionModelsfor3DShapeGeneration.arXiv(2022). VarunJampani.2023.ZipLoRA:AnySubjectinAnyStylebyEffectivelyMerging
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimo LoRAs.arXiv(2023).
Aila.2020.AnalyzingandImprovingtheImageQualityofStyleGAN.InCVPR. JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015.
BernhardKerbl,GeorgiosKopanas,ThomasLeimkÃ¼hler,andGeorgeDrettakis.2023. DeepUnsupervisedLearningusingNonequilibriumThermodynamics.InICML.
3DGaussianSplattingforReal-TimeRadianceFieldRendering.ToG(2023). YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoEr-
SeungWookKim,BradleyBrown,KangxueYin,KarstenKreis,KatjaSchwarz,Daiqing mon,andBenPoole.2021.Score-BasedGenerativeModelingthroughStochastic
Li,RobinRombach,AntonioTorralba,andSanjaFidler.2023. NeuralField-LDM: DifferentialEquations.InICLR.
SceneGenerationwithHierarchicalLatentDiffusionModels.InCVPR. JingxiangSun,BoZhang,RuizhiShao,LizhenWang,WenLiu,ZhendaXie,andYebin
WenboLi,ZheLin,KunZhou,LuQi,YiWang,andJiayaJia.2022.Mat:Mask-aware Liu.2023.Dreamcraft3d:Hierarchical3dgenerationwithbootstrappeddiffusion
transformerforlargeholeimageinpainting.InCVPR. prior.arXiv(2023).
YixunLiang,XinYang,JiantaoLin,HaodongLi,XiaogangXu,andYingcongChen. RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,Arsenii
2023. LucidDreamer:TowardsHigh-FidelityText-to-3DGenerationviaInterval Ashukha,AlekseiSilvestrov,NaejinKong,HarshithGoka,KiwoongPark,andVictor
ScoreMatching.arXiv(2023). Lempitsky.2022.Resolution-robustlargemaskinpaintingwithfourierconvolutions.
Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang, InWACV.
KarstenKreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin.2023.Magic3D:High- LumingTang,NatanielRuiz,ChuQinghao,YuanzhenLi,AleksanderHolynski,DavidE
ResolutionText-to-3DContentCreation.InCVPR. Jacobs,BharathHariharan,YaelPritch,NealWadhwa,KfirAberman,andMichael
HuanLing,SeungWookKim,AntonioTorralba,SanjaFidler,andKarstenKreis.2023. Rubinstein.2023.RealFill:Reference-DrivenGenerationforAuthenticImageCom-
AlignYourGaussians:Text-to-4DwithDynamic3DGaussiansandComposed pletion.arXiv(2023).
DiffusionModels.arXiv(2023). DongqingWang,TongZhang,AlaaAbboud,andSabineSÃ¼sstrunk.2023b. Inpaint-
HongyuLiu,ZiyuWan,WeiHuang,YibingSong,XintongHan,andJingLiao.2021. NeRF360:Text-Guided3DInpaintingonUnboundedNeuralRadianceFields.arXiv
Pd-gan:Probabilisticdiverseganforimageinpainting.InCVPR. (2023).
Hao-KangLiu,IShen,Bing-YuChen,etal.2022.NeRF-In:Free-formNeRFinpainting ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJun
withRGB-Dpriors.CGA(2022). Zhu.2023a.ProlificDreamer:High-FidelityandDiverseText-to-3DGenerationwith
AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,andLuc VariationalScoreDistillation.InNeurIPS.
VanGool.2022.Repaint:Inpaintingusingdenoisingdiffusionprobabilisticmodels. EthanWeber,AleksanderHoÅ‚yÅ„ski,VarunJampani,SaurabhSaxena,NoahSnavely,
InCVPR. AbhishekKar,andAngjooKanazawa.2023. NeRFiller:CompletingScenesvia
ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon. Generative3DInpainting.arXiv(2023).
2022. Sdedit:Imagesynthesisandeditingwithstochasticdifferentialequations. SilvanWeder,GuillermoGarcia-Hernando,AronMonszpart,MarcPollefeys,GabrielJ
ICLR(2022). Brostow,MichaelFirman,andSaraVicente.2023.Removingobjectsfromneural
LarsMescheder,AndreasGeiger,andSebastianNowozin.2018.Whichtrainingmethods radiancefields.InCVPR.
forGANsdoactuallyconverge?.InICML. RundiWu,BenMildenhall,PhilippHenzler,KeunhongPark,RuiqiGao,DanielWatson,
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa- PratulP.Srinivasan,DorVerbin,JonathanT.Barron,BenPoole,andAleksander
mamoorthi,andRenNg.2020.NeRF:Representingscenesasneuralradiancefields Holynski.2023. ReconFusion:3DReconstructionwithDiffusionPriors. arXiv
forviewsynthesis.InECCV. (2023).
AshkanMirzaei,TristanAumentado-Armstrong,MarcusABrubaker,JonathanKelly, ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKunZhang.2023.Smartbrush:
AlexLevinshtein,KonstantinosGDerpanis,andIgorGilitschenski.2023a.Reference- Textandshapeguidedobjectinpaintingwithdiffusionmodel.InCVPR.
guidedControllableInpaintingofNeuralRadianceFields.ICCV(2023). BangbangYang,YindaZhang,YinghaoXu,YijinLi,HanZhou,HujunBao,Guofeng
AshkanMirzaei,TristanAumentado-Armstrong,KonstantinosGDerpanis,Jonathan Zhang,andZhaopengCui.2021.LearningObject-CompositionalNeuralRadiance
Kelly,MarcusABrubaker,IgorGilitschenski,andAlexLevinshtein.2023b.SPIn- FieldforEditableSceneRendering.InICCV.
NeRF:Multiviewsegmentationandperceptualinpaintingwithneuralradiance JiahuiYu,ZheLin,JimeiYang,XiaohuiShen,XinLu,andThomasSHuang.2019.
fields.InCVPR. Free-formimageinpaintingwithgatedconvolution.InICCV.
AlexNichol,HeewooJun,PrafullaDhariwal,PamelaMishkin,andMarkChen.2022. XiaohuiZeng,ArashVahdat,FrancisWilliams,ZanGojcic,OrLitany,SanjaFidler,and
Point-E:ASystemforGenerating3DPointCloudsfromComplexPrompts. KarstenKreis.2022.LION:LatentPointDiffusionModelsfor3DShapeGeneration.
PakkaponPhongthawee,WoramethChinchuthakun,NontaphatSinsunthithet,Amit InNeurIPS.
Raj,VarunJampani,PramookKhungurn,andSupasornSuwajanakorn.2023.Diffu- RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
sionLight:LightProbesforFreebyPaintingaChromeBall.arXiv(2023). TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.InCVPR.
DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,Jonas ShengyuZhao,JonathanCui,YilunSheng,YueDong,XiaoLiang,EricIChang,andYan
MÃ¼ller,JoePenna,andRobinRombach.2023.SDXL:ImprovingLatentDiffusion Xu.2021.Largescaleimagecompletionviaco-modulatedgenerativeadversarial
ModelsforHigh-ResolutionImageSynthesis.arXiv(2023). networks.ICLR(2021).
BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.2023. DreamFusion: HaitianZheng,ZheLin,JingwanLu,ScottCohen,EliShechtman,ConnellyBarnes,
Text-to-3Dusing2DDiffusion.ICLR(2023). JianmingZhang,NingXu,SohrabAmirghodsi,andJieboLuo.2022.Imageinpainting
KiraPrabhu,JaneWu,LynnTsai,PeterHedman,DanBGoldman,BenPoole,and withcascadedmodulationGANandobject-awaretraining.InECCV.
MichaelBroxton.2023.Inpaint3D:3DSceneContentGenerationusing2DInpainting YufengZheng,XuetingLi,KokiNagano,SifeiLiu,OtmarHilliges,andShaliniDeMello.
Diffusion.arXiv(2023). 2023. Aunifiedapproachfortext-andimage-guided4dscenegeneration. arXiv
JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao,GangZeng,andZiweiLiu. (2023).
2023.DreamGaussian4D:Generative4DGaussianSplatting.arXiv(2023).
BarbaraRoessle,NormanMÃ¼ller,LorenzoPorzi,SamuelRotaBulÃ²,PeterKontschieder,
andMatthiasNieÃŸner.2023. GANeRF:LeveragingDiscriminatorstoOptimize
NeuralRadianceFields.ToG(2023).
RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
2022a.High-ResolutionImageSynthesiswithLatentDiffusionModels.InCVPR.10 â€¢ AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked View Reference View Novel View #1 Novel View #2 Novel View #3
Fig.9. QualitativeobjectremovalresultsontheSPIn-NeRFdataset.RefFusionsynthesizesplausiblecontentthatishighlymulti-viewconsistent.RefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting â€¢ 11
Masked View Reference w/o personalization Ours
Masked View Reference w/o adversarial loss Ours
Masked View Reference w/o local SDS Ours
Masked View Reference w/o depth loss Ours
Masked View Reference w/o splitting the Gaussians Ours
Fig.10. QualitativeresultsoftheablationstudyonSPIn-NeRFdataset.Notehowdifferentcomponentsofourmethodhelpimprovedifferenttypesofartifacts.
Masked View 1 View 2 Views 4 Views
Sparse Views
Fig.11. QualitativeevaluationofsparseviewreconstructiononSPIn-NeRFdataset.BothRefFusionand3DGSusethereconstructionlossonsparseinput
imagesinthemaskedregion.Additionally,RefFusionusesgenerativepriorsofthereferenceadaptedLDMthroughSDSlossesaswellasthedepthand
adversarialregularizationterms.NotehowgenerativepriorscansuccessfullyguidethereconstructionevenintheextremecaseofasingleGTview.
SGD3
sruO