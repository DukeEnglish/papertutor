TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net
ZhuoChen12 JacobMcCarran*1 EstebanVizcaino*1 MarinSoljacË‡icÂ´12 DiLuo123
Abstract intothestudyofPDEs(Hanetal.,2017;Yuetal.,2018;
Long et al., 2018; Carleo & Troyer, 2017; Raissi et al.,
Partial differential equations (PDEs) are instru-
2019;Lietal.,2020;Luetal.,2019;Hanetal.,2018;Sirig-
mentalformodelingdynamicalsystemsinscience
nano&Spiliopoulos,2018;Chenetal.,2022;2023b)has
andengineering. Theadventofneuralnetworks
marked a transformative shift in both fields, particularly
hasinitiatedasignificantshiftintacklingthese
highlightedintherealmsofcomputationalmathematicsand
complexitiesthoughchallengesinaccuracyper-
data-driven discovery. Machine learning offers new pos-
sist,especiallyforinitialvalueproblems. Inthis
sibilities for tackling the complexities inherent in PDEs,
paper, we introduce the Time-Evolving Natural
whichoftenposesignificantchallengesfortraditionalnu-
Gradient (TENG), generalizing time-dependent
mericalmethodsduetohighdimensionality,nonlinearity,
variationalprinciplesandoptimization-basedtime
orchaoticbehavior. Byleveragingneuralnetworksâ€™ability
integration,leveragingnaturalgradientoptimiza-
to approximate complex functions, algorithms have been
tion to obtain high accuracy in neural-network-
developedtosolve,simulate,andevendiscoverPDEsfrom
based PDE solutions. Our comprehensive de-
data,circumventingtheneedforexplicitformulations.
velopmentincludesalgorithmslikeTENG-Euler
anditshigh-ordervariants,suchasTENG-Heun, Partialdifferentialequationswithinitialvalueproblems,cru-
tailored for enhanced precision and efficiency. cialindescribingtheevolutionofdynamicalsystems,repre-
TENGâ€™seffectivenessisfurthervalidatedthrough sentafundamentalclasswithintherealmofPDEs. Despite
itsperformance,surpassingcurrentleadingmeth- the promising advancements made by machine learning
odsandachievingmachineprecisioninstep-by- techniquesinapproximatingthesolutionstothesecomplex
step optimizations across a spectrum of PDEs, PDEs,theyfrequentlyencounterdifficultiesinmaintaining
includingtheheatequation,Allen-Cahnequation, highlevelsofaccuracy,achallengethatbecomesparticu-
andBurgersâ€™equation. larlypronouncedwhennavigatingtheintricateinitialcondi-
tions. Thischallengelargelyoriginatesfromthecumulative
and propagative of errors in PDE solvers over time, ne-
1.Introduction cessitatingprecisesolutionsateachtimestepforaccuracy.
Although various training strategies, both global-in-time
Partialdifferentialequations(PDEs)holdprofoundsignifi-
training(MuÂ¨ller&Zeinhofer,2023)andsequential-in-time
canceinboththetheoreticalandpracticalrealmsofmathe-
training(Chenetal.,2023a;Berman&Peherstorfer,2023),
matics,science,andengineering.Theyareessentialtoolsfor
have been proposed to address this issue, it continues to
describingandunderstandingamultitudeofphenomenathat
standasacriticalchallengeinthefield.
exhibitvariationsacrossdifferentdimensionsandpointsin
time. ThestudyandsolutionofPDEshavedrivenadvance- Contributions.Inthispaper,weintroduceahighlyaccurate
mentsinnumericalanalysisandcomputationalmethods,as andefficientapproachfortacklingtheabovechallengebyin-
manyreal-worldproblemsmodeledbyPDEsaretoocom- troducingTime-EvolvingNaturalGradient(TENG).Our
plex for analytical solutions. The long-pursued quest for keycontributionsarethree-foldandhighlightedasfollows:
anefficientandaccuratenumericalPDEsolvercontinues
tobeacentralendeavorpassionatelypursuedbyresearch
â€¢ ProposetheTENGmethodwhichgeneralizestwofun-
communities.
damentalapproachesinthefield,time-dependentvari-
Inrecentyears,theintroductionofmachinelearning(ML) ationalprinciple(TDVP)andoptimization-basedtime
integration (OBTI), and achieves highly accurate re-
*Equalcontribution 1DepartmentofPhysics,Massachusetts
sultsbyintegratingnaturalgradientwithsequential-in-
InstituteofTechnology2NSFAIInstituteforArtificialIntelligence
timeoptimization.
andFundamentalInteractions3DepartmentofPhysics,Harvard
University.Correspondingauthor:DiLuo.
â€¢ Developefficientalgorithmswithsparseupdateforthe
Preprint.Copyright2024bytheauthor(s). realizationofTENG,includingthebasicTENG-Euler
1
4202
rpA
61
]GL.sc[
1v17701.4042:viXraTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
andthehighlyaccuratehigher-orderversions,suchas 2020),andPINNtraining(MuÂ¨ller&Zeinhofer,2023).
TENG-Heun.
Time Dependent Variational Principle
â€¢ Demonstratethatourapproachobtainsordersofmagni-
tudebetterperformancesthanstate-of-the-artmethods
â„’ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡
suchasOBTI,TDVPwithsparseupdates,andPINN ğ‘‡ğ‘‡ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡â„³Î˜
Generalize
with energy natural gradient, and achieves machine
Repeated tangent projection
ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡
precisionaccuracyduringper-stepoptimizationona ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡+Î”ğ‘¡ğ‘¡
variety of PDEs, including the heat equation, Allen-
Cahnequation,andBurgersâ€™equation. Time
â„³Î˜
Evolving
2.Relatedwork Natural Optimization-Based Time Integration
Gradient
MachineLearninginPDEs. Machinelearninghasbeen ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡+Î”ğ‘¡ğ‘¡â„’ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡
usedtosolvePDEsbyusingneuralnetworksasfunctionap-
proximatorstothesolutions. Ingeneral,therearetwotypes
Optimize in -space ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡
ofstrategies,global-in-timeoptimizationandsequential-in- Generalize ğ‘¢ğ‘¢ï¿½ğœƒğœƒğ‘¡ğ‘¡+Î”ğ‘¡ğ‘¡
ğ‘¢ğ‘¢
time optimization. Global-in-time optimization includes
thephysics-informedneuralnetwork(PINN)(Raissietal., â„³Î˜
2019;Wang&Perdikaris,2023;Wangetal.,2021b;Sirig-
Figure1.TENGgeneralizestheexistingTDVPandOBTImethods.
nano & Spiliopoulos, 2018; Wang et al., 2021a), which
Withinasingletimestep,TDVPprojectstheupdatedirectionLuË†
optimizestheneuralnetworkrepresentationovertimeand Î¸t
ontothetangentspaceoftheneuralnetworkmanifoldT M at
spacesimultaneously,ordeepRitzmethod(Weinanetal., uË†Î¸t Î˜
timet,andevolvestheparametersÎ¸accordingtothistangentspace
2021; Yu et al., 2018) when the variational form of the
projection. OBTIoptimizesÎ¸toobtainanapproximationtothe
PDE exists. In contrast, sequential-in-time optimization
targetfunctionuË† +âˆ†tLuË† onthemanifoldM .Generalizing
(sometimesalsocalledneuralGalerkinmethod)onlyuses thesetwomethodÎ¸t s,TENGdÎ¸ et finesthelossfunctioÎ˜ ndirectlyinthe
the neural network to represent the solution at a particu- u-spaceandoptimizesthelossfunctionviarepeatedprojections
lar time step and updates the neural network representa- tothetangentspaceT M .
uË†Î¸t Î˜
tion step-by-step in time. There are different approaches
toachievingsuchupdates,includingtime-dependentvari-
ational principle (TDVP) (Dirac, 1930; Koch & Lubich, 3.ProblemFormulationandChallenges
2007;Carleo&Troyer,2017;Du&Zaki,2021;Berman
3.1.Problemformulation
&Peherstorfer,2023)andoptimization-basedtimeintegra-
tion(OBTI)(Chenetal.,2023a;Kochkov&Clark,2018; Given a spatial domain X âŠ† Rd and temporal domain
GutieÂ´rrez&Mendl,2022;Luoetal.,2022;2023). Machine T âŠ‚ R,letubeafunctionX Ã—T â†’ Rthatsatisfiesthe
LearninghasalsobeenappliedtomodelPDEsbasedondata.
followinginitialvalueproblemofaPDE
Such data-driven approaches include neural ODE (Chen
et al., 2018), graph neural network methods (Pfaff et al., âˆ‚u(x,t)
=Lu(x,t) for (x,t)âˆˆX Ã—T and
2020;Sanchez-Gonzalezetal.,2020),neuralFourieropera- âˆ‚t (1)
tor(Lietal.,2020),andDeepONet(Luetal.,2019). u(x,0)=u (x),
0
NaturalGradient. Theconceptofnaturalgradients,first
with appropriate boundary conditions. The sequential-in-
introducedbyAmari(Amari,1998)hasbecomeacorner-
timeoptimizationapproachusesneuralnetworktoparame-
stoneintheevolutionofoptimizationtechniqueswithinma-
terizethesolutionofthePDEataparticulartimesteptâˆˆT
chinelearning. Thesemethodsmodifytheupdatedirection
as uË† (x) : Î˜Ã—X â†’ R, where the parameters have an
ingradient-basedoptimizationasasecond-ordermethod, Î¸t
typicallyinvolvingusingtheFishermatrix. Distinctfrom
explicitly time dependence Î¸
t
: T â†’ RNp (with N
p
the
number of parameters) and evolves over time. To solve
traditionalgradientmethodsduetoitsconsiderationofthe
thePDE,theneuralnetworkisfirstoptimizedtomatchthe
underlying data geometry, natural gradient descent leads
initialconditionuË† (x) = u (x),andthenoptimizedina
tofasterandmoreeffectiveconvergenceinvariousscenar- Î¸0 0
time-step-by-time-stepfashiontoupdatetheparameters.
ios. Natural gradient descent and its variants have found
widespreadapplicationinareassuchasneuralnetworkop- We contrasted this with the global-in-time optimization
timization (Peters et al., 2003; Pascanu & Bengio, 2013; method,suchasPINN(Raissietal.,2019),wheretheneu-
Zhang et al., 2019), reinforcement learning (Peters et al., ralnetworkisusedtoparameterizethesolutionforalltime
2003;Kakade,2001),quantumoptimization(Stokesetal., uË† (x,t)withasinglesetofparameters. Inthiscontext,a
Î¸
2TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
lossfunctionthatgivesrisetotheglobalsolutionofthePDE lossfunction
isusedtooptimizetheparameters.
Î¸ =argminL(uË† ,u ). (3)
t+âˆ†t Î¸ target
Î¸âˆˆÎ˜
3.2.TimeDependentVariationalPrinciple
Dependingonthediscrete-timeintegrationschemesused,
Time-dependentvariationalprinciple(TDVP)isanexisting
u can be different. The most commonly used in-
target
sequential-in-time method. It aims to derive an ODE in
tegration scheme is the forward Eulerâ€™s method, where
theparameterÎ¸-spacebasedonthefunctionu-spacePDE
u =uË† +âˆ†tLuË† Sometypicallossfunctionsused
(Fig.1). Themostcommonlyusedprojectionmethodisthe inta Org Bet TImeÎ¸ tt hodsincluÎ¸ dt etheL2-distance,theL1-distance,
Diracâ€“Frenkelvariationalprinciple(Dirac,1930),whichde-
andtheKL-divergence.
finestheODEbysolvingthefollowingleastsquareproblem
ateachtimestep Limitations. AlthoughtheoptimalsolutiontoEq.(3)gives
thebestapproximationofu inM ,inpractice,theop-
(cid:13) (cid:13)2 target Î˜
âˆ‚ Î¸ =argmin(cid:13)LuË† (Â·)âˆ’(cid:80) J âˆ‚ Î¸ (cid:13) , (2) timizationcanbeverydifficultwithanon-convexlandscape.
t (cid:13) Î¸ j (Â·),j t j(cid:13)
âˆ‚tÎ¸âˆˆRNp L2(X) CommonoptimizerssuchasAdamandBFGS(L-BFGS)
whereJ :=âˆ‚uË† (x)/âˆ‚Î¸ istheJacobian. often require a significant number of iterations to obtain
(x),j Î¸ j
anacceptablelossvalue. Sincethisoptimizationhastobe
DenotingthefunctionspaceofuwithU,themanifoldof
repeatedoveralltimesteps,theaccumulationoferrorand
neuralnetworkparameterizedfunctionsuË† withM ,and
Î¸ Î˜ costoftenresultsinpoorperformance. Inaddition,theinte-
the tangent space to the manifold at uË† with T M ,
Î¸t uË†Î¸t Î˜ grationschemeusedincurrentimplementationsofOBTIis
Eq. (2) gives the orthogonal projection of the evolution
oftenlimitedtotheforwardEulerâ€™smethod,whichrequires
directionâˆ‚ u=LuontothetangentspaceT M gener-
t uË†Î¸t Î˜ small time steps and further amplifies the issue of error
atedbythepushfowardofâˆ‚ Î¸. TheresultingODEinthe
t accumulation and cost. We note that while higher-order
Î¸-space can then be evolved in discrete time steps using
integrationschemeshavebeenexploredinpriorworks,they
numerical integrators such as the 4th-order Rungeâ€“Kutta eitherinvolveapplyingLmultipletimesonuË† (Donatella
Î¸
(RK4)method. et al., 2023) or require differentiating through LuË† with
Î¸
Limitations. TheDiracâ€“Frenkelvariationalprinciplepro-
respecttoÎ¸(Luoetal.,2022;2023),bothofwhichrequires
ducestheorthogonalprojectionoftheevolutionontothe high-order differentiation, leading to stability issues and
tangentspaceT M atuË† duringeachtimestep. For furtherincreaseofthecost.
uË†Î¸t Î˜ Î¸t
nonzero time step sizes âˆ†t, however, the result becomes
onlyanapproximationtotheoptimalprojectionofthetarget 4.Time-EvolvingNaturalGradient(TENG)
solutionontothemanifoldM . TheevolutiononM can
Î˜ Î˜
alsodeviatefromtheprojecteddirectiononT M due 4.1.GeneralizationfromTDVPandOBTI
uË†Î¸t Î˜
tononzerotimestepsizes,whichgivesrisetothefollow-
Wefirstmakethefollowingobservation.
ing consequence: although Eq. (2) is reparameterization
invariant,itsnonzeroâˆ†tversionisnot(seeAppendixThe- Observation: TDVPcanbeviewedassolvingEq.(3)with
oremA.1fordetail). Inaddition,theleastsquareproblem
the(squared)L2-distanceasthelossfunctionusingasingle
in Eq. (2) is often ill-conditioned and the solution could tangentspaceapproximationateachtimestep.
be sensitive to the number of parameters, the number of
samples used, and the regularization method. Although Proof. Attimet,theneuralnetworkmanifoldM Î˜canbe
Ref.(Berman&Peherstorfer,2023)proposedasparseup- approximatedatthepointuË† Î¸t byitstangentspaceas
datemethod,wherearandomsubsetofparametersareup-
datedateachtimestep,itisstillhardtoverifywhethersuch uË† =uË†
+(cid:88)âˆ‚uË†
Î¸Î´Î¸ +O(Î´Î¸2), (4)
Î¸+Î´Î¸ Î¸ âˆ‚Î¸ j
choicegivesthebestprojectioninpractice. Meanwhile,the j j
solution of Eq. (2) after regularization could be different
fromoptimal. Let L(uË† Î¸+Î´Î¸,u target) = âˆ¥uË† Î¸+Î´Î¸âˆ’u targetâˆ¥2 L2(X). For
small Î´t, u = uË† +Î´tLuË† +O(Î´t2). Keeping ev-
target Î¸ Î¸
3.3.Optimization-basedTimeIntegration erythingtofirstorder,thelossfunctiontakesitsminimum
when
Optimization-basedtimeintegration(OBTI)isanalterna-
tivesequential-in-timemethod. Itdirectlydiscretizesthe (cid:13) (cid:13) (cid:13) (cid:13)2
PDEintotimestepsintheoriginalfunctionu-space;ineach Î´Î¸ =argmin(cid:13) (cid:13) (cid:13)Î´tLuË† Î¸(Â·)âˆ’(cid:88)âˆ‚u âˆ‚Ë† Î¸ Î¸(Â·) Î´Î¸ j(cid:13) (cid:13)
(cid:13)
. (5)
timestep,OBTIfirstfindsthenext-time-steptargetfunction Î´Î¸âˆˆRNp (cid:13) j j (cid:13)
L2(X)
u based on the current-time-step uË† , and then opti-
target Î¸t
mizesthenext-time-stepparametersÎ¸ byminimizinga DividingbothsidesbyÎ´trecoverstheTDVP(Eq.(2)).
t+âˆ†t
3TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Inspired by such observation, we introduce the time- Proof. TENGachievesitsoptimumwhenuË†âˆˆM isclos-
Î˜
evolving natural gradient (TENG) method, which gener- esttou ateachtimestep. Sinceareparameterization
target
alizesTDVPandOBTImethodsinthefollowingway: doesnotchangethemanifoldM andthelossisdefined
Î˜
inthefunctionspace,therefore,theoptimalsolutiondiffers
TENGsolvesEq.(3)viaarepeatedtangentspaceapproxi-
bymerelyarelabelingofparametersatthesamepointin
mationtothemanifoldforeachtimestep.
M .
Î˜
TENGsubroutinewithineachtimestep. Thekeyideaof
TENGisshowninFig.1. Duringeachtimestep, TENG
Theglobalconvergenceofnaturalgradienthasbeenstudied
minimizesthelossfunctioninmultipleiterations(similarto
in certain non-linear neural network (Zhang et al., 2019).
OBTI),andwithineachiteration,itupdatestheparameters
Althoughachievingglobaloptimalisnottheoreticallyguar-
basedonthefunctionu-spacegradient(ofthelossfunction)
anteedingeneral, inpractice, thelossfunctionisusually
projectedtotheparameterÎ¸-space(similartoTDVP).Here,
convexintheu-space,anduË† isoftenclosetou be-
Î¸ target
wereservethephraseâ€œtimestepâ€forphysicaltimestepsof
cause of small time step sizes. The result is likely to be
thePDEandâ€œiterationâ€foroptimizationstepswithineach
close to global optimal. In practice, we observe the opti-
physicaltimestep.
mizationcanresultinlossvaluesclosetomachineprecision
The details of TENG iterations within a single time (O(10âˆ’14)).
step are shown in Subroutine TENG stepper, where
In addition, while TDVP may require solving an ill-
Î± is the learning rate at the nth iteration, and the
n conditioned least square equation and an inaccurate so-
least square(J ,âˆ†u(x)) should be interpreted as
(x),j lution directly affects the Î¸-space ODE, solving the least
solvingtheleastsquareproblem
squareproblemisonlypartoftheoptimizationprocedure
forTENG,whichturnsouttohaveasmallersideeffect. An
(cid:13) (cid:13)2
âˆ†Î¸ =argmin(cid:13)âˆ†u(Â·)âˆ’(cid:80) J âˆ†Î¸ (cid:13) . (6) inaccurate least square solution does not lead to an inac-
(cid:13) j (Â·),j j(cid:13)
âˆ†Î¸âˆˆRNp L2(X) curatesolutiontoEq.(3),givensufficientiterations. The
resultinglossvalueofEq.(3)alsoprovidesaconcretemet-
ricforTENGontheaccuracyduringoptimization.
SubroutineTENG stepper
AsdiscussedinSec.3.3,themainchallengeforthecurrent
Input: Î¸ init,u target OBTImethodliesinthedifficultyofoptimizingthetime
nâ†0,Î¸ â†Î¸ init integratinglossfunction(Eq.(3)). Whilethelossfunction
whilen<N itdo is a complicated non-convex function in the parameter Î¸-
âˆ†u(x)â†âˆ’Î±
âˆ‚L(uË† Î¸,u target)
(x)
space, it is usually convex in the u-space; therefore, it is
n âˆ‚uË† Î¸ advantageoustoperformgradientdescentinu-spaceand
âˆ‚uË† (x)
J â† Î¸ project the solution to Î¸-space. Furthermore, TENG can
(x),j âˆ‚Î¸ j alsobenefitfromthereparametrizationinvariantproperty
âˆ†Î¸ â†least square(J (x),j,âˆ†u(x)) describedabove. Whileanefficienthigher-ordertimeinte-
Î¸ â†Î¸+âˆ†Î¸ grationmethodisstilllackinginthecurrentOBTImethod,
nâ†n+1 inthisworkweshowhowtoincorporatehigher-ordermeth-
endwhile odsintoTENG.
Output: Î¸
TENGformulationovertimesteps. Themostsimpletime
integration scheme is the forward Eulerâ€™s method, which
onlykeepsthelowestorderTaylorexpansionofthePDE.
We note that when Subroutine TENG stepper is per-
When integrated in the TENG method, we set u =
formed under certain approximations, it can be reduced target
uË† +âˆ†tLuË† anduseSubroutineTENG steppertosolve
toTDVPorOBTI(seeAppendixAfordetail). Î¸t Î¸t
for uË† . The full algorithm is summarized in Algo-
Î¸t+âˆ†t
TENGresolvesthelimitationsofbothTDVPandOBTI. rithmTENG Euler.
As mentioned in Sec. 3.2, TDVP suffers from inaccurate
Beyond the first-order Eulerâ€™s method, Algorithm
tangentspaceapproximationfornonzeroâˆ†t. TENGdoes
TENG Heunprovidesanexampleofapplyingsecond-order
notsufferfromthisissuebecauseoftherepeateduseoftan-
integrationmethod. Inthismethod,anintermediatetarget
gentspaceprojections,whicheventuallyminimizesEq.(3)
solutionu isused,andasetofintermediateparameters
onthemanifold. Thisalsogivesthefollowingtheoremasa temp
Î¸ istrained. Theintermediateparametersareusedto
directconsequence,whichdoesnotholdforTDVP. temp
constructu andÎ¸ . Ourmethodavoidstermslike
target t+âˆ†t
Theorem4.1. TheoptimalsolutionofTENGisreparame- LnuË† or âˆ‚LuË† /âˆ‚Î¸ that often appear in existing OBTI
Î¸t Î¸t t
terizationinvariantevenwithnonzeroâˆ†t. methods (Donatella et al., 2023; Luo et al., 2022; 2023),
4TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
AlgorithmTENG Euler: A1st-orderintegrationscheme AlthoughthecostofTENGincludesbothC andN ,
lstsq it
Input: Î¸ ,âˆ†t,T
bothofthetermscanbesignificantlysmallerthanthosein
t=0
tâ†0 TDVPandOBTIduetothefollowingreasons.
whilet<T do In TDVP, the quality of the least square solution directly
u (x)â†uË† (x)+âˆ†tLuË† (x)
target Î¸t Î¸t correspondstotheaccuracy;thereforetheleastsquareequa-
Î¸ â†TENG stepper(Î¸ ,u )
t+âˆ†t t target tion must be solved with high accuracy and thus require
tâ†t+âˆ†t
highcost. Evenwithsparseupdate(Berman&Peherstor-
endwhile fer,2023),onemaynotbeabletousetoofewparameters;
Output: Î¸ t=T otherwise, theupdatemaybeinaccurate. Incontrast, the
leastsquareequationcanbesolvedapproximatelyinTENG
AlgorithmTENG Heun: A2nd-orderintegrationscheme withoutcompromisingaccuracy. Inthiswork, wedesign
a sparse update scheme for TENG. In each iteration, we
Input: Î¸ ,âˆ†t,T
t=0 randomlysub-sampleparametersandonlysolvetheleast
tâ†0
squareequationwithinthespaceoftheseparameters,which
whilet<T do
significantlyhelpsreducethecost.
u (x)â†uË† (x)+âˆ†tLuË† (x)
temp Î¸t Î¸t
Î¸ â†TENG stepper(Î¸ ,u ) OBTI,ontheotherhand,requiresalargenumberofitera-
temp t temp
âˆ†t(cid:0) (cid:1) tionsineverytimesteptominimizethelossfunction,due
u (x)â†uË† (x)+ LuË† (x)+Lu (x)
target Î¸t 2 Î¸t Î¸temp tothedifficultyofthenon-convexoptimizationlandscape.
Î¸ â†TENG stepper(Î¸ ,u )
t+âˆ†t temp target Incontrast,inourTENGmethod,thelossvaluesdecrease
tâ†t+âˆ†t
toclosetomachineprecisionwithonlyO(1)iterations(see
endwhile
Sec.5.2andAppendixBfordetail). Inpractice,weobserve
Output: Î¸
t=T that TENG is able to improve the accuracy by orders of
magnitude while keeping a similar computational cost to
TDVPandOBTI(alsoseeAppendixB).
reducingthecostandimprovingnumericalstability.
TheerrorofTENGisingeneraldeterminedby(i)theex-
Connection to natural gradient. We note that the algo- pressivity of the neural network (ii) the optimization al-
rithmoutlinedinSubroutineTENG steppercanberefor-
gorithm (iii) the time integration scheme. Based on the
mulatedusingtheconventionalHilbertnaturalgradientin universalapproximationtheorem,withaproperchoiceof
theform neural network, it is likely that the neural network is suf-
ficiently powerful to represent the underlying solution at
âˆ†Î¸ i =âˆ’Î±(cid:88) Gâˆ’1(Î¸) i,jâˆ‚L(uË† Î¸ âˆ‚, Î¸u target) (x), (7) everytimestep;thustheerrorfrom(i)issmallingeneral.
j j OurTENGalgorithmisabletoachievelossvaluesclose
to machine precision at every time step; therefore the er-
with G(Î¸) the Hilbert gram matrix (see Appendix A for
ror from (ii) error is also small. Given sufficiently small
detail). However, solving least square equations is more
errorsin(i)and(ii),factor(iii)dominatestheconvergence
stable, withtheaddedflexibilityofchoosingleastsquare
property of TENG. At the same time, higher-order time-
solvers. Therefore, we use the formulation in Subrou-
integrationschemescanbeintegratedwithTENG,inwhich
tineTENG stepperforpracticalimplementation.
casetheerrorfrom(iii)followsthestandardnumericalanal-
Alternatively, Subroutine TENG stepper can also be ysis results for solving differential equations. From the
viewed as a generalized Gaussâ€“Newton method. There- aboveperspectives,wefurthercontrastTENGwithother
fore,TENGcanalsobeinterpretedastheabbreviationof algorithms. While TDVP does not have an optimization
time-evolvingNewtonâ€“Gauss(alsoseeAppendixA). error, the projection step already introduces some errors,
whichcanbeseverefornonzerotimestepsizesandwhen
theleastsquareequationinEq.(2)islowrank(Berman&
4.2.ComplexityandErrorAnalysis
Peherstorfer,2023). ForOBTI,theerrorfrom(ii)canbe
ThecomputationalcomplexityofTENGfromt = 0toT large,resultinginpoorperformances,inadditiontothelack
isO(C lstsqN itT/âˆ†t),whereC lstsq =O(N sN p2)(withN s ofefficienthigher-ordertime-integrationschemesinprior
thenumberofsamplesandN p thenumberofparameters) works.
isthecostofsolvingtheleastsquareequationineachitera-
TENGalsopermitserrorestimationbasedonthedecompo-
tion,N isthenumberofiterationsineachtimestep,and
it
sitionoferrorsabove. Below,weoutlinetheerrorestima-
T/âˆ†tisthenumberofphysicaltimesteps. Incomparison,
thecomputationalcomplexityofTDVPisO(Câ€² T/âˆ†t) tionforTENG-Euler. ErrorsforTENGwithhigher-order
andthecomputationalcomplexityofOBTIisO(l Nsts â€²q
T/âˆ†t).
integrationmethodscanbeestimatedanalogously.
it
5TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Let Îµ (t) be the Lp-error between the TENG-Euler solu- cangeneratesharpgradientsthatpropagateovertime,es-
p
tion and the exact solution at time t, ÎµEE(t) between the pecially for small Î½, which can be challenging to solve.
p
exactsolutionandthesolutionevolvedexactlyaccording SimilartotheAllenâ€“Cahnequation,Burgersâ€™equationdoes
to the Eulerâ€™s method, ÎµTE(t) between the TENG-Euler not have a general analytical solution either. Therefore,
p
and the solution evolved exactly according to the Eulerâ€™s wealsousethespectralmethod(Canutoetal.,2007)solu-
method,r(Â·,t)theresidualfunctionaftertheTENG-Euler tionasaproxyoftheexactsolutionasthereference(see
optimizationofthetimestepatt,andG :=1+âˆ†tL. AppendixDfordetail).
Theorem 4.2. The error Îµ p(t) is bounded by ÎµE pE(t) + PDE domain, boundary, and initial condition. For all
ÎµTE(t), where ÎµEE(t) is an order O(âˆ†t) quantity, and threeequations,wefirstbenchmarkontwospatialdimen-
p p
(cid:13) (cid:13)
ÎµTE(t)=(cid:13)(cid:80)t/âˆ†tâˆ’1Gnr(Â·,tâˆ’nâˆ†t)(cid:13)
.
sionsinthedomainX = [0,2Ï€)Ã—[0,2Ï€)andT = [0,4],
p (cid:13) n=0 (cid:13) Lp(X) withperiodicboundaryconditionandthefollowinginitial
condition
Proof. SeeAppendixTheoremA.2. 1 (cid:18) (cid:16) (cid:17)
u (x ,x )= exp 3sin(x )+sin(x )
0 1 2 100 1 2
5.Experiments
(cid:16) (cid:17)
+exp âˆ’3sin(x )+sin(x )
1 2
5.1.EquationsandSetup
(cid:16) (cid:17)
âˆ’exp 3sin(x )âˆ’sin(x )
Heat equation. The first example we choose is the two- 1 2
dimensionalisotropicheatequation (cid:16) (cid:17)(cid:19)
âˆ’exp âˆ’3sin(x )âˆ’sin(x )
1 2
âˆ‚u (cid:18) âˆ‚2u âˆ‚2u(cid:19) (11)
=Î½ + (8)
âˆ‚t âˆ‚x2 âˆ‚x2 This initial condition is anisotropic, contains peaks and
1 2
valleys at four different locations, and consists of many
with a diffusivity constant Î½ = 1/10. The heat equation frequenciesbesidesthelowestfrequency,whichcanresult
describesthephysicalprocessofheatfloworparticlediffu- inchallengingdynamicsforvariousPDEs.
sioninspace. Sinceitpermitsananalyticalsolutioninthe
FortheHeatequation,weinadditionconsiderachalleng-
frequencydomain,theheatequationisanidealtestbedfor
ingthree-dimensionalbenchmark,whereweagainchoose
benchmarking(seeAppendixDfordetails).
periodicboundaryconditionsinthedomainX =[0,2Ï€)3
Allenâ€“Cahnequation. WealsoconsiderAllenâ€“Cahnequa- andT =[0,8]. Theinitialconditionischosentobeacom-
tion bination of sinusoidal terms in the following form so the
âˆ‚u (cid:18) âˆ‚2u âˆ‚2u(cid:19)
=Î½ + +uâˆ’u3 (9) exactsolutioncanbeanalyticallycalculated.
âˆ‚t âˆ‚x2 âˆ‚x2
1 2
u (x ,x ,x )=A
0 1 2 3 000
withadiffusivityconstantÎ½ =1/200,whichisareaction-
2 2 2 3
diffusion model that describes the process of phase sep- (cid:88) (cid:88) (cid:88) (cid:89)
+ A cos(k x )
aration. The Allenâ€“Cahn equation is nonlinear and does k1k2k3 i i
notpermitanalyticalsolutionsingeneral. Inaddition,its
k1=1k2=1k3=1 i=1
2 2 2 3
solutionusuallyinvolvessharpboundariesandcanbechal- (cid:88) (cid:88) (cid:88) (cid:89)
+ B sin(k x ),
lenging to solve numerically. As a benchmark, we solve k1k2k3 i i
it using a spectral method (Canuto et al., 2007) (see Ap-
k1=1k2=1k3=1 i=1
(12)
pendixDfordetail)andconsideritssolutiontobeaproxy
wherethecoefficientsarerandomlychosen(seeAppendixC
oftheexactsolutionasthereference.
for coefficients used in this work). In Appendix C, we
Burgersâ€™equation. Wefurtherbenchmarkourmethodon also explore the heat equation on a 2D disk X = D 2 =
theviscousBurgersâ€™equation {(x 1,x 1):x2 1+x2 2 â‰¤1}.
âˆ‚u (cid:18) âˆ‚2u âˆ‚2u(cid:19) (cid:18) âˆ‚u âˆ‚u (cid:19) For the Burgersâ€™ equation, cases with unequal domains
=Î½ + âˆ’u + (10) and additional initial conditions are also explored in Ap-
âˆ‚t âˆ‚x2 âˆ‚x2 âˆ‚x âˆ‚x
1 2 1 2 pendixC.
withadiffusivity(viscosity)constantÎ½ = 1/100. InAp- Baselines. WhileTENGissequential-in-time,ourbench-
pendixB,wealsoexplorecaseswithsmallerÎ½. Burgersâ€™ marksincludebothsequential-in-time(TDVPandOBTI)
equationisaconvection-diffusionequationthatdescribes andglobal-in-time(PINN)methods. ForTDVP,wechoose
phenomenainvariousareas,suchasfluidmechanics,nonlin- the recently proposed sparse update method (Berman &
earacoustics,gasdynamics,andtrafficflow. Thisequation Peherstorfer,2023),whichhasbeenshowntooutperform
6TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Heat Equation 2D Heat Equation 3D Allen Cahn Equation Burgers' Equation
TENG-Euler (Ours) TENG-Heun (Ours) TDVP-RK4 OBTI-Adam OBTI-LBFGS PINN-ENGD PINN-BFGS
100
10 2
10 4
10 6
10 8
0 1 2 3 4 0 2 4 6 8 0 1 2 3 4 0 1 2 3 4
Time Time Time Time
Figure2.Benchmark of TENG, in terms of relative L2-error as a function of time, against various algorithms on two- and three-
dimensionalheatequations,Allenâ€“CahnequationandBurgersâ€™equation.Allsequential-in-timemethodsusethesametimestepsize
âˆ†t=0.005forheatandAllenâ€“Cahnequationsandâˆ†t=0.001forBurgersâ€™equation.
previousfullupdatemethods. Inaddition,weusethesame 5.2.Results
fourth-orderRungeâ€“Kuttaintegrationscheme.FortheOBTI
Benchmarkagainstothermethods. Westartthebench-
method,wechoosethestandardEulerâ€™sintegrationscheme
withL(uË† ,u ) = âˆ¥uË† âˆ’u âˆ¥2 (thesameloss mark of our method against other methods described in
Î¸ target Î¸ target L2(X) Sec.5.1intermsofboththerelativeL2-error(Eq.(13))as
asTENG).BothAdamandL-BFGSoptimizersareusedas
afunctionoftime(Fig.2)andtheglobalrelativeL2-error
benchmarks. Forallsequential-in-timemethods,weusethe
(Eq.(14))integratedoveralltime(Table1).
sametimestepâˆ†t = 5Ã—10âˆ’3 fortheheatequation. For
Allenâ€“CahnequationandBurgersâ€™equation,wefirstcom-
0.4
pareallsequential-in-timemethodswithâˆ†t = 5Ã—10âˆ’3
andâˆ†t=1Ã—10âˆ’3respectively,beforeanalyzingtheeffect 0
ofvariousâˆ†t. Inaddition,Allsequential-in-timemethods 0.4
sharethesameneuralnetworkarchitectureandinitialpa- 0.4
rametersatt = 0. ForPINN,wetestbothBFGSandthe 0
recentlyproposedENGDoptimizer. SinceRef.(MuÂ¨ller& 0.4
Zeinhofer, 2023) did not provide the implementation for 5Ã—10 6
Allenâ€“CahnequationandBurgersâ€™equation,weomitthe 0
benchmarkofENGDoptimizerforthetwoequations. We 5Ã—10 6
useanetworkarchitecturesimilartoRef.(MuÂ¨ller&Zein- T=0 T=1 T=2 T=3 T=4
hofer,2023)(seeAppendixEfordetail).
Figure3.Referencesolution,TENGsolution,andthedifference
Errormetric. Weconsiderthefollowingtwoerrormetrics:
betweenthemforBurgersâ€™equation. Thereferencesolutionis
generatedusingthespectralmethod,andtheTENGsolutionshown
1. relativeL2-errorateachtimestep hereusestheTENG-Heunmethodwithâˆ†t=0.001.
âˆ¥uË†(Â·,t)âˆ’u (Â·,t)âˆ¥ Inallcases, ourTENG-Heunmethodachievesresultsor-
reference L2(X)
Îµ(t)= , (13) dersofmagnitudebettercomparedtoothermethods. Upon
âˆ¥u (Â·,t)âˆ¥
reference L2(X)
closerinspection,ourTENGmethodwithEulerâ€™sintegra-
tion scheme is already comparable to or better than the
2. globalrelativeL2-errorintegratedoveralltimesteps
TDVPmethodwiththeRK4integrationscheme. Inaddi-
tion, TENG-Euler is significantly better than OBTI with
âˆ¥uË†(Â·,Â·)âˆ’u (Â·,Â·)âˆ¥
Îµ = reference L2(XÃ—T) , (14) bothAdamandL-BFGSoptimizers,bothofwhichusethe
g âˆ¥u (Â·,Â·)âˆ¥
reference L2(XÃ—T) sameintegrationscheme. InFig.3,Weshowthedifference
betweenTENG-Heunandthereferencesolutionbyplotting
whereu referstotheanalyticalsolutionfortheheat the function evolution over time. It can be seen that our
reference
equation,andthespectralmethodsolutionforAllenâ€“Cahn method traces closely with the reference solution with a
andBurgersâ€™equation(seeAppendixDfordetail). tinydeviationontheorderofO(10âˆ’6). InAppendixB,we
7
rorrE-2L
evitaleR
u
u
u
u
tcaxE
GNET
tcaxE
GNETTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
GlobalRelativeL2-Errorâ†“
Method
Heat(2D) Heat(3D) Allenâ€“Cahn Burgersâ€™
TENG-Euler(Ours) 3.006Ã—10âˆ’4 3.664 Ã—10âˆ’4 1.249 Ã—10âˆ’3 3.598 Ã—10âˆ’4
TENG-Heun(Ours) 1.588Ã—10âˆ’6 1.139Ã—10âˆ’5 6.187Ã—10âˆ’6 2.643Ã—10âˆ’6
TDVP-RK4 3.279Ã—10âˆ’4 3.841Ã—10âˆ’3 1.258Ã—10âˆ’3 2.437Ã—10âˆ’3
OBTI-Adam 1.391Ã—10âˆ’2 â€“ 4.966Ã—10âˆ’2 1.696Ã—10âˆ’1
OBTI-LBFGS 6.586Ã—10âˆ’3 8.743Ã—10âˆ’2 4.180Ã—10âˆ’2 1.047Ã—10âˆ’1
PINN-ENGD 1.403Ã—10âˆ’5 2.846Ã—10âˆ’3 â€“ â€“
PINN-BFGS 1.150 Ã—10âˆ’5 1.389Ã—10âˆ’2 5.540Ã—10âˆ’3 6.538Ã—10âˆ’2
Table1.BenchmarkofTENG,intermsofglobalrelativeL2-error,againstvariousalgorithmsontheheatequation,Allenâ€“Cahnequation
andBurgersâ€™equation.Thebestresultineachcolumnismarkedinboldfaceandthesecondbestresultismarkedinitalicfont.Here,the
sameâˆ†tasinFig.2isused.
showadditionaldetailsofruntime,andtherelationbetween InFig.5,weshowtheglobalrelativeL2-error(definedin
runtimeandperformance. Eq.(14))asafunctionoftimestepsizeâˆ†t. Itcanbeseen
fromthefigurethatwhileTENG-Euleralreadyachievesa
Convergencespeedandmachineprecisionaccuracy. We
globalrelativeL2-errorofO(10âˆ’4)forsmallâˆ†t,usinga
further demonstrate the convergence of TENG-Euler in
higher order integration scheme significantly reduces the
Fig.4comparedtoOBTI-AdamandOBTI-LBFGS.Inasin-
errortoO(10âˆ’6). Inaddition,TENG-Heuncanmaintain
gletimestep,TENGachievesatraininglossvalue(withthe
thelowerrorevenatrelativelylargeâˆ†t,signifyingthead-
squaredL2distanceasthelossfunction)closetomachine
vantageofourimplementationofhigher-orderintegration
precisionO(10âˆ’14)withonlyafewiterations,whileOBTI
schemes.Wenotethat,forsmalltimestepsizes,theaccumu-
canonlygettoO(10âˆ’7)lossafterafewhundrediterations.
lationofper-steperrordominates,whileforlargetimestep
Wealsoplotthefinallossofeachtimestepoptimization
sizes,thediscretizationerrorfromtheintegrationscheme
andshowthatTENGstablyreachesthemachineprecision
dominates,resultingintheTENG-Heunwiththesmallest
foralltime,whichissevenordersofmagnitudebetterthan
âˆ†t not as good as larger âˆ†t. In addition, the curves for
OBTI.OurresultshaveshownthehighaccuracyofTENG
TENG-EulerandTENG-Heunhavedifferentslopes. Both
comparedtotheexistingapproaches(seeAppendixBfor
phenomenaareconsistentwithnumericalanalysisresults
additionalresults). Sincethefinallossvaluesarenearma-
fortraditionalPDEsolvers. Additionalexplorationswith
chineprecisionforalltimesteps,webelievethemainerror
TENG-RK4methodcanbefoundinAppendixB.
sourceofTENG-EulercomesfromtheEulerâ€™sexpansion,
insteadoftheneuralnetworkapproximation. Thisisfurther
verifiedlaterduringtimeintegrationschemecomparisons. Allen Cahn Equation Burgers' Equation
TENG-Euler (Ours) TENG-Heun (Ours)
10 2
TENG-Euler (Ours) OBTI-Adam OBTI-LBFGS
104 104 10 3
106 106
10 4 108 108
1010 1010 10 5
1012 1012
1014 1014 10 6
10 2 10 3 10 2
1016 1016
1 7 50 150 300 0 1 2 3 4 Time Step Size Time Step Size
Iteration Time
Figure5.Comparison of different time integration schemes of
Figure4.TraininglossduringthetimestepatT = 1andfinal
TENGwithrespecttothetimestepsizesonAllenâ€“Cahnequation
traininglossesforalltimestepsfortheTENG-Eulermethodand
andBurgersâ€™equation,usingglobalrelativeL2-errorasametric.
thetwoOBTImethodsforAllenâ€“Cahnequation.
Compare time integration schemes. We further exam- 6.DiscussionandConclusion
inetheeffectsoftimeintegrationschemesonTENGand
compareourTENG-EulerandTENG-Heunmethodswith WeintroduceTime-EvolvingNaturalGradient,anovelap-
differenttimestepsizes. proachthatgeneralizestime-dependentvariationalprinci-
8
ssoL
gniniarT
1=T
ssoL
laniF
petS
emiT
rorrE-2L
evitteleR
labolGTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
plesandoptimization-basedtimeintegration,resultingin Acknowledgements
a highly accurate and efficient PDE solver utilizing natu-
TheauthorsacknowledgesupportfromtheNationalScience
ralgradient. TENG,encompassingalgorithmslikeTENG-
Foundation under Cooperative Agreement PHY-2019786
EulerandadvancedvariantssuchasTENG-Heun,signifi-
(TheNSFAIInstituteforArtificialIntelligenceandFunda-
cantlyoutperformsexistingstate-of-the-artmethodsinac-
mentalInteractions,http://iaifi.org/). Thismate-
curacy,achievingmachineprecisioninsolvingarangeof
rialisbaseduponworksupportedbytheU.S.Departmentof
PDEs. Forfuturework,itwouldbeinterestingtoexplore
Energy,OfficeofScience,NationalQuantumInformation
theapplicationofTENGinmorediverseandcomplexreal-
ScienceResearchCenters,Co-designCenterforQuantum
worldscenarios,particularlyinareaswheretraditionalPDE
Advantage(C2QA)undercontractnumberDE-SC0012704.
solutions are currently unfeasible. While this work is fo-
Thismaterialisalsoinpartbaseduponworksupportedby
cusedontwo-andthree-dimensional(spatial)scalar-valued
theAirForceOfficeofScientificResearchundertheaward
PDEswithperiodicboundaryconditions,thesamemethod
numberFA9550-21-1-0317. Theresearchwassponsoredby
can be considered for generalizing to vector-valued PDE
theUnitedStatesAirForceResearchLaboratoryandtheDe-
inothernumbersofdimensions,andotherboundarycon-
partmentoftheAirForceArtificialIntelligenceAccelerator
ditions, such as the Dirichlet boundary condition or the
andwasaccomplishedunderCooperativeAgreementNum-
Neumannboundarycondition. Itwillalsobeimportantto
berFA8750-19-2-1000. TheauthorsacknowledgetheMIT
developTENGforbroaderclassesofPDEsbesidesinitial
SuperCloudandLincolnLaboratorySupercomputingCen-
value problems with applications to nonlinear and multi-
terforproviding(HPC,database,consultation)resources
scalephysicsPDEsinvariousdomains.AdvancingTENGâ€™s
thathavecontributedtotheresearchresultsreportedwithin
integrationwithcutting-edgemachinelearningarchitectures
thispaper. Somecomputationsinthispaperwererunonthe
and optimizing its performance for large-scale computa-
FASRCclustersupportedbytheFASDivisionofScience
tionaltaskswillbeavitalareaofresearchforcomputational
ResearchComputingGroupatHarvardUniversity.
scienceandengineering.
BroaderImpact
Through the advancement of the Time-Evolving Natural
Gradient (TENG), which solves partial differential equa-
tions (PDEs) with enhanced accuracy and efficiency, our
workexhibitsbroaderimpactspansmultipledisciplines,in-
cludingbutnotlimitedto,climatemodeling,fluiddynamics,
materialsscience,andbiomedicalengineering. Whilethe
primarygoalofthisworkistopushforwardcomputational
techniques within the field of machine learning, it inher-
entlycarriesthepotentialforsignificantsocietalbenefits,
suchasimprovedenvironmentalforecastingmodels,more
efficientengineeringdesigns, andadvancementsinmedi-
caltechnology. Ethically,thedeploymentofTENGshould
beapproachedwithconsiderationtoensurethatenhanced
computationalcapabilitiestranslateintopositiveoutcomes
without unintended consequences given its accuracy and
reliabilityinreal-worldapplications. Therearenospecific
ethical concerns that we feel must be highlighted at this
stage;however,weacknowledgetheimportanceofongoing
evaluation of the societal and ethical implications as this
technology is applied. This acknowledgment aligns with
ourcommitmenttoresponsibleresearchandinnovation,un-
derstandingthatthetruevalueofadvancementsinmachine
learning is realized through their contribution to societal
progressandwell-being.
9TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
References Han,J.,Jentzen,A.,etal. Deeplearning-basednumerical
methodsforhigh-dimensionalparabolicpartialdifferen-
Amari,S.-I. Naturalgradientworksefficientlyinlearning.
tialequationsandbackwardstochasticdifferentialequa-
Neuralcomputation,10(2):251â€“276,1998.
tions. Communicationsinmathematicsandstatistics,5
Berman,J.andPeherstorfer,B. Randomizedsparseneural (4):349â€“380,2017.
galerkin schemes for solving evolution equations with
Han,J.,Jentzen,A.,andE,W. Solvinghigh-dimensional
deepnetworks. InThirty-seventhConferenceonNeural
partial differential equations using deep learning. Pro-
InformationProcessingSystems,2023.
ceedingsoftheNationalAcademyofSciences,115(34):
8505â€“8510,2018.
Canuto, C., Hussaini, M. Y., Quarteroni, A., and Zang,
T.A. Spectralmethods: evolutiontocomplexgeometries
Kakade,S.M.Anaturalpolicygradient.Advancesinneural
andapplicationstofluiddynamics. SpringerScience&
informationprocessingsystems,14,2001.
BusinessMedia,2007.
Koch,O.andLubich,C. Dynamicallow-rankapproxima-
Carleo,G.andTroyer,M. Solvingthequantummany-body tion. SIAMJournalonMatrixAnalysisandApplications,
problem with artificial neural networks. Science, 355 29(2):434â€“454,2007.
(6325):602â€“606,2017.
Kochkov,D.andClark,B.K.Variationaloptimizationinthe
Chen,H.,Wu,R.,Grinspun,E.,Zheng,C.,andChen,P.Y. aiera: Computationalgraphstatesandsupervisedwave-
Implicitneuralspatialrepresentationsfortime-dependent functionoptimization. arXivpreprintarXiv:1811.12423,
pdes. InInternationalConferenceonMachineLearning, 2018.
pp.5162â€“5177.PMLR,2023a.
Langley,P.Craftingpapersonmachinelearning.InLangley,
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud, P.(ed.),Proceedingsofthe17thInternationalConference
D.K. Neuralordinarydifferentialequations. Advances onMachineLearning(ICML2000),pp.1207â€“1216,Stan-
inneuralinformationprocessingsystems,31,2018. ford,CA,2000.MorganKaufmann.
Chen,Z.,Luo,D.,Hu,K.,andClark,B.K.Simulating2+1d Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-
latticequantumelectrodynamicsatfinitedensitywithneu- tacharya, K., Stuart, A., and Anandkumar, A. Fourier
ralflowwavefunctions.arXivpreprintarXiv:2212.06835, neuraloperatorforparametricpartialdifferentialequa-
2022. tions. arXivpreprintarXiv:2010.08895,2020.
Chen,Z.,Newhouse,L.,Chen,E.,Luo,D.,andSoljacic,M. Long,Z.,Lu,Y.,Ma,X.,andDong,B. Pde-net: Learning
Antn:Bridgingautoregressiveneuralnetworksandtensor pdesfromdata. InInternationalconferenceonmachine
networks for quantum many-body simulation. In Oh, learning,pp.3208â€“3216.PMLR,2018.
A.,Neumann,T.,Globerson,A.,Saenko,K.,Hardt,M.,
Lu,L.,Jin,P.,andKarniadakis,G.E. Deeponet: Learning
and Levine, S. (eds.), Advances in Neural Information
nonlinearoperatorsforidentifyingdifferentialequations
Processing Systems, volume 36, pp. 450â€“476. Curran
basedontheuniversalapproximationtheoremofopera-
Associates,Inc.,2023b.
tors. arXivpreprintarXiv:1910.03193,2019.
Dirac,P.A. Noteonexchangephenomenainthethomas
Luo, D., Chen, Z., Carrasquilla, J., andClark, B.K. Au-
atom. In Mathematical proceedings of the Cambridge
toregressiveneuralnetworkforsimulatingopenquantum
philosophical society, volume 26, pp. 376â€“385. Cam-
systemsviaaprobabilisticformulation. Physicalreview
bridgeUniversityPress,1930.
letters,128(9):090501,2022.
Donatella, K., Denis, Z., Le BoiteÂ´, A., and Ciuti, C. Dy- Luo,D.,Chen,Z.,Hu,K.,Zhao,Z.,Hur,V.M.,andClark,
namicswithautoregressiveneuralquantumstates: Ap- B.K.Gauge-invariantandanyonic-symmetricautoregres-
plication to critical quench dynamics. Physical Re- siveneuralnetworkforquantumlatticemodels. Physical
view A, 108(2), August 2023. ISSN 2469-9934. doi: ReviewResearch,5(1):013216,2023.
10.1103/physreva.108.022210.
MuÂ¨ller, J. and Zeinhofer, M. Achieving high accuracy
Du, Y. and Zaki, T. A. Evolutional deep neural network. with pinns via energy natural gradient descent. In In-
PhysicalReviewE,104(4),October2021. ISSN2470- ternationalConferenceonMachineLearning,pp.25471â€“
0053. doi: 10.1103/physreve.104.045303. 25485.PMLR,2023.
GutieÂ´rrez,I.L.andMendl,C.B. Realtimeevolutionwith Pascanu,R.andBengio,Y. Revisitingnaturalgradientfor
neural-networkquantumstates. Quantum,6:627,2022. deepnetworks. arXivpreprintarXiv:1301.3584,2013.
10TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Peters,J.,Vijayakumar,S.,andSchaal,S. Reinforcement
learning for humanoid robotics. In Proceedings of the
thirdIEEE-RASinternationalconferenceonhumanoid
robots,pp.1â€“20,2003.
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and
Battaglia, P. W. Learning mesh-based simulation with
graphnetworks. arXivpreprintarXiv:2010.03409,2020.
Raissi,M.,Perdikaris,P.,andKarniadakis,G.E. Physics-
informedneuralnetworks:Adeeplearningframeworkfor
solvingforwardandinverseproblemsinvolvingnonlinear
partialdifferentialequations. JournalofComputational
physics,378:686â€“707,2019.
Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R.,
Leskovec, J., and Battaglia, P. Learning to simulate
complexphysicswithgraphnetworks. InInternational
conferenceonmachinelearning,pp.8459â€“8468.PMLR,
2020.
Sirignano,J.andSpiliopoulos,K. Dgm: Adeeplearningal-
gorithmforsolvingpartialdifferentialequations. Journal
ofcomputationalphysics,375:1339â€“1364,2018.
Stokes,J.,Izaac,J.,Killoran,N.,andCarleo,G. Quantum
naturalgradient. Quantum,4:269,2020.
Wang,J.,Chen,Z.,Luo,D.,Zhao,Z.,Hur,V.M.,andClark,
B. K. Spacetime neural network for high dimensional
quantum dynamics. arXiv preprint arXiv:2108.02200,
2021a.
Wang,S.andPerdikaris,P. Long-timeintegrationofpara-
metricevolutionequationswithphysics-informeddeep-
onets. JournalofComputationalPhysics,475:111855,
2023.
Wang,S.,Wang,H.,andPerdikaris,P.Learningthesolution
operatorofparametricpartialdifferentialequationswith
physics-informed deeponets. Science advances, 7(40):
eabi8605,2021b.
Weinan,E.,Han,J.,andJentzen,A. Algorithmsforsolving
high dimensional pdes: from nonlinear monte carlo to
machinelearning. Nonlinearity,35(1):278,2021.
Yu,B.etal.Thedeepritzmethod:adeeplearning-basednu-
mericalalgorithmforsolvingvariationalproblems. Com-
munications in Mathematics and Statistics, 6(1):1â€“12,
2018.
Zhang,G.,Martens,J.,andGrosse,R.B. Fastconvergence
ofnaturalgradientdescentforover-parameterizedneural
networks. AdvancesinNeuralInformationProcessing
Systems,32,2019.
11TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
A.AdditionalTheoreticalResults
TheoremA.1. TDVPisnotreparameterizationinvariantwithâˆ†t.
Proof. Wewillconstructanexplicitcounter-example. Forsimplicity,considerazero-dimensionalPDE(anODE)âˆ‚ u=u,
t
whosesolutionisu = u exp(t). LetuË† = Î¸ andvË† = expÏ•,whicharejustreparameterizationsofeachother. Inthe
0 Î¸ Ï•
parameterspace,thetwoODEsreadâˆ‚ Î¸ =Î¸andâˆ‚ Ï•=1. Letbothofthemevolveforadiscretetimestepâˆ†tfromt=0,
t t
wehaveÎ¸ =Î¸ +âˆ†tÎ¸ andÏ• =Ï• +âˆ†t.Pluggingbackintothefunctions,uË† =uË† +âˆ†tuË† andvË†=vË† exp(âˆ†t)
âˆ†t 0 0 âˆ†t 0 Î¸âˆ†t Î¸0 Î¸0 Î¸0
Itisevidentialthatthetwoparameterizationsgivedifferentsolutions.
TENGcanbereducedtoTDVPundercertainassumptions. Forsimplicity,wewillbefocusingonthefirst-orderEulerâ€™s
method. ConsiderSubroutineTENG stepper. LetthelossfunctionL(uË† ,u )=âˆ¥uË† âˆ’u âˆ¥2 andN =1.
Î¸t target Î¸t target L2(X) it
Forsimplicity,letu =uË† +âˆ†tLuË† bethefirst-orderEulerexpansion. Then,
target Î¸t Î¸t
âˆ‚L
(x)=2(uË† (x)âˆ’u (x))â‰¡2âˆ†tLuË† . (A.1)
âˆ‚uË† Î¸t target Î¸t
Î¸t
ChoosingÎ±=1/2,wehaveâˆ†u=âˆ†tLuË† . Then,theleastsquareequationbecomes
Î¸t
(cid:13) (cid:13)2
âˆ†Î¸ =argmin(cid:13)âˆ†tLuË† (Â·)âˆ’(cid:80) J âˆ†Î¸ (cid:13) , (A.2)
âˆ†Î¸âˆˆRNp
(cid:13) Î¸t j (Â·),j j(cid:13)
L2(X)
whichisthesameastheTDVPalgorithmwithnonzerotimestepsizes.
TENGcanbereducedtoOBTIundercertainassumptions. LetN >0. Asmentionedinthemainpaper,approximate
it
methodscanbeusedtosolvetheleastsquareequationinSubroutineTENG stepper.Here,letitssolutionbeapproximated
byasinglegradientdescent,whichgivesriseto
(cid:90) (cid:90) âˆ‚uË† (x) âˆ‚L âˆ‚L
âˆ†Î¸ = J âˆ†u(x)dxâ‰¡âˆ’Î± Î¸ (x)dx=âˆ’Î± , (A.3)
j (x),j âˆ‚Î¸ âˆ‚uË† âˆ‚Î¸
j Î¸ j
whichreducestotheregulargradientdescentintheÎ¸-spacewithmanyiterations.
HilbertnaturalgradientformulationofSubroutineTENG stepper. Considertheleastsquareequation
(cid:13) (cid:13)2
âˆ†Î¸ =argmin(cid:13)âˆ†u(Â·)âˆ’(cid:80) J âˆ†Î¸ (cid:13) . (A.4)
(cid:13) j (Â·),j j(cid:13)
âˆ†Î¸âˆˆRNp L2(X)
Itâ€™ssolutionisgivenbythenormalequationJTJâˆ†Î¸ =JTâˆ†uwhereweusethematrixnotationandomittheindices. The
solutiontothenormalequationisgivenbyâˆ†Î¸ =(JTJ)âˆ’1JTâˆ†u.Noticethat
(cid:90) (cid:90) âˆ‚uË† (x) âˆ‚L âˆ‚L
(JTâˆ†u) = J âˆ†u(x)dxâ‰¡âˆ’Î± Î¸ (x)dx=âˆ’Î± . (A.5)
j (x),j âˆ‚Î¸ âˆ‚uË† âˆ‚Î¸
j Î¸ j
Inaddition,
(cid:90) âˆ‚uË† (x)âˆ‚uË† (x)
(JTJ) = Î¸ Î¸ dxâ‰¡G (Î¸), (A.6)
i,j âˆ‚Î¸ âˆ‚Î¸ i,j
i j
whereG(Î¸)istheHilbertgrammatrix(MuÂ¨ller&Zeinhofer,2023). Therefore, Eq.(A.4)canbewrittenastheHilbert
naturalgradientdescent
âˆ†Î¸
=âˆ’Î±(cid:88)
Gâˆ’1(Î¸)
âˆ‚L(uË† Î¸,u target)
(x). (A.7)
i i,j âˆ‚Î¸
j
j
Wenotethatwhilethesetwoformulationsaremathematicallyequivalent,theleastsquareformulationhasafewpractical
advantages. First,itallowsformorestablenumericalsolvers. Ingeneral,theHilbertgrammatrixhasaconditionnumber
twiceaslargeastheoriginalJacobianmatrix. Iftheoriginalleastsquareequationisill-conditioned,Eq.(A.7)isevenworse.
Inaddition,whentheleastsquareequationisunderdetermined,solvingtheoriginalleastsquareproblemgivestheminimum
normsolution,whereasEq.(A.7)hastobesolvedwithpseudo-inverse,whichcanbenumericallyunstableinpractice.
12TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Generalized Gaussâ€“Newton formulation of Subroutine TENG stepper. Let the loss function be the squared L2-
distance. Definer(x):=uË† (x)âˆ’u (x).Thederivativeoflossinfunctionspaceisgivenby
Î¸ target
âˆ‚L
(x)=2(uË† (x)âˆ’u (x))â‰¡2r(x). (A.8)
âˆ‚uË† Î¸ target
Î¸
Inmatrixnotation,theiterationabovebecomes
âˆ†Î¸ =âˆ’Î±(JTJ)âˆ’1JTr. (A.9)
WhenÎ±=1/2,thisreducestooneiterationoftheGaussâ€“Newtonmethod. Therefore,SubroutineTENG steppercan
alsobeviewedasageneralizedGaussâ€“Newtonmethod.
TheoremA.2. TheerrorÎµ (t)isboundedbyÎµEE(t)+ÎµTE(t),whereÎµEE(t)isanorderO(âˆ†t)quantity,andÎµTE(t)=
p p p p p
(cid:13) (cid:13)
(cid:13)(cid:80)t/âˆ†tâˆ’1Gnr(Â·,tâˆ’nâˆ†t)(cid:13)
.
(cid:13) n=0 (cid:13)
Lp(X)
Proof. Denote D(Â·,t) = u(Â·,t) âˆ’ uË† (Â·,t) = (u(Â·,t) âˆ’ uEu(Â·,t)) + (uEu(Â·,t) âˆ’ uË† (Â·,t)) â‰¡ DEE(Â·,t) + DTE(Â·,t),
Î¸ Î¸
where u(Â·,t) is the exact solution, uEu(Â·,t) is the solution from Euler method, uË† (Â·,t) is the TENG-Euler solu-
Î¸
(cid:13) (cid:13) (cid:13) (cid:13)
tion at time t. By definition, Îµ p(t) = âˆ¥D(Â·,t)âˆ¥ Lp(X), (cid:13)u(Â·,t)âˆ’uEu(Â·,t)(cid:13)
Lp(X)
= (cid:13)DEE(Â·,t)(cid:13)
Lp(X)
= ÎµEE(t),
(cid:13) (cid:13) (cid:13) (cid:13)
and (cid:13)uEu(Â·,t)âˆ’uË†(Â·,t)(cid:13)
Lp(X)
= (cid:13)DTE(Â·,t)(cid:13)
Lp(X)
= ÎµTE(t). It follows by the triangular inequality that Îµ p(t) â‰¤
ÎµEE(t)+ÎµTE(t). SincetheEulermethodisafirst-ordermethod,ÎµEE(t)hasanerroroforderO(âˆ†t).
p p p
Denotetheoptimizationerrorintimetasr(Â·,t),suchthatuË† (Â·,t+âˆ†t)âˆ’GuË† (Â·,t)=r(Â·,t).ItfollowsthatuEu(Â·,t+âˆ†t)âˆ’
Î¸ Î¸
DTE(Â·,t+âˆ†t)âˆ’G(uEu(Â·,t)âˆ’DTE(Â·,t))=r(Â·,t),whichimpliesthatDTE(Â·,t+âˆ†t)=GDTE(Â·,t)âˆ’r(Â·,t)duetothecan-
(cid:13) (cid:13)
cellationofuEu(Â·,t)âˆ’GuEu(Â·,t)fromtheexactEulermethod.Byinduction,ÎµTE(t)=(cid:13)(cid:80)t/âˆ†tâˆ’1Gnr(Â·,tâˆ’nâˆ†t)(cid:13)
.
p (cid:13) n=0 (cid:13)
Lp(X)
B.AdditionalExperimentalResults
Inthissection,weshowadditionalbenchmarkresults. InFig.B.1,weshowthetraininglossesofTENGandOBTImethods
duringthetimestepsatT =0.8,1.6,2.4,3.2,4.0forAllenâ€“Cahnequation. ForEulerâ€™sintegrationscheme,wecompare
ourTENG-EulermethodwithbothOBTI-AdamandOBTI-LBFGSalgorithmsandfindthatouralgorithmconsistently
achieveslossvaluesordersofordersofmagnitudesbetterthanOBTI,withonly7iterations. ForTENG-Heunmethod,each
timesteprequirestrainingasetofintermediateparameters. Therefore,eachfigureincludestwocurves. Asshowninthe
figure,allstagesconvergetomachineprecisionwithinasmallnumberofiterations.
InFig.B.2B.3andB.4,weshowthedensityplotsforthetwo-dimensionalheatequation,Burgersâ€™equationandAllenâ€“Cahn
equation. Ineachfigure,weplotthereferencesolution(seeAppendixDfordetailsonobtainingthereferencesolution),
theTENG-Heunsolution,theTDVP-RK4solution,theOBTI-LBFGSsolutionandthePINN-BFGSsolution,andtheir
differencetothereferencesolution. Inallcases,theTENG-Heunsolutioncloselytracksthereferencesolution,witha
maximumerroroforderO(10âˆ’6),whereassolutionsgeneratedbyothermethodscanhaverelativelylargersolutions.
InFig.B.5,weplottheglobalrelativeL2-errorofPINNduringtraining. WeshowbothPINN-ENGDandPINN-BFGSfor
theheatequation,andPINN-BFGSforAllenâ€“CahnequationandBurgersâ€™equation. WeobservethatwhilePINN-ENGD
converges very quickly on the heat equation, PINN-BFGS eventually surpases PINN-ENGD. In addition, Allenâ€“Cahn
equationandBurgersâ€™equationappeartobesignificantlymorechallengingforPINN,whereitfindsdifficultyoptimizing
theerrortobelowO(10âˆ’2).
InFig.B.6,wefurtherexploretheadvantageofhigher-orderintegrationschemes. Inparticular,weplottherelativeL2-error
asafunctionoftimeforTENG-Euler,TENG-Heun,andTENG-RK4.Fortheheatequation,TENG-RK4failstosignificantly
surpassTENG-Heun,whichcouldbeattributedtotheerroraccumulationundersmallâˆ†tinthiscase. Fortheothertwo
equations,weexplorelargerâˆ†tandfindthatTENG-RK4isabletoachievesmallerrors,whileTENG-Heunâ€™sperformance
startstodeteriorate.
InTable.B.1,wereporttheruntimefortherunsinFig.2. OurTENGmethodssignificantlyimprovethesimulationaccuracy
withasimilarruntimetootheralgorithms. WenotethatTENG-HeunisroughlytwiceascostlyasTENG-Eulerduetothe
13TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
T=0.8 T=1.6 T=2.4 T=3.2 T=4.0
10 4
10 7
TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours)
10 10 O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S
10 13
10 16
1 7 50 150 300 1 7 50 150 300 1 7 50 150 300 1 7 50 150 300 1 7 50 150 300
Iteration Iteration Iteration Iteration Iteration
10 4 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1
TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2
10 7
10 10
10 13
10 16
1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7
Iteration Iteration Iteration Iteration Iteration
FigureB.1.TraininglossduringmanytimestepsforTENG-Euler,TENG-Heun,andthetwoOBTImethodsforAllenâ€“Cahnequation
withatimestepsizeâˆ†t=0.005.TENG-Heunmethodrequirestwotrainingstages,oneforÎ¸ ,andtheotherforÎ¸ .Therefore,
temp t+âˆ†t
eachfigurecontainstwocurves.
Heat Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.0000005 0.0001
0.0000000 0.0000
0.0000005 0.0001
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.0025 0.00001
0.0000 0.00000
0.0025 0.00001
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.2.Exact,TENG,TDVP,OBTI,andPINNsolutionsandtheirdifferencesfromthereferencesolutionforthetwo-dimensional
heatequation.Thereferencesolutionisgeneratedusingtheanalyticalsolution,theTENGsolutionshownhereusestheTENG-Heun
method,theOBTIshownhereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.Theerrorofour
TENGmethodisordersofmagnitudesmallerthanothermethods.
two-stagetrainingprocessineachtimestep. Inaddition,allsequential-in-timemethodsusesignificantlymoretimeon
14
ssoL
gniniarT
ssoL
gniniarT
petS
emiT
gnirud
petS
emiT
gnirud
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPuTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Burgers' Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.000005 0.0025
0.000000 0.0000
0.000005 0.0025
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.1 0.1
0.0 0.0
0.1 0.1
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.3.Exact,TENG,TDVP,OBTIandPINNsolutionsandtheirdifferencesfromthereferencesolutionforBurgersâ€™equation.The
referencesolutionisgeneratedusingthespectralmethod,theTENGsolutionshownhereusestheTENG-Heunmethod,theOBTIshown
hereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.TheerrorofourTENGmethodisordersof
magnitudesmallerthanothermethods.
Allen Cahn Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.000025 0.005
0.000000 0.000
0.000025 0.005
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.02
0.25
0.00 0.00
0.25
0.02
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.4.Exact,TENG,TDVP,OBTIandPINNsolutionsandtheirdifferencesfromthereferencesolutionforAllenâ€“Cahnequation.
Thereferencesolutionisgeneratedusingthespectralmethod,theTENGsolutionshownhereusestheTENG-Heunmethod,theOBTI
shownhereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.TheerrorofourTENGmethodis
ordersofmagnitudesmallerthanothermethods.
15
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPuTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Heat Equation Heat Equation Allen Cahn Equation Burgers' Equation
101
PINN-ENGD PINN-BFGS PINN-BFGS PINN-BFGS
100
10 1
10 2
10 3
10 4
0 2000 4000 0 50000 100000 0 50000 100000 0 50000 100000
Iteration Iteration Iteration Iteration
FigureB.5.GlobalrelativeL2-errorforPINNasafunctionoftrainingiterationsforthetwo-dimensionalheatequation,Allenâ€“Cahn
equation,andBurgersâ€™equation.
Heat Equation Allen Cahn Equation Burgers' Equation
100 100 100
TENG-Euler TENG-Euler TENG-Euler
TENG-Heun TENG-Heun TENG-Heun
10 2 TENG-RK4 10 2 TENG-RK4 10 2 TENG-RK4
10 4 10 4 10 4
10 6 10 6 10 6
10 8 10 8 10 8
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4
Time Time Time
FigureB.6.RelativeL2-errorasafunctionoftimeforthetwo-dimensionalheatequation(âˆ†t=0.005),Allenâ€“Cahnequation(âˆ†t=
0.01),andBurgersâ€™equation(âˆ†t=0.01)forvariousintegrationmethods.
Burgersâ€™equation,duetothereducedtimestepâˆ†t. WhiletheresultofPINNcouldbenefitfromalongertrainingprocess
fortheBurgersâ€™equation,webelieveitisunlikelyasshowninthetrainingdynamicsinFig.B.5. InFig.B.7,weplotthe
globalrelativeL2-errorasafunctionofruntime,withvariouschoicesofhyperparameterslistedinAppendixE.Thefigure
showsthatTENGachievessignificantlylowererrorcomparedtoothermethods,evenforlowruntimes. (Thefivepoints
withthehighesterrorsforTENGallusetheEulerintegrationscheme,wherethedominanterroristheEulerdiscretization
error.) WenotethatallexperimentsareperformedonasingleNVIDIAV100GPUwith32GBmemory. Inallcases,the
32GBmemoryissufficientforourbenchmarks.
Runtime(Hours)
Method
Heat Allenâ€“Cahn Burgersâ€™
TENG-Euler(Ours) 2.5 2.5 12.7
TENG-Heun(Ours) 4.1 4.2 20.9
TDVP-RK4 4.6 4.4 21.1
OBTI-Adam 3.0 3.2 19.6
OBTI-LBFGS 4.4 4.1 22.1
PINN-ENGD 1.1 â€“ â€“
PINN-BFGS 2.0 2.9 3.6
TableB.1.Runtimeforvariousalgorithmsforthetwo-dimensionalheatequation,Allenâ€“Cahnequation,andBurgersâ€™equation.
16
rorrE-2L
evitaleR
labolG
rorrE-2L
evitaleR
rorrE-2L
evitaleR
rorrE-2L
evitaleRTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
10 2
10 3
10 4
TENG (Ours)
10 5 TDVP
OBTI
PINN
103 104 105
Runtime (Second)
FigureB.7. GlobalrelativeL2-errorasafunctionofruntimeforvariousalgorithmsundervarioushyperparameters.
C.AdditionalInitialConditionsandBenchmarks
Asmentionedinthemainpaper,forthethree-dimensionalheatequation,weconsiderainitialconditionintheform
2 2 2 (cid:32) 3 3 (cid:33)
(cid:88) (cid:88) (cid:88) (cid:89) (cid:89)
u (x ,x ,x )=A + A cos(k x )+B sin(k x ) . (C.10)
0 1 2 3 000 k1k2k3 i i k1k2k3 i i
k1=1k2=1k3=1 i=1 i=1
Here,wechoosethefollowingcoefficients: A =0.043withtherestofA â€™sandA â€™sshowninTableC.2.
000 k1k2k3 k1k2k3
k =1 k =2
A 1 1
k1k2k3 k =1 k =2 k =1 k =2
2 2 2 2
k =1 0.047 -0.021 0.034 -0.02
3
k =2 -0.021 -0.041 0.024 0
3
k =1 k =2
B 1 1
k1k2k3 k =1 k =2 k =1 k =2
2 2 2 2
k =1 -0.075 -0.056 -0.027 -0.008
3
k =2 0.074 -0.007 0.032 0
3
TableC.2. A â€™sandB â€™sfortheinitialconditionofthree-dimensionalheatequation.
k1k2k3 k1k2k3
In addition, we consider an example of the heat equation defined on a two-dimensional disk with Dirichlet boundary
condition. Here,theboundaryconditionisenforcedviaanadditionallossterminEq.(3),andtheinitialconditionisshown
below.
(cid:18)
1 1 1 1
u (r,Î¸)= Z (r,Î¸)âˆ’ Z (r,Î¸)+ Z (r,Î¸)âˆ’ Z (r,Î¸)
0 4 01 4 02 16 03 64 04
(C.11)
(cid:19)
1 1 1
+Z (r,Î¸)âˆ’ Z (r,Î¸)+ Z (r,Î¸)âˆ’ Z (r,Î¸)+Z (r,Î¸)+Z (r,Î¸)+Z (r,Î¸) ,
11 2 12 4 13 8 14 21 31 41
whererandÎ¸isthepolarcoordinatevariablesandZ representthediskharmonicsdefinedas
mn
Z (r,Î¸)=J (Î» r)cos(mÎ¸) (C.12)
mn m nm
withJ themthBesselfunctionandÎ» thenthzeroofthemthBesselfunction. Wenotethatwhiletheanalyticalsolution
m nm
is solved in the polar coordinates, all neural network based methods solve the equation and benchmark in the original
Cartesiancoordinates.
ForBurgersâ€™equation,we,inaddition,considerbenchmarksthatincludeacasewithsmallerÎ½ =3/1000withtheoriginal
domain,boundary,andinitialconditions,andacasewithÎ½ =1/100butwithnonequaldomainX =[0,2)Ã—[0,2Ï€)with
17
rorrE-2L
evitaleR
labolGTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
periodicboundarycondition,andT =[0,4],andthefollowinginitialcondition.
1
u (x ,x )= exp(cos(Ï€x âˆ’2)+sin(x âˆ’1))2. (C.13)
0 1 2 50 1 2
Heat Equation on a Disk Burgers' Equation =0.003 Burgers' Equation New u 0
TENG (Ours) TDVP OBTI PINN
100 100 100
10 2 10 2 10 2
10 4 10 4 10 4
10 6 10 6 10 6
10 8 10 8 10 8
0.0 0.5 1.0 1.5 2.0 0 1 2 3 4 0 1 2 3 4
Time Time Time
FigureC.8.RelativeL2-errorasafunctionoftimeforadditionalbenchmarks.Forallsequential-in-timemethods,wechoosetimestep
sizeâˆ†t=0.005fortheheatequationandâˆ†t=0.001forBurgersâ€™equation.
InFig.C.8,weshowtheadditionalbenchmarksfortheaforementionedexamples. Here,TENGreferstotheTENG-Heun
method,OBTIreferstotheOBTI-LBFGSmethod,andPINNreferstothePINN-BFGSmethod. Wenotethatfortheheat
equationonadiskwithDirichletboundarycondition,anadditionalboundarytermisincludedinthelossfunctiondefinedin
Eq.(3)forTENGandOBTImethod. (PINNcanalsoincorporatethisboundarytermanalogously.) However,itisunclear
howtoenforcetheboundaryconditioninTDVPwithoutredesigningtheneuralnetworkarchitecture;therefore,wechoose
tonotenforcetheboundaryconditionforTDVP,whichcouldbethereasonwhyTDVPperformsparticularlybadlyonthe
heatequationonadisk.
D.DetailsonObtainingReferenceSolutions
Heatequation. Asmentionedinthemainpaper,theheatequationpermitsananalyticalsolutionintermsofFourierseries.
Forexample,weshowthetwo-dimensionalcasebelow.
u(x ,x ,t)=
(cid:88) exp(cid:0) âˆ’Î½(cid:0) k2+k2(cid:1) t(cid:1)
uËœ (k ,k )exp(ik x +ik x ), (D.14)
1 2 1 2 0 1 2 1 1 2 2
k1,k2
whereweomittheterms2Ï€/P becauseinourcaseP =2Ï€. Forthetwo-dimensionalcase,evaluatingtheanalyticalsolution
isnotpracticalsinceitisdifficulttoexpressourinitialconditioninFourierseriesanalytically,
(cid:88)
uËœ (k ,k )= u (x ,x )exp(âˆ’ik x âˆ’ik x ) (D.15)
0 1 2 0 1 2 1 1 2 2
x1,x2
not to mention calculating an infinite sum of frequencies. Therefore, we choose to evaluate the initial condition on a
2048Ã—2048=4194304grid. Then,weusethediscreteFouriertransformtocalculatetheinitialconditionintheFourier
space,beforetruncatingthemaximumfrequency48. (Thesummationcontainsaround(2Â·48)2 â‰ˆ9000termsintotal). For
thethree-dimensionalcaseandthecasewherethedomainisadisk,sincetheinitialconditionisalreadydefinedintermsof
sinusoidalfunctions(orBesselfunctions),thesolutionisanalyticallycalculated.
Allenâ€“Cahnequation.Differentfromtheheatequation,Allenâ€“Cahnequationgenerallydoesnotpermitanalyticalsolutions.
Therefore,wesolveitusingthespectralmethodandconsiderthesolutionasaproxyfortheexactsolutionasthereference.
Here,thebasisfunctionsofthespectralmethodarechosentobethesameFourierplanewaves,sothesolutioninrealspace
canbewrittenas
(cid:88)
u(x ,x ,t)= uËœ(k ,k ,t)exp(ik x +ik x ). (D.16)
1 2 1 2 1 1 2 2
k1,k2
18
rorrE-2L
evitaleR
rorrE-2L
evitaleR
rorrE-2L
evitaleRTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
WhenswitchingfromrealspacetoFourierspace,wehave
âˆ‚u
â†’ik uËœ and uv â†’uËœâ—¦vËœ, (D.17)
âˆ‚x j
j
whereâ—¦meansconvolution. Therefore,thePDEcanberewrittenintheFourierspaceas
âˆ‚uËœ
=âˆ’Î½(k2+k2)uËœ+uËœâˆ’uËœâ—¦uËœâ—¦uËœ. (D.18)
âˆ‚t 1 2
Here,wechooseamaximumfrequencycut-offof128. (Noticethatthemaximumnumberoffrequenciesencounteredis
(3Â·2Â·128)2 â‰ˆ600000whencalculatingthedoubleconvolution.) Theinitialconditioniscalculatedanalogoustothecase
oftheheatequation,viaadiscreteFouriertransformonthe2048Ã—2048=4194304grid. Then,Eq.(D.18)issolvedusing
thefourth-orderRungeâ€“Kuttaintegrationschemewithatimestepâˆ†t=2Ã—10âˆ’4.
Burgersâ€™equation. AnalogoustoAllenâ€“Cahnequation,Burgerâ€™sequationdoesnothaveageneralanalyticalsolutioneither,
exceptinthecaseofÎ½ =0. Therefore,weusethesamespectralmethodusedtosolveAllenâ€“Cahnequation. Noticethatthe
termuâˆ‚u/âˆ‚x =âˆ‚u2/âˆ‚x . Therefore,Burgersâ€™equationintheFourierspacereads
j j
âˆ‚uËœ i
=âˆ’Î½(k2+k2)uËœâˆ’ (k +k )uËœâ—¦uËœ. (D.19)
âˆ‚t 1 2 2 1 2
Here,wechooseamaximumfrequencycut-offof192(withamaximumofaround(2Â·2Â·192)2 â‰ˆ 600000termswhen
calculatingtheconvolution.) TheinitialconditioniscalculatedinthesamewayastheheatandAllenâ€“Cahnequation,and
Eq.(D.19)issolvedusingthefourth-orderRungeâ€“Kuttaintegrationschemewithatimestepâˆ†t=1Ã—10âˆ’4.
Accuracyofthesolutions. Ineachcase,wecarefullyverifythatthenumberofgridpoints,themaximumfrequency,and
theâˆ†taresufficienttoobtainasolutionthatisaccuratetonearnumericalprecision,byvaryingthemovermultiplevalues
andobservingthatthesolutionconverges. WenotethatthecaseforBurgersâ€™equationwithÎ½ =0.003ischallengingforthe
spectralmethodandthesolutionmaynotconvergeyet,whichmeanstheerrorswereportcouldbelargerthantheactual
values.
E.DetailsofNeuralNetworkArchitecturesandOptimization
AllthealgorithmsusedinthisworkareimplementedinJAXandusedoubleprecision. OurcodewillbepostedtoGitHub
subsequenttotheacceptanceofthiswork.
Neuralnetworkarchitectures. Wechoosethesamearchitectureforallsequential-in-timemethods,whichallowsafair
comparison. OurneuralnetworkarchitectureislooselybasedonRef.(Berman&Peherstorfer,2023)whichconsistsof
multiplefeedforwardlayerswithtanhactivationfunctionas
uË†(x)=W tanh(Â·Â·Â·tanh(W periodic embed(x)+b )Â·Â·Â·)+b , (E.20)
nl 1 1 nl
wheretheperiodicembeddingfunctionisdefinedas
(cid:16)(cid:104) (cid:105)(cid:17)
(cid:80) (cid:80)
periodic embed(x)=concatenate a cos(x +Ï• )+c , a cos(x +Ï• )+c (E.21)
j j 1 j j j j 2 j j
toexplicitlyenforcetheperiodicboundaryconditionintheneuralnetwork. Here,allW,b,a,andcaretrainableparameters.
Here,wechoosen =7layersandd =40hiddendimensions(periodicembeddingvectorwithsize20foreachx ).
l h j
ForPINN,weadoptthesamearchitecturefromRef.(MuÂ¨ller&Zeinhofer,2023)withtheadditionofperiodicembedding.
Inaddition,weincreasethehiddendimensionto64comparedtoRef.(MuÂ¨ller&Zeinhofer,2023)forbetterexpressivity.
Inthecaseoftheheatequationona2Ddisk,wesimplyremovetheperiodicembeddinglayer.
Optimizationmethods. ForTENG,werandomlysub-sampletrainableparameterswhensolvingtheleastsquareproblems.
Thiscanbeviewedasaregularizationmethodwhentheoriginalleastsquareproblemisill-conditionedandcansignificantly
reducethecomputationalcost. Duringeachtimestep,werandomlysub-sample1536parametersinthefirstiterationand
sub-sample1024parametersinthesubsequentiterations. InTENG-Euler,theneuralnetworkisoptimizedfor7iterationsin
19TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
eachtimestep;inTENG-Heun,theneuralnetworkisoptimizedfor7iterationstoobtainÎ¸ ,followedby5iterationsfor
temp
Î¸ . WereducethenumberofiterationsinthesecondstagebecauseÎ¸ alreadygivesagoodinitializationforÎ¸ .
t+âˆ†t temp t+âˆ†t
ForTDVP,weusethesparseupdatemethodproposedbyRef.(Berman&Peherstorfer,2023),whichisalsoarandom
sub-sample of parameters for each TDVP step, and has been shown to significantly improve the result compared to a
full update of a smaller neural network. Here, we randomly sub-sample 2560 parameters at each time step so that the
computationalcostofTDVPateachtimesteproughlymatchesthatofTENG(overthetrainingiterationswithineachtime
step).
ForOBTI,wecompareourmethodwithboththeAdamoptimizerandtheL-BFGSoptimizer. Withineachtimestep,the
neuralnetworkisoptimizedfor300iterationswhenusingtheAdamoptimizer,and150iterationswhenusingtheL-BFGS
optimizer. The Adam optimizer uses an initial learning rate of 1Ã—10âˆ’5 and an exponential scheduler that decays the
learningrateby1/2bytheendofthe300iterations.
Forallsequential-in-timemethods,weneedtotraintheinitialparameterstomatchtheinitialconditions. Hereweusethe
sameinitialparametersforafaircomparison. Theinitialparametersaretrainedbyfirstminimizingthelossfunction
L(uË† Î¸,u 0)=âˆ¥uË† Î¸âˆ’u 0âˆ¥2 L2(X)+(cid:13) (cid:13) (cid:13) (cid:13)âˆ‚ âˆ‚u xË† Î¸ âˆ’ âˆ‚ âˆ‚u x0(cid:13) (cid:13) (cid:13) (cid:13)2 +(cid:13) (cid:13) (cid:13) (cid:13)âˆ‚ âˆ‚u xË† Î¸ âˆ’ âˆ‚ âˆ‚u x0(cid:13) (cid:13) (cid:13) (cid:13)2 (E.22)
1 1 L2(X) 2 2 L2(X)
usingnaturalgradientdescent,whereweusetheleastsquareformulationasmentionedinthemainpaperand(approximately)
solvetheleastsquareproblemusingCGLSmethod,untilthelossvaluedecaysbelow1Ã—10âˆ’7. Then,weswitchtheloss
functionto
L(uË† ,u )=âˆ¥uË† âˆ’u âˆ¥2 (E.23)
Î¸ 0 Î¸ 0 L2(X)
andusetherandomsub-sampleversionofthenaturalgradientdescent,with1536parametersupdatedforeachiteration
untilthelossvaluedecaystonearmachineprecision(1Ã—10âˆ’14). TheL2-norminbothstagesareintegratedona2Dgrid
of1024pointsineachdimension(around1000000pointsintotal).
ForPINN,boththeinitialconditionandthetimeevolutionareoptimizedsimultaneously;therefore,itdoesnotusethe
initialparametersmentionedabove. Inaddition,allthetimestepsofPINNareoptimizedsimultaneously,insteadofstepby
step. Fortheoptimization,wetesttheBFGSoptimizer,andtherecentlyproposedENGDoptimizer(Berman&Peherstorfer,
2023). We note that the ENGD optimizer requires custom implementation for individual PDEs. Since Ref. (Berman
&Peherstorfer,2023)didnotprovidetheimplementationforAllenâ€“CahnequationandBurgersâ€™equation,weomitthe
benchmarkofENGDoptimizerforthetwoequations. Wetraintheneuralnetworkfor100000iterationswhenusingthe
BFGSoptimizer,and4000iterationswhenusingtheENGDoptimizer.
ForFigB.7,theresultsincludevarioushyperperameters. Forallsequential-in-timemethods,weincludedifferenttimestep
sizesâˆ†t=0.0016,0.0032,0.005,0.01and0.02. ForTENG,weincludeTENG-Euler,TENG-Heun,andTENG-RK4with
differentnumbersofiterations(withineachtimestep)rangingfrom2to20anddifferentnumbersofrandomlysubselected
parameters for solving least squares (within each iteration) ranging from 384 to 2048; for TDVP, we include different
numbersofrandomlysubselectedparametersforsolvingleastsquareprojectionsrangingfrom384to2560(wherewereach
thememorylimitofV100GPU);forOBTI,weincludebothOBTI-AdamandOBTI-LBFGSwithdifferentnumbersof
iterations(withineachtimestep)rangingfrom150to300;andforPINN,weusetheBFGSoptimizer,andincluderesultsof
differentnumberofiterations(globally)anddifferentneuralnetworksizes.
20