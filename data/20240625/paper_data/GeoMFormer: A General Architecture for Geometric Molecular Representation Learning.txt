GeoMFormer: A General Architecture for
Geometric Molecular Representation Learning
TianlangChen*1 ShengjieLuo*2 DiHe2 ShuxinZheng3 Tie-YanLiu3 LiweiWang24
Abstract 1.Introduction
Deeplearningapproacheshaveemergedasapowerfultool
Molecularmodeling,acentraltopicinquantum
forawiderangeoftasks(Heetal.,2016;Devlinetal.,2019;
mechanics,aimstoaccuratelycalculatetheprop-
Brownetal.,2020). Recently,researchershavestartedin-
ertiesandsimulatethebehaviorsofmolecularsys-
vestigatingwhetherthepowerofneuralnetworkscouldhelp
tems. Themolecularmodelisgovernedbyphys-
solveproblemsinphysicsandchemistry,suchaspredicting
ical laws, which impose geometric constraints
thepropertyofmoleculeswith3Dcoordinatesandsimu-
suchasinvarianceandequivariancetocoordinate
lating how each atom moves in Euclidean space (SchÃ¼tt
rotation and translation. While numerous deep
etal.,2018;Gasteigeretal.,2020b;Satorrasetal.,2021).
learningapproacheshavebeendevelopedtolearn
Thesemolecularmodelingtasksrequirethelearnedmodel
molecularrepresentationsundertheseconstraints,
tosatisfygeneralphysicallaws,suchastheinvarianceand
mostofthemarebuiltuponheuristicandcostly
equivariance conditions: The modelâ€™s prediction should
modules. We argue that there is a strong need
reactphysicallywhentheinputcoordinateschangeaccord-
forageneralandflexibleframeworkforlearning
ingtothetransformationofthecoordinatesystem,suchas
both invariant and equivariant features. In this
rotationandtranslation.
work, we introduce a novel Transformer-based
molecularmodelcalledGeoMFormertoachieve Avarietyofmethodshavebeenproposedtodesignneural
thisgoal. UsingthestandardTransformermod- architectures that intrinsically satisfy the invariance or
ules,twoseparatestreamsaredevelopedtomain- equivarianceconditions(Thomasetal.,2018;SchÃ¼ttetal.,
tainandlearninvariantandequivariantrepresenta- 2021;Batzneretal.,2022). Tosatisfytheinvariantcondi-
tions. Carefullydesignedcross-attentionmodules tion,severalapproachesincorporateinvariantfeatures,such
bridgethetwostreams,allowinginformationfu- astherelativedistancebetweeneachatompair,intoclassic
sionandenhancinggeometricmodelingineach neuralnetworks(SchÃ¼ttetal.,2018;Shietal.,2022). How-
stream. As a general and flexible architecture, ever,thismayhinderthemodelfromeffectivelyextracting
weshowthatmanypreviousarchitecturescanbe the molecular structural information (Pozdnyakov et al.,
viewedasspecialinstantiationsofGeoMFormer. 2020;Joshietal.,2023). Forexample,computingdihedral
Extensiveexperimentsareconductedtodemon- angles from coordinates is straightforward but requires
stratethepowerofGeoMFormer. Allempirical much more operations using relative distances (SchÃ¼tt
results show that GeoMFormer achieves strong etal.,2021). Tosatisfytheequivariantcondition,several
performance on both invariant and equivariant works design neural networks with equivariant operation
tasksofdifferenttypesandscales. Codeandmod- only,suchastensorproductbetweenirreduciblerepresenta-
els will be made publicly available at https: tions(Thomasetal.,2018;Fuchsetal.,2020;Batzneretal.,
//github.com/c-tl/GeoMFormer. 2022)andvectoroperations(Satorrasetal.,2021;SchÃ¼tt
et al., 2021; ThÃ¶lke & De Fabritiis, 2022). However, the
numberofsuchoperationsarelimitedduetotheequivariant
*Equal contribution 1School of EECS, Peking University constraints, which are either costly to scale or lead to
2National Key Laboratory of General Artificial Intelligence, fairly complex network architecture designs to guarantee
SchoolofIntelligenceScienceandTechnology,PekingUniversity
sufficient expressive power. More importantly, many
3MicrosoftResearchAI4Science4CenterforMachineLearning
real-worldapplicationsrequireamodelthatcaneffectively
Research,PekingUniversity. Correspondenceto: ShengjieLuo
<luosj@stu.pku.edu.cn>,DiHe<dihe@pku.edu.cn>,LiweiWang perform both invariant and equivariant prediction with
<wanglw@pku.edu.cn>. strongperformanceatthesametime. Whilesomerecent
works study this direction (SchÃ¼tt et al., 2021; ThÃ¶lke &
Proceedings of the 41st International Conference on Machine
DeFabritiis,2022),mostproposednetworksaredesigned
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
heuristicallyandlackgeneraldesignprinciples.
theauthor(s).
1
4202
nuJ
42
]GL.sc[
1v35861.6042:viXraGeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
We argue that developing a general and flexible architec- ourarchitecturecanpreciselyforecastthepositions(equiv-
ture that can effectively learn both invariant and equiv- ariant) for a set of particles controlled by physical rules.
ariant representations of high quality and simultaneously Ablationstudyfurthershowsbenefitsbroughtbyeachde-
achievestrongperformanceonbothtasksisessential. In sign choice of our framework. All the empirical results
thiswork,weintroduceGeoMFormertoachievethisgoal. highlightthegeneralityandeffectivenessofGeoMFormer.
GeoMFormerusesastandardTransformer-basedarchitec-
ture(Vaswanietal.,2017)butwithtwostreams. Aninvari-
2.RelatedWorks
antstreamlearnsinvariantrepresentations,andanequivari-
antstreamlearnsequivariantrepresentations. Eachstream
InvariantRepresentationLearning. Inrecentyears,in-
iscomposedofinvariant/equivariantself-attentionandfeed-
variancehasbeenrecognizedasoneofthefundamentalprin-
forwardlayers. ThekeydesigninGeoMFormeristouse
ciplesguidingthedevelopmentofmolecularmodels. Tode-
cross-attentionmechanismsbetweenthetwostreams,let-
scribethepropertiesofamolecularsystem,themodelâ€™spre-
tingeachstreamincorporatetheinformationfromtheother
dictionshouldremainunchangedifweconductanyrotation
and enhance itself. In each layer of the invariant stream,
ortranslationactionsonthecoordinatesofthewholesystem.
wedevelopaninvariant-to-equivariantcross-attentionmod-
Previous works usually rely on relative structural signals
ule,wheretheinvariantrepresentationsareusedtoquery
from the coordinates, which intrinsically preserve the in-
key-valuepairsintheequivariantstream. Anequivariant-
variance. InSchNet(SchÃ¼ttetal.,2018), theinteratomic
to-invariantcross-attentionmoduleisdesignedsimilarlyin
distancesareencodedviaradialbasisfunctions,whichserve
theequivariantstream. Weshowthatthedesignofallself-
astheweightsofthedevelopedcontinuous-filterconvolu-
attentionandcross-attentionmodulesisflexibleandhowto
tionallayers. PhysNet(Unke&Meuwly,2019)similarly
satisfytheinvariant/equivariantconditionseffectively.
incorporatedbothatomicfeaturesandinteratomicdistances
Our proposed architecture has several advantages com- initsinteractionblocks. Graphormer-3D(Shietal.,2022)
pared to previous works. GeoMFormer decomposes the developedaTransformer-basedmodelbyencodingtherela-
invariant/equivariantrepresentationlearningthroughself- tivedistanceasattentionbiasterms,whichperformwellon
attentionandcross-attentionmodules. Byinteractingthe large-scaledatasets(Chanussotetal.,2021).
two streams using cross-attention modules, the invariant
Beyond the interatomic distance, other works further in-
streamreceivesmorestructuralsignals(fromtheequivariant
corporatehigh-orderinvariantsignals. BasedonPhysNet,
stream),andtheequivariantstreamobtainsmorenon-linear
DimeNet/DimeNet++(Gasteigeretal.,2020b;a)addition-
transformation (from the invariant stream), which allows
allyencodethebondangleinformationusingFourier-Bessel
simultaneouslyandcompletelymodelinginteratomicinter-
basisfunctions.Moreover,GemNet/GemNet-OC(Gasteiger
actions within/across feature spaces in a unified manner.
etal.,2021;2022)carefullystudiedtheconnectionsbetween
Furthermore,wedemonstratethattheproposeddecomposi-
sphericalrepresentationsanddirectionalinformation,which
tionisgeneralbyshowingthatmanyexistingmethodscan
inspiredtoleveragethedihedralangles,i.e.,anglesbetween
beregardedasspecialcasesinourframework. Forexample,
planes formed by bonds. SphereNet (Liu et al., 2022b)
PaiNN(SchÃ¼ttetal.,2021)andTorchMD-NET(ThÃ¶lke&
and ComENet (Wang et al., 2022) consider the torsional
DeFabritiis,2022)canbeformulatedasaspecialinstanti-
informationtoaugmentthemolecularmodels. Duringthe
ationbyfollowingthedesignphilosophyofGeoMFormer
development in the literature, more complex features are
andusingproperinstantiationsofkeybuildingcomponents.
incorporatedduetothelossystructuralinformationwhen
Fromthisperspective,webelieveourmodelcanoffermany
purelylearninginvariantrepresentations,whilelargelyin-
differentoptionsindiversescenariosinrealapplications.
creasingthecosts.Besides,theseinvariantmodelsaregener-
Weconductexperimentscoveringdiversedatamodalities, allyunabletodirectlyperformequivariantpredictiontasks.
scalesandtaskswithbothinvariantandequivarianttargets.
OntheOpenCatalyst2020(OC20)dataset(Chanussotetal.,
EquivariantRepresentationLearning. Insteadofbuild-
2021),whichcontainslargeatomicsystemscomposedof
inginvariantblocksonly,therearevariousworksthataimto
adsorbate-catalystpairs,ourmodelisabletopredictthesys-
learnequivariantrepresentations. Inreal-worldapplications,
temâ€™senergy(invariant)andrelaxedstructure(equivariant)
therearealsomanymoleculartasksthatrequirethemodel
withhighaccuracy. Additionally,ourarchitectureachieves
toperformequivariantpredictions,e.g.,predictingtheforce,
state-of-the-artperformanceforpredictinghomo-lumoen-
position, velocity, and other tensorized properties in dy-
ergygap(invariant)ofamoleculeonPCQM4Mv2(Huetal.,
namic simulation tasks. If a rotation action is performed
2021)andMolecule3D(Xuetal.,2021)datasets,bothof
on each position, then these properties should also corre-
which consist of molecules collected from the chemical
spondinglyrotate. Oneclassicalapproach(Thomasetal.,
database(Maho,2015;Nakata&Shimazaki,2017). More-
2018;Fuchsetal.,2020;Batzneretal.,2022;Musaelian
over,weconductanN-bodysimulationexperimentwhere
etal.,2023)toencodingtheequivariantconstraintsisusing
2GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
irreducible representations (irreps) via spherical harmon- theory(Cotton,1991;Cornwell,1997;Scott,2012).
ics(Goodman&Wallach,2000). Withequivariantconvolu-
Formally, let Ï• : X â†’ Y denote a function mapping be-
tionsbasedontensorproductsbetweenirreps,eachblock
tweenvectorspaces.GivenagroupG,letÏX andÏY denote
of the model preserves the equivariance. However, their
itsgrouprepresentations. AfunctionÏ•:X â†’Y issaidto
operationsareingeneralcostly(SchÃ¼ttetal.,2021;Satorras
beequivariant/invariantifitsatisfiesthefollowingcondi-
et al., 2021; Frank et al., 2022; Luo et al., 2024), which
tionsrespectively:
largely hinders the model from deploying on large-scale
molecularsystems. Besides, thesemodelsalsodonotal- Equivariance:ÏY(g)[Ï•(x)]=Ï•(cid:0) ÏX(g)[x](cid:1) ,forallgâˆˆG,xâˆˆX
wayssignificantlyoutperforminvariantmodelsoninvariant
Invariance:
Ï•(x)=Ï•(cid:0) ÏX(g)[x](cid:1)
,forallgâˆˆG,xâˆˆX
tasks(Liuetal.,2022b). (1)
Intuitively,anequivariantfunctionmappingtransformsthe
On the other hand, several recent works maintain both
output predictably in response to transformations on the
invariant and equivariant representations. The invariant
input,whereasaninvariantfunctionmappingproducesan
representationsinEGNN(Satorrasetal.,2021)encodetype
outputthatremainsunchangedbytransformationsapplied
informationandrelativedistance,andarefurtherusedinvec-
totheinput. Forfurtherdetailsonthebackgroundofgroup
torscalingfunctionstotransformtheequivariantrepresenta-
theory,wereferreaderstotheappendixof(Thomasetal.,
tions. PaiNN(SchÃ¼ttetal.,2021)extendedthisframework
2018;Andersonetal.,2019;Fuchsetal.,2020).
to include the Hardamard product operation to transform
theequivariantrepresentations. Basedontheoperationsof Molecular systems are naturally located in the three-
PaiNN,TorchMD-Net(ThÃ¶lke&DeFabritiis,2022)further dimensionalEuclideanspace,andthegrouprelatedtotrans-
proposedamodifiedversionoftheself-attentionmodules lationsandrotationsisknownasSE(3). Foreachelement
to update invariant representations and achieved better g intheSE(3)group,itsrepresentationonR3 canbepa-
performanceoninvarianttasks. Allegro(Musaelianetal., rameterizedbypairsoftranslationvectorst âˆˆ R3 andor-
2023) instead uses tensor product operations to update thogonaltransformationmatricesRâˆˆR3Ã—3,det(R)=1,
equivariantfeaturesandinteractsequivariantandinvariant i.e., g = (t,R). Given a vector x âˆˆ R3, we have
featuresbyusingweight-generationmodules. Incontrast, ÏR3(g)[x] := Rx+t. For molecular modeling, it is es-
our GeoMFormer aims to achieve strong performance
sentialtolearnmolecularrepresentationsthatencodethe
on both invariant and equivariant tasks at the same time,
rotationequivarianceandtranslationinvarianceconstraints.
whichmotivatesageneraldesignphilosophytolearnboth
Formally,letV denotethespaceofmolecularsystems,for
M
invariantandequivariantrepresentationsofhighquality,en- eachatomi,wedefineequivariantrepresentationÏ•E and
ablingsimultaneouslyandcompletelymodelinginteratomic invariantrepresentationÏ•I ifâˆ€g =(t,R)âˆˆSE(3),M=
interactionswithin/acrossfeaturespacesinaunifiedmanner
(X,R)âˆˆV ,thefollowingconditionsaresatisfied:
M
(Sec4.1). WereferinterestedreaderstoAppendixC.3for
morediscussionsandAppendixDformorerelatedworks. Ï•E :V M â†’R3Ã—d,
RÏ•E(X,{r ,...,r })=Ï•E(X,{Rr ,...,Rr })
1 n 1 n
3.Preliminary
Ï•E :V â†’R3Ã—d,
M
(2)
3.1.Notations&GeometricConstraints Ï•E(X,{r ,...,r })=Ï•E(X,{r +t,...,r +t})
1 n 1 n
Ï•I :V â†’Rd,
WedenoteamolecularsystemasM,whichismadeupofa M
collectionofatomsheldtogetherbyattractiveforces. Let Ï•I(X,{r ,...,r })=Ï•I(X,{Rr +t,...,Rr +t})
1 n 1 n
X âˆˆRnÃ—d denotetheatomswithfeatures,wherenisthe
number of atoms, and d is the feature dimension. Given 3.2.Attentionmodule
atomi,weuser âˆˆ R3 todenoteitscartesiancoordinate
i The attention module lies at the core of the Transformer
inthethree-dimensionalEuclideanspace. WedefineM=
architecture (Vaswani et al., 2017), and it is formu-
(X,R),whereR={r ,...,r }.
1 n lated as querying a dictionary with key-value pairs, e.g.,
Innature, molecularsystemsaresubjecttophysicallaws Attention(Q,K,V) = softmax(Q âˆšKT )V, where d is the
d
thatimposegeometricconstraintsontheirpropertiesand hiddendimension,andQ(Query),K (Key),V (Value)are
behaviors. For instance, if the position of each atom in specifiedasthehiddenfeaturesofthepreviouslayer. The
a molecular system is translated by a constant vector in multi-headvariantoftheattentionmoduleiswidelyused,
Euclidean space, the total energy of the system remains asitallowsthemodeltojointlyattendtoinformationfrom
unchanged. If a rotation is applied to each position, the differentrepresentationsubspaces. Itisdefinedasfollows:
directionoftheforceoneachatomwillalsorotate. Math-
Multi-head(Q,K,V)=Concat(head ,Â·Â·Â· ,head )WO
ematically,thesegeometricconstraintsaredirectlyrelated 1 H
to the concepts of invariance and equivariance in group head =Attention(QWQ,KWK,VWV),
k k k k
3GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
whereWQ âˆˆ RdÃ—dH,WK âˆˆ RdÃ—dH,WV âˆˆ RdÃ—dH,and red) and equivariant (colored in blue) representations are
k k k
WO âˆˆRHdHÃ—darelearnablematrices,H isthenumberof updatedinthefollowingmanner:
heads. d isthedimensionofeachattentionhead.
H
ï£±
Servingasagenericbuildingblock,theattentionmodule
ï£´ï£´ï£² Zâ€²I,l =ZI,l +Inv-Self-Attn(QI,l,KI,l,VI,l)
Zâ€²â€²I,l =Zâ€²I,l +Inv-Cross-Attn(QI,l,KI_E,l,VI_E,l)
can be used in various ways. On the one hand, the self- ï£´ï£´ï£³
ZI,l+1 =Zâ€²â€²I,l+Inv-FFN(Zâ€²â€²I,l), InvariantStream
attention module specifies Query, Key, and Value as the
ï£±
samehiddenrepresentation,therebyextractingcontextual ï£´ï£´ï£² Zâ€²E,l =ZE,l +Equ-Self-Attn(QE,l,KE,l,VE,l)
informationfortheinput. Ithasbeenoneofthekeycom- Zâ€²â€²E,l =Zâ€²E,l +Equ-Cross-Attn(QE,l,KE_I,l,VE_I,l)
ponents in Transformer-based foundation models across
ï£´ï£´ï£³
ZE,l+1 =Zâ€²â€²E,l+Equ-FFN(Zâ€²â€²E,l), EquivariantStream
variousdomains(Devlinetal.,2019;Brownetal.,2020; (3)
Dosovitskiyetal.,2021;Liuetal.,2021;Yingetal.,2021a;
whereldenotesthelayerindex. Inthisframework,theself-
Jumper et al., 2021; Ji et al., 2023). On the other hand,
attentionmodulesandfeed-forwardnetworksareusedto
thecross-attentionmodulespecifiesthehiddenrepresenta-
iterativelyupdaterepresentationsineachstream. Thecross-
tionfromonespaceasQuery,andtherepresentationfrom
attentionmodulesuserepresentationsfromonestreamto
theotherspaceasKey-Valuepairs, e.g. encoder-decoder
queryKey-Valuepairsfromtheotherstream. Byusingthis
attentionforsequence-to-sequencelearning. Asthecross-
mechanism, abidirectionalbridgeisestablishedbetween
attentionmodulebridgestwospaces,ithasbeenalsowidely
invariantandequivariantstreams. Besidesthecontextual
usedbeyondTransformerforinformationfusionandimprov-
informationfromtheinvariantstreamitself, theinvariant
ing representations (Lee et al., 2018; Huang et al., 2019;
representationscanfreelyattendtomoregeometricalsig-
Jaegleetal.,2021;2022).
nals from the equivariant stream. Similarly, the equivari-
antrepresentationscanbenefitfromusingmorenon-linear
4.GeoMFormer transformationsintheinvariantrepresentations. Withthe
cross-attentionmodules,theexpressivenessofbothinvariant
In this section, we introduce GeoMFormer, a novel
andequivariantrepresentationlearningislargelyimproved,
Transformer-basedmolecularmodelforlearninginvariant
whichallowssimultaneouslyandcompletelymodelingin-
andequivariantrepresentationsofhighquality. Webeginby
teratomicinteractionswithin/acrossfeaturespacesinauni-
elaboratingonthekeydesignsofGeoMFormer,whichform
fied manner. In this regard, as highlighted by different
ageneralframeworktoguidethedevelopmentofgeometric
colors,theQuery,Key,andValueintheself-attentionmod-
molecularmodels(Sec4.1),Nextwethoroughlydiscussthe
ules(Inv-Self-Attn,Equ-Self-Attn)andthecross-attention
implementationdetailsofGeoMFormer(Sec4.2).
modules (Inv-Cross-Attn,Equ-Cross-Attn) are differ-
entlyspecified,whichshouldcarefullyencodethegeometric
4.1.AGeneralDesignPhilosophy constraintsmentionedinSection3.1,asintroducedbelow.
Aspreviouslystated,severalexistingworkslearnedinvari-
antrepresentationsusinginvariantfeatures,e.g.,distancein- DesiderataforInvariantSelf-Attention. Giventhein-
formation,whichmayhavedifficultyinextractingotheruse- variant representation ZI, the Query, Key and Value
fulstructuralsignalsandcannotdirectlyperformequivariant in Inv-Self-Attn are calculated via a function mapping
tasks. Someotherworksdevelopedequivariantmodelsvia ÏˆI : RnÃ—d â†’ RnÃ—d, i.e., QI = Ïˆ QI(ZI),KI =
equivariantoperations,whichareeitherheuristicorcostly ÏˆI (ZI),VI =ÏˆI (ZI). Essentially,theattentionmodule
K V
anddonotguaranteebetterperformanceoninvarianttasks linearlytransformstheValueVI,withtheweightsbeing
comparedtoinvariantmodels. Instead,weaimtodevelopa calculatedfromthedotproductbetweentheQueryandKey
generaldesignprinciple,whichguidesthedevelopmentof (i.e., attention scores). In this regard, if both VI and the
amodelthataddressesthedisadvantagesaforementionedin attention scores preserve the invariance, then the output
bothinvariantandequivariantrepresentationlearning. satisfiestheinvariantconstraint, i.e., ÏˆI isrequiredtobe
invariant.Underthiscondition,itiseasytochecktheoutput
We call our model GeoMFormer, which is a two-stream
representationofthismodulekeepstheinvariance,whichis
Transformermodeltoencodeinvariantandequivariantinfor-
provedinAppendixB.1.
mation. EachstreamisbuiltupusingstackedTransformer
blocks, eachofwhichconsistsofaself-attentionmodule
Desiderata for Equivariant Self-Attention. Similarly,
andacross-attentionmodule,followedbyafeed-forward
network. For each atom k âˆˆ [n], we use zI âˆˆ Rd and
giventheequivariantinputZE,theQuery,KeyandValue
zE âˆˆ R3Ã—d to denote its invariant and equivk ariant repre- in Equ-Self-Attn are calculated via a function mapping
k ÏˆE : RnÃ—3Ã—d â†’ RnÃ—3Ã—d, i.e., QE = ÏˆE(ZE),KE =
sentationsrespectively. LetZI = [zIâŠ¤ ;...;zIâŠ¤ ] âˆˆ RnÃ—d Q
1 n ÏˆE(ZE),VE =ÏˆE(ZE). Similarly,ÏˆE isrequiredtobe
andZE =[zE;...;zE]âˆˆRnÃ—3Ã—d,theinvariant(coloredin K V
1 n equivariant. However,thisstillcannotguaranteethemodule
4GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
tobeequivariantifstandardattentionisused. Wemodified 4.2.ImplementationDetailsofGeoMFormer
Î± =(cid:80)d QE KE âŠ¤ ,whereQE âˆˆR3de-
ij k=1 [i,:,k] [j,:,k] [i,:,k] Following the design guidance in Section 4.1, we pro-
notesthek-thdimensionoftheatomiâ€™sQuery. Itisstraight-
pose Geometric Molecular Transformer (GeoMFormer).
forward to check the equivariance is preserved, which is
TheoverallarchitectureofGeoMFormerisshowninFig-
provedinAppendixB.1.
ure1,whichiscomposedofstackedGeoMFormerblocks
(Eqn.(4)). We introduce the instantiations of the self-
DesiderataforCross-attentionsbetweenthetwoStreams.
attention,cross-attentionandFFNmodulesbelowandprove
In each stream, the cross-attention module is used to
thepropertiestheysatisfyinAppendixB.2. Wealsoincor-
leverage information from the other stream. We call
poratewidelyusedmoduleslikeLayerNormalization(Ba
thecrossattentionintheinvariantstreaminvariant-cross-
etal.,2016)andStructuralEncodings(Shietal.,2022)for
equivariant attention, and call the cross attention in the
betterempiricalperformance. Duetothespacelimits,we
equivariant stream equivariant-cross-invariant attention,
referreaderstoAppendixAforfurtherdetails.
i.e.,Inv-Cross-AttnandEqu-Cross-Attn. Thedifference
betweenthetwocrossattentionliesinhowtheQuery,Key,
InstantiationofSelf-Attention. InGeoMFormer,thelin-
Valuearespecified:
earfunctionisusedtoimplementbothÏˆI :RnÃ—d â†’RnÃ—d
Invariant-cross-EquivariantAttention (Inv-Cross-Attn)
andÏˆE :RnÃ—3Ã—d â†’RnÃ—3Ã—d:
QI_E =ÏˆI(ZI),KI_E =ÏˆI_E(ZI,ZE),VI_E =ÏˆI_E(ZI,ZE)
Q K V QI =ÏˆI(ZI)=ZIWI, QE =ÏˆE(ZE)=ZEWE,
Q Q Q Q
Equivariant-cross-InvariantAttention (Equ-Cross-Attn) KI =ÏˆI (ZI)=ZIWI, KE =ÏˆE(ZE)=ZEWE, (5)
K K K K
QE_I =ÏˆE(ZE),KE_I =ÏˆE_I(ZE,ZI),VE_I =ÏˆE_I(ZE,ZI) VI =ÏˆI(ZI)=ZIWI VE =ÏˆE(ZE)=ZEWE
Q K V V V V V
(4)
First, for Query QI_E and QE_I, the requirement to ÏˆI whereW{I,E} arelearnableparameters.
{Q,K,V}
andÏˆE remainsthesameaspreviouslystated. Moreover,
as distinguished by different colors, the Key-Value pairs InstantiationofCross-Attention. Aspreviouslystated,
andtheQueryarecalculatedindifferentways,forwhich both ÏˆI_E and ÏˆE_I in the cross-attention modules fuse
therequirementshouldbeseparatelyconsidered. Notethat representationsfromdifferentspaces(invariant&equivari-
bothVI_E andVE_I arestilllinearlytransformedbythe ant)intotargetspaces. IntheInvariant-cross-Equivariant
cross-attentionmodules. IfVI_E preservestheinvariance attention module (Inv-Cross-Attn), to obtain the Key-
andVE_I preservestheequivariance, thentheremaining Valuepairs,theequivariantrepresentationsaremappedto
condition is to keep the invariance of the attention score the invariant space. For the sake of simplicity, we use
calculation. Thatistosay,fortheInv-Cross-Attn,bothÏˆI the dot-product operation < Â·,Â· > to instantiate ÏˆI_E.
and ÏˆI_E are required to be invariant. It is similar to the Given X,Y âˆˆ RnÃ—3Ã—d, Z =< X,Y >âˆˆ RnÃ—d, where
Equ-Cross-AttnthatbothÏˆE andÏˆE_I arerequiredtobe Z = X âŠ¤Y . Then the Key-Value pairs in
[i,k] [i,:,k] [i,:,k]
equivariant. Inthisway,theoutputsofbothcross-attention Inv-Cross-Attnarecalculatedas:
modulesareunderthecorrespondinggeometricconstraints,
KI_E =ÏˆI_E(ZI,ZE)=<ZEWI_E,ZEWI_E >,
whichisprovedinAppendixB.1. K K,1 K,2 (6)
VI_E =ÏˆI_E(ZI,ZE)=<ZEWI_E,ZEWI_E >
V V,1 V,2
Discussion. Thecarefullydesignedblocksoutlinedabove whereWI_E,WI_E,WI_E,WI_E âˆˆ RdÃ—dH forKeyand
provide a general design philosophy for encoding the ge- K,1 K,2 V,1 V,2
Value are learnable parameters. On the other hand, the
ometric constraints and bridging the invariant and equiv-
invariant representations are mapped to the equivariant
ariant molecular representations, which lie at the core of
space in the Equivariant-cross-Invariant attention mod-
ourframework. Notethatthetranslationinvariancecanbe
ule (Equ-Cross-Attn). To achieve this goal, we use
easilypreservedbyencodingrelativestructuresignalsofthe
the scalar product âŠ™ to instantiate ÏˆE_I. Given X âˆˆ
input. Itisalsoworthpointingoutthatwedonotrestrictthe RnÃ—3Ã—d,Y âˆˆ RnÃ—d, Z = X âŠ™ Y âˆˆ RnÃ—3Ã—d, where
specificinstantiationofeachcomponent,andvariousdesign
Z = X Â·Y . Using this operation, the Key-
choices can be adopted as long as they meet the require- [i,j,k] [i,j,k] [i,k]
ValuepairsinEqu-Cross-Attnarecalculatedas:
mentsmentionedabove.Moreover,weprovethatourframe-
workcanincludemanypreviousmodelsasaninstantiation, KE_I =ÏˆE_I(ZE,ZI)=ZEWE_I âŠ™ZIWE_I,
K K,1 K,2 (7)
e.g.,PaiNN(SchÃ¼ttetal.,2021)andTorchMD-Net(ThÃ¶lke VE_I =ÏˆE_I(ZE,ZI)=ZEWE_I âŠ™ZIWE_I
V V,1 V,2
&DeFabritiis,2022),canbeextendedtoencodeadditional
geometricconstraints(Cornwell,1997),whicharepresented whereWE_I,WE_I,WE_I,WE_I âˆˆRdÃ—dH arelearnable.
K,1 K,2 V,1 V,2
inAppendixB.1.Inthiswork,wepresentasimpleyeteffec-
tivemodelinstancethatimplementsthisdesignphilosophy, InstantiationofFeed-ForwardNetworks. Besidesthe
whichwewillthoroughlyintroduceinthenextsubsection. attention modules, the feed-forward networks also play
5GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
GeoMFormer Block Inv-Self-Attn Inv-Cross-Attn
ZI ZI ZE
ZI, l ZE, l
ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ Inv-FFN
Qğ‘Šğ‘Šğ‘„ğ‘„ğ¼ğ¼ Kğ‘Šğ‘Šğ¾ğ¾ğ¼ğ¼ Vğ‘Šğ‘Šğ‘‰ğ‘‰ğ¼ğ¼ ğ‘Šğ‘Šğ‘„ğ‘„ğ¼ğ¼_ğ¸ğ¸ ğ‘Šğ‘Šğ¾ğ¾<ğ¼ğ¼ ,_ 1ğ¸ğ¸ ï‚£,ï‚£>ğ‘Šğ‘Šğ¾ğ¾ğ¼ğ¼ ,_ 2ğ¸ğ¸ ğ‘Šğ‘Šğ‘‰ğ‘‰<ğ¼ğ¼ ,_ 1ğ¸ğ¸ ï‚£,ï‚£>ğ‘Šğ‘Šğ‘‰ğ‘‰ğ¼ğ¼ ,_ 2ğ¸ğ¸
ZI
Inv Inv Equ Equ Q K V
Invariant Attention
Self Cross Cross Self ï‚£
Invariant Attention
Attn Attn Attn Attn AttnProb Ã— AttnProb Ã— GELU( ğ‘Šğ‘Š1ğ¼ğ¼ )ğ‘Šğ‘Š2ğ¼ğ¼
Invariant Output Invariant Output Invariant Output
ZE ZE ZE ZI ZE ZI
ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£ ï‚£
Inv-FFN Equ-FFN Qğ‘Šğ‘Šğ‘„ğ‘„ğ¸ğ¸ Kğ‘Šğ‘Šğ¾ğ¾ğ¸ğ¸ Vğ‘Šğ‘Šğ‘‰ğ‘‰ğ¸ğ¸ Qğ‘Šğ‘Šğ‘„ğ‘„ğ¸ğ¸_ğ¼ğ¼ ğ‘Šğ‘Šğ¾ğ¾ğ¸ğ¸ ,1_ğ¼ğ¼âŠ™ Kğ‘Šğ‘Šğ¾ğ¾ğ¸ğ¸ ,2_ğ¼ğ¼ ğ‘Šğ‘Šğ‘‰ğ‘‰ğ¸ğ¸ ,1_âŠ™ğ¼ğ¼ Vğ‘Šğ‘Šğ‘‰ğ‘‰ğ¸ğ¸ ,2_ğ¼ğ¼ ğ‘Šğ‘Š1ğ¸ğ¸âŠ™ GELU( ğ‘Šğ‘Š2ğ¼ğ¼ )
ï‚£
ğ¸ğ¸
Equivariant Attention Equivariağ‘Šğ‘Šnt3 Output
Equivariant Attention
AttnProb Ã— AttnProb Ã— Equ-FFN
Equivariant Output Equivariant Output
ZI, l+1 ZE, l+1
Equ-Self-Attn Equ-Cross-Attn
Figure1. AnillustrationofourGeoMFormermodelarchitecture.
important roles in refining contextual representations. In 5.Experiments
the invariant stream, the feed-forward network is kept
Inthissection,weempiricallyinvestigateourGeoMFormer
unchanged from the standard Transformer model, i.e.,
Inv-FFN(Zâ€²â€²I) = GELU(Zâ€²â€²IWI)WI, where WI âˆˆ onextensivetasks. Inparticular,wecarefullydesignseveral
RdÃ—r,WI âˆˆ RrÃ—d and r denotes1 the h2 idden dimen1 sion experimentscoveringdifferenttypesoftasks(invariant&
2 equivariant),data(simplemolecules&adsorbate-catalyst
of the FFN layer. In the equivariant stream, it is worth
complexes & particle systems), and scales, as shown in
notingthatcommonlyusednon-linearactivationfunctions
Table1. Wealsoconductanablationstudytothoroughly
breaktheequivariantconstraints. InourGeoMFormer,we
verify the effectiveness of each design choice of our Ge-
use the invariant representations as a gating function to
oMFormer. Due to space limits, we present more results
non-linearly activate the equivariant representations, i.e.,
Equ-FFN(Zâ€²â€²E) = (Zâ€²â€²EWE âŠ™ GELU(Zâ€²â€²IWI))WE, (MD17,AblationStudy)inAppendixE.
1 2 3
whereWE,WI âˆˆRdÃ—r,WE âˆˆRrÃ—d.
1 1 2
5.1.OC20Performance(Invariant&Equivariant)
Input Layer. Given a molecular system M = (X,R),
TheOpenCatalyst2020(OC20)dataset(Chanussotetal.,
we set the invariant representation at the input as ZI,0 =
2021)wascreatedforcatalystdiscoveryandoptimization,
X, where X âˆˆ Rd is a learnable embedding vector in-
i whichisoneofthelargestmolecularmodelingbenchmarks
dexed by the atom iâ€™s type. For the equivariant repre-
and has great significance to advance renewable energy
sentation, we set ZE,0 = Ë†râ€²g(||râ€²||)âŠ¤ âˆˆ R3Ã—d, where
i i i processes for crucial social and energy challenges. Each
we consider both the direction Ë†râ€² âˆˆ R3 and the scale
i data is in the form of the adsorbate-catalyst complex.
g(||râ€²||) âˆˆ Rd of the each atomâ€™s mean-centered posi-
i Giventheinitialstructureofacomplex,DensityFunctional
tion râ€². g : R â†’ Rd is instantiated by the Gaussian
i Theory (DFT) tools are used to accurately simulate the
Basis Kernel, i.e., g(||râ€²||) = Ïˆ W, Ïˆ = [Ïˆ1;...;Ïˆd]âŠ¤,
Ïˆk = âˆ’âˆš 1 expi (cid:18) âˆ’1(cid:16) Î³i iâˆ¥râ€² iâˆ¥+i Î²iâˆ’Âµk(cid:17)i 2(cid:19) ,k i = r se cl ea nx aa rt ii oo sn ,p thr eoc re es las xu en dti el na ec rh gi yev ai nn dg se tq ru ui cl ti ub rr eiu om f. thIn ep cora mct pi lc ea xl
i 2Ï€|Ïƒk| 2 |Ïƒk|
are of great interest for catalyst discovery. In this regard,
1,...,d,whereW âˆˆRdÃ—dislearnable,Î³ ,Î² arelearnable
i i we focus on two significant tasks: Initial Structure to
scalarsindexedbytheatomtype,andÂµk,Ïƒk arelearnable
RelaxedEnergy(IS2RE)andInitialStructuretoRelaxed
kernelcenterandscalingfactorofthek-thKernel.Notethat
Structure (IS2RS), which require a model to directly
ourGeoMFormerisnotrestrictedtothesechoices,which
predict the relaxed energy and structure given the initial
canencodeadditionalfeaturesiftheconstraintsaresatisfied,
asdiscussedinAppendixB.2.
6GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table1. Summarizationofempiricalevaluationsetup.
Dataset TaskDescription TaskType DataType Trainingsetsize
OC20,IS2RE(Chanussotetal.,2021) EquilibriumEnergyPrediction(Sec5.1.1) Invariant Adsorbate-Catalystcomplex 460,328
OC20,IS2RS(Chanussotetal.,2021) EquilibriumStructurePrediction(Sec5.1.2) Equivariant Adsorbate-Catalystcomplex 460,328
PCQM4Mv2(Huetal.,2021) HOMO-LUMOGapPrediction(Sec5.2) Invariant Simplemolecule 3,378,606
Molecule3D(Wangetal.,2022) HOMO-LUMOGapPrediction(Sec5.3) Invariant Simplemolecule 2,339,788
N-BodySimulation(Satorrasetal.,2021) PositionPrediction(Sec5.4) Equivariant ParticleSystem 3,000
MD17(Chmielaetal.,2017) ForceFieldModeling(SecE.6) Inv&Equ Simplemolecule 950
AblationStudy Energy/Force/PositionPrediction(SecE.7) Inv&Equ - -
Table2.ResultsonOC20IS2REvalset.Wereporttheofficialresultsofbaselinesfromtheoriginalpaper.Boldvaluesdenotethebest.
EnergyMAE(eV)â†“ EwT(%)â†‘
Model ID OODAds. OODCat. OODBoth Average ID OODAds. OODCat. OODBoth Average
CGCNN(Xie&Grossman,2018) 0.6203 0.7426 0.6001 0.6708 0.6585 3.36 2.11 3.53 2.29 2.82
SchNet(SchÃ¼ttetal.,2018) 0.6465 0.7074 0.6475 0.6626 0.6660 2.96 2.22 3.03 2.38 2.65
DimeNet++(Gasteigeretal.,2020a) 0.5636 0.7127 0.5612 0.6492 0.6217 4.25 2.48 4.4 2.56 3.42
GemNet-T(Gasteigeretal.,2021) 0.5561 0.7342 0.5659 0.6964 0.6382 4.51 2.24 4.37 2.38 3.38
SphereNet(Liuetal.,2022b) 0.5632 0.6682 0.5590 0.6190 0.6024 4.56 2.70 4.59 2.70 3.64
Graphormer-3D(Shietal.,2022) 0.4329 0.5850 0.4441 0.5299 0.4980 - - - - -
GNS(Pfaffetal.,2020) 0.47 0.51 0.48 0.46 0.4800 - - - - -
Equiformer(Liao&Smidt,2023) 0.4156 0.4976 0.4165 0.4344 0.4410 7.47 4.64 7.19 4.84 6.04
GeoMFormer(ours) 0.3883 0.4562 0.4037 0.4083 0.4141 11.26 6.70 9.97 6.42 8.59
structureasinputrespectively1. Thetrainingsetforboth Table3.ResultsonOC20IS2RSvalidationset. Allmodelsare
tasks is composed of over 460,328 catalyst-adsorbate trained and evaluated under the direct prediction setting. Bold
complexes. Tobetterevaluatethemodelâ€™sperformance,the valuesindicatethebest.
validationandtestsetsconsiderthein-distribution(ID)and ADwT(%)â†‘
Model ID OODAds OODCat OODBoth Average
out-of-distributionsettingswhichusesunseenadsorbates
PaiNN(SchÃ¼ttetal.,2021) 3.29 2.37 3.10 2.33 2.77
(OOD-Ads), catalysts (OOD-Cat) or both (OOD-Both), TorchMD-Net(ThÃ¶lke&DeFabritiis,2022) 3.32 3.35 2.94 2.89 3.13
containingapproximately200,000complexesintotal. Spinconv(Shuaibietal.,2021) 5.81 4.88 5.63 4.84 5.29
GemNet-dT(Gasteigeretal.,2021) 6.87 7.10 6.03 7.08 6.77
GemNet-OC(Gasteigeretal.,2022) 11.31 12.20 4.40 5.55 8.36
5.1.1.IS2REPERFORMANCE(INVARIANT) GeoMFormer(ours) 11.45 10.52 9.94 10.78 10.67
As an energy prediction task, the IS2RE task evaluates significant considering the challenging task. The results
how well the model learns invariant representations. We indeeddemonstratetheeffectivenessofourGeoMFormer
followtheexperimentalsetupofGraphormer-3D(Shietal., frameworkonlearninginvariantrepresentations.
2022). ThemetricoftheIS2REtaskistheMeanAbsolute
Error(MAE)andthepercentageofdatainstancesinwhich 5.1.2.IS2RSPERFORMANCE(EQUIVARIANT)
thepredictedenergyiswithina0.02eVthreshold(EwT).
Furthermore,weusetheIS2RStasktoevaluatethemodelâ€™s
We choose several strong baselines covering geometric
ability to perform the equivariant prediction task. The
molecular models using different approaches. Due to
metric of the IS2RS task is the Average Distance within
space limits, the detailed description of training settings
Threshold (ADwT) across different thresholds. The Dis-
and baselines is presented in Appendix E.1. The results
tance within Threshold is computed as the percentage of
areshowninTable2. OurGeoMFormeroutperformsthe
structureswiththeatompositionMAEbelowthethreshold.
compared baselines significantly, achieving impressive
Were-implementseveralcompetitivebaselinesunderthe
performanceespeciallyontheout-of-distributionvalidation
directpredictionsettingforcomparison. Werefertheread-
sets, e.g., 42.2% relative EwT improvement on average
erstoAppendixE.2formoredetailsonthesettings. From
comparedtothebestbaseline. Inparticular,theimprove-
Table 3, we can see that the IS2RS task under the direct
mentontheEnergywithinThreshold(EwT)metricisalso
predictionsettingisratherdifficult. Thecomparedbaseline
1Insteadofusingtheiterativerelaxationsettingthatrequires modelsconsistentlyachievelowADwT.OurGeoMFormer
massivesingle-pointstructure-to-energy-forcedatatotraininga achievesthebest(e.g.,27.6%relativeADwTimprovement
force-fieldmodel(Chanussotetal.,2021),wefocusonthedirect on average compared to the best baseline), which indeed
predictionsettingthatonlyusesinitial-relaxedstructurepairsdata
verifiesthesuperiorabilityofourGeoMFormerframework
astheinputandlabel,whichisefficientwhilemorechallenging.
toperformequivariantmoleculartasks.
7GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table4.Results on PCQM4Mv2. The evaluation metric is the Table6.Results on N-body System Simulation. We report the
Mean Absolute Error (MAE). We report the official results of officialresultsofbaselines.Boldvaluesindicatethebest.
baselines. âˆ—indicatesthebestperformanceachievedbymodels Model MSEâ†“
withthesamecomplexity(ndenotesthenumberofatoms). SE(3)Transformer(Fuchsetal.,2020) 0.0244
Model Complexity ValidMAEâ†“ TensorFieldNetwork(Thomasetal.,2018) 0.0155
MLP-Fingerprint(Huetal.,2021) 0.1735 GraphNeuralNetwork(Gilmeretal.,2017) 0.0107
GINE-VN(Brossardetal.,2020;Gilmeretal.,2017) 0.1167 RadialField(KÃ¶hleretal.,2019) 0.0104
GCN-VN(Kipf&Welling,2017;Gilmeretal.,2017) O(n) 0.1153 EGNN(Satorrasetal.,2021) 0.0071
GIN-VN(Xuetal.,2019;Gilmeretal.,2017) 0.1083
GeoMFormer(ours) 0.0047
DeeperGCN-VN(Lietal.,2020;Gilmeretal.,2017) 0.1021*
TokenGT(Kimetal.,2022) 0.0910
EGT(Hussainetal.,2022) 0.0869
GRPE(Parketal.,2022) 0.0867
FromTable4. OurGeoMFormerachievesthelowestMAE
Graphormer(Yingetal.,2021a;Shietal.,2022) O(n2) 0.0864
GraphGPS(RampÃ¡Å¡eketal.,2022) 0.0858 amongthequadraticmodels, e.g., 6.7%relativeMAEre-
GPS++(Mastersetal.,2022) 0.0778
duction compared to the previous best model. Besides,
Transformer-M(Luoetal.,2023) 0.0787
GEM-2(Liuetal.,2022a) O(n3) 0.0793 compared to the best model Uni-Mol+ (Lu et al., 2023),
Uni-Mol+(Luetal.,2023) 0.0708* ourGeoMFormerachievescompetitiveperformancewhile
GeoMFormer(ours) O(n2) 0.0734*
keepingtheefficiency(O(n2)complexity),whichcanbe
morebroadlyappliedtolargemolecularsystems. Overall,
Table5.Results on Molecule3D for both random and scaffold
theresultsfurtherverifytheeffectivenessofGeoMFormer
splits. We report the official results of baselines. Bold values
oninvariantrepresentationlearning.
denotethebest.
MAEâ†“
Model Random Scaffold 5.3.Molecule3DPerformance(Invariant)
GIN-Virtual(Huetal.,2021) 0.1036 0.2371
SchNet(SchÃ¼ttetal.,2018) 0.0428 0.1511 Molecule3D (Xu et al., 2021) is a newly proposed large-
DimeNet++(Gasteigeretal.,2020a) 0.0306 0.1214
scaledatasetcuratedfromthePubChemQCproject(Maho,
SphereNet(Liuetal.,2022b) 0.0301 0.1182
ComENet(Wangetal.,2022) 0.0326 0.1273 2015;Nakata&Shimazaki,2017). Eachmoleculehasthe
PaiNN(SchÃ¼ttetal.,2021) 0.0311 0.1208
DFT-calculatedequilibriumgeometricstructure. Thetaskis
TorchMD-Net(ThÃ¶lke&DeFabritiis,2022) 0.0303 0.1196
GeoMFormer(ours) 0.0252 0.1045 topredicttheHOMO-LUMOenergygap,whichisthesame
asPCQM4Mv2. Thedatasetcontains3,899,647molecules
in total and is split into training, validation, and test sets
5.2.PCQM4Mv2Performance(Invariant)
withthesplittingratio6:2:2. Inparticular,bothrandom
PCQM4Mv2isoneofthelargestquantumchemicalprop- and scaffold splitting methods are adopted to thoroughly
erty datasets from the OGB Large-Scale Challenge (Hu evaluatethein-distributionandout-of-distributionperfor-
etal.,2021). Givenamolecule,itsHOMO-LUMOenergy manceofgeometricmolecularmodels. Following(Wang
gapoftheequilibriumstructureisrequiredtopredict,evalu- et al., 2022), we compare our GeoMFormer with several
atingthemodelâ€™sabilityofinvariantprediction. Thisprop- competitive baselines. Detailed descriptions of the train-
ertyishighlyrelatedtoreactivity,photoexcitation,charge ing settings and baselines are presented in Appendix E.4.
transport,andotherrealapplications. DFTtoolsareused It can be easily seen from Table 5 that our GeoMFormer
tocalculatetheHOMO-LUMOgapforground-truthlabels. consistentlyoutperformsallbaselinesonbothrandomand
Thetotalnumberoftrainingsamplesisaround3.37million. scaffoldsplitsettings,e.g.,16.3%and11.6%relativeMAE
reductioncomparedtothepreviousbestmodelrespectively.
Inapracticalsetting,theDFT-calculatedequilibriumgeo-
metricstructureofeachtrainingsampleisprovided,while
5.4.N-BodySimulationPerformance(Equivariant)
onlyinitialstructurescanbegeneratedbyefficientbutinac-
curatetools(e.g.,RDKit(Landrum,2016))foreachvalida- Simulating dynamical systems consisting of a set of geo-
tionsample. Inthisregard,weadoptonerecentapproach metricobjectsinteractingunderphysicallawsiscrucialin
(Uni-Mol+ (Lu et al., 2023)) to handle this task. During manyapplications,e.g. moleculardynamicsimulation. Fol-
training,themodelreceivesRDKit-generatedinitialstruc- lowingFuchsetal.(2020);Satorrasetal.(2021),weusea
tures as the input, and predicts both the HOMO-LUMO syntheticn-bodysystemsimulationtaskasanextensionof
energygapandtheequilibriumstructurebyusingbothin- molecularmodelingtasks. Thistaskrequiresthemodelto
variantandequivariantrepresentations. Aftertraining,the forecastthepositionsofasetofparticles,whicharemod-
modelcanbeusedtopredicttheHOMO-LUMOgaptarget eled by simple interaction rules, yet can exhibit complex
byonlyusingtheinitialstructure,whichmeetstherequire- dynamics. Thus,themodelâ€™sabilitytoperformequivariant
mentofthesettings. Wecomparevariousbaselinesinthe predictiontasksiscarefullyevaluated. Inthisdataset,the
leaderboardforcomparison. Moredetailsofthesettingsare simulatedsystemconsistsof5particles,eachofwhichcar-
presentedinAppendixE.3. riesapositiveornegativechargeandhasaninitialposition
8GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
andvelocityinthethree-dimensionalEuclideanspace. The Therealsoexistsomelimitationstoourwork. Servingas
systemiscontrolledbyphysicalrulesinvolvingattractive ageneralarchitecture,theabilitytoscaleupboththemodel
andrepulsiveforces. Thedatasetcontains3.000trajectories anddatasetsizesisofconsiderableinteresttothecommu-
fortraining,2.000trajectoriesforvalidation,and2.000tra- nity, which has been partially explored in our extensive
jectoriesfortesting. Wecompareseveralstrongbaselines experiment.Additionally,ourmodelcanalsobeextendedto
following Satorras et al. (2021). Due to space limits, the encompassadditionaldownstreaminvariantandequivariant
detailsofthedatageneration,trainingsettingsandbaselines tasks,whichwehaveearmarkedforfutureresearch.
arepresentedinAppendixE.5. TheresultsareshowninTa-
ble6.OurGeoMFormerachievesthebestperformancecom-
References
paredtoallbaselines. Inparticular,thesignificant33.8%
MSE reduction indeed demonstrates the GeoMFormerâ€™s Anderson, B., Hy, T.S., andKondor, R. Cormorant: Co-
superiorabilityonlearningequivariantrepresentations. variantmolecularneuralnetworks. Advancesinneural
informationprocessingsystems,32,2019.
6.Conclusion
Ba,J.L.,Kiros,J.R.,andHinton,G.E.Layernormalization.
In this paper, we propose a general and flexible architec- arXivpreprintarXiv:1607.06450,2016.
ture,calledGeoMFormer,forlearninggeometricmolecular
representations. UsingthestandardTransformerbackbone, Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa,
twostreamsaredevelopedforlearninginvariantandequiv- J. P., Kornbluth, M., Molinari, N., Smidt, T. E., and
ariantrepresentationsrespectively. Inparticular,thecross- Kozinsky,B. E(3)-equivariantgraphneuralnetworksfor
attention mechanism is used to bridge these two streams, data-efficientandaccurateinteratomicpotentials. Nature
lettingeachstreamleveragecontextualinformationfromthe
communications,13(1):2453,2022.
otherstreamandenhanceitsrepresentations. Thissimple
Brandstetter, J., Hesselink, R., van der Pol, E., Bekkers,
yeteffectivedesignsignificantlyboostsbothinvariantand
E. J., and Welling, M. Geometric and physical quan-
equivariant modeling. Within the newly proposed frame-
tities improve e(3) equivariant message passing. In
work, many existing methods can be regarded as special
InternationalConferenceonLearningRepresentations,
instances, showing the generality of our method. All the
2022. URLhttps://openreview.net/forum?
empiricalresultsshowthatourGeoMFormercanachieve
id=_xwr8gOBeV1.
strongperformanceindifferentscenarios. Thepotentialof
ourGeoMFormercanbefurtherexploredinabroadrange
Brossard,R.,Frigo,O.,andDehaene,D. Graphconvolu-
ofapplicationsinmolecularmodeling.
tionsthatcanfinallymodellocalstructure. arXivpreprint
arXiv:2011.15069,2020.
Acknowledgements
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
We thank all the anonymous reviewers for the very care-
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
ful and detailed reviews as well as the valuable sugges-
Askell,A.,etal. Languagemodelsarefew-shotlearners.
tions. Their help has further enhanced our work. Di He
Advancesinneuralinformationprocessingsystems,33:
is supported by National Key R&D Program of China
1877â€“1901,2020.
(2022ZD0160300) and National Science Foundation of
China(NSFC62376007). LiweiWangissupportedbyNa-
Chanussot,L.,Das,A.,Goyal,S.,Lavril,T.,Shuaibi,M.,
tionalScienceFoundationofChina(NSFC62276005).
Riviere,M.,Tran,K.,Heras-Domingo,J.,Ho,C.,Hu,W.,
etal. Opencatalyst2020(oc20)datasetandcommunity
ImpactStatement
challenges. AcsCatalysis,11(10):6059â€“6072,2021.
This work newly proposes a general framework to learn
Chmiela,S.,Tkatchenko,A.,Sauceda,H.E.,Poltavsky,I.,
geometricmolecularrepresentations,whichhasgreatsig-
SchÃ¼tt,K.T.,andMÃ¼ller,K.-R. Machinelearningofac-
nificance in molecular modeling. Our model has demon-
curateenergy-conservingmolecularforcefields. Science
stratedconsiderablepositivepotentialforvariousphysical
advances,3(5):e1603015,2017.
andchemicalapplications,suchascatalystdiscoveryandop-
timization,whichcansignificantlycontributetotheadvance-
Cornwell,J.F. Grouptheoryinphysics: Anintroduction.
mentofrenewableenergyprocesses.However,itisessential
Academicpress,1997.
toacknowledgethepotentialnegativeimpactsincludingthe
developmentoftoxicdrugsandmaterials. Thus,stringent
Cotton,F.A. Chemicalapplicationsofgrouptheory. John
measuresshouldbeimplementedtomitigatetheserisks.
Wiley&Sons,1991.
9GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: J., and Battaglia, P. Simple GNN regularisation for
Pre-training of deep bidirectional transformers for lan- 3d molecular property prediction and beyond. In In-
guageunderstanding. InProceedingsofthe2019Confer- ternational Conference on Learning Representations,
enceoftheNorthAmericanChapteroftheAssociationfor 2022. URLhttps://openreview.net/forum?
ComputationalLinguistics: HumanLanguageTechnolo- id=1wVvweK3oIb.
gies,Volume1(LongandShortPapers),pp.4171â€“4186,
Goodman, R. and Wallach, N. R. Representations and
2019.
invariantsoftheclassicalgroups. CambridgeUniversity
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, Press,2000.
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn-
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
ingforimagerecognition. InProceedingsoftheIEEE
N. An image is worth 16x16 words: Transformers for
conferenceoncomputervisionandpatternrecognition,
imagerecognitionatscale. InInternationalConference
pp.770â€“778,2016.
on Learning Representations, 2021. URL https://
openreview.net/forum?id=YicbFdNTTy. Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B.,
Catasta, M., andLeskovec, J. Opengraphbenchmark:
Frank,T.,Unke,O.,andMÃ¼ller,K.-R. So3krates: Equiv-
Datasets for machine learning on graphs. Advances in
ariantattentionforinteractionsonarbitrarylength-scales
neuralinformationprocessingsystems,33:22118â€“22133,
inmolecularsystems. AdvancesinNeuralInformation
2020.
ProcessingSystems,35:29400â€“29413,2022.
Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y., and
Fuchs, F., Worrall, D., Fischer, V., and Welling, M. Se
Leskovec, J. OGB-LSC: A large-scale challenge for
(3)-transformers: 3droto-translationequivariantattention machinelearningongraphs. InThirty-fifthConference
networks. AdvancesinNeuralInformationProcessing onNeuralInformationProcessingSystemsDatasetsand
Systems,33:1970â€“1981,2020. Benchmarks Track (Round 2), 2021. URL https:
//openreview.net/forum?id=qkcLxoC52kL.
Gasteiger, J., Giri, S., Margraf, J. T., and GÃ¼nnemann,
S. Fast and uncertainty-aware directional message Huang,W.,Han,J.,Rong,Y.,Xu,T.,Sun,F.,andHuang,J.
passingfornon-equilibriummolecules. arXivpreprint Equivariantgraphmechanicsnetworkswithconstraints.
arXiv:2011.14115,2020a. InInternationalConferenceonLearningRepresentations,
2022. URLhttps://openreview.net/forum?
Gasteiger, J., GroÃŸ, J., and GÃ¼nnemann, S. Direc-
id=SHbhHHfePhP.
tional message passing for molecular graphs. In In-
ternational Conference on Learning Representations, Huang,Z.,Wang,X.,Huang,L.,Huang,C.,Wei,Y.,and
2020b.URLhttps://openreview.net/forum? Liu,W.Ccnet:Criss-crossattentionforsemanticsegmen-
id=B1eWbxStPH. tation. In Proceedings of the IEEE/CVF international
conferenceoncomputervision,pp.603â€“612,2019.
Gasteiger,J.,Becker,F.,andGÃ¼nnemann,S. Gemnet: Uni-
versaldirectionalgraphneuralnetworksformolecules. Hussain,M.S.,Zaki,M.J.,andSubramanian,D. Global
AdvancesinNeuralInformationProcessingSystems,34: self-attentionasareplacementforgraphconvolution. In
6790â€“6802,2021. Proceedingsofthe28thACMSIGKDDConferenceon
Knowledge Discovery and Data Mining, pp. 655â€“665,
Gasteiger, J., Shuaibi, M., Sriram, A., GÃ¼nnemann, S., 2022.
Ulissi, Z. W., Zitnick, C. L., and Das, A. Gemnet-
OC: Developing graph neural networks for large and Jaegle,A.,Gimeno,F.,Brock,A.,Vinyals,O.,Zisserman,
diverse molecular simulation datasets. Transactions A.,andCarreira,J. Perceiver: Generalperceptionwithit-
on Machine Learning Research, 2022. ISSN 2835- erativeattention. InInternationalconferenceonmachine
8856. URLhttps://openreview.net/forum? learning,pp.4651â€“4664.PMLR,2021.
id=u8tvSxm4Bs.
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C.,
Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock,
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and
A.,Shelhamer,E.,Henaff,O.J.,Botvinick,M.,Zisser-
Dahl,G.E. Neuralmessagepassingforquantumchem-
man, A., Vinyals, O., and Carreira, J. Perceiver IO: A
istry. InInternationalconferenceonmachinelearning,
generalarchitectureforstructuredinputs&outputs. In
pp.1263â€“1272.PMLR,2017.
InternationalConferenceonLearningRepresentations,
Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez- 2022. URLhttps://openreview.net/forum?
Gonzalez,A.,Rubanova,Y.,VelicË‡kovicÂ´,P.,Kirkpatrick, id=fILj7WpI-g.
10GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Ji, G.-P., Zhuge, M., Gao, D., Fan, D.-P., Sakaridis, C., Liao, Y.-L., Wood, B., Das, A., and Smidt, T.
andGool,L.V. Maskedvision-languagetransformerin Equiformerv2:Improvedequivarianttransformerforscal-
fashion. MachineIntelligenceResearch,20(3):421â€“434, ing to higher-degree representations. In The Twelfth
2023. InternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?
Joshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and id=mCOBKZmrzD.
Lio, P. On the expressive power of geometric graph
neuralnetworks. InKrause,A.,Brunskill,E.,Cho,K., Liu,L.,He,D.,Fang,X.,Zhang,S.,Wang,F.,He,J.,and
Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro- Wu, H. Gem-2: Next generation molecular property
ceedings of the 40th International Conference on Ma- prediction network by modeling full-range many-body
chineLearning,volume202ofProceedingsofMachine interactions,2022a.
LearningResearch,pp.15330â€“15355.PMLR,23â€“29Jul
2023.URLhttps://proceedings.mlr.press/ Liu, Y., Wang, L., Liu, M., Lin, Y., Zhang, X., Oztekin,
v202/joshi23a.html. B.,andJi,S. Sphericalmessagepassingfor3dmolec-
ular graphs. In International Conference on Learning
Jumper,J.,Evans,R.,Pritzel,A.,Green,T.,Figurnov,M., Representations(ICLR),2022b.
Ronneberger,O.,Tunyasuvunakool,K.,Bates,R.,Å½Ã­dek,
Liu, Y., Cheng, J., Zhao, H., Xu, T., Zhao, P., Tsung, F.,
A.,Potapenko,A.,etal. Highlyaccurateproteinstructure
Li,J.,andRong,Y. SEGNO:Generalizingequivariant
predictionwithalphafold. Nature,596(7873):583â€“589,
graph neural networks with physical inductive biases.
2021.
In The Twelfth International Conference on Learning
Kim,J.,Nguyen,D.T.,Min,S.,Cho,S.,Lee,M.,Lee,H., Representations,2024. URLhttps://openreview.
andHong,S. Puretransformersarepowerfulgraphlearn- net/forum?id=3oTPsORaDH.
ers. InOh,A.H.,Agarwal,A.,Belgrave,D.,andCho,
Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,
K. (eds.), Advances in Neural Information Processing
Systems,2022. URLhttps://openreview.net/ S.,andGuo,B. Swintransformer: Hierarchicalvision
forum?id=um2BxfgkT2_. transformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision
Kipf, T. N. and Welling, M. Semi-supervised classi- (ICCV),pp.10012â€“10022,October2021.
fication with graph convolutional networks. In In-
Lu, S., Gao, Z., He, D., Zhang, L., and Ke, G. Highly
ternational Conference on Learning Representations,
2017. URLhttps://openreview.net/forum? accuratequantumchemicalpropertypredictionwithuni-
id=SJU4ayYgl. mol+,2023.
Luo,S.,Li,S.,Zheng,S.,Liu,T.-Y.,Wang,L.,andHe,D.
KÃ¶hler,J.,Klein,L.,andNoÃ©,F. Equivariantflows: sam-
Yourtransformermaynotbeaspowerfulasyouexpect.
pling configurations for multi-body systems with sym-
InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,
metricenergies. arXivpreprintarXiv:1910.00753,2019.
Cho, K., and Oh, A. (eds.), Advances in Neural Infor-
Landrum, G. Rdkit: Open-source cheminformat- mationProcessingSystems,volume35,pp.4301â€“4315.
ics software. Github, 2016. URL https: CurranAssociates,Inc.,2022.
//github.com/rdkit/rdkit/releases/
tag/Release_2016_09_4. Luo,S.,Chen,T.,Xu,Y.,Zheng,S.,Liu,T.-Y.,Wang,L.,
andHe,D. Onetransformercanunderstandboth2d&3d
Lee,K.-H.,Chen,X.,Hua,G.,Hu,H.,andHe,X. Stacked moleculardata.InTheEleventhInternationalConference
crossattentionforimage-textmatching. InProceedings on Learning Representations, 2023. URL https://
oftheEuropeanconferenceoncomputervision(ECCV), openreview.net/forum?id=vZTp1oPV3PC.
pp.201â€“216,2018.
Luo,S.,Chen,T.,andKrishnapriyan,A.S. Enablingeffi-
Li, G., Xiong, C., Thabet, A., and Ghanem, B. Deep- cientequivariantoperationsinthefourierbasisviagaunt
ergcn: Allyouneedtotraindeepergcns. arXivpreprint tensor products. In The Twelfth International Confer-
arXiv:2006.07739,2020. enceonLearningRepresentations,2024. URLhttps:
//openreview.net/forum?id=mhyQXJ6JsK.
Liao,Y.-L.andSmidt,T. Equiformer: Equivariantgraph
attention transformer for 3d atomistic graphs. In The Maho, N. The pubchemqc project: A large chemical
EleventhInternationalConferenceonLearningRepresen- database from the first principle calculations. In AIP
tations, 2023. URL https://openreview.net/ conferenceproceedings,volume1702,pp.090058.AIP
forum?id=KwmPfARgOTD. PublishingLLC,2015.
11GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Masters, D., Dean, J., Klaser, K., Li, Z., Maddrell- SchÃ¼tt,K.,Unke,O.,andGastegger,M. Equivariantmes-
Mander, S., Sanders, A., Helal, H., Beker, D., Ram- sage passing for the prediction of tensorial properties
pavsek, L., and Beaini, D. Gps++: An optimised hy- andmolecularspectra. InInternationalConferenceon
bridmpnn/transformerformolecularpropertyprediction. MachineLearning,pp.9377â€“9388.PMLR,2021.
ArXiv,abs/2212.02229,2022.
SchÃ¼tt, K. T., Sauceda, H. E., Kindermans, P.-J.,
Musaelian,A.,Batzner,S.,Johansson,A.,Sun,L.,Owen, Tkatchenko,A.,andMÃ¼ller,K.-R. Schnetâ€“adeeplearn-
C. J., Kornbluth, M., and Kozinsky, B. Learning local ingarchitectureformoleculesandmaterials. TheJournal
equivariantrepresentationsforlarge-scaleatomisticdy- ofChemicalPhysics,148(24):241722,2018.
namics. NatureCommunications,14(1):579,2023.
Scott,W.R. Grouptheory. CourierCorporation,2012.
Nakata,M.andShimazaki,T. Pubchemqcproject: alarge-
scalefirst-principleselectronicstructuredatabasefordata- Shi, Y., Zheng, S., Ke, G., Shen, Y., You, J., He, J.,
drivenchemistry. Journalofchemicalinformationand Luo,S.,Liu,C.,He,D.,andLiu,T.-Y. Benchmarking
modeling,57(6):1300â€“1308,2017. graphormeronlarge-scalemolecularmodelingdatasets.
arXivpreprintarXiv:2203.04810,2022.
Park, W., Chang, W.-G., Lee, D., Kim, J., et al. Grpe:
Relative positional encoding for graph transformer. In Shuaibi,M.,Kolluru,A.,Das,A.,Grover,A.,Sriram,A.,
ICLR2022MachineLearningforDrugDiscovery,2022. Ulissi, Z., and Zitnick, C. L. Rotation invariant graph
neuralnetworksusingspinconvolutions. arXivpreprint
Passaro, S. and Zitnick, C. L. Reducing SO(3) convo- arXiv:2106.09575,2021.
lutions to SO(2) for efficient equivariant GNNs. In
Krause, A., Brunskill, E., Cho, K., Engelhardt, B., ThÃ¶lke,P.andDeFabritiis,G. Torchmd-net: equivariant
Sabato, S., and Scarlett, J. (eds.), Proceedings of transformersforneuralnetworkbasedmolecularpoten-
the 40th International Conference on Machine Learn- tials. arXivpreprintarXiv:2202.02541,2022.
ing, volume 202 of Proceedings of Machine Learn-
ing Research, pp. 27420â€“27438. PMLR, 23â€“29 Jul Thomas, N., Smidt, T., Kearnes, S., Yang, L., Li, L.,
2023.URLhttps://proceedings.mlr.press/ Kohlhoff, K., and Riley, P. Tensor field networks:
v202/passaro23a.html. Rotation-andtranslation-equivariantneuralnetworksfor
3dpointclouds. arXivpreprintarXiv:1802.08219,2018.
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and
Battaglia, P. W. Learning mesh-based simulation with Unke,O.T.andMeuwly,M. Physnet: Aneuralnetworkfor
graphnetworks. arXivpreprintarXiv:2010.03409,2020. predictingenergies,forces,dipolemoments,andpartial
charges. Journalofchemicaltheoryandcomputation,15
Pozdnyakov, S. N., Willatt, M. J., BartÃ³k, A. P., Ort- (6):3678â€“3693,2019.
ner, C., CsÃ¡nyi, G., and Ceriotti, M. Incompleteness
of atomic structure representations. Phys. Rev. Lett., Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
125:166001,Oct2020. doi: 10.1103/PhysRevLett.125. L.,Gomez,A.N.,Kaiser,Å.,andPolosukhin,I. Atten-
166001. URL https://link.aps.org/doi/10. tionisallyouneed. InAdvancesinNeuralInformation
1103/PhysRevLett.125.166001. ProcessingSystems,pp.6000â€“6010,2017.
RampÃ¡Å¡ek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf, Wang, L., Liu, Y., Lin, Y., Liu, H., and Ji, S. Comenet:
G.,andBeaini,D. Recipeforageneral,powerful,scal- Towardscompleteandefficientmessagepassingfor3d
ablegraphtransformer. AdvancesinNeuralInformation moleculargraphs. AdvancesinNeuralInformationPro-
ProcessingSystems,35:14501â€“14515,2022. cessingSystems,35:650â€“664,2022.
Satorras, V. G., Hoogeboom, E., and Welling, M. E (n) Wang, Y. and Chodera, J. Spatial attention kinetic net-
equivariantgraphneuralnetworks. InInternationalCon- works with e(n)-equivariance. In The Eleventh In-
ference on Machine Learning, pp. 9323â€“9332. PMLR, ternational Conference on Learning Representations,
2021. 2023. URLhttps://openreview.net/forum?
id=3DIpIf3wQMC.
Scholkopf,B.,Sung,K.-K.,Burges,C.J.,Girosi,F.,Niyogi,
P.,Poggio,T.,andVapnik,V. Comparingsupportvector Xie, T. and Grossman, J. C. Crystal graph convolutional
machineswithgaussiankernelstoradialbasisfunction neuralnetworksforanaccurateandinterpretablepredic-
classifiers. IEEEtransactionsonSignalProcessing,45 tionofmaterialproperties. Physicalreviewletters,120
(11):2758â€“2765,1997. (14):145301,2018.
12GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing,
C.,Zhang,H.,Lan,Y.,Wang,L.,andLiu,T. Onlayer
normalizationinthetransformerarchitecture. InInter-
nationalConferenceonMachineLearning,pp.10524â€“
10533.PMLR,2020.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerful
aregraphneuralnetworks? InInternationalConference
on Learning Representations, 2019. URL https://
openreview.net/forum?id=ryGs6iA5Km.
Xu,Z.,Luo,Y.,Zhang,X.,Xu,X.,Xie,Y.,Liu,M.,Dicker-
son,K.,Deng,C.,Nakata,M.,andJi,S. Molecule3d: A
benchmarkforpredicting3dgeometriesfrommolecular
graphs. arXivpreprintarXiv:2110.01717,2021.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,
andLiu,T.-Y. Dotransformersreallyperformbadlyfor
graphrepresentation? AdvancesinNeuralInformation
ProcessingSystems,34:28877â€“28888,2021a.
Ying, C., Yang, M., Zheng, S., Ke, G., Luo, S., Cai, T.,
Wu, C., Wang, Y., Shen, Y., and He, D. First place
solution of kdd cup 2021 & ogb large-scale challenge
graphpredictiontrack. arXivpreprintarXiv:2106.08279,
2021b.
Zhang, B., Luo, S., Wang, L., and He, D. Rethinking
theexpressivepowerofGNNsviagraphbiconnectivity.
InTheEleventhInternationalConferenceonLearning
Representations,2023. URLhttps://openreview.
net/forum?id=r9hNv76KoT3.
Zitnick, C.L., Das, A., Kolluru, A., Lan, J., Shuaibi, M.,
Sriram, A., Ulissi, Z. W., and Wood, B. M. Spheri-
cal channels for modeling atomic interactions. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
Advances in Neural Information Processing Systems,
2022. URLhttps://openreview.net/forum?
id=5Z3GURcqwT.
13GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
A.ImplementationDetailsofGeoMFormer
LayerNormalizations. BeingaTransformer-basedmodel,GeoMFormeralsoadoptsthelayernormalization(LN)(Ba
etal.,2016)modulefortrainingstability. Intheinvariantstream,theLNmoduleremainsunchangedfromthestandard
design(Baetal.,2016;Xiongetal.,2020). Inparticular,wespecializedtheLNmoduleasEqu-LNintheequivariant
stream to satisfy the geometric constraints. Formally, given the equivariant representation zE âˆˆ R3Ã—d of the atom i,
i
Equ-LN(zE)=U(zEâˆ’Âµ1âŠ¤)âŠ™Î³,whereÂµ= 1(cid:80)d ZE âˆˆR3,Î³ âˆˆRdisalearnablevector,andUâˆˆR3Ã—3denotes
i i d k=1 [i,:,k]
theinversesquarerootofthecovariancematrix,i.e.,Uâˆ’2 = (zE i âˆ’Âµ1âŠ¤)(zE i âˆ’Âµ1âŠ¤)âŠ¤ .
d
StructuralEncodings. Wefollow(Shietal.,2022)toincorporatethe3Dstructuralencoding,whichservesasthebias
terminthesoftmaxattentionmodule. Inparticular,weconsidertheEuclideandistance||r âˆ’r ||betweenatomiand
i j
j. TheGaussianBasisKernelfunction(Scholkopfetal.,1997)isusedtoencodetheinteratomicdistance,i.e.,bk =
(i,j)
âˆ’âˆš 1
exp(âˆ’1(Î³(i,j)||riâˆ’rj||+Î²(i,j)âˆ’Âµk
)2),k = 1,...,K, whereK isthenumberofGaussianBasiskernels. The3D
2Ï€|Ïƒk| 2 |Ïƒk|
structuralencodingisobtainedbyB =GELU(b W1)W2,whereb =[b1 ;...;bK ]âŠ¤,W1 âˆˆRKÃ—K,W2 âˆˆ
ij (i,j) D D (i,j) (i,j) (i,j) D D
RKÃ—1arelearnableparameters.Î³ ,Î² arelearnablescalarsindexedbythepairofatomtypes,andÂµk,Ïƒkarelearnable
(i,j) (i,j)
kernelcenterandlearnablescalingfactorofthek-thGaussianBasisKernel. DenoteBasthematrixformofthe3Ddistance
encoding,whoseshapeisnÃ—n. Thentheattentionprobabilityiscalculatedbysoftmax(Q âˆšKâŠ¤ +B),whereQandK are
d
thequeryandkeyintroducedinSection3.
B.ProofofGeometricConstraints
In this section, we provide thorough proof of the aforementioned conditions in Section 4 that satisfy the geometric
constraints. Forthesakeofconvenience,werestatethenotationsandgeometricconstraintshere. Formally,letV denote
M
thespaceofmolecularsystems,foreachatomi,wedefineequivariantrepresentationÏ•E andinvariantrepresentationÏ•I if
âˆ€g =(t,R)âˆˆSE(3),M=(X,R)âˆˆV ,thefollowingconditionsaresatisfied:
M
Ï•E :V â†’R3Ã—d, RÏ•E(X,{r ,...,r })=Ï•E(X,{Rr ,...,Rr }) (8a)
M 1 n 1 n
Ï•E :V â†’R3Ã—d, Ï•E(X,{r ,...,r })=Ï•E(X,{r +t,...,r +t}) (8b)
M 1 n 1 n
Ï•I :V â†’Rd, Ï•I(X,{r ,...,r })=Ï•I(X,{Rr +t,...,Rr +t}) (8c)
M 1 n 1 n
where t âˆˆ R3,R âˆˆ R3Ã—3,det(R) = 1 and X âˆˆ RnÃ—d denotes the atoms with features, R = {r ,...,r },r âˆˆ R3
1 n i
denotesthecartesiancoordinateofatomi. WepresenttheproofoftheGeneralDesignPhilosophy(SectionB.1)andour
GeoMFormermodel(SectionB.2)respectively.
B.1.ProofoftheGeneralDesignPhilosophy.
GiveninvariantandequivariantrepresentationsZI,0 âˆˆRnÃ—d,ZE,0 âˆˆRnÃ—3Ã—dattheinput,weprovethattheupdaterules
showninEqn.(3)satisfytheaboveconstraintsinproperconditions. Inparticular,wefirstseparatelystudyeachcomponent
oftheblock,i.e.,Inv-Self-Attn, Equ-Self-Attn, Inv-Cross-Attn, Equ-Cross-Attn,andthencheckthepropertiesofthe
wholeframework.
InvariantSelf-Attention. GiveninvariantrepresentationZI,l âˆˆ RnÃ—d,QI,l = ÏˆI,l(ZI,l),KI,l = ÏˆI,l(ZI,l),VI,l =
Q K
ÏˆI,l(ZI,l), as stated in Section 4.1, where ÏˆI,l : RnÃ—d â†’ RnÃ—d is invariant. In this regard, âˆ€g = (t,R) âˆˆ SE(3),
V
QI,l,KI,l,VI,l remainunchanged,whichmeansthatInv-Self-Attn(QI,l,KI,l,VI,l)alsoremainsunchanged. Thenthe
invarianceoftheoutputrepresentationsispreserved.
Equivariant Self-Attention. Given equivariant representation ZE,l âˆˆ RnÃ—3Ã—d, QE,l = ÏˆE,l(ZE,l),KE,l =
Q
ÏˆE,l(ZE,l),VE,l = ÏˆE,l(ZE,l), as stated in Section 4.1, where ÏˆE,l : RnÃ—3Ã—d â†’ RnÃ—3Ã—d is equivariant. Be-
K V
sides, the attention score is modified as Î± = (cid:80)d QE KE âŠ¤ , where QE âˆˆ R3 denotes the k-
ij k=1 [i,:,k] [j,:,k] [i,:,k]
th dimension of the atom iâ€™s Query. First, we check the rotation equivariance of the Equ-Self-Attn. Given
any orthogonal transformation matrix R âˆˆ R3Ã—3,det(R) = 1, we have (cid:80)d QE R(KE R)âŠ¤ =
k=1 [i,:,k] [j,:,k]
(cid:80)d QE RRâŠ¤KE âŠ¤ =(cid:80)d QE KE âŠ¤ =Î± ,whichpreservestheinvariance.AsÏˆE,lisequivariant,
k=1 [i,:,k] [j,:,k] k=1 [i,:,k] [j,:,k] ij
14GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
wehaveÏˆE,l([RZE,l;,...,;RZE,l])=[RÏˆE,l(ZE,l) ;,...,;RÏˆE,l(ZE,l) ]. Sincetheoutputequivariantrepresentation
1 n 1 n
of atom i preserves the equivariance, i.e., (cid:80)n exp(Î±ij) RVE,l = R((cid:80)n exp(Î±ij) VE,l), the rotation
j=1 (cid:80)n jâ€²=1exp(Î± ijâ€²) j j=1 (cid:80)n jâ€²=1exp(Î± ijâ€²) j
equivarianceissatisfied. Moreover,sincetheequivariantrepresentationZE,l preservesthetranslationinvariance(Eqn.(8b)),
theoutputequivariantrepresentationofEqu-Self-Attnnaturallysatisfiesthisconstraint.
Cross-Attention modules. As stated in Section 4.1, the Query, Key, and Value of Inv-Cross-Attn are specified
as QI_E,l = ÏˆI,l(ZI,l),KI_E,l = ÏˆI_E,l(ZI,l,ZE,l),VI_E,l = ÏˆI_E,l(ZI,l,ZE,l), where ÏˆI,l,ÏˆI_E,l are invari-
Q K V
ant. That is to say, âˆ€g = (t,R) âˆˆ SE(3), QI_E,l,KI_E,l,VI_E,l remain unchanged. Then the invariance
of its output representations is preserved as in Inv-Self-Attn. On the other hand, the Query, Key, and Value of
Equ-Cross-AttnarespecifiedasQE_I,l =ÏˆE,l(ZE,l),KE_I,l =ÏˆE_I,l(ZE,l,ZI,l),VE_I,l =ÏˆE_I,l(ZE,l,ZI,l),where
Q K V
ÏˆE,l,ÏˆE_I,l areequivariant,i.e.,ÏˆE_I,l([RZE,l;,...,;RZE,l],ZI,l)=[RÏˆE_I,l(ZE,l,ZI,l) ;,...,;RÏˆE,l(ZE,l,ZI,l) ]
1 n 1 n
andÏˆE,l([RZE,l;,...,;RZE,l])=[RÏˆE,l(ZE,l) ;,...,;RÏˆE,l(ZE,l) ]. AsstatedinEqu-Self-Attn,theoutputequivari-
1 n 1 n
antrepresentationsofEqu-Cross-Attnpreservetherotationequivariance. Similarly,thetranslationinvariancepropertyis
alsonaturallysatisfied.
Feed-Forward Networks. As Inv-FFN and Equ-FFN satisfy the invariance and equivariance constraints re-
spectively, we can directly obtain that âˆ€g = (t,R) âˆˆ SE(3), the output of Inv-FFN remains un-
changed, and the output of Equ-FFN preserves the rotation equivariance, i.e., Equ-FFN([RZE,l;,...,;RZE,l]) =
1 n
[REqu-FFN(ZE,l) ;,...,;REqu-FFN(ZE,l) ]. ThetranslationinvarianceisalsonaturallypreservedbyEqu-FFN.
1 n
Withtheaboveanalysis,theupdaterulesstatedinEqn.(3)satisfythegeometricconstraints(Eqn.(8a),Eqn.(8b)andEqn.(8c)).
Asourmodeliscomposedofstackedblocks,theinvariantandequivariantoutputrepresentationsofthewholemodelalso
preservetheconstraints.
B.2.ProofoftheGeoMFormer
Next, we provide proof of the instantiation of our GeoMFormer in Section 4.2 that satisfies the geometric constraints.
Similarly,weseparatelycheckthepropertiesofeachcomponentasourGeoMFormeriscomposedofstackedGeoMFormer
blocks. Oncetheconstraintsaresatisfiedbyeachcomponent,theoutputinvariantandequivariantrepresentationsofthe
wholemodelnaturallysatisfythegeometricconstraints(Eqn.(8a),Eqn.(8b)andEqn.(8c)).
Input layer. As stated in Section 4.2, the invariant representation at the input is set as ZI,0 = X, where X âˆˆ Rd
i
isa learnableembeddingvectorindexedbythe atomiâ€™stype. Since ZI,0 doesnot containany informationfromR =
{r ,...,r },itnaturallysatisifiestheinvarianceconstraint(Eqn.(8c)). Theequivariantrepresentationattheinputisset
1 n
as ZE,0 = Ë†râ€²g(||râ€²||)âŠ¤ âˆˆ R3Ã—d, where râ€² denotes the mean-centered position of atom i, i.e., râ€² = r âˆ’ 1 (cid:80)n r ,
i i i i i i n k=1 k
Ë†râ€² = râ€² i ,andg :Râ†’RdisinstantiatedbytheGaussianBasisKernelfunction. First,thetranslationinvarianceconstraint
i ||râ€²||
i
(Eqn.(8b)) is satisfied. Given any translation vector t âˆˆ R3, r +tâˆ’ 1 (cid:80)n (r +t) = r âˆ’ 1 (cid:80)n r , and ZE,0
i n k=1 k i n k=1 k i
remainsunchanged. Second,therotationequivariance(Eqn.(8a))isalsopreserved. Givenanyorthogonaltransformation
matrix R âˆˆ R3Ã—3,det(R) = 1, we have ||Rrâ€²|| = ||râ€²||. With Rr as the input, we have Rr âˆ’ 1 (cid:80)n Rr =
R(r âˆ’ 1 (cid:80)n r )=Rrâ€² andg(||Rrâ€²||)=g(|i |râ€²||),whi ichmeansthai ttherotationequivariancecoi nstran intik s= s1 atisfik ed.
i n k=1 k i i i
Self-Attentionmodules. ForInv-Self-AttnandEqu-Self-Attn,weusethelinearfunctiontoimplementbothÏˆI andÏˆE,
i.e.,QI =ÏˆI(ZI)=ZIWI,KI =ÏˆI (ZI)=ZIWI,VI =ÏˆI (ZI)=ZIWI andQE =ÏˆE(ZE)=ZEWE,KE =
Q Q K K V V Q Q
ÏˆE(ZE) = ZEWE,VE = ÏˆE(ZE) = ZEWE. ItisstraightforwardthattheconditionsmentionedinSectionB.1are
K K V V
satisfied. ThelinearfunctionkeepstheinvarianceofZI (Eqn.(8c))andtherotationequivarianceofZE (Eqn.(8a)),e.g.,
âˆ€RâˆˆR3Ã—3,det(R)=1,(RZE)WE =R(ZEWE)=RZE. NotethatthetranslationinvarianceofZE (Eqn.(8b))isnot
i Q i Q i
changedbythelinearfunction.
Cross-Attention modules. For Inv-Cross-Attn, we use the linear function to implement ÏˆI, which satisfies
Q
the constraints as previously stated. Besides, we instantiate KI_E and VI_E as KI_E = ÏˆI_E(ZI,ZE) =<
K
ZEWI_E,ZEWI_E >,VI_E = ÏˆI_E(ZI,ZE) =< ZEWI_E,ZEWI_E >. Here we prove that such instantia-
K,1 K,2 V V,1 V,2
tion preserve the invariance. First, given any orthogonal transformation matrix R âˆˆ R3Ã—3,det(R) = 1, we have
< ([RZE;...;RZE])WI_E,([RZE;...;RZE])WI_E >=< ZEWI_E,ZEWI_E >. The reason is that given X,Y âˆˆ
1 n K,1 1 n K,2 K,1 K,2
15GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
RnÃ—3Ã—d,Z =< X,Y >âˆˆ RnÃ—d,whereZ = X âŠ¤Y = X âŠ¤RâŠ¤RY = (RX )âŠ¤(RY ). The
[i,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k] [i,:,k]
translationinvarianceofZE isalsopreserved.
ForEqu-Cross-Attn,wealsousethelinearfunctiontoimplementÏˆE,whichsatisfiestheconstraintsaspreviouslystated.
Q
Besides,weinstantiateKE_I andVE_I asKE_I = ÏˆE_I(ZE,ZI) = ZEWE_I âŠ™ZIWE_I,VE_I = ÏˆE_I(ZE,ZI) =
K K,1 K,2 V
ZEWE_I âŠ™ZIWE_I. First,givenanyorthogonaltransformationmatrixRâˆˆR3Ã—3,wehave([RZE;...;RZE])WE_I âŠ™
V,1 V,2 1 n K,1
ZIWE_I =[R(ZEWE_I âŠ™ZIWE_I) ;...;R(ZEWE_I âŠ™ZIWE_I) ],whichpreservestherotationequivariance. The
K,2 K,1 K,2 1 K,1 K,2 n
reason lies in that given X âˆˆ RnÃ—3Ã—d,Y âˆˆ RnÃ—d, Z = RX âŠ™Y âˆˆ R3Ã—d, where Z = (RX )Â·Y =
i i i [i,:,k] [i,:,k] [i,k]
R(X Â·Y ). Additionally,thetranslationinvarianceofbothKE_I andVE_I ispreservedbecauseofthetranslation
[i,:,k] [i,k]
invarianceofZE andZI. Inthisway,theinstantiationsofcross-attentionmodulessatisfythegeometricconstraints.
Feed-ForwardNetworks. ForInv-FFN(Zâ€²â€²I)=GELU(Zâ€²â€²IWI)WI,theinvarianceconstraint(Eqn.8c)isnaturally
1 2
preserved. ForEqu-FFN(Zâ€²â€²E)=(Zâ€²â€²EWEâŠ™GELU(Zâ€²â€²IWI))WE,therotationequivarianceconstraintisalsosimilarly
1 2 3
preservedasinEqu-Cross-Attn. Besides,thetranslationinvarianceofEqu-FFN(Zâ€²â€²E)isalsopreservedwiththeproperty
ofZâ€²â€²E andZâ€²â€²I.
Layer Normalizations. As introduced in Section A, we use the layer normalization modules for both invariant and
equivariant streams. For the invariant stream, the layer normalization remains unchanged, and the invariance con-
straint is naturally preserved. For the equivariant stream, given the equivariant representation zE âˆˆ R3Ã—d of the
i
atom i, Equ-LN(zE) = U(zE âˆ’ Âµ1âŠ¤) âŠ™ Î³, where Âµ = 1(cid:80)d ZE âˆˆ R3, Î³ âˆˆ Rd is a learnable vector, and
i i d k=1 [i,:,k]
U âˆˆ R3Ã—3 denotestheinversesquarerootofthecovariancematrix,i.e.,Uâˆ’2 = (zE i âˆ’Âµ1âŠ¤)(zE i âˆ’Âµ1âŠ¤)âŠ¤ . First,givenany
d
orthogonaltransformationmatrixR âˆˆ R3Ã—3,det(R) = 1, (RzE i âˆ’RÂµ1âŠ¤)(RzE i âˆ’RÂµ1âŠ¤)âŠ¤ = (RzE i âˆ’RÂµ1âŠ¤)(RzE i âˆ’RÂµ1âŠ¤)âŠ¤ =
d d
R(zE i âˆ’Âµ1âŠ¤)(zE i âˆ’Âµ1âŠ¤)âŠ¤ RâŠ¤ = RUâˆ’2RâŠ¤ = RUâˆ’1RâŠ¤RUâˆ’1RâŠ¤ = (RURâŠ¤)âˆ’2, then we have Equ-LN(RzE) =
d i
RURâŠ¤(RzEâˆ’RÂµ1âŠ¤)âŠ™Î³ =R(U(zEâˆ’Âµ1âŠ¤))=REqu-LN(zE),whichpreservestherotationequivariance(Eqn.(8a)).
i i i
ThetranslationinvarianceofZE isalsopreserved.
StructuralEncodings. AsintroducedinSectionA,thestructuralencodingsserveasthebiasterminthesoftmaxattention
module. Sinceonlytherelativedistance||r âˆ’r ||,âˆ€i,j âˆˆ [n]isused,theinvarianceconstraintispreserved,i.e.,given
i j
âˆ€g =(t,R)âˆˆSE(3),||Rr +tâˆ’Rr +t||=||r âˆ’r ||.
i j i j
C.Discussions
C.1.ConnectionstoPreviousApproaches
In this section, we present a detailed discussion of how previous models (PaiNN (SchÃ¼tt et al., 2021) and TorchMD-
Net(ThÃ¶lke&DeFabritiis,2022))canbeviewedasspecialinstantiationsbyextendingthedesignphilosophydescribed
inSection4.1. Withoutlossofgenerality,weomitthecutoffconditionsusedintheseworksforreadability,whichcanbe
naturallyincludedinourframework.
PaiNN(SchÃ¼ttetal.,2021). BothinvariantrepresentationsZI =[zIâŠ¤ ;...;zIâŠ¤ ]âˆˆRnÃ—dandequivariantrepresentations
1 n
ZE =[zE;...;zE]âˆˆRnÃ—3Ã—daremaintainedinPaiNN,wherezI âˆˆRdandzE âˆˆR3Ã—daretheinvariantandequivariant
1 n i i
representationsforatomi,respectively. Ineachlayer,therepresentationsareupdatedasfollows:
Zâ€²I,l =ZI,l +Message-Block-Inv(ZI,l)
Zâ€²E,l =ZE,l +Message-Block-Equ(ZI,l,ZE,l)
(9)
ZI,l+1 =Zâ€²I,l +Update-Block-Inv(Zâ€²I,l,Zâ€²E,l)
ZE,l+1 =Zâ€²E,l +Update-Block-Equ(Zâ€²I,l,Zâ€²E,l)
16GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Inthemessageblock,theinvariantandequivariantrepresentationsareupdatedinthefollowingmanner. Forbrevity,we
omitthelayerindexl.
Message-Block-Inv(zI)=(cid:88) Ï• (zI)â—¦W (||r âˆ’r ||)
i s j s i j
j
Message-Block-Equ(zI i,zE i )=(cid:88) zE j âŠ™(cid:16) Ï• vv(zI j)â—¦W vv(||r iâˆ’r j||)(cid:17) (10)
j
+ r iâˆ’r j (cid:16) Ï• (zI)â—¦Wâ€² (||r âˆ’r ||)(cid:17)âŠ¤
||r âˆ’r || vs j vs i j
i j
ThescalarproductâŠ™isdefinedthesamewayasinSection4.2,i.e.,givenxâˆˆR3Ã—d,y âˆˆRd,z =xâŠ™y âˆˆR3Ã—d,where
z = x Â·y . â—¦ denotes the element-wise product, Ï• ,Ï• ,Ï• : Rd â†’ Rd are all 2-layer MLP with the SiLU
[i,j] [i,k] [k] s vv vs
activation,W ,W ,Wâ€² :Râ†’Rdareinstantiatedbylearnableradialbasisfunctions. riâˆ’rj âˆˆR3denotestherelative
s vv vs ||riâˆ’rj||
directionbetweenatomiâ€™sandjâ€™spositions.
Intheupdateblock,theinvariantandequivariantrepresentationsareupdatedinthefollowingmanner:
Update-Block-Inv(zI,zE)=a (zI,||zEV||)+a (zI,||zEV||)â—¦<zEU,zEV>
i i ss i i sv i i i i
Update-Block-Equ(zI,zE)=a (zI,||zEV||)âŠ™(zEU)
i i vv i i i
V,UâˆˆRdÃ—darelearnableparameters. <Â·,Â·>isdefinedthesamewayasinSection4.2,i.e.,givenx,y âˆˆR3Ã—d,z =<
x,y >âˆˆ Rd, where z = x âŠ¤y . Norm || Â· || : R3Ã—d â†’ Rd is calculated along the spatial dimension, i.e.,
[k] [:,k] [:,k]
||Â·||=<Â·,Â·>. â—¦denotestheelement-wiseproduct. âŠ™isalsodefinedthesameasinSection4.2. a(Â·,Â·):RdÃ—Rd â†’Rd
firstconcatenatesthetwoinputsalongthefeaturedimensionandthenapplya2-layerMLPwithSiLUactivation.
Weprovethatboththeinvariantandequivariantmessageblockscanbeviewedasspecialinstancesbyextendingtheinvariant
self-attentionmoduleandtheequivariantcross-attentionmoduleofourframeworkrespectively. Inparticular,weextend
ÏˆI ,ÏˆE_I introducedintheSection4.1tobequery-dependent,i.e.,ÏˆI,i,ÏˆE_I,ithatdependsontheatomiâ€™srepresentations.
V V V V
Concretely,intheinvariantself-attentionmodule,wesetÏˆI,i(zI)=Ï• (zI)âŠ™W (||r âˆ’r ||). Similarly,intheequivariant
V j s j s i j
cross-attentionmodule,wesetÏˆE_I,i(zI,zE)=zE âŠ™Ï• (zI)Â·W (||r âˆ’r ||)+Ï• (zI)Â·Wâ€² riâˆ’rj . Insuchway,
V j j j vv j vv i j vs j vs||riâˆ’rj||
theinvariantself-attentionmoduleandtheequivariantcross-attentionmodulecanexpresstheinvariantandequivariant
messageblocksrespectively,e.g.,theparameterstotransformQueryandKeyaretrained/initializedtozero,andthenumber
ofatomscanbeequippedbyinitialization,whichisnecessarytoexpressthesumoperatorbyusingtheattentionasshown
in(Yingetal.,2021a).
Moreover, we prove that the update blocks can also be viewed as special instances by extending the FFN blocks in
our framework. In particular, we set Inv-FFN(zI) = a (zI,||zEV||) + a (zI,||zEV||) < zEU,zEV > and
Equ-FFN(zE) = a (zI,||zEV||)(cid:0) zEU(cid:1) , then bi oth Inv-s Fs FNi andi Equ-FFNsv cani expi ress the updi ate bli ocks. Note
i vv i i i
thattheparametersoftheremainingblocks(Inv-Cross-Attn,Equ-Self-Attn)canbetrained/initializedtobezero. Insuch
way,thePaiNNmodelcanbeinstantiatedthroughourdesignphilosophyintroducedinSection4.1.
TorchMD-Net(ThÃ¶lke&DeFabritiis,2022). SimilarlytoPaiNN,bothinvariantrepresentationsZI =[zIâŠ¤ ;...;zIâŠ¤ ]âˆˆ
1 n
RnÃ—dandequivariantrepresentationsZE =[zE;...;zE]âˆˆRnÃ—3Ã—daremaintainedinTorchMD-Net,wherezI âˆˆRdand
1 n i
zE âˆˆR3Ã—daretheinvariantandequivariantrepresentationsforatomi,respectively. Ineachlayer,therepresentationsare
i
updatedasfollows:
Zâ€²I,l =ZI,l +TorchMD-Inv-Block-1(ZI,l)
ZI,l+1 =Zâ€²I,l +TorchMD-Inv-Block-2(Zâ€²I,l,ZE,l) (11)
ZE,l+1 =ZE,l +TorchMD-Equ-Block(ZI,l,ZE,l)
17GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
TheinvariantrepresentationsinTorchMD-Inv-Block-1andTorchMD-Inv-Block-2areupdatedasfollows. Forbrevity,
weomitthelayerindexl.
Q =WQzI,K =WKzI,V(1) =WV(1)zI
i i j j j j
Î±
=SiLU(cid:16) QâŠ¤(cid:0)
K
â—¦DK(cid:1)(cid:17)
ij i j ij
(cid:32) (cid:33)
TorchMD-Inv-Block-1(zI)=O (cid:88) Î± Â·V(1)â—¦DV(1) (12)
i 1 ij j ij
j
(cid:32) (cid:33)
TorchMD-Inv-Block-2(zI,zE)=O (cid:88) Î± Â·V(1)â—¦DV(1) â—¦<zEU ,zEU >
i i 2 ij j ij i 1 i 2
j
WQ,WK,WV(1),U ,U âˆˆRdÃ—darelearnableparameters. â—¦denotestheelement-wiseproduct. DK,DV(1) :Râ†’Rd
1 2 ij ij
takes||r âˆ’r ||asinputandusesradialbasisfunctionsfollowedbyanon-linearactivationtotransformit.O ,O :Rd â†’Rd
i j 1 2
arelearnablelineartransformations. < Â·,Â· >isdefinedthesamewayasinSection4.2, i.e., givenx,y âˆˆ R3Ã—d,z =<
x,y >âˆˆRd,wherez =x âŠ¤y . Ontheotherhand,theequivariantrepresentationsareupdatedasfollows:
[k] [:,k] [:,k]
V(2) =WV(2)zI,V(3) =WV(3)zI
j j j j
(cid:18) (cid:19)
TorchMD-Equ-Block(zI,zE)=(cid:88) (V(2)â—¦DV(2))âŠ™zE+ r iâˆ’r j (V(3)â—¦DV(3))âŠ¤
i i j ij j ||r âˆ’r || j ij (13)
i j
j
(cid:32) (cid:33)
+O (cid:88) Î± Â·V(1)â—¦DV(1) âŠ™zEU
3 ij j ij i 3
j
WV(2),WV(3),U âˆˆRdÃ—darelearnableparameters. â—¦denotestheelement-wiseproduct. âŠ™isdefinedthesamewayasin
3
Section4.2,i.e.,givenx âˆˆ R3Ã—d,y âˆˆ Rd,z = xâŠ™y âˆˆ R3Ã—d,wherez = x Â·y . DV(2),DV(3) : R â†’ Rd takes
[i,j] [i,k] [k] ij ij
||r âˆ’r ||asinputanduseradialbasisfunctionsfollowedbyanon-linearactivationtotransformit. O :Rd â†’Rd isa
i j 3
learnablelineartransformation. riâˆ’rj âˆˆR3denotestherelativedirectionbetweenatomiâ€™sandjâ€™spositions.
||riâˆ’rj||
WeprovethattheTorchMD-Inv-Block-1andTorchMD-Inv-Block-2canbeviewedasspecialinstancesbyextending
theinvariantself-attentionmoduleandinvariantcross-attentionmoduleofourframeworkrespectively. Concretely,inthe
(cid:16) (cid:17)
invariantself-attentionmodule,wesetÏˆI(zI)=WQzI,ÏˆI,i(zI)=WKzI â—¦DK,ÏˆI,i(zI)=O WV(1)zI â—¦DV(1)
Q i i K j j ij V j 1 j ij
anduseSiLUinsteadofSoftmaxforcalculatingattentionprobability.ByrewritingTorchMD-Inv-Block-1intheequivalent
(cid:16) (cid:17)
formTorchMD-Inv-Block-1(zI) = (cid:80) Î± Â·O V(1)â—¦DV(1) , theinvariantself-attentionmodulecanexpressitby
i j ij 1 j ij
equippingthenumberofatomsforexpressingthesumoperationusingtheattention.
In the invariant cross-attention module, we set ÏˆI(zI) = WQzI,ÏˆI_E,i(zI,zE) = WKzI â—¦DK,ÏˆI_E,i(zI,zE) =
Q i i K j j j ij V j j
(cid:16) (cid:17)
O WV(1)zI â—¦DV(1) â—¦ < U zE,U zE >, and use SiLU instead of Softmax for calculating attention probability.
2 j ij 1 i 2 i
(cid:16) (cid:17)
ByusingtheequivalentformTorchMD-Inv-Block-2(zI,zE) = (cid:80) Î± Â·O V(1)â—¦DV(1) â—¦ < U zE,U zE >, the
i i j ij 2 j ij 1 i 2 i
invariantcross-attentionmodulecanexpressitbyequippingthenumberofatoms.
Moreover, we prove that the TorchMD-Equ-Block can be viewed as a special instance by extending the equivari-
ant cross-attention module of our framework. In particular, we set ÏˆE_I,i(zI,zE) = (WV(2)zI â—¦ DV(2)) âŠ™ zE +
V j j j ij j
(cid:16) (cid:17)
riâˆ’rj (WV(3)zI â—¦ DV(3))âŠ¤ + Î± Â· O WV(1)zI â—¦DV(1) âŠ™ U zE. By rewriting TorchMD-Equ-Block in the
||riâˆ’rj|| j ij ij 3 j ij 3 i
(cid:16) (cid:17)
equivalent form TorchMD-Equ-Block(zI,zE) = (cid:80) (V(2)â—¦DV(2))âŠ™zE + riâˆ’rj (V(3)â—¦DV(3))âŠ¤ +(cid:80) Î± Â·
i i j j ij j ||riâˆ’rj|| j ij j ij
(cid:32) (cid:33)
(cid:16) (cid:17) (cid:16) (cid:17)
O V(1)â—¦DV(1) âŠ™U zE =(cid:80) (V(2)â—¦DV(2))âŠ™zE+ riâˆ’rj (V(3)â—¦DV(3))âŠ¤+Î± Â·O V(1)â—¦DV(1) âŠ™U zE ,
3 j ij 3 i j j ij j ||riâˆ’rj|| j ij ij 3 j ij 3 i
itisstraightforwardthattheequivariantcross-attentionmodulecanexpresstheTorchMD-Equ-Block,e.g.,theparameters
totransformQueryandKeyaretrained/initializedtozero,andthenumberofatomscanbeequippedbyinitialization. Note
thattheparametersoftheremainingblocks(Equ-Self-Attn,Inv-FFN,Equ-FFN)canbetrained/initializedtobezero. In
suchways,theTorchMD-NetmodelcanbeinstantiatedthroughourdesignphilosophyintroducedinSection4.1.
18GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
C.2.ExtensiontoOtherGeometricConstraints
Inthissubsection,weshowcasehowtoextendourframeworktoencodeothergeometricconstraints. Inparticular,we
consider the E(3) group, which comprises translation, rotation and reflection. Formally, let V denote the space of
M
molecular systems, for each atom i, we define equivariant representation Ï•E and invariant representation Ï•I if âˆ€ g =
(t,R)âˆˆE(3),M=(X,R)âˆˆV ,thefollowingconditionsaresatisfied:
M
Ï•E :V â†’R3Ã—d, RÏ•E(X,{r ,...,r })+t1âŠ¤ =Ï•E(X,{Rr +t,...,Rr +t}) (14a)
M 1 n 1 n
Ï•I :V â†’Rd, Ï•I(X,{r ,...,r })=Ï•I(X,{Rr +t,...,Rr +t}) (14b)
M 1 n 1 n
wheret âˆˆ R3 isatranslationvector,R âˆˆ R3Ã—3,det(R) = Â±1isanorthogonaltransformationmatrixandX âˆˆ RnÃ—d
denotestheatomswithfeatures,R = {r ,...,r },r âˆˆ R3 denotesthecartesiancoordinateofatomi. Inparticular,the
1 n i
additionalrequirementistoencodethetranslationandreflectionequivarianceoftheequivariantrepresentations,whichcan
beachievedbymodifyingtheconditionsofourframework(Eqn.(3)).
With the invariant representation ZI and the equivariant representation ZE that satisfy the constraints (Eqn.(14a) and
Eqn.(14b)), we separately redefine the conditions of each component. It is worth noting that the reflection invariance
isdirectlysatisfied(RRâŠ¤ = RâŠ¤R = I)fromtheanalysisinSectionB.1andSectionB.2,whichisrequiredin(1)the
calculationofattentionprobabilityinEqu-Self-Attn,Equ-Cross-Attn;(2)thecalculationofKI_E andVI_E. Thus,we
onlyneedtoencodethetranslationequivarianceconstraint. Giventheupdaterules(Eqn.(3)),itcanbeachievedbysimply
setting each component (Inv-Self-Attn,Inv-Cross-Attn,Equ-Self-Attn,Equ-Cross-Attn,Inv-FFN,Equ-FFN) to be
translation-invariant. Inthisway,theoutputequivariantrepresentationcanpreservetheequivariancetotheE(3)group.
Weextendourframeworktoachievethisgoal,whichisintroducedbelow:
Self-Attentionmodules. ForInv-Self-Attn,theconditionremainsunchanged. ForEqu-Self-Attn,theadditionalcondition
is that ÏˆE should keep the translation invariance. Here we give a simple instantiation: QE = ÏˆE(ZE) = (ZE âˆ’
Q
Âµ )WE,KE =ÏˆE(ZE)=(ZEâˆ’Âµ )WE,VE =ÏˆE(ZE)=(ZEâˆ’Âµ )WE,whereÂµ = 1(cid:80)n ZE 1âŠ¤.
ZE Q K ZE K V ZE V ZE,i d k=1 [i,:,k]
Cross-Attentionmodules. ForInv-Cross-Attn,theconditionforÏˆI remainsunchanged,whileÏˆI_E shouldkeepthe
translationinvariance. ForEqu-Cross-Attn,bothÏˆE andÏˆE_I arerequiredtobetranslation-invariant. Herewegivean
instantiation: QE =ÏˆE(ZE)=(ZE âˆ’Âµ )WE,and
Q ZE Q
KI_E =<(ZEâˆ’Âµ )WI_E,(ZEâˆ’Âµ )WI_E >, VI_E =<(ZEâˆ’Âµ )WI_E,(ZEâˆ’Âµ )WI_E >
ZE K,1 ZE K,2 ZE V,1 ZE V,2 (15)
KE_I =(ZEâˆ’Âµ )WE_I âŠ™ZIWE_I, VE_I =(ZEâˆ’Âµ )WE_I âŠ™ZIWE_I
ZE K,1 K,2 ZE V,1 V,2
Feed-ForwardNetworks. Similarly,theconditionforInv-FFNremainsunchanged. ForEqu-FFN,italsoshouldkeep
thetranslationinvariance,e.g.,Equ-FFN(Zâ€²â€²E)=((Zâ€²â€²E âˆ’Âµ )WE âŠ™GELU(Zâ€²â€²IWI))WE.
ZE 1 2 3
Remark. Withtheaboveadditionalconditions,ourframeworkcanadditionallybeextendedtoencodegeometricconstraints
towardsE(3)group. Notethatthedesignoftheinputlayershouldalsoencodetheconstraints(Eqn.(14a)andEqn.(14b)).
Forexample,theinvariantrepresentationremainsunchangedasZI,0 = X. whiletheequivariantrepresentationcanbe
directlysetasZE,0 =r . Inthisway,thegeometricconstraintsarewellsatisfied.
i i
C.3.FurtherDiscussionontheDesignPhilosophyoftheCross-AttentionModule
In this subsection, we further elucidate the disparity between invariant and equivariant representations in terms of the
informationtheyencapsulate,substantiatedbyadditionalsupportingevidence. Specifically,theadvantagesofinvariant
andequivariantrepresentationscomplementeachother,promptingustobridgethemandcombinetheirstrengthsthrough
cross-attentionmodules.
Fromtheperspectiveofneuralnetworkdesign,equivariantrepresentationspreservemorecompletestructuralinformation
inastraightforwardway,buttheequivariantconstraintsrestrictthechoiceofoperationsthatcanbeused. Ontheother
hand,invariantrepresentationsoffermoreflexibilityfornon-linearoperations,butintroducepotentialrisksofstructural
informationlossorinefficientutilization.
First,weprovidemoreexplanationsandsupportingevidenceonwhyequivariantrepresentationspreservemorecomplete
structuralinformationinastraightforwardway. Intheliterature,existinginvariantmodelscommonlyuseinvariantfeatures
of molecules (e.g., interatomic distances, bond angles, dihedral/torsion angles, improper angles, and so on) which are
19GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
extractedfromtheraw3Dgeometricstructuresofmolecules(thepositionofeachatominthemolecule). Althoughthese
invariant features are important to describe the properties of molecules, the mapping from raw geometric structures to
invariantfeaturesmayintroduceinformationlossorinefficiency:
â€¢ Inarecentstudyontheexpressivepowerofgeometricgraphneuralnetworks(Joshietal.,2023),theauthorsprovided
athoroughtheoreticalanalysisoftheexpressivepowerofinvariantlayersandequivariantlayerscommonlyusedin
geometricgraphneuralnetworksbyleveragingtheGeometricWeisfeiler-Lehmantest.Eachinvariantlayeringeometric
graph neural networks corresponds to the proposed Invariant Geometric Weisfeiler-Lehman test (IGWL). In their
framework,previouslymentionedinvariantfeaturescanbecategorizedintodifferentclassesintermsoftheleastnumber
ofnodes(atomsinthecontextofmolecularmodeling)thatareinvolvedincomputingit. Forexample,interatomic
distancesinvolvetwoatoms,whilebondanglesinvolvethreeatoms. Thesefeaturesarethencalledk-bodyscalars.
Basedonthisframework,theauthorscarefullycharacterizedtheexpressivepowerofinvariantlayersandprovedthat
"anynumberofiterationsofIGWLcannotdistinguishany1-hopidenticalgeometricgraphsG andG andwheretheun-
1 2
derlyingattributedgraphsareisomorphic"and"GWL(correspondstoequivariantlayer)candistinguishbypropagating
informationfromthegeometricstructure",whichindeedindicatestheinformationlossfromrawgeometricstructures
toinvariantfeatures(e.g.,usinginteratomicdistancesorbondangles). Althoughaddingmorecomplexfeatureslike
dihedralanglescouldmitigatethisissue,italsobringsinefficiencyduetothehighcomplexityofsuchfeatures.
â€¢ In PaiNN (SchÃ¼tt et al., 2021), the authors also provided intuition on the point that there exist limits of invariant
representations. Firstly,theauthorsdemonstratedthattoexpressenoughinformation(likebondanglesofallneighbors
ofanatom),usinginvariantrepresentationswouldrequirecomputationswithhighercomplexitythanusingequivariant
representations(seetheanalysisofTable1in(SchÃ¼ttetal.,2021)). Moreover,theauthorsalsoshowedthatequivariant
representations allow the propagation of crucial geometrical information beyond the neighborhood, which is not
possibleintheinvariantcase(seetheanalysisofFigure1in(SchÃ¼ttetal.,2021)). Thisevidencemotivatedtheauthors
todevelopmodelsbasedonequivariantrepresentations.
Second, we also demonstrate that invariant representations allow more flexibility for non-linear operations while the
equivariantconstraintsrestrictthechoiceofequivariantoperationsthatcanbeused. Forinvariantrepresentations,there
existnoconstraintsontheoperationsthatcanbeused. Theinvariantconstraintsarenaturallysatisfiedoncetheinvariant
featuresareextracted. Hence,wecanfreelychoosedifferentoperationdesignswitharbitrarynon-linearitytoincreasethe
modelcapacityandincorporatepriorsfortargetedtasks. However,itisnotthesameforequivariantrepresentations. To
satisfytheequivariantconstraints,operationclassesthatarequalifiedtocorrectlyprocessequivariantrepresentationsare
restricted,andnon-lineartransformationscommonlyusedinneuralnetworkswouldbreaktheconstraintsonequivariant
representations. Thatistosay,althoughequivariantrepresentationscontainmorecompleteinformation,itisrestrictedfor
modelstowellprocessandtransformthisinformation:
â€¢ Most existing works use (1) vector operations; and (2) tensor products of irreducible representations to develop
equivariant operations. For the former one, the non-linearity is constrained. For the latter one, the computational
complexityishighwhichimpedesthedeploymenttolarge3Dsystems.
â€¢ Empirically,previousequivariantmodelscannotconsistentlyachievesuperiorperformanceoninvarianttaskscompared
toinvariantmodels,duetotherestrictednon-linearityorcomputationalcomplexity. Forexample,previousinvariant
modelsarestilldominantininvariantpredictiontasksonPCQM4Mv2andMolecule3D(Table4and5). Moreover,the
modelcapacityofexistingequivariantmodelsisthuslimited,andtheabilitytoscalethemodelsizeupisalsorestricted.
InourpreliminaryexperimentsonpreviousmodelslikePaiNN/TorchMD-Net,itishardtotrainthesemodelswith
largermodelsizes,onwhichweobservedthetraininginstabilityissues.
Based on the above explanations and evidence, we can see that invariant and equivariant representations play indeed
differentroles. Comparedtoinvariantrepresentations,equivariantrepresentationscontainmorecompleteinformationonthe
geometricalstructures. Comparedtoequivariantrepresentations,theinformationcontainedbyinvariantrepresentations
canbebettertransformedandprocessedwithmoreflexibledesignchoicesofoperations. Fromtheseperspectives, the
advantagesofinvariantandequivariantrepresentationscomplementeachother,whichmotivatesustobridgethemand
combinetheirpowerviathecross-attentionmodules,bringingtheunifiedframeworkofourGeoMFormer.
20GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
D.MoreRelatedWorks
Insteadofstickingtothestandardtensorproductoperation,SCN(Zitnicketal.,2022)usedarotationtricktoencodethe
atomicenvironmentbyusingtheirreps. NotethatSCNdoesnotstrictlyobeytheequivarianceconstraint. eSCN(Passaro&
Zitnick,2023)presentedaninterestingobservationthatthesphericalharmonicsfiltershavespecificsparsitypatternsifa
properrotationactsontheinputsphericalcoordinates. CombinedwithpatternsoftheClebsch-Gordancoefficients,eSCN
proposedanefficientequivariantconvolutiontoreduceSO(3)tensorproductstoSO(2)linearoperations,achievingefficient
computation. BuiltupontheeSCNconvolution,EquiformerV2(Liaoetal.,2024)developedascalableTransformer-based
modelthatcanbeappliedtothedomainof3Datomisticsystems.
EGNN(Satorrasetal.,2021)proposedasimpleyeteffectiveequivariantgraphneuralnetworkframework. BasedonEGNN,
GMN(Huangetal.,2022)furtherdevelopsmulti-channelequivariantmodelingusingforwardkinematicsinformationin
physicalsystem. SEGNN(Brandstetteretal.,2022)generalizesthenodeandedgefeaturestoincludevectorsortensors,
andincorporatesgeometricandphysicalinformationbyusingthetensorproductoperations. SAKE(Wang&Chodera,
2023)proposedspatialattentionmechanismthatusesneurallyparametrizedlinearcombinationsofedgevectorstoachieve
equivariance,whichislesscomputationallyintensivethantraditionalsphericalharmonics-basedmethods. UsingGMNas
itsbackbone,SEGNO(Liuetal.,2024)integratedNeuralODEtoapproximatecontinuoustrajectoriesbetweenstatesand
incorporatedsecond-ordermotionequationstoupdatethepositionandvelocityinphysicalsimulation.
E.ExperimentalDetails
E.1.OC20IS2RE
Baselines. WecompareourGeoMFormerwithseveralcompetitivebaselinesforlearninggeometricmolecularrepresenta-
tions. CrystalGraphConvolutionalNeuralNetwork(CGCNN)(Xie&Grossman,2018)developednovelapproachesto
modelingperiodiccrystalsystemswithdiversefeaturesasnodeembeddings. SchNet(SchÃ¼ttetal.,2018)leveragedthe
interatomicdistancesencodedviaradialbasisfunctions,whichserveastheweightsofcontinuous-filterconvolutionallayers.
DimeNet++(Gasteigeretal.,2020a)introducedthedirectionalmessagepassingthatencodesbothdistanceandangular
informationbetweentripletsofatoms.
GemNet(Gasteigeretal.,2021)embeddedallatompairswithinagivencutoffdistancebasedoninteratomicdirections,
andproposedthreeformsofinteractiontoupdatethedirectionalembeddings: Two-hopgeometricmessagepassing(Q-MP),
one-hopgeometricmessagepassing(T-MP),andatomself-interactions. AnefficientvariantnamedGemNet-Tisproposed
tousecheaperformsofinteraction.
SphereNet(Liuetal.,2022b)usedthesphericalcoordinatesystemtorepresenttherelativelocationofeachatominthe
3Dspaceandproposedthesphericalmessagepassing. GNS(Pfaffetal.,2020)isaframeworkforlearningmesh-based
simulations using graph neural networks and can handle complex physical systems. Graphormer-3D (Shi et al., 2022)
extendedGraphormer(Yingetal.,2021a)tolearngeometricmolecularrepresentations, whichencodestheinteratomic
distanceasattentionbiastermsandperformedwellonlarge-scaledatasets. Equiformer(Liao&Smidt,2023)usesthe
tensorproductoperationstobuildanewscalableequivariantTransformerarchitectureandoutperformsstrongbaselineson
thelarge-scaleOC20dataset(Chanussotetal.,2021).
Settings. AsintroducedinSection5.1.1,wefollowtheexperimentalsetupofGraphormer-3D(Shietal.,2022)fora
faircomparison. OurGeoMFormermodelconsistsof12layers. Thedimensionofhiddenlayersandfeed-forwardlayers
issetto768. Thenumberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128. Weuse
AdamWastheoptimizerandsetthehyper-parameterÏµto1e-6and(Î² ,Î² )to(0.9,0.98). Thegradientclipnormissetto
1 2
5.0. Thepeaklearningrateissetto2e-4. Thebatchsizeissetto128. Thedropoutratiosfortheinputembeddings,attention
matrices,andhiddenrepresentationsaresetto0.0,0.1,and0.0respectively. Theweightdecayissetto0.0. Themodelis
trainedfor1millionstepswitha60k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero.
FollowingLiao&Smidt(2023),wealsousethenoisynodedataaugmentationstrategy(Godwinetal.,2022)toimprovethe
performance. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.2.OC20IS2RS
Baselines. Inthisexperiment,wechooseseveralcompetitivebaselinesthatperformwellonequivariantpredictiontasks
formolecules. PaiNN(SchÃ¼ttetal.,2021)builtupontheframeworkofEGNN(Satorrasetal.,2021)tomaintainboth
21GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
invariantandequivariantrepresentationsandfurtherusedtheHardamardproductoperationtotransformtheequivariant
representations. Specializedtensorpredictionblockswerealsodevelopedfordifferentmolecularproperties. TorchMD-
Net(ThÃ¶lke&DeFabritiis,2022)developedanequivariantTransformerarchitecturebyusingsimilarHardamardproduct
operationsandachievedstrongperformanceonvarioustasks.
SpinConv(Shuaibietal.,2021)encodedangularinformationwithalocalreferenceframedefinedbytwoatomsandused
aspinconvolutiononthesphericalrepresentationtocapturerichangularinformationwhilemaintainingrotationinvariance.
Anadditionalpredictionheadisusedtoperformtheequivariantpredictiontask,GemNet-dT(Gasteigeretal.,2021)is
avariantofGemNet-Tthatcandirectlyperformforcepredictionandotherequivarianttasks,e.g.,therelaxedpositions
inthisexperiment. GemNet-OC(Gasteigeretal.,2022)isanextensionofGemNetbyusingmoreefficientcomponents
andachievedbetterperformanceonOC20tasks.
Settings. As introduced in Section 5.1.2, we adopt the direct prediction setting for comparing the ability to perform
equivariantpredictiontasksonOC20IS2RS.Inparticular,were-implementedthebaselinesandcarefullytrainedthese
modelsforafaircomparison. OurGeoMFormermodelconsistsof12layers. Thedimensionofhiddenlayersandfeed-
forwardlayersissetto768. Thenumberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128.
WeuseAdamWastheoptimizerandsetthehyper-parameterÏµto1e-6and(Î² ,Î² )to(0.9,0.98). Thegradientclipnormis
1 2
setto5.0. Thepeaklearningrateissetto2e-4. Thebatchsizeissetto64. Thedropoutratiosfortheinputembeddings,
attentionmatrices,andhiddenrepresentationsaresetto0.0,0.1,and0.0respectively. Theweightdecayissetto0.0. The
model is trained for 1 million steps with a 60k-step warm-up stage. After the warm-up stage, the learning rate decays
linearlytozero. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.3.PCQM4Mv2
Baselines. WecompareourGeoMFormerwithseveralcompetitivebaselinesfromtheleaderboardofOGBLarge-Scale
Challenge(Huetal.,2021). First,wecompareseveralmessage-passingneuralnetwork(MPNN)variants. Twowidelyused
models,GCN(Kipf&Welling,2017)andGIN(Xuetal.,2019)arecomparedalongwiththeirvariantswithvirtualnode
(VN)(Gilmeretal.,2017;Huetal.,2020). Besides,wecompareGINE-VN(Brossardetal.,2020)andDeeperGCN-VN(Li
et al., 2020). GINE is the multi-hop version of GIN. DeeperGCN is a 12-layer GNN model with carefully designed
aggregators. TheresultofMLP-Fingerprint(Huetal.,2021)isalsoreported. Thecomplexityofthesemodelsisgenerally
O(n),wherendenotesthenumberofatoms.
Additionally,wecomparewithafamilyofstrongarchitectures,GraphTransformer(Yingetal.,2021a;Luoetal.,2022;
2023;Zhangetal.,2023),whosecomputationalcomplexityisO(n2). TokenGT(Kimetal.,2022)purelyusednodeand
edgerepresentationsastheinputandadoptedthestandardTransformerarchitecturewithoutgraph-specificmodifications.
EGT(Hussainetal.,2022)usedglobalself-attentionasanaggregationmechanismandutilizededgechannelstocapture
structuralinformation. GRPE(Parketal.,2022)consideredbothnode-spatialandnode-edgerelationsandproposedagraph-
specificrelativepositionalencoding. Graphormer(Yingetal.,2021a)developedgraphstructuralencodingsandintegrated
themintoastandardTransformermodel,whichachievedimpressiveperformanceacrossseveralworldcompetitions(Ying
etal.,2021b;Shietal.,2022). GraphGPS(RampÃ¡Å¡eketal.,2022)proposedaframeworktointegratethepositionaland
structuralencodings,localmessage-passingmechanism,andglobalattentionmechanismintotheTransformermodel. All
thesemodelsaredesignedtolearn2Dmolecularrepresentations.
Therealsoexistseveralmodelscapableofutilizingthe3DgeometricstructureinformationinthetrainingsetofPCQM4Mv2.
Transformer-M(Luoetal.,2023)isaTransformer-basedMolecularmodelthatcantakemoleculardataof2Dor3Dformatsas
inputandlearnmolecularrepresentations,whichwaswidelyadoptedbythewinnersofthe2ndOGBLarge-ScaleChallenge.
GPS++(Mastersetal.,2022)isahybridMPNNandTransformermodelbuiltontheGraphGPSframework (RampÃ¡Å¡eketal.,
2022).ItfollowsTransformer-Mtoutilize3Datompositionsandauxiliarytaskstowinfirstplaceinthelarge-scalechallenge.
Last,weincludetwocomplexmodelswithO(n3)complexity. GEM-2(Liuetal.,2022a)usedmultiplebranchestoencode
thefull-rangeinteractionsbetweenmany-bodyobjectsanddesignedanaxialattentionmechanismtoefficientlyapproximate
theinteractionwithlowcomputationalcost. Uni-Mol+(Luetal.,2023)proposedaniterativepredictionframeworkto
achieveaccuratequantumpropertyprediction. Itfirstgenerated3Dgeometricstructuresfromthe2Dmoleculargraphusing
fastyetinaccuratemethods,e.g.,RDKit(Landrum,2016). Giventheinaccurate3Dstructureastheinput,themodelis
requiredtopredicttheequilibriumstructureinaniterativemanner. Thepredictedequilibriumstructureisusedtopredictthe
quantumproperty. Uni-Mol+simultaneouslymaintainbothatomrepresentationsandpairrepresentations,whichinducethe
tripletcomplexitywhenupdatingthepairrepresentations. Withthecarefullydesignedtrainingstrategy,Uni-Mol+achieves
22GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
state-of-the-artperformanceonPCQM4Mv2whileyieldinghighcomputationalcosts.
Settings. Aspreviouslystated,DFT-calculatedequilibriumgeometricstructuresareprovidedformoleculesinthetraining
set. Themoleculesinthevalidationsetdonothavesuchinformation. WefollowUni-Mol+(Luetal.,2023)totrainour
GeoMFormer. Inparticular,ourmodeltakestheRDKit-generatedgeometricstructuresastheinputandisrequiredtopredict
boththeHOMO-LUMOenergygapandtheequilibriumstructurebyleveraginginvariantandequivariantrepresentations
respectively. Aftertraining, themodelisabletopredicttheHOMO-LUMOgapusingtheRDKit-generatedgeometric
structures. WereferthereaderstoUni-Mol+(Luetal.,2023)formoredetailsonthetrainingstrategies.
OurGeoMFormermodelconsistsof8layers. Thedimensionofhiddenlayersandfeed-forwardlayersissetto512. The
numberofattentionheadsissetto32. ThenumberofGaussianBasiskernelsissetto128. WeuseAdamWastheoptimizer,
andsetthehyper-parameterÏµto1e-8and(Î² ,Î² )to(0.9,0.999). Thegradientclipnormissetto5.0. Thepeaklearning
1 2
rateissetto2e-4. Thebatchsizeissetto1024. Thedropoutratiosfortheinputembeddings,attentionmatrices,andhidden
representationsaresetto0.0,0.1,and0.1respectively. Theweightdecayissetto0.0. Themodelistrainedfor1.5million
stepswitha150k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero. Otherhyper-
parametersarekeptthesameastheUni-Mol+forafaircomparison. Themodelistrainedon16NVIDIATeslaV100GPUs.
E.4.Molecule3D
Baselines. Wefollow(Wangetal.,2022)touseseveralcompetitivebaselinesforcomparisonincludingGIN-Virtual(Hu
etal.,2021),SchNet(SchÃ¼ttetal.,2018),DimeNet++(Gasteigeretal.,2020a),SphereNet(Liuetal.,2022b)whichhave
alreadybeenintroducedinprevioussections. ComENet(Wangetal.,2022)proposedamessage-passinglayerthatoperates
withinthe1-hopneighborhoodofatomsandencodedtherotationanglestofulfillglobalcompleteness. Wealsoimplement
bothPaiNN(SchÃ¼ttetal.,2021)andTorchMD-Net(ThÃ¶lke&DeFabritiis,2022)forcomparisons.
Settings. Following(Wangetal.,2022),weevaluateourGeoMFormermodelonbothrandomandscaffoldsplits. Our
GeoMFormer modelconsists of12layers. The dimensionofhidden layersand feed-forwardlayers isset to768. The
numberofattentionheadsissetto48. ThenumberofGaussianBasiskernelsissetto128. WeuseAdamWastheoptimizer,
andsetthehyper-parameterÏµto1e-8and(Î² ,Î² )to(0.9,0.999). Thegradientclipnormissetto5.0. Thepeaklearningrate
1 2
issetto3e-4. Thebatchsizeissetto1024. Thedropoutratiosfortheinputembeddings,attentionmatrices,andhidden
representationsaresetto0.0,0.1,and0.1respectively. Theweightdecayissetto0.0. Themodelistrainedfor1million
stepswitha60k-stepwarm-upstage. Afterthewarm-upstage,thelearningratedecayslinearlytozero. Themodelistrained
on16NVIDIAV100GPUs.
E.5.N-BodySimulation
Baselines. Following(Satorrasetal.,2021),wechooseseveralcompetitivebaselinesforcomparison. RadialField(KÃ¶hler
etal.,2019)developedtheoreticaltoolsforconstructingequivariantflowsandcanbeusedtoperformequivariantprediction
tasks.TensorFieldNetwork(Thomasetal.,2018)embeddedthepositionofanobjectintheCartesianspaceintohigher-order
representationsviaproductsbetweenlearnableradialfunctionsandsphericalharmonics. InSE(3)-Transformer(Fuchsetal.,
2020),thestandardattentionmechanismwasadaptedtoequivariantfeaturesusingoperationsintheTensorFieldNetwork
model. EGNN(Satorrasetal.,2021)proposedasimpleframework. Itsinvariantrepresentationsencodetypeinformation
andrelativedistance,andarefurtherusedinvectorscalingfunctionstotransformtheequivariantrepresentations.
Settings. Theinputofthemodelincludesinitialpositionsp0 = {p0,...,p0} âˆˆ R5Ã—3 offiveobjects,andtheirinitial
1 5
velocitiesv0 = {v0,...,v0} âˆˆ R5Ã—3 andrespectivechargesc = {c ,...,c } âˆˆ {âˆ’1,1}5. Weencodepositionsand
1 5 1 5
velocitiesviaseparateequivariantstreams,andupdatedthemwithseparateinvariantrepresentationsviacross-attention
modules. Theequivariantpredictionisbasedonbothequivariantrepresentations.
Wefollowthesettingsin(Satorrasetal.,2021)forafaircomparison. OurGeoMFormermodelconsistsof4layers. The
dimensionofhiddenlayersandfeed-forwardlayersissetto80. Thenumberofattentionheadsissetto8. Thenumberof
GaussianBasiskernelsissetto64. WeuseAdamastheoptimizer,andsetthehyper-parameterÏµto1e-8and(Î² ,Î² )to
1 2
(0.9,0.999). Thelearningrateisfixedto3e-4. Thebatchsizeissetto100. Thedropoutratiosfortheinputembeddings,
attentionmatrices,activationfunctions,andhiddenrepresentationsareallsetto0.4,andthedroppathprobabilityissetto
0.4. Themodelistrainedfor10,000epochs. Thenumberoftrainingsamplesissetto3.000. Themodelistrainedon1
NVIDIAV100GPUs.
23GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table7.ResultsonMDtrajectoriesfromtheMD17dataset.ScoresaregivenbytheMAEofenergypredictions(kcal/mol)andforces
(kcal/mol/Ã…).NequIPdoesnotprovideerrorsonenergy,forPaiNNweincludetheresultswithlowerforceerroroutoftrainingonlyon
forcesversusonforcesandenergy.BenzenecorrespondstothedatasetoriginallyreleasedinChmielaetal.(2017),whichissometimes
leftoutfromtheliterature.Ourresultsareaveragedoverthreerandomsplits.
Molecule SchNet PhysNet DimeNet PaiNN NequIP TorchMD-Net GeoMFormer
energy 0.37 0.230 0.204 0.167 - 0.123 0.118
Aspirin
forces 1.35 0.605 0.499 0.338 0.348 0.253 0.171
energy 0.08 - 0.078 - - 0.058 0.052
Benzene
forces 0.31 - 0.187 - 0.187 0.196 0.146
energy 0.08 0.059 0.064 0.064 - 0.052 0.047
Ethanol
forces 0.39 0.160 0.230 0.224 0.208 0.109 0.062
energy 0.13 0.094 0.104 0.091 - 0.077 0.071
Malondialdehyde
forces 0.66 0.319 0.383 0.319 0.337 0.169 0.133
energy 0.16 0.142 0.122 0.116 - 0.085 0.081
Naphthalene
forces 0.58 0.310 0.215 0.077 0.097 0.061 0.040
energy 0.20 0.126 0.134 0.116 - 0.093 0.099
SalicylicAcid
forces 0.85 0.337 0.374 0.195 0.238 0.129 0.098
energy 0.12 0.100 0.102 0.095 - 0.074 0.078
Toluene
forces 0.57 0.191 0.216 0.094 0.101 0.067 0.041
energy 0.14 0.108 0.115 0.106 - 0.095 0.095
Uracil
forces 0.56 0.218 0.301 0.139 0.173 0.095 0.068
E.6.MD17
MD17(Xuetal.,2021)consistsofmoleculardynamicstrajectoriesofseveralsmallorganicmolecules. Eachmoleculehas
itsgeometricstructurealongwiththecorrespondingenergyandforce. Thetaskistopredictboththeenergyandforceof
themoleculeâ€™sgeometricstructureinthecurrentstate. Toevaluatetheperformanceofmodelsinalimiteddatasetting,all
modelsaretrainedononly1,000samplesfromwhich50areusedforvalidation. Theremainingdataisusedforevaluation.
Foreachmolecule,wetrainaseparatemodelondatasamplesofthismoleculeonly. Wesetthemodelparameterbudgetthe
sameasThÃ¶lke&DeFabritiis(2022). Following(ThÃ¶lke&DeFabritiis,2022),wecompareourGeoMFormerwithseveral
competitivebaselines: (1)SchNet(SchÃ¼ttetal.,2018);(2)PhysNet(Unke&Meuwly,2019);(3)DimeNet(Gasteigeretal.,
2020b);(4)PaiNN(SchÃ¼ttetal.,2021);(5)NequIP(Batzneretal.,2022);(6)TorchMD-Net(ThÃ¶lke&DeFabritiis,2022).
TheresultsarepresentedinTable7. ItcanbeeasilyseenthatourGeoMFormerachievescompetitiveperformanceonthe
energypredictiontask(5bestand1tieoutof8molecules)andconsistentlyoutperformsthebestbaselinesbyasignificantly
largemarginontheforcepredictiontask,i.e.,30.6%relativeforceMAEreductioninaverage.
E.7.AblationStudy
Table8.ImpactoftheattentionmodulesonMD17energypredictiontask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn Aspirin Benzene Ethanol Malondialdehyde Naphthalene SalicylicAcid Toluene Uracil
âœ“ âœ“ âœ“ âœ“ 0.118 0.052 0.047 0.071 0.081 0.099 0.078 0.095
Ã— âœ“ âœ“ âœ“ 0.156 0.063 0.058 0.085 0.092 0.115 0.090 0.102
âœ“ Ã— âœ“ âœ“ 0.161 0.062 0.060 0.086 0.111 0.113 0.094 0.101
âœ“ âœ“ Ã— âœ“ 0.131 0.059 0.052 0.081 0.084 0.104 0.085 0.097
âœ“ âœ“ âœ“ Ã— 0.143 0.056 0.051 0.078 0.097 0.106 0.081 0.099
âœ“ Ã— Ã— âœ“ 0.169 0.062 0.061 0.087 0.113 0.115 0.094 0.103
âœ“ Ã— âœ“ Ã— 0.172 0.064 0.061 0.086 0.112 0.116 0.095 0.103
Ã— âœ“ Ã— âœ“ 0.184 0.069 0.064 0.094 0.121 0.124 0.102 0.107
Ã— âœ“ âœ“ Ã— 0.181 0.071 0.067 0.093 0.118 0.121 0.101 0.109
âœ“ Ã— Ã— Ã— 0.193 0.076 0.071 0.097 0.126 0.129 0.105 0.113
In this subsection, we conduct comprehensive experiments for ablation study on each building component of
our GeoMFormer model, including self-attention modules (Inv-Self-Attn,Equ-Self-Attn), cross-attention modules
(Inv-Cross-Attn,Equ-Cross-Attn),feed-forwardnetworks(Inv-FFN,Equ-FFN),layernormalizations(Inv-LN,Equ-LN)
andthestructuralencoding. Alltheresultsshowthateachdesignchoiceconsistentlycontributetotheoverallperformance
ofGeoMFormer,stronglysupportingthemotivationofourdesignphilosophy.
ImpactoftheAttentionModules. AsstatedinSection4,ourGeoMFormermodelconsistsoffourattentionmodules.
Without loss of generality, we conduct the experiments on the MD17 task and the N-body Simulation task to evaluate
the contribution of these attention modules to the overall performance. In particular, we consider all possible ablation
24GeoMFormer:AGeneralArchitectureforGeometricMolecularRepresentationLearning
Table9.ImpactoftheattentionmodulesonMD17forcespredictiontask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn Aspirin Benzene Ethanol Malondialdehyde Naphthalene SalicylicAcid Toluene Uracil
âœ“ âœ“ âœ“ âœ“ 0.171 0.146 0.062 0.133 0.040 0.098 0.041 0.068
Ã— âœ“ âœ“ âœ“ 0.223 0.152 0.086 0.162 0.051 0.117 0.062 0.079
âœ“ Ã— âœ“ âœ“ 0.257 0.159 0.104 0.178 0.063 0.126 0.076 0.091
âœ“ âœ“ Ã— âœ“ 0.292 0.167 0.143 0.311 0.079 0.203 0.096 0.134
âœ“ âœ“ âœ“ Ã— 0.281 0.160 0.167 0.272 0.094 0.185 0.081 0.113
Ã— âœ“ âœ“ Ã— 0.313 0.164 0.196 0.319 0.134 0.191 0.105 0.195
âœ“ Ã— âœ“ Ã— 0.366 0.187 0.212 0.352 0.159 0.264 0.161 0.237
Ã— âœ“ Ã— âœ“ 0.324 0.173 0.189 0.331 0.129 0.237 0.127 0.167
âœ“ Ã— Ã— âœ“ 0.358 0.182 0.219 0.337 0.172 0.272 0.134 0.241
Ã— Ã— âœ“ Ã— 0.411 0.194 0.227 0.361 0.188 0.289 0.173 0.258
Table10.ImpactoftheattentionmodulesontheN-bodySimulationtask.Allotherhyperparametersarethesameforafaircomparison.
Inv-Self-Attn Inv-Cross-Attn Equ-Self-Attn Equ-Cross-Attn PerformanceDrop
âœ— âœ“ âœ“ âœ“ -8.5%
âœ“ âœ— âœ“ âœ“ -8.5%
âœ“ âœ“ âœ— âœ“ -19.1%
âœ“ âœ“ âœ“ âœ— -14.9%
âœ— âœ“ âœ“ âœ— -14.9%
âœ“ âœ— âœ“ âœ— -21.3%
âœ— âœ“ âœ— âœ“ -17.0%
âœ“ âœ— âœ— âœ“ -21.3%
âœ— âœ— âœ“ âœ— -25.5%
configurations that involve ablating one or more of the four modules. Note that for an equivariant prediction task (N-
bodysimulationtaskandMD17forcespredictiontask),thepreservationofatleastoneequivariantattentionmoduleis
necessary. Similarly,foraninvariantpredictiontask(MD17energypredictiontask),atleastoneinvariantattentionmodule
should be preserved. The results are presented in Table 8 , Table 9 and Table 10. All the results consistently indicate
thatallfourattentionmodulessignificantlycontributetoboostingthemodelâ€™sperformance. Specifically,theinclusion
of the cross-attn modules (Inv-Cross-Attn, Equ-Cross-Attn) consistently yields notably significant improvements on
bothinvariantandequivariantpredictiontasks. IntheMD17energypredictiontask(invariant),thereisan18.7%relative
improvementforInv-Cross-Attn,a9.8%relativeimprovementforEqu-Cross-Attn,anda20.8%relativeimprovement
whenbothInv-Cross-AttnandEqu-Cross-Attnareutilized. FortheMD17forcepredictiontask(equivariant),therelative
improvementsare28.0%forInv-Cross-Attn,43.9%forEqu-Cross-Attn,andanimpressive60.8%forthecombineduse
ofInv-Cross-AttnandEqu-Cross-Attn. IntheN-bodysimulationtask(equivariant),therelativeimprovementsare7.8%
forInv-Cross-Attn,13.0%forEqu-Cross-Attn,and17.5%forbothInv-Cross-AttnandEqu-Cross-Attn.
Table11.ImpactoftheFFNmodulesonGe- Table12.ImpactoftheLNmodulesonGeoM- Table13.Impactofstructuralencod-
oMFormer.Allotherhyperparametersarekept Former.Allotherhyperparametersarekeptthe ing on GeoMFormer. All other hy-
thesameforafaircomparison. sameforafaircomparison. perparametersarekeptthesamefor
Inv-FFN Equ-FFN PerformanceDrop Inv-LN Equ-LN PerformanceDrop afaircomparison.
âœ— âœ“ -4.26% âœ— âœ“ -8.51% StructuralEncoding PerformanceDrop
âœ“ âœ— -17.0% âœ“ âœ— -63.8% âœ— -53.2%
âœ— âœ— -21.3% âœ— âœ— -55.3%
Impact of the FFN. We perform ablation study on the N-body Simulation task to ascertain the contribution of both
invariantandequivariantFFNmodulestothemodelâ€™sperformance. Specifically,weexamineallpossiblesettingsinvolving
theablationofoneorbothoftheFFNmodules. TheresultsarepresentedinTable11,whichdemonstratesthatbothFFN
modulespositivelycontributetoenhancingperformance.
ImpactoftheLN. WeemployinvariantandequivariantLNtostabilizetraining. Toinvestigatewhethertheinvariantand
equivariantLNmodulesimproveperformance,weconductablationstudyontheN-bodySimulationtaskthatencompassall
possiblesettingsofablatingoneorbothLNmodules. TheresultsaredisplayedinTable12,demonstratingthatbothLN
modulesconsistentlyhelptoenhanceperformance.
ImpactoftheStructuralEncoding. Weincorporatethestructuralencodingasabiastermwhencalculatingattention
probabilityinourGeoMFormer,asdescribedinSectionA.WeconductablationstudyontheN-bodySimulationtasktosee
ifithelpsboostperformance. ResultsareshowninTable13. Itcanbeseenthattheintroductionofstructuralencodingleads
toimprovedperformance.
25