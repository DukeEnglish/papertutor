Understanding and Mitigating Tokenization Bias in Language Models
BuuPhan*1 MartonHavasi2 MatthewMuckley2 KarenUllrich2
Abstract enhancingvocabularydesignandencodingalgorithmsfor
betterperformanceindownstreamtasks. However,there-
State-of-the-artlanguagemodelsareautoregres-
lationship between compression and model performance
siveandoperateonsubwordunitsknownasto-
remainsunclear. Someresearchsuggeststheimpactofcom- kens. Specifically, one must encode the condi-
pressionisnotalwayspositive(Schmidtetal.,2024;Dagan
tioning string into a list of tokens before pass-
etal.,2024;Goyaletal.,2023). Consequently,understand-
ing to the language models for next-token pre-
ingtokenizationâ€™seffectonmodelperformancecontinues
diction. We show that, for encoding schemes
tobeanopenquestion.
suchasmaximumprefixmatching,tokenization
inducesasamplingbiasthatcannotbemitigated Tokenizationhasbeencriticizedforintroducingmanyshort-
withmore trainingor data. To counterthis uni- comingsinLMs. Theseincludesensitivitytospellingand
versalproblem,weproposeanovelalgorithmto morphologicalstructure(Xueetal.,2022),language-based
obtainunbiasedestimatesfromamodelthatwas biases(Petrovetal.,2024),subparperformanceinspecific
trainedontokenizeddata. Ourmethoddoesnot taskssuchasarithmetic(Singh&Strouse,2024), ornew
require finetuning the model, and its complex- domains(Liuetal.,2023a). Oneapproachtoaddressthese
ity,definedasthenumberofmodelruns,scales issuesisthroughfine-tuningthemodelwithnewvocabu-
linearly with the sequence length. As a conse- laries;however,thisoftencomplicatesthetrainingprocess
quence,weshowthatonecansimulatetoken-free andrequiresdomain-specificexpertise(Chenetal.,2023;
behaviorfromatokenizedlanguagemodel. We Liuetal.,2023b). Furthermore,theperformancegainsdo
empiricallyverifythecorrectnessofourmethod not provide a theoretical understanding of whether these
throughaMarkov-chainsetup,whereitaccurately limitationstrulyarisefromthetokenizationprocessorre-
recoversthetransitionprobabilities,asopposed sultfromsuboptimalmodeltraining. Anotherdirectionis
totheconventionalmethodofdirectlyprompting todeveloptoken-freeLMs(Yuetal.,2024;Nawrotetal.,
tokensintothelanguagemodel. 2022;Tayetal.,2021). Whilethisapproachhaspotential
asiteliminatestokenization-relatedissues,itsignificantly
increasesthecontextlength,resultinginperformancethat
1.Introduction stilllagsbehindtheSOTAtokenizedLMs1(Yuetal.,2024).
Tokenization is a preprocessing procedure used in many Inthisworkweoffernewtheoreticalinsightsonthebehav-
state-of-the-art (SOTA) language models (LMs) such as ioroftokenizedLMs. Weshowthattheyarestatistically
GPTs(Brownetal.,2020),Llama(Touvronetal.,2023)and equivalenttotheirtoken-freecounterparts. Specifically,we
Gemini(Gemini,2023).Itdividestheinputtextintosmaller examinethemaximumprefixencodingschemeemployed
subwordunitswhileretaininglinguisticimportance,helping intheWordPiecetokenizationmethod(Devlinetal.,2018;
toaddressvocabularylimitationssuchasunknownwords. Songetal.,2020)andfindthatthisprocessnotonlyresults
Tokenizationalsoshortens(compresses)theinputcontext inbiasedestimatesofnexttokenprobabilities,butalsoleads
length(Sennrichetal.,2015;Kudo&Richardson,2018). tooverallskewedestimatesofsubsequentcharacterproba-
Sinceeffectivecompressionallowstransformer-basedLMs bilities. Ingeneral,thisbiaspersistsdespiteanincreasein
tohandlelongercontextstrings,manyworks(Zouharetal., trainingdata,evenwithinthesimplesettingofa1st-order
2023;GalleÂ´,2019;Goldmanetal.,2024)havefocusedon Markovchain. Suchbiasoccursduetotheimplicitdisparity
between the domain of the conditioning context, namely,
*Work done during internship at Meta FAIR 1University
charactersversustokens. Nevertheless,wewillshowthat
of Toronto 2Meta AI. Correspondence to: Buu Phan
<truong.phan@mail.utoronto.ca>, Karen Ullrich itispossibletocorrectthisbiaswithoutresortingtofine-
<karenu@meta.com>. tuning. Once adjusted, it becomes possible to simulate
Accepted in ICML 2024 Workshop on Theoretical Foundations 1We refer language models that process tokenized texts as
ofFoundationModels,Vienna,Austria. Copyright2024bythe tokenizedlanguagemodels(tokenizedLMs).
author(s).
1
4202
nuJ
42
]LC.sc[
1v92861.6042:viXraUnderstandingandMitigatingTokenizationBiasinLanguageModels
thetoken-freebehaviorlearnedimplicitlybythetokenized withtheencodingfunctionencode(.)andvocabulary ,and
V
LMandeven(theoretically)mimicthebehaviorofanother theparametersÎ¸areoptimizedtomaximizethepredictive
tokenized model employing a distinct vocabulary set, all likelihoodofthenexttokeninthetokenizeddataset.
withoutrequiringfinetuning. Ourspecificcontributionsare
2.2.Next-CharacterSamplingBias
asfollows:
Wefirstdefinethe(next-character)samplingbiasproblem
â€¢ Weshowthepresenceofabiasinthenext-tokendistribu- thatdescribesthediscrepancybetweenthecharacterlevel
tionthatarisesasaresultofthetokenizationprocess. andtokenlevelpredictionsfortokenizedLMs.
â€¢ We present a novel algorithm capable of transforming Definition2.1. (Next-CharacterSamplingBias)Letthein-
atokenizedLMintoitstoken-freecounterpart,thereby
putpromptstringxn
1
hasti 1=encode(xn 1)asthecorrespond-
effectivelyeliminatingthesamplingbias. ing encoding. The next-character sampling bias occurs
forthispromptwhenP (x xn)=P (x ti)where
â€¢ Weverifythecorrectnessofouralgorithmonlearningthe P (x ti)=(cid:80) Pgt (tn+1 =| t1 tiÌ¸ ) wg ht eren+1 | =1 t
transitionmatrixofak-thorderMarkovchain. g dt ecn o+ d1 e| (t1 ) (xtâˆˆE )gt . i+1 | 1 E { âˆˆ
n+1
V| âˆˆS }
2.ProblemSetup Inotherwords,theprobabilityofthenextcharacterbeing
â€œcâ€ may be different from the sum of the probabilities of
Webeginbyestablishingthetokenizationandlanguagemod-
alltokensthatstartwithâ€œcâ€. Notethatthischaracter-level
elssetupinourpaper. Wethendescribethenext-character
probability offers a broader perspective compared to the
samplingbiasproblemduetotokenization.
probabilityofthesubsequenttokenbeingexactlyâ€œcâ€.
2.1.NotationsandSetup.
Example. Consider a first order Markov chain with two
String Notations. For any string s, we denote its sub- states â€œAâ€,â€œBâ€ asshowninFigure1(left). Eachstring
string from i to j as xj i:=x ix i+1..x j, where each x is is toke{ nized with} = â€œAAâ€,â€œAâ€,â€œBâ€ , which leads to
a character of the alphabet A. For a given string xN 1 , a new Markov chV ain{ whose states and} transition matrix
we define the prefix function that generates a set con- is shown in Figure 1 (right). Details on computing the
taining all possible prefix strings of xN 1 , represented as transitionmatrixofthenewMarkovchainisinAppendix
prefix(xN 1 )= {x1 1,x2 1,x3 1,...,xN 1 }. Also, we define a con- F. We first observe that for the prompt s 1=â€œAAâ€ and
catenationfunctionconcat(.)thatconcatenatesthegiven s =â€Bâ€, there is no bias problem after marginalization2.
2
list of strings, e.g given s 1=xN 11 and s 2=y 1N2, we obtain However, for the prompt s 3=â€œAâ€, the sampling bias oc-
concat(s 1,s 2)=concat(xN 11,y 1N2)=x 1...x N1y 1...y N2. Fi- cursasP gt(x 2=â€œBâ€t 1=â€œAâ€)=1.0,whichisnotequalto
nally,wedenotethesetofallstringsthatstartwithaprefix P (x =â€œBâ€x =â€œA| â€)=Î±,i.e. theoptimallytrainedLM
gt 2 1
xn 1 as S(xn 1)= {s |xn 1âˆˆprefix(s) }. willalwaysou| tputâ€œBâ€. Infact,foranycontextstringthat
endswithtokenâ€œAâ€,e.gâ€œAAAâ€andâ€œB Aâ€(tokensare
Tokenization Setting. We assume having a predefined
| |
separatedbyâ€œâ€),suchLMwillalwaysoutputâ€œBâ€.
vocabulary constructedusinganytokenizationalgorithm
|
V
suchasByte-Pair-Encoding(BPE)orLempel-Ziv-Welch SincethisappliestoanyoptimallytrainedLM,increasing
(LZW),withtheconditionthat . Weusettodenotea the training set size does not mitigate this problem. The
AâŠ†V
tokenin ,i.e.t .Importantly,weusethelongestprefix reasonforthissamplingbiasisthat,duringthetokenization
V âˆˆV
matchingstrategyfortokenization(encoding),denotedas processwithlongestprefixmatching,thetokenâ€œAâ€must
encode(.),similartotheapproachusedintheWordpiece be followed by the token â€œBâ€. Otherwise, the encoding
algorithm(Devlinetal.,2018;Songetal.,2020). Givena process will merge to create a longer token â€œAAâ€. To
sequenceoftokenstk 1, thefunctiondecode(.)returnsthe generalize this phenomenon, we start with the definition
concatenatedstringresultingfromprocessingeachtokenin ofinvalidencodings.
thesequence. Finally,thesetofallstringsthatstartswith
Definition2.2. (InvalidEncodings)Thelistoftokens(an
thetokenstk isdefinedas (tk)= stk=encode(s)k .
1 S 1 { | 1 1} encoding)ti 1 isinvalidifencode(decode(ti 1)) Ì¸=ti 1. Other-
Tokenized LMs. We assume having access to a tok- wise,itisavalidencoding.
enizedautoregressiveLMwithparametersÎ¸thatistrained
For example, let = â€œcâ€,â€œaâ€,â€œtâ€,â€œatâ€,â€œcatâ€ then
with tokens from and maximum prefix matching. The V { }
V [â€œcâ€,â€œatâ€,â€œtâ€]and[â€œcâ€,â€œaâ€,â€œtâ€,â€œtâ€]areinvalidencodings
targetdistributionsonthecharacterdomainisdenotedas
ofâ€œcattâ€. WenowshowinProposition2.3thattheexis-
P (xN xn)andonthetokendomainisP (t ti). For
gt n+1| 1 gt i+1 | 1 tenceofinvalidencodingsintroducessamplingbias,gen-
simplicity, unless otherwise stated, we implicitly assume
eralizing the observed phenomenon in the Markov chain
eachprobabilityterminvolvesÎ¸. Usingthemodel,weas-
exampletoanyautoregressivedistribution.
sumethatonecancomputeP(t ti)foranyintegeri>0.
i+1 | 1
Inthiswork,weconsiderLMstrainedunderthestandard 2For example, we have P (t =â€œAAâ€|t =â€œAAâ€) +
gt i+1 i
setup,whereeachstringsinthedatasetisfirsttokenized P (t =â€œAâ€|t =â€œAAâ€)=Î±=P (x =â€œAâ€|x =â€œAâ€)
gt i+1 i gt n+1 n
2UnderstandingandMitigatingTokenizationBiasinLanguageModels
(1âˆ’Î±)2 Î± 1âˆ’Î²
AA B
TokenVocabulary (1âˆ’Î±)Î²
Î±
ID Token 0.0 Î±Î²
1âˆ’Î± A B 1âˆ’Î² Î±(1âˆ’Î±) 1.0
1 A
Î² 2 B A
3 AA
BeforeTokenization 0.0
InputString:â€œAABABAAAABAABâ€| âˆ’â†’ WordPieceEncoding âˆ’â†’ OutputTokens:â€œAA|B|A|B|AA|AA|B|AA|Bâ€
Figure1:Next-CharactersamplingbiasintroducedbytheWordPieceencodingalgorithm.Inthisexample,giventhecontexttokenâ€œAâ€,
themodelwillalwayspredictthenexttokenasâ€œBâ€withprobability1.0.Wepresentatechniquethat,givenalanguagemodeltrainedon
tokenizeddomain,eliminatethisbiasandrecovertheaccurateunbiasedsamplingdistribution.
Proposition 2.3. (Token-Induced Zero Probability) Let this corresponds to the tokens â€œAAâ€ and â€œBâ€. Also, we
ti be a sequence of input tokens. For any invalid en- assume that any string xN has the first token t âˆ—3.
1 1 1 âˆˆ V
coding ti, we have P (ti)=0.0 and the conditional Considertheinputstringxn anditscorrespondingencod-
1 gt 1 1
probability P (t ti) is undefined. In the case ti ing ti=encode(xn), Proposition 3.1 shows the sufficient
is valid, thengt P i+ (t1 | 1 ti)=0.0 if ti+1 is invalid. Fur1 - condi1 tionfor (ti1 )= (xn).
gt i+1 | 1 1 S 1 S 1
thermore, let xn 1=decode(ti 1), then for any string xN n+1 Proposition3.1. Letsâˆ— =xn,whereti =encode(sâˆ—)=
s Pu gc th
(xN
nth +a 1t
|ti
1e )n =co 0d .0e .(concat(decode(ti 1),xN n+1)) Ì¸=ti 1, then e stn rc ino gde s(x wn
1
h) e. reT th
i
1en =w ee nch oa dv ee (sS1 )(
i
1t ,i
1
w)
eâŠ‚ haS
v1 ( ex Pn 1) (, xi
n
1.e |t.
i
1)fo =r a 1n .0y
.
Inthecaset âˆ—,thenwealsohavethat (ti)= (xn),
Proof. SeeAppendixC. i âˆˆV S 1 S 1
i.e. anystringswherexn prefix(s)musthavethefirsti
Remark1. Proposition2.3impliesthatLMsmaynotfunc- tokensasti andP(ti xn1 )âˆˆ =1.0.
tion as expected when presented with invalid encodings,
1 1| 1
becausethesemodelswillneverbeexposedtosuchinputs Proof. SeeAppendixD.
withinthedataset. Thisdirectlyimpliesthatthepracticeof
TheintuitionforProposition3.1isthatthesubsequentstring
evaluatingLMsunderdifferentencodings(Cao&Rimell, aftert âˆ—cannotchangethetokenizationforxn. Wenow
2021;Chirkovaetal.,2023)issuboptimal. i âˆˆV 1
establishoneofthemainresultsinCorollary3.2.
3.AlleviatingSamplingBias Corollary3.2. FollowingProposition3.1,supposet âˆ—
i
âˆˆV
We propose a method to remove the described bias and thenwehaveP(xN xn)=P(xN ti). Similarly,wealso
n+1| 1 n+1| 1
recover the original token-free autoregressive model, i.e. haveP(tj xn)=P(tj ti).
expressing the implicitly learned P(xN xn) using the
i+1| 1 i+1| 1
n+1| 1 Proof. SeeAppendixD.
tokenized LM that outputs the conditional probability
P(t i+1 |ti 1). For N=n+1, this captures the behavior of WenotethatProposition3.1andCorollary3.2alwayshold,
atoken-freemodel,i.e. samplingthenextcharacterinstead regardless of the value of Î¸. In general, consider when
ofawholetoken. WeassumeourLMfollowsProposition thelasttokenofencode(xn)isnotin âˆ—,wecanrefactor
2.3 on zero probability events and undefined conditional P(xN xn)asfollow: 1 V
probabilityforinvalidencodings. AppendixGjustifiesthis
n+1| 1
P(xN tk)
assumptionandprovidesitspracticalimplementation. P(xN xn)= nk+1| 1 , (1)
n+1| 1 P(xn tk)
nk+1| 1
Our method consists of two stages. In the first stage,
the idea is to identify the condition when P(xN ti) = wherekisthelasttokeninencode(xn)suchthatt âˆ—and
P(xN xn)whereti =encode(xn). Onceidenn t+ ifi1| ed1 ,we xnk=decode(tk),wheren n. Pr1 oofdetailsok fâˆˆ thV isstep
n+1| 1 1 1 1 1 k â‰¤
canrefactortheconditionalprobabilitytomatchthecondi- canbefoundintheAppendixE.WethenusetheBPTree
tioningevents.Inthesecondstage,wecomputeP(xN ti) algorithmtocomputeeachtermintheRHSindividually.
n+1| 1
usingtheLMoutputprobability,i.e. P(t ti),through
i+1 | 1 3.2.TheBranchandPassAlgorithm
thenovelBranchandPass(BPTree)Algorithm.
We present the BPTree algorithm in Algorithm 1, that
3.1.Refactoring allows us to compute the probabilities P(xN tk) and
nk+1| 1
P(xn tk)inEquation(1). Notethatthisalgorithmdoes
Our method removes the bias by connecting character nk+1| 1
notrequiret âˆ—. Detailsonthealgorithmiccorrectness
and token domains through a special subset of tokens k
âˆˆV
âˆ— , whose elements tâˆ— âˆ— are not a substring of areshowninAppendixE.
V âŠ‚V âˆˆV
any other tokens in but itself. For example, given
V
3Thisisnotrestrictive,asmanycurrentlanguagemodelsstarts
= â€œAAAâ€,â€œAAâ€,â€œCBâ€,â€œAâ€,â€œBâ€,â€œCâ€ , then âˆ— = withaspecialbeginningtokeninVâˆ—,e.g.thestarttoken<start>
V { } V
â€œAAAâ€,â€œCBâ€ . In the Markov example in Section 2, inSentencePiece(Kudo&Richardson,2018).
{ }
3UnderstandingandMitigatingTokenizationBiasinLanguageModels
Algorithm1BranchandPassAlgorithmonDiscreteProbability
Vocabulary: ğ’± = { ğ‘ , ğ‘’ , ğ‘ , ğ‘Ÿ , ğ‘› , ğ‘’ ğ‘ , ğ‘’ ğ‘’ ğ‘› , ğ‘ ğ‘’ ğ‘’ ğ‘Ÿ } Language Model: ğ‘ƒ(ğ‘¡ !"#|ğ‘¡ #!)
Tree.ThisalgorithmrecursivelycomputesP(xN |tk). Encoding Rule: Maximum Prefix Matching
nk+1 1
Recursive Recursive Base
1 2:
:
proc //e Bd ru ar ne cB hiP nT gR SE teE p( :xN nk+1,tk 1) Q Inu pe ur
t
y
T
S oktr ein ng s:
:
â€œ
ğ‘¡
ğ‘
#&
ğ‘’ ğ‘’ â€ ğ‘C ğ‘’al ğ‘’l ğ‘Ÿ1 C ğ‘’a ğ‘’ll
ğ‘›
2 C ğ‘’a ğ‘’s ğ‘›e
53 4 :: : /B b /v Ba= l as={ et Ct(cid:80)âˆˆ
âˆˆ aB
sV P e| :x (tN n kk ++ 11 =âˆˆp tr (cid:12) (cid:12)e tfi k 1)x(decode(t))} *ğ‘ƒO *( Nu ğ‘¥t op $$ tu
!
e! ""t
*
*#P %
:
r O=ob nâ€œa lğ‘ yb ğ‘’ ti ol ğ‘’it kâ€y | e: ğ‘¡ n#& s) Branch b val Branch b val
Br
Ban rc ah
nch
ğ‘’ğ‘
6: ifencode(xN
i
)âˆˆVthen selectedbythe BPTree ğ‘¡ !" Pass ğ‘ Pass ğ‘’ Branch ğ‘’
7 8:
:
endr ie fturnb val a el ag co hr i rth em cu ra sr ie ves h co aw ll.n in P Tr oe kv eio nu ss p val p val b val
9: //ExtracttheNextToken: Figure2:BPTreeVisualization.Ateachrecursivecall,theBranch
10: t =encode(xN ) stepfindstokensthatstartswiththequerystringwhilethePass
11:
//k P+ a1
ssingStep:
nk+1 1
stepextractsandemploysthenexttokenandleftoverstringforthe
12: p
val
=P(t k+1(cid:12) (cid:12)tk 1) nextrecursivecalluntilmeetingthebasecase.
1 13 4:
:
p rev ta ul r= nbp vv aa ll +Ã— pB vP aT lree(xN nk+1+1,tk 1+1) 01 .. 80 G Or uo ru Mn ed thTr ou dt :h: PPg (t x( nx +n 1+ |1 x|n 1x )n nâˆ’2) Baseline:P(xn+1|ti 1)
15: endprocedure
0.6
TheideaistomarginalizeoutP(xN tk)byconsidering 0.4
nk+1| 1
twocomplementaryevents: whenthenexttokent k+1hasa 0.2
prefixxN (b intheBranchStep)versuswhenthenext
t Fo ok re mn at lk ln y+ ,k 1+ Bi1 s PTco rv ena etl a cin oe md pw ui tt eh sin thx eN n fk o+ ll1 ow(p iv na gli pn roth be abP ia lis ts ieS st :ep). Figu0 r.0 e3:A OA uA rmeA tA hB odacAB cA urateA lI yB npB eu st tS it maB te aA s tA esthB eAB transiB tB ioA nprB oB bB abil-
ityofa3rdorderMarkovchainwhilethebaselinemethodfailsto.
b
val
=P(xN nk+1,t
k+1
âˆˆB(xN nk+1))(cid:12) (cid:12)tk 1), (2)
= â€œAâ€,â€œBâ€,â€œAAâ€,â€œBAABâ€,â€œBBAAâ€,â€œBBBAâ€,
p val =P(xN nk+1,t k+1 âˆˆ/ B(xN nk+1))(cid:12) (cid:12)tk 1), (3) â€œV BA{ â€,â€œBBAâ€ }. We train a LM model using GPT-2
architecture with 6 hidden layers. Since the model is
where (xN )= t xN prefix(decode(t)) and
B nk+1 { âˆˆV| nk+1âˆˆ } agnostic to the Markov chain order, we average the
weimmediatelyseethatP(xN tk)=b +p .
nk+1| 1 val val probability from 100 runs on different context length
Weprovideanintuitiveexplanationforthealgorithmfol- whilefixingthelast3characters. Wecompareourmethod
lowing the example in Figure 2. Here, we would like to withthebaselineestimatorP(x ti),equivalenttoone
computetheprobabilityP(xnk+3=â€œbeeâ€tk).Thefirstpos- Branch step in the BPTree algn o+ ri1 t| hm1 . Figure 3 shows
nk+1 | 1
sibility is that â€œbeeâ€ is a prefix of the next token, so we theresultswherethebaselinemethodexhibitssignificant
searchforallsuchtokens(line3inthealgorithm)andsum samplingbiasduetotokenization. FollowingProposition
uptheirprobability(line4),i.e. b =P(t =â€œbeerâ€tk). 2.3,onecanclarifythezeroprobabilityeventsoutputfrom
val k+1 | 1
Figure 2 visualizes this step as branching out the tree by thebaselineestimator. Ourmethod,incontrast,accurately
finding all tokens completing the string. Since â€œbeerâ€ is estimatesthegroundtruthprobabilityusedtogeneratethe
nottheonlystringthatcontainsâ€œbeeâ€,e.g. â€œbeepâ€,â€œbeenâ€, data, showing that it is possible to recover the implicitly
etc. we need to compute the probability for these other learnedcharacterinformationfromthetokenizedLMs.
scenarios,eachofwhichhast =â€œbâ€(thefirsttokenin
k+1 5.Conclusion
â€œbeeâ€, line 10 and 12) due to maximum prefix encoding.
Then, we want to compute the probability that the subse- This work identifies the next-character sampling gap be-
quent string is â€œeeâ€ (line 13), given the previous tk and tweenatokenizedmodelandatoken-freeone,whichper-
1
t =â€œbâ€,whichistheoutputoftheBPTreealgorithmbut sistsevenforoptimallytrainedmodels. Wepresentaproba-
k+1
for xnk+3=â€œeeâ€ and tk+1. Formally, inthe Passingstep: bilisticapproachtoeffectivelyeliminatethisbiaswithout
p
=n Pk+ (t2 =â€œbâ€tk)P1
(xnk+3=â€œeeâ€tk,t =â€œbâ€). We requiringadditionaltraining. Thisclosesthesamplinggap
val k+1 | 1 nk+2 | 1 k+1 betweentokenizedandtoken-freemodels,suggestingthat
continuetheprocedureuntilmeetingthebasecase,where
languagemodelsimplicitlyabsorbcharacter-levelinforma-
thestringmustbeaprefixofthenexttoken(usually,when
tion despite being trained solely on tokenized text. This
thereisonlyasinglecharacterleft). Finally,bycomputing
resultimpliesthatitistheoreticallypossibletosimulatethe
thesumofthebranchandpasssteps,weobtainthedesired
conditionalprobabilityb +p =P(xnk+3=â€œbeeâ€tk). behaviorofanotherlanguagemodeltrainedusingdifferent
val val nk+1 | 1 vocabularywithoutanyfine-tuning,sinceitispossibleto
4.Experiments transferfromtoken-freemodelstotokenizedcounterparts.
We validate our method on a 3rd order Markov chain Futureworkseekstoextendourinvestigationforalternative
experiment with = â€œAâ€,â€œBâ€ , where we randomly encodingstrategies,mostnotablythewidelyusedBPEtok-
construct the traA nsit{ ion matrix} and the vocabulary enization(Sennrichetal.,2015),instate-of-the-artmodels.
4
txeNfoytilibaborP AgniebretcarahCUnderstandingandMitigatingTokenizationBiasinLanguageModels
References guidanceai. Guidanceai,2023. URLhttps://github.
com/guidance-ai/guidance. GitHubrepository.
Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Gutierrez-Vasques,X.,Bentz,C.,andSamardzË‡icÂ´,T. Lan-
Askell,A.,etal. Languagemodelsarefew-shotlearners.
guages through the looking glass of bpe compression.
Advancesinneuralinformationprocessingsystems,33:
ComputationalLinguistics,49(4):943â€“1001,2023.
1877â€“1901,2020.
Kudo, T. and Richardson, J. Sentencepiece: A sim-
Cao,K.andRimell,L. Youshouldevaluateyourlanguage
ple and language independent subword tokenizer and
modelonmarginallikelihoodovertokenisations. InPro-
detokenizer for neural text processing. arXiv preprint
ceedingsofthe2021ConferenceonEmpiricalMethods
arXiv:1808.06226,2018.
inNaturalLanguageProcessing,pp.2104â€“2114,2021.
Chen, Y., Marchisio, K., Raileanu, R., Adelani, D., Liu,S.,Deng,N.,Sabour,S.,Jia,Y.,Huang,M.,andMihal-
Saito Stenetorp, P. L. E., Riedel, S., and Artetxe, M. cea,R. Task-adaptivetokenization: Enhancinglong-form
Improvinglanguageplasticityviapretrainingwithactive textgenerationefficacyinmentalhealthandbeyond. In
forgetting. AdvancesinNeuralInformationProcessing The2023ConferenceonEmpiricalMethodsinNatural
Systems,36:31543â€“31557,2023. LanguageProcessing,2023a.
Chirkova,N.,Kruszewski,G.,Rozen,J.,andDymetman,
Liu, Y., Lin, P., Wang, M., and SchuÂ¨tze, H. Ofa: A
M. Shouldyoumarginalizeoverpossibletokenizations?
frameworkofinitializingunseensubwordembeddingsfor
InThe61stAnnualMeetingOfTheAssociationForCom-
efficientlarge-scalemultilingualcontinuedpretraining.
putationalLinguistics,2023.
arXivpreprintarXiv:2311.08849,2023b.
Cleary,J.andWitten,I. Datacompressionusingadaptive
Makkuva,A.V.,Bondaschi,M.,Girish,A.,Nagle,A.,Jaggi,
codingandpartialstringmatching. IEEEtransactionson
M., Kim, H., and Gastpar, M. Attention with markov:
Communications,32(4):396â€“402,1984.
Aframeworkforprincipledanalysisoftransformersvia
Cognetta,M.,Zouhar,V.,Moon,S.,andOkazaki,N. Two markovchains. arXivpreprintarXiv:2402.04161,2024.
counterexamplesto textit TokenizationandtheNoise-
\ {
lessChannel . arXivpreprintarXiv:2402.14614,2024. Minixhofer,B.,Ponti,E.M.,andVulicÂ´,I. Zero-shottok-
}
enizertransfer. arXivpreprintarXiv:2405.07883,2024.
Dagan, G., Synnaeve, G., and Rozie`re, B. Getting the
mostoutofyourtokenizerforpre-traininganddomain
Nawrot,P.,Chorowski,J.,ÅanÂ´cucki,A.,andPonti,E.M.
adaptation. arXivpreprintarXiv:2402.01035,2024.
Efficienttransformerswithdynamictokenpooling. arXiv
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: preprintarXiv:2211.09761,2022.
Pre-training of deep bidirectional transformers for lan-
Petrov,A.,LaMalfa,E.,Torr,P.,andBibi,A. Language
guageunderstanding. arXivpreprintarXiv:1810.04805,
modeltokenizersintroduceunfairnessbetweenlanguages.
2018.
AdvancesinNeuralInformationProcessingSystems,36,
GalleÂ´,M. Investigatingtheeffectivenessofbpe: Thepower 2024.
ofshortersequences. InProceedingsofthe2019confer-
enceonempiricalmethodsinnaturallanguageprocess- Provilkov,I.,Emelianenko,D.,andVoita,E. Bpe-dropout:
ingandthe9thinternationaljointconferenceonnatural Simple and effective subword regularization. arXiv
languageprocessing(EMNLP-IJCNLP),pp.1375â€“1381, preprintarXiv:1910.13267,2019.
2019.
Rajaraman, N., Jiao, J., and Ramchandran, K. To-
Gemini,T. Gemini: afamilyofhighlycapablemultimodal
ward a theory of tokenization in llms. arXiv preprint
models. arXivpreprintarXiv:2312.11805,2023.
arXiv:2404.08335,2024.
Goldman,O.,Caciularu,A.,Eyal,M.,Cao,K.,Szpektor,I.,
Schmidt, C. W., Reddy, V., Zhang, H., Alameddine, A.,
andTsarfaty,R. Unpackingtokenization: Evaluatingtext
Uzan,O.,Pinter,Y.,andTanner,C. Tokenizationismore
compressionanditscorrelationwithmodelperformance.
than compression. arXiv preprint arXiv:2402.18376,
arXivpreprintarXiv:2403.06265,2024.
2024.
Goyal,S.,Ji,Z.,Rawat,A.S.,Menon,A.K.,Kumar,S.,and
Nagarajan,V.Thinkbeforeyouspeak:Traininglanguage Sennrich,R.,Haddow,B.,andBirch,A. Neuralmachine
modelswithpausetokens. InTheTwelfthInternational translation of rare words with subword units. arXiv
ConferenceonLearningRepresentations,2023. preprintarXiv:1508.07909,2015.
5UnderstandingandMitigatingTokenizationBiasinLanguageModels
Singh,A.K.andStrouse,D. Tokenizationcounts: theim-
pactoftokenizationonarithmeticinfrontierllms. arXiv
preprintarXiv:2402.14903,2024.
Song, X., Salcianu, A., Song, Y., Dopson, D., and
Zhou, D. Fast wordpiece tokenization. arXiv preprint
arXiv:2012.15524,2020.
Tay, Y., Tran, V. Q., Ruder, S., Gupta, J., Chung, H. W.,
Bahri,D.,Qin,Z.,Baumgartner,S.,Yu,C.,andMetzler,
D. Charformer: Fastcharactertransformersviagradient-
basedsubwordtokenization. InInternationalConference
onLearningRepresentations,2021.
Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,
M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,N.,Hambro,E.,
Azhar,F.,etal. Llama:Openandefficientfoundationlan-
guagemodels. arXivpreprintarXiv:2302.13971,2023.
Willems, F.M., Shtarkov, Y.M., andTjalkens, T.J. The
context-treeweightingmethod: Basicproperties. IEEE
transactionsoninformationtheory,41(3):653â€“664,1995.
Xue,L.,Barua,A.,Constant,N.,Al-Rfou,R.,Narang,S.,
Kale, M., Roberts, A., and Raffel, C. Byt5: Towards
a token-free future with pre-trained byte-to-byte mod-
els. TransactionsoftheAssociationforComputational
Linguistics,10:291â€“306,2022.
Yu,L.,Simig,D.,Flaherty,C.,Aghajanyan,A.,Zettlemoyer,
L., and Lewis, M. Megabyte: Predicting million-byte
sequences with multiscale transformers. Advances in
NeuralInformationProcessingSystems,36,2024.
Zouhar, V., Meister, C., Gastaldi, J., Du, L., Sachan, M.,
andCotterell,R. Tokenizationandthenoiselesschannel.
InProceedingsofthe61stAnnualMeetingoftheAsso-
ciationforComputationalLinguistics(Volume1: Long
Papers),pp.5184â€“5207,2023.
6UnderstandingandMitigatingTokenizationBiasinLanguageModels
A.RelatedWork
TheoryofTokenization. Existingworksontokenizationgenerallysupporttheideathatcompressingtokensenhances
modelperformance(GalleÂ´,2019;Gutierrez-Vasquesetal.,2023;Zouharetal.,2023). However,theseempericallyfindings
areinconflictedwithotherlaterstudiesCognettaetal.(2024);Schmidtetal.(2024). Onthetheoreticalside,Rajaraman
etal.(2024)examinedtokenizationthroughthelensofunigrammodels,motivatedbytheobservationmadebyMakkuva
etal.(2024)thattransformersstrugglestolearn2nd-orderMarkovchains. We,however,donotobservethisphenomenonin
ourexperiment. Assuch,ourworkonbiasduetotokenizationisnotaffectedbytheirobservation.
Tokenization and Perplexity. Our work relates to the statistical evaluation of LMs, where we provide an algorithm
todirectlyevaluatethecharacter-levelperplexityp(xN xn),usingatokenizedLM.Intermsoftoken-levelperplexity
n+1| 1
evaluation,somerecentstudies(Cao&Rimell,2021;Chirkovaetal.,2023)havesuggestedusingstochastictokenization
(Provilkovetal.,2019)attesttimetoevaluateperplexityscoresofLMs(p(ti)). However,theseevaluationsweredoneon
1
LMstrainedwithdeterministictokenizationwhichcouldbesuboptimalasdemonstratedbyourexaminationofundefined
statesinSection2. Assuch,byutilizingourapproach,onecanobtainamuchmoreaccurateinsightsonLMsevaluation.
RelatedAlgorithms. Ouralgorithmisinspiredfromtheliteratureofuniversalcompressionsuchaspredictionbypartial
matching(Cleary&Witten,1984)andcontext-treeweighting(Willemsetal.,1995), whichhavebeenappliedfortext
predictionbutformuchsimplersettingswithoutanytokenizationinvolved. Recently,Minixhoferetal.(2024);Liuetal.
(2023a)proposetokenizationadaptationmethods,whichstillrequiresaheuristicoptimizationthatcomplicatesthetraining
pipeline. Somerecentstudieshaveproposedmethodtotargettheproblemoflanguagemodelsencounteringdifficulties
generating text nearprompt boundaries (Daganet al., 2024;guidance ai, 2023), which bears someresemblance to our
proposedalgorithm. Thesemethods,however,areheuristicandonlyapplicabletocertainscenarios. Ontheotherhand,our
biasremovalalgorithmistheoreticallycorrect,versatileforvarioussituations,andenablesconversionbetweentoken-free
andtokenizedLMsduetoitsaccuraterepresentationofconditionalsamplingdistributions.
B.SupportingTheoremsonTokenization
Thissectionprovidessupportingtheoremsfortheproofofthemainresults. Wefirstremindthereadersthattheset (xn)
S 1
correspondstothesetofallstringsthatcontainsxnasaprefix. Similarly,theeventset (ti)correspondstothesetofall
1 S 1
stringswhosefirstitokensareti. Considerwhenti =encode(xn),itshouldbenotedthatthetwosetsS(ti)andS(xn)
1 1 1 1 1
arenotguaranteedtobeequivalent. Thatisbecausethesubsequentcharactersafterxncanaffectthetokenizationwithinthe
1
firstncharacter. Weillustratethisinmoredetailsinthefollowingexample.
Example. Consider the Markov chain example in Section 2, where = â€œAAâ€,â€œAâ€,â€œBâ€ . Then, the string s =
1
V { }
â€œAABAABABâ€,thens (x =â€œAâ€)ands (t =â€œAAâ€)sincethefirstcharacterofs isâ€œAâ€andthefirsttoken
1 1 1 1 1
âˆˆS âˆˆS
ofs isâ€œAAâ€. Ontheotherhand,s / (t =encode(x )=â€œAâ€)sinceitsfirsttokenisâ€œAAâ€,notâ€œAâ€.
1 1 1 1
âˆˆS
Weintroducethefollowingpropositionthatcontainstwofactsregardingthemaximumprefixencodingprocess.
PropositionB.1. Letsbeastringwiththeprefixxn(xn prefix(s)). Definetheminimalsuperstringrtobetheprefixof
1 1 âˆˆ
swiththefewesttokensthatcontainsxn asaprefix: r =argmin (k tk =encode(r) xn prefix(r) r prefix(s)).
1 r | 1 âˆ§ 1 âˆˆ âˆ§ âˆˆ
Then,wehavethefollowings:
1. For1 i<k,encode(s) =encode(xn) . Furthermore,whenr =xn,wealsohaveencode(s) =encode(xn) .
â‰¤ i 1 i 1 k 1 k
2. Letâ„“bethenumberoftokensinencode(xn),thenwehavedecode(encode(xn)â„“) prefix(decode(encode(s) )).
1 1 k âˆˆ k
Proof. (Result1.) Proofbycontradiction. Letsbethecounter-examplewiththefewestnumberoftokens. Assumethatfor
1 i<k,encode(s) =encode(xn) . Letj bethesmallestofsuchi.
â‰¤ i Ì¸ 1 i
Considerencode(s) andencode(xn) .
j 1 j
â€¢ Case1: decode(encode(s)j) < xn .
| 1 | | 1|
â€“ Case1.a: decode(encode(s) ) < decode(encode(xn) ). Thisleadstoacontradiction,sincexn isaprefixofs,
| j | | 1 j | 1
thereforealongestprefixmatchingalgorithmwouldalwaysgeneratethelongertoken(encode(xn) )overtheshorter
1 j
one(encode(s) )whenitisavailable.
j
â€“ Case1.b: decode(encode(s) ) > decode(encode(xn) ). Thisleadstoacontradiction,sinceconcat(encode(s)j)
| j | | 1 j | 1
7UnderstandingandMitigatingTokenizationBiasinLanguageModels
isaprefixofxn(Case1assumption),thereforealongestprefixmatchingalgorithmwouldalwaysgeneratethelonger
1
token(encode(s) )overtheshorterone(encode(xn) )whenitisavailable.
j 1 j
â€“ Case1.c: decode(encode(s) ) = decode(encode(xn) ).Thismeansthatthetwotokensarethesame,contradicting
| j | | 1 j |
ourinitialassumption.
â€¢ Case2: decode(encode(s)j) xn . Inthiscase,r =decode(encode(s)j)isasuperstringofxnimplyingthatkisat
| 1 |â‰¥| 1| 1 1
mostj,whichcontradictsourinitialassumptionthat1 j <k.
â‰¤
Finally,inthecaser =xn,thismeansdecode(encode(s) )isasuffixofxn. Sinceallthetokensbeforekwithinxnhas
1 k 1 1
beenmatched,i.e. encode(s) =encode(xn) for1 i<k,thelasttokenmustalsomatchastheresult(else, r = xn ,
i 1 i â‰¤ | |Ì¸ | 1|
leadstocontradiction),wehaveencode(s) =encode(xn) .
k 1 k
(Result 2.) The proof idea is that since r contains xn and any tokens within r and xn has been matched up to k 1,
1 1 âˆ’
then what is left in xn must be in the last token in r (which is the kth token of r). Formally, following Result 1,
1
we have decode(encode(xn)kâˆ’1) = decode(encode(s)kâˆ’1). Since r has k tokens in total and xn prefix(r), this
1 1 1 1 âˆˆ
means that decode(encode(s) ) must cover the rest of xn, i.e. decode(encode(xn)â„“). As the result, we must have
k 1 1 k
decode(encode(xn)â„“) prefix(decode(encode(s) )).
1 k âˆˆ k
Weremindthereaderthedefinitionofinvalidencodingbelow.
DefinitionB.2. (InvalidEncodings)Thelistoftokens(anencoding)tk isinvalidifencode(decode(tk))=tk. Otherwise,it
1 1 Ì¸ 1
isavalidencoding.
CorollaryB.3. (tk)= ifandonlyiftk isinvalid.
S 1 âˆ… 1
Proof. Weproveeachdirectionasfollow.
â€¢ If (tk) = thentk isinvalid: Since (tk) = ,weknowthatthereexistnostringssuchthatencode(s)k = tk. As
S 1 âˆ… 1 S 1 âˆ… 1 1
such,fors=decode(tk),wedonothaveencode(decode(tk))=tk,whichprovestheresult.
1 1 1
â€¢ Iftk isinvalidthen (tk) = : Weprovethisbyshowingthatitisimpossibletohaveastringwithprefixdecode(tk)
1 S 1 âˆ… 1
whosefirstktokensaretk. LetsÂ¯=decode(tk),wehaveanystrings (tk)musthavesÂ¯ prefix(s). Assumesuch
1 1 âˆˆS 1 âˆˆ
stringsexists,wehaveencode(s)k =tk. Letxn =decode(tk),accordingtoPropositionB.1,wehaver =sÂ¯andasthe
1 1 1 1
result,for1 i k,wehaveencode(s) =t =encode(xn) ,whichcontradictsourinitialassumption. Thiscompletes
â‰¤ â‰¤ i i 1 i
theproof.
C.ProofofProposition2.3intheMainPaper
Proposition2.3 (Token-InducedZeroProbability)Letti beasequenceofinputtokens.Foranyinvalidencodingti,wehave
1 1
P (ti)=0.0andtheconditionalprobabilityP (t ti)isundefined. Inthecaseti isvalid,thenP (t ti)=0.0ifti+1
gt 1 gt i+1 | 1 1 gt i+1 | 1 1
isinvalid. Furthermore,letxn=decode(ti),thenforanystringxN suchthatencode(concat(decode(ti),xN ))=ti,
1 1 n+1 1 n+1 Ì¸ 1
thenP (xN ti)=0.0.
gt n+1| 1
Proof. Forthefirsttwostatements,wehave:
â€¢ Foraninvalidti whereti =encode(decode(ti)),wehave (ti)= ,asimpliedbyCorollaryB.3. Assuch,wehave
1 1 Ì¸ 1 S 1 âˆ…
P (ti)=0.0whichleadsP (t ti)tobeanundefinedconditionalprobability.
gt 1 gt i+1 | 1
â€¢ Foravalidti butinvalidti+1,weknowthatP (ti+1)=0.0,whichresultsinP (t ti)=0.0.
1 1 gt 1 gt i+1 | 1
Forthelaststatement,wefirstnotethefollowing:
1. Letsâ€² =decode(ti),notethatP (xN ,ti)=P (sâ€²,xN ,ti)=P (sÂ¯,ti)wheresÂ¯=concat(sâ€²,xN )=xN.
1 gt n+1 1 gt n+1 1 gt 1 n+1 1
2. ConsiderP (sÂ¯,ti)=P (sÂ¯)P (ti sÂ¯),wewillprovethatP (ti sÂ¯)=0.0ifencode(sÂ¯)i =ti.
gt 1 gt gt 1| gt 1| 1 Ì¸ 1
Theideaisthat,whenweappendadditionalcharacterstosÂ¯,itwillneverchangethetokenizationsuchthatthefirstitokens
areti. Formally:
1
8UnderstandingandMitigatingTokenizationBiasinLanguageModels
â€¢ Letj bethefirstpositionsuchthatencode(sÂ¯) =t thenweknowthat decode(encode(sÂ¯) )) > decode(t ) (Proposi-
j j j j
Ì¸ | | | |
tionB.1(Result2)).
â€¢ Following Proposition B.1 (Result 2), let sâˆ— (sÂ¯), then we know that decode(encode(sÂ¯) ) must be a substring of
j
âˆˆ S
withinanotherlongertoken(itcannotbebrokendown). Thiscompletestheproof.
Finally,wenotethatP (ti)=0.0doesnotimpliesencode(decode(ti)) ,sinceitcanbeduetotheoriginaldistribution
gt 1 1 âˆˆV
onthecharacterdomain. AclassicexampleforthisisaMarkovmodelwithanabsorptionstate.
D.ProofofProposition3.1andCorollary3.2intheMainPaper
Proposition3.1Letsâˆ— = xn,whereti = encode(sâˆ—) = encode(xn). Thenwehave (ti) (xn),i.e. foranystring
1 1 1 S 1 âŠ‚ S 1
swhereti =encode(s)i,wehaveP(xn ti)=1.0. Inthecaset âˆ—,thenwealsohavethat (ti)= (xn),i.e. any
1 1 1| 1 i âˆˆV S 1 S 1
stringswherexn prefix(s)musthavethefirstitokensasti andP(ti xn)=1.0.
1 âˆˆ 1 1| 1
Proof. Weproveeachcaseasfollow.
1)GeneralCase: Sincethereexistsastringswithprefixxnwhereencode(s)i =ti,whichfollowsdirectlyfromour1st
1 1 Ì¸ 1
orderMarkovchainexampleinthemainpaper,i.e. thestrings=â€œAAâ€hasâ€œAâ€asprefixbuthavethet =â€œAAâ€=â€œAâ€.
1
Ì¸
Ontheotherhand,weknowthatanystringsthathasthefirstitokensasti musthavethefirstncharactersasxn,assuch
1 1
(ti) (xn)andP(xn ti)=1.0.
S 1 âŠ‚S 1 1| 1
2)t âˆ—: Theproofideaisthat,sincet cannotbeapartofanytokenin ,itisimpossibletomergeitbyappending
i i
âˆˆ V V
additionalcharactersaftert . Formally,similartoPropositionB.1:
i
â€¢ Foranystrings (decode(ti)),letâ„“bethenumberoftokensintheminimalsuperstringrofsthatcontainsxnasa
âˆˆS 1 1
prefix.
â€¢ FollowingPropositionB.1(Result2),weknowthatt mustbeasubstringofdecode(encode(s) ).
i â„“
â€¢ Duetot âˆ—,thent =encode(s) . WealsoknowfromPropositionB.1(Result1)thatencode(s) =t for1 i<â„“,
i i â„“ i i
âˆˆV â‰¤
thismeansthatâ„“=i. Thisgivesusti =encode(s)i andP(ti xn)=1.0.
1 1 1| 1
Thiscompletestheproof.
Remarks. Webrieflynotethattheconditiont âˆ— isthesufficientcondition. Ingeneral,anytokensequenceti that
i âˆˆ V 1
satisfies (ti) = (xn)willhaveP(ti xn) = 1.0. Developinganefficientalgorithmtoverifywhether (ti) = (xn)
S 1 S 1 1| 1 S 1 S 1
remainsataskforfutureresearch.
Corollary3.2 FollowingProposition3.1,supposet âˆ— thenwehaveP(xN xn)=P(xN ti). Similarly,wealso
i âˆˆ V n+1| 1 n+1| 1
haveP(tj xn)=P(tj ti).
i+1| 1 i+1| 1
Proof. Forthefirstcase,weprovethroughthefollowingequations:
P(xN ti)=P(xN ti,xn) (4)
n+1| 1 n+1| 1 1
P(xN ,ti xn)
= n+1 1| 1 (5)
P(ti xn)
1| 1
P(xN xn)P(ti xn,xN )
= n+1| 1 1| 1 n+1 (6)
P(ti xn)
1| 1
=P(xN xn) (7)
n+1| 1
wherethefirstequalityisduetoP(xn ti)=1.0andthelastequalityisduetoP(ti xn)=1.0fort âˆ—.
1| 1 1| 1 i âˆˆV
9UnderstandingandMitigatingTokenizationBiasinLanguageModels
Similarly,forthesecondcase,wehave:
P(tj ti)=P(tj xn,ti) (8)
i+1| 1 i+1| 1 1
P(tj xn)P(ti xn,tj )
= i+1| 1 1| 1 i+1 (9)
P(ti xn)
1| 1
=P(tj xn), (10)
i+1| 1
whichcompletestheproof.
E.ProofforTheBiasRemovalMethod
E.1.Refactoring
OurgoalistoexpressthequantityP(xN xn)usingthetokenizedLMthatoutputstheconditionalprobabilityP(t tiâˆ’1).
Letxnk prefix(xn)wheretk =encon d+ e1 (| xn1 k)andt âˆ—. FollowingProposition3.1,anystringswithprefixxni k| m1 ust
1 âˆˆ 1 1 1 k âˆˆV 1
havethefirstktokensastk. Wenowperformthefollowingfactorization:
1
P(xN xn)=P(xN xnk,xn ) (11)
n+1| 1 n+1| 1 nk+1
P(xN ,xn xnk)
= n+1 nk+1| 1 (12)
P(xn xnk)
nk+1| 1
P(xN xnk)
= nk+1| 1 (13)
P(xn xnk)
nk+1| 1
P(xN tk)
= nk+1| 1 , (14)
P(xn tk)
nk+1| 1
wherethelastinequalityisduetoCorollary3.2. Finally,wewillusetheBranchandPassAlgorithmtocomputeeachterm
in(14)individually. Notethatthealgorithmdoesnotrequiret âˆ—. Here,weexplicitlyhighlighttheimportanceof
k
âˆˆ V
havingt âˆ—,asitbridgesbetweenthecharacterandtokendomainthroughEquation(14).
k
âˆˆV
E.2.TheBranchandPassAlgorithm
Overview. TheBranchandPass(BPTree)algorithmhastwoinputs: thequerystringvaluexN andtheconditioning
nk+1
tokenstk,whichoutputsthevalueP(xN tk). Notethatwedonotrequiret âˆ—intheBPTreealgorithm. First,using
1 nk+1| 1 k âˆˆV
marginalization,wehavethefollowing:
(cid:88)
P(xN tk)= P(xN ,t =ttk) (15)
nk+1| 1 nk+1 k+1 | 1
tâˆˆV
(cid:88) (cid:88)
= P(xN ,t =ttk)+ P(xN ,t =ttk) (16)
nk+1 k+1 | 1 nk+1 k+1 | 1
tâˆˆTbval tâˆˆTpval
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
bval pval
where = tt xN prefix(decode(t)) and = tt xN /prefix(decode(t)) ,where =
Tbval { | âˆˆVâˆ§ nk+1âˆˆ } Tpval { | âˆˆVâˆ§ nk+1âˆˆ } Tbvalâˆ©Tpval
,arethecorrespondingsetweiteratesovereachsuminthebranchstep,i.e. b ,andpassstep,i.e. p ,respectively.
val val
âˆ…
Intuitvely, isthesetoftokensthathaveaprefixxN and istheonesthatdonot. Wenowshowhowtocompute
Tbval nk+1 Tpval
eachtermseparatelyusingatokenizedLMthatoutputsP(t ti).
i+1 | 1
BranchStep. Notethatb canbedescribedastheprobabilitythat,giventhelistofprevioustokenstk,thenexttokenof
val 1
thestringshasxN asaprefix. Assuch,toobtainb ,wefirstcomputeP(t =ttk)forallt usingonemodel
nk+1 val k+1 | 1 âˆˆV
run,thensumtheprobabilitiescorrespondstoalltokenswhoseprefixisxN . Specifically:
nk+1
(cid:88)
b = P(t =ttk), (17)
val k+1 | 1
tâˆˆTbval
10UnderstandingandMitigatingTokenizationBiasinLanguageModels
Proof. Toseethis,foreachsummandofb inEq.(16),wehave:
val
P(xN ,t =ttk)=P(t =ttk) P(xN tk,t =t) (18)
nk+1 k+1 | 1 k+1 | 1 Ã— nk+1| 1 k+1
=P(t =ttk), (19)
k+1 | 1
wherethesecondequationisduetoxN prefix(t)soP(xN tk,t =t)=1.0. Thisconcludestheproof.
nk+1âˆˆ nk+1| 1 k+1
PassStep. Incontrasttob ,thep istheprobabilitythat,giventhelistofprevious(valid)tokenstk,thestringshasthe
val val 1
subsequentstringasxN thatisnotaprefixofthenexttoken. Wewillshowthatunderthemaximumprefixencoding,we
nk+1
computethevaluep asfollow:
val
p =P(t =ttk) P(xN tk,t =t), (20)
val k+1 | 1 Ã— nk+1+1| 1 k+1
wheret=encode(xN ) andxnk+1 =decode(t). Thatis,duringthepassingstep,therearetwosubroutines:
nk+1 1 nk+1
1. ObtainthenexttokentwithinxN andcomputeP(t =ttk). IfxN =decode(t),thenreturns0.0sincethisis
nk+1 k+1 | 1 nk+1
notallowedaccordingtotheconditionrequiredin .
Tpval
2. Else,recursivelycomputeP(xN tk,t =t).
nk+1+1| 1 k+1
Proof. We first rewrite as the union of the following disjoint sets, i.e = and = ,
Tpval Tpval Tpval,1âˆªTpval,2 Tpval,1âˆ©Tpval,2
âˆ…
where:
â€¢ = tt xN / prefix(decode(t)) decode(t) / prefix(xN ) : thesubsetof wherethestring
Tpval,1 { | âˆˆ V âˆ§ nk+1 âˆˆ âˆ§ âˆˆ nk+1 } Tpval
representedbythetokentisnotaprefixofxN .
nk+1
â€¢ = tt xN / prefix(decode(t)) decode(t) prefix(xN ) : the subset of where string
Tpval,2 { | âˆˆ V âˆ§ nk+1 âˆˆ âˆ§ âˆˆ nk+1 } Tpval
representedbythetokentisactuallyaprefixofxN .
nk+1
Assuch,wehave:
(cid:88) (cid:88) (cid:88)
p = P(xN ,t =ttk)= P(xN ,t =ttk)+ P(xN ,t =ttk), (21)
val nk+1 k+1 | 1 nk+1 k+1 | 1 nk+1 k+1 | 1
tâˆˆTpval tâˆˆTpval,1 tâˆˆTpval,2
(cid:124) (cid:123)(cid:122) (cid:125)
0.0
wherewenotethatfort ,wehaveP(xN tk,t =t)=0.0sincetheconditionalt =timpliesthatthe
âˆˆTpval,1 nk+1| 1 k+1 k+1
stringrepresentedbytmustbeaprefixofxN ,whichleadstothefirstsumbeing0.0.
nk+1
Forthesecondterm, i.e. t , ift = encode(xN ) , byfollowingProposition2.3, wehaveP(xN ,t =
âˆˆ Tpval,2 Ì¸ nk+1 1 nk+1 k+1
ttk) = 0.0sinceanytokenizationthatisnotavalidmaximumprefixencodingmusthappenwithprobability0.0. This
| 1
standscorrectedaslongastheoutputprobabilityofourLMfollowsProposition2.3(seeSectionG.2).
As a result, we are left with the token t = encode(xN ) , which is the Equation (20) after applying chain rule of
nk+1 1
probability.
BaseCase. WenotethatthebasecaseofouralgorithmcorrespondstothesituationwherexN =decode(t). Inthis
nk+1
scenario,weonlyneedstocomputeb (branchingstep)whilep =0.0.
val val
ComplexityAnalysis. Thecomplexityofouralgorithm(numberofinferencesonthelanguagemodel)scaleswiththe
lengthofthethequerystring,i.e. N n . NotethatthecomplexityofthesummationattheBranchingstepisrelatively
k
âˆ’
cheapcomparedtotheruntimeofthelanguagemodel.
F.ConvertingToken-FreeLanguageModeltoTokenizedLanguageModel.
WeintroduceanalgorithmtocomputeP(t tk)usingatoken-freelanguagemodelP(xN xn),despitehavingnoaccess
k+1 | 1 n+1| 1
toanytokenizedLM.Thisapproachenablestheoreticalconversionofatoken-freemodeltoatokenizedone. Themethod
involvestwostages. First,werefactortheconditionalprobabilitysimilartothetechniquepresentedinSectionE.Next,we
aggregatetheprobabilitiesofallpossiblestringsleadingtothedesiredtokenization. ItisimportanttonotethataMarkov
chainisaspecialtypeofautoregressivemodel,meaningthismethodcanbeemployedtoeffortlesslycalculateMarkovchain
transitionmatriceswithinthetokenizeddomain.
11UnderstandingandMitigatingTokenizationBiasinLanguageModels
F.1.Refactoring
ConsidertheprobabilityP(t ti)thatwewouldliketoexpressedusingP(xN xn). Lett bethelasttokenwithinti
i+1 | 1 n+1| 1 k 1
suchthatt âˆ—. Wenowperformthefollowingfactorization:
k
âˆˆV
P(ti+1 tk)
P(t ti)= k+1| 1 (22)
i+1 | 1 P(ti tk)
k+1| 1
P(ti+1 xnk)
= k+1| 1 , (23)
P(ti xnk)
k+1| 1
where xnk = decode(tk). The second equality is due to Corollary 3.2. Each term can then be computed using the
1 1
aggregationprocedureshownnext.
F.2.Aggregation.
In this step, we would like to compute P(ti xnk) where encode(xnk) = tk and t âˆ—, using the token-free
representationP(x xn). Here,wedenotek d+ ec1 o| d1 e(ti )=xni and1 M =m1 ax k decâˆˆ odV e(t) bethelengthofthe
n+1 | 1 k+1 nk+1 tâˆˆV | |
longesttokeninV andâ„¦= M istheenumerationofallstringoflengthM.
A
ComputingP(ti xnk)involvesconsideringallpossiblestringsswithprefixxni andti =encode(s)i . Although
k+1| 1 1 k+1 k+1
iterating through every possible string is infeasible, we can restrict our search by only examining strings with length
s =n +M,asanyadditionalstringbeyondthispointwillnotimpactthetokenizationofprefixxniduetoM beingthe
| m| aximui mtokenlength. Formally,wewillshowthatonecanexpressP(ti xnk)asfollows: 1
k+1| 1
(cid:88)
P(ti xnk)= P(xni+M=c (sâ€²)xnk)1(ti =encode(c (sâ€²))i ), (24)
k+1| 1 nk+1 1 | 1 k+1 2 k+1
sâ€²âˆˆAM
wherec (sâ€²):=concat(xni ,sâ€²)andc (sâ€²):=concat(xni,sâ€²). Thefirsttermcanbecomputedusingthegiventoken-
1 nk+1 2 1
freeLM,i.e. P(xni+M xnk). Thesecondtermisanindicatorfunctionthatcheckswhetherti =encode(s)i andcan
nk+1 | 1 k+1 k+1
becomputeddeterministically.
Proof. Wehave:
P(ti xnk)=P(ti ,xni xnk) (25)
k+1| 1 k+1 nk+1| 1
(cid:88)
= P(ti ,xni+M=c (sâ€²)xnk) (26)
k+1 nk+1 1 | 1
sâ€²âˆˆAM
(cid:88)
= P(xni+M=c (sâ€²)xnk)P(ti xni+M =c (sâ€²)) (27)
nk+1 1 | 1 k+1| 1 2
sâ€²âˆˆAM
(cid:88)
= P(xni+M=c (sâ€²)xnk)1(ti =encode(c (sâ€²))i ) (28)
nk+1 1 | 1 k+1 2 k+1
sâ€²âˆˆAM
Therestistoprovethefollowingequality:
P(ti xni+M =c (sâ€²))=1(ti =encode(c (sâ€²))i ) (29)
k+1| 1 2 k+1 2 k+1
Wefirstnotethatthefirstktokensmustbetk =encode(xnk)duetoourconditionthatt âˆ—. SinceM isthelengthof
thelongesttokenin ,appendingextrachar1 acterscannotc1 hangethetokenizationhappek neâˆˆ dV forxni. Inotherwords,any
stringswithprefixcV (sâ€²)musthavethesameminimalsuperstringrcontainingxni (seePropositio1 nB.1). Wethenapply
2 1
thisprincipletothetwocases:
â€¢ ti =encode(c (sâ€²))i : In this case, we know that the string must contains the first i tokens as ti, hence
k+1 2 k+1 1
P(ti xni+M =c (sâ€²))=1.0
k+1| 1 2
â€¢ ti =encode(c (sâ€²))i : Incontrast,thiscaseisequivalenttoP(ti xni+M =c (sâ€²))=0.0sincewearesurethat
k+1Ì¸ 2 k+1 k+1| 1 2
thestringdonotcontainsthetokensti .
k+1
12UnderstandingandMitigatingTokenizationBiasinLanguageModels
Thisconcludestheproof.
F.3.TheMarkovChainExample.
WeprovideadetailcomputationoftheMarkovchainexampleinthemainpaper. Recallthatintheoriginalchain(inthe
characterdomain),wehavethefollowing:
P(x =â€œAâ€x =â€œAâ€)=Î± (30)
2 1
|
P(x =â€œBâ€x =â€œAâ€)=1 Î± (31)
2 1
| âˆ’
P(x =â€œAâ€x =â€œBâ€)=Î² (32)
2 1
|
P(x =â€œBâ€x =â€œBâ€)=1 Î² (33)
2 1
| âˆ’
WealsoassumetheinitialprobabilityÏ€ = Î³,1 Î³ forâ€œAâ€andâ€œBâ€respectively. Inthetokendomain,letfirstcompute
{ âˆ’ }
P(t = â€œAâ€t = â€œAAâ€),wherewedonothavetodotherefactoringstepsinceweknowthatt âˆ—. Followingthe
2 1 1
| âˆˆ V
Aggregationstep,wehave:
P(t =â€œAâ€t =â€œAAâ€)=P(x6 =â€œABAâ€x2 =â€œAAâ€)+P(x6 =â€œABBâ€x2 =â€œAAâ€) (34)
2 | 1 3 | 1 3 | 1
=P(x5 =â€œABâ€x2 =â€œAAâ€) (35)
3 | 1
=Î±(1 Î±), (36)
âˆ’
whereinthefirstequality,wedonotincludethecasex6 =â€AAAâ€andx6 =â€AABâ€sinceencode(â€AAAâ€) =â€œAAâ€
3 3 1
and encode(â€AABâ€) = â€œAAâ€, which are not the token â€œAâ€ that we are interested in. For other tokens and when
1
t =â€œBâ€,thecomputationfollowsthesamearguments.
1
WenowconsiderthecaseP(t =â€œBâ€t =â€œAâ€),wecanrefactoritas:
2 1
|
P(t =â€œBâ€,t =â€œAâ€)
P(t =â€œBâ€t =â€œAâ€)= 2 1 (37)
2 | 1 P(t =â€œAâ€)
1
WefirstcomputeP(t =â€œAâ€)usingtheaggregationstep:
1
P(t =â€œAâ€)=P(x3 =â€œABBâ€)+P(x3 =â€œABAâ€) (38)
1 1 1
=P(x2 =â€œABâ€) (39)
1
=Î³(1 Î±), (40)
âˆ’
wherewedoagainincludethecasex6 =â€AAAâ€andx6 =â€AABâ€forthesamereasonabove.ForP(t =â€œAâ€,t =â€œAâ€)
3 3 2 1
wehave:
P(t =â€œBâ€,t =â€œAâ€)=P(x4=â€œABAAâ€)+P(x4=â€œABABâ€)+P(x4=â€œABBAâ€)+P(x4=â€œABBBâ€) (41)
2 1 1 1 1 1
=P(x2 =â€œABâ€) (42)
1
=Î³(1 Î±) (43)
âˆ’
whichgivesusP(t =â€œBâ€t =â€œAâ€)=1.0. Finally,inthisspecificcase,sinceorderoftheMarkovchaininthecharacter
2 1
|
domainis1,wedonotneedtoconsiderthehigherorderoftheMarkovchaininthetokendomain.
G.OnPredictiveDistributionofLMs
Inpractice,ourLMdoesnotfollowProposition2.3duetosoftmaxactivations. Assuch,inourBPTreealgorithm,when
t and t = encode(xN ) , then P (xN ,t = ttk) may not be 0.0 (where Î¸ is the model weights).
âˆˆ Tpval,2 Ì¸ nk+1 1 Î¸ nk+1 k+1 | 1
Eventually,thiscanpotentiallyincreasethecomplexityofourBPTreealgorithmduringthePassingstep.
Inthefirstpartofthissection,weshowthatgivenanytokenizedLM,wecanforceitsoutputprobabilitestoobeyProposition
2.3, without any loss in terms of perplexity score on the token domain. In the second part, for completeness, we will
showthathavingatokenizedLMsatifyingPropostion2.3willguaranteethecorrectnessofthePassingstepinourBPTree
algorithm.
Finally,beforegoingtothemethod,weremindthereadersthatProposition3.1andCorollary3.2arefactuallycorrectand
holdforallÎ¸. Assuch,therefactoringstepholdsregardless.
13UnderstandingandMitigatingTokenizationBiasinLanguageModels
G.1.Truncate-RenormalizationProcess
WejustifytheassumptionthatourtokenizedlanguagemodelP (t ti)followsProposition2.3. Theideaisthatwecan
Î¸ i+1 | 1
turnalanguagemodelthatdoesnotfollowProposition2.3totheonethatdoeswhileguaranteeingthatthenewmodelwill
alwaysresultsinahighertoken-levelperplexityscore.
WefirstintroducePropositionG.1. Inthisproposition,wearegivenatargetdiscreteprobabilitydistributionpwherewe
knowsomeofthevalueswillnothappen,saysÎ¦âˆ—. Assumethatwehaveanotherdistributionqthatapproximatesp,thenwe
canproduceanotherdistributionqâˆ—thatisclosertopintermsofKLdivergencebysettingcorrespondingprobabilitiesofq
inÎ¦âˆ—to0.0andrenormalizeit(similartorejectionsampling).
PropositionG.1. Givenadiscretedistributionp= p ,p ,...,p andq = q ,q ,...,q withq >0.0foralli. Let
1 2 m 1 2 m i
Î¦= i Zp =0.0 andÎ¦âˆ— Î¦,wedefineqâˆ— = { qâˆ—,qâˆ—,...,qâˆ— } whereqâˆ— ={ 0.0fori Î¦} âˆ—,andqâˆ— =q /((cid:80) q ).
{ âˆˆ | i } âŠ† { 1 2 m} i âˆˆ j j lâˆˆ/Î¦âˆ— l
Thenwehave:
D (p qâˆ—) D (p q), (44)
KL KL
|| â‰¤ ||
whichimpliesthatqâˆ—isclosertopthanq. Werefertheprocessofproducingqâˆ—astruncate-renormalization(TR).
Proof. Let Z = ((cid:80) q ) is the normalizing factor in qâˆ—. Note that Z 1 and as such log(Z) 0. The proof is as
lâˆˆ/Î¦ l â‰¤ â‰¤
follows:
(cid:18) (cid:19)
D (p qâˆ—)=(cid:88) p log p i (45)
KL || i qâˆ—
i i
(cid:18) (cid:19)
= (cid:88) p log p i ,use0log0=0.0 (46)
i qâˆ—
iâˆˆ/Î¦âˆ— i
(cid:18) (cid:19)
= (cid:88) p log p i (47)
i q /Z
i
iâˆˆ/Î¦âˆ—
(cid:34) (cid:18) (cid:19)(cid:35)
= (cid:88) p log p i +log(Z) (48)
i q
i
iâˆˆ/Î¦âˆ—
(cid:18) (cid:19)
(cid:88) p log p i =D (p q), (49)
â‰¤ i q KL ||
i
iâˆˆ/Î¦âˆ—
whichcompletestheproof.
Applyingthistoourscenario,wherewearegivenanyautoregressivelanguagemodelsPË† (t ti)thatdoesnotfollow
Î¸ i+1 | 1
Proposition2.3(duetothesoftmaxactivations),wecanperformtheTRprocess(sinceweknowwhichencodingisinvalid)
toobtainanewdistributionP (t ti),whichisguaranteedtobeclosertotheground-truthmodelP (t ti). Asthe
Î¸ i+1 | 1 gt i+1 | 1
results,weareguaranteedthatthetoken-levelperplexityscoreofP (t ti)isalwayslowerthanorequaltoPË† (t ti).
Î¸ i+1 | 1 Î¸ i+1 | 1
G.2.OnPassingStepinBranchandPassAlgorithm.
Once our tokenized LM follows Proposition 2.3, intuitively it can be regarded as an autogressive source (in the token
domain)soitdoesnotalternatethecorrectnessofthePassingstep. Thissectionprovidesaproofforcompleteness.
Notethatweonlyneedtoconsiderthesecondtermofp ,i.e.:
val
(cid:88) (cid:88)
p = P(xN ,t =ttk)+ P(xN ,t =ttk), (50)
val nk+1 k+1 | 1 nk+1 k+1 | 1
tâˆˆTpval,1 tâˆˆTpval,2
(cid:124) (cid:123)(cid:122) (cid:125)
0.0
as the other term is always 0.0 regardless. Specifically, we will prove that, during the Pass step, when t and
âˆˆ
Tpval,2
t=encode(xN ) ,thenP(xN ,t =ttk)=0.0giventhatourLMfollowsProposition2.3. Iftk+1isaninvalid
Ì¸ nk+1 1 nk+1 k+1 | 1 1
encoding,weimmediatelyhaveP(xN ,t =ttk)=0.0,soweonlyconsiderwhentk+1isavalidencoding. Wefirst
nk+1 k+1 | 1 1
provethefollowingProposition.
14UnderstandingandMitigatingTokenizationBiasinLanguageModels
PropositionG.2. Iftk isinvalid,thentk+1isalsoinvalid.
1 1
Proof. Proofbycontradiction,supposetk+1isvalidthen (tk+1)= . However,anystrings (tk+1)mustalsobein
(tk). But (tk+1)= (CorollaryB.3).1 Assuch,tk+1isS inva1 lid. Ì¸ âˆ… âˆˆS 1
S 1 S 1 âˆ… 1
SincexN hastotallym = N (n +1)characters,itcanhavemaximummtokens. Let = Ï„ mâˆ’1 xN
nk+1 âˆ’ k C { âˆˆ V | nk+2 âˆˆ
prefix(decode(Ï„)) bethesetofallpossiblesequenceofm 1tokens(bothvalidandinvalid). Wethenhave:
} âˆ’
(cid:88)
P(xN ,t =ttk)= P(xN ,t =t,tk+m =Ï„ tk) (51)
nk+1 k+1 | 1 nk+1 k+1 k+2 | 1
Ï„âˆˆC
(cid:88)
= P(t =t,tk+m =Ï„ tk)P(xN tk,t =t,tm =Ï„) (52)
k+1 k+2 | 1 nk+1| 1 k+1 k+2
Ï„âˆˆC
Then,itissufficienttoshowthatP(t =t,tk+m =Ï„ tk)=0.0foranyÏ„ aslongastdoesnotfollowthemaximum
k+1 k+2 | 1 âˆˆC
prefixencoding,i.e. t=encode(xN ) . Wehavethefollowings:
Ì¸ nk+1 1
â€¢ Sincet =tdoesnotfollowthemaximumprefixencoding,thenencode(decode(tk+m))=tk+msincewecanalways
k+1 1 Ì¸ 1
changethetokenizationatt .
k+1
â€¢ Lett bethefirsttokenwheretk+j isinvalid. ThenfollowPropositionG.2,sincetk+j isinvalid,thenanyextratokens
k+j 1 1
aftertk+j mustalsobeinvalid.
1
Assuch,wehave: P(t tk+jâˆ’1)=0.0,whichleadstoP(t =t,tk+m =Ï„ tk)=0.0,whichprovestheresults.
k+j | 1 k+1 k+2 | 1
15