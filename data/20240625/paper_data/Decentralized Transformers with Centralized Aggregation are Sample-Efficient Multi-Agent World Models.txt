Decentralized Transformers with Centralized
Aggregation are Sample-Efficient Multi-Agent World
Models
YangZhang1,2âˆ—, ChenjiaBai2â€ , BinZhao2,3, JunchiYan2,4, XiuLi1, XuelongLi2,5
1TsinghuaUniversity,2ShanghaiAILaboratory,3NorthwesternPolytechnicalUniversity
4SchoolofArtificialIntelligence,ShanghaiJiaoTongUniversity
5InstituteofArtificialIntelligence(TeleAI),ChinaTelecom
Abstract
Learningaworldmodelformodel-freeReinforcementLearning(RL)agentscan
significantly improve the sample efficiency by learning policies in imagination.
However,buildingaworldmodelforMulti-AgentRL(MARL)canbeparticularly
challengingduetothescalabilityissueinacentralizedarchitecturearisingfrom
a large number of agents, and also the non-stationarity issue in a decentralized
architecturestemmingfromtheinter-dependencyamongagents. Toaddressboth
challenges,weproposeanovelworldmodelforMARLthatlearnsdecentralized
localdynamicsforscalability,combinedwithacentralizedrepresentationaggrega-
tionfromallagents. Wecastthedynamicslearningasanauto-regressivesequence
modelingproblemoverdiscretetokensbyleveragingtheexpressiveTransformer
architecture,inordertomodelcomplexlocaldynamicsacrossdifferentagentsand
provideaccurateandconsistentlong-termimaginations. Asthefirstpioneering
Transformer-basedworldmodelformulti-agentsystems,weintroduceaPerceiver
Transformer as an effective solution to enable centralized representation aggre-
gationwithinthiscontext. ResultsonStarcraftMulti-AgentChallenge(SMAC)
showthatitoutperformsstrongmodel-freeapproachesandexistingmodel-based
methodsinbothsampleefficiencyandoverallperformance.
1 Introduction
Multi-AgentReinforcementLearning(MARL)hasmaderemarkableprogress,whichwasdriven
largelybymodel-freealgorithms[31]. However,duetothecomplexityofmulti-agentsystemsarising
fromlargestate-actionspaceandpartialobservability,suchalgorithmsusuallydemandextensive
interactionstolearncoordinativebehaviors[16]. Apromisingsolutionisbuildingaworldmodel
thatapproximatestheenvironment,whichhasexhibiteditssuperiorsampleefficiencycomparedto
model-freeapproachesinsingle-agentRL[9,52,11,12,13,14]. However,extendingthedesignof
worldmodelinsingle-agentdomaintothemulti-agentcontextencounterssignificantchallengesdue
totheuniquebiasesandcharacteristicsinherenttomulti-agentenvironments.
Thechallengesprimarilystemfromtwodifferentmeansformulti-agentdynamicslearning: central-
izedanddecentralized.Learningaworldmodeltoapproximatethecentralizeddynamicsencapsulates
theinter-dependencybetweenagentsbutstrugglestobescalabletoanincreasingnumberofagents,
whichleadstotheexponentialsurgeinspatialcomplexity[16,31]. Conversely,applyingadecentral-
izedworldmodeltoapproximatingthelocaldynamicsofeachagentmitigatesthescalabilityissue
âˆ—WorkdoneduringaninternshipatShanghaiAILab.
â€ Correspondenceto:ChenjiaBai<baichenjia@pjlab.org.cn>
Preprint.Underreview.
4202
nuJ
22
]GL.sc[
1v63851.6042:viXrayetincursnon-stationarity,asunexpectedinterventionsfromotheragentsmayoccurineachagentâ€™s
individualenvironment[32]. Furthermore,beyondtheseuniquechallengesinherenttomulti-agent
scenarios, existing model-based MARL approaches [47, 5, 48] excessively neglect the fact that
the policy learned in imaginations of the world model heavily relies on the quality of imagined
trajectories[30]. Ittherebynecessitatesaccuratelong-termprediction,especiallywithrespecttothe
non-stationarylocaldynamics. InspiredbythecapabilityofTransformer[45]inmodelingcomplex
sequencesandlong-termdependency[2,4,30],weseektoconstructaTransformer-basedworld
model within the multi-agent context for decentralized local dynamics together with centralized
featureaggregation,combiningthebenefitsoftwodistinctivedesigns.
Inthispaper,weintroduceMARIE(Multi-Agentauto-RegressiveImaginationforEfficientlearning),
thefirstTransformer-basedmulti-agentworldmodelforsample-efficientpolicylearning. Specifically,
thehighlightsofthispaperare:
1. Totackletheinherentchallengeswithinthemulti-agentcontext,webuildaneffectiveworldmodel
viascalabledecentralizeddynamicsmodelingandessentialcentralizedrepresentationaggregating,
whichmirrorstheprincipleofCentralizedTrainingandDecentralizedExecution.
2. Toenableaccurateandconsistentlong-termimaginationsfromthenon-stationarylocaldynamics,
wecastthedecentralizeddynamicslearningassequencemodelingoverdiscretetokensbylever-
aginghighlyexpressiveTransformerarchitectureasthebackbone. Inparticular,wesuccessfully
presentthefirstTransformer-basedworldmodelformulti-agentsystems.
3. WhileitremainsopenforhowtoeffectivelyenablecentralizedrepresentationwiththeTransformer
as the backbone, we achieve it by innovatively introducing a Perceiver Transformer [19] for
efficientglobalinformationaggregationacrossallagents.
4. ExperimentsontheStarcraftMulti-AgentChallenge(SMAC)[38]benchmarkinlowdataregime
showMARIEoutperformsbothmodel-freeandexistingmodel-basedMARLmethodsw.r.t. both
sampleefficiencyandoverallperformanceanddemonstratetheeffectivenessofMARIE.
2 RelatedWorksandPreliminaries
Multi-AgentReinforcementLearning. Inamodel-freesetting,atypicalapproachforcooperative
MARLiscentralizedtraininganddecentralizedexecution(CTDE),whichtacklesthescalability
and non-stationarity issues in MARL. During the training phase, it leverages global information
tofacilitateagentsâ€™policylearning;whileduringtheexecutionphase,itblindsitselfandhasonly
accesstothepartialobservationaroundeachagentformulti-agentdecision-making. Model-free
MARLmethodswiththisparadigmcanbedividedinto2categories: value-based[42,35,41,46]
andpolicy-based[28,7,18,37,27,25,33,49,51,50]. Incontrasttomodel-freeapproaches,model-
basedMARLalgorithmsremainfairlyunderstudied. MAMBPO[47]incorporatesMBPO-style[20]
techniquesintomulti-agentpolicylearningundertheCTDEframework. Tesseract[29]introduces
thetensorisedBellmanequationandevaluatestheQ-valuefunctionusingDynamicProgramming
(DP)togetherwithanestimatedenvironmentmodel. Similartooursettingwhereagentslearninside
ofanapproximateworldmodel,MAMBA[5]integratesthebackboneproposedinDreamerV2[11]
with an attention mechanism across agents to sustain an effective world model in environments
withanarbitrarynumberofagents,whichleadstonotablysuperiorsampleefficiencytoexisting
model-freeapproaches. Intermsofmodel-basedalgorithmcoupledwithplanning,MAZero[26]
expands the MCTS planning-based Muzero [39] framework to the model-based MARL settings.
However,learning-basedorplanning-basedpoliciesinthesetwoapproachesarebothoverlycoupled
withtheirworldmodels,downgradingtheirinferenceefficiencyandfurtherlimitingexpansionin
combinationswithotherpopularmodel-freeapproaches.
Learningbehaviorswithintheimaginationofworldmodels. TheDynaarchitecture[43]first
emphasizestheutilityofanestimateddynamicsmodelinfacilitatingthetrainingofthevaluefunction
andpolicy. Inspiredbythecognitivesystemofhumanbeings,theconceptofworldmodel[8]is
initiallyintroducedbycomposingavariationalAuto-Encoder(VAE)[24]andarecurrentnetworkto
mimicthecompleteenvironmentaldynamics,thenanartificialagentistrainedentirelyinsidethe
hallucinatedimaginationgeneratedbytheworldmodel. SimPLe[52]showsthataPPOpolicy[40]
learnedinapredictivemodeldelivererasuper-humanperformanceinAtaridomains. Dreamer[9]
builtstheworldmodeluponaRecurrentStateSpaceModel(RSSM)[10]thatcombinesthedetermin-
isticlatentstatewiththestochasticlatentstatetoallowthemodeltonotonlycapturemultiplefutures
butalsorememberinformationovermulti-steps. DreamerV2[11]furtherdemonstratestheadvantage
2Joint obs-action sequence
Agents
ğ‘›ğ‘› ğ¾ğ¾+1 Ã—ğ·ğ·
â€¦ â€¦
1:ğ‘›ğ‘› ğ’“ğ’“ğ‘¡ğ‘¡ğ‘–ğ‘– ğœ¸ğœ¸ğ‘¡ğ‘¡ğ‘–ğ‘– ğ’™ğ’™1ğ‘¡ğ‘¡,1 ğ’™ğ’™1 ğ‘¡ğ‘¡,ğ¾ğ¾ ğ’‚ğ’‚1ğ‘¡ğ‘¡ Lateğ’™ğ’™ğ‘¡ğ‘¡ğ‘›ğ‘› n,1
t array
ğ’™ğ’™ğ‘¡ğ‘¡ğ‘›ğ‘› ,ğ¾ğ¾ ğ’‚ğ’‚ğ‘¡ğ‘¡ğ‘›ğ‘›
MLğ‘œğ‘œPğ‘–ğ‘–âˆ’ E1 nğ‘œğ‘œcğ‘–ğ‘– oğ‘œğ‘œdğ‘–ğ‘– e+ r1 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,2 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,3 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,ğ¾ğ¾ ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– +1 ,1 K V (ğ‘›ğ‘›Ã—â€¦
ğ·ğ·)
Cross Q
Codebook ğ¸ğ¸ Shared Transformer Attention
Quantize Agents
ğ’µğ’µ 1:ğ‘›ğ‘› â€¦
(â‹…)
ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™2ğ‘–ğ‘– ğ’™ğ’™3ğ‘–ğ‘– ğ’™ğ’™ğ¾ğ¾ğ‘–ğ‘–
ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,1 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,2 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,3 ğ’™ğ’™ğ‘¡ğ‘¡ğ‘–ğ‘– ,ğ¾ğ¾ ğ’‚ğ’‚ğ‘¡ğ‘¡ğ‘–ğ‘– ğ’†ğ’†ğ‘¡ğ‘¡ğ‘–ğ‘– Perceiver Transformer
MLP Decoder Agent-wise
Aggregation
ğ·ğ· ğ’†ğ’†1ğ‘¡ğ‘¡ ğ’†ğ’†ğ‘¡ğ‘¡2 ğ’†ğ’†ğ‘¡ğ‘¡3 ğ’†ğ’†ğ‘¡ğ‘¡ğ‘›ğ‘›
Observation tokens Aggregated features
Legend:
Action tokens Invalid outputs
â€¦ â€¦
ğ‘–ğ‘–âˆ’1 ğ‘–ğ‘– ğ‘–ğ‘–+1
Figure 1:ğ‘œğ‘œï¿½Ovğ‘œğ‘œï¿½erğ‘œğ‘œï¿½view of the proposed world model architecture in MARIE. VQ-VAE (left) maps
local observations oi of each agent i into discrete latent codes (xi,...,xi ), where (E,D,Z) is
1 K
sharedacrossallagents. Togetherwithdiscreteactions,thisprocessformslocaldiscretesequences
(...,xi ,...,xi ,ai,...)ofeachagent. ThenthePerceiver(right)performsaggregationofjointdis-
t,1 t,K t
cretesequencesofallagents(x1 ,...,x1 ,a1,...,xn ,...,xn ,an)independentlyateachtimestep
t,1 t,K t t,1 t,K t
t, and inserts the aggregated global representations (e1,e2,...,en) into original local discrete se-
t t t
quencesrespectively. Theresultingsequences(...,xi ,...,xi ,ai,ei...)containrichinformation
t,1 t,K t t
betweentransitionsinlocaldynamicsandarefedintothesharedTransformer(middle),whichlearns
observationtokenpredictionsinanautoregressivemanner. Predictionsofindividualrewardriand
t
discountÎ³iattimesteptarecomputedbasedonallhistoricalsequence(xi ,...,xi ,ai ,ei ).
t â‰¤t,1 â‰¤t,K â‰¤t â‰¤t
ofdiscretelatentstatesoverGaussianstates. ForMARL,MAMBA[5]extendsDreamerV2tomulti-
agentcontextsbyusingRSSM,underscoringthepotentialofmulti-agentlearningintheimagination
ofworldmodels. Recently, motivatedbythesuccessoftheTransformer[45], TransDreamer[3]
andTWM[36]exploredvariantsofDreamerV2,whereinthebackbonesoftheworldmodelwere
substitutedwithTransformers. Insteadofincorporatingdeterministicandstochasticlatentstates,
IRIS[30]appliestheTransformertodirectlymodelingsequencesofobservationtokensandactions
ofsingle-agentRLandachievesimpressiveresultsonAtari-100k. Incontrast,theproposedMARIE
concentratesonestablishingeffectiveTransformer-basedworldmodelsinmulti-agentcontextswith
shareddynamicsandglobalrepresentations.
Preliminaries. Wefocusonfullycooperativemulti-agentsystemswhereallagentsshareateam
rewardsignal. WeformulatethesystemasadecentralizedpartiallyobservableMarkovdecision
process (Dec-POMDP) [32], which can be described by a tuple (N,S,A,P,R,â„¦,O,Î³). N =
{1,...,n}denotesasetofagents,S isthefiniteglobalstatespace,A=(cid:81)n Aiistheproductof
i=1
finiteactionsspacesofallagents,i.e.,thejointactionspace,P :SÃ—AÃ—S â†’[0,1]istheglobal
transitionprobabilityfunction,R:SÃ—Aâ†’Risthesharedrewardfunction,â„¦=(cid:81)n â„¦iisthe
i=1
productoffiniteobservationspacesofallagents,i.e.,thejointobservationspace,O ={Oi,iâˆˆN}
isthesetofobservingfunctionsofallagents. Oi :S â†’â„¦i mapsglobalstatestotheobservations
foragenti,andÎ³ isthediscountfactor. Givenaglobalstates attimestept,agentiisrestricted
t
to obtaining solely its local observation oi = Oi(s ), takes an action ai drawn from its policy
t t t
Ï€i(Â·|oi )basedonthehistoryofitslocalobservationsoi ,whichtogetherwithotheragentsâ€™actions
â‰¤t â‰¤t
gives a joint action a = (a1,...,an) âˆˆ A, equivalently drawn from a joint policy Ï€(Â·|o ) =
t t t â‰¤t
(cid:81)n Ï€i(Â·|oi ). Thentheagentsreceiveasharedrewardr =R(s ,a ),andtheenvironmentmoves
i=1 â‰¤t t t t
tonextstates withprobabilityP(s |s ,a ). TheaimofallagentsistolearnajointpolicyÏ€
t+1 t+1 t t
thatmaximizestheexpecteddiscountedreturnJ(Ï€)=E [(cid:80)âˆ Î³tR(s ,a )].
s0,a0,...âˆ¼Ï€ t=0 t t
3 Methodologies
Ourapproachcomprisesthreetypicalparts:(i)collectingexperiencebyexecutingthepolicy,(ii)learn-
ing the world model from the collected experience, and (iii) learning the policy via imagination
inside the world model. Throughout the process, the historical experiences stored in the replay
3
â€¦bufferareusedfortrainingtheworldmodelonly,whilepoliciesarelearnedfromunlimitedimagined
trajectoriesfromtheworldmodel. Inthefollowing,wefirstdescribethreecorecomponentsofour
worldmodelinÂ§3.1andÂ§3.2,andgiveanoverviewoftheproposedworldmodelinFig.1. Thenwe
describethepolicy-learningprocessinsidetheworldmodelinÂ§3.3. Thecomprehensivedetailsof
themodelarchitectureandhyperparameterareprovidedinÂ§A.
3.1 DiscretizingObservation
WeconsideratrajectoryÏ„iofagenticonsistsofT localobservationsandactions,as
Ï„i =(oi,ai,...,oi,ai,...,oi ,ai ).
1 1 t t T T
ToutilizetheexpressiveTransformerarchitecture,weneedtoexpressthetrajectoryintoadiscrete
tokensequenceformodeling. Accountingforcontinuousobservations,aprevalentbutnaiveapproach
fordiscretizationinvolvesdiscretizingthescalarintooneofmfixed-widthbinsineachdimension
independently[21].However,withahigherdimensionoftheobservation,theTransformerencounters
highercomputationalcomplexity,whichnecessitatesanapproachthatusesadiscretecodebookof
learnedrepresentations. Tothisend,weemploytheideafromneuraldiscreterepresentationlearning
[44],andlearnaVectorQuantised-VariationalAutoEncoder(VQ-VAE)toplayarolethatresembles
thetokenizerinNaturalLanguageProcessing[4,2]. TheVQ-VAEiscomposedofanencoderE,
adecoderD,andacodebookZ. WedefinethediscretecodebookZ ={z j}N
j=1
âŠ‚Rnz,whereN
isthesizeofthecodebookandn isthedimensionofcodes. TheencoderE takesanobservation
z
oi âˆˆRnobs asinputandoutputsaK n z-dimensionallatentszË†i âˆˆRKÃ—nz reshapedfromthedirect
outputsofencoder. Subsequently,thetokens{xi}K âˆˆ {0,1,...,N âˆ’1}K forrepresentingoi is
k k=1
obtainedbyanearestneighbourlook-upusingthecodebookZ wherexi = argmin âˆ¥zË†i âˆ’z âˆ¥.
k j k j
ThenthedecoderD : {0,1,...,N âˆ’1}K â†’ Rnobs convertsK tokensbackintoanreconstructed
observationoË†i. Bylearningthisdiscretecodebook,wecompresstheredundantinformationviaa
succinctsequenceoftokens,whichhelpsimprovesequencemodeling. SeeÂ§4.2foradiscussion.
3.2 ModelingLocalDynamicswithGlobalRepresentations
Here, we consider discrete actions like those in SMAC, and the continuous actions can also be
discretizedbysplittingthevalueineachdimensionintofixedbins[21,1]. Therefore,atrajectoryÏ„i
ofagenticanbetreatedasasequenceoftokens,
Ï„i =(...,oi,ai,...)=(...,xi ,xi ,...,xi ,ai,...) (1)
t t t,1 t,2 t,K t
wherexi isthej-thtokenoftheobservationofagentiattimestept. Givenarbitrarysequencesof
t,j
observationandactiontokensinEq.(1),wetrytolearnoverdiscretemultimodaltokens.
Theworldmodelconsistsofatokenizertodiscretethelocalobservation,aTransformertolearnthe
localdynamics,anagent-wiserepresentationaggregationmodule,andpredictorsfortherewardand
discount. TheTransformerÏ•predictsthefuturelocalobservation{xË†i }K ,thefutureindividual
t+1,j j=1
reward rË†i and discount Î³Ë†i, based on the agentâ€™s individual historical observation-action history
t t
(xi ,ai )andaggregatedglobalfeatureei oftheagent. ThemodulesareshowninEqs.(2)â€“(5).
â‰¤t,Â· â‰¤t t
Transition: xË†i âˆ¼p (xË†i |xi ,ai ,ei )with xË†i âˆ¼p (xË†i |xi ,ai ,ei ,xi )
t+1,Â· Ï• t+1,Â· â‰¤t,Â· â‰¤t â‰¤t t+1,k Ï• t+1,k â‰¤t,Â· â‰¤t â‰¤t t+1,<k
(2)
Reward: rË†i âˆ¼p (rË†i|xi ,ai ,ei ) (3)
t Ï• t â‰¤t,Â· â‰¤t â‰¤t
Discount: Î³Ë†i âˆ¼p (Î³Ë†i|xi ,ai ,ei ) (4)
t Ï• t â‰¤t,Â· â‰¤t â‰¤t
Aggregation: (e1,e2,...,en)=f (x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an) (5)
t t t Î¸ t,1 t,2 t,K t t,1 t,2 t,K t
TransitionPrediction.InthetransitionpredictioninEq.(2),thek-thobservationtokenisadditionally
conditioned on the tokens that were already predicted xi â‰œ (xi ,xi ,...,xi ),
t+1,<k t+1,1 t+1,2 t+1,kâˆ’1
ensuringtheautoregressivetokenpredictiontofacilitatemodelingoverthetrajectorysequence.
DiscountPrediction. ThediscountpredictoroutputsaBernoullilikelihoodandletsusestimatethe
probabilityofanindividualagentâ€™sepisodeendingwhenlearningbehaviorsfrommodelpredictions.
RewardPrediction.Sincetherewardsdistributioniswidespreadanddiverseinmulti-agentscenarios
[38], we discrete the reward and learn a reward predictor via discrete regression [6] instead of a
mean-squarederror. Specifically,therewardpredictoroutputsanestimatedcategoricaldistribution
4MLP Encoder
1:ğ‘›ğ‘›
ğ‘œğ‘œ1 Agenğ¸ğ¸t-wise Aggregation ğ‘Ÿğ‘Ÿ1Ì‚ğ‘–ğ‘– Agent-wise Aggregation ğ‘Ÿğ‘Ÿ2Ì‚ğ‘–ğ‘–
ğ‘–ğ‘– ğ‘–ğ‘–
ğ›¾ğ›¾ï¿½1 ğ›¾ğ›¾ï¿½2
â€¦
ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–1 1ğ‘–ğ‘–,1,1 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–2 1ğ‘–ğ‘–,2,2 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–3 1ğ‘–ğ‘–,3,3 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–ğ¾ğ¾ 1ğ‘–ğ‘–,ğ¾ğ¾,ğ¾ğ¾ ğ’‚ğ’‚ğ’‚ğ’‚1ğ‘–ğ‘– ğ’‚ğ’‚1ğ‘–ğ‘– 1ğ‘–ğ‘– ğ’†ğ’†ğ’†ğ’†1ğ‘–ğ‘– ğ’†ğ’†1ğ‘–ğ‘– 1ğ‘–ğ‘– ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–1 2ğ‘–ğ‘–,1,1 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–2 2ğ‘–ğ‘–,2,2 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–3 2ğ‘–ğ‘–,3,3 ğ’™ğ’™ğ’™ğ’™1ğ‘–ğ‘– ğ’™ğ’™1,ğ‘–ğ‘–ğ¾ğ¾ 2ğ‘–ğ‘–,ğ¾ğ¾,ğ¾ğ¾ ğ’‚ğ’‚ğ’‚ğ’‚1ğ‘–ğ‘– ğ’‚ğ’‚1ğ‘–ğ‘– 2ğ‘–ğ‘– ğ’†ğ’†ğ’†ğ’†1ğ‘–ğ‘– ğ’†ğ’†1ğ‘–ğ‘– 2ğ‘–ğ‘–
Agents
1:ğ‘›ğ‘› MLP Decoder MLP Decoder
ğ·ğ· ğ·ğ·
1:ğ‘›ğ‘› 1:ğ‘›ğ‘›
1:ğ‘›ğ‘› ğœ‹ğœ‹ğœ“ğœ“ 1:ğ‘›ğ‘› 1:ğ‘›ğ‘› ğœ‹ğœ‹ğœ“ğœ“ 1:ğ‘›ğ‘›
Figure2: Imaginğ‘œğ‘œï¿½1ationprocedurğ‘ğ‘e1inMARIE.Weunrolltheimğ‘œğ‘œï¿½a2ginationofallğ‘ğ‘a2gents{1,...,n}in
parallel. Initially,eachagentâ€™sobservationisderivedfromajointobservationsampledfromareplay
buffer. Apolicy,depictedinredarrows,generatesactionsbasedonreconstructedobservations. Then,
thePerceiverintegratesjointactionsandobservationsintoglobalrepresentationsfromeachagent,
appendingthemtoeachagentâ€™slocalsequence. TheTransformerthenpredictsindividualrewards
anddiscounts,depictedbygreenandpurplearrowsrespectively,whilegeneratingnextobservation
tokensforeachagentinanautoregressivemanner,shownbybluearrows. Thisparallelimagination
iteratesforH steps. ThepoliciesÏ€1:nareexclusivelytrainedusingimaginedtrajectories.
Ïˆ
R over M buckets centered at {b }M through the softmax function. The estimated reward is
m m=1
representedastheexpectedvalueofthisdistribution,as
rË†i =E(cid:2) R(xi ,ai ,ei )(cid:3) , R(xi ,ai ,ei )=(cid:88)M p (b |xi ,ai ,ei )Â·Î´ (6)
t â‰¤t,Â· â‰¤t â‰¤t â‰¤t,Â· â‰¤t â‰¤t
m=1
Ï• m â‰¤t,Â· â‰¤t â‰¤t bm
Thenwelearntherewardpredictorbyusingthecross-entropylossfunction. Thetargetdistribution
R
=(cid:80)M
p Â·Î´ canbecalculatedviaHL-Gauss[6],whichperformslabelsmoothingforthe
tar m=1 m bm
targetcategoricaldistributionusingaGaussiandistribution. ThedetailsaregiveninÂ§B.
Agent-wise Aggregation. Due to the partial environment, the non-stationarity issue stems from
thesophisticatedagent-wiseinter-dependencyonlocalobservationsgeneration. Toaddressit,we
introduceaPerceiver[19]toperformagent-wiserepresentationaggregationwhichplaysasimilarrole
tocommunication. Tosustainthedecentralizedmannerintransitionprediction,wehopeeveryagent
canpossessitsowninnerperceptionofthesituationsthatallagentsarein. Nonetheless,withdiscrete
representationforlocalobservation,theobservation-actionpairofagentiattimesteptisprojected
intoasequence(xi ,xi ,...,xi ,ai)oflengthK+1. Itleadstoasequenceoflengthn(K+1)
t,1 t,2 t,K t
thatlinearlyscaleswiththenumberofagents,whichrepresentsthejointobservation-actionpairof
allagents. Therefore,wechoosethePerceiverastheagent-wiserepresentationaggregationmodule,
whichexcelsatdealingwiththecasethatthesizeofinputsandoutputsscaleslinearly. Equipped
withaflexiblequeryingmechanismandself-attentionmechanism,thePerceiveraggregatesthejoint
representationsequence(x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an)oflengthn(K+1)intoa
t,1 t,2 t,K t t,1 t,2 t,K t
sequenceofnfeaturevectors(e1,e2,...,en),whereeachfeaturevectorservesasanintrinsicglobal
t t t
abstractionoftheenvironmentalcontextsperceivedfromeachagentâ€™sviewpoint.
The world model Ï• is trained with trajectory segments of a fixed horizon H sampled from the
replaybufferDinaself-supervisedmanner. Thetransitionpredictor,discountpredictor,andreward
predictorareoptimizedtomaximizethelog-likelihoodoftheircorrespondingtargets:
(cid:104)(cid:88)H
L (Ï•,Î¸)=E E âˆ’logp (ri|xi ,ai ,ei)âˆ’logp (Î³i|xi ,ai ,ei)
Dyn iâˆ¼N Ï„iâˆ¼D Ï• t â‰¤t,Â· â‰¤t t Ï• t â‰¤t,Â· â‰¤t t
t=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
rewardloss discountloss
(cid:16)(cid:88)K (cid:17)(cid:105)
âˆ’ logp (xi |xi ,ai ,ei,xi ) (7)
Ï• t+1,k â‰¤t,Â· â‰¤t t t+1,<k
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
transitionloss
where(e1,e2,...,en)=f (x1 ,x1 ,...,x1 ,a1,...,xn ,xn ,...,xn ,an),âˆ€t.
t t t Î¸ t,1 t,2 t,K t t,1 t,2 t,K t
We jointly minimize this loss function in Eq. (7) with respect to the model parameters of local
dynamics(i.e.,Ï•)andglobalrepresentation(i.e.,Î¸)usingtheAdamoptimizer[23].
3.3 LearningBehavioursinImagination
WeutilizetheActor-Criticframeworktolearnthebehaviorofeachagent,wheretheactorandcritic
areparameterizedbyÏˆ andÎ¾,respectively. Inthefollowing,wetakeagentiasanexemplarcase
5forclarityandomitthesuperscriptfordenotingtheindexoftheagenttoavoidpotentialconfusion.
Benefitedfromthesharedlocaldynamics,thelocaltrajectoriesofallagentsareimaginedinparallel,
asillustratedinFig.2. Attimestept,theactortakesareconstructedobservationoË† asinput,and
t
samplesanactiona âˆ¼Ï€ (a |oË†).TheworldmodelthenpredictstheindividualrewardrË†,individual
t Ïˆ t t t
discountÎ³Ë† andnextlocalobservationoË† . Startingfrominitialobservationssampledfromthe
t t+1
replaybuffer,thisimaginationprocedureisrolledoutforH steps.Tostimulatelong-horizonbehavior
learning, the critic accounts for rewards beyond the fixed imagination horizon and estimates the
individualexpectedreturnV (oË†)â‰ƒE [(cid:80) Î³lâˆ’trË†].
Î¾ t Ï€Ïˆ lâ‰¥t l
Inourapproach,wetraintheactorandcriticinaMAPPO-like[49]manner. UnlikeotherCTDE
model-freeapproachesthatrequireaglobaloraclestatefromtheenvironment,wecannotobtainthe
oraclestatefromtheworldmodel,andonlythepredictedobservationsofeachagentareavailable.
To approximate the oracle information in critic training, we enhance each agentâ€™s critic with the
capability to access the observations of other agents. Since the actor and critic only rely on the
reconstructedobservations,decouplingfromtheinnerhiddenstatesoftheTransformer-basedworld
model,weallowfastinferenceintheenvironmentwithouttheparticipationoftheworldmodel. It
is important for the deployment of policies learned with data-efficient imagination in real-world
applications. Î»-targetinDreamer[9]isusedtoupdatedthevaluefunction. Thedetailsofbehavior
learningobjectivesandalgorithmicdescriptionofMARIEarepresentedinÂ§CandÂ§G,respectively.
4 Experiments
Weconsiderthemostcommonbenchmarkâ€“StarCraftIIMulti-AgentChallenge(SMAC)[38]for
evaluatingourmethod. Tohighlightthesampleefficiencybroughtbymodel-basedimagination,we
adoptalowdataregimethatresemblesasimilarsettinginsingle-agentAtaridomain[52].
4.1 EvaluationsonSMAC
StarCraftIIMulti-AgentChallenge. SMAC[38],asuiteofcooperativemulti-agentenvironments
basedonStarCraftII,consistsofasetofStarCraftIIscenarios. Eachscenariodepictsaconfrontation
betweentwoarmiesofunits,oneofwhichiscontrolledbythebuilt-ingameAIandtheotherby
ouralgorithm. Theinitialposition,number,andtypeofunitsineacharmyvariesfromscenarioto
scenario,asdoesthepresenceorabsenceofelevatedorimpassableterrain. Andthegoalistowinthe
gamewithinthepre-specifiedtimelimit. SMACemphasizesmasteringmicromanagementtechniques
acrossmultipleagentstoachieveeffectivecoordinationandovercomeadversaries. Thisnecessitates
bothsufficientexplorationandappropriatecreditassignmentforeachagentâ€™saction. Anothernotable
propertyofSMACisthatnotallactionsareaccessibleduringdecision-makingofeachagent,which
requiresworldmodelstopossessanin-depthcomprehensionoftheunderlyinggamemechanicssoas
toconsistentlyprovidevalidavailableactionmaskestimationwithintheimaginationhorizon.
ExperimentalSetup. Wechoose13representativescenariosfromSMACthatincludesthreelevels
ofdifficultyâ€“Easy,Hard,andSuperHard. SpecificchosenscenarioscanbefoundinTable1. In
termsofdifferentlevelsofdifficulty,weadoptasimilarsettingakintothatin[5]andrestrictthe
numberofsamplesfromtherealenvironmentto100kforEasyscenarios,200kforHardscenarios
and400kforSuperHardscenarios,toestablishalowdataregimeinSMAC.WecompareMARIE
withthreestrongmodel-freebaselinesâ€“MAPPO[49],QMIX[35]andQPLEX[46],andtwostrong
model-basedbaselinesâ€“MBVD[48]andMAMBA[5]onSMACbenchmark.Specially,asarecently
proposedmulti-agentvariantofDreamerV2[11],MAMBAachievesstate-of-the-artsampleefficiency
invariousSMACscenariosvialearninginimagination. Foreachrandomseed,wecomputethewin
rateacross10evaluationgamesatfixedintervalsofenvironmentalsteps.
MainResults. Overall,wefindMARIEachievessignificantlybettersampleefficiencyandahigher
winratecomparedwithotherstrongbaselines. Wereportthemedianwinratesoverfourseedsin
Table 1 and provide additional learning curves of several chosen scenarios, shown as Fig. 3. As
presentedinTable1andFig.3,MARIEdemonstratessuperiorperformanceandsampleefficiency
across almost all scenarios. The improvements in sample efficiency and performance become
particularlypronouncedwithincreasingdifficultyofscenarios,especiallycomparedtoMAMBAthat
adoptsRSSMasthebackbonefortheworldmodel. Weattributesuchresultstothemodelcapability
oftheTransformerinlocaldynamicsmodelingandglobalfeatureaggregation. Benefitingfrommore
powerfulstrengthinmodelingsequences,theTransformer-basedworldmodelcangeneratemore
accurateandconsistentimaginationsthanthoserelyingontherecurrentbackbone,whichfacilitates
6MAPPO QMIX QPLEX MAMBA MBVD MARIE
2s3z 8m 1c3s5z 3s_vs_3z
1.0 1.0 0.8 1.0
0.8 0.8 0.8
0.6
0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
3s_vs_4z 3s_vs_5z 2c_vs_64zg corridor
0.6 1.0 0.30 0.6
0.5 0.8 0.25 0.5 0.4 0.6 0.20 0.4
0.3 0.15 0.3 0.2 0.4 0.10 0.2
0.1 0.2 0.05 0.1
0.0 0.0 0.00 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.8 1.6 2.4 3.2 4.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure3: Curvesofevaluationwinrateformethodsin8chosenSMACmaps. SeeTable1forwin
rates. Yaxis: winrate;Xaxis: numberofstepstakenintherealenvironment.
Table1: Medianevaluationwinrateandstandarddeviationon13SMACmapsfordifferentmethods
over4randomseeds. Weboldthevaluesofthemaximum.
Methods
Maps Difficulty Steps
MARIE(Ours) MAMBA[5] MAPPO[49] QMIX[35] QPLEX[46] MBVD[48]
1c3s5z 84.1(9.4) 76.8(15.3) 22.7(11.0) 44.1(29.2) 67.2(7.4) 62.7(11.4)
2m_vs_1z 100.0(7.9) 95.0(2.3) 86.7(3.2) 74.1(14.8) 88.4(10.8) 33.9(24.5)
2s_vs_1sc 100.0(7.1) 99.1(7.1) 100.0(0.0) 0.0(0.0) 7.1(19.5) 0.3(14.8)
2s3z 81.8(9.3) 69.1(12.7) 31.3(12.9) 37.7(15.5) 51.1(8.4) 52.3(4.1)
3m 99.5(0.4) 86.4(7.1) 84.4(12.8) 53.7(22.7) 89.0(6.9) 72.4(6.9)
Easy 100K
3s_vs_3z 99.5(1.5) 92.3(10.1) 0.8(1.3) 0.0(0.0) 0.0(0.0) 0.0(0.0)
3s_vs_4z 63.6(24.9) 27.7(12.3) 0.0(0.0) 0.0(0.0) 0.0(0.0) 0.0(0.0)
8m 89.1(3.9) 65.0(7.7) 77.3(19.5) 74.5(12.8) 83.0(6.4) 74.7(9.7)
MMM 25.0(3.4) 45.0(27.6) 4.7(4.5) 25.0(17.3) 88.4(35.1) 20.4(2.1)
so_many_baneling 97.7(5.9) 93.6(4.1) 43.8(15.0) 22.7(8.9) 31.2(6.1) 12.6(10.4)
3s_vs_5z 88.6(38.8) 10.5(14.0) 0.0(0.0) 0.0(0.0) 0.0(0.0) 0.0(0.0)
Hard 200K
2c_vs_64zg 23.6(14.3) 7.7(8.7) 3.1(10.2) 0.5(0.5) 0.0(0.1) 0.0(0.4)
corridor SuperHard 400K 47.1(32.2) 21.1(15.2) 0.0(0.7) 0.0(0.0) 0.0(0.0) 0.0(0.0)
betterpolicylearningwithintheimaginationoftheworldmodel. Whilethescenariosbecomeharder,
e.g. 3s_vs_5z, our world model can address the challenge of learning more intricate underlying
dynamicsandfurtherlargequantitiesofaccurateimaginations,therebysignificantlyoutperforming
otherbaselinesonthesescenarios. ForMMM wherethemodel-freebaselineperformsbetter,we
hypothesizethattheagentmainlyreliesonshort-termbehaviorsforcooperation,withoutrequiring
long-termpredictionsforbetterperformance. Moreover, aspecialscenario2c_vs_64zgdeserves
attention,whichfeaturesonly2agentsbutwithaconsiderablylargeactionspaceofupto70discrete
actions for each agent. It is easy for the world model to generate ridiculous estimated available
actionmaskswithoutunderstandingthemechanicsbehindthisscenario,furtherleadingtoinvalid
orevenerroneouspolicylearningintheimaginationsoftheworldmodel. Theperformancegapon
2c_vs_64zgprovesthatourTransformer-basedworldmodelhashigherpredictionaccuracyanda
deeperunderstandingoftheunderlyingmechanics.
4.2 AblationStudies
Incorporating CTDE principle with the design of the world model makes MARIE scalable
and robust to different number of agents. We compare our method with a centralized variant
ofourmethod,whereintheworldmodellearnsthejointdynamicsofallagentstogetheroverthe
jointtrajectoryÏ„ =(...,o1,o2,...,on,a1,a2,...,an,...). GiventhatÏ„ alreadycontainsthejoint
t t t t t t
observationsandactions,wedisabletheaggregationmoduleinthiscentralizedvariant. Asillustrated
inFigure4,ourcomparisonsspanscenariosinvolving2to7agents. Whenthenumberofagentsis
smallenough,reducingthemulti-agentsystemtoasingle-agentoneoverthejointobservationand
actionspacewouldnotcauseaprominentscalabilityissue,asindicatedbytheresultin2s_vs_1sc.
However,thescalabilityissueisexacerbatedbyagrowingnumberofagents. Inscenariosfeaturing
morethan3agents,thesampleefficiencyofthecentralizedvariantencountersasignificantdrop,
suffering from the exponential surge in spatial complexity of the joint observation-action space.
7
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niWDecentralized Manner Centralized Manner
2s_vs_1sc (2 agents) 3s_vs_3z (3 agents) 2s3z (5 agents) so_many_baneling (7 agents)
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure4: Ablationonwhatmannertointegrateintothedesignoftheworldmodel. Decentralized
MannerdenotesthestandardimplementationofMARIE,whileCentralizedMannerdenotesthatthe
worldmodelisdesignedforlearningthejointdynamicsofallagentsoverthejointtrajectory.
MARIE MARIE w/o aggregation
3s_vs_3z (3 agents) so_many_baneling (7 agents) 1c3s5z (9 agents)
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e5 Environment Steps 1e5 Environment Steps 1e5
Figure5: ComparisonsbetweenMARIEwithandwithouttheusageoftheaggregationmodule.
Furthermore, withequalpredictionhorizons, theparameteramountsinthecentralized variantis
increased by a factor of 4 or larger. And to achieve the same number of environment steps, the
centralizedvariantdemandsovertwicetheoriginalcomputationaltime. Instead,withdecentralized
localdynamicsandaggregatedglobalfeatures,MARIEdeliversstableandsuperiorsampleefficiency.
Agent-wiseaggregationhelpsMARIEcapturethesophisticatedinter-dependencyonthegenera-
tionofeachagentâ€™slocalobservation. Tostudytheinfluenceofagent-wiseaggregation,weconduct
ablationexperimentsontheaggregationmoduleoverscenarioswherethenumberofagentsgradually
increases. AsshowninFig.5,inthe3-agentsscenario(e.g.,3s_vs_3z),thecorrelationamongeach
agentâ€™slocalobservationtendstobenegligible. Therefore,thenearlyindependentgenerationofeach
agentâ€™slocalobservationwithoutanyaggregatedglobalfeaturestillleadstoperformancecomparable
tothatofstandardimplementation. Butasmoreagentsgetinvolved,theinter-dependencybecomes
dominant. Lackingtheglobalfeaturesderivedfromagent-wiseaggregation,thesharedTransformer
strugglestoinferaccuratefuturelocalobservations,thushinderingpolicylearningintheimaginations
oftheworldmodelandresultinginnotabledegradationinthewinrateevaluation.
VQ-VAEencapsulateslocalobservationswithinasuccinctsequenceoftokens,promotingthe
learningoftheTransformer-basedworldmodelandeffectivelyimprovingalgorithmperfor-
mance. ComparedtoVQ-VAEthatdiscretizeseachobservationtoK tokensfromZ, perhapsa
morenaivetokenizerisprojectingthevalueineachdimensionintooneofmfixed-widthbins[21],
resultinginan -longtokensequenceforeachobservation,whichwetermBinsDiscretization.
obs
We set the number of bins m equal to the size of codebook |Z| and compare these two types of
tokenizers in different environments with various n . As shown in Fig. 6, the performance of
obs
thetwotokenizersarecomparableonlyin2s_vs_1scwheren iscloseto16. Evenworse,Bins
obs
Discretizationexperiencesapronounceddeclineasn increasesinmorecomplexenvironments
obs
(e.g.,3s_vs_4z)underidenticaltrainingdurations. Wehypothesizethatforasinglelocalobservation,
an -token-longverbosesequenceyieldedbyBinsDiscretizationcontainsmoreredundantinfor-
obs
mationcomparedtoVQ-VAEthatlearnsamorecompacttokenizerthroughreconstructionThisnot
onlyrendersthetokensequencesofBinsDiscretizationobscureandchallengingtocomprehend,but
alsoresultsinanincreaseinmodelparameteramounts,beingmorecomputationallycostly. Dueto
thesetwofactors,BinsDiscretizationexhibitsanotablyslowconvergence. Meanwhile,theresultin
2m_vs_1zindicatesBinsDiscretizationmayignorethecorrelationofdifferentdimensions,which
wouldbehelpfulinsequencemodeling.
4.3 ModelAnalysis
ErrorAccumulation. Aquantitativeevaluationofthemodelâ€™saccumulatederrorversusprediction
horizonisprovidedinFig.7. Sincelearningtheworldmodelistiedtoaprogressivelyimproving
8
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niW
etaR
niWTokenizer (16 tokens) Bins Discretization (nobs tokens)
2m_vs_1z (nobs=16) 2s_vs_1sc (nobs=17) 3s_vs_4z (nobs=42) 2s3z (nobs=80)
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10 0 4 8 12 16 20
Timecost(hours) Timecost(hours) Timecost(hours) Timecost(hours)
Figure 6: Ablation on the type of discretization for local observations. Tokenizer denotes the
standardimplementationofMARIE;BinsDiscretizationdenotesthevariantofMARIEwherethe
n -dimensional observation discretization is performed by projecting the value into one of m
obs
fixed-widthbinsineachdimensionindependently. X-axis: cumulativeruntimeofalgorithmsinthe
sameplatform.
Agent 1 local imagination Agent 2 local imagination Agent 3 local imagination
2.00 2.00 2.00
MARIE MARIE MARIE
1.75 MAMBA 1.75 MAMBA 1.75 MAMBA
1.50 1.50 1.50
1.25 1.25 1.25
1.00 1.00 1.00
0.75 0.75 0.75
0.50 0.50 0.50
0.25 0.25 0.25
0.00 0.00 0.00
5 10 15 20 25 5 10 15 20 25 5 10 15 20 25
Horizons Horizons Horizons
Figure7: Compoundingmodelerrors. WecomparetheimaginationaccuracyofMARIEtothatof
MAMBAoverthecourseofaplanninghorizonin3s_vs_5zscenario. MARIEhasremarkablybetter
errorcompoundingwithrespecttopredictionhorizonthanMAMBA.
policybothinMARIEandMAMBA,weseparatelyusetheirfinalpoliciestosample10episodesfor
fairness. WethencomputeL errorsperobservationdimensionbetween1000trajectorysegments
1
randomly sampled from these 20 episodes and their imagined counterpart. The result in Fig. 7
suggests architecture differences play a large role in the world modelâ€™s long-horizon accuracy.
This also provides additional evidence that policy learning can benefit from accurate long-term
imaginations,explainingMARIEâ€™snotableperformanceinthe3s_vs_5zscenario. Moreprecisely,
lowergeneralizationerrorbetweentheestimateddynamicsandtruedynamicsbringsatighterbound
betweenoptimalpoliciesderivedfromthesetwodynamicsaccordingtotheoreticalresults[20].
AttentionPatterns. Duringmodelprediction,wedelveintotheattentionmapsinsidetheshared
TransformerandthecrossattentionmapsinthePerceiver. Interestingly,weobservetwodistinct
attention patterns involved in the local dynamics prediction. One exhibits a Markovian pattern
whereintheobservationpredictionlaysitsfocusmostlyontheprevioustransition,whiletheotheris
regularlystriatedwhereinthemodelattendstospecifictokensinmultiplepriortransitions. During
theagent-wiseaggregation,wealsoidentifytwodistinctpatternsâ€“individualityandcommonality
amongagents. SuchdiversepatternsintheTransformerandPerceivermaybepivotalforachieving
accurateandconsistentimaginationsofthesophisticatedlocaldynamics. WerefertoÂ§Dforfurther
detailsandvisualizationresults.
5 Conclusion
Wehaveintroducedamodel-basedmulti-agentalgorithmâ€“MARIE,whichutilizesasharedTrans-
formeraslocaldynamicmodelandaPerceiverasaglobalagent-wiseaggregationmoduletoconstruct
a world model within the multi-agent context. By providing long-term imaginations with policy
learning, it significantly boosts the sample efficiency and improves final performance compared
state-of-the-artmodel-freemethodsandexistingmodel-basedmethodswithsamelearningparadigm,
inthelowdataregime. AsthefirstTransformer-basedmulti-agentworldmodelforsample-efficient
policylearning,weopenanewavenueforcombiningthepowerfulstrengthoftheTransformerwith
sample-efficientMARL.Consideringthenotorioussampleinefficiencyinmulti-agentscenarios,it
holdsimportantpromiseforapplicationinmanyrealisticmulti-robotsystems,whereincollecting
tremendoussamplesforoptimalpolicylearningiscostlyandimpracticalduetothesafety. Whileit
hasthegreatpotentialtobrightthefuturetowardsachievingsmartermulti-agentsystems,therestill
9
etaR
niW
srorrE
detalumuccA
etaR
niW
srorrE
detalumuccA
etaR
niW
srorrE
detalumuccA
etaR
niWexistlimitationsinMARIE.Forinstance,itwouldsufferfrommuchslowerinferencespeedwhen
usedwithaverylongpredictionhorizons,duetotheauto-regressiveproperty.
References
[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, AlexHerzog, JasmineHsu, JulianIbarz,
BrianIchter,AlexIrpan,TomasJackson,SallyJesmonth,NikhilJoshi,RyanJulian,Dmitry
Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav
Malla, DeekshaManjunath, IgorMordatch, OfirNachum, CarolinaParada, JodilynPeralta,
EmilyPerez,KarlPertsch,JornellQuiambao,KanishkaRao,MichaelRyoo,GreciaSalazar,
PannagSanketi,KevinSayed,JaspiarSingh,SumedhSontakke,AustinStone,ClaytonTan,
HuongTran,VincentVanhoucke,SteveVega,QuanVuong,FeiXia,TedXiao,PengXu,Sichun
Xu,TianheYu,andBriannaZitkovich. Rt-1: Roboticstransformerforreal-worldcontrolat
scale. InRobotics: ScienceandSystems(RSS),2023.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya
Sutskever,andDarioAmodei. Languagemodelsarefew-shotlearners. InAdvancesinNeural
InformationProcessingSystems,2020.
[3] ChangChen,Yi-FuWu,JaesikYoon,andSungjinAhn. Transdreamer: Reinforcementlearning
withtransformerworldmodels. arXivpreprintarXiv:2202.09481,2022.
[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training
of deep bidirectional transformers for language understanding. In Proceedings of the 2019
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,NAACL-HLT,2019.
[5] Vladimir Egorov and Alexei Shpilman. Scalable multi-agent model-based reinforcement
learning. In Proceedings of the 21st International Conference on Autonomous Agents and
MultiagentSystems,2022.
[6] JesseFarebrother,JordiOrbay,QuanVuong,AdrienAliTaÃ¯ga,YevgenChebotar,TedXiao,
Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al. Stop regressing:
Trainingvaluefunctionsviaclassificationforscalabledeeprl. arXivpreprintarXiv:2403.03950,
2024.
[7] Jakob N. Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon
Whiteson. Counterfactualmulti-agentpolicygradients. InProceedingsoftheThirty-Second
AAAIConferenceonArtificialIntelligence,2018.
[8] David Haand JÃ¼rgen Schmidhuber. Recurrentworld modelsfacilitate policy evolution. In
AdvancesinNeuralInformationProcessingSystems,2018.
[9] DanijarHafner, TimothyLillicrap, JimmyBa, andMohammadNorouzi. Dreamtocontrol:
Learningbehaviorsbylatentimagination. InInternationalConferenceonLearningRepresenta-
tions,2020.
[10] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,and
JamesDavidson. Learninglatentdynamicsforplanningfrompixels. InProceedingsofthe36th
InternationalConferenceonMachineLearning,ProceedingsofMachineLearningResearch.
PMLR,2019.
[11] DanijarHafner,TimothyPLillicrap,MohammadNorouzi,andJimmyBa. Masteringatariwith
discreteworldmodels. InInternationalConferenceonLearningRepresentations,2021.
[12] DanijarHafner,JurgisPasukonis,JimmyBa,andTimothyLillicrap. Masteringdiversedomains
throughworldmodels. arXivpreprintarXiv:2301.04104,2023.
[13] NicklasHansen,HaoSu,andXiaolongWang.Temporaldifferencelearningformodelpredictive
control.InProceedingsofthe39thInternationalConferenceonMachineLearning,Proceedings
ofMachineLearningResearch.PMLR,2022.
10[14] NicklasHansen,HaoSu,andXiaolongWang. TD-MPC2: Scalable,robustworldmodelsfor
continuous control. In The Twelfth International Conference on Learning Representations,
2024.
[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415,2016.
[16] Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A very condensedsurvey and
critiqueofmultiagentdeepreinforcementlearning. InProceedingsofthe19thInternational
ConferenceonAutonomousAgentsandMultiAgentSystems,2020.
[17] EhsanImaniandMarthaWhite. Improvingregressionperformancewithdistributionallosses.
In Proceedings of the 35th International Conference on Machine Learning, Proceedings of
MachineLearningResearch.PMLR,2018.
[18] Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In
Proceedingsofthe36thInternationalConferenceonMachineLearning,ProceedingsofMachine
LearningResearch.PMLR,2019.
[19] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao
Carreira. Perceiver: Generalperceptionwithiterativeattention. InInternationalconferenceon
machinelearning,2021.
[20] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. In Advances in Neural Information Processing Systems,
2019.
[21] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big
sequencemodelingproblem. InAdvancesinNeuralInformationProcessingSystems,2021.
[22] AndrejKarpathy. mingpt: Aminimalpytorchre-implementationoftheopenaigpt(generative
pretrainedtransformer)training. https://github.com/karpathy/minGPT,2020.
[23] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua
BengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,
2015.
[24] DiederikP.KingmaandMaxWelling. Auto-EncodingVariationalBayes. In2ndInternational
ConferenceonLearningRepresentations,2014.
[25] JakubGrudzienKuba,RuiqingChen,MuningWen,YingWen,FangleiSun,JunWang,and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. In
InternationalConferenceonLearningRepresentations,2021.
[26] Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, and Chongjie Zhang. Efficient
multi-agentreinforcementlearningbyplanning. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024.
[27] YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,andYangGao. Multi-agent
gameabstractionviagraphattentionneuralnetwork. InTheThirty-FourthAAAIConferenceon
ArtificialIntelligence,2020.
[28] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.Multi-agentactor-
criticformixedcooperative-competitiveenvironments. InProceedingsofthe31stInternational
ConferenceonNeuralInformationProcessingSystems,2017.
[29] AnujMahajan,MikayelSamvelyan,LeiMao,ViktorMakoviychuk,AnimeshGarg,JeanKos-
saifi,ShimonWhiteson,YukeZhu,andAnimashreeAnandkumar. Tesseract: Tensorisedactors
formulti-agentreinforcementlearning. InProceedingsofthe38thInternationalConferenceon
MachineLearning,ProceedingsofMachineLearningResearch.PMLR,2021.
[30] VincentMicheli,EloiAlonso,andFranÃ§oisFleuret. Transformersaresample-efficientworld
models. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
[31] ThanhThiNguyen,NgocDuyNguyen,andSaeidNahavandi. Deepreinforcementlearningfor
multiagentsystems: Areviewofchallenges,solutions,andapplications. IEEETransactionson
Cybernetics,50:3826â€“3839,2020.
[32] FransAOliehoek,ChristopherAmato,etal. AconciseintroductiontodecentralizedPOMDPs,
volume1. Springer,2016.
11[33] BeiPeng,TabishRashid,ChristianSchroederdeWitt,Pierre-AlexandreKamienny,PhilipTorr,
WendelinBoehmer,andShimonWhiteson. FACMAC:Factoredmulti-agentcentralisedpolicy
gradients. InAdvancesinNeuralInformationProcessingSystems,2021.
[34] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,2019.
[35] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,
andShimonWhiteson. QMIX:Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcement learning. In Proceedings of the 35th International Conference on Machine
Learning,ProceedingsofMachineLearningResearch.PMLR,2018.
[36] JanRobine,MarcHÃ¶ftmann,TobiasUelwer,andStefanHarmeling. Transformer-basedworld
modelsarehappywith100kinteractions.InTheEleventhInternationalConferenceonLearning
Representations,2023.
[37] HeechangRyu,HayongShin,andJinkyooPark. Multi-agentactor-criticwithhierarchicalgraph
attentionnetwork. InProceedingsoftheAAAIConferenceonArtificialIntelligence,2020.
[38] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,Nantas
Nardelli,TimG.J.Rudner,Chia-ManHung,PhilipH.S.Torr,JakobFoerster,andShimon
Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International
ConferenceonAutonomousAgentsandMultiAgentSystems,2019.
[39] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,L.Sifre,Simon
Schmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,TimothyP.Lillicrap,
andDavidSilver.Masteringatari,go,chessandshogibyplanningwithalearnedmodel.Nature,
588:604â€“609,2020.
[40] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximal
policyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[41] KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi. QTRAN:
Learningtofactorizewithtransformationforcooperativemulti-agentreinforcementlearning.
In Proceedings of the 36th International Conference on Machine Learning, Proceedings of
MachineLearningResearch.PMLR,2019.
[42] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,
MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
InProceedingsofthe17thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,2018.
[43] RichardSSutton. Dyna,anintegratedarchitectureforlearning,planning,andreacting. ACM
SigartBulletin,2(4):160â€“163,1991.
[44] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentation
learning. InAdvancesinNeuralInformationProcessingSystems,2017.
[45] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ÅukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInforma-
tionProcessingSystems,2017.
[46] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.QPLEX:Duplexdueling
multi-agentq-learning. InInternationalConferenceonLearningRepresentations,2021.
[47] DaniÃ«l Willemsen, Mario Coppola, and Guido CHE de Croon. Mambpo: Sample-efficient
multi-robotreinforcementlearningusinglearnedworldmodels.In2021IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS).IEEE,2021.
[48] ZhiweiXu,DapengLi,BinZhang,YuanZhan,YunpengBaiia,andGuoliangFan. Mingling
foresightwithimagination: Model-basedcooperativemulti-agentreinforcementlearning. In
AdvancesinNeuralInformationProcessingSystems,2022.
[49] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu.
ThesurprisingeffectivenessofPPOincooperativemulti-agentgames. InThirty-sixthConfer-
enceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022.
[50] QiaoshengZhang,ChenjiaBai,ShuyueHu,ZhenWang,andXuelongLi. Provablyefficient
information-directedsamplingalgorithmsformulti-agentreinforcementlearning.arXivpreprint
arXiv:2404.19292,2024.
12[51] YangZhang,ShixinYang,ChenjiaBai,FeiWu,XiuLi,XuelongLi,andZhenWang.Towardsef-
ficientllmgroundingforembodiedmulti-agentcollaboration. arXivpreprintarXiv:2405.14314,
2024.
[52] ÅukaszKaiser,MohammadBabaeizadeh,PiotrMiÅ‚os,BÅ‚azË™ejOsinÂ´ski,RoyHCampbell,Konrad
Czechowski,DumitruErhan,ChelseaFinn,PiotrKozakowski,SergeyLevine,AfrozMohiuddin,
RyanSepassi,GeorgeTucker,andHenrykMichalewski. Modelbasedreinforcementlearning
foratari. InInternationalConferenceonLearningRepresentations,2020.
13A WorldModelsDetailsandHyperparameters
A.1 ObservationTokenizer
Ourtokenizerforlocalobservationdiscretizationisbasedontheimplementation3ofavanillaVQ-
VAE [44]. Faced with continuous non-vision observation, we build the encoder and decoder as
Multi-LayerPerceptrons(MLPs). Thedecoderisdesignedwiththesamehyperparametersastheones
oftheencoder. ThehyperparametersarelistedasTable2. Duringthephaseofcollectingexperience
fromtheexternalenvironment, eachagenttakesthereconstructedobservationsprocessedbythe
VQ-VAEasinputinsteadtoavoidthedistributionshiftbetweenpolicylearningandpolicyexecution.
FortrainingthisvanillaVQ-VAE,weuseastraight-throughestimatortoenablegradientbackpropa-
gationthroughthenon-differentiablequantizationoperationinthequantizationofVQ-VAE.Theloss
functionforlearningtheautoencoderisasfollows:
L (E,D,Z)=E E (cid:2) âˆ¥oiâˆ’oË†iâˆ¥2+âˆ¥sg[E(oi)]âˆ’ziâˆ¥2+Î²âˆ¥sg[zi]âˆ’E(oi)âˆ¥2(cid:3) (8)
VQâˆ’VAE iâˆ¼N oi q q
whereN ={1,2,...,n}denotesthesetofagents,sg[Â·]denotesthestop-gradientoperationandÎ² is
thecoefficientofthecommitmentlossâˆ¥sg[zi]âˆ’E(oi)âˆ¥2. Inpractice,wefoundthecodebookZ can
q
sufferfromcodebookcollapsewhenlearningfromscratch. Thus,weadopttheExponentialMoving
Averages(EMA)[44]techniquetoalleviatethisproblem.
Table2: VQVAEhyperparameters.
Hyperparameter Value
Encoder&Decoder
Layers 3
Hiddensize 512
Activation GELU[15]
Codebook
Codebooksize(N) 512
Tokensperobservation(K) 16
Codedimension 128
Coef. ofcommitmentloss(Î²) 10.0
A.2 Transformer
The shared Transformer serving as the local dynamics model is based on the implementation of
minGPT[22].GivenafixedimaginationhorizonH,itfirsttakesatokensequenceoflengthH(K+1)
composedofobservationtokensandactiontokens,andembedsitintoaH(K+1)Ã—Dtensorvia
separateembeddingtablesforobservationsandactions. Then,theaggregatedfeaturetensor,returned
bytheagent-wiseaggregationmodule,isinsertedaftertheactionembeddingtensorateverytimestep,
formingafinalembeddingtensorofshapeH(K+2)Ã—D. Thistensorisforwardedthroughfixed
Transformerblocks. Here,weadoptGPT2-likeblocks[34]asthebasicblocks. Thehyperparameters
arelistedasTable3. ToenabletrainingacrossallenvironmentsonasingleNVIDIARTX3090GPU,
weadaptimaginationhorizonH basedonthenumberofagents.
Table3: Transformerhyperparameters.
Hyperparameter Value
Imaginationhorizon(H) {15,8,5}
Embeddingdimension 256
Layers 10
Attentionheads 4
Weightdecay 0.01
Embeddingdropout 0.1
Attentiondropout 0.1
Residualdropout 0.1
3Codecanbefoundinhttps://github.com/lucidrains/vector-quantize-pytorch
14Table4: Perceiverhyperparameters.
Hyperparameter Value
Lengthoflatentquerying n(numberofagents)
Crossattentionheads 8
InnerTransformerlayers 2
Transformerattentionheads 8
Dimensionperattentionhead 64
Embeddingdropout 0.1
Attentiondropout 0.1
Residualdropout 0.1
A.3 Perceiver
ThePerceiver[19]isbasedontheopen-sourceimplementation4. Byaligningthelengthofthelatent
queryingarraywiththenumberofagentsn,weobtaintheintrinsicglobalrepresentationfeature
correspondingtoeachindividualagent. Wefurtherdiveintotheprocessofagent-wiserepresentation
aggregation: (i) the embedding tensor of shape (K + 1) Ã— D at each timestep, mentioned in
AppendixA.2,isconcatenatedwithothersfromallagents,therebygettingan(K+1)Ã—Dsequence
forthejointobservation-actionpairatthecurrenttimestep;(ii)throughthecross-attentionmechanism
withthelatentqueryingarray,theoriginalsequenceiscompressedfromlengthn(K+1)ton;(iii)the
compressedsequenceisthenforwardedthroughastandardtransformerwithbidirectionalattention
insidethePerceiver. ThehyperparametersarelistedasTable4.
B RewardLabelsinHL-Gauss
HL-Gaussmethod[6]leveragesHistogramLossesintroducedby[17]. Firstly,wecandefinethe
random variable Y with probability density f and cumulative distribution function F whose
Y Y
expectation is ri. Given M buckets of width Ï‚ = (r âˆ’r )/M centered at {b }M , the
t max min m m=1
distributionofY projectedontothehistogramwiththesebucketscanbecalculatedviaintegrating
overtheinterval[b âˆ’Ï‚/2,b +Ï‚/2],
m m
(cid:90) bm+Ï‚/2
p (b )= f (y)dy
m m Y
bmâˆ’Ï‚/2
=F (b +Ï‚/2)âˆ’F (b âˆ’Ï‚/2) (9)
Y m Y m
To stabilize classification learning over potential diverse reward distribution, we can model the
distributionofY asaGaussiandistributionN(Âµ = ri,Ïƒ2),whichsmoothsthetargetdistribution
t
computedbyEq.(9). ThehyperparameterÏƒisusedtocontrolthedegreeoflabelsmoothing,andis
typicallysettobe0.75Ï‚,asadvisedby[6]. Wereferto[6]formoredetails.
C BehaviourLearningDetails
InMARIE,weuseMAPPO-like[49]actorandcritic,wheretheactorandcriticshouldhavebeen
3-layerMLPs. However,unlikeotherCTDEmodel-freeapproaches,whosecritictakesadditional
globaloraclestatesfromtheenvironmentinthetrainingphase,ourworldmodelhardlyprovides
relatedpredictionsintheimaginedtrajectories. Toalleviatethisissue,weaugmentthecriticwith
anattentionmechanismandprovideitallreconstructedobservationsoË† ofallagents. Therefore,
t
the actor Ïˆ remains a 3-layer MLP with ReLU activation, while the critic Î¾ is enhanced with an
extralayerofself-attention, builtontopoftheoriginal3-layerMLP,i.e., weoverwritethecritic
Vi(oË†)â‰ƒE ((cid:80) Î³lâˆ’trË†i)foragenti. Similartooff-the-shelfCTDEmodel-freeapproaches,we
Î¾ t Ï€i lâ‰¥t l
Ïˆ
adoptparametersharingacrossagents.
Criticlossfunction WeutilizeÎ»-returninDreamer[9],whichemploysanexponentially-weighted
averageofdifferentk-stepsTDtargetstobalancebiasandvarianceastheregressiontargetforthe
4Codecanbefoundinhttps://github.com/lucidrains/perceiver-pytorch
15Table5: Behaviourlearninghyperparameters.
Hyperparameter Value
ImaginationHorizon(H) {15,8,5}
PredicteddiscountlabelÎ³ 0.99
Î» 0.95
Î· 0.001
ClippingparameterÏµ 0.2
critic. Givenanimaginedtrajectory{oË†i,ai,rË†i,Î³Ë†i}H foragenti,Î»-returniscalculatedrecursively
Ï„ Ï„ Ï„ Ï„ t=1
as,
(cid:40) (cid:104) (cid:105)
rË†i+Î³Ë†i (1âˆ’Î»)Vi(oË†)+Î»Vi(oË† ) if t<H
Vi(oË†)= t t Î¾ t Î» t+1 (10)
Î» t Vi(oË†) if t=H
Î¾ t
The objective of the critic Î¾ is to minimize the mean squared difference Li with Î»-returns over
Î¾
imaginedtrajectoriesforeachagenti,as
(cid:20) (cid:21)
Li =E (cid:88)Hâˆ’1(cid:0) Vi(oË†)âˆ’sg(Vi(oË†))(cid:1)2 (11)
Î¾ Ï€i Î¾ t Î» t
Ïˆ t=1
wheresg(Â·)donotesthestop-gradientoperation. Weoptimizethecriticlosswithrespecttothecritic
parametersÎ¾usingtheAdamoptimizer.
Actorlossfunction TheobjectivefortheactionmodelÏ€ (Â·|oË†i)istooutputactionsthatmaximize
Ïˆ t
thepredictionoflong-termfuturerewardsmadebythecritic. Toincorporateintermediaterewards
moredirectly,wetraintheactortomaximizethesameÎ»-returnthatwascomputedfortrainingthe
critic. Intermsofthenon-stationarityissueinmulti-agentscenarios,weadoptPPOupdates,which
introduceimportantsamplingforactorlearning. Theactorlossfunctionforagentiis:
Hâˆ’1
(cid:104) (cid:88) (cid:16) (cid:17) (cid:105)
Li =âˆ’E min ri(Ïˆ)Ai,clip(ri(Ïˆ),1âˆ’Ïµ,1+Ïµ)Ai +Î·H(Ï€i(Â·|oË†i)) (12)
Ïˆ pÏ•,Ï€ Ïˆi
old
t t t t Ïˆ t
t=0
whereri(Ïˆ) = Ï€i/Ï€i isthepolicyratioandAi = sg(Vi(oË†)âˆ’Vi(oË†))istheadvantage. We
t Ïˆ Ïˆold t Î» t Î¾ t
optimize the actor loss with respect to the actor parameters Ïˆ using the Adam optimizer. In the
discountpredictionofMARIE,wesetitslearningtargetÎ³ tobe0.99. Overallhyperparametersare
showninTable5.
D ExtendedAnalysisonAttentionPatterns
Toprovidequalitativeanalysisofourworldmodel,weselecttypicalscenariosâ€“3s_vs_5zwhere
ourmethodachievesthemostsignificantimprovementcomparedtootherbaselinesforvisualizing
attentionmapsinsidetheTransformer. Forthesakeofsimpleandclearvisualization, wesetthe
imaginationhorizonH as5. Intermsofcross-attentionmapsintheaggregationmodule,weselecta
scenario2s3zincluding5agentsforvisualization. VisualizationresultsaredepictedasFig.8and
Fig.9.
Thepredictionoflocaldynamicsentailstwodistinctattentionpatterns. TheleftoneinFig.8can
be interpreted as a Markovian pattern, in which the observation prediction lays its focus on the
previoustransition. Incontrast,therightoneisregularlystriated,withthemodelattendingtospecific
tokensinmultiplepriorobservations. Intermsoftheagent-wiseaggregation,wealsoidentifytwo
distinctpatterns: individualityandcommonality. ThetoponeinFig.9illustratesthateachagent
flexibly attends to different tokens according to their specific needs. In contrast, the bottom one
exhibitsconsistentattentionallocationacrossallagents,withattentionhighlightedinnearlyidentical
positions. ThediversepatternsintheTransformerandPerceivermaybethekeytoaccurateand
consistentimagination.
16ğ‘–ğ‘– ğ‘–ğ‘–
ğ‘œğ‘œğ‘¡ğ‘¡ ğ‘œğ‘œğ‘¡ğ‘¡
ğ‘–ğ‘– ğ‘–ğ‘–
ğ‘ğ‘ğ‘¡ğ‘¡ ğ‘ğ‘ğ‘¡ğ‘¡
ğ‘–ğ‘– ğ‘–ğ‘–
ğ‘’ğ‘’ğ‘¡ğ‘¡ ğ‘’ğ‘’ğ‘¡ğ‘¡
ğ‘–ğ‘– ğ‘–ğ‘–
ğ‘œğ‘œğ‘¡ğ‘¡+5 ğ‘œğ‘œğ‘¡ğ‘¡+5
ğ‘–ğ‘– ğ‘–ğ‘–
ğ‘ğ‘ğ‘¡ğ‘¡+5 ğ‘ğ‘ğ‘¡ğ‘¡+5
Figğ‘–ğ‘–ure8:AttentionpatternsintheTransformer. Weğ‘–ğ‘–observetwodistincttypesofattentionweights
ğ‘’ğ‘’ğ‘¡ğ‘¡+5 ğ‘’ğ‘’ğ‘¡ğ‘¡+5
during the prediction of local dynamics. In the first one (left), the next observation prediction is
primarilydependentonthelasttransition,whichmeanstheworldmodelhaslearnedtheMarkov
propertycorrespondingtoDec-POMDPs. Thesecondtype(right)exhibitsaregularlystriatedpattern,
wherethenextobservationpredictionhingesoverwhelminglyonthesamedimensionofmultiple
previous timesteps. The above attention weights are produced by a sixth-layer and ninth-layer
attentionheadduringimaginationsonthe3s_vs_5zscenario.
Cross Attention Head 1
agent 1
agent 2
agent 3
agent 4
agent 5
agent
1â€²
s
ğ‘œğ‘œğ‘¡ğ‘¡1 ,ğ‘ğ‘ğ‘¡ğ‘¡1
tokens agent
2â€²
s
ğ‘œğ‘œğ‘¡ğ‘¡2 ,ğ‘ğ‘ğ‘¡ğ‘¡2
tokens agCernots s3
Aâ€² stteğ‘œğ‘œnğ‘¡ğ‘¡3 t,ioğ‘ğ‘nğ‘¡ğ‘¡3
H teoakde 6ns agent
4â€²
s
ğ‘œğ‘œğ‘¡ğ‘¡4 ,ğ‘ğ‘ğ‘¡ğ‘¡4
tokens agent
5â€²
s
ğ‘œğ‘œğ‘¡ğ‘¡5 ,ğ‘ğ‘ğ‘¡ğ‘¡5
tokens
agent 1
agent 2
agent 3
agent 4
agent 5
Figure9ag:enCt
1â€² rsoğ‘œğ‘œsğ‘¡ğ‘¡1 ,sğ‘ğ‘ğ‘¡ğ‘¡1
a totkteensntionagepnt
a2â€² sttğ‘œğ‘œeğ‘¡ğ‘¡2 ,rğ‘ğ‘ğ‘¡ğ‘¡2
n tsokeinns theagPenet
3râ€² sceğ‘œğ‘œğ‘¡ğ‘¡3 i,vğ‘ğ‘ğ‘¡ğ‘¡3
e tro.keWns eobagsenet
r4vâ€² seğ‘œğ‘œğ‘¡ğ‘¡4 t,ğ‘ğ‘hğ‘¡ğ‘¡4
e toiknendsividaugeantl
5iâ€² tsyğ‘œğ‘œğ‘¡ğ‘¡5 a,ğ‘ğ‘nğ‘¡ğ‘¡5
d tokcenosmmonality
intheagent-wiseaggregation. Thetoppartofthefigurerepresentstheindividuality,whereagents
adjusttheirattentionsoverthewholejointtokensequenceattimesteptflexiblyaccordingtotheir
ownneeds. Incontrast, thebottomexhibitsthecommonality, whereeveryagentâ€™sattentionover
thejointtokensequenceisemphasizedinthesimilarpositionsofthesequence. Thecrossattention
weightsmentionedaboveareproducedbythefirstandsixthheadofthecrossattentionwithinthe
Perceiver,duringtheagent-wiseaggregationonthe2s3zscenario.
E BaselineImplementationDetails
MAMBA [5] is evaluated based on the open-source implementation: https://github.com/
jbr-ai-labs/mambawiththehyperparametersinTable6.
MAPPO [49] is evaluated based on the open-source implementation: https://github.com/
marlbenchmark/on-policywiththecommonhyperparametersinTable7.
QMIX [35] is evaluated based on the open-source implementation: https://github.com/
oxwhirl/pymarlwiththehyperparametersinTable8.
QPLEX [46] is evaluated based on the open-source implementation: https://github.com/
wjh720/QPLEX with the hyperparameters in Table 9. Since its implementation is mostly based
ontheopen-sourceimplementation: PyMARL[38],itsmosthyperparameterssettingremainsthe
sameastheoneinQMIXinadditiontoitsownspecialhyperparameters.
MBVD[48]isevaluatedbasedontheimplementationinitssupplementarymaterialfromhttps:
//openreview.net/forum?id=flBYpZkW6ST with the hyperparameters in Table 10. Akin to
QPLEX,itsimplementationisbasedontheopen-sourceimplementation:PyMARL,itsmosthyperpa-
rameterssettingremainsthesameastheoneinQMIXinadditiontoitsownspecialhyperparameters.
17
â€¦
â€¦
â€¦
â€¦
â€¦
â€¦Table6: HyperparametersforMAMBAinSMACenvironments.
Hyperparameter Value
Batchsize 256
Î»forÎ»-returncomputation 0.95
Entropycoefficient 0.001
Entropyannealing 0.99998
Numberofpolicyupdates 4
Epochsperpolicyupdate 5
ClippingparameterÏµ 0.2
ActorLearningrate 0.0005
CriticLearningrate 0.0005
DiscountfactorÎ³ 0.99
ModelLearningrate 0.0002
Numberofmodeltrainingepochs 60
Numberofimaginedrollouts 800
Sequencelength 20
ImaginationhorizonH 15
Buffersize 2.5Ã—105
Numberofcategoricals 32
Numberofclasses 32
KLbalancingentropyweight 0.2
KLbalancingcrossentropyweight 0.8
Gradientclipping 100
Collectedtrajectoriesbetweenupdates 1
Hiddensize 256
Table7: CommonhyperparametersforMAPPOinSMACenvironments.
Hyperparameter Value
Batchsize numenvsÃ—bufferlengthÃ—numagents
Minibatchsize batchsize/mini-batch
Recurrentdatachunklength 10
GAEÎ» 0.95
DiscountfactorÎ³ 0.99
Valueloss huberloss
Huberdelta 10.0
Optimizer Adam
Optimizerlearningrate 0.0005
Optimizerepsilon 1Ã—10âˆ’5
Weightdecay 0.0
Gradientclipping 10
Networkinitialization orthogonal
Userewardnormalization True
Usefeaturenormalization True
18Table8: HyperparametersforQMIXinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0â†’0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
DiscountfactorÎ³ 0.99
Optimizer RMSProp
RMSPropÎ± 0.99
RMSPropÏµ 10âˆ’5
Gradientclipping 10
Table9: HyperparametersforQPLEXinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0â†’0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
DiscountfactorÎ³ 0.99
Optimizer RMSProp
RMSPropÎ± 0.99
RMSPropÏµ 10âˆ’5
Gradientclipping 10
NumberoflayersinHyperNetwork 1
Numberofheadsintheattentionmodule 4
Table10: HyperparametersforMBVDinSMACenvironments.
Hyperparameter Value
Batchsize 32episodes
Buffersize 5000episodes
Epsiloninepsilon-greedy 1.0â†’0.05
Epsilonannealtime 50000timesteps
Traininterval 1episode
DiscountfactorÎ³ 0.99
Optimizer RMSProp
RMSPropÎ± 0.99
RMSPropÏµ 10âˆ’5
Gradientclipping 10
NumberoflayersinHyperNetwork 1
Numberofheadsintheattentionmodule 4
Horizonoftheimaginedrollout 3
KLbalancingÎ± 0.3
DimensionofthelatentstatesË† numagentsx16
19Table11: HyperparametersforMARIEinSMACenvironments.
Hyperparameter Value
Batchsizefortokenizertraining 256
Batchsizeforworldmodeltraining 30
Optimizerfortokenizer AdamW
Optimizerforworldmodel AdamW
Optimizerforactor&critic Adam
Tokenizerlearningrate 0.0003
Worldmodellearningrate 0.0001
Actorlearningrate 0.0005
Criticlearningrate 0.0005
Gradientclippingforactor&critic 100
Gradientclippingfortokenizer 10
Gradientclippingforworldmodel 10
Weightdecayforworldmodel 0.01
Î»forÎ»-returncomputation 0.95
DiscountfactorÎ³ 0.99
Entropycoefficient 0.001
Buffersize(transitions) 2.5Ã—105
Numberoftokenizertrainingepochs 200
Numberofworldmodeltrainingepochs 200
Collectedtransitionsbetweenupdates 100or200
Epochsperpolicyupdate(PPOepochs) 5
PPOClippingparameterÏµ 0.2
Numberofimaginedrollouts 600or400
ImaginationhorizonH {15,8,5}
Numberofpolicyupdates {4,10,30}
Numberofstackingobservations 5
Observeagentid False
Observelastactionofitself False
F ParametersSettinginSMAC
AllourexperimentsarerunonamachinewithasingleNVIDIARTX3090GPU,a36-coreCPU,
and128GBRAM.WeprovidethehyperparametersofMARIEforexperimentsinSMAC,shownas
Table11. ToenabletherunningofexperimentsinallscenarioswithasingleNVIDIARTX3090
GPU, we set the imagination horizon H as 8 for other scenarios involving the number of agents
n > 5, 15 for n â‰¤ 5. In so_many_baneling and 2s3z, we set the imagination horizon H as 5.
Correspondingly,thenumberofpolicyupdatesinimaginationsvarieswithimaginationhorizonH.
Additionally,itshouldbenotedthattheresult HL-Gauss Smooth L1
in 3s_vs_5z was obtained with using L1 loss
3s_vs_5z
functionforrewardprediction,whichwasthe 1.0
sameastherewardpredictioninMAMBA.We 0.8
foundtheperformanceinthisscenarioisquite
0.6
sensitive to the selection of reward loss func-
tion, illustrated by Fig. 10. Considering the 0.4
significantlylargeactionspaceof2c_vs_64zg,
0.2
weenabletheobservationofagentidandlast
actionforeachagentanddisablestackingthe 0.0
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
last5observationsasinputtothepolicy.
Environment Steps 1e5
Figure 10: Performance between different loss
functionsforrewardpredictionin3s_vs_5z.
20
etaR
niWG OverviewofMARIEAlgorithm
Pseudo-codeissummarizedasAlgorithm1.
Algorithm1MARIE
//mainloopoftraining
forepochsdo
collect_experience(num_transitions)
forlearning_world_model_steps_per_epochdo
train_world_model()
endfor
forlearning_behaviour_steps_per_epochdo
train_agents()
endfor
endfor
functioncollect_experience(n):
o â†env.reset()
0
fort=0,...,nâˆ’1do
//processedbyVQ-VAE
oË† â†D(E(o ))
t t
Sampleai âˆ¼Ï€i(ai|oË†i),âˆ€i
t Ïˆ t t
o ,r ,doneâ†env.step(a )
t+1 t t
ifdone=Truethen
o â†env.reset()
t+1
Î³ â†0.
t
else
Î³ â†0.99
t
endif
endfor
D â†Dâˆª{o ,a ,r ,Î³ }nâˆ’1
t t t t t=0
functiontrain_world_model():
Sample{o ,a ,r ,Î³ }t=Ï„+Hâˆ’1
t t t t t=Ï„
Update(E,D,Z)viaL overobservations{o }t=Ï„+Hâˆ’1
VQâˆ’VAE t t=Ï„
foragenti=1,...,ndo
UpdateÏ•,Î¸viaL (Ï•,Î¸)overlocaltrajectories{oi,ai,r ,Î³ }t=Ï„+Hâˆ’1
Dyn t t t t t=Ï„
endfor
functiontrain_agents():
Sampleaninitialobservationo âˆ¼D
0
{xi }K â†E(oi),oË†i â†D(E(oi)),âˆ€i
0,j j=1 0 0 0
fort=0,...,H âˆ’1do
Sampleai âˆ¼Ï€i(ai|oË†i),âˆ€i
t Ïˆ t t
Aggregate(x1 ,...,x1 ,a1,...,xn ,...,xn ,an)into(e1,...,en)viathePerceiverÎ¸
t,1 t,K t t,1 t,K t t t
SamplexË†i ,rË†i,Î³Ë†i âˆ¼p (xË†i ,rË†i,Î³Ë†i|xi ,ai,ei,...,xË†i ,ai,eË†i),âˆ€i
t+1,Â· t t Ï• t+1,Â· t t 0,Â· 0 0 t,Â· t t
oË†i â†D(xË†i ),âˆ€i
t+1 t+1,Â·
endfor
foragenti=1,...,ndo
UpdateactorÏ€i andcriticViviaL (Ï•,Î¸)overimaginedtrajectories{oË†i,ai,rË†i,Î³Ë†i}t=Hâˆ’1
Ïˆ Î¾ Dyn t t t t t=0
endfor
21