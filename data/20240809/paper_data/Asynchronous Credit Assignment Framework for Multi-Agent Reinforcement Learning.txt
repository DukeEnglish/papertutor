Asynchronous Credit Assignment Framework for
Multi-Agent Reinforcement Learning
YonghengLiang HejunWu
SchoolofComputerScienceandEngineering SchoolofComputerScienceandEngineering
SunYat-SenUniversity SunYat-SenUniversity
liangyh38@mail2.sysu.edu.cn wuhejun@mail.sysu.edu.cn
HaitaoWang HaoCai
SchoolofComputerScienceandEngineering CollegeofEngineering
SunYat-SenUniversity ShantouUniversity
wanght39@mail2.sysu.edu.cn haocai@stu.edu.cn
Abstract
Creditassignmentisacoreproblemthatdistinguishesagentsâ€™marginalcontribu-
tionsforoptimizingcooperativestrategiesinmulti-agentreinforcementlearning
(MARL).Currentcreditassignmentmethodsusuallyassumesynchronousdecision-
making among agents. However, a prerequisite for many realistic cooperative
tasksisasynchronousdecision-makingbyagents,withoutwaitingforothersto
avoiddisastrousconsequences. Toaddressthisissue,weproposeanasynchronous
creditassignmentframeworkwithaproblemmodelcalledADEX-POMDPand
a multiplicative value decomposition (MVD) algorithm. ADEX-POMDP is an
asynchronousproblemmodelwithextravirtualagentsforadecentralizedpartially
observablemarkovdecisionprocess. WeprovethatADEX-POMDPpreservesboth
thetaskequilibriumandthealgorithmconvergence. MVDutilizesmultiplicative
interactiontoefficientlycapturetheinteractionsofasynchronousdecisions,and
wetheoreticallydemonstrateitsadvantagesinhandlingasynchronoustasks. Ex-
perimentalresultsshowthatontwoasynchronousdecision-makingbenchmarks,
OvercookedandPOAC,MVDnotonlyconsistentlyoutperformsstate-of-the-art
MARLmethodsbutalsoprovidestheinterpretabilityforasynchronouscooperation.
1 Introduction
Multi-agentreinforcementlearning(MARL)[1â€“3]ispromisingformanycooperativetasks,such
as video games [4] and collaborative control [5â€“7]. Previously, such tasks were formulated as a
Dec-POMDP(decentralizedpartiallyobservablemarkovdecisionprocess)[8],assumingthateach
atomicactionisperformedsynchronouslyinatimestep. However,mostrealisticcooperativetasks
of real-time require that agents should act without waiting for other agents, to avoid disastrous
consequences[9â€“11].
TocarryoutasynchronouscooperationtasksusingtheoriginalMARLdesignedforsynchronous
scenarios, researchers have proposed two types of methods as follows. 1) Discarding: Agents
collect data of other agents only when they make decisions, but discard the data of others while
executingtheiractions,asshowninFigure1aand1b.Thistypeofmethodusuallyneedspolicy-based
MARLfortraining[9,11,12]. 2)Padding: Thistypestudiestransformasynchronoustasksinto
synchronousonesviapaddingblankactionssoastoapplymostexistingMARL[13,14],asshown
inFigure1c.
Preprint.Underreview.
4202
guA
7
]AM.sc[
1v29630.8042:viXrağ‘ ğ‘ ğ‘ ğ‘
ğ‘1 ğ‘1 ğ‘1
3 3 3 3 3 3 3
2
ğ‘ 20 ğ‘ 22 ğ‘ 25
2
ğ‘ 22
2
ğ‘ 22
2
ğ‘ 22
1 1
ğ‘ 14
1
ğ‘ 10
1
ğ‘ 10
ğ‘‡ ğ‘‡ ğ‘‡ ğ‘‡
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6
(a)Discarding (b)CAAC (c)Padding (d)MVD
Figure1: IllustrationofdifferentasynchronousMARLmethods. Blue/yellowcirclesdepictcollected
decisiondata. Smallwhiteonesdepictpaddingactions. (a)Agent#2disregardsinformationofother
agentsfromt tot . (b)Agent#2neglectsactiona1whenassigningcreditatt . (c)Creditatt is
0 2 3 2 2
attributedtothepaddingactions. (d)OurproposedMVDfullyaccountsfortheinteractionsamong
asynchronousdecisionsbeingexecutedatt .
2
Unfortunately,bothdiscardingandpaddingareharmfultocreditassignment[15]ofasynchronous
agents. On the one hand, in the discarding type methods, the discarded information leads to an
inaccurateestimationoftheimpactsofactionsfromotheragents. Forinstance,CAAC[10]explored
the asynchronous credit assignment in the application of bus holding control [16]. As shown in
Figure1b,attimestept ,agent#2discardstheinformationofactiona1thatisnotwithinthetime
2 3
window. Ontheotherhand, the padding typemethodsgenerallyusevaluedecomposition(VD)
[1â€“3]tosolvethecreditassignmentproblem. VDisawidely-usedalgorithmtolearnthemarginal
contributionofeachagentanddecomposetheglobalQ-valueQ intoindividualagent-wiseutilities
tot
Q toguideagentsâ€™behaviors. DespitethesuccessofVDalgorithmsinoptimizingthecollective
i
policyforsynchronoustasks,theyarenotapplicabletoasynchronoussettings,sincetheinteractions
amongagentsatdifferenttimestepsarenottakenintoaccountintheseVDalgorithms. Asshownin
Figure1c,theimpactsofactionsa1anda0areignoredbyagent#2attimestept ,thusVDhasto
3 1 2
decomposeaQ consideringtheimpactsofblankactionsatthistimestep.
tot
Inthispaper,weproposeanasynchronouscreditassignmentframeworkwithanewproblemmodel,
ADEX-POMDP, and a VD algorithm, MVD. ADEX-POMDP is named after Asynchronous De-
centralizedPOMDPwithExtraAgents,whichextendsthepreviousDec-POMDPviaextravirtual
agents (also called extra/virtual agents) to migrate asynchronous decisions to a single time step.
Weprovethataddingextraagentsneitherchangestheoriginaltaskequilibriumnorinterfereswith
thealgorithmconvergence. WederiveageneralmultiplicativeinteractionformforVDanddesign
aMultiplicativeValueDecomposition(MVD)algorithmtosolveADEX-POMDP.MVDutilizes
multiplicativeinteractions[17,18]toefficientlycapturethemutualimpactsamongasynchronous
decisions. IntheoreticalcomparisonwithtraditionalVD,MVDnotonlyexpandsthefunctionclass
strictly but alsobears advantagesin tackling asynchronousity. We evaluateMVD across various
difficultylevelsintwoasynchronousdecision-makingbenchmarks,Overcooked[19]andPOAC[20].
ExperimentalresultsshowthatMVDachievesconsiderableperformanceimprovementsincomplex
scenariosandprovideseasy-to-understandinteractionprocessesamongasynchronousdecisions.
Our contributions are summarized as follows: 1) To the best of our knowledge, we propose the
firstgeneralasynchronouscreditassignmentframeworkwithanewasynchronousproblemmodel,
ADEX-POMDP,andaVDalgorithm,MVD.2)WeprovethatADEX-POMDPpreservesthetask
optimalityandthealgorithmconvergenceandwetheoreticallydemonstratetheadvantagesofMVD,
aswell. 3)Weconductextensiveexperimentsinasynchronousbenchmarks,showingMVDâ€™ssuperior
performanceandenhancedinterpretabilityofasynchronousinteractions.
2 Preliminaries
2.1 Dec-POMDP
Afullycooperativemulti-agenttaskwhereallagentsmakedecisionssimultaneouslycangenerally
beformulatedasaDec-POMDP.Dec-POMDPisdefinedbyatupleâŸ¨N,S,A,P,r,O,â„¦,Î³âŸ©,where
N denotesafinitesetofnagentsandsâˆˆS representstheglobalstateoftheenvironment. Ateach
timestep,eachagentiâˆˆN obtainsitsownobservationo âˆˆâ„¦determinedbythepartialobservation
i
O(s,i)andselectsanactiona âˆˆ Atoformajointactiona = [a ]n âˆˆ An. Subsequently, all
i i i=1
agentssimultaneouslycompletetheiractions,leadingtothenextstatesâ€²accordingtothetransition
2functionP(sâ€²|s,a):SÃ—An â†’S andearningaglobalrewardr(s,a):SÃ—An â†’R. Eachagenti
hasitsownpolicyÏ€ (a |Ï„ ):T Ã—Aâ†’[0,1]basedonlocalaction-observationhistoryÏ„ âˆˆT. The
i i i i
objectiveofallagentsistofindanoptimaljointpolicyÏ€ =[Ï€ ]n andmaximizetheglobalvalue
functionQÏ€ =E[(cid:80)âˆ Î³trt+1]withadiscountfactorÎ³ âˆˆ[0,1i )i .=1
t=0
2.2 CreditAssignmentandValueDecomposition
CreditassignmentisacorechallengingproblemindesigningreliableMARLmethods[15].Itfocuses
onattributingteamsuccesstoindividualagentsbasedontheirrespectivemarginalcontributions,aim-
ingatcollectivepolicyoptimization. VDalgorithmsarethemostpopularbranchesinMARL.They
leverageglobalinformationtolearnagentsâ€™contributionanddecomposetheglobalQ-valuefunction
Q (s,a)intoindividualutilityfunctionsQ (Ï„ ,a ). Intheexecutionphase,agentscooperatevia
tot i i i
theircorrespondingQ (Ï„ ,a ), therebyrealizingcentralizedtraininganddecentralizedexecution
i i i
(CTDE)[21]. TraditionalVDalgorithms, includingVDN[1], QMIX[2], andQatten[3], canbe
representedbythefollowinggeneraladditiveinteractionformulation[22]:
n
(cid:88)
Q =Q (s,Q ,Q ,Â·Â·Â· ,Q )=k + k Q , (1)
tot tot 1 2 n 0 i i
i=1
wherek isaconstantandk denotesthecreditthatreflectsthecontributionsofQ toQ .
0 i i tot
To address the problem of capturing high-order interaction among agents, which traditional VD
algorithmsignored,NA2Q[23]recentlyintroducedageneralizedadditivemodel(GAM)[24],as
follows:
n
(cid:88) (cid:88)
Q =f + k f (Q )+Â·Â·Â·+ k f (Q )+Â·Â·Â·+k f (Q ,Â·Â·Â· ,Q ), (2)
tot 0 i i i j j j 1Â·Â·Â·n 1Â·Â·Â·n 1 n
i=1 jâˆˆDl
wheref isaconstant, f âˆˆ {f ,Â·Â·Â· ,f }m takesl (l âˆˆ {1,Â·Â·Â· ,n})individualutilitiesQ as
0 j 1 1Â·Â·Â·n i
inputandcapturethel-orderinteractionsamongtheseagents,andD ={i Â·Â·Â·i }isanon-empty
l 1 l
subsets.
Inordertomaintaintheconsistencybetweenlocalandglobaloptimalactionsafterdecomposition,
theseVDalgorithmsshouldsatisfythefollowingindividual-global-max(IGM)principle[25]:
argmaxQ (s,a)=(argmaxQ (Ï„ ,a ),Â·Â·Â· ,argmaxQ (Ï„ ,a )). (3)
tot 1 1 1 n n n
a a1 an
Forexample,QMIXholdsthemonotonicity âˆ‚Qtot â‰¥0andachievesIGMbetweenQ andQ . The
âˆ‚Qi tot i
furtherintroductionofVDandothercreditassignmentmethodscanbereferredtoinAppendixA.1.
2.3 AsynchronousMARL
Currently,researchinasynchronousMARLprimarilyfocusesonprocessingasynchronousdatafor
compatibilitywithexistingMARLmethods. AsmentionedinSection1,theseworkscanbroadlybe
categorizedintotwotypesofmethodsasdepictedindetailinthefollowing.
The discarding type method focus solely on decision time-step information, treating the sum of
rewards during the execution of an agentâ€™s decision as the reward for its asynchronous decision.
Agentscollecttheirownasynchronousdecisioninformationandemployexistingpolicy-basedMARL
methodsfortraining[9,11,12]. However,discardingthedecisioninformationofotheragentscan
causenon-stationarity[15]andhurtcreditassignment. AsshowninFigure1a,agent#2isunableto
determineifthetransitionfromglobalstates2tos5andtheaccumulatedreward(cid:80)5 rtarecaused
t=3
byitsownasynchronousdecisiona2orbydecisionsofotheragents,suchasa0,a4,anda1.
2 1 1 3
Thepaddingtypemethodtransformstheasynchronousdecision-makingproblemintoasynchronous
onethroughpaddingaction,therebyobtainingaDec-POMDPsoastoapplyexistingMARLmethods
[13,14]. SinceDec-POMDPrequiresthecollectionofdecisioninformationfromallagentsateach
timestep,paddingactioncanbeusedasasubstituteforthedecisioninformationofagentsthatare
executingactions. Collectingdecisiondataateachtimestepcanmitigatenon-stationarityissues
and enable one to address asynchronous problems in a traditional way. Nevertheless, redundant
paddingactionmayinterferewiththecreditassignmentprocess[9]. AsshowninFigure1c,agent
3#2canonlylearnthecontributionofactiona2att . Infact,a2continuouslyaffectstheenvironment
2 2 2
andthedecision-makingofotheragentsduringitsexecution. However, thecreditsfromt tot
4 5
aremistakenlyattributedtopaddingactionpa3 andpa4,causingthefailureoftheoriginalcredit
2 2
assignmentprocessinasynchronousscenarios. ThefurtherintroductionofasynchronousMARL
methodscanbereferredtoinAppendixA.2.
3 ADEX-POMDP
Inasynchronouscreditassignment,thekeyfactoristheinteractionbetweenasynchronousdecisions
ofdifferentagents. Specifically,theagentwhoexecutesfirstmustpredicthowlaterchoicesofother
agentswouldaffectitsexecution. Meanwhile,theagentwhoexecuteslaterneedstoconsiderthe
impactofthecurrentactionsofotheragentsonitsdecision. However,bothdiscardingandpadding
methodshavelimitationsincapturingthiskeyfactor. Theformerignoresdecisionsfromotheragents,
whilethelattermakesdecisionsaccordingtothepaddingblankactions. Intuitively, thesimplest
implementationofasynchronouscreditassignmentistousethemostrecentactionaspaddingaction
[13,14]. Unfortunately,suchanapproachintroducesambiguity,asitcannotdistinguishwhetherthe
agentiscontinuingtheexecutionoftheoriginalactionorrestartingtheexecutionofthesameaction.
Thiscouldcausesemanticdifferencesbetweentheoriginalexecutionandthenewexecutionand
subsequently,thetraininginsimulatedenvironmentsmightnotconverge.
OurmotivationforADEX-POMDPistomigrateasynchronousde-
cisionsfromdifferenttimestepstothesametimestepbymeansof ğ‘
virtualagentstomaintainsemanticconsistencyandensureaccurate
3â€²
asynchronouscreditassignment. ThegeneralideaofADEX-POMDP
2â€²
is as follows: Suppose the action a of agent i takes multiple time
i 1â€²
stepstocomplete,e.g.,timestept ,t ,andt ofagent#1inFigure2.
1 2 3
3
ADEX-POMDPstillusesblankactionaspaddingforeachtimestep ğ‘1
3
whenagentiiscontinuingtheexecutionofactiona .ADEX-POMDP 2
i
introducesanextravirtualagentiâ€²,toconstructapairofreal-virtual 1
ğ‘0 ğ‘‡
agents<i,iâ€² >. Agentiâ€²usesthesamepolicyasthatofagentiand 1
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡
executesthesamedecisionofa ,asshowninFigure2. Thepolicy 0 1 2 3 4 5 6
i
ofiwillbeupdatedaccordinglywheniâ€²â€™spolicyisupdated. Wecan Figure 2: ADEX-POMDP.
alsoadjustiâ€™spolicyaccordingtotheinfluenceofitsasynchronous At t 2, a0
1
and a1
3
are still
decisionsa onotheragentsandtheenvironment. Thelifetimeofan executing, extra agents #1â€²
i
extraagentisonlyonetimestep. and#3â€²areintroducedtore-
executethesetwodecisions.
ADEX-POMDPisanasynchronousdecision-makingproblemmodel
inwhichatomicactionstakedifferentexecutiontimestepstocom-
plete. BeforegoingtothedetailedformulationofADEX-POMDP,thefrequentlyuseddefinitions
needtobelistedinthefollowing: Aregulartimestepisdenotedast. Theobservationformaking
adecisionisdenotedasoËœ,referringtothemostrecentobservationuponwhichanagentmakesthe
decisiontoexecuteanewasynchronousaction. aËœdenotesthemostrecentnewdecisionmadeby
theagent. Anasynchronousactioniscompletedwithinadifferentnumberoftimestepsfromeach
agentâ€™sperspective.
Definition1(ADEX-POMDP). ADEX-POMDPisatupleâŸ¨N(cid:98),I,S(cid:98),A,P(cid:98),r (cid:98),O(cid:98),O P,â„¦,Î³âŸ©,where
N(cid:98) ={N,Nâ€²}. TherealagentsetisN ={1,2,Â·Â·Â· ,n}andthecorrespondingsetofvirtualagents
isNâ€² ={1â€²,2â€²,Â·Â·Â· ,nâ€²}.Lettheagentthatismakingthedecisionbei âˆˆN andtheagentexecuting
d
anactionbei âˆˆN. GivenanoriginalstatesâˆˆS,functionI(s)givesasetNâ€² âŠ†Nâ€²,whereagent
c c
iâ€² âˆˆNâ€²andagenti formanreal-virtualpair,<i ,iâ€² >. Thetwoagentsinthispairhavethesame
c c c c c
policy. s (cid:98)=[s;oËœ c]âˆˆS(cid:98)accordingtotheoriginals,where[Â·;Â·]istheconcatenationoperation. oËœ cis
obtainedaccordingtoO (oËœ |s),whichisthejointobservationofallagentsi formakingdecisions.
P c c
Eachextraagentiâ€²
c
receiveso
iâ€² c
= oËœ
ic
âˆˆ â„¦accordingtoO(cid:98)(s (cid:98),iâ€² c)andselectsactiona
iâ€² c
= aËœ
ic
to
forma
(cid:98)
=[a;aËœ c]âˆˆAn+nâ€² c,wherenâ€²
c
=|N câ€²|,aâˆˆAnistheoriginaljointaction,andaËœ cisthemost
recentnewdecisionmadebyallagentsi . Subsequently,theymovetothenextstatesâ€²accordingto
c (cid:98)
P(cid:98)(s (cid:98)â€²|s (cid:98),a (cid:98))=O P(oËœâ€² c|sâ€²)P(sâ€²|s,a)andearnajointrewardr (cid:98)(s (cid:98),a (cid:98))=r(s,a).
4AsshowninTheorem1,ADEX-POMDPnotonlyfacilitatesthehandlingofinteractionsbetween
asynchronousdecisions,butthisasynchronousMARLmodelalsopreservestheinherentcharacteris-
ticsofthetaskandthesolutionprocess.
Theorem1. IntroducingextraagentsinADEX-POMDPpreservestheoriginalequilibriumofthe
taskandmaintainstheconvergenceofthevaluedecompositionmethod.
Proof. Firstly, We prove that ADEX-POMDP preserves the original markov perfect equilibrium
(MPE) mentioned in Definition 2. According to Lemma 1, given any joint policy Ï€ 1, the con-
vergedglobalQ-valuefunctionQÏ€ underDec-POMDPisequaltotheconvergedglobalQ-value
Dec
functionQÏ€ underADEX-POMDP.Thus,wehaveQÏ€ (s,a) = QÏ€ (s,a),whichim-
ADEX Dec ADEX (cid:98) (cid:98)
pliesVÏ€ (s) = VÏ€ (s). Therefore,theoriginalMPEÏ€âˆ— inDec-POMDPisalsoanMPEin
Dec ADEX (cid:98)
ADEX-POMDP.
Secondly,regardingtheVDoperatorTâˆ— =(cid:81) Â·Tâˆ—mentionedinDefinition5,wesupposethat
VD VD
theVDoperatorTâˆ— guaranteesconvergenceunderDec-POMDP.Ontheonehand,accordingto
VD
Lemma2,theintroductionofextraagentsdoesnotaffectTâˆ—,whichenableslearningofthesame
Qâˆ— asinDec-POMDP.Ontheotherhand,sinceweassumethatTâˆ— convergesinDec-POMDP,
whichmeans(cid:81) canfindtheoptimalfâˆ— (Q (s,a ),Â·Â·Â· ,Q (s,V aD ))toapproximateQâˆ—within
the function claV ssD Q . In ADEX-POMD Dec P, to1 app1 roximate n the san me Qâˆ—, (cid:81) simply needs
tot VD
to set fâˆ— = fâˆ— (Q (s,a ),Â·Â·Â· ,Q (s,a ))+0Ã—(Q (s,a )+Â·Â·Â·+Q (s,a )). In
summarA y,D iE ftX heconvD ee rc genc1 eofT1 âˆ— isguan ranten edinaDec-PO1â€² c MDP,1 tâ€² c hesameguaran nâ€² c teeapnâ€² c pliesto
VD
anADEX-POMDP.
4 MVD
4.1 MultiplicativeInteractionofAsynchronousIndividualQ-values
Aftermigratingasynchronousdecisionstoasingletimestepviaextraagentsandmodelingtheasyn-
chronousproblemasanADEX-POMDP,wecanconsiderthemarginalcontributionofasynchronous
decisionsateachtimestepoftheirexecution. AccordingtotheunifiedframeworkofgeneralVD
algorithms,theglobalQ-valueQ withinanADEX-POMDPcanbeexpressedasageneralformula
tot
intermsoftheindividualutilitiesQ asfollows:
i
Q =Q (s,Q ,Â·Â·Â· ,Q ,Q ,Â·Â·Â· ,Q ,Q ,Â·Â·Â· ,Q ). (4)
tot tot 1d nd 1c nc 1â€²
c
nâ€²
c
Different from (1), where agents execute actions synchronously according to Q , agents in the
i
asynchronoussettingmustaccountfortheinfluenceofcurrentlyexecutingactionsandthepotential
impact of their own decisions during execution. This mutual influence is represented in (4) as
the interaction between Q and Q . To capture this interplay, we enrich (1) by introducing the
id iâ€²
c
multiplicative interaction between them and propose the following general Multiplicative Value
Decomposition(MVD)formula:
n+nâ€²
(cid:88)c (cid:88)
Q =k + k Q + k Q Q . (5)
tot 0 i i idiâ€²
c
id iâ€²
c
i id,iâ€²
c
Infact,wecanperformaTaylorexpansionofQ nearanoptimaljointactiontotheoreticallyderive
i
(5),providingtheoreticalsupportforMVD.ThedetailedderivationsareprovidedinAppendixC.1.
Comparedtosolving(4)withtheadditiveinteractionVD,thestraightforwardmultiplicativeinter-
action between Q and Q in our MVD unexpectedly and significantly boosts representational
id iâ€²
c
capabilities in learning agent interactions. Intuitively, for (1), the obtained gradient to update
Q
âˆ‚ âˆ‚Q
Qi
t
ii
o
â€²
cs
t
âˆ‚ =âˆ‚Q Qt ko it
iâ€²
c= +(cid:80)k i. idI kn idc
iâ€²
co Qnt ir da ,st a, nt dhe
âˆ‚
âˆ‚Qg Qr
t
ioa ctdi =ent kd ice .ri Tve hd erf er fo om re,(5 M) Var De iâˆ‚ âˆ‚ nQ Q tet i go dt rat= esk thid e+ no(cid:80) nlii nâ€² c ek ai rdi aâ€² c gQ ei nâ€² c t,
interactionsascontextualinformationduringupdatesandenablesbothQ andQ torefinetheir
id iâ€²
c
policiesbasedontheirmutualinfluence. Theoretically,asshowninTheorem2,weprovethatMVD
bearsadvantagesinhandlingasynchronoustasksovertraditionalVD.
1AlthoughADEX-POMDPinvolvesn+nâ€² agents,thejointpolicycanstillbedenotedbyÏ€fromDec-
c
POMDPwithnagents,astheextraagentiâ€² shareitspolicywiththecorrespondingagenti .
c c
5Theorem2. LetQ denotethefunctionclassoftheadditiveinteractionVDas(1),andletQ
Add Mul
denotethefunctionclassofthemultiplicativeinteractionVDas(5),thenwehaveQ âŠŠQ .
Add Mul
Proof. Bycomparing(1)oftheadditiveinteractiveVDwith(5)ofthemultiplicativeinteractiveVD,
weclearlyseethatQ âŠ‚ Q . Therefore,theremainingpartistoprovethestrictnessofthis
Add Mul
inclusion.
Weconsideratwo-playerasynchronousdecision-makinggamewheretherow ğ’‚ ğ’ƒ
agent makes the first move, choosing either action a or b, followed by the
columnagentwhocanalsoselectactionaorb. Thefinalrewardisdetermined ğ’‚ ğ’‚ğ’‚ ğ’‚ğ’ƒ
by multiplying the values of the chosen actions, and the reward matrix is
ğ’ƒ ğ’‚ğ’ƒ ğ’ƒğ’ƒ
illustratedinFigure3. FormultiplicativeinteractiveVD,wesettheindividual
utilityasanidentitymappingQ (a )=a andletk =k =k =0,k =1
i i i 0 1 2 12
Figure 3: Example
tolearnthegroundtruthrewardfunction. However,foradditiveinteractiveVD,itisevidentthatit
payoffmatrix.
cannotlearntherelationshipofmultiplyingdifferentactionvalues.
4.2 High-OrderInteractionofAsynchronousIndividualQ-values
Withmultiplicativeinteraction,wecontinuetoexplorehigh-orderinteractionsamongasynchronous
decisions. (5)primarilyconsidersthemutualinfluenceofQ andQ . However,asillustratedin
id iâ€²
c
Figure2,thedecisionsa0 anda1 re-selectedbyextraagents#1â€² and#3â€² att actuallyoriginate
1 3 2
fromdifferenttimesteps,implyinghigh-orderinteractionsamongQ ,Q ,andQ . Assuch,we
1â€² 3â€² 2
proposeaKth-order(where1â‰¤K â‰¤n)interactiveVDasfollows:
n+nâ€²
(cid:88)c (cid:88) (cid:88)
Q =k + k Q + k Q Q +Â·Â·Â·+ k Q Q Â·Â·Â·Q .
tot 0 i i idiâ€²
c
id iâ€²
c
idiâ€² c,1Â·Â·Â·iâ€²
c,Kâˆ’1
id iâ€²
c,1
iâ€²
c,Kâˆ’1
i id,iâ€²
c
id,iâ€² c,1,Â·Â·Â·,iâ€²
c,Kâˆ’1
(6)
Thedetailedderivationof(6)canbefoundinAppendixC.2.
Nevertheless, as the order increases, the deep interaction information between agents does not
provide significant benefits [23, 26]. Therefore, this paper primarily focuses on multiplicative
interactionsbetweenQ andQ . Wefurtherdiscussthepracticalperformancecomparisonbetween
id iâ€²
c
multiplicativeinteractionsandhigh-orderinteractionsinSection5.3.
4.3 Implementation
Finally,wediscusstheissueofIGMconsistencyinthepracticalimplementationofMVD.InADEX-
POMDP,theagenti currentlyexecutinganactiondoesnotneedtochooseanewone,andextra
c
agentsiâ€² canonlyexecutetheasynchronousdecisionsofi . Thisimpliesthati andiâ€² donotneed
c c c c
tosatisfytheIGMcondition. Consequently,weobtaintheMVD-basedIGMasfollows:
argmaxQ (s,a)=(argmaxQ (Ï„ ,a ),Â·Â·Â· ,argmaxQ (Ï„ ,a ),
a
tot
a1d
1d 1d 1d
and
nd nd nd
(7)
a ,Â·Â·Â· ,a ,a ,Â·Â·Â· ,a ).
1c nc 1â€²
c
nâ€²
c
Toachieves(7),weneedtomaintainthemonotonicitybetweenQ andQ ,i.e., âˆ‚Qtot = k +
(cid:80) k Q â‰¥ 0.
Therefore,duringtraining,wekeeptrackoftht eot minimui md valueâˆ‚ QQi
md in
ofi Qd
iâ€² c idiâ€² c iâ€² c c iâ€² c
andQmincanberegardedasaconstantspecifictothetask. Weemployhypernetworks[27]f (s)to
c i
approximatetheweightsin(5),resultinginthefollowingformthatsatisfiesMVD-basedIGM:
Q â‰ˆf +n (cid:88)+nâ€² c |f |Q +(cid:88) |f |Q iâ€² c +Qm c in Q . (8)
tot 0 i i idiâ€² c 2 id
i id,iâ€²
c
Detailedderivationof(8)isprovidedinAppendixC.3.
TheoverallframeworkofMVDisillustratedinFigure4. Weproposethreedistinctpracticalimple-
mentationsofthemixingnetwork. Thefirstapproachdirectlyutilizes(8)toobtainQ . Further,
tot
6ğ‘„
ğ‘¡ğ‘œğ‘¡
ğ‘„
ğ‘¡ğ‘œğ‘¡
ğ‘„
ğ‘¡ğ‘œğ‘¡
ğ‘„ ğ‘¡ğ‘œğ‘¡(ğ‰,ğ’‚)
Softmax MLP Mixing Network ğ‘ 
ğ‘„
ğ‘–
ğ‘ ğ‘ 
ğ‘„ (ğœ ,ğ‘ ) ğ‘„ (ğœ ,ğ‘ ) ğ‘„ (ğœ ,ğ‘ )
|âˆ™| 1ğ‘‘ 1ğ‘‘ 1ğ‘‘ 1ğ‘ 1ğ‘ 1ğ‘ 1 ğ’„â€² 1 ğ‘â€² 1 ğ‘â€² MLP
ğ‘¸Î¤ğ’˜ â„ğ‘¡âˆ’1 â„ğ‘¡
ğ‘– ğ‘–
ğ‘¸ ğ‘‘Î¤ğ•ğ‘¸ ğ‘â€² |âˆ™| AgenA t g ğŸe ğ’…nt ğ’ ğ’… AgenA t g ğŸe ğ’„nt ğ’ ğ’„ AgenA t g ğŸe ğ’„n â€²t ğ’ ğ’„â€² GRU
â‹¯ MLP
ğ‘¸ ğ‘‘ ğ‘¸ ğ‘ ğ‘¸ ğ‘â€² (ğ‘œ 1ğ‘¡ ğ‘‘,ğ‘ 1ğ‘¡âˆ’ ğ‘‘1) (ğ‘œ 1ğ‘¡ ğ‘,ğ‘ 1ğ‘¡âˆ’ ğ‘1) (ğ‘œ 1ğ‘¡ ğ’„â€²,ğ‘ 1ğ‘¡âˆ’ ğ‘â€²1) (ğ‘œ ğ‘–ğ‘¡,ğ‘ ğ‘–ğ‘¡âˆ’1)
Figure4: TheoverallframeworkofMVD.Left: Mixingnetworkstructure. Inredarethehyper-
networks that generate the weights and biases for mixing network. Middle: The overall MVD
architecture. Right: Agentnetworkstructure.
weemployamulti-headstructurethatallowsthemixingnetworktofocusonasynchronousinterac-
tioninformationfromdifferentrepresentationsub-spaces,therebyenhancingtherepresentational
capabilityandstability. Hence,forthesecondimplementation,weoutputtheheadQ-valueQh
tot
fromdifferentheads,whichareprocessedthroughaSoftmaxfunctiontoobtainthefinalQ . To
tot
simplifymodelimplementation,thethirdapproachapproximatestheSoftmaxusinganMLPwith
nonlinearactivationfunctions. Inthispaper,weprimarilyfocusonthethirdway. Wefurtherdiscuss
thedifferentimplementationsinSection5.3. Thepseudo-codeforMVDisinAppendixD.
5 Experiments
Inthissection,weevaluateourMVDacrossvariousscenariosintwoasynchronousdecision-making
benchmarks,OvercookedandPOAC.Thebaselinesfallintothreecategories: 1)Thedecentralized
traininganddecentralizedexecution(DTDE)method,IPPO[28],whereagentstreatotheragents
aspartoftheenvironment,makingitsuitableforasynchronousdecision-makingscenarios. 2)The
discarding type method, MAC IAICC. Note that we do not select CAAC because its code is not
open-source and its application is limited to bus holding control. 3) The padding type method,
includingVDN,QMIX,Qatten,SHAQ[29],andNA2Qthatconsiders2nd-orderinteractions. The
implementationdetailsofthebenchmarks,allbaselines,andourMVDareprovidedinAppendixE.
Thegraphsillustratetheperformanceofallcomparedalgorithmsbyplottingthemeanandstandard
deviationofresultsobtainedacrossfiverandomseeds.
5.1 PerformanceonOvercooked
WefirstruntheexperimentsontheOvercookedbenchmark,wherethreeagentscooperatetoprepare
aTomato-Lettuce-Onionsaladandpromptlyserveitatthecounter. Theymustlearntosequentially
gather raw vegetables, chop them, and merge them into a plate before delivering. Each action,
includingchopping,movingtotheobservedingredients,anddelivering,carriesadifferenttimecost.
Successfullyservingthecorrectdishearnsapositivereward,whilemistakesleadtonegativeones.
ThedetailsofthisbenchmarkcanbefoundinAppendixE.1.
Figure5showstheperformancecomparisonagainstbaselines
on map A of this benchmark. The results indicate that our
Map A
MVDsurpassesallbaselines. BothIPPOandMACIAICCex-
MVD
hibitslowertrainingspeeds. Thissuggeststhediscardingtype 200 VDN
QMIX
methods suffer from low efficiency. NA2Q and SHAQ mis- Qatten
150 SHAQ
takenlyconsidertheinfluenceamongpaddingactions,result- NA2Q
inginnon-convergence. ThisimpliesthatDec-POMDPwith 100 IPPO
MAC IAICC
paddingactionisalsounsuitableforasynchronousdecision- 50
making scenarios, either. QMIX and Qatten perform better
thanNA2Qbecausetheyusesimplermodelstohandlecredit 0
assignment,leadingtostrongerrobustnesstopaddingaction.
0.0 0.2 0.4 0.6 0.8 1.0
T (mil)
Figure5: Meantestreturnonmap
7
AofOvercookedbenchmark.
nruteR
tseT
naeMTheslowtrainingspeedofVDNisduetoitsneglectofthe
marginal contribution of agents. Furthermore, we discover
that MVD outperforms other baselines on various maps of
Overcooked. Thecompleteexperimentalresultsandanalysis
areinAppendixF.
5.2 PerformanceonPOAC
WefurtherconductexperimentsonthemorechallengingPOACbenchmark. POACisaconfrontation
wargamebetweentwoarmies,witheacharmyconsistingofthreediverseagents: infantry,chariot,
andtank. Theseagentspossessdistinctattributesanddifferentactionexecutiontimes. Ourobjective
istolearnasynchronouscooperationstrategiestodefeatthebuilt-inrule-basedbots. Thedetailsof
thisbenchmarkcanbefoundinAppendixE.2.
MVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Scenario 3 Scenario 4 Scenario 5
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Figure6: Testwinrate%onthreechallengingscenariosofPOACbenchmark.
ThewinrateofthePOACbenchmarkwithincreasingdifficultiesisshowninFigure6. Weobserve
that MVD demonstrates increasingly better performance. IPPO and MAC IAICC learn slowly
in simple scenarios but struggle to train in complex tasks. Compared to Overcooked, only the
movementactionsinPOACrequiremultipletimesteps,introducinglesspaddingactionwhenusing
Dec-POMDP. Therefore, NA2Q performs relatively well among the baselines. However, due to
interferencefrompaddingactionandthecomplexityoftheadoptedmodel,thetrainingefficiencyof
NA2QisconsistentlyinferiortothatofMVD,andthesameappliestoSHAQ.Theadditiveinteraction
VDalgorithms,includingVDN,QMIX,andQatten,donotyieldsatisfactoryperformance,sincethey
cannothandlethemutualinfluenceamongagents. Furthermore,wecanfindthatMVDdemonstrates
highlycompetitiveperformanceinotherscenariosofPOAC.Thecompleteexperimentalresultsand
analysiscanbefoundinAppendixG.
5.3 AblationStudy
ToobtainadeeperinsightintoourproposedADEX-POMDPandMVD,weperformablationstudies
toillustratetheimpactofthefollowingfactorsontheperformance: 1)Theintroductionofextra
agentsinADEX-POMDP.2)Theinteractionsofdifferentordersamongagents. 3)Distinctpractical
implementationsofMVD.
Tostudyfactor1), weextendQMIXandQattentoADEX-POMDP,denotingthemasQMIX(A)
andQatten(A).AsshowninFigure7a,theintroductionofextraagentsnotablyimprovestheper-
formanceofbothQMIXandQatten,highlightingthepowerfuladvantagesofADEX-POMDPin
an asynchronous setting. However, these additive interaction VD algorithms fail to capture the
mutualinfluenceamongagents,leadingtopoorerperformancethanMVD.Wealsoextendother
VDalgorithmstoADEX-POMDP.Thecompleteablationexperimentsandanalysiscanbefoundin
AppendixH.1.
Tostudyfactor2),weconsiderMVDandNA2Qwithlth-orderinteractionsunderADEX-POMDP,
denoting them as MVD(l) and NA2Q(l). As shown in Figure 7b, the performance of MVD(2)
incorporating multiplicative interactions consistently outperforms MVD(1) which only involves
8
%
etaR
niW
tseT
%
etaR
niW
tseT
%
etaR
niW
tseTScenario 5 Scenario 5 Scenario 5
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
MVD MVD(2)
QMIX MVD(1)
0.2 QMIX(A) 0.2 MVD(3) 0.2 MVD(MLP)
Qatten NA2Q(2) MVD
0.0 Qatten(A) 0.0 NA2Q(3) 0.0 MVD(Softmax)
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
(a)Extraagents (b)Differentordersinteractions (c)Differentimplementations
Figure7: AblationStudiesofMVDonscenario5ofPOACbenchmark.
additiveinteractions. Further,neitherMVD(3)norNA2Q(3)experiencesanimprovementbyutilizing
higher-orderinteraction. ThisimpliesthatthestraightforwardmultiplicativeinteractionbetweenQ
id
andQ issufficienttoefficientlysolvetheasynchronouscreditassignmentproblem. Thecomplete
iâ€²
c
ablationexperimentsinotherscenariosofPOACandanalysiscanbefoundinAppendixH.2.
Tostudyfactor3),wecomparedthreedifferentpracticalimplementationsofMVD.Asshownin
Figure7c,directlyapplying(8)toobtainQ canconvergetotheoptimaljointpolicy,yetitsuffers
tot
fromslowtrainingspeedandinstability. Employingamulti-headstructurecaneffectivelyaddress
thesetwoshortcomings. However,combiningSoftmaxwithmultiplicativeinteractionscomplicates
theentiremixingnetworkexcessively. Therefore,MVDderivesthegreatestbenefitfromMLPwith
nonlinearactivationfunctions. ThecompleteablationexperimentsinotherscenariosofPOACand
analysisarepresentedinAppendixH.3.
5.4 InterpretabilityofMVD
Tovisuallyillustratetheasynchronouscreditassignmentprocess,we
exhibitkeyframesfromscenario5ofPOACandcomparethecon-
1 1 Tank
vergedMVDandNA2QonADEX-POMDP.Wehighlighttheindivid- 2 2 Chariot
ualQ andcrucialweightswithinthemixingnetworktodemonstrate 3 Infantry
i
theiralignmentwithagentsâ€™asynchronousbehaviors. Figure8shows ğ’Œğ’Š ğ‘¸ğ’Š
ahiddentankandinfantry-chariotcooperation. Theinfantrysuccess- 1 0.955 -0.733 3
2 3.678 0.055 ğ’Œğ’Šğ’‹
fullyattacksanenemy,resultinginhighQ 3andcreditk 3.Meanwhile, 3 1.676 0.785 1 2â€™ 0.310
thechariotisperformingamovementactiontoluretheenemydeeper, 2â€™ 1.122 -0.182 3 2â€™ 0.711
butsinceitisunderattack,Q 2â€² isnegative. Thus,theadditiveinterac- Figure 8: Visualization of
tionfailstoexplainthemutualinfluencebetweenthem. Fortunately, evaluationforMVD.
MVDaccuratelyattributestheimportanceoftheircooperationtothe
2 3 1 Tank
wholeusingmultiplicativeinteractionandassignsahighercreditk 32â€². 2 Chariot
Figure9showsasimilarcaseofNA2Q.Theattackinginfantryhas 3 Infantry
1
ğ‘¸ğ’Š ğ’‡ğ’Š
ahigherQ 3,whereasthekitingtankhasalowerQ 1â€². Boththetank 1 0.466 0.097
andthechariotareexecutingactions,yetNA2Qmistakenlyregards ğ’‡ğ’Šğ’‹ 2
3
0 1. .7 69 32
8
2 2. .2 17 28
2
theasynchronousinfantry-chariotcooperationasmoreimportantthan 3 1â€™ 0.795 1â€™ 0.522 0.082
infantry-tankcooperation. Therefore,eventhoughbothstrategiesul- 3 2â€™ 3.614 2â€™ 0.242 0.079
timatelyachievevictory,MVDoffersasuperiorabilitytocapturethe Figure 9: Visualization of
interplaybetweenagentsâ€™asynchronousdecision-making,providing evaluationforNA2Q.
higherinterpretability.
6 Conclusion
Inthispaper,weproposeanasynchronouscreditassignmentframeworkconsistingoftheADEX-
POMDP problem model and MVD algorithm, providing a solid basis for further exploration of
asynchronouscreditassignmentproblems. ADEX-POMDPsynchronizesasynchronousdecisions
forenhancedprocessing,withoutdisruptingtaskequilibriumorVDconvergence. MVDintroduces
multiplicativeinteractions,strictlyextendingthefunctionclassandefficientlycapturingtheinter-
playbetweenasynchronousdecisions. ExperimentalresultsindicatethatMVDmaintainssuperior
9
%
etaR
niW
tseT
%
etaR
niW
tseT
%
etaR
niW
tseTperformanceacrossvariousasynchronousscenariosandprovidesinterpretabilityforasynchronous
cooperation. Inaddition, ablationstudiesrevealtheneedforcarefulselectionoftheinteractions
betweenagentsandthoughtfuldesignofmixingnetworkstructurestoavoidexcessivemodelcom-
plexity. Therefore,apotentialfutureresearchdirectionistorelaxtheseconstraintstofurtherenhance
therepresentationalcapabilitiesforhandlingmorecomplexasynchronouscooperationtasks.
References
[1] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,
MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel.
Value-decomposition networks for cooperative multi-agent learning based on team reward.
InProceedingsofthe17thInternationalConferenceonAutonomousAgentsandMultiAgent
Systems,page2085â€“2087,2018.
[2] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,Jakob
Foerster,andShimonWhiteson. Monotonicvaluefunctionfactorisationfordeepmulti-agent
reinforcementlearning. JournalofMachineLearningResearch,21(178):1â€“51,2020.
[3] YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyao
Tang. Qatten: Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXiv
preprintarXiv:2002.03939,2020.
[4] KaiArulkumaran,AntoineCully,andJulianTogelius. Alphastar: Anevolutionarycomputation
perspective. InProceedingsofthegeneticandevolutionarycomputationconferencecompanion,
pages314â€“315,2019.
[5] BRaviKiran,IbrahimSobh,VictorTalpaert,PatrickMannion,AhmadAAlSallab,Senthil
Yogamani,andPatrickPÃ©rez. Deepreinforcementlearningforautonomousdriving: Asurvey.
IEEETransactionsonIntelligentTransportationSystems,23(6):4909â€“4926,2021.
[6] YananWang,TongXu,XinNiu,ChangTan,EnhongChen,andHuiXiong. Stmarl: Aspatio-
temporalmulti-agentreinforcementlearningapproachforcooperativetrafficlightcontrol. IEEE
TransactionsonMobileComputing,21(6):2228â€“2242,2020.
[7] XiaoboZhou,ZhihuiKe,andTieQiu. Recommendation-drivenmulti-cellcooperativecaching:
A multi-agent reinforcement learning approach. IEEE Transactions on Mobile Computing,
2023.
[8] FransAOliehoek,ChristopherAmato,etal. AconciseintroductiontodecentralizedPOMDPs,
volume1. Springer,2016.
[9] YonghengLiang,HejunWu,andHaitaoWang. ASM-PPO:Asynchronousandscalablemulti-
agentppoforcooperativecharging. InProceedingsofthe21stInternationalConferenceon
AutonomousAgentsandMultiagentSystems,pages798â€“806,2022.
[10] JiaweiWangandLijunSun. Reducingbusbunchingwithasynchronousmulti-agentreinforce-
ment learning. In Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence,,pages426â€“433,2021.
[11] YuchenXiao,WeihaoTan,andChristopherAmato. Asynchronousactor-criticformulti-agent
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems,35:4385â€“4400,
2022.
[12] Yongheng Liang, Hejun Wu, and Haitao Wang. Asynchronous multi-agent reinforcement
learning for collaborative partial charging in wireless rechargeable sensor networks. IEEE
TransactionsonMobileComputing,2023.
[13] Yuxin Chen, Hejun Wu, Yongheng Liang, and Guoming Lai. Varlenmarl: A framework of
variable-lengthtime-stepmulti-agentreinforcementlearningforcooperativecharginginsensor
networks. In202118thAnnualIEEEInternationalConferenceonSensing,Communication,
andNetworking,pages1â€“9.IEEE,2021.
10[14] HangtianJia,YujingHu,YingfengChen,ChunxuRen,TangjieLv,ChangjieFan,andChongjie
Zhang. Feverbasketball: Acomplex,flexible,andasynchronizedsportsgameenvironmentfor
multi-agentreinforcementlearning. arXivpreprintarXiv:2012.03204,2020.
[15] AfshinOroojlooyandDavoodHajinezhad. Areviewofcooperativemulti-agentdeepreinforce-
mentlearning. AppliedIntelligence,53(11):13677â€“13722,2023.
[16] CarlosFDaganzoandYanfengOuyang. Publictransportationsystems: Principlesofsystem
design,operationsplanningandreal-timecontrol. WorldScientific,2019.
[17] DavidE.RumelhartandJamesL.McClelland. AGeneralFrameworkforParallelDistributed
Processing,pages45â€“76. 1987.
[18] SiddhantM.Jayakumar,WojciechM.Czarnecki,JacobMenick,JonathanSchwarz,JackRae,
SimonOsindero,YeeWhyeTeh,TimHarley,andRazvanPascanu. Multiplicativeinteractions
andwheretofindthem. InInternationalConferenceonLearningRepresentations,2020.
[19] RoseEWang,SarahAWu,JamesAEvans,JoshuaBTenenbaum,DavidCParkes,andMax
Kleiman-Weiner. Toomanycooks: Coordinatingmulti-agentcollaborationthroughinverse
planning. 2020.
[20] MengYao,QiyueYin,JunYang,TongtongYu,ShengqiShen,JungeZhang,BinLiang,and
KaiqiHuang. Thepartiallyobservableasynchronousmulti-agentcooperationchallenge. arXiv
preprintarXiv:2112.03809,2021.
[21] FransAOliehoek,MatthijsTJSpaan,andNikosVlassis. Optimalandapproximateq-value
functionsfordecentralizedpomdps. JournalofArtificialIntelligenceResearch,32:289â€“353,
2008.
[22] Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Changjie Fan, Fei Wu, and
Jun Xiao. Deconfounded value decomposition for multi-agent reinforcement learning. In
InternationalConferenceonMachineLearning,pages12843â€“12856.PMLR,2022.
[23] ZichuanLiu,YuanyangZhu,andChunlinChen. NA2Q:Neuralattentionadditivemodelfor
interpretablemulti-agentq-learning. InInternationalConferenceonMachineLearning,pages
22539â€“22558.PMLR,2023.
[24] Trevor J Hastie. Generalized additive models. In Statistical models in S, pages 249â€“307.
Routledge,2017.
[25] KyunghwanSon, DaewooKim, WanJuKang, DavidEarlHostallero, andYungYi. Qtran:
Learningtofactorizewithtransformationforcooperativemulti-agentreinforcementlearning.
InInternationalconferenceonmachinelearning,pages5887â€“5896.PMLR,2019.
[26] YingWen,YaodongYang,RuiLuo,JunWang,andWeiPan. Probabilisticrecursivereasoning
formulti-agentreinforcementlearning. InInternationalConferenceonLearningRepresenta-
tions,2019.
[27] DavidHa,AndrewM.Dai,andQuocV.Le. Hypernetworks. InInternationalConferenceon
LearningRepresentations,2017.
[28] ChristianSchroederDeWitt,TarunGupta,DenysMakoviichuk,ViktorMakoviychuk,PhilipHS
Torr,MingfeiSun,andShimonWhiteson. Isindependentlearningallyouneedinthestarcraft
multi-agentchallenge? arXivpreprintarXiv:2011.09533,2020.
[29] JianhongWang, YuanZhang, YunjieGu, andTae-KyunKim. Shaq: Incorporatingshapley
valuetheoryintomulti-agentq-learning. AdvancesinNeuralInformationProcessingSystems,
35:5941â€“5954,2022.
[30] MengZhou,ZiyuLiu,PengweiSui,YixuanLi,andYukYingChung. Learningimplicitcredit
assignmentforcooperativemulti-agentreinforcementlearning. Advancesinneuralinformation
processingsystems,33:11853â€“11864,2020.
[31] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.QPLEX:Duplexdueling
multi-agentq-learning. InInternationalConferenceonLearningRepresentations,2021.
11[32] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas.
Duelingnetworkarchitecturesfordeepreinforcementlearning. InInternationalconferenceon
machinelearning,pages1995â€“2003.PMLR,2016.
[33] David H Wolpert and Kagan Tumer. Optimal payoff functions for members of collectives.
AdvancesinComplexSystems,4(02n03):265â€“279,2001.
[34] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume32,2018.
[35] Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu,
ChangjieFan,andZhongyuWei.Q-valuepathdecompositionfordeepmultiagentreinforcement
learning. InInternationalConferenceonMachineLearning,pages10706â€“10715.PMLR,2020.
[36] MukundSundararajan,AnkurTaly,andQiqiYan. Axiomaticattributionfordeepnetworks. In
Internationalconferenceonmachinelearning,pages3319â€“3328.PMLR,2017.
[37] JianhongWang,YuanZhang,Tae-KyunKim,andYunjieGu. Shapleyq-value: Alocalreward
approachtosolveglobalrewardgames. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pages7285â€“7292,2020.
[38] LloydSShapley. Avalueforn-persongames. 1953.
[39] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu.
The surprising effectiveness of ppo in cooperative multi-agent games. Advances in Neural
InformationProcessingSystems,35:24611â€“24624,2022.
[40] GeorgiosTheocharousandLeslieKaelbling. Approximateplanninginpomdpswithmacro-
actions. Advancesinneuralinformationprocessingsystems,16,2003.
[41] YiyuanLee,PanpanCai,andDavidHsu. Magic: Learningmacro-actionsforonlinepomdp
planningusinggenerator-critic. InRobotics: ScienceandSystemsConference,2021.
[42] AmatoChristopher,DKonidarisGeorge,andPKaelblingLeslie. Planningwithmacro-actions
indecentralizedpomdps. InInternationalConferenceonAutonomousAgentsandMultiagent
Systems,2014.
[43] ChristopherAmato,GeorgeKonidaris,LesliePKaelbling,andJonathanPHow. Modeling
andplanningwithmacro-actionsindecentralizedpomdps. JournalofArtificialIntelligence
Research,64:817â€“859,2019.
[44] Yuchen Xiao, Joshua Hoffman, Tian Xia, and Christopher Amato. Learning multi-robot
decentralizedmacro-action-basedpoliciesviaacentralizedq-net. In2020IEEEInternational
conferenceonroboticsandautomation,pages10695â€“10701.IEEE,2020.
[45] TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson.WeightedQMIX:expanding
monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning.InAdvances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
ProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual,2020.
12A RelatedWork
A.1 CreditAssignmentMethods
Thecentralizedtraininganddecentralizedexecution(CTDE)[21]frameworkiswidelyadoptedby
modernMARLmethods. Itleveragesglobalinformationforcentralizedtraining,enablingagents
tomakedecentralizeddecisionsbasedontheirlocalinformation. AkeychallengeinCTDEisthe
creditassignmentproblem,whichinvestigatesagentsâ€™marginalcontributionstotheoverallsuccess,
facilitatingbetterlearningofagentpoliciesandenhancingcooperationamongagents[15].
Valuedecomposition(VD)standsasoneofthemostprevalentimplicitcreditassignmentmethods[30],
whichlearnsthemarginalcontributionofeachagentanddecomposestheglobalQ-valuefunction
Q intoindividualagentutilityfunctionsQ . VDN[1]directlyassumesequalcontributionsfrom
tot i
allagentsanddecomposesQ intothesumofQ s. QMIXutilizesamixingnetworktolearnthe
tot i
nonlinearrelationshipbetweenQ andQ . SinceQMIXconstructsthemixingnetworkusingMLP
tot i
layers,westillconsiderQMIXastheadditiveinteractionform[18]. QPLEX[31]introducesthe
duelingstructureQ=V +A[32]andperformsVDontheglobalstatevaluefunctionV aswell
tot
as the global advantage value function A , respectively. Qatten theoretically derives a general
tot
additiveinteractionformulaofQ intermsofQ andproposespracticalmulti-headattentionto
tot i
approximatetheweightofQ . DVD[22]employsbackdooradjustmenttoremovetheconfounding
i
biasinVD,aimingtoobtainbetterweightsofQ . Insummary,theVDalgorithmsmentionedabove
i
canbeformulatedastheadditiveinteractionformas(1),whichleadstolimitationsinaddressing
theunderlyingdependenciesamongagents[2,23]. Therefore,NA2Q[23]introducesageneralized
additive model (GAM) [24] to model all possible higher-order interactions among agents and to
interprettheircollaborationbehavior.
Ontheotherhand,explicitcreditassignmentmethods[30]introducedifferentconceptstoexplicitly
calculatethemarginalcontributionofeachagent. Inspiredbydifferencerewards[33],COMA[34]
proposesacounterfactualbaselinetocalculatethespecificcriticnetworkforeachagent. QPD[35]
employsintegratedgradients[36]toattributetheQ toeachagentâ€™sfeaturechanges. SQPPDG
tot
[37]andSHAQ[29]extendshapleyvalue[38]forthecreditassignmentinanextendedconvexgame
withagrandcoalition. However,comparedwithexplicitcreditassignment,implicitmethodsoffer
superiorgeneralizationcapabilitiesandcanautonomouslylearnthemarginalcontributionsofagents
[30]. Therefore,inthispaper,wechoosetoextendtheimplicitcreditassignmentmethod,VD,tothe
asynchronousdomain.
A.2 AsynchronousMARL
Despite the significant progress in MARL, most existing MARL works rely on the premise of
synchronous decision-making which doesnâ€™t reflect reality in many practical applications. The
simplest approach to relax the synchronous decision-making constraint in MARL and extend it
to the asynchronous domain is to split asynchronous actions spanning multiple time steps into
several sub-actions or to wait for all agents to complete their actions before making decisions
simultaneously. Evidently,thesemethodssignificantlyincreasetrainingcostsanddecreasetraining
efficiency. Therefore, several works have been conducted to exploit the strengths of MARL in
asynchronoussettings. AsmentionedinSection1,theseworkscanbroadlybecategorizedintotwo
typesofmethodsasdepictedindetailinthefollowing.
Thediscardingtypemethodsrecognizeasynchronousactionswithvaryingdurationsasawholeand
focussolelyontheinformationatdecisiontimesteps. ASM-PPO[9]andASM-HPPO[12]propose
thateachagentcollectsitsowndecisioninformationandutilizesMAPPO[39]fortraining. MAC
IAICC[11]adoptsasimilarapproachtoASM-PPOandASM-HPPO,treatingasynchronousactions
asmacro-actions[40,41]andstrictlymodelingthetaskasaMacDec-POMDP[42,43]. CAAC[10]
focusesonthebusholdingcontrol[16]andutilizesagraphattentionneuralnetworktocapturethe
influenceofotheragentsâ€™decisionsduringasynchronousactionexecution.
Thepaddingtypemethodsconvertasynchronousproblemsintosynchronousonesbyusingpadding
action,therebyallowingforthedirectapplicationofexistingMARLmethods. VarLenMARL[13]
employsthemostrecentactionforpadding. EXP-Ms[14]proposestomaskongoingactionsand
treat them as idle during the collection of joint transitions. Parallel-MacDec-MADDRQN [44]
introducesaparallelway. Inoneenvironment,itcollectssynchronousdatausingpaddingactionand
13trainsacentralizedQ-network. Inanotherenvironment,itcollectsasynchronousdataandtrainsa
decentralizedQ-networkwiththehelpofthecentralizedQ-network.
Overall,whileasynchronousMARLhasmadenotableprogress,thereremainsalackofappropriate
modelsforasynchronousdecision-makingscenarios,aswellastheoreticalandvisualanalysesof
asynchronouscreditassignment. Further, Sinceonlyconsideringasynchronousdataexacerbates
issues of environmental non-stationarity and asynchronous credit assignment, in this paper, we
proposeageneralasynchronousmodelbasedonthesynchronousdata.
B ProofofLemmas
Definition2. AjointpolicyÏ€âˆ— ={Ï€ ,Ï€ }iscalledaMarkovPerfectEquilibrium(MPE)if:
i âˆ’i
VÏ€i,Ï€âˆ’i(s)â‰¥VÏ€Ëœi,Ï€âˆ’i(s), âˆ€sâˆˆS, iâˆˆN, âˆ€Ï€Ëœ .
i
Definition3. TheBellmanoptimalityoperatorTâˆ—forQ-learningvariantsinMARLis:
Tâˆ—Q(s,a)=E [r+Î³maxQ(sâ€²,aâ€²)].
sâ€²âˆ¼P
aâ€²
Definition4. ThefunctionclassQ oftheapproximateglobalQ-valuefunctionQ inVDis:
tot tot
Q ={Q |Q (s,a)=f (Q (Ï„ ,a ),Â·Â·Â· ,Q (Ï„ ,a )), whereQ andQ satisfyIGM}.
tot tot tot s 1 1 1 n n n tot i
Definition5. TheVDoperatorisTâˆ— :=(cid:81) Â·Tâˆ—. Tâˆ—istheBellmanoptimalityoperatortolearn
thegroundtruthglobalQ-valuefuV nD ctionQV âˆ—.D(cid:81)
istheoperatorthatfindstheglobalQ-value
VD
functionQ inthefunctionclassQ toapproximateQâˆ—[45],whichisdefinedas:
tot tot
(cid:89)
Q(s,a)=argmin(Q(s,a)âˆ’q(s,a))2.
VD qâˆˆQtot
Lemma1. Givenanasynchronousdecision-makingtaskandajointpolicyÏ€,letQÏ€ andQÏ€
Dec ADEX
representtheglobalQ-valuefunctionconvergedbymodelingthetaskwithDec-POMDPandADEX-
POMDP,respectively. ThenwehaveQÏ€ (s,a)=QÏ€ (s,a).
Dec ADEX (cid:98) (cid:98)
Proof. WeusethesetX torepresentdifferentsinADEX-POMDPthatcorrespondtothesame
s (cid:98)
originals,andthesetY torepresentdifferent(s,a)thatcorrespondtothesameoriginal(s,a).
(s,a) (cid:98) (cid:98)
AsspecifiedinDefinition1,s=[s;oËœ ],anda=[a;aËœ ]. Foranysandsâ€²,wehaveX âˆ©X =âˆ….
(cid:98) c (cid:98) c s sâ€²
Similarly,wealsohaveY âˆ©Y =âˆ….Therefore,thereisaone-to-onecorrespondencebetween
(s,a) (sâ€²,aâ€²)
sandX s,aswellasbetween(s,a)andY (s,a),whichimpliesthatP(cid:98)(X sâ€²|Y (s,a))â‰¡P(sâ€²|s,a)and
r(Y )â‰¡r(s,a).
(cid:98) (s,a)
GivenajointpolicyÏ€,theglobalQ-valuefunctionconvergedinDec-POMDPisdefinedas:
QÏ€ (s,a)=E [r1+Î³r2+Â·Â·Â·+Î³Tâˆ’1rT].
Dec (s,a)âˆ¼(P,Ï€)
AndtheglobalQ-valuefunctionconvergedinADEX-POMDPisdefinedas:
QÏ€ (s,a)=E [r1+Î³r2+Â·Â·Â·+Î³Tâˆ’1rT]
ADEX (cid:98) (cid:98) (s(cid:98),a(cid:98))âˆ¼(P(cid:98),Ï€) (cid:98) (cid:98) (cid:98)
=E E [r1+Î³r2+Â·Â·Â·+Î³Tâˆ’1rT]
Y(s,a)âˆ¼(P(cid:98),Ï€) (oËœc,aËœc)âˆ¼(OP,Ï€) (cid:98) (cid:98) (cid:98)
=E [E [r1]+Â·Â·Â·+Î³Tâˆ’1E [rT]]
Y(s,a)âˆ¼(P(cid:98),Ï€) (oËœc,aËœc)âˆ¼(OP,Ï€) (cid:98) (oËœc,aT cd)âˆ¼(OP,Ï€) (cid:98)
=E [r1+Î³r2+Â·Â·Â·+Î³Tâˆ’1rT] (9)
Y(s,a)âˆ¼(P(cid:98),Ï€) (cid:98) (cid:98) (cid:98)
=E [r1+Î³r2+Â·Â·Â·+Î³Tâˆ’1rT],
(s,a)âˆ¼(P,Ï€)
where (9) represents the expectation over different cases in Y . However, since r(Y ) â‰¡
(s,a) (cid:98) (s,a)
r(s,a), meaning that different cases in this set correspond to the same reward, the expectation
operationcanberemoved. Finally,weprovethatgiventhesameÏ€,QÏ€ (s,a) = QÏ€ (s,a).
Dec ADEX (cid:98) (cid:98)
Lemma2. Givenanasynchronousdecision-makingtask,wedenotetheBellmanoptimalityoperator
inDec-POMDPasTâˆ— andinADEX-POMDPasTâˆ— . Similarly,Qâˆ— andQâˆ— represent
Dec ADEX Dec ADEX
thegroundtruthglobalQ-valuefunctionswhenmodelingthetaskusingDec-POMDPandADEX-
POMDP,respectively. ThenwehaveQâˆ— (s,a)=Qâˆ— (s,a).
Dec ADEX (cid:98) (cid:98)
14Proof. BasedonLemma1,wehave
Tâˆ— QÏ€ (s,a)=E [r+Î³maxQÏ€ (sâ€²,aâ€²)]
ADEX ADEX (cid:98) (cid:98) s(cid:98)â€²âˆ¼P(cid:98) (cid:98)
a(cid:98)â€²
ADEX (cid:98) (cid:98)
=E E [r+Î³maxQÏ€ (sâ€²,aâ€²)]
X sâ€²âˆ¼P(cid:98) oËœcâˆ¼OP (cid:98)
a(cid:98)â€²
ADEX (cid:98) (cid:98)
=E [r+Î³maxQÏ€ (sâ€²,aâ€²)] (10)
X sâ€²âˆ¼P(cid:98) (cid:98)
a(cid:98)â€²
ADEX (cid:98) (cid:98)
=E [r+Î³maxQÏ€ (sâ€²,aâ€²)]
sâ€²âˆ¼P Dec
aâ€²
=Tâˆ— QÏ€ (s,a),
Dec Dec
where(10)isderivedbasedonthefactthatfordifferents ands inX ,wehaveQÏ€ (s ,a)=
(cid:98)1 (cid:98)2 s ADEX (cid:98)1 (cid:98)
QÏ€ (s ,a) = QÏ€ (s,a), thus allowing us to eliminate the expectation operation. Hence,
ADEX (cid:98)2 (cid:98) Dec
assumingthesameinitialjointpolicyÏ€ ,bothTâˆ— andTâˆ— canconvergetothesameoptimal
0 Dec ADEX
globalQ-valuefunction,implyingQâˆ— (s,a)=Qâˆ— (s,a).
Dec ADEX (cid:98) (cid:98)
C DerivationofMultiplicativeValueDecomposition
C.1 MultiplicativeInteractionForm
Accordingto[3],wecanexpandQ intermsofQ as:
tot i
(cid:88) (cid:88) (cid:88)
Q =c+ Âµ Q + Âµ Q Q +Â·Â·Â·+ Âµ Q Â·Â·Â·Q +Â·Â·Â· , (11)
tot i i ij i j i1Â·Â·Â·ik i1 ik
i ij i1,Â·Â·Â·,ik
where c is a constant and Âµ = 1 âˆ‚kQtot , and then we can expand Q near the optimal
actionaâˆ—as:
i1Â·Â·Â·ik k!âˆ‚Qi1Â·Â·Â·âˆ‚Qik i
i
Q (a )=Î± +Î² (a âˆ’aâˆ—)2+o((a âˆ’aâˆ—))2. (12)
i i i i i i i i
In the ADEX-POMDP setting, we denote the first-order term (cid:80) Âµ Q = (cid:80) Î»d Q +
(cid:80) Î»c Q +(cid:80) Î»câ€² Q . Wethenapply(12)intothesecond-ordi eri teri min(11)i .d Foid r,1 brei vd ity,
ic ic, (cid:80)1 ic iâ€² c iâ€² c,1 iâ€² c (cid:80) (cid:80)
wesplit Âµ Q Q intotwoparts, Âµ Q Q and Âµ Q Q ,representingthe
i,j ij i j id,iâ€² c idiâ€² c id iâ€² c i dÂ¯,icÂ¯ i dÂ¯icÂ¯ i dÂ¯ icÂ¯
interactionsbetweenallpairsofi andiâ€²,andallotherinteractionsin{N,Nâ€²},respectively. For
(cid:80) d c c
Âµ Q Q ,leti âˆˆN ,wehave:
i dÂ¯,icÂ¯ i dÂ¯icÂ¯ i dÂ¯ icÂ¯ d d
(cid:88) (cid:88)
Âµ Q Q = Âµ (Î± +Î² (a âˆ’aâˆ— )2)(Î± +Î² (a âˆ’aâˆ—)2)+o(âˆ¥aâˆ’aâˆ— âˆ¥2)
i dÂ¯icÂ¯ i dÂ¯ icÂ¯ i dÂ¯icÂ¯ i dÂ¯ i dÂ¯ i dÂ¯ i dÂ¯ icÂ¯ icÂ¯ icÂ¯ icÂ¯
i dÂ¯,icÂ¯ i dÂ¯,icÂ¯
(cid:88) (cid:88) (cid:88)
= Âµ Î± Î± + Âµ Î± Î² (a âˆ’aâˆ— )2+ Âµ Î± Î² (a âˆ’aâˆ—)2
i dÂ¯icÂ¯ i dÂ¯ icÂ¯ i dÂ¯icÂ¯ icÂ¯ i dÂ¯ i dÂ¯ i dÂ¯ i dÂ¯icÂ¯ i dÂ¯ icÂ¯ icÂ¯ icÂ¯
i dÂ¯,icÂ¯ i dÂ¯,icÂ¯ i dÂ¯,icÂ¯
+o(âˆ¥aâˆ’aâˆ— âˆ¥2)
(cid:88) (cid:88) (cid:88)
= Âµ Î± Î± + Âµ Î± (Q âˆ’Î± )+ Âµ Î± (Q âˆ’Î± )
i dÂ¯icÂ¯ i dÂ¯ icÂ¯ i dÂ¯icÂ¯ icÂ¯ i dÂ¯ i dÂ¯ i dÂ¯icÂ¯ i dÂ¯ icÂ¯ icÂ¯
i dÂ¯,icÂ¯ i dÂ¯,icÂ¯ i dÂ¯,icÂ¯
+o(âˆ¥aâˆ’aâˆ— âˆ¥2)
(cid:88) (cid:88) (cid:88)
=âˆ’ Âµ Î± Î± + Âµ Î± Q + Âµ Î± Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2)
i dÂ¯icÂ¯ i dÂ¯ icÂ¯ i dÂ¯icÂ¯ icÂ¯ i dÂ¯ i dÂ¯icÂ¯ i dÂ¯ icÂ¯
i dÂ¯,icÂ¯ i dÂ¯,icÂ¯ i dÂ¯,icÂ¯
(cid:88) (cid:88) (cid:88)
=c + Âµ Î± Q + Âµ Î± Q + Âµ Î± Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2)
i dÂ¯icÂ¯ iid i id iiâ€²
c
i iâ€²
c
iic i ic
iâˆˆ/N câ€²,id iâˆˆ/Nd,iâ€²
c
i,ic
=c +(cid:88) Î»d Q +(cid:88) Î»câ€² Q +(cid:88) Î»c Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2).
2 id,2 id iâ€² c,2 iâ€² c ic,2 ic
id iâ€²
c
ic
Therefore,forthesecond-orderterm,weget:
(cid:88) Âµ Q Q =c +(cid:88) Î»d Q +(cid:88) Î»câ€² Q +(cid:88) Î»c Q +(cid:88) Î»dcâ€² Q Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2).
ij i j 2 id,2 id iâ€² c,2 iâ€² c ic,2 ic idiâ€² c,2 id iâ€² c
i,j id iâ€²
c
ic id,iâ€²
c
15(cid:80)
Similarly,wenextconsiderthethird-orderterm Âµ Q Q Q andobtain:
i1,i2,i3 i1i2i3 i1 i2 i3
(cid:88) (cid:88) (cid:88)
Âµ Q Q Q =c + Âµ Î± Î± Q + Âµ Î± Î± Q
i1i2i3 i1 i2 i3 3 i1i2id i1 i2 id i1i2iâ€²
c
i1 i2 iâ€²
c
i1,i2,i3 i1,i2âˆˆ/N câ€²,id i1,i2âˆˆ/Nd,iâ€²
c
(cid:88) (cid:88)
+ Âµ Î± Î± Q + Âµ Î± Î± Q
i1i2ic i1 i2 ic iidiâ€²
c
id iâ€²
c
i
i1âˆˆ/Ndori2âˆˆ/N câ€²,ic i,id,iâ€²
c
(cid:88)
+ Âµ Î± Q Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2)
iidiâ€²
c
i id iâ€²
c
i,id,iâ€²
c
=c +(cid:88) Î»d Q +(cid:88) Î»câ€² Q +(cid:88) Î»c Q +(cid:88) Î» Q
3 id,3 id iâ€² c,3 iâ€² c ic,3 ic dcâ€²,3 i
id iâ€²
c
ic i
+(cid:88) Î»dcâ€² Q Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2).
idiâ€² c,3 id iâ€² c
id,iâ€²
c
Consequently,thegeneralformforthek-thordertermcanbeexpressedas:
(cid:88) Âµ Q Â·Â·Â·Q =c +(cid:88) Î»d Q +(cid:88) Î»câ€² Q +(cid:88) Î»c Q +(cid:88) Î» Q
i1Â·Â·Â·ik i1 ik k id,k id iâ€² c,k iâ€² c ic,k ic dcâ€²,k i
i1,Â·Â·Â·,ik id iâ€²
c
ic i
+(cid:88) Î»dcâ€² Q Q +o(âˆ¥aâˆ’aâˆ— âˆ¥2).
idiâ€² c,k id iâ€² c
id,iâ€²
c
WeuseÂ¯i ,Â·Â·Â· ,Â¯i âˆˆ{N,Nâ€²}torepresentthatanyÂ¯i andÂ¯i donotsimultaneouslysatisfyÂ¯i âˆˆN
1 k c x y x d
andÂ¯i âˆˆNâ€². Sowetake:
y c
(cid:88)
Î»d = Âµ Î± Â·Â·Â·Î± ,
id,k i1Â·Â·Â·ikâˆ’1id i1 ikâˆ’1
i1,Â·Â·Â·,ikâˆ’1âˆˆ/N câ€²
Î»câ€²
=
(cid:88)
Âµ Î± Â·Â·Â·Î± ,
iâ€² c,k i1Â·Â·Â·ikâˆ’1iâ€² c i1 ikâˆ’1
i1,Â·Â·Â·,ikâˆ’1âˆˆ/Nd
(cid:88)
Î»c
ic,k
= ÂµÂ¯i1Â·Â·Â·Â¯ikâˆ’1icÎ±Â¯i1Â·Â·Â·Î±Â¯ikâˆ’1,
Â¯i1,Â·Â·Â·,Â¯ikâˆ’1
(cid:88)
Î» =(kâˆ’2) Âµ Î± Â·Â·Â·Î± Î± Î± ,
dcâ€²,k i1Â·Â·Â·ikâˆ’2idiâ€²
c
i1 ikâˆ’3 id iâ€²
c
i1,Â·Â·Â·,ikâˆ’3,id,iâ€²
c
Î»dcâ€²
=
(cid:88)
Âµ Î± Â·Â·Â·Î± .
idiâ€² c,k i1Â·Â·Â·ikâˆ’2idiâ€² c i1 ikâˆ’2
i1,Â·Â·Â·,ikâˆ’2
Therefore,underADEX-POMDP,wecanobtainthemultiplicativeinteractionvaluedecomposition
formula(5)neartheoptimaljointactionasfollows:
Q â‰ˆ(cid:88) c +(cid:88)(cid:88) (Î»d +Î» )Q +(cid:88)(cid:88) (Î»câ€² +Î» )Q
tot k id,k dcâ€²,k id iâ€² c,k dcâ€²,k iâ€² c
k id k iâ€²
c
k
+(cid:88)(cid:88) (Î»c +Î» )Q +(cid:88)(cid:88) Î»dcâ€² Q Q
ic,k dcâ€²,k ic idiâ€² c,k id iâ€² c
ic k id,iâ€²
c
k
=c+(cid:88) Î»d Q +(cid:88) Î»câ€² Q +(cid:88) Î»c Q +(cid:88) Î»dcâ€² Q Q (13)
id id iâ€² c iâ€² c ic ic id,iâ€² c id iâ€² c
id iâ€²
c
ic id,iâ€²
c
n+nâ€²
=c+
(cid:88)c
Î» Q
+(cid:88) Î»dcâ€²
Q Q .
i i id,iâ€²
c
id iâ€²
c
i id,iâ€²
c
The convergence of Î» just needs mild conditions, like boundedness of Î± or small growth of
i
âˆ‚kQtot
intermsofk.
âˆ‚Qi1Â·Â·Â·âˆ‚Qik
16C.2 High-OrderInteractionForm
Basedonthederivationprocessof(13),wecanfurtherexplorethehigher-orderinteractionformsbe-
tweenagentsi andiâ€²,andobtaintheKth-order(where1â‰¤K â‰¤n)interactionvaluedecomposition
d c
formula(6)asfollows:
(cid:88) (cid:88)(cid:88)
Q â‰ˆ c + (Î»d,K +Î»K +Â·Â·Â·+Î»K )Q
tot k id,k dcâ€²,k dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,k id
k id k
+(cid:88)(cid:88) (Î»câ€²,K +Î»K +Â·Â·Â·+Î»K )Q
iâ€² c,k dcâ€²,k dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,k iâ€² c
iâ€² k
c
(cid:88)(cid:88)
+ (Î»c,K +Î»K +Â·Â·Â·+Î»K )Q
ic,k dcâ€²,k dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,k ic
ic k
+(cid:88)(cid:88) Î»dcâ€²,KQ
Q +Â·Â·Â·+
(cid:88) (cid:88) Î»dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,K
Q Q Â·Â·Â·Q
idiâ€² c,k id iâ€² c idiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1,k id iâ€² c,1 iâ€² c,Kâˆ’1
id,iâ€²
c
k idiâ€² c,1Â·Â·Â·iâ€²
c,Kâˆ’1
k
=c+(cid:88) Î»d,KQ +(cid:88) Î»câ€²,KQ +(cid:88) Î»c,KQ
id id iâ€² c iâ€² c ic ic
id iâ€²
c
ic
+(cid:88) Î»dcâ€²,KQ
Q +Â·Â·Â·+
(cid:88) Î»dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,K
Q Q Â·Â·Â·Q
id,iâ€² c id iâ€² c idiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1 id iâ€² c,1 iâ€² c,Kâˆ’1
id,iâ€²
c
idiâ€² c,1Â·Â·Â·iâ€²
c,Kâˆ’1
n+nâ€²
=c+ (cid:88)c Î»KQ +(cid:88) Î»dcâ€²,KQ Q +Â·Â·Â·+ (cid:88) Î»dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,K Q Q Â·Â·Â·Q ,
i i id,iâ€² c id iâ€² c idiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1 id iâ€² c,1 iâ€² c,Kâˆ’1
i id,iâ€²
c
idiâ€² c,1Â·Â·Â·iâ€²
c,Kâˆ’1
where:
(cid:88)
Î»d,K = Âµ Î± Â·Â·Â·Î± ,
id,k i1Â·Â·Â·ikâˆ’1id i1 ikâˆ’1
i1,Â·Â·Â·,ikâˆ’1âˆˆ/N câ€²
Î»câ€²,K
=
(cid:88)
Âµ Î± Â·Â·Â·Î± ,
iâ€² c,k i1Â·Â·Â·ikâˆ’1iâ€² c i1 ikâˆ’1
i1,Â·Â·Â·,ikâˆ’1âˆˆ/Nd
(cid:88)
Î»c ic,K
,k
= ÂµÂ¯i1Â·Â·Â·Â¯ikâˆ’1icÎ±Â¯i1Â·Â·Â·Î±Â¯ikâˆ’1,
Â¯i1,Â·Â·Â·,Â¯ikâˆ’1
(cid:88)
Î»K =(kâˆ’2) Âµ Î± Â·Â·Â·Î± Î± Î± ,
dcâ€²,k i1Â·Â·Â·ikâˆ’2idiâ€²
c
i1 ikâˆ’3 id iâ€²
c
i1,Â·Â·Â·,ikâˆ’3âˆˆ/N câ€²,id,iâ€²
c
(cid:88)
Î»K =(kâˆ’3) Âµ Î± Â·Â·Â·Î± Î± Î± Î± ,
dcâ€² 1câ€² 2,k i1Â·Â·Â·ikâˆ’3idiâ€² c,1iâ€² c,2 i1 ikâˆ’4 id iâ€² c,1 iâ€² c,2
i1,Â·Â·Â·,ikâˆ’4âˆˆ/N câ€²,id,iâ€² c,1,iâ€²
c,2
Â·Â·Â·
(cid:88)
Î»K =(kâˆ’K) Âµ Î± Â·Â·Â·Î± Î± Î± Â·Â·Â·Î± ,
dcâ€² 1Â·Â·Â·câ€² Kâˆ’1,k i1Â·Â·Â·ikâˆ’Kidiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1 i1 ikâˆ’Kâˆ’1 id iâ€² c,1 iâ€² c,Kâˆ’1
i1,Â·Â·Â·,ikâˆ’Kâˆ’1,câ€² 1,Â·Â·Â·,câ€²
Kâˆ’1
Î»dcâ€²,K
=
(cid:88)
Âµ Î± Â·Â·Â·Î± ,
idiâ€² c,k i1Â·Â·Â·ikâˆ’2idiâ€² c i1 ikâˆ’2
i1,Â·Â·Â·,ikâˆ’2âˆˆ/N câ€²
dcâ€²câ€²,K (cid:88)
Î» idi1 â€² c,2 1iâ€² c,2,k = Âµ i1Â·Â·Â·ikâˆ’3idiâ€² c,1iâ€² c,2Î± i1Â·Â·Â·Î± ikâˆ’3,
i1,Â·Â·Â·,ikâˆ’3âˆˆ/N câ€²
Â·Â·Â·
dcâ€²Â·Â·Â·câ€² ,K (cid:88)
Î» 1 Kâˆ’1 = Âµ Î± Â·Â·Â·Î± .
idiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1,k i1Â·Â·Â·ikâˆ’Kidiâ€² c,1Â·Â·Â·iâ€² c,Kâˆ’1 i1 ikâˆ’K
i1,Â·Â·Â·,ikâˆ’K
17C.3 PracticalImplementationForm
ToachieveMVD-basedIGM,weonlyneedQ tosatisfyIGM.Therefore,wecanconvert(5)into
id
anadditiveinteractiveform,Q =b+WQ ,andthenensurethatW >0. Specifically:
tot d
n+nâ€²
(cid:88)c (cid:88)
Q =k + k Q + k Q Q
tot 0 i i idiâ€²
c
id iâ€²
c
i id,iâ€²
c
(cid:88) (cid:88) (cid:88) (cid:88)
=(k + k Q + k Q )+ (k + k Q )Q .
0 iâ€²
c
iâ€²
c
ic ic id idiâ€²
c
iâ€²
c
id
iâ€²
c
ic id iâ€²
c
However,incertainscenarios,aswecannotensureQ >0,weneedtokeeptrackoftheminimum
iâ€²
c
valueofQ duringthetrainingprocess:
iâ€²
c
Q =(k +(cid:88) k Q +(cid:88) k Q )+(cid:88) ((k âˆ’(cid:88) k Qmin)+(cid:88) 2k Q iâ€² c +Qm c in )Q .
tot 0 iâ€² c iâ€² c ic ic id idiâ€² c c idiâ€² c 2 id
iâ€²
c
ic id iâ€²
c
iâ€²
c
InthespecificimplementationofMVD,weutilizeahypernetworkf (s)tolearntheweightsin(5).
i
Finally,weobtain:
Q â‰ˆ(f +(cid:88) |f |Q +(cid:88) |f |Q )+(cid:88) (|f |+(cid:88) |f |Q iâ€² c +Qm c in )Q .
tot 0 iâ€² c iâ€² c ic ic id idiâ€² c 2 id
iâ€²
c
ic id iâ€²
c
D PseudoCode
Algorithm1MultiplicativeValueDecomposition
InitializeasetofagentsN ={1,2,Â·Â·Â· ,n}
InitializenetworksofindividualagentsQ (Ï„ ,a ;Î¸)andtargetnetworksQâ€²(Ï„â€²,aâ€²;Î¸â€²)
i i i i i i
InitializemixingnetworkQ (s,a;Ï‘)andtargetnetworkQâ€² (s,a;Ï‘â€²)
tot (cid:98) (cid:98) tot (cid:98) (cid:98)
InitializeareplaybufferBforstoringepisodes
InitializeajointobservationoËœandajointactionaËœofallagents
repeat
Initializeahistoryembeddingh0andanactionvectora0foreachagent
i i
Observeeachagentâ€™spartialobservation[o1]n
i i=1
Foreachagenti executingaction,introduceacorrespondingextraagentiâ€²
c c
UpdateoËœ
,obtains1and[o1]n+nâ€²
c
c (cid:98) i i=1
for t=1:T do
GetÏ„t ={ot,htâˆ’1}foreachagentandcalculatetheindividualutilityfunctionQ (Ï„t,atâˆ’1)
i i i i i i
Getthehiddenstateht andselectactionat viaQ withprobabilityÏµexploration. Forall
i i i
agentsiâ€²,useanactionmasktorestricttheirchoicestoaËœ
c c
Executeat,updateaËœ
(cid:98)
UpdateNâ€²andoËœ
c c
Receivearewardrt,transitiontothenextstatest+1
(cid:98) (cid:98)
endfor
StoretheepisodetrajectorytoB
SampleabatchofepisodestrajectorieswithbatchsizebfromB
for t=1:T do
CalculatetheglobalQ-valueQ via(8)
tot
Calculatethetargety =rt+Qâ€² usingtargetnetwork
(cid:98) tot
endfor
UpdateÎ¸andÏ‘byminimizingthelossL(Î¸,Ï‘)=(Q âˆ’y)2
tot
PeriodicallyupdateÎ¸â€² â†Î¸,Ï‘â€² â†Ï‘
untilQ (Ï„ ,a ;Î¸)converges
i i i
18E ExperimentalDetails
E.1 Overcooked
(a)Overcooked-A (b)Overcooked-B (c)Overcooked-C (d)SaladRecipe
Figure10: Overcookedenvironments
IntheOvercooked2benchmark,threeagentsmustcollaboratetoprepareaTomato-Lettuce-Onion
saladanddeliverittothestarcountercellassoonaspossible. Thechallengeliesinthefactthat
theagentsarenotawareofthecorrecttasksequencebeforehand: fetchingrawvegetables,placing
themonthecut-boardcell,choppingthem,mergingtheminaplate,andfinallydeliveringthedish
tothecounter. Agentscanonlyobservealimited5Ã—5gridsurroundingthemselves. Theaction
spaceisdividedintoprimitive-actionspaceandmacro-actionspace. Primitive-actionspaceincludes
fiveactionsthatallowagentstomovearoundandcompleteothertasks: up,down,left,right,and
stay. Forexample, ifanagentholdingrawvegetablesstopsinfrontofthecut-boardcell, itwill
automaticallystartchopping. Themacro-actionspaceincludestenactionsthatarecombinations
ofmultipleprimitive-actions: Chop,Get-Lettuce/Tomato/Onion,Get-Plate-1/2,Go-Cut-Board-1/2,
Go-Counter,Deliver. Chopreferstocuttingarawvegetableintopieces,whichrequirestheagent
to hold vegetables and stay in front of the cutting board cell for three time steps. Get-Lettuce,
Get-Tomato,andGet-Onionrefertotheagentmovingtotheobservedlocationofrawingredients
andpickingupthecorrespondingvegetables. Get-Plate-1andGet-Plate-2refertotheagentmoving
totheobservedlocationsoftwoplates. Get-Cut-Board-1andGet-Cut-Board-2refertotheagent
movingtothelocationsoftwocut-boardcells. Go-CounterisonlyavailableinmapBandrefersto
theagentmovingtothecentercelltopickuporputdownitems. Deliverreferstotheagentmoving
tothestarcountercell. Choppingvegetablessuccessfullyyieldsa+10reward. Correctlydelivering
asaladtothestarcountercellearnsa+200reward,whereasdeliveringthewrongdishincursaâˆ’5
penalty. Furthermore,aâˆ’0.1penaltyisimposedforeachtimestep. Themaximaltimestepsofan
episodeis200. Eachepisodeterminatesifagentssuccessfullydeliveratomato-lettuce-onionsalad.
E.2 POAC
Partially observable asynchronous multi-agent cooperation
challenge(POAC)3isaconfrontationwargamebetweentwo
armies, with each army consisting of three heterogeneous
agents: infantry,chariot,andtank. Theseagentspossessdis-
tinctattributesanddifferentmovingspeeds, asdescribedin
Table1. Therewardisdeterminedbythedifferenceinhealth
lossbetweenalliesandenemies. Theultimateobjectiveisto
learnasynchronouscooperationstrategiestomaximizethewin
rateineverybattle,ensuringthatagentssustainlessdamage Figure11:Anexampleofamap,the
thantheiropponentswithin600timesteps. Allbotsareplaced solidgirdsarethehiddenterrainand
on a hexagonal map which contains hidden terrain where it thesquaresarethebots.
isdifficultforenemiestoobservewhenagentsarelocated,as
showninFigure11. POACprovidesthreebuilt-inrule-based
botswithdifferentstrategies: 1)KAI0: anaggressiveAIthatprioritizesattackingandheadsstraight
forthecenterofthebattlefield. 2)KAI1: anAIprefershidinginspecialterrainforambush,aclassic
wargametactic. 3)KAI2: anAIutilizingguidedshooting,whereoperatorssquatonspecialterrain
2ThecodeofOvercookedisfromhttps://github.com/WeihaoTan/gym-macro-overcooked?tab=readme-ov-file.
3ThecodeofPOACisfromhttp://turingai.ia.ac.cn/data_center/show/10.
19andattackenemiesduetotheadvantageofsight. Inthispaper,weconductexperimentsonthefive
scenariosconsistingofdifferentmapsanddistinctstrategiesshowninTable2. Inaddition,inthe
originalPOACbenchmark,infantry,chariots,andtanksmoveatspeedsof0.2,1,and1,respectively.
Thismeansthatonlyinfantrymovementrequiresfivetimesteps,withallotheractionsconcludingin
one. ThislimitedasynchronicityintheoriginalPOACdoesnotfullydemonstratethestrengthsofour
asynchronouscreditassignmentmethod. Therefore,inthispaper,weadjustthemovementspeedsof
infantry,chariots,andtanksto0.2,0.3,and0.5.
Table1: Operatorsdetails
Attribution Infantry Chariot Tank
blood 7 8 10
speed 0.2 0.3 0.5
observeddistance 5 10 10
attackeddistance 3 7 7
attackdamageagainsttankorchariot 0.8 1.5 1.2
probabilityofcausingdamageagainsttankorchariot 0.7 0.7 0.8
attackdamageagainstinfantry 0.8 0.8 0.6
Probabilityofcausingdamageagainstinfantry 0.6 0.6 0.6
shootcool-down 1 1 1
shootpreparationtime 2 2 0
canguideshoot True True False
Table2: Operatorsattributesindifferentscenarios.
Scenarios MapSize SpecialTerrain GuideShoot
scenario1 (13,23) False False
scenario2 (13,23) True False
scenario3 (17,27) True True
scenario4 (27,37) True True
scenario5 (27,37) True True
E.3 Hyperparameters
WecompareourMVDagainstfivepopularvalue-basedbaselinesbasedonDec-POMDPwithpadding
action,includingVDN[1],QMIX[2],Qatten[3],SHAQ[29],andNA2Q[23]thatconsiders2nd-
orderinteractions. Additionally,wealsoevaluateAC-basedmethods,IPPO[28]basedonDTDEand
MACIAICC[11]basedonMacDec-POMDP.WedonotselectCAACbecauseitscodeisnotopen-
sourceanditsapplicationislimitedtobusholdingcontrol. Weimplementthesebaselinesandour
MVDviaPyMARL24. AllhyperparametersfollowthecodeprovidedbythePOACbenchmarkand
aremaintainedatalearningrateof0.0005bytheAdamoptimizer,asshowninTable3. Additionally,
fortheAC-basedmethods,weutilize4parallelenvironmentsfordatacollectionandsetthebuffer
sizeandbatchsizeto16. OurexperimentsareperformedonanNVIDIAGeForceRTX4090GPU
andanIntelXeonSilver4314CPU.
InourimplementationofMVD,weoptedtousetheoriginalstatesinsteadofthestatesinADEX-
(cid:98)
POMDP, as we found that using s did not significantly improve the performance. We employed
(cid:98)
blankactionsaspaddingactionsforADEX-POMDP.Toensureconsistentjointactiondimensionsin
ADEX-POMDP,wealwaysintroducednagentsateachtimestepandutilizedanactionmasktofilter
outtheinvalidagentactions.
F PerformanceonAllOvercookedMaps
We conducted experiments on all three maps provided by the Overcooked benchmark shown in
Figure10. TheresultsshowninFigure12demonstratethatourMVDoutperformsotherbaselinesin
4ThecodeofPyMARL2isfromhttps://github.com/hijkzzz/pymarl2.
20Table3: ExperimentalsettingsofOvercookedandPOAC.
Hyper-parameters Value Description
batchsize 32 numberofepisodesperupdate
testinterval 2000 frequencyofevaluatingperformance
testepisodes 20 numberofepisodestotest
buffersize 5000 maximumnumberofepisodesstoredinmemory
discountfactorÎ³ 0.99 degreeofimpactoffuturerewards
totaltimesteps 5050000 numberoftrainingsteps
startÎµ 1.0 thestartÎµvaluetoexplore
finishÎµ 0.05 thefinishÎµvaluetoexplore
annealstepsforÎµ 50000 numberofstepsoflinearannealing
targetupdateinterval 200 thetargetnetworkupdatecycle
MVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Map A Map B Map C
200 200 200
150 150 150
100 100 100
50 50 50
0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Figure12: MeantestreturnonthreemapsofOvercookedbenchmark.
differentscenarios. IPPOandMACIAICC,whichexhibitedslowertrainingspeedsonthesimple
mapA,becomeunabletoconvergeonthemorecomplexandhighlycollaborativemapsBandC.This
indicatesthatdiscardingdecisioninformationfromotheragentsstruggleswithcomplexasynchronous
cooperationtasks. Similarly,VDNâ€™sassumptionthatallagentscontributeequallycannotcopewith
complexscenarios.Duetointerferencefromredundantpaddingactionandcomplexcreditassignment
models,NA2QandSHAQfailtoconvergeinallscenarios. QMIXandQattenutilizerelativelysimple
mixingnetworkstructures,demonstratinggreaterrobustnesstopaddingaction. Consequently,they
exhibitcomparativelybetterperformancethanNA2QandSHAQ.MapBisdividedintotwosections,
posingspecialdemandsoncooperationbetweenagents: Agentsintherightsectioncanonlyprepare
rawvegetablesandplates, whileagentsintheleftsectionarerestrictedtochoppingandserving.
Thecooperationbetweenagentsindifferentsectionsisweak. Thisarrangementchallengesmost
baselinestograsptheoptimalcooperationstrategy. VDNdirectlyoverlooksthemutualinfluence
amongdifferentagents,facilitatingthelearningofsub-optimalstrategyinstead.
G PerformanceonAllPOACScenarios
AsshowninFigure13,wecomparetheperformanceofourMVDwiththebaselinesinfivescenarios
providedbythePOACbenchmark. ItcanbeobservedthatMVDremainscompetitiveinsimple
scenarios1and2. Inscenario1,bothIPPOandMACIAICCachievedoptimalperformance. This
isprimarilyduetothefactthatinthisparticularsetting,asingleagentaloneissufficienttodefeat
theopposingarmy. Over-consideringcooperationamongagentsmayactuallyleadthealgorithmto
convergetoasub-optimalstrategy. SHAQrequirestheexplicitcomputationoftheshapleyvaluefor
eachagent. Nonetheless,becausepaddingactionshavenoactualimpactontheenvironment,this
ultimatelycontributestoitsfailure. Furthermore,inScenario2wheretheopposingarmyadoptsa
conservativestrategybyhidinginspecialterrains,onlythemultiplicativeinteractioninMVDandthe
GAMinNA2Qprovideadequaterepresentationalcapabilitiestolocatetheenemyâ€™shidingposition
andlearntheoptimaljointstrategy. Moreover,becauseMVDemploysasimplermodeltocapture
21
nruteR
tseT
naeM
nruteR
tseT
naeM
nruteR
tseT
naeMMVD QMIX SHAQ IPPO
VDN Qatten NA2Q MAC IAICC
Scenario 1 Scenario 2 Scenario 3
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure13: Testwinrate%onallscenariosofPOACbenchmark.
interactionsamongagents,itstrainingefficiencyishigherthanthatofNA2Q.Conversely,dueto
limitedmodelrepresentationorexploratorycapabilities,theotherbaselinesrequiremoretraining
datatoachieveasophisticatedcooperationstrategy.
H AdditionalAblationStudy
H.1 AdditionalAblationStudyonADEX-POMDP
Tocomprehensivelyevaluatetheadvantagesofmodelingasynchronousdecision-makingscenarios
usingADEX-POMDP,weextendtheadditiveinteractionVDalgorithms,includingVDN,QMIX,Qat-
ten,andthehigh-orderinteractionVDalgorithmNA2Q,toADEX-POMDP.Thisallowssynchronous
creditassignmentmethodstouniformlyconsiderthemarginalcontributionsofasynchronousdeci-
sionsfromdifferenttimesteps. WecomparetheimpactofintroducingextraagentsontheseVD
algorithms.
AsshowninFigure14,15,16,fortheadditiveinteractionVDalgorithms,includingVDN,QMIX,
andQatten,theintroductionofextraagentssignificantlyimprovesmethodperformance,especially
forQMIXinscenario2. Thissuggeststhatmodelingasynchronousdecision-makingscenariosas
ADEX-POMDPfacilitatesthesolutionofasynchronouscreditassignmentproblemsandenhances
training efficiency. However, as shown in Figure 17, this is not the case for NA2Q. Introducing
extraagentscanactuallybringredundantagentinteractionstoNA2Q,suchasthosebetweenagent
i andagentiâ€². Thesehigh-orderinteractioninformationdeterioratestheperformanceofNA2Q.
c c
ThisimpliesthatforADEX-POMDP,wemustcarefullyhandlethehigh-orderinteractionsbetween
agents.
H.2 AdditionalAblationStudyonHigh-OrderInteraction
Toevaluatetheimpactofhigher-orderinteractionsontheperformanceofVDalgorithmsinasyn-
chronous settings, we extend NA2Q to ADEX-POMDP and compared MVD and NA2Q under
differentordersofinteractionacrossfivescenariosprovidedbythePOACbenchmark. Asshownin
22
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseTMVD VDN VDN(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8
0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure14: InfluenceofintroducingextraagentsonVDN.
MVD QMIX QMIX(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure15: InfluenceofintroducingextraagentsonQMIX.
Figure18,MVD(2)whichintroducesmultiplicativeinteractiontocapturetheeffectsofasynchronous
decision-making,consistentlyoutperformsMVD(1)whichonlyusesadditiveinteraction. Basedon
23
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseTMVD Qatten Qatten(A)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure16: InfluenceofintroducingextraagentsonQatten.
MVD NA2Q NA2Q(A)
Scenario 1 Scenario 2 Scenario 3
1.0
1.0 1.0
0.8
0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure17: InfluenceofintroducingextraagentsonNA2Q.
thederivationinAppendixC.1,high-orderinteractionsnotonlyprovidedeepinformationonthe
interplaybetweenagentsbutalsoreducethenumberofTaylorexpansionsusedinthederivation
24
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseT
nruteR
tseT
naeM
nruteR
tseT
naeM
%
etaR
niW
tseT
%
etaR
niW
tseTMVD(2) MVD(1) MVD(3) NA2Q(2) NA2Q(3)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure18: InfluenceofdifferentorderinteractionsonMVDandNA2Q.
process,therebyenhancingtheaccuracyoftheVDformula. However,theintroductionofhigh-order
interactionscomplicatesthemodel,thustheperformanceofMVD(3)remainsinferiortoMVD(2).
In the case of NA2Q, higher-order interactions may slightly improve performance, but they also
carrytheriskofdegradingit. Therefore,inADEX-POMDP,theinteractionbetweenQ andQ is
id iâ€²
c
sufficienttoefficientlysolvetheasynchronouscreditassignmentproblem.
H.3 AdditionalAblationStudyonOtherImplementationForms
WecomparedthreedifferentpracticalimplementationsofMVDacrossfivescenariosinPOAC.As
showninFigure19,themethodofdirectlyobtainingtheglobalQ-valueusing(8)exhibitsslower
trainingspeedandunstabletrainingprocess. AlthoughtheuseofSoftmaxtoimplementamulti-head
structurecansomewhatimprovetrainingspeedandstability,itsperformanceremainspoorincertain
scenarios. Therefore,thissuggeststhatforMVDthatincorporatesmultiplicativeinteractions,the
structureofthemixingnetworkshouldnotbeoverlycomplex.
25
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseTMVD(MLP) MVD MVD(Softmax)
Scenario 1 Scenario 2 Scenario 3
1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2
0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Scenario 4 Scenario 5
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil)
Figure19: InfluenceofdifferentimplementationsonMVD.
26
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT
nruteR
tseT
naeM
%
etaR
niW
tseT