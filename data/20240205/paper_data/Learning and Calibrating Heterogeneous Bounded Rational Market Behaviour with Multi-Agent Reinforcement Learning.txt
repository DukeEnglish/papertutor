Learning and Calibrating Heterogeneous Bounded Rational
Market Behaviour with Multi-Agent Reinforcement Learning
BenjaminPatrickEvans SumitraGanesh
JPMorganAIResearch JPMorganAIResearch
London,UnitedKingdom NewYork,USA
benjamin.x.evans@jpmorgan.com sumitra.ganesh@jpmorgan.com
ABSTRACT progressinreinforcementlearning(RL)helpstobringthiscloser
Agent-basedmodels(ABMs)haveshownpromiseformodelling toreality[1,14,62].However,somesignificantchallengesmust
variousrealworldphenomenaincompatiblewithtraditionalequilib- be addressed before this can happen. In this work, we address
riumanalysis.However,acriticalconcernisthemanualdefinition thefollowingessentialquestion:Howcanwelearnheterogeneous
ofbehaviouralrulesinABMs.Recentdevelopmentsinmulti-agent boundedrationalbehavioursinanABM?
reinforcementlearning(MARL)offerawaytoaddressthisissue Toaddressthisquestion,weintroduceanovelmulti-agentRL
fromanoptimisationperspective,whereagentsstrivetomaximise (MARL)approachwhereagentsexhibitskillheterogeneity[53],
theirutility,eliminatingtheneedformanualrulespecification.This constrainedbytheirstrategicprocessingcosts.Inthelimit,where
learning-focusedapproachalignswithestablishedeconomicand theseprocessingcostsâ†’0,perfectlyrationalmutuallyconsistent
financialmodelsthroughtheuseofrationalutility-maximising equilibriumcanbeapproximated.Withuniformpriorbeliefsand
agents.However,thisrepresentationdepartsfromthefundamen- homogenousprocessingcostsamongagents,wecanapproximate
talmotivationforABMs:thatrealisticdynamicsemergingfrom quantalresponsetypeequilibrium.Withprocessingcostsâ†’ âˆ,
boundedrationalityandagentheterogeneitycanbemodelled.To agentsactbasedontheirpriorbeliefs,e.g.,drivenbyheuristics[27]
resolvethisapparentdisparitybetweenthetwoapproaches,wepro- orbiases[17].However,moregenerally,wecanmodelawiderange
poseanoveltechniqueforrepresentingheterogeneousprocessing- ofrealisticbehaviourwithheterogenousagentbounds.Thisframe-
constrainedagentswithinaMARLframework.Theproposedap- workaimstoenhancethesimulationofcomplexsocialsystems,
proachtreatsagentsasconstrainedoptimiserswithvaryingdegrees whichdiffersfrommanyMARLmethodologiesfocusedonlearning
ofstrategicskills,permittingdeparturefromstrictutilitymaximisa- optimalbehaviours.Instead,theworkalignswiththeliteratureon
tion.Behaviourislearntthroughrepeatedsimulationswithpolicy ABMs,focusingonunderstandingtheresultingrealisticdynamics
gradientstoadjustactionlikelihoods.Toallowefficientcomputa- emergingfromhumandecision-making.
tion,weuseparameterisedsharedpolicylearningwithdistribu- Contributions:Weproposeanapproachtoeffectivelylearn
tionsofagentskilllevels.Sharedpolicylearningavoidstheneed heterogeneousagentskilllevels(exhibitingdiversedeviationsfrom
foragentstolearnindividualpoliciesyetstillenablesaspectrum priorbeliefs)withinaMARLframework.Wedemonstratetheeffi-
ofboundedrationalbehaviours.Wevalidateourmodelâ€™seffective- cacyoftheapproachacrossseveralfundamentalmulti-agenteco-
nessusingreal-worlddataonarangeofcanonicalğ‘›-agentsettings, nomicenvironments.Theproposedapproachofferssubstantially
demonstratingsignificantlyimprovedpredictivecapability. enhancedaccuracyinpredictinghumandecisions(alongwiththe
subsequentdynamics)incontrolledexperimentscomparedtocur-
KEYWORDS rentstate-of-the-artRLapproachesandotherequilibriumbench-
marks.Toimproveefficiency,weutiliseagentsupertypes[66]and
Multi-agentreinforcementlearning;Boundedrationality;Market
sharedpolicylearningtolearndiverseboundedrationalbehaviours.
simulation;Agent-basedmodelling;Skillheterogeneity
Heterogeneityisintroducedbyvaryingstrategicprocessingcostsof
agents,measuredthroughregulariseddivergencesfromtheirprior
1 INTRODUCTION
beliefs.Thisapproachisgeneral,expandinguponanewMARL
Agent-basedmodels(ABM)haveachievedsignificantsuccessin frameworkformodellingcomplexsystems,Phantom[3].
variousdomains,includingbusiness,epidemiology[33],economics,
and finance [5, 25]. However, despite these achievements, criti-
cismspersistwithindifferentcommunities[2],notablyineconom- 2 RELATEDWORK
ics[37,39,64],primarilyduetoconcernsregardingthedecision- While MARL algorithms have made significant advances in ap-
makingrulesinthesesystems.Frequently,theserulesaremanually proximatingequilibriawithincomplicatedenvironments[50],a
specifiedheuristics,placingsubstantialrelianceonthemodellerâ€™s limitationisthatmostprevailingmethodologiesassumeagents
judgementasthesimulationresultsandvaliditydependuponthe tobeperfectlyrational.Thisassumptionisoftenoverlystringent
specificbehaviouralrulesutilised[48,64].Ontheotherhand,adap- whensimulatingcomplexsocialsystems[4],andmaymisscrucial
tiveagentsthatoptimiseautilityfunctionfindgreateracceptance real-worlddynamics[7].Tobroadentheapplicabilityoftheseap-
acrossdisciplines,astheagentsâ€™behaviourisautomaticallyderived proachesformodellingcomplexsystems,ourobjectiveistoextend
inaprincipledmanner. MARLframeworkstoaccountforagentheterogeneityandbounded
Hence,introducingadaptiveandlearningagentsintoABM,while strategicabilities.Indoingso,wemaketheconnectionwithABMs
allowingforheterogeneityandboundedrationality,couldalleviate whileautomatingsomeofthedifficultmodellingdesigndecisions
these concerns and improve the realism of the models. Recent (e.g.,determiningthebehaviouralrulesoftheagents)usingRL.
4202
beF
1
]AM.sc[
1v78700.2042:viXraRecentcomprehensiveexaminationscoveringRLtechniquesin ğ‘ âˆˆğ´.Importantly,theseagentsmaynotactperfectlyrationally.
ABMarefeaturedin[51,63,70],emphasizingtheusefulnessof Thesystemischaracterisedbyastatespaceğ‘†,andagentspossess
learningagentbehaviours. a(potentiallypartial)observationofthecurrentstateğ‘  alongwith
ğ‘–
Behaviouraleconomicshasdevelopedmorerealisticmodelsof priorbeliefsabouttheirpotentialactionsğ‘ (apriorprobability
ğ‘–
decision-makingthanthetraditionalhomoeconomicusperfectly distributionovertheactionspace).Thebehaviourofeachagent
rationalrepresentativeagent[40].Instead,thesemodelsoperate is governed by their policy ğœ‹ , which is a mapping from states
ğ‘–
undertheframeworkofboundedrationality.Oneprominentap- to a distribution over actions. Agents act based on their policy
proachforrelaxingthestrictperfectlyrationalNashequilibrium ğ‘
ğ‘–
âˆ¼ğœ‹ ğ‘–,receivingrewardğ‘ˆ ğ‘–(ğ‘ ğ‘–,ğ‘  ğ‘–).Again,theseactionsmaynot
(NE) assumption and incorporating bounds in reasoning is the beperfectlyrationalandinsteadmaybesatisficing[9,58].
QuantalResponseEquilibrium(QRE)[43,44],whichallowsfor
deviationsfromoptimalresponsesandthepossibilityoferroneous 3.1 Components
play.Suchapproachesgenerallyfeatureconsistent(andcommon) 3.1.1 Reward. InstandardRL,agentsaimtolearnanoptimalpolicy
beliefsamongagents,forexample,byhavingthesameprocessing thatmaximisestheirexpectedcumulativediscountedrewardğ‘ˆ:
costs across the population (mutual consistency). However, the
(cid:34) âˆ (cid:35)
n abe ie ld ityto toco "n bs ri id ne gr aa bg oen ut th ne et wero og ue tcn oe mity esha ns otbe foen res st er ee as bse led fd ru oe mto anth ae l- ğœ‹ ğ‘–âˆ—(ğ‘|ğ‘  ğ‘–)=m ğœ‹a ğ‘–xE ğœ‹ğ‘– âˆ‘ï¸ ğ‘¡=0ğ›¾ğ‘¡ğ‘ˆ(ğ‘ ğ‘¡|ğ‘  ğ‘–,ğ‘¡) (1)
ysisofthehomogeneousdynamics"[45].Giventhatpopulations
However,often,inrealworldsystems,theagentsweseekto
inherentlyencompassaspectrumofbehavioursandbeliefs,a"rep-
modelareboundedlyrational,drivenbypriorbeliefsandalimited
resentative"agentoftenprovesinsufficient[30].Recognizingthe
amount of processing power to improve upon these priors. To
importance of strategic diversity, various extensions have been
addresstheseconsiderations,weincorporategenericlimitations
developedtoaccommodatearangeofagentbehaviours[52,53],re-
inagentsâ€™reasoningabilitiesbyreformulatingthemaximisation
laxingthemutualrationalityandmutualconsistencyassumptions
problemintoaconstrainedone:1
[12,19].Relaxingmutualconsistencyisbeneficialformulti-agent
settings,allowingforapopulationcharacterizedbyvaryinglevels
(cid:34) âˆ (cid:35)
o off ss utr ca hte gg ai mc eb -o tu hn ed oe rd etn ie cs ms[ o3 d8 e, ls49 is,6 o8 ft] e. nH lo imw ie tv ee dr, toth re elc ao tim vep lu yt sa ib mil pit ly
e
ğœ‹ ğ‘–ğœ† (ğ‘|ğ‘  ğ‘–)=m ğœ‹axE ğœ‹ğ‘– âˆ‘ï¸ ğ›¾ğ‘¡ğ‘ˆ(ğ‘ ğ‘¡|ğ‘  ğ‘–,ğ‘¡)
(2)
ğ‘¡=0
domains,promptingtheuseofapproximationmethods(suchas subjectto ğ¼(ğœ‹ ğ‘–,ğ‘  ğ‘–,ğ‘¡,ğ‘ ğ‘–) <ğ¼Â¯
MARL)withinmorecomplicatedenvironments[71].
DespitetheachievementsinbehaviouralgametheoryandRL,a whereagentsmaximiseğ‘ˆ whileadheringtoaconstraintğ¼Â¯ontheir
gappersistsincombiningthesemethodswithABMs,e.g.forlearn- processingcostsğ¼.Theprocessingcostrestrictshowfarthelearnt
ingthedecision-makingrules.WhileRLapproachesoftenprioritize policyğœ‹ ğ‘– candivergefromthepriorbeliefğ‘ ğ‘–.Wecanequivalently
convergencetowardsrationalequilibrium,thisfocuscontrastswith reformulateEq.(1)asthemaximisationofamodifiedreward:
theprimaryobjectiveoftheABMcommunity:understandingthe (cid:34) âˆ (cid:35)
Tp oro tp he ert bie es stem ofe or ugi rn kg nf oro wm leh de gt ee ,r to hg een see mou ins ab lo wun od rked oly nr ca at pio tun ra il na gge ren ats l-. ğœ‹ ğ‘–ğœ† (ğ‘|ğ‘  ğ‘–)=m ğœ‹a ğ‘–xE
ğœ‹ğ‘–
âˆ‘ï¸ ğ‘¡=0ğ›¾ğ‘¡ (cid:0)ğ‘ˆ(ğ‘ ğ‘¡|ğ‘  ğ‘–,ğ‘¡)âˆ’ğœ†ğ¼(ğœ‹ ğ‘–,ğ‘  ğ‘–,ğ‘¡,ğ‘ ğ‘–)(cid:1) (3)
isticboundedbehavioursinMARListhatof[46],basedonrational whereğœ† controlsthestrengthofregularisation,modulatingthe
inattention(RI).Theauthorsof[46]underlinethattheexisting boundedness(orskill)oftheagent.Asğœ†â†’âˆ,theagentisentirely
bodyofwork"failstoaddressthemodelingofboundedrationality drivenbytheirpriorbeliefs(performingnostrategicreasoning),
formoreaccurate[MARL]simulations,"proposinganinnovative whereasasğœ†â†’0,theagentisunboundedandapproximatesratio-
frameworktoaddressthislimitation.Ourworkdiffersinsomeim- nalbehaviour.Eq.(3)makestheformulationgeneralandcompatible
portantways.Specifically,weremovethedifficultprocessingcost withexistingRLalgorithmswithoutrequiringin-depthmodifica-
estimationrequiredin[46](asdiscussedinSection5)andallowfor tiontothelossoroptimisationprocess.
arbitrarypriorbeliefsforencodingbehaviouralbiases.Additionally,
weintroduceagentskillheterogeneity,learntthroughregularised 3.1.2 ProcessingCosts. Quantifyinginformationprocessingcosts
policies,andproposeanapproachforefficientlycalibratingthese inageneralisedmannerisdesirable,asthisenablescompatibility
policiestorealworlddynamics,allfeaturesyettobeconsidered. withexistingoptimisationalgorithms.Followingrecentachieve-
mentsinconstrainingagentdecision-makingusinginformation-
theoreticcosts[19],weadoptasimilarapproach.Thisinformation-
3 PROPOSEDAPPROACH theoretictreatmentabstractstheunderlyingcausesofsuchcon-
WeintroduceanovelMARLapproachtoeffectivelymodeladiverse straints,allowingafocusonlearningbehaviourwithoutnecessitat-
rangeofboundedrationalbehaviours(orvaryingskill).Thisap- inganin-depthunderstandingofthespecificpsychologicalfactors
proachprovesvaluableforcalibratingABMstoreal-worldsystems atplay.Fromanoptimisationstandpoint,thisisadvantageous,as
through learning regularised policies and, under limiting cases, theprocessremainsindependentoftheparticulardetailsofhow
establishinglinkstovariousexistingequilibriumsolutionconcepts. decisionsareformulated[59].
Our general formulation is as follows. We focus on ğ‘-agent
1Wemaintainexponentialdiscountinghere.However,hyperbolicdiscountingmight
systems,whereeachagentğ‘– âˆˆğ‘ seekstomaximisetheirreward
provideabetteralignmentwithhumandecision-making[54],althoughcurrentRL
(orutility)functionğ‘ˆ ğ‘– bytakingactionsfromtheiractionspace approacheshavenotyetexhibitedsignificantdifferencesbetweenthetwo[22].Oneofthemostcommoninformation-theoreticconstraintsisan 3.1.3 HeterogeneousBehaviours. Effectivelylearningbehaviours
entropyconstraint,e.g.ğ»(ğœ‹ ğ‘–)=âˆ’(cid:205)ğ‘(ğœ‹ ğ‘–)âˆ—log(ğ‘(ğœ‹ ğ‘–)),restricting thatcapturethediversityofthepopulationâ€™sdecision-makingis
deviationsfromuniformbehaviour.Forexample,(thelogitform crucialforintegrationintoagent-basedsimulations.
of)QREcanbeseenasmaximisationunderanentropyconstraint. Twoinitialapproachescouldbeemployedforcapturinghet-
However,muchresearchhasshowntheusefulnessofincorporating erogeneous skills of agents: Firstly, learning optimal (homoge-
arbitrarypriorbeliefs(notjustuniform)[18],motivatingextensions nous) behaviours ğœ‹âˆ— as in Eq. (1) and applying heterogeneous
thatmeasurethedivergencefromanarbitrarypriordistribution boundsatinference,e.g.,ğœ‹ ğ‘– = ğœ‹âˆ—+ğœ‚ ğ‘–,whereğœ‚ ğ‘– isanoiseterm.
basedontheKullback-Leibler(KL)divergenceD KL[47]. Secondly, individual learning with heterogeneousğœ†â€™s, i.e., ğœ‹ ğ‘– =
KLregularisationhasshownsuccessinrelevantdomains[36,60] max ğœ‹E
ğœ‹
(cid:2)(cid:205) ğ‘¡âˆ =0ğ›¾ğ‘¡ (cid:0)ğ‘ˆ(ğ‘ ğ‘¡|ğ‘  ğ‘–,ğ‘¡)âˆ’ğœ† ğ‘–ğ¼(ğœ‹,ğ‘  ğ‘–,ğ‘ ğ‘–)(cid:1)(cid:3).Inthefollowingsec-
forformallycapturingthesecosts.Forexample,[36]demonstrates tion, we describe why these approaches are insufficient before
theusefulnessofapenaltyforminimisingD fromexpertpolicies proposinganalternativethatovercomestheselimitations.
KL
intreesearchand[60]analysesD inaRLcontextforimproving
KL
convergenceintwo-playergames.However,neitherofthesecon- Post-hocboundsatinference. Onepotentialapproachinvolves
optimisingğ‘ˆ withoutincludinginformationprocessingcostsdur-
sidersthedomainweproposehereforbettercapturinghuman-like
ingtraining(eliminatingboundedness)tolearnğœ‹âˆ—.Subsequently,
playincomplexmulti-agentsocialsystems.
thisboundednessparameterisappliedonlytothelearnedpoli-
Specifically,weproposeusingthefollowinginformationpro-
cessingcosts
ciesduringinferenceğœ‹
ğ‘–
=ğœ‹âˆ—+ğœ‚ ğ‘–,whereğœ‚
ğ‘–
isthenoiseterm.For
example,[8]appliesdropoutduringsimulation,andnoisyintro-
âˆ‘ï¸ ğœ‹(ğ‘|ğ‘ ) spectionappliesnoiseintothedecisionsrelaxingtheequilibrium
ğ¼(ğœ‹,ğ‘ ,ğ‘)=D KL(ğœ‹ âˆ¥ğ‘)= ğœ‹(ğ‘|ğ‘ )log
ğ‘(ğ‘|ğ‘ )
(4)
requirement[28].Suchmethodsintroducearangeofskilllevelsin
ğ‘âˆˆğ´
actionexecution,e.g.throughintroducingnoiseğœ‚ intotheaction
ğ‘–
toconstrainğœ‹ ğ‘–fromdivergingtoofarfromagentsâ€™priorbeliefsğ‘at selectionprocess.However,ifheterogeneousboundswereimple-
eachstate,limitingtheirstrategicabilities.Eq.(4)canalsobeseen mentedinthismanner,agentswouldnotlearnhowtoadaptto
asequivalenttoenforcinganğ»constraintwhenassumingtheprior thebehaviourofotherboundedagents,asthebestresponsetothe
beliefsareuniform(makingconnectionswithQRE,asdiscussedin optimalpolicyisnotnecessarilythebestresponsetoanoisypolicy
Section5).Additionally,withthisrepresentation,thecontribution ğµğ‘…(ğœ‹âˆ—) (cid:46)ğµğ‘…(ğœ‹ ğ‘–).Toillustratethispoint,considerasimplerock-
ofaspecificactionğ‘tothedivergencecanbeidentified,e.g., paper-scissors(RPS)environment.InRPS,theperfectlyrational
equilibriumpolicyisğœ‹âˆ—={ğ‘(R),ğ‘(P),ğ‘(S)}={1,1,1 }.However,
3 3 3
ğ¼
ğ‘(ğœ‹,ğ‘ ,ğ‘)=ğœ‹(ğ‘|ğ‘ )logğœ‹(ğ‘|ğ‘ )
(5)
ifagent2spolicyisinsteadfixedasğœ‹2 = {0 3, 30,3 3}(e.g.theyare
ğ‘(ğ‘|ğ‘ ) boundedlyrationalandbiasedtowardsplayingğ‘†),therationalbest
meaningtheadjustmenttotherewardfunctioninEq.(3)canbe responseforagent1isğœ‹1 =ğµğ‘…(ğœ‹2) = {3 3,0 3,0 3}.Hadagent1not
directlylinkedtoğ‘ratherthanğœ‹ ,i.e.,
observedğ‘2 âˆ¼ ğœ‹2 duringtraining,theywouldnothavelearned
ğ‘–
toexploitğœ‹2.Whileasimpleexample,thisconsiderationbecomes
pivotalindemonstratingtheemergenceofauto-curricula[6].To
(cid:34) âˆ (cid:35) capturetheinterplayamongheterogeneouslyskilledagents,the
ğœ‹ ğ‘–ğœ† (ğ‘|ğ‘ )=m ğœ‹a ğ‘–xE
ğœ‹ğ‘–
âˆ‘ï¸ ğ‘¡=0ğ›¾ğ‘¡ (cid:0)ğ‘ˆ(ğ‘ ğ‘¡|ğ‘  ğ‘–,ğ‘¡)âˆ’ğœ†ğ¼ ğ‘ğ‘¡(ğœ‹ ğ‘–,ğ‘  ğ‘–,ğ‘¡,ğ‘ ğ‘–)(cid:1) (6)
n pro oti co en sso rf ab tho eu rn td he ad nn oe nss lym du us rt inb ge ip nr fe es re enn ct et .hroughoutthelearning
whichisadvantageousforoptimisationpurposes.Weusethisfor-
mulationthroughout.Aswesamplemoreactionsfromthispolicy, IndividualLearning. Analternativeapproachinvolvesassigning
wewouldapproximateD KLasğ¼(ğœ‹,ğ‘ ,ğ‘)=(cid:205) ğ‘âˆˆğ´ğ¼ ğ‘(ğœ‹,ğ‘ ,ğ‘). h Ee qt .e (r 3o ),g ee .n ge .o wu is thpr ho ec te es rs oi gn eg np eoen usal lt oie gs itğœ† rğ‘– et so poe nac dh era sg [e 3n 0t ].ğœ‹ Iğ‘– n= thğœ‹ isğœ†ğ‘– sci en
-
InRL,information-theoreticregularisationisoftenemployedto
nario,anystandardMARLalgorithmcouldbeemployed,wherein
enhancetheconvergenceorrobustnessofalgorithms.Forinstance,
allagents,eachgovernedbytheiruniqueconstraint,strivetoopti-
ProximalPolicyOptimization(PPO)utilisesaD penaltytermto
KL
misetheirrewards,adjustingtheirbehavioursinresponsetothe
preventexcessivelylargechangesinthepolicyduringtrainingsteps
observedoutcomes.However,thismethodwouldproveinefficient
andimprovetheconvergence.Similarly,theSoftActor-Critic(SAC)
duetothenecessityoflearningğ‘individualpoliciesandcalibrating
algorithmemploysD initspolicyimprovementstep,limiting
KL
ğ‘ differentindividualisedprocessingcostsğœ† (oneforeachagent).
divergencefromthepreviousQ-function[31].Moreover,Maximum ğ‘–
ThisinefficiencybecomesacrucialconcernasABMsoftenhave
EntropyRLintroducesanentropytermtoenhanceexploration,con-
alargeğ‘.Additionally,thelearntpolicieswouldnotgeneralise
vergence,androbustness[20,21].Incontrast,ourapproachrestricts
acrossdifferentğœ† ,requiringnewtrainingeachtimeanewğœ† is
divergencefromanarbitrarypriorbeliefğ‘toreflecttheconstraints ğ‘– ğ‘–
introduced.
ininformationprocessingpresentduringhumandecision-making
Itbecomesclearweneedascalablealternativethatcandealwith
ratherthanbeingaimedatimprovingthealgorithmâ€™sconvergence.
theheterogeneousboundedlyrationalbehaviourofagents.
Thesepriorbeliefsğ‘(alsocalled"magnets"[60]or"anchors"[36])
maychangethroughouttrainingandinference(e.g.withupdated
information)andcantakemanyforms,forexample,demonstrating 3.2 SharedPolicyLearning
biastowardscertainactions,encodingheuristics,averagingover Rather than learning individual policies with heterogeneousğœ† ,
ğ‘–
pastdecisions,orpreferringhistoricallywell-performingactions. allaimingtosolveEq.(3),wewishtolearnageneralisedpolicyğœ‹(...|ğ‘  ğ‘–,ğ‘–,ğœ† ğ‘–,ğ‘ ğ‘–).Thisrepresentationtreatsagentsâ€™priorbeliefs useofsupertypestocapturediverseboundedrationalbehaviours
andprocessingresourcesaspartoftheobservationspace(andfor basedontheregularisedpoliciesintroducedinEq.(3).
simplicityofnotation,wewilluseğœ‹ ğ‘–(...|ğ‘  ğ‘–) =ğœ‹(...|ğ‘  ğ‘–,ğ‘–,ğœ† ğ‘–,ğ‘ ğ‘–)), Undertheproposedapproach,aregularisedpolicyforthesuper-
enablinggeneralisedpolicylearningbasedonthesestateobserva- typeisestablishedğœ‹D,whichisexposedtodifferentregularisation
tions.Thisformulationprovidesawayofefficientlyrepresentinga strengthsğœ† ğ‘– âˆ¼ D throughouttraining.ğœ‹D learnstoextrapolate
diversepopulationofagentsğ‘– âˆˆğ‘ withvaryingboundsinstrategic overtheregularisationstrengths,reducingthenumberofpolicies
reasoningabilitiesğœ† ğ‘– throughasingleparameterisedpolicywith to train while still enabling heterogeneous behaviour. Through
anaugmentedobservationspace. thisprocess,agentslearntoadapttheirbehaviourinresponseto
However,calibratingğœ† ğ‘– remainsanimportantissue.Whilecali- thevaryingprocessingresourcesacrosstheagentpopulation,ac-
bratingğœ† ğ‘– toeachagentğ‘–mayseemideal,thisiscomputationally countingforpotentialautocurricula.Theinputstothesupertype
impracticalwithlargerğ‘,andadditionally,couldleadtooverfit- (ğœ‡,ğœ)arecalibratedsuchthatthatthesimulationoutcomeclosely
tingtospecificbehaviouralparametersduetothelargenumberof matchestherealworlddynamics(fromthecalibrationdata).An
requiredparameters(ğ‘).Furthermore,inpractice,asğœ† ğ‘– areunob- overviewofthishigh-levelprocessisdepictedinFig.1.
served,assigningthesevaluesexactlyisdifficult. Thesharedpolicyğœ‹Dtakesanagentâ€™sidğ‘–,processingresources
Weadoptanalternativeapproachtoaddressthischallengeby ğœ† ,andpriorbeliefsğ‘ asinputs(ascomponentsofğ‘  ).Includingğ‘–
ğ‘– ğ‘– ğ‘–
assigningindividualstrategicprocessingresourcesassamplesfrom facilitatesthelearningof(potentially)competitivebehaviourbe-
aprobabilitydistributionğœ† ğ‘– âˆ¼ D,addressingtheuncertaintyof tween agents of the same supertype. By adjusting the input ğœ† ğ‘–
theagentsâ€™exactğœ† ğ‘– valuesandkeepingthenumberoffreeparame- values,ğœ‹D caneffectivelydemonstrateaspectrumofskilllevels,
terslow.AnyDcouldbeutilised(andtheproposedapproachis whileonlyneedingtolearnasingle(generic)policy.Allowingfor
agnostictotheparticulardistributionused),buthere,weemploy arbitraryğ‘ accountsfortheeffectofvariouspriorbeliefs.
ğ‘–
theGaussiandistributionD =N(ğœ‡,ğœ),whereğœ‡controlsthemean Theunderlyingassumptionofthissupertypeapproachisthatall
processingcosts,andğœtheheterogeneity.Thisway,weonlyneed agentsinthesupertypehavethesameğ‘ˆ function;however,they
tocalibratetheparametersğœ‡andğœ(ratherthanğ‘ separateparam- possessvaryinglevelsofskillinmaximisingğ‘ˆ.Giventheinher-
eters),whichistypically<<ğ‘.Forinstance,whendealingwith entuncertaintysurroundingtheprecisenatureofagentdecision-
ğ‘ =100agents,wearecalibratingjust2parametersinsteadof100, making,acompellingcaseismadeforcapturingaspectrumof
helpingtoavoidoverspecification.Duringlearning,thissampling regularisedbehaviours.Theregularisationoffersdualadvantages:
approachallowsforinterpolationacrossarangeofğœ† ğ‘–,reducingthe firstly,itenablesdeviationsfromperfectrationalityinagentbe-
computationalcomplexityandenforcinga"smoother"policy.This haviour;secondly,iteffectivelyencompassesuncertaintiesfrom
smoothnessarisesfromobservingmanydifferentbehavioursdur- boththemodellerâ€™sperspectiveandtheagentsbeingmodelled(i.e.,
ingtraining,resultinginamorerobustpolicyforcedtointerpolate uncertaintyinthemodelâ€™sformulationandtheagentsâ€™decision
acrossğœ† ğ‘– values,reducingthepotentialofoverfitting. processes)[18].Thismotivationalignswiththeuseofbounded
rationalityinsituationscharacterisedbyfundamentaluncertainty
Reward r1, r2,... rn oftheagent[26]andalsohelpstoaddressconcernsregardingmod-
Action a1 ellerjudgement(e.g.modelmisspecification)bypermittingarange
Share ğœ‹d ğ’ŸPolicy Action a2 Environment ofinformation-constrainedbehaviour[55].
Action an
4 EMPIRICALRESULTS:ğ‘›âˆ’AGENTSETTINGS
New State s1, s2,... sn
Toverifythattheproposedapproachcancapturearangeofinterest-
ingbehaviournotpredictedbytheanalyticallyderivedequilibrium
Î»1 Î»2 Î»N
Real world outcomes
orstandardstate-of-the-artMARLapproaches,wecomparethe
predictionsfromtheproposedmodelagainsttheseapproacheson
Î¼, Ïƒ
Supertype Calibrated to data arangeofcanonicalğ‘›-agenteconomicenvironmentsinvolving
Î»i ~ ğ’Ÿ humanparticipants.
Î¼, Ïƒ
Regularisation for Boundedness strength
4.1 ProcessOverview
Figure1:ProposedApproach:Sharedpolicylearningwith Weassesstheperformanceoftheproposedapproachinthreewell-
heterogeneousboundsthroughagentsupertypes. established multi-agent economic environments: supply chains,
oligopolies, and cobweb markets. To validate our approach, we
To learn a generalised policy ğœ‹D for ğœ† ğ‘– âˆ¼ D, we use agent leveragelaboratoryexperimentsconductedineachsetting,com-
supertypes[66],enablingefficientscalingthroughsharedpolicy paringthepredictionswithactualhumanbehaviour.Wecompare
learning,whilestillcapturingarangeofbehaviours.Agentsuper- theproposedapproachwithanalyticallyderivedsolutionsanda
typeshaveexhibitedpromise,particularlyinapplicationssuchas state-of-the-artMARLalgorithm(PPO).Ineachcase,weperform
calibratingrationalbehaviourinover-the-countermarkets[65].
However,thepotentialforincorporatingheterogeneousstrategic
2TheCournotcompetitionenvironmentseachcarry1weightwhencomputingtheav-
reasoningskillstobetterapproximatehumandecision-makinghas 2
eragetoaddresstheinterdependenceandavoidbiasingtheaverageranking(although
yettobeexplored.Here,weproposeanovelapproachtoextendthe inthiscase,therankingswouldnotchangewithoutsuchaweighting)Table 1: 5ğ‘¥2-fold validation results for each environment. 4.2 Results
Each cell displays the root mean squared errors as mean The out-of-sample performance of each algorithm is compared
Â± standard deviation, along with (rankings) for between- inTable1.Theproposedapproachachievesthehighestaccuracy
environment comparison [15]. The last row presents the acrossallthreeenvironments,resultinginthebestoverallrank.
2
averagerank .Lowerrankingsindicatebetterperformance. Thestate-of-the-art(standard)MARLapproachandtheanalytically
derivedrationalcasegenerallyperformequivalently,indicatingthat
Rational MARL Proposed theMARLalgorithmapproximatedthetruerationalequilibrium
SupplyChain 0.33Â±0.004(2.5) 0.33Â±0.004(2.5) 0.02Â±0.005(1) well.However,boththealternativesperformedpoorlyincapturing
theexperimentaldata,demonstratingthatrationalityandhomo-
Cournot
-Duopoly 0.16Â±0.001(3) 0.13Â±0.001(2) 0.04Â±0.001(1) geneity are too strict of an assumption even in these relatively
-Triopoly 0.16Â±0.002(3) 0.15Â±0.002(2) 0.03Â±0.001(1) simplemulti-agentsettings.Theseresultsmotivatetherelaxation
Cobweb 0.02Â±<0.001(2) 0.03Â±<0.001(3) 0.01Â±<0.001(1) ofperfectrationalityandtheintroductionofskillheterogeneity
whenusingMARLtomodelcomplexsystems.
Rank 2.5 2.5 1
Tobetterunderstandtheresultsandthereasonfortheimproved
capabilitiesoftheproposedapproach,weanalyseeachenviron-
mentinmoredetail.Foreachenvironment,webeginwithabrief
Worst Worst description,beforepresentingtheresults.
4.2.1 SupplyChains.
Description. Thesupplychainenvironmentisacapacityalloca-
Best tionproblemwithasinglegoodwithcostğ‘andpriceğ‘.Thereis
0.250.5 1.0 Âµ2.5 5.010.0 Best 0.00 0.25 0 Ïƒ.5 âˆ—0 0.75 1.00 onesupplierwithalimitedcapacityğ¾,andğ¼ retailers.Eachretailer
(a)ğœ‡,ğœâˆ— (b)ğœâˆ—averagedacrossvaluesofğœ‡ ğ‘– makesarequest0 < ğ‘¥ ğ‘– â‰¤ ğ‘‹,ğ‘¥ ğ‘– âˆˆ Z,andthesupplierresponds
byofferingğ‘¦ .Retailersareallocatedgoodsproportionatetotheir
ğ‘–
Figure2:Triopolycalibrationresultsforvaluesofthebound-
requestğ‘¦
ğ‘–
âˆğ‘¥ ğ‘–:
ednessparameterğœ‡andheterogeneityparameterğœâˆ—
ğ‘¥
ğ‘¦
ğ‘–
=ğ¾Ã—
(cid:205)
ğ‘–
ğ‘¥
(7)
ğ‘—âˆˆğ¼ ğ‘—
inducingthepotentialfor(rationally)inflatedordersizestoen-
repeated5x2cross-foldvalidation[15]toestimatethegeneralisa-
suretherequiredquantitiesaremet.Eachretailerreceivesafixed
tionability,ensuringthemodelsdonotoverfittothecalibration
demandğ· > ğ¾,i.e.,resourcesarelimited.Therewardisgivenby
data.Thesquaredğ¿2-lossfunction(themeansquarederror)isused ğ¼
asourperformancemetric[16].Inthepresentedtables,wereport
therootmeansquarederrorforinterpretability.Tofacilitatecom-
ğ‘ˆ(ğ‘¥ ğ‘–)=ğ·Ã—(ğ‘âˆ’ğ‘)âˆ’ğœ”Ã—max(ğ‘¦ ğ‘–âˆ’ğ·,0)âˆ’ğ‘ Ã—max(ğ·âˆ’ğ‘¦ ğ‘–,0) (8)
parisonsacrossenvironments,weuserankingsbasedonresulting
errors(wherethelowesterrorreceivesrank=1,andtiesaresplit whereğœ”isthewastagecost,andğ‘ istheshortagecost(ğ‘  =ğ‘âˆ’ğ‘).
byusingtheaveragerankhadtherebeennoties)[15].Tocalibrate Therational(Nash)solutiontothistask,irrespectiveofğœ”,ğ‘ (when
ourmodel,weperformagridsearchoverğœ‡,ğœvalues,choosingğœ‡,ğœ inthelimitedcapacitycaseofğ¾ <ğ¼âˆ—ğ·),isforretailerstosubmit
withthelowesttrainingerrorforuseontheunseentestset.Theop- theirmaximumrequestğ‘‹,eachretailerthenreceivingğ‘¦ ğ‘– = ğ¾ ğ¼ units
timisationprocessneverseesthetestdata.Additionalexperiment duetotheproportionalallocation.Anylowerofarequestwould
detailsaregiveninAppendixA. resultintheretailerreceivingğ‘¦ < ğ¾ <ğ·.
ğ‘– ğ¼
Weutilisetheexperimentalresultsof[11],withğ¼ = 2,ğœ” = 2,
4.1.1 Calibration. The calibration results for one environment andğ‘  =5.Therewere30subjects,composedofuniversitystudents,
(triopolies), displaying the values of theheterogeneity (ğœâˆ—) and
randomlypairedin30repeateddecisionroundstomakeagame
boundedness(ğœ‡)parameters,areshowninFig.2.Similarplotsare
withtworetailersineachround.Thecapacityisğ¾ =90,witheach
availableforallenvironmentsinAppendixA.1.1.Theproposed
retailerreceivingdemandğ· = 50andabletomakeamaximum
approachofferstheflexibilitytoincorporateperfectrationalityor
requestofğ‘‹ =100.As2ğ· >ğ¾,wearefacedwithlimitedcapacity.
homogeneitybysettingğœ‡ = 0(removingbounds)orğœâˆ— = 0(re-
movingheterogeneity).However,itâ€™snoteworthythattheoptimal Results. TheexperimentalresultsaredisplayedinFig.3,showing
valuesneveralignwithğœ‡ = 0orğœâˆ— = 0,highlightingtheuseful- substantialdeviationsfrompurelyrational(Nash)play.TheNE
nessofbothheterogeneityandprocessingcostsacrossallthree istorequestthemaximumğ‘¥ ğ‘– = ğ‘‹ = 100.ThestandardMARL
environments.Furthermore,sincehomogeneityandunbounded approachlearnstheNEhere;however,thisisapoorpredictorof
reasoningcanbeconsideredspecialcasesofourproposedapproach, whathappensexperimentally(Fig.3).Experimentally,themost
thiseliminatestheneedtodeterminesuchassumptionsapriori(as commonlyoccurringrequestsareinthe60âˆ’80range,farlower
oftenrequiredinmanyexistingmethods).Instead,ourapproach than the NE. The proposed approach is a very good fit for the
enablesthecalibrationofthesepropertiesbasedonthespecific experimentallyobservedbehaviour,capturingtheoveralltrend,
environmentsofinterest. demonstratingthatsubjectshavevaryingstrategicbounds,giving
âˆ—Ïƒ
0.050.01.052.05.00.1
knaR
naeM1.0 Proposed Proposed Proposed
MARL MARL MARL
0.8 Rational Experimental Experimental Data
Experimental Data
Rational Rational
0.6
1.0 1.0
0.4
0.8 0.8
0.2
0.0 0.6 0.6
0-3940-4950-5960-6970-7980-8990-99100
Request x i 0.4 0.4
0.2 0.2
Figure 3: Supply Chain. Experimental data from [11] are
shownasgreybars.Theproposedapproachisshownwith
0.0 0.0
theorangeline(foronecalibrationfold).ThestandardMARL 10 20 30 10 20 30
approachisshownasthedashedpurpleline,andtheNEis q i q i
denotedbytheblackbar.
(a)Duopoly (b)Triopoly
risetoarangeofoutcomesnotpredictedbyarationalrepresentative 0.2
agent,andhelpingtomotivatetheboundedrationalityassumptions.
4.2.2 CournotOligopoly. 0.0
10 15 20 25 30
Description. TheCournotcompetitionisanenvironmentmod- q i
ellingoligopoliesinamarket.InaCournotmarket,ğ¾ firmsmust
simultaneouslychoosewhatquantitiesğ‘ ğ‘– âˆˆ Zofahomogenous (c)Duopolywithaprioripreferencetowardsprominentnumbers
goodtoproduce.Therewardforfirmğ‘–dependsonthemarketprice
(10,15,20,25,30),reflectingacognitivebias.
ğ‘ofthegoodandtheindividualğ‘ ,i.e.:
ğ‘–
Figure4:Cournotcompetitions.Experimentaldatafrom[23]
ğ‘ˆ
ğ‘–
=ğ‘Ã—ğ‘
ğ‘–
(9) isshownasgreybars.Theproposedapproachisshownwith
theorangeline(foronecalibrationfold).ThestandardMARL
whereğ‘isdeterminedbythetotalproductionofallgoods,i.e.,
approachisshownasthedashedpurpleline,andtheNEis
ğ¾ denotedbytheblackbar.
âˆ‘ï¸
ğ‘ =ğ´âˆ’ğµÃ— ğ‘
ğ‘˜
(10)
ğ‘˜=1
Weusetheexperimentaldatafrom[23,32],forduopoliesand whichthemodelwithuniformpriorscannotcapture.Thesepeaks
triopolies(experiments7,8,9,10from[23]).Following[32],wegroup cannotbeexplainedfromexpectedrewardalone,asthereisno
experiments7,10(duopoly)togetherandexperiments8,9(triopoly) particularreasonthat15wouldhavesuchhighpreference.Instead,
together.Intheseexperiments,ğ´=2.4,ğµ=0.04,and8â‰¤ğ‘ ğ‘– â‰¤32. these demonstrate an a priori preference of the agents towards
Therewere64participantsfortheduopolyexperiments,and66for particularprominentnumbers(0,5,10,...),aknowncognitivebias
thetriopoly,composedofuniversitystudents. [10,13].Owingtothemodelâ€™sflexibilityinallowingforarbitrary
priorbeliefs,thiscanbemodelledwithğ‘ withhigherweightings
Results. Theresultsforduopolies(triopolies)arepresentedin ğ‘–
ontheseprominentnumbers.Anexampleoftheresultingdecisions
Fig.4a(Fig.4b).Withtheexperimentaldata,weseeasignificant
whenusingsuchpriorsisshowninFig.4c,providingasignificantly
deviationfromboththerationalbehaviourandthestandardMARL
improvedfit,capturingalloftheexperimentalpeaks.Wedonot
predictions.TheuniqueNEforduopoliesandtriopoliesis20and15
usesuchamodelwhencomparingresultsinTable1,asthismodifi-
respectively.Whiletheseactionsarethemostcommonineachcase,
cationwasmadepost-hoc(afterseeingtheexperimentaldata),but
theseoccurrencescompriseâ‰¤20%ofthetotaldecisions,andthe
itshowstheusefulnessofincorporatingpriorbeliefswhenknown,
remainingâ‰ˆ80%aresub-optimaldecisions(undertheassumption
demonstratinganadditionalstrengthofthemodel.
ofmutualrationality).Again,theproposedmodelisagoodfitforthe
experimentaldatainbothduopoliesandtriopolies,capturingthis
4.2.3 CobwebMarket.
significantdeviationfromtheoptimalchoicewhilestillcapturing
themaximalpeakfromtheexperimentaldata. Description. Inacobwebmarket[35],thereareğ¾producerswho
Undertheprocessingcostconstraint,agentschooseactionspro- mustestimatethepriceğ‘Ë† ğ‘–,ğ‘¡ ofagoodatthenexttimestepğ‘¡.The
portionatetotheexpectedrewardandthelevelofregularisation rewardforaproducerğ‘–isbasedontheaccuracyoftheirprediction
intheirdecisionfunction(theirskilllevel).Thismeansthereare comparedtothemarketpriceğ‘ ğ‘¡:
specificoverrepresentativepeaksintheexperimentaldata,forex-
ample,at15and25intheduopolycaseand20inthetriopolycase, ğ‘ˆ ğ‘–,ğ‘¡ =max(0,1300âˆ’260(ğ‘ ğ‘¡ âˆ’ğ‘Ë† ğ‘–,ğ‘¡)2 ) (11)
stseuqer
fo
noitroporP
noitroporP
noitroporP
noitroporPwhichislowerboundby0,e.g.,theproducerscannotreceivenega- 4.3 KeyTakeaways
tiveutilities.Producershavenocontactwithothers,butattheend Theproposedapproachdemonstratedstrongout-of-sampleper-
ofeachround,producersobservetherealisedmarketpriceğ‘ ğ‘¡. formanceacrossthesethreeeconomicandfinancialenvironments,
Themarketpricedependsonthedemandğ·andsupplycurves outperformingthecomparisonsandvalidatingthemodelincon-
ğ‘†.ğ·islinearwithpriceandissubjecttosmallnormallydistributed trolledenvironments.Specifically,weshowcasedthevalueof:
demandshocksğœ‚ ,andğ‘†non-linearlyincreaseswiththeproducerâ€™s
ğ‘¡
expectedprice,i.e., â€¢ Boundedness:Incorporatingboundedrationalityresulted
insubstantiallyimprovedpredictiveaccuracy(Table1).
ğ·(ğ‘ ğ‘¡)=ğ‘âˆ’ğ‘ğ‘
ğ‘¡
+ğœ‚
ğ‘¡
(12) â€¢ Heterogeneity:Allowingforheterogeneousprocessing
ğ‘†(ğ‘Ë† ğ‘–,ğ‘¡)=tanh(ğœ“(ğ‘Ë†
ğ‘–,ğ‘¡
âˆ’ğ¾))+1
costsimproveduponassumingmutualconsistency(Fig.2b)
whereğœ“ controlsthenon-linearityandstabilityofthemarket.The â€¢ Nonuniformpriors:Arbitrarypriorbeliefsexplained
realisedmarketpriceisgivenby phenomenaincompatiblewithdeviationsfromexpected
utilityalone(Fig.4c)
ğ‘ ğ‘¡ =
ğ‘âˆ’(cid:205) ğ‘˜âˆˆğ¾ğ‘†(ğ‘Ë† ğ‘˜,ğ‘¡)
+ğœ– ğ‘¡ (13) Additionally,whiletheproposedapproachrelaxesthesethreeas-
ğ‘
sumptions,ifdesired,thesecanstillberecoveredasspecial(limit)
whereğœ–
ğ‘¡
âˆğœ‚ ğ‘¡.Underrationalexpectations,producersallpredictthe
casesasdiscussedinSection5.Thebenefitoftheproposedap-
pricetobetheintersectionofğ‘†andğ·,ğ‘âˆ—,e.g.,ğ‘Â¯
ğ‘–,ğ‘¡
=ğ‘âˆ—+ğœ– ğ‘¡,âˆ€ğ‘– âˆˆğ¾,
meaningtherationalpredictionswill,onaverage,fallinlinewith
proachisthattheseassumptionsdonotneedtobeestablisheda
theequilibriumpricewithfluctuationsâˆğœ– ğ‘¡.
priori,rathertheyarecalibratedtotheenvironmentofinterest.
Weutilisetheexperimentaldataof[35],withğ‘=13.8,ğ‘ =1.5,
ğœ– ğ‘¡ âˆ¼ N(0,0.5) andğœ“ = 2.Therewere36participants,generally 5 DISCUSSIONANDRELATIONTO
undergraduateeconomics,psychology,andsciencestudents. EQUILIBRIUMSOLUTIONS
Flexibilityisoneofthemodelâ€™sstrengths.However,thisflexibility
comesattheexpenseofexactanalyticaltractability,andgenerally,
0.15
Rational wearelimitedbythetheoreticalguaranteesoftheunderlyingRL
MARL
algorithm(here,PPO).Despitethis,inthissectionweshowtherela-
Proposed
0.10 Experimental tiontothedecisionfunctionsofotherequilibriumsolutionconcepts
andprovidediscussionsontheequilibriumapproximations.
0.05 QuantalResponseEquilibrium. Withhomogeneousprocessing
costsğœ† ğ‘– = ğœ† anduniformpriorbeliefsğ‘ ğ‘–(ğ‘) = ğ‘,theapproach
canbeseenasapproximatingQRE(asQREconvergestoNEwith
0.00
0 2 4 6 8 10 ğœ†â†’0[29],approximationofNEtoo).Usingasimilarformulation
Realized price p t toSection3.1.1,withQRE,eachagentchoosesğ‘tomaximiseğ‘ˆ,
subjecttoanentropyğ» constraint:
Figure5:Distributionofğ‘ ğ‘¡ incobwebmarkets.Experimental
datafrom[35]isshownasgreybars.Theproposedapproach maxğœ‹ ğ‘–(ğ‘)ğ‘ˆ(ğ‘|ğœ‹ âˆ’ğ‘–)
(14)
isshownwiththeorangeline(foronecalibrationfold).The subjectto ğ»(ğœ‹ ğ‘–) â‰¥ğ» min
standardMARLapproachisshownasthedashedpurpleline,
andtheblacklinedenotestherationalexpectationssolution. whereğœ‹ âˆ’ğ‘– givestheactionprofileoftheotheragents.Toderive
thequantalresponsedecisionfunctionQR,weusethemethod
ğ‘–
Results. ThecobwebmarketresultsarevisualisedinFig.5,dis- ofLagrangemultipliersandtheprincipleofmaximumentropyto
playingthedistributionofrealisedprices.BoththestandardMARL convertthisintoanunconstrainedoptimisationproblem.Giventhe
approachandtherationalexpectationsarepoorpredictorsofthe usualconstraintsontheprobabilityfunction(thatQR ğ‘–(ğ‘) â‰¥0,âˆ€ğ‘
observedphenomenafromtheexperimentaldata.Whilethemean and(cid:205) ğ‘âˆˆğ´QR ğ‘–(ğ‘)=1),wegetthefollowingLagrangian[18]:
oftheexperimentaldataoftenalignswiththerationalandMARL
case,thedistributionspreadisfarbroader,indicatingpersistent
(cid:32) (cid:33) (cid:32) (cid:33)
excessvolatility,withmuchlargerstandarddeviationsthanthose âˆ‘ï¸ âˆ‘ï¸
expectedundertherationalexpectationshypothesisorMARLap-
L=âˆ’ QR ğ‘–(ğ‘)ğ‘ˆ(ğ‘|QR âˆ’ğ‘–)âˆ’ğœ QR ğ‘–(ğ‘)âˆ’1 +ğœ† ğ»(QR ğ‘–)âˆ’ğ» min
ğ‘âˆˆğ´ ğ‘âˆˆğ´
proach.Thisisanoteworthystylisedfactofmarketsincompatible (15)
withtherationalityassumptionofallagents[34,35].Asexcess wheretakingthefirstorderconditionsandsolvingforQR yields
ğ‘–
volatilityisknowntooccurinmanymarkets[67],understanding
thecausesandbeingabletomodelthisvolatilityisanimportantuse ğ‘’ğ‘ˆ(ğ‘|QR âˆ’ğ‘–)/ğœ†
o thf eA mBM ea. nTh ofe tp hr eop do as te ad aa np dpr thoa ec oh vo eff rae lr ls da ism tru ibch utb ioe ntte or ffi pt r, ic ca ep fltu ur ci tn ug
-
QR ğ‘–(ğ‘)=
(cid:205) ğ‘â€²âˆˆğ´ğ‘’ğ‘ˆ(ğ‘â€²|QR âˆ’ğ‘–)/ğœ†
(16)
ations, reproducing the observed excess price volatility (Fig. 5), TodemonstratethatthedecisionfunctionimpliedbytheD
KL
providinganexplanationoftheendogenousformationofexcess constraintinEq.(4)(withuniformpriorsandhomogenousğœ†)re-
volatilitybasedonboundedrationality. ducestothesamefunctionalformasEq.(16),weget:
noitroporPutiliseinEq.(4)doesnothavethissamedependence,suchestima-
âˆ‘ï¸ ğœ‹(ğ‘|ğ‘ ) âˆ‘ï¸ tionisnotrequired,providinganalternativeformulationallowing
ğ¼(ğœ‹,ğ‘ ,ğ‘)= ğœ‹(ğ‘|ğ‘ )log = ğœ‹(ğ‘|ğ‘ )(log(ğœ‹(ğ‘|ğ‘ ))âˆ’ğ¶) forarbitrarypriorbeliefsğ‘ ,usefulforrepresentingcognitivebiases
ğ‘(ğ‘|ğ‘ ) ğ‘–
ğ‘âˆˆğ´ ğ‘âˆˆğ´ (asdemonstratedinFig.4c)orencodingbehaviouralheuristics.Fur-
(17)
thermore,asdiscussed,weallowforarangeofheterogeneousagent
pluggingintoL
skillslearntthroughregularisedpolicies,andproposeanapproach
(cid:32) (cid:33)
âˆ‘ï¸ âˆ‘ï¸ forefficientlycalibratingthesepolicieswithagentsupertypesand
L=âˆ’ ğœ‹ ğ‘–(ğ‘|ğ‘  ğ‘–)ğ‘ˆ(ğ‘|ğœ‹ âˆ’ğ‘–)âˆ’ğœ ğœ‹ ğ‘–(ğ‘|ğ‘  ğ‘–)âˆ’1 +
sharedpolicylearning,bothyettobeconsidered.
ğ‘âˆˆğ´ ğ‘âˆˆğ´
(18)
(cid:32) (cid:33)
ğœ† âˆ‘ï¸ ğœ‹(ğ‘|ğ‘ )(log(ğœ‹(ğ‘|ğ‘ ))âˆ’ğ¶)âˆ’ğ¼Â¯ 6 CONCLUSIONS
Agent-basedmodelshavemuchpromiseforexplainingcomplex
ğ‘âˆˆğ´
phenomenainabroadrangeofdisciplines.However,akeycriti-
andthedecisionfunctionreducesto:
cismishowthebehaviouralrulesaredefined.Learningrealistic
ğ¶ğ‘’ğ‘ˆ(ğ‘|ğ‘ ğ‘–)/ğœ† ğ‘’ğ‘ˆ(ğ‘|ğ‘ ğ‘–)/ğœ†
behaviouralrulescalibratedtoreal-worldsystemsisessentialtoim-
ğœ‹ ğ‘–(ğ‘|ğ‘  ğ‘–)= = (19)
(cid:205) ğ‘â€²âˆˆğ´ğ¶ğ‘’ğ‘ˆ(ğ‘â€²|ğ‘ ğ‘–)/ğœ† (cid:205) ğ‘â€²âˆˆğ´ğ‘’ğ‘ˆ(ğ‘â€²|ğ‘ ğ‘–)/ğœ† provethemodelsandpromotecontinueduptake.Inthiswork,we
proposedanefficientMARLapproachforinferringthesedecisions
confirmingequivalentfunctionalformstoEq.(16)underuniformity
bycalibratingheterogeneouslyskilledlearningagentstoreal-world
andhomogeneity.ThekeydifferenceisQR dependsdirectlyonthe
ğ‘– systemsthroughsharedpolicylearningandagentsupertypes.
policiesofotheragentsQR ,whereasğœ‹ capturesthisindirectly
âˆ’ğ‘– ğ‘– Undertheproposedapproach,agentspossessdiversestrategic
viathestateğ‘  .
ğ‘–
processing abilities, represented through regularisation in their
TheQREthencorrespondstoafixedpointoftheseQRfunctions
decisionfunction.Thisregularisationisintheformofinformation
[29],assumingthatğœ† ishomogeneousandcommonknowledge
processingcosts,leadingtovaryinglevelsofboundedlyrational
amongtheagents.Incontrast,undertheproposedapproach,rather
strategicbehaviour,dependingonthestrengthofregularisation.
thanexplicitlyattemptingtofindthefixedpointsolution,gradient
Thisagentskillheterogeneityisacriticalaspectofmanysystems
descentandsimulationareusedtofindğœ‹ thatmaximisesğ‘ˆ,witha
ğ‘–
andisadeparturefromtraditionalequilibriumdefinitions.How-
neuralnetworkğ‘“ (withinputsğ‘  includingğ‘,ğœ† ),andnocommon
ğ‘– ğ‘– ğ‘–
ever,wedemonstratethatthisheterogeneitybettercapturesmany
knowledgeofğœ† ğ‘—,ğ‘ ğ‘—,ğ‘— â‰ ğ‘–.Theoutputsofğ‘“ are|ğ´|logits(onefor
phenomena,asdemonstratedunderthevariouslaboratorysettings
eachğ‘âˆˆğ´),whicharepassedthroughasoftmaxfunction,giving
hereandobservedinmanyotherreal-worldsituations.Forexample,
learntpoliciesoftheform:
inmarketsettings,institutionalinvestorsmayhavehigheraccess
ğœ‹Ë† ğ‘–(ğ‘)=
ğ‘’ğ‘“ğ‘–(ğ‘|ğ‘ ğ‘–)
(20) toinformationandmoreextensiveprocessingabilitiesthanretail
(cid:205) ğ‘â€²âˆˆğ´ğ‘’ğ‘“ğ‘–(ğ‘|ğ‘ ğ‘–) investors,alteringtheresultingmarketdynamicsandpotentially
givingrisetobehaviourdeviatingfromthemutuallyconsistent
whereeachagentiscontinuallyattemptingtolearnğ‘“ thatmax-
ğ‘–
imisestheirexpectedrewardfromusingğœ‹Ë† (hereusingPPOwith equilibrium.Relaxingthisstrictnotionofequilibriumallowsmod-
ğ‘–
ellingamuchbroaderrangeofdynamics.
GeneralizedAdvantageEstimation[56]).Ofcourse,exceptinvery
Theproposedapproachdoesnotimposestrictassumptionsonra-
specificsettings[69],wedonothavegeneralconvergenceguaran-
tionality,mutualconsistency,orhomogeneitybutinsteadsimulates
tees,sowesaytheproposedapproachapproximatestheseequilibria.
theemergentoutcomesthroughlearningamongtheinteracting
Thebenefitoftheproposedapproachistheflexibilityof ğ‘“ in
agents.Whiletheseassumptionsarenotimposed,theycanbere-
allowingforvariousbehavioursfromheterogenousagents(e.g.,
coveredasspecialcasesoftheproposedapproach,eliminatingthe
varyingğœ† andğ‘ )andcomputabilitywhenderivingtheequilibria
ğ‘– ğ‘–
wouldotherwisebeintractable,suchaswhenğœ† andğ‘ arenot
requirementofdeterminingwhichfeaturesarerelevantapriori.
ğ‘– ğ‘–
Weevaluatedtheproposedapproachinvariouseconomicenviron-
commonknowledge.Whenallowingheterogeneousğœ† andğ‘ ,we
ğ‘– ğ‘–
ments,demonstratingimprovedout-of-samplepredictiveaccuracy
approximateaSubjectiveHeterogeneousQuantalResponseEqui-
comparedtoexistingstate-of-the-artMARLmethods(PPO)and
librium[53],atypeofBayesianequilibrium[24],whereagentsmay
analyticallyderivedequilibriumsolutions.Thisworkprovidesa
havedifferent(potentiallyincorrect)subjectivebeliefsaboutthe
valuabletoolformodellingcomplexsocialsystemsandcalibrating
typedistributionsoftheotheragents(inthiscase,thevaluesofğœ†
ğ‘–
thesemodelstoreal-worlddynamics,particularlywhenanalytical
andğ‘ inthepopulation).
ğ‘–
approachesbecomeintractable,settingthefoundationformore
RelationtoRationalInattention. AsmentionedinSection2,the advancedsimulations,e.g.limitorderbooks[42].
keyrelevantworkinthisareais[46].Whileweshareasimilargoal,
ourworkdiffersinsomeimportantways.[46]requiresestimating
themutualinformation(MI)forprocessingcostsusingaseparate
estimationengine.MIisdefinedoverthejointprobabilitiesas:
âˆ‘ï¸ ğ‘(ğ‘,ğ‘  ğ‘–)
ğ‘€ğ¼ =âˆ’ ğ‘(ğ‘,ğ‘  ğ‘–)log (21)
ğ‘(ğ‘  ğ‘–)ğ‘(ğ‘)
ğ‘âˆˆğ´
whichhasadependenceontheunconditionalğ‘(ğ‘)whichmustbe
solvedwithapproximationtechniques[18].AsthedivergenceweDISCLAIMER [18] BenjaminPatrickEvansandMikhailProkopenko.2021.Amaximumentropy
modelofboundedrationaldecision-makingwithpriorbeliefsandmarketfeed-
ThispaperwaspreparedforinformationalpurposesbytheArtifi-
back.Entropy23,6(2021),669.
cialIntelligenceResearchgroupofJPMorganChase&Coandits [19] BenjaminPatrickEvansandMikhailProkopenko.2023.Boundedrationalityfor
affiliates(â€œJ.P.Morganâ€)andisnotaproductoftheResearchDe- relaxingbestresponseandmutualconsistency:thequantalhierarchymodelof
decisionmaking.TheoryandDecision(17May2023). https://doi.org/10.1007/
partmentofJ.P.Morgan.J.P.Morganmakesnorepresentationand
s11238-023-09941-z
warrantywhatsoeveranddisclaimsallliability,forthecomplete- [20] BenjaminEysenbachandSergeyLevine.2019.Ifmaxentrlistheanswer,what
ness,accuracyorreliabilityoftheinformationcontainedherein. isthequestion?arXivpreprintarXiv:1910.01913(2019).
[21] BenjaminEysenbachandSergeyLevine.2022. MaximumEntropyRL(Prov-
Thisdocumentisnotintendedasinvestmentresearchorinvest- ably)SolvesSomeRobustRLProblems.InInternationalConferenceonLearning
ment advice, or a recommendation, offer or solicitation for the Representations. https://openreview.net/forum?id=PtSAD3caaA2
[22] WilliamFedus,CarlesGelada,YoshuaBengio,MarcGBellemare,andHugo
purchaseorsaleofanysecurity,financialinstrument,financial
Larochelle.2019.Hyperbolicdiscountingandlearningovermultiplehorizons.
productorservice,ortobeusedinanywayforevaluatingthe arXivpreprintarXiv:1902.06865(2019).
meritsofparticipatinginanytransaction,andshallnotconstitute [23] L.E.FourakerandS.Siegel.1963.BargainingBehavior.McGraw-Hill.
[24] JohnGeanakoplos.1994.Commonknowledge.Handbookofgametheorywith
asolicitationunderanyjurisdictionortoanyperson,ifsuchsolici-
economicapplications2(1994),1437â€“1496.
tationundersuchjurisdictionortosuchpersonwouldbeunlawful. [25] JohnGeanakoplos,RobertAxtell,DoyneJFarmer,PeterHowitt,BenjaminConlee,
Â©2024JPMorganChase&Co.Allrightsreserved. JonathanGoldstein,MatthewHendrey,NathanMPalmer,andChun-YiYang.
2012.Gettingatsystemicriskviaanagent-basedmodelofthehousingmarket.
AmericanEconomicReview102,3(2012),53â€“58.
REFERENCES [26] GerdGigerenzer.2020.Whatisboundedrationality?InRoutledgehandbookof
boundedrationality.Routledge,55â€“69.
[1] LiAn,VolkerGrimm,YuBai,AbigailSullivan,BLTurnerII,NicolasMalleson,
[27] GerdGigerenzerandWolfgangGaissmaier.2011. Heuristicdecisionmaking.
AlisonHeppenstall,ChristianVincenot,DerekRobinson,XinyueYe,etal.2023.
Annualreviewofpsychology62(2011),451â€“482.
Modelingagentdecisionandbehaviorinthelightofdatascienceandartificial
[28] JacobKGoereeandCharlesAHolt.2004.Amodelofnoisyintrospection.Games
intelligence.EnvironmentalModelling&Software(2023),105713.
andEconomicBehavior46,2(2004),365â€“382.
[2] LiAn,VolkerGrimm,AbigailSullivan,BLTurnerIi,NicolasMalleson,Alison
[29] JacobKGoeree,CharlesAHolt,andThomasRPalfrey.2020.Stochasticgame
Heppenstall,ChristianVincenot,DerekRobinson,XinyueYe,JianguoLiu,etal.
theoryforsocialscience:Aprimeronquantalresponseequilibrium.Handbook
2021. Challenges,tasks,andopportunitiesinmodelingagent-basedcomplex
ofExperimentalGameTheory(2020),8â€“47.
systems.EcologicalModelling457(2021),109685.
[30] RussellGolman.2011.Quantalresponseequilibriawithheterogeneousagents.
[3] LeoArdon,JaredVann,DeepekaGarg,ThomasSpooner,andSumitraGanesh.
JournalofEconomicTheory146,5(2011),2013â€“2028.
2023.Phantom-ARL-drivenMulti-AgentFrameworktoModelComplexSystems.
[31] TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine.2018.Soft
InProceedingsofthe2023InternationalConferenceonAutonomousAgentsand
actor-critic:Off-policymaximumentropydeepreinforcementlearningwitha
MultiagentSystems.2742â€“2744.
stochasticactor.InInternationalconferenceonmachinelearning.PMLR,1861â€“
[4] JasminaArifovicandJohnDuffy.2018.Heterogeneousagentmodeling:exper-
1870.
imentalevidence. InHandbookofComputationalEconomics.Vol.4.Elsevier,
[32] Teck-HuaHo,So-EunPark,andXuanmingSu.2021.Abayesianlevel-kmodel
491â€“540.
inn-persongames.ManagementScience67,3(2021),1622â€“1638.
[5] RobertLAxtellandJDoyneFarmer.2022.Agent-basedmodelingineconomics
[33] NicolasHoertel,MartinBlachier,CarlosBlanco,MarkOlfson,MarcMassetti,
andfinance:Past,present,andfuture.JournalofEconomicLiterature(2022).
MarinaSÃ¡nchezRico,FrÃ©dÃ©ricLimosin,andHenriLeleu.2020. Astochastic
[6] BowenBaker,IngmarKanitscheider,TodorMarkov,YiWu,GlennPowell,
agent-basedmodeloftheSARS-CoV-2epidemicinFrance.Naturemedicine26,9
Bob McGrew, and Igor Mordatch. 2020. Emergent Tool Use From Multi-
(2020),1417â€“1421.
AgentAutocurricula.InInternationalConferenceonLearningRepresentations.
[34] CarsHommesandThomasLux.2013. INDIVIDUALEXPECTATIONSAND
https://openreview.net/forum?id=SkxpxJBKwS
AGGREGATE BEHAVIOR IN LEARNING-TO-FORECAST EXPERIMENTS.
[7] Jean-PhilippeBouchaud.2008.Economicsneedsascientificrevolution.Nature
Macroeconomic Dynamics 17, 2 (2013), 373â€“401. https://doi.org/10.1017/
455,7217(2008),1181â€“1181.
S1365100511000162
[8] LuigiCampanaro,DanieleDeMartini,SiddhantGangapurwala,WolfgangMerkt,
[35] CarsHommes,JoepSonnemans,JanTuinstra,andHenkVanDeVelden.2007.
andIoannisHavoutis.2023.Roll-Drop:accountingforobservationnoisewith
Learningincobwebexperiments.MacroeconomicDynamics11,S1(2007),8â€“33.
asingleparameter.InLearningforDynamicsandControlConference.PMLR,
[36] AthulPaulJacob,DavidJWu,GabrieleFarina,AdamLerer,HengyuanHu,Anton
718â€“730.
Bakhtin,JacobAndreas,andNoamBrown.2022.Modelingstrongandhuman-
[9] AndrewCaplin,MarkDean,andDanielMartin.2011. Searchandsatisficing.
likegameplaywithKL-regularizedsearch.InInternationalConferenceonMachine
AmericanEconomicReview101,7(2011),2899â€“2922.
Learning.PMLR,9695â€“9728.
[10] TaoChen.2018.Round-numberbiasesandinformedtradinginglobalmarkets.
[37] ArnoldKling.2018. Agent-basedmodeling:Promisesandpitfalls. https:
JournalofBusinessResearch92(2018),105â€“117.
//www.econlib.org/archives/2011/02/agent-based_mod.html
[11] YefenChen,XuanmingSu,andXiaoboZhao.2012.ModelingBoundedRationality
[38] MaciejÅatek,RLAxtell,andBogumilKaminski.2009. Boundedrationality
inCapacityAllocationGameswiththeQuantalResponseEquilibrium.Manage-
viarecursion.InProceedingsofEighthInternationalConferenceonAutonomous
mentScience58,10(2012),1952â€“1962. https://doi.org/10.1287/mnsc.1120.1531
AgentsandMulti-AgentSystems(AAMAS2009).457â€“464.
arXiv:https://doi.org/10.1287/mnsc.1120.1531
[39] RobertoLeombruniandMatteoRichiardi.2005.Whyareeconomistssceptical
[12] Juin-KuanChong,Teck-HuaHo,andColinCamerer.2016.Ageneralizedcogni-
aboutagent-basedsimulations?PhysicaA:StatisticalMechanicsanditsApplica-
tivehierarchymodelofgames.GamesandEconomicBehavior99(2016),257â€“274.
tions355,1(2005),103â€“109.
[13] BenjaminAConverseandPatrickJDennis.2018. Theroleofâ€œProminent
[40] StevenDLevittandJohnAList.2008.Homoeconomicusevolves.Science319,
Numbersâ€inopennumericaljudgment:Straineddecisionmakerschoosefroma
5865(2008),909â€“910.
limitedsetofaccessiblenumbers.OrganizationalBehaviorandHumanDecision
[41] EricLiang,RichardLiaw,RobertNishihara,PhilippMoritz,RoyFox,KenGold-
Processes147(2018),94â€“107.
berg,JosephGonzalez,MichaelJordan,andIonStoica.2018.RLlib:Abstractions
[14] MoloodAleEbrahimDehkordi,JonasLechner,AminehGhorbani,IgorNikolic,
fordistributedreinforcementlearning.InInternationalconferenceonmachine
EmileChappin,andPaulienHerder.2023.UsingMachineLearningforAgent
learning.PMLR,3053â€“3062.
SpecificationsinAgent-BasedModelsandSimulations:ACriticalReviewand
[42] PenghangLiu,KshamaDwarakanath,andSvitlanaSVyetrenko.2022.Biased
Guidelines.JournalofArtificialSocietiesandSocialSimulation26,1(2023).
orlimited:Modelingsub-rationalhumaninvestorsinfinancialmarkets.arXiv
[15] JanezDemÅ¡ar.2006.Statisticalcomparisonsofclassifiersovermultipledatasets.
preprintarXiv:2210.08569(2022).
TheJournalofMachinelearningresearch7(2006),1â€“30.
[43] RichardDMcKelveyandThomasRPalfrey.1995.Quantalresponseequilibria
[16] Gregdâ€™Eon,SophieGreenwood,KevinLeyton-Brown,andJamesWright.2023.
fornormalformgames.Gamesandeconomicbehavior10,1(1995),6â€“38.
LossFunctionsforBehavioralGameTheory. arXivpreprintarXiv:2306.04778 [44] RichardDMcKelveyandThomasRPalfrey.1998.Quantalresponseequilibria
(2023).
forextensiveformgames.Experimentaleconomics1(1998),9â€“41.
[17] BenjaminEnke,UriGneezy,BrianHall,DavidMartin,VadimNelidov,Theo
[45] RichardDMcKelvey,ThomasRPalfrey,andRobertoAWeber.2000.Theeffects
Offerman,andJeroenVanDeVen.2023.Cognitivebiases:Mistakesormissing
ofpayoffmagnitudeandheterogeneityonbehaviorin2Ã—2gameswithunique
stakes?TheReviewofEconomicsandStatistics105,4(2023),818â€“832.
mixedstrategyequilibria. JournalofEconomicBehavior&Organization42,4
(2000),523â€“548.[46] TongMu,StephanZheng,andAlexanderRTrott.2022. ModelingBounded [59] ChristopherASims.2003. Implicationsofrationalinattention. Journalof
RationalityinMulti-AgentSimulationsUsingRationallyInattentiveReinforce- monetaryEconomics50,3(2003),665â€“690.
mentLearning. TransactionsonMachineLearningResearch(2022). https: [60] SamuelSokota,RyanDâ€™Orazio,JZicoKolter,NicolasLoizou,MarcLanctot,
//openreview.net/forum?id=DY1pMrmDkm IoannisMitliagkas,NoamBrown,andChristianKroer.2023.Aunifiedapproach
[47] PedroAOrtegaandDanielABraun.2013. Thermodynamicsasatheoryof toreinforcementlearning,quantalresponseequilibria,andtwo-playerzero-sum
decision-makingwithinformation-processingcosts. ProceedingsoftheRoyal games.ICLR(2023).
SocietyA:Mathematical,PhysicalandEngineeringSciences469,2153(2013), [61] YunhaoTangandShipraAgrawal.2020.Discretizingcontinuousactionspace
20120683. foron-policyoptimization.InProceedingsoftheAAAIconferenceonArtificial
[48] OsondeAOsoba,RaffaeleVardavas,JustinGrana,RushilZutshi,andAmber Intelligence,Vol.34.5981â€“5988.
Jaycocks.2020.Modelingagentbehaviorsforpolicyanalysisviareinforcement [62] CallumRhysTilbury.2023.ReinforcementLearningforEconomicPolicy:ANew
learning.In202019thIEEEInternationalConferenceonMachineLearningand Frontier? TechnicalReport.
Applications(ICMLA).IEEE,213â€“219. [63] YakupTurgutandCaferErhanBozdag.2023.Aframeworkproposalformachine
[49] BenjaminPatrickEvansandMikhailProkopenko.2023. Boundedstrategic learning-drivenagent-basedmodelsthroughacasestudyanalysis.Simulation
reasoningexplainscrisisemergenceinmulti-agentmarketgames.RoyalSociety ModellingPracticeandTheory123(2023),102707.
OpenScience10,2(2023),221164. [64] ArthurTurrell.2016.Agent-basedmodels:understandingtheeconomyfromthe
[50] JulienPerolat,BartDeVylder,DanielHennes,EugeneTarassov,FlorianStrub, bottomup.BankofEnglandQuarterlyBulletin(2016),Q4.
VincentdeBoer,PaulMuller,JeromeTConnor,NeilBurch,ThomasAnthony, [65] NelsonVadori,LeoArdon,SumitraGanesh,ThomasSpooner,SelimAmrouni,
etal.2022. MasteringthegameofStrategowithmodel-freemultiagentrein- JaredVann,MengdaXu,ZeyuZheng,TuckerBalch,andManuelaVeloso.[n.d.].
forcementlearning.Science378,6623(2022),990â€“996. Towardsmulti-agentreinforcementlearning-drivenover-the-countermarket
[51] AshreetaPrasanna,SaschaHolzhauer,andFriedrichKrebs.2019.Overviewof simulations.MathematicalFinancen/a,n/a([n.d.]). https://doi.org/10.1111/mafi.
machinelearninganddata-drivenmethodsinagent-basedmodelingofenergy 12416arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/mafi.12416
markets.INFORMATIK2019:50JahreGesellschaftfÃ¼rInformatikâ€“InformatikfÃ¼r [66] NelsonVadori,SumitraGanesh,PrashantReddy,andManuelaVeloso.2020.
Gesellschaft(2019). CalibrationofsharedequilibriaingeneralsumpartiallyobservableMarkov
[52] JeevantRampalandFernandoStragliotto.2023.HeterogeneousAgentQuantal games. AdvancesinNeuralInformationProcessingSystems33(2020),14118â€“
ResponseEquilibrium.(2023). 14128.
[53] BrianWRogers,ThomasRPalfrey,andColinFCamerer.2009.Heterogeneous [67] SvitlanaVyetrenko,DavidByrd,NickPetosa,MahmoudMahfouz,DanialDer-
quantalresponseequilibriumandcognitivehierarchies. JournalofEconomic vovic,ManuelaVeloso,andTuckerBalch.2020. Getreal:Realismmetricsfor
Theory144,4(2009),1440â€“1467. robustlimitorderbookmarketsimulations.InProceedingsoftheFirstACM
[54] ArielRubinstein.2003.â€œEconomicsandpsychologyâ€?Thecaseofhyperbolic InternationalConferenceonAIinFinance.1â€“8.
discounting.InternationalEconomicReview44,4(2003),1207â€“1216. [68] YingWen,YaodongYang,andJunWang.2021.ModellingBoundedRationality
[55] EllisScharfenaker.2020.Implicationsofquantalresponsestatisticalequilibrium. inMulti-AgentInteractionsbyGeneralizedRecursiveReasoning.InProceed-
JournalofEconomicDynamicsandControl119(2020),103990. ingsoftheTwenty-NinthInternationalJointConferenceonArtificialIntelligence
[56] JohnSchulman,PhilippMoritz,SergeyLevine,MichaelI.Jordan,andPieter (Yokohama,Yokohama,Japan)(IJCAIâ€™20).Article58,8pages.
Abbeel.2016. High-DimensionalContinuousControlUsingGeneralizedAd- [69] KaiqingZhang,ZhuoranYang,andTamerBasar.2019.Policyoptimizationprov-
vantageEstimation.In4thInternationalConferenceonLearningRepresentations, ablyconvergestoNashequilibriainzero-sumlinearquadraticgames.Advances
ICLR2016,SanJuan,PuertoRico,May2-4,2016,ConferenceTrackProceedings, inNeuralInformationProcessingSystems32(2019).
YoshuaBengioandYannLeCun(Eds.). http://arxiv.org/abs/1506.02438 [70] WeiZhang,AndreaValencia,andNi-BinChang.2021.Synergisticintegration
[57] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. betweenmachinelearningandagent-basedmodeling:Amultidisciplinaryreview.
2017.Proximalpolicyoptimizationalgorithms.arXivpreprintarXiv:1707.06347 IEEETransactionsonNeuralNetworksandLearningSystems(2021).
(2017). [71] StephanZheng,AlexanderTrott,SunilSrinivasa,DavidCParkes,andRichard
[58] HerbertASimon.1979.Rationaldecisionmakinginbusinessorganizations.The Socher.2022. TheAIEconomist:Taxationpolicydesignviatwo-leveldeep
Americaneconomicreview69,4(1979),493â€“513. multiagentreinforcementlearning.Scienceadvances8,18(2022),eabk2607.Worst Worst A TRAINING
EachenvironmentisconfiguredinPhantom[3],withaRLLibback-
end[41].Agentsarestrategicagents,learningviaPPO[57],witha
neuralnetworkwith2hiddenlayers,of64nodesineachlayer,and
discreteordinaldiscreteactionspaces[61].Allotherparameters
keeptheirdefaultvaluesfromRLLib.Toensureequitablecompari-
0.250.5 1.0 2.5 5.010.0 Best 0.250.5 1.0 2.5 5.010.0 Best
Âµ Âµ son,theproposedapproachandthestandardMARLalgorithmuse
thesamehyperparameters,observationspaces,andactionspaces,
(a)SupplyChain (b)Duopoly
andthetrainingprocessisexecutedforanidenticalnumberof
Worst Worst iterations(500)acrossbothapproaches,ensuringampletimefor
convergence,asdemonstratedinFig.A.6.
1.0
0.250.5 1.0 2.5 5.010.0 Best 0.250.5 1.0 2.5 5.010.0 Best
Âµ Âµ 0.8
(c)Triopoly (d)Cobweb
0.6
FigureA.7:CalibrationResultsforvaluesofğœ‡andğœâˆ—
0.4
Worst Worst
0.2 Supply Chain
Oligopoly
Cobweb
0.0
0 100 200 300 400 500
Training Iterations
Best Best
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Ïƒâˆ— Ïƒâˆ—
FigureA.6:Trainingconvergence
(a)SupplyChain (b)Duopoly
Worst Worst
A.1 Calibration
Fortheproposedapproach,duetotheprincipleofinsufficientrea-
son,weassumeuniformpriorbeliefsamongtheagents.Although
Best 0.00 0.25 0.50 0.75 1.00 Best 0.00 0.25 0.50 0.75 1.00 weprovidediscussionwithvaryingpriorstoshowtheflexibility
Ïƒâˆ— Ïƒâˆ—
oftheproposedapproach(e.g.Fig.4c),wedonotusethesefor
(c)Triopoly (d)Cobweb comparisonduetopotentialleakingeffectsfromsettingpriorsafter
observingdata.Wedonotcalibrateoralterthepriors,butshow
FigureA.8:CalibrationResultsforvaluesofğœâˆ—(averaging
thepossibilityandbenefitofdoingso.
acrossvaluesofğœ‡).Thelowertherank,thebetter.
Wecalibrateğœ‡,ğœfrom
ğœ‡ âˆˆ{0,0.25,0.5,1,2.5,5,10}
Worst Worst
ğœâˆ— âˆˆ{0,0.05,0.1,0.25,0.5,1}
whereğœ =ğœ‡Ã—ğœâˆ—.Werestrictğœâˆ— â‰¤1aswearedealingwithnormal
distributionsanddonotwantnegativeprocessingpenalties(ğœ† <0
ğ‘–
Best Best
0.0 2.5 5 Âµ.0 7.5 10.0 0.0 2.5 5 Âµ.0 7.5 10.0 isclippedatğœ†
ğ‘–
=0).Thecalibrationistheresultofthelowestmean
squarederroronthetrainingfold.Thetestingfoldsareneverused
(a)SupplyChain (b)Duopoly forcalibration.
Worst Worst
A.1.1 CalibrationResults. Wevisualisetheresultsofthecalibra-
tioninFig.A.7.Toanalysetheimpactofeachparameterindividu-
ally,wepresenttheresultsforafixedvaluewhileaveragingacross
theotherparameterrangeinFigs.A.8andA.9.
Best Best
0.0 2.5 5.0 7.5 10.0 0.0 2.5 5.0 7.5 10.0
Âµ Âµ
(c)Triopoly (d)Cobweb
Figure A.9: Calibration Results for values of ğœ‡ (averaging
acrossvaluesofğœâˆ—).Thelowertherank,thebetter.
âˆ—Ïƒ
âˆ—Ïƒ
knaR
naeM
knaR
naeM
knaR
naeM
knaR
naeM
0.050.01.052.05.0
0.1
0.050.01.052.05.0
0.1
âˆ—Ïƒ
âˆ—Ïƒ
knaR
naeM
knaR
naeM
knaR
naeM
knaR
naeM
0.050.01.052.05.0
0.1
0.050.01.052.05.0
0.1
drawer
desilamroN