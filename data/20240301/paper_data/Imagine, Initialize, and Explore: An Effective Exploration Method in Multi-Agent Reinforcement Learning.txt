Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent
Reinforcement Learning
ZeyangLiu,LipengWan,XinruiYang,ZhuoranChen,XingyuChen,XuguangLan*
NationalKeyLaboratoryofHuman-MachineHybridAugmentedIntelligence
NationalEngineeringResearchCenterforVisualInformationandApplication
InstituteofArtificialIntelligenceandRobotics,Xiâ€™anJiaotongUniversity,Xiâ€™an,China,710049
{zeyang.liu,wanlipeng,xinrui.yang,zhuoran.chen}@stu.xjtu.edu.cn,xingyuchen1990@gmail.com,xglan@mail.xjtu.edu.cn
Abstract paradigm,valuedecompositionmethods(Rashidetal.2018;
Son et al. 2019; Rashid et al. 2020) factorize the joint Q-
Effectiveexplorationiscrucialtodiscoveringoptimalstrate-
value as a function of individual utility functions, ensuring
giesformulti-agentreinforcementlearning(MARL)incom-
consistencybetweenthecentralizedpolicyandtheindivid-
plexcoordinationtasks.Existingmethodsmainlyutilizein-
ualpolicies.Consequently,theyhaveachievedstate-of-the-
trinsicrewardstoenablecommittedexplorationoruserole-
artperformanceinchallengingtasks,suchasStarCraftunit
based learning for decomposing joint action spaces instead
ofdirectlyconductingacollectivesearchintheentireaction- micromanagement(Samvelyanetal.2019).
observation space. However, they often face challenges ob- Despite their success, the simple Ïµ-greedy exploration
taining specific joint action sequences to reach success- strategy used in these methods has been found ineffective
ful states in long-horizon tasks. To address this limitation, insolvingcoordinationtaskswithcomplexstateandreward
we propose Imagine, Initialize, and Explore (IIE), a novel
transitions(Mahajanetal.2019;Wangetal.2020c;Zheng
method that offers a promising solution for efficient multi-
et al. 2021). To address this limitation, MAVEN (Mahajan
agentexplorationincomplexscenarios.IIEemploysatrans-
et al. 2019) adopts a latent space for hierarchical control,
formermodeltoimaginehowtheagentsreachacriticalstate
allowing agents to condition their behavior on shared la-
that can influence each otherâ€™s transition functions. Then,
we initialize the environment at this state using a simulator tentvariablesandenablingcommittedexploration.EITIand
beforetheexplorationphase.Weformulatetheimagination EDTI(Wangetal.2020c)quantifyandcharacterizethein-
as a sequence modeling problem, where the states, obser- fluenceofoneagentâ€™sbehavioronothersusingmutualinfor-
vations,prompts,actions,andrewardsarepredictedautore- mationandthedifferenceofexpectedreturns,respectively.
gressively.Thepromptconsistsoftimestep-to-go,return-to- ByoptimizingEITIorEDTIobjectivesasintrinsicrewards,
go, influence value, and one-shot demonstration, specifying agents are encouraged to coordinate their exploration and
the desired state and trajectory as well as guiding the ac-
learnpoliciesthatoptimizeteamperformance.EMC(Zheng
tion generation. By initializing agents at the critical states,
etal.2021),ontheotherhand,usespredictionerrorsofindi-
IIE significantly increases the likelihood of discovering po-
vidualQ-valuestocapturethenoveltyofstatesandtheinflu-
tentially important under-explored regions. Despite its sim-
ence from other agents for coordinated exploration. Unfor-
plicity, empirical results demonstrate that our method out-
performs multi-agent exploration baselines on the StarCraft tunately, these methods suffer from the exponential growth
Multi-AgentChallenge(SMAC)andSMACv2environments. of the action-observation space with the number of agents
Particularly,IIEshowsimprovedperformanceinthesparse- andbecomeinefficientinlong-horizontasks.
reward SMAC tasks and produces more effective curricula It is possible to decompose the complex task rather than
overtheinitializedstatesthanothergenerativemethods,such
directly conducting collective searches across the entire
asCVAE-GANanddiffusionmodels.
action-observation space. To this end, RODE (Wang et al.
2020b) decomposes joint action spaces into restricted role
Introduction action spaces by clustering actions based on their effects
on the environment and other agents. Low-level role-based
Recent progress in cooperative multi-agent reinforcement
policies explore and learn within these restricted action-
learning (MARL) has shown attractive prospects for real-
observation spaces, reducing execution and training com-
worldapplications,suchasautonomousdriving(Zhouetal.
plexity.However,RODEstillstruggleswithlong-termtasks
2021) and active voltage control on power distribution net-
asitisstillunlikelytoobtainalongsequenceofspecificac-
works (Wang et al. 2021). To utilize global information
tionstoachievesuccessfulstates.TheGo-Explorefamilyof
during training and maintain scalability in execution, cen-
algorithms(Ecoffetetal.2019;Guoetal.2020;Ecoffetetal.
tralized training with decentralized execution (CTDE) has
2021) decomposes the exploration into two phases: return-
become a widely adopted paradigm in MARL. Under this
ingtothepreviousstateandthenstartingtoexplore.These
methodsstorethehigh-scoringtrajectoriesinanarchiveand
*Correspondingauthor.
CopyrightÂ©2024,AssociationfortheAdvancementofArtificial returntosampledstatesfromthisarchivebyrunningagoal-
Intelligence(www.aaai.org).Allrightsreserved. conditionedpolicy.However,inthemulti-agentfield,agents
4202
beF
82
]GL.sc[
1v87971.2042:viXraEnvironment Simulator
Reset Reset ğ‘ ğ“£ Initialize
Explorefromğ’”ğŸ ğ’”ğŸ Imagined trajectory Explorefrom ğ’”ğ“£
ğ‘¥0:ğ‘‡
ğ’”ğŸâ€²
Promğ‘  p0
t
ğ’”ğŸâ€²
ğ‘¥ ğ’¯â€² :ğ‘‡â€²
Decentralized execution Generator â€¦ ğ‘¥0:ğ’¯âˆ’1
ğœğ‘– MLP GRU MLP ğ‘„ğ‘– ğ“Ÿ(ğ’”ğŸâ€²)
Few-shot ğ’”ğ“£â€² ğ‘¥0:ğ‘‡â€²
Demonstrations
Buffer
Centralized training ğ“Ÿğ’”ğŸâ€² â‹ƒğ’…ğ’†ğ’ğ’ Imagine
ğ‘„ â€¦ ğ‘„1
ğ‘›
miM xio nn go nt eo tn wic
o rk
ğ‘„ğ‘¡ğ‘œğ‘¡
â€¦ ğ’ğŸ ğ’«(ğ‘ 0)
ğ’–0Imagi ğ‘Ÿn 0ation
ğ‘ 
1mod â€¦el
ğ’ğ’¯ ğ’«(ğ‘ ğ’¯) ğ’–ğ’¯ ğ‘Ÿğ’¯
EC xo pn lc oa rate tin oa nt e
p hase
Training phase
Prompt-based Causal Transformer
Buffer
ğ‘¥0:ğ‘¡ğ‘˜ğ‘˜
ğ’…ğ’†ğ’ğ’ ğ‘ 0 ğ’ğŸ ğ’«(ğ‘ 0) ğ’–0 ğ‘Ÿ0
â€¦
ğ‘ ğ’¯ ğ’ğ’¯ ğ’«(ğ‘ ğ’¯) ğ’–ğ’¯
Token embeddings
Figure 1: An overview of Imagine, Initialize, and Explore. In the pretraining phase, individual agents collect data from the
initial state s provided by the environment simulator. The interaction sequence is divided into several trajectory segments
0
usinginfluencevalues,whichserveasthetrainingdatasetfortheimaginationmodelandfew-shotdemonstrations.Givens ,
0
thepromptgeneratoristrainedtoproducecriticalstates,andtheimaginationmodellearnstopredicthowtoreachsuchcritical
states from s . After pretraining, the imagination model generates a trajectory from the initial state s to a critical state s
0 0 T
conditioned on P(s ) sampled from the prompt generator, and the most related trajectory from the few-shot demonstration
0
dataset.Theagentsareinitializedats bytheenvironmentsimulatorandtheninteractwiththeenvironmentusingtheÏµ-greedy
T
strategy.Weconcatenatetheimaginedandtheexploredtrajectorytotrainthejointpolicyinthecentralizedtrainingphase.
should be encouraged to teleport to the states where inter- explorationphase,theagentsaregivenanewinitialstateand
actions happen and may lead to critical under-explored re- few-shotdemonstrationstoconstructthepromptandgener-
gions. These approaches only consider the sparse and de- atetheimaginedtrajectory.Theagentsoperatinginpartially
ceptive rewards in single-agent settings, making them im- observable environments can benefit from this imagination
practicalforMARLwithcomplexrewardandtransitionde- byutilizingittoinitializetherecurrentneuralnetworkstate.
pendenciesamongcooperativeagents. Then,theagentsareinitializedtothelaststateoftheimag-
Teleportingagentstointeractionstatesthatinfluenceeach ined trajectory by the simulator and interact with the envi-
otherâ€™stransitionfunctioncansignificantlyincreasethelike- ronment to explore. IIE gradually provides high-influence
lihoodofdiscoveringpotentiallyimportantyetrarelyvisited startingpointsthatcanbeviewedasauto-curriculathathelp
statesandreducetheexplorationspace.However,thereof- agents collect crucial under-explored regions. To make the
tenexistmultiplefeasiblebutinefficienttrajectoriestoreach bestuseoftheimagination,wealsostitchitwiththeinter-
suchinteractionstatesduetotheabundanceofagentsâ€™tac- actionsequencefromexplorationforpolicytraining.
ticsandthecompositionalnatureoftheirfunctionalities.In The main contributions of this paper are threefold: First,
lightofthis,weproposeanovelMARLexplorationmethod it introduces Imagine, Initialize, and Explore, which lever-
named Imagine, Initialize, and Explore (IIE). It leverages agestheGPTarchitecturetoimaginehowtheagentsreach
the GPT architecture (Radford et al. 2018) to imagine tra- critical states before exploration. This method bridges se-
jectoriesfromtheinitialstatetointeractionstates,actingas quence modeling and transformers with MARL instead of
apowerfulâ€œmemorizationengineâ€thatcangeneratediverse usingGPTasareplacementforreinforcementlearningalgo-
agentbehaviors.Weusetargettimestep-to-go,return-to-go, rithms(Chenetal.2021).Second,empiricalresultsdemon-
influence value, and a one-shot demonstration as prompts strate significant performance improvements of IIE on par-
to specify the â€œpathâ€ of the imagined trajectory. The in- tially observable MARL benchmarks, including StarCraft
fluence value is an advantage function that compares an Multi-Agent Challenge (SMAC) with dense and sparse re-
agentâ€™s current action Q-value to a counterfactual baseline wardsettingsaswellasSMACv2.Andthird,guidedbythe
that marginalizes this agent. Specifically, we obtain sev- target timestep-to-go, return-to-go, influence value, and a
eraltrajectorysegmentsbydividingtheexplorationepisode, one-shotdemonstration,theimaginationmodelcanproduce
where each segment starts at the initial state from the en- more effective curricula and outperforms behavior cloning,
vironment and ends at an interaction state with the high- CVAE-GAN,andclassifier-guideddiffusion.
est influence value. Then, the GPT model learns to predict
states,observations,prompts,actions,andrewardsinanau- Background
toregressivemanneronthesesegments. Decentralized Partially Observable Markov Decision
Fig.1presentsanoverviewofIIEarchitecture.Beforethe Process. A fully cooperative multi-agent task in the par-tially observable setting can be formulated as a Decentral- where t represents the timestep, n is the number of agents,
ized Partially Observable Markov Decision Process (Dec- andP(s )isthepromptforactiongenerationatthestates ,
t t
POMDP)(OliehoekandAmato2016),consistingofatuple withthedefinitionprovidedinthenextsubsection.
G = âŸ¨A,S,â„¦,O,U,P,r,Î³âŸ©, where a âˆˆ A â‰¡ {1,...,n} Weobtainthetokenembeddingsforstates,observations,
is a set of agents, S is a set of states, and â„¦ is a set of prompts, actions, and rewards through a linear layer fol-
joint observations. At each time step, each agent obtains lowedbylayernormalization.Moreover,anembeddingfor
its observation o âˆˆ â„¦ based on the observation function eachtimestepislearnedandaddedtoeachtoken.Thetrans-
O(s,a) : S Ã—A â†’ â„¦, and an action-observation history formermodelprocessesthetokensandperformsautoregres-
Ï„ âˆˆ T â‰¡ (â„¦ Ã— U)âˆ—. Each agent a chooses an action sivemodelingtopredictfuturetokens.
a
u a âˆˆU byastochasticpolicyÏ€ a(u a|Ï„ a):T Ã—U â†’[0,1], The imagination model is trained to focus on interaction
whichformsajointactionuâˆˆU.Itresultsinajointreward stateswheretheagentscaninfluenceeachotherâ€™stransition
r(s,u)andatransittothenextstatesâ€² âˆ¼P(Â·|s,u).Thefor- function by prioritizing trajectories with a high-influence
malobjectivefunctionistofindthejointpolicyÏ€thatmaxi- last state in the sampled batch. Specifically, we split inter-
mizesajointaction-valuefunctionQÏ€(s t,u t)=r(s t,u t)+ actionsequencesfromthesampledbatchintoK segments,
Î³E sâ€²[VÏ€(sâ€²)], where VÏ€(s) = E[(cid:80)âˆ t=0Î³tr t|s 0 =s,Ï€], startingfromtheinitialstatebythesimulatorandendingat
andÎ³ âˆˆ[0,1)isadiscountedfactor. thestateswithtop-Khigh-influencelevels.Toensuregener-
alizationacrosstheentireexploredregions,wealsoenforce
CentralizedTrainingwithDecentralizedExecution. In
all state-action pairs of the sampled batch to be assigned a
this paradigm, agentsâ€™ policies are trained with access to
minimum probability of Î», where Î» is a hyperparameter,
global information in a centralized way and executed only N
andN isthenumberofstate-actionpairsinthebatch.The
based on local histories in a decentralized way (Kraemer
influence I is defined as an advantage function that com-
andBanerjee2016).Oneofthemostsignificantchallenges
parestheQ-valueforthecurrentactionua toacounterfac-
istoguaranteetheconsistencybetweentheindividualpoli-
tualbaseline,marginalizingoutuaâˆ—atagivenstates:
cies and the centralized policy, which is also known as
Individual-GlobalMax(Sonetal.2019): I(s)=max(cid:8) Q (s,Ï„,u)âˆ’E Q (cid:0) s,Ï„,(uaâˆ—,uâˆ’a)(cid:1)(cid:9) ,
j uaâˆ— j
ï£± argmax Q1(s ,u1)ï£¼ aâˆˆA
ï£² u1 t ï£½ (4)
argmaxQ j(s t,u)= ... (1) whereQ (s,Ï„,u)=f (Q1(Ï„1,u1;Î¸),...,Qn(Ï„n,un;Î¸);Ï•)
u ï£³argmax Qn(s ,un)ï£¾ j s
un t representsthejointQ-valuefunction,âˆ’adenotesallagents
To address this problem, QMIX (Rashid et al. 2018) ap- A except agent a, f s denotes the monotonic mixing
plies a state-dependent monotonic mixing network f to network whose non-negative weights are generated by
s
combineper-agentQ-valuefunctionswiththejointQ-value hyper-networksthattakethestateasinput.
function Q (s,u). The restricted space of all Q (s,u) that TheimaginationmodelparameterizedbyÏˆistrainedby:
j j
QMIXcanberepresentedas:
T (cid:20)
Qm :={Q j|Q j =f s(Q1(s,u1),...,Qn(s,un))}, (2) L m =(cid:88) logqÏˆ(s t|x <st)+logqÏˆ(r t|x <rt)
whereQa(s,ua)âˆˆR, âˆ‚fs â©¾0,âˆ€aâˆˆA. t=1 (5)
âˆ‚Qa n (cid:21)
+(cid:88)(cid:0) logqÏˆ(ua|x )+logqÏˆ(oa|s )(cid:1) ,
Method t <ua t t t
a=1
This section presents a novel multi-agent exploration
method,Imagine,Initialize,andExplore,consistingofthree whereT istheepisodelength.Sincetheobservationisonly
key components: (1) the imagination model, which utilizes relatedtothecurrentstateandthevisionrangeoftheagents,
a transformer model to generate the trajectory represent- wefilteroutthehistoricalmemoriesinx < o ta anduses t
inghowagentsreachatargetstateautoregressively,(2)the astheinputoftheobservationmodelqÏˆ(oa t|s t).
prompt generator, which specifies the target state and tra-
jectory for imagination, and (3) the environment simulator, PromptForImagination
whichcanteleporttheagentstoastateinstantly.
Theprimaryobjectiveofourimaginationmodelistoiden-
tify critical states and imagine how to reach them from the
ImaginationModel
initialpoint.However,multiplefeasibletrajectoriesexistto
Ithasbeenfoundthatagentsoperatinginapartiallyobserv-
reach these states due to the diverse tactics and composi-
able environment can benefit from the action-observation
tional nature of agentsâ€™ functionalities. Despite their feasi-
history(HausknechtandStone2015;Karkus,Hsu,andLee
bility, many of these trajectories are highly inefficient for
2017;Rashidetal.2018),e.g.,amodelthathasarecallca-
imagination.Forexample,agentsmaywanderaroundtheir
pabilitysuchasgatedrecurrentunit(GRU).Therefore,itis
initialpointsbeforeengagingtheenemy,decreasingsuccess
necessary to imagine the trajectory from the initial state to
rates within a limited episode length. Therefore, a prompt,
the selected interaction state. We formulate the problem of
servingasaconditionforactiongeneration,isnecessaryto
trajectorygenerationasasequencemodelingtask,wherethe
specifythetargettrajectory.
sequenceshavethefollowingform:
Given the starting state s , we propose a prompt gener-
i
x={...,s ,o1,...,on,P(s ),u1,...,un,r ,st+1,...}, (3) ator PÎ¾(s ) to predict the sequence {s ,I ,T ,R }, where
t t t t t t t i i i i iI istheinfluencevalue,T isthetimestep-to-gorepresent- The individual Q-value functions Qa(oa,Ï„a,ua) are op-
i i
ing how quickly we can achieve the interaction state, and timizedjointlytominimizethefollowingloss:
R
=(cid:80)i+T
r isthereturn-to-go.
i t=i t Tâˆ’1
Asthetrainingdatasetsfortheimaginationmodelcontain min(cid:88) [Q (s ,Ï„ ,u ;Î¸,Ï•)âˆ’y(s ,Ï„ ,u )]2, (7)
amixtureoftrajectorysegments,directlysamplingfromthe Î¸,Ï• j t t t t t t
t=0
promptgeneratorisunlikelytoproducethecriticalstatecon-
sistently.Instead,weaimtocontrolthepromptgeneratorto wherey(s ,Ï„ ,u )=r +Î³max Qâ€²(s ,Ï„ ,u)isthe
t t t t u j t+1 t+1
producefrequentlyvisitedbuthigh-influencestatesandthe target value function, and Qâ€² is the target network whose
j
trajectoryleadingtothem.Tothisend,wesamplethetarget parametersareperiodicallycopiedfromQ .
j
influencevalueaccordingtothelog-probability:
RelatedWork
I =logp(I|s )+Îº(Iâˆ’I )/(I âˆ’I ), (6)
i i low high low
Multi-agent Exploration. Individual exploration suffers
whereÎºisahyperparameterthatcontrolsthepreferenceon
from the inconsistency between local and global informa-
high-influence states, I and I are the lower bound
low high tion as well as the non-stationary problem in multi-agent
andupperboundofI ,respectively.Thetargettimestep-to-
i settings. To address these limitations, Jaques et al. (2019)
goandreturn-to-goareobtainedinthesameway.
introduceintrinsicrewardsbasedonâ€œsocialinfluenceâ€toin-
Inaddition,wealsoprovideaone-shotdemonstrationfor centivizeagentsinselectingactionsthatcaninfluenceother
theimaginationtoavoiditbeingtoofarawayfromthetar- agents. Similarly, Wang et al. (2020c) leverage mutual in-
gettrajectory,alsoknownastheâ€œhallucinationâ€problemin formation to capture the interdependencies of rewards and
the large language model (McKenna et al. 2023; Manakul, transitions, promoting exploration efficiency and facilitat-
Liusie,andGales2023).Specifically,westorethetrajectory ing the policies training. EMC (Zheng et al. 2021) lever-
segments in a few-shot demonstration dataset and use the agespredictionerrorsfromindividualQ-valuesasintrinsic
prompt to characterize the segments. Before imagination, rewards and uses episodic memory to improve coordinated
wesearchforthetrajectorywhosedescriptionhasthehigh- exploration.MAVEN(Mahajanetal.2019)employsahier-
estsimilaritywithcurrentpromptP(s i)andthenprependit archical policy for committed and temporally extended ex-
intotheoriginalinputx.Thisprocessonlyaffectstheinfer- ploration, learning multiple state-action value functions for
ence procedure of the model â€“ training remains unaffected each agent through a shared latent variable. RODE (Wang
and can rely on standard next-token prediction frameworks et al. 2020b) introduces a role selector to enable informed
andinfrastructure. decisions and learn role-based policies in a smaller action
space based on the actionsâ€™ effects. However, learning in
InitializeandExplore long-horizoncoordinationtasksremainschallengingdueto
Beforeexploration,webeginbysamplingapromptP(s )= theexponentialstate-actionspaces.
0
{I,T,R} for imagination given the initial state s from
0 Go-Explore. Go-Explore (Ecoffet et al. 2019) is one of
theenvironmentsimulator.Then,theagentsimaginehowto
the most famous exploration approaches, particularly well-
reachtheinteractionstates inanautoregressivemannerby
T suitedforhard-explorationdomainswithsparseordeceptive
conditioning on the prompt and the most related trajectory
rewards. In the Go-Explore family of algorithms (Ecoffet
from the few-shot demonstration dataset. After rolling out
et al. 2021; Guo et al. 2020), an agent returns to a promis-
eachimaginationstepandobtainingthenextstate,wehave
ingstatewithoutexplorationbyrunningagoal-conditioned
the next prompt through decrementing the target timestep-
policy or restoring the simulator state and then explores
to-go T and return-to-go R by 1 and the predicted reward
from this state. However, these methods are primarily de-
r from the imagination model, respectively. We maintain
t signedforsingle-agentscenarios,neglectingtheinterdepen-
the influence value I as a constant and repeat this process
dencies between agent policies and the strategies of others
untilthetargettimestep-to-goiszero.Theimaginedtrajec-
inMARL.Moreover,thesemethodsrequirerestoringprevi-
tory x = {s ,o ,u ,r ,s ,...,u ,r } is then used to
0:T 0 0 0 0 1 T T ouslyvisitedsimulatorstatesortrainingagoal-conditioned
initializeGRUstateoftheagentnetwork.
policytogenerateactionsforreturning.Thisleadstosignif-
Next, we define a probability Î± of initializing the agents
icantcomputationalcosts,especiallyincoordinationscenar-
at state s to emphasize the importance of the target task.
0 iosinvolvingcomplexobservationsandtransitions.
With the probability 1âˆ’Î±, we initialize the agents at the
last state s of the imagined trajectory using the simula- TransformerModel. Severalworkshaveexploredthein-
T
tor. For StarCraft II, we add a distribution over team unit tegrationoftransformermodelsintoreinforcementlearning
types, start positions, and health points in the reset func- (RL) settings. We classify them into two major categories
tion of the environment simulator. We anneal Î± from 1.0 depending on the usage pattern. The first category focuses
to 0.5 over a fixed number of steps after the pretraining onrepresentingcomponentsinRLalgorithms,suchaspoli-
phase of the imagination model. Finally, the agents inter- ciesandvaluefunctions(Parisottoetal.2020;Parisottoand
actwiththe environmenttocollecttheonlinedata x = Salakhutdinov2021).ThesemethodsrelyonstandardRLal-
T:T
{s ,o ,u ,r ,s ,...,u ,r }.Westitchtheimagined gorithmistoupdatepolicy,wherethetransformeronlypro-
T T T T T+1 T T
(cid:76)
trajectoryandtheonlinedataX =x x astrain- videsthelargerepresentationcapacityandimprovesfeature
0:Tâˆ’1 T:T
ingdatasetsforpolicytraining. extraction.Conversely,thesecondcategoryaimstoreplace , , , , ( (  & & : :   4 4 0 0 , , ; ;  4 4 3 3 / / ( ( ; ;  ( ( 0 0 & &  0 0 $ $ 9 9 ( ( 1 1  5 5 2 2 ' ' ( (  4 4 0 0 , , ; ;  0 0 $ $ 3 3 3 3 2 2
  D   0 0 0    E    K B Y V B  ]   F    F B Y V B   ] J   G    V  ] B Y V B  V  ]
               
           
           
           
           
       
      0   0     0   0     0   0   0   0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  H    P B Y V B  P   I    V B Y V B  ]   J     P B Y V B   P   K   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  L   S U R W R V V B  Y    M   W H U U D Q B  Y    N   ] H U J B  Y    O   S U R W R V V B   Y  
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure2:Performancecomparisonsonthedense-rewardSMACandSMACv2benchmarks.
the RL pipeline with sequence modeling. They autoregres- Eachtaskneedstotrainforabout12to20hours,depending
sively generate states, actions, and rewards by condition- on the number of agents and the episode length limit. We
ingonthedesiredreturn-to-goduringinference(Chenetal. evaluate32episodeswithdecentralizedgreedyactionselec-
2021; Lee et al. 2022; Reed et al. 2022). However, there is tionevery10k timestepsforeachalgorithm.Allfiguresare
ariskofunintendedbehaviorswhenofflinedatasetscontain plotted using mean and standard deviation with confidence
destructivebiases.Moreover,itremainsanopenquestionof internal95%.Weconductfiveindependentrunswithdiffer-
howtoextendthesesupervisedlearningparadigmstoonline entrandomseedsforeachlearningcurve.
settingsandsatisfythescalabilityinpractice.
We utilize a transformer model to imagine the trajectory PerformanceComparison
toreachtheinteractionstateguidedbyaprompt,offeringin- In our evaluation, we compare the performance of CW-
creased representational capacity and stability compared to QMIX (Rashid et al. 2020), QPLEX (Wang et al. 2020a),
existing Go-Explore methods. In contrast to current multi- MAVEN (Mahajan et al. 2019), EMC (Zheng et al. 2021),
agentexplorationmethods,weinitializeagentsatthestates RODE (Wang et al. 2020b), QMIX (Rashid et al. 2018),
withhighinfluencevalues.Thisformofcurriculumlearning MAPPO (Yu et al. 2022), and IIE on the StarCraft Multi-
significantlyreducestheexplorationspaceandenhancesco- Agent Challenge (SMAC) (Samvelyan et al. 2019) and
ordinationexploration.Weaimtobridgesequencemodeling SMACv2 (Ellisetal.2022)benchmarks.
andtransformerswithMARLratherthanreplacingconven- SMACisapartiallyobservableMARLbenchmarkknown
tionalRLalgorithms. foritsrichenvironmentsandhighcontrolcomplexity.Itre-
quireslearningpoliciesinalargeobservation-actionspace,
Results
where agents take various actions, such as â€œmoveâ€ in car-
Inthissection,weconductempiricalexperimentstoanswer dinal directions, â€œstopâ€, and selecting an enemy to attack.
the following questions: (1) Is Imagine, Initialize, and Ex- The maximum number of actions and the episode length
plore(IIE)betterthantheexistingMARLexplorationmeth- varyacrossdifferentscenarios,rangingfrom7to70actions
ods in complex cooperative scenarios or sparse reward set- and60to400timesteps,respectively.Incontrast,SMACv2
tings? (2) Can IIE generate a reasonable curriculum of the presents additional challenges of stochasticity and general-
last states over timesteps and outperform other returning ization. It includes procedurally generated scenarios with
methods?Wealsoinvestigatethecontributionofeachcom- startpositions,unittypes,attackrange,andsightrange.The
ponentintheproposedprompttotheimaginationmodel. agents must generalize their learned policies to previously
We conduct experiments on NVIDIA RTX 3090 GPUs. unseensettingsduringtesting.
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7 , , , , ( (  / / , , , , 5 5  0 0 $ $ 6 6 ( ( 5 5  ( ( 0 0 & &  0 0 $ $ 9 9 ( ( 1 1  5 5 2 2 ' ' ( (  4 4 0 0 , , ; ;  0 0 $ $ 3 3 3 3 2 2
  D    P   E    P   F    P B Y V B  ]   G    V  ]
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure3:Performancecomparisonsonthesparse-rewardSMACbenchmark.
 , , , , ( (  & & * *   G G L L I I I I X X V V L L R R Q Q  & & 9 9 $ $ ( (   * * $ $ 1 1  * * & &   S S R R O O L L F F \ \  % % & &  4 4 0 0 , , ; ;
  D   0 0 0    E    F B Y V B   ] J   F    K B Y V B  ]   G   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0     0   0   0   0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  H   * H Q H U D W H G B D O O \ B K H D O W K   I   * H Q H U D W H G B H Q H P \ B K H D O W K   J   ' L V W D Q F H   K   7 K H   '  W  6 1 (  H P E H G G L Q J V
             7 D U J H W  W U D M H F W R U \
 , P D J L Q H G  W U D M H F W R U \
          
          
          
          
     
      0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure4:(a-d)PerformancecomparisonswithdifferentreturningmethodsontheSMACbenchmark.(e-g)Themeanhealthof
alliesandenemies,aswellastherelativedistancebetweentwogroupsatthelaststateintheMMM2scenario.(h)The2Dt-SNE
embeddingsofthetrajectoryreturnedfromIIEintheMMM2scenarioafterpretraining.
Fig.2showsthatIIEconsiderablyoutperformsthestate- TheimaginationmodelinIIEcanimproverobustnessin
of-the-art MARL methods in both SMAC and SMACv2 thesesettingsbecauseitmakesminimalassumptionsonthe
mapswiththedenserewardsetting.Thisresultdemonstrates density of the reward. As shown in Fig. 3, sparse and de-
thatIIEsignificantlyenhanceslearningspeedandcoordina- layedrewardsminimallyaffectIIE.Wehypothesizethatthe
tionperformanceincomplextasks.Inspecificscenarioslike transformerarchitectureintheimaginationmodelcanbeef-
3s5z vs 3s6zandcorridor,EMCshowsfasterlearn- fectivecriticsandenablemoreaccuratevalueprediction.In
inginthebeginning,whichcanbeattributedtothefactthat contrast,QMIXandMAPPOfailtosolvethesetaskssince
IIE requires more time to pre-train the imagination model theyheavilyrelyondenselypopulatedrewardsforcalculat-
inmorecomplextasks.However,asthetrainingprogresses, ingtemporaldifferencetargetsorgeneralizedadvantagees-
IIEexcelsinprovidingamoretargetedandefficientexplo- timates. LIIR, MASER, RODE, and MAVEN exhibit slow
rationofcomplexcoordinationscenarios,leadingtothebest learning and instability, particularly on the heterogeneous
finalperformanceacrossallscenarios. map 2s3z, suggesting the difficulty of learning such in-
trinsic rewards, subgoals, roles, or noise-based hierarchical
Sparse-rewardBenchmark policiesinhard-explorationscenarios.
We investigate the performance of IIE, EMC, MAVEN,
DifferentReturningMethods
RODE,QMIX,andMAPPO,inadditiontotwomulti-agent
explorationmethodsdesignedforsparserewards,including In this section, we seek insight into whether the imagina-
LIIR (Du et al. 2019) and MASER (Jeon et al. 2022), on tion model in IIE can be thought of as performing efficient
theSMACbenchmarkwiththesparse-rewardsetting.Inthis curriculumlearningandisbetterthanotherreturningmeth-
setting,globalrewardsaresparselygivenonlywhenoneor ods.Toinvestigatethis,wecompareitwithbehaviorcloning
allenemiesaredefeated,withnoadditionalrewardforstate (BC),agoal-conditionedpolicymethod(GC-policy)(Ecof-
informationsuchasenemyandallyhealth.Thisproblemis fet et al. 2021), and generative models, including CVAE-
difficultforcreditassignmentsbecausecreditmustbeprop- GAN(Baoetal.2017)andclassifier-guideddiffusion(CG-
agatedfromtheendoftheepisode. diffusion)(Ajayetal.2023).Beforeexploration,weinitial-
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   K W O D H +  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   K W O D H +  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
 H F Q D W V L '  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7 , , , , ( (  5 5 7 7 * *  , , 9 9  , , , , ( (   Z Z R R   , , 9 9  , , , , ( (   Z Z R R   7 7  , , , , ( (   Z Z R R   5 5 7 7 * *  4 4 0 0 , , ; ;
  D   0 0 0    E    F B Y V B   ] J   F    K B Y V B  ]   G   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0     0   0   0   0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure5:IIEwithdifferentpromptsontheSMACbenchmark.
ize the agents to the last state of the imagination trajectory DifferentPromptsforImagination
produced by BC, CVAE-GAN, and CG-diffusion using the
In this section, we conduct ablation studies to analyze the
environmentsimulatororrunagoal-conditionedpolicyfrom
contributions of each component in the prompts, including
GC-policytoreturnagentstothetargetstate.
thetimesteps,theaccumulatedreturn-to-go(RTG),thecon-
We show the performance comparison and visualization stant influence value (IV), and the most related trajectory
resultsinFig.4.IIEprovidesagentswithinteractionpoints (T). We integrate the desired timesteps into the following
for exploration with the shortest relative distance between prompts:(1)RTG,(2)IV,(3)RTGwiththeinfluencevalue,
agents and enemies and decreasing but comparable health denotedasIIE-wo-T,(4)RTGwiththemostrelatedtrajec-
levels as training progresses. This contributes to early and tory,denotedasIIE-wo-IV,(5)IVwiththemostrelatedtra-
more frequent multi-agent interactions against the enemies jectory,denotedasIIE-wo-RTG.WecomparethemwithIIE
quickly, reducing the complexity of the multi-agent explo- andQMIXontheSMACbenchmark.
ration space. As a result, IIE outperforms BC, GC-policy,
Fig. 5 shows that RTG and IIE-wo-IV achieve poor per-
CVAE-GAN, and CG-diffusion across all tasks, indicating formanceinMMM2,6h vs 8z,andcorridorduetotheir
that the prompt-based imagination can be more effective
ignorance of the importance of interactions in multi-agent
and has better generalization than simply performing imi-
exploration and limited positive samples for value decom-
tationlearningorothergenerativemodels.Wealsoillustrate
position. IV and IIE-wo-T perform worse than IIE-wo-IV
the data distribution of imagined trajectories in a 2D space
and IIE with a considerable gap, as they do not exploit
withdimensionalreductionviaT-SNE.Theresultsshowthat
one-shot demonstration to guide action generation, lead-
theimaginedtrajectorywillbeclosetothetargettrajectory,
ing to a potential risk that the imagined trajectories may
which verifies that IIE can capture and learn the dynamics
deviate significantly from the target trajectory, especially
ofthemulti-agentenvironmentbasedontheprompt.
in long-horizon tasks with complex transition functions.
CG-diffusion and GC-policy have plateaued in perfor- IIE outperforms IIE-wo-IV across MMM2, 6h vs 8z, and
mance,showingasimilartrendofemergentcomplexitywith 2c vs 64zgmapsbecauseRTGcanfurtherspecifyatra-
IIEbuttakingfarlongertolearnthejointpolicy.Ontheone jectory,improvingtheefficiencyandrobustnessoftheimag-
hand,thecontinuousdiffusionmodelshavebeenextremely ination learning. However, IIE-wo-IV performs better than
successfulinvisionandaudiodomains,buttheydonotper- IIE in corridor, implying that the imagination model
form well in text because of the inherently discrete nature may have some degree of generalization without RTG and
oftext(Lietal.2022).Theimaginationofthetrajectoryis providemorediversesamplesforpolicytraining.
morerelatedtotextthanimagegenerationbecauseithaslow
redundancy,andthetransitionbetweentwoconnectedstates Conclusion
is essential. On theother hand, the applicability of the cur-
rentgoal-conditionedmethodswithcomplexobservationsis WeproposedImagine,Initialize,andExplore,whichenables
limited,astheirflexibilityisoftenconstrainedtotaskspaces ustobreakdownadifficultmulti-agentexplorationproblem
usinglow-dimensionalparametersandrequireswell-defined into a curriculum of subtasks created by initializing agents
task similarity. CVAE-GAN does not show positive results at the interaction state - the last state in the imagined tra-
and keeps generating similar health levels. We hypothesize jectory. We empirically evaluated our algorithm and found
that the mode collapse problem is a bottleneck for CVAE- itoutperformscurrentmulti-agentexplorationmethodsand
GAN. The generator can find data that can easily fool the generative models on various benchmarks. We also show
discriminator becauseof the unbalancedtraining data from thattheprompt-basedimaginationmodelperformsefficient
theexploration.BCshowstheworstresultsbecauseitimi- conditional sequence generation and has one-shot general-
tatesallpastinteractionsequencesandlacksthegeneraliza- ization. We hope this work will inspire more investigation
tion ability to avoid sub-optimal solutions. From Fig. 4e-g, of sequence-prediction modelsâ€™ applications in multi-agent
wecanseethatBCprioritizesstateswithshortdistancesbut reinforcement learning (MARL) rather than using them to
does not provide reasonable health levels for enemies - the replace conventional MARL. In future work, we consider
meanhealthlevelofenemiesfarexceedsthatofallies,mak- learning continuous prompts to specify the imagination, as
ingitdifficultorevenimpossibletoachieveanysuccess. opposedtothesimpleinfluencevalueusedinthispaper.
   H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7Acknowledgments Karkus,P.;Hsu,D.;andLee,W.S.2017. Qmdp-net:Deep
This work was supported in part by National Key R&D learningforplanningunderpartialobservability. Advances
ProgramofChinaundergrantNo.2021ZD0112700,NSFC inneuralinformationprocessingsystems,30.
under grant No. 62125305, No. 62088102, No. 61973246, Kraemer,L.;andBanerjee,B.2016. Multi-agentreinforce-
No. 62203348, and No. U23A20339, the Fundamental Re- mentlearningasarehearsalfordecentralizedplanning.Neu-
search Funds for the Central Universities under Grant rocomputing,190:82â€“94.
xtr072022001.
Lee, K.-H.; Nachum, O.; Yang, M. S.; Lee, L.; Free-
man, D.; Guadarrama, S.; Fischer, I.; Xu, W.; Jang, E.;
References Michalewski, H.; et al. 2022. Multi-game decision trans-
formers. Advances in Neural Information Processing Sys-
Ajay, A.; Du, Y.; Gupta, A.; Tenenbaum, J. B.; Jaakkola,
tems,35:27921â€“27936.
T.S.;andAgrawal,P.2023.IsConditionalGenerativeMod-
eling all you need for Decision Making? In The Eleventh Li, X.; Thickstun, J.; Gulrajani, I.; Liang, P. S.; and
InternationalConferenceonLearningRepresentations. Hashimoto,T.B.2022. Diffusion-lmimprovescontrollable
textgeneration. AdvancesinNeuralInformationProcessing
Bao,J.;Chen,D.;Wen,F.;Li,H.;andHua,G.2017.CVAE-
Systems,35:4328â€“4343.
GAN: fine-grained image generation through asymmetric
training. In Proceedings of the IEEE international confer- Mahajan, A.; Rashid, T.; Samvelyan, M.; and Whiteson, S.
enceoncomputervision,2745â€“2754. 2019.Maven:Multi-agentvariationalexploration.Advances
Chen, L.; Lu, K.; Rajeswaran, A.; Lee, K.; Grover, A.; inNeuralInformationProcessingSystems,32.
Laskin,M.;Abbeel,P.;Srinivas,A.;andMordatch,I.2021. Manakul, P.; Liusie, A.; and Gales, M. J. 2023. Self-
Decisiontransformer:Reinforcementlearningviasequence checkgpt: Zero-resource black-box hallucination detection
modeling. Advances in neural information processing sys- for generative large language models. arXiv preprint
tems,34:15084â€“15097. arXiv:2303.08896.
Du,Y.;Han,L.;Fang,M.;Liu,J.;Dai,T.;andTao,D.2019.
McKenna, N.; Li, T.; Cheng, L.; Hosseini, M. J.; Johnson,
Liir:Learningindividualintrinsicrewardinmulti-agentre-
M.; and Steedman, M. 2023. Sources of Hallucination by
inforcementlearning. AdvancesinNeuralInformationPro-
LargeLanguageModelsonInferenceTasks. arXivpreprint
cessingSystems,32.
arXiv:2305.14552.
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
Oliehoek,F.A.;andAmato,C.2016.Aconciseintroduction
Clune, J. 2019. Go-explore: a new approach for hard-
todecentralizedPOMDPs. Springer.
explorationproblems. arXivpreprintarXiv:1901.10995.
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and Parisotto, E.; and Salakhutdinov, R. 2021. Efficient Trans-
Clune, J. 2021. First return, then explore. Nature, formers in Reinforcement Learning using Actor-Learner
590(7847):580â€“586. Distillation. InInternationalConferenceonLearningRep-
resentations.
Ellis,B.;Moalla,S.;Samvelyan,M.;Sun,M.;Mahajan,A.;
Foerster, J. N.; and Whiteson, S. 2022. SMACv2: An Im- Parisotto, E.; Song, F.; Rae, J.; Pascanu, R.; Gulcehre, C.;
provedBenchmarkforCooperativeMulti-AgentReinforce- Jayakumar, S.; Jaderberg, M.; Kaufman, R. L.; Clark, A.;
mentLearning. arXivpreprintarXiv:2212.07489. Noury, S.; et al. 2020. Stabilizing transformers for rein-
Guo, Y.; Choi, J.; Moczulski, M.; Feng, S.; Bengio, S.; forcementlearning. InInternationalconferenceonmachine
Norouzi, M.; and Lee, H. 2020. Memory based trajectory- learning,7487â€“7498.PMLR.
conditioned policies for learning from sparse rewards. Ad-
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;
vances in Neural Information Processing Systems, 33:
et al. 2018. Improving language understanding by gener-
4333â€“4345.
ativepre-training. InOpenAI.OpenAI.
Hausknecht, M.; and Stone, P. 2015. Deep recurrent q-
Rashid, T.; Farquhar, G.; Peng, B.; and Whiteson, S.
learning for partially observable mdps. In 2015 aaai fall
2020. Weighted qmix: Expanding monotonic value func-
symposiumseries.
tionfactorisationfordeepmulti-agentreinforcementlearn-
Jaques,N.;Lazaridou,A.;Hughes,E.;Gulcehre,C.;Ortega,
ing.Advancesinneuralinformationprocessingsystems,33:
P.;Strouse,D.;Leibo,J.Z.;andDeFreitas,N.2019. Social
10199â€“10210.
influence as intrinsic motivation for multi-agent deep rein-
forcementlearning. InInternationalconferenceonmachine Rashid,T.;Samvelyan,M.;Schroeder,C.;Farquhar,G.;Fo-
learning,3040â€“3049.PMLR. erster, J.; and Whiteson, S. 2018. Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement
Jeon, J.; Kim, W.; Jung, W.; and Sung, Y. 2022. MASER:
learning. In International Conference on Machine Learn-
Multi-Agent Reinforcement Learning with Subgoals Gen-
ing,4295â€“4304.PMLR.
erated from Experience Replay Buffer. In Chaudhuri, K.;
Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and Sabato, Reed, S.; Zolna, K.; Parisotto, E.; Colmenarejo, S. G.;
S., eds., Proceedings of the 39th International Conference Novikov, A.; Barth-Maron, G.; Gimenez, M.; Sulsky, Y.;
on Machine Learning, volume 162 of Proceedings of Ma- Kay,J.;Springenberg,J.T.;etal.2022. Ageneralistagent.
chineLearningResearch,10041â€“10052.PMLR. arXivpreprintarXiv:2205.06175.Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-
erster,J.;andWhiteson,S.2019. Thestarcraftmulti-agent
challenge. arXivpreprintarXiv:1902.04043.
Son, K.; Kim, D.; Kang, W. J.; Hostallero, D. E.; and Yi,
Y. 2019. Qtran: Learning to factorize with transformation
for cooperative multi-agent reinforcement learning. In In-
ternational Conference on Machine Learning, 5887â€“5896.
PMLR.
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2020a.
QPLEX: Duplex Dueling Multi-Agent Q-Learning. In In-
ternationalConferenceonLearningRepresentations.
Wang,J.;Xu,W.;Gu,Y.;Song,W.;andGreen,T.C.2021.
Multi-agent reinforcement learning for active voltage con-
trol on power distribution networks. Advances in Neural
InformationProcessingSystems,34:3271â€“3284.
Wang, T.; Gupta, T.; Mahajan, A.; Peng, B.; Whiteson, S.;
and Zhang, C. 2020b. Rode: Learning roles to decompose
multi-agenttasks. arXivpreprintarXiv:2010.01523.
Wang,T.;Wang,J.;Wu,Y.;andZhang,C.2020c.Influence-
Based Multi-Agent Exploration. In International Confer-
enceonLearningRepresentations.
Yu,C.;Velu,A.;Vinitsky,E.;Gao,J.;Wang,Y.;Bayen,A.;
and Wu, Y. 2022. The surprising effectiveness of ppo in
cooperative multi-agent games. Advances in Neural Infor-
mationProcessingSystems,35:24611â€“24624.
Zheng,L.;Chen,J.;Wang,J.;He,J.;Hu,Y.;Chen,Y.;Fan,
C.;Gao,Y.;andZhang,C.2021.EpisodicMulti-agentRein-
forcementLearningwithCuriosity-drivenExploration. Ad-
vancesinNeuralInformationProcessingSystems,34.
Zhou, M.; Luo, J.; Villella, J.; Yang, Y.; Rusu, D.; Miao,
J.; Zhang, W.; Alban, M.; FADAKAR, I.; Chen, Z.; et al.
2021.SMARTS:AnOpen-SourceScalableMulti-AgentRL
TrainingSchoolforAutonomousDriving. InConferenceon
RobotLearning,264â€“285.PMLR.