Class-attribute Priors: Adapting Optimization to
Heterogeneity and Fairness Objective
XuechenZhang1,MingchenLi2,JiasiChen2,ChristosThrampoulidis3,SametOymak2
1UniversityofCalifornia,Riverside
2UniversityofMichigan,AnnArbor
3UniversityofBritishColumbia
xzhan394@ucr.edu,milii@umich.edu,jiasi@umich.edu,cthrampo@ece.ubc.ca,oymak@umich.edu
Abstract assigningindividualweightstoclassesduringoptimization.
Therecentproposalsonimbalancedclassification(Lietal.
Modernclassificationproblemsexhibitheterogeneitiesacross
2021;Chawlaetal.2002)canbeviewedasgeneralization
individualclasses:Eachclassmayhaveuniqueattributes,such
of weighting and can be interpreted as developing unique
assamplesize,labelquality,orpredictability(easyvsdiffi-
loss functions for individual classes. More generally, one
cult),andvariableimportanceattest-time.Withoutcare,these
canuseclass-specificdataaugmentationschemes,regular-
heterogeneities impede the learning process, most notably,
whenoptimizingfairnessobjectives.Confirmingthis,under izationorevenoptimizers(e.g.Adam,SGD,etc)toimprove
agaussianmixturesetting,weshowthattheoptimalSVM targettestobjective.Whilepromising,thisapproachsuffers
classifierforbalancedaccuracyneedstobeadaptivetothe whentherearealargenumberofclassesK:naivelylearning
classattributes.ThismotivatesustoproposeCAP:Aneffec- class-specificstrategieswouldrequireO(K)hyperparame-
tiveandgeneralmethodthatgeneratesaclass-specificlearning ters(O(1)strategyhyperparameterperclass).Thisnotonly
strategy(e.g.hyperparameter)basedontheattributesofthat createscomputationalbottlenecksbutalsoraisesconcernsof
class.Thisway,optimizationprocessbetteradaptstohetero-
overfittingfortailclasseswithsmallsamplesize.
geneities. CAP leads to substantial improvements over the
To overcome such bottlenecks, we introduce the Class-
naiveapproachofassigningseparatehyperparameterstoeach
attribute Priors (CAP) approach. Rather than treating hy-
class.WeinstantiateCAPforlossfunctiondesignandpost-hoc
perparametersasfreevariables,CAPisameta-approachthat
logitadjustment,withemphasisonlabel-imbalancedproblems.
WeshowthatCAPiscompetitivewithpriorartanditsflex- treatsthemasafunctionoftheclassattributes.Aswediscuss
ibilityunlocksclearbenefitsforfairnessobjectivesbeyond later,exampleattributesAofaclassincludeitsfrequency,
balancedaccuracy.Finally,weevaluateCAPonproblemswith label-noiselevel,trainingdifficulty,similaritytootherclasses,
labelnoiseaswellasweightedtestobjectivestoshowcase test-timeimportance,andmore.OurprimarygoalwithCAP
howCAPcanjointlyadapttodifferentheterogeneities. isbuildinganattribute-to-hyperparameterfunctionA2H
thatgeneratesclass-specifichyperparametersbasedonthe
attributes associated with that class. This process infuses
1 Introduction
high-levelinformationaboutthedatasettoacceleratethede-
Contemporary machine learning problems arising in natu- signofclass-specificstrategies.TheA2Hmapstheattributes
rallanguageprocessingandcomputervisionofteninvolve Atoaclass-specificstrategyS.Theprimaryadvantageis
largenumberofclassestopredict.Collectinghigh-quality robustnessandsampleefficiencyofA2H,asitrequiresO(1)
trainingdatasetsforalloftheseclassesisnotalwayspossi- hyperparameterstogenerateO(K)strategies.Themaincon-
ble,andrealisticdatasets(Menonetal.2020;Feldman2020; tributionofthisworkisproposingCAPframeworkandinstan-
Hardt,Price,andSrebro2016)sufferfromclass-imbalances, tiatingitforlossfunctiondesignandpost-hocoptimization
missing or noisy labels (among other application-specific whichrevealsitsempiricalbenefits.Specifically,wemake
considerations).Optimizingdesiredaccuracyobjectiveswith thefollowingcontributions:
suchheterogeneitiesposesasignificantchallengeandmo-
1. IntroducingClass-attributePriors(Sec3).Wefirstpro-
tivatesthecontemporaryresearchonimbalancedclassifica-
videtheoreticalevidenceonthebenefitsofusingmultiple
tion,fairness,andweak-supervision.Additionally,besides
attributes (see Fig 2). This motivates CAP: A meta ap-
distributionalheterogeneities,wemighthaveobjectivehet-
proachthatutilizesthehigh-levelattributesofindividual
erogeneity.Forinstance,thetargettestaccuracymaybea
classes to personalize the optimization process. Impor-
particularweightedcombinationofindividualclasses,where
tantly,CAPisparticularlyfavorabletotailclasseswhich
importantclassesareupweighted.
containtoofewexamplestooptimizeindividually.
Aplausibleapproachtoaddressthesedistributionaland
2. CAPimprovesexistingapproaches(Sec4).Byintegrat-
objectiveheterogeneitiesisdesigningoptimizationstrategies
ingCAPwithinexistinglabel-imbalancedtrainingmeth-
thataretailoredtoindividualclasses.Aclassicalexampleis
ods,CAPnotonlyimprovestheirperformancebutalso
CopyrightÂ©2024,AssociationfortheAdvancementofArtificial increases their stability, notably, AutoBalance (Li et al.
Intelligence(www.aaai.org).Allrightsreserved. 2021)andlogit-adjustmentloss(Menonetal.2020).
4202
naJ
52
]GL.sc[
1v34341.1042:viXraC Acla cs us r ad cis ytribution Previous class distribution InstantiationsofCAP:
Target class distribution 1.Bileveloptimizationoflossfunction
2.Post-hocoptimizationoflogits
DistinctFairnessObjectives(Sec4.3):
Sample size & Predictability Sample size & Distribution shift Label quality
Heterogeneous CAP Balancederror
Function QuantilesorConditionalvalueatRisk
Class 1 Attributes S Standarddeviationofclass-conditionalerrors
Weightederror
= Class 2 Dictionary DistinctHeterogeneities(Sec4.2):
Full Dataset â€¦ â€¦ Train Objective Classfrequency
Predictiondifficulty
Class n Variableclassimportance
Labelnoiselevel
Figure1:Lefthandside:CAPviewstheglobaldatasetasacompositionofheterogeneoussub-datasetsinducedbyclasses.Weextract
high-level attributes from these classes and use these attributes to generate class-specific optimization strategies (which correspond to
hyperparameters).Ourproposalisefficientlygeneratingthesehyperparametersbasedonclass-attributesthroughameta-strategy.Righthand
side:WedemonstratethatCAPleadstostate-of-the-artstrategiesforlossfunctiondesignandpost-hocoptimization.CAPcanleveragemultiple
attributestoflexiblyoptimizeavarietyoftestobjectivesunderheterogeneities.
3. CAPadaptstofairnessobjective(Sec4.2).CAPâ€™sflex- weightthelossfunctionsotheoptimizationgeneratesaclass-
ibilityisparticularlypowerfulfornon-standardsettings balancedmodel.Inadditiontothesemethods,(Lietal.2021)
thatpriorworksdonotaccountfor:CAPachievessignif- proposesabileveltrainingschemethatdirectlyoptimizesl
icantimprovementwhenoptimizingfairnessobjectives andâˆ†onasufficientsmallimbalancedvalidationdatawith-
otherthanbalancedaccuracy,suchasstandarddeviation, outthepriortheoreticalinsights.However,thetheory-based
quantileerrors,orConditionalValueatRisk(CVaR). methods require expertise and trial and error to tune one
4. CAPadaptstoclassheterogeneities(Sec4.3).CAPcan temperature variable, making it time-consuming and chal-
alsoeffortlesslycombinemultipleattributes(suchasfre- lengingtoachieveafine-grainedlossfunctionthatcarefully
quency, noise, class importance) to boost accuracy by handleseachclassindividually.Althoughthebilevel-based
adaptingtoproblemheterogeneity. methodconsidereachclassseparatelyandpersonalizesthe
weightusingvalidationdata,optimizingthebilevelproblem
Finally,whileweinstantiateCAPfortheproblemsofloss-
istypicallytime-consumingduetotheHessiancomputations.
functiondesignandpost-hocoptimization,CAPcanbeap-
Bileveloptimizationisalsobrittle,especiallywhen(Lietal.
plied for the design of class-specific augmentations, regu-
2021)optimizestheinnerlossfunction,whichcontinually
larization schemes, and optimizers. This work makes key
changestheinneroptimaduringthetraining.
contributions to fairness and heterogeneous learning prob-
Regardingthebroadergoaloffairnesswithrespecttopro-
lemsintermsofmethodology,aswellaspracticalimpact.
tectedgroups,theliteraturecontainsseveralproposals(Sahu
AnoverviewofourapproachisshowninFig.1
etal.2018;Kleinberg,Mullainathan,andRaghavan2016).
Balancederrorandstandarddeviation(Calmonetal.2017;
1.1 RelatedWork
Alabi, Immorlica, and Kalai 2018) between subgroup pre-
Theexistingliteratureestablishesaseriesofalgorithms,in- dictionsarewidelyusedmetrics.However,theymaybein-
cludingsampleweighting(Alshammarietal.2022;Maldon- sensitivetocertaintypesofimbalances.TheDifferenceof
ado et al. 2022; Kubat, Matwin et al. 1997; Wallace et al. EqualOpportunity(DEO)(Kinietal.2021;Hardt,Price,and
2011; Chawla et al. 2002), post-hoc tuning (Menon et al. Srebro 2016) was proposed to measure true positive rates
2020; Zhang et al. 2019; Kim and Kim 2020; Kang et al. across groups. (Zafar et al. 2017) focus on disparate mis-
2019;Yeetal.2020),andlossfunctionstuning(Caoetal. treatmentinbothfalsepositiverateandfalsenegativerate.
2019;Kinietal.2021;Menonetal.2020;Khanetal.2017; ManymodernMLtasksnecessitatemodelswithgoodtail
Cuietal.2019;Lietal.2022;Tanetal.2020;Zhangetal. performance,focusingonunderrepresentedgroups.Recent
2017),andmore(Zhangetal.2023).Thisworkaimstoes- works have introduced techniques that promote better tail
tablishaprincipledapproachfordesigningalossfunction accuracy(Hashimotoetal.2018;Sagawaetal.2019,2020;
for imbalanced datasets. Traditionally, a Bayes-consistent Lietal.2021;KiniandThrampoulidis2020).Theworst-case
loss function such as weighted cross-entropy (Xie et al. subgrouperroriscommonlyusedinrecentpapers(Kinietal.
2018; Morik, Brockhausen, and Joachims 1999) has been 2021;Sagawaetal.2019,2020).Anotherpopularmetricto
used. However, recent work shows it only adds marginal evaluatethemodelâ€™stailperformanceistheCVaR(Condi-
benefit to the over-parameterized model due to overfitting tional Value at Risk) (Williamson and Menon 2019; Zhai
during training. (Menon et al. 2020; Ye et al. 2020; Kini etal.2021;Michel, Hashimoto, andNeubig2021),which
etal.2021)proposeafamilyoflossfunctionsformulatedas computes the average error over the tails. Previous works
(cid:16) (cid:17)
â„“(y,f(x)) = log 1+(cid:80) elkâˆ’ly Â·eâˆ†kfk(x)âˆ’âˆ†yfy(x) (Hashimoto et al. 2018; Duchi and Namkoong 2021; Hu
kÌ¸=y
et al. 2018; Michel, Hashimoto, and Neubig 2021; Lahoti
withtheoreticalinsights,wheref(x)denotestheoutputlog-
etal.2020)alsomeasuretailbehaviourusingDistributionally
itsofxandf (x)representstheentrythatcorrespondsto
y RobustOptimization(DRO).
labely.Abovemethodsdeterminethevalueoflandâˆ†tore-2 ProblemSetup
Thispaperinvestigatestheadvantagesofutilizingattribute-
basedpersonalizedtrainingapproachesforaddressinghetero-
geneousclassesinthecontextofclassimbalance,labelnoise,
andfairnessobjectiveproblems.Webeginbypresentingthe
generalframework,followedbyanexaminationofspecific
fairnessissues,whichencompassbothdistributionalandob-
jectiveheterogeneities.Consideramulti-classclassification
problemforadataset(x ,y )N sampledi.i.dfromadistri-
i i i=1
butionwithinputspaceX andK classes.Let[K]denotethe (a)ImbalanceÏ€=0.1 (b)ImbalanceÏ€=0.2
set{1..K}andforthetrainingsample(x,y),xâˆˆX isthe
inputandy âˆˆ[K]istheoutput.f :X â†’RK representsthe Figure2:TheoptimalhyperparameterÎ´ âˆ— dependsonboth
attributes:frequency(Ï€)anddifficulty(Ïƒ /Ïƒ ).
modelandoistheoutputlogits.yË† = argmax o + âˆ’
f(x) kâˆˆ[K] k
is the predicted label of the model f(x). We also denote
K Ã—K identity matrix by I . Moreover, in the post-hoc
K assigninglargermargintotheminorities.Inparticular,setting
setup,alogitadjustmentfunctiong :RK â†’RK isemployed
Î´ = 1 recovers the vanilla SVM. CS-SVM is particularly
tomodifythelogits,resultinginadjustedlogitsoË†=g(o).
relevanttooursettingbecauseithasrigorousconnectionsto
Ourgoalistotrainamodelthatminimizesaspecificclassi-
thegeneralizedcross-entropylossinSec3.2(seeAppendix
ficationerrormetric.Theclass-conditionalerrorsaredefined
Ffordetails).GivenCS-SVMsolution(wË† ,Ë†b ),wemeasure
overthedatadistributionasErr = P[y Ì¸=yË† (x)|y =k]. Î´ Î´
k f
thebalancederrorasfollows:
The standard classification error is denoted by Err =
plain
P[y Ì¸=yË† f(x)].Insituationswithlabelimbalance,Err plainis R (Î´):=P (cid:110) y(xTwË† +Ë†b )>0(cid:111) .
dominatedbythemajorityclasses.Tothisend,balancedclas- bal (x,y)âˆ¼GMM Î´ Î´
sificationerrorErr bal = K1 (cid:80)K k=1Err k iswidelyemployed Weask:HowdoestheoptimalCS-SVMclassifier(i.e,the
asafairnessmetric.Wewilllaterintroduceotherobjectives optimalhyperparameterÎ´)dependonthedataattributes,i.e.
thataimtoachievedifferentfairnessgoals.Acompletelist onthefrequencyÏ€andonthedifficultyÏƒ /Ïƒ ?Toanswer
+1 âˆ’1
oftheobjectivesweexaminecanbefoundinAppendix. this we consider a high-dimensional asymptotic setting in
whichn,d â†’ âˆatalinearrated/n =: dÂ¯.Thisregimeis
3 OurApproach:Class-attributePriors(CAP)
convenientaspreviousworkhasshownthatthelimitingbe-
Westartwithamotivatingquestion: haviorofthebalancederrorR (Î´)canbecapturedprecisely
bal
Q:Doesutilizingmultipleclassattributesprovablyhelp? by analytic formulas (Montanari et al. 2019). Specifically,
To answer this, we consider the benefits of multiple at- (Kinietal.2021)computesformulasfortheoptimalhyper-
tributes for a binary gaussian mixture model (GMM) and parameterÎ´whenÏ€isvariablebutbothclassesareequally
provideasimpletheoreticaljustificationwhysynergistically difficult, i.e. Ïƒ = Ïƒ . Here, we derive risk curves for
+1 âˆ’1
leveragingattributescanhelpbalancedaccuracy.Considera arbitrary Ïƒ by extending their study and investigate the
Â±1
GMMwheredatafromthetwoclassesaregeneratedas synergisticeffectoffrequencyanddifficulty.
Figure 2 confirms our intuition: the optimal hyperpa-
(cid:26)
+1 ,withprob.Ï€
y =
âˆ’1 ,withprob.1âˆ’Ï€
and x|y âˆ¼N(yÂµ,Ïƒ yI d). r aa nm de dt ie fr ficÎ´ âˆ— ul( ty y-a (ax sis w)d elo le as sin thd eee trd aid ne inp gen sd amon plt eh se izf ere ,q xu -ae xn ic sy
).
Specifically,weobserveinbothFigures2(a,b)thatasthemi-
Noteherethatthetwoclassesareimbalancedasafunction
norityclassbecomeseasier(aka,thesmallerratioÏƒ /Ïƒ ),
of the value of Ï€ âˆˆ (0,1), which models class frequency. +1 âˆ’1
Î´ decreases. That is, there is less need to assign an even
Also, the two classes are allowed to have different noise
largermarginfortheminority.Conversely,asÏƒ /Ïƒ in-
variancesÏƒ .Thisisourmodelforthedifficultyattribute: +1 âˆ’1
Â±1 creasesandminoritybecomesmoredifficult,itsmargingets
examplesgeneratedfromtheclasswithlargervarianceare
aboost.Finally,comparingFigures2(a)to2(b),notethatÎ´
â€œmoredifficult"toclassifyastheyfallfurtherapartfromtheir âˆ—
takes larger values for larger imbalance ratio (i.e., smaller
mean.Intuitively,aâ€œgood"classifiershouldaccountforboth
frequencyÏ€),againaggreeingwithcommonsense.
attributes.Weshowherethatthisisindeedthecaseforthe
SinceGMMdataissynthetic,Î´ canbecomputedanalyti-
modelabove.Oursettingisasfollows:DrawnIIDsamples âˆ—
cally.OurapproachCAPwillfacilitaterealizingsuchbenefits
(x ,y )fromtheGMMdistributionabove.Withoutlossof
i i systematicallyandefficientlyforarbitraryclassattributes.
generality,assumeclassy = +1isminority,i.e.Ï€ < 1/2.
Wetrainlinearclassifier(w,b)bysolvingthefollowingcost-
3.1 AttributesandAdaptationtoHeterogeneity
sensitivesupport-vector-machines(CS-SVM)problem:
Toproceed,weintroduceourCAPapproachataconceptual
(cid:26) Î´ y =+1 levelandprovideconcreteapplicationsofCAPtolossfunc-
(wË† ,Ë†b ):=argminâˆ¥wâˆ¥ s.t.y (xTw+b)â‰¥ i .
Î´ Î´ w,b 2 i i 1 y i =âˆ’1 tion design in the next section. Recall that our high-level
goalisdesigningamapfromA2HthattakesattributesA of
k
Here,Î´ isahyperparameterthatwhentakingvalueslarger classkandgeneratesthehyperparametersoftheoptimization
thanone,itpushestheclassifiertowardsthemajority,thus strategyS .EachcoordinateA [i]characterizesaspecific
k kAttributes Definition Notation Applicationscenario
A Classfrequency Ï€ =P(y =k) Imbalancedclasses
FREQ k
A Class-conditionalerror P(y Ì¸=yË†) Difficultvseasyclasses
DIFF
A Test-timeclassweights Ï‰test of(3) Weightedtestaccuracy
WEIGHTS k
(cid:12)
A Labelnoiseratio P(yCLEAN Ì¸=y(cid:12)yclean =k) Datasetswithlabelnoise
NOISE
A Normofclassifierweights See(Caoetal.2019) Imbalancedclasses
NORM
Table1:Definitionofexampleattributesandassociatedapplicationscenarios.AttributesA andA arecomputedduring
DIFF NORM
thetraining(forpost-hocoptimization,itispre-training).Forbileveltrainingtheyarecomputedattheendofwarm-up.The
upperattributesinredcolorarethoseweutilizeinourexperiments.AlsoweuseA todenotecombinedattributes.
ALL
attributeofclassksuchaslabelfrequency,labelnoiseratio, [Ï‰ ,l ,âˆ† ]correspondtoaparticularrowofW âˆˆ R3Ã—M
k k k
trainingdifficultyshowninTable1.TomodelA2H,onecan sinceW =[w ,w ,w ]âŠ¤.OurgoalisthentuningtheW
Ï‰ l âˆ†
useanyhypothesisspaceincludingdeepnets.However,since matrixovervalidationdata.Inpracticalimplementation,we
A2Hwillbeoptimizedoverthevalidationloss,dependingon defineafeaturedictionary
theapplicationscenario,itisoftenpreferabletouseasimpler
linearizedmodel. D =[F(A 1) Â·Â·Â· F(A K)]âŠ¤ âˆˆRKÃ—M. (2)
Linearizedapproach.Supposeeachclasshasnattributes
Each row of this dictionary is the features associated to
withA k âˆˆ Rn.WewilluseanonlinearfeaturemapF(Â·) : the attributes of class k. We generate the strategy vectors
Rn â†’ RM whereM istheembeddingspace.Supposethe âˆ†,l,Ï‰ âˆˆ RK (for all classes) via Ï‰ = Dw , âˆ† =
c tel ra is zs e- dsp be yci afi wc est igra ht teg my atS rik xâˆˆ WRs âˆˆ.T Rh se Ã—n M,A s2 oH thc aa tnbeparame- sigmoid(âˆš K âˆ¥DD ww âˆ†âˆ† âˆ¥),l=Dw l. Ï‰
Forbothlossfunctiondesignandpost-hocoptimization,
S =A2H(A ):=WF(A ). (1) weuseadecomposablefeaturemapF.Concretely,suppose
k k k
wehavebasisfunctions(F )m .Thesefunctionsarechosen
OurgoalbecomesfindingW sothattheresultingstrategies i i=1
to be poly-logarithms or polynomials inspired by (Menon
maximizethetargetvalidationobjective.ObservethatW has
etal.2020;Yeetal.2020).ForithattributeA [i] âˆˆ R,we
sÃ—M parametersratherthansÃ—K parameterswhichisthe k
generateF(A [i])âˆˆRmobtainedbyapplying(F )m .We
naiveapproachthatlearnsindividualstrategies.Inpractice, k i i=1
thenstitchthemtogethertoobtaintheoverallfeaturevector
K canbesignificantlylarge,sofortypicalproblems,M â‰ª
F(A )=[F(A [1])âŠ¤ Â·Â·Â· F(A [m])âŠ¤]âˆˆRM:=mÃ—n.We
K. Moreover, W ties all classes together during training k k k
emphasizethatpriorapproachesarespecialinstanceswhere
throughweight-sharingwhereasthenaiveapproachwould
wechooseasinglebasisfunctionandsingleattributeÏ€ .
bebrittlefortailclassesthatcontainverylimiteddata. k
Whichattributestouseandwhymultipleattributeshelp?
3.2 CAPforLossFunctionDesign Attributesshouldbechosentoreflecttheheterogeneityacross
individualclasses.Theseincludeclassfrequency,difficulty
Considerthegeneralizedcross-entropyloss
of prediction, noise level and more. We list such potential
â„“(y,f(x))=Ï‰ ylog(1+(cid:88) elkâˆ’ly Â·eâˆ†kfk(x)âˆ’âˆ†yfy(x)). attributesAinTable1.ThefrequencyA FREQ iswidelyused
tomitigatelabelimbalance,andA isinspiredbytheim-
kÌ¸=y NORM
balancedlearningliterature(Caoetal.2019).However,these
Here,(Ï‰ ,l ,âˆ† )K arehyperparametersthatcanbetuned maynotfullycapturetheheterogenousnatureoftheproblem.
k k k k=1
tooptimizethedesiredtestobjective.Forclassk,wegetto As discussed above for GMMs (Fig 2), some classes can
choose the tuple S := [Ï‰ ,l ,âˆ† ] which can be consid- bemoredifficulttolearnandrequiremoreupweightingde-
k k k k
eredasitstrainingstrategy.HereelementsofS arisefrom spitecontainingsufficienttrainingexamples.Thismotivates
k
existingimbalance-awarestrategies,namelyweightingÏ‰ , the use of A . Moreover, rather than balanced accuracy,
k DIFF
additive logit-adjustment l and multiplicative adjustment wemaywishtooptimizegeneraltestobjectivesincluding
k
âˆ† . weighted accuracy with varying class importance. We can
k
Example: LA and CDT losses viewed as CAP. For label declarethesetest-timeweightsasanattributeA .Ap-
WEIGHTS
imbalanced problems, (Menon et al. 2020; Ye et al. 2020) pendix provides theoretical justification for incorporating
propose to set hyperparameters l and âˆ† as a function A by showing CAP can accomplish Bayes optimal
k k WEIGHTS
of frequency Ï€ = P(y = k). Concretely, they propose logitadjustmentforweightederror.Morebroadly,anyclass-
k
l = âˆ’Î³log(Ï€ ) (Menon et al. 2020) and âˆ† = Ï€Î³ (Ye specificmeta-featurecanbeusedasanattributewithinCAP.
k k k k
etal.2020)forsomescalarÎ³.Thesecanbeviewedasspecial Reducedsearchspaceandincreasedstability.Searching
instancesofCAPwherewehaveasingleattributeA =Ï€ landâˆ†onRK withveryfewvalidationsamplesraisesthe
k k
andA2H(x)isâˆ’Î³log(x)orxÎ³ respectively. problemofunstableoptimization.(Lietal.2021)indicates
Our approach can be viewed as an extension of these to thebileveloptimizationisbrittleandhardtooptimize.They
attributes beyond frequency and general class of A2H. In introducealongwarm-upphaseandaggregateclasseswith
lightof(1),hyperparametersofaspecificelementofS = similarfrequencyintog groups,reducingthesearchspace
ktok/gdimensions.However,toachieveafine-grainedloss adjusts the output of f to minimize the fairness objective.
function,gcannotbeverylarge,sothesearchspaceremains Thusthefinalmodelofpost-hocoptimizationisgâ—¦f(x).
large. In our method, with a good design of D (normally
n â‰ˆ 2 and m â‰ˆ 3), we can utilize a constant 2mn â‰ª K 4 ExperimentsandMainResults
thatefficientlyreducesthesearchspaceandprovidesbetter
In this section, we present our experiments in the follow-
convergenceandstability.
ing way. Firstly, we demonstrate the performance of CAP
Weremarkthatdictionaryisageneralandefficientdesign
on both loss function design via bilevel optimization and
thatcanrecovermultipleexistingsuccessfulimbalancedloss
post-hoclogitadjustmentinSec.4.1.Sec.4.2demonstrates
functiondesignalgorithms.Forexample,(Menonetal.2020)
thatCAPprovidesnoticeableimprovementsforfairnessob-
and(Yeetal.2020)bothutilizethefrequencyasAandapply
jectivesbeyondbalancedaccuracy.ThenSec.4.3discusses
logarithm and polynomial functions as F on frequency to
theadvantageofutilizingattributesandhowCAPleverages
determinelandâˆ†respectively.Moreover,letA = I and
k
theminnoisy,long-taileddatasetsthroughperturbationex-
F beanidentityfunction,thentrainingw ,w isequivalent
l âˆ†
periments.Lastly,wedefertheexperimentdetailsincluding
totrainl,âˆ†whichrecoversthealgorithmof(Lietal.2021).
hyper-parameters,numberoftrails,andotherreproducibility
Despitetheabilitytogeneralize,thedictionaryismoreflexi-
informationtoappendix.
bleandpowerfulsincetheattributescanbechosenbasedon
Dataset.Inlinewithpreviousresearch(Menonetal.2020;
thescenarios.Forexample,naturally,classfrequencyisacrit-
Yeetal.2020;Lietal.2021),weconducttheexperiments
icalcriterioninanimbalanceddataset,butclassificationerror
on CIFAR-LT and ImageNet-LT datasets. The CIFAR-LT
inearlytrainingcanalsobeagoodcriterionforevaluating
modifies the original CIFAR10 or CIFAR100 by reducing
classtrainingdifficulty.Furthermore,somespecificattributes
thenumberofsamplesintailclasses.Theimbalancefactor,
canbeintroducedtonoisyorpartial-labeleddatasetstohelp
representedasÏ=N /N ,isdeterminedbythenum-
designabetterlossfunction.Ourempiricalstudyelucidates max min
ber of samples in the largest (N ) and smallest (N )
thebenefitofcombiningmultipleattributesandthedictionary max min
classes. To create a dataset with the imbalance factor, the
performanceonthenoisyimbalanceddataset.
samplesizedecreasesexponentiallyfromthefirsttothelast
3.3 Class-specificLearningStrategies:Bilevel class. We use Ï = 100 in all experiments, consistent with
OptimizationandPost-hocoptimization previous literature. The ImageNet-LT, a long-tail version
ofImageNet,has1000classeswithanimbalancedratioof
ToinstantiateCAPasameta-strategy,wefocusontwoim-
Ï = 256. The maximum and minimum samples per class
portantclass-specificoptimizationproblems:lossfunction
are 1280 and 5, respectively. During the search phase for
designviabileveloptimizationandpost-hoclogitadjustment.
bilevelCAP,wesplitthetrainingsetinto80%trainingand
Wedescribetheminthissectionanddemonstratethatboth
20% validation to obtain the optimal loss function design.
methodsoutperformthestate-of-the-artapproaches.Thefig-
We remark that the validation set is imbalanced, with tail
ureinAppendixillustrateshowCAPisimplementedunder
classescontainingveryfewsamples,makingitchallenging
bi-leveloptimizationandpost-hocoptimizationindetail.
tofindoptimalhyper-parameterswithoutoverfitting.Forall
â€¢Strategy1:Lossfunctiondesignviabileveloptimization.
otherpost-hocexperiments(Sec.4.2and4.3),wefollowthe
Inspiredby(Lietal.2021)andfollowingourexpositionin
setupof(Menonetal.2020;Hardt,Price,andSrebro2016)
Section 3.1, we formalize the meta-strategy optimization
bytrainingamodelonentiretrainingdatasetasthepre-train
problemas
model,andoptimizingalogitadjustmentgonabalancedval-
min L (w ,w ,f) s.t. minL (w ,w ,f)
val l âˆ† train l âˆ† idationdataset.Additionally,allCIFAR-LTexperimentsuse
wl,wâˆ† f
ResNet-32(Heetal.2016),andImageNet-LTexperiments
wheref isthemodelandL ,L arevalidationandtrain-
val train useResNet-50.
inglossesrespectively.OurgoalisfindingCAPparameters
w ,w thatminimizethevalidationlosswhichisthetarget
l âˆ† 4.1 CAPImprovesPriorMethodsUsingPost-hoc
fairnessobjective.Followingtheimplementationof(Lietal.
orBilevelOptimization
2021),wesplitthetrainingdatato80%trainingand20%val-
idationtooptimizeL andL .Theoptimizationprocess Thissectionpresentsourlossfunctiondesignexperimentson
train val
issplittotwophases:thesearchphasethatfindsCAPparam- imbalanceddatasetsbyincorporatingCAPintothetraining
etersw ,w andtheretrainingphasethatusestheoutcome schemeof(Lietal.2021;Menonetal.2020;Yeetal.2020),
l âˆ†
ofsearchandentiretrainingdatatoretrainthemodel.We asdiscussedinSec.3.3.Table2demonstratesourresults.The
notethat,duringinitialsearchphase,(Lietal.2021)employs firstpartdisplaystheoutcomesofvariousexistingmethods
alongwarm-upphasewheretheyonlytrainf whilefixing withtheiroptimalhyper-parameters.Itisworthnotingthat
w ,w toachievebetterstability.Incontrast,wefindthat the original best results for single-level methods ((Menon
l âˆ†
CAPeitherneedsveryshortwarm-upornowarm-upatall et al. 2020; Ye et al. 2020)) are obtained from grid search
pointingtoitsinherentstability. onthetestdataset,whichleadstomuchbetterperformance
â€¢Strategy2:Post-hocoptimization.In(Menonetal.2020; thanourreproducedresultsusingvalidationgridsearchin
Feldmanetal.2015;Hardt,Price,andSrebro2016),theau- Table.2.Moreover,bothofthegridsearchmethodsdemand
thordisplaysthatthepost-hoclogitadjustmentcanefficiently substantial computation budgets. As illustrated in the sec-
addressthebiaswhentrainingwithimbalanceddatasets.For- ondpartofTable2,bilevelandpost-hocCAPsignificantly
mally,givenamodelf,apost-hocfunctiong : RK â†’ RK improvethebalancederroracrossalldatasets.Method CIFAR10-LT CIFAR100-LT ImageNet-LT
Crossentropy 30.45(Â±0.49) 61.94(Â±0.28) 55.59(Â±0.26)
Logitadjustment(LA)(Menonetal.2020) 21.29â€ (Â±0.43) 58.21â€ (Â±0.31) 52.46â™¯
CDT(Yeetal.2020) 21.57â€ (Â±0.50) 58.38â€ (Â±0.33) 53.47â™¯
Plain (AutoBalance(Lietal.2021)) 21.15â™¯ 56.70â™¯ 50.91â™¯
Bilevel
CAP :A 20.22(Â±0.35) 56.38(Â±0.19) 49.31(Â±0.34)
Bilevel ALL
CAP :A 20.87(Â±0.38) 57.63(Â±0.26) 51.46(Â±0.20)
Post-hoc ALL
Table2:Balancederroronlong-taileddatausinglossfunctiondesignedviabileveloptimization.â™¯:bestreportedresultstaken
from(Lietal.2021).â€ :Reproducedresults.
20 15 70 Plain Plain Plain
LA 15 LA LA
CAP 10 CAP 60 CAP
10
50 5
5
40
0 0
0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 15 20 25
a a Standarddeviation(Err SDev)
(a)Errorsofquantileclasses (b)Conditionalvalueoferrors (c)Aggregationoferrors
Figure3:BenefitofCAPforoptimizingdifferentFairnessObjectives.Wecompareamongplainpost-hoc,LApost-hocand
CAP .(a):ResultsofoptimizingquantileclassperformanceQuant = P[y Ì¸=yË† (x)|y =K ],whereK denotesthe
post-hoc a f a a
class index with the worst âŒˆKÃ—aâŒ‰-th error. (b): Results of optimizing tail performance CVaR . (c): Results of optimizing
a
R(Err)=Î»Â·Err +(1âˆ’Î»)Â·Err .Theplotshowsthetrade-offbetweenstandarddeviationofclass-conditionalerrors
plain SDev
Err andStandardmisclassificationerrorErr asÎ»varies.SeeSec.4.2fordetaileddefinitionanddiscussions.
SDev plain
4.2 BenefitsofCAPforOptimizingDistinct mizingtheCVaR tendtoimprovethetailbehaviorofthe
a
FairnessObjectives classifier,whichisamoregeneralfairnessobjective.Fig.3b
showsthetestimprovementsoverthreeapproaches,andCAP
Recentworksonlabel-imbalanceplacesasignificantempha-
isconsistentlybetterthanallothermethods.
sisonthebalancedaccuracyevaluations(Menonetal.2020;
Finally,forthecombinedriskR(Err),wedefineR(Err)=
Lietal.2021;Caoetal.2019).However,inpractice,thereare
Î»Â·Err +(1âˆ’Î»)Â·Err where Err is the regular
manydifferentfairnesscriteriaandbalancedaccuracyisonly plain SDev plain
classificationerrorandErr denotesthestandarddeviation
oneofthem.Infact,aswediscussin(3),wemightevenwant SDev
ofclassificationerrors.Weplottheerror-deviationcurveby
to optimize arbitrary weighted test objectives. In this sec-
varyingÎ»from0to1withstepsize0.1onthreeapproachesin
tion,wedemonstratetheflexibilityandmeritsofCAPwhen
Fig.3c,eachpointcorrespondstoadifferentÎ».Weobserve
optimizingfairnessobjectivesotherthanbalancedaccuracy.
thatplainpost-hoccannotachieveasmallstandarddeviation,
TheexperimentsareconductedontheCIFAR100-LTdataset
andpost-hocLAdegradeswhenachievingsmallerErr ,
usingthepost-hocapproaches.Forthefairnessobjectives,we SDev
CAP accomplish the best performance and are flexible to
mainlyfocusonthreeobjectives:quantileclasserrorQuant ,
a adapttodifferentobjectives.
conditionalvalueatrisk(CVaR)CVaR ,andthecombined
a
risk R(Err), which consists of standard deviation of error Whenusingplainpost-hoc(withoutCAP),eachclasspa-
andtheregularclassificationerror. rameterisupdatedindividually.Thus,optimizingforspecific
classes (e.g., Quant ) dramatically hurts the performance
We first demonstrate the performance on quantile class a
errorQuant =P[y Ì¸=yË† (x)|y =K ],whereK denotes of other classes which are ignored. Confirming this, we
a f a a
theclassindexwiththeworstâŒˆKÃ—aâŒ‰-therror.Forinstance, foundthatoptimizingplainpost-hocisunstableanddoesnot
in CIFAR100-LT, where K = 100, Quant denotes the achievedecentresults.Ontheotherhand,whilepost-hocLA
0.2
outperformsplainpost-hoc,optimizingonlyonetemperature
test error of the worst 20 percentile class. That is, we sort
variablelacksfine-grainedadaptationtovariousobjectives.
theclassesindescendingorderoftesterrorandreturnthe
Incontrast,CAPachievesanoticeablybetterperformanceon
error of the class 20%th class ID. Thus, each selection of
a raises a new objective. Fig. 3a shows the improvement allobjectivesthankstoitsbestofbothworldsdesign.
over the pre-trained model when optimizing Quant
a
with Table3showsmoreresults.Err weighteddenotesaweighted
multipleselectionsofa.WeobservethatCAPsignificantly testobjectiveinducedbyweightsÏ‰test âˆˆRK givenby
k
outperformsbothlogitadjustmentandplainpost-hoc.
CVaR =E[Err |Err >Quant ]measurestheaverage K K
errorofâŒˆa KÃ—aâŒ‰clak sseswk ithworstea rrors.InsteadofQuant a, Err weighted =(cid:88) Ï‰ ktestErr k where (cid:88) Ï‰ ktest =K. (3)
whichonlyfocusesonthespecificquantileclasserror,opti- k=1 k=1
tnemevorpmi
tnauQ a
tnemevorpmiaRaVC )
rrE(rorredradnatS
nialpPost-hocmethods Err Err CVaR Quant Err
bal SDev 0.2 0.2 weighted
Pretrained 61.94(Â±0.28) 27.13(Â±0.35) 96.95(Â±0.15) 93.01(Â±0.58) 62.53(Â±0.53)
Plain -1.62(Â±0.36) -8.51(Â±0.75) -11.48(Â±0.81) -12.79(Â±0.43) -2.82(Â±0.56)
Post-hoc
LA -3.73(Â±0.29) -8.72(Â±0.66) -12.21(Â±0.50) -15.01(Â±0.35) -3.62(Â±0.37)
Post-hoc
CAP -4.36(Â±0.25) -13.92(Â±0.24) -14.75(Â±0.87) -18.34(Â±0.47) -6.21(Â±0.49)
Post-hoc
Table3:Theerrordifferencebetweenotherapproachescomparedtopre-trainedmodel.Thefirstlineshowstheperformanceof
Pretrainedmodel,andthefollowinglineshowstheerrordifferenceofothermethods(smallerisbetter).Forobjectiveswitha,
weseta=0.2.Thisiscommonlyusedfordifficultorfewclassesinotherpapers(Zhaietal.2021;Liuetal.2019).
CIFAR100-LT ImageNet-LT CIFAR10-LT+Noise
Err Err Err Err Err Err
bal SDev bal SDev bal SDev
Crossentropy 61.94(Â±0.28) 27.13(Â±0.35) 55.59(Â±0.26) 29.10(Â±0.64) 43.76(Â±0.74) 31.69(Â±0.81)
Plain (AutoBalance(Lietal.2021)) 56.70(Â±0.32) 20.13(Â±0.68) 50.93(Â±0.16) 26.06(Â±0.61) 40.04(Â±0.79) 36.30(Â±0.89)
Bilevel
CAP :A 56.64(Â±0.21) 19.10(Â±0.67) 50.82(Â±0.13) 24.36(Â±0.49) 39.91(Â±0.66) 26.54(Â±0.80)
Bilevel FREQ
CAP :A 58.27(Â±0.24) 17.62(Â±0.65) 52.97(Â±0.30) 21.28(Â±0.58) 40.61(Â±0.61) 14.49(Â±0.72)
Bilevel DIFF
CAP :A +A 56.38(Â±0.19) 18.53(Â±0.63) 49.31(Â±0.34) 22.14(Â±0.46) 38.36(Â±0.79) 19.78(Â±0.75)
Bilevel FREQ DIFF
Table4:Attributeshelpoptimizationadapttodatasetheterogeneity.Weconductexperimentsusingbilevellossdesignandreport
thebalancedmisclassificationerror,andstandarddeviationofclass-conditionalerrorswithdifferentclass-specificattributes.
Overall,Table3showsthatCAPconsistentlyachievesthe portantly,CAPisparticularlyfavorabletotailclasseswhich
best results on multiple fairness objectives. An important containtoofewexamplestooptimizeindividually.Onlyus-
conclusionisthat,thebenefitofCAPismoresignificantfor ingA achievessmallestErr demonstratingthatopti-
DIFF SDev
objectivesbeyondbalancedaccuracyandimprovementsare mizationwithA tendstokeepbetterclass-wisefairness
DIFF
around2%ormore(comparedto(Menonetal.2020)orplain becauseA isdirectlyrelatedtoclasspredictability.The
DIFF
post-hoc).Thisisperhapsnaturalgiventhatpriorworksput combination of A and A shows that incorporating
FREQ DIFF
anoutsizedemphasisonbalancedaccuracyintheiralgorithm multiple class-specific attributes provides additional infor-
design(Menonetal.2020;Lietal.2021). mationaboutthedatasetandjointlyenhancesperformance.
Overall,theresultsindicatethatCAPestablishesaprincipled
4.3 BenefitsofCAPforAdaptingtoDistinctClass approachtoadapttomultiplekindsofheterogeneity.
Heterogeneities
5 Discussion
Continuing the discussion in Sec. 3.1, we investigate the
advantageofdifferentattributesinthecontextofdatasethet- WeproposedCAPasaflexiblemethodtotackleclasshetero-
erogeneity adoption. In Table 4, we conduct loss function geneitiesandgeneralfairnessobjectives.CAPachieveshigh
design CAP experiments on CIFAR-LT and ImageNet-LT performance by efficiently generating class-specific strate-
dataset. Specifically, besides using regular CIFAR100-LT giesbasedontheirattributes.Wepresentedstrongtheoretical
andImageNet-LT,weintroducelabelnoiseintoCIFAR10- andempiricalevidenceonthebenefitsofmultipleattributes.
LTfollowing(Tanakaetal.2018;Reedetal.2014)toextend Evaluationsonpost-hocoptimizationandlossfunctionde-
theheterogeneityofthedataset.Toaddthelabelnoise,firstly, signrevealedthatCAPsubstantiallyimprovesmultipletypes
wesplitthetrainingdatasetto80%trainand20%validation offairnessobjectivesaswellasgeneralweightedtestobjec-
to accommodate bilevel optimization. Then we randomly tives.InAppendixD,wealsodemonstratethetransferability
generateanoiseratior âˆˆ RK,r âˆ¼ U(0,0.5)thatdenotes acrossourstrategies:Post-hocCAPcanbepluggedinasa
i
thelabelnoiseratioforeachclass.Finally,keepingtheval- lossfunctiontofurtherboostaccuracy.
idation set clean, we add label noise into the train set by Broaderimpacts.Althoughourapproachandapplications
randomlyflippingthelabelsofselectedtrainingsamples(ac- primarilyfocusonlossfunctiondesignandposthocoptimiza-
cordingtothenoiseratio)toallpossiblelabels.Asaresult, tion,CAPapproachcanalsohelpdesignclass-specificdata
allclassescontainanunknownfractionoflabelnoiseinthe augmentation,regularization,andoptimizers.Additionally,
noisyCIFAR10-LTdataset,whichraisesmoreheterogeneity rather than heterogeneities across classes, one can extend
andchallengeinoptimization.Throughbileveloptimization, CAP-stylepersonalizationtoproblemsinmulti-tasklearning
weoptimizethebalancedclassificationlossandreportthe andrecommendationsystems.Limitations.Withaccessto
balancedtesterroranditsstandarddeviationaftertheretrain- infinitedata,onecansearchforoptimalstrategiesforeach
ingphaseinTable4.AsshowninTable4,weemploylabel class.Thus,theprimarylimitationofCAPisitsmulti-taskde-
frequencyA whichisdesignedforsamplesizehetero- signspacethatsharesthesamemeta-strategyacrossclasses.
FREQ
geneityandA whichisdesignedforclasspredictability However,asexperimentsdemonstrate,inpracticalfinitedata
DIFF
as the attributes in CAP approach. Table 4 highlights that settings,CAPachievesbetterdataefficiency,robustness,and
CAPconsistentlyoutperformsothermethodswhiledifferent testperformancecomparedtoindividualtuning.
attributescanshapetheoptimizationprocessdifferently.Im-Acknowledgements He,K.;Zhang,X.;Ren,S.;andSun,J.2016. Deepresidual
ThisworkwassupportedbytheNSFgrantsCCF-2046816 learningforimagerecognition. InProceedingsoftheIEEE
andCCF-2212426,NSFCAREER1942700,NSERCDis-
conferenceoncomputervisionandpatternrecognition,770â€“
coveryGrantRGPIN-2021-03677,GoogleResearchScholar 778.
award,AdobeDataScienceResearchaward,andArmyRe- Hu, W.; Niu, G.; Sato, I.; and Sugiyama, M. 2018. Does
searchOfficegrantW911NF2110312. distributionallyrobustsupervisedlearninggiverobustclas-
sifiers? InInternationalConferenceonMachineLearning,
References 2029â€“2037.PMLR.
Kang,B.;Xie,S.;Rohrbach,M.;Yan,Z.;Gordo,A.;Feng,
Alabi,D.;Immorlica,N.;andKalai,A.2018. Unleashing
J.; and Kalantidis, Y. 2019. Decoupling representation
linearoptimizersforgroup-fairlearningandoptimization. In
ConferenceOnLearningTheory,2043â€“2066.PMLR. and classifier for long-tailed recognition. arXiv preprint
arXiv:1910.09217.
Alshammari, S.; Wang, Y.-X.; Ramanan, D.; and Kong, S.
Khan,S.H.;Hayat,M.;Bennamoun,M.;Sohel,F.A.;and
2022. Long-tailedrecognitionviaweightbalancing. InPro-
Togneri, R. 2017. Cost-sensitive learning of deep feature
ceedingsoftheIEEE/CVFConferenceonComputerVision
representationsfromimbalanceddata. IEEEtransactionson
andPatternRecognition,6897â€“6907.
neuralnetworksandlearningsystems,29(8):3573â€“3587.
Calmon,F.;Wei,D.;Vinzamuri,B.;NatesanRamamurthy,
Kim,B.;andKim,J.2020. Adjustingdecisionboundaryfor
K.;andVarshney,K.R.2017. Optimizedpre-processingfor
classimbalancedlearning. IEEEAccess,8:81674â€“81685.
discriminationprevention. Advancesinneuralinformation
processingsystems,30. Kini, G.; and Thrampoulidis, C. 2020. Analytic study of
doubledescentinbinaryclassification:Theimpactofloss.
Cao,K.;Wei,C.;Gaidon,A.;Arechiga,N.;andMa,T.2019.
arXivpreprintarXiv:2001.11572.
Learningimbalanceddatasetswithlabel-distribution-aware
Kini,G.R.;Paraskevas,O.;Oymak,S.;andThrampoulidis,
marginloss. arXivpreprintarXiv:1906.07413.
C. 2021. Label-Imbalanced and Group-Sensitive Classifi-
Chawla,N.V.;Bowyer,K.W.;Hall,L.O.;andKegelmeyer,
cationunderOverparameterization. acceptedtotheThirty-
W.P.2002. SMOTE:syntheticminorityover-samplingtech-
fifthConferenceonNeuralInformationProcessingSystems
nique. Journalofartificialintelligenceresearch,16:321â€“
(NeurIPS).
357.
Kleinberg,J.;Mullainathan,S.;andRaghavan,M.2016. In-
Chen,X.;andHe,K.2021. Exploringsimplesiameserep-
herenttrade-offsinthefairdeterminationofriskscores.arXiv
resentationlearning. InProceedingsoftheIEEE/CVFCon-
preprintarXiv:1609.05807.
ferenceonComputerVisionandPatternRecognition,15750â€“
Kubat, M.; Matwin, S.; et al. 1997. Addressing the curse
15758.
of imbalanced training sets: one-sided selection. In Icml,
Cui,Y.;Jia,M.;Lin,T.-Y.;Song,Y.;andBelongie,S.2019.
volume97,179â€“186.Citeseer.
Class-balancedlossbasedoneffectivenumberofsamples.
Lahoti,P.;Beutel,A.;Chen,J.;Lee,K.;Prost,F.;Thain,N.;
InProceedingsoftheIEEE/CVFConferenceonComputer
Wang,X.;andChi,E.2020. Fairnesswithoutdemograph-
VisionandPatternRecognition,9268â€“9277.
icsthroughadversariallyreweightedlearning. Advancesin
Deng, Z.; Kammoun, A.; and Thrampoulidis, C. 2019. A
neuralinformationprocessingsystems,33:728â€“740.
ModelofDoubleDescentforHigh-dimensionalBinaryLin-
Li,M.;Zhang,X.;Thrampoulidis,C.;Chen,J.;andOymak,
earClassification. arXivpreprintarXiv:1911.05822.
S. 2021. AutoBalance: Optimized Loss Functions for Im-
Duchi,J.C.;andNamkoong,H.2021. Learningmodelswith balancedData. InBeygelzimer,A.;Dauphin,Y.;Liang,P.;
uniformperformanceviadistributionallyrobustoptimization. andVaughan,J.W.,eds.,AdvancesinNeuralInformation
TheAnnalsofStatistics,49(3):1378â€“1406. ProcessingSystems.
Feldman,M.;Friedler,S.A.;Moeller,J.;Scheidegger,C.; Li, T.; Cao, P.; Yuan, Y.; Fan, L.; Yang, Y.; Feris, R. S.;
andVenkatasubramanian,S.2015. Certifyingandremoving Indyk, P.; and Katabi, D. 2022. Targeted supervised con-
disparateimpact. Inproceedingsofthe21thACMSIGKDD trastivelearningforlong-tailedrecognition. InProceedings
internationalconferenceonknowledgediscoveryanddata oftheIEEE/CVFConferenceonComputerVisionandPattern
mining,259â€“268. Recognition,6918â€“6928.
Feldman, V. 2020. Does learning require memorization? Liu,Z.;Miao,Z.;Zhan,X.;Wang,J.;Gong,B.;andYu,S.X.
a short tale about a long tail. In Proceedings of the 52nd 2019. Large-scalelong-tailedrecognitioninanopenworld.
AnnualACMSIGACTSymposiumonTheoryofComputing, InProceedingsoftheIEEE/CVFConferenceonComputer
954â€“959. VisionandPatternRecognition,2537â€“2546.
Hardt, M.; Price, E.; and Srebro, N. 2016. Equality Maldonado,S.;Vairetti,C.;Fernandez,A.;andHerrera,F.
of opportunity in supervised learning. arXiv preprint 2022. FW-SMOTE: A feature-weighted oversampling ap-
arXiv:1610.02413. proachforimbalancedclassification. PatternRecognition,
Hashimoto,T.;Srivastava,M.;Namkoong,H.;andLiang,P. 124:108511.
2018. Fairnesswithoutdemographicsinrepeatedlossmini- Menon,A.K.;Jayasumana,S.;Rawat,A.S.;Jain,H.;Veit,
mization. InInternationalConferenceonMachineLearning, A.;andKumar,S.2020. Long-taillearningvialogitadjust-
1929â€“1938.PMLR. ment. arXivpreprintarXiv:2007.07314.Michel, P.; Hashimoto, T.; and Neubig, G. 2021. Model- Learning with Long-Tailed Distributions. arXiv preprint
ingthesecondplayerindistributionallyrobustoptimization. arXiv:1912.04486.
arXivpreprintarXiv:2103.10282. Zhang, X.; Fang, Z.; Wen, Y.; Li, Z.; and Qiao, Y. 2017.
Montanari, A.; Ruan, F.; Sohn, Y.; and Yan, J. 2019. The Rangelossfordeepfacerecognitionwithlong-tailedtraining
generalizationerrorofmax-marginlinearclassifiers:High- data. InProceedingsoftheIEEEInternationalConference
dimensional asymptotics in the overparametrized regime. onComputerVision,5409â€“5418.
arXivpreprintarXiv:1911.01544. Zhang,Y.;Kang,B.;Hooi,B.;Yan,S.;andFeng,J.2023.
Morik,K.;Brockhausen,P.;andJoachims,T.1999. Com- Deeplong-tailedlearning:Asurvey. IEEETransactionson
biningstatisticallearningwithaknowledge-basedapproach: PatternAnalysisandMachineIntelligence.
acasestudyinintensivecaremonitoring. Technicalreport,
TechnicalReport.
Reed,S.;Lee,H.;Anguelov,D.;Szegedy,C.;Erhan,D.;and
Rabinovich,A.2014.Trainingdeepneuralnetworksonnoisy
labelswithbootstrapping. arXivpreprintarXiv:1412.6596.
Sagawa,S.;Koh,P.W.;Hashimoto,T.B.;andLiang,P.2019.
Distributionallyrobustneuralnetworksforgroupshifts:On
theimportanceofregularizationforworst-casegeneralization.
arXivpreprintarXiv:1911.08731.
Sagawa,S.;Raghunathan,A.;Koh,P.W.;andLiang,P.2020.
An investigation of why overparameterization exacerbates
spuriouscorrelations. InInternationalConferenceonMa-
chineLearning,8346â€“8356.PMLR.
Sahu, A. K.; Li, T.; Sanjabi, M.; Zaheer, M.; Talwalkar,
A.; and Smith, V. 2018. On the convergence of federated
optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127,3.
Tan,J.;Wang,C.;Li,B.;Li,Q.;Ouyang,W.;Yin,C.;and
Yan,J.2020. Equalizationlossforlong-tailedobjectrecog-
nition. InProceedingsoftheIEEE/CVFconferenceoncom-
putervisionandpatternrecognition,11662â€“11671.
Tanaka,D.;Ikami,D.;Yamasaki,T.;andAizawa,K.2018.
Jointoptimizationframeworkforlearningwithnoisylabels.
InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,5552â€“5560.
Wallace, B. C.; Small, K.; Brodley, C. E.; and Trikalinos,
T. A. 2011. Class imbalance, redux. In 2011 IEEE 11th
internationalconferenceondatamining,754â€“763.Ieee.
Williamson,R.;andMenon,A.2019. Fairnessriskmeasures.
In International Conference on Machine Learning, 6786â€“
6797.PMLR.
Xie, S.; Zheng, H.; Liu, C.; and Lin, L. 2018. SNAS:
stochastic neural architecture search. arXiv preprint
arXiv:1812.09926.
Ye,H.-J.;Chen,H.-Y.;Zhan,D.-C.;andChao,W.-L.2020.
Identifyingandcompensatingforfeaturedeviationinimbal-
anceddeeplearning. arXivpreprintarXiv:2001.01385.
Zafar, M. B.; Valera, I.; Gomez Rodriguez, M.; and Gum-
madi, K. P. 2017. Fairness beyond disparate treatment &
disparateimpact:Learningclassificationwithoutdisparate
mistreatment. InProceedingsofthe26thinternationalcon-
ferenceonworldwideweb,1171â€“1180.
Zhai,R.;Dan,C.;Suggala,A.;Kolter,J.Z.;andRavikumar,
P.2021. BoostedCVaRClassification. AdvancesinNeural
InformationProcessingSystems,34.
Zhang,J.;Liu,L.;Wang,P.;andShen,C.2019. ToBalance
or Not to Balance: A Simple-yet-Effective Approach forTrain dataset Feature dictionary Hyperparameter Train Test
dataset dataset
â€¦ ğ‘€ ğ¾
Attribute ğ¾s ğ‘€ â€¦ ğ‘  â€¦ â€¦ â€¦ ğ¾ â€¦â€¦ ğ‘€ Bi- Post-
â€¦
â€¦â€¦â€¦â€¦â€¦ â€¦
â€¦
ğ‘› ğ¾ â€¦â€¦ â€¦ Mâ‰ªğ¾ = ğ‘  â€¦ â€¦â€¦ âŸ¹ level hoc Evaluate
A2H
Figure 4: The overview of CAP approach. CAP is the overall framework proposed in our paper, with A2H being the core
algorithm.A2Hisameta-strategythattransformstheclass-attributepriorknowledgeintohyper-parameterS foreachclass
throughatrainablematrixW,formingatrainingstrategythatsatisfiesthedesiredfairnessobjective.Thelefthalfofthefigure
specificallyillustrateshowouralgorithmcalculatesandtrainstheweights.Inthefirststage,wecollectclass-relatedinformation
andconstructanattributetableofnÃ—K dimension.Thisisageneralprior,whichisrelatedtothedistributionoftrainingdata,
thetrainingdifficultyofeachclass,andotherfactors.Then,hefirststepofA2HistocomputeaKÃ—M FeatureDictionary
D =F(A)byapplyingasetoffunctionsF.WeremarkthatM <<K andM isonlyrelatedtothenumberofattributesnand
|F|,makingitaconstant.Therefore,thesearchspaceisO(1).Then,inthesecondstep,theweightmatrixWistrainedthrough
bi-levelorpost-hocmethodstoconstructthehyperparameterS.
A Listoffairnessobjectives
Welistallthenotationofobjectivesweusedinthemainpaperinthissection.
Symbol Meaning
â„“,f Lossfunction(specificallycross-entropy),predictor
Err(f) Erroroff onentirepopulation
Err Class-conditionalerroroff onclassK=k
k
Err Standardmisclassificationerror
plain
Err Balancedmisclassificationerror,averageofclass-conditionalerrors
bal
Err Weightedmisclassificationerror
weighted
Err Standarddeviationofclass-conditionalerrors
SDev
Quant Errorsofquantileclassesatlevela
a
CVaR Conditionalvalueoferrorsatlevela
a
R(Err) Aggregationofclass-conditionalerrors
B Frameworkoverview.
C ExtendedDiscussionofWarm-upandTrainingStability
InSec.3.2,wediscusshowCAPstabilizesthetrainingandeasesthenecessarilyofwarm-up.Now,weextendthediscussion
andprovidemoreexperimentstodemonstratefurtherthebenefitoftheCAPstrategyinthissection.InTable5,weconduct
experimentsonbilevellossfunctiondesignonCIFAR10-LT.Firstly,weinvestigatetheperformanceofthedefaultinitialization
(DI)ofPlain wherel=0andâˆ†=1with100,120and200warm-upepochs.Thenweprovidetheresultwherelstarts
Bilevel
withlogitadjustmentprior.Finally,weimplementtheself-supervisionpre-trainedmodelbySimSiam(ChenandHe2021).
Table5presentstherelationshipbetweentheErr ofthepre-trainedmodelandthefinalErr afterbileveltraining.Onedirect
SDev bal
observationisthatErr highlycorrelateswithErr .ConsideringErr measuresthefairnessofthepre-trainedmodel,we
SDev bal SDev
believethatabetterpre-trainedmodelpromotesthetestperformanceaccordingly.
Moreover,regardingLAinitialization,onecanconcludethatinitializingthetrainingwithadesignedlosssuchasLAlosscan
significantlyimprovetheresult.Still,itrequiresadditionaleffortandexpertiseindesigningthatspecificloss,especiallywhenthe
fairnessobjectiveisnotonlybalancederrorandvariousheterogeneitiesexistinthedata.Whiletheself-supervisedpre-trained
modelachievesthebestErr andErr amongallmethods,trainingtheself-supervisionmodelrequiresalongtime.Our
SDev bal
proposedCAP ,whichutilizestheattributes,notonlyensurestotakeadvantageofpriorknowledgebutalsostabilizes
Bilevel
theoptimizationbysimultaneouslyupdatingweightsofallclassesthankstothedictionarydesign.CAP achieves20.16
Bilevel
Err onCIFAR10-LTand56.55Err onCIFAR100-LTwithonly5epochsofwarm-up,whichimprovesonbothcomputation
bal bal
efficiencyandtestperformance.
D Furtherpost-hocdiscussion
Connectiontopost-hocadjustmentTobetterunderstandthepotentialofCAPandtheconnectionbetweenlossfunctiondesign
andpost-hocadjustment,wedesignanexperimentwithresultsshowninTable6.Inthisexperiment,weusethesamedictionary,
splittheoriginaltrainingdatato80%trainand20%validation,andtrainamodelf usingregularcross-entropylossonthe80%
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦Train Dataset Train Dataset Test Dataset
20% Validation 80% Training
Network trained
Bi- Optimize ğ‘¾to Network trained ğ‘¾â‹† using optimal hyper-
level meet fairness using ğ‘º=ğ‘¾ğ‘«" parameter ğ‘ºâˆ— Fairness-focused
network
objectives.
Optimal ğ‘¾â‹†by A2H Optimal model
Search phase Retrain phase Evaluation phase
Training Validation Transferring from Test Dataset
post-hoc to loss
function design
Post Network trained Optimize the post-hoc
-hoc without hyper- function gfor fairness Fairness-focused network
parameters objectives with post-hoc function
Optimal model Optimal g
Pre-train phase Post-hoc Evaluation phase
training phase
Figure5:CAPframeworkfordetailedimplementation.ThisfigureillustrateshowCAPisimplementedunderbi-leveloptimization
andpost-hocoptimization.Throughouttheentirefigure,theonlytrainableparametersareWandthenetwork(inthegreenbox).
Inthesearchphaseofbileveloptimization,wefirstconductan80-20%train-valsplit.Then,wetrainthenetworkwithparametric
lossfunctionforinneroptimizationon80%trainingdatasetandtrainWtoachievefairnessobjectiveforouteroptimizationon
20%validationdataset.Andinpost-hocimplementation,wefirsttrainthenetworkwithouthyperparametersonthetraining
datasetanddothepost-hocoptimizationonthevalidationset.Bothbilevelandpost-hocyieldoptimalfairnessweightWâˆ—,for
bi-levelandpost-hoctransferring,weusetheoptimalWâˆ—toretrainafairness-focusedmodelontheentiretrainingdataset.If
onlypost-hocadjustmentsareconducted,wedirectlymodifythepre-trainedmodelâ€™slogitwithapost-hocfunction.
DI,100epoch DI,120epoch DI,200epoch LA,120epoch Self-sup(ChenandHe2021)
Err whensearchphasebegin 0.23 0.20 0.28 0.17 0.13
SDev
Err 24.58 21.39 23.36 21.15 20.57
bal
Table5:Bileveltrainingwithdifferentwarm-upleadtodifferentresultonCIFAR10-LT.Weinvestigatetheperformanceofthe
defaultinitialization(DI)ofPlain wherel=0andâˆ†=1with100,120and200warm-upepochs,andwealsoprovide
bilevel
theresultwherelstartswithlogitadjustmentprior.Weimplementtheself-supervisionpre-trainedmodelbySimSiam(Chenand
He2021).Weremarkthat120epochsWarm-upwithDIorLAlossareusedin(Lietal.2021).
trainsetasthepre-trainedmodel,whichisbiasedtowardtheimbalanceddistribution.Ourgoalistofindapost-hocadjustmentg
sothatgâ—¦f achievesminimumbalancedlossonthe20%validationset.InTable6,thesearchingphasedisplaysthetesterrorof
adjustedmodelgâ—¦f.FollowingthetransferabilitydiscussioninSec.3.3,weusethesearchedpost-hocadjustmentastheloss
functiondesigntoretrainthemodelfromscratchontheentiretrainingdataset.Interestingly,retrainingfurtherimprovesthe
post-hocperformance.Aspost-hocadjustmentrequiresonlyabout1/5ofthetimeandfewercomputationalresourcesthanloss
functiondesign,itprovidesasimpleandefficientapproachforlossfunctiondesign.
Wealsoobservethattrainingw alongwithw leadstoperformancedegradationcomparedtoonlytrainingw ,andtraining
l âˆ† l
onlyw alsoperformsworsethanw .Weconductmoreexperimentsandprovideexplanationsforthis.Ineachpartofthe
âˆ† l
Table6,wecomparetheperformanceofoptimizinglandâˆ†inthesimilarsetup,forexample,LAprovidesadesignoflwhile
CDTadjuststhelossbydesignaspecificâˆ†.Amongallthemethods,optimizinglorw alwaysachievethebestresult.We
l
observeadegenerationwhenoptimizingonlyâˆ†orbothl&âˆ†.Throughthissection,Fig.6and7exhibitsomeinsightsand
intuitionstowardsthisphenomenon.
Fig.6showsthelogitsvaluebeforeandafterpost-hocadjustment.Withoutproperearly-stoppingorregularization,âˆ†in
Fig.6cwillkeepincreasingandresultinastretchedlogitsdistribution,wherethelogitsbecomelargerandlarger.Notethat
Fig 6c stops after 500 epochs, but longer training will even further enlarge the logits. Furthermore, because the data is not
linearseparable,âˆ†mayreducethelossinunexpectedways.ThemismatchbetweentestlossandbalancedtesterrorinFig.7b
verifiedthisconjecture.Thelossdecreasesattheendofthetrainingwhilethebalancederrorincreases.Thatmighthappen
becauseâˆ†performsamultiplicativeupdateonlogitsasshowninFig.6c.Finally,thelogitsvaluebecomesmuchlarger,but
theimprovementislimited.Lemma1inpaper(Lietal.2021)alsoofferspossibleexplanationbyprovinglossfunctionisnot
consistentforstandardorbalancederrorsiftherearedistinctmultiplicativeadjustmentsi.e.âˆ† Ì¸=âˆ† forsomei,j âˆˆ[K].
i jCIFAR10-LT CIFAR100-LT
searchphase retrain searchphase retrain
Post-hocLA(Menonetal.2020) 21.43(Â±0.30) 22.34(Â±0.34) 58.48(Â±0.23) 57.65(Â±0.25)
Post-hocCDT(Yeetal.2020) 23.58(Â±0.37) 21.79(Â±0.40) 58.60(Â±0.26) 57.86(Â±0.27)
l 20.90(Â±0.28) 21.71(Â±0.29) 57.98(Â±0.22) 57.82(Â±0.19)
Plain âˆ† 23.74(Â±0.34) 24.06(Â±0.36) 58.61(Â±0.29) 58.80(Â±0.31)
Post-hoc
l&âˆ† 23.41(Â±0.30) 23.38(Â±0.33) 57.80(Â±0.24) 58.57(Â±0.23)
w 20.81(Â±0.15) 20.65(Â±0.36) 57.73(Â±0.25) 57.15(Â±0.30)
l
CAP w 22.31(Â±0.38) 21.06(Â±0.43) 58.07(Â±0.32) 57.26(Â±0.35)
Post-hoc âˆ†
w &w 20.87(Â±0.38) 20.32(Â±0.64) 57.63(Â±0.26) 57.08(Â±0.21)
l âˆ†
Table6:Balancederroronlong-taileddatausingpost-hoclogitsadjustment.Thesearchphaseresultsrevealthetestaccuracyof
post-hocadjustment,whichissearchedona20%validationset.Theretrainresultsshowthetransferabilityfrompost-hoclogits
adjustmenttolossfunctiondesign.
Decisionboundary Decisionboundary Decisionboundary
Majoritylogitsvalue(f y=0(x)) Majoritylogitsvalue(f y=0(x)) Majoritylogitsvalue(f y=0(x))
(a)Pretrain (b)CAP :w (c)CAP :w
Post-hoc l Post-hoc âˆ†
Figure6:Theevolutionoflogitsinpost-hoclogitsadjustmentCAPwhenoptimizingw andw individually.Inthisexperiment,
l âˆ†
wetrainaResNet-32asthepre-trainedmodelonCIFAR10-LT,wheretheclassy =0hasthelargestsamplesizeandy =9
hasthesmallestsamplesizewhentraining.InFig.6a,weplotthelogitsvalueftest(x)oftestdataset.Specifically,forbetter
y
visualizationandunderstanding,weonlypicktwoclasses,thelargestclass(y =0)asmajorityandthesmallestclass(y =9)
asminority.Thex-axisisthelogitvalueofmajorityclassf (x)andthey-axisisthelogitvalueofminorityclassf (x).
y=0 y=9
Thus,theblueline(y =x)canbetreatedasthedecisionboundarybetweenthetwoclasses.InFig.6bshowsthelogitsafter
CAP withonlyoptimizingw andFig.6bshowsthelogitsafterCAP thatonlyoptimizingw .Forclarification,the
Post-hoc l Post-hoc âˆ†
logitsaredirectlypickedfromCIFAR10-LTclassificationproblemwhicharenotbinaryclassificationlogits.Wealsoremark
thatanychoiceofmajorityandminoritythatsatisfiesNtrain >Ntrain showsthesimilarresultevenunderanothertraining
majority minority
distributiondifferedfromCIFAR10-LT(e.g.flippingtheminorityandmajority).
Insum,themaindifferencebetweenusingthetwodifferenthyperparametersforpost-hoclogitadjustmentisthatlperforms
anadditiveupdateonlogits,however,âˆ†performsamultiplicativeupdate.Thatwillleadstodifferentbehaviors.Forexample,
ifthereisatruebutrarelabelk =iwithnegativelogitsvalueo ;meanwhile,thereareotherlabelswithpositiveornegative
i
values,multiplicativeupdateusingâˆ†couldnâ€™thelplabelkchangestheclassbecausethelogitsisalreadynegative.Forpost-hoc
logitadjustmentusingl,itcaneliminatetheinfluenceoftheoriginalvalue.Smallervaluesofl couldalwaysmakeoË† =o âˆ’l
i i i i
havealargerboostthanoË† Fig.6indeedshowsthatthereexistmanysampleslikethis.
kÌ¸=i
E ProofsofFisherConsistencyonWeightedLoss
FormoreinsightoftheweightedtestlosswediscussedinSec.4.3,(Menonetal.2020)proposesafamilyofFisherconsistent
pairwisemarginlossas
(cid:88)
â„“(y,f(x))=Î± yÂ·log[1+ eâˆ† yyâ€² Â·ef yâ€²(x)âˆ’fy(x)]
yâ€²Ì¸=y
wherepairwiselabelmarginsâˆ† denotesthedesiredgapbetweenscoresfory andyâ€².Logitadjustmentloss(Menonetal.
yyâ€²
2020)correspondstothesituationwhereÎ± = 1andâˆ† = logÏ€ yâ€² whereÏ€ = P(y).Theyestablishthetheoryshowing
y yyâ€² Ï€y y
thatthereexistsafamilyofpairwiseloss,whichFisherconsistentwithbalancedlosswhenâˆ† =logÎ± yâ€²Ï€ yâ€² foranyÎ±âˆˆRK.
yyâ€² Î±yÏ€y +
))x(
9=yf(eulavstigolytironiMEpoch Epoch
(a)CAP :w (b)CAP :w
Post-hoc l Post-hoc âˆ†
Figure7:TesterrorandlossduringCIFAR10-LTpost-hoctraining.InFig.7aweonlyoptimizew andweobservethatbalanced
l
testerrordecreaseswithtestlosssimultaneously.However,inFig.7bwhereweonlyoptimizew ,thetestloss(theorange
âˆ†
curve)iskeepingdecreasing,buttestbalancederror(thebluecurve)firstreachesminimumandthenincreases.Thismismatch
togetherwithFig.6furtherexplainthereasonofdegenerationwhenoptimizew bypost-hoc.
âˆ†
However,Sec.4.3focusesontheweightedlosswhichismoregeneralandformulatedasfollowing.
(cid:88)
â„“ Ï‰(y,f(x))=Î± yÂ·Ï‰ ytestlog[1+ eâˆ† yyâ€² Â·ef yâ€²(x)âˆ’fy(x)] (4)
yâ€²Ì¸=y
Following(Menonetal.2020),Thm.1deducesthefamilyofFisher-consistentlosswithweightedpairwiseloss.Thefollowed
discussiondemonstratesthatCAPusingA andA isabletorecoverFisher-consistentlossforanyÏ‰test.
FREQ WEIGHTS
Theorem1. ForanyÎ´ âˆˆRK,theweightedpairwiseloss(4)isFisherconsistentwithweightsandmargins
+
Î´
Î± = y âˆ† =log(Î´â€²/Î´ )
y Ï€ yyâ€² y y
y
Proof. Supposeweusemarginâˆ† =logÎ´ yâ€²,theweightedlossbecome
yyâ€² Î´y
â„“ (y,f(x))=âˆ’Ï‰testlog
Î´ yefy(x)
Ï‰ y (cid:80) yâ€²âˆˆ[K]Î´ yâ€²ef yâ€²(x)
efy(x)+log(Î´y)
=âˆ’Ï‰testlog
y (cid:80) ef yâ€²(x)+log(Î´ yâ€²)
yâ€²âˆˆ[K]
LetP (y |x)âˆÏ‰ P(y |x)denotethedistributionwithweightingÏ‰.TheBayes-optimalscoreoftheweightedpairwiseloss
Ï‰ y
willsatisfyfâˆ—(x)+log(Î´ )=logP (y |x),whichisfâˆ—(x)=logPÏ‰(y|x).
y y Ï‰ y Î´y
SupposewehaveagenericweightsÎ±âˆˆRK,theriskwithweightedlosscanbewrittenas
+
(cid:88)
E [â„“ (y,f(x))]= Ï€ Â·E [Î± â„“ (y,f(x))]
x,y Ï‰,Î± y x|y=y y Ï‰
yâˆˆ[L]
(cid:88)
= Ï€ Î± Â·E [â„“ (y,f(x))]
y y x|y=y Ï‰
yâˆˆ[L]
(cid:88)
âˆ Ï€Â¯ Â·E [â„“ (y,f(x))]
y x|y=y Ï‰
yâˆˆ[L]
whereÏ€Â¯ âˆÏ€ Î± .ThatmeansbymodifythedistributionbasetoÏ€Â¯,learningwiththeÏ‰andÎ±weightedloss4isequivalentto
y y y
learningwiththeÏ‰weightedloss.Undersuchadistribution,wehaveclass-conditionaldistribution.
P (x|y)Â·Ï€Â¯ Ï€Â¯ P (x)
P(y |x)= Ï‰ y =P (y |x)Â· y Â· Ï‰ âˆP (y |x)Â·Î± Ï‰test
P(x) Ï‰ Ï€ P(x) Ï‰ y y
y
ThenforanyÎ´ âˆˆ RK,letÎ± = Î´y,theBayes-optimalscorewillsatisfyfâˆ—(x) = logP(y|x) = logPÏ‰(y|x) +C(x)where
+ Ï€y y Î´y Ï€y
C(x)doesnotdependony.Thus,argmax fâˆ—(x)=argmax PÏ‰(y|x),whichistheBayes-optimalpredictionforthe
yâˆˆ[L] y yâˆˆ[L] Ï€y
weightederror.
rorredecnalaB
ssoL
rorredecnalaB
ssoLInconclusion,thereisaconsistentfamilyofweightedpairwiselossbychooseanysetofÎ´ >0andletting
y
Î´
Î± = y
y Ï€
y
Î´
âˆ† =log
yâ€²
.
yyâ€²
Î´
y
Corollary1.1. InCAP,settingattributesas[A ,A ],F =[log(Â·)].Whenw =[1,âˆ’1],CAPfullyrecoversaloss(5),
FREQ WEIGHTS l
whichisFisher-consistentwithweightedpairwiseloss.
Proof. ThisresultcanbedirectlydeducedbysettingÎ´ = Ï€y .Wehave
y Ï‰test
y
Ï€ Ï‰test
Î± =1/Ï‰test and âˆ† = yâ€² y
y y yyâ€² Ï€ Ï‰test
y yâ€²
Thenthecorrespondinglogit-adjustedlosswhichisFisher-consistentwithweightedpairwiselossis
â„“(y,f(x))=âˆ’Î± Ï‰testlog
Î´ yÂ·efy(x)
=âˆ’log
efy(x)+logÏ€yâˆ’logÏ‰ ytest
. (5)
y y (cid:80) yâ€²âˆˆ[L]Î´
yâ€²
Â·ef yâ€²(x) (cid:80) yâ€²âˆˆ[L]ef yâ€²(x)+logÏ€ yâ€²âˆ’logÏ‰ yte â€²st
ForaforementionedCAPsetup,wehaveD=[log(Ï€),log(Ï‰test)],sotheCAPadjustedlosswithw =[1,âˆ’1]is
y l
efy(x)+logÏ€yâˆ’logÏ‰ ytest
â„“ (y,f(x))=âˆ’log . (6)
CAP (cid:80) ef yâ€²(x)+logÏ€ yâ€²âˆ’logÏ‰ yte â€²st
yâ€²âˆˆ[L]
Whichisexactlythesameas5.
F MultipleAttributesBenefitAccuracyinGMM
Inthissection,wegiveasimpletheoreticaljustificationwhymultipleattributesactingsynergisticallycanfavoraccuracy.To
illustratethepoint,weconsiderabinaryGaussianmixturemodel(GMM),wheredatafromthetwoclassesaregeneratedas
follows:
(cid:26)
+1 ,withprob.Ï€
y = and x|y âˆ¼N(yÂµ,Ïƒ I ). (7)
âˆ’1 ,withprob.1âˆ’Ï€ y d
NoteherethatthetwoclassescanbeimbalanceddependingonthevalueofÏ€ âˆˆ(0,1),whichmodelsclassfrequency.Also,the
twoclassesareallowedtohavedifferentnoisevariancesÏƒ .Thisisourmodelforthedifficultyattribute:examplesgenerated
Â±1
fromtheclasswithhighestvarianceareâ€œmoredifficult"toclassifyastheyfallfurtherapartfromtheirmean.Intuitively,aâ€œgood"
classifiershouldaccountforbothattributes.Weshowherethatthisisindeedthecaseforthemodelabove.
Oursettingisasfollows.LetnIIDsamples(x ,y )fromthedistributiondefinedin(7).Withoutlossofgenerality,assumeclass
i i
y =+1isminority,i.e.Ï€ <1/2.Wetrainlinearclassifier(w,b)bysolvingthefollowingcost-sensitivesupport-vector-machines
(CS-SVM)problem:
(cid:26)
Î´ y =+1
(wË† ,Ë†b ):=argminâˆ¥wâˆ¥ sub.toy (xTw+b)â‰¥ i . (8)
Î´ Î´ w,b 2 i i 1 y i =âˆ’1
Here,Î´isahyperparameterthatwhentakingvalueslargerthanone,itpushestheclassifiertowardsthemajority,thusgiving
largermargintotheminorities.Inparticular,settingÎ´ =1recoversthevanillaSVM.ThereasonwhyCS-SVMisparticularly
relevanttooursettingisthatitrelatescloselytotheVS-loss.Specifically,(Kinietal.2021)showthatinlinearoverparameterized
(akad>n)settingstheVS-losswithmultiplicativeweightsâˆ† 1leadstosameperformanceastheCS-SVMwithÎ´ =âˆ† /âˆ† .
Â± + âˆ’
Finally,givenCS-SVMsolution(wË† ,Ë†b ),wemeasurebalancederrorasfollows:
Î´ Î´
(cid:110) (cid:111)
R (Î´):=P y(xTwË† +Ë†b )>0 .
bal (x,y)âˆ¼(7) Î´ Î´
Weask:HowdoestheoptimalCS-SVMclassifier(i.e,theoptimalhyperparameterÎ´)dependonthedataattributes,i.e.on
thefrequencyÏ€ andonthedifficultyÏƒ /Ïƒ ?Toanswerthisweconsiderahigh-dimensionalasymptoticsettinginwhich
+1 âˆ’1
n,d â†’ âˆatalinearrated/n =: dÂ¯.Thisregimeisconvenientaspreviousworkhasshownthatthelimitingbehaviorofthe(a) (b)
Figure8:Theoptimalhyperparameterdependsonbothattributes:frequency(Ï€)anddifficulty(Ïƒ /Ïƒ ).
+ âˆ’
balancederrorR (Î´)canbecapturedpreciselybyanalyticformulas(Deng,Kammoun,andThrampoulidis2019;Montanari
bal
etal.2019).Specifically,(Kinietal.2021)usedthatanalysistocomputeformulasfortheoptimalhyperparameterÎ´.However,
theyonlydiscussedhowÎ´varieswiththefrequencyattributedandonlystudiedscenarioswherebothclassesareequallydifficult,
i.e.Ïƒ =Ïƒ .Ourideaistoextendtheirstudytoinvestigateapotentialsynergisticeffectoffrequencyanddifficulty.
+1 âˆ’1
Figure8confirmsourintuition:theoptimalhyperparameterÎ´ dependsbothonthefrequencyandonthedifficulty.Specifically,
âˆ—
weseeinbothFigures8(a,b)thattheeasiertheminorityclass(aka,thesmallerratioÏƒ /Ïƒ ),Î´decreases.Thatis,thereisless
+1 âˆ’1
needtofavormuchlargermargintotheminority.Ontheotherhand,asÏƒ /Ïƒ increasesandminoritybecomesmoredifficult,
+1 âˆ’1
evenlargermarginisfavoredforit.Finally,comparingFigures8(a)toFigure8(b),notethatÎ´ takeslargervaluesforlarger
âˆ—
imbalanceratio(i.e.,smallerfrequencyÏ€),againaggreeingwithourintuition.
G Experimentdetailsandreproducibility
The functions are always fixed regardless of the datasets and objectives change F = [log(A),A,AÎ²,A2Î²,A4Î²]. In our
experiments,wesetÎ² =0.075.
ForreproducedresultinTable2,wegridsearchonvalidationdatasetandretrainforfaircomparison,sotheresultisworse
thanthevaluereportedin(Menonetal.2020;Yeetal.2020)whicharegridsearchedonwholetestdataset.
Forbileveltraining,followingthetrainingprocessin(Lietal.2021),westartthevalidationoptimizationafter120epochs
warmup and training 300 epochs in total. The learning rate decays at epochs 220 and 260 by a factor 0.1. The lower-level
optimizationuseSGDwithaninitiallearningrate0.1,momentum0.9,andweightdecay1eâˆ’4,over300epochs.Atthesame
time,theupper-levelhyper-parameteroptimizationalsousesSGDwithaninitiallearningrate0.05,momentum0.9,andweight
decay1eâˆ’4.Togetbetterresults,weinitializelusingLAlossinexperimentsinTable2.Forafaircomparison,thereisno
initializationinotherexperiments.ForLAandCDTresultsinTable2,wedogridsearchontheimbalancedvalidationdataset
andretrainforafaircomparison.
Formostoftheexperiments,exceptErr inTable3,weplotorreporttheaverageresultof3runs.ForErr where
weighted weighted
thetargetweightÎ±wasgeneratedrandomly,werepeattentimeswithdifferentrandomseeds.Wereporttheaverageresultof10
trailsofdifferentÏ‰testforErr .Ateachtrial,weightsÏ‰testaregeneratedi.i.d.fromtheuniformdistributionover[0,1]and
weighted k
thennormalized.
Alltheexperimentsarerunwith2GeForceRTX2080TiGPUs.