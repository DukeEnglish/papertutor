Counterfactual Fairness through Transforming Data
Orthogonal to Bias
ShuyiChen ShixiangZhu
shuyic@alumni.cmu.edu shixianz@andrew.cmu.edu
CarnegieMellonUniversity CarnegieMellonUniversity
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
ABSTRACT Fairly hired Obs hired Unfairly hired
Machinelearningmodelshaveshownexceptionalprowessinsolv- Fairly not hired Obs not hired Unfairly not hired
ingcomplexissuesacrossvariousdomains.Nonetheless,thesemod-
elscansometimesexhibitbiaseddecision-making,leadingtodis-
paritiesintreatmentacrossdifferentgroups.Despitetheextensive
researchonfairness,thenuancedeffectsofmultivariateandcon-
tinuoussensitivevariablesondecision-makingoutcomesremain
insufficientlystudied.Weintroduceanoveldatapre-processingal-
gorithm,OrthogonaltoBias(OB),designedtoremovetheinfluenceof
agroupofcontinuoussensitivevariables,therebyfacilitatingcoun-
terfactualfairnessinmachinelearningapplications.Ourapproach Female Male Socio-economic status
is grounded in the assumption of a jointly normal distribution
Figure1:Thismotivatingexampleillustratesthedistinction
withinastructuralcausalmodel(SCM),provingthatcounterfactual
betweenbinaryandcontinuoussensitivevariablesinthe
fairnesscanbeachievedbyensuringthedataisuncorrelatedwith
contextofhiringdecisions.Thedashedlinesindicatethe
sensitivevariables.TheOBalgorithmismodel-agnostic,cateringto
predictedhiringdecisionsandtheshadedareaindicatethe
awidearrayofmachinelearningmodelsandtasks,andincludesa
unbiasedtruedecisions.Theleftpanelsimplifiesthesce-
sparsevarianttoenhancenumericalstabilitythroughregulariza-
nariowithabinarysensitivevariable,suchasGender,where
tion.Throughempiricalevaluationonsimulatedandreal-world
adjustmentsforfairnessaremorestraightforwardduetothe
datasetsâ€”includingtheadultincomeandtheCOMPASrecidivism
cleardichotomyindata.Therightpanel,however,delves
datasetsâ€”our methodology demonstrates its capacity to enable
intothecomplexityintroducedbyacontinuoussensitive
faireroutcomeswithoutcompromisingaccuracy.
variable,likesocio-economicstatus,demonstratingtheintri-
catetaskofachievingfairnessacrossaspectrum.
KEYWORDS
Counterfactualfairness,Datapre-processing,AlgorithmicFairness (orprotected)variableofanindividualweredifferent,allelsebe-
ingequal.Thisconceptisparticularlypowerfulasitalignsclosely
withintuitivenotionsofindividualfairnessandjustice,offeringa
1 INTRODUCTION rigorousstandardagainstwhichtomeasureandrectifybias.
Inrecentyears,machinelearninghasemergedasapivotaltechnol- Numerous previous research efforts have focused on attain-
ogy,drivingadvancementsacrossabroadspectrumofreal-worldap- ingcounterfactualfairness,makingsignificantstridesinthisarea
plications,fromhealthcarediagnostics[16],hiringdecision-making [11,19,20,24,30].Akeyexampleisthestudyby[30],whichtargets
systems [12], to loan assessments [26]. Its ability to learn from non-discriminationthroughadjustingthepredictionsbasedonthe
and make predictions or decisions based on large data sets has empiricaljointdistributionofthedata,aimingtoachieveequalbias
beentransformativeinaddressingcomplexproblems.However, andaccuracyacrossdifferentgroups.However,theseapproaches
thebroaderapplicationprospectsofsomemachinelearningtech- typicallypresupposeacompleteunderstandingofthecausalre-
niquesishamperedbytheimplicitbiasesingrainedinthedatait lationshipsamongallvariableswithoutunmeasuredconfounders
learnsfrom,leadingtooutcomesthatsystematicallyandunfairly andconsidersensitivevariablestobebinary(orcategorical),so
disadvantagecertaingroups[4,9,19,23].Thisissueofbiasinma- thattheycanbeeasilyisolatedoradjusted.Asaresult,traditional
chinelearningmodelsisnotmerelyatechnicalchallengebuta methodsoffairlearningfacechallengesinaddressingsituations
fundamental concern that threatens to reinforce existing social involvingmultivariateandcontinuoussensitivevariablewithintri-
inequalities. cateinter-dependency.
Suchfairnessconcerninmachinelearninghascatalyzedagrow- Taketheemploymenthiringprocessasaninstanceshownin
ingbodyofresearchaimedatidentifying,understanding,andmiti- Figure1,socioeconomicstatuscanâ€™tbeeasilyclassifiedintoasmall
gatingbiasespresentindataandalgorithms.Amongthevarious numberofdiscretecategories.Itisacomplex,multidimensional
conceptualframeworksdevelopedtoaddressthisissue[9,15,18, variableinfluencedbyeducation,income,occupation,andevensub-
20, 34, 35], the notion of counterfactual fairness [24] stands out. tlerfactorslikeneighborhoodandparentaleducation.Acandidateâ€™s
Counterfactualfairnessseekstoensurethatadecisionmadeby profileisacomplexamalgamationoftheirexperiences,skills,edu-
amachinelearningmodelwouldremainunchangedifasensitive cationalbackground,andpersonalcharacteristics.Amodeltrained
4202
raM
62
]GL.sc[
1v25871.3042:viXraonhistoricalhiringdatamightinadvertentlylearntofavorcandi- quantifyingfairness.Researcherscommonlyadopteitherobserva-
datesfromprestigiousuniversitiesorwithcertaintypesofwork tionalorcounterfactualapproachestoformalizefairness.Obser-
experiencesâ€”criteriathatareoftencorrelatedwithsocioeconomic vationalmethodstypicallycharacterizefairnessthroughmetrics
status.Thisformofimplicitbiasemergeseitherbecausethedata derivedfromobserveddataandpredictedoutcomes[19,21,25,31].
usedfortraininglackscomprehensiverepresentationofallpossible Metricssuchasindividualfairness(IF)[15],demographicparityor
candidateprofilesorbecausetraditionalapproachestofairnessdo GroupFairness[22,35]andequalizedodds[20]fallunderthiscate-
notadequatelyaddressthesubtleinterplayandimpactofvarious gory.Thekeyideafortheobservationalfairnessmetricisviewing
sensitivevariablesondecision-makingoutcomes. fairnessastreatingsimilarindividualsorindividualsbelongingto
Totacklethesechallenges,wedevelopanoveldatapre-processing thesamegroupssimilarly.Forexample,IFdefinesfairnessastreat-
approachthataimstoremovetheinfluenceofagroupofcontinuous inganytwoindividualswhoaresimilarwithrespecttoaparticular
sensitivevariablesfromthedata,therebyensuringcounterfactual tasksimilarly[35].
fairnessinsubsequentmachinelearningtasks.Wefirstprovethat In contrast, counterfactual approaches proposes a causal ap-
thecounterfactualfairnesscanbeattainableeasilybymakingthe proachtodefiningfairness.Thesedefinitionsassessfairnessbased
datauncorrelatedwiththegroupofsensitivevariables,basedon onhowpredictionswouldchangeifsensitiveattributeswereal-
theassumptionofajointlynormaldistributionwithinastructural tered[11,19,20,24,30].Withthehelpofthepotentialoutcome
causalmodel(SCM)framework[29].Thisassumptionisapplicable concept,themeasuringoffairnessisnolongerrestrictedtothe
acrossabroadspectrumofapplicationswherestandardizationof observablequantities.Forinstance,theEqualOpportunity(EO)
dataisrequired,ensuringthatallvariables,bothsensitiveandnon- definition,akintoindividualfairness,directlycomparestheactual
sensitive,arenormalizedtohaveameanofzeroandavarianceof1. andcounterfactualdecisionsofthesameindividual,ratherthan
Motivatedbythisunderstanding,weconsiderallsensitivevariables relyingoncomparisonsbetweentheobservationoftwosimilar
collectivelyandproposeadatapre-processingalgorithm,referred individuals[30].
toasOrthogonaltoBias(OB).Thisalgorithmisdesignedforminimal Whileobservationaldefinitionsoffairnesscanbeincorporated
dataadjustmentstoachieveorthogonalitybetweennon-sensitive intooptimizationproblems,eitherbytreatingthefairnesscondi-
andsensitivedata.Tofacilitatenumericalstability,wealsopresent tionasaconstraint[15]ordirectlyoptimizingthefairnessmetric
asparsevariantofthisalgorithmwhichincorporatesaregular- asanobjectivefunction[35],achievingcounterfactualdefinitions
izationterm.Thentheresultingdataisreadytoserveasinput offairnessoftenrequireanapproximationofthecausalmodelor
formachinelearningmodelsindownstreamtaskswithoutbeing thecounterfactualssincethecounterfactualsareunobservable.For
influencedbytheundesirablebiasassociatedwiththecomplexities example,intheFairLearningalgorithmproposedby[24],theunob-
ofsensitivevariables.Itisalsoimportanttonotethatourproposed servedpartsofthegraphicalcausalmodelaresampledthroughthe
algorithmismodel-agnostic,makingitsuitableforavarietyofma- MarkovchainMonteCarlomethod.Thentheyuseonlythenon-
chinelearningmodelsandtasks.Lastly,weevaluateouralgorithmâ€™s descendantsofsensitivevariabletomakethedecision.However,
performanceonasimulateddatasetandtworeal-worlddatasets, thisapproachmaysacrificeasignificantamountofinformationin
includingtheadultincomedataandtheCOMPASrecidivismdata, thedataandleadtolowerpredictionaccuracy.Moreover,whenesti-
demonstratingthatourapproachenablesmachinelearningmodels matingthecounterfactualdistributionisnotdirectlyfeasibledueto
toachievefaireroutcomeswithcomparableaccuracytocurrent sparseorcontinuoussensitivevariables,unexpectedrelationships
state-of-the-artfairlearningmethods.Inthenumericalresults,we betweensensitiveandnon-sensitiveattributesmaypersist.This
foundthatourapproachisnotlimitedbytheassumptionsabout caneithercompromisefairnessbyconsideringvariablesrelated
datadistribution,indicatingitsapplicabilitytoawiderrangeof withsensitivevariables,orcompromisetheaccuracybyremoving
scenarios. allrelatedinformationregardingsensitivevariablesfromthetrain-
Ourcontributionsinthisworkcanbesummarizedasfollows: ingdata.Aswediscusslater,ouraimistominimizedatachanges
(1) We show that achieving counterfactual fairness is feasi- whileensuringcounterfactualfairnessundercertainassumptions,
blebyensuringorthogonalitybetweennon-sensitiveand therebybetteraddressingthetrade-offbetweenaccuracyandfair-
sensitivedatawhentheyarejointlynormal. ness.
(2) Weintroduceamodel-agnosticdata-pre-processingalgo-
rithm,termedasOrthogonaltoBias(OB),whichfacilitates
counterfactualfairnessacrossabroadspectrumofdown- Fairlearningapproaches. Fairnesslearninginmachinelearning
streammachinelearningapplications. aimstopreventdiscriminationandcanbegenerallycategorized
(3) Wevalidatetheenhancedefficacyofouralgorithmcom- intothreestages.Firstly,pre-processingapproaches[10,13,19],
paredtotheexistingstate-of-the-artsthroughevaluations whicharemostcloselyrelatedtoourwork,involvemodifyingthe
onbothsyntheticandreal-worlddatasets. datatoeliminateorneutralizeanypreexistingbias,followedbythe
applicationofstandardMLtechniques.Secondly,in-processingap-
proaches[2,28]eitherbyafairnessregularizertothelossfunction
2 LITERATUREREVIEW
objective,whichpenalizesdiscrimination,orimposingaconstraint,
Thisworkisrelatedtoseveralstreamsofalgorithmicfairnessliter-
therebymitigatingdisparatetreatment.Suchmethodsarenormally
aturewhichwereviewinthissection.
model-spefiic.Thethirdtypeofapproachispost-processingap-
FairnessinMachineLearning. Thepursuitoffairdecision-making proachesadjustpredictorslearnedusingstandardMLtechniques
inmachinelearninghasledtodiverseapproachesfordefiningand afterthefacttoenhancetheirfairnessproperties[7,14,17,20].
2ğ‘ˆ ğµ ğ´ ğ‘ˆ ğ‘ˆ ğµ ğ´ ğ‘ˆ
$ ! $ !
ğ‘¨!âŠ¥ğ‘©
ğ´& mi
n
A fair predictor ğ‘Œ$ needs ğ‘¨ Orthogonal to Bias
to take into account the ğ‘Œ is implicitly affected âˆ’
influence of ğµ by ğµ through ğ´ ğ‘¨!
ğ‘ˆ ğ‘Œ$ ğ‘Œ ğ‘ˆ ğ‘ˆ ğ‘Œ$ ğ‘Œ ğ‘ˆ
"# " "# "
(a)TheSCMandthetypicalfairlearning (b)Fairlearningbytransformingdataorthogonaltobias
Figure2:Illustrationof(a)thestructuralcausalmodel(SCM)andacommonfairlearningstrategies,aswellas(b)theproposed
datapre-processingalgorithmOrthogonaltoBias(OB).Thewhitenodesğ´,ğ‘Œ,andğ‘ŒË†
arethenon-sensitivevariables,thedecision
variable,anditsprediction,respectively.Therednodesğµrepresentthesensitivevariable.Theğ´Ë†
isthetransformeddatathatis
orthogonaltobiasinğµ.Thegraynodesrepresentexogenousvariables.
Ourapproachismostrelatedtotheworkby[19],wherethe eachcomponentinğ‘‰,detailedasfollows:
authorsproposetwodistributionadjustmentprocedures,Orthogo-
ğµ=ğ‘“ ğµ(ğ‘ˆ ğµ),
nalizationandMarginalDistributionMapping,formakingcounter-
factuallyfairdecisionsbasedonadjusteddata.Whilebothproce-
ğ´=ğ‘“ ğ´(ğµ,ğ‘ˆ ğ´), (1)
duresremoveattributesâ€™dependenceonsensitivevariablesunder ğ‘Œ =ğ‘“ ğ‘Œ (ğ´,ğ‘ˆ ğ‘Œ).
respectiveconditions,theirmethodsprovidenoguaranteeregard- AccordingtotheaboveSCM,thebiaspresentinthesensitivevari-
ingthescaleofmodificationtothedatadistribution.Incontrast, ablesğµcantransmittothepredictorğ‘ŒË† viathenon-sensitivevari-
ourworkintroducesanexactapproachtosolvinganoptimization ablesğ´.Thismeansthat,ifthereareanydifferencesinthedistri-
problemthatguaranteesminimalmodificationtothedatawhile butionofğ´conditioningonğµ,thedecisionvariableğ‘ŒË† basedonğ´
ensuringcounterfactualfairnessunderspecificassumptions.We mightbeunfair.
believethatouremphasisonminimaldatamodificationplacesour Inthispaper,weaimtodesignapredictorğ‘ŒË† thatachievesthe
proposedalgorithminauniquepositioninthewidelyobserved
counterfactualfairness[19,24]withoutbeinginfluencedbythe
fairness-accuracyspectrum[1,6,7].Throughourapproach,we
biasinğµ.Formally,thecounterfactualfairnessinourSCMcanbe
aimtocaptureasmuchinformationaspossiblebetweenthetarget
definedasfollows:
variableğ‘Œ andfeatures,includingthesensitivefeatures.Addition-
ally,themethodproposedby[19]presupposesensitivevariables Definition 3.1 (Counterfactual Fairness). Given a new pair of
tobebinary(orcategorical)sothattheycanbeeasilyisolatedor attributes(b,a),adecisionvariableğ‘Œisconsideredcounterfactually
adjustedbasedonempiricalprobabilitymassfunction.Itdoesnot fairif,foranybâ€² âˆˆB,
vad ard ir ae bs ls es wit iu that ii no tn ris ci an tevo inlv tein r-g dm epu el nti dv ea nr cia yt ,e wo hr ec reo an sti on uu ro pu rs os pe on ss ei dti Ov Be ğ‘Œ bâ€²(ğ‘ˆ)|(cid:8)ğµ=bâˆ—,ğ´=aâˆ—(cid:9) =ğ‘‘ ğ‘Œ bâˆ—(ğ‘ˆ)|(cid:8)ğµ=bâˆ—,ğ´=aâˆ—(cid:9), (2)
algorithmaimstoremovetheinfluenceofagroupofcontinuous ğ‘‘
whereğ‘ƒ =ğ‘„indicatesthatrandomvariablesğ‘ƒ andğ‘„areequalin
sensitivevariablesfromthedata,therebyensuringcounterfactual
distribution,andğ‘Œ (ğ‘ˆ)representsthecounterfactualoutcomeof
fairnessinsubsequentmachinelearningtasks. b
ğ‘Œ whenğµ=b.
Theabovedefinitionimpliesthatthedistributionofthecounter-
factualresultshouldnotdependonthesensitivevariablescondi-
3 METHODOLOGY tionalontheobserveddata.NotethatalthoughDefinition3.1uses
3.1 Problemsetup thedecisionvariableğ‘Œ,italsoappliestoitspredictorğ‘ŒË† without
We jointly defineğ‘ non-sensitive variables asğ´ âˆˆ A âŠ† Rğ‘,ğ‘ anylossofgenerality[19].
sensitivevariablesasğµ âˆˆRğ‘,anddecisionvariableasğ‘Œ âˆˆY.The
3.2 Achievingcounterfactualfairnessviadata
data generation process in our problem setup can be described
byaStructuralCausalModel(SCM)[29]asshowninFig.2(a). decorrelation
Tobespecific,weconsiderthesetofendogenousvariablesğ‘‰ = Toclarifyandstreamlinethepresentationofourfindings,webegin
{ğµ,ğ´,ğ‘Œ,ğ‘ŒË† },where{ğµ,ğ´,ğ‘Œ}aretheobservedvariablesandğ‘ŒË† isthe byillustratingthatcounterfactualfairnesscanbeattainedunder
predictionofğ‘Œ wemadebasedonğµ andğ´.Weassumethatğ‘ˆ , conditionswheresensitiveandnon-sensitivevariablesexhibitno
ğµ
ğ‘ˆ ,andğ‘ˆ ,whicharetheexogenousvariablesthataffectğµ,ğ´, correlationandaretogethernormallydistributed.
ğ´ ğ‘Œ
andğ‘Œ respectively,areindependentofeachother.Thestructural ConsideradatasetD = {(bğ‘–,ağ‘–,ğ‘¦ ğ‘–)}ğ‘›
ğ‘–=1
withğ‘›observeddata
equationsaredescribedwiththefunctionsğ¹ ={ğ‘“ ğ‘Œ,ğ‘“ ğ´,ğ‘“ ğµ},onefor tuples,wherebğ‘–,ağ‘–,andğ‘¦
ğ‘–
representtheğ‘–-thobservationofthe
3sensitive,non-sensitive,anddecisionvariables,respectively.We SubstitutingÎ£aboveintothejointprobabilitydensityfunction
useA = [a1,...,ağ‘›]âŠ¤ âˆˆ Rğ‘›Ã—ğ‘ todenotethedatamatrixofnon- ofğ´andğµ,wehave
sensitivevariablesğ´,anduseB= [b1,...,bğ‘›]âŠ¤ âˆˆRğ‘›Ã—ğ‘ todenote
1
thedatamatrixofsensitivevariablesğµindatasetD. P(a,b)=
âˆšï¸
(2ğœ‹)ğ‘+ğ‘|Î£|Â·
Toestablishtheconnectionbetweencounterfactualfairnessand
datauncorrelation,weintroducethefollowingassumption: exp(cid:32) âˆ’1 (cid:20) aâˆ’ğğ´ (cid:21)âŠ¤ Î£âˆ’1(cid:20) aâˆ’ğğ´ (cid:21)(cid:33)
2 bâˆ’ğğµ bâˆ’ğğµ
Assumption3.2. Giventhestructuralmodeldefinedin(1),the 1
= Â·
sensitivevariableğ´andnon-sensitivevariablesğµarejointnormal. âˆšï¸ (2ğœ‹)ğ‘+ğ‘|Î£ ğ´||Î£ ğµ| (4)
(cid:18) 1 (cid:19)
Buildingonthisassumption,wepresentthefollowingtheorem: exp âˆ’ 2(aâˆ’ğğ´)âŠ¤Î£ ğ´âˆ’1 (aâˆ’ğğ´) Â·
(cid:18) 1 (cid:19)
Theorem3.3.
UnderAssumption3.2,ğ‘ŒË†
iscounterfactuallyfairwhen
exp âˆ’ 2(bâˆ’ğğµ)âŠ¤Î£ ğµâˆ’1 (bâˆ’ğğµ)
ğ´andğµareuncorrelated. =P ğ´(a)P ğµ(b).
As we can observe that the joint distribution decomposes into
theproductoftheirmarginaldistributions,hencedemonstrating
Proof. We first demonstrate that ğ‘ŒË† is counterfactually fair, theirstatisticalindependencewhenğ´andğµarejointlynormaland
uncorrelated.
whichisachievedwhenthemodelâ€™spredictionsforğ‘Œ arenotin-
Finally,establishingğ´andğµasjointlynormalanduncorrelated
fluencedbythesensitivevariableğµ.Inthefollowingproof,we
focus on the case of a binary predictorğ‘ŒË† for simplicity, noting leadstotheinferencethat,giventheSCMin(1),predictorğ‘ŒË† is
counterfactuallyfair.
thatourfindingscanbeseamlesslyappliedtopredictorsthatyield
â–¡
continuousoutcomes.Thissimplificationallowsustoestablishfair-
nessbyshowingthattheexpectedoutcomesareequivalent,which
Theorem3.3suggeststhatachievingcounterfactualfairnessin
for a Bernoulli random variable, also indicates a distributional
thepredictorğ‘ŒË† ispossiblebydecorrelatingnon-sensitivevariables
equivalence. Following the well-established â€œAbduction-Action-
Predictionâ€methodfrom[29],theconditionalexpectationofğ‘ŒË† ğ´fromsensitivevariablesğµ.Thisinsightmotivatesustodevelop
bâ€² adatapre-processingalgorithmaimedatadjustingtheobserved
givenğµ=bâˆ—,ğ´=aâˆ—canbewrittenas:
datawithminimalchangestoachieveuncorrelationbetweennon-
sensitiveandsensitivevariables.
ItisimportanttoemphasizethatAssumption3.2isapplicable
E(ğ‘ŒË† bâ€²|ğµ=bâˆ—,ğ´=aâˆ—)
acrossabroadspectrumofapplicationswherestandardizationof
âˆ« (3)
= ğ‘“ ğ‘ŒË† (cid:0)ğ‘“ ğ´(cid:0) bâ€²,ğ‘¢(cid:1);D(cid:1)P ğ‘ˆğ´|ğµ,ğ´(cid:0)ğ‘¢ |ğµ=bâˆ—,ğ´=aâˆ—(cid:1)ğ‘‘ğ‘¢, d sea nta sii ts ivr ee ,q au ri ere nd o, re mns au lir zi en dg tt oha ht aa vl elv aa mria eb anles o, fb zo et rh os ae nn dsi ativ ve ara in and cn eo on f-
1.Moreover,ourinvestigationshaverevealedthatthisassumption
isnotalwayscriticalforthesuccessofourdatapre-processing
whereğ‘“ ğ‘ŒË†(Â·;D):Aâ†’Ydenotesthepredictorofğ‘ŒË† trainedusing algorithm.Ourapproachhasdemonstratedeffectivenessevenwhen
data D and P ğ‘ˆğ´|ğµ,ğ´(ğ‘¢ |ğµ=bâˆ—,ğ´=aâˆ—) denotes theconditional appliedtodatasetsthatdonotmeetthiscriterion.Inparticular,as
densityofğ‘ˆ ğ´givenğµ=bâˆ—andğ´=aâˆ—.Toargueforcounterfactual elaboratedinSection4,ourstrategyhasproventobepromisingin
fairness,itsufficestoshow experimentsthatinvolvecategoricalsensitivevariablesğµ,rather
thancontinuousones.
E(ğ‘ŒË† bâ€²|ğµ=bâˆ—,ğ´=aâˆ—)=E(ğ‘ŒË† bâˆ—|ğµ=bâˆ—,ğ´=aâˆ—),
3.3 Orthogonaltobias
Inthissection,wedevelopadatapre-processingalgorithm,termed
asOrthogonaltoBias(OB).Wefirststandardizebothnon-sensitive
ifthedatageneratingprocessfortheobserveddatağ‘“ ğ´(b,ğ‘¢)does
variablesğ´andsensitivevariablesğµ,toachieveanormaldistribu-
notdependonthevalueofb,indicatingğ´â€™sindependencefromğµ.
tionforeach.Thentheempiricalcovariancebetweenğ´andğµcan
Next,weprovethatğ´isindependentofğµ whentheyareun-
beestimatedby
correlatedunderAssumption3.2,whichisacommonlyaccepted
s ğt ğµat ,i rs et sic pa el cr te ivs eu ll yt. ,wCo itn hs cid oe vr arth iae nm cee man atv re icc et sor Î£s ğ´fo ,Î£r ğµğ´ .a Rn ed cağµ lla ths ağ tğ´ whan end cov(ğ´,ğµ)=E[(ğ´âˆ’E[ğ´])(ğµâˆ’E[ğµ])] â‰ˆ ğ‘›1âˆ‘ï¸ğ‘› ağ‘–âŠ¤bğ‘– =âŸ¨A,BâŸ©.
ğ´andğµuncorrelated,thecovariancematrixÎ£ofğ´andğµis ğ‘–=1
Given that two variables are uncorrelated if their covariance is
zero,orthogonalitybetweenobserveddataAandBguarantees
Î£= (cid:20) Î£ 0ğ´ Î£0
ğµ
(cid:21) andÎ£âˆ’1 = (cid:20) Î£ 0ğ´âˆ’1 Î£0 ğµâˆ’1 (cid:21) . u sen rc vo er dre nl oat ni -o sn e. nT sih tie vr eef do ar te a, Athe inO sB ua cl hgo ar with aym thai am tis tt io soad rtj hu ost goth ne alo tb o-
4theobservedsensitivedataB,whileensuringminimalchangesto Algorithm1SparseOrthogonaltoBias(SOB)
non-sensitivedataA. 1: Input:Non-sensitiveandsensitivedataAandB,rankğ‘˜;
Specifically,wefollowtheideaofOrthogonaltoGroupsintro- 2: StandardizeAandB;
ducedby[3],anddefinearankğ‘˜approximationofAasA(cid:101)=SUâŠ¤, 3: forğ‘– =1,...,ğ‘˜do
whereU= [u1,...,uğ‘˜]isağ‘Ã—ğ‘˜orthonormalmatrixandS={ğ‘  ğ‘–ğ‘—} 4: Setğ‘¡ =1,ğœƒ =1,andğ‘ (0) =0;
ğ‘–
isağ‘›Ã—ğ‘˜matrix.Thegoalistofindatransformedğ‘›Ã—ğ‘matrixA(cid:101)
5:
Randomlyinitializeğ‘¢(0);
t bh yat this eo Fr rt oh bo eg no in ua sl nto oB rmwi âˆ¥t ğ‘‹h âˆ¥m Fin =im âˆšï¸ƒa (cid:205)lc ğ‘–h (cid:205)an ğ‘—g ğ‘¥e ğ‘–2 ğ‘—t .o Fm ora mtr aix ll, ya ,s wm ee aas imure tod 6: while(cid:13) (cid:13) (cid:13)ğ‘¢ ğ‘–(ğ‘¡) âˆ’ğ‘¢ ğ‘–(ğ‘¡âˆ’1)(cid:13) (cid:13) (cid:13)ğ‘– ğ¹ >ğœ‚and(cid:13) (cid:13) (cid:13)ğ‘  ğ‘–(ğ‘¡) âˆ’ğ‘  ğ‘–(ğ‘¡âˆ’1)(cid:13) (cid:13) (cid:13)ğ¹ >ğœ‚ do
solvethefollowingconstrainedoptimizationproblem: 7: Computeğ›½(ğ‘¡) with
ğ‘–
argm S,i Un(cid:13) (cid:13)Aâˆ’SUâŠ¤(cid:13) (cid:13)2 ğ¹, ğ›½ ğ‘– â† (cid:0) BâŠ¤B(cid:1)âˆ’1 BâŠ¤ğ‘ƒ ğ‘–âˆ’1Ağ‘¢ ğ‘–
s.t. (cid:10) SUâŠ¤,B(cid:11) =0, (5) 8: withğ‘ƒ ğ‘–âˆ’1=ğ¼ ğ‘›Ã—ğ‘›âˆ’(cid:205) ğ‘™ğ‘– =âˆ’ 11ğ‘  ğ‘™ğ‘  ğ‘™âŠ¤
9: Updateğ‘  ğ‘– as
where the last constraint
requirU eâŠ¤ sU U= toIğ‘˜ b,
e orthonormal matrix, ğ‘  ğ‘–(ğ‘¡) â† âˆ¥ğ‘ƒğ‘ƒ ğ‘–ğ‘– âˆ’âˆ’ 11 AA ğ‘¢ğ‘¢ ğ‘–ğ‘– âˆ’âˆ’ ğ›½ğ›½ ğ‘–ğ‘– BB
âˆ¥2
whichhelpspreventdegeneracy,suchasbasisvectorsbecoming
identicallyzeroorencounteringsolutionswithdoublemultiplicity. 10: Updateğ‘¢ ğ‘– as
Asshownby[3],theoriginalproblem(5)canbereformulated ğ‘¢(ğ‘¡)
â†
Sğœƒ (cid:0) AâŠ¤ğ‘  ğ‘–(cid:1)
usingLagrangemultipliersasfollows: ğ‘– âˆ¥Sğœƒ (AâŠ¤ğ‘  ğ‘–)âˆ¥2
The
faca tr og rm
S
2,i
U
/n ğ‘›ğ‘›1 isâˆ‘ï¸
ğ‘–
i=ğ‘›
n1
tr(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
oa dğ‘– uâˆ’ câˆ‘ï¸ eğ‘—ğ‘˜
= d1
tğ‘  oğ‘–ğ‘—u siâŠ¤ ğ‘— m(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
p2 li+ fyğ‘›2 tâˆ‘ï¸ hğ‘—ğ‘˜
= e1
eğœ† xğ‘— pâˆ‘ï¸
ğ‘–
r=ğ‘›
e1
sğ‘  sğ‘– iğ‘— oğµ nğ‘–.
for
th(6 e) 1 1
1
11 2
3
4: :
:
:
e
ifnğ‘¡w
(cid:13) (cid:13)d
Aâ†h
w
âŠ¤e hr ğ‘ ğ‘¡e
ğ‘–i (cid:13)
(cid:13)+ lS 1e1ğœƒ â‰¤(ğ‘¥ â„) t= hes nign(ğ‘¥)(|ğ‘¥|âˆ’ğœƒ)1(|ğ‘¥| â‰¥ğœƒ)
15: Setğœƒ =0
optimalsolutions.Itensuresthatthefirst-orderconditionfor(6)
16: else
withrespecttoğ‘ 
ğ‘–ğ‘—
involvesacommonfactorof2/ğ‘›,whichcan
17: Setğœƒ
>0suchthat(cid:13) (cid:13)ğ‘¢(ğ‘¡)(cid:13)
(cid:13) =â„
thenbecanceledoutduringcomputations.LetSâˆ—andUâˆ—denote (cid:13) ğ‘– (cid:13)1
theoptimalsolutionsofSandU,respectively.Asdemonstrated 18: endif
inAppendixA,thereformulatedequation(6)yieldsaclosed-form 19: endfor
solution: 20: SetSË† = [ğ‘‘1ğ‘ 1,...,ğ‘‘ ğ‘˜ğ‘  ğ‘˜]whereğ‘‘ ğ‘– =ğ‘  ğ‘–âŠ¤Ağ‘¢ ğ‘–,andUË† = [ğ‘¢1,...ğ‘¢ ğ‘˜].
ğœ†âˆ— ğ‘— =
(cid:68)
A âŸ¨u Bâˆ— ğ‘— ,âŠ¤ B,
âŸ©B(cid:69) 22 21 :: OFi una tpll uy tc :a A(cid:101)lculatetheattributematrixA(cid:101)=SË†UË†âŠ¤.
(7)
Uâˆ—= (cid:2) uâˆ— 1,...,u ğ‘˜âˆ—(cid:3)
Sâˆ—={ğ‘  ğ‘–âˆ— ğ‘—}, be found in Appendix B. Following derivation by [3, 33], Algo-
rithm1illustratesthekeystepstoimplementtheSOBalgorithm,
whereUâˆ— consistsofthefirstğ‘˜ rightsingularvectorsofAand
whereğœ‚representstheminimumchangetoterminatetheiterative
ğ‘  ğ‘–âˆ—
ğ‘—
=ağ‘–uâˆ— ğ‘—âŠ¤âˆ’ğœ†âˆ— ğ‘—ğµ ğ‘–.Detailedderivationsoftheclosed-formsolution
optimizationprocess.
(7)canbefoundinAppendixA.Theprocessednon-sensitivedata
Notethatwiththeadditionalregularizationconstraintsinthis
matrixisthereforeA(cid:101)=Sâˆ—Uâˆ—âŠ¤.
case,thesolutionfavorssparsitywhilesatisfyingtheorthogonal
Itisnoteworthythatwhenthecorrelationbetweenthesensitive
constraint.Therefore,SOBalsoachievesatcounterfactualfairness
variableandtheobserveddataisminimal,A(cid:101)yieldsareconstruction underSCMframeworkwithTheorem3.3.
errorsimilartothatofthestandardSingularValueDecomposition
(SVD).ItisbecausetheadditionalreconstructionerrorofA(cid:101)relative
4 EXPERIMENTS
toSVDwiththesamerankisproportionaltothecollinearitybe-
In this section, we present the results of our experiments con-
tweenthesubspacespannedbyBandtheleftsingularvectorsofthe
ducted on both synthetic and real data sets. In Section 4.2 and
dataA[3].TheexactexpressionforOBâ€™sadditionalreconstruction
4.3,wecomparetheproposedOBwithseveralexistingmethods:
errorcomparedtoSVDcanbefoundinAppendixA.
MachineLearning(ML),alogisticregressionbaselinewhichuses
SparseOrthogonaltoBias(SOB). Whenthenumberoffeaturesğ‘ allattributeswhethersensitiveornot;FairnessthroughUnaware-
exceedsthenumberofobservationsğ‘›,estimatingalow-dimensional ness(FTU),whichsimplyfitsalogisticmodelwithA,excluding
structurefromhigh-dimensionaldatacanbecomenumericallyun- thesensitivevariablesfromthemodel;EqualizedOdds(EO),apost-
stable[36].Toaddressthischallenge,weintroduceasparsevariant processingalgorithmchosentobalancefalsepositivesandfalse
oftheOBalgorithm,referredtoasSOB.TheSOBimposesanâ„“1-norm negativeswhileminimizingtheexpectedlossproposedby[30];
penaltyforUtoencouragesparsityandimprovenumericalstabil- FairLearningAlgorithm(FL),whichachievescounterfactualfair-
ity,inadditiontotheorthogonalityconstraintsin(5).Wedefine nessbysamplingunobservedpartsofthegraphicalcausalmodel
â„astheâ„“1constraintonğ‘¢.DetailsoftheformulationofSOBcan usingMarkovchainMonteCarlomethods[24];AffirmativeAction
5(AA),anpostprocessingalgorithmthatproducesfairequalizedodds
(EO)andaffirmativeaction(AA)predictorsbypositingacausal
modelandconsideringcounterfactualdecisions[30];FairLearning
throughDatapre-processing(FLAP)by[19].
Amongthecomparedmethods,FTUandFLAParepre-processing
methods,FLisanin-processingapproach,andAAandEOarepost-
processingapproaches.NotethatforbothFLAPandOB,thepredic-
torclassusedforpredictioncanincludesensitivevariables.Specifi-
cally,inadditiontothepredictorclassğ‘“ ğ‘ŒË†(ğ´):Aâ†’Ydiscussedin
Section3.2,foramachinelearningpredictorğ‘“ ğ‘ŒË†(ğ´,ğµ):AÃ—Bâ†’Y
thatutilizesbothsensitiveandnon-sensitivevariables,anAveraged
MachineLearning(AML)predictorğ‘“ ğ‘ŒË†â€²(ğ´)=âˆ« ğ‘“ ğ‘ŒË†(ğ´,ğµ)P(ğµ)ğ‘‘ğµor
ğ‘“ ğ‘ŒË†â€²(ğ´) = (cid:205)ğ‘“ ğ‘ŒË†(ğ´,ğµ)P(ğµ),dependingonwhetherğµiscontinuous
orbinary,canbeconstructedrespectively.Therefore,wedenote
OB1andOB2forscenariosinvolvingthetrainingofğ‘“ ğ‘ŒË†(ğ´,ğµ)orğ‘“
ğ‘ŒË†
withOB-processeddata,respectively.Similardesignationsareused
forFLAP1andFLAP2.Additionally,since[19]introducestwopre-
processingmethods,OrthogonalizationandMarginalDistribution
Mapping,wedenotethemasFLAP(O)andFLAP(M)respectively.
Logisticregressionpredictorsareutilizedforallmodels,denoted
asğ‘ŒË†.TheexperimentswereconductedinaJupyterNotebookenvi-
ronmentwith16GBRAM.
4.1 Evaluationmetrics
Weassesstheaccuracyofthedecisionsconcerningtheground
truthwithAreaUndertheCurve(AUC)andAccuracy(ACC).As
Figure 3: Synthetic loan data. The x-axis shows different
fortheevaluationofcounterfactualfairness,weadopttwometrics
levels of the effect of the sensitive variable on education
introducedby[19]:CF-metricsmeasurescounterfactualfairnessby
calculatingtheaveragechangeinpredictedscoresbetweengroups
level, ğ›½ ğ¸, as defined in (9). With 10 repeated experiments,
the lines represent the averaged results for each method,
withthemostsignificantdifference.Additionally,weincorporate
andthebarsrepresenttheirstandarddeviation.Theredline
CFBoundtoevaluatecounterfactualfairnessintheabsenceofnon-
representstheresultsoftheproposedmethodOB.
sensitiveconditions.Thismetriccomputesthemaximumabsolute
valueoftheboundsâ€™averageofpredictedscoresforasampleran-
domlyselectedfromtheset.Duetocomputationalchallenges,we theeducationyearandannualincomeforeachracegroupfollows
onlyevaluateCF-Boundforthesimulateddataset.Foracompre- thefollowingdistribution:
hensivecomparisonofthemethods,weadditionallyincorporate ğ¸=max{0,ğ‘ˆ ğ¸},
twoobservationalfairnessmetrics:EOFairness,asdefinedby[30], (8)
andAAFairness,proposedin[30].InTables1to3,wehighlight
ğ¼ =exp{0.1ğ‘ˆ ğ¸+ğ‘ˆ ğ¼}.
thebest-performingmethodinboldandunderlinethesecond-best Thebankâ€™sdecisionissimulatedusingalogisticmodel:
foreachmetricused. ğ‘Œ =1{ğ‘ˆ
ğ‘Œ
<expit(ğ›½0+ğ›½11{ğµ=1}+ğ›½21{ğµ=2}+ğ›½ ğ¸ğ¸+ğ›½ ğ¼ğ¼)},
(9)
4.2 Syntheticdata whereğ‘ˆ
ğ‘Œ
âˆ¼Uniform(0,1)andexpit(ğ‘¢)=(1+ğ‘’âˆ’ğ‘¢)âˆ’1.
Wefirstapplyourmethodstoasyntheticloandatasetexample Inthisexample,theparametersğœ† andğœ† determinetheextent
ğ¸1 ğ¸2
which is a modification from [19]. Using synthetic data allows ofthemeandifferenceineducationyearsacrossthethreerace
ustorepeattherandomdatagenerationprocessandprovidethe groups,whiletheparametersğœ† andğœ† dictatethemagnitudeof
ğ¼1 ğ¼2
average results. It also enables us to observe how fair learning themeandifferenceinlogincomeamongthesethreeracegroups.
modelsrespondtochangingeffectsresultingfromdifferentlevels ğ›½1andğ›½2characterizethedirecteffectoftheraceinformationon
ofunfairtreatmentamongdifferentgroups.Thepresentedexample theloanapprovalrate.
illustratesascenarioinwhichabankevaluatesloanapplications Itisimportanttonotethatthesensitivevariableğµiscategori-
basedontheapplicantâ€™seducationlevel(ğ¸)andannualincome(ğ¼), cal,andthedatageneratingprocessdoesnotexactlyconformto
determiningapproval(ğ‘Œ =1)orrejection(ğ‘Œ =0).Thepopulation Assumption3.2.AsevidencedinTable1andFigure3,despitethe
comprisesthreepossibleracegroups:ğµ={0,1,2}.Similarto[19], deviationfromtheassumptionsinthetestedsyntheticdataset,
wegenerateğµ accordingtoğµ = 1{ğ‘ˆ ğµ < 0.76}+1{ğ‘ˆ ğµ > 0.92}, ourmethodconsistentlyshowcasescomparativelyhighAUCand
whereğ‘ˆ
ğµ
âˆ¼Uniform(0,1).Letğ‘ˆ
ğ¸
andğ‘ˆ
ğ¼
betwostandardnormal ACCcomparedtomostmethods.Notably,itsaccuracyoutperforms
randomvariableswithmeanğœ‡
ğ¸
=ğœ† ğ¸0+1{ğµ=1}ğœ† ğ¸1+1{ğµ=2}ğœ†
ğ¸2
FLandFLAP,twoothercounterfactuallyfairmethods.Moreover,
andğœ‡
ğ¼
=log(ğœ† ğ´0+1{ğµ=1}ğœ† ğ´1+1{ğµ=2}ğœ† ğ´2),respectively.Then ourmethodachieveslowCF-metricandCFBound,akintoFLand
6Table1:Performancecomparisonofourmethodandotherexistingmethodswithsyntheticloandata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) OB1 OB2
ACC 0.6618 0.6481 0.6224 0.6237 0.6224 0.6237 0.6224 0.6237 0.6224 0.6406 0.6279
AUC 0.9457 0.8986 0.5867 0.6682 0.5714 0.5668 0.5837 0.5875 0.5863 0.5704 0.5856
CF-metrics 0.6291 0.3906 0.0031 0.0355 0.0034 0.0016 0.0032 0.0002 0.0002 0.0011 0.0026
CFBound 0.8690 0.9464 0.1836 0.1071 0.0918 0.0937 0.1847 0.0690 0.0670 0.0830 0.2340
EOFairness 0.5469 0 0.0156 0 0.0336 0.0321 0.0156 0.0301 0.0180 0 0
AAFairness 0.6235 0.4559 5.6e-18 0.0370 1.1e-18 3.3e-18 6.7e-18 0.0012 0.0038 4.6e-17 4.3e-17
Table2:PerformancecomparisonofourmethodandotherexistingmethodsonAdultdata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) SOB1 SOB2
ACC 0.7612 0.7604 0.7594 0.7680 0.7644 0.7357 0.7151 0.7548 0.7594 0.7655 0.7597
AUC 0.8128 0.8036 0.7680 0.7991 0.7682 0.7682 0.7680 0.7651 0.7649 0.7806 0.7809
CF-metric 0.2779 0.2338 0.0228 0.2047 0.0268 0.0280 0.0228 0.0280 0.0228 0.0529 0.0600
EOFairness 0.1536 0 0.2853 0 0.2811 0.2780 0.2853 0.2780 0.2853 0.0002 0.0005
AAFairness 0.3034 0.2574 0 0.2259 0 2.2e-17 2.2e-17 2.8e-17 2.8e-17 0.0001 0.0004
FLAP,indicatingahighdegreeofcounterfactualfairness.Thisde- COMPAS. TheCOMPASrecidivismdataincludesdemographic
sirablecharacteristiccanbeattributedtothepropertyofOB,which informationsuchassex,age,race,andrecorddata(priorcounts,
minimallymodifiesinthenon-sensitivedatawhileensuringcoun- juvenilefeloniescounts,andjuvenilemisdemeanorscounts)for
terfactualfairness.Furthermore,intermsofobservationalfairness over10,000criminaldefendantsinBrowardCounty,Florida.The
metrics,OBexhibitsanoverallbetterperformancecomparedtoFL goalistopredictwhethertheywillre-offendinthenexttwoyears.
andFLAPwithlowerEOandAAFairnessmetrics. AsdepictedinTable3,similartoourmethodâ€™sperformancein
thetwopreviousdatasets,theaccuracywithOBiscomparatively
4.3 Realdata high.Moreover,itsCF-metricissimilarcomparedtothatof FLAP
andFL,implyingcounterfactualfairnessattributedtoOB.Addition-
We also apply our methods to two real data sets: the Adult In-
ally,itachievesbothrelativelylowEOandAAFairnessmetrics
comedatasetfromtheUCIMachineLearningRepository1andthe
comparedtoFLAPandFL,suggestingbetterobservationalfairness.
COMPASrecidivismdatafromProPublica[5].
Adult. IntheAdultIncomedataset,weaimtopredictwhether Insummary,acrossthreedatasets,ourapproachconsistentlyex-
anindividualâ€™sincomeexceeds$50,000,consideringfeaturessuch hibits its effectiveness in maintaining an overall better balance
assex,race,age,workclass,education,occupation,maritalstatus, betweenaccuracy,observationalfairness,andcounterfactualfair-
capitalgain,andloss.Thesensitiveattributesaresexandrace.The ness.Additionally,weobservedthatusingSOBforthesynthetic
trainingsetconsistsof32,561samples,andthetestsetcomprises loandataandCOMPASresultsinslightlylowerACCandAUC,
16,281samples. whileachievingsimilarfairnessmetrics.
DuetothelargesamplesizeofAdultdataset,weemploySOB.As
illustratedinTable2,similartoOBâ€™sperformanceinsyntheticdata,
theaccuracyiscomparativelyhighcomparedtoallothertestedfair 4.4 CaseStudywithContinuousDecision
learningapproaches.Notably,theaccuracyisevenhigherthanthe
Variables
vanillaMLmodel,whichutilizesbothsensitiveandnon-sensitive
Toshowcasetheempiricalefficacyof OBindecorrelatingAand
attributesandgeneratesunfairresults.Asnotedby[3,8],theaddi-
Bandachievingcounterfactuallyfairpredictions,wepresentan
tionalregulationwithSOBmaycontributetohighout-of-sample
additionalcasestudyoftwosyntheticdatasetsfeaturingcontinuous
predictionperformances.Moreover,itsCF-metriciscomparableto
decisionvariableğ‘Œ.WeapplyFTU,FL,andOBtothetwosimulated
thatof FLAPandFLandismuchlowerthanbaselines,implying
datasets.Weevaluatetheaccuracyofğ‘ŒË† generatedbydifferent
counterfactualfairnessattributedtoOB.Additionally,itachieves
methodsusingRootMeanSquareError(RMSE)comparedtoground
bothlowEOandAAFairnessmetrics.
truthğ‘Œ.Weexaminethecounterfactualfairnessbyexaminingthe
KL-divergencebetweenpredictionsusingtheobserved(actual)data
1https://archive.ics.uci.edu/dataset/2/adult andcounterfactualdatawithadifferentsensitivevariablefollowing
7Table3:PerformancecomparisonofourmethodandotherexistingmethodsonCOMPASdata
Baselines ComparedMethods Ours
Metrics
ML FTU FL EO AA FLAP1(O) FLAP2(O) FLAP1(M) FLAP2(M) OB1 OB2
ACC 0.5744 0.5726 0.5598 0.5710 0.5609 0.5605 0.5599 0.5607 0.5607 0.5666 0.5674
AUC 0.7206 0.7225 0.6928 0.7225 0.6927 0.6927 0.6928 0.7015 0.7019 0.6764 0.6744
CF-metric 0.2274 0.1406 0.0054 0.1377 0.0060 0.0058 0.0054 0.0026 0.0027 0.0060 0.0065
EOFairness 0.1046 0 0.1374 0 0.1405 1.7e-06 3.3e-06 6.7e-07 1.2e-06 0 0
AAFairness 0.2258 0.1460 0 0.1424 0 2.9e-07 5.6e-07 8.2e-07 3.0e-07 1.6e-16 1.1e-16
Table4:Performancecomparisononsyntheticdatasetswith
continuousdecisionvariable !# !$
GPA
Syn(Cont.Y) LSAT
Metrics "# â€¦ "% â€¦ "& $%&' LSAT ()*+.
FTU FL OB FTU FL OB
# FYA
RMSE 0.50 0.51 0.53 0.86 0.89 0.18
KLDiv. 0.35 0.00 0.00 1.42 0.28 0.01 (a)SyntheticData (b)LSAT
ğ¶ğ‘œğ‘Ÿğ‘Ÿ(A(cid:101),B)âˆ— 0.52 0.52 0.00 0.30 0.30 0.00
Figure4:(a)representsthecausalmodelforasyntheticdata
*TheaveragepairwisecorrelationbetweenA(cid:101)andB. setusedinSection4.4,withğµbeingthebinarysensitivevari-
ableandğ‘Œ beingacontinuousoutcomeofinterest,along
withtheintermediatevariableğ´thatisinfluencedbyğµ.(b)
[30].Weadditionallyincludetheaveragepair-wisecorrelations representsthecausalmodelforLSAT,asemi-syntheticex-
betweenA(cid:101)andBtoverifytheiruncorrelation. ampleusedby[24].Weextractalatentvariable,studentâ€™s
Whilethedetailedsetupofthetwosyntheticcasesisprovided knowledge(K),andassumesuchavariableaffectsGPA,LSAT,
inAppendixC,wepresentabriefoverviewhere.Causalmodels andFYAscorestoapplytheFLmethodinthiscase.
for the two synthetic cases with continuous decision variables
areoutlinedinFigure4.Thefirstsyntheticdataset,Syn(Cont.Y),
featuresacontinuousğ‘Œ followinganormaldistributionwithits Observed Observed
Counterfactual Counterfactual
meanlinearlydependentonnon-sensitivevariablesğ´,whichin
turnareaffectedbytwocategoricalsensitivevariables.Theother
syntheticdataset,LSAT,isderivedfromasurveyconductedby
theLawSchoolAdmissionCouncilacross163lawschoolsinthe
United States [32]. Here, the decision variableğ‘Œ represents the
first-yearaveragegrade(FYA),whilethesensitivevariableisthe
KL Div. = 1.4238 KL Div. = 0.0145
studentsâ€™race.TheresultssummarizedinTable4indicatethatOB
Y Prediction Y Prediction
effectivelydecorrelatesAandBinbothcases.Itachievesasimilar
(a)FTU (b)OB
RootMeanSquareError(RMSE)insyntheticdataandlowerRMSE
in the LSAT dataset compared to FTU and FL. Additionally, the Figure5:Theredlinerepresentstheğ‘ŒË†
distributionutilizing
predicteddistributionsofFYAunderobservedandcounterfactual
theobservedvariables,whilethebluelinerepresentsthe
sensitivevariablesfortheLSATcasearenearlyidenticalforOB,
predictiondistributionutilizingthecounterfactualdata.(a)
asevidencedbytheKL-divergencemeasureinTable4andhighly
shows the distributions
ofğ‘ŒË†
using FTU method, while (b)
overlappedcurvesinFigure5.Thus,thepredictionsofOBalignwith presentsthedistributionsofğ‘ŒË†
usingOBprocesseddata.
thedefinitionofcounterfactualfairnessoutlinedinDefinition3.1.
5 CONCLUSION resultingdatapre-processingalgorithmeffectivelyremovesbias
Inconclusion,thispaperdemonstratesthatachievingcounterfac- inpredictionswhilemakingminimalchangestotheoriginaldata.
tualfairnessisfeasiblebyensuringtheuncorrelationbetweennon- Importantly,OBismodel-agnostic,ensuringitsadaptabilitytoa
sensitiveandsensitivevariablesundercertainconditions.Building varietyofmachinelearningmodels.Throughcomprehensiveeval-
onthisinsight,wepresenttheOrthogonaltoBias(OB)algorithm, uationsonsimulatedandreal-worlddatasets,wedemonstratethat
a novel approach to addressing fairness challenges in machine OBstrikesagreatbalancebetweenfairnessandaccuracy,outper-
learningmodels.OBachievescounterfactualfairnessbydecorre- formingcomparedmethodsandofferingapromisingsolutionto
lating data from sensitive variables under mild conditions. The thecomplexissueofbiasinmachinelearning.
8
ytisneD ytisneDREFERENCES [21] FaisalKamiranandToonCalders.2012. Datapreprocessingtechniquesfor
[1] AlekhAgarwal,AlinaBeygelzimer,MiroslavDudÃ­k,JohnLangford,andHanna classificationwithoutdiscrimination.Knowledgeandinformationsystems33,1
Wallach.2018. Areductionsapproachtofairclassification.InInternational (2012),1â€“33.
conferenceonmachinelearning.PMLR,60â€“69. [22] AriaKhademi,SanghackLee,DavidFoley,andVasantHonavar.2019.Fairness
[2] SinaAghaei,MohammadJavadAzizi,andPhebeVayanos.2019. Learning inalgorithmicdecisionmaking:Anexcursionthroughthelensofcausality.In
OptimalandFairDecisionTreesforNon-DiscriminativeDecision-Making.In TheWorldWideWebConference.AssociationforComputingMachinery,New
York;NY;UnitedStates,2907â€“2914.
ProceedingsoftheThirty-ThirdAAAIConferenceonArtificialIntelligenceand
[23] NikiKilbertus,ManuelGomezRodriguez,BernhardSchÃ¶lkopf,KrikamolMuan-
Thirty-FirstInnovativeApplicationsofArtificialIntelligenceConferenceandNinth
AAAISymposiumonEducationalAdvancesinArtificialIntelligence(Honolulu, det,andIsabelValera.2020. FairDecisionsDespiteImperfectPredictions.In
Hawaii,USA)(AAAIâ€™19/IAAIâ€™19/EAAIâ€™19).AAAIPress,Article175,9pages. ProceedingsoftheTwentyThirdInternationalConferenceonArtificialIntelligence
https://doi.org/10.1609/aaai.v33i01.33011418 andStatistics(ProceedingsofMachineLearningResearch,Vol.108),SilviaChiappa
[3] EmanueleAliverti,KristianLum,JamesEJohndrow,andDavidBDunson.2021. andRobertoCalandra(Eds.).PMLR,277â€“287. https://proceedings.mlr.press/
Removingtheinfluenceofgroupvariablesinhigh-dimensionalpredictivemod- v108/kilbertus20a.html
elling.JournaloftheRoyalStatisticalSociety.SeriesA,(StatisticsinSociety)184,3 [24] MattJKusner,JoshuaLoftus,ChrisRussell,andRicardoSilva.2017. Coun-
(2021),791. terfactual Fairness. In Advances in Neural Information Processing Systems,
[4] McKaneAndrus,ElenaSpitzer,JeffreyBrown,andAliceXiang.2021. What I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
WeCanâ€™tMeasure,WeCanâ€™tUnderstand:ChallengestoDemographicData wanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc., Long
ProcurementinthePursuitofFairness.InProceedingsofthe2021ACMConference Beach,CA,USA. https://proceedings.neurips.cc/paper_files/paper/2017/file/
onFairness,Accountability,andTransparency(VirtualEvent,Canada)(FAccTâ€™21). a486cd07e4ac3d270571622f4f316ec5-Paper.pdf
AssociationforComputingMachinery,NewYork,NY,USA,249â€“260. https: [25] BinhThanhLuong,SalvatoreRuggieri,andFrancoTurini.2011. k-NNasan
//doi.org/10.1145/3442188.3445888 implementationofsituationtestingfordiscriminationdiscoveryandprevention.
[5] JuliaAngwinandJeffLarson.2023.MachineBias. https://www.propublica.org/ InProceedingsofthe17thACMSIGKDDinternationalconferenceonKnowledge
article/machine-bias-risk-assessments-in-criminal-sentencing discoveryanddatamining.502â€“510.
[6] SinaBaharlouei,MaherNouiehed,AhmadBeirami,andMeisamRazaviyayn. [26] AmitabhaMukerjee,RitaBiswas,KalyanmoyDeb,andAmritPMathur.2002.
2019.R\â€™enyiFairInference.arXivpreprintarXiv:1906.12005(2019). Multiâ€“objectiveevolutionaryalgorithmsfortheriskâ€“returntradeâ€“offinbank
[7] RichardA.Berk,HodaHeidari,ShahinJabbari,MatthewJoseph,MichaelKearns, loanmanagement.InternationalTransactionsinoperationalresearch9,5(2002),
JamieMorgenstern,SethNeel,andAaronRoth.2017.AConvexFrameworkfor 583â€“597.
FairRegression.ArXivabs/1706.02409(2017). https://api.semanticscholar.org/ [27] CarlM.Oâ€™Brien.2016.StatisticalLearningwithSparsity:TheLassoandGeneral-
CorpusID:12641090 izations.InternationalStatisticalReview84,1(2016),156â€“157. https://doi.org/10.
[8] ChristopherM.Bishop.2006.PatternRecognitionandMachineLearning(Infor- 1111/insr.12167arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12167
mationScienceandStatistics).Springer-Verlag,Berlin,Heidelberg. [28] PranitaPatilandKevinPurcell.2022.Decorrelation-BasedDeepLearningfor
[9] TolgaBolukbasi,Kai-WeiChang,JamesYZou,VenkateshSaligrama,andAdamT BiasMitigation.FutureInternet14,4(2022). https://doi.org/10.3390/fi14040110
Kalai.2016. Manistocomputerprogrammeraswomanistohomemaker? [29] JudeaPearl.2009.Causality(2ed.).CambridgeUniversityPress. https://doi.
debiasingwordembeddings.Advancesinneuralinformationprocessingsystems org/10.1017/CBO9780511803161
29(2016). [30] YixinWang,DhanyaSridhar,andDavidMBlei.2019.Equalopportunityand
[10] ToonCalders,FaisalKamiran,andMykolaPechenizkiy.2009.Buildingclassifiers affirmativeactionviacounterfactualpredictions.arXivpreprintarXiv:1905.10870
withindependencyconstraints.In2009IEEEinternationalconferenceondata (2019).
miningworkshops.IEEE,13â€“18. [31] SongWei,XiangruiKong,AlinsonSantosXavier,ShixiangZhu,YaoXie,andFeng
[11] SilviaChiappa.2019.Path-specificcounterfactualfairness.InProceedingsofthe Qiu.2024.AssessingElectricityServiceUnfairnesswithTransferCounterfactual
AAAIconferenceonartificialintelligence,Vol.33.7801â€“7808. Learning.arXivpreprintarXiv:2310.03258(2024).
[12] LeeCohen,ZacharyC.Lipton,andYishayMansour.2020.EfficientCandidate [32] LindaF.Wightman.1998.LSACNationalLongitudinalBarPassageStudy.LSAC
ScreeningUnderMultipleTestsandImplicationsforFairness.In1stSymposium ResearchReportSeries. https://api.semanticscholar.org/CorpusID:151073942
[33] DanielaMWitten,RobertTibshirani,andTrevorHastie.2009. Apenalized
onFoundationsofResponsibleComputing,FORC2020,June1-3,2020,Harvard
University,Cambridge,MA,USA(virtualconference)(LIPIcs,Vol.156),AaronRoth matrixdecomposition,withapplicationstosparseprincipalcomponentsand
(Ed.).SchlossDagstuhl-Leibniz-ZentrumfÃ¼rInformatik,Dagstuhl,Germany, canonicalcorrelationanalysis.Biostatistics10,3(2009),515â€“534.
1:1â€“1:20. https://doi.org/10.4230/LIPIcs.FORC.2020.1 [34] MuhammadBilalZafar,IsabelValera,ManuelGomezRogriguez,andKrishnaP.
[13] ElliotCreager,DavidMadras,JÃ¶rn-HenrikJacobsen,MarissaWeis,KevinSwer- Gummadi.2017. FairnessConstraints:MechanismsforFairClassification.In
sky,ToniannPitassi,andRichardZemel.2019. Flexiblyfairrepresentation Proceedingsofthe20thInternationalConferenceonArtificialIntelligenceand
learningbydisentanglement.InInternationalconferenceonmachinelearning. Statistics(ProceedingsofMachineLearningResearch,Vol.54),AartiSinghand
PMLR,1436â€“1445. JerryZhu(Eds.).PMLR,Sydney,Australia,962â€“970. https://proceedings.mlr.
[14] Briandâ€™Alessandro,CathyOâ€™Neil,andTomLaGatta.2017.Conscientiousclassi- press/v54/zafar17a.html
fication:Adatascientistâ€™sguidetodiscrimination-awareclassification.Bigdata [35] RichZemel,YuWu,KevinSwersky,ToniPitassi,andCynthiaDwork.2013.
5,2(2017),120â€“134. LearningFairRepresentations.InProceedingsofthe30thInternationalConference
[15] CynthiaDwork,MoritzHardt,ToniannPitassi,OmerReingold,andRichard onMachineLearning(ProceedingsofMachineLearningResearch,Vol.28),Sanjoy
Zemel.2012.FairnessthroughAwareness.InProceedingsofthe3rdInnovations DasguptaandDavidMcAllester(Eds.).PMLR,Atlanta,Georgia,USA,325â€“333.
inTheoreticalComputerScienceConference(Cambridge,Massachusetts)(ITCS https://proceedings.mlr.press/v28/zemel13.html
â€™12).AssociationforComputingMachinery,NewYork,NY,USA,214â€“226. https: [36] HuiZou,TrevorHastie,andRobertTibshirani.2006.Sparseprincipalcomponent
//doi.org/10.1145/2090236.2090255 analysis.Journalofcomputationalandgraphicalstatistics15,2(2006),265â€“286.
[16] QizhangFeng,MengnanDu,NaZou,andXiaHu.2022.Fairmachinelearning
inhealthcare:Areview.arXivpreprintarXiv:2206.14397(2022).
[17] BenjaminFish,JeremyKun,andÃdÃ¡mDLelkes.2016. Aconfidence-based
approachforbalancingfairnessandaccuracy.InProceedingsofthe2016SIAM
internationalconferenceondatamining.SIAM,SIAM,Miami,Florida,USA,144â€“
152.
[18] NinaGrgic-Hlaca,MuhammadBilalZafar,KrishnaPGummadi,andAdrian
Weller.2016.Thecaseforprocessfairnessinlearning:Featureselectionforfair
decisionmaking.InNIPSsymposiumonmachinelearningandthelaw,Vol.1.
Barcelona,Spain,CurranAssociates,Inc.,Barcelona,Spain,11.
[19] RuiSongHaoyuChen,WenbinLuandPulakGhosh.2023. OnLearning
andTestingofCounterfactualFairnessthroughDataPreprocessing. J.Amer.
Statist.Assoc.0,0(2023),1â€“11. https://doi.org/10.1080/01621459.2023.2186885
arXiv:https://doi.org/10.1080/01621459.2023.2186885
[20] MoritzHardt,EricPrice,andNathanSrebro.2016.EqualityofOpportunityin
SupervisedLearning.InProceedingsofthe30thInternationalConferenceonNeural
InformationProcessingSystems(Barcelona,Spain)(NIPSâ€™16).CurranAssociates
Inc.,RedHook,NY,USA,3323â€“3331.
9A CLOSED-FORMOBSOLUTIONDERIVATION referto[3].Forexample,asnotedby[3],anintuitiveinterpreta-
Westartbyconsidering(6)whenğ‘˜ =1.TheOBalgorithmaims tionofthesolutionin(14)isthattheoptimalscoresforthe ğ‘—-th
to find the closest rank-1 matrix (vector) approximation to the dimensionareobtainedbyprojectingtheoriginaldataoverthe
originalsetofdatathatsatisfiestheorthogonalcondition.(6)can ğ‘—-thbasisandthensubtracting ğ‘—-timestheobservedvalueofğ‘.
bereformulatedas: Moreover,astheconstraintsofOBdonotinvolveanyvectorğ‘¢ ğ‘—,the
optimizationwithrespecttothebasiscanbederivedfromknown
(cid:13) (cid:13)2
argm ğ‘†,ğ‘ˆinï£±ï£´ï£´ï£² ï£´ï£´ğ‘›1âˆ‘ï¸ ğ‘–=ğ‘› 1(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)ğ‘ ğ‘– âˆ’âˆ‘ï¸ ğ‘—ğ‘˜ =1ğ‘  ğ‘–ğ‘—ğ‘¢ğ‘‡ ğ‘—(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
+ ğ‘›2 ğœ†1âˆ‘ï¸ ğ‘–=ğ‘› 1ğ‘  ğ‘–1ğ‘ ğ‘–ï£¼ï£´ï£´ï£½ ï£´ï£´. (10) r ağ‘—e c=s cu o1l rt , ds .. i.i n.n , gğ‘˜ ll yi ,n i tse oa er tq ha u el ag l ae stb o sr oa t ch. ieT ath fi ee drso stp ik nti grm uiga lh al rtv vsa i al nu lg ue u ef slo a [r r 8t v ,h a 2e l 7uv ]e .e sc oto fr ğ´ğ‘¢ ,ğ‘— s, ow rti eth d
ï£³ ï£¾
Somealgebraandtheorthonormalconditiononğ‘¢1allowusto WenotethefollowingusefulLemmaadaptedfrom[3]thatquan-
expressthelossfunctiontobeminimizedas: tifiestheadditionalreconstructionerrorofğ´duetousingOBcom-
ğ¿(ğ‘ 1,ğ‘¢1)=
ğ‘›1âˆ‘ï¸ğ‘›
(ğ‘ ğ‘– âˆ’ğ‘  ğ‘–1ğ‘¢ğ‘‡ 1)ğ‘‡ (ğ‘ ğ‘– âˆ’ğ‘  ğ‘–1ğ‘¢ğ‘‡ 1)+
ğ‘›2
ğœ†1âˆ‘ï¸ğ‘›
ğ‘  ğ‘–1ğ‘ ğ‘–
paredtoSVDis:
=
ğ‘›1âˆ‘ï¸ğ‘– ğ‘–= =ğ‘›1
1(ğ‘ğ‘‡ ğ‘–ğ‘ ğ‘– âˆ’2ğ‘  ğ‘–1ğ‘ ğ‘–ğ‘¢ğ‘‡ 1 +ğ‘  ğ‘–2 1)+ ğ‘›2 ğœ†1âˆ‘ï¸ ğ‘–=ğ‘›
1ğ‘–=
ğ‘ 
ğ‘–1
1ğ‘ ğ‘–.
L
o
[ğ‘ƒfe ]m
t ğ‘–h
ğ‘—m
e
=maA
a
ğ‘›1t. r1 +i.
x
(cid:205)L
A ğ‘
ğ‘›e ğ‘–t ğ‘oA
ğ‘—
ğ‘Ë†
b
2t=
.a
Tiğ‘‰
n
hğ‘˜
e
eğ·
d
ağ‘˜
f
dğ‘ˆ
r
doğ‘˜ğ‘‡
m
itid oe
t
nhn aeo lte
t rr
et uh
cn
oe
c
nb
a
se
t
ts
e
rt udr ca
S
tn
iV
ok
D
n-k eoa rfp rop
r
rr ao
n
ox
k
fim
tğ‘˜
ha
.
et Li Oo en
Bt
ğ‘–=1 ğ‘–
Thefunctionisquadratic,anditspartialderivativewithrespectto algorithmcomparedtoSVDisâˆ¥ğ‘˜ğ‘ƒğ‘‰ ğ‘˜ğ· ğ‘˜âˆ¥F.
ğ‘  ğ‘–1is
ğœ• 1 2
ğœ•ğ‘ 
ğ‘–1ğ¿(ğ‘ 1,ğ‘¢1)= ğ‘›(âˆ’2ğ‘ ğ‘–ğ‘¢ğ‘‡
1
+2ğ‘  ğ‘–1)+ ğ‘›ğœ†1ğ‘ ğ‘–. B FORMULATIONOFSOB
Solvingitfindsastationarypointof
ToenhancetheapplicabilityoftheOBalgorithm,particularlyinsce-
narioswithalargenumberoffeatures,weincorporateanâ„“1-norm
ğ‘  ğ‘–1=ğ‘ ğ‘–ğ‘¢ğ‘‡ 1 âˆ’ğœ†1ğ‘ ğ‘–. (11) penaltyforthematrixğ‘ˆ.Thisadditionaimstopromotesparsityin
Sotheoptimalscorefortheğ‘–-thsubjectisobtainedbyprojecting ğ‘ˆ andenhancethenumericalstabilityoftheapproximation.The
theobserveddataontothefirstbasisandthensubtractingğœ†1ğ‘.The modifiedalgorithm,denotedasSOB,isformulatedasfollows:
constraintdoesnotinvolvetheorthonormalbasisğ‘¢1,hencethe
solutionof (10)forğ‘¢1isequivalenttotheunconstrainedscenario.
Astandardresultoflinearalgebrastatesthattheoptimalğ‘¢1 for argmin(cid:13) (cid:13)ğ´âˆ’ğ‘†ğ‘ˆğ‘‡(cid:13) (cid:13)2
(10)withoutconstraintsequivalenttothefirstrightsingularvector ğ‘†,ğ‘ˆ (cid:13) (cid:13)ğ¹ (16)
o [3f 3ğ´ ]., Po lr uge gq iu ni gva inle tn ht ely sot lo utt ih oe nfi for rst ğ‘¢e 1i ag ne dnv se ec ttt io nr go thf eth de erm iva at tr ivix eğ´ wğ‘‡ itğ´
h
subjectto (cid:13) (cid:13)ğ‘¢ ğ‘—(cid:13)
(cid:13)2
â‰¤1,(cid:13) (cid:13)ğ‘¢ ğ‘—(cid:13)
(cid:13)1
â‰¤ğ‘¡,(cid:13) (cid:13)ğ‘  ğ‘—(cid:13)
(cid:13)2
â‰¤1,ğ‘ ğ‘‡ ğ‘—ğ‘ 
ğ‘™
=0,ğ‘ ğ‘‡ ğ‘—ğµ=0,
respecttoğœ†1equalto0leadsto
for ğ‘— = 1,...,ğ‘˜,andğ‘™ â‰  ğ‘—.Thedetailediterativeapproachto
ğ‘›
âˆ‘ï¸ (ğ‘ ğ‘–ğ‘¢ğ‘‡
1
âˆ’ğœ†1ğ‘ ğ‘–)ğ‘‡ğ‘
ğ‘–
=0. (12) s isol tv hi an tg at lh ti hs op ur go hble thm ei mso inu it mlin ize ad ta ion nd pex rop bla li en med isin n[ o3 t]. joT ih ne tlm ya ci on ni vd ee xa
ğ‘–=1
inğ‘  andğ‘¢, it can be addressed iteratively. Whenğ‘  is fixed, the
Therefore,
minimizationstepisequivalenttoasparsematrixdecomposition
ğœ†1=
(cid:205)ğ‘›
ğ‘– (cid:205)=1
ğ‘›
ğ‘–=ğ‘ 1ğ‘–ğ‘¢ ğ‘ğ‘‡
1
ğ‘–2ğ‘
ğ‘– =
âŸ¨ğ´ âŸ¨ğ‘ğ‘¢ ,ğ‘‡
1
ğ‘, âŸ©ğ‘âŸ©
, (13) w hai nth d,c won hs et nra ğ‘¢in ists fio xn edt ,h te heri sg oh lt us tii on ngu fola rr ğ‘ v ie sc ot bo tr as ino ef dğ´ b. yO rn eat rh re ano gth ine gr
whichstatesğœ†isisaleastsquaresestimateofğ´ğ‘¢ğ‘‡ overğ‘. theconstraintsandsolvingaunivariateoptimizationproblem.This
1
Nowconsiderthemoregeneralcasewhenğ‘˜ >1.Thederivatives iterativeprocessensuresorthogonalityamongthevectorsğ‘  ğ‘—.
with respect to the generic elementğ‘  can be calculated easily
ğ‘–ğ‘—
duetotheconstraintonğ‘ˆ,whichsimplifiesthecomputation.The C ADDITIONALEXPERIMENTRESULTS
optimalsolutionforthegenericscoreğ‘  isgivenby
ğ‘–ğ‘— Weincludeanadditionalexperimentswithcontinuousğ‘Œ todemon-
ğ‘ 
ğ‘–ğ‘—
=ğ‘ ğ‘–ğ‘¢ğ‘‡
ğ‘—
âˆ’ğœ† ğ‘—ğ‘ ğ‘–, (14) stratesomeadditionalpropertiesof OB.
sinceğ‘¢ğ‘‡ ğ‘–ğ‘¢ ğ‘— =0forallğ‘– â‰  ğ‘— andğ‘¢ğ‘‡ ğ‘—ğ‘¢ ğ‘— =1forğ‘— =1,....,ğ‘˜.
C.1 Evaluationmetrics
Theglobalsolutionforğœ†=(ğœ†1,...ğœ† ğ‘˜)canbederivedfromleast
squaresprojectionsincewecaninterpret(14)asamultivariatelin- Inthissection,weevaluatethemodelperformanceandfairness
earregressionwheretheğ‘˜columnsoftheprojectedmatrixğ´ğ‘ˆğ‘‡ are usingthreekeymetrics:RootMeanSquareError(RMSE)andthe
responsevariablesandğ‘acovariant.Therefore,theoptimalvalue KL-divergencebetweenobserved(actual)datapredictionsandcoun-
forgeneralğ‘˜isthenequaltothemultipleleastsquaressolution terfactualdatapredictions.WealsoincludeVariableCorrelation
âŸ¨ğ´ğ‘¢ğ‘‡,ğ‘âŸ©
andFrobeniusnormofA(cid:101)âˆ’ğ´tovalidatetheeffectof OB.
ğœ† ğ‘˜ = âŸ¨ğ‘,ğ‘˜ ğ‘âŸ© . (15) RMSE. RMSEisawidelyadoptedmetricforevaluatingprediction
Thisresultsintheclosed-formsolutionin(7).Foramorecom- performance.Iteffectivelyquantifiestheoverallaccuracyofour
pleteproofanddiscussionoftheimplicationsofthesolution,we modelâ€™spredictions.
10KL-Divergence. WeuseKL-divergencetomeasurethedistance OCobusenrtveerfdactual OCobusenrtveerfdactual OCobusenrtveerfdactual
betweenthedistributionofobserveddatapredictionsandcounter-
factualdatapredictions.Additionally,visualizationofthesedistri-
butionsservesasanintuitiveindicatorofcounterfactualfairness.
Ideally,ifourmodelsatisfiescounterfactualfairness,thesedistri-
KL Div. = 1.4238 KL Div. = 0.2843 KL Div. = 0.0145
butionsshouldperfectlyoverlap,resultinginaKL-divergenceof Y Prediction Y Prediction Y Prediction
0. (a)FTU (b)FL (c)OB
VariableCorrelations. Wecalculatetheaveragepairwisecorrela-
Figure6:Theredlinerepresentstheğ‘ŒË†
distributionutilizing
tionsbetweenvariablesğ´&ğµoverthegivenvariables.Correlation theobservedvariables,whilethebluelinerepresentsthe
closetozeroindicatesthatourframeworksuccessfullymitigates predictiondistributionutilizingthecounterfactualdata.(a)
theimpactofğµ.
and(b)showsthedistributionsofğ‘ŒË†
usingFTUandFLmethods
respectively,while(c)presentsthedistributionsofğ‘ŒË†
using
OBprocesseddata.
C.2 Datadescription
Weconductadditionalexperimentsusingsyntheticdatasetsand
distributions:
areal-worlddataset.Here,weprovideanoverviewofthesedata
sets: ğºğ‘ƒğ´âˆ¼N(ğ‘ ğ‘”+ğ‘¤ ğºğ¾ğ¾+ğ‘¤ ğºğ‘…ğ‘…,ğœ ğº),
ğ¹ğ‘Œğ´âˆ¼N(ğ‘¤ğ¾ğ¾+ğ‘¤ğ‘…ğ‘…,1),
Syntheticdatasets. Thecasualgraphusedtogeneratethesyn- ğ¹ ğ¹
theticdataisshowninFigure4a.Itiscraftedtosimulatehigh- ğ¿ğ‘†ğ´ğ‘‡ âˆ¼Poisson(ğ‘’ğ‘¥ğ‘(ğ‘ ğ¿+ğ‘¤ ğ¿ğ¾ğ¾+ğ‘¤ ğ¿ğ‘…ğ‘…)),
dimensionaldata,incorporatingalargernumberofvariables(ğ‘›= ğ¾ âˆ¼N(0,1).
10,000,ğ‘ =3,ğ‘ =40,withadditional8featuresthatareindepen-
Weperforminferenceonthismodelusinganobservedtrainingset
dentofsensitivevariables).Itconsistof10,000samples,ensuringa
toestimatetheposteriordistributionofğ¾.Weusetheprobabilistic
substantialsamplesizeforanalysis.
programminglanguageStantolearnğ¾.WeutilizeKtopredictFYA.
Letğ‘›denotethenumberofsamplesandğ‘ representthenumber
ğ‘
offeaturesofA.Letğ‘ denotethenumberoffeaturesofBand
ğ‘ C.3 SyntheticResults
ğ‘ representthenumberoffeaturesofX,whichisunrelatedtoA
ğ‘¥
Table4summarizestheresultsforthreemodelsonbothsynthetic
andB.Letğµ followtheBernoullidistributionwithaprobability
ğµeq au na dlt ğ‘‹o0 âˆ¼.7. NTh (0e ,n
ğ‘
ğ‘ğ´ âˆ—ğ‘— =
ğ‘
ğ‘((cid:205) âˆ—0ğ‘–ğ‘ =ğ‘ .01 5ğµ )ğ‘– .+ Leğœ€ t) ğ‘Œâˆ—( =ğ‘–âˆ— (cid:205)ğ‘— ğ‘–ğ‘) =ğ‘. 1ğ‘‹
ğ´
ğ‘–is +u (cid:205)nr ğ‘–ğ‘e =ğ‘¥l 1at ğ‘‹e ğ‘–d +w ğœ€i ,th
ğœ€
d t ti oa vt Tea avs bae lr et is a 4a b ,n l oed u, rfr ae fi ra r al n md ea s est wa l oes a re krt n: eu i ffnn ega cw [ t2 iva 4 er ]e l, yam n reo d dd t ue h cl e, ew p sr th o hi pc eh o imsi eg pdn ao O cr B te . os A ft sch ece nos srde itn i in vs g ei-
isthenoiseandğœ€ âˆ¼ N(0,0.5).Wesplitthedataset75/25intoa
variablesğµonğ‘Œ andğ´withminimalinformationlosswhileachiev-
train/testset.Forthecounterfactualdataset,weonlygenerate80%
ingdesirablepredictionperformance.Moredetaileddiscussions
counterfactualdataforallsensitivevariablesğµ .
ğ‘– arelistedasfollows:
Togeneratethedata,wefollowthecasualgraphasFigure4aand
setğ‘›=10000,ğ‘ ğ‘ =3,ğ‘ ğ‘ =40,ğ‘ ğ‘¥ =8.Thecasualgraphissimilar PredictionPerformance. Inordertoachieveimprovedcounter-
withFigure4abutwithdifferentnumberofvariables.Bycomparing factualfairness,ourframeworkmakesatrade-offbyslightlysacri-
(c) and (d) to rest of the figures, we observe that OB effectively ficingpredictionperformance.AccordingtoTable4,theresulting
increasesoverlapoftheobservedandcounterfactualdistributions, degradationinpredictionperformanceislowintheboththehigh-
indicatingimprovedcounterfactualfairnesscomparedtoFTUorFL. dimensionalsyntheticdatasetandLSAT.Comparedtofairlearning,
theOBtechniquegainsgreaterflexibilityinadjustingthedatawith
LawSchooldataset. Thisreal-worlddatasetisderivedfroma minimalmodificationsinhigh-dimensionaldatamatrices,resulting
surveyconductedbytheLawSchoolAdmissionCouncilacross163 inlessdenttoperformanceinthesecases.
lawschoolsintheUnitedStates[32].Thecasualgraphisshown
CounterfactualFairness. KL-divergencemetricinTable4and
inFigure4b.Itcontainscomprehensiveinformationon21,790law
Figure6clearlyillustratethatOBachievesimprovedcounterfactual
students,includingtheirentranceexamscores(LSAT),grade-point
fairness,asevidencedbythedecreasedKL-divergenceandincreased
averages(GPA)collectedpriortolawschool,andtheirfirst-year
overlapbetweentheblueandreddistributions.Inparticular,in
averagegrade(FYA).Thedatasetservesthepurposeofpredicting
thehigh-dimensionalexperiment,ourframeworkexhibitsaminor
whetheranapplicantwillachieveahighFYA,whileensuringthat
degradationof3.43%inpredictionperformance,whilesignificantly
thesepredictionsremainunbiasedbyanindividualâ€™sraceandsex.
enhancingcounterfactualfairness.Therefore,ourframeworkef-
However,theLSAT,GPA,andFYAscoresmayexhibitbiasdue
fectivelyisolatestheimpactofsensitivevariablesonnon-sensitive
to underlying social factors. We split the data set into training
variablesandoutcomes,whilemaintainingahighlevelofprediction
(80%)andtesting(20%)subsetstofitourmodel.Ourframeworkis
performance.
comparedagainstthefairlearning(FL)methodproposedby[24].
Wepostulatethatalatentvariable,astudentâ€™sknowledge,af- VariableCorrelations. ToverifyhowOBaffectstherawdatasets,
fectsGPA,LSAT,andFYAscores.Thecasualgraphcorresponding wenotethecorrelationresultsdemonstratethatourframework
tothismodelisshowninFigure4b.Thisisashort-handforthe significantlydecreasesthecorrelationbetweenğ´&ğµacrossalldata
11
ytisneD ytisneD ytisneDsets.Thisreductionindicatesthatourframeworksuccessfullymiti- betweenğµandğ‘Œ.ThisindicatesthattheOGtechniquesuccessfully
gatestheimpactofğµ. removestheimpactofthesensitivevariable,race,onGPA,LSAT,
andFYAwhileintroducingminimalchangestotheoriginaldataset.
C.4 LSATresult Furthermore,ourmodeloutperformsfairlearning(FL)intermsof
Inparticular,herewediscusstheimpactof OBonrealdataset. predictionperformance,asevidencedbysmallerRMSEandMAPE
In Table 4, we observe that our model achieves lower variable values.Figure6furtherdemonstratesourmodelâ€™seffectiveness
correlationsbetweenthesensitivevariablesğµ andğ´,aswellas in achieving better counterfactual fairness, as indicated by the
increasedoverlapbetweentheredandbluedistributions.
12