Learning a Generalized Physical Face Model From Data
LINGCHENYANG,ETHZurich,Switzerland
GASPARDZOSS,DisneyResearch|Studios,Switzerland
PRASHANTHCHANDRAN,DisneyResearch|Studios,Switzerland
MARKUSGROSS,ETHZurich,SwitzerlandandDisneyResearch|Studios,Switzerland
BARBARASOLENTHALER,ETHZurich,Switzerland
EFTYCHIOSSIFAKIS,UniversityofWisconsinMadison,USA
DEREKBRADLEY,DisneyResearch|Studios,Switzerland
actuations bones skin
Generalized
Physical
Face Model
Input Image Fitting Fitted Physical Face Model Simulated Facial Animation
Fig.1. Wepresentadeepgeneralizedphysicalfacemodelthatcanbefittoasinglefaceimageor3Dscan.Themodelproducesanidentity-specificmaterial
spacewithbones,skinandsofttissue,togetherwithper-expressionjawtransformationsandelementactuationsforfacialsimulation.Oncefittoanunseen
identity,themodelcanbeanimatedtocreatephysics-basedfacialanimation.Applicationslikeretargeting,interpolation,anatomyediting,andphysical
effectssuchascollisionavoidanceandmuscleparalysisareshowninSec.6.
Physically-basedsimulationisapowerfulapproachfor3Dfacialanimationas areoneoftheleadingcontributorstowidespreadadoptionofCG
theresultingdeformationsaregovernedbyphysicalconstraints,allowingto contentinfilms,videogamesandvirtualenvironments.Thereverse
easilyresolveself-collisions,respondtoexternalforcesandperformrealistic isalsotrue,inthatunrealistic3Dcharacterscanleadtoawidespread
anatomyedits.Todayâ€™smethodsaredata-driven,wheretheactuationsfor rejection of the CG content, which is what makes high-quality
finiteelementsareinferredfromcapturedskingeometry.Unfortunately,
facialanimationsocritical.Mostoften,facesareparameterizedand
theseapproacheshavenotbeenwidelyadoptedduetothecomplexity
controlledbylinearblendshaperigs,whicharepopularduetotheir
ofinitializingthematerialspaceandlearningthedeformationmodelfor
simplicityandeaseofuse.Unfortunately,blendshaperigssuffer
eachcharacterseparately,whichoftenrequiresaskilledartistfollowedby
fromwell-knowndrawbackslikeprovidingonlylinearmotionand
lengthynetworktraining.Inthiswork,weaimtomakephysics-basedfacial
animationmoreaccessiblebyproposingageneralizedphysicalfacemodel exhibitingsurfaceinter-penetrations(particularlyaroundthelips).
thatwelearnfromalarge3Dfacedataset.Oncetrained,ourmodelcanbe Analternative,andmorephysicallyaccurateapproachistouse
quicklyfittoanyunseenidentityandproduceaready-to-animatephysical physicalsimulationforfacialanimation,whereanatomicalmuscle
facemodelautomatically.Fittingisaseasyasprovidingasingle3Dface actuationsdrivethesofttissuedeformation.Asabenefit,withina
scan,orevenasinglefaceimage.Afterfitting,weofferintuitiveanimation simulatoritiseasytoaddconstraintstoidentifysurfacecontacts
controls,aswellastheabilitytoretargetanimationsacrosscharacters.All andpreventcollisions,whileproducingmorenaturalnonlinearface
thewhile,theresultinganimationsallowforphysicaleffectslikecollision
motion.Originally,physics-basedmethodswerepopularonlyin
avoidance,gravity,paralysis,bonereshapingandmore.
academicsettingsduetotheircomplexityofadoptioninproduction,
AdditionalKeyWordsandPhrases:DifferentiablePhysics,DeepLearning, howeverrecentlymoreanatomicalandphysics-inspiredapproaches
Physically-BasedFacialAnimation,DigitalHumans havegainedpopularityinthefilmindustry[Choietal.2022]1.
Therearegenerallytwoapproachestophysics-basedfacialani-
1 INTRODUCTION
mation.Inthefirst-principles-basedmethod,researchersmodelthe
Fordecades,3Dfacialanimationhascontinuedtobeachallenging muscleactuationmechanismaccordingtotheauthenticcomplex
andwell-studiedproblemincomputergraphics.Thequestformore musclestructuresintheface,eitherobtainedthroughCT/MRI[Sifakis
realisticfacialmotioncontinuesbecausebelievable3Dcharacters etal.2005]oranoff-the-shelfmuscletemplate[Baoetal.2018;Ichim
etal.2017].Althoughphysiologicallysound,thisstrategyislabori-
Authorsâ€™ addresses: Lingchen Yang, ETH Zurich, Switzerland, lingchen.yang@
ous,anditsrealismhighlydependsontheaccuracyoftheanatomical
inf.ethz.ch; Gaspard Zoss, DisneyResearch|Studios, Switzerland, gaspard.zoss@
disneyresearch.com; Prashanth Chandran, DisneyResearch|Studios, Switzerland, model.Assuch,asecondapproachemergedbasedonshapetar-
prashanth.chandran@disneyresearch.com;MarkusGross,ETHZurich,Switzerlandand geting[KlÃ¡retal.2020],wherefine-grainedandstructure-agnostic
DisneyResearch|Studios,Switzerland,gross@disneyresearch.com;BarbaraSolenthaler,
ETHZurich,Switzerland,solenthaler@inf.ethz.ch;EftychiosSifakis,UniversityofWis-
consinMadison,USA,sifakis@cs.wisc.edu;DerekBradley,DisneyResearch|Studios, 1https://www.fxguide.com/fxfeatured/exclusive-joe-letteri-discusses-weta-fxs-new-
Switzerland,derek.bradley@disneyresearch.com. facial-pipeline-on-avatar-2/
4202
beF
92
]VC.sc[
1v77491.2042:viXra2 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
actuationisinferredfromthecapturedskingeometryandusedto abilitytoparalyzepartsoftheface,editanatomicalbonestructures,
targettheundeformedsofttissue[Srinivasanetal.2021;Yangetal. andobeygravitationalorotherexternalforces.
2022,2023].Sinceonlytheskingeometryandskeletonarerequired Insummary,weproposeaphysicalfacemodelthatisasgener-
tolearnthemodel,thismethodlargelyeasesthemodelinglabor alizableandcontrollableascurrentlinearblendshapemodels,but
whileassuminggreatflexibility,highfidelity,andimprovedrealism. withtheaddedbenefitofmorephysically-accuratefacialdeforma-
Whiletheshapetargetingapproachisattractive,itiscurrently tions.Webelieveourworkwillhelptodemocratizephysics-based
onlytractableforafewherocharactersinaproduction.Thislim- facialanimation,makingitassimpleasfittingananimation-ready
itation stems from two aspects. First, although the modeling ef- simulationmodeltoasinglescanorimage.
fort is less than a first-principles approach, there is still a dedi-
catedsetupprocedurerequiredforeachcharacter.Thisinvolves
2 RELATEDWORK
scanning and tracking a dataset of facial geometry from the ac-
tor in various expressions, building a simulation-ready material Facialmodelsusedinanimationrangefromsimplegloballinear
spacetomodelthesofttissueandbonesatrest,andthentrain- shapemodelstocomplexlocalmodelsthatincorporatetheunder-
ingadedicatedneuralnetworktopredictthemuscleactuations lyingfacialanatomythroughanatomicalconstraintsorphysical
foreachexpression-dependentdeformation.Secondly,thereisa simulation.Wewillfocusourdiscussiononanatomicalfacemodels
time-consuming,memory-intensive,andidentity-specificdifferen- andthegenerationof3Dfacemodelsfrommonocularinput.
tiablesimulationinthetrainingloop.Itwouldbeinfeasibletotrain
manycharacter-specificnetworkswithinareasonableamountof
2.1 Physics-BasedFacialAnimation
timeandresources,usingthecurrently-availablearchitecturesfor
soft-tissue-basedphysicalfaceanimation. Physics-based facial animation typically employs two main ap-
Inthisworkweaimtoalleviatetheseissuesandmakephysics- proaches. The first, known as the first-principles-based method,
basedfacialanimationmorefreelyaccessible.Tothisend,wepro- requiresuserstointricatelymodelthemuscleactuationmechanism
pose a new, generalized physical face model that can be easily basedonthecomplexstructuresoffacialmuscles.Thesestructures
adaptedtoanynewcharacterwithoutmanualsetupcosts.Weac- arederivedfromsourcessuchasCT/MRIscans[Sifakisetal.2005]
complishthisbylearningasingleimplicitneuralmodelforactuation orpre-existingmuscletemplates[Baoetal.2018;Ichimetal.2017].
mechanismstrainedonalargedatasetofhundredsofidentitiesper- Whilethisapproachensuresphysiologicalaccuracy,thegeneration
formingavarietyofexpressions.Naturally,theabove-mentioned ofsuchmodelsislabor-intensiveandtherealismoftheresulting
limitationswouldposeaconsiderableproblemforthisapproach animation heavily relies on the precision of the anatomical and
aseachidentitywouldrequireapersonalizedmaterialspaceand biomechanicalmodels.
trainingonsuchalargedatasetwithsimulationintheloopwould Inresponsetothechallengesposedbythefirst-principles-based
beimpossible.Weovercomethesehurdleswithourtwomaincon- method,asecondapproachhasemerged,knownasshapetargeting
tributions.First,weproposeadesignthatallowsforsimulation-free [KlÃ¡retal.2020].Inthismethodology,everyelementwithinthe
training,whichisfastandmemoryefficient,allowingustotrainon meshisconsideredactiveandsubjecttoactuationforthepurposeof
alargedatasetoffacesandlearnthegeneralizabilityonewouldneed inducingforcesthatdrivethemotionofthefaceorbody.Torepro-
forsuchanapplication.Second,weproposeanarchitecturethat duceidentity-specificfacialexpressions,anactuationmechanism
automaticallypredictsanidentity-conditionedmaterialspaceby isoptimizedbasedonfacecapturedata.IntheworkofSrinivasan
warpingasinglecanonicalmaterialspace.Despitenothavingsimu- etal.[2021],theintegrationofneuralnetworkswithacomprehen-
lationinthetrainingloopnorhand-craftedper-identitymaterial sivemusclemodelfacilitatedthelearningofthemuscleactuation
spaces,ournetworkstillproducesoutputsthatarecompatiblewith mechanism.Subsequentadvancementsinthefieldtransitionedto-
aphysicssimulator,suchthattheresultingfacescanbeanimated wardsanimplicitneuralrepresentation,renderingtheactuation
usingphysics-basedfacialanimation. mechanism more compact and independent of resolution [Yang
Ourmodelisconditionedontwolatentcodes,oneforidentityand etal.2022].Buildinguponthisfoundation,Yangetal.[2023]fur-
oneforexpression,andtheoutputisanidentity-specificmaterial therextendedthemethodologybytrainingtheneuralnetworkon
space(i.e.,skull,jaw,skinandsofttissueinbetween)coupledwith captureddatafromasmallnumberofindividualssimultaneously.
identity-andexpression-specificactuationsandbonekinematics Thismulti-identitytrainingapproachenablesthemodeltolearn
thatcanallbereadilyprovidedtoanoff-the-shelfphysicssimulator. diverseexpressionstylesbasedondistinctactuationpatternsand
Asaresult,ourpre-trainednetworkcanbeusedtogeneratean allowsapplicationssuchasstyleretargeting.
animation-readyphysics-basedsimulationmodelforanynewchar- Notably,theseactuation-basedapproachesstreamlinethemod-
acter,simplybymodifyingtheidentitylatentcode.Wedemonstrate elingprocessbyonlynecessitatingskingeometryandbonedata
thatourmodelcanbefittoasingle3Dneutralscanofanactor,or formodeltraining.Nevertheless,theirheavyrelianceonidentity-
evensimplytoasinglefaceimage.Oncefitted,themodelallows specificdata,hand-craftedmaterialspacesandarchitecturesthat
animationthroughthecontrolsofacommon3Dmorphableface onlyallowtrainingononeorfewidentitiesatatimerepresent
model,whicharemappedtoourlatentspace.Furthermore,our significantlimitations,particularlyhinderingtheirgeneralizability
methodsupportsanimationretargetingbyswappingidentitycodes. andwidespreadapplicationinproductionsettings.Weaddressthis
Inallcases,theresultinganimationbenefitsfromphysicaleffects byintroducingageneralizedphysicalfacemodelthatcanbeeasily
likethedetectionofsurfacecontactsandcollisionavoidance,the adaptedtoanynewcharacter.LearningaGeneralizedPhysicalFaceModelFromData â€¢ 3
TherecentworkofWagneretal.[2023]istheclosestinspirit etal.2020a],withthemaindifferencethatoursallowstosimulate
toourwork,asweshareacommonmotivationalthoughverydif- physicaleffects.Acommonapplicationofmost3DMMsistheiruse
ferentsolutions.Theyproposeanextensiontolinearblendshape inmonocularfacereconstruction,e.g.,fittingthemodeltoimages.As
modelsthataimstomimicphysics-basedfacialanimationwithin such,wepresentahigh-levelsummaryofexistingtechniqueswhere
alinearframework.Physicalsimulationisonlyusedtogenerate amorphablemodelisusedtorecoverapersonâ€™sfacialgeometryin
trainingdata,notatruntime,andthustheyaretiedtotheprescribed 3Deitherbydirectlyoptimizingthefaceshapebasedonanobserved
simulationeffectspresentinthedataset.Forexample,theycannot imageorthroughaninference-drivenapproachthattrainsneural
handlelipcollisions,whichisaprimaryreasontoemploysimula- networkstopredicttheparametersofamorphablemodel.
tionforfacialanimation.Ontheotherhand,weproposeanonlinear Determiningtheoptimal3DMMshape,expression,andposepa-
physics-basedfacemodeltrainedonrealcapturedata,whichcan rametersforagivenRGBimageisachievedthrougheitheranalysis-
befittoanynovelidentityandprovideatruephysicalmodelthat by-synthesisoptimization[Geceretal.2019,2021]ordeepneural
employssimulationatruntimetoallowanydesiredphysicaleffect, networkregression[Fengetal.2021;Zhangetal.2023;Zielonka
includinglipcollisionavoidance. etal.2022].Recently,therehavealsobeenseveralapproachesthat
relyonadditionalperceptionbasedlosstermstoimprovethevisual
2.2 Anatomically-ConstrainedFaceModels qualityofthesemorphablemodelfits[Daneceketal.2022;Filntisis
etal.2022;Ottoetal.2023].
Anatomicalconstraintsonthefacialsurfaceareoftenusedtoplau-
Comprehensivesurveyson3DMMsandtheirapplicationinmonoc-
siblyrestricttherangeoftheskindeformations.Theanatomically-
ularfacecaptureareprovidedbyEggeretal.[2020],andMoraleset
constrainedlocaldeformationmodel,introducedinthecontextof
al.[2020].Asanapplicationofourgeneralizedphysicalfacemodel
monocularfacialperformancecapturebyWuetal.[2016],initially
wealsoshowtheabilitytofitthemodeltounseenidentitiesinthe
establishedaconnectionbetweentheskinsurfaceandanatomical
formofasinglefaceimage.However,wedonotproposeacompet-
bones. This was achieved by modeling the thickness of the soft
ingmethodforaccurate3Dgeometryreconstructionbutrathera
tissuebetweenabonepointandtheskinsurface,togetherwithskin
convenientapproachtoobtainananimation-readyphysicalmodel,
slidingcoupledwithbulging.Thisanatomically-constrainedface
usingasimilarfittingapproachasinthefieldofmonocularface
modelwasalsobeneficialinfacemodelingapplications,helping
capture.
untraineduserstoquicklycreatebelievabledigitalcharacters[Gru-
beretal.2020].Inthedomainoffacialperformanceretargeting, 3 PRELIMINARIES:ACTUATEDFACESIMULATION
Chandranetal.[2022]employedthesamemodeltoconfineare-
Incontinuummechanics,motionischaracterizedbyaninvertible
targetedshapewithintherealmofanatomicallyplausibleshapes
mapğœ™ : X âˆˆ Î©0 â†’ x âˆˆ Î©fromtheundeformedmaterialspace
specifictoatargetactor.Animplicitvariantoftheanatomicalface
Î©0 tothedeformedspace Î©.Thedeformationgradient,F(X) =
modelwaspresentedbyChandranandZoss[2023],whichfacili-
tatesthelearningofacontinuousanatomicalstructurethatdensely
âˆ‡ Xğœ™(X),encodesthelocaltransformationsincludingrotationand
stretch.Thequasi-staticstateofğœ™ intheabsenceofexternalforce
constrainstheskinsurface.Themodelcandisentangledeforma-
isgovernedbythepoint-wiseequilibrium:
tionarisingfromrigidbonemotionandnon-rigiddeformations
ğœ•Î¨
createdbymuscleactivations.Qiuetal.[2022]learnedananatomi- divÂ·P=divÂ· (F)=0, (1)
calfacialshapemodelfrommedicalimagingdata,andpresenteda ğœ•F
morphablemodelthatisabletogeneratefacesthatjointlymodel where P is the first Piolaâ€“Kirchhoff stress tensor that measures
theskull,facialsurfaceandappearance.Choietal.[2022]replaced theinternalforce.Forhyperelasticmaterial,Pisassociatedwith
themuscle-basedparameterizationusedearlier[Sifakisetal.2005; a specific energy density function Î¨ that describes the material
Srinivasanetal.2021]byacollectionofmusclefibercurves,whose behavior.Intuitively,Eqn.(1)meansthenetforcewithinthematerial
contractionandrelaxationprovideafine-grainedparameterization iszeroeverywhere.
ofhumanfacialexpression.Theapproachstrikesabalancebetween Inthecontextofactuatedfacesimulation,thematerialspaceÎ©0
therequirementsforanatomically-basedandartist-friendlymod- isdefinedastheundeformedsofttissuespaceconfinedbetweenthe
els,butcomesattheexpenseofreducedphysicalaccuracy,asthe restbonesğœ•Î©0 andskinğœ•Î©0 .ğœ•Î©0 consistsoftheskull
bones skin bones
simulationissolelyemployedinapre-processingsteptoacquirean ğœ•Î©0 andthejawğœ•Î©0 ,whichwillconstrainanddragthesoft
skull jaw
approximatedeformationmodelofmusclefibers. tissueduringarticulation.ThedeformedspaceÎ©isthesofttissue
Similarly, our approach is designed for ease of usability, as it spaceofthetargetexpression.ForÎ¨,theshapetargetingmodel[KlÃ¡r
sharesthegeneralityandcontrollabilitycharacteristicsfoundin etal.2020]isemployed:
existinglinearblendshapemodels,yetitofferstheadditionaladvan-
tageofachievingmorephysicallyaccuratefacialdeformation. Î¨(F,A)= min ||Fâˆ’RA||2 ğ¹ =||Fâˆ’Râˆ—A||2 ğ¹, (2)
RâˆˆSO(3)
whereAisasymmetricactuationtensormimickingthelocalmuscle
2.3 Morphable3DFaceModels
actuation.Râˆ—isthepolardecompositionofFA,makingÎ¨rotationally-
Asourgeneralizedphysicalfacemodelisdrivenbyidentityand invariant. Based on embedded simulation, Î©0 is uniformly dis-
expressionandproducesadeformed3Dfaceastheoutput,itisakin cretizedintoasimulationmeshM simusingregularelementswith
totraditional3Dmorphablefacemodelsincurrentliterature[Blanz nodal vertices u0, where the discretized skin, skull and jaw are
andVetter1999;Chaietal.2022;Daietal.2019;Lietal.2017;Yang linearly embedded with barycentric weights Wğ‘ u0, Wğ‘šu0, and4 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
Wğ‘›u0respectively.WiththeFiniteElementMethod(FEM)applied 4.1 Simulation-freeLearning
to Eqn. (1), the simulation then reduces to an energy minimiza- Given a material space Î©0, where the skin ğœ•Î©0 , skull ğœ•Î©0
tionproblemw.r.t.thedeformedverticesusuchthattheboundary andjawğœ•Î© jaw areexplicitlydefined,wewantts oki in nferaconts ik nu ull -
conditionsfromthearticulatedbonearesatisfied,asfollows: ousactuationfieldA(Â·) andajawtransformation{Rjaw,tjaw}to
matchagivenexpressioninasimulation-freemanner.Formally,the
arg uminâˆ‘ï¸
ğ‘’
ğ‘‰ 2ğ‘’ ||Fğ‘’(u,u0)âˆ’Rğ‘’âˆ—Ağ‘’||2 ğ¹ (3) objectiveforthei an rv ge mrs inedesi âˆ«gnisas ||f ğœ™ol (l Xow )âˆ’s:
ğœ™Ë†(X)||2ğ‘‘X (5)
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 2
s.t. Wğ‘š u= Wğ‘š u0+ 0 , (4) ğœ™(Â·),A(Â·),Rjaw,tjaw ğœ•Î© s0 kin
Wğ‘› RjawWğ‘› tjaw s.t. divÂ·(âˆ‡ Xğœ™(X)âˆ’Râˆ—(X)A(X))=0, âˆ€XâˆˆÎ©0AâˆˆSO(6) (6)
ğœ™(X)=X, âˆ€Xâˆˆğœ•Î©0 (7)
w thh ee rr ie gğ‘‰ idğ‘’ ti rs at nh se fv oo rmlu am tie of nor foe rac thh eel je am we .ntğ‘’while{Rjaw,tjaw}denotes ğœ™(X)=RjawX+tjaw,sku âˆ€ll
Xâˆˆğœ•Î© j0 awRjaw âˆˆSO(3) (8)
Insummary,thesimulationentailsthreecoreelements: ğœ™ âˆˆÎ¦ (9)
bio
â€¢ Identitymaterialspace(Î©0)togetM sim withembedded whereğœ™Ë†denotesthegroundtruthdeformationthatisonlydefined
skin,skull,andjaw. onğœ•Î©0 ,i.e.,through3Dscanning.Eqn.(6)comesfromthepoint-
skin
â€¢ FacialactuationdefinedbythetensorfieldAoverM sim. wiseequilibriumofEqn.(1)withPinstantiatedfromEqn.(2).Eqn.(7)
â€¢ Jawkinematicsviamandibletransformation{Rjaw,tjaw}. and Eqn. (8) guarantee that the skull is fixed and jaw is rigidly
articulated.Eqn.(9)constrainsthemappingğœ™suchthatitresembles
Thelattertwocomponentsfunctionasmuscleactuationmecha- bio-mechanicallyplausiblesofttissuedeformation,whichwerefer
nismsandareutilizedastheinputphysicalconstraints.Thissimu- toasthespaceÎ¦ (andwillelaborateonlaterinthediscussionof
bio
lationprocesscanbemadeefficientlydifferentiablewiththeadjoint theSoftLoss).Themotivationofoursolutionliesinthefactthat
method,allowinginversedesignofAand{Rjaw,tjaw}fromatarget givenanyinvertiblemappingfunctionğœ™,Eqn.(6)canbesatisfied
expressionoftheidentity.However,theexpensivecomputation bysettingtheactuationtensorfieldA(Â·)as:
fromthesimulationandtherequirementofthepre-definedma-
terialspaceÎ©0preventthismethodfromlendingitselftoalarge A(X)=Rğœ™(X)âŠ¤âˆ‡ Xğœ™(X), (10)
facedatasetofhundredsofidentities,wheretheidentity-specific whereRğœ™(X)isthepolardecompositionofâˆ‡ Xğœ™(X)atX.Thisrep-
materialspace(underlyinganatomy)istypicallymissing.
resentationgivesthezerostresstensorPandhencethezerodiver-
genceeverywhere.Basedonthisobservation,theinversedesigncan
4 METHOD besimplifiedtofindingğœ™âˆ—thatcloselyapproximatesğœ™Ë†onğœ•Î© while
skin
atthesametimesatisfyingEqn.(7),Eqn.(8)andEqn.(9)asmuchas
Ourgoalistobuildageneralizedphysicalfacemodelfromalarge
3Dfacedatasetwhereonlyskingeometryisgiven.Thisfacemodel
possible.Then,wecancomputeA(Â·)and{Rjaw,tjaw}fromğœ™âˆ—auto-
shouldbegenerativeandanimatable,suchthatitcanbefittoava-
matically.Specifically,A(Â·)isobtainedwithEqn.(10).{Rjaw,tjaw}is
rietyofinputdata(e.g.,3Dscansorfaceimages)andthenanimated obtainedwithProcrustesalignmentbetweenğœ™âˆ—(ğœ•Î© j0 aw)andğœ•Î© j0 aw.
withintuitivecontrols,providingaconvenientmechanismtoobtain Thesephysicalconstraintswillbecompatiblewiththesimulation
aphysicalfacemodelforanimationpurposes. afterdiscretizationwithFEM.
To achieve this goal, we propose a new network architecture
Toachievethisgoal,weparameterizethemappingfunctionğœ™
forimplicitactuationmechanisms(illustratedinFig.2).Atahigh withanimplicitneutralnetworkNğ‘’.Then,weintroduceseveral
level,ourmodelisdrivenbytwolatentvariablesğ›½ andğ›¾,which novellossfunctionstoconstrainit.
representtheidentityandexpression,respectively.Theoutputisthe
ReconstructionLossL .Thefirstlossisthereconstruction
identity-specificmaterialspaceÎ©0withdiscretizedsimulationmesh
lossdefinedonğœ•Î©0 :
skin
M simatrest,coupledwiththejawtransformation{Rjaw,tjaw}and skin
actuationtensorfieldAthat,whensimulated,deformsthefaceto âˆ‘ï¸ğ‘ğ‘£
1
performthegivenexpression. L skin(ğœ•Î© s0 kin)=
ğ‘
ğ‘£||Nğ‘’(Xğ‘–)âˆ’xË†ğ‘–||2 2, (11)
AsmentionedinSec.3,actuatedfacesimulationtraditionallycan- ğ‘–=1
notbetrainedonlargedatasets.Ourmethodispossiblethanksto whereXğ‘–representstheğ‘–-thsampledpointfromğœ•Î©0 ,andxË†ğ‘–indi-
twonovelcontributions.Thefirstoneisdesignedforefficienttrain- skin
catesthecorrespondinggroundtruthposition.Thecorrespondence
ing,whereweparameterizeandlearnthephysicalconstraintsina
istypicallyapproximatedthroughsurfaceregistration.
simulation-freemanner(Sec.4.1).Second,weintroduceamaterial-
spacegenerativenetworkthatproducestheidentity-specificmate- RigidityLossL rigid.Thesecondlossistoenforcetherigidityof
rialspacesautomatically,withnomanualmodeling(Sec.4.2).We thebone,inspiredbyEqn.(8):
w (o Sri eal cl t .efi 4r o .s 3nt )d .he os wcri tb he et yh ae rs eet cw omoc bo inn etr dib tu ot pio rn os vii dn eis to hl eat cio on m, pa ln ed teth pe in pee ll ia nb e- L rigid(ğœ•Î© b0)= RâˆˆSOm (3in ),tâˆˆR3âˆ‘ï¸ğ‘ ğ‘–ğ‘ ğ‘1 ğ‘||Nğ‘’(Xğ‘–)âˆ’(RXğ‘– +t)||2 2, (12)LearningaGeneralizedPhysicalFaceModelFromData â€¢ 5
continuous discretized
canonical identity expression ground truth
simulator
Fig.2. Overviewofourmodel.Drivenbyidentityğ›½andexpressionğ›¾latentcodes,Nğ¶ learnstodeformacanonicalmaterialspaceÎ©ğ¶ tobeidentity-specific,
Î©0,ğ›½.ThenNğ‘’learnstodeformthematerialspacetomatchagivenexpression,Î©ğ›¾,ğ›½.Thelatentcodesareparameterizedbyacommon3DMM(ğ›½Ë†,ğ›¾Ë†).The
networkistrainedwithphysically-inspiredconstraintssothat,afterdiscretization,asimulatorwillproducephysics-basedfacialanimation.
wherewesampleğ‘ ğ‘ pointsintotalforeachregionğœ•Î©0.Weapply themethodmuchfasterandeasilyscalablesinceallthelossfunc-
b
thislossseparatelytotheskullğœ•Î©0 andthejawğœ•Î©0 .Therefore, tionsarepoint-wise.Insummary,thisnetworkgeneratesphysically-
skull jaw
wehaveL =L (ğœ•Î©0 )+L (ğœ•Î©0 ). constraineddeformationsthatarestrategicallyconvertedintophys-
rigid rigid skull rigid jaw icalconstraintsusedinthesimulation.
FixationLossL .Specificallytoğœ•Î©0 ,weintroducethethird
fix skull 4.2 MaterialSpaceMorphing
loss to enforce the fixation of the skull area, corresponding to
Eqn.(7): Wenowdescribeoursecondmaincontributiontoaddressthefact
thateveryidentityinthedatasetneedsacustommaterialspace.Our
ğ‘ğ‘“ approachistoinferthematerialspaceofanidentityautomatically
L fix(ğœ•Î© s0 kull)=âˆ‘ï¸ ğ‘1 ||Nğ‘’(Xğ‘–)âˆ’Xğ‘–||2 2, (13) bylearningtomorphasinglecanonicalmaterialspacetoanynew
ğ‘– ğ‘“ person.
GivenacanonicalmaterialspaceÎ©ğ‘
,wherethecanonicalskin
wherewesampleğ‘ ğ‘“ pointsintotalontheskullareağœ•Î©0 .
ğœ•Î©ğ‘ skinandbonesğœ•Î©ğ‘
bonesareexplicitlydefined,wewanttomorph
skull itintothematerialspaceÎ©0ofanidentityusinganotherimplicitnet-
SoftLossL soft.Thefourthlossistolearnbio-mechanicallyplau- workNğ‘.Weproposethreelossfunctionstoconstrainthesolution
sibledeformationbasedontheYoungâ€™sModulus(ğ¸)andPoissonâ€™s
duringtraining.
Ratio(ğœˆ),inspiredbyEqn.(9).Thislossconsistsoftwoterms,an
IdentityLossL .Thefirstlossistoprovidesupervisiononthe
elasticoneandavolume-preservingone: id
skinarea,similartoEqn.(11).
L soft(Î©0)=âˆ‘ï¸ğ‘ ğ‘–ğ‘  ğ‘1
ğ‘ 
Râˆˆm SOin (3)ğœ‡||âˆ‡ XN e(Xğ‘–)âˆ’R||2 2+
(14)
L id(ğœ•Î©ğ‘ skin)=âˆ‘ï¸ ğ‘–ğ‘ =ğ‘£
1
ğ‘1 ğ‘£||Nğ‘(Xğ‘ ğ‘–)âˆ’XË† ğ‘–||2 2, (15)
detm (Din )=1ğœ†||âˆ‡ XN e(Xğ‘–)âˆ’D||2 2, whereXğ‘
ğ‘–
representstheğ‘–-thsampledpointfromğœ•Î©ğ‘ skin,andXË†
ğ‘–
indicatesthecorrespondinggroundtruthpositionontheneutral
identitymeshinthedataset,attainablefrom3Dscanning.
wherewesampleğ‘
ğ‘ 
pointsintotalintheentirematerialspaceÎ©0.
ğœ‡andğœ†aretheLamÃ©parameters,describingthematerialbehavior. BoneShapeLossL bone.Sinceweonlyhavedirectsupervision
Thesetwoparametersareparameterizedbyğ¸andğœˆasğœ†=ğ¸ğœˆ/(1+ ontheskinarea,weproposetoconstraintheboneshapesusingan
ğœˆ)(1âˆ’2ğœˆ)andğœ‡=ğ¸/2(1+ğœˆ)respectively.Thislossisessentialas off-the-shelfparametricbonegenerator[Qiuetal.2022],whichcan
itnotonlyconstrainsthedeformationbutalsoimplicitlybuildsthe predictplausibleskullandjawshapesgivenaneutralfacemesh.
connectionbetweentheboneandtheskin.Whentheoutputskin Formally,thelossisasfollows:
issupervisedtowardsthegroundtruth,thejawisalsoplacedina
c oo un T rs h et er na s di en -tfe ood -u ep r no l dosi s tt s rio atn ie n, r imh ne gsn (c fL ue nsi kn
ci
tnf ie , or L nri rn digg
eid
fit ,h nLe edfija xw i, nLk Ssi eon cfe
t
.)m 4wa .3t ii l .c l Ts b. he eu ss ie gd ni in
f-
L bone(ğœ•Î©ğ‘ bones)=âˆ‘ï¸ ğ‘–ğ‘ =ğ‘
1
ğ‘1 ğ‘||Nğ‘(Xğ‘ ğ‘–)âˆ’XË† ğ‘–||2 2, (16)
icantbenefitofthisformulationisthatwemovethetraditional wherewesampleğ‘ ğ‘ pointsonğœ•Î©ğ‘ intotal,andhereXË† ğ‘– isthe
bones
FEM-discretizedsimulationoutofthelearningloop,whichmakes pseudo-groundtruthbonepositiononthebonesurfacesgenerated6 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
bytheparametricmodelgiventhegroundtruthidentityneutral
shape.
L =ğœ† L +ğœ† L +ğœ† L +ğœ† L
test skin skin rigid rigid fix fix soft soft
ElasticRegularizationL ereg.Finally,sinceweonlysuperviseon +ğœ† L +ğœ† L +ğœ† L . (19)
bone bone ereg ereg lreg lreg
thesurfaceareas,weincorporateanelasticregularizationtosmooth
thevolumetricmorphing: Here,L andL canvarydependingontheformoftheinput
skin bone
data.Forexample,whenfittingtoanunseen3Dscanwecanuse
L ereg(Î©ğ‘ )=âˆ‘ï¸ğ‘ ğ‘–ğ‘  ğ‘1
ğ‘ 
Râˆˆm SOin (3)||âˆ‡ Xğ‘Nğ‘(Xğ‘ ğ‘–)âˆ’R||2 2, (17) t ah ne ds Ea qm ne .(s 1c 6a ))n a-t so d- um re ins gh tl ro as is na inn gd .Hpr oe wdi ec vt ee rd ,ob uo rn me olo ds es l( isi.e fl., exE iq bn le.( a1 n1 d)
allowsfittingtootherdata,suchas2Dfaciallandmarkscomputed
wherewesampleğ‘ ğ‘  pointsonÎ©ğ‘ intotal.Thislossalsomakesthe fromanimage,wherewecanformulateL skinasa2Dprojectionloss
trainingrobusttoanyincorrectestimationfromtheboneprediction andL asaself-supervisedboneloss,aswewilldescribelater
bone
inEqn.(16).Thesethreelossterms(L id,L bone,L ereg)willbeadded inSec.6.1.Finally,asmentionedearlierwecandirectlymanipulate
toourend-to-endtrainingapproach,describednext. thephysicalmodelfromtheartist-friendly3DMMparameters,or
swaptheidentitylatentcodeforanimationretargeting.Wecan
4.3 GeneralizedPhysicalFaceModel
alsointerpolatebetweenidentitycodesfornovelfacegeneration.
Wecannowdescribeourcompletepipeline,illustratedinFig.2.The Inallcases,givenağ›½andğ›¾ codeattesttime,wecangeneratethe
twocontributionsdescribedinSec.4.1andSec.4.2makeitpossible materialspaceviaNğ‘ andevaluatethephysicalconstraintswith
tolearnageneralizedphysicalfacemodelfromalargedatasetof Nğ‘’ andperformphysicalsimulation.Themainsuperiorityofour
3Dfacialscans(skinonly).Tosummarize,givenanidentitylatent physicalfacemodeloverothertraditionaldeepfacemodelsisthat
codeğ›½,ourgenerativeimplicitnetworkNğ‘ learnstodeformpoints duringsimulation,wecanaddadditionalphysicaleffectssuchas
fromacanonicalmaterialspaceÎ©ğ‘ toanidentity-specificmaterial collisionhandling,externalforces,etc.Alloftheseapplicationswill
spaceÎ©0,ğ›½
.Thengivenboththeidentitycodeandanexpression
bedemonstratedinSec.6.
latentcodeğ›¾,ourimplicitdeformationnetworkNğ‘’ generatesthe
identity-andexpression-specificdeformationsÎ©ğ›¾,ğ›½
,fromwhich
5 IMPLEMENTATION
thephysicalconstraintscanbeobtainedforsimulation. Inordertotrainournetworkthereareanumberofimplementation
Inordertoregularizetheidentityandexpressionlatentspaces, detailstoconsider.
encouragedisentanglement,andallowforartist-drivenanimation
TrainingData.Weusethe3DfacedatasetpresentedbyChan-
aftertraining,weparameterizethelatentcodesusinganoff-the-
dranetal.[2020],whichconsistsof336identitiesundervarious
shelfmorphable3Dfacemodel[Lietal.2017].Specifically,ğ›½ =
expressionstotaling13000facescansintopologicalcorrespondence,
P id(ğ›½Ë†) andğ›¾ = Pexp(ğ›¾Ë†),whereP
id
andPexp aresmallMLPswith
withrigidheadmotionremoved.Everyidentityhasoneâ€œneutralâ€
threelayerseachthatarelearnedwiththerestofthenetwork,and
expressionthatisusedinEqn.(15).Asthedatasetcontainsonlyskin
ğ›½Ë†andğ›¾Ë†aretheidentityandexpressionparametersofthe3DMM.To
geometry,wepre-fitthebonesusingaparametricskeletalmodel
obtaintheinputfortraining,wepre-computethe3DMMparameters
calledSCULPTOR[Qiuetal.2022],whichprovidestheconstraints
correspondingtoeachfaceinourdatasetusingleast-squaresfitting.
fortheboneshapelossinEqn.(16).Theskinandthebonesurfaces
CompleteLossFunction.Ourfullsetofoptimizationvariables discretizeğœ•Î©âˆ— andğœ•Î©âˆ— ,respectively.AsmentionedinSec.4.3
skin bones
includethenetworkweightsofNğ‘,Nğ‘’,P idandPexp.Weregularize ofthemaintext,wefitFLAMEmodelparameters(ğ›½Ë†,ğ›¾Ë†)toalldata
thelatentcodeswithğ‘™-2regularizationL lreg = ||ğ›½||2 2+||ğ›¾||2 2.For inapre-processingstep.
smoothness,wealsoapplyLipschitzregularizationL liptoP idand CanonicalSpace.ThecanonicalmaterialspaceÎ©ğ‘
isdefinedusing
Pexp,asinYangetal.[2023].Puttingitalltogether,thecomplete
themeanboneandskinsurfacesofSCULPTOR.Fortheboneswe
objectivefunctionfortrainingourfacemodelis:
usethetopologydirectlyfromSCULPTOR,butforthefacialskinwe
fitthetopologyfromour3DfacedatasettotheSCULPTORmean
L train=ğœ† skinL skin+ğœ† rigidL rigid+ğœ† fixL fix+ğœ† softL soft faceinordertoobtainvertexconsistencywithourdataset,forthe
+ğœ† idL id+ğœ† boneL bone+ğœ† eregL ereg (18) constraintsinEqn.(11)andEqn.(15).
+ğœ† lregL lreg+ğœ† lipL lip, TrainingDetails.Allthelossesdefinedonğœ•Î©âˆ— ğœ•Î©âˆ— ,ğœ•Î©âˆ— ,
skin bones skull
andğœ•Î©âˆ— areevaluateddirectlyonthemeshvertices,whilelosses
whereğœ† âˆ—arebalancingweights.Ourmodelistrainedend-to-endin definedja iw
nÎ©âˆ—arecomputedthroughuniformsamplingofthesoft
asimulation-freemannerwiththedirectsupervisiononlyfromthe
tissuespace.Wemaskoutthenecessaryfaceandboneregionsfor
skinscanswhiletheanatomicalfeaturessuchastheboneshapes,
regression.Specifically,weonlyconsiderthefrontalfaceandassign
jawkinematics,andthefacialactuation,areinferredautomatically.
10timeslesssupervisionweighttothelow-confidenceregions(con-
Test-timeOptimizationandApplications.Oncetrained,one fidencepervertexisavailableinthe3Ddataset).Duringtestingand
applicationofourmodelistofitittounseenidentities.Toaccom- evaluation,weusethesamemasksforconsistency.Forsimulation,
plishthisweoptimizethelatentcodesğ›½andğ›¾ usingthefollowing thediscretizationresolutionisapproximately2mmifnotexplicitly
objective: mentioned.WeusethesamesolverasYangetal.[2022].LearningaGeneralizedPhysicalFaceModelFromData â€¢ 7
Table1. Timingofdifferentcomponents.
Fittingtoa3DScan.Ithasbecomecommonpracticetoperformat
leastasmallamountof3Dfacescanningfortheprimaryactorsin
Training Testing
high-endproductions.Givenjustasinglescan,ourmodelcanbefit
Total PerIter Optim. PhyCons. Sim.
toprovidephysics-basedanimation.Whenfittingtoa3Dscan,L
skin
23.13h 0.50s 6.98s 0.05s 2.58s andL aredefinedthesamewayasduringtraining(Eqn.(11)
bone
andEqn.(16)).SeveralfittingresultsareshowninFig.3,including
theinputscan,thepredictedactuations(forthepredictedsimulation
NetworkandHyper-Parameters.ForbothNğ‘’ andNğ‘,weuse
meshes),theestimatedboneshapesandmandibletransformation,
aconditionalSIRENnetworkinspiredbyYangetal.[2022]forits
andthefinalsimulatedfacialskinsurface.Wehighlighttheverylow
sounddifferentialproperties.Thedimensionsforourlatentcodes
errorbetweenthesimulatedresultandtheinputscan.Notethatthe
ğ›½ andğ›¾ are set to 128 while the corresponding 3DMM codes ğ›½Ë† inputscandoesnotneedtobeinarestconfigurationsincewefit
andğ›¾Ë† usedforparameterizationaresetto100dimensionseach. boththeidentityandtheexpressionparameters.Inthisapplication,
Thebalancingweightsfortrainingaresetasfollows:ğœ† skin = 20, fitting takes less than 15 seconds on a desktop machine with a
ğœ† rigid=20,ğœ† fix=20,ğœ† soft=0.1,ğœ† id=1,ğœ† bone=0.1,ğœ† lreg=1ğ‘’âˆ’4,
commodityGPU(NvidiaGTX1080Ti),yieldinganincrediblyfast
ğœ†
ereg
=0.1,andğœ†
lip
=2ğ‘’âˆ’6.Thenumberofsamplingpointsare
methodtoobtainanidentity-specificphysicalfacemodel.
ğ‘
ğ‘£
=45ğ‘˜,ğ‘
ğ‘
=5ğ‘˜,ğ‘
ğ‘“
=10ğ‘˜andğ‘
ğ‘ 
=10ğ‘˜.Thevaluesfordensity,
Youngâ€™sModulus(ğ¸)andPoissonratio(ğœˆ)aresetto0.9g/ml,5kPa FittingtoaFaceImage.Whena3Dscanisnotavailable,our
and0.47,respectively,accordingtorelatedliterature[XuandYang modelisflexibleandcanbefiteventoasinglefaceimage.When
2015]. fittingtoanimagetheskinlossisdefinedintermsof2Dfacial
landmarks,andourmethodpredictsnotonlyğ›½andğ›¾ butalsothe
TestSet.Fortesting,wepreparetwodatasets.Thefirstdatasetisa cameraprojectionmatrixC.L isthendefinedas:
skin
staticdatasetthatcontains28unseenidentitiestotaling529scans,
w i dd ah e tni ac t sh i eti tw e csil ol w nb i se t ih su tds inie vd gert oo s fee 5ex xa upm nri sen ese s ei nt oh n pe s e.m rT fo ohd ree mslâ€™ e as c ng o ce n en de sd era a ql t ui az esa net t cio i esn sat oo d fyu 5nn a ss eme ee i nn c L skin(ğœ•Î© s0 kin)=âˆ‘ï¸ ğ‘–ğ‘ =ğ‘™
1
ğ‘1 ğ‘™||CÂ·Nğ‘’(Xğ‘–)âˆ’xË†ğ‘–||2 2, (20)
identities,witheachsequencelastingaround10s.Thisdynamic
testsetwillbeusedtoexaminethemodelâ€™sgeneralizationtoun- whereXğ‘– representstheğ‘–-thlandmarkoutofthe ğ‘ ğ‘™ landmarks
sampledfromğœ•Î©0 ,andxË†ğ‘– indicatesthecorrespondingground
seenexpressionblendweightvectors.Thisisvaluableforanimation skin
truth2Dlandmarkpositioninscreenspace.Toobtaintheground
retargetingpurposes.
truthlandmarksweemployarecenthigh-qualitydenselandmark
Timing.ThedetailedtimingresultsarepresentedinTable(1).Dur- detector [Chandran et al. 2023] and predict approximately 8000
ingthetrainingphase,theaveragedurationperiterationisapprox- landmarksdistributedontheface(pleaserefertothesupplemental
imately0.5seconds.Inthetestingphase,foreachframe,thelatent videoforanillustration).Inthisscenario,thereisnogroundtruth
spaceoptimization(Optim.)requiresabout7seconds,whereasthe neutralscantoobtainapredictedboneshapefortheboneshape
generationoftheinputphysicalconstraints(PhyCons.)takesap- lossL inEqn.(16).However,wecanreformulateL asa
bone bone
proximately0.05seconds,andthesimulationprocess(Sim.)takes self-supervisedbonelossusingtheestimatedskintopredictthe
around2.6seconds.Thesetimingexperimentswereconductedona bones:
systemequippedwithasingleRTXA6000GPUanda16-coreCPU.
W6 enR oE wSU deL mTS
onstrateresultsandapplicationsofourgeneralized
L bone(ğœ•Î©ğ‘ bones)=âˆ‘ï¸ ğ‘–ğ‘ =ğ‘
1
ğ‘1 ğ‘||Nğ‘(Xğ‘ ğ‘–)âˆ’P(Nğ‘(ğœ•Î©ğ‘ skin))(Xğ‘ ğ‘–)||2 2, (21)
physicalfacemodel,startingwithfittingtounseendata(Sec.6.1),
showcasingthebenefitofhavingaphysicalmodelbyillustrating where P represents the SCULPTOR predictor model, evaluated
physicaleffects(Sec.6.2),facialanimationretargeting(Sec.6.3), ontheregressedneutralfacialskinNğ‘(ğœ•Î©ğ‘ ).Fittingresultsare
skin
andidentitygeneration/blendingthroughlatentspaceinterpolation showninFig.1andFig.4.Wesystematicallyevaluateavarietyof
(Sec.6.4).Pleaserefertothesupplementalvideoforamorevivid imageswithdifferingresolutionsandlightingconditions.While
visualization. not as accurate as fitting to a ground truth 3D scan, the image
fittingresultsshowcaseourabilitytopreservecriticalfacialfeatures
6.1 FittingtoUnseenData whileprovidingthemostflexibleandeasy-to-employversionofour
model.
Theprimarybenefitofourfacemodelisitsabilitytofittounseen
Forallfittingresults,eithertoscanortoimage,oncetheparame-
identitiesautomatically,alleviatingtheexpensiveburdenofcreating
tersarefitthenthemodelcanbeanimatedasillustratedinFig.1
anidentity-specificmaterialspacebyhandandtraininganidentity-
andthesupplementalvideo.
specificactuationnetwork.Fittingproceedsbyoptimizingforthe
identityandexpressionlatentcodes(ğ›½,ğ›¾)usingEqn.(19).Here,the
6.2 PhysicalEffects
skinlossL andbonelossL areadaptedbasedonthetype
skin bone
ofdatabeingfit.Weillustratefittingtotwodifferentmodalities: Ourmodelallowsthesimulationofphysicaleffects,whichisoneof
fittingtoasingle3Dfacescanandfittingtoasinglefaceimage. thekeybenefitsofusingphysically-basedanimation.8 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
0 7mm
Input Actuations Bones Skin Error Input Actuations Bones Skin
Fig.3. Modelfittingtoa3Dscan.Fiveexamplesareshownfordifferent Fig.4. Modelfittingtoasinglefaceimage.Fivedifferentidentitiesand
identitiesanddifferentexpressions.Afterfitting,thepredictedactuations expressionsareshown.Afterfitting,thepredictedactuationsandbones
andbonesallowtosimulatethefinalfacialskinmesh,whichmatchesthe allowtosimulatethefinalfacialskinmesh.Theresultisananimatable
inputwithaverylowerror. physicalmodelfromaverylightweightinput.
CollisionHandling.Fig.5illustratesourmodelâ€™scapacitytoac-
Gravity.Ourmodelcansimulatetheeffectsofgravity.Fig.8shows
curatelydetectandresolvecollisions,includinglip-lipandbone-lip
thiseffectwherewerotatetheheadtodifferentorientationsand
penetrations,acommonchallengeinfacialanimation.
thesofttissueisnaturallypulledinthecorrespondingdirection.
Paralysis.Byadjustingmuscleactuationparameters,ourmodel
canreplicatesomedegreesoffacialparalysisasillustratedinFig.6. 6.3 Retargeting
Thisexampledemonstratesthemodelâ€™ssensitivityandprecisionin
Ourmodelcanbeusedforphysics-basedanimationretargeting,
depictingsubtlephysiologicalchanges.
where we transfer facial animations between identities without
BoneReshaping.Ourphysicalmodelcansimulatevariouscran- interpenetration(seeFig.9).Thisisaccomplishedbychangingthe
iofacialeffectslikeosteotomy,illustratedinFig.7wherethejaw identitylatentcodetoatargetsubjectwhilekeepingtheexpression
bonehasbeenscaleddown,showingthecomparativeanalysisbe- codeofthesourcesubject.Thisapplicationconfirmsourmodelâ€™s
tweenpre-andpost-treatmentstatesandhighlightingourmodelâ€™s abilitytomaintainrealismandphysicalintegritywhileproducing
effectivenessinrepresentingandadaptingtoskeletaldeformations. identity-specificfacialexpressionsthatmatchasourceinput.LearningaGeneralizedPhysicalFaceModelFromData â€¢ 9
Animation Samples
Sources Target Identities
Fig.5. Collisionhandling.Ourphysicalmodelisabletoaccuratelydetect
andresolveinterpenetratinggeometries. Fig.9. Ourphysicalmodelisabletotransferfacialanimationsbetween
differentidentitieswhilebeinginterpenetration-free.
Facial Actuation Animation Samples
Fig.6. Paralysis.Ourphysicalmodelcansimulatesomedegreesoffacial
paralysis,demonstratingthemodelâ€™ssensitivityandprecisionindepicting Fig.10. Ourphysicalmodelisabletosmoothlyinterpolatebetweendiffer-
subtlephysiologicalchanges. entidentitieswhilepreservingphysicalplausibility,suchascollision-free
properties.
Fig.10.Foreachnovelidentity,themodelcanbeevaluatedwith
differentexpressioncodestoobtainphysics-basedfacialanimation.
7 EVALUATION
We evaluate our model from multiple perspectives numerically,
includingreconstructionaccuracy,jawrigidity,skullfixationand
anatomicalfidelity.Forreconstructionaccuracy,weadoptfourtypes
Bones Animation Samples ofmetrics,including3Dvertex-to-vertexerror(V2V),3Dscan-to-
mesherror(S2M),F-Score,andnormalerror.Finally,weevaluate
Fig.7. Bonereshaping.Ourphysicalmodelcansimulatevariouscraniofa-
anatomicalfidelityintermsofthebonefidelityandthebone-skin
cialeffectslikeosteotomy,wherethejawhasbeenscaleddownandthe
penetration.Thespecificdetailsofeachmetricareelaboratedas
actuationsarereplayedontheeditedanatomy.
follows:
â€¢ Vertex-to-vertex Error (V2V) measures the average of
theEuclideandistancesbetweentheground-truthandthe
reconstructionvertices.
â€¢ Scan-to-meshError(S2M)measurestheaverageofthe
Euclideandistancesbetweentheground-truthverticesand
thereconstructedmeshsurface.
Fig.8. Gravity.Weshowtheeffectofgravityonastabilizedfacewith â€¢ F-Scoreevaluatesthereconstructionqualityfromthepoint
differentheadorientations(inset). cloudaspect.Itharmonizestherecallandtheprecisionby
computingtheirharmonicmean.AhighF-Scoreisindica-
tiveofareconstructionthatisbothaccurateandcomplete.
6.4 LatentSpaceInterpolation
Wesample32kpointsintotalanduse1mmastheerror
Anotherapplicationofourgeneralizedmodelisthatwecansample threshold.
newidentitiesfromthelatentspace.Wedemonstratethisbyinter- â€¢ NormalErrormeasurestheaverageofthecosinedistances
polatingbetweentwodifferentidentitiesfromourtrainingsetin betweentheground-truthandthereconstructionnormals.
noisilloC
o/w
noisilloC
/w
sisylaraP
o/w
sisylaraP
/w
gnipahseR
o/w
gnipahseR
/w
gnildnaH
gnildnaH10 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
Table2. Numericalevaluationofdifferentsimulationresolutions.
Res.(mm) V2Vâ†“ S2Mâ†“ F-Scoreâ†‘ Normalâ†“
6.8 1.4679 0.6615 0.7346 0.0178
4.5 1.1270 0.5466 0.8033 0.0157
3.0 0.9469 0.4892 0.8427 0.0146
2.0(Ours) 0.8975 0.4721 0.8546 0.0143 7mm
1.3 0.8848 0.4666 0.8583 0.0142
0
0 (Disp) 0.8642 0.4532 0.8672 0.0141
V2V Metric F-Score Metric
0 mm (disp) 1.3 mm 2.0 mm 3.0 mm 4.5 mm 6.8 mm
Fig.12. Qualitativecomparisonofdifferentsimulationresolutions.
Fig.11. CumulativecurvesofV2VandF-Scoremetricsfromdifferentsimu-
lationresolutions.
â€¢ JawRigidityquantifieshowrigidlythenetworkmovesthe
jaw,basedonV2Vmetric(seeEqn.(12)).
â€¢ SkullFixationquantifieshowwellthenetworkfixesthe
skull,basedonV2Vmetric(seeEqn.(13)).
â€¢ BoneFidelityquantifieshowwellthenetworkpreserves
theSCULPTORbonespace,basedonV2Vmetric(seeEqn.(16)).
â€¢ PenetrationPairscountsthepenetrationbetweenthebone
andtheskinmeshes.Weusetheedge-trianglepair.
ModelAccuracy.Wefirstevaluateourmodelaccuracyintermsof
fittingaccuracy.Specifically,wefitourmodeltothestatictestset 7mm
byoptimizingourlatentcodesscanbyscan,usingL test(Eqn.(19)). 0
Ours Ours w/o Ours w/o Ours Ours w/o Ours w/o
Wethenrunthediscretizationandthesimulationtogettheresults
forcalculatingthemetrics.Table(2)reportstheaccuracyofour
Fig.13. Qualitativecomparisonofdifferentmethods.
modelwithdifferentdiscretizationresolutions.Thediscretization
resolutionlargelyimpactsthesimulationaccuracy.Thehigherthe
resolutionis,themoreaccuratethesimulatedresultsare.Weinclude
animationretargeting.Wealsoreportthejawrigidityandskullfixa-
arowforevaluatingthepuredisplacementoutputofthenetwork,
tioninbothdatasets,provingthattheconstraintsarewellenforced
labelledâ€œ0(Disp)â€,whichhasthelowesterrorasthisisthetarget
(seeTable(3)).
ofthetrainingobjective.Withadiscretizationresolutionofaround
2mmwecanachievecomparableaccuracywithourdisplacement Ablation. There are two main integral loss terms in our learn-
outputs,substantiatingthatoursimulation-freelearningframework ingframework,therigiditylossL (Eqn.(12))andthesoftloss
rigid
iseffectiveininferringplausiblephysicalconstraintsusedinsimula- L (Eqn.(14)).InTable(3)andFig.13,weshowtheablationstud-
soft
tion.Fig.11plotsthecumulativecurvesonV2VandF-Scoremetrics. iesofthereconstructionsdenotedas"Oursw/oL "and"Ours
rigid
Fig.12furtherdemonstratesthatoursimulationresultscanachieve w/o L " to assess the importance of each loss term. For both
soft
highreconstructionquality. thequantitativeandqualitativemeasurements,removingoneloss
Tofurtherevaluateourmodelintermsofanimationquality,we termresultsinasevereperformancedecrease,whichconfirmsboth
testourmodelonthedynamicset,wherewedirectlyusetheunseen losstermscontributetohigherreconstructionaccuracyandbetter
expressionblendweightvectorstoanimatetheseenidentitieswith- physicalplausibility.Specifically,L helpstoenforcetherigid
rigid
outanyoptimization.Table(3)showsthatthemodelgeneralizes movementofthebone,givingbetterjawrigidityandskullfixation
welltotheunseenexpressionblendweights,pavingthewayfor metrics.Thisleadstobetteraccuracysincethelearnedactuation
noitalumiS
rorrE
snoitautcA
noitalumiS
snoitautcA
senoB
rorrELearningaGeneralizedPhysicalFaceModelFromData â€¢ 11
Table3. Numericalevaluationofdifferentcomponentsonstaticanddynamictestsets.Weturnonthelatentspaceoptimizationforstaticsetwhileturnoffit
fordynamicset.
Methods V2Vâ†“ S2Mâ†“ F-Scoreâ†‘ Normalâ†“ JawRig.â†“ SkullFix.â†“
Oursw/oL 0.8942 0.4702 0.8560 0.0143 0.1378 0.0724
bone
Oursw/oL 1.2891 0.6758 0.7552 0.0158 2.2540 0.1145
rigid
Oursw/oL 0.9282 0.5077 0.8567 0.0158 0.0460 0.0647
soft
Oursw/oL 0.9178 0.4804 0.8486 0.0144 0.1338 0.0764
lip
Ours 0.8975 0.4721 0.8546 0.0143 0.1349 0.0785
Oursw/oL 0.7237 0.3182 0.9396 0.0070 0.0883 0.0557
bone
Oursw/oL 1.0632 0.4918 0.8407 0.0077 2.1084 0.0866 rigid
Oursw/oL 0.7815 0.3723 0.9059 0.0086 0.0296 0.0400
soft
Oursw/oL 0.7751 0.3399 0.9270 0.0071 0.0915 0.0612
lip
Ours 0.7248 0.3235 0.9354 0.0070 0.0915 0.0638
0 5mm
Error Animation Samples Ours Ours w/o SCULPTOR
Fig.14. QualitativecomparisonwithFLAME.Ontheleft,weshowthe Fig.15. Qualitativecomparisonofdifferentmethodsintermsofanatomy.
fittingerrormaps.Ontheright,weshowtheanimationsampleswiththe
mouthcut-awayonthesideofeachframe.
Table(4)indicatethatourmethodattainsaccuracyonparwith
mechanismsaremorecompatiblewiththerigidjawkinematicsthat
FLAMEacrossbothdatasets.Asignificantadvantageofourmodel,
arestrictlyenforcedduringsimulation.L givesbirthtotwomer-
soft however,liesinitsabilitytosupportadditionalphysicaleffects,
its.First,ithelpsthemodellearnplausiblesofttissuedeformation
suchascollisionhandlingâ€”areaswheretraditionalmethodsoften
(seeActuationsinFig.13).Second,itbuildstheconnectionbetween
falter.Fig.14providesaqualitativecomparisontohighlightthis
theboneandtheskin,thereforebeingabletograduallydragthejaw
capability.
tothephysicallyplausibleplacesduringtraining.Therefore,"Ours
Inaddition,weconductacomparisonofourmodelwithSCULP-
w/oL "failstoinferthejawkinematicsandalwaysproduces
soft TOR,specificallyfocusingontheissueofbone-skinpenetration.
fixedjawposition(seeBonesinFig.13).Thisiswhyitsjawrigidity
OurexperimentsrevealthatfittingSCULPTORtoaneutralscanfre-
metricsandskullfixationarebetterthanourfullmodel(Table(3)).
quentlyresultsinbone-skinpenetrationproblems.However,thanks
WealsoexaminetheeffectivenessofourboneshapelossL
bytraininganothermodelwithoutit,denotedas"Oursw/oL
bone
".
toourelasticregularization(L ereg),ourmodeleffectivelymitigates
bone theseissues.Atthesametime,itmaintainscomparableboneshape
Table(5)andFig.15showthatL canbetterconstraintheoverall
bone (seeFig.15andTable(5)).
boneshape.Finally,ourLipschitzregularizationL leadstobetter
lip Finally,ourmethodisstillapplicableinYangetalâ€™sscenariowhen
reconstructionaccuracywithoutspoilingothermetrics(see"Ours
thematerialspaceisprovided[Yangetal.2023].Toshowthis,we
w/oL "vs."Ours"inTable(3)andTable(5)).
lip integrateouruniquelossfunctionsâ€”L ,L ,andL â€”into
skin rigid fix
Comparison. As the first generalized physical face model, we Yangetal.â€™smodelarchitecture,whilestilladheringtotheirestab-
benchmarkourmethodagainstthecloselyrelatedFLAMEmodel, lishedregularizationandtrainingprotocols.Table(6)showsthe
focusingonfittingaccuracy.RecognizingthatFLAMEoptimizes evaluationontheirtestset.Ourmethodnotonlyachievescompa-
usingscan-to-meshdistance,weadoptthesamemetric,replacing rableaccuracy,butitalsoofferssignificantlyimprovedscalability.
thevertex-to-vertexenergyinourfittingschemewithscan-to-mesh Forinstance,whereasYangetal.requiredsixGPUsandnearly2
distance,toensureanequitablecomparison.Beyondourstatictest daystotrainmodelsforsixidentities,ourmethodologycanbeef-
set,wehavecompiledanadditionaltestsetofcomparablesizefrom ficientlyimplementedonasingleGPU,enablingtrainingforover
theFACESCAPEdataset[Yangetal.2020b].Resultspresentedin 300identitiesinjustoneday.
EMALF
sruO
citatS
cimanyD
1
esaC
2
esaC12 â€¢ L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
Table4. Scan-to-meshdistanceofdifferentmodelsondifferentdatasets. REFERENCES
MichaelBao,MatthewCong,StÃ©phaneGrabli,andRonaldFedkiw.2018.High-Quality
Datasets Ours* FLAME-100 FLAME-300
FaceCaptureUsingAnatomicalMuscles.ProceedingsoftheIEEE/CVFConference
OurDataset 0.3487 0.4858 0.4318 onComputerVisionandPatternRecognition(CVPR)(122018). http://arxiv.org/abs/
1812.02836
FaceScape 0.4713 0.6355 0.5596 VolkerBlanzandThomasVetter.1999. Amorphablemodelforthesynthesisof3D
faces.InSiggraph,Vol.99.187â€“194.
ZenghaoChai,HaoxianZhang,JingRen,DiKang,ZhengzhuoXu,XuefeiZhe,Chun
Yuan,andLinchaoBao.2022.REALY:RethinkingtheEvaluationof3DFaceRecon-
Table5. Anatomicalevaluationofdifferentmodelsonneutralscans.
struction.InProceedingsoftheEuropeanConferenceonComputerVision(ECCV).
PrashanthChandran,DerekBradley,MarkusGross,andThaboBeeler.2020.Semantic
Methods PenetrationPairsâ†“ BoneFidelityâ†“ DeepFaceModels.In2020InternationalConferenceon3DVision(3DV).IEEE,345â€“354.
https://doi.org/10.1109/3DV50981.2020.00044
Oursw/oL bone 0 1.9979 PrashanthChandran,LoÃ¯cCiccone,MarkusGross,andDerekBradley.2022. Local
Oursw/oL 0 1.3124 Anatomically-ConstrainedFacialPerformanceRetargeting.ACMTrans.Graph.41,
lip 4,Article168(jul2022).
Ours 0 1.3164 PrashanthChandranandGaspardZoss.2023.AnatomicallyConstrainedImplicitFace
Models. arXiv:2312.07538[cs.GR]
Sculptor 2628 0 PrashanthChandran,GaspardZoss,PauloGotardo,andDerekBradley.2023.Continu-
ousLandmarkDetectionWith3DQueries.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition.16858â€“16867.
ByungkukChoi,HaekwangEom,BenjaminMouscadet,StephenCullingford,KurtMa,
Table6. QuantitativeComparisonwith[Yangetal.2023].
StefanieGassel,SuziKim,AndrewMoffat,MillicentMaier,MarcoRevelant,Joe
Letteri,andKaranSingh.2022.Animatomy:AnAnimator-Centric,Anatomically
Metric Yangetal Ours InspiredSystemfor3DFacialModeling,AnimationandTransfer.InSIGGRAPHAsia
2022ConferencePapers.AssociationforComputingMachinery,Article16,9pages.
V2V 0.3849 0.3813
HangDai,NickPears,WilliamSmith,andChristianDuncan.2019.StatisticalModeling
ofCraniofacialShapeandTexture.InternationalJournalofComputerVision(2019).
RadekDanecek,MichaelJ.Black,andTimoBolkart.2022.EMOCA:EmotionDriven
MonocularFaceCaptureandAnimation.InConferenceonComputerVisionand
8 CONCLUSION PatternRecognition(CVPR).20311â€“20322.
BernhardEgger,WilliamA.P.Smith,AyushTewari,StefanieWuhrer,MichaelZollhoe-
Wepresentanewmodelforphysics-basedfacialanimationthatis fer,ThaboBeeler,FlorianBernard,TimoBolkart,AdamKortylewski,SamiRomdhani,
ChristianTheobalt,VolkerBlanz,andThomasVetter.2020. 3DMorphableFace
trainedonrealdataofhundredsofidentitiesperformingvarious
Modelsâ€”Past,Present,andFuture.ACMTransactionsonGraphics39,5(102020),
expressions,andassuchisextremelygeneralizableandcanbefit 1â€“38. https://doi.org/10.1145/3395208
tonewunseenidentitiesatruntime.Asaresult,weproposeavery YaoFeng,HaiwenFeng,MichaelJ.Black,andTimoBolkart.2021.LearninganAni-
matableDetailed3DFaceModelfromIn-the-WildImages.ACMTransactionson
convenientwaytogenerateactor-specificphysicalfaceanimation
Graphics(ToG),Proc.SIGGRAPH40,4(Aug.2021),88:1â€“88:13.
withoutanymanualmodelsetup.Thisispossibleduetoourtwo PanagiotisP.Filntisis,GeorgeRetsinas,FoivosParaperas-Papantoniou,AthanasiosKat-
maincontributions:anapproachforsimulation-freelearningwhere samanis,AnastasiosRoussos,andPetrosMaragos.2022. VisualSpeech-Aware
Perceptual3DFacialExpressionReconstructionfromVideos. arXivpreprint
neuralnetworksaretrainedtoproducedeformationsthatarecom- arXiv:2207.11094(2022).
patiblewithphysicalsimulationbutwithoutrequiringsimulation BarisGecer,StylianosPloumpis,IreneKotsia,andStefanosZafeiriou.2019.GANFIT:
GenerativeAdversarialNetworkFittingforHighFidelity3DFaceReconstruction.
inthetrainingloop,andamaterialspacemorphingmethodthatcan
InTheIEEEConferenceonComputerVisionandPatternRecognition(CVPR).
predictactor-specificskin,bonesandsoft-tissuevolumesautomati- BarisGecer,StylianosPloumpis,IreneKotsia,andStefanosPZafeiriou.2021. Fast-
cally.Thesecontributionsarethekeytobeingabletotrainonsuch GANFIT:GenerativeAdversarialNetworkforHighFidelity3DFaceReconstruction.
IEEETransactionsonPatternAnalysisandMachineIntelligence(2021).
alargedatasetefficiently,providingthegeneralizabilityneededfor
AurelGruber,MarcoFratarcangeli,GaspardZoss,RomanCattaneo,ThaboBeeler,
fittingtonewidentities. MarkusGross,andDerekBradley.2020. InteractiveSculptingofDigitalFaces
Intermsoflimitations,wenotethatwedonotattempttoaccu- UsinganAnatomicalModelingParadigm.ComputerGraphicsForum(2020),93â€“102.
https://doi.org/10.1111/cgf.14071
ratelymodeltheinsideofthemouth,inpartbecausethetraining Alexandru-EugenIchim,PetrKadleÄek,LadislavKavan,andMarkPauly.2017.Phace:
datasetdoesnotaccuratelytrackthisregion.Assuch,weignore Physics-basedFaceModelingandAnimation.ACMTransactionsonGraphics36,4
(72017),1â€“14. https://doi.org/10.1145/3072959.3073664
theteethregionontheanatomymodel.Also,whileweendeavor
GergelyKlÃ¡r,AndrewMoffat,KenMuseth,andEftychiosSifakis.2020.ShapeTargeting:
tocreateconstraintsthatproducephysically-accurateanimations, AVersatileActiveElasticityConstitutiveModel.InACMSIGGRAPH2020Talks
thereisnoguaranteethatthelearnedactuationsarebiologically (VirtualEvent,USA)(SIGGRAPHâ€™20).AssociationforComputingMachinery,New
York,NY,USA,Article59,2pages. https://doi.org/10.1145/3388767.3407379
accurate,especiallyforunseenidentitiesthatarefarfromtheones
TianyeLi,TimoBolkart,Michael.J.Black,HaoLi,andJavierRomero.2017.Learninga
seenattrainingtime. modeloffacialshapeandexpressionfrom4Dscans.ACMTransactionsonGraphics,
Nevertheless,wedemonstratethesuccessofourtrainedmodel (Proc.SIGGRAPHAsia)36,6(2017).
AraceliMorales,GemmaPiella,andFedericoM.Sukno.2020. Surveyon3Dface
withveryplausibleapplicationsoffittingtonewfacescans,fittingto reconstructionfromuncalibratedimages.Comput.Sci.Rev.40(2020),100400.
faceimages,animatingwithphysicaleffects,animationretargeting ChristopherOtto,PrashanthChandran,GaspardZoss,MarkusGross,PauloGotardo,
andDerekBradley.2023.APerceptualShapeLossforMonocular3DFaceRecon-
andidentityinterpolation.Weadditionallyprovideadetailedevalu-
struction. arXiv:2310.19580[cs.CV]
ationofourmethod.Wehopethatourworkwillhelptodemocratize ZesongQiu,YuweiLi,DongmingHe,QixuanZhang,LongwenZhang,YinghaoZhang,
physics-basedfacialanimationformanyapplications. JingyaWang,LanXu,XudongWang,YuyaoZhang,etal.2022.SCULPTOR:Skeleton-
consistentfacecreationusingalearnedparametricgenerator.ACMTransactionson
Graphics(TOG)41,6(2022),1â€“17.
ACKNOWLEDGMENTS
EftychiosSifakis,IgorNeverov,andRonaldFedkiw.2005.Automaticdeterminationof
facialmuscleactivationsfromsparsemotioncapturemarkerdata.ACMTransactions
TheworkissupportedbytheSwissNationalScienceFoundation onGraphics24,3(72005),417â€“425. https://doi.org/10.1145/1073204.1073208
underGrantNo.:200021_197136.LearningaGeneralizedPhysicalFaceModelFromData â€¢ 13
SangeethaGramaSrinivasan,QisiWang,JuniorRojas,GergelyKlÃ¡r,LadislavKavan, VisionandPatternRecognition(CVPR).
andEftychiosSifakis.2021.Learningactivequasistaticphysics-basedmodelsfrom HaotianYang,HaoZhu,YanruWang,MingkaiHuang,QiuShen,RuigangYang,and
data.ACMTransactionsonGraphics40,4(82021),1â€“14. https://doi.org/10.1145/ XunCao.2020b.Facescape:alarge-scalehighquality3dfacedatasetanddetailed
3450626.3459883 riggable3dfaceprediction.InProceedingsoftheieee/cvfconferenceoncomputer
NicolasWagner,MarioBotsch,andUlrichSchwanecke.2023. SoftDECA:Compu- visionandpatternrecognition.601â€“610.
tationallyEfficientPhysics-BasedFacialAnimations.InProceedingsofthe16th LingchenYang,ByungsooKim,GaspardZoss,BaranGÃ¶zcÃ¼,MarkusGross,andBarbara
ACMSIGGRAPHConferenceonMotion,InteractionandGames(MIGâ€™23).Asso- Solenthaler.2022.Implicitneuralrepresentationforphysics-drivenactuatedsoft
ciation for Computing Machinery, New York, NY, USA, Article 11, 11 pages. bodies.ACMTransactionsonGraphics(TOG)41,4(2022),1â€“10.
https://doi.org/10.1145/3623264.3624439 LingchenYang,GaspardZoss,PrashanthChandran,PauloGotardo,MarkusGross,
ChengleiWu,DerekBradley,MarkusGross,andThaboBeeler.2016.Ananatomically- BarbaraSolenthaler,EftychiosSifakis,andDerekBradley.2023.AnImplicitPhysical
constrainedlocaldeformationmodelformonocularfacecapture.ACMTransactions FaceModelDrivenbyExpressionandStyle.InSIGGRAPHAsia2023Conference
onGraphics35,4(72016),1â€“12. https://doi.org/10.1145/2897824.2925882 Papers.Article106,12pages.
MingXuandJamesYang.2015.Humanfacialsofttissuethicknessandmechanicalprop- TiankeZhang,XuangengChu,YunfeiLiu,LijianLin,ZhendongYang,ZhengzhuoXu,
erties:aliteraturereview.InInternationalDesignEngineeringTechnicalConferences ChengkunCao,FeiYu,ChangyinZhou,ChunYuan,andYuLi.2023.Accurate3D
andComputersandInformationinEngineeringConference,Vol.57045.American FaceReconstructionwithFacialComponentTokens.InProceedingsoftheIEEE/CVF
SocietyofMechanicalEngineers,V01AT02A045. InternationalConferenceonComputerVision(ICCV).9033â€“9042.
HaotianYang,HaoZhu,YanruWang,MingkaiHuang,QiuShen,RuigangYang,and WojciechZielonka,TimoBolkart,andJustusThies.2022. TowardsMetricalRecon-
XunCao.2020a.FaceScape:aLarge-scaleHighQuality3DFaceDatasetandDetailed structionofHumanFaces.InEuropeanConferenceonComputerVision.
Riggable3DFacePrediction.InProceedingsoftheIEEEConferenceonComputer