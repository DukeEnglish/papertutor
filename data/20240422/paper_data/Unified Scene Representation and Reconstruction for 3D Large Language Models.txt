Unified Scene Representation and Reconstruction
for 3D Large Language Models
Tao Chu1,2,‚àó, Pan Zhang2, Xiaoyi Dong2, Yuhang Zang2, Qiong Liu1,‚Ä†, and
Jiaqi Wang2
1 South China University of Technology
liuqiong@scut.edu.cn
2 Shanghai AI Laboratory
{chutao,zhangpan,dongxiaoyi,wangjiaqi}@pjlab.org.cn
Abstract. Enabling Large Language Models (LLMs) to interact with
3Denvironmentsischallenging.Existingapproachesextractpointclouds
either from ground truth (GT) geometry or 3D scenes reconstructed by
auxiliary models. Text-image aligned 2D features from CLIP are then
liftedtopointclouds,whichserveasinputsforLLMs.However,thisso-
lution lacks the establishment of 3D point-to-point connections, leading
to a deficiency of spatial structure information. Concurrently, the ab-
senceofintegrationandunificationbetweenthegeometricandsemantic
representationsofthesceneculminatesinadiminishedlevelof3Dscene
understanding. In this paper, we demonstrate the importance of having
a unified scene representation and reconstruction framework, which is
essentialforLLMsin3Dscenes.Specifically,weintroduceUni3DR2 ex-
tracts 3D geometric and semantic aware representation features via the
frozen pre-trained 2D foundation models (e.g., CLIP and SAM) and a
multi-scale aggregate 3D decoder. Our learned 3D representations not
only contribute to the reconstruction process but also provide valuable
knowledge for LLMs.Experimental results validate that our Uni3DR2
yieldsconvincinggainsoverthebaselineonthe3Dreconstructiondataset
ScanNet (increasing F-Score by +1.8%). When applied to LLMs, our
Uni3DR2-LLM exhibits superior performance over the baseline on the
3D vision-language understanding dataset ScanQA (increasing BLEU-1
by+4.0%and+4.2%onthevalsetandtestset,respectively).Further-
more, it outperforms the state-of-the-art method that uses additional
GT point clouds on both ScanQA and 3DMV-VQA.
Keywords: 3D Reconstruction ¬∑ 3D Representation ¬∑ Large Language
Models
1 Introduction
Recent Large Language Models (LLMs) [8,25,26,42] exhibit remarkable profi-
ciency in processing both 1D text and 2D images. A key factor contributing to
‚àóIntern at Shanghai AI Laboratory. ‚Ä†Corresponding author.
4202
rpA
91
]VC.sc[
1v44031.4042:viXra2 T. Chu et al.
(a) Baseline (3D-LLM)Point Cloud
Features
NeRF
SLAM Depth Lift
Raw Images LLM
(b) Ours
Representation & Reconstruction
Fig.1:Comparisonbetweentherepresentationfor3DLLMs.(a)Theprevious
baseline [15] is isolated and complex, which requires extra NeRF, SLAM models, or
depth to extract the point cloud and then lifts features to the 3D representations.
(b) By contrast, our Uni3DR2 is unified and neat. We unified learn geometric and
semantically rich volumetric representation with high-quality reconstruction as LLM
inputs.OurlearnedrepresentationandreconstructionsignificantlyenhancetheLLM‚Äôs
performance in 3D environments.
their notable success is the LLMs‚Äô capability to learn comprehensive represen-
tations from extensive sets of language and visual data. However, LLMs face
challenges with representation learning in 3D environments, which are unstruc-
tured and inherently complex. Directly using a sequence of 2D image inputs
for 2D LLMs fails to yield satisfactory results in the 3D world [15]. Therefore,
learning high fidelity and accuracy of 3D representations could enable more in-
tuitive and versatile AI systems that understand and interact with the physical
3D world in a more human-like manner.
A recent advancement 3D-LLM [15] marks the inaugural attempt to utilize
point clouds as representations for 3D LLMs, as illustrated in Fig. 1 (a). How-
ever,3D-LLMgeneratesrepresentationsofflinethroughanisolatedandcomplex
process, lifting 2D features to point clouds extracted from input depth, ground
truth(GT)geometry,or3Dscenesreconstructedbyauxiliarymodels.Whileus-
ing GT point clouds as input can yield commendable performance for 3D-LLM,
the acquisition of such GT data is both expensive and challenging. Conversely,
depending on the geometry predicted by the reconstruction models as input to
3D-LLM has demonstrated subpar performance, as discussed in Sec. 4.6. This
performance discrepancy arises because 3D-LLM relies solely on point clouds
to provide spatial positions without establishing 3D point-to-point connections
or fusing 3D geometric and semantic information. Consequently, the 3D rep-
resentations exhibit sparse scene connectivity and a lack of spatial structural
information, causing 3D-LLM‚Äôs performance to be notably affected by the qual-
ity of the scene geometry. These limitations highlight the necessity for furtherUni3DR2 3
advancements in 3D representation design, which can effectively bridge the gap
between LLMs and complex 3D environments.
Tothisend,wepresentUni3DR2 (Fig.1(b)),aneatandunifiedscenerepre-
sentationandreconstructionmoduleforLLM.AndUni3DR2-LLMaddressesthe
substantialimpactofpredictedgeometryon3D-LLM,achievingresultsthatsur-
passtheperformanceof3D-LLMwhichusesGTpointcloudsasinputs.Specifi-
cally,Uni3DR2 comprisesa2Dencoderanda3Ddecoderfor3Drepresentations,
and a reconstruction module.Unlike the previous reconstruction methods that
utilize trainable encoders solely for geometric features, our 2D encoder is com-
bined with two frozen vision foundation backbones. Notably, one backbone is
derived from SAM [22], pretrained on extensive sets of human-verified object
masks, for extracting object-level information. The other backbone is sourced
from CLIP [30], pretrained on massive image-text data, to capture semantically
rich features. Then a 3D decoder equipped with multi-scale Gated Recurrent
Unit (GRU) fusion is introduced for the generation of 3D geometric and seman-
tically rich representations from 2D features. Subsequently, a lightweight recon-
structionmoduleleveragesthese3Drepresentationstopredictprecisegeometry
results. Finally, the combined representation and reconstruction outcomes serve
asinputsforLLMstoaddress3Dvision-languagetasks.Incomparisontoprevi-
ous methods, our unified scene representation and reconstruction module excels
incapturingrichsemanticsandobjectdetailswithin3Dspaces,steeringclearof
the use of expensive GT point clouds or the performance limitations associated
with relying solely on predicted point clouds.
Extensive experiments show that our proposed method outperforms prior
competitiveapproachesonavailabledatasetsthatincludevideoinputs.Interms
of reconstruction, our Uni3DR2 attains a remarkable 1.8% improvement in F-
ScorecomparedtothebaselineonScanNet.Regarding3Dvision-languagetasks,
incomparisontothebaseline3D-LLM[15]withpredictedgeometry,ourUni3DR2-
LLM also exhibits significant 4.0% and 4.2% increases in BLEU-1 on the val
set and test set of ScanQA, respectively. Moreover, it outperforms the state-
of-the-art which uses additional GT point clouds. Specifically, our Uni3DR2-
LLM achieves a notable gain of 3.4% in overall accuracy on 3DMV-VQA and
a commendable improvement of 1.4% in BLEU-1 on the test set of ScanQA,
respectively.
2 Related Work
3D Vision and Language plays a pivotal role in enabling machines to com-
prehend 3D environments in applications like robotics [13,41] and embodied
AI[21,39].Severalrecent3Dtasks[1,2,4,5,14]evaluatethevisualandlanguage
understandingabilitiesof3Dmodels,suchasquestionanswering[2,14,46],dense
captioning [5], and grounding [1,4]. State-of-the-art algorithms commonly use
graph representation [12,18] or multimodal Transformer [3,6,44,45,50]. Re-
cent work 3D-LLM [15] combines 3D representations and large language models
(LLMs) [23] to solve 3D vision-language tasks. Specifically, 3D-LLM extracts4 T. Chu et al.
3D representations from 2D images via extra depth map, SLAM [20], or NeRF
model [37]. Different from 3D-LLM, our Uni3DR2 uses the volumetric represen-
tationandreconstructionfromasequenceofimages,whichismoreaccurateand
general in handling different scenes for LLMs. Our Uni3DR2 primarily focuses
on feature representation and reconstruction, which is orthogonal to previous
methods [17] and can be integrated together.
3D Reconstruction aims to reconstruct 3D scenes from input images, which
main contains depth-based methods [7,33,35] and volumetric methods [34,38].
SimpleRecon [35] is proposed to input metadata into the feature volume and
then follow a 2D encoder-decoder to predict depth. VolumeFusion [7] utilizes
differentialdepthfusionwithproposedPosedConvtooptimizemulti-viewdepth
maps.3DVNet[33]constructsamulti-scalevolumetricsceneencodingfromaset
ofestimateddepthmapinputsandthenpredictsaresidualforthesedepthmaps.
PIFu [34] predicts the continuous inside/outside probability field to obtain the
3D occupancy. NeuralRecon [38] is proposed to predict a discrete TSDF volume
using GRU fusion. Depth-based reconstruction methods are prone to distur-
bancesindepthvalues,makingitchallengingtoobtainstablereconstructionre-
sults.Volumetricmethodsdemonstratestrongerstabilityandcanaccommodate
morefeatureinformationin3Dspace.However,previousvolumetricreconstruc-
tion approaches [34,38] usually rely on small, learnable image encoders, which
limited their ability to provide semantically rich 3D representations. Different
from previous methods, we use the frozen encoders from segmentation-anything
(SAM) [22] and CLIP [30] to extract semantically rich 3D representations.
2DVisionFoundationModelsarebuiltuponthepre-trainingtechniquethat
aimstotrainageneralmodelusingmassivedataandcanbefine-tunedeasilyin
differentdownstreamtasks.Forexample,CLIPmodel[30]ispre-trainedonlarge
amounts of image-text pairs with contrastive loss. SAM [22] model is another
examplethatispre-trainedonbillion-levelimageswithperceptionpromptssuch
as points, boxes, and segmentation masks. These vision foundation models con-
tain rich semantic information for downstream applications. Our Uni3DR2 uses
these foundation models as image encoders, extracting both rich geometric and
semantic features.
Large Language ModelsrepresentagroundbreakingadvancementinNatural
Language Processing, exhibiting exceptional performance across diverse tasks.
At the forefront of this technology is OpenAI‚Äôs GPT series [26,31,32]. These
models have set new standards in language understanding and generation, with
a profound impact on various applications. In addition , other notable LLMs
have emerged, such as PaLm [8], OPT [49] and LLaMa [42]. Notably, the field
has seen recent strides in the development of Multimodal LLMs, such as GPT-
4 [26], BLIP2 [23], PaLmE [10] and 3D-capable models 3D-LLM [15]. These
advancements mark a pivotal moment, extending the capabilities of LLMs be-
yondlanguageprocessingtoseamlesslyintegrateandinterpretbothtextualand
visual information. Our Uni3DR2-LLM is built upon BLIP2 [23] with a unified
representation and reconstruction framework for 3D environments.Uni3DR2 5
Input Representation & Reconstruction Large Language Model
Feature ùêû Feature ùê´
v1
E
sam Question
v2
Sample
LLM
‚Ä¶
E Feature ùêù2
clip QFormer
vt
Feature ùêù1
TSDF ùê†ùëõ
Answer
Fig.2: Overview of our Uni3DR2-LLM framework. Given video inputs,
Uni3DR2-LLM employs (1) a frozen encoder integrating SAM [22] and CLIP [30]
image encoders, followed by a decoder for 3D representations, (2) a light-weight
modulefocuson3D reconstruction,and(3)anLLMintegratedwithQFormer[23]
for 3D vision-language understanding. ( : frozen.).
3 Methodology
This section describes the framework of our Uni3DR2-LLM ‚Äî a unified model
understanding 3D scenes from 2D video inputs. Fig. 2 illustrates the framework
of Uni3DR2-LLM, which comprises a unified module for predicting 3D repre-
sentations r and reconstruction results g , and a Large Language Model for 3D
n
vision-language tasks. Unlike the previous method [15], which lifts extracted 2D
features into expensive GT point clouds or predicted geometry for LLMs, re-
sulting in generated representation with sparse scene connectivity and a lack of
spatial structural information, our proposed Uni3DR2 aims to establish 3D geo-
metric and semantically rich representations for both reconstruction and LLMs.
3.1 Unified Representation and Reconstruction (Uni3DR2)
GivenavideoV={v ,v ,...,v },ourUni3DR2 aimtomapitto3Drepresen-
1 2 t
tation features r and 3D reconstruction results g . Here, t serves as a temporal
n
sequenceidentifier,andv‚ààRH√óW√ó3 representsanimageframe,withH andW
denotingtheimageheightandwidth,respectively.Specifically,thevideoframes
V are input to the 2D strong encoder E for object-level and semantically rich
features e, and e are then fed into the 3D decoder D for 3D point-to-point
connection and feature fusion. The decoder D generates 3D geometric and se-
mantically rich representations r. Finally, r is used to predict 3D geometry g
n
with a light-weight reconstruction module.
Encoder. Our encoder E is combined of two foundation pre-trained visual en-
coders: E from SAM [22] and E from CLIP [30]. Since E and E are
sam clip sam clip
pre-trained on the massive object-level masks and image-text corpus, respec-
tively, we first use them to extract object-level and semantically rich features:
e =E (V)‚ààRB√óC0√óH0√óW0,
sam sam
(1)
e =E (V)‚ààRB√óC1√óH1√óW1,
clip clip6 T. Chu et al.
wheresymbolsB,C,H,andW refertobatchsize,outputchannels,height,and
width, respectively. And then, concatenation (concat) and reshape (reshape)
operationsareusedtogetfinal2Dencoderfeaturese=concat(reshape(e ),
sam
reshape(e clip))‚ààRB√óC2√óH2√óW2. Here the output channel C
2
=C 0+C 1.
Unlike previous 3D reconstruction methods [19,38] where the encoders are
trainedforreconstruction,ourmethodintroducesthefrozenfoundationencoders
inreconstructionforthefirsttime.Theseencoderscannaturallyextract2Dfea-
tureswithrichinformation,enablingthemtoeffectivelycapturebothgeometric
and semantic details essential for 3D representations.
Decoder. We use a 3D decoder D to fuse the 2D features e into 3D repre-
sentations r. The decoder D builds the relationship of voxels in 3D space with
multi-scale and coarse-to-fine strategies. Specifically, the encoder feature e is
input into n levels of convolution operators. This process yields the multi-scale
features, denoted as ein ={ein,ein,...,ein}. For the i-th level,
1 2 n
ein =conv (e), (2)
i i
whereconvolutionconv isusedtoadjusttheinputsofthei-thlevelindecoder,
i
whose stride is 2(n‚àíi). And 3D decoder feature d of level i is derived as follows:
i
(cid:40)
m (backproj(ein)), if i=1,
d = i 1 (3)
i m (dfilter,backproj(ein)), otherwise,
i i‚àí1 i
where backproj is a backprojection function which lifts 2D features to the cor-
respondingcameraraywithin3Dspacewiththehelpofcamerapose.Weadopt
GRUFusion[38]followingSPVConv[40]asour3Dmodulem ,whichisusedto
i
fuse features of each voxel in 3D space at level i. The symbol dfilter = o √ód
i i i
represents the process of filtering d using the 3D occupancy o , as further de-
i i
tailedinEquation(4).Finally,the3Drepresentationsr=d areobtainedfrom
n
the last level of the 3D decoder D.
Theprevious3DLLMmethod[15]directlylifts2DfeaturestoGT/predicted
point clouds as 3D representations, resulting in sparse scene connectivity and a
lack of spatial structural information. In contrast, our 3D decoder tackles these
issues by establishing 3D point-to-point connections, fusing both geometric and
semantic information into 3D representations. Consequently, it enhances both
reconstruction and LLMs.
Reconstruction.Afterobtainingthe3Drepresentationsr,thesubsequentsteps
involve acquiring 3D reconstruction results g .The symbol g denotes to the
n n
Truncated Signed Distance Function (TSDF). For reconstruction, we predict
geometry in each level to filter the free regions in 3D space due to limited GPU
memory.Thereconstructionresultsatleveliaredenotedasg viaEquation(4):
i
g =o √óhtsdf(d ),
i i i i (4)
o =hocc(d ),
i i i
where htsdf and hocc represent the TSDF head and 3D occupancy head, respec-
i i
tively.Bothhtsdf andhoccarecomposedoflinearlayers.Thepredictedgeometry
i i
at level n is denoted as g , which is the reconstruction result of our model.
n
OurUni3DR2firsthighlightsthesignificanceofunifiedrepresentationandre-
construction for LLMs within 3D scenes. Unlike the previous method [15] whichUni3DR2 7
reliesonexpensiveGTpointcloudsorfacesperformancelimitationswhenlifting
featurestopredictedpointclouds,ourmethodexcelsbysimultaneouslypredict-
inggeometryanddelivering3Dgeometricandsemanticallyrichrepresentations.
3.2 Uni3DR2-LLM
Aftergettingrepresentationfeaturesrandreconstructionresultsg ,weexecute
n
a sampling operation guided by g to prepare 3D representations for LLM. The
n
sampled 3D representation features r, denoted as r , are obtained using the
s
random sampling function randsamp:
r =randsamp(r√ó(|g |<Œ¥),N). (5)
s n
Here, r
s
‚àà RB√óN√óC3, where N represents the fixed point number for the
randomsamplingfunction,andC isthechannelnumber.Thehyper-parameter
3
Œ¥ denotes the threshold distance from the TSDF surface, guiding the selection
of the sampled 3D representation features r .
s
Subsequently, we combine the sampled 3D representations r with the 3D
s
point coordinates c‚ààRB√óN√ó3 to serve as the inputs for the LLM. The coordi-
nates c refer to the points in r . To enhance spatial understanding and capture
s
location information, our position embeddings p are derived by concatenating
the position embeddings along each axis dimension:
p=concat{L (c ),L (c ),L (c )}. (6)
x x y y z z
Give the axis dimension j ‚àà {x,y,z}, the symbol c denotes the values
j
along axis j, and L refers to the embedding layer of axis j. The concatenation
j
operation concat is performed along the channel dimension.
Afterthat,the3Drepresentationfeaturesandpositionembeddingsarefused
through the expression:
r =proj (r )+proj (p), (7)
llm r s p
where proj refers to linear layers that adjust channel numbers. The results r
llm
constitutes the final feature inputs for the LLM.
Drawing inspiration from BLIP2 [23], we first feed our input features r
llm
into the QFormer model. Subsequently, the obtained features undergo process-
ing through LLM for dialogue generation. The 3D visual features for LLM are
computed as follows:
l =QFormer(r ). (8)
llm llm
Following this, we keep the 3D visual features l fixed and introduce ques-
llm
tions q to LLM, generating corresponding answers a through the expression:
a=LLM(q,l ). (9)
llm
In summary, our Uni3DR2-LLM uniformly predicts both scene representa-
tionsandreconstructions,feedingthese3Dgeometricandsemanticallyrichrep-
resentations into LLM for engaging dialogue with 3D scenes.
3.3 Loss
ThelossfunctionforUni3DR2-LLMisacombinationoftwocrucialcomponents:
the 3D reconstruction loss L and the LLM loss L , represented as:
rec llm
L=L +L . (10)
rec llm8 T. Chu et al.
ThereconstructionlossL isdesignedtoensureaccurate3Dreconstruction
rec
across multiple levels and is computed as follows:
n
(cid:88)
L = BCE(o ,ÀÜo )+L1(g ,gÀÜ ), (11)
rec i i i i
i=1
where BCE is binary cross-entropy loss, and L1 is L1 loss. The terms ÀÜo and gÀÜ
i i
correspond to the ground truth of 3D occupancy and TSDF in level i.
The LLM loss, L , captures the linguistic aspects by incorporating cross-
llm
entropy loss CE for the token identifier of each word:
L =CE(w,wÀÜ), (12)
llm
wherewdenotestothetokenidentifierofthewordinqanda,andwÀÜ represents
the corresponding ground truth.
4 Experiments
In this section, we conduct experiments on different tasks, including 3D recon-
struction (see Section 4.2) and 3D vision-language understanding (discussed in
Section 4.3 and Section 4.4). We compare our method with the state-of-the-art
methods on 3D vision-language understanding and provide an ablation study
(see Section 4.6) to highlight the effectiveness of each component.
4.1 Experiment Setup
Datasets.Oursettingexclusivelyacceptsvideoasinput,requiringexperiments
to be conducted on datasets that offer video or multi-view images. Therefore,
datasets featuring only point clouds as inputs, such as the Held-In Dataset in
3D-LLM [14], are unsuitable for our experimentation. Hence, we select the (1)
ScanNet [9] dataset for 3D reconstruction analysis. ScanNet includes 1,513
indoor scenes with videos, depths, and meshes for reconstruction, and we follow
the training/validation splits of Atlas [24]. We also select two representative 3D
vision-language datasets: (2) the ScanQA dataset [2] contains 41,363 questions
from 800 indoor scenes. (3) the 3DMV-VQA [14] dataset that encompasses
50k questions. The questions of 3DMV-VQA are designed to test models on
various aspects like concepts, counting, relational, and comparative analyses. In
summary, our dataset selections provide a well-rounded basis for evaluating the
effectiveness of our method in both 3D reconstruction and 3D vision-language
understanding capabilities.
Implementation Details. We use pre-trained image encoders from SAM [22]
and CLIP [30] as the foundational modules for our 2D encoding process. The
dimensionsofe arespecifiedas256channels,withaheightof64andawidth
sam
of 64. Similarly, e is characterized by 1408 channels, 36 in height, and 36 in
clip
width. The level number n of our 3D decoder is 3, with an input feature of 128
channels.FortheScanNetdataset,theinputscalesateachlevelofein aresetto
30√ó40, 60√ó80, and 120√ó160, while for 3DMV-VQA, the corresponding scales
are 32√ó32, 64√ó64, and 128√ó128. The output scales at each level of d areUni3DR2 9
Table 1: Reconstruction results on ScanNet for 3D reconstruction analysis. We
report standard 2D depth metrics defined in Eigen et al. [11] and the F-Score metric
for geometry.
Abs Rel ‚Üì Abs Diff ‚Üì RMSE ‚Üì F-Score ‚Üë
GPMVS [16] 0.130 0.239 0.472 0.304
MVDepthNet [43] 0.098 0.191 0.293 0.329
DPSNet [19] 0.087 0.158 0.232 0.344
COLMAP [36] 0.137 0.264 0.502 0.558
NeuralRecon [38] 0.065 0.106 0.195 0.562
Uni3DR2 (Ours) 0.060 0.094 0.182 0.580
definedas243,483,and963 foralldatasets.Allscenesaredividedintofragments
with 9 views each, and these fragments are then reconstructed sequentially, one
afteranother.RegardingtheLLM,weleveragepre-trainedQFormerandFlanT5
sourcedfromBLIP-2[23]toestablishtheinitialweights.Thenumbersofpoints
and channels of QFormer input r are 10,000 and 1408, respectively.
llm
Ourtrainingstrategyunfoldsintwodistinctstages.Initially,the2Dencoder
isfrozenwhilethe3Ddecoderistrainedtogenerategeometricandsemantically
rich3Drepresentationsandachieve3Dreconstruction.Thistrainingphasespans
50epochswithaninitiallearningrateof5e-3,concludingatafinallearningrate
of 1e-5, and adopts a cosine learning rate scheduler. The training is executed
withabatchsizeof2fragmentsacross16A100GPUs.Wetrainthe3Ddecoder
for ScanNet and 3DMV-VQA within 23.5 hours and 29 hours, respectively.
Inthesubsequentstage,the2Dencoder,3Ddecoder,andthemajorityofthe
FlanT5componentsremainfrozen,exceptfortheinputandoutputembeddings.
ThissettingisconducivetotrainingtheQFormerandLLMtogenerate3Dscene-
aware dialogue. In this phase, we extract 3D representations and reconstruct
wholescenesrelevanttoquestion-answerpairs.Thelearningschedulefortraining
theLLMmirrorsthatofthe3Ddecoder,withinitialandfinallearningratesset
at1e-4and1e-5,respectively.Thebatchsizeforthisstageis16question-answer
pairs, and the training is distributed across 16 A100 GPUs. We train LLM for
ScanQA and 3DMV-VQA within 12.5 hours and 33 hours, respectively.
4.2 3D Reconstruction on ScanNet
Baselines. We choose NeuralRecon [38] as the baseline for our reconstruction.
NeuralRecon is widely recognized for its simplicity and effectiveness in 3D re-
construction, providing seamless feature fusion through volumetric features. To
create our 3D geometric and semantic representation, we modify the encoder-
decoder framework of NeuralRecon. Furthermore, we compare our reconstruc-
tion model with several other competitive approaches. COLMAP [36] designs a
pixel-wiseviewselectionmechanism.MVDepthNet[43]usesanencoder-decoder
structure with geometric data augmentation. GPMVS [16] uses temporal infor-
mationbetweenframes.DPSNet[19]usesthetraditionalplanesweepalgorithm
for dense depth reconstruction.10 T. Chu et al.
RGB NeuralRecon Ours Ground Truth
Fig.3: 3D reconstruction visualization results on ScanNet. Compared to the
baseline method [38] (second column), our method (third column) predicts the recon-
struction results with more semantic details. (Zoom in for details.).
Metrics. We utilize the metrics introduced in Atlas [24] to evaluate our re-
construction results, including Abs Rel, Abs Diff, and RMSE for depth and
F-Score for geometry. These metrics represent the relative error, absolute error,
and mean-square error when comparing the depth rendered from predicted ge-
ometry to the ground truth depth. Additionally, the F-Score metric denotes to
the F1 score calculated for the the predicted point cloud in comparison to the
ground truth point cloud.
Results. As shown in Tab. 1, the results highlight the superior reconstruction
performanceofourUni3DR2,achievingnotableimprovementsonScanNetacross
all metrics compared to other competitive methods. In comparison to the base-
lineNeuralRecon,the2Dfeaturesextractedbyourfrozenencodercontributeto
the 3D geometric and semantic representations in the decoder. Our 3D repre-
sentationsnotonlyencompassmorecomprehensiveinformationbutalsoexhibit
a stronger reconstruction capability, leading to a 1.8% increase in F-Score.
Qualitative 3D Reconstruction. Fig. 3 presents the 3D reconstruction visu-
alization results on ScanNet. Thanks to our geometric and semantic represen-
tations, Uni3DR2 excels in reconstructing objects with high completeness and
more intricate edge details. This is evident in finer aspects, such as the back of
a chair (top row), the deck (second row), and the wash basin (third row). Our
reconstructed objects exhibit detailed coherence without any interruptions or
breaks.Uni3DR2 11
Table2:Experimentalresults(%)onScanQAval set.B-1,B-4denoteBLEU-1,
BLEU-4respectively.Notethatourgeometryispredicted(pred),whileothersrelyon
ground truth (GT).
Recon. B-1 B-4 METEOR ROUGE-L CIDER EM
SingleImage (Flamingo) - 23.8 8.5 10.7 29.6 52 16.9
MultiView (Flamingo) - 25.6 8.4 11.3 31.1 55 18.8
SingleImage (FlanT5) - 28.6 5.1 10.6 25.8 42.6 13.3
MultiView (FlanT5) - 29.7 5.9 11.3 26.6 45.7 13.6
VoteNet [28]+MCAN* GT 28.0 6.2 11.4 29.8 54.7 17.3
ScanRefer [4]+MCAN* GT 26.9 7.9 11.5 30.0 55.4 18.6
ScanQA [2]* GT 30.2 10.1 13.1 33.3 64.9 21.0
3D-LLM (Flamingo) [15] GT 30.3 7.2 12.2 32.3 59.2 20.4
3D-LLM (OPT) [15] GT 35.9 9.4 13.8 34.0 63.8 19.3
3D-LLM (FlanT5) [15] GT 39.3 12.0 14.5 35.7 69.4 20.5
3D-LLM (OPT) [15] pred 32.4 8.3 12.4 31.5 60.1 14.7
Uni3DR2-LLM (OPT) pred 36.6 11.0 14.0 34.4 64.3 15.8
3D-LLM (FlanT5) [15] pred 35.8 9.7 13.2 32.8 61.9 15.2
Uni3DR2-LLM (FlanT5) pred 39.8 12.2 14.9 36.3 70.3 17.3
4.3 Vision-Language Understanding on ScanQA
Baselines.Weadoptthestate-of-the-artmethod3D-LLMasourbasemodelfor
3D Vision-Language Understanding. However, a direct comparison between our
Uni3DR2-LLM and 3D-LLM which uses GT point clouds is not fair. Therefore,
we replace the GT point clouds in 3D-LLM with the point clouds predicted by
ourUni3DR2toestablishafairbaseline.Furthermore,wecompareourUni3DR2-
LLMwithseveralothercompetitiveapproaches.VoteNet+MCAN[28,48]applies
VoteNet to extract 3D point cloud objects and uses extracted features in the
MCAN model for VQA. ScaneRefer+MCAN [4,48] employs PointNet++ [29]
to extract point cloud and lifting features by MCAN. ScanQA [2] fuses the
point cloud features with pre-trained language embeddings such as GloVe [27].
Additionally,wecompareourUni3DR2-LLMwithLLMsthattakesingle/multi-
view image features as input.
Metrics. We utilize a comprehensive set of metrics to assess performance, in-
cluding BLEU, METEOR, ROUGE-L, CIDEr, and EM. BLEU (Bilingual Eval-
uation Understudy) serves as an indicator of text generation accuracy by com-
paring the overlap of n-grams (sequential word or character sequences) in the
generatedtextwiththosein referencetext.METEOR(MetricforEvaluation of
Translation with Explicit Ordering) offers a nuanced evaluation of translation
quality,takingintoaccountfactorssuchasprecision,recall,stemming,andsyn-
onymy. ROUGE-L focuses specifically on measuring the overlap of the longest
common subsequences between a reference text (ground truth) and a system-
generated summary or text. CIDEr evaluates both word accuracy and the con-
sensus among multiple human-generated reference captions. Lastly, EM (Exact12 T. Chu et al.
Table 3: Experimental results (%) on ScanQA test set.B-1,B-4denoteBLEU-
1, BLEU-4 respectively. Note that our geometry is predicted (pred), while others rely
on ground truth (GT). (*: using explicit object representations).
Recon. B-1 B-4 METEOR ROUGE-L CIDER EM
VoteNet[28]+MCAN* GT 29.5 6.0 12.0 30.9 58.2 19.7
ScanRefer[4]+MCAN* GT 27.9 7.5 11.9 30.7 57.4 20.6
ScanQA[2]* GT 31.6 12.0 13.5 34.3 67.3 23.5
3D-LLM(Flamingo)[15] GT 32.6 8.4 13.5 34.8 65.6 23.2
3D-LLM(OPT)[15] GT 37.3 10.7 14.3 34.5 67.1 19.1
3D-LLM(FlanT5)[15] GT 38.3 11.6 14.9 35.3 69.6 19.1
3D-LLM(OPT)[15] pred 33.0 8.9 12.6 31.7 60.9 14.8
Uni3DR2-LLM(OPT) pred 38.4 10.9 14.7 34.8 68.2 17.2
3D-LLM(FlanT5)[15] pred 35.5 9.6 13.0 32.2 62.1 14.9
Uni3DR2-LLM(FlanT5) pred 39.7 11.9 15.3 36.6 71.7 19.3
Table 4: Experimental results (%) on 3DMV-VQA test setwithfourquestion
types: Concept (Conc), Counting (Coun), Relation (Rela), and Comparison (Comp).
Note that our geometry is predicted (pred), while others rely on ground truth (GT)
except SingleImage and MultiView. (*: using explicit object representations).
Recon. Conc Coun Rela Comp Overall
SingleImage (Flamingo) - 58.7 18.5 38.4 60.1 40.3
MultiView (Flamingo) - 60.0 18.3 40.2 61.4 41.6
SingleImage (FlanT5) - 58.0 20.4 42.3 62.3 43.1
MultiView (FlanT5) - 61.9 21.1 48.0 62.3 47.1
NS-VQA* [47] GT 59.8 21.5 33.4 61.6 38.0
3D-CLR* [14] GT 66.1 41.3 57.6 72.3 57.7
3D-LLM (Flamingo) [15] GT 68.9 32.4 61.6 68.3 58.6
3D-LLM (OPT) [15] GT 63.4 30.7 57.6 65.2 54.9
3D-LLM (FlanT5) [15] GT 68.1 31.4 55.1 69.7 54.6
3D-LLM (OPT) [15] pred 58.2 25.8 53.2 59.1 51.8
Uni3DR2-LLM (OPT) pred 67.4 27.3 59.0 69.5 58.0
3D-LLM (FlanT5) [15] pred 61.3 27.1 55.1 62.3 53.9
Uni3DR2-LLM (FlanT5) pred 70.8 28.6 62.8 77.7 62.0
Match) provides a stringent measure of accuracy, quantifying the percentage of
predictions that precisely match the ground truth or expected correct answer.
Results. The effectiveness of our proposed Uni3DR2-LLM on the ScanQA val
andtest setisdemonstratedinTab.2andTab.3.Thankstoourunifiedrepresen-
tation and reconstruction design, our Uni3DR2-LLM outperforms the baseline
3D-LLM[15]‚Äîwhichusespredictedpointclouds‚Äîby4.0%and4.2%inBLEU-1
on val and test of ScanQA. Furthermore, it surpasses all the competitive meth-
ods, including 3D-LLM which uses GT point clouds, across the most of metrics
on both the ScanQA val and test set.
4.4 Vision-Language Understanding on 3DMV-VQA
Baselines. We compare Uni3DR2-LLM with several competitive approaches.
NS-VQA[47]takes2Dgroundingfeaturesandpriorsymbolicstructureasinputs.Uni3DR2 13
Reconstruction Question & Answer Reconstruction Question & Answer
How many chalk describethescene
boardsarethere?
This room is a
2 cozyandinviting
Wherecanasingle space. There are
seatsofabeseen? several windows
for natural light
incornerofroom and the bed is
whatcolorisit? located near the
brown window.
Is there any Whatisinfrontof
instrumentsinthis thecouches?
room?
table
yes,piano
What is on the
Isthereanythingin table?
frontofit?
bouquet
yes,abench
Fig.4:3Dvision-languageunderstandingvisualizationresults.OurUni3DR2-
LLM predicts accurate reconstructions and answers the user input questions. (Zoom
in for details.).
Table 5: Ablation studies about encoder design choices on ScanNet for re-
construction analysis.
Recon. CLIPSAM Abs Rel ‚ÜìAbs Diff ‚ÜìRMSE ‚ÜìF-Score ‚Üë
NeuralRecon ‚úó ‚úó 0.065 0.106 0.195 0.562
Rec ‚úì ‚úó 0.069 0.113 0.213 0.534
C
Rec ‚úó ‚úì 0.070 0.106 0.205 0.570
S
Rec ‚úì ‚úì 0.060 0.094 0.182 0.580
CS
3D-CLR[14]associates3Dfeatureswith2Dfeaturesviaa3D-2Dalignmentloss.
And the state-of-the-art 3D-LLM [15] which uses GT point clouds.
Results.Thankstoourunified3Drepresentationandreconstructiondesign,our
Uni3DR2-LLMachievesthehighestoverallaccuracyat62.0%withthepredicted
reconstruction results instead of relying on GT point clouds. It also leads in the
individual categories of Concept, Relation, and Comparison with 70.8%, 62.8%,
and 77.7% respectively.
4.5 Qualitative 3D Vision-Language Understanding.
We also provide the qualitative results about 3D vision-language understanding
inFig.4.Ourunified3Drepresentationandreconstructioncapabilitiesempower
the LLM to accurately answer questions necessitating a comprehensive under-
standingofobjectsandtheirspatiallocationsin3Dscenes.Ourmethodprovides
accurateanswerstoquestionsaboutcolor,objectnumbers,andlocations,thanks
to our semantically rich representation.14 T. Chu et al.
Table 6: Ablation studies on ScanQA val setaboutrepresentation(Repre.)and
reconstruction (Recon.).
Repre. Recon. B-1 B-4 METEORROUGE-LCIDER EM
CLIP(MultiView) - 29.7 5.9 11.3 26.6 45.7 13.6
CLIP (3D-LLM) GT 39.3 12.0 14.5 35.7 69.4 20.5
proj
CLIP (3D-LLM) Rec 35.8 9.7 13.2 32.8 61.9 15.2
proj CS
3DFeat(basedonCLIP) Rec 37.8 11.2 14.3 35.6 67.9 17.8
C
3DFeat(basedonSAM) Rec 37.9 10.2 14.3 35.7 68.1 18.3
S
3DFeat(Uni3DR2-LLM) Rec 39.812.2 14.9 36.3 70.3 17.3
CS
4.6 Ablation Studies
Encoder Design Choices. The encoder in Our Uni3DR2 incorporates two
frozen pre-trained models: the CLIP visual encoder and the SAM visual en-
coder.InTab.5,wedetailtheresultsofourablationstudies,whichquantifythe
contribution of each component to the 3D reconstruction. The study compares
four configurations: ‚ÄúNeuralRecon‚Äù without either pretrained encoder, ‚ÄúRec ‚Äù
C
with only the frozen CLIP encoder, ‚ÄúRec ‚Äù with only the frozen SAM encoder,
S
and‚ÄúRec ‚Äù withbothencodersactive. Acomparisonbetweentheperformance
CS
of ‚ÄúRec ‚Äù and ‚ÄúNeuralRecon‚Äù indicates that Clip features, which focus on se-
C
mantics, display reduced performance while still providing valuable geometric
information for 3D reconstruction. When Comparing ‚ÄúRec ‚Äù to ‚ÄúNeuralRecon‚Äù,
S
it becomes apparent that SAM features, enriched with object-level information,
demonstratesuperiorgeometriccapabilitiescomparedtothebaselinemodelwith
a trained encoder. Moreover, an analysis of the performance of ‚ÄúRec ‚Äù in re-
CS
lation to both ‚ÄúRec ‚Äù and ‚ÄúRec ‚Äù reveals that the fusion of Clip features and
C S
SAM features yields more comprehensive information, resulting in superior re-
construction outcomes.
Representation. In Tab. 6, we demonstrate the importance of representation
on the ScanQA dataset. The study compares six configurations: ‚ÄúCLIP (Multi-
View)‚Äù withoutliftingandgeometry,3D-LLMdenotedas‚ÄúCLIP (3D-LLM)‚Äù
proj
with ‚ÄúGT‚Äù point clouds, the baseline represented by ‚ÄúCLIP (3D-LLM)‚Äù with
proj
point clouds predicted by ‚ÄúRec ‚Äù, ‚Äú3D Feat‚Äù involving unified representation
CS
andreconstructionof‚ÄúRec ‚Äù,‚ÄúRec ‚Äù,and‚ÄúRec ‚Äù.Comparingtheperformance
C S CS
of‚ÄúCLIP(MultiView)‚Äù andthebaselineunderscoresthesignificanceofgeometry
in3DLLM.Theevaluationofthebaselineagainstallconfigurationsof‚Äú3DFeat‚Äù,
unifiedrepresentation,andreconstructionsignificantlyenhancesperformancein
3D vision-language understanding. And our final Uni3DR2-LLM demonstrates
a notable improvement of 4.0% and 4.2% in BLEU-1 on val and test of ScanQA
comparedtothebaseline.ComparingUni3DR2-LLMwithboth‚Äú3DFeat(based
on CLIP)‚Äù and ‚Äú3D Feat (based on SAM)‚Äù, the fusion of two features demon-
stratesanenhancementinLLMcapabilities.Furthermore,acomparisonbetween
thebaselineand3D-LLMrevealsasignificantinfluenceonreconstructionquality.
Finally, when comparing our Uni3DR2-LLM with 3D-LLM, our unified model
effectively mitigate the impact of reconstruction quality, resulting in higher per-
formance compared to 3D-LLM which uses GT point clouds.Uni3DR2 15
5 Conclusion
Our research addresses a critical challenge in applying Large Language Mod-
els (LLMs) to 3D environments: the acquisition of effective 3D representations
suitable for LLMs. Compared to structural 1D text or 2D images, 3D scene
representations are hard to interpret and manipulate, which highlights the need
for research of advanced processing and representation learning algorithms. We
emphasize the significance of geometric and semantically rich features and in-
troduceaunifiedmodulefor3Drepresentationandreconstruction.Futurework
will explore scaling our method to enhance more 3D capabilities with LLMs,
including advancing 3D scene perception and 3D generation.
Acknowledgement
This work is supported by Guangdong Basic and Applied Basic Research Foun-
dation(GrantNo.2024A1515012043),theNationalKeyR&DProgramofChina
(2022ZD0160201), and Shanghai Artificial lntelligence Laboratory.
References
1. Achlioptas, P., Abdelreheem, A., Xia, F., Elhoseiny, M., Guibas, L.: ReferIt3D:
Neural listeners for fine-grained 3d object identification in real-world scenes. In:
ECCV (2020) 3
2. Azuma, D., Miyanishi, T., Kurita, S., Kawanabe, M.: ScanQA: 3d question an-
swering for spatial scene understanding. In: CVPR (2022) 3, 8, 11, 12
3. Cai, D., Zhao, L., Zhang, J., Sheng, L., Xu, D.: 3DJCG: A unified framework for
joint dense captioning and visual grounding on 3d point clouds. In: CVPR (2022)
3
4. Chen, D.Z., Chang, A.X., Nie√üner, M.: ScanRefer: 3d object localization in rgb-d
scans using natural language. In: ECCV (2020) 3, 11, 12
5. Chen,Z.,Gholami,A.,Nie√üner,M.,Chang,A.X.:Scan2Cap:Context-awaredense
captioning in rgb-d scans. In: CVPR (2021) 3
6. Chen, Z., Hu, R., Chen, X., Nie√üner, M., Chang, A.X.: UniT3D: A unified trans-
former for 3d dense captioning and visual grounding. In: ICCV (2023) 3
7. Choe, J., Im, S., Rameau, F., Kang, M., Kweon, I.S.: Volumefusion: Deep depth
fusionfor3dscenereconstruction.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 16086‚Äì16095 (2021) 4
8. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: PaLM: Scaling lan-
guage modeling with pathways. arXiv preprint arXiv:2204.02311 (2022) 1, 4
9. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scan-
Net: Richly-annotated 3d reconstructions of indoor scenes. In: CVPR (2017) 8
10. Driess,D.,Xia,F.,Sajjadi,M.S.M.,Lynch,C.,Chowdhery,A.,Ichter,B.,Wahid,
A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P.,
Duckworth,D.,Levine,S.,Vanhoucke,V.,Hausman,K.,Toussaint,M.,Greff,K.,
Zeng,A.,Mordatch,I.,Florence,P.:PaLM-E:Anembodiedmultimodallanguage
model. arXiv preprint arXiv:2303.03378 (2023) 416 T. Chu et al.
11. Eigen,D.,Puhrsch,C.,Fergus,R.:Depthmappredictionfromasingleimageusing
a multi-scale deep network. In: NeurIPS (2014) 9
12. Feng,M.,Li,Z.,Li,Q.,Zhang,L.,Zhang,X.,Zhu,G.,Zhang,H.,Wang,Y.,Mian,
A.: Free-form description guided 3d visual graph network for object grounding in
point cloud. In: ICCV (2021) 3
13. Ha, H., Song, S.: Semantic Abstraction: Open-world 3d scene understanding from
2d vision-language models. In: CoRL (2022) 3
14. Hong,Y.,Lin,C.,Du,Y.,Chen,Z.,Tenenbaum,J.B.,Gan,C.:3dconceptlearning
and reasoning from multi-view images. In: CVPR (2023) 3, 8, 12, 13
15. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., Gan, C.: 3D-LLM:
Injecting the 3d world into large language models. In: NeurIPS (2023) 2, 3, 4, 5,
6, 11, 12, 13
16. Hou, Y., Kannala, J., Solin, A.: Multi-view stereo by temporal nonparametric
fusion. In: ICCV (2019) 9
17. Huang, J., Yong, S., Ma, X., Linghu, X., Li, P., Wang, Y., Li, Q., Zhu, S.C.,
Jia, B., Huang, S.: An embodied generalist agent in 3d world. arXiv preprint
arXiv:2311.12871 (2023) 4
18. Huang,P.H.,Lee,H.H.,Chen,H.T.,Liu,T.L.:Text-guidedgraphneuralnetworks
for referring 3d instance segmentation. In: AAAI (2021) 3
19. Im, S., Jeon, H.G., Lin, S., Kweon, I.S.: DPSNet: End-to-end deep plane sweep
stereo. In: ICLR (2019) 6, 9
20. Jatavallabhula, K.M., Saryazdi, S., Iyer, G., Paull, L.: gradSLAM: Automagically
differentiable slam. In: ICRA (2019) 4
21. Khandelwal,A.,Weihs,L.,Mottaghi,R.,Kembhavi,A.:SimplebutEffective:Clip
embeddings for embodied ai. In: CVPR (2022) 3
22. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment Anything. arXiv preprint
arXiv:2304.02643 (2023) 3, 4, 5, 8
23. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: bootstrapping language-image pre-
training with frozen image encoders and large language models. In: ICML (2023)
3, 4, 5, 7, 9
24. Murez, Z., Van As, T., Bartolozzi, J., Sinha, A., Badrinarayanan, V., Rabinovich,
A.:Atlas:End-to-end3dscenereconstructionfromposedimages.In:ECCV(2020)
8, 10
25. OpenAI: Chatgpt: Optimizing language models for dialogue (2022), https://
openai.com/blog/chatgpt, https://openai.com/blog/chatgpt 1
26. OpenAI: GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 1, 4
27. Pennington, J., Socher, R., Manning, C.D.: GloVe: Global vectors for word repre-
sentation. In: EMNLP (2014) 11
28. Qi,C.R.,Litany,O.,He,K.,Guibas,L.J.:Deephoughvotingfor3dobjectdetec-
tion in point clouds. In: ICCV (2019) 11, 12
29. Qi,C.R.,Yi,L.,Su,H.,Guibas,L.J.:PointNet++:Deephierarchicalfeaturelearn-
ing on point sets in a metric space. In: NeurIPS (2017) 11
30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 3, 4, 5, 8
31. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-
guage understanding by generative pre-training. OpenAI blog (2018) 4
32. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners. OpenAI blog (2019) 4Uni3DR2 17
33. Rich, A., Stier, N., Sen, P., H√∂llerer, T.: 3dvnet: Multi-view depth prediction and
volumetricrefinement.In:2021InternationalConferenceon3DVision(3DV).pp.
700‚Äì709. IEEE (2021) 4
34. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: Pifu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In:
Proceedings of the IEEE/CVF international conference on computer vision. pp.
2304‚Äì2314 (2019) 4
35. Sayed, M., Gibson, J., Watson, J., Prisacariu, V., Firman, M., Godard, C.: Sim-
plerecon: 3d reconstruction without 3d convolutions. In: European Conference on
Computer Vision. pp. 1‚Äì19. Springer (2022) 4
36. Sch√∂nberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection
for unstructured multi-view stereo. In: ECCV (2016) 9
37. Sun, C., Sun, M., Chen, H.T.: Direct voxel grid optimization: Super-fast conver-
gence for radiance fields reconstruction. In: CVPR (2022) 4
38. Sun,J.,Xie,Y.,Chen,L.,Zhou,X.,Bao,H.:NeuralRecon:Real-timecoherent3d
reconstruction from monocular video. In: CVPR (2021) 4, 6, 9, 10
39. Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre,
N., Mukadam, M., Chaplot, D.S., Maksymets, O., et al.: Habitat 2.0: Training
home assistants to rearrange their habitat. In: NeurIPS (2021) 3
40. Tang,H.,Liu,Z.,Zhao,S.,Lin,Y.,Lin,J.,Wang,H.,Han,S.:Searchingefficient
3d architectures with sparse point-voxel convolution. In: European Conference on
Computer Vision (2020) 6
41. Thomason, J., Shridhar, M., Bisk, Y., Paxton, C., Zettlemoyer, L.: Language
grounding with 3d objects. In: CoRL (2022) 3
42. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozi√®re, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023) 1, 4
43. Wang, K., Shen, S.: MVDepthNet: Real-time multiview depth estimation neural
network. In: 3DV (2018) 9
44. Wu,Y.,Cheng,X.,Zhang,R.,Cheng,Z.,Zhang,J.:EDA:Explicittext-decoupling
and dense alignment for 3d visual grounding. In: CVPR (2023) 3
45. Yang,Z.,Zhang,S.,Wang,L.,Luo,J.:SAT:2dsemanticsassistedtrainingfor3d
visual grounding. In: ICCV (2021) 3
46. Ye, S., Chen, D., Han, S., Liao, J.: 3D question answering. TVCG (2022) 3
47. Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: Neural-
Symbolic VQA: Disentangling reasoning from vision and language understanding.
In: NeurIPS (2018) 12
48. Yu,Z.,Yu,J.,Cui,Y.,Tao,D.,Tian,Q.:Deepmodularco-attentionnetworksfor
visual question answering. In: CVPR (2019) 11
49. Zhang,S.,Roller,S.,Goyal,N.,Artetxe,M.,Chen,M.,Chen,S.,Dewan,C.,Diab,
M.,Li,X.,Lin,X.V.,etal.:OPT:Openpre-trainedtransformerlanguagemodels.
arXiv preprint arXiv:2205.01068 (2022) 4
50. Zhu, Z., Ma, X., Chen, Y., Deng, Z., Huang, S., Li, Q.: 3D-VisTA: Pre-trained
transformer for 3d vision and text alignment. In: ICCV (2023) 3