Ring-a-Pose: A Ring for Continuous Hand Pose Tracking
TIANHONGCATHERINEYU,CornellUniversity,USA
GUILINHU,CornellUniversity,USA
RUIDONGZHANG,CornellUniversity,USA
HYUNCHULLIM,CornellUniversity,USA
SAIFMAHMUD,CornellUniversity,USA
CHI-JUNGLEE,CornellUniversity,USA
KELI,CornellUniversity,USA
DEVANSHAGARWAL,CornellUniversity,USA
SHUYANGNIE,CornellUniversity,USA
JINSEOKOH,CornellUniversity,USA
FRAN√áOISGUIMBRETI√àRE,CornellUniversity,USA
CHENGZHANG,CornellUniversity,USA
Fig.1. Ring-a-Poseisanuntetheredringthattrackshandposescontinuously.Withactiveacousticsensing,weanalyzethe
reflectionstrengthsatdifferentdistancesfromtheringtoreconstructhandposes.(a)showsanexamplereflectionprofile.(b)
showsexamplesofRing-a-Pose‚Äôsreal-timeinferencesatdifferentwristandforearmorientations.
WepresentRing-a-Pose,asingleuntetheredringthattrackscontinuous3Dhandposes.Locatedinthecenterofthehand,the
ringemitsaninaudibleacousticsignalthateachhandposereflectsdifferently.Ring-a-Poseimposesminimalobtrusionson
thehand,unlikemulti-ringorglovesystems.Itisnotaffectedbythechoiceofclothingthatmaycoverwrist-wornsystems.
Inaseriesofthreeuserstudieswithatotalof30participants,weevaluateRing-a-Pose‚Äôsperformanceonposetrackingand
micro-fingergesturerecognition.Withoutcollectinganytrainingdatafromauser,Ring-a-Posetrackscontinuoushand
poseswithajointerrorof14.1mm.Thejointerrordecreasesto10.3mmforfine-tuneduser-dependentmodels.Ring-a-Pose
recognizes7-classmicro-gestureswitha90.60%and99.27%accuracyforuser-independentanduser-dependentmodels,
respectively.Furthermore,theringexhibitspromisingperformancewhenwornonanyfinger.Ring-a-Poseenablesthefuture
ofsmartringstotrackandrecognizehandposesusingrelativelylow-poweracousticsensing.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat
copiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirst
page.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopy
otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom
permissions@acm.org.
Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
¬©2024ACM.
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
1
4202
rpA
91
]CH.sc[
1v08921.4042:viXraConferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
ACMReferenceFormat:
TianhongCatherineYu,GuilinHu,RuidongZhang,HyunchulLim,SaifMahmud,Chi-JungLee,KeLi,DevanshAgarwal,
ShuyangNie,JinseokOh,Fran√ßoisGuimbreti√®re,andChengZhang.2024.Ring-a-Pose:ARingforContinuousHandPose
Tracking.In.ACM,NewYork,NY,USA,26pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Thesmartringisapromisingandrisingwearableforeverydayuse[42].However,duetothespatialconstraint,
thesensingcapabilityoftheringhasbeenlimited:commodityringsonlyintegrateminiaturizedandlow-power
sensors(e.g.,heartratesensors,IMUs).Despitebeingwornonthefinger,thecurrentsmartring,onitsown,
cannottrackhandposescontinuously,whichiscrucialinunderstandingtheuser‚Äôsactionsandintentions(e.g.,
VR/AR interaction, human activity recognition). To address this problem, prior work has explored different
sensingmodalitiestorecognizehand/fingergesturesonaring[7,17,24,41,52,60,70‚Äì72],butmultiplerings
alongwithawristband[77,78],notdesirableforeverydayuses,arerequiredtotrackfullhandposes.Thistaskis
particularlydifficultashandjointmovementshave21degreesoffreedominvolvingfingerflexions/extensions
andabductions/adductions[77].Thekeychallengeoftrackingfull-handposesonaringistosenseenough
informationontheposeandmovementoneachfingerfromasinglering-wornposition,whichischallengingto
captureusingtraditionalmotionsensorslikeIMUs.Inthispaper,weseektoanswertheresearchquestionof
whetherwecancontinuouslytrackhandposeswithjustasingleuntetheredring.
Tothisend,wepresentRing-a-Pose(Fig.1),asingleuntetheredringthattracksfullhandposescontinuously.
Wearethefirsttoexploreembeddingactiveacousticsensingonaringasa"SONAR"forcontinuoushandpose
tracking.Itonlyrequiresamicrophoneandaspeakerembeddedintothering,whichalready(partially)existin
commoditysmartrings(e.g.,[42]).Thespeakeremitsinaudiblefrequency-modulatedcontinuouswaves(FMCW)
aroundthefinger,whicharereflectedbythepalmandfingerswithuniquepatterns.Themicrophoneonthesame
ringreceivesthereflectedsoundwavesandtransmitsthesignalstoanearbycomputingdevice(e.g.,smartphones
orcomputers)forprocessingusingaBluetoothLowEnergy(BLE)module.Then,alightweightcustomizeddata
processinganddeep-learningpipeline(asshowninFig.1)processestheacousticdatatoestimatetherelative
(tothewrist)positionsof20fingerjointsin3Dorrecognizethemicrofingergestures.Ring-a-Poseislow-cost
(‚àºUS$30)withalowprofile,comparablewiththatofcommercialsmartrings[42].Furthermore,thebattery
consumptionisrelativelylowpower(148.0mW)withthepossibilityoffurtheroptimization.
TothoroughlyevaluatetheperformanceofRing-a-Pose,weconductedthreestudies.Inthefirststudywith10
participants,wecomparedtheperformancewhentheringwaswornondifferentfingers.Theresultsshowed
thatRing-a-Posecantrackthehandposeswellonanywornfinger,whichwasunderexploredinpriorring-based
sensingsystems[58].Inthesecondstudywith12participants,weshowedthatRing-a-Poseachievesamean
per-jointpositionerror(MPJPE)of14.1mmand10.3mminuser-independentanduser-dependentevaluations,
respectively. In the third user study with 10 participants, Ring-a-Pose was able to recognize 7 micro-finger
gestureswithaccuraciesof90.60%and99.27%inuser-independentanduser-dependentevaluationrespectively.
Unlikemostdata-drivenhandposetrackingsystemsthathavesignificantperformancedegradationwhenthe
sensorisremounted,Ring-a-Poseisindependentofwearingsessions.Additionally,Ring-a-Poseexhibitsexciting
performanceevenwhenusedout-of-the-box,i.e.nocalibrationdatafromthenewuser,furtherhighlightingthe
practicalityofring-basedhandposetrackingsystems.Insummary,themaincontributionsofthispaperare:
‚Ä¢ Tothebestofourknowledge,Ring-a-Poseisthefirstsingle-ringsystemtodemonstratecontinuoushand
posetracking.
‚Ä¢ Wevalidatedthattheringeffectivelytracksthehandposewhenwornonanyfingerwith10participants.
‚Ä¢ We evaluated Ring-a-Pose‚Äôs performance in continuous hand pose tracking and micro-finger gesture
recognitionwithuserstudies.
2Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
‚Ä¢ WefurtherdiscusstheopportunitiesandchallengesofintegratingRing-a-Posetothefuturecommodity
ringplatforms.
2 RELATEDWORK
Motivated by attractive applications of hand pose tracking in natural interaction, rehabilitation, immersive
AR/VRexperiences,signlanguagetranslation,etc.,thecomputervisioncommunityhasexploredvariousrobust
opticalsensingapproachesoff-the-body:multi-camerasystems[59],monocularRGBcameras[36,73],anddepth
cameras[45,56].However,sensorinstrumentationintheenvironmentlacksportabilityandtheocclusionissue
limitstheusers‚Äômovements.Thus,thereisaneedforreliablewearableapproachesthatareportableandsuffer
lessfromocclusionsfromtheenvironment.Here,wefocusonreviewingwearableapproachesthatarepertinent
toourcontribution.Intherestofthesection,wedividerelatedworksintothreekeyareas:(1)handposesensing,
thefunctionalityofRing-a-Pose,(2)acousticsensing,thesensingtechniqueofRing-a-Pose,and(3)sensingrings,
theformfactorofRing-a-Pose.
2.1 HandPoseSensingonWearables
Mounting the sensor directly on the user allows mobile hand sensing. Cameras mounted on the heads are
suitableforVRuses[1,35],butarenotpracticalforday-to-dayuses.Wristisapopularinstrumentationsite.
Wrist-worndeviceshavelowproximitytothehandanddonotfullycoverthehandlikedatagloves[10,14]
whichareunrealistictobewornatalltimes.Camerasmountedonthewrists[21,66,68]andotherrange-finding
sensors[12,22,27,52]canreconstructhandposesorrecognizehandgesturesfromlimitedviewpoints,butthey
requireaclearline-of-sightfromthewristthatcannotbecoveredbylong-sleeveclothing.Otherhandpose
trackingsensingprinciplesthatdonotsufferfromsensorocclusionincludeimpedancecharacteristics[26,61]
thatdonotgeneralizeacrossusersandelectromyography(EMG)[32]withabulkyformfactor.Ourringisnot
affectedbyclothingchoices,generalizeswellacrossusers,andislow-profile.
Handgesturerecognitionisaneasiersensingtaskthanhandposetrackingbutiscrucialtogesturalcontrols.
Prior systems of full-hand gesture recognition are implemented with impedance [76], wrist pressure [11],
ultrasonic beamforming [22], capacitive sensing [55], etc. To enable more discrete and acceptable natural
interactive contorl [20], micro-finger gesture recognition that requires fine-grained sensing is gaining trac-
tion[4,15,39,50,63,71,72].Inourwork,wedemonstratethatRing-a-Poseeffectivelyrecognizesbothfull-hand
gesturesandmicro-fingergestures.
2.2 AcousticSensingonWearables
On-body acoustic sensing has emerged as a reliable approach for tracking contexts and movements due to
itsrobustnesstonoisefactorslikelightingconditionsandelectricfields.Theapplicationsspanhandgesture
sensing [18, 22, 27, 72], motion tracking [5, 34, 62], pose tracking[33], activity recognition [67], food intake
recognition[3],facialexpressiontracking[29,30],teethandtonguegesturerecognition[53,75],gazeTracking[28],
silentspeechrecognition[23,51,74,75],etc.
The most recent work, EchoWrist [27] is the closest to our system. It used two pairs of microphones and
speakersonawristbandtotrackhandposesandrecognizehand-objectinteractions.Forthesimilarsensing
task,designingasensingsystemforaringexhibitsmuchgreaterchallengesthanforawristbandasthering
hasasmallerphysicalspace.Forexample,aringcannotaffordtwopairsofmicrophonesandspeakersdueto
spatialconstraintsandenergyconsumptionconsiderations.Althoughbothsystemsemployacousticsensing,
EchoWristandRing-a-Poseadaptdifferentsensingprinciplestoinferhandposes:EchoWristinfershandposes
andhand-objectinteractionfromthecontourshapearoundthewristcapturedbyacousticsensors,whileRing-a-
Poseinfershandposesandrecognizeshandgesturesfromtheacousticreflectionfromthefingersdirectly.The
3Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Table1. AHigh-LevelOverviewofHandSensingRings.Ring-a-Poseisthefirsttotrackcontinuoushandposeswithjusta
singlering.Inthelabels,‚ÄúUI‚Äùstandsforuser-independenceand‚ÄúUD‚Äùstandsforuser-dependence.
Tracking Power #of Un-
System Sensor UI UD
Output Consumption Components tethered
ElectroRing,2021[25] DiscreteTap ElectricField 220mW 1Ring ‚úì √ó ‚úì
ThumbTrak,2018[52] DiscreteGesture Proximity 120mW 1Ring √ó √ó ‚úì
FingerPing,2018[72] DiscreteGesture ActiveAcoustic - 1Ring,Wristband √ó √ó ‚úì
Bolduetal.,2018[4] DiscreteGesture Capacitive 475mW 1Ring,Wristband ‚úì ‚úì ‚úì
CyclopsRing,2015[7] DiscreteGesture Camera - 1Ring √ó ‚úì ‚úì
DiscreteGesture
EFRing,2022[9] ElectricField - 1Ring √ó ‚úì ‚úì
1DContinuous
DiscreteGesture
Z-Ring,2023[60] Bio-Impedance 2.4W 1Ring,Wristband √ó ‚úì ‚úì
2DContinuous
Zhouetal.,2023[78] ContinuousPose IMUandPPG 44mW/ring 2-5Rings,Wristband ‚úì ‚úì ‚úì
ssLOTR,2022[77] ContinuousPose IMU 198mW 2-5Rings,Wristband ‚úì ‚úì ‚úì
Ring-a-Pose ContinuousPose ActiveAcoustic 148mW 1Ring ‚úì ‚úì ‚úì
ringbenefitsfromitsuniquewornlocation.Long-sleevedclothingeasilycoversthewristbandbutnotthering
Thering‚Äôscloserproximitytothefingersallowsstrongeracousticreflectionsfromthefingersandenables(a)
fine-grainedtrackinglikemicro-fingergestures,whichishardtocaptureinformationfromthewristband[27]
and(b)similartrackingperformancewithonlyhalfthenumberofthespeakerandthemicrophone.Last,unlike
EchoWrist,Ring-a-Posedoesnotnecessitatereturningtoaneutralhandposebetweentransitioningbetween
poses(furtherdetailedinSec.8.1.1).Wearethefirsttoexploreactiveacousticsensingontheformfactorofa
ringtoenableaccuratecontinuoushandposetrackingandmicro-fingergesturerecognition.
2.3 SensingRings
Commoditysmartringstodayspecializeinfitness&wellness[42,46]andcontactlesspayment[47],missing
thehand-relatedinteractionspaceunveileduniquelybytheringformfactor.Researcherstriedtofillthegap
with IMUs [13, 57, 77], proximity sensors [52], electric field sensing [9, 54, 60], capacitive sensing [4, 25],
electromagneticsensing[43],infraredsensor[41],minaturecameras[7,37],acousticsignals[17,69],etc.Sensing
rings applications span text input [16, 40, 54], health sensing [44, 78], authentication [49, 60], and gestural
inputs [7, 8, 43, 58, 79]. Vatavu and Bilius [58] reviewed hand gesture inputs with rings, ring-like, and ring-
ready devices. Ring-a-Pose falls in the ‚Äúrings‚Äù category: finger-worn device with a ring form factor. In past
literature,onlymulti-ringsystems[77,78]cantrackhandposecontinuously.ssLLOR[77]trackshandposes
withawristbandand2-5rings,eachembeddedwithanIMUunit.AsshowninTable1,Ring-a-Posepresents
thefirstuntetheredringthattracks20DoFhandposeswithjustasinglering,extendingpracticaleveryday
smartringcapabilities.Notethesensingprincipleisnottheonlyfactordeterminingenergyconsumption.Other
factors like microcontroller and communication method choices make the direct comparison unfair, but we
includetheoverallpowersignaturetobettersituateourrelativelylow-powersystem.Onebenefitofouractive
acousticsensingapproachisthattheringsensesallfingermovementswell(unlikeIMUs),nomatterwhichfinger
theringisinstrumentedon.Thumb-In-Motion[4]employscapacitivesensingonanindexfingerringtosense
thumb-to-indexmicrogestures.Ring-a-Poserecognizesthumb-to-indexmicrogestureswiththeringonthe
middlefingeranddemonstratesthesystem‚Äôspotentialtodetectfine-grainmovementsevenwhenthemoving
fingersarenotinstrumented.Furthermore,itisimportanttoacknowledgethattheringhasalimitedspacefor
electronics(especiallythebattery),whichmakesitchallengingtoprototypeanuntethereddevice.
4Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.2. (a)TheuntetheredRing-a-Poseprototypeusedintheuserstudies.(b)TheRing-a-Poseprototypeelectronicswithout
thecase.(c)AphysicalmockupoffutureRing-a-Pose.WereplacedthebatterywithanarcbatteryandremovedtheMCU.(d)
DetailsofRing-a-Pose‚ÄôsPCBs.
3 IMPLEMENTATION:THERINGPROTOTYPE
Thesensingring,anovelformfactorforcontinuoushandposetracking,placesthesensorsonthefinger,insidethe
hand.Thisaffordsuniqueopportunitiestosimplifythehandreconstructiontask:maintaininggreatvisibilityofthe
handandconsistencybetweentheorientationsofthewristandthesensor.Wearableactiveacousticsensinghas
beenshownasapromisinglow-powersensingtechniquetotrackfine-grainedbodypostures[27,29,30,38,75].
Theyarelow-powercomparedwithvision-basedsystems.Inthiswork,weadopttheactiveacousticsensing
techniqueforthering,detailedinSec.4.Ring-a-Pose‚Äôsimplementationconsistsofaphysicalringprototypeand
asoftwarestackthatprocessestheinaudibleacousticsignalscapturedbytheringforhandposetrackingand
gesturerecognition.
3.1 DesignObjective
Ourdesignaimstopreservetheslimandminiaturizedformfactorofaringforcomfortableprolongedwearwhile
achievingpromisingsensingperformance.Thecomplexsensingtaskthatpredicts3Dcoordinatesof20hand
jointsrequiresinformationaboutthethumbandallother4fingers.Thus,workingwiththespaceconstraint,we
experimentwithonlyonespeakerandonemicrophoneinsteadofmultiplespeakersandmicrophones[22,27,30,
75].Webelievethatthenatureofacousticsignalpropagationenablesustocapturetheshapeandmovementsof
fingersthroughintricatesoundwavereflections.Furthermore,tominimizethewidthofthering,thespeakerand
microphoneareplacedsidebysidehorizontally,insteadofvertically.
3.2 FormFactorDesign
WestartedtheprototypeprocesswithrigidPCBislandsbutwemovedtotheflexibleprintedcircuitboard(FPCB)
foraslimmerringbodyandabetterfittotheringcurvature.Thespeakerandthemicrophoneshouldbeas
closetoeachotheraspossibleforaccurateround-trippropagationtime[64],butduetotheringcurvatureand
thicknessofthecomponents,theyareplaced5.4mmawayfromeachother.Ourringhasawidthof11mm.
Theringbody‚Äôsthickestpart,containingthespeakerandmicrophone,is3.58mmthick,andtherestis2mm
thick,comparablewiththatofthecommercialOuraRing[42].Theringisalsolightweight,weighing4.3grams
includingthebattery(1.8g).Finally,tofitthefingersizesofalluserstudyparticipants,weoptedfora3/4circle
ringandusedyarntoadjusttheringsizewithaslidingknot.
3.3 Hardware
Theringincludes(a)aringbody,acustomizedFPCBenclosedina3D-printedPLAcase,and(b)acustomized
capturingandprocessingPCB.Fig.2(b)showstheassembledhardware.ThetopPCB(leftinFig.2(d))houses
theSGW1110module,featuringthenRF52840microcontrollerunit(MCU).Thelow-powerMCUimplements
5Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.3. EchoFrameCalculation.Thecross-correlation(orangeline)betweenthetransmittedFMCWsignal(blue)andthe
receivedsignal(green)ismappedfromthetimedomaintothedistancedomainasanechoprofile.Ring-a-Posecrops54
pixels,equivalentto18.52cm,oftheechoframetoanalyzehandposes.Theblacklinesinthe3Dvisualizationoverlayedon
thehanddenotea3cmdistanceincrement.Thecolorofeachspheredenotesthesummationofreflectionstrengthsfrom
thatradius.
Bluetooth5andprovidesBluetoothLowEnergy(BLE)functionswithabuilt-inPCB-mountedantenna.The
bottomPCB,rightinFig.2(d),containsanaudioamplifier(MAX98357A),avoltageregulator(TPS62743)that
providesaconstant3.3Vsource,aswitchthatturnsON/OFFthering,aFlexiblePrintedCircuit(FPC)connector
thatconnectstotheringbody,andabatteryconnector.TheringbodyFPCBfeaturesaspeaker(USoundUT-
P2019)andamicrophone(TDKICS-43434).ThetotalcostoftheprototypeisaboutUS$30andcouldbedecreased
whenmassmanufactured.Withthe3.7V70mAhLipoBattery,wemeasured(usingaCurrentRanger)theenergy
consumptionoftheringtobe148.0mW.TheenergyconsumptionbreaksdownintotheMCUconsuming24.0
mW,thespeakerandthemicrophoneconsuming120.0mW,andtheBLEtransmissionconsuming4.0mWon
average.WewillfurtherdiscusstheenergyconsumptioninSec.10.Similarto[30],thespeakeremitsFMCW
signals(detailedinSec.4.1.1)intherangeof20-24KHz,outsideofthecommonlystatedrangeofhumanhearing.
The 16-bit sampled signals are transmitted to a nearby device (smartphone or computer) using BLE UART
communicationat800Kbps.
4 SENSINGPRINCIPLE
Weuseactiveacousticsensingasthesensingprinciplefortheringtotrackcontinuoushandposes.Thering
emitsinaudiblesoundwaves,whicharethenreflectedandrefractedbythesurroundingfingersandthepalm.As
aresult,thesoundwavesarereceivedbythemicrophonewithuniquepatterns.AsshowninFig.4,different
handgeometriesyielddistinctreflectedsignalswhichwelaterprocesstoreconstructhandposes.Fortherestof
thesection,webrieflycoverhowwecalculateourdeeplearningmodelinputs:echoframesandechoprofile
(pleaserefertoWangetal.[64]andLietal.[30]formoredetailedexplanations)andhowRing-a-Poseutilizes
echoprofiles.
4.1 EchoProfileCalculation
Theechoprofiles(bottom2rowsinFig.4)aremadeupof1-pixelwideechoframes(Fig.3)stackedalongthetime
axis.Theoriginalechoprofilecapturestheabsolutehandgeometries,andthedifferentialechoprofilecaptures
thehandgeometrymovements.
4.1.1 EchoFrameCalculation. AsshowninFig.3,thering‚ÄôsspeakeremitsaFrequency-ModulatedContinuous-
Wave(FMCW)signal.Byapplyingcross-correlation[64]betweenthetransmittedandband-passfiltered(20-
24KHz,thesameasemittedsignals)receivedsignals(orangelineinFig.3),weacquirethestrengthsofthesignals
atdifferentreturn/reflectiontimes.Thevalueofthepixelrepresentsthestrengthofcorrelation.Thestrongest
correlation(near6msinthecorrelationgraph)atthetimestampùë° denotesthedirectpathbetweenthespeaker
0
6Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.4. OriginalandDifferentialEchoProfileforasequenceofHandPoses.Theblacklinesinthe3Dechoframesvisualization
overlayedonthehanddenotea3cmdistanceincrement.Thecolorofeachspheredenotesthesummationofreflection
strengthsfromthetravelpathlengthsoftheradiusofthesphere.
andthemicrophone.Thedelayfromùë° 0,ùúè isequivalenttoùëì ùë†ùúè pixelsintheechoframe,whereùëì ùë† =50KHzisthe
samplingrate.Meanwhile,ùúè = 2ùëë ,whereùëë isthedistancebetweentheringandthereflectedobject,i.e.the
ùëê
hand,andùëê isthespeedofsound,343ùëö/ùë†.ThuswiththeC-FMCWmethod[64](differsinrangeresolution
fromconventionalFMCW),asinglepixelisequivalentto ùëê = 343ùëö/ùë† =3.43mmindistancealongtheyaxis
2ùëìùë† 2√ó50KHz
and12msintimealongthexaxis.ThehandissmallcomparedtothetheoreticalsensingrangeofC-FMCW
(3.43mm/pixel√ó600pixels=2.06m).Toremovereflectionsfromtheenvironmentandfocusonsensingthehand,
weonlyuse54pixelsstartingfromùë° ,equivalentto18.52cm,largeenoughtocoverfull-handmovements.
0
4.1.2 Original&DifferentialEchoProfileCalculation. Whenwestackechoframesalongthetime(x-axis),we
acquirecontinuousreflectionstrengthsatdifferentdistances.Thestackedechoframestheoriginalechoprofileis
calledtheoriginalechoprofile,asshowninFig.4middlerow.Tocapturethechangesbetweenechoframes,we
calculatethedifferentialechoprofilebysubtractingthepreviousechoframefromthecurrentechoframe.The
bottomrowinFig.4showsahigh-passfiltereddifferentialechoprofileforvisualizationpurposes.
4.2 EchoProfilesforRing-a-Pose
Similarsensingapproacheshavebeenusedfortrackingfacialmovementsfromearables[30]andhandposes
fromawristband[27],butadditionalchallengessurfacewhenadaptingthetechniquetohandposetrackingwith
aring:(1)thelimitedphysicalspaceallowsfewerelectronics,reducingtheamountofsensedinformation;and(2)
themovingfingerscanpartiallyblockthesensors,occludingtheline-of-sight.Weexperimentedwithdifferent
sizesofmicrophonesandspeakerstominimizetheobstruction.
Thoughthereareonlytwoinputarraysofinformation,theycomplementeachotherefficiently:theoriginal
echo profile encodes the positions of fingers related to poses, and the differential echo profile encodes the
movementofhandgeometry(betweenposes).Everyfinger‚Äôspositionanditsmovementsaffecttheoriginal
anddifferentialechoprofile,respectively.Together,theyallowustotrack3Dhandposesfromasinglepointof
instrumentation.Withtheomnidirectionalspeaker,wecanabstracttheechoframetobehalfspheresofvarious
radiicenteredatthering(Fig.4).Thecolorofthespheredenotesthetotalstrengthofreflectionfromthetravel
pathlengthsoftheradiusofthesphere.Note,thattheechoframeonlycontainsreflectionstrengthsinformation
fromdifferentdistances,notexactpoints.Infigure4,weseethatforthe‚ÄúASL5‚Äùpose,theflathandyieldslittle
7Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
reflectionbeyondtheinnerspheres.‚ÄúASL8‚Äùand‚ÄúMBent‚Äùaretwosimilarhandposesthatonlyvaryintheposeof
thethumb,sotheyshareechoframesthatarevisuallysimilarbutdistincttothedeeplearningmodel.
Lineofsightsensors,thatrequireaclearlinebetweenthesensorandtheobservedobject,facethecommon
sensor occlusion issue. Though our active acoustic sensing approach shares the obstacle, we found that the
ringaffordsuniqueplacementthatturnshandgeometricalocclusions(wherefingersfullyorpartiallyblockthe
ring)intousefulandrepeatableinformation.‚ÄúShaka‚Äùand‚ÄúFist‚Äùaretwohandposeswheretheringismostly
blocked,sotheybothhaveverystrongreflections.However,interestingly,theslightdifferencesinthethumb
andpinkyfingerpositionsleadtoechoframedifferences:‚ÄúFist‚Äùhasstrongerreflectionsthan‚ÄúShaka‚Äù.Laterin
theevaluation,weinvestigatetheeffectofsensorocclusionquantitatively.
Differentfrommanypriordata-drivenhandposetrackingsystems[12,21,26],onenotablestrengthofour
systemisthatoursensingsystemisrelativelyindependentoftheuser(detailedresultsshowninSec.8.3.1),
becauseoursensingsystemsrelyonthemultipathechosreflectedbythehands,whicharelargelydeterminedby
theanatomicalstructureofhands.Becausehumanhandsaresimilaranatomicallywithminorvariationsinsize
andshape,theyleadtosimilarechoprofilesforthesamehandposeacrossusers.
5 IMPLEMENTATION:DEEPLEARNINGPIPELINE
As described above, echo profiles encode temporal (x-axis) and spatial (y-axis) information of reflection &
diffractionstrengths.Withthecalculatedechoprofilesasinputs,ourdeeplearningpipelinepredictsthehand
pose/gesturewithdataaugmentationtechniques.
5.1 LabelsforTraining/TestingHandPoses:GroundTruthAcquisition&Normalization
Tocapturegroundtruthfortrainingandevaluation,weuseMediaPipeHands[73]thatacquirethe3-dimensional
(3D)coordinatesof21keypoints,showninFig.9.Wesubtractthewristkeypointfromtheother20keypoints
toacquiretherelativegroundtruthjointpositionwithrespecttothewrist.MediaPipeHandsisalsousedasthe
groundtruthmethodin[12,26]toinfer3DcoordinatesbasedonRGBimages.Wenoticethedepthprediction
ofMediaPipeisnotprecise,butitisconsistentunderthesamelightingconditions.Comparedwithadepth
camera-basedgroundtruthacquisitionmethod,likeLeapMotion[56],MediaPipetracksoccludedjointsbetter.
Toaccountforhandpositionandorientationdifferencesacrosssessionsandparticipants,wenormalizethe
handorientation.Foreachdetectedhandpose,wefindtheplanedefinedbyvectors(a)startingfromthewrist
(joint0inFig.9)andendingattheindexfingermetacarpophalangeal(MCP)joint(joint5inFig.9),and(b)
startingfromthewrist(joint0inFig.9)andendingatthelittlefingerMCPjoint(joint17inFig.9).Wethen
re-alignthehandbyrotatingthepalmplanetothatofareferenceimage.Furthermore,toensurethesamehand
sizeforthesameparticipant,wenormalizethehandsizebasedonthephysicallengthmeasuredbetweenthe
wrist(joint0inFig.9)andendingatthelittlefingerMCPjoint(joint17inFig.9).
Thecamera(built-inofAppleMacbookAir2022)weusewithMediaPipesamplesat30fps,andourring
samplesat83fps,amuchhigherfrequency.Wesynchronizethegroundtruthwithoursensorsignalsbased
ontimestamps.Tominimizereal-timeinferencelagging,wepickthelastsensorreading‚Äôscorrespondinghand
coordinatesasthegroundtruthfortheechoprofilewindow.
5.2 ModelFramework
The trained deep learning model takes echo profiles as input and outputs either 3D hand joint coordinates
orclassificationlabels,dependingonthesensingtask.Theregressionandclassificationmodelssharesimilar
architectures.WepresentdetailedcomparisonresultsofdifferentalgorithmsintheSec.10.2.
Input: Thedifferentialechoprofile(capturinghandmovements)andoriginalechoprofile(capturingstatic
handposes)arestackedas2-channelinput.Thedimensionsoftheinputsare [2√ó54√ó100] and [2√ó54√ó160],
8Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.5. Encoder-decoderArchitectureforHandPoseTrackingandGestureClassification.Examplevisualizedinputshavethe
differentialechoprofilechannelinthefrontandtheoriginalechoprofilechannelintheback.
equivalenttoa1.2-second(100pixels)windowanda1.92-second(160pixels)windowfortheregressionand
classification models, respectively. Note, for the hand pose tracking study data collection, each pose lasts 2
seconds,sosomeinputinstancescapturethetransitionbetweenposes(posetrackingexample1inFig.5),while
otherinputscapturestaticposeswithoutmovements(posetrackingexample2inFig.5).Thus,theposetracking
modellearnsboththestaticposesanddynamictransitions.Forhandgestureclassificationdatacollection,each
gestureisperformedwithina2-secondwindow,slightlylongerthanthetraininginstance.Everycollectedgesture
hasitsownuniqueinputinstance.Theinputechoprofilesarenormalizedperchanneltoaccountforinconsistent
magnitudes.
Encoder-DecoderArchitecture: WeadoptResNet18[19]astheencoderbackbonethatoutputsafeaturevector
ofsize[512√ó4√ó10].Thedecoderconsistsofanaveragepoolinglayer(poolsize=[1,1]),adropoutlayer(ùëù =0.8),
andafullyconnectedlayer.Thefinalfullyconnectedlayereitheroutputs20jointsx3coordinates=60coordinates
forthehandposetrackingmodel,oroutputsthepredictedlabelforgestureclassificationmodels.
5.3 TrainingScheme
OurmodelsareimplementedinPyTorchandtrainedonanNVIDIAGeForceRTX2080Ti.
HandPoseTrackingModel. WeusetheAdamOptimizerwithalearningrateschedulerstartingat0.002.We
traintheregressionmodeltopredictthe20√ó3handjointcoordinatesusingthemean-squareerror(MSE)loss.
Wetraintheuser-independentmodelwith10epochsandabatchsizeof256.Wefine-tunethemodelwithanother
15epochsandabatchsizeof32.
GestureClassificationModel. WeusetheAdamOptimizerwithalearningrateschedulerstartingat0.0002.We
traintheclassificationmodeltopredictthegesturelabelusingthecrossentropyloss.Thebatchsizeis32.We
traintheuser-independentmodelwith120epochsandfine-tunethemodelwithanother120epochs.
5.4 DataAugmentation
Toimprovethemodel‚Äôsrobustnessagainstwornlocations,ringorientations,andhanddimensionvariations,we
applythefollowingin-placedataaugmentationtechniques:
‚Ä¢ Randomness:Appliedinbothposetrackingandclassificationmodels,random [‚àí5%,5%] increasesat80%
chanceineachpixeloftheechoprofilethatintroducesnoisetohandgeometry.
‚Ä¢ Vertical Shift: Applied in both pose tracking and classification models, vertical shifts account for the
differentringpositionsrelativetothehand.Allinputechoprofilesarerandomlyverticallyshiftedby¬±3
pixels,equivalentto¬±10.02mm.
9Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
‚Ä¢ HorizontalShift:Appliedonlyintheclassificationmodels,horizontalshiftsaccountforinconsistentreaction
timesthattheparticipantstaketostartperformingthegesturewhenrequested.Theinputechoprofilesare
randomlyhorizontallyshiftedby [‚àí13%,13%] pixelsat80%chance,equivalentto¬±0.15s.Horizontalshifts
arenotneededforhandposetrackingbecausethemovementsarecontinuous.
6 EVALUATIONOVERVIEW
ToextensivelyevaluateRing-a-Pose‚Äôscontinuoushandposetrackingandrecognitionperformance,weconducted
threeuserstudiesapprovedbytheInstitutionalReviewBoard(IRB):(1)thecomparisonontheperformance
ofdifferentwornfingers,detailedinSec.7,(2)continuoushandposetracking,detailedinSec.8,and(3)
microgesturerecognition,detailedinSec.9.
Thefirstuserstudyaimstoassesstheimpactsofwornfingerchoiceontrackingperformance.Becausethe
deeplearningmodelsarefinger-dependent,weselectedafinger,themiddlefinger,forin-depthevaluation.The
sensingtaskswereseparatelyevaluatedtoavoiduncomfortablylongstudiesfortheparticipantsandeachstudy
hadaUS$15orUS$20compensation,dependingonthestudylengths.Thegoalsofthelattertwostudiesareto
assess:
‚Ä¢ Ring-a-Pose‚Äôsstabilitywithinaparticipantsothatastheuserremovestheringandputsitbackon,no
additionaltrainingdataisneeded:Weaddressthisbyaskingtheparticipanttoremounttheringbefore
eachsession;
‚Ä¢ Ring-a-Pose‚Äôsgeneralizabilityacrossparticipantswithdifferenthandshapesandhandmovement
patternssothatnoorlittletrainingdataisneededfromanewparticipant:weaddressthisbyevaluating
user-independentandfine-tunedmodelsperformance;and
‚Ä¢ Ring-a-Pose‚Äôsrobustnesstonoisefactorsforreal-worlduses:weaddressthisbytestingscenarioswith
variouswrist&forearmorientations,sounds,andmovements.
Attheendofeachstudy,theparticipantcompletedasurveyinquiringabouttheirage,height,weight,and
experiencewiththering.Theresearcheralsomeasuredthelengthandsizeoftheirfingers.Intotal,weconducted
aseriesof3studieswithatotalof30differentparticipants(14self-identifiedasmale,16female,meanage=23.1,
stdage=3.5),includingavarietyofhanddimensions,detailedinTable2.Twoofthethirdstudyparticipants
participatedinthefirstandsecondstudies,respectively.Notethehandlengthisthemeasurementbetweenthe
tipofthemiddlefingerandthecenterofthewrist.
Table2. Statisticaldetailsofparticipants‚Äôhanddimensions.
MiddleFingerRingSize HandLength(cm) Height(cm) Weight(kg)
mean 6.2 17.9 169.7 62.0
std 1.4 2.1 9.5 10.4
max 10.0 22.0 189.0 93.0
min 4.0 14.0 153.0 44.0
Overall,acrossall3studies,theparticipantsfeltcomfortablewithwearingthering(Median=4onthe5-point
Likertscale;1=veryuncomfortable,5=verycomfortable).46%oftheparticipantscouldhearthesoundswhen
theyperformedthegestures,mostlywhenthehandwasnearthefistpose,butthesounddidnotbotherthem:
Median=4onthe5-pointLikertscale(1=veryuncomfortable,5=verycomfortable).
7 USERSTUDY1:WORNFINGERCOMPARISON
Asanaccessory,aringcanbewornondifferentfingersbasedonaestheticpreferencesand/ortoindicatemarital
status.Thus,whendesigningasensingringforwideadoption,itiscrucialtoinvestigatetheperformanceofthe
10Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.6. Thetwentyterminalhandposesevaluatedinourhandposetrackingstudy.Posesarelabeledblueorgreenbased
onwhetherthehandgeometriesoccludethesensorwhentheringiswornonthemiddlefinger.Thefourrowsshow(1)
referenceimagesdisplayedduringtheuserstudy,(2)exampleMediaPipegroundtruthsofaparticipant,visualizedusing
MANO[48],(3)examplepredictionsusingthefine-tunedmodel,and(4)examplepredictionsusingtheuser-independent
model.(3)and(4)sharethesametimestampsas(2).
ringsensingsystemonvariouswornfingers.It‚Äôsworthnotingthatthemajorityofprevioussingle-ringsensing
systemsonlytestedthesystemononefinger[58].ToevaluateRing-a-Pose‚Äôsperformanceonallpossibleworn
fingers,weconductedauserstudywith10participants,whereweinvestigatedRing-a-Pose‚Äôsuser-independent
andfine-tuneduser-dependenttrackingperformanceoneachfinger.
7.1 PoseSet
Becausewetestedallfivefingersoneachparticipant,toensureacomfortablestudyduration,welimitedtoa
smallposeset.Weselected(a)4isolatedindividualfingermovements:‚ÄúASL4‚Äù(equivalenttobendingthethumb),
11Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
‚ÄúRBent‚Äù,‚ÄúMBent‚Äù,and‚ÄúIBent‚ÄúinFig.6;and(b)5compoundfingermovements:oddASLdigitsbetween0and9.
Betweeneach2poses,thehandreturnstoaneutralposition(‚ÄúASL5‚Äù).
7.2 Procedure
7.2.1 Studysetup. Weconductedauserstudywith10participants.Theuserstudywasconductedinanexperi-
mentalroomonauniversitycampus.Atthebeginningofthestudy,oneresearcherexplainedthestudytask
andtheuserinterface,whichwasdisplayedonalaptopmonitorplacedonthedesk.Participantssatindesk
chairsduringthestudywiththeirelbowsonthetabletoensurethatthecameracouldcapturetheirhandsfor
ground-truthpurposes.
Asmartphone(XiaomiRedmiNote10Pro)wasusedtoreceiveandsaveBLE-transmittedacousticdatafrom
thering.Participantswereinstructedtoindependentlyweartheringontheirrightmiddlefingerbeforeeach
session,followingtheexperimenter‚Äôsguidanceforalignment.Groundtruthdataforhandposewascollected
usingthebuilt-inlaptopcamera(AppleMacbookAir,2022)positionedabout55cmawayfromthehandonthe
tableandpoweredbyMediaPipe.Theexperimenteradjustedthecameraangletoensurethatthepalmsstayed
paralleltothecameraforreliablegroundtruths.Thestudylastedabout90minutes.
7.2.2 DataCollectionSessions. Eachparticipantunderwent21sessions:1practicesessionfollowedby4sessions
for each finger in a randomized order ( 1 practice + 4 sessions * 5 fingers = 21 sessions). Participants were
instructedtoindependentlyweartheringontheselectedfingerbeforeeachsession,followingtheexperimenter‚Äôs
guidanceforalignment:centeringtheinneredgeofthespeakerwithrespecttotheselectedfinger.Withineach
session(2.7min),eachposeisperformed9timesinarandomizedorder:9√ó9=81terminalposes.Areference
poseimagewasdisplayedonthemonitorfor2secondsforeachposeasthevisualstimuli.Within2seconds,the
participant‚Äôshandleavestheneutral(‚ÄúASL5‚Äù)positiontothereferencedposeandreturnstotheneutralposition.
Foreachfinger,wecollect4sessions√ó2.7min=10.8minofdatacontaining4sessions√ó81terminalposes=324
terminalposeinstances.
7.3 Results
EvaluationMetrics. Weadoptthemeanper-jointpositionerrorMPJPEtobeourquantitativeevaluationmetric:
themeanEuclideandistanceerrorsofall20relative(tothewrist)jointpositions.Note,MPJPEmeasuresdistance
errorssoitdependsonthehandsize.Forexample,inthecasewherethegroundtruthisanopenpalmandthe
predictionisaclosedfist,alargerhandhasalargerMPJPEthanthatofasmallerhand.Analternativeevaluation
metric is the mean angular error, invariant to the hand size, but due to MediaPipe Hands‚Äô unreliable depth
predictions,wechoseMPJPE.ToaccountforMPJPE‚Äôsdependencyonthehandsizeintheuser-independent
models,wenormalizethepredictedhandwiththeparticipant‚Äôsphysicalhandsize:asinglemeasurementmade
fromthewrist(joint0inFig.9)tothelittlefingerMCPjoint(joint17inFig.9).Inreal-worlduses,itisan
additionalstepanewuserneedstodowhenreceivingthedevice.Forfine-tuneduser-dependentmodels,such
informationisnolongerneededasthemodeleasilyandquicklylearnsthehanddimensionasshowninthe
convergenceofMPJPEwithonly2.67minofdatainFig.8(c).
WecompareRing-a-Pose‚Äôsperformanceoneachfingerwithuser-independentmodelsandthefine-tuned
user-dependentmodels.Foreachfinger,toevaluatetheuser-independentperformance,weusetheleave-one-
participant-out(LOPO)cross-validation(97.2minoftrainingdatafromotherparticipants);andtoevaluatethe
fine-tuneduser-dependentperformance,wefine-tunetheuser-independentmodelwithdatafromthefirstthree
sessions(8.1minoffine-tuningdata)andtestonthelastsession.InadditiontotheMPJPEoftheentirehand,we
12Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.7. WornFingerComparisonStudyResults.(a)Ring-a-Pose‚Äôstrackingperformancewhenwornondifferentfingers.(b)
Trackingperformancebreakdownoneachtrackedfingerwhenwornondifferentfingers.Thelegenddenotesthetracked
fingerandmodeltype.Forexample,‚ÄúthumbUI‚Äùand‚ÄúthumbUD‚Äùaretheuser-independentandtheuser-dependenttracking
errorsonthethumb,respectively.Errorbarsinthisfigurerepresentthestandarddeviation.
furtherbreakdowntheresultsintotheMPJPEsofeachfingertoanalyzewhetherthewornfingeraffectsthe
trackingaccuraciesofindividualfingers.
Ring-a-Posehassimilarperformanceswhenwornonallfivefingers.TheresultsaredetailedinFig.7(a).For
user-independentmodels,theresultsrangefrom9.2mm(thumb)to10.7mm(indexfinger).Foruser-dependent
models,theresultsrangefrom5.6mm(thumb)to6.1mm(middlefinger).Wefurtherbreakdowntheindividual
trackedfinger‚Äôsperformance,showninFig.7(b):thewornfingergenerallytracksitselfbetterthanotherfingers
do.Themultivariateanalysisofvariance(MANOVA)withmodeltypeandwornfingersasindependentvariables
andindividualtrackedfingererrorsasdependentvariablesshowedthatthewornfingerhasasignificanteffect
(p=0.008<0.05)onindividualtrackedfingererrors.WethenfollowuptheMANOVAtestwithindividualANOVA
testsforeachtrackedfinger,wefindthethumb(p=0.001<0.05),ringfinger(p=0.020<0.05),andthepinkyfinger
(p=0.015<0.05)dependonthewornfinger,andtheothertrackedfingersdonot.Butinmostpracticalcases,we
trackthehandasawhole.Atwo-wayrepeatedANOVAtestonmodeltypesandwornfingersshowedthatthe
wornfingerfactorisnotstatisticallysignificant(ùëù =0.08).
Insummary,whileRing-a-Posehascomparableperformanceacrossallwornfingersforfull-handtracking,it
tracksindividualfingersdifferentlybasedonthewornfinger.
7.4 Discussion
Theresultimpliesthatforageneral-purposehand-posetrackingsolution,thewornfingerlocationdoesnot
matter:unlikemostpriorworks[58],usersarefreetochoosewhichfingertoweartheringbasedontheirown
preferenceswithoutperformancelimitation.However,iftheapplicationdemandshighertrackingaccuracies
oncertainfingers(e.g.,thumbandindexfingerforpinchdetections),thewornfingerlocationcanalterthe
performance.
8 USERSTUDY2:CONTINUOUSHANDPOSETRACKING
Inthissection,wedetailtheproceduresandfindingsfromthecontinuoushandposetrackingstudy.
8.1 PoseSet
Tocaptureawiderangeofhandposesandmovements,weselectasetof20terminalposes,showninFig.6.
Informed by prior works [12, 21, 26, 68], our pose set includes all 10 American Sign Language (ASL) digits
evaluatedinOpisthenar[68]andall11posesevaluatedinpriorwork[26].
13Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
8.1.1 NaturalTransitionbetweenPoses. Justlikeourfirstuserstudy,manypriorworksrelatedtocontinuous
handposetracking,suchasthosecitedin[21,31,66],necessitatethatparticipantsreverttoaneutralhandpose
betweentransitioningtodifferentposes.However,thisrequirementdeviatesfrommanyreal-worldscenarios
whereindividualsseamlesslymovefromoneposetoanotherwithoutreturningtoaneutralposition.
Trackingposescontinuouslywithouttheneedtoreturntoaneutralposeposessignificantlygreaterchallenges
fordata-drivenpose-sensingsystems.Thisisprimarilybecauseitexponentiallyincreasesthenumberofpotential
handshapesandposesbetweentwotargetposes,demandingamuchlargervolumeoftrainingdata.Toillustrate,
ifthereare20targethandposes,therewillbeatotalof20√ó19=380transitionpathsifindividualsdonotreturn
toaneutralpose,incontrasttoonly20transitionpathswhenreturningtoaneutralpose.Additionally,based
onpreliminarystudiesconductedbyresearchers,itwasobservedthatincludingtheneutralposebetweentwo
requestedposessubstantiallyreducesreconstructioncomplexity,resultinginasmallersetoftransitionpaths,and
cutsthemeanper-jointpositionerror(MPJPE)inhalfduetothesignificantlyfeweruniquetransitionposepaths.
Tocloselyreplicatethereal-worldprocessofperforminghandposesandshowcasetheremarkableperformance
ofourpose-trackingsystem,wedeliberatelyoptedforthemorechallengingtaskofinstructinguserstoperform
handposeswithoutrevertingtotheneutralpose.
8.1.2 TheimpactofSensorOcclusionbyFingers. Toinvestigatetheimpactofsensorocclusionbythefingers,
wedividetheposesetbasedonwhetherthering,wornonthemiddlefinger,ispartially(e.g.‚ÄúASL8‚Äù)oreven
fully(e.g.‚ÄúFist‚Äù)occludedbythehandgeometry:(a)NO-poses,no-occlusionposescoloredingreen,and(b)
WO-poses,with-occlusionposescoloredinblueinFig.6.Further,wepurposelyincludeterminalposeswith
similarocclusions(e.g.‚ÄúShaka‚Äùv.s.‚ÄúThumbUp‚Äùand‚ÄúShoot‚Äùv.s.‚ÄúILoveU‚Äù)toprobeRing-a-Pose‚Äôsperformance
withocclusions.
8.2 StudyProcedure
8.2.1 Studysetup. Werecruited12participantsontheuniversitycampusforthisstudy.Thestudysetupis
similartothatinthefirststudy,exceptalldataarecollectedwiththeringwornonthemiddlefinger.
8.2.2 DataCollectionSessions. Eachparticipantunderwent25sessions,andwithineachsession,theywere
taskedwithperformingthreesetsofgestures:1)all20poses;2)11No-poses;and3)9WO-poses.Theorderof
theseterminalposes,withineachsetofposes,wasrandomized.Betweentwoterminalposes,transitionposesare
alsorecordedforevaluation.Toassistparticipantsinperformingthetargetposes,areferenceposeimagewas
displayedonthemonitorfor2secondsforeachpose,accompaniedbyaprogressbarasavisualstimulus.These
25sessionscanbecategorizedintothreesections.
1)PracticeSection(Sessions1-2,2.56mineach):Thefirsttwosessionsweredesignedaspracticesessionsto
helptheparticipantsgetfamiliarwiththetargetposesandthedatacollectioninterface.Thedatafrompractice
sessionswerenotusedintheevaluation.
2)MajorTestingSection(Sessions3-14,2.56mineach):Duringthesubsequent12sessions,theprocedure
closelymirroredthatofthepracticesessions.Ineachofthesesessions,eachparticipantperformed2√ó(20+11+9)
=80poses.Intotal,oneparticipantperformed12√ó2√ó(20+11+9)=960terminalposesinthemajortestingsection.
3)RobustnessTestingSection(Sessions15-25,1.28mineach):Intherobustnesstestingsection,wesoughtto
evaluateoursystemunder11distinctconditionsthatcouldpotentiallyaffectitsperformanceinreal-worldsettings.
Eachsessionwasspecificallydesignedtoassessonecondition.Unliketheprevioussessions,theparticipantonly
performsthethreesetsofgesturesoncepersession,leadingto1√ó(20+11+9)=40terminalposeinstancesper
session.These11sessionsrepresentthefollowingscenarios:
(1) Pose-Neutral-Pose:Betweeneachpromptedpose,thehandreturnstoaneutralposition(‚ÄúASL5‚Äù).
14Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.8. UserInpendentandUser-DependentFine-TunedHandPoseTrackingResults.(a):Ring-a-Pose‚Äôsperformswellon
bothposesthatfully/partiallyoccludethesensor(WO-poses)andthosethatdonot(NO-poses).(b):OverallPer-JointError
distribution.(c):Withmoretrainingdatafromanewuser,thesensingperformanceincreases.Themodelquicklylearnsthe
physicaldimensionofthehandinjust2.5min.Errorbarsinthisfigurerepresentthestandarddeviation.
(2) UncontrolledHandMovement:Participantsmovedtheirhandsfreelyinfrontofthescreenwhileperforming
theposes,aslongastheirhandsstayedparallelandinsidethecameraviewforgroundtruthacquisition.
(3) EnvironmentalAcousticNoise(MusicPlaying):Participantsplayedmusicoftheirchoiceusingtheirphone‚Äôs
speakeratavolumetheytypicallylistentomusicat.
(4) Talking:Participantschosetotalkeitherwiththeexperimenterormonologuethroughoutthesession.
(5) RadialDeviation:Wriststayedintheradialdeviationorientation.
(6) UlnarDeviation:Wriststayedintheulnardeviationorientation.
(7) Flexion:Wriststayedintheflexionorientation.
(8) Extension:Wriststayedintheextensionorientation.
(9) Neutral:Theforearmstayedintheneutralorientation.
(10) Supination:Theforearmstayedinthesupinationorientation.
(11) Hand-Down:Therighthandpointeddownwardsnexttotheparticipants‚Äôlegs.
Intheend,wecollected11sessions√ó40terminalposespersession=440terminalposeinstancesinthis
robustnesstestingsection.Duetoahardwaremalfunction,1participant‚Äôsdatawasdamagedandthisparticipant
wasinvitedtoparticipateintheuserstudyagain.Intotal,wecollected531minsofpose(eachparticipant44.25
minx12participants)containing22080(12participantsx(960+440)=22080)terminalposeinstances,whichare
savedtoevaluatetheperformanceasdetailedinlatersections.
8.3 Results
Usingthedatacollectedfromtheuserstudy,weevaluateRing-a-Pose‚Äôsperformancewith(a)user-independent
modelssimulatinganout-of-the-boxusecase,(b)fine-tuneduser-dependentmodelssimulatingprovidingsome
calibrationdatafromtheuser,and(c)noisefactorslikewrist&forearmorientations,sounds,andmovements
simulatingreal-worlduses.Theevaluationmetricisthesameasthefirststudy:meanper-jointpositionerror
MPJPE.Forthenoisefactors,wefirstfitalinearmixed-effectsmodelwiththemodeltypeandtherobustness
testingconditionasindependentvariables,andthenwefollowwithapost-hocDunnett‚Äôstestforcomparisons
withacontrol(i.e.,themajortestingcondition).Notewedonotevaluatewithin-sessionperformancebecauseRing-
a-Posegeneralizeswellwithremounting.Aswedescribedearlier,eachsessionduringthedatacollectionlasts
about2.67and1.33minutes.Theringisremountedbeforeeachsession.
8.3.1 User-IndependentPerformance. Weusealeave-one-participant-out(LOPO)cross-validationtosimulate
whenthenewuserdoesnotprovideanycalibrationdata.Foreachparticipant,inadditiontothedatafromthe
15Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.9. JointerrorsBreakdown.Thelabelindicesin(a)maptothemediapipejointlabelsin(b)thatshowMediaPipehand
jointlabels.Errorbarsinthisfigurerepresentthestandarddeviation.
other11participants(43.97minx11=8.06h),wealsoadddatacollectedfrom5researchers(9.96h).Intotal,18.02
hoursoftrainingdatafrom16handsareused.
Overall, we achieved a mean MPJPE of 14.1 mm (SD=5 mm) across 12 participants. The impact of sensor
occlusionisshowninFig.8(a).Theerrordistributionofjointdistanceerrorsisshownastheorangelinein
Fig. 8(b). We further break down the result into individual joints as shown in orange bars in Fig. 9 (a). Not
surprisingly,Thejointsthatarefurtherawayfromthewristexhibitlargererrors.
8.3.2 Fine-tunedUser-DependentPerformance. Humanhandsvaryinsize,shape,andmovementpatternsacross
differentpeople.Basedontheuser-independentmodelstrainedabove,wefine-tunethemodelswithdifferent
amounts of training data from the new participant. The additional training data were not seen in the base
model.Additionaltrainingdatafromanewparticipantimprovesthetrackingperformance,asshowninFig.8(c).
With26.7minofadditionalfine-tuningdata,theoverallMPJPEdecreasesto10.3mm,3.8mmsmallerthanthe
user-independentmodel.Examplehandpredictionsareincludedinthe3rdrowsofFig.6.InFig.8(b),theblueline
showstheerrordistributionofjointdistanceerrors.ThebluebarsinFig.9(b)showtheper-jointimprovement:
theperformanceincreasesmoreforthejointsthatarefurtherawayfromthewrist.AsillustratedinFig.8(c),the
quickMPJPEconvergencebetweenwithandwithoutnormalizationshowsthatthemodelquicklylearnsthe
physicalhanddimensionwithjust2.67minofdata.Further,theslopesinFig.8(c)decreasesasmoredataisadded,
buttheydonotyetflatout.Thisindicatesthatwithmorefine-tuningdata,thereisstillroomforimprovement.
8.3.3 The impact of Sensor Occlusion by Fingers. As mentioned above, to investigate the impact of sensor
occlusionbythefingers,wedividetheposesetsNO-poses(Noocclusion)andWO-poses(fully/partiallyocclude
thesensor)basedonwhethertheringispartiallyorfullyoccludedbythehandgeometry.Fig.8(a)showsthe
trackingperformanceonthetwoposesets.Weperformedatwo-wayrepeatedANOVAtest,withthemodel
type and occlusion conditions as the independent variables, and did not find statistically significant effects
(p=0.33)oftheposetypeonthetrackingperformance,andtherewerenointeractions(p=0.27)betweentheuser-
independent/dependentmodelsandNO-/WO-poses.
8.3.4 RobustnesstoWrist&ForearmOrientations. Aclearbenefitofusingtheringtotrackhandposes,compared
withwrist-worndevices,istheminimizeddiscrepancybetweenthehandorientationandthesensororientation.
Theuser-independentmodelistrainedwithbothneutralwristpositiondataandallotherwristorientation
datafromotherusers.Theuser-dependentmodelisonlyfine-tunedwithdatacollectedintheneutralwrist
orientation.Wethentestthemodelswithdatacollectedfromsessionswiththe6extremeorientationsasshown
inFig10(b).ThewristorientationcolumnsinFig.10(a)showthat,exceptforsupination(p=0.003<0.05from
theDunnett‚Äôstest),theuser-independentmodelperformssimilarlyacrossallorientations:0.55mmdecrease
16Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig. 10. Ring-a-Pose‚Äôs Robustness to Noise Factors. (a) Using models trained with data collected in the same setting,
thetestingresultsondatacollectedinvarioussettingsshowthatRing-a-Poseisrobusttomanynoisefactors.(b)Wrist
Orientations.Errorbarsinthisfigurerepresentthestandarddeviation.
in performance without statistical significance (p-value range: 0.36-0.99). The user-dependent models have
smalldecreases(mean=2.4mm)inperformanceandallbuttheulnarwristorientation(p=0.10)havestatistical
significance(p-valuerange:0.000-0.001).Forthesupinationorientation,MPJPEhasbigincreasesforboththe
user-independentmodelandthefine-tunedmodels.Duringtheuserstudies,thisorientationwashardtoperform
tokeepthepalmparalleltothescreen.Inthedetectedgroundtruths,wealsonoticedmanyframeswithvisually
incorrecthandposes.Thus,theincreaseinerrorsmaynotaccuratelydepictoursystem‚Äôsperformance.
Furthermore,wetestedtheperformancewhenthehandisdownonthesideofthebody(‚ÄúHandDown‚Äùcolumn
inFig.10(a)),amorenaturalinteractionsite.Theuser-independentmodelperformedsimilarly(p=0.97)tothatin
thetrainingorientation,butthefine-tunedmodel(p<0.001)performancedegradedtothatoftheuser-independent
model.
Insummary,weconcludethatthewrist&forearmorientationshavelittleimpactonthemoregenericuser-
independentmodel,buthaveaperformancedegradationforthefine-tunedmodel.
8.3.5 RobustnesstoSounds. BecauseRing-a-Poseleveragesactiveacousticsensing,itisnecessarytoevaluate
theperformancewithenvironmentalsounds.Thetwoselectedsoundnoisescenariosarevisualizedinthe‚ÄúMusic‚Äù
and‚ÄúTalking‚ÄùcolumnsinFig10(a).Withtheparticipant‚Äôschoiceofmusicplayedattheirpreferredvolume,we
donotseeperformancedegradationforboththeuser-independent(p=0.94)and-dependent(p=1)modelswhen
comparedtotheresultinthetrainingsettingshowninFig10(a).However,whentheparticipanttalksasthey
performthegestures,weseeasmalldegradationwithoutstatisticalsignificance:0.4mmand1.7mmincreasein
MPJPEsfortheuser-dependent(p=1)anduser-independent(p=0.08)models,respectively.Weareunsureabout
thecauseofthedifferenteffectsbroughtbymusicandtalking,butregardless,thesystemstillperformswellwith
environmentalsounds.
8.3.6 RobustnesstoMovement. Sincepeople‚Äôshandsmovewhentheyspeak,gesture,andinteractwithspatial
interfaces,weevaluateoursystemunderthesescenarios.The‚ÄúHandMovement‚ÄùcolumninFig.10(a)showsthat
themovementsslightlyharmtheperformanceby0.5mmforauser-independentmodel(p=0.99)butyieldsan
additional1.8mmincreasefortheuser-dependentmodel(p=0.03<0.05).
8.4 Discussion
TosituateRing-a-Pose‚Äôscontinuoushandposetrackingperformancewithotherwearablesolutions,wecompare
itsperformancewiththatofpriorworksinTable3.BecauseRing-a-Poseisthefirstsingle-ringsystemthattracks
continuoushandposeacrosssessionsandusers,wecomparetheperformancewithmulti-ringsystems[77],
17Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Table3. Continuoushandposetrackingperformancecomparison.Inthe‚ÄúPose/GestureSet‚Äùcolumn,‚Äúgestures‚Äùreferto
whenaneutralhandposeisrequiredbetweentransitioningtodifferentposes,and‚Äúposes‚Äùrefertosuchneutralposeisnot
requiredbetweentransitions.Intheerrorcolumns,‚ÄúSD‚Äùstandsforsessiondependence,‚ÄúUD‚Äùstandsforuserdependence,
and‚ÄúUI‚Äùstandsforuserindependence.Allerrorsrefertomeanper-jointpositionerror(MPJPE).
SD UD UI
System Devices Pose/GestureSet Error Error Error
(mm) (mm) (mm)
ssLOTR,2022[77] 5ringsandawristband freemovements ‚Äî 6.55 ‚Äî
WR-Hand,2021[31] anarmband 11gestures+3freemotions ‚Äî ‚Äî 18.57
FingerTrak,2020[21] awristband 19gestures 12 27.2 ‚Äî
EchoWrist,2024[27] awristband 18gestures ‚Äî 4.81 12.2
DiscoBand,2022[26] awristband 10poses 11.69 17.87 19.98
Z-pose,2023[61] aring 10poses 8.5 ‚Äî ‚Äî
Ring-a-Pose aring 20poses ‚Äî 10.3 14.1
armbands[31]andwristbands[12,21],inadditiontosingle-ringsystems[61].Althoughwecandirectlycompare
theMPJPEs,theyareheavilyaffectedbythepose/gesturesets(detailedinSec.8.1.1),theevaluationcondition,
and the amount of training data. Regardless, Ring-a-Pose demonstrates promising session-independent and
user-independentperformanceswhencomparedwithothersingle-devicesystems.Ring-a-Posefallsshortofthe
multi-devicesystemthatcontains5ringsandawristbandbutgreatlyalleviatestheburdenofwearingasensing
ringoneveryfinger.
Ouruser-independentmodelgeneralizeswellacrossdifferenthandsizes/shapesandisrobusttonoisefactors.
Thefine-tunedmodelindeedbenefitsfromuser-specificdata.Bothmodelsshowresiliencetothenoisefactors,
butcomparedwiththeuser-independentmodel,thefine-tunedmodelismorevulnerabletonoisefactors.One
reasonforthisisthattheuser-independentmodelistrainedwithsome"noise"(usingboth12longsessionsand
11shortsessions),butthefine-tunedmodelisnotfine-tunedwith"noise"(onlyusingthe12longsessions),so
includingmorediversetrainingdatafortrainingfine-tunedmodelswillbehelpful.Inthisevaluation,weisolate
the"noise"factorsforcontrolledevaluation.Futureworksonin-the-wildstudieswillhelpusunderstandthe
systembetter,butitischallengingtoacquiregroundtruthdatainthewild.
9 USERSTUDY3:THUMB-TO-INDEXMICRO-GESTURERECOGNITION
The previous showed Ring-a-Pose continuously track the hand poses effectively. In this study, we further
evaluateRing-a-Pose‚Äôsperformanceontrackingfine-grainedmicro-gestureswhicharemorediscreet,intuitive,
andnatural[13].
9.1 Thumb-to-IndexMicro-GestureSet
Amongvariousmicrogesturesets[26,39,52],wechosethumb-to-indexmicro-gestures[4,9,15]duetotheirease
ofperformanceandsocialacceptance[6].ShowninFig.11,wechose7gestures,includinga‚ÄúRest‚Äùno-gesture
class.Similartothatinthepreviousstudy,eachgesturestartedfromthe‚ÄúRest‚Äù,movedthethumbandindex
finger,andreturnedtothe‚ÄúRest‚Äùposition.
9.2 Procedure
Weconductedauserstudywith10participants,inwhich2participantseachparticipatedin1ofthe2previous
studies.Thestudylastedabout60minuteswithasimilarprocedureasthethirdstudy.Eachparticipantperformed
18Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.11. Thesevenmicrothumb-to-indexgesturesevaluatedinouruserstudy.Toprow:referenceimagessimilartothose
displayedduringtheuserstudy.Middlerow:Exampledifferentialechoprofilesfromparticipant3.Bottomrow:Example
differentialechoprofilesfromparticipant9.
Fig.12. Micro-FingerGestureRecognitionStudyResults.Forthe7-classclassificationtask,theuser-independentmodel
achievesanaccuracyof90.60%.Theuser-dependentmodelachievesanaccuracyof99.27%with9.3minoftrainingdata.
10sessionsofgestures,andthefirstsessionwasdiscardedasapracticesession.Beforeeachsession,theparticipants
remountedtheringbythemselves.Fromeachparticipant,wecollected:7gestures √ó10repetitions √ó9sessions =
630instances.Theparticipantswereseatedthroughoutthestudywiththeirhandsnaturallyplacedonthetable.
9.3 Results
Similartothatinthepreviousstudy,weuseLOPOcross-validation(with3.15hoftrainingdata)toevaluate
theuser-independentperformanceforthe7-classclassification.Across10participants,Ring-a-Posehasamean
accuracyof90.60%(confusionmatrixdepictedinleftinFig.12).Wethenfine-tunetheuser-independentmodels
withtheparticipant‚Äôsdata.Themeanaccuracyquicklyincreasesto96.6%with2.3minoftrainingdataand99.27%
with9.3minoftrainingdata(confusionmatrixdepictedinrightinFig.12).Wefurthercomparetheresultswith
priorringsystemsthatrecognizethumb-indexmicro-gesturesinTable4.Thepromisingresultsdemonstrate
thatRing-a-Posedetectsfine-grainmovementseffectively.
10 DISCUSSION,LIMITATIONSANDFUTUREWORK
Whileoursystemeffectivelytrackshandpositionscontinuouslyandrecognizesdiscretegestures,showcasing
advantagessuchasresiliencetovaryingnoisefactorsandwristorientations,etc.,itstillhasshortcomingsthat
19Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Table4. Thumb-to-indexmicro-gesturerecognitionperformancecomparison.‚ÄúUD‚Äùstandsforuserdependenceand‚ÄúUI‚Äù
standsforuserindependence.
System GestureSet UDAccuracy UIAccuracy
Thumb-in-Motion,2018[4] 5gestures ‚Äî 89 %F1
Z-ring,2023[60] 9gestures 88 % 83.67%
EF-ring,2023[9] 9gestures 89.5 % 85.2 %
Ring-a-Pose 7gestures 99.27% 90.60% 90.58%F1
mustberesolvedbeforelarge-scaledeploymentinreal-worldsettings.Inthissection,wediscussthelimitations
ofRing-a-Poseandthechallengesandopportunitiesofbroadersmartringsystems.
10.1 Real-TimeTrackingDelay.
Ourcurrentsystemisdeployedonasmartphoneforreal-timeinference.However,asthedemovideoshows,we
observeanoticeabledelay.Thoughwealreadyalleviatedthedelaybyusingthelastframeintheslidingwindow
asgroundtruth,thetransmissionandcomputingtimesarenotnegligible.Wemeasuredtheaverageddelayusing
(a)theringthatcapturesandsendstheacousticsignalstoasmartphoneviaBluetooth;(b)asmartphone(Redmi
Note12Pro)thatreceivesthesignals,decodestheechoprofiles,makesinferenceswithPyTorchMobile1,and
sendsthepredictiontoacomputerviaWiFi;and(c)acomputer(AppleMacbookAir,2022)thatreceivesand
processesthepredictionsforvisualizationorcontrolpurposes.
Table5. Real-timeTrackingDelayTimeBreakdown.
Step BLEtransmission EchoProfileCalculation Inference WiFiTransmission HandVisualization
Time(ms) 200.0 14.7 54.6-70.4 16.7 126.9-190.5
WereportinTable5thetimeforeachstep.Forhandposetracking,wefoundatotaldelaytimeof413-492ms
dependingontheprocessingavailabilityofthephoneandthecomputer,inwhich126.9-190.5msattributedto
thehandvisualization.Thevisualizationtimeislongbecauseitusesaninversekinematicsolverandrendersa
high-fidelityhandin3-dimensions.Thevisualizationtimecanbeomittedifthejointcoordinatesaredirectlyused.
Whiletheecho-profilecalculationandinferencetimes,69.3-85.1ms,areshort,theunavoidableBLEtransmission
takes200ms.TheselectedAndroidphonehashighBluetoothlatencyasaresultofthetradeoffwithreliablehigh
bandwidth.Inthefuture,thisdelaytimecanbefurtherreducedwithdirecthardwarecontrol(e.g.nRF52840
Dongle)oradvancedBLEhardware/protocols,orcompressingdataforamuchsmallertransmissionpackage.
10.2 ModelandModelInputSelection.
Usingtheuser-independentanduser-dependentevaluationdatacollectedandsimilartrainingschemesinthe
firstuserstudy,wecompareourcurrentapproachwiththealternatives.
10.2.1 ModelComparisonStudy. InadditiontotheadoptedResNetbackbonemodel,weexperimentedwith(a)
morecomplexmodels,suchasaddingRNN(GRU/LSTM)layersattheendofResNetencoderandreplacingthe
ResNetencoderwithatransformerencoder;and(b)simplermodels,likea3-layerCNN.Weshowtheresultsin
Fig.10.2(a):ResNet(13.99mmforuser-independent,10.05mmforuser-dependent)andResNet+RNN(13.89mm,
10.08mm)similarperformance.Thesimple2DCNNhasequallygooduser-independentperformance(13.99mm)
1https://pytorch.org/mobile/home/
20Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
Fig.13. Comparativestudiesonthedeeplearningmodelandmodelinputs.(a):Echoprofileablationstudy:‚ÄúOrig‚Äùdenotes
theoriginalechoprofileand‚ÄúDiff‚Äùdenotesthedifferentialechoprofile.(b):Modelcomparisonstudy:Resnet,ourcurrent
approach,andResNet+RNNhavebetterperformancesthansimple2DCNNandtransformer.(c)Slidingwindowlength
comparisonstudy:areasonableincreaseinthewindowsizeincreasestheperformance.(d)Sensingrangecomparisonstudy:
sensingrangehaslittleimpactonthesensingperformance.
butmuchworseuser-dependentperformance(11.64mm).Thetransformermodelhastheworstperformancefor
bothindependent(16.75mm)anddependent(16.17mm)evaluations.WeconjectureResNet‚Äôsgoodperformanceis
aresultofechoprofiles‚Äôefficient2Drepresentationoftimeanddistance,sotheadditionaltemporallearning
isunnecessary.ResNetwaspickedforthesmallesterrorsandthelowercomputationalcosts,comparedwith
ResNet+RNN.
10.2.2 EchoProfileAblationStudy. Weusedthecombinationoforiginalanddifferentialechoprofilestocapture
absolutehandshapesandfingermovements.Inthisablationstudy(Fig.13(b)),wetrainandtestwithonlythe
originalordifferentialechoprofiles.Thetwo-wayrepeatedANOVAtestwithmodeltypeandinputtypesas
independentvariablesfurthershowsstatisticalsignificance(p=0.000<0.5).Foruser-independentmodels,onlythe
originalechoprofile(14.67mm)hasalargererrorthantheoriginal+differentialechoprofiles(13.99mm),butthe
differentialechoprofileitselfhasevensmallererrors.Thisisattributedtotherelativelylargeindividualhand
shapedifferences,incomparisontofingermovements.However,fortheuser-dependentmodel,boththeoriginal
echoprofile(10.64mm)andthedifferentialechoprofile(10.36mm)contributeheavilytotheprediction.Thisis
becausetheadditionalfine-tuneddatacontainshandshapeandmovementinformationtailoredtotheuser.
10.2.3 WindowSizeComparisonStudy. Inourcurrentsystemimplementation,wechoseawindowlengthof1.2
seconds.However,thewindowsizecanpotentiallyimpacttheperformance.Theexperimentresultsondifferent
windowsizesareshowninFig.13(c),hisshowedtheperformanceimprovedasthewindowsizeincreased:the
user-independentanddependenterrorsdecreasedfrom14.78mmand11.43mmto13.34mmand9.75mmwhen
thewindowsizeincreasedfrom0.82sto1.58s.Thetwo-wayrepeatedANOVAtestwithmodeltypeandwindow
sizeasindependentvariablesfurthershowsstatisticalsignificance(p=0.000<0.5).
21Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
10.2.4 SensingRangeComparisonStudy. Theselectionofsensingrangeaffectsthebalancebetweencaptured
noiseandinformation.Inourexperimentcomparingsensingranges(Fig.13(d)),weobservedminimalimpact
(p=0.56 in two-way repeated ANOVA test with model type and sensing range as independent variables) on
performance based on varying sensing ranges: errors of user-independent models range from 13.78mm to
13.99mm,anderrorsforuser-dependentmodelsrangefrom10.05and10.20.Ourchoiceofsensingrangewas
determinedbytheaveragehandsize.
10.3 EvaluationinUnconstrainedSettings.
Our hand pose tracking study, capturing a wide range of noise factors, evaluates the system in a relatively
controlledsettingtodemonstratethefeasibilityoftheproposedsensingsystem.Ourposesetofsize20islarge
whencomparedamongliterature(e.g.,11in[26]and16in[68]),butdoesnotexhaustallpossiblehumanhand
poses.Itremainsimportantforfutureworktoevaluatethesystem‚Äôsperformancewithoutanypre-definedpose
set.Furthermore,intermsoflocation,ourindoorstudiesdonotaccountforoutdoorsettingswheretherecould
beultrasonicinterference.BecauseRing-a-Poseleveragesreflectionstrengthsinformationfromwithin18.52cm,
objectsandotherbodyparts(e.g.,legsandhands)withintherangealterthesignals.Thisisalimitationshared
bymanyexistingwearablehandposesensingsystems.Theoretically,thedifferentialechoprofilescancelout
suchnoise,buttheoriginalechoprofileswillbeinevitablyimpacted.Morerigorousstudiesonsuchblockage
interferencesareneeded.Conversely,this‚Äúnoise‚Äùmightbeseenasinformationregardingadjacentobjects[27]
andthehandstatus.Forinstance,whenanobjectisdetectedandmovesclosertothehand,thereflectedacoustic
signalcouldpossessauniquepatternthatnotonlyindicatestheobject‚Äôsshapebutalsoprovidesinsightsintothe
handpose.Itwouldbeinterestingtoseeothersensingtaskstheringcanaccomplish,suchasobjectdetection
andactivityrecognition.
10.4 IntegrationintoSmartRingPlatforms.
EnergyConsumption. Fromthehardwareperspective,thesimplicityofoursensorsetupallowseasyintegration
ofthehandposetrackingsolutionintoanexistingsmartring.InFig.2(c),weshowamodificationofourprototype
withanarcbattery(GepowGRP1507028)andwithoutthemicrocontrollerunit.Forinstance,tointegrateour
solutionintoacommercialproductliketheOuraRing[42],whichalreadyfeaturesaspeakerandBluetooth
capability, the addition of just a microphone would be enough. However, there are other obstacles. Though
relativelylow-powerforawearablehand-posetrackingsolution(Table.1),activeacousticsensing,whileeffective,
doesnotpresenttheabsolutebestenergyefficiencyforaring.Thecurrentpowerconsumptionofthedevicestill
onlylastsabout1.75hwithourLipobatteryduetothe(relatively)highenergyconsumptionofourselectedflat
speaker.Thisspeakerwasatradeoffbetweenaslimformfactorandalowerefficiency(weexperimentedwith
thickerspeakerswithmuchlowerenergyconsumption).Futureworksonalternativehardwarewillmakethe
devicemoreenergyefficient.
MultimodalSensing. Inthispaper,wesolelyemployedactiveacousticsensing.Whenintegratedintoasmart
ring platform like [42, 78], multimodal sensing, using existing inertial measurement units (IMUs) and Pho-
toplethysmography (PPG), can further enrich Ring-a-Pose‚Äôs hand sensing capabilities [2, 78]. Other sensing
principleswillsurelycomplementsomeofRing-a-Pose‚Äôslimitations:electricfieldsensingmaycorrecttheblock-
ageinterferencewithinthering‚Äôssensingrange[9];IMUs/capacitiveproximitysensorcanprovideadditional
movementinformationontheinstrumentedfinger[77]andonadditionaltwoneighboringfingers[65].Additional
sensorsbringadditionalenergyconsumptionandspaceconstraints,posingchallengesandopportunitiesfor
low-powerandminiaturizedsensorsolutions.
22Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
WearableEcosystem. Aswerevisittheresearchquestionstatedintheintroduction,asingleuntetheredring
succeedsincontinuouslytrackinghandposes.However,asanaccessory,itisnotuncommonforonetowear
multipleringsononehandandevenononefinger.Currentmulti-ringsystems[54,77]containawristband
inadditiontomultiplerings.BuildingontopofRing-a-Pose,itwouldbeinterestingtoexplorethedesignand
technicalspaceofmultipleringswhereuserschoosetheirnumberofringsandplacements.Further,weenvision
thatinthefuture,theringwillworktogetherwithotherwearablesaspartoftheecosystemtoprovideusers
withtheoptimalalways-availableinteractionexperience.
11 CONCLUSION
Inthispaper,wepresent,Ring-a-Pose,thefirstsmartringthattrackshandposescontinuouslyandrecognizes
handposturesusinginaudibleactiveacousticsensing.Aseriesof3userstudieswith30participantsshowedthat
itcanachieve14.1mmaccuracyininferringtherelative(tothewrist)positionsof20fingerjointswithoutthe
needtocollecttrainingdatafromanewuser.Thejointerrordecreasesto10.3mmwithadditionalcalibration
datafromtheuser.Thepromisingresultspavethewayforring-basedhandposesensingsystems.
REFERENCES
[1] Apple.2023.AppleVisionPro.https://www.apple.com/apple-vision-pro/.
[2] Apple.2023.AppleWatchSeries9.https://www.apple.com/shop/buy-watch/apple-watch.
[3] YinBi,MingsongLv,ChenSong,WenyaoXu,NanGuan,andWangYi.2015.AutoDietary:Awearableacousticsensorsystemforfood
intakerecognitionindailylife.IEEESensorsJournal16,3(2015),806‚Äì816.
[4] RogerBoldu,AlexandruDancu,DenysJCMatthies,PabloGallegoCasc√≥n,ShanakaRansir,andSurangaNanayakkara.2018.Thumb-
In-Motion:EvaluatingThumb-to-RingMicrogesturesforAthleticActivity.InProceedingsofthe2018ACMSymposiumonSpatialUser
Interaction.150‚Äì157.
[5] GaoshuaiCao,KuangYuan,JieXiong,PanlongYang,YuboYan,HaoZhou,andXiang-YangLi.2020.Earphonetrack:involvingearphones
intotheecosystemofacousticmotiontracking.InProceedingsofthe18thConferenceonEmbeddedNetworkedSensorSystems.95‚Äì108.
[6] EdwinChan,TeddySeyed,WolfgangStuerzlinger,Xing-DongYang,andFrankMaurer.2016.Userelicitationonsingle-handmicroges-
tures.InProceedingsofthe2016CHIConferenceonHumanFactorsinComputingSystems.3403‚Äì3414.
[7] LiweiChan,Yi-LingChen,Chi-HaoHsieh,Rong-HaoLiang,andBing-YuChen.2015.Cyclopsring:Enablingwhole-handandcontext-
awareinteractionsthroughafisheyering.InProceedingsofthe28thAnnualACMSymposiumonUserInterfaceSoftware&Technology.
549‚Äì556.
[8] LiweiChan,Rong-HaoLiang,Ming-ChangTsai,Kai-YinCheng,Chao-HuaiSu,MikeYChen,Wen-HuangCheng,andBing-YuChen.
2013.FingerPad:privateandsubtleinteractionusingfingertips.InProceedingsofthe26thannualACMsymposiumonUserinterface
softwareandtechnology.255‚Äì260.
[9] TaizhouChen,TianpeiLi,XingyuYang,andKeningZhu.2023.EFRing:EnablingThumb-to-Index-FingerMicrogestureInteraction
throughElectricFieldSensingUsingSingleSmartRing. ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous
Technologies6,4(2023),1‚Äì31.
[10] JamesConnolly,JoanCondell,BrendanO‚ÄôFlynn,JavierTorresSanchez,andPhilipGardiner.2017. IMUsensor-basedelectronic
goniometricgloveforclinicalfingermovementanalysis.IEEESensorsJournal18,3(2017),1273‚Äì1281.
[11] ArtemDementyevandJosephA.Paradiso.2014.WristFlex:Low-PowerGestureInputwithWrist-WornPressureSensors.InProceedings
ofthe27thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(Honolulu,Hawaii,USA)(UIST‚Äô14).Associationfor
ComputingMachinery,NewYork,NY,USA,161‚Äì166. https://doi.org/10.1145/2642918.2647396
[12] NathanDevrioandChrisHarrison.2022.DiscoBand:MultiviewDepth-SensingSmartwatchStrapforHand,BodyandEnvironment
Tracking.InProceedingsofthe35thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.1‚Äì13.
[13] Bogdan-FlorinGheran,JeanVanderdonckt,andRadu-DanielVatavu.2018.Gesturesforsmartrings:Empiricalresults,insights,and
designimplications.InProceedingsofthe2018DesigningInteractiveSystemsConference.623‚Äì635.
[14] OliverGlauser,ShihaoWu,DanielePanozzo,OtmarHilliges,andOlgaSorkine-Hornung.2019.Interactivehandposeestimationusinga
stretch-sensingsoftglove.ACMTransactionsonGraphics(ToG)38,4(2019),1‚Äì15.
[15] JunGong,YangZhang,XiaZhou,andXing-DongYang.2017.Pyro:Thumb-tipgesturerecognitionusingpyroelectricinfraredsensing.
InProceedingsofthe30thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.553‚Äì563.
[16] YizhengGu,ChunYu,ZhipengLi,ZhaohengLi,XiaoyingWei,andYuanchunShi.2020.Qwertyring:Textentryonphysicalsurfaces
usingaring.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies4,4(2020),1‚Äì29.
23Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
[17] JeremyGummeson,BodhiPriyantha,andJieLiu.2014.Anenergyharvestingwearableringplatformforgestureinputonsurfaces.In
Proceedingsofthe12thannualinternationalconferenceonMobilesystems,applications,andservices.162‚Äì175.
[18] ChrisHarrison,DesneyTan,andDanMorris.2010.Skinput:appropriatingthebodyasaninputsurface.InProceedingsoftheSIGCHI
conferenceonhumanfactorsincomputingsystems.453‚Äì462.
[19] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition.770‚Äì778.
[20] MasoumehsadatHosseini,TjadoIhmels,ZiqianChen,MarionKoelle,HeikoM√ºller,andSusanneBoll.2023. TowardsaConsensus
GestureSet:ASurveyofMid-AirGesturesinHCIforMaximizedAgreementAcrossDomains.InProceedingsofthe2023CHIConference
onHumanFactorsinComputingSystems.1‚Äì24.
[21] FangHu,PengHe,SonglinXu,YinLi,andChengZhang.2020.FingerTrak:Continuous3Dhandposetrackingbydeeplearninghand
silhouettescapturedbyminiaturethermalcamerasonwrist.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous
Technologies4,2(2020),1‚Äì24.
[22] YashaIravantchi,MayankGoel,andChrisHarrison.2019.BeamBand:Handgesturesensingwithultrasonicbeamforming.InProceedings
ofthe2019CHIConferenceonHumanFactorsinComputingSystems.1‚Äì10.
[23] YinchengJin,YangGao,XuhaiXu,SeokminChoi,JiyangLi,FengLiu,ZhengxiongLi,andZhanpengJin.2022.EarCommand:"Hearing"
YourSilentSpeechCommandsInEar. Proc.ACMInteract.Mob.WearableUbiquitousTechnol.6,2,Article57(jul2022),28pages.
https://doi.org/10.1145/3534613
[24] LeiJing,ZixueCheng,YinghuiZhou,JunboWang,andTongjunHuang.2013.Magicring:Aself-containedgestureinputdeviceon
finger.InProceedingsofthe12thInternationalConferenceonMobileandUbiquitousMultimedia.1‚Äì4.
[25] WolfKienzle,EricWhitmire,ChrisRittaler,andHrvojeBenko.2021.ElectroRing:SubtlePinchandTouchDetectionwithaRing.In
Proceedingsofthe2021CHIConferenceonHumanFactorsinComputingSystems(Yokohama,Japan)(CHI‚Äô21).AssociationforComputing
Machinery,NewYork,NY,USA,Article3,12pages. https://doi.org/10.1145/3411764.3445094
[26] DaehwaKimandChrisHarrison.2022.EtherPose:ContinuousHandPoseTrackingwithWrist-WornAntennaImpedanceCharacteristic
Sensing.InProceedingsofthe35thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.1‚Äì12.
[27] Chi-JungLee,RuidongZhang,DevanshAgarwal,TianhongCatherineYu,VipinGunda,OliverLopez,JamesKim,SichengYin,BoaoDeng,
KeLi,MoseSakashita,FrancoisGuimbretiere,andChengZhang.2024.EchoWrist:ContinuousHandPoseTrackingandHand-Object
InteractionRecognitionUsingLow-PowerActiveAcousticSensingOnaWristband. arXiv:2401.17409[cs.HC]
[28] KeLi,RuidongZhang,BoaoChen,SiyuanChen,SichengYin,SaifMahmud,QikangLiang,Fran√ßoisGuimbreti√®re,andChengZhang.
2024.GazeTrak:ExploringAcoustic-basedEyeTrackingonaGlassFrame.arXivpreprintarXiv:2402.14634(2024).
[29] KeLi,RuidongZhang,SiyuanChen,BoaoChenMoseSakashita,FrancoisGuimbretiere,andChengZhang.2024.EyeEcho:Continuous
andLow-powerFacialExpressionTrackingonGlasses.InToAppearinProceedingsofthe2024CHIConferenceonHumanFactorsin
ComputingSystems.NA.
[30] KeLi,RuidongZhang,BoLiang,Fran√ßoisGuimbreti√®re,andChengZhang.2022. Eario:Alow-poweracousticsensingearablefor
continuouslytrackingdetailedfacialmovements.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies6,
2(2022),1‚Äì24.
[31] YangLiu,ChengdongLin,andZhenjiangLi.2021. WR-Hand:Wearablearmbandcantrackuser‚Äôshand. ProceedingsoftheACMon
Interactive,Mobile,WearableandUbiquitousTechnologies5,3(2021),1‚Äì27.
[32] YilinLiu,ShijiaZhang,andMahanthGowda.2021.NeuroPose:3DhandposetrackingusingEMGwearables.InProceedingsoftheWeb
Conference2021.1471‚Äì1482.
[33] SaifMahmud,KeLi,GuilinHu,HaoChen,RichardJin,RuidongZhang,Fran√ßoisGuimbreti√®re,andChengZhang.2023.PoseSonic:3D
UpperBodyPoseEstimationThroughEgocentricAcousticSensingonSmartglasses. ProceedingsoftheACMonInteractive,Mobile,
WearableandUbiquitousTechnologies7,3(2023),1‚Äì28.
[34] WenguangMao,JianHe,andLiliQiu.2016. CAT:High-PrecisionAcousticMotionTracking.InProceedingsofthe22ndAnnual
InternationalConferenceonMobileComputingandNetworking(NewYorkCity,NewYork)(MobiCom‚Äô16).AssociationforComputing
Machinery,NewYork,NY,USA,69‚Äì81. https://doi.org/10.1145/2973750.2973755
[35] Meta.2023.MetaQuestVRHeadsets,Accessories&Equipment:MetaQuest.https://www.meta.com/quest/.
[36] FranziskaMueller,FlorianBernard,OleksandrSotnychenko,DushyantMehta,SrinathSridhar,DanCasas,andChristianTheobalt.2018.
Ganeratedhandsforreal-time3dhandtrackingfrommonocularrgb.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.49‚Äì59.
[37] SurangaNanayakkara,RoyShilkrot,KianPeenYeo,andPattieMaes.2013.EyeRing:afinger-worninputdeviceforseamlessinteractions
withoursurroundings.InProceedingsofthe4thAugmentedHumanInternationalConference.13‚Äì20.
[38] RajalakshmiNandakumar,VikramIyer,DesneyTan,andShyamnathGollakota.2016. Fingerio:Usingactivesonarforfine-grained
fingertracking.InProceedingsofthe2016CHIConferenceonHumanFactorsinComputingSystems.1515‚Äì1525.
[39] VietNguyen,SiddharthRupavatharam,LuyangLiu,RichardHoward,andMarcoGruteser.2019.HandSense:capacitivecoupling-based
dynamic,microfingergesturerecognition.InProceedingsofthe17thConferenceonEmbeddedNetworkedSensorSystems.285‚Äì297.
24Ring-a-Pose Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
[40] ShahriarNirjon,JeremyGummeson,DanGelb,andKyu-HanKim.2015. Typingring:Awearableringplatformfortextinput.In
Proceedingsofthe13thAnnualInternationalConferenceonMobileSystems,Applications,andServices.227‚Äì239.
[41] MasaOgata,YutaSugiura,HirotakaOsawa,andMichitaImai.2012.iRing:intelligentringusinginfraredreflection.InProceedingsofthe
25thannualACMsymposiumonUserinterfacesoftwareandtechnology.131‚Äì136.
[42] OuraRing.2023.OuraRing,OuraHealthOy.https://ouraring.com.
[43] FarshidSalemiParizi,EricWhitmire,andShwetakPatel.2019.Auraring:Preciseelectromagneticfingertracking.Proceedingsofthe
ACMonInteractive,Mobile,WearableandUbiquitousTechnologies3,4(2019),1‚Äì28.
[44] MPoongodi,MounirHamdi,MohitMalviya,AshutoshSharma,GauravDhiman,andSVimal.2022.DiagnosisandcombatingCOVID-19
usingwearableOurasmartringwithdeeplearningmethods.Personalandubiquitouscomputing(2022),1‚Äì11.
[45] ChenQian,XiaoSun,YichenWei,XiaoouTang,andJianSun.2014. RealtimeandRobustHandTrackingfromDepth.In2014IEEE
ConferenceonComputerVisionandPatternRecognition.1106‚Äì1113. https://doi.org/10.1109/CVPR.2014.145
[46] Ringly.2023.SmartJewelryandAccessories.https://ringly.com/.
[47] RingPay.2023.RingPay,McLEAR.https://mclear.com/product/payment-ring/.
[48] JavierRomero,DimitriosTzionas,andMichaelJBlack.2022. Embodiedhands:Modelingandcapturinghandsandbodiestogether.
arXivpreprintarXiv:2201.02610(2022).
[49] SougataSenandDavidKotz.2020.VibeRing:Usingvibrationsfromasmartringasanout-of-bandchannelforsharingsecretkeys.In
Proceedingsofthe10thInternationalConferenceontheInternetofThings.1‚Äì8.
[50] AdwaitSharma,ChristinaSalchow-H√∂mmen,VimalSureshMollyn,AdityaShekharNittala,MichaelAHedderich,MarionKoelle,
ThomasSeel,andJ√ºrgenSteimle.2022. SparseIMU:ComputationalDesignofSparseIMULayoutsforSensingFine-GrainedFinger
Microgestures.ACMTransactionsonComputer-HumanInteraction(2022).
[51] RujiaSun,XiaoheZhou,BenjaminSteeper,RuidongZhang,SichengYin,KeLi,ShengzhangWu,SamTilsen,FrancoisGuimbretiere,and
ChengZhang.2023.EchoNose:SensingMouth,BreathingandTongueGesturesinsideOralCavityusingaNon-contactNoseInterface.
InProceedingsofthe2023ACMInternationalSymposiumonWearableComputers.22‚Äì26.
[52] WeiSun,FranklinMingzheLi,CongshuHuang,ZhenyuLei,BenjaminSteeper,SongyunTao,FengTian,andChengZhang.2021.
Thumbtrak:Recognizingmicro-fingerposesusingaringwithproximitysensing.InProceedingsofthe23rdInternationalConferenceon
MobileHuman-ComputerInteraction.1‚Äì9.
[53] WeiSun,FranklinMingzheLi,BenjaminSteeper,SonglinXu,FengTian,andChengZhang.2021.Teethtap:Recognizingdiscreteteeth
gesturesusingmotionandacousticsensingonanearpiece.In26thInternationalConferenceonIntelligentUserInterfaces.161‚Äì169.
[54] RyoTakahashi,MasaakiFukumoto,ChangyoHan,TakuyaSasatani,YoshiakiNarusue,andYoshihiroKawahara.2020.TelemetRing:A
BatterylessandWirelessRing-ShapedKeyboardUsingPassiveInductiveTelemetry.InProceedingsofthe33rdAnnualACMSymposium
onUserInterfaceSoftwareandTechnology(VirtualEvent,USA)(UIST‚Äô20).AssociationforComputingMachinery,NewYork,NY,USA,
1161‚Äì1168. https://doi.org/10.1145/3379337.3415873
[55] HoangTruong,ShuoZhang,UfukMuncuk,PhucNguyen,NamBui,AnhNguyen,QinLv,KaushikChowdhury,ThangDinh,andTam
Vu.2018.Capband:Battery-freesuccessivecapacitancesensingwristbandforhandgesturerecognition.InProceedingsofthe16thACM
ConferenceonEmbeddedNetworkedSensorSystems.54‚Äì67.
[56] UltraLeap.2023.World-leadingHandTrackingProducts:Small.Fast.Accurate.|Ultraleap. RetrievedSep12,2023fromhttps://www.
ultraleap.com/product/
[57] Radu-DanielVatavu.2023.iFADgestures:understandingusers‚Äôgestureinputperformancewithindex-fingeraugmentationdevices.In
Proceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.1‚Äì17.
[58] Radu-DanielVatavuandLaura-BiancaBilius.2021.GestuRING:Aweb-basedtoolfordesigninggestureinputwithrings,ring-like,and
ring-readydevices.InThe34thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.710‚Äì723.
[59] Vicon.2023.Vicon|AwardWinningMotionCaptureSystems.https://www.vicon.com/.
[60] AnandghanWaghmare,YoussefBenTaleb,IshanChatterjee,ArjunNarendra,andShwetakPatel.2023. Z-Ring:Single-PointBio-
ImpedanceSensingforGesture,Touch,ObjectandUserRecognition.InProceedingsofthe2023CHIConferenceonHumanFactorsin
ComputingSystems.1‚Äì18.
[61] AnandghanWaghmare,IshanChatterjee,andShwetakPatel.2023.Z-Pose:Continuous3DHandPoseTrackingUsingSingle-Point
Bio-ImpedanceSensingonaRing.InProceedingsofthe2ndWorkshoponSmartWearableSystemsandApplications.1‚Äì6.
[62] AnranWangandShyamnathGollakota.2019.Millisonic:Pushingthelimitsofacousticmotiontracking.InProceedingsofthe2019CHI
ConferenceonHumanFactorsinComputingSystems.1‚Äì11.
[63] SaiwenWang,JieSong,JaimeLien,IvanPoupyrev,andOtmarHilliges.2016.Interactingwithsoli:Exploringfine-graineddynamic
gesturerecognitionintheradio-frequencyspectrum.InProceedingsofthe29thAnnualSymposiumonUserInterfaceSoftwareand
Technology.851‚Äì860.
[64] TianbenWang,DaqingZhang,YuanqingZheng,TaoGu,XingsheZhou,andBernadetteDorizzi.2018. C-FMCWbasedcontactless
respirationdetectionusingacousticsignal.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies1,4
(2018),1‚Äì20.
25Conferenceacronym‚ÄôXX,June03‚Äì05,2018,Woodstock,NY
[65] MathiasWilhelm,DanielKrakowczyk,andSahinAlbayrak.2020. PeriSense:Ring-basedmulti-fingergestureinteractionutilizing
capacitiveproximitysensing.Sensors20,14(2020),3990.
[66] ErwinWu,YeYuan,Hui-ShyongYeo,AaronQuigley,HidekiKoike,andKrisMKitani.2020.Back-hand-pose:3dhandposeestimation
forawrist-worncameraviadorsumdeformationnetwork.InProceedingsofthe33rdAnnualACMSymposiumonUserInterfaceSoftware
andTechnology.1147‚Äì1160.
[67] KojiYataniandKhaiNTruong.2012.Bodyscope:awearableacousticsensorforactivityrecognition.InProceedingsofthe2012ACM
ConferenceonUbiquitousComputing.341‚Äì350.
[68] Hui-ShyongYeo,ErwinWu,JuyoungLee,AaronQuigley,andHidekiKoike.2019.Opisthenar:Handposesandfingertappingrecognition
byobservingbackofhandusingembeddedwristcamera.InProceedingsofthe32ndAnnualACMSymposiumonUserInterfaceSoftware
andTechnology.963‚Äì971.
[69] BoningZhang,YiqiangChen,YueliangQian,andXiangdongWang.2011.ARing-ShapedInteractiveDeviceforLargeRemoteDisplay
andMobileDeviceControl.InProceedingsofthe13thInternationalConferenceonUbiquitousComputing(Beijing,China)(UbiComp‚Äô11).
AssociationforComputingMachinery,NewYork,NY,USA,473‚Äì474. https://doi.org/10.1145/2030112.2030177
[70] ChengZhang,AnandghanWaghmare,PranavKundra,YimingPu,ScottGilliland,ThomasPloetz,ThadEStarner,OmerTInan,and
GregoryDAbowd.2017.FingerSound:Recognizingunistrokethumbgesturesusingaring.ProceedingsoftheACMonInteractive,Mobile,
WearableandUbiquitousTechnologies1,3(2017),1‚Äì19.
[71] ChengZhang,XiaoxuanWang,AnandghanWaghmare,SumeetJain,ThomasPloetz,OmerTInan,ThadEStarner,andGregoryD
Abowd.2017.FingOrbits:interactionwithwearablesusingsynchronizedthumbmovements.InProceedingsofthe2017ACMInternational
SymposiumonWearableComputers.62‚Äì65.
[72] ChengZhang,QiuyueXue,AnandghanWaghmare,RuichenMeng,SumeetJain,YizengHan,XinyuLi,KennethCunefare,Thomas
Ploetz,ThadStarner,etal.2018.FingerPing:Recognizingfine-grainedhandposesusingactiveacousticon-bodysensing.InProceedings
ofthe2018CHIConferenceonHumanFactorsinComputingSystems.1‚Äì10.
[73] FanZhang,ValentinBazarevsky,AndreyVakunov,AndreiTkachenka,GeorgeSung,Chuo-LingChang,andMatthiasGrundmann.2020.
Mediapipehands:On-devicereal-timehandtracking.arXivpreprintarXiv:2006.10214(2020).
[74] RuidongZhang,HaoChen,DevanshAgarwal,RichardJin,KeLi,Fran√ßoisGuimbreti√®re,andChengZhang.2023.HPSpeech:Silent
SpeechInterfaceforCommodityHeadphones.InProceedingsofthe2023ACMInternationalSymposiumonWearableComputers.60‚Äì65.
[75] RuidongZhang,KeLi,YihongHao,YufanWang,ZhengnanLai,Fran√ßoisGuimbreti√®re,andChengZhang.2023.EchoSpeech:Continuous
SilentSpeechRecognitiononMinimally-obtrusiveEyewearPoweredbyAcousticSensing.InProceedingsofthe2023CHIConferenceon
HumanFactorsinComputingSystems.1‚Äì18.
[76] YangZhangandChrisHarrison.2015.Tomo:Wearable,low-costelectricalimpedancetomographyforhandgesturerecognition.In
Proceedingsofthe28thAnnualACMSymposiumonUserInterfaceSoftware&Technology.167‚Äì173.
[77] HaoZhou,TaitingLu,YilinLiu,ShijiaZhang,andMahanthGowda.2022.LearningontheRings:Self-Supervised3DFingerMotion
TrackingUsingWearableSensors.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies6,2(2022),1‚Äì31.
[78] HaoZhou,TaitingLu,YilinLiu,ShijiaZhang,RunzeLiu,andMahanthGowda.2023. OneRingtoRuleThemAll:AnOpenSource
SmartringPlatformforFingerMotionAnalyticsandHealthcareApplications.InProceedingsofthe8thACM/IEEEConferenceonInternet
ofThingsDesignandImplementation.27‚Äì38.
[79] Alexandru-Ionut,S,IEAN.2022.ASetofSmartRingGesturesforDroneControl.(2022).
26