Grasper: A Generalist Pursuer for Pursuit-Evasion Problems
PengdengLiâˆ— ShuxinLiâˆ— XinrunWangâ€ 
NanyangTechnologicalUniversity NanyangTechnologicalUniversity NanyangTechnologicalUniversity
Singapore Singapore Singapore
pengdeng.li@ntu.edu.sg shuxin.li@ntu.edu.sg xinrun.wang@ntu.edu.sg
JakubÄŒernÃ½ YouzhiZhang StephenMcAleer
ColumbiaUniversity CAIR,HKISI,CAS CarnegieMellonUniversity
NewYorkCity,UnitedStates HongKong,China Pittsburgh,UnitedStates
cerny@disroot.org youzhi.zhang@cair-cas.org.hk mcaleer.stephen@gmail.com
HauChan BoAn
UniversityofNebraska-Lincoln NanyangTechnologicalUniversity
Lincoln,Nebraska,UnitedStates Singapore
hchan3@unl.edu boan@ntu.edu.sg
ABSTRACT approachforsolvingpursuit-evasionproblemsacrossabroadrange
Pursuit-evasiongames(PEGs)modelinteractionsbetweenateam ofscenarios,enablingpracticaldeploymentinreal-worldsituations.
ofpursuersandanevaderingraph-basedenvironmentssuchas
KEYWORDS
urbanstreetnetworks.Recentadvancementshavedemonstrated
theeffectivenessofthepre-trainingandfine-tuningparadigmin Multi-AgentLearning;Pursuit-EvasionProblems;Generalizability;
Policy-SpaceResponseOracles(PSRO)toimprovescalabilityin Pre-trainingandFine-tuning;Hypernetwork
solvinglarge-scalePEGs.However,thesemethodsprimarilyfo-
ACMReferenceFormat:
cusonspecificPEGswithfixedinitialconditionsthatmayvary PengdengLiâˆ—,ShuxinLiâˆ—,XinrunWangâ€ ,JakubÄŒernÃ½,YouzhiZhang,
substantiallyinreal-worldscenarios,whichsignificantlyhinders
StephenMcAleer,HauChan,andBoAn.2024.Grasper:AGeneralistPursuer
theapplicabilityofthetraditionalmethods.Toaddressthisissue, forPursuit-EvasionProblems.InProc.ofthe23rdInternationalConference
weintroduceGrasper,aGeneRAlistpurSuerforPursuit-Evasion onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland,
pRoblems,capableofefficientlygeneratingpursuerpoliciestai- NewZealand,May6â€“10,2024,IFAAMAS,9pages.
loredtospecificPEGs.Ourcontributionsarethreefold:First,we
presentanovelarchitecturethatoffershigh-qualitysolutionsfor 1 INTRODUCTION
diversePEGs,comprisingcriticalcomponentssuchas(i)agraph
Thedeploymentofsecurityresourcestodetect,deter,andcatch
neuralnetwork(GNN)toencodePEGsintohiddenvectors,and
criminals is a critical task in urban security [30, 32]. Statistics
(ii)ahypernetworktogeneratepursuerpoliciesbasedonthese
showthatpolicepursuitsprobablyâ€œinjureorkillmoreinnocent
hiddenvectors.Asasecondcontribution,wedevelopanefficient
bystandersthananyotherkindofforceâ€[26].Therefore,itiscrucial
three-stagetrainingmethodinvolving(i)apre-pretrainingstage
tocomeupwithscalableapproachesforeffectivelycoordinating
forlearningrobustPEGrepresentationsthroughself-supervised
varioussecurityresources,ensuringtheswiftapprehensionofa
graphlearningtechniqueslikegraphmaskedauto-encoder(Graph-
fleeingcriminaltominimizeharmandpropertydamage.Duetothe
MAE),(ii)apre-trainingstageutilizingheuristic-guidedmulti-task
adversarialnaturebetweenattackersanddefenders,game-theoretic
pre-training(HMP)whereheuristic-derivedreferencepolicies(e.g.,
modelshavebeenusedtomodelvariousreal-worldurbansecurity
throughDijkstraâ€™salgorithm)regularizepursuerpolicies,and(iii)
scenarios.Inparticular,thepursuit-evasiongame(PEG)hasbeen
afine-tuningstagethatemploysPSROtogeneratepursuerpolicies
extensivelyemployedtomodeltheinteractionsbetweenateam
ondesignatedPEGs.Finally,weperformextensiveexperiments
ofpursuers(e.g.,policeforces)andanevader(e.g.,acriminal)on
onsyntheticandreal-worldmaps,showcasingGrasperâ€™ssignif-
graphs(e.g.,urbanstreetnetworks)[21,38,42,43].Toeffectively
icantsuperiorityoverbaselinesintermsofsolutionqualityand
solvePEGsundervarioussettings,severalmethods,suchascoun-
generalizability.WedemonstratethatGrasperprovidesaversatile
terfactualregretminimization(CFR)[47]andpolicy-spaceresponse
oracles(PSRO)[18],havebeendevelopedintheliterature.Among
âˆ—Equalcontribution.
â€ Correspondingauthor. thesealgorithms,PSRO,adeepreinforcementlearningalgorithm,
providesaversatileframeworkforlearningthe(approximate)Nash
equilibria(NEs)ofPEGs(refertoSection3.2foranintroduction
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. ofthePSROframework).Furthermore,recentworkshavealsoin-
tegratedthepre-trainingandfine-tuningparadigmintothePSRO
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems frameworktofurtherenhanceitsscalability[20].
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6â€“10,2024,
Althoughmanyexistingworkshaveachievedsignificantsuccess,
Auckland,NewZealand.Â©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). theyonlyfocusonsolvingspecificPEGswithpredeterminedinitial
4202
rpA
91
]IA.sc[
1v62621.4042:viXraconditions,e.g.,theinitiallocationsofallplayersandexits,andthe incremental strategy generation [42, 43] have been introduced.
timehorizonofthegame.Unfortunately,theseconditionsmayvary Nonetheless,thesemethodsencounterscalabilityissuesasthey
substantiallyinreal-worldscenarios,wherecrimescanoccurat typicallyrelyonlinearprogramming.Ontheotherhand,PEGcan
anylocationinacityandatanytime.Whentheinitialconditions beviewedasaparticulartypeoftwo-playerimperfect-information
change,existingalgorithmsmustsolvethenewPEGfromscratch, extensive-formgame(IIEFG).Thus,thealgorithmsusedforsolv-
which is computationally demanding and time-consuming [20], inglarge-scaleIIEFGs,suchascounterfactualregretminimization
restrictingthereal-worlddeploymentofcurrentalgorithms.Thus, (CFR)[47]andPolicy-SpaceResponseOracles(PSRO)[18],have
thereisanurgentnecessitytodevelopanewapproachcapableof beenappliedtotacklelarge-scalePEGs[21,38].However,when
solvingdifferentPEGswithvaryinginitialconditionseffectively. solvinglarge-scalePEGsusingPSRO,thereexistsignificantcom-
Tothisend,weintroduceGrasper:aGeneRAlistpurSuerfor putationalchallengesasitinvolvescomputingthebestresponse
Pursuit-EvasionpRoblems,whichcaneffectivelysolvedifferent strategymultipletimes.Tomitigatethisissue,recentresearch[20]
PEGsbygeneratingthepursuerâ€™spoliciesconditionalontheinitial integratesthepre-trainingandfine-tuningparadigmintoPSRO
conditionsofthePEGs.Grasperconsistsoftwocriticalcomponents. toimproveitsscalability.Despitethesuccess,allthesealgorithms
First,asthePEGisplayedonagraph,itisnaturaltouseagraph aretailoredtosolveaspecificPEGwithpredeterminedinitialcon-
neuralnetwork(GNN)toencodethePEGwiththegiveninitial ditions.Whentheseconditionschange,theymustrecomputethe
conditions into a hidden vector. Then, inspired by recent work NEstrategyfromscratch(onetotwohoursforaPEGona10Ã—10
ongeneralizationovergameswithdifferentpopulationsizes[19], gridmap[20]),whichhinderstheirreal-worldapplicability1.To
weintroduceahypernetworktogeneratethebasepolicyforthe addressthislimitation,weproposeGrasper,whichusesPSROto
pursuerconditionalonthehiddenvectorobtainedbytheGNN. computetheNEstrategyandcangeneratedifferentpursuersâ€™strate-
Thisgeneratedbasepolicythenservesasastartingpointforthe giesfordifferentPEGsbasedontheirinitialconditionswithout
pursuerâ€™sbestresponsepolicytrainingateachPSROiteration. recomputingtheNEstrategyfromscratch.
TotrainthenetworksofGrasper,wefindthatnaivelyapplying The generalizability of algorithms and models over different
multi-tasktraining[44]isinefficient.Furthermore,jointlytraining gameshasgainedincreasingattentionandremarkableprogresshas
theGNNandhypernetworkcouldbetime-consumingastheGNNis beenachievedinrecentresearch.Neuralequilibriumapproximators
onlyusedtoencodetheinitialconditionswhicharefixedduringthe thatdirectlypredicttheequilibriumstrategyfromgamepayoffsin
gameplaying.Toaddressthesechallenges,weproposeanefficient normal-formgames(NFGs)havebeentheoreticallyprovenPAC
three-stagetrainingmethodtotrainthenetworksofGrasper.First, learnable[7,8]andareabletogeneralizetodifferentgameswith
weintroduceapre-pretrainingstagetotraintheGNNbyusing desirablesolutionquality[7â€“9,24].However,itremainsunder-
self-supervisedgraphlearningmethodssuchasGraphMAE[15]. exploredwhengoingbeyondNFGs.Inthiswork,wemakethefirst
Second,wefixtheGNNandpre-trainthehypernetworkbyusing attempttoconsiderthegeneralizationprobleminthedomainof
amulti-tasktrainingprocedurewherethetrainingdataissampled PEGs,atypeofgamethathasawiderangeofreal-worldapplica-
fromdifferentPEGswithdifferentinitialconditions.Inthisstage, tions[14,22]andisfarmorecomplicatedthanNFGs.Wepropose
toovercomethelowexplorationefficiencyduetothepursuersâ€™ anovelalgorithmicframeworkthatisabletoefficientlysolvedif-
random exploration and the evaderâ€™s rationality, we propose a ferentPEGswithvaryinginitialconditionsanddemonstratethe
heuristic-guidedmulti-taskpre-training(HMP)whereareference generalizationabilitythroughextensiveexperiments.
policyderivedbyheuristicmethodssuchasDijkstraisusedto Ourworkisalsorelatedtoself-supervisedgraphlearningand
regularizethepursuerpolicy.Finally,wefollowthePSROprocedure multi-tasklearning.Recentworkshaveshownthatgenerativeself-
andobtainthepursuerâ€™sbestresponsepolicyateachiterationby supervisedlearning[13]canbeappliedtographlearningandout-
fine-tuningthebasepolicygeneratedbythehypernetwork. performcontrastivemethodswhichrequirecomplextrainingstrate-
Insummary,weprovidethreecontributions.First,wepropose gies[25,31],high-qualitydataaugmentation[41],andnegative
Grasperwhichisthefirstgeneralizableframeworkcapableofef- samplesthatareoftenchallengingtoconstructfromgraphs[46].
ficientlyprovidinghighlyqualifiedsolutionsfordifferentPEGs Therefore,weemploytherecentstate-of-the-art,GraphMAE[15],
withdifferentinitialconditions.Second,toefficientlytrainthenet- tolearnagoodrepresentationofaPEGwiththegiveninitialcon-
worksofGrasper,weproposeathree-stagetrainingmethod:(i) ditions.Multi-tasklearning[27,44]hasbeenappliedtovarious
apre-pretrainingstagetotraintheGNNthroughGraphMAE,(ii) domainsincludingnaturallanguageprocessing[6],computervi-
apre-trainingstagetotrainthehypernetworkthroughheuristic- sion[11],andreinforcementlearning(multi-taskRL)[34,36,40,45].
guidedmulti-taskpre-training(HMP),and(iii)afine-tuningstage Duetoitsstronggeneralizability,weemploymulti-taskRLforthe
toobtainthepursuerâ€™sbestresponsepolicyateachPSROitera- pre-trainingprocess,enablingthepre-trainedpolicytobequickly
tion.Finally,weperformextensiveexperiments,andtheresults fine-tunedforefficientpolicydevelopmentinnewtasks.
demonstratethesuperiorityofGrasperoverdifferentbaselines. Finally,ourworkisrelatedtothemulti-agentpatrollingproblem
wheretheevaderoftenanticipatespatrollingstrategiesandmay
chooseatargetorasinglepathasanaction[2,4,16,29].Conversely,
2 RELATEDWORK
ourpursuit-evasiongamefeaturessimultaneousactionswiththe
Pursuit-evasiongames(PEGs)havebeenextensivelyappliedto evaderunawareofthepursuerâ€™sreal-timelocations.
model various real-world problems such as security and robot-
1NotethatclassicalheuristicalgorithmssuchasDijkstraarealsolessapplicableowing
ics[3,14,17,22,23,33,35].ToefficientlysolvePEGsanddiffer-
tothisreason.Moreover,itislessmeaningfultoassumethatthereisatleastone
ent variants, many algorithms such as value iteration [14] and pursuerateachexitasthepursuerâ€™sresourcesaretypicallylimited[32].3 PROBLEMFORMULATION employHigh-LevelActionsfortheevader.Thatis,theevaderonly
Inthissection,wefirstpresentalltheelementsfordefiningthe choosestheexitnodetoescapefromandthensamplesoneshortest
PEGs.Then,wepresentthestate-of-the-art(SOTA)methodfor pathfromtheinitiallocationtothechosenexitnode,insteadof
solvingPEGs.Finally,wegivetheproblemstatementofthiswork. decidingwheretogointhenexttimestep.Specifically,attimestep
ğ‘¡ =0,theevaderdeterminesanexitnodeğ‘£â€² âˆˆğ‘‰â€²usingthepolicy
3.1 Preliminaries ğœ‹ğ‘’ :ğ‘‰ â†’Î”(ğ‘‰â€²),samplesashortestpathfromğ‘™ğ‘’ âˆˆğ‘‰ tothechosen
0
exitnodeğ‘£â€²,andthentakesactionsbasedonthepath.
Apursuit-evasiongame(PEG)isatwo-playergameplayedbetween
apursuerandanevader,i.e.,ğ‘ ={ğ‘,ğ‘’}.Followingpreviousworks Here,wegivesomeremarksontheassumptionofHigh-Level
[21, 38, 43], we assume that the pursuer comprisesğ‘› members Actions of the evader. (i) In our game setting, the evader lacks
denotedasğ‘ = {1,2,...,ğ‘›},andthepursuercanobtainthereal- real-timeaccesstothepursuersâ€™locations,requiringtheevaderto
actwithoutanyinformationabouttheirwhereabouts.Therefore,
timelocationinformationoftheevaderwiththehelpoftracking
samplingonepathfortheevaderwouldnotlosemuchinformation
devices.Inreality,PEGsaretypicallyplayedonurbanroadmaps,
whichcanberepresentedbyagraphğº = (ğ‘‰,ğ¸),whereğ‘‰ isthe comparedwiththecasewheretheevaderactsstepbystep.(ii)
setofverticesandğ¸ isthesetofedges.Letğ‘‰â€² âŠ‚ ğ‘‰ denotethe Trainingthepursueragainstanevaderwhochoosestheshortest
set of exit nodes from which the evader can escape andğ‘‡ the path,aworst-casescenarioforthepursuer,enhancestherobustness
predeterminedtimehorizonofthegame.Atğ‘¡ â‰¤ğ‘‡,thelocations ofthepursuerâ€™spolicy.(iii)Thoughitisasimplification,theproblem
oftheevaderandpursueraredenotedbyğ‘™ ğ‘¡ğ‘’ andğ‘™ ğ‘¡ğ‘ =(ğ‘™ ğ‘¡1,ğ‘™ ğ‘¡2,...,ğ‘™ ğ‘¡ğ‘›), s pe latt yin erg sâ€™re inm ia tii an ls ch oi ng dh il ty ioc no sm ,ep nle lx ard gu ine gto thth ee tam su kl sti pp al ce eex foit rs ta hn ed pd ui rv se ur ese
r,
respectively.Then,thehistoryofthegameatğ‘¡ isasequenceof
pastlocationsofbothplayers,i.e.,â„ = (ğ‘™ 0ğ‘’,ğ‘™ 0ğ‘ ,...,ğ‘™ ğ‘¡ğ‘’ âˆ’1,ğ‘™ ğ‘¡ğ‘ âˆ’1).The asd Inet sa uil med mi an ryth ,ge iI vn et nro ad gu rc at pio hn ğºan wd itA hp tp he en sd pi ex cA ifiQ cs2 e.
tofexitnodes
a
t
{hv (ğ‘™ea 1i ,pl ğ‘™a
l
2b
a
,yl .e
.e
.,ra ğ‘™â€™c ğ‘›st )i co
|u
ğ‘™n ğ‘–rrs âˆˆee nt Ntfo
l
(or
ğ‘™
ğ‘¡c
ğ‘–b âˆ’ao
t
1t
i
)h
o ,n
âˆ€p
,
ğ‘–la
i.
âˆˆy ee
.,
ğ‘r ğ´s }ğ‘’i ws (â„t hh
)
ee
r=
ene
N
Nig
(
(h
ğ‘™
ğ‘£ğ‘¡ğ‘’b )âˆ’o
1
dr
)
ein nag
n
otdv ee sğ´rt
ğ‘
ti hc
(
ee â„s
)
so
e=
tf ğ‘‰
p
(ğºrâ€² e,
,d
ğ‘‰t eh â€²te
,e
ğ‘™ri ğ‘n
m
,i ğ‘™it ğ‘’nia
,e
ğ‘‡l dl )o
t
.ic
Im
nat
e
tio hhn eos
r
Pio
z
Eof Gnt ,h
ğ‘‡
pe
,
lap
w
yu
e
er rs
c
su
an
wer
id
la
e
ln
fi
gd
n
etee tv
a
ha
s
ed pe ner oci( nfiğ‘™ -0ğ‘
c
z, ePğ‘™ r0ğ‘’
E
o) G, ra ean wsd aGt rh d=e
s
ofneighboringverticesofvertexğ‘£.Accordingtothedefinitionof 0 0
only when the game is terminated. The termination conditions
history,wedefinetheinformationsetforeachplayerastheset
includethreecases:(i)thepursuercatchestheevaderwithinthe
c
t th
ho
e
ens
p
ei vus at ri
s
dn
u
eg
e
rro
iâ€™
ssf di rn
e
ed
a
fili n-s tt eii
m
dng
e
au sli
o
ğ¼s ğ‘’ch aa =tb iol {e
n
â„h
|i
â„nis
f
=t oo
r
(r
m
ğ‘™i 0ğ‘’e
a
,s
t
ğ‘™.
i
0ğ‘oA ,ns
ğ‘™,
1ğ‘’t
t
,h
h
âˆ—e
e
,.e
i .n
.v
,f
ğ‘™a
o
ğ‘¡ğ‘’d âˆ’re
m
1r
,a
âˆ—c
t
)a
i
}on ,nn wo st
het
eg roe et
f
t
a
ti hnm eee
gx
ah
it
mo nr ei oz rdo een acwğ‘‡ hi,
et
si h.e
i tn
h., eğ‘™ tğ‘¡ğ‘’
h tie
mâˆˆ
t
eiğ‘™ mğ‘¡ğ‘ h,
e
oğ‘¡
rh
iâ‰¤
zo or
nğ‘‡ iz; ğ‘‡on( .ii Lğ‘‡) e,t th
i
ğ‘¡.e
e
â€².e
,
â‰¤v
ğ‘™
ğ‘¡ğ‘’a ğ‘‡d
âˆˆ
ber eğ‘‰e
tâ€²
hs ,c eğ‘¡a tp
â‰¤
imes
ğ‘‡
ef
;
sr (o
ti
em
i pi)
âˆ— PEr Gep ir se ase sn imts ua ltn ay nep oo us ss -i mbl oe velo gc aa mtio e,n wo ef ct ah ne mp ou drs eu lie tr a. sA al nth eo xu tg enh sit vh ee
-
t Fh oa rt ğ‘¡th =e ğ‘¡g â€²a ,m ine cis ast ee srm (ii )n aa nte dd (. iiT i)h ,e tn h, efo pr ura sl ul eğ‘¡ r< recğ‘¡ eâ€², ivğ‘Ÿ eğ‘¡ğ‘
s
=
a
rğ‘Ÿ eğ‘¡ğ‘’ w= ard0.
ğ¼f t
c
eğ‘o h
u
vr e arm
=r
dp
e
eu
n
{g rr
â„
â€™ta ss |m au
â„ lc
oee tr
=
ci( aocE
tn
(o iF
ğ‘™
om
.
0ğ‘’G
nT
,m)
ğ‘™h
b0ğ‘b i
e
u,ty s
.
t.pa
.
nw ,us
ğ‘™
oğ‘¡s ri
ğ‘’
tt âˆ’su h
u
t1m o
he
,u
ğ‘™
ei
r
ğ‘¡ğ‘n â€™t
âˆ’
esg a v1in ,t anh âˆ—y
df
)a
o
ei }t rrn â€™mt sf sh o
ina
ce r
t
ucm ie
e
rov a
rn
eta t hi nd o
s e
te enr
at
pa ca
uc
tbc
a r
iot osns u
u
nfi bt
e
.rer ths kdt e nea e ofin v wnd a
e
sdt dh e tre haâ€™n ess ğ‘Ÿ
ğ‘Ÿt
hhğ‘¡ ğ‘¡ğ‘
ğ‘
ae
v==
eev
ğ‘‰âˆ’1
a
ğ‘1dw
.
(e
ğœ‹Grh ğ‘ii
g
vl ,ae
ğ‘£ei
â€²nnt )h
s
te
=ha
ee Erv
e
e
(cid:2)a
w
xd
(cid:205)ia
te
ğ‘‡
ğ‘¡rr
n
=d
o
0in
ğ‘Ÿ
d
ğ‘Ÿğ‘¡ğ‘’c
ğ‘¡e
ğ‘u
(cid:3)=
cr hs
fo1
oa
, rsa
ep
tn
n
he
d
en ba
pt
yhl ut tey
rh
sp
eğ‘Ÿ uuğ‘¡ğ‘’
eer
rvs=
au
ad
neâˆ’
er
dr1
s
ğ‘‰. ğ‘£uI ğ‘’â€²ffn
(e
âˆ¼
ğœ‹rc ğ‘sa
ğœ‹
,s
a
ğ‘’
ğ‘£e
,
â€²l
)o( wi si =e) s,
Abehaviorpolicyofaplayerassignsaprobabilitydistribution E(cid:2)(cid:205)ğ‘‡ ğ‘¡=0ğ‘Ÿ ğ‘¡ğ‘’(cid:3) fortheevader,wheretheexpectationistakenover
overtheactionsetforeveryinformationsetbelongingtotheplayer.
thetrajectoriesinducedbyğœ‹ğ‘ .Then,forthepolicypair(ğœ‹ğ‘,ğœ‹ğ‘’),
Noticethatthepursuerâ€™sactionspaceiscombinatorialandexpands we haveğ‘‰ğ‘(ğœ‹ğ‘,ğœ‹ğ‘’) = E ğ‘£â€²âˆ¼ğœ‹ğ‘’(cid:2)ğ‘‰ğ‘(ğœ‹ğ‘,ğ‘£â€²)(cid:3) for the pursuer and
exponentiallywiththenumberofpursuermembers.Asaresult,
ğ‘‰ğ‘’(ğœ‹ğ‘,ğœ‹ğ‘’)=E ğ‘£â€²âˆ¼ğœ‹ğ‘’(cid:2)ğ‘‰ğ‘’(ğœ‹ğ‘,ğ‘£â€²)(cid:3)
fortheevader.
directlylearningajointpolicyofthepursuermemberswouldbe
computationallydifficult.Toaddressthisissue,insteadoflearning 3.2 Policy-SpaceResponseOracles
ajointpolicy,previousworkslearntheindividualpolicieseither
throughvaluedecomposition[21]orglobalcritic[20],whichare
theparadigmofcentralizedtrainingwithdecentralizedexecution Algorithm1:PSROforaspecificPEGG
(CTDE)forthepursuers.Furthermore,previousworks[20]also 1 Î ğ‘ 0 ={ğœ‹ 0ğ‘ },Î ğ‘’ 0 ={ğœ‹ 0ğ‘’},ğ‘ˆ 0,ğœ 0ğ‘ ,ğœ 0ğ‘’ ;
introduceanewstaterepresentationignoringthegameâ€™shistorical 2 forepochk=1,2,Â·Â·Â·ğ¾ do
ğ‘œwi sn
e
ğ‘¡ğ‘–ef
r
A
=o vfr o
a
tm (tl
e
ğ‘™l
i
ğ‘¡ğ‘o
o
aa ,w cnt ğ‘™i
hs
ğ‘¡ğ‘’o t ,n
a
th ğ‘–in, ,me ğ‘¡dw )p eih r
âˆˆn
sei tdc v
e
Oh ii pvo ğ‘–l iu ,ğ‘¡e
d
,
wsa
u
eld y
a
has
l
icm
cp
ht ho oe pn
l
ii
i
num ct cri io sp
e
luun sr do ee
f
erv od se
r
mcd ato
e
lh
lnp
me
pve
bp
lr e aef n uo yrt
r
er i
s
gm o
ru
e
sn
e
t
â€™a s srn cst
a
uc .o ne rr. d
o
eI e nn bfi tso n
e
lou e
r
cr vt ah aw te
t
io oio
o
nr bk
n
s-,
,
63
4
5
UC
C
Exo
o
ppm
m
da
ap
p
n
tu
u
esit
t
moe
e
n
et
t
:h
h
taÎ e
e -ğ‘˜ğ‘
ge pv
au
=
ma rd
s
Î 
eue ğ‘˜ğ‘er mâ€™ âˆ’rs
â€™
a1sB trâˆªBR
ixR
{p ğ‘ˆğœ‹po
ğ‘˜ğ‘
ğ‘˜oli }lc
ti
,
hy
c Î y
rğœ‹ oğ‘˜ğ‘’ğœ‹ğ‘˜ğ‘’ uğ‘˜ğ‘
=
ga hg
a
Î a
g
sğ‘˜ğ‘’i
a
in mâˆ’ins
1
ut
s
âˆª
ltğœ ağ‘˜ ğœğ‘
t{
iâˆ’ ğ‘˜ğ‘’
ğœ‹
oâˆ’1
ğ‘˜
nğ‘’;
1
}
;;
;
t
m
ph
re
oe
m
bi ad
b be
io
r
lif
tğ‘–
yt ch
o
de
n
issp
tt
ru
r
iburs ucu
t ts
ie or
a
nm
p oo
ve lm
ei
rcb tye h2r e,
ğœ‹
aa
ğ‘
cn td
:
ioOt nh ğ‘–e
sâ†’
et tim
Î”
ğ´e
( ğ‘–ğ´
(ğ‘œs ğ‘–t ğ‘¡ğ‘–)e ),p
w
=. hE Nia cc
(h
ğ‘™h
ğ‘¡ğ‘–a
)p
s
,su âˆ€igr ğ‘–s
n
âˆˆu se ğ‘ar
.
87 RetC uro nm :p Î u
ğ‘
ğ¾te ,ğœ Î ğ‘˜ğ‘
ğ‘’
ğ¾a ,n
ğœ
ğ¾d
ğ‘
,ğœ ğœğ‘˜ğ‘’ ğ¾ğ‘’usingameta-solveronğ‘ˆ ğ‘˜;
Asfortheevaderâ€™spolicy,wefollowpreviousworks[37,38]that
2Allthepursuermembersshareonepolicy.Astheobservationsincludepursuersâ€™ids, Asoneofthepopularalgorithms,PSRO[18]canbeemployed
differentpursuermemberscanhavedifferentbehaviors[10].Î”denotesthesimplex. tosolveaPEGG,showninAlgorithm1.Itcommenceswitheach(a) Graphical Rep. (b) Pre-pretraining (c) Pre-training(HMP) (d) Fine-tunning
h T Hypernetwork Solve
Pooling Meta Strategy M
Rep. Layer BR Simulate
GNN â‹¯ ğ‘œ ğ‘¡ğ‘– ğœ½ ğ… Oracle Policy Space
Expand
Evader Pursuer Exit Trained via GraphMAE
Figure1:ArchitectureandtrainingpipelineofGrasper.
playerusingarandompolicy(Line1)andthenexpandsthepolicy suchanissueasthebasepolicyispre-trainedunderthepremise
spacesofthepursuerandevaderinaniterativemanner.Ateach thattheinitialconditionofthePEGisfixed.Inotherwords,anew
epoch1â‰¤ğ‘˜ â‰¤ğ¾:(1)Computethebestresponse(BR)policiesof basepolicymustbepre-trainedfromscratchforthemodifiedPEG
thepursuerğœ‹ğ‘ andevaderğœ‹ğ‘’
andaddthemtotheirpolicyspaces sincetheoriginalbasepolicymaynotbeagoodstartingpointfor
ğ‘˜ ğ‘˜
Î  ğ‘˜ğ‘ and Î  ğ‘˜ğ‘’ (Line 3â€“5); (2) Construct a meta-gameğ‘ˆ ğ‘˜ using all thepursuerâ€™sBRpolicyinthemodifiedgame(evenworsethana
randomlyinitializedBRpolicy).Inthispaper,weaimtoaddressthis
policies in each playerâ€™s policy space (Line 6); (3) Compute the
meta-strategyofthepursuerğœğ‘ âˆˆÎ”(Î ğ‘ )andevaderğœğ‘’ âˆˆÎ”(Î ğ‘’) issuebydevelopingageneralistpursuercapableoflearningand
ğ‘˜ ğ‘˜ ğ‘˜ ğ‘˜
usingameta-solver(e.g.,PRD[18])onthemeta-gameğ‘ˆ ğ‘˜ (Line7). adaptingtodifferentPEGswithvaryinginitialconditionswithout
Theseprocessesarerepeatedforğ¾epochsandthenoutputthefinal theneedtorestartthetrainingprocessfromthebeginning.
meta-strategyacrosstheplayersâ€™policyspaces(Line9).
4 GRASPER
As the evaderâ€™s policy is a probability distribution over exit
nodes,tocomputetheBRpolicy(Line3),weonlyneedtoestimate Inthissection,weintroduceGrasper,illustratedinFigure1.Wefirst
the value of each exit node through simulations,
i.e.,ğ‘‰ğ‘’(ğ‘£â€²)
= presentthearchitectureofGrasperincludingseveralinnovative
E ğœ‹ğ‘âˆ¼ğœğ‘ (cid:2)ğ‘‰ğ‘’(ğœ‹ğ‘,ğ‘£â€²)(cid:3) ,âˆ€ğ‘£â€² âˆˆ ğ‘‰â€².Then,theevaderâ€™sBRpolicyis components,andthenthetrainingpipelinewhichconsistsofthree
ğ‘˜âˆ’1
constructedbyapplyingsoftmaxoperationonthevaluesofallthe stagestoefficientlytrainthenetworksofGrasper.
exitnodes.Forthepursuer,computingtheBRpolicyistosolve
theproblemğœ‹ ğ‘˜ğ‘ = argmaxğœ‹ğ‘E ğœ‹ğ‘’âˆ¼ğœ ğ‘˜ğ‘’ âˆ’1(cid:2)ğ‘‰ğ‘(ğœ‹ğ‘,ğœ‹ğ‘’)(cid:3) (Line4).As 4.1 Architecture
therearemultiplepursuermembers,wecanuseMAPPO[39]to 4.1.1 GraphicalRepresentationsofPEGs. Togeneratethepur-
learntheBRpolicy.InthetraditionalPSROalgorithm,thepursuerâ€™s suerâ€™spolicybasedonthespecificPEG,weproposetotakethe
BRpolicyislearnedfromscratch,i.e.,theBRpolicyisrandomly specificPEGasaninputofaneuralnetwork.Tothisend,weencode
initialized,whichisinefficient.Toaddressthisissue,recentworks theinitialconditionsofaPEGexceptforğ‘‡ intoagraph(Figure1(a)).
integratethepre-trainingandfine-tuningparadigmintoPSROto Thetimehorizonğ‘‡ canbedirectlyfedintotheneuralnetwork.
improvelearningefficiency[20].Specifically,beforerunningPSRO, Specifically,givenaPEGG = (ğº,ğ‘‰â€²,ğ‘™ğ‘ ,ğ‘™ğ‘’,ğ‘‡),theseinitialcondi-
abasepursuerpolicyistrainedthroughmulti-taskRLwhereeach tionsğ‘‰â€²,ğ‘™ğ‘ andğ‘™ğ‘’ canbeencodedinto0 the0 graphğº byassociating
taskisgeneratedwitharandomlyinitializedevaderâ€™spolicy.Then, 0 0
eachnodeofthegraphwithavectorconsistingofthefollowing
ateachPSROepoch,thepursuerâ€™sBRpolicyisinitializedwiththe parts:(i)abinarybit{0,1}where1indicatesthatthenodeisan
pre-trainedbasepolicy,ratherthanlearningfromscratch,which exit,(ii)abinarybit{0,1}where1signifiesthattheevaderâ€™sini-
canlargelyimprovethelearningefficiencyofthePSROalgorithm.
tiallocationisthisnode,(iii)thenumberofpursuersonthisnode
{0,...,ğ‘›}(thetotalnumberofpursuersacrossallnodesequalsto
3.3 ProblemStatement
ğ‘›),and(iv)additionalinformationregardingthetopologyofthe
AlthoughPSROhasbeensuccessfullyappliedtosolvePEGs,un- graph,suchasthedegreeofthenode.Thisprovidesauniversal
fortunately,previousworkstypicallyfocusonsolvingaspecific representationofanyPEGwithanyinitialcondition.
PEGwithpredeterminedinitialconditionswhicharenotalways
fixedinreal-worldscenarios:(i)Theinitiallocationsofthepursuers 4.1.2 Game-conditionalBasePoliciesGeneration. Afterrep-
andtheevader(ğ‘™ğ‘ ,ğ‘™ğ‘’
)arenotalwaysfixedsinceattacks(thieves, resentingaPEGasagraph,itisnaturaltoleverageagraphneural
0 0
crimes,terrorists)canoccuratanytimeandlocationinacity;(ii) network(GNN)toencodethePEGwiththegiveninitialconditions
Thelocationsoftheexitnodesğ‘‰â€²maychangeduetotemporary intoahiddenvector.AsshowninFigure1(b),wefirstfeedthe
closuresandopenings;(iii)Thetimehorizonğ‘‡ mightvary,asthe graphicalrepresentationoftheinitialconditionsintotheGNNand
timerequiredtopursuetheevaderisnotalwaysthesame.When gettherepresentationsofallthenodesofthegraph.Then,weuse
anyoftheinitialconditionschange,thePEGadaptsaccordingly. apoolingoperationtointegrateallthenoderepresentationsintoa
Asaconsequence,currentalgorithmscanonlysolvethemodified hiddenvectorwhichwillbeconcatenatedwiththetimehorizonğ‘‡
PEGfromscratch,leadingtosignificanttimeconsumptionandinef- togetthefinalrepresentationofthePEG.Next,togenerateabase
ficiency.EventheSOTAmethodpresentedintheprevioussection policyconditionalonthePEG,weintroduceahypernetwork[12],
â€“PSROwithpre-trainedbasepursuerpolicyâ€“stillsuffersfrom aneuralnetworkthattakesthefinalrepresentationofthePEGasinputandoutputstheparameters(weightsandbiases)ofthepolicy issolelyemployedtoderivetheeffectiverepresentationfromthe
network(Figure1(c)).Finally,thebasepolicynetworkservesasa PEGâ€™sgraphicalinterpretation,weintroduceapre-pretrainingstage
startingpointforthetrainingofthepursuerâ€™sbestresponsepolicy (Figure1(b))topre-traintheGNNbeforetheactualpre-training
ineachiterationofthePSROalgorithm(Figure1(d)). stage.Thisapproachismoreefficientcomparedtojointlytraining
theGNNandhypernetworkinthepre-trainingstage.Specifically,
4.1.3 ObservationRepresentationLayer. Asdescribedearlier, foreachgameinthetrainingsetG âˆˆI,letğ‘¨Gandğ‘¿Gdenotethe
thepursuerâ€™spolicyisamappingthatassociateseachobservation
adjacencymatrixandfeaturematrixoftheunderlyinggraph,respec-
withaprobabilitydistributionovertheavailableactionset.Notably, tively.Wefirstobtainthelatentcodematrixğ‘¯G =ğ‘“ GNN(ğ‘¿G,ğ‘¨G)
anobservationconsistsofthepositionsofbothplayers.Represent-
bytheGNNandtraintheGNNviaanyself-supervisedgraphlearn-
ingthesepositionsbymereindexnumbersofverticesinthegraph
ingmethod(weusetherecentSOTAmethod,GraphMAE[15]).
does not provide much useful information for training, though.
Then,wegetthehiddenvectorbypoolingthelatentcodematrix
Therefore,weseekamorecompactandmeaningfulrepresenta- ğ’‰G =ğ‘ğ‘œğ‘œğ‘™(ğ‘¯G),whichwillbefedintothehypernetwork.
tionoftheseobservations.Previousworks[20,38]typicallytrain
anodeembeddingmodelforthispurpose.Unfortunately,sucha
Algorithm2:Pre-training
modelisoftentailoredandtrainedforaspecificgraph,limitingits
generalizabilitytoothergraphs.Thislackofgeneralizabilitymakes 1
InitializeGrasperandtheepisodebufferD â†âˆ…;
thismethodunsuitableforourproblem.Toaddressthisissue,we 2 fortrainepoch=1,2,Â·Â·Â· do
adoptarepresentationlayertoencodethepursuerâ€™sobservations, 3
Uniformlysampleğ‘ 1gamesfromthetrainingsetI;
anapproachthatisnotlimitedtoaspecificgraph. 4 foreachoftheğ‘ 1gamesGdo
AsgiveninSection3.1,thepursuerâ€™sobservationğ‘œ ğ‘¡ğ‘– =(ğ‘™ ğ‘¡ğ‘ ,ğ‘™ ğ‘¡ğ‘’,ğ‘–,ğ‘¡) 5 Randomlygenerateğ‘ 2evaderâ€™spolicies;
includesthreeparts:theplayersâ€™currentlocations(ğ‘™ ğ‘¡ğ‘ ,ğ‘™ ğ‘¡ğ‘’),thepur- 6 Generatepursuerâ€™spolicyğœ‹ ğœ½ğ‘ â†Grasper(G);
suermemberâ€™sidğ‘–,andthecurrenttimestepğ‘¡.Thus,therepre- 7 foreachoftheğ‘ 2evaderâ€™spoliciesğœ‹ğ‘’ do
sentationlayerconsistsofthreecomponents,eachofwhichisa 8 Sampledatausingğœ‹ğ‘’ ,ğœ‹ ğœ½ğ‘ ,andğœ‹Ë†ğ‘ ;
â€œtorch.nn.Embeddingâ€whichhasbeenextensivelyusedtoencode
9
AddthedataintotheepisodebufferD;
anintegertoacompactrepresentation.Theoutputsofthethree
10
Trainthenetworksbyoptimizingthelossfunctionğ¿;
componentsareconcatenatedtoobtaintherepresentationofthe
11
CleartheepisodebufferD â†âˆ…;
pursuerâ€™sobservation.Thisrepresentationlayerwillbetrained
jointly with the hypernetwork during the pre-training process.
PleaserefertoAppendixBfordetailsonthearchitectureofthe 4.2.2 StageII:Pre-training. Givenafixedevaderâ€™spolicyina
representationlayer,theGNN,andthehypernetwork. specificPEG,computingthepursuerâ€™sbestresponsepolicycanbe
Intuitively,thegeneralizationabilityofGrasperbenefitsfrom seenasanRLtask.Thus,wecanapplythemulti-taskRLalgorithm
severaldesignsofourarchitecture.First,thegraphicalrepresen- toguidethepre-trainingprocess,whichisshowninAlgorithm2.
tationoffersauniversalrepresentationofanyPEG,regardlessof Differentfrompreviouswork[20],intheseRLtasks,exceptfor
thegraphâ€™stopology.Second,GNNcanencodedifferentPEGsinto thechangeintheevaderâ€™spolicy,thegameâ€™sinitialconditionsalso
fixed-sizehiddenvectors,whichcanbedirectlyfedintothehy- change.ToobtaintheseRLtasksforpre-training,wefirstrandomly
pernetwork(otherwise,additionaltechniquesarerequiredifthe sampleğ‘ 1gamesfromthetrainingset(Line3),andthenforeach
sizesofthehiddenvectorsarevaried).Finally,thehypernetworkis game,werandomlysampleğ‘ 2evaderâ€™spolicies(Line5).Oncethe
designedtogenerateaspecializedpolicytailoredtoagivenPEG. gameandtheevaderâ€™spolicyarefixed,theRLtaskisgenerated.
Duringpre-training,foreachgame,wefirstfeedthehiddenvector
4.2 TrainingPipeline ofthegame(obtainedbythetrainedGNN)andthetimehorizon
NowweintroducethetrainingpipelineofGrasper,whichinvolves intothehypernetworktogeneratethepursuerâ€™sbasepolicy,and
threestages:pre-pretraining,pre-training,andfine-tuning.Priorto thenforeachevaderâ€™spolicy,wecollectthetrainingdatausing
delvingintothespecifics,wefirstdescribehowthetrainingsetis thepursuerâ€™sbasepolicy(accompanybytherepresentationlayer)
generated.ThetrainingsetIshouldconsistofdifferentPEGsfor intotheepisodebuffer.Finally,wetrainthehypernetworkand
training.Tothisend,wegeneratethetrainingsetbyrandomizing therepresentationlayerjointlybasedontheepisodebuffer(Lines
theinitialconditions,denotedby
(ğº,ğ‘‰â€²,ğ‘™ğ‘ ,ğ‘™ğ‘’,ğ‘‡).However,this 6-9).Todealwiththemultiplepursuermemberscases,weemploy
0 0
MAPPO[39]astheunderlyingRLalgorithm.
approachmayyieldcertaingamesthatlackmeaningfultraining
However,wefoundthatsimplyapplyingtheMAPPOunderthe
value.Forexample,whentheevaderâ€™sinitiallocationisinsuchclose
multi-tasklearningframeworkcanresultinlowefficiencydueto
proximitytotheexitnodesthatthepursuerbecomesincapableof
randomexplorationintheenvironment.Toclarify,considerthe
capturingtheevaderregardlessofitsmovements.Toexcludethese
exampleillustratedinFigure2,whichshowstheneedforamore
trivialcases,weintroduceafilterconditionwhengeneratingthe
efficientpre-trainingmethod.Assumethattheevaderâ€™spolicyisto
trainingset:theshortestpathfromtheevaderâ€™sinitiallocationto
taketheshortestpathtooneoftheexits(denotedbytheredpath).If
anyexitnodesmustexceedapredeterminedlength.
thepursuerexplorestheenvironmentrandomly(theorangepath),
4.2.1 StageI:Pre-pretraining. Asthehypernetworktakesafea- itwillprobablylosethegameandthenreceiveanegativereward.
turevectorasinput,wefirstuseaGNNtoencodethegraphicalrep- Thissituationcanoccurfrequentlyatthebeginningofthepre-
resentationofthePEGintoafixed-sizehiddenvector.AstheGNN trainingprocessbecausethepursuerâ€™sinitialpolicyisinvariablyrandom.Tomitigatethisexplorationinefficiency3,weproposea computationofthepursuerâ€™sBRpolicy(Line5),ratherthantraining
novelscheme:heuristic-guidedmulti-taskpre-training(HMP). fromscratch.Thisallowsustosimplyfine-tunethepre-trained
ğ‘
policyğœ‹ overafewepisodes(Line6)toquicklyobtaintheBR
0
Succeed! policy,significantlyenhancingthelearningefficiency.
ğ’“ğ’‘=+ğŸ
Reference
Policy Algorithm3:Fine-tuning
Guided 1 Require:Grasper,PEGG;
Exploration 2 Î ğ‘ 0 ={ğœ‹ 0ğ‘ â†Grasper(G)},Î ğ‘’ 0 ={ğœ‹ 0ğ‘’},ğ‘ˆ 0,ğœ 0ğ‘ ,ğœ 0ğ‘’ ;
3 forepochk=1,2,Â·Â·Â·ğ¾ do
ExR pa ln od rao tm
io n
ğ’“F ğ’‘a =ile âˆ’d ğŸ! 4
5
C Ino itm iap liu zt ee tt hh ee pe uv ra sd ue er râ€™ â€™s sB BR Rp po ol li ic cy yğœ‹ ğœ‹ğ‘˜ğ‘’ ğ‘˜ğ‘a â†gai ğœ‹n 0s ğ‘t ;ğœ ğ‘˜ğ‘ âˆ’1;
Evader Pursuer Exit 76 ET xra pi an nğœ‹ siğ‘˜ğ‘ ona :g Î ai ğ‘˜ğ‘ns =tğœ Î ğ‘˜ğ‘’ ğ‘˜ğ‘âˆ’ âˆ’1 1f âˆªor {f ğœ‹e ğ‘˜w ğ‘ }e ,p Î i ğ‘˜ğ‘’so =de Î s; ğ‘˜ğ‘’ âˆ’1âˆª{ğœ‹ ğ‘˜ğ‘’};
8 Updatemeta-gamematrixğ‘ˆ ğ‘˜ throughsimulation;
Figure2:IllustrationofHMP. 9 Computeğœ ğ‘˜ğ‘ andğœ ğ‘˜ğ‘’ usingameta-solveronğ‘ˆ ğ‘˜;
10
Return:Î ğ‘ ğ¾,Î ğ‘’ ğ¾,ğœ ğ¾ğ‘ ,ğœ ğ¾ğ‘’
NotethatintheRLtasksusedforpre-training,wecanacquire
theevaderâ€™spolicy,whichcanbeusedtoguidetheexploration
ofthepursuerâ€™spolicy.Specifically,giventheexitnodechosen
bytheevaderâ€™spolicyğœ‹ğ‘’ ,wefirstinduceareferencepolicyğœ‹Ë†ğ‘ 5 EXPERIMENTS
(represented by the green path) for the pursuer using heuristic Inthissection,weperformexperimentstoevaluatetheperformance
methodssuchastheDijkstraalgorithm.Then,apartfromtheac- ofGrasperandtheeffectivenessofdifferentcomponents4.
ğ‘
tionssampledbythegeneratedpolicyğœ‹ (Line6),wealsosample
thereferenceactionsusingthereferenğœ½ cepolicyğœ‹Ë†ğ‘
andaddthe
5.1 Setup
datatothetrainingbuffer(Lines8-9).Letğ¿(ğœ½)denotetheoriginal Hyperparameters.Thenumberofpursuersisğ‘›=5,thenumber
lossfunctionfortrainingtheactorintheMAPPOalgorithm.The ofexitnodesis8,thetimehorizonğ‘‡ is6â‰¤ğ‘‡ â‰¤10,andthenumber
HMPisimplementedbyintroducinganadditionallossintothe ofpre-trainingepisodesis20million(20M).ForPSRO,thenumber
originallossfunction:ğ¿ = ğ¿(ğœ½)+ğ›¼KL(ğœ‹ğ‘âˆ¥ğœ‹Ë†ğ‘) whereğ›¼ âˆˆ [0,1] ofepisodesusedfortrainingthebestresponseis10.Weconduct
controlstheweightoftheguidanceofthereferencepolicyand experiments on four maps: the grid map with size 10Ã—10, the
KLrepresentstheKullbackâ€“Leiblerdivergence(forthereference scale-freegraphwith300nodes,theSingaporemap[38]with372
policyğœ‹Ë†ğ‘
,theactionprobabilitydistributionisobtainedbysetting nodes,andtheScotland-Yardmap[28]with200nodes.Tosimulate
theprobabilityofthereferenceactionto1whileallothersto0). thesituationwherearoadmightbetemporallyblockeddueto
congestionortrafficaccidents,wesettheprobabilityofanedge
4.2.3 StageIII:Fine-tuning. Inthisphase,weintegratethepre-
betweentwonodesto0.8forthegridmap,0.9fortheSingapore
trainedpursuerpolicyintothePSROframework,asshowninAl-
gorithm3.Thepursuerâ€™spolicyğœ‹ğ‘
isinitializedusingtheoutput
map,and1.0(i.e.,nocongestion)fortheothertwomaps.More
0 detailsonthehyperparameterscanbefoundinAppendixB.
neuralnetworkfromthepre-trainedGrasper,whichtakesthegraph-
Worst-caseUtility.GiventhataPEGisazero-sumgame,weuse
icalrepresentationofthespecificPEGasaninput.Simultaneously,
theevaderâ€™spolicyğœ‹ğ‘’
israndomlyinitialized(Line2).Thenwe
thepursuerâ€™sworst-caseutility(astheevaderalwayschoosesthe
f ro el sl po ow nsth ee (Bst Ra )n pd oa lr id ciP eS s0 R foO rf br oa tm he pw lao yrk er: sin ,ğœ‹e ğ‘˜ğ‘ac ah ndite ğœ‹r ğ‘˜ğ‘’a ,ti ao rn eğ‘˜ co,t mh pe ub te es dt s t inhh neo er qt re u es a xt l pp it ea y cth to aff tr ito ohm ne it s sh o te l aui kn t eii o nti na o:l vğ‘¢l eo ğ‘ rca t= hti eo En tğœ‹ rt ağ‘o jâˆ¼ et ğœ ch ğ‘ te o,ğœ‹c rğ‘’ ih eâˆ¼o sğœs ğ‘’ ie nEn d[e uğ‘Ÿx cğ‘i et ] d) ,t w bo yhm e ğœ‹e r ğ‘a es atu nhr dee
u ths ein ng adth de ei dr tr oes tp he ec pti ov le icyBR seo tsra Î c ğ‘le as n(L di Î ne ğ‘’s (L4- in6) e. 7T )h ,ae nse dB thR ep mo eli tc ai -e gs aa mre
e
ğœ‹ğ‘’ whicharerespectivelysampledaccordingtoğœğ‘ andğœğ‘’
.
ğ‘˜ ğ‘˜ TrainingandTestSets.(1)Wegenerate |I| = 1000gamesas
matrixğ‘ˆ ğ‘˜ isupdatedthroughsimulation(Line8).Finally,themeta
distribution(ğœğ‘ ,ğœğ‘’)iscomputedusinganymeta-solver(Line9). thetrainingset.Duringthegeneration,theminimumlengthof
ğ‘˜ ğ‘˜ theevaderâ€™sshortestpathissetto6forthegridmapand5for
TheBRoraclesforbothplayersaretheimportantcomponents
othermaps.(2)Wecreatetwotestsets,I 1andI 2,eachcontaining
ofthePSROalgorithm.Theevaderâ€™sBRoraclefollowsthestandard
30games.(i)I 1includesthegamessampledfromthetrainingset
PSROalgorithmgiveninAlgorithm1.Thekeydifferencebetween
I 1 âŠ‚I(in-distributiontest).(ii)I 2containsthegamesdistinctfrom
ourfine-tuningprocessandthestandardPSROalgorithmliesin
thetrainingsetI 2âˆ©I=âˆ…(out-of-distributiontest).Toavoidtrivial
thetrainingofthepursuerâ€™sBRpolicy,whichishighlightedin
ğ‘ cases(thegamesthatareeithertoodifficultortoosimpleforthe
blue.Specifically,giventhepre-trainedpolicyğœ‹ conditionalto
0 pursuers),weconstrainthezero-shotperformanceofGrasper(i.e.,
theinitialconditions,wecanuseitasthestartingpointforthe
theworst-caseutilityofthegeneratedpolicywithoutfine-tuning)
3NoticethatmanyexplorationmethodsinRLsuchasRND[5]typicallyencourage withintherange:[0.8,0.9]forI 1and[0.1,0.2]forI 2.
thepolicytoexplorenovelstatesoftheenvironment,whicharedifferentfromour
designwhererandomexplorationislessfavored. 4Thecodeisavailableathttps://github.com/IpadLi/Grasper. * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P  * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P
                   
     1      2      1      2
                   
                   
                   
                   
                   
                   
                   
                                                                                   
 5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V 
(a)GridMap (b)Scale-FreeMap
 * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P  * U D V S H U  0 7  3 6 5 2  0 7  3 6 5 2  $ X J  3 6 5 2  5 D Q G R P
                   
     1      2      1      2
                   
                   
                   
                   
                   
                   
                   
                                                                                       
 5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V   5 X Q W L P H   V 
(c)SingaporeMap (d)Scotland-YardMap
Figure3:Evaluationperformance.Theshadedarearepresentsthestandarderror.
Baselines. (i) Multi-task PSRO (MT-PSRO): the state-of-the-art bealsopartlyverifiedbycomparingMT-PSROandMT-PSRO-Aug
(SOTA)approachadaptedfrom[20],whichalsousestheobserva- wheretheirperformanceiscomparable,meaningthatnaivelyinte-
tionrepresentationlayerandHMP.(ii)MT-PSROwithaugmen- gratingtheinformationabouttheinitialconditionsdoesnotbring
tation(MT-PSRO-Aug):thehiddenvectorobtainedfromthepre- muchbenefitandnoveldesignsarenecessary.(iii)Aninteresting
trainedGNNandthetimehorizonareconcatenatedtotheoutput resultisthatevenonthetestsetI 1(in-distributiontest),MT-PSRO
oftheobservationrepresentationlayer.(iii)PSRO:thestandard andMT-PSRO-Aug,thestrongestbaselines,performworseonall
PSROmethod.(iv)Random:thepursuerrandomlyselectsactions. theothermapsthanonthegridmap.Wehypothesizethereason
isthattheothermapsaremoreheterogeneousthanthegridmap.
5.2 Results Forexample,thedegreeofthenodesvariesfrom1to16intheSin-
TheexperimentalresultsaresummarizedinFigure3.Theğ‘¥-axisis gaporemapwhileitremainsbetween2to4inthegridmap.Thus,
therunningtime.Forthepurposeofafaircomparison,apartfrom thegamesgeneratedontheSingaporemapsharemuchlesssimi-
therunningtimeofthefine-tuningstage(thePSROprocedure),we larity.Inthissense,theinformationabouttheinitialconditionsis
alsoincludetherunningtimeofpre-pretrainingandpre-training particularlyimportantwhensolvingdifferentPEGs.(iv)Inallcases,
(called the pre-training time for convenience). Since the games theperformanceofGrasperismuchmorestablethanthebaselines
inthetrainingsetareuniformlyrandomlysampledduringpre- (smallerstandarderror)asGraspercangeneratedistinctpoliciesfor
training,weapproximatethepre-trainingtimeofeachgameby differentPEGs.Incontrast,otherbaselineseitherentirelyignore
averagingthetotalpre-trainingtimeoverthetrainingset.Then, ornaivelyintegratetheinformationabouttheinitialconditions
foreachtestinggame,weaddthepre-trainingtimetotherunning ofthePEGs,whichrendersthemhardtogeneralizetodifferent
time(thehorizontalgapbetween0andthestartoftheline).Note PEGs,leadingtolargerperformancevariancethanGrasper.(v)The
thattheamortizedpre-trainingtimeforGrasper,MT-PSRO,and resultsonthetestsetI 2showthatGraspercansolveunseengames,
MT-PSRO-Augissimilarsincethepre-pretrainingtimeisveryshort exhibitingbettergeneralizabilitythanthebaselines.
(Table2).Fromtheresults,wecandrawseveralconclusions.
5.3 Ablations
(i)Givenafixednumberofepisodesforthefine-tuningprocess,
Graspercanstartfromandconvergetoahigheraverageworst-case EffectivenessofDifferentModules.First,westudythecontri-
utilitythanthebaselines,althoughittakesacertainpre-training butionofHMPandtheobservationrepresentationlayer(Rep.)to
time,demonstratingtheeffectivenessofthepre-pretrainingandpre- theperformanceofGrasper,asshowninTable1.Theresultsshow
traininginacceleratingthePSROprocedure.NotethatMT-PSRO thatwecangetbetterperformance(highworst-caseutilityand
andMT-PSRO-Augalsoemploypre-pretrainingorpre-training,but smallstandarderror)onlywhencombiningthetwocomponents,
theyperformworsethanGrasper,showcasingthesuperiorityof meaningthatbothtwocomponentsareindispensableforGrasper.
Grasper.(ii)Forafaircomparison,MT-PSRO-Augalsointegrates EffectivenessofPre-pretraining.Next,weinvestigatetheef-
theinformationabouttheinitialconditionsofthePEGs.Theresults fectivenessofthepre-pretrainingstageinacceleratingthewhole
clearlyshowthenecessityofthehypernetworkinGrasper.Thiscan trainingprocedureofGrasper.SincejointlytrainingGNNandthe
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :
 \ W L O L W 8  H V D F  W V U R :Table1:Ablationstudies.Theresultsareobtainedinthegrid isnearthetwoexits,itcouldbeeasyforthepursuertocatchthe
map.âœ“meansthemoduleisused.
evader,eventhoughthereisonlyonepursuerinthisarea.There-
sultsreflecttheintuitionthatGraspercangeneratedistinctpolicies
HMP âœ“ âœ“ fordifferentgamesandhence,theperformanceismorestable.
I 1 Rep. âœ“ âœ“
Utility 0.90Â±0.01 âˆ’0.54Â±0.06 âˆ’0.05Â±0.17 âˆ’0.52Â±0.08
 
           
 
           
HMP âœ“ âœ“                            
I 2 Rep. âœ“ âœ“              
         
Utility 0.45Â±0.04 âˆ’0.60Â±0.06 âˆ’0.64Â±0.11 âˆ’0.63Â±0.06              
         
   
         
                   
 [  [
otherpartsofGrasperfor20Mpre-trainingepisodesrequiresa
longrunningtime,inthisablationstudy,wefocusonthefirst2M
Figure5:Zero-shotworst-caseutilityofthepursuerforeach
pre-trainingepisodesandcomparetherunningtimeofGrasper
possibleevaderâ€™sinitiallocation.Reddotsareexitsandblue
withpre-pretraining(w/PP)andwithoutpre-pretraining(w/oPP).
dotsarepursuersâ€™initiallocations.
ThetrainingcurvesareshowninFigure4,whichshowsthatusing
pre-pretrainingcansignificantlyacceleratethetrainingprocedure
(3.9timesfasterthanwithoutusingpre-pretraining).
6 CONCLUSIONS
Inthiswork,weinvestigatehowtoefficientlysolvedifferentPEGs
        [
withvaryinginitialconditions.First,weproposeanovelgener-
alizableframework,Grasper,whichincludesseveralcriticalcom-
    ponents:(i)aGNNtoencodeaspecificPEGintoahiddenvector,
(ii)ahypernetworktogeneratethebasepoliciesforthepursuers
    conditionalonthehiddenvectorandtimehorizon,(iii)anobserva-
tionrepresentationlayertoencodethepursuersâ€™observationsinto
 Z  R  3 3  Z   3 3
compactandmeaningfulrepresentations.Second,weintroduce
   
anefficientthree-stagetrainingmethodwhichincludes:(i)apre-
                          pretrainingstagethatlearnsrobustPEGrepresentationsthrough
 7 L P H   V  GraphMAE,(ii)aheuristic-guidedmulti-taskpre-trainingstage
thatleveragesareferencepolicyderivedfromheuristicmethods
suchasDijkstratoregularizepursuerpolicies,and(iii)afine-tuning
Figure4:Pre-trainingcurves.
stagethatutilizesPSROtogeneratepursuerpoliciesondesignated
PEGs.Finally,extensiveexperimentsdemonstratethesuperiority
Thequantitativevaluesoftherunningtimeofthepre-pretraining ofGrasperoverbaselinesintermsofsolutionqualityandgeneral-
andpre-trainingaregiveninTable2.Asthepre-pretrainingtime izability.Tothebestofourknowledge,thisisthefirstattemptto
(304.2seconds)ismuchshorterthanthepre-trainingtime(9954.9 considerthegeneralizationprobleminthedomainofPEGs.Future
seconds), the curves of Grasper, MT-PSRO, and MT-PSRO-Aug directionsinclude(i)moreefficienttasksamplingstrategiesfor
showninFigure3startfromasimilarpositionintheğ‘¥-axis.
pre-training,e.g.,AdA[1],(ii)amodelcapableofgeneralizingto
differentPEGswithdifferentunderlyinggraphtopologies,e.g.,gen-
Table2:Runningtime(second). eralizingfromgridmapstoscale-freemaps,and(iii)amodelcapable
oftacklingmorecomplexsettings,e.g.,learning-basedevader.
w/oPP w/PP
ACKNOWLEDGMENTS
Pre-pretraining N/A 304.2
ThisworkissupportedbytheNationalResearchFoundation,Singa-
Pre-training 39977.3 9954.9
poreunderitsIndustryAlignmentFundâ€“Pre-positioning(IAF-PP)
Total 39977.3 10259.1(3.9x) FundingInitiative.Anyopinions,findingsandconclusions,orrec-
ommendationsexpressedinthismaterialarethoseoftheauthor(s)
InfluenceofEvaderâ€™sInitialLocation.Weperformsomeex- anddonotreflecttheviewsofNationalResearchFoundation,Sin-
perimentsusingGraspertoprovidesomeinsightsintothePEG. gapore.YouzhiZhangissupportedbytheInnoHKFund.HauChan
InFigure5,wepresentthepursuerâ€™sutilitywhentheevaderran- issupportedbytheNationalInstituteofGeneralMedicalSciences
domizestheinitiallocationoverthegridmap.Wefoundthatin oftheNationalInstitutesofHealth[P20GM130461],theRuralDrug
someareasthepursuerscanhavehighutility.Forexample,inthe AddictionResearchCenterattheUniversityofNebraska-Lincoln,
top-rightoftheleftfigure,therearethreepursuersandonlyone andtheNationalScienceFoundationundergrantIIS:RI#2302999.
exit,whichmeansitcouldbehardfortheevadertoescape.Inthe Thecontentissolelytheresponsibilityoftheauthorsanddoesnot
bottom-rightoftherightfigure,asthepursuerâ€™sinitiallocation necessarilyrepresenttheofficialviewsofthefundingagencies.
 \ W L O L W 8  J Q L Q L D U 7
 \
 \ W L O L W 8  H V D F  W V U R :
 \
 \ W L O L W 8  H V D F  W V U R :REFERENCES
neuralequilibriumsolvers.InNeurIPS.5586â€“5600.
[1] AdaptiveAgentTeam,JakobBauer,KateBaumli,SatinderBaveja,FeryalBe- [25] JiezhongQiu,QibinChen,YuxiaoDong,JingZhang,HongxiaYang,MingDing,
hbahani,AvishkarBhoopchand,NathalieBradley-Schmieg,MichaelChang,Na- KuansanWang,andJieTang.2020.GCC:Graphcontrastivecodingforgraph
talieClay,AdrianCollister,VibhavariDasagi,LucyGonzalez,KarolGregor, neuralnetworkpre-training.InSIGKDD.1150â€“1160.
EdwardHughes,SheleemKashem,MariaLoks-Thompson,HannahOpenshaw, [26] FrederickPRivaraandChristopherDMack.2004.Motorvehiclecrashdeaths
JackParker-Holder,ShreyaPathak,NicolasPerez-Nieves,NemanjaRakicevic, relatedtopolicepursuitsintheUnitedStates. InjuryPrevention10,2(2004),
TimRocktÃ¤schel,YannickSchroecker,JakubSygnowski,KarlTuyls,SarahYork, 93â€“95.
AlexanderZacherl,andLeiZhang.2023. Human-timescaleadaptationinan [27] SebastianRuder.2017.Anoverviewofmulti-tasklearningindeepneuralnet-
open-endedtaskspace.arXivpreprintarXiv:2301.07608(2023). works.arXivpreprintarXiv:1706.05098(2017).
[28] MartinSchmid,MatejMoravÄÃ­k,NeilBurch,RudolfKadlec,JoshDavidson,
[2] NoaAgmon,GalAKaminka,andSaritKraus.2011. Multi-robotadversarial
KevinWaugh,NolanBard,FinbarrTimbers,MarcLanctot,GZachariasHol-
patrolling:facingafull-knowledgeopponent. JournalofArtificialIntelligence
land,DavoodiDavoodi,AldenChristianson,andMichaelBowling.2023.Student
Research42(2011),887â€“916.
ofGames:aunifiedlearningalgorithmforbothperfectandimperfectinformation
[3] ShaunakDBopardikar,FrancescoBullo,andJoaoPHespanha.2008.Ondiscrete-
games.ScienceAdvances9,46(2023),eadg3256.
timepursuit-evasiongameswithsensinglimitations.IEEETransactionsonRo-
[29] EfratSless,NoaAgmon,andSaritKraus.2014.Multi-robotadversarialpatrolling:
botics24,6(2008),1429â€“1439.
facingcoordinatedattacks.InAAMAS.1093â€“1100.
[4] JanBuermannandJieZhang.2022.Multi-robotadversarialpatrollingstrategies
[30] MilindTambe.2011.SecurityandGameTheory:Algorithms,DeployedSystems,
vialatticepaths.ArtificialIntelligence311(2022),103769.
LessonsLearned.CambridgeUniversityPress.
[5] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2018.Exploration
[31] ShantanuThakoor,CorentinTallec,MohammadGheshlaghiAzar,MehdiAzabou,
byrandomnetworkdistillation.InICLR.
EvaLDyer,RemiMunos,PetarVeliÄkoviÄ‡,andMichalValko.2022.Large-scale
[6] RonanCollobertandJasonWeston.2008. Aunifiedarchitecturefornatural
representationlearningongraphsviabootstrapping.InICLR.
languageprocessing:Deepneuralnetworkswithmultitasklearning.InICML.
[32] JasonTsai,ZhengyuYin,Jun-youngKwak,DavidKempe,ChristopherKiek-
160â€“167.
intveld,andMilindTambe.2010.Urbansecurity:Game-theoreticresourceallo-
[7] ZhijianDuan,WenhanHuang,DinghuaiZhang,YaliDu,JunWang,Yaodong
cationinnetworkeddomains.InAAAI.881â€“886.
Yang,andXiaotieDeng.2023.IsNashequilibriumapproximatorlearnable?.In
[33] ReneVidal,OmidShakernia,HJinKim,DavidHyunchulShim,andShankar
AAMAS.233â€“241.
Sastry.2002.Probabilisticpursuit-evasiongames:Theory,implementation,and
[8] ZhijianDuan,YunxuanMa,andXiaotieDeng.2023.Areequivariantequilibrium
experimentalevaluation. IEEETransactionsonRoboticsandAutomation18,5
approximatorsbeneficial?arXivpreprintarXiv:2301.11481(2023).
(2002),662â€“669.
[9] XidongFeng,OliverSlumbers,ZiyuWan,BoLiu,StephenMcAleer,YingWen,
[34] Tung-LongVuong,Do-VanNguyen,Tai-LongNguyen,Cong-MinhBui,Hai-
JunWang,andYaodongYang.2021.Neuralauto-curriculaintwo-playerzero-sum
DangKieu,Viet-CuongTa,Quoc-LongTran,andThanh-HaLe.2019.Sharing
games.InNeurIPS.3504â€“3517.
experienceinmultitaskreinforcementlearning.InIJCAI.3642â€“3648.
[10] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,and
[35] YuandaWang,LuDong,andChangyinSun.2020.Cooperativecontrolformulti-
ShimonWhiteson.2018.Counterfactualmulti-agentpolicygradients.InAAAI.
playerpursuit-evasiongameswithreinforcementlearning.Neurocomputing412
2974â€“2982.
(2020),101â€“114.
[11] RossGirshick.2015.FastR-CNN.InICCV.1440â€“1448.
[36] AaronWilson,AlanFern,SoumyaRay,andPrasadTadepalli.2007.Multi-task
[12] DavidHa,AndrewM.Dai,andQuocV.Le.2017.HyperNetworks.InICLR.
reinforcementlearning:AhierarchicalBayesianapproach.InICML.1015â€“1022.
[13] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollÃ¡r,andRossGirshick.
[37] WanqiXue,BoAn,andChaiKiatYeo.2022.NSGZero:efficientlylearningnon-
2022.Maskedautoencodersarescalablevisionlearners.InCVPR.16000â€“16009.
exploitablepolicyinlarge-scalenetworksecuritygameswithneuralMonteCarlo
[14] KarelHorÃ¡kandBranislavBoÅ¡ansky`.2017.Dynamicprogrammingforone-sided
treesearch.InAAAI.4646â€“4653.
partiallyobservablepursuit-evasiongames.InICAART.503â€“510.
[38] WanqiXue,YouzhiZhang,ShuxinLi,XinrunWang,BoAn,andChaiKiatYeo.
[15] ZhenyuHou,XiaoLiu,YukuoCen,YuxiaoDong,HongxiaYang,ChunjieWang,
2021. Solvinglarge-scaleextensive-formnetworksecuritygamesvianeural
andJieTang.2022.GraphMAE:self-supervisedmaskedgraphautoencoders.In
fictitiousself-play.InIJCAI.3713â€“3720.
KDD.594â€“604.
[39] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,
[16] LiHuang,MengChuZhou,KuangrongHao,andEdwinHou.2019.Asurveyof
andYiWu.2022.ThesurprisingeffectivenessofPPOincooperativemulti-agent
multi-robotregularandadversarialpatrolling.IEEE/CAAJournalofAutomatica
games.InNeurIPSDatasetsandBenchmarksTrack.24611â€“24624.
Sinica6,4(2019),894â€“903.
[40] SihanZeng,MalikAqeelAnwar,ThinhTDoan,ArijitRaychowdhury,andJustin
[17] LinanHuangandQuanyanZhu.2021.Adynamicgameframeworkforrational
Romberg.2021.Adecentralizedpolicygradientapproachtomulti-taskreinforce-
andpersistentrobotdeceptionwithanapplicationtodeceptivepursuit-evasion.
mentlearning.InUAI.1002â€“1012.
IEEETransactionsonAutomationScienceandEngineering19,4(2021),2918â€“2932.
[41] HengruiZhang,QitianWu,JunchiYan,DavidWipf,andPhilipSYu.2021.
[18] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,Karl
Fromcanonicalcorrelationanalysistoself-supervisedgraphneuralnetworks.In
Tuyls,JulienPÃ©rolat,DavidSilver,andThoreGraepel.2017. Aunifiedgame-
NeurIPS.76â€“89.
theoreticapproachtomultiagentreinforcementlearning.InNeurIPS.4190â€“4203.
[42] YouzhiZhang,BoAn,LongTran-Thanh,ZhenWang,JiaruiGan,andNicholasR
[19] PengdengLi,XinrunWang,ShuxinLi,HauChan,andBoAn.2023.Population-
Jennings.2017.Optimalescapeinterdictionontransportationnetworks.InIJCAI.
size-awarepolicyoptimizationformean-fieldgames.InICLR.
3936â€“3944.
[20] ShuxinLi,XinrunWang,YouzhiZhang,WanqiXue,JakubÄŒernÃ½,andBoAn.
[43] YouzhiZhang,QingyuGuo,BoAn,LongTran-Thanh,andNicholasRJennings.
2023.Solvinglarge-scalepursuit-evasiongamesusingpre-trainedstrategies.In
2019.Optimalinterdictionofurbancriminalswiththeaidofreal-timeinforma-
AAAI.11586â€“11594.
tion.InAAAI.1262â€“1269.
[21] ShuxinLi,YouzhiZhang,XinrunWang,WanqiXue,andBoAn.2021.CFR-MIX:
[44] YuZhangandQiangYang.2021.Asurveyonmulti-tasklearning.IEEETransac-
Solvingimperfectinformationextensive-formgameswithcombinatorialaction
tionsonKnowledgeandDataEngineering34,12(2021),5586â€“5609.
space.InIJCAI.3663â€“3669.
[45] MandiZhao,PieterAbbeel,andStephenJames.2022. Ontheeffectivenessof
[22] XiuxianLi,MinMeng,YiguangHong,andJieChen.2022.Asurveyofdecision
fine-tuningversusmeta-reinforcementlearning.InNeurIPS.26519â€“26531.
makinginadversarialgames.arXivpreprintarXiv:2207.07971(2022).
[46] YanqiaoZhu,YichenXu,FengYu,QiangLiu,ShuWu,andLiangWang.2021.
[23] VictorGLopez,FrankLLewis,YanWan,EdgarNSanchez,andLinglingFan.
Graphcontrastivelearningwithadaptiveaugmentation.InWWW.2069â€“2080.
2019.Solutionsformultiagentpursuit-evasiongamesoncommunicationgraphs:
[47] MartinZinkevich,MichaelJohanson,MichaelBowling,andCarmeloPiccione.
Finite-timecaptureandasymptoticbehaviors.IEEETransactionsonAutomatic
2008.Regretminimizationingameswithincompleteinformation.InNeurIPS.
Control65,5(2019),1911â€“1923.
1729â€“1736.
[24] LukeMarris,IanGemp,ThomasAnthony,AndreaTacchetti,SiqiLiu,andKarl
Tuyls.2022.Turbochargingsolutionconcepts:SolvingNEs,CEsandCCEswith