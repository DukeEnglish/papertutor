LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliency
Prediction
FLORIANSTROHM,InstituteforVisualizationandInteractiveSystems,Germany
MIHAIBÃ‚CE,InstituteforVisualizationandInteractiveSystems,Germany
ANDREASBULLING,InstituteforVisualizationandInteractiveSystems,Germany
Fig.1. Ourmethod(left)takesasinputasmallsetofimagesandtheircorrespondinguser-specificsaliencymapsobtainedfrom
humangazerecordedusingastationaryeyetrackerandproducesoneuserembeddingforeachuser,whichcapturestheuser-specific
differencesinviewingbehaviour.Wethendemonstrate(right)howthelearnedembeddingscanbeusedtorefineauniversalsaliency
map(e.g.obtainedfromasaliencypredictor)toanindividual,personalsaliencymapforanynewimages.
Reusableembeddingsofuserbehaviourhaveshownsignificantperformanceimprovementsforthepersonalisedsaliencyprediction
task.However,priorworksrequireexplicitusercharacteristicsandpreferencesasinput,whichareoftendifficulttoobtain.Wepresent
anovelmethodtoextractuserembeddingsfrompairsofnaturalimagesandcorrespondingsaliencymapsgeneratedfromasmall
amountofuser-specificeyetrackingdata.AtthecoreofourmethodisaSiameseconvolutionalneuralencoderthatlearnstheuser
embeddingsbycontrastingtheimageandpersonalsaliencymappairsofdifferentusers.Evaluationsontwopublicsaliencydatasets
showthatthegeneratedembeddingshavehighdiscriminativepower,areeffectiveatrefininguniversalsaliencymapstotheindividual
users,andgeneralisewellacrossusersandimages.Finally,basedonourmodelâ€™sabilitytoencodeindividualusercharacteristics,our
workpointstowardsotherapplicationsthatcanbenefitfromreusableembeddingsofgazebehaviour.
CCSConcepts:â€¢Human-centeredcomputingâ†’Usermodels;â€¢Computingmethodologiesâ†’Interestpointandsalientregion
detections.
AdditionalKeyWordsandPhrases:gaze,eye-tracking,saliency,personalsaliency,userembeddings,usermodel,deeplearning
1 INTRODUCTION
Saliencypredictionisthetaskofidentifyingsalientregionswithinanimagewhicharelikelytoattractgaze.Various
modelshavebeendevelopedwhichtakeintoaccountbothlow-levelfeatures[26,27,57]andhigh-levelimagecharac-
teristics[16,37,43],incorporatingbottom-upattentionmechanisms,aswellastaskdemands[11,41,61],whichinvolve
top-downattentionprocesses.Giventhepotentialforanticipatinguserattention,saliencypredictionmodelshavehad
1
4202
raM
02
]VC.sc[
1v35631.3042:viXraStrohmetal.
significantimpactincomputervisionandbeyond,andhaveprovenhighlybeneficialforawiderangeoftasks,from
servingasaninductivebiasforneuralattentionmechanisms[19,50,51]toestimatingusersâ€™cognitivestates[1,25,56],
orenablingpersonalisedpredictionsforvarioushuman-computer-interactiontasks[3,20,39,55,64].
Alargebodyofworkonsaliencymodellinghasfocusedonuniversalsaliency,i.e.thetaskofpredictingsaliencymaps
thataggregategazedatafrommultipleobserversand,assuch,disregardindividualdifferencesinviewingbehaviour.
Thereare,however,significantindividualvariationsinhowvisualattentionisdeployedonimagestimulithatare
duetoarangeoffactors,suchasscenecomplexityandsemantics[13,62],levelofexpertise[6,9,14,49],age[63],or
personalitytraits[4,45].
Despitethesedifferencesamongindividualsandthemanyapplications(e.g.assistivesystems[47,53,54])that
couldbenefitfromabetterunderstandingoftheindividual,onlyfewpreviousworkshaveproposedmethodsto
predictpersonalisedsaliency.Oneapproachinvolvedtraininganindividual(sub-)modelforeachuser,whichlacks
generalisability[42,59,60].Anotheroneleveragesperson-specificinformationsuchasage,gender,orpreference
towardsspecificobjectcategoriesorcolours[59].However,inadditiontoraisingprivacyconcerns,collectingthese
characteristicsistediousandrequiresexplicituserinput.
Incontrasttoexplicitlycollectinguserinformation,weintroduceanovelmethodtoextractembeddingsfromusersâ€™
gazebehaviourwhileviewingnaturalimages.OurmethodusesaSiameseconvolutionalneuralencoderthattakes
multipleimagesandtheircorrespondingsaliencymapsofaparticularuserasinputandproducesauserembeddingas
output.Theembeddingislearnedbycontrastinginputpairsfromoneusertootherusersexhibitingdifferentgaze
behavioursonthesameimagestimuli.Byintegratingtheuserembeddingintoasaliencypredictionnetwork(Figure1),
thisadditionalinputplaysacrucialroleinpredictingfiltersthatareconvolvedoverextractedimagefeatures,thus
integratinguser-specificinformationessentialforthepersonalisedsaliencypredictiontask.Ourfindingsdemonstrate
thehighlydiscriminativenatureoftheseembeddings,enablingustoeffectivelycompareindividualsbasedontheir
distinctivegazebehaviour(Figure6).Resultsonthedownstreamtaskofpersonalsaliencypredictiontaskshowhow
thegeneratedembeddingscanbeusedtoeffectivelyrefinetheuniversalsaliencymappredictionsandtailorthemto
individualusers.Moreover,weobservethattheseembeddingsexhibitgoodgeneralisationcapabilitieswhenappliedto
bothunseenusersandimages.1
2 RELATEDWORK
2.1 IndividualDifferencesinVisualSaliency
Traditionalsaliencypredictionmethodsignoreindividualdifferencesinvisualsaliencebetweenhumansandinstead
predictanaverage,universalsaliencymap[5].However,therearemultiplepriorworksthatshowthathumanshave
differentvisualpreferenceswhichdrawstheirattention,whicharestableandpredictable.DeHaasandLinkaet
al.[13,38]haveidentifiedmultiplesemanticdimensionsalongwhichhumansaliencesignificantlydiffers,likefaces.
Priorworkshaveincorporatedfacedetectorsinsaliencypredictionpipelines,astheygenerallytendtoattractsignificant
attention[5,10].However,theresultsfromDeHaasetal.showthatforspecifichumans,suchpredictionsareimprecise
astheirattentionisnotattractedbyfaces.LaterBrodaetal.[7,8]identifiedthathumanscanberoughlyclusteredin
twocategorieswhenobservingpersons.Eithertheytendtofocustheheadandinnerfacialfeaturesortheyfixateon
bodypartslikearmsandlegs.Priorworkhasalsostudiedwhichhumantraitsinfluencetheindividualattentionand
foundthatforexampleage[33]isanimportantfactoraswellaspersonality[23]andgender[46].Xuetal.[59]were
1Projectcodewillbemadepubliclyavailableuponacceptance.
2LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
thefirstthatproposedamethodtopredictpersonalisedinsteadofuniversalsaliencybyutilisingsuchusertraitsasan
additionalinputtotheirnetwork.However,itisstillunclearwhichusertraitsareusefulforsaliencyprediction,and
explicitlycollectingpersonalinformationmightnotbeappropriate.Morotoetal.[42]laterproposedamethodinvolving
amulti-taskCNNtopredictpersonalisedsaliencyforeachuserinthetrainingdataset.Duringinference,unseenusers
werematchedtotheseenusersbasedontheirsimilarityinattentionallocation.However,thisrequiresmanydifferent
usersinthetrainingdatatogeneralisetounseenusers.Moreover,findinganappropriatesimilarityfunctiontomatch
unseentoseenusersbasedontheirgazebehaviourischallenging.Incontrast,weproposetotrainasingle-taskCNN
thatincorporatesanadditionaluserembeddingasinput,enablingthenetworktoleverageuser-specificinformation.
2.2 UserEmbeddings
Userembeddingshavefoundapplicationsacrossdiversedomains,servingeitherasameanstoinferuser-related
informationortopersonalisetheuserexperience.InaseminalworkbyPazzanietal.[44],anapproachwasintroduced
whereanagentlearnsauserembeddingbyleveragingexplicitfeedbackprovidedbyusersintheformofpageratings.
Thisuserembeddingwasthenutilisedtoprovidepersonalisedwebsiterecommendationstailoredtotheuserâ€™sinterests
andpreferences.Later,variousmethodshavebeenproposedtoconstructuserembeddingsbyincorporatingdiverse
formsofapplication-specificexplicitfeedback[34,48].Methodsthatcollectuserinformationexplicitlyrequiretheuser
tobeactiveandmightbecomeinaccurateovertimeastheuserâ€™sinterestchanges.
Toaddressthislimitation,implicitmethodstocreateuserembeddingshavegainedpopularity.Thesemethods
allowuserstosimplyinteractwithasystemwhilethesystemcreatesauserembedding,withoutrelyingonexplicit
feedback[15,17].InarecentworkbyWuetal.[58],anovelapproachcalledAuthor2Vecwasintroducedtoderiveuser
embeddingsbyanalysingthetextualcontentauthoredbyusersonsocialmediaplatforms.Theauthorsdemonstrated
thesuperiorperformanceoftheseembeddingsinpredictinguserpersonalitytraitsormentalstatescomparedto
alternativemethods.Similarly,Anetal.[2]usedwebbrowsingeventstoextractrichuserembeddingsfordifferent
downstreamtasks.Inotherrelatedworks,researchersinvestigatedtheincorporationofuserembeddingsasunique
wordtokensalongsidetheuserâ€™stext.Thisapproachenablesthemodeltocomprehendthesentenceinthecontextof
theuserembedding[40,65].Intheeye-gazedomainHeetal.[18]haveutilisedappearancebaseduserembeddings
extractedfromfacestopersonaliseanappearance-basedgazeestimator.Theseworksshowthepotentialtoleverage
implicituserembeddingsonarangeofapplications.Inourwork,tothebestofourknowledge,wearethefirstto
learnuserembeddingsfromvisualattentivebehaviourandleveragethemtoenhanceperformanceonthepersonalised
saliencypredictiontask.
3 METHODOLOGY
Traditionalsaliencypredictionmodelslearnafunctionğ‘“(ğ¼)=USMğ¼,whereğ¼isanimageandUSMğ¼ isthecorresponding
UniversalSaliencyMap(USM).ThegroundtruthUSMiscalculatedbyaveragingğ‘›PersonalSaliencyMaps(PSM)obtained
fromdifferenthumansobservingthesameimage:USMğ¼ = ğ‘›1 (cid:205)ğ‘› ğ‘˜=1PSMğ¼,ğ‘˜.InsteadofpredictingUSMğ¼,ourgoalis
topredictPSMğ¼,ğ‘ˆ foragivenimageğ¼ anduserğ‘ˆ.TopredictPSMğ¼,ğ‘ˆ itisessentialtoincorporateadditionalinput
informationthatisspecifictotheuserğ‘ˆ forwhomthepredictionsareintended.
3.1 LearningUserEmbeddings
Weproposeanovelmethodforextractinguserembeddingsğ‘’
ğ‘ˆ
foreachindividualuserğ‘ˆ.Theseembeddingsare
derivedfromthedistinctvariationsobservedinthevisualattentionpatternsofindividualusers.Ourhypothesisisthat
3Strohmetal.
Fig.2. Thearchitectureofourproposeduserembeddingextractorinvolvesprocessingmultipleimagesalongsideanadditional
channelthatincludesthesaliencyinformationofaspecificuser,fromwhichweaimtoextractanembedding.Toaccomplishthis,we
employaSiameseconvolutionalneuralnetwork,whichisresponsibleforextractingfeaturesfromeachpairofimage-saliencymaps.
Subsequently,theextractedfeaturesareaveragedandnormalised,resultingintheuserembedding.
theseembeddingscanbeeffectivelyusedbyapersonalisedsaliencypredictionmodeltogenerateuser-specificsaliency
outputs.
ThearchitectureofourproposedembeddingneuralnetworkisshowninFigure2.ASiameseuserembedding
extractorğ¸ takesğ‘šdifferentimages{ğ¼ 1,...,ğ¼ ğ‘š}alongwiththecorrespondingPSMs{PSMğ¼ 1,ğ‘ˆ,...,PSMğ¼ğ‘š,ğ‘ˆ}forthe
sameuserğ‘ˆ asinput.Thegoalistoextractjointimage-saliencyfeatureswhichallowthenetworktounderstand
thevisualpreferencesoftheuserandextractameaningfuluserembedding.ThePSMforeachimageistreatedas
anadditionalimagechannelbesidestheexistingthreeRGBchannelsandresizedtoaresolutionof160Ã—120.Each
image-PSMtensorispassedthroughfourconvolutionblocksconsistingofa2Dconvolutionlayerwithstridetwo,a
4LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
batchnormalisationlayer[24]andaRectifiedLinearUnit(ReLU)activationfunction.Subsequently,aglobalaverage
poolinglayer[36]isemployedtoreducetheoutputtoaone-dimensionalvector,whichhelpspreventoverfittingin
conjunctionwithadropoutlayer[52].Finally,alinearlayerpredictstheoutputfeaturesforeachimage-PSMinput.
ThisSiamesenetworkisusedtoextractjointimage-saliencyfeaturesforeachoftheğ‘šimage-PSMpairs,which
aresubsequentlyaveragedandnormalisedtounitlengthresultingintheextracteduserembedding.Preliminary
experimentsrevealedthatcombiningtheextractedfeaturesfromeachimage-PSMpairwitharecurrentortransformer
networkresultsinsevereoverfitting.Thus,averagingthefeatureshelpspreventoverfittingandyieldsbetterresults
overall.
Thenetworkisoptimisedtominimisethetripletmarginlosswithonline(semi-)hardtripletmining[21]definedas:
ğ‘’
ğ‘
=(cid:13) (cid:13)ğ¸({ğ¼ 1,...,ğ¼ ğ‘š},{PSMğ¼
1,ğ‘ˆ
1,...,PSMğ¼ğ‘š,ğ‘ˆ 1})(cid:13)
(cid:13)
ğ‘’
ğ‘
=(cid:13) (cid:13)ğ¸({ğ¼ ğ‘š+1,...,ğ¼ 2ğ‘š},{PSMğ¼ğ‘š+1,ğ‘ˆ 1,...,PSMğ¼
2ğ‘š,ğ‘ˆ
1})(cid:13)
(cid:13)
(1)
ğ‘’
ğ‘›
=(cid:13) (cid:13)ğ¸({ğ¼ 2ğ‘š+1,...,ğ¼ 3ğ‘š},{PSMğ¼
2ğ‘š+1,ğ‘ˆ
2,...,PSMğ¼
3ğ‘š,ğ‘ˆ
2})(cid:13)
(cid:13)
Lğ¸ =max(ğ‘’ ğ‘Â·ğ‘’ ğ‘ âˆ’ğ‘’ ğ‘Â·ğ‘’ ğ‘›+ğ‘š,0).
Tocalculatetheanchoruserembeddingğ‘’ ğ‘inEquation(1),arandomuserğ‘ˆ 1andğ‘šrandomimages{ğ¼ 1,...,ğ¼ ğ‘š}with
correspondingPSMs{PSMğ¼ 1,ğ‘ˆ 1,...,PSMğ¼ğ‘š,ğ‘ˆ 1}areselectedandpassedthroughourembeddingnetwork.Similarly,the
positiveembeddingexampleğ‘’ ğ‘ iscalculatedusingthesameuserğ‘ˆ 1butwithdifferentimages{ğ¼ ğ‘š+1,...,ğ¼ 2ğ‘š}andPSMs
{PSMğ¼ğ‘š+1,ğ‘ˆ 1,...,PSMğ¼ 2ğ‘š,ğ‘ˆ 1},whilethenegativeembeddingexampleğ‘’ ğ‘› isobtainedbyselectingadifferentrandom
userğ‘ˆ 2,alsowithdifferentimages{ğ¼ 2ğ‘š+1,...,ğ¼ 3ğ‘š}andcorrespondingPSMs{PSMğ¼ 2ğ‘š+1,ğ‘ˆ 2,...,PSMğ¼ 3ğ‘š,ğ‘ˆ 2}.Basedonthe
threeembeddingsğ‘’ ğ‘,ğ‘’ ğ‘ andğ‘’ ğ‘›thestandardtripletlosscanbecalculatedasdefinedEquation(1).Itiscrucialtonote
thateachimageğ¼ inthetrainingdatasethasacorrespondingpersonalisedsaliencymapPSMğ¼,ğ‘ˆ foreachuserğ‘ˆ.This
ensuresthattheuserembeddingsaresolelyderivedfromtheindividualdifferencesinusersâ€™visualattentionbehaviour,
ratherthanbeinginfluencedbytheimagesthemselves.Byminimisingthedistancebetweentheanchorandthepositive
embedding,thenetworklearnstorecognisesimilarhigh-levelgazebehaviourbetweendifferentinputsforthesame
user.Similarly,maximisingthedistancebetweentheanchorandthenegativeembeddingencouragesthenetworkto
distinguishthedifferentattentivebehaviourbetweentwousers.
Whilethetrainingobjectiveistodifferentiatebetweendifferentusers,priorresearchhasshownthatembeddings
learnedthroughoptimisingthetripletlosshavethecapabilitytoencodesubstantialclass-specificinformation[21].
Additionally,similarclassestendtobeclosetoeachotherintheembeddingspace,whiledissimilarclassestendto
havegreaterseparation.Thisindicatesthattheembeddingspaceeffectivelycapturestherelevantcharacteristicsof
theclassesandpresentsastructuredrepresentationthatreflectstheunderlyingrelationshipsbetweenthem.Byusing
theuserembedding,adownstreamtaskmodelcanleveragethecapturedinformationandtailoritspredictionstothe
specificcharacteristicsandvisualattentionbehaviourofeachuser.
3.2 PersonalisedSaliencyPrediction
Todemonstratetheeffectivenessoftheuserembeddings,weadaptedanexistingmethodforpersonalisedsaliency
prediction[59]andreplacethemanually,explicitlydefinedusercharacteristicswithourlearnedembeddingsfrom
implicitgazebehaviour.
SimilartoXuetal[59]wedefinethePSMpredictiontaskasarefinementoftheUSM:
PSM(ğ¼,ğ‘ˆ)=USM(ğ¼)+Î”(ğ¼,ğ‘ˆ), (2)
5Strohmetal.
Fig.3. Thepersonalisedsaliencymap(PSM)networkoperatesbytakinganimagestimulusanditscorrespondinguniversalsaliency
map(USM)asinput.Inaddition,itincorporatesuserembedding,whichisutilisedtopredictkernelweights,whicharethenconvolved
overtheimage-USMfeatures.ThenetworkoutputsadiscrepancymapwhichcanbeaddedtotheUSMinordertogeneratethePSM.
wherePSM(ğ¼,ğ‘ˆ) isthePSMofauserğ‘ˆ foragivenimageğ¼,USM(ğ¼) istheUSMforthatimageandÎ”(ğ¼,ğ‘ˆ) isthe
discrepancymapofthatuserandthatimage.Thediscrepancymapessentiallydefineswhichpartsoftheimageattract
thespecificusersattentionmoreorlesscomparedtotheaverageuser.Usingthisdefinitionallowsustodisentanglethe
predictionofthePSM,focusingonpredictingthediscrepancymapÎ”(ğ¼,ğ‘ˆ)whileusingexistingstate-of-the-artmodels
topredicttheUSM.Figure3visualisesourneuralnetworkarchitectureforpersonalisedsaliencyprediction.Inputtothe
networkğ‘ aretheimage,theUSMasaforthimagechannelandauserembedding.First,aseriesofconvolutionlayers
extractimage-saliencyfeatures.Inparallel,aseriesoflinearlayerspredictfiltersbasedontheuserembedding,which
aresubsequentlyconvolvedovertheextractedimage-saliencyfeatures.Thiswaythenetworkcanlearnfeaturesthat
arespecificallyimportanttotheuserbasedonthatusersembedding[59].Afterafinalsequenceofconvolutionlayers
themodeloutputsadiscrepancymapofsize15Ã—20withavaluerangebetween-1and1.Weoptimisethenetwork
withthemeansquarederror(MSE)lossbetweenthepredictedandthetargetdiscrepancymap.Previousworkhas
shownthatbyprojectingextractedfeaturesfromintermediatelayerstotheoutputspaceforadditionalsupervisioncan
improveperformance[12,35].Thereforeweaddanadditionalprojectionlayerbeforethelasttwoconvolutionlayersto
predictintermediatediscrepancymaps.OuroveralllossobjectiveLğ‘ƒğ‘†ğ‘€ isthengivenas:
(cid:13) (cid:13)âˆ‘ï¸3 (cid:13) (cid:13)
Lğ‘ƒğ‘†ğ‘€ =(cid:13)
(cid:13)
ğ‘(USM(ğ¼),ğ¼,ğ¸ ğ‘ˆ)ğ‘˜ âˆ’Î”(ğ¼,ğ‘ˆ)(cid:13) (cid:13), (3)
(cid:13)ğ‘˜=1 (cid:13)
whereğ‘(...)ğ‘˜ isoneofthethreepredicteddiscrepancymapsofthepersonalisedsaliencynetwork.Tocalculatethe
groundtruthdiscrepancymapsÎ”(ğ¼,ğ‘ˆ)weuseEquation(2)andsubtracttheUSMğ¼ fromthePSMğ¼,ğ‘ˆ.
6LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.622 0.556 0.904 2.121 1.123
Fine-tunedDG 0.715 0.619 0.905 2.140 0.526
MultiCNNw/DG 0.735 0.643 0.897 2.142 0.708
Oursw/DG 0.736 0.651âˆ— 0.907 2.170âˆ— 0.509âˆ—
GroundTruthUSM 0.801 0.685 0.921 2.373 0.372
MultiCNNw/GT 0.804 0.683 0.919 2.378 0.511
Oursw/GT 0.813âˆ— 0.706âˆ— 0.922 2.405âˆ— 0.357âˆ—
Table1. Closed-setresultsfortheIDdataset[13].*indicatessignificantimprovementoverthestrongestbaseline.
4 EXPERIMENTS
4.1 ImplementationDetails
UserEmbeddingNetwork. ThearchitectureoftheuserembeddingnetworkisshowninFigure2.Thenumberofinput
image-PSMsamplesğ‘štoourproposedSiameseCNNdependsontheexperimentandvariesbetween4and32.Each
modelwasoptimisedwiththetripletmarginlossasdefinedinEquation(1)withamarginof0.05andonlinesemi-hard
andhardtripletminingwithinabatchof256samples.TheweightswereupdatedusingtheAdamoptimiser[32]witha
learningrateof0.001anddefaultparametersotherwise.TheDropoutlayer[52]maskedneuronactivations50%ofthe
timeduringtraining.
PersonalisedSaliencyNetwork. Figure3illustratesthearchitectureofourpersonalisedsaliencynetwork.Toeffectively
trainthisnetwork,wefirsttrainedtheembeddingnetworktoextractuserembeddings,whichserveasinput.Sincethe
embeddingsrelyonğ‘šimage-PSMpairsfortheircalculation,theycanexhibitvariability,particularlywhenğ‘šissmall.
Hence,aftertrainingtheembeddingnetworks,wegenerated100embeddingsforeachparticipantwithineachdataset
byrandomlysamplingdatafromthecorrespondingparticipant.Duringthetrainingprocessofthepersonalisedsaliency
network,werandomlyselectedembeddingsfromthispooltoaccountforthesevariationsandimprovegeneralisation.
ThemodelsweretrainedwithanSGDoptimiserwithinitiallearningrateof0.02,momentumof0.9,weightdecayof
0.0005andbatchsize32.Thelearningratewasreducedbyafactorof2every25epochs.Thetrainedmodelrunsat27
framespersecondonanRTX4070GPU,achievingreal-timepersonalisedsaliencypredictions.
EvaluationMetrics. Wereportmultiplemetricscommonlyusedtoevaluatethesimilaritybetweensaliencymaps[30,
43,59].ThesemetricsarethePearsonâ€™scorrelationcoefficient(CC),similarity/histogramintersection(SIM),Area
underROCCurve(AUC-Judd)[31],normalisedscanpathsaliency(NSS)andKullback-Leiblerdivergence(KLD).In
addition,toassesstheaccuracyoftheuserembeddings,weemployalabellingapproachwhereanextractedembedding
isconsideredcorrectifitsnearestneighbourintheembeddingspacebelongstothesameuser.Thisevaluationmetricis
commonlyreferredtoasprecisionatone[28].
4.2 Datasets
Saliencypredictionmodelsaretypicallypre-trainedusingthelarge-scaleSALICONdataset[29].However,wecannot
utilisetheSALICONdatasetforlearningtheembeddingsaseachsubjectfromthedatasetobservedadifferentsubsetof
imagestimuli.AsdiscussedinSubsection3.1,itiscriticalthateachparticipantlooksatallimages,oratleastthatthere
7Strohmetal.
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.584 0.530 0.881 2.288 1.294
Fine-tunedDG 0.734 0.622 0.887 2.293 0.562
MultiCNNw/DG 0.746 0.637 0.892 2.299 0.604
Oursw/DG 0.760âˆ— 0.649âˆ— 0.896 2.308 0.481âˆ—
GroundTruthUSM 0.821 0.698 0.912 2.500 0.366
MultiCNNw/GT 0.845 0.711 0.914 2.609 0.415
Oursw/GT 0.846 0.725âˆ— 0.915 2.595 0.336âˆ—
Table2. Closed-setresultsforthePSdataset[59].*indicatessignificantimprovementoverstrongestbaseline.
isasignificantoverlapbetweenparticipants,asotherwisethemodelcansimplyidentifytheuserbasedontheimages
theyobservedignoringtheirspecificvisualattentionbehaviour.
Basedontheaboverequirement,weselectedtwopubliclyavailabledatasetswhereeachparticipantobservedeach
imagewhiletheirgazewasrecordedwithaneye-tracker.ThefirstdatasetwascollectedbyXuetal.[59],which
wecallthePersonalisedSaliency(PS)dataset.ThePSdatasetcontains1,600images,whichtheyselectedtocontain
manydifferentsemanticcategoriesineachimage,astheyarguethatthismaximisesthevariationofvisualattention
betweenparticipants.Eachstimuliinthedatasetwasobservedby30participantsforthreesecondsatotaloffourtimes,
allowingthemtoaverageoutstochasticvariationsineachparticipantsattentivebehaviour.Toevaluatehowwellour
systemgeneralisestounseenimageswesplittheimagesinto80%fortraining,10%forvalidationand10%fortesting.
Furthermore,toevaluatehowwelloursystemgeneralisestounseenparticipants,wesplitthe30participantsinto20
fortrainingand5eachforvalidationandtesting.
TheseconddatasetwascollectedbyHaasetal.[13]whichwecalltheIndividualDifferences(ID)dataset.Thestimuli
wereselectedtobecomprisedofcomplexscenescontainingmultipledifferentsemanticcategoriesandobjects.They
recordedgazedatafromatotalof102differentparticipantseachlookingat700differentimagestimuli.Similartothe
PSdatasetwesplittheimagesinto80%fortraining,10%forvalidationand10%fortesting.Furthermore,wesplitthe
102participantsinto80fortraining,10forvalidationand12testing.
FollowingXuetal.[59]weconductbothclosed-setandopen-setexperimentsforeachdataset.Intheclosed-set
experiments,thesameparticipantswereusedinboththetraining,validation,andtestsetswiththevalidationandtest
setscontainingunseenimages.Thisenablesustoassessthecapabilitytopredictpersonalisedsaliencyforfamiliarusers.
Asfortheopen-setexperiments,weassessthepersonalisedsaliencypredictionperformanceonunseenparticipants
fromthetestset.Thisanalysishelpsusunderstandhoweffectivelymodelscangeneralisetonewusers.
4.3 Baselines
Weevaluateourmethodagainstthreebaselines.Firstly,weutiliseDeepGazeIIE[37],astate-of-the-artuniversalsaliency
predictionmodel.WecomparethepredictedUSMsbyDeepGazeIIEwithourrefinedPSMsbasedontheDeepGazeIIE
prediction.SinceDeepGazeIIEwasnotoriginallytrainedonourdataset,wefine-tunethemodelâ€™spredictiononeach
datasetandpresentresultsbothwithandwithoutfine-tuning.Secondly,wecomparethegroundtruthUSMswithour
predictedPSMsgeneratedthroughrefiningtheUSMs.
Intheliterature,therearetworelevantworksbyXuetal.[59]andMorotoetal.[42]thatproposemethodsforPSM
prediction,asdiscussedinSection2.Unfortunately,wewereunabletodirectlycompareourresultswithXuetal.due
8LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.615 0.553 0.916 2.239 1.083
Fine-tunedDG 0.721 0.624 0.914 2.244 0.542
MultiCNNw/DG 0.723 0.627 0.910 2.244 0.620
Oursw/DG 0.726 0.638âˆ— 0.916 2.253 0.527âˆ—
GroundTruthUSM 0.804 0.682 0.922 2.506 0.387
MultiCNNw/GT 0.804 0.690 0.929 2.550 0.440
Oursw/GT 0.813âˆ— 0.702âˆ— 0.933 2.552 0.366âˆ—
Table3. Open-setresultsfortheIDdataset[13].*indicatessignificantimprovementoverstrongestbaseline.
Model CC SIM AUC NSS KLD
DeepGazeIIE(DG) 0.564 0.522 0.860 1.944 1.431
Fine-tunedDG 0.692 0.618 0.862 2.032 0.685
MultiCNNw/DG 0.693 0.615 0.862 2.006 0.682
Oursw/DG 0.712 0.634 0.870 2.033 0.539
OursCDw/DG 0.713âˆ— 0.634âˆ— 0.871âˆ— 2.033 0.533âˆ—
GroundTruthUSM 0.788 0.691 0.892 2.218 0.399
MultiCNNw/GT 0.800 0.694 0.892 2.290 0.516
Oursw/GT 0.804 0.701 0.893 2.273 0.371
OursCDw/GT 0.807 0.704âˆ— 0.894 2.288 0.361âˆ—
Table4. Open-setresultsforthePSdataset[59].*indicatessignificantimprovementoverstrongestbaseline.
totheunavailabilityoftheuser-specificinformationrequiredfortheirmethod.However,fortheclosedsetexperiments
theyalsoproposedMultiCNN,wheretheytrainaseparateclassifierforeachparticipant,whichwewilluseasourthird
baseline.TheMultiCNNarchitectureisidenticaltoournetworkshowninFigure3withouttheembeddingpathway.
Morotoetal.[42]proposeamethodtomapunseenuserstothetraininguserwiththemostsimilarvisualattention
behaviour,whichallowsthemtomakepersonalisedsaliencypredictionswiththecorrespondingtrainedclassifier.
Inspiredbythis,weproposeanoraclemappingbyevaluatingunseenparticipantsusingeverytrainedMultiCNNmodel
andthenchoosethemodelachievingthelowestloss.Thisallowsustoprovidetheupper-boundMultiCNNperformance
fortheopensetexperiments.
4.4 PersonalisedSaliencyPrediction
Closed-SetResults. Table1showstheclosed-setresultsfortheIDdatasetandTable2showstheresultsforthePS
dataset.Theresultswithourmethodwereobtainedusingembeddingsextractedfromğ‘š =32image-PSMpairsfor
eachuser.Thebestperformingmodelsarehighlightedinbold,whiletheperformanceofthesecondbestmodelis
underlined.SignificancetestswereconductedusingtheMann-Whitney-Utestforeachmetricwithap-valuethreshold
of<0.05.Anasterisknexttometricsforourmethodindicatesthattheimprovementcomparedtothestrongestbaseline
wasstatisticallysignificant.ThegroundtruthUSMistheupperboundtraditionalsaliencypredictionmethodscould
potentiallyachieve.TheresultsforMultiCNNandOursshowthatitispossibletofurtherrefinetheseUSMsasoverall
bothmethodsoutperformthegroundtruthUSMbaseline.Furthermore,weobservethatforbothdatasetsourmethod
9Strohmetal.
Fig.4. ExamplePSMpredictionsfortwousersfromthePS[59]testsetwithourproposedmethodcomparedtothegroundtruths.
outperformsallbaselinesincludingMultiCNNsinallmetricsexceptforNSSonthePSdataset.However,forareal-world
scenariothegroundtruthUSMmightnotbeavailableandhastobepredictedfirst.TheresultsshowthatMultiCNNs
andOursusingthenonfine-tunedUSMpredictionsfromDeepGazeIIEstilloutperformthefine-tunedDeepGazeIIE
baseline,withourmethodachievingthebestperformance.Thisindicatesthatourmethodcanbeappliedtodifferent
potentialsuboptimalUSMpredictionsandstillproduceamorerefinedPSMprediction.
Open-SetResults. Table3showstheopen-setresultsfortheIDdatasetandTable4showstheresultsforthePSdataset.
Wecanobserveaverysimilartrendaswiththeclosed-setexperimentswithourmethodoveralloutperformingall
baselines,indicatingthatourembeddingshelpthepersonalsaliencynetworktogeneralisewelltounseenparticipants.
SincethePSdatasetconsistsonlyof30differentparticipantsofwhich20areusedfortraining,learninggeneralisable
userembeddingsismorechallenging.WethereforeexperimentwithcombiningthetrainingsplitsofthePSandID
datasetswhentrainingtheuserembeddingnetwork,allowingthenetworktoobservetheattentivebehaviourofatotal
of100differentparticipantsduringtraining.Notethatwestilltrainedandevaluatedourpersonalisedsaliencynetwork
usingonlythePSdataset.Wereporttheresultingperformanceonthecombineddataset(CD)asOursCDinTable4.We
canobservethatusingthesenewembeddingstheperformanceofthepersonalisedsaliencymodelfurtherimprovesfor
allmetrics.
Inadditiontothequantitativeresults,Figure4showsexampleopen-setPSMpredictionsformultipleimagestimuli
onthetestsplitofthePSdataset.Intheprovidedexamples,itisevidentthattheuserrepresentedinthelasttwocolumns
exhibitsamorefocusedattentionbehaviourcomparedtothefirstuser.Ourembeddingssuccessfullycapturethis
distinction,asindicatedbythecorrespondingsaliencypredictions.Notably,intheexampledepictedinthebottomrow,
theseconduserappearstoallocatelessattentiontofacesandinsteadalsofocusesonotherbodyparts.Ourembeddings
seemtocapturethisbehaviour,asevidencedbythesaliencyallocationbelowthefaceregioninthecorrespondingPSM
10LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
1.0
0.9
0.8
0.7
0.6
0.5
ID
0.4 PS
CD (ID + PS train split)
0.3
4 8 16 32 48 64
Number of examples
Fig.5. Performancecomparisonofdifferentuserembeddingextractionmodels.They-axisindicatesthemodelâ€™saccuracyandthe
x-axishowmanyimage-PSMexamplesğ‘šwereusedasinputtoextracttheembedding(4,8,16,32,48or64).Wereporttheaccuracy
forunseenparticipantsontheIndividualDifferences(ID)dataset,thePersonalSaliency(PS)datasetandforthecombinedCD
dataset.
predictions.Togetherwithourquantitativeresultsthisfurtherdemonstratestheeffectivenessofusinguserembeddings
learnedfromvisualattentionformorepersonalisedsaliencypredictions.
4.5 UserEmbeddingsAnalysis
Togainabetterunderstandingofourextracteduserembeddings,wefurtheranalysetheperformanceoftheuser
embeddingnetwork.
NumberofExamplesforEmbeddingExtraction. Figure5showsthemodelâ€™stestsetaccuracyonthey-axisfordifferent
datasetsandthenumberofexamplesğ‘šusedforembeddingextractiononthex-axis.Wereportthetestsetaccuracy
forbothdatasetsIDandPS,aswellasthePStestsetaccuracywhentrainingonthecombinedtrainingsetCD.The
userembeddingnetworkachievesanaccuracyof98.1%ontheIDtestsplitwhenusingğ‘š =64examples,showing
thatitisabletodifferentiateverywellbetweenparticipantsthatthemodelneversawduringtraining.Similarlythe
modeltrainedonthePSdatasetachievesanaccuracyof79.1%whenonlyusingthePStrainingsplitandanaccuracyof
92.7%whencombiningthetrainingsplitsfromPSandID.Asalreadyindicatedbytheimprovedpersonalisedsaliency
predictionresultsreportedinTable4,trainingonbothdatasetsincreasestheperformanceonthePSdataset,which
onlycontainsasmallnumberofparticipants.NotethattherandombaselineaccuracyfortheIDdatasetis8.3%(12
participantsinthetestset),whileitis20%forthePSdataset(5participantsinthetestset).Furthermore,wecanobserve
thatthemodelâ€™saccuracyincreaseswiththenumberofexamplesğ‘šprovidedasinput.Combiningthetrainingdatasets
especiallyimprovestheaccuracyforlargerğ‘š.Thisislikelyduetotheincreaseduserinformationthemodelcanextract
withlargerğ‘š,resultinginbettergeneralisationifprovidedwithmorediverseusersduringtraining.
11
ycaruccAStrohmetal.
(a) (b)
Fig.6. Weuset-SNEtoreducethe32-dimensionalembeddingsintotwodimensions.Figure(a)showstheembeddingspacefor
embeddingsextractedusingğ‘š=8image-PSMpairswhilein(b)weusedğ‘š=32pairs.Eachdotrepresentsoneembeddingextracted
usingğ‘šrandomlysampledimage-PSMpairs.EachcolourcorrespondstoauniqueuserfromtheIDtestset.
VisualisationoftheEmbeddingSpace. Toanalysetheembeddingspacewereducedthe32dimensionaluserembeddings
to2dimensionsusingt-SNE.Figure6illustratesthe2-dimensionalembeddingsforthe12participantsfromtheIDtest
split.Inthisvisualisation,eachdotrepresentsanembedding,andthecolourofeachdotcorrespondstoaspecificuser.
Wecalculated20embeddingsforeachparticipantbyrandomlysamplingğ‘šinputimage-PSMpairs.In(a)wevisualise
theembeddingsforğ‘š=8while(b)showstheembeddingsforğ‘š=32.Weobservethattheembeddingsforğ‘š=8have
amuchlargervariancewithinaparticipantcomparedtotheembeddingsextractedwithğ‘š=32,aslessinformation
abouttheusercanbeextracted.Foralowğ‘štheboundariesbetweenparticipantsbecomesfuzzywhichisalsoreflected
bytheloweraccuracyreportedinFigure5.Wecanobservethatforğ‘š=32theembeddingsarehighlydiscriminative
asclearuserclustersareformed.
5 BROADERIMPACT
Ourmethoddemonstratestheabilitytoextractuserembeddingsfromasmallamountofgazedata,showcasingthat
theseembeddingseffectivelycapturerelevantinformationformodellingpersonalsaliency.Predictingsaliencyisa
crucialtaskwithimplicationsforvariousdownstreamapplicationsincomputervisionandhuman-computerinteraction.
Forinstance,attentiveuserinterfacesaimtomanagetheuserâ€™sattentioneffectively,relyingonknowledgeabouttheir
visualattention[55].Recommendersystemscanbenefitfromunderstandingusersâ€™visualattentiontoenhancethe
visibilityoftop-rankedentities[64].Otherdownstreamtasksthatbenefitfromaccuratepersonalisedsaliencyinclude
videosummarization[39],automatedimagecropping[3],andimagecaptioning[20].Whilethedownstreamtask
ofthisworkispersonalisedsaliencyprediction,ourproposedmethodforextractinguserembeddingscouldprove
advantageousforotherdownstreamtaskswherepersonalisationiscrucial.
12LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
Althoughpersonalisedsaliencyishelpfulformanytasks,theunderlyingcomputationalusermodelcouldalsobe
misusedtosynthesisedataforagivenuserbyimpersonatingtheuserâ€™sownsaliency.Furthermore,insteadofusingthe
embeddingstopredictpersonalsaliency,theymightbemisusedtodirectlyextractusersensitiveinformation.These
embeddingsmightencodeusercharacteristicsorprivateinformationabouttheuserthatcorrelateswiththeirgaze
behaviour.Forexample,priorworkhasshownthatgenderisastrongmodulatorofsaliencypreference[22],andas
such,thisprivateinformationmightbeextractablefromourembeddings.Thisencodedinformationcouldbeusedto
identifyusersbasedontheirpersonalsaliency,especiallyasattentiontrackingbecomesmorepervasiveandcheap
throughappearancebasedgazeestimationormousetracking.Anotherfactortoconsideristhatourembeddingsmay
bebiasedastheyareextractedfromasmallnumberofstimuliwithcorrespondinggazedata.AswecanseeinFigure6,
theembeddingsvaryforthesamepersonandwecannoticemultipleoutlierembeddingsthatarefarapartformtheir
cluster,evenwhenusingmoredata.Thus,whenusedfortaskslikeuserprofilingthismightleadtoincorrectuser
profilesandsubsequentlyincorrectpredictionsindownstreamtasks.
Continuingthislineofresearch,itisconceivablethatfutureworkwillnotonlysynthesisepersonalisedsaliencybut
evenrawgazeofspecificuserswithsuchembeddings.Accuratecomputationalmodelsofuser-specificgazebehaviour
wouldbeofgreatsignificanceforevenmoredownstreamtasks.However,thisadvancementmightraisepotential
securityrisksforapplicationsthatfundamentallyrelyongazebehaviouranalysis,suchasgaze-basedauthentication.
6 SUMMARY
Inthiswork,weproposedanovelmethodthatextractsuserembeddingsfrompairsofnaturalimagesandcorresponding
user-specificsaliencymaps.Thelearnedembeddingscapturetheusersâ€™uniquecharacteristicsandcanbeusedto
addressthepersonalisedsaliencypredictiontask.Incontrasttopriorworkforthistaskthatrequiredexplicituserinput,
ourmethodonlyrequiresimplicitinputfromgazebehaviourcollectedusinganeyetracker.Ourproposedmethoduses
aSiameseconvolutionalneuralencodertolearntheembeddingmodel,trainedbycontrastingauserâ€™sgazebehaviour
withthatofdifferentusers.Resultsontwosaliencydatasetsdemonstratedtheembeddingsâ€™discriminativepower,our
methodâ€™sgeneralisabilitytounseenusersandimages,andimprovedperformanceoveruniversalsaliencyprediction
models.Assuch,ourworkpresentsapromisingapproachtolearningandleveraginguserembeddingsfromimplicit
behaviouralsoforothertasksorapplicationsthatrequireindividualusercharacteristics.
REFERENCES
[1] AhmedAbdou,EktaSood,PhilippMÃ¼ller,andAndreasBulling.2022.Gaze-enhancedCrossmodalEmbeddingsforEmotionRecognition.Proceedings
oftheACMonHuman-ComputerInteraction6,ETRA(2022),1â€“18.
[2] MingxiaoAnandSundongKim.2021.Neuraluserembeddingfrombrowsingevents.InMachineLearningandKnowledgeDiscoveryinDatabases:
AppliedDataScienceTrack:EuropeanConference,ECMLPKDD2020,Ghent,Belgium,September14â€“18,2020,Proceedings,PartIV.Springer,175â€“191.
[3] EdoardoArdizzone,AlessandroBruno,andGiuseppeMazzola.2013.Saliencybasedimagecropping.InImageAnalysisandProcessingâ€“ICIAP2013:
17thInternationalConference,Naples,Italy,September9-13,2013.Proceedings,PartI17.Springer,773â€“782.
[4] AdrienBaranes,Pierre-YvesOudeyer,andJacquelineGottlieb.2015.Eyemovementsrevealepistemiccuriosityinhumanobservers.Visionresearch
117(2015),81â€“90.
[5] AliBorji,Ming-MingCheng,QibinHou,HuaizuJiang,andJiaLi.2019.Salientobjectdetection:Asurvey.Computationalvisualmedia5(2019),
117â€“150.
[6] StephanieBrams,GalZiv,OronLevin,JochimSpitz,JohanWagemans,AMarkWilliams,andWernerFHelsen.2019.Therelationshipbetween
gazebehavior,expertise,andperformance:Asystematicreview.Psychologicalbulletin145,10(2019),980.
[7] MaximilianDavideBrodaandBenjaminDeHaas.2022.Individualdifferencesinlookingatpersonsinscenes.JournalofVision22,12(2022),9â€“9.
[8] MaximilianDBrodaandBenjamindeHaas.2022.Individualfixationtendenciesinpersonviewinggeneralizefromimagestovideos.i-Perception
13,6(2022),20416695221128844.
[9] GuyThomasBuswell.1935.Howpeoplelookatpictures:astudyofthepsychologyandperceptioninart.(1935).
13Strohmetal.
[10] MoranCerf,JonathanHarel,WolfgangEinhÃ¤user,andChristofKoch.2007.Predictinghumangazeusinglow-levelsaliencycombinedwithface
detection.Advancesinneuralinformationprocessingsystems20(2007).
[11] XianyuChen,MingJiang,andQiZhao.2021.Predictinghumanscanpathsinvisualquestionanswering.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition.10876â€“10885.
[12] MarcellaCornia,LorenzoBaraldi,GiuseppeSerra,andRitaCucchiara.2016. Adeepmulti-levelnetworkforsaliencyprediction.In201623rd
InternationalConferenceonPatternRecognition(ICPR).IEEE,3488â€“3493.
[13] BenjaminDeHaas,AlexiosLIakovidis,DSamuelSchwarzkopf,andKarlRGegenfurtner.2019.Individualdifferencesinvisualsaliencevaryalong
semanticdimensions.ProceedingsoftheNationalAcademyofSciences116,24(2019),11687â€“11692.
[14] ShahramEivazi,RomanBednarik,MarkkuTukiainen,MikaelvonundzuFraunberg,VilleLeinonen,andJuhaEJÃ¤Ã¤skelÃ¤inen.2012.Gazebehaviour
ofexpertandnovicemicroneurosurgeonsdiffersduringobservationsoftumorremovalrecordings.InProceedingsoftheSymposiumonEyeTracking
ResearchandApplications.377â€“380.
[15] ChristopherIfeanyiEke,AzahAnirNorman,LiyanaShuib,andHenryFridayNweke.2019.Asurveyofuserprofiling:State-of-the-art,challenges,
andsolutions.IEEEAccess7(2019),144907â€“144924.
[16] CamiloFosco,AneliseNewman,PatSukhum,YunBinZhang,NanxuanZhao,AudeOliva,andZoyaBylinskii.2020.Howmuchtimedoyouhave?
modelingmulti-durationsaliency.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.4473â€“4482.
[17] SusanGauch,MircoSperetta,AravindChandramouli,andAlessandroMicarelli.2007. Userprofilesforpersonalizedinformationaccess. The
adaptiveWeb:methodsandstrategiesofWebpersonalization(2007),54â€“89.
[18] JunfengHe,KhoiPham,NachiappanValliappan,PingmeiXu,ChaseRoberts,DmitryLagun,andVidhyaNavalpakkam.2019.On-devicefew-shot
personalizationforreal-timegazeestimation.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervisionworkshops.0â€“0.
[19] ShengfengHe,ChuHan,GuoqiangHan,andJingQin.2019.Exploringdualityinvisualquestion-driventop-downsaliency.IEEEtransactionson
neuralnetworksandlearningsystems31,7(2019),2672â€“2679.
[20] SenHe,HamedRTavakoli,AliBorji,andNicolasPugeault.2019.Humanattentioninimagecaptioning:Datasetandanalysis.InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision.8529â€“8538.
[21] AlexanderHermans,LucasBeyer,andBastianLeibe.2017.Indefenseofthetripletlossforpersonre-identification.arXivpreprintarXiv:1703.07737
(2017).
[22] JohannesHewig,RalfHTrippe,HolgerHecht,ThomasStraube,andWolfgangHRMiltner.2008.Genderdifferencesforspecificbodyregionswhen
lookingatmenandwomen.JournalofNonverbalBehavior32(2008),67â€“78.
[23] SabrinaHoppe,TobiasLoetscher,StephanieAMorey,andAndreasBulling.2018.Eyemovementsduringeverydaybehaviorpredictpersonality
traits.Frontiersinhumanneuroscience(2018),105.
[24] SergeyIoffeandChristianSzegedy.2015. Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift.In
Internationalconferenceonmachinelearning.pmlr,448â€“456.
[25] ShamsiTIqbalandBrianPBailey.2004.Usingeyegazepatternstoidentifyusertasks.InTheGraceHopperCelebrationofWomeninComputing,
Vol.4.2004.
[26] LaurentIttiandChristofKoch.2001.Computationalmodellingofvisualattention.Naturereviewsneuroscience2,3(2001),194â€“203.
[27] LaurentItti,ChristofKoch,andErnstNiebur.1998.Amodelofsaliency-basedvisualattentionforrapidsceneanalysis.IEEETransactionsonpattern
analysisandmachineintelligence20,11(1998),1254â€“1259.
[28] KalervoJÃ¤rvelinandJaanaKekÃ¤lÃ¤inen.2017.IRevaluationmethodsforretrievinghighlyrelevantdocuments.InACMSIGIRForum,Vol.51.ACM
NewYork,NY,USA,243â€“250.
[29] MingJiang,ShengshengHuang,JuanyongDuan,andQiZhao.2015.Salicon:Saliencyincontext.InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition.1072â€“1080.
[30] TilkeJudd,FrÃ©doDurand,andAntonioTorralba.2012.Abenchmarkofcomputationalmodelsofsaliencytopredicthumanfixations.(2012).
[31] TilkeJudd,KristaEhinger,FrÃ©doDurand,andAntonioTorralba.2009.Learningtopredictwherehumanslook.In2009IEEE12thinternational
conferenceoncomputervision.IEEE,2106â€“2113.
[32] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980(2014).
[33] OnkarKrishna,AndreaHelo,PiaRÃ¤mÃ¤,andKiyoharuAizawa.2018.Gazedistributionanalysisandsaliencypredictionacrossagegroups.PloSone
13,2(2018),e0193149.
[34] BruceKrulwich.1997.Lifestylefinder:Intelligentuserprofilingusinglarge-scaledemographicdata.AImagazine18,2(1997),37â€“37.
[35] Chen-YuLee,SainingXie,PatrickGallagher,ZhengyouZhang,andZhuowenTu.2015.Deeply-supervisednets.InArtificialintelligenceandstatistics.
PMLR,562â€“570.
[36] MinLin,QiangChen,andShuichengYan.2013.Networkinnetwork.arXivpreprintarXiv:1312.4400(2013).
[37] AkisLinardos,MatthiasKÃ¼mmerer,OriPress,andMatthiasBethge.2021.DeepGazeIIE:Calibratedpredictioninandout-of-domainforstate-of-
the-artsaliencymodeling.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.12919â€“12928.
[38] MarcelLinkaandBenjamindeHaas.2020.OSIEshort:Asmallstimulussetcanreliablyestimateindividualdifferencesinsemanticsalience.Journal
ofvision20,9(2020),13â€“13.
[39] Yu-FeiMa,LieLu,Hong-JiangZhang,andMingjingLi.2002.Auserattentionmodelforvideosummarization.InProceedingsofthetenthACM
internationalconferenceonMultimedia.533â€“542.
14LearningUserEmbeddingsfromHumanGazeforPersonalisedSaliencyPrediction
[40] FatemehsadatMireshghallah,VaishnaviShrivastava,MiladShokouhi,TaylorBerg-Kirkpatrick,RobertSim,andDimitriosDimitriadis.2021.
Useridentifier:Implicituserrepresentationsforsimpleandeffectivepersonalizedsentimentanalysis.arXivpreprintarXiv:2110.00135(2021).
[41] SounakMondal,ZhiboYang,SeoyoungAhn,DimitrisSamaras,GregoryZelinsky,andMinhHoai.2023.Gazeformer:Scalable,EffectiveandFast
PredictionofGoal-DirectedHumanAttention.arXivpreprintarXiv:2303.15274(2023).
[42] YuyaMoroto,KeisukeMaeda,TakahiroOgawa,andMikiHaseyama.2020.Few-shotpersonalizedsaliencypredictionbasedonadaptiveimage
selectionconsideringobjectandvisualattention.Sensors20,8(2020),2170.
[43] JuntingPan,ElisaSayrol,XavierGiro-iNieto,KevinMcGuinness,andNoelEOâ€™Connor.2016. Shallowanddeepconvolutionalnetworksfor
saliencyprediction.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.598â€“606.
[44] MichaelJPazzani,JackMuramatsu,DanielBillsus,etal.1996.Syskill&Webert:Identifyinginterestingwebsites.InAAAI/IAAI,Vol.1.54â€“61.
[45] EvanFRisko,NicolaCAnderson,SophieLanthier,andAlanKingstone.2012. Curiouseyes:Individualdifferencesinpersonalitypredicteye
movementbehaviorinscene-viewing.Cognition122,1(2012),86â€“90.
[46] NegarSammaknejad,HamidrezaPouretemad,ChangizEslahchi,AlirezaSalahirad,andAshkanAlinejad.2017.Genderclassificationbasedoneye
movements:Aprocessingeffectduringpassivefaceviewing.Advancesincognitivepsychology13,3(2017),232.
[47] HosniehSattar,MarioFritz,andAndreasBulling.2020. Deepgazepooling:Inferringandvisuallydecodingsearchintentsfromhumangaze
fixations.Neurocomputing387(2020),369â€“382.
[48] JudeShavlik,SusanCalcari,TinaEliassi-Rad,andJackSolock.1998.Aninstructable,adaptiveinterfacefordiscoveringandmonitoringinformation
ontheworld-wideweb.InProceedingsofthe4thinternationalconferenceonIntelligentuserinterfaces.157â€“160.
[49] AnaFilipaSilva,FranciscoTomÃ¡sGonzÃ¡lezFernÃ¡ndez,etal.2022.Differencesinvisualsearchbehaviorbetweenexpertandnoviceteamsports
athletes:Asystematicreviewwithmeta-analysis.(2022).
[50] EktaSood,FabianKÃ¶gel,FlorianStrohm,PrajitDhar,andAndreasBulling.2021.VQA-MHUG:Agazedatasettostudymultimodalneuralattention
invisualquestionanswering.arXivpreprintarXiv:2109.13116(2021).
[51] EktaSood,FabianKÃ¶gel,PhilippMÃ¼ller,DominikeThomas,MihaiBÃ¢ce,andAndreasBulling.2023. MultimodalIntegrationofHuman-
Like Attention in Visual Question Answering. In Proc. Workshop on Gaze Estimation and Prediction in the Wild (GAZE), CVPRW. 2647â€“
2657. https://openaccess.thecvf.com/content/CVPR2023W/GAZE/papers/Sood_Multimodal_Integration_of_Human-Like_Attention_in_Visual_
Question_Answering_CVPRW_2023_paper.pdf
[52] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov.2014.Dropout:asimplewaytopreventneural
networksfromoverfitting.Thejournalofmachinelearningresearch15,1(2014),1929â€“1958.
[53] FlorianStrohm,EktaSood,SvenMayer,PhilippMÃ¼ller,MihaiBÃ¢ce,andAndreasBulling.2021. NeuralPhotofit:gaze-basedmentalimage
reconstruction.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.245â€“254.
[54] FlorianStrohm,EktaSood,DominikeThomas,MihaiBÃ¢ce,andAndreasBulling.2023.FacialCompositeGenerationwithIterativeHumanFeedback.
InAnnualConferenceonNeuralInformationProcessingSystems.PMLR,165â€“183.
[55] RoelVertegaal.2002.Designingattentiveinterfaces.InProceedingsofthe2002symposiumonEyetrackingresearch&applications.23â€“30.
[56] LennartWachowiak,PeterTisnikar,GerardCanal,AndrewColes,MatteoLeonetti,andOyaCeliktutan.2022.Analysingeyegazepatternsduring
confusionanderrorsinhumanâ€“agentcollaborations.In202231stIEEEInternationalConferenceonRobotandHumanInteractiveCommunication
(RO-MAN).IEEE,224â€“229.
[57] DirkWaltherandChristofKoch.2006.Modelingattentiontosalientproto-objects.Neuralnetworks19,9(2006),1395â€“1407.
[58] XiaodongWu,WeizheLin,ZhilinWang,andElenaRastorgueva.2020.Author2vec:Aframeworkforgeneratinguserembedding.arXivpreprint
arXiv:2003.11627(2020).
[59] YanyuXu,ShenghuaGao,JunruWu,NianyiLi,andJingyiYu.2018.Personalizedsaliencyanditsprediction.IEEEtransactionsonpatternanalysis
andmachineintelligence41,12(2018),2975â€“2989.
[60] YanyuXu,NianyiLi,JunruWu,JingyiYu,ShenghuaGao,etal.2017.BeyondUniversalSaliency:PersonalizedSaliencyPredictionwithMulti-task
CNN..InIJCAI.3887â€“3893.
[61] ZhiboYang,LihanHuang,YupeiChen,ZijunWei,SeoyoungAhn,GregoryZelinsky,DimitrisSamaras,andMinhHoai.2020.Predictinggoal-directed
humanattentionusinginversereinforcementlearning.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.193â€“202.
[62] AlfredLYarbusandAlfredLYarbus.1967.Eyemovementsduringperceptionofcomplexobjects.Eyemovementsandvision(1967),171â€“211.
[63] BingqingYu.2018.Personalizationofsaliencyestimation.McGillUniversity(Canada).
[64] QianZhao,ShuoChang,FMaxwellHarper,andJosephAKonstan.2016.Gazepredictionforrecommendersystems.InProceedingsofthe10thACM
ConferenceonRecommenderSystems.131â€“138.
[65] WanjunZhong,DuyuTang,JiahaiWang,JianYin,andNanDuan.2021.UserAdapter:Few-shotuserlearninginsentimentanalysis.InFindingsof
theAssociationforComputationalLinguistics:ACL-IJCNLP2021.1484â€“1488.
15