DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents
YilunXu*12 GabrieleCorso2 TommiJaakkola2 ArashVahdat1 KarstenKreis1
https://research.nvidia.com/labs/lpr/disco-diff
Abstract etal.,2022;Balajietal.,2022)ortheycangenerateaccurate
molecularstructures(Corsoetal.,2023;Yimetal.,2023;
Ingrahametal.,2023;Watsonetal.,2023). DMsleveragea
Diffusionmodels(DMs)haverevolutionizedgen-
forwarddiffusionprocessthateffectivelyencodesthetrain-
erativelearning. Theyutilizeadiffusionprocess
ingdatainasimple,unimodalGaussianpriordistribution.
to encode data into a simple Gaussian distribu-
Generationcanbeformulatedeitherasastochasticor,more
tion. However,encodingacomplex,potentially
conveniently,asadeterministicprocessthattakesasinput
multimodaldatadistributionintoasinglecontin-
random noise from the Gaussian prior and transforms it
uous Gaussian distribution arguably represents
intodatathroughagenerativeordinarydifferentialequation
an unnecessarily challenging learning problem.
(ODE)(Songetal.,2021). TheGaussianpriorcorresponds
WeproposeDiscrete-ContinuousLatentVariable
totheDM‚Äôscontinuouslatentvariables,wherethedatais
Diffusion Models (DisCo-Diff) to simplify this
uniquelyencodedthroughtheODE-definedmapping.
task by introducing complementary discrete la-
tentvariables. WeaugmentDMswithlearnable However, realistic data distributions are typically high-
discrete latents, inferred with an encoder, and dimensional,complexandoftenmultimodal. Directlyen-
train DM and encoder end-to-end. DisCo-Diff coding such data into a single unimodal Gaussian distri-
doesnotrelyonpre-trainednetworks,makingthe butionandlearningacorrespondingreversenoise-to-data
frameworkuniversallyapplicable. Thediscrete mappingischallenging. Themapping,orgenerativeODE,
latentssignificantlysimplifylearningtheDM‚Äôs necessarilyneedstobehighlycomplex,withstrongcurva-
complexnoise-to-datamappingbyreducingthe ture, and one may consider it unnatural to map an entire
curvatureoftheDM‚ÄôsgenerativeODE.Anaddi- datadistributiontoasingleGaussiandistribution. Inprac-
tionalautoregressivetransformermodelsthedis- tice,conditioninginformation,suchasclasslabelsortext
tributionofthediscretelatents,asimplestepbe- prompts,oftenhelpstosimplifythecomplexmappingby
causeDisCo-Diffrequiresonlyfewdiscretevari- offeringtheDM‚Äôsdenoiseradditionalcuesformoreaccu-
ableswithsmallcodebooks. WevalidateDisCo- rate denoising. However, such conditioning information
Diffontoydata,severalimagesynthesistasksas is typically of a semantic nature and, even given a class
wellasmoleculardocking,andfindthatintroduc- ortextprompt,themappingremainshighlycomplex. For
ingdiscretelatentsconsistentlyimprovesmodel instance,inthecaseofimages,evenwithinaclasswefind
performance. Forexample,DisCo-Diffachieves imageswithvastlydifferentstylesandcolorpatterns,which
state-of-the-artFIDscoresonclass-conditioned correspondstolargedistancesinpixelspace.
ImageNet-64/128datasetswithODEsampler.
Here, we propose Discrete-Continuous Latent Variable
DiffusionModels(DisCo-Diff),DMsaugmentedwithaddi-
tionaldiscretelatentvariablesthatencodeadditionalhigh-
1.Introduction levelinformationaboutthedataandcanbeusedbythemain
DMtosimplifyitsdenoisingtask(Fig.1). Thesediscrete
Diffusionmodels(DMs)(Sohl-Dicksteinetal.,2015;Ho
latentsareinferredthroughanencodernetworkandlearnt
etal.,2020;Songetal.,2021)haverecentlyledtobreak-
end-to-endtogetherwiththeDM.Thereby,thediscretela-
throughsforgenerativemodelingindiversedomains. For
tentsdirectlylearntoencodeinformationthatisbeneficial
instance,theycansynthesizeexpressivehigh-resolutionim-
forreducingtheDM‚Äôsscorematchingobjectiveandmaking
agery(Sahariaetal.,2022;Rameshetal.,2022;Rombach
theDM‚Äôshardtaskofmappingsimplenoisetocomplexdata
*WorkdoneduringaninternshipatNVIDIA. 1NVIDIA2MIT. easier. Indeed,inpractice,wefindthattheysignificantlyre-
Correspondenceto:YilunXu<yilunx@nvidia.com>. ducethecurvatureoftheDM‚ÄôsgenerativeODEandreduce
theDMtraininglossinparticularforlargediffusiontimes,
Proceedings of the 41st International Conference on Machine wheredenoisingismostambiguousandchallenging.Incon-
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
trasttopreviouswork(Baoetal.,2022;Huetal.,2023;Har-
theauthor(s).
1
4202
luJ
3
]GL.sc[
1v00330.7042:viXraDisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure1.Discrete-Continuous Latent Variable
Diffusion Models (DisCo-Diff) augment DMs
withadditionaldiscretelatentvariablesthatcap-
tureglobalappearancepatterns,hereshownfor
imagesofhuskies.(a)Duringtraining,discretela-
tentsareinferredthroughanencoder,forimages
a vision transformer (Dosovitskiy et al., 2021),
and fed to the DM via cross-attention. Back-
propagationisfacilitatedbycontinuousrelaxation
withaGumbel-Softmaxdistribution.Tosample
novelimages,anadditionalautoregressivemodel
islearntoverthedistributionofdiscretelatents.
(b)Schematicvisualizationofgenerativedenois-
ingdiffusiontrajectories.Differentcolorsindicate
differentdiscretelatentvariables,pushingthetra-
jectoriestowarddifferentmodes.
vey&Wood,2023),wedonotrelyondomain-specificpre- tentsagainimproveperformancebylearningtoindicatecrit-
trainedencodernetworks,makingourframeworkgeneral icalatomsintheinteractionand,inthisway,deconvolving
anduniversallyapplicable.Tofacilitatesamplingofdiscrete themultimodaluncertaintygivenbydifferentpossibleposes
latentvariablesduringinference,welearnanautoregressive fromcontinuousvariabilityofeachpose.Moreover,weaug-
modeloverthediscretelatentsinasecondstep. Weonly ment Poisson Flow Generative Models (Xu et al., 2022;
useasmallsetofdiscretelatentswithrelativelysmallcode- 2023b)withdiscretelatentvariablestoshowcasethatthe
books, whichmakestheadditionaltrainingoftheautore- frameworkcanalsobeappliedtoother‚Äúiterative‚Äùgenerative
gressivemodeleasy. Wespecificallyadvocatefortheuseof models,otherthanregularDMs,observingsimilarbenefits.
auxiliarydiscreteinsteadofcontinuouslatents;seeSec.3.2.
Contributions. (i)WeproposeDisCo-Diff,anovelframe-
While previous works (Esser et al., 2021; Ramesh et al., workforcombiningdiscreteandcontinuouslatentvariables
2021; Chang et al., 2022; Yu et al., 2022; Pernias et al., inDMsinauniversalmanner. (ii)Weextensivelyvalidate
2023;Changetal.,2023)usefullydiscretelatentvariable- DisCo-Diff,significantlyboostingmodelqualityinallex-
basedapproachestomodelimages,thistypicallyrequires periments, and achieving state-of-the-art performance on
largesetsofspatiallyarrangedlatentswithlargecodebooks, severalimagesynthesistasks. (iii)Wepresentdetailedanal-
whichmakeslearningtheirdistributionchallenging. DisCo- ysesaswellasablationandarchitecturedesignstudiesthat
Diff,incontrast,carefullycombinesitsdiscretelatentswith demonstratetheuniquebenefitsofdiscretelatentvariables
thecontinuouslatents(Gaussianprior)oftheDMandeffec- andhowtheycanbefedtothemaindenoisernetwork. (iv)
tivelyseparatesthemodelingofdiscreteandcontinuousvari- Overall,weprovideinsightsfordesigningperformantgen-
ationswithinthedata. Itrequiresonlyafewdiscretelatents. erative models. We make the case for discrete latents by
showingthatreal-worlddataisbestmodeledwithgenera-
Todemonstrateitsuniversality,wevalidatetheDisCo-Diff
tiveframeworksthatleveragebothdiscreteandcontinuous
frameworkonseveraldifferenttasks. Asamotivatingexam-
latents. Weintentionallydevelopedasimpleanduniversal
ple,westudy2Dtoydistributions,wherethediscretelatents
frameworkthatdoesnotrelyonpre-trainedencoderstooffer
learntocapturedifferentmodeswithsmallercurvaturedur-
abroadlyapplicablemodelingapproachtothecommunity.
ingsampling.Wethentackleimagesynthesis,wherethedis-
cretelatentslearnlarge-scaleappearance,oftenassociated
withglobalstyleandcolorpatterns.Thereby,theyoffercom- 2.Background
plementarybenefitstosemanticconditioninginformation.
DisCo-Diffbuildson(continuous-time)DMs(Songetal.,
Quantitatively,DisCo-Diffuniversallyboostsoutputquality
2021), andwefollowtheEDMframework(Karrasetal.,
andachievesstate-of-the-artperformanceonseveralIma-
2022). DMsperturbthecleandatay ‚àºp (x)inafixed
geNetgenerationbenchmarks. Inaddition,weexperimen- data
forwardprocessusingœÉ2(t)-varianceGaussiannoise,where
tallyvalidatethatauxiliarydiscretelatentsaresuperiorto
y‚ààRdandtdenotesthetimealongthediffusionprocess.
continuouslatentsinoursetup,andstudydifferentnetwork
Theresultingdistributionisdenotedasp(x;œÉ(t))withx‚àà
architecturesforinjectingthediscretelatentsintotheDM
Rd. ForsufficientlylargeœÉ ,thisdistributionisalmost
network. Acarefulhierarchicaldesigncanencouragediffer- max
identical to pure random Gaussian noise. DMs leverage
entdiscretelatentstoencodedifferentimagecharacteristics,
thisobservationtosamplex ‚àºN(x ;0,œÉ2 I)andthen
suchasshapevs.color,reminiscentofobservationsfromthe 0 0 max
iterativelydenoisethesamplethroughasequenceofM +1
literatureongenerativeadversarialnetworks(Karrasetal.,
graduallydecreasingnoiselevelsœÉ < œÉ (œÉ = œÉ ),
2019;2020). WealsoapplyDisCo-Difftomoleculardock- i+1 i 0 max
wherei‚àà[0,...,M]andx ‚àºp(x;œÉ ). TheœÉ correspond
ing,acriticaltaskindrugdiscovery,wherethediscretela- i i i
2DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
toadiscretizationofacontinuousœÉ(t)function. IfœÉ =0,
M
thenthefinalx followsthedatadistribution. Sampling
M
correspondstosimulatingadeterministicorstochasticdif-
ferentialequation
dx=‚àíœÉÀô(t)œÉ(t)‚àá logp(x;œÉ(t))dt
x
(cid:124) (cid:123)(cid:122) (cid:125)
ProbabilityFlowODE
(1)
(cid:112)
‚àíŒ≤(t)œÉ2(t)‚àá logp(x;œÉ(t))dt+ 2Œ≤(t)œÉ(t)dœâ ,
x t
(cid:124) (cid:123)(cid:122) (cid:125)
LangevinDiffusionSDE
where dœâ is a standard Wiener process and (a) 128√ó128 (b) Shareddiscretelatents
t
‚àá xlogp(x;œÉ(t)) is the score function of the dif- Figure2.SamplesgeneratedfromDisCo-DifftrainedontheIma-
fuseddistributionp(x;œÉ(t)). ThefirstterminEq.(1)isthe geNetdataset:(a)randomlysampleddiscretelatentsandclassla-
ProbabilityFlowODE,whichpushessamplesfromlargeto bels;(b)samplesineachgridsharingthesamediscretelatent.The
smallnoiselevels. ThesecondtermisaLangevinDiffusion classlabelforthetop/bottomrowisfixedtocoffeepot/malamute.
SDE,anequilibriumsamplerfordifferentnoiselevelsœÉ(t),
a categorical distribution of codebook size k. There are
whichcanhelpcorrecterrorsduringsynthesis(Karrasetal.,
three learnable components: the denoiser neural network
2022).Thiscomponentcanbescaledbythetime-dependent D : Rd√óR√óNm ‚ÜíRd,correspondingtoDisCo-Diff‚Äôs
Œ∏
parameterŒ≤(t). SettingŒ≤(t)=0leadstopureODE-based
DM,whichpredictsdenoisedimagesconditionedondiffu-
synthesis. Generally, different sampling methods can be siontimetanddiscretelatentz;anencoderE : Rd ‚ÜíNm,
œï
usedtosolvethegenerativeODE/SDE.
used to infer discrete latents given clean images y. It
TrainingaDMcorrespondstolearningamodeltoapproxi- outputsacategoricaldistributionoverthekcategoriesfor
mate the intractable score function ‚àá logp(x;œÉ(t)). eachdiscretelatent;andapost-hocauto-regressivemodel
x
Following the EDM framework, we parametrize A œà, which approximates the distribution of the learned
‚àá xlogp(x;œÉ(t)) = (D Œ∏(x,œÉ(t)) ‚àí x)/œÉ2(t), where
discretelatentszby(cid:81)m
i=1p œà(z i|z <i). DisCo-Diff‚Äôstrain-
D (x,œÉ(t)) is a learnable denoiser neural network that ingprocessisdividedintotwostages. Inthefirststage,the
Œ∏
is trained to predict clean data from noisy inputs and is denoiserD Œ∏andtheencoderE œïareco-optimizedinanend-
conditionedonthenoiselevelœÉ(t). Itcanbetrainedusing to-endfashion. Thisisachievedbyextendingthedenoising
denoising score matching (Hyva¬®rinen, 2005; Lyu, 2009; scorematchingobjective(asexpressedinEq.2)toinclude
Vincent,2011;Song&Ermon,2019),minimizing learnablediscretelatentszassociatedwitheachdatay:
E E (cid:2) Œª(t)||D (y+n,œÉ(t))‚àíy||2(cid:3) (2) E E E (cid:2) Œª(t)||D (y+n,œÉ(t),z)‚àíy||2(cid:3) , (3)
y‚àºpdata(y) t,n Œ∏ y z‚àºEœï(y) t,n Œ∏
wheret‚àºp(t)foradistributionp(t)overdiffusiontimes wherey‚àºp (y). Incontrasttothestandardobjectivein
data
t, n ‚àº N(n;0,œÉ2(t)I), and Œª(t) is a function that gives Eq.2,whichfocusesonlearningthereparameterizationof
differentweighttotheobjectivefordifferentnoiselevels. thescore‚àá logp(x;œÉ(t)), thedenoiserinourapproach
x
isessentiallylearningthereparameterizationofthecondi-
In this work, we use œÉ(t) = t and follow the EDM
tional score ‚àá logp(x|z;œÉ(t)), with the convolution of
work‚Äôsconfiguration(Karrasetal.,2022),unlessotherwise x
the probability density functions p(¬∑|z;œÉ(t)) = p(¬∑|z) ‚àó
noted. Moreover,wealsoleverageclassifier-freeguidance
N(0,œÉ2(t)I). Thisconditionalscoreoriginatesfromcon-
inDisCo-Diffwhenconditioningonthediscretelatentvari-
ditioningtheDMonthediscretelatentsz,whichareinferred
ables. Classifier-freeguidancecombinesthescorefunctions
bytheencoderE .ThedenoisernetworkD canbettercap-
of an unconditional and a conditional diffusion model to œï Œ∏
turethetime-dependentscore(i.e.,achievingareducedloss)
amplifytheconditioning;seeHo&Salimans(2021).
ifthescoreforeachsub-distributionp(x|z;œÉ(t))issimpli-
fied. Therefore,theencoderE ,whichhasaccesstoclean
3.DisCo-Diff œï
inputdata,isencouragedtoencodeusefulinformationinto
InSec.3.1, wefirstformallydefineDisCo-Diff‚Äôsgenera- thediscretelatentsandhelpthedenoisertomoreaccurately
tivemodelandtrainingframework,beforediscussingand reconstruct the data. Naively backpropagating gradients
carefullymotivatingourapproachindetailinSec.3.2. In intotheencoderthroughthesamplingofthediscretelatent
Sec.3.3,wehighlightcriticalarchitectureconsiderations. variableszisnotpossible. Hence,duringtrainingwerely
onacontinuousrelaxationbasedontheGumbel-Softmax
3.1.GenerativeModelandTrainingObjective distribution(Jangetal.,2016)(seeApp.Dfordetails).
InourDisCo-Diffframework(Fig.1),weaugmentaDM‚Äôs Whentrainingthedenoisernetwork,werandomlyreplace
learning process with an m-dimensional discrete latent the discrete latent variables with a non-informative null-
z‚ààNm,whereeachdimensionisarandomvariablefrom embeddingwithprobability0.1. Thereby, theDMlearns
3DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
40 DisCo-Diff 1.25 DisCo-Diff
30 Diffusion Model 1.00 Diffusion Model
100
20 100 0.75 102
0.50
102
10 5 10 0.25 104 0 10
0 0.00
2 4 6 8 10 0.0 2.5 5.0 7.5 10.0
backward ODE time t backward ODE time t
Figure4.Modeling 2D mixture of Gaussians: analysis. The
meancurvature(left)andnormoftheneuralnetworks‚ÄôJacobians
(right)alongthereverse-timeODEtrajectoriesasfunctionoft.
Figure3.Modeling2DmixtureofGaussians.Left:Datadistri-
bution.Middle:GenerateddatabyregularDM.Right:Generated
plex noise-to-data mapping. The noise is drawn from an
databyDisCo-Diff. Weusedifferentcolorstodistinguishdata
generatedbydifferentdiscretelatents.Wefurtherprovidezoom- analyticallytractable,unimodalGaussiandistribution. As
insandvisualizesomeODEtrajectoriesbydottedlines. thedataisencodedinthisdistribution,wecanconsiderthis
high-dimensional Gaussian distribution the DM‚Äôs contin-
both a discrete latent variable-conditioned and a regular,
uouslatentvariables(DMscangenerallybeseenasdeep
unconditional score. During sampling, we can combine
latentvariablemodels(Huangetal.,2021;Kingmaetal.,
these scores for classifier-free guidance (Ho & Salimans,
2021)). However,themappingfromunstructurednoisetoa
2021)withrespecttothemodel‚Äôsowndiscretelatents,and
diverse,typicallymultimodaldatadistributionnecessarily
amplifytheirconditioningeffect(detailsinApp.B).
needstobehighlycomplex. Thiscorrespondstoahighly
WecaninterpretDisCo-Diffasavariationalautoencoder non-lineargenerativeODEwithstrongcurvature,whichis
(VAE) (Kingma & Welling, 2014; Rezende et al., 2014; challengingtolearnandalsomakessynthesisslowbyrequir-
vandenOordetal.,2017;Rolfe,2017)withdiscretelatents ingafinediscretization. Toillustratethispoint,wetrained
andaDMasdecoder. VAEsoftenemployregularization aDMonasimple2DmixtureofGaussians,whereweob-
ontheirlatents. Wedidnotfindthistobenecessary,aswe servebentODEtrajectoriesnearthedata(Fig.3,middle).
useonlyverylow-dimensionallatentvariables,e.g.,10in Thiseffectissignificantlystrongerinhighdimensions.
ourImageNetexperiments,withrelativelysmallcodebooks.
Asimplermappingwithdiscretelatentvariables. The
Moreover,weemployastrictlynon-zerotemperatureinthe
roleofthediscretelatentsinDisCo-Diffistoreducethis
Gumbel-Softmaxrelaxation,encouragingstochasticity.
complexityandmaketheDM‚Äôslearningtaskeasier. The
Inthesecondstage,wetraintheautoregressivemodelA singlenoise-to-datamappingiseffectivelypartitionedintoa
œà
to capture the distribution of the discrete latent variables setofsimplermappings,eachwithlesscurvatureinitsgen-
p (z)definedbypushingthecleandatathroughthetrained erativeODE.Wearguethatitisunnaturaltomapanentire
œï
encoder.Weuseamaximumlikelihoodobjectiveasfollows: multimodalcomplexdatadistributiontoasinglecontinu-
ousGaussiandistribution. Instead,webelievethatanideal
(cid:34) m (cid:35)
(cid:88) generativemodelshouldcombinebothdiscreteandcontin-
E logp (z |z ) (4)
y‚àºpdata(y),z‚àºEœï(y) œà i <i uouslatentvariables,wherediscretelatentscaptureglobal
i=1 multimodalstructureandthecontinuouslatentsmodellocal
Since we set m to a relatively small number, it becomes continuous variability. With this in mind, we suggest to
veryeasyforthemodeltohandlesuchshortdiscretevec- onlyuseamoderatenumberofdiscretelatentswithsmall
tors,whichmakesthissecond-stagetrainingefficient. Also codebooks. Ontheonehand,afewlatentscanalreadysig-
the additional sampling overhead due to this autoregres- nificantly simplify the DM‚Äôs learning task. On the other
sivecomponentontopoftheDMbecomesnegligible. At hand, if we only have few latents with small codebooks,
inferencetime, whenusingDisCo-Difftogeneratenovel trainingagenerativemodel‚Äîanautoregressiveoneinour
samples,wefirstsampleadiscretelatentvariablefromthe case‚Äîover the discrete latent variable distribution itself,
autoregressivemodel,andthensampletheDMwithanODE willbesimple(whichweobserve,seeSec.4).
orSDEsolver. Weprovidethealgorithmpseudocodefor
Validationin2D.Tovalidateourreasoning,letusrevisit
trainingandsamplinginAppendixC.
thetoy2DmixtureofGaussians. InFig.3,right,weshow
3.2.MotivationandRelatedWork the DisCo-Diff model‚Äôs synthesized data. The discrete
latents learn to capture the different modes, and DisCo-
We will now critically discuss and motivate our design
Diff‚ÄôsDMcomponentmodelstheindividualmodes. The
choices and also discuss the most relevant related works.
DM‚ÄôsODEtrajectoriesfordifferentlatentsarenowalmost
ForanextendeddiscussionofrelatedworkseeApp.A.
perfectly straight, indicating a simple conditional score
Thecurvatureofdiffusionmodels. DMs,intheirsimpler function. In Fig. 4, left, we quantitatively show strongly
ODE-basedformulation(Œ≤(t)=0inEq.(1)),learnacom- reducedcurvaturealongtheentirediffusiontimet.InFig.4,
4
.xoppa
erutavruc
naibocaJ
fo
mronDisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
However,couldweusepre-trainedencodernetworks,such
asCLIP(Radfordetal.,2021)orothers(Heetal.,2020;
Caronetal.,2021),toproduceencodingstoconditionon
andwhosedistributioncouldbemodeledinasecondstage?
Thisisexploredbypreviousworks(Harvey&Wood,2023;
Baoetal.,2022;Huetal.,2023;Lietal.,2023), buthas
importantdisadvantages: (i)Themostcrucialdownsideis
thatsuchencodersarenotuniversallyavailable,buttypically
onlyforimages. However,weseektodevelopauniversally
applicableframework. Forinstance,wealsoapplyDisCo-
Difftomoleculardocking(seeSec.4.2),wherenosuitable
pre-trainednetworksareavailable. (ii)InDisCo-Diff,the
Figure5.GrouphierarchicalDisCo-Diff. Differentdiscretela- jobofthediscretelatentsistomakethedenoisingtaskof
tentsarefedtothedenoiserU-Netatdifferentfeatureresolutions.
theDMeasier,whichisespeciallyambiguousatlargenoise
levels(infact,wefindthatthelatentshelpinparticularto
reduce the loss at these high noise levels, see Fig. 7). It
right,asameasureofnetworkcomplexitywealsoshowthe
isnotobviouswhatinformationaboutthedatathelatents
normsoftheJacobiansoftheemployeddenoisernetworks.
shouldbestencodeforthis. Bylearningthemjointlywith
WeseesignificantlyreducednormsforDisCo-Diffforall
theDMobjectiveitself,theyaredirectlytrainedtohelpthe
t, suggesting that the denoiser‚Äôs task is indeed strongly
DMlearnbetterdenoisersandlowercurvaturegenerative
simplifiedandlessnetworkcapacityisrequired.
ODEs. (iii)Agenerativemodelneedstobetrainedoverthe
Usingfew,globallatentswithrelativelysmallcodebooks encodingsinthesecondstage. InDisCo-Diff,wecanfreely
isimportant. DisCo-Diffisfundamentallydifferentfrom chooseanappropriatenumberoflatentsandcodebooksize
mostcontemporarygenerativemodelingframeworksusing tosimplifytheDM‚Äôsdenoisingtask,whilealsofacilitating
discretelatentvariables(vandenOordetal.,2017;Esser easy learning of the autoregressive model in the second
et al., 2021; Ramesh et al., 2021; Chang et al., 2022; Yu stage.Whenusingpre-trainedencoders,onemustworkwith
etal.,2022;Perniasetal.,2023;Changetal.,2023). These theencodingsbythesemethods,whichwerenotdeveloped
worksuseautoencoderstoencodeimagesinitsentiretyinto forgenerativemodeling. WeattributeDisCo-Diff‚Äôsstrong
spatially-arranged,downsampledrepresentationsofthein- generationperformancetoitsend-to-endlearning.
puts,focusingmoreonpreservingimagefidelitythanoncap-
Thelatentvariablesmustbediscrete. Couldwealsouse
turingdiversetrainingdatamodes. However,thisisalsoun-
auxiliarycontinuouslatentvariables? Generativemodels
natural: Encodingcontinuousvariability,likesmoothpose,
oncontinuouslatentsarealmostalwaysbasedonmappings
shape, or color variations in images, into discrete latents
fromauni-modalGaussiandistributiontothedistribution
requirestheuseofverylargecodebooksand,ontopofthat,
of latents. Hence, if such continuous latents learnt mul-
thesemodelsgenerallyrelyonveryhigh-dimensionalspa-
timodal structure in the data to simplify the main DM‚Äôs
tialgridsofdiscretelatents(e.g. 32x32=1024latentswith
denoising task, as DisCo-Diff‚Äôs discrete latents do, then
codebooks>1,000(Esseretal.,2021),whileweusejust10
learningadistributionovertheminthesecondstagewould
latentswithacodebooksizeof100inourmainimagemod-
againrequireahighlynon-lineardifficult-to-learnmapping
els). Thismakeslearningthedistributionoverthediscrete
from Gaussian noise to the multimodal encodings. This
latents very challenging for these types of models, while
istheproblemDisCo-Diffaimstosolveinthefirstplace.
itissimpleinDisCo-Diff,wheretheyjustsupplementthe
Preechakul et al. (2022) augment DMs with non-spatial
DM.InDisCo-Diff,wegetthebestfrombothcontinuous
continuouslatentvariables,buttheyonlyfocusonseman-
anddiscretelatentvariables,usingonlyfewgloballatents.
tic face image manipulation. InfoDiffusion (Wang et al.,
End-to-end training is essential. DisCo-Diff‚Äôs discrete 2023) conditions DMs on discrete latent variables. How-
latentsareinspiritsimilartoleveragingnon-learntcondi- ever, it focuses on learning disentangled representations,
tioninginformation. AspointedoutbyBaoetal.(2022), alsoprimarilyforlow-resolutionfacesynthesis,anduses
thishasbeencrucialtofacilitatetraininghigh-performance amutualinformation-basedobjective. ContrarytoDisCo-
generativemodelslikestrongclass-conditional(Dhariwal Diff,neitheroftheseworkstackleshigh-qualitysynthesis
& Nichol, 2021; Kingma & Gao, 2023) or text-to-image forchallenging,diversedatasets.
DMs(Rameshetal.,2022;Hoetal.,2022;Rombachetal.,
Inourablationstudies(Sec.4.1), wefurthervalidateour
2022). However,DisCo-Diffaimstofundamentallyaddress
designchoicesandmotivationsthatwepresentedhere.
theproblem,ratherthanrelyingongivenconditioningdata.
Moreover,thedatausuallyhassignificantvariabilityeven
given,forinstance,aclasslabel. Ourdiscretelatentscan
furtherreducethecomplexity(asobserved,seeSec.4).
5DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Table1. FIDscoretogetherwithNFEonImageNet-64. Table2.FIDscoreandNFEonclass-cond.ImageNet-128.
FID NFE FID NFE
withoutclass-conditioning ADM(Dhariwal&Nichol,2021) 5.91 250
ADM-G(Dhariwal&Nichol,2021) 2.97 250
IC-GAN(Casanovaetal.,2021) 9.20 1 CDM(32,64,128)(Hoetal.,2021) 3.52 8100
BigGAN(Brocketal.,2018) 16.90 1 RIN(Jabrietal.,2022) 2.75 1000
iDDPM(Nichol&Dhariwal,2021) 16.38 50
EDM(Karrasetal.,2022) 6.20 50 VDM++,w/ODEsampler(Kingma&Gao,2023) 2.29 115
SCDM(Baoetal.,2022) 3.94 50 DisCo-Diff,w/ODEsampler(ours) 1.98 114
DisCo-Diff(ours) 3.70 50 VDM++,w/DDPMsampler(Kingma&Gao,2023) 1.88 512
DisCo-Diff,w/ODEsampler,VDM++correction 1.73 414
class-conditioned,ODEsampler
EDM(Karrasetal.,2022) 2.36 79
PFGM++(Xuetal.,2023b) 2.32 79 layer,enablingdiscretelatentstogloballyshapetheimage
DisCo-PFGM++(ours) 1.92 78 features. We add a cross-attention layer after each self-
DisCo-Diff(ours) 1.65 78
attentionlayerwithintheU-Net. Inourmainmodels,all
class-conditioned,stochasticsampler discretelatentsaregiventoallcross-attentionlayers.
iDDPM(Nichol&Dhariwal,2021) 2.92 250
ADM(Dhariwal&Nichol,2021) 2.07 250 Grouphierarchicalmodels.Toenhancetheinterpretability
CDM(Hoetal.,2021) 1.48 8000 ofdiscretelatents,wealsoexploretheinductivebiasinher-
VDM++(Kingma&Gao,2023) 1.43 511
entintheU-Netarchitectureandfeeddistinctlatentgroups
EDM(w/Restart(Xuetal.,2023a)) 1.36 623
RIN(Jabrietal.,2022) 1.23 1000 intovariousresolutionfeaturesintheup-samplingbranch
DisCo-Diff(ours;w/Restart(Xuetal.,2023a)) 1.22 623 of the U-Net, as shown in Fig. 5. This approach draws
class-conditioned,w/adversarialobjective inspirationfromStyleGAN(Karrasetal.,2019),wheredis-
IC-GAN(Casanovaetal.,2021) 6.70 1 tinctlatentsareintroducedatdifferentresolutions,enabling
BigGAN-deep(Brocketal.,2018) 4.06 1 eachtocapturedifferentimagecharacteristicsbytheneural
CTM(Kimetal.,2023a) 1.92 1
network‚Äôsinductivebias. Thisdesignfostersagrouphier-
StyleGAN-XL(Saueretal.,2022) 1.51 1
archy,wherethegroupsassociatedwithhigher-resolution
featuresoffersupplementaryinformation,conditionedupon
3.3.Architecture
thegroupsrelatedtolower-resolutionfeatures. Wereferto
Asdiscussed,DisCo-Diffenhancesthetrainingofcontinu- thisrefinedmodelasthegrouphierarchicalDisCo-Diff.
ousDMsbyincorporatinglearnablediscretelatentvariables
In the molecular docking task, existing denoisers oper-
that are meant to capture the global underlying discrete
atethroughmessagepassinginapermutationequivariant
structureofthedata. ToensurethatDisCo-Diffworksasin-
way over 3D point clouds representing molecular struc-
tended,suitablenetworkarchitecturesarenecessary. Below,
tures(Corsoetal.,2023). Webuildthispropertyandarchi-
wesummarizeourdesignchoices,focusingonDisCo-Diff
tecturalbiasdirectlyintothelatentvariables,allowingthem
forimagesynthesis. However,theframeworkisgeneral,re-
totakevaluesindicatingonenodeinthepointcloud(there-
quiringonlyanencodertoinferdiscretelatentsfromclean
fore, for every point cloud, the codebook size equals the
input data and a conditioning mechanism that integrates
numberofnodes). Thislatentdesignchoicealignswiththe
thesediscretelatentsintothedenoisernetwork. Infact,we
intuitionoftheencoderdeterminingtheatomsplayingkey
alsoapplyourmodelto2Dtoydataandmoleculardocking.
rolesinthestructureandallowsforminimalmodification
Encoder. For image modeling, we utilize a ViT (Doso- ofthescoremodelwherethelatentssimplyrepresentaddi-
vitskiyetal.,2021)asthebackbonefortheencoder. We tionalfeaturesforeverynode.Theencoderisalsocomposed
extendtheclassificationmechanisminViTs,andtreateach ofasimilarequivariantmessagepassing,e3nn(Geiger&
discretetokenasadifferentclassificationtoken. Concretely, Smidt,2022), networkwhereforeachnodeonelogitper
weaddmextraclassificationtokenstothesequenceofim- latentwillbepredicted. Moredetailsonthearchitecturefor
agepatches. Thisarchitecturaldesignnaturallyallowseach themoleculardockingtaskcanbefoundinApp.E.4.
discretelatenttoeffectivelycapturetheglobalcharacteristic
Theauto-regressivemodeloverthedistributionofthedis-
oftheimages,akintoperformingdataclassification.
cretelatentsisimplementedinimageexperimentsusinga
Discretelatentvariableconditioning. Forimageexperi- standardTransformerdecoder(Vaswanietal.,2017). For
ments,DisCo-Diff‚ÄôsdenoisersareU-Netsaswidelyused moleculardocking,itagainusesane3nnnetworkthatis
forDMs(Karrasetal.,2022;Hoogeboometal.,2023). For fed the conditioning information of the protein structure
thediscrete latentvariable conditioning, we utilizecross- andmoleculargraph. Generally,DisCo-Diffiscompatible
attention(Rombachetal.,2022). Drawinginspirationfrom withotherconditionalinputs,e.g. classlabels,whichcan
text-to-imagegeneration,DisCo-Diff‚Äôsdiscretelatentsfunc- beaddedasinputstodenoiserandauto-regressivemodel.
tionanalogouslytotext,exertingaglobalinfluenceonthe Weuseanauto-regressivemodelforsimplicityandexpect
denoiser‚Äôsoutput. Specifically,imagefeaturesactasqueries DisCo-Diff‚Äôssecondstagetoworkequallywellwithother
anddiscretelatentsarekeysandvaluesinthecross-attention discretedatagenerativemodels,e.g. discretestatediffusion
6DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
discretelatentsandimages,andtheencoder. Wefine-tune
Table3. Ablationsonclass-cond.ImageNet-64.
the pre-trained U-Net in EDM (Karras et al., 2022) with
FID
discrete latents for ImageNet-64. For ImageNet-128, we
EDM(Karrasetal.,2022) 2.36 implementtheU-ViTinVDM++(Kingma&Gao,2023),
Oraclesetting and fine-tune our trained VDM++ model using discrete
Continuouslatent(KLDweight=0.1) 1.67 latents. Wealsoadheretotheirrespectivenoiseschedules
Continuouslatent(KLDweight=1) 2.36 and loss weightings during the training process. We use
DisCo-Diff(cfg=0) 1.65
Heun‚Äôs second-order method as ODE sampler, and a 12-
Generativeprioronlatent
layer Transformer as the auto-regressive model. We set
Continuouslatent(KLDweight=0.1) 11.12 thelatentdimensiontom = 10andthecodebooksizeto
Continuouslatent(KLDweight=1,cfg=0) 2.36
k =100inDisCo-Diff.
Continuouslatent(KLDweight=1,cfg=1) 2.36
DisCo-Diff(cfg=0) 1.81
Results. See Tables 1 and 2. (1) DisCo-Diff achieves
DisCo-Diff(cfg=1) 1.65
DisCo-Diff(cfg=2) 2.33 thenewstate-of-the-artonclass-conditionedImageNet-
64/ImageNet-128whenusingODEsampler. Specifically,
DisCo-Diffreducesthepreviousstate-of-the-artFIDscore
models(Austinetal.,2021;Campbelletal.,2022). Archi-
from2.36to1.65onImageNet-64,andfrom2.29to1.98
tecturedetails,alsofor2Dtoydataexperiments,inApp.F.
onImageNet-128. Thisalignswithouranalysis(Sec.3.2)
Animportantquestionsurroundingthearchitecturedesign thatDisCo-DiffyieldsstraighterODEtrajectories.
ishowtochooseanappropriatenumberoflatentsand
(2)DisCo-Diffoutperformsallbaselinesintheuncondi-
codebooksize. Whileintuitively,increasingthenumberof
tionalsetting,orwhenusingstochasticsampler. DisCo-
latents and the codebook size might seem like a straight-
Diffalsosurpassesthepreviousbestmethod(SCDM(Bao
forwardmethodtoreducethereconstructionerrorfurther
etal.,2022))intheunconditionalsetting,eventhoughtheir
bycapturingmoreintricatedatastructures,thisapproach
method relies on pre-trained MoCo features. In addition,
alsointroducesadditionalcomplexity. Specifically,alarger
DisCo-DiffsetsthenewrecordImageNet-64FIDof1.22
set of latents or an expanded codebook size complicates
whenusingRestartsampler(Xuetal.,2023a). Notethatthe
theauto-regressivemodel‚Äôstask,potentiallyleadingtoin-
competitivemethodRIN(Jabrietal.,2022)employsanovel
creasederrors. Thus,findingabalancebetweenenhancing
architecturedistinctfromconventionalU-Nets/U-ViTs.
model performance through more detailed discrete struc-
turesandmaintainingmanageablemodelingcomplexityfor OnImageNet-128,weobservethatDisCo-Diffdoesnotper-
theauto-regressivemodeliscrucial. Werecommendusing formwellwithstochasticsamplers. WhenusingtheDDPM
amodestnumberoflatent(e.g. 1030)andcodebooksize sampler as in VDM++ (Kingma & Gao, 2023), DisCo-
(e.g.50100)forcurrentdiffusionmodelsandleavingtheex- Diff achieves an FID score of 2.80, which is worse than
plorationofoptimalhyper-parameterstofutureworks. For VDM++‚ÄôsFIDscoreof1.88. Asthediscretelatentsreduce
example,inourimagegenerationexperiments,wefound thelossatlargertimes(c.f.Figure7)andthetrainingtargets
thataconfigurationof10latentswithacodebooksizeof atthesetimestypicallycorrespondtolow-frequencycom-
100significantlyenhancesperformanceonthecomplexIm- ponentsofimages, wehypothesizethatinthismodelthe
ageNetdataset. Wedidnottunethishyper-parameterdue discretelatentslearnttooverlyemphasizeglobalinforma-
to computational constraints. We believe that more care- tion,divertingthemodeltooverlooksomehigh-frequency
ful hyper-parameter optimization over the exact number details necessary at smaller times. We provide empirical
of latents and the codebook size would further boost the evidenceinAppendixG.3toshowthatVDM++w/DDPM
performanceofDisCo-Diff. bettercapturesdetailsatsmallertimes,whichsupportsthis
hypothesis. Notethat,intheory,VDM++andDisCo-Diff
shouldperformsimilarlyatsmallertimes. Apotentialso-
4.Experiments
lution could concatenate the discrete latents with a null
4.1.ImageSynthesis token,similartotext-to-imagemodels(Balajietal.,2022),
allowingthemodeltolearnmoreeasilytoexcludethein-
WeusetheImageNet(Dengetal.,2009)datasetandtackle
fluenceofdiscretelatentsatsmallertimes. Weleaveitfor
both class-conditional (at varying resolutions 64√ó64 and
futureexploration.Wewouldliketoemphasizethatweonly
128√ó128)andunconditionalsynthesis. Tomeasuresample
observedthisbehaviorforthisonemodelanddataset. In
quality,wefollowtheliteratureanduseFre¬¥chetInception
allotherexperiments,discretelatentsuniversallyimproved
Distance(FID)(Heuseletal.,2017)(lowerisbetter). We
performanceforallstochasticandnon-stochasticsamplers,
alsoreportthenumberofneuralfunctionevaluations(NFE).
andwhenusedforalltimest. TobetterutilizetheDDPM
Intheclass-conditionalsetting,theDisCo-Diff‚Äôsdenoiser sampler for the current model, we substituted the DisCo-
isinitializedusingpre-trainedImageNetmodels,exceptfor DiffODEwiththeVDM++DDPMtrajectoriesatsmaller
the new components: the cross-attention layers between times (t < 10), (DisCo-Diff, w/ ODE sampler, VDM++
7DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
fromtrainingimages)andgenerativeFID(latentssampled
fromsecond-stagelatentgenerativemodels). Conversely,
ahigherKLDweight(1)causesencodercollapse,andthe
continuous latents are not used (no latent (EDM), oracle
latents and generative latents all produce same FIDs). In
contrast,DisCo-Diff‚ÄôsgenerativeFIDshowsonlyaminor
degradationcomparedtotheoracleFID,indicatingtheease
ofmodelingthediscretepriorwithasimpleTransformer.
The DM training objective (Eq. (2)) has most variability
atlargediffusiontimesduetothemultimodalposteriorof
cleandatagivennoisyinputs(Xuetal.,2023c). Condition-
ing information can reduce this ambiguity. For instance,
Balajietal.(2022)showthattextconditioningprimarilyin-
Figure6.Top:Imagescreatedfromtwo30-dimdiscretelatentsz fluencesthedenoiseratlargertimes.Fig.7(a)showsthatthe
andzÀÜ,withthefar-rightcolumncombiningtheirsub-coordinates. learneddiscretelatentsbehavesimilarlytotextcondition-
Bottom:Variationsinimagesbyfixingportionsofz(originating ing,significantlyloweringthetraininglossathighertime
fromthered-boxedimage).Weseethatlower-resolutionlatents steps. Complementarily,Fig.7(b)indicatesthatswitching
affectlayout/shape;high-resolutionlatentsaltercolor/texture. discretelatentstowardstheendofsamplingbarelyaffects
thesamples,implyingtheyarenotusedatsmallertimest.
correctioninTable2),whichimprovestheFIDto1.73. In
In DisCo-Diff, the sampling time of the auto-regressive
conclusion,whilethediscretelatentsdidnothelpatsmall
modelisnegligiblecomparedtotheDM‚Äôs. Forinstance,for
timeshere, theystillboostedperformanceatlargertimes
generating32imagesonImageNet-128,theauto-regressive
andallowedustooutperformthepureVDM++modeland,
modelsrequiresonly0.44seconds,whileDisCo-Diff‚ÄôsDM
onceagain,achievestate-of-the-artperformance.
componenttakes78secondsfor114NFE,withanaverage
(3)Discretelatentscapturevariabilitycomplementary of0.68second/NFE,allonasingleNVIDIAA100GPU.
toclasssemantics. Fig.2(b)illustratesthatsamplesshar-
GroupHierarchicalDisCo-Diff. Weevaluatethegroup
ingthesamediscretelatentexhibitsimilarcharacteristics,
hierarchicalDisCo-Diff(Sec.3.3),feedingthreeseparate
andtherearenoticeabledistinctionsfordifferentdiscrete
10-dim. discretelatentsintotheU-Netateachlevelofreso-
latents under the same class. It suggests that the discrete
lution.Fig.6showsthatlatentsforlower-resolutionfeatures
latentscapturevariationsthatareusefulinsimplifyingthe
mainlygovernoverallshapeandlayout, whilelatentsfor
diffusionprocessdefinedinEuclideanspacebeyondclass
higher-resolution control color and texture. For example,
labels,underpinningtheimprovementsofDisCo-Diffover
inthebottomfigure,whengraduallyfixinggroupsinorder,
thepre-trainedclass-conditionedDMs. (4)Discretelatents
theimagesfirstconvergeinshapeandthenincolor.
boost the performance on PFGM++. When applied to
anotherODE-basedgenerativemodelPFGM++(Xuetal., 4.2.MolecularDocking
2023b),DisCo-PFGM++alsoimprovesoverthebaseline
WetestDisCo-Diffalsoonmoleculardocking,afundamen-
version(seeTable1). MoresamplesinApp.G.
tal task in drug discovery that consists of generating the
Ablations and Analyses. Table 3 shows that employing 3D structure with which a small molecule will bind to a
moderate classifier-free guidance with respect to the dis- protein. WebuildontopofDiffDock(Corsoetal.,2023),
cretelatents(scalecfg=1)enhancestheFIDscore(studied aDMthatrecentlyachievedstate-of-the-artperformance,
usingODEsampler),implyingthatthediscretelatentsef- integratingdiscretelatentvariables(seeSec.3andApp.E.4
fectivelylearnmodessimilartotheroleofclasslabelsand fordetails). Forcomputationalreasons,weusethereduced
text. Wefurthersubstitutedthediscretelatentswith1000- DiffDock‚Äôsarchitecture(referredtoasDiffDock-S)from
dim. continuouslatents(1000tooffercapacityatleastas Corsoetal.(2024),which,althoughlessaccurate,ismuch
highaswiththem=10andk=100discretelatents),using fasterfortrainingandinference.Fortrainingandevaluation,
Kullback-Leibler divergence-based (KLD) regularization wefollowthestandardfromSta¬®rketal.(2022)usingthe
as in VAEs to control the information retained. For fair PDBBinddataset(Liuetal.,2017)(seeApp.E.5).
comparison,wetrainedaDiT-basedDM(Peebles&Xie,
Results. Table 4 reports performance of our (DisCo-
2023)onthecontinuouslatentsusingthesameTransformer
DiffDock-S)andrelevantbaselinemethods. Weseethatalso
architectureasinDisCo-Diff‚Äôsauto-regressivemodel. Ta-
inthisdomaindiscretelatentsprovideimprovements,with
ble3showsthatwithalowKLDweight(0.1),thecontin-
thesuccessrateonthefulldatasetincreasingfrom32.9%
uouslatentsareunder-regularized,challengingtheDiTin
to35.4%andfrom13.9%to18.5%whenconsideringonly
modeling the complex encoding distribution and leading
testcomplexeswithunseenproteins. Thisimprovementis
toasignificantgapbetweenoracleFID(latentspredicted
8DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
1.0
Diffusion Model
DisCo-Diff
0.8
0.6 1.0
0.4 0.5
0.2
0.01 1.0
0 10 20 30 40 50
t
Figure7.Left:Lossversustime.Right:ImpactofdiscretelatentswitchingduringtheiterativesamplingprocessofDisCo-Diff‚Äôsdiffusion
modelcomponent.Thenumbersrepresentthepercentageofthetotalsamplingsteps.Theblue/greenarrowsmeanthesamplingstepsthat
utilizethediscretelatentassociatedwiththeleftmost/rightmostgridinthefigure.Forthemiddletwoimages,theprocessinvolvesinitially
employingthediscretelatentfromtheleftmostgridforacertainproportionofthetotalsamplingsteps(e.g.,75%),beforetransitioningto
thediscretelatentfromtherightmostgridtocompletetheremainingsteps(e.g.,thelast25%ofthetotalsamplingsteps).
particularlystrongonthehardercomponentofthetestset, 5.Conclusions
wherethebaselinemodelis,likely,highlyuncertain. This
We have proposed Discrete-Continuous Latent Variable
supportstheintuitionthatDisCo-Diffboostsperformance
DiffusionModels(DisCo-Diff),anovelanduniversalframe-
by more appropriately modeling discrete and continuous
workforcombiningdiscretelatentvariableswithcontinuous
variationsinthedata. InFig.8,wevisualizetwoexamples
DMs. The approach significantly boosts performance by
fromthetestsetwhichhighlighthowthemodellearnsto
simplifyingtheDM‚Äôsdenoisingtaskthroughthehelpofaux-
associatedistinctsetsofposeswithdifferentlatents,decom-
iliarydiscretelatentvariables,whileintroducingnegligible
posingthemultimodalcomponentsoftheposedistribution
overhead. Extensiveexperimentsandanalysesdemonstrate
fromthecontinuousvariationsthateachposecanhave.
theuniquebenefitsofglobaldiscretelatentvariablesthat
are learnt end-to-end with the denoiser. DisCo-Diff does
Table4.MoleculardockingperformanceonPDBBind.Foreach
method,wereportthepercentageoftop-1predictionswithin2AÀö notrelyonanypre-trainedencodernetworks. Assuch,we
validatedourmethodnotonlyonimagesynthesis,butalso
ofthegroundtruthforthefulltestsetandthesubsetrestrictedto
unseenproteins.Runtimeinseconds(*referstorunonCPU). formoleculardocking,demonstratingitsuniversality.
Full Unseen Runtime LimitationsandFutureWork. Thereareseveralpotential
GNINA(McNuttetal.,2021) 22.9 14.0 127 futuredirectionsandlimitationsinboththeexperimentsand
SMINA(Koesetal.,2013) 18.7 14.0 126*
designofDisCo-Diff. First,ourexperimentshavebeenpri-
GLIDE(Halgrenetal.,2004) 21.8 19.6 1405*
EquiBind(Sta¬®rketal.,2022) 5.5 0.7 0.04 marilyfocusedonstandardbenchmarkssuchasImageNet.
TankBind(Luetal.,2022) 20.4 6.3 0.7 Withmorecomputeresources,DisCo-Diffcouldbefurther
DiffDock-S(Corsoetal.,2024) 32.9 13.9 8.1 validatedontaskssuchastext-to-imagegeneration,where
DisCo-DiffDock-S(ours) 35.4 18.5 9.1 wewouldexpectdiscretelatentvariablestooffercomple-
DiffDock(Corsoetal.,2023) 38.2 20.8 40 mentary benefits to the text conditioning, similar to how
discretelatentsboostperformanceinourclass-conditional
experiments. Secondly,theGroupHierarchicalmodelrelies
ùëß ùëßÃÇ on inductive biases in its architecture, such as the differ-
entimagecharacteristicscapturedatdifferentresolutions
intheU-Net. Itwouldbeinterestingtoexplorehowsuch
architecturescouldbeconstructedandsimilarhierarchical
effectscouldbeachievedwhenworkingwithdifferentdata
True pose ~ 0.9 √Ö RMSD ~ 6.9 √Ö RMSD modalities(molecules,etc.). Thirdly,onecouldapplythe
ùëß ùëßÃÇ ideaofDisCo-Difftoothercontinuousflowmodels,suchas
flow-matching(Lipmanetal.,2022)orrectifiedflow(Liu
etal.,2022), tofurtherboosttheirperformance. Concep-
tually,duetothecloserelationbetweendiffusionmodels
True pose ~ 1.2 √Ö RMSD ~ 6.0 √Ö RMSD
and flow matching, we expect discrete latents to behave
Figure8.Examplesofalternativedockingposesmodeledwhen similarlythereandimproveperformance. Finally,thecur-
conditioningondifferentdiscretelatents,the‚Äúcorrect‚Äùz(i.e.same rentDisCo-Diffframeworkleveragesatwo-stagetraining
astheencoder)andanincorrectzÀÜ. TheDMmapsthemtotwo process. Initially,wejointlytrainthedenoiserandtheen-
distinctsetsoforientationswithwhichtheligandcouldfitinthe coder, followedbythepost-hocauto-regressivemodelin
pocket. Notably,thecorrectlatentcorrespondstoposeswithin
thesecondstage. Futureworkcouldinvestigatecombining
2AÀö ofthegroundtruth. Thecoloredbeadsaresetontheatoms
thetwo-stagetrainingintoaseamlessend-to-endfashion.
correspondingtothefirstlatentvariable.
9
)t(ssolDisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
ImpactStatement Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,
S.W.,Fidler,S.,andKreis,K. AlignyourLatents: High-
Deep generative modeling is a burgeoning research field
ResolutionVideoSynthesiswithLatentDiffusionMod-
with widespread implications for science and society.
els. In Proceedings of the IEEE/CVF Conference on
Our model DisCo-Diff advances the modeling power of
ComputerVisionandPatternRecognition(CVPR),2023.
diffusion models for data generation. While enhancing
datagenerationcapabilities,notablyinhigh-qualityimage Brock,A.,Donahue,J.,andSimonyan,K. Largescalegan
and video creation, these models also present challenges, trainingforhighfidelitynaturalimagesynthesis. ArXiv,
such as the potential misuse in deepfake technology abs/1809.11096,2018.
leading to social engineering concerns. Addressing
these issues necessitates further research into watermark Campbell, A., Benton, J., Bortoli, V. D., Rainforth, T.,
algorithms for diffusion models and collaboration with Deligiannidis, G., and Doucet, A. A continuous time
socio-technical disciplines to balance innovation with frameworkfordiscretedenoisingmodels. InAdvancesin
ethicalconsiderations. Wewouldalsoliketohighlightthe NeuralInformationProcessingSystems,2022.
promiseofdeepgenerativemodelslikeDisCo-Diffinthe
Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J.,
naturalsciences,asexemplifiedbyourmoleculardocking
Bojanowski, P., andJoulin, A. Emergingpropertiesin
experiments. Such models have the potential to provide
self-supervisedvisiontransformers.InProceedingsofthe
novelinsightsinto,forinstance,theinteractionsbetween
IEEE/CVFConferenceonComputerVisionandPattern
proteinsandligandsandadvancedrugdiscovery.
Recognition(CVPR),2021.
Acknowledgements Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., and
Romero-Soriano,A. Instance-conditionedgan. InNeural
YXandTJacknowledgesupportfromMIT-DSTASinga- InformationProcessingSystems,2021.
porecollaboration;YX,GC,andTJfromNSFExpeditions
grant(award1918839)‚ÄúUnderstandingtheWorldThrough Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,
Code‚Äù,fromMIT-IBMGrandChallengeproject,NSFExpe- W.T. Maskgit: Maskedgenerativeimagetransformer. In
ditionsgrant(award1918839: CollaborativeResearch: Un- ProceedingsoftheIEEE/CVFConferenceonComputer
derstandingtheWorldThroughCode),MachineLearning VisionandPatternRecognition(CVPR),2022.
forPharmaceuticalDiscoveryandSynthesis(MLPDS)con-
Chang,H.,Zhang,H.,Barber,J.,Maschinot,A.,Lezama,
sortium;GCandTJfurtheracknowledgesupportfromthe
J.,Jiang,L.,Yang,M.-H.,Murphy,K.P.,Freeman,W.T.,
AbdulLatifJameelClinicforMachineLearninginHealth
Rubinstein, M., Li, Y., and Krishnan, D. Muse: Text-
and the DTRA Discovery of Medical Countermeasures
to-imagegenerationviamaskedgenerativetransformers.
AgainstNewandEmerging(DOMANE)threatsprogram.
InProceedingsofthe40thInternationalConferenceon
MachineLearning(ICML),2023.
References
Corso,G.,Sta¬®rk,H.,Jing,B.,Barzilay,R.,andJaakkola,T.
Austin,J.,Johnson,D.D.,Ho,J.,Tarlow,D.,andvanden
Diffdock: Diffusionsteps,twists,andturnsformolecular
Berg, R. Structured denoising diffusion models in dis-
docking. InternationalConferenceonLearningRepre-
crete state-spaces. In Advances in Neural Information
sentations(ICLR),2023.
ProcessingSystems,2021.
Corso,G.,Deng,A.,Polizzi,N.,Barzilay,R.,andJaakkola,
Bahmani, S., Skorokhodov, I., Rong, V., Wetzstein, G., T. Thediscoveryofbindingmodesrequiresrethinking
Guibas,L.,Wonka,P.,Tulyakov,S.,Park,J.J.,Tagliasac- dockinggeneralization. InInternationalConferenceon
chi,A.,andLindell,D.B. 4d-fy: Text-to-4dgeneration LearningRepresentations,2024.
usinghybridscoredistillationsampling. arXivpreprint
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,
arXiv:2311.17984,2023.
L. ImageNet: Alarge-scalehierarchicalimagedatabase.
In Computer Vision and Pattern Recognition (CVPR),
Balaji,Y.,Nah,S.,Huang,X.,Vahdat,A.,Song,J.,Zhang,
2009.
Q.,Kreis,K.,Aittala,M.,Aila,T.,Laine,S.,Catanzaro,
B., Karras, T., and Liu, M.-Y. eDiff-I: Text-to-Image
Dhariwal, P. and Nichol, A. Q. Diffusion Models Beat
Diffusion Models with Ensemble of Expert Denoisers.
GANsonImageSynthesis. InAdvancesinNeuralInfor-
arXivpreprintarXiv:2211.01324,2022.
mationProcessingSystems,2021.
Bao, F., Li, C., Sun, J., and Zhu, J. Why are conditional Dockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher-
generativemodelsbetterthanunconditionalones? arXiv orderdenoisingdiffusionsolvers. InAdvancesinNeural
preprintarXiv:2212.00362,2022. InformationProcessingSystems(NeurIPS),2022a.
10DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Dockhorn,T.,Vahdat,A.,andKreis,K. Score-basedgener- Ho,J.andSalimans,T. Classifier-FreeDiffusionGuidance.
ativemodelingwithcritically-dampedlangevindiffusion. InNeurIPS2021WorkshoponDeepGenerativeModels
InInternationalConferenceonLearningRepresentations andDownstreamApplications,2021.
(ICLR),2022b.
Ho,J.,Jain,A.,andAbbeel,P. DenoisingDiffusionProb-
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, abilistic Models. In Advances in Neural Information
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, ProcessingSystems(NeurIPS),2020.
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
N. An image is worth 16x16 words: Transformers for Ho,J.,Saharia,C.,Chan,W.,Fleet,D.J.,Norouzi,M.,and
imagerecognitionatscale. InInternationalConference Salimans,T. Cascadeddiffusionmodelsforhighfidelity
onLearningRepresentations,2021. imagegeneration. J.Mach.Learn.Res.,23:47:1‚Äì47:33,
2021.
Esser,P.,Rombach,R.,andOmmer,B.Tamingtransformers
forhigh-resolutionimagesynthesis.InProceedingsofthe Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Grit-
IEEE/CVFConferenceonComputerVisionandPattern senko,A.,Kingma,D.P.,Poole,B.,Norouzi,M.,Fleet,
Recognition(CVPR),2021. D.J.,andSalimans,T. ImagenVideo: HighDefinition
VideoGenerationwithDiffusionModels. arXivpreprint
Ge,S.,Nah,S.,Liu,G.,Poon,T.,Tao,A.,Catanzaro,B.,Ja-
arXiv:2210.02303,2022.
cobs,D.,Huang,J.-B.,Liu,M.-Y.,andBalaji,Y.Preserve
YourOwnCorrelation:ANoisePriorforVideoDiffusion Hoogeboom,E.,Heek,J.,andSalimans,T. SimpleDiffu-
Models. InProceedingsoftheIEEE/CVFInternational sion: End-to-EndDiffusionforHighResolutionImages.
ConferenceonComputerVision(ICCV),2023. InProceedingsofthe40thInternationalConferenceon
MachineLearning(ICML),2023.
Geiger,M.andSmidt,T. e3nn: Euclideanneuralnetworks.
arXivpreprintarXiv:2207.09453,2022. Hu,V.T.,Zhang,D.W.,Asano,Y.M.,Burghouts,G.J.,and
Snoek,C.G.M. Self-guideddiffusionmodels. Proceed-
Guo,H.,Liu,S.,Mingdi,H.,Lou,Y.,andJing,B. Diffdock-
ingsoftheIEEE/CVFConferenceonComputerVision
site: Anovelparadigmforenhancedprotein-ligandpre-
andPatternRecognition(CVPR),2023.
dictionsthroughbindingsiteidentification. InNeurIPS
2023 Generative AI and Biology (GenBio) Workshop,
Huang,C.-W.,Lim,J.H.,andCourville,A. Avariational
2023.
perspective on diffusion-based generative models and
scorematching. InNeuralInformationProcessingSys-
Halgren,T.A.,Murphy,R.B.,Friesner,R.A.,Beard,H.S.,
tems(NeurIPS),2021.
Frye, L. L., Pollard, W. T., and Banks, J. L. Glide: a
new approach for rapid, accurate docking and scoring.
Hyva¬®rinen, A. Estimation of non-normalized statistical
2.enrichmentfactorsindatabasescreening. Journalof
modelsbyscorematching. JournalofMachineLearning
medicinalchemistry,2004.
Research,6:695‚Äì709,2005. ISSN1532-4435.
Harvey,W.andWood,F. Visualchain-of-thoughtdiffusion
Ingraham,J.,Baranov,M.,Costello,Z.,Frappier,V.,Ismail,
models. arXivpreprintarXiv:2303.16187,2023.
A., Tie, S., Wang, W., Xue, V., Obermeyer, F., Beam,
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- A., andGrigoryan, G. Illuminatingproteinspacewith
mentumcontrastforunsupervisedvisualrepresentation a programmable generative model. Nature, 623:1070‚Äì
learning. InProceedingsoftheIEEE/CVFConferenceon 1078,2023.
ComputerVisionandPatternRecognition(CVPR),2020.
Jabri,A.,Fleet,D.J.,andChen,T. Scalableadaptivecom-
Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and putationforiterativegeneration. InInternationalConfer-
Hochreiter,S. Ganstrainedbyatwotime-scaleupdate enceonMachineLearning,2022.
ruleconvergetoalocalnashequilibrium. InAdvancesin
neuralinformationprocessingsystems,2017. Jang, E., Gu, S. S., and Poole, B. Categorical reparame-
terizationwithgumbel-softmax. ArXiv,abs/1611.01144,
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., 2016.
Botvinick, M., Mohamed, S., and Lerchner, A. beta-
VAE:Learningbasicvisualconceptswithaconstrained Karras, T., Laine, S., and Aila, T. A style-based genera-
variationalframework. InInternationalConferenceon torarchitectureforgenerativeadversarialnetworks. In
LearningRepresentations,2017. ProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),2019.
Hinton,G.E. Apracticalguidetotrainingrestrictedboltz-
mannmachines. InNeuralnetworks: Tricksofthetrade, Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen,
pp.599‚Äì619.Springer,2012. J., and Aila, T. Analyzing and improving the image
11DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
qualityofStyleGAN. InProceedingsoftheIEEE/CVF Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel,
ConferenceonComputerVisionandPatternRecognition M., and Le, M. Flow matching for generative mod-
(CVPR),2020. eling. ArXiv, abs/2210.02747, 2022. URL https:
//api.semanticscholar.org/CorpusID:
Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating 252734897.
the design space of diffusion-based generative models.
ArXiv,abs/2206.00364,2022. Liu,R.,Wu,R.,VanHoorick,B.,Tokmakov,P.,Zakharov,
S.,andVondrick,C.Zero-1-to-3:Zero-shotOneImageto
Ketata,M.A.,Laue,C.,Mammadov,R.,Sta¬®rk,H.,Wu,M., 3DObject.InProceedingsoftheIEEE/CVFInternational
Corso,G.,Marquet,C.,Barzilay,R.,andJaakkola,T.S. ConferenceonComputerVision(ICCV),2023.
Diffdock-pp: Rigidprotein-proteindockingwithdiffu-
sionmodels. arXivpreprintarXiv:2304.03889,2023. Liu, X., Gong, C., and Liu, Q. Flow straight and fast:
Learningtogenerateandtransferdatawithrectifiedflow.
Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., arXivpreprintarXiv:2209.03003,2022.
Uesaka,T.,He,Y.,Mitsufuji,Y.,andErmon,S. Consis-
tencytrajectorymodels: Learningprobabilityflowode Liu,Z.,Su,M.,Han,L.,Liu,J.,Yang,Q.,Li,Y.,andWang,
trajectoryofdiffusion. ArXiv,abs/2310.02279,2023a. R. Forgingthebasisfordevelopingprotein‚Äìligandinter-
actionscoringfunctions. AccountsofChemicalResearch,
Kim,S.W.,Brown,B.,Yin,K.,Kreis,K.,Schwarz,K.,Li, 2017.
D.,Rombach,R.,Torralba,A.,andFidler,S.NeuralField-
Lu, W., Wu, Q., Zhang, J., Rao, J., Li, C., and Zheng, S.
LDM:SceneGenerationwithHierarchicalLatentDiffu-
Tankbind: Trigonometry-awareneuralnetworksfordrug-
sion Models. In Proceedings of the IEEE/CVF Con-
proteinbindingstructureprediction. Advancesinneural
ference on Computer Vision and Pattern Recognition
informationprocessingsystems,2022.
(CVPR),2023b.
Lyu,S. Interpretationandgeneralizationofscorematching.
Kingma,D.P.andGao,R. Understandingdiffusionobjec-
InProceedingsoftheTwenty-FifthConferenceonUncer-
tives as the ELBO with simple data augmentation. In
tainty in Artificial Intelligence, UAI ‚Äô09, pp. 359‚Äì366,
Thirty-seventh Conference on Neural Information Pro-
Arlington,Virginia,USA,2009.AUAIPress.
cessingSystems,2023.
Masters,M.R.,Mahmoud,A.H.,andLill,M.A. Fusion-
Kingma,D.P.andWelling,M. Auto-encodingvariational
dock: Physics-informeddiffusionmodelformolecular
bayes. In The International Conference on Learning
docking. ICML Workshop on Computational Biology,
Representations,2014.
2023.
Kingma, D.P., Salimans, T., Poole, B., andHo, J. Varia-
McNutt, A. T., Francoeur, P., Aggarwal, R., Masuda, T.,
tionaldiffusionmodels. InAdvancesinNeuralInforma-
Meli,R.,Ragoza,M.,Sunseri,J.,andKoes,D.R. Gnina
tionProcessingSystems,2021.
1.0: moleculardockingwithdeeplearning. Journalof
cheminformatics,2021.
Koes, D. R., Baumgartner, M. P., and Camacho, C. J.
Lessons learned in empirical scoring with smina from
Nichol,A.andDhariwal,P. Improveddenoisingdiffusion
thecsar2011benchmarkingexercise. Journalofchemi-
probabilisticmodels. ArXiv,abs/2102.09672,2021.
calinformationandmodeling,2013.
Nichol,A.,Jun,H.,Dhariwal,P.,Mishkin,P.,andChen,M.
Kriva¬¥k,R.andHoksza,D. P2rank: machinelearningbased Point-E:ASystemforGenerating3DPointCloudsfrom
toolforrapidandaccuratepredictionofligandbinding ComplexPrompts,2022.
sitesfromproteinstructure. Journalofcheminformatics,
10:1‚Äì12,2018. Peebles, W. and Xie, S. Scalable diffusion models with
transformers. InProceedingsoftheIEEE/CVFInterna-
Landrum,G. Rdkitdocumentation. Release,2013. tionalConferenceonComputerVision,pp.4195‚Äì4205,
2023.
Li,T.,Katabi,D.,andHe,K. Self-conditionedimagegen-
eration via generating representations. arXiv preprint Pernias, P., Rampas, D., Richter, M. L., Pal, C. J., and
arXiv:2312.03701,2023. Aubreville,M. Wuerstchen: Anefficientarchitecturefor
large-scaletext-to-imagediffusionmodels.arXivpreprint
Ling, H., Kim, S. W., Torralba, A., Fidler, S., and Kreis, arXiv:2306.00637,2023.
K. Align your gaussians: Text-to-4d with dynamic 3d
gaussiansandcomposeddiffusionmodels.arXivpreprint Plainer, M., Toth, M., Dobers, S., Sta¬®rk, H., Corso, G.,
arXiv:2312.13763,2023. Marquet,C.,andBarzilay,R. Diffdock-pocket:Diffusion
12DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
for pocket-level docking with sidechain flexibility. In Sauer,A.,Schwarz,K.,andGeiger,A. Stylegan-xl: Scaling
NeurIPS2023WorkshoponNewFrontiersofAIforDrug stylegantolargediversedatasets. ACMSIGGRAPH2022
DiscoveryandDevelopment,2023. ConferenceProceedings,2022.
Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn, Schu¬®tt,K.,Kindermans,P.-J.,SaucedaFelix,H.E.,Chmiela,
T.,Mu¬®ller,J.,Penna,J.,andRombach,R.SDXL:Improv- S., Tkatchenko, A., and Mu¬®ller, K.-R. Schnet: A
ingLatentDiffusionModelsforHigh-ResolutionImage continuous-filterconvolutionalneuralnetworkformodel-
Synthesis. arXivpreprintarXiv:2307.01952,2023. ingquantuminteractions.Advancesinneuralinformation
processingsystems,30,2017.
Poole,B.,Jain,A.,Barron,J.T.,andMildenhall,B. Dream-
Fusion: Text-to-3Dusing2DDiffusion. InTheEleventh Schwarz, K., Kim, S. W., Gao, J., Fidler, S., Geiger, A.,
InternationalConferenceonLearningRepresentations and Kreis, K. WildFusion: Learning 3D-Aware La-
(ICLR),2023. tent Diffusion Models in View Space. arXiv preprint
arXiv:2311.13570,2023.
Preechakul,K.,Chatthee,N.,Wizadwongsa,S.,andSuwa-
janakorn,S. Diffusionautoencoders: Towardameaning- Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,
fulanddecodablerepresentation. InProceedingsofthe S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh,
IEEE/CVFConferenceonComputerVisionandPattern D., Gupta, S., and Taigman, Y. Make-A-Video: Text-
Recognition(CVPR),2022. to-Video Generation without Text-Video Data. In The
Eleventh International Conference on Learning Repre-
Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,
sentations(ICLR),2023a.
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J.,Krueger,G.,andSutskever,I. LearningTransferable
Singer,U.,Sheynin,S.,Polyak,A.,Ashual,O.,Makarov,I.,
VisualModelsFromNaturalLanguageSupervision. In
Kokkinos,F.,Goyal,N.,Vedaldi,A.,Parikh,D.,Johnson,
Proceedingsofthe38thInternationalConferenceonMa-
J.,andTaigman,Y. Text-to-4DDynamicSceneGenera-
chineLearning(ICML),2021.
tion. InProceedingsofthe40thInternationalConference
onMachineLearning,2023b.
Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,Rad-
ford,A.,Chen,M.,andSutskever,I. Zero-shottext-to-
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
image generation. In Proceedings of the 38th Interna-
Ganguli,S. DeepUnsupervisedLearningusingNonequi-
tionalConferenceonMachineLearning(ICML),2021.
libriumThermodynamics. InInternationalConference
onMachineLearning(ICML),2015.
Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen,
M. HierarchicalText-ConditionalImageGenerationwith
Song,Y.andErmon,S. Generativemodelingbyestimat-
CLIPLatents. arXivpreprintarXiv:2204.06125,2022.
ing gradients of the data distribution. In Proceedings
of the 33rd Annual Conference on Neural Information
Rezende,D.J.,Mohamed,S.,andWierstra,D. Stochastic
ProcessingSystems,2019.
backpropagationandapproximateinferenceindeepgen-
erativemodels. InInternationalConferenceonMachine
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
Learning,2014.
mon,S.,andPoole,B.Score-BasedGenerativeModeling
Rolfe,J.T. Discretevariationalautoencoders. InInterna- through Stochastic Differential Equations. In Interna-
tionalConferenceonLearningRepresentations,2017. tionalConferenceonLearningRepresentations(ICLR),
2021.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer,B. High-ResolutionImageSynthesiswithLa- Sta¬®rk, H., Ganea, O., Pattanaik, L., Barzilay, R., and
tentDiffusionModels. InProceedingsoftheIEEE/CVF Jaakkola,T. Equibind: Geometricdeeplearningfordrug
ConferenceonComputerVisionandPatternRecognition bindingstructureprediction. InInternationalConference
(CVPR),2022. onMachineLearning,2022.
Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Den- Trott,O.andOlson,A.J. Autodockvina: improvingthe
ton,E.,Ghasemipour,S.K.S.,Gontijo-Lopes,R.,Ayan, speedandaccuracyofdockingwithanewscoringfunc-
B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, tion,efficientoptimization,andmultithreading. Journal
M. PhotorealisticText-to-ImageDiffusionModelswith ofcomputationalchemistry,2010.
DeepLanguageUnderstanding. InAdvancesinNeural
Vahdat, A., Andriyash, E., and Macready, W. G. Dvae#:
InformationProcessingSystems(NeurIPS),2022.
DiscretevariationalautoencoderswithrelaxedBoltzmann
Salakhutdinov,R.andHinton,G.Deepboltzmannmachines. priors. In Advances in Neural Information Processing
InArtificialintelligenceandstatistics,2009. Systems(NeurIPS),2018a.
13DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Vahdat,A.,Macready,W.,Bian,Z.,Khoshaman,A.,and withapplicationtoproteinbackbonegeneration. arXiv
Andriyash, E. DVAE++: Discrete variational autoen- preprintarXiv:2302.02277,2023.
coderswithoverlappingtransformations. InProceedings
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
ofthe35thInternationalConferenceonMachineLearn-
Vasudevan,V.,Ku,A.,Yang,Y.,Ayan,B.K.,Hutchinson,
ing(ICML),2018b.
B.,Han,W.,Parekh,Z.,Li,X.,Zhang,H.,Baldridge,J.,
Vahdat,A.,Kreis,K.,andKautz,J. Score-basedGenera- andWu, Y. Scalingautoregressivemodelsforcontent-
tive Modeling in Latent Space. In Neural Information richtext-to-imagegeneration. TransactionsonMachine
ProcessingSystems(NeurIPS),2021. LearningResearch(TMLR),2022.
vandenOord,A.,Vinyals,O.,andkavukcuoglu,k. Neural Zeng,X.,Vahdat,A.,Williams,F.,Gojcic,Z.,Litany,O.,
discreterepresentationlearning. InAdvancesinNeural Fidler,S.,andKreis,K. LION:LatentPointDiffusion
InformationProcessingSystems(NeurIPS),2017. Modelsfor3DShapeGeneration. InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2022.
Vaswani, A., Shazeer, N. M., Parmar, N., Uszkoreit, J.,
Jones,L.,Gomez,A.N.,Kaiser,L.,andPolosukhin,I. Zheng,Y.,Li,X.,Nagano,K.,Liu,S.,Kreis,K.,Hilliges,
Attentionisallyouneed. InNeuralInformationProcess- O., and Mello, S. D. A unified approach for text-
ingSystems,2017. andimage-guided4dscenegeneration. arXivpreprint
arXiv:2311.16854,2023.
Vincent,P. Aconnectionbetweenscorematchingandde-
noisingautoencoders. NeuralComputation,23(7):1661‚Äì
1674,2011.
Wang,Y.,Schiff,Y.,Gokaslan,A.,Pan,W.,Wang,F.,DeSa,
C.,andKuleshov,V. InfoDiffusion:Representationlearn-
ingusinginformationmaximizingdiffusionmodels. In
Proceedingsofthe40thInternationalConferenceonMa-
chineLearning(ICML),2023.
Watson,J.L.,Juergens,D.,Bennett,N.R.,Trippe,B.L.,
Yim,J.,Eisenach,H.E.,Ahern,W.,Borst,A.J.,Ragotte,
R.J.,Milles,L.F.,Wicky,B.I.M.,Hanikel,N.,Pellock,
S.J.,Courbet,A.,Sheffler,W.,Wang,J.,Venkatesh,P.,
Sappington, I., Torres, S.V., Lauko, A., Bortoli, V.D.,
Mathieu, E., Barzilay, R., Jaakkola, T. S., DiMaio, F.,
Baek, M., and Baker, D. De novo design of protein
structure and function with rfdiffusion. Nature, 620:
1089‚Äì1100,2023.
Xu,Y.,Liu,Z.,Tegmark,M.,andJaakkola,T. Poissonflow
generativemodels. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),2022.
Xu,Y.,Deng,M.,Cheng,X.,Tian,Y.,Liu,Z.,andJaakkola,
T. Restartsamplingforimprovinggenerativeprocesses.
ArXiv,abs/2306.14878,2023a.
Xu, Y., Liu, Z., Tian, Y., Tong, S., Tegmark, M., and
Jaakkola, T. PFGM++: Unlocking the potential of
physics-inspiredgenerativemodels. InProceedingsof
the40thInternationalConferenceonMachineLearning
(ICML),2023b.
Xu, Y., Tong, S., and Jaakkola, T. Stable target field for
reduced variance score estimation in diffusion models.
ArXiv,abs/2302.00670,2023c.
Yim,J.,Trippe,B.L.,Bortoli,V.D.,Mathieu,E.,Doucet,
A.,Barzilay,R.,andJaakkola,T. Se(3)diffusionmodel
14DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Appendix
A RelatedWork 15
B DiscreteLatentVariableClassifier-FreeGuidance 16
C AlgorithmPseudocode 17
D ImageNetExperiments 17
D.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
D.2 TrainingandSampling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E MolecularDocking 19
E.1 TaskOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.3 LatentVariables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.4 ArchitectureDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
E.5 ExperimentalDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
F GaussianMixtureExperiments 21
F.1 DataGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.2 TrainingandSampling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
F.3 Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
G AdditionalSamplesandExperiments 22
G.1 LossAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.2 Class-conditonedImageNet-64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.3 Class-conditonedImageNet-128. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.4 GroupHierarchicalDisCo-Diff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
G.5 OverfittingandEncoderCollapseinContinuousLatent. . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A.RelatedWork
OurworkbuildsonDMs(Sohl-Dicksteinetal.,2015;Hoetal.,2020;Songetal.,2021;Karrasetal.,2022),whichhave
beenwidelyusednotonlyforimagegeneration(Dhariwal&Nichol,2021;Nichol&Dhariwal,2021;Rombachetal.,2022;
Dockhornetal.,2022b;a;Sahariaetal.,2022;Rameshetal.,2022;Podelletal.,2023),butalsoforvideo(Blattmannetal.,
2023;Singeretal.,2023a;Hoetal.,2022;Geetal.,2023),3D(Nicholetal.,2022;Zengetal.,2022;Kimetal.,2023b;
Pooleetal.,2023;Schwarzetal.,2023;Liuetal.,2023)and4D(Singeretal.,2023b;Lingetal.,2023;Bahmanietal.,
2023;Zhengetal.,2023)synthesis,aswellasinvariousotherdomains,including,forinstance,moleculardockingand
proteindesign(Corsoetal.,2023;Yimetal.,2023;Ingrahametal.,2023;Watsonetal.,2023).
IntheDMliterature,latentvariableshavebeenmostpopularaspartoflatentdiffusionmodels,whereaDMistrained
inacompressed, usuallycontinuous, latentspace(Rombachetal.,2022;Vahdatetal.,2021). Incontrast, DisCo-Diff
leveragesdiscretelatentvariablesandusesthemtoaugmentaDM.Thefirstmodelsusingdiscretelatentvariablesfor
high-dimensionalgenerativemodelingtasksincludeBoltzmannmachines(Salakhutdinov&Hinton,2009;Hinton,2012)
andearlydiscretevariationalautoencoders(Rolfe,2017;Vahdatetal.,2018a;b). Morerecently,avarietyofworksencode
imagesintolarge2Dspatialgridsofdiscretetokenswithvectorquantizationorsimilartechniques(vandenOordetal.,
15DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
2017;Esseretal.,2021;Rameshetal.,2021;Changetal.,2022;Yuetal.,2022;Perniasetal.,2023;Changetal.,2023). As
discussed,thesemodelstypicallyrequireaverylargenumberoftokensandrelyonlargecodebooks,whichmakesmodeling
theirdistributionchallenging. DisCo-Diff,incontrast,leveragesonlyfewdiscretelatentswithsmallcodebooksthatactin
harmonywiththeadditionalcontinuousDM.
TherearepreviousrelatedworksthatalsoconditionDMsonauxiliaryencodings. Preechakuletal.(2022)augmentDMs
withnon-spatiallatentvariables,buttheirlatentsarecontinuousandhigh-dimensional,whichmakestrainingtheirlatent
DMmorechallenging. Thisispreciselywhatweavoidbyinsteadusinglow-dimensionalanddiscretelatents. Moreover,
theyfocusonsemanticfaceimagemanipulation,nothigh-qualitysynthesisforchallenging,diversedatasets.
Harvey&Wood(2023)usetherepresentationsofapre-trainedCLIPimageencoder(Radfordetal.,2021)forconditioning
aDMandlearnanotherDMovertheCLIPembeddingsforsampling. Similarly,Baoetal.(2022)andHuetal.(2023)
use clustered MoCo-based (He et al., 2020) and clustered DINO-based (Caron et al., 2021) features, respectively, for
conditioning. Hence,thesethreeapproachesarestrictlylimitedtoimagesynthesis,wheresuchencoders,pre-trainedon
large-scaledatasets,areavailable. Incontrast,wepurposefullyavoidtheuseofpre-trainednetworksandlearnthediscrete
latentsjointlywiththeDM,makingourframeworkuniversallyapplicable. AnotherrelatedworkisInfoDiffusion(Wang
etal.,2023),whichalsoconditionsDMsondiscretelatentvariables. However,contrarytoDisCo-Diff,thisworkfocuseson
learningdisentangledrepresentations,similartoŒ≤-VAEs(Higginsetal.,2017),primarilyforlow-resolutionfacesynthesis.
Itusesamutualinformation-basedobjectiveanddoesnotfocusondiverseandhigh-qualitysynthesisofcomplexdatasuch
asImageNet.
In contrast to the above works, we show how discrete latent variables boost generative performance itself and we
significantlyoutperformtheseworksincomplexanddiversehigh-qualitysynthesis. Furthermore,wemotivateDisCo-Diff
fundamentally,withreducedODEcurvatureandmodelcomplexity,providinganewandcomplementaryperspective.
Inthemoleculardockingliterature,sinceDiffDock(Corsoetal.,2023)introducedtheuseofdiffusionmodelsinthetask,a
numberofworkshaveproposeddifferentmodificationstoitsframework. Inparticular,some(Mastersetal.,2023;Plainer
etal.,2023;Guoetal.,2023)haveproposedtoseparatetheblinddockingtaskbetweenpocketidentification(i.e. identifying
theregionoftheproteinwherethesmallmoleculewouldbind)andposeprediction(i.e. predictingthespecificposewith
whichtheligandwouldbindtotheprotein),aspreviouslydoneinmanytraditionalapproaches(Kriva¬¥k&Hoksza,2018).
Onecouldseethisashand-craftinga(roughlydiscrete)latentvariableinthepocketandusingittodecomposethetask. By
allowingtheencodertolearnarbitrarydiscretelatentsthroughitsinteractionwiththedenoiser,DisCo-Difflargelyincludes
theabove-mentionedstrategyasaparticularcase.
B.DiscreteLatentVariableClassifier-FreeGuidance
Classifier-freeguidance(Ho&Salimans,2021)(cfg)isamode-seekingtechniquecommonlyusedindiffusionliterature,
suchasclass-conditionedgenreation(Peebles&Xie,2023)ortext-to-imagegeneration(Rombachetal.,2022). Itgenerally
guides the sampling trajectories toward higher-density regions. We can similarly apply classifier-free guidance in the
DisCo-Diff,wherewetreatthediscretelatentasconditionalinputs. Wefollowtheconventionin(Sahariaetal.,2022),
andtheclassifier-freeguidanceattimesteptisasfollows: DÀú (x,œÉ(t),z)=wD (x,œÉ(t),z)+(1‚àíw)D (x,œÉ(t),‚àÖ),
Œ∏ Œ∏ Œ∏
whereD (x,œÉ(t),z)/D (x,œÉ(t),‚àÖ)istheconditional/unconditionalmodels,sharingparameters. Wedropthediscrete
Œ∏ Œ∏
latentwithprobability0.1duringtraining,totraintheunconditionalmodelD (x,œÉ(t),‚àÖ). Amildwwouldusuallyleadto
Œ∏
improvementinsamplediversity(Peebles&Xie,2023). Table3demonstratesthatusingamoderateguidancescalew=1
(weusew =1andcfg=1interchangeablyinthepaper)improvestheFIDscore,suggestingthatthelearneddiscretelatentin
theDisCo-Diffframeworkhasstrongindicationsofmodeofdatadistribution. Wefurtherexplorevaryingtheguidance
scaleonImageNet-128. AsshowninFig9,increasingtheclassifier-freeguidancescalewwouldstrengthentheeffectof
guidance.
16DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
(a) cfg=0 (b) cfg=1 (c) cfg=4 (d) cfg=8
Figure9.GeneratedsamplesinDisCo-Diffwithacfgscalerangingfrom0to8,undertheclasslabel‚Äúmalamute‚ÄùonImageNet-128.
C.AlgorithmPseudocode
Weprovidealgorithmpseudocodeforthetraininginthefirststagefordenoiserandencoder(Alg1)andthesecondstagefor
auto-regressivemodel(Alg2)forclarity. WealsoincludethepseudocodeforsamplinginAlg3. Notethatwegeneralizethe
equationsinthemaintextbyconsideringtheconditionalgenerationwithconditionc.
Algorithm1Mini-batchtrainingofdenoiserandencoderinDisCo-Diff
1: Input:DenoiserD Œ∏,encoderE œï,trainingdatasetD,batchsizeB,Gumbel-SoftmaxtemperatureœÑ,trainingiterationT
2: fori=0,...,T ‚àí1do
3: Samplemini-batchdata{(y i,c i)}B i=1fromD
4: Sampletimevariables{t i}B i=1fromp(t)andnoisevectors{n i ‚àºN(0,œÉ i2I)}B i=1
5: Getperturbeddata{yÀÜ i =y i+n i}B i=1
6: Samplethediscretelatentfromtheencoder{z i ‚àºE œï(y i)}B i=1usingGumbel-SoftmaxrelaxationwithtemperatureœÑ
7: Calculateloss‚Ñì(Œ∏,œï)=(cid:80)B i=1Œª(t)‚à•D Œ∏(yÀÜ i,t i,z i,c i)‚àíy i‚à•2
2
8: UpdatethenetworkparameterŒ∏andœïviaAdamoptimizer
9: endfor
Algorithm2Mini-batchtrainingofauto-regressivemodelinDisCo-Diff
1: Input: Auto-regressivemodelA œà, encoderE œï, trainingdatasetD, batchsizeB, Gumbel-SoftmaxtemperatureœÑ,
trainingiterationT
2: fori=0,...,T ‚àí1do
3: Samplemini-batchdata{(y i,c i)}B i=1fromD
4: Samplethediscretelatentfromtheencoder{z i ‚àºE œï(y i)}B i=1usingGumbel-SoftmaxrelaxationwithtemperatureœÑ
5:
Calculateloss‚Ñì(œà)=(cid:80)B i=1(cid:80)m
j
logp œà((z i) j|(z i) <j,c i)
6: UpdatethenetworkparameterœàviaAdamoptimizer
7: endfor
D.ImageNetExperiments
D.1.Architecture
Featuremaps Attentionresolution Encoderpatchsize
ImageNet-64 1-2-3-4(√ó192) (8,16,32) 8√ó8
ImageNet-128 1-2-4-16(√ó128) (16,32,64) 16√ó16
Table5. SpecificnetworkconfigurationsonImageNet
ForalltheImageNetexperiments, wefixthelatentdimensionm = 10inDisCo-Diff, andthecodebooksizeforeach
17DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Algorithm3SamplingprocedureofDisCo-Diff
1: Input: Auto-regressivemodelA œà,denoiserD œï,timediscretization{t i}n i=0,conditionc
2: /* Sample discrete latent from auto-regressive model */
3: fori=1,...,mdo
4: Samplez i ‚àºp œà(z i|z <i,c)
5: endfor
6: /* Diffusion ODE (Heun‚Äôs 2nd order method) */
7: Samplex 0 ‚àºN(0,t2 0I)
8: fori=0,...,n‚àí1do
9: d i =(x i‚àíD Œ∏(x i,t i,z,c))/t i
10: x i+1 =x i+(t i+1‚àít i)d i
11: ift i+1 Ã∏=0then
12: d‚Ä≤ i =(x i+1‚àíD Œ∏(x i+1,t i+1,z,c))/t i+1
13: x i+1 =x i+(t i+1‚àít i)( 21d i+ 1 2d‚Ä≤ i)
14: endif
15: endfor
discretelatentto1000. Belowweprovidearchitecturedetailsforthedenoisernetwork,encoder,andauto-regressivemodel.
Table 5 also lists some key network configurations. Please see the source code in the Supplementary Material for all
low-leveldetails.
Denoiser Neural Network. (1) ImageNet-64: We use the same UNet architecture in EDM (Karras et al., 2022) for
ImageNet-64,withnewlyinjectedcross-attentionlayersaftereachself-attentionlayersineachresidualblock. Wefeedthe
discretevectorintoasix-layerTransformerencoder(withlatentdimension192)toobtainthecorrespondingembeddingsfor
discretevariables. Theseembeddingsaretheninputintothecross-attentionlayers. (2)ImageNet-128: WeemploytheUViT
designinsimplediffusion(Hoogeboometal.,2023)andVDM++(Kingma&Gao,2023). UViTusesconvolutionallayers
fordown-/up-sampling,anda36-layerViTtoprocessthelowest-resolutionfeaturemapsinthebottleneck,tostrikeabetter
balancebetweenexpressivenessandcomputation. Sincetheauthorsdidn‚Äôtreleasethecodeandmodel,wereimplemented
thearchitecturebyourselves. WeempiricallyobservethattheconvolutionalblocksinEDMworkbetterthantheones
describedinthesimplediffusionpaper,sowecombinetheup-/down-samplingblocksinEDMwiththe36-layerViTatthe
bottlenecklayer. Wefurtherintroduceacross-attentionlayerfordiscretelatentineachup-/down-samplingblock,andevery
threeTransformerblocksintheViT(e.g.,12newcross-attentionlayersintheViT)tosavecomputation.
Encoder. Weutilizea12-layerstandardViT(Dosovitskiyetal.,2021)asthebackboneforencoder. Itslatentdimensionis
768andthenumberofattentionheadsis12. ThepatchsizeforImageNet-64is8√ó8andforImageNet-128is16√ó16. We
treateachofthemdiscretelatentsasaclassificationtoken,andconcatenatetheirembeddingswiththepathembeddings.
Auto-regressivemodel. WeuseastandardTransformerdecoder(Vaswanietal.,2017),withadepthof12,anumber
ofheadsof8,andalatentdimensionof512. Theinferencetimeoftheauto-regressivemodelismuchsmallerthanthe
iterativedenoisingprocess,giventhatthediscretelatentonlyhas10dimensions. Togenerate32imagesonImageNet-128,
theauto-regressivemodeltakes0.44seconds,whilethediffusionmodeltakes78secondsfor114NFE,withanaverageof
0.68s/NFEonasingleNVIDIAA100GPU.
D.2.TrainingandSampling
Weborrowthepreconditioningtechniques,trainingnoisyschedule,optimizers,exponentialmovingaverage(EMA)schedule,
andhyper-parametersfrompreviousstate-of-the-artdiffusionmodelEDM(Karrasetal.,2022)onImageNet-64. Weemploy
theshiftedEDM-monotonicnoisyscheduleproposedinVDM++(Kingma&Gao,2023)onImageNet-128,andkeepother
trainingdetailsthesameinImageNet-64. WeusetheGumbel-Softmax(Jangetal.,2016)asthecontinuousrelaxationfor
thediscretelatents. ThetemperatureœÑ inGumbel-Softmaxcontrolsthesmoothnessofthecategoricaldistribution. When
œÑ ‚Üí0,theexpectedvalueoftheGumbel-Softmaxisthesameastheoneoftheunderlyingpredicteddistribution. Aswe
increaset,theGumbel-Softmaxwouldgraduallyconvergetoauniformdistribution. Hence,arelativelylargeœÑ effectively
providesregualizationeffects. Duringtraining,wesettheœÑ toaconstant1. However,fortheextractionoflatentsfrom
trainingimages,whichaidsinconstructingthedatasetforthesecondstageoftheauto-regressivemodel,weadjustœÑ toa
18DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
lowervalueof0.01.
WeuseHeun‚Äôssecond-orderODEsolverasthedefaultODEsampler,whichisproveneffectiveinpreviousworks(Karras
etal.,2022;Xuetal.,2023b). Wedirectlyusethehyper-parametersforthe623NFEsettinginRestartsampler(Xuetal.,
2023a)onImageNet-64,forDisCo-Diff.
D.3.Evaluation
Fortheevaluation,wefollowthestandardprotocolandcomputetheFre¬¥chetdistancebetween50000generatedsamplesand
thetrainingimages.
E.MolecularDocking
Inthisappendix,wewillintroducethetaskofmoleculardockingandsomeoftheexistingapproachestotackleitforreaders
whoarenotfamiliarwiththisfield. ExperiencedreadersmayskiptoSectionE.3wherewestartdescribingthedetailsofour
dockingapproach.
E.1.TaskOverview
Moleculardockingconsistsoffindingthe3Dstructurethataprotein(alsoreferredtoasreceptor)andasmallmolecule(or
ligand)takewhenbinding. Thisisanimportanttaskindrugdesignbecausemostdrugsaresmallmoleculesthatoperate
bybindingtoaspecificproteinofinterestinourbodyandinhibitingorenhancingitsfunction. Thecommonparticular
instantiationofthedockingproblemthatweconsiderisalsoreferredtoasrigidblinddockingi.e. wherewearegiven
asinputthecorrectproteinstructure(rigid)butarenotprovidedanyinformationaboutwheretheligandwillbindonthe
proteinnortheconformationsitwilltake(blind).
Groundtruthdatafortrainingandtestingisobtainedthroughexperimentalmethods,likeX-raycrystallography,that,for
eachprotein-ligandcomplex,allowtoobserveaparticularposethatproteinandligandtookwhenbindingtogetherinsideof
thecrystal. Althoughtheremaybeotherposesthatthisparticularproteinandligandmaytakewhenbindinginanatural
environment,methodsareevaluatedbasedontheircapacitytoretrievethecrystalpose. Thisaccuracyistypicallycomputed
asthepercentageoftestcomplexeswherethepredictedstructureoftheligand(alsoreferredtoasligandpose)iswithina
rootmeansquaredistance(RMSD)of2AÀö fromthegroundtruthwhenaligningtheproteinstructures.
E.2.RelatedWork
Traditionalapproachestackledthetaskviaasearch-basedparadigmwhere,givenascoringfunction,theywouldsearchover
possibleligandposeswithanoptimizationalgorithmtofindaglobalminimum(Halgrenetal.,2004;Trott&Olson,2010).
Recently,deeplearningmethodshavebeentryingtospeedupthissearchprocessbygeneratingposesdirectlythrougha
neuralnetwork. Initialapproaches(Sta¬®rketal.,2022;Luetal.,2022)usedregression-basedframeworkstopredictthepose,
but,althoughsignificantlyfaster,theydidnotoutperformtraditionalmethods.
Corsoetal.(2023)arguedthattheissuewiththeseregression-basedapproachesistheirtreatmentoftheuncertaintyinthe
multimodalmodelposteriorposedistribution. TheyalsoproposedDiffDock,adiffusion-basedgenerativemodeltogenerate
dockedposesthatwasabletooutperformpreviousmethods,whichweuseasastartingpointfortheintegrationofour
DisCo-Diffapproachtodiffusion.
Mostdeeplearningapproachestodockingmodelthedataasageometricgraphorpointcloudin3D.Thenodesofthisgraph
arethe(heavy)atomsoftheligandand,typically,theC-alphacarbonatomsoftheproteinbackbone(sometimesfull-atom
representationsarealsousedfortheproteinbutthesearelesscommonforcomputationalcomplexityreasons). Thesenodes
areconnectedbyedgesincaseofchemicalbondsorpairwisedistancesbelowacertaincutoff. Neuralarchitectureslearn
featuresoverthenodesofthisgraphthroughanumberofmessagepassinglayers,thegeometricstructureisencodedvia
invariant(e.g. relyingonlyondistanceembeddings,seeSchu¬®ttetal.(2017))orequivariantoperations(Geiger&Smidt,
2022).
E.3.LatentVariables
Wedesigneachlatentvariabletotakevaluesindicatingoneofthenodesintheprotein-ligandjointgraph. Thereforethe
codebooksizeforthelatentvariableofanygivenprotein-ligandcomplexisequaltothetotalnumberofnodesinthegraph
19DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
i.e. thesumofthenumberofatomsintheligandandthenumberofresiduesintheprotein. Withthischoice,intuitively
eachlatentvariablewillindicateoneparticularatomorresidueinvolvedinsomekeycomponentoftheprotein-ligand
interaction. Forexample,usingtwolatentsthemodelcanlearntoindicateageometriccontactbetweenapairofnodesin
thefinalrepresentation. Furthernote,each‚Äùcodebook‚Äù,whenconsideredasasetofone-hotvectorsindicatingnotes,hasa
permutationequivariancepropertywiththenodesofthegraph(becausetheyareassociatedwithnodeproperties): ifthe
nodesoftheinputgrapharepermutedeachlatentvariablecomingoutoftheencoderorautoregressivemodelwillalsohave
itscodebookrepresentationpermuted.
E.4.ArchitectureDetails
Denoiser. ThisdesignchoiceforthediscretelatentscodebooksfitsverywellwiththepreexistingDiffDock‚Äôsdenoiser
architecturecomposedofequivariantmessagepassinglayers(Geiger&Smidt,2022). Eachlatentvariableisencodedin
abinarylabelforeachnodewhichissettozeroforallnodesexcepttheoneindicatedbythelatent. Thesebinarylabels
areconcatenatedtotheinitialnodefeatureswhiletherestofthedenoiseriskeptunchanged. Withprobability0.1during
trainingwedropthelatents,inthiscase,thebinarylabelsaresettozeroforalllatents,andalearnablenull-embeddingis
fedtoallinitialnodefeatures.
Encoder. Theencoderandautoregressivemodelsadoptverysimilararchitecturestothedenoiserwithafewkeydistinctions.
Theencodertakesasinputthegroundtruthposeoftheligand,learnsfeaturesforeachnodethroughmessagepassing,and
finallymseparatefeedforwardMLPs(wheremisthenumberoflatents)withaone-dimensionaloutputareappliedtoeach
noderepresentation. TheconcatenatedoutputsofeachoftheseMLPsformthelogitvectorsforeachofthelatentvariables
whicharepassedthroughtheGumbel-Softmaxdiscretizationstep.
Autoregressivemodel. Unliketheimagesynthesisexperimentssettingwheretheimagesareoftengeneratedwithrelatively
vagueconditioninginformation,fordocking,weareinterestedingeneratingligandposesconditionedonaparticularprotein
(structure)andligand.Thisconditioninginformationsignificantlyinfluencestheposteriorposedistributionandconsequently
thelearnedlatentvariables. Therefore, weneedtoconditiontheautoregressivemodelontheproteinstructureandthe
ligand. Weachievethis,onceagain,throughanequivariantmessagepassingnetwork,operatingonaninputcomposedofthe
proteinstructureandtheligand. Thelatteriscenteredattheprotein‚Äôscenter,givenanarbitraryconformer(i.e. molecular
conformation)fromRDKit(Landrum,2013)andauniformlyrandomorientation. Likethedenoiser,theautoregressive
modeltakesasinputtheadditionalbinarynodelabelsfortheexistinglatents(maskedoutappropriatelyduringtraining),
and,liketheencoder,itusesitsfinalnodeembeddingstopredictthelogitsforthenextlatentvariable.
E.5.ExperimentalDetails
Forthedockingexperiments,wefollowthedatasetsandproceduresestablishedbySta¬®rketal.(2022)andCorsoetal.(2023).
DatafortrainingandevaluationcomesfromthePDBBinddataset(Liuetal.,2017)withtime-basedsplits(complexes
before2019fortrainingandvalidation,selectedcomplexesfrom2019fortesting).
Denoiser. WeuseadenoiserarchitectureanalogoustotheoneproposedbyCorsoetal.(2024),whichisasmallerversionof
DiffDock‚Äôsoriginalarchitecturewherethemainchangesare: (1)5convolutionallayers(vs6oftheoriginalDiffDock‚Äôs
architecture)(2)noderepresentationswith24scalarsandpseudoscalarsand6vectorsandpseudovectors(vsrespectively48
and10)(3)sphericalharmonicsorderlimitedto1(vs2). Thesechanges,althoughsomewhataffectingtheinferencequality,
maketrainingandtestingofthemodelssignificantlymoreaffordable(from18dayson4GPUsto9dayson2GPUsfor
training).
Wekeepthesamedenoiserarchitectureforboththebaselinewithoutdiscretelatents(DiffDock-S)andourmodelandapply
similarhyperparametersearcheswhenapplicabletobothmodels. Atinferencetime,similarlytoCorsoetal.(2023),we
take40independentsamplesandusetheoriginalDiffDock‚Äôsconfidencemodel1toselectthetopone. ForDisCo-DiffDock
eachofthesamplesistakenbyindependentlysamplingfromtheautoregressivemodelandthenthe(conditioned)denoiser.
Encoder. Fortheencoder,weuseasimilarbutslightlysmallerarchitecturewith3convolutionallayers,24scalars,and4
vectors. Wesetthenumberofdiscretelatentvariables(eachtakingvaluesoverthewholesetofpossiblenodesinthejoint
graph)totwo,aswefoundthistoequilibratethecomplexityofthegenerativetaskbetweenthescoreandautoregressive
models.
1Theconfidencemodelisanadditionalmodel,Corsoetal.(2023)trainedtoselectthemostlikelycorrectposesoutofthediffusion
modelssamples.Thereadercanthinkofthisastryingtoselectthemaximumlikelihoodpose.
20DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Autoregressivemodel. Onechallengewiththeautoregressivemodelinthisdomainisitstendencytooverfitthelatent
variablesinthetrainingsetgiventhelimitedtrainingdata,thecomplexityoftheconditioninginformation,andthelow
trainingsignalthatdiscretelabelsprovide. Thereforewefounditbeneficialtodesigntheautoregressivemodeltousethe
pretrainedlayersofthedenoiseritself. Inparticular,wesimplyaddindependentMLPsforeachlatentvariablethatare
appliedtothefinalscalarrepresentationsofthenodes. Duringtheautoregressivetraining,forthefirstfiveepochs,the
weightsoftheconvolutionallayersarefrozen.
Inferencehyperparameters. Forinference,wemaintainthenumberofinferencestepsfromDiffDock(20)and,forboth
DisCo-DiffDockandthebaseline,wetuneonthevalidationsetthesamplingtemperatureforthedifferentcomponentsof
thediffusionsimilarlytohowitwasdonebyKetataetal.(2023). ForDisCo-DiffDockwealsotunethetemperatureusedto
sampletheautoregressivemodel. Wefind,with40samples,tobebeneficialtosetthistemperature>1whilethediffusion
samplingtemperature<1,thiscorrespondstoencouragingexplorationofdifferentbindingmodeswhiletryingtoobtainthe
maximumlikelihoodposeforeachmode. Thisfurtherhighlightstheadvantageprovidedbyenablingthedecomposition
ofdifferentdegreesofuncertainty. PleaseseethesourcecodeintheSupplementaryMaterialforalllowleveldetailsand
hyperparametersused.
F.GaussianMixtureExperiments
F.1.DataGeneration
Forthetoyexampleinsection3,wesetthetruedatadistributiontoamixtureofeightGaussiancomponents:
8
1(cid:88)
p (x)= N(x;¬µ ,œÉ I )
data 8 i i 2√ó2
i=1
where‚àÄi,œÉ =0.2,and
i
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
3 ‚àí3 0 0
¬µ = , ¬µ = , ¬µ = , ¬µ = ,
1 0 2 0 3 3 4 ‚àí3
(cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33) (cid:32) (cid:33)
‚àö3 ‚àö3 ‚àö3 ‚àö‚àí3
¬µ = 2 , ¬µ = 2 , ¬µ = 2 , ¬µ = 2 .
5 ‚àö3 6 ‚àö‚àí3 7 ‚àö‚àí3 8 ‚àö‚àí3
2 2 2 2
Toconstructthetoydataset,werandomlysampled1000datapointsfromeachcomponent,totaling8000datapoints. We
visualizetheKDEplotofthegenerateddatainFig.3(a).
F.2.TrainingandSampling
Weemployafour-layerMLPasthediffusiondecoder(DenoiserNeuralNetwork)G,forbothDisco-Diffanddiffusion
models. Weuseathree-layerMLPastheencoderE inDisco-Diff. Wesetthelatentdimensionofdiscretelatentto1and
thevocabularysizeto8. Ideally,eachdiscretelatentshouldcorrespondtoaGaussiancomponent,andthetime-dependent
scoresforasingleGaussiancomponenthaveasimpleanalyticalexpression. Weleveragethissimplicityandreparameterize
theoutputofdiffusiondecoderasG(x,t,z)= F(z)‚àíx +H(x,t),whereF istheembeddingforeachdiscretelatentzand
t2+œÉ2
1
Hisafour-layerMLP.ThemodeloptimizationusestheAdamoptimizerwithalearningrateof1e-3.
Forsampling,weusetheHeun‚Äôssecond-ordersampler. WefollowedthetimediscretizationschemeinEDM(Karrasetal.,
2022)with50samplingsteps.
F.3.Metric
We detail the metrics used in Fig. 4. The curvature for points x(t) on ODE trajectory dx/dt = G(x,t,z) (z is null in
diffusionmodels)isdefinedasŒ∫(x(t)) = ||‚àÇtT(x(t),t)|| whereT(x,t) = G(x(t),t,z) istheunittangentvector. Wecan
||x‚Ä≤(t)|| ||G(x(t),t,z)||
approximatethecurvaturebyfinitedifference: Œ∫(x(t))= ||‚àÇtT(t)|| ‚âà ||T(t)‚àíT(t‚àí‚àÜt)||. Weapproximatex(t‚àí‚àÜt)bya
||x‚Ä≤(t)|| ||x(t)‚àíx(t‚àí‚àÜt)||
singleEulerstep,i.e.,x(t‚àí‚àÜt)=x(t)‚àíG(x,t,z)‚àÜt. InFig.4(a),wereporttheexpectedcurvaturegiventhebackward
(cid:104) (cid:105)
timewhensimulatingtheODE,i.e.,E ||T(t)‚àíT(t‚àí‚àÜt)|| . Wesetthetimeelapsedto‚àÜt=0.001.
x(t) ||x(t)‚àíx(t‚àí‚àÜt)||
21DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
1.0
Diffusion Model Diffusion Model
0.8
DisCo-Diff DisCo-Diff
0.8
0.6
0.6 1.0
0.75
0.4
0.4 0.5 0.50
0.25
0.2
0.2
0.01 1.0 1.0 100.0
0 10 20 30 40 50 0 20 40 60 80 100
t t
(a) ImageNet-64 (b) ImageNet-128
Figure10. Averagedtraininglossversusnoiselevelt.
InFig.4(b),wemeasurethecomplexityofthetrainedneuralnetworksusingtheexpectedsquaredFrobeniusnormofthe
network‚ÄôsJacobians,i.e.,E (cid:2) ||‚àá G(x,t,z)||2(cid:3) .
x(t) x F
Additionally, toquantitativelyevaluatethegenerationquality, wereporttheWasserstein-2(W-2)distancebetweenthe
generateddistributionandthegroundtruthdistribution. IntheDisCo-Diffmodel,theW-2distanceisat0.118,comparedto
0.27inthestandarddiffusionmodel. ItsuggeststhatDisCo-Diffbettercapturesthemultimodaldistribution,evenin2-dim
space.
G.AdditionalSamplesandExperiments
G.1.LossAnalysis
InFig.10,weprovidethelossversustimecurveonbothImageNet-64andImageNet-128datasets. Wehavealsoincludeda
log-scaleversionofthex-axisintheinsetplot.
G.2.Class-conditonedImageNet-64
WeprovideextendedsamplesgeneratedbyDisCo-DiffinFig.11.
G.3.Class-conditonedImageNet-128
WeprovideextendedsamplesgeneratedbyDisCo-DiffinFig.12. Wealsovisualizesampleswithshareddiscretelatentsin
Fig.14. WefurtherprovidesamplesusingdifferentsamplersinFig.13,tohighlighttheover-smoothingissueofDisCo-Diff
atsmallertimesweobservedforthismodelwhenusingtheDDPMsampler. Although,intheory,theVDM++modeland
DisCo-Diffshouldlearnthesamefieldatsmalltimes,inpractice,weobservethatVDM++DDPMprovidessamplerswith
richerdetailscomparedtoDisCo-DiffDDPMatsmallertimes. Itsupportsourhypothesisthatthediscretelatentstend
todivertthemodeltooverlookthehigh-leveldetailsonImageNet-128,whenusingtheDDPMsamplerandthenetwork
architecturein(Kingma&Gao,2023). Wewouldliketoemphasizethatweonlyobservedthisbehaviorforthisonemodel
anddataset. Inallotherexperiments,discretelatentsuniversallyimprovedperformanceforallstochasticandnon-stochastic
samplers,evenwhenusedforalltimest.
G.4.GroupHierarchicalDisCo-Diff
WefurtherprovideextendedsamplesfromtheGrouphierarchicalDisCo-Diff. Fig.15showcasesthegeneratedimages
whencomposingtwodiscretelatentstogether,i.e.,(z ,zÀÜ ). Wecanseethatthegeneratedimagesfromcomposed
0:20 20:30
latentgenerallyinherittheshapefromimagesgeneratedbyz,andthecolorfromimagesgeneratedbyzÀÜ.
Fig. 16 further shows the effect when progressively fixing more coordinates of the discrete latent, and sampling the
remainingcoordinatesbytheauto-regressivemodel. Theimagesfirstconvergeinshape/layout,andsubsequentlyconverge
incolor/texture.
22
)t(ssol )t(ssolDisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure11.GeneratedsamplesbyDisCo-Diffonclass-conditionedImageNet-64,withODEsampler(FID=1.65,NFE=78).
23DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure12.GeneratedsamplesbyDisCo-Diffonclass-conditionedImageNet-128,withODEsampler(FID=2.08,NFE=114).
24DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
(a) DisCo-DiffODE,FID=1.98 (b) DisCo-DiffODE(t ‚â• 10)andDisCo- (c) DisCo-Diff ODE (t ‚â• 10) and VDM
DiffDDPM(t<10),,FID=2.78 DDPM(t<10),FID=1.73
Figure13.Imagesgeneratedbydifferentsamplers.WhenusingtheDDPMsampleratsmallertimes(b),thegeneratedimagesexhibita
slightover-smoothingissue,losingsomehigh-frequencydetailsincomparisontothoseproducedwiththeVDM++DDPMsamplerat
smallertimes.NotethatFIDscoretypicallypenalizesover-smoothsamples.Thisobservationsupportsourhypothesisthattheuseofthe
DDPMsamplerinDisCo-Diffatsmallertimescanincertainsituationsoverlookhigh-frequencydetails.
G.5.OverfittingandEncoderCollapseinContinuousLatent
Inthissection,weshowthatitisdifficulttocontroltheamountofinformationstoredinthecontinuouslatents,whichwould
leadtoeitheroverfittingorencodercollapse. Toillustratetheissue,wederivedthecontinuous/discretelatentsfroma
specificrealimage,andfedthecorrespondinglatentsintothedenoisers. AsshowninFigure17,whenKLDweight=0.1,the
continuouslatentmodelexhibitstheoverfittingissue,asallthegeneratedimagesareverysimilartothetrainingimage. It
alsoindicatesthattheencodersqueezesexcessiveinformationinthecontinuouslatentwhenusingasmallerKLDweight,
whichcomplicatesgenerativetraininginthesecondstage. WhenKLDweight=1,themodelexhibitsencodercollapse‚Äì
thedenoiserwouldignorethecontinuouslatent. Weobservethatevenusingadifferentcontinuouslatent,themodelwill
stillgenerateanidenticalbatchofsamples. Incontrast,DisCo-Diffgeneratedabatchofdiversesamples,sharingasimilar
high-levellayoutandcolorwiththetrainingimage. ThisindicatesthatthediscretelatentsinDisCo-Diffencodeglobal
layoutandcolorattributes‚ÄîkeystatisticalelementscrucialforthediffusionprocessinEuclideanspace. Thisalignswith
moredirectandstraighterODEtrajectories.
25DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure14.GeneratedsamplesbyDisCo-Diffonclass-conditionedImageNet-128,withODEsampler.Samplesineachgridsharethesame
latent,andgridsineachrowsharethesameclasslabels.Wecanseethatgenerally,imagessharingthesamediscretelatentsdemonstrate
similarglobalcharacteristics,suchasshape,layout,andcolor,despitebeingunderthesameclass.Itsuggeststhatdiscretelatentsprovide
complementaryinformationtotheclasslabels.
26DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure15.Generatedimageswithasharedlatent,usinggrouphierarchicalDisCo-DifftrainedonImageNet-64.Left:Sharedlatentz.
Middle:SharedlatentzÀÜ. Right:Sharedlatent(z ,zÀÜ ),wherethefirst20coordinatesarefromzandthelast10coordinatesare
0:20 20:30
fromzÀÜ.Wecanseethatthegeneratedimagesfromcomposedlatentsgenerallyinherittheshapefromimagesgeneratedbyz,andthe
colorfromimagesgeneratedbyzÀÜ.
27DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
Figure16.Progressivelyfixingmoresubcoordinatesofthediscretelatents,usingourgrouphierarchicalDisCo-DiffonImageNet-64.Left:
Randomlysampledz.Middle:Fixingthefirst20coordinatesz astheonederivedfromthered-boxedimage,samplingtherest.Right:
:20
Fixingthewhole30-dim.zastheonederivedfromthered-boxedimage.Thefigureshowstheeffectwhenprogressivelyfixingmore
coordinatesofthediscretelatent,andsamplingtheremainingcoordinatesbytheauto-regressivemodel.Theimagesfirstconvergein
shape/layout,andsubsequentlyconvergeincolor/texture.
28DisCo-Diff:EnhancingContinuousDiffusionModelswithDiscreteLatents
(a) Realimage (b) KLDweight=0.1 (c) KLDweight=1 (d) DisCo-Diff
Figure17.Wederivedthecontinuous/discretelatentsfromtherealimagein(a)andfedthelatentsintothedenoisers.(b):Thecontinuous
latentmodelexhibitstheoverfittingissuewhenKLDweightequalsto0.1,asallthegeneratedimagesareverysimilartotherealimage.
(c):WhenapplyingastrongerKLregulation(KLDweight=1)onthecontinuouslatent,themodelexhibitsencodercollapse‚Äìthedenoiser
wouldignorethecontinuouslatent.(d):DisCo-Diffgeneratedabatchofdiversesamples,sharingasimilarhigh-levellayoutandcolor
withtherealimage.
29