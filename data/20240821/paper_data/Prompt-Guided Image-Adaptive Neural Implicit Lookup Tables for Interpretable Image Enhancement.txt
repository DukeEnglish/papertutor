Prompt-Guided Image-Adaptive Neural Implicit Lookup Tables
for Interpretable Image Enhancement
SatoshiKosugi
TokyoInstituteofTechnology
Yokohama,Kanagawa,Japan
kosugi.s.aa@m.titech.ac.jp
Abstract adjustingtheirbrightnessandcolor.Thisprocesssignificantlyin-
Inthispaper,wedelveintotheconceptofinterpretableimageen- creasesanimageâ€™sutilityacrossvariousapplications.Thispaper
hancement,atechniquethatenhancesimagequalitybyadjusting focusesonimageenhancementtechniques,examiningtheirscope
filterparameterswitheasilyunderstandablenamessuchasâ€œExpo- andpotentialindetail.Especially,wedelveintotheconceptofin-
sureâ€andâ€œContrastâ€.Unlikeusingpredefinedimageeditingfilters, terpretableimageenhancement,atechniquethatimprovesimages
ourframeworkutilizeslearnablefiltersthatacquireinterpretable throughtheadjustmentoffilterparameterswitheasilyunderstand-
namesthroughtraining.Ourcontributionistwo-fold.Firstly,we ablenames,suchasâ€œExposureâ€,â€œContrastâ€,andâ€œSaturationâ€.This
introduceanovelfilterarchitecturecalledanimage-adaptiveneural approachallowstheusertoadjusttheenhancementresultsaccord-
implicitlookuptable,whichusesamultilayerperceptrontoimplic- ingtohisorherpreferenceandtolearnandmoreeffectivelyutilize
itlydefinethetransformationfrominputfeaturespacetooutput theimageenhancementprocessitself.Consequently,interpretable
colorspace.Byincorporatingimage-adaptiveparametersdirectly imageenhancementisanticipatedtosubstantiallyenhanceusersâ€™
intotheinputfeatures,weachievehighlyexpressivefilters.Sec- comprehensionandmanipulationofimageprocessing.
ondly,weintroduceapromptguidancelosstoassigninterpretable Previousinterpretableimageenhancementmethods[8,12,19,
namestoeachfilter.Weevaluatevisualimpressionsofenhancement 20]employpredefinedimageeditingfilters,andconvolutionalneu-
results,suchasexposureandcontrast,usingavisionandlanguage ralnetworks(CNNs)aretrainedtodeterminetheoptimalparame-
modelalongwithguidingprompts.Wedefineaconstrainttoensure tersforthesefilters.Sincethesefiltersaredesignedinamannerthat
thateachfilteraffectsonlythetargetedvisualimpressionwithout isunderstandabletohumans,theyfacilitateinterpretableimage
influencingotherattributes,whichallowsustoobtainthedesired enhancement.However,theeffectivenessofenhancementmaybe
filtereffects.Experimentalresultsshowthatourmethodoutper- constrainedbythelimitationsinherentinthedesignofthesepre-
formsexistingpredefinedfilter-basedmethods,thankstothefilters definedfilters.Forinstance,theâ€œExposureâ€filtercanbedesigned
optimizedtopredicttargetresults.Oursourcecodeisavailableat invariousways,makingitchallengingtomanuallycraftanopti-
https://github.com/satoshi-kosugi/PG-IA-NILUT. malExposurefilterforachievingspecificresults.Incontrast,most
recentimageenhancementmethods[18,29,30,33,34]employ3D
CCSConcepts lookuptables(LUTs)[31],whicharetablesthatrecordinputRGB
valuesandcorrespondingoutputRGBvalues.Multiple3DLUTs
â€¢Computingmethodologiesâ†’Imageprocessing;Computa-
areemployedtoapplyvariouseffects,andimage-adaptiveenhance-
tionalphotography.
mentisachievedbylinearlysummingthese3DLUTs,weightedby
image-adaptiveparameters.Unlikepredefinedimageeditingfilters,
Keywords
3DLUTsarelearnablefiltersoptimizedforpredictingenhancement
Imageenhancement,Lookuptable,Implicitneuralrepresentation, results,enablinghighqualityenhancement.However,thereare
Visionandlanguage,CLIP,Interpretability twonotableissuesassociatedwiththeuseof3DLUTs.Firstly,the
expressivepowerislimited.Thisisbecausethemultiple3DLUTs
ACMReferenceFormat:
SatoshiKosugi.2024.Prompt-GuidedImage-AdaptiveNeuralImplicitLook- aremerelysummedinalinearfashion,weightedbyimage-adaptive
upTablesforInterpretableImageEnhancement.InProceedingsofthe32nd parameters,whichmeanstheimage-adaptiveparameterscanonly
ACMInternationalConferenceonMultimedia(MMâ€™24),October28-November adjusttheenhancementeffectinalinearmanner.Secondly,3D
1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,14pages. LUTslackinterpretablenames.Sincetheyareoptimizedsolelyfor
https://doi.org/10.1145/3664647.3680743 predicting target enhancement results, their effects may not be
intuitivelyunderstoodbyhumans.
1 Introduction Toachievehigh-performingandinterpretableenhancement,we
proposelearnableandinterpretablefiltersnamedaPrompt-Guided
Imageenhancementhasbecomeanessentialtaskinmoderndig-
Image-AdaptiveNeuralImplicitLookupTable(PG-IA-NILUT).Our
italimageprocessing,enhancingthevisualqualityofimagesby
contributionistwofold.Firstly,weintroduceanovellearnablefil-
terarchitecturecalledanImage-AdaptiveNeuralImplicitLookup
MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. Table(IA-NILUT).Inspiredbyapreviousmethod[5],weutilizeim-
Thisistheauthorâ€™sversionofthework.Itispostedhereforyourpersonaluse.Not plicitneuralrepresentations[23]foracolortransformation.While
forredistribution.ThedefinitiveVersionofRecordwaspublishedinProceedingsofthe
previousresearchershaveused3DLUTstoexplicitlyrecordinput-
32ndACMInternationalConferenceonMultimedia(MMâ€™24),October28-November1,
2024,Melbourne,VIC,Australia,https://doi.org/10.1145/3664647.3680743. outputRGBvaluepairs,weemployamultilayerperceptron(MLP)
4202
guA
02
]VC.sc[
1v55011.8042:viXraMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
toimplicitlydefinethetransformationfrominputfeaturespaceto personalizedimageenhancementmodels[11,14].Becauseencoder-
outputcolorspace.Themostsignificantdistinctionfromthe3D decoder-based methods are computationally costly, filter-based
LUT-basedmethodsisthatweincorporateimage-adaptiveparam- approacheshaverecentlybecomemoreprevalent.
etersdirectlyintotheinputfeatures.SinceanMLPcanrepresent
nonlinearandcomplexrelationshipsbetweeninputsandoutputs, 2.2 PredefinedFilter-BasedMethods
ourapproachenablestheseimage-adaptiveparameterstoexerta
Predefinedfilter-basedmethodstrainCNNstopredicttheparame-
complexinfluenceontheoutputRGBvalues,therebyachieving
tersofpredefinedimageeditingfilters.Parketal.[20]employed
highlyexpressivefiltereffects.Additionally,toaddresstheproblem
reinforcementlearningtotrainanagentthatiterativelydetermines
ofhighcomputationalcostsofMLPs,weintroducethetechnique
theparameters.Huetal.[8]utilizedgenerativeadversarialnet-
calledLUTbypassing.InsteadofapplyingtheMLPdirectlytoeach
works(GANs)togeneratemorerealisticresults.KosugiandYa-
pixel,weconverttheMLPintoa3DLUT,whichisthenappliedto
masaki[12]reproducedPhotoshopfilters,enablingmoreefficient
eachpixel.Colortransformationthroughthe3DLUTiscomputa-
predictionofenhancementresults.Biancoetal.[3]andLietal.[16]
tionallyinexpensive,enablingcost-effectiveimageenhancement.
usedcolortransformationcurveforflexibleenhancement.Ouyang
Asasecondcontribution,weproposeapromptguidanceloss
etal.[19]achievedlocalenhancementwithregion-specificcolor
toassigninterpretablenamestoeachfilter.Thislossfunctionuti-
filters.Someresearchersproposedmethodsforcrowdworkersto
lizesCLIP[21],avisionandlanguagemodelcapableofembedding
adjustthefilterparameters[13,15].Thesemethodscanachievein-
imagesandtextwithinthesamefeaturespace.CLIPhasdemon-
terpretableenhancementsbecausethepredefinedfiltersarenamed
strateditsabilitytoquantifyimageimpressions[25].Forexample,
inawaythatisunderstandabletohumans,buttheenhancement
foranimpressionwordsuchasâ€œExposure,â€wepreparepairsof
performancecanbelimitedbythedesignofthesepredefinedfilters.
positiveandnegativeprompts(e.g.,â€œOverexposed photo.â€and
â€œUnderexposed photo.â€)andcalculatetheratioofthesimilarities
betweentheimagefeatureandeachpromptfeature.Thisallowsus 2.3 LearnableFilter-BasedMethods
toquantitativelyevaluatetheâ€œExposureâ€impressionconveyedby Learnablefilter-basedmethodsoptimizethefiltersusingtraining
theimage.Inthisstudy,weproposeusingthepairsofpositiveand data.Heetal.[6]successfullyreplicatedanimageeditingprocess
negativepromptsasguidingpromptstoguidethefilterstoward usinganMLP.Wangetal.[27]furtherenhancedtheseresultsby
achievingthedesiredeffects.Ourpromptguidancelossensures applyingsequentialimageretouching.
thatwhentheparameterassociatedwithâ€œExposureâ€isaltered,only Recentlearnablefilter-basedmethodslargelyuse3DLUTs,which
theâ€œExposureâ€scorechanges,whilethescoresforotherimpres- aretrainabletablesthatmapinputRGBvaluestocorresponding
sionsremainunaffected.Byminimizingthispromptguidanceloss outputvalues.Zengetal.[31]utilizedmultiple3DLUTs,combin-
inconjunctionwithareconstructionlossofthetargetresults,we ingthemwithimage-adaptiveweights.Wangetal.[26]introduced
achievehigh-performingandinterpretablefilters. spatial-aware3DLUTs.Yangetal.[29]madethesamplingpoints
Toevaluatetheproposedmethod,weperformexperimentswith of3DLUTsadapttoimages.Yangetal.[30]incorporateda1DLUT
theFiveK[4]andPPR10K[17]datasets.Weshowthattheproposed alongside3DLUTs.Zhangetal.[33]proposedacompressedrepre-
methodachievesinterpretablefilters,whichareunderstandableto sentationof3DLUTstoefficientlyincreasetheirnumber.Zhanget
humans.Inaddition,theproposedmethodachieveshigherperfor- al.[34]introducedhashingtechniquestoreduceparameters.Liuet
mancethanexistingpredefinedfilter-basedmethods. al.[18]defined4DLUTsforlocalenhancement.Shietal.[22]devel-
Thecontributionsofthispaperareasfollows: opedanetworkthatconsiderscrossattentionbetweenRGBvalues
andLUTs.Zhangetal.[32]combined3DLUTswithlocallaplacian
â€¢ Forinterpretableandlearnablefilters,wedeveloptheIA-
filters[2]foradvancedeffects.Despitethehighperformance,they
NILUT,ahighlyexpressivefilterarchitecture.
lackinterpretability,presentingachallengeforunderstandingthe
â€¢ Toassigninterpretablenamestoeachfilter,weintroduce
modificationstheymaketotheimages.
thepromptguidanceloss.
â€¢ The proposed method achieves higher performance than
3 Preliminary
existingpredefinedfilter-basedmethods.
Thissectiondescribesthekeyexistingmethod:image-adaptive
3DLUTs[31].3DLUTsarelearnabletablesthatrecordinputRGB
2 RelatedWorks
valuesandthecorrespondingoutputRGBvalues.Wedenotethe
2.1 Encoder-Decoder-BasedMethods matrixrepresentingthesamplingpointsbyI âˆˆ Rğ‘3Ã—3 andthe
EarlyCNN-basedimageenhancementmethodsutilizedencoder-
matrixrecordingthecorrespondingoutputvaluesbyOâˆˆRğ‘3Ã—3,
decoder-basedCNNs.Kimetal.[10]developedasequentialap- whereğ‘ isthenumberofsamplingcoordinates.Givenaninput
proachtoimageenhancement,applyingglobalandlocaladjust- RGBvalueof[ğ‘Ÿx,ğ‘”x,ğ‘x],anindexğ‘ issearchedforsuchthatthe
mentsinstages.Kimetal.[9]developedarepresentativecolor vectorintheğ‘ -throwofImatches[ğ‘Ÿx,ğ‘”x,ğ‘x];then,theğ‘ -throwof
transformtechniqueforimprovedcoloraccuracy.Zhaoetal.[36] O,denotedas[ğ‘Ÿy,ğ‘”y,ğ‘y],isreturned.IftheinputRGBvalueisnot
exploredtheuseofinvertibleneuralnetworkstorestorecontent includedinI,aninterpolatedvalueisreturnedbasedonthesur-
accuratelywhileavoidingbias.Zhangetal.[35]leveragedTrans- roundingRGBvalues.Thisprocessisperformedonallpixels.LetX
former [24] for structure-aware enhancement. Recognizing the andYbeinputandoutputimages,respectively,thetransformation
diversityinuserpreferences,someresearchershavefocusedon isrepresentedasY=Lookup(X,{I,O}).Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Reconstruction loss
Parameter Image-adaptive
predictor parameters w
Target image Guiding prompts
IA-NILUT
LUT bypassing Prompt
guidance
loss
Lookup
Input image Image editing filters Output image
Figure1:Overviewofourinterpretableimageenhancementmethod.Forahighlyexpressivefilterarchitecture,weproposean
IA-NILUT.ByemployingLUTbypassing,wecanexpeditethetransformationprocess.Additionally,weintroduceaprompt
guidancelosstoassigninterpretablenamestoeachfilter.Asourmethodprovidesaninterpretableandlearnableframework
forenhancement,itoutperformsotherpredefinedfilter-basedmethodsintermsofperformance.
Inimage-adaptive3DLUTs[31],multipleLUTs{I,O1},...,{I,Oğ½} 4.1 IA-NILUT
areemployedfordifferenteffects.Toachieveoptimalenhancement WeproposeanovelfilterarchitecturecalledanIA-NILUT.Inspired
foreachimage,image-adaptiveparametersw âˆˆ Rğ½ areusedto bytheexistingmethodknownasNILUTs[5],ourapproachemploys
weighteachLUT.Theenhancedresultisrepresentedas animplicitneuralrepresentation[23],whereinweimplicitlydefine
thetransformationfrominputspacetooutputspaceusinganMLP.
Y=Lookup(X,{I,O1})Ã—ğ‘¤ 1+Â·Â·Â·+Lookup(X,{I,Oğ½})Ã—ğ‘¤ ğ½. (1) Wevisualizethedifferencebetweenthe3DLUTsandourIA-NILUT
inFigure2.Themostsignificantdistinctionbetweentheprevious
EachLookup(X,{I,Oğ‘—})canberegardedastheresultofapplying
image-adaptive3DLUTsandourIA-NILUTisthattheIA-NILUT
differentfilterstoX,andeachğ‘¤ ğ‘—worksasafilterparameterthatde-
incorporatestheimage-adaptiveparametersdirectlyintotheinput
terminesthestrengthofthefiltereffect.O1,...,Oğ½ canbeoptimized
features.GiventhatanMLPiscapableofcapturingnonlinearand
topredictenhancementresults,whichmakesthelookuptables
intricate relationships between input and output variables, our
asefficientimageeditingfilters.Becausepixelsaretransformed
methodallowstheimage-adaptiveparameterstointricatelyaffect
independently,Eq.(1)canbesimplifiedas
theoutputRGBvalues,therebyachievinghighlyexpressivefilter
effects.Wedefinethecolortransformationprocessasfollows,
Y=Lookup(X,{I,O1Ã—ğ‘¤ 1+Â·Â·Â·+Oğ½ Ã—ğ‘¤ ğ½}). (2)
[ğ‘Ÿy,ğ‘”y,ğ‘y] = [ğ‘Ÿx,ğ‘”x,ğ‘x]
The image-adaptive parameters w are predicted by CNN-based +e(cid:0)[ğ‘Ÿx,ğ‘”x,ğ‘x]âŠ•sort([ğ‘Ÿx,ğ‘”x,ğ‘x])âŠ•w(cid:1) (3)
parameterpredictorFasw = F(X).TheparameterpredictorF âˆ’e(cid:0)[ğ‘Ÿx,ğ‘”x,ğ‘x]âŠ•sort([ğ‘Ÿx,ğ‘”x,ğ‘x])âŠ•0(cid:1),
processesimagesthataredownscaledtoafixedsize,andLookup
functionoperatesquickly.Asaresult,thisframeworkenablesreal- whereerepresentstheMLP,andâŠ•denotesvectorconcatenation.
timeenhancementforimagesofanysize. Wemaketwoimprovementstothecolortransformation.First,we
Thisapproachfacestwomainissues.First,thereâ€™stheissueof usethesortedRGBvalues,whicharedenotedbysort([ğ‘Ÿx,ğ‘”x,ğ‘x]),
limitedexpressivepower.Theenhancementresultsaresummed becausetheyplayanimportantroleinfilterinterpretability.For
linearlyasshowninEq.(1),meaningthatimage-adaptiveparam- instance,intheHSVcolorspace,saturationisdeterminedbythe
eterscannotproducecomplexeffects.Second,the3DLUTslack maximumandminimumRGBvalues.Second,weaddthedifference
interpretablenames.Sincethe3DLUTsareoptimizedsolelyfor
betweene(cid:0)[ğ‘Ÿx,ğ‘”x,ğ‘x]âŠ•sort([ğ‘Ÿx,ğ‘”x,ğ‘x])âŠ•w(cid:1) ande(cid:0)[ğ‘Ÿx,ğ‘”x,ğ‘x]âŠ•
predictingtargetresults,thereâ€™snoassurancethattheireffectswill
sort([ğ‘Ÿx,ğ‘”x,ğ‘x])âŠ•0(cid:1)
intotheinputRGBvalues.Thisensuresthat
bemeaningfulorunderstandabletohumans.Weaddressthesechal- theoriginalRGBvaluesareretainedintheoutputwhenwisset
lengesbyintroducinghighlyexpressiveandinterpretablefilters. to0,acommoncharacteristicofimageeditingfilters.WedefineEË†
asafunctionthatappliesEq.(3)toeachpixelofimageX,andthe
4 ProposedMethod imagetransformationprocessisrepresentedasfollows:
Toachievehigh-performingandinterpretableenhancement,we Y=EË†(X,w). (4)
maketwocontributions.First,weproposeanovelfilterarchitecture
calledanIA-NILUT.Second,weintroduceapromptguidanceloss LUTbypassing. SinceMLPsinvolvemultiplenonlineartrans-
togiveinterpretablenamestoeachfilter.Weshowtheoverviewin formations,thecomputationalcostissignificant,especiallywhen
Figure1anddescribethecontributionsinthefollowingsections. processinglargesizedimages.Toaddressthisissue,wepropose
CLIPscoreMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
0 0 0 0 0 0 0 0 0 136112 92 223171118 0 0 0 0 0 0
8 0 0 12 2 3 15 0 0 136112 92 223172122 8 0 0 9 1 2
16 0 0 23 5 9 23 1 1 137113 93 222170124 16 0 0 20 3 6
255255255 255255255 255255255 255255255 255255255
Sampling 82 71 63 151102 64 Sampling Output
points points values
IA-NILUT IA-NILUT
Lookup Lookup
Input Output Input Output Input Output
(a) 3D LUTs (b) IA-NILUT (c) IA-NILUT + LUT bypassing
(Low expressiveness, High efficiency) (High expressiveness, Low efficiency) (High expressiveness, High efficiency)
Figure2:Comparisonbetweenthe3DLUTs[31],theIA-NILUT,andtheIA-NILUTwiththeLUTbypassing.
LUTbypassing.InsteadofdirectlyapplyingtheMLPtoeverypixel visionandlanguagemodelthatembedsimagesandtextwithin
oftheimage,weconverttheMLPintoanLUTandapplythisLUTto thesamefeaturespace.CLIPhasdemonstrateditsabilitytoquan-
theimageasshowninFigure2(c).Eq.(4)istransformedasfollows, titativelyassessvisualimpressions[25].Whenevaluatinganim-
ageâ€™sâ€œExposure,â€wecreatepairsofpromptsthatcontrastposi-
Y=E(X,w)=Lookup(X,{I,O}),
tiveandnegativeaspects,suchasâ€œOverexposed photo.â€versus
(5)
whereO=EË†(I,w). â€œUnderexposed photo.â€Wedenotethesimilaritiesbetweentheim-
agefeatureandeachpromptfeatureasğ‘ +andğ‘ âˆ’,respectively.The
ThesamplingpointsI âˆˆ Rğ‘3Ã—3areconsideredasanimagewith imageâ€™sExposureimpressioncanbeevaluatedusingtheformula
ğ‘3 pixels.ThisisthenconvertedintoObytheMLP.Following exp(ğ‘ +)/(exp(ğ‘ +)+exp(ğ‘ âˆ’)).
thisconversion,theinputimageXistransformedusingthelookup Weproposeusingthepairsofpositiveandnegativeprompts
tablecomprisingpairsofIandO.Inourexperiment,wesetğ‘ to asguidingpromptstoguidethefilterstowardachievingthede-
33,whichresultsinIbeingtreatedasanimagecomposedof35,937 siredeffects.WeillustrateourmotivationinFigure3.Weprepare
pixels.Forcomparison,a512Ã—512imagecontains262,144pixels, ğ½ filternamesalongwithpairsofcorrespondingguidingprompts,
indicatingthatIrepresentsarelativelysmallimage.Evenwhen assigningafilternametoeachdimensionofthe ğ½-dimensional
processinglarge-sizedimages,theMLPisappliedonlytoI,which image-adaptiveparametersw.Duringthetrainingphase,weassess
meansthatthecomputationalcostoftheMLPremainsconstant. theimpressionsoftheenhancedresultswitheachguidingprompt.
LUTbypassingleveragestheexpressivepowerofMLPswhilealso Whenweassignthefilternameâ€œExposureâ€toğ‘¤ 1,weexpectthata
benefitingfromthelowcomputationalcostassociatedwithLUTs. changeinğ‘¤ 1willonlyaffecttheExposurescore,withoutimpact-
ingotherscoressuchasâ€œContrastâ€orâ€œSaturationâ€asshownin
ComparisonwithadvancedLUT-basedmethods. Recentre-
Figure3(b).IftheContrastandSaturationscoreschangeasshown
searchershavemadevariousimprovementstoLUTstoenhance
inFigure3(c),thiscouldbeconsideredundesiredbehaviorforthe
theirexpressiveness.Forexample,AdaInt[29]makesthesampling
Exposurefilter,potentiallyconfusingusers.Therefore,wepropose
pointsItobeimage-adaptive.CLUTNet[33]usesacompressed
aconstraintthatensuresonlyspecificscoresareaffectedwhen
representationof3DLUTs.Themostsignificantdifferencebetween
parametersarealtered,whileotherscoresremainunchanged.
ourmethodandtheseexistingmethodsliesinthenumberofimage-
Wedefinerandomlysampledweightsasw,anddenotethescores
adaptiveparameters.Theexistingmethodsimproveexpressiveness
câˆˆRğ½ evaluatedonğ½ promptpairsasfollows.
byincreasingthenumberofimage-adaptiveparameters;forexam-
ple,AdaIntandCLUTNetuse99and20image-adaptiveparameters, c=CLIPscore(cid:0) E(X,w)(cid:1). (6)
respectively.However,thisapproachmakesinterpretabilitymore
complex.Toomanyparameterscanmaketheimageeditingprocess Toensurethataspecificfiltereffectisappliedwhenğ‘¤ ğ‘— isaltered,
confusingforusers.Incontrast,ourmethodboostsexpressiveness wedefinethepromptguidanceloss.InsteadofaddingÎ”ğ‘¤ ğ‘— toğ‘¤ ğ‘—,
wedirectlyapplyaconstrainttothegradientinthefollowingway.
byusinganimplicitneuralrepresentation,withoutincreasingthe
numberofimage-adaptiveparameters.Inourexperiments,weuse âˆ‘ï¸ğ½ (cid:18) (cid:12)ğœ•ğ‘ ğ‘— (cid:12) âˆ‘ï¸(cid:12)ğœ•ğ‘ ğ‘—â€² (cid:12)(cid:19)
onlyfiveimage-adaptiveparameters.Thisresultsinafilterarchi- L PG= ğœ† ğ‘—(cid:12)
(cid:12)ğœ•ğ‘¤
âˆ’1(cid:12) (cid:12)+ğœ† (cid:12)
(cid:12)ğœ•ğ‘¤
âˆ’0(cid:12)
(cid:12)
, (7)
ğ‘— ğ‘—
tecturethatâ€™seasiertounderstand. ğ‘—=1 ğ‘—â€²â‰ ğ‘—
whereğœ†
ğ‘—
andğœ†arehyperparameters.Thisconstraintguarantees
4.2 PromptGuidanceLoss thatğ‘¤ ğ‘— affects only the targeted scoreğ‘ ğ‘—, while the remaining
Weintroduceapromptguidancelossthatassignsinterpretable scoresğ‘ ğ‘—â€²(ğ‘—â€² â‰  ğ‘—) are unaffected. By minimizing L PG, we can
namestoeachfilter.Inthislossfunction,weutilizeCLIP[21],a assigninterpretablenamestoeachfilter.
epahseR ReshapePrompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
: Exposure Table1:Guidingprompts.
: Saturation Filtername Positiveprompt Negativeprompt
Filter names Guiding prompts ğ‘¤ 1 Exposure â€œOverexposed photo.â€ â€œUnderexposed photo.â€
(FiveK)Contrast â€œClear photo.â€ â€œUnclear photo.â€
Exposure: ğ‘¤ 2 (PPR10K)Contrast â€œHigh contrast photo.â€ â€œLow contrast photo.â€
Contrast:
ğ‘¤ 3 Saturation â€œFull color photo.â€ â€œNo color photo.â€
ğ‘¤ 4 Colortemperature â€œYellow tinted photo.â€ â€œBlue tinted photo.â€
Input Output Saturation: ğ‘¤ 5 Tintcorrection â€œMagenta tinted photo.â€ â€œGreen tinted photo.â€
(a) Impression scoring with CLIP and guiding prompts
5 Experiments
Exposure:
5.1 DatasetsandImplementation
Contrast:
Weutilizetwowidelyuseddatasets:FiveK[4]andPPR10K[17].
Output Saturation: FiveKcontains5,000images,eachretouchedbyfiveexperts.Fol-
lowingthesettingofpreviouspapers[29,33],weuse4,500ofthese
(b) Desired Exposure filter
imagesfortrainingandtheremaining500fortesting,employingthe
imagesretouchedbyExpertCasthetargetimages.Weconductex-
Exposure: perimentsinboth480presolution(wheretheshortersideisresized
Contrast: to480pixels)andtheoriginal4Kresolution.Totrainefficiently,
weperformthetrainingat480presolutionandusetheoriginal4K
Output Saturation: resolutiononlyfortesting.PPR10Kincludes11,161portraitimages,
eachretouchedbythreeexperts.Weconductourexperimentsusing
(c) Undesired Exposure filter
theresultsretouchedbyExpertA.Accordingtotheofficialsetup,
wehave8,875pairsfortrainingand2,286pairsfortesting.Allim-
Figure3:Motivationforourpromptguidanceloss.
agesareusedinaresizedformatat360p.Weevaluateeachmethod
usingPSNR,SSIM[28],andtheL2-distanceinCIELABcolorspace
4.3 TrainingandTesting
(Î”ğ¸ ğ‘ğ‘).Whenmeasuringruntime,weusetheNVIDIARTXA6000
GPU.OurexperimentsarebasedonMMEditingtoolbox[1].
Thepairsofinputandtargetimagesfortrainingaredenotedas
In the IA-NILUT, we employ an MLP consisting of five fully
{X1,T1},...,{Xğ¼,Tğ¼}.Wedividethetrainingstepsintothreestages.
connectedlayers.ThehiddenfeatureswithinthisMLPare256-
Inthefirsttrainingstage,onlythefiltersEaretrained,usingonly
dimensional,andweutilizethehyperbolictangentasouractivation
thepromptguidancelossL PG.
function.FortheparameterpredictorF,afive-layerCNNisused
E=argminL PG. (8) onFiveK,andResNet18[7]isappliedtoPPR10K,followingthe
E configurationsreportedinpreviousstudies[17,29].
Inthesecondstage,weintroduceimage-adaptiveparametersw1, InspiredbythebasicfiltersinAdobeLightroom,wedefinefive
...,wğ¼ forimagesX1,...,Xğ¼.Thetrainingprocessisdefinedas filternamesandemployfivecorrespondingguidingpromptpairsas
outlinedinTable1.FortheContrastfilter,weusedifferentprompts
E,w1,...,wğ¼ = argmin L PG foreachdataset,tailoringthemtoachievethedesiredeffects.
E,w1,...,wğ¼
âˆ‘ï¸ğ¼ âˆ‘ï¸ğ¼ (9) 5.2 VisualizationofFilterEffects
+ MSE(Tğ‘–,E(Xğ‘–,wğ‘–))+ MSE(Xğ‘–,E(Tğ‘–,âˆ’wğ‘–)),
ğ‘–=1 ğ‘–=1 Todemonstratethattheproposedmethodachievesinterpretable
filtereffects,wevisualizefiltereffectsinFigure4.Inthesevisual-
whereMSErepresentsthemeansquarederrorfunction.Thethird
izations,onlycertainparametersarevariedwhileothersareheld
termisaconstraintensuringthattheinputimageisreconstructed
constant at 0. These results indicate that each filter produces a
from the target image when the parameters wğ‘– are reversed, a
specificeffectassociatedwiththecorrespondingguidingprompts.
propertythatexistingfiltersalsopossess.Inthefinalstage,the
Figure 5 shows examples of sequential application of predicted
parameterpredictorFistrainedas
parameters,wheretheenhancementprocessisvisualizedinaway
ğ¼ thatiseasyforhumanstounderstand.Thesequentialapplication
âˆ‘ï¸
F=argmin MSE(Tğ‘–,E(Xğ‘–,F(Xğ‘–))). (10) ofthefiltereffectsinFigure5isforvisualizationpurposesonly,
F ğ‘–=1 andtheallfiltereffectsareappliedsimultaneouslyinpractice.
Attesttime,theenhancementresultsaregeneratedusingthe
trainedEandF,asY=E(X,F(X)).ThefiltersEcanachievefast 5.3 AblationStudies
transformationsthroughtheLUTbypassing,andtheparameter Filterarchitecture. WeusetheIA-NILUTforahighlyexpressive
predictor F resizes the input image to a fixed resolution before filterarchitecture.ToassessthesignificanceoftheIA-NILUT,we
processing,resultinginreal-timeenhancement. train 3D LUTs [31] instead of the IA-NILUT using the prompt
Filters
Filters
Filters
CLIPscore
CLIPscore
CLIPscoreMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Figure4:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesontheleftandrightaresamplesfromFiveKandPPR10K,respectively.
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Figure5:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.ThetopandbottomimagesaresamplesfromFiveKandPPR10K,respectively.
guidanceloss.Toensuretheoriginalimageispreservedwhenwis inTable3,wedividetheimageintofourpatchesandsequentially
setto0,wemodifyEq.(1)asfollows, applytheMLPtoeachpatch.GiventhatanMLPiscomputation-
Y=X+Lookup(X,{I,O1})Ã—ğ‘¤ 1+Â·Â·Â· allyintensive,theabsenceoftheLUTbypassingleadstoincreased
(11) computationalcosts,particularlywhenprocessinglarge-sizedim-
+Lookup(X,{I,Oğ½})Ã—ğ‘¤ ğ½.
ages.Incontrast,byemployingtheLUTbypassing,theMLPis
AsshowninTable2,theIA-NILUTachieveshigherperformance, onlyappliedtosamplingpoints,thesizeofwhichareindependent
indicatingthehigherexpressivepoweroftheIA-NILUT.Thefil- oftheoverallimagesize.Inaddition,theLUTbypassinghaslittle
tereffectsofthe3DLUTstrainedwiththepromptguidanceloss effectonthePSNR.Thisapproachleadstocomputationallyefficient
areshowninFigure6.Thedesiredfiltereffectsarenotachieved, enhancementthatisnearlyunaffectedbytheimagesize.
indicatingthattheIA-NILUTisessentialforinterpretablefilters.
Promptguidanceloss. Weemploythepromptguidanceloss
LUTbypassing. WeusetheLUTbypassingtoreducethecomputa- toassigninterpretablenamestoeachfilter.Todemonstratethe
tionalcost.TodemonstratetheeffectivenessoftheLUTbypassing, significanceofthepromptguidanceloss,wepresenttheeffectsof
wepresentacomparisonofPSNRandruntimeinTable3,andacom- filterswhentrainingtheIA-NILUTwithoutthislossinFigure8.Itis
parisonoftherequiredGPUmemoryinFigure7.Whenprocessing difficulttoassigninterpretablenamestothesefilters.Forinstance,
some4Kimagesthatrequiremorememorythantheavailablelimit thefirstfilterinfluencesbothexposureandcolorsimultaneously.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Table2:ComparisonoffilterarchitectureusingFiveK(480p).
35
w/o LUT bypassing
Method PSNRâ†‘ SSIMâ†‘ ğš«ğ‘¬ â†“ 30 w/ LUT bypassing
ğ’‚ğ’ƒ
3DLUTs[31]w/promptguidanceloss 24.92 0.924 8.23 25
IA-NILUTw/promptguidanceloss 25.22 0.930 7.76 20
15
10
5
0
Exposure
480 960 1440 1920 2400
Image resolution [p]
Figure7:RequiredGPUmemoryw/andw/oLUTbypassing.
Contrast
Saturation
Color temperature
Tint correction
Figure 6: Filter effects of 3DLUTs [31] trained with the
promptguidanceloss.
Table3:EffectivenessoftheLUTbypassingonFiveK.
Figure8:FiltereffectsoftheIA-NILUTwithouttheprompt
480p FullRes.(4K) guidanceloss.
Method
PSNRâ†‘Runtimeâ†“ PSNRâ†‘Runtimeâ†“
Oursw/oLUTbypassing 25.22 1.9ms 25.06 7.8ms
Oursw/LUTbypassing 25.22 1.9ms 25.05 2.0ms uninterpretablemethodsisprovidedsolelyforreference,asour
primaryfocusisoninterpretableimageenhancement.
WepresentquantitativecomparisonsinTable4andvisualcom-
Similarly,thesecondfilteraffectsexposureandsaturationtogether, parisons with otherinterpretable methods in Figure 9. Because
whilethefourthfilterimpactscolorandcontrastatthesametime. ourfiltersarelearnableandoptimizedtopredictthegroundtruth,
Boththethirdandfifthfiltersareabletomodifytheimageâ€™scon- our method achieves better performance than other predefined
trast;ifbothfiltershadthesameâ€œContrastâ€name,userswould filter-basedmethods.WhiletheruntimeforExposureâ€™sfiltersand
beconfused.Theseresultshighlightthepromptguidancelossâ€™s UIEâ€™sfiltersislongduetotheircomplexcolortransformations,
criticalroletoassigninterpretablenamestoeachfilter. theruntimeofourmethodisalmostunaffectedbytheimagesize
thankstotheLUTbypassing.Ourmethodachievescomparable
5.4 ComparisonwiththeState-of-the-Arts performancetothatofuninterpretablemethodsonsomemetrics.
Theseresultshighlightthepotentialofourmethodtobridgethe
Weemployfourinterpretablemethods:D&R[20],Exposure[8],
gapbetweeninterpretabilityandhighperformance.
UIE[12],andRSFNet[19].Forafaircomparison,weutilizeonlythe
filtersadoptedinthesemethodsandapplythesameparameterpre-
5.5 VariousFilterEffects
dictorasours.ForthefiltersfromUIE,weexcludenon-differentiable
filters. Additionally, we include three uninterpretable methods: Byusingdifferentguidingprompts,wecanachievevariousfilter
ourbaselinemethod(3DLUTs[31]),andthetwostate-of-the-art effects.InadditiontotheguidingpromptslistedinTable1,we
methods(AdaInt[29]andCLUTNet[33]).Sincethepre-trained assignadditionalguidingpromptstoğ‘¤ 6andthentrainthefilters
weightsforCLUTNetwithPPR10Karenotpubliclyavailable,we usingonlythepromptguidanceloss.Figure10displaysexamples
onlyshowtheperformanceonFiveK.Theperformanceofthese ofsomeguidingpromptsandtheircorrespondingfiltereffects.Our
]BG[
yromem
UPGMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Table4:Quantitativecomparisonson(a)FiveKand(b)PPR10K.Thetopthreemethodsareuninterpretablemethods,whilethe
bottomfiveareinterpretablemethods.
(a)FiveK (b)PPR10K
480p FullResolution(4K) 360p
Method
PSNRâ†‘ SSIMâ†‘ ğš«ğ‘¬ â†“ Runtimeâ†“ PSNRâ†‘ SSIMâ†‘ ğš«ğ‘¬ â†“ Runtimeâ†“ PSNRâ†‘ SSIMâ†‘ ğš«ğ‘¬ â†“
ğ’‚ğ’ƒ ğ’‚ğ’ƒ ğ’‚ğ’ƒ
3DLUTs[31] 25.36 0.927 7.56 1.5ms 25.32 0.933 7.61 1.5ms 26.29 0.961 6.58
AdaInt[29] 25.50 0.930 7.47 1.5ms 25.50 0.935 7.46 1.5ms 26.29 0.961 6.59
CLUTNet[33] 25.55 0.931 7.50 1.9ms 25.50 0.935 7.53 2.1ms - - -
D&Râ€™sfilters[20] 23.86 0.903 9.07 1.9ms 23.76 0.907 9.16 1.9ms 24.27 0.934 8.11
Exposureâ€™sfilters[8] 25.04 0.920 7.83 4.3ms 24.91 0.924 7.92 15.9ms 25.53 0.954 7.55
UIEâ€™sfilters[12] 24.74 0.923 8.06 5.0ms 24.61 0.928 8.14 58.9ms 25.45 0.956 7.53
RSFNetâ€™sfilters[19] 24.86 0.924 7.89 2.8ms 24.82 0.928 7.96 2.8ms 25.41 0.946 7.48
PG-IA-NILUT(ours) 25.22 0.930 7.76 1.9ms 25.05 0.934 7.88 2.0ms 26.00 0.957 6.81
Input D&Râ€™s filters Exposureâ€™s filters UIEâ€™s filters RSFNetâ€™s filters Ours Ground truth
Figure9:Visualcomparisonsofinterpretivemethods,withthetopimagefromFiveKandthebottomfromPPR10K.
Table5:ImpactofthepromptguidancelossonFiveK.
Method PSNRâ†‘ SSIMâ†‘ ğš«ğ‘¬ â†“
ğ’‚ğ’ƒ
Oursw/opromptguidanceloss 25.46 0.930 7.60
Oursw/promptguidanceloss 25.22 0.930 7.76
guidancelossslightlydeterioratesperformanceasshowninTable5.
Apotentialapproachtoimproveperformancewhilepreservingin-
terpretabilityinvolvesrefiningtheselectionoftheguidingprompts.
WeselectedthepromptslistedinTable1heuristically;however,it
remainsuncertainwhethertheyareoptimalforbothinterpretabil-
ityandperformance.Thedevelopmentofanautomaticprompt
Figure10:Filtereffectsbyvariousguidingprompts.
selectionmechanismisidentifiedasanavenueforfutureresearch.
7 Conclusion
filterishighlyexpressive,enablingustoachievevariouseffectsand
Inthispaper,weexploredinterpretableimageenhancement.We
demonstratingitspracticalutilityforimageeditingapplications.
proposedahighlyexpressivefilterarchitecturenamedanIA-NILUT.
Additionally,weintroducedthepromptguidancelosstoassign
6 Limitation
interpretablenamestoeachfilter.Ourexperimentsdemonstrated
Althoughourmethodachievesinterpretableandhigh-performing thatourmethodnotonlyprovidesinterpretabilitybutalsoachieves
enhancement,itencountersadrawbackwheretheuseoftheprompt higherperformancecomparedtoexistinginterpretablefilters.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Acknowledgments
ColorFilters.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision.12160â€“12169.
ApartofthisresearchwassupportedbyJSPSKAKENHIGrant
[20] JongchanPark,Joon-YoungLee,DonggeunYoo,andInSoKweon.2018.Distort-
Number23K19997. and-recover:Colorenhancementusingdeepreinforcementlearning.InProceed-
ingsoftheIEEEConferenceonComputerVisionandPatternRecognition.5928â€“
References 5936.
[21] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
[1] 2020-08-31. https://github.com/open-mmlab/mmediting SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
[2] MathieuAubry,SylvainParis,SamuelWHasinoff,JanKautz,andFrÃ©doDurand. etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
2014.Fastlocallaplacianfilters:Theoryandapplications.ACMTransactionson InInternationalConferenceonMachineLearning.8748â€“8763.
Graphics33,5(2014),1â€“14. [22] TengfeiShi,ChenglizhaoChen,YuanboHe,WenfengSong,andAiminHao.2023.
[3] SimoneBianco,ClaudioCusano,FlavioPiccoli,andRaimondoSchettini.2020. RGBandLUTbasedCrossAttentionNetworkforImageEnhancement.InThe
Personalizedimageenhancementusingneuralsplinecolortransforms. IEEE BritishMachineVisionConference.
TransactionsonImageProcessing29(2020),6223â€“6236. [23] VincentSitzmann,JulienMartel,AlexanderBergman,DavidLindell,andGordon
[4] VladimirBychkovsky,SylvainParis,EricChan,andFrÃ©doDurand.2011.Learning Wetzstein.2020.Implicitneuralrepresentationswithperiodicactivationfunc-
photographicglobaltonaladjustmentwithadatabaseofinput/outputimagepairs. tions.Advancesinneuralinformationprocessingsystems33(2020),7462â€“7473.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition. [24] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
97â€“104. AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
[5] MarcosVConde,JavierVazquez-Corral,MichaelSBrown,andRaduTimofte. youneed.Advancesinneuralinformationprocessingsystems30(2017).
2024.Nilut:Conditionalneuralimplicit3dlookuptablesforimageenhancement. [25] JianyiWang,KelvinCKChan,andChenChangeLoy.2023.Exploringclipfor
InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.38.1371â€“1379. assessingthelookandfeelofimages.InProceedingsoftheAAAIConferenceon
[6] JingwenHe,YihaoLiu,YuQiao,andChaoDong.2020.Conditionalsequential ArtificialIntelligence,Vol.37.2555â€“2563.
modulationforefficientglobalimageretouching.InProceedingsoftheEuropean [26] TaoWang,YongLi,JingyangPeng,YipengMa,XianWang,FenglongSong,
ConferenceonComputerVision.679â€“695. andYouliangYan.2021.Real-timeimageenhancervialearnablespatial-aware
[7] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresidual 3dlookuptables.InProceedingsoftheIEEE/CVFInternationalConferenceon
learningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputer ComputerVision.2471â€“2480.
visionandpatternrecognition.770â€“778. [27] YiliWang,XinLi,KunXu,DongliangHe,QiZhang,FuLi,andErruiDing.2022.
[8] YuanmingHu,HaoHe,ChenxiXu,BaoyuanWang,andStephenLin.2018. NeuralColorOperatorsforSequentialImageRetouching.InProceedingsofthe
Exposure:Awhite-boxphotopost-processingframework.ACMTransactionson EuropeanConferenceonComputerVision.38â€“55.
Graphics37,2(2018),1â€“17. [28] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli.2004.Image
[9] HanulKim,Su-MinChoi,Chang-SuKim,andYeongJunKoh.2021.Represen- qualityassessment:fromerrorvisibilitytostructuralsimilarity.IEEEtransactions
tativeColorTransformforImageEnhancement.InProceedingsoftheIEEE/CVF onimageprocessing13,4(2004),600â€“612.
InternationalConferenceonComputerVision.4459â€“4468. [29] CanqianYang,MeiguangJin,XuJia,YiXu,andYingChen.2022.AdaInt:Learning
[10] Han-UlKim,YoungJunKoh,andChang-SuKim.2020.GlobalandLocalEnhance- AdaptiveIntervalsfor3DLookupTablesonReal-timeImageEnhancement.In
mentNetworksforPairedandUnpairedImageEnhancement.InProceedingsof ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
theEuropeanConferenceonComputerVision. 17522â€“17531.
[11] Han-UlKim,YoungJunKoh,andChang-SuKim.2020.PieNet:Personalizedimage [30] CanqianYang,MeiguangJin,YiXu,RuiZhang,YingChen,andHuaidaLiu.
enhancementnetwork.InProceedingsoftheEuropeanConferenceonComputer 2022.SepLUT:SeparableImage-AdaptiveLookupTablesforReal-TimeImage
Vision.374â€“390. Enhancement.InProceedingsoftheEuropeanConferenceonComputerVision.
[12] SatoshiKosugiandToshihikoYamasaki.2020.Unpairedimageenhancementfea- 201â€“217.
turingreinforcement-learning-controlledimageeditingsoftware.InProceedings [31] HuiZeng,JianruiCai,LidaLi,ZishengCao,andLeiZhang.2022. Learning
oftheAAAIConferenceonArtificialIntelligence,Vol.34.11296â€“11303. Image-Adaptive3DLookupTablesforHighPerformancePhotoEnhancementin
[13] SatoshiKosugiandToshihikoYamasaki.2023.Crowd-PoweredPhotoEnhance- Real-Time.IEEETransactionsonPatternAnalysis&MachineIntelligence44,04
mentFeaturinganActiveLearningBasedLocalFilter. IEEETransactionson (2022),2058â€“2073.
CircuitsandSystemsforVideoTechnology33,7(2023),3145â€“3158. [32] FengZhang,MingTian,ZhiqiangLi,BinXu,QingboLu,ChangxinGao,andNong
[14] SatoshiKosugiandToshihikoYamasaki.2024.PersonalizedImageEnhancement Sang.2024.LookupTablemeetsLocalLaplacianFilter:PyramidReconstruction
FeaturingMaskedStyleModeling.IEEETransactionsonCircuitsandSystemsfor NetworkforToneMapping.AdvancesinNeuralInformationProcessingSystems
VideoTechnology34,1(2024),140â€“152. 36(2024).
[15] YukiKoyama,IsseiSato,DaisukeSakamoto,andTakeoIgarashi.2017.Sequential [33] FengyiZhang,HuiZeng,TianjunZhang,andLinZhang.2022.Clut-net:Learning
linesearchforefficientvisualdesignoptimizationbycrowds.ACMTransactions adaptivelycompressedrepresentationsof3dlutsforlightweightimageenhance-
onGraphics(TOG)36,4(2017),1â€“11. ment.InProceedingsofthe30thACMInternationalConferenceonMultimedia.
[16] ChongyiLi,ChunleGuo,ShangchenZhou,QimingAi,RuichengFeng,and 6493â€“6501.
ChenChangeLoy.2023.FlexiCurve:FlexiblePiecewiseCurvesEstimationfor [34] FengyiZhang,LinZhang,TianjunZhang,andDongqingWang.2023. Adap-
PhotoRetouching.InProceedingsoftheIEEE/CVFConferenceonComputerVision tivelyHashing3DLUTsforLightweightReal-timeImageEnhancement.InIEEE
andPatternRecognitionWorkshops.1092â€“1101. InternationalConferenceonMultimediaandExpo.2771â€“2776.
[17] JieLiang,HuiZeng,MiaomiaoCui,XuansongXie,andLeiZhang.2021.Ppr10k: [35] ZhaoyangZhang,YitongJiang,JunJiang,XiaogangWang,PingLuo,andJinwei
Alarge-scaleportraitphotoretouchingdatasetwithhuman-regionmaskand Gu.2021. STAR:AStructure-AwareLightweightTransformerforReal-Time
group-levelconsistency.InProceedingsoftheIEEE/CVFConferenceonComputer ImageEnhancement.InProceedingsoftheIEEE/CVFInternationalConferenceon
VisionandPatternRecognition.653â€“661. ComputerVision.4106â€“4115.
[18] ChengxuLiu,HuanYang,JianlongFu,andXuemingQian.2023.4DLUT:learn- [36] LinZhao,Shao-PingLu,TaoChen,ZhengluYang,andArielShamir.2021.Deep
ablecontext-aware4dlookuptableforimageenhancement.IEEETransactions SymmetricNetworkforUnderexposedImageEnhancementwithRecurrent
onImageProcessing32(2023),4742â€“4756. AttentionalLearning.InProceedingsoftheIEEE/CVFInternationalConferenceon
[19] WenqiOuyang,YiDong,XiaoyangKang,PeiranRen,XinXu,andXuansongXie. ComputerVision.12075â€“12084.
2023.RSFNet:AWhite-BoxImageRetouchingApproachusingRegion-SpecificSupplementary Material
A ComparisonofNetworkParameters
TableApresentsacomparisonofthenumberofnetworkparam-
eters. Our method has a slightly higher number of parameters
Exposure
comparedtootherpredefinedfilter-basedmethodsduetotheinclu-
sionofanMLPinourIA-NILUT.However,ourmethodusesfewer
parametersthan3DLUTsandAdaInt,whichutilizemultipleLUTs
containingmoreparametersthanourMLP.
Contrast
B AblationStudyaboutSortedRGBValues
IntheIA-NILUT,weusethesortedRGBvaluesasdescribedinEq.
(3)ofthemainpaper.Todemonstratetheeffectivenessofthesorted
Saturation
RGBvalues,wepresentthefiltereffectswhenthesortedRGBvalues
arenotusedinFigureA.Thecolortemperaturefilterisexpected
toonlyaffectcolor;however,withoutthesortedRGBvalues,it
alsoimpactscontrast.Thisresulthighlightstheimportanceofthe
Color temperature
sortedRGBvaluesforeachfiltertoachievethedesiredeffects.
C AblationStudyaboutthePromptGuidance
Loss
Tint correction
Tofurtheranalyzethepromptguidanceloss,weexcludethecon-
FigureA:FiltereffectsoftheIA-NILUTwithoutthesorted
straintsontheuntargetedCLIPscoresasfollows,
RGBvalues.
L Pâ€² G=âˆ‘ï¸ğ½ (cid:18) ğœ† ğ‘—(cid:12) (cid:12) (cid:12)ğœ•ğœ• ğ‘¤ğ‘ ğ‘—
ğ‘—
âˆ’1(cid:12) (cid:12) (cid:12)(cid:19) . (A)
ğ‘—=1
WepresenttheeffectsoffilterswhentrainingtheIA-NILUTwith
Lâ€² inFigureB.WhenweuseLâ€² ,allCLIPscorescanbechanged; Exposure
PG PG
therefore,thedesiredeffectisnotachieved.Thisresulthighlights
thesignificanceofourdesignofthepromptguidanceloss.
Contrast
D AdditionalVisualizationsofFilterEffects
WepresentadditionalvisualizationsoffiltereffectsinFiguresC
andD,whereeachfilterproducesaspecificeffectassociatedwith
thecorrespondingguidingprompts.Weshowfurtherexamplesof
Saturation
sequentiallyapplyingpredictedparametersinFiguresEandF.The
enhancementprocesscanbevisualizedinawaythatiseasyfor
humanstounderstand.
Color temperature
TableA:Comparisonofnetworkparameters.
#Network
Method
parameters
Tint correction
3DLUTs[31] 593.5K FigureB:FiltereffectsoftheIA-NILUTwithLâ€² .
AdaInt[29] 619.7K PG
CLUTNet[33] 278.7K
D&Râ€™sfilters[20] 248.6K
Exposureâ€™sfilters[8] 266.0K
UIEâ€™sfilters[12] 250.6K
RSFNetâ€™sfilters[19] 250.6K
PG-IA-NILUT(ours) 449.3KPrompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
FigureC:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesaresamplesfromFiveK.MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
Exposure Exposure
Contrast Contrast
Saturation Saturation
Color temperature Color temperature
Tint correction Tint correction
FigureD:Visualizationoflearnedfiltereffects.Onlycertainparametersarevariedwhileothersareheldconstantat0.The
imagesaresamplesfromPPR10K.Prompt-GuidedImage-AdaptiveNeuralImplicitLookupTablesforInterpretableImageEnhancement MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
FigureE:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.TheimagesaresamplesfromFiveK.MMâ€™24,October28-November1,2024,Melbourne,VIC,Australia SatoshiKosugi
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
Input +Exposure +Contrast +Saturation +Color temperature +Tint correction GGrroouunndd ttrruuthth
FigureF:Sequentialapplicationofpredictedparameters.Thissequentialapplicationisforvisualizationpurposesonly,andthe
alleffectsareappliedsimultaneouslyinpractice.TheimagesaresamplesfromPPR10K.