Enhancing End-to-End Autonomous Driving Systems Through
Synchronized Human Behavior Data
YiqunDuan ZhuoliZhuang JinzhaoZhou
yiqun.duan-1@uts.edu.au zhuoli.zhuang@uts.edu.au jinzhao.zhou@student.uts.edu.au
HAICentre,AAII,Universityof UniversityofTechnologySydney UniversityofTechnologySydney
TechnologySydney Sydney,NewSouthWales,Australia Broadway,NewSouthWales
Sydney,NSW,AU Australia
Yu-ChengChang Yu-KaiWang Chin-TengLin
fred.chang@uts.edu.au yukai.wang@uts.edu.au chin-teng.lin@uts.edu.au
UniversityofTechnologySydney UniversityofTechnologySydney UniversityofTechnologySydney
Broadway,NewSouthWales Broadway,NewSouthWales Broadway,NewSouthWales
Australia Australia Australia
ABSTRACT 1 INTRODUCTION
Thispaperpresentsapioneeringexplorationintotheintegration Thispositionpaperaimstopioneertheexplorationofintegrat-
offine-grainedhumansupervisionwithintheautonomousdriving inggranularhumansupervisionintotheburgeoningfieldofau-
domaintoenhancesystemperformance.Thecurrentadvancesin tonomousdrivingtoenhanceitsperformance.Presently,thema-
End-to-Endautonomousdrivingnormallyaredata-drivenandrely jorityofautonomousdrivingapproaches,whetherend-to-endor
ongivenexperttrials.However,thisreliancelimitsthesystemsâ€™gen- pipeline-based,relyheavilyonexperttrials.Suchrelianceisinade-
eralizabilityandtheirabilitytoearnhumantrust.Addressingthis quateforconsiderationssuchasgeneralizabilityandearninghuman
gap,ourresearchintroducesanovelapproachbysynchronously trust.Thispapercollectssynchronousdatafrombothhumanand
collectingdatafromhumanandmachinedriversunderidentical machinedrivingscenariostoinvestigatethisaspect.
drivingscenarios,focusingoneye-trackingandbrainwavedata Thecurrenttrendinautonomousdrivingsystemscouldbecat-
toguidemachineperceptionanddecision-makingprocesses.This egorizedintopipelineformations[14,17,28,35]andEnd-to-End
paperutilizestheCarlasimulationtoevaluatetheimpactbrought (E2E)approaches[10,23,31,40].Thesesystemsprimarilytransform
byhumanbehaviorguidance.Experimentalresultsshowthatusing rawsensoryinputsintomachine-readablerepresentationsusinga
humanattentiontoguidemachineattentioncouldbringasignifi- varietyoffeaturefusiontechniques,suchasBEVorrangeview.The
cantimprovementindrivingperformance.However,guidanceby goalistodevelopautonomousdrivingmodelsthatcancomplete
humanintentionstillremainsachallenge.Thispaperpioneersa predeterminedrouteswhilesafelynavigatingdynamicenviron-
promising direction and potential for utilizing human behavior mentsandcomplyingwithtrafficrules,allthroughobservational
guidancetoenhanceautonomoussystems. data-drivenpolicies.
These approaches mentioned above are predominantly data-
KEYWORDS driven,relyingheavilyonmodelslearningfromexpertexamples.
However,thepuredata-drivenformationalsobringsstrongreli-
Human-GuidedAutonomousDriving;Brain-ComputerInterfaces
abilitytothedatadistribution.Wesuggestanaugmentationof
thismethodologybyincorporatingfine-grained,immediatehuman
ACMReferenceFormat:
labelsasfine-tuningfeedbacktofurtherenhancetherobustness
YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,
ofthedrivingprocess.Ourproposalinvolvesanovelapproach
andChin-TengLin.2024.EnhancingEnd-to-EndAutonomousDrivingSys-
where humans and machines share the same driving scenarios.
temsThroughSynchronizedHumanBehaviorData.InProceedingsofthe1st
InternationalWorkshoponBrain-ComputerInterfaces(BCI)forMultimedia Duringthesesharedexperiences,weaimtogatherdatafromboth
Understanding(BCIMMâ€™24),October28-November1,2024,Melbourne,VIC, humanandmachinedriversconcurrently.Thisdual-datacollection
AustraliaProceedingsofthe32ndACMInternationalConferenceonMultime- focusesoneye-gazingmarkersandbrainwavedata,offeringacom-
dia(MMâ€™24),October28-November1,2024,Melbourne,Australia.ACM,New prehensiveviewofthedrivingenvironmentanddecision-making
York,NY,USA,8pages.https://doi.org/10.1145/3688862.3689108 processes.Incorporatinghuman-guideddataintotheautonomous
systemisintuitivelyrightashumansstillperformbetterability
while dealing with a lot of scenarios. The utilization of human
guidancecouldleveragehumansuperiorintelligenceincomplex
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed scenarios(eg.correctattentionincomplexscenarios)intuitively
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation benefitingfromexpertguidanceinthelearningprocess.
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
Forallotheruses,contacttheowner/author(s). Introducinghumanguidanceintothelearningloopisafeasible
BCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia andeffectivestrategy,asevidencedbypreviousstudies[5,22,29,
Â©2024Copyrightheldbytheowner/author(s). 39].However,thisareahasverylimitedresearch.Followingthe
ACMISBN979-8-4007-1189-3/24/10
autonomousdrivingscenariosmentionedabove,weproposethe
https://doi.org/10.1145/3688862.3689108
4202
guA
02
]OR.sc[
1v80901.8042:viXraBCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Human State
Representation
Human Brain Cognition State
+ Human driving behaviour
data
Human and Machine Perform
Same Task at the Same Time
Simulation Platform +
Camera & VR Headset+ EEG
Vision sensors Eye Tracking Attention
Lidar
Radar
HD Maps 1. Eye-Tracking Guided
driving attention
Driving 2. Decision Enhance with
States Cognition and driving
behaviour data feed back.
Sensory Input
Machine Perception Framework
Figure1:Visualschemaofhumanenhancedautonomousdriving.Thismodelsubjectsboththemachineandthehumanto
identicaldrivingscenariosalongthesameroute.Throughoutthisprocess,wecollectsynchronousdataonhumanbehavior,
includingeye-trackingmetrics,brainwavepatterns,andbrakesignaling,capturingtheseelementsinunison.
drivingframeworkwheretheoverallstructureofourexplorationis humaneye-trackingdataasanauxiliaryfeedbackmechanismin
illustratedinFig1.Itdelineatestheresearchintotwomainaspects imitationlearninghasyieldedpositiveimprovementsindriving
ofexploringhowhumanguidancecouldenhancetheautonomous performance.Specifically,thisapproachenhancedthedrivingscore
drivingsystem:1)eyetrackingattentionduringhumandrivingand ontheCarlaLong-Set6datasetfrom50.63to51.29.Conversely,the
2)humancognitiondatageneratedthroughthedrivingprocedure. integrationofcognitiondatadidnotresultinasignificantdirect
Weinitiallyexploretheconceptofâ€œobservinglikeahuman" enhancementofthemachineâ€™sdrivingscore.Atthepositionpaper
inSection3.5.1.Thisinvolvesanalyzingeyemovementsduring stage,wethinkthisissueisreasonablebecauseoftwoissues:1)
drivingtouncoverdeeperinsightsintothecognitiveprocessesand Therecognitionaccuracyofthehumanbraindataisstilllimited.
attentionalfocusofhumandrivers.Leveraginghumanattention Thislimitedtheguidanceaccuracyofhumanintentionfordriv-
asguidance,thisapproachseekstoaddressoverlookedaspectsby ing.2)Thedataqualityisstillrelativelylow,wherethehuman
machines.Itallowsforthecorrectionofinstanceswhereobjects driverâ€™sreactionspeedsvary,andtheirdecision-makingcapability
ofpotentialsignificanceareignoredbyautomatedsystems.This varieswhenfacingdangeroussituations.Thisoutcomesuggests
human-guidedattentionisinstrumentalinhighlightingobjectsthat thattheeffectiveapplicationofcognitiondatainautonomousdriv-
alignwithhumanempiricalskillsbutmaynotberepresentedin ingremainsanarearipeforfurtherinvestigation.However,the
experttrainingdatasets.Suchguidanceisinvaluableformachines, effectivenessofhuman-observingdatastilloutlinesfutureresearch
as it aids in recognizing and responding to critical elements in integrating human guidance in a collaborative human-machine
variousdrivingscenarios,includingsuddenhazardsorintricate drivingcontext.Thecontributionofthispapercouldbecategorized
trafficsituations.Byintegratingthesehuman-centricinsights,we intofourfolds.
cansignificantlyenhancethemachineâ€™sperceptualanddecision-
â€¢ Thispositionpaperpioneersenhancingmachinedrivingby
makingabilitiesindynamicdrivingenvironments.
twoaspects1)observinglikehumandrivers,and2)making
Secondly,weexplorehowthehumancognitionorbehaviordata
decisionslikehumandrivers.
couldhelpguidetheautonomousdrivingmodelinSection3.5.2.
â€¢ Thispapercollectstheparallelhumancognitionandbehav-
Weexplorethehumancognitiondatatotrainanadditionalreward
iordataofsimultaneousmachineandhumandriving.
criticizertoprovideadditionalfeedbackforadditionalimitation
â€¢ Experimentalresultssuggesteffectivenessofintroducing
learning.Torealizethisgoal,wefirsttrainasimpleEEGwaveclas-
human-guidanceintoautonomousdriving.
sifiertorecognizewhetherhumanswilldecidetoemergencybreak
accordingtothecurrentsituation.Then,wefeedthesimultaneously
2 RELATEDWORKS
collectedEEGwaveswhilehumandrivingtothisclassifieranduse
thebreaksignalastheadditionalfeedbacktothereinforcement End-to-EndAutonomousDriving. Althoughrapidlydeveloping,
fine-tuningofthedrivingmodel. autonomousdriving(AD)technology[24],itfaceschallengesin
ExperimentalresultsareconductedinSection4,wherewere- generalizationandsafety.Toremedythisissue,anumberofworks
spectivelyexplorehoweye-trackingguidanceandhumancogni- haveexploredimitationlearning(IL)andreinforcementlearning
tiondataguidanceinSection3.5.1.Theresultssuggestleveraging methodstoleveragehumanguidanceforenhancingtrainingeffi-
ciencyandthesafetyofthelearnedofdrivingpolicy[5,8,9,25,30].
Transformer
DecisionEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Currentmethodologiesforintegratinghumanguidanceintoau- denotestheexperttrajectoryofwaypoints.Here,ğ‘¥ ğ‘¡,ğ‘¦ ğ‘¡ denotes
tonomousdriving(AD)systemsprimarilycategorizebasedontheir the2Dcoordinatesinego-vehicle(BEV)space.Thus,thelearning
impactontheADsystemintotwodistinctapproaches:1)leveraging targetcouldbedefinedasinEq.1.
humanexpertknowledgetotrainreliabledrivingpolicies,and2)in- argmin E (X,W)âˆ¼D (cid:2) Lğ‘¤ğ‘(W,ğœ‹(X))(cid:3) (1)
corporatinghumanoversightduringmodelexecution.Techniques ğœ‹
suchasConditionalImitationLearning(CIL)[4],andGenerative whereLğ‘¤ğ‘ isthewaypointlossdefinedinEq.4,andğœ‹(X)isthe
AdversarialImitationLearning(GAIL)[13]relyonhumanexpert predictedwaypointsgivenobservationXthroughpolicyğœ‹ tobe
trajectoriesfortrainingthedrivingpolicymodel.However,these learned.
methodsfacesignificantchallenges,includingtheinefficiencyof Formation:Inthispaper,thepolicyğœ‹(X) isrealizedbythe
datacollectionanddistributionshifts.Alternatively,adifferentset combinationofahybridfusionnetwork(Sec.3.2)andthedecision
ofapproachesfocusesoninterpretinghumanbiosignalsduringthe transformer(Sec.3.3),wherethefusionnetworktransfermulti-
evaluationphase,enablingthemodeltomaneuverthecarmore modalitysensoryinputsXintosemantictokensFğ‘ ,anddecision
safelyunderhumanguidanceorfeedback[11].Subsequentresearch transformerpredictsfuturegoalpointsWgivenFğ‘ .Thehuman
hasproposedstrategiesforemployingreal-timehumanguidance guidanceisinjectedbyThenaPIDControllerisappliedonthe
toenhancethesafetyandperformanceofthehuman-AIco-driving waypointsWdecisionanddecomposedintopracticalcontrol,ie.,
system[12,15,36,37].Thesestrategiesinvolvedynamiccontrol steer,throttle,andbrake.
transferbetweenhumandriversandAIagentstofacilitatetimely
intervention,therebyimprovingtheco-drivingexperience.This 3.2 HybridFusionTransformerEncoder
evolutioninapproachunderscorestheimportanceofhuman-AIcol-
ForbasicsensorfusionweutilizeMaskFuser[8]asourmainourk.
laborationinenhancingtheeffectivenessandsafetyofautonomous
MaskFuserproposedahybridnetworkshowninthelowerpartof
drivingsystems.Tocombinethemeritsoftheabovemethods,our
Fig.2thatcombinestheadvantagesofearlyfusionandlatefusion.
proposedmethodsalsomakeuseofhumanbiosignalstoimprove
Thenetworkconsistsoftwostages.
thetrainingefficiencyofAImodelswhilealsobeingusedindriving
EarlyFusion:Atthefirststage,weapplytwoseparateCNN
interventions.
branchestoextractshallowfeaturesrespectivelyfrommonotonic
imageandLiDARinputs.Fortheimagebranch,MaskFusercon-
Real-TimeHumanGuidanceforAutonomoussystems. Current
catenatesthreefrontviewcamerainputseachwith60Fovintoa
methodsforintroducingreal-timehumanguidanceinautonomous
monotonicviewandreshapedintoshape3Ã—160Ã—704.Forthe
drivingscenariospredominantlyrelyonheadmovement[20]and
LiDARbranch,MaskFuserreprocessestherawLiDARinputwith
eye-trackingsignals[1,41].However,theseapproaches,focuspri-
PointPillar[16]intoBEVfeaturewithshape33Ã—256Ã—256.Sincethe
marilyonexternalindicatorsofattentionandintentwithoutcon-
lower-levelfeaturesstillretainstronggeometricrelations,thesep-
sideringthecriticalroleofcognitiveprocessesindriving.Thereare
aratedencodercouldextracttightlocalfeaturerepresentationwith
onlylimitedworksexploringthereal-timemonitoringofthecogni-
fewerdistractions.Anovelmonotonic-to-BEVtranslation(MBT)
tiveprocessinthecontextofdrivingsafety[2,3,26,34],however,
attentionisappliedtoenricheachmodalitywithcross-modality
theydidnotconsiderusingthecognitivebrainsignalasguidance
assistance.MBTattentiontranslatesbothimagesandLiDARfea-
totheADsystem.Toaddressthisgap,weproposeaninnovativere-
turesintoBEVfeaturespaceandperformsamoreprecisespatial
searchdirectionthatintegrateshumancognitivesignalsalongside
featurealignmentcomparedtopreviouselement-wiseapproaches.
eye-trackingdatatoprovideamorecomprehensiveunderstanding
LateFusion:Atthesecondstage,thenetworkrespectivelytok-
ofdriverintentions.Bycombiningthesesourcesofinformation,we
enizes[6]featuremapsfromImageandLiDARstreamintosemantic
aimtoachieveamoreseamlessintegrationofthedriverâ€™sobjectives
tokens,respectivelydenotedbygreenandblueinlowerpartof
andsafetyconsiderationswiththeperformanceoftheautonomous
Fig.2.Thelatefusionisperformedbydirectlyapplyingashared
driving(AD)system.Thisholisticapproachpromisestoenhance
transformerencoderovertheconcatenatedtokenrepresentation.
thesymbiosisbetweenhumandriversandADtechnologies,paving
Thesharedencoderwithpositionembeddingcouldforcethetokens
thewayforadvancementsindrivingsafetyandefficiency.
fromvariousmodalitiesalignedintoaunifiedsemanticspace.Also,
bytreatingmulti-sensoryobservationassemantictokens,wecould
3 METHODOLOGY
furtherintroducemaskedauto-encodertrainingmentionedbelow.
3.1 Overview
3.2.1 Monotonic-to-BEVTranslation(MBT). MBTattentionper-
ProblemSetup:Wefollowthepreviouswidelyacceptedsettingof formscross-modalityattentionmorepreciselybyintroducinghu-
E2Edriving([10,32,40])thatthegoalistocompleteagivenroute manpriorknowledge(BEVtransformation).InspiredbyMonotonic-
whilesafelyreactingtootherdynamicagents,trafficrules,and Translation[21],wemodelthetranslationasasequence-to-sequence
environmentalconditions.Thus,thegoalistolearnapolicyğœ‹given processwithacameraintrinsicmatrix.Thedetailedstructureof
observation.WechoosetheImitationLearning(IL)approach MBTattentionsisshowninFig.3,wherethefeaturemapfrom
tolearnthepolicy.Thegoalistoobtainpolicyğœ‹ byimitatingthe imagestreamwithshapeFğ‘–ğ‘š âˆˆ Rğ‘Ã—ğ¶Ã—ğ»Ã—ğ‘Š isreshapedalong
behaviorofanexpertğœ‹âˆ—.Givenanexpert,thelearningdatasetD = widthdimensionğ‘Š intoimagecolumnsFğ‘ âˆˆ Rğ‘ğ‘ŠÃ—{ğ»Ã—ğ¶}.The
{(Xğ‘–,Wğ‘–)}couldbecollectedbylettingtheexpertperformsimilar
columnvectorsareprojectedintoasetofmediateencoding{hğ‘– âˆˆ
r so enu ste os r, yw oh be sere rvX atğ‘– io= ns{ o( fx tğ‘– ğ‘–ğ‘š he,x cğ‘– ğ¿ uğ‘– r) rğ‘¡ e} nğ‘‡ ğ‘¡= t1 std ae ten ,o at ne ds i Wmag =e {a (ğ‘¥n ğ‘¡d ,ğ‘¦L ğ‘¡i )D }ğ‘‡A ğ‘¡=R
1
WRğ» eÃ— tğ¶ re} ağ‘–ğ‘ t=ğ‘Š 1 met dh ir ao tu eg eh na ct or da in ns gfo or fm mer ol na oy te or nw icith vim ewult hi- ğ‘–h wea hd ics helf p- ra ott jeen ctt sion.BCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Waypoint Prediction
Status Status
Prediction Prediction
Velocity Eye-Tracking Human Intention Label
wH ayis pt oo ir ny t s GRU H Oea ffd sein tg HumanM AA tE te nL to ioss n Attention T Sr ta offi p c S iL gig n ht Cross Entropy
BBox Prediction Junction Break Signal
Decision Transformer
â€¦
waypoints queries Hidden State Tokens Traffic Query Cognition Query
Nx80x384x10 Nx40x176x216 MLP Layer
8
Range Decode
Patch r Head
Depth
Vision with Monotonic View Head
MBT MBT Seg
Head
Image
Nx256x256x
3
BEV
PointPill BEV Head
ar Patch
Decode
r Head
LiDAR
Nx128x128x108 Nx64x64x216 LiDAR
Figure2:Frameworkofinjectinghumanguidanceintotheautonomoussystem,takingautonomousdrivingasanexample.
ThelowerpartisthehybridfusiontransformerencoderdefinedinSection3.2.Thelearnedmachinestateisfedintoadecision
transformertocomeoutwiththefinaldrivingwaypointprediction.Thedecisiontransformerissupervisedbywaypoints
predictionGTandothersafetyconstraints.Thehumanguidanceisinjectedbyaddingtwohuman-behaviordatasupervision
branches.Thedecisiontransformerisrequiredtoreconstructhumanbehavior(eye-trackingattentionandbrakeintention)
jointlywithothertargets.
Image
Block
featuremapFğ‘šğ‘ğ‘¡.
K,V Transfromer ğ‘„(gğ‘–)ğ¾(hğ‘—)ğ‘‡
s(gi,hj)= âˆš , ğ‘„(gğ‘–)=gğ‘–ğ‘Š ğ‘„
Q ğ·
(3)
NWx{HxC} Query Emb
Grid
Sampling Fğ‘šğ‘–ğ‘‘ =âˆ‘ï¸
ğ‘–
(cid:205)e ğ‘—x ep x( ps (( sg (i g,h i,j h)) j))ğ‘‰(hğ‘–)
Position Query
wheres(gi,hj)isthescaleddotproduct[33]regularizedbydimen-
sionğ·.Yet,sincethemonotonicviewonlysuggestsinformation
LiDAR BEV
insidecertainFOV(Fieldofview),weapplyasamplingprocess
LiDAR
Block Fğ‘šğ‘ğ‘¡ = P(F mid) tosamplepointsinsideFOVdecidedbycamera
intrinsicmatrixintoBEVfeaturemapFğ‘šğ‘ğ‘¡.Thetranslatedfeature
Figure3:ThestructureoftheMBTattentionmodule,where mapFğ‘šğ‘ğ‘¡ andfeaturemapfromLiDarFğ¿ğ‘– isreshapedbyflatten
thefeaturesfrommonotonicviewareprojectedintoBEV alongwidthğ‘Š anddepthğ‘Ÿ intovectorsandconcatenateintose-
spacethroughasequence-to-sequenceformation. quenceFğ‘–ğ‘› = cat(F mbt,FLi).Thetransformerlayerisappliedon
Fğ‘–ğ‘›
âˆˆRğ‘â˜…Ã—ğ¶
toperformselfmulti-headattention([33])between
eachtokeninğ‘â˜…
dimension.
thekeyandvaluerepresentingtherangeviewinformationtobe
translated.
3.3 DecisionTransformer
ğ¾(hğ‘–)=hğ‘–ğ‘Š ğ¾, ğ‘‰(hğ‘–)=hğ‘–ğ‘Š ğ‘‰ (2)
InspiredbythepreviousworkInterFuser[23],weuseasimilar
Agridmatrixisgeneratedindicatingthedesiredshapeofthetarget transformer decoder as the decision layer. The decoder follows
BEVspace.Wegeneratepositionencoding{ğ‘” ğ‘– âˆˆRğ‘ŸÃ—ğ¶} ğ‘–ğ‘ =ğ‘Š
1
along thestandardtransformerarchitecture,transformingsomequery
sideeachradiusdirectioninsidethegridwithdepthğ‘Ÿ.Thegrid embeddingsofsizeğ‘‘usingğ¾ layersofmulti-headedself-attention
positionğ‘” ğ‘–istokenizedintoqueryembeddingandquerythehğ‘–with mechanisms.Fivetypesofinputaredesigned:{ğ‘¤ ğ‘–} ğ‘–ğ‘¡
=1
previous
radiusdirections1asdefinedinEq.3andgeneratethetranslated waypointsquery,ğ‘…2densitymapqueries(toquerycurrentvehi-
clestatus),humanattentionquery,onetrafficrulequery,andone
humanintentionquery.Thesequeriesareconcatenatedintoase-
1Theradiuscoordinatesarecalculatedbygivencameraintrinsicmatrix,FOV,and
prefixeddepthlength.
quenceoftokenswithshapeqâˆˆRğ‘‘Ã—ğ‘
whichwiththesameshape
Transfromer
Multi-Head
Attention
Encoding
Unified
Position
Transformer
Encoder
Transformer
Decoder
ClassificationEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
withz.Toindicatethequeryorderweaddastandardpositional 3.5 Human-GuidanceHeaders
embeddingintothequeryqwhendoingtheconcatenation.Ineach 3.5.1 HumanEye-TrackingAttentionPrediction. Wedesigntheeye
decoderlayer,weemploythesequeriestoinquireaboutthede-
trackingattentionquerieswithshapeQeye âˆˆ
Rğ» 16Ã—ğ‘Š 16Ã—ğ‘‘
,where
siredinformationfromthemulti-modalmulti-viewfeaturesvia ğ» ğ‘Š
the Ã— denotesthe16timesdown-sampledrange-viewratio
theattentionmechanism.Morespecifically,ineachtransformer 16 16
ofthecamera,whichhastheexactlysameratiowiththehuman
layer,wetreatperceptionstatezaskeyKandvalueV,andwe
treatthementionedqueriesqasthequeryQ.Thisstructureal-
attentiongroundtruthmapwithratioğ» Ã—ğ‘Š.ThequeryEeyeis
ğ» ğ‘Š
fedintothedecisiontransformerbyflattening Ã— intoafixed
lowsthedecisiontransformerqueryperceptionstateaccordingly 16 16
lengthandoutputsthepredictionembeddingthesameshape.Then
withdifferentqueriesinsidethesequence,resultingintheoutput
weuseatransposeconvolutiontoupsampletheembeddingfrom
independentlydecodedintowaypoints,onedensitymap,human
attentionprediction,trafficstatus,andthehumanintentionbythe shape 1ğ» 6Ã—ğ‘Š 16 Ã—ğ‘‘intohumanattentionpredictionEeyewithshape
followingpredictionheaders. ğ» Ã—ğ‘Š Ã—1.Wedefinetheeyetrackingpredictionloss L eye by
calculatingreconstructionlossbetweenpredictionEeyeandground
3.4 BasicPredictionHeaders
truthhumanattentionEÂ¯ eye:
The transformer decoder is followed by five parallel prediction L eye=âˆ¥Eeyeâˆ’EÂ¯ eyeâˆ¥2, (6)
modulestopredictthewaypoints,theobjectdensitymap,human
whereweusemeansquareerror(MSE)losstosupervisethehuman
attention,trafficrule,andhumanintentionrespectively.
attentionpredictorhead.
3.4.1 WaypointsPrediction. Forthewaypointsprediction,follow- 3.5.2 Human Intention Prediction. Given the synchronized col-
ingthementionedwaypointspredictionnetworkdefined[24],we lecteddatahumanintentionEEGdata,weuseapre-trainedclassi-
takeasinglelayerGRUtoauto-regressivelypredictasequenceof fier[7,42,43]todeterminewhetherthehumanhastheintentionto
threefuturewaypoints{ğ‘¤ ğ‘¡+ğ‘™} ğ‘™3 =1.TheGRUpredictstheğ‘¡+1-thway- breakthecarwerespectivelyrepresenttheEEGclassifiedlabelas
pointsbytakinginthehiddenstatefromtheğ‘¡-thdecodedwaypoint Iğ¸ğ¸ğº.Meanwhilegiventhebreakbehaviorsignalfromthehuman
embeddingfromthetransformerdecoderrelatedtothewaypoints driverâ€™spedalasIğ‘ğ‘Ÿğ‘ğ‘˜ğ‘’,weproposetoletthetransformerjointly
queries,andpreviousinputsfromtherecordedğ‘¤ ğ‘¡âˆ’2,ğ‘¤ ğ‘¡âˆ’1,ğ‘¤ ğ‘¡.Also predictthehumanintentionscore.Similartotrafficruleprediction,
toinformthewaypointsGRUpredictoroftheegovehicleâ€™sgoal weutilizeacombinedbinarycross-entropylosstosupervisethe
location,weconcatenatetheGPScoordinatesofthegoallocation decisiontransformer.Thelossfunctionisgivenbelow:
atthebeginningoftheinputsequence.Morespecifically,theloss
functionisdefinedinEquation4asfollows: L hb=ğœ† EEGL EEG+ğœ† bL brake, (7)
ğ‘‡ wherethehumanintentionissupervisedbycalculatingthebinary
Lğ‘¤ğ‘ =âˆ‘ï¸(cid:13) (cid:13) (cid:13)wğ‘¡ âˆ’wğ‘” ğ‘¡ğ‘¡(cid:13) (cid:13)
(cid:13)
(4) cross-entropycombination.
ğ‘¡=1 1
3.6 TrainingLossCombination
ğ‘”ğ‘¡
wherethewğ‘¡ isthegroundtruthfromtheexpertroute. Thelossfunctionisdesignedtosimultaneouslypredictmultiple
targetsincluding,waypoints(Lğ‘ğ‘¡),objectdensitymap(Lğ‘šğ‘ğ‘),hu-
3.4.2 DensityMapStatusPrediction. Thedensitymapprediction manattention(Lğ‘’ğ‘¦ğ‘’),trafficrule(Lğ‘¡ğ‘“),andhumanbreakintention
forcesthemodeltolearntopredictthecurrentvehiclestatuson (Lâ„ğ‘).
adensitymap.WebasicallyfollowtheoriginalsettingofInter-
Fuser[23]asthisisnotourresearchtarget.Pleaserefertoappen-
ğ¿=ğœ† ptL pt+ğœ† mapL map+ğœ† mapL map+ğœ† tfL tf+ğœ† hbL hb, (8)
dixAfordetaileddefinitionsoflossLğ‘šğ‘ğ‘. whereğœ†balancesthethreelossterms.Here,thewaypoints(Lğ‘ğ‘¡),
objectdensitymap(Lğ‘šğ‘ğ‘),andtrafficrule(Lğ‘¡ğ‘“)consistsofthe
3.4.3 TrafficRulePrediction. Fortrafficruleprediction,thecor-
puredatadrivingsupervisionforautonomousdrivingsimilarInter-
responding embedding from the transformer decoder is passed
Fuser[23]andthetwoadditionallossitemshumanattention(Lğ‘’ğ‘¦ğ‘’)
throughasinglelinearlayertopredictthestateofthetrafficlight
andhumanbreakintention(Lâ„ğ‘)indicatesthehumanguidance.
ahead,whetherthereisastopsignahead,andwhethertheego
vehicleisatanintersection.Whenpredictingthetrafficinformation 4 EXPERIMENTS
L tf,weexpecttorecognizethetrafficlightstatusLğ‘™,stopsignLğ‘ ,
andwhetherthevehicleisatajunctionofroadsLğ‘—.Allthesethree 4.1 DataCollection
statusesarerepresentedasaone-hot0-1labelacquiredfromthe Thedataarecollectedbycollectingdatasimultaneouslyfromhu-
CARLAsimulator.Theselossesaresimplycalculatedusingcross mansandmachinesdrivingunderthesamerouteandsimultane-
entropybetweenpredictionandgroundtruthasbelow: ouslycollectingthehumandataincludinghumaneye-tracking,
humanbrainwaves,andbrakingbehaviors.Thesimultaneoushu-
L tf=ğœ† ğ‘™Lğ‘™ +ğœ† ğ‘ Lğ‘  +ğœ† ğ‘—Lğ‘—, (5) mancollectionprocessisrealizedbylettinghumansubjectsdrive
usingtheLogitechG920drivingforcewheels,pedals,anddriving
whereğœ†balancesthelossterms,whicharecalculatedbybinary chair.Thehumanvisualattentioniscollectedbylettinghumans
cross-entropyloss. wearaHTCViveProEyevirtualrealitygear.WeusethewidelyBCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
Figure4:Visualizationexampleofthecollectedcombined
humaneye-trackingattentiondata.
Figure5:Exampleofthestatisticaldistributionofleftand
righteyeâ€™spupilpositionanddiametersizechangeduring
usedCARLAsimulator2 version0.9.13asthedrivingsimulator oneepisode.
Thedrivingscenariosareprojectedintoembodiedperspectivefor
thehumandriver.ThankstothepreviousworkDReyeVR3[27],
wedirectlyutilizetheproposedmodifiedCARLAsimulatortocom- withcorrespondinglabeladjustments.Inter-modalbridgingtrans-
pletethedatacollection.Tovisualizethehumaneyeâ€™sattention,we formers(MBTs)integratefeaturespost-initialconvolutions,atres-
provideexamplesofcollectedeye-trackingdatainFigure4.Simul- olutions(ğ¶ 1,40,176)and(ğ¶ 2,20,88),whereğ¶ 1=72andğ¶ 2=216.
taneously,wecollecttheEEGdatathrougha64-channelcollection TheMBTcomprisestwotransformerlayerswith512hiddendi-
device,whereexcludingtheground,reference,CB1,andCB2,the mensionsand4heads.Followingestablishedpolarraygridsam-
availablecountshouldbe60outof64.Wecollect12humansub- pling[21],MBTattendstovaryingdepthranges,translatingtoa
jectsusingthedefaultroutesunderTown4andTown7forhuman real-worldcoverageofupto30.5meters.Post-MBT,featuremaps
guidance. of20Ã—88Ã—512and32Ã—32Ã—512aresectionedinto4Ã—4patches,
Forthemachineperceptiondataset,weemployedarule-based combinedintoatokensequenceforaunifiedViTencoderwithfour
expertagentthatadherestothemethodologiesoutlinedinTrans- layersand4attentionheads.Thisresultsinasemanticsequenceof
Fuser[10]andInterFuser[23].Thisagentwasdeployedacrossa 174tokens,maintainingthe512-dimensionalityfromearlyfusion.
diverserangeofeighturbanlayoutsandvaryingweathercondi- Forthedecisiontransformer,thenumberoflayersğ¾inthetrans-
tions,operatingatafrequencyof2framespersecond.Through formerdecoderandthetransformerencoderis6,andthefeature
thisprocess,weamassedanextensiveexpertdatasetconsistingof dimensionğ‘‘ is256.Thedimensionofsemantictokensequence
3millionframes,whichequatestoapproximately410hoursofdata. outputfromtheHybridNetworkisprojectedinto256andfedinto
Thisdatasetservedasthefoundationfortheinitialpretraining thedecisiontransformer.WetrainourmodelsusingtheAdamW
phaseofourmachineperceptionmodel. optimizer[19]withacosinelearningratescheduler[18].Theini-
Theeye-trackingisprojectedinto2Dpositionswhiletraining. tiallearningrateissetto5Ã—10âˆ’4Ã— BatchSize forthetransformer
512
Forfurtherillustrationofthecollecteddatadistribution,wefurther encoder&decoder,and2Ã—10âˆ’4Ã— BatchSize fortheencoders.The
visualizethestatisticaldistributionoftheeyedatainFigure5. 512
weightdecayforallmodelsis0.07.Allthemodelsaretrainedfora
maximumof35epochswiththefirst5epochsforwarm-up[18].
4.2 ImplementationDetails
Fordataaugmentation,weusedrandomscalingfrom0.9to1.1and
Themodeltrainingisdividedintotwostages,wherestage1follows colorjittering.
thenormalautonomousmodeltraining,andstage2utilizesthe Human-GuidedFinetuneUponcompletionofthepretraining
human-guideddatatofurtherfinetunethedecisionmodel. withthe3millionframesderivedsolelyfrommachine-generated
Machine State Pretraining: For the feature extractor part, data,wetransitionedtoaphaseoftargetedrefinement.Inthisstage,
we follow the setting proposed in Section 4.2, the Hybrid Net- theperceptionmodelâ€™sparameterswerefixed,andweexclusively
work,whichemployscamerasandLiDARasdualmodalities.Cam- fine-tunedthedecision-makingmoduleofthetransformer.Forthe
erainputsaremergedintoa120-degreeFOVandreformattedto EEGwaveclassifier,wepre-trainaâ€œhumanintentiontobrake"
(160,704),whileLiDARdataistransformedtoa256Ã—256BEVfor- classifierbasedonthesimultaneouslycollectedEEGwaveandthe
mat[16].Forfeatureextraction,ImageNet-trainedRegNet-32[38]is humanbehaviortobrakelabel.Thisrefinementutilizedasmaller,
utilizedonbothimageandBEVLiDARdata.Trainingincludesangu- yethighlynuanceddatasetcomprising12,000framesofhuman-
laraugmentationbyÂ±20degreesonLiDAR,akintoTransfuser[10], deriveddata.Theobjectivewastointegratehumandecision-making
nuancesintothemachineperceptionmodel,therebyenhancingits
2https://github.com/carla-simulator/carla abilitytointerpretcomplexdrivingscenarios.Whenweconduct
3https://github.com/HARPLab/DReyeVR thehuman-guideddrivingtraining,weusetheinitiallearningrateEnhancingEnd-to-EndAutonomousDrivingSystemsThroughSynchronizedHumanBehaviorData BCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia
Table1:DrivingEvaluationonLongSet6withHumanGuidance,whereDTdenotesdecisiontransformer.
HumanGuidance DT Network DSâ†‘ RCâ†‘(%) ISâ†‘
Ã— Ã— TransFuser 46.95Â±5.81 89.64Â±2.08 0.52Â±0.08
Ã— Ã— MaskFuser 49.05Â±6.02 92.85Â±0.82Ã— 0.56Â±0.07
Ã— âœ“ InterFuser 49.86Â±4.37 91.05Â±1.92 0.60Â±0.08
Ã— âœ“ MaskFuser+DT 50.63Â±5.98 92.89Â±0.80 0.62Â±0.08
Eye-Attention âœ“ MaskFuser+DT 51.39Â±5.33 92.01Â±0.97 0.65Â±0.09
Intention âœ“ MaskFuser+DT 50.06Â±5.81 90.97Â±1.80 0.63Â±0.09
Eye-Attention+Intention âœ“ MaskFuser+DT 50.59Â±6.12 91.39Â±0.80 0.63Â±0.09
Eye-Attention+FakeIntention(GT) âœ“ MaskFuser+DT 51.28Â±6.17 92.03Â±0.98 0.64Â±0.08
Expert 75.83Â±2.45 89.82Â±0.59 0.85Â±0.03
as1Ã—10âˆ’4Ã— BatchSize forthedecisiontransformer.Theweight changingsituations.However,theimprovementbroughtbythe
512
decayiskeptthesameas0.07. humanattentionguidanceunderscoresthepotentialofleveraging
nuancedhumanbehavioralcuestoenhanceautonomousdriving
4.3 EvaluationwithHumanGuidance systems.
EvaluationBenchmark:Wedirectlykeeptheexperimentalset-
5 LIMITATION
tingsthesameasthepuremachinedrivingevaluationonCARLA
LongSet6[10].Weconductourdetailedablationandcomparison Intheexperimentalsection,althoughusingeye-trackingdatahasa
basedontheLongeset6BenchmarkproposedbyTransFuser[10], positiveimpactonautonomousdrivingperformance,usinghuman-
whichchoosesthe6longestroutespertownfromtheofficially intentiondatadoesnot.Weattributethisissuetotwomainfactors:
releasedroutesfromtheCARLAChallenge2019andsharesquitea First, human intention relies on a pre-trained EEG recognition
similaritywiththeofficialevaluation.
model,whichcanonlyachievea60âˆ’70%accuracyrateinidenti-
QuantativeEvaluationForbothonlineevaluationandofflineeval- fyingpeopleâ€™sdangerorbrakingintentions,therebyintroducing
uation,wefollowtheofficialevaluationmetricstocalculatethree noise.Second,comparedtotheabundantmachine-generatedau-
mainmetrics,RouteCompletion(RC),InfractionScore(IS), tonomousdrivingdata,collectingdriverintentiondataisrelatively
andDrivingScore(DS).TheRCscoreisthepercentageofroutedis- costly.Weutilizeddrivingdatafromonly12individuals,which
tancecompleted.Givenğ‘… ğ‘–asthecompletionbytheagentinrouteğ‘–, maynotbesufficienttosignificantlyinfluencethetrainingoflarge
RCiscalculatedbyaveragingthecompletionrateğ‘…ğ¶ = ğ‘1 (cid:205) ğ‘–ğ‘ğ‘… ğ‘–. models.Moreover,intentiondata,comparedtohumanattention,
TheISiscalculatedbyaccumulatingtheinfractionsğ‘ƒ ğ‘– incurred providessparsersupervision,necessitatingmoresuperviseddata
fortraining.Thisaspectwarrantsfurtherdiscussioninfuturework.
bytheagentduringcompletingtheroutes.Thedrivingscoreis
calculatedbyaccumulatingroutecompletionğ‘… ğ‘– withinfraction
multiplierğ‘ƒ ğ‘– asğ·ğ‘† = ğ‘1 (cid:205) ğ‘–ğ‘ğ‘… ğ‘–ğ‘ƒ ğ‘–.Wealsocalculatethedetailedin- 6 CONCLUSION
fractionstatisticaldetailsaccordingtotheofficialcodes.Wereport Inthispaper,wedelveintotheutilizationofhumanbehavioraldata
theevaluationwiththehuman-guidedfine-tuningmodelsonthese toimproveautonomousdrivingperformance.Weexploreharness-
metricsinTable1. inginsightsfromhumandriverstoenhancethedrivingsystemâ€™s
ItisobservedfromTable1thattheintegrationofhuman-guided capabilitiesbytwoaspects1)observinglikeahuman,and2)deci-
fine-tuningwiththeMaskFuserdecisiontransformer(DT)leads sionlikeahuman.Toachievethis,wecollectedeye-trackingand
toanoticeableimprovementinDrivingScore(DS).Theimprove- brake&cognitiondatafrom12humansubjectsbylettingmachines
mentismainlybroughtbyhumanattentionguidance,yetforthe andhumansdrivethesameroute.Theexperimentalresultsindicate
humanintentionguidance,wedidnâ€™tobserveaclearimprovement. thatguidingmachineattentionwithhumanattentioncanleadto
Here,theEye-Attentionguidancecombinedwiththedecisiontrans- aclearimprovementinperformance.However,theexperiments
formerachievesthehighestmeandrivingscoreof51.39,indicating didnotdemonstratethathumancognitiondatacouldsignificantly
theeffectivenessofincorporatinghumanattentiontoguidethe enhanceoutcomes.Integratinggranularhumansupervisioninto
drivingmodel.Furthermore,theadditionofintentiondataslightly machine driving merits further in-depth investigation. Such an
reducesthedrivingscoreunderbothsettings.Thisphenomenon approachisbeneficialforincreasingthemachineâ€™strustworthi-
isstillreasonablebecauseoftworeasons.1)Thehumanintention nesstohumanswhilemakingitsdecision-makingprocessesmore
recognitionisnotasaccurateatthisstageasthesuperviselabel. anthropomorphic.
Duringourexperiments,thehumanintentionclassifierhasanaccu-
racybetween60%âˆ’70%whichisstillcomparedlowatthisstage.2) REFERENCES
Along-existingproblemappearsthatthesimultaneouslycollected [1] YamanAlbadawi,MaenTakruri,andMohammedAwad.2022.Areviewofrecent
developmentsindriverdrowsinessdetectionsystems.Sensors22,5(2022),2069.
datahassmalltimeshiftsbetweenthehumantimestampandthe
[2] EmadAlyan,StefanArnau,JulianEliasReiser,StephanGetzmann,Melanie
machinetimestamp,thiswillleadtothewronglabelsinrapidly Karthaus,andEdmundWascher.2023. Blink-relatedEEGactivitymeasuresBCIMMâ€™24,October28-November1,2024,Melbourne,VIC,Australia YiqunDuan,ZhuoliZhuang,JinzhaoZhou,Yu-ChengChang,Yu-KaiWang,andChin-TengLin
cognitiveloadduringproactiveandreactivedriving. ScientificReports13,1 [28] HaoranSong,WenchaoDing,YuxuanChen,ShaojieShen,MichaelYuWang,and
(2023),19379. QifengChen.2020.Pip:Planning-informedtrajectorypredictionforautonomous
[3] ZehongCao,Chun-HsiangChuang,Jung-KaiKing,andChin-TengLin.2019. driving.InEuropeanConferenceonComputerVision.Springer,598â€“614.
Multi-channelEEGrecordingsduringasustained-attentiondrivingtask.Scientific [29] JingkaiSun,QiangZhang,YiqunDuan,XiaoyangJiang,ChongCheng,and
data6,1(2019),19. RenjingXu.2024. Prompt,plan,perform:Llm-basedhumanoidcontrolvia
[4] FelipeCodevilla,MatthiasMÃ¼ller,AntonioLÃ³pez,VladlenKoltun,andAlexey quantizedimitationlearning.In2024IEEEInternationalConferenceonRobotics
Dosovitskiy.2018.End-to-enddrivingviaconditionalimitationlearning.InIEEE andAutomation(ICRA).IEEE,16236â€“16242.
InternationalConferenceonRoboticsandAutomation.IEEE,4693â€“4700. [30] LeiTai,JingweiZhang,MingLiu,andWolframBurgard.2018.Sociallycompli-
[5] FelipeCodevilla,EderSantana,AntonioMLÃ³pez,andAdrienGaidon.2019.Ex- antnavigationthroughrawdepthinputswithgenerativeadversarialimitation
ploringthelimitationsofbehaviorcloningforautonomousdriving.InProceedings learning.In2018IEEEinternationalconferenceonroboticsandautomation(ICRA).
oftheIEEE/CVFInternationalConferenceonComputerVision.9329â€“9338. IEEE,1111â€“1117.
[6] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi- [31] ArdiTampuu,TambetMatiisen,MaksymSemikin,DmytroFishman,andNaveed
aohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,Georg Muhammad.2020.Asurveyofend-to-enddriving:Architecturesandtraining
Heigold,SylvainGelly,etal.2020.Animageisworth16x16words:Transformers methods.IEEETransactionsonNeuralNetworksandLearningSystems(2020).
forimagerecognitionatscale.arXivpreprintarXiv:2010.11929(2020). [32] MarinToromanoff,EmilieWirbel,andFabienMoutarde.2020.End-to-endmodel-
[7] YiqunDuan,CharlesChau,ZhenWang,Yu-KaiWang,andChin-tengLin.2024. freereinforcementlearningforurbandrivingusingimplicitaffordances.In
Dewave:Discreteencodingofeegwavesforeegtotexttranslation.Advancesin ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
NeuralInformationProcessingSystems36(2024). 7153â€“7162.
[8] YiqunDuan,XiandaGuo,ZhengZhu,ZhenWang,Yu-KaiWang,andChin-Teng [33] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Lin.2024. MaskFuser:MaskedFusionofJointMulti-ModalTokenizationfor AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
End-to-EndAutonomousDriving.arXivpreprintarXiv:2405.07573(2024). youneed.InAdvancesinNeuralInformationProcessingSystems.5998â€“6008.
[9] YiqunDuan,QiangZhang,andRenjingXu.2024.PromptingMulti-ModalTokens [34] Yu-KaiWang,Tzyy-PingJung,andChin-TengLin.2015.EEG-basedattention
toEnhanceEnd-to-EndAutonomousDrivingImitationLearningwithLLMs. trackingduringdistracteddriving. IEEEtransactionsonneuralsystemsand
arXivpreprintarXiv:2404.04869(2024). rehabilitationengineering23,6(2015),1085â€“1094.
[10] Prakashetal.[n.d.]. Multi-ModalFusionTransformerforEnd-to-EndAu- [35] MingyunWen,JisunPark,andKyungeunCho.2020. Ascenariogeneration
tonomousDriving.InCVPR2021.7077â€“7087. pipelineforautonomousvehiclesimulators. Human-centricComputingand
[11] DeepakGopinath,GuyRosman,SimonStent,KatsuyaTerahata,LukeFletcher, InformationSciences10,1(2020),1â€“15.
BrennaArgall,andJohnLeonard.2021.Maad:Amodelanddatasetfor"attended [36] JingdaWu,ZhiyuHuang,ZhongxuHu,andChenLv.2023. Towardhuman-
awareness"indriving.InProceedingsoftheIEEE/CVFInternationalConferenceon in-the-loopAI:Enhancingdeepreinforcementlearningviareal-timehuman
ComputerVision.3426â€“3436. guidanceforautonomousdriving.Engineering21(2023),75â€“91.
[12] ShaneGriffith,KaushikSubramanian,JonathanScholz,CharlesLIsbell,and [37] JingdaWu,ZhiyuHuang,WenhuiHuang,andChenLv.2022. Prioritized
AndreaLThomaz.2013. Policyshaping:Integratinghumanfeedbackwith experience-basedreinforcementlearningwithhumanguidanceforautonomous
reinforcementlearning. Advancesinneuralinformationprocessingsystems26 driving.IEEETransactionsonNeuralNetworksandLearningSystems(2022).
(2013). [38] JingXu,YuPan,XinglinPan,StevenHoi,ZhangYi,andZenglinXu.2022.
[13] JonathanHoandStefanoErmon.2016.Generativeadversarialimitationlearning. RegNet:self-regulatednetworkforimageclassification. IEEETransactionson
Advancesinneuralinformationprocessingsystems29(2016). NeuralNetworksandLearningSystems(2022).
[14] JunjieHuang,GuanHuang,ZhengZhu,andDalongDu.2021. Bevdet:High- [39] QiangZhang,PeterCui,DavidYan,JingkaiSun,YiqunDuan,ArthurZhang,
performancemulti-camera3dobjectdetectioninbird-eye-view.arXivpreprint andRenjingXu.2024. Whole-bodyhumanoidrobotlocomotionwithhuman
arXiv:2112.11790(2021). reference.arXivpreprintarXiv:2402.18294(2024).
[15] WBradleyKnoxandPeterStone.2011. Augmentingreinforcementlearning [40] QingwenZhang,MingkaiTang,RuoyuGeng,FeiyiChen,RenXin,andLujia
withhumanfeedback.InICML2011WorkshoponNewDevelopmentsinImitation Wang.2022. MMFN:Multi-Modal-Fusion-NetforEnd-to-EndDriving. arXiv
Learning(July2011),Vol.855.3. preprintarXiv:2207.00186(2022).
[16] AlexHLang,SourabhVora,HolgerCaesar,LubingZhou,JiongYang,andOs- [41] FengZhou,XJessieYang,andJoostCFDeWinter.2021. Usingeye-tracking
carBeijbom.2019. Pointpillars:Fastencodersforobjectdetectionfrompoint datatopredictsituationawarenessinrealtimeduringtakeovertransitionsin
clouds.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern conditionallyautomateddriving.IEEEtransactionsonintelligenttransportation
Recognition.12697â€“12705. systems23,3(2021),2284â€“2295.
[17] ZhijianLiu,HaotianTang,AlexanderAmini,XinyuYang,HuiziMao,Daniela [42] JinzhaoZhou,YiqunDuan,ZiyiZhao,Yu-ChengChang,Yu-KaiWang,Thomas
Rus,andSongHan.2022. BEVFusion:Multi-TaskMulti-SensorFusionwith Do,andChin-TengLin.2024.TowardsLinguisticNeuralRepresentationLearning
UnifiedBirdâ€™s-EyeViewRepresentation.arXivpreprintarXiv:2205.13542(2022). andSentenceRetrievalfromElectroencephalogramRecordings.arXivpreprint
[18] IlyaLoshchilovandFrankHutter.2016.Sgdr:Stochasticgradientdescentwith arXiv:2408.04679(2024).
warmrestarts.arXivpreprintarXiv:1608.03983(2016). [43] JinzhaoZhou,JustinSia,YiqunDuan,Yu-ChengChang,Yu-KaiWang,andChin-
[19] IlyaLoshchilovandFrank.Hutter.2017.Decoupledweightdecayregularization. TengLin.2024.MaskedEEGModelingforDrivingIntentionPrediction.arXiv
arXivpreprintarXiv:1711.05101(2017). preprintarXiv:2408.07083(2024).
[20] MahdiRezaeiandReinhardKlette.2014.Lookatthedriver,lookattheroad:No
distraction!noaccident!.InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition.129â€“136.
[21] AvishkarSaha,OscarMendez,ChrisRussell,andRichardBowden.2022.Translat-
ingimagesintomaps.In2022InternationalConferenceonRoboticsandAutomation
(ICRA).IEEE,9200â€“9206.
[22] StefanSchaal.1999.Isimitationlearningtheroutetohumanoidrobots?Trends
incognitivesciences3,6(1999),233â€“242.
[23] HaoShao,LetianWang,RuobingChen,HongshengLi,andYuLiu.2022.Safety-
enhancedautonomousdrivingusinginterpretablesensorfusiontransformer.
arXivpreprintarXiv:2207.14024(2022).
[24] HaoShao,LetianWang,RuobingChen,HongshengLi,andYuLiu.2023.Safety-
enhancedautonomousdrivingusinginterpretablesensorfusiontransformer.In
ConferenceonRobotLearning.PMLR,726â€“737.
[25] JiaminShi,TangyikeZhang,JunxiangZhan,ShitaoChen,JingminXin,andNan-
ningZheng.2023.EfficientLane-changingBehaviorPlanningviaReinforcement
LearningwithImitationLearningInitialization.In2023IEEEIntelligentVehicles
Symposium(IV).IEEE,1â€“8.
[26] JustinSia,Yu-ChengChang,Chin-TengLin,andYu-KaiWang.2023. EEG-
BasedTNNforDriverVigilanceMonitoring.In2023IEEESymposiumSerieson
ComputationalIntelligence(SSCI).IEEE,53â€“57.
[27] GustavoSilvera,AbhijatBiswas,andHennyAdmoni.2022.DReyeVR:Democra-
tizingVirtualRealityDrivingSimulationforBehavioural&InteractionResearch.
InProceedingsofthe2022ACM/IEEEInternationalConferenceonHuman-Robot
Interaction.639â€“643.