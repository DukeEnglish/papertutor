Atmospheric Transport Modeling of
CO with Neural Networks
2
Vitus Benson1,2,3, Ana Bastos4,2, Christian Reimers1,2, Alexander J. Winkler1,2,
Fanny Yang3, and Markus Reichstein1,2
1MaxPlanckInstituteforBiogeochemistry,Jena,Germany
2ELLISUnitJena,Jena,Germany
3ETHZuÂ¨rich,ZuÂ¨rich,Switzerland
4LeipzigUniversity,Leipzig,Germany
Correspondingauthor: VitusBenson,vbenson@bgc-jena.mpg.de
Key Points:
â€¢ CarbonBench: a systematic benchmark for machine learning emulators of atmospheric
tracer transport
â€¢ AdaptedSwinTransformerdeepneuralnetworktoachievestableandmass-conservingtrans-
port of CO by including physical constraints
2
â€¢ UNet,GraphCastandSphericalFourierNeuralOperatorbaselineswiththesamecustomiza-
tion are also strong models, for shorter lead times (up to 90 days)
Abstract
Accurately describing the distribution of CO in the atmosphere with atmospheric tracer trans-
2
port models is essential for greenhouse gas monitoring and verification support systems to aid
implementation of international climate agreements. Large deep neural networks are poised to
revolutionize weather prediction, which requires 3D modeling of the atmosphere. While similar in
thisregard,atmospherictransportmodelingissubjecttonewchallenges. Both,stablepredictions
for longer time horizons and mass conservation throughout need to be achieved, while IO plays
a larger role compared to computational costs. In this study we explore four different deep neural
networks(UNet,GraphCast,SphericalFourierNeuralOperatorandSwinTransformer)whichhave
proven as state-of-the-art in weather prediction to assess their usefulness for atmospheric tracer
transport modeling. For this, we assemble the CarbonBench dataset, a systematic benchmark
tailored for machine learning emulators of Eulerian atmospheric transport. Through architectural
adjustments, we decouple the performance of our emulators from the distribution shift caused by
a steady rise in atmospheric CO . More specifically, we center CO input fields to zero mean and
2 2
then use an explicit flux scheme and a mass fixer to assure mass balance. This design enables
stableandmassconservingtransportforover6monthswithallfourneuralnetworkarchitectures.
In our study, the SwinTransformer displays particularly strong emulation skill (90-day R2 > 0.99),
with physically plausible emulation even for forward runs of multiple years. This work paves the
wayforwardtowardshighresolutionforwardandinversemodelingofinerttracegaseswithneural
networks.
1
4202
guA
02
]GL.sc[
1v23011.8042:viXra1 Introduction
Limiting greenhouse gas emissions in line with the Paris agreement to mitigate anthropogenic
climate change requires monitoring, reporting and verification (MRV) efforts, especially of carbon
dioxide (CO ) [1]. Atmospheric measurements of CO from ground-based observatories, aircraft
2 2
and satellite can provide independent, science-based estimates. However, these observations
represent the concentration in the free air, not directly the emissions and other surface fluxes.
Atmospherictransportmodelsbuildthenecessarybridge,allowingtounderstandCO concentra-
2
tions from the perspective of anthropogenic emissions, biosphere and ocean fluxes [2â€“4]. They
solve the continuity equation of the mass of CO in the atmosphere by computing horizontal ad-
2
vection and vertical movement of air parcels using driving meteorological reanalysis fields [5].
Since its early ages in the late 1980s, solving 3D tracer transport with numerical schemes
has been hampered by prohibitive computational costs when going to higher resolution [6]. Yet,
low resolution transport models, suffer from a variety of modeling errors [7, 8]. More specifically,
representationsofconvectivetransport[9â€“12],turbulentverticalmixing[13],summertimediabatic
mixing [14], numerical advection scheme [15, 16] and reanalysed meteorological fields [17, 18]
in atmospheric transport models display significant uncertainties. Increasing resolution has been
proposed as one potential remedy to the situation [19, 20].
However,aprimaryapplicationoftransportmodelsisininversemodelingofthesurfacefluxes
tocontributeregularlytoMRVeffortssuchastheannualGlobalCarbonBudgetupdates[1]. Start-
ing from prior surface fluxes, the transport model is used to map them to atmospheric concentra-
tions which can be compared against observations to subsequently optimize the fluxes through
Bayesian calibration [19, 21â€“28]). This iterative process typically requires many expensive calls
of the transport model and its adjoint, thereby rendering the usage of high fidelity solvers difficult
[29].
Recently, AI-based emulation has revolutionized numerical weather prediction: deep neural
networks trained on high resolution meteorological reanalysis can both, outpace and outperform,
traditionalmedium-rangeweatherforecastingsystems[30â€“37]. Crucially,theseemulatorsrequire
less vertical layers, allow for larger time steps and leverage computing infrastructure optimized
for matrix multiplication like GPUs. Hence, the neural networks learn to solve the Navier Stokes
equations,byimplicitlyrepresentingboth,thelarge-scaledynamicsthatcouldbeexplicitlysolved,
and subgrid-scale processes that have to be parameterized, some works even make this division
explicit [35, 38, 39]. Furthermore, foundation models are being introduced which support other
tasksbeyondmedium-rangeweatherforecasting,suchasclimatemodeling[40,41]orshort-term
forecasts of atmospheric composition [42].
Modeling the atmospheric carbon cycle with neural networks has not yet gathered as much
attention. Still, there are works on emulating the footprints obtained from Lagrangian particle
dispersionmodelsofCH ,whichareusefulforregionalinversemodeling: OverafewUKregions,
4
the NAME model has been emulated with CNNs [43] and with Gradient Boosting Trees [44] and
over a few US regions, STILT has been emulated with FootNet [45], also a CNN. If more broadly
consideringapproachestomodelingtheCO andCH surfacefluxes,machinelearninghasbeen
2 4
usedtoupscaleeddycovariancemeasurementsasfunctionsofclimateandremotesensingtothe
globe, to obtain land fluxes of CH [46] and CO [47â€“50]. For the latter, Upton et al. [51] recently
4 2
introduced additional atmospheric constraints, bridging between atmospheric inverse modeling
and machine learning-based upscaling.
Here,weintroduceatmospherictransportmodelingofCO withneuralnetworkemulators. Our
2
main contributions are three-fold:
1. We create a new dataset (CarbonBench), the first systematic benchmark for training and
testing machine learning emulators of Eulerian atmospheric transport.
2. WedevelopaSwinTransformer-basedemulatortailoredfortransportmodelingthroughphysics-
based adjustments that allow for strong empirical performance: forward runs with global
RMSE below 1 ppm are possible for multiple years.
2Tracer
CO
2 ð‘¡ âŸ¼ ð‘¡+1
Forcing Deep
Meteorology Neural
Network
Surface
Fluxes
Figure 1: Offline atmospheric tracer transport modeling with deep neural networks.
3. Wecompareperformanceagainstthreeotherlargedeepneuralnetworkarchitectures(UNet,
GraphCast&SFNO).WhiletheSwinTransformeroutperforms,withourgenericarchitectural
changesalsothebaselinesachievestableandmass-conservingtransportforover6months.
Thus, we provide the first step towards a high resolution CO inversion system leveraging AI to
2
supporttheWorldMeteorologicalOrganizationsGlobalGreenhouseGasWatch(G3W)andother
efforts in line with the Paris agreement.
2 Methods
2.1 Task
In this work, we are tackling offline tracer transport with neural networks. That is, we solve the
continuity equation for the inert trace gas CO given prescribed meteorology. In other words, we
2
predict the 3D field of CO concentration in the atmosphere at time t+1 given the CO concen-
2 2
tration field from the previous time step t and meteorology and surface fluxes as additional inputs
(fig.1). Likeconventionalsolvers,ourlearnedneuralnetworksareautoregressive: longerforward
runs can be produced by feeding the predicted CO concentrations back in as inputs, alongside
2
prescribed fluxes and meteorology from the next time step. This allows in principle to generate
arbitrarily long trajectories of CO fields, if sufficient forcing data is available.
2
More specifically consider the CO mass mixing ratio Âµ, a source/sink term Î£ and the vector
2
of wind fields V, then tracer transport follows from integrating
dÂµ
+V Â·âˆ‡Âµ = Î£ (1)
dt
overthesphericalshellD = S2Ã—[r,r+h] âŠ‚ R3,withS2 thesphere,rtheradiusofEarthandhthe
height of the atmosphere. The integration is typically done by specifying von Neumann boundary
conditions dÂµ = 0, with n being the outward-facing normal derivative on D, in other words: the
dn
flux out of the atmosphere is none. This would model surface fluxes with the source/sink term
Î£, allowing for emissions inside the atmosphere. However, one may alternatively want to model
surface fluxes as the lower boundary condition. In offline tracer transport models, the winds V
are prescribed. An alternative approach would be online tracer transport, where in addition to the
tracer transport, the full atmospheric dynamics are modeled[52].
Whennumericallyintegratingthecontinuityequation,oneneedstodiscretizeoveragrid,which
requiressplittingtheoperatorintoresolvedandunresolvedscales. Foratmospherictransport,one
furthermoretypicallysplitstheoperatorintohorizontaladvectionandverticalconvection,whereby
fortheformeranysubgrid-scaleclosureisignored,butforthelatteritisparameterized[53]. Hence
3we end up with the equation
dÂµ dÂµ dÂµ dÂµ
+u +v +w(Ï‰,T,q,z) = Î£ (2)
dt dx dy dz
with the vertical velocity w being a function of updraft Ï‰, temperature T, specific humidity q and
geopotential height z. Throughout this work, we use neural networks to solve directly for the time
derivative:
dÂµ
= f(Âµ,u,v,Ï‰,T,q,z,...;Î¸) (3)
dt
withf(Â·;Î¸)beinganeuralnetworkswithparametersÎ¸. WethenintegrateusingEulerstepsÂµ =
t+1
Âµ +dÂµ. Duringtraining,thismeansweapproximateâˆ†Âµ = Âµ âˆ’Âµ withtheneuralnetworkf(Â·;Î¸)
t dt t t+1 t
by optimizing parameters through minimizing the squared loss:
Î¸Ë†= argminE||(f(X ;Î¸)âˆ’âˆ†Âµ )||2 (4)
t t 2
Î¸
2.2 CarbonBench Dataset
Fortrainingtheneuralnetworkemulators,wecollecttwoexistingdatasetsandreprocesstheminto
a deep learning-ready format. The first dataset (CarbonTracker) is an inversion of CO , i.e. it has
2
been obtained by optimizing the surface fluxes by transporting them and then matching modeled
atmospheric concentrations against observed ones. The second dataset (ObsPack) contains at-
mosphericmeasurementsofCO ,allowingtocompareourmodelpredictionsagainstanabsolute
2
baseline, independent of the training targets.
2.2.1 CarbonTracker
The CarbonTracker North America inversions [25] utilize the TM5 [54] transport model and the
ensemble Kalman filter to perform inverse modeling of the surface fluxes. More specifically, they
start with a set of prior fluxes for the land and ocean (e.g. from Earth system models) and add
these to prescribed fluxes for anthropogenic emissions and wildfires to obtain a first version of
total CO surface fluxes. In a next step, they leverage an atmospheric transport model and the
2
ensemble Kalman filter to optimize the surface fluxes such that they match well to observed data
of atmospheric CO concentrations. Finally, the optimized fluxes are transported one more time
2
to obtain a 3D field of atmospheric CO concentrations. Here, we only use the final product
2
from the inverse modeling process: the optimized surface fluxes and corresponding 3D fields.
Moreover, we treat all surface fluxes as prescribed inputs, and not just the anthropogenic and
wildfire components.
We collect 3D atmospheric CO concentration fields, 2D CO surface fluxes and 3D mete-
2 2
orological fields of q,T,u,v,Ï‰,z from the CarbonTracker CT2022 version [55]. These represent
a closed system, i.e. they fulfill a discretized version of the continuity eq. 1. Moreover, as they
have been produced through inverse modeling, they are also closely resembling observations of
atmospheric CO .
2
We prepare three versions of the dataset through aggregation that allow for quicker experi-
mentation and testing of methods at multiple resolution. Each dataset we split into training (years
2000-2016), validation (2017) and testing (2018-2020) sets, the three resolutions are:
â€¢ LowRes: 5.625â—¦Ã—5.625â—¦Ã—10 hybrid vertical levels Ã—6h.
â€¢ MidRes: 2.8125â—¦Ã—2.8125â—¦Ã—20 hybrid vertical levels Ã—6h.
â€¢ OrigRes: 2â—¦Ã—3â—¦Ã—34 hybrid vertical levels Ã—3h.
Note, while OrigRes is close to the original data resolution, it is not exact â€“ we shift the time
steps in comparison to CarbonTracker by 1.5h (except for fluxes) and we still regrid the surface
fluxes, which had been optimized at 1â—¦ Ã— 1â—¦ in CarbonTracker. In addition, in CarbonTracker
4North America, the full atmosphere is modeled at this higher resolution over a zoomed window
in North America. We deliberately chose the horizontal resolution such that LowRes (MidRes)
horizontalfieldshave32Ã—64(64Ã—128)pixels,whichisidealformostmoderndeepneuralnetwork
architectures from computer vision [56].
2.2.2 Data preprocessing
In order to prepare the three deep learning-ready dataset versions, we introduce a preprocessing
chain. Through this chain, we aim to standardize dataset format and ensure that the processed
data is directly useable to implement offline tracer transport emulators in the spirit of eq. 3. Fur-
thermore, the chain enables future work to leverage the presented neural networks on datasets
from other transport models. We perform the following preprocessing steps:
1. Horizontalregridding: intensivemeteorologicalvariableswithbilinearinterpolation,extensive
quantities(CO mixingratioandairmass)aredividedbycellarea,andthen,alongsideCO
2 2
surface fluxes regridded with conservative interpolation.
2. Conversion to standard units and variables: masses in [Pg], Cconcentrations as ppm mass
mixing ratio [10âˆ’6kgCO2], fluxes as [kgCO2], pressure in [hPa]. We aggregate surface fluxes
kgDryAir m2s
into ocean, land and anthropogenic fluxes, where the former two would be optimized during
an inversion and the latter one prescribed.
3. Vertical aggregation: pressure weighted mean for intensive quantities, sum for extensive
quantities (masses).
4. Temporal resampling: linear resampling to target resolution.
5. Flux staggering: surface fluxes are staggered, such that they represent the mean flux be-
tween a time step and the next time step.
6. Flux mass correction: anthropogenic surface fluxes are corrected, such that any mass con-
servation errors introduced through preprocessing are removed and the mass difference
between two time steps matches exactly the surface fluxes.
7. Temporally splitting into independent training, validation and testing datasets.
8. Deep learning-optimized storage: we store our dataset in Zarr files, with chunking that opti-
mizes loading of all data at a single time step: We store two arrays per time step, one with
all 2D fields and one with all 3D fields.
9. Statistics: wecomputemeanandstd. dev. statisticsforallfieldsandforallper-leveltemporal
deltas of all fields.
The preprocessing routines are implemented as part of the Neural Transport Python library
(https://github.com/vitusbenson/neural_transport).
2.2.3 ObsPack station data
The NOAA ObsPack GLOBALVIEWplus product [57] collects measurements of atmospheric CO
2
frommanydifferentscientificlaboratoriesaroundtheglobewithinstrumentsatground-basedsta-
tions and towers and onboard ships, aircraft and weather balloons. In this study, we use all mea-
surements flagged as representative from the v9.1 2023-12-08 product. We compare these CO
2
measurements with our modeled data by extracting the grid cells closest to the horizontal (lat/lon)
and vertical position (geopotential height) of the measurement and averaging over 6h time win-
dows. WeusetheexactsamemethodtoextractstationtimeseriesfromthetargetCarbonTracker
data,asweusefortheAImodels. Thisallowsforanabsolutecomparisonpoint: thetargetCarbon-
Tracker data does not achieve perfect prediction of the ObsPack data, meaning we can compare
5the performance of AI models directly with TM5, the transport model used in CarbonTracker. In
future work, the ObsPack station data does also allow for cross-dataset comparison. Note, how-
ever,ifAImodelstrainedontwodifferentdatasetsarecompared,differencesinperformancemay
alsostemfromthedifferencesintheprescribedsurfacefluxes,meteorologyandinitialconditions,
and not merely from the learned transport model.
2.2.4 Evaluation
We evaluate models by performing quarterly forward runs starting on Jan 1st, April 1st, etc. and
running for 3 months each. We then average statistics over the full test period (2018â€“2020) and
compute a range of performance metrics, such as RMSE, R2, decorrelation time (#days with
R2 > 0.9), RMS mass error, relative mean and relative variability. We compute these metrics over
individual spatial and temporal coordinate axes and also over sets of axes, to obtain a full picture.
2.3 Neural Networks
In this section we describe the neural networks studied in this work. We restrict ourselves to a
rather conceptual description and refer the reader to the original papers for in-depth explanations
of each architecture. In addition we report the adjustment to the original architectures which we
introduce in this work to enable their applicability to atmospheric transport modeling.
2.3.1 Motivation
Atmospheric transport modeling requires processing high dimensional data: at the coarsest reso-
lution,ourmodelinputhas32Ã—64Ã—(10Ã—10+7) â‰ˆ 220kdimensions(andâˆ¼ 20koutputdimensions).
At such scales, training a standard 2-layer neural network, the multi-layer perceptron (MLP), be-
comes computationally intractible. In deep learning this challenge is typically approached by in-
troducing inductive biases, that allow to significantly reduce the dimensionality of each matrix
multiplication. In this study, from the vast variety of available architectures, we pick four that are
representative of generic architectural classes and that previous work has found successful at
emulating weather and climate data.
Moreover,threeoutofthefournetworkscoincidewithgeneralclassesofconventionalnumeri-
calmethods(comparefig.2): a)UNetusesaregularmesh,likefinitedifferencesolversonregular
grids, b) GraphCast uses an icosahedral mesh, again analogous to finite difference solvers, c)
SFNOissimilartoapseudo-spectralsolver,onlyd)SwinTransformerisunconventionalintheway
thatitfavorsabrute-forcesplit-process-combineapproach,withlittleresemblancetoconventional
numerical methods, i.e. it has the least inductive bias.
2.3.2 Vertical discretization
In all four approaches, we only consider inductive biases for the horizontal dimension, in the
vertical direction we stack all data along the channel dimension and feed that as input. In other
words,themodelsreceiveanarrayofvalues(forforcing,tracersandsurfacefluxes)perhorizontal
grid cell, and then process these in a latent space, allowing for vertical mixing and interactions
across variables. This approach is independent of the partical vertical discretization pertinent in
the data.
In this work, we use CarbonTracker data, which comes at hybrid model levels. Hybrid levels
interpolatesmoothlybetweenaterrain-followingcomponentinthelowertroposphere(closetothe
surface)andconstantpressurelevelsintheupperstratosphere. Morespecifically,thepressureof
eachverticallayerisanaffinetransformationofthesurfacepressure(whichvarieswithorography).
2.3.3 UNet
UNets[58]arefullyconvolutionalneuralnetworks(CNNs)consistingofanencoderandadecoder
arranged in a U-shape â€“ referring to gradual spatial downsampling and subsequent upsampling.
6a UNet (regular mesh) b GraphCast (icosahedral mesh)
Zonally Graph
Periodic Regrid Neural Regrid
CNN Network
ð‘¡ âŸ¼ ð‘¡+1 ð‘¡ âŸ¼ ð‘¡+1
c SFNO (pseudo-spectral) d SwinTransformer (brute-force)
Fourier Shifted
Window Window
SHT Neural iSHT Window
Split Merge
Operator Attention
ð‘¡ âŸ¼ ð‘¡+1 ð‘¡ âŸ¼ ð‘¡+1
Figure 2: Conceptual depiction of the four deep neural networks included in this study.
We employ UNets that treat the globe as a cylinder, having periodic convolutions in zonal (longi-
tude) direction and zero-padded convolutions at the poles [59, 60]. Vertical layers and different
variables are simply stacked along the channel dimension.
Our UNet has 4 stages within the encoder and decoder, each consiting of two 3Ã—3 2d conv
layers, that are followed by LeakyReLU and BatchNorm layers and a residual connection. Spa-
tial downsampling is achieved through 2Ã—2 MaxPooling and upsampling through 2Ã—2 nearest
interpolation. In the first encoder stage, we use a single 7Ã—7 conv layer instead. We add skip
connections between the encoder and decoder stages. The network operates on input sizes that
are divisible by 16, but through bilinear upsampling in the first and nearest downsampling in the
last layer, we allow for other input shapes as well.
2.3.4 SwinTransformer
SwinTransformers[61]aretransformerneuralnetworksprocessing2Dinputsbyattentionbetween
embeddings of windows, which are shifted in each layer. We allow for periodic shifts in zonal
(longitude) direction, retain processing at the highest resolution (no hierarchical layers) and adopt
relative positional encoding, three architectural design choices which have been proven useful for
weather forecasting [62].
Our SwinTransformer has 12 layers each consisting of a Multi-head Self-Attention block fol-
lowed by LayerNorm and a pixelwise MLP (with GELU activation and LayerNorm) and residual
connectionsbetweenblocks. Theself-attentionismaskedinsuchaway,thatonlyattentionwithin
windows of nearby pixels is computed, we use 4Ã—8 pixel windows. Windows are shifted by half
their size at every second layer, with zonally periodic shifts. In contrast to previous work we found
using patch embedding to introduce artifacts at longer rollouts, which is why our model directly
operates at pixel level (i.e. in 1 Ã— 1 patches). Input shapes need to be divisible by the window
shape, we allow for other input shapes through nearest interpolation.
2.3.5 GraphCast
GraphCastisagraphneuralnetwork(GNN)tailoredforweatherforecasting. Itfollowsanencode-
process-decode layout [63], with the encoder and decoder mapping between the regular grid (lat-
lon) and an icosahedral mesh [31]. Thus, they are responsible for two tasks: first, they perform
regridding,akintoconventionalregriddingtools,butherelearned,andsecond,theymaptheinput
data into an high-dimensional latent space, as typical for deep neural networks. On the icosa-
hedral mesh in latent space, the processor component processes the data to obtain a powerful
embedding from which the time delta of the target variables can be extracted. More specifically,
theprocessorusesmessagepassinglayersinlocalneighborhoodsofeachgridcellwithadditional
long-range connections [36]. This can be understood as local stencils on the sphere that process
7information just like in a conventional finite difference solver, with the addition of some non-local
interactions between supernodes, that can further enhance predictions.
Our GraphCast has a processor with 8 layers, each performing message passing between
neigboring nodes on an icosahedral multi-mesh that has been refined 3 times (levels 0-3). The
encoder uses a bipartite graph to map between the regular grid representation and multi-mesh
nodes by assigning all grid cells to a multi-mesh node whose center is less than 0.75 times the
maximuminter-nodedistanceinthelevel3meshawayfromthatnode. Theencoderanddecoder
map between data space and a latent space with 256 channels. Like the original GraphCast
we use Swish activations and layer norm. Our message passing layer use a mean operation to
aggregate incoming information from neighboring nodes.
2.3.6 Spherical Fourier Neural Operator
Spherical Fourier Neural Operators (SFNO) [33] are an extension of the Fourier Neural Operator
(FNO) [64] to the sphere, by replacing Fourier transforms with spherical harmonics transforms
(SHT). An FNO Block performs channel-wise spatial processing in the spectral domain and com-
binesthiswithchannel-mixinginthegriddomain. TheSFNOconsistsofmanyblocks,eachusing
theSHTandinverseSHTtomapbetweengridandspectralspace. Weuselineartransformations
in spectral space and local MLPs in grid space.
2.4 Details
We train our deep neural networks using the Neural Transport Python library (https://github.
com/vitusbenson/neural_transport). OurexperimentscriptsarepublishedintheCarbonBench
Python repo (https://github.com/vitusbenson/carbonbench).
2.4.1 Optimization
WetrainourmodelswithADAMinatwo-stagefashion. First,withacosinelearningrateschedule
and linear warm up on next-step prediction. Afterwards with a constant learning rate and a n-
steps-ahead schedule, where we iteratively increase the lead time during training every 2 epochs
until 31-steps-ahead. For hyperparameter tuning and ablation studies, we do next-step training
for 100k steps, and for the final models for 300k steps. In this work, we optimize always against
thefull3DCO fieldfromCarbonTracker,futureworkmayconsideradditionallyincludingapartof
2
the ObsPack measurements (which are only used for evaluation in this work) or weighting targets
differently.
2.4.2 CentFlux
We scale and shift the model output with the std. dev. and mean of the temporal deltas of each
target variable vertical layer. Afterwards, we add the previous time step 3D field to obtain a raw
prediction for the next time step. In addition, we add the surface fluxes to the lowest vertical layer.
Duetosteadilyrisinganthropogenicemissions,theinputCO meanisincreasingovertime,which
2
would represent a covariate shift, to which neural networks are rarely robust. To account for this,
we center the input CO field at each time step to have zero mean. This fix should allow stable
2
transport for arbitrary levels of atmospheric CO . Throughout this manuscript we call the addition
2
of surface fluxes at the lowest vertical level and the centering of CO input fields jointly CentFlux.
2
2.4.3 SpecLoss
Previous work identified divergence in the power spectra to be symptomatic for models becoming
unstableforlongerrollouts[65]. Toimproveinthisregardweintroduceanadditionallosstermthat
regularizes predictions. SpecLoss measures the difference in spectral power densities between
observedandpredicted2Dfields(i.e. ateachverticallevel). Weleveragethesphericalharmonics
8transform to obtain spectral coefficients, from which we compute the spectral power density. Our
approach is similar to a regularization term used in NeuralGCM [35].
2.4.4 Massfixer
Tracer transport fulfills the continuity equation, which stems from mass conservation, in other
words, the total mass of simulated CO in the atmosphere at t + 1 should match the mass at
2
t plus the total mass input through the surface fluxes. While some conventional numerical ap-
proaches like finite volume methods fulfill tracer mass conservation by design, others, such as
semi-lagrangian or pseudo-spectral schemes do not. Also deep neural networks are only softly
constrained to fulfill mass conservation (if zero emulation error is achieved, mass is necessarily
conserved). Similarly to previous attempts to correct conventional approaches [66], we adopt a
simplemassfixer,thatscalesthepredictedmassateachtimestepbythedesiredmasscalculated
fromthesurfacefluxes. Thisfixerleadstoproportionallylargeradjustmentsingridcellswithmore
tracer mass.
3 Results
3.1 Model intercomparison
We evaluate global and local test set performance of the four neural network architectures, each
with tuned hyperparameters, and report the results in fig. 3. UNet, GraphCast, SFNO and Swin-
Transformerallachievestabletransportforatleast6monthswithlocalperformancealmostequal
toTM5,thatis,tothegroundtruththatmodelshadbeentrainedon. ThebestmodelisSwinTrans-
former, which achieves a global R2 of 0.99 over quarterly forecasts, i.e. almost perfect emulation.
Performancedegradeswhenlookingattheotherthreemodels,withUNet>SFNO>GraphCast.
Here, GraphCast has more than double the global RMSE compared to SwinTransformer, but still
stays below 1 ppm over 90 day forward runs. Furthermore, GraphCast runs become unstable
after 178 days, while the other three models display decorrelation times above 3 years, indicating
long-term stability (fig. 3a). At station level, the difference are of lower magnitude, but still signif-
icant (fig. 3c&d). In the following we assess the performance of the SwinTransformer, the best
performing model, in more detail, with the equivalent plots for the other models provided in the
supplementary material.
3.2 Best performing model
TheSwinTransformerproducesstableforecastsintermsofRMSE,R2,relativemeanandrelative
std. dev. over 90 days. Fig. 4 compares the performance for different levels. Mostly, the perfor-
mance varies little for different layers, with the exception that the surface layer has a significantly
largerRMSEcomparedtoallotherlayers(over1.5x). Moreover,whileinthelowertroposphereaf-
ter a brief annealing phase during the first few forecast steps the predictions are of approximately
constantquality,thereisadriftwithincreasedperformancedegradationintheupperstratosphere
(the top three layers).
Qualitatively, SwinTransformer captures the large-scale motion of CO in the atmosphere, as
2
depicted bymaps oftotal columnCO (fig. 5). The largesterrors appear ineastern Asia, aregion
2
known for large anthropogenic emission. Otherwise, error patterns appear to follow fronts in the
atmospheric field, indicating mildly decreased performance over sharper gradients (fig. 5).
Zooming in on a few stations from the ObsPack Globalview product, SwinTransformer gener-
allyperformssimilartothetrainingtargetTM5(fig.6). Interestingly,fortheSvalbardstation,Swin-
Transformer captures the seasonal cycle in the observations well, whereas TM5 oversmoothes
it. There are barely any jumps visible at the quarterly intervals (grey dotted lines), where the
SwinTransformer initial state is reset. This is in line with the previous result, that SwinTransformer
9a1000 b 1.00 Test Data
Global
800 Station
0.75
600
0.50
400
0.25
200
0 0.00
c d
1 3
0.8
TM5 TM5
2
0.6
0.4
1
0.2
0 0
UNet GraphCast SFNO SwinTransformer UNet GraphCast SFNO SwinTransformer
Figure 3: Intercomparison between the best models per architecture. In blue (a&b), the perfor-
mance is evaluated by scoring the global predicted 3D field against the ground truth CO field
2
from the test period of the LowRes dataset â€“ this allows for comparisons between the AI mod-
els. In orange (c&d), the performance is evaluated at ObsPack stations. This allows, in addition,
to compare against TM5 (dashed black lines), the transport model used to produce the ground
truth dataset. At ObsPack stations, in addition to the mean scores, we also display uncertainty
estimates: the std. dev. over stations scaled by the square root of the number of stations. Local
R2 (c) and global (b) and local RMSE (d) are computed for quarterly 90-day forward runs, the
decorrelation time (a) is estimated from a single 3 year forward run.
10
]syad[
emit
noitalerroceD
2R
]mpp[
ESMR
]mpp[
ESMR2.0
>90 days stable 1.0 Hybrid Level [hPa]
1013
1.5 1005
995
971
1.0 0.5 943
843
642
0.5 441
243
>90 days stable 73
0.0 0.0
1.002 1.002
>90 days stable >90 days stable
1.000 1.000
0.998 0.998
0 30 60 90 0 30 60 90
Lead time [days] Lead time [days]
Figure4: KeymetricsperverticallayerforquarterlyforecastsoverthetestsetforSwinTransformer.
We report metrics per time step and vertical level, i.e. they represent properties of the 2D maps
of atmospheric CO mass mixing ratios at different vertical levels. The metrics are averaged
2
over quarterly reset 90-day forward runs. Dashed lines indicate arbitrarily set thresholds which
subjectively signify stable simulation (e.g. RMSE<1 ppm is a goal for many CO MRV systems).
2
Target Prediction Targ - Pred
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â°
402.65 404.77 406.89 409.01 411.13 -1.00 -0.50 0.00 0.50 1.00
CO2 molemix [ppm] Delta [ppm]
Figure5: MapsofTotalColumnCO Target,PredictionbySwinTransformerandErrorfordifferent
2
lead times. Shown is a single forward run starting from Jan 1st, 2018.
11
naeM
graT
/ naeM
derP
daeha
syad
7
daeha
syad
03
daeha
syad
09
]mpp[
ESMR
dtS
graT
/ dtS
derP
2RNy-Alesund, Svalbard, Level 1 Cold Bay, Alaska, Level 4
440
420
400
Mauna Loa, Hawaii, Level 426 Ascension Island, Level 1
420
415
410
405
400
Crozet Island, Level 1 Palmer Station, Antarctica, Level 4
415
410
405
400
2018 May Sep 2019 May Sep 2020 May Sep 2021 2018 May Sep 2019 May Sep 2020 May Sep 2021
Observed Inversion Predicted
Figure 6: Performance of SwinTransformer (orange line) compared to TM5 (the training target,
here: Inversion, green line) at six measurement stations from the ObsPack Globalview product.
Shownare90-dayforwardruns,thelightgreylinesindicatethedatesonwhichtherunsarereset.
displayslittleperformancedegradationover90dayhorizons. WhileitisunclearexactlywhySwin-
Transformer outperfroms TM5 in Svalbard, it may be related to the stations vicinity to the poles
and differences in the boundary layer vertical transport of the two models.
3.3 Mass Conservation
Fig. 7 presents global and per-level mass conservation results with SwinTransformer. Globally
SwinTransformerwiththemassfixerachievesanRMSEof0.00058PgC,whichmaybeconsidered
neglible in comparison to the total atmospheric mass of âˆ¼ 865 PgC in 2018. This remaining
mass error likely stems from numerical problems: our deep neural networks operate with 32-
bit floating points, which can give performance issues especially when dealing with division of
relatively large numbers. Notably, the mass fixer greatly enhances the conservative properties of
SwinTransformer in comparison to the free-running neural network (purple line, fig. 7 left side): it
has over 0.01% relative mass RMSE, which particularly manifests in an overprediction of mass in
november and december.
Analyzing the mass error per vertical layer gives insight into the vertical transport learned
by SwinTransformer. Fig. 7, right side, indicates that the upward vertical transport is too weak
in northern hemisphere winter (too little mass in upper stratosphere) and too strong in summer.
Notably, vertical transport in the lower layers close to the surface displays little mass error, albeit
those layers being more heavily influenced by diurnal variability and surface fluxes.
3.4 Long-term Stability
While this paper mostly focuses on prediction horizons up to 90 days, we also performed a 3-year
rolloutoftheSwinTransformeroverthefulltestperiod. SwinTransformerremainsstableevenafter
over 3 years rollout, but starts to display errors above 1ppm in many regions (fig. 9).
12
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC875
W/ Mass fixer 73
RMSE: 0.00058 PgC 0.10
Rel. RMSE: 0.00007 % 243
870 W/o Mass fixer
RMSE: 0.14136 PgC 441
Rel. RMSE: 0.01633 %
0.05
642
865
843
0.00
943 860 Target Atmosphere
Predicted Atmosphere 971
Predicted w/o Massfixer 0.05
Land Flux 995
855
Ocean Flux
1005
Anthropogenic Flux 0.10
Flux Sum 1013
850
2018 Mar May Jul Sep Nov 2019 Mar May Jul Sep Nov 2019
Figure 7: Mass Conservation of SwinTransformer globally (left) and per level (right). In the left
panel, the total mass in the target atmosphere (grey line) and in the predicted atmosphere (red
line) match exactly with a cumulative sum of the surface fluxes (orange dotted line), i.e. they
are plotted on top of each other indicating mass conservation. The flux sum is the sum of the
Anthropogenic (brown), Land (green) and Ocean (blue) fluxes. In addition, we show performance
without the massfixer (purple line). The right panel shows the difference of the total mass per
levelandtimestepbetweentheSwinTransformerprediction(afterapplyingthemassfixer)andthe
target.
2.0
217.0 days stable 1.0 Hybrid Level [hPa]
1013
1.5 1005
995
971
1.0 0.5 943
843
642
0.5 441
243
284.75 days stable 73
0.0 0.0
1.0050 1.0050
351.5 days stable 51.5 days stable
1.0025 1.0025
1.0000 1.0000
0.9975 0.9975
0.9950 0.9950
0 360 720 1080 0 360 720 1080
Lead time [days] Lead time [days]
Figure 8: Key metrics per vertical layer for a single 3-year rollout with SwinTransformer starting
fromJan1st,2018. Asinfig.4,wereportmetricspertimestepandverticallevel. Themetricsare
averagedoverquarterlyreset90-dayforwardruns. Dashedlinesindicatearbitrarilysetthresholds.
13
]CgP[
ssaM
latoT
naeM
graT
/ naeM
derP
]mpp[
ESMR
dtS
graT
/ dtS
derP
2R
]aPh[
leveL
dirbyH
]CgP[
rorre
ssaMTarget Prediction Targ - Pred
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â°
404.95 411.02 417.10 423.17 429.25 -1.00 -0.50 0.00 0.50 1.00
CO2 molemix [ppm] Delta [ppm]
Figure 9: Maps of surface layer (1013 hPa in a standard atmosphere) CO Target, Prediction by
2
SwinTransformer and Error for different lead times of a 3-year rollout starting from Jan 1st, 2018.
More specifically, the surface layer RMSE first crosses 1 ppm after 217 days (fig. 8) and the
RMSE near the surface generally displays cyclical behavior, with highest errors in northern hemi-
sphere summer. The highest layer, representing the upper stratosphere, is unstable over rollout
time: itisbeingoversmoothedandaccumulatestoolittlemassovertime. Formostinversemodel-
ing purposes, this is of lesser concern, as the upper stratosphere contains less carbon and there
are typically no direct measurements of CO taken at such altitude.
2
Overall the results are particularly promising as previous work has repeatedly noted chal-
lenges in the stability of long-term rollouts of neural network-based PDE emulators [33, 67, 68].
Moreover,CO transportmaybeconsideredparticularlychallengingasatmosphericCO concen-
2 2
trations keep rising, naturally pushing the distribution of the atmospheric tracer field away from
thetrainingdistributionandconstitutinganout-of-domain(OOD)problem. Still,futureworkneeds
to assess the robustness of our models to distribution shifts beyond the rise in CO during the
2
test set. For example considering generalization to significantly different surface fluxes could be
relevant. While preliminary experiments with transporting zeroed-out surface fluxes indicated no
non-physical behavior, caution needs to preside and thus extrapolation far from training data may
be a limitation of the transport emulator.
3.5 Differences between AI model architectures
The four AI models included in this study build on different underlying principles (mesh-based
vs. pseudo-spectral vs. brute-force). Hence it is less surprising that there are differences in the
patterns of model residuals between models. Fig. 10 presents RMSE patterns. For all models,
RMSE seemingly scales with CO variability: regions with large biosphere dynamics such as the
2
tropics or boreal forests, and areas with large anthropogenic emissions such as eastern Asia
in the near-surface layers have consistently larger errors. UNet, SFNO and GraphCast all have
higher errors at the poles. For SFNO it is very limited to the pole grid cell itself, likely because the
spherical harmonics there do not allow for zonal variability. For UNet, the impact is a bit larger,
mirroring the smoothing effect of convolutions with zero padding at the poles. GraphCast has
14
daeha
syad
081
daeha
syad
563
daeha
syad
0801UNet 1.98
60Â°N 60Â°N 60Â°N 1.76
1.54
30Â°N 30Â°N
1.32
1.10
0Â° 0Â°
0.88
30Â°S 30Â°S 0.66
0.44
60Â°S 60Â°S 60Â°S
0.22
180Â° 120Â°W 60Â°W 0Â° 60Â°E 120Â°E 180Â° 10131005 995 971 943 843 642 441 243 73 0.00
Pressure Level [hPa]
GraphCast
1.98
60Â°N 60Â°N 60Â°N
1.76
1.54
30Â°N 30Â°N
1.32
1.10
0Â° 0Â°
0.88
30Â°S 30Â°S 0.66
0.44
60Â°S 60Â°S 60Â°S
0.22
180Â° 120Â°W 60Â°W 0Â° 60Â°E 120Â°E 180Â° 10131005 995 971 943 843 642 441 243 73 0.00
Pressure Level [hPa]
SFNO
1.98
60Â°N 60Â°N 60Â°N
1.76
1.54
30Â°N 30Â°N
1.32
1.10
0Â° 0Â°
0.88
30Â°S 30Â°S 0.66
0.44
60Â°S 60Â°S 60Â°S
0.22
180Â° 120Â°W 60Â°W 0Â° 60Â°E 120Â°E 180Â° 10131005 995 971 943 843 642 441 243 73 0.00
Pressure Level [hPa]
SwinTransformer 1.98
60Â°N 60Â°N 60Â°N 1.76
1.54
30Â°N 30Â°N
1.32
1.10
0Â° 0Â°
0.88
30Â°S 30Â°S 0.66
0.44
60Â°S 60Â°S 60Â°S
0.22
180Â° 120Â°W 60Â°W 0Â° 60Â°E 120Â°E 180Â° 10131005 995 971 943 843 642 441 243 73 0.00
Pressure Level [hPa]
Figure 10: RMSE patterns of the four AI models. For each model, shown is the RMSE per hori-
zontal grid cell averaged over time and vertical level (left side) and per latitude and vertical level
averaged over time and longitude (right side). Scores are for quarterly 90-day forward runs.
15
edutitaL
edutitaL
edutitaL
edutitaL
]mpp[
ESMR
]mpp[
ESMR
]mpp[
ESMR
]mpp[
ESMRModel CentFlux SpecLoss #Params Decorr Time R2 RMSE
UNet S âœ— âœ— 9.6M 1.5 0.07 > 100
UNet S âœ“ âœ— 9.6M > 90 0.98 0.57
UNet S âœ“ âœ“ 9.6M > 90 0.98 0.52
UNet XS âœ“ âœ“ 2.7M > 90 0.98 0.62
UNet M âœ“ âœ“ 35.7M > 90 0.98 0.52
GraphCast XS âœ— âœ— 5.2M 41.25 0.87 1.63
GraphCast XS âœ“ âœ— 5.2M > 90 0.95 0.96
GraphCast XS âœ“ âœ“ 5.2M > 90 0.96 0.86
GraphCast XXS âœ“ âœ“ 1.3M > 90 0.95 0.92
GraphCast S âœ“ âœ“ 8.8M > 90 0.96 0.87
GraphCast XS mesh=0â€“2 âœ“ âœ“ 5.2M > 90 0.94 0.99
SFNO M âœ— âœ— 35.7M > 90 0.97 0.67
SFNO M âœ“ âœ— 35.7M > 90 0.98 0.59
SFNO M âœ“ âœ“ 35.7M > 90 0.98 0.58
SFNO S âœ“ âœ“ 8.9M > 90 0.98 0.59
SFNO L âœ“ âœ“ 53.5M > 90 0.98 0.59
SwinTransformer M âœ— âœ— 37.9M > 90 0.97 0.79
SwinTransformer M âœ“ âœ— 37.9M > 90 0.99 0.37
SwinTransformer M âœ“ âœ“ 37.9M > 90 0.99 0.34
SwinTransformer S âœ“ âœ“ 6.4M > 90 0.99 0.36
SwinTransformer L âœ“ âœ“ 85.2M > 90 0.99 0.34
SwinTransformer M ps=4 âœ“ âœ“ 38.8M > 90 0.97 0.70
Table 1: Ablation study highlighting the best configuration per model architecture (underline) and
thebestoverallmodel(bold). Foreachmodel,wecomparethreedifferentsizes,whethertocenter
the input CO field to account for covariate shift and to add surface fluxes directly to the lowest
2
vertical layer (Centering & Flux Addition, i.e. CentFlux), and, whether to leverage an additional
losstermwhichmeasuresdivergenceinthespectralpowerdensities(SpecLoss). ForGraphCast,
we additionally ablate the resolution of the icosahedral multi-mesh (mesh, default is 0â€“3), and for
SwinTransformer,weablatethepatchsize(ps,defaultis1). Wereportthreemetrics: decorrelation
time, R2 and RMSE - all over 90-day forward runs.
the most severe problems with the poles. This might be related to the encoder and decoder of
GraphCast,whichmapbetweengridcellsontheregulargridandnodesontheicosahedralmesh.
Near the poles, many grid cells are mapped to a single node, which could potentially result in
stability problems.
3.6 Ablations
InourexperimentswefoundthefourAImodelstonotworkverywellforCO predictionout-of-the-
2
box. Especially the mesh-based methods UNet and GraphCast displayed issues with low stability
over longer rollouts. In contrast, the final models presented in this paper are stable and mass-
conserving for over 90 days. Table 1 presents insights into the design choices that lead to the
improved performance on the LowRes dataset.
For each of the four models, we ablate the model size and two training tricks that particularly
increased the stability. The first one, CentFlux, is a combination of centering the 3D CO fields
2
and adding the prescribed surface fluxes to the lowest vertical layer. The second one, SpecLoss,
isanadditionallosstermthatpenalizesdeviationsinthespectralpowerspectrum(computedwith
the spherical harmonic transform) between the model output and the target.
For all models, CentFlux is essential to achieve stable rollouts and improves the performance
16100
faster & less accurate faster & more accurate
Model
UNet
75 GraphCast
SFNO
SwinTransformer
50
TM5
Resolution
25 LowRes
MidRes
OrigRes
0
25
50
75
slower & less accurate slower & more accurate
100
100 75 50 25 0 25 50 75 100
RMSE Improvement [%]
Figure 11: Pareto frontier of inference runtime vs. model performance (RMSE at ObsPack Glob-
alviewplus stations), plotted relative to the TM5 OrigRes target data. Runtime improvements are
excluding IO and based on estimated TM5 runtimes.
significantly. SpecLoss additionally enhances scores, but the gains are smaller. The four models
have different optimal model sizes. While the best GraphCast in our experiment (size XS), has
5.2Mparameters,thebestUNet(sizeS)has9.6M,andthebestSFNOandSwinTransformer(both
size M) have 35.7M and 37.9M parameters respectively. Note, models with more parameters do
not necessarily have better performance: UNet outperforms SFNO slightly on RMSE. Still, that in
our experiments it was significantly more challenging to scale GraphCast to larger size compared
to SwinTransformer is probably one of the reasons why it is the worst model architecture in our
intercomparison.
3.7 Computational Costs
One reason why AI-based emulators of ERA5 have garnered interest is because they offer signif-
icant speed-ups over conventional NWP models at inference time. Conceptually these speed ups
arise as most ERA5 emulators use 10x less vertical layers (13 instead of 137), 30x higher time
step(6hinsteadof12min),purelyexplicitsolvers(noiterativeschemeforimplicitstepsnecessary)
and compute accelerators (GPUs/TPUs instead of CPUs). These speed ups at inference time
comewithatrade-off: firstsignificantcomputeresourcesneedtobeallocatedinordertotrainthe
models. ForNWPsuchaninvestmentisoftenquicklyjustified,asmanymodelrunsarenecessary
every day.
Naturally, one may wonder if a parallel to neural network emulation of atmospheric transport
canbedrawn. However,theSwinTransformerisnotsignificantlyfastercomparedtoTM5(fig.11).
Performance here is highly hardware dependent, but as a rough estimate SwinTransformer takes
âˆ¼ 1.5sec for a 30 day forward run on a single Nvidia A40 GPU. Fig. 11 compares the speed of
the four AI models and TM5 at different resolutions. For this, we measured the model time in an
idealizedscenario,removingallpre-andpostprocessingofmodelinputsandoutputs,andinstead
directly reading and writing the raw tensors from memory. We then measure the speed of 30 day
forecasts with 10 repetitions on a Nvidia A40 GPU. Generally, we notice only small differences
between the AI models.
RunningtheTM5-MPmodel,whichimprovesuponTM5throughOpenMPIparallelization[69],
takes âˆ¼ 8 minutes on a machine with 24 CPUs for a 1 month forward run on a 3â—¦ Ã—2â—¦ grid and
âˆ¼ 2 minutes on 6â—¦ Ã—4â—¦ [70]. We assume 50% time is spent in IO and plot estimated runtimes for
TM5withoutIOinFig.11,withOrigResandMidResrunstotake4minutesandLowRestotake1
minute on a single modern machine with 24 CPUs.
17
]%[
tnemevorpmI
emitnuRThe lack of speed-up can possibly be explained with a number of factors. First, TM5 is run
on a 2â—¦ Ã—3â—¦ grid, which does not require an extremely small time step. Second, TM5 uses about
the same number of vertical layers as SwinTransformer. Third, tracer transport in TM5 is entirely
linear(inthesurfacefluxes),andthemassfluxesforeachgridcellarepre-computed. Afterthisis
done, transport becomes cheap. Fourth, while TM5 still does not run on GPUs, it reaps a number
of benefits from its maturity, such as leveraging fast FORTRAN code and parallelization through
OpenMPI.
4 Discussion
In this work, we trained deep neural networks to emulate the atmospheric transport of CO . We
2
test four models and find SwinTransformer to perform best, with almost perfect emulation for 90
days, and stable and mass-conserving emulation for multiple years ahead. For this we adjust
the model architecture, decoupling the drift in CO from its dynamics by leveraging centered CO
2 2
fieldsasinputsandusingapost-hocfluxschemetocorrectthemassbalance. Yet,thepresented
model is not giving large computational advantages compared to conventional approaches, at
least not at low resolution.
Storm-resolving models allow for explicit treatment of convection, with large impact on vertical
transport of air masses and CO . Some modeling centers are already experimenting with storm-
2
resolving transport model runs [71â€“74], which typically require to run an online transport model.
Here, AI models could leverage model output and offer an alternative route ahead.
Considering higher resolution might offer room for speed ups: doubling the horizontal reso-
lution of conventional solvers increases the computation costs by roughly 10x [75]. Yet, some of
the errors of transport representation in current inverse modeling schemes are attributed to low
resolution[19,20]. Hence,developingmulti-resolutiontrainingschemes,e.g. byutilizingthecross
attentionmechanism[76â€“79], whichis straight-forwardwiththedata inCarbonBench, mayenable
moreaccuratelow-resolutionmodelsthatarestillcomputationallyfeasibleforinversemodelingby
emulatingthehighresolutionsolvers. Moreover,modelingtheatmosphereinahighlycompressed
space may yield further improvements [80], for instance, such a transport model could render the
usage of full resolution wind fields from ERA5 feasible.
Furthermore, there is still a lot of room for common techniques used to speed up AI models.
Model distillation is a technique to significantly reduce the parameter count of neural networks
without loosing much in terms of skill. Quantization leverages lower numerical precision to de-
crease memory footprint and increase speed. On a programming language level, just-in-time
compilation, e.g. through torchscript, can speed up certain operations. And more generally, data
loading can be optimized through asynchronous techniques, clever caching and parallelization.
Futureworkmayalsoexploretheapplicabilityoftheneuralnetworksolversforinversemodel-
ing,thatisinferringsurfacefluxesfromobservedatmosphericmeasurements. Theimplementation
of the neural networks is fully differentiable, which opens new avenues for obtaining the sensitiv-
ities required for the inversions. Furthermore, some inverse modeling approaches rely on the
creation of large ensembles. Since neural networks natively support batched processing, there is
potential for speed ups (generating a full ensemble can be as cheap as a single forward run).
Open Research Section
We construct the CarbonBench dataset from existing open data from CarbonTracker North Amer-
icaversionCT2022[55](http://doi.org/10.25925/z1gj-3254)andfromObsPackGLOBALVIEW-
plus CO v9.1 [57] (http://doi.org/10.25925/20231201). We provide code that downloads the
2
dataefficientlyfromtheoriginaldataprovidersandprocessesitintotheformatsusedinthisstudy,
yet in line with the original data licenses we do not re-distribute the data.
We publish the code to run all our experiments and reproduce the results in this paper in
the CarbonBench Python repo (https://github.com/vitusbenson/carbonbench). The deep
18neural networks are implemented in the Neural Transport Python library (https://github.com/
vitusbenson/neural_transport),aversatilesoftwarepackagecontainingdatasetcreation,data
loading, training and evaluation routines intended to be easily usable in other research projects.
Acknowledgments
VB is grateful for stimulating discussions to Fabian Gans, Maximilian Gelbrecht, Martin Heimann,
Martin Jung, Albrecht Schall, Sam Upton and many others at MPI Jena and ETH ZuÂ¨rich. AW,
CR & MR acknowledge funding by the European Research Council (ERC) Synergy Grant Un-
derstanding and modeling the Earth System with Machine Learning (USMILE) under the Horizon
2020 research and innovation programme (Grant agreement No. 855187).
195 Supplementary Information
3
8.75 days stable 1.0 Hybrid Level [hPa]
1013
1005
2 995
971
0.5 943
843
1 642
441
243
>90 days stable 73
0 0.0
1.0050 1.0050
21.75 days stable 7.75 days stable
1.0025 1.0025
1.0000 1.0000
0.9975 0.9975
0.9950 0.9950
0 30 60 90 0 30 60 90
Lead time [days] Lead time [days]
Figure 12: Same as fig. 4 but for UNet.
20
naeM
graT
/ naeM
derP
]mpp[
ESMR
dtS
graT
/ dtS
derP
2R2.0
0.75 days stable 1.0 Hybrid Level [hPa]
1013
1.5 1005
995
971
1.0 0.5 943
843
642
0.5 441
243
2.75 days stable 73
0.0 0.0
1.002 1.002
75.75 days stable >90 days stable
1.000 1.000
0.998 0.998
0 30 60 90 0 30 60 90
Lead time [days] Lead time [days]
Figure 13: Same as fig. 4 but for GraphCast.
2.0
11.75 days stable 1.0 Hybrid Level [hPa]
1013
1.5 1005
995
971
1.0 0.5 943
843
642
0.5 441
243
>90 days stable 73
0.0 0.0
1.002 1.002
>90 days stable >90 days stable
1.000 1.000
0.998 0.998
0 30 60 90 0 30 60 90
Lead time [days] Lead time [days]
Figure 14: Same as fig. 4 but for SFNO.
21
naeM
graT
/ naeM
derP
naeM
graT
/ naeM
derP
]mpp[
ESMR
]mpp[
ESMR
dtS
graT
/ dtS
derP
dtS
graT
/ dtS
derP
2R
2RTarget Prediction Targ - Pred
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â°
402.65 404.77 406.89 409.01 411.13 -1.00 -0.50 0.00 0.50 1.00
CO2 molemix [ppm] Delta [ppm]
Figure 15: Same as fig. 5 but for UNet.
Target Prediction Targ - Pred
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â°
402.65 404.77 406.89 409.01 411.13 -1.00 -0.50 0.00 0.50 1.00
CO2 molemix [ppm] Delta [ppm]
Figure 16: Same as fig. 5 but for GraphCast.
22
daeha
syad
7
daeha
syad
03
daeha
syad
09
daeha
syad
7
daeha
syad
03
daeha
syad
09Target Prediction Targ - Pred
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
30Â°N
0Â°
30Â°S
180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â° 180Â°120Â°W60Â°W 0Â° 60Â°E120Â°E180Â°
402.65 404.77 406.89 409.01 411.13 -1.00 -0.50 0.00 0.50 1.00
CO2 molemix [ppm] Delta [ppm]
Figure 17: Same as fig. 5 but for SFNO.
Ny-Alesund, Svalbard, Level 1 Cold Bay, Alaska, Level 4
440
420
400
Mauna Loa, Hawaii, Level 426 Ascension Island, Level 1
420
415
410
405
400
Crozet Island, Level 1 Palmer Station, Antarctica, Level 4
415
410
405
400
2018 May Sep 2019 May Sep 2020 May Sep 2021 2018 May Sep 2019 May Sep 2020 May Sep 2021
Observed Inversion Predicted
Figure 18: Same as fig. 6 but for UNet.
23
daeha
syad
7
daeha
syad
03
daeha
syad
09
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OCNy-Alesund, Svalbard, Level 1 Cold Bay, Alaska, Level 4
440
420
400
Mauna Loa, Hawaii, Level 426 Ascension Island, Level 1
420
415
410
405
400
Crozet Island, Level 1 Palmer Station, Antarctica, Level 4
415
410
405
400
2018 May Sep 2019 May Sep 2020 May Sep 2021 2018 May Sep 2019 May Sep 2020 May Sep 2021
Observed Inversion Predicted
Figure 19: Same as fig. 6 but for GraphCast.
Ny-Alesund, Svalbard, Level 1 Cold Bay, Alaska, Level 4
440
420
400
Mauna Loa, Hawaii, Level 426 Ascension Island, Level 1
420
415
410
405
400
Crozet Island, Level 1 Palmer Station, Antarctica, Level 4
415
410
405
400
2018 May Sep 2019 May Sep 2020 May Sep 2021 2018 May Sep 2019 May Sep 2020 May Sep 2021
Observed Inversion Predicted
Figure 20: Same as fig. 6 but for SFNO.
24
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC
]mpp[
ximelom
2OC875
W/ Mass fixer 73
RMSE: 0.00091 PgC 0.10
Rel. RMSE: 0.00011 % 243
870 W/o Mass fixer
RMSE: 0.12216 PgC 441
Rel. RMSE: 0.01412 %
0.05
642
865
843
0.00
943 860 Target Atmosphere
Predicted Atmosphere 971
Predicted w/o Massfixer 0.05
Land Flux 995
855
Ocean Flux
1005
Anthropogenic Flux 0.10
Flux Sum 1013
850
2018 Mar May Jul Sep Nov 2019 Mar May Jul Sep Nov 2019
Figure 21: Same as fig. 7 but for UNet.
875
W/ Mass fixer 73
RMSE: 0.00081 PgC 0.10
Rel. RMSE: 0.00009 % 243
870 W/o Mass fixer
RMSE: 0.32055 PgC 441
Rel. RMSE: 0.03712 %
0.05
642
865
843
0.00
943 860 Target Atmosphere
Predicted Atmosphere 971
Predicted w/o Massfixer 0.05
Land Flux 995
855
Ocean Flux
1005
Anthropogenic Flux 0.10
Flux Sum 1013
850
2018 Mar May Jul Sep Nov 2019 Mar May Jul Sep Nov 2019
Figure 22: Same as fig. 7 but for GraphCast.
875
W/ Mass fixer 73
RMSE: 0.00093 PgC 0.10
Rel. RMSE: 0.00011 % 243
870 W/o Mass fixer
RMSE: 0.13109 PgC 441
Rel. RMSE: 0.01517 %
0.05
642
865
843
0.00
943 860 Target Atmosphere
Predicted Atmosphere 971
Predicted w/o Massfixer 0.05
Land Flux 995
855
Ocean Flux
1005
Anthropogenic Flux 0.10
Flux Sum 1013
850
2018 Mar May Jul Sep Nov 2019 Mar May Jul Sep Nov 2019
Figure 23: Same as fig. 7 but for SFNO.
25
]CgP[
ssaM
latoT
]CgP[
ssaM
latoT
]CgP[
ssaM
latoT
]aPh[
leveL
dirbyH
]aPh[
leveL
dirbyH
]aPh[
leveL
dirbyH
]CgP[
rorre
ssaM
]CgP[
rorre
ssaM
]CgP[
rorre
ssaMReferences
[1] P. Friedlingstein et al. â€œGlobal Carbon Budget 2023â€. In: Earth System Science Data 15.12
(2023), pp. 5301â€“5369. DOI: 10.5194/essd-15-5301-2023.
[2] T.KaminskiandM.Heimann.â€œInverseModelingofAtmosphericCarbonDioxideFluxesâ€.In:
Science 294.5541 (2001), pp. 259â€“259. DOI: 10.1126/science.294.5541.259a.
[3] K. R. Gurney, R. M. Law, A. S. Denning, P. J. Rayner, D. Baker, P. Bousquet, L. Bruhwiler,
Y.-H. Chen, P. Ciais, S. Fan, I. Y. Fung, M. Gloor, M. Heimann, K. Higuchi, J. John, T. Maki,
S. Maksyutov, K. Masarie, P. Peylin, M. Prather, B. C. Pak, J. Randerson, J. Sarmiento,
S. Taguchi, T. Takahashi, and C.-W. Yuen. â€œTowards Robust Regional Estimates of CO2
Sources and Sinks Using Atmospheric Transport Modelsâ€. In: Nature 415.6872 (2002),
pp. 626â€“630. DOI: 10.1038/415626a.
[4] P.Ciais,P.Rayner,F.Chevallier,P.Bousquet,M.Logan,P.Peylin,andM.Ramonet.â€œAtmo-
spheric Inversions for Estimating CO2 Fluxes: Methods and Perspectivesâ€. In: Greenhouse
Gas Inventories: Dealing With Uncertainty. Ed. by M. Jonas, Z. Nahorski, S. Nilsson, and
T. Whiter. Dordrecht: Springer Netherlands, 2011, pp. 69â€“92. DOI: 10.1007/978-94-007-
1670-4_6.
[5] G. P. Brasseur and D. J. Jacob. Modeling of Atmospheric Chemistry. 1st ed. Cambridge
University Press, 2017. DOI: 10.1017/9781316544754.
[6] D. L. Williamson. â€œReview of Numerical Approaches for Modeling Global Transportâ€. In: Air
Pollution Modeling and Its Application IX. Ed. by H. Van Dop and G. Kallos. Boston, MA:
Springer US, 1992, pp. 377â€“394. DOI: 10.1007/978-1-4615-3052-7_38.
[7] A.E.Schuh,A.R.Jacobson,S.Basu,B.Weir,D.Baker,K.Bowman,F.Chevallier,S.Crow-
ell,K.J.Davis,F.Deng,S.Denning,L.Feng,D.Jones,J.Liu,andP.I.Palmer.â€œQuantifying
theImpactofAtmosphericTransportUncertaintyonCO2SurfaceFluxEstimatesâ€.In:Global
Biogeochemical Cycles 33.4 (2019), pp. 484â€“500. DOI: 10.1029/2018GB006086.
[8] B. Gaubert, B. B. Stephens, S. Basu, F. Chevallier, F. Deng, E. A. Kort, P. K. Patra, W.
Peters, C. RoÂ¨denbeck, T. Saeki, D. Schimel, I. Van der Laan-Luijkx, S. Wofsy, and Y. Yin.
â€œGlobal Atmospheric CO Inverse Models Converging on Neutral Tropical Land Exchange,
2
but Disagreeing on Fossil Fuel and Atmospheric Growth Rateâ€. In: Biogeosciences 16.1
(2019), pp. 117â€“134. DOI: 10.5194/bg-16-117-2019.
[9] D. A. Belikov, S. Maksyutov, M. Krol, A. Fraser, M. Rigby, H. Bian, A. Agusti-Panareda,
D. Bergmann, P. Bousquet, P. Cameron-Smith, M. P. Chipperfield, A. Fortems-Cheiney, E.
Gloor, K. Haynes, P. Hess, S. Houweling, S. R. Kawa, R. M. Law, Z. Loh, L. Meng, P. I.
Palmer,P.K.Patra,R.G.Prinn,R.Saito,andC.Wilson.â€œOff-LineAlgorithmforCalculation
of Vertical Tracer Transport in the Troposphere Due to Deep Convectionâ€. In: Atmospheric
Chemistry and Physics 13.3 (2013), pp. 1093â€“1114. DOI: 10.5194/acp-13-1093-2013.
[10] S.Munassar,G.Monteil,M.Scholze,U.Karstens,C.RoÂ¨denbeck,F.-T.Koch,K.U.Totsche,
and C. Gerbig. â€œWhy Do Inverse Models Disagree? A Case Study with Two European CO
2
Inversionsâ€. In: Atmospheric Chemistry and Physics 23.4 (2023), pp. 2813â€“2828. DOI: 10.
5194/acp-23-2813-2023.
[11] M.Remaud,J.Ma,M.Krol,C.Abadie,M.P.Cartwright,P.Patra,Y.Niwa,C.Rodenbeck,S.
Belviso,L.Kooijmans,S.Lennartz,F.Maignan,F.Chevallier,M.P.Chipperfield,R.J.Pope,
J.J.Harrison,I.Vimont,C.Wilson,andP.Peylin.â€œIntercomparisonofAtmosphericCarbonyl
Sulfide (TransCom-COS; Part One): Evaluating the Impact of Transport and Emissions on
Tropospheric Variability Using Ground-Based and Aircraft Dataâ€. In: Journal of Geophysical
Research: Atmospheres 128.6 (2023), e2022JD037817. DOI: 10.1029/2022JD037817.
[12] A.E.SchuhandA.R.Jacobson.â€œUncertaintyinParameterizedConvectionRemainsaKey
Obstacle for Estimating Surface Fluxes of Carbon Dioxideâ€. In: Atmospheric Chemistry and
Physics 23.11 (2023), pp. 6285â€“6297. DOI: 10.5194/acp-23-6285-2023.
26[13] R. Kretschmer, C. Gerbig, U. Karstens, and F.-T. Koch. â€œError Characterization of CO Ver-
2
tical Mixing in the Atmospheric Transport Model WRF-VPRMâ€. In: Atmospheric Chemistry
and Physics 12.5 (2012), pp. 2441â€“2458. DOI: 10.5194/acp-12-2441-2012.
[14] Y. Jin, R. F. Keeling, B. B. Stephens, M. C. Long, P. K. Patra, C. RoÂ¨denbeck, E. J. Morgan,
E. A. Kort, and C. Sweeney. â€œImproved Atmospheric Constraints on Southern Ocean CO2
Exchangeâ€.In:ProceedingsoftheNationalAcademyofSciences121.6(2024),e2309333121.
DOI: 10.1073/pnas.2309333121.
[15] A. Agusti-Panareda, M. Diamantakis, V. Bayona, F. Klappenbach, and A. Butz. â€œImproving
the Inter-Hemispheric Gradient of Total Column Atmospheric CO and CH in Simulations
2 4
withtheECMWFSemi-LagrangianAtmosphericGlobalModelâ€.In:GeoscientificModelDe-
velopment 10.1 (2017), pp. 1â€“18. DOI: 10.5194/gmd-10-1-2017.
[16] S. D. Eastham and D. J. Jacob. â€œLimits on the Ability of Global Eulerian Models to Resolve
Intercontinental Transport of Chemical Plumesâ€. In: Atmospheric Chemistry and Physics
17.4 (2017), pp. 2543â€“2553. DOI: 10.5194/acp-17-2543-2017.
[17] K. Yu, C. A. Keller, D. J. Jacob, A. M. Molod, S. D. Eastham, and M. S. Long. â€œErrors and
ImprovementsintheUseofArchivedMeteorologicalDataforChemicalTransportModeling:
An Analysis Using GEOS-Chem V11-01 Driven by GEOS-5 Meteorologyâ€. In: Geoscientific
Model Development 11.1 (2018), pp. 305â€“319. DOI: 10.5194/gmd-11-305-2018.
[18] B. Zhang, H. Liu, J. H. Crawford, G. Chen, T. D. Fairlie, S. Chambers, C.-H. Kang, A. G.
Williams, K. Zhang, D. B. Considine, M. P. Sulprizio, and R. M. Yantosca. â€œSimulation of
Radon-222 with the GEOS-Chem Global Model: Emissions, Seasonality, and Convective
Transportâ€. In: Atmospheric Chemistry and Physics 21.3 (2021), pp. 1861â€“1887. DOI: 10.
5194/acp-21-1861-2021.
[19] M. Remaud, F. Chevallier, A. Cozic, X. Lin, and P. Bousquet. â€œOn the Impact of Recent
Developments of the LMDz Atmospheric General Circulation Model on the Simulation of
CO
2
Transportâ€. In: Geoscientific Model Development 11.11 (2018), pp. 4489â€“4513. DOI:
10.5194/gmd-11-4489-2018.
[20] A. AgustÂ´Ä±-Panareda, M. Diamantakis, S. Massart, F. Chevallier, J. MunËœoz-Sabater, J. BarreÂ´,
R. Curcoll, R. Engelen, B. Langerock, R. M. Law, Z. Loh, J. A. MorguÂ´Ä±, M. Parrington, V.-H.
Peuch, M. Ramonet, C. Roehl, A. T. Vermeulen, T. Warneke, and D. Wunch. â€œModelling
CO Weatherâ€“WhyHorizontalResolutionMattersâ€.In:AtmosphericChemistryandPhysics
2
19.11 (2019), pp. 7347â€“7376. DOI: 10.5194/acp-19-7347-2019.
[21] C. RoÂ¨denbeck, S. Houweling, M. Gloor, and M. Heimann. â€œCO Flux History 1982â€“2001
2
Inferred from Atmospheric Data Using a Global Inversion of Atmospheric Transportâ€. In:
Atmospheric Chemistry and Physics 3.6 (2003), pp. 1919â€“1964. DOI: 10.5194/acp-3-
1919-2003.
[22] F. Chevallier, M. Fisher, P. Peylin, S. Serrar, P. Bousquet, F.-M. BreÂ´on, A. CheÂ´din, and P.
Ciais. â€œInferring CO2 Sources and Sinks from Satellite Observations: Method and Applica-
tion to TOVS Dataâ€. In: Journal of Geophysical Research: Atmospheres 110.D24 (2005).
DOI: 10.1029/2005JD006390.
[23] C. RoÂ¨denbeck. Estimating CO2 Sources and Sinks from Atmospheric Mixing Ratio Mea-
surements Using a Global Inversion of Atmospheric Transport. 2005.
[24] F. Chevallier, N. Viovy, M. Reichstein, and P. Ciais. â€œOn the Assignment of Prior Errors
in Bayesian Inversions of CO2 Surface Fluxesâ€. In: Geophysical Research Letters 33.13
(2006). DOI: 10.1029/2006GL026496.
[25] W. Peters, A. R. Jacobson, C. Sweeney, A. E. Andrews, T. J. Conway, K. Masarie, J. B.
Miller, L. M. P. Bruhwiler, G. PeÂ´tron, A. I. Hirsch, D. E. J. Worthy, G. R. van der Werf, J. T.
Randerson, P. O. Wennberg, M. C. Krol, and P. P. Tans. â€œAn Atmospheric Perspective on
NorthAmericanCarbonDioxideExchange:CarbonTrackerâ€.In:ProceedingsoftheNational
Academy of Sciences 104.48 (2007), pp. 18925â€“18930. DOI: 10.1073/pnas.0708986104.
27[26] I. T. van der Laan-Luijkx, I. R. van der Velde, E. van der Veen, A. Tsuruta, K. Stanislawska,
A. Babenhauserheide, H. F. Zhang, Y. Liu, W. He, H. Chen, K. A. Masarie, M. C. Krol,
and W. Peters. â€œThe CarbonTracker Data Assimilation Shell (CTDAS) v1.0: Implementation
andGlobalCarbonBalance2001â€“2015â€.In:GeoscientificModelDevelopment 10.7(2017),
pp. 2785â€“2800. DOI: 10.5194/gmd-10-2785-2017.
[27] C. RoÂ¨denbeck, S. Zaehle, R. Keeling, and M. Heimann. â€œHistory of El NinËœo Impacts on
the Global Carbon Cycle 1957â€“2017: A Quantification from Atmospheric CO2 Dataâ€. In:
Philosophical Transactions of the Royal Society B: Biological Sciences 373.1760 (2018),
p. 20170303. DOI: 10.1098/rstb.2017.0303.
[28] N.Chandra,P.K.Patra,Y.Niwa,A.Ito,Y.Iida,D.Goto,S.Morimoto,M.Kondo,M.Takigawa,
T. Hajima, and M. Watanabe. â€œEstimated Regional CO Flux and Uncertainty Based on an
2
Ensemble of Atmospheric CO Inversionsâ€. In: Atmospheric Chemistry and Physics 22.14
2
(2022), pp. 9215â€“9243. DOI: 10.5194/acp-22-9215-2022.
[29] F. Chevallier, Z. Lloret, A. Cozic, S. Takache, and M. Remaud. â€œToward High-Resolution
Global Atmospheric Inverse Modeling Using Graphics Acceleratorsâ€. In: Geophysical Re-
search Letters 50.5 (2023), e2022GL102135. DOI: 10.1029/2022GL102135.
[30] K.Bi,L.Xie,H.Zhang,X.Chen,X.Gu,andQ.Tian.Pangu-Weather:A3DHigh-Resolution
Model for Fast and Accurate Global Weather Forecast. 2022.
[31] R.Keisler.â€œForecastingGlobalWeatherwithGraphNeuralNetworksâ€.In:arXiv:2202.07575
[physics] (2022).
[32] J. Pathak, S. Subramanian, P. Harrington, S. Raja, A. Chattopadhyay, M. Mardani, T. Kurth,
D.Hall,Z.Li,K.Azizzadenesheli,P.Hassanzadeh,K.Kashinath,andA.Anandkumar.Four-
CastNet:AGlobalData-drivenHigh-resolutionWeatherModelUsingAdaptiveFourierNeu-
ral Operators. 2022. DOI: 10.48550/arXiv.2202.11214.
[33] B. Bonev, T. Kurth, C. Hundt, J. Pathak, M. Baust, K. Kashinath, and A. Anandkumar.
Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere. 2023. DOI:
10.48550/arXiv.2306.03838.
[34] K. Chen, T. Han, J. Gong, L. Bai, F. Ling, J.-J. Luo, X. Chen, L. Ma, T. Zhang, R. Su, Y. Ci,
B.Li,X.Yang,andW.Ouyang.FengWu:PushingtheSkillfulGlobalMedium-rangeWeather
Forecast beyond 10 Days Lead. 2023. DOI: 10.48550/arXiv.2304.02948.
[35] D. Kochkov, J. Yuval, I. Langmore, P. Norgaard, J. Smith, G. Mooers, J. Lottes, S. Rasp, P.
DuÂ¨ben,M.KloÂ¨wer,S.Hatfield,P.Battaglia,A.Sanchez-Gonzalez,M.Willson,M.P.Brenner,
andS.Hoyer.NeuralGeneralCirculationModels.2023. DOI:10.48550/arXiv.2311.07222.
[36] R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri,
T. Ewalds, Z. Eaton-Rosen, W. Hu, A. Merose, S. Hoyer, G. Holland, O. Vinyals, J. Stott,
A. Pritzel, S. Mohamed, and P. Battaglia. â€œLearning Skillful Medium-Range Global Weather
Forecastingâ€. In: Science 0.0 (2023), eadi2336. DOI: 10.1126/science.adi2336.
[37] I. Price, A. Sanchez-Gonzalez, F. Alet, T. Ewalds, A. El-Kadi, J. Stott, S. Mohamed, P.
Battaglia, R. Lam, and M. Willson. GenCast: Diffusion-based Ensemble Forecasting for
Medium-Range Weather. 2023. DOI: 10.48550/arXiv.2312.15796.
[38] V. M. Krasnopolsky and M. S. Fox-Rabinovitz. â€œComplex Hybrid Models Combining Deter-
ministic and Machine Learning Components for Numerical Climate Modeling and Weather
Predictionâ€. In: Neural Networks. Earth Sciences and Environmental Applications of Com-
putational Intelligence 19.2 (2006), pp. 122â€“134. DOI: 10.1016/j.neunet.2006.01.002.
[39] T.Arcomano,I.Szunyogh,A.Wikner,J.Pathak,B.R.Hunt,andE.Ott.â€œAHybridApproach
to Atmospheric Modeling That Combines Machine Learning With a Physics-Based Numeri-
calModelâ€.In:JournalofAdvancesinModelingEarthSystems14.3(2022),e2021MS002712.
DOI: 10.1029/2021MS002712.
[40] T. Nguyen, J. Brandstetter, A. Kapoor, J. K. Gupta, and A. Grover. ClimaX: A Foundation
Model for Weather and Climate. 2023. DOI: 10.48550/arXiv.2301.10343.
28[41] C.Lessig,I.Luise,B.Gong,M.Langguth,S.Stadtler,andM.Schultz.AtmoRep:AStochas-
ticModelofAtmosphereDynamicsUsingLargeScaleRepresentationLearning.2023. DOI:
10.48550/arXiv.2308.13280.
[42] C. Bodnar, W. P. Bruinsma, A. Lucic, M. Stanley, J. Brandstetter, P. Garvan, M. Riechert,
J. Weyn, H. Dong, A. Vaughan, J. K. Gupta, K. Tambiratnam, A. Archibald, E. Heider, M.
Welling, R. E. Turner, and P. Perdikaris. Aurora: A Foundation Model of the Atmosphere.
2024. DOI: 10.48550/arXiv.2405.13063.
[43] L. Cartwright, A. Zammit-Mangion, and N. M. Deutscher. â€œEmulation of Greenhouse-Gas
Sensitivities Using Variational Autoencodersâ€. In: Environmetrics 34.2 (2023), e2754. DOI:
10.1002/env.2754.
[44] E.Fillola,R.Santos-Rodriguez,A.Manning,S.Oâ€™Doherty,andM.Rigby.â€œAMachineLearn-
ing Emulator for Lagrangian Particle Dispersion Model Footprints: A Case Study Using
NAMEâ€. In: EGUsphere (2022), pp. 1â€“19. DOI: 10.5194/egusphere-2022-1174.
[45] T.-L. He, N. Dadheech, T. M. Thompson, and A. J. Turner. FootNet: Development of a Ma-
chine Learning Emulator of Atmospheric Transport. 2023.
[46] G. McNicol, E. Fluet-Chouinard, Z. Ouyang, S. Knox, Z. Zhang, T. Aalto, S. Bansal, K.-Y.
Chang, M. Chen, K. Delwiche, S. Feron, M. Goeckede, J. Liu, A. Malhotra, J. R. Melton,
W. Riley, R. Vargas, K. Yuan, Q. Ying, Q. Zhu, P. Alekseychik, M. Aurela, D. P. Billesbach,
D. I. Campbell, J. Chen, H. Chu, A. R. Desai, E. Euskirchen, J. Goodrich, T. Griffis, M.
Helbig, T. Hirano, H. Iwata, G. Jurasinski, J. King, F. Koebsch, R. Kolka, K. Krauss, A. Lo-
hila, I. Mammarella, M. Nilson, A. Noormets, W. Oechel, M. Peichl, T. Sachs, A. Sakabe,
C. Schulze, O. Sonnentag, R. C. Sullivan, E.-S. Tuittila, M. Ueyama, T. Vesala, E. Ward, C.
Wille, G. X. Wong, D. Zona, L. Windham-Myers, B. Poulter, and R. B. Jackson. â€œUpscaling
Wetland Methane Emissions From the FLUXNET-CH4 Eddy Covariance Network (UpCH4
v1.0): Model Development, Network Assessment, and Budget Comparisonâ€. In: AGU Ad-
vances 4.5 (2023), e2023AV000956. DOI: 10.1029/2023AV000956.
[47] M.Jung,M.Reichstein,H.A.Margolis,A.Cescatti,A.D.Richardson,M.A.Arain,A.Arneth,
C. Bernhofer, D. Bonal, J. Chen, D. Gianelle, N. Gobron, G. Kiely, W. Kutsch, G. Lasslop,
B. E. Law, A. Lindroth, L. Merbold, L. Montagnani, E. J. Moors, D. Papale, M. Sottocornola,
F.Vaccari,andC.Williams.â€œGlobalPatternsofLand-AtmosphereFluxesofCarbonDioxide,
LatentHeat,andSensibleHeatDerivedfromEddyCovariance,Satellite,andMeteorological
Observationsâ€. In: Journal of Geophysical Research: Biogeosciences 116.G3 (2011). DOI:
10.1029/2010JG001566.
[48] G.Tramontana,M.Jung,C.R.Schwalm,K.Ichii,G.Camps-Valls,B.RaÂ´duly,M.Reichstein,
M. A. Arain, A. Cescatti, G. Kiely, L. Merbold, P. Serrano-Ortiz, S. Sickert, S. Wolf, and D.
Papale. â€œPredicting Carbon Dioxide and Energy Fluxes across Global FLUXNET Sites with
Regression Algorithmsâ€. In: Biogeosciences 13.14 (2016), pp. 4291â€“4313. DOI: 10.5194/
bg-13-4291-2016.
[49] M.Jung,C.Schwalm,M.Migliavacca,S.Walther,G.Camps-Valls,S.Koirala,P.Anthoni,S.
Besnard,P.Bodesheim,N.Carvalhais,F.Chevallier,F.Gans,D.S.Goll,V.Haverd,P.KoÂ¨hler,
K. Ichii, A. K. Jain, J. Liu, D. Lombardozzi, J. E. M. S. Nabel, J. A. Nelson, M. Oâ€™Sullivan,
M. Pallandt, D. Papale, W. Peters, J. Pongratz, C. RoÂ¨denbeck, S. Sitch, G. Tramontana, A.
Walker, U. Weber, and M. Reichstein. â€œScaling Carbon Fluxes from Eddy Covariance Sites
to Globe: Synthesis and Evaluation of the FLUXCOM Approachâ€. In: Biogeosciences 17.5
(2020), pp. 1343â€“1365. DOI: 10.5194/bg-17-1343-2020.
[50] J. A. Nelson, S. Walther, F. Gans, B. Kraft, U. Weber, K. Novick, N. Buchmann, M. Migli-
avacca, G. Wohlfahrt, L. SË‡igut, A. Ibrom, D. Papale, M. GoÂ¨ckede, G. Duveiller, A. Knohl, L.
HoÂ¨rtnagl, R. L. Scott, W. Zhang, Z. M. Hamdi, M. Reichstein, S. Aranda-Barranco, J. ArdoÂ¨,
M. Op de Beeck, D. Billdesbach, D. Bowling, R. Bracho, C. BruÂ¨mmer, G. Camps-Valls, S.
Chen, J. R. Cleverly, A. Desai, G. Dong, T. S. El-Madany, E. S. Euskirchen, I. Feigenwinter,
M. Galvagno, G. Gerosa, B. Gielen, I. Goded, S. Goslee, C. M. Gough, B. Heinesch, K.
29Ichii, M. A. Jackowicz-Korczynski, A. Klosterhalfen, S. Knox, H. Kobayashi, K.-M. Kohonen,
M. Korkiakoski, I. Mammarella, G. Mana, R. Marzuoli, R. Matamala, S. Metzger, L. Mon-
tagnani, G. Nicolini, T. Oâ€™Halloran, J.-M. Ourcival, M. Peichl, E. Pendall, B. Ruiz Reverter,
M. Roland, S. Sabbatini, T. Sachs, M. Schmidt, C. R. Schwalm, A. Shekhar, R. Silberstein,
M. L. Silveira, D. Spano, T. Tagesson, G. Tramontana, C. Trotta, F. Turco, T. Vesala, C.
Vincke, D. Vitale, E. R. Vivoni, Y. Wang, W. Woodgate, E. A. Yepez, J. Zhang, D. Zona,
and M. Jung. â€œX-BASE: The First Terrestrial Carbon and Water Flux Products from an Ex-
tended Data-Driven Scaling Framework, FLUXCOM-Xâ€. In: EGUsphere (2024), pp. 1â€“51.
DOI: 10.5194/egusphere-2024-165.
[51] S. Upton, M. Reichstein, F. Gans, W. Peters, B. Kraft, and A. Bastos. â€œConstraining Bio-
spheric Carbon Dioxide Fluxes by Combined Top-down and Bottom-up Approachesâ€. In:
Atmospheric Chemistry and Physics 24.4 (2024), pp. 2555â€“2582. DOI: 10.5194/acp-24-
2555-2024.
[52] P.K.Patra,M.Takigawa,S.Watanabe,N.Chandra,K.Ishijima,andY.Yamashita.â€œImproved
Chemical Tracer Simulation by MIROC4.0-Based Atmospheric Chemistry-Transport Model
(MIROC4-ACTM)â€. In: Sola 14 (2018), pp. 91â€“96. DOI: 10.2151/sola.2018-016.
[53] M. Heimann and S. KoÂ¨rner. The Global Atmospheric Tracer Model TM3. 2003.
[54] M. Krol, S. Houweling, B. Bregman, M. van den Broek, A. Segers, P. van Velthoven, W.
Peters,F.Dentener,andP.Bergamaschi.â€œTheTwo-WayNestedGlobalChemistry-Transport
Zoom Model TM5: Algorithm and Applicationsâ€. In: Atmospheric Chemistry and Physics 5.2
(2005), pp. 417â€“432. DOI: 10.5194/acp-5-417-2005.
[55] A. R. Jacobson et al. CarbonTracker CT2022. 2023. DOI: 10.25925/Z1GJ-3254.
[56] S. Rasp, S. Hoyer, A. Merose, I. Langmore, P. Battaglia, T. Russell, A. Sanchez-Gonzalez,
V. Yang, R. Carver, S. Agrawal, M. Chantry, Z. Ben Bouallegue, P. Dueben, C. Bromberg,
J. Sisk, L. Barrington, A. Bell, and F. Sha. â€œWeatherBench 2: A Benchmark for the Next
Generation of Data-Driven Global Weather Modelsâ€. In: Journal of Advances in Modeling
Earth Systems 16.6 (2024), e2023MS004019. DOI: 10.1029/2023MS004019.
[57] K. N. Schuldt et al. Multi-Laboratory Compilation of Atmospheric Carbon Dioxide Data for
the Period 1957-2022; obspack co2 1 GLOBALVIEWplus v9.1 2023-12-08. 2023. DOI: 10.
25925/20231201.
[58] O. Ronneberger, P. Fischer, and T. Brox. â€œU-Net: Convolutional Networks for Biomedical
Image Segmentationâ€. In: Medical Image Computing and Computer-Assisted Intervention â€“
MICCAI2015.Ed.byN.Navab,J.Hornegger,W.M.Wells,andA.F.Frangi.Cham:Springer
International Publishing, 2015, pp. 234â€“241. DOI: 10.1007/978-3-319-24574-4_28.
[59] S. Scher. â€œToward Data-Driven Weather and Climate Forecasting: Approximating a Simple
General Circulation Model With Deep Learningâ€. In: Geophysical Research Letters 45.22
(2018), pp. 12, 616â€“12, 622. DOI: 10.1029/2018GL080704.
[60] S.Rasp,P.D.Dueben,S.Scher,J.A.Weyn,S.Mouatadid,andN.Thuerey.â€œWeatherBench:
A Benchmark Data Set for Data-Driven Weather Forecastingâ€. In: Journal of Advances in
Modeling Earth Systems 12.11 (2020), e2020MS002203. DOI: 10.1029/2020MS002203.
[61] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. â€œSwin Transformer:
Hierarchical Vision Transformer Using Shifted Windowsâ€. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. 2021, pp. 10012â€“10022.
[62] J. D. Willard, P. Harrington, S. Subramanian, A. Mahesh, T. A. Oâ€™Brien, and W. D. Collins.
AnalyzingandExploringTrainingRecipesforLarge-ScaleTransformer-BasedWeatherPre-
diction. 2024.
30[63] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski,
A.Tacchetti,D.Raposo,A.Santoro,R.Faulkner,C.Gulcehre,F.Song,A.Ballard,J.Gilmer,
G.Dahl,A.Vaswani,K.Allen,C.Nash,V.Langston,C.Dyer,N.Heess,D.Wierstra,P.Kohli,
M.Botvinick,O.Vinyals,Y.Li,andR.Pascanu.RelationalInductiveBiases,DeepLearning,
and Graph Networks. 2018. DOI: 10.48550/arXiv.1806.01261.
[64] Z. Li, N. B. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anand-
kumar. â€œFourier Neural Operator for Parametric Partial Differential Equationsâ€. In: Interna-
tional Conference on Learning Representations. 2021.
[65] A. Chattopadhyay and P. Hassanzadeh. Long-Term Instabilities of Deep Learning-Based
Digital Twins of the Climate System: The Cause and a Solution. 2023. DOI: 10.48550/
arXiv.2304.07029.
[66] M. Diamantakis and J. Flemming. â€œGlobal Mass Fixer Algorithms for Conservative Tracer
TransportintheECMWFModelâ€.In:GeoscientificModelDevelopment 7.3(2014),pp.965â€“
979. DOI: 10.5194/gmd-7-965-2014.
[67] J. Brandstetter, D. E. Worrall, and M. Welling. â€œMessage Passing Neural PDE Solversâ€. In:
International Conference on Learning Representations. 2022.
[68] P. Lippe, B. Veeling, P. Perdikaris, R. Turner, and J. Brandstetter. â€œPDE-Refiner: Achieving
Accurate Long Rollouts with Neural PDE Solversâ€. In: Advances in Neural Information Pro-
cessing Systems 36 (2023), pp. 67398â€“67433.
[69] J. E. Williams, K. F. Boersma, P. Le Sager, and W. W. Verstraeten. â€œThe High-Resolution
Version of TM5-MP for Optimized Satellite Retrievals: Description and Validationâ€. In: Geo-
scientific Model Development 10.2 (2017), pp. 721â€“750. DOI: 10.5194/gmd-10-721-2017.
[70] A. Segers, J. Tokaya, S. Houweling, J. van Peet, and V. Huijnen. TM5-MP-4DVAR, AND
SOMETHING ON CH4 SINKS. 2020.
[71] A. AgustÂ´Ä±-Panareda, S. Massart, F. Chevallier, S. Boussetta, G. Balsamo, A. Beljaars, P.
Ciais, N. M. Deutscher, R. Engelen, L. Jones, R. Kivi, J.-D. Paris, V.-H. Peuch, V. Sherlock,
A.T.Vermeulen,P.O.Wennberg,andD.Wunch.â€œForecastingGlobalAtmosphericCO â€.In:
2
Atmospheric Chemistry and Physics 14.21 (2014), pp. 11959â€“11983. DOI: 10.5194/acp-
14-11959-2014.
[72] R. Gelaro, W. M. Putman, S. Pawson, C. Draper, A. Molod, P. M. Norris, L. Ott, N. Prive,
O. Reale, D. Achuthavarier, M. Bosilovich, V. Buchard, W. Chao, L. Coy, R. Cullather, A.
da Silva, A. Darmenov, R. Koster, W. McCarty, and S. Schubert. Evaluation of the 7-Km
GEOS-5 Nature Run. Tech. rep. 36. 2015.
[73] A. AgustÂ´Ä±-Panareda, J. McNorton, G. Balsamo, B. C. Baier, N. Bousserez, S. Boussetta, D.
Brunner, F. Chevallier, M. Choulga, M. Diamantakis, R. Engelen, J. Flemming, C. Granier,
M. Guevara, H. Denier van der Gon, N. Elguindi, J.-M. Haussaire, M. Jung, G. Janssens-
Maenhout, R. Kivi, S. Massart, D. Papale, M. Parrington, M. Razinger, C. Sweeney, A. Ver-
meulen, and S. Walther. â€œGlobal Nature Run Data with Realistic High-Resolution Carbon
Weather for the Year of the Paris Agreementâ€. In: Scientific Data 9.1 (2022), p. 160. DOI:
10.1038/s41597-022-01228-2.
[74] A. AgustÂ´Ä±-Panareda, J. BarreÂ´, S. Massart, A. Inness, I. Aben, M. Ades, B. C. Baier, G. Bal-
samo, T. Borsdorff, N. Bousserez, S. Boussetta, M. Buchwitz, L. Cantarello, C. Crevoisier,
R. Engelen, H. Eskes, J. Flemming, S. Garrigues, O. Hasekamp, V. Huijnen, L. Jones, Z.
Kipling,B.Langerock,J.McNorton,N.Meilhac,S.NoeÂ¨l,M.Parrington,V.-H.Peuch,M.Ra-
monet, M. Razinger, M. Reuter, R. Ribas, M. Suttie, C. Sweeney, J. Tarniewicz, and L. Wu.
â€œTechnical Note: The CAMS Greenhouse Gas Reanalysis from 2003 to 2020â€. In: Atmo-
spheric Chemistry and Physics 23.6 (2023), pp. 3829â€“3859. DOI: 10.5194/acp-23-3829-
2023.
31[75] T.Hoefler,B.Stevens,A.F.Prein,J.Baehr,T.Schulthess,T.F.Stocker,J.Taylor,D.Klocke,
P. Manninen, P. M. Forster, T. KoÂ¨lling, N. Gruber, H. Anzt, C. Frauen, F. Ziemen, M. KloÂ¨wer,
K. Kashinath, C. SchaÂ¨r, O. Fuhrer, and B. N. Lawrence. â€œEarth Virtualization Engines: A
Technical Perspectiveâ€. In: Computing in Science & Engineering 25.3 (2023), pp. 50â€“59.
DOI: 10.1109/MCSE.2023.3311148.
[76] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira. â€œPerceiver: Gen-
eral Perception with Iterative Attentionâ€. In: arXiv:2103.03206 [cs, eess] (2021).
[77] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zo-
ran, A. Brock, E. Shelhamer, O. HeÂ´naff, M. M. Botvinick, A. Zisserman, O. Vinyals, and
J. Carreira. â€œPerceiver IO: A General Architecture for Structured Inputs & Outputsâ€. In:
arXiv:2107.14795 [cs, eess] (2022).
[78] B. Alkin, A. FuÂ¨rst, S. Schmid, L. Gruber, M. Holzleitner, and J. Brandstetter. Universal
Physics Transformers: A Framework For Efficiently Scaling Neural Operators. 2024. DOI:
10.48550/arXiv.2402.12365.
[79] L. Serrano, T. X. Wang, E. L. Naour, J.-N. Vittaut, and P. Gallinari. AROMA: Preserving
Spatial Structure for Latent PDE Modeling with Local Neural Fields. 2024.
[80] T. Han, Z. Chen, S. Guo, W. Xu, and L. Bai. CRA5: Extreme Compression of ERA5 for
Portable Global Climate and Weather Research via an Efficient Variational Transformer.
2024. DOI: 10.48550/arXiv.2405.03376.
32