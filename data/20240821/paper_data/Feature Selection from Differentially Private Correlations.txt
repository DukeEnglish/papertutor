Feature Selection from Differentially Private Correlations
RyanSwopeâˆ— AmolKhannaâˆ— PhilipDoldoâˆ—
Swope_Ryan@bah.com Khanna_Amol@bah.com Doldo_Philip@bah.com
BoozAllenHamilton BoozAllenHamilton BoozAllenHamilton
Philadelphia,Pennsylvania,USA Boston,Massachusetts,USA Baltimore,Maryland,USA
SaptarshiRoy EdwardRaff
roysapta@umich.edu Raff_Edward@bah.com
UniversityofMichigan BoozAllenHamilton
AnnArbor,Michigan,USA UniversityofMaryland,
BaltimoreCounty
Syracuse,NewYork,USA
ABSTRACT 1 INTRODUCTION
Datascientistsoftenseektoidentifythemostimportantfeaturesin Linearmodelsremainoneofthemostcommonandwidelyused
high-dimensionaldatasets.Thiscanbedonethroughğ¿1-regularized techniquesinpracticeandresearchtoday.Inparticular,linearre-
regression,butthiscanbecomeinefficientforveryhigh-dimensional gressionandlogisticregressionarestraightforwardtosolveus-
datasets.Additionally,high-dimensionalregressioncanleakinfor- ingeithergeneral-purposeconvexsolverslikeLimited-memory
mationaboutindividualdatapointsinadataset.Inthispaper,we BFGS[28]orbespokeoptimizerslikethoseprovidedinLIBLINEAR
empiricallyevaluatetheestablishedbaselinemethodforfeature andotherlibraries[14,34].Inhigh-dimensionalsituations,where
selectionwithdifferentialprivacy,thetwo-stageselectiontech- thenumberoffeaturesğ‘‘ isgreaterthanthenumberofsamples
nique,andshowthatitisnotstableundersparsity.Thismakesit ğ‘,theneedtoperformfeatureselectiontoavoidover-determined
performpoorlyonreal-worlddatasets,soweconsideradifferent systems is particularly pertinent. While classic approaches like
approachtoprivatefeatureselection.Weemployacorrelations- forward-backwardselection[2]andmutual-information[38,49]
basedorderstatistictochooseimportantfeaturesfromadataset arestillstudied,theincorporationofsparsity-inducingpenalties
andprivatizethemtoensurethattheresultsdonotleakinformation hasbecomethepredominantapproach.
aboutindividualdatapoints.Wefindthatourmethodsignificantly Theinductionofsparsityinthesolutionisusefulfromapure
outperformstheestablishedbaselineforprivatefeatureselection engineering,practicaldeployment,andanalyticalunderstanding
onmanydatasets. sincetheğ¿1penaltywasintroducedviatheâ€œLASSOâ€regularizer
byTibshirani[47].Theğ¿1penaltyfurtherhasprovableadvantages
CCSCONCEPTS inhighdimensionalsettings[32]thathaveledtosignificanteffort
incustomsolvers[15,16,56]andthebroadpreferenceformaking
â€¢ Security and privacy â†’ Formal methods and theory of
featureselectionajointprocesswiththeregressionitself[21].
security;Databaseandstoragesecurity.
However,thewidespreadsuccessofğ¿1basedoptimizationfor
jointsolvingoffeatureselectionandmodelweightsisnotsoclear
KEYWORDS
cutwhenweareconcernedwiththeprivacyofthedatausedtobuild
Differential Privacy, Feature Selection, Model Selection, Sparse,
themodel.Insuchacase,thereisaneedtointroduceanadditional
Correlations,LinearRegression
frameworktoprotectdataprivacy.Specifically,differentialprivacy
ACMReferenceFormat: isastatisticalframeworkthatguaranteesdataprivacyinanalgo-
Ryan Swope, Amol Khanna, Philip Doldo, Saptarshi Roy, and Edward rithm[30].Givenparametersğœ–andğ›¿,onanytwodatasetsğ·and
Raff.2024.FeatureSelectionfromDifferentiallyPrivateCorrelations.In ğ·â€²differingononeexample,anapproximatedifferentiallyprivate
ProceedingsofProceedingsofthe17thACMWorkshoponArtificialIntelli- algorithmAsatisfiesPr[A(ğ·) âˆˆğ‘‚] â‰¤exp{ğœ–}Pr[A(ğ·â€²) âˆˆğ‘‚]+ğ›¿
genceandSecurity(AISecâ€™24).ACM,NewYork,NY,USA,12pages.https: foranyğ‘‚ âŠ† image(A).Notethatlowervaluesofğœ– andğ›¿ corre-
//doi.org/XXXXXXX.XXXXXXX
spondtostrongerprivacy.Differentialprivacyistypicallyachieved
byaddingcalibratedamountsofnoiseinthemechanismortothe
âˆ—Equalcontribution.
outputofA.
Differentiallyprivateregressionalgorithmsensurethattheweight
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
ofaregressiondoesnotrevealsignificantinformationaboutits
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation training data. This is especially important in high-dimensional
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe models,wheretheratioofparameterstotrainingdatapointsis
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
republish,topostonserversortoredistributetolists,requirespriorspecificpermission higher, and thus, the parameters can encode more information
and/orafee.Requestpermissionsfrompermissions@acm.org. aboutthedatapoints.Forthisreason,severaldifferentiallypri-
AISecâ€™24,October18,2024,SaltLakeCity,UT vatehigh-dimensionalregressionalgorithmshavebeendeveloped.
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
4202
guA
02
]GL.sc[
1v26801.8042:viXraAISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
However,thesealgorithmshavetoaddnoisescaledbythedi- 2 RELATEDWORK
mensionalityofthedata,whichcanquicklyoverwhelmthe Inthissection,wewilldescriberelatedworksonhigh-dimensional
signal,anddestroyallsparsityofthesolution[25].Thenoise regressioninthenon-privateandprivatesettings.Weaimtopro-
addedbydifferentialprivacyprovestobeasignificantroadblock videanoverviewofthespacewhilefocusingoncorrelation-based
tosuccessfulDPapproachesthatinducehard-zerosasthefeature selectionstrategiesinthenon-privatesetting.Wemakenotethat
selectionstepbecausethenoisecontinuallyknockssolutionsaway thenotionsofğ¿1basedfeatureselectionasaprocessindependent
fromexactzerocomponents.Forthisreason,wechoosetoexplore oftheoptimizationprocesshasbeendevelopedinthenon-private
differentiallyprivatefeatureselectionstrategieswhichreducethe settingviaâ€œscreeningrulesâ€[27,29,33,37,50]thatattempttoiden-
dimensionalityofthedatasetpriortousingprivateregressionalgo- tifycoefficientswhichwillhaveazerovalueoncetheoptimization
rithms. isdone,andsocanbediscardedearlyforcomputationalefficiency.
Fewpastworkshaveconsidereddifferentiallyprivatefeature Unfortunately,negativeresultshavebeenshownforconverting
selection,andofthosethathave,mostproducecomputationally screeningrulesintoadifferentiallyprivateform[23].Itisforthis
infeasibleprocedures[39,45].Kiferetal.[26]developaâ€œtwo-stageâ€ reasonthatwelookbacktotheolderapproachofaseparatefeature
methodwhichiscomputationallyfeasiblebutstillintensiveasit selectionprocessfrommodeltrainingtoseeifimprovedresults
requirestrainingmultiplemodels.Itisinspiredbytheğ¿1penalty canbeobtained.Below,wewillreviewthepertinentnon-private
asitinvolvestrainingmultipleğ¿1-regularizedmodels,butakey andprivatehigh-dimensionalregressionliterature.
insightweempiricallydemonstrateisthatğ¿1penalizationisnot
algorithmicallystable,leadingtoinconsistentperformance 2.1 Non-privateHigh-dimensionalRegression
underdifferentialprivacy.
Statisticianstypicallyconstrainthestructureofhigh-dimensional
Asimpleyeteffectivenon-privatefeatureselectionalgorithm
regressions.Onesuchconstraintiscoefficientsparsity,whichwe
isSureIndependenceScreening(SIS),whichselectstheğ‘˜features
willfocusoninthispaper.
withthehighestabsolutecorrelationtothetargetvector[13].This
methodisalsoflexible,allowinguserstoswitchoutcorrelation
Constrainingsparsityisequivalenttoğ¿0-constrainedorpenal-
izedregression.Indeed,itisnotdifficulttoseethatforvaryingğœ†,
withanyothermetrictheybelievetobebettersuitedtoidentifying
bothof
importantvariables.Wecomparethenon-privateSISalgorithmto
ğ‘
anon-privateversionof[26]todisambiguatetheimpactofnoise âˆ‘ï¸
vsalgorithmicinstabilityof[26].ThistestshowsthatSISperforms
argmin â„“(xğ‘–,ğ‘¦ ğ‘–;w) (1)
better,andthusisolatestheinstabilityofğ¿1-regularizationasa
wâˆˆRğ‘‘:âˆ¥wâˆ¥0â‰¤ğœ†ğ‘–=1
ğ‘
majorfactorinthelowerperformanceof[26].Wethenprovide âˆ‘ï¸
intuitionforwhytheSISalgorithmwillperformbetterinthepri-
argmin â„“(xğ‘–,ğ‘¦ ğ‘–;w)+ğœ†âˆ¥wâˆ¥0 (2)
vatesettingthanthetwo-stageapproachandproceedtoprivatize
wâˆˆRğ‘‘ ğ‘–=1
SIS.OurexperimentsshowthatprivateSISperformsbetter
willproducesolutionsofvaryingsparsity.However,ğ¿0-constrained
orpenalizedregressionisNP-hard,makingitimpossibletofind
thantwo-stageonavarietyofhigh-dimensionaldatasets,
solutionstotheseproblemsinpolynomialtime[48].
achievingsimilarorimprovedaccuraciesforğœ– inausable
rangeof[1,10],whilebeingeasytoreasonandprove.
Tomakeoptimizationfeasible,ğ¿1-constrainedorregularized
regressioncanbeused.Here,theoptimizationfunctions
Here,weprovideanoverviewofthefollowingsections.Insec-
tion2,wewillreviewrelatedworksonnon-privateandprivate ğ‘
âˆ‘ï¸
featureselectionstrategies.Insection3,wewillcompareanon- argmin â„“(xğ‘–,ğ‘¦ ğ‘–;w) (3)
privateversionoftwo-stagewithasimplecorrelation-basedse- wâˆˆRğ‘‘:âˆ¥wâˆ¥1â‰¤ğœ†ğ‘–=1
lectionapproachandshowthatthebaselineissignificantlyless ğ‘
âˆ‘ï¸
stableundersparsitythanthesimpleapproach.Insection4,we argmin â„“(xğ‘–,ğ‘¦ ğ‘–;w)+ğœ†âˆ¥wâˆ¥1 (4)
willdescribehowtomakethiscorrelation-basedselectionprivate, wâˆˆRğ‘‘ ğ‘–=1
andwillprovidemathematicalargumentshighlightingwhenour are convex and feasible. However, employing ğ¿1 constraints or
methodwillperformwell.section5willdetailexperimentsand penaltiescreatesabiasawayfromtheğ¿0 solution,meaningthe
theirresults,withsection6concludingthepaper.Wemakespecial optimalwinEquations(3)and(4)willnotequalthoseinEquations
notethattoimplementausefulandprivateSISfeatureselector, (1)or(2)[48].Nevertheless,DonohoandHuoandDonohoand
aprivatetop-ğ‘˜ selectionstepwasneeded.Theonlymethodfor Eladshowedthatthesupportsetsofpenalizedğ¿0solutionscanbe
top-ğ‘˜selectionthatimprovedoverthebaselinewasthecanonical foundthroughoptimizationwithğ¿1penaltieswhenwissufficiently
Lipschitzmechanism[40],whichinitsoriginalpresentationisdif- sparse[9,10].
ficulttoproveandunderstand.Asanadditionalcontributionto However,evenifconvexoptimizationisfeasible,itcanbein-
thiswork,were-stateandsimplifytheexpositionofthisvaluable creasingly difficult on very high-dimensional datasets. For this
techniqueinAppendixAtoimproveitsutility. reason,statisticianshavedevelopedavarietyoffeatureselection
mechanismstoremoveirrelevantfeaturespriortooptimization.
Wedescribeasubsetofthesehere.
Oneofthefirstmethodsforfeatureselectionwhichdoesnot
optimize a variant of Equations (3) or (4) is Sure Independence
Screening(SIS)[13].ThismethodtreatseachfeatureasindependentFeatureSelectionfromDifferentiallyPrivateCorrelations AISecâ€™24,October18,2024,SaltLakeCity,UT
andmeasuresthecorrelationbetweeneachfeatureandthetarget inthispaper.Instead,wecommentthatsimilartothenon-private
variable.Thetop-ğ‘˜absolutelycorrelatedfeaturesareretained,with case,thesetechniquesbecomeincreasinglyinefficientwhenthe
theothersbeingscreenedoutpriortooptimization.Experiments dimensionalityofthedataisveryhigh,andamethodtoreducethe
withSISdemonstratethatitworksparticularlywellondatawith supportsetpriortooptimizationwouldbeuseful[25].Additionally,
independentfeaturesbutcanalsoperformreasonablywellwhen theutilityofeachofthesemethodsreliesonthedimensionality
featuresaremildlycorrelated. oftheirinputdata,andasthedimensionalityincreases,theirper-
SimilartoSISarescreeningrules,whichseektoboundthedual formancewilldecrease[36].Ifinsteadthesealgorithmsreceived
solutions of linear optimization problems within a compact set asupportsetofreasonablesizeafterfeatureselection,theycould
[52].Thiscompactsetcanidentifyfeatureswhicharesurelynot operatewithbetterexpectedutility.
partofthefeaturesetselectedbyğ¿1-regularizedestimators,but Giventheusefulnessofaprivatefeatureselectionmechanism,
candosowithoutperformingoptimization.Inthisway,screening wechoosetostudytheeffectivenessoffeatureselectionfrompri-
rulescansafelyremovefeaturespriortooptimizationtomakethe vatecorrelations.Weempiricallycomparetheperformanceofour
optimizationproblemmorefeasible. methodtothefeatureselectionstageofthetwo-stageapproach,as
Thecompactsetswhichscreeningrulesusecanbespheresor thisistheonlyprivatefeatureselectionstrategywhichiscomputa-
spheresincombinationwithhalfspaces.Asthenumberofhalfs- tionallyefficient.
pacesemployedgrows,thecomputationalcomplexityofscreening
grows,andoftenwithmarginalbenefits.Asaresult,sphericalsets 3 EVALUATINGTHEINSTABILITYOFTHE
areoftenused.Interestingly,inthecaseofsphericalsets,there
TWO-STAGEAPPROACH
existsanequivalencebetweenSISandscreeningrulesforsome
Alineofrecentworksonprivatehigh-dimensionalregressionhas
regularizationvalueofğœ†[52].Thisequivalencedemonstratesthat
discussedtheroleofalgorithmicinstabilityinKiferet.al.â€™stwo-
itisreasonabletoselectfeaturesforğ¿1-regularizedproblemsbased
stageapproach[23â€“25,36].However,noneoftheseworksseemto
oncorrelations.
haveevaluatedwhetherthisinstabilityaffectsactualperformance
throughanempiricallens.
2.2 PrivateHigh-dimensionalRegression Inthissection,weruntwosimpleexperimentstoshowthateven
inthenon-privatesetting,thetwo-stageapproachsuffersfromin-
High-dimensionalregressionhasalsobeenconsideredindiffer-
stabilitycomparedtothesimpleSISbaseline.Inthefirstexperiment,
entialprivacy.Wereviewsomekeyworkshere,butseethecited
surveybyKhannaetal.foramorecomprehensiveview[25].
wegenerate100datapointsXâˆ¼N(0,I100)whereeachxğ‘– âˆˆR100.
Kiferetal.firstconsideredhigh-dimensionaldifferentiallypri- Wethenuseaweightvectorw1= (cid:2)1 1 1 1 1 0 Â·Â·Â· 0(cid:3)âŠ¤
withvalue1incomponents1through5andvalue0incomponents
vateregressionbybuildingatwo-stageprocedure[26].Thefirst
stageofthisprocedureprivatelychoosesasupportsetofsizeğ‘˜by
6through100.Finally,wegeneratetargetsğ‘¦
ğ‘–
=xğ‘–âŠ¤w1+ğœ– ğ‘–,where
splittingthedataintoâˆš
ğ‘
blocksofsizeâˆš
ğ‘ andprivatelycomput-
ğœ–
ğ‘–
âˆ¼N(0,0.1).
Withthisdataset,weemployedthefirststageofKiferet.al.â€™s
ingwhichfeaturesaremostconsistentlyincludedinthesupportsof
two-stagemethodwithanon-privateselectionsteptoidentifya
regressionestimatorsbuiltoneachblock.Oncethealgorithmfinds âˆš
supportset.Wedidthisbysplittingthedatainto ğ‘ blocksofsize
asupportset,ittrainsafinalmodelrelyingononlythefeatures âˆš
inthesupportset.Tothebestofourknowledge,thisalgorithmis ğ‘ andidentifyingthe5featu âˆšreswhichweremostconsistently
theonlycomputationallyefficientalgorithmwhichemploysfea- includedinthesupportsofthe ğ‘ regressionestimators.Wealso
tureselectiontobuildhigh-dimensionalregressionestimatorswith employedthenon-privateSISalgorithmonthisdataset.SISchose
differentialprivacy. asupportsetbyidentifyingwhich5featuresinthedatasetwere
Heuristically,weseetwochallengeswithKiferetal.â€™sapproach. mostcorrelatedwiththetargetvariable.
First,theiralgorithmiscomputationallyintensive:itrequiresbuild- Thisexperimentwasrepeated1000times,andresultsarein-
âˆš
ing ğ‘ +1high-dimensionalregressionestimators,whichmay cludedasexperimentW1inFigure1.ItisclearthatSISchoosesthe
notbepossibleifthedimensionalityofthedataisveryhigh.Sec- truenonzerofeaturesmuchmoreoftenthanthetwo-stagemethod,
ond,theiralgorithmreliesonthealgorithmicstabilityofsparse makingitabetterfeature-selectionmechanismforthisproblem.
estimators:theyneeddisjointpartitionsofthedatasettoagreeon Theaboveexperimentdemonstratesthatanintuitivemethod
whichfeaturesshouldbeincludedinthesupportset.Thismaybe likeSIScanoutperformthetwo-stagemethodonasimpledataset.
anunreasonableassumption,aseveninthenon-privatesetting, However,conditionsofreal-datasetsarerarelysoideal.Tosim-
sparseestimatorsarenotalgorithmicallystable[53].Combining ulatehowoutliersaffectthetwo-stageandSISmethods,weem-
theirrequirementforalgorithmicallystablesparsesupportselec- ploythefollowingexperiment.Wegenerate100datapointsX âˆ¼
tionwithaneedforprivate(noisy)supportselectionmayrender N(0,I100) where each xğ‘– âˆˆ R100, like above. We then use two
thefinalsupportsetineffective.Thisheuristicanalysisisthereason weightvectors:w1= (cid:2)1 1 1 1 1 0 Â·Â·Â· 0(cid:3)âŠ¤andw2=
whywechosetostudyaprocedurewhichdidnotrequirebuilding (cid:2)0 Â·Â·Â· 0 1 1 1 1 1(cid:3)âŠ¤.w1 thesamevectordescribed
regressionestimatorsforsupportselection. inthepreviousexperiment,whereasw2hasvalue0incomponents
Othermethodsfordifferentiallyprivatehigh-dimensionalre- 1through95andvalue1incomponents96through100.Finally,
gressionexist,andtheytypicallyrelyonprivateoptimizationtech- usingthefirst90datapoints,wegeneratey1=X[1:90,:]w1+ğ[1:90]
niques.Wedonotprovidedetailsontheseoptimizationtechniques and y2 = X[91:100,:]w2 +ğ[91:100], whereğœ– ğ‘– âˆ¼ N(0,0.1). When
here,sincethisisout-of-scopeforthemethodsandexperiments constructingourfinaldataset,wechoosetorepeatedlyintersperseAISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
correlationbetweeneachfeatureandthetargetvariable.Thetop-ğ‘˜
absolutelycorrelatedfeaturesareretained.Toprivatelyfindthetop-
ğ‘˜highestabsolutelycorrelatedfeatureswithatargetvariable,we
needtocomputeprivatecorrelations.Todothis,weneedtobound
thesensitivityofthedotproductbetweenafeatureandthetarget
vector.Unlikethenon-privateSISalgorithm,whichcentersand
normalizeseachfeaturetoavarianceof1,weboundthesensitivity
bycenteringandnormalizingeachfeaturetoamaximumabsolute
valueof1.OnereasonwechoosetoprivatizeSISisbecauseitis
easytounderstandandcanbeeasilyadapted.
Inthefollowingsteps,XâˆˆRğ‘Ã—ğ‘‘ isthedesignmatrix,withx(ğ‘–)
representingtheğ‘–thcolumn(orfeature)ofX.yâˆˆRğ‘ isthevector
oftargets.DP-SISworksasfollowsinAlgorithm1:
Algorithm1DP-SIS
Require:
DesignmatrixXâˆˆRğ‘Ã—ğ‘‘,targetvectoryâˆˆRğ‘,privacy
parameterğœ–.
1: CentereachcolumninXandcentery.Thisensuresthateach
xâŠ¤ yrepresentsacorrelation.
(ğ‘–)
2: ScaleeachcolumninXsothatthemaximumabsolutevalue
ofanyelementineachcolumnvectoris1.Scaleysothatthe
maximumabsolutevalueofanyelementinthevectoris1.This
ensuresthatthesensitivityof|xâŠ¤ y|is1.
(ğ‘–)
3: Employaprivatetop-ğ‘˜ selectionstrategywithparameterğœ–
giventhatthesensitivityof|xâŠ¤ y|is1.Theâ€œscoreâ€orâ€œqualityâ€
(ğ‘–)
offeatureğ‘– ismeasuredwiththeabsolutecorrelationofthe
featurewiththetarget,namely|xâŠ¤ y|.Theprivatetop-ğ‘˜algo-
(ğ‘–)
rithmreturnsğ‘˜indices,correspondingtotheselectedfeatures.
Figure1:Occurrencesofselectingfeatureğ‘›inthetwo-stage
Thisalgorithmissimpletounderstandandimplement,butwe
and SIS algorithms. An ideal result for both experiments
believethatitmayoutperformKiferetal.â€™stwo-stageapproach
wouldselectfeatures1through51000timesandallother
sinceitemploysallthedataanddoesnotrelyonthealgorithmic
featureszerotimes.SISisclosertotheidealresultthantwo-
stabilityofsparseestimators.Inthefollowingsection,weprovide
stage.
atheoreticalunderstandingofthisalgorithm.
NotethatforDP-SIStoworkitisnecessarytouseahigh-quality
privatetop-ğ‘˜selectionalgorithm.Weinitiallytestedmechanisms
9datapointsgeneratedfromw1with1datapointgeneratedfrom
in[35],[17],and[11],butfoundthattheyaddedtoomuchnoise
w2sothateachdisjointblockofthetwo-stagealgorithmreceives
toyieldfavorableresults.However,wefoundthatthecanonical
oneoutlier.
LipschtizalgorithmbyShekelyan&Loukidesdoeswork,thoughit
Thisexperimentwasrepeated1000times,andtheresultsare
isnoteasilyaccessibleinitsoriginalpresentation.Asasignificant
includedinFigure1asW1/W2.Itisclearthateveninthepresence
componentofourwork,were-derivedandformalizedthecanonical
oftheseoutliers,SISstillchoosesthenonzerocomponentscorre-
Lipschtizmechanism,presentingitinamoreaccessibleway.This
spondingw1 muchmorefrequentlythanthetwo-stagemethod.
explanationisplacedintheappendix,highlightingthatwearenot
Thisisdesirableifwebelievethatdatapointsgeneratedwithw2
claimingShekelyan&Loukidesinnovation,butwebelievethat
should be attributed to noise, and it demonstrates that SIS can
ourexplanationwasnecessaryandwillsupportotherworksinthe
outperformthetwo-stagemethodinthepresenceofsuchnoise.
future.
4 PRIVATIZINGSIS
4.2 TheoreticalAnalysis
Inthissection,wewilldetailourapproachtoprivatefeaturese-
Tobetterunderstandwhenthismethodworkswell,weprovidethe
lection.Wewillbeginbydescribingouralgorithmandgoonto
followinganalysis.WeseektoidentifytheprobabilitythatDP-SIS
provideatheoreticalanalysis.
identifiesthesamefeaturesasnon-privateSIS.
4.1 FeatureSelectionfromPrivateCorrelations Theorem4.1. LetXandybethedesignmatrixandtargetvector
GiventheperformanceofSISintheprevioussection,ouralgorithm after transformations in steps 1 and 2. Then |(XâŠ¤y)ğ‘–| = |xâŠ¤ (ğ‘–)y|.
employsaprivatizedversionofSIS.Asareminder,SISmeasuresthe Denote (cid:2) |XâŠ¤y|(cid:3) tobetheğ‘—thlargestelementin|XâŠ¤y|.
ğ‘—FeatureSelectionfromDifferentiallyPrivateCorrelations AISecâ€™24,October18,2024,SaltLakeCity,UT
Wearetryingtofindthetop-ğ‘˜highestabsolutelycorrelatedfeatures Dataset n p R2
withthetargetvariable.Assume (cid:2) |XâŠ¤y|(cid:3) âˆ’(cid:2) |XâŠ¤y|(cid:3) =ğœ‰.Then
ğ‘˜ ğ‘˜+1 Alon 62 2000 0.4822
theprobabilitythatDP-SISexactlyidentifiesthetruetop-ğ‘˜features
Borovecki 31 22283 0.9011
isatleast
Burczynski 127 22283 0.2175
(cid:26) (cid:18)ğ‘‘(cid:19) ğœ‰ğ›¾ğœ–(cid:27)
1âˆ’exp ğ‘˜log
ğ‘˜
+log(cid:0)ğ‘ ğ‘‘,ğ‘˜(cid:1)âˆ’
2
, C Ch hi ia nretti 1 12 18
8
1 22 26 22 15
5
0 0. .2 57 80 75
9
whereğ‘ ğ‘‘,ğ‘˜ = (cid:0)ğ‘‘ ğ‘˜(cid:1)/ğ‘‘ ğ‘˜ğ‘˜ğ‘˜ â‰¤ ğ‘˜ andğ›¾ isahyperparameterofcanonical C Ch ho riw std ea nr sy en 1 20 14 7 2 12 42 18 33 0 0. .8 73 83 78 0
Lipschitzwhichmustbebetween0and1.
Golub 72 7129 0.6819
Proof. ThisstatementfollowsdirectlyfromtheoremsA.19and Gordon 181 12533 0.2432
A.20byShekelyan&Loukides[40].1Thistheoremprovidesahigh- Gravier 168 2905 0.0689
probabilityboundforthedifferencebetweenthesmallestprivately Khan 63 2308 0.6961
selectedelementandthetrueğ‘˜thlargestelement.Ifthisdifference Shipp 77 7129 0.4204
islessthanğœ‰,weknowthatthesmallestprivatelyselectedelement Singh 102 12600 0.6449
isthetrueğ‘˜thlargestelement,andthusthesetsoverlapexactly. Sorlie 85 457 0.9300
Giventhis,wemustsolve Su 102 5564 0.9602
Subramanian 50 10100 0.5340
2 (cid:18) (cid:18)ğ‘‘(cid:19) (cid:19)
ğ›¾ğœ–
ğ‘˜log
ğ‘˜
âˆ’logğ›¼+(cid:0)ğ‘ ğ‘‘,ğ‘˜(cid:1) <ğœ‰ Tian 173 12625 0.0080
West 49 7129 0.7044
forğ›¼.Rearrangingthisinequality,wefind Yeoh 248 12625 0.4611
ğ›¼ >exp(cid:26) ğ‘˜log(cid:18)ğ‘‘ ğ‘˜(cid:19) +log(cid:0)ğ‘ ğ‘‘,ğ‘˜(cid:1)âˆ’ ğœ‰ğ›¾ 2ğœ–(cid:27) . T vaa tb ele L1 A: SS Su Om rm ega rr ey sso if ond sat was ie thts. ğœ†R =2 0a .1re
.
Tin hc elu Rd 2e id nf do icr an teon hp or wi-
TheoremA.20,weknowthatwithprobabilityatleast1âˆ’ğ›¼,the â€œsparselylinearâ€adatasetis.
smallestprivatelyselectedelementislessthanğœ‰farfromthetrue
ğ‘˜thlargestelement.Choosingthesmallestğ›¼ fromtheinequality
aboveproducestheresult. â–¡
Theweightvectorwwasgeneratedbyrandomlyselectingeight
Althoughitislikelydifficulttoidentifythevalueofğœ‰privately, indicestoserveasthenon-zerovaluesintheweightvector.The
thisanalysisisusefultounderstandhowthehyperparametersof valueofanon-zeroindexğ‘¤ ğ‘– isgivenbyEquation6.
DP-SISimpactitsperformance.Asğ‘˜ andğ‘‘ increase,itbecomes
lesslikelyfortheDP-SIStoexactlyidentifythetop-ğ‘˜scores.This ğ‘¤
ğ‘–
=(âˆ’1)ğ‘¢ (ğ‘+|ğ‘§|) (6)
makessense-asğ‘˜ andğ‘‘ increase,thealgorithmmustaddmore
where
noisewhichproducesahigherlikelihoodforrandomvariations
inselectedfeatures.Next,asğœ‰,ğ›¾,andğœ– increase,DP-SISwillbe ğ‘¢ âˆ¼Bernoulli(0.4)
moreconsistentwiththenon-privatealgorithm.Thisalsomatches
ğ‘§âˆ¼N(0,1)
intuition-higherğœ‰meansgreaterseparationbetweentheğ‘˜-and
logğ‘›
ğ‘˜+1-thscoresandhigherğœ–meansweakerprivacyandlessnoise. ğ‘=4 âˆš
Althoughouranalysisindicatesthathigherğ›¾ wouldalsoproduce ğ‘›
betterresults,thetop-ğ‘˜simulationsdoneinShekelyan&Loukides Thesyntheticdatasetservedasausefulbenchmarkasitprovided
indicatesthatfortheaveragecase,ğ›¾ = 1 performsbetterthan amodelwithaknownsolutionandthereforeaknownsetoftop-k
2
ğ›¾ =1,soweuseğ›¾ = 1 inourexperiments[40]. features.Fortherealdatasets,weusedusedAlon[1],Borovecki[3],
2
Burczynski[4],Chiaretti[5],Chin[6],Chowdary[7],Christensen
5 EXPERIMENTS [8],Golub[18],Gordon[19],Gravier[20],Khan[22],Shipp[41],
WeusedDP-SISandthetwo-stageapproachtoperformdifferen- Singh[42],Sorlie[43],Subramanian[44],Tian[46],West[51],and
tiallyprivatetop-ğ‘˜ featureselectiononnineteencommonhigh- Yeoh[54]datasets.ThesearesummarizedinTable1.
dimensionaldatasetsandanadditionalsyntheticdataset.Thesyn- Threehyperparameterswereusedinourexperiments:ğœ–,ğ‘˜,and
theticdatawasgeneratedusingthetechniquedescribedinsection ğœ†.ğœ–controlledtheprivacybudget:smallervaluesofğœ–corresponded
3.3.1of[12]with(ğ‘›,ğ‘‘)=(100,2000).Weusetheusuallinearmodel, tohigherlevelsofprivacy.ğœ–valuesbetween0.1and20weretested.
foundinEquation5. ğ‘˜correspondedtothenumberoffeaturestobeselected.Finally,ğœ†
wasusedasğ¿1regularizertoperformnon-privateLASSOregres-
y=Xw+ğ siononthedatasets.Thisisnecessaryinthetwo-stageprocedure
Xâˆ¼N(0,Iğ‘‘) (5) butwealsousedthetop-ğ‘˜componentsofthenon-privateLASSO
regressionasaproxyforthetruetop-ğ‘˜features,sinceitiscompu-
ğ âˆ¼N(0,1.5Iğ‘‘)
tationallyinfeasibletoidentifytheoptimalfeaturestoağ‘˜-sparse
1TheoremA.20oftheirpaperhasatypo:thepositionsofğ‘¥(cid:174)[ğ‘‡]andğ‘¥(cid:174)[ğ‘˜]shouldbe constrainedregressionproblemondatasetswithmanyfeatures.
switched.ThisisclearwhenreadingthederivationofTheoremsA.19andA.20. Inallpresentedresults,ğœ† wassetto0.1becausewhenrunningAISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
Top-5 Accuracy for Christensen Top-5 Accuracy for Sorlie
DP SIS DP SIS
0.6 two-stage 0.4 two-stage
0.5
0.3
0.4
0.3 0.2
0.2
0.1
0.1
0.0 0.0
10âˆ’1 100 101 10âˆ’1 100 101
Epsilon Epsilon
Top-7 Accuracy for Christensen Top-6 Accuracy for Sorlie
DP SIS DP SIS
0.5 two-stage 0.35 two-stage
0.30
0.4
0.25
0.3 0.20
0.15
0.2
0.10
0.1
0.05
0.0 0.00
10âˆ’1 100 101 10âˆ’1 100 101
Epsilon Epsilon
Figure2:Top-ğ‘˜ accuracyofmodelsfitonfeaturesselected Figure3:Top-ğ‘˜ accuracyofmodelsfitonfeaturesselected
fromontheChristensendataset.DP-SISoutperformsthe fromontheSorliedataset.DP-SISoutperformsthetwo-stage
two-stagemechanismonğœ–valuesbetween100 and101
,which
mechanismonğœ–valuesgreaterthan100
,whicharecommonly
arecommonlyusedforprivatecomputation[31]. usedforprivatecomputation[31].
experimentswithdifferentğœ†valueswefoundlittledifferencein
results.NotethattheR2scoresforthenon-privateLASSOarelisted
inTable1. Figure2,Figure3,Figure4,andFigure5displaytheperformance
Forthesyntheticdatasetweselectedğ‘˜ âˆˆ {5,8},astherewere of DP-SIS and two-stage for the Christensen, Sorlie, Yeoh, and
only8significantfeaturesinthissyntheticdataset.Fortheother syntheticdatasets,respectively.Itisexpectedthatforsmallvalues
datasets,wetestedğ‘˜ âˆˆ {5,âŒŠlog(ğ‘‘)âŒ‹}.Wechosetouse5sincein ofğœ–like0.1,methodswillperformpoorlyduetothedifficultyof
manycasesdataanalystsseektoidentifyasmallsubsetofimportant featureselection,andDPingeneral,underhighprivacyconstraints.
featuresinadataset,andweusedlog(ğ‘‘)sincetheoreticalliterature Inallplots,wecanseethatatverysmallvaluesofğœ–,bothDP-SIS
onfeatureselectionoftenfocusesoneffectiverecoveryoflog(ğ‘‘) andtwo-stagehavepooraccuracyasexpected.
features. However,oneachdataset,DP-SISoutperformsthetwo-stage
Foreach(ğ‘˜,ğœ–)pairweperformedprivatefeatureselectionus- mechanism for a range of intermediateğœ– values. This range of
ingtheDP-SISmechanismandthetwo-stagetechniqueforone intermediatevaluesisinlinewithreal-worldğœ–usage,indicating
hundredtrialsforeachdataset.Toscoreprivatefeatureselection, thatourmethodisin-lineandpotentiallyusefulforpracticalmodel
wecomparedthenumberofcorrectlychosentop-ğ‘˜featurestoğ‘˜, building[31].ThisindicatesthatDP-SISisamoreusefulmethod
producinganaccuracyscore.Experimentswererunonanhigh- thanthetwo-stageapproachsinceitcanselectcorrectfeaturesat
performancecomputingclusterparallelizedacrossseveralnodesat lowerğœ–valuesthanthetwo-stagemechanism,meaningthatitcan
atimeusingtheSLURMHPCresourcemanager[55].Eachnode produceusefulresultswhilemaintainingtheprivacyofindividuals
had40CPUsand512GBofRAM. inthedatasets.
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poTFeatureSelectionfromDifferentiallyPrivateCorrelations AISecâ€™24,October18,2024,SaltLakeCity,UT
Top-5 Accuracy for Yeoh Top-5 Accuracy for Synth
0.5 DP SIS DP SIS
two-stage 1.0 two-stage
0.4
0.8
0.3
0.6
0.2
0.4
0.1 0.2
0.0 0.0
10âˆ’1 100 101 10âˆ’1 100 101
Epsilon Epsilon
Top-9 Accuracy for Yeoh Top-8 Accuracy for Synth
0.35 DP SIS 0.8 DP SIS
two-stage two-stage
0.30 0.7
0.6
0.25
0.5
0.20
0.4
0.15
0.3
0.10
0.2
0.05 0.1
0.00 0.0
10âˆ’1 100 101 10âˆ’1 100 101
Epsilon Epsilon
Figure4:Top-ğ‘˜ accuracyofmodelsfitonfeaturesselected Figure5:Top-ğ‘˜ accuracyofmodelsfitonfeaturesselected
fromontheYeohdataset.DP-SISoutperformsthetwo-stage fromontheSynthdataset.DP-SISoutperformsthetwo-stage
mechanismonğœ– valuesgreaterthan2Ã—100 ,whichcanbe mechanismonğœ–valuesgreaterthan101 .Althoughsuchhigh
usedinprivatecomputation[31]. ğœ–valuesaretypicallynotusedforprivatecomputation,this
resultstilldemonstratesthatDP-SIShasbetterresultsthan
thetwo-stagebaseline.
Figure6andFigure7showthatthistrendholdsformoredatasets.
Theseheatmapsshowthatthetwo-stagemethoddoesnotsignifi-
cantlyoutperformDP-SISonanydataset,andformanydatasets, Wedemonstratethiswhenexaminingtheğ‘‡ğ‘‚ğ‘ƒ,ğºğ‘…ğ¸ğ´ğ‘‡,andğºğ‘‚ğ‘‚ğ·
DP-SIShasmuchbetteraccuracythanthetwo-stagemethodfor performanceofDP-SIS.Thesemetricsaredefinedin[40]as:
someğœ–values.However,somedatasetslikeAlonandBoroveckido (1) ğ‘‡ğ‘‚ğ‘ƒ:DP-SISselectsallğ‘˜ofthetruetop-korderstatistics
notshowadifferencebetweenDP-SISandtwo-stageforanyğœ–value. (2) ğºğ‘…ğ¸ğ´ğ‘‡:DP-SISselectsallofthetop ğ‘˜ orderstatisticsand
10
Thisislikelybecausethesedatasetsarehigherdimensionaland therestcomefromthetop 11ğ‘˜
havelessstableselectedfeaturesthanotherdatasets.Indeed,we 10
(3) ğºğ‘…ğ¸ğ´ğ‘‡:DP-SISselectsallofthetop ğ‘˜ orderstatisticsand
foundthatDP-SISoutperformsthetwo-stagemethodtoagreater 100
therestcomefromthetop 3ğ‘˜
degreewhenthereisastrongerlinearrelationshipbetweenthe 2
featuresandthetarget.Forexample,DP-SISoutperformstwo-stage Forexample,ifğ‘˜ =200,DP-SISperformsğºğ‘…ğ¸ğ´ğ‘‡ ifitselectsallof
aroundğœ– =5ontheChristensendatasetbutnotonAlondespitethe thetop20truetop-korderstatistics,andtheremaining180features
factthattheyhavecomparablesizes((217,1413)forChristensen areselectedfromthetop220.
and(62,2000)forAlon).However,linearregressionachievesanğ‘…2 DP-SISonlyusestheorderstatisticsduringitstop-ğ‘˜selection
of0.7870onChristensen,butonly0.4822forAlon. process,anditcanperformwellinselectingğºğ‘…ğ¸ğ´ğ‘‡ andğºğ‘‚ğ‘‚ğ·
Wealsonotethatevenwhentheaccuracyofselectedfeaturesis featuresonhigh-dimensionaldatasets.Figure8givesanexample
poor,DP-SISachievesahighlevelofaccuracyontheorderstatistics ofthisontheChindataset.Whileitstop-10accuracyisaround20%
themselvessinceitisbuiltonthecanonicalLipschitzmechanism. atğœ– = 20,thefiguredemonstratesthatitperformsverywellon
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poT
ycaruccA
k-poTAISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
DP-SIS - two-stage Top-5 Accuracy Difference DP-SIS Metrics for Chin with k=10
alon 1.0 Top
christensen
khan Great
gravier 0.8 Good
golub 0.8
sorlie
shipp
0.6
borovecki
subramanian 0.6
chiaretti
singh
0.4
west 0.4
synth
gordon
chowdary
0.2
chin 0.2
tian
burczynski
yeoh 0.0 0.0
10âˆ’1 100 101
Epsilon Epsilon
Figure6:DifferencebetweenDP-SISandtwo-stagetop-ğ‘˜for Figure8:PerformanceofDP-SISonğ‘‡ğ‘‚ğ‘ƒ,ğºğ‘…ğ¸ğ´ğ‘‡,andğºğ‘‚ğ‘‚ğ·
ğ‘˜ =5.Onnodatasetandğœ–valuedoestwo-stagesignificantly metrics.ThisgraphdemonstratesthattheDP-SISselection
outperform DP-SIS, but DP-SIS significantly outperforms procedureisaccuratewithrespecttoorderstatistics,mean-
two-stageonrangesofğœ–valuestypicallygreaterthan1.0. ingthatifabetterrepresentationoffeatureimportantthan
correlationcanbeengineered,theprivateselectionproce-
durecanbeappliedwithexpectationsofgoodperformance.
DP-SIS - two-stage Top-log(d) Accuracy Difference
alon 0.7 âˆš
christensen ğ‘ estimatorsondisjointpartitionsofadatasettoidentifythe
khan 0.6 mostcommonlyselectedfeaturesintheseestimators.Whilethisis
gravier
golub computationallyfeasible,itisstilldifficultsinceitrequiresbuild-
sorlie 0.5 ingmanyhigh-dimensionalestimators.Additionally,itrequires
shipp
borovecki high-dimensionalsparseregressionestimatorstobealgorithmically
0.4
subramanian stable,whichtheyarenot.
chiaretti
singh 0.3 SISisanalternativemethodforhigh-dimensionalfeatureselec-
west
tion.Itselectstheğ‘˜ featureswhicharemostcorrelatedwiththe
synth
gordon 0.2 targetvector.BymakingSISprivatewithadifferentiallyprivate
chowdary
top-ğ‘˜selector,wecandevelopadifferentiallyprivatefeatureselec-
chin
0.1
tian torbasedonSIS.WechoosetouseDP-SISbasedonitssuperior
burczynski
performance.
yeoh 0.0
SISoutperformsthetwo-stagemethodonmostdatasetsinrea-
sonablerangesofğœ–.Additionally,DP-SIScanbeusedwithany
Epsilon
featureselectionmetric,makingitflexibletoimprovedmetricsfor
Figure7:DifferencebetweenDP-SISandtwo-stagetop-ğ‘˜ac- featureselectiondevelopedinthefuture.
curacyforğ‘˜ =logğ‘‘.Onnodatasetandğœ–valuedoestwo-stage Finally,weendwithafinalcomment.Thispaperexploresem-
ployingasimplemetricforfeatureselectionindifferentialprivacy,
significantlyoutperformDP-SIS,butDP-SISsignificantlyout-
performstwo-stageonrangesofğœ– valuestypicallygreater andpitsitagainstamorecomplicatedmechanism.Thesimpler
metricworksbetterdespiteitnotbeingabletoidentifycasesin
than1.0.
whichsubsetsoffeaturesareindividuallyweaklycorrelatedwith
thetargetbutjointlystronglycorrelatedwiththetarget.Thisis
theğ‘‡ğ‘‚ğ‘ƒ,ğºğ‘…ğ¸ğ´ğ‘‡,andğºğ‘‚ğ‘‚ğ·metrics.Thisindicatesthatthelower becausewhenusingdifferentialprivacy,thereisaconstanttug-
accuracyisafunctionofthecorrelationmetricusedinDP-SIS,and of-warbetweenemployingmoreexpressivemethodswithmore
ifanothermetricwhichbetterpredictsfeatureimportanceisused, noiseandsimplermethodswithlessnoise.Inthecasepresentedin
DP-SIScanbeusedeffectivelyforprivatetop-ğ‘˜selection. thispaper,featureselectionhadhigheraccuracywhenasimpler
methodwasselectedwhichwasmorestableandrequiredlessnoise.
6 CONCLUSION
Thispaperseekstoimproveuponthestate-of-the-artincompu- REFERENCES
tationallyfeasibledifferentiallyprivatefeatureselectionforhigh- [1] U.Alon,N.Barkai,D.A.Notterman,K.Gish,S.Ybarra,D.Mack,andA.J.Levine.
dimensionallinearregression.Weidentifythatthecurrentcomputa- 1999.Broadpatternsofgeneexpressionrevealedbyclusteringanalysisoftumor
andnormalcolontissuesprobedbyoligonucleotidearrays.Proceedingsofthe
tionallyfeasiblemethod,thetwo-stageapproach,requiresbuilding NationalAcademyofSciences96,12(1999),6745â€“6750.
tesataD
tesataD
1.0
1.0
21.0
21.0
61.0
61.0
91.0
91.0
42.0
42.0
3.0
3.0
83.0
83.0
74.0
74.0
85.0
85.0
37.0
37.0
19.0
19.0
0.1
0.1
31.1
31.1
14.1
14.1
67.1
67.1
2.2
2.2
47.2
47.2
24.3
24.3
62.4
62.4
23.5
23.5
36.6
36.6
72.8
72.8
0.01
0.01
13.01
13.01
68.21
68.21
40.61
40.61
0.02
0.02
dooGFeatureSelectionfromDifferentiallyPrivateCorrelations AISecâ€™24,October18,2024,SaltLakeCity,UT
[2] GiorgosBorboudakisandIoannisTsamardinos.2019.Forward-BackwardSelec- [21] TrevorHastie,RobertTibshirani,andRyanTibshirani.2020.BestSubset,Forward
tionwithEarlyDropping. JournalofMachineLearningResearch20,8(2019), StepwiseorLasso?AnalysisandRecommendationsBasedonExtensiveCompar-
1â€“39. http://jmlr.org/papers/v20/17-334.html isons.Statist.Sci.35,4(Nov.2020),579â€“592. https://doi.org/10.1214/19-STS733
[3] F.Borovecki,L.Lovrecic,J.Zhou,H.Jeong,F.Then,H.D.Rosas,S.M.Hersch,P. Publisher:InstituteofMathematicalStatistics.
Hogarth,B.Bouzou,R.V.Jensen,andD.Krainc.2005.Genome-wideexpression [22] J.Khan,J.S.Wei,M.Ringner,L.H.Saal,M.Ladanyi,F.Westermann,F.Berthold,
profilingofhumanbloodrevealsbiomarkersforHuntingtonâ€™sdisease.Proceedings M.Schwab,C.R.Antonescu,C.Peterson,andP.S.Meltzer.2001.Classification
oftheNationalAcademyofSciences102,31(2005),11023â€“11028. anddiagnosticpredictionofcancersusinggeneexpressionprofilingandartificial
[4] M.E.Burczynski,R.L.Peterson,N.C.Twine,K.A.Zuberek,B.J.Brodeur,L.Cas- neuralnetworks.NatureMedicine7,6(2001),673â€“679.
ciotti,V.Maganti,H.M.Sackett,N.Novoradovskaya,B.Cook,P.S.Reddy,A.Strahs, [23] AmolKhanna,FredLu,andEdwardRaff.2023.Thechallengeofdifferentially
M.L.Clawson,R.M.Goldschmidt,S.Chaturvedi,A.M.Slager,L.A.Marshall,and privatescreeningrules.arXivpreprintarXiv:2303.10303(2023).
B.Renault.2006.MolecularclassificationofCrohnâ€™sdiseaseandulcerativecolitis [24] AmolKhanna,FredLu,EdwardRaff,andBrianTesta.2023.DifferentiallyPri-
patientsusingtranscriptionalprofilesinperipheralbloodmononuclearcells.The vateLogisticRegressionwithSparseSolutions.InProceedingsofthe16thACM
JournalofMolecularDiagnostics8,1(2006),51â€“61. WorkshoponArtificialIntelligenceandSecurity.1â€“9.
[5] S.Chiaretti,X.Li,R.Gentleman,A.Vitale,M.Vignetti,F.Mandelli,J.Ritz,andR. [25] AmolKhanna,EdwardRaff,andNathanInkawhich.2024. SoK:AReviewof
Foa.2004.GeneexpressionprofileofadultT-cellacutelymphocyticleukemia DifferentiallyPrivateLinearModelsForHigh-DimensionalData.In2024IEEE
identifiesdistinctsubsetsofpatientswithdifferentresponsetotherapyand ConferenceonSecureandTrustworthyMachineLearning(SaTML).IEEE,57â€“77.
survival.Blood103,7(2004),2771â€“2778. [26] DanielKifer,AdamSmith,andAbhradeepThakurta.2012.Privateconvexempiri-
[6] KoeiChin,SanjeevDeVries,JaneFridlyand,PaulT.Spellman,RajikaRoydasgupta, calriskminimizationandhigh-dimensionalregression.InConferenceonLearning
Wen-LinKuo,AnnaLapuk,RichardM.Neve,ZhenQian,TimRyder,NoraBayani, Theory.JMLRWorkshopandConferenceProceedings,25â€“1.
JonathanBrock,KimberlyMontgomery,DavidGinzinger,DanSeah,WeiKuo, [27] JohanLarsson.2021.Look-AheadScreeningRulesfortheLasso.(2021). http:
JeffreyL.Stilwell,DanielPinkel,DonnaG.Albertson,FredericM.Waldman, //arxiv.org/abs/2105.05648arXiv:2105.05648.
AnilN.Jain,ColinCollins,andJoeW.Gray.2006.Genomicandtranscriptional [28] DongCLiuandJorgeNocedal.1989. OnthelimitedmemoryBFGSmethod
aberrationslinkedtobreastcancerpathophysiologies.CancerCell10,6(2006), forlargescaleoptimization.MathematicalProgramming45,1(1989),503â€“528.
529â€“541. https://doi.org/10.1007/BF01589116
[7] D.R.Chowdary,J.Lathrop,J.Skelton,K.Curtin,T.Briggs,L.Zhang,A.Rashidi,S. [29] EugeneNdiaye,OlivierFercoq,Alex,ReGramfort,andJosephSalmon.2017.
White,D.Curtis,D.D.VonHoff,andD.Kravitz.2006.Prognosticgeneexpression GapSafeScreeningRulesforSparsityEnforcingPenalties.JournalofMachine
signaturescanbemeasuredintissuescollectedinRNAlaterpreservative.The LearningResearch18,128(2017),1â€“33. http://jmlr.org/papers/v18/16-577.html
JournalofMolecularDiagnostics8,1(2006),31â€“39. [30] JosephPNearandChikÃ©Abuah.2021.Programmingdifferentialprivacy.URL:
[8] BrockCChristensen,EAndresHouseman,CarmenJMarsit,ShichunZheng, https://uvm(2021).
MargaretRWrensch,JosephLWiemels,HeatherHNelson,MargaretRKaragas, [31] JosephPNear,DavidDarais,NaomiLefkovitz,GaryHowarth,etal.2023.Guide-
JamesFPadbury,RaphaelBueno,DavidJSugarbaker,Ru-FangYeh,JohnK linesforEvaluatingDifferentialPrivacyGuarantees.TechnicalReport.National
Wiencke,andKarlTKelsey.2009.AgingandEnvironmentalExposuresAlter InstituteofStandardsandTechnology.
Tissue-SpecificDNAMethylationDependentuponCpGIslandContext.PLOS [32] AndrewY.Ng.2004.Featureselection,L1vs.L2regularization,androtational
Genetics5,8(Aug.2009),e1000602. invariance.Twenty-firstinternationalconferenceonMachinelearning-ICMLâ€™04
[9] DLDonohoandMElad.2003.MaximalsparsityrepresentationviaL1minimiza- (2004),78. https://doi.org/10.1145/1015330.1015435Publisher:ACMPressPlace:
tion.ProceedingsofNationalAcademyofSciences100(2003),2197â€“2202. NewYork,NewYork,USAISBN:1581138285.
[10] DavidLDonoho,XiaomingHuo,etal.2001.Uncertaintyprinciplesandideal [33] KoheiOgawa,YoshikiSuzuki,andIchiroTakeuchi.2013.Safescreeningofnon-
atomicdecomposition. IEEEtransactionsoninformationtheory47,7(2001), supportvectorsinpathwiseSVMcomputation.ICML(2013). http://jmlr.org/
2845â€“2862. proceedings/papers/v28/ogawa13b.html
[11] DavidDurfeeandRyanMRogers.2019. Practicaldifferentiallyprivatetop-k [34] FPedregosa,GVaroquaux,AGramfort,VMichel,BThirion,OGrisel,MBlondel,
selectionwithpay-what-you-getcomposition.AdvancesinNeuralInformation PPrettenhofer,RWeiss,VDubourg,JVanderplas,APassos,DCournapeau,
ProcessingSystems32(2019). MBrucher,MPerrot,andEDuchesnay.2011.Scikit-learn:MachineLearning
[12] JianqingFanandJinchiLv.2006.Sureindependencescreeningforultrahighdi- inPython. JournalofMachineLearningResearch12(2011),2825â€“2830. http:
mensionalfeaturespace.JournaloftheRoyalStatisticalSociety:SeriesB(Statistical //jmlr.csail.mit.edu/papers/v12/pedregosa11a.html
Methodology)70(2006). https://api.semanticscholar.org/CorpusID:5001358 [35] GangQiao,WeijieSu,andLiZhang.2021.Oneshotdifferentiallyprivatetop-k
[13] JianqingFanandJinchiLv.2008.Sureindependencescreeningforultrahighdi- selection.InInternationalConferenceonMachineLearning.PMLR,8672â€“8681.
mensionalfeaturespace.JournaloftheRoyalStatisticalSocietySeriesB:Statistical [36] EdwardRaff,AmolKhanna,andFredLu.2024. ScalingUpDifferentiallyPri-
Methodology70,5(2008),849â€“911. vateLASSORegularizedLogisticRegressionviaFasterFrank-WolfeIterations.
[14] Rong-EnFan,Kai-WeiChang,Cho-JuiHsieh,Xiang-RuiWang,andChih-JenLin. AdvancesinNeuralInformationProcessingSystems36(2024).
2008.LIBLINEAR:ALibraryforLargeLinearClassification.J.Mach.Learn.Res. [37] AlainRakotomamonjy,GillesGasso,andJosephSalmon.2019. Screening
9(jun2008),1871â€“1874. rulesforLassowithnon-convexSparseRegularizers.InProceedingsofthe
[15] EmanueleFrandi,RicardoÃ‘anculef,StefanoLodi,ClaudioSartori,andJohan 36thInternationalConferenceonMachineLearning.PMLR,5341â€“5350. https:
A.K.Suykens.2016.FastandscalableLassoviastochasticFrankâ€“Wolfemethods //proceedings.mlr.press/v97/rakotomamonjy19a.htmlISSN:2640-3498.
withaconvergenceguarantee.MachineLearning104,2(Sept.2016),195â€“221. [38] BrianC.Ross.2014.Mutualinformationbetweendiscreteandcontinuousdata
https://doi.org/10.1007/s10994-016-5578-4 sets.PLoSONE9,2(2014). https://doi.org/10.1371/journal.pone.0087357
[16] JeromeFriedman,TrevorHastie,andRobTibshirani.2010.RegularizationPaths [39] SaptarshiRoyandAmbujTewari.2023.OntheComputationalComplexityof
forGeneralizedLinearModelsviaCoordinateDescent. JournalofStatistical PrivateHigh-dimensionalModelSelectionviatheExponentialMechanism.arXiv
Software33,1(2010),1â€“22. arXiv:1501.0228ISBN:9781439811870. preprintarXiv:2310.07852(2023).
[17] JenniferGillenwater,MatthewJoseph,AndresMunoz,andMonicaRiberoDiaz. [40] MichaelShekelyanandGrigoriosLoukides.2022.DifferentiallyPrivateTop-k
2022.AJointExponentialMechanismForDifferentiallyPrivateTop-ğ‘˜.InInter- SelectionviaCanonicalLipschitzMechanism.arXivpreprintarXiv:2201.13376
nationalConferenceonMachineLearning.PMLR,7570â€“7582. (2022).
[18] ToddR.Golub,DonnaK.Slonim,PabloTamayo,ChristineHuard,Michelle [41] MargaretA.Shipp,KennethN.Ross,PabloTamayo,AlexP.Weng,JefferyL.
Gaasenbeek,JillP.Mesirov,HilaryColler,MignonL.Loh,JamesR.Downing, Kutok,RicardoC.T.Aguiar,MichelleGaasenbeek,MariaAngelo,MargaretReich,
MichaelA.Caligiuri,ClaraD.Bloomfield,andEricS.Lander.1999.Molecular GeraldineS.Pinkus,ThomasS.Ray,MichaelA.Koval,KennethW.Last,Andrew
classificationofcancer:classdiscoveryandclasspredictionbygeneexpression Norton,T.AndrewLister,JillMesirov,DonnaS.Neuberg,EricS.Lander,JonC.
monitoring.Science286,5439(1999),531â€“537. Aster,andToddR.Golub.2002.DiffuselargeB-celllymphomaoutcomeprediction
[19] G.J.Gordon,R.V.Jensen,L.L.Hsiao,S.R.Gullans,J.E.Blumenstock,S.Ramaswamy, bygene-expressionprofilingandsupervisedmachinelearning.NatureMedicine
W.G.Richards,D.J.Sugarbaker,andR.Bueno.2002.Translationofmicroarray 8,1(2002),68â€“74.
dataintoclinicallyrelevantcancerdiagnostictestsusinggeneexpressionratios [42] DineshSingh,PhillipG.Febbo,KennethRoss,DouglasG.Jackson,JudithManola,
inlungcancerandmesothelioma.CancerResearch62,17(2002),4963â€“4967. CarolLadd,PabloTamayo,AndrewA.Renshaw,AnthonyV.Dâ€™Amico,JeromeP.
[20] Gravier,Eleonore,GaellePierron,AnneVincent-Salomon,Nadegegruel,Virginie Richie,EricS.Lander,MassimoLoda,PhilipW.Kantoff,ToddR.Golub,and
Raynal,AlexiaSavignoni,YannDeRycke,Jean-YvesPierga,CarloLucchesi, WilliamR.Sellers.2002.Geneexpressioncorrelatesofclinicalprostatecancer
FabienReyal,AlainFourquet,SergioRoman-Roman,FrancoisRadvanyi,Xavier behavior.CancerCell1,2(2002),203â€“209.
Sastre-Garau,BernardAsselain,andOlivierDelattre.2010.AprognosticDNA [43] ThereseSÃ¸rlie,CharlesMPerou,RobertTibshirani,TuridAas,StephanieGeisler,
signatureforT1T2node-negativebreastcancerpatients.Genes,Chromosomes HildeJohnsen,TrevorHastie,MichaelBEisen,MattvandeRijn,StefanieSJeffrey,
andCancer49,12(Sept.2010),1125â€“1125. ThorThorsen,HanneQuist,JohnCMatese,PatrickOBrown,DavidBotstein,
PerEysteinLÃ¸nning,andAnne-LiseBÃ¸rresen-Dale.2001.Geneexpressionpat-
ternsofbreastcarcinomasdistinguishtumorsubclasseswithclinicalimplications.AISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
ProceedingsoftheNationalAcademyofSciences98(Sept.2001),10869â€“10874. suchthattheselectedsetisâ€œcloseâ€tothetruesetofğ‘˜largestscores,
[44] AravindSubramanian,PabloTamayo,VamsiK.Mootha,SayanMukherjee,Ben- insomesense,withreasonablyhighprobability.
jaminL.Ebert,MichaelA.Gillette,AmandaPaulovich,ScottL.Pomeroy,ToddR.
ThecanonicalLipschitzmechanismaimstosolvethisproblemby
Golub,EricS.Lander,andJillP.Mesirov.2005.Genesetenrichmentanalysis:
Aknowledge-basedapproachforinterpretinggenome-wideexpressionprofiles. consideringall(cid:0)ğ‘‘(cid:1)possibleğ‘˜-subsetsofthescoreindices{1,...,ğ‘‘},
ProceedingsoftheNationalAcademyofSciences102,43(2005),15545â€“15550. assigningavalueğ‘˜
toeachsubset,andreturningthesubsetwiththe
[45] AbhradeepGuhaThakurtaandAdamSmith.2013.Differentiallyprivatefeature
selectionviastabilityarguments,andtherobustnessofthelasso.InConference largestvalue.Thevalueassignedtoağ‘˜-subsetisthesumoftwo
onLearningTheory.PMLR,819â€“850. terms:adeterministicutilityterm(whichisanegatedlossterm)and
[46] EugeneTian,JamesR.Sawyer,AmeliaH.Ligon,AnandS.Lagoo,SherrylL.
arandomizednoiseterm.Theutilityandnoisearescaledinspecific
Hubbard,KathyL.Myers,SusanG.Hilsenbeck,RonaldJ.Berenson,DavidO.
Dixon,JenniferR.Sawyer,BartBarlogie,andJohnD.Shaughnessy.2003.High- wayssuchthatğœ–-differentialprivacyisachieved.Theintuitionis
resolutionfluorescenceinsituhybridizationmappingofrecurrentbreakpoint thattheutilitytermwillbelarger(i.e.lessnegative)forindexsets
regionsinmultiplemyelomatranslocations.CancerResearch63,2(2003),532â€“
539. thatareâ€œcloserâ€tothetruetop-ğ‘˜indexsetandeachnoisetermis
[47] RobertTibshirani.1994. RegressionShrinkageandSelectionViatheLasso. largeenoughsuchthatdifferentialprivacyismaintained.
JournaloftheRoyalStatisticalSociety,SeriesB58,1(1994),267â€“288.
[48] MartinJWainwright.2019.High-dimensionalstatistics:Anon-asymptoticview-
point.Vol.48.Cambridgeuniversitypress. A.2 Top-ğ‘˜
[49] ChiWangandBailuDing.2019.FastApproximationofEmpiricalEntropyviaSub-
sampling.In25THACMSIGKDDCONFERENCEONKNOWLEDGEDISCOVERY Supposewehaveadatasetğ‘¥Ë† âˆˆ Xandascorefunction ğ‘“ : X â†’
ANDDATAMINING. Rğ‘‘ whereeachcomponentofthescorefunctionhassensitivity
[50] JieWang,JiayuZhou,PeterWonka,andJiepingYe.2013. LassoScreening
RulesviaDualPolytopeProjection.InAdvancesinNeuralInformationProcessing Î” ğ‘“ meaningthat |ğ‘“ ğ‘–(ğ‘¥Ë† 1) âˆ’ ğ‘“ ğ‘–(ğ‘¥Ë† 2)| â‰¤ Î” ğ‘“ forğ‘– âˆˆ {1,...,ğ‘‘} forall
Systems,Vol.26.CurranAssociates,Inc. https://papers.nips.cc/paper/2013/hash/ ğ‘¥Ë† 1,ğ‘¥Ë† 2 âˆˆ Xthatdifferbyexactlyoneindividual.Letğ‘¥ âˆˆ Rğ‘‘ bea
8b16ebc056e613024c057be590b542eb-Abstract.html
[51] MikeWest,ChristianBlanchette,HollyDressman,ErichHuang,SueIshida,
vectorofnormalizedscoressothatğ‘¥
ğ‘–
=ğ‘“ ğ‘–(ğ‘¥Ë†)/Î”
ğ‘“
forğ‘– âˆˆ{1,...,ğ‘‘}.
RainerSpang,HannahZuzan,JosephA.Olson,JeffreyR.Marks,andJosephR. Thescorevectorğ‘¥ hasdescendingorderstatisticsğ‘¥ [1] â‰¥...â‰¥ğ‘¥ [ğ‘‘]
Nevins.2001.Predictingtheclinicalstatusofhumanbreastcancerbyusinggene and,justasin[40],weletğ‘—1,...,ğ‘—
ğ‘‘
âˆˆ{1,...,ğ‘‘}betheindicessuch
1ex 14p 6re 2s â€“s 1io 1n 46p 7r .ofiles.ProceedingsoftheNationalAcademyofSciences98,20(2001),
thatğ‘¥
ğ‘—1
=ğ‘¥ [1],...,ğ‘¥
ğ‘—ğ‘‘
=ğ‘¥ [ğ‘‘].Theindexset{ğ‘—1,...,ğ‘— ğ‘˜}isthetrue
[52] ZhenJamesXiang,YunWang,andPeterJRamadge.2016.Screeningtestsfor top-ğ‘˜indexset.
lassoproblems.IEEEtransactionsonpatternanalysisandmachineintelligence39, LetYbethesetofallsize-ğ‘˜subsetsof{1,...,ğ‘‘}.Thisisthesetof
5(2016),1008â€“1027.
[53] HuanXu,ConstantineCaramanis,andShieMannor.2011.Sparsealgorithmsare size-ğ‘˜indexsetsandultimatelywewillusethecanonicalLipschitz
notstable:Ano-free-lunchtheorem.IEEEtransactionsonpatternanalysisand mechanismtorandomlyselectanindexsetthatapproximatesthe
machineintelligence34,1(2011),187â€“193.
top-ğ‘˜indexsetbyassigningavaluetoeveryğ‘¦ âˆˆY.Wenotethat
[54] Eng-JuhYeoh,MeganE.Ross,SheilaA.Shurtleff,WilliamK.Williams,Divya
Patel,RamyMahfouz,FrederickG.Behm,SusanaC.Raimondi,MaryV.Relling, theâ€œcanonicalâ€Lipschitzmechanismisaspecialcaseofthemore
AshaPatel,ChengCheng,DarioCampana,DavidWilkins,XiaoweiZhou,Jia generalLipschitzmechanismthatusesaspecificâ€œcanonicalâ€œloss
Li,HanLiu,Ching-HonPui,WilliamE.Evans,CliffordNaeve,LawrenceWong,
function.BelowwedescribetheLipschitzmechanismfortop-ğ‘˜
andJamesR.Downing.2002.Classification,subtypediscovery,andpredictionof
outcomeinpediatricacutelymphoblasticleukemiabygeneexpressionprofiling. selection,butadditionallynotethattheLipschitzmechanismfor
CancerCell1,2(2002),133â€“143. top-ğ‘˜isaspecialcaseofthegeneralLipschitzmechanismdescribed
[55] AndyBYoo,MorrisAJette,andMarkGrondona.2003. Slurm:Simplelinux
utilityforresourcemanagement.InWorkshoponjobschedulingstrategiesfor in[40]whichcontainsanadditionalparameterğœ…(nottobeconfused
parallelprocessing.Springer,44â€“60. withğ‘˜)andreturnsthetop-ğœ… highestvaluedobjects.Inthecase
[56] Guo-xunYuan,Chia-HuaHo,andChih-jenLin.2012.AnimprovedGLMNET
oftop-ğ‘˜ selection,weuseğœ… =1aseachobjectweareconcerned
forL1-regularizedlogisticregression.JournalofMachineLearningResearch13
(2012),1999â€“2030. https://doi.org/10.1145/2020408.2020421 Publisher:ACM withisasize-ğ‘˜ indexset.Withğœ… > 1,theLipschitzmechanism
PressPlace:NewYork,NewYork,USAISBN:9781450308137. would(abitconfusingly)returnthetop-ğœ…size-ğ‘˜subsetsthathave
thehighestvalue(inotherwords,thetop-ğœ…differentiallyprivate
approximationsofthetop-ğ‘˜indexset).
A APPENDIX
A.1 CanonicalLipschitzMechanismforTop-ğ‘˜ DefinitionA.1(LipschitzMechanism,ğœ… = 1). Letğ¹ beacumu-
lativedistribituionfunctionsuchthatlog(1âˆ’ğ¹(ğ‘¥))is1-Lipschitz
Selection
continuousandletğ¹âˆ’1bethecorrespondinginversecumulative
TheoriginalpaperintroducingthecanonicalLipschitzmechanism
distributionfunction.LetYbethedomainthemechanismselects
[40]hasreceivedlittleattentiondespiteitsstrongresults.Webelieve
itsoutputfromandletğ‘ˆ ğ‘¦ âˆ¼ ğ‘ˆğ‘›ğ‘–ğ‘“(0,1) foreachğ‘¦ âˆˆ Ybei.i.d.
thatthisisinpartduetothepaperâ€™spresentation.Inthisappendix,
Letğ‘¥Ë† âˆˆ Xbeadatasetandğ‘“ :Xâ†’Rğ‘‘ beascorefunctionwith
wewillattempttoreviewandclarifythemainideasbehindusing
component-wisesensitivityÎ” sothatwehavethe(normalized)
thecanonicalLipschitzmechanismforğœ–-differentiallyprivatetop-ğ‘˜ ğ‘“
selection. scorevectorğ‘¥ âˆˆRğ‘‘ suchthatğ‘¥ ğ‘– = ğ‘“ ğ‘–(ğ‘¥Ë†)/Î” ğ‘“ forğ‘– âˆˆ {1,...,ğ‘‘}.Let
Supposewehaveadatasetcontaininginformationaboutsome LOSS(Â·|Â·):YÃ—Rğ‘‘ â†’R â‰¥0bealossfunctionwithsensitivityÎ” LOSS.
individuals.Forexample,intheregressioncontextwecanimagine Letğœ– >0betheprivacylossparameter.TheoutputoftheLipschitz
thatourdesignmatrixğ‘‹ âˆˆRğ‘Ã—ğ‘‘ isadatasetcontaininginforma- mechanismis
tionaboutğ‘ individuals,witheachrowcorrespondingtoasingle (cid:18) ğœ– (cid:19)
individual.Givenadatasetğ‘¥Ë† âˆˆXwecandefineascoringfunction ğ‘Œ =argmax âˆ’
2Î”
LOSS(ğ‘¦|ğ‘¥)+ğ¹âˆ’1 (ğ‘ˆ ğ‘¦) .
ğ‘“ :Xâ†’Rğ‘‘ whichreturnsavectorofğ‘‘scoresgivensomedataset ğ‘¦âˆˆY LOSS
inX.Thetop-ğ‘˜problemisconcernedwithselectingasetofğ‘˜ âˆˆZ+ Inthetop-ğ‘˜ setting,theselectiondomainYwillbethesetof
scoresfromasetofğ‘‘ >ğ‘˜scoresinadifferentiallyprivatemanner size-ğ‘˜ subsetsoftheindexset {1,...,ğ‘‘}.However,noticethatYFeatureSelectionfromDifferentiallyPrivateCorrelations AISecâ€™24,October18,2024,SaltLakeCity,UT
isexponentiallylarge,containing(cid:0)ğ‘‘(cid:1)elements,eachofwhichwe generalizedlossfunctionalsohasasensitivityof1(provideda
ğ‘˜
wouldneedtocomputetheutilityandnoisetermsofifwewere normalizedscorevector)and[40]consideradditionaloptimizations
tonaivelyimplementtheLipschitzmechanism,whichwouldbe thatcanbemadetothealgorithmintheğ›¾ =1case,butweonly
prohibitively expensive. The authors of the original paper [40] discusstheğ›¾ âˆˆ [0,1)caseforsimplicity.
cleverlygetaroundthisbymakingaspecificchoiceoflossfunction
suchthatmanyelementsofYsharethesamelossvalueandYcan
A.4 Implementation
ultimatelybepartitionedintoonlyğ‘‚(ğ‘‘ğ‘˜)utilityclasses.Usingthe
TheLipschitzmechanisminthetop-ğ‘˜ settingreturnsthesize-ğ‘˜
factthatğ‘ˆ 01/ğ‘š isequalindistributiontomax{ğ‘ˆ1,...,ğ‘ˆ ğ‘š}fori.i.d.
indexsetwhichmaximizes,overallğ‘¦ âˆˆ Y,avaluewhichisthe
standard uniformvariables{ğ‘ˆ ğ‘–}ğ‘š ğ‘–=0 alongwiththefactthatğ¹âˆ’1 sumofadeterministicutilitytermandarandomnoiseterm.To
isanincreasingfunction,weonlyneedtogenerateğ¹âˆ’1(ğ‘ˆ1/ğ‘š)to efficientlyimplementthecanonicalLipschitzmechanism,weonly
findthemaximalnoiseterm(whichisallwecareaboutsinceall needtoiteratethrougheveryutilityclassCâ„,ğ‘¡ andcomputethe
elementsintheutilityclasssharethesamelossvalue)ratherthan maximalnoisetermforeachutilityclassbecauseeveryğ‘¦ âˆˆCâ„,ğ‘¡ has
generatingğ‘šdifferentsamplesandthentakingthemaximum. thesameutilityandtheadditivenoisesarei.i.d.soeveryğ‘¦ âˆˆCâ„,ğ‘¡
isequallylikelytoreceivethelargestnoiseterm.Oncewehavethe
A.3 CanonicalLossFunction ğ‘¦ âˆˆYwiththelargestvalueforeachclass,wesimplyreturnthe
Letğ‘¦ âˆˆYbesomesize-ğ‘˜indexsetandğ‘¥ âˆˆRğ‘‘ besomenormalized onewiththelargestvalueacrossallclasses.Thatis,foreachCâ„,ğ‘¡
scorevectorcorrespondingtosomescorefunctionğ‘“ anddataset wecompute
ğ‘¥Ë† âˆˆ X such thatğ‘¥ ğ‘– = ğ‘“ ğ‘–(ğ‘¥Ë†)/Î” ğ‘“ forğ‘– âˆˆ {1,...,ğ‘‘}. The so-called
canonicallossfunctionthat[40]introducesisgivenby ğ‘Œ â„,ğ‘¡ =argmax(cid:18) âˆ’ 2Î”ğœ– LOSS(ğ‘¦|ğ‘¥)+ğ¹âˆ’1 (ğ‘ˆ ğ‘¦)(cid:19)
LOSS(ğ‘¦|ğ‘¥)= min âˆ¥ğ‘¥âˆ’ğ‘£âˆ¥âˆ ğ‘¦âˆˆCâ„,ğ‘¡ LOSS
ğ‘£âˆˆOPTâˆ’1(ğ‘¦)
andthecanonicalLipschitzmechanismultimatelyreturnstheğ‘Œ
whereğ‘‚ğ‘ƒğ‘‡âˆ’1(ğ‘¦) isthesetofallğ‘£ âˆˆ Rğ‘‘ whoseğ‘˜ largestvalues â„,ğ‘¡
withthelargestvalueacrossallpossibleâ„andğ‘¡.Thereareonly
havetheindicesğ‘¦ = {ğ‘¦1,...,ğ‘¦ ğ‘˜}.Wenotethatthislossfunction ğ‘˜(ğ‘‘âˆ’ğ‘˜)+1=ğ‘‘ğ‘˜âˆ’ğ‘˜2+1=ğ‘‚(ğ‘‘ğ‘˜)differentutilityclasses,butsome
implicitlydependsonthedatasetğ‘¥Ë†throughitsexplicitdependence
utilityclassescancontainmanyelements.Itiseasytoseethatthe
onthescorevectorğ‘¥ anditcanbeshown(see[40])thatthisloss
sizeofautilityclassisdeterminedbythenumberofpossiblebodies
functionhasasensitivityofÎ” LOSS=1,aresultwhichreliesonthe
itcanhaveandeachbodycontains|B|=ğ‘˜âˆ’â„âˆ’1elementsand
factthatthescorevectorwasappropriatelynormalizedbyÎ” .
ğ‘“ thereare(ğ‘¡âˆ’1)âˆ’(â„+1)+1=ğ‘¡âˆ’â„âˆ’1possiblevaluesthatcan
Thiscanonicallossfunctionisspecialbecauseitallowsusto
partitionYintodisjointutilityclasseswhereallelementsinagiven
bechosenfromtoformagivenbody,thus|Câ„,ğ‘¡| = (cid:0) ğ‘˜ğ‘¡âˆ’ âˆ’â„ â„âˆ’ âˆ’1 1(cid:1).This
meansthatnaivelywewouldhavetogenerateexponentiallymany
utilityclasshavethesamecanonicallossvalue.Thereasonthis
partitioncanbemadeisbecauseitcanbeshownthat
standarduniformnoisetermsforasingleutilityclasswhen|Câ„,ğ‘¡|=
(cid:0)ğ‘¡âˆ’â„âˆ’1(cid:1) islarge,butaswenotedearlierwecangetaroundthis
LOSS(ğ‘¦|ğ‘¥)= ğ‘£âˆˆOm PTi âˆ’n 1(ğ‘¦)âˆ¥ğ‘¥âˆ’ğ‘£âˆ¥âˆ=
ğ‘¥ [â„+1] 2âˆ’ğ‘¥ [ğ‘¡]
b
dğ‘˜
iy
sâˆ’ toâ„
rn
iâˆ’ bl1
uy ts ioa nm tp oli tn hg eğ‘ˆ m1 a/ x| iC mâ„, uğ‘¡| mw oi vth erğ‘ˆ allâˆ¼ oğ‘ˆ ftğ‘› hğ‘– eğ‘“ s( t0 a, n1 d) aa rs dt uh ni is fois rmeq nu oa il si en
s
whereâ„isthelargestintegerstrictlylessthanğ‘˜ suchthatallof andğ¹âˆ’1,whichisappliedtothenoise,isanincreasingfunction.
{ğ‘—1,...,ğ‘— â„}areinğ‘¦andğ‘¡isthesmallestintegergreaterthanorequal ThisallowsustoperformthecanonicalLipschitzmechanismfor
toğ‘˜suchthatallof{ğ‘— ğ‘¡+1,...,ğ‘— ğ‘‘}arenotincludedinğ‘¦.Eachutility top-ğ‘˜selectioninonlyğ‘‚(ğ‘‘ğ‘˜)time,whichisadramaticreduction
classCâ„,ğ‘¡ isparameterizedbytwointegers:â„âˆˆ{0,1,...,ğ‘˜âˆ’1}and fromthenaiveexponentialcomplexity,notingthatwecancompute
ğ‘¡ âˆˆ {ğ‘˜,...,ğ‘‘}whereğ‘¡ =ğ‘˜ isonlyallowedifâ„ =ğ‘˜âˆ’1inorderto thebinomialcoefficients|Câ„,ğ‘¡|efficientlybyupdatingthemaswe
ensurethateachelementofCâ„,ğ‘¡,definedbelow,containsatotalof iteratethroughâ„andğ‘¡usingthefactthat(cid:0)ğ‘›+1(cid:1) = (cid:0)ğ‘›(cid:1)Â·ğ‘›+1 forsome
ğ‘˜indices. ğ‘—+1 ğ‘— ğ‘—+1
0â‰¤ ğ‘— â‰¤ğ‘›,seeAlgorithm2.
DefinitionA.2(UtilityClass). TheutilityclassCâ„,ğ‘¡ isthesetofall
subsetsoftheform{ğ‘—1,...,ğ‘— â„}âˆªBâˆª{ğ‘— ğ‘¡}withB âŠ†{ğ‘— â„+1,...,ğ‘— ğ‘¡âˆ’1} A.5 StandardExponentialNoiseGeneration
andâ„+|B|+1 =ğ‘˜.Ifâ„ = 0,thenwedefine{ğ‘—1,...,ğ‘— â„}tobethe Theoriginalpaper[40]achievedthebestresultswhenusingthein-
emptyset.
versecumulativedistributionfunctioncorrespondingtoastandard
Borrowingterminologyfrom[40],eachğ‘¦ âˆˆ Câ„,ğ‘¡ comprisesof exponentialdistributionsothatğ¹âˆ’1 = âˆ’log(1âˆ’ğ‘¥).Wediscuss
aâ€œheadâ€{ğ‘—1,...,ğ‘— â„},aâ€œbodyâ€B,andaâ€œtailâ€{ğ‘— ğ‘¡}.Everyğ‘¦ âˆˆ Câ„,ğ‘¡ someimplementationdetailswhenusingthischoiceofğ¹âˆ’1 and
containsthetruetop-â„indicesandtheremainingğ‘˜âˆ’â„indicesareall alsodiscusssomeusefulpropertiesofthenoisedistributionthat
withinthetruetop-ğ‘¡indices,soğ‘— isthebestindexnotincludedin weobserved.
â„+1
ğ‘¦andğ‘—
ğ‘¡
istheworstindexincludedinğ‘¦.Consequently,weseethat Asthebinomialcoefficients|Câ„,ğ‘¡|canbeverylarge,caremustbe
thecanonicallossLOSS(ğ‘¦|ğ‘¥)isconstantacrossallğ‘¦ âˆˆCâ„,ğ‘¡ because takenwhencomputingthenoisetermğ¹âˆ’1(ğ‘ˆ1/|Câ„,ğ‘¡|)=âˆ’log(1âˆ’
the loss only depends on the best missing scoreğ‘¥
[â„+1]
and the ğ‘ˆ1/|Câ„,ğ‘¡|)numerically.Forlarge|Câ„,ğ‘¡|,thequantityğ‘ˆ1/|Câ„,ğ‘¡| will
worstincludedscoreğ‘¥ .Wenotethatthecanonicallossfunction usuallybeverycloseto1andsotheargumentofthelogarithm
[ğ‘¡]
canbegeneralizedtotheformLOSS(ğ‘¦|ğ‘¥)=(1âˆ’ğ›¾)ğ‘¥ [â„+1]âˆ’ğ›¾ğ‘¥
[ğ‘¡]
willbeveryclosetozeroandnumericalprecisionbecomesim-
forğ›¾ âˆˆ [0,1],whereabovewehadthecasewhereğ›¾ = 1/2.This portantsothatweavoidaddinganartificiallylargenoisetermAISecâ€™24,October18,2024,SaltLakeCity,UT RyanSwope,AmolKhanna,PhilipDoldo,SaptarshiRoy,andEdwardRaff
Algorithm2CanonicalLipschitzMechanismforTop-ğ‘˜ Theorem A.3. The expected value ofğ‘‹ ğ‘š = âˆ’log(1âˆ’ğ‘ˆ1/ğ‘š)
1: Input: dataset ğ‘¥Ë† âˆˆ X, score function ğ‘“ : X â†’ Rğ‘‘ with withğ‘ˆ âˆ¼ ğ‘ˆğ‘›ğ‘–ğ‘“(0,1) andğ‘š âˆˆ Z+ is theğ‘šth harmonic number
component-wisesensitivityÎ” ğ‘“,subsetsizeğ‘˜ âˆˆ {1,...,ğ‘‘âˆ’1},
ğ»
ğ‘š
=1+1/2+1/3+Â·Â·Â·+1/ğ‘š.
privacylossğœ– â‰¥0,inverseCDFğ¹âˆ’1,lossparameterğ›¾ âˆˆ [0,1) Proof. Itisclearthat
2: Letğ‘¥ ğ‘– =ğ‘“ ğ‘–(ğ‘¥Ë†)/Î” ğ‘“ forallğ‘– âˆˆ{1,...,ğ‘‘} âŠ²Definenormalized âˆ« 1 1âˆ’ğ‘¥ğ‘š
scorevectorğ‘¥ ğ» ğ‘š = ğ‘‘ğ‘¥
0
1âˆ’ğ‘¥
3: C ino ğ‘‚m (p ğ‘‘u lt oe gd ğ‘‘e )sc te imnd eingorderstatisticsğ‘¥ [1] â‰¥...â‰¥ğ‘¥ [ğ‘‘] âŠ²Sortğ‘¥ because 1 1âˆ’ âˆ’ğ‘¥ ğ‘¥ğ‘š = 1+ğ‘¥ +ğ‘¥2+Â·Â·Â·ğ‘¥ğ‘šâˆ’1 forğ‘¥ â‰  1.Integratingby
parts,wesee
4: Letğ‘ˆ â„,ğ‘¡ âˆ¼ğ‘ˆğ‘›ğ‘–ğ‘“(0,1)forallâ„ âˆˆ {0,...,ğ‘˜âˆ’1},ğ‘¡ âˆˆ {ğ‘˜,...,ğ‘‘}be
5:
i L.i e. td.
ğœ–1=(1âˆ’ğ›¾)ğœ–andğœ–2=ğ›¾ğœ– âŠ²Notethatğœ–1+ğœ–2=ğœ–
ğ»
ğ‘š
=âˆ« 01 1 1âˆ’ âˆ’ğ‘¥ ğ‘¥ğ‘š ğ‘‘ğ‘¥ =âˆ’(1âˆ’ğ‘¥ğ‘š )log(1âˆ’ğ‘¥)(cid:12) (cid:12)
(cid:12)
(cid:12)1 0âˆ’âˆ« 01 log(1âˆ’ğ‘¥)ğ‘šğ‘¥ğ‘šâˆ’1ğ‘‘ğ‘¥
6: Initializeğ» =ğ‘˜âˆ’1,ğ‘‡ =ğ‘˜ âŠ²InitializetheutilityclassCğ»,ğ‘‡ to
wherethefirsttermcanbeshowntobezerowithanapplicationof
onlyincludethetruetop-ğ‘˜
Lâ€™HÃ´pitalâ€™srule.Itisalsotruethat
7: ğ¿ ğ»,ğ‘‡ = ğœ–2âˆ’ 2ğœ–1ğ‘¥ [ğ‘˜] âŠ²Initialutility:âˆ’ 2Î”Lğœ– OSSLOSS(ğ‘¦|ğ‘¥)for
âˆ« 1 âˆ« 1
ğ‘¦ âˆˆCğ‘˜âˆ’1,ğ‘˜. E[ğ‘‹ ğ‘š] =âˆ’ log(1âˆ’ğ‘¢1/ğ‘š )ğ‘‘ğ‘¢ =âˆ’ log(1âˆ’ğ‘¥)ğ‘šğ‘¥ğ‘šâˆ’1ğ‘‘ğ‘¥
8: ğ‘‹ ğ»,ğ‘‡ =ğ¹âˆ’1(ğ‘ˆ ğ»,ğ‘‡) âŠ²RandomnoisecorrespondingtoCğ‘˜âˆ’1,ğ‘˜ 0 0
9: Initializeğ‘£ =ğ¿ ğ»,ğ‘‡ +ğ‘‹ ğ»,ğ‘‡ âŠ²ValueforCğ‘˜âˆ’1,ğ‘˜,value=utility+ usingasubstitutionofğ‘¥ =ğ‘¢1/ğ‘š.ItfollowsthatE[ğ‘‹ ğ‘š] =ğ» ğ‘š. â–¡
randomnoise
10: forğ‘¡ âˆˆ{ğ‘˜+1,...,ğ‘‘}do Thisfactisusefulbecauseitallowsustoquantifytheaverage
11: Initializeâ„=ğ‘˜âˆ’1 magnitudeofthemaximalnoiseaddedtoeachutilityclasswhich
12: Initializeğ‘š=1 âŠ²Initializesizeofutilityclass givesusaroughideaofhowlargeourutilitytermsneedtobein
ğ‘š=|Cğ‘˜âˆ’1,ğ‘¡|= (cid:0)ğ‘¡âˆ’ 0ğ‘˜(cid:1)=1 wor hd ee tr ht eo rc oo rm np oe tt ae cw hi oth set nhe scn oo ri ese f. uT nh ctis ioi nnf wor im lla yt ii eo ldn gca on odhe relp suin ltf so fr om
r
13: whileâ„ â‰¥0do
14: ifâ„<ğ‘˜âˆ’1then
somedataset.Forexample,thelargestutilityclassC0,ğ‘‘ willhave
15: Updateğ‘š=ğ‘šÂ· ğ‘¡âˆ’â„âˆ’1 âŠ²Efficientlyupdate thelargestexpectednoisetermandonecaneasilycomputeğ» ğ‘š
binomialcoefficient ğ‘˜âˆ’â„âˆ’1 withğ‘š = (cid:0)ğ‘‘ ğ‘˜âˆ’âˆ’ 11(cid:1) togetaroughsenseofhowlargethenoiseis.If
theutilitytermistoosmall,thenthenoisewilldominateandthe
16: endif
17: ğ¿ â„,ğ‘¡ = ğœ– 22ğ‘¥ [ğ‘¡]âˆ’ğœ– 21ğ‘¥ [â„+1] âŠ²UtilitytermforCâ„,ğ‘¡ quality of the returned index set will be worse on average. We
18: ğ‘‹ â„,ğ‘¡ =ğ¹âˆ’1(ğ‘ˆ â„1 ,/ ğ‘¡ğ‘š ) âŠ²RandomnoisetermforCâ„,ğ‘¡ n effiot ce ieth na tlt yv ce or my pg uo to ed da up sp inro gx wim ela lt -i ko nn os wo nfh aa syrm mo pn toic tin cu em xpb ae nr ss ioca nn s.be
19: ğ‘£â€² =ğ¿ â„,ğ‘¡ +ğ‘‹ â„,ğ‘¡ âŠ²ValueforCâ„,ğ‘¡
20: ifğ‘£â€² >ğ‘£then Received20February2007;revised12March2009;accepted5June2009
21: Updateğ‘£ =ğ‘£â€²,ğ» =â„,ğ‘‡ =ğ‘¡ âŠ²UpdateCğ»,ğ‘‡ tobe
Câ„,ğ‘¡ ifCâ„,ğ‘¡ hasalargervalue
22: endif
23: Updateâ„=â„âˆ’1
24: endwhile
25: endfor
26: ReturnarandomindexsetfromCğ»,ğ‘‡
andconsequentlyreturnbadapproximationsofthetruetop-ğ‘˜in-
dex set. To address the issues of numerical stability, we Taylor
expanded the functionğ‘”(ğ‘¦) = 1âˆ’ğ‘¢ğ‘¦ forğ‘¢ âˆˆ [0,1] andğ‘¦ â‰¥ 0
aboutğ‘¦ = 0andkeptatleastaquadraticapproximationsothat
ğ‘”(ğ‘¦) â‰ˆ âˆ’ğ‘¦log(ğ‘¥)âˆ’ 1ğ‘¦2log2 (ğ‘¥).Giventhattheapproximationis
2
onlygoodwhenğ‘¦issufficientlyclosetozero,weonlyuseditwhen
ğ‘¦ â‰¤10âˆ’8andsimplyused1âˆ’ğ‘¢ğ‘¦ otherwise,whichworkedwellin
practice.
A useful property of the noise termğ‘‹ â„,ğ‘¡ = ğ¹âˆ’1(ğ‘ˆ1/|Câ„,ğ‘¡|) =
âˆ’log(1âˆ’ğ‘ˆ1/|Câ„,ğ‘¡|)withğ‘ˆ âˆ¼ğ‘ˆğ‘›ğ‘–ğ‘“(0,1)isthatitsexpectedvalue
isaharmonicnumber.Inparticular,
| âˆ‘ï¸Câ„,ğ‘¡|
1
E[ğ‘‹ â„,ğ‘¡] = ğ‘˜.
ğ‘˜=1