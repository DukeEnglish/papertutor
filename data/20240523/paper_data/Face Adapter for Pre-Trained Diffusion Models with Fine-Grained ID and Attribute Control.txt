for Pre-Trained Diffusion Models
with Fine-Grained ID and Attribute Control
Yue Hanâ‹†1, Junwei Zhuâ‹†2, Keke He2, Xu Chen2, Yanhao Ge3, Wei Li3,
Xiangtai Li4, Jiangning Zhang2, and Chengjie Wang2, Yong Liuâ€ ,1
1Zhejiang University, 2Tencent, 3VIVO, 4Nanyang Technological University
12432015@zju.edu.cn, yongliu@iipc.zju.edu.cn
https://faceadapter.github.io/face-adapter.github.io/
Abstract. Currentfacereenactmentandswappingmethodsmainlyrely
on GAN frameworks, but recent focus has shifted to pre-trained diffu-
sion models for their superior generation capabilities. However, training
thesemodelsisresource-intensive,andtheresultshavenotyetachieved
satisfactoryperformancelevels.Toaddressthisissue,weintroduceFace-
Adapter, an efficient and effective adapter designed for high-precision
andhigh-fidelityfaceeditingforpre-traineddiffusionmodels.Weobserve
thatbothfacereenactment/swappingtasksessentiallyinvolvecombina-
tions of target structure, ID and attribute. We aim to sufficiently de-
couple the control of these factors to achieve both tasks in one model.
Specifically,ourmethodcontains:1)ASpatialConditionGeneratorthat
provides precise landmarks and background; 2) A Plug-and-play Iden-
tityEncoderthattransfersfaceembeddingstothetextspacebyatrans-
formerdecoder.3)AnAttributeControllerthatintegratesspatialcondi-
tionsanddetailedattributes.Face-Adapterachievescomparableoreven
superior performance in terms of motion control precision, ID retention
capability,andgenerationqualitycomparedtofullyfine-tunedfacereen-
actment/swapping models. Additionally, Face-Adapter seamlessly inte-
grates with various StableDiffusion models.
Keywords: Face Reenactment Â· Face Swapping Â· Diffusion Model
1 Introduction
Face reenactment aims to transfer the target motion onto the source identity
and attributes, while face swapping aims to transfer the source identity onto
thetargetmotionandattributes.Bothtasksrequirecompletedisentanglingand
fine-grainedcontrolofidentity,attributes,andmotion.Currentfacereenactment
and swapping techniques mainly rely on GAN-based frameworks [2,4,12,16,24,
27,29,30,33,48]. However, GAN-based methods encounter limitations in their
â‹† co-first authors; â€ corresponding author
4202
yaM
12
]VC.sc[
1v07921.5042:viXra, large pose, fine-grained motion control
Large Face Shape Variations Fine-grained Motion Control
Handle Extreme Pose Background Preservation
reenact
Face-Adapter supports a 'one-model-two-tasks' approach and demonstrates robustness under various challenging scenarios.
2 Y. Han et al.
Large Face Shape Variations Fine-Grained Motion Control
Reenact Swap
Handle Extreme Pose Background Preservation
(A) Fully fine-tuned (B) Current adapters (C) Face-Adapter
ID Enc ID CA CA Text ID Dec
Lmk Attr
Attr
Lmk ID Target Structure Img Attr
Fig.1: Top: Face-Adapter supports a â€™one-model-two-tasksâ€™ approach and demon-
stratesrobustnessundervariouschallengingscenarios.Bottom:Thedesignmotivation
is(1)BothfacereenactmentandswappingrequirefullydisentangledID,targetstruc-
ture,andattributecontrol;(2)Addressingoverlookedissuesunifiedintargetstructure;
(3) Effective ID injection avoids SD fine-tuning, making Face-Adapter plug-and-play.
generative capabilities, making it challenging to tackle hard cases, such as han-
dlinglargeposesinfacereenactmentandaccommodatingfacialshapevariations
in face swapping.
Existing studies [42,49] have attempted to address these challenges by lever-
agingthepowerfulgenerativecapabilitiesofthediffusionmodels.However,these
methods necessitate full model training, resulting in significant computational
overhead,andtheyhavenotbeensuccessfulindeliveringsatisfactoryoutcomes.
Forinstance,FADM[42]refinestheresultsofGAN-basedreenactmentmethods,
which improves image quality but still fails to resolve the blurring issue caused
by large pose variation. On the other hand, DiffSwap [49] produces blurry fa-
cial outcomes due to the lack of background information during training, which
hampersmodellearning.Moreover,thesemethodsdonotfullyexploitthepoten-
tial of large pre-trained diffusion models. To reduce training costs, some meth-
ods [31,39] have introduced face editing adapter plugins for large pre-trained
diffusion models. However, these approaches primarily focus on attribute edit-
ingusingtext,whichinevitablyweakensspatialcontroltoensuretexteditability.
For example, they can only use five points [31] to control facial poses, limiting
their ability to control expressions and gaze precisely. On the other hand, direct
inpainting with masks of the face area does not take into account facial shape
changes, leading to a decrease in identity preservation.Face-Adapter 3
Toaddresstheabovechallenges,wearecommittedtodevelopinganefficient
andeffectivefaceeditingadapter(Face-Adapter)forpre-traineddiffusionmod-
els, specifically targeting face reenactment and swapping tasks. The design mo-
tivationofFace-Adapteristhreefold:(1)FullydisentangledID,targetstructure,
and attribute control enable a â€™one-model-two-tasksâ€™ approach; (2) Addressing
overlooked issues; (3) Simple yet effective, plug and play. Specifically, the pro-
posed Face-Adapter comprises three components: 1) Spatial Condition Genera-
tor (SCG in Sec. 3.1) is designed to automatically predict 3D prior landmarks
and the mask of the varying foreground area, which provides more reasonable
and precise guidance for subsequent controlled generation. In addition, for face
reenactment, this strategy mitigates potential problems that could occur when
only extracting the background from the source image, such as inconsistencies
causedbyalterationsinthetargetbackgroundduetothemovementofthecam-
eraorfaceobjects;Forfaceswapping,themodellearnstomaintainbackground
consistency, glean clues about global lighting and spatial reference, and try to
generate content in harmony with the background. 2) Identity Encoder (IE in
Sec. 3.2) uses the pre-trained recognition model to extract face embeddings and
then transfers them to the text space by learnable queries from the transformer
decoder.Thismannergreatlyimprovestheidentityconsistencyofthegenerated
images. 3) Attribute Controller (AC in Sec. 3.3) includes two sub-modules: The
spatial control combines the landmarks of target motion with the unchanged
background obtained from the Spatial Condition Generator. The attribute tem-
plate supplements the absent attribute, encompassing lighting, a portion of the
background, and hair. Both two tasks can be perceived as a procedure that exe-
cutesconditionalinpainting,utilizingtheprovidedidentityandabsentattribute
content. This process adheres to the stipulations of the given spatial control,
attaining congruity and harmony with the background. Our contributions can
be summarized as follows:
â€“ We introduce Face-Adapter, a lightweight facial editing adapter designed to
facilitateprecisecontroloveridentityandattributesforpre-traineddiffusion
models. This adapter efficiently and proficiently tackles face reenactment
and swapping tasks, surpassing previous state-of-the-art GAN-based and
diffusion-based methods.
â€“ We propose a novel Spatial Condition Generator module to predict the req-
uisite generation areas, collaborating with the Identity Encoder and At-
tribute Controller to frame reenactment and swapping tasks as conditional
inpaintingwithsufficientspatialguidance,identity,andessentialattributes.
Through reasonable and highly decoupled condition designs, we unleash the
generative capabilities of pre-trained diffusion models for both tasks.
â€“ Face-Adapterservesasatraining-efficient,plug-and-play,face-specificadapter
for pre-trained diffusion models. By freezing all parameters in the denoising
U-Net, our method effectively capitalizes on priors and prevents overfitting.
Furthermore, Face-Adapter supports a "one model for two tasks" approach,
enablingsimpleinputmodificationstoindependentlyaccomplishsuperioror
competitive results of two facial tasks on VoxCeleb1/2 datasets.4 Y. Han et al.
2 Related Work
Face Reeactment involves extracting motion from a human face and transfer-
ring it to another face [1,3,22,28,35,38,43â€“45], which can be broadly divided
into warping-based and 3DMM-based methods. Warping-based methods [14,15,
28,29,32,48]typicallyextractlandmarksorregionpairstoestimatemotionfields
and perform warping on the feature maps to transfer motions. When dealing
with large motion variations, these methods tend to produce blurry and dis-
torted results due to the difficulty in predicting accurate motion fields. 3DMM-
based methods [24]usefacialreconstructioncoefficientsorrenderedimagesfrom
3DMMasmotioncontrolconditions.Thefacialpriorprovidedby3DMMenables
these methods to obtain more robust generation results in large pose scenarios.
Despite offering accurate structure references, it only provides coarse facial tex-
tureandlacksreferencesforhair,teeth,andeyemovement.StyleHEAT[40]and
HyperReenact[2]useStyleGAN2toimprovegenerationquality.However,Style-
HEAT is limited by the dataset of frontal portraits, while HyperReenact suffers
from resolution constraints and background blurring. To further improve gen-
eration quality, diffusion models have gained popularity. FADM [42] combines
the previous reenactment model with diffusion refinements but the base model
limits the driving accuracy. Recently, AnimateAnyone [17] employs heavy tex-
ture representation encoders (CLIP and a copy of U-Net) to ensure the textural
quality of animated results, but this manner is costly. In contrast, we aim to
leveragethegenerativecapabilitiesofpre-trainedT2Idiffusionmodelsfullyand
seektocomprehensivelyovercomethechallengespresentedinpreviousmethods,
e.g., low -resolution generation, difficulty in handling large variations, efficient
training, and unexpected artifacts.
Face Swapping aims to transfer the facial identity of the source image to the
target image, with other attributes (i.e., lighting, hair, background, and mo-
tion) of the target image unchanged. Recent methods can be broadly classified
into GAN-based and diffusion-based approaches. 1) Most GAN-based meth-
ods [4,19,20,36,37,50] are dedicated to resolving the disentanglement and fu-
sionoftheidentityandotherattributes.Effortsincludeintroducingfaceparsing
masks,variouslossesforattribute-preserving,anddesigningfusionmodules.De-
spite promising improvement, these methods often produce noticeable artifacts
when dealing with significant changes in face shape or occlusions. HifiFace [33]
alleviatesthisissuebyutilizing3DMMtoreconstructareferencefacewhichcom-
bines the source face shape with other attributes of the target. However, relying
on GAN to ensure generation quality, HifiFace still fails to inpaint harmonious
results when dealing with large blank areas caused by face shape variation. 2)
Diffusion-basedmethodsutilizethegenerativecapabilitiesofthediffusionmodel
to enhance sample quality. However, the numerous denoising steps during infer-
encesignificantlyincreasethetrainingcostswhenusingattribute-preservingloss.
DiffSwap[49]proposesmidpointestimationtoaddressthisissue,buttheresult-
ingerrorandthelackofbackgroundinformationforinpaintingreferenceleadto
unnaturalresults.Moreover,thesemethodsrequirecostlytrainingfromscratch.Face-Adapter 5
Identity Encoder Identity to Tokens 3D Landmark Projector
ğ‘°ğ‘º Face Transformer{ğ’’ğ’Š} ğ“ğ‘ºğ‘« ğ‘°ğ‘º ğ‘¬ğŸ‘ğ’… id exp pose ğ¼-./
Encoder ğ‘¬ğ’Šğ’… Decoder ğ“ğ’…ğ’†ğ’„
ğŸ‘ğƒğŒğŒ
Denoising U-Net
ğ‘°ğ‘»
Attribute Controller ğ‘¬ğŸ‘ğ’… id exp pose
3D Landmark
ğ‘°ğ‘»
Ad P
aP rpr eto dinj ie cgc
t A
oto rrr
ea
ğ‘°ğ’”ğ‘»ğ’‘ğ‘°ğ’”ğ‘»ğ’‘
ğ“ğ’„ğ’•ğ’ Ad ğ¼a -.p /ting Area Predictor ğ‘´ğ‘¹ğ’‡ğ’ˆ ğ’†
Spatial Condition SpatialCondition ğ‹ğ‘¹ğ’†
Generator
Transformer ğ‘°ğ‘º â„’ğ‘´ğ‘ºğ‘¬
Reenactment ğ‘°ğ‘» Decoder ğ‹ğ’…ğ’†ğ’„ Dilate
Face Swapping ğ‘°ğ‘º
Frozen Modules {ğ’’ğ’Š} ğ‘°ğ‘»
Trainable Modules Attribute to Tokens ğ‘´ğ‘¹ğ’ˆğ’• ğ’†
Attribute Template
Fig.2: Overview pipeline of our proposed Face-Adapter that consists of three
modules:1)TheSpatialConditionGeneratorpredicts3Dpriorlandmarksandadapts
theforegroundmaskautomatically,offeringmoreaccurateguidanceforcontrolledgen-
eration.2)TheIdentityEncoderimprovesidentityconsistencyingeneratedimagesby
ğ‘´ğ‘¹ğ’‡ğ’ˆ
ğ’†
transferringfaceembeddingstotheğ‘° tğ‘» extspaceusinglearnablequeries.3)Theğ‘°ğ’ğ‘» Ağ’ğ’Œ ttributğ‹ eğ‘¹ğ’†
Controllerfeatures(i)spatialcontrolthaDt3DcFoRmbinestağ‹ğ‘¹rğ’†getmoti.onlağ‘°ğ’”ğ‘ºğ’‘ndmarksğ‘°ğ‘ºwiththe
â„’ğ‘´ğ‘ºğ‘¬
ğ‘°ğ‘º
invariantbackgroundfromtheSpatialConditioğ‘°ğ’ğ‘»nğ’ğ’ŒGeneratoğ‘´r ğ‘¹ğ’‡,ğ’ˆ
ğ’†
and(ii)anattributetem-Dilate
plate to fill in missing attributes. ğ‘°ğ‘»
ğ‘´ğ‘¹ğ’ˆğ’•
ğ’†
In contrast, our Face-Adapter ensure image quality only relying on the denoise
losswithcompletedisentanglementofthecontrolofthetargetstructure,IDand
other attributes. Moreover, Face-Adapter further significantly reduces training
costsbyfreezingallofU-Netâ€™sparameters,whichalsopreservespriorknowledge
and preDv3DeFRnts overfitting.
Personalization of Pretrained Diffusion Models. Personalization aims to
insertagivenidentityintothepre-trainedT2Idiffusionmodels.Earlyworks[11,
26] insert identity by using optimization or fine-tuning manners. Subsequent
studies [5,23,34] introduce coarse spatial control, achieving multi-subject gen-
eration and regional attribute editing with text, but these methods require fine-
tuningofmostparameters.IP-adapter(-FaceID)[39]andInstantID[31]fine-tune
only a few parameters. The latter achieves robust identity preservation. How-
ever, as a tradeoff for text editability, InstantID could only apply weak spatial
control.Therefore,itstruggleswithfinemovements(expressionandgaze)inface
reenactmentandswapping.Bycomparison,ourFace-Adapterisaneffectiveand
lightweight adapter designed for pre-trained diffusion models to accomplish face
reenactment and swapping simultaneously.
3 Methods
ThecomprehensivestructureoftheproposedFace-AdapterisillustratedinFig.2,
which aims to integrate identity into the attribute template, which provides es-
ğ’‘ğ’Šğ’ğ’„ğ‘¬
PILC6 Y. Han et al.
sential attributes (e.g., lighting, a portion of the background, and hair) based
on the target motion (e.g., pose, expression, and gaze).
3.1 Spatial Condition Generator
To provide more reasonable and precise guidance for subsequent controlled gen-
eration, we design a novel Spatial Condition Generator (SCG) to automatically
predict 3D prior landmarks and the mask of the varying foreground area. In
detail, this component consists of two sub-modules:
3D Landmark Projector. To surmount alterations in facial shape, we utilize
a 3D facial reconstruction method [8] to extract the identity, expression indi-
vidually and pose coefficients of the source and target faces. Subsequently, we
recombine the identity coefficients of the source with the expression and pose
coefficients of the target, reconstruct a new 3D face, and project it to acquire
the corresponding landmarks.
Adapting Area Predictor. For face reenactment, prior methods assume that
only the subject is in motion, while the background remains static in the train-
ing data. However, we observe that the background actually undergoes changes,
encompassing the movement of both the camera and objects in the background,
as illustrated in Fig. 3. If the model lacks knowledge of the background motion
duringtraining,itwilllearntogenerateablurrybackground.Forfaceswapping,
supplying the target background can also give the model clues about environ-
mental lighting and spatial references. This added constraint of the background
significantlydiminishesthedifficultyofthemodellearning,transitioningitfrom
learningataskofgeneratingfromscratchtoataskofconditionalinpainting.As
aresult,themodelbecomesmoreattunedtopreservingbackgroundconsistency
and generating content that seamlessly integrates with it.
Case 1: Moving Objects Case 2: Moving Camera
Fig.3:Backgroundinconsistencybetweentheinput(i.e.,source)andthegroundtruth
(i.e.,target)makesthemodelconfusedandfailtolearntogenerateclearbackground.
Thus, we provide the background of the target image in the spatial condition during
training to address this inconsistency.
Inviewoftheabove,weintroducealightweightAdaptingAreaPredictorfor
both face reenactment and swapping, automatically predicting the region the
model needs to generate (the adapting area) while maintaining the remaining
area unchanged. For face reenactment, the adapting area constitutes the region
occupied by the source image head before and after reenactment. We train aFace-Adapter 7
Source Target Results Pre-trained Adaptive
Fig.4: Comparisons with mask generated by pre-trained face parsing model (green)
and Ï† (white). The green mask cannot fully cover the entire portrait.
Re
mask predictor Ï† that accepts the target image IT and motion landmarks
Re
I from the 3D Landmark Projector to predict the adapting area mask Mfg.
lmk Re
ThemaskgroundtruthMgt isgeneratedbytakingtheunionoftheheadregions
Re
(including hair, face, and neck) of the source and target, followed by outward
dilation. Head regions are obtained using a pre-trained face parsing model [41].
It should be noted that we cannot directly utilize the pre-trained face parsing
model in face reenactment. As shown in Fig. 4 row 4, when the portrait area of
thesourceimageislarger(e.g.,longhairandhat)thanthatinthetargetimage,
the green mask created by the pre-trained parsing model cannot fully cover the
entire portrait and may result in artifacts at the boundary. However, the white
mask created by Ï† in Fig. 4 row 5 can encapsulate the whole portrait, as Ï†
Re Re
merelyusesthesourceimageand3Dlandmarksasinput,andexhibitsexcellent
generalization when the source and target images possess different identities.
For face-swapping, the adapting area constitutes the facial region of the tar-
get image IT. We employ a pre-trained face parsing model [41] to predict the
adapting area mask Mfg of the target image IT. Nonetheless, to accommodate
Sw
faceshapedifferencesduringtesting,wedesignatethegroundtruthMgt asthe
Sw
region obtained by dilating the facial area outward.
3.2 Identity Encoder
AsdemonstratedbyIP-Adapter-FaceID[39]andInstantID[31],ahigh-levelface
embedding can ensure more robust identity preservation. As we observed, there
is no need for heavy texture encoders [17] or additional identity networks [31]
in face reenactment/swapping. By merely tuning a lightweight mapping module
to map the face embedding into the fixed textual space, identity preservation
is guaranteed. Specifically, given a face image IS, the face embedding f is
id
obtained by a pre-trained face recognition model E [7]. Subsequently, a three-
id
layer transformer decoder Ï• is employed to project the face embedding f
dec id
into the fixed text semantic space of the pre-trained diffusion model, obtaining
the identity tokens. The specified number N (we set N = 77 in this paper) of
learnable queries q = {q ,q ,Â·Â·Â· ,q } in the transformer decoder constrains
id 1 2 N8 Y. Han et al.
the sequence length of the identity embedding, ensuring it does not exceed the
maximum length of the text embedding. Through this approach, the U-Net of
the pre-trained diffusion model does not require any fine-tuning to adapt to the
face embedding.
3.3 Attribute Controller
Spatial Control. In line with ControlNet [46], we create a copy of U-Net Ï•
Ctl
and add spatial control I as the conditioning input. The spatial control image
Sp
IS /IT is obtained by combining the target motion landmarks IT and the
Sp Sp lmk
non-adapting area obtained by the Adapting Area Predictor Ï† (or Ï† ).
Re Sw
IS =IS âˆ—(1âˆ’Mfg)+IT , for face reenactment,
Sp Re lmk
IT =IT âˆ—(1âˆ’Mfg)+IT , for face swapping.
Sp Sw lmk
At this juncture, both reenactment and swapping tasks can be viewed as pro-
cessesofperformingconditionalinpainting,utilizingthegivenidentityandother
missing attribute content, following the provided spatial control.
Attribute Template.Givenidentityandspatialcontrolwithpartoftheback-
ground, the attribute template is designed to supplement the missing informa-
tion, including lighting and part of the background and hair. Attribute embed-
dings f âˆˆ R257âˆ—d are extracted from the attribute template (IS for reenact-
attr
mentandIT forswapping)usingCLIPE .Tosimultaneouslyobtainlocaland
clip
global features, we use both the patch tokens and the global token. The feature
mapper module is also constructed as a three-layer transformer layer Ï† with
dec
learnable queries q ={q ,q ,Â·Â·Â· ,q }, K =77.
attr 1 2 K
3.4 Strategies for Boosting Performance
Training. 1) Data Stream: For both reenactment and face-swapping tasks, we
usetwoimagesofthesamepersonindifferentposesassourceandtargetimages.
To support a â€œone model for both taskâ€ approach, we use a 50% probability to
choose between reenactment and face-swapping data streams during training,
i.e., the spatial control and attribute template in the Attribute Controller use
the data streams indicated by red and blue respectively. 2) Condition Dropping
for Classifier-free Guidance: The conditions we need to drop include identity
tokensandattributetokensinputintotheU-NetandControlNetcross-attention.
We use a 5% probability to simultaneously drop identity tokens and attribute
conditions to enhance the realism of the image. To fully utilize the identity
tokens for generation face images and improve identity preservation, we use an
additional 45% probability to drop attribute tokens.
Inference.1)AdaptingAreaPredictor :Forreenactment,theinputisthesource
(whichisdifferentfromtraining)andcorrectedlandmarks,andtheoutputisthe
adapting area. For face-swapping, the input is the target, and the output is the
adapting area. 2) Negative Prompt for Classifier-Free Guidance: For reenact-
ment,negativepromptsofbothidentityandattributetokensareemptypromptFace-Adapter 9
embeddings. For face-swapping, to overcome the negative impact of the target
identity in attribute tokens, we use the identity tokens of the target image as
the negative prompt for identity tokens.
4 Experiments
4.1 Experimental Setup
Datasets.Duringtraining,weleveragetheVoxCeleb1andVoxCeleb2[6]dataset.
During the evaluation, we leverage the 491 test videos from the VoxCeleb1 [21]
dataset and randomly sample 1,000 images in quantitative evaluation for face
reenactment. We use FaceForensics++ [25] in quantitative evaluation for face
swapping. We also spare 1,000 images from VoxCeleb2 for qualitative evalua-
tion.FollowingthepreprocessingmethodinFOMM[29],wecropfacesfromthe
original videos and resize them to 512Ã—512 for training and evaluation.
Evaluation Metrics. For face reenactment, we use PSNR and LPIPS [47]
to evaluate the reconstruction quality for same-identity reenactment. We use
FID [13] to evaluate the overall quality of the generated images. We use co-
sine similarity (CSIM) calculated by [18] to evaluate identity preservation. The
motion transfer error is measured by Pose, Exp, and Gaze, which calculate the
average Euclidean distances of pose, expression, and gaze coefficients between
the generated and drive images. For face swapping, ID retrieval (ID) retrieves
the closest face to evaluate identity modification, while Pose, Exp, and Gaze
evaluate the attribute error between the generated faces and target faces.
Implementation Details. The Adapting Area Predictor is modified from the
parsingmodel[41],with6inputchannelsand1outputchannel.Theidentity-to-
tokensisimplementedwitha3-layertransformerdecoder,alinearlayerisadded
to project the identity feature dimensions to 768. The architecture of attribute-
to-tokens is the same as the identity-to-tokens, except the input dimensions of
the linear layer are consistent with the output dimensions of the CLIP model.
We adopt the StableDiffusion v1-5 [9] as the pre-trained diffusion model and
clip-vit-large-patch14 [10] from OpenAI as the CLIP vision model in this paper.
We train our face-adapter for 70,000 steps on 8Ã—V100 NVIDIA GPUs with a
constant learning rate of 1e-4 and a batch size of 32.
4.2 Comparison with State-of-the-Art Methods
Face Reenactment. In Tab. 1, we compare with SoTA methods quantita-
tivelyonVoxCeleb1testset,includingGAN-basedFOMM[29],PIRenderer[24],
DG[16],TPSM[48],DAM[30],HyperReenact[2]anddiffusion-basedFADM[42].
FOMM, TPSM,andDAMarewarping-basedtechniques, whilePIRenderer and
HyperReenact are 3DMM-based.
We achieve comparable or even optimal results in image quality. Owing to
theSpatialConditionGenerator,duringtraining,incorporatingthetargetback-
ground area in spatial condition avoids the interference of background motion.10 Y. Han et al.
Fig.5: Same-identity face reenactment results on Voxceleb2 test set. Our
method faithfully reconstructs the background and facial details.
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
Fig.6: Cross-identity face reenactment results on Voxceleb2 test set. Our
method significantly surpasses previous methods in terms of image quality and mo-
tioncontrolaccuracy,includingpose,expression,andgaze,evenunderextremeposes.
src, target, 'simswap', 'hififace', 'Infoswap', 'blendface_raw', 'DiffSwap', 'ours'
Moreover, we faithfully maintain consistency with the source in local details such as
background and accessories, as well as global lighting.Face-Adapter 11
Table 1: Quantitative evaluations among SoTAs on Voxceleb1 test set. Bold and
underline correspond to the optimal and sub-optimal values, respectively.
Same-Identity Cross-Identity
Methods
PSNRâ†‘ LPIPSâ†“ FIDâ†“ Expâ†“ Poseâ†“ Gazeâ†“ CSIMâ†‘ Expâ†“ Poseâ†“ Gazeâ†“ CSIMâ†‘ FIDâ†“
FOMM[29] 22.77 0.1344 31.19 2.92 0.0276 0.0566 0.8499 6.89 0.0644 0.1003 0.539 51.57
PIRenderer[24] 21.65 0.1388 29.98 3.08 0.0409 0.0798 0.819 6.42 0.0646 0.0963 0.5361 40.71
DG[16] 14.01 0.4928 102.17 6.16 0.0707 0.112 0.0972 7.16 0.074 0.1287 0.0834 102.61
TPSM[48] 23.8 0.1367 34.11 2.70 0.0234 0.0627 0.8536 6.58 0.0548 0.0959 0.5514 54.83
DAM[30] 23.85 0.1484 38.6 2.87 0.027 0.0675 0.8505 6.82 0.0636 0.1034 0.5198 62.77
HyperReenact[2] 15.73 0.3361 88.72 3.68 0.0381 0.0743 0.5455 5.94 0.0452 0.0812 0.4665 88.02
FADM[42] 22.70 0.1392 31.58 3.11 0.0324 0.086 0.8472 7.03 0.0786 0.1239 0.6152 42.7
Ours 22.36 0.1281 29.27 3.24 0.0243 0.0415 0.7146 6.45 0.0355 0.0543 0.6429 41.09
Duringinference,addingthesourcebackgroundinspatialconditionsignificantly
reduces the difficulty of generating backgrounds, improving background consis-
tency. As a result, our method is capable of producing high-quality images with
clear advantages in FID scores as well as in reconstruction metrics, i.e., PSNR
and LPIPS. In terms of motion control, our method performs well in pose and
gaze error, but not as well in expression error. As our landmarks are derived
from D3DFR, both the reconstruction and projection processes, along with the
sparsity of the landmarks, result in a loss of expression accuracy. Therefore, our
methodachievesarelativelymoderateperformanceintermsofexpressionerror.
In Fig. 5 and Fig. 6, we compare with SoTA methods qualitatively on Vox-
Celeb1 and Voxceleb2 test set. The Spatial Condition Generator effectively en-
sures that our results are consistent with the source background and meanwhile
reduces the training difficulty of the model, allowing it to focus more on face
generation and improve the image quality. Freezing all parameters of the U-
Net avoids overfitting and preserves as much of the powerful prior from the
pre-trained diffusion model as possible. As a result, compared to other GAN-
based methods and diffusion-based methods trained from scratch like FADM,
ourmethodiscapableofgeneratingfaithfulattributedetails, i.e.,hairtexture,
hat, and accessories, that are consistent with the source image.
In addition to local details, the attribute tokens in the Attribute Controller
effectively extract global illumination from the source image, significantly out-
performingothermethods.Thisfurtherhighlightsthestrengthsandcapabilities
of our proposed approach in capturing both local and global features, leading
to more realistic and accurate results. Even when dealing with large poses, the
Identity Encoder ensures robust identity preservation, and the pre-trained dif-
fusionmodelreasonablygeneratesattributessuchaslonghairthatmovesalong
with the face, demonstrating the superiority of our proposed adapter.
Face Swapping. In Tab. 2, we compare with SoTA methods quantitatively on
FaceForensics++ test set, including GAN-based FaceShifter [19], SimSwap [4],
HifiFace [33], InfoSwap [12], BlendFace [27] and diffusion-based DiffSwap [49].
Our 3D Landmark Projector helps to fuse the source face shape and target
pose, expression and gaze to obtain the target motion landmarks in our spatial12 Y. Han et al.
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours
Fig.7: Face swapping qualitative comparison results on Voxceleb2 test set.
Our method handles large facial shape changes effectively. It is capable of reasonably
inpainting the blank area of the background caused by alternations in facial shape.
control.OurAdaptingAreaThepredictorallowsamplespaceforchangesinface
shape while keeping enough background for inpainting. This combined spatial
condition benefits the modelâ€™s generation of natural images. Although DiffSwap
alsoutilizesshape-awarelandmarksviaD3DFRasspatialcontrol,itsinpainting
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
processonlytakesplaceduringDDIMsampling.Lackingabackgroundreference
makesitdifficultforthemodeltogenerateclearfacialresults,whichsignificantly
affectsimagequalityandIDsimilarity.OnthecommonlyusedFaceForensics++
src, target, 'simswap', 'hififace', 'Infoswap', 'blendface_raw', 'DiftfeSswtasept',, 'oouurrsm' ethodiscomparabletoGAN-basedmethodsintermsofID,Pose,
Exp and Gaze. Therefore, our method exhibits remarkable advantages in terms
of ID while maintaining high motion accuracy compared to both GAN-based
and diffusion-based SoTAs.
Fig.7andFig.8showsaqualitativecomparisonbetweenourmethodandre-
centSoTAmethods.Previousmethodsstrugglewithhandlingsignificantchanges
infaceshapeandlargepose.Whentransferringathin-facedpersontoafat-faced
target image, these methods typically maintain the face shape of the target im-
age, leading to a significant loss of identity. In contrast, our spatial control ef-
fectively addresses the issue of face shape changes. Unlike previous approaches
thatmerelycropoutthefacialregion,ourAdaptingAreaPredictorallowsample
space for changes in face shape. With the powerful generation capability of the
pre-trained SD model, we can naturally complete the regions with facial shape
variations. Furthermore, by using the identity tokens of the target image as aFace-Adapter 13
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours Source Target SimSwap Hififace InfoSwap Blendface DiffSwap Ours
Fig.8: Face swapping qualitative comparison results on Voxceleb2 test set.
Comparedtopreviousmethods,ourapproachfaithfullymaintainsidentityevenunder
significant pose changes.
negativepromptduringface-swappinginference,wefurtherenhancetheidentity
similarity with the source face. As for large poses, previous methods struggle to
generate plausible results, while our method directly generates faces from 3D
landmarks without being affected by the pose.
Source Target FOMM PIRender DG TPSM DAM FADM HyperReenact Ours
4.3 Ablation Study and Further Analysis
Weconductedanablatsirocn, tsatrugdeyt,o 'snimthsewAadpa',p 'htiinfigfaAcere',a 'InPfroesdwicatpo'r, 'abnlednadsfsaecssee_dratwhe', 'DiffSwap', 'ours'
necessity of fine-tuning CLIP. For a fair comparison, all three models here were
trained for 35,000 steps. Quantitative evaluations are conducted on Voxceleb1
cross-identity test set for both face reenactment and swapping tasks.
Adapting Area Predictor. As demonstrated in Tab. 3 and Fig. 9, without
the Adapting Area Predictor, the spatial control lacks the background and only
includeslandmarksfromthe3DLandmarkProjector.Duringtraining,themodel
extracts the background features from the source image in face reenactment,
while using the target image background as the ground truth. This discrepancy
tendstoresultinthemodelhallucinatingbackground,andthemodelstrugglesto
maintain consistency with the background of the source image during inference.
As for face swapping, the model is not trained with inpainting task, which leads
to noticeable unnatural artifacts when blending the face with the surrounding
area during inference.14 Y. Han et al.
Table2:QuantitativeresultsonthetaskoffaceswappingonFF++.Comparedtothe
diffusion-based DiffSwap, our method significantly improves the metrics and achieves
highly competitive results. Note that our method can simultaneously perform
both face reenactment and swapping.Boldcorrespondstotheoptimalvalues.âˆ—:
evaluated results are from the official code. â€ : evaluated results are from the officially
released generated videos.
Methods IDâ†‘ Poseâ†“ Expâ†“ Gazeâ†“
FaceShifter[19]â€  87.99 0.0342 6.32 0.072
SimSwap[4]âˆ— 96.78 0.0261 5.94 0.0549
HifiFace[33]â€  94.26 0.0382 6.50 0.0573
InfoSwap[12]âˆ— 99.26 0.0371 7.25 0.0617
BlendFace[27]âˆ— 89.91 0.0286 6.15 0.0556
DiffSwap[49]âˆ— 19.16 0.0237 4.94 0.0665
Ours 96.47 0.0319 6.66 0.0607
Fine-tuning CLIP for Extracting Attribute Features. As demonstrated
in Tab.3and Fig.9,freezingtheCLIPresultsinadeclineindetailedattributes
and image quality. The pre-trained CLIP is trained for discrimination tasks and
lacks detailed texture features needed for generation tasks. Fine-tuning CLIP
helps to extract detailed attribute features, including hair, clothing, part of the
missing backgrounds, and global lighting; in addition to this, the fine-tuned
CLIP model also extracts some features related to face identity, which benefits
the identity similarity score in face reenactment.
Source Target w/o AAP w/o CLIP FT Ours
Fig.9:AblationstudyforSpatialConditionGeneratorandCLIPfinetuning.
The red boxes highlight the artifacts in the picture.Face-Adapter 15
Table3:Quantitativecomparisonofourmodelunderdifferentablativeconfigurations.
FaceReenactmnet FaceSwapping
Methods
FIDâ†“ Poseâ†“ Expâ†“ Gazeâ†“ IDâ†‘ FIDâ†“ Poseâ†“ Expâ†“ Gazeâ†“ IDâ†‘
w/oAAP 33.61 0.0281 3.72 0.045 0.6355 33.97 0.0395 6.13 0.0548 0.4530
w/oCLIPFT 33.09 0.0287 3.74 0.0435 0.6474 31.97 0.0396 6.21 0.0540 0.4696
FullModel 31.18 0.0266 3.61 0.0422 0.6616 30.78 0.0406 6.14 0.0547 0.4688
5 Conclusion
In this paper, we present a novel Face-Adapter framework, a plug-and-play fa-
cial editing adapter that supports fine control over identity and attributes for
pretrained diffusion models. Utilizing only one model, this adapter effectively
addressesfacereenactmentandswappingtasks,surpassingpreviousstate-of-the-
art GAN-based and diffusion-based methods. It comprises a Spatial Condition
Generator, an Identity Encoder, and an Attribute Controller. The Spatial Con-
dition Generator is used to predict the 3D prior landmarks and the mask of the
areathatneedstobechanged,workingwiththeIdentityEncoderandAttribute
Controller to formulate reenactment and swapping tasks as conditional inpaint-
ing with sufficient spatial guidance, identity, and essential attributes. Through
reasonableandhighlydecoupledconditiondesign,weunleashthegenerativeca-
pabilitiesofpretraineddiffusionmodelsforfacereenactmentandswappingtasks.
Extensive qualitative and quantitative experiments demonstrate the superiority
of our method.
Limitations Our unified model is unable to achieve temporal stability in video
face reenactment/ swapping, which requires incorporating additional temporal
fine-tuning in the future.
PotentialSocialImpactForthefirsttime,weexplorealightweightframework
based on diffusion for simultaneous face reenactment and swapping, which has
higher practical value while improving the quality of generated content. How-
ever, the potential misuse of Face-Adapter can lead to privacy invasion, mis-
information spread, and ethical concerns. To mitigate these risks, both visible
and invisible digital watermarks can be incorporated to help identify the origin
and authenticity of the content. On the other side, Face-Adapter can contribute
to the field of forgery detection, further enhancing the ability to identify and
combat deepfakes.
References
1. Agarwal,M.,Mukhopadhyay,R.,Namboodiri,V.P.,Jawahar,C.:Audio-visualface
reenactment.In:ProceedingsoftheIEEE/CVFWinterConferenceonApplications
of Computer Vision. pp. 5178â€“5187 (2023) 4
2. Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I., Tzimiropoulos, G.: Hyper-
reenact: one-shot reenactment via jointly learning to refine and retarget faces. In:16 Y. Han et al.
Proceedings of theIEEE/CVF InternationalConference on ComputerVision. pp.
7149â€“7159 (2023) 1, 4, 9, 11
3. Bounareli, S., Tzelepis, C., Argyriou, V., Patras, I., Tzimiropoulos, G.: Hyper-
reenact: one-shot reenactment via jointly learning to refine and retarget faces. In:
Proceedings of theIEEE/CVF InternationalConference on ComputerVision. pp.
7149â€“7159 (2023) 4
4. Chen, R., Chen, X., Ni, B., Ge, Y.: Simswap: An efficient framework for high
fidelity face swapping. In: Proceedings of the 28th ACM International Conference
on Multimedia. pp. 2003â€“2011 (2020) 1, 4, 11, 14
5. Choi, J., Choi, Y., Kim, Y., Kim, J., Yoon, S.: Custom-edit: Text-guided image
editing with customized diffusion models. arXiv preprint arXiv:2305.15779 (2023)
5
6. Chung, J.S., Nagrani, A., Zisserman, A.: Voxceleb2: Deep speaker recognition.
arXiv preprint arXiv:1806.05622 (2018) 9
7. Deng,J.,Guo,J.,Xue,N.,Zafeiriou,S.:Arcface:Additiveangularmarginlossfor
deepfacerecognition.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 4690â€“4699 (2019) 7
8. Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3d face recon-
struction with weakly-supervised learning: From single image to image set. In:
Proceedings of the IEEE/CVF conference on computer vision and pattern recog-
nition workshops. pp. 0â€“0 (2019) 6
9. Face, H.: Runwayml stable diffusion v1.5. https://huggingface.co/runwayml/
stable-diffusion-v1-5, accessed on: yyyy-mm-dd 9
10. Foundations,M.:Openclip:Open-sourceimplementationofclip.https://github.
com/mlfoundations/open_clip (2022), accessed on: yyyy-mm-dd 9
11. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
Cohen-Or, D.: An image is worth one word: Personalizing text-to-image gener-
ation using textual inversion. arXiv preprint arXiv:2208.01618 (2022) 5
12. Gao,G.,Huang,H.,Fu,C.,Li,Z.,He,R.:Informationbottleneckdisentanglement
for identity swapping. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 3404â€“3413 (2021) 1, 11, 14
13. Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,Hochreiter,S.:Ganstrained
byatwotime-scaleupdateruleconvergetoalocalnashequilibrium.Advancesin
neural information processing systems 30 (2017) 9
14. Hong,F.T.,Xu,D.:Implicitidentityrepresentationconditionedmemorycompen-
sationnetworkfortalkingheadvideogeneration.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 23062â€“23072 (2023) 4
15. Hong, F.T., Zhang, L., Shen, L., Xu, D.: Depth-aware generative adversarial net-
workfortalkingheadvideogeneration.In:ProceedingsoftheIEEE/CVFconfer-
ence on computer vision and pattern recognition. pp. 3397â€“3406 (2022) 4
16. Hsu,G.S.,Tsai,C.H.,Wu,H.Y.:Dual-generatorfacereenactment.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
642â€“650 (2022) 1, 9, 11
17. Hu,L.,Gao,X.,Zhang,P.,Sun,K.,Zhang,B.,Bo,L.:Animateanyone:Consistent
and controllable image-to-video synthesis for character animation. arXiv preprint
arXiv:2311.17117 (2023) 4, 7
18. Huang,Y.,Wang,Y.,Tai,Y.,Liu,X.,Shen,P.,Li,S.,Li,J.,Huang,F.:Curricular-
face:adaptivecurriculumlearninglossfordeepfacerecognition.In:proceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.5901â€“
5910 (2020) 9Face-Adapter 17
19. Li,L.,Bao,J.,Yang,H.,Chen,D.,Wen,F.:Faceshifter:Towardshighfidelityand
occlusion aware face swapping. arXiv preprint arXiv:1912.13457 (2019) 4, 11, 14
20. Liu,Z.,Li,M.,Zhang,Y.,Wang,C.,Zhang,Q.,Wang,J.,Nie,Y.:Fine-grainedface
swappingviaregionalganinversion.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 8578â€“8587 (2023) 4
21. Nagrani,A.,Chung,J.S.,Zisserman,A.:Voxceleb:alarge-scalespeakeridentifica-
tion dataset. arXiv preprint arXiv:1706.08612 (2017) 9
22. Nirkin,Y.,Keller,Y.,Hassner,T.:Fsgan:Subjectagnosticfaceswappingandreen-
actment. In: Proceedings of the IEEE/CVF international conference on computer
vision. pp. 7184â€“7193 (2019) 4
23. Peng, X., Zhu, J., Jiang, B., Tai, Y., Luo, D., Zhang, J., Lin, W., Jin, T., Wang,
C., Ji, R.: Portraitbooth: A versatile portrait model for fast identity-preserved
personalization. arXiv preprint arXiv:2312.06354 (2023) 5
24. Ren, Y., Li, G., Chen, Y., Li, T.H., Liu, S.: Pirenderer: Controllable portrait im-
age generation via semantic neural rendering. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 13759â€“13768 (2021) 1, 4, 9, 11
25. Rossler, A., Cozzolino, D., Verdoliva, L., Riess, C., Thies, J., NieÃŸner, M.: Face-
forensics++:Learningtodetectmanipulatedfacialimages.In:Proceedingsofthe
IEEE/CVF international conference on computer vision. pp. 1â€“11 (2019) 9
26. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 22500â€“22510 (2023) 5
27. Shiohara, K., Yang, X., Taketomi, T.: Blendface: Re-designing identity encoders
for face-swapping. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 7634â€“7644 (2023) 1, 11, 14
28. Siarohin,A.,LathuiliÃ¨re,S.,Tulyakov,S.,Ricci,E.,Sebe,N.:Animatingarbitrary
objectsviadeepmotiontransfer.In:ProceedingsoftheIEEE/CVFConferenceon
Computer Vision and Pattern Recognition. pp. 2377â€“2386 (2019) 4
29. Siarohin, A., LathuiliÃ¨re, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion
modelforimageanimation.Advancesinneuralinformationprocessingsystems32
(2019) 1, 4, 9, 11
30. Tao, J., Wang, B., Xu, B., Ge, T., Jiang, Y., Li, W., Duan, L.: Structure-aware
motiontransferwithdeformableanchormodel.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 3637â€“3646 (2022)
1, 9, 11
31. Wang, Q., Bai, X., Wang, H., Qin, Z., Chen, A.: Instantid: Zero-shot identity-
preserving generation in seconds. arXiv preprint arXiv:2401.07519 (2024) 2, 5,
7
32. Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head syn-
thesis for video conferencing. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 10039â€“10049 (2021) 4
33. Wang, Y., Chen, X., Zhu, J., Chu, W., Tai, Y., Wang, C., Li, J., Wu, Y., Huang,
F.,Ji,R.:Hififace:3dshapeandsemanticpriorguidedhighfidelityfaceswapping.
arXiv preprint arXiv:2106.09965 (2021) 1, 4, 11, 14
34. Xiao, G., Yin, T., Freeman, W.T., Durand, F., Han, S.: Fastcomposer: Tuning-
free multi-subject image generation with localized attention. arXiv preprint
arXiv:2305.10431 (2023) 5
35. Xu,C.,Zhang,J.,Han,Y.,Tian,G.,Zeng,X.,Tai,Y.,Wang,Y.,Wang,C.,Liu,Y.:
Designing one unified framework for high-fidelity face reenactment and swapping.
In: European Conference on Computer Vision. pp. 54â€“71. Springer (2022) 418 Y. Han et al.
36. Xu, C., Zhang, J., Hua, M., He, Q., Yi, Z., Liu, Y.: Region-aware face swapping.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 7632â€“7641 (2022) 4
37. Xu, Z., Hong, Z., Ding, C., Zhu, Z., Han, J., Liu, J., Ding, E.: Mobilefaceswap:
A lightweight framework for video face swapping. In: Proceedings of the AAAI
Conference on Artificial Intelligence. vol. 36, pp. 2973â€“2981 (2022) 4
38. Yang, K., Chen, K., Guo, D., Zhang, S.H., Guo, Y.C., Zhang, W.: Face2face Ï:
Real-time high-resolution one-shot face reenactment. In: European conference on
computer vision. pp. 55â€“71. Springer (2022) 4
39. Ye, H., Zhang, J., Liu, S., Han, X., Yang, W.: Ip-adapter: Text compati-
ble image prompt adapter for text-to-image diffusion models. arXiv preprint
arXiv:2308.06721 (2023) 2, 5, 7
40. Yin, F., Zhang, Y., Cun, X., Cao, M., Fan, Y., Wang, X., Bai, Q., Wu, B., Wang,
J., Yang, Y.: Styleheat: One-shot high-resolution editable talking face generation
viapre-trainedstylegan.In:Europeanconferenceoncomputervision.pp.85â€“101.
Springer (2022) 4
41. Yu,C.,Wang,J.,Peng,C.,Gao,C.,Yu,G.,Sang,N.:Bisenet:Bilateralsegmenta-
tionnetworkforreal-timesemanticsegmentation.In:ProceedingsoftheEuropean
conference on computer vision (ECCV). pp. 325â€“341 (2018) 7, 9
42. Zeng,B.,Liu,X.,Gao,S.,Liu,B.,Li,H.,Liu,J.,Zhang,B.:Faceanimationwith
anattribute-guideddiffusionmodel.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 628â€“637 (2023) 2, 4, 9, 11
43. Zeng, X., Pan, Y., Wang, M., Zhang, J., Liu, Y.: Realistic face reenactment via
self-supervised disentangling of identity and pose. In: Proceedings of the AAAI
Conference on Artificial Intelligence. vol. 34, pp. 12757â€“12764 (2020) 4
44. Zhang, B., Qi, C., Zhang, P., Zhang, B., Wu, H., Chen, D., Chen, Q., Wang, Y.,
Wen, F.: Metaportrait: Identity-preserving talking head generation with fast per-
sonalizedadaptation.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 22096â€“22105 (2023) 4
45. Zhang,J.,Zeng,X.,Wang,M.,Pan,Y.,Liu,L.,Liu,Y.,Ding,Y.,Fan,C.:Freenet:
Multi-identity face reenactment. In: Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition. pp. 5326â€“5335 (2020) 4
46. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 3836â€“3847 (2023) 8
47. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 586â€“595 (2018) 9
48. Zhao,J.,Zhang,H.:Thin-platesplinemotionmodelforimageanimation.In:Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion. pp. 3657â€“3666 (2022) 1, 4, 9, 11
49. Zhao, W., Rao, Y., Shi, W., Liu, Z., Zhou, J., Lu, J.: Diffswap: High-fidelity and
controllable face swapping via 3d-aware masked diffusion. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8568â€“
8577 (2023) 2, 4, 11, 14
50. Zhu,Y.,Li,Q.,Wang,J.,Xu,C.Z.,Sun,Z.:Oneshotfaceswappingonmegapix-
els. In: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. pp. 4834â€“4844 (2021) 4