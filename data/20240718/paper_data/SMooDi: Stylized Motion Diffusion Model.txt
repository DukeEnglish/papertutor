SMooDi: Stylized Motion Diffusion Model
Lei Zhong1, Yiming Xie1, Varun Jampani2, Deqing Sun3, and Huaizu Jiang1
1 Northeastern University
2 Stability AI
3 Google Research
{le.zhong,xie.yim,h.jiang}@northeastern.edu
varunjampani@gmail.com
deqingsun@google.com
Abstract. WeintroduceanovelStylizedMotionDiffusionmodel,dubbed
SMooDi, to generate stylized motion driven by content texts and style
motion sequences. Unlike existing methods that either generate motion
ofvariouscontentortransferstylefromonesequencetoanother,SMooDi
canrapidlygeneratemotionacrossabroadrangeofcontentanddiverse
styles.Tothisend,wetailorapre-trainedtext-to-motionmodelforstyl-
ization.Specifically,weproposestyleguidancetoensurethatthegener-
ated motion closely matches the reference style, alongside a lightweight
style adaptor that directs the motion towards the desired style while
ensuring realism. Experiments across various applications demonstrate
that our proposed framework outperforms existing methods in stylized
motion generation. Project Page: https://neu-vi.github.io/SMooDi/
Keywords: Motion synthesis ¬∑ Diffusion model ¬∑ Stylized motion
1 Introduction
We address the problem of generating stylized motion from a content text and
a style motion sequence, as shown in Fig. 1. Human motion can typically be
characterized by two components: content and style. Motion content represents
thenatureofamovement,suchaswalkingandwaving,andmotionstylereflects
individual characteristics, such as personality traits (e.g., old, childlike) and
emotions (e.g., happy, angry). Traditional pipelines create stylized motions via
motion capture from actors, and are both labor-intensive and time-consuming.
Therefore, decades of research have focused on developing automatic methods
to assist stylized motion creation [1,11,19].
Motion style transfer [1,45] is a practical and popular approach for the cre-
ation of stylized motion. It transfers the style from an existing style motion
sequence to another existing content motion sequence. However, when a broad
array of motion needs to be stylized, the pipeline may be inefficient ‚Äì it would
first require the collection of a large number of content motion sequences and
then apply a motion style transfer method to process each sequence indepen-
dently. Moreover, motion sequences are not always readily available, especially
4202
luJ
71
]VC.sc[
1v38721.7042:viXra2 Zhong et al.
Style Motion
Style
Content Aeroplan Chicken In the Dark Superman
‚ÄòA person walks
forward and then
sitsdown.‚Äô
(a) (b) (c) (d)
Fig.1: SMooDi can generate realistic, stylized human motions given a con-
tent text a style motion sequence. It also accepts a motion sequence as content
input. Darker color indicates later frames in the sequence. To better showcase the
stylized motion generation, we place the style label for the each of the style motion
sequence. Note that such style labels are not used as model input and shown here for
visualization purpose only. (Best viewed in color.)
for some customized content, such as running along a specific trajectory. They
may still need to be created first by actors or animators for stylization.
Recent advances of human motion generation with diffusion models [10,17]
have shown impressive results of creating diverse and realistic human motions.
Butmosteffortshaveconcentratedonefficientlyandaccuratelytranslatingtex-
tual prompts into human motions, focusing on the content only [6,31,46]. Inte-
gratingthestyle conditiontogeneratestylizedmotionsremainsunder-explored.
Combiningthesetwolinesofresearchisastraightforwardapproachtotackle
stylized motion generation, where a motion style transfer method [1,45] can be
applied to each motion sequence generated by a text-driven motion diffusion
model [6,31,46]. However, in addition to the aforementioned inefficiency issue,
this approach has two more limitations. First, error may accumulate across the
pipeline.As,motionstyletransfermethodsareusuallytrainedwithhigh-quality
real-worldmotionsequences,weempiricallyobservethattheirperformancemay
significantly degrade for imperfect motions produced by text-to-motion tech-
niques. Second, existing motion style transfer methods rely on specialized style
datasets[1,27,52]withlimitedmotioncontent,whichrestrictstheirapplications
to motion diffusion models.
In this paper, we present a novel stylized motion diffusion model, dubbed
SMooDi, that customizes a pre-trained text-to-motion model for stylization.
Built upon the pre-trained motion latent diffusion model (MLD) [6], SMooDi
inherits MLD‚Äôs ability to generate diverse motion content. At the same time,
txeT
/
noitoM
tnetnoCSMooDi: Stylized Motion Diffusion Model 3
SMooDi can generate motions in a variety of styles according to different style
referenceconditions,asshowninFig.1.Ourmainnoveltyisthestylemodulation
module, which consists of a style adaptor and a style guidance module. First of
all,drawinginspirationfromcontrollableimagegeneration[61],thestyleadaptor
is designed to predict residual features conditioned on style reference motion se-
quencewithineachattentionlayerofMLD.Itisusefulforincorporatingthestyle
conditionwhileensuringtherealismofthegeneratedmotion.Second,wedesign
both classifier-free and classifier-based style guidance to more precisely control
the stylized motion generation. Specifically, the classifier-free style and content
guidance are linearly combined, where we can easily strike a balance between
preservingcontentandreflectingstylewithinthegeneratedmotion.Atthesame
time, wedesignan additionalclassifier-basedstyle guidance mechanism. Itis an
analytic function quantifying the disparity between the generated motion and
the style reference motion in a style-centric embedding space, whose gradients
are subsequently employed to guide the generated motion closer to the intended
style. Our style adaptor and guidance module are designed to be complemen-
tary,whichleadtohigh-qualitystylizedmotiongeneration.Thetwomodulesare
jointly optimized in a feature space instead of sequence-wise separate stitching,
thereby avoiding the error accumulation issue.
Although our approach is primarily designed for stylized motion generation
driven by content text, we can utilize DDIM-Inversion [43] to identify the noisy
latent corresponding to the content motion sequence. Following the same pro-
cedure as for text-driven content, SMooDi is capable of facilitating stylized mo-
tiongenerationbasedoncontentmotionsequences.Inotherwords,motionstyle
transfer is a downstream application of our approach should it be desired in
practice, e.g., to stylize the already created motion sequences.
Experiments on the HumanML3D [16] and 100STYLE [27] datasets demon-
stratethatSMooDisurpassesotherbaselinemodelsingeneratingstylizedmotion
drivenbycontenttext,excellinginbothcontentpreservationandstylereflection.
More importantly, unlike previous methods that require individual fine-tuning
for each style [12,30,54], SMooDi successfully integrates diverse content from
the HumanML3D dataset and various styles from the 100STYLE dataset into a
single model without requiring additional tuning during inference.
To summarize, our contributions are: (1) To our knowledge, SMooDi is the
first approach that adapts a pre-trained text-to-motion model to generate di-
verse stylized motion. (2) We introduce a novel style modulation module that
utilizes a stylized adaptor and a style classifier guidance to enable stylized mo-
tiongenerationwhileensuringstylereflection,contentpreservation,andrealism.
(3)ExperimentsdemonstratethatSMooDinotonlysetsanewstateoftheartin
stylizedmotiongenerationdrivenbycontenttextbutalsoachievesperformance
comparable to state-of-the-art methods in motion style transfer.4 Zhong et al.
2 Related Work
2.1 Human Motion Generation
Human motion generation has attracted great attention [4,5,7,9,14,16,20,32,
34‚Äì36,47,50,51,57,58,63]. Inspired by the impressive performance of diffusion
modelsinimagegeneration,alotofworks[6,8,13,18,22,23,25,29,32,33,39,41,46,
48,53,56,60,62,64]utilizediffusionmodelstogeneratehumanmotion.MDM[46]
facilitates high-quality generation and versatile conditioning, providing a solid
baseline for novel motion generation tasks. MLD [6] minimizes computational
overheadduringbothtrainingandinferencebyestablishingthediffusionprocess
withinthelatentspace.Drivenbytheefficacyofdiffusionmodelsforcontroland
conditioning, several studies have leveraged pre-trained motion diffusion models
to generate long-sequence motions [41], enable human-object interactions [29],
andcontrolthejointtrajectoryofgeneratedmotions[23,53].However,thereisno
work exploring how to leverage pre-trained motion diffusion models to generate
diversestylizedmotion.Whilesomestudies[2,3,37]haveenabledstylizedmotion
generation in their diffusion pipeline, their methods are trained from scratch,
and the supported styles are restricted by their motion content dataset. It is
challengingforthemtosimultaneouslysupportdiversemotioncontentandstyle.
In this work, we build upon a pre-trained motion diffusion model, MLD, and
explorehowtofine-tuneitonalargermotionstyledataset,100STYLE,tolearn
diverse motion styles while retaining the ability to support motion generation
across a wide range of content.
2.2 Motion Style Transfer
Recently,motionstyletransferhasseenqualityenhancementsthroughtheadop-
tion of various advanced neural architectures and generative models, such as
graph neural networks [28], time-series models [27,45], normalizing flows [49],
anddiffusionmodels[3,37].Specifically,Abermanetal.[1]designedatwo-branch
generative adversarial network to disentangle motion style from content and fa-
cilitate their re-composition. Their approach effectively breaks the constraint
of requiring a paired motion dataset. Motion Puzzle [19] realizes a framework
thatcancontrolthestyleofindividualbodyparts.Abovemethodsextractboth
content and style features from the motion sequence. Moreover, Guo et al. [15]
leverage the latent space of pre-trained motion models to enhance the extrac-
tion and infusion of motion content and style. However, a major limitation of
these models is their reliance on specialized style datasets [1,52] with limited
motion content, which restricts their applications. In this work, we customize
a pre-trained text-to-motion model for stylization, thus inheriting its ability to
generate diverse motion content.
3 Stylized Motion Diffusion Model
In this section, we introduce our proposed SMooDi for incorporating style con-
ditions from a style motion sequence into a content-oriented pre-trained motionSMooDi: Stylized Motion Diffusion Model 5
Style Motion ùê¨
(Sec. 3.1) (Sec. 3.2)
Stylized Motion
Style Adaptor Style Guidance
Repeat T Times
‚ÄòSomeone walks ùêú ùíô
backward.‚Äô ùùê ùíï ùíõ ùíï"ùüè ùíõ ùüé
ùíõ
ùíï
Decoder ùêÉ
NoisyLatent LatentDenoiser ùùêùúΩ
Fig.2: Overview of SMooDi. Our model generates stylized human motions from
contenttextandastylemotionsequence.Atthedenoisingstept,ourmodeltakesthe
content text c, style motion s, and noisy latent z as input and predicts œµ , which is
t t
then transferred to z . This denoising step is repeated T times to obtain the noise-
t‚àí1
free motion latent z , which is fed into a motion decoder D to produce the stylized
0
motion.
diffusionmodel(MLD[6]).Fig.2presentsanoverviewofSMooDi.Followingthe
settinginMLD[6],weplacethediffusionprocessinthemotionlatentspace.Let
œµ denote the latent denoiser (a UNet parameterized by Œ∏), and {z }T denote
Œ∏ t t=0
the sequence of noisy latents, where z is a Gaussian noise. Given a content
T
prompt c and a style prompt s, we define œµ =œµ (z ,t,c,s) for the denoising at
t Œ∏ t
step t (0 < t ‚â§ T). A cleaner noisy latent z can be obtained by subtract-
t‚àí1
ing œµ from z . The denoising step is repeated T iterations until a clean latent
t t
z is obtained. It can then be decoded by a motion decoder D into a realis-
0
tic motion sequence x ‚àà RN√óH that accurately reflects both the content and
style conditions. Here, N represents the length of the motion sequence, and H
is the dimension of human motion representations. We employ the same motion
representations as in HumanML3D [16], where H =263.
As shown in the Fig. 2, the content prompt is a text description, and the
style prompt is provided by a reference style motion sequence s ‚àà RN√óH. In
this section, we focus on using a text description as the content prompt c to
explain our proposed stylized motion diffusion model. By employing the DDIM-
Inversion [43] to identify the noisy latent corresponding to a motion content
sequence, we can effectively use motion sequences as content prompts to gen-
erate stylized motions. In other words, motion style transfer is a downstream
application of our proposed approach.
Ourproposedstylizationmoduleconsistsofastyleadaptorandastyleguid-
ance module. We will explain them separately in the rest of this section.
3.1 Style Adaptor
Although LoRA has been successfully used to incorporate ‚Äústyle‚Äù into the mod-
els in image domain [21,42], they typically require training a separate LoRA for
each style. In contrast, we focus on fine-tuning the model just once to adapt to6 Zhong et al.
Latent Diffusion Model‚ùÑ Diffuse T Times
CN onoi ts ey n tla Tt ee xn tt ùê≥ ùë°ùêúùê≠ PI rn op cu est s Tr Ean ns cf oo dr em rer + Tr Ean ns cf oo dr em rer + ‚Ä¶ Tr Ean ns cf oo dr em rer + Norm ùùêùíï P nore isd eicted
Timestep
Style Adaptorüî• Zero Linear Zero Linear Zero Linear
Style Motion ùê¨ EnS cty ol de er (trT ar E ia nn n as c bf oo ld er em cr oe pr y ) (tT rar E ia nn n as c bf o lo d er em cr oe pr y ) ‚Ä¶ (trT ar E ia nn n as c bf oo ld er em cr oe pr y )
Fig.3: Detailed illustration of our proposed style adaptor. The style adaptor
is connected to the motion diffusion model via zero linear layer. The output of the
style adaptor from each Transformer encoder is added to the motion diffusion model
to steer the predicted noise towards the target style.
various motion styles, where adapting ControlNet [61] is more suitable. There-
fore,wedesignacontent-awarestyleadaptorbasedonControlNet.Thisadaptor
incorporates the motion style condition into the pre-trained MLD [6].
Instead of learning to disentangle motion style from content from scratch on
alargemotiondataset,weredirectourfocustowardscapturingthemotionstyle
whileensuringthepreservationofdiversemotioncontentwithinthepre-trained
MLD framework. Specifically, it consists of a trainable copy of the Transformer
Encoder from the latent diffusion model in MLD. The architecture of the style
adaptor is illustrated in Fig. 3. An independent style encoder is utilized to ex-
tract the style embedding from the style motion sequence s. The style adaptor
takes the same content prompt c, the noised latent z and timestep t as in
t
MLD, and the extracted style embedding. Each Transformer layer in the origi-
nal latent diffusion model and the style adaptor is connected via a linear layer,
with both weight and bias initially set to zeros. As training progresses, the style
adaptor learns the style constraints and gradually applies the learned feature
corrections to the corresponding layers in the latent diffusion model, thereby
implicitly steering the output towards the desired style.
3.2 Style Guidance
Thestyleadaptoralonemaynotbesufficienttosuccessfullyincorporatethestyle
condition. We further leverage both the classifier-free and classifier guidance to
further enhance the stylization of a motion diffusion model. The combination of
two types of guidance effectively ensures that generated motion meets multiple
constraints while maintaining realism, complementing the style adaptor.
Classifier-free Style Guidance. With the introduction of an extra style con-
dition,wecandividetheconditionedclassifier-freeguidanceintotwoparts.
œµ (z ,t,c,s)=œµ (z ,t,‚àÖ,‚àÖ)+
Œ∏ t Œ∏ t
w (œµ (z ,t,c,‚àÖ)‚àíœµ (z ,t,‚àÖ,‚àÖ))+w (œµ (z ,t,c,s)‚àíœµ (z ,t,c,‚àÖ)), (1)
c Œ∏ t Œ∏ t s Œ∏ t Œ∏ t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Classifier-freeContentGuidance Classifier-freeStyleGuidanceSMooDi: Stylized Motion Diffusion Model 7
where w and w represent the strengths of the classifier-free guidance for the
c s
condition c and s, respectively. We slightly abuse the notations here by using ‚àÖ
todenoteaconditionisnotused.Theclassifier-freecontentguidanceisthesame
as in MLD, which can facilitate the text-to-motion generation process in combi-
nationwiththefirstterm‚Äôsunconditionedguidance.Ourproposedclassifier-free
style guidance works in a similar way. Note that œµ (z ,t,c,s) is MLD model
Œ∏ t
with the style adaptor incorporated introduced in the previous section, which
takesbothatextualpromptandstylemotionsequenceasinput.Bycontrasting
the text-driven denoising output with and without the style condition, it can
highlight the effectiveness of the style input s and facilitate the generation of
stylized motion driven by content text. Our insight here is that by dividing the
conditioned guidance into content and style components separately, we can eas-
ily strike a balance between preserving content and reflecting style within the
generated motion.
To better understand the classifier-free content and style guidance, we visu-
alize each of them through decoding denoised latent z into the motion space.
0
As illustrated in Fig.4(a), the content guidance ensures the motion generation
is faithful to the textual prompt, while the style guidance, as shown in Fig.4(b),
emphasizes style-related characteristics in the output. Combining both forms of
guidance results in a stylized motion that adheres to both content and style
conditions, as illustrated in Fig. 4(c).
Classifier-based Style Guidance.Tofurtherimprovethestylizationofamo-
tiondiffusionmodel,weadopttheclassifierguidance[10,59]toprovidestronger
guidance to the generated motion towards the desired style. The core of our
classifier-based style guidance is a novel analytic function G(z ,t,s), which cal-
t
culates the L distance between the style embedding of the generated clean
1
motion xÀÜ at denoising step t and the reference style motions s. The function‚Äôs
0
gradient is utilized to steer the generated motion towards the desired style.
œµ (z ,t,c,s)=œµ (z ,t,c,s)+œÑ‚àá G(z ,t,s),
Œ∏ t Œ∏ t zt t
G(z ,t,s)=|f(xÀÜ )‚àíf(s)|, (2)
t 0
where œÑ adjusts the strength of reference-style guidance and f denotes the style
feature extractor. The generated motion xÀÜ is obtained by first converting the
0
denoising output latent z into the predicted clean latent as shown below:
t
‚àö
z ‚àí 1‚àíŒ± Œµ (z ,t,c,s)
zÀÜ = t ‚àöt Œ∏ t , (3)
0 Œ±
t
where Œ± denotes the pre-defined noise scale in the forward process of the diffu-
t
sion model. The predicted clean latent zÀÜ is then input into the motion decoder
0
D to obtain the generated motion.
We obtain the style feature extractor by training a style classifier on the
100STYLE dataset [27] and removing its final layer. We refer the readers to
more details in the supplementary materials. The training of the style feature
extractor with ground-truth style labels for supervision enables it to effectively8 Zhong et al.
Text: A person walks forwardand then sits down.
( ùúî +ùúî ) ùúè‚àá!!ùê∫(ùëß";ùë°,ùë†)
! "
Classifier-based
Style Guidance
Style Motion (a)Classifier-Free (b) Classifier-Free (c) Initial Stylized Motion (d) Refined StylizedMotion
Content Guidance StyleGuidance
Fig.4:Visualillustrationsoftheclassifier-freeandclasifier-basedstyleguid-
ance. (a) and (b) respectively show the classifier-free content and style guidance; (c)
displays the initial stylized motion resulting from the combination of (a) and (b); (d)
illustrates the refined stylized motion modified by the classifier-based style guidance.
capture style-related features. Therefore, style classifier guidance can provide
more guidance to the stylized motion generation.
Combination of the Two Style Guidance. The classifier-free and classifier-
based style guidance are designed to complement each other, each playing a
vital role in accurately reflecting the target style in the generated motions. As
illustrated in Fig. 4, the desired style motion is ‚Äúarms open wide to the sides
like an airplane‚Äù. The classifier-free style guidance (Fig. 4(b)) can capture style-
relatedcharacteristicinareasonablyaccuratemanner.Whencombinedwiththe
classifier-free content guidance, it depicts the desired style (Fig.4(c)). Refined
further by the classifier-based style guidance, the stylization is more authentic,
where the person‚Äôs harms are more open (Fig.4(d)). In addition to such visual
results,quantitativeablationstudiesalsoverifytheeffectivenessofourproposed
both classifer-free and classifer-based style guidance.
Atthesametime,althoughclassifier-basedstyleguidanceoffersprecisestyle
control,itseffectivenessmaybecompromisedwhenthecontenttextsignificantly
diverges from locomotion-related movements. This is because the style feature
function is trained solely on the 100STYLE dataset, which contains only such
movements. Over-reliance on style classifier guidance risks producing motions
that fail to execute the desired actions, leading to unrealistic and physically
implausible movements. Therefore, we leverage a content-aware style adaptor
that establishes the fundamental style direction, while style classifier guidance
refines this base for a more precise outcome. The effectiveness of this design is
verfied in our ablation studies.
3.3 Learning Scheme
Following [61], a straightforward approach to train SMooDi is to freeze the pa-
rameters of MLD and solely train the style adaptor on the 100STYLE dataset
using the following loss function:
(cid:104) (cid:105)
L =E ‚à•œµ (z ,t,c,s)‚àíœµ‚à•2 , (4)
std œµ,z Œ∏ t 2
where œµ ‚àº N(0,I) represents the ground-truth noise added to z . In our ex-
0
periments, however, we found that this loss function alone leads to an issue ofSMooDi: Stylized Motion Diffusion Model 9
‚Äúcontent-forgetting‚Äù, where the model progressively looses the MLD‚Äôs ability to
generate motions with diverse contents. To address this issue, we design a con-
tent prior preservation loss L . Specifically, we randomly sample motions from
pr
the HumanML3D dataset to compute a prior preservation loss when fine-tuning
SMooDi on the 100STYLE dataset.
(cid:104) (cid:105)
L =E ‚à•œµ (z‚Ä≤,t,c‚Ä≤,s‚Ä≤)‚àíœµ‚Ä≤‚à•2 , (5)
pr œµ‚Ä≤,z‚Ä≤ Œ∏ t 2
wherez‚Ä≤,c‚Ä≤ands‚Ä≤representsthemotionlatent,contentpromptandstylemotion
t
sequence derived from the HumanML3D dataset. œµ‚Ä≤ is the noise map added to
z‚Ä≤. A similar solution is used in DreamBooth [40] to solve the ‚Äúlanguage drift‚Äù
0
problem, where images generated from the frozen pretrained image generation
model are utilized to enforce a class-prior preservation loss during model fine-
tuning. Our content preservation loss can effectively mitigate content forgetting
while learning diverse motion styles from the 100STYLE dataset.
To further encourage the style adaptor to focus on motion style, while also
ensuringthatthelatentdiffusionmodelinMLDhandlesmotioncontentwell,we
introduce an additional cycle prior-preservation loss, inspired by [19,55]. Specif-
ically,westartthisprocessbyrandomlysamplingcontenttextandmotionstyle
sequencesfromboththe100STYLEandHumanML3Ddatasetssimultaneously.
Then, we intermix the content text and motion style from these sequences with
each other. Finally, we repeat this process to reconstruct the original motion
sequences. The formula is expressed as follows:
L
cyc
=E z,z‚Ä≤,œµ,œµ‚Ä≤(cid:104)(cid:13) (cid:13)œµ Œ∏(z tsh,t,c,shs)+œµ Œ∏(z ths,t,c‚Ä≤,ssh)‚àíœµ‚àíœµ‚Ä≤(cid:13) (cid:13)2 2(cid:105) , (6)
where shs denotes the motion sequence created by merging content from the
HumanML3D dataset with style from the 100STYLE dataset. Similarly, ssh
represents the sequence where content is sourced from the 100STYLE dataset
and style from the HumanML3D dataset. The noised latent codes zsh and zhs
t t
correspond to ssh and shs, respectively. Essentially, the cycle prior-preservation
loss exchanges diverse content and style between two datasets, encouraging the
content text to remain invariant in the generated motion under forward and
backward translation. Overall, the training loss function of our framework is
defined as follows:
L =L +Œª L +Œª L (7)
all std pr pr cyc cyc
where Œª and Œª are hyperparameters. We refer readers to the pseudocode
pr cyc
and illustration for training in the supplementary material for more details.
4 Experiments
Weconductexperimentsonbothstylizedtext2motionandmotionstyletransfer
to demonstrate the effectiveness of our framework. Both tasks use a motion se-
quenceasastyleprompt,withtheprimarydifferencebeingtheircontentprompt
input: the former utilizes text, while the latter relies on motion sequences.10 Zhong et al.
Table 1: Comparison with baseline methods on stylized motion generation driven by
contenttext,usingacombinationofthe100STYLE(providingstyle)andHumanML3D
datasets (providing content).
Method FID‚ÜìFootskatingMMDist‚ÜìR-precision‚ÜëDiversity‚Üí SRA‚Üë
ratio‚Üì (Top-3)
Ours 1.609 0.124 4.477 0.571 9.235 72.418
MLD+MotionPuzzle[19] 6.127 0.185 6.467 0.290 6.4762 63.769
MLD+Abermanetal.[1] 3.309 0.347 5.983 0.406 8.816 54.367
ChatGPT+MLD 0.614 0.131 4.313 0.605 8.836 4.819
‚ÄòA person kicks.‚Äô
Style Motion Content Text (a) MLD+Motion Puzzle (b) MLD+Abermanetal. (c) Ours
Style Motion Content Motion (d) Motion Puzzle (e) Aberman et al. (f) Ours
Fig.5: Qualitativecomparisonsofourapproachandbaselinemethodsontwostylized
motion generation task.
Datasets. We utilize the HumanML3D dataset [16] as our motion content
dataset and the 100STYLE dataset [27] as our motion style dataset. The Hu-
manML3D dataset is the largest motion capture dataset, featuring text anno-
tations and comprising 14,646 motions and 44,970 motion annotations. Follow-
ing the processing approach outlined in [16], we preprocess the HumanML3D
dataset to obtain consistent motion representations. On the other hand, the
100STYLE dataset [27], being the largest motion style dataset, comprises up to
1,125 minutes of motion sequences, showcasing a wide array of 100 diverse lo-
comotion styles. Due to differences in skeletons between the 100STYLE dataset
and HumanML3D, we retarget the motions from 100STYLE to match the Hu-
manML3D(SMPL-H)skeleton.Followingthisalignment,weapplythesamepro-
cessingstepsasusedfortheHumanML3Ddatasettopreprocessthe100STYLE
dataset.Moreover,asthe100STYLEdatasetlackstextdescriptions,weleverage
MotionGPT [20] to generate pseudo text descriptions for the motion sequences
in the 100STYLE dataset.
Evaluationmetricsaredesignedtoassessthreedimensions:ContentPreserva-
tion,StyleReflection,andRealism.Forcontentpreservationandstylereflection
assessment,weemploymetricsconsistentwiththoseusedin[6]:motion-retrieval
precision(Rprecision),Multi-modalDistance(MMDist),Diversity,andFrechet
InceptionDistance(FID).Additionally,recognizingthecommonfootskatingis-
sues in kinetics-based motion generation methods, we incorporate the foot skat-
ing ratio metric proposed by [23] into our motion quality evaluation. For style
noitoM2txeT
dezilytS
refsnarTelytS
noitoMSMooDi: Stylized Motion Diffusion Model 11
reflection,weemployStyleRecognitionAccuracy(SRA)[19].Duringevaluation,
we randomly select a content text from the HumanML3D dataset and a motion
style sequence from the 100STYLE dataset to generate the stylized motion. We
thenuseapre-trainedstyleclassifiertocomputetheSRAforthegeneratedmo-
tion. It‚Äôs noteworthy that some motion style labels in the 100STYLE dataset,
like ‚Äôkick‚Äô and ‚Äôjump,‚Äô inherently convey motion content, which may conflict
with the content text in HumanML3D dataset. To address this, we categorize
the motion style labels into server groups following the approach by Kim et
al. [24], Specifically excluding the ‚ÄôACT‚Äô group ensures that only motion style
labels not conflicting with motion content are considered when computing the
SRA metric. Further details about the evaluation metrics are provided in the
supplementary material.
Baselines. For motion style transfer task, we compare our methods with two
state-of-the-art methods, namely Motion Puzzle [19] and Aberman et al. [1]. To
ensureafaircomparison,wetrainthecomparedmethodsunderthesamesettings
asours,usingacombineddatasetcomprisingHumanML3Dand100STYLE.Due
to the constraints of the multi-class discriminator in Aberman et al., which re-
quires style labels, we adopt the training method outlined in Motion Puzzle
to eliminate the need for style labels. For stylized text2motion task, we com-
pare our method against baselines capable of generating stylized motion from
content text and style motion sequences. The straightforward baselines involve
applyingmotionstyletransfermethodstothemotionsequencesgeneratedbythe
text2motion model. To align with our approach that uses a pre-trained motion
diffusionmodel,thetext2motionmodelsinthebaselinesforstylizedmotiongen-
eration select MLD, and the motion style transfer methods are consistent with
those used in the motion style transfer task. For the stylized text2motion task,
we compare our method against two kinds of baselines capable of generating
stylized motion from content text and style motion sequences. The first kind of
baselineinvolvesapplyingmotionstyletransfermethodstothemotionsequences
generatedbythetext2motionmodel.Toalignwithourapproachthatusesapre-
trainedmotiondiffusionmodel,thetext2motionmodelsselectMLD[6],andthe
motionstyletransfermethodsareconsistentwiththoseusedinthemotionstyle
transfer task. The second kind of baseline involves using ChatGPT to merge
style labels from 100STYLE with text from HumanML3D into a sentence. For
example, given the content text ‚Äôa person walks.‚Äô and the style label ‚Äôold,‚Äô we
obtain ‚Äôan elderly person walks.‚Äô This merged sentence is then fed to MLD.
4.1 Comparison to Baseline Methods
Quantitative and Qualitative For the task of stylized text2motion, Table 1
reports the comparisons of our method with the three baseline methods.
As shown in the 3rd row of Table 1, ChatGPT+MLD only achieves around
5.29% in terms of SRA, indicating that MLD cannot enable stylized generation
fromtextalone,eventhoughitcontainsstyledescriptions.Notably,ourmethod
outperforms the two baselines that combine MLD with motion style transfer
methods in all metrics.12 Zhong et al.
Table 2: Comparison with baseline methods on motion style transfer.
(a) EvaluationonHumanML3Ddataset (b) EvaluationonXiadataset
Method FootskatingFID‚ÜìSRA‚Üë(%) Method Footskating FID‚Üì SRA‚Üë(%)CRA‚Üë(%)
ratio‚Üì ratio‚Üì
Ours 0.095 1.582 65.147 Ours 0.0317 4.663 61.111 45.555
MotionPuzzle[19] 0.197 6.871 67.233 MotionPuzzle[19] 0.0316 5.360 67.778 25.556
(Abermanetal[1]) 0.338 3.892 61.006 (Abermanetal[1]) 0.0260 5.681 56.667 34.444
Specifically, our method performs better than MLD+Motion Puzzle in the
SRA metric by 13.56% and significantly outperforms MLD+Aberman et al. in
the FID metric by 51.38% and 0.64% in the R-precision metric. The first row
of Fig. 5 validates our observation, where the motion generated by our method
performs better in adhering to both content and style constraints than baseline
methods. In contrast, MLP+Aberman et al. can successfully perform the action
butfailtoreflectthemotionstyleinFig.5(b),whileMotionPuzzlecanaccurately
reflectthemotionstylebutstrugglestoeffectivelyperformtheactioninFig.5(a).
For the task of motion style transfer, since it does not take content text as
input, text-motion related metrics such as MM Dist, R-precision, and Diver-
sity are not applicable and thus are not reported. Part (a) of Table 2 presents
a comparison between our method and the two baseline methods, using the
HumanML3D dataset as the motion content source and drawing motion styles
from the 100STYLE dataset. Our method delivers competitive results in the
SRA metric and excels in the FID and foot skating ratio metrics. Specifically,
we see a substantial 59.35% improvement in the FID metric over Aberman et
al. [1]. To more effectively compare the generalizability of different methods,
we conduct experiments on the Xia dataset [52], a small, specific motion style
dataset that was unseen by our and the baseline models during training. Be-
cause motion content labels are present in the Xia dataset, we report the Con-
tent Recognition Accuracy (CRA). Part (b) of Table 2 showcases the results.
Our method maintains competitive performance in the SRA metric, with only a
marginal 9.83% decrease in SRA compared to Motion Puzzle. On the contrary,
our method exhibits a significant 32.26% increase in the CAR metric relative to
Aberman et al., and a notable 78.26% enhancement over Motion Puzzle. Our
method achieves a better balance between style reflection and content preserva-
tion.ThesecondrowofFig.5validatesthisobservation.Itisworthnotingthat,
unlikeothermotionstyletransfermethods,ourmethoddoesnotincorporateob-
jectives forenablingstylizedmotion generationusingmotion content sequences.
Through simple DDIM-Inversion and without any additional optimization or
regularization, our method achieves performance comparable to existing motion
style transfer methods.
User Study. Due to the highly subjective nature of stylized motion, we con-
duct User studies using pairwise comparisons to further evaluate our proposed
methodinthetasksofstylizedmotiongenerationandmotionstyletransfer.WeSMooDi: Stylized Motion Diffusion Model 13
Table 3: Ablation Studies on HumanML3D Content and 100STYLE Styles.
Method FID‚ÜìFootskatingMMDist‚Üì R-precision‚Üë Diversity‚ÜíSRA(%)‚Üë
ratio‚Üì (Top-3)
Ours(onall) 1.609 0.124 4.477 0.571 9.235 72.418
w/oLcyc 2.046 0.136 4.465 0.569 8.869 64.866
w/oLpr+Lcyc 5.996 0.166 6.098 0.335 7.456 81.841
w/oclassifier-based1.050 0.111 4.085 0.630 9.445 20.245
w/oadaptor 2.984 0.123 4.526 0.550 8.372 69.952
Text: A person walks forward and then sits down.
Style Motion (a) W/o ùë≥ùíëùíì+ùë≥ùíÑùíöùíÑ (b) W/o ùë≥ùíÑùíöùíÑ (c) W/o adaptor (d) W/o classifier-based (e) Full model
Fig.6: Visual comparisons of the ablation designs and our full model.
recruited22humansubjectstoparticipateinthestudy.Ineachtest,participants
are presented with two 4-second video clips synthesized by our method and one
comparison method. They are then required to select their preferred clip while
considering Realism, Style Reflection, and Content Preservation dimensions, re-
spectively.AsshowninFig.7,ourmethodreceivesmoreuserappreciationcom-
pared to two baselines across three dimensions in two tasks. Further user study
details are provided in supplementary.
4.2 Ablation studies
To validate the effectiveness of our framework‚Äôs design choices, we have con-
ducted several ablation studies: the first assesses the impact of each loss func-
tionterm,whilethesecondevaluatestheinfluenceofthestyleadaptorandstyle
guidance during sampling.
Loss Components.Firstly, we exclude the cycle-prior term in the loss func-
tion, denoted as w/o L . Comparing the results in the 1st and 2nd rows in
cyc
Table 3, we observe that our full model outperforms in all content preserva-
tion and style reflection metrics. The motion generated by our approach can
still perform the content adhering to the text description but performs worse in
accurately reflecting the motion style, as reflected by the arms not being fully
extended horizontally.
Since the cycle prior-preservation term is built upon the prior-preservation
term, it is meaningless to exclude L while retaining L . Therefore, we fur-
pr cyc
ther exclude both the prior-preservation and cycle-prior term, denoted as
w/oL + L in the 3rd row. By comparing the results in the 2nd and 3rd
pr cyc
rows of Table 3, we notice that while the number of SRAs is higher in the third
row, other metrics show a significant decline. Specifically, in terms of the FID
metric, performance deteriorates by more than 229%. Indeed, without L , the
pr14 Zhong et al.
Stylized Text2Motion Motion Style Transfer
789 000 ... 000 000 %%% 72.00%70.50%78.00% 64.40%73.50%77.30% 789 000 ... 000 000 %%% 68.20%74.20%83.30% 69.70%81.80%80.30%
60.00% 60.00%
50.00% 50.00%
40.00% 40.00%
30.00% 30.00%
20.00% 20.00%
10.00% 10.00%
0.00% 0.00%
Our vs MLD+Motion Puzzle Our vs MLD+Aberman et al Our vs Motion Puzzle Our vs Aberman et al
Style Reflection Content Preservation Realism Style Reflection Content Preservation Realism
Fig.7: User Study on two stylized motion generation tasks.
model tends to lose the ability to translate content text into corresponding mo-
tion,aphenomenonnamed‚Äôcontent-forgetting‚ÄôasdescribedinSec.3.3.Fig.6(a)
validatesourobservation,showingthatthecontentinthegeneratedmotionsig-
nificantly deviates from the text descriptions and closely resembles the style
motion sequence.
Style Adaptor and Style Guidance. Initially, we compare our model to a
variant without the classifier-based guidance, w/o classifier-based, to demon-
strate its effectiveness. The 5th row of Table 3 presents the results. Consistent
withthefindingsin[19],areasonabletrade-offbetweencontentpreservationand
stylereflectionisobserved.Althoughclassifier-basedstyleguidancemayslightly
affectthecontentpreservationmetrics,itsignificantlybooststhemodel‚Äôsperfor-
mance in the SRA metric, yielding an impressive 208% improvement.Fig. 6(d)
demonstrates that, without classifier-based style guidance, the generated mo-
tions can reflect the motion style, yet they still fall short of fully achieving the
target style. Classifier-based style guidance can effectively bridge this gap.
Subsequently, as shown in Table 3, we evaluate a variant without the style
adaptor, denoted as w/o adaptor (the last row). In cases where the SRA values
areclose,thestyleadaptorimprovedtheFIDmetricbyabout80.46%.Fig.6(c)
shows that the generated motion can greatly perform the ‚Äôwalk‚Äô action while
successfully reflecting the style, but fails to perform the ‚Äôsit‚Äô action. This indi-
catesthattheeffectivenessofclassifier-basedstyleguidancediminisheswhenthe
content text deviates from locomotion-related movements. Relying solely on it
may even adversely affect action performance.
5 Conclusion
In this work, we introduce the Stylized Motion Diffusion Model, a novel ap-
proach that leverages a pre-trained motion diffusion model to facilitate stylized
motion generation driven by content text. By integrating a style adaptor and
styleclassifierguidance,ourmethodiscapableofproducingrealistichumanmo-
tions that accurately reflect both the content text descriptions and the desired
motionstylefrommotionsequences.Throughdetailedablationstudies,wehave
demonstrated the effectiveness of each component in our framework.SMooDi: Stylized Motion Diffusion Model 15
References
1. Aberman,K.,Weng,Y.,Lischinski,D.,Cohen-Or,D.,Chen,B.:Unpairedmotion
style transfer from video to animation. TOG (2020)
2. Alexanderson,S.,Nagy,R.,Beskow,J.,Henter,G.E.:Listen,denoise,action!audio-
driven motion synthesis with diffusion models. TOG (2023)
3. Ao, T., Zhang, Z., Liu, L.: Gesturediffuclip: Gesture diffusion model with clip
latents. TOG (2023)
4. Cen,Z.,Pi,H.,Peng,S.,Shen,Z.,Yang,M.,Zhu,S.,Bao,H.,Zhou,X.:Generating
human motion in 3d scenes from text descriptions. In: CVPR (2024)
5. Chen, L.H., Lu, S., Zeng, A., Zhang, H., Wang, B., Zhang, R., Zhang, L.: Mo-
tionllm: Understanding human behaviors from human motions and videos. ArXiv
(2024)
6. Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,G.:Executingyour
commands via motion diffusion in latent space. In: CVPR (2023)
7. Cohan, S., Tevet, G., Reda, D., Peng, X.B., van de Panne, M.: Flexible motion
in-betweening with diffusion models. ArXiv (2024)
8. Dabral,R.,Mughal,M.H.,Golyanik,V.,Theobalt,C.:Mofusion:Aframeworkfor
denoising-diffusion-based motion synthesis. In: CVPR (2023)
9. Dai, W., Chen, L.H., Wang, J., Liu, J., Dai, B., Tang, Y.: Motionlcm: Real-time
controllable motion generation via latent consistency model. ArXiv (2024)
10. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS
(2021)
11. Du,H.,Herrmann,E.,Sprenger,J.,Fischer,K.,Slusallek,P.:Stylisticlocomotion
modelingandsynthesisusingvariationalgenerativemodels.In:Proceedingsofthe
12th ACM SIGGRAPH Conference on Motion, Interaction and Games (2019)
12. Everaert, M.N., Bocchio, M., Arpa, S., S√ºsstrunk, S., Achanta, R.: Diffusion in
style. In: ICCV (2023)
13. Ghosh, A., Dabral, R., Golyanik, V., Theobalt, C., Slusallek, P.: Remos: Reactive
3d motion synthesis for two-person interactions. In: ArXiv (2023)
14. Guo, C., Mu, Y., Javed, M.G., Wang, S., Cheng, L.: Momask: Generative masked
modeling of 3d human motions. In: CVPR (2024)
15. Guo, C., Mu, Y., Zuo, X., Dai, P., Yan, Y., Lu, J., Cheng, L.: Generative human
motion stylization in latent space. ArXiv (2024)
16. Guo, C., Zou, S., Zuo, X., Wang, S., Ji, W., Li, X., Cheng, L.: Generating diverse
and natural 3d human motions from text. In: CVPR (2022)
17. Ho, J., Salimans, T.: Classifier-free diffusion guidance. ArXiv (2022)
18. Huang, S., Wang, Z., Li, P., Jia, B., Liu, T., Zhu, Y., Liang, W., Zhu, S.C.:
Diffusion-based generation, optimization, and planning in 3d scenes. In: CVPR
(2023)
19. Jang,D.K.,Park,S.,Lee,S.H.:Motionpuzzle:Arbitrarymotionstyletransferby
body part. TOG (2022)
20. Jiang,B.,Chen,X.,Liu,W.,Yu,J.,Yu,G.,Chen,T.:Motiongpt:Humanmotion
as a foreign language. ArXiv (2023)
21. Jones,M.,Wang,S.Y.,Kumari,N.,Bau,D.,Zhu,J.Y.:Customizingtext-to-image
models with a single image pair. ArXiv (2024)
22. Karunratanakul, K., Preechakul, K., Aksan, E., Beeler, T., Suwajanakorn, S.,
Tang,S.:Optimizingdiffusionnoisecanserveasuniversalmotionpriors.In:Arxiv
(2023)16 Zhong et al.
23. Karunratanakul, K., Preechakul, K., Suwajanakorn, S., Tang, S.: Gmd: Control-
lable human motion synthesis via guided diffusion models. In: ICCV (2023)
24. Kim, H.J., Lee, S.H.: Perceptual characteristics by motion style category. In: Eu-
rographics (Short Papers) (2019)
25. Kulkarni,N.,Rempe,D.,Genova,K.,Kundu,A.,Johnson,J.,Fouhey,D.,Guibas,
L.:Nifty:Neuralobjectinteractionfieldsforguidedhumanmotionsynthesis.ArXiv
(2023)
26. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. ArXiv (2017)
27. Mason,I.,Starke,S.,Komura,T.:Real-timestylemodellingofhumanlocomotion
viafeature-wisetransformationsandlocalmotionphases.ProceedingsoftheACM
on Computer Graphics and Interactive Techniques (2022)
28. Park, S., Jang, D.K., Lee, S.H.: Diverse motion stylization for multiple style do-
mainsviaspatial-temporalgraph-basedgenerativemodel.ProceedingsoftheACM
on Computer Graphics and Interactive Techniques (2021)
29. Peng, X., Xie, Y., Wu, Z., Jampani, V., Sun, D., Jiang, H.: Hoi-diff: Text-driven
synthesis of 3d human-object interactions using diffusion models. ArXiv (2023)
30. Peng,X.B.,Ma,Z.,Abbeel,P.,Levine,S.,Kanazawa,A.:Amp:Adversarialmotion
priors for stylized physics-based character control. TOG (2021)
31. Petrovich, M., Black, M.J., Varol, G.: Temos: Generating diverse human motions
from textual descriptions. In: ECCV (2022)
32. Petrovich,M.,Litany,O.,Iqbal,U.,Black,M.J.,Varol,G.,BinPeng,X.,Rempe,
D.: Multi-track timeline control for text-driven 3d human motion generation. In:
CVPR (2024)
33. Pi, H., Peng, S., Yang, M., Zhou, X., Bao, H.: Hierarchical generation of human-
object interactions with diffusion probabilistic models. In: ICCV (2023)
34. Pinyoanuntapong,E.,Saleem,M.U.,Wang,P.,Lee,M.,Das,S.,Chen,C.:Bamm:
Bidirectional autoregressive motion model. ArXiv (2024)
35. Pinyoanuntapong, E., Wang, P., Lee, M., Chen, C.: Mmm: Generative masked
motion model. In: CVPR (2024)
36. Raab, S., Gat, I., Sala, N., Tevet, G., Shalev-Arkushin, R., Fried, O., Bermano,
A.H., Cohen-Or, D.: Monkey see, monkey do: Harnessing self-attention in motion
diffusion for zero-shot motion transfer. ArXiv (2024)
37. Raab,S.,Leibovitch,I.,Tevet,G.,Arar,M.,Bermano,A.H.,Cohen-Or,D.:Single
motion diffusion. ArXiv (2023)
38. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
39. Rempe,D.,Luo,Z.,Peng,X.B.,Yuan,Y.,Kitani,K.,Kreis,K.,Fidler,S.,Litany,
O.:Traceandpace:Controllablepedestriananimationviaguidedtrajectorydiffu-
sion. In: CVPR (2023)
40. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
In: CVPR (2023)
41. Shafir, Y., Tevet, G., Kapon, R., Bermano, A.H.: Human motion diffusion as a
generative prior. ArXiv (2023)
42. Shah, V., Ruiz, N., Cole, F., Lu, E., Lazebnik, S., Li, Y., Jampani, V.: Ziplora:
Any subject in any style by effectively merging loras. ArXiv (2023)
43. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. ArXiv (2020)
44. Song,Y.,Dhariwal,P.,Chen,M.,Sutskever,I.:Consistencymodels.ArXiv(2023)
45. Tao,T.,Zhan,X.,Chen,Z.,vandePanne,M.:Style-erd:Responsiveandcoherent
online motion style transfer. Arxiv (2022)SMooDi: Stylized Motion Diffusion Model 17
46. Tevet,G.,Raab,S.,Gordon,B.,Shafir,Y.,Cohen-or,D.,Bermano,A.H.:Human
motion diffusion model. In: ICLR (2023)
47. Wan, W., Dou, Z., Komura, T., Wang, W., Jayaraman, D., Liu, L.: Tlcontrol:
Trajectory and language control for human motion synthesis. ArXiv (2023)
48. Wang, Z., Wang, J., Lin, D., Dai, B.: Intercontrol: Generate human motion inter-
actions by controlling every joint. ArXiv (2023)
49. Wen, Y.H., Yang, Z., Fu, H., Gao, L., Sun, Y., Liu, Y.J.: Autoregressive stylized
motion synthesis with generative flow. In: CVPR (2021)
50. Wu, Q., Zhao, Y., Wang, Y., Tai, Y.W., Tang, C.K.: Motionllm: Multimodal
motion-language learning with large language models. ArXiv (2024)
51. Wu,Q.,Shi,Y.,Huang,X.,Yu,J.,Xu,L.,Wang,J.:Thor:Texttohuman-object
interaction diffusion via relation intervention. ArXiv (2024)
52. Xia, S., Wang, C., Chai, J., Hodgins, J.: Realtime style transfer for unlabeled
heterogeneous human motion. TOG (2015)
53. Xie,Y.,Jampani,V.,Zhong,L.,Sun,D.,Jiang,H.:Omnicontrol:Controlanyjoint
at any time for human motion generation. In: ICLR (2024)
54. Xu, P., Xie, K., Andrews, S., Kry, P.G., Neff, M., McGuire, M., Karamouzas, I.,
Zordan,V.:Adaptnet:Policyadaptationforphysics-basedcharactercontrol.TOG
(2023)
55. Xu,S.,Ma,Z.,Huang,Y.,Lee,H.,Chai,J.:Cyclenet:Rethinkingcycleconsistency
in text-guided diffusion for image manipulation. NeurIPS (2024)
56. Xu, S., Li, Z., Wang, Y.X., Gui, L.Y.: Interdiff: Generating 3d human-object in-
teractions with physics-informed diffusion. In: ICCV (2023)
57. Xu, S., Wang, Z., Wang, Y.X., Gui, L.Y.: Interdreamer: Zero-shot text to 3d dy-
namic human-object interaction. ArXiv (2024)
58. Yi, H., Thies,J., Black, M.J., Peng, X.B.,Rempe,D.: Generatinghuman interac-
tion motions in scenes with text control. ArXiv (2024)
59. Yu,J.,Wang,Y.,Zhao,C.,Ghanem,B.,Zhang,J.:Freedom:Training-freeenergy-
guided conditional diffusion model. ArXiv (2023)
60. Yuan, Y., Song, J., Iqbal, U., Vahdat, A., Kautz, J.: Physdiff: Physics-guided hu-
man motion diffusion model. In: ICCV (2023)
61. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: ICCV (2023)
62. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondiffuse:
Text-driven human motion generation with diffusion model. PAMI (2024)
63. Zhang, Y., Huang, D., Liu, B., Tang, S., Lu, Y., Chen, L., Bai, L., Chu, Q., Yu,
N.,Ouyang,W.:Motiongpt:Finetunedllmsaregeneral-purposemotiongenerators.
ArXiv (2023)
64. Zhou, W., Dou, Z., Cao, Z., Liao, Z., Wang, J., Wang, W., Liu, Y., Komura, T.,
Wang, W., Liu, L.: Emdm: Efficient motion diffusion model for fast, high-quality
motion generation. Arxiv (2023)
65. Zhou,Y.,Barnes,C.,Lu,J.,Yang,J.,Li,H.:Onthecontinuityofrotationrepre-
sentations in neural networks. In: CVPR (2019)18 Zhong et al.
A Appendix
Video. We provide a supplemental video, which we encourage the reviewer to
watchsincemotioniscriticalinourresults,andthisishardtoconveyinastatic
document.
CodeandModel.Thecode,trainedmodel,andre-targeted100STYLEdatasets
will be made publicly available upon acceptance.
A.1 Pseudo Code
Algorithm 1 SMooDi‚Äôs inference
Require: AmotiondiffusionmodelM withparametersŒ∏ ,astyleadaptormodelA
M
with parameters Œ∏ , style motion sequence s (if any), content texts c (if any).
A
1: z ‚àºN(0,I) # Sample from pure Gaussian distribution
T
2: for all t from T to 1 do
3: {r}‚ÜêA(z ,t,c,s;Œ∏ ) # Style Adaptor model
t A
4: œµ ‚ÜêM(x ,t,c,{r};Œ∏ ) # Model diffusion model
t t M
5: for all k from 1 to K do # Classifier-based style guidance
6: œµ =œµ +œÑ‚àá G(z ,t,s)
t t zt t
7: end for
8: z ‚àºS(z ,œµ ,t) # S(¬∑,¬∑,¬∑) represents the DDIM sampling method [10].
t‚àí1 t t
9: end for
10: x = D(z )
0 0
11: return x
0
A.2 Motion Style Transfer
This task involves taking a content motion sequence along with a style motion
sequenceandthengeneratingastylizedmotionsequence.Wetreatmotionstyle
transfer as one of our downstream applications and can enable SMooDi to sup-
port it without additional training. Firstly, we adopt the deterministic DDIM
reverseprocess[43]toobtainthenoisedlatentcodezInv forthecontentmotion
T
sequence. The reverse process can be represented at step t as:
(cid:114) (cid:32) (cid:32)(cid:115) (cid:33) (cid:18)(cid:114) (cid:19)(cid:33)
Œ± 1 1
z = t+1 z + ‚àí1 ‚àí ‚àí1 ¬∑Œµ (z ;t,c,‚àÖ), (8)
t+1 Œ± t Œ± Œ± Œ∏ t
t t+1 t
where Œ± represents the noise scale. zInv can be obtained at the last reverse
T
step T. We substitute z , which is initially from a pure Gaussian distribution,
T
with the DDIM-reversed latent zInv in Alg. 1 and adhere to the same infer-
T
enceproceduretointegratethestyleconditionintothemotioncontentsequence
throughout the denoising steps. Because there are fewer denoising steps com-
pared to the stylized text2motion process, we slightly increase the weights of
each style guidance. Specifically, the number of denoising steps is 30, w = 6.5
s
and œÑ =‚àí0.4SMooDi: Stylized Motion Diffusion Model 19
A.3 Implementation details
Training details. Our framework is implemented in PyTorch and trained on
a single NVIDIA A5000 GPU. We use a batch size of 64, train for 50 epochs,
and use the AdamW optimizer [26] with a learning rate of 1e-5. Training takes
about 1 hour on a single A5000 GPU, totaling 3700 iterations. During train-
ing, we optimize the style adaptor while keeping the parameters of MLD frozen.
Furthermore, to learn both the unconditioned and conditioned models simul-
taneously during training, we randomly set the content text c = ‚àÖ and mask
out the style motion sequence s in the time dimension by 10%. The number of
diffusion steps is 1K during training while 50 during interfering. The weight of
classifier-free content guidance w is set to 7.5, classifier-free style guidance w
c s
is set to 1.5, and classifier-based style guidance œÑ is set to ‚àí0.2.
Model details. We select MLD [6] as our pre-trained motion diffusion model
anduseitspre-trainedweightstoinitializebothMLDandourstyleadaptor.The
style adaptor is composed of 4 Transformer Encoder blocks. The input process,
as shown in Fig. 3, primarily involves a CLIP model [38] to encode the content
textcintotextembeddings,andlinearlayerstoprojectthetimesteptintotime
embeddings.Thesetextembeddingsarethenaddedtothetimeembeddingsand
concatenatedwiththenoisylatentz ,servingasinputtothesubsequentTrans-
t
formerEncoderinthelatentdiffusionmodel.Thestyleencoder,asillustratedin
Fig.3,primarilyconsistsofasingleTransformerEncoderdesignedtoencodethe
stylemotionsequencessintostyleembeddings.Thesestyleembeddingsarethen
addedtotheconcatenatedembeddingsfromtheinputprocessandsubsequently
fed into the next Transformer Encoder within the style adaptor.
Style Function details.Weopttofirsttrainastyleclassifier,whichconsistsof
a one-layer Transformer block, on the 100STYLE dataset for 100 epochs, using
ground-truthstylelabelsforsupervision.Then,weomitthelastfullyconnected
layer to serve as our style function.
Baseline details. Due to the baselines being trained on a small style motion
dataset and using different skeletons, their released pre-trained weights cannot
be directly utilized. We leverage the source code from Motion Puzzle [19] and
Aberman et al. [1] to implement their methods on the combined dataset, Hu-
manML3D + 100STYLE. For a fair comparison, we replace their 4D rotation
withour6-Drotation-basedfeature[65].Giventherequirementforstyle-labeled
motiondatainAbermanetal.[1],wefollowthesameprocessfromMotionPuz-
zle [19] to allow Aberman et al.‚Äôs approach to bypass this constraint. Because
these baselines are trained from scratch, we increased their training iterations
to five times more than ours.
Dataset details.Duetosomestylelabelsinthe100STYLEdatasetinherently
containing content meanings, like ‚Äôjump‚Äô and ‚Äôkick‚Äô, which may conflict with
the content text in the HumanML3D dataset. For example, style motion about
‚Äôkick‚Äôwillconflictwithcontenttext‚Äôapersonwalksforwardandthenbackward.‚Äô
To fairly compute the SRA metric, we follow [24] to categorize style labels in
the 100STYLE dataset into six groups: character (CHAR), personality (PER),20 Zhong et al.
emotion (EMO), action (ACT), objective (OBJ), and motivation (MOT). No-
tably, the ‚ÄôACT‚Äô group contains content meaning; we exclude the ‚ÄôACT‚Äô group
style motion when computing the SRA metric for content text from the Hu-
manML3D dataset. It is worth noting that we use all categories of style motion
duringtraining.Table.4isthedetailedgroupingofstylelabelsinthe100STYLE
dataset.
Table 4: The detailed grouping of style labels in the 100STYLE dataset.
Category Label
CHAR Aeroplane, Cat, Chicken, Dinosaur, Fairy, Monk, Morris, Penguin,
Quail, Roadrunner, Robot, Rocket, Star, Superman, Zombie (15)
PER Balance, Heavyset, Old, Rushed, Stiff (5)
EMO Angry, Depressed, Elated, Proud (4)
ACT kimbo, ArmsAboveHead, ArmsBehindBack, ArmsBySide,
ArmsFolded, BeatChest, BentForward, BentKnees, BigSteps,
BouncyLeft, BouncyRight, CrossOver, FlickLegs, Followed,
GracefulArms, HandsBetweenLegs, HandsInPockets, HighKnees,
KarateChop, Kick, LeanBack, LeanLeft, LeanRight, LeftHop,
LegsApart, LimpLeft, LimpRight, LookUp, Lunge, March, Punch,
RaisedLeftArm, RaisedRightArm, RightHop, Skip, SlideFeet,
SpinAntiClock, SpinClock, StartStop, Strutting, Sweep, Teapot,
Tiptoe, TogetherStep, TwoFootJump, WalkingStickLeft,
WalkingStickRight, Waving, WhirlArms, WideLegs, WiggleHips,
WildArms, WildLegs (58)
MOT CrowdAvoidance, InTheDark, LawnMower, OnHeels, OnPhoneLeft,
OnPhoneRight, OnToesBentForward, OnToesCrouched, Rushed (9)
OBJ DragLeftLeg, DragRightLeg, DuckFoot, Flapping, ShieldedLeft,
ShieldedRight, Swimming, SwingArmsRound, SwingShoulders (9)
A.4 Inference times
To evaluate the inference efficiency of our submodules, full model, and baseline
methods for stylized text2motion tasks, we report the average Inference Time
perSentencemeasuredinseconds(AITS)[6],inTable5.TheAITSiscalculated
by setting the batch size to 1 and excluding the time cost formodel and dataset
loading on an NVIDIA A5000 GPU.
A.5 More details on classifier-based style guidance
In our experiments, we observed a phenomenon similar to that described in
Text2Image [59]: In the early denoising stages, the generated motion gradually
transitions from random movement to motion that adheres to the content text.
Oncetheglobalmotioncontentisshaped,subsequentdenoisingstagesprimarilySMooDi: Stylized Motion Diffusion Model 21
Sub- MLD w/o w/o Methods Ours MLD+ MLD+
Modules adaptor classifier-based Overall MotionPuzzle Abermanetal.
Time(s) 0.2139 2.5081 0.5563 Time(s) 3.1133 0.2420 0.2275
Table5:Inferencetime.WereporttheAverageInferenceTimeperSentence(AITS)
in seconds for baselines and each submodule of ours on stylized text2motion tasks.
focus on modifying the local details and enhancing the quality of the motion.
Introducingclassifier-basedstyleguidanceatanearlystagenotonlyposeschal-
lenges in steering the motion toward the desired style but also affects the mo-
tion‚Äôs adherence to the content text. Therefore, we apply classifier-based style
guidancenearthelaststage,oncetheroughoutlineoftheglobalmotioncontent
has been established and the focus shifts to modifying local details. Moreover,
wecaniterateclassifier-basedguidancemultipletimesK toimprovethesteered
accuracy:
(cid:40)
K if T <t<T,
K = e s
K if t‚â§T .
l s
We use K =0, K =5, and T =300 in our experiments.
e l s
Cycle prior-preservation Loss
Sample from ùúÄ $(ùëß !;ùë°,ùëê,ùë†‚Ä≤) ùúÄ $(ùëß !"#,ùë°,ùëê,ùë†#")
100STYLE ùëß ! ùëß !"# ùëß# !
ùëß!,c,s s
ùê∑(ùëß!" #) ùë†"!
=
s‚Ä≤ ùë†!"= ùê∑(ùëß
#"!)
Sample from ùëß‚Ä≤ ùúÄ $(ùëß !‚Ä≤;ùë°,ùëê‚Ä≤,ùë†) ùëß#" ùúÄ $(ùëß !#",ùë°,ùëê‚Ä≤,ùë†"#) ùëß#‚Ä≤
HumanML3D ! ! !
ùëß",c",s‚Ä≤
!
Cycle prior-preservation Loss
Fig.8: Visual pipeline of the cycle prior-preservation loss.
A.6 More details on cycle prior-preservation loss
We introduce the cycle prior-preservation loss to ensure that generated motion
retains content-invariant characteristics from the content text. Fig. 8 illustrates22 Zhong et al.
the cycle prior-preservation loss‚Äôs visual pipeline. At timestep t, the process be-
gins with sampling content text c, style motion sequence s, and noisy motion
latent z from the 100STYLE dataset, alongside their equivalents c‚Ä≤, s‚Ä≤, and z‚Ä≤
t t
from the HumanML3D dataset. Following this, we facilitate the transfer of con-
tentandstyleconditionsbetweenthesedatasets,yieldingzsh andzhs.Decoding
t t
zhs into the motion space generates the shs motion sequence. Viewed as a style
t
motionsequence,shs iscombinedwiththeoriginalcontenttextctoreconstruct
the noisy latent z¬Ø. The cycle prior-preservation loss then operates between the
t
original noisy latent z and the reconstructed noisy latent z¬Ø.
t t
A.7 User study details
Tomitigatethepotentialchallengesinparticipantselectionwhentheyareasked
torankorscorevariousmethods,wedevelopedanonlinequestionnairewithpair-
wise A/B tests. We randomly selected 12 sets of stylized motion for the stylized
text-to-motiontaskand10setsforthemotionstyletransfertasks.Werecruited
22 human subjects from various universities, representing a range of academic
backgrounds,toparticipateinourstudy.Atthestartoftheuserstudy,weintro-
ducedtheconceptofmotionstylization,providingexamplesofboththecontent
text/motionandstylemotionforreference.Withthereferencestylemotionand
content text/motion provided, participants were asked to evaluate and choose
the better one based on the dimensions of Realism, Style Reflection, and Con-
tentPreservation,respectively.AsshowninFig.7,ourapproachachievesbetter
performancethanthebaselinesontwotasksacrossthreeevaluationdimensions.
A.8 More ablation studies
VaryingtheweightofClassifier-basedstyleguidance. Duetotheflexibil-
ity of the style guidance weights, we explore the effects of varying the classifier-
based style guidance weight in Fig. 9. We observe that increasing the classifier-
based style guidance weight boosts the SRA metric but reduces R Precision,
MM Dist, and FID, which means less content preservation but reflecting style
more accurately. It is observed that when the absolute value of the weight of
classifier-based style guidance œÑ exceeds 0.2, the rate of increase for SRA met-
ricsslowsdown,yettheothermetricscontinuetodeterioraterapidly.Therefore,
we set œÑ =‚àí0.2 as a trade-off.
Varying the weights of the classifier-free style guidance. Similar to how
we can adjust the weights of classifier-based style guidance to balance style re-
flectionandcontentpreservation,asdiscussedinSec.A.8,adjustingtheweights
ofclassifier-freestyleguidancealsoinvolvesatrade-off.Fig.10illustratestheef-
fects of varying the classifier-free style guidance weights w , while setting œÑ =0.
s
As the weights w increase, the SRA gradually increases, while the R-precision
s
and FID metrics deteriorate. It is observed that when w exceeds 1.5, FID, R
sSMooDi: Stylized Motion Diffusion Model 23
(a) Weights of Classifier-based Style Guidance (b) Weights of Classifier-based Style Guidance (c) Weights of Classifier-based Style Guidance
Fig.9: Varying the weights of the classifier-based style guidance.
(a) Weights of ùíòùíî (b) Weights of ùíòùíî (c) Weights of ùíòùíî
Fig.10: Varying the weights of the classifier-free style guidance.
Precision, and MM Dist decrease more rapidly, whereas SRA continues to in-
creaseatthesamerate.Therefore,wesetw =1.5topreventrapiddeterioration
s
in content preservation metrics while ensuring optimal performance in the SRA
metric.
The alternative approach of prior preservation loss. In Sec. 3.3, we in-
troduce our prior preservation loss, which involves sampling instances from the
HumanML3D dataset as well as from the 100STYLE dataset, and then cal-
culating the loss to prevent ‚Äôcontent-forgetting.‚Äô A straightforward alternative
approach involves simply combining the 100STYLE and HumanML3D datasets
to create a larger dataset, and then only utilizing L to fine-tune the style
std
adaptor. Given the larger number of samples in the HumanML3D dataset com-
pared to the 100STYLE dataset, this approach struggles to effectively capture
style features from instances in the 100STYLE dataset and maintain learned
content in a single optimization step. We term this alternative method the
combined dataset approach, utilizing it to train the style adaptor across the
same number of training iterations. Compared to the second and third rows
in Table 6, the combined dataset approach shows markedly worse performance
in content preservation metrics, such as FID and MM Dist values, indicating
a failure to preserve content. These results demonstrate that our simple prior
preservation loss can effectively learn style features and simultaneously preserve
the learned content with minimal training steps.
)%(ARS
)%(ARS
noisicerP
R
)%(ARS
no )%isi
(c Aer RP
S R
tsiD
MM
tsiD
MM
)%(ARS
)%(ARS
DIF
DIF24 Zhong et al.
Table 6: Ablation Studies on HumanML3D Content and 100STYLE Styles.
Method FID‚ÜìFootskatingMMDist‚Üì R-precision‚Üë Diversity‚ÜíSRA(%)‚Üë
ratio‚Üì (Top-3)
Ours(onall) 1.609 0.124 4.477 0.571 9.235 72.418
combineddataset 3.892 0.332 6.152 0.379 6.833 57.573
A person swings
a golf club.
Style Motion Content Text (a) MLD (b) Ours.
Fig.11: A visual example showing conflicts between content text and style motion in
a specific body part.
A.9 Limitation and future plans
A primary limitation of our approach is its reliance on a pre-trained motion dif-
fusionmodel,whichimpactstherealismofthegeneratedmotions.Consequently,
our approach may produce motions with foot skating for certain content texts.
We present these failure cases in the supplementary video. Incorporating real-
ism guidance [53] or physical constraints [60] might be a promising direction to
improve the realism of the generated motions.
Another limitation is that, due to the classifier-based style guidance po-
tentially requiring iteration, our approach is more time-consuming than MLD
by nearly 10 times. A potential direction for improvement involves decreasing
the number of denoising steps, inherently reducing the iterations required for
classifier-based guidance. Exploring the integration of a one-step model, such
as the consistency model [44], in the motion generation could be a valuable
direction.
noitoM2txeTdezilytSSMooDi: Stylized Motion Diffusion Model 25
Fig.12: Comparing our approach with the variant without separating the
classifier-free style guidance from content guidance.
)%(ARS
DIF
tsiD
MM