Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for
Multi-Robot Cooperation Tasks
Pu Feng, Junkang Liang, Size Wang, Xin Yu, Rongye Shi, and Wenjun Wu
Abstractâ€”In multi-agent reinforcement learning (MARL),
theCentralizedTrainingwithDecentralizedExecution(CTDE)
framework is pivotal but struggles due to a gap: global state
guidance in training versus reliance on local observations
in execution, lacking global signals. Inspired by human so-
cietal consensus mechanisms, we introduce the Hierarchical
Consensus-based Multi-Agent Reinforcement Learning (HC-
MARL) framework to address this limitation. HC-MARL Cooperation
employscontrastivelearningtofosteraglobalconsensusamong
agents,enablingcooperativebehaviorwithoutdirectcommuni- Environment State Local Observation
cation.Thisapproachenablesagentstoformaglobalconsensus
Fig.1:Therelationshipbetweentheenvironmentalstateand from local observations, using it as an additional piece of
informationtoguidecollaborativeactionsduringexecution.To localobservationsintheCTDEframework.Despitediffering
cater to the dynamic requirements of various tasks, consensus local observations, they all correspond to the same environ-
is divided into multiple layers, encompassing both short-term mental state at each timestep, providing diverse perspectives
and long-term considerations. Short-term observations prompt
of a unified global state. In traditional CTDE approaches,
the creation of an immediate, low-layer consensus, while long-
agents rely solely on these local observations for decision-
term observations contribute to the formation of a strategic,
high-layer consensus. This process is further refined through making during execution.
an adaptive attention mechanism that dynamically adjusts the
influenceofeachconsensuslayer.Thismechanismoptimizesthe mation sharing and increased bandwidth requirements. The
balance between immediate reactions and strategic planning, second method leverages intrinsic rewards to create leader-
tailoring it to the specific demands of the task at hand.
follower dynamics [7], yet this approach is constrained by
Extensive experiments and real-world applications in multi-
itstask-specificeffectivenessandencountersdifficultieswith
robotsystemsshowcaseourframeworkâ€™ssuperiorperformance,
marking significant advancements over baselines. broad applicability and generalization. The third approach,
employingmeanfieldtheory[8],offersapromisingdirection
I. INTRODUCTION
butoftenstrugglestoeffectivelyhandlecomplextasks.Given
Multi-Agent Reinforcement Learning (MARL) is garner- existing methodsâ€™ limitations, sociologyâ€™s research [9], [10]
ing increasing attention for its capability to tackle complex on consensus team agents interacting to align on shared
tasks [1]. Tasks involving distributed multi-robots [2] often values offers potential solutions for CTDEâ€™s challenges with
require several agents to collaborate based on their local ob- partial observations.
servationstoaccomplishagivenobjective.Thisrequirement Inspired by consensus mechanisms in multi-agent sys-
aligns with the commonly adopted MARL framework of tems [11], we introduce the Hierarchical Consensus-based
Centralized Training with Decentralized Execution (CTDE), Multi-Agent Reinforcement Learning (HC-MARL) frame-
as exemplified by methods such as MADDPG (Multi-Agent work, designed to facilitate substantive multi-agent collabo-
DeepDeterministicPolicyGradient)[3]andMAPPO(Multi- ration in settings characterized by local observations and the
Agent Proximal Policy Optimization) [4]. These approaches absenceofdirectcommunication.AsshowninFig.1,despite
utilize global information during the training phase through differing local observations, agents all correspond to the
the critic, while the actor relies solely on individual obser- same environmental state at each timestep, merely offering
vations during execution. This setup leads to a significant diverseperspectivesofaunifiedglobalstate.Adaptingideas
challenge: agents lack a consensus during task execution, from contrastive learning [12], we first map local obser-
hindering their collective performance for cooperation [5]. vations into discrete latent spaces as forms of invariances
In addressing this issue, three primary methodolo- using the consensus builder. We define these invariances
gies have been advanced. The first involves strategies as global consensus. This global consensus is then treated
for communication-based multi-agent reinforcement learn- as an additional piece of local observation information fed
ing [6], which are faced with challenges in selective infor- into the actor network. Notably, utilizing the consensus only
requires an agentâ€™s local observations, aligning with the
*This work was supported in part by the National Key R&D Program
CTDE frameworkâ€™s prerequisite for partial observability.
of China (2022ZD0116401) and the National Natural Science Foundation
of China (62306023). All authors are with Beihang University, Xueyuan The initial findings, however, highlighted a limitation:
Road No.37, Beijing 100191, China. Emails: {fengpu, liangjunkang, size-
relyingexclusivelyonobservationsfromasingletimestepto
wang,nlsdeyuxin,shirongye,wwj09315}@buaa.edu.cn.WenjunWuisthe
correspondingauthor. establish a single-layer consensus falls short in fully captur-
4202
luJ
11
]IA.sc[
1v46180.7042:viXraingthenuancesofsequentialtasks.Forexample,asdepicted
obs
in Fig. 2, within the CTDE paradigm, the agentsâ€™ execution
obsrange
information is confined to local observations. Incorporating Neighbor obswith static
static environmental states, or short-term consensus, supple- state information
Agent
ments execution by integrating additional static information obswith dynamic
state information
from other agents, such as their positions and orientations,
Environment Information
not initially available. To effectively incorporate dynamic for execution
attributes, like other agentsâ€™ velocities, introducing dynamic
Fig. 2: Importance of Dynamic State Information: The di-
stateinformationthroughlong-termconsensusbecomescru-
agram illustrates agents as green triangles and neighbors
cial. To address this, we propose the hierarchical consensus
as blue triangles. The orientation of agents is indicated
mechanism, where consensus based solely on single-step
by the vertical position of the triangles, and their motion
observations is defined as low-layer consensus, focusing on
direction is shown by the arrows. The left side displays the
short-term optimality. High-layer consensus, on the other
environmental state, while the right side shows information
hand, aggregates local observations from multiple timesteps,
usableinexecutionwithinCTDE.Staticenvironmentalinfor-
forming a long-term group consensus that considers the
mation provides position and orientation, whereas dynamic
broader strategic trends or states of multiple agents. Lastly,
information additionally offers speed data.
weemployanattentionmechanismtodynamicallyweighthe
influence of each consensus layer for collaborative needs,
(MARL) [4], [20], [21] has become a focal point within the
ensuring that the system can adaptively prioritize either
third category, emphasizing the utilization of consensus for
short-termorlong-termconsiderationsbasedontheevolving
enhancing agent cooperation in complex environments.
context of the task. This hierarchical consensus serves as
additional local observation during the execution of actions,
B. Contrastive Learning
providingagentswithcontext-richgroupbehavioralinsights.
Recentstudieshaveincreasinglyleveragedself-supervised
Our HC-MARL framework can be seamlessly integrated
learning [22] to enhance model capabilities for downstream
as components of almost all MARL algorithms. The contri-
tasks.Amongthese,contrastivelearning[23]standsoutasa
butions of this paper are summarized as follows:
particularlytractableapproach.Itoperatesontheprincipleof
â€¢ Leveraged contrastive learning to construct global con-
minimizing the distance between augmented versions of the
sensus from local observations, enhancing cooperative
samesamplewhilemaximizingthedistancebetweendistinct
action execution in multi-agent reinforcement learning.
ones,asnotedbyWang[24].Thismethodeffectivelyboosts
â€¢ Introduced the HC-MARL framework with a hierarchi-
themodelâ€™sunderstandingofequivalentdatarepresentations.
calconsensusmechanism,featuringbothshort-termand
However, the collection of positive and negative samples
long-term consensuses.
in contrastive learning presents challenges that significantly
â€¢ Implemented an adaptive attention mechanism that dy-
impactitseffectiveness.Inreinforcementlearning(RL)[25],
namically tunes the influence of each consensus layer,
samples are collected through ongoing interactions with the
optimizing the balance between immediate responses
environment,highlightingthepotentialbenefitsofintegrating
andstrategicplanningaccordingtothespecificdemands
contrastive learning with RL. Recent research efforts [26],
of the task.
havesuccessfullyemployedcontrastivelearningforrepresen-
â€¢ Demonstrated the superior performance of our frame-
tationlearningpriortoreinforcementlearning,achievingun-
work over baselines through extensive evaluations in
paralleleddataefficiencyinpixel-basedRLtasks.Moreover,
both simulated tasks and real-world robot experiments.
contrastive learning has been utilized to formulate reward
functions within RL systems [27].
II. RELATEDWORK
A. Consensus in Multi-Agent System C. Contrastive Learning for MARL
Consensusmeanstheinteractionbetweengroupsofagents Despite the progress in single-agent RL contexts, the
in a team to reach an agreement on a common value or exploration of contrastive learning in MARL remains com-
state [13], [14]. Consensus in Multi-Agent Systems has paratively underdeveloped. Lin [28] proposed incorporating
garnered extensive research interest, primarily focusing on contrastive learning outcomes as a loss function in the
achieving shared agreement among agents. Research in this MARL training process, applying it to multi-agent path
area spans three main categories: first, studies inspired by planningtasks.Xu[29]introducedusingcontrastivelearning
biologicalmechanismsandwildlifecollectivebehaviors[15], to articulate the observational differences among agents.
[16]; second, theoretical explorations using models like the Further, Liu [30] promoted the development of a common
graph theory for foundational insights into consensus [17], languagebymaximizingthemutualinformationbetweenthe
[18]; and third, practical applications, including the devel- messages of given trajectories in a contrastive manner. Our
opment of consensus models and protocols [19], with a HC-MARLmethodemployscontrastivelearningtoestablish
keenfocusonconvergence,equilibrium,andimplementation amulti-layerglobalconsensus.Tothebestofourknowledge,
challenges. Notably, Multi-Agent Reinforcement Learning thisisthefirststudyutilizingcontrastivelearningtocreateastructured global consensus for collaborative agent behavior For the actors, which operate based on their local obser-
under local observations. vations during decentralized execution, the policy gradient
is adjusted to reflect their dependence on local observations
III. PRELIMINARIES
o. The policy gradient for optimizing each agentâ€™s policy Ï€,
A. Problem Formulation considering local observations, is derived as follows:
The multi-robot cooperation task can be formulated as a
decentralized partially observable Markov decision process âˆ‡ J(Ï€)=E [âˆ‡ logÏ€(o,a|Î¸)Q (s,a)] (2)
Î¸ o,aâˆ¼ÏÏ€ Î¸ Ï•
(Dec-POMDP) [31], defined as (I,S,A,T,R,O,Z,Î³). The
index set (I = {1,..,N}) represents the set of agents. C. Contrastive Learning
S is the global state space. Note that each agent is only
The Knowledge Distillation with No Labels (DINO) [32]
capable of partial observation of the environment s âˆˆ S,
method offers a solution as a form of self-supervised con-
and the individual observation o âˆˆO comes from the local
i trastive learning that leverages a teacher-student network
observation function o = Z (s) : S â†’ O. Each agent i
i i architecture. In this framework, for a given sample u, a new
chooses its action according to its policy a i âˆ¼ Ï€ i(Â·|o i). sample uâ€² is generated through data augmentation. Both u
The joint action space A consists of the union of all agentâ€™s and uâ€² are then fed into the student and teacher networks,
actionspace(cid:83)N
A .Wedefinethestatetransitionfunction
i=1 i respectively,producingclassificationdistributionsP S(u)and
T : S Ã— A â†’ S and the discount factor Î³ âˆˆ (0,1). All P (uâ€²). In the absence of true labels, the teacher networkâ€™s
T
agents share the same joint reward function R(s,a). The
outputservesaspseudo-labels,andthestudentnetworkaims
agents aim to maximize the expected joint return, defined as
tooptimizebyminimizingthecross-entropylossbetweenits
E [(cid:80)âˆ Î³tR(s ,a )]
Ï€ t=0 t t output and these pseudo-labels.The cross-entropy loss used
for optimization can be formalized as:
B. Centralized Training with Decentralized Execution
(CTDE)
(cid:88)
L =âˆ’ P (uâ€²) logP (u) (3)
CL T c S c
To address the problems under decentralized partially
c
observable Markov decision processes (Dec-POMDPs), the
where c indexes over the classes, P (uâ€²) is the pseudo-
Centralized Training with Decentralized Execution (CTDE) T c
labelprobabilityforclasscproducedbytheteachernetwork
framework emerges as a critical approach. CTDE delineates
for augmented sample uâ€², and P (u) is the probability
amethodologywherethetrainingphaseiscentralized,allow- S c
produced by the student network for the original sample u.
ingagentstoaccessglobalinformationandlearncoordinated
The teacher and student networks share the same archi-
strategies. In contrast, during execution, each agent operates
tecture, with the teacherâ€™s parameters being an exponential
independently based on its local observations, aligning with
moving average (EMA) of the studentâ€™s parameters. This
the decentralized nature of many real-world applications.
arrangementfacilitatesacontinuousrefinementofthestudent
Although value decomposition methods and policy-based
networkâ€™s learning through guidance from a slowly evolving
multi-agent methods differ significantly in structure, they
version of itself, represented by the teacher network.
bothadheretotheCTDEprinciplesandencounterchallenges
In multi-agent reinforcement learning scenarios, local ob-
inprovidingunifiedguidanceduringdecentralizedexecution
servations made by different agents can be considered as
in fully cooperative tasks. To illustrate the optimization
diverse augmented samples of the same global state. The
process within the CTDE paradigm, we focus on the most
global consensus, therefore, corresponds to the classification
commonlyusedmethod,MAPPO.MAPPOisanactor-critic
outputfromtheteacher-studentnetworkframework.Bycon-
method based on the CTDE paradigm. Each agent learns a
policy Ï€ in an on-policy manner. MAPPO consists of a cen- structing this consensus metric, our aim is to guide agents,
operating under local observations, towards forming global
tralized critic and several independent actors corresponding
cooperation.
toeachagent.Duringthecentralizedtrainingphase,thecritic
utilizes global state information to estimate the joint action-
IV. METHODS
value Q. The critic is trained by minimizing the Temporal
Difference (TD) error as follows: In this section, we introduce Hierarchical Consensus-
based Multi-Agent Reinforcement Learning (HC-MARL), a
novel framework that dynamically guides agents towards
(cid:104) (cid:105)
LCritic(Ï•)=E (s,a,r,sâ€²) (Q Ï•(s,a)âˆ’(r+Î³Q Ï•â€²(sâ€²,aâ€²)))2 cooperative execution under partial observations through a
(1) hierarchical consensus mechanism.
where Q (s,a) represents the criticâ€™s current estimate of
Ï•
A. Consensus Builder
the joint action-value for the global state s and actions a,
parameterizedbyÏ•;r istheimmediaterewardreceivedafter In multi-agent reinforcement learning, agents execute ac-
taking action a in state s; sâ€² is the next state, and aâ€² is the tionsbasedonlocalobservations,leadingtoalackofglobal
action taken in the next state as per the current policy; Î³ is information guidance during execution. This section focuses
the discount factor; and Ï•â€² refers to the parameters of the on building an effective consensus within the Centralized
target critic used for bootstrapping. Training with Decentralized Execution (CTDE) framework.Asdiscussedinaprevioussection,theDINOframeworkpro- 1-st layer Consensus
cesses a sample and its data-augmented equivalent through Teacher
1
a teacher-student network. We treat an agentâ€™s observation
EMA
ğ‘ƒğ‘ƒğ‘‡ğ‘‡(ğ‘¥ğ‘¥ğ‘—ğ‘—1
)
o as the sample, and observations from other agents as the 1 ğ‘ğ‘1 1
ğ‘¥ğ‘¥ğ‘—ğ‘—={oj} ğ¿ğ¿S
equivalent samples. In other words, the observation function ğ‘ƒğ‘ƒğ‘†ğ‘†(ğ‘¥ğ‘¥ğ‘–ğ‘–1
)
Student
oZ bsis erc vo an tis oid ne sre id sa an ugau mg em nte en dta it nio ton eo ap ce hra ati go en n, tâ€™w sh oe br se et rh ve atg iolo nb oal
. 1Â·
ğ‘¥ğ‘¥ğ‘–ğ‘–1 ={oğ‘–ğ‘–ğ‘ğ‘1
}
â€¦ 1 M Au ttl eti n-h tie oa nd
m-thlayer Consensus
These observations o correspond to the same global state s. Teacher
m
Drawing inspiration from human patterns of situational ğ‘šğ‘š
ğ‘ƒğ‘ƒğ‘‡ğ‘‡(ğ‘¥ğ‘¥ğ‘—ğ‘—) ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
a stw anar de in ne gss oâ€” f tw heh ie rre eni vn id roiv ni mdu ea nl ts fo roft men lod ce ar liv ce uea s,b sr uo ca hd au snd de isr- - Trajectory history ğ‘¥ğ‘¥ğ‘—ğ‘—ğ‘šğ‘š ={ojğ‘ğ‘1 â€¦oğ‘—ğ‘—ğ‘ğ‘ğ‘šğ‘š }EMA ğ¿ğ¿ğ‘šğ‘š S ğ‘ƒğ‘ƒğ‘†ğ‘†(ğ‘¥ğ‘¥ğ‘–ğ‘–ğ‘šğ‘š )
ğ‘ğ‘ğ‘–ğ‘–
Attention-weighted
cerninggeneralcardinaldirectionsinacitywithoutknowing Consensus
precise coordinatesâ€”we propose a model that leverages ğ‘¥ğ‘¥ğ‘–ğ‘–ğ‘šğ‘š ={oğ‘–ğ‘–ğ‘ğ‘1 â€¦oğ‘–ğ‘–ğ‘ğ‘ğ‘šğ‘š } Studentğ‘šğ‘š
discrete categories for consensus in multi-agent systems. Fig. 3: An overview of the Hierarchical Consensus Mech-
This approach simulates how agents might infer a macro anism. xm i and xm j represent different local observations
classification of the current state from limited, local infor- from the same environmental state for the m-th layer, which
mation. Specifically, we define consensus in terms of K are used to derive a global consensus classification through
distinctclasses,enablingtheconsensusmoduletocategorize theteacher-studentnetwork.Consensusfromdifferentlayers
an agentâ€™s local observations into a unified class k, which is aggregated into an attention-weighted consensus through
subsequently acts as the global consensus for guiding the multi-head attention.
agentsâ€™ actions.
this issue, this section introduces a hierarchical consensus
For each agent i in a set of n agents, the classification
mechanism, divided into short-term consensus and long-
distribution resulting from local observations is denoted by
term consensus. Short-term consensus considers only the
P (o ) for the student network and P (o ) for the teacher
S i T i
currenttimestepâ€™sstate,whilelong-termconsensustakesinto
network. The consensus among agents is evaluated by pair-
accountinformationacrossmultipletimesteps,incorporating
wise comparison, optimizing the Consensus Builder through
a longer-term utilization of historical state information. This
minimizing the sum of cross-entropy loss:
is dynamically leveraged through an attention mechanism
(cid:88)n (cid:88)n (cid:88) that weighs the importance of short-term and long-term
L (Î¸)=âˆ’ P (o ) logP (o ) (4)
S T j k S i k consensus. For instance, in scenarios requiring immediate
i=1j=1 k collision avoidance, agents prioritize short-term consensus.
where i and j are the indices of the agents. P (o ) Conversely, in collaborative search tasks, agents rely more
T j k
and P (o ) denote the probability of category k in the on long-term consensus to allocate search areas efficiently.
S i k
distribution output by the teacher and student networks, We expand the foundational consensus builder into a
respectively. The consensus c for each agent is obtained as Hierarchical Consensus Mechanism, distinguishing between
follows: short-term and long-term consensus. Short-term consen-
sus leverages observations or consensus from a single
c =argmaxP (o ) k (5) timestep. For long-term consensus, we introduce xm =
i S i c i
k {ot1,ot2,ot3,...,otm}, a set representing the agent iâ€™s ob-
Thisprocedureidentifiestheconsensusclassc foragenti i i i i
i servations at various timesteps within the trajectory history,
byselectingtheclassk withthemaximumprobabilityinthe
wheremindicatestheinclusionofmhistoricalobservations.
classificationdistributionP (o ),asgeneratedbythestudent
S i Itâ€™s crucial to note that t , t , t , and so forth, denote
1 2 3
network for observation o .
i distinct timesteps, which are not required to be consecutive.
Such a method underscores the agentsâ€™ collaborative push
This approach proves especially beneficial in scenarios with
towards a harmonized environmental perception, thereby
brief training intervals, where successive states may exhibit
enabling a collective consensus derived from individual
minimal differences. By considering observations at spaced
observations.Byemployingacross-entropylossfunction,the
intervals, we capture more pronounced changes over time,
framework promotes similarity in probability distributions
thereby gaining insights into significant state transitions.
for observations by different agents, even in disparate local
As we transition from utilizing single-timestep observa-
contexts, thereby fostering a unified understanding of the
tions to aggregating multi-timestep observations for con-
global state.
trastivelearning,theoptimizationcriterionforthem-thlayer
B. Hierarchical Consensus Mechanism studentnetworkiscorrespondinglyrevised.Theupdatedloss
function is defined as follows:
Through the consensus builder, we have obtained a global
consensus among agents based on their local observations. (cid:88)n (cid:88)n (cid:88)
Lm(Î¸)=âˆ’ P (xm) logP (xm) (6)
However, as shown in Fig. 2, obtaining a complete and S T j k S i k
effective global consensus from a single momentâ€™s local ob- i=1j=1 k
servationsischallenginginpracticalapplications.Toaddress Therefore,theconsensusforthem-thlayer,cm,isredefinedtions. We integrate this consensus as an augmented obser-
ğ‘œğ‘œğ‘–ğ‘– vation input within the multi-agent reinforcement learning
Hierarchical
ğ‘¥ğ‘¥ğ‘–ğ‘–ğ‘šğ‘š C Bon us ie ldn es rus ğ‘ğ‘ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ framework. Itâ€™s pivotal to emphasize that while information
Policy Network from other agents is leveraged during the training phase of
Hierarchical
Agent
ğ‘¥ğ‘¥jğ‘šğ‘š C Bon us ie ldn es rus ğ‘ğ‘ğ‘—ğ‘—ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ the student network, the action execution phase exclusively
relies on an agentâ€™s local observations. This design principle
Hierarchical
Consensus Critic Network ensures that our HC-MARL approach is compatible with a
Environment Builder
Local Observation Consensus Class widerangeofMARLalgorithms,adheringtotheCentralized
Global state s Training with Decentralized Execution (CTDE) paradigm.
Fig.4:OverviewoftheHC-MARLframework.Sequentially, Taking MAPPO as an example, by incorporating consen-
from left to right: Agents initially acquire local observations susinformationintotheobservations,theupdateforthecritic
from the environment. These observations are subsequently network becomes:
processedby thehierarchicalconsensusbuilder, yieldingthe
current consensus class. This derived consensus, denoted as (cid:104)(cid:16)
catt,enrichestheagentsâ€™observationalorstatedata.Itisthen LCritic(Ï•)= E (s,a,r,sâ€²) Q Ï•(s,catt,a)
i
incorporated into both policy and critic networks, thereby (cid:17)2(cid:105)
âˆ’(r+Î³Q (sâ€²,cattâ€² ,aâ€²)) (9)
steering agent actions in alignment with the collectively Ï•â€²
determined global consensus.
where catt and cattâ€² represent the attention-weighted
for each agent i as follows: consensus information at the current and next time steps,
respectively. The actor network update is formulated as
cm i =argmaxP S(xm i ) k (7) follows:
k
Given the multiple layers of consensus achieved through âˆ‡ ÏˆJ(Ï€)=E o,catt,aâˆ¼ÏÏ€(cid:2) âˆ‡ Î¸logÏ€(o,catt,a|Ïˆ)Q Ï•(s,catt,a)(cid:3)
the hierarchical structure, it becomes crucial to evaluate (10)
and weigh these layers differently across various scenarios. These formulations illustrate how consensus information,
This differentiation acknowledges that the importance of specifically the attention-weighted consensus catt, is seam-
short-term and long-term consensus can vary significantly lessly integrated into the MARL process, enhancing the
depending on the context. To address this, we incorporate learning mechanism by providing a more informed perspec-
an attention mechanism that treats the consensus from each tive on the environment.
layerasinput,asdepictedinFig.3.Thisapproachenablesus
V. EXPERIMENTANDRESULTS
to integrate these varied consensus inputs into a multi-head
attention framework, effectively allowing for the dynamic Weconductedbothsimulationsandhardwareexperiments
weighting of each layerâ€™s consensus. to validate the efficacy of our proposed HC-MARL.
To formalize this approach, we consider the output con-
A. Environment Settings
sensusfromeachlayer,cm,asinputtoamulti-headattention
i
mechanism. This mechanism aims to dynamically weigh
the consensus from different layers according to the current
scenarioâ€™s specific requirements, resulting in a contextually
weighted combination. The formalization is given by:
Unspecified
Predator Target
ca itt =MultiHead(Q(cm i ),K(cm i ),V(cm i )) (8) Prey
In this equation, ca itt represents the attention-weighted (a) Predator-Prey (b) Rendezvous (c) Navigation
consensusforagenti.ThefunctionsQ,K,andV correspond
Fig. 8: The simulated tasks considered in the experiments.
tothequery,key,andvaluefunctions,respectively,whichare
applied to the consensus inputs. These functions facilitate We constructed three cooperative multi-agent tasks within
the mapping of consensus from each layer into a space the Webots simulation [33], including Predator-Prey, Ren-
that allows for the evaluation of the layersâ€™ relevance. The dezvous [34], and Navigation, as shown in Fig. 8. We
MultiHead attention mechanism aggregates these mapped implemented our HC-MARL in these three tasks and com-
representations, allocating weights based on their assessed pared it against two main-stream MARL baselines Multi-
importance to the current decision-making context. Agent Proximal Policy Optimization (MAPPO) [4] and its
variant Heterogeneous-Agent Proximal Policy Optimization
C. HC-MARL Framework
(HAPPO) [35]. Note that our HC-MARL framework can
AsillustratedinFig.4, thehierarchicalconsensusmecha- seamlessly integrate with various MARL algorithms. To
nism enables us to derive the attention-weighted consensus, ensure fair comparisons with these variants of MAPPO, we
catt. This consensus serves as the agentâ€™s inferred under- constructedourHC-MARLframeworkbasedontheMAPPO
i
standing of the global state, derived from partial observa- architecture in this experiment.H MC A- PM PA ORL 700 1500
400 HAPPO
600
1250
500
300 1000
400
750
200 300
200 500
100 100 250
0 HC-MARL 0 HC-MARL
0 MAPPO MAPPO
âˆ’100 HAPPO HAPPO
âˆ’250
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e6 Steps 1e6 Steps 1e6
(a) 3 Predators - 1 Prey (b) 5 Predators - 1 Prey (c) 10 Predators - 1 Prey
Fig. 5: Learning curves of the HC-MARL, MAPPO, HAPPO on the Predator-Prey task. Each experiment was executed 5
times with different random seeds.
0 0 0 HC-MARL
MAPPO
HAPPO
âˆ’50 âˆ’50 âˆ’50
âˆ’100 âˆ’100 âˆ’100
âˆ’150 âˆ’150 âˆ’150
âˆ’200 âˆ’200
âˆ’200 HC-MARL HC-MARL
MAPPO MAPPO
HAPPO HAPPO
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e7 Steps 1e7 Steps 1e7
(a) 3 Agents (b) 5 Agents (c) 10 Agents
Fig. 6: Learning curves of the HC-MARL, MAPPO, HAPPO on the Rendezvous task. Each experiment was executed 5
times with different random seeds.
1200 2000 HC-MARL
MAPPO
600 1000 1750 HAPPO
800 1500
400 600 1250
1000
400
200 750
200
500
0 HC-MARL 0 HC-MARL 250
MAPPO MAPPO
HAPPO HAPPO 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Steps 1e6 Steps 1e6 Steps 1e6
(a) 3 Agents (b) 5 Agents (c) 10 Agents
Fig.7:LearningcurvesoftheHC-MARL,MAPPO,HAPPOontheNavigationtask.Eachexperimentwasexecuted5times
with different random seeds.
B. Main Results of predators was set to 3, 5, and 10, respectively, with the
number of prey fixed at one. The preyâ€™s escape trajectory
This section describes and analyzes the experimental re-
was randomly generated during both the training and testing
sults in three tasks. The performance of each algorithm was
phases.Fig.5a,5b,and5crespectivelyshowcasethelearning
evaluated with five different random seeds. The learning
curvesunderthesettingsofthree,five,andtenpredators.The
curves, in terms of episode reward under varying numbers
resultsdemonstratethatourwork,HC-MARL,surpassesthe
of agents, are presented in Fig. 5, 6, and 7. In addition
baseline algorithms in terms of episode reward convergence
to episode rewards, Table I compares the differences in
and convergence speed. Furthermore, Table I reveals that
algorithmperformancethroughthenumberofstepsrequired
HC-MARLrequiressignificantlyfewerstepstocompletethe
to complete the tasks after training. These results demon-
task post-training compared to the baseline algorithms, indi-
strate that our work, HC-MARL, achieved varying degrees
cating that our method accomplishes tasks more efficiently
of advantage over all baseline algorithms.
and enhances the performance of the algorithm.
Predator-Prey task. In this scenario, predators must
pursue and catch the prey through movement. The number Rendezvous task. In the Rendezvous task, where no
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipE
sdraweR
edosipEs ap ge gc ri efi gc ateta fr rg oe mt p tho ein irts raa nr de omass ii ng in tie ad l, loa cg ae tn iots nsau ot non to hm eo mu asl py
.
âˆ’âˆ’ 75 .. 50 kkkk====14816 Convergence Reward âˆ’âˆ’ 75 .. 50 mmmm====13510 Convergence Reward
âˆ’10.0 âˆ’10.0
Thenumberofagentswassetto3,5,and10.FromFig.6,it âˆ’12.5 âˆ’12.5
can be observed that HC-MARL outperforms the baselines âˆ’15.0 âˆ’15.0
in terms of convergence episode rewards and convergence âˆ’17.5 âˆ’17.5
âˆ’20.0 âˆ’20.0
speed. The performance advantage of HC-MARL over the
âˆ’22.5 âˆ’22.5
baselinesbecomesmorepronouncedwithanincreasingnum- âˆ’25.0 3 Number of 5Agents 10 âˆ’25.0 3 Number of 5Agents 10
ber of agents. Analyzing this phenomenon, it is evident that (a) Influence of k (b) Influence of m
as the number of agents increases, and thus the complexity
Fig.9:AblationstudyonHC-MARLintheRendezvoustask
of the task escalates, the hierarchical consensus mechanism
contributes more significantly to enhancing the algorithmâ€™s
which can be extended to GPS or other visual positioning
performance. Table I also demonstrates that HC-MARL
methods.
surpasses the baseline algorithms in terms of the number
WeconductedexperimentsonPredator-Prey,Rendezvous,
of steps required to complete the task.
and Navigation tasks. In the Predator-Prey scenario, the
Navigation task. In the navigationtask, agents are asked
HC-MARL algorithm required 16% fewer steps to capture
to navigate through two obstacles and reach the target point
the prey than MAPPO and 19% fewer than HAPPO. For
while avoiding collisions with other agents and obstacles.
the Rendezvous task, agents using HC-MARL completed
Fig.7demonstratesthatourHC-MARLmethodsignificantly
the gathering objective with 10% fewer steps compared to
improvesepisoderewardsacrosstaskswithvaryingnumbers
MAPPO and 15% fewer than HAPPO. In the Navigation
of agents. Specifically, HC-MARLâ€™s episode rewards are
task, HC-MARL agents reached their target destinations
approximately20%higherthanthoseofHAPOandMAPPO
with 30% less distance traveled than MAPPO and 34%
in tasks with three agents, and about 35% higher in tasks
less than HAPPO, without any collisions with obstacles.
with ten agents. Furthermore, Table I indicates that at ten
Taking the Navigation task as an example, Fig. 10 displays
agents, the improvement in the number of steps required
four representative scenes captured during both simulated
to complete the obstacle navigation task is even more sub-
and real-world experiments. For a visual representation,
stantial. HC-MARL requires only 700 steps to complete the
supplementary videos can be found in the Appendix.
task, representing a reduction of 30% and 40% compared to
HAPO and MAPPO, respectively.
C. Ablation Study
In the HC-MARL framework, we employ a hierarchical
consensus mechanism to derive short-term and long-term
consensus. To assess the efficacy of both the consensus
mechanism and the hierarchical approach, we conducted (a) Start (b) Nearly Colliding
ablation studies varying the number of consensus categories
and consensus layers.
Initially, we investigated the effect of global consensus
categories k on the Rendezvous task, testing k values of 1,
4,8,and16acrossagentcountsof3,5,and10.Fig.9ashows
scenarios with k >1 yield higher convergence rewards than
those with k = 1, highlighting the consensus mechanismâ€™s (c) Collision avoidance (d) Arrived
benefit. Optimal rewards for 3 and 5 agents occurred at k = Fig. 10: Navigation Task Demonstrations. Left is the real-
4; for 10 agents, k = 8 was most effective. This indicates world environment, and Right is the Webots simulation.
that simpler scenarios benefit from fewer categories, while
VI. CONCLUSION
moreagentsnecessitatemorecategoriesforoptimaltraining.
Additionally,theimpactofconsensuslayersmontraining We introduced the Hierarchical Consensus-Based Multi-
rewards was examined for m = 1 (no hierarchy), 3, 5, and Agent Reinforcement Learning (HC-MARL) framework, a
10, depicted in Fig. 9b. Optimal rewards were achieved at novel approach that employs hierarchical consensus to facil-
m = 5, suggesting that increasing consensus layers up to itate cooperative execution among agents based on local ob-
a point enhances task performance. However, beyond this servations. Recognizing that each agentâ€™s local observations
optimal level, training efficiency declined, due to increased aresubsetsofaconsistentglobalstate,ourframeworklever-
training complexity and instability with additional layers. ages contrastive learning from these observations to achieve
a global consensus, which then serves as additional local
D. Real World Experiments
observations for the agents. By implementing a hierarchical
We validated the real-world applicability of HC-MARL mechanism, we construct short-term and long-term consen-
by conducting experiments on E-puck swarm. We utilized sus to cater to the dynamic requirements of various tasks.
the NOKOV motion capture system for indoor positioning, This process is further refined through an adaptive attention
draweR draweRTABLE I: Number of steps required to complete the Task for HC-MARL and baselines (after training) across three tasks.
Error bars indicate the standard error of the mean.
Steps Predator-Prey Rendezvous Navigation
Agents MAPPO HAPPO HC-MARL(ours) MAPPO HAPPO HC-MARL(ours) MAPPO HAPPO HC-MARL(ours)
3 720Â±60 740Â±50 580Â±45 575Â±25 585Â±35 550Â±25 635Â±55 630Â±70 520Â±40
5 550Â±65 640Â±60 510Â±55 640Â±35 645Â±40 610Â±40 710Â±60 680Â±70 590Â±65
10 520Â±60 530Â±60 450Â±55 670Â±35 695Â±45 620Â±45 960Â±60 890Â±75 700Â±65
mechanism that dynamically adjusts the influence of each [14] R.Olfati-SaberandR.M.Murray,â€œConsensusproblemsinnetworks
consensus layer. Extensive experiments demonstrate that the of agents with switching topology and time-delays,â€ IEEE Transac-
tionsonautomaticcontrol,vol.49,no.9,pp.1520â€“1533,2004.
HC-MARL method significantly enhances the performance
[15] R. Olfati-Saber, â€œFlocking for multi-agent dynamic systems: Algo-
of multi-robot cooperation tasks. rithmsandtheory,â€IEEETransactionsonautomaticcontrol,vol.51,
no.3,pp.401â€“420,2006.
VII. ACKNOWLEDGMENT [16] T.Vicsek,A.CziroÂ´k,E.Ben-Jacob,I.Cohen,andO.Shochet,â€œNovel
typeofphasetransitioninasystemofself-drivenparticles,â€Physical
We gratefully acknowledge the support of the National reviewletters,vol.75,no.6,p.1226,1995.
Key R&D Program of China (2022ZD0116401) and the [17] Y.LiandC.Tan,â€œAsurveyoftheconsensusformulti-agentsystems,â€
Systems Science & Control Engineering, vol. 7, no. 1, pp. 468â€“482,
National Natural Science Foundation of China (Grant No.
2019.
62306023). [18] W.RenandR.W.Beard,â€œConsensusseekinginmultiagentsystems
under dynamically changing interaction topologies,â€ IEEE Transac-
REFERENCES
tionsonautomaticcontrol,vol.50,no.5,pp.655â€“661,2005.
[19] X. Yu, W. Wu, P. Feng, and Y. Tian, â€œSwarm inverse reinforcement
[1] C.Berner,G.Brockman,B.Chan,V.Cheung,P.Dkbiak,C.Dennison,
learningforbiologicalsystems,â€in2021IEEEInternationalConfer-
D.Farhi,Q.Fischer,S.Hashme,C.Hesse,etal.,â€œDota2withlarge ence on Bioinformatics and Biomedicine (BIBM). IEEE, 2021, pp.
scaledeepreinforcementlearning,â€arXivpreprintarXiv:1912.06680, 274â€“279.
2019. [20] Y.Gao,W.Wang,andN.Yu,â€œConsensusmulti-agentreinforcement
[2] A.Yahya,A.Li,M.Kalakrishnan,Y.Chebotar,andS.Levine,â€œCol- learning for volt-var control in power distribution networks,â€ IEEE
lective robot reinforcement learning with distributed asynchronous TransactionsonSmartGrid,vol.12,no.4,pp.3594â€“3604,2021.
guided policy search,â€ in 2017 IEEE/RSJ International Conference [21] Y.ZhangandM.M.Zavlanos,â€œCooperativemulti-agentreinforcement
onIntelligentRobotsandSystems(IROS). IEEE,2017,pp.79â€“86. learning with partial observations,â€ IEEE Transactions on Automatic
[3] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor- Control,2023.
datch, â€œMulti-agent actor-critic for mixed cooperative-competitive [22] A.Jaiswal,A.R.Babu,M.Z.Zadeh,D.Banerjee,andF.Makedon,â€œA
environments,â€ Advances in neural information processing systems, survey on contrastive self-supervised learning,â€ Technologies, vol. 9,
vol.30,2017. no.1,p.2,2020.
[4] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and [23] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola,
Y.Wu,â€œThesurprisingeffectivenessofppoincooperativemulti-agent A. Maschinot, C. Liu, and D. Krishnan, â€œSupervised contrastive
games,â€AdvancesinNeuralInformationProcessingSystems,vol.35, learning,â€Advancesinneuralinformationprocessingsystems,vol.33,
pp.24611â€“24624,2022. pp.18661â€“18673,2020.
[5] M.Wen,J.Kuba,R.Lin,W.Zhang,Y.Wen,J.Wang,andY.Yang, [24] T. Wang and P. Isola, â€œUnderstanding contrastive representation
â€œMulti-agentreinforcementlearningisasequencemodelingproblem,â€ learning through alignment and uniformity on the hypersphere,â€ in
Advances in Neural Information Processing Systems, vol. 35, pp. International conference on machine learning. PMLR, 2020, pp.
16509â€“16521,2022. 9929â€“9939.
[6] J. Sheng, X. Wang, B. Jin, J. Yan, W. Li, T.-H. Chang, J. Wang, [25] K.Arulkumaran,M.P.Deisenroth,M.Brundage,andA.A.Bharath,
and H. Zha, â€œLearning structured communication for multi-agent â€œDeepreinforcementlearning:Abriefsurvey,â€IEEESignalProcess-
reinforcementlearning,â€AutonomousAgentsandMulti-AgentSystems, ingMagazine,vol.34,no.6,pp.26â€“38,2017.
vol.36,no.2,p.50,2022. [26] M.Laskin,A.Srinivas,andP.Abbeel,â€œCurl:Contrastiveunsupervised
[7] G.WenandB.Li,â€œOptimizedleader-followerconsensuscontrolusing representations for reinforcement learning,â€ in International confer-
reinforcementlearningforaclassofsecond-ordernonlinearmultiagent enceonmachinelearning. PMLR,2020,pp.5639â€“5650.
systems,â€ IEEE Transactions on Systems, Man, and Cybernetics: [27] D. Dwibedi, J. Tompson, C. Lynch, and P. Sermanet, â€œLearning ac-
Systems,vol.52,no.9,pp.5546â€“5555,2021. tionablerepresentationsfromvisualobservations,â€in2018IEEE/RSJ
[8] Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, â€œMean international conference on intelligent robots and systems (IROS).
fieldmulti-agentreinforcementlearning,â€inInternationalconference IEEE,2018,pp.1577â€“1584.
onmachinelearning. PMLR,2018,pp.5571â€“5580. [28] L.Chen,Y.Wang,Y.Mo,Z.Miao,H.Wang,M.Feng,andS.Wang,
[9] J.Cook,N.Oreskes,P.T.Doran,W.R.Anderegg,B.Verheggen,E.W. â€œMultiagent path finding using deep reinforcement learning coupled
Maibach,J.S.Carlton,S.Lewandowsky,A.G.Skuce,S.A.Green, withhotsupervisioncontrastiveloss,â€IEEETransactionsonIndustrial
et al., â€œConsensus on consensus: a synthesis of consensus estimates Electronics,vol.70,no.7,pp.7032â€“7040,2022.
on human-caused global warming,â€ Environmental research letters, [29] Z. Xu, B. Zhang, D. Li, Z. Zhang, G. Zhou, H. Chen, and G. Fan,
vol.11,no.4,p.048002,2016. â€œConsensuslearningforcooperativemulti-agentreinforcementlearn-
[10] S. Suzuki, R. Adachi, S. Dunne, P. Bossaerts, and J. P. Oâ€™Doherty, ing,â€inProceedingsoftheAAAIConferenceonArtificialIntelligence,
â€œNeural mechanisms underlying human consensus decision-making,â€ vol.37,no.10,2023,pp.11726â€“11734.
Neuron,vol.86,no.2,pp.591â€“602,2015. [30] Y. Liu, Q. Yan, and A. Alahi, â€œSocial nce: Contrastive learning
[11] J.Qin,Q.Ma,Y.Shi,andL.Wang,â€œRecentadvancesinconsensusof of socially-aware motion representations,â€ in Proceedings of the
multi-agentsystems:Abriefsurvey,â€IEEETransactionsonIndustrial IEEE/CVF International Conference on Computer Vision, 2021, pp.
Electronics,vol.64,no.6,pp.4972â€“4983,2016. 15118â€“15129.
[12] P. H. Le-Khac, G. Healy, and A. F. Smeaton, â€œContrastive represen- [31] F.A.Oliehoek,C.Amato,etal.,Aconciseintroductiontodecentral-
tation learning: A framework and review,â€ Ieee Access, vol. 8, pp. izedPOMDPs. Springer,2016,vol.1.
193907â€“193934,2020. [32] M.Caron,H.Touvron,I.Misra,H.JeÂ´gou,J.Mairal,P.Bojanowski,
[13] A.AmirkhaniandA.H.Barshooi,â€œConsensusinmulti-agentsystems: and A. Joulin, â€œEmerging properties in self-supervised vision trans-
areview,â€ArtificialIntelligenceReview,vol.55,no.5,pp.3897â€“3935, formers,â€ in Proceedings of the IEEE/CVF international conference
2022. oncomputervision,2021,pp.9650â€“9660.[33] O.Michel,â€œCyberboticsltd.webotsâ„¢:professionalmobilerobotsim-
ulation,â€ International Journal of Advanced Robotic Systems, vol. 1,
no.1,p.5,2004.
[34] M. HuÂ¨ttenrauch, A. SË‡osË‡icÂ´, and G. Neumann, â€œDeep reinforcement
learningforswarmsystems,â€JournalofMachineLearningResearch,
vol.20,no.54,pp.1â€“31,2019.
[35] Y. Zhong, J. G. Kuba, X. Feng, S. Hu, J. Ji, and Y. Yang,
â€œHeterogeneous-agentreinforcementlearning,â€2023.