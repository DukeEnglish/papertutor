PACE: marrying generalization in PArameter-efficient
fine-tuning with Consistency rEgularization
YaoNi‚Ä† ShanZhang‚Ä† PiotrKoniusz‚àó,¬ß,‚Ä†
‚Ä†TheAustralianNationalUniversity ¬ßData61 CSIRO
‚Ä†firstname.lastname@anu.edu.au ¬ßpiotr.koniusz@data61.csiro.au
Abstract
Parameter-EfficientFine-Tuning(PEFT)effectivelyadaptspre-trainedvisiontrans-
formerstodownstreamtasks. However,theoptimizationfortasksperformance
oftencomesatthecostofgeneralizabilityinfine-tunedmodels. Toaddressthis
issue,wetheoreticallyconnectsmallerweightgradientnormsduringtrainingand
largerdatasetstotheimprovedmodelgeneralization. Motivatedbythisconnection,
weproposereducinggradientnormsforenhancedgeneralizationandaligningfine-
tunedmodelwiththepre-trainedcounterparttoretainknowledgefromlarge-scale
pre-trainingdata. Yet,naivealignmentdoesnotguaranteegradientreductionand
canpotentiallycausegradientexplosion,complicatingeffortstomanagegradients.
Toaddresssuchissues,weproposePACE,marryinggeneralizationofPArameter-
efficientfine-tuningwithConsistencyrEgularization. Weperturbfeatureslearned
from the adapter with the multiplicative noise and ensure the fine-tuned model
remains consistent for same sample under different perturbations. Theoretical
analysisshowsthatPACEnotonlyimplicitlyregularizesgradientsforenhanced
generalization,butalsoimplicitlyalignsthefine-tunedandpre-trainedmodelsto
retainknowledge.Experimentalevidencesupportsourtheories.PACEoutperforms
existingPEFTmethodsinfourvisualadaptationtasks: VTAB-1k,FGVC,few-shot
learninganddomainadaptation. CodewillbeavailableatMaxwellYaoNi/PACE.
1 Introduction
Visiontransformers[12],withtheself-attentionmechanism[2]capturinglong-rangedependenciesin
data,havebeensuccessfulinvariouscomputervisiontasks,includingimageclassification(ViT[12],
Swin[38]),multimodallearning(CLIP[49],BLIP[33]),imagesynthesis(StableDiffusion[51]),and
semanticsegmentation(SAM[27]). Thesuccessofvisiontransformerscanbelargelyattributed
to the availability of abundant data, such as ImageNet [8] and Laion5B [54], which has enabled
researcherstoscaleupthesemodelsbytrainingthemwithanenormousnumberofparameters.
Such huge models, with knowledge from large-scale pre-training [57], have become foundation
models that can be easily adapted to various downstream tasks through full fine-tuning or linear
probing[15],eliminatingtheneedfortask-specificmodeldesign[6]. However,fullfine-tuningis
storage-intensiveandinfeasibleformaintainingseparatemodelweightsasthenumberoftasksgrows,
whilelinearprobing,whichonlytrainsthelastheadlayer,yieldsinferioradaptationperformance.
Toovercometheselimitations,Parameter-EfficientFine-Tuning(PEFT)[18]fine-tunesonlyasmall
subsetofparameters,therebyreducingstoragerequirementswhilesurpassingtheperformanceof
full fine-tuning and linear probing. These advantages have popularized PEFT and inspired the
development of various PEFT methods for computer vision, which can be categorized into two
groups: thoseincreasinginferencecostandcost-efficientones. Thefirstgroupintroducesadditional
‚àóThecorrespondingauthor. ThispaperisacceptedbyNeurIPS2024asaspotlight. Thispreliminary
versionwillsoonbeextendedwiththeexperimentsandanalysesfromtherebuttal.
4202
peS
52
]GL.sc[
1v73171.9042:viXralearningbranches,suchasnon-linearadapters[19,6],orconcatenateslearnableparameterswith
inputtokens,e.g.,visualprompts[22,73,46],increasinginferencecost. Thesecondgroup,focuses
oncost-efficiencyinvolvinglower-rankadaptationinlinearlayers[5,20],oraffinetransformations
suchasSSF[36]andRepAdapters[39],whichcanbereparameterizedduringinferenceforefficiency.
Despite the superiority and efficiency of PEFT, prioritizing optimization for downstream tasks
compromisesthegeneralizabilityoffine-tunedmodels,yieldingsuboptimalperformance. Although
some analyses have been conducted on PEFT [57, 21, 14, 64, 34], they fail to fully explain the
generalizationofPEFT,leadingtoineffectivestrategiesforimprovinggeneralization.
ToaddressthisgapinunderstandinggeneralizationinPEFT,weestablishatheoreticalconnection
fromgeneralizationtheory: smallerweightgradientnormsandlargerdatavolumescontributeto
bettergeneralization. Motivatedbythis,weproposereducingweightgradientnormsandaligning
outputspaceofthefine-tunedmodelwiththepre-trainedonetoretainknowledgecapturedfromlarge
pre-trainingdata. Yet,theoreticalanalysesrevealthisnaivealignmentdosenotguaranteegradient
regularizationandcanevencausegradientexplosion,complicatingeffortsforgradientmanagement.
Toaddressthisissue,weproposeperturbingfeatureslearnedfromtheadapterwithmultiplicative
noiseandconstrainingthenetworkoutputtobeconsistentacrossdifferentperturbations.
WecallourmethodPACE.ItmarriesgeneralizationofPArameter-efficientfine-tuningwithConsis-
tencyrEgularization. Thenamereflectsourgoalofkeepingtheoutputbehaviorofthefine-tuned
modelinpacewithpre-trainedone. Despiteitssimplicity,theoreticalanalysisconfirmsthatPACE
notonlyimplicitlyregularizesweightgradientsforbettergeneralizationbutalsoimplicitlyalignsthe
fine-tunedmodelwiththepre-trainedcounterparttoretainknowledgefromlarge-scalepre-training
data. Experimental evidence supports our theories. PACE outperforms existing PEFT methods,
achievingsuperiorresultsacrossfouradaptationbenchmarks. Ourkeycontributionsare:
i. Weestablishatheoryconnectingsmallerweightgradientnormsandlargerdatasetswithen-
hancedgeneralization,motivatinggradientreductionandmodelalignmentforfine-tuning.
ii. WeproposePACE,asimpleyeteffectivemethodperturbingfeaturesfromadapterswithmulti-
plicativenoiseandconstrainingoutputoffine-tunedmodeltobeconsistentacrossperturbations.
iii. OurtheoreticalandempiricalevidenceconfirmsthatPACEimplicitlyregularizesgradientsand
alignsthefine-tunedmodelwiththepre-trainedone. PACEexcelson4visualadaptationtasks.
iv. Weprovidenoveltheoreticalexplanationsforhowgradientpenalizationandconsistencyregu-
larizationbenefitgeneralization,offeringfundamentalinsightsapplicableacrossdeeplearning.
2 Relatedwork
Parameter-Efficient Fine-Tuning (PEFT). LoRA [20] uses low-rank decomposition to reduce
parameters and treats adapters as side paths. SSF [36] proposes affine transformations on latent
features. FacT[24]decomposesandreassemblesparametermatricesinViT.Surgicalfine-tuning[30]
differentnetworkpartsresultsindifferentperformancefordifferentdatasets. FLoRA[66]aimsat
real-timeglobalservice.GLoRA[5]unifiescost-efficientPEFTmethods.NOAH[73]usesparameter
searchonneuralprompts. ARC[10]leveragescross-layerViTsimilarity,parameter-sharingadapter
andscalingfactorsforlowerfine-tuningcost. RLRR[11]incorporatesaresidualtermforflexibility
whilepreservingpre-trainedrepresentation. RepAdapter[39]reparameterizesadaptersforefficient
inference. Res-tuning[23]unbindstunersfromthebackboneformemoryefficiency. Zhaoetal.[74]
showimpressivefine-tuningresultsbytuningonlytheattentionlayernormalization. OFT[48]and
BOFT[37]proposeorthogonalfine-tuningtopreservehypersphereenergybetweenneurons.
Consistency Regularization. Fixmatch [55] applies consistency regularization over augmented
imagesforsemi-supervisedlearning. Openmatch[53]utilizesitonoutlierpredictionsforopen-set
semi-supervisedlearning. R-Drop[67]appliesittotransformers[61]withdropoutforNLPtasks.
CR[70]appliesitoveraugmentedrealandfakeimagesforGANtraining. CAGAN[44]enforces
consistency on discriminators with dropout for GAN training. Despite the empirical success of
consistencyregularizationdemonstratedbypreviousworks,theoreticalanalysisislacking. While
NICE[42]demonstratesthatconsistencyregularizationlowerslatentfeaturegradientsforstable
GANtraining,itfailstorevealreducedweightgradientforenhancedgeneralization. Ourstudygoes
beyondpriorworksbyprovidingatheoreticallinkbetweensmallerweightgradientsandimproved
generalization,effectivelymarryinggeneralizationofPEFTwithconsistencyregularization.
2Generalization of Fine-Tuning. Li et al. [32] constrain the fine-tuned model‚Äôs closeness to the
pre-trainedmodelinweightspace. Fuetal.[14]inducesparsityonPEFTmethodsforenhanced
generalization. Wangetal.[64]findsPEFTmethodsimprovegeneralizationonfine-tuninggraph
neuralnetwork. Recentworks,includingVioLET[65],PromptSRC[25],CoPrompt[52],propose
aligning the fine-tuned model with the pre-trained one for enhanced generalization or avoiding
forgetting,whichcanbeseenasournaivealignment. Additionally,L2SP[68],DELTA[35],andFTP
[58]aimtoretainpre-trainedknowledgebyaligningfinetunedmodelswithpre-trainedones,reducing
distanceinweightspace,featurespaceandusingprojectedgradientdescent,respectively. However,
theyfailtoprovideatheoreticalanalysisforthisalignment. Ourstudygoesbeyondunderstanding
generalizationofPEFTbydiscoveringthebenefitsofgradientregularizationandmodelalignment.
WeproposePACEtomatchbothrequirements,pavingacomprehensiveunderstandingforPEFT.
Gradient regularization. Previous studies have empirically shown that gradient regularization
improvesneuralnetworkperformance[60,75,41,43]. However,theyfailedtotheoreticallyestablish
the connection between smaller gradient norms and better generalization [13, 72, 4]. Our work
bridgesthisgapbyestablishingafundamentaltheorybetweenreducedgradientnormsandimproved
generalization,providingasolidfoundationforfutureresearchonenhancinggeneralization.
3 Approach
Webeginwithaunifiedperspectiveoncost-efficientPEFTbasedonGLoRA[5],linkinggeneraliza-
tionwithgradientsandlarge-scaledataandmotivatingthealignmentofthefine-tunedmodelwiththe
pre-trainedmodeltoleverageitsknowledge. Weidentifylimitationsofnaivealignmentingradient
regularizationandintroducePACE,whichimplicitlyenhancesgradientregularizationandmodel
alignment. Weconcludewiththeoreticaljustificationandefficientimplementations.
3.1 Aunifiedperspectiveoncost-efficientPEFTmethods
VisionTransformer(ViT)[12]extendsthesequentialmodelingcapabilitiesoftheTransformer[61],
originallydesignedfornaturallanguageprocessing,tocomputervisiontasks. Itachievesthisby
splittingimagesintonon-overlappingpatchesandextractingfeaturesusingLtransformerblocks.
Eachblockcontainsself-attentionandMLPmodules,primarilycomposedoflinearlayers. These
linearlayersunderpintheself-attentionmechanism,allowingViTtocapturelong-rangedependencies
inimagesandoutperformconvolutionalnetworkswhentrainedonlarge-scaledata.
TheViT,withmassiveparameterspretrainedonlarge-scaledata,servesasafoundationmodelthatcan
befine-tunedfordownstreamtasksusinglimiteddata. However,fullyfine-tuningallViTparameters
forvariousdownstreamtasksrequiressubstantialmemoryandcanleadtheforgettingofpretrained
knowledge. Toalleviatethiswithoutincreasinginferencecost,adapterswithlightweightparameters
areoftenpreferredforfine-tuning. Leth¬Ø ()beatransformationwithinthepre-trainedViT.Current
0
adapterscanbeunifiedasintroducingares¬∑ idualbranch‚àÜh¬Ø toformanewtransformationh¬Ø:
h¬Ø(a)=h¬Ø (a)+‚àÜh¬Ø(a). (1)
0
Here,aistheinputandh¬Ø canrepresentMLPmodules,asinAdapter[19]andAdaptFormer[6],or
0
linearlayersinself-attentionandMLPmodules,asin[20,5,9,28]. InSSF[36],h¬Ø istheidentity
0
mappingand‚àÜh¬Ø(a)=a (Œ≥ 1)+Œ≤withŒ≥ andŒ≤asaffinetransformationparameters.
‚äô ‚àí
Giventhatlinearlayersarekeycomponentsintransformer,tuningthemoffersaflexibleandeffective
waytoadaptmodelstodownstreamtasks. Thisworkfocusesonmethodsthattunethelinearlayer
without increasing inference cost. Let (W ,b ), (‚àÜW,‚àÜb), and (W,b) be the parameters of
0 0
pretrainedmodel,adapterandfinetunedmodel,respectively,whereW 0,‚àÜW,W Rdout√ódin and
b ,‚àÜb,b Rd ,finetuningalinearlayerinself-attentionorMLPmodulecanbefo‚àà rmedas:
0 0 ‚àà out
h(a)=Wa+b=(W +‚àÜW)a+(b +‚àÜb)
0 0
=h (a)+‚àÜh(a)=(W a+b )+(‚àÜWa+‚àÜb). (2)
0 0 0
BasedonGLoRA[5],cost-efficientPEFTmethodsforlinearlayersvaryintheformof‚àÜW,‚àÜb:
LoRA add: ‚àÜW =W dW u,‚àÜb=b lorawhereW
d
Rdout√ór,W
u
Rr√ódin,andristherank.
‚àà ‚àà
LoRA : ‚àÜW=W (W W ),‚àÜb=b b ,includingRepAdapter[39]viareparameterization.
mul 0 d u 0 lora
‚äô ‚äô
3VPT add: ‚àÜW iszero,‚àÜb=W 0P,withlearnableP Rdin√ó1aslayer-wisevisualprompt. Weuse
‚àà
VPT todifferentiatefromVPT[22],whichconcatenatesP withtokens,increasinginferencecost.
add
3.2 Generalizationofdeepneuralnetworks
Havingestablishedaunifiedperspectiveoncost-efficientPEFT,wenowmotivateourmethodfroma
perspectiveonimprovinggeneralizationofneuralnetworkstoenhanceperformanceonunseendata.
Consideranetworkf :=œï(g(x))withllayers,wheregisfeatureextractorandœïistheclassification
head. LetŒ∏ := (W(i),b(i)) l betheparametersetwithdimensiondand n := (x ,y ) n
be the training{ set of size n} di r= a1 wn i.i.d. from distribution D, which containD s infin{ ite di atai . } Ti= he1
followinglemmafrom[13]buildsarelationshipbetweentheempiricalandpopulationloss.
Lemma1 (Theorem1from[13])Let Dn(Œ∏)betheempiricallossfunctionoverf ontrainingset
nand D(Œ∏)bethepopulationloss.L ForanyœÅ>0,withhighprobabilityover n D,wehave
D L D ‚àº
(cid:16) Œ∏ 2 1(cid:17)
LD(Œ∏) ‚â§‚à•m œµ‚à•2a ‚â§x œÅLDn(Œ∏+œµ)+R ‚à• œÅ2‚à•2,
n
, (3)
whereR:(R +,R +) R +isastrictlyincreasingfunction(undersomeconditionson D(Œ∏)).
‚Üí L
Lemma1boundsthepopulationlossbytheempiricallosswithperturbedweights,indicatingthat
minimalempiricallossincreasefromsmallweightperturbationsimplieslowpopulationloss.
Byobservingthatthemaximumof LDn isachievedatœµ= ‚à•œÅ ‚àá‚àá Œ∏Œ∏ ‚à•2,where‚àá Œ∏ isthegradientof LDn
atŒ∏,andperformingaTaylorexpansionof aroundŒ∏,weformulatethefollowingtheorem:
Dn
L
Theorem1 Denote‚àá asthegradientandŒªH asthelargesteigenvaluesoftheHessianmatrix
Œ∏ max
H
Œ∏
of
Dn
atŒ∏. ForanyœÅ>0,withhighprobabilityovertrainingset n D,wehave
L D ‚àº
œÅ2 (cid:16) Œ∏ 2 1(cid:17)
LD(Œ∏) ‚â§LDn(Œ∏)+œÅ ‚à•‚àá
Œ∏
‚à•2+
2
ŒªH max+R ‚à• œÅ2‚à•2,
n
. (4)
Here,
higher-ordertermsfromtheTaylorexpansionareincorporatedintoR(cid:16) ‚à•Œ∏‚à•2
2,
1(cid:17)
, whichis
œÅ2 n
relatedtoweightsnormandinverselyrelatedtothetrainingdatasizen.
Theorem1(proofisin¬ßB.1)outlinesstrategiesforenhancinggeneralization. Theseinvolveregulariz-
ingweightnormsandthelargesteigenvaluesintheHessianmatrix,andcrucially,increasingdatasize
nandreducingtheweightgradientnorms. However,cautionisneededtoavoidexcessivereduction,
asthiscouldimpairnetwork‚Äôsrepresentationcapacity,yieldinghigherempiricalandpopulationloss.
3.3 Motivationandlimitationofaligningthefine-tunedmodelwiththepre-trainedmodel
Theorem1emphasizesthatlarge-scaledataandsmallergradientmagnitudesareessentialforbetter
generalizationinneuralnetworktraining. Therefore, aligningthefine-tunedmodelwiththepre-
trainedoneiscrucial,asitensuresretentionofknowledgedevelopedfromlarge-scaledata,preserving
generalizability.PEFTmethodsachievethisalignmentbylimitingthenumberoftrainableparameters,
restrictingmodel‚Äôscapacitytodeviatefromthepre-trainedoneandoftenoutperformingfullfine-
tuning. However, thetrainingobjectiveprioritizesdownstreamtaskperformance, compromising
alignment with pre-trained knowledge. While sparsity regularization [14] and weight decay on
adapter weights help, they do not ensure alignment, as even smaller weight changes can lead to
significantdivergenceinoutputspace. Therefore,weproposetoachievethealignmentbyreducing
theFP-distance(outputdistancebetweenfine-tunedandpre-trainedmodelsontrainingsamples):
n
1 (cid:88)
Dfp(Œ∏)= f(x ;Œ∏) f(x ;Œ∏ ) 2, Œ∏ =Œ∏ +‚àÜŒ∏, (5)
n ‚à• i ‚àí i 0 ‚à•2 0
i=1
whereŒ∏,Œ∏ ,‚àÜŒ∏ Rdareparametersforthefine-tunedmodel,pre-trainedmodelandtheadapter.
0
‚àà
WhilereducingFP-distancekeepsthefine-tunedmodelclosetothepre-trainedmodel,thuspreserving
itsknowledge,itdoesnotensurereducedgradientmagnitudes,leadingtosuboptimalgeneralization.
Tounderstandthegradient-relatedlimitationsinthisalignment,weassume‚àÜŒ∏issmallenoughfora
Taylorexpansionapproximation. Followingstandardpractices[13,71,1],weperformtheexpansion
4uptothesecond-orderterms. Simplifyingourapproach,weanalyzeaone-dimensionaloutputfora
singlei.i.d.sample,whichleadsustothefollowingproposition.
Proposition1 Assuming‚àÜŒ∏issmall,denotef(Œ∏) Rastheone-dimensionaloutputforx,with
‚àà
‚àáandH asitsgradientandHessianatŒ∏. FP-distanceoverxcanbedecomposedasfollows:
[f(Œ∏) f(Œ∏ )]2 =[f(Œ∏) f(Œ∏ ‚àÜŒ∏)]2 (cid:2) f(Œ∏) [f(Œ∏) ‚àÜŒ∏T‚àá+ 1 ‚àÜŒ∏TH‚àÜŒ∏](cid:3)2
0
‚àí ‚àí ‚àí ‚âà ‚àí ‚àí 2
1
[‚àÜŒ∏T‚àá ‚àÜŒ∏TH‚àÜŒ∏]2. (6)
‚âà ‚àí 2
Prop. 1 establishes the relationship between weight gradients, adapter weights, and FP-distance.
However, itremainsunclearifitregulatesgradients. OurexperimentsshowthatminimizingFP-
distancecansometimesincreasegradientmagnitude,complicatingeffortsformanaginggradient.
3.4 Consistencyregularization
Toachievebettergeneralizationbybothregularizinggradientsandaligningthefine-tunedmodelwith
thepre-trinedmodel,weproposeaconsistencyregularizationlossforf,encouraginginvarianceoff
tothesameinputundervaryingmultiplicativenoiseperturbationsontheadapterweights,asfollows:
n
1 (cid:88)
Dpace(Œ∏)= E f(x ;Œ∏ +z ‚àÜŒ∏) f(x ;Œ∏ +z ‚àÜŒ∏) 2, (7)
n z1,z2‚à• i 0 1 ‚äô ‚àí i 0 2 ‚äô ‚à•2
i=1
wherez ,z (1,œÉ2I)isthemultiplicativenoiseappliedonadapterweight. Tounderstandthe
1 2
‚àºN
generalizationbenefitsinthisconsistencyregularization,wesimplifytheanalysisbyfocusingon
one-dimensionaloutputforasinglesample,resultinginthefollowingtheorem.
Theorem2 UsingnotationsfromProp. 1,letf(Œ∏ +z ‚àÜŒ∏) Rbetheone-dimensionaloutput
0
‚äô ‚àà
forx. Define‚àÜŒ∏ asj-thelementin‚àÜŒ∏, asthej-thelementin‚àáandH asthe(j,k)-entryin
j j jk
‚àá
H. Withz ,z (1,œÉ2I),theconsistencylossoverxcanbeapproximatedas:
1 2
‚àºN
E [f(Œ∏ +z ‚àÜŒ∏) f(Œ∏ +z ‚àÜŒ∏)]2
z1,z2 0 1
‚äô ‚àí
0 2
‚äô
2œÉ2(cid:80) ‚àÜŒ∏2 2+œÉ4(cid:80) ‚àÜŒ∏2‚àÜŒ∏2H2 =2œÉ2 ‚àÜŒ∏ ‚àá 2+œÉ4 (‚àÜŒ∏‚àÜŒ∏T) H 2. (8)
‚âà j j‚àáj j,k k j jk ‚à• ‚äô ‚à•2 ‚à• ‚äô ‚à•F
Theorem 2 (Proof is in ¬ßB.2) shows that the consistency regularization essentially penalizes the
first-andsecond-ordergradientsoff atŒ∏,withtheregularizationstrengthcontrolledbythenoise
varianceœÉ2andadaptivelyinfluencedbythemagnitudeofelementsintheadapterweight‚àÜŒ∏. Thus,
minimizingtheconsistencylossimplicitlyregularizesthegradients,improvinggeneralization.
WiththeFP-distanceinProp. 1andconsistencylossinTheorem2,weestablishtheirrelationshipas:
Theorem3 WithdasthedimensionofŒ∏,Eq. 6canbeupperboundedas:
1
[‚àÜŒ∏T‚àá ‚àÜŒ∏TH‚àÜŒ∏]2 2d ‚àÜŒ∏ ‚àá 2+d2 (‚àÜŒ∏‚àÜŒ∏T) H 2. (9)
‚àí 2 ‚â§ ‚à• ‚äô ‚à•2 ‚à• ‚äô ‚à•F
Theorem3(proofisinB.3)establishestherelationshipbetweenEq. 6andEq. 8,showingthatEq. 6
isupper-boundedbytermsinvolving ‚àÜŒ∏ ‚àá 2and (‚àÜŒ∏‚àÜŒ∏T) H 2 whichappearinEq. 8.
‚à• ‚äô ‚à•2 ‚à• ‚äô ‚à•F
ReducingthesetermsresultsinadecreaseinEq. 6. Thusminimizingtheconsistencylossimplicitly
alignsthefine-tunedwithpre-trainedmodels,preservingknowledgeinpre-trainedmodel.
3.5 EfficientimplementationofPACE
Providing different weight perturbations for each input in a mini-batch increases memory and
computationaldemands. Toavoidthisinefficiency,weperturbfeatureoutputsfromtheadapter‚àÜh,
effectivelysimulatingperturbationthatsharesnoiseacrosseachrowintheweight‚àÜW. Oursimple
pipelineisillustratedinFigure1. ConsiderX RB√óT√ódin asabatchofdatawhereB,T bethe
‚àà
5Transformerblockwithadapterperturbedbynoise Consistencyregularizationbetweentwooutputsofx
Transformer
MLP ‚àÜ‚Ñé(‚ãÖ) Block √óùêø head loss
Norm ùëì !(ùíô)
‚Ñé(‚ãÖ)
# ‚Ñé(‚ãÖ)=‚Ñé #(‚ãÖ)+ùíõ‚äôŒî‚Ñé(‚ãÖ) ùíô shareweights ||ùëì ùíô ‚àíùëì(ùíô)||"
Multi-Head Œîùëæ ùëæ whereùíõ‚àºùí©(ùüè,ùúé"ùë∞) non-sharednoises ! "
Attention #
‚àÜùíÉ ùíÉ # ùëì "(ùíô)
Transformer
Norm head
Block
√óùêø
Adapter‚àÜ‚Ñéin
Transformer linearlayer‚Ñé
Block
Figure 1: Our pipeline. Adapter ‚àÜh and h from pre-trained model form the linear layer h of
0
Multi-HeadAttentionandMLPinfine-tunedmodel. Weperturb‚àÜhwithmultiplicativenoiseand
ensurethenetworkremainsconsistenttosameinputsundervaryingperturbations.
batchandtokensizes. Thecalculationforthelinearlayerofthefine-tunedmodel,whichutilizes
pre-trainedweightsW ,b andadapterweights‚àÜW,‚àÜb,processesanoutputsizeofd as:
0 0 out
h (X)=W X+b ; ‚àÜh(X)=‚àÜWX+‚àÜb, (10)
0 0 0
h(X)=h (X)+Z ‚àÜh(X). (11)
0
‚äô
Here istheelement-wisemultiplicationafterexpandingtheleftmatrixZ RB√ódout (1,œÉ2I)
‚äô ‚àà ‚àºN
intoB T d wheretokenswithinthesameexamplesharesamenoise. Motivatedby[31],theœÉ
out
√ó √ó
decreaseslinearlyasblockdepthincreases. Letf andf betwonetworkssharesameweightsbut
1 2
non-sharenoises. ThelossfunctionforPACEis:
n
1 (cid:88)
PACE = ‚Ñì(f (x ),y )+Œª f (x ) f (x ) 2, (12)
L n 1 i i ‚à• 1 i ‚àí 2 i ‚à•2
i=1
where‚ÑìistheclassificationlossandŒªisahyperparametercontrollingregularizationstrength. During
inference,noiseandregularizationareommitted,‚àÜW,‚àÜbareintegratedwithW ,b forefficiency:
0 0
W =W +‚àÜW; b=b +‚àÜb; h(X)=WX+b. (13)
0 0
4 Experiments
WecombineLoRA andVPT toformastrongbaselineLoRA +VPT ,outperformingother
mul add mul add
combinationsinmostcases. Weevaluateourmethodacrossfourvisualclassificationadaptation
tasks: VTAB-1K[69],few-shotlearning[24],FGVC[22]anddomainadaptation[73].
Datasetsandevluations. VTAB-1Kcomprises19datasetsclusteredinto(i)Naturalimages,(ii)
Specializeddatasets(remotesensing,medical)and(iii)Structureddatasets(scenestructure)domains.
Eachdatasethas1Ktrainingexamples. Following[69,22],weusetheprovided800-200trainsplit
forhyperparameterselection,evaluateusingthefulltrainingsetandreportaverageaccuracyacross
threetrails. Few-shotlearninginvolves5fine-graineddatasets: FGVC-Aircraft[40],Food101[3],
OxfordFlowers102 [45], OxfordPets [47] and StanfordCars [29]. Following [24], we evaluate 1,
2,4,8and16shots,trainontheprovidedtrainingset,tunehyperparametersusingvalidationand
reportaveragetestaccuracyoverthreerandomseeds. FGVCincludes5fine-graineddatasets: CUB-
200-2011[62],NABirds[59],OxfordFlowers[45],StanfordDogs[7]andStanfordCars[29]. We
follow[22]tousevalidationsetforhyperparameterandreporttestresults. Fordomainadaptation,
following[73,5],wetrainonImageNet[8]witha16-shotsetting,usethevalidationsplitby[73]
forhyperparameterselectionandreporttheresultsontheofficialvalidationsetand4out-of-domain
datasets: ImageNet-Sketch[63],ImageNet-V2[50],ImageNet-A[17]andImageNet-R[16].
Pre-trainedbackbones. Weexperimentwithtwovisiontransformers,VisionTransforms(ViT-B/16)
[12]andSwinTransformer(Swin-B)[38]. Thesetwoarepre-trainedonImageNet-21K[8]. Wetest
aViT-B-Laion-IN12Kmodel,pre-trainedonLaion-2B[54]andfine-tunedonImageNet-12K[8].
Implementationdetails. Wefollow[22]forimageprocessing. 224 224resizingforVTAB-1K;
√ó
randomflipsandcropsto224 224forFGVCandfew-shotlearning;strongeraugmentationfor
√ó
domainadaptationtask,following[12,73,36]. WeusetheAdamoptimizer[26]withcosinelearning
6Table1: ResultsonVTAB-1KwithViT-B/16. MeanAcc. istheaverageofgroupmeanvalues.
Natural Specialized Structured
Method
Full 68.9 87.7 64.3 97.3 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9
Linear 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6
VPT-Deep 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0
Adapter 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9
AdaptFormer 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7
LoRA 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.5
NOAH 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 74.2
RepAdapter 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 76.1
RLRR 75.6 92.4 72.9 99.3 91.5 89.8 57.0 86.8 95.2 85.3 75.9 79.7 64.2 53.9 82.1 83.9 53.7 33.4 43.6 76.7
GLoRA 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0
Baseline 74.9 93.3 72.0 99.4 91.0 91.5 54.8 83.2 95.7 86.9 74.2 83.0 70.5 51.9 81.4 77.9 51.7 33.6 44.4 76.4
+PACE 79.0 94.2 73.6 99.4 92.4 93.7 58.0 87.4 96.4 89.3 77.1 84.9 70.9 54.9 84.3 84.7 57.3 39.3 44.8 79.0
Table2: ClassificationaccuracyonFew-shotlearningwithViT-B/16pretrainedonImageNet-21K.
Shot FGVCAircraft Food101 Flowers102
Method 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
LoRAadd 10.4 15.2 27.2 41.7 59.2 33.9 51.9 59.3 66.0 71.3 93.3 96.4 98.0 98.6 98.7
+PACE 10.7 16.3 28.2 42.1 61.0 40.6 55.9 63.8 70.3 75.2 95.0 98.0 98.9 99.5 99.6
VPTadd 11.2 15.1 23.7 36.3 51.5 34.3 56.6 64.8 71.7 75.4 94.3 97.6 98.2 99.3 99.6
+PACE 11.6 16.2 24.0 37.0 52.4 39.9 57.2 66.7 72.4 76.1 95.3 97.8 98.6 99.4 99.6
LoRAadd+VPTadd 10.5 15.6 28.4 44.8 61.8 35.4 54.3 64.8 72.1 76.4 90.4 97.3 98.4 99.4 99.5
+PACE 12.3 16.8 29.9 45.7 62.5 39.3 57.2 66.7 73.4 77.8 93.4 98.1 99.1 99.5 99.7
OxfordPets StanfordCars Average
LoRAadd 73.2 83.1 87.5 89.2 91.1 8.7 15.3 30.2 55.3 74.5 43.9 52.3 60.4 70.1 78.9
+PACE 75.3 85.0 90.7 90.8 92.4 9.4 16.0 30.9 56.1 75.9 46.2 54.2 62.5 71.7 80.8
VPTadd 75.9 85.6 90.3 90.6 92.3 9.3 15.0 27.8 46.6 65.1 45.0 53.9 60.9 68.9 76.7
+PACE 78.2 87.4 90.3 91.1 92.3 9.9 15.4 27.9 47.0 65.9 46.9 54.8 61.5 69.3 77.2
LoRAadd+VPTadd 69.9 84.1 89.1 91.3 91.9 9.0 16.3 32.7 59.0 76.4 43.0 53.5 62.6 73.2 81.2
+PACE 76.5 88.0 90.3 91.4 92.4 9.7 16.4 33.7 59.8 77.3 46.2 55.3 63.9 73.9 81.9
ratedecayandalinearwarm-upforthefirst10epochs. Modelsarefine-tunedfor300epochson
VTAB-1Kand100epochsonFGVC,few-shotlearninganddomainadaptationtasks,withabatch
sizeof64. AllexperimentswereconductedonanNVIDIAH100GPUwith96GBmemory.
Baseline. Foreachdataset,weidentifiedthebettermethod(LoRA +VPT orLoRA )andtuned
mul add add
therank,learningrate,andweightdecaytoformastrongbaseline. Thedetailedbaselinesettings
foreachtaskandthenumberoftrainableparameters,areprovidedin¬ßD,whereLoRA +VPT
mul add
generallyoutperformedothervariants. BuildingonstrongbaselineLoRA +VPT ,weusegrid
mul add
searchforourhyper-parametersŒªandœÉ,followingstrategiesfrompreviousstudies[22,36,20].
4.1 ComparisonwiththeStateoftheArts
Results on VTAB-1K. Table 1 presents the results comparing PACE with recent state-of-the-art
PEFTmethods. PACEimprovesthestrongbaselineby2.6%accuracy,surpassingthepreviousSOTA
GLoRA[5]by1%,whichusestwostageslearningforneuralparametersearch.
Results on Few-shot Learning. Table 2 compares performance w/ and w/o our PACE. PACE
improvesLoRA ,VPT ,LoRA +VPT ,withLoRA +VPT +PACEperformingbestin
add add mul add mul add
mostcases. PACEyieldsnotableimprovement,especiallywhenthenumberofshotissmall.
ResultsonFGVC.Table3showsthatPACEimprovesthestrongLoRA +VPT by0.7%,outper-
mul add
formingSSF[36],ARC[10]andRLRR[11]thatusestronglypre-trainedViTwithaugmentations.
7
001rafiC 101hcetlaC
DTD
201srewolF
steP
NHVS 793nuS noylemaC TASoruE 54csiseR yhtaponiteR tnuoC-rvelC tsiD-rvelC baLMD tsiD-ITTIK coL-rpSd irO-rpSd
mizA-BRONs
elE-BROsN .ccAnaeMTable5: ClassificationresultsondomainadaptationandCIFAR-100inVTAB-1Kbaseddifferent
pretrainedmodels. Src. isshortfor‚Äòsource‚ÄôinTable4.
ViT-B(ImageNet-21K) ViT-B(Laion2B-ImageNet-12K) Swin-B(ImageNet-21K)
Method CIFAR ImageNet-1K CIFAR ImageNet-1K CIFAR ImageNet-1K
-100 Src. -S -V -A -R -100 Src. -S -V -A -R -100 Src. -S -V -A -R
Full 51.6 63.918.552.5 3.2 21.2 51.2 66.029.056.1 8.1 27.9 65.6 71.727.061.110.824.4
Linear 63.4 67.914.460.8 9.4 25.6 61.9 79.243.269.523.440.9 65.0 78.836.768.823.235.9
LoRAadd 71.2 73.827.164.813.625.0 71.3 77.539.867.820.435.6 74.3 76.330.765.716.828.9
VPTadd 73.6 74.327.165.911.526.7 71.8 78.440.468.722.438.4 72.7 76.230.666.217.629.1
LoRAmul 73.4 78.131.268.313.432.7 73.2 78.641.968.822.637.8 73.9 76.130.865.718.128.9
LoRAadd+VPTadd 70.3 76.828.766.613.729.9 71.8 78.041.468.320.636.9 74.5 76.330.765.716.828.9
LoRAmul+VPTadd 74.9 78.330.668.514.132.5 73.8 78.341.568.621.638.2 74.6 76.631.266.518.529.4
+PACE 79.0 79.031.869.416.335.2 78.0 80.145.871.224.643.6 78.9 79.639.270.125.238.0
Table3: ResultsonFGVCwithViT-B/16. Table4: ResultsondomainadaptationwithViT-
*denotesusingaugmentedViTbyAugReg[56]. B/16pretrainedonImageNet-21K.
CUB NA- Oxford Stan. Stan.Mean Source Target Mean
Method Method
-2011BirdsFlowersDogs Cars Acc. ImageNet -Sketch -V2 -A -R Acc.
Full 87.3 82.7 98.8 89.4 84.5 85.9 Full 63.9 18.5 52.5 3.2 21.2 31.8
Linear 85.3 75.9 97.9 86.2 51.3 79.3 Linear 67.9 14.4 60.8 9.4 25.6 35.6
VPT 88.5 84.2 99.0 90.2 83.6 89.1 Adapter 70.5 16.4 59.1 5.5 22.1 34.7
LoRA 88.3 85.6 99.2 91.0 83.2 89.5 VPT 70.5 18.3 58.0 4.6 23.2 34.7
SSF* 89.5 85.7 99.6 89.6 89.2 90.7 LoRA 70.8 20.0 59.3 6.9 23.3 36.0
ARC* 89.3 85.7 99.7 89.1 89.5 90.7 NOAH 71.5 24.8 66.111.928.5 40.5
RLRR* 89.8 85.3 99.6 90.0 90.4 91.0 GLoRA 78.3 30.6 67.513.331.0 44.1
LoRAmul+VPTadd 88.9 87.1 99.4 91.2 87.5 90.8 LoRAmul+VPTadd 78.3 30.6 68.514.132.5 44.8
+PACE 89.8 87.3 99.5 92.2 88.8 91.5 +PACE 79.0 31.8 69.416.335.2 46.3
Resultsondomainadaptation. Table4comparesPACEwithothers. LoRA +VPT outperforms
mul add
GLoRA[5]whichreliesonparametersearch. Meanwhile,PACEimprovesLoRA +VPT by
mul add
1.5%,outperformingotherPEFTmethods,demonstratingsuperiorperformanceondomainadaptation.
Generalizetootherbackbones. WeevaluatePACEonCIFAR-100(VTAB-1K)anddomainadapta-
tionusingSwin-B[38]pretrainedonImageNet-21KandViT-B(pretrainedonLaion2B,thenfine-
tunedonImageNet-12K).Table5showsPACEeffectivelyoutperformsbaselineLoRA +VPT
mul add
andotherPEFTmethodsacrossallbackbones,demonstratingitseffectivegeneralizability.
4.2 Analyses
To verify our theories, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16 and
Camelyon(VTAB-1K)onSwin-B.Figure2&3plotthegradientnormandFP-distance(Eq. 5)and
thetrain&validationaccuracyduringtrainingforbaselineLoRA +VPT andPACEonvalidation
mul add
set. Figures2a&3ashowthatPACEhasasmallergradientnormthanbaseline,verifyingTheorem
2thatPACEcanimplicitlylowertheweightgradientnormforbettergeneralization. Figures2b&
3bdemonstratethatPACEmaintainsalowerFP-distancethanthebaseline,verifyingTheorem3
thatPACEcanimplicitlyalignthefine-tunedmodelwithpre-trainedmodel,retainingknowledge
developedfromlarge-scalepre-training. Owingtotheadvantagesofthegradientregularizationand
modelalignment,PACEshortenstheperformancegapbetweenseenandunseendata,yieldinghigher
classificationaccuracyontheunseenvalidationset,asshowninFigures2c&3c.
Toclarifywhynaivealignmentisproblematic,wevarytheregularizationstrengthŒªoverawide
range(1e-3to5e4)forbothFine-tunedPre-trainedmodelAlignment(FPA)byminimizingDfpin
Eq. 5)andPACE.Figure4plotstheaveragedgradientnormovertraining(seealsoFigures7&8
formorevisualizations). PACErobustlylowersgradientnormswithlargerŒª,whileFPAexhibits
unpredictablebehavior,evencausinggradientexplosion. ThisverifiesProp. 1thatminimizingDfpis
problematicforgradientregularization,complicatinggradientmanagement.
4.3 Ablationstudies
WeablatePACEbasedonthebaselineLoRA +VPT onCIFAR-100(VTAB-1K)andImageNet-
mul add
1KindomainadaptionasshowninTable6.TheablationsincludeNoise(baselinew/noiseperturbing
adapter),PACE (replacingmultiplicativenoisewithadditivenoise),PACE (perturbinghinstead
add h
of‚àÜhinEq. 11),PACE (replacingGaussiannoisewithdropoutnoise),PACE (alltransformer
drop œÉ=
8‚àÇf Dfp Acc
‚à•‚àÇŒ∏‚à•2
12e3 140 100
Baseline Baseline trainacc Baseline
9e3 100 90
+PACE +PACE valacc +PACE
6e3 60 80
3e3 20 70
epoch= 100 200 300 epoch= 100 200 300 epoch= 100 200 300
(a)GradientNorm. (b)FP-Distance (c)Trainandvalidationaccuracy.
Figure2: AnalysisforPACE.(a)gradientnorm,(b)FP-Distanceand(c)train&valaccuracy,are
evaluatedonvalidationsetofCIFAR-100(VTAB-1K)withbaselineLoRA +VPT onViT-B/16.
mul add
‚àÇf Dfp Acc
‚à•‚àÇŒ∏‚à•2
8e3 100 1.00
Baseline trainacc Baseline
6e3 70 0.95
+PACE valacc +PACE
4e3 Baseline 40 0.90
+PACE
2e3 10 0.85
epoch= 100 200 300 epoch= 100 200 300 epoch= 100 200 300
(a)GradientNorm. (b)FP-distance (c)Trainandvalidationaccuracy.
Figure3: AnalysisforPACE.(a)gradientnorm,(b)FP-Distanceand(c)train&valaccuracy,are
evaluatedonvalidationsetofCamelyon(VTAB-1K)withbaselineLoRA +VPT onSwin-B.
mul add
blocks share the same œÉ), PACE (œÉ increases linearly with depth), FPA (fine-tuned and pre-
œÉ‚Üë
trinedalignmentbyminimizingEq. 5),SAM(sharpness-awareminimization[13]),GP(gradient
penalization),‚Ñì (sparsityregularization). Wegrid-searchhyperparametersandreportthebestresults.
1
Table6presentstheresultsforallvariants. PACEimprovesoverNoise,whichitselfisbetterthan
baseline,justifyingouradapterperturbationandconsistencyregularization. PACE performsworse
add
thanPACE,showingthesuperiorityofmultiplicativenoise.AlthoughPACE canimplicitlyregularize
h
gradients,itunderperformsPACE,verifyingtheadvantagesofperturbingadaptertoimplicitlyalign
models. PACE isworsethanPACE,indicatingdropoutnoiseissuboptimal. PACE andPACE
drop œÉ= œÉ‚Üë
performsworse,justifyingourdesignoflinearlydecreasingœÉ. FPA,SAMandGP,whicheitheronly
alignmodelsoronlyregularizegradients,areoutperformedbyPACE.DespitecombiningFPA+GP,it
stillunderperformsours,suggestingineffectivecombination. ‚Ñì obtainsworseresultsthanPACE,
1
verifyingineffectivenessofsparseregularizationforimprovinggeneralization. PACEregularizes
gradientsforbettergeneralizationandalignmodelstoretainknowledge,surpassingallothervariants.
WefurtherevaluateapplyingPACEacrossmultipleM networksduringtrainingorapplyingitlazily
ateveryN steps. Figure5presentstheresults,showingthatapplyingPACEamongtwonetworksat
everytrainingstepyieldsthebestresults. However,lazyregularizationappliedeveryfewstepscan
stillprovidereasonableresultswhilesavingcomputationtime.
WetestthesensitivityofhyperparameterŒªandœÉintroducedinourPACEonOxfordPetsforfew-shot
learningaccross1,2,4,8shots. TheresultspresentedinFigure6demonstratethatwithlessdata,
largerŒªandœÉarefavoured,verifyingthattheeffectivenessofPACEinimprovinggeneralization.
5 Conclusions
WehaveintroducedPACE,anovelandeffectivemethodthatcombinesgeneralizationofPArameter-
efficientfine-tuningwithConsistencyrEgularization. Throughrigoroustheoreticalanalyses,wehave
shownPACEreducesweightgradientforimprovedgeneralizationandalignsthefine-tunedmodel
withthepre-trainedmodelforretainingpre-trainingknowledge. Ourexperimentalresultssupport
thetheoreticalanalyses,justifyingthegeneralizationadvantagesofPACEoverotherPEFTmethods.
Withitsdualadvantages,PACEconsistentlyoutperformsothervariantsacrossdifferentbackbones,
firmlyestablishingPACEasapowerfulsolutionforenhancinggeneralizationforPEFTmethods.
Limitationsandborderimpactsarediscussedin¬ßA.
9‚àÇf 79.0
‚à•‚àÇŒ∏‚à•2 Baseline
78.5
+FPA
78.0
2e4 +PACE
M=2 3 4 5 6 7 8
79.0
1e4
78.5
78.0
Œª=1e-35e-30.010.05 0.1 0.5 1 5 10 50 100 500 1e3 5e3 1e4 5e4
N=1 2 4 6 8 10 12
Figure4: Gradientnormsofmodelsacrosswiderangeofregu- Figure5: Ablationresultsforap-
larizationstrengthsŒªonCIFAR-100(VTAB-1K)w/ViT-B/16. plyingPACEamongM networks
Lineandshadowrepresentmeanandstdacrosstrainingepochs. andateveryN steps.
Method CIFAR ImageNet-1K Œª 1-Shot Œª 2-Shot
-100 Source-Sketch -V2 -A -R 0.02 0.02
75 87
LoRAmul+VPTadd 74.9 78.3 30.6 68.514.132.5 0.05 0.05
+Noise 77.4 78.3 31.3 68.614.333.0 0.1 72 0.1 86
+PACE 79.0 79.0 31.8 69.416.335.2 0.2 69 0.2 85
+ +P PA AC CE Ea md ed rge 7 75 5. .7 9 7 78 8. .3 4 3 31 1. .2 2 6 68 8. .7 11 13 3. .7 83 32 2. .7 6 0 œÉ.5 = 0.1 0.2 0.5 1.0 1.5 66 0 œÉ.5 = 0.1 0.2 0.5 1.0 1.5 84
+PACEdrop 78.3 78.9 31.2 68.916.034.6
4-Shot 8-Shot
+ +P PA AC CE EœÉ œÉ= ‚Üë 7 77 7. .9 3 7 78 8. .8 7 3 31 1. .6 3 6 68 8. .3 91 16 4. .6 033 34 .. 67 0.0Œª 2 90 0.0Œª 2 91
+FPA 76.6 78.8 31.2 68.614.733.5 0.05 0.05
+SAM 75.4 78.4 31.4 68.513.832.9 0.1 89 0.1
+GP 75.8 78.3 31.7 68.414.232.1 0.2 0.2
+FPA+GP 74.9 78.1 31.5 68.113.532.6 0.5 0.5
+‚Ñì1 75.2 78.2 30.6 68.613.732.8 œÉ= 0.1 0.2 0.5 1.0 1.5 œÉ= 0.1 0.2 0.5 1.0 1.5 90
Table6: Accuracyresultsondomainadaptation Figure 6: Results for varied Œª and œÉ as well as
andVTAB-1Kbaseddifferentpretrainedmodels. shotondatasetOxfordPetsinfew-shotlearning.
Acknowledgements
We thank Moyang Liu, Melody Ip, Chenyi Du, and Yinuo Xu for their valuable discussions and
support. YNisfundedbyChinaScholarshipCouncil,whilePKbyCSIRO‚ÄôsScienceDigital.
References
[1] GuillaumeAlainandYoshuaBengio. Whatregularizedauto-encoderslearnfromthedata-generating
distribution. JMLR,15(110):3743‚Äì3773,2014. 4
[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearning
toalignandtranslate. arXivpreprintarXiv:1409.0473,2014. 1
[3] LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101‚Äìminingdiscriminativecomponents
withrandomforests. InComputerVision‚ÄìECCV2014:13thEuropeanConference,Zurich,Switzerland,
September6-12,2014,Proceedings,PartVI13,pages446‚Äì461.Springer,2014. 6
[4] JunbumCha,SanghyukChun,KyungjaeLee,Han-CheolCho,SeunghyunPark,YunsungLee,andSungrae
Park. Swad:Domaingeneralizationbyseekingflatminima. AdvancesinNeuralInformationProcessing
Systems,34:22405‚Äì22418,2021. 3
[5] ArnavChavan,ZhuangLiu,DeepakGupta,EricXing,andZhiqiangShen. One-for-all:Generalizedlora
forparameter-efficientfine-tuning. arXivpreprintarXiv:2306.07967,2023. 2,3,6,7,8,17
[6] ShoufaChen,ChongjianGe,ZhanTong,JiangliuWang,YibingSong,JueWang,andPingLuo. Adapt-
former: Adaptingvisiontransformersforscalablevisualrecognition. AdvancesinNeuralInformation
ProcessingSystems,35:16664‚Äì16678,2022. 1,2,3
[7] EDataset. Noveldatasetsforfine-grainedimagecategorization. InFirstWorkshoponFineGrainedVisual
Categorization,CVPR.Citeseer.Citeseer.Citeseer,2011. 6
[8] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248‚Äì255.
Ieee,2009. 1,6
[9] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora: Efficientfinetuningof
quantizedllms. AdvancesinNeuralInformationProcessingSystems,36,2024. 3
[10] WeiDong,DaweiYan,ZhijunLin,andPengWang. Efficientadaptationoflargevisiontransformervia
adapterre-composing. AdvancesinNeuralInformationProcessingSystems,36,2024. 2,7
10[11] Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, and Yang
Yang. Low-rankrescaledvisiontransformerfine-tuning: Aresidualdesignapproach. arXivpreprint
arXiv:2403.19067,2024. 2,7
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. In
InternationalConferenceonLearningRepresentations,2021. 1,3,6
[13] PierreForet,ArielKleiner,HosseinMobahi,andBehnamNeyshabur. Sharpness-awareminimizationfor
efficientlyimprovinggeneralization. InInternationalConferenceonLearningRepresentations,2021. 3,4,
9
[14] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the
effectivenessofparameter-efficientfine-tuning. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume37,pages12799‚Äì12807,2023. 2,3,4
[15] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick. Momentumcontrastforunsupervised
visualrepresentationlearning.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages9729‚Äì9738,2020. 1
[16] DanHendrycks,StevenBasart,NormanMu,SauravKadavath,FrankWang,EvanDorundo,RahulDesai,
Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of
out-of-distributiongeneralization. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages8340‚Äì8349,2021. 6
[17] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
15262‚Äì15271,2021. 6
[18] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGes-
mundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternational
conferenceonmachinelearning,pages2790‚Äì2799.PMLR,2019. 1
[19] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGes-
mundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternational
conferenceonmachinelearning,pages2790‚Äì2799.PMLR,2019. 2,3
[20] EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConferenceon
LearningRepresentations,2022. 2,3,7
[21] ShengdingHu,ZhenZhang,NingDing,YadaoWang,YashengWang,ZhiyuanLiu,andMaosongSun.
Sparsestructuresearchfordeltatuning.InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyun
Cho,editors,AdvancesinNeuralInformationProcessingSystems,2022. 2
[22] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and
Ser-NamLim. Visualprompttuning. InEuropeanConferenceonComputerVision, pages709‚Äì727.
Springer,2022. 2,4,6,7,17
[23] ZeyinziJiang,ChaojieMao,ZiyuanHuang,AoMa,YiliangLv,YujunShen,DeliZhao,andJingrenZhou.
Res-tuning: Aflexibleandefficienttuningparadigmviaunbindingtunerfrombackbone. Advancesin
NeuralInformationProcessingSystems,36,2024. 2
[24] ShiboJieandZhi-HongDeng. Fact:Factor-tuningforlightweightadaptationonvisiontransformer. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages1060‚Äì1068,2023. 2,6
[25] MuhammadUzairKhattak,SyedTalalWasim,MuzammalNaseer,SalmanKhan,Ming-HsuanYang,and
FahadShahbazKhan. Self-regulatingprompts: Foundationalmodeladaptationwithoutforgetting. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages15190‚Äì15200,2023. 3
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014. 6
[27] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4015‚Äì4026,2023. 1
[28] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix
adaptation. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. 3
[29] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-grained
categorization. InProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops,pages
554‚Äì561,2013. 6
[30] YoonhoLee,AnnieSChen,FahimTajwar,AnanyaKumar,HuaxiuYao,PercyLiang,andChelseaFinn.
Surgicalfine-tuningimprovesadaptationtodistributionshifts. InTheEleventhInternationalConference
onLearningRepresentations,2023. 2
[31] Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu.
Dropkeyforvisiontransformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages22700‚Äì22709,2023. 6
11[32] Dongyue Li and Hongyang Zhang. Improved regularization and robustness for fine-tuning in neural
networks. AdvancesinNeuralInformationProcessingSystems,34:27249‚Äì27262,2021. 3
[33] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration.InInternationalconferenceonmachinelearning,
pages12888‚Äì12900.PMLR,2022. 1
[34] ShengruiLi,XuetingHan,andJingBai. Adaptergnn: Parameter-efficientfine-tuningimprovesgener-
alizationingnns. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
13600‚Äì13608,2024. 2
[35] XingjianLi,HaoyiXiong,HanchaoWang,YuxuanRao,LipingLiu,ZeyuChen,andJunHuan. Delta:
Deep learning transfer using feature map with attention for convolutional networks. arXiv preprint
arXiv:1901.09229,2019. 3
[36] DongzeLian,DaquanZhou,JiashiFeng,andXinchaoWang. Scaling&shiftingyourfeatures: Anew
baselineforefficientmodeltuning. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),
2022. 2,3,6,7,17
[37] WeiyangLiu, ZejuQiu, YaoFeng, YuliangXiu, YuxuanXue, LonghuiYu, HaiwenFeng, ZhenLiu,
JuyeonHeo,SongyouPeng,YandongWen,MichaelJ.Black,AdrianWeller,andBernhardSch√∂lkopf.
Parameter-efficientorthogonalfinetuningviabutterflyfactorization. InICLR,2024. 2
[38] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),2021. 1,6,8
[39] GenLuo,MinglangHuang,YiyiZhou,XiaoshuaiSun,GuannanJiang,ZhiyuWang,andRongrongJi.
Towardsefficientvisualadaptionviastructuralre-parameterization. arXivpreprintarXiv:2302.08106,
2023. 2,3,17
[40] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,andAndreaVedaldi. Fine-grainedvisual
classificationofaircraft. arXivpreprintarXiv:1306.5151,2013. 6
[41] YaoNiandPiotrKoniusz. Chain:Enhancinggeneralizationindata-efficientgansvialipschitzcontinuity
constrainednormalization. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages6763‚Äì6774,June2024. 3
[42] Yao Niand PiotrKoniusz. Nice: Noise-modulated consistency regularizationfor data-efficientgans.
AdvancesinNeuralInformationProcessingSystems,36,2024. 2,15
[43] YaoNi,PiotrKoniusz,RichardHartley,andRichardNock.Manifoldlearningbenefitsgans.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages11265‚Äì11274,2022. 3
[44] Yao Ni, Dandan Song, Xi Zhang, Hao Wu, and Lejian Liao. Cagan: Consistent adversarial training
enhancedgans. InIJCAI,pages2588‚Äì2594,2018. 2
[45] Maria-ElenaNilsbackandAndrewZisserman. Avisualvocabularyforflowerclassification. InIEEE
ConferenceonComputerVisionandPatternRecognition,volume2,pages1447‚Äì1454,2006. 6
[46] ChangdaeOh,HyejiHwang,Hee-youngLee,YongTaekLim,GeunyoungJung,JiyoungJung,HosikChoi,
andKyungwooSong. Blackvip:Black-boxvisualpromptingforrobusttransferlearning. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages24224‚Äì24235,2023. 2
[47] OmkarM.Parkhi, AndreaVedaldi, AndrewZisserman, andC.V.Jawahar. Catsanddogs. InIEEE
ConferenceonComputerVisionandPatternRecognition,2012. 6
[48] ZejuQiu,WeiyangLiu,HaiwenFeng,YuxuanXue,YaoFeng,ZhenLiu,DanZhang,AdrianWeller,and
BernhardSch√∂lkopf. Controllingtext-to-imagediffusionbyorthogonalfinetuning. InNeurIPS,2023. 2
[49] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748‚Äì8763.PMLR,
2021. 1
[50] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalizetoimagenet? InInternationalconferenceonmachinelearning,pages5389‚Äì5400.PMLR,2019.
6
[51] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684‚Äì10695,2022. 1
[52] ShuvenduRoyandAliEtemad. Consistency-guidedpromptlearningforvision-languagemodels. InThe
TwelfthInternationalConferenceonLearningRepresentations,2024. 3
[53] KuniakiSaito,DonghyunKim,andKateSaenko. Openmatch:Open-setsemi-supervisedlearningwith
open-setconsistencyregularization.AdvancesinNeuralInformationProcessingSystems,34:25956‚Äì25967,
2021. 2
[54] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeWGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaR
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5b:An
openlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InThirty-sixthConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022. 1,6
12[55] KihyukSohn,DavidBerthelot,NicholasCarlini,ZizhaoZhang,HanZhang,ColinARaffel,EkinDogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with
consistencyandconfidence. Advancesinneuralinformationprocessingsystems,33:596‚Äì608,2020. 2
[56] AndreasPeterSteiner,AlexanderKolesnikov,XiaohuaZhai,RossWightman,JakobUszkoreit,andLucas
Beyer. Howtotrainyourvit?data,augmentation,andregularizationinvisiontransformers. Transactions
onMachineLearningResearch,2022. 8
[57] YushengSu,XiaozhiWang,YujiaQin,Chi-MinChan,YankaiLin,HuadongWang,KaiyueWen,Zhiyuan
Liu,PengLi,JuanziLi,etal. Ontransferabilityofprompttuningfornaturallanguageprocessing. arXiv
preprintarXiv:2111.06719,2021. 1,2
[58] JunjiaoTian,Yen-ChengLiu,JamesSSmith,andZsoltKira.Fasttrainableprojectionforrobustfine-tuning.
AdvancesinNeuralInformationProcessingSystems,36,2024. 3
[59] GrantVanHorn,SteveBranson,RyanFarrell,ScottHaber,JessieBarry,PanosIpeirotis,PietroPerona,
andSergeBelongie. Buildingabirdrecognitionappandlargescaledatasetwithcitizenscientists:The
fineprintinfine-graineddatasetcollection. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages595‚Äì604,2015. 6
[60] D√°niel Varga, Adri√°n Csisz√°rik, and Zsolt Zombori. Gradient regularization improves accuracy of
discriminativemodels. arXivpreprintarXiv:1712.09936,2017. 3
[61] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈Åukasz
Kaiser, andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon, U.VonLuxburg, S.Bengio, H.
Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformationProcessing
Systems,volume30.CurranAssociates,Inc.,2017. 2,3
[62] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. Thecaltech-ucsdbirds-200-2011dataset.
TechnicalReportCNS-TR-2011-001,CaliforniaInstituteofTechnology,2011. 6
[63] HaohanWang,SongweiGe,ZacharyLipton,andEricPXing. Learningrobustglobalrepresentationsby
penalizinglocalpredictivepower. AdvancesinNeuralInformationProcessingSystems,32,2019. 6
[64] YihanWang,JatinChauhan,WeiWang,andCho-JuiHsieh. Universalityandlimitationsofprompttuning.
AdvancesinNeuralInformationProcessingSystems,36,2024. 2,3
[65] YaomingWang,YuchenLiu,XiaopengZhang,JinLi,BowenShi,ChenglinLi,WenruiDai,Hongkai
Xiong,andQiTian. Violet:Vision-languageefficienttuningwithcollaborativemulti-modalgradients. In
Proceedingsofthe31stACMInternationalConferenceonMultimedia,pages4595‚Äì4605,2023. 3
[66] YemingWenandSwaratChaudhuri. Batchedlow-rankadaptationoffoundationmodels. arXivpreprint
arXiv:2312.05677,2023. 2
[67] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. R-
drop: Regularizeddropoutforneuralnetworks. AdvancesinNeuralInformationProcessingSystems,
34:10890‚Äì10905,2021. 2
[68] LIXuhong, YvesGrandvalet, andFranckDavoine. Explicitinductivebiasfortransferlearningwith
convolutionalnetworks. InInternationalConferenceonMachineLearning,pages2825‚Äì2834.PMLR,
2018. 3
[69] XiaohuaZhai,JoanPuigcerver,AlexanderKolesnikov,PierreRuyssen,CarlosRiquelme,MarioLucic,
JosipDjolonga,AndreSusanoPinto,MaximNeumann,AlexeyDosovitskiy,etal. Alarge-scalestudyof
representationlearningwiththevisualtaskadaptationbenchmark. arXivpreprintarXiv:1910.04867,2019.
6
[70] HanZhang,ZizhaoZhang,AugustusOdena,andHonglakLee. Consistencyregularizationforgenerative
adversarialnetworks. arXivpreprintarXiv:1910.12027,2019. 2
[71] LinjunZhang,ZhunDeng,KenjiKawaguchi,AmirataGhorbani,andJamesZou. Howdoesmixuphelp
withrobustnessandgeneralization? InICLR,2021. 4
[72] ShanZhang,YaoNi,JinhaoDu,YanxiaLiu,andPiotrKoniusz. Semantictransferfromheadtotail:
Enlargingtailmarginforlong-tailedvisualrecognition.InProceedingsoftheIEEE/CVFWinterConference
onApplicationsofComputerVision,pages1350‚Äì1360,2024. 3
[73] YuanhanZhang,KaiyangZhou,andZiweiLiu. Neuralpromptsearch. arXivpreprintarXiv:2206.04673,
2022. 2,6
[74] Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention:
Towardsefficientmulti-modalLLMfinetuning. InTheTwelfthInternationalConferenceonLearning
Representations,2024. 2
[75] YangZhao,HaoZhang,andXiuyuanHu.Penalizinggradientnormforefficientlyimprovinggeneralization
indeeplearning. InInternationalConferenceonMachineLearning,pages26982‚Äì26992.PMLR,2022. 3
13PACE: marrying generalization of PArameter-efficient
fine-tuning with Consistency rEgularization
(Supplementary Material)
YaoNi‚Ä† ShanZhang‚Ä† PiotrKoniusz‚àó,¬ß,‚Ä†
‚Ä†TheAustralianNationalUniversity ¬ßData61 CSIRO
‚Ä†firstname.lastname@anu.edu.au ¬ßpiotr.koniusz@data61.csiro.au
A Broaderimpactsandlimitations
A.1 Broaderimpacts
OurworkprovidesapowerfulsolutionforimprovinggeneralizationinParameterEfficientFine-
Tuning(PEFT),allowingforeffectivefine-tuningofpre-trainedmodelswhilereducetheheavily
relianceonpretrainingfromscratchusingmassivedata. OuradvancementinPEFT,supportedby
Theorems1,2and3,offernovelinsightsintogradientregularizationandmodelalignment. These
insightsextendbeyondPEFTandcanbeappliedtootherareassuchascontinuallearningandtransfer
learning,potentiallyenhancingtheperformanceandefficiencyofmodelsinvariousdomains. By
leveragingourfindings,practitionerscandevelopmorerobustandadaptablemodelsthatgeneralize
welltonewtasksandenvironments,leadingtomoreintelligentandversatileAIsystems. Intermsof
negativeimpacts,therobustnessofourfine-tuningmethodcouldpotentiallybemisusedtocreate
moreconvincingdeepfakes,raisingconcernsaboutthespreadofmisinformation,manipulationof
publicopinion,andmaliciousactivitiessuchasfraud,blackmail,orharassment.
A.2 Limitations
Whileourworkeffectivelyimprovesgeneralizationability,itintroducesadditionalcomputational
costsbyrequiringinputsamplestobepassedthroughthenetworktwiceforregularization. However,
thiscanbemitigatedbyusinglazyregularization,wherethenetworkisregularizedeveryN steps,as
showninFigure5. Lazyregularizationyieldsreasonableimprovementscomparedtothebaseline;for
example,with12steps,itachievesanaccuracyof78.0comparedtothebaseline‚Äôs74.9. Additionally,
ourmethodintroducesextrahyperparametersŒªandœÉ,whichrequirecautionduringhyperparameter
search. Nonetheless, Figure 6 suggests that fewer training data requires larger Œª and œÉ values,
providinginsightforhyperparametertuning.
*Thecorrespondingauthor. ThispaperisacceptedbyNeurIPS2024asaspotlight. Thispreliminary
versionwillsoonbeextendedwiththeexperimentsandanalysesfromtherebuttal.
14B Proofs
B.1 ProofofTheorem1
S the ett hti in gg heœµ r-= ord‚à•œÅ ‚àá e‚àá rŒ∏ tŒ∏ ‚à• e2 r, mw se frp oe mrfo tr hm eTa as ye lc oo rn ed x- po ard ne sr ioT nay inlo tore Rx (cid:16)pa ‚à•n Œ∏s ‚à•i
2
2o ,n 1o (cid:17)f ,L wD en da er ro ivu en :dŒ∏.Byincorporating
œÅ2 n
(cid:16) œÅ‚àá (cid:17) (cid:16) Œ∏ 2 1(cid:17)
LD(Œ∏) ‚â§LDn Œ∏+
‚àá
Œ∏ +R ‚à• œÅ2‚à•2,
n
Œ∏ 2
‚à• ‚à•
œÅ2 (cid:16) Œ∏ 2 1(cid:17)
‚âàLDn(Œ∏)+œÅ ‚à•‚àá
Œ∏
‚à•2+
2 ‚àá
2‚àáT Œ∏H Œ∏‚àá Œ∏+R ‚à• œÅ2‚à•2,
n
(14)
‚à• Œ∏ ‚à•2
Assumingthattheapproximationdoesnotaltertheinequalityrelationship,i.e.,itpreservesthe
relationonbothsidesandconsideringthelargesteigenvalueofH asŒªH ,implyingvTH v ‚â§
Œ∏ max Œ∏ ‚â§
ŒªH v 2foranyv,wefurtherboundEq. 14asfollowsandarriveat:
max‚à• ‚à•2
œÅ2 (cid:16) Œ∏ 2 1(cid:17)
LD(Œ∏) ‚â§LDn(Œ∏)+œÅ ‚à•‚àá
Œ∏
‚à•2+
2
ŒªH max+R ‚à• œÅ2‚à•2,
n
B.2 ProofofTheorem2
Theproofismotivatedfrom[42]. Weincludetheproofprocessforcompleteness. Denotem =
1
z 1,m =z 1thusm ,m (0,œÉ2)
1 2 2 1 2
‚àí ‚àí ‚àºN
dpace =E [f(Œ∏ +z ‚àÜŒ∏) f(Œ∏ +z ‚àÜŒ∏)]2
z1,z2 0 1
‚äô ‚àí
0 2
‚äô
=E [f(Œ∏ +‚àÜŒ∏+(z 1) ‚àÜŒ∏) f(Œ∏ +‚àÜŒ∏+(z 1) ‚àÜŒ∏)]2
z1,z2 0 1
‚àí ‚äô ‚àí
0 2
‚àí ‚äô
=E [f(Œ∏+m ‚àÜŒ∏) f(Œ∏+m ‚àÜŒ∏)]2 (15)
m1,m2 1
‚äô ‚àí
2
‚äô
Definingv := m ‚àÜŒ∏ andu := m ‚àÜŒ∏, wherev,u (0,œÉ2diag(‚àÜŒ∏ ‚àÜŒ∏)), wecan
1 2
‚äô ‚äô ‚àº N ‚äô
rewriteEq. 15asfollows:
E [f(Œ∏+v) f(Œ∏+u)]2
v,u
‚àí
E (cid:2) f(Œ∏)+vT‚àá+ 1 vTHv f(Œ∏) uT‚àá 1 uTHu(cid:3)2
v,u
‚âà 2 ‚àí ‚àí ‚àí 2
=E (cid:2) vT‚àá+ 1 vTHv uT‚àá 1 uTHu(cid:3)2
v,u
2 ‚àí ‚àí 2
=E (cid:2) (v u)T‚àá+ 1 vTHv 1 uTHu(cid:3)2
v,u
‚àí 2 ‚àí 2
=E (cid:2) (v u)T‚àá(cid:3)2 (16)
v,u
‚àí
+E (cid:2)(cid:0) (v u)T‚àá(cid:1)(cid:0) vTHv uTHu(cid:1)(cid:3) (17)
v,u
‚àí ‚àí
1 1
+ E [vTHv]2+ E [uTHu]2 (18)
v u
4 4
1 E (cid:2) (vTHv)(uTHu)]. (19)
v,u
‚àí 2
Next,wederivethefourterms,Eq. 16,17,18,and19,respectivelyasfollows:
Eq. 16. UsingE [(z z )2]=2œÉ2forz ,z (0,œÉ2),wecansimplify(Eq. 16)asfollows,
z1,z2 1
‚àí
2 1 2
‚àºN
notingthattermsrelatedtodifferentdimensionsarecanceledduetozero-meanindependentGaussian
noise:
E (cid:2) (v u)T‚àá(cid:3)2 =E (cid:2)(cid:88) (v u )2 2(cid:3) =2œÉ2(cid:88) ‚àÜŒ∏2 2. (20)
v,u ‚àí v,u j ‚àí j ‚àáj j‚àák
j j
Eq. 17. UtilizingE[z3]=¬µ3+3¬µœÉ2 forz (¬µ,œÉ2),andnotingthatE[z3]=0for¬µ=0,Eq.
‚àºN
17isderivedas:
E (cid:2)(cid:0) (v u)T‚àá(cid:1)(cid:0) vTHv uTHu(cid:1)(cid:3)
v,u
‚àí ‚àí
=E (cid:2) (vT‚àá)(vTHv)]+E (cid:2) (uT‚àá)(uTHu)] E (cid:2) (vT‚àá)(uTHu)] E (cid:2) (uT‚àá)(vTHv)]
v u v,u v,u
‚àí ‚àí
=2E (cid:2) (vT‚àá)(vTHv)]=0. (21)
v
15Eq. 18. WefirstdecomposeEq. 18,thendiscusseachcaseandobtainthefinalresult.
1 E [vTHv]2+ 1 E [uTHu]2 = 1 E [vTHv]2 = 1 E (cid:2) (cid:88) v H v v H v (cid:3) . (22)
v u v v j jk k p pq q
4 4 2 2
j,k,p,q
Giventheindependenceofelementsinv,onlytermswithanelementrepeatedtwoorfourtimes
contributenon-zeroresults,leadingtofourdistinct,non-overlappingcases. UsingE[z2]=œÉ2+¬µ2
andE[z4]=¬µ4+6¬µ2œÉ2+3œÉ4forz (¬µ,œÉ2),andsimplifyingtoE[z2]=œÉ2andE[z4]=3œÉ4
‚àºN
when¬µ=0,wehave:
Case1: j =k =p=q,giventheindependenceofv andv ,wehave:
j p
Ã∏
E (cid:2)(cid:88)(cid:88) v2H v2H (cid:3) = (cid:88) H H E[v2]E[v2]=œÉ4(cid:88) H H ‚àÜŒ∏2‚àÜŒ∏2. (23)
v j jj p pp jj pp j p jj kk j k
j pÃ∏=j j,pÃ∏=j j,kÃ∏=j
Case2: Forj =p=k =q,theindependenceofv andv simplifiesourcalculation,leadingto:
j k
Ã∏
E (cid:2)(cid:88)(cid:88) v H v v H v (cid:3) = (cid:88) H2 E[v2]E[v2]=œÉ4 (cid:88) H2 ‚àÜŒ∏2‚àÜŒ∏2. (24)
v j jk k j jk k jk j k jk j k
j kÃ∏=j j,kÃ∏=j j,kÃ∏=j
Case 3: For j = q = k = p, utilizing the independence of v and v as well as the symmetry
j k
Ã∏
H =H ,weobtain:
jk kj
E (cid:2)(cid:88)(cid:88) v H v v H v (cid:3) = (cid:88) H2 E[v2]E[v2]=œÉ4 (cid:88) H2 ‚àÜŒ∏2‚àÜŒ∏2. (25)
v j jk k k kj j jk j k jk j k
j kÃ∏=j j,kÃ∏=j j,kÃ∏=j
Case4: Forj =q =k =p,usingE[z4]=3œÉ4wherez (0,œÉ2),wehave:
‚àºN
(cid:104)(cid:88) (cid:105) (cid:88) (cid:88)
E v H v v H v = H2 E[v4]=3œÉ4 H2 ‚àÜŒ∏4. (26)
v j jj j j jj j jj j jj j
j j j
Combiningabovefourcasestogether,wehavetheresultforEq. 18:
œÉ4(cid:16)(cid:88) (cid:88) (cid:17)
3H2 ‚àÜŒ∏4+ (H H +2H2 )‚àÜŒ∏2‚àÜŒ∏2 . (27)
2 jj j jj kk jk j k
j j,kÃ∏=j
Eq. 19:
1 E (cid:2) (vTHv)(uTHu)]
v,u
‚àí 2
= 1 E (cid:2) (vTHv)(cid:3)E (cid:2) (uTHu)(cid:3)
v u
‚àí 2
= 1 E (cid:2)(cid:88) H v2(cid:3)E (cid:2)(cid:88) H v2(cid:3)
‚àí 2 v jj j u kk k
j k
1(cid:16)(cid:88) (cid:17)(cid:16)(cid:88) (cid:17)
= H E[v2] H E[v2]
‚àí 2 jj j kk k
j k
œÉ4(cid:16)(cid:88) (cid:88) (cid:17)
= H2 ‚àÜŒ∏4+ H H ‚àÜŒ∏2‚àÜŒ∏2 . (28)
‚àí 2 jj j jj kk j k
j j,kÃ∏=j
WithresultsofEq. 20,21,27,28,wehavethefinalresults:
(cid:88)
dpace 2œÉ2 ‚àÜŒ∏2 2+0
‚âà j‚àáj
j
œÉ4(cid:16)(cid:88) (cid:88) (cid:88) (cid:88) (cid:17)
+ 3H2 ‚àÜŒ∏4+ (H H +2H2 )‚àÜŒ∏2‚àÜŒ∏2 H2 ‚àÜŒ∏4 H H ‚àÜŒ∏2‚àÜŒ∏2
2 jj j jj kk jk j k‚àí jj j ‚àí jj kk j k
j j,kÃ∏=j j j,kÃ∏=j
(cid:88) (cid:16)(cid:88) (cid:88) (cid:17)
=2œÉ2 ‚àÜŒ∏2 2+œÉ4 H2 ‚àÜŒ∏4+ H2 ‚àÜŒ∏2‚àÜŒ∏2
j‚àáj jj j jk j k
j j j,kÃ∏=j
(cid:88) (cid:88)
=2œÉ2 ‚àÜŒ∏2 2 +œÉ4 H2 ‚àÜŒ∏2‚àÜŒ∏2 =2œÉ2 ‚àÜŒ∏ ‚àá 2+œÉ4 (‚àÜŒ∏‚àÜŒ∏T) H 2 (29)
j‚àák jk j k ‚à• ‚äô ‚à•2 ‚à• ‚äô ‚à•F
j j,k
16B.3 ProofofTheorem3
TheCauchy-Schwarzinequalitystatesthatforu,v Rd,wehave((cid:80) u v )2 ((cid:80) u2)((cid:80) v2).
Letu=1,itfollowsthat((cid:80) v )2 d v 2. Using‚àà thisinequality,wj ethj enj pro‚â§ vethej foj llowinj g:j
j j ‚â§ ‚à• ‚à•2
1
[‚àÜŒ∏T‚àá ‚àÜŒ∏TH‚àÜŒ∏]2 2[‚àÜŒ∏T‚àá]2+[‚àÜŒ∏TH‚àÜŒ∏]2
‚àí 2 ‚â§
(cid:16)(cid:88) (cid:17)2
[‚àÜŒ∏T‚àá]2 = ‚àÜŒ∏ d ‚àÜŒ∏ ‚àá 2 (30)
j ‚àáj ‚â§ ‚à• ‚äô ‚à•2
j
[‚àÜŒ∏TH‚àÜŒ∏]2 =(cid:16)(cid:88) ‚àÜŒ∏ j‚àÜŒ∏ kH jk(cid:17)2 ‚â§d2(cid:13) (cid:13)(‚àÜŒ∏‚àÜŒ∏T) ‚äôH(cid:13) (cid:13)2
F
(31)
j,k
Here,theinequalityisobtainedbytreating‚àÜŒ∏ ‚àÜŒ∏ H asanelementofavectorwithsizeofd2.
j k jk
Thisleadstothefinalresults.
C AdditionalPlots
Baseline 0.001 0.005 0.01 0.05 0.1 0.5
3e4 3e4
2e4 2e4
1e4 1e4
epoch= 100 200 300 epoch= 100 200 300
(a)FPA (b)PACE
Figure7: Gradientnormsof(a)FPAand(b)PACEwithdifferentregularizationstrengthsŒªduring
trainingonCIFAR-100(VTAB-1K)w/ViT-B/16. Figure4illustratestheaveragegradientnormover
trainingepochs.
‚àÇf
2 Baseline +FPA +PACE
‚à•‚àÇŒ∏‚à•
6e3
3e3
Œª= 1e-35e-30.010.05 0.1 0.5 1 5 10 50 100 500 1e3 5e3 1e4 5e4
Figure8: GradientnormsofmodelsacrosswiderangeofregularizationstrengthsŒªonCamelyon
(VTAB-1K)w/Swin-B.Lineandshadowrepresentmeanandstdovertrainingepochs.Whilegradient
explosionislessfrequentforFPAinthissetting,itexhibitsunpredictablegradientnormwithvaried
regularizationstrengths. Incontrast,PACEreliablylowersgradientnormsasregularizationstrength
Œªincreases,demonstratingitsrobustnessforeffectivegradientcontrol.
D Hyperparametersettings
Foreachdataset,wefollowstrategiesfrompreviousworks[36,22,5,39]toapplygridsearchon
therank,learningrateandweightdecaytoestablishstrongbaselines. Table7,8,9and10present
17
Œò‚àÇ/f‚àÇ
2
‚à•
‚à•thehyperparametersandnumberoftrainableparametersusedinourstrongbaselineforVTAB-1K,
few-shotlearning,FGVCanddomainadaptationtasks.
With these strong baselines, we apply grid search on Œª 0.02,0.05,0.1,0.2,0.5,1 and œÉ
‚àà { } ‚àà
0.1,0.5,1,1.5,2 forPACEtooptimizeitsperformance.
{ }
Table 7: Hyperparameters for baseline on VTAB-1K with ViT-B/16. A: LoRA +VPT , B:
mul add
LoRA . lr: learningrate. WD:weightdecay.
add
Natural Specialized Structured
Method A A A A A A A A A A B B B A A A A A B
Rank 10 14 12 18 18 14 10 8 8 10 2 2 8 18 4 10 10 22 4
1.81
lr 1e-3 1e-3 1e-3 1e-3 1e-3 1e-2 1e-3 5e-3 5e-3 5e-3 5e-4 5e-4 1e-4 5e-3 5e-3 5e-3 5e-3 1e-2 2e-4
WD 1e-4 1e-4 1e-3 1e-2 1e-3 1e-3 1e-2 1e-2 1e-2 1e-2 1e-4 1e-3 1e-4 1e-3 1e-3 1e-4 1e-2 1e-2 1e-2
Table8: RanksforbaselinesinFew-shotlearning. Weightdecayisfixedat1e-4.
learningrate FGVCAircraft Food101 Flowers102 OxfordPets StanfordCars Mean
Baseline 5e-3 5e-3 5e-3 2e-3 2e-3 Parameter(M)
LoRAadd 4 4 4 4 10 0.93
VPTadd 1 1 1 1 1 0.14
LoRAmul+VPTadd 14 10 18 18 24 2.70
Table9: HyperparametersforthebaselineLoRA +VPT inFGVC.
mul add
Hyperparameter CUB-200-2011 NABirds OxfordFlowers StanfordDogs StanfordCars MeanParameter(M)
learningrate 5e-3 5e-4 5e-3 5e-3 2e-4
weightdecay 1e-2 1e-3 1e-3 1e-2 1e-3 2.80
rank 14 18 18 24 14
Table10: HyperparametersforbaselineLoRA +VPT indomainadaptation.
mul add
Baseline rank learningrate weightdecay Parameter(M)
LoRAmul+VPTadd 10 5e-4 1e-2 2.39
18
retemaraprepyH
001rafiC
101hcetlaC
DTD
201srewolF
steP
NHVS 793nuS noylemaC TASoruE 54csiseR
yhtaponiteR tnuoC-rvelC
tsiD-rvelC baLMD
tsiD-ITTIK
coL-rpSd irO-rpSd
mizA-BRONs
elE-BROsN
)M(retemarapegarevA