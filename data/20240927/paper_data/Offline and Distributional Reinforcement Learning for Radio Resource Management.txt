Offline and Distributional Reinforcement Learning
for Radio Resource Management
Eslam Eldeeb and Hirley Alves
Centre for Wireless Communications (CWC), University of Oulu, Finland
Email: firstname.lastname@oulu.fi
Abstract‚ÄîReinforcement learning (RL) has proved to have a adversarial networks (GANS) and deep RL. All in all, the
promising role in future intelligent wireless networks. Online majority of the literature, if not all, focused on online RL
RL has been adopted for radio resource management (RRM),
while solving the RRM problem.
taking over traditional schemes. However, due to its reliance on
Although online RL contributes greatly to solving RRM, it
onlineinteractionwiththeenvironment,itsrolebecomeslimited
in practical, real-world problems where online interaction is not faces serious challenges when transferred to real-world prob-
feasible. In addition, traditional RL stands short in front of the lems. Online RL heavily relies on exploring the environment,
uncertainties and risks in real-world stochastic environments. which is a random exploration at the beginning of the op-
In this manner, we propose an offline and distributional RL
timization, through online interaction with the environments.
scheme for the RRM problem, enabling offline training using a
Traditional online RL faces four main obstacles:
static dataset without any interaction with the environment and
considering the sources of uncertainties using the distributions 1) random exploration at the beginning of the optimization
of the return. Simulation results demonstrate that the proposed
introduces poor service to the users;
scheme outperforms conventional resource management models.
2) random exploration wastes time and resources. In addi-
In addition, it is the only scheme that surpasses online RL and
achieves a 16% gain over online RL. tion, it might not be safe to interact randomly with the
Index Terms‚ÄîDistributional reinforcement learning, offline environment;
reinforcement learning, radio resource management 3) incomplexapplications,suchasRRM,onlineRLneeds
a huge amount of interaction and takes long periods to
I. INTRODUCTION converge, which might not be practically feasible; and
Recent advances towards 6G networks include complex 4) traditional RL only considers the average performance,
real-world wireless environments requiring continuous mon- neglecting sources of uncertainties and risks.
itoringandcontrol.Suchadvancementsurgetheneedfornew These challenges motivate the importance of offline RL and
learning-based methods for modeling and control. Recently, distributional RL. The former suggests optimizing the op-
reinforcement learning (RL) has become attractive to the timum policy using a static dataset without any interaction
wireless domain due to its outstanding ability to provide with the environment [8]. In contrast, the latter considers the
model-free decision-making [1]. In online RL, an agent ob- environment uncertainties relying on the return distribution
serves the current state of the environment, takes a decision instead of the average return [9].
(action), transits to a new state, receives a feedback reward This work proposes a novel offline and distributional RL
evaluating the decision, and improves its policy1. To this end, algorithm for the RRM problem. In contrast, to [10], which
deployingRLtoradioresourcemanagement(RRM)problems applies the offline RL scheme to the RRM, comparing a
is appealing due to their complex optimization objectives and mixture of datasets, we rely on combining both offline and
the settled monitoring of their environments through feedback distributional RL to overcome the stochastic behavior of
signals [2]. the environment. To our knowledge, this is the first work
Several works have exploited RL techniques in the RRM to combine offline RL and distributional RL for the RRM
problem [3]‚Äì[7]. For instance, the work in [3] proposes a problem. The contributions of this paper are summarized as
power allocation and resource management scheme using follows:
deep RL, achieving a high average sum rate in different
‚Ä¢ We propose an offline and distributional RL solution for
user densities. The authors in [4] leverage RL to perform a
the RRM problem. The proposed model maximizes the
distributed resource scheduling while minimizing the on-grid
combination of the weighted sum and tail rates.
energy consumption. In [5], a multi-agent RL (MARL) solu-
‚Ä¢ We demonstrate the theoretical details of the proposed
tiontotheRRMispresented,whereasagraphneuralnetwork
algorithmaswellaspracticalimplementationanddataset
(GNN) architecture is proposed to solve the RRM problem
collection.
in [6]. In [7], the authors propose a resource management
‚Ä¢ Wecomparedtheproposedmodeltomultiplebenchmark
algorithminnetworkslicingusingacombinationofgenerative
schemes and an online RL algorithm.
1A policy is a behavior that describes which actions are selected at each ‚Ä¢ Numerical results show that the proposed offline and
state. distributional RL scheme outperforms the state-of-the-
4202
peS
52
]GL.sc[
1v46761.9042:viXraart schemes. In addition, it is the only algorithm that At time t, the received signal of UE m that is associated
surpasses online RL. with AP n is
The rest of the paper is summarized as follows: Section II (cid:88)
y (t)=h (t)x (t)+ h (t)x (t)+n (t), (2)
presentsthesystemmodel.SectionIIIpresentstheRLformu- m mn n mi i m
iÃ∏=n
lation. The proposed offline and distributional RL algorithm
is presented in Section IV. Section V depicts the results, and where h mn is the channel between UE m and AP n and
finally, Section VI concludes the paper. n m(t) ‚àº CN(0,n2 o) is the additive white Gaussian noise
(AWGN) with variance n2. At time t, the instantaneous rate
o
II. SYSTEMMODEL (Shannon capacity) of UE m that is associated with AP n is
Consider a wireless network of area L√óL square meters (cid:18) |h (t)|2p (cid:19)
that comprises N randomly deployed access points (APs) R (t)=log 1+ mn t , (3)
m 2 (cid:80) |h (t)|2p +n2
and M randomly deployed user equipment (UEs) as shown iÃ∏=n mi t o
in Fig. 1. To ensure practical scenarios, the random deploy-
where the term
|hmn(t)|2pt
represents the signal-to-
ment of APs and UEs are controlled by minimum AP-UE (cid:80) iÃ∏=n|hmi(t)|2pt+n2 o
interference-plus-noise ratio (SINR) of UE m at time t. The
distance d and minimum AP-AP distance d . Our simulator
0 1
average rate (throughput) of UE m in an episode is
considers an episodic environment consisting of time slots
t ‚àà {1,2,¬∑¬∑¬∑ ,T}, where T is the total number of slots in T
one episode. At each time slot t, each UE moves randomly R¬Ø m = T1 (cid:88) R m(t). (4)
in the network with a speed v(t). The APs and UEs locations t=1
are fixed during each episode. RRM problems aim to maximize the average data rates
across all users. However, this objective is trivial to be solved
in a way that each AP always schedule the user with the best
SINR. Therefore, fairness across users must be considered in
the RRM objective. In this work, we consider joint optimiza-
tion of the sum-rate and the 5-percentile rate. The former is
calculated as
M
R = (cid:88) R¬Ø , (5)
sum m
m=1
where the latter is the average rate achieved by 95% of the
UEs. The 5-percentile rate is calculated as
R = max R (6)
Association Interference 5%
UE AP link link s.t. P[R¬Ø ‚â•R]‚â•0.95, ‚àÄm‚àà1,2,¬∑¬∑¬∑ ,M.
m
To this end, the objective is a weighted sum (joint combina-
Fig.1:ThewirelessmodelconsistsofN APsservingM UEs.Thebluelines tion) of the sum rate and the 5-percentile rate
represent the user association performed at the beginning of each episode,
whiletheredlinesrepresentinterferencelinks. R =¬µ R +¬µ R , (7)
score 1 sum 2 5%
At the beginning of each episode, user association occurs, where ¬µ and ¬µ are user-chosen weights.
1 2
where each UE is associated with one of the APs. User-
A. Problem Definition
association is performed according to the reference signal
received power (RSRP) of each user, where UE m is as- The objective in this work is user scheduling, i.e., which
sociated to AP n, such that n = arg max RSRP , where APs serve which users, to maximize the sum-rate and the 5-
i mi
i ‚àà {1,2,¬∑¬∑¬∑ ,N}. The RSRP is affected by the transmit percentile rate combination (R score)
powerp andthechannel.ThechannelbetweenUEnandAP
t T
m is characterized by indoor path loss, log-normal shadowing P1: max (cid:88) R (t), (8)
score
with standard deviation œÉ , and short-term frequency-flat A(t)
sh t=1
Rayleigh fading. According to 3GPP [11], the indoor path
where A(t) is the joint action of all APs, i.e., the scheduling
loss between UE m and AP n is calculated as
policy of all APs. However, optimizing the R using (7) in
score
PL =15.3+37.6log(d )+PL , (1) theobjectivedirectlyishardtooptimizeandshowsinstability
mn mn o
convergence [5]. Therefore, we adjust the objective using the
whered istheeuclideandistancebetweenUEmandAPn,
mn proportional fairness (PF) factor [5]. The PF factor describes
d >d and PL is a reference path loss. The total power
mn 0 o the priority of each UE and is calculated as
loss is the addition of the path loss, shadowing, and Rayleigh
fading. PF (t)=w (t)R (t), (9)
m m mwhere w (t) is a weighting factor calculated recursively as whereas the performance is evaluated using the R
m score
in (7).
1
w (t)= , (10)
m RÀú (t) B. Online RL
m
RÀú (t)=Œ∑R (t)+(1‚àíŒ∑)RÀú (t‚àí1), (11) RL frameworks aim to find the optimum policy œÄ‚àó that
m m m
maximizes the accumulative rewards. Recently, deep neural
where Œ∑ is a step parameter and RÀú (0) = R (0). The
m m networks provide power RL algorithms, such as deep Q-
PF factor is inversely proportional to the long-term rate of
network(DQN)[12],proximalpolicyoptimization(PPO)[13],
the user, reflecting that the higher the PF factor for a user
andsoftactor-critic(SAC)[14],thatcansolvelargedimension
indicates its need to be served. Therefore, the objective in (8)
problems. We choose DQN as our online RL algorithm in
is simplified as
this work due to its simplicity and stability [15]. In addition,
T M it is straightforward to introduce our algorithm in the next
(cid:88) (cid:88)
P1: max (w m(t))Œª¬∑R m(t), (12) section on top of a DQN algorithm. DQN is a model-free
A(t)
t=1m=1 online RL algorithm that uses a neural network to estimate
where Œª ‚àà [0,1] controls the trade-off between the sum-rate theQ-function.Inaddition,itisanoff-policyalgorithm,where
and the 5-percentile rate. previousexperiencessaved(inabuffercalledreplaymemory)
from previous policies are sampled to update the current Q-
III. REINFORCEMENTLEARNING
function.
Inthissection,weformulateandsolvetheobjectivein(12) TofindtheoptimumQ-function,DQNupdatestheBellman
using online reinforcement learning. optimality equation by minimizing the loss
A. Markov Decision Process
(cid:20)(cid:16) (cid:17)2(cid:21)
L =EÀÜ r+Œ≥maxQÀÜ(g)(s‚Ä≤,a‚Ä≤)‚àíQ(s,a) , (15)
DQN
The RRM problem can be formulated as a Markov deci- a‚Ä≤
sion process (MDP). An MDP is characterized by the tuple where EÀÜ is the average taken over the sampled experiences
‚ü®s a ,r ,s ,Œ≥‚ü©, where s is the current state, a is the
t t t t+1 t t from the replay memory, Œ≥ is the discount factor, g is the
current action, r t is the reward received from taking action a t gradientstep(iteration),s‚Ä≤ isthenextstateanda‚Ä≤ istheaction
at state s t and transiting to the next state s t+1, and Œ≥ ‚àà[0,1] to be chosen at s‚Ä≤. The Q-function Q(s,a) is modeled using
is the discount factor that controls how much future rewards
a neural network with parameters œï. The main challenge in
are considered in the RL problem. For a more practical and
online RL is the need for continuous online interaction with
generalRLformulation,welimitthenumberofUEseachAP
theenvironment,whichmightnotbefeasibleorsafe.Next,we
can select to K UEs. At the beginning of each episode, user
present offline and distributional RL as practical alternatives.
association occurs for each AP, then, among the associated
users, only the best K users (the highest K users in terms IV. OFFLINEANDDISTRIBUTIONALRL
of the weighting factor w (0) calculated from (10)) are Thissectionpresentstheproposedofflineanddistributional
mn
includedforselection.ThedetailedMDPintheRRMproblem RL algorithm to solve the objective in (12) offline.
is as follows
A. Offline RL
1) State: each AP can observe two components related
OfflineRLresortstoastaticofflinedatasetwithoutanyon-
to each device among the selected top K devices, the
line interaction with the environment. The dataset is collected
SINRmeasurementSINR (t)andtheweightingfactor
kn
using a behavioral policy, an online learning agent, a baseline
w (t). For N available APs, the overall state of the
kn
policy,orevenarandomone.Notethat,fortheRRMproblem,
system is
it has been proved in [10] that the dataset‚Äôs quality heavily
(cid:0)
s t = SINR 11(t),w 11(t),¬∑¬∑¬∑ ,SINR K1(t),w K1(t),¬∑¬∑¬∑ , affects the convergence of offline RL algorithms. Deploying
(13) traditional deep RL algorithms offline fails to converge due
SINR (t),w (t),¬∑¬∑¬∑ ,SINR (t),w (t)(cid:1) . to the distributional shift between the actions seen in the
1N 1N KN KN
dataset and the learned actions [8]. Conservative Q-learning
The state space size is 2NK.
(CQL) [16] is a well-known offline RL algorithm that adds
2) Action:eachAPschedulesaresourcetoonedeviceonly
a regularization term (conservative parameter) to the Bellman
among the top K devices at each time slot (or chooses
update, overcoming the overestimation problem from the out-
to be silent and serve no UEs). The overall action space
of-distribution (OOD) actions learned compared to those in
isthescheduleddeviceschosenbyeachAP,anditssize
the dataset.
is (K+1)N.
Building the CQL algorithm on top of DQN architecture is
3) Reward: since the objective in (8) is hard to optimize,
straightforward, where the CQL loss is calculated as
the reward function is formulated using the objective
(cid:20) (cid:21)
in (12) L = 1 L +Œ±EÀÜ log(cid:88) exp(cid:0) Q(s,aÀú)(cid:1) ‚àí Q(s,a) ,
M CQL 2 DQN
r = (cid:88) (w (t))ŒªR (t), (14) aÀú
t m m (16)
m=1Algorithm 1: Conservative quantile regression algo-
ùúè4
ùúèùúè 23
ùíü
ùë†ùë°,ùëéùë°,ùëçùë°,ùë†ùë°+1 O pop lt ii cm yu ùúãm
‚àó
1ri Dth em finf eor nuth me bR erR oM
f
Apr Po sbl Nem ,.
number of UEs M, number
ùúè1
of best weighting factor users K, discount factor Œ≥,
Expected return
Return distributionùëçùë° Offline Training learning rate Œ∂, number of quantiles I, conservative
penalty constant Œ±, number of training epochs E,
number of gradient steps G, offline dataset D, input
layer size 2NK, and output layer size (K+1)NI
2 Initialize network parameters
3 for epoch e in {1,...,E} do
4 for gradient step g in {1,...,G} do
Deployment 5 Sample a batch B from the dataset D
6 Estimate the CQR loss L CQR in (18)
Fig.2:AnillustrativefigurefortheproposedCQRalgorithm. 7 Perform a stochastic gradient step based on the
estimated loss with a learning rate Œ∂
8 end
where Œ± > 0 is a constant and the summation (cid:80) is taken 9 end
aÀú
over all the actions. 10 Return {Œ∏ÀÜ j(s,a)}I j=1
B. Distributional RL
DistributionalRLisavariantofRLthatusesthedistribution
A. Baseline Schemes
over return instead of the average return while optimizing the
optimumpolicy[9].Quantile-regressionDQN(QR-DQN)[17] We compare the proposed algorithm to some of the state-
is a distributional RL algorithm that estimates the return of-the-art baseline methods:
distributions Z(s,a) using I fixed dirac delta functions. In ‚Ä¢ Random-walk: each AP randomly chooses one of the
QR-DQN,theoutputlayeroftheneuralnetworkhasasizeof top K UEs to serve at each time slot.
the number of actions times the number of quantiles I. The ‚Ä¢ Full-reuse: each AP chooses the user with the highest
distributional Bellman loss is calculated as PF ratio among the top K UEs at each time slot.
I I ‚Ä¢ Time-division multiplexing: each AP serves the top K
L = 1 (cid:88)(cid:88) Œ∂ (cid:0) r+Œ≥Œ∏ (s‚Ä≤,a‚Ä≤)‚àíŒ∏ (s,a)(cid:1) , UEs in a round-robin fashion. This scheme prioritizes
QR-DQN I2 œÑj j‚Ä≤ j
fairness among users.
j=1j‚Ä≤=1
(17) ‚Ä¢ Information-theoretic link scheduling (ITLinQ): each
where Œ∏ j(s,a) is an estimate of the quantile inverse PDF of AP prioritizes its associated UEs according to their PF
the return distribution and Œ∂ œÑ is the quantile regression Huber ratios. Afterward, each AP goes through an interference
loss [17]. tolerancecheckforeachUEtomakesuretheinterference
level is lower than a certain threshold MSNRŒ∑ . If
C. Conservative Quantile Regression mn
no UEs associated with the AP passes the interference
Conservative quantile regression (CQR) [18] is a variant of tolerancecheck,thisAPisturnedoff.Thismethodproved
RL algorithms that combines CQL with QR-DQN, where the to reach sub-optimal performance in the literature [19].
optimum Q-function is optimized offline using distributional
RL. The CQR loss function is formulated as follows B. Simulation Parameters and Dataset
1 We consider an environment of size 50 m √ó 50 m, N =
L = L (18)
CQR 2 QR-DQN 3 APs and M = 15 UEs. For the online RL, we build a
(cid:34) I (cid:34) (cid:35)(cid:35) DQN, where the neural network has 2 hidden layers with 256
+Œ±EÀÜ 1(cid:88) log(cid:88) exp(cid:0) Œ∏ (s,aÀú)(cid:1) ‚àíŒ∏ (s,a) . neurons each. Each episode consists of T =2000 time steps.
I j j
j=1 aÀú We collect the offline dataset using a behavioral policy from
an online DQN agent. In other words, we use the last 20% of
Fig.2presentsanillustrationfortheproposedCQRalgorithm,
the transitions of training an online DQN agent. We simulate
andAlgorithm1summarizestheproposedcentralizedtraining
a single NVIDIA Tesla V100 GPU using Pytorch framework.
using the CQR algorithm for the RRM problem.
All the simulation parameters are shown in Table I.
V. EXPERIMENTALRESULTS
C. Simulation Results
This section presents the numerical results of the pro-
posed offlineand distributional RLalgorithm comparedto the Fig. 3 demonstrates the convergence of online RL (DQN)
baseline models. First, we show the online RL algorithm‚Äôs as a function of training episodes. We first observe that the
(DQN) performance. Then, we present the proposed offline random scheme has the worst Rscore, while TDM and full-
and distributional RL algorithm. reuse show close Rscore values. The sub-optimal schemeTABLEI:Simulationparameters
Parameter Value Parameter Value
N 3 M 15
K 3 L 50
d0 10m d1 1m
v(t) 1m/s PLo 10dB
pt 10dBm T 2000
¬µ1 M1 ¬µ2 3
Œª 0.8 Œ≥ 0.99
I 8 Œ± 1
Œ∂ 10‚àí5 Replaymemory 105
Layers 2 Neurons 256
Optimizer Adam Activation ReLu
Online TDM CQL
ITLinQ Random DQN
Full-reuse CQR QR-DQN
Online
ITLinQ
Full-reuse Epochs
TDM
Random
Fig. 4: The convergence of the proposed CQR algorithm as a function of
training epochs compared to other offline RL schemes and the baseline
methods;theOnlinemethodisshownafterconvergence.Alltheresultsshown
areaverageover100uniquetestepisodes.
offline dataset, the CQR algorithm outperforms other offline
RLschemes.Inaddition,thesizeofthedatasetslightlyaffects
the performance of CQR as the rates are higher with a larger
dataset, which is often better in quality because it comes
Episodes from the last experiences seen by a DQN agent, usually good
experiences.Inaddition,asignificantgapisrecordedbetween
the proposed CQR algorithm and other offline schemes with
Fig. 3: The convergence of online RL as a function of training episodes smaller datasets. This highlights that the proposed model
compared to the baseline methods. All the results shown are average over
requireslessdatatoachievereasonableratesthanotheroffline
100uniquetestepisodes.
RL methods.
ITLinQ has the highest Rscore among all the baseline meth-
ods.Moreover,DQNreachesconvergence(Rscore=2.6)after
VI. CONCLUSIONS
around 150 episodes. It outperforms all the baseline schemes,
including the sub-optimal scheme ITLinQ, by 20%.
Inthispaper,wedevelopedanovelofflineanddistributional
In Fig. 4, we report the convergence of the proposed
RL algorithm for RRM. First, we formulated the problem as
offlineanddistributionalCQRalgorithmcomparedtomultiple
MDP and then introduced the practical limitations of online
offline/distributionalRLalgorithms,namely,CQL,DQN(inan
RL. Afterward, we introduced the proposed model using a
offline manner), and QR-DQN. This figure also illustrates the
combination of CQL and QR-DQN. Simulation results show
online scheme (after convergence) and the baseline methods.
that the proposed model achieved a higher Rscore than all
Although DQN and QR-DQN achieve higher Rscore than
the baseline schemes. In addition, it is the only scheme to
TDM and full-reuse, they fail to converge. In contrast, CQL
surpass online RL with a 20% gain in terms of the Rscore.
surpasses the sub-optimal ITLinQ; however, it fails to reach
InvestigatingtheRRMproblemusingofflineanddistributional
the Rscore as the online RL after convergence. Finally, the
multi-agent RL is left for future work.
proposed CQR algorithm is the only one that can reach
convergence offline and outperform the online RL.
Fig. 5 shows the performance of the proposed CQR algo-
rithm compared to other baseline offline RL methods during ACKNOWLEDGMENTS
testing after full training. It reports the sum-rate, 5-percentile
rate, and Rscore using an offline dataset contains 20% of This research was supported by the Research Council of
the experience of an online DQN (Fig.5a-5c) and using an Finland(formerAcademyofFinland)6GFlagshipProgramme
offline dataset contains 10% of the experience of an online (Grant Number: 346208).
DQN (Fig.5d-5f), respectively. Regardless of the size of the
erocsR
erocsRCQR
CQL
DQN
QR-DQN
(a) (b) (c)
(d) (e) (f)
Fig.5:Thesumrate,5-percentilerate,andRscorereportedfortheproposedCQRalgorithmcomparedtootherofflineRLschemes:(a)to(c)usingadataset
of20%oftheexperienceofonlineDQNand(d)to(f)usingadatasetof10%oftheexperienceofonlineDQN.
REFERENCES [12] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski
[1] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. et al., ‚ÄúHuman-level control through deep reinforcement learning,‚Äù
Liang, and D. I. Kim, ‚ÄúApplications of deep reinforcement learning nature,vol.518,no.7540,pp.529‚Äì533,2015.
in communications and networking: A survey,‚Äù IEEE Communications [13] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,‚ÄúProx-
Surveys&Tutorials,vol.21,no.4,pp.3133‚Äì3174,2019. imalpolicyoptimizationalgorithms,‚ÄùarXivpreprintarXiv:1707.06347,
[2] M.Zangooei,N.Saha,M.Golkarifard,andR.Boutaba,‚ÄúReinforcement 2017.
learning for radio resource management in RAN slicing: A survey,‚Äù [14] T.Haarnoja,A.Zhou,K.Hartikainen,G.Tucker,S.Ha,J.Tan,V.Ku-
IEEECommunicationsMagazine,vol.61,no.2,pp.118‚Äì124,2023. mar, H. Zhu, A. Gupta, P. Abbeel et al., ‚ÄúSoft actor-critic algorithms
[3] F.Meng,P.Chen,andL.Wu,‚ÄúPowerallocationinmulti-usercellular andapplications,‚ÄùarXivpreprintarXiv:1812.05905,2018.
networks with deep Q learning approach,‚Äù in ICC 2019 - 2019 IEEE [15] E. Eldeeb, J. M. de Souza Sant‚ÄôAna, D. E. Pe¬¥rez, M. Shehab, N. H.
InternationalConferenceonCommunications(ICC),2019,pp.1‚Äì6. Mahmood,andH.Alves,‚ÄúMulti-UAVpathlearningforageandpower
[4] H.-S. Lee, D.-Y. Kim, and J.-W. Lee, ‚ÄúRadio and energy resource optimizationinIoTwithUAVbatteryrecharge,‚ÄùIEEETransactionson
managementinrenewableenergy-poweredwirelessnetworkswithdeep VehicularTechnology,vol.72,no.4,pp.5356‚Äì5360,2022.
reinforcement learning,‚Äù IEEE Transactions on Wireless Communica- [16] A.Kumar,A.Zhou,G.Tucker,andS.Levine,‚ÄúConservativeQ-learning
tions,vol.21,no.7,pp.5435‚Äì5449,2022. for offline reinforcement learning,‚Äù in Advances in Neural Information
[5] N.Naderializadeh,J.J.Sydir,M.Simsek,andH.Nikopour,‚ÄúResource Processing Systems, vol. 33. Curran Associates, Inc., 2020, pp.
management in wireless networks via multi-agent deep reinforcement 1179‚Äì1191. [Online]. Available: https://proceedings.neurips.cc/paper
learning,‚Äù IEEE Transactions on Wireless Communications, vol. 20, files/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf
no.6,pp.3507‚Äì3523,2021. [17] W.Dabney,M.Rowland,M.Bellemare,andR.Munos,‚ÄúDistributional
[6] N. NaderiAlizadeh, M. Eisen, and A. Ribeiro, ‚ÄúLearning resilient reinforcementlearningwithquantileregression,‚ÄùinProceedingsofthe
radioresourcemanagementpolicieswithgraphneuralnetworks,‚ÄùIEEE AAAIConferenceonArtificialIntelligence,vol.32,no.1,2018.
TransactionsonSignalProcessing,vol.71,pp.995‚Äì1009,2023. [18] E. Eldeeb, H. Sifaou, O. Simeone, M. Shehab, and H. Alves,
[7] Y. Hua, R. Li, Z. Zhao, X. Chen, and H. Zhang, ‚ÄúGAN-powered ‚ÄúConservativeandrisk-awareofflinemulti-agentreinforcementlearning
deep distributional reinforcement learning for resource management in for digital twins,‚Äù 2024. [Online]. Available: https://arxiv.org/abs/2402.
networkslicing,‚ÄùIEEEJournalonSelectedAreasinCommunications, 08421
vol.38,no.2,pp.334‚Äì349,2020. [19] N.NaderializadehandA.S.Avestimehr,‚ÄúITLinQ:Anewapproachfor
[8] S. Levine, A. Kumar, G. Tucker, and J. Fu, ‚ÄúOffline reinforcement spectrumsharingindevice-to-devicecommunicationsystems,‚Äùin2014
learning: Tutorial, review, and perspectives on open problems,‚Äù arXiv IEEEInternationalSymposiumonInformationTheory,2014,pp.1573‚Äì
preprintarXiv:2005.01643,2020. 1577.
[9] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Rein-
forcementLearning. MITPress,2023,http://www.distributional-rl.org.
[10] K.Yang,C.Shi,C.Shen,J.Yang,S.-p.Yeh,andJ.J.Sydir,‚ÄúOffline
reinforcement learning for wireless network optimization with mixture
datasets,‚Äù IEEE Transactions on Wireless Communications, pp. 1‚Äì1,
2024.
[11] 3GPP, ‚ÄúSimulation assumptions and parameters for FDD HeNB RF
requirements,‚ÄùTech.Rep.R4-092042.