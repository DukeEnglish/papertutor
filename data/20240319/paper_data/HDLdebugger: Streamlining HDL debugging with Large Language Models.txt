HDLdebugger: Streamlining HDL debugging with Large
Language Models
XufengYaoâˆ— HaoyangLiâˆ— TszHoChanâˆ— WenyiXiao
CUHK&Huawei Huawei Huawei Huawei
HongKongSAR,China HongKongSAR,China HongKongSAR,China HongKongSAR,China
xfyao@cse.cuhk.edu.hk li.haoyang1@huawei.com chantszho1@huawei.com wxiaoae@cse.ust.hk
MingxuanYuan YuHuang LeiChenâ€  BeiYuâ€ 
Huawei HiSilicon HKUST&HKSUT(GZ) CUHK
HongKongSAR,China ShenZhen,China HongKongSAR,China HongKongSAR,China
Yuan.Mingxuan@huawei.com huangyu61@hisilicon.com leichen@cse.ust.hk byu@cse.cuhk.edu.hk
ABSTRACT ACMReferenceFormat:
Inthedomainofchipdesign,HardwareDescriptionLanguages XufengYaoâˆ—,HaoyangLiâˆ—,TszHoChanâˆ—,WenyiXiao,MingxuanYuan,
YuHuang,LeiChenâ€ ,andBeiYuâ€ .2024.HDLdebugger:StreamliningHDL
(HDLs)playapivotalrole.However,duetothecomplexsyntaxof
HDLsandthelimitedavailabilityofonlineresources,debugging
debuggingwithLargeLanguageModels.InProceedingsof (KDDâ€™24).ACM,
NewYork,NY,USA,13pages.https://doi.org/XXXXXXX.XXXXXXX
HDLcodesremainsadifficultandtime-intensivetask,evenfor
seasonedengineers.Consequently,thereisapressingneedtode-
velopautomatedHDLcodedebuggingmodels,whichcanalleviate 1 INTRODUCTION
theburdenonhardwareengineers.Despitethestrongcapabilities HardwareDescriptionLanguages(HDLs)arecrucialintherealmof
ofLargeLanguageModels(LLMs)ingenerating,completing,and chipdesign,servingasthecornerstoneforcreating,testing,andim-
debuggingsoftwarecode,theirutilizationinthespecializedfield plementingdigitalsystems[9,13,48].Duetotheircriticalrole,the
ofHDLdebugginghasbeenlimitedand,todate,hasnotyielded domainofHDLdebugginghasreceivedcomparativelyscantatten-
satisfactoryresults.Inthispaper,weproposeanLLM-assistedHDL tion.Traditionaldebuggingapproachesprimarilyinvolvemanual
debuggingframework,namelyHDLdebugger,whichconsistsof codecorrectionbasedonsyntacticguidelines,followedbyiterative
HDLdebuggingdatagenerationviaareverseengineeringapproach, testingthroughcompilers.Thisprocess,whilestraightforwardfor
asearchengineforretrieval-augmentedgeneration,andaretrieval- languagessuchasPythonandJava,becomesmarkedlymorecom-
augmentedLLMfine-tuningapproach.Throughtheintegration plexforHDLsduetotheirsophisticatedsyntaxandthescarcity
ofthesecomponents,HDLdebuggercanautomateandstreamline ofaccessibleresourcesonline.Furthermore,compiler-basedtest-
HDLdebuggingforchipdesign.Ourcomprehensiveexperiments, ingofHDLcode,especiallyinthecontextofchipdevelopment,is
conductedonanHDLcodedatasetsourcedfromHuawei,reveal exceptionallytime-consumingandresource-intensive.
that HDLdebugger outperforms 13 cutting-edge LLM baselines, DespitethehighdemandintheindustryforeffectiveHDLdebug-
displayingexceptionaleffectivenessinHDLcodedebugging. gingtechniquesandthepromisingdirectionstheyoffer,existing
methodologiesoftenfallshortinaddressingthecomplexitiesofthe
CCSCONCEPTS problem.Forexample,thetemplate-basedmethod[15,17,20,25],a
traditionalstrategyincodedebugging,utilizesexpert-definedcode
â€¢Theoryofcomputationâ†’Programsemantics;â€¢Computing
patternsorheuristicstoidentifyandcorrecterrors.However,this
methodologiesâ†’Naturallanguageprocessing;â€¢Hardware
approachisinherentlylimited,capableofrectifyingonlythoseer-
â†’Hardwaredescriptionlanguagesandcompilation.
rorswithpredefinedpatterns.Consequently,itlackstheflexibility
andadaptabilitynecessarytotackleadiversearrayofbugs.
KEYWORDS
Recently,researchershavedelvedintothedirectapplicationof
Code Debugging, Large Language Model, Retrieval Augmented
largelanguagemodels(LLMs)torectifybuggycode.Theunderly-
Generation
inghypothesisisthatLLMs,pre-trainedonextensiverepositories
âˆ—Equalcontribution,â€ Correspondingauthor, ofopen-sourcecodesnippetsandtext,suchasPython,caneffec-
ThisworkwascompletedduringXufengYaoâ€²sinternshipatHuawei. tivelydiscernbugpatternsandautomaticallyrepairbuggycode.
ToassesstheefficacyofcurrentLLM-basedapproachesinaddress-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor ingindustry-levelHDLdebuggingchallenges,weconductapilot
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation experimentonthreetypicalmethodsasshowninTable1.Among
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe themethodsevaluated,GPT4[1]isthecurrentstate-of-artLLM,
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
RTLFixer[35]leveragesretrievalaugmentedgeneration(RAG)[12]
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. and advanced prompt engineering [46], specifically tailored for
KDDâ€™24,August25â€“29,2024,Barcelona,Spain HDLdebuggingtasks.VeriGen[34]isahardwarelargelanguage
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. modeltrainedbyaself-containedhardwaredataset.Despitethese
ACMISBN978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX advancedapproaches,ourobservationsindicatethatnoneofthe
4202
raM
81
]RA.sc[
1v17611.3042:viXraKDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
Table1:PilotDebuggingExperiments Table2:Importantnotations.
Method GPT4[1] RTLFixer[35] VeriGen[34] Notation Description
Pass-rate@1 6.35% 28.35% 1.34% ğ‘ğ‘– Buggycode
ğ‘’ğ‘— Errorid
ğ‘šğ‘– Errormessagesforğ‘ğ‘–
ğ‘ğ‘– Correctcodeforğ‘ğ‘–
methodsdeliveredresultsthatmetourcriteriaforsatisfactionin
(ğ‘‘ğ‘—,ğ‘Ÿğ‘—,ğ‘ ğ‘—) Descriptionsğ‘‘ğ‘—,reasonsğ‘Ÿğ‘—,andpotentialsolutionsğ‘ ğ‘— forğ‘’ğ‘—
thecontextofindustry-levelHDLdebuggingscenarios.Aprimary ğ·ğ‘’,ğ·ğ‘ errordatabase,codedatabase
contributingfactortothisshortfallistheinsufficiencyofHDLcode zğ‘–ğ‘¤ Keywordvectorforbuggycodeğ‘ğ‘–witherrormessageğ‘šğ‘–
resourcesfortraining.Consequently,thesepre-trainedLLMsstrug- ğ‘ ğ‘–ğ‘š(ğ¼ğ‘–,ğ¼ğ‘—) Similaritybetweenbuggycodesğ¼ğ‘–andğ¼ğ‘—
gletoaccuratelycomprehendthesyntaxandfunctionalityinherent ğ‘=(ğ‘,ğ‘’) Codequeryconsistingofbuggycodeğ‘anderrormessageğ‘š
toHDLcodes. ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ ThedocumentRAGbasedonerrormessageğ‘šğ‘–andğ‘ğ‘–
Totackletheproblem,weproposeanHDLdebuggingframe- ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘ ThecodeRAGbasedonbuggycodeğ‘ğ‘–
work,namelyHDLdebugger,whichconsistsofthreecomponents,
ğ‘ğ‘¡ Promptofthoughtgeneration
i.e.,datageneration,searchengine,andretrieval-augmentedLLM
ğ‘ğ‘ Promptofbuggycodecorrection
fine-tuning.Firstly,thedatagenerationproceduretargetsovercom-
ğ‘¡ğ‘– Thethoughtforsolvingbuggycodeğ‘ğ‘–
ingtheobstacleofthelimitedavailabilityofHDLbugs.Specifically,
weemployreverseengineeringtoinsertspecificmodificationsinto
theoriginalerror-freecode.Therefore,wecanproducecorrespond-
ingbuggyversionsanderrormessagesviacompilers,whichare
usedtoconstructacodedatabaseandfurtherfine-tuneLLMs.Sec-
Buggy Code Error Message
ondly,weproposeaneffectiveandefficientsearchengine,which Data Generation Input
issupportedbythecodedatabaseconstructedbythedatagener- Database Search
Input
ationapproachandthedocumentdatabasewithvariousinternal
HDL documents which contain relevant information for buggy
Code RAG
codes.Givenabuggycodeanditserrormessage,thesearchen-
Search Engine
gineretrievesrelevantinformation(i.e.,documentRAG)andbuggy LLM Thoughts Correct Code
codes(i.e.,codeRAG)withsimilarpatternsfromthedocument Database Input
databaseandcodedatabase,respectively.ThedocumentRAGand
Doc RAG
codeRAGarecrucialforboththefine-tuningandinferencestages HDL doc
ofourretrieval-augmentedLLM,enhancingtheabilityofLLMs Figure1:FrameworkoverviewofHDLdebugger.
tocomprehensivelyunderstandtheHDLbuggycodeandrepair
iteffectively.Thirdly,toenhancetheabilityofLLMstogenerate
accuratecodesolutions,weproposeanovelfine-tuningapproach
2 METHODOLOGY
forLLMs.Thisapproachincorporatesaself-guidedthoughtgener-
ationmechanismandaretrieval-augmentedfine-tuningprocess, Thissectionprovidesacomprehensiveoverviewoftheproposed
significantlyimprovingtheLLMâ€™sperformanceindebuggingHDL HDLdebugger.Initially,wedelveintothebuggycodegeneration
code. pipeline,asdetailedinSection2.1.Subsequently,thesearchengine
Ourcontributionsaresummarizedasfollows: mechanismtailoredforRetrieval-AugmentedGeneration(RAG)is
presentedinSection2.2.TheRetrieval-AugmentedLLMfine-tuning
â€¢ WeintroduceanadvancedLLM-basedHDLdebuggingframe- iselaborateduponinSection2.3.Theimportantnotationsinour
worksupportingchipdesignsintheindustry,namelyHDLde- paperareshowninTable2.
bugger,whichconsistsofbuggydatageneration,searchengine, Framework.AsshowninFig.1,givenabuggycodeanditsasso-
andretrieval-augmentedLLMfine-tuning. ciatederrormessage,ourproposedHDLdebuggertargetstorepair
â€¢ Toaddressthescarcityofhigh-qualityHDLdebuggingtraining thisbuggycodeintothecorrectone.Specifically,wefirstproposea
data,weproposeadatagenerationapproachbasedonreverse datagenerationapproachinSec.2.1togenerateasetofHDLcode
engineeeringtocomprehensivelygeneratediverseandrealistic instances,whereeachinstanceconsistsofbuggycode,errormes-
HDLbuggycodeswiththecorrectversion. sages,anditscorrectversion.ThesegeneratedHDLcodeinstances
â€¢ WeproposeasearchenginetocreatecodeRAG(resp.docRAG) willbeusedtoprovidecontextforbuggycodequeriesandfine-tune
forHDLbuggycode(resp.relevantinformation)effectivelyand theLLMs.Second,weproposeasearchengineinSec.2.2,which
efficiently,enhancingthefine-tuningandinferenceofLLMs. targetstoretrieverelevanttextinformation(i.e.,documentRAG)
â€¢ Wepresentanovelretrieval-augmentedfine-tuningapproach forerrormessagesandretrievebuggycodeswithsimilarbuggy
forHDLdebugging,whichintegratesself-guidedthoughtgen- patterns(i.e.,codeRAG).Then,HDLdebuggertakesthebuggycode,
erationwithRAG-basedfine-tuningstrategies. itserrormessage,taskprompt,documentRAG,andcodeRAGto
â€¢ ExtensiveexperimentsontheHDLcodedatasetfromHuawei theLLMs,andenablesLLMstopredictthecorrectcode.Specifically,
demonstratesuperiorperformanceagainst13state-of-the-art weintroducearetrieval-augmentedfine-tuneapproachtofine-tune
baselines,includingGPT4andvariousHDLdebuggingLLMs. theLLMsforHDLcodedebugginginSec.2.3.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
Step 1: Modification Function Generation
HDL Documents Error Reasons Modification Functions Q Eu rre or ry InR foe rle mv aa tn iot n L Sin ee lfa -r A+ tS teo nft tm ioa nx
Wrong values in script FuncValue_modify Message
HDLmanualExpertnotesUser logs ReC dy uc nle d alo ns tt i nin s ts rc ur ci tp iot n FF unu cnc InC sy trc ul ce t_ iod nel _e at de d Parser Search DaE tr ar bo ar se Bi-LSTM
S onu m thm e a Hri Dze L eP mrr r aoo nrm u r ap e lat ,s s eo xn ps e b rta s ne od te s, LLM Wr po lani cgn e a s isc nsr ii sgp cnt rm ipe tnt FuncAssig â€¦nment_move Documents Store BERT BERT BERT BERT
and user logs. â€¦ ğ‘ ! ğ‘ " â€¦ ğ‘ #
(a) RAG for the error message (b) BERT-LSTM
Step 2: Sample Generation
BugQ gyue Cr oy de+ TF-IDF Search Keywords
, , Error Message Extractor Store Vector DB Top-ğ‘ Top-ğ¾
buggy codeerror messagecorrect code Code Database Search Candidates Reranker Candidates
Generated Samples Error Message Compiler Buggy Code Correct Code (BuggyCode+ BERT- Semantic
Error Message, LSTM Store Vector DB
Correct Code) (c) RAG for the buggy code
Figure2:ReverseEngineeringPipelineforinducingerrorto
correctcodebydiversemodificationfunctionssummarized Figure3:Thesearchenginetoretrieverelevantinformation
fromHDLdocuments. givenerrormessagesandretrievesimilarbuggycodesbased
onabuggycodequery.
2.1 HDLBuggyDataGeneration
UnlikesoftwarelanguageslikePythonorC++,wherecodecan codes,wesystematicallyintroduceerrors,therebyproducingaset
oftenbecrawledfromopenwebsitesandplatformslikeGitHub, ofbuggycodes.Particularly,wecanapplyvariousmodification
HDLcodes,particularlythoseusedforchiptesting,arerarelymade functionstooneHDLcode,whichallowsustogeneratemultiple
publicduetoprivacyandcommercialconcerns.Thislimitation instancesofbuggycode.Next,weemployanHDLcompilerto
presentsasignificantobstacleonfine-tuningLLMsinthedomain compiletheseintentionallybuggycodesontheirrespectivede-
ofHDL.Inthissection,wewillintroduceareverseengineering sign,whichinevitablyresultsincompilationerrors.Then,foreach
pipelineforgeneratinghigh-qualityHDLcodepairsthatconsistof correctHDLcodeğ‘ ğ‘– âˆˆğ¶,wecancollectoneofitsbuggycodeğ‘ ğ‘–
bothbuggyandcorrectedversions.AsshowninFig.2,TheHDL withassociatederrormessageğ‘š ğ‘– asaninstanceğ¼ ğ‘– = (ğ‘ ğ‘–,ğ‘š ğ‘–,ğ‘ ğ‘–)
datagenerationconsistsoftwosteps,i.e.,modificationfunction oftrainingdata.Normally,diagnosingandrectifyingHDLerrors
generationandsamplegeneration. requiretheexpertiseofchipengineers.Giventhatourinstructions
systematically brought the errors, we can employ reverse engi-
2.1.1 ModificationFunctionGeneration. Thefirststeptargetsto neeringtoidentifythefaultsandproducesolutionsdirectly.This
generateasetofhigh-qualitymodificationfunctions.Thesefunc- bypassestheneedformanualerrordiagnosis,streamingtheprocess
tionsarethenemployedtomodifyHDLcodesprovidedbyindustry ofcreatingavastarrayofcomprehensivedatasetsoferrorscripts,
engineers,therebygeneratingadiversecollectionofbuggycodeex- errormessages,andcorrespondingsolutions.Thismethodensures
amples.Toensurethatthemodifiedcodesexhibitarangeofrealistic arichdiversityinthetypesoferrorsproduced,whichiscriticalfor
a bn yd led vi ev re ar gse ine grr to hr ep ca at pt ae brn ils it, iw ese oc fo Ln Lst Mru sc at nth de am coo mdi pfi rc ea hti eo nn sif vu en cc oti lo len cs
-
creatinganextensiveandeffectivetrainingdatasetğ·
ğ‘
={ğ¼ ğ‘—}| ğ‘—ğ· =1ğ‘|.
tionofindustrialHDLdocuments.Specifically,asshowninFig.2,
2.2 SearchEngineforRAG
wefirstcollectasetofHDLdocuments,includingtheHDLmanual,
expertnotes,anduserlogs.Subsequently,wethencarefullydesign Inthissubsection,weproposeasearchenginetooptimizeretrieval-
apromptthatguidestheLLMstoextractandsummarizeprevalent augmentedgenerations(RAG)forretrievingrelevantinformation
andcriticalerrorpatternsinHDL,suchassyntaxmisuseorlogical intheHDLdocumentsandcodesinstances.TheretrievedRAG
errors.Withtheseinsights,weproceedtodevelopthemodification contentwillserveascontextualinformationforqueries,thereby
functions.Thedistilledfunctionsfocusonsimpleoperationssuch enhancingthecapabilityofLLMstounderstandbuggycontext
asadding,deleting,modifying,andadjustingsegmentswithHDL informationandidentifyissueswithinbuggycodes.Weillustrate
scripts,whiletheuniquesetofrulesgoverningHDLensuresthat theoverviewofsearchengineframeworkinFig.3.
similaroperationscanresultinvastlydifferenterrorsrecordedin
HDLdocuments.Thesefunctionsexplicitlyintroduceerrorsinto 2.2.1 DocumentRAG. AsshowninFig.3(a),wefirstcollecta
comprehensivecollectionofinstructionaldocumentsforthisHDL,
theoriginalcorrectHDLcodesthatmirrorthosecommonlyen-
encompassinglanguagespecifications,errordiagnostics,andtrou-
counterederrorsinindustry,therebycreatinganinvaluabledataset
bleshootingtechniques.Then,wemeticulouslycuratethecontent
tofine-tuneLLMs.
ofdocuments,distillingadedicatederrordatabaseğ· tailoredto
ğ‘’
2.1.2 SampleGeneration. Inthisstep,wegatherabroadrangeof thisHDL.Thiserrordatabasecontainsdetailederrordescriptions
accurateandhigh-qualityHDLcodesğ¶ ={ğ‘ ğ‘–} ğ‘–|ğ¶ =1| fromexperienced ğ‘‘ ğ‘—,underlyingreasonsğ‘Ÿ ğ‘—,andsuggestedremedialstrategiesğ‘  ğ‘— for
chipengineers.Thesecodes,whichhavebeenutilizedacrossvari- eacherroridğ‘’ ğ‘—. Weillustrateexamplesoferrorinformationinerror
ouschipdesigns,arecomprehensivetoencompassawiderange databaseinTab.7intheAppendix.Givenanerrormessagequery
offunctionaltestingscenariosforchips.ThesevariousHDLcodes ğ‘š ğ‘–,wefirstparseğ‘š ğ‘– toextractğ‘› ğ‘’,ğ‘– constituenterrorcodes{ğ‘’ ğ‘—}ğ‘› ğ‘—=ğ‘’, 1ğ‘–.
serveastheseedcodeforerrorcaseconstruction.Specifically,by Subsequently,weretrievethedescriptions,reasons,andpotential
applyingthepreviousmodificationfunctionstothesecorrectHDL solutionsforeachidentifiederrorcodefromourerrordatabase.KDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
Then, the document RAG of the queryğ‘š can be assembled as similarityscore.Inthesecond-rankingstage,ourgoalistopinpoint
ğ‘–
ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ = {(ğ‘’ ğ‘—,ğ‘‘ ğ‘—,ğ‘Ÿ ğ‘—,ğ‘  ğ‘—)}ğ‘› ğ‘—=ğ‘’, 1ğ‘–,therebyhelpingLLMstounderstand thetop-ğ‘˜ relevantyetdiversebuggycodes.Theseselectionsare
theerrormessageğ‘š . aimedatprovidingabroaderrangeofbuggypatterns,whichcan
ğ‘–
helpLLMrepairthebugsinthequery.Formally,givenğ‘ buggy
2.2.2 BuggyCodeRAG. AsshowninFig.3(c),inthecoderetrieval instancesğ·Ë† ğ‘ğ‘ andthequerybuggycodeğ‘ = (ğ‘,ğ‘’),weselectthe
component,wemaintainacodedatabaseğ· ğ‘ = {ğ¼ ğ‘–} ğ‘–|ğ· =1ğ‘|,where top-ğ‘˜relevantanddiversecodeğ· ğ‘ğ‘ bymaximizingthefollowing
each code instanceğ¼ ğ‘– = (ğ‘ ğ‘–,ğ‘š ğ‘–,ğ‘ ğ‘–) consists of a buggy codeğ‘ ğ‘–, objective:
itsassociatederrormessagesğ‘š ğ‘–,andthecorrectcodeğ‘ ğ‘–.Givena âˆ‘ï¸ 1 âˆ‘ï¸ ğ‘
q asu se or cy iağ‘ te= d( eğ‘ r, rğ‘š or) mth ea st sai gn ecl ğ‘’u ,d te hs ea ais mnip op fe tht eof cob du egg Ry Ac Go id se tğ‘ ow idi et nh tii ft ys ğ·m ğ‘ğ‘ âŠ†a ğ·x Ë† ğ‘ğ‘
ğ¼ğ‘–âˆˆğ·
ğ‘ğ‘ğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)+ ğ‘˜ Â·
ğ¼ğ‘–âˆˆğ·
ğ‘ğ‘ğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ· ğ‘), (2)
a simsu ib las ret toğ· tğ‘ğ‘ heâŠ† bğ· ugğ‘ go yf ct oh de et ğ‘op i- nğ‘˜ thco ed qe ui en rs yt .a In nc se us ct hha at wh aa yv ,e thth ee bm ugo gs yt wheredistanceğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ· ğ‘ğ‘ ) =min ğ¼ğ‘—âˆˆğ· ğ‘ğ‘ \ğ¼ğ‘– (2âˆ’ğ‘ ğ‘–ğ‘š(ğ¼ ğ‘–,ğ¼ ğ‘—))denotes
thediversityvaluebetweeneachinstanceğ¼ andtheotherinstances
ğ‘–
codeğ‘ hassimilarbuggypatternswiththebuggycodesineach ğ‘ ğ‘
ğ‘ inğ· ğ‘ andğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ· ğ‘) âˆˆ [0,2].Eq.(2)isanNP-hardproblem,which
instanceğ¼ ğ‘– = (ğ‘ ğ‘–,ğ‘š ğ‘–,ğ‘ ğ‘–) âˆˆ ğ· ğ‘.Thus,thecorrectcodeğ‘ ğ‘– foreach can be reduced from the well-knownğ‘˜-clique problem [36] by
buggy codeğ‘ can be provided to LLMs. As a result, LLMs can
ğ‘– settingğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–) = 1 for all ğ¼ ğ‘– âˆˆ ğ· ğ‘. Therefore, we propose a
usethelearnedpatternsfromthetop-ğ‘˜similarinstancestofixfor
greedyalgorithmwithanapproximationratio1âˆ’1/ğ‘’toidentify
thebuggycodeğ‘inthequery.Specifically,wefirstintroducehow
thetop-ğ‘˜relevantbuggycodes.Thedetailsofthegreedyalgorithm
tolearnlow-dimensionalvectorsforbuggycodesandthenthen
andapproximationratioareintroducedinAppx.B.2.Thus,given
introduceatwo-stagerankerthatretrievesthetop-ğ‘˜buggycode
abuggycodeğ‘ anderrormessageğ‘š ,wecangeneratethecode
ğ‘– ğ‘–
instancesforcodequeryğ‘. retrieval-augmentedgenerationğ‘Ÿğ‘ğ‘” ğ‘–ğ‘ ={(ğ‘ ğ‘—,ğ‘š ğ‘—,ğ‘ ğ‘—)}ğ‘˜ ğ‘—=1.
VectorDatabaseConstruction.Giventhecomplexityandlength
of HDL code and associated error messages, as well as similar- 2.3 Retrieval-augmentedLLMFine-tuning
lookingcodesnippetscontainingdistinctbuggypatterns,comput-
Inthissubsection,weintroducehowtofine-tunetheLLMsbased
ingthesimilaritybetweenbuggycodesischallenging.Toaddress
onthetrainingdatasetconstructedinSec.2.1andthesearchengine
thischallenge,weproposetomeasurethesimilaritybetweenbuggy
proposedinSec.2.2.Specifically,wefirstproposetogeneratea
fromtwoaspects,i.e.,keywordsimilarityandsemanticsimilarity.
thoughttohelpLLMrepaireachbuggycodeinthetrainingdataset.
First,weextractwordsfromallbuggycodesandtheirerrormes-
Second,basedonthegeneratedthoughtandtheretrievedbuggy
sages and use the TF-IDF [2] technique to compute the weight
codes,weproposearetrieval-augmentedsupervisedfine-tuning
ofeachword.Then,wecangeneratethekeywordvectorz ğ‘–ğ‘¤ for
techniqueforLLMs.
eachbuggycodeğ‘ withitserrormessageğ‘š .Second,wedesigna
ğ‘– ğ‘–
BERT-LSTMmodelthatcombinesBERT,foritspowerfullanguage 2.3.1 Self-guidedThoughtGeneration. Onestraightforwardway
understandingcapabilities,withanLSTM,foritssequentialdata istofeedbuggycodeanderrormessagestoLLMsandletLLMs
processingstrengths.TheBERT-LSTMmodelencodesabuggycode directlypredictthecorrectcodes.However,thisapproachisinsuf-
ğ‘
ğ‘–
withitserrormessageğ‘š
ğ‘–
intoalow-dimensionalembeddingzğ‘  ğ‘–. ficientforLLMstodeeplycomprehendtheproblemandprovide
Specifically,asintroducedinSec.2.1,thebuggycodesaregener- accuratesolutions.Recentresearch[40,45]suggeststhatwhen
atedbydifferentmodificationfunctions,andherewetakethese LLMsarepromptedtoproduceaseriesofintermediateandex-
modificationfunctionsasthelabelsforeachbuggyandoptimize planatory thought before finally outputting the solution to the
theBERT-LSTMmodel.Then,weusethefinalrepresentationof giventask,theperformanceofLLMscanbesignificantlyimproved.
BERT-LSTMasthesemanticembeddingforeachbuggycodeand Itisbecausethesereasoningthoughtsimprovetheunderstanding
itserrormessages.ThearchitecturedetailsofBERT-LSTMmodels ofLLMsontheinputtasksandthusgeneratemorerelevantandac-
areintroducedinAppx.B.1.BasedontheTF-IDFandBERT-LSTM curateoutputs.Therefore,beforefine-tuningtheLLMs,wepropose
models,webuildakeywordvectordatabaseandasemanticvector togeneratehigh-qualitythoughtforeachtrainingcodeinstance.
database,whichtogetherfacilitatearobustframeworkforanalyz- Specifically,asillustratedinFig.4,wefirstdesignapreciseand
ingthesimilaritybetweeninstancesofbuggyHDLcode. explicitpromptğ‘ ğ‘¡ toclarifythethoughtgenerationtaskforLLMs.
Followingthis,weinputthethoughtgenerationpromptğ‘ ,the
Two-stageRanker.Ingeneral,wedesignatwo-stageranking ğ‘¡
buggycodeğ‘ ,itsassociatederrormessageğ‘š ,andthedocument
approachtoidentifythetop-ğ‘˜mostrelevantbuggycodeinstances ğ‘– ğ‘–
RAGğ‘Ÿğ‘ğ‘”ğ‘‘,anditscorrectversionğ‘ intotheLLMwithitsinference
toagivenqueryofbuggycode.Inthefirst-rankingstage,forany ğ‘– ğ‘–
givenbuggycodequeryğ‘=(ğ‘,ğ‘’),wefirstusetheTF-IDFencoder
modeğ¿ğ¿ğ‘€1.ThisenablestheLLMtogenerateathoughtğ‘¡
ğ‘–
onhow
andBERT-LSTMencodertogeneratethekeywordvectorzğ‘¤ and torepairthebuggycodeğ‘ ğ‘– intothecorrectcodeğ‘ ğ‘– asfollows:
ğ‘–
semantic vector zğ‘  ğ‘–. Then, we define the similarity between the ğ‘¡
ğ‘–
=ğ¿ğ¿ğ‘€1(ğ‘
ğ‘¡
â—¦ğ‘
ğ‘–
â—¦ğ‘š
ğ‘–
â—¦ğ‘
ğ‘–
â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ ), (3)
querycodeğ‘andeachinstanceğ¼
ğ‘–
âˆˆğ·
ğ‘
inthecodedatabaseğ·
ğ‘
as:
whereâ—¦meansconcatenateoperator.Weomitthegeneraltask
ğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)=ğœ†Â·ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(zğ‘¤,zğ‘–ğ‘¤ )+(1âˆ’ğœ†)Â·ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(zğ‘ ,zğ‘  ğ‘–)+1, (1) requirementspromptforsimplification.Empirically,wefindthat
whenLLMsgeneratethethoughtğ‘¡ onlyonce,theoutputmightbe
ğ‘–
whereğœ†âˆˆ [0,1]isahyper-parameterbetweensemanticsimilarity irrelevanttothebuggycodeğ‘ ğ‘–orincorrectduetothehallucination
andkeywordsimilarityandğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–) âˆˆ [0,2].Weselectthetop- phenomenonandrandomnessofLLMs.Therefore,toguaranteethe
ğ‘
similarinstancesğ·Ë†ğ‘
fromthecodedatabaseğ· basedonthe qualityofthegeneratedthought,weiterativelygenerateğ¿different
ğ‘ ğ‘HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
thoughtsğ‘‡ ğ‘– ={ğ‘¡ ğ‘–,ğ‘—}ğ¿ ğ‘—=1foreachbuggycodeğ‘ ğ‘– byrunningğ¿ğ¿ğ‘€1ğ¿ #1: Task Requirement Prompt
timesfollowingEq.(3).Thisisachievedbymodifyingtemperature Now you are expert in HDL(Hardware Description Language).
parametersandthedetailsaredescribedinappendix. Your task involves debugging in HDL. You are given snippets of HDL
script that contain errors.
Toassessthequalityofeachthoughtğ‘¡
ğ‘–,ğ‘—
âˆˆğ‘‡ ğ‘–,weadoptaself-
Your objective is to identify these errors and provide helpful
guidancestrategytoselectthehighestqualitythoughtfromthe instructions to fix the bug.
thoughtsetğ‘‡ forthebuggycodeğ‘ .Specifically,wefeedthebuggy
ğ‘– ğ‘–
#2: Buggy Code #3: Correct Code
codeğ‘ ,itsassociatederrormessageğ‘š ,andthedocumentRAG
ğ‘– ğ‘–
ğ‘Ÿğ‘ğ‘”ğ‘‘ totheLLM.Apromptğ‘ ,"Basedontheanalysis,thecorrect cycle { cycle {
ğ‘– ğ‘ assign clk_s_top 0 assign clk_s_top 0
scriptis,"isappendedtoguidetheLLMtowardsgeneratingthe
assign clk_top_core_3 0 }
predictedcorrectscript,representedas:
} cycle {
ğ‘Ë†
ğ‘–,ğ‘—
=ğ¿ğ¿ğ‘€1(ğ‘
ğ‘
â—¦ğ‘
ğ‘–
â—¦ğ‘š
ğ‘–
â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ â—¦ğ‘¡ ğ‘–,ğ‘—). (4) c y c al se
s
i{
gn clk_s_top 1 }
assign clk_s_top 1
Afterweobtaintheoutputğ‘Ë† ğ‘–,ğ‘— foreachthoughtğ‘¡ ğ‘–,ğ‘—,weemploy } (cid:1)
theeditdistancemetric[26]toevaluatethesimilaritybetweenthe (cid:1)
predictedcorrectcodeğ‘Ë† andtheground-truthofcorrectedcode
ğ‘–,ğ‘—
#4: Error Messages #5: Retrieved Information
ğ‘ as
ğ‘– â€¦ â€¦
ğ‘‘ ğ‘–,ğ‘— =ğ¸ğ‘‘ğ‘–ğ‘¡ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘ ğ‘–,ğ‘Ë† ğ‘–,ğ‘—). (5) Error: Can't find core when The port defined at the core level is
set prt cat top hierarchy. not consistent with the actual port.
Intuitively,ifthedistanceğ‘‘ betweenthepredictedcorrectcode
ğ‘–,ğ‘— Error: RULE(PRT-1)() test Reconnect the port that is not
ğ‘Ë† ğ‘–,ğ‘— andthegroundtruthofcorrectedcodeğ‘ ğ‘–issmaller,thethought failed" connected to the top-level.
ğ‘¡ ismorehelpfultoLLMstorepairthebuggycodeğ‘ .Therefore, â€¦ â€¦
ğ‘–,ğ‘— ğ‘–
weselectthethoughtwiththesmallesteditdistancesforeachğ‘ ,
ğ‘–
i D.e., =ğ‘¡ ğ‘– {= (ğ‘ ğ‘–m ,ğ‘šin ğ‘–ğ‘¡ ,ğ‘– ğ‘Ÿ,ğ‘— ğ‘âˆˆ ğ‘”ğ‘‡ ğ‘–ğ‘‘ğ‘–ğ‘‘ ,ğ‘Ÿğ‘–, ğ‘ğ‘— ğ‘”. ğ‘–ğ‘F ,i ğ‘¡n ğ‘–,a ğ‘l ğ‘–ly ), } ğ‘–w |ğ· =1e ğ‘|c .a Tn heob dt ea ti an ilsth oe ft thra ein thin og ugd ha tta gs ee nt - # P bul6 e g: a g sP ye r cpo oro dm v ei p bd at e s I e efn df is oct nir e u n thtc eat n eio rd r n oc rl e mar e sin sast gr eu sc t ai no dn s c f oo rr r em cto cd oif dy ei n pg ro t vh ie d g ediv . en
erationareillustratedinAlg.2intheAppendix. Your instructions should help improve the buggy code without
directly including any information from the correct code. The helpful
2.3.2 Retrieval-augmentedFine-tuning. Afterobtainingthefinal instruction is: Based on the error message
trainingdatasetD ={(ğ‘ ğ‘–,ğ‘š ğ‘–,ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘,ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘,ğ‘¡ ğ‘–,ğ‘ ğ‘–)} ğ‘–|ğ· =1ğ‘|,wesupervise
#7: Thought Generation
fine-tunetheLLMsbasedonbuggycodesandretrieval-augmented
Based on the error message, it appears that there is an inconsistency
generationinSec.2.2.Specifically,giveneachtraininginstance between the ports defined at the core level and the actual ports. To fix
ğ· ğ‘– = (ğ‘ ğ‘–,ğ‘š ğ‘–,ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘,ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘,ğ‘¡ ğ‘–,ğ‘ ğ‘–) âˆˆ D,wefirstfeedthoughtgener- t th oi ps
-
lb eu vg e,
l
.y Io nu
t
hsh iso cu ald
se
r ,e tc ho en pn oe rc tt "th cle
k
p _o tor pt _th ca ot
r
eis
_
n 3o
"
t
i
sc o nn on
t
e cc ot ne nd
e
t co
t
eth de
t o
ationpromptğ‘ ,buggycodeğ‘ ,itserrormessageğ‘š ,document
ğ‘¡ ğ‘– ğ‘– the top-level, so you should reconnect it
RAGğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘,codeRAGğ‘Ÿğ‘ğ‘” ğ‘–ğ‘,toatargetLLM,ğ¿ğ¿ğ‘€2togeneratethe
Figure4:Thoughtgenerationexample.
predictedthoughtğ‘¡Ë† asfollows:
ğ‘–
ğ‘¡Ë†
ğ‘–
=ğ¿ğ¿ğ‘€2(ğ‘
ğ‘¡
â—¦ğ‘
ğ‘
â—¦ğ‘
ğ‘–
â—¦ğ‘š
ğ‘–
â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘ ). (6)
3 EXPERIMENTS
Then,basedonthepredictedthoughtğ‘Ë† andcodecorrectionprompt
ğ‘¡
ğ‘ ğ‘,LLMğ¿ğ¿ğ‘€2predictsthecorrectcodeğ‘Ë†
ğ‘–
asfollows: 3.1 ExperimentSetting
ğ‘Ë†
ğ‘–
=ğ¿ğ¿ğ‘€2(ğ‘
ğ‘¡
â—¦ğ‘
ğ‘
â—¦ğ‘
ğ‘–
â—¦ğ‘š
ğ‘–
â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘ â—¦ğ‘¡Ë† ğ‘–). (7) 3 H.1 D.1 LcoH dD eL filD esat fa rs oe mts Hin uH awua ew i.e Ti h. eW see fig la et sh ae rr ea md eiv tie cr us le ouco sll yle cc uti ro an teo df
,
Following[33,39],wefine-tunethetargetLLMwithitstraining witheachfilebeingspecificallyutilizedfordistinctchipdesign
modeğ¿ğ¿ğ‘€2byusingtheconventionalnext-tokenpredictionobjec- scenarios,reflectingthevariedandspecializeddemandsofcircuit
tiveandminimizethecross-entropylossL D: design.EachHDLcodefilecontainsanextensivearrayofdata,
âˆ‘ï¸ includingvariableassignments,detailedcircuitdesigns,clocking
Lğ‘¡ğ‘– = logğ‘ ğ¿ğ¿ğ‘€2(ğ‘¤ ğ‘–,ğ‘—|ğ¶â—¦ğ‘¤ ğ‘–,<ğ‘—),
information,functiontestingprotocols,andvarioustestitemsthat
ğ‘¤ğ‘–,ğ‘—âˆˆğ‘¡ğ‘–
arecriticalforthechipdesignprocess.BasedontheseHDLcode
Lğ‘ğ‘– = âˆ‘ï¸ logğ‘ ğ¿ğ¿ğ‘€2(ğ‘¤ ğ‘–,ğ‘˜|ğ¶â—¦ğ‘¡Ë† ğ‘– â—¦ğ‘¤ ğ‘–,<ğ‘˜), files,weusethedatagenerationinSec.2.1togenerate92,143dis-
ğ‘¤ğ‘–,ğ‘˜âˆˆğ‘ğ‘– tinctHDLtrainingcodeinstances.EachHDLtrainingcodeinstance
1 âˆ‘ï¸ consistsofthebuggyHDLcode,errormessages,andthecorrect
L D = 2|D| (Lğ‘¡ğ‘– +Lğ‘ğ‘–), (8) HDLcode.Specifically,intheexperiments,wesplitthedatainto
ğ·ğ‘–âˆˆD
trainingandtestingsetsataratioof8:2,respectively.
wherethecontextğ¶ =ğ‘
ğ‘¡
â—¦ğ‘
ğ‘
â—¦ğ‘
ğ‘–
â—¦ğ‘š
ğ‘–
â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘‘ â—¦ğ‘Ÿğ‘ğ‘” ğ‘–ğ‘ denotesthe
concatenateoftheinputsforclarificationğ‘¤ ğ‘–,ğ‘— âˆˆ ğ‘¡ ğ‘– denotesthe 3.1.2 Baselines. WecompareourproposedHDLdebuggerwith13
ğ‘—-thwordinğ‘¡ andğ‘¤ denotesasetofwordsinğ‘¡ beforeğ‘¤ ,
ğ‘– ğ‘–,<ğ‘— ğ‘– ğ‘–,ğ‘— baselinesinthreetypesofcodedebuggingapproachesasfollows:
andğ‘ ğ¿ğ¿ğ‘€2(ğ‘¤ ğ‘–,ğ‘—|ğ‘¤ ğ‘–,<ğ‘—)denotetheprobabilityofğ‘¤ ğ‘–,ğ‘—.Lğ‘¡ğ‘– andLğ‘ğ‘–
denotethepredictioncross-entropylossonthoughtgroundtruth â€¢ Fivelargelanguagemodels:WecompareHDLdebuggeragainst
ğ‘¡ andcorrectcodegroundtruthğ‘ regardingthebuggycodeğ‘ , twoprofoundandstate-of-the-artLLMsavailablethroughAPI
ğ‘– ğ‘– ğ‘–
respectively. services,includingChatGPT[4]andGPT-4[1].Additionally,KDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
wecompareHDLdebuggerwiththreeopen-sourceLLMs:Open- Table3:MainResults.Pass-Ratedescribestheabsolutevalue,
Chat[37],Orca2[24],andMistral[16].Theseopen-sourcemod- whichisthehigherthebetter.<1%describesthePass-Rate
elshaveshownperformanceonparwithChatGPTacrossvari- lessthan1%.Run-TimeandEdit-Distancearerelativevalues
ousopenLLMbenchmarks. comparedwithours,whichareboththelowerthebetter.
â€¢ FourCodeDebuggingandHDLModels:WecompareHDLde- Method Pass-Rate Run-Time Edit-Distance
buggerwithtwocodedebuggingandhardwarecodegeneration
ChatGPTâˆ— 3.01% 2.25 31.28
models,i.e.,Self-debug[7]andRTLfixer[35].Self-debug[7]
GPT4âˆ— 6.35% 1.94 18.17
isoneofthemostclassicalmethodsofcodedebugging.RTL-
OpenChat <1% 2.51 34.47
fixer[35]isproposedtosolveHDLdebuggingproblems.Veri-
OpenChatw/RAG 3.01% 2.21 10.37
Gen[34]andRTLCoder[22]aretwoLLMstargetinghardware Orca2 2.68% 2.08 2.94
language. Orca2w/RAG 9.03% 2.10 4.74
â€¢ FourCodeLanguageModels:WecomparefourSOTApre- Mistral 7.36% 2.39 6.68
trainedcodeLLMs,i.e.,Deepseek[3],Starcoder[18],Stable- Mistralw/RAG 24.75% 2.01 9.88
code[31],andWizardCoder[23].
Self-debug 5.02% 2.49 10.24
ForbaselinesexceptforChatGPT[4]andGPT-4[1].,weadopt RTLfixer 28.35% 2.11 11.05
threestrategies,i.e.,therawmodel,therawmodelwithRAG,and VeriGen <1% 2.46 49.16
therawmodelwithsupervisedfine-tuning(SFT). VeriGenw/RAG 1.34% 2.56 37.12
VeriGenw/SFT 67.55% 1.35 1.38
â€¢ Rawmodel:Weonlyfeedbuggycodeanderrormessagestothe RTLCoder <1% 2.49 43.08
modelandenablerawmodelstoinferthecorrectcodedirectly. RTLCoderw/RAG <1% 2.65 34.85
â€¢ RawModelwithRAG:Forbuggycode,wefeedthebuggy RTLCoderw/SFT 64.21% 1.53 3.83
code,itserrormessage,andthedocumentRAGandcodeRAG
Deepseek 2.34% 2.58 32.17
obtainedinSec.2.2totherawmodelandenabletherawmodels
Deepseekw/RAG 3.34% 2.51 34.38
toinferthecorrectcodedirectly. Deepseekw/SFT 51.63% 1.64 3.35
â€¢ RawModelwithSFT:Wetakethebuggycodeanderrormes- Starcoder <1% 2.58 28.85
sageasinputsofLLMsandusethecorrectcodeasground-truth Starcoderw/RAG <1% 2.56 34.85
tofine-tunetherawmodels. Starcoderw/SFT 68.27% 1.21 1.57
Stablecode 6.69% 2.46 4.56
Specifically,weselectedCodeLlama-13basourbasemodelfromthe
Stablecodew/RAG 8.02% 2.50 6.25
availablecodeLLMs.Thechoiceof13bwasdrivenbyitsoptimal
Stablecodew/SFT 41.47% 2.38 6.10
modelsize,whichstrikesabalancebetweentraininganddeploy-
WizardCoder 3.01% 2.41 7.19
ment,takingintoaccountbothperformanceandcostfactors. WizardCoderw/RAG 4.68% 2.58 8.67
WizardCoderw/SFT 71.57% 1.04 1.06
3.1.3 EvaluationMetrics. Fortheoveralldebugsystem,wemainly
evaluateitspassrateforcorrectingcodes,relativecoderuntimes, HDLdebugger(ours) 81.93% 1.00 1.00
andeditdistancebetweencorrectcodeandbuggycode.Thecalcu-
lationofthesemetricsislistedbelow.
â€¢ MRR@K:Meanreciprocalrank(MRR)fortop-ğ¾ resultsonğ‘› ğ‘¡
â€¢ P ma es ts h- oR da it se: dP efias ns er dat ae sf ğ‘ƒor =exe (cid:205)c ğ‘› ğ‘–u =ğ‘ 1t ğ‘›i S ğ‘n (g ğ‘¦ğ‘–c )o ,d we hfi el re ec ğ‘¦o ğ‘–rr se tc at ne dd sb fy orea thch e queriesisformulatedasğ‘€ğ‘…ğ‘…@ğ¾ = ğ‘›1 ğ‘¡ (cid:205)ğ‘› ğ‘–=ğ‘¡ 1 ğ¾1 (cid:205) ğ‘˜ğ¾ =1 I(ğ‘¦ğ‘–, ğ‘˜ğ‘˜,ğ‘¦ğ‘–).
correctedcode,Sdenotesforexecutingcodesuccessfully.
â€¢ Run-Time:TherelativelyaveragecompilationtimeforHuaweiâ€™s 3.2 MainResults
internalHDLcompilertoexecutealltestcodefiles.TheRun-Time Table3demonstratesthemainperformanceofourresultsandother
forresultsfromourHDLdebuggerissetasthebaseunit. methods.Itâ€™sclearthatourmethodoutperformsothermethodsby
â€¢ Edit-Distance:Edit-Distancecalculatestheminimumnumber allmeansbyalargemarginincludingdirectapproach,RAGand
ofoperations(insertion,deletion,substitution)requiredtotrans- SFT,whichdemonstratestheeffectivenessofourframework.For
formonecodesnippetintotheother. bothruntimeandeditdistancemetrics,wenormalizeallresults
Also,weevaluateourcodesearchengineinSec.2.2bythehit andpresentonlytherelativevaluesincomparisonwithoursto
ratio,meanaverageprecision,andmeanreciprocalrankmetrics, enhancetheclarityandeffectivenessofthecomparison.
whichareformulatedasfollows.
3.2.1 ComparisonwithdifferenttypesofLLMs. Inourstudy,we
â€¢ H@K:Hitratiofortop-ğ¾ recommendationresultsonğ‘› ğ‘¡ code conductedacomparativeevaluationofbothgeneral-purposeLLMs
queriesisformulatedasğ»@ğ¾ = ğ‘›1 ğ‘¡ (cid:205)ğ‘› ğ‘–=ğ‘¡ 1 ğ¾1 (cid:205) ğ‘˜ğ¾ =1I(ğ‘¦ ğ‘–,ğ‘˜,ğ‘¦ ğ‘–),where suchasChatGPT,GPT-4,OpenChat,Orca,Mistral,Deepseek,Star-
ğ‘¦ ğ‘–,ğ‘˜ denotestheerrortypeoftheretrievedğ‘˜-thbuggycodefor coder,Stablecode,WizardCoder,andspecializedcodeLMsinclud-
querycodeğ‘ ğ‘–,andtheindicatorfunctionI(ğ‘¦ ğ‘–,ğ‘˜,ğ‘¦ ğ‘–)=1ifğ‘¦ ğ‘–,ğ‘˜ =ğ‘¦ ğ‘– ingSelf-debug,RTLfixer,VeriGen,RTLCoder,withintheHDLde-
â€¢ MAP@K:Meanaverageprecision(MAP)fortop-ğ¾resultsonğ‘› ğ‘¡ buggingscenario.Duetoprivacyconcerns,certaincodespecifica-
queriesisdefinedasğ‘€ğ´ğ‘ƒ@ğ¾ = 1 (cid:205)ğ‘›ğ‘¡ 1 (cid:205)ğ¾ I(ğ‘¦ğ‘–,ğ‘˜,ğ‘¦ğ‘–)Â·ğ‘›(ğ‘ğ‘–,â‰¤ğ‘˜), tionshavebeenomittedfortestingpurposeswhenusingChatGPT
ğ‘›ğ‘¡ ğ‘–=1 ğ¾ ğ‘˜=1 ğ‘˜
whereğ‘›(ğ‘ ğ‘–,â‰¤ğ‘˜)denotesthenumberofrecommendationsinthe orGPT4.Todistinguishtheseversions,wewillrefertothemas
firsttop-ğ‘˜thathasthesameerrorlabelwithqueryğ‘ . ChatGPTâˆ— and GPT4âˆ—. It is evident that our approach exhibits
ğ‘–HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
Table4:Ablationondifferentstrategies Table5:EvaluationonthesearchengineinSec.2.2onthe
top-ğ‘˜recommendation.XGBandRFdenotetheXFBoostand
Method Pass-Rate Run-Time Edit-Distance
RandomForest,respectively.MAP@1andMRR@1arethe
Direct(CodeLlama) 4.01% 2.26 35.51 sameasH@1mathematically.ThefirstrowOptimalisthe
+RAG 15.05% 2.20 5.34
optimalperformanceundereachmetric.
+SFT 70.56% 1.19 1.19
H@1 H@3 H@10 MAP@3 MAP@10 MRR@3 MRR@10
+SFTw/th 74.91% 1.09 1.13
Optimal 1.00 1.00 1.00 1.00 1.00 0.61 0.29
+RAG&SFTw/th 81.93% 1.00 1.00 TF-IDF 0.97 0.87 0.75 0.86 0.71 0.55 0.24
BM25 0.95 0.80 0.59 0.81 0.54 0.53 0.22
XGB 1.00 0.35 0.18 0.34 0.14 0.34 0.12
superiorperformanceagainstall13state-of-the-artbenchmarks,in- RF 0.31 0.43 0.41 0.34 0.29 0.25 0.11
cludingGPT-4andotherdomain-specifichardware-basedlanguage Ours 1.00 0.98 0.94 0.98 0.93 0.60 0.28
models.
HDLdebugger HDLdebugger
3.2.2 AnalysisofDifferentStrategies. Inthisstudy,weimplement 84 2.0
threedistinctevaluationstrategiestoassesstheefficacyofvarious 82 1.8
methodologies:withretrieval-augmentedgeneration(RAG),with 80 1.6
78 1.4
supervisedfine-tuning(SFT),andviaadirectapproach.Within 76
thecontextofHDLdebugging,ouranalysisrevealsthatSFTholds 74 1.2
72 1.0
greatersignificanceandapplicabilityacrossallevaluatedbaselines, 0 1 3 5 7 9 0 1 3 5 7 9
includingVeriGen,RTLCoder,Deepseek,Starcoder,Stablecode,and NumberofRetrievedCodeInstances NumberofRetrievedCodeInstances
WizardCoder.Inthemajorityofscenarios,wenotethatmethods Figure5:Pass-rateandcode Figure6:Inferencetimeand
enhancedthroughSFTconsistentlyoutperformthoseaugmented retrievedcodeinstances retrievedcodeinstances
withRAGbyasubstantialmargin.Besides,ourproposedmethod
integratesbothRAGandSFTstrategies,achievingunparalleled 85.00
performance,indicatingthatitisbettertoincorporateSFTand 80.00 Pass-rate@5
RAGfortheHDLdebuggingtask.
75.00 Pass-rate@3
3.2.3 ImpactonDomain-SpecificSolutions. Inadditiontogeneral
Pass-rate@1
andcode-specificLanguageModels(LLMs),methodologiessuch 70.00
asSelf-debugandRTLfixerarespecificallydevisedtoaddresscode 65.00
0.10.2 0.4 0.6 0.8 1.0 1.2
debuggingscenarios.Whiletheseapproachesdemonstrateimprove-
Temperature
mentsoverothergeneralandcodeLLMs,theireffectivenessstill
Figure7:Pass-rate@kandTemperature
fallsshortofbeingfullysatisfactory.Ouranalysisextendstoevalu-
atingourdatasetwithLLMsexclusivelytrainedonhardwarelan-
guages,namelyVeriGenandRTLCoder.Contrarytoexpectations, low-dimensionalrepresentationsforcodesanderrormessages,sim-
thesespecializedhardwarelanguagemodelsdonotoutperform ilartoourBERT-LSTMmodelâ€™sapproachtocomputingrelevance
theirgeneralandcodeLLMcounterpartsinourHDLdebugging throughrepresentationsimilarity.
context,suggestingthepossibilityofaninherenttaskdomaingener- Table5indicatesourenginesurpassesallbaselinesinaccurately
alizationissuewithinHDLdebuggingscenarios.Ontheotherhand, retrievingandcorrectingbuggycodequeries,highlightingitssu-
ourapproachconsistentlysurpassesdomain-specificsolutionsre- periorabilitytodecodecomplexbuggycodepatternsbeyondthe
gardlessofthevariedpromptengineeringtechniquesemployedor capabilitiesoftraditionalandmachinelearningmodels.Traditional
thedomain-specificdatausedfortraining,whichunderscoresthe methodslikeTF-IDFandBM25lackthedepthtounderstandcom-
effectivenessofourmethodology. plexcodebugs,whilemodelslikeXGBoostandRandomForest
fallshortinsemanticcomprehension.Ourengineeffectivelycom-
3.3 AblationStudies binestextualandsemanticanalysis,enhancingbugdetectionand
Firstly,weprovideadetailedanalysisoftheimpactofdifferent correction.
strategiesofourmethod.Table4illustratestheperformanceof
differentstrategies.Forbaseline,weonlyusedirectinferencestrat- 3.4 ParameterSensitivity
egyonbasemodel,i.e.,CodeLlama.SFTw/thindicatessupervised
Inthefollowingexperiments,weevaluateparametersensitivity
fine-tuningwithgeneratedthoughts.IntermsofRAG&SFTw/
acrossdifferenthyper-parametersincludingretrievedcodeinstances
th,wemainlyrefertoretrievalaugmentedLLMfine-tuningwhere
samplesandrelatedinferencetime.Wealsoconsidertemperature
bothretrievedcodeinstancesandrelevantinformationarecom-
andpass-rate@ğ‘˜forvariousğ‘˜.
binedtogetherforLLMfine-tuning.FromTable4wecanobserve
thatSFTw/thoutperformsbaselinebyalargemargin.Moreover, 3.4.1 Thenumberofretrievedcodeinstances. Figure5depictsthe
combingRAG&SFTalsosignificantlyimprovestheperformance. impactofvaryingthenumberofretrievedcodeinstancesonmodel
Besides, we evaluate our search engine for RAG, comparing performance.Wefindthatperformanceimproveswitheachad-
itwithfourmethods:F-IDF[2],BM25[30],randomforest[29], ditionalinstancebetween1to5,achievingthemostsignificant
and XGBoost [6]. TF-IDF and BM25 assess relevance scores be- gains.However,beyondfiveinstances,gainsplateauorevende-
tweenbuggycodes,whileRandomForestandXGBoostgenerate crease,indicatingdiminishingreturns.Thissuggeststhatwhile
)%(etar-ssaP
)%(k@etar-ssaP
emit-ecnerefnIKDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
addingretrievedcodeinstancesenhancesmodelperformanceup therepairofdefectivecode.Contemporarystrategiesemploying
toapoint,increasinginstancesbeyondthisthresholdleadstoinef- LLMspredominantlyutilizeretrievalaugmentedgeneration(RAG)
ficiency.Moreover,weobserveatendencyforthelanguagemodel andsophisticatedpromptengineeringtechniquestoaddressdebug-
togeneraterepetitiveorredundantcontentderivedfromprevious gingchallenges.Notably,Self-debug[7]representsapioneering
inputasthenumberofretrievedcodeinstancesincreases.This effortinapplyingLLMstocodedebugging,employingtargeted
phenomenonunderscoresacriticalareaforfutureexploration. promptengineeringforenhancedeffectiveness.RTLfixer[35]uti-
lizesbothRAGandpromptengineeringtotackletheHDLdebug-
3.4.2 FeasibilityonInference. Weassesstheimpactofincorporat- gingproblem.However,thesemethodsdonotshowsatisfactory
ingtheretrievedcodeinstancesontheadditionalinferencebud-
resultsinourindustry-levelcasesduetothelackofrequisiteknowl-
get.Figure6illustratesthenormalizedinferencetimestoclearly
edgeofHDLcodes.Fine-tuningwithHDLcoderesourcesisone
highlighttheincrementalbudgetrequired.Avalueof0indicates
alternativetotackletheproblem.Nevertheless,theseapproaches
theabsenceofretrievedcodeinstances.Ourobservationsreveal
needabundantandhigh-qualitylabeleddata,whichisnotsuit-
thatasthenumberofretrievedcodeinstancesincreases,thecor-
ableforHDLcodes,sincetherelatedHDLcodesarelimiteddueto
respondinginferencetimeexhibitsaslow,logarithmicincrease
privacyandcommercialissues.
ratherthanalinearone.Thispatternunderscorestheefficiency
ofourapproach,demonstratingthatintegratingretrievedcodein- 4.2 largelanguagemodelsforCodeGeneration
stancessignificantlyenhancesperformancewithoutproportionally Largelanguagemodels(LLMs)havetransformedthelandscape
increasingtheinferenceoverhead. of code generation by leveraging vast amounts of code data to
predictandgeneratesyntacticallyandsemanticallycorrectcode
3.4.3 Pass-rate@kandTemperature. Weexploretheeffectsofvary-
snippets[38].NotableamongthesemodelsisOpenAIâ€™sCodex[5],
ingthetemperaturesettingsandthepass-rate@kfordifferentval-
whichpowersGitHubCopilot,offeringcontext-awarecodesug-
ues ofğ‘˜, whereğ‘˜ represents the number of answers generated
gestionsandcompletionstodevelopersdirectlywithintheirIDEs.
bytheLLM,asdiscussedin[5].Typically,alowertemperature
Another key contribution is from DeepMindâ€™s AlphaCode [19],
settingyieldsmoredeterministicoutcomes,whereashighertem-
whichexcelsingeneratingcodesolutionsforcompetitiveprogram-
peraturesresultinmorevariedoutputs.Figure7demonstratesthat
mingchallengesandobtainsthetoppercentileofparticipantsin
asğ‘˜increases,sodoesoverallperformance.Specifically,atalower
codingcompetitions.RecentadvancementsinLLMstailoredforthe
temperature,suchas0.1,outputsaremoreconsistent,leadingtoa
codingdomainhaveseensignificantcontributions,withnotable
narrowerperformancerange.Conversely,athighertemperatures,
examplesincludingDeepSeek[3],Starcoder[18],Stabelcode[31],
like1.2,outputsbecomemorevaried,enhancingthelikelihood
Codellama[32],andWizardCoder[23].Thesemodelshavebeen
ofgeneratingcorrectanswersasğ‘˜ increases.Notably,forapass-
trainedonextensivedatasets,bothproprietaryandopen-source,to
rate@5,theperformanceatatemperatureof1.2surpassesthat
enhancecapabilitiesincodegeneration,completion,anddebugging.
at0.7,indicatingthatincreasedtemperaturesettingscanimprove
Withintherealmofhardwaredescriptionlanguages(HDLs),Chip-
outcomes,particularlyathighervaluesofğ‘˜.
NeMo[21]representsapioneeringeffortindevelopingadomain-
specificLLM,highlightingthechallengesandpotentialofapply-
4 RELATEDWORK
ingLLMstothehardwaredomain.Despitebeingtrainedonvast
4.1 AutomaticCodeDebugging hardware-specificdatasets,ChipNeMoachievesperformancethat,
Automaticcodedebugginghasemergedasapromisingareawithin whilecompetitive,doesnotsurpassthatofstate-of-the-artgeneral
softwareengineering[15,25].Givenacodewithbugs,thetaskisto LLMs,suchasGPT-4.Thisunderscorestheinherentcomplexitiesof
automaticallyfixthecodebugswiththecorrectfunctions,which adaptingLLMstothenuancesofHDL.Concurrently,initiativeslike
alleviatestheburdenofmanualdebuggingandfixingcodefaults. VeriGen[34]andRTLCoder[22],whichfocusonfine-tuningLLMs
Classictechniquescanbemainlyclassifiedastemplate-based[17, usingspecializeddatasets,havedemonstratedremarkableresults.
20],heuristic-based[41,47],constraint-based[43,44],andneural ThesedevelopmentsunderscoretheevolvinglandscapeofLLM
network-basedapproaches[8,11,49].Specifically,template-based applicationsinHDLdebugging,highlightingbothachievements
approachesapplyexpert-definedcodepatternstofixbugs.Theseap- andareasripeforfurtherexploration.
proachescanonlyrepaircodesinspecificpatternsandlackgeneral-
izationtootherbugs.Heuristic-basedapproachesapplypredefined 5 CONCLUSION
heuristicsandcannotcoveralltypesofbugs.Constraint-basedap- Inthispaper,weproposeanLLM-assistedHDLdebuggingframe-
proachesrepairbuggycodesbysolvingaconstraintproblem.These work,namelyHDLdebugger,whichconsistsofHDLdebuggingdata
methodscanbeaccurate,buttheyarecomputationallyexpensive. generation,asearchengine,andaretrieval-augmentedLLMfine-
Neuralnetwork-basedapproachesneednumeroushigh-qualityla- tuningapproach.Throughextensiveandvariedexperimentation
beledtrainingdatapairs(i.e.,pairsofbuggycodesandfixedcodes) withmultipleLLMs,wehaveunearthedpivotalfindingswithinthe
tooptimizeparameters,whichistime-consumingtocollectthe domain.Ourmethodsignificantlysurpassesexistingtechniques,
high-qualitycodepairs. achievinganexceptionalpass-rateofupto81.93%,indicatingthat
Recently,largelanguagemodels(LLMs)haveshednewlighton HDLdebuggercouldautomateandstreamlineHDLdebuggingfor
automaticcodedebugging.Theprevailinghypothesissuggeststhat chipdesign.Inaddition,weprovidein-depthexperimentalanalysis
LLMs,throughtrainingonextensiverepositoriesofopen-source andoutlinepotentialfuturedirectionsforHDLdebuggingwith
codesnippets,areadeptatidentifyingbugpatternsandfacilitating LLMsinAppendixD.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
REFERENCES [26] GonzaloNavarro.2001.Aguidedtourtoapproximatestringmatching.ACM
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,Ale- computingsurveys(CSUR)33,1(2001),31â€“88.
man,etal.2023.Gpt-4technicalreport.arXivpreprintarXiv:2303.08774(2023). [27] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[2] AkikoAizawa.2003.Aninformation-theoreticperspectiveoftfâ€“idfmeasures. Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
InformationProcessing&Management39,1(2003),45â€“65. Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
[3] XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,etal.2024. inneuralinformationprocessingsystems32(2019).
DeepSeekLLM:ScalingOpen-SourceLanguageModelswithLongtermism.arXiv [28] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe.2020.Deep-
preprintarXiv:2401.02954(2024). speed:Systemoptimizationsenabletrainingdeeplearningmodelswithover
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, 100billionparameters.InProceedingsofthe26thACMSIGKDDInternational
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda ConferenceonKnowledgeDiscovery&DataMining.3505â€“3506.
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural [29] StevenJRigatti.2017.Randomforest.JournalofInsuranceMedicine47,1(2017),
informationprocessingsystems33(2020),1877â€“1901. 31â€“39.
[5] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveira [30] StephenRobertson,HugoZaragoza,andMichaelTaylor.2004. SimpleBM25
Pinto,JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman, extensiontomultipleweightedfields.InProceedingsofthethirteenthACM
etal.2021. Evaluatinglargelanguagemodelstrainedoncode. arXivpreprint internationalconferenceonInformationandknowledgemanagement.42â€“49.
arXiv:2107.03374(2021). [31] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rn
[6] TianqiChenandCarlosGuestrin.2016.Xgboost:Ascalabletreeboostingsystem. Ommer.2022. High-resolutionimagesynthesiswithlatentdiffusionmodels.
InProceedingsofthe22ndacmsigkddinternationalconferenceonknowledge InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
discoveryanddatamining.785â€“794. recognition.10684â€“10695.
[7] XinyunChen,MaxwellLin,NathanaelSchÃ¤rli,andDennyZhou.2023.Teaching [32] BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,Xiao-
largelanguagemodelstoself-debug.arXivpreprintarXiv:2304.05128(2023). qingEllenTan,YossiAdi,JingyuLiu,TalRemez,JÃ©rÃ©myRapin,etal.2023.Code
[8] ZiminChen,SteveKommrusch,andMartinMonperrus.2022.Neuraltransfer llama:Openfoundationmodelsforcode.arXivpreprintarXiv:2308.12950(2023).
learningforrepairingsecurityvulnerabilitiesinccode.IEEETransactionson [33] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos
SoftwareEngineering49,1(2022),147â€“165. Guestrin,PercyLiang,andTatsunoriB.Hashimoto.2023.StanfordAlpaca:An
[9] JasonCong,BinLiu,StephenNeuendorffer,JuanjoNoguera,KeesVissers,and Instruction-followingLLaMAmodel. https://github.com/tatsu-lab/stanford_
ZhiruZhang.2011.High-levelsynthesisforFPGAs:Fromprototypingtodeploy- alpaca.
ment.IEEETransactionsonComputer-AidedDesignofIntegratedCircuitsand [34] ShailjaThakur,BaleeghAhmad,HammondPearce,BenjaminTan,BrendanDolan-
Systems30,4(2011),473â€“491. Gavitt,RameshKarri,andSiddharthGarg.2023.Verigen:Alargelanguagemodel
[10] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRÃ©.2022.Flashat- forverilogcodegeneration.arXivpreprintarXiv:2308.00708(2023).
tention:Fastandmemory-efficientexactattentionwithio-awareness.Advances [35] YunDaTsai,MingjieLiu,andHaoxingRen.2023.RTLFixer:AutomaticallyFixing
inNeuralInformationProcessingSystems35(2022),16344â€“16359. RTLSyntaxErrorswithLargeLanguageModels.arXivpreprintarXiv:2311.16543
[11] MichaelFu,ChakkritTantithamthavorn,TrungLe,VanNguyen,andDinh (2023).
Phung.2022.VulRepair:aT5-basedautomatedsoftwarevulnerabilityrepair.In [36] CharalamposTsourakakis.2015. Thek-cliquedensestsubgraphproblem.In
Proceedingsofthe30thACMJointEuropeanSoftwareEngineeringConference Proceedingsofthe24thinternationalconferenceonworldwideweb.1122â€“1132.
andSymposiumontheFoundationsofSoftwareEngineering.935â€“947. [37] GuanWang,SijieCheng,XianyuanZhan,XiangangLi,SenSong,andYangLiu.
[12] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai, 2023.Openchat:Advancingopen-sourcelanguagemodelswithmixed-quality
JiaweiSun,andHaofenWang.2023.Retrieval-augmentedgenerationforlarge data.arXivpreprintarXiv:2309.11235(2023).
languagemodels:Asurvey.arXivpreprintarXiv:2312.10997(2023). [38] JunjieWang,YuchaoHuang,ChunyangChen,ZheLiu,SongWang,andQing
[13] MikeGordon.1995.ThesemanticchallengeofVerilogHDL.InProceedingsof Wang.2023.Softwaretestingwithlargelanguagemodel:Survey,landscape,and
tenthannualIEEEsymposiumonlogicincomputerscience.IEEE,136â€“145. vision.arXivpreprintarXiv:2307.07221(2023).
[14] DoritSHochbaum.1996.Approximatingcoveringandpackingproblems:set [39] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel
cover,vertexcover,independentset,andrelatedproblems. InApproximation Khashabi,andHannanehHajishirzi.2022.Self-instruct:Aligninglanguagemodel
algorithmsforNP-hardproblems.94â€“143. withselfgeneratedinstructions.arXivpreprintarXiv:2212.10560(2022).
[15] KaiHuang,ZhengziXu,SuYang,HongyuSun,XuejunLi,ZhengYan,andYuqing [40] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,
Zhang.2023.ASurveyonAutomatedProgramRepairTechniques.arXivpreprint QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtpromptingelicitsreasoning
arXiv:2303.18184(2023). inlargelanguagemodels.AdvancesinNeuralInformationProcessingSystems
[16] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,De- 35(2022),24824â€“24837.
vendraSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel, [41] MingWen,JunjieChen,RongxinWu,DanHao,etal.2018. Context-aware
GuillaumeLample,LucileSaulnier,etal.2023. Mistral7B. arXivpreprint patchgenerationforbetterautomatedprogramrepair.InProceedingsofthe40th
arXiv:2310.06825(2023). internationalconferenceonsoftwareengineering.1â€“11.
[17] JiajunJiang,YingfeiXiong,HongyuZhang,QingGao,andXiangqunChen. [42] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
2018.Shapingprogramrepairspacewithexistingpatchesandsimilarcode.In AnthonyMoi,PierricCistac,TimRault,RÃ©miLouf,MorganFuntowicz,etal.
Proceedingsofthe27thACMSIGSOFTinternationalsymposiumonsoftware 2019.Huggingfaceâ€™stransformers:State-of-the-artnaturallanguageprocessing.
testingandanalysis.298â€“309. arXivpreprintarXiv:1910.03771(2019).
[18] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov, [43] YingfeiXiong,JieWang,RunfaYan,JiachenZhang,ShiHan,GangHuang,andLu
ChenghaoMou,MarcMarone,ChristopherAkiki,JiaLi,JennyChim,etal.2023. Zhang.2017.Preciseconditionsynthesisforprogramrepair.In2017IEEE/ACM
StarCoder:maythesourcebewithyou!arXivpreprintarXiv:2305.06161(2023). 39thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,416â€“426.
[19] YujiaLi,DavidChoi,JunyoungChung,NateKushman,etal.2022.Competition- [44] JifengXuan,MatiasMartinez,FavioDemarco,MaximeClement,SebastianLame-
levelcodegenerationwithalphacode.Science378,6624(2022),1092â€“1097. lasMarcote,ThomasDurieux,DanielLeBerre,andMartinMonperrus.2016.
[20] KuiLiu,AnilKoyuncu,etal.2019.TBar:Revisitingtemplate-basedautomatedpro- Nopol:Automaticrepairofconditionalstatementbugsinjavaprograms.IEEE
gramrepair.InProceedingsofthe28thACMSIGSOFTInternationalSymposium TransactionsonSoftwareEngineering43,1(2016),34â€“55.
onSoftwareTestingandAnalysis.31â€“42. [45] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasLGriffiths,YuanCao,
[21] MingjieLiu,Teodor-DumitruEne,RobertKirby,ChrisCheng,NathanielPinckney, andKarthikNarasimhan.2023. Treeofthoughts:Deliberateproblemsolving
RongjianLiang,JonahAlben,HimyanshuAnand,SanmitraBanerjee,Ismet withlargelanguagemodels.arXivpreprintarXiv:2305.10601(2023).
Bayraktaroglu,etal.2023. Chipnemo:Domain-adaptedllmsforchipdesign. [46] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,
arXivpreprintarXiv:2311.00176(2023). andYuanCao.2022.React:Synergizingreasoningandactinginlanguagemodels.
[22] ShangLiu,WenjiFang,YaoLu,QijunZhang,HongceZhang,andZhiyaoXie.2023. arXivpreprintarXiv:2210.03629(2022).
Rtlcoder:Outperforminggpt-3.5indesignrtlgenerationwithouropen-source [47] YuanYuanandWolfgangBanzhaf.2018. Arja:Automatedrepairofjavapro-
datasetandlightweightsolution.arXivpreprintarXiv:2312.08617(2023). gramsviamulti-objectivegeneticprogramming.IEEETransactionsonsoftware
[23] ZiyangLuo,CanXu,PuZhao,QingfengSun,XiuboGeng,etal.2023.Wizard- engineering46,10(2018),1040â€“1067.
Coder:EmpoweringCodeLargeLanguageModelswithEvol-Instruct. arXiv [48] ChenZhang,PengLi,GuangyuSun,YijinGuan,BingjunXiao,andJasonCong.
preprintarXiv:2306.08568(2023). 2015.OptimizingFPGA-basedacceleratordesignfordeepconvolutionalneural
[24] ArindamMitra,LucianoDelCorro,ShwetiMahajan,AndresCodas,Clarisse networks.InProceedingsofthe2015ACM/SIGDAinternationalsymposiumon
Simoes,SahajAgarwal,XuxiChen,AnastasiaRazdaibiedina,ErikJones,Kriti field-programmablegatearrays.161â€“170.
Aggarwal,etal.2023.Orca2:Teachingsmalllanguagemodelshowtoreason. [49] XiaoyuZhang,JuanZhai,ShiqingMa,andChaoShen.2021.Autotrainer:An
arXivpreprintarXiv:2311.11045(2023). automaticdnntrainingproblemdetectionandrepairsystem.In2021IEEE/ACM
[25] MartinMonperrus.2018. Automaticsoftwarerepair:Abibliography. ACM 43rdInternationalConferenceonSoftwareEngineering(ICSE).IEEE,359â€“371.
ComputingSurveys(CSUR)51,1(2018),1â€“24.KDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
A ADDITIONALMATERIALSFORDATA ThebasicideaofAlg.1istogreedilyselectthebuggycodeinstance
GENERATION thatcanbringthemaximuminformationgainintotheselected
ğ‘
instancesetğ· untilitexceedsthenumberğ‘˜.Specifically,given
Table6indicatesthecommonerrortypeinHDLcodesummarized ğ‘
ğ‘
theinstancesetğ· ,wefirstdefinethemarginalinformationgain
byLLM.Bylocatingthemaincauseoferrors,wecandesignmulti- ğ‘
ofğ¼ asfollows:
plemodificationfunctionstorecurtheerrorsinagivenHDLcode. ğ‘—
Itiseasytorevealthatsomesimpleandsimilarmodificationscan â–³ğ‘†(ğ¼ ğ‘—|ğ· ğ‘ğ‘ )=ğ‘†(ğ¼ ğ‘— âˆªğ· ğ‘ğ‘ )âˆ’ğ‘†(ğ· ğ‘ğ‘ ) (9)
resultintotallydifferenterrorswhenmatchingtheerrorcauses ğ‘
AsillustratedinAlg.1,wefirstinitializetheinstancesetğ· as
andmodificationfunctions.Thesemodificationfunctionsthenbe- ğ‘
c foo um ne daa tn ioe nss oe fn sti oa ll up tia or nt so if nth the ere Rv Ae Grse see an rg ci hne ee nr gi in ng e.pipelineandthe ğ¼âˆ… ğ‘—( âˆˆlin ğ·Ë†e ğ‘ğ‘1) (. liT nh esen 2, -4w ).e Nc eo xm t,p wu ete ct oh me ps uim teil ta hr eit my as rc go ir ne ağ‘  lğ‘– iğ‘š nf( oğ‘ r, mğ¼ ğ‘— a) tf ioo nre ga ac inh
The error message and solution database of the RAG search ofeachğ¼ ğ‘— âˆˆğ·Ë† ğ‘ğ‘ andselectnodeğ¼âˆ—withthemaximumâ–³ğ‘†(ğ¼ ğ‘—|ğ· ğ‘ğ‘ )
followingEq.(9)(line6-9).Then,weaddğ¼âˆ—intoğ·ğ‘
,andremoveit
engine is built on the foundation of the modification functions. ğ‘
Table7indicatesinformationinthedatabase.Foreachcommon fromğ·Ë† ğ‘ğ‘ (lines10-11).Werepeattheselectionprocedureuntilwe
errorcodeinHDLcodecompilation,itprovidesrelateddescriptions haveselectedğ‘˜buggycodeinstances(lines5-12).
anderrorrootreasonsthathelpdebug.Andmostessentially,the TheoremB.1. Alg.1canachievea1âˆ’1/ğ‘’approximationratio.
databasecollectsrecommendingsolutionsforagivenerrorcode.
ğ‘
WhenretrievingknowledgewiththeRAGsearchengine,theroot Proof. Theğ‘†(ğ· ğ‘)inEq.(2)ismonotoneandsubmodular.
reasonandsolutiontogetherwiththeerrormessageandsolution â€¢ Monotone: Givenanyğ¼ ğ‘– âˆˆğ·Ë† ğ‘ğ‘ andselectedinstancessetğ· ğ‘ğ‘,we
willbereturnedtoformLLMinput. canobtainğ‘†(ğ· ğ‘ğ‘ âˆªğ¼ ğ‘–)âˆ’ğ‘†(ğ· ğ‘ğ‘ ) â‰¥ 0.Thus,ğ‘†(ğ· ğ‘ğ‘ )ismonotone
increasing.
B ADDITIONALMATERIALSFORRAG â€¢ Submodularity: Givenğ·Ëœ ğ‘ğ‘ âŠ‚ ğ· ğ‘ğ‘ ,andabuggycodeinstance
B.1 BERT-LSTM ğ¼ ğ‘– âˆ‰ğ·Ëœ ğ‘ğ‘ ,ğ· ğ‘ğ‘ ,wecanobtain:
T bih lie tiB esE oR fT B-L ES RT TM (Bm ido id ree cl ts iy on ne ar lg Ei nze cs odth ere Rco epn rte ex setu na tal te iom nb se fd rd oi mng Tc ra ap na s-- ğ‘†(ğ·Ëœ ğ‘ğ‘ âˆªğ¼ ğ‘–)âˆ’ğ‘†(ğ·Ëœ ğ‘ğ‘ )=ğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)+ ğ‘˜1 ğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ·Ëœ ğ‘ğ‘ ) (10)
(f Lo orm nger Ss h) ow rti -t Th erth me Mse eq mu oe rn yt )ia nl ed twat oa rp kr so .Sce ps es ci in fig cas lt lr ye ,n gg ivth eno af bL uS gT gM
y
ğ‘†(ğ· ğ‘ğ‘ âˆªğ¼ ğ‘–)âˆ’ğ‘†(ğ· ğ‘ğ‘ )=ğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)+ ğ‘˜1 ğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ· ğ‘ğ‘ ) (11)
c sio nd ge leğ‘ sa en qd ueit ns ce err ğ‘o ğ‘ r =me [s Cs La Sg ,e ğ‘ğ‘š ,S, Ew P,e ğ‘šfi ]r .s St ic no cn ec ta hte en ta ot ke et nhe lem ngin thto oa f wSi enc ce anğ‘‘ğ‘– oğ‘  b(ğ¼ tğ‘– a, iğ· nËœ ğ‘ğ‘ t) he= im nei qn uğ¼ğ‘— aâˆˆ lğ· iËœ tğ‘ yğ‘( a2 sâˆ’ : ğ‘ ğ‘–ğ‘š(ğ¼ ğ‘–,ğ¼ ğ‘—))andğ·Ëœ ğ‘ğ‘ âŠ‚ğ· ğ‘ğ‘ ,thus
buggycodeanderrormessageistoolong,weseparatethequery
s sueq bu see qn uc ee nğ‘ cğ‘  ehin at so ğ‘›a
ğ‘ 
ts oe kt eo nf ss .u Tb h- es ne ,q wue en fc ee ds e{ ağ‘  cğ‘˜ h} sğ‘˜|ğ‘ u=ğ‘  b1| s/ğ‘› eqğ‘ , uw enh ce er ğ‘ e ğ‘˜e ia nc th
o
Itdemonğ‘†
s
ğ‘( tğ· rËœ ağ‘ğ‘ teâˆª stğ¼ hğ‘–) atâˆ’ ğ‘†ğ‘† (( ğ·ğ·Ëœ ğ‘ğ‘ğ‘ğ‘ )) inâ‰¥ Eğ‘† q( .ğ· (2ğ‘ğ‘ )âˆª isğ¼ sğ‘–) ubâˆ’ mğ‘† o( dğ· uğ‘ğ‘ l) a.
r.
BERT,andBERTwilloutputasentence-levelvectorhğ‘˜ =ğµğ¸ğ‘…ğ‘‡(ğ‘  ğ‘˜), Sinceğ‘†(ğ· ğ‘)ismonotoneandsubmodular,accordingto[14],the
wherehğ‘˜ capturesthecontextinformationoftokensinğ‘  ğ‘˜.After approximationratioofAlg.1is1âˆ’1/ğ‘’. â–¡
wob eta ei mni pn lg oyall as Bu ib -s Le Sq Tu Menc toee cm apb te ud rd ein log ns g{ -h rağ‘˜ n} ğ‘˜ g|ğ‘ = eğ‘  1| d/ğ‘› eğ‘  pefo nr d{ eğ‘  nğ‘˜ c} iğ‘˜ e|ğ‘ s=ğ‘  1| a/ nğ‘›ğ‘  d, TimeComplexity.Assumethedimensionofzğ‘¤ andzğ‘  areğ‘‘ ğ‘¤
intricatepatternsacrossdifferentsubsequencesas{h ğ‘˜â€²} ğ‘˜|ğ‘ =ğ‘  1|/ğ‘›ğ‘  = a sin md ilğ‘‘ ağ‘  r, itr yes sp ce oc retiv ğ‘ ğ‘–e ğ‘šly (. ğ‘F ,i ğ¼r ğ‘—s )t, foit rt ea ak ce hs ğ¼ğ‘‚ ğ‘—( âˆˆğ‘ ğ·Ë†(ğ‘‘ ğ‘ğ‘ğ‘¤ (l+ inğ‘‘ eğ‘  2) -) 4t )o .Tc ho em np ,iu tt te akth ee s
BiLSTM({hğ‘˜} ğ‘˜|ğ‘ =ğ‘  1|/ğ‘›ğ‘ ).UponacquiringthesequenceofBi-LSTM ğ‘‚(ğ‘˜2(ğ‘‘ ğ‘¤+ğ‘‘ ğ‘ ))toselectthetop-ğ‘˜buggycodeinstancesfromğ·Ë† ğ‘ğ‘
outputs,weuseaself-attentionmechanismtogeneratethefinal (line 5-12). Thus, the time complexity of the top-ğ‘˜ buggy code
representationofğ‘ğ‘  aszğ‘  =Self_Attention({hâ€²}|ğ‘|/ğ‘›ğ‘ ).Theself- selectionalgorithmisğ‘‚((ğ‘ +ğ‘˜2)(ğ‘‘ ğ‘¤+ğ‘‘ ğ‘ ))intotal.
ğ‘˜ ğ‘˜=1
attentionblocksattendtodifferentpartsoftheLSTMsequence,
enablingthemodeltofocusonthemostrelevantfeaturesforclassi- C RETRIEVAL-AUGMENTEDLLM
fication.Then,wefeedzğ‘  toalinearlayertopredictbuggypattern FINE-TUNING
probabilityforthebuggycodequery(ğ‘,ğ‘’).
C.1 ImplementationDetails
We implement our approach in PyTorch [27], and fine-tune on
B.2 GreedyAlgorithm
CodeLlama[32]whichisprovidedbyhuggingface[42]modelzoo.
AsintroducedinSec.2.2,thetop-ğ‘˜buggycodeprobleminEq.(2)is
The foundation of our code is built upon the FastChat and Al-
NP-hard,indicatingthatwecannotobtaintheoptimaltop-ğ‘˜buggy
pacaframeworks,incorporatingcutting-edgetechnologiessuchas
ğ‘
codeinstancesğ· inanypolynomialtime.Thus,weproposea
ğ‘ flashattention[10],deepspeed[28],toenhanceeffectivenessdur-
greedyalgorithminAlg.1withatheoreticalguaranteetooptimize
ingbothtrainingandinferencephases.Ourexperimentalsetup
there-rankobjectiveinEq.(2).Forclarification,wedenotethe
utilizeseightNVIDIA-GTXA100GPUswith80Gmemorytoen-
ğ‘
Eq.(2)byğ‘†(ğ· ğ‘)asfollows.
sureenoughcomputationalcapacity.Fortraining,weprimarily
ğ‘ âˆ‘ï¸ 1 âˆ‘ï¸ ğ‘ adheretothedefaulthyperparameters.Duringtheinferencestage,
ğ‘†(ğ· ğ‘)= ğ·m
ğ‘ğ‘
âŠ†a ğ·x
Ë† ğ‘ğ‘
ğ¼ğ‘–âˆˆğ·
ğ‘ğ‘ğ‘ ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)+
ğ‘˜
Â·
ğ¼ğ‘–âˆˆğ·
ğ‘ğ‘ğ‘‘ğ‘–ğ‘ (ğ¼ ğ‘–,ğ· ğ‘)
w ine Ce hm ipp Nlo ey ma ogr [e 2e 1d ],y td oe mco id tii gn ag test tr ha ete sg iy g, na ik fii cn at no tt ch oe ma pp ip laro tia oc nh cu os se td
s
associatedwiththisprocess.HDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
Table6:SampleHDLerrorandmodificationfunctions.
ErrorType ErrorCause ModificationFunction
Memory RAMconnectionorRAMport Pulsesignalmodification
Clock Clockunitconnectionandavailability Pulsesignalorassignmentmodification
Trace Traceunitavailabilityandusage Assignmentmodification
Compression CompressionunitdefinitionandI/O Registerorassignmentmodification
Data DataValue Registerorpulsesignalmodification
TestPoint Procedurescompleteness Allvariablesmodification
PatternReform Patternequivalence Assignmentandprobemodification
SyntaxE Sscriptsyntaxerror Syntaxmodification
NetlistE Netlistattribute Netlistmodification
SimulationE Simulationconfig Configmodification
1 Theerrorshereareabstractedandmodifiedforinformationsecurity,butstillreflectthecategoriesandcauseswithoutrevealing
details.
Table7:Examplesoferrorinformation.
Errorcode Descriptions Root-Reasons Solutions
C-error-1 Netlistnotcorrectlyobtaindefinedsignal Definitionnotmatchnetlist Modifydefinitionornetlist
C-error-2 Notdefinetopportasoutput Signalnotoutput Modifytopportasoutput
C-error-6 Clocksignalnotoffduring** **getwrongsignal Correctlyoffclocksignal
T-error-2 Clockdefinitionduplicate clocknameconflict Deleteonedefinition
T-error-4 Probeinitilizedas0in** Invalidinitialization Modifyprobeinitializationtonon-0
T-error-18 Assignmentinnon-initializationstage Wrongassignment Deleteassignment
T-error-27 Pulsenon-existvariable wrongpulse Deletepulse
M-error-1 Memoryclosewhenread Wronglyoffmemory Correctmemoryandconstraint
M-error-17 Logicalloopinnetlist Logicalloopinnetlist Modifynetlist
P-error-8 Definitionlackofshift Wrongdefinition Modifydefinition
1 Theruleshereareabstractedandmodifiedforinformationsecurity,butstillreflectthemeanings,causesandsolutionswithoutrevealingdetails.
Algorithm2ThoughtsGenerationFlow Algorithm1Top-ğ‘˜relevantcodeselection
Input: Thoughtsgenerationpromptğ‘ ğ‘¡,correctresponseprompt Input: Buggycodequeryğ‘=(ğ‘,ğ‘’)andğ‘ codeinstancesğ·Ë† ğ‘ğ‘ ,and
ğ‘ ğ‘,buggycodeğ‘,correctcodeğ‘,errormessageasğ‘š,retrieved parameterğ‘˜.
informationasğ‘Ÿğ‘ğ‘”ğ‘‘,largelanguagemodelasğ¿ğ¿ğ‘€,Numberof Output: Top-ğ‘˜instancesğ· ğ‘ğ‘ âŠ†ğ·Ë† ğ‘ğ‘ .
thoughtstogenerateğ‘˜. 1: Initialize:ğ·Ë† ğ‘ğ‘ â†âˆ….
Output: Generatedthoughtsti. 2: forğ¼ ğ‘— âˆˆğ·Ë† ğ‘ğ‘ do
1: Initialize:ğº â†{},ğ¶ â†{},ğ· â†{}. 3: ğ‘†ğ‘–ğ‘š(ğ‘,ğ¼ ğ‘–)â†Eq.(1)
2: forğ‘— =1toğ‘˜do 4: endfor
3: ğ‘¡ ğ‘— â†ğ¿ğ¿ğ‘€(ğ‘ ğ‘¡ â—¦ğ‘â—¦ğ‘â—¦ğ‘šâ—¦ğ‘Ÿğ‘ğ‘”ğ‘‘)//Generate thoughts. 5: forğ‘– =1toğ‘˜do
4: ğº â†ğºâˆªğ‘¡ ğ‘— //Append ğ‘¡ ğ‘— to ğº. 6: forğ¼ ğ‘— âˆˆğ·Ë† ğ‘ğ‘ do
5: endfor 7: â–³ğ‘†(ğ¼ ğ‘—|ğ· ğ‘ğ‘ )=ğ‘†(ğ¼ ğ‘— âˆªğ· ğ‘ğ‘ )âˆ’ğ‘†(ğ· ğ‘ğ‘ )
6: foreachğ‘¡ ğ‘— inğº do 8: endfor
87 ::
scr
ğ¶ğ‘ iğ‘—
p
â†tâ†
.
ğ¶ğ¿ğ¿ âˆªğ‘€
ğ‘
ğ‘—(ğ‘ /ğ‘ /â—¦ Apğ‘ pâ—¦ enğ‘š dâ—¦ ğ‘ğ‘Ÿ ğ‘–ğ‘ğ‘” tğ‘‘ oâ—¦ ğ¶ğ‘¡ .ğ‘—)//Predicted correct
1 10
19
:
:: ğ¼
ğ·
ğ·Ë†âˆ—
ğ‘ ğ‘ğ‘
ğ‘=
=
=a
ğ·
ğ·r Ë†g
ğ‘ ğ‘ğ‘
ğ‘m
\
âˆªa
ğ¼
ğ¼x
âˆ—
âˆ—ğ¼ğ‘—âˆˆğ·Ë† ğ‘ğ‘â–³ğ‘†(ğ¼ ğ‘—|ğ· ğ‘ğ‘ )
9: endfor
12: endfor
11 10 :: for Cea alc ch uğ‘ lağ‘— tein edğ¶ itd do
istanceğ‘‘ ğ‘— â†EditDistance(ğ‘,ğ‘ ğ‘—)
13: returnğ· ğ‘ğ‘ .
12: ğ· â†ğ·âˆªğ‘‘ ğ‘— //Append ğ‘‘ ğ‘— to ğ·.
13: endfor C.2 ThoughtsGeneration
14: Sortğ·andobtainrelatedthoughtsğ‘¡âˆ— ğ‘—, AsAlgorithm2illustratetheflowofthoughtsgeneration,Forsim-
15: returnğ‘¡âˆ— ğ‘—. plicity,weomitthesampleindex.Theprocessmainlyconsistof4
majorsteps.WefirstlyconcatenateThoughtsgenerationpromptğ‘ ,
ğ‘¡
Buggycodeğ‘,Correctcodeğ‘,Errormessageğ‘š,relevantinforma-
tionğ‘Ÿğ‘ğ‘”ğ‘‘,andpasstoLLMforinference,resultofwhichstandsfor
generatedthoughtsğ‘¡1...ğ‘¡ ğ‘˜.Afterobtainingthethoughtsforthe
errorcode,wethenconcatenatecorrectresponsepromptğ‘ ,Buggy
ğ‘KDDâ€™24,August25â€“29,2024,Barcelona,Spain TrovatoandTobin,etal.
#1: Task Requirement Prompt thecorrectcodeasğ‘ ğ‘–andthoughtsasğ‘¡ ğ‘–.Generally,computeğ‘(ğ‘ ğ‘–|ğ‘¡ ğ‘–)
proveschallengingduetotheinaccessibilityofsuitablethoughts.
Below is an error snippets, paired with an error message that provides
error information. ByleveragingBayesâ€™rule,wecanreparameterizetheformulation
Return the correct snippets based on the error message. asğ‘(ğ‘ ğ‘–|ğ‘¡ ğ‘–) âˆğ‘(ğ‘¡ ğ‘–|ğ‘ ğ‘–)ğ‘(ğ‘ ğ‘–).Byfocusingonğ‘(ğ‘¡ ğ‘–|ğ‘ ğ‘–),weencourage
theLLMtoalignitsoutputswithverifiedcorrectcode.
#2: Retrieved Code Instances
Example1: Example2: Example3: Example4: C.3 Temperaturesetting
Buggy Code: Buggy Code: Buggy Code: Buggy Code: â€¦ Bymodifyingtemperaturewecangeneratemultiplethoughtsdiffer-
â€¦ â€¦ â€¦ â€¦ ently.Weusemultinomialsamplingtogeneratesamplesrandomly.
Correct Code: Correct Code: Correct Code: Correct Code: Defineasequenceofinputtokensx={ğ‘¥1,ğ‘¥2,...,ğ‘¥ ğ‘›},denoteğœ‹ ğœƒ as
â€¦ â€¦ â€¦ â€¦ theLLMgenerationwithoutpassingfinalsoftmaxlayer.Thenext
tokenğ‘¥ ğ‘›+1canbeobtainedby:
â€¦#3 : Buggy Code #4: Error M Soe ls usa tig oe
n
&
s
Retrieved ğ‘œğ‘¢ğ‘¡ =ğœ‹ ğœƒ(x)
assign_pi
1 procedure rule TPT-83 occurred(Not
ğ‘ğ‘Ÿ =ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ ğ‘‡(ğ‘œğ‘¢ğ‘¡) (12)
ap sro sib ge n_ rp so
t_n 1
a â€¦llow to pulse non-clock pin in timeplate) ğ‘¥ ğ‘›+1âˆ¼ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™(ğ‘ğ‘Ÿ),
pulse_clocks The reason is that the define_timeplate whereğ‘œğ‘¢ğ‘¡ isthenexttokenlogitsoutput.ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ ğ‘‡ meansthe
â€¦ block â€“clocks contains a non-clock pin softmax function with temperature where the formula of prob-
â€¦
ability ğ‘ğ‘Ÿ is
ğ‘’ğ‘œğ‘¢ğ‘¡ğ‘–/ğ‘‡
.ğ‘‡ is the temperature parameter where
ğ‘– (cid:205) ğ‘—ğ‘’ğ‘œğ‘¢ğ‘¡ğ‘—/ğ‘‡
#5: Prompt Instruction higherğ‘‡ makestheoutputdistributionmoreuniform,thusintro-
P#l7ea: sTe hreotuurng hthtes cGorerencet rsnaitpipoents based on the current error snippets. ducingmorerandomness.ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ meansCategoricaldistribu-
tion.Forexample,assumethenexttokenprobabilitydistribution
#6: Response is{ğ‘‘ğ‘œğ‘” : 0.4,ğ‘ğ‘ğ‘¡ : 0.5,ğ‘ğ‘–ğ‘˜ğ‘’ : 0.1},thenthenexttokenisselected
It appears that the primary issue in your HDL script is related accordingtotheirprobabilities.Wecangenerateğ¾thoughtsbythis
to the incorrect use or definition of a clock signal. The error samplingstrategy.Therefore,wecangeneratethoughtsrandomly
message "A floating pin is defined as a clock" suggests that bycontrollingthetemperatureparameter.
there is a problem with how a clock It appears that the primary
issue in your HDL script is related to the incorrect use or
C.4 Debuggingexample
definition of a clock signal. The error message "A floating pin
is defined as a clock" suggests that there is a problem with how Figure8illustratesanexampleofHDLdebugging.Whengiven
a clock signal is being handled or defined in your script. The anerrorscriptandrelatederrormessages,wefirstuseoursearch
error seems to be associated with the rst_n signal. is being enginetoretrievedrelatedexpertsolutionsandsimilarin-context
handled or defined in your script. The error seems to be samples (default 5 buggy code instances). Then we feed buggy
associated with the rst_n signal.
codes,errormessages,andrelatedin-contextsamplesandexpert
solutionstoourfine-tunedLLMs.Finally,theLLMwillproducea
The correct script is:
correspondinganalysisandrelatedcorrectscript.
(cid:1)
assign_pi
probe_po D ANALYSISANDDISCUSSION
pulse_clocks This section delves into a detailed examination of our research
(cid:1) findings,focusingontheutilityandimplicationsofleveragingLLMs
Figure8:AcaseofHDLdebugging. fordebuggingwithinHDLenvironments.Weprovideathorough
assessmentofLLM-baseddebuggingcapabilitiesandtheeffects
codeğ‘,Errormessageğ‘š,,andgeneratedthoughtğ‘¡ ğ‘— toconstruct ofiterativedebuggingprocedures.Additionally,weexploreboth
thestep2inputforLLM.Theinferenceresultdenotesthepredicted thechallengesandprospectsofincorporatingLLMsintotheHDL
correct code scriptğ‘1...ğ‘ ğ‘˜ for correction the given error code. debuggingframework.
Givenabovetwosteps,thecodecorrectionispreparedsuccessfully.
Thereststepswillfocusonfindingthebestsuitedcorrectionfor D.1 Trade-offsinRAGandSFT
givenerrorcode.Toaccomplishthat,forgivenpredictedcorrect
Typically,inaRetrieval-AugmentedGeneration(RAG)system,the
codescriptğ‘1...ğ‘ ğ‘˜,weobtaintheeditdistanceğ‘‘1...ğ‘‘
ğ‘˜
fromeach
LLMactsasanagentthatremainsuntunedtopreserveitsgen-
tooriginalcorrectcodeğ‘.Thefinalprocedureistosorttheedit
eralizationability,whichmightbecompromisedbytask-specific
distancesandretrievethegeneratedthoughtğ‘¡âˆ—correspondingto
ğ‘— parameteroptimization.However,toenhanceitsefficacyindebug-
thelowesteditdistanceğ‘‘ ğ‘—.Thefinalğ‘¡âˆ— ğ‘— servesasourthoughoutput ging,wefinditessentialtofine-tunetheLLM,whichpresentsa
fortheflow. significantchallengeasLLMsareexpectedtoaddressabroaderar-
Wedelvedeeperintotheprocessofself-guidedthoughtgenera- rayofproblems.Onesolutionincorporateawiderrangeofcommon
tionThecoreprincipleofourapproachinvolvessubmittingboth instructionaldataduringtrainingtoretainitsgeneralapplicability.
thebuggycodeanditscorrectedversiontotheLLM,subsequently Anotherinnovativestrategyinvolvesconstructingamulti-LLM
promptingthemodeltogenerateinstructiveguidance.WedenoteHDLdebugger:StreamliningHDLdebuggingwithLargeLanguageModels KDDâ€™24,August25â€“29,2024,Barcelona,Spain
architecturehousingnumerous"expert"modelstotacklespecific D.3 EnhancingtheUserExperiencein
domainchallenges.Additionally,HDLdebuggingencompassesvar- DebuggingSystems
ioustasksacrossdifferentstagesofelectronicdesignautomation,
Withinourdebuggingframework,wecanquantitativelyassess
promptingustoconsiderdevelopingacomprehensiveframework
performancemetricssuchaspassratesandexecutiontimes.How-
infutureinvestigations.
ever,duringthedeploymentphase,weobservedthattraditional
debuggingapproachesâ€”transformingerroneousscriptsintocorrect
D.2 IterativeDebugging
onesareinsufficient.InthecontextofHDL,theemphasisextends
Ourinvestigationpredominantlyconcentratesonsingle-roundde- beyondmereerrorcorrectiontoenhancingthequalityoftheimple-
bugging,notingthatexistingstudies,suchas[35],demonstrate mentation,giventhedirectimpactofhardwarelanguageonchip
thatavastmajorityofissues(about90%)areresolvableinasingle performance.Codethatsuccessfullycompilesmaynotnecessarily
iteration.Nonetheless,complexscenariosnecessitatemultiplede- optimizechipdesignperformance.Additionally,exactsolutionsare
buggingrounds,posingasubstantialchallengeduetothehighcosts notalwaysrequired;engineersoftenbenefitfromsuggestive"hints"
associatedwithgatheringmulti-rounddata.Moreover,effective thatinspiresolutionstocomplexissues.Nonetheless,developinga
iterativedebuggingdemandsLLMscapableofenhancedcontextual metrictoevaluatethequalityofthesehintsrepresentsasignificant
analysisandcomprehension.Thisareawillbeafocalpointofour challenge.
subsequentresearchendeavors.