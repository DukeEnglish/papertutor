EAGLE: Exploring The Design Space for Multimodal
LLMs with Mixture of Encoders
MinShi2‚àó,FuxiaoLiu3‚àó,ShihaoWang4,ShijiaLiao1,SubhashreeRadhakrishnan1,
De-AnHuang1,HongxuYin1,KaranSapra1,YaserYacoob3,HumphreyShi2,
BryanCatanzaro1,AndrewTao1,JanKautz1,ZhidingYu1‚Ä†,GuilinLiu1‚Ä†
1NVIDIA 2GeorgiaTech 3UMD 4HKPU
https://github.com/NVlabs/Eagle
MME MME Eagle
MMM 43U .142.7Te 3x 36t 6.V 4 .1Q 7A 7 37. 511 .. 7 89 1 6.7 67 5.1 6.2 4.9 11 1 1 1155 5 5 5575 4 2 1599 7 3 94 81.881.983 8.V 8 6 64 .Q 5.7A 87v .2 688.58P 8O .9PE ChartQ 67A .4D 6o 7c .0 6V 2Q .0A 78.9 77.5 63.3 45.1 1 11 1 5 45 5 1 87 3 1 89 1 3694965O 55 44C 8RB 86e .n 7ch 87.988.5POPE HiL gL hB a a V rs eA &e s-l oi vn l1 ue t.5 ion High S T Fl t uhi rg loo lyh rn o t ogs u p p g ehe nr d -f so e or sm uig ra n cn e s c p (e da ac te a e +x p relo cir pa eti )on
41.8 85.6 Better recipe
M Via s Mt th a MB5 e2. n5 c4 h9 7. 50 7 O. 593 C.99 R.0 B63 e84 . n. 86 ch63 6 21 7
6
7. .4 24 4554 876 137 ... 030 64.2 76 0 75 .2 3.6 71 .4
5
47 7. S.0 60
7
E. .1
3
ED766 144 -.. 6 I. 2 m2 86 0 a4 . g. 46 S8 e6 4 c4 . i3. e9 ncG eM QQ M AA Benc M6h 8 a.8 th6 V8 i.7 sta6 37 9. .4 036 46 .1.5 32.332. 81
38 .28 6.1
97 .9 7.6 67. 77
70 1. 77 .6
3.535.15 38 9.5 .6 46 74 4.. 10
8.9
In7 fo1. V6 Q7 A1.9TextVQA PM rM eo e -o ar x re lp e
i &
ge v D nris t masio t ean
n t
V Ai ds vio M S O ain npmi- x ctC pt i emue l dern a e T&t l r ro e i aec f n
i
x nv cD t oi ies ne di no gs edi n rg Ra cn e b eon l ce mc
i
po f bu ed is sne i ar os tn io d nesign
ChartQA VQAv2 SEED-Image High resolution & fusion adaptation
Llama3-8B Vicuna-7B
Eagle-X5 Eagle-X5 LLaVA-HR Design Space Exploration (Vicuna-7B) Vision expert pre-alignment
Cambrian-1 LLaVA-NeXt Mini-Gemini
Figure1: OverviewofEagle.Eagleisafamilyofmultimodallargelanguagemodels(MLLMs)with
amixtureofvisionencoders. Left: comparisonsbetweenEagleandexistingcompetitiveMLLMs
withVicuna-7B[15]andLlama3-8B[3],withEagleachievingfavorableresultsonall12benchmarks.
Middle: an evolutionary road map of the design space and advanced training recipes leading to
consistentandsignificantimprovements. Right: highlightsandcorefeaturesofEagle.
Abstract
Theabilitytoaccuratelyinterpretcomplexvisualinformationisacrucialtopicof
multimodallargelanguagemodels(MLLMs). Recentworkindicatesthatenhanced
visualperceptionsignificantlyreduceshallucinationsandimprovesperformance
onresolution-sensitivetasks,suchasopticalcharacterrecognitionanddocument
analysis. AnumberofrecentMLLMsachievethisgoalusingamixtureofvision
encoders. Despite their success, there is a lack of systematic comparisons and
detailedablationstudiesaddressingcriticalaspects,suchasexpertselectionandthe
integrationofmultiplevisionexperts. Thisstudyprovidesanextensiveexploration
ofthedesignspaceforMLLMsusingamixtureofvisionencodersandresolutions.
Our findings reveal several underlying principles common to various existing
strategies,leadingtoastreamlinedyeteffectivedesignapproach. Wediscoverthat
simplyconcatenatingvisualtokensfromasetofcomplementaryvisionencodersis
aseffectiveasmorecomplexmixingarchitecturesorstrategies. Weadditionally
introducePre-Alignmenttobridgethegapbetweenvision-focusedencodersand
languagetokens,enhancingmodelcoherence. TheresultingfamilyofMLLMs,
Eagle,surpassesotherleadingopen-sourcemodelsonmajorMLLMbenchmarks.
‚àóEqualcontribution.WorkdoneduringaninternshipatNVIDIA.
‚Ä†Equaladvising.Correspondingauthors:{guilinl,zhidingy}@nvidia.com.
Preprint.Underreview.
4202
guA
82
]VC.sc[
1v89951.8042:viXra1 Introduction
The success of large language models (LLMs) has triggered significant interest in enabling their
visualperceptioncapability,suchthattheycouldsee,understand,andreasonintherealworld. Atthe
centerofthesemultimodallargelanguagemodels(MLLMs)[23]isatypicaldesignwhereimagesare
convertedintoaseriesofvisualtokensbythevisionencodersandappendedwiththetextembeddings.
CLIP[63]isoftenchosenasthevisionencodersinceitsvisualrepresentationisalignwiththetext
spacebypre-trainingonimage-textpairs. Dependingonthearchitectures,trainingrecipesandthe
wayhowvisiontokensareinjectedintothelanguagemodel,thereexistvariousnotablefamiliesof
MLLMssuchasFlamingo[4],BLIP[44,43,16],PaLI[11],PaLM-E[18]andLLaVA[54,52]. Most
oftheseworkskeeprelativelylowinputresolutionsduetothelimitsonpre-trainedvisionencoders
andLLMsequencelength.
Recentstudies[47,53]showthatstrongervisionencoderdesignisimportantformitigatingMLLM
hallucinations[50,77]andimprovingresolution-sensitivetaskslikeopticalcharacterrecognition
(OCR).Aconstellationofworksthusfocusesonenhancingthecapabilityofthevisionencoder. For
example,scalingupthepre-trainingdataandparametersofvisionencoder[13]ordividingimages
intolow-resolutionpatches[53,67]. However, theseapproachesusuallyintroducelargetraining
resources. Anefficientyetpowerfulstrategyistomixingvisualencoderspre-trainedwithdifferent
tasksandinputresolutions,eitherfusinghigherresolutionencoderswiththeCLIPencoder[59,45],
sequentiallyappendingfeaturesfromdifferentencoders[20,49],oradoptingmorecomplexfusion
androutingstrategiestomakethebestofdifferentencoders[37,88]. Sucha‚Äúmixture-of-vision-
experts‚Äùmodeisshowntobeeffective. However,adetailedstudyofitsdesignspacewithrigorous
ablationisstilllacking,whichmotivatesustorevisitingthisarea. Questionssuchaswhichvision
encodercombinationtochoose,howtofusedifferentexpertstogether,andhowtoadjusttraining
strategieswithmorevisionencodersremaintobeanswered.
Toaddresstheabovequestions,ourworksystematicallyinvestigatesthemixture-of-vision-encoders
designspaceforimprovedMLLMperception. AsshowninFig.1,ourexplorationofthedesign
spaceconsistsofthefollowingsteps: 1)Benchmarkingvariousvisionencodersandsearchingrecipes
forhigherresolutionadaptation;2)‚ÄúApplestoapples‚Äùcomparisonbetweenvisionencoderfusion
strategies;3)Progressiveidentificationoftheoptimalcombinationofmultiplevisionencoders;4)
Improvedvisionexpertpre-alignmentanddatamixture. Ourstudycoverstheperformanceofvision
encoderspre-trainedondifferenttasksandresolutions(e.g.,vision-languagealignment[31,14,63,
66],self-supervisedlearning[62],detection[22,21],segmentation[36],andOCR[38]). Weuse
around-robinapproachtoincorporateadditionalvisionexperts. StartingwiththebasicCLIP[63]
encoder,weaddoneadditionalexpertatatimewiththebestimprovementineachround.
OurworkisnotthefirstonetoleveragemultiplevisionencodersinMLLM.However,thesystematic
studyleadstoseveralinterestingnewfindingsunderthissetting:
‚Ä¢ UnlockingthevisionencodersduringMLLMtrainingmatters. Thisisinsharpcontrasttothe
LLaVA[54,52]familyandmanyworksthatconsidermultiplevisionencodersorteachers[49,
55,20,33,64,37],wherefreezingthevisionencodershasbeenacommonchoice.
‚Ä¢ Somerecentlyproposedfusionstrategies[59,45]donotshowsignificantadvantagesdespite
theiradvanceddesigns. Instead,wefindthatstraightforwardchannelconcatenationstandsoutas
asimpleyetcompetitivefusionstrategy,offeringthebestefficiencyandperformance.
‚Ä¢ Incorporatingadditionalvisionexpertsleadstoconsistentgain,makingitapromisingpathto
systematicenhanceMLLMperceptionbesidesscalingupsingleencoders. Theimprovementis
particularlypronouncedwhenvisionencodersareunlocked.
‚Ä¢ Weproposeapre-alignmentstagewherenon-text-alignedvisionexpertsareindividuallyfine-
tunedwithafrozenLLMbeforetrainedtogether. ThisstageisfoundtoenhancetheMLLM
performancesignificantlyunderthemixture-of-vision-encoderdesign.
WefinallyconcludeourfindingsintoafamilyofMLLMstermedEagle.Eagleisevaluatedonaseries
ofbenchmarks,includingvisualquestionanswering,OCR/documentrelatedtasks,andbenchmarks
tailoredforMLLMs. Ourmodelattainsstate-of-the-artperformanceacrossdifferentbenchmarks
anddemonstratesobviousadvantagesonOCRanddocumentunderstandingtasks. Usingthesame
pre-train and supervised fine-tuning data from Cambrian-1 [73] - a concurrent family of vision-
centricMLLMssharingsimilardesignspirits,Eaglemodelsoverallachievebetterperformance. We
2Step1: High-resolution Adaptation and Training Recipe Step2: Fusion Paradigm Exploration
Input Images Freeze/Unfreeze/Pre-Align
Vision Feature
Vision LanE gx up ae gr et1 Alignment in flt ae tr tp enolate . . . . . . . . . . . . . . . le d o
Freeze/Unfreeze/Pre-Align M
Text E Rx ep ce or gt n3 ition in flt ae tr tp eo nlate . . . Projector e g a
u
Freeze/Unfreeze/Pre-Align g
n
ObjecE tx p De er tt e2 ction in flt ae tr tp eo nlate . . . . . . a L e
Freeze/Unfreeze/Pre-Align g
SemantiE cx Sp ee gr mt4 entation in flt ae tr tp enolate . . . . . .
ra
L
Text Embedding
Sequence Append Channel-wise Concatenation LLAVA-HR MiniGemini Deformable Attention
Low Resolution Low Resolution Low Resolution Low Resolution Low Resolution
flatten. . . . . . flatten. . . . . . . . . BlV oi cT k fs use. . . BlV oi cT k fs useflatten. . . Q K&V flatten . . . Q flatten . . .
cross-
flatten . . . . . . res +iz e . . . BC loo cn kv s . . . BC loo cn kv s attention Re sf ae mre pn lc ine g p po oin int ts
flatten
High Resolution High Resolution High Resolution High Resolution High Resolution
Figure2: OverviewoftheEagleexplorationpipeline. WeinvestigatetheMLLMdesignspace
withmultiplevisionencoders,aimingtoidentifyoptimizeddesignchoicesandimprovetheMLLM
perception. Weincludevarioustypesofvisionexpertspre-trainedondifferenttasksandresolutions.
Wethensystematicallycompareaseriesofpopularfusionstrategiesundercontrolledsettings,includ-
ingSequenceAppend,ChannelConcatenation,LLaVA-HR[59],Mini-Gemini[45],andDeformable
Attention[86]. Finally,theoptimalfusiondesignisfurtherextendedtomultiplevisionencodersto
formastrongMLLMperception.
hopethattheEaglecanprovideahighlyperformantandeasy-to-reproduceMLLMsolutiontothe
community.
2 Designspaceexploration
Inthissection,weshowhowtoutilizetheadvantagesofdifferentvisionencodersviastep-by-step
investigations,yieldingtheEaglemodelfamily. Unlikepreviousmethodsfocusingonnewfusion
strategiesorarchitecturesamongvisionencodes,ourgoalistoidentifyasetofminimalisticdesign
tofusedifferentvisionencoderssupportedwithdetailedablations,removinganyunnecessaryparts.
AsshowninFig.2,westartfromextendingthebasicCLIPencoder[63]toasetofvisionexperts
withdifferentarchitectures,pre-trainingtasksandresolutions. Withtheseexperts,wethencompare
differentfusionarchitecturesandmethods,andstudyhowtooptimizethepre-trainingstratgiesgiven
moreencoders. Wealsogiveadetailedanalysisinhowtoselectthevisionencoderstobeintegrated.
Finally,weputallthefindingstogetherandfurtherextendtomultipleexpertvisionencoderswith
differentresolutionsanddomainknowledge.
2.1 Basesetup
WeadoptLLaVA‚Äôs[54]modelarchitectureasthebasis,whichconsistsofalargelanguagemodel,a
visionencoder,andaprojectionlayer. Theprojectionlayerprojectsthevisualembeddingfromthe
visionencoderintothetextembeddingspace. Wemainlyfocusonthevisionencoderdesign.
Base training data. We use the same pre-training data as LLaVA-1.5 [52], which consists of
595kimagetextpairs. Forthesupervisedfine-tuningstage,wecollectdatafromaseriesoftasks
and convert them into multimodal conversations, including: LLaVA-1.5 [52], Laion-GPT4V [1],
ShareGPT-4V [8], DocVQA [61], synDog-EN [35], ChartQA [60], DVQA [32], and AI2D [34],
resultingin934ksamples.
3Table1: Comparisonofdifferenthigh-resolutionadaptionmethods. Directinterpolationismore
efficientandeffective. ‚ÄúRes‚Äùdenotesinputimageresolution. ‚Äú#Token/s‚Äùdenotestheinferencespeed
ofthewholeframework. ‚Äú#Token(V)‚ÄùdenotesthenumberofvisualfeaturetokensfedintoLLM.
‚ÄúOCR‚ÄùdenotesOCRBench[57]. ‚ÄúAvg‚Äùdenotesaverage. NotethatCLIP-448getsverycloseto
InternVLintermsofperformance,despitethesignificantlysmallermodelsize(300Mvs. 6B).
Method Unfreeze Res #Token/s #Token(V) GQA MME MMMU OCR SQA POPE Avg
Original ‚úó 336 47.2 576 62.46 1488 36.1 369 72.79 86.77 615
Original ‚úì 336 47.2 576 63.68 1496 36.6 369 72.22 85.9 617
Interpolate ‚úó 448 47.9 1024 62.45 1484 34.4 285 72.27 86.54 598
Interpolate ‚úì 448 47.9 1024 64.62 1531 35.7 496 72.77 87.88 645
Interpolate ‚úì 672 44.5 2304 64.51 1492 36.2 492 72.86 87.49 641
Tiled-input ‚úì 672 44.8 2304 63.59 1455 34.8 445 71.94 87.83 625
InternVL[13] ‚úì 448 46.2 1024 65.12 1521 36.2 527 72.46 87.13 649
Implementationdetails. Wefirstpre-trainthemodelwithimage-textpairsforoneepochwitha
batchsizeof256,wherethewholemodelisfrozenandonlytheprojectorlayerisupdated. Inthe
secondstage,wefurtherfine-tunethemodelonthesupervisedfine-tuningdataforoneepochwitha
batchsizeof128. Forthisexploration,weemployVicuna-7Bastheunderlyinglanguagemodel. The
learningratesaresetto1e-3forthefirststageand2e-5forthesecondstage,respectively.
2.2 StrongerCLIPencoder
WestartourexplorationwiththeCLIPmodelsinceithasbecomethemainchoiceformanyMLLMs.
WhileCLIPmodelsareknowntobenefitmultimodaltasks,theirdrawbackshavealsobeenwell-
studied. Forinstance,manyexistingMLLMs[52]tendtousethepre-trainedCLIPresolutions(such
as 224√ó224 or 336√ó336) as their input resolutions. In these cases, the encoders often fail to
capturefine-graineddetailsthatareimportantforresolution-sensitivetaskslikeOCRanddocument
understanding[47].
To handle increased input resolution, a common practice is to use tiling where input images are
dividedintotilesandencodedseparately[53,47]. Anothersimplermethodistodirectlyscaleup
the input resolution and interpolate the position embeddings of the vision transformer model if
needed[9,10,6]. Wecomparethesetwoapproacheswithfrozen/unfrozenvisionencodersunder
differentresolutions,withtheresultsshowninTable1. Ourfindingscanbesummarizedasfollows:
‚Ä¢ UnfreezingtheCLIPencoderleadstosignificantimprovementwheninterpolatingtoahigher
MLLMinputresolutionthatisdifferentfromtheCLIPpre-trainingresolution. Thereisalsono
performancedegradationwhenresolutionsremainthesame.
‚Ä¢ When the CLIP encoder is frozen, directly adapting it to a higher MLLM input resolution
considerablyhurtstheperformance.
‚Ä¢ Among the compared strategies, directly interpolating to 448√ó448 with an unfrozen CLIP
encoderisshowntobebotheffectiveandefficientintermsofperformanceandcost.
‚Ä¢ ThebestCLIPencodergetsclosetoInternVLinperformancedespitethesignificantlysmaller
modelsize(300Mvs. 6B)andlesspre-trainingdata.
ItisworthnotingthatCLIP-448allowsustomatchthesettingwithLLaVA-HR[59]andInternVL[13]
wheretheCLIPencodersaresimilarlyadaptedtotake448√ó448inputandoutput1024patchtokens3.
Inthefollowinginvestigation,westicktothissimplestrategyofscalinguptheinputresolutionand
unlockingthevisionencoderduringtraining.
2.3 Visionexperts
Tobetterestablishthefoundationformulti-visionexpertfusion,weextendthetoolboxwithvision
experts pre-trained on different tasks and resolutions, and verify our findings on high-resolution
3ThelatestInternVL1.0andInternVL1.2modelsalsoadoptapixelshufflemodule. Weremoveitinour
experimenttoensurefullalignment.Seehttps://github.com/OpenGVLab/InternVLfordetails.
4Table2: Detailedsettingofthepre-trainedvisionexperts.
Expert Category Resolution TokenNumber Post-Processing VariantHyper-links
CLIP Image-TextMatching 448 1024 None VIT-L
ConvNeXt ImageClassification 1024 1024 None ConvNeXt-XXL
EVA-02 ObjectDetection 1024 1024 Resize EVA-02-Large
Pix2Struct TextRecognition 1024 1024 Resize Pix2Struct-02-Large
DINOv2 Self-supervised 448 1024 None dinov2_vitl14_reg
SAM ImageSegmentation 1024 1024 Pixel-unshuffle SAM-Large
Table3: ComparisonbetweendifferentvisionexpertsastheMLLMencoders. ‚ÄúRes‚Äùdenotes
resolution. ‚ÄúOCR‚ÄùdenotesOCRBench[57].
Category VisionTower Unfreeze Res GQA MME MMMU OCR SQA POPE TextVQA
‚úó 1024 63.71 1473 34.2 402 71.92 87.42 66.95
VLAlignment ConvNeXt
‚úì 1024 63.71 1433 37.0 527 72.01 87.91 71.66
‚úó 1024 54.87 1193 34.8 32 70.34 83.31 45.04
Segmentation SAM
‚úì 1024 60.05 1305 35.9 38 70.64 86.62 45.71
‚úó 1024 63.54 1422 35.8 58 70.86 88.50 47.86
ObjectDetection EVA-02
‚úì 1024 64.93 1474 35.7 387 71.78 88.96 59.79
‚úó 1024 52.47 1217 35.7 443 69.61 77.54 56.56
TextRecognition Pix2Struct
‚úì 1024 54.05 1220 34.3 425 70.29 79.21 57.05
‚úó 448 60.81 1269 34.0 39 70.76 86.42 45.71
Self-supervised DINOv2
‚úì 448 64.06 1371 34.2 37 71.35 87.29 46.77
adaptationwiththeseexperts. Thisalsohelpsusidentifythedistinctadvantagesofdifferentexperts.
We collect a set of vision experts, including: (1) Vision-Language Alignment: CLIP [63] and
ConvNeXt [76] from OpenCLIP [31, 66]. (2) Object-Centric: EVA-02 [22, 21] pre-trained on
detection datasets. (3) OCR: Pix2Struct [38].(4) Segmentation: SAM [36]. (5) Self-supervised:
DINOv2[62]. Thedetailedinputresolutionandcheckpointofeachvisionencodercanbefoundin
Table2. Weresizetheoutput2Dfeaturemapsofeachvisionencoderusingbilinearinterpolationand
pixelshuffle[68]toensurethatthevisualtokennumberequals1024.
TheresultsinTable3revealthatMLLMswiththesetask-specificvisionencodersachieveoptimal
performanceintheirpre-trainingdomains. EVA-02excelsintheobjecthallucinationevaluation
benchmark POPE [46] and general visual question answering benchmark GQA [30]. CLIP and
ConvNeXtperformwellacrossallbenchmarks,benefitingfromtheirtrainingonlarge-scaleimage-
textpairsusingcontrastiveloss. Conversely,whilePix2Structexcelsintextrecognition,itshows
limitedcapabilityinobjectrecognitionandgeneralVQAtasks,likePOPEandGQA.DINOv2and
SAM,pre-trainedwithself-supervisedlearningandsemanticsegmentationrespectively,strugglewith
textrecognitiontasks. NotethattheconclusionmadeinSec.2.2alignswiththeresultshere. Both
ConvNeXtandEVA-02showsignificantperformancegainswhenthevisionencodersareupdated
duringtraining.
2.4 Fusionstrategy
ExistingMLLMframeworkshaveproposedvariousfusionstrategiestoensembletheencodersin
Table3,withthehopethattheirdomain-specificstrengthscanbeleveraged.Inallcases,improvements
inMLLMperformancehavebeenreportedwiththefusionofvisionencoders. However,theroles
ofthefusionstrategiesaspartoftheirMLLMarchitectureinnovations,havenotbeendecoupled
andclearlystudiedunderan‚Äúapplestoapples‚Äùcomparison. Itisthusnotentirelyclearhowmuch
improvement is from the fusion strategies themselves versus the improved representations from
variousencoders.
Wenoticethatexistingpopularfusionstrategies,despitetheirvariationsindesigns,canbebroadly
representedbythefollowingseveralcategories: (1)SequenceAppend: directlyappendingthevisual
tokensfromdifferentbackbonesasalongersequence[20,33,55];(2)ChannelConcatenation: con-
catenatingthevisualtokensalongthechanneldimensionwithoutincreasingthesequencelength[49];
5Table4: Comparisonofdifferentfusionmethodsfordifferentvisionexperts. SA,CC,LH,MG
andDAdenotethefusionstrategies: SequenceAppend,ChannelConcatenation,LLaVA-HR[59],
Mini-Gemini[45]andDeformableAttention,respectively. ‚ÄúToken(V)‚Äùdenotesthenumberofvision
featuretokensfedintoLLM.‚Äú#Tokens/s‚Äùdenotesthespeedofthewholepipeline. ‚ÄúAvg‚Äùdenotes
average. NotethatChannelConcatenationachievesboththebestperformanceandbetterthroughput.
VisionEncoder(s) Fusion Token(V) #Tokens/s GQA MME MMMU OCR SQA POPE TextVQA Avg
CLIP ‚Äì 1024 47.9 64.6 1531 35.7 496 72.8 87.9 64.0 644.4
ConvNeXt ‚Äì 1024 46.8 63.7 1433 37.0 527 72.0 87.9 71.7 652.3
SA 2048 46.1 64.6 1482 35.3 536 72.5 88.3 71.8 657.2
CC 1024 47.3 64.0 1486 36.0 533 72.7 88.6 71.9 658.2
CLIP
LH 1024 47.0 62.9 1488 36.8 521 72.6 88.9 68.7 652.1
+ConvNeXt
MG 1024 45.2 64.9 1481 36.7 474 72.8 88.2 68.6 646.7
DA 1024 47.3 63.6 1497 35.3 459 71.9 88.0 69.1 640.9
CLIP SA 3072 40.3 64.9 1439 35.0 504 72.7 88.2 71.3 649.2
+ConvNeXt+SAM CC 1024 46.3 64.6 1497 35.2 558 71.7 87.9 72.2 660.4
Stage1: Pre-Alignment Stage2: Joint-Projector Training Stage3: Supervised Finetuning
Vision Expert Aüßä Vision Expert A üî•
Vision Expert A üî• Vicuna Vision Language Alignment Vision Language Alignment
Text Recognition self 7Büßä
attention Vision Expert B üßä Visual Features Vision Expert B üî• Visual Features OV bis jeio cn t DE ex tp ee cr tt io B nüî• atts ee nl tf i on Vi 7cu Bn üßäa VTe isx it o R ne c Eo xg pn eit ri to n Cüßä MFu os dio un le VTe isx it o R ne c Eo xg pn eit ri to n Cüî• MFu os dio un le
Vision Expert C üî• Vicuna Object Detection Object Detection
Semantic Segmentation atts ee nl tf i on 7Büßä Vision Expert D üßä Projector üßä Vision Expert D üî• Projector üî•
Generative loss Semantic Segmentation üî• Semantic Segmentation üî•
Figure3: TheproposedtrainingstrategyofEagle. Itconsistsofthreeprogressivestages,including
vision-language pre-alignment training, joint-project training and supervised fine-tuning. These
stageseffectivelyleveragepublicdatafromdiversesources,rangingfromnoisyimage-textpairson
thewebtohigh-qualitycaption,VQA,andmultimodaldialoguedatasets.
(3)LLaVA-HR:injectinghigh-resolutionfeaturesintolow-resolutionvisionencodersusingmixture-
of-resolutionadapter[59]; (4)Mini-Gemini: usingtheCLIPtokensasthelowresolutionqueries
to cross-attend another high-resolution vision encoder in the co-located local windows [45]. (5)
DeformableAttention: anewbaselineweintroduceontopofMini-Gemini,wherethevanillawindow
attentionisreplacedwithdeformableattention[86]. Fig.2givesadetailedillustrationofthesefusion
strategies. Tobetterstudythem,wechoose‚ÄúCLIP+ConvNeXt‚Äùand‚ÄúCLIP+ConvNeXt+SAM‚Äùasthe
basemulti-encodercombinationstoperformcomparisons.
OurstudyinTable4showsthatChannelConcatenationachievesthebestaverageperformancewhile
maintainingbetterthroughputcomparedtosequenceappend. The‚Äúinjection-based‚Äùmethods,suchas
LLaVA-HR,Mini-GeminiandDeformableAttention,areingenerallesscompetitiveonTextVQA[69]
andOCRBench[57],performingworsethanusingConvNeXtaloneasthevisionencoder. Aplausible
explanationisthattheCLIPfeaturescontinuetoplayadominantroleinthevisualtokens. Although
sequenceappendshowscomparableperformancetochannelconcatenation,itfacesthechallengeto
handlemorevisionencodersduetotheincreasingsequencelength. Hence,wechoosedirectchannel
concatenationasourfusionstrategyconsideringitsperformance,expandability,andefficiency.
2.5 Vison-languagePre-Alignment
AsshowninTable3,visionbackbonespre-trainedexclusivelyonvisiontasks(e.g.,detection,OCR,
andsegmentation)arelesscompetitivecomparedtoencoderpre-trainedonvisionlanguagealignment.
Thisispossiblyduetorepresentationalinconsistencieswhenintegratedwithlargelanguagemodels.
Additionally,whencombiningdifferentencoders,thereisagapbetweentheseencoders,creating
difficultiesinthetrainingprocess. Toaddressthisfeatureinconsistency,weproposeaPre-Alignment
trainingstagethatfirstalignseachindividualvisionencoderwiththesamelargelanguagemodel,
fosteringbettersynergybetweenvisualandlinguisticcapabilities.
6
egaugnaL
egraL ledoM
egaugnaL
egraL ledoMTable5:ExperimentstoshowtheeffectivenessofPre-Align.‚ÄúOCR‚ÄùdenotesOCRBench[57].‚ÄúAvg‚Äù
denotesaverage.
CLIP VisionExpert(X) Unfreeze Pre-Align MME MMMU OCR SQA POPE TextVQA Avg
‚úó ‚úó 1495 35.1 292 71.8 85.6 59.7 593
CLIP-448 SAM-1024 ‚úì ‚úó 1504 36.9 482 73.5 87.7 63.2 641
‚úì ‚úì 1554 36.7 505 73.8 88.2 62.9 649
‚úó ‚úó 1499 35.1 416 72.3 86.7 67.1 629
CLIP-448 ConvNext-1024 ‚úì ‚úó 1522 36.6 550 72.7 87.9 71.9 667
‚úì ‚úì 1538 36.3 556 71.9 87.9 71.6 667
‚úó ‚úó 1500 35.8 467 71.9 86.1 60.9 627
CLIP-448 Pix2Struct-1024 ‚úì ‚úó 1529 36.2 535 73.1 87.1 64.5 651
‚úì ‚úì 1564 35.3 541 73.3 86.6 63.9 652
‚úó ‚úó 1484 35.8 305 72.0 86.0 59.9 597
CLIP-448 EVA-02-L-1024 ‚úì ‚úó 1537 37.0 474 72.6 87.5 62.6 639
‚úì ‚úì 1586 37.3 495 73.1 88.9 64.2 653
Fig. 3 depicts our pre-alignment strategy. Instead of training a projector to simultaneously align
multiple vision experts as in LLaVA‚Äôs [54] original pre-training strategy, we first align the repre-
sentationofeachindividualexpertwithasmallerlanguagemodel(Vicuna-7Binpractice)using
next-token-predictionsupervision. AsshowninFig.3,withpre-alignment,thewholetrainingprocess
consistsofthreesteps: 1)trainingeachpre-trainedvisionexpertwiththeirownprojectoronSFT
data,whilekeepingthelanguagemodelfrozen;2)combiningallthevisionexpertsfromthefirststep
andtrainingonlytheprojectorwithimage-textpairsdata;3)trainingthewholemodelontheSFT
data.
Toverifytheproposedmethod,wecomparethepre-alignmentstrategywiththenormaltwo-stage
trainingstrategyinTable5,consideringbothfreezingandunfreezingvisionexpertsforcomparison.
AsshowninTable5,althoughunfreezingthevisionexpertsduringSFThelpsimproveperformance
by updating the vision experts to fit the language model, the Pre-Align strategy more effectively
mitigatestheinherentbiasesofeachvisionexpertandstabilizesthetrainingprocess,subsequently
improvingoverallperformance.
2.6 Extensiontomulti-experts
With the optimized strategies and training recipes of incorporating individual vision experts, we
considertheincorporationofevenmorevisionexpertstopushthelimit. Toconductthesearchingin
asystematicandprincipledmanner,weadoptastep-by-stepgreedystrategytoincorporateadditional
visionexperts.
WeconsiderthevisionexpertsdiscussedinSection2.3forexperiments. WemarkCLIP,ConvNeXt,
SAM, DINOv2, Pix2Struct, and EVA-02-L as A, B, C, D, E, and F, respectively. A round-robin
scheme,asshowninTable6,isadopted. Wefirstusethetwotopperformingvisionencoders,CLIP
andConvNeXt,asthebasisandgraduallyaddonemorevisionencodereachtime. Ineachround,the
best-performingvisionencodercombinationisretainedforthenextround.
Table6revealsseveralinsights. Generally,introducingadditionalvisionencodersenhancesthe
performance. Thisindicatesthatthedistinctadvantagesofdifferentencoderscanbepreservedand
utilized;forexample,integratingtheEVA-02encoderimprovesmetricsonthePOPEbenchmark.
Although individual metrics may vary, the aggregated performance shows an upward trend, as
evidenced by normalized average metrics, suggesting that the overall efficacy of the system is
enhancedwithmoreencoders. Also,Table6showsthatthebestcombinationofvisionexpertsare
CLIP,ConvNeXt,SAM,Pix2Struct,andEVA-02. Wewillusethisrecipeinourfinalmodel.
3 Experiments
Aftermeticulouslydevelopingourrecipes,wehaveformulatedthefollowingprincipalsforourmodel,
Eagle: (1)integratingmorevisionexpertswithoptimizedtrainingrecipe;(2)combiningmultiple
7Table6: Resultsofvisionexpertselectionprocess. A,B,C,D,EandFdenoteCLIP,ConvNeXt,
SAM, DINOv2, Pix2Struct and EVA-02-L models respectively. ‚ÄúOCR‚Äù denotes OCRBench [57].
‚ÄúAvg‚Äùdenotesaverage. Underthesamenumberofencoders,thebestscoresofvariouscombination
variantsarebold.
#Encoders EncoderCombination GQA MME MMMU OCR SQA POPE TextVQA Avg
2 A+B 64.0 1486 36.0 533 72.7 88.6 71.9 658.3
A+B+C 64.6 1497 35.2 558 71.7 87.9 72.2 660.3
A+B+D 65.7 1506 35.0 509 72.6 87.9 70.1 653.6
3
A+B+E 65.5 1459 35.3 565 73.1 87.9 70.8 660.1
A+B+F 64.7 1506 35.9 562 73.2 88.3 72.1 665.3
A+B+F+C 65.9 1498 36.6 540 72.0 88.8 71.6 662.6
4 A+B+F+D 65.6 1521 35.8 508 73.0 89.0 70.0 657.5
A+B+F+E 65.3 1539 37.1 552 73.0 87.4 71.3 666.1
A+B+F+E+C 65.7 1511 37.6 548 72.7 88.6 71.6 666.5
5
A+B+F+E+D 65.6 1512 36.0 528 72.9 88.4 70.5 660.0
6 A+B+F+E+C+D 65.18 1504 35.3 534 71.9 87.6 70.86 656.3
Table7: Theadvancedtrainingdatausedinthesupervisedfine-tuningstage.
Note DataSource
LLaVA-1.5[52](665k),DocVQA[61](39k),synDog-EN[35](50k),
ChartQA[60](28k),DVQA[32](25k),AI2D[34](15k),ShareGPT-4V[8](100k),
TotalNumber(1,809k) laion-GPT4V[1](11k),LVIS-Instruct4V[74](220k),LRV-Instruct[51](150k),
Geo170k[25](120k),LLaVAR[84](20k),
Visual7W[87](70k),Open-Hermes2.5[72](1809k)
visionexpertsthroughdirectchannelconcatenation;(3)pre-trainingthevisionexpertsseparately
viapre-alignment. Inthissection,tofurthershowtheadvantagesoftheEaglemodels,weaddmore
trainingdataandcompareEagleagainstthecurrentstate-of-the-artMLLMsondifferenttasks.
3.1 Implementationdetails
Models. WeuseVicuna-v1.5-7B[15],Llama3-8B[3]andVicuna-v1.5-13B[15]asthelanguage
model. Forvisionencoder,followingtheresultsinSec.2.6,wedenotethemodelwithfourvision
encoders: CLIP,ConvNeXt,Pix2StructandEVA-02asEagle-X4andthemodelwithanadditional
SAMvisionencoderasEagle-X5.
Augmented training data. Considering the training cost, we only use 934K data during the
investigationandablations, aslistedinSec.2.1. Inthissection, inspiredbyrecentpractices[73,
45], we introduce more datasets, including LVIS-INSTRUCT4V [74], LRV [51], Geo170K [25],
LLaVAR[84],Visual7w[87],andOpenHermes-2.5[72],resultinginatotalof1,809kdataforthe
supervisedfine-tuningstage. SeeTable7formoredetailsonthisdatamixture. Tobetterperforman
apple-to-applecomparisonwithrecentopen-sourceMLLMprojects,wealsotrainEagle-X5with
the same data as Cambrian-1 [73], including 2.5 million and 7 million data for pre-training and
supervisedfine-tuning.
3.2 Mainresults
Evaluationonvisualquestionansweringtasks. WecompareEaglemodelseriesacrossthreeVisual
QuestionAnswering(VQA)benchmarks,includingGQA[30],VQAv2[26]andVizWiz[27]. As
showninTable8,Eagle-X5achievesstate-of-the-artperformanceonGQAandVQAv2,underscoring
theadvantagesofincorporatingadditionalvisionexperts.
EvaluationonOCRandchartunderstandingtasks. ToevaluatetheOCR,documentandchart
understandingcapabilitiesofEagle,webenchmarkourmodelonOCRBench[57],TextVQA[69],
and ChartQA [60]. As illustrated in Table 8, our model significantly surpasses competitors on
TextVQA,benefitingfromitshigh-resolutionarchitectureandintegrationofdifferentvisionencoders.
8Table8: Resultswiththeadvancedtrainingdatarecipe. SQAIdenotesScienceQA-IMG[65].
Model
LLaVA-1.5[52] 1510 64.3 58.6 - - 85.9 66.8 62.0* 50.0 78.5* 297 58.2 -
LLaVA-NeXt[53] 1519 67.4 70.2 34.6 35.8 86.5 70.1 64.2* 57.6 80.0* 490 64.9 -
InternVL[13] 1525 - 65.4 - - 86.4 - 62.9* 52.5 79.3* - 57.0 -
LLaVA-HR[59] 1554 - 64.2 - - 87.6 65.1 64.2* 48.7 81.9* - 67.1 -
Monkey[47] - - - - - - - 60.7* 61.2* 80.3* 514 67.6 65.1
Mini-Gemini[45] 1523 65.8 - 32.2 36.8 - 71.1 64.5* - - 477 65.2 -
Eagle-X4 1561 67.8 73.4 37.3 34.9 88.4 70.4 64.8* 50.8 83.4* 540 70.9 67.5
+Pre-Align 1561 68.1 73.4 38.8 36.8 88.7 70.8 64.9* 51.0 83.7* 544 71.9 67.9
Eagle-X5 1528 68.4 73.9 37.0 36.3 88.8 70.0 64.9* 54.4 83.4* 529 71.2 67.7
+Pre-Align 1579 68.8 73.5 39.0 36.4 88.5 71.6 64.0* 56.0 83.6* 554 71.9 67.0
LLaVA-1.5[52] 1531 67.7 61.6 - 36.4 85.9 71.6 63.3* 53.6 80.0* 331 61.3 -
LLaVA-NeXt[53] 1575 70.0 71.9 35.3 36.2 86.2 73.5 65.4* 60.5 82.8* 514 67.1 62.2
InternVL[13] 1546 - - - - 87.1 - 63.9* 54.6 80.2* 517 58.7 -
LLaVA-UHD[79] 1535 68.0 - - - 89.1 72.0 65.2* 56.1 81.7* - 67.7 -
LLaVA-HR[59] 1540 - 64.5 - - 87.8 68.1 64.8* 57.9 82.6* - 68.1 -
Mini-Gemini[45] 1565 68.6 70.6 37.0 37.3 - 71.9 65.8* - - 466 65.9 56.6
Eagle-X4 1627 69.9 74.4 37.6 36.9 87.7 73.1 66.3* 56.2 83.7* 561 73.9 70.5
+Pre-Align 1621 69.9 74.9 39.4 37.3 89.1 73.5 64.9* 55.9 84.5* 581 74.3 72.2
Eagle-X5 1609 69.2 74.1 38.8 36.6 87.8 72.7 66.2* 59.3 83.8* 574 74.2 69.9
+Pre-Align 1604 70.5 74.9 39.7 38.0 88.2 73.1 64.4* 60.9 84.5* 573 73.9 71.0
Notably,Eaglemaintainsastraightforwarddesign,supportingupto1024tokenswithoutrequiring
complextiledecompositionofimages.
Fig. 5 shows some examples of OCR and document understanding cases. With high-resolution
adaptationandmorevisionexperts,ourmodelcanidentifiessmalltextwithinimagesandaccurately
extracts information according to the users‚Äô instructions. To better understand the benefits of
introducingexpertspre-trainedonothervisiontasks,wevisualizetheresultsofamodelwithonlythe
ConvNeXtandCLIPvisionencoders,comparedtotheresultsofEagle-X5inFig.4. Withthefullset
ofvisionencoders,themodelcansuccessfullycorrectmistakes,showingthatevenwhenequipped
withhigh-resolutionvisionencoderspre-trainedonvision-languagealignment,themodel‚Äôsabilities
canstillbeenhancedbyintegratingadditionalvisionexpertspre-trainedondiversevisiontasks.
Evaluationonmultimodalbenchmarks. WeevaluateEagleonsevenbenchmarksforMLLMs
todemonstrateitscapabilitiesfromdifferentperspectives,includingMME[24],MMBench[56],
SEED[41],MathVista[58],MMMU[81],ScienceQA[65],andPOPE[46]. Specifically,MME,
MMBench,andSEEDassesstheoverallperformanceonvariousreal-worldtasksbasedonreasoning,
recognition,knowledge,andOCR.MMMUfocusesonchallengingproblemsfromdiversedomains
thatrequirecollege-levelknowledge. POPEevaluatesthevisualhallucinationsofMLLMs. The
metricsusedinourpaperadheretothedefaultsettingsofthesebenchmarks. Wereporttheperception
scoreforMME,theen_devsplitforMMBench,theimagesplitofSEED,thetest-minisplitof
MathVista,thevalsplitofMMMU,theF1-scoreofPOPE,andtheimageofSQAtoalignwiththe
reportedscoresfromothermodels.
From the data presented in Table 8, Eagle consistently surpasses existing models across various
MLLMsonSEEDandMME,demonstratingthecomprehensiveknowledgeandreasoningabilitiesof
Eagle. Withthehelpofvisionencodersonobject-centrictasks,Eaglealsoachievesthebestperfor-
manceonthePOPEbenchmark. Additionally,thePre-AlignmentstrategydiscussedinSec.2.5has
beenfoundtofurtherenhanceperformancewhenintegratingmultipletask-specificvisionbackbones.
Thisapproachnotonlymitigatestheinherentbiasesofeachvisionexpertandthesynergybetween
differentmodalitiesbutalsoestablishesarobustframeworkforthemultiple-expertfusion.
9
B7-newQ&B7-anuciV
B31-anuciV
EMM
hcneBMM
DEES
atsiVhtaM UMMM
EPOP IAQS AQG
ziWziV 2vAQV hcneBRCO AQVtxeT AQtrahCTable9: ResultsusingCambrian-1trainingdata. TheEagle-X5modelsaretrainedusingthesame
pre-trainingandsupervisedfine-tuningdataasCambrian-1[73].SQAIdenotesScienceQA-IMG[65].
RWQAdenotesRealworldQA[78].
Model Knowledge General OCRandChart Vision-Centric
Llama3-8B
MGM-HD 55.7 75.1 37.3 37.0 73.5 72.7 1606 72.7 73.2 64.5 62.9 59.1 47.7 70.2 74.6 40.4 18.7 62.1
Cambrian-1 61.3 80.4 42.7 49.0 73.0 73.1 1547 75.9 74.7 64.6 71.3 73.3 62.4 71.7 77.8 57.6 51.3 64.2
Eagle 64.2 84.3 43.8 52.7 76.1 73.8 1559 75.9 76.3 64.9 76.6 80.1 62.6 77.1 86.6 69.1 71.6 66.5
Vicuna-13B
MGM-HD 54.1 71.9 37.3 37 70.1 70.7 1597 68.6 70.6 63.7 60.8 56.6 46.6 70.2 69.8 38.4 19.3 57.5
Cambrian-1 60.2 79.3 40.0 48.0 73.6 73.7 1610 75.7 74.4 64.3 71.3 73.8 61.9 72.8 76.8 52.2 41.3 63.0
Eagle 63.0 82.0 41.6 54.4 74.0 74.6 1651 75.7 74.8 65.3 75.1 77.6 61.9 75.5 85.4 61.4 58.0 64.8
Yi-34B
MGM-HD 62.4 77.7 48 43.4 80.5 76.2 1659 80.6 75.3 65.8 68.1 67.6 51.8 74.1 78.9 52.3 37.3 67.2
Cambrian-1 67.0 85.6 49.7 53.2 79.7 76.8 1689 81.4 75.3 65.8 71.9 75.6 60.0 76.7 75.5 60.3 52.7 67.8
Eagle 68.6 85.5 53.2 57.9 79.1 76.3 1677 81.0 75.6 64.9 75.4 77.2 62.4 78.8 83.0 68.3 67.0 69.5
Input Image: Input Image:
User: Tell me the locations of all the number 10 jerseys in the picture.
Answer the questions in details with a few sentences
Eagle (baseline): There are two number 10 jerseys in the picture. One is on the left side of the group, and the
other is on the right side
Eagle (more vision experts): In the image, there are two players wearing jerseys with the number 10. One of
them is standing in the center of the group, while the other is positioned towards the right side of the group. Both
players are part of the basketball team, as indicated by their uniforms and the context of the image.
Input Image: User: What‚Äôs the value of the row ‚Äúlow‚Äù and column ‚Äúreferents‚Äù? User: What‚Äôs the opening time of the
restaurant on Sunday?
Eagle (baseline): 2012-13 Eagle (baseline): 11:30-2:00
Eagle (more vision experts):10,970 (60%) Eagle (more vision experts): closed
Input Image: Input Image:
User: What is the man in the picture doing with User: Describe the information in the red
his right hand? bounding box.
E cha ig ldle w (b ita hs he il sin re ig): h T t h he a nm da .n in the image is holding his E Ta hg el e M ( eb mas oe rl iein se i) s: hT ih ge h ls igo hn tg e " dl f i nlt r M edeans Losing You" by
E p ria o gg i hnl te t i h n( am g n o a dtr . e a v wis hio itn e e ox bp jee cr tt ,s w):T hh ice h m isa an din o lt ph he in p ,i c wt iu tr he h is is E h wa i hgg ihl ce l hi g ( ihm s t to s h r t ee h nev ai ss toi io onn ng a e " lLx fop inse i ar n lt g s s ) oY: no T u gh ."e b r ye d T hb eo u Mn ed min og r ib eo sx ,
Figure4: Qualitativecomparisonofdifferentnumbersofvisionexperts. BaselinemeansEagle
with only CLIP+ConvNext. More Vision Experts denotes the Eagle-X5 model. We highlight a
failurecaseinRED.BLUEindicatesthecorrectanswers. Withmorevisionexperts,Eaglecanmore
preciselycapturetheinformationintheimageandprovidethecorrectanswer.
ComparisonwithCambrian-1. Usingthesamepre-trainingandsupervisedfine-tuningdatasets
fromCambrian-1[73],Eagledemonstratessuperiorperformanceacrossalltheevaluatedbenchmarks
withoutbellsandwhistles. AsshowninTable9,EagleoutperformstheCambrian-1counterparts
considerablyontheOCRandChartcategory. Consistentimprovementsarealsoobservedonthe
General,KnowledgeandVision-Centriccategories,showingtherobustnessandgeneralizationability
oftheimprovedperceptiondesigninEagle.
4 Relatedwork
4.1 Multimodallargelanguagemodels
Ourworkisrelatedtothegeneralarchitecturedesignofmultimodallargelanguagemodels. Besides
thelineofrepresentativeopen-sourceresearchmentionedintheintroductionsection,othernotable
10
gvA
IAQS
UMMM atsiVhtaM
D2IA
gvA
EMM
hcneBMM
DEES AQG
gvA
AQtrahC hcneBRCO AQVtxeT AQVcoD
gvA
PVMM AQWRInput Image: Input Image:
User: Render the figure in Markdown format.
User: What was the committee
Eagle: strength for the first meeting?
Eagle: 6
User: What was the committee
strength for the last meeting?
Eagle: 5
Input Image: Input Image: User: Describe the chart in detail?
Eagle: The image presents a bar chart that provides a
forecast for the cloud computing market size in China for the
years 2019 to2022. The chart is divided into two columns,
one for private cloud and the other for public cloud. The
years are labeled on the x-axis, while the market size is
User: What‚Äôs the total cost of Fat cell size (Mt. represented on the y-axis in billions of RMB, The chart shows
Sinai) in the -05 year? that the market size for private cloud and public cloud have
been steadily increasing over the years. The highest market
size is for 2022, which is for public cloud, indicating a
Eagle: $35,864 significant growth in this sector. The lowest market size is for
2019, which is for private cloud, suggesting that the private
cloud market has been growing at a slower pace compared
to the public cloud market.
Figure 5: Qualitative samples on OCR and document understanding tasks. Eagle is able to
extractusefulinformationfromsmalltext.
Input Image: Input Image: Input Image:
User: What are the similarities and differences between
User: Below are shown, for five metals, reduction reactions these two pictures? User: The original placement of this painting was
and standard electrode potential values. Which of these Option:
metals is the least reactive? explain your answers in detail. Eagle: The two pictures show a red panda in a natural setting, (A)in the Prado Museum in Madrid, showing a willingness to
Option: (A)Au (B) Cu (C) Ni(D) Fe (E) Na resting on a tree branch. The similarities between the two images be compared to great Spanish painters of the past
E E t h I (n Aea x ig n )pg t Ahd hll ae uee en: r :n g aT +tc ihh t y 1vi e e eo .o 4 nn El f 2e : " ta 0 a aT v s mh Vbat e l le er (u Be ts ,e ata )t,a l hc C tn tt e uhoi d v :e s a +e l t o m ar 0 m sd n .e o 3 de e r 4e at l ee 0a l r ec rl d Vc eta tr ea (m ro l Cco ed to )n ce i vn Ns t rep g ia o :o t -nt dht 0h d ee e .e 2 n u p m 5tg n oi 0i a ed tv Vel tee a( n rn (E lg t D " io io as ))p . l Fi a t s v ei o ara :e -n l 0um ds .eu 4ei scs 4a t a 0( s iA o r Vu en) r (e A a Er seu o ) a. ff No ct at lh li :ooe -w 2n .. s 9 T . 2 h 4e V i n T o a r an e wnha fc d t de tl au hu p kd srd e eala i e e f .npl f e Te det h ph hrn ao iee e v nt in o s i gp fr c g i o sr roe r ie n sts na ts tm p ile pt ni he hen hgs n ec oi .n ue t ttI. p on rto e rh f ct iee gh at hh be pp te r to f a uo isr r n rne i s et ct d i sth o p h ,p tn i e hwca o etn bhuf d r r irt al ea e eh n d, , e cit t n p hh hr ae ,e te h nd l ot r e d r ep o e asda ke e i in n p ncb d oa g ar a n na a mdn a d len ac op rh d ri tis, c e t a ta ch ru nn u ee r dd r l e a a l e ,t xn h d t eg he dl u ee p ( a ( t ( w vB C Dh ic h se)) c ) i e e tii imnn irnp ne ut gtt ta hh thl htn dee ei etc i u gsc e r pet da n uw oct e ih d te r i ate oy tph rrd f t iao ei r t o iif sa th n n s l e tk ei oi rn nC r o f pga o M trt me hPha t eo hd a a ilr t rli ti iic o d op t y, C n h aIi s eh vn l, u d t fRc ahri h oc mc eh a ya iaplt ll yi e lan n Pi wg ng a t oa li i an n u cd g g ldee h rs biei ni mr p e e Mr vett io aso ede wcf n ri o i an t dn s bd ,s l eid be yr
Comparing these values, we can see that the metal with the highest E"value state, while the second photo shows the animal in a more active Eagle: (C) in the study of king Philip Iv, challenging him to
(+1,420 V) is Au. which indicates that it is the and attentive position. consider the multitude of interpretations the painting
least reactive among the given options. The other metals have lower E" represents
values, making them more reactive than Au.
Input Image: User: Which car in the picture is more aerodynamic? And explain your Input Image:
answers in detail. User: Who was the patron of the building?
Options:
Eagle: The car in the picture that is more aerodynamic is the one on the right. This is (A) Abd al-Malik
because it has a streamlined shape, with a smooth and curved body that reduces air (B) Justinian
r ae irs , i wsta hn icc he . c aT nh e re s st ure lta im n l bin ee ttd e s r h fa up ee l e a fl fl io cw ies n cth y e a nca dr l oto w m ero ev me m isso iore n se .f f Ti hci ee n catl ry o t nh r to hu eg lh e ft th . e o n (C) Charlemagne (D) shah Jahan
the other hand. has amore rectangular shape, which can cause more air resistance and Eagle: (B)
make it less aerodynamic.
Figure6: QualitativeresultsofEagleonVQAtasksrequiringcollege-levelknowledgeordeliberate
reasoning.
familiesofMLLMsinclude,butarenotlimitedtoMiniGPT-4[85,7],Lynx[82],Otter[40,39],Qwen-
VL[5],CogVLM[75,29],VILA[48],GPT-4V [2],Gemini[71],andLlama3.1[19]. Dependingon
howvisionsignalsareintegratedintothelanguagemodel,MLLMscanbebroadlycategorizedinto
‚Äúcross-modalattention‚Äùonesand‚Äúprefix-tuning‚Äùones[80].Theformerinjectsvisualinformationinto
differentlayersofLLMsusingcross-modalattention[4,40],whereasthelatterviewsthevisualtokens
aspartofthelanguagetokensequenceanddirectlyappendthemwithtextembeddings[54,44,43].
Ourmodelbelongstotheprefix-tuningfamilybyfollowingaLLaVA-styledmultimodalarchitecture.
ConsideringthatMLLMisafastgrowingfield,readersarerecommendedtorefertomoredetailed
studiesandsurveyssuchas[80,42,23].
4.2 VisionencoderdesignforMLLMs
OurworkiscloselyrelatedtoresearchfocusingonimprovedvisionencoderdesignsforMLLM.Early
works[54,44]usuallyadoptedvisionencoderpre-trainedonvision-languagealignmenttaskssuch
asCLIP[63]andEVA-CLIP[70]. StrongervisionencoderssuchasSigLIP[83]andInternVL[13]
havebeenproposedtoimprovevision-languagetaskswithimproveddesigns,largermodelsizes,and
11bettertrainingrecipes. Consideringthatthemodelsareoftenpre-trainedonlow-resolutionimages
andlacktheabilitytoencodefine-graineddetails,higherresolutionadaptationisoftenperformedto
increasetheMLLMinputresolution[9,10,6,12].
Besideshigherresolutionadaptation,modelssuchasLLaVA-NeXT [53],LLaVA-UHD[79],Mon-
key[47],InternLM-XComposer[17],andInternVL[12]useeithertilingoradaptivetilingtohandle
high resolution input, where input images are divided into lower-resolution patches and process
themseparately. Althoughtheabilitytohandlehigherresolutionissimilarlymadepossiblewiththe
introductionofadditionalvisionexperts,itsspiritisslightlyorthogonaltothetilingtechniquesin
termsofintroducingthediversityofvisualrepresentations. Bothtechniquesarecompatibleandcan
becombinedtogether.
Ourworkismostrelatedtoexistingmodelsusingmultiplevisionencodersforimprovedperception.
Mini-Gemini[45]andLLaVA-HR[59]proposetofusehigh-resolutionvisualfeaturesintothelow-
resolutionvisualtokens. Apartfromtheresolutionissue,thesepre-trainedvisionencodersmaylack
specificabilitiessuchasreadingtextandlocalizingobjects. Hence,aseriesofworkshaveintegrated
vision models pre-trained on various vision tasks to extend the capabilities of vision encoders.
For example, Mousi [20], and Brave [33] fuse visual tokens from different vision encoders by
concatenatingalongthechannelortokendirection. RADIO[64]proposesamulti-teacherdistillation
methodtounifytheabilitiesofdifferentvisionencodersintoaunifiedmodel. MoAI[37],IVE[28],
andPrismer[55]furtherusetheoutputofvisionexperts,suchasOCR,detection,ordepthestimation,
to supplement more information for MLLM to generate answers. MoVA [88] devises a routing
networktoassignanoptimalvisionmodelconditionedonthegivenimageandinstructions.
5 Conclusion
We conduct an in-depth analysis study on the design space for integrating vision encoders for
multimodal large language models. Unlike previous works that focus on designing novel fusion
paradigms,wefindsystematicdesignchoicemattersanddiscoveraseriesofusefultechniques. Step
bystep,weoptimizethetrainingrecipeofindividualvisionencoders,identifyanextendableand
efficientfusingmethod,andgraduallycombinevisionencoderswithdifferentdomainknowledge.
Theresultsshowtheimportanceofbasicdesignspace. Wehopeourworkcanserveasanewbasis
andbringnewinspirationforthevisionencoderdesignforMLLM.
References
[1] LAION-GPT4vdataset. https://huggingface.co/datasets/laion/gpt4v-dataset,2023. 3,8
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report.
arXiv:2303.08774,2023. 11
[3] AI@Meta. Llama3modelcard,2024. 1,8
[4] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. InNeurIPS,2022. 2,11
[5] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou.Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities.arXiv:2308.12966,
2023. 11
[6] Lucas Beyer, Andreas Steiner, Andr√© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz,
MaximNeumann,IbrahimAlabdulmohsin,MichaelTschannen,EmanueleBugliarello,etal. Paligemma:
Aversatile3bvlmfortransfer. arXiv:2407.07726,2024. 4,12
[7] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny.MiniGPT-v2:Largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXiv:2310.09478,2023. 11
[8] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
ShareGPT4V:Improvinglargemulti-modalmodelswithbettercaptions. arXiv:2311.12793,2023. 3,8
[9] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz,SebastianGoodman,XiaoWang,YiTay,etal. PaLI-X:Onscalingupamultilingualvisionand
languagemodel. arXiv:2305.18565,2023. 4,12
12[10] XiChen,XiaoWang,LucasBeyer,AlexanderKolesnikov,JialinWu,PaulVoigtlaender,BasilMustafa,
SebastianGoodman,IbrahimAlabdulmohsin,PiotrPadlewski,etal. PaLI-3visionlanguagemodels:
Smaller,faster,stronger. arXiv:2310.09199,2023. 4,12
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. PaLI:Ajointly-scaledmultilinguallanguage-
imagemodel. InICLR,2023. 2
[12] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu,JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v?closingthegaptocommercialmultimodal
modelswithopen-sourcesuites. arXiv:2404.16821,2024. 12
[13] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,QinglongZhang,
XizhouZhu,LeweiLu,BinLi,PingLuo,TongLu,YuQiao,andJifengDai. InternVL:Scalingupvision
foundationmodelsandaligningforgenericvisual-linguistictasks. arXiv:2312.14238,2023. 2,4,9,11
[14] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,CadeGordon,
Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive
language-imagelearning. InCVPR,2023. 2
[15] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:Anopen-sourcechat-
botimpressinggpt-4with90%*chatgptquality. https://lmsys.org/blog/2023-03-30-vicuna/,
March2023. 1,8
[16] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. InstructBLIP:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. InNeurIPS,2024. 2
[17] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
HaodongDuan,WenweiZhang,YiningLi,etal. InternLM-XComposer2-4KHD:Apioneeringlarge
vision-languagemodelhandlingresolutionsfrom336pixelsto4khd. arXiv:2404.06512,2024. 12
[18] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid,JonathanTompson,QuanVuong,TianheYu,etal. PaLM-E:Anembodiedmultimodallanguage
model. arXiv:2303.03378,2023. 2
[19] AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal.TheLlama3herdofmodels.arXiv:2407.21783,
2024. 11
[20] XiaoranFan,TaoJi,ChanghaoJiang,ShuoLi,SenjieJin,SiruiSong,JunkeWang,BoyangHong,LuChen,
GuodongZheng,etal. MouSi:Poly-visual-expertvision-languagemodels. arXiv:2401.17221,2024. 2,5,
12
[21] YuxinFang,QuanSun,XinggangWang,TiejunHuang,XinlongWang,andYueCao. EVA-02:Avisual
representationforneongenesis. arXiv:2303.11331,2023. 2,5
[22] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,XinlongWang,
andYueCao. EVA:Exploringthelimitsofmaskedvisualrepresentationlearningatscale. InCVPR,2023.
2,5
[23] HaoFei,YuanYao,ZhuoshengZhang,FuxiaoLiu,AoZhang,andTat-SengChua. Frommultimodalllm
tohuman-levelai: Modality,instruction,reasoning,efficiencyandbeyond. InLREC-ColingTutorials,
2024. 2,11
[24] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,etal. MME:Acomprehensiveevaluationbenchmarkformultimodallarge
languagemodels. arXiv:2306.13394,2023. 9
[25] JiahuiGao,RenjiePi,JipengZhang,JiachengYe,WanjunZhong,YufeiWang,LanqingHong,Jianhua
Han,HangXu,ZhenguoLi,andLingpengKong. G-llava:Solvinggeometricproblemwithmulti-modal
largelanguagemodel. arXiv:2312.11370,2023. 8
[26] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVinVQA
matter:ElevatingtheroleofimageunderstandinginVisualQuestionAnswering. InCVPR,2017. 8
[27] DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP.
Bigham. Vizwizgrandchallenge:Answeringvisualquestionsfromblindpeople. InCVPR,2018. 8
[28] XinHe,LonghuiWei,LingxiXie,andQiTian. Incorporatingvisualexpertstoresolvetheinformation
lossinmultimodallargelanguagemodels. arXiv:2401.03105,2024. 12
[29] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
YuxiaoDong,MingDing,etal. CogAgent:Avisuallanguagemodelforguiagents. InCVPR,2024. 11
13[30] DrewAHudsonandChristopherDManning. GQA:Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InCVPR,2019. 5,8
[31] GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip,July2021. Ifyouusethissoftware,pleaseciteitasbelow. 2,5
[32] KushalKafle,ScottCohen,BrianPrice,andChristopherKanan.DVQA:Understandingdatavisualizations
viaquestionanswering. InCVPR,2018. 3,8
[33] OgÀòuzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico
Tombari. BRAVE:Broadeningthevisualencodingofvision-languagemodels. arXiv:2404.07204,2024.
2,5,12
[34] AniruddhaKembhavi,MichaelSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi.
Adiagramisworthadozenimages. arXiv:1603.07396,2016. 3,8
[35] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,Won-
seokHwang,SangdooYun,DongyoonHan,andSeunghyunPark. OCR-Freedocumentunderstanding
transformer. InECCV,2022. 3,8
[36] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InICCV,2023. 2,5
[37] Byung-KwanLee,BeomchanPark,ChaeWonKim,andYongManRo. MoAI:Mixtureofallintelligence
forlargelanguageandvisionmodels. arXiv:2403.07508,2024. 2,12
[38] KentonLee,MandarJoshi,IuliaRalucaTurc,HexiangHu,FangyuLiu,JulianMartinEisenschlos,Urvashi
Khandelwal,PeterShaw,Ming-WeiChang,andKristinaToutanova. Pix2Struct:Screenshotparsingas
pretrainingforvisuallanguageunderstanding. InICML,2023. 2,5
[39] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,FanyiPu,JingkangYang,ChunyuanLi,andZiwei
Liu. MIMIC-IT:Multi-modalin-contextinstructiontuning. arXiv:2306.05425,2023. 11
[40] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu.Otter:Amulti-modal
modelwithin-contextinstructiontuning. arXiv:2305.03726,2023. 11
[41] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-Bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXiv:2307.16125,2023. 9
[42] ChunyuanLi,ZheGan,ZhengyuanYang,JianweiYang,LinjieLi,LijuanWang,JianfengGao,etal.
Multimodalfoundationmodels:Fromspecialiststogeneral-purposeassistants. FoundationsandTrends¬Æ
inComputerGraphicsandVision,2024. 11
[43] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InICML,2023. 2,11
[44] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. BLIP:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,2022. 2,11
[45] YanweiLi, YuechenZhang, ChengyaoWang, ZhishengZhong, YixinChen, RuihangChu, Shaoteng
Liu, and Jiaya Jia. Mini-Gemini: Mining the potential of multi-modality vision language models.
arXiv:2403.18814,2024. 2,3,6,8,9,12
[46] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. arXiv:2305.10355,2023. 5,9
[47] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,andXiang
Bai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. InCVPR,
2024. 2,4,9,12
[48] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,Mohammad
Shoeybi,andSongHan. VILA:Onpre-trainingforvisuallanguagemodels. arXiv:2312.07533,2023. 11
[49] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,WenqiShao,
KeqinChen,etal. SPHINX:Thejointmixingofweights,tasks,andvisualembeddingsformulti-modal
largelanguagemodels. arXiv:2311.07575,2023. 2,5
[50] FuxiaoLiu,TianruiGuan,ZongxiaLi,LichangChen,YaserYacoob,DineshManocha,andTianyiZhou.
HallusionBench: You see what you think? or you think what you see? an image-context reasoning
benchmarkchallengingforgpt-4v(ision),llava-1.5,andothermulti-modalitymodels. arXiv:2310.14566,
2023. 2
[51] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large
multi-modalmodelwithrobustinstructiontuning. arXiv:2306.14565,2023. 8
[52] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXiv:2310.03744,2023. 2,3,4,8,9
14[53] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
LLaVA-NeXT:Improvedreasoning,ocr,andworldknowledge.https://llava-vl.github.io/blog/
2024-01-30-llava-next/,January2024. 2,4,9,12
[54] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2023.
2,3,7,11
[55] ShikunLiu,LinxiFan,EdwardJohns,ZhidingYu,ChaoweiXiao,andAnimaAnandkumar. Prismer:A
vision-languagemodelwithanensembleofexperts. TMLR,2024. 2,5,12
[56] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,JiaqiWang,
ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. MMBench:Isyourmulti-modalmodelanall-around
player? arXiv:2307.06281,2023. 9
[57] YuliangLiu,ZhangLi,BiaoYang,ChunyuanLi,XuchengYin,ChenglinLiu,LianwenJin,andXiang
Bai. Onthehiddenmysteryofocrinlargemultimodalmodels. arXiv:2305.07895,2023. 4,5,6,7,8
[58] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-Wei
Chang,MichelGalley,andJianfengGao. MathVista:Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. InICLR,2024. 9
[59] GenLuo,YiyiZhou,YuxinZhang,XiawuZheng,XiaoshuaiSun,andRongrongJi. Feastyoureyes:
Mixture-of-resolutionadaptationformultimodallargelanguagemodels. arXiv:2403.03003,2024. 2,3,4,
6,9,12
[60] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. ChartQA:Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. arXiv:2203.10244,2022. 3,8
[61] MineshMathew,DimosthenisKaratzas,andC.V.Jawahar. DocVQA:Adatasetforvqaondocument
images. InWACV,2021. 3,8
[62] MaximeOquab,Timoth√©eDarcet,Th√©oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. DINOv2:Learningrobustvisual
featureswithoutsupervision. arXiv:2304.07193,2023. 2,5
[63] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021. 2,3,5,11
[64] MikeRanzinger,GregHeinrich,JanKautz,andPavloMolchanov. AM-RADIO:Agglomerativevision
foundationmodelreducealldomainsintoone. InCVPR,2024. 2,12
[65] TanikSaikh,TirthankarGhosal,AmishMittal,AsifEkbal,andPushpakBhattacharyya. ScienceQA:A
novelresourceforquestionansweringonscholarlyarticles. InternationalJournalonDigitalLibraries,
2022. 9,10
[66] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeWGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaR
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5B:
Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InNeurIPSDatasetsand
BenchmarksTrack,2022. 2,5
[67] BaifengShi,ZiyangWu,MaolinMao,XinWang,andTrevorDarrell. Whendowenotneedlargervision
models? arXiv:2403.13043,2024. 2
[68] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel
Rueckert,andZehanWang. Real-timesingleimageandvideosuper-resolutionusinganefficientsub-pixel
convolutionalneuralnetwork. InIEEEConferenceonComputerVisionandPatternRecognition,pages
1874‚Äì1883,2016. 5
[69] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. TowardsVQAmodelsthatcanread. InCVPR,2019. 6,8
[70] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao.EVA-CLIP:Improvedtrainingtechniques
forclipatscale. arXiv:2303.15389,2023. 11
[71] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodalmodels. arXiv:2312.11805,2023. 11
[72] Teknium. OpenHermes2.5: AnopendatasetofsyntheticdataforgeneralistLLMassistants. https:
//huggingface.co/datasets/teknium/OpenHermes-2.5,2023. 8
[73] ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharithaAkula,Jihan
Yang,ShushengYang,AdithyaIyer,XichenPan,AustinWang,RobFergus,YannLeCun,andSainingXie.
Cambrian-1:Afullyopen,vision-centricexplorationofmultimodalllms. arXiv:2406.16860,2024. 2,8,
10
15[74] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,ZuxuanWu,andYu-GangJiang. Toseeistobelieve:
Promptinggpt-4vforbettervisualinstructiontuning. arXiv:2311.07574,2023. 8
[75] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,ZhuoyiYang,Lei
Zhao,XixuanSong,etal. CogVLM:Visualexpertforpretrainedlanguagemodels. arXiv:2311.03079,
2023. 11
[76] SanghyunWoo,ShoubhikDebnath,RonghangHu,XinleiChen,ZhuangLiu,InSoKweon,andSaining
Xie. ConvNeXtV2:Co-designingandscalingconvnetswithmaskedautoencoders. InCVPR,2023. 5
[77] XiyangWu,RuiqiXian,TianruiGuan,JingLiang,SouradipChakraborty,FuxiaoLiu,BrianSadler,Dinesh
Manocha,andAmritSinghBedi. Onthesafetyconcernsofdeployingllms/vlmsinrobotics:Highlighting
therisksandvulnerabilities. arXivpreprintarXiv:2402.10340,2024. 2
[78] xAI. Grok-1.5VisionPreview. https://x.ai/blog/grok-1.5v,2024. 10
[79] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan
Liu, andGaoHuang. LLaVA-UHD:anlmmperceivinganyaspectratioandhigh-resolutionimages.
arXiv:2403.11703,2024. 9,12
[80] ShukangYin, ChaoyouFu, SiruiZhao, KeLi, XingSun, TongXu, andEnhongChen. Asurveyon
multimodallargelanguagemodels. IEEETrans.PAMI,2024. 11
[81] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. MMMU:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertAGI. InCVPR,2024. 9
[82] YanZeng,HanboZhang,JianiZheng,JiangnanXia,GuoqiangWei,YangWei,YuchenZhang,andTao
Kong. Whatmattersintrainingagpt4-stylelanguagemodelwithmultimodalinputs? arXiv:2307.02469,
2023. 11
[83] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. InICCV,2023. 11
[84] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTongSun. LLaVAR:
Enhancedvisualinstructiontuningfortext-richimageunderstanding. arXiv:2306.17107,2023. 8
[85] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXiv:2304.10592,2023. 11
[86] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai.DeformableDETR:Deformable
transformersforend-to-endobjectdetection. InICLR,2021. 3,6
[87] YukeZhu,OliverGroth,MichaelBernstein,andLiFei-Fei. Visual7W:GroundedQuestionAnsweringin
Images. InCVPR,2016. 8
[88] ZhuofanZong,BingqiMa,DazhongShen,GuangluSong,HaoShao,DongzhiJiang,HongshengLi,and
YuLiu. MoVA:Adaptingmixtureofvisionexpertstomultimodalcontext. arXiv:2404.13046,2024. 2,12
16