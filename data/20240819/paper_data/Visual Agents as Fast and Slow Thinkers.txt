Visual Agents as Fast and Slow Thinkers
GuangyanSun1,MingyuJin2,ZhentingWang2,Cheng-LongWang5,
SiqiMa6,QifanWang4,YingNianWu3,YongfengZhang2,DongfangLiu1
1RIT2RutgersUniversity3UCLA4MetaAI5KAUST6WestlakeUniversity
Abstract
Achievinghuman-levelintelligencerequiresrefiningcognitivedistinctionsbetween
System1andSystem2thinking. WhilecontemporaryAI,drivenbylargelanguage
models,demonstrateshuman-liketraits,itfallsshortofgenuinecognition. Transi-
tioningfromstructuredbenchmarkstoreal-worldscenariospresentschallenges
forvisualagents,oftenleadingtoinaccurateandoverlyconfidentresponses. To
addressthechallenge,weintroduceFAST,whichincorporatestheFastandSlow
Thinkingmechanismintovisualagents. FASTemploysaswitchadaptertodynam-
icallyselectbetweenSystem1/2modes,tailoringtheproblem-solvingapproach
todifferenttaskcomplexity. Ittacklesuncertainandunseenobjectsbyadjusting
modelconfidenceandintegratingnewcontextualdata. Withthisnoveldesign,we
advocateaflexiblesystem,hierarchicalreasoningcapabilities,andatransparent
decision-makingpipeline,allofwhichcontributetoitsabilitytoemulatehuman-
likecognitiveprocessesinvisualintelligence. Empiricalresultsdemonstratethat
FASToutperformsvariouswell-knownbaselines,achieving80.8%accuracyover
VQAv2forvisualquestionansweringand48.7%GIoU scoreoverReasonSegfor
reasoningsegmentation,demonstrateFAST‚Äôssuperiorperformance. Extensivetest-
ingvalidatestheefficacyandrobustnessofFAST‚Äôscorecomponents,showcasing
itspotentialtoadvancethedevelopmentofcognitivevisualagentsinAIsystems.
1 Introduction
Inthefieldofartificialintelligence,System2delin-
eatesacognitivemodedistinguishedbydeliberate,
analytical,andconsciouslyreasonedprocesses[1‚Äì
5]. This mode is juxtaposed to System 1, which
embodiesintuitive,automatic,andunconsciouscog-
nition. Achieving human-level intelligence in AI
systemsnecessitatesthedeliberatecultivationand
Q1: Which holiday do you Q2: Is the telephone on the left
Qr 1:e Wfi hn ice hm hoe lidn ayt do of yot uh es Qe 2c : Io sg thn ei tt eiv lee phod ni es ot ni n thc e t li eo ftn s. Thispro- think they are celebrating? or right side of the hand lamp?
thcinek stsheiys arce rcuelcebiraaltinfgo?rotrh reighdt esvidee loof pthme heanndt laomfp?advancedrea-
soninganddecision-makingcapabilities[6‚Äì8]. ‚úÖ Fast ‚úÖ Slow
‚úÖ Fast [Clue] Ans: Maybe it‚Äôs Ans: The telephone
AnsT: Mhaeybee mit‚Äôse rgenceoffoundaTteiloepnhomne ios delsmarksasig- Christmas! is on the right.
Cnhriifistcmaasn! tturningpoint,whelikreely loan rtghee languagemodel
table. Chain of Evidence Construction
(LLM)basedagentshavemaderemarkablestrides
Segm ie nnt mthe a e nv yide anc re eas‚úÖ,shSloowwcasinghuman-likeintelligence 1‚É£ [Clue] 2‚É£
The Telephone is
<sega>crossdiverA sn es: tT ah se kte slep [h 9o ‚Äìne
1 1]. However,thisachieve- likely on the table.
is on the right. [Pixel-level Instances] <Lamp> <Telephone>
mentisprimarilyattributedtosomefeaturesoffoun- Figure1: WorkingPipeline. FASTrepresents
dationmodels: overparameterizationandtheavail- asolutionrootedinsystemswitching,demon-
abilityofvast,general-purposedatasets[12,13]. It stratingpronouncedcapabilitiesinhierarchi-
isimperativetonotethatwhilethesemodelsexhibit calreasoningandad-hocexplainability.
Preprint.Underreview.
4202
guA
61
]GL.sc[
1v26880.8042:viXrahuman-liketraits(e.g.,inductiveanddeductivereasoning[14‚Äì16]),thesecharacteristicsdonotequate
totheprocessesofSystem1/2thinking[17,18]andarefarlessintelligentthanhumanthinking.
In practice, visual agents often encounter challenges when moving from controlled, structured
benchmarkstocomplex,real-worldenvironments[11,19]. Thisdifficultcircumstancewillresult
inspuriousreasoningpathways,akintohallucinations,wheretheystruggletoacknowledgetheir
limitations or uncertainties [20, 21]. Such an issue arises from the absence of explicit modeling
of the fast and slow cognitive processes, reminiscent of human System 1 and System 2 [18, 22].
Consequently,whenfacedwithintricateinquiries,theMultimodalLargeLanguageModel(MLLM)
frequentlyoffersoverlyconfidentyetinaccurateresponses[19,23,24]. Addressingthisproblem
entailsreassessingMLLMalgorithmstoincorporateinsightsfromtheinterplaybetweenfastand
slowthinking(System1/2)observedinhumancognition. Thisdesignphilosophyguidesourwork.
Inthisstudy,weintroducetheFastandSlowThinking(FAST)mechanismintovisualagents. More
concretely, we design a switch adapter to determine whether the encountered problems are best
addressedusingwhichthinkingmode. Simpletasksrequireonlyfastthinking(System1thinking)
forastraightforwardproblem-solvingpipeline,whilecomplextasksnecessitatetheslow,deliberate
processingofSystem2(seeFigure. 1). Specifically,System2istriggeredwhenweencountervisual
challengesthathave: ‚ë†Uncertainty: Whenthemodelhaslowconfidenceindirectlyidentifyingthe
objecttowhichthecomplexqueryisreferring. Forexample,thequeryasks‚Äútheapplianceforstoring
andcoolingfood‚Äùinsteadof‚Äúrefrigerator,‚Äùand‚ë°Invisibility: Whendealingwithminuscule-sized
objectsthatevadedetectionbystandardvisualencoders,wherenormalvisualagentscannottellwhat
itis. Thisswitchadapterisachievedbydesigningnegativecontextualdatatore-adjustthemodel‚Äôs
confidenceandigniteworldknowledge(seedetailedanalysisin¬ß4.2). Subsequently, aproposal
adapterisengagedtooutlineregionsthatarerelatedtothequestions. Thisallowsvisualagentsto
leveragethenewlyacquireddata,therebyfacilitatingamoredetailedandpreciseresponse. Further,if
theinquirynecessitatesdetailedinsightsintoparticularinstances,asegadapterprovidessegmentation
masks,offeringadditionalcontextualinformationfordeeperanalysis(seedetailedanalysisin¬ß4.3).
FASTenjoysafewattractivequalities. ‚ù∂Flexiblesystem: Buildingonafoundationthatexplicitly
modelsSystem1/2thinking, ourproposedmethodadeptlyhandlescomplexvisualtasks, demon-
stratingcompetitiveperformanceinastreamlinedpipeline(see¬ß3.2). FAST‚Äôscoreepistemology
combinesanintuitivemechanismforstraightforwardcaseswithdeliberateanalyticsformoreintri-
catescenarios,therebyenhancingthedevelopmentofahuman-likevisualagent. ‚ù∑Hierarchical
reasoning: FAST perceivesvisualtaskswithatop-downgranularity, encompassingimage-level
cues, box-level candidates, and pixel-level targets (see Fig. 2). This progressive approach facili-
tatesasensibleunderstandingofvisualcontent,startingfromglobalconcepts,progressingthrough
region-specificcandidateassessment,andculminatinginprecisetargetidentification. Eachstageof
thisprocessinvolvesdevelopingconcreteideasandestablishingacoherent‚Äúchainofevidence‚Äùto
supportthefinalinference. ‚ù∏Transparentpipeline: FAST‚Äôsdecision-makingprocessembodiesa
neuro-symbolicessence,yieldingintermediatestepoutputsasreadilyinterpretablesymbols(e.g.,
RoI-drivenboxesormasks),facilitatingdirectvisualinspectionbyhumans. Thisinherentreasoning
mechanismenablesad-hocexplainabilityofthemodel‚Äôsbehavior(seeFig. 3),distinguishingFAST
frompriorapproaches[25,26]thatlackpreciseexplicationoftheiroperationalmechanismsregularly.
Weconductedaseriesofexperimentstovalidatetheefficacyofourproposedmethod. In¬ß4.1.1,
we apply FAST to visual question answering and multimodal benchmarks. FAST demonstrates
significantlyimprovedperformanceoverbaselinessuchasLLaVA-v1.5[26],achievingperformance
gains on benchmarks like TextVQA [27] with a 2.5% increase in accuracy and a total score im-
provementof6.7onMME[28]. In¬ß4.1.2,weexploretheversatilityofourapproachthroughits
applicationtotaskssuchasreferringandreasoningsegmentation,withperformancegainsincluding
anincreaseof4.1%CIoU withLLaVA-v1.5,andimprovementsof3.2%CIoU and2.7%GIoU on
theReasonSegdatasetoverLISA-7B[29]. Therobustnessandeffectivenessofthecorecomponents
ofourframeworkarefurthersubstantiatedthroughaseriesofablationstudies,aselaboratedin¬ß4.3.
2 RelatedWork
LLMasVisualAgents. WiththecapabilitiesthatLLMshavedemonstratedinlanguageunderstand-
ingandgeneration[13,30‚Äì33],theresearchcommunityhasprogressedtoexplorehowLLMscanbe
enhancedwithvisioninputformultimodaltasksasvisualagents[11,25,26,34‚Äì37]. Therearetwo
2paradigmsforLLM-basedvisualagents: end-to-endbasedandtool-usingvisualagents. Following
theprincipleofinstructiontuning,end-to-endvisualagentsaretrainedwithacuratedvisualinstruc-
tiontuningdatasettodigestfeaturesfrommulti-modality,unlockingthecapabilitytoanswervisual
questions[27,38‚Äì45]. Forothervisualtasks(e.g.,Segmentation,Detection,etc),end-to-endtrained
tailoredagentscanfurtherperformdownstreamtasks[29,37,46‚Äì53]. Recentresearchhasfocused
onleveragingimprovedvisionencodersandfosteringmoredetailedvisualunderstanding,yielding
promisingresults[54‚Äì56]. Whiletheseapproachescanbeimplementedwithdirectinstructiontuning
data,theyrepresenta‚ÄòSystem1‚Äôtypeoftraining.Thistypeoftrainingprimarilyreliesonthedataset‚Äôs
qualityandtendstoprovidedirectanswersthatarepronetohallucinations,aconsequenceinherentto
thenatureofSystem1instructiontuningdata. Forthesecondparadigm,tool-usingmodelsarebuilt
ontopofafrozenLLMwithaccesstopretrainedvisualperceptiontools[57‚Äì59]. Inthisscenario,the
LLMfirstselectsvisualtoolsandthendecidesbythoroughlyanalyzingthefine-grainedinformation
extracted by visual tools [59‚Äì61]. Although the utilization of external visual tools increases the
interpretabilityofthereasoningprocess,inaccuraciescanariseinlightofthesystem‚Äôscomplexity. In
addition,duetothecomplexityofreasoning,thevastamountofinformationgeneratedbytheagent
mayoverwhelmkeyinformationrelatedtothequestionquery,leadingtoincorrectresponses.
Ourresearchintroducesanovelandadaptableframeworkdesignedtoenhanceresponseaccuracy
byadoptingdistinctslowthinkingcognitivemodes. Unliketraditionalend-to-endvisualagents,our
framework, FAST,systematicallyassessesinformationsufficiency,therebymitigatingtheriskof
overconfidence. WhenSystem2(slow,analyticalthinking)isactivated, FAST employsmultiple
expertstoconstructacoherentchainofevidence. Thisapproachensuresthegenerationofaccurate
andinterpretableresponses,significantlyadvancingthereliabilityandtransparencyofvisualagents.
System2inAI.Recently,LLMshavebeenengineeredtoproducetextthatmimicsthestep-by-step
reasoningprocesscharacteristicofhumancognition,akintotheanalyticalanddeliberatethought
processesassociatedwithwhatistermedasSystem2inthehumancognitionprocess[62‚Äì66]. The
systematicapproachtoproblem-solvingisahallmarkacrossvariousdomains,includingmathematical
wordproblems[2,10,67‚Äì71],logicalreasoning[16,72‚Äì77],andmulti-modalreasoning[19,60,78].
In Explainable AI, this systematic method is emulated by the model as it generates a text-based
elucidationofitsreasoninganddecision-makingprocessthroughstepbystepreasoningprocess
(e.g.,chainofthought)[5,79‚Äì81]. However,itiscrucialtorecognizethatLLMs,whilepowerful,
arenotexemptfromencounteringchallengeswhenfacingcomplexproblems. Onesuchchallenge
istheissueofhallucination[82‚Äì87],whichcandistortthemodel‚Äôsreasoningprocessandleadto
inaccuraciesintheexplanationsprovided. Initially,LLMreasoningisseenasonlyalinearchain
ofthoughts,whereeachstepinthereasoningprocessisclearlyarticulated. Asmodelsevolve,they
adoptmorecomplexstructureslikehierarchicaltrees[18,88]andintricategraphs[74],whichenable
themtohandlemuchmorecomplexproblemsbutalsorestricttheirgeneralapplicabilitybecauseof
increasedtopologicalcomplexity[18,73‚Äì76]. Moreover,thesecomplexstructurescanleadtoerrors
thatpropagatethroughthemodel‚Äôsreasoning,causingacascadeofmistakes[89]. Tocounterthis,
incorporatingfeedbackfromintermediatereasoningstepsandemployingiterativerefinement,which
issimilartohumanreflection, couldhelpmitigateerrors[19,90‚Äì94]. Inunsupervisedscenarios,
suchfeedbackisvitalforenhancingthereasoningcapabilitiesofLLMsandreducingerrors[95].
Our key contribution is the introduction of the chain of evidence within multimodal reasoning
frameworks. Thismethodologyenricheseachreasoningstepwithaccurate,image-basedcascading
information,effectivelymirroringhumanvisualandSystem2cognitiveprocesses. Ourapproachnot
onlyenhancesaccuracybutalsosignificantlyimprovesinterpretabilityandgeneralizationcapabilities.
3 Methodology
3.1 Preliminary
Notation. TheintegrationofcomponentsinvisualagentsF (basedontheLargeLanguageModel)
typicallyinvolvesavisualencoder,denotedasE ,anaturelanguageencoder,representedbyE ,and
V L
alanguagemodelsuchasVicuna[96]. Initially,thevisualagentispresentedwithanimageI andan
accompanyingtextualpromptQ,whichcouldbeaquestionorinstruction. Thenthevisualagent
combinesthesemultimodaltokensintoaunitedspace. Finally,thevisualagentoutputsatextual
responseRgiventhetextualandimageinput. ThegenerationprocesscanbeexpressedasEq. 1:
R=F[E (I),E (Q)] (1)
V L
3Image Query Slow Answer
Switch Adapter üî•
Is the umbrella The umbrella is on
on the left or Vision-Language the right sideof
right side of the Model the traffic light.
traffic light?
Chain of Evidence Seg Instances
(Remark 3.5)
Switch Adapter (Remark 3.2) üî•
Proposal Adapter üî•
(Remark 3.3)
Slow Thinking Template
Context Clue
Object Missing: Sorry, the current visual
information is not enough. I need to find Traffic Light Umbrella
the [umbrella, traffic light] first.
Context Clue: [umbrella is held by people Seg Adapter
üî•
, traffic light is beside the street] (Remark 3.4)
Figure2: SlowThinkingModeofFAST.Ourslowthinkingmodecomprisesthreecoremodules:
SwitchAdapter,whichselectivelyactivatesaslowandanalyticalthinkingmodewhenencountering
complexvisualqueries,supplementingwithextensiveworldknowledgetoprovidemissingobjects
andcontextualclues;ProposalAdapter,whichidentifiesandemphasizesregionsofinterestwithin
thevisualinputs;SegAdapter,whichdeliversprecisepixel-levelsegmentation,enhancingdepthof
thevisualanalysis. Theoutputsfromeachmoduleareintegratedintoachainofevidence(seeFig.
3),providingamethodicalandaccurateresponse. FASTrepresentsaneural-symbolicapproachthat
combinesthestrengthsofsymbolicreasoning,ensuringthatoursystemiseffectiveandinterpretable.
Definition1 (System1andSystem2)System1andSystem2aretwodifferentsystemsofthinking
proposedbyNobelLaureateDanielKahnemaninhisbookThinking,FastandSlow[22].
System1(FastThinkingSystem): Unconscious,automatedthinkingprocesses,fast,intuitive,effort-
less,responsibleforautomaticresponsesandbasiccognitiveoperationsindailyactivities,vulnerable
toheuristicbiasesanderrors,e.g.,recognizingfamiliarfaces,andknowingthelocationofobjects.
System2(SlowThinkingSystem): Conscious,energeticthinkingprocesses,slow,effortful,logical,
andanalytical,responsibleforcomplexcalculations,reasoning,anddecision-making,canmonitor
andcontrolSystem1processes,e.g.fillingoutataxform,findingthepositionofawordinasentence.
3.2 FAST
Wepresent FAST (seeFig. 2),anovelframeworkdesignedtoefficientlyhandlebothsimpleand
complex visual queries. FAST features a dynamic system switch mechanism that enables rapid
responsestostraightforwardquestions(System1)andaccommodatesdeliberatereasoningforintricate
scenarios(System2). Duringslowthinking,thesystemusescontextualcluestoidentifyarelevant
region,facilitatedbyaproposaladapter. Theadaptergeneratesaboundingboxaroundthetarget
object,andifneeded,apixel-levelmaskadapterrefinestheproposalforfurtherdetails. Finally,we
summarizethegatheredinformationfromthewholesystemtoprovideacomprehensiveanswer.
SystemSwitch. Currentworksonvisualagentsmostlyrelyonvisualquestion-answeringdata,which
givesdirectanswers(System1)afterinquiryasEq. 1. However, attemptingtoanswerquestions
directly in this way can compromise the reliability of the responses. Agents tend to hallucinate
overquestionsthatrequiremoredeliberatereasoningandvisualdetails. Toreducehallucination
andmakethemodelreliable,weutilizeasystemswitchtriggertotellwhentorequiremorevisual
information. Specifically,foraquestionQandanimageI,wedefineaMLLMwithswitchadapter
S andformulatethefastF andslowthinkingprocessF . Whenthequeryiseasy,theframe
fast slow
doesnotneedtheswitchadapterS andonlyoutputresultRbyF asEq. 2.
adapter fast
R=F [E (I),E (Q)] (2)
fast V L
Remark3.1 (Switching-friendlydataset)ANegativeDataforTargetObjectsReasoningDatasetD
of100,000(image,question,answer)tripleswasconstructedtofacilitatetheidentificationoftarget
4regionsorobjectsrequiredtoansweraquestion. Thedatasetconstructsquestionsabouttheabsence
ordetailsofcertainobjects,deliberatelymadetoosmalltobeperceivedbythevisualencoder.
Remark3.2 (Switch Adapter) A light-weight adapter that is fine-tuned with both positive fast-
thinkingdataandnegativedata(Def. 3.1)toacquiresystemswitchingcapability. Whentheadapter
encountersharderquestions,theswitchmechanismwillbetriggeredforlaterslowthinking.
Notethataslowthinkingprocessisnotalwaysactivated. ThesystemswitchadapterasRemark. 3.2
S willdeterminewhetherthequestionfortheparticularimageissufficienttogiveadirect
adapter
answer. Ifso,thefastmodeF willgiveaquickanddirectresponseasEquation2. Ifthereisany
fast
missinginformationaboutthequestionthatcurrentagentcannotsolve,theswitchadapterwillbe
activatedandfindthepatterntoelicitallthepossiblemissingobjectsO relatedtoquestion
missing
andcontextcluesC whichisthepossiblelocationofthemissingobjectsasEquation3.
clue
O ,C =S [E (I),E (Q)] (3)
missing clue adapter V L
Specifically,weusenegativedatathatcontainmissingobjectsO andcontextcluesC for
missing clue
trainingthesystemswitchadapterfortriggeringtheslowmodeasDef. 3.2. Theslowmodeshould
dealwithquestion-imagepairsthatare1)uncertaininpinpointingthespecificobjectinquestion,
and2)toosmalltoperceiveforthestandardvisualencoder. SoweutilizetripletdataindatasetDef.
3.1(image,question,answer)aswherethequestionrequiresobjectsthatarenotintheimageortoo
smalltobeperceivedbythevisualencoder. Thethresholdissettobe20√ó20. Werequirethemodel
totellthatcertainobjectsaremissinginsteadofadirectanswer,andweutilizetheworldknowledge
tolistalltheobjectsandalsothecontextinformationforlaterdeliberatereasoning.
HierarchicalReasoning. Weuseatop-downschemetoreasonovermulti-scalegranularityimages
effectively in order to reason and take advantage of world knowledge progressively. Similar to
humanswouldlookforsomecontextcluetofindspecificobjectsrelatingtoquestionsandzoominif
theythinktheanswerliesinaparticularregion,wemodelthisprocesswithsystemswitchadapters
asEq. 3.2tofocusonthecontextclueC generatedfromtheswitchadapterasEq. 3.
clue
WedenotetheMLLMasaproposaladapterP (visualagent). InSystem2,FaSTusesmany
adapter
visualagentstoaccomplishhierarchicalreasoning. Theframetriestonarrowdownthesearchspace
byusingthequestionQandthepreviouslyobtainedclueC tolettheproposaladapteroutputa
clue
regionRegionthatalignswiththequestionandthecontextclueasEq. 4.
Region=P [E (I),E (Q),C ] (4)
adapter V L clue
Aftergettingtheregion,thevisualagentsP willbeaskedtofocusonamorespecifictarget
adapter
withaboundingbox[Bboxes]complementedbythecontextclueC andregionRegiongetfrom
clue
Eq. 4. Thisprocesscanrevealthestep-by-stepreasoningandbemodeledasEq. 5.
[Bboxes]=P [E (Q),Region,C ] (5)
adapter L clue
Remark3.3 (ProposalAdapter)Alightweightadapterthatisfine-tunedwithproposaldatatoacquire
thecapabilityoffindingthecorrespondingregiongiventhecontextclueorobjectname.
Remark3.4 (Pixel-levelmaskdecoder)ThePixel-levelmaskdecoderisthedecoderofsegment
anything(SAM[97]). Thepixel-levelmaskdecoderisfine-tunedtoproducetargetmasksbasedon
thehiddenembeddings.
Whenwehaveamorespecifictargetproposal(boundingbox[Bboxes]), FAST willapplyafine-
grainedpixel-levelmaskdecoderP asEq. 3.4tooutputthespecificmaskpart[Mask]ofthe
Seg
targetproposal[Bboxes]tofocusonasEq. 6. WenamethiswholeprocessfromRegionto[Mask]
chainofevidenceasDef. 3.5similartothinkingmoreandmoredeeplybyhumans.
[Mask]=P [E (Q),[Bboxes],O ] (6)
seg L missing
Remark3.5 (ChainofEvidence)Chainofevidenceislikethechainofthoughtinalargelanguage
model. Butwedefineitasadeeperanddeeperstepofthinkingbasedoncorrectevidenceinour
frameFAST.Thecompletionofthechainofevidenceneedsmanyvisualagentstoworktogether.
5After getting the target proposal (bounding box
[Bboxes]) from context clue C with proposal Chain of Evidence
clue
adapterandspecificmaskpart[Mask]bymissingob-
jectswithsegadapter,achainofevidenceisconstructed CLIP Image
asRemark3.5andFig. 3. OurFASTframeworkthen Vision
summarizes all this information (I and Q) and the Encoder Context Clue
chainofevidencewithswitchadaptertogivethefinal
correctreasoninganswerAnsasEq. 7 Seg Instance
Ans=F [E (Q),E (I),[Bboxes/Mask]] (7)
Slow L V
Query
Thedecision-makingprocessinherentinFASTisdis-
tinguishedbyitsneuro-symbolicnature,whichgener-
atesintermediateoutputsaseasilyinterpretablesym- Large Language Model Slow
Answer
bols, including region-of-interest (RoI) driven boxes Switch Adapter üî•
andobject-drivenmasks. Thiscapabilityallowsfordi-
rectvisualinspectionbyhumans,therebyaugmenting Figure3: ChainofEvidence. FASTrepre-
thetransparencyofthemodel‚Äôsoperations. Moreover, sentsasolutionrootedinswitching,demon-
theintrinsicreasoningmechanismofFASTenhances stratingpronouncedcapabilitiesinhierar-
ad-hocexplainabilityofitsbehavior,showninFig. 2. chicalreasoningandad-hocexplainability.
3.3 ImplementationDetails
TheframeworkofFASTasFig. 2‚Äôsimplementationdetailsandinareshowninthissectionbelow.
‚Ä¢VisualAgents. WechoosethearchitectureandconfigurationofLLaVA-v1.5[26]asourvisual
agent. ThemostimportantcomponentinavisualagentisthevisualencoderE (I): ACLIP-ViT-L-
V
336pxmodel[98]isused,whereinputimagesareresizedorpaddedto336‚àó336pixels,learningto
associatevisualfeatureswithcorrespondingtextualdescriptions. AnMLPprojectionwithchannels
of[256,4096,4096]isusedforconnectingimagerepresentationsintothewordembeddingspace.
‚Ä¢MaskDecoder. ThemaskdecoderP architectureisidenticaltoSAM.Besides,itisfullyfine-
seg
tunedwithacollectionofsemanticsegmentation[99‚Äì103]andreferringsegmentation[104,105]
datasetstoefficientlymapthe<seg>tokenrepresentationstoamaskiftheFASTneedtosegment.
‚Ä¢ChainofEvidence. Whenweapplythechainofevidenceas3.5inthelargelanguagemodeltoget
theanswerasthefinalsteplikeEq. 7. Thewholesequenceofthechainofevidenceistoolongtoload
intheF Slow. SoFASTneedsavisualsamplerbasedoncross-attentionthatistrainedtodecreasethe
numberofimagetokenstoasuitablelength(from256to32),apartfromMLPprojection.
4 Experiment
WeutilizeeightpopularbenchmarkstoevaluateourframeworkFASTcomprehensively,categorized
into general visual question answering (VQA) datasets and multimodal benchmarks. The VQA
benchmarksincludeVQA-v2[106],GQA[107],ScienceQA[108],andTextVQA[27]whichfocus
onopticalcharacterrecognition. Formultimodalbenchmarksevaluation,weusethehallucination
benchmarkPOPE[109],alongwithcomprehensivebenchmarkssuchasMME[28],MM-Vet[110],
andSEED[111]. WecompareourmodelwiththebaselineLLaVA-v1.5[112],andothermultimodal
largelanguagemodels. Tothoroughlyassessourmodel‚Äôsunderstandingofpixel-levelinstances,
weevaluateitsperformanceonreferringsegmentationandgroundingbenchmarks,includingref-
COCO[105], refCOCO+[105], andrefCOCOg[99]. Further, toexaminethemodel‚Äôsreasoning
capabilitiesonFASTframework,weconsidertheReasoningSegmentationbenchmark[29].
4.1 MainResults
4.1.1 ExperimentsonVQAandMultimodalBenchmarks
Training. IndevelopingtheSwitchAdapter,weemployedtheLLaVA-v1.5[26]framework,con-
formingstrictlytoitsestablishedtrainingprotocols. WeincorporatednegativesamplesfromV‚àó[19]
withcontextualcuestoenhancesystemswitchingcapabilitytoamplifymultimodalinferentialand
worldknowledge. ThisaugmenteddatasetwascombinedwithLLaVA-v1.5‚Äôssuperviseddataset
6VQADatasets MultimodalBenchmarks
Method LLM
VQAv2 GQA VQAT SQAI POPE MME SEED MM-Vet
BLIP-2[ICML23][36] Vicuna-13B 65.0 32.3 42.5 61.0 85.3 1293.8 46.4 22.4
InstructBLIP[NeurIPS24][37] Vicuna-13B - 49.5 50.7 63.1 78.9 1212.8 53.4 25.6
Qwen-VL-Chat[arXiv23][41] Qwen-7B 78.2 57.5 61.5 68.2 - 1487.5 58.2 -
mPLUG-Owl2[CVPR24][45] LLaMA-7B 79.4 56.1 58.2 68.7 - 1450.2 61.6 36.2
Monkey[CVPR24][114] Qwen-7B 80.3 60.7 - 69.4 67.6 - - -
LLaVA-v1.5[CVPR24][26] Vicuna-7B 78.5 62.0 58.2 66.8 85.9 1510.7 58.6 30.5
FAST(Ours) Vicuna-7B 80.8 63.8 60.7 68.9 86.4 1517.4 60.1 31.0
‚àÜ(vsLLaVA-v1.5) Vicuna-7B +2.3 +1.8 +2.5 +2.1 +0.4 +6.7 +1.5 +0.5
Table 1: Main results on eight VQA and multimodal benchmarks. Our FAST consistently
outperformsthebaselineLLaVA1.5modelacrossallevaluatedbenchmarks,denotedwithline‚àÜ.
andtrainedforoneepoch. FortheProposalAdapter,weaugmentedtheLLaVA-v1.5datasetwith
region-specificboundingboxesbasedoncontextualcuesandqueries,thenfine-tunedforoneepoch
tooptimizeproposalgeneration. TheSegmentationAdapterutilizedtheLISA[29]architectureinte-
gratedwiththeLLaVA-v1.5,employingSAMasthemaskdecoder. Theadapterwasfine-tunedusing
thesamedatasetsasLisa,includingsemanticsegmentation,referringsegmentation,andreasoning
segmentation. This fine-tuning process involved 10,000 steps to improve the model‚Äôs segmenta-
tioncapabilities. ThroughoutdevelopingtheSwitchAdapter,ProposalAdapter,andSegmentation
Adapter,weemployedtheLoRA(Low-RankAdaptation)technique[113]. ByleveragingLoRA,we
introduceminimaladditionalparameterswhilepreservingtheoriginalmultimodallargelanguage
model‚Äôsarchitectureandefficiency. Allexperimentsused8NVIDIATESLAA100-80GBGPUs.
Metric. Inmodelevaluationacrossdiversedatasets,variousperformancemetricsareutilized.
Accuracy. TheprimaryevaluationmetricutilizedintheVQAv2,GQA,TextVQA,ScienceQA,and
SEEDbenchmarksisaccuracy. Accuracyisaperformancemeasurethatquantifiestheexactmatch
percentagebetweenpredictedandacceptablegroundtruthanswers,indicatingamodel‚Äôsprecision.
F1Score. ThePOPEdatasetusestheF1Scoretobalanceprecisionandrecall,providingacompre-
hensiveassessmentbyharmonizingthetrade-offbetweenpositivepredictionaccuracyandrecall.
Total Score. The MME evaluation metrics include accuracy (based on individual questions) and
accuracy+(consideringbothquestionsperimage),reflectingastricterandmorecomprehensivemodel
understanding. Randomaccuraciesforthesemetricsare50%and25%, respectively. Perception
scores,calculatedasthesumofthesemetricsacrosssubtasks,total2000forperception.
GPT-Evaluation. IntheMM-Vetdataset,performanceisevaluatedbyGPT-4throughacomparative
analysisofpredictedandgroundtruthanswers,generatingascoretoquantifyalignment.
Results. AsdepictedinTable1, FAST demonstratessuperiorperformanceacrossmultipleVQA
datasets and multimodal benchmarks when compared to established methods. Notably, FAST
surpassestheLLaVA-v1.5model,withmarkedimprovementsinperformanceacrossalldatasets.
Specifically,intheVQAdatasets,ourmodelachievesanenhancedaccuracyof2.3%inVQAv2,1.8%
inGQAand2.5%inVQAT comparingtoLLaVA-1.5. Similarly,inthemultimodalbenchmarks,
FASTenhancesperformancenotablywithanincrementof6.7intheMME,1.5intheSEEDand
0.5intheMM-Vetscore,underliningitseffectivenessinhandlingdiversedomains. Thisconsistent
performanceunderscorestherobustnessofFAST,especiallyincomplexvisualandtextualtasks.
4.1.2 ExperimentsonReferringandReasoningSegmentation
Training. ThetrainingsettingsfortheSwitchAdapterandProposalAdapterremainconsistentwith
thosepreviouslydescribedas¬ß4.1.1. DuringthetrainingphaseoftheSegmentationAdapter,certain
specificdatasetsareintentionallyomittedtoupholdanunbiasedevaluationofreferringandreasoning
segmentation datasets. This strategic exclusion is a crucial measure implemented to prevent any
potentialdataleakage,therebyensuringtheintegrityandreliabilityoftheevaluationresults.
Metric. Followingpriorresearchonsegmentation[104,105],twoevaluationmetricsareemployed:
GeneralizedIntersectionoverUnion(GIoU)andcompleteIntersectionoverUnion(CIoU).
CIoU. The CIoU is calculated based on the cumulative intersection over the cumulative union
acrossallimagesinthedataset. Thisapproachcanintroduceasignificantbiastowardslargerobjects
orimageswithmoreobjects,astheycontributemoretothecumulativeunionarea.
7GIoU. TheGIoU iscomputedastheaverageperimageIoU,wheretheIoU iscalculatedforeach
individualimage,andthentheaverageistakenacrossallimagesinthedataset. Thismetricprovides
abalancedassessmentbytreatingallimagesequally,regardlessoftheirsizeorthenumberofobjects.
Results. Table 2 presents the ReferringSegmentation ReasoningSegmentation
results of referring and reason- Method refCOCO refCOCO+ refCOCOg ReasoSeg
ing segmentation benchmarks. CIoU CIoU CIoU CIoU GIoU
FASTcomparesfavorablytore- LAVT[CVPR22][115] 72.7 62.1 61.2 - -
OVSeg[CVPR23][116] - - - 28.5 18.6
cent visual agents like LISA- GRES[CVPR23][117] 73.8 66.0 65.0 22.4 19.9
7B.Notably,FASToutperforms X-Decoder[CVPR23][118] - - 64.6 22.6 17.9
LISA-7BontherefCOCO+and SEEM[NeurIPS24][119] - - 65.7 25.5 21.2
LISA-7B[CVPR24][29] 74.1 62.4 66.4 44.4 46.0
refCOCOgbenchmarksbyasub-
LLaVAwSegAdapter 70.8 57.5 64.0 43.0 41.0
stantialmarginof2.0%and0.6% FAST(Ours) 73.3 64.4 67.0 47.6 48.7
CIoU, respectively. For the Table2:Mainresultsonreferringandreasoningsegmentationbench-
morechallengingreasoningseg- marks.OurFASTexhibitscompetitiveresultsinreferringsegmentation
mentation task, FAST achieves taskslikerefCOCOg+whileshowcasingsuperiorperformanceinreason-
a significant improvement over ingsegmentation,particularlywhenevaluatedagainstLISA-7B.
LISA-7B,with3.2%CIoU and2.7%GIoU gain,demonstratingitsefficacyacrossdiversesegmen-
tationbenchmarksandshowcasingitsrobustnessinhandlingcomplexvisualreasoningqueries.
4.2 AnalysisofSystemSwitchingAdapter
Ourstudyinvestigatestheefficacyoftheswitchadapter
GQA MME
mechanism in balancing accuracy and computational
efficiency. AsdepictedinFig.4,ouranalysisillustrates
thesystem‚ÄôsadeptnessindiscerningbetweentheSystem System 2
Answer System 1
1andSystem2cognitivemodestriggeredbyquerycom- 36% System 1 System 2 Answer
plexity. Notably,theswitchadapteractivatestheSystem Answer Answer 41%
64% 59%
1modein41.8%and64.4%ofcasesfortheMMEand
GQAdatasets,respectively. Notably,responsesgener-
atedundertheSystem1modeexhibitsignificantlyhigher Figure4:System1ModeAnalysis.Wein-
accuracyrates(78.8%forMMEand64.7%forGQA) vestigatethesystemswitchingratio,along
compared to those under System 2 mode (52.2% for withfastthinkingperformanceoneasyor
MMEand56.8%forGQA),highlightingtheadapter‚Äôs hardqueriesdefinedbytheswitchadapter.
abilitytodifferentiatequerycomplexitiesandoptimizetaskperformanceaccordingly,emphasizing
theimportanceofmaintainingafastthinkingmodeforpromptandconfidentresponses.
Table 3 offers valuable insights into the run-
MME GQA
timecomparisonofdifferentsystemconfigura- Method Runtime Result Runtime Result
tions. System1Onlyoperateswithjustaswitch
System1Only 734ms 1508.7 737 61.9
adapter,withtheslowthinkingprocessdisabled.
System2Only 2938ms 1518.6 2937ms 64.0
ForSystem2Only,weforcetoconstructchain
OURS 2023ms 1517.4 1475ms 63.8
ofevidenceforeverysinglequery. Notably,Sys-
tem 1 only operates efficiently, requiring only Table3:RuntimeAnalysisandComparisonononly
one-timeinference. Incontrast,System2only System1(fast),ourFASTandonlySystem2(slow).
demandssignificantlymorecomputationalresources. Meanwhile,ourFASToffersastrategicad-
vantagebyoptimizingthebalancebetweencomputationalefficiencyandperformance. Itoperates
approximately 31% faster than System 2 Only for MME and about 50% faster for GQA, while
maintainingcomparableperformanceoutcomes. FASTenhancestheoverallefficiencyofcognitive
processingtasks,offeringapragmaticapproachtooptimizingcomputationalresources.
4.3 AblationStudy
AlgorithmComponent GQA POPE MME OutputComponent MME refCOCOg
BASELINE 62.1 85.7 1509.2 BASELINE* 1511.8 66.0
+ProposalAdapter 63.2 86.0 1516.5 +MissingObjects 1513.4 66.8
+SegAdpater 62.8 85.8 1514.4 +ContextClue 1516.6 66.4
OURS(both) 63.8 86.2 1517.4 OURS(both) 1517.4 67.0
Table4:KeyComponentAnalysis Table5:SwitchAdapterOutputAnalysis
8KeyComponentAnalysis. WeinvestigatethecoreelementsofournovelframeworkFAST,focusing
ontheproposaladapterforcontextualregionlocalizationandthesegadapterforpixel-levelmask
segmentation. To establish a BASELINE, we deploy a model devoid of proposal adapter and seg
adapter,relyingsolelyonaswitchadaptertosupplymissingobjectsandcontextclues. Asshown
inTable4,afterintegratingtheproposaladapter,segadapter,orboth,weobserveprogressiveand
significantenhancementsinperformanceacrossvariousevaluationmetrics(62.1%‚Üí63.8%accuracy
onVQAv2). Thesefindingsunderscorethepivotalrolesplayedbytheproposalandsegadaptersin
augmentingthemodel‚Äôscapabilities,therebyaffirmingtheirsignificancewithintheFASTframework.
Furthermore,weanalyzetheoutputcomponentsofourswitchadapterinFAST,focusingonincluding
missingobjectsandcontextclues. WeconstructBASELINE*modelwhichoperateswithoutthese
enhancements. AsillustratedinTable5,addingthemissingobjectscomponentshowsanoticeable
improvementinbothMMEandrefCOCOgscores. Similarly,theintegrationofcontextcluesalso
leadstofurtherperformancegains,albeitnotassignificantasmissingobjects.Whenbothcomponents
areintegrated,themodeldemonstratesthebestresultmetrics. Theseresultshighlightthesignificant
impactofincorporatingmissingobjectsandcontextcluesonthemodel‚Äôsoveralleffectiveness.
4.4 QualitativeComparisonsofFAST
Query Query
What are the jars sitting on top of? Is the monitor on top of a person?
LLaVA-v1.5 LLaVA-v1.5
A Coffee maker. No. The monitor is below the person.
Chain of Evidence Chain of Evidence
Clue: The jar is most likely to appear Clue: The monitor is likely on the roof.
on the kitchen counter.
Proposal + <seg> Ans: Stove. Proposal + <seg> Ans: Yes.
Query Query
What items should you find when you are Segment out the containers that make
hungry? Please output the seg mask. me feel energized after drinking.
Chain of Evidence LISA-7B Chain of Evidence LISA-7B
Clue: The item is likely Sure, it‚Äôs <seg>. Clue: The container is Sure, it‚Äôs <seg>.
on the table. likely on the desk.
Proposal Proposal
<seg> <seg>
Figure 5: Qualitative Comparisons of FAST. The top row shows the VQA results on FAST
comparedtoLLaVA-v1.5. ThebottomrowpresentsthesegmentationresultscomparedtoLISA-7B.
InFig.5,wepresentqualitativecomparisonstoillustratetheenhancementsintroducedbyFAST.The
toprowabovethedottedlineshowstheresultsfortheVQAtask,comparingFASTwithLLaVA-v1.5.
LLaVA-v1.5failstoidentifythecorrectfocusingareas,resultinginincorrectresponses. Incontrast,
FASTleveragestheextensiveworldknowledgeofLLMstoconstructachainofevidence(e.g.,it
canidentifyawomanonthestreetoramonitorontheroof). ThenFASTdeterminestheappropriate
focusareawithobject-levelpixelmasksforanaccurateanddeliberateanswer. Thebottomrowunder
thedottedlinedisplayssegmentationresultscomparedwithLISA-7B.LISA-7Bexhibitsconfusionin
accuratelydistinguishingitemsthatarecomparativelysmallerandrequiremoreextensivereasoning
todiscern. Conversely,FASTeffectivelyisolatestherelevantobjectswithgreatergranularityand
accuracy. ThisdemonstratesthesuperiorcapabilityofFASTinbothVQAandsegmentationtasks.
5 Conclusion
Inthisstudy,weintroducedFAST,aninnovativeframeworkthatintegratestheconceptsofSystem1
(fast,intuitive)andSystem2(slow,deliberate)thinkingintovisualagents,aimingtoenhancetheir
reasoninganddecision-makingcapabilities. FASTdynamicallyadaptstoqueriesofvaryingcomplex-
itybyemployingaflexiblesystemswitchmechanism,ensuringrapidresponsesforstraightforward
questionsandemployinghierarchicalreasoningformoreintricatescenarios.Ourresultsacrossawide
arrayofbenchmarksdemonstratesignificantimprovementsinperformance,highlightingtheefficacy
ofFAST‚Äôschainofevidencemethodologyinreducinghallucinationandenhancinginterpretability.
Ourablationstudieshighlightthecrucialroleofcontextualcluesandpixel-leveladaptersinachieving
comprehensive visual reasoning and understanding. FAST significantly advances visual agents‚Äô
capability,pavingthewayforthemorereliable,accurate,andhuman-likeAIcognitionprocess.
9References
[1] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou,etal. Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguageModels.
InNeurIPS,2022.
[2] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,Aakanksha
Chowdhery,andDennyZhou. Self-ConsistencyImprovesChainofThoughtReasoningin
LanguageModels. InICLR,2023.
[3] EricZelikman,YuhuaiWu,JesseMu,andNoahD.Goodman. Star: Self-TaughtReasoner
BootstrappingReasoningwithReasoning. InNeurIPS,2022.
[4] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans,ClaireCui,OlivierBousquet,QuocLe,etal. Least-to-MostPromptingEnables
ComplexReasoninginLargeLanguageModels. InICLR,2023.
[5] WenyueHuaandYongfengZhang. System1+System2=BetterWorld: Neural-Symbolic
ChainofLogicReasoning. InEMNLP,2022.
[6] YifanZhang,YangYuan,andAndrewChi-ChihYao. MetaPromptingforAISystems. arXiv
preprintarXiv:2311.11482,2023.
[7] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic Chain of Thought
PromptinginLargeLanguageModels. InICLR,2023.
[8] ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhiting
Hu. ReasoningwithLanguageModelisPlanningwithWorldModel. InEMNLP,2023.
[9] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
ModelsareFew-ShotLearners. InNeurIPS,2020.
[10] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
LargeLanguageModelsareZero-ShotReasoners. InNeurIPS,2022.
[11] YingqiangGe,WenyueHua,KaiMei,JianchaoJi,JuntaoTan,ShuyuanXu,ZelongLi,and
YongfengZhang. OpenAGI:WhenLLMMeetsDomainExperts. InNeurIPS,2023.
[12] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. ScalingLawsforNeural
LanguageModels. arXivpreprintarXiv:2001.08361,2020.
[13] OpenAI. GPT-4TechnicalReport. arXivpreprintarXiv:2303.08774,2024.
[14] JieHuangandKevinChen-ChuanChang. TowardsReasoninginLargeLanguageModels: A
Survey. InACL,2023.
[15] Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan
Kumaran,JamesLMcClelland,andFelixHill. LanguageModelsShowHuman-LikeContent
EffectsonReasoning. arXivpreprintarXiv:2207.07051,2022.
[16] MingyuJin,QinkaiYu,DongShu,HaiyanZhao,WenyueHua,YandaMeng,YongfengZhang,
andMengnanDu. TheImpactofReasoningStepLengthonLargeLanguageModels. arXiv
preprintarXiv:2401.04925,2024.
[17] MaxwellNye,MichaelTessler,JoshTenenbaum,andBrendenMLake. ImprovingCoherence
andConsistencyinNeuralSequenceModelswithDual-System,Neuro-SymbolicReasoning.
InNeurIPS,2021.
[18] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,andKarthik
Narasimhan. TreeofThoughts: DeliberateProblemSolvingwithLargeLanguageModels. In
NeurIPS,2023.
10[19] PenghaoWuandSainingXie. GuidedVisualSearchasaCoreMechanisminMultimodal
LLMs. InCVPR,2024.
[20] AnishaGunjal,JihanYin,andErhanBas. DetectingandPreventingHallucinationsinLarge
VisionLanguageModels. InAAAI,2024.
[21] YuyanChen,QiangFu,YichenYuan,ZhihaoWen,GeFan,DayihengLiu,DongmeiZhang,
ZhixuLi,andYanghuaXiao. HallucinationDetection: RobustlyDiscerningReliableAnswers
inLargeLanguageModels. InCIKM,2023.
[22] DanielKahneman. Thinking,FastandSlow. Farrar,StrausandGiroux,2011.
[23] Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao, and Yin Xie.
Plug-and-PlayGroundingofReasoninginMultimodalLargeLanguageModels.arXivpreprint
arXiv:2403.19322,2024.
[24] ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyes
WideShut? ExploringtheVisualShortcomingsofMultimodalLLMs. InCVPR,2024.
[25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. VisualInstructionTuning. In
NeurIPS,2023.
[26] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. ImprovedBaselineswithVisual
InstructionTuning. InCVPR,2024.
[27] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,Devi
Parikh,andMarcusRohrbach. TowardsVQAModelsthatCanRead. InCVPR,2019.
[28] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui
Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. MME: A Com-
prehensiveEvaluationBenchmarkforMultimodalLargeLanguageModels. arXivpreprint
arXiv:2306.13394,2024.
[29] XinLai,ZhuotaoTian,YukangChen,YanweiLi,YuhuiYuan,ShuLiu,andJiayaJia. Lisa:
ReasoningSegmentationviaLargeLanguageModel. InCVPR,2024.
[30] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. TrainingLanguageModels
toFollowInstructionswithHumanFeedback. InNeurIPS,2022.
[31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang,ZiLin,ZhuohanLi,DachengLi,EricXing,etal. JudgingLLM-as-a-Judgewith
MT-BenchandChatbotArena. InNeurIPS,2023.
[32] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
th√©eLacroix,BaptisteRozi√®re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andEfficientFoundationLanguageModels. arXivpreprintarXiv:2302.13971,2023.
[33] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
FoundationandFine-TunedChatModels. arXivpreprintarXiv:2307.09288,2023.
[34] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo:AVisual
LanguageModelforFew-ShotLearning. InNeurIPS,2022.
[35] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,
AyzaanWahid,JonathanTompson,QuanVuong,TianheYu,etal. PaLM-E:AnEmbodied
MultimodalLanguageModel. arXivpreprintarXiv:2303.03378,2023.
[36] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: BootstrappingLanguage-
Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML,
2023.
11[37] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang,BoyangLi,PascaleNFung,andStevenHoi. InstructBLIP:TowardsGeneral-Purpose
Vision-LanguageModelswithInstructionTuning. InNeurIPS,2024.
[38] ShaohanHuang,LiDong,WenhuiWang,YaruHao,SakshamSinghal,ShumingMa,Tengchao
Lv, LeiCui, OwaisKhanMohammed, BarunPatra, etal. LanguageisNotAllYouNeed:
AligningPerceptionwithLanguageModels. InNeurIPS,2023.
[39] GenLuo,YiyiZhou,TianheRen,ShengxinChen,XiaoshuaiSun,andRongrongJi. Cheap
andQuick: EfficientVision-LanguageInstructionTuningforLargeLanguageModels. In
NeurIPS,2023.
[40] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4:
EnhancingVision-LanguageUnderstandingwithAdvancedLargeLanguageModels. arXiv
preprintarXiv:2304.10592,2023.
[41] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin,
ChangZhou,andJingrenZhou. Qwen-VL:AFrontierLargeVision-LanguageModelwith
VersatileAbilities. arXivpreprintarXiv:2308.12966,2023.
[42] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,WenqiShao,WenweiZhang,KaiChen,
andPingLuo. GPT4ROI:InstructionTuningLargeLanguageModelonRegion-of-Interest.
arXivpreprintarXiv:2307.03601,2023.
[43] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTong
Sun. LLAVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding.
arXivpreprintarXiv:2306.17107,2023.
[44] DelongChen,JianfengLiu,WenliangDai,andBaoyuanWang. VisualInstructionTuning
withPoliteFlamingo. InAAAI,2024.
[45] QinghaoYe, HaiyangXu, GuohaiXu, JiaboYe, MingYan, YiyangZhou, JunyangWang,
AnwenHu,PengchengShi,YayaShi,etal. mPLUG-Owl: ModularizationEmpowersLarge
LanguageModelswithMultimodality. arXivpreprintarXiv:2304.14178,2023.
[46] RenjiePi,JiahuiGao,ShizheDiao,RuiPan,HanzeDong,JipengZhang,LeweiYao,Jianhua
Han, Hang Xu, and Lingpeng Kong Tong Zhang. DetGPT: Detect What You Need via
Reasoning. InEMNLP,2023.
[47] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuru
Wei. Kosmos-2: GroundingMultimodalLargeLanguageModelstotheWorld. InICLR,2024.
[48] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
UnleashingMultimodalLLM‚ÄôsReferentialDialogueMagic. arXivpreprintarXiv:2306.15195,
2023.
[49] WenhaiWang,ZheChen,XiaokangChen,JiannanWu,XizhouZhu,GangZeng,PingLuo,
TongLu,JieZhou,YuQiao,etal. VisionLLM:LargeLanguageModelisAlsoanOpen-Ended
DecoderforVision-CentricTasks. InNeurIPS,2023.
[50] WeiyunWang,MinShi,QingyunLi,WenhaiWang,ZhenhangHuang,LinjieXing,ZheChen,
HaoLi,XizhouZhu,ZhiguoCao,etal. TheAll-SeeingProject: TowardsPanopticVisual
RecognitionandUnderstandingoftheOpenWorld. InICLR,2024.
[51] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,etal. CogVLM:VisualExpertforPretrainedLanguageModels.
arXivpreprintarXiv:2311.03079,2023.
[52] DongshengJiang,YuchenLiu,SonglinLiu,XiaopengZhang,JinLi,HongkaiXiong,and
QiTian.FromCLIPtoDINO:VisualEncodersShoutinMulti-ModalLargeLanguageModels.
arXivpreprintarXiv:2310.08825,2023.
12[53] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. MiniGPT-V2:
LargeLanguageModelasaUnifiedInterfaceforVision-LanguageMulti-TaskLearning. arXiv
preprintarXiv:2310.09478,2023.
[54] XiaoranFan,TaoJi,ChanghaoJiang,ShuoLi,SenjieJin,SiruiSong,JunkeWang,Boyang
Hong,LuChen,GuodongZheng,etal. MouSi: Poly-Visual-ExpertVision-LanguageModels.
arXivpreprintarXiv:2401.17221,2024.
[55] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua,
ZhiyuanLiu,MaosongSun,andGaoHuang. Llava-UHD:AnLMMPerceivingAnyAspect
RatioandHigh-ResolutionImages. arXivpreprintarXiv:2403.11703,2024.
[56] BaifengShi,ZiyangWu,MaolinMao,XinWang,andTrevorDarrell. WhenDoWeNotNeed
LargerVisionModels? arXivpreprintarXiv:2403.13043,2024.
[57] D√≠dac Sur√≠s, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python
ExecutionforReasoning. InICCV,2023.
[58] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
HuggingGPT:SolvingAITaskswithChatGPTandItsFriendsinHuggingFace. InNeurIPS,
2023.
[59] PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-WeiChang,YingNianWu,Song-Chun
Zhu, andJianfengGao. Chameleon: Plug-and-PlayCompositionalReasoningwithLarge
LanguageModels. InNeurIPS,2023.
[60] HaoxuanYou,RuiSun,ZhecanWang,LongChen,GengyuWang,HammadAAyyubi,Kai-
WeiChang, andShih-FuChang. Idealgpt: IterativelyDecomposingVisionandLanguage
ReasoningviaLargeLanguageModels. InEMNLP,2023.
[61] YixuanWu,YizhouWang,ShixiangTang,WenhaoWu,TongHe,WanliOuyang,JianWu,
andPhilipTorr. DetToolChain: ANewPromptingParadigmtoUnleashDetectionAbilityof
MLLM. arXivpreprintarXiv:2403.12488,2024.
[62] ShuofeiQiao,YixinOu,NingyuZhang,XiangChen,YunzhiYao,ShuminDeng,Chuanqi
Tan,FeiHuang,andHuajunChen. ReasoningwithLanguageModelPrompting: ASurvey. In
ACL,2023.
[63] JieHuangandKevinChen-ChuanChang. TowardsReasoninginLargeLanguageModels: A
Survey. InACL,2023.
[64] BoshiWang,SewonMin,XiangDeng,JiamingShen,YouWu,LukeZettlemoyer,andHuan
Sun. Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What
Matters. InACL,2023.
[65] OmarShaikh,HongxinZhang,WilliamHeld,MichaelBernstein,andDiyiYang. OnSecond
Thought,Let‚ÄôsNotThinkStepbyStep! BiasandToxicityinZero-ShotReasoning. InACL,
2023.
[66] HaoShao,ShengjuQian,HanXiao,GuangluSong,ZhuofanZong,LetianWang,YuLiu,and
HongshengLi. Visualcot: Advancingmulti-modallanguagemodelswithacomprehensive
dataset and benchmark for chain-of-thought reasoning. arXiv preprint arXiv:2403.16999,
2024.
[67] HunterLightman,VineetKosaraju,YuraBurda,HarriEdwards,BowenBaker,TeddyLee,
JanLeike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let‚ÄôsVerifyStepbyStep. arXiv
preprintarXiv:2305.20050,2023.
[68] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. TrainingVerifiersto
SolveMathWordProblems. arXivpreprintarXiv:2110.14168,2021.
13[69] YixinLiu,AviSingh,CDanielFreeman,JohnDCo-Reyes,andPeterJLiu. ImprovingLarge
LanguageModelFine-TuningforSolvingMathProblems. arXivpreprintarXiv:2310.10047,
2023.
[70] XinyuZhu,JunjieWang,LinZhang,YuxiangZhang,YongfengHuang,RuyiGan,Jiaxing
Zhang,andYujiuYang. SolvingMathWordProblemsviaCooperativeReasoningInduced
LanguageModels. InACL,2023.
[71] PanLu,LiangQiu,Kai-WeiChang,YingNianWu,Song-ChunZhu,TanmayRajpurohit,Peter
Clark,andAshwinKalyan.DynamicPromptLearningviaPolicyGradientforSemi-Structured
MathematicalReasoning. InICML,2023.
[72] YaoYao,ZuchaoLi,andHaiZhao. BeyondChain-of-Thought,EffectiveGraph-of-Thought
ReasoninginLargeLanguageModels. arXivpreprintarXiv:2305.16582,2023.
[73] Fanglong Yao, Changyuan Tian, Jintao Liu, Zequn Zhang, Qing Liu, Li Jin, Shuchao Li,
Xiaoyu Li, and Xian Sun. Thinking Like an Expert: Multimodal Hypergraph-of-Thought
(HOT)ReasoningtoBoostFoundationModals. arXivpreprintarXiv:2308.06207,2023.
[74] MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,MichalPodstawski,Lukas
Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al.
GraphofThoughts: SolvingElaborateProblemswithLargeLanguageModels. InAAAI,2024.
[75] YilinWen,ZifengWang,andJimengSun. MindMap: KnowledgeGraphPromptingSparks
GraphofThoughtsinLargeLanguageModels. arXivpreprintarXiv:2308.09729,2023.
[76] BinLei,ChunhuaLiao,CaiwenDing,etal. BoostingLogicalReasoninginLargeLanguage
ModelsThroughaNewFramework: TheGraphofThought. arXivpreprintarXiv:2308.08614,
2023.
[77] Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, and Nan
Du. Self-PlayingAdversarialLanguageGameEnhancesLLMReasoning. arXivpreprint
arXiv:2404.10642,2024.
[78] ZhenfangChen,RuiSun,WenjunLiu,YiningHong,andChuangGan. GENOME:GenerativE
Neuro-SymbOlicVisualReasoningbyGrowingandReusingModulEs. InICLR,2024.
[79] Cheng Han, James C Liang, Qifan Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao,
YingNianWu,andDongfangLiu. ImageTranslationasDiffusionVisualProgrammers. In
ICLR,2024.
[80] HaiyanZhao,HanjieChen,FanYang,NinghaoLiu,HuiqiDeng,HengyiCai,Shuaiqiang
Wang,DaweiYin,andMengnanDu. ExplainabilityforLargeLanguageModels: ASurvey.
ACMTransactionsonIntelligentSystemsandTechnology,15(2):1‚Äì38,2024.
[81] AlonJacoviandYoavGoldberg. TowardsFaithfullyInterpretableNLPSystems: HowShould
WeDefineandEvaluateFaithfulness? InACL,2020.
[82] YiyangZhou,ChenhangCui,JaehongYoon,LinjunZhang,ZhunDeng,ChelseaFinn,Mohit
Bansal, andHuaxiuYao. AnalyzingandMitigatingObjectHallucinationinLargeVision-
LanguageModels. InICLR,2024.
[83] ChenhangCui,YiyangZhou,XinyuYang,ShirleyWu,LinjunZhang,JamesZou,andHuaxiu
Yao. HolisticAnalysisofHallucinationinGPT-4V(ision): BiasandInterferenceChallenges.
arXivpreprintarXiv:2311.03287,2023.
[84] JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-YunNie,andJi-RongWen. HALUEVAL:A
Large-ScaleHallucinationEvaluationBenchmarkforLargeLanguageModels. InEMNLP,
2023.
[85] MuruZhang,OfirPress,WilliamMerrill,AlisaLiu,andNoahASmith. HowLanguageModel
HallucinationsCanSnowball. InarXivpreprintarXiv:2305.13534,2023.
14[86] XiangChen,ChenxiWang,YidaXue,NingyuZhang,XiaoyanYang,QiangLi,YueShen,
JinjieGu,andHuajunChen. UnifiedHallucinationDetectionforMultimodalLargeLanguage
Models. arXivpreprintarXiv:2402.03190,2024.
[87] TianruiGuan, FuxiaoLiu, XiyangWu, RuiqiXian, ZongxiaLi, XiaoyuLiu, XijunWang,
LichangChen,FurongHuang,YaserYacoob,etal. Hallusionbench: AnAdvancedDiagnostic
Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language
Models. InCVPR,2024.
[88] ShijieGeng,JianboYuan,YuTian,YuxiaoChen,andYongfengZhang. Hiclip: Contrastive
language-imagepretrainingwithhierarchy-awareattention. InICLR,2023.
[89] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is Inevitable: An Innate
LimitationofLargeLanguageModels. arXivpreprintarXiv:2401.11817,2024.
[90] ZhengChu,JingchangChen,QianglongChen,WeijiangYu,TaoHe,HaotianWang,Weihua
Peng,MingLiu,BingQin,andTingLiu. ASurveyofChainofThoughtReasoning:Advances,
FrontiersandFuture. arXivpreprintarXiv:2309.15402,2023.
[91] YongqiTong,DaweiLi,SizheWang,YujiaWang,FeiTeng,andJingboShang. CanLLMs
LearnfromPreviousMistakes? InvestigatingLLMs‚ÄôErrorstoBoostforReasoning. arXiv
preprintarXiv:2403.20046,2024.
[92] Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun.
MitigatingLargeLanguageModelHallucinationsviaAutonomousKnowledgeGraph-Based
Retrofitting. InAAAI,2024.
[93] AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,SarahWiegreffe,
Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-Refine: Iterative
RefinementwithSelf-Feedback. InNeurIPS,2023.
[94] WeizheYuan,RichardYuanzhePang,KyunghyunCho,SainbayarSukhbaatar,JingXu,and
JasonWeston. Self-RewardingLanguageModels. arXivpreprintarXiv:2401.10020,2024.
[95] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuan
Cao. React: Synergizingreasoningandactinginlanguagemodels. InICLR,2023.
[96] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:
AnOpen-SourceChatbotImpressingGPT-4with90%*ChatGPTQuality,March2023.
[97] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,
TeteXiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. SegmentAnything. In
ICCV,2023.
[98] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable
VisualModelsfromNaturalLanguageSupervision. InICML,2021.
[99] HolgerCaesar,JasperUijlings,andVittorioFerrari. COCO-Stuff: ThingandStuffClassesin
Context. InCVPR,2018.
[100] BoleiZhou, HangZhao, XavierPuig, SanjaFidler, AdelaBarriuso, andAntonioTorralba.
SceneParsingthroughADE20KDataset. InCVPR,2017.
[101] VigneshRamanathan,AnmolKalia,VladanPetrovic,YiWen,BaixueZheng,BaishanGuo,
RuiWang,AaronMarquez,RamaKovvuri,AbhishekKadian,etal.PACO:PartsandAttributes
ofCommonObjects. InCVPR,2023.
[102] JuHe,ShuoYang,ShaokangYang,AdamKortylewski,XiaodingYuan,Jie-NengChen,Shuai
Liu,ChengYang,QihangYu,andAlanYuille. PartImageNet: ALarge,High-QualityDataset
ofParts. InECCV,2022.
15[103] XianjieChen,RoozbehMottaghi,XiaobaiLiu,SanjaFidler,RaquelUrtasun,andAlanYuille.
DetectWhatYouCan: DetectingandRepresentingObjectsUsingHolisticModelsandBody
Parts. InCVPR,2014.
[104] JunhuaMao,JonathanHuang,AlexanderToshev,OanaCamburu,AlanLYuille,andKevin
Murphy. GenerationandComprehensionofUnambiguousObjectDescriptions. InCVPR,
2016.
[105] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraBerg.ReferItGame:Referring
toObjectsinPhotographsofNaturalScenes. InEMNLP,2014.
[106] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthe
VinVQAMatter: ElevatingtheRoleofImageUnderstandinginVisualQuestionAnswering.
InCVPR,2017.
[107] DrewAHudsonandChristopherDManning. GQA:ANewDatasetforReal-WorldVisual
ReasoningandCompositionalQuestionAnswering. InCVPR,2019.
[108] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via
ThoughtChainsforScienceQuestionAnswering. InNeurIPS,2022.
[109] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluating
ObjectHallucinationinLargeVision-LanguageModels. InACL,2023.
[110] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,Xinchao
Wang, and Lijuan Wang. MM-Vet: Evaluating Large Multimodal Models for Integrated
Capabilities. InICML,2024.
[111] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-Bench:
BenchmarkingMultimodalLLMswithGenerativeComprehension. InCVPR,2024.
[112] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. VisualInstructionTuning. In
NeurIPS,2023.
[113] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
LuWang,andWeizhuChen. LoRA:Low-RankAdaptationofLargeLanguageModels. In
ICLR,2022.
[114] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,Yuliang
Liu, and Xiang Bai. Monkey: Image Resolution and Text Label are Important Things for
LargeMulti-ModalModels. InCVPR,2024.
[115] Zhao Yang, Jiaqi Wang, Yansong Tang, KaiChen, HengshuangZhao, andPhilip HSTorr.
LAVT:Language-AwareVisionTransformerforReferringImageSegmentation. InCVPR,
2022.
[116] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao
Zhang,PeterVajda,andDianaMarculescu. Open-VocabularySemanticSegmentationwith
Mask-AdaptedCLIP. InCVPR,2023.
[117] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized Referring Expression
Segmentation. InCVPR,2023.
[118] XueyanZou,Zi-YiDou,JianweiYang,ZheGan,LinjieLi,ChunyuanLi,XiyangDai,Harkirat
Behl,JianfengWang,LuYuan,etal. GeneralizedDecodingforPixel,Image,andLanguage.
InCVPR,2023.
[119] XueyanZou, JianweiYang, HaoZhang, FengLi, LinjieLi, JianfengWang, LijuanWang,
JianfengGao,andYongJaeLee. SegmentEverythingEverywhereAllatOnce. InNeurIPS,
2023.
[120] IlyaLoshchilovandFrankHutter. DecoupledWeightDecayRegularization. InICLR,2019.
16[121] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.BLIP:BootstrappingLanguage-Image
Pre-TrainingforUnifiedVision-LanguageUnderstandingandGeneration. InICML,2022.
[122] HilaChefer,ShirGur,andLiorWolf. GenericAttention-ModelExplainabilityforInterpreting
Bi-ModalandEncoder-DecoderTransformers. InICCV,2021.
[123] ZuyanLiu,YuhaoDong,YongmingRao,JieZhou,andJiwenLu. Chain-of-Spot: Interactive
ReasoningImprovesLargeVision-LanguageModels. arXivpreprintarXiv:2403.12966,2024.
[124] AnnaRohrbach,MarcusRohrbach,RonghangHu,TrevorDarrell,andBerntSchiele. Ground-
ingofTextualPhrasesinImagesbyReconstruction. InECCV,2016.
[125] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,
TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. PyTorch: AnImperative
Style,High-PerformanceDeepLearningLibrary. InNeurIPS,2019.
17SUMMARYOFTHEAPPENDIX
ThisappendixcontainsadditionaldetailsfortheNeurips2024submission,titled‚ÄúVisualAgentsas
FastandSlowThinkers‚Äù. Theappendixisorganizedasfollows:
‚Ä¢ ¬ßAprovidesImplementationDetailsandPseudoCode.
‚Ä¢ ¬ßBreportsmoreQuantitativeResultsforVisualQuestionAnswering.
‚Ä¢ ¬ßCshowsmoreQuantitativeResultsforSegmentation.
‚Ä¢ ¬ßDanalyzesFailureCase.
‚Ä¢ ¬ßEexaminestheLimitationandFutureWorkofourresearch.
‚Ä¢ ¬ßFdiscussestheSocialImpactofourresearch.
‚Ä¢ ¬ßGoffersEthicalGuardorourdataset.
‚Ä¢ ¬ßHclaimsReproducibilityofourapproach.
‚Ä¢ ¬ßIsuppliesDataLicenseforthemethodsweusedforcomparison.
A ImplementationDetailsandPseudo-codeof FAST
Hyper-parameters. Wefollowestablishedmethodologies[26]andutilizeLLaVA-v1.5 [26]asthe
foundationalvisualagent. Theimageresolutionispreprocessedto336√ó336pixelstoaccommodate
theclip-vit-large-patch14-336visionencoder[98].TheAdamWoptimizer[120]isemployedwiththe
DeepSpeedZeRO21configurationforfine-tuningtheswitch,proposal,andsegmentationadapters
withLoRA[113]. FortheLoRAconfiguration,wesettherankto128andalphato256,consistent
with the settings of LLaVA-v1.5. Additionally, we adjust the learning rate of the vision encoder
projectionlayerto2e-5toachievebetteralignment. AnMLPprojectionwithchannelsof[256,4096,
4096]isusedtoconnectimagerepresentationsintothewordembeddingspacefortheprojection
layer. Anadditionalresamplerprojectionlayerisusedtoreducethenumberofimagetokens.
TrainingDataforSwitchAdapter.ConsistentwiththepretrainingstageofLLaVA-v1.5,weinitially
pretrainVicuna-v1.5asabasefrozenlargelanguagemodelandfortheMLPprojectionlayerand
samplerlayeroftheCLIPvisionencoderusinga558KsubsetoftheLAION-CC-SBUdataset2with
BLIP[121]captions. Duringthefine-tuningstage,weintegratethenegativedatasetacquiredfrom
V‚àó[19]withtheoriginalLLaVA-v1.5instructiontuning665kdata3forLoRAbasedfinetuning.
TrainingDataforProposalAdapterTodeterminethecorrespondingregionforaquery,weuse
LRP++[122]fordataconstruction,similartoChainofSpot[123]. Ourinitialpromptisasfollows:
<Image>
To answer the question: [Q],
where is the region of interest in the image based on [C]?
Ans.str[w , w , h , h ]
0 1 0 1
ThequestionQandthecontextclueC areformattedtogettheanswerintermsofaboundingbox.
Inthisformat,w ,w representtheleftandrightboundaries,respectively,whileh ,h denotethe
0 1 0 1
upperandlowerboundaries. Toidentifythecorrectregion,wesampledonequestionperimagefrom
theLLaVAinstructiontuningdata,consistingofatotalof665kdataforproposalfinetuning.
TrainingDataforSegAdapter. AdoptinganapproachsimilartoLISA[29],thetrainingdataforour
modelcomprisesthreedistinctsegments: asemanticsegmentationdataset,areferringsegmentation
dataset,andareasoningsegmentationdataset. Wedeliberatelyexcludevisualquestion-answering
datasetstoenhancethemodel‚Äôssegmentationperformance. Thesemanticsegmentationsegment
includestheADE20K[100],COCO-Stuff[99],andLVIS-PACO[101]partsegmentationdatasets.
ThereferringsegmentationdatasetsencompassrefCOCO[105],refCOCO+[105],refCOCOg[99],
1https://github.com/microsoft/DeepSpeed
2https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain
3https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
18andrefCLEF[124]. ThereasoningsegmentationdatasetincludesReasonSeg[29]. Itisimportant
tonotethatthereferringsegmentationandreasoningsegmentationdatasetsarecarefullyexcluded
duringtheevaluationofthesegmentationbenchmarkstopreventanypotentialdataleakage.
Pseudo-codeImplementation. Thepseudo-codeofFASTisgiveninPseudo-code1.
Algorithm1:Pseudo-codeofFASTinaPyTorch-likestyle.
class FaST:
def __init__(self, switch_llm, proposal_llm, seg_llm):
self.switch_llm = switch_llm
self.proposal_llm = proposal_llm
self.seg_llm = seg_llm
def get_contextual_clues(self, image, question):
# Get missing objects and context clues using switch adapter
return self.switch_llm(image, question)
# Construct Chain of Evidence
def construct_coe(self, image, question, context_clues, missing_objects):
# Step 1: Get region proposals
region = self.proposal_llm(image, question, context_clues)
# Step 2: Get pixel-level mask for the missing objects
mask = self.seg_llm(region, missing_objects)
return (context_clues, region, missing_objects, mask)
# Main Function
def forward(self, image, question):
# Get initial answer
initial_answer = self.switch_llm(image, question)
# Check if slow thinking is needed based on the initial answer
if "sorry, i can not answer" in initial_answer.lower():
# Perform slow thinking
missing_objects, context_clues = initial_answer[‚Äôobj‚Äô], initial_answer[‚Äôclue‚Äô]
chain_of_evidence = self.construct_coe(image, question, context_clues,
missing_objects)
# Generate the final answer using the constructed chain of evidence
final_answer = self.switch_llm(image, question, chain_of_evidence)
else:
# Perform fast thinking
final_answer = initial_answer
return final_answer
B MoreQualitativeResultsforVisualQuestionAnswering
Figure6presentsadditionalqualitativeresultsforVisualQuestionAnswering(VQA).OurFAST
frameworkconsistentlydemonstratesremarkableperformanceacrossvariouschallengingscenarios.
Notably, in the bottom right corner of Figure 6, our FAST leverages extensive world knowledge
toidentifythekeyboard,whichsubsequentlyaidsindiscoveringthehiddencomputermouseand
providingthecorrectanswer.Thisabilitytointegrateandutilizecontextualinformationshowcasesthe
model‚Äôsadvancedcapabilitiesandhighlightsitspotentialforpracticalapplications. Thequalitative
resultsfurtherunderscoreFAST‚ÄôsrobustnessandversatilityinhandlingdiverseVQAtasks.
C MoreQualitativeResultsforSegmentation
Figure7showcasesfurtherqualitativeresultsfortheSegmentationtask. OurFASTmodelexcelsin
variouschallengingscenarios,accuratelylocatingdifficulttargetsandperformingcomplexreasoning
formoredemandingqueries.Forinstance,inthebottomrightcorner,themodelsuccessfullyidentifies
anappliancethatcanbeturnedonwhenfeelinghotbyrecognizingrelevantcontextualcluesthat
suggesttheapplianceshouldprobablyappearonthewall,therebyresultinginthecorrectanswer.
Thisexampledemonstratesthemodel‚Äôsadvancedunderstanding,adaptability,andprecision.
19Query Query
Are the trousers the same color
Is the red oven below a microwave?
as the shoes?
ChainQ oufe Eryvidence Are theC htraoinu sQ oefu r e Esr vty ihdee nscaeme color
ClIuse :t hTeh er erde do voevne nb ealnodw ma icmriocwroawvea vies? Clue: The traosu stehres sihs omeoss?t likely to
most likely to appear next to the appear on the person.
Crheafinr iogfe rEavtiodre.nce Chain of Evidence
ClueP:r Tohpoes rael d + o <vseeng >a nd microwave is ClueP:r Tophoes tarl o +u s<esresg >i s most likely to
most l ‚úÖikely rA et nfo sr a :i gp Yep ere saa .tr o rn .ext to the a‚úÖppeaArn osn: Nthoe. person.
Proposal + <seg> Proposal + <seg>
Is the m‚úÖale pA en rs sQ: o nYu ee brs ey. hind the catcher Is the‚úÖ comA pn us tQ: e N ru eo mr. oy use on top of
wearing a face mask? the mousepad both black and small?
Is the maleC hpaeirnsQ oonfu e bEr evy hidinedn ctehe catcher Is the coC mh pa uin tQ eo ruf e mEr oyv uid se en oc ne top of
Clue: Thwee amrainleg pae frascoen misa msko?st likely thCel umeo: uTsheep acdo mbpouthte brl amcoku asned i ss mmaolsl?t
to appea Cr hin
a
inth oe
f
b Ea vs ie db ea nl cl euniform. likely to apCpheaairn noefx Etv tido etnhcee keyboard.
CluPer: oTphoes aml a +le < speegr>s on is most likely CluPer:o pTohsea lc o+m <psuetge> r mouse is most
to app ‚úÖear Ain
n
st :h Ye eb sa .seball uniform. likely to
‚úÖ
appeAanrs n: eNxot. to the keyboard.
Proposal + <seg> Proposal + <seg>
Query
Figure6: M‚úÖoreAnqs: QuYeuaes.lriytativeresultsforVisualQuestionSAegmnesn‚úÖwt oeutAr inthns:e g Na.po.pliances that can
Please segment the plant in this room. be turned on when it's hot
Please segCmheainnt otQ fh u eEe vpr ily daenntc ien this room. Se Cg lm ubee e:n Tt tC huoh reua n ti aen pt d oh p Q f ole i na u Ea ne wvp cr ip hedy l eseia n n tn c hic te ae 'tss cht ah ona t t b eca n
Clue: The plant is most likely to appear turned on when it‚Äôs hot is most likely
Chaionn o tfh Eev tidaebnlec.e tCoh aapinp eoafr Eovni dtehnec weall.
Clue: The appliances that can be
Clue: The plant is most likely to appear turPnreodp oosna wl h+e <ns eitg‚Äôs> hot is most likely
Proposal + <osne gt>h e table.
to appear on the wall.
Proposal + <seg> Proposal + <seg>
Query Query
Please segment the black and white Please segment the vehicle to take
dog with pointy ears. after eating.
Please seC gh ma ein n tQo f tu h eE erv yi bd le acn kc e and white Please sC eh ga min e noQ tf tu E he v er id y ve en hc ie cle to take
Clue: Tdhoeg b wlaictkh apnodin wtyh ietaer sd.og with Clue: The vehaifctleer t eoa ttainkge. after eating
pointy ear Cs h i as i nm oo fs t El vy i dli ek ne cly e to appear is mostly C l hik ae inly o t fo Ea vp ip de ea nr c eon the road.
on a bed.
Clue: The black and white dog with Clue: The vehicle to take after eating
pPorinotpyo seaalr s+ i<ss emgo>s tly likely to appear is mPorsotploys laikl e +ly < steo ga>p pear on the road.
on a bed.
Proposal + <seg> Proposal + <seg>
Figure7: MorequalitativeresultsforVisualQuestionAnswering.
D FailureCase
InFig. 8,wepresentanoverviewofthemostnotablefailurecases,providinginsightsintothedistinct
patternsthatleadtosuboptimaloutputsinourFASTmodel. Thesechallengesincludedifficultyin
triggeringtheSystem2thinkingmode,constructingadequatecontextualclues,generatingappropriate
proposals, and providing accurate pixel masks. The model often fails to recognize the need for
deliberatereasoning,relyinginsteadonSystem1thinking,whichleadstoincorrectresponses,asseen
inFig. 8a. Inadequatecontextualcluesgeneratedbytheswitchadapterimpairthemodel‚Äôsfocuson
thecorrectregion,resultinginvagueorincorrectresponses,asillustratedinFig. 8b. Theproposal
adapter‚Äôsinaccurateidentificationofregionsofinterest,asshowninFig.8c,leadstoproposalsthatdo
notcorrespondtothequery. Additionally,thesegmentationadapterstruggleswithproducingprecise
masks,particularlyforsmalloroccludedobjects,causingerroneousconclusions,ashighlightedin
Fig. 8d. These failure cases underscore the urgent need for refinement in our FAST framework,
emphasizingtheimportanceofsignificantlyenhancingtheprecisionofthesystemswitchadapter,
improvingcontextualclueconstruction,andoptimizingtheproposalandsegmentationadaptersto
achievemorereliableandconsistentlyaccurateresponsesincomplexvisualandtextualtasks.
E LimitationandFutureWork
Whilethe FAST frameworkhasdemonstratedsignificantadvancementsinemulatinghuman-like
cognitiveprocessesinvisualAIthroughitsfastandslowthinkingmechanisms,severallimitations
warrantattention. Firstly,thesystem‚Äôsrelianceonapredefinedsetofnegativedatafortrainingthe
switchadaptermaynotencapsulatethefullspectrumofreal-worldcomplexities. Firstly,thiscould
leadtosuboptimalperformancewhenfacedwithnovelorunexpectedscenarios. Secondly,despiteits
fine-grainedanalysiscapability,thepixel-levelmaskdecodermightstrugglewithhighlytextured
orpatternedimageswheresegmentationbecomeschallenging. Lastly,thegeneralizabilityofFAST
acrossvariousdomainsandtasksnecessitatesfurthervalidationtoensureitsrobustnessandreliability
20Query Query
Is this a picture of Canelles de Baix
Does the flower look yellow?
(la Vall de Binya)?
Query Query
Is this a pictuGrTe oAfn Cswaneerlles de Baix GT Answer
(la VaQAll n uds ee: r BN yion.ya)? Does the f QAlon uws ee: r rN y loo.ok yellow?
Is this a pictFuaGrsTet oAAfnn Csswwaneeerrlles de Baix Does thFe aG fslTto wAAennrss wwloeeorrk yellow?
(l ‚ùåa VaAQAlln n udss ee::
r
YBN yeions.y.a)?
‚ùåAns:
Yes,QA tn hus ee:
r
fN yloo w.
er is yellow.
Is t (h ais ) a T p (l ‚ùåhi ac etF Vua aGr mAs AlQTe lt n n d uosoAA see:f : d nn r YBNCss yeeww ia onlsnee .y.e frr al a)le ?i s l sde t oBa ti rx i ggerSystem2thinkingmIosd ‚ùåtehi.D sA o ae nrs st :wt Yoh erFe ks aG f Q ,s A dlT tto u in h sw eAA s epe r:nn l yr afNss ylwwl ooo ewee .o drr ek ir ny ie Ssl l tyo .e w vl? liotuws.'s
What is tFhaeGs Trto AApenn ssawwteetrrached to? cathFeaGdsTrta AAl, nnPssrwwageeurre?
‚ùåGAAQ Tnn usAse:: nr YN syewose..r
Is
‚ùåthisA an rs t:
w
Y oe rGksQ ,TA dt u inhAsese pnr:
l
sy afN wyloo eew. dre ir
n
i Ss ty .e vll io tuw s.
's
What is tFhaeAs nrtso A:p eCn soawattet.rached to? cathFeAadsnrtsa :Al ,N nPosr.wageure?
Query Query
Ch‚ùåainG AoTnf sAE: nvYsidewese.nrce Is ‚ùåthisA anrCsth:w aYoiernGks o ,T dft i hAsEepnvl siafdwyleoeenwdrce eirn iSs ty.e vlliotuws.'s
What is the rope attached to?
Clue: The ropeA inss m: Coosat tl.ikely to appear Clue: Thec Satth. evAidtnrusas:l ',N sP ocr.aagthuee?dral, Prague
noCt hparienGs oQeTfn u tAEe nivrnsiy d wtehenerc eimage. Iis s m tho is st a l riCtkhweal oyirn Gkt oQo Tdf a u i AsEpe pnpvr lsiey adwa yer een drnc eeina r S t th . e v ic tl ui sf 'f s.
CPlurWeo:p hToashtae li sr +ot ph<seeAe inrgsso> m:p eCo osaatt ttl.iak‚ùåcehlye dAt otn osa?:p Hpeaatr. ClPureo:p Toshaelc S a+tt h.< sevAeidtngrus>as :l ',N sP ocr.aagthu‚ùåee?drAaln, sP:r Yageuse.
noCt hparienGs oeTfn tAE nivnsid wtehenerc eimage. is most liCkhealyin Gt ooTf a AEpnpvsiedwaeren rnceear the cliff.
CPlureo:p Toshael r +o p<seAe Qingss>u m:e Croy osat tl.ik‚ùåely Aton sa:p Hpeaatr. C IslPu r teo h:p eTo sh bae lul eS + t
l
u.< s gvAe giQtng aus> gus :ee 'N rs oy onc. a tt hh‚ùåee d ler fAa tln, osP r:r Ya rg ie gus he.
t
Whnaott ipsr tehsee ncto lionr t ohfe tihmea gdeo.g? is most likely to appear near the cliff.
Chain of Evidence Chasiind eo fo fE vthidee bncues?
C(Plbure)o:p TToshhaele r +om p<seGoe Q iTgsd >u A mee nrlosy sfwta eliirkl‚ùåeslyt oAton csa:op Hpneasatrt.r uctadequatecontextualCIc slPu rl teou h:p eeTo sshba.elul eS + tl u.< s gvGe giQ tTg au> g u s Aee 'n r sosy ncw atethrh‚ùå ee dlerfAatln, osPr:r Yargie gus he. t
Whnaott ipsr tehAseen nsct:o lWionr ht oihtfee t.ihmea gdeo.g? is most likeslyid tAeo n osaf:p pRtheigaehr bt n.uesa?r the cliff.
\ Pro Wpo hs aa tCl h i s+a i t<ns hG e eoQ Tg f c >u A oEe lnvr osiy rdw e oen frc ‚ùå te he A don gs ?: Hat. \ IsP r to hp eo s ba lulC e h+ al u<ins gG e goQ Tg af> g u AEee n vr osiy dnw etenhrc‚ùåee lefAtn osr: Yriegsh.t
Clue: The dogA isn sm: oWsth iltikee.ly to appear Clue: The sbidluAee n oslfu: gRtghiagegh ebt,.u bs?us is most
\ CPlruoeWp: oThsahateC l h idn s+a o ei <tgnx shG A t eeioQ T s ng ft sc> uo mA :oEe oa lWnvr os siy rpd tw h e e oiler tn ifkrs ec ‚ùåeo t.e hn ly. e At dono sag:p? Bpelaacrk . \ Is PC rt loh ul pi eek o: e sb Tl aly u hC l et eh +o sal b u ii <a dn lsgp uG A eego epQ T ngafe osl>g u fa uA :Eee r gRtn vr go hisiy in adn gw es g e h tei ebn td hr ,.c ue e ‚ùåe bs ?t uleh sAfe it ns b s o mu :r s Lor. seig tf h t.t
\ ChnaeinxG toQT ft uo AEe anvr siypdweeernrscoen. \ likelyC thoa ianpG opTfe Q aAEr unv eisindrws yeeindrcee the bus.
CPlruoep: oITssha tel h d+eo <lgsaA emisngp s>m :t oWos tthh ilteike r‚ùåe.ilgyh Atto no sar:p Bpelaacrk . PCrloupeo:A sTraehl e t+ h b<elsuAreeeng sl>au :n gRygi agygheetll,.o ‚ùåbwu ssAh ionses m:s Losetf t.
\ t Co htn ahe ie nx tl oQe ftf uo t Ee a vro iyfp d e etr nhs ceo e nc.ar? \ likely C t ho a ia ninp o ptfeh Q a Eer u v i ei imn drs yaeigndece?e the bus.
CPlruoep: oITssha tel h d+eo <lgsaG emiTsgp > m A tonoss ttwh leeikr r‚ùåeilgyh Atto no sar:p Bpelaacrk . PCrloupeo:A sTraehl e t+ h b<elsureeeGg l>Tau n gAyg naysgewell,eo ‚ùåbrwu ssAh ionses m:s Losetf t.
to tnheex tlA Qe tnf uost e : ar o L yfpe etfrhts.eo nc.ar? likely to ainp pteAh Qaenr u s i eim:n r Ns yaigode.e? the bus.
Pro(pcoI)ssa TtCl h hh+ea e<ilnsaG emmoTgfp> oA Etnodvs idtewheleenrf cr‚ùåaeigihlAst notsro: Bglaecnk.erateappropriateproposaPlrsop.oAsrael Ct+hh <easrieneGg o>Ta fn AyE nvysiedwleleon‚ùårwc esAhones:s Left.
C lil ku Ie e s: lt y tT Co ht hht o eaeh ia le nc apG a mlA opTer Q fe pnsf a As t u Eta r: e nov on L r os ifd dtye wn het fl eta eh nt rhm c.e ree p ic g sai htsr tr ?m e oeo rts .t tC ol u ae p A: p rT e eah Cr te hh oy eai nen ri n l etGlt o h Ah oTaw Qefe nn Ays fus Eim: eh e nv yN ero sia edye twg lo e lse e. o on ? rf wi cs et sm h heo o s ept sl a li yk ee rl sy .
CPrluoep:ot Tsoa htle h +ec a<lAesrnesfs tga: > on Lfde tflahtm.e pc aisr ?most CluPer:o Tpohsea yli en + llt o<Ahswene sgsim:h> Noaegose. ?is most likely
lik ‚ùåelyC Atho na sian:pG RopTife g aAhEr tnv .osidwne etnrhcee street. to ap ‚ùåpeaCrA honansi n:tG YhoTeef s AfE.envesidtwe eonrfc ethe players.
CPrluoep:o Tsahle +c a<Asrness ga:> n Lde flatm.p is most CluPer:o Tpohsea yl e +ll o<Aswne sgs:h> Noeos. is most likely
PCl ri lk ‚ùå uoe epl :oy Ts ACt aho n lh e s aa +: cip nR a<p srioe g esfa h g r aEt > n .ov dind lte ah mnec p e s it sr mee ot s. t t Co l ua Pep r:‚ùå op Tpe oa hsr eA a C o yn lh n es a +: lti l n oYh <s we eo e sff gs. e h>E e ov etid soe f in s ct mhee o sp tl a liy ke er ls y.
li ‚ùåkely Ato n sa : p Rp ie ga hr t .on the street. to ap ‚ùåpearA onns :t Yhee sf.eet of the players.
Proposal + <seg> Proposal + <seg>
‚ùå Ans: Right. ‚ùå Ans: Yes.
(d)Themodelfailstoprovideaccuratepixelmasks.
Figure8: FailurecasesofFAST.
indiverseapplications. Weplantodevelopadvancedlearningmechanismsthatwillallowthemodel
togeneralizemoreeffectivelybeyondthepredefinednegativedataset. Additionally,wewillfocuson
optimizingitforreal-timeapplicationstoreducecomputationaloverheadandresponsetimes.
F SocialImpacts
ThedevelopmentanddeploymentofFASTcarrysignificantimplicationsforthefieldofAI,especially
inaugmentingthehuman-likecognitivecapabilitiesofLLM-basedvisualagents. Onthepositive
side,FAST‚ÄôsemulationofhumancognitiveprocessesholdspromiseforcreatingmoreadvancedAI
systemsthatcanassistincriticalareaslikegeneralvisionquestiondialogueandsecuritysurveillance,
whereprecisevisualcomprehensionisparamount.Moreover,itstransparentdecision-makingpipeline
contributestotheexplainabilityofAI,fosteringtrustandfacilitatingethicalAIpractices. However,
it‚Äôs imperative also to acknowledge potential negative social impacts. The reliance on extensive
modelsanddatasetscouldperpetuatebiasesifnotmeticulouslycurated,potentiallyresultinginunjust
ordiscriminatoryoutcomes. Addressingtheseethicalconsiderationsandestablishingguidelinesfor
theresponsibleutilizationofsuchsophisticatedAIsystemsareparamountrequirements.
G EthicalSafeguards
In our paper introducing a novel framework FAST, we implement rigorous ethical measures to
prevent potential misuse and promote responsible application. These measures are delineated in
21comprehensive protocols accompanying the final release of models and datasets. Our protocols
encompassstringentusageguidelines,accesscontrols,incorporationofsafetyfilters,andmonitoring
systems. These concerted efforts reflect our steadfast dedication to upholding the utmost ethical
standardsinscientificexploration.Ourobjectiveistoprotecttherightsandprivacyofallstakeholders
involved,therebyfosteringacultureofresponsibleandethicalresearchwithinourcommunity.
H Reproducibility
Our FAST framework is implemented in PyTorch [125]. All the experiments are conducted
on eight NVIDIA A100-80GB GPUs. Our full implementation shall be publicly released upon
paper acceptance to guarantee reproducibility. The codes are available at the anonymous link
https://anonymous.4open.science/r/Sys2-LLaVA-8B0F/ forthereviewprocess.
AllExperiments(switch,proposal,andsegadapter)areconductedoneightNVIDIAA100-80GB
SXMGPUs4. Reproducingthefine-tuningprocesswouldrequireapproximately15A100GPUdays.
I Licensesforexistingassets
All the methods we used for comparison are publicly available for academic usage. The switch
adapterisimplementedbasedonthereleasedcode(https://github.com/penghao-wu/vstar)
withanMITlicense. Theproposaladapterisimplementedonthereleasedcode(https://github.
com/dongyh20/Chain-of-Spot)withanApache-2.0license. Thesegadapterisimplementedon
thereleasedcode(https://github.com/dvlab-research/LISA)withanApache-2.0license.
4https://www.nvidia.com/en-sg/data-center/a100/
22