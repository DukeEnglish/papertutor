EasyRec: Simple yet Effective Language Models
for Recommendation
XubinRen ChaoHuangâˆ—
UniversityofHongKong UniversityofHongKong
xubinrencs@gmail.com chaohuang75@gmail.com
ABSTRACT Inrecentyears,therehasbeenanotablesurgeofadvancements
Deepneuralnetworkshavebecomeapowerfultechniqueforlearn- inenhancingrecommendersystemsthroughtheincorporationof
ingrepresentationsfromuser-iteminteractiondataincollaborative neuralnetwork-poweredcollaborativefilteringframeworks,partic-
filtering(CF)forrecommendersystems.However,manyexisting ularlyinthedomainofgraphneuralnetworks(GNNs)[10,32,37].
methodsheavilyrelyonuniqueuseranditemIDs,whichlimits Byeffectivelyleveragingtheinherentgraphstructurepresentin
theirabilitytoperformwellinpracticalzero-shotlearningscenar- thedata,GNNshavedemonstratedexceptionalcapabilitiesincap-
ioswheresufficienttrainingdatamaybeunavailable.Inspiredby turinghigh-orderrelationshipsandcomplexdependenciesamong
thesuccessoflanguagemodels(LMs)andtheirstronggeneraliza- usersanditems.NotableexamplesofsuchGNN-basedapproaches
tioncapabilities,acrucialquestionarises:Howcanweharnessthe includeNGCF[32]andLightGCN[10].Thesemethodshaveshow-
potentialoflanguagemodelstoempowerrecommendersystems casedimpressiveperformanceinrecommendationtasksbyvirtue
andelevateitsgeneralizationcapabilitiestonewheights?Inthis oftheirabilitytomodelthecollaborativesignalspresentinthedata
study,weproposeEasyRec-aneffectiveandeasy-to-useapproach throughrecursivemessagepassingmechanisms.
thatseamlesslyintegratestext-basedsemanticunderstandingwith Theissueofdatascarcityinrecommendersystemsposesasig-
collaborativesignals.EasyRecemploysatext-behavioralignment nificantchallengeforexistingdeepcollaborativefilteringmodels,
framework,whichcombinescontrastivelearningwithcollabora- hinderingtheirabilitytolearnpreciseuser/itemrepresentations,
tivelanguagemodeltuning,toensureastrongalignmentbetween particularlywhendealingwithsparseinteractiondata[9,18,26,33].
thetext-enhancedsemanticspaceandthecollaborativebehavior Thischallengeprimarilyarisesfromthelimitedavailabilityoftrain-
information.Extensiveempiricalevaluationsacrossdiversereal- inglabels,whichinturnimpedesthemodelsâ€™capacitytocapture
worlddatasetsdemonstratethesuperiorperformanceofEasyRec theintricaterelationshipsanddependenciesthatexistbetween
comparedtostate-of-the-artalternativemodels,particularlyinthe usersanditems.Toalleviatedatascarcityeffects,recentstudies
challengingtext-basedzero-shotrecommendationscenarios.Fur- exploredthepotentialofself-supervisedlearningtoprovideef-
thermore,thestudyhighlightsthepotentialofseamlesslyintegrat- fectivedataaugmentation.Forexample,intermsofcontrastive
ingEasyRecasaplug-and-playcomponentintotext-enhancedcol- augmentation,methodslikeSGL[34]andNCL[19]leveragegraph
laborativefilteringframeworks,therebyempoweringexistingrec- contrastivelearningtosupplementthesupervisedrecommendation
ommendersystemstoelevatetheirrecommendationperformance taskwiththecross-viewmutualinformationmaximization.For
andadapttotheevolvinguserpreferencesindynamicenviron- generativeaugmentation,approachessuchasAutoCF[36]arebuilt
ments.ForbetterresultreproducibilityofourEasyRecframework, uponthemaskedautoencodingmechanism,enablingthemodels
themodelimplementationdetails,sourcecode,anddatasetsare toreconstructtheinteractionstructuresforself-supervision.
availableatthelink:https://github.com/HKUDS/EasyRec. Whiletherecentadvancementsinself-supervisedlearningtech-
niqueshaveofferedpromisingavenuesformitigatingtheimpact
ACMReferenceFormat:
ofdatascarcityincurrentcollaborativefilteringmodels,theyalso
XubinRenandChaoHuang.2024.EasyRec:SimpleyetEffectiveLanguage
comewithasignificantlimitation[45].Thislimitationstemsfrom
ModelsforRecommendation.InProceedingsofACMConference.ACM,
theinherentdesignofthemodels,whichheavilyreliesonusing
Barcelona,Spain,13pages.https://doi.org/10.1145/nnnnnnn.
uniqueidentities(IDs)torepresentusersanditemsthroughout
theentireprocessoflearningrepresentations.Inpracticalrecom-
1 INTRODUCTION
menders,however,weoftencomeacrossnewrecommendationdata
Deep learning has established itself as a highly promising and collectedfromdifferentdomainsortimeperiods,involvingdiverse
powerfulsolutionforcapturinguserpreferencesinthecontext sets ofusers and items. This creates achallenge as existingID-
ofonlinerecommendersystems[42,50].Thisapproachharnesses basedrecommendationmodelsstruggletoeffectivelyincorporate
thepowerofdeepneuralnetworkstolearnrichandmeaningful andadapttosuchnewdata,especiallywhenthereisachangeinthe
useranditemrepresentationsbyanalyzingthecomplexpatterns setofidentitytokensofusersanditemsforzero-shotrecommen-
ofuser-iteminteractions.This,inturn,enablesrecommendation dation.TherigiddependenceonuseranditemIDsinthesemodels
algorithmstoaccuratelyinferuserpreferencesandprovidehighly hinderstheirabilitytogeneralizeandperformwellinscenarios
relevantandpersonalizedrecommendations[30,41]. wheretheuseranditemspacesarenotstaticorfullyoverlapping.
Forinstance,theconstantgenerationofnewitemslikevideos
âˆ—ChaoHuangistheCorrespondingAuthor.
andsocialmediacontentnecessitatesaccuraterecommendations
in real-life recommender systems, even when there are limited
ACMConference,XXX,XXX
interaction observations. While cross-domain recommendation
2024.ACMISBN978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.
4202
guA
61
]RI.sc[
1v12880.8042:viXraACMConference,XXX,XXX XubinRenandChaoHuang
82M 125M 356M Insummary,thisworkmakesthefollowingkeycontributions:
0.06 EasyRec EasyRec
0.05 BGE 0.020 â€¢ Motivation.Theprimaryobjectiveofthisstudyistointroducea
0.04 GTR 0.015 BG GTR E novelrecommendersystemthatisbuiltuponlanguagemodelsto SimCSE functionasazero-shotlearner.Thisinnovativeapproachaimsto
0.03
0.02 BART 0.010 Sim BC AS RE T demonstrateexceptionaladaptabilitytodiverserecommendation
data,whilealsoexhibitingrobustgeneralizationcapabilities.
75 100 125~ 350 400 75 100 125~ 350 400 â€¢ Methodology.Toaligntext-basedsemanticencodingwithcol-
Model Parameters (Miliion) Model Parameters (Miliion)
laborativesignalsfromuserbehavior,weproposeanovelcon-
Figure1:EasyRecdemonstratessuperiorperformancecom- trastivelearning-poweredcollaborativelanguagemodelingap-
paredtostate-of-the-artlanguagemodelsintext-basedzero- proach,whichallowsthesystemtocaptureboththesemantic
shotrecommendationontheAmazonandSteamdatasets. representationsofusers/items,aswellastheunderlyingbehav-
ioralpatternsandinteractionswithintherecommendationdata.
methods[1,39]drawinspirationfromleveraginginformationand â€¢ Zero-ShotRecommendationCapacity.TheEasyRecisexten-
knowledgeacrossmultipledomainstoenhancerecommendation sivelyevaluatedthroughrigorousexperimentsasatext-based
performance,theyoftenassumethatusersfromdifferentdomains zero-shotrecommender.Performancecomparisonsrevealthatit
belongtothesameset[3,39].Unfortunately,thisassumptionsig- consistentlyachievessignificantadvantagesoverbaselinemeth-
nificantlyrestrictstheflexibilityandgeneralizationcapabilitiesof odsintermsofrecommendationaccuracyandgeneralization
recommenders.Asaresult,existingmethodsmayencounterdiffi- capabilities.Furthermore,thestudydemonstratesthemodelâ€™s
cultiesinadaptinganddeliveringaccuraterecommendationswhen remarkablepotentialingeneratingdynamicuserprofilesthat
confrontedwithdiverseuserpopulationsacrossdifferentdomains. arehighlyadaptivetotime-evolvinguserpreferences.
LanguageModelsasZero-ShotRecommenders.Considering â€¢ ExistingCFModelEnhancement.OurproposedEasyRechas
thechallengesandmotivationsdiscussedearlier,theobjectiveof beenseamlesslyintegratedasalightweight,plug-and-playcom-
thisstudyistointroducearecommendersystemthatfunctionsasa ponentwithstate-of-the-artcollaborativefiltering(CF)models.
zero-shotlearner,possessingrobustgeneralizationcapabilitiesand Thelightweightandmodulardesignofourlanguagemodelis
theflexibilitytoadapttonewrecommendationdataseamlessly.To akeystrength,asitfacilitatestheadoptionofournovelrecom-
accomplishthisobjective,weproposeintegratinglanguagemod- mendationparadigmacrossawiderangeofusecases.
elswithcollaborativerelationmodeling,forminganeffectivetext
embedder-EasyRecthatisbothlightweightandeffective.Thisin-
2 PRELIMINARIES
tegrationseamlesslycombinestext-basedsemanticencodingwith
high-ordercollaborativesignals,resultinginarecommendersystem Inrecommendersystems,wehaveasetofusersU andasetof
thatoffersastrongmodelgeneralizationability. items I, along with the interactions between them (e.g., clicks,
Recentresearchhasexploredleveraginglargelanguagemod- purchases).Foreachuserğ‘¢ âˆˆU,wedefineNğ‘¢ asthesetofitems
els(LLMs)toenhancerecommendersystems.Existingapproaches
thatuserğ‘¢hasinteractedwith.Likewise,foreachitemğ‘– âˆˆI,we
broadlyfallintotwocategories.ThefirstusesLLMsfordataaug- defineNğ‘– asthesetofuserswhohaveinteractedwiththatitem.To
mentation(e.g.,RLMRec[25],AlterRec[15]),encodingtextualinfor- representtheseuser-iteminteractions,wecanuseaninteraction
mationtocomplementcollaborativefiltering.Whilethiscombines
matrixA|U|Ã—|I|,wheretheentryAğ‘¢,ğ‘– is1ifuserğ‘¢hasinteracted
LLMandcollaborativestrengths,thesemethodsremainID-based
withitemğ‘–,and0otherwise.Theprimarygoalofarecommender
andstruggletogeneralize.ThesecondapproachutilizesLLMstodi- model is to estimate the probabilityğ‘ ğ‘¢,ğ‘– of a future interaction
rectlygenerateuser-iteminteractionpredictionsbasedonlanguage
betweenauserğ‘¢andanitemğ‘–.Thispredictedprobabilitycanthen
(e.g., LLaRA [17], CoLLM [49]). However, such LLM-based rec- beusedtogeneratepersonalizeditemrecommendationsforeach
ommendersfacesignificantreal-worldlimitations,sufferingfrom user,tailoredtotheirindividualpreferencesandpastbehavior.
poorefficiency(e.g.,âˆ¼1secondperprediction),renderingthem Text-basedZero-ShotRecommendationisessentialinrecom-
impracticalforlarge-scalerecommendationtasks.Thesechallenges mendersystems,asitcanaddressthecommoncold-startproblem.
highlighttheneedformoreefficient,scalablesolutionsthatseam- Inreal-worldscenarios,newusersandnewitemsoftenlacksuffi-
lesslyintegratethesemanticunderstandingoftextualinformation cientinteractiondata,makingitchallengingtoprovideeffective
withthecollaborativestrengthsofzero-shotrecommenders. personalizedrecommendations.Byleveragingthetextualdescrip-
Ourproposedmodelhasdemonstratedsuperiorperformance tionsofusersanditems,suchasproducttitles,details,anduserpro-
incomparisontostate-of-the-artlanguagemodels,asillustrated files,thelanguagemodelscanconstructsemanticrepresentationsto
inFigure1.Thisperformanceadvantageisachievedwithinacost- enabletext-basedrecommendationsforthesecold-startsituations.
efficientparameterspace,rangingfrom100to400millionparame- Thisovercomesthelimitationsoftraditionalcollaborativefilter-
ters,withthecomputationalcostofâˆ¼0.01secondperprediction. ingmethods,byofferingadistinctadvantageoverthetraditional
Notably,ourmodelexhibitsthescalinglawphenomenon,where ID-basedparadigm.Byleveragingrobustlanguagemodels,this
itsperformancecontinuallyimprovesastheparametersizeisin- approachexhibitsremarkablepotentialinâ€œzero-shotâ€scenarios,
creased.Furthermore,incontrasttoexistingapproachesthatsuffer wherethetestingdatahavenotbeenpreviouslyencountered.
frompoorefficiency,ourmodelisdesignedtobehighlyscalable Formally,wedefinePğ‘¢ andPğ‘– asthegeneratedtext-basedpro-
andpracticalforlarge-scalerecommendationtasks. files of userğ‘¢ and itemğ‘–, respectively, which are encoded into
)nozamA(
02@llaceR
)maetS(
02@llaceREasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Item MetaInformation
[ R e c o m m e n d e d I t e m L is t ] IU ns te er r- aI ct te im on atT th r laai bs c e t lb s =a N s pk oB se A itt i b vfa ea nll s . Th swis li ai m bte em lm =i n nis g e g n l aoi tc iv ve e e rfo sr Th bis a ss lkh ae bo ete lb =i as l p lf o pa siv l ta io vyr eee rd s .by It Ie t Rem m a wT Fi t Dele a e: t s uB cra re is psk t e ( iT otb a nga )ll CoC nl ce aa tn en& ate M Tee b xt a ta s Dd ea d et sa c- .
Ranking â‹¯ EasyRec
This basketball Generated
Other LLM attractsNBA fans. Item Profile
ğ‘ğ‘ğ‘¢ğ‘¢,ğ‘–ğ‘–1 > ğ‘ğ‘ğ‘¢ğ‘¢,ğ‘–ğ‘–2 Items
EasyRec â‹¯ EasyRec This basketball This ball This basketball
Text embedding draws in NBA captivates fans captivates
It Be am ğ‘–ğ‘–s1k P er tbo afi ll le U Ss pe ğ‘¢ğ‘¢or r P tsr o fafi nle Item C oP ğ‘–ğ‘–ar 2o tfile PP uu sll h c alo ws ae y baskeT th bi as l u l-s re er la l to ev de is t ems. Th swis iu ms mer i nlik ge .s enthusiasts. of the NBA. â‹¯ sports lovers.
LLM-diversified profiles
(a) Profiles for Rec. (b) Collaborative Language Models with Contrastive Learning (c) Profile Diversification
ğ‘¡ğ‘¡
Figure2:Theoverallframeworkofourproposedcollaborativeinformation-guidedlanguagemodelEasyRec.
representationseğ‘¢ andeğ‘– usingalanguagemodel,asshownas: andcollaborativeaspectsoftheitemsinaunifiedtextualprofile.
eğ‘¢ =LM(Pğ‘¢), eğ‘– =LM(Pğ‘–). (1) 3.1.1 ItemProfiling. Giventherawiteminformation,suchas
Theinteractionlikelihoodğ‘
ğ‘¢,ğ‘–
betweenauserğ‘¢andanitemğ‘– titleâ„ ğ‘–,categoriesğ‘ ğ‘–,anddescriptionğ‘‘ ğ‘– (e.g.,booksummary),we
aim to generate a comprehensive item profile Pğ‘– that captures
canbecalculatedasthecosinesimilaritybetweentheirrespective
boththesemanticandcollaborativeaspects.Toreflectuser-item
textembeddingseğ‘¢ andeğ‘–,i.e.,ğ‘ ğ‘¢,ğ‘– =ğ‘ğ‘œğ‘ (eğ‘¢,eğ‘–).Wecanthenrec-
interactions,weincorporatethetextualinformation(e.g.,posted
ommendtotheuserthetop-ğ‘˜uninteracteditemswiththehighest
reviewsğ‘Ÿ ğ‘¢,ğ‘–)providedbytheitemâ€™scorrespondingusers.Formally,
similarityscores,yieldingapersonalizedrecommendationset.
theitemprofilegenerationprocessis:
Rğ‘¢ =top-kğ‘–âˆˆI\Nğ‘¢ğ‘ğ‘œğ‘ (eğ‘¢,eğ‘–). (2) Pğ‘– =LLM(Mğ‘–,â„ ğ‘–,ğ‘ ğ‘–,{ğ‘Ÿ ğ‘¢,ğ‘–}) orLLM(Mğ‘–,â„ ğ‘–,ğ‘‘ ğ‘–), (4)
Here,Mğ‘– representsthegenerationinstruction,whileâ„ ğ‘– andğ‘‘ ğ‘–
Text-enhancedCollaborativeFiltering.Collaborativefiltering
serveasinput.Byleveraginglargelanguagemodels(LLMs),we
(CF)isawidelyusedrecommendersystemapproachthatleverages
cangenerateaconciseyetinformativeitemprofilePğ‘–.
thecollaborativerelationshipsamongusersanditems.Thisexisting
CFparadigmcanbeenhancedbyintegratingencodedsemanticrep- 3.1.2 User Profiling. In practical scenarios, privacy concerns
resentations.Typically,thelikelihoodvalueğ‘ ğ‘¢,ğ‘– iscalculatedbased oftenlimitthefeasibilityofgeneratinguserprofilesbasedondemo-
ontheinteractiondata,i.e.,ğ‘
ğ‘¢,ğ‘–
=ğ‘“(ğ‘¢,ğ‘–,A),whereArepresents
graphicinformation.Instead,wecanprofileusersbyconsidering
theinteractiondata.Text-enhancedcollaborativefilteringbuilds theircollaborativerelationships,usingthegeneratedprofileinfor-
uponthisfoundationbyincorporatingtextualfeatureseencoded mationfromtheirinteracteditems.Thisapproachallowstheuser
bylanguagemodelsassupplementaryrepresentations.Thisinte- profilestoeffectivelycapturethecollaborativesignalsthatreflect
grationaimstofurtherimprovetherecommendationperformance theirpreferences.Formally,theuserprofilegenerationprocessis:
ofthetraditionalID-basedcollaborativefilteringframeworks.
Pğ‘¢ =LLM(Mğ‘¢,{â„ ğ‘–,Pğ‘–,ğ‘Ÿ ğ‘¢,ğ‘–|ğ‘– âˆˆNğ‘¢}). (5)
ğ‘ ğ‘¢,ğ‘– =ğ‘“(ğ‘¢,ğ‘–,A,eğ‘¢,eğ‘–). (3)
Here,Mğ‘¢ representstheinstructionforgeneratingtheuserprofile
usingalargelanguagemodel(LLM).Wesampleasetofinteracted
3 METHODOLOGY
itemsNğ‘¢ fromtheuserâ€™spurchasehistory.Wethencombinethe
In this section, we first discuss how we gather textual profiles userâ€™sfeedbackğ‘Ÿ ğ‘¢,ğ‘–withthepre-generateditemprofilesPğ‘–tocreate
forusersanditemsinrecommendersystems,whichareessential theuserâ€™stextdescriptionPğ‘¢,whichcapturestheirpreferences.
forpre-trainingandevaluatingourmodel.Next,weâ€™lldiveinto
the specifics of EasyRec and its training approach. Lastly, weâ€™ll 3.1.3 AdvantagesofCollaborativeProfiling. Ourcollaborative
introduceourmethodfordiversifyinguserprofiles,whichimproves useranditemprofilingframeworkofferstwokeyadvantagesfor
themodelâ€™sabilitytoadapttovarioussituations. real-worldrecommendationscenarioswhichareelaboratedas:
â€¢ CollaborativeInformationPreserved.Ourcollaborativepro-
3.1 CollaborativeUserandItemProfiling filingapproachgoesbeyondjusttheoriginaltextualcontent,
Inreal-worldrecommenders,theonlyavailableinformationmaybe also capturing the semantics of user/item characteristics and
rawtextdata,suchasproducttitlesandcategories,associatedwith theirinteractionpatterns.Byencodingtheserichprofilesintoa
theitems.Privacyconcernsoftenmakeitdifficulttocollectcom- sharedfeaturespaceusingarecommendation-orientedlanguage
prehensiveuser-sideinformation.Furthermore,directlyleveraging model,theresultingembeddingsofinteractedusersanditems
suchtextualinformationmayoverlookthecrucialcollaborative arebroughtclosertogether.Thisenablestherecommendersto
relationshipsneededforaccurateuserbehaviormodelingandpref- betteridentifyrelevantmatches,evenfor"zero-shot"usersand
erenceunderstanding.Toaddresstheselimitations,weproposeto items(thosewithoutpriorinteractions)whichareubiquitousin
generatetextualprofilesbyleveraginglargelanguagemodels(e.g., real-worldscenarios.Thesystemcanleveragethecollaborative
GPT,LLaMAseries)[25]toinjectcollaborativeinformationinto signalsencodedwithinthetext-basedprofilestomakebetter
thetextualprofiles.Thisallowsustocaptureboththesemantic recommendations,bridgingthegapforthesecold-startcases.
SuperviseACMConference,XXX,XXX XubinRenandChaoHuang
â€¢ FastAdaptationtoDynamicScenarios.Ouruseranditem / Pos. item for / Neg. items for both users Nice optimization
Bad optimization
profilingapproach,poweredbyrobustlanguagemodels,enables ğ‘–ğ‘–1 ğ‘–ğ‘–2 User in ğ‘¢ğ‘¢ t1he ğ‘¢ğ‘¢ f2eature space Recommendation rage
therecommendersystemtoeffectivelyhandlethetime-evolving
nature of user preferences and interaction patterns. The key
advantageisthatsimpleupdatestothetextualuserprofilescan ğ‘–ğ‘–1 ğ‘–ğ‘–2 ğ‘–ğ‘–1 ğ‘–ğ‘–2
seamlessly reflect shifts in user interests and behaviors. This
ğ‘–ğ‘–3 ğ‘–ğ‘–3
flexibilityandresponsivenessmakesourapproachwell-suited ğ‘–ğ‘–4 ğ‘–ğ‘–4
fordeployingrecommendersystemsindynamicenvironments
whereuserinterestscanevolveovertime.
In-bağ‘¢ğ‘¢tc1
h
ğ‘¢ğ‘¢2 In-batchğ‘¢ğ‘¢1 ğ‘¢ğ‘¢2
Trn. Data Trn. Data
ğ‘¢ğ‘¢1,pğ‘–ğ‘–o1 s.,ğ‘–ğ‘– ne4 g. ğ‘¢ğ‘¢1,ğ‘–ğ‘– p1 os, .[ğ‘–ğ‘–3 n, egğ‘–ğ‘– .4]
3.2 ProfileEmbedderwithCollaborativeLM (a) One-Neg. It{eğ‘¢ğ‘¢m2 ,pğ‘–ğ‘–e2r, Sğ‘–ğ‘–a3m}ple (b) Multi-Ne{gğ‘¢ğ‘¢. 2It,eğ‘–ğ‘–m2s, [pğ‘–ğ‘–e3r, Sğ‘–ğ‘–4a]m}ple
Figure3:Contrastivetuningofcollaborativelanguagemodel
Sofar,wehavegeneratedrichtextualprofilesforusersanditems,
empowers the model to learn rich representations that
movingbeyondconventionalID-basedembeddings.However,di-
closelyalignthetext-basedsemanticspacewiththeglob-
rectlyencodingthesetextualprofilesintolatentembeddingsfor
allycollaborativesignalsfromuserbehaviorpatterns.
makingrecommendationsmayhavetwokeylimitations:
fortheTransformerlayers:
â€¢ CapturingRecommendation-SpecificSemantics.Thetext-
basedembeddings,whileexpressive,maynotbeoptimizedfor {x [(0 C) LS],...xğ‘›(0) }=Tokenization({ğ‘¤ [CLS],...,ğ‘¤ ğ‘›}). (6)
thespecificsemanticsandrelationshipsmostrelevanttotherec-
ommendationtask.Forexample,considertwoitemprofiles:(1) Here,ğ‘¥(0) âˆˆ Rğ‘‘ istheembeddingretrievedfromtheembedding
â€œThisuserispassionateaboutadvancedAItechniques,focusing
tablecorrespondingtothetokens,andthe(0)superscriptindicates
ondeeplearningandAIresearchworksâ€.(2)â€œWithapassion
thatitistheinputtothe(0)-thlayerofthelanguagemodel.The
foradvancedAIdevelopment,thisuserdelvesintosciencefic- tokenizationprocessalsoaddspositionalembeddingstotheinitial
tionandAI-themednovelsâ€.Thoughtheprofilessharetextual embeddings.Thelanguagemodelthenencodesasequenceoffinal
similarityaboutAI,theirtargetaudiencesdiffer-thefirstcaters embeddings(oneforeachtoken):
t tho eA seIs pc ri oe fin lt ei sst ms, at yhe ovse erc lo on od kt no us ac ni c-fi edr ,e ra ed ce or ms. mD ei nre dc at tl iy one -n sc po ed ci in fig
c
{e[ğ¶ğ¿ğ‘†],...,eğ‘›}=Encoder({x [(0 C) LS],...xğ‘›(0) }), (7)
semantics.Refinementisneededtobetteralignembeddingswith whereEncoder(Â·)referstotheTransformer-basedencoder-only
thespecificcontextandrequirementsoftherecommendation languagemodel.Thekeyoperationintheencodingprocessisthe
system,beyondjusttextualsimilarity. self-attentionmechanism:
âˆš
â€¢ OverlookingHigh-OrderCollaborativeSignals.Whiletex- Attention(ğ‘„,ğ¾,ğ‘‰)=softmax(ğ‘„ğ¾ğ‘‡ / ğ‘‘)ğ‘‰ (8)
tualprofilesofferrichsemanticinformation,relyingsolelyon
w.r.t.ğ‘„ =ğ‘‹ğ‘Šğ‘„,ğ¾ =ğ‘‹ğ‘Šğ¾,ğ‘‰ =ğ‘‹ğ‘Šğ‘‰. (9)
them may cause us to overlook valuable high-order collabo-
rative patterns that emerge from complex user-item interac- Here,ğ‘‹ âˆˆ Rğ‘›Ã—ğ‘‘ representsthestackoftokenembeddings,and
tions[32,36].Thesehigher-ordersignals,suchastransitiveasso- ğ‘Šğ‘„/ğ¾/ğ‘‰
aretheparametermatricesthatmaptheseembeddings
ciationsandcommunity-levelpreferences,canprovideadditional
intoqueries,keys,andvalues.Thisself-attentionmechanismallows
insightsthatcomplementthetextualdataforuserpreference
eachtokentoaggregateinformationfromallothertokens,ensuring
learninginrecommendersystems.
that each token is informed about the entire sequence. Finally,
Toaddresstheselimitations,weproposeacollaborativelanguage
we select the first embedding e[CLS], which corresponds to the
[CLS]token,astherepresentativeembeddingfortheentireprofile.
modelingparadigmthatseamlesslyintegratesthestrengthsofthe
Thisembeddingisthenpassedthroughamulti-layerperceptronto
semanticrichnessoftheprofilesandthevaluablecollaborative
obtainthefinalencodedrepresentationğ‘’,asmentionedinEq.(1):
signalsencodedfromcomplexuser-iteminteractionbehaviors.
e=MLP(e[CLS])=LM(P). (10)
3.2.1 BidirectionalTransformerEncoderasEmbedder. We
Withtheseencodedtextembeddingseforeachuseranditem,we
leverageamulti-layerbidirectionalTransformerencoderasthe
canpredictthelikelihoodofinteractionusingcosinesimilarityand
embedder backbone, considering two key benefits: 1) Efficient
makerecommendationsasdescribedinEq.(2).
Encoding:Theencoder-onlyarchitecturefocusessolelyongen-
eratingeffectivetextrepresentations,enablingfasterinferencein 3.2.2 CollaborativeLMwithContrastiveLearning. Themo-
recommendationsystems.2)FlexibleAdaptation:Bybuildingon tivationbehindfine-tuningthecollaborativelanguagemodelus-
pre-trainedTransformermodels,wecanleveragetransferlearning ingcontrastivelearningistoeffectivelycaptureandincorporate
tooptimizetheembedderforspecificrecommendationtasks. high-ordercollaborativesignalsintotherecommendationmodel.
Letâ€™s consider a userâ€™s profile as a passage ofğ‘› words: P = TraditionalrecommendersusingBayesianPersonalizedRanking
ğ‘¤ 1,...,ğ‘¤ ğ‘›.Westartbyaddingaspecialtoken[CLS]atthebegin- (BPR)[27]optimizeencodedembeddingswithonlyonenegative
ningofthewordsequence.Thetokenizationlayerthenencodesthe itempertrainingsample.Thisapproachlimitsthemodelâ€™sability
inputsequenceintoinitialembeddings,whichserveastheinput tocapturecomplexglobaluser-itemrelationships.EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Thesupervisedcontrastivelossprovidesastrongalternativeto Table1:Statisticsofthedatasets,with"Avg.n"representing
traditionalmethods.Bytreatinginteracteduser-itempairsasposi- theaveragenumberofinteractionsperuser.Datasetsmarked
tiveviewsandnon-interactedpairsasnegatives,itbringsrelated withunderlinearefromdifferentplatforms.
itemembeddingsclosertogetherinthefeaturespace.Asshown
Datasets #Users #Items #Inters. Avg.n Density
inFigure3,contrastivelearningincorporatesabatchofnegatives,
allowingforacomprehensiveadjustmentoftheencodedfeature TrainData 124,732 67,455 802,869 6.44 -
space,whichenablesthemodeltocapturehigh-ordercollabora- -Arts 14,470 8,537 96,328 6.66 7.8ğ‘’-4
-Movies 17,397 8,330 120,255 6.91 8.3ğ‘’-4
tive relationships. To evaluate this, we conduct experiments in
-Games 16,994 9,370 134,649 7.92 8.5ğ‘’-4
Section4.2.3toassesstheimpactofdifferentlearningobjectives.
-Home 22,893 13,070 131,556 5.75 4.4ğ‘’-4
Specifically,thesupervisedcontrastivelossfortuningthelanguage -Electronics 26,837 14,033 165,628 6.17 4.4ğ‘’-4
modelcanbeexpressedasfollows: -Tools 26,141 14,155 154,453 5.91 4.2ğ‘’-4
âˆ‘ï¸
exp(cos(eğ‘¢,eğ‘– )/ğœ)
TestData 55,877 28,988 615,210 11.01 -
L con=âˆ’
(ğ‘¢,ğ‘– pos,ğ‘–
neg)log âˆ’(cid:205)
ğ‘šâˆˆN
negexp(cosp (o es ğ‘¢,eğ‘š)/ğœ), (11) -- SS tp eo ar mts 2 21 3, ,4 37 16
0
12 5, ,7 24 31
7
1 33 12 6, ,4 10 90
0
16 3. .1 57
6
24 .. 68ğ‘’ ğ‘’- -4
3
-Yelp 11,091 11,010 166,620 15.02 1.4ğ‘’-3
Here,ğœisatemperaturehyperparameterthatcontrolsthedegreeof
contrastivelearning,andN negrepresentsthesetofin-batchnega-
Table2:ThespecificsofEasyRecâ€™smodelvariantsdifferbased
tiveitems,includingthenegativeitemğ‘– negaswellasotheritemsin
ontheirmodelarchitectureandparametersizes.
thebatch,excludingthepositiveitemğ‘– pos.Additionally,webuildon
priorwork[12,16]andincorporateanauxiliarymaskedlanguage Model Layers Hiddensize Heads Params
modeling(MLM)lossL .Thistechniquerandomlymasksinput
mlm EasyRec-Small 6 768 12 82M
tokensandtrainsthemodeltopredictthem,whichhelpsstabilize
EasyRec-Base 12 768 12 125M
thetrainingprocessandenhancethemodelâ€™sgeneralizationability.
EasyRec-Large 24 1024 16 355M
Thefinaltrainingobjectiveisthecombinationofthesupervised
contrastivelossandtheMLMloss:
L=L con+ğœ†L mlm. (12)
H LLe Mre -, rP epğ‘¢/ hğ‘– rare sp edre pse rn ot fis leth s,e wo ir ti hgi ğ‘¡na inl dp ir co afi til ne, gw thhi ele nP umğ‘¢1 /âˆ’ bğ‘–ğ‘¡ erde on fo dt ie vs et rh sie
-
whereğœ†isahyperparameterthatcontrolsthebalancebetweenthe ficationsteps.Duringtraining,werandomlyselectaprofilefrom
theprofilesetfortheuseroritemineachbatchofuser-itempairs.
contrastivelearningandthemaskedlanguagemodelingloss.
4 EVALUATION
3.3 AugmentationwithProfileDiversification
ThissectionevaluatestheperformanceoftheproposedEasyRec
Thegoalofourprofilediversificationapproachistoenhancethe
frameworkinaddressingthefollowingresearchquestions(RQs):
modelâ€™s ability to generalize to unseen users and items. Repre-
sentingeachuseroritemwithasingleprofileinherentlylimits â€¢ RQ1:HoweffectivelydoestheproposedEasyRecperformin
thediversityoftherepresentations,whichcannegativelyimpact matchingunseenusersanditems(zero-shot)withintext-based
themodelâ€™sperformanceandgeneralization.Toaddressthis,we recommendationscenarios?
proposeaugmentingtheexistinguser/itemprofilestoallowfor â€¢ RQ2:HoweffectivelydoesEasyRecintegratewithandenhance
multipleprofilesperentity.Theseaugmentedprofilescapturethe variousrecommenderswithintext-basedcollaborativefiltering
samesemanticmeaning,suchasthepersonalizedinteractionpref- scenarios,leveragingitscapabilitiesasalanguageembedder?
erencesofusersorthevariedcharacteristicsofitems.Ourtwo â€¢ RQ3:Howeffectiveisourproposedprofilediversificationmech-
specificaugmentationmethodsintroducecontrolledvariationsin anismfordataaugmentationinimprovingtheperformanceof
theprofileswhilepreservingthecoresemanticmeaning. therecommendationlanguagemodel?
Inspiredbyself-instructionmechanisms[40,40],largelanguage
â€¢ RQ4:Howwellcanourproposedtext-basedEasyRecparadigm
models(LLMs)canbeleveragedtorephraseuseroritemprofiles
adapttoaccommodatechangesinusersâ€™dynamicpreferences?
whilepreservingtheirunderlyingmeaning.Thisallowsgenerating
multiplesemanticallysimilaryetdistinctlywordedprofilesfroma
4.1 ExperimentalSettings
singleinput.Applyingthisiterativerephrasingprocesscancreatea
diversesetofaugmentedprofiles,substantiallyexpandingtheavail- 4.1.1 Datasets. Toassessourproposedmodelâ€™scapabilityinen-
abletrainingdata.Thisdataaugmentationtechniqueisparticularly codinguser/itemtextualprofilesintoembeddingsforrecommen-
valuablewhentheoriginaldatasetislimited,astheLLM-generated dation,wecurateddiversedatasetsacrossvariousdomainsand
profilescanimprovemodelgeneralizationandrobustness. platforms.Aportionwasusedfortraining,whiletheremainder
Byleveraginglargelanguagemodelsforprofilediversification, servedastestsetsforzero-shotevaluation.Thedatasetstatistics
wecancreateasetofdiverseprofilesforeachuserğ‘¢anditemğ‘–. areshowninTable1.Duetothepagelimit,weplacethedetailof
dataresourcesaredescribedinAppendixA.2.1
{P}ğ‘¢ ={Pğ‘¢; Pğ‘¢1, Pğ‘¢2, ..., Pğ‘¢ğ‘¡ }, (13)
4.1.2 EvaluationProtocols. Weemploytwocommonlyused
{P}ğ‘– ={Pğ‘–; P ğ‘–1, P ğ‘–2, ..., P ğ‘–ğ‘¡ }. (14) ranking-basedevaluationmetrics,Recall@ğ‘ andNDCG@ğ‘,toACMConference,XXX,XXX XubinRenandChaoHuang
Table3:Text-basedrecommendationperformanceofvariouslanguagemodelsacrossdifferentdatasets.Thebestperformance
isindicatedinbold,whilethesecond-bestishighlightedwithunderline.Thescriptâˆ—denotessignificance(p<0.05).
Data Sports Steam Yelp
Methods Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20
ProprietaryModels
OpenAIv3-Small 0.0324 0.0444 0.0198 0.0230 0.0066 0.0119 0.0049 0.0068 0.0028 0.0056 0.0021 0.0031
OpenAIv3-Large 0.0300 0.0436 0.0180 0.0217 0.0070 0.0137 0.0049 0.0073 0.0029 0.0055 0.0023 0.0032
BaseSizeModels
BERT-Base 0.0015 0.0032 0.0008 0.0013 0.0015 0.0031 0.0011 0.0017 0.0009 0.0018 0.0006 0.0010
RoBERTa-Base 0.0121 0.0206 0.0065 0.0087 0.0041 0.0078 0.0031 0.0043 0.0018 0.0034 0.0014 0.0020
BART-Base 0.0088 0.0143 0.0048 0.0063 0.0033 0.0062 0.0024 0.0035 0.0014 0.0027 0.0012 0.0016
SimCSE-Base 0.0240 0.0341 0.0143 0.0170 0.0048 0.0091 0.0035 0.0050 0.0020 0.0039 0.0015 0.0022
BLaIR-Base 0.0251 0.0352 0.0145 0.0173 0.0041 0.0081 0.0030 0.0044 0.0025 0.0046 0.0019 0.0026
GTR-Base 0.0290 0.0417 0.0172 0.0206 0.0075 0.0136 0.0056 0.0077 0.0025 0.0052 0.0019 0.0029
BGE-Base 0.0315 0.0437 0.0194 0.0227 0.0094 0.0170 0.0069 0.0095 0.0029 0.0053 0.0022 0.0031
LargeSizeModels
BERT-Large 0.0011 0.0021 0.0006 0.0008 0.0019 0.0040 0.0014 0.0022 0.0008 0.0019 0.0007 0.0010
RoBERTa-Large 0.0039 0.0065 0.0021 0.0028 0.0027 0.0052 0.0021 0.0029 0.0012 0.0023 0.0010 0.0014
BART-Large 0.0114 0.0170 0.0064 0.0080 0.0036 0.0065 0.0026 0.0037 0.0017 0.0033 0.0014 0.0019
SimCSE-Large 0.0232 0.0328 0.0134 0.0160 0.0051 0.0095 0.0037 0.0052 0.0023 0.0044 0.0017 0.0025
BLaIR-Large 0.0227 0.0322 0.0133 0.0159 0.0057 0.0108 0.0041 0.0059 0.0027 0.0047 0.0019 0.0027
GTR-Large 0.0329 0.0446 0.0198 0.0229 0.0095 0.0168 0.0069 0.0094 0.0021 0.0042 0.0018 0.0025
BGE-Large 0.0324 0.0449 0.0196 0.0230 0.0089 0.0153 0.0066 0.0088 0.0025 0.0052 0.0020 0.0029
EasyRecSeries
EasyRec-Small 0.0186 0.0286 0.0108 0.0135 0.0097 0.0174 0.0070 0.0097 0.0022 0.0046 0.0017 0.0026
EasyRec-Base 0.0360 0.0518 0.0210 0.0253 0.0114 0.0203 0.0081 0.0112 0.0034 0.0063 0.0026 0.0037
EasyRec-Large 0.0396* 0.0557* 0.0236* 0.0279* 0.0129* 0.0225* 0.0093* 0.0127* 0.0034* 0.0065* 0.0026* 0.0037*
Improve â†‘20.36% â†‘24.05% â†‘19.19% â†‘21.30% â†‘35.79% â†‘32.35% â†‘39.13% â†‘33.68% â†‘17.24% â†‘16.07% â†‘13.04% â†‘15.63%
Table4:ComparisonofEasyRectrainingwithdifferentopti- BART[14];(ii)LanguageModelsforDenseRetrieval:SimCSE[7],
mizationobjectives(where"Contrast"standsforcontrastive). GTR[23],andBGE[38];(iii)Pre-trainedLanguageModelsforRec-
ommendation:BLaIR[12].Additionally,wealsocomparedagainst
Sports Yelp
Objective thestate-of-the-arttextembeddingmodelsprovidedbyOpenAI.
Recall@10 NDCG@10 Recall@10 NDCG@10
Thedetaileddescriptionsofthesebaselinemodelscanbefoundin
BPRLoss 0.0381 0.0226 0.0028 0.0021 AppendixA.3.Thisselectionoflanguagemodelscoversabroad
ContrastLoss 0.0395 0.0236 0.0034 0.0026
spectrum,fromgeneral-purposecontextualencoderstospecialized
modelstailoredfortaskslikedenseretrievalandrecommendation.
assessperformanceinbothtext-basedrecommendationandcollab-
orativefilteringscenarios.Specifically,wecomputethesemetrics
4.2.2 ResultAnalysis. Theoverallcomparisonofdifferentmod-
forğ‘valuesof10and20[10,11].Theevaluationisconductedusing
elsispresentedinTable3.Thisevaluationrevealsseveralnotewor-
theall-rankprotocol[10]withthepredictedpreferencescores.In
thyobservations,whichareoutlinedbelow:
thecontextoftext-basedrecommendation,particularlyfordatasets
containingmultipleLLM-diversifiedprofiles(asdiscussedinSec- â€¢ SuperiorityacrossDiverseDatasets.Ourevaluationconsis-
tion3.3),wecalculatethemetricsseparatelyğ‘¡ timesbasedondif- tently shows that the EasyRec outperforms all other models
ferentprofilepairs(Pğ‘¢1,...,ğ‘¡,P ğ‘–1,...,ğ‘¡ ).Subsequently,wecomputethe acrossthethreedatasetsspanningdifferentplatforms.Thispro-
videsstrongevidencefortheeffectivenessoftheEasyRec.We
meanvalueforeachmetrictoobtainacomprehensiveassessment.
attributetheseimprovementstotwokeyfactors:i)Byinjecting
Forthetrainingdatasets,weutilizethevalidationsplitforevalua-
collaborativesignalsintothelanguagemodels,weeffectively
tion,whileforthetestdatasets,weemploythetestsplit.
optimizedourEasyRecusingsupervisedcontrastivelearning
4.2 PerformanceComparisionforText-based withintherecommendationcontext.Thisapproachallowsthe
modeltoinherentlyencodeuseranditemtextembeddingsthat
Recommendation(RQ1)
arewell-suitedforrecommendationtasks.ii)Byintegratinga
Weevaluatetheperformanceofvariouslanguagemodels(LMs)for
diversearrayofdatasetsacrossmultiplecategoriesandutilizing
zero-shottext-basedrecommendationontheunseenSports,Steam,
dataaugmentationtechniquestoenrichthetextdescriptionsfor
andYelpdatasets.Thisapproachdirectlyleveragestheencodedem-
training,ourEasyRecexhibitsimpressivegeneralizationcapabil-
beddingsderivedfromuser/itemprofilestomakerecommendations,
ities,enablingittoeffectivelyhandleunseendata.
withoutanyadditionaltrainingonthetargetdatasets.
â€¢ ScalingLawInvestigationofEasyRecModel.Ourexperi-
4.2.1 BaselineMethodsandSettings. Forourcomparativeeval- mentsrevealedthatasthesizeoftheEasyRecmodelincreases
uation,weincludedadiversesetoflanguagemodelsastextembed- (from small to large), its performance consistently improves
ders:(i)GeneralLanguageModels:BERT[4],RoBERTa[21],and acrossallthreedatasets.Thisobservationreflectsascalinglaw,EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Table 5: Recommendation performance in text-enhanced
collaborativefiltering.Theexperimentwasconductedonthe 2.25
1.75 2.00
Steamdatasetwith5-runstoobtainthemeanresults. 2.20
1.95
1.70
Metric Recall NDCG small 1.90 base 2.15 large
@5 @10 @20 @5 @10 @20 2.10
0 1 2 3 0 1 2 3 0 1 2 3
Augmentation Count Augmentation Count Augmentation Count
ID-basedMethods
GCCF 0.0497 0.0826 0.1314 0.0555 0.0665 0.0830 (a)PerformanceonSteamdata
LightGCN 0.0518 0.0851 0.1349 0.0575 0.0686 0.0854
4.65
Text-enhancedGCCF 4.60 small 6.40 6.60 large
BERT 0.0500 0.0822 0.1313 0.0556 0.0663 0.0829 4.55 6.20
RoBERTa 0.0517 0.0848 0.1351 0.0573 0.0684 0.0854 6.40
4.50 6.00 base
BART 0.0529 0.0874 0.1383 0.0585 0.0701 0.0874
SimCSE 0.0529 0.0877 0.1395 0.0588 0.0706 0.0881 0 1 2 3 0 1 2 3 0 1 2 3
Augmentation Count Augmentation Count Augmentation Count
BLaIR 0.0535 0.0880 0.1392 0.0593 0.0708 0.0882
(b)PerformanceonYelpdata
GTR 0.0532 0.0873 0.1387 0.0592 0.0706 0.0880
BGE 0.0535 0.0875 0.1393 0.0591 0.0705 0.0881 Figure4:Performancewithrespecttodatasize."Augmenta-
EasyRec 0.0540 0.0881 0.1402 0.0597 0.0712 0.0888 tionCount"indicatesthenumberğ‘¡ ofdiversifiedprofiles.
Text-enhancedLightGCN â€¢ Comparedtothebackbonemethodsalone,theintegrationofthe
BERT 0.0518 0.0849 0.1347 0.0575 0.0684 0.0852 text-enhancedframeworkgenerallyimprovestheperformance
RoBERTa 0.0531 0.0867 0.1374 0.0587 0.0699 0.0870 forbothGCCFandLightGCN.Thisobservationhighlightsthe
BART 0.0540 0.0887 0.1407 0.0599 0.0715 0.0891
significanceofincorporatingtextmodalityintotherecommen-
SimCSE 0.0541 0.0891 0.1417 0.0602 0.0719 0.0898
dationparadigm,asitcanenhanceuseranditemprofiles.
BLaIR 0.0551 0.0897 0.1418 0.0609 0.0724 0.0901
â€¢ AmongthevariousLMstested,EasyRecconsistentlyachieves
GTR 0.0542 0.0894 0.1417 0.0601 0.0719 0.0896
thehighestperformanceinthetext-enhancedrecommenders.
BGE 0.0547 0.0891 0.1407 0.0603 0.0718 0.0893
EasyRec 0.0554 0.0908 0.1430 0.0614 0.0732 0.0908 This outcome not only illustrates the efficacy of EasyRec for
recommendationtasks,butalsoemphasizestheadvantagesof
incorporatingcollaborativeinformationintolanguagemodels.
where the modelâ€™s performance growth is directly correlated
withitssize.Furthermore,thisfindingeffectivelyreinforcesthe 4.4 EffectivenessofProfileDiversification(RQ3)
validityoftext-basedrecommendationsystems.Italsovalidates
Inthissection,weexaminetheimpactofdiversifyinguseranditem
ourapproachtotrainingthelanguagemodel,whichenablesit
profileswithlargelanguagemodels(LLMs)onmodelperformance.
tolearncollaborativesignalsfromanewperspective.
AsmentionedinSection3.3,weperformLLM-baseddiversifica-
4.2.3 ImpactofTrainingObjectives. Furthermore,weevaluate tionthreetimesontheoriginalgeneratedprofiles.Thisprocess
theimpactofdifferenttrainingobjectivesonthelanguagemodelâ€™s continuouslyincreasesthenumberofprofilesinthetrainingset.
learningprocess.Tothisend,weimplementedEasyRec-Largetrain- Toinvestigatewhetherdataaugmentationpositivelyaffectsmodel
ingusingBPRloss(i.e.,onenegativeitempertrainingsample)for performance,weconductexperimentswiththreevariantsofthe
comparisonwiththecontrastivelearningresults.Thisapproach EasyRecunderdifferentnumbersofdiversifiedprofiles.Theresults
allowsustodirectlyassesshowthechoiceoftrainingobjectiveinflu- areshowninFigure4,leadingtothefollowingkeyobservations:
encesmodelperformance.AsshowninTable4,theperformanceof â€¢ EffectivenessofProfileDiversification.Theincreaseinthe
themodeltrainedwithcontrastivelearninggenerallyoutperforms numberofdiversifiedprofiles(from0to3)enhancesmodelperfor-
thatofthemodeltrainedwithBPRloss.Thisoutcomehighlights mance,particularlyforlargermodels.Thisfindingunderscores
theeffectivenessofemployingcontrastivelearningtobetterincor- theeffectivenessofouraugmentationapproachusingLLMsfor
poratecollaborativeinformationintothelanguagemodels,thereby profilediversification,andemphasizesthesignificanceofincreas-
enhancingtheiroverallperformanceinrecommendationtasks. ingtrainingdataforimprovedoutcomes.
â€¢ ScalingRelationship:Thescalingexperimentsonbothmodel
4.3 PerformanceofText-enhancedCF(RQ2) sizeanddatasizerevealacrucialrelationshipthatinfluences
Inadditiontoourinvestigationofzero-shotrecommendationsce- model performance. This demonstrates that our approach of
narios,weexplorethepotentialofEasyRecasanenhancement trainingthelanguagemodelwithcollaborativesignalsfollows
whenintegratedwithCFmodels.Toassesstheeffectivenessof ascalinglaw,indicatingthatmodelperformancebenefitsfrom
variousLMsinCF,weemploytwowidelyusedID-basedmethods bothincreasedcapacityanddatavolume.Suchscalinglawsare
asbackbonemodels:GCCF[2]andLightGCN[10],whichwere vitalastheyprovideinsightsintohowmodelcapacityanddata
chosenfortheirproveneffectivenessandefficiency.Furthermore, availabilityinteract,guidingfutureresearchanddevelopment.
weutilizetheadvancedmodel-agnostictext-enhancedframework
4.5 ModelFastAdaptationCaseStudy(RQ4)
RLMRec[25]withcontrastivealignmenttoconductourinvesti-
gation.WecomparethelargeversionsofbothEasyRecandother AsmentionedinSection3.1.3,akeyadvantageofEasyRecisitsabil-
open-sourceLMs.ThekeyfindingsfromtheresultsinTable5are: itytoempowerrecommendersystemstoefficientlyadapttoshifts
)2e1Ã—(
02@llaceR
)3e1Ã—(
02@llaceR
)2e1Ã—(
02@llaceR
)3e1Ã—(
02@llaceR
)2e1Ã—(
02@llaceR
)3e1Ã—(
02@llaceRACMConference,XXX,XXX XubinRenandChaoHuang
EasyRec Feature Space personalization[46].Forinstance,thegraphcollaborativefiltering
This user used to be a network[20]hasbeenintroducedtoaggregatebothcommonand
basketball fan. But now EasyRec
he likes swimming. domain-specificuserfeaturesusingGraphNeuralNetworks(GNNs)
Only thatcapturehigh-orderuser-itemconnections.Recentstudieshave
PreferenceShift Edit
furtherenrichedcross-domainrecommendersystemsthroughthe
Profile
integrationofself-supervisedlearningtechniques.Forexample,
This user is a basketball fan
and likes to play basketball EasyRec C2DSR[1]utilizescontrastivelearningwithbothsingle-domainand
and watch NBA games.
cross-domainrepresentations.CCDR[39]proposesintra-domain
andinter-domaincontrastivelearningtoenhancerecommendation
Matched Top Items (Before) Matched Top Items (After)
performance.Moreover,SITN[29]employsself-attentionmodules
1. Spalding NBA Zi/O Excel Basketball 1. IspeedMenâ€™s Competition Jammer Swimsuit
2. Spalding NBA Street Basketball 2. Speedo Swedish Two-Pack Swim Goggles assequenceencoderstorepresenttwouser-specificsequencesfrom
3. Spalding NBA Zi/O Indoor/Outdoor Basketballâ€“ 3. FINIS Foam Pull Buoy
Official Size 7 4. Speedo Menâ€™s Endurance+ Polyester Solid thesourceandtargetdomains,followedbycontrastivelearning
4. Spalding NBA Tack Soft Basketball Jammer Swimsuit
betweenthem.However,asignificantlimitationincurrentcross-
Figure5:Casï¿½eï¿½ï¿½ studyonhandlinguserpreï¿½fï¿½ï¿½erenceshift. domainresearchisitsrelianceoncorrelations(e.g.,overlapping
users) between source and target data, which constrains its ap-
inuserpreferencesandbehaviordynamicsovertime.Toevaluate plicabilityandgeneralizability.Incontrast,text-basedzero-shot
thiscapability,wecreatetwouserprofilesreflectingshiftedprefer- learningdoesnothassuchconstraint,allowingforeffectivetrans-
encesontheAmazon-Sportdatasetandexaminetherecommended ferevenacrossdifferentdatasets.OurproposedEasyRectakesa
itemsfromtheEasyRec.AsshowninFigure5,theoriginaluser fundamentalstepforwardinthislineofresearch.
profileindicatesthattheuserenjoysplayingbasketball.However, GraphCollaborativeFilteringforRecommendation.Graph
theuserâ€™spreferencelatertransitionstoapreferenceforswimming. NeuralNetworkshaverecentlyemergedasapromisingapproach
Wevisualizealltheencodedembeddingsusingt-SNE[31],which forrecommendationsystems,enablingthemodelingofcomplex
revealsasignificantshiftintheuserembeddingswithinthefea- user-iteminteractionsandcapturinghigh-orderdependenciesin
turespace.Correspondingly,recommendeditemstransitionfrom recommendation[6].Thesemodelsarecapableoflearningrep-
basketball-relatedproductstoswimminggear,reflectingtheuserâ€™s resentationsofusersanditemsbyaggregatingtheirinteractions
changingpreferences.Notably,thisadjustmentisaccomplished inagraphstructure,suchasPinSage[43],NGCF[32],andLight-
solelybymodifyingtheuserâ€™sprofile,withouttheneedforfurther GCN[10].ToenhancetherepresentationcapacityofGNNsagainst
trainingofthemodel.Thisunderscorestheefficiencyandflexibility datasparsityinrecommendersystems,furtherstudiesrealizethe
ofourapproachinadaptingtoevolvinguserpreferences. marriagebetweenself-supervisedlearningandcollaborativefilter-
ingwithdataaugmentation.ExamplesincludeSGL[34],SimGCL[44],
5 RELATEDWORK
andHCCF[37].Thesedevelopmentsshowcasethepotentialofself-
LMs-PoweredRecommenderSystems.Recentadvancements supervised graph learning in powering recommenders through
inrecommendersystemsincreasinglyincorporatetextualmodal- graphaugmentationbasedonnodeself-discrimination.
ities [45], thus enhancing traditional approaches. The semantic
representationsencodedbypre-trainedlanguagemodelsareessen- 6 CONCLUSION
tialfeaturesforimprovingrecommendermodels,particularlyin
TheEasyRecframeworkeffectivelyintegratesLMstoenhancerec-
click-throughrateprediction[8,35]andtransferablesequencerec-
ommendationtasks.Ournewparadigm,whichisbothstraightfor-
ommendations[13].Theseembeddingscaptureinformativecontent
wardandeffective,hasconsistentlyproventoexcelacrossvari-
relevanttorecommendationtasks[28].Someworksalsoleverage
ousscenarios,includingtext-basedzero-shotrecommendationand
text-basedagentstoenhanceperformance[47,48].Anotablere-
text-enhancedcollaborativefiltering.AttheheartofEasyRecâ€™ssuc-
centcontributionisRLMRec[25],atext-enhancedframeworkthat
cessliesaninnovativemethodologythatcombinescollaborative
improvesID-basedrecommendersusingprinciplesfrominforma-
languagemodeltuningwiththetransformativecapabilitiesofcon-
tiontheory.However,manypriorstudieshavereliedongeneral
trastivelearning.ThisuniqueapproachhasempoweredEasyRec
textembeddings,suchasBERT-basedmodels[5,45]orproprietary
to capture nuanced semantics and high-order collaborative sig-
OpenAIembeddings[25],ratherthanthosespecificallytailored
nalsâ€”criticalelementsthathavebeeninstrumentalindrivingre-
for recommendation purposes. Arecent work BLaIR [12] lever-
markableimprovementsinrecommendationperformance.Ourex-
agestheitemmetadataandinteraction-leveluserfeedbackonthis
tensiveexperiments,whichspanadiversearrayofdatasets,have
itemforLMstraining,yieldingpromisingresultsinquery-based
consistently validated the superiority of EasyRec over existing
itemretrieval.Incontrast,ourapproachassignseachuseranditem
languagemodels.Thisrobustandgeneralizedperformanceunder-
acollaborativelygeneratedprofilethatreflectstheirpreferences.
scorestheframeworkâ€™sremarkablecapacitytoadapttodynamic
EasyRecoptimizesthelearnedcorrelationsbetweenuseranditem
userpreferences,makingitwell-suitedforreal-worldindustrysce-
entitiesusingCFsignals,whichnotonlydemonstratesimpressive
narios.Furthermore,theconsistentimprovementsobservedacross
zero-shotperformancebutalsoenhancestext-augmentedresults.
differentsettingsindicatethatEasyRecisnotonlyeffectivebut
Cross-DomainRecommendation.Thefundamentalconceptbe-
alsoversatileinitsapplication.Lookingahead,thepotentialfor
hindcross-domainrecommendationistoenhancerecommenda-
EasyRectobeseamlesslyintegratedwithmulti-modalinformation
tionsinonedomainbyleveragingdatafromanotherdomain,which
presentsanenticingfrontierforourfutureinvestigations.
istypicallymoreabundant,toaddressdatasparsityandimproveEasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
REFERENCES
forrecommendation.InWWW.3464â€“3475.
[1] JiangxiaCao,XinCong,JiaweiSheng,TingwenLiu,andBinWang.2022.Con- [26] XubinRen,LianghaoXia,YuhaoYang,WeiWei,TianleWang,XuhengCai,and
trastiveCross-DomainSequentialRecommendation.InCIKM.138â€“147. ChaoHuang.2024.Sslrec:Aself-supervisedlearningframeworkforrecommen-
[2] LeiChen,LeWu,RichangHong,KunZhang,andMengWang.2020.Revisiting dation.InWSDM.567â€“575.
graphbasedcollaborativefiltering:Alinearresidualgraphconvolutionalnetwork [27] SteffenRendle,ChristophFreudenthaler,ZenoGantner,andLarsSchmidt-Thieme.
approach.InAAAI,Vol.34.27â€“34. 2012.BPR:Bayesianpersonalizedrankingfromimplicitfeedback.arXivpreprint
[3] MaurizioFerrariDacrema,IvÃ¡nCantador,IgnacioFernÃ¡ndez-TobÃ­as,Shlomo arXiv:1205.2618(2012).
Berkovsky,andPaoloCremonesi.2012.Designandevaluationofcross-domain [28] LehengSheng,AnZhang,YiZhang,YuxinChen,XiangWang,andTat-Seng
recommendersystems.InRecommenderSystemsHandbook.Springer,485â€“516. Chua.2024.LanguageModelsEncodeCollaborativeSignalsinRecommendation.
[4] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert: arXivpreprintarXiv:2407.05441(2024).
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv [29] GuoqiangSun,YibinShen,SijinZhou,XiangChen,HongyanLiu,ChunmingWu,
preprintarXiv:1810.04805(2018). ChenyiLei,XianhuiWei,andFeiFang.2023.Self-SupervisedInterestTransfer
[5] HaoDing,YifeiMa,AnoopDeoras,YuyangWang,andHaoWang.2021.Zero- NetworkviaPrototypicalContrastiveLearningforRecommendation. arXiv
shotrecommendersystems.arXivpreprintarXiv:2105.08318(2021). preprintarXiv:2302.14438(2023).
[6] ChenGao,YuZheng,NianLi,YinfengLi,YingrongQin,JinghuaPiao,Yuhan [30] JianingSun,ZhaoyueCheng,SabaZuberi,FelipePÃ©rez,andMaksimsVolkovs.
Quan,JianxinChang,DepengJin,XiangnanHe,etal.2023.Asurveyofgraph 2021.Hgcf:Hyperbolicgraphconvolutionnetworksforcollaborativefiltering.
neuralnetworksforrecommendersystems:Challenges,methods,anddirections. InWWW.593â€“601.
ACMTORS1,1(2023),1â€“51. [31] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdatausingt-SNE.
[7] TianyuGao,XingchengYao,andDanqiChen.2021.Simcse:Simplecontrastive JMLR9,11(2008).
learningofsentenceembeddings.arXivpreprintarXiv:2104.08821(2021). [32] XiangWang,XiangnanHe,MengWang,FuliFeng,andTat-SengChua.2019.
[8] BinzongGeng,ZhaoxinHuan,XiaoluZhang,YongHe,LiangZhang,FajieYuan, Neuralgraphcollaborativefiltering.InSIGIR.165â€“174.
JunZhou,andLinjianMo.2024.Breakingthelengthbarrier:Llm-enhancedCTR [33] YinweiWei,XiangWang,QiLi,LiqiangNie,YanLi,XuanpingLi,andTat-Seng
predictioninlongtextualuserbehaviors.InSIGIR.2311â€“2315. Chua.2021.Contrastivelearningforcold-startrecommendation.InMM.5382â€“
[9] BowenHao,JingZhang,HongzhiYin,CuipingLi,andHongChen.2021.Pre- 5390.
traininggraphneuralnetworksforcold-startusersanditemsrepresentation.In [34] JiancanWu,XiangWang,FuliFeng,XiangnanHe,LiangChen,JianxunLian,and
WSDM.265â€“273. XingXie.2021.Self-supervisedgraphlearningforrecommendation.InSIGIR.
[10] XiangnanHe,KuanDeng,XiangWang,YanLi,YongdongZhang,andMeng 726â€“735.
Wang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor [35] YunjiaXi,WeiwenLiu,JianghaoLin,XiaolingCai,HongZhu,JiemingZhu,Bo
recommendation.InSIGIR.639â€“648. Chen,RuimingTang,WeinanZhang,RuiZhang,etal.2023.Towardsopen-world
[11] XiangnanHe,LiziLiao,HanwangZhang,LiqiangNie,XiaHu,andTat-Seng recommendationwithknowledgeaugmentationfromlargelanguagemodels.
Chua.2017.Neuralcollaborativefiltering.InWWW.173â€“182. arXivpreprintarXiv:2306.10933(2023).
[12] YupengHou,JiachengLi,ZhankuiHe,AnYan,XiusiChen,andJulianMcAuley. [36] LianghaoXia,ChaoHuang,ChunzhenHuang,KangyiLin,TaoYu,andBen
2024. Bridginglanguageanditemsforretrievalandrecommendation. arXiv Kao.2023.Automatedself-supervisedlearningforrecommendation.InWWW.
preprintarXiv:2403.03952(2024). 992â€“1002.
[13] YupengHou,ShanleiMu,WayneXinZhao,YaliangLi,BolinDing,andJi-Rong [37] LianghaoXia,ChaoHuang,YongXu,JiashuZhao,DaweiYin,andJimmyHuang.
Wen.2022.Towardsuniversalsequencerepresentationlearningforrecommender 2022.Hypergraphcontrastivecollaborativefiltering.InSIGIR.70â€“79.
systems.InKDD.585â€“593. [38] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighof.2023.C-pack:
[14] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,Abdelrahman Packagedresourcestoadvancegeneralchineseembedding. arXivpreprint
Mohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.2019.Bart:Denoising arXiv:2309.07597(2023).
sequence-to-sequencepre-trainingfornaturallanguagegeneration,translation, [39] RuobingXie,QiLiu,LiangdongWang,ShukaiLiu,BoZhang,andLeyuLin.2022.
andcomprehension.arXivpreprintarXiv:1910.13461(2019). Contrastivecross-domainrecommendationinmatching.InKDD.4226â€“4236.
[15] JuanhuiLi,HaoyuHan,ZhikaiChen,HarryShomer,WeiJin,AminJavari,and [40] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,
JiliangTang.2024. EnhancingIDandTextFusionviaAlternativeTrainingin ChongyangTao,andDaxinJiang.2024.Wizardlm:Empoweringlargelanguage
Session-basedRecommendation.arXivpreprintarXiv:2402.08921(2024). modelstofollowcomplexinstructions.InICLR.
[16] JiachengLi,MingWang,JinLi,JinmiaoFu,XinShen,JingboShang,andJulian [41] ShuyuanXu,YingqiangGe,YunqiLi,ZuohuiFu,XuChen,andYongfengZhang.
McAuley.2023. Textisallyouneed:Learninglanguagerepresentationsfor 2023.Causalcollaborativefiltering.InSIGIR.235â€“245.
sequentialrecommendation.InKDD.1258â€“1267. [42] MenglinYang,MinZhou,JiahongLiu,DefuLian,andIrwinKing.2022.HRCF:
[17] JiayiLiao,SihangLi,ZhengyiYang,JiancanWu,YanchengYuan,XiangWang, Enhancingcollaborativefilteringviahyperbolicgeometricregularization.In
andXiangnanHe.2024. Llara:Largelanguage-recommendationassistant.In WWW.2462â€“2471.
SIGIR.1785â€“1795. [43] RexYing,RuiningHe,KaifengChen,PongEksombatchai,WilliamLHamilton,
[18] XixunLin,JiaWu,ChuanZhou,ShiruiPan,YananCao,andBinWang.2021. andJureLeskovec.2018. Graphconvolutionalneuralnetworksforweb-scale
Task-adaptiveneuralprocessforusercold-startrecommendation.InWWW. recommendersystems.InKDD.974â€“983.
1306â€“1316. [44] JunliangYu,HongzhiYin,XinXia,TongChen,LizhenCui,andQuocVietHung
[19] ZihanLin,ChangxinTian,YupengHou,andWayneXinZhao.2022.Improving Nguyen.2022.Aregraphaugmentationsnecessary?simplegraphcontrastive
graphcollaborativefilteringwithneighborhood-enrichedcontrastivelearning. learningforrecommendation.InSIGIR.1294â€“1303.
InWWW.2320â€“2329. [45] ZhengYuan,FajieYuan,YuSong,YouhuaLi,JunchenFu,FeiYang,Yunzhu
[20] MengLiu,JianjunLi,GuohuiLi,andPengPan.2020.Crossdomainrecommen- Pan,andYongxinNi.2023.Wheretogonextforrecommendersystems?id-vs.
dationviabi-directionaltransfergraphcollaborativefilteringnetworks.InCIKM. modality-basedrecommendermodelsrevisited.InSIGIR.2639â€“2649.
885â€“894. [46] TianziZang,YanminZhu,HaobingLiu,RuohanZhang,andJiadiYu.2022.A
[21] YinhanLiuetal.2019.Roberta:Arobustlyoptimizedbertpretrainingapproach. surveyoncross-domainrecommendation:taxonomies,methods,andfuture
arXivpreprintarXiv:1907.11692(2019). directions.ACMTOIS41,2(2022),1â€“39.
[22] JianmoNi,JiachengLi,andJulianMcAuley.2019.Justifyingrecommendations [47] AnZhang,YuxinChen,LehengSheng,XiangWang,andTat-SengChua.2024.
usingdistantly-labeledreviewsandfine-grainedaspects.InEMNLP-IJCNLP. Ongenerativeagentsinrecommendation.InSIGIR.1807â€“1817.
188â€“197. [48] JunjieZhang,YupengHou,RuobingXie,WenqiSun,JulianMcAuley,WayneXin
[23] JianmoNi,ChenQu,JingLu,ZhuyunDai,GustavoHernÃ¡ndezÃbrego,JiMa, Zhao,LeyuLin,andJi-RongWen.2024.Agentcf:Collaborativelearningwith
VincentYZhao,YiLuan,KeithBHall,Ming-WeiChang,etal.2021.Largedual autonomouslanguageagentsforrecommendersystems.InWWW.3679â€“3689.
encodersaregeneralizableretrievers.arXivpreprintarXiv:2112.07899(2021). [49] YangZhang,FuliFeng,JizhiZhang,KeqinBao,QifanWang,andXiangnanHe.
[24] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory 2023.Collm:Integratingcollaborativeembeddingsintolargelanguagemodels
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019. forrecommendation.arXivpreprintarXiv:2310.19488(2023).
Pytorch:Animperativestyle,high-performancedeeplearninglibrary.NeurIPS [50] YidingZhang,ChaozhuoLi,XingXie,XiaoWang,ChuanShi,YumingLiu,Hao
32(2019). Sun,LiangjieZhang,WeiweiDeng,andQiZhang.2022.Geometricdisentangled
[25] XubinRen,WeiWei,LianghaoXia,LixinSu,SuqiCheng,JunfengWang,Dawei collaborativefiltering.InSIGIR.80â€“90.
Yin,andChaoHuang.2024.RepresentationlearningwithlargelanguagemodelsACMConference,XXX,XXX XubinRenandChaoHuang
Table6:Costsforuseranditemprofilegenerationanddiver- Theinstructionpromptsforgenerationthatweutilizedreference
sification,includingthreeiterationsofdiversification. previousresearch[25]toconductefficientuseranditemprofiling.
Theprofilingprocessadoptsanitem-to-userparadigm,wherewe
Operation #Data #In.Tokens #Out.Tokens #Cost($)
firstgenerateitemprofilesinparallelusingmulti-threadprocessing,
Generation 7 169M 20M âˆ¼114 followedbytheparallelgenerationofuserprofilesthatincorporate
Diversification 9 102M 30M âˆ¼97 collaborativeinformation.Thelargelanguagemodelemployedis
GPT-3.5-TurbofromOpenAI.
Specifically,weprocessdatasetsfromAmazonreviewdata[22],
A APPENDIX
whichincludethecategoriesArts,Movies,Games,Home,Electron-
A.1 ImplementationandTrainingDetails ics,Tools,andSports.Foritemprofilegeneration,eachitemğ‘–
WeimplementedourEasyRecandconductedallexperimentsus- includesatitleâ„ ğ‘– andanoriginaldescriptionğ‘‘ ğ‘–.Weleveragethese
ing PyTorch [24]. For the transformer-based encoder backbone, twopiecesofinformationtogeneratetheprofile,asdescribedon
weadoptedthearchitectureofRoBERTa[21]andutilizeditspre- theleft-handsideofEq.4.Next,foruserprofilegeneration,we
trainedparametersasinitialization.Wetrainedthreeversionsof uniformlysampleamaximumoffiveinteracteditemsfromeach
EasyRecwithvaryingparametersizes(small,base,andlarge),as userâ€™sbehaviorhistoryascollaborativeinformation.Wethenar-
detailedinTable2.Forthelossfunction,wesetthehyperparam- rangetheinputpromptforthelargelanguagemodelaccording
etersğœ to0.05andğœ†to0.1.Thetokenmaskingratioformasked toEq.5.Allpromptsandinstructionsforuseranditemprofile
languagemodelingis0.15,andthelearningrateissetto5Ã—10âˆ’5. generationareprovidedinthecodeforreference.
Wetrainthemodelfor25epochs.Forprofileaugmentation,weset
A.2.3 DetailsofProfileDiversification. AsdescribedinSec-
thediversificationtimetforLLM-basedmethodsto3,resulting
tion3.3,wealsoconductprofilediversificationusinglargelanguage
in4userprofilesand5itemprofilesperdataset.Duringtraining,
models(LLMs)toenhancethediversityofthetrainingandtest
weevaluatethemodelevery1000stepsandusethevalidationin-
datasets,therebyimprovingandbetterevaluatingthemodelâ€™sgen-
teractionsfromeachtrainingdatasettoselecttheoptimalmodel
eralizationabilityacrossdifferentuseranditemprofiles.Foreach
parameters,employingtheRecall@20metric.Detailedimplemen- useroritem,weperformğ‘¡iterationsofdiversificationstartingfrom
tationofourmodelisprovidedinouranonymousreleasedcode.
theinitiallygeneratedprofile.Thismeansthatweobtainthefirst
diversifiedprofilebasedontheoriginalprofileandthenusethis
A.2 DatasetsandUser/ItemProfiles
diversifiedprofileforfurtherdiversificationwiththeLLMs.
Inthissection,weprovidedetailedinformationonthedatasets, Forreference,examplesofuseranditemprofilediversification
aswellastheprocessesforprofilegenerationanddiversification, areprovidedinFigureAandFigureA,respectively.Asillustratedin
includinginstructions,examples,andassociatedcosts. thecaseofuserprofilediversification,theprofilesforthesameuser
differatthewordlevelwhilestillrepresentingthesamepreferences.
A.2.1 DetailsofDataset. WeutilizedatasetsfromAmazonre-
Suchdiversificationsignificantlyenhancesthediversityandquality
viewdata[22]acrosssixcategoriestoformthetrainingdata:Arts,
ofthetextualdatawhilealsoincreasingtheoveralldatasetsize.
CraftsandSewing(Arts),MoviesandTV (Movies),VideoGames
(Games),HomeandKitchen(Home),Electronics(Electronics),and A.2.4 CostofGenerationandDiversification. Wesummarize
ToolsandHomeImprovement(Tools).Forthetestdatasets,weuse thetotalnumberoftokensandtheassociatedcostsforutilizing
onedomain,SportsandOutdoors(Sports),fromtheAmazonre- theproprietarymodelforprofilegenerationanddiversification
viewdata,alongwithtwocross-platformdatasets:SteamandYelp, inTable6.ItisworthnotingthattheprofilesfortheSteamand
forcomprehensiveevaluation.ForthedatasetsfromtheAmazon Yelpdatasetshavealreadybeengenerated;therefore,thenumber
platform,wefirstfilterthedatatoincludeonlythosewitharating ofprofileddatasetsanddiversifieddatasetsdiffers.Asshownin
scoregreaterthan3andapplya10-corefilteringtodensifythe theresults,thetotalnumberoftokensrequiredtoprocessboth
dataset.Subsequently,foreachcategory,wesplittheinteractions profilegenerationanddiversificationisapproximately322million,
intotraining,validation,andtestsplitsinaratioof8:1:1.Incontrast, includingbothinputandoutputtokens.Thisgenerallyincursa
fortheSteamandYelpdatasets,wedirectlyusethedataprocessed costofaround200dollarswithGPT-3.5-TurboAPItoprocessthe
inpreviouswork[25],whichmaintainsasplitratioof3:1:1. entiredataset,makingitanaffordableoption.
A.2.2 DetailsofProfileGeneration. Afterdataprocessing,each
A.3 DetailsofText-basedRecommendation
datasetcontainsasplitoftraininginteractions.Weusetheseinter-
actionstogenerateuseranditemprofilesfollowingtheparadigm A.3.1 BaselineModels. Inthissection,weprovideadetailed
describedinSection3.1,asthisrequiresuser-iteminteractionin- descriptionofthelanguagemodelscomparedinthiswork.
formation.FortheSteamandYelpdatasets,wedirectlyusethe (i)GeneralLanguageModels.
providedprofiles,whichadheretothesamegenerationprotocol.It â€¢ BERT[4]:Alandmarktransformer-basedmodelrenownedfor
isimportanttonotethat,foreachdataset,theprofilesaregener- stronglanguageunderstandingthroughbidirectionaltraining.
atedexclusivelybasedonthetraininginteractions.Thisapproach WeusethepooledBERToutputasthetextembedding.
ensuresthatthevalidationandtestinteractionsarereservedfor â€¢ RoBERTa[21]:AnoptimizedBERTthatemploysdynamicmask-
evaluationpurposes,preventingdataleakageandallowingfora ingandlargerdatasets.Weusethefinal[CLS]tokenembedding.
more accurate assessment of the modelâ€™s generalization perfor-
manceonunseenrecommendationdata.EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
User Profile Diversification
Instruction
You will assist me in revising a userâ€™s profile while maintaining its original meaning. I will present you
with the userâ€™s initial profile.
Instructions:
USER PROFILE: The original user profile.
Requirements:
1. Please provide the revised profile directly begin with "REVISED PROFILE: ".
2. The rephrased profile should minimize duplication with the original text while preserving its intended
meaning.
3. The revised profile should exhibit varied sentence structures while faithfully conveying the original
profileâ€™s essence.
InputPrompt
USER PROFILE: This user is likely to enjoy items related to baking, entertaining, and colorful table settings.
They appreciate convenience, efficiency, and practicality in kitchen appliances.
Response
REVISED PROFILE: An individual who finds pleasure in baking, hosting gatherings, and vibrant table
arrangements. They value kitchen devices that offer convenience, efficiency, and practicality.
Figure6:Anexampleoflargelanguagemodels-baseduserprofilediversification.
Table7:Detailsofcomparedlanguagemodels. â€¢ BGE[38]:BGEisastate-of-the-artfamilyofwell-trainedmodels
Model Pre-trainedWeights(FromHuggingFace) forgeneraltextembeddingmodels.WeutilizetheEnglishversion
ofthismodelforevaluation.
BERT-Base google-bert/bert-base-uncased
BERT-Large google-bert/bert-large-uncased (iii)LangaugeModelsforRecommendation.
BART-Base facebook/bart-base â€¢ BLaIR[12]:Aseriesofsentenceembeddingmodelsforrecom-
BART-Base facebook/bart-large mendation.BLaIRlearnsinteraction-levelcorrelationsbetween
RoBERTa-Base FacebookAI/roberta-base itemmetadataanduserfeedbackonthisitem,improvingquery-
RoBERTa-Large FacebookAI/roberta-large
baseditemretrievalandalsorecommendation.
SimCSE-Base princeton-nlp/sup-simcse-roberta-base
The pre-trained weights utilized for each baseline language
SimCSE-Large princeton-nlp/sup-simcse-roberta-large
BLaIR-Base hyp1231/blair-roberta-base model are listed in Table 7. For proprietary embedding models
BLaIR-Large hyp1231/blair-roberta-large namedOpenAIv3,weusethelatesttext-embedding-3-small
GTR-Base sentence-transformers/gtr-t5-base andtext-embedding-3-largefromOpenAIasbaselines.
GTR-Large sentence-transformers/gtr-t5-large
A.3.2 DetailofBaselineSettings. Giventhattheexperiment
BGE-Base BAAI/bge-base-en-v1.5
BGE-Large BAAI/bge-large-en-v1.5 wasconductedinazero-shotsetting,wherethetestdataremained
unseenduringtrainingforbothourEasyRecandtheotherlan-
guagemodelbaselines,wedirectlyutilizedtheoriginalreleased
â€¢ BART[14]:Adenoisingautoencodertransformertrainedon
parametersofthesebaselinesfromHuggingFaceforinitialization
corruptedtextreconstruction.Weapplymeanpoolingonthe
andcomparison.Besides,forBGE,weaddedtherecommended
lasthiddenstateforthetextembedding.
retrievalinstructioninfrontoftheuserprofiles,followingthepro-
(ii)LanguageModelsforDenseRetrieval.
videdguidelinesfromtheopen-sourcecode,aswefounditoffered
â€¢ SimCSE[7]:Aframeworkthatleveragescontrastivelearning betterperformanceinzero-shotrecommendations.Additionally,
togeneratehigh-qualitysentenceembeddings,enhancingthe weappliednormalizationtotheoutputsfromthelanguagemodels
modelâ€™sabilitytodiscernsemanticsimilaritybetweensentences. toderivetextembeddingsandcomputedcosinesimilarityusing
â€¢ GTR[23]:GTRisageneralizableT5-baseddenseretrieverthat thedotproductoftheuseranditemembeddings.
improvesretrievaltasksacrossvariousdomainsbyovercoming
limitationsoftraditionaldualencoders.ACMConference,XXX,XXX XubinRenandChaoHuang
Item Profile Diversification
Instruction
You will assist me in revising a itemâ€™s profile while maintaining its original meaning. I will present you
with the itemâ€™s initial profile.
Instructions:
ITEM PROFILE: The original item profile.
Requirements:
1. Please provide the revised profile directly begin with "REVISED PROFILE: ".
2. The rephrased profile should minimize duplication with the original text while preserving its intended
meaning.
3. The revised profile should exhibit varied sentence structures while faithfully conveying the original
profileâ€™s essence.
InputPrompt
ITEM PROFILE: The Innovee Lemon Squeezer is a high-quality stainless steel manual citrus press that comes
with a lemon recipes ebook. Ideal for those who enjoy fresh lemon juice and recipes.
Response
REVISED PROFILE: The Innovee Lemon Squeezer is a stainless steel manual citrus press that includes a lemon
recipes ebook, perfect for individuals who appreciate the taste of freshly squeezed lemon juice and love
trying out new recipes.
Figure7:Anexampleoflargelanguagemodels-baseditemprofilediversification.