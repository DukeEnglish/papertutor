MESA: Cooperative Meta-Exploration in Multi-Agent Learning
through Exploiting State-Action Space Structure
ZhichengZhangâˆ— YanchengLiangâˆ—
CarnegieMellonUniversity UniversityofWashington
Pittsburgh,Pennsylvania,UnitedStates Seattle,Washington,UnitedStates
zhichen3@cs.cmu.edu yancheng@cs.washington.edu
Yi Wu FeiFang
TsinghuaUniversity CarnegieMellonUniversity
Beijing,China Pittsburgh,Pennsylvania,UnitedStates
jxwuyi@gmail.com feif@cs.cmu.edu
ABSTRACT
Multi-agentreinforcementlearning(MARL)algorithmsoftenstrug-
gletofindstrategiesclosetoParetooptimalNashEquilibrium,ow-
inglargelytothelackofefficientexploration.Theproblemisex-
acerbatedinsparse-rewardsettings,causedbythelargervariance
exhibitedinpolicylearning.ThispaperintroducesMESA,anovel
meta-explorationmethodforcooperativemulti-agentlearning.It
learns toexplore by first identifying the agentsâ€™ high-rewarding
jointstate-actionsubspacefromtrainingtasksandthenlearning
asetofdiverseexplorationpoliciestoâ€œcoverâ€thesubspace.These Figure1:Illustrationofstructuredexplorationandunstruc-
trainedexplorationpoliciescanbeintegratedwithanyoff-policy tured exploration behavior in the 2-player climb game.
MARL algorithm for test-time tasks. We first showcase MESAâ€™s The rows and columns indicate the playersâ€™ action space.
advantageinamulti-stepmatrixgame.Furthermore,experiments While unstructured exploration aims to visit novel states,
showthatwithlearnedexplorationpolicies,MESAachievessignif- structuredexplorationexploitsstructuresinthejointstate-
icantlybetterperformanceinsparse-rewardtasksinseveralmulti- action space, helping agents coordinatedly and more effi-
agentparticleenvironmentsandmulti-agentMuJoCoenvironments, cientlyexplorethepotentialhigh-rewardsubspace.
andexhibitstheabilitytogeneralizetomorechallengingtasksat
testtime.
KEYWORDS
Theexplorationchallengehasbeenstudiedextensivelyandex-
Multi-AgentReinforcementLearning;Meta-Learning;Exploration istingworkscanbecategorizedmainlyintotwostreams.Onecore
Strategy ideawithgreatsuccessistoincentivizetheagenttovisittheunder-
exploredstatesmorefrequentlybyaddinganintrinsicrewardbased
ACMReferenceFormat:
onavisitationmeasure[3,25,28,37]orsomeotherheuristics[17,
ZhichengZhang,YanchengLiang,YiWu,andFeiFang.2024.MESA:Co-
operative Meta-ExplorationinMulti-AgentLearningthroughExploiting 39].
State-ActionSpaceStructure.InProc.ofthe23rdInternationalConference However,inmulti-agentsettings,duetotheexponentialgrowth
onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland, ofthe joint state-actionspace, simply visiting more novel states
NewZealand,May6â€“10,2024,IFAAMAS,15pages. canbeincreasinglyineffective.Explorationpoliciesneedtobetter
capture the low-dimensional structureof the tasks and leverage
1 INTRODUCTION
thestructuralknowledgeforhigherexplorationefficiency.
Reinforcement learning (RL) algorithms often adopt a trial-and- Anotherlineofworkspecificallylearnsexplorationstrategies.
errorlearningparadigmandoptimizethepolicybasedonthere- However, these works do not explicitly consider the underlying
ward signals given by the environment. The effectiveness of RL taskstructure.Forexample,Mahajanetal.conditionsthepolicyon
reliesonefficientexploration,especiallyinsparserewardsettings, asharedlatentvariable[24]learnedviamutualinformationmaxi-
asitiscriticaltogetsufficientexperienceswithhighrewardsto mization.Liuetal.adoptsagoal-conditionedexplorationstrategy
guidethetraining. bysettingstatefeaturesasgoals[21].Otherworksinthesingle-
agentsettings[6,26,35]learnexplorationpoliciesthroughapre-
âˆ—Equalcontribution.
definedintrinsicreward.Alltheseworkstraintheexplorationpol-
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSys- icyusingtask-agnosticexploration-specificrewards.InSection4,
tems(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6â€“ wewillpresentasimplematrixgametoshowthatpopularexplo-
10,2024,Auckland,NewZealand.Â©2024InternationalFoundationforAutonomous
rationmethodscanhavedifficultiesfindingtheoptimalsolution
AgentsandMultiagentSystems(www.ifaamas.org).Thisworkislicencedunderthe
CreativeCommonsAttribution4.0International(CC-BY4.0)licence. duetotherewardstructureofthegame.
4202
yaM
1
]GL.sc[
1v20900.5042:viXraHow canwe enable theagents to moreeffectively exploreby The key idea is to meta-learn a separate exploration policythat
leveragingtheintrinsicstructureoftheenvironment?Weadopta canbeusedinthetestingtask.Mostcloselyrelatedtoourworkis
meta-explorationframework(i.e.,learningtoexplore)forMARL: [26],whereanexplorationpolicyispretrainedonasetoftraining
wefirsttrainmultiplestructuredexplorationpoliciesfromasetof tasks.However,theirmethodisdesignedforthesingle-agentset-
trainingtasks(referredtoasthemeta-trainingstage),andthenuse tingandlearnstheexplorationpolicybyusingatask-agnosticin-
these exploration policies to facilitate agentsâ€™ learning in a test- trinsicrewardtoincentivizevisitationofinterestingstates,while
timetask,whichistypicallyanewtasksampledfromthetaskdis- we directly utilize the task reward to learn the structure of the
tribution(referred toasmeta-testingstage). Wedevelop amulti- environments.Otherexistingworksinmeta-explorationpropose
agent meta-explorationmethod,CooperativeMeta-Exploration in to learn a latent-space exploration policythat is conditioned on
Multi-AgentLearningthroughExploitingState-ActionSpaceStruc- ataskvariable,whichcanbeaccomplishedbymeta-policygradi-
ture (MESA) for fully cooperative settings. MESA leverages the ent[14,20,40],variationalinference[32]orinformationmaximiza-
taskstructuresbyexplicitlyidentifyingtheagentsâ€™high-rewarding tion[42]overthetrainingtasks.Therefore,attesttime,posterior
joint state-action subspace in the training tasks. It then trains a inferencecanbeperformedforthelatentvariabletowardsfastex-
setofdiverseexplorationpoliciestocoverthisidentifiedsubspace. plorationstrategyadaption.Ourapproachfollowsasimilarmeta-
Theexplorationpoliciesaretrainedwitharewardschemeinduced explorationparadigmbylearningadditionalexplorationpolicies.
bythedistancetothehigh-rewardingsubspace.Themeta-learned However,existingmeta-explorationmethodsfocusonthesingle-
explorationpoliciescanbecombinedwithanyoff-policyMARLal- agentsettingwhileweconsidermuchmorechallengingmulti-agent
gorithmduringthemeta-testingstagebyrandomlyselectinglearned gameswithadistributionofsimilarly-structuredtasks,forexam-
explorationpoliciestocollectvaluableexperiences.Suchstructured ple,theMPEenvironment[23]withadistributionoftargetland-
exploration can help the agents to learn good joint policies effi- marks that theagents need toreach. In addition, we meta-learn
ciently(Figure1).WeempiricallyshowthesuccessofMESAonthe adiscretesetofexplorationpoliciesthroughaniterativeprocess,
matrixclimbgameanditshardermulti-stagevariant.Inaddition, whichresultsinamuchsimpler meta-testing phase withoutthe
weevaluateMESAintwocontinuouscontroltasks,i.e.,theMPE need for posterior sampling or gradient updates on exploration
environment [23] and the multi-agent MuJoCo benchmark [29]. policies.Besides,someothermethodspretrainexplorationpolicies
WedemonstratethesuperiorperformanceofMESAcomparedto fromanofflinedataset[7,31,36],whichisbeyondthescopeofthis
existingmulti-agentlearningandexplorationalgorithms.Further- paper.
more, we show that MESA is capable of generalizing to unseen Finally,ourapproachlargelydiffersfromthesetting ofmulti-
test-timetasksthataremorechallengingthananyofthetraining tasklearning[1,2,11,16,27],whicharecommonlyevaluatedin
tasks. environments withheterogeneous tasksorscenarios. Ourexplo-
rationpoliciesarenottrainedtoachievehighreturnsinthetrain-
ingtasks.Instead,theyaretrainedtoreachasmanyhigh-reward
2 RELATEDWORK
state-actionpairsaspossiblecollectedinadiversesetoftasks.There-
Explorationhasbeenalong-standingchallengeinRLwithremark- fore,thestate-actionpairscoveredbyasingleexplorationpolicy
ableprogressachievedinthesingle-agentsetting[3,5,10,25,28, areverylikelytobedistributedacrossdifferenttrainingtasks.
34,37].Mostoftheseworksmaintain pseudo-countsover states
and construct intrinsic rewards to encourage the agents to visit
3 PRELIMINARIES
rarelyvisited statesmorefrequently[3,25,28,37].Thesecount-
basedmethodshavebeenextendedtothemulti-agentsettingbyin- Dec-POMDP. We consider fully-cooperative Markov games de-
centivizingintra-agentinteractionsorsocialinfluence[17â€“19,39]. scribed by a decentralized partially observable Markov decision
However,inthemulti-agentsetting,asimplecount-basedmethod process(Dec-POMDP),whichisdefinedbyhS,A,ğ‘ƒ,ğ‘…,Î©,O,ğ‘›,ğ›¾i.
canbelesseffectiveduetothepartialobservabilityofeachagent, Sisthestatespace.Aâ‰¡A1Ã—...Ã—Ağ‘›isthejointactionspace.The
anexponentiallylargejointstate-actionspace,andtheexistenceof dynamicsisdefinedbythetransitionfunctionğ‘ƒ(ğ‘ â€² |ğ‘ ,ğ’‚).Agents
multiplenon-Pareto-optimalNE.Therefore,recentworksfocuson sharearewardfunctionğ‘…(ğ‘ ,ğ’‚),andğ›¾ âˆˆ (0,1)isthediscountfactor.
discovering thestructuresofpossiblemulti-agent behaviors.For Î© â‰¡ Î© 1Ã—..Ã—Î© ğ‘› isthejointobservationspace,whereÎ© ğ‘– isthe
example,[24]adoptsvariationalinference tolearningstructured observationspaceforagentğ‘–.Ateachtimestep,eachagentğ‘– only
latent-space-policies;[15]generatessimilartaskswithsimplerre- hasaccesstoitsownobservationğ‘œğ‘– âˆˆ Î© ğ‘– definedbythefunction
wardfunctionstopromotecooperation;[21]learnstoselectasub- O:SÃ—Aâ†¦â†’Î©.ThegoalofagentsinDec-POMDPistomaximize
setofstatedimensionsforefficientexploration.Wefollowameta- thecommonexpecteddiscountedreturnunderthejointpolicyğ…:
learningframeworkandlearnstructuredexplorationstrategiesby J(ğ…)=E ğ… ğ‘¡ğ›¾ğ‘¡ğ‘…(ğ‘ ğ‘¡,ğ’‚ ğ‘¡) .
exploitinghigh-rewardingsubspaceinthejointstate-actionspace. LearningtoExplore.Meta-RLassumesataskdistributionğ‘(T)
(cid:2)Ã (cid:3)
Our methodalsoleverages acount-basedtechniqueasasubrou- overtasks,andanagent aimstolearntoquicklyadapttoatest-
tineduringthemeta-training phasetoprevent over-exploitation timetaskTtestdrawnfromğ‘(T)aftertraininginabatchoftrain-
andmodecollapse. ing tasks {Tğ‘– | Tğ‘– âˆ¼ ğ‘(T)} ğ‘–ğµ =1. Inspired by the explicit explo-
Meta reinforcement learning (meta-RL) is a popular RL para- ration methods [6, 42], we adopt a meta-exploration framework
digmthatfocusesontrainingapolicythatcanquicklyadapton for MARL: we learn joint exploration policies ğ… ğ‘’ from training
an unseen task at test time [9, 12, 14, 20, 32, 40, 42, 44]. Such a tasks {Tğ‘– | Tğ‘– âˆ¼ ğ‘(T)} ğ‘–ğµ =1 and useğ… ğ‘’ tocollectexperiences for
paradigmhasbeenextendedtothesettingoflearningtoexplore. thetrainingoftheagentsâ€™policyprofileğ…intaskTtest,denotedasğ…(ğ… ğ‘’,Ttest).Formally,theobjectiveofmeta-explorationis thecommonly-usedmulti-layerperceptron,andtherearealsothe-
oreticalresults[8]analyzingneuralnetworkswithquadraticacti-
m ğ…a ğ‘’xE Ttestâˆ¼ğ‘(T) "E ğ…(ğ…ğ‘’,Ttest)
" ğ‘¡
ğ›¾ğ‘¡ğ‘…ğ‘–(ğ‘ ğ‘¡,ğ’‚ ğ‘¡) ##. (1) v coat ei ffion c. ieF no tr st mh ae kc eli tm heb jog ia nm tğ‘„e, fi ut nis cte ia os ny suto ffiv ce ieri nfy tlyth ea xt prth ese siq vu ea td or pat ei rc
-
Ã•
fectlyfittherewardfunctionbysettingWtobetherewardmatrix.
NashEquilibriumandParetoOptimality.Ajointpolicyğ…
Therefore,thelearningprocessofğ‘„ismainlyaffectedbyhowthe
isanNEifeachagentâ€™spolicyğœ‹ğ‘– isabestresponsetotheother
explorationpolicysamplesthedata.
ğœ‹ag ğ‘–â€²e ,n wt esâ€™ hp ao vl eic ğ‘„ie ğ‘–s (ğ…ğ… )âˆ’ğ‘– â‰¥.T ğ‘„h ğ‘–a (ğœ‹t ğ‘–â€²i ,s, ğ…f âˆ’o ğ‘–r ),a wny hea rg ee ğ‘„nt ğ‘–ğ‘– iâ€™ ss ta hl ete vr an la ut eiv fe unp co tl ii oc ny Consideranexplorationpolic (y ğ‘¡)ğ‘ ğ‘’(ğ‘¡) thatselectsjointactionğ’‚=
foragentğ‘–.Ajointpolicyğ…isParetooptimaliftheredoesnotexist (ğ‘–,ğ‘—)atstepğ‘¡withprobabilityğ‘ ğ‘’ (ğ‘–,ğ‘—).Theefficiencyofanexplo-
analternativejointpolicyğ…â€² suchthatâˆ€ğ‘–, ğ‘„ğ‘–(ğ…â€²) â‰¥ ğ‘„ğ‘–(ğ…) and rationpolicycanbemeasuredbytherequirednumberofstepsfor
âˆƒğ‘–, ğ‘„ğ‘–(ğ…â€²) >ğ‘„ğ‘–(ğ…). learninganequivalentlyoptimalğ‘„ functionusingthemaximum
(ğ‘¡)
likelihoodestimatoroverthedatasampledfromğ‘ .Thelearning
ğ‘’
4 AMOTIVATINGEXAMPLE:CLIMBGAME objectiveincludesboththepriorğ‘(W)andthelikelihoodofpredic-
WeanalyzeafullycooperativematrixgameknownasClimbGame.
tionerrorğ‘(ğ¸ğ‘–ğ‘—),wherethepredictionerrorğ¸ğ‘–ğ‘— =ğ‘(eğ‘–,eğ‘—;Â·)âˆ’ğ‘…ğ‘–ğ‘—.
IfthepredictionerrorisassumedtobedepictedbyaGaussiandis-
In Section 4.1, we show how popular exploration strategies, in-
cludingunstructuredstrategieslikeuniformexplorationandtask-
tributionğ‘(ğ¸ğ‘–ğ‘—) =N(ğ¸ğ‘–ğ‘—;0,ğœ ğ‘’2)foreveryvisitedjointaction(ğ‘–,ğ‘—),
thenthelearningobjectivefortheğ‘„ functioncanbeformulated
specificstrategieslikeğœ–âˆ’greedy,failtoefficientlyexploretheclimb
as:
game.Bycontrast,weshowinSection4.2thatasimplestructured
explorationstrategycansubstantiallyimprovetheexplorationef-
J(ğ‘‡)(W,b,c,ğ‘‘)
ficiency. ğ‘‡
Ağ‘–A =c {li 0m ,.b ..g ,a ğ‘ˆm âˆ’e 1ğº }ğ‘“ fo( rğ‘› a,ğ‘¢ n, yğ‘ˆ p) lai ys ea rğ‘› ğ‘–.- Tp hla ey re er wg aa rm de ofw ait joh inac tt ai co tn ios np ğ’‚ac âˆˆe =E {(ğ‘–(ğ‘¡),ğ‘—(ğ‘¡))âˆ¼ğ‘ğ‘’(ğ‘¡)}ğ‘‡ ğ‘¡=1log ğ‘(W)
ğ‘¡
Ã–â€²=1ğ‘(ğ¸ ğ‘–(ğ‘¡)ğ‘—(ğ‘¡))
!
A isdeterminedbythenumberofplayersperformingaspecific ğ‘‡
actionğ‘¢(denotedas#ğ‘¢),whichis = E (ğ‘–,ğ‘—)âˆ¼ğ‘ğ‘’(ğ‘¡) logN(ğ‘(eğ‘–,eğ‘—;W,b,c,ğ‘‘)âˆ’ğ‘…ğ‘–ğ‘—;0,ğœ ğ‘’2)
ğ‘¡=1
1, if#ğ‘¢ =ğ‘›, +Ã• logN(W;0,ğœ2(cid:2)
ğ¼)+Const.
(cid:3)
(4)
ğ‘…(ğ’‚) = 1âˆ’ğ›¿ (0<ğ›¿ <1), if#ğ‘¢ =0, . (2) ğ‘¤
ï£±ï£´ï£´ï£´ï£²0, otherwise. thaW tme au xse imğ‘ iJ ze(ğ‘‡ s) J(W (ğ‘‡, )b, ac t, sğ‘‘ t) epto ğ‘‡d .e ğ‘n Jo (t ğ‘‡e )t (h We ,l bea ,cr ,n ğ‘‘e )d ij so din et teğ‘„ rmfu inn ec dtio bn
y
4.1 Exploratioï£´ï£´ï£´nChallenge theexplorationpolicyğ‘(ğ‘¡) andtheexplorationstepsğ‘‡.Thenwe
ï£³ ğ‘’
havethefollowingtheoremfortheuniformexplorationstrategy.
Aclimbgameğº (ğ‘›,ğ‘¢,ğ‘ˆ)hasthreegroupsofNE:theParetoopti-
ğ‘“
malNE(ğ‘¢,ğ‘¢,...,ğ‘¢),thesub-optimalNEs{(ğ‘1,ğ‘2,...,ğ‘ğ‘›) |âˆ€ğ‘–,ğ‘ğ‘– â‰  Theorem4.2(uniformexploration). Assumeğ›¿ â‰¤ 1 6,ğ‘ˆ â‰¥3.Using
ğ‘¢},andthezero-rewardNEs{(ğ‘1,ğ‘2,...,ğ‘ğ‘›) | 1 < #ğ‘¢ < ğ‘›}.The auniformexplorationpolicyintheclimbgameğº ğ‘“(2,0,ğ‘ˆ),itcanbe
sheer difference in the size ofthethree subsets ofNEsmakes it provedthatğ‘ J(ğ‘‡)(W,b,c,ğ‘‘)willbecomeequivalentlyoptimalonly
particularlychallengingforRLagentstolearntheoptimalpolicy afterğ‘‡ = Î©(|A|ğ›¿âˆ’1) steps.Whenğ›¿ = 1,ğ‘‡ =ğ‘‚(1) stepssufficeto
profilewithoutsufficientexploration,asevidencedbythetheoret- learntheequivalentlyoptimaljointQfunction,suggestingtheinef-
icalanalysisbelowandempiricalevaluationinSection6. ficiencyofuniformexplorationisduetoalargesetofsub-optimal
Considera2-agentclimbgameğº ğ‘“(2,0,ğ‘ˆ).Ajointactionğ’‚can NEs.
berepresentedbyapairofone-hotvectors[eğ‘–,eğ‘—] âˆˆ{0,1}2ğ‘ˆ.Let
TheintuitionbehindTheorem4.2isthatthehardnessofexplo-
ğ‘(x,y;ğœƒ)beajointQfunctionparameterizedbyğœƒthattakesinput
rationinclimbgameslargelycomesfromthesparsityofsolutions:
x,y âˆˆ {0,1}ğ‘ˆ and is learned to approximate the reward of the
asetofsub-optimalNEsexistbutthereisonlyasingleParetoop-
game.WehopethejointQfunctionhasthesameoptimalpolicy
timalNE.Learningthejointğ‘„ functioncanbeinfluencedbythe
profile.
sub-optimalNEs.Andiftheexplorationattemptsarenotwellco-
Definition4.1. Wecallajointğ‘„ functionğ‘(x,y;ğœƒ) equivalently ordinated,alotofzeroreward wouldbeencountered,making it
optimalwhenğ‘(e0,e0;ğœƒ) = max0â‰¤ğ‘–,ğ‘—<ğ‘ˆğ‘(eğ‘–,eğ‘—;ğœƒ).Whenajoint hardtofindtheParetooptimalNE.Wealsoremarkthatuniform
ğ‘„ functionisequivalentlyoptimal,onecanuseittofindtheopti- explorationcanbeparticularlyinefficientsincetheterm|A| can
malpolicy. beexponentiallylargeinamulti-agentsystem.Thisindicatesthat
moreefficientexplorationcanpotentiallybeachievedbyreducing
Sinceneuralnetworksaredifficulttoanalyzeingeneral[4],we thesearchspaceandidentifyingasmallerâ€œcriticalâ€subspace.
parameterizethejointğ‘„ functioninaquadraticform: ToformallyproveTheorem4.2,wedefine ğ‘“1,ğ‘“2,ğ‘“3 asthestep-
ğ‘(x,y;W,b,c,ğ‘‘)=xâŠ¤Wy+bâŠ¤x+câŠ¤y+ğ‘‘ (3) averagedprobabilityoftakingthejointactioninoptimalNE,sub-
optimalNEandzero-reward,respectively.Weshowthattomake
AGaussianpriorğ‘(W) = N(W;0,ğœ2ğ¼) isintroducedunderthe thejointğ‘„functionequivalentlyoptimal,thereisanecessarycon-
ğ‘¤
assumptionthatanon-linearWisharderandslowertolearn.Qua- ditionthatğ‘“1,ğ‘“2,ğ‘“3shouldfollow.Whenğ‘‡ isnotlargeenough,this
draticfunctionshavebeenusedinRL[13,38]asareplacementfor conditioncannotbesatisfied.DetailedproofisinAppendixA.2.Figure2:MESAâ€™smeta-learningframework.Inthemeta-trainingstage,MESAlearnsexplorationpoliciestocoverthehigh-
rewardingsubspace.Inthemeta-testingstage,MESAusesthelearnedexplorationpoliciestoassistthelearninginanunseen
task.Eachcolorcorrespondstoadifferenttask,andthecoloredpointsrepresentthehigh-rewardingjointstate-actionpairs
collectedinthattask.
Next,weconsiderthecaseofanotherpopularexplorationpar- Theorem4.4showstheefficiencyofexplorationcanbegreatly
adigm,ğœ–-greedyexploration. improvediftheexplorationstrategycapturesaproperstructureof
theproblem,i.e.,allagentstakingthesameaction.Wefurtherre-
Theorem4.3(ğœ–-greedyexploration). Assumeğ›¿ â‰¤ 1,ğ‘ˆ â‰¥4,ğ‘ˆ â‰¥ markthatbyconsideringasetofsimilarclimbgamesG,whereG=
32
ğœğ‘¤ğœ ğ‘’âˆ’1.Intheclimbgameğº ğ‘“(2,0,ğ‘ˆ),underğœ–-greedyexploration {ğº ğ‘“(2,ğ‘¢,ğ‘ˆ)} ğ‘¢ğ‘ˆ =âˆ’ 01,thestructuredexplorationstrategyğ‘ ğ‘’(ğ‘¡) (ğ‘–,ğ‘—) =
withfixedğœ– â‰¤ 1 2,ğ‘ J(ğ‘‡)(W,b,c,ğ‘‘)willbecomeequivalentlyoptimal ğ‘ˆâˆ’1 1 ğ‘–=ğ‘— canbeinterpretedasauniformdistributionoverthe
onlyafterğ‘‡ = Î©(|A|ğ›¿âˆ’1ğœ–âˆ’1) steps.Ifğœ–(ğ‘¡) = 1/ğ‘¡,itrequiresğ‘‡ = optimalpoliciesofthisgamesetG.Thisinterestingfactsuggests
exp Î© |A|ğ›¿âˆ’1 explorationstepstobeequivalentlyoptimal. thatw(cid:2) ecan(cid:3) firstcollectasetofsimilarlystructuredgamesandthen
derive effective exploration strategies from these similar games.
(cid:0) (cid:0) (cid:1)(cid:1)
TheproofissimilartothatofTheorem4.2(detailedinAppendix Onceasetofstructuredexplorationstrategiesarecollected,wecan
A.3). By comparing 4.2 and 4.3,ğœ–-greedy results in even poorer furtheradoptthemforfastlearninginanovelgamewithasimi-
explorationefficiencythanuniformexploration.Notetheğœ–-greedy larproblemstructure.Wetaketheinspirationhereanddevelopa
strategy is training policy specific, i.e., the exploration behavior generalmeta-explorationalgorithminthenextsection.
varies asthe training policychanges. Theorem4.3 suggests that
whenthepolicyissub-optimal,theinducedğœ–-greedyexploration 5 METHOD
strategycanbeevenworsethanuniformexploration.Hence,itcan
WedetailourmethodCooperativeMeta-ExplorationinMulti-Agent
bebeneficialtoadoptaseparateexplorationindependentfromthe
Learning through Exploiting State-Action Space Structure (MESA)
trainingpolicy.
forcooperativemulti-agentlearning.AsshowninFigure2,MESA
Theaboveanalysisshowsthatcommonexplorationstrategies
consistsofameta-trainingstage(Algo.1)andameta-testingstage
likeuniformexplorationorğœ–-greedyexplorationareinefficientfor
(Algo.2).Inthemeta-trainingstage,MESAlearnsexplorationpoli-
suchasimplegameandthemainreasonisthatitrequirescoordina-
cies by training in a batch of training tasks that share intrinsic
tionbetweendifferentagentstoreachhigh-rewardingstates,but
structuresinthestate-actionspace.Inthemeta-testingstage,MESA
naiveexplorationstrategieslacksuchcooperation.
utilizesthemeta-learnedexplorationpoliciestoassistlearningin
anunseentasksampledfromthedistributionofthetrainingtasks.
4.2 StructuredExploration
Wewillshowthatitispossibletodesignabetterexplorationstrat- 5.1 Meta-Training
egywithsomepriorknowledgeoftheclimbgamestructure.Con-
sideraspecificstructuredexplorationstrategyğ‘ ğ‘’(ğ‘¡) (ğ‘–,ğ‘—) =ğ‘ˆâˆ’1 1 ğ‘–=ğ‘— , T reh we am rde it na g-t sr ta ain tei -n ag cts it oa nge suc bo sn pt aa ci en ,s at nw do 2s )t te rp as in:1 a) sid eten ot fif ey xpth loe rah ti ig oh n-
where both agents always choose the same action. With such a
(cid:2) (cid:3) policiesusingthesubspace-inducedrewards.
strategy, we can quickly find the optimal solution to the game.
Moreformally,wehavethefollowingtheorem. 5.1.1 IdentifyingHigh-RewardingJointState-ActionSubspace. For
eachtrainingtaskTğ‘–,wecollectexperiencesDğ‘– ={(ğ‘ ğ‘¡,ğ’‚ ğ‘¡,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+1)}.
Theorem4.4(structuredexploration). Intheclimbgameğº (2,0,ğ‘ˆ), â˜…
ğ‘“ Iftherewardğ‘Ÿğ‘¡ is higher thanathresholdğ‘… ,we callthisjoint
understructuredexplorationğ‘ ğ‘’(ğ‘¡) (ğ‘–,ğ‘—) =ğ‘ˆâˆ’1 1 ğ‘–=ğ‘— ,ğ‘ J(ğ‘‡)(W,b,c,ğ‘‘) state-action pair (ğ‘ ğ‘¡,ğ’‚ ğ‘¡) valuable and store it into a dataset Mâˆ—.
isequivalentlyoptimalatstepğ‘‡ =ğ‘‚(1).
(cid:2) (cid:3)
Forgoal-orientedtaskswhereğ‘Ÿ = 1 ğ‘ =ğ‘”ğ‘œğ‘ğ‘™,thethreshold canbeAlgorithm1MESA:Meta-Training Algorithm2MESA:Meta-Testing
Input:Meta-trainingtasks{Tğ‘–} ğ‘–ğµ =1âˆ¼ğ‘(T),off-policyMARL Input:TesttaskTË†,meta-trainedexplorationpolicies{ğ…ğ‘– ğ‘’} ğ‘–ğ¸ =1,
algorithmğ‘“,distancemetrickÂ·k F off-policyMARLalgorithmğ‘“
â˜…
Parameter:#policiesğ¸,thresholdğ‘… ,horizonâ„ Parameter:horizonâ„
Output:Explorationpolicies{ğ…ğ‘–}ğ¸ Output:Policyğ… fortaskTË†
ğ‘’ ğ‘–=1 ğœƒ
1: Mâˆ—â†âˆ…,globalpseudo-countğ‘Ë† â†0 1: Initializepolicyğ… ğœƒ,D =âˆ…,annealingğœ–
2: fori=1toBdo 2: whilenotconvergeddo
3: Initializepolicyğ… ğœƒ 3: Determineğ‘ğ‘’ underannealingprobabilityscheduleğœ–
4: Trainğ… ğœƒ withğ‘“ andcollectdatasetğ·ğ‘– ={(ğ’” ğ‘¡,ğ’‚ ğ‘¡,ğ‘Ÿğ‘¡,ğ’” ğ‘¡+1)} 4: Choosepolicytoperformrolloutsby
â˜…
5 6 7: :
:
e fon rM d ifâˆ— =oâ† r 1toM Eâˆ— dâˆª o{ğœ |ğ‘…(ğœ) â‰¥ğ‘… ,ğœ âˆˆğ·ğ‘–} ğ… ğ‘‘ = (ğ… ğ…ğ‘’ ğœƒ,âˆ¼U({ğ…ğ‘– ğ‘’} ğ‘–ğ¸ =1), ow t. hp e. rğ‘ wğ‘’
ise.
18
9
0:
:
:
I wn hi It niia
l ie
tli iz ağ…e
liğ‘– ğ‘’
ze
â€™
esxp ğ‘trlo
a
air
n
sa it ğ‘ni Ë†o
g
,n Dnp oo â†tl cic
o
âˆ…y nvğ… eğ‘– ğ‘’
rgeddo
765 ::: for OExt
be
s=
c
eu0 rvtt
e
eo
ğ’‚
th
rğ‘¡
a-1
âˆ¼
nsd
ğ…
io
tiğ‘‘ o( nğ‘ ğ‘¡ () ğ‘ .
ğ‘¡,ğ’‚ ğ‘¡,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+1).
1 11 2:
:
for Ext e= cu0 tt eo ğ’‚h ğ‘¡-1 âˆ¼d ğ…o
ğ‘– ğ‘’(ğ‘ ğ‘¡),andobserve(ğ‘ ğ‘¡,ğ’‚ğ’• ,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+1)
98 :: enD dfâ† orDâˆª(ğ‘ ğ‘¡,ğ’‚ ğ‘¡,ğ‘Ÿğ‘¡,ğ‘ ğ‘¡+1)
11 43 :: SC ta ol rc eul (a ğ‘ t ğ‘¡e ,ğ’‚ğ‘ŸË† ğ‘¡ğ‘¡ ,b ğ‘ŸË†ğ‘¡a ,s ğ‘ e ğ‘¡d +1o )n inE tq o. D5or6 1 10 1:
:
enO dp wti hm ii lz eeğ… ğœƒ withalgorithmğ‘“ onreplaybufferD
15: ğ‘(ğœ™(ğ‘ ğ‘¡,ğ’‚ ğ‘¡)) â†ğ‘(ğœ™(ğ‘ ğ‘¡,ğ’‚ ğ‘¡))+1
12:
returnğ…
ğœƒ
16: endfor
17: Optimizepolicyğ…ğ‘– ğ‘’ withalgorithmğ‘“
18: endwhile
19: Updateğ‘Ë† usingD
pairsareclose.Thenifavisitedjointstate-actionpair(ğ‘ ,ğ’‚)isclose
20: endfor
21: return{ğ…ğ‘– ğ‘’} ğ‘–ğ¸ =1 ğœ–en ,io tu wg oh ut lo dth beei ad se sn igt nifi ee dd asu db es rip va ec de pM osâˆ— i, ti i. ve e., rm ewin ağ‘‘ râˆˆ dM ğ‘ŸË†.âˆ— Ik n( cğ‘  r, eğ’‚ a) s, iğ‘‘ nk gF th<
e
valueofğµ inthecollectionstepwouldgenerallyresultinamore
accuratedistancemeasurement. However, thiscomesatthecost
setasğ‘…â˜…=1.Forothertasks,thethresholdcanbesetasahyper- ofmakingtheminimizationcalculationmorecomputationallyex-
pensive.
parameter,forexample,acertainpercentileofallcollectedrewards.
â˜… Toencourageabroadercoverageofthesubspaceandtoavoid
Asmallerğ‘… resultsinalargeridentifiedsubspacebutalesseffi-
modecollapse,therewardassignmentschemeensuresthatrepeated
cientexplorationpolicy.
visitstosimilarjointstate-actionpairswithinonetrajectorywould
ThedatastoredinMâˆ—ishighlydiversifiedsinceitcomesfrom
result in a decreasing reward for each visit. Similar to [37], we
alltheğµ training tasks,whichareexpectedtoshareanintrinsic
adoptapseudo-countfunctionğ‘ withahashfunctionğœ™(ğ’”,ğ’‚) to
structure. Weexpectthatwiththisintrinsic structure,thehigh-
generalizebetweensimilarjointstate-actionpairs.Wethenapplya
rewardingjointstate-actionpairsfallintosomelow-dimensional
decreasingfunctionğ‘“ :N â†¦â†’[0,1]onthetrajectory-levelpseudo-
subspace.Inthesimplestcase,theymayformseveraldenseclus- ğ‘‘
countğ‘(ğœ™((ğ‘ ,ğ’‚)).Theresultedrewardassignmentschemeisde-
ters,ormanyofthemlieinahyperplane.Evenifthesubspaceis
finedasfollows:
noteasilyinterpretabletohumans,itmaystillbeeffectivelyâ€œcov-
eredâ€byasetofexplorationpolicies(tobefoundinthesubsequent ğ‘ŸËœğ‘¡ =ğ‘ŸË†ğ‘¡ğ‘“ ğ‘‘(ğ‘(ğœ™((ğ‘ ğ‘¡,ğ’‚ ğ‘¡))) 1 minğ‘‘âˆˆMâˆ—k(ğ‘ ğ‘¡,ğ’‚ğ‘¡),ğ‘‘kF<ğœ– (6)
step).
h i
Wealsoexplicitlydealwiththerewardsparsityproblembyas- After one exploration policy is trained with this reward, we
signing apositivereward toajoint state-actionpair (ğ‘ ğ‘¡,ğ’‚ ğ‘¡) if it willtrainanewpolicytocoverthepartoftheidentifiedsubspace
haszerorewardbutleadstoavaluablestate-actionpair (ğ‘ ğ‘¡â€²,ğ’‚ ğ‘¡â€²) thathasnotyetbeencovered.Thisisachievedbyhavingaglobal
laterinthesametrajectory.Wealsoputtheserelabeledpairsinto pseudo-countğ‘Ë† whichisupdatedaftertrainingeachexploration
thedatasetMâˆ—.Letğ‘¡â€² = argminğ‘¡â€²>ğ‘¡[ğ‘Ÿğ‘¡â€² > 0],wethereforehave policyusingitsvisitationcountsandismaintainedthroughoutthe
thefollowingdensifiedrewardfunction trainingofallexplorationpolicies.Thisiterativeprocesscontinues
untilthesubspaceiswell-coveredbythesetoftrainedexploration
ğ‘ŸË†ğ‘¡ = (ğ›¾ ğ‘Ÿğ‘¡ğ‘¡ ,â€²âˆ’ğ‘¡ Â·ğ‘Ÿğ‘¡â€², ğ‘Ÿ ğ‘Ÿğ‘¡ğ‘¡ = >0 0,
.
(5) policies.
5.2 Meta-Testing
5.1.2 LearningExplorationPolicies. Inthisstep,weaimtolearn
a diverse set ofexplorationpolicies to cover the identified high- Duringmeta-testing,MESAusesthemeta-learnedexplorationpoli-
rewarding joint state-action subspace. We use a distance metric cies{ğ…ğ‘– ğ‘’} ğ‘–ğ¸ =1toassistthetrainingofanygenericoff-policyMARL
k Â· k F (e.g., ğ‘™2 distance) to determine whether two state-action algorithmonatest-timetaskTË†.Specifically,foreachrolloutepisode,wechoosewithprobabilityğœ–toexecuteoneuniformlysampledex-
plorationpolicyğ… ğ‘’ âˆ¼ U({ğ…ğ‘– ğ‘’} ğ‘–ğ¸ =1).Forthebestempiricalperfor-
mance,wealsoadoptanannealingscheduleğœ– :ğ‘‡ â†¦â†’[0,1]sothat
theexplorationpoliciesprovidemorerolloutsattheinitialstage
ofthetrainingandaregraduallyturnedofflater.
Herewefurtherprovidesomeanalysisofdeployingthemeta-
learnedexplorationpolicyonunseentestingtasks.
Theorem5.1 (Explorationduring Meta-Testing). Consider goal-
oriented tasks with goal space G âŠ† S. Assume the training and
testinggoalsaresampledfromthedistributionğ‘(ğ‘¥) on G,andthe
datasethasğ‘ i.i.d.goalssampledfromadistributionğ‘(ğ‘¥) onS.If Figure3:Learningcurveofthetwoclimbgamevariantsw.r.t
theexplorationpolicygeneralizestoexploreğœ–nearbygoalsforevery numberofenvironmentsteps.Thereturnisaveragedover
trainingsample,wehave thatthetestinggoalisnotexploredwith timestepsforthemulti-stagegames.Thedottedlinesindi-
probabilityatmost catethe suboptimalreturnof 0.5 (purple) and the optimal
ğ¾ğ¿(ğ‘||ğ‘)+H(ğ‘) return1(blue)foreachagent.
ğ‘ƒ â‰ˆ ğ‘(ğ‘¥)(1âˆ’ğœ–ğ‘(ğ‘¥))ğ‘ğ‘‘ğ‘¥ â‰¤ğ‘‚ . (7)
fail log(ğœ–ğ‘)
âˆ« (cid:18) (cid:19)
Theorem5.1showsthatthegoodperformanceofmeta-learned
explorationpolicyrelieson1)asmalldifferencebetweenthetrain-
ingandtestingdistribution;and2)astructured,e.g.,low-dimensional, meta-learningbaselines,includingonewithanunconditionedshared
high-rewardingsubspaceGtoreduceH(ğ‘).Andwhenuniformly policy, which is trained over all training tasks, and one with a
samplingthetrainingdata,ğ¾ğ¿(ğ‘||ğ‘)isboundedbylogÎ© G inour goal-conditionedpolicy,whichtakesthetargetlandmarksasparts
method.Thisterm,however,canbeuptologÎ© S withanuncoor- of the input. We also adapt the single-agent meta-RL algorithm
dinatedexplorationonthejointstatespaceS,where Î© S canbe MAESN[14]tothemulti-agentsetting.Finally,weadaptthesingle-
exponentiallylargerthanÎ© G. agent C-BET [26]tomulti-agent settingsbased onMAPPO.The
trainingandtestingtasksareasdefinedinSection6.1.Pleaserefer
5.3 ImplementationDetailofMESA totheAppendixformorevisualizationandexperimentalresults.
Environments.WeexperimentontheClimbGame,Multi-agent
WechooseMADDPG,followingthecentralizedtrainingwithde-
ParticleEnvironment(MPE)[23],andmulti-agentMuJoCo[29],on
centralized execution (CTDE) paradigm, as the off-policy MARL
whichgeneratingadistributionofmeta-trainingtasksğ‘(T)isfea-
algorithmforMESAsince it canbeappliedtobothdiscrete and
sible.
continuousactionspace,asshowninitsoriginalpaper[23].Weuse
aclusteringmappingğ‘“ğ‘ asthehashfunctionğœ™ sothatthedataset
Mâˆ—isclusteredintoğ¶clustersdefinedbytheclusteringfunction 6.2 ClimbGameVariants
ğ‘“ğ‘ :SÃ—A â†¦â†’ [ğ¶].Theclustermappingisimplementedwiththe First,weconsidertaskspacesconsistingofvariantsoftheafore-
KMeansclusteringalgorithm[22].Thenumberofexplorationpoli- mentioned climb games. We extend previous climb game to (1)
ciestolearnisviewedasahyperparameter.SeetheAppendixfor one-stepclimbgameğº(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ),whichisağ‘›-playergamewith
detailedhyperparametersettings. ğ‘ˆ actionsforeachplayer,andthejointrewardis1if#ğ‘¢=ğ‘˜,1âˆ’ğ›¿
if #ğ‘¢ = 0, and 0 otherwise. The task space Tone consists of all
6 EXPERIMENTS ğ‘ˆ
one-stepclimbgamesthatcontaintwoplayersandğ‘ˆ actions;(2)
Our experimental evaluationaims toanswer thefollowingques- multi-stageclimbgame,whichisanğ‘†-stagegamewhereeach
tions: (1) Are the meta-learned exploration policies capable of stage is a one-stage climb game withthe same number of avail-
achievingmoreefficientexplorationduringmeta-testingonnewly ableactions.Eachstageğ‘¡ hasitsownconfiguration(ğ‘˜ğ‘¡,ğ‘¢ğ‘¡)ofthe
sampledtasksinmatrixclimbgamevariants(Section6.2)andhigh- one-stage climb gameğº(2,ğ‘˜ğ‘¡,ğ‘¢ğ‘¡,ğ‘ˆ). Agents observe thehistory
dimensional domains (Section 6.3 and 6.4)? (2) Can these meta- ofjointactionsandthecurrentstageğ‘¡.ThetaskspaceTmulticon-
ğ‘†,ğ‘ˆ
learnedexplorationpoliciessuccessfullygeneralizetounseentest- sistsofall multi-stageclimb games withğ‘† stages andğ‘ˆ actions.
time tasksfromamorechallenging (e.g., withmoreagents) test Inourexperiments,weuseToneandTmultiasthetaskspacefor
10 5,10
taskdistributionwhich isdifferent thetraining taskdistribution theone-stepandmulti-stageClimbGames.Wechooseuniformly
(Section6.5)? atrandomtentrainingtasksandthreedifferenttesttasksfromthe
taskspaceT,andwekeepğ›¿ = 1 asintheclassicclimbgames.
6.1 EvaluationSetup 2
ResultsonClimbGameVariants.Forthematrixgames,we
ComparedMethods.Wecompareto3multi-agentreinforcement additionallycomparewithMA-MAESN,whichisouradaptation
learningalgorithms:MADDPG[23],MAPPO[41],andQMIX[33], oftheoriginalsingle-agentmeta-learningalgorithmMAESN[14]
tomeasuretheeffectiveness ofourexplorationpolicies.Wealso tothemulti-agentscenarioInthesingle-stepmatrixgame,MESA
compareto3multi-agentexplorationalgorithm:MAVEN[24],MAPPO exhibitsbetterperformance,beingabletofindtheoptimalreward
withRNDexploration[5],andEMC[43].Tocomparewithbase- insomehardertaskswhenğ‘˜ =2,whileotherbaselinesarestuck
lines that adoptasimilar meta-training stage, we addtwo naive atthesub-optimalrewardforalmostalltasks.Figure4:LearningcurvesofMESAandthecomparedbaselinesw.r.tthenumberofenvironmentinteractionsduringthemeta-
testingstageintheMPEdomainandthemulti-agentMuJoCoenvironmentSwimmer.Thetwodottedlinesindicatetheideal
optimal(purple)andsub-optimal(blue)returnsummedovertimesteps.Areturnabovethebluelinewouldtypicallyindicate
thattheagentsareabletolearntheoptimalstrategy.
Inthemorechallenging10-actionmulti-stagegamewheretask
spaceisexponentiallylarger,MESAoutperformsallcomparedal-
gorithmsbyalargemargin.Withthehelpoftheexplorationpoli-
ciesthathavelearnedthehigh-rewardingjointactionpairs,MESA
quicklylearnstheoptimaljointactionforeachstageandavoidsbe-
ingstuckatthesub-optimal.
Figure6:Visualizationofstructuredexplorationbehaviors
discoveredbythemeta-trainedexplorationpolicyinMESA.
onthe2-agenttasks(TMPE andTMPE)and3-agenttasks(TMPE
2,5 2,6 3,5
andTMPE)whilefixingğ‘˜ = 2.Eachsampledtrainingandtesting
3,6
taskhasadifferentconfigurationoflandmarkpositions.
AdaptationPerformanceinMPE.WeshowinFigure4the
learning curve of ourapproach MESA compared with theafore-
mentioned baseline methods. MESA outperforms the compared
baselinesbyalargemargin,beingabletocoordinatelyreachthe
tasklandmarkquickly,asevidenced bythenear-optimal reward.
EvenwhencombinedwithRND-basedexploration,MAPPOeasily
sticks to the sub-optimal equilibrium. Value-based methods like
QMIXandMAVENareunabletolearnthecorrectğ‘„-functionbe-
Figure5:Visualizationsofa2-player3-landmarkMPEclimb
cause the reward is quite sparse before agents can consistently
game.
movethemselvestoalandmark.EMCsometimesjumpsoutofthe
suboptimalequilibriumwithcuriosity-drivenexploration,butthe
performanceisnotrobust.Furthermore,asthemeta-learningbase-
6.3 MPEDomain
lines only learn the sub-optimal behavior during meta-training,
WeextendthematrixclimbgamestoMPE[23],whichhasacon- theyfailtolearntheoptimalequilibriumduringtesttimeandquickly
tinuoushigh-dimensional state space. Agents mustfirst learn to convergetothesuboptimalequilibrium.
reachthelandmarksundersparserewardsandthenlearntoplay Visualizationof Exploration Policies. To answer question
theclimbgamesoptimally. (2), we visualize the learned exploration policies in a 2-agent 3-
InaMPEClimbGameğºÂ¯(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ,{ğ¿ğ‘—}ğ‘ˆ 0âˆ’1)(Figure5),thereare landmarkMPE taskin Figure 6.We can see that the learned ex-
ğ‘ˆnon-overlappinglandmarkswithpositions{ğ¿ğ‘—}ğ‘ˆ ğ‘—=âˆ’ 01.Thereward plorationpolicyconsecutivelyvisited the3landmarkswithin20
isnon-zeroonlywheneveryagentisonsomelandmark.Agents timestepsinonetrajectory.
will be given a reward of 1 if there are exactlyğ‘˜ agents located
6.4 Multi-agentMuJoCoEnvironments
ontheğ‘¢-thlandmark(targetlandmark),andasuboptimalreward
of1âˆ’ğ›¿ willbegivenwhennoneoftheagentsarelocatedonthe Wealsoextendthematrixclimbgamestomulti-agentMuJoCoen-
targetlandmark.Otherwise,therewardwillbezero.Asbefore,ğ‘¢ vironments[29].Weconsiderspecificallythe2-agentSwimmeren-
andğ‘˜ arenotpresentintheobservationandcanonlybeinferred vironmentwhereeachagentisahingeontheswimmerâ€™sbody,and
fromthereceivedreward.AtaskspaceTMPEconsistsofallMPE eachagentâ€™sactionistheamountoftorqueappliedtohingerotors.
ğ‘›,ğ‘ˆ
climbgameswithğ‘›playersandğ‘ˆ landmarks.WeevaluateMESA Theextensionconsiderstheanglesbetweenthetwohingesandthebodysegments.Eachtaskinthetaskspaceisatargetanglesuch
that areward of 1 will be given only if the two angles are both
closetothetargetangles,a0.5suboptimalrewardisgivenifnone
oftwoanglesareclosetothetarget,andarewardof0ifonlyone
ofthetwoanglesareclose.
Thismulti-agentenvironment isextremelyhardasagentsare
verylikelytoconvergetothesuboptimalrewardof0.5,whichis
confirmed bytheresultsthat none ofthebaselines were ableto
findtheoptimalequilibriuminFigure4.Therefore,MESAvastly
outperformsallthecomparedbaselinesbylearningafinalpolicy
thatfrequentlyreachesthetargetangle.
Figure7:GeneralizationresultsofMESAonthehard3-agent
MPE Climb game. Left: Zero-shot generalizability of the
meta-explorationpolicies,measuredbythenumberofvis-
itationsonhigh-rewardtransitionsperepisodeonthetest
6.5 GeneralizationPerformanceofMESA tasks.Thepurpledottedlinecorrespondstotherandomex-
In this section, our goal is to evaluate the generalization perfor- plorationpolicy.Theplotshowstheconcatenatedtraining
manceofthemeta-trainedexplorationpolicyinscenarioswhere curvesforallexplorationpolicies.Right:Learningcurvesof
themeta-trainingandmeta-testingtaskdistributionsaredifferent. MESAunder differentsettingsusing the meta-exploration
Inparticular,wefocusonthesettingwherethetest-timetasksare policiestrainedonthetwodifferenttraining-taskdistribu-
morechallengingthanthetraining-timetasksandexaminehowan tions.
explorationpolicylearnedfromsimplertaskscanboosttraining
performancesonhardertasks.
Thetesttaskhereisuniformonthe3-agenthigh-difficultyMPE 7 CONCLUSIONS
Climbgames.Thetaskdifficultyisdefinedbytheaveragepairwise
Thispaperintroducesameta-explorationmethod,MESA,formulti-
distancesbetweenthelandmarkpositionsandtheinitialpositions
agent learning. The key idea is to learn a diverse set of explo-
oftheagents.Weconsidertwosimplertrainingtaskdistributions,
rationpoliciestocoverthehigh-rewardingstate-actionsubspace
including(1)a2-agentsettingwiththesamedifficulty,and(2)a
and achieve efficient exploration in an unseen task. MESA can
3-agentsettingwithalowerdifficulty.Inbothsettings,themeta-
workwithanyoff-policyMARLalgorithm,andempiricalresults
trainingtasksarelesschallengingthanthetest-timetasks.Foreval-
confirmtheeffectivenessofMESAinclimbgames,MPEenviron-
uation,themeta-trainedexplorationpolicyfromeachsettingwill
ments, and multi-agent MuJoCo environments and its generaliz-
bedirectlyappliedtoassistthetrainingonthemorechallenging
abilitytomorecomplextest-timetasks.
test-timetasks,withoutanyfine-tuning.
Wemodifiedtheneuralnetworkarchitecturebyadoptinganat-
ACKNOWLEDGMENTS
tentionlayerinbothactorandcritictoensuretheyarecompatible
withavaryingnumberofagents.Theattentionmechanismactsas ThisresearchissupportedinpartbyNSFIIS-2046640(CAREER)
anaggregationfunctionbetweentherelativepositionsoftheother andSloanResearchFellowship.WethankNVIDIAforproviding
agents and its own relative positionto the landmarks to handle computingresources.ZhichengZhangissupportedinpartbySCS
the varying observation dimensions. Additionally, we employed Deanâ€™sFellowship.Thefundershavenoroleinstudydesign,data
behaviorcloning(BC)[30]ontherolloutsoftheexplorationpoli- collectionandanalysis,decisiontopublish,orpreparationofthe
ciesasawarm-uptoacceleratelearningofthefinalpolicy. manuscript.
InFigure7,wepresentthegeneralizationresultsfromourstudy.
Weevaluatethezero-shotgeneralizationabilityofthemeta-exploration REFERENCES
policy by measuring the average number of high-reward transi- [1] JacobAndreas,DanKlein,andSergeyLevine.2017.Modularmultitaskreinforce-
tionshitinatesttaskrandomlysampledfromthetesttaskdistri- mentlearningwithpolicysketches.InICML.PMLR,166â€“175.
[2] MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,Pe-
bution.AsshownontheleftofFigure7,themeta-explorationpoli- terWelinder,BobMcGrew,JoshTobin,PieterAbbeel,andWojciechZaremba.
ciesareabletoexplorethetest-timetasksmuchmoreefficiently 2017.Hindsightexperiencereplay.arXivpreprintarXiv:1707.01495(2017).
[3] MarcBellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,
thanarandomexplorationpolicy,evenontest-timetasksthatare
andRemiMunos.2016.Unifyingcount-basedexplorationandintrinsicmotiva-
drawnfromahardertaskdistribution.Notably,thegeneralization tion.NeurIPS29(2016),1471â€“1479.
abilityincreaseswiththenumberofexplorationpolicies(ğµ).Using [4] AvrimBlumandRonaldRivest.1988. Traininga3-nodeneuralnetworkisNP-
complete.NeurIPS1(1988).
themeta-explorationpoliciestrainedonthesimplertasks,MESA
[5] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2018. Explo-
isabletoconsistentlyreachthehigh-rewardregionintheunseen rationbyrandomnetworkdistillation.InICLR.
hard3-agenttasks,asopposedtothevanillaMADDPGalgorithm [6] CÃ©dricColas,OlivierSigaud,andPierre-YvesOudeyer.2017. GEP-PG:Decou-
plingExplorationandExploitationinDeepReinforcementLearningAlgorithms.
thatonlylearnsthesub-optimalequilibrium.Wealsoseethatwith InICML.
anincreasingnumberofmeta-explorationpolicies,theperformance [7] RonDorfman,IdanShenfeld,andAvivTamar.2020. OfflineMetaLearningof
Exploration.arXivpreprintarXiv:2008.02598(2020).
ofMESAincreases,buttheimprovementbecomesmarginal,while
[8] SimonDuandJasonLee.2018.Onthepowerofover-parametrizationinneural
themeta-trainingtimeincreaseslinearlywithE. networkswithquadraticactivation.InICML.PMLR,1329â€“1338.[9] YanDuan,JohnSchulman,XiChen,PeterLBartlett,IlyaSutskever,andPieter 20516â€“20530.
Abbeel.2016.Rl2:Fastreinforcementlearningviaslowreinforcementlearning. [27] EmilioParisotto,LeiJimmyBa,andRuslanSalakhutdinov.2016. Actor-Mimic:
arXivpreprintarXiv:1611.02779(2016). DeepMultitaskandTransferReinforcementLearning.InICLR(Poster).
[10] AdrienEcoffet,JoostHuizinga,JoelLehman,KennethOStanley,andJeffClune. [28] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. 2017.
2021.Firstreturn,thenexplore.Nature590,7847(2021),580â€“586. Curiosity-driven exploration by self-supervised prediction. In ICML. PMLR,
[11] LasseEspeholt,HubertSoyer,RemiMunos,KarenSimonyan,VladMnih,Tom 2778â€“2787.
Ward,YotamDoron,VladFiroiu,TimHarley,IainDunning,etal.2018.Impala: [29] BeiPeng,TabishRashid,ChristianSchroederdeWitt,Pierre-AlexandreKami-
Scalabledistributeddeep-rlwithimportanceweightedactor-learnerarchitec- enny,PhilipTorr,WendelinBÃ¶hmer,andShimonWhiteson.2021.Facmac:Fac-
tures.InICML.PMLR,1407â€“1416. toredmulti-agentcentralisedpolicygradients.NeurIPS34(2021).
[12] ChelseaFinn,PieterAbbeel,andSergeyLevine.2017. Model-AgnosticMeta- [30] DeanAPomerleau.1991.Efficienttrainingofartificialneuralnetworksforau-
LearningforFastAdaptationofDeepNetworks.InProceedingsofthe34thICML tonomousnavigation.Neuralcomputation3,1(1991),88â€“97.
(PMLR, Vol. 70), Doina Precupand Yee Whye Teh(Eds.). PMLR, 1126â€“1135. [31] VitchyrHPong,AshvinNair,LauraSmith,CatherineHuang,andSergeyLevine.
https://proceedings.mlr.press/v70/finn17a.html 2021.OfflineMeta-ReinforcementLearningwithOnlineSelf-Supervision.arXiv
[13] ShixiangGu,TimothyLillicrap,IlyaSutskever,andSergeyLevine.2016. Con- preprintarXiv:2107.03974(2021).
tinuousdeepq-learningwithmodel-basedacceleration.InICML.PMLR,2829â€“ [32] KateRakelly,AurickZhou,ChelseaFinn,SergeyLevine,andDeirdreQuillen.
2838. 2019. Efficientoff-policymeta-reinforcementlearningviaprobabilisticcontext
[14] Abhishek Gupta, RussellMendonca, Yuxuan Liu,Pieter Abbeel, and Sergey variables.InICML.PMLR,5331â€“5340.
Levine.2018. Meta-ReinforcementLearningofStructuredExplorationStrate- [33] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar,
gies.NIPS2018(2018),5302â€“5311. JakobFoerster,andShimonWhiteson.2018. Qmix:Monotonicvaluefunction
[15] TarunGupta,AnujMahajan,BeiPeng,WendelinBÃ¶hmer,andShimonWhite- factorisationfordeepmulti-agentreinforcementlearning.InICML.PMLR,4295â€“
son.2021. Uneven:Universalvalueexplorationformulti-agentreinforcement 4304.
learning.InICML.PMLR,3930â€“3941. [34] MartinRiedmiller,RolandHafner,ThomasLampe,MichaelNeunert,JonasDe-
[16] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon grave,TomWiele,VladMnih,NicolasHeess,andJostTobiasSpringenberg.2018.
Schmitt,andHadovanHasselt.2019. Multi-taskdeepreinforcementlearn- Learningbyplayingsolvingsparserewardtasksfromscratch.InICML.PMLR,
ingwithpopart.InProceedingsoftheAAAIConferenceonArtificialIntelligence, 4344â€“4353.
Vol.33.3796â€“3803. [35] LukasSchÃ¤fer,FilipposChristianos,JosiahPHanna,andStefanoVAlbrecht.
[17] EdwardHughes, Joel ZLeibo,Matthew Phillips, KarlTuyls, Edgar Duenez- 2022. DecoupledReinforcementLearningtoStabiliseIntrinsically-Motivated
Guzman,Antonio GarciaCastaneda,IainDunning,TinaZhu,KevinMcKee, Exploration.InProceedingsofthe21stAAMAS.1146â€“1154.
RaphaelKoster,etal.2018.Inequityaversionimprovescooperationinintertem- [36] AviSingh,HuihanLiu,GaoyueZhou,AlbertYu,NicholasRhinehart,andSergey
poralsocialdilemmas.NIPS201831(2018). Levine.2020.Parrot:Data-DrivenBehavioralPriorsforReinforcementLearning.
[18] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,Pedro InICLR.
Ortega,DJStrouse,JoelZLeibo,andNandoDeFreitas.2019. Socialinfluence [37] HaoranTang,ReinHouthooft,DavisFoote,AdamStooke,XiChen,YanDuan,
asintrinsicmotivationformulti-agentdeepreinforcementlearning.InICML. JohnSchulman,FilipDeTurck,andPieterAbbeel.2017.#exploration:Astudy
PMLR,3040â€“3049. ofcount-basedexplorationfordeepreinforcementlearning.In31stNIPS,Vol.30.
[19] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroA 1â€“18.
Ortega,DJStrouse,JoelZLeibo,andNandodeFreitas.2018. Intrinsicsocial [38] Pin Wang, Hanhan Li, and Ching-Yao Chan. 2019. Quadratic Q-network
motivationviacausalinfluenceinmulti-agentRL.(2018). for learning continuous control for autonomous vehicles. arXiv preprint
[20] LinLan,ZhenguoLi,XiaohongGuan,andPinghuiWang.2019. Metarein- arXiv:1912.00074(2019).
forcementlearningwithtaskembeddingandsharedpolicy. arXivpreprint [39] TonghanWang,JianhaoWang,YiWu,andChongjieZhang.2019. Influence-
arXiv:1905.06527(2019). BasedMulti-AgentExploration.InICLR.
[21] Iou-JenLiu,UnnatJain,RaymondAYeh,andAlexanderSchwing.2021.Cooper- [40] TianbingXu,QiangLiu,LiangZhao,andJianPeng.2018. Learningtoexplore
ativeexplorationformulti-agentdeepreinforcementlearning.InICML.PMLR, withmeta-policygradient.arXivpreprintarXiv:1803.05044(2018).
6826â€“6836. [41] ChaoYu,AkashVelu,EugeneVinitsky,YuWang,AlexandreBayen,andYiWu.
[22] StuartLloyd.1982. LeastsquaresquantizationinPCM. IEEEtransactionson 2021. TheSurprisingEffectivenessofPPOinCooperative,Multi-AgentGames.
informationtheory28,2(1982),129â€“137. arXiv:2103.01955[cs.LG]
[23] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch. [42] JinZhang,JianhaoWang,HaoHu,TongChen,YingfengChen,ChangjieFan,
2017. Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnviron- and Chongjie Zhang. 2021. Metacure: Meta reinforcement learning with
ments.InNIPS. empowerment-drivenexploration.InICML.PMLR,12600â€“12610.
[24] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019. [43] LuluZheng,JiaruiChen,JianhaoWang,JiaminHe,YujingHu,YingfengChen,
MAVEN:Multi-AgentVariationalExploration.InNeurIPS,Vol.32.7613â€“7624. ChangjieFan,YangGao,andChongjieZhang.2021.EpisodicMulti-agentRein-
[25] GeorgOstrovski,MarcGBellemare,AÃ¤ronOord,andRÃ©miMunos.2017.Count- forcementLearningwithCuriosity-drivenExploration.NeurIPS34(2021).
basedexplorationwithneuraldensitymodels.InICML.PMLR,2721â€“2730. [44] LuisaZintgraf,KyriacosShiarlis,MaximilianIgl,SebastianSchulze,YarinGal,
[26] SimoneParisi,VictoriaDean,DeepakPathak,andAbhinavGupta.2021.Interest- KatjaHofmann,andShimonWhiteson.2020. VariBAD:AVeryGoodMethod
ingobject,curiousagent:Learningtask-agnosticexploration.NeurIPS34(2021), forBayes-AdaptiveDeepRLviaMeta-Learning.InICLR.Appendix
ğ‘‡
A PROOFS
J =âˆ’
2ğœ2
ğ‘“0(ğ‘Š0+2ğµ+ğ·âˆ’ğ‘Ÿ)2
ğ‘’
(cid:16)
A.1 ProofofLemmaonEquivalentOptimality +ğ‘“1(ğ‘Š1+ğµ+ğ¶+ğ·)2
L spe am cem |Aa |A =.1 ğ‘ˆ. aIn ndth re ew2- aa rg de mnt aC trl ii xmbGamewithsingle-agentaction + ğ‘“2(ğ‘Š2+2ğ¶+ğ·âˆ’ğ‘Ÿ(1âˆ’ğ›¿))2
1 (cid:17)
ğ‘Ÿ 0 Â·Â·Â· 0 âˆ’ ğ‘Š2+2ğ‘šğ‘Š2+ğ‘š2ğ‘Š2 (10)
2ğœ2 0 1 2
0 ğ‘Ÿ(1âˆ’ğ›¿) Â·Â·Â· ğ‘Ÿ(1âˆ’ğ›¿) ğ‘¤
(cid:16) (cid:17)
ğ‘…= Â© 0. . . ğ‘Ÿ(1. . . âˆ’ğ›¿) Â·. Â·. Â·. ğ‘Ÿ(1. . . âˆ’ğ›¿)Âª Â® Â®, Further,let ğ¾0=2ğµ+ğ·âˆ’ğ‘Ÿ
(ğ‘¡)
Â®
Â®
ğ¾1=ğµ+ğ¶+ğ·
f to ryr ia nn gy ace tx iop nlor (a ğ‘–,ti ğ‘—o )n
aÂ«
tp to il mic ey sğ‘ teğ‘’ pw ğ‘¡,h ge ir ve eğ‘ nğ‘’ the(ğ‘– o, bğ‘— j) ecis tivt Â¬h ee fup nr co tb ioa nbility of ğ¾2=2ğ¶+ğ·âˆ’ğ‘Ÿ(1âˆ’ğ›¿)
andimmediately
J(ğ‘‡)(W,b,c,ğ‘‘)
ğ¾0+ğ¾2=2ğ¾1âˆ’ğ‘Ÿ(2âˆ’ğ›¿) (11)
ğ‘‡
= E (ğ‘–,ğ‘—)âˆ¼ğ‘ğ‘’(ğ‘¡) logN(ğ‘(eğ‘–,eğ‘—;W,b,c,ğ‘‘)âˆ’ğ‘…ğ‘–ğ‘—;0,ğœ ğ‘’2) Followingequation(10),
+Ã• lğ‘¡ o= g1
N(ğ‘Š;0,ğœ
ğ‘Š2(cid:2)
ğ¼)+Constant
(cid:3)
(8)
(cid:18)2 ğœ•ğ‘Šğœ•
0
+ ğœ•ğ‘Šğœ•
1
âˆ’ ğœ•ğœ•
ğµ
(cid:19)J =0â‡’ğ‘Š0=âˆ’ğ‘šğ‘Š1
maximizedbyparametersWâˆ—,bâˆ—,câˆ—,ğ‘‘âˆ—,thejointQfunction ğœ• ğœ• ğœ•
ğ‘(eğ‘–,eğ‘—;Wâˆ—,bâˆ—,câˆ—,ğ‘‘âˆ—)isequivalentlyoptimalifthefollowingcrite-
(cid:18)2
ğœ•ğ‘Š2
+
ğœ•ğ‘Š1
âˆ’
ğœ•ğ¶
(cid:19)J =0â‡’ğ‘Š1=âˆ’ğ‘šğ‘Š2
rionh ğ‘Ÿo ğ›¿ld â‰¥s
ğ‘“2
âˆ’1
ğ‘š2 ğ‘Ÿ(2âˆ’ğ›¿)
. (9)
ğœ•ğ‘Šğœ•
0J =0â‡’ğ‘Š0=âˆ’
ğ‘“0ğœ†ğ‘“0ğœ†
+1ğ¾0
(cid:18)ğ‘“0 (cid:19) ğ‘“2ğœ†+ğ‘š2 1+ 2ğ‘“ ğ‘“2 1( (ğ‘“ ğ‘“1 2ğœ† ğœ†+ +2 ğ‘šğ‘š 2) )ğ‘š + ğ‘“ ğ‘“2 0( (ğ‘“ ğ‘“0 2ğœ† ğœ†+ +1 ğ‘š)ğ‘š 2)2 ğœ•ğ‘Šğœ• 1J =0â‡’ğ‘Š1=âˆ’ ğ‘“1ğœ†ğ‘“1 +ğœ† 2ğ‘šğ¾1
Hereweuse
ğ‘“0= ğ‘‡1 ğ‘‡ ğ‘ ğ‘’(ğ‘¡) (0,0)
ğœ•ğ‘Šğœ•
2J =0â‡’ğ‘Š2=âˆ’
ğ‘“2ğœ†ğ‘“2 +ğœ†
ğ‘š2ğ¾2
Ã•ğ‘¡=1 ğœ•
J
=0â‡’ğ‘Š0+ğ¾0
=âˆ’
ğ‘“1
ğ‘“1= ğ‘‡1 Ã•ğ‘¡ğ‘‡ =1ğ‘ˆ Ã•ğ‘–=âˆ’ 11 (cid:16)ğ‘ ğ‘’(ğ‘¡) (0,ğ‘–)+ğ‘ ğ‘’(ğ‘¡) (ğ‘–,0)
(cid:17)
ğœ•ğœ• ğœ• ğ¶ğµ
J
=0â‡’ğ‘Š ğ‘Šğ‘Š1 21+ ++ğ¾ ğ¾ğ¾1 21 =âˆ’2 2 ğ‘“ğ‘“ ğ‘“ 10 2
ğ‘‡ ğ‘ˆâˆ’1ğ‘ˆâˆ’1
ğ‘šğ‘“2 == ğ‘ˆğ‘‡1
âˆ’Ã•ğ‘¡= 11 Ã•ğ‘–=1
Ã•ğ‘—=1ğ‘ ğ‘’(ğ‘¡) (ğ‘–,ğ‘—) andtogetherw ğ¾i 2th =eq 1u +at 2io ğ‘“2n (ğ‘“( 11 ğœ†1 +)
ğ‘Ÿ
2, ğ‘š(w
2
)ğ‘še âˆ’o
ğ›¿
+b )ta ğ‘“2i (n
ğ‘“0ğœ†+1)ğ‘š2
. (12)
ğ‘“1(ğ‘“2ğœ†+ğ‘š2) ğ‘“0(ğ‘“2ğœ†+ğ‘š2)
ğ‘‡ğœ2
ğœ†= ğ‘¤ Finallywededucethecriterion
ğœ2
ğ‘’ ğ‘Š0+2ğµ+ğ· â‰¥ğ‘Š2+2ğ¶+ğ·
foraclearerdemonstrationofthecriterion.
ğ‘“2
Proof. Fromthesymmetryoftheparametersandtheconcavity â‡”ğ‘Ÿğ›¿ â‰¥ (cid:18)1âˆ’ ğ‘“0(cid:19)(ğ‘Š2+ğ¾2)
oftheobjectivefunction,âˆƒğ‘Š0,ğ‘Š1,ğ‘Š2,ğµ,ğ¶,ğ·suchthat ğ‘“2 ğ‘š2 ğ‘Ÿ(2âˆ’ğ›¿)
ğ‘Š ğ‘Š0 1= =W Wâˆ— 0
âˆ—
0ğ‘–0
=W ğ‘–âˆ— 0, âˆ€ğ‘–â‰ 0
â‡”ğ‘Ÿğ›¿ â‰¥ (cid:18)ğ‘“0 âˆ’1 (cid:19) ğ‘“2ğœ†+ğ‘š2 1+ 2ğ‘“ ğ‘“2 1( (ğ‘“ ğ‘“1 2ğœ† ğœ†+ +2 ğ‘šğ‘š 2) )ğ‘š + ğ‘“ ğ‘“2 0( (ğ‘“ ğ‘“0 2ğœ† ğœ†+ +1 ğ‘š)ğ‘š 2)2 .
(cid:3)
ğ‘Š2=W ğ‘–âˆ— ğ‘—, âˆ€ğ‘–,ğ‘— â‰ 0
ğµ=bâˆ—=câˆ— A.2 ProofforTheorem4.2(uniform
0 0
ğ¶=bâˆ—=câˆ—, âˆ€ğ‘–â‰ 0 exploration)
ğ‘– ğ‘–
ğ· =ğ‘‘ Theorem4.2. Assumeğ›¿ â‰¤ 61,ğ‘ˆ â‰¥3.IntheClimbGameğº ğ‘“(2,0,ğ‘ˆ),
given the quadratic joint Q function form ğ‘(x,y;W,b,c,ğ‘‘) and a
Rewritetheobjectivefunction(8)weobtain Gaussianpriorğ‘(W) =N(W;0,ğœ2ğ¼),usingauniformexploration
ğ‘¤
policy,ğ‘ J(ğ‘‡)(W,b,c,ğ‘‘)willbecomeequivalentlyoptimalonlyafter
ğ‘‡ = Î©(|A|ğ›¿âˆ’1) steps.Whenğ›¿ = 1,ğ‘‡ =ğ‘‚(1)stepssufficetolearn
theequivalentlyoptimaljointQfunction,meaningtheinefficiency
ofuniformexplorationisduetoalargesetofsuboptimalNEs.Proof. Underuniformexploration,
1 2ğ‘š ğ‘š2 ğ‘“2 ğ‘š2 ğ‘Ÿ(2âˆ’ğ›¿)
Criterion(9)canbeğ‘“0 re= forğ‘ˆ m2 u, lğ‘“ a1 te= dğ‘ˆ to2,ğ‘“2= ğ‘ˆ2. ğ‘Ÿğ›¿ â‰¥ (cid:18)ğ‘“0 âˆ’1 (cid:19) ğ‘“2ğœ†+ğ‘š2 1+ 2ğ‘“ ğ‘“2 1( (ğ‘“ ğ‘“1 2ğœ† ğœ†+ +2 ğ‘šğ‘š 2) )ğ‘š + ğ‘“ ğ‘“2 0( (ğ‘“ ğ‘“0 2ğœ† ğœ†+ +1 ğ‘š)ğ‘š 2)2
(ğ‘š2âˆ’1)(2âˆ’ğ›¿) =
ğ‘“2
âˆ’1
ğ‘š2 ğ‘Ÿ(2âˆ’ğ›¿)
ğ›¿ â‰¥
1+ ğœ† (ğ‘š+1)2
(13) (cid:18)ğ‘“0 (cid:19) ğ‘“2ğœ†+ğ‘š2 1+ ğ‘“2(ğ‘“0 ğ‘“0ğœ† (+ ğ‘“1 2ğœ†)( +ğ‘š ğ‘š2 2+ )2ğ‘š)
andthuswithğœ†= ğ‘‡ ğœğœ ğ‘’2ğ‘¤2 ,ğ‘š(cid:16) â‰¥2,ğ›¿ğ‘ˆ2 â‰¤(cid:17) 61, = (cid:18)ğ‘“ ğ‘“2
0
âˆ’1
(cid:19)
ğ‘“2ğœ†ğ‘š +2 ğ‘š2 (ğ‘Ÿ ğ‘š(2 +âˆ’ 1ğ›¿ )) 2 (14)
ğ‘‡
â‰¥ğ‘ˆ2ğœ ğ‘’2 (ğ‘š2âˆ’1)(2âˆ’ğ›¿)
âˆ’1
â‰¥(ğ‘š2âˆ’1) ğœ†ğ‘š +ğ‘š2
2
(ğ‘Ÿ ğ‘š(2 +âˆ’ 1ğ›¿ ))
2
ğœ2 (ğ‘š+1)2
ğ‘¤ (cid:18) (cid:19)
Similartoinequality(13),thisyieldsto
ğ‘ˆ2ğœ2 3 6
â‰¥ ğ‘’ âˆ’
ğœ ğ‘¤2 (cid:18)ğ›¿ ğ›¿ (cid:19) ğ‘š2
=ğ‘ˆ2ğœ ğ‘’2 ğœ† â‰¥
6ğ›¿
(15)
6ğœ ğ‘¤2ğ›¿ Followinginequality(14),wefurtherget
Ontheotherhand,innon-penaltyClimbGamewhereğ›¿ =1,if ğ‘“2 ğ‘š2 ğ‘Ÿ(2âˆ’ğ›¿)
w
pat
ae
ra
i
agn mhy
s
et tai em
c rt
se
io
rs
n
et le a(p
ğ‘
teğ‘–âˆƒ d,ğ‘( tğ‘–
ğ‘—
o, )ğ‘— ()
m
ğ‘–,â‰ 
o
ğ‘—)re( w0 t,
ih
t0 ha)
n
tw
ht
ohh seer rae
ec
lt
t
ah
i to
ee
n
djo
( tğ‘
oin
0
(t
,
0ğ‘Q
,0 0)
)f ,u ajn
u
nc
s
dt ti to
s
hwn eağ‘
op
bJ jt( ehğ‘‡ ce)
-
ğ‘Ÿğ›¿ â‰¥
â‰¥(cid:18)
ğ‘“2ğ‘“0 âˆ’ ğ‘š1
2(cid:19)
ğ‘“2 ğ‘Ÿğœ†+ğ‘š2 (ğ‘š+1)2
tivefunctionJ(ğ‘‡)willbeincreased,whichmakesacontradiction. 2ğ‘“0ğœ†+ğœ†4ğ‘š2
ğ‘Ÿ
Hence,ğ‘‡ =1sufficesforthenon-penaltyClimbGame. â‰¥ ,
(cid:3) 16ğ‘“0ğœ†
whichis
A.3 ProofforTheorem4.3(ğœ–-greedy
ğ›¿âˆ’1
ğœ† â‰¥ (16)
exploration) 16ğ‘“0
Forfixedğœ–,
Theorem4.3 Assumeğ›¿ â‰¤ 31 2,ğ‘ˆ â‰¥ max(4,ğœğ‘¤ğœ ğ‘’âˆ’1).IntheClimb
ğœ– 1 ğœ–
Gameğº ğ‘“(2,0,ğ‘ˆ),giventhequadraticjointQfunctionform ğ‘“0 â‰¤
ğ‘ˆ2
+
ğ‘‡ğ‘ˆ2
â‰¤
ğ‘ˆ2
+ğœ†âˆ’1,
ğ‘(x,y;W,b,c,ğ‘‘) andaGaussianpriorğ‘(W) = N(W;0,ğœ2ğ¼),un-
ğ‘¤ andfurther
derğœ–-greedy exploration with fixed ğœ– â‰¤ 1 2,ğ‘ J(ğ‘‡)(W,b,c,ğ‘‘) will
ğ›¿âˆ’1
becomeequivalentlyoptimalonlyafterğ‘‡ =Î©(|A|ğ›¿âˆ’1ğœ–âˆ’1)steps.If ğœ† â‰¥
ğœ–(ğ‘¡) =1/ğ‘¡,itrequiresğ‘‡ =exp Î© |A|ğ›¿âˆ’1 explorationstepstobe
16( ğ‘ˆğœ–
2
+ğœ†âˆ’1)
equivalentlyoptimal. ğ‘ˆ2 ğ›¿âˆ’1 ğ‘ˆ2ğ›¿âˆ’1
(cid:0) (cid:0) (cid:1)(cid:1) â‡’ğœ† â‰¥ âˆ’1 â‰¥
ğœ– 16 32ğœ–
Proof. Underthecircumstanceshere,afterthefirststepofuni- (cid:18) (cid:19)
formexploration,thesub-optimalpolicywillbeusedforğœ–-greedy whichshowsthat
exploration.Thenforbothfixedğœ– orlinearlydecayingğœ–,thefol- ğ‘‡ =Î˜(ğœ†) =Î©(ğ‘ˆ2ğ›¿âˆ’1ğœ–âˆ’1).
lowingalwaysholds: Whenğœ– = 1,
ğ‘‡
ğ‘“ğ‘“ 01 =2ğ‘š ğ‘“0 â‰¤ ğ‘ˆ1
2
Ãğ‘‡ ğ‘¡ ğ‘‡=1ğ‘¡1 â‰¤ 2l ğ‘ˆog 2ğ‘‡(ğ‘‡)
ğ‘“2
â‰¥max(2,ğ‘š2)
andfurther
ğ‘“0
ğœ† â‰¥
ğ›¿âˆ’1
ğ‘“2 â‰¥min(1âˆ’ğœ–,ğ‘š2ğ‘ˆâˆ’2) â‰¥ 1 . 162l ğ‘ˆog 2ğ‘‡(ğ‘‡)
2
ğ‘ˆ2ğœ2
Thenitcanbederivedfromcriterion9that â‡’log(ğ‘‡) â‰¥ ğ‘’
32ğœ2ğ›¿
ğ‘¤
whichshowsthat
ğ‘‡ =exp(Î©(ğ‘ˆ2ğ›¿âˆ’1)).
(cid:3)A.4 ProofforTheorem4.4(Structured (cid:3)
exploration)
Youmayrefertothefollowinglemmawhichisusedintheabove
Theorem4.4 IntheClimbGameğº ğ‘“(2,0,ğ‘ˆ),giventhequadratic proof.
jointQfunctionformğ‘(x,y;W,b,c,ğ‘‘)andaGaussianpriorğ‘(W)=
ğ‘N J( (W ğ‘‡); (0 W,ğœ ,ğ‘¤2 b,ğ¼ c) ,, ğ‘‘u )n id se er qs utr iu vc at lu enre td lye ox pp tlo imra at lio an tğ‘ stğ‘’( eğ‘¡ p) ( ğ‘‡ğ‘–, =ğ‘—) ğ‘‚= (ğ‘ˆ 1)âˆ’ .1 (cid:2)1 ğ‘–=ğ‘— (cid:3), LemmaA.3. âˆ€ğ‘˜ ğ‘’> âˆ’1 ğ‘˜6 ğ‘¥,ğ‘¥ â‰¤> lo0,
g ğ‘¥1
+
ğ‘¥ +ğ‘¥
(18)
1logğ‘˜ 1logğ‘˜ ğ‘˜
Proof. ItiseasytoverifythatW=c=0,b= (1,0,....,0)âŠ¤,ğ‘‘ = 2 2
0isthelearnedparameterthatmaximizesboththepriorofWand Proof. Letğ‘¥0 = 2lo ğ‘˜gğ‘˜ .Wecanproveinequality18byproving
thelikelihoodofpredictionerroratanystepğ‘‡.Thisparametercon- thefollowingthreeconditions.
figurationdirectlygives thejointğ‘„ functionthatisequivalently
ğ‘¥
optimal. 1.âˆ€ğ‘¥ â‰¥ğ‘¥0, â‰¥ğ‘’âˆ’ğ‘˜ğ‘¥ (19)
(cid:3) ğ‘˜
log1
A.5 ProofforTheorem5.1(Explorationduring 2.âˆ€0â‰¤ğ‘¥ â‰¤ğ‘¥0, 1logğ‘¥
ğ‘˜
â‰¥ğ‘’âˆ’ğ‘˜ğ‘¥ (20)
Meta-Testing) 2
1
DefinitionA.2(ğœ–Generalization). Supposetherearetrainingand 3.âˆ€ğ‘¥ >0,ğ‘¥+log â‰¥0. (21)
ğ‘¥
testing data from the same space S. Letğ‘”(ğ‘¥,ğ‘¦) âˆˆ {0,1} denote
Toprove(19),itsufficestoshow
whetheraexplorationpolicytrainedontrainingsampleğ‘¥canlearn
toexploreğ‘¦duringtestingtime.Andwealwaysassumeğ‘”(ğ‘¥,ğ‘¦) =
ğ‘¥0
=
2logğ‘˜
â‰¥
1
=ğ‘’âˆ’ğ‘˜ğ‘¥0,
ğ‘”(ğ‘¦,ğ‘¥). Then we say a exploration policygeneralizes to explore
ğ‘˜ ğ‘˜2 ğ‘˜2
ğœ– nearbygoalsifforeverytrainingsampleğ‘¥,âˆƒaneighbourhood as ğ‘¥ ismonotoneincreasingandğ‘’âˆ’ğ‘˜ğ‘¥ ismonotonedecreasing.
ğ‘˜
|Î©(ğ‘¥)| â‰¥ğœ– ofğ‘¥ s.t.âˆ€ğ‘¦ âˆˆ Î©(ğ‘¥),ğ‘”(ğ‘¥,ğ‘¦) =1.Intuitively,thatmeans Nowweprove(20).Let
theexplorationpolicieslearnstoexploreğœ–nearbyregionofevery log1
trainingsample. ğ‘“(ğ‘¥) = ğ‘¥ âˆ’ğ‘’âˆ’ğ‘˜ğ‘¥.
1logğ‘˜
2
Theorem5.1[ExplorationduringMeta-Testing] Considergoal-
Since
orientedtaskswithgoalspaceG âŠ†S.Assumethetrainingandtest- 2
ğ‘¥ğ‘“â€²(ğ‘¥)=ğ‘˜ğ‘¥ğ‘’âˆ’ğ‘˜ğ‘¥ âˆ’
inggoalsaresampledfromthedistributionğ‘(ğ‘¥) on G,thedataset logğ‘˜
hasğ‘ i.i.d.goalssampledfromadistributionğ‘(ğ‘¥)onS.Iftheexplo- isincreasing forğ‘¥ âˆˆ (0,1/ğ‘˜) and decreasing forğ‘¥ âˆˆ (1/ğ‘˜,+âˆ),
rationpolicygeneralizestoexploreğœ–nearbygoalsforeverytraining âˆƒ0<ğ‘¥1 <1/ğ‘˜ <ğ‘¥2s.t.ğ‘“â€²(ğ‘¥) >0â‡”ğ‘¥1 â‰¤ğ‘¥ â‰¤ğ‘¥2.Hereğ‘¥1,ğ‘¥2are
sample,wehavethatthetestinggoalisnotexploredwithprobability twosolutionsofğ‘˜ğ‘¥ğ‘’âˆ’ğ‘˜ğ‘¥ âˆ’2/logğ‘˜ =0.
atmost
Therefore,toprove(20),itsufficestoshowğ‘“(ğ‘¥1) â‰¥0andğ‘“(ğ‘¥0) â‰¥
ğ‘ƒ â‰ˆ ğ‘(ğ‘¥)(1âˆ’ğœ–ğ‘(ğ‘¥))ğ‘ğ‘‘ğ‘¥ â‰¤ğ‘‚
ğ¾ğ¿(ğ‘||ğ‘)+H(ğ‘)
. (17) 0,andthelateronecanbeverifiedas
H tiner
uef oa uwil
se
.mâˆ«
aketheassumptionthatğœ–
iss(cid:18) mallalo ng d( ğ‘”ğœ–ğ‘ is) Lipsch(cid:19)
itzcon-
ğ‘“(ğ‘¥0)=lo
1
2g l2 ol goğ‘˜ g ğ‘˜ğ‘˜
âˆ’.
ğ‘˜1
2
log2 1
Proof. Foranytestinggoalğ‘¥,everytrainingsampleğ‘¡ âˆˆ Î©(ğ‘¥) â‰¥ âˆ’
1logğ‘˜ ğ‘˜2
enablestheexplorationpolicytoexploreğ‘¦duringtestingtime.As 2
Î©(ğ‘¥)isaneighborhoodofğ‘¥andğ‘”isLipschitzcontinuous,wecan â‰¥0
selectğœ– trainingsamplesğ‘¡ fromÎ©(ğ‘¥) thatisclosesttoğ‘¥,andwe Sinceğ‘¥1 <1/ğ‘˜,wehave
thinkthosesamplesğ‘¡ hasasimilarsamplingdensityfunction,i.e.,
ğ‘”(ğ‘¡) â‰ˆ ğ‘”(ğ‘¥). Thus, with ğ‘ i.i.d samples, there is approximately
2
=ğ‘˜ğ‘¥1ğ‘’âˆ’ğ‘˜ğ‘¥1 >
ğ‘˜ğ‘¥1
â‡’ğ‘¥1 <
1
.
logğ‘˜ ğ‘’ 2
(1âˆ’ğœ–ğ‘(ğ‘¥))ğ‘ probabilitythattestinggoalsğ‘¥ willnotbeexplored
Thus,
duringthetestingtime.Thenwehave
ğ‘ƒ fail â‰ˆ ğ‘(ğ‘¥)(1âˆ’ğœ–ğ‘(ğ‘¥))ğ‘ğ‘‘ğ‘¥
lo2
gğ‘˜ =ğ‘˜ğ‘¥1ğ‘’âˆ’ğ‘˜ğ‘¥1 >ğ‘˜ğ‘¥1(1âˆ’ğ‘˜ğ‘¥1) >
ğ‘˜ 2ğ‘¥1
â‡’ğ‘¥1 <
ğ‘˜lo4
gğ‘˜
âˆ«
â‰¤ ğ‘(ğ‘¥)ğ‘’âˆ’ğ‘ğœ–ğ‘(ğ‘¥)ğ‘‘ğ‘¥
âˆ«
log 1 +2ğ‘(ğ‘¥)+16
ğ‘(ğ‘¥)
â‰¤ ğ‘(ğ‘¥) ğ‘‘ğ‘¥
1log(ğœ–ğ‘)
âˆ« 2
ğ¾ğ¿(ğ‘||ğ‘)+H(ğ‘)
=ğ‘‚
log(ğœ–ğ‘)
(cid:18) (cid:19)and theobservationandcanonlybeinferredfromthereceivedreward.
log 1 AtaskspaceTMPE ={ğºÂ¯(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ,ğ¿) |ğ‘˜ =ğ‘›,0â‰¤0<ğ‘ˆ,ğ¿âˆ¼Î¨ğ‘ˆ}
ğ‘“(ğ‘¥1)= 1logğ‘¥1
ğ‘˜
âˆ’ğ‘’âˆ’ğ‘˜ğ‘¥1 consistsofallğ‘› M,ğ‘ˆ
PEclimbgameswithğ‘›playersandğ‘ˆ landmarks,
2 andisfullycooperativebysettingğ‘˜ =ğ‘›.
logğ‘˜logğ‘˜ WeevaluateMESAonthe2-agenttasks(TMPEandTMPE)and
â‰¥ 4 âˆ’1 2,5 2,6
1 2logğ‘˜ 3-agenttasks(T 3,M 5PEandT 3,M 6PE)whilefixingğ‘˜ =2.Thetaskdistri-
logğ‘˜+loglogğ‘˜âˆ’log4
butionğ‘(T)isdefinedbytheprobabilitydensityfunctionğ‘(ğºÂ¯(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ,ğ¿)) =
=
1logğ‘˜
âˆ’1 ğ‘ˆâˆ’1Î¨ğ‘ˆ(ğ¿).
2 Wesetğ›¿ = 1.Theenvironmentsaredifferentinlandmarkposi-
â‰¥0. 2
tions,andthetasksaredifferentintargetlandmarks.
Finally,(21)isdirectfromthefactğ‘’ğ‘¥ â‰¥ğ‘¥. (cid:3)
B.1.3 Multi-agent MuJoCo Domain. In the multi-agent MuJoCo
Swimmer environment, we similarly define a climb game ğº(ğ›¼)
B ADDITIONALEXPERIMENTDETAIL
wherethereare2agentsandtheangularstatesofthetwojoints
B.1 EnvironmentSettings
determinethecurrentactionofthetwoagents.Specifically,ifstate
B.1.1 Climb Game Variants. (i) One-step climb game. A one- ğ‘ correspondstothetwojointsforminganglesofğ›¼1andğ›¼2,then
timeclimbgameğº(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ)isağ‘›-playermatrixgamewhereev- therewardcanbedefinedas:
eryplayerhasğ‘ˆ actionstochoosefrom.Therewardisdetermined 1, if|ğ›¼1âˆ’ğ›¼| <ğœ–,|ğ›¼2âˆ’ğ›¼| <ğœ–
bythenumberofplayerswhochooseactionğ‘¢,whichcanbede- ğ‘…(ğ‘ ,ğ’‚)= 1âˆ’ğ›¿, if|ğ›¼1âˆ’ğ›¼| >ğœ–,|ğ›¼2âˆ’ğ›¼| >ğœ–
finedas ï£±ï£´ï£´ï£´ï£²0,
otherwise,
1, if#ğ‘¢=ğ‘˜,
ğ‘…(ğ’‚)= ï£±ï£´ï£´ï£´ï£²1 0,âˆ’ğ›¿, i of t# hğ‘¢ er= wi0 s,
e.
w ğ›¼anh <ge lr
e
3e
s
0â—¦ğœ–
b }e
.i ts
w
Wa
e
eev ane lsr
âˆ’
oy
3
s0sï£´ï£´ï£´
ï£³
em
d
tea ğ›¿gll =rea en 1sg .l ae n. dT 3h 0e dt ea gs rk ees sp ,a ic .ee .,c Ton =sis {t ğ›¼s o |f âˆ’t 3a 0r â—¦ge <t
2
wh Ter he eğ›¿ taâˆˆ sk( s0 p,1 a) c. eToneconï£´ï£´ï£´
ï£³tainsall2-playerone-stepclimbgames
B.2 HyperparameterandComputationSettings
ğ‘ˆ
withğ‘ˆ actionsforeachplayer,i.e.,Tone={ğº(2,ğ‘˜,ğ‘¢,ğ‘ˆ) |1â‰¤ğ‘˜ â‰¤ The hyperparameters are detailed in Table 1. All tasks are sam-
ğ‘ˆ
ğ‘›,0â‰¤ğ‘¢ <ğ‘ˆ}. pleduniformlyatrandomfromthetaskspacedetailedinSection
(ii)Multi-stageclimbgame.Amulti-stageclimbgame 6andthendividedintothetrainingandtestingtasks.Weusedif-
ğºË†(ğ‘†,ğ‘›,[(ğ‘˜ğ‘¡,ğ‘¢ğ‘¡)]ğ‘† ğ‘¡=1,ğ‘ˆ)isanğ‘†-stagegame,whereeachstageğ‘¡itself ferenttasksforthemeta-trainingstage,whichincludesthehigh-
isaone-stepclimbgameğº(ğ‘›,ğ‘˜ğ‘¡,ğ‘¢ğ‘¡,ğ‘ˆ).Atstageğ‘¡,agentğ‘–isgiven rewarddatasetcollectionandthetrainingoftheexplorationpoli-
theobservation O ğ‘–ğ‘¡ = [ğ‘¡,â„ğ‘¡âˆ’1],whereâ„ğ‘¡âˆ’1 isthehistoryofthe cies. Weevaluate themeta-trained explorationpoliciesonnovel
jointactions. meta-testingtasksover3runswithdifferent seeds,eachconsist-
ThetaskspaceTmulticonsistsofall2-playermulti-stageclimb ingofadifferent set ofmeta-testing tasks.Computationisdone
ğ‘†,ğ‘ˆ
gamewithğ‘†stagesandğ‘ˆactions,i.e.,T ğ‘†m ,ğ‘ˆulti=ğºË†(ğ‘†,2,[(ğ‘˜ğ‘¡,ğ‘¢ğ‘¡)]ğ‘† ğ‘¡=1,ğ‘ˆ) | ona32-coreCPUwith256GBRAMandanNVIDIAGeForceRTX
3090.
âˆ€ğ‘¡ 1 â‰¤ ğ‘˜ğ‘¡ â‰¤ ğ‘›,0 â‰¤ğ‘¢ğ‘¡ <ğ‘ˆ}.WechooseT 5,m 10ulti (10-Multi)forthe
experiments. B.3 BaselineMethods
Inallexperimentsğ›¿ issetto 1.WeuseToneandTmultiinour
2 10 5,10 ForMAPPO[41]andRandomNetworkDistillation[5],weusethe
experiments.The taskdistributionğ‘(T) isuniformover thetask
releasedcodebase1.
space. Ten training tasks aresampled fromthetaskdistribution,
ForQMIX[33]andMAVEN[24],weusethereleasedcodebase2.
and threetesting tasksthat are different fromthe training tasks
InMA-MuJoCoenvironments,wediscretizetheactioninto11even
arechosentoevaluatetheperformance.
pointsforQMIX.
B.1.2 MPEDomain. InaMPEClimbGameğºÂ¯(ğ‘›,ğ‘˜,ğ‘¢,ğ‘ˆ,ğ¿),there ForMAESN[14],wemodifythesingle-agentversiontoamulti-
areğ‘ˆnon-overlappinglandmarksonthemapwithpositions{ğ¿ğ‘—}ğ‘ˆ ğ‘—=âˆ’ 01. agentversionbytreatingagentsasindependentlyoptimizingtheir
Weassumeadistributionğ¿ âˆ¼ Î¨ğ‘ˆ fromwhichthelandmarkposi- ownreturnswithoutconsiderationoftheotheragents.
tionsğ¿.Therewardisdeterminedbythenumberofagentslocating
ForEMC[29],weusethereleasedcodebase3.
ontheğ‘¢-thlandmark.Moreformally,supposeğ‘“ğ‘—(ğ‘ )isthenumber Formeta-training-based methods,wepretrainapolicy(condi-
ofagentslocatingontheğ‘—-thlandmark,therewardcanbedefined tioned or unconditioned) with the same configuration of MESA
as (numberoftasks,numberoftrainingsteps)andthendeployiton
1, ifğ‘“ğ‘¢(ğ‘ )=ğ‘˜and ğ‘ˆ ğ‘—=âˆ’ 01ğ‘“ğ‘—(ğ‘ )=ğ‘›, t mh ae tm ioe nta o- fte thst ein tag st kas gk o. aT lh (ke eg yo aa cl- tc ioo nnd foit rio cn lie md bp go ali mcy e,ta kk ee ys lt ah ne di mnf ao rr k-
ğ‘…(ğ‘ ,ğ’‚) = ï£±ï£´ï£´ï£´ï£²01 ,âˆ’ğ›¿, oif thğ‘“ğ‘¢ e( rğ‘  w) i= se0
.
and ÃÃğ‘ˆ ğ‘—=âˆ’ 01ğ‘“ğ‘—(ğ‘ )=ğ‘›,
1 2h ht tt tp ps s: :/ // /g gi it th hu ub b. .c co om m/ /m Ana url jb Men ahch am janar Ok x/o f/n M-p Ao Vli Ec Ny
3https://github.com/kikojay/EMC
The observationï£´ï£´ï£´of agent ğ‘– contains the relative positions of all
landmarksandotï£³heragents.Asbefore,ğ‘¢andğ‘˜willnotbegiveninHyperparameter Value
off-policyMARLAlgorithmğ‘“ MADDPG
50kinOne-stepClimbGame
100kinMulti-stageClimbGame
Meta-trainingsteps
3MinMPE
3MinMA-MuJoCo
30KinClimbGame
High-rewarddatacollectionsteps 500kinMPE
1MinMA-MuJoCo
10inClimbGame
Meta-trainingtasksize
30inMPEandMA-MuJoCoSwimmer
50kinOne-stepClimbGame
100kinMulti-stageClimbGame
Meta-testingsteps
3M/6MinMPE
2.5MinMA-MuJoCoSwimmer
5inClimbGame
Meta-testingtasksize 15,18in5-agentMPEand6-agentMPE
6inMA-MuJoCoSwimmer
3000stepsinClimbGame
Randomexploration
50KstepsinMPEandMA-MuJoCo
RecurrentNeuralNetwork
Networkarchitecture
(oneGRUlayerwith64hiddenunits)
â˜…
Thresholdğ‘… 1(sparse-rewardtasks)
Relabelğ›¾ 0.05
Decreasingfunctionğ‘“ 1/ğ‘¥5
ğ‘‘
DistanceMetrickÂ·k L2norm
F
4inClimbGame
NumberofExplorationPolicies 2/4inMPE
2inMA-MuJoCo
Learningrate 5e-3/1e-4(Adamoptimizer)
32trajectoriesinClimbGame
Batchsize 300trajectoriesinMPE
8trajectoriesinMA-MuJoCo
Table1:HyperparametersusedinMESA
B.4 ResultsOnAllEnvironments
Table2 gives the final performance foreach algorithmin all en-
vironments.WeobservethatourproposedMESAoutperformsall
otherbaselinemethodsacrossallenvironments.
C VISUALIZATIONOFLEARNED
EXPLORATION POLICIES
We visualize two exploration policiesin the 2-agent 3-landmark
Figure8:Visualizationoftwolearnedexplorationpoliciesin MPEClimbGametasks.BothexplorationpoliciesareshowninFig-
2-agent3-landmarkMPEtasks.Thefigureshowsthecritical ure8.Inaddition,thelearnedpolicyvisitedthethreelandmarks
stepsinthetrajectorywhereagentscoordinatelyreachhigh- within20timesteps,lessthanathirdofthelengthofthetrajectory,
rewardingstatesthatwerepreviouslycollected. whichshowcasesitsabilitytoquicklycoverthecollectedpromis-
ingsubspace.Bothpoliciessuccessfullyvisitedallthreelandmarks
consecutivelyandwithinonly1/3oftheepisodelength.
idforMPEDomain,keyanglesforMA-MuJoCo)asadditionalob-
servation.WeadaptC-BET[26]tomulti-agentbasedonMAPPO D ABLATION STUDIES
[41].
Tofigureouttheextenttowhichthehigh-rewardingstate-action
datasetMâˆ— andthetrainedexplorationpoliciescontributetothe
overallbetterperformance,weperformanablationstudyonthe
MPEdomain.One-step Multi-stage MA-MuJoCo
2A5LMPE 2A6LMPE 3A5LMPE 3A6LMPE
ClimbGame ClimbGame Swimmer
MESA 0.83Â±0.20 0.81Â±0.06 61.32Â±8.24 58.73Â±10.16 51.83Â±13.37 44.71Â±15.92 599.32Â±35.93
MADDPG 0.74Â±0.25 0.50Â±0.00 21.09Â±23.07 19.44Â±18.41 5.45Â±6.49 3.16Â±5.73 499.33Â±0.52
MAPPO 0.50Â±0.00 0.54Â±0.05 24.42Â±2.99 27.57Â±4.86 10.12Â±5.83 13.52Â±2.91 496.88Â±1.98
MAPPO-RND 0.50Â±0.00 0.53Â±0.04 24.05Â±2.45 27.10Â±5.67 17.18Â±3.52 7.30Â±2.76 496.36Â±1.76
QMIX 0.50Â±0.00 0.50Â±0.00 0.21Â±0.04 0.48Â±0.12 0.06Â±0.02 0.04Â±0.03 499.97Â±0.03
MAVEN 0.50Â±0.00 0.50Â±0.00 0.06Â±0.03 0.13Â±0.10 0.06Â±0.05 0.07Â±0.00 495.41Â±2.63
EMC 0.50Â±0.00 0.50Â±0.00 50.49Â±17.26 50.61Â±17.41 36.63Â±0.11 36.47Â±0.28 499.66Â±0.20
Pretrain 0.55Â±0.00 0.55Â±0.00 23.26Â±2.19 29.81Â±4.97 17.91Â±2.63 17.72Â±5.28 496.49Â±1.33
CBET 0.55Â±0.00 0.55Â±0.00 23.69Â±1.01 28.37Â±3.59 19.85Â±5.05 19.80Â±4.36 497.54Â±1.49
Goal 0.55Â±0.00 0.55Â±0.00 31.61Â±0.90 32.14Â±0.10 30.60Â±1.03 30.44Â±0.60 498.13Â±0.45
Table2:Summaryoffinalperformanceforallthealgorithmsevaluatedonalltheenvironments.Numbersinboldindicate
thebestperforminginaparticularenvironment.Forsimplicity,2A5LMPEstandsfor2-agentMPEwith5landmarksandthe
sameforotherMPEresults.
WeobserveinFigure9thatbyinitializingthebufferwithMâˆ—,
the training process is accelerated (from 1M steps to 2M steps).
However,eventhoughMâˆ—containsthecollectedhigh-rewardstates,
directlylearningtheintrinsicstructureandgeneralizingtounseen
meta-testingtasksisnontrivial.Hence,MADDPGwithMâˆ—initial-
izationstillfailstolearntheoptimalNE.
Butwhenassistedwiththetrainedexplorationpolicies,thealgo-
rithmisabletofindtheglobaloptimumwhilealsotrainingfaster
than the vanilla MADDPG. The greater variance of the ablated
methodalsosuggeststhatMâˆ—containsusefulbutsubtleinforma-
tionforlearning,andtheexplorationpolicieshelpwithextracting
thatinformation.
Figure 9: Ablation study.Comparing our approach with1)
vanilla MADDPG without using the high-rewarding state-
action dataset Mâˆ— or the trained exploration policies and
2) initializingthereplaybufferwiththeMâˆ— but notusing
theexplorationpolicies,weshowthatbothcomponentscon-
tributetooverallperformance.