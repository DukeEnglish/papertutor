Accelerating Convergence in Bayesian Few-Shot Classification
TianjunKe1 HaoqunCao1 FengZhou1
Abstract ationsintroducesuncertaintyovermodelparameters,com-
monlyreferredtoasepistemicuncertainty. Effectivelyman-
Bayesian few-shot classification has been a fo-
agingepistemicuncertaintyservestoregularizethemodel,
calpointinthefieldoffew-shotlearning. This mitigatingtheriskofoverfitting. Additionally,thisuncer-
paperseamlesslyintegratesmirrordescent-based
taintyplaysacrucialroleinassessingconfidence,especially
variationalinferenceintoGaussianprocess-based
inrisk-averseapplicationslikemedicaldiagnosis(Prabhu
few-shotclassification,addressingthechallenge
etal.,2019)andautonomousdriving(Bojarskietal.,2016).
ofnon-conjugateinference. Byleveragingnon-
Euclideangeometry,mirrordescentachievesac- TheBayesianframeworkoffersanaturalapproachtocap-
celeratedconvergencebyprovidingthesteepest tureepistemicuncertaintybyintroducingapriordistribu-
descentdirectionalongthecorrespondingmani- tion over model parameters and computing the posterior
fold. Italsoexhibitstheparameterizationinvari- using Bayesâ€™ theorem based on observed data. In recent
ance property concerning the variational distri- years,therehasbeenasignificantsurgeinresearchapplying
bution. Experimental results demonstrate com- Bayesianapproachestofew-shotlearning(Yoonetal.,2018;
petitiveclassificationaccuracy,improveduncer- Finnetal.,2018;Ravi&Beatson,2018). Leveragingthe
taintyquantification,andfasterconvergencecom- advantagesoftheBayesianframework,somerecentstudies
pared to baseline models. Additionally, we in- haveemployedGaussianprocesses(GPs)inthecontextof
vestigatetheimpactofhyperparametersandcom- few-shot classification (FSC), demonstrating competitive
ponents. Code is publicly available at https: performanceinaccuracyanduncertaintyquantification(Pat-
//github.com/keanson/MD-BSFC. acchiolaetal.,2020;Snell&Zemel,2021;Keetal.,2023).
BayesianinferenceposeschallengesforGaussianProcess
(GP)classificationduetothenon-Gaussianlikelihoodbe-
1.Introduction
ing non-conjugate to the GP prior, rendering exact poste-
Humans have the ability to learn new skills and adapt to rior computation infeasible. To address this, Patacchiola
newenvironmentsbasedonveryfewinstances. Incontrast, etal.(2020)assumesaGaussianlikelihoodforclasslabels,
most machine learning techniques, especially deep learn- achievingconjugacyininference. However,thisapproach
ing, require vast amounts of examples to achieve similar is not entirely reasonable since class labels are discrete
performanceandmayyetstruggletogeneralize. Thereason rather than continuous. Snell & Zemel (2021); Ke et al.
humansaremoreadvancedthanmachinesinadaptingand (2023)combinePoÂ´lya-Gammaaugmentation(Polsonetal.,
generalizing is that they can leverage prior experience to 2013)withtheone-vs-eachsoftmaxapproximation(Titsias,
solvenewtasks.Itisthusofgreatinteresttodesignmachine 2016)andlogistic-softmaxlikelihoodseparatelytoestablish
learningalgorithmsthatcangeneralizetonoveltaskswitha conditionallyconjugateinference. Thesemethodsprovide
limitedamountofdata. improveduncertaintyquantificationbutnecessitatethein-
troductionofadditionalauxiliaryvariables.
Few-shotclassificationfocusesontheclassificationofnew
datagivenonlyalimitednumberoftrainingsampleswith Differingfromtheapproachesmentionedearlier,weincor-
classlabels. Itprovesparticularlyusefulwhencollecting poratemirrordescent(Nemirovskij&Yudin,1985)-based
trainingexamplesischallengingorwhenannotatingdata variationalinference(Bleietal.,2017)intoGP-basedFSC
incurshighcosts. Thescarcityoflabeleddatainsuchsitu- withoutintroducinganyauxiliaryvariables. Thisapproach
is consequently named Mirror Descent based Bayesian
1CenterforAppliedStatisticsandSchoolofStatistics,Ren- Few-Shot Classification (MD-BFSC). In our method, the
min University of China. Correspondence to: Feng Zhou
optimizationwithinvariationalinferenceisaccomplished
<feng.zhou@ruc.edu.cn>.
through conjugate computation. Notably, mirror descent
Proceedings of the 41st International Conference on Machine leveragesnon-Euclideangeometry,providingthesteepest
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by descentdirectionalongthecorrespondingmanifold,thereby
theauthor(s).
1
4202
yaM
2
]GL.sc[
1v70510.5042:viXraAcceleratingConvergenceinBayesianFew-ShotClassification
enhancingtheconvergencerate. Italsoexhibitstheparam- ing tradition, we use the softmax likelihood p(y |f ) =
n n
eterizationinvariancepropertyconcerningthevariational (cid:81) exp(fc Â·yc)/(cid:80) exp(fc) with fc = fc(x ) and
c n n c n n n
distribution(Raskutti&Mukherjee,2015). f = [f1,...,fC]âŠ¤, andtheposterioroflatentfunctions
n n n
canbecomputedas:
Specifically,wemakeseveralcontributions: (1)Weintro-
ducevariationalinferencebasedonmirrordescenttoGP-
p(y|f)p(f) (cid:81) p(y |f )(cid:81) p(fc)
b ina ts oed anF oS pC ti, mth ie zr ae tib oy nt pra rn os bf lo er mm win ig thn co on n-c juo gn aju tega ct oe min pf ue tr ae tn ioc ne
.
p(f|y)=
p(y)
= (cid:82) (cid:81) nn
p(y
nn
|f
nn )(cid:81) cc p(fc)df,
(2)Asdemonstratedlater,MD-BFSCprovidesthesteepest (1)
descentdirection alongthecorrespondingnon-Euclidean wherefc =[f 1c,...,f Nc]âŠ¤,f =[f1âŠ¤,...,fCâŠ¤]âŠ¤,p(fc)=
manifold,enhancingtheconvergencerateandmaintaining N(fc|0,Kc)withK ic j =k Î·c(x i,x j).
invariancetotheparameterizationofthevariationaldistribu-
However,theexactcomputationofposteriorisinfeasible
tion. (3)Weshowcasethatourapproachachievescompeti-
sincethenon-Gaussianlikelihoodisnon-conjugatetothe
tiveclassificationaccuracyanduncertaintyquantification,
Gaussianprior. Variationalinference(VI)(Bleietal.,2017;
demonstratingafasterconvergencerateonstandardFSC
Hoffmanetal.,2013)isanapproximateinferencemethod
benchmarksincomparisontofew-shotbaselinemodels.
inwhichtheexactposteriorisapproximatedbyavariational
distributionq(f|Î¸)withÎ¸beingthevariationalparameter.
2.Background Theoptimalvariationaldistributionisobtainedbyminimis-
ing the Kullback-Leibler (KL) divergence between q and
Ourapproachinvolvesconceptsinfew-shotclassification,
theexactposterior,orequivalentlymaximizingtheevidence
GPclassification,variationalinference,exponentialfamily, lowerbound(ELBO)L(Î¸):Î˜â†’R(Bishop,2006):
naturalgradientdescentandmirrordescent. Weillustrate
thebasicconceptsineachpart. Thesamenotationindiffer-
argmaxL(Î¸)=E (cid:2) logp(y,f)âˆ’logq(f|Î¸)(cid:3) ,
entsectionsreferstothesamevariableunlessspecified. q
Î¸âˆˆÎ˜
2.1.Few-shotClassification
whereÎ˜isthesetofvalidvariationalparameters. InGP
InFSC(Snelletal.,2017b;Milleretal.,2000;Wangetal.,
classification,thevariationaldistributionqisassumedtobe
2020),aclassifiermustadapttonewclasseswhicharenot aGaussiandistribution. TheGPkernelhyperparameterÎ·
observedintraining,givenonlyafewsamplesofeachclass. canbedeterminedthroughempiricalBayesbymaximizing
ConsideraL-shotandC-wayFSCtaskswithsupportset themarginallikelihood.
S = {x ,y }L andquerysetQ = {x ,y }M ,x
s l l l=1 s m m m=1 m
istheinputfeatureandy âˆˆ{1,...,C}istheclasslabel. 2.3.ExponentialFamily
m
Wecansampledistincttasksfromadistributionovertasks
Anexponentialfamily(Wainwright&Jordan,2008)isaset
toformthetrainingdatasetD ={S ,Q }S .Similarly,
train s s s=1 ofdistributionswhoseprobabilitydensityfunctioncanbe
thevalidationdatasetD andtestdatasetD . Given
validation test expressedintheform:
anewtasksâˆ—fromthetestdatasetwithsupportandquery
set{S ,Q },ourgoalistotrainaclassifieronS based
sâˆ— sâˆ— sâˆ—
ontheregularityextractedfromD topredictthelabelof
q(f|Î¸)=h(f)exp(Î¸âŠ¤Ï•(f)âˆ’A(Î¸)),
train
samplesinQ . Thevalidationdatasetisusedfortuning
sâˆ—
hyperparameters,e.g.,learningrate. where Î¸ âˆˆ Î˜ âŠ‚ RP is a vector of natural param-
eters, Ï•(f) is a vector of sufficient statistics, A(Î¸) =
2.2.GaussianProcessClassificationwithVariational log(cid:82) h(f)exp(Î¸âŠ¤Ï•(f))df isthelog-partitionfunctionthat
Inference is convex. An exponential family is referred to as min-
imal if each distribution has a unique natural parameter.
Consideramulti-classclassificationtaskconsistingofN
Aminimalexponentialfamilydistributioncanalsobepa-
samples with the input features X = [x ,...,x ]âŠ¤ and
1 N rameterized by the mean parameter that is the mean of
thecorrespondingclasslabelsy=[yâŠ¤,...,yâŠ¤]âŠ¤,where
1 N sufficient statistics: Âµ = E [Ï•(f)] âˆˆ M âŠ‚ RP. The
x âˆˆ X âŠ‚ RD and y is the one-hot encoding for C q
n n mean parameter also can be obtained by the gradient of
classes. Themulti-classGPclassificationmodel(Williams
log-partitionfunction: Âµ=âˆ‡A(Î¸). Theconvexconjugate
& Rasmussen, 2006) includes latent GP functions for all
function(Rockafellar,2015)ofA(Î¸)isthenegativeentropy
classes, i.e., {f1,...,fC} where fc(Â·) : X â†’ R is the
functionH(Âµ) = E [logq(f|Î¸)] = Î¸âŠ¤Âµâˆ’A(Î¸)+const.
latent function for c-th class. A GP prior is applied over q
Similarly,thenaturalparametercanbeobtainedbythegra-
each latent function fc âˆ¼ GP(0,k ) where k is the
Î·c Î·c dientofnegativeentropyfunctionÎ¸ = âˆ‡H(Âµ). Thepair
GPkernelforc-thclasswithhyperparametersÎ·c. Follow-
ofâˆ‡A(Â·)andâˆ‡H(Â·)areinversefunctions.
2AcceleratingConvergenceinBayesianFew-ShotClassification
2.4.VariationalInferencewithNaturalGradient predictclasslabelsofaquerysetforanewunseentask. A
Descent bi-leveloptimizationframework,whichissimilartomodel-
agnosticmeta-learning(MAML)(Finnetal.,2017),isused
The variational inference using gradient descent can be
to learn the common GP prior (Patacchiola et al., 2020;
generalizedtothatusingnaturalgradientdescent(Amari,
Snell&Zemel,2021): intheinnerloop,VIisperformed
1997)whichtakesthenon-Euclideangeometryintoaccount,
to estimate the posterior of latent functions for each task
because each set of parameters corresponds to a distribu-
(task-specificparameters);intheouterloop,theGPkernel
tion (Salimans & Knowles, 2013; Hoffman et al., 2013;
is designed as a deep kernel (Wilson et al., 2016) whose
Hensman et al., 2013; Salimbeni et al., 2018). Natural
hyperparameters(task-commonparameters),includingthe
gradientdescentassumestheELBOismaximizedovera
kernel hyperparameters and neural network weights, are
distributionspaceandthevariationalparameterslieonaRie-
updatedbyempiricalBayes(Maritz&Lwin,2018).
mannianmanifold. Givenanexponential-familyvariational
distribution q(f|Î¸), we can denote a Riemannian metric Inthebi-leveloptimizationframework,boththeinnerand
(Fisherinformationmatrix)I(Î¸)=âˆ’E [âˆ‡2logq(f|Î¸)]= outer loops consistently employ first-order optimization
q Î¸
âˆ‡2A(Î¸)(Rissanen,1996;Hoffmanetal.,2013)toinduce methods,leadingtosluggishconvergenceinthesearchfor
aP-dimensionalRiemannianmanifold(Î˜,I(Î¸)). Tomax- task-commonparameters. Moreover,theconvergencerate
imizetheELBOontheRiemannianmanifold,wecanuse oftheinnerloopisdependentontheparameterizationofthe
thefollowingnaturalgradientupdate: variationaldistribution,andanarbitraryparameterization
mayimpedefasterconvergence. Inthisstudy,ourobjective
Î¸ t+1 =Î¸ t+Ï tIâˆ’1(Î¸ t)âˆ‡L(Î¸ t), (2) istoenhancetheconvergencerateoftheinnerloop. Impor-
tantly,wedonotanticipatetheconvergencerateoftheinner
whereÏ isthestepsize. Thenaturalgradientselectsthe
t looptobedependentontheparameterizationofthevaria-
steepestdescentdirectionalongtheRiemannianmanifold
tionaldistributionsinceinpracticeweoftenlackknowledge
andtheoptimizationpathontheRiemannianmanifoldwith
aboutwhichparameterizationissuperior. Consequently,the
infinitesimallysmallstepsisinvarianttotheparameteriza-
accelerated convergence of the inner loop also facilitates
tionofvariationaldistribution(Martens,2020).
improvementsintheconvergenceoftheouterloop.
2.5.VariationalInferencewithMirrorDescent
3.1.FromNaturalGradientDescenttoMirrorDescent
Mirror descent (Nemirovskij & Yudin, 1985) is another
Toaddresstheaboveissues,anintuitiveapproachistoopti-
generalizationofgradientdescent. Givenaspecificparame-
mizetheELBOwithnaturalgradientdescent(Amari,1997)
terizedELBOL(Âµ):Mâ†’Rthatneedstobemaximized,
whichisasecond-orderoptimizationmethodandpossesses
the gradient descent update is Âµ = Âµ + Ï âˆ‡L(Âµ )
t+1 t t t theparameterizationinvariancepropertyinthemeanwhile.
where Ï is the step size. The above update is equiva-
t Alargenumberofexistingworkappliednaturalgradient
lenttomaximizealocalquadraticapproximationofL(Âµ):
descenttothevariationalinferenceofGP(Hensmanetal.,
Âµ =argmax âˆ‡L(Âµ )âŠ¤Âµâˆ’ 1 âˆ¥Âµâˆ’Âµ âˆ¥2.Mirror
t+1 ÂµâˆˆM t 2Ït t 2 2013; Malago` & Pistone, 2015; Salimbeni et al., 2018).
descentreplacesEuclideannormwithaproximityfunction
However,aseriousdisadvantagehinderingtheapplication
Î¨(Â·,Â·):MÃ—Mâ†’R+:
ofnaturalgradientdescentisthecomplexcomputationdue
totherequirementofsecondinformation(Fisherinforma-
1
Âµ t+1 =argmaxâˆ‡L(Âµ t)âŠ¤Âµâˆ’ Ï Î¨(Âµ,Âµ t). tion matrix). To facilitate the implementation of natural
ÂµâˆˆM t
gradientdescent,weproposetousemirrordescentinplace
The proximity function characterizes the non-Euclidean ofnaturalgradientdescent. Thefollowingtheoremestab-
geometryanddifferentchoicesofÎ¨correspondtodifferent lishestheequivalencebetweennaturalgradientdescentand
manifoldsthatvariationalparameterslieon. mirrordescent. AdetailedproofisprovidedinAppendixB.
Theorem 3.1 (Raskutti & Mukherjee 2015). Given two
3.Methodology parameterized ELBO with mean parameter and natural
parameterL(cid:101)(Âµ)=L(Î¸),tomaximizetheELBO,themirror
TheGP-basedFSC(Patacchiolaetal.,2020;Titsiasetal., descentoverthemeanparameter
2020;Snell&Zemel,2021)isanemergingtopicinrecent
years,whichcombinestheBayesianframeworkwithfew- 1
shot learning. Given a large number of small but related Âµ t+1 =argmaxâˆ‡L(cid:101)(Âµ t)âŠ¤Âµâˆ’ Ï B H(Âµ,Âµ t), (3)
ÂµâˆˆM t
classification tasks, the main idea of GP-based FSC is to
performGPclassificationforeachtaskandlearnacommon using the Bregman divergence (Bregman, 1967)
GPpriorthatrepresentsthemeta-knowledge. Thisprioris B (Âµ,Âµ ) = H(Âµ) âˆ’ H(Âµ ) âˆ’ âˆ‡H(Âµ )âŠ¤(Âµ âˆ’ Âµ )
H t t t t
thenusedtotrainaGPclassifieronasmallsupportsetto inducedbythenegativeentropyfunctionH(Â·)isequivalent
3AcceleratingConvergenceinBayesianFew-ShotClassification
ğœƒ>
ğ‘˜ (ğ‘¥, ğ‘¥ )
!" # $
Acceleration
Gaussian Process
ğœƒ> % Mirror maxELBO(ğœ‚)
ğœƒ> Descent !
&
â€¦
ğœƒ>
â€™(% Bi-level Optimization Hyperparameter
ğœƒ>
â€™ Update
ğœ‚@
Figure1.TheoverviewofthetrainingprocessofMD-BSFC.Thediagramillustratesthebi-leveloptimizationprocessinvolvingan
iterativeapplicationofmirrordescentforVIandhyperparametertuning.
tothenaturalgradientdescentoverthenaturalparameter Theaforementionedtheoremassertsthattheoriginalsoft-
maxlikelihoodinEquation(1)isapproximatedbyaGaus-
Î¸ =Î¸ +Ï [âˆ‡2A(Î¸ )]âˆ’1âˆ‡ L(Î¸ ),
t+1 t t Î¸ t Î¸ t siandistribution,withitsnaturalparameteriterativelyup-
usingtheRiemannianmetricinducedbythelog-partition datedbythegradientoftheexpectationofthelog-likelihood.
functionA(Â·). Subsequently, the variational parameter is updated by
addingthenaturalparametersoftheapproximatedGaussian
Thetheoremaboveassertsthatemployingnaturalgradient distributionandtheprior.
descentoverthenaturalparameterusingFisherinformation,
The motivation for using mirror descent should now be
inducedbythelog-partitionfunction,isequivalenttoim-
clear: forthecurrentproblem,mirrordescentoverthemean
plementingmirrordescentoverthemeanparameterusing
parameterisanequivalentapproachtonaturalgradientde-
Bregmandivergenceinducedbythenegativeentropyfunc-
scentoverthenaturalparameter.Therefore,italsoexhibitsa
tion. Thenotableadvantageofmirrordescentovernatural
second-orderconvergencerateandpossessestheparameteri-
gradientdescentliesinitsrelianceononlythefirst-order
zationinvarianceproperty.Moreover,mirrordescentstream-
gradient,asopposedtothesecond-ordergradientrequired
linestheoptimizationprocessbyrequiringonlyfirst-order
bythelatter. Consequently,theimplementationofnatural
information,andtheoptimizationstepcanbesimplifiedas
gradientdescentcanbereformulatedasacomputationally
aconjugateBayesianinferencecomputation.
moreefficientmirrordescent.
3.3.Algorithm
3.2.FromMirrorDescenttoConjugateBayesian
Inference The bi-level optimization framework of MD-BFSC is de-
picted in Figure 1 and summarized as follows. In the in-
AsdemonstratedbyKhan&Lin(2017),Equation(3)can
ner loop, the variational parameters Î¸ (task-specific pa-
beadditionallystreamlinedintoaBayesianinferencewithin
rameters) are updated by using Theorem 3.2 and Equa-
aconjugatemodel. Thesubsequenttheoremestablishesthe
tion(4). Forâˆ‡ E [logp(y|f)]inEquation(4),weassume
equivalencebetweenEquation(3)andBayesianinference Âµ q
the variaitonal distribution q(f|Î¸) = (cid:81) q(fc|Î¸c). Since
withinaconjugatemodel. WerefertoKhan&Lin(2017) c
(cid:81)
p(y|f) = p(y |f ), we can compute the gradient of
forthecompleteproof. n n n
each point âˆ‡ E [logp(y |f )] separately. If we de-
Theorem 3.2 (Khan & Lin 2017). The mirror descent Âµn q(fn) n n
fine m and v to be the mean and covariance diagonal
in Equation (3) is equivalent to the following conjugate n n
(non-diagonal entries are 0) of the marginal distribution
Bayesianinference:
q(f ),thenâˆ‡ E [logp(y |f )]canbeexpressedas:
n Âµn q(fn) n n
q(f|Î¸ t+1)âˆexp(Î¸(cid:101) tâŠ¤Ï•(f))p(f|Î·),
âˆ‡ E [logp(y |f )]=g âˆ’2g âŠ™m ,
Î¸(cid:101)t =(1âˆ’Ï t)Î¸(cid:101)tâˆ’1+Ï tâˆ‡ ÂµE q[logp(y|f)]| Âµ=Âµt, (4)
Âµ( n1) q âˆ‡(fn)
E
[n logn
p(y
|fm )n
]=g
vn
,
n
Âµ( n2) q(fn) n n vn
withÎ¸(cid:101)0 =0andÎ¸
1
=Î·. Becausep(f|Î·)istheGaussian
where Âµ(1) and Âµ(2) are the two mean parame-
priorinEquation(1)thatisalsoaexponential-familydis- n n
ters of q(f ), g = âˆ‡ E [logp(y |f )] =
tribution,Î¸ t+1canbeobtainedbyconjugatecomputation (cid:104) n m (cid:105)n mn q n n
Î¸ t+1 =Î¸(cid:101)t+Î·. E q y nâˆ’ (cid:80)e cx ep x( pfn (f) n) , g vn = âˆ‡ vnE q[logp(y n|f n)] =
4AcceleratingConvergenceinBayesianFew-ShotClassification
(cid:104) (cid:105)
1 2E q ((cid:80)e cx ep x( p2 (fn fn) ))2 âˆ’ (cid:80)ex cp ex(f pn f) n , âŠ™ is the element-wise Algorithm1:MirrorDescentbasedBayesianFew-Shot
product (Opper & Archambeau, 2009). A proof of Sec- Classification
tion3.3isprovidedinAppendixC. Training:
Input: Input feature and class labels for S tasks:
Intheouterloop,theGPkernelisdesignedasadeepker-
{X }S ,{y }S
nel(Wilsonetal.,2016)andthekernelhyperparameterÎ· s s=1 s s=1
Output: GPkernelhyperparameterÎ·
includesthetraditionalkernelhyperparametersandneural
InitializeGPkernelhyperparameterÎ· andvariational
networkweights. ThehyperparameterÎ·(task-commonpa-
rameters)isupdatedbymaximizingtheELBO,thatisan
parametersÎ¸(cid:101)0 =0andÎ¸
1
=Î·;
forIterationdo
approximationoflog-marginallikelihood:
forTasksdo
argmaxL(Î·)=E (cid:2) logp(y|f)+logp(f|Î·)âˆ’logq(f)(cid:3) . #Updatetask-specificparameters
q forSteptdo
Î·
(5) UpdateÎ¸(cid:101)sbyEquation(4)andSection3.3;
t
For prediction in a new test task with support set: S = UpdateÎ¸s =Î¸(cid:101)s+Î·;
t+1 t
{X,y} and query set without labels Q = X , the label
âˆ— end
predictivedistributionofonesamplex inthequerysetis:
âˆ— #Updatetask-commonparameters
UpdateÎ·byEquation(5).
p(y =r |x ,X,y,Î·Ë†) (6a)
âˆ— âˆ— end
(cid:90) (cid:89)C end
= p(y =r |f ) q(fc |X,Y,Î·Ë†)df ,
âˆ— âˆ— âˆ— âˆ— Test:
c=1 Input: Support set S = {X,y}; query set Q = X ;
âˆ—
q(f âˆ—c |X,y,Î·Ë†) (6b) learnedhyperparameterÎ·Ë†
= (cid:90) p(fc |fc)q(fc |X,y,Î·Ë†)dfc =N(fc |Âµc,Ïƒ2c ), Output: Predictedlabels
âˆ— âˆ— âˆ— âˆ— InitializevariationalparametersÎ¸(cid:101)0 =0andÎ¸
1
=Î·Ë†;
#Updatetask-specificparameters
whereÎ·Ë† istheestimatedkernelhyperparameterfromthe forSteptdo
training dataset, f âˆ— = [f1(x âˆ—),...,fC(x âˆ—)]âŠ¤, q(fc | UpdateÎ¸(cid:101)tbyEquation(4)andSection3.3;
X,y,Î·Ë†) is a Gaussian distribution computed by The-
orem 3.2 and Equation (4). By converting the natu-
UpdateÎ¸
t+1
=Î¸(cid:101)t+Î·Ë†;
end
ral parameters into traditional parameters, N(fc|Î¸c) â†’
#Predictlabels
N(fc|mc,Î£c), we can derive Âµc = kc Kcâˆ’1mc and
âˆ— âˆ—L LL forx âˆˆX do
Ïƒ âˆ—2c =k âˆ—c âˆ—âˆ’kc âˆ—LKc Lâˆ’ L1kc Lâˆ—+kc âˆ—LKc Lâˆ’ L1Î£cKc Lâˆ’ L1kc Lâˆ—where Prâˆ— edictyâˆ— âˆ—byEquation(6).
L is the number of samples in the support set of the test end
dataset, andallthekâ€™saretherespectivekernelmatrices.
TheintegralinEquation(6a)isintractable,necessitatingthe
useofMonteCarloforcomputation. Thetrainingandtest
processofMD-BFSCissummarizedinAlgorithm1.
inBayesianmeta-learningforafaircomparison. Consistent
withthemethodologyemployedinpreviousstudies(Patac-
4.Experiments
chiolaetal.,2020),weutilizeastandardConv4architecture
(Vinyalsetal.,2016)astheunderlyingbackbone,andeval-
Inthissection,wepresenttheperformanceofMD-BFSC
uateourmodelacrosssixdistinctscenarios,encompassing
onfew-shotclassificationtasks,encompassingaccuracy,un-
1-shotand5-shotsituationsforbothin-domainandcross-
certaintyquantification,andconvergencerate. Weaddress
domain tasks. We subject our model to a comprehensive
threechallengingtasksusingbenchmarkdatasets,including
assessment against a range of baselines and state-of-the-
Caltech-UCSD Birds (Wah et al., 2011), mini-ImageNet
artmodels,includingFeatureTransfer(Chenetal.,2019),
(Ravi & Larochelle, 2017), Omniglot (Lake et al., 2011),
Baseline++(Chenetal.,2019),MatchingNet(Vinyalsetal.,
andEMNIST(Cohenetal.,2017).
2016), ProtoNet (Snell et al., 2017a), RelationNet (Sung
etal.,2018),MAML(Finnetal.,2017),DKT(Patacchiola
4.1.Accuracy
etal.,2020),BayesianMAML(Yoonetal.,2018),ABML
In this section, we present the experimental arrangement (Ravi&Beatson,2019),OVE(Snell&Zemel,2021),and
anddisclosethefindingsofourfew-shotclassificationtasks. LS(Keetal.,2023). ItisworthnotingthatDKT,LS,and
Whileweacknowledgetheexistenceofvariousconfigura- OVEsharesimilaritieswithourproposedapproachasthey
tionsforfew-shotclassification,weoptforthebasicsetup areallGP-basedmodelswithdifferentlikelihoodfunctions
5AcceleratingConvergenceinBayesianFew-ShotClassification
Table1.Accuracyperformanceforallmodelsin1-shotand5-shot5-wayfew-shotclassificationtasks.BaselineresultsarefromSnell&
Zemel(2021)andKeetal.(2023).Resultsareevaluatedover5batchesof600episodeswithdifferentrandomseeds.
CUB mini-ImageNet â†’ CUB Omniglot â†’ EMNIST
Method 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot
FeatureTransfer 46.19Â±0.64 68.40Â±0.79 32.77Â±0.35 50.34Â±0.27 64.22Â±1.24 86.10Â±0.84
Baseline++ 61.75Â±0.95 78.51Â±0.59 39.19Â±0.12 57.31Â±0.11 56.84Â±0.91 80.01Â±0.92
MatchingNet 60.19Â±1.02 75.11Â±0.35 36.98Â±0.06 50.72Â±0.36 75.01Â±2.09 87.41Â±1.79
ProtoNet 52.52Â±1.90 75.93Â±0.46 33.27Â±1.09 52.16Â±0.17 72.04Â±0.82 87.22Â±1.01
RelationNet 62.52Â±0.34 78.22Â±0.07 37.13Â±0.20 51.76Â±1.48 75.62Â±1.00 87.84Â±0.27
MAML 56.11Â±0.69 74.84Â±0.62 34.01Â±1.25 48.83Â±0.62 72.68Â±1.85 83.54Â±1.79
DKT 63.37Â±0.19 77.73Â±0.26 40.22Â±0.54 55.65Â±0.05 73.06Â±2.36 88.10Â±0.78
BayesianMAML 55.93Â±0.71 72.87Â±0.26 33.52Â±0.36 51.35Â±0.16 63.94Â±0.47 65.26Â±0.30
BayesianMAML(Chaser) 53.93Â±0.72 71.16Â±0.32 36.22Â±0.50 51.53Â±0.43 55.04Â±0.34 54.19Â±0.32
ABML 49.57Â±0.42 68.94Â±0.16 29.35Â±0.26 45.74Â±0.33 73.89Â±0.24 87.28Â±0.40
OVEPGGP(ML) 63.98Â±0.43 77.44Â±0.18 39.66Â±0.18 55.71Â±0.31 68.43Â±0.67 86.22Â±0.20
OVEPGGP(PL) 60.11Â±0.26 79.07Â±0.05 37.49Â±0.11 57.23Â±0.31 77.00Â±0.50 87.52Â±0.19
CDKT(ML)(Ï„ <1) 65.21Â±0.45 79.10Â±0.33 40.43Â±0.43 55.72Â±0.45 - -
CDKT(ML)(Ï„ =1) 60.85Â±0.38 75.98Â±0.33 35.57Â±0.30 52.42Â±0.50 - -
CDKT(PL)(Ï„ <1) 59.49Â±0.35 76.95Â±0.28 39.18Â±0.34 56.18Â±0.28 - -
CDKT(PL)(Ï„ =1) 52.91Â±0.29 73.34Â±0.40 37.62Â±0.32 54.32Â±0.19 - -
MD-BFSC 65.45Â±0.42 78.38Â±0.21 40.75Â±0.31 56.98Â±0.30 74.02Â±0.49 87.05Â±0.23
andinferencemethods. Thedefaultnumberofepochsfrom
Keetal.(2023)isemployedfortrainingMD-BSFC.During
training, we run 3 steps for the inner loop updates in all
experimentsduetoitsfastconvergenceandthenconducta1
stepupdateforthehyperparameterswithAdam. Forfurther
experimentaldetails,pleaserefertoAppendixE.
Wereporttheaverageaccuracyandstandarddeviationof (a) CUB (b) MIâ†’CUB (c) Omniâ†’EMNIST
ourmodelevaluatedon5batchesof600episodeswithdif-
ferent random seeds in Table 1. Our model achieves the Figure2.Reliabilitydiagramson5-shotclassificationwithECE
highestaccuracyin1-shotexperimentsontheCUBdataset andMCEmetrics.MIdenotesmini-ImageNetandOmnidenotes
(65.45%, in-domain) and the CUB â†’ mini-ImageNet Omniglot.Resultsarecomputedon3,000testtasks.
dataset(40.75%,cross-domain). Asforotherdatasetsand
settings, our model also achieves near-optimal or compa-
rable results. While the main strength of our method is
itstheoreticallyfastconvergence, wefindthatitsclassifi- difference. Followingtheevaluationprotocoloutlinedby
cation accuracy is comparable to state-of-the-art models, Patacchiolaetal.(2020),wecomputetheECEandMCE
demonstratingitsutilitybeyondrapidlearning. onthetestset. Thesummarizedresultsofthe5-shotexperi-
mentscanbefoundinTable2. Specifically,weachievethe
4.2.UncertaintyQuantification lowestECEvaluesonCUB(0.005,in-domain)andmini-
ImageNetâ†’CUB(0.005,cross-domain),aswellasthelow-
IntherealmofFSC,thequantificationofuncertaintyholds
estMCEvaluesonmini-ImageNetâ†’CUB(0.014,cross-
greatsignificanceduetothepotentialhighuncertaintyin
domain)andOmniglotâ†’EMNIST(0.024,cross-domain).
predictions made by classifiers trained with limited data.
While our model slightly lags behind the state-of-the-art
Particularlyinhigh-riskdomains,itiscrucialforarobust
modelsinotherscenarios,theoveralloutcomehighlights
modeltoeffectivelycharacterizesuchuncertainty.
itsremarkablereliabilityinuncertaintycalibration,thereby
Toquantifyuncertainty,weemploytwowidely-usedmet- demonstratingpromisingrobustnessinfew-shotscenarios.
rics: theexpectedcalibrationerror(ECE)(Guoetal.,2017)
ThereliabilitydiagramsdisplayedinFigure2illustratethe
andthemaximumcalibrationerror(MCE)(Ovadiaetal.,
performanceofourmodelintermsofreliability. Arobust
2019). ECEmeasurestheaveragedifferencebetweenconfi-
uncertaintyquantificationmodelshouldexhibitaconfidence
dence(probabilityoutputs)andaccuracywithinpredefined
barplotthatalignswiththediagonalline. Ourmodelclosely
bins, while MCE is similar but focuses on the maximum
adherestothediagonalline.
6AcceleratingConvergenceinBayesianFew-ShotClassification
Table2.ECEandMCEperformanceforallmodelsin5-shot5-waytasksonCUB,mini-ImageNetâ†’CUB,andOmniglotâ†’EMNIST.
BaselineresultsarefromSnell&Zemel(2021)andKeetal.(2023).Allmetricsarecomputedon3,000randomtasksfromthetestset.
CUB mini-ImageNetâ†’CUB Omniglot â†’ EMNIST
Method ECE MCE ECE MCE ECE MCE
FeatureTransfer 0.187 0.250 0.275 0.646 0.004 0.092
Baseline++ 0.421 0.502 0.315 0.537 0.390 0.475
MatchingNet 0.023 0.031 0.030 0.079 0.057 0.259
ProtoNet 0.034 0.059 0.009 0.025 0.010 0.243
RelationNet 0.438 0.593 0.234 0.554 0.552 0.594
DKT 0.187 0.250 0.236 0.426 0.413 0.483
BayesianMAML 0.018 0.047 0.048 0.077 0.090 0.124
BayesianMAML(Chaser) 0.047 0.104 0.066 0.260 0.235 0.306
OVEPGGP(ML) 0.026 0.043 0.049 0.066 0.112 0.183
OVEPGGP(PL) 0.005 0.023 0.020 0.032 0.064 0.240
CDKT(ML) 0.005 0.036 0.007 0.020 - -
CDKT(PL) 0.018 0.223 0.010 0.029 - -
MD-BFSC 0.005 0.067 0.005 0.014 0.010 0.025
4.3.ConvergenceRate
Table3.AccuracyofMD-BSFCin1-shot5-waytasksontheCUB
Inthissection,wewouldliketoinvestigatetheempirical datasetwithdifferenthyperparameters.Resultsareevaluatedover
convergencerateofourproposedmethod. Therefore,we 5batchesof600episodeswithdifferentrandomseeds.
compareMD-BSFCtothemethodthatemploystheexact Ï MD-BSFC kernel MD-BSFC step MD-BSFC
samevariationaldistribution,butitsinner-loopVIisimple- 0.01 32.22Â±0.41 COS 65.45Â±0.42 1 62.63Â±0.26
mentedthroughvanillagradientdescent.Wedisentanglethe 0.1 61.54Â±0.26 RBF 62.75Â±0.39 3 65.45Â±0.42
0.5 63.86Â±0.38 POL1 64.69Â±0.36 5 64.18Â±0.52
bi-leveloptimizationintotwoparts: fixingthehyperparam-
1 65.45Â±0.42 POL2 64.74Â±0.25 7 62.55Â±0.46
etersofthekernelandneuralnetworktocheckinner-loop
convergenceandcheckingtheouter-loopconvergencewith
fullbi-leveloptimization.
toreflecttheperformanceofconvergence. Specifically,we
performinner-loopupdatesonthesupportsamplesandcom-
Inner-loopconvergence Forsimplicity,wefixarandom
putethecross-entropylossonthevalidationsamples. We
batchofdatafromarandomepisodefortheCUBdatasetand
runeachmethodfor30iterationsfortheouter-loopupdates
themini-ImageNetdatasetandobservethelearningcurve
as well and plot the cross-entropy loss. The results can
forELBO.Toensureafaircomparison,weemploythesame
be seen in Figure 4. We observe that MD-BSFC takes a
initializationschemeforthevariationalparametersanda
greatleadinthe5-shotclassificationscenarioandlearnsat
comparable learning rate of 0.005 for the inner loop and
aslightlyhigherpaceinthe1-shotclassification, demon-
runeachmethodfor30steps. Noticethatinthisempirical
stratinganimprovementinconvergencespeed.
study,weemployarelativelysmalllearningratetobetter
observethealterationoftheELBOcurve. Theresultscan
4.4.HyperparameterSettings
beseeninFigure3. Theempiricalobservationalignswith
ourtheoreticalanalysis: theELBOofMD-BSFCincreases Inthissection,weinvestigatetherobustnessofourmethod
atafasterratethanthevanillamethodforboth1-shotand indifferenthyperparametersettings,includingvariousinner-
5-shot scenarios. This is attributed to the fact that MD- loopstepsizeÏ,variouskernels,anddifferentnumberof
BSFCemployssecond-orderoptimizationintheinnerloop, steps for the inner loop. For simplicity, we present the
whereasthevanillamethodutilizesfirst-orderoptimization. resultsontheCUBdatasetforthe1shotscenario.
Outer-loopconvergence Forthefullbi-levelconvergence Inner-loopstepsize NoticethatinEquation(4),theinner-
experiment,weusealargerlearningrate(0.1for1-shotand loopstepsizeÏcontrolstheupdaterateofasinglenatural
0.02for5-shot)with2stepsfortheinnerloop. Theouter gradientstep,wherealargerÏmayresultinfasterconver-
loopisupdatedbyAdamwithalearningrateof0.001. We gencebutgreatervarianceingeneral. However,inabi-level
empiricallyfindthatgradientdescentdisplayshighnumer- optimization setting, the potential impact of varying step
ical instability for ELBO when a larger learning rate is sizescombinedwiththehighlynonconvexlosslandscape
applied. Thus, weusepredictivelikelihoodfortheouter- ofneuralnetworksisnotclearasopposedtosingle-level
looploss(Keetal.,2023),whichisaplausiblealternative optimization. Therefore,wepresentsomeempiricalresults
7AcceleratingConvergenceinBayesianFew-ShotClassification
CUB mini-ImageNet CUB mini-ImageNet
150
150.0 152 185 185
152.5 154 190 190
155.0 156
157.5 158 195 195
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Iteration Iteration Iteration Iteration
GD MD GD MD
(a) 1-shotclassification (b) 5-shotclassification
Figure3.TheELBOcurveof30iterationsontheCUBdatasetandthemini-ImageNetdatasetfor1-shotand5-shotclassification.Our
methodwithmirrordescent(MD)learnsatafasterratethanthevanillamethodwithgradientdescent(GD)inbothscenarios.
CUB mini-ImageNet CUB mini-ImageNet
1.6 1.60 1.6 1.6
1.5
1.55
1.5 1.4 1.4
1.50
1.3
1.4 1.45 1.2 1.2
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
Iteration Iteration Iteration Iteration
GD MD GD MD
(a) 1-shotclassification (b) 5-shotclassification
Figure4.Thelosscurveof30iterationsontheCUBdatasetandthemini-ImageNetdatasetfor1-shotand5-shotclassification. Our
methodwithmirrordescent(MD)learnsatafasterratethanthevanillamethodwithgradientdescent(GD)inbothscenarios.
toshedsomelightonthisissueinTable3. Empirically,a (2023),wealsofindthatarelativelysmallstepnumberis
largerÏleadstobetterperformance,whileforanextremely optimalforourmethod. Wesupposethereasonbehindthis
smallÏthemodeltendstocollapse. Wesupposethephe- phenomenonisthatonly1stepofupdateproffersaninfe-
nomenonisduetoalargerstepsizeassistinescapingthe riorapproximationoftheoptimaltask-specificparameters,
saddlepointofthehighlynonconvexlossfunction. whilelargerstepnumbersmayaffectthegradientflow.
Base kernel Patacchiola et al. (2020); Snell & Zemel 5.Limitation
(2021);Keetal.(2023)illustratedtheperformanceofvar-
ious base kernels being close and the tendency of perfor- Inthiswork,wefocusonupgradinginner-loopoptimiza-
mancedecayforsmootherkernels,e.g.,RBFkernel. We tionfromafirst-ordertoasecond-ordermethod,leadingto
followtheirprotocolandtesttheresultsonsimilarcandi- enhancedoverallconvergence. However,theupdatesofthe
datebasekernelsinTable3,includingcosinekernelwith neuralnetwork(outer-loop)relyonafirst-ordermethoddue
normalization(COS),RBFkernel,andpolynomialkernel tocomputationalandmemoryconstraints. Futureworkwill
oforder1(POL1)and2(POL2)(Patacchiolaetal.,2020). exploresecond-ordermethodsinouter-loopoptimization.
MoredetailsaboutthekernelsareprovidedinAppendixE.4.
Ourempiricalresultconformswiththepreviousinsights, 6.Conclusion
where finite-rank kernels (POL1, POL2, COS) generally
Inconclusion,ourapproachhascontributedtoBayesianfew-
obtainasuperiorperformanceontheCUBdataset.
shot classification by addressing the non-conjugate infer-
Inner-loopsteps Weexploretheinherentrepercussions encechallengeinGPclassification. Moreover,theintegra-
of varying inner-loop update steps. For single-level opti- tionofnon-Euclideangeometrythroughmirrordescenthas
mizationwheretheouter-loophyperparametersarefixed, proveneffectiveinenhancingtheconvergenceratetheoreti-
the inner-loop parameters should be optimized until con- callyandempirically. Experimentalresultsrobustlyunder-
vergence. However, due to the complex training dynam- scoretheeffectivenessofourproposedmethod,achieving
icscausedbybi-leveloptimization,itisnotclearhowwe competitiveclassificationaccuracyanduncertaintyquantifi-
shouldselecttheinner-loopupdatesteps.Snelletal.(2017b) cationonseveralbenchmarkdatasetscomparedtobaseline
employed a 1-step Gibbs sampling for training while Ke models. Additionally,oursystematicexplorationofvarious
etal.(2023)foundastepnumberof2isoptimalforvarious hyperparametersandcomponentswithinthemodelprovides
datasetsfortheirmethod. Herewedisplaysomeempirical valuableinsightsintotheirimpactonperformance.
resultsonthishyperparameterinTable3.SimilartoKeetal.
8
OBLE
yportnE
ssorC
OBLE
yportnE
ssorCAcceleratingConvergenceinBayesianFew-ShotClassification
ImpactStatement Guo,C.,Pleiss,G.,Sun,Y.,andWeinberger,K.Q. Oncali-
brationofmodernneuralnetworks. InInternationalCon-
Thispaperpresentsworkwhosegoalistoadvancethefield
ference on Machine Learning, pp. 1321â€“1330. PMLR,
of Machine Learning. There are many potential societal
2017.
consequencesofourwork,noneofwhichwefeelmustbe
specificallyhighlightedhere. Hensman, J., Fusi, N., and Lawrence, N. D. Gaussian
processes for big data. In Proceedings of the Twenty-
NinthConferenceonUncertaintyinArtificialIntelligence,
References
UAI2013,Bellevue,WA,USA,August11-15,2013,2013.
Amari,S. Neurallearninginstructuredparameterspaces-
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
naturalriemanniangradient. Advancesinneuralinforma-
Stochastic variational inference. Journal of Machine
tionprocessingsystems,pp.127â€“133,1997.
LearningResearch,14(5),2013.
Bishop,C.M. PatternRecognitionandMachineLearning.
Ke,T.,Cao,H.,Ling,Z.,andZhou,F. Revisitinglogistic-
springer,2006.
softmaxlikelihoodinbayesianmeta-learningforfew-shot
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- classification. InThirty-seventhConferenceonNeural
tional inference: A review for statisticians. Journal of InformationProcessingSystems,2023.
theAmericanstatisticalAssociation,112(518):859â€“877,
2017. Khan, M. and Lin, W. Conjugate-computation varia-
tionalinference: Convertingvariationalinferenceinnon-
Bojarski,M.,DelTesta,D.,Dworakowski,D.,Firner,B., conjugatemodelstoinferencesinconjugatemodels. In
Flepp,B.,Goyal,P.,Jackel,L.D.,Monfort,M.,Muller, ArtificialIntelligenceandStatistics,pp.878â€“887.PMLR,
U.,Zhang,J.,etal. Endtoendlearningforself-driving 2017.
cars. arXivpreprintarXiv:1604.07316,2016.
Kingma,D.P.andWelling,M. Auto-encodingvariational
Bregman,L.M. Therelaxationmethodoffindingthecom- bayes. InInternationalConferenceonLearningRepre-
monpointofconvexsetsanditsapplicationtothesolu- sentations,2014.
tion of problems in convex programming. USSR com-
putationalmathematicsandmathematicalphysics,7(3): Lake,B.,Salakhutdinov,R.,Gross,J.,andTenenbaum,J.
200â€“217,1967. Oneshotlearningofsimplevisualconcepts. InProceed-
ingsoftheannualmeetingofthecognitivesciencesociety,
Chen, W., Liu, Y., Kira, Z., Wang, Y. F., and Huang, J. volume33,2011.
A closer look at few-shot classification. In Interna-
tionalConferenceonLearningRepresentations.OpenRe- Malago`, L. and Pistone, G. Information geometry of the
view.net,2019. gaussiandistributioninviewofstochasticoptimization.
InProceedingsofthe2015ACMConferenceonFounda-
Cohen,G.,Afshar,S.,Tapson,J.,andVanSchaik,A. Em-
tionsofGeneticAlgorithmsXIII,pp.150â€“162,2015.
nist: Extendingmnisttohandwrittenletters. In2017in-
ternationaljointconferenceonneuralnetworks(IJCNN), Maritz,J.S.andLwin,T. Empiricalbayesmethods. Chap-
pp.2921â€“2926.IEEE,2017. manandHall/CRC,2018.
Finn,C.,Abbeel,P.,andLevine,S. Model-agnosticmeta- Martens, J. Newinsightsandperspectivesonthenatural
learningforfastadaptationofdeepnetworks. InInterna- gradientmethod. J.Mach.Learn.Res.,21:146:1â€“146:76,
tionalConferenceonMachineLearning,pp.1126â€“1135. 2020.
PMLR,2017.
Miller, E.G., Matsakis, N.E., andViola, P.A. Learning
Finn, C., Xu, K., and Levine, S. Probabilistic model- fromoneexamplethroughshareddensitiesontransforms.
agnosticmeta-learning.arXivpreprintarXiv:1806.02817, InProceedingsIEEEConferenceonComputerVisionand
2018. Pattern Recognition. CVPR 2000 (Cat. No. PR00662),
volume1,pp.464â€“471.IEEE,2000.
Galy-Fajou,T.,Wenzel,F.,Donner,C.,andOpper,M.Multi-
classgaussianprocessclassificationmadeconjugate: Ef- Nemirovskij,A.S.andYudin,D.B. Problemcomplexity
ficientinferenceviadataaugmentation. InUncertainty andmethodefficiencyinoptimization. SIAMReview,27
inArtificialIntelligence,pp.755â€“765.PMLR,2020. (2):264â€“265,1985.
Grant,E.,Finn,C.,Levine,S.,Darrell,T.,andGriffiths,T. Opper,M.andArchambeau,C. Thevariationalgaussian
Recastinggradient-basedmeta-learningashierarchical approximationrevisited. Neuralcomputation,21(3):786â€“
bayes. arXivpreprintarXiv:1801.08930,2018. 792,2009.
9AcceleratingConvergenceinBayesianFew-ShotClassification
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Snell,J.,Swersky,K.,andZemel,R. Prototypicalnetworks
Nowozin, S., Dillon, J., Lakshminarayanan, B., and forfew-shotlearning. Advancesinneuralinformation
Snoek,J. Canyoutrustyourmodelâ€™suncertainty? evalu- processingsystems,30,2017a.
atingpredictiveuncertaintyunderdatasetshift. Advances
Snell,J.,Swersky,K.,andZemel,R.S. Prototypicalnet-
inneuralinformationprocessingsystems,32,2019.
worksforfew-shotlearning. InAdvancesinNeuralIn-
Patacchiola, M., Turner, J., Crowley, E. J., Oâ€™Boyle, M. formationProcessingSystems30: AnnualConferenceon
F.P.,andStorkey,A.J. Bayesianmeta-learningforthe NeuralInformationProcessingSystems2017,December
few-shotsettingviadeepkernels. InAdvancesinNeural 4-9,2017,LongBeach,CA,USA,pp.4077â€“4087,2017b.
InformationProcessingSystems,2020.
Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H., and
Polson, N. G., Scott, J. G., and Windle, J. Bayesian in- Hospedales, T.M. Learningtocompare: Relationnet-
ference for logistic models using PoÂ´lya-Gamma latent workforfew-shotlearning. InProceedingsoftheIEEE
variables. JournaloftheAmericanstatisticalAssociation, conferenceoncomputervisionandpatternrecognition,
108(504):1339â€“1349,2013. pp.1199â€“1208,2018.
Prabhu, V., Kannan, A., Ravuri, M., Chaplain, M., Son- Titsias, M. K. One-vs-each approximation to softmax
tag, D., and Amatriain, X. Few-shot learning for der- forscalableestimationofprobabilities. InAdvancesin
matologicaldiseasediagnosis. InMachineLearningfor NeuralInformationProcessingSystems,pp.4161â€“4169,
HealthcareConference,pp.532â€“552.PMLR,2019. 2016.
Raskutti,G.andMukherjee,S. Theinformationgeometry Titsias,M.K.,Nikoloutsopoulos,S.,andGalashov,A.Infor-
of mirror descent. IEEE Transactions on Information mationtheoreticmetalearningwithgaussianprocesses.
Theory,61(3):1451â€“1457,2015. arXivpreprintarXiv:2009.03228,2020.
Ravi,S.andBeatson,A. Amortizedbayesianmeta-learning. Vinyals,O.,Blundell,C.,Lillicrap,T.,Wierstra,D.,etal.
InInternationalConferenceonLearningRepresentations, Matchingnetworksforoneshotlearning. Advancesin
2018. neuralinformationprocessingsystems,29,2016.
Ravi,S.andBeatson,A. Amortizedbayesianmeta-learning. Wah,C.,Branson,S.,Welinder,P.,Perona,P.,andBelongie,
InInternationalConferenceonLearningRepresentations, S. Thecaltech-ucsdbirds-200-2011dataset. 2011.
2019.
Wainwright, M. J. and Jordan, M. I. Graphical models,
Ravi,S.andLarochelle,H.Optimizationasamodelforfew- exponential families, and variational inference. Now
shot learning. In International conference on learning PublishersInc,2008.
representations,2017.
Wang,Y.,Yao,Q.,Kwok,J.T.,andNi,L.M. Generalizing
Rissanen,J.J. Fisherinformationandstochasticcomplexity. from a few examples: A survey on few-shot learning.
IEEEtransactionsoninformationtheory,42(1):40â€“47, ACMComputingSurveys(CSUR),53(3):1â€“34,2020.
1996.
Williams,C.K.andRasmussen,C.E. Gaussianprocesses
formachinelearning,volume2. MITpressCambridge,
Rockafellar, R.T. Convexanalysis. Princetonuniversity
MA,2006.
press,2015.
Wilson, A.G., Hu, Z., Salakhutdinov, R., andXing, E.P.
Salimans,T.andKnowles,D.A.Fixed-formvariationalpos-
Deepkernellearning. InArtificialintelligenceandstatis-
teriorapproximationthroughstochasticlinearregression.
tics,pp.370â€“378.PMLR,2016.
BayesianAnalysis,8(4):837â€“882,2013.
Yoon,J.,Kim,T.,Dia,O.,Kim,S.,Bengio,Y.,andAhn,S.
Salimbeni,H.,Eleftheriadis,S.,andHensman,J. Natural
Bayesianmodel-agnosticmeta-learning. InProceedings
gradients in practice: Non-conjugate variational infer-
ofthe32ndInternationalConferenceonNeuralInforma-
enceingaussianprocessmodels. InInternationalConfer-
tionProcessingSystems,pp.7343â€“7353,2018.
enceonArtificialIntelligenceandStatistics,pp.689â€“697.
PMLR,2018.
Snell, J. and Zemel, R. S. Bayesian few-shot classifica-
tionwithone-vs-eachpoÂ´lya-gammaaugmentedgaussian
processes. InInternationalConferenceonLearningRep-
resentations.OpenReview.net,2021.
10AcceleratingConvergenceinBayesianFew-ShotClassification
A.ConjugateVariationalInferenceforMulti-classGPClassification
Consideramulti-classclassificationtaskconsistingofN observationswiththeinputfeaturesX=[x ,...,x ]âŠ¤andthe
1 N
correspondingclasslabelsy=[yâŠ¤,...,yâŠ¤]âŠ¤,wherex isaD-dimensionalvectorx âˆˆX âŠ‚RD andy istheone-hot
1 N i i i
encodingforC classes. Themulti-classGPclassificationmodelincludeslatentGPfunctionsforeachclass,i.e.,f1,...,fC
wherefc(Â·):X â†’Risthecorrespondinglatentfunctionforc-thclass. Inordertoaddressthemulti-classclassification
withGPs,aGPpriorisassumedforeachlatentfunction(Williams&Rasmussen,2006),i.e.,fc âˆ¼GP(0,k )wherek
Î·c Î·c
istheGPkernelforc-thclasswithhyperparametersÎ· . Inpractice,thehyperparametersaredifferentforeachclass.
c
Followingtradition,weutilizethesoftmaxlinkfunctiontomodelthedatalikelihood(categoricaldistribution):
(cid:81) exp(fc Â·yc)
p(y |f )= c n n , (7)
n n (cid:80) exp(fc)
c n
wherefc =fc(x )andf =[f1,...,fC]âŠ¤. Inordertopredicttheclasslabelofanewdatapointxâˆ—,weneedtocompute
n n n n n
theposterioroflatentfunctionsusingBayesianframework:
p(y|f)p(f) (cid:81) p(y |f )(cid:81) p(fc)
p(f|y)=
p(y)
= (cid:82) (cid:81)n p(yn |fn )(cid:81)c p(fc)df, (8)
n n n c
wherefc =[fc,...,fc]âŠ¤,f =[f1âŠ¤,...,fCâŠ¤]âŠ¤,p(fc)=N(fc|0,Kc)withKc =k (x ,x ).
1 N ij Î·c i j
However,anissueisthatthenon-GaussianlikelihoodinEquation(7)isnon-conjugatetotheGaussianpriormakingthe
exactcomputationofposteriorinfeasible,sosomeapproximateinferencemethodshavetobeutilizedtoapproximatethe
posteriorinEquation(8). Variationalinference(VI)(Bleietal.,2017)isoneofthemostpopularmethodsforapproximate
inference. VI can transform the inference problem into an optimization one which can be addressed by optimization
techniques. Specifically,inVI,Equation(8)isapproximatedbyaGaussiandistributionq:
(cid:89)
q(f|Î¸)= N(fc|Î¸c),
c
whereÎ¸ ={Î¸c}C isthenaturalparameterforeachclass. Thisposteriorapproximationassumesindependenceamong
c=1
differentlatentfunctionsofthemodel. TheoptimalGaussiandistributionisobtainedbyminimisingtheKullback-Leibler
(KL)divergencebetweenqandtheexactposteriororequivalentlymaximizingtheevidencelowerbound(ELBO)(Bishop,
2006):
argmaxL(Î¸):=E (cid:2) logp(y|f)p(f)âˆ’logq(f)(cid:3)
q
Î¸âˆˆÎ˜
(cid:88) (cid:88) (9)
= E [logp(y |f )]âˆ’ KL[N(fc|Î¸c)âˆ¥N(fc|0,Kc)],
q n n
n c
whereÎ˜isthesetofvalidvariationalparametersandKListheKLdivergence.
One problem that arises when computing the ELBO in Equation (9) is that the expectation of log-likelihood, i.e.,
E [logp(y |f )], does not have a closed-form expression. Despite its intractability, the ELBO can still be optimized
q n n
byMonteCarlocombinedwiththereparametrizationtrick(Kingma&Welling,2014),butthemajorissuesare: (1)Anaive
implementationofnumericaloptimizationincurslowefficiencyduetothecomplicatedandtime-consumingnon-conjugate
computation. (2)Therateofconvergencedependsontheparameterizationofthevariationaldistribution. Theordinary
gradientintheEuclideanspaceisanunnaturaldirectiontofollowforVIbecausetheparameterscorrespondtodistributions
andourgoalistooptimizeadistributionratherthanitsparameters(Hoffmanetal.,2013;Salimbenietal.,2018).
InspiredbyKhan&Lin(2017),toaddressthesetwoissuesmentionedabove,weproposeanalgorithmtooptimizethe
ELBOwithmirrordescentdevelopedbyNemirovskij&Yudin(1985)thatcombinesthebestofbothworlds: ontheone
hand,eachgradientstepinourmethodcanbeimplementedinaconjugatewaybyapproximatingthenon-conjugateterm
byaGaussiandistribution;ontheotherhand,ourapproachcanmaximizetheELBOmoreefficientlybyexploitingthe
non-EuclideangeometryasthemirrordescentisthesteepestdescentdirectionalongthecorrespondingRiemannianmanifold
andinvarianttotheparameterizationofthevariationaldistribution(Raskutti&Mukherjee,2015).
B.DerivationofTheorem3.1
Consideraexponential-familydistributionp(f|Î¸)=h(f)exp(Î¸âŠ¤Ï•(f)âˆ’A(Î¸))whereÎ¸isthenaturalparameter,Ï•(f)is
thesufficientstatistics,A(Î¸)isthelog-partitionfunctionA(Î¸) = log(cid:82) h(f)exp(Î¸âŠ¤Ï•(f))df thatisconvex. Aminimal
11AcceleratingConvergenceinBayesianFew-ShotClassification
exponentialfamilydistributioncanalsobeparameterizedbythemeanparameterÂµwhichisthemeanofthesufficient
statisticsÂµ=E [Ï•(f)]. ConsideringthenegativeentropyofthedistributionasafunctionofthemeanparameterH(Âµ)=
p
E [logp(f|Î¸)]=Î¸âŠ¤Âµâˆ’A(Î¸)whereweomittheconstantterm,itisawellknownresultthatthelogpartitionfunctionand
p
negativeentropyfunctionareconvexconjugatefunctions:H(Âµ)=sup [Î¸â€²âŠ¤Âµâˆ’A(Î¸â€²)]andA(Î¸)=sup [Î¸âŠ¤Âµâ€²âˆ’H(Âµâ€²)].
Î¸â€² Âµâ€²
Further,ifA(Â·)isstrictlyconvexandtwicedifferentiable,thensoisH(Â·). Thesetwoconvexconjugatefunctionsprovidean
importantrelationbetweennaturalparameterandmeanparameter: Âµ=âˆ‡A(Î¸)andÎ¸ =âˆ‡H(Âµ)(Rockafellar,2015).
ThetwoconvexconjugatefunctionswillinduceapairofdualBregmandivergences,andthenthedualBregmandivergences
will further induce a pair of dual Riemannian manifolds. Given the log-partition function A(Î¸), it induces a Bregman
divergence B (Î¸,Î¸â€²) : Î˜Ã—Î˜ â†’ R+ that defines a positive definite Riemannian metric âˆ‡2A(Î¸) because A(Î¸) is a
A
strictlyconvexandtwicedifferentiablefunction. Therefore,B (Î¸,Î¸â€²)inducestheRiemannianmanifold(Î˜,âˆ‡2A(Î¸)).
A
Similarly, ifallÎ¸ âˆˆ Î˜aretransformedbyâˆ‡A(Î¸), wecangetthemeanparameterÂµ âˆˆ M. TheBregmandivergence
B (Âµ,Âµâ€²):MÃ—Mâ†’R+inducesaRiemannianmanifold(M,âˆ‡2H(Âµ)).(Î˜,âˆ‡2A(Î¸))iscalledtheprimalRiemannian
H
manifoldand(M,âˆ‡2H(Âµ))iscalledthedualRiemannianmanifold. Raskutti&Mukherjee(2015)provedtheequivalence
betweenmirrordescentandnaturalgradientdescentbytheapplicationofchainruleanddualRiemannianmanifolds. We
restatetheproofhere.
Proof. AssumethatweuseamirrordescenttomaximizetheELBOLËœ(Âµ)=L(Î¸)overthemeanparameterÂµusingthe
BregmandivergenceinducedbythenegativeentropyfunctionH(Âµ):
1
Âµ =argmaxâˆ‡LËœ(Âµ )âŠ¤Âµâˆ’ B (Âµ,Âµ ).
t+1 t Ï H t
ÂµâˆˆM t
SubstitutingB (Âµ,Âµ )=H(Âµ)âˆ’H(Âµ )âˆ’âˆ‡H(Âµ )âŠ¤(Âµâˆ’Âµ ),wecanobtainthesolution:
H t t t t
âˆ‡H(Âµ )=âˆ‡H(Âµ )+Ï âˆ‡LËœ(Âµ ).
t+1 t t t
UsingÂµ=âˆ‡A(Î¸)andÎ¸ =âˆ‡H(Âµ),weobtainthefollowingequivalentexpression:
Î¸ =Î¸ +Ï âˆ‡ LËœ(âˆ‡A(Î¸ )).
t+1 t t Âµ t
Substitutingthechainruleâˆ‡ LËœ(âˆ‡A(Î¸))=âˆ‡2A(Î¸)âˆ‡ LËœ(âˆ‡A(Î¸)),weobtain:
Î¸ Î¸ Âµ
Î¸ =Î¸ +Ï [âˆ‡2A(Î¸ )]âˆ’1âˆ‡ L(Î¸ ),
t+1 t t Î¸ t Î¸ t
whichisjustthenaturalgradientdescentinEquation(2).
C.Gradientsw.r.t. MeanParameters
Inthissection,weprovideaproofofSection3.3inthemainpaper.
Proof. Giventhesoftmaxlikelihood: p(y |f )=(cid:81) exp(fc Â·yc)/(cid:80) exp(fc),wecanobtainthelog-likelihoodandthe
n n c n n c n
firstandthediagonalofsecond-ordergradientsoflog-likelihoodw.r.t. f :
n
(cid:88)
logp(y |f )=f Â·y âˆ’log( exp(f )),
n n n n n
c
exp(f )
âˆ‡ logp(y |f )=y âˆ’ n ,
fn n n n (cid:80) exp(f )
c n
exp(2f ) exp(f )
diag(âˆ‡2 logp(y |f ))= n âˆ’ n .
fn n n ((cid:80) exp(f ))2 (cid:80) exp(f )
c n c n
Givenafactorizedvariaitonaldistributionq(f|Î¸)=(cid:81) N(fc|Î¸c),theELBOcanbewrittenas:
c
(cid:88) (cid:88)
L= E [logp(y |f )]âˆ’ KL[N(fc|Î¸c)âˆ¥N(fc|0,Kc)].
q n n
n c
12AcceleratingConvergenceinBayesianFew-ShotClassification
Ifweparameterizeq(f )asN(f |m ,diag(v )),wherediag(v )isadiagonalmatrixcorrespondingtovectorv dueto
n n n n n n
theassumptionofindependencebetweenlatentfunctionsofdifferentclasses,thegradientofE [logp(y |f )]w.r.t. m
q(fn) n n n
andv canbeexpressedas:
n
(cid:34) (cid:35)
exp(f )
g :=âˆ‡ E [logp(y |f )]=E âˆ‡ logp(y |f )=E y âˆ’ n ,
mn mn q(fn) n n q fn n n q n (cid:80) exp(f )
c n
(cid:34) (cid:35)
1 1 exp(2f ) exp(f )
g :=âˆ‡ E [logp(y |f )]= E diag(âˆ‡2 logp(y |f ))= E n âˆ’ n .
vn vn q(fn) n n 2 q fn n n 2 q ((cid:80) exp(f ))2 (cid:80) exp(f )
c n c n
accordingtoOpper&Archambeau(2009).
GiventherelationbetweenthetraditionalparametersandthemeanparametersofanindependentmultivariateGaussian
distribution: m =Âµ(1) andv =Âµ(2)âˆ’Âµ(1)2 whereÂµ(1) andÂµ(2) aretwomeanparametersofq(f ),wecanusethe
n n n n n n n n
chainruletoexpressthegradientw.r.t. themeanparameters:
âˆ‡ E [logp(y |f )]=g âˆ’2g â—¦m ,
Âµ( n1) q(fn) n n mn vn n
âˆ‡ E [logp(y |f )]=g .
Âµ( n2) q(fn) n n vn
D.ComputationalComplexityforEquation(6a)
NotethatthecomputationalcomplexityofMonteCarloinEquation(6a)comesfromcovariancedecompositionandsample
drawing. ThefirstpartcostsO(MN2)andthesecondpartcostsO(N3),whereN isthesupportsizeandM isthesamples
drawn. Inafew-shotsetting,M ismuchlargerthanN,sothetotalcomputationalcomplexityisO(MN2),whichdoesnot
poseachallengetocomputationalresourcesandiseasilymanageable.
E.ExperimentalDetails
E.1.Datasets
Weusefourdatasetsasdescribedbelow.
1. Caltech-UCSDBirds(CUB).CUBconsistsofatotalof11788birdimagesfrom200distinctclasses. Thestandard
splitof100training,50validation,and50testclassesisemployed(Snell&Zemel,2021).
2. mini-ImageNet. mini-ImageNetisasmallpartofthefullImageNetdataset,where100classeswith600imageseach
areselectedtoformthedataset. Weemployedthecommonsplitof64training,16validation,and20testclassesas
well(Snell&Zemel,2021).
3. Omniglot. Thereare1623grayscalecharactersselectedfrom50languagescontainedinthisdataset,whichisfurther
expandedto6492imagesbydataaugmentation(Lakeetal.,2011). Weuse4114fortraining.
4. EMNIST.EMNIST(Cohenetal.,2017)consistsof62classesofsingledigitsandEnglishcharacters. Inthedomain
transfertask,weutilize31forvalidationandtheotherfortest.
E.2.ComparisonofBaselines
Asforthedescriptionofbaselinemethods,werefertoSnell&Zemel(2021)foradetailedoverview. Hereweonlycompare
themethodsthataremostsimilartoours,whichincludeDKT,Logistic-softmax,andOVE.Allofthesemethodsutilizea
similarframeworkbutwithdifferentlikelihoodandinferencemethodsfromours.
1. DeepKernelTransfer(DKT)(Patacchiolaetal.,2020)employedtheGaussianlikelihoodtocircumventtheconjugacy
issuewherethelabels{+1,âˆ’1}aretreatedascontinuousvalues,leadingtosuboptimalaccuracy.
13AcceleratingConvergenceinBayesianFew-ShotClassification
2. Logistic-softmax(Galy-Fajouetal.,2020)appliedthelogistic-softmaxforaconditionalconjugatemodelafterdata
augmentation. TheGibbssamplingversionimplementedbySnell&Zemel(2021)andthemean-fieldapproximation
versionimplementedbyKeetal.(2023)areconsideredforcomparison.
3. One-vs-Each Approximation (OVE) (Snell & Zemel, 2021) proposed to approximate the softmax function for
conditionalconjugacyafterdataaugmentationandappliedGibbssamplingforposteriorinference.
E.3.TrainingProtocols
TheAdamoptimizerwithastandardlearningrateof10âˆ’3 fortheneuralnetworkandalearningrateof10âˆ’4 forother
kernelparametersisemployedacrossallourexperimentsintheouterloop. Forasingleepoch,100randomepisodesare
sampledfromthecompletedatasetforallmethods. Asforthestepsusedforvariationalinference,werun3stepswithÏ=1
duringtrainingtimeand50stepsduringtestingtimewithÏ=0.5. WeemploytheCOSkernelforCUBandmini-ImageNet
andtheRBFkernelforOmniglotandEMNIST.
E.4.Kernels
To begin with, we briefly review the definition of deep kernel proposed in Wilson et al. (2016). Deep kernel com-
binesthetraditionalkernelwithmodernneuralnetworksinastraightforwardformulation,definedask(x,xâ€² | Î¸,w) =
kâ€²(g (x),g (xâ€²)|Î¸),wherekâ€²representsapredefinedbasekernelwithparametersÎ¸. Theinputsareprojectedbyadeep
w w
neuralnetworktoincreaseflexibility. NotethatthedeepkernelparametersconsistofÎ¸andw. Oneupsideofdeepkernelis
itspotentialtolearnbeyondEuclideandistance-basedmetricsviadata-drivenoptimization. Inthefollowing,weprovidethe
expressionsfortherelevantbasekernelsdiscussedinthemainpaper.
1. PolynomialKernel(POL).Thepolynomialkernelisdefinedas
kâ€²(x,xâ€²)=(xâŠ¤xâ€²+c)s,
wheresistheorderofthekernelandcisatrainableparameter.
2. RadialBasisFunctionKernel(RBF).TheRBFkernelisdefinedas
(cid:16)
(cid:13) (cid:13)xâˆ’xâ€²(cid:13) (cid:13)2
(cid:17)
kâ€²(x,xâ€²)=exp âˆ’ ,
2l2
wherelisreferredtoasthelength-scaleparameter. TheRBFkernelissmoothbecauseoftheexponentialeigenvalue
decayrate.
3. CosineSimilarityKernel(COS).Thecosinesimilaritykernelisdefinedas
xâŠ¤xâ€²
kâ€²(x,xâ€²)= .
âˆ¥xâˆ¥âˆ¥xâ€²âˆ¥
Weuseavariantofthekernelwithbatchnormalizationthatcenterstheinputvectors(Patacchiolaetal.,2020).
E.5.ComparisionofActualRunningTime
Tofurtherinvestigatetheconvergencerateinpractice,wegiveabriefcomparisonoftheactualrunningtimebetweenour
methodandotherfirst-ordermethodsinTable4. WeusetheCUBdatasetandruneachmethodwith3stepsfortheinner
loopexceptCDKT(PL).Thetimereportedfortheinnerloopandouterlooparefromthelastsampledtaskforthefirst
epoch. WeuseoneQuadroRTX6000toruneachmethod. NotethatCDKT(ML)isoneofthesotabaselinesthatusea
verysimilartrainingprocedure,whereitsinnerloopisthemean-fieldapproximationsteps,andCDKT(PL)usespredictive
likelihoodwithnoinnerloopsowejustcomparethetotalruntimeofanepoch. Ascanbeseenfromthetable,ourmethod
hasasimilarruntimecomparedtootherfirst-ordermethod. Theresultprovesthatourmethodindeedenjoysfirst-order
computationandiscomparabletootherfirst-ordermethodsinactualrunningtime.
14AcceleratingConvergenceinBayesianFew-ShotClassification
Table4.ComparisonofactualrunningtimebetweenMD-BSFCandotherfirst-ordermethodswithsimilarimplementationsontheCUB
dataset.Theinnerloopisrunwith3steps.
MD GD CDKT(ML) CDKT(PL)
InnerLoop 0.0181 0.0172 0.0141 NA
OuterLoop 0.1035 0.0989 0.0814 NA
OneEpoch 12.1253 12.0518 10.8549 12.5445
F.RelatedWork
Toaddressthenon-conjugateissue,Patacchiolaetal.(2020)assumesaGaussianlikelihoodforclasslabelstocircumvent
thenon-conjugatecomputation,whichexhibitsinferioruncertaintyquantificationsinceclasslabelsarediscreteratherthan
continuous. Differently,Snell&Zemel(2021)proposesanovelapproachthatcombinesthePoÂ´lya-Gammaaugmentation
technique(Polsonetal.,2013)andtheone-vs-eachsoftmaxapproximation(Titsias,2016)totransformthenon-conjugateGP
classificationmodelintoaconditionallyconjugatemodel. Keetal.(2023)alsoutilizesthePoÂ´lya-Gammaaugmentationwith
thelogistic-softmaxlikelihoodtoachieveconditionalconjugacy. Thesemethodsofferbetteruncertaintyquantificationbut
requireadditionalauxiliaryvariables. Differingfromtheapproachesmentionedabove,weincorporatemirrordescent-based
variationalinferenceintoGPclassificationtoobtainaconjugatealgorithmwithoutintroducinganyauxiliaryvariables.
TheexplorationofBayesianmeta-learninghasyieldeddiverseapproachesaimedatleveragingpriorknowledgeandadapting
tonewtasks. TheframeworkproposedbyFinnetal.(2018)adoptsaBayesianhierarchicalmodelingperspective,enabling
thecaptureofuncertaintyatvariouslevels. Inadifferentvein,Grantetal.(2018)reformulatedmeta-learningasinferencein
aGP,whileYoonetal.(2018)introducedBayesianModel-AgnosticMeta-Learning(MAML)buildingupontheworkof
Finnetal.(2017). Notably,Patacchiolaetal.(2020);Snell&Zemel(2021)employedGPswithdeepkernelstofacilitate
task-specificinference,therebycontributingtoBayesianmeta-learningintermsofparameterupdates,uncertaintymodeling,
andpriordistributions. Inthiscontext,ourworkmakesavaluablecontributionbypresentinganeffectivealternativefor
task-levelupdatesandofferinginsightsintothecoordinationproblemassociatedwithbi-leveloptimization.
15