Marker or Markerless? Mode-Switchable Optical Tactile Sensing for
Diverse Robot Tasks
Ni Ou1,2,â€ , Zhuo Chen2 and Shan Luo2
Abstractâ€”Optical tactile sensors play a pivotal role in robot Marker Markerless
perception and manipulation tasks. The membrane of these sen-
sorscanbepaintedwithmarkersorremainmarkerless,enabling
themtofunctionineithermarkerormarkerlessmode.However,
this uni-modal selection means the sensor is only suitable for
eithermanipulationorperceptiontasks.Whilemarkersarevital
for manipulation, they can also obstruct the camera, thereby
impeding perception. The dilemma of selecting between marker
andmarkerlessmodespresentsasignificantobstacle.Toaddress
this issue, we propose a novel mode-switchable optical tactile
sensing approach that facilitates transitions between the two Markerless Marker
modes. The marker-to-markerless transition is achieved through
agenerativemodel,whereasitsinversetransitionisrealizedusing
a sparsely supervised regressive model. Our approach allows a
single-mode optical sensor to operate effectively in both marker
andmarkerlessmodeswithouttheneedforadditionalhardware,
makingitwell-suitedforbothperceptionandmanipulationtasks.
Extensive experiments validate the effectiveness of our method.
For perception tasks, our approach decreases the number of
categories that include misclassified samples by 2 and improves
contactareasegmentationIoUby3.53%.Formanipulationtasks,
ourmethodattainsahighsuccessrateof92.59%inslipdetection. Fig. 1. Bidirectional transitions between marker and markerless modes.
Code,datasetanddemovideosareavailableattheprojectwebsite Marker-markerless transition (top): black markers are replaced with photo-
realisticpixels;markerless-markertransition(bottom):pseudomarkermotions
https://gitouni.github.io/Marker-Markerless-Transition/
(yellowarrows)aregeneratedfromamarkerlesstactileimage.
Index Termsâ€”Force and Tactile Sensing, Generative Models,
Robot Perception, Robot Grasping.
Nevertheless, the presence of markers decreases the conti-
nuityofthetactileimagesandreducestheefficientperception
I. INTRODUCTION
areaofthecamera,thusinevitablydisruptstheperformanceof
Tactile feedback offers valuable contact information for
robot perception in tasks like texture classification. Although
robot actuators, providing insights into the shape and texture
these marker-overlaid tactile images can be classified by deep
oftouchedobjects,aswellasthecontactforces,deformations
learningmodels[10],[11],otherdownstreamtasksthatrequire
and slip information [1]. In recent years, the development of
dense correspondences across tactile frames such as image
vision-basedtactilesensors,suchasGelSight[2],GelSlim[3],
stitching and segmentation [12], remain challenging since
and TacTip [4], has enabled robots to utilize high-resolution
theseopaquemarkersdisruptfeatureextractionandmatching.
tactile images to facilitate challenging perception and manip-
Someresearchersmanagetodealwiththisconflictingprob-
ulation tasks. During the fabrication of these sensors, markers
lem from the perspective of hardware. One popular approach
are often applied to their elastomer for external force estima-
substitute conventional opaque markers with transparent ul-
tion, which is crucial for tasks like robotic manipulation [2],
traviolet (UV) markers [13], [14]. These markers that are
[3].Eachmarkerâ€™sdisplacementisapproximatelyproportional
made of UV ink appear transparent, and become illuminated
to the local applied force. The recovered force distribution
when exposed to UV light. This type of sensor can switch
from marker motions enables the optical tactile sensors with
betweentwomodes:LEDmodefortactileperceptionandUV
capabilitiesofanalyzingshear[5],estimatingcontactforce[6],
LED mode for force field estimation. However, the mode-
anddetectingslip[7].Thesecapabilitiesareessentialforrobot
switch operation is controlled by toggling the UV or white
manipulation and closed-loop control [8], [9].
LED lights, thereby complicating the design of the electrical
circuit. Additionally, the illumination of UV markers is not
*ThisworkwassupportedbytheNationalNaturalScienceFoundationof
China under Grant 62173038 and the EPSRC project â€œViTac: Visual-Tactile instantaneous and the delay potentially introduces temporal
SynergyforHandlingFlexibleMaterialsâ€(EP/T033517/2). inconsistencies between two modes.
1NiOuiswiththeStateKeyLaboratoryofIntelligentControlandDecision
Unlike the aforementioned studies, we propose a novel
ofComplexSystems,BeijingInstituteofTechnology,Beijing,100081,China.
2Ni Ou, Zhuo Chen and Shan Luo are with the Robot Perception Lab, mode-switchable method to implement bidirectional transi-
Centre for Robotics Research, Department of Engineering, Kingâ€™s Col- tions between marker and markerless status at the software
lege London, London WC2R 2LS, United Kingdom. Emails: {ni.ou,
zhuo.7.chen, shan.luo}@kcl.ac.uk. level, with the function of our method illustrated in Fig. 1.
â€ WorkwasdonewhileNiOuwasvisitingKingâ€™sCollegeLondon. On the one hand, we utilize a generative model for marker-
4202
guA
51
]OR.sc[
1v67280.8042:viXramarkerless transition. It transfers tactile images with markers B. Generative Models for Optical Tactile Sensing
into pseudo markerless tactile images, in which markers are
Deep generative models exhibit high performance in
replacedwithphoto-realisticRGBpixels.Ontheotherhand,a
synthesizing realistic images. They can be classified into
sparselysupervisedregressivemodelisemployedtorealizethe
two branches: Generative-Adversarial Networks (GANs) and
markerless-marker transition, which involves placing pseudo
likelihood-based methods. GANs have dominated the field of
markers on a markerless image. As a result, our method
image generation for several years [20]. Their frameworks
enables a single sensor to possess dual modes, allowing it
includeageneratorandadiscriminatorthatarejointlytrained
to perform both robot perception and manipulation tasks.
under an adversarial strategy. Comparatively, likelihood-based
The key contributions of this paper are summarized below: methods encompass various sub-branches, such as variational
â€¢ We propose a mode-switchable optical tactile sensing autoencoders [21], autoregressive models [22], normalizing
method without need of any additional hardware, which flows [23] and diffusion models [24].
enables bidirectional transitions between marker and Generative models have been employed in tactile image
markerlessmodesandallowsasingle-modetactilesensor generation to address the need for a large volume of tactile
to perform diverse robot tasks; images in data-driven approaches. For example, GANs have
â€¢ A novel diffusion-based framework is proposed for the been utilized to generate realistic tactile images, which are
marker-markerless transition, with a marker-offset strat- labor-intensive to collect in the real world [25]â€“[27]. GANs
egy devised to make it adaptive to new sensors; have also been applied to produce tactile outputs with the
â€¢ Extensive experiments are conducted to verify the effec- prompts of visual images [28] or to produce haptic rendering
tiveness of our method. Results show that our method from visual inputs [29]. Additionally, some researchers have
exhibitshighperformanceinbothmarker-markerlessand leveraged a masked autoencoder for tactile image comple-
markerless-marker transitions. tion [30], and demonstrated the effectiveness of generative
models in reconstructing missing tactile signals. In this study,
The remainder of this paper are organized as follows:
we leverage a diffusion-based generative model [24] for our
Section II investigates recent related works to our study; Sec-
marker-markerless transition, as it has demonstrated superior-
tionIIIdescribesourmode-switchabletactilesensingmethod;
ity over GANs in various image editing tasks [31].
Section IV showcases a series of extensive experiments that
validatetheeffectivenessofourmethodinvariousrobottasks;
Section V concludes the paper and presents future research.
III. METHOD
A. Overview
As illustrated in Fig. 2, the framework of our mode-
II. RELATEDWORKS
switchable method consists of two transitions. The marker-
markerless transition (detailed in Section III-B) starts with a
A. Optical Tactile Sensors
tactile image I of width W and height H with markers,
w
Optical tactile sensors employ vision sensors and auxiliary and markers I extracted from I following the marker
M w
light sources to capture detailed tactile information. Over last extractionapproachin[14].Subsequently,I andI areused
w M
decades, researchers have developed a wide range of optical as the inputs for an inpainting method to generate a pseudo
tactilesensorsthatcanbebroadlycategorizedintotwogroups: markerless tactile image IË† , wherein the regions of markers
w/o
theTacTipfamily[4]andtheGelSightfamily[2].TheTacTip are replaced with RGB pixels to maintain texture and color
sensors [15] comprise a black hemisphere membrane with continuity of I . On the other hand, the markerless-marker
w
white pins embedded in the tips, along with LED lights and a transition(detailedinSectionIII-C)employsaregressiveneu-
CCD camera. By mounting the camera to track the pin array, ral network to predict a pseudo marker motion field MË† from
v
thesesensorsenablethemeasurementofappliedforcethrough the input markerless image I . The centers of markers are
w/o
marker movement. The original TacTip design has also been predeterminedintheimage,andtheirmotionsarerepresented
expanded with variants such as TacCylinder [16] and TacTip- by 2D vectors.
M2 [17] to cater to specific applications.
On the other hand, GelSight sensors [18] utilize a transpar-
B. Marker-Markerless Transition
entgelmaterialcoveredwithatopreflectivelayer.Byutilizing
The core of our marker-markerless transition is the in-
light sources of different colors, the camera can capture de-
painting module. Inpainting indicates restoring the missing
formationandgeometryinformationthroughcoloredillumina-
pixels in a designated region of an image to maintain the
tion,whichcanalsobeleveragedfordepthreconstruction[2].
color and texture consistency, with the pixels outside the
Furthermore, when the GelSightâ€™s elastomer is overlaid with
regionunchanged.Mathematically,theoutputoftheinpainting
markers, this sensor gains the ability to estimate external
method IË™ is rectified by I within the region of I ,
forces [6] and detect slip [7]. Overall, GelSight sensors offer w/o w M
yielding IË† , which can be formulated as:
versatility in both perception and manipulation tasks. In this w/o
paper, we verify the effectiveness of our method on cube- IË† =I Â·IË™ +(1âˆ’I )Â·I (1)
w/o M w/o M w
shaped GelSight sensors, however, it can be adapted to other
optical tactile sensors like GelTip [19] for perception and Here, inspired by [24], we propose an iterative inpainting
manipulation tasks. methodcalledTacDiff basedondiffusionmodels.AsoutlinedMarker-Markerless Transition Markerless-Marker Transition
ğ¼
ğ‘¤
Cropping In Mpa ei tn ht oi dng ğ¼áˆ˜
ğ‘¤/ğ‘œ
Encoder Decoder S Oel ue tc pt uiv te
Patch
Merging
Extract
Cropping
ğ¼
ğ‘€
Fig. 2. Pipelines of the two transitions in our mode-switchable approach. The marker-markerless transition (left) is implemented by an inpainting method,
with the tactile image with markers Iw and the mask of markers IM as inputs. The markerless-marker transition (right) is realized by a encoder-decoder
networkthatgeneratespseudomarkermotionsMË† v fromamarkerlesstactileimageI w/o.Theselectiveoutputmoduleretrievesfeaturesofsparsepixelsto
output2Dmarkermotions.Yellowarrowsshowtheorientationandmagnitudeofmarkermotions.
in Algorithm 1, TacDiff predicts IË† w/o from I w and I M based Patches
on (1). It initializes a noisy image IË†(T) by adding Gaussian Merged Image
w/o
noise Ïµ âˆ¼ N(0,1) to the I region of I . As a result,
M w
the differences between I and I only exist in I . For
w/o w M
each subsequent iteration t, a denoising function D(IË†(t) ,t)
w/o
estimates the noise-free image IË™(tâˆ’1) from IË†(t) , while a
w/o w/o
noise-addingfunctionA(IË™(tâˆ’1),tâˆ’1)addstâˆ’1levelofnoise
w/o
totheI M regionofIË™ w(t /âˆ’ o1),yieldingalessnoisyimageIË† w(t /âˆ’ o1). Fig. 3. Patch-based Merging. The resulting image is obtained by merging
After T iterations, we can obtain IË†(0) as the final output for sixsmallpatchesthatareseparatelypredictedbytheinpaintingmodel.The
w/o regions of patches in the merged image are annotated with white dashed
IË† . rectangles,whichoverlapeachother.
w/o
Algorithm 1: TacDiff while those within the overlapping areas achieve the average
Input: I ,I values of the patches that cover this area.
w M
Output: IË†
w/o
Sample Ïµâˆ¼N(0,1) ğ¼ ğ¼
IË†(T) =I Â·Ïµ+(1âˆ’I )Â·I ğ‘¤ ğ‘€
w/o M M w
for t=T,T âˆ’1,...,1 do
IË™(tâˆ’1) =D(IË†(t) ,t)
w/o w/o
IË†(tâˆ’1) =I Â·A(IË™(tâˆ’1),tâˆ’1)+(1âˆ’I )Â·I
w/o M w/o M w
end
As introduced in [24], the noise-adding function A is ğ¼ ğ‘¤/ğ‘œ ğ¼ ğ‘¤/ğ‘œ+ğ¼ ğ‘€
predetermined while the denoising function D is learned by a
neural network. A U-Net [32] is employed as DË† (IË†(t) ,t) to
Î¸ w/o
function as D and trained using the following loss:
(cid:13) (cid:13)2
L =(cid:13)I Â·[DË† (IË†(t) ,t)âˆ’I ](cid:13) (2)
dif (cid:13) M Î¸ w/o w/o (cid:13)
2
where Î¸ is the parameters of the network to be trained.
Patch-based Merging. Due to the resolution limitation of U- Fig.4. Trainingdataacquisitionforthemarker-markerlesstransition.IM is
Net, the final output of is synthesized by merging multiple extractedfromIw andthenplacedontoI w/o toformthenewtactileimage
low-resolution patches predicted by DË† using a weighted al- withmarkersI w/o+IM.I w/o+IM servesasinputwhileI w/o servesas
Î¸ theground-truthoutput.
gorithm.Forinstance,asdemonstratedinFig.3,togeneratea
640x480tactileimage,themodelpredictssix256x256patches Training Data Acquisition. We fabricate two GelSight sen-
withoverlappingregions.Pixelswithinthenon-overlappingar- sors with the same tactile resolution and membrane material:
easdirectlyretainthevaluesfromtheircorrespondingpatches, one with markers (Sensor WM) and one without markers(Sensor WO). The object is pressed onto both Sensor WO ğ¼ğ‘¤ğ‘Ÿ ğ¼ ğ‘€ğ‘Ÿ
and Sensor WM, respectively, at the same position and depth,
yielding a pair of I and I . Since inpainting methods
w/o w
require the background of I and I to be the same, we
ğ‘€ğ‘£
w w/o
extract the markers of I , i.e., I , and then place I onto
w M M
I w/o to satisfy this condition. In the case of the marker- ğ¼ğ‘¤ ğ¼ğ‘€
markerless transition, I +I and I serve as input and
w/o M w/o
output, respectively.
ğ¼ ğ‘¤ ğ¼áˆ˜ ğ‘¤/ğ‘œ Joint Cropping Cropped
Data Pair
TELEA ğ¶(ğ¼áˆ˜ ğ‘¤/ğ‘œ) Fig. 6. Training data acquisition for the markerless-marker transition. Iw
and I wr (reference) are collected from Sensor WM, and IM and I Mr are
theirrespectivemarkers.YellowarrowsinMv representthemarkermotions
ğ¼
ğ‘€
ğ¼ ğ‘€â€² obtainedfromIM andI Mr .
ğ¶(ğ¼â€²)
ğ‘€
Offset
Baselines. We also include two non-data-driven inpainting
methods in our experiments for comparison, i.e., NS [34] and
TELEA [33]. The NS method draws inspiration from fluid
Fig. 5. Marker-offset strategy for training TacDiff if Sensor WO is not dynamics and utilizes a vector field defined by the stream
available.TheTELEAinpaintingalgorithm[33]isappliedtogenerateIË† w/o. function to transport the Laplacian of the image intensity
IM istranslatedbyanoffsettogetI Mâ€² .IË† w/oandI Mâ€² arejointlycroppedto into the inpainting region. In contrast, the TELEA method
obtainapairofcroppedpatchesC(IË† )andC(Iâ€² )fortrainingTacDiff.
w/o M estimates the smoothness of unknown pixels by computing
a weighted average over neighboring known pixels. In this
Marker-offset Strategy. The aforementioned pipeline is only
approach, the missing regions are treated as level sets, and
applicable to the scenarios where paired I and I are
w w/o image information is propagated for inpainting using a Fast
available. When users utilize their own Sensor WM to collect
Marching Method (FMM) [35].
I and do not have a matched Sensor WO to collect I ,
w w/o
we also develop a marker-offset strategy to train TacDiff. As
C. Markerless-Marker Transition
illustrated in Fig. 5, this strategy enables users to finetune
our TacDiff in their own dataset, eliminating the need for As shown in the right half of Fig. 2, the regressive network
fabricating Sensor WO. Initially, I M is extracted from the predicts displacements of markers MË† v from a markerless
original tactile image I w. Next, a non-data-driven inpainting imageI w/o.Intermsofimplementation,thecentercoordinates
algorithm, such as TELEA [33], is employed to generate a of these markers M c are predetermined in I M, allowing
pseudo markerless image IË†
w/o
with I
w
and I
M
as inputs. the model to output a 2D vector (Î´x i,Î´y i) for each marker
Meanwhile, a new mask Iâ€² is created through translating the center i. Our regressive network is modified from Encoder-
M
positions of markers by a constant offset (âˆ†x,âˆ†y): Decoder network named DeeplabV3 [36]. We substitute its
cross-entropy loss with Mean Squared Error (MSE) loss to
ï£±
ï£´ï£²I M(i,j)=1 makeitapplicabletothisregressiontask.Inthiscase,sincewe
Iâ€² (i+âˆ†x,j+âˆ†y)=1, s.t. 0â‰¤i+âˆ†x<W (3) only have ground-truth (Î´x ,Î´y ) at M (M ), we exclusively
M i i c v
ï£´ï£³0â‰¤j+âˆ†y <H back-propagate gradients with respect to M v, meaning that
DeeplabV3 is sparsely supervised.
and Marker Motions vs. Marker Generation. There are three
I Mâ€² (i,j)=0, s.t.I M(i,j)=1, (4) reasonsbehindourchoicetosolelyoutputmarkermotionsM
v
toensurethatI âˆ©Iâ€² =âˆ….Empirically,forevenlydistributed instead of generating the binary mask of markers IË† M. First,
M M in robotic grasping with optical tactile sensors [3], [7], [37],
markers like those shown in Fig. 5, âˆ†x and âˆ†y are set to
the motions of markers rather than the markers themselves
halfhorizontalandverticaldistancebetweenmarkers.Finally,
a joint cropping operation is performed on IË† and Iâ€² to are used for force estimation and slip detection. Second,
w/o M compared to generating marker shapes directly, predicting
obtain cropped markerless image C(IË† ) and corresponding
w/o marker center motions is a sparse task that requires fewer
mask C(Iâ€² ) for training TacDiff,then the updated loss func-
M computational resources for training. Third, the artifacts in
tion is formulated as:
IË† can be incorrectly recognized as markers and result in the
M
(cid:13) (cid:13)2
Lâˆ— =(cid:13)C(Iâ€² )Â·[DË† (IË†(t) ,t)âˆ’C(IË† )](cid:13) (5) failure of marker tracking, while directly predicting marker
dif (cid:13) M Î¸ w/o w/o (cid:13)
2 motions can avoid this problem.
where TacDiff is trained on surrounding RGB pixels rather Training Data Acquisition. The approach to acquiring train-
thanthosegeneratedbyTELEA.Thecroppingoperationforce ing data for our sparsely supervised marker motion prediction
the resolution of patches to match the input resolution of U- is presented in Fig. 6. First, a reference tactile image without
Net and augment the positions of markers to avoid overfitting any contact Ir and a tactile image with contact I are
w w
as well. collected from Sensor WM. Ground-truth M is obtained by
vInput NS TELEA pix2pix TacDiff Ground-Truth
seen
unseen
depth
unseen
indenter
Fig.7. Qualitativemarker-markerlessassessment.Somesignificantdifferencesacrosstactileimagesareannotatedwithredcircles.Theleftmostcircleinthe
1st rowdemonstratesthatNS,TELEA,andpix2pixproduceinconsistenttexturesonthering,whereasTacDiffdoesnot.Similarpatternscanbeobservedin
thethreecirclesannotatedinthetactileimagesinthe2nd row.
subtracting marker positions in I and Ir , where marker printed objects from [39] and 4 daily objects with richer
M M
correspondences are built through nearest neighbor searching. textures compared to primitive geometric shapes in [39]: a
Slip detection. To apply this markerless-marker transition sandpaper, a star-shaped screw driver bit, a hexagon-shaped
to robot manipulation tasks, we also devise a slip detection screw driver bit, and a screw driver (whose handle was used
algorithm adapted from [7]: if the maximum marker motion fordatacollection).Theseobjectswereverticallypressedonto
between the current and the first collected tactile image Sensor WM and WO at the same position and depth (1mm)
exceeds a predetermined threshold Ïµ , we assume the slip to obtain pairs of I and I . This process is controlled
v w w/o
happens: by a UR5e robot, whose position precision is Â±0.03 mm. In
(cid:114) total, there are 128 pairs of tactile images in the dataset, and
(cid:16) (cid:17)2 (cid:16) (cid:17)2
max x âˆ’x(0) + y âˆ’y(0) >Ïµ (6) the contact area of each sample is manually annotated for
i i i i v
(xi,yiâˆˆMc the segmentation task. The dataset was randomly divided into
where (x ,y ) and (x(0),y(0)) are the locations of the ith training and testing sets with a ratio of 85:15.
i i i i
Paired Dataset B.
marker in the current and the first collected tactile images,
respectively. The data collection methodology employed for this dataset
isakintothatofPairedDatasetA,differinginmarkerpatterns,
light conditions of sensors, and the inclusion of additional
IV. EXPERIMENTS
shearmotionsofindenters.Thisdatasetcontains2320pairsof
Thissectionpresentsaseriesofexperimentstoevaluatethe
I andI recordedatthedepthof0.5mm,1mmand1.5mm.
effectivenessofourmethod.Weconductupstreamexperiments w/o w
Due tosignificant labor costs, datacollection was limitedto 8
toevaluatetheimagequalityofpseudomarkerlessimagesand
objects from [39]. Paired Dataset B was partitioned to assess
theprecisionofpseudomarkermotions,aswellasdownstream
the generalization capabilities of our approach on unseen
experiments that involve tactile recognition and manipulation
indentersandcontactdepths.Forunseenindenterexperiments,
taskstovalidateourmarker-markerlessandmarkerless-marker
ourmodelsaretrainedondatafrom6indentersandevaluated
transitions, respectively. We also include a baseline method
on data from the other 2. For unseen depth experiments, the
pix2pix [38] for comparison, which is designed for image-
training data contain depths of 0.5mm and 1.5mm, while the
to-image translation. For the marker-markerless transition,
test data include depth of 1.0mm.
the input and output of pix2pix are I + I and IË† ,
w/o M w/o ViTac. ViTac is a visual-tactile dataset [40] of 24 classes of
respectively.Forthemarkerless-markertransition,I serves
w/o garmentswithdifferenttextures.Weonlyuseitstactilemodal-
asitsinputwhiletheestimatedmaskofmarkersIË† servesas
M ity, which contains tactile images with markers of different
its output.
textures. Notably, we excluded empty tactile images without
any contact using a imaging high-pass filter, and sampled 720
A. Dataset Description samples (30 for each class) among the remaining data. The
Paired Dataset A. To provide ground-truth tactile images in dataset was randomly divided into training and testing sets
mode transitions, pairs of I and I are needed. Following with a ratio of 0.5:0.5.
w/o w
priorworks[39],weuseindenterstopressagainstthesensors Slip Dataset. As illustrated in the first row of Fig. 9, we
to obtain tactile images. The indenters used include 21 3D controlledarobotgrippertoholdanobjectandappliedmanualTABLEI TABLEII
IMAGEQUALITYASSESSMENT MARKERMOTIONACCURACY
FIDâ†“ KIDâ†“ MSEâ†“ SSIMâ†‘ PSNRâ†‘ ermseâ†“ emagâ†“
Nan%â†“
mean median mean median
NS 25.98 2.08E-2 4.104 0.982 42.41
seen
TELEA 30.94 2.73E-2 3.469 0.982 42.84 pix2pix 3.372 2.211 2.079 1.219 5.26
seen
pix2pix 142.4 1.40E-1 30.77 0.914 33.32 Ours 1.814 0.730 1.369 0.507 0
TacDiff 2.169 1.12E-4 3.780 0.978 42.48
unseen pix2pix 5.253 2.566 0.959 0.458 41.85
unseen NS 20.57 1.54E-2 4.022 0.987 43.18 depth Ours 2.059 1.278 0.137 0.078 0
depth TELEA 23.37 2.15E-2 2.268 0.986 44.76
unseen pix2pix 7.029 4.004 1.537 1.536 80.45
pix2pix 62.45 6.12E-2 4.901 0.974 41.42
indenter Ours 2.598 1.668 0.214 0.124 0
TacDiff 3.216 2.48E-3 3.147 0.981 43.27
unseen NS 20.86 1.73E-2 3.517 0.987 43.96
indenter TELEA 25.41 2.50E-2 1.976 0.986 45.31
pix2pix 59.31 5.78E-2 5.328 0.975 41.23 pix2pix Ours Ground-Truth
TacDiff 3.186 2.57E-3 2.387 0.980 44.40
seen
external force to induce slippage. We only collected tactile
images from the Sensor WO used in Paired Dataset A, and
the serially collected tactile images cover the entire process
before and after the sliding occurs. In total, we collected 12
unseen
sequenceswherenoslipoccurredand15sequenceswhereslip depth
occurred for slip detection.
B. Upstream Experiments
unseen
1) Marker-markerless: Regarding the marker-markerless
indenter
transition, we conduct experiments on Paired Dataset A &
B to evaluate the similarity between the pseudo markerless
tactileimagesandthegroundtruthtactileimages.Asshownin
Fig.7,pix2pixgeneratestactileimageswithartifactsatmarker Fig. 8. Marker motion prediction. Figures in the 1st row depict normal
positions, due to its attempt to generate the entire image indentermotion,whilethoseinthe2nd and3rd rowsillustrateshearindenter
instead of specific regions. In contrast, TacDiff yields visually motion
. The orientation and magnitude of marker motions predicted
superior results compared to the other methods, particularly
by our method are visually better than those predicted by
noticeable at the edges of the in-contact object.
pix2pix. For better visualization, the length of the arrows are
In line with previous works [39], we employed five metrics
sixtimestherealmagnitudeofcorrespondingmarkermotions
to quantitatively evaluate the image quality among different for the 1st row and two times for the 2nd and 3rd rows.
methods: Frechet Inception Distance (FID), Kernel Incep-
tion Distance (KID), Mean Squared Error (MSE), Structural
Similarity Index Measure (SSIM), and Peak Signal-to-Noise
Furthermore, as we discussed in Section III-C, pix2pix can
Ratio(PSNR).AsdemonstratedinTableI,allthreeinpainting
generate artifacts in IË† so that the marker tracking process
methods, i.e., TacDiff, NS and TELEA, outperform pix2pix, M
can fail in some cases. Two failure examples can be found in
showingtheefficacyoftheinpaintingapproachinthemarker-
the 2nd and 3rd rows of Fig. 8, where the number of reference
markerless transition. Furthermore, TacDiff exhibits superior
markers in â€˜pix2pixâ€™ does not equal that in â€˜Ground-Truthâ€™.
performance to NS and TELEA in terms of FID and KID,
We only compute the metrics of pix2pix on success cases and
despite slightly lower scores in other three metrics. This
record its proportion of failure with the metric Nan%.
phenomenon is also observed in other diffusion-based appli-
cations [41]. FID and KID scores have been widely taken As shown in the first row of Table II, both the average
as more important metrics as they measure the distribution of e and the average of e of our method are less
rmse mag
similarity while the others are pixel-wise metrics that are than two-thirds of pix2pixâ€™s results while the median of e
rmse
susceptible to noise in the tactile images. Furthermore, it is and the median of e of our method are less than half of
mag
observed in the last two groups of Table I that TacDiff still pix2pixâ€™s results. On unseen datasets, our method achieves
achieves the best FID and KID and performs second only to almost 60% lower e and e compared with pix2pix,
rmse mag
TELEA in terms of MSE and PSNR on both unseen datasets, showing its generalization capability on contact depth and
highlighting its strong generalization ability. indenters.Notably,someresultsonunseendataarebetterthan
2) Markerless-marker: Astheprecisionofmarkermotions thoseonseendatabecausethequantityoftrainingdataofour
reflects the efficacy of our markerless-marker transition, fol- Paired Dataset B is much larger than that of Paired Dataset
lowing [42], Root Mean Squared Error (RMSE) e and the A. In terms of qualitative results, Fig. 8 demonstrates that our
rmse
magnitude error e are employed to qualify the discrep- method produces marker motion fields more closely aligned
mag
ancy between the ground-truth and predicted marker motions. with ground truth than pix2pix.TABLEIII feedbacktoenablegrippercontrolforpromptlyhaltingslip.In
PERFORMANCEOFTEXTURECLASSIFICATION our experiment, the robot gripper ascends at a rate of 6mm/s,
while Sensor WO captures data at 30Hz. A successful trial
none NS TELEA pix2pix TacDiff
requires the accurate slip detection and the stable lifting of
Accâ†‘ 96.7% 96.7% 96.9% 96.7% 97.5% the object bythe robot gripper. We used fiveobjects shown in
Num1â†“ 7 7 7 6 5
Fig.10,andallofthemweresuccessfullyliftedbythegripper.
1Numberofclassescontainingmisclassifiedsamples.
A group of qualitative results is showcased in Fig. 9. At
the first frame, the initial motions of markers are recorded as
TABLEIV reference.Next,ouralgorithmdecidesiftheslipcondition(6)
PERFORMANCEOFCONTACTAREASEGMENTATION
satisfies for each subsequent frame. Afterwards, when slip is
successfully detected (the 2nd column), the robot gripper is
none NS TELEA pix2pix TacDiff
controlled to increase grasping force to halt slip, as shown
Accâ†‘ 98.25% 98.22% 98.42% 98.43% 98.44%
in the 2nd and 3rd columns. Finally, the object is successfully
IoUâ†‘ 83.53% 82.94% 83.53% 83.53% 87.06%
lifted, with the corresponding tactile signal almost unchanged
(the 4th column).
C. Downstream Experiments
V. CONCLUSION
We follow prior works [5], [12], [14] to carry out down-
In this paper, we propose a mode-switchable optical tactile
stream experiments that show the significance of our method
sensing approach to carry out bidirectional marker-markerless
in tactile perception and manipulation tasks. The experiments
transitions. Experiments show that our method has the po-
include texture classification, contact area segmentation, slip
tential to facilitate both perception and manipulation tasks.
detectionandgraspingtasks.Theclassificationandsegmenta-
In the future study, we plan to eliminate reliance on data
tionexperimentillustrateshowtherecoveredintricatetextures
collection and leverage simulation tools [27], [39] to train the
in our marker-markerless transition help tactile recognition,
models and investigate the sim2real capability of our method.
while slip detection and grasping tasks verify the effective-
Furthermore, we will extend the application of our method to
ness of our markerless-marker transition in predicting marker
more dexterous manipulation tasks like in-hand rotation.
motions during grasping tasks.
1) Classification and Segmentation: We conduct texture
classification and contact area segmentation experiments on
VI. ACKNOWLEDGMENTS
ViTac Dataset and Paired Dataset A, respectively. With the We thank Prof. Edward Adelson at Massachusetts Institute
marker-markerless transition, tactile images with markers are ofTechnologyforengagingdiscussionsoninpaintingmarkers
transformed into pseudo markerless tactile images for these and for sharing insights into mode-switchable optical tactile
two perception tasks. For comparison, we include a control sensing.
groupwhereintactileimageswithmarkersaredirectlyutilized
in perception tasks, designated as â€˜noneâ€™ in the subsequent REFERENCES
discussion. Since we did not have ground-truth I , we
w/o [1] S.Luo,J.Bimbo,R.Dahiya,andetal.,â€œRobotictactileperceptionof
trained TacDiff and pix2pix using the marker-offset strategy objectproperties:Areview,â€Mechatronics,vol.48,pp.54â€“67,2017.
introduced in Fig. 5. We employ a ResNet-18 [43] network [2] W.Yuan,S.Dong,andE.H.Adelson,â€œGelSight:High-Resolutionrobot
for classification and a DeepLabV3 [36] network for segmen- tactile sensors for estimating geometry and force,â€ Sensors, vol. 17,
no.12,p.2762,2017.
tation.
[3] E. Donlon, S. Dong, M. Liu, and et al., â€œGelSlim: A high-resolution,
Regarding classification results, Table III demonstrates that compact, robust, and calibrated tactile-sensing finger,â€ in IROS, 2018,
pp.1927â€“1934.
TacDiff achieves the best performance in all metrics. Com-
[4] B. Ward-Cherrier, N. Pestell, L. Cramphorn, and et al., â€œThe TacTip
pared with â€˜noneâ€™, TacDiff improves the classification accu- family: Soft optical tactile sensors with 3d-printed biomimetic mor-
racy to by 0.8% and reduces the misclassified classes by 2. phologies,â€Softrobotics,vol.5,no.2,pp.216â€“227,2018.
[5] W. Yuan, R. Li, M. A. Srinivasan, and et al., â€œMeasurement of shear
Concerning segmentation segmentation tasks, it is shown in
andslipwithaGelSighttactilesensor,â€inICRA,2015,pp.304â€“311.
Table IV that our method improves 3.53% IoU and 0.19% [6] D.Ma,E.Donlon,S.Dong,andetal.,â€œDensetactileforceestimation
accuracy compared with â€˜noneâ€™. usingGelSlimandinversefem,â€inICRA,2019,pp.5418â€“5424.
[7] S.Dong,W.Yuan,andE.H.Adelson,â€œImprovedGelsighttactilesensor
2) Slip detection and grasping: Our slip detection exper-
formeasuringgeometryandslip,â€inIROS,2017,pp.137â€“144.
iment is carried out on Slip Dataset using the slip detection [8] J.Ueda,A.Ikeda,andT.Ogasawara,â€œGrip-forcecontrolofanelastic
algorithm detailed in Section III-C. Our method achieves an object by vision-based slip-margin feedback during the incipient slip,â€
IEEETransactionsonRobotics,vol.21,no.6,pp.1139â€“1147,2005.
accuracy of 92.59% in this task, with only two slip cases
[9] S.Denei,P.Maiolino,E.Baglini,andetal.,â€œDevelopmentofaninte-
mis-recognized. For grasping experiments, a robotic gripper gratedtactilesensorsystemforclothesmanipulationandclassification
is programmed to lift an object. Sensor WO and WM are using industrial grippers,â€ IEEE Sensors Journal, vol. 17, no. 19, pp.
6385â€“6396,2017.
positioned on either side of the gripper, where Sensor WM
[10] W.Yuan,Y.Mo,S.Wang,andetal.,â€œActiveclothingmaterialperception
remains inactive and serves solely as a physical support to usingtactilesensinganddeeplearning,â€inICRA,2018,pp.4842â€“4849.
ensure the contact points on the both sides hold at the same [11] G. Cao, Y. Zhou, D. Bollegala, and et al., â€œSpatio-temporal attention
modelfortactiletexturerecognition,â€inIROS,2020,pp.9896â€“9902.
height. The gripping force was set to just the right amount
[12] Z. Lin, J. Zhuang, Y. Li, and et al., â€œGelFinger: A novel visual-tactile
to make slip happens. The slip detection algorithm serves as sensorwithmulti-angletactileimagestitching,â€RA-L,2023.First Frame Slip Detected Grasping Lifting
Vision
Tactile
Fig.9. AnillustrationofgraspingwithourslipdetectionfeedbackfortheleftmostobjectinFig.10.Initially,therobotattemptstoliftacupbutencounters
slip (2nd column). The slip is detected using our algorithm and the grasping force is then increased (3rd column), with the cup being lifted successfully
thereafter(4th column).Greenarrowsdenotetheinitiallyappliedgraspingforce,whileredarrowsindicateforceadjustments.Themarkermotiontriggering
slip condition (6) is highlighted with a red rectangle in the 1st and 2nd columns of tactile images. Arrow lengths are scaled 14 times for clarity. Visit our
projectwebsiteforademovideo.
[26] X.Jing,K.Qian,T.Jianu,andetal.,â€œUnsupervisedadversarialdomain
adaptationforsim-to-realtransferoftactileimages,â€IEEETransactions
onInstrumentationandMeasurement,2023.
[27] T.Jianu,D.F.Gomes,andS.Luo,â€œReducingtactilesim2realdomain
gapsviadeeptexturegenerationnetworks,â€inICRA,2022,pp.8305â€“
8311.
[28] J.-T. Lee, D. Bollegala, and S. Luo, â€œâ€˜Touching to seeâ€™ and â€˜seeing
tofeelâ€™:Roboticcross-modalsensorydatagenerationforvisual-tactile
perception,â€inICRA,2019,pp.4276â€“4282.
[29] G. Cao, J. Jiang, N. Mao, D. Bollegala, and et al., â€œVis2Hap: Vision-
basedhapticrenderingbycross-modalgeneration,â€inICRA,2023,pp.
Fig. 10. Objects used for grasping. They are numbered 1 to 5 from left to 12443â€“12449.
rightinourpaper. [30] G. Cao, J. Jiang, D. Bollegala, and et al., â€œLearn from incomplete
tactiledata:Tactilerepresentationlearningwithmaskedautoencoders,â€
inIROS,2023,pp.10800â€“10805.
[31] C. Saharia, W. Chan, H. Chang, and et al., â€œPalette: Image-to-image
[13] A.C.AbadandA.Ranasinghe,â€œLow-costGelSightwithuvmarkings: diffusionmodels,â€inACMSIGGRAPH,2022,pp.1â€“10.
Featureextractionofobjectsusingalexnetandopticalflowwithout3d [32] O. Ronneberger, P. Fischer, and T. Brox, â€œU-Net: Convolutional net-
imagereconstruction,â€inICRA,2020,pp.3680â€“3685. worksforbiomedicalimagesegmentation,â€inMICCAI. Springer,2015,
[14] W.Kim,W.D.Kim,J.-J.Kim,andetal.,â€œUVtac:Switchableuvmarker- pp.234â€“241.
based tactile sensing finger for effective force estimation and object [33] A. Telea, â€œAn image inpainting technique based on the fast marching
localization,â€RA-L,vol.7,no.3,pp.6036â€“6043,2022. method,â€Journalofgraphicstools,vol.9,no.1,pp.23â€“34,2004.
[15] C.Chorley,C.Melhuish,T.Pipe,andetal.,â€œDevelopmentofatactile [34] M. Bertalmio, A. L. Bertozzi, and G. Sapiro, â€œNavier-stokes, fluid
sensor based on biologically inspired edge encoding,â€ in International dynamics, and image and video inpainting,â€ in CVPR, vol. 1, 2001,
ConferenceonAdvancedRobotics,2009,pp.1â€“6. pp.Iâ€“I.
[16] B. Winstone, C. Melhuish, T. Pipe, and et al., â€œToward bio-inspired [35] J. A. Sethian, â€œA fast marching level set method for monotonically
tactilesensingcapsuleendoscopyfordetectionofsubmucosaltumors,â€ advancing fronts.â€ Proceedings of the National Academy of Sciences,
IEEESensorsJournal,vol.17,no.3,pp.848â€“857,2016. vol.93,no.4,pp.1591â€“1595,1996.
[17] B.Ward-Cherrier,L.Cramphorn,andN.F.Lepora,â€œTactilemanipula- [36] L.-C. Chen, G. Papandreou, F. Schroff, and et al., â€œRethinking
tion with a tacthumb integrated on the open-hand m2 gripper,â€ RA-L, atrous convolution for semantic image segmentation,â€ arXiv preprint
vol.1,no.1,pp.169â€“175,2016. arXiv:1706.05587,2017.
[18] R.Li,R.Platt,W.Yuan,andetal.,â€œLocalizationandmanipulationof [37] S. Wang, Y. She, B. Romero, and E. Adelson, â€œGelSight Wedge:
small parts using gelsight tactile sensing,â€ in IROS, 2014, pp. 3988â€“ Measuring high-resolution 3d contact geometry with a compact robot
3993. finger,â€inICRA,2021,pp.6468â€“6475.
[19] D.F.Gomes,Z.Lin,andS.Luo,â€œGelTip:Afinger-shapedopticaltactile [38] P.Isola,J.-Y.Zhu,T.Zhou,andetal.,â€œImage-to-imagetranslationwith
sensorforroboticmanipulation,â€inIROS,2020,pp.9903â€“9909. conditionaladversarialnetworks,â€inCVPR,2017,pp.1125â€“1134.
[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, and et al., â€œGenerative [39] D. F. Gomes, P. Paoletti, and S. Luo, â€œGeneration of GelSight tactile
adversarialnets,â€NeurIPS,vol.27,2014. imagesforsim2reallearning,â€RA-L,vol.6,no.2,pp.4177â€“4184,2021.
[21] D.P.KingmaandM.Welling,â€œAuto-encodingvariationalbayes,â€arXiv [40] S.Luo,W.Yuan,E.Adelson,andetal.,â€œViTac:Featuresharingbetween
preprintarXiv:1312.6114,2013. visionandtactilesensingforclothtexturerecognition,â€inICRA,2018,
[22] K. Gregor, I. Danihelka, A. Mnih, and et al., â€œDeep autoregressive pp.2722â€“2727.
networks,â€inICML,2014,pp.1242â€“1250. [41] A.Bansal,E.Borgnia,H.-M.Chu,andetal.,â€œColddiffusion:Inverting
[23] D. Rezende and S. Mohamed, â€œVariational inference with normalizing arbitraryimagetransformswithoutnoise,â€NeurIPS,vol.36,2024.
flows,â€inICML,2015,pp.1530â€“1538. [42] W. D. Kim, S. Yang, W. Kim, and et al., â€œMarker-Embedded tactile
[24] J.Ho,A.Jain,andP.Abbeel,â€œDenoisingdiffusionprobabilisticmodels,â€ imagegenerationviagenerativeadversarialnetworks,â€RA-L,2023.
NeurIPS,vol.33,pp.6840â€“6851,2020. [43] K.He,X.Zhang,S.Ren,andetal.,â€œDeepresiduallearningforimage
[25] W.Chen,Y.Xu,Z.Chen,andetal.,â€œBidirectionalsim-to-realtransfer recognition,â€inCVPR,2016,pp.770â€“778.
for GelSight tactile sensors with cyclegan,â€ RA-L, vol. 7, no. 3, pp.
6187â€“6194,2022.