Strategies for Pretraining Neural Operators
AnthonyZhou1 ayz2@andrew.cmu.edu
DepartmentofMechanicalEngineering
CarnegieMellonUniversity
CooperLorsung1 clorsung@andrew.cmu.edu
DepartmentofMechanicalEngineering
CarnegieMellonUniversity
AmirPouyaHemmasian ahemmasi@andrew.cmu.edu
DepartmentofMechanicalEngineering
CarnegieMellonUniversity
AmirBaratiFarimani∗ barati@cmu.edu
DepartmentofMechanicalEngineering
DepartmentofMachineLearning
CarnegieMellonUniversity
Abstract
Pretraining for partial differential equation (PDE) modeling has recently shown promise in scal-
ing neural operators across datasets to improve generalizability and performance. Despite these
advances,ourunderstandingofhowpretrainingaffectsneuraloperatorsisstilllimited;studiesgen-
erally propose tailored architectures and datasets that make it challenging to compare or examine
different pretraining frameworks. To address this, we compare various pretraining methods with-
out optimizing architecture choices to characterize pretraining dynamics on different models and
datasets as well as to understand its scaling and generalization behavior. We find that pretraining
ishighlydependentonmodelanddatasetchoices,butingeneraltransferlearningorphysics-based
pretrainingstrategiesworkbest.Inaddition,pretrainingperformancecanbefurtherimprovedbyus-
ingdataaugmentations. Lastly,pretrainingisadditionallybeneficialwhenfine-tuninginscarcedata
regimes or when generalizing to downstream data similar to the pretraining distribution. Through
providinginsightsintopretrainingneuraloperatorsforphysicsprediction,wehopetomotivatefu-
tureworkindevelopingandevaluatingpretrainingmethodsforPDEs.
1 Introduction
Pretrainingisanimmenselypopulartechniqueindeeplearninginwhichmodelslearnmeaningfulcontextfromalarge
datasetandapplythisknowledgetodownstreamtasks(Devlinetal.,2019;Chenetal.,2023;Schiappaetal.,2022).
Inparticular,recentworkhashighlightedtheimportanceofself-supervisedlearning,whichcanleveragetheinherent
structureofunlabeleddataandlearnmeaningfullatentrepresentations(Bardesetal.,2022;Chenetal.,2020;Leyva-
Vallina et al., 2023; He et al., 2021). The success of these self-supervised pretraining strategies has motivated their
application to broad scientific and engineering problems (Wang et al., 2022a;b; Cao et al., 2023; Zhou & Farimani,
2024a; Meidani et al., 2024; Li et al., 2024; Nguyen et al., 2023). In particular, pretraining has been used in partial
differential equation (PDE) modeling to improve neural operators and evaluate their scalability and generalizability
(McCabeetal.,2023;Haoetal.,2024).
NeuraloperatorsforPDEshavegainedsubstantialinterestinrecentyearsduetotheirabilitytoquicklypredictphysics
through inference (Li et al., 2021; Lu et al., 2021a; Brandstetter et al., 2023). Despite potential speed gains, neural
1Theseauthorscontributedequallytothiswork
*CorrespondingAuthor
1
4202
nuJ
21
]GL.sc[
1v37480.6042:viXraoperators currently struggle to generalize to unseen physics, and initial training can be slow (Lu et al., 2022; Gupta
&Brandstetter,2022). Toaddressthisissue,manyworkshaveexploreddifferentstrategiestoimprovegeneralization
by incorporating additional system information (Lorsung et al., 2024; Liu et al., 2023; Takamoto et al., 2023) and
pretrainingneuraloperatorsacrosslarge,diversephysicstoquicklyfine-tunetosolvePDEs(Haoetal.,2024;McCabe
et al., 2023; Shen et al., 2024; Hang et al., 2024; Goswami et al., 2022). Despite showing good performance, these
worksusuallyrequiretheuseoftailoredneuraloperatorsanddatasetstolearndifferentphysics. Thiscontrastswith
broader deep learning trends in which pretraining methods can universally benefit models; for example, pretraining
losses that are applied across CNN models (Noroozi & Favaro, 2016; Lee et al., 2017) or GNN models (Hu et al.,
2020). Asaresult, inthiswork, weconsiderexistingpretrainingframeworks, aswellasproposenovelmethodsfor
pretrainingPDEmodelsthatareflexibleandcanbeappliedacrossarchitecturesordatasets.
Byconsideringpretrainingmethodsthataremodelagnostic,wecanprovideadetailedandlevelcomparisonofpre-
training methods on a shared experimental setup. To our knowledge, this is the first work that makes an effort to
comparepretrainingstrategieswithouttailoredarchitecturechoices,whichallowsanunderstandingofhowpretrain-
ing affects learning in different regimes. Specifically, we compare different pretraining strategies and consider the
effectofPDEdataaugmentations,apopulartechniquetoimprovepretrainedmodelperformance(Heetal.,2021;Xie
etal.,2022;Zhou&Farimani,2024b;Brandstetteretal.,2022). Additionally,westudytheperformanceofpretrained
modelswithscarcefine-tuningdataaswellastheirgeneralizationbehaviortounseencoefficientsorPDEs.
Throughthiswork,wehopetobroadentheunderstandingofhowneuraloperatorscanbepretrainedforphysicspre-
diction. We organize existing pretraining strategies, propose novel vision-inspired strategies, and include common
pretrainingbaselinestoassembleabroadsetofmethodsforlearningPDErepresentations. WefindthatPDEpretrain-
ingvariesdependingonmodelanddatasetchoice,butingeneralusingtransferorphysics-basedpretrainingstrategies
work well. In addition, transformer or CNN-based architectures tend to benefit more from pretraining than vanilla
neuraloperators. Furthermore,theuseofdataaugmentationsconsistentlyimprovespretrainingperformanceindiffer-
entmodels,datasets,andpretrainingstrategies. Lastly,wefindthatpretrainingismorebeneficialwhenfine-tuningin
low-data regimes, or when downstream data is more similar to pretraining data. We hope that these insights can be
usedtoguidefutureworkinthedevelopmentandevaluationofpretrainingmethodsforPDEs. Weopensourceour
codehere: https://github.com/anthonyzhou-1/pretraining_pdes.
2 Related Works
The field of neural operators has grown rapidly in recent years, with many architectures developed to accurately
approximatesolutionstoPDEs(Lietal.,2021;2023a;Gupta&Brandstetter,2022;Brandstetteretal.,2023;Luetal.,
2021a). ManyworksexpandedonthistoproposearchitecturestosolvePDEsmorequickly,withlesscompute,oron
irregulargrids(Lietal.,2023b;Hemmasian&BaratiFarimani,2023;Lietal.,2023c),andasaresult,withinarange
oftestproblems,neuraloperatorscansolvePDEsquicklyandaccurately. However,neuraloperatorsstillstruggleto
generalizeacrossdiversephysics,andasaresultmanyapproacheshavebeendevelopedtopretrainneuraloperators.
WesummarizethesepastworksinTable1,andbrieflydescribethemainapproacheshere.
2.1 PDE Transfer Learning
Many past works consider transferring knowledge between PDE parameters and domains as a form of pretraining.
These works often design specific architectures that are tailored for transferring weights or layers between tasks.
Forexample, Goswamietal.(2022)designtask-specificlayersofaDeepONettobeusedwithdifferentdomainsof
2D Darcy Flow and Elasticity problems. Another approach proposed by Tripura & Chakraborty (2023) is to design
differentoperatorsthatlearnspecificPDEdynamicsandcombinetheseinamixtureofexpertsapproach, motivated
by the observation that PDEs can often be compositions of each other. To address the issue of transferring between
physicaldomainsthatcanhavedifferentnumbersofvariables,Rahmanetal.(2024)extendpositionalencodingsand
self-attentiontodifferentcodomains/channels.
2.2 Large PDE Modeling
An extension of transfer learning is to train large models on diverse physics datasets, with the intention of learning
transferable representations through scaling behavior (Wei et al., 2022; Kaplan et al., 2020; Brown et al., 2020). 54
2Category PDEs Characteristic Reference
Darcy,Elasticity Fine-tuningtasklayerstotransferbetweendomains/dynamics Goswamietal.(2022)
Poisson,INS DirecttransferacrossPDEdomains Chakrabortyetal.(2022)
Transfer Poisson,INS,Wave,FP Designofatransferrablemodelthroughreparameterizingneurons Zhangetal.(2023c)
Heat,Adv,Nag,Burg,NS,AC CombiningoperatormoduleswithgatingfordifferentPDEs Tripura&Chakraborty(2023)
NS,Elasticity Usingvariablepos.encodingandmaskedpretrainingacrossdomains Rahmanetal.(2024)
Poisson,Helm Scalingmodelanddatasetsizetocharacterizetransferbehavior Subramanianetal.(2023)
SWE,DiffReact,CNS EmbeddingPDEstoacommonspaceandusinganAxialViT McCabeetal.(2023)
INS,CNS,SWE,DiffReact UsingdenoisingandFourierattentionwithlargemodelsanddatasets Haoetal.(2024)
LargeModels
Adv,Burg,Diff-Sorp,SWE,NS AligningLLMguidanceacrossdiversePDEs Shenetal.(2024)
INS,CNS,SWE,DiffReact TrainingaconditionaltransformeracrosslargePDEdatasets Hangetal.(2024)
Poisson,Helm,NS,Wave,AC Scalingoperatortransformerstolarge,diversedatasets Herdeetal.(2024)
KdV,Burg,KS,INS UsingLieSymmetriestoinself-supervisedcontrasivelearning Mialonetal.(2023)
Contrastive Heat,Advection,Burg Usingphysics-informeddistancemetricsinacontrastiveframework Lorsung&Farimani(2024)
Burg,Adv-Diff,NS Usingphysicalinvariancestocontrastivelylearnanencoder Zhangetal.(2023b)
HGO,Elasticity,Tissue Usingamodel-agnosticmeta-learninglosstolearnacrosstasks Zhangetal.(2023a)
Meta-Learning LV,GS,NS UsinganovellosstermtomaximizesharedlearningbetweenPDEs Yinetal.(2021)
LV,GS,GO,NS Usingahyper-networktoadaptPDEoperatorsforspecifictasks Kirchmeyeretal.(2022)
Poisson,Helm,DiffReact,NS Evaluatingmaskedpretrainingandin-contextlearningforPDEs Chenetal.(2024)
In-Context
Poisson,DiffReact In-contextlearningforPDEsthroughpromptingatransformer Yangetal.(2023)
Table 1: A review of past works on pretraining neural operators for PDEs. We organize works by approximate
categoriesanddescribetheirdataandmethods.
initiallyexploresthisscalingbehaviorbytraininglargeneuraloperatormodelsonlargePDEdatasetstoevaluateits
ability to adapt to different coefficients. McCabe et al. (2023) propose a tailored architecture for solving problems
across different physics, and Hao et al. (2024) expand on this by making architectural advancements and training
onmorediverse physics. Despitedifferentapproaches anddatasets, theseworksgenerallyrely ontailored, scalable
architecturesforlargePDEdatasets;pretrainingisframedasphysicspredictionacrossdiversephysicsandfine-tuning
isdoneonthepretrainingdistributionoronunseencoefficients/PDEs.
2.3 PDE Contrastive Learning
Following the success of contrastive learning in the vision domain (Chen et al., 2020; Bardes et al., 2022; Zbontar
et al., 2021), various methods for PDE contrastive learning have been proposed. Mialon et al. (2023) propose a
contrastive learning framework in which augmented PDE samples are represented in a similar way in latent space;
notably augmentations are done with physics-preserving Lie augmentations (Brandstetter et al., 2022). Zhang et al.
(2023b)followasimilarapproachinwhichphysicallyinvariantsamplesareclusteredtogetherinlatentspace,while
Lorsung&Farimani(2024)relyonPDEcoefficientstodefineacontrastiveloss. Ingeneral,contrastivemethodshave
extensiveliteratureandtheory,howevertheytendtobechallengingtopretrainandmayhaveincrementalgainsinthe
PDEdomain.
2.4 Meta/In-context Learning for PDEs
Additionalpastworkconsidersadaptingmeta-learning(Finnetal.,2017)paradigmsfromthebroaderdeeplearning
community to the PDE domain. Zhang et al. (2023a) consider a direct adaptation of model-agnostic meta-learning
toPDEtasks,whileYinetal.(2021)andKirchmeyeretal.(2022)applynovellossesandarchitecturestomaximize
sharedlearningacrossdifferenttasks. Followingin-contextlearningtrendsoftransformermodels(Dongetal.,2023),
Chenetal.(2024)andYangetal.(2023)exploreusingin-contextlearningtopromptmodelswithPDEsolutionsto
generalizetounseenPDEcoefficients.
3Figure1: Anillustrationofpretrainingstrategiesadaptedfromcomputervision(CV)andpredictingPDEcharacter-
istics. Left: CV methods can be described by different shuffling mechanisms and losses. Binary pretraining only
classifies if a sequence is shuffled or not, while TimeSort, SpaceSort and Jigsaw sort sequences shuffled in various
ways, either along the spatial, temporal, or combined dimensions. Right: PDE data has inherent structure that can
beleveragedtopredictunderlyingcharacteristics. CoefficientsofthePDEcanberegressed,aswellasitsspatialand
temporalderivatives.Additionally,inputscanbemaskedtoregressthesolutionfielduandlearnunderlyingdynamics.
3 Methods
3.1 Data Augmentations
Followingtheprevalenceofdataaugmentationinthebroaderdeeplearningcommunity(Chenetal.,2020;Perez&
Wang,2017),weconsidertheuseofdataaugmentationsadaptedtothePDEdomain.
3.1.1 Lie Point Symmetry Data Augmentations
We consider a recent body of work proposing Lie Point Symmetry Data Augmentations (Brandstetter et al., 2022;
Mialonetal.,2023),asetofPDE-specificdataaugmentationsthatpreservetheunderlyingdynamics.Mathematically,
givenaPDE,onecanderiveasetoftransformations{g ,g ,...,g },eachwithaparameter{ε ,ε ,...,ε }thatcanbe
1 2 n 1 2 n
randomlysampledtomodulatethestrengthofthetransformation.SincesomePDEsmayexhibitmoreLiesymmetries
than others, we consider only shifting the PDE solution in space (Shift), which is valid for all PDEs considered, to
ensure a fair comparison between datasets. For further details on mathematical theory and its implementation in
augmentingPDEs,wereferthereadertoMialonetal.(2023)andOlver(1986).
3.1.2 Physics-Agnostic Data Augmentations
Incomputervisionliterature,manysuccessfuldataaugmentationsheavilymodifyinputs(Chenetal.,2020);inpartic-
ular,croppingandcuttingoutportionsofanimagewouldnotrespectphysicsifadaptedtothePDEdomain.Following
this,weinvestigatetheeffectofdataaugmentationsthatarephysics-agnostic,inthattheycanbeappliedtoanyPDE
sincetheaugmentationdoesnotpreservetheunderlyingdynamics. Followingrecentworkondenoisingneuraloper-
atorarchitectures(Haoetal.,2024),weconsideraddingGaussiannoiseduringpretraining(Noise). Furthermore,we
considerscalingthePDEsolution(Scale),anapproachsimilartoacolordistortion,inwhichthePDEsolutionvalues
aremultipliedbyarandomconstant. ForcertainsimplePDEs,scalingcanpreservephysics,butthisisnotgenerally
trueduetononlinearitiesinmorecomplexPDEs. Additionaldetailsonhyperparametersandtheimplementationof
dataaugmentationscanbefoundinAppendixD.4.
3.2 Pretraining Strategies
In this work, we consider using pretraining strategies that are agnostic to the neural operator architecture to ensure
compatibility with different applications and future architecture advances, and describe them in Figure 1. This ap-
proachisalsoconsistentwiththebroadercomputervisiondomain,wheremodelsarefullysharedbetweenpretraining
and downstream tasks and can be adapted to different architectures (e.g. CNN, ViT) (Chen et al., 2020; Xie et al.,
2022; He et al., 2021). We provide further details on design considerations and the implementation of pretraining
strategiesinAppendixD.3.
43.2.1 Computer Vision Strategies
Inspiredbydiversepretrainingstrategiestolearnimagerepresentations,weadaptmanypretrainingstrategiesfromthe
computervision(CV)domaintothePDEdomain. Ingeneral,thesestrategiesaimtotrainmodelsthroughpredicting
visualattributesorsortingspatio-temporalsequencestolearnvisualrepresentationswithoutlabels.
Firstly, we consider an early work that pretrains a model to verify if a video is in the correct temporal order (Misra
etal.,2016).Thisproblemisformulatedasabinaryclassificationtaskinwhichashuffledvideoandtheoriginalvideo
areassignedseparatelabels;withinthiswork,werefertothisasBinarypretraining.
Subsequent work proposed methods that not only verify temporal order, but can also sort temporally shuffled video
frames(Leeetal.,2017). Thisisgenerallyformulatedasan−wayclassificationtask,wherendenotesthenumberof
permutations in which a sequence of frames can be sorted. In the context of physics data, we can opt to shuffle the
dataspatiallyortemporally,assuchwerefertothesetwopretrainingstrategiesasTimeSortorSpaceSort.Empirically,
SpaceSortdoesnotperformwell,soweomitthisstrategyfromourresults.
Anextensionofsortingsamplesthathavebeenshuffledalongasingledimension(e.g.,time,space)istosortsamples
shuffled across all dimensions. For images, sorting images shuffled along both the x and y axes is implemented by
solving jigsaw puzzles, a challenging task that reassembles an image from its shuffled patches (Noroozi & Favaro,
2016). Thisworkhasbeenextendedtothevideodomainbysolvingspatio-temporalpuzzles(Kimetal.,2018). The
extension to PDE data requires sorting data that have been partitioned into discrete patches and shuffled along the
spaceandtimeaxes;werefertothisstrategyasJigsaw. Oneissueisthatthenumberofpossibleclassesscaleswith
thefactorialofthenumberofpatches,andmanyshuffledsequencesarenotsignificantlydifferentfromeachother. To
mitigatethis,wesamplethetopkshuffledpermutationsthatmaximizetheHammingdistancebetweentheshuffledand
theoriginalsequence(Noroozi&Favaro,2016);thisensuresthatmodelscanseediversesamplesduringpretraining
whilelimitingthenumberofclassesinthepretrainingtask.
3.2.2 PDE Predictive Strategies
Within the PDE domain, there are physics-specific characteristics that PDE data exhibit that can be leveraged for
pretraining; this is analogous to predicting motion or appearance statistics in vision pretraining tasks (Wang et al.,
2019;Yaoetal.,2020). OnestrategyconsidersthefactthatPDEdatadependsonequationvariablesandcoefficients,
andpredictingthesecoefficientsfromthePDEdatacouldbeuseful. Thisisimplementedasaregressiontask,where
thecoefficientvaluesareregressedfromasnapshotofPDEdata;werefertothisstrategyasCoefficient.
Additionally, PDE data can be described by the derivatives of current physical values. For example, many finite
differenceschemesrelyonspatialandtemporalderivativesofthecurrentvectororscalarfieldtoadvancethesolution
intime. Inspiredbythis,weproposeapretrainingstrategythatpredictsthespatialandtemporalderivativesofPDE
data. For2DPDEs,thisisimplementedasaregressiontaskswherethefields(u ,u ,u ,u ,u)areregressedfroma
x y xx yy t
solutionu;werefertothisstrategyasDerivative.
Lastly,numericalsolutionsofPDEstendtoleverageinformationoflocalrelationshipstosolveequations.Forexample,
finitedifferenceschemesuseinformationfromneighboringnodestocalculatespatialderivatives. Motivatedbythis,
we propose a pretraining strategy that randomly masks data in space and time and uses this incomplete information
toreconstructthefullsolution. Thisisimplementedbypatchingthesolutioninspaceandtime,randomlyreplacing
maskedpatcheswithalearnablemasktoken,andregressingthetruesolution;werefertothisstrategyasMasked.
3.2.3 Contrastive Strategies
Acommonstrategyforpretrainingincomputervisiondomainsistoexploitsimilaritiesinthedatatoalignsamplesin
latentspace. AproposedstrategytodothisforPDEsisPhysicsInformedContrastiveLearning(PICL),whichuses
a Generalized Contrastive Loss (Leyva-Vallina et al., 2023) to cluster PDE data based on their coefficients in latent
space(Lorsung&Farimani,2024).Anotherstrategyforself-supervisedlearningofPDEdynamicsisusinganencoder
to align Lie augmented or physically invariant latent PDE samples (Mialon et al., 2023; Zhang et al., 2023b). Both
worksrequiretheuseofaspecificencoderalongwiththeneuraloperatorbackbone; toadaptthesestrategiestoour
experimentalsetupweconsiderdirectlypretrainingtheneuraloperatorcontrastivelywiththesestrategies. However,
5Figure 2: Experimental Setup. During pretraining we consider different data augmentations, model choices, and
pretrainingtasksandevaluatetheirdownstreamperformancethroughfine-tuningonphysicspredictiontasks. During
fine-tuning,weleveragethesamepretrainedmodeltoimprovefixed-futureorautoregressivepredictiononthesame
pretrainingdatadistribution,unseencoefficients,ornewPDEs. Throughthissetupwecanexploreawidevarietyof
pretrainingstrategiesandaugmentationsandquantifytheireffectsondifferentmodels,PDEs,datasets,andtasks.
these methods did not seem to show significant improvements over no pretraining, as such, the results are omitted
fromthepaper.
4 Experiments
Toevaluatetheeffectivenessoftheproposedpretrainingstrategiesanddataaugmentations,weconsideradiverseset
of experiments and neural operator architectures to train on. In particular, we hope to understand whether different
architecturesordatasetsinfluencepretrainingperformanceandconstructaholisticviewofpretrainingfordiversePDE
applications. WeprovideanoverviewofthesetupandthedifferentexperimentspossibleinFigure2.
4.1 Data
We consider predicting physics for the 2D Heat, Advection, Burgers, and incompressible Navier-Stokes equations.
These equations describe a diverse range of fluid phenomena and form tasks of varying difficulties. For our experi-
ments,weconsiderpretrainingonacombinedsetof2DHeat,Advection,andBurgersdata,whichcontain9216data
samples(3072foreachequation), aswellasfine-tuningonasmallersetof1024unseensamplesforeachPDE.We
onlypretrainontheHeat,Advection,andBurgersequationssincethenumericaldataforthesePDEsareeasiertogen-
erate, andasaresult, transferringpretrainedknowledgetomorechallengingPDEscanbeevaluatedasapotentially
usefulmethod.
4.1.1 Heat, Advection, and Burgers Equations
The2DHeat,Advection,andBurgersequationsaregivenby:
∂u−ν∇2u=0, Heat (1)
t
∂u+c·∇u=0, Advection (2)
t
∂u+u(c·∇u)−ν∇2u=0, Burgers (3)
t
Toensureadiversesetofphysicsdata,theequationcoefficientsarerandomlysampledaccordingtoZhou&Farimani
(2024b). In particular, for the Heat equation, we sample ν ∈[2×10−3,2×10−2], for the Advection equation, we
samplec=[c ,c ]∈[0.1,2.5]2,andfortheBurgersequation,wesampleν∈[7.5×10−3,1.5×10−2],andc=[c ,c ]∈
x y x y
[0.5,1.0]2; we refer to this dataset as in-distribution (In). Since these equations also comprise the pretraining set,
we additionally consider a case where the downstream dataset comes from a separate distribution; in this case, we
sample ν ∈[2×10−2,3×10−2] for the Heat equation, c=[c ,c ]∈[2.5,3.0]2 for the Advection equation, and ν ∈
x y
[5.0×10−3,7.5×10−3], and c=[c ,c ]∈[1.0,1.25]2 for the Burgers equation. We refer to this dataset as out-of-
x y
distribution(Out).
Inallcases, periodicboundaryconditionsareenforcedandthesolutionissolvedinadomain(x,y)=[−1,1]2 from
t =0tot =2. Furthermore, initialconditionsarerandomlyfromasummationofsinefunctions; theparametersare
6uniformlysampledfromfromA ∈[−0.5,0.5],ω ∈[−0.4,0.4],l ∈{1,2,3},l ∈{1,2,3},φ ∈[0,2π)whilefixing
j j xj yj j
J=5,L=2:
J
u(0,x,y)= ∑A sin(2πl x/L+2πl y/L+φ ) (4)
j xj yj j
j=1
Foradditionalinformationondatasplitsandnumericalmethods,wereferreaderstoAppendixD.1.
4.1.2 Incompressible Navier Stokes Equations
The incompressible Navier Stokes equations are considered for fine-tuning pretrained models to predict more chal-
lengingphysics. Toensureconsistencybetweenthepretrainingandfine-tuningtasks,weusethevorticityformofthe
Navier-StokesequationinordertopredictascalarfieldfollowingthesetupinLietal.(2021):
∂ω+u·∇ω−ν∇2ω = f(x,y), ∇·u=0, ∇×u=ω (5)
t
f(x,y)=A(sin(2π(x+y))+cos(2π(x+y))) (6)
We formulate this problem with periodic boundary conditions, variable viscosity ν, and variable forcing function
amplitude A. Specifically, the viscosity is sampled uniformly from ν ∈{{1,2,3,4,5,6,7,8,9}×10−{6,7,8,9}} and
theamplitudeisuniformlysampledfromA∈{{1,2,3,4,5,6,7,8,9,10}×10−3}. Thedataisgeneratedinadomain
(x,y)=[0,1]2 and from t =0 to t =7.75, following the setup from Lorsung et al. (2024); furthermore, the initial
conditionsω aregeneratedfromaGaussianrandomfieldaccordingtoLietal.(2021).
0
4.2 Neural Operators
To compare different pretraining and data augmentation strategies, we consider their effects on improving the PDE
prediction performance of different neural operators. Specifically, we consider the neural operators: Fourier Neural
Operator (FNO) (Li et al., 2021), DeepONet (Lu et al., 2021a) and OFormer (Li et al., 2023a). Additionally, we
considertheUnetmodel;whileitisnotexplicitlyaneuraloperator,itiscommonlyusedinliteratureandhasshown
good performance (Ronneberger et al., 2015; Gupta & Brandstetter, 2022). These neural operators are first trained
using a pretraining strategy before being fine-tuned on a PDE prediction task; this could either be fixed-future pre-
dictiontomodelastaticsolutionorautoregressivepredictiontomodelatime-dependentsolution. Inallexperiments,
predictiontasksareformulatedusingonlysolutionfieldvaluesandgridinformation. Additionaldetailsonthemodel
hyperparametersandimplementationcanbefoundinAppendixD.2.
4.3 Pretraining Strategies
We compare models pretrained with different strategies with a baseline model that has not been pretrained (None)
as well as a model trained with the same physics prediction objective on the pretraining dataset, more commonly
knownastransferlearning(Transfer). Furthermore,wevarythesizeofthefine-tuningdatasettostudytheeffectsof
pretrainingwhengivenscarcedownstreamdata. Thefine-tuningdatasetisalsovariedbetweendatasamplesthatare
withinthepretrainingdistribution(In),outsidethepretrainingdistributionwithrespecttothePDEcoefficients(Out),
oronsamplesfromanunseenPDE(NS).Lastly,westudytheeffectsofaddingdataaugmentationsduringpretraining
andfine-tuning.
4.4 Data Augmentation
Data augmentation is implemented by doubling the pretraining and fine-tuning data, where each sample has a 50%
chanceofbeingaugmented. OurnoiseaugmentationaddsasmallamountofGaussiannoisetoeachframeindepen-
dently,whileourshiftandscaleaugmentationsareapplieduniformlytotheentiretrajectory.
7Table 2: Effects of Pretraining for Auto-regressive Prediction: We present comparisons of different pretraining
strategiesafterpretrainingontheHeat,Advection,andBurgersequationsandfine-tuningin500unseensamples.The
insightsaredistilledintotwotables,forfullresultsseeAppendixA.
(a) The best pretraining strategy varies with model and (b) Different models display different benefits from pre-
datasetchoice.Wecomparethehighestperformingpretraining training.Wecomparetheimprovementofthehighestperform-
strategies on autoregressive prediction; although performance ing pretraining strategy to no pretraining. The models show
varieswidely,transferlearningperformswellinmanysettings. differentcapacitiestobepretrained.
BestPretrainingMethod Improvementw/BestStrategy
Model Heat Advection Burgers NS Model Heat Advection Burgers NS
FNO Derivative Transfer PICL None FNO 14.43% 7.459% 1.430% 0.000%
DeepONet PICL Transfer Transfer Transfer DeepONet 3.580% 1.852% 15.74% 2.894%
OFormer Transfer PICL Transfer None OFormer 38.91% 4.594% 17.12% 0.000%
Unet Transfer TimeSort Transfer Transfer Unet 29.16% 1.899% 9.706% 1.862%
4.5 Fixed Future and Auto-regressive Prediction
Tomodelphysicsproblemswithstaticsolutions,weconsiderpredictingaPDEsolutionfieldatafixedtimestepafter
aninitialsnapshotofthePDEdata. Inparticular,giventhePDEdatafromt=1tot=8,modelsaretrainedtopredict
thePDEsolutionatt=32.
Alternatively,tomodelphysicsproblemswithtime-dependentsolutions,weconsiderauto-regressivelypredictingPDE
solutionsdirectlyafteracurrentsnapshotofPDEdata. ThisisimplementedusingPDEdataontheinterval[t,t+8)
as an input to predict future PDE solutions on the interval [t+8,t+16). In addition, we use the pushforward trick
(Brandstetteretal.,2023)tostabilizetraining. Thisintroducesmodelnoiseduringtrainingbyfirstpredictingafuture
timewindowfromground-truthdataandthenusingthisnoisypredictionasamodelinput;importantly,nogradients
arepropagatedthroughthefirstforwardpass. AdditionaldetailsontrainingparameterscanbefoundinD.5.
5 Results
We now systematically benchmark our pretraining and data augmentation strategies, as well as their combination.
Presented below are results on our autoregressive task. Fixed-future results are given in appendices A and B and
generally show the same trends as our autoregressive results. We use Relative L2 error (Li et al., 2021) for both
trainingandevaluationinallofourexperiments.
5.1 Comparison of Pretraining Strategies
We benchmark our proposed PDE pretraining strategies on different neural operators and datasets, and show the
condensedresultsforauto-regressivepredictioninTable2. Foradetailedcomparison,wepresentresultsofdifferent
PDEpretrainingstrategiesforfixed-futureandauto-regressivetasksonalldatasetsinAppendixA.Additionally,we
considercaseswherethefine-tuningdatasetcontainscoefficientsunseenduringpretraining,andpresenttheseout-of-
distributionresultsinAppendixAaswell.
Throughtheseexperiments,wefindmultipleinsights. Firstly,weobservethatthepretrainingperformancevarieswith
thechoiceofmodelanddataset. Specifically, differentmodelsbenefitdifferentlyfrompretraining, aswellasbased
onthepredictedPDEandtask(i.e. fixed-futurevs. auto-regressive). However, transferlearninggenerallyperforms
well across different tasks, models, and datasets, suggesting that it is a good choice for a pretraining task. This is
also reflected in the literature, where previous work generally focuses on transferring knowledge between datasets
(Chen et al., 2021; Goswami et al., 2022; Chakraborty et al., 2022; Tripura & Chakraborty, 2023) or pretrain by
predictingphysicsoflargedatasets(Haoetal.,2024;McCabeetal.,2023;Subramanianetal.,2023). Wehypothesize
that transfer learning is effective since PDE data is inherently unlabeled; physics prediction uses future timesteps
as a label, similar to next-token prediction for GPT models, which is cast as self-supervised learning. When the
data is sufficient, using surrogate objectives such as derivatives or sorting sequences may not be as effective as the
8Table3: EffectsofDataAugmentationforAuto-regressivePrediction: Wepresentcomparisonsofdifferentpre-
trainingstrategiescombinedwithdataaugmentationsafterpretrainingontheHeat,Advection,andBurgersequations
andfine-tuningin500unseensamples. Thedataisdistilledintotwotables;forfullresultsseeAppendixB.
(a) The best pretraining with data augmentation strategy (b) Adding data augmentations consistently improves per-
varieswithmodelanddataset. Differentmodelsbenefitfrom formance.Whenchoosingthecorrectcombinationofpretrain-
differentaugmentationswhenpairedwithpretrainingstrategies. inganddataaugmentationstrategies,wefinditimprovesperfor-
Note: pdenotesPICLandt denotestransferlearning. manceduringautoregressivepredictioncomparedtobaselines.
BestAugmentation Improvementw/BestAugmentation
Model Heat Advection Burgers NS Model Heat Advection Burgers NS
FNO Shift Scalet Nonep None FNO 14.21% 8.287% 1.411% 0.000%
DeepONet Shift Shiftt Shiftt Shiftt DeepONet 8.745% 1.537% 13.21% 3.761%
OFormer Noiset Nonep Noiset Noiset OFormer 35.35% 4.426% 16.288% 13.23%
Unet Shiftt Shiftp Shiftt Noise Unet 30.051% 0.735% 10.322% 3.434%
true objective of fixed-future of auto-regressive prediction. Another observation is that pretraining frameworks are
generally dependent on specific architectures; for example, many CV pretraining strategies shuffle patches of data,
which can introduce arbitrary discontinuities and high-frequency modes in FNO models, yet are not as challenging
for convolutional models such as Unet. Furthermore, pretraining strategies are also dependent on the downstream
task;forexample,Derivativepretrainingworkswellforauto-regressivepredictionbutnotfixedfutureprediction,as
thesolutionatadistanttimestepisverydifferentfromthecurrentderivatives,butthesolutionatthenexttimestepis
highlydependentonthecurrentderivatives.
Secondly,weobservethatdirectlyadaptingcomputervisionmethodstothephysicsdomaingenerallyresultsinpoor
performance. In many experiments, using a CV pretraining method would often hurt performance compared to not
pretraining. This points to a general difference between CV and physics tasks. In the vision domain many down-
streamtasksareclassification-based(i.e. ImageNet, ObjectDetection, etc.), whichresultsinmanypretrainingtasks
modeledaroundclassification,whereasphysicspredictionisahigh-dimensionalregressiontask. Beyondthis,physics
predictionsnotonlyneedtobevisuallyconsistent,butalsonumericallyaccurate,whichcanbedifficulttolearnfrom
aclassificationtask. Infact,usingphysics-basedpretrainingmethods,suchastransferringbetweenpredictiontasks,
regressingderivatives,oraphysics-informedcontrastiveloss,generallyresultsinbetterperformance.
Lastly, we observe that different models have different capacities for pretraining. For example, the OFormer archi-
tecture, which is based on transformers, benefits greatly from pretraining in many scenarios; this could be because
transformers lack inductive bias and can model arbitrary relationships. Furthermore, Unet architectures also benefit
consistentlyfrompretraining; thisisreflectedincommonconvolutionalarchitecturesusedforpretrainingintheCV
domain, such as ResNet (He et al., 2015). DeepONet and FNO show smaller improvements with pretraining, sug-
gestingthatthearchitecturesarelesstailoredforpretraining. ThisisespeciallytrueforFNO;wehypothesizethatthe
learnedFouriermodesmaybeverydifferentbetweentasks,resultinginchallengeswhentransferringweightstonew
tasks.
5.2 Comparison of Data Augmentations
To study the effects of augmenting data during pretraining and finetuning, we conduct experiments in which data
augmentationsareaddedtothreepretrainingstrategies(None,Transfer,PICL).Theseexperimentsareruntocompare
data augmentations to a baseline model that is not pretrained, as well as its effects on the most effective pretraining
strategies(i.e. Transfer,PICL).TheresultsaresummarizedinTable3forauto-regressiveprediction,andthecomplete
resultscanbefoundinAppendixB.Wefindthebestaugmentationbyconsideringthepretrainingstrategyandaug-
mentation pairing with the lowest error. To calculate its improvement, this error is compared to a model that is not
pretrained.
Wefindthatdifferentmodelsbenefitfromdifferentaugmentations;forexample,DeepONetperformswellwithshifted
data, but OFormer performs well with noised data. However, across models, datasets, and downstream tasks, one
cangenerallyfindadataaugmentationthatimprovesperformance. Thissuggeststhatthemosteffectivepretraining
9Table4: EffectsofDownstreamDataset: Wecomparetheeffectofpretrainingwithoutdataaugmentationwhenthe
downstreamdatasetisvaried—bothwiththenumberofsamplesorthedistributionofsamples.Wefindthatpretraining
benefitsmoreindata-scarceregimes,aswellaswhenthedownstreamdataissimilartothepretrainingdata.
(a)Pretrainingismorebeneficialwhendownstreamdatais (b)Pretrainingismorebeneficialwhendownstreamdatais
scarce. Improvementismeasuredbycomparingthebestpre- similartopretrainingdata. Foragivendistributionofdown-
trainingmethodwithnopretraining. Wereporttheaverageim- streamdata, wecomparethebestpretrainingstrategywithno
provementacrosstheHeat,Adv,andBurgersPDEsforagiven pretraining and average across PDEs in the distribution. The
#ofsamplesandmodel. modelsshowdifferentgeneralizationcapacities.
BestImprovementoverNone BestImprovementoverNone
#Samples FNO DeepONet OFormer Unet Distribution FNO DeepONet OFormer Unet
100 -5.953% 6.755% 47.60% 23.05% In 6.875% 6.630% 19.57% 13.592%
250 4.111% 7.361% 29.60% 18.18% Out 3.920% -3.383% 34.058% 27.696%
500 6.875% 6.630% 19.57% 13.59% NS -8.658% 2.899% -1.608% 1.865%
1000 2.026% 6.135% 13.27% 2.995%
frameworks should incorporate a data augmentation strategy, and indeed the best-performing models considered in
this study often make use of data augmentations. Transfer learning performs best in nine of our 12 cases, and shift
augmentation performs best in eight of our 12 cases, with their combination performing best in six, suggesting that
thiscombinationimprovesperformancebestacrossdifferentdatasetsandmodels.Webelievethatdataaugmentations
canhelpduetothefactthatPDEdataremainsscarce;numericalsimulationisneededforhighqualitydata,andasa
resultemulatingalargerdatasetwithaugmentationsisbeneficial.
5.3 Scaling Behavior
WecomparetheeffectofpretrainingfordifferentnumbersofdownstreamsamplesinTable4. Wemeasurethiseffect
byfindingthebestpretrainingmethodforagivenmodel,PDE,anddatasetsize,thencalculatingitsimprovementover
no pretraining; after calculating the improvement, we average this metric across the Heat, Advection, and Burgers
PDEsforauto-regressiveprediction. Ingeneral, weobserveatrendinwhichtheimprovementofpretrainedmodels
diminishes as the number of fine-tuning samples increases, which is expected as fine-tuning data approaches the
pretrainingdatasetsize. Itfollowsthatifthedownstreamdataisabundant,directlytrainingonthiswouldbeoptimal.
Additionally,despitethesetrends,therelativeimprovementofdifferentpretrainingstrategiesremainsapproximately
constantbetweendifferentdownstreamdatasetsizes. AnexceptiontothesetrendsistheFNOmodel;wehypothesize
that learned Fourier modes may be more challenging to fine-tune than other learning mechanisms such as attention
matricesorconvolutionalkernels.
Foradetailedcomparisonofthescalingbehaviorinindividualdatasetsandmodels,wereferreaderstoAppendixC.
Empirically, weobserveahighervariancebetweenrandomseedswhenusingasmallerdatasetforfine-tuning. Fur-
thermore,theadvectionequationcangenerallybelearnedwithfewersamplesandtheperformanceisapproximately
constantwithincreasingdatasetsize.Additionally,differentmodelsandpretrainingstrategiesdisplaydifferentscaling
behaviors,withsomemodelsandpretrainingstrategiesdisplayinggreaterincreasesinperformancewhenfine-tuning
to scarce data. This further underscores the importance of proper architecture choices that scale well, such as using
transformer-based neural operators. Lastly, scaling behavior is more pronounced in fixed-future experiments; this
couldbebecausethereislessdatainfixed-futureexperimentsduetoonlypredictingasingletargetperdatasampleas
opposedtopredictingmultipletargetsacrossalongerauto-regressiverollout.
5.4 Generalization Behavior
Wecomparetheeffectofvaryingthedistributionofthedownstreamdatasetontheperformanceofpretrainedmodels.
In particular, we compare fine-tuning to unseen coefficients of the same equation (Out) as well as fine-tuning to
an unseen PDE with novel initial conditions and forcing terms (NS); these results are shown in Table 4 with 500
fine-tuningsamplesforauto-regressiveprediction. Ingeneral, weobservereducedperformancewhenfine-tuningto
theNavier-Stokesequations, comparedtofine-tuningtosampleswithinthepretrainingdistribution(In). Forcertain
10models, thisalsoholdswhenfine-tuningtoadatasetwithunseencoefficients(Out). Thesegeneralizationbehaviors
are also approximately consistent between different sample sizes of the fine-tuning dataset. It is important to note
that certain pretraining frameworks generalize better than others; for example, Coefficient pretraining largely hurts
performance,sincethefine-tuningdistributioncontainsdifferentcoefficientsbyconstruction.
We note that the OFormer and Unet architectures show better performance when fine-tuning to out-of-distribution
samples; we hypothesize that this is due to shifts in coefficients causing easier phenomena to model. For example,
increasingthediffusivityintheheatequationcausestransienteffectstobeconcentratedinafewinitialtimestepsand
sparsebehaviorforthemajorityoftherollout.Nevertheless,undercertainconditions,pretrainingshowsgeneralization
tounseencoefficientsandPDEs,whichisapromisingdirection.
6 Conclusion
Inthiswork,wecomparepretrainingstrategiesforPDEsbyexaminingpretrainingframeworksthatcanbeusedacross
differentmodelsanddatasets. Inparticular,weconsideradaptingCVpretrainingtothePDEdomainthroughsorting
spatio-temporaldatatolearnunderlyingdynamicswithoutlabels. Furthermore,wederiveseveralPDEcharacteristics
that can be predicted, such as its coefficients, derivatives, or reconstructed input. Lastly, we implement existing
contrastive as well as transfer learning strategies to construct a diverse set of pretraining strategies. Notably, these
strategiescanbeappliedtoanymodelandPDEproblemandareflexibletofutureadvancesinarchitecturesordatasets.
Throughpretrainingwithdifferentframeworksanddataaugmentations,wecomparetheireffectsondifferentPDEs,
models,downstreamdatasets,andfine-tuningtasks. Wefindthatpretrainingcanbehighlydependentonmodeland
datasetchoices,butingeneraltransferlearningorphysics-basedstrategiesdowell. Furthermore,wefindthatdirectly
adaptingpretrainingstrategiesfromotherdomainsoftenfails,motivatingtheneedtodesignPDE-specificpretraining
frameworks. Lastly, weobservethatdifferentmodelshavedifferentcapacitiesforpretraining, withtransformerand
CNN based architectures benefiting the most from pretraining and highlighting the need for architectures that have
highcapacityandtransferability.
TofurtherunderstandPDEpretraining, weinvestigatetheeffectofaddingdataaugmentationsandvaryingthefine-
tuningdataset.Wefindthatdataaugmentationsconsistentlybenefitperformance,withtheshiftaugmentationshowing
bestperformancemostoften. Combiningtransferlearningwithshiftaugmentationshowsthebestperformanceinthe
majorityoftestcases. Additionally,pretrainingperformanceisaccentuatedwhenthefine-tuningdatasetisscarceor
similartothepretrainingdistribution. ThroughestablishingadeeperunderstandingofpretrainingforPDEs,wehope
thatfutureworkcanleveragetheseinsightstoproposenewpretrainingstrategiesandexpandoncurrentarchitectures.
References
M.Baer. findiffsoftwarepackage,2018. URLhttps://github.com/maroba/findiff. https://github.com/
maroba/findiff.
Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-
supervised learning. In International Conference on Learning Representations, 2022. URL https://
openreview.net/forum?id=xm6YD62D1Ub.
JohannesBrandstetter,MaxWelling,andDanielE.Worrall. LiePointSymmetryDataAugmentationforNeuralPDE
Solvers,May2022. URLhttp://arxiv.org/abs/2202.07643. arXiv:2202.07643[cs].
JohannesBrandstetter,DanielWorrall,andMaxWelling. Messagepassingneuralpdesolvers,2023.
TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan,RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,Mark
Chen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,
AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020.
ZhonglinCao, RishikeshMagar, YuyangWang, andAmirBaratiFarimani. Moformer: Self-supervisedtransformer
modelformetal–organicframeworkpropertyprediction. JournaloftheAmericanChemicalSociety,145(5):2958–
2967,2023. doi: 10.1021/jacs.2c11420. URLhttps://doi.org/10.1021/jacs.2c11420. PMID:36706365.
11Ayan Chakraborty, Cosmin Anitescu, Xiaoying Zhuang, and Timon Rabczuk. Domain adaptation based transfer
learning approach for solving PDEs on complex geometries. Engineering with Computers, 38(5):4569–4588,
October 2022. ISSN 1435-5663. doi: 10.1007/s00366-022-01661-2. URL https://doi.org/10.1007/
s00366-022-01661-2.
Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey
on vision-language pre-training. Machine Intelligence Research, 20(1):38–56, Feb 2023. ISSN 2731-5398. doi:
10.1007/s11633-022-1369-5. URLhttps://doi.org/10.1007/s11633-022-1369-5.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive
learningofvisualrepresentations. CoRR,abs/2002.05709,2020. URLhttps://arxiv.org/abs/2002.05709.
Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, and Michael W. Mahoney. Data-
efficientoperatorlearningviaunsupervisedpretrainingandin-contextlearning,2024.
Xinhai Chen, Chunye Gong, Qian Wan, Liang Deng, Yunbo Wan, Yang Liu, Bo Chen, and Jie Liu. Transfer
learning for deep neural network-based partial differential equations solving. Advances in Aerodynamics, 3(1):
36, December2021. ISSN2524-6992. doi: 10.1186/s42774-021-00094-7. URLhttps://doi.org/10.1186/
s42774-021-00094-7.
Ze Cheng, Zhongkai Hao, Xiaoqiang Wang, Jianing Huang, Youjia Wu, Xudan Liu, Yiru Zhao, Songming Liu, and
HangSu. Referenceneuraloperators: Learningthesmoothdependenceofsolutionsofpdesongeometricdeforma-
tions,2024.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingofdeepbidirectionaltrans-
formersforlanguageunderstanding,2019.
QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,LeiLi,andZhifang
Sui. Asurveyonin-contextlearning,2023.
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationofdeepnetworks,
2017.
SomdattaGoswami,KatianaKontolati,MichaelD.Shields,andGeorgeEmKarniadakis.Deeptransferoperatorlearn-
ingforpartialdifferentialequationsunderconditionalshift.NatureMachineIntelligence,4(12):1155–1164,Decem-
ber 2022. ISSN 2522-5839. doi: 10.1038/s42256-022-00569-2. URL https://www.nature.com/articles/
s42256-022-00569-2. Number: 12Publisher: NaturePublishingGroup.
JayeshK.GuptaandJohannesBrandstetter.TowardsMulti-spatiotemporal-scaleGeneralizedPDEModeling,Novem-
ber2022. URLhttp://arxiv.org/abs/2209.15616. arXiv:2209.15616[cs].
ZhouHang,YuezhouMa,HaixuWu,HaowenWang,andMingshengLong. Unisolver: Pde-conditionaltransformers
areuniversalpdesolvers,2024.
ZhongkaiHao,ChangSu,SongmingLiu,JuliusBerner,ChengyangYing,HangSu,AnimaAnandkumar,JianSong,
andJunZhu. Dpot: Auto-regressivedenoisingoperatortransformerforlarge-scalepdepre-training,2024.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition,2015.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are
scalablevisionlearners,2021.
AmirPouyaHemmasianandAmirBaratiFarimani. Reduced-ordermodelingoffluidflowswithtransformers. Physics
ofFluids,35(5):057126,052023.ISSN1070-6631.doi:10.1063/5.0151515.URLhttps://doi.org/10.1063/
5.0151515.
Maximilian Herde, Bogdan Raonic´, Tobias Rohner, Roger Käppeli, Roberto Molinaro, Emmanuel de Bézenac, and
SiddharthaMishra. Poseidon: Efficientfoundationmodelsforpdes,2024.
12WeihuaHu,BowenLiu,JosephGomes,MarinkaZitnik,PercyLiang,VijayPande,andJureLeskovec. Strategiesfor
pre-traininggraphneuralnetworks,2020.
Nail H. Ibragimov. CRC Handbook of Lie Group Analysis of Differential Equations: Symmetries, Exact Solutions,
and Conservation Laws. Number v. 1. Taylor & Francis, 1993. ISBN 9780849344886. URL https://books.
google.com/books?id=mpm5Yq1q6T8C.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford,JeffreyWu,andDarioAmodei. Scalinglawsforneurallanguagemodels,2020.
DahunKim,DonghyeonCho,andInSoKweon. Self-supervisedvideorepresentationlearningwithspace-timecubic
puzzles. 112018. URLhttp://arxiv.org/abs/1811.09795.
Matthieu Kirchmeyer, Yuan Yin, Jérémie Donà, Nicolas Baskiotis, Alain Rakotomamonjy, and Patrick Gallinari.
Generalizingtonewphysicalsystemsviacontext-informeddynamicsmodel,2022.
Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised representation learning by
sortingsequences. 82017. URLhttp://arxiv.org/abs/1708.01246.
MaríaLeyva-Vallina,NicolaStrisciuglio,andNicolaiPetkov. Data-efficientlargescaleplacerecognitionwithgraded
similaritysupervision. CVPR,2023.
ZijieLi,KazemMeidani,andAmirBaratiFarimani. Transformerforpartialdifferentialequations’operatorlearning,
2023a.
ZijieLi,DuleShu,andAmirBaratiFarimani. Scalabletransformerforpdesurrogatemodeling,2023b.
ZijieLi,AnthonyZhou,SaurabhPatil,andAmirBaratiFarimani. Cafa: Globalweatherforecastingwithfactorized
attentiononsphere,2024.
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and
AnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferentialequations,2021.
ZongyiLi,NikolaBorislavovKovachki,ChrisChoy,BoyiLi,JeanKossaifi,ShouryaPrakashOtta,MohammadAmin
Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, and Anima Anandkumar. Geometry-
informedneuraloperatorforlarge-scale3dpdes,2023c.
Yuxuan Liu, Zecheng Zhang, and Hayden Schaeffer. Prose: Predicting operators and symbolic expressions using
multimodaltransformers. arXivpreprintarXiv:2309.16816,2023.
Cooper Lorsung and Amir Barati Farimani. PICL: Physics Informed Contrastive Learning for Partial Differential
Equations,January2024. URLhttp://arxiv.org/abs/2401.16327. arXiv:2401.16327[physics].
Cooper Lorsung, Zijie Li, and Amir Barati Farimani. Physics informed token transformer for solving partial differ-
ential equations. Machine Learning: Science and Technology, 5(1):015032, feb 2024. doi: 10.1088/2632-2153/
ad27e3. URLhttps://dx.doi.org/10.1088/2632-2153/ad27e3.
Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear opera-
tors via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):
218–229, March 2021a. ISSN 2522-5839. doi: 10.1038/s42256-021-00302-5. URL http://dx.doi.org/10.
1038/s42256-021-00302-5.
Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library for solving
differentialequations. SIAMReview,63(1):208–228,2021b. doi: 10.1137/19M1274067.
Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George Em Karni-
adakis. A comprehensive and fair comparison of two neural operators (with practical extensions) based on fair
data. ComputerMethodsinAppliedMechanicsandEngineering,393:114778,April2022. ISSN0045-7825. doi:
10.1016/j.cma.2022.114778. URLhttp://dx.doi.org/10.1016/j.cma.2022.114778.
13Michael McCabe, Bruno Régaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bi-
etti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee, Tiberiu Tesileanu,
KyunghyunCho,andShirleyHo. MultiplePhysicsPretrainingforPhysicalSurrogateModels,October2023. URL
http://arxiv.org/abs/2310.02994. arXiv:2310.02994[cs,stat].
Kazem Meidani, Parshin Shojaee, Chandan K. Reddy, and Amir Barati Farimani. Snip: Bridging mathematical
symbolicandnumericrealmswithunifiedpre-training,2024.
Grégoire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak T. Kiani. Self-
Supervised Learning with Lie Symmetries for Partial Differential Equations, July 2023. URL http://arxiv.
org/abs/2307.05432. arXiv:2307.05432[cs,math].
IshanMisra,C.LawrenceZitnick,andMartialHebert. Shuffleandlearn: Unsupervisedlearningusingtemporalorder
verification. 32016. URLhttp://arxiv.org/abs/1603.08561.
Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K. Gupta, and Aditya Grover. Climax: A foundation
modelforweatherandclimate,2023.
MehdiNorooziandPaoloFavaro. Unsupervisedlearningofvisualrepresentationsbysolvingjigsawpuzzles. 32016.
URLhttp://arxiv.org/abs/1603.09246.
PeterOlver. ApplicationsofLieGroupstoDifferentialEquations. SpringerNewYork,NY,1986.
LuisPerezandJasonWang. Theeffectivenessofdataaugmentationinimageclassificationusingdeeplearning,2017.
MdAshiqurRahman,RobertJosephGeorge,MogabElleithy,DanielLeibovici,ZongyiLi,BorisBonev,ColinWhite,
Julius Berner, Raymond A. Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, and Anima Anandkumar. Pretraining
codomainattentionneuraloperatorsforsolvingmultiphysicspdes,2024.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedicalimagesegmen-
tation,2015.
MadelineC.Schiappa,YogeshS.Rawat,andMubarakShah. Self-supervisedlearningforvideos: Asurvey. 62022.
doi: 10.1145/3577925. URLhttp://arxiv.org/abs/2207.00419http://dx.doi.org/10.1145/3577925.
JunhongShen,TanyaMarwah,andAmeetTalwalkar. Ups: Efficientlybuildingfoundationmodelsforpdesolvingvia
cross-modaladaptation,2024.
Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael Mahoney, and
AmirGholami. TowardsFoundationModelsforScientificMachineLearning: CharacterizingScalingandTransfer
Behavior,May2023. URLhttp://arxiv.org/abs/2306.00258. arXiv:2306.00258[cs,math].
MakotoTakamoto,FrancescoAlesiani,andMathiasNiepert. CAPE:Channel-attention-basedPDEparameterembed-
dingsforsciML,2023. URLhttps://openreview.net/forum?id=22z1JIM6mwI.
Tapas Tripura and Souvik Chakraborty. A foundational neural operator that continuously learns without forgetting,
October2023. URLhttp://arxiv.org/abs/2310.18885. arXiv:2310.18885[cs].
JiangliuWang,JianboJiao,LinchaoBao,ShengfengHe,YunhuiLiu,andWeiLiu. Self-supervisedspatio-temporal
representationlearningforvideosbypredictingmotionandappearancestatistics. 42019. URLhttp://arxiv.
org/abs/1904.03597.
Yuyang Wang, Rishikesh Magar, Chen Liang, and Amir Barati Farimani. Improving molecular contrastive learning
via faulty negative mitigation and decomposed fragment contrast. Journal of Chemical Information and Model-
ing, 62(11):2713–2725, 2022a. doi: 10.1021/acs.jcim.2c00495. URL https://doi.org/10.1021/acs.jcim.
2c00495. PMID:35638560.
YuyangWang,JianrenWang,ZhonglinCao,andAmirBaratiFarimani. Molecularcontrastivelearningofrepresen-
tationsviagraphneuralnetworks. NatureMachineIntelligence,4(3):279–287,Mar2022b. ISSN2522-5839. doi:
10.1038/s42256-022-00447-x. URLhttps://doi.org/10.1038/s42256-022-00447-x.
14Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,
andWilliamFedus. Emergentabilitiesoflargelanguagemodels,2022.
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A
simpleframeworkformaskedimagemodeling,2022.
LiuYang,SitingLiu,TingweiMeng,andStanleyJOsher. In-contextoperatorlearningwithdatapromptsfordiffer-
entialequationproblems. ProceedingsoftheNationalAcademyofSciences,120(39):e2310142120,2023.
Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, and Qixiang Ye. Video playback rate perception for self-
supervisedspatio-temporalrepresentationlearning. 62020. URLhttp://arxiv.org/abs/2006.11476.
YuanYin,IbrahimAyed,EmmanueldeBézenac,NicolasBaskiotis,andPatrickGallinari.Leads:Learningdynamical
systemsthatgeneralizeacrossenvironments. 62021. URLhttp://arxiv.org/abs/2106.04546.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via
redundancyreduction,2021.
LuZhang,HuaiqianYou,TianGao,MoYu,Chung-HaoLee,andYueYu. Metano: Howtotransferyourknowledge
onlearninghiddenphysics,2023a.
Rui Zhang, Qi Meng, and Zhi-Ming Ma. Deciphering and integrating invariants for neural operator learning with
variousphysicalmechanisms. NationalScienceReview,pp.nwad336,December2023b. ISSN2095-5138,2053-
714X. doi: 10.1093/nsr/nwad336. URLhttp://arxiv.org/abs/2311.14361. arXiv:2311.14361[physics].
ZezhongZhang,FengBao,LiliJu,andGuannanZhang.Transnet:Transferableneuralnetworksforpartialdifferential
equations,2023c.
AnthonyZhouandAmirBaratiFarimani. Faultformer: Pretrainingtransformersforadaptablebearingfaultclassifi-
cation. IEEEAccess,12:70719–70728,2024a. doi: 10.1109/ACCESS.2024.3399670.
AnthonyZhouandAmirBaratiFarimani. Maskedautoencodersarepdelearners,2024b.
15A Comparison of Pretraining Strategies
A.1 Fixed Future Experiments
Table 5: Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetuned on 500
samplesforeachPDE.NormalizedL2errors(×10−1)arecalculatedon256validationsamplesandaveragedoverfive
seeds. Thelowesterrorsaregivenin darkgrey,andsecondlowesterrorsaregivenin lightgrey.
(a)FixedFuturePretrainingResults.
PDE Model None Transfer Binary TimeSort Jigsaw CoefficientDerivative Masked PICL
FNO 0.240 0.467 0.813 0.771 0.838 0.387 2.742 0.557 0.182
DeepONet 0.669 0.246 0.755 0.435 0.598 0.467 0.602 0.483 0.675
Heat
OFormer 0.762 0.275 7.520 8.822 3.450 1.898 0.413 0.531 0.630
Unet 0.150 0.061 0.378 0.339 0.483 0.234 0.131 0.118 0.145
FNO 3.533 1.517 7.741 5.427 6.138 6.442 6.205 4.522 3.555
DeepONet 9.907 9.587 10.006 9.814 9.978 9.875 9.926 9.840 9.952
Adv
OFormer 9.645 5.334 10.006 10.022 10.006 10.010 9.878 9.795 9.206
Unet 3.962 1.488 5.747 5.568 6.509 9.286 4.909 4.058 3.802
FNO 0.704 0.675 1.238 1.120 1.226 1.139 4.461 0.896 0.694
DeepONet 4.096 3.37 4.758 3.638 3.869 3.776 3.987 3.674 4.195
Burgers
OFormer 1.92 1.517 9.318 9.792 5.024 3.610 2.112 1.997 1.994
Unet 1.027 0.771 1.382 1.174 1.450 1.168 0.918 0.822 0.989
FNO 2.112 2.147 3.500 6.285 3.530 3.874 6.200 2.386 2.232
DeepONet 5.560 5.226 7.650 5.907 15.208 5.712 5.990 5.610 5.514
NS
OFormer 3.744 3.801 6.056 6.099 6.099 5.445 4.631 4.670 4.056
Unet 2.279 1.403 3.261 2.847 3.488 2.493 2.341 2.332 2.262
(b)Out-of-DistributionFixedFuturePretrainingResults.
PDE Model None Transfer Binary TimeSort Jigsaw CoefficientDerivative Masked PICL
FNO 7.507 1.619 8.842 8.371 8.957 8.966 13.610 7.219 8.282
DeepONet 1.008 3.187 1.507 2.570 13.584 2.845 1.846 1.517 2.089
Heat
OFormer 11.142 6.87 12.291 11.981 12.106 11.568 11.862 11.408 11.317
Unet 5.510 1.034 7.738 6.147 12.880 6.240 4.944 3.968 5.373
FNO 1.456 1.766 2.406 2.221 2.048 2.013 2.298 2.477 1.407
DeepONet 9.600 9.558 9.955 9.581 9.654 9.616 9.558 9.555 9.563
Adv
OFormer 8.749 8.23 9.923 9.974 9.750 10.013 7.558 9.187 8.775
Unet 1.952 2.563 2.746 2.486 2.614 3.162 2.294 1.84 1.802
FNO 0.195 0.454 0.685 0.688 0.742 0.307 1.734 0.627 0.19
DeepONet 0.189 0.122 0.307 0.150 0.240 0.150 0.154 0.144 0.164
Burgers
OFormer 0.528 0.163 6.134 6.928 2.400 1.558 0.387 0.448 0.585
Unet 0.342 0.054 0.333 0.218 0.333 0.147 0.202 0.240 0.340
16A.2 Auto-regressive Experiments
Table 6: Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetuned on 500
samplesforeachPDE.NormalizedL2errors(×10−1)arecalculatedon256validationsamplesandaveragedoverfive
seeds. Thelowesterrorsaregivenin darkgrey,andsecondlowesterrorsaregivenin lightgrey.
(a)AutoregressivePretrainingResults.
PDE Model None Transfer Binary TimeSort Jigsaw CoefficientDerivative Masked PICL
FNO 2.730 5.507 3.888 4.704 3.878 4.838 2.336 3.984 2.584
DeepONet 2.374 2.429 2.618 2.592 3.046 2.589 2.352 2.32 2.289
Heat
OFormer 4.410 2.694 24.214 11.398 5.498 6.982 3.277 3.274 4.418
Unet 3.357 2.378 3.286 2.768 2.586 2.406 2.464 2.39 3.019
FNO 30.890 28.586 30.669 29.888 31.571 29.571 29.875 29.584 30.676
DeepONet 27.971 27.453 28.058 28.778 28.637 28.307 28.861 28.387 28.016
Adv
OFormer 30.102 30.784 29.677 29.674 29.293 29.299 30.467 30.774 28.719
Unet 30.640 30.832 30.17 30.058 30.992 31.027 30.579 30.310 30.355
FNO 5.104 5.696 6.362 6.640 5.373 6.310 5.168 5.466 5.031
DeepONet 5.101 4.298 5.706 5.341 5.638 5.702 5.024 5.190 5.167
Burgers
OFormer 7.734 6.41 25.731 18.102 10.157 10.026 7.059 7.101 8.293
Unet 5.440 4.912 6.339 5.763 5.277 5.312 5.280 5.197 5.656
FNO 5.884 6.708 8.626 11.211 7.293 7.276 9.245 6.393 6.086
DeepONet 6.461 6.274 8.954 6.607 7.118 6.659 6.526 6.587 6.427
NS
OFormer 10.300 10.466 18.433 16.358 13.065 12.592 10.996 11.750 12.380
Unet 5.854 5.745 6.461 6.110 6.233 6.011 5.902 5.813 6.285
(b)Out-of-DistributionAutoregressivePretrainingResults.
PDE Model None Transfer Binary TimeSort Jigsaw CoefficientDerivative Masked PICL
FNO 25.418 22.835 26.810 41.450 26.346 26.400 25.968 23.83 25.623
DeepONet 3.062 3.914 3.981 4.208 15.344 5.101 4.291 29.002 3.166
Heat
OFormer 35.888 17.203 33.862 34.864 35.389 33.472 32.803 34.528 32.857
Unet 16.998 3.965 20.390 17.475 21.936 20.397 13.866 9.126 12.961
FNO 24.733 23.405 24.166 24.970 24.579 24.426 24.922 24.710 24.730
DeepONet 26.480 26.179 27.219 26.755 26.832 26.864 26.214 26.288 26.406
Adv
OFormer 25.210 25.344 29.968 28.877 25.325 25.658 25.174 24.269 25.014
Unet 24.627 24.976 25.053 25.062 24.733 24.688 24.630 24.749 24.558
FNO 1.443 3.830 2.688 4.630 2.778 2.646 1.498 3.376 1.424
DeepONet 1.357 1.408 1.677 1.552 2.150 1.642 1.133 1.629 1.247
Burgers
OFormer 3.181 1.706 21.411 8.346 3.942 5.360 2.141 2.198 3.032
Unet 1.594 1.491 1.930 1.706 1.661 1.517 1.514 1.498 1.804
17B Comparison of Data Augmentations
B.1 Fixed Future Experiments
Table 7: Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetuned on 500
samplesforeachPDE.Eachbaseline(e.g, None, PICL,Transfer)isfollowedbyitsvariantswithdifferentaugmen-
tations (e.g, Noise, P-Shift, T-Scale). Normalized L2 errors (×10−1) are calculated on 256 validation samples and
averagedoverfiveseeds. Thelowesterrorsaregivenin darkgrey,andsecondlowesterrorsaregivenin lightgrey.
(a)FixedFuturePretrainingResults.
PDE Model None Noise Shift Scale PICL P-Noise P-Shift P-Scale Transfer T-Noise T-Shift T-Scale
FNO 0.246 0.183 0.182 0.179 0.182 0.389 0.332 0.169 0.418 0.407 0.445 0.449
DeepONet 0.670 0.638 0.637 0.661 0.493 0.455 0.457 0.562 0.449 0.407 0.401 0.434
Heat
OFormer 0.763 0.482 0.535 0.650 0.901 0.425 0.467 0.918 0.510 0.340 0.338 0.497
Unet 0.147 0.163 0.162 0.092 0.145 0.176 0.099 0.163 0.130 0.137 0.133 0.081
FNO 3.539 3.509 3.549 3.339 3.631 3.722 3.704 3.464 2.017 1.703 1.679 1.695
DeepONet 9.906 9.807 9.792 9.794 9.713 9.594 9.599 9.737 9.606 9.565 9.557 9.560
Adv
OFormer 9.643 9.508 9.566 8.661 9.690 9.689 9.591 9.559 9.296 8.818 8.716 6.948
Unet 3.979 3.893 3.906 3.457 3.802 3.320 3.142 3.473 2.401 2.211 2.192 1.930
FNO 0.698 0.583 0.584 0.721 0.640 0.643 0.627 0.761 0.645 0.643 0.636 0.831
DeepONet 4.092 3.840 3.841 3.791 4.101 3.929 3.937 4.018 3.956 3.774 3.770 3.854
Burgers
OFormer 1.919 1.612 1.617 1.811 2.408 1.718 1.778 2.158 1.752 1.496 1.506 1.556
Unet 1.026 0.950 0.988 0.844 0.989 0.722 0.633 0.727 0.952 0.877 0.883 0.812
FNO 2.112 2.122 2.124 2.301 2.232 2.574 2.534 2.335 2.428 2.581 2.602 2.747
DeepONet 5.560 5.543 5.544 5.552 5.514 5.316 5.320 5.517 5.492 5.313 5.313 5.333
NS
OFormer 3.744 3.415 3.399 3.569 4.056 3.696 3.694 3.922 3.962 3.629 3.609 3.605
Unet 2.279 2.247 2.238 2.309 2.262 2.290 2.131 2.318 2.678 2.603 2.573 2.520
(b)Out-of-DistributionFixedFuturePretrainingResults.
PDE Model None Noise Shift Scale PICL P-Noise P-Shift P-Scale Transfer T-Noise T-Shift T-Scale
FNO 7.721 7.818 7.805 6.409 8.282 8.670 8.807 7.528 2.431 2.842 2.726 2.806
DeepONet 1.019 1.056 1.073 1.027 2.089 2.023 2.091 1.228 1.150 1.368 1.384 1.250
Heat
OFormer 11.14 11.58 11.61 11.78 11.32 11.34 11.42 11.51 11.46 11.00 11.18 9.956
Unet 5.504 5.128 5.203 3.299 5.373 5.070 3.894 3.261 2.491 2.139 2.040 1.783
FNO 1.457 1.374 1.372 1.305 1.407 1.496 1.487 1.317 1.583 1.601 1.604 1.537
DeepONet 9.599 9.581 9.585 9.584 9.563 9.513 9.511 9.497 9.523 9.511 9.493 9.492
Adv
OFormer 8.750 8.214 8.251 7.179 8.775 8.833 8.682 8.472 8.997 9.082 8.968 7.807
Unet 1.955 1.967 2.014 1.623 1.802 1.597 1.582 1.281 2.117 2.137 2.165 1.757
FNO 0.214 0.173 0.170 0.148 0.190 0.261 0.265 0.115 0.400 0.425 0.455 0.592
DeepONet 0.188 0.145 0.146 0.182 0.164 0.126 0.131 0.192 0.126 0.122 0.122 0.122
Burgers
OFormer 0.526 0.288 0.326 0.536 0.585 0.289 0.343 0.780 0.362 0.201 0.201 0.368
Unet 0.341 0.322 0.323 0.313 0.340 0.281 0.216 0.303 0.284 0.229 0.223 0.252
18B.2 Auto-regressive Results
Table 8: Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetuned on 500
samplesforeachPDE.Eachbaseline(e.g, None, PICL,Transfer)isfollowedbyitsvariantswithdifferentaugmen-
tations (e.g, Noise, P-Shift, T-Scale). Normalized L2 errors (×10−1) are calculated on 256 validation samples and
averagedoverfiveseeds. Thelowesterrorsaregivenin darkgrey,andsecondlowesterrorsaregivenin lightgrey.
(a)AutoregressivePretrainingResults.
PDE Model None Noise Shift Scale PICL P-Noise P-Shift P-Scale Transfer T-Noise T-Shift T-Scale
FNO 2.731 2.345 2.343 2.405 2.584 2.908 2.830 2.428 4.338 5.398 5.174 5.215
DeepONet 2.184 2.410 1.993 2.158 2.289 2.120 2.166 2.371 2.347 2.297 2.118 2.378
Heat
OFormer 4.540 3.778 3.854 4.074 4.418 3.658 3.433 4.674 3.190 2.935 3.010 3.218
Unet 3.301 2.695 2.776 2.387 3.019 2.339 2.349 2.494 2.301 2.340 2.276 2.494
FNO 30.89 30.88 31.02 30.67 30.68 29.94 30.22 30.11 28.59 28.33 28.65 28.33
DeepONet 27.98 28.66 28.04 28.01 28.02 28.13 27.840 28.20 28.14 28.04 27.55 27.81
Adv
OFormer 30.05 31.04 30.29 30.31 28.72 28.89 28.84 29.83 30.18 30.23 30.61 30.11
Unet 29.94 30.56 30.12 30.55 30.36 30.37 29.72 30.64 30.45 30.17 30.64 30.75
FNO 5.103 5.127 5.076 5.302 5.031 5.098 5.142 5.311 5.874 6.428 5.988 7.448
DeepONet 4.884 5.080 4.740 4.577 5.167 4.675 4.666 4.671 4.757 4.484 4.239 4.653
Burgers
OFormer 7.754 7.157 7.058 7.519 8.293 7.267 7.044 8.183 6.867 6.491 6.538 6.866
Unet 5.503 5.334 5.335 5.202 5.656 5.206 5.229 4.980 4.957 4.976 4.935 4.940
FNO 5.884 6.129 6.092 6.032 6.086 6.068 6.078 5.973 6.724 6.874 6.931 7.553
DeepONet 6.461 6.397 6.404 6.400 6.427 6.393 6.376 6.434 6.310 6.241 6.218 6.276
NS
OFormer 10.30 8.96 9.011 9.012 12.38 10.60 10.25 11.16 10.78 8.937 9.037 9.183
Unet 5.854 5.653 5.799 5.851 6.285 5.880 5.883 5.676 5.756 5.686 5.705 5.697
(b)Out-of-DistributionAutoregressivePretrainingResults.
PDE Model None Noise Shift Scale PICL P-Noise P-Shift P-Scale Transfer T-Noise T-Shift T-Scale
FNO 25.42 25.48 25.50 23.92 25.62 26.01 25.84 23.69 23.89 25.775 25.82 25.73
DeepONet 3.267 3.182 3.129 3.197 3.166 3.400 3.232 3.119 3.704 4.027 3.922 3.445
Heat
OFormer 35.67 36.71 36.93 35.54 32.86 32.47 32.53 35.80 25.34 26.61 25.97 23.23
Unet 16.95 15.37 15.66 11.66 12.96 9.51 10.43 8.279 4.649 4.563 4.687 4.172
FNO 24.73 25.06 25.05 24.92 24.73 25.22 25.16 24.96 23.69 23.39 23.15 23.316
DeepONet 26.48 26.03 25.81 25.64 26.41 25.96 25.61 25.64 26.23 25.74 25.98 25.57
Adv
OFormer 25.24 25.27 25.36 25.08 25.01 24.69 24.69 25.35 25.26 25.36 25.23 24.99
Unet 24.48 24.96 24.70 24.90 24.56 24.93 24.58 24.81 25.14 24.46 24.88 25.09
FNO 1.442 1.414 1.443 1.469 1.424 2.121 1.787 1.597 4.185 4.580 4.505 5.062
DeepONet 1.212 1.335 1.167 1.229 1.247 1.273 1.261 1.291 1.411 1.278 1.467 1.460
Burgers
OFormer 3.135 2.624 2.688 2.920 3.032 2.437 2.216 3.404 2.044 1.941 1.908 2.161
Unet 1.560 1.471 1.543 1.632 1.804 1.529 1.637 1.463 1.377 1.348 1.477 1.447
19C Comparison of Downstream Dataset Size
(a)FNOerrorsacrossHeat,Advection,andBurgersdatasets.
(b)DeepONeterrorsacrossHeat,Advection,andBurgersdatasets.
(c)OFormererrorsacrossHeat,Advection,andBurgersdatasets.
(d)UneterrorsacrossHeat,Advection,andBurgersdatasets.
Figure 3: Fixed Future Scaling Behavior: For each model, a specific PDE/distribution is displayed. Within each
graph, the performance of various pretraining strategies at different sample sizes is displayed. Validation errors are
averagedover5seeds,anderrorbarsdenote1standarddeviation. Derivativeerrorsareomittedasoutliers.
20(a)FNOerrorsacrossHeat,Advection,andBurgersdatasets.
(b)DeepONeterrorsacrossHeat,Advection,andBurgersdatasets.
(c)OFormererrorsacrossHeat,Advection,andBurgersdatasets.
(d)UneterrorsacrossHeat,Advection,andBurgersdatasets.
Figure4: Auto-regressiveScalingBehavior: Foreachmodel,aspecificPDE/distributionisdisplayed. Withineach
graph, the performance of various pretraining strategies at different sample sizes is displayed. Validation errors are
averagedover5seeds,anderrorbarsdenote1standarddeviation.
21D Implementation Details
D.1 Dataset Details
Wegeneratedataaccordingtotheequationsoutlinedin4.1. Weprovideadditionaldetailshere:
Pretraining Duringpretraining,9216totalsamplesaregenerated,with3072samplesofthe2DHeat,Advection,and
Burgersequationsrespectively. Thesamplesaregeneratedwitharesolutionof(n,n ,n )=(32,64,64)or
t x y
(n,n ,n )=(32,32,32)onthedomain(x,y)=[−1,1]2fromt=0tot=2;thediscretizationdependsonthe
t x y
downstreamresolutionofthedata. Wesampleequationcoefficientsfromadefinedpretrainingdistribution.
Heat,Advection,andBurgersequationsamplesaregeneratedwithafinite-differencesscheme;afirst-order
centraldifferenceisusedtodiscretizethediffusiveterm,afirst-orderupwindingschemeisusedtodiscretize
thenonlinearconvectionterm,andtimeisdiscretizedwithaforwardEulerscheme.Inaddition,theadvection
equationissolvedwithitsanalyticalsolution.
Training/Finetuning Duringtraining/fine-tuning,wegenerateequationsusingaproceduresimilartopretrainingand
sample coefficients either in the pretraining distribution or from a disjoint distribution to test generaliza-
tion to unseen coefficients. For fine-tuning on the Navier-Stokes equations, we use a higher resolution of
(n,n ,n )=(32,64,64), otherwise experiments are run with a resolution of (n,n ,n )=(32,32,32). We
t x y t x y
generate1024samplesfortheHeat,Advection,Burgers,andNavier-Stokesequationstotrainwith. Anad-
ditional 1024 out-of-distribution samples for the Heat, Advection, and Burgers equations is also generated.
Additionally, the Burgers equation, initial conditions are unchanged to evaluate fine-tuning to a reference
problemundergoingdifferentdynamics,suchasindesignoptimizationproblems(Chengetal.,2024).
Validation Validationsamplesaregeneratedsimilarlytofine-tuningsamples,alsowithequationcoefficientssampled
fromeitherthepretrainingordisjointdistribution.Wegenerate256samplesfortheHeat,Advection,Burgers,
andNavier-Stokesequations.
D.2 Model Details
We implement modern FNO and Unet architectures according to Gupta & Brandstetter (2022). Furthermore, we
implementDeepONetarchitecturesaccordingtoDeepXDE(Luetal.,2021b),andusetheoriginalimplementationfor
OFormer(Lietal.,2023a). ThehyperparametersusedforthemodelsaredescribedinTable9.
Table9: Hyperparametersforarchitecturesused.
(a)FNO (b)DeepONet (c)OFormer (d)Unet
Parameter Value Parameter Value Parameter Value Parameter Value
Modes 4 BranchSize 256 Hiddendim 32 Hiddenchannels 16
Width 48 TrunkSize 256 Heads 2 #Blocks 8
#Layers 4 BranchLayers 3 Encoderdepth 2 DimScaling (1,2,4)
#Params 300k TrunkLayers 3 Decoderdepth 1 #Params 1M
Activation SiLU Latentchannels 32
#Params 250k #Params 70k
D.3 Pretraining Details
During pretraining, different strategies require different implementations and hyperparameters. A consideration is
that many models need a linear head during pretraining to project model outputs to the classification or regression
dimension. Sincemodelswillbeusedforphysicsprediction, theiroutputswillbeintheshapeofthesolutionfield,
ratherthancrossentropyprobabilitiesorregressedvalues. WeusealightweightCNNprojectortodownsampleand
flattenmodeloutputstothedesireddimension. Modelsaregenerallytrainedfor200epochswithabatchsizeof32
usingAdam,weightdecay,andaOneCycleschedulerforfiveseeds.
22(a) Embeddings after Binary pretrain- (b) Embeddings after TimeSort pre- (c) Embeddings after Jigsaw pretrain-
ing.Labelsaredefinedas0forshuffled training. Labels are scaled between 0 ing. Labelsarescaledbetween0and1
or1fororiginalsamples. and1for24classes. for1000classes.
Figure5: t-SNEEmbeddingsofCVPretrainedModels: Wedisplaylatentembeddingsafterpretrainingmodelson
Binary,TimeSort.orJigsawobjectives.Weseethatmodelslearntosort/classifysortedsampleswellandcanvisualize
therelativedifficultiesoftheproposedpretrainingstrategies.
Binary: Binarypretrainingisimplementedbyshufflingasampleintimeandrandomlychoosingashuffledororig-
inal input with corresponding labels of 0 or 1 to be used for classification. We use a CNN head to project
modeloutputstoasinglelogitforabinarycross-entropyloss. Withinthisframeworkthereareafewdesign
decisions. ThedifficultyofthetaskcanbemodulatedbytheHammingdistancebetweentheshuffledsample
andtheoriginalsample. Forexample,iftheshuffledsampleisnotchangedmuch(e.g. onlytwoframesare
swapped),thedifferencebetweenasortedandshuffledsampleissmallandthusmorechallengingtodistin-
guish.WecanleveragethistograduallydecreasetheHammingdistanceofshuffledsamplestoincrementally
increasethedifficultyofthetaskoverpretraining. Empirically,thisdoesnotmakealargedifferenceduring
trainingsowechoosetoomitthiscurriculumlearningforsimplicity.
An additional consideration is the probability of sampling a shuffled or sorted sample. In theory, there are
manymoreshuffledsamplesthansortedsamples(i.e.morelabelswith0vs.1);therefore,itmaybebeneficial
to sample more shuffled samples and use a weighted binary cross-entropy loss. In practice this does not
significantly affect training, so we uniformly sample sorted or shuffled samples. A final consideration is
thatPDEsolutionsgenerallydonotexhibitlargechangesintime,therefore,wepatchifythetimedimension
whenshufflingtocreatelargerchangesinshuffledpatches. Ingeneral,modelsareabletolearntodistinguish
betweenshuffledandoriginalinputsverywell,andwedisplayt-SNEembeddingsofapretrainedFNOmodel
onavalidationsetofshuffledandunshuffledsamplesinFigure5a.
TimeSort/SpaceSort: Sorting along a single dimension is implemented by patchifying the solution field along the
desireddimensionandshufflingthesepatches. Thisisdonetocreatemoredistinctdifferencesintheshuffled
solution, with the patch size controlling the number of permutations of the shuffled sequence. The permu-
tationnumberaffectsthedifficultyofthesortingtask, withlargepermutationnumbersbeingmoredifficult
since each permutation represents a different class. To mitigate this, we set the patch size to ensure a se-
quence length of 4 when shuffling, resulting in 4!=24 classes or permutations of the solution field. The
CNNprojectionheadismodifiedaccordinglytooutput24logitsforacross-entropyloss. Ingeneral,spatial
sortingdoesnotworkwellnordoestrainingconverge,soweomitthisfromtheresults;aliasingeffectsorpe-
riodicboundaryconditionscanmakesomespatiallyshuffledsamplesextremelysimilaroridenticaltosorted
samples. However, temporalsortingtendstoworkwell, andwedisplayt-SNEembeddingsofapretrained
FNOmodelonavalidationsetoftemporallyshuffledsamplesinFigure5b.
Jigsaw: Jigsaw is implemented similarly to other sorting frameworks, however due to sorting along multiple axes
thenumberofpossibleshuffledsequencesquicklyincreases. Wemitigatethisbyusingspatialandtemporal
patchestoensureasequencelengthof8whenshuffled,resultingin8!=40320possiblepermutations. This
is still a large number of classes for a task, therefore we deterministically choose 1000 samples with the
23largestHammingdistancebetweentheshuffledsequenceandoriginalsequence. Contrarytothebinarycase,
shuffled samples with larger Hamming distances are more challenging due to needing to sort more patches
TheCNNprojectionheadismodifiedaccordinglytooutput1000logitsforacross-entropyloss. Ingeneral,
jigsawsortingtendstobemorechallenging, however, modelscanstilldisplayreasonableperformance; we
displayt-SNEembeddingsofapretrainedFNOmodelonavalidationsetofjigsawshuffledsamplesinFigure
5c.
Coefficient: CoefficientregressionisimplementedbyextractingcoefficientvaluesfromthePDEmetadata.TheCNN
projectionheadisthenmodifiedtooutputthecorrespondingnumberoflogitsforanMSEloss.
Derivative: Wegeneratelabelsforderivativeregressionthroughtakingspatialandtimederivatives{u,u ,u ,u ,u }
t x y xx yy
ofthePDEsolutionfieldusingFinDiff(Baer,2018). Thisintroducesanadditionaldesignconsiderationas
the label has more values than the input. We modify the CNN projection head to upsample model outputs
afterconvolutiontothedesireddimensionandapplyanMSEloss.
Masked: Maskedinputsaregeneratedbysplittinginputsintospatialandtemporalpatches,andselectingarandom
subsetofthesetobemasked. Inourexperiments, wechoosetomask75%ofpatches. Maskedpatchesare
replaced with a learnable mask token, and the full input is passed to the model to reconstruct the original
solutionfield. Sincetheoutputshapeisthesameasthedownstreamtarget,aprojectionheadisnotstrictly
needed, but we still include a CNN projection head and apply an MSE loss. This follows previous work;
modelslearntransferablelatentfeaturesbyabstractingreconstruction-specificbehaviortoadecoder(Chen
etal.,2020;Heetal.,2021).
PICL: PICLusestheGeneralizedContrastiveLossfunction(Leyva-Vallinaetal.,2023)giveninequation7:
ψ 1−ψ
L (u,u )= i,j d (u,u )2+ i,j max(τ−d (u,u ),0)2 (7)
GCL i j physics i j physics i j
2 2
Whenworkingwithmultipledatasetssimultaneously, avectorofoperatorcoefficientsisconstructedasθ.
(cid:113)
Thesimilaritybetweensystemsisgivenbymagnitude-awarecosinesimilarity: ψ (θ,θ )=
|θi·θj|
.
i,j i j max(∥θi∥,∥θj∥)
The distance between samples is calculated in two parts for a given timet: d (u,u )=ut+1−ut, and
system i j i j
d =F(G (u))−G (u ), where G is our parameterized model, and F(·) is our numerical update.
update Θ i Θ j Θ
d is anchored to d to account for mode collapse, giving us the loss function: d (u,u )=
update system physics i j
(cid:13) (cid:13)2
(cid:13)d system(u i,u j)−d update(u i,u j)(cid:13) . τ is a hyperparameter that defines a margin, above which samples are
consideredtobefromdifferentclasses. Forpretraining,weconstructtheoperatorcoefficientvectorasθ =
(cid:2)(cid:13) (cid:13) (cid:3)
(cid:13)c Burgers(cid:13),ν,∥c Advection∥
D.4 Data Augmentation Details
Weimplementthreedataaugmentationstoevaluatetheireffectsonmodelperformance: noise,shift,andscale.
Noise GaussiannoiseisaddedtodatasamplesandtargetsthroughsamplingaGaussianatzeromeanandaprescribed
variance: X =X+σ2N (0,I). Empirically,wesetthevarianceto10−7;whennoiselevelsaretoohigh,
noise
modelperformancecansignificantlydeteriorate.
Shift UsingtheFouriershifttheorem,samplescanbeshiftedinspaceandresampledinthespectraldomain(Brand-
stetter et al., 2022). Shifting PDE solutions in space preserves physics, since the PDEs considered in this
workareinvariantacrossspace. Mathematically,thiscanbeverifiedbyderivingorlookinguptheLiegroups
forthe2DAdvection, Heat, andBurgersequations,forwhichtherearemany, andnotingthatthesolutions
can be shifted along the X or Y axes (Ibragimov, 1993). We uniformly sample the magnitude of the shift
between[−0.5,0.5].
Scale Scaling PDE solutions respects physics for the Heat and Advection equations, but not the Burgers equation.
However, westillchoosetoincludethisaugmentationtoevaluatetheeffectofphysicallyinconsistentaug-
mentations; in practice, scaling PDE solutions still improves model performance. The implementation is
donebymultiplyingPDEsolutionsbyaconstant,whichweuniformlysamplebetween[−0.5,0.5].
24D.5 Fine-tuning Details
During fine-tuning, models trained until convergence for fixed-future or auto-regressive prediction and repeated for
fiveseeds. Infixed-futureprediction, modelsaregiventhesolutionfieldatt =[0,8)andthetargetisatt =32. For
auto-regressive prediction, models are given the solution field att =[0,8) and the target is att =[8,16). After this
prediction, the models use their own output to predict the next step t =[16,24) until the time horizon of t =32.
To stabilize auto-regressive rollout, we implement temporal bundling and the pushforward trick (Brandstetter et al.,
2023). Losses are calculated using a relative L2 norm (Li et al., 2021); validation losses are averaged across batch
sizeandaccumulatedovertimestepsor,inthecaseoffixed-futureprediction,atonlyonetimestep. Forexperiments
withdifferentfine-tuningsamplesizes,samplesarerandomlychosenfrom1024possiblesamplestoreachthedesired
numberof samplesfor eachseed. Weuse anAdam optimizerwithweight decayand aCosineAnnealing scheduler.
AllexperimentsarerunonaNVIDIAGeForceRTX2080TiGPU.
25