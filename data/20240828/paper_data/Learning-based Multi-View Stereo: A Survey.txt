1
Learning-based Multi-View Stereo: A Survey
Fangjinhua Wang*‚Ä†, Qingtian Zhu*, Di Chang*, Quankai Gao, Junlin Han, Tong Zhang,
Richard Hartley, Fellow, IEEE, Marc Pollefeys, Fellow, IEEE
Abstract‚Äî3D reconstruction aims to recover the dense 3D images, MVS aims to reconstruct the dense 3D geometry
structure of a scene. It plays an essential role in various for an observed scene. Based on the scene representations,
applications such as Augmented/Virtual Reality (AR/VR), au-
traditionalMVSmethodscanbedividedintothreecategories:
tonomous driving and robotics. Leveraging multiple views of
volumetric, point cloud, and depth map. Volumetric meth-
a scene captured from different viewpoints, Multi-View Stereo
(MVS)algorithmssynthesizeacomprehensive3Drepresentation, ods [9]‚Äì[12] discretize the continuous 3D space into voxels
enabling precise reconstruction in complex environments. Due and label each as inside or outside of the surface. They are
to its efficiency and effectiveness, MVS has become a pivotal limited to scenes of small scale because of the large memory
method for image-based 3D reconstruction. Recently, with the
consumption. Point cloud methods [13], [14] operate directly
success of deep learning, many learning-based MVS methods
on 3D points and often employ propagation to gradually
have been proposed, achieving impressive performance against
traditionalmethods.Wecategorizetheselearning-basedmethods densify the reconstruction. As the propagation of point clouds
as: depth map-based, voxel-based, NeRF-based, 3D Gaussian is proceeded sequentially, these methods are difficult to be
Splatting-based, and large feed-forward methods. Among these, parallelized and thus take much time in processing [15]. In
we focus significantly on depth map-based methods, which are
addition, the irregularity and large size of point cloud are also
the main family of MVS due to their conciseness, flexibility and
not very suitable for deep learning, especially in large-scale
scalability. In this survey, we provide a comprehensive review
of the literature at the time of this writing. We investigate scenes. In contrast, methods relying on depth maps [16]‚Äì[20]
these learning-based methods, summarize their performances use patch matching with photometric consistency to estimate
on popular benchmarks, and discuss promising future research the depth maps for individual images. Subsequently, these
directions in this area.
depth maps are fused into a dense representation, e.g., point
Index Terms‚ÄîMulti-View Stereo, 3D Reconstruction, Deep cloud or mesh, as a post-processing step. Such design decou-
Learning. plesthereconstructionproblemintoper-viewdepthestimation
3D reconstruction describes the general task of recovering anddepthfusion,whichexplicitlyimprovestheflexibilityand
the 3D structure of a scene. It is widely applied in scalability. Although MVS has been studied extensively for
augmented/virtual reality (AR/VR), autonomous driving, and severaldecades,traditionalMVSmethodsrelyonhand-crafted
robotics [1]. The advancement of 3D acquisition techniques matching metrics and thus encounter challenges in handling
has led to the increased affordability and reliability of depth various conditions, e.g., illumination changes, low-textured
sensors,suchasdepthcamerasandLiDARs.Thesesensorsare areas, and non-Lambertian surfaces [21]‚Äì[24].
extensivelyutilizedforreal-timetasks,enablingroughestima- To overcome these challenges, recent works [15], [25] have
tions of the surrounding environment, such as simultaneous turned to learning-based methods, leveraging the emergence
localization and mapping (SLAM) [2]‚Äì[5] or dense recon- of Convolutional Neural Networks (CNNs) that have shown
struction [6]‚Äì[8]. Nevertheless, depth maps captured by such remarkablesuccessinvarious2Dvisiontasks.Theselearning-
sensors tend to be partial and sparse, resulting in incomplete based MVS techniques have achieved remarkable success and
3Drepresentationswithlimitedgeometricdetails.Inaddition, explicitlyoutperformtraditionalmethods[17],[18]onvarious
these sensors are active and usually consume a lot of power. benchmarks [21]‚Äì[23], [26], [27].
In contrast, camera-based solutions, commonly found in edge In this survey, we categorize existing learning-based MVS
devices like smartphones and AR/VR headsets, offer a more methods based on their characteristics as follows: depth map-
economically viable alternative for 3D reconstruction. based,voxel-based,NeRF-based,3DGaussianSplatting-based
One fundamental technique in image-based 3D reconstruc- and large feed-forward methods. Voxel-based methods esti-
tion is Multi-View Stereo (MVS). Given a set of calibrated matethegeometrywithvolumetricrepresentationandimplicit
function, e.g., Signed Distance Functions (SDF). They are
FangjinhuaWangandMarcPollefeysarewiththeDepartmentofComputer limited to small-scale scenes due to the high memory con-
Science,ETHZurich,Switzerland. sumption. NeRF and 3D Gaussian Splatting-based methods
Qingtian Zhu is with the Graduate School of Information Science and
adapt NeRF [28] and 3D Gaussian Splatting [29], which are
Technology,UniversityofTokyo,Japan.
DiChangandQuankaiGaoarewiththeDepartmentofComputerScience, originally used for novel view synthesis, to extract surface
UniversityofSouthernCalifornia,USA. from the implicit field or point cloud. They typically need to
Junlin Han is with the Department of Engineering Science, University of
optimizethegeometryforeachnewscene,whichneedslotsof
Oxford,UK.
TongZhangiswiththeSchoolofComputerandCommunicationSciences, run-timeandmemoryconsumption.Largefeed-forwardmeth-
EPFL,Switzerland. odstypicallyuselargetransformermodelstodirectlylearnthe
RichardHartleyiswithAustralianNationalUniversity,Australia.
3D representation from given images. They require massive
MarcPollefeysisadditionallywithMicrosoft,Zurich.
*:Equalcontribution.‚Ä†:Projectlead. computation because of the huge network, and are mainly
4202
guA
72
]VC.sc[
1v53251.8042:viXra2
Camera Trajectory
Multi-View Stereo Cost Volume
Online Image Capture TSDF Fusion TSDF Representation Mesh
. .
.
...
Plane Sweep Depth Estimation
Reference View Point Cloud
View Selection
Offline Image Capture Point Cloud Fusion
Fig.1. Anoverallillustrationofbothonlineandofflinedepthmap-basedMVSpipelines.OnlineMVSusuallydealswithsequentialdata,e.g.,video,and
employs TSDF volumes as an intermediate representation for mesh extraction. Given a full set of images, offline MVS holds the global information of the
capturedscene,andusuallyfusesestimateddepthmapsintoapointcloudwithfiltering.
limited to object-level scenes [30]. Depth map-based methods methods on different benchmarks, and discuss the potential
utilize deep learning in depth estimation and then fuse depth future directions for deep learning-based MVS methods.
maps with traditional fusion algorithm. Comparatively, depth
map-basedmethods[15],[25]arethemainfamilyoflearning-
I. PRELIMINARIES
based methods since they inherit the advantages from those
traditionalmethods[17]‚Äì[20]bydecoupling3Dreconstruction Depth map-based MVS, including most traditional and
into depth estimation and depth fusion. Therefore, we focus learning-based methods, typically consists of several compo-
more on the depth map-based methods and discuss them in nents: camera calibration, view selection, multi-view depth
more details in this survey. estimationanddepthfusion.Inthissection,weintroducethese
components to provide readers a clear picture of the MVS
For clarity, we further categorize depth map-based methods
problem. Note that camera calibration and view selection are
into online and offline methods, shown in Fig. 1. Specifically,
componentsforotherlearning-basedmethodsaswell.Sec.I-A
online MVS methods [25], [31] usually perform multi-view
introduces camera calibration with Structure from Motion
depth estimation with a video sequence of low-resolution
(SfM)orSLAM.Sec.I-Bdiscusseshowtoselectneighboring
images, e.g., ScanNet [26]. They typically use small reso-
viewstoaccuratelyreconstructthegeometry.Sec.I-Cexplains
lution inputs and a lightweight network structure to ensure
how to build cost volumes in learning-based MVS methods
quickinferenceconsideringspeedandsimplicity.Theprimary
with plane sweep [33]. Sec. I-D introduces the typical depth
focus of online MVS is to provide instant reconstruction for
fusionstrategiesafterdepthestimation.Sec.I-Elistscommon
applications where time efficiency is critical, e.g., augmented
datasets and benchmarks for MVS and Sec. I-F summarizes
reality and live video processing. In contrast, offline MVS
the common evaluation metrics.
methods [15], [32] prioritize better quality reconstruction at
the cost of computation. These methods excel in multi-view
depth estimation and point cloud fusion using high-resolution A. Camera Calibration
image sets, e.g., DTU [21], Tanks and Temples [22] and
Camera calibration is a process of determining the intrinsic
ETH3D [23]. By operating on higher-resolution inputs and
and extrinsic parameters of a camera to understand its ge-
employing complex network designs, these approaches thor-
ometry and characteristics accurately [55]. It serves as the
oughly analyze the multi-view information in the images. The
foundationalstepinMVS,ensuringthatthesubsequentrecon-
emphasis on achieving high accuracy and capturing intricate
struction process is built on accurate and consistent geometric
details often comes at the cost of requiring considerable
information, ultimately leading to a more reliable and precise
computational resources and time. As a result, offline MVS
3D representation of the scene. Typically, obtaining calibrated
is frequently employed in applications demanding precise and
camera parameters is usually achieved by running off-the-
photorealistic scene representations, including 3D modeling
shelf SfM algorithms [17], [56] or SLAM [57], which jointly
and archaeological reconstruction.
optimizesparsetriangulated3Dpointsandcameraparameters.
In summary, our survey covers the most recent literature ThecameraparametersincludetheextrinsicmatrixT=[R|t]
on learning-based MVS methods until 2023, including four and intrinsic matrix K. Depth map-based MVS methods [15],
main families: depth map-based, voxel-based, NeRF-based, [32], [40] require a bounded depth range [d ,d ] to im-
min max
3D Gaussian Splatting-based and large feed-forward methods. prove the estimation accuracy. For offline methods [15], [32],
We provide a comprehensive review and insights on different thedepthrangecanbeestimatedbyprojectingthesparsepoint
aspects, including the pipelines and algorithmic intricacies. cloudfromSfMtoeachviewpointandcomputetheminimum
Moreover, we summarize the performance of the reviewed andmaximumz values[15].Incontrast,onlinemethods[25],3
TraditionalMethods COLMAP[34],Furu[14],Gipuma[18]...
Datasets&Benchmarks ScanNet[26],DTU[21],TanksandTemples[22]...
Pipeline CameraCalibration,ViewSelection,Multi-ViewDepthEstimation,DepthFusion
OnlineMethods MVDepthNet[25],DeepvideoMVS[31],SimpleRecon[35]...
Direct3DCNN MVSNet[15],CIDER[36]...
Supervised
RNN R-MVSNet[37],D2HC-RMVSNet[38]...
Learning-basedMethods OfflineMethods
Coarse-to-fine CasMVSNet[32],UCSNet[39]...
withDepthEstimation
IterativeUpdate PatchmatchNet[40],IterMVS[41]...
MVS
End-to-end JDACS[42],RC-MVSNet[43]...
Unsupervised
Multi-stage U-MVSNet[44],KD-MVS[45]...
Semi-Supervised SGT-MVSNet[46]
Voxel-based Atlas[47],NeuralRecon[48]...
Learning-basedMethods NeRF-based VolSDF[49],NeuS[50]...
withoutDepthEstimation
3DGaussianSplatting-based DreamGaussian[51],SuGaR[52]...
Largefeed-forward LRM[53],DUSt3R[54]...
FutureResearchDirections Dataset&Benchmarks,ViewSelection,DepthFusion,Features,Run-time&MemoryEfficiency,PriorAssistance
Fig.2. TaxonomyofMulti-ViewStereo.
[31] usually set constant depth ranges, e.g., [0.25m, 20.00m], (180/œÄ)arccos((c )‚àíP)¬∑((c )‚àíP)) represents the base-
i j
since the scene scale is usually fixed as room. line angle for P and c ,c are the camera centers. Œ∑(¬∑) is a
i j
piece-wise Gaussian function [59] to favor a certain baseline
angle Œ∏ :
0
B. View Selection
Ô£± (Œ∏‚àíŒ∏ )2
The selection of views is an important step for reconstruc-
Ô£¥Ô£¥Ô£≤exp(‚àí 2œÉ20 ),Œ∏ ‚â§Œ∏
0
tion. It is important to balance triangulation quality, matching Œ∑(Œ∏)= 1 (2)
(Œ∏‚àíŒ∏ )2
accuracy, and view frustum overlap [31]. Currently, there are Ô£¥Ô£¥Ô£≥exp(‚àí 2œÉ20 ),Œ∏ >Œ∏
0
two main strategies for view selection. 2
First, for most online MVS depth estimation methods [25], whereŒ∏ 0,œÉ 0,œÉ 1arehyper-parameters.Thentheviewselection
[31], [58], a frame is selected as a keyframe when its pose is done by choosing neighboring views with highest scores.
hassufficientdifferencecomparedwiththepreviouskeyframe. AlmostallthefollowingofflineMVSmethods[32],[37],[40]
Then each keyframe adopts several previous keyframes to use the same strategy.
estimate depth. GP-MVS [58] proposes a heuristic pose-
distance measure as: C. Multi-view Depth Estimation with Plane Sweep
(cid:114) To form a structured data format that is more suitable for
2
disc(T ij)= ||t ij||2+ 3tr(I‚àíR ij), (1) convolution operations, most learning-based MVS methods
rely on the plane sweep algorithm [33] to calculate matching
where T ij = [R ij|t ij] is the relative transformation between costs. The plane sweep algorithm discretizes the depth space
view i and j. This strategy is used by many following with a set of fronto-parallel planes along the depth direction.
methods [31], [35]. This practice is deeply inspired by learning-based binocular
Second, for most offline MVS methods [15], [32], [37], stereo methods [60]‚Äì[62], which assess matching costs for
view selection is done with the sparse point cloud ob- a set of disparity hypotheses and subsequently estimate the
tained by Structure-from-Motion [34], [56]. For a refer- disparity.
ence view i, MVSNet [15] computes a score s(i,j) = In a nutshell, the plane sweep algorithm in MVS entails
(cid:80)
Œ∑(Œ∏ (P)) for the neighboring view j, where P is iteratively sweeping planes through the object space, comput-
P ij
a 3D point observed by both view i and j. Œ∏ (P) = inghomographiesbetweenimages,andselectingdepthvalues
ij4
reference view 0 can be computed as:
ùëë
!$% (cid:18) (‚àíR‚ä§t +R‚ä§t )n‚ä§R (cid:19)
H (d)=K R I‚àí 0 0 i i 0 R‚ä§K‚àí1,
i i i d 0 0
(3)
where K , K denote camera intrinsics, [R |t ], and [R |t ]
0 i 0 0 i i
denote camera extrinsics, n denote the principle axis of the
reference view. Equivalently, we can also project a reference
pixel into source views with depth hypothesis [36], [40], [41].
ùëÄ
We compute p (d) in the i-th source view for pixel p in the
ùëë i
!"#
reference and depth hypothesis d as follows:
ùëÄ‚Ä≤
cam_1 cam_2
p (d)=K ¬∑(cid:0) R R‚ä§¬∑(K‚àí1¬∑p¬∑d)‚àíR R‚ä§¬∑t +t (cid:1) . (4)
i i i 0 0 i 0 0 i
cam_x
The warped source feature maps are then obtained via differ-
entiable interpolation.
Regarding selecting D depth hypotheses from the triangu-
lated depth range [d ,d ], there are two main schemes,
min max
namely the forward depth sampling and the inverse depth
sampling.Thenaiveforwardsamplingdividesthedepthrange
Fig.3. Illustrationofplanesweepalgorithm[33].Toestimatethedepthmap intoD‚àí1depthintervalswithidenticallengths.Givenadepth
forreferenceimage(cam x),neighboringsourceimages(cam 1,cam 2)are
projected with homography to fronto-parallel planes of the reference view index k, we have
frustum. k
d =d + (d ‚àíd ),k =0,...,D‚àí1, (5)
k min D‚àí1 max min
based on consensus among different views, ultimately facili- where the depth hypotheses distribute uniformly between the
tating accurate 3D reconstruction. The plane sweep algorithm two ends. While for the inverse sampling scheme [36], we
discretizes the depth space using a series of parallel planes sample uniformly in the multiplicative inverse of d, such that
alongthedepthdirection.Itoperatesbysweepingaconceptual 1 1 k 1 1
plane through the object space and evaluating the spatial = + ( ‚àí ),k =0,...,D‚àí1. (6)
d d D‚àí1 d d
k max min max
distribution of geometric surfaces. It is worth noting that,
In this way, the distribution of sampled depth values becomes
for learning-based MVS, plane sweep is conducted purely on
sparser with d approaching to d . When reconstructing
GPU, and instead of warping hand-crafted features or edges, max
unbounded outdoor scenes, where the depth range is rather
the dense pixels are efficiently warped.
large, the inverse sampling will be a reasonable choice since
We take a toy example of three cameras viewing the
it samples more densely at the foreground. In addition, as
same object as illustrated in Fig. 3. Plane sweep is then
revealed in [63], the inverse sampling leads to a uniform
performed on the frustum of cam x (termed as the reference
sampling on the projected epipolar lines of source images.
camera) by creating a series of fronto-parallel hypothesized
So far, for each depth hypothesis made for cam x, we have
planes within a given range (typically [d ,d ]), with each
min max thewarpedsourcefeaturemapsaswellasthescaledreference
plane corresponding to a depth value w.r.t. cam x. Let‚Äôs then
feature map. The photometric similarity (or matching cost) is
examine two 3D points, M and M‚Ä≤, as examples of occupied
measured in a one-to-many manner between reference and
andunoccupiedpositionsonhypothesizedplanes,respectively.
warped source features for the following depth estimation,
For M, all of the three cameras capture the identical point
which will be discussed in Sec. II-B. By performing plane
lying on the geometry surface with photometric consistency.
sweep, we obtain a cost volume with a regular spatial shape,
In contrast, the photometric consistency of the observations
making it easy for CNNs to process in parallel.
of M‚Ä≤ are poor, indicating that M‚Ä≤ is an invalid hypothesis.
To evaluate the similarity of the observations towards M and
M‚Ä≤, the images of cam 1 and cam 2 (termed as the source D. Depth Fusion
cameras) are warped to each plane by homography. These For depth map-based MVS, after estimating all the depth
warped images are then compared against the scaled images maps, we need to fuse them into a dense 3D representation,
ofcam x.Depthhypotheseswithhighsimilaritymeasuresare e.g., point cloud or mesh. Online MVS methods [31], [35]
considered reliable. usually adopt TSDF (Truncated Signed Distance Function)
In practice, we divide the depth range, which is either fusion [64], [65] to fuse the depth maps into a TSDF volume
manually set [25] or estimated by SfM [15], into discrete and then use Marching Cube [66] to extract the mesh. How-
samplesandassignhypothesesatthesevalues.Tomapcoordi- ever,thereusuallyexistoutliersinthedepthmaps,whichmay
nates at depth hypothesis d, homography transformation [15] reduce the reconstruction accuracy. To overcome this problem
p (d)‚àºH (d)¬∑p,isapplied,wherep (d)isthecorresponding and improve the accuracy, offline MVS methods [15], [32],
i i i
pixelinthei-thsourceviewforpixelpofthereferenceview. [40] typically filter the depth maps before fusing into a point
The homography H (d) between the i-th source view and the cloud, which is motivated by Galliani et al. [18]. There are
i5
TABLEI
ANOVERVIEWOFCOMMONLYUSEDDATASETSANDBENCHMARKSFORLEARNING-BASEDMULTI-VIEWSTEREO.
ProvidedGroundTruth1 Online Evaluation
Dataset Synthetic
Camerapose DepthMap PointCloud Mesh Benchmark Target
ScanNet[26] ‚úì ‚úì ‚úì DepthMap/Mesh
7-Scenes[27] ‚úì ‚úì DepthMap
DTU[21] ‚úì ‚úì PointCloud
TanksandTemples[22] ‚úì ‚úì PointCloud
ETH3D[23] ‚úì ‚úì ‚úì PointCloud
BlendedMVS[24] ‚úì ‚úì ‚úì DepthMap
1Fordatasetswithonlinebenchmark,thepointcloudgroundtruthoftestsetisnotreleased.
two main filtering steps: photometric consistency filtering and 7-Scenes: 7-Scenes dataset [27] is a RGB-D dataset captured
geometric consistency filtering [15]. For photometric consis- inindoorsceneswithahandheldKinectRGB-Dcamera.Since
tency filtering, a per-pixel confidence, discussed in Sec. II-G, it is relatively small, 7-Scenes is usually used to test the gen-
is estimated to measure the confidence of depth estimation, eralizationperformanceofthemodelstrainedonScanNet[26]
i.e., probability that the ground truth depth is within a small without finetuning.
rangeneartheestimation.Thenathresholdcanbesettofilter DTU: DTU dataset [21] is an object-centric MVS dataset
depth values with low confidence. For geometric consistency collected under well-controlled laboratory conditions with
filtering, the consistency of depth estimations are measured known accurate camera trajectory. It contains 128 scans with
among multiple views. For a pixel p in the reference view 0, 49 or 64 views under 7 different lighting conditions. Since
we project it to pixel p‚Ä≤ in its i-th neighboring view through DTU dataset officially provides scanned ground truth point
its depth prediction d (p). After looking up the depth for clouds instead of depth maps, it is required to generate mesh
0
p‚Ä≤, d (p‚Ä≤), we re-project p‚Ä≤ back to the reference view at models with surface reconstruction, e.g., screened Poisson
i
pixel p‚Ä≤‚Ä≤ and look up its depth D (p‚Ä≤‚Ä≤). We consider pixel surface reconstruction algorithm [67], and then render depth
0
p and its depth as consistent to the i-th neighboring view, if maps [15] for training.
the distances, in image space and depth, between the original Tanks and Temples: Tanks and Temples [22] is a large-scale
estimate and its re-projection satisfy: benchmarkcapturedinmorecomplexrealindoorandoutdoor
scenarios. It is divided into intermediate and advanced sets.
Œæ =‚à•p‚àíp‚Ä≤‚Ä≤‚à• ‚â§œÑ , (7)
d 2 1 Different scenes have different scales, surface reflection and
‚à•d (p‚Ä≤‚Ä≤)‚àíd (p)‚à• exposureconditions.Itisusuallyusedtotestthegeneralization
Œæ = 0 0 ‚â§œÑ , (8)
p d (p) 2 performance. Note that Tanks and Temples does not provide
0
ground truth camera parameters, which are usually estimated
where œÑ and œÑ are thresholds. The pixels are considered
1 2 with Structure-from-Motion methods such as COLMAP [17]
to be reliable estimations if they are consistent in at least
or OpenMVG [56].
N neighboring views. Recently, instead of using predefined
ETH3D: ETH3D benchmark [23] contains 25 large-scale
thresholds for œÑ and œÑ , dynamic consistency checking [38]
1 2 scenes, including indoor and outdoor, with high-resolution
isproposedtodynamicallyaggregategeometricmatchingerror
RGB images. The scenes typically contain many low-textured
amongallviewsandimprovetherobustnessofreconstruction.
regions and non-Lambertian surfaces, e.g., white walls and
Specifically, the dynamic multi-view geometric consistency
reflective floor. In addition, the images are usually sparse and
C (p) is computed as:
geo have strong viewpoint variations and occlusions. Therefore,
C (p)=(cid:88) exp(‚àí(Œæ +ŒªŒæ )), (9) ETH3D is considered as a very challenging benchmark. Sim-
geo p d
ilar to Tanks and Temples, ETH3D is usually used to test the
i
generalization performance.
where Œª is a weight to balance the reprojection error in
BlendedMVS: BlendedMVS dataset [24] is a recently in-
two metrics. Then the outliers with C (p) smaller than a
geo troduced large-scale synthetic dataset for MVS training that
threshold are filtered.
contains a variety of scenes, such as cities, sculptures and
shoes.Thedatasetconsistsofover17khigh-resolutionimages
E. Datasets and Benchmarks rendered with reconstructed models and is split into 106
Tab. I is a brief summary of commonly used public MVS training scenes and 7 validation scenes. Since images are
datasets and benchmarks for training and evaluation. rendered through virtual cameras, the camera parameters are
ScanNet: ScanNet [26] is a large RGB-D dataset that con- accurate enough for training.
tains 1613 indoor scenes with ground-truth camera poses,
F. Evaluation Metrics
depthmaps,surfacereconstructionandsemanticsegmentation
labels. Online MVS methods [25], [31], [35] mainly use As mentioned in Tab. I, based on the ground truth, e.g.,
ScanNet for training and testing. depth maps or point clouds, evaluation metrics can be catego-6
rized into 2D metrics and 3D metrics. estimated points of high certainty. On the other hand, if only
1) 2DMetrics: 2Ddepth-basedmetricsarecommonlyused recall/completenessisreporteditwouldfavorMVSalgorithms
by online MVS methods [25], [31] to evaluate the depth that include everything, regardless of accuracy. Therefore,
maps[68],[69].Welistthecommonlyusedmetricsasfollows: F-score, an integrated metric, is introduced [22], [23]. F-
meanabsolutedeptherror(Abs),meanabsoluterelativedepth score is the harmonic mean of precision and recall. It is
error (Abs Rel), mean absolute inverse depth error (Abs Inv), sensitive to extremely small values and tends to get more
and inlier ratio with threshold 1.25 (Œ¥ <1.25): affectedbysmallervalues,whichmeansthatF-scoredoesnot
encourageimbalancedresults.However,inmostcases,F-score
Abs= 1 (cid:88) |d(p)‚àídÀÜ(p)|, still suffers from unfairness due to the limitations of ground
N
p truth,e.g.,sparseandincompletepointcloudsmaypenalizefor
1 (cid:88)|d(p)‚àídÀÜ(p)| fillingintheareasthatarenotpresentinthegroundtruth[47].
Abs Rel= ,
N d(p)
p (10) II. SUPERVISEDMETHODWITHDEPTHESTIMATION
1 (cid:88) 1 1
Abs Inv= | ‚àí |, This section mainly introduces supervised learning-based
N d(p) dÀÜ(p)
p MVS methods with depth estimation. A typical depth map-
(cid:20) (cid:21)
Inlier Ratio= 1 (cid:88) 1 d(p) <dÀÜ(p)<1.25d(p) , based MVS pipeline mainly consists of the feature extraction
N 1.25 (Sec. II-A), cost volume construction (Sec. II-B), cost volume
p
regularization (Sec. II-C) and depth estimation (Sec. II-E).
where d(p) denotes the ground truth depth, dÀÜ(p) denotes the The pipelines of MVDepthNet [25] and MVSNet [15] are
estimation, N denotes the number of pixels with valid depth illustrated in Fig. 4 and Fig. 5, which are the representative
measurements and 1[¬∑] denotes the indicator function. methods of online and offline MVS methods respectively.
2) 3DMetrics: 3Dpointcloudevaluationiswidelyusedby
offline MVS methods [15], [37]. Note that the reconstructed A. Feature Extraction
point clouds should be aligned to ground truth point clouds Considering efficiency, most methods use simple CNN
beforeevaluation,e.g.,byIterativeClosestPoint(ICP),iftheir structures to extract deep features from images, e.g.,
extrinsics are differently calibrated. ResNet [70], U-Net [71] and FPN [72]. For online MVS
Precision/Accuracy: Precision/Accuracy measures the per- methods, feature extraction networks are usually used in
centage of predicted points that can be matched to the ground line with the real-time operation goal. DeepVideoMVS [31]
truthpointcloud.ConsideringapointP pinthepredictedpoint combines MNasNet [73], which is lightweight and has low
cloud,itisconsideredtohaveagoodmatchinthegroundtruth latency, with FPN. SimpleRecon [35] proposes utilizing the
point cloud {P g} if first two blocks from ResNet18 [70] and an EfficientNet-
v2 [74] encoder, which maintains efficiency and yields a
‚à•P ‚àíargmin‚à•P‚àíP ‚à• ‚à• ‚â§Œª, (11)
p p 2 2
sizeableimprovementindepthmapaccuracy.ForofflineMVS
P‚àà{Pg}
methods, MVSNet [15] uses a stacked eight-layer 2D CNN
where Œª is a scene-dependent threshold assigned by datasets.
to extract deep features for all the images. Coarse-to-fine
Usually Œª is set to a large value for large-scale scenes.
methods further extract multi-scale features for estimation
Note that in some datasets, instead of being measured by
on multiple scales with FPN [32], [39], [40] or multi-scale
percentage[22],[23],precision/accuracyismeasuredbymean
RGB images [75], [76]. Recently, many following works
or median absolute distance [21].
have been paying more attention to feature extraction to
Recall/Completeness:Recall/Completenessmeasurestheper-
improve the representation power of deep features. [77]‚Äì
centage of ground truth points that can be matched to the
[79] introduce deformable convolutions to adaptively learn
predicted point cloud. For a point P in the ground truth
g receptive fields for areas with varying richness of texture.
point cloud, it is considered a good match in the predicted
CDS-MVSNet [80] uses a dynamic scale network, CDSFNet,
point cloud {P } if
p to extract the discriminative features by analyzing the normal
‚à•P ‚àíargmin‚à•P‚àíP ‚à• ‚à• ‚â§Œª. (12) curvature of the image surface. GeoMVSNet [76] proposes a
g g 2 2
P‚àà{Pp} geometry fusion network that fuses the image features with
coarse depth maps. With FPN as the main feature extractor,
Similar to precision/accuracy, recall/completeness is some-
WT-MVSNet [81] and MVSFormer [82], further introduces
times measured by the percentage of points [22], [23]
VisionTransformers[83]‚Äì[85]forimagefeatureenhancement.
and sometimes measured by mean or median absolute dis-
ET-MVSNet [86] integrates the Epipolar Transformer into the
tance [21], whose definition is similar to Chamfer distance.
feature extraction, which performs non-local feature aggrega-
F-Score: The two aforementioned metrics measure the ac-
tion on the epipolar lines.
curacy and completeness of predicted point clouds. How-
ever, each of these metrics alone cannot present the overall
B. Cost Volume Construction
performance since different MVS methods adopt different
assumptions. A stronger assumption usually leads to higher For both online and offline MVS methods, the cost volume
accuracy but lower completeness. If only precision/accuracy is constructed with the plane sweep algorithm as discussed in
is reported, it would favor MVS algorithms that only include Sec. I-C.7
1) Online MVS: To reduce computation and improve the C. Cost Volume Regularization
efficiency for online applications, online MVS methods usu- Usually, the raw cost volume constructed from image fea-
ally construct 3D cost volumes C ‚àà RH√óW√óD, which store tures may be noisy and should be incorporated with smooth-
a single value as matching cost for each pixel p and depth ness constraint for depth estimation [15]. Therefore, cost
sample d i. MVDepthNet [25] and GP-MVS [58] compute the volume regularization is an important step to refine the raw
per-pixel intensity difference between the reference and each cost volume and aggregate matching information from a large
sourceviewasmatchingcost.Ifthereismorethanonesource receptive field.
view, the cost volumes are averaged. Instead of intensity dif- 1) Online MVS: A 2D encoder-decoder architecture is
ference,NeuralRGB-D[87]computesdeepfeaturedifference. commonly used to aggregate information. MVDepthNet [25]
DeepVideoMVS[31]andMaGNet[88]calculatethecostfrom and GP-MVS [58] concatenate the reference image and cost
pixel-wise correlation between the reference feature and the volumeandsendthemtoanencoder-decoderarchitecturewith
warped source feature. In addition to the dot product between skip connections. In addition, GP-MVS [58] uses Gaussian
reference image features and warped source image features, process to fuse information from previous views. Neural-
SimpleRecon [35] further adds metadata, e.g., ray direction RGBD [87] accumulates depth probability volumes over time
and relative pose, into the 4D cost volume. Then an MLP is withaBayesianfilteringframeworktoeffectivelyreducedepth
used to reduce the dimension of 4D cost volume to a 3D cost uncertainty and improve robustness and temporal stability.
volume. DeepVideoMVS [31] applies 2D U-Net [71] on the cost
2) OfflineMVS: OfflineMVSmethodsmainlyfocusonre- volumeandaddsskipconnectionsbetweentheimageencoder
constructinghigh-qualitydensegeometrywithhigh-resolution and cost volume encoder at all resolutions. It further uses
images. To encode more matching information and improve ConvLSTM [93] to propagate past information with a small
the quality, offline methods [15], [32] usually construct 4D overheadofcomputationtimeandmemoryconsumption.Long
cost volumes C ‚àà RH√óW√óD√óC, where each pixel p and et al. [94] use MatchNet to regularize the cost volume and
depth sample d is associated with a matching cost of di- ContextNettolearn2Dcontextinformationfromthereference
i
mension C. Since there may exist an arbitrary number of view. By concatenating the cost volume and context feature, a
source views and serious occlusion [23], robustly aggregating hybrid volume is fed into the Epipolar Spatio-Temporal trans-
matchinginformationfromallthesourceviewsisanimportant former to aggregate temporal information. SimpleRecon [35]
step. MVSNet [15] proposes a variance-based cost metric as fusesmulti-scaleimagefeatures,extractedfromthepretrained
follows: EfficientNetv2 [74], into the cost volume encoder to improve
the performance.
2) Offline MVS: Among most learning-based offline MVS
C=Var(V ,¬∑¬∑¬∑ ,V )=
(cid:80)N i=‚àí 01(cid:0) V i‚àíV(cid:1)2
, (13)
methods that use 4D cost volumes, there are three main
0 N‚àí1 N categories to perform cost volume regularization: direct 3D
CNN, coarse-to-fine and RNN. Fig. 6 illustrates these three
main schemes.
where {V }N‚àí1 are (warped) feature volumes, V is the aver-
i i=0 Direct 3D CNN: Similar to stereo estimation [60]‚Äì[62],
agefeaturevolume.Tofurtherreducedimension,CIDER[36]
3D CNN is widely used for cost volume regularization in
adopts group-wise correlation [89] to compute a lightweight
MVS [15], [36], [95]‚Äì[97]. As the blueprint of learning-
cost volume between reference and each warped source view.
based offline MVS methods, MVSNet [15] adopts a 3D U-
Then N ‚àí 1 cost volumes are averaged for regularization.
Net [71] to regularize the cost volume, which aggregates
Without considering occlusions, these methods consider all
contextinformationfromalargereceptivefieldwithrelatively
source views equally with the averaging operation.
low computation cost. It is found that the 3D regularization
Nonetheless, it is essential to emphasize that occlusions are
can capture better geometry structures, perform photometric
crucial to consider, as they pose common challenges in MVS,
matching in 3D space, and alleviate the influence of image
frequently leading to invalid matching and inaccurate estima-
distortion caused by perspective transformation and occlu-
tions [34]. Incorporating visibility information to aggregate
sions [95]. However, since 3D CNN is memory and run-
matching details from source views can substantially bolster
time consuming, many offline methods [15], [36], [95], [96]
robustness against occlusions, thereby enhancing the accuracy
use limited depth hypotheses and estimate depth maps at low
of the reconstruction process [34].
resolution.
To estimate view weights for source views, PVA- RNN:Insteadof3DCNN,R-MVSNet[37]sequentiallyregu-
MVSNet [90] applies gated convolution [91] to adaptively larizes2Dslicesofthecostvolumealongthedepthdimension
aggregate cost volumes. View aggregation tends to give oc- with a convolutional GRU [98], which is able to gather
cluded areas smaller weights and the reweighting map is spatial and uni-directional context information in the depth
yielded according to the volume itself. Vis-MVSNet [92] dimension. D2HC-RMVSNet [38] augments R-MVSNet [37]
aggregates pair-wise cost volumes by weighted sum, where withacomplexconvolutionalLSTM[93].AA-RMVSNet[77]
the weight is negatively related to the uncertainty of depth furtherintroduceanintra-viewfeatureaggregationmodulefor
probabilitydistribution. [40],[41],[63]utilizepixel-wiseview feature extraction and an inter-view cost volume aggregation
weightnetworkstolearnviewweightsfromthepair-wisecost module to adaptively aggregate cost volumes of different
volumes without supervision. views. With sequential process of 2D slices, these methods8
upconv4 upconv3 upconv2 upconv1
Reference Frame Decoder Layer
conv1 conv2 conv3 conv4 conv5
Cost Volume Depth Estimation
disp3 disp2 disp1 disp0
Reference Image Source Images Encoder Layer Skip Connection
Fig.4. PipelineofMVDepthNet[25].Multipleimageframesareencodedinthecostvolume.MVDepthNettakesthereferenceimageandthecostvolume
astheinputandoutputsinversedepthmapsoffourdifferentresolutions.Skipconnectionsbetweentheencoderanddecoderofthesameresolutionareused
fortheper-pixeldepthestimation.
H Homopgraphy
+ Addition
. C Concatenation
.
L Loss
.
GT Depth Map
Shared Weights L
Source Images
Initial Depth Map
L
H C +
Reference Image Soft Argmin
Refined Depth Map
Feature Extraction Cost Volume Regularization Depth Map Refinement
Fig.5. PipelineofMVSNet[15].Referenceandsourceimagesgothroughthefeatureextractionnetwork,followedbythedifferentiablehomographwarping
toconstructthecostvolume.A3DU-Netisusedtoregularizethecostvolumeintoaprobabilityvolume.Thefinaldepthmapisestimatedfromtheprobability
volumeandrefinedwiththereferenceimagefeatures.
improve the scalability for high-resolution reconstruction as ‚àÜ(k+1) is the residual depth to be determined in the current
well as large-scale scenes and reduce memory, however, at stage. Following MVSNet [15], 3D CNN is used on each
the cost of run-time. stage to regularize the cost volume. UCS-Net [39] estimates
Coarse-to-Fine: Predicting depth in a coarse-to-fine man- uncertainty from the coarse prediction to adaptively adjust
ner [32], [39], [40], [63], [75], [78], [99] is another solution search ranges in finer stages. CVP-MVSNet [75] constructs
of reducing both memory consumption and running-time. A the cost volume with a proposed optimal depth resolution
coarse depth map is first predicted and then upsampled and of half pixel to narrow depth range in finer stages. EPP-
refined during finer stages to construct fine details. CasMVS- MVSNet [100] introduces an epipolar-assembling module to
Net [32] constructs cascade cost volumes by warping features assemble high-resolution information into cost volume and
with reduced depth ranges around the previous coarse maps. an entropy-based process to adjust depth range. TransMVS-
Based on Eq. (3), the homography of differential warping at Net[79]usesaFeatureMatchingTransformerforrobustlong-
stage k+1 is range global context aggregation within and across images.
WT-MVSNet[81]usesawindow-basedEpipolarTransformer
H(k+1)(d(k)+‚àÜ(k+1))=(d(k)+‚àÜ(k+1))K T T‚àí1K‚àí1, for enhanced patch-to-patch matching, and a window-based
i 0 0 i i
(14) Cost Transformer to better aggregate global information.
where d(k) is the estimated depth value at stage k and
. . .9
H
Coarse Cost 3DCNN Coarse Probability
Volume Volume
H
H√óW√óD H√óW√óD
H
Fine Cost Fine Probability
3DCNN
Volume Volume
Cost Volume 3DCNN Probability Volume H Differentiable Homography
(a) Direct 3DCNN Regularization (b) Corse-to-Fine Regularization
H√óW√ó1 H√óW√ó1
2DCNN
RNN
H√óW√óD H√óW√óD
2DCNN D
. . .
. . .
. . .
D
RNN
Cost Volume 2DCNN Probability Volume
ÔºàcÔºâRNN Regularization
Fig.6. IllustrationoftypicalcostregularizationschemesforofflineMVSmethodsthatuse4Dcostvolumes.(a)direct3DCNNregularization[15],[36],
[95],[96]appliesa3DCNNtoaggregatecontextualinformation;(b)coarse-to-fineregularization[32],[39],[63],[75]constructsmulti-scalecostvolumes
basedoncoarsepredictionanduses3DCNNregularizationoneachscale;(c)RNNregularization[37],[38],[77]sequentiallyregularizes2Dslicesofthe
costvolumetoreducememoryconsumption.
D. Iterative Update requirements.
Diverging from conventional approaches, certain meth- Some methods [40], [103] combine iterative Patch-
ods[40],[41],[101],[102]adoptiterativeupdatestogradually Match [104] with deep learning. PatchMatch algorithm is
refine depth maps. Iterative methods introduce a dynamic widely used in many traditional MVS methods [18], [19],
approach to depth map estimation, enabling iterative refine- [34], [105]. The PatchMatch algorithm mainly consists of:
mentofthereconstructionprocess.Byiterativelyupdatingthe random initialization, propagation of hypotheses to neighbors,
depth maps based on successive iterations, these methods can andevaluationforchoosingbestsolutions.Afterinitialization,
progressively improve the accuracy and consistency of the re- the approach iterates between propagation and evaluation
constructed 3D scene. This iterative refinement is particularly until convergence. Traditional methods usually design fixed
advantageous in scenarios where initial estimations may be patterns for propagation, e.g., propagation of Gipuma [18]
coarse or inaccurate, as it allows for finer adjustments to be as shown in Fig. 7. Recently, PatchmatchNet [40] proposes
made to the depth maps over multiple iterations. Moreover, learned adaptive propagation and cost aggregation modules,
the ability to control the number of iterations provides users whichenablesPatchMatchtoconvergefasteranddelivermore
withtheflexibilitytoprioritizeeithercomputationalefficiency accurate depth maps. Instead of propagating depth samples
or reconstruction quality, depending on specific application naivelyfromastaticsetofneighborsasdonetraditionally[18],10
iteratively refines index fields with a 3-level GRUs. MaG-
Net [88] iteratively updates the Gaussian distribution of depth
for each pixel with the matching scores. IGEV-MVS [112]
iteratively updates a disparity map regressed from geometry
encoding cost volumes and pairs of group-wise correlation
volumes.
E. Depth Estimation
1) OnlineMVS: Manymethods[25],[31],[35],[58]apply
encoder-decoder on the cost volume C and reduce the feature
channelto1,i.e.,C‚Ä≤ ‚ààRH√óW√ó1.Thenthesigmoidactivation
Fig. 7. Traditional chessboard propagation scheme of Galliani et al. [18].
Planesfromalocalneighborhood(redpoints)serveascandidatestoupdate œÉ is applied for normalization. Together with the predefined
agivenpixel(black).(a)Standardpatternwith20neighbors.(b)Simplified depth range [d ,d ] (mostly manually set), the depth map
min max
patternwith8neighbors.
canbecomputed.Forexample,DeepVideoMVS[31]estimates
depth as:
(cid:18)(cid:18)
1 1
(cid:19)
1
(cid:19)‚àí1
dÀÜ(p)= ‚àí ¬∑œÉ(C‚Ä≤(p))+ , (15)
d d d
min max max
where p is the pixel coordinate. For other methods, Neural-
RGBD[87]directlymodelsthecostvolumeasprobabilityvol-
umePandadoptssoftargmax [60]topredictdepth,Eq.(16).
MaGNet [88] takes the mean of Gaussian distribution at last
Fig. 8. Adaptive propagation scheme of Wang et al. [40]. Pixels located iteration as depth.
at the object boundary (yellow) and a textureless region (red) receive depth 2) Offline MVS: For a cost volume C ‚àà RH√óW√óD√óC, a
hypothesesfromsampledneighbors(greenandorange).(a)Referenceimage.
(b) Fixed sampling locations of classic propagation. (c) Adaptive sampling probability volume P ‚àà RH√óW√óD is usually generated after
locationswithadaptivepropagation.Thegrayscaleimagein(b)and(c)isthe cost volume regularization, which is then used for depth esti-
groundtruthdepthmap.
mation.Currently,almostallthelearning-basedMVSmethods
useexclusivelyeitherregression(softargmax)orclassification
(argmax) to predict depth.
PatchmatchNet adaptively gathers samples from pixels of the
Following GCNet [60], MVSNet [15] uses soft argmax to
same surface with the guidance of image features, shown in
regress the depth map with sub-pixel precision. Specifically,
Fig. 8. It iteratively updates the depth maps in a coarse-to-
the expectation value along the depth direction of probability
finestructure.Without3DCNN,PatchmatchNetdemonstrates
volume P is computed as the final prediction:
a remarkably low computation time and memory than other
coarse-to-fine methods [32], [39], [75]. Another PatchMatch- D
based method, PatchMatch-RL [103], jointly estimates depth,
dÀÜ(p)=(cid:88)
d i¬∑P(i,p), (16)
normal and visibility with a coarse-to-fine structure. Consid- i=1
ering argmax based hard decisions/sampling of PatchMatch is where p is the pixel coordinate, d is i-th depth sample and
i
non-differentiable, PatMatch-RL adopts reinforcement learn- P(i,p) is the predicted depth probability. For coarse-to-fine
ing in the training process. methods [32], [39], [75], soft argmax is applied on each
Recently, RAFT [106] estimates optical flow by itera- stage to regress the depth maps. On the other hand, some
tively updating a motion field through a GRU and achieves methods [36], [40] compute the expectation value of inverse
state-of-the-art performance. The idea is further adopted in depthsamplessincethissamplingismoresuitableforcomplex
stereo[107],sceneflow[108]andSfM[109].InMVS,several and large-scale scenes [36]. The depth prediction is computed
methods [41], [101], [102] also adopted this approach to as:
e lin gh ha twnc ee ige hf tfi Gci Ren Uc -y baa sn edd pfl re ox bi ab bil ii lt iy ty. I et se tr iM maV toS r[ th4 a1 t] ep nr co op do es ses tha
e
dÀÜ(p)=(cid:32) (cid:88)D 1 ¬∑P(i,p)(cid:33)‚àí1
. (17)
d
i
per-pixelprobabilitydistributionofdepthinitshiddenstate.In i=1
each iteration, multi-scale matching information is injected to In contrast, methods [37], [38], [77] that use RNN for cost
updatethepixel-wisedepthdistribution.CER-MVS[101]and volume regularization mainly adopt argmax operation. They
Effi-MVS[102]bothembedtheRAFTmoduleinacoarse-to- choose the depth sample with the highest probability as the
fine structure. With the injection of matching information, the final prediction, which is similar to classification. Since the
hidden state is updated and outputs a residual that is added argmax operation adopted by winner-take-all cannot produce
to the previous depth estimation. DELS-MVS [110] proposes depthestimationswithsub-pixelaccuracy,thedepthmapmay
Epipolar Residual Network to search for the corresponding be refined in post-processing [37].
point in the source image directly along the corresponding Comparatively, soft argmax operation corresponds to mea-
epipolar line and follows an iterative manner to narrow down suring the distance of the expectation to the ground truth
the search space. Based on IterMVS [41], RIAV-MVS [111] depth. While the expectation can take any continuous value,11
the measure cannot handle multiple modes in P and strictly
prefers unimodal distributions [41]. The argmax operation
corresponds to measuring the Kullback-Leibler divergence
between a one-hot encoding of the ground truth and P, but
cannot achieve sub-pixel precision [41]. Recently, Wang et
al. [41] propose a simple yet effective hybrid strategy that
combines regression and classification. Similar to [113], it is
robusttomulti-modaldistributionsbutalsoachievessub-pixel Fig. 9. Visualization of confidence on DTU [21]. The regions with low
precision. First, similar to R-MVSNet [37], the index X(p) confidencetypicallycorrespondtoocclusionsandlowtexture.
with the highest probability for pixel p from probability P
the estimation [15], [32], [40] from the probability volume as
is found. Then the expectation in the local inverse range is
computed as the depth estimate dÀÜ(p): confidence. Fig. 9 provides a set of RGB images, estimated
depth maps, and the visualized confidence. In stereo match-
Ô£´ Ô£∂‚àí1
X(p)+r ing, some methods learn confidence from disparity [122],
dÀÜ(p)=Ô£≠ 1 (cid:88) 1 ¬∑P(j,p)Ô£∏, (18) RGB image [123], [124] or matching costs [125] and obtain
(cid:80)X j=( Xp) (+ pr )‚àírP(j,p) j=X(p)‚àírd j confidence scores in [0,1] interval. Motivated by this, some
traditional MVS methods [126], [127] based on classical
where d is the j-th depth sample. Concurrently, Peng et
j
PatchMatch [105] propose to estimate the confidence with
al. [114] propose a different strategy to unify regression and
deep learning and use this to refine the results from Patch-
classification. They first use classification to get the optimal
Match. Recently, IterMVS [41] estimates confidence from the
hypothesis and then regress the proximity for it.
pixel-wise depth probability distributions, which are encoded
DMVSNet [115] estimates two depth maps on multi-stages
by the hidden state of a convolutional GRU. A 2D CNN
and composes the final depth map by alternating between
followedbyasigmoid isappliedtothehiddenstatetopredict
selecting the maximum and minimum predicted depth values,
theconfidence.DELS-MVS[110]feedspixel-wiseentropyof
whichgeneratesanoscillatingdepthmapforimprovedgeom-
the partition probabilities, which is computed for each source
etry.
image, into a confidence network to learn a confidence map.
The confidence map is used to guide the fusion of multiple
F. Depth Refinement
depth maps.
Given that the raw depth estimation from MVS may be
noisy, refinement is usually used to improve the accuracy.
H. Loss Function
R-MVSNet [37] enforces multi-view photo-consistency to
alleviatethestaireffectandachievesub-pixelprecision.Point- 1) Online MVS: Many methods [25], [31], [58] compute
MVSNet [95] and VA-Point-MVSNet [116] use PointFlow to the regression loss of estimated inverse depth maps for train-
refine the point cloud iteratively by estimating the residual ing:
(cid:88) 1 1
between the depth of the current iteration and that of the L= ‚à• ‚àí ‚à• , (19)
ground truth. Fast-MVSNet [117] adopts an efficient Gauss-
d(p) dÀÜ(p) 1
p
Newton layer to optimize the depth map by minimizing
wherepdenotesthepixelcoordinate,d(p)denotestheground
the feature residuals. PatchmatchNet [40] refines the final
truth depth, dÀÜ(p) denotes the estimation and ||¬∑|| denotes
upsampled depth map with a depth residual network [118] 1
the L loss. For other methods, Neural-RGBD [87] uses
and reference image feature. [41], [101], [102] use the mask 1
Negative-Log Likelihood (NLL) over the depth with its depth
upsampling module from RAFT [106] to upsample and refine
probability volume. Similarly, MaGNet [88] uses NLL loss
the depth map to full resolution by computing the weighted
since the per-pixel depth is modeled as Gaussian distribution.
sum of depth values in a window based on the mask. Based
SimpleRecon [35] computes the regression loss with log-
on a coarse depth map, RayMVSNet [119], [120] aggregates
depth. To improve performance, SimpleRecon further uses
multi-view image features with an epipolar transformer and
gradientlossondepth,normallosswherenormaliscomputed
use a 1D implicit field to estimate the SDF of the sampled
with depth and intrinsics, and multi-view regression loss.
points and the location of the zero-crossing point. GeoMVS-
2) Offline MVS: Based on the depth estimation strategy as
Net [76] filters the depth map by geometry enhancement in
discussed in Sec. II-E, loss functions can be mainly catego-
the frequency domain. EPNet [121] uses a hierarchical edge-
rizedintoregressionandclassification.Formethods[15],[36]
preservingresiduallearningmoduletorefinemulti-scaledepth
that predict depth with soft argmax [60], (smooth) L loss is
estimation with image context features. 1
usually adopted as the loss function, which is stated as:
G. Confidence Estimation L=(cid:88) ‚à•d(p)‚àídÀÜ(p)‚à• . (20)
1
As discussed in Sec. I-D, photometric confidence is impor- p
tanttofilteroutunreliableestimationsduringdepthfusionfor For coarse-to-fine methods [32], [39], [40], [75], the L loss
1
offline MVS methods. Following MVSNet [15], most offline is computed on each stage for multi-stage supervision.
MVS methods take the probability of the estimation [37], For methods [37], [38], [77] that predict depth with an
[38], [77] or the probability sum over several samples near argmax operation, cross entropy loss is commonly used for12
A. Photometric Consistency Assumption
In the realm of unsupervised depth map prediction, extant
methods [44], [131]‚Äì[133] endeavor to establish photometric
consistency between reference and source perspectives. This
pivotal notion revolves around the augmentation of similarity
between the reference image I and individual source images
0
I after their warping to align with the reference view.
i
InrelationtothedepthestimationdenotedasdÀÜfortheinitial
image I , the process involves the projection of reference
0
pixels into the subsequent image I using the formulation
i
presented in Eq. (4).
Subsequently, the distorted version of the source image
denotedasÀÜIi iscreatedbyinterpolatingtheRGBvaluesofthe
0
source image at the displaced pixel positions through bilinear
sampling. This interpolation is carried out at the locations
where the pixels have been transformed due to the warping
process.Additionally,alongsidethewarpedimageÀÜIi,abinary
0
Fig. 10. Ambiguous supervision from photometric consistency assump- mask M i is commonly generated. This mask is employed to
tion [43]. (a) Complicated lighting condition caused by varying camera excludepixelsthathavebeenprojectedbeyondtheboundaries
exposures.(b)Invisibilityofspecificareacausedbyocclusion.
of the image and are, thus, considered invalid.
The photometric consistency loss L can be written as:
thelossfunctionsincetheproblemismulti-classclassification. PC
The cross entropy loss function is defined as:
L
=N (cid:88)‚àí1 1 ((cid:13) (cid:13)(cid:16)
ÀÜIi ‚àíI
(cid:17)
‚äôM
(cid:13)
(cid:13)
(cid:88)(cid:32) (cid:88)D (cid:33) PC i=1 ‚à•M i‚à• 1 (cid:13) 0 0 i(cid:13) 2 (22)
L= ‚àíG(i,p)¬∑log[P(i,p)] , (21) (cid:13)(cid:16) (cid:17) (cid:13)
+(cid:13) ‚àáÀÜIi ‚àí‚àáI ‚äôM (cid:13) ),
p i=1 (cid:13) 0 0 i(cid:13) 2
where G(i,p) is the ground truth one-hot vector of depth at where ‚àá denotes the gradient at the pixel level, while ‚äô
pixel p, and P(i,p) is the predicted depth probability. symbolizes element-wise Hadamard multiplication. In most
cases [42]‚Äì[44], [131], [132], the incorporation of structural
For methods that predict depth with a hybrid strategy of
similaritylossanddepthsmoothnesslossintothecomputation
classification and regression, Wang et al. [41] adopt both L
1
is commonplace. This practice aims to enhance the training
loss and cross entropy loss to supervise the regression and
process‚Äôs stability and speed up the convergence.
classification respectively, while Peng et al. [114] use focal
The computation of the structural similarity loss occurs
loss [128].
between a synthesized image and the initial reference image,
aimingtoupholdcontextualcongruity.Moreprecisely,theas-
III. UNSUPERVISED&SEMI-SUPERVISEDMETHODSWITH sessment of contextual similarity often involves the utilization
DEPTHESTIMATION of the Structural Similarity Index (SSIM) proposed by [134],
a metric commonly employed for quantifying contextual con-
In this section, we introduce unsupervised learning-based
sistency. This metric is defined as follows:
MVS methods [42]‚Äì[44], [129], [130] and semi-supervised
method [46]. Supervised MVS methods mentioned in Sec. II
N‚àí1
dependextensivelyontheavailabilityofaccurategroundtruth L =
(cid:88) (cid:104) 1‚àíSSIM(cid:16)
I
,ÀÜIi(cid:17)(cid:105)
‚äôM , (23)
SSIM 0 0 i
depth maps obtained through depth-sensing equipment. This
i=1
requirement not only complicates the data collection process,
making it labor-intensive and costly but also restricts these (2¬µ ¬µ +c )(2œÉ +c )
SSIM(x,y)= x y 1 xy 2 , (24)
methods to limited datasets and primarily indoor settings. To (cid:0) ¬µ2 +¬µ2 +c (cid:1)(cid:0) œÉ2+œÉ2+c (cid:1)
x y 1 x y 2
make MVS practical in more general real-world scenarios,
where ¬µ, œÉ2 represent the mean and variance of the images,
it is vital to consider alternative unsupervised learning-based
c ,c are constants to avoid numerical issues.
methods that can provide competitive accuracy compared to 1 2
The incorporation of a smoothness loss term serves to
the supervised ones without any ground truth depth. Existing
promotethecontinuityofdepthinformationwithinthecontext
unsupervised methods are built upon the assumption of pho-
of image and depth disparity alignment. This continuity is
tometric consistency (Sec. III-A), which indicates that corre-
evaluated based on the color intensity gradient present in the
sponding pixels from different views of the same 3D point
input reference image. The computation of the smoothness
share similar features. These approaches are categorized into
loss, L , is defined as follows:
end-to-endones(Sec.III-B)andmulti-stageones(Sec.III-C). SM
SGT-MVSNet [46] is the only semi-supervised method so far, L =(cid:88)(cid:12) (cid:12)‚àá dÀú(x)(cid:12) (cid:12)e‚àí|‚àáuI0(x)|+(cid:12) (cid:12)‚àá dÀú(x)(cid:12) (cid:12)e‚àí|‚àávI0(x)|,
andweintroduceitinSec.III-D.Wealsoanalyzetheprimary SM (cid:12) u (cid:12) (cid:12) v (cid:12)
x
challenges in unsupervised MVS in Sec. III-B. (25)13
where ‚àá and ‚àá refer to the gradient along x and y axis, this learned representation, enhancements are proposed for
u v
dÀú=d/d¬Øis the mean-normalized inverse depth, M represents both depth hypotheses propagation and evaluation within the
the set of valid pixels in the reference image. context of part-aware patchmatch. Finally, the network is
optimized through the incorporation of a dense contrastive
loss for self-supervised training, coupled with the integration
B. End-to-end Unsupervised Methods
of a spatial concentration loss [137] designed to promote the
End-to-endmethods[42],[43],[131],[135],[136]arethose isotropic isolation of all pixel embeddings.
methods that are trained from scratch with the same input CL-MVSNet [136] proposes a framework that aims to
information as supervised methods (Sec. II), while not using enhance proximity among positive pairs by ensuring con-
groundtruthdepthforsupervision.Instead,theyusuallyadopt trastive consistency between a regular branch and two con-
photometric consistency, structural similarity and smoothness trastive branches. The regular branch is a CasMVSNet [32]
constraint as parts of their loss terms. Unsup MVS [131] structure that has been used in [42], [43], [135] as well,
proposes to inherit the supervision signal of view synthesis, while contrastive branches incorporate an image-level branch
as discussed in Sec. III-A, and dynamically select X ‚Äúbest‚Äù and a scene-level branch. These branches enforce contrastive
(lowest loss) values out of Y loss maps. consistencyguidedbytheconfidencemaskestimatedfromthe
However, the bottleneck in unsupervised MVS is to find regularbranch.Additionally,theL0.5photometricconsistency
such accurate photometric correspondences. As illustrated in is proposed to enhance the accuracy of the reconstruction.
Fig.10,intherealscenarios,non-Lambertiansurfaces,varying Note that these end-to-end methods are all trained from
camera exposures, and occlusions will make the assumption scratch without any pre-processing which saves training time
of photometric consistency invalid. Hence, such an invalid and reduces the complexity of application in real scenarios.
assumption will lead to ‚Äúambiguous supervision‚Äù.
To alleviate the ambiguous supervision, JDACS [42] in-
C. Multi-stage Unsupervised Methods
troduces semantic consistency in addition to photometric
consistency. It proposes to extract semantic features with a Multi-stagemethodsrequireeitherpre-trainingofaspecific
pre-trained network and create semantic classification map module or pre-processing on the training data. These methods
by applying non-negative matrix factorization. Then pixel- are built upon the idea of pseudo-label generation.
wise cross-view segmentation consistency is calculated. The Self-supervised CVP-MVSNet [132] proposes to gener-
difference here is that the semantic map is warped, instead ate pseudo ground truth of depth by training the CVP-
of the image. In this way, the semantic classification maps MVSNet [75] backbone with photometric consistency loss
are supervised by cross-entropy loss between the warped (Sec. III-A). The pseudo ground truth depth maps are refined
map S and reference map S , both encoded as pixel-wise with high-resolution images, filtered with cross-view depth
i 0
one-hot vector. However, the proposed semantic cross-view consistency check and then fused in a point cloud [15]. Then
consistency loss is unstable to converge during training and the mesh is reconstructed with the screened Poisson surface
such high-level semantic information excludes details and reconstructionalgorithm[67]torendercompletepseudodepth
boundaries in MVS. maps for training in the next iteration. The whole pipeline
RC-MVSNet [43] digs into providing reliable and consists of several iterations to improve the performance.
occlusion-aware supervision by introducing neural rendering. However, [132] cannot provide a satisfying reconstruction
It takes the advantage of both strong representation ability of of the 3D objects since it heavily relies on the photometric
Neural Radiance Field [28] and strong generalization ability consistency assumption. Additionally, the self-training stage
of cost volume. First, the unsupervised CasMVSNet [32] of the whole pipeline takes few days to complete.
backbone in RC-MVSNet yields initial depth map supervised U-MVSNet [44] introduces a flow-depth consistency loss
by photometric consistency. In addition, it generates depth by pre-training an optical-flow estimation network, PWC-
priors for the Gaussian-Uniform mixture sampling in the Net [138], in an unsupervised manner. The dense 2D optical
rendering-consistency (RC) network. Then after volumetric flow correspondences are used to generate pseudo labels for
rendering of the reference view, the depth map is supervised cross-view flow-depth consistency, which alleviates the am-
byanextrareferenceviewsynthesislossandadepthrendering biguous supervision in the foreground. Then the uncertainty-
consistencyloss.Inthisway,occlusion-awareneuralrendering aware self-training consistency further reduces the invalid
with mixture sampling is utilized to alleviate the invisibility supervision in the background utilizing the generated pseudo
phenomenon and ambiguous supervision from different light- labels and uncertainty map.
ing condition is solved by reference view synthesis. Recently, KD-MVS [45] achieves superior performance by
ElasticMVS [135] introduces an architecture of part-aware leveraging a typical teacher-student scheme of knowledge
patchmatch to address limitations in photometric loss-based distillation [139]‚Äì[141]. It trains a teacher MVS model, e.g.,
geometry information, which tends to have missing data and MVSNet [15] or CasMVSNet [32], in a self-supervised man-
artifacts in certain areas, particularly textureless regions. The ner, by enforcing both photometric and feature-metric con-
proposed framework incorporates an elastic part represen- sistency between the reconstructed and original images. Then
tation to encode geometric details for guiding piecewise- the teacher model produces pseudo ground-truth labels of the
smooth depth map prediction. The initial step involves the training set by cross-view consistency check and probabilistic
formal definition of part-aware representation. Building upon encoding. In addition to the reliable estimation by the teacher14
model, the pseudo labels also contain probabilistic knowledge
with estimation uncertainty. By training MVS networks [15],
[32] with the pseudo labels, the knowledge of the teacher
model is distilled to the student models.
D. Semi-supervised Methods
SGT-MVSNet[46]proposestouseonlyafewsparseground
truth 3D points to estimate the depth map of the reference Fig. 11. Pipeline of Atlas [47]. 2D image features are back-projected into
view. 3D point consistency loss is used to minimizes the 3Dvolumes,whichareaggregatedandpassedthrougha3DCNNtodirectly
regressaTSDFvolume.
difference between the 3D points back-projected from the
corresponding pixels and the ground truth. To tackle the
problem of inaccurate estimation on the edge and boundary, a
coarse-to-fine reliable depth propagation module rectifies the
erroneous predictions.
IV. LEARNING-BASEDMVSWITHOUTDEPTH
ESTIMATION
Though the learning-based methods that predict individual
Fig. 12. Pipeline of NeRF [28]. Given a 3D position and 2D viewing
depthmapswithplane-sweeparethemainfamilyoflearning-
direction(a),anMLPproducesthecolorandvolumedensity(b).Thenvolume
based multi-view stereo, there are many methods of other renderingisusedtocompositethesevaluesintoanimage(c).Theoptimization
families that achieve impressive 3D reconstruction quality in isminimizingtherenderingloss(d).
therecentyears.Inthissection,Wediscussfourmainfamilies:
voxel-based, NeRF-based, 3D Gaussian Splatting-based and
c and volume density œÉ. For a specific ray at a novel view-
large feed-forward methods.
point, NeRF uses approximated numerical volume rendering
to compute the accumulated color as:
A. Voxel-based Methods
N
(cid:88)
These methods [47], [48], [142], [143] estimate the scene C= T (1‚àíexp(‚àíœÉ Œ¥ ))c , (26)
i i i i
geometry with volumetric representation by leveraging im-
i=1
plicit function, e.g., SDF. Specifically, Atlas [47] and Neural-
whereiistheindexofsample,T
=exp(‚àí(cid:80)i‚àí1œÉ
Œ¥ )isthe
Recon [48] attempt to predict the TSDF volume from the 3D i j=1 j j
accumulated transmittance, and Œ¥ =t ‚àít is the distance
feature volume constructed by lifting 2D image features. As i i+1 i
betweenadjacentsamples.Themodelistrainedbyminimizing
shown in Fig. 11, Atlas uses 3D CNN to regress the TSDF
the loss between the predicted and ground truth color:
volume based on the feature volume accumulated from all
images of the scene, which exhibits great completeness of L =E[(||C‚àíC ||2]. (27)
color gt
reconstruction. The TSDF reconstruction is supervised using
L loss to the ground truth TSDF vaules. Since dense feature Many subsequent endeavors [144]‚Äì[152] further improve
1
volume brings lots of computational overhead, NeuralRecon NeRF in quality, fast training, memory efficiency and real-
furtherimprovestheefficiencybyincrementallyreconstructing time rendering.
the scene in a fragment-wise and coarse-to-fine manner. The Though the initial purpose of NeRF is to perform novel
3D features from different fragments are passed through view synthesis, VolSDF [49] and NeuS [50] integrate NeRF
the whole incremental reconstruction process with a RNN. with SDF for surface reconstruction. The SDF, denoted as
TransformerFusion [142] fuses coarse and fine image features f, is transformed into the density œÉ for volume rendering.
in a voxel grid with two transformers and then predicts an Specifically, for a point p(t), VolSDF computes the volume
occupancyfieldtorepresentthescenegeometry.VoRTX[143] density œÉ(p(t)) from the signed distance f(p(t)) as:
uses a similar design to TransformerFusion and the scene 1
geometry is obtained by passing three different-level features œÉ(p(t))= Œ≤Œ® Œ≤(‚àíf(p(t))), (28)
output from transformers through a 3D CNN.
where Œ® denotes the Cumulative Distribution Function
Œ≤
(CDF)ofazero-meanLaplacedistributionwithlearnablescale
B. NeRF-based Methods
parameter Œ≤ > 0. NeuS computes the density in a different
Innovelviewsynthesis,NeuralRadianceField(NeRF)[28] way as:
has kicked off a new emerging representation of 3D, which
‚àíf‚Ä≤(p(t))Œ¶‚Ä≤(f(p(t)))
offersadifferentiablevolume-renderingschemetosupervisea œÉ(p(t))=max( s ,0), (29)
Œ¶ (f(p(t)))
3D radiance-based representation with 2D image-level losses. s
The pipeline of NeRF is shown in Fig. 12. NeRF employs where Œ¶ is the sigmoid function with learnable scale param-
s
multi-layer perceptron (MLP) to map a position (x,y,z) and eters.Aftertraining,themeshcanbeextractedfromtheSDF
thenormalizedviewdirection(Œ∏,œï)tothecorrespondingcolor field with Marching Cubes [66].15
Note that without ground truth geometry supervision, e.g., SfM [17], [171] or MVS [34], which performs better than
depth map or TSDF volume, these methods are trained in a random initialization [170].
self-supervisedwayasNeRF,Eq.(27).Sincetrainingwithren- Motivated by the NeRF-based MVS methods, researchers
dering loss only like NeRF has ambiguity in geometry [153], try to adapt 3DGS for reconstruction. Though 3DGS achieves
some following methods improve the reconstruction with high-quality novel-view synthesis, it is challenging to recover
explicit geometry supervision, e.g., monocular depth/normal high quality geometry since no explicit geometry constraint
priors [154], [155] and sparse SfM point cloud [156]. Moti- is used and 3D Gaussians do not correspond well to the
vatedbyInstant-NGP[146]thatacceleratestrainingwithhash actual surface because of the 3D covariance [52]. As one of
grids, other methods [157]‚Äì[160] use hash grids to speed up the earliest methods, DreamGaussian [51] follows NeRF [28]
training and improve surface details. In addition, to overcome to compute a dense density grid with the 3D Gaussians and
the drawback that NeRF typically needs to be trained for then extracts mesh with Marching Cubes [66]. SuGaR [52]
each new scene, some recent methods [161]‚Äì[163] propose introducesageometryregularizationtermencouragingthe3D
generalizable pipelines for implicit reconstruction even under Gaussians to be well-aligned over the scene surfaces so that
sparse-view settings. Specifically, these methods project 3D the Gaussians can contribute to better scene geometry. To
points on the image planes and aggregate the corresponding reconstruct the mesh, SuGaR samples 3D points on a level
image features as a feature volume like Atlas [47], which is set of the density computed from 3D Gaussians and then runs
used to estimate the surface location. Poisson Reconstruction [172] on these points. NeuSG [173]
Recall that the depth map-based MVS methods predict jointly trains Neuralangelo [158], a NeRF-based method, and
depth with photometric consistency across multiple views. 3DGS. During training, the normals from the SDF field of
However, this assumption fails for glossy surfaces with re- Neuralangelo regularize the rotation of 3D Gaussians, while
flections and thus they cannot reconstruct them accurately. the SDF field is regularized to ensure that the SDF values at
In contrast, some NeRF methods can handle reflections well. 3D Gaussians‚Äô positions are close to zero. However, retriev-
Recently, Ref-NeRF [164] reparameterizes the appearance ing good geometry from Gaussian-based representations still
with separate diffuse and reflective components by using the needs to be explored.
reflectedviewdirection,whichimprovestherenderingofspec-
ular surfaces. Therefore, recent methods [160], [165]‚Äì[168]
D. Large Feed-forward Methods
adoptthisrepresentationinreconstructionandcansuccessfully
reconstruct specular surfaces. Specifically, based on VolSDF / In3Dreconstructionand3Dgeneration,arecentprominent
NeuS, these methods mainly replace the view direction with trendistodirectlylearnthe3Drepresentationfromlarge-scale
the reflected view direction following Ref-NeRF. 3D datasets, such as Objaverse [30]. These methods typically
adopt large-scale transformers [174] to generate a 3D repre-
sentationbasedonvariousinputs,includingsingleimage[53],
C. 3D Gaussian Splatting-based Methods
[175], [176], textual description [177], posed multi-view im-
Unlike implicit representations with a coordinate-based
ages [178], and un-posed multi-view images [54], [179].
MLP such as NeRF [169], 3D Gaussian Splatting [170]
LRM [53] uses a transformer-based feed-forward network to
(3DGS) explicity represents the scene with point primitives,
regress the 3D object from a single image. Specifically, LRM
each of which is parameterized as a scaled Gaussian with 3D
predicts features of a tri-plane representation [180], which is
covariance matrix Œ£ and mean ¬µ:
subsequently converted into an implicit field like NeRF [53].
G(x)=e‚àí 21(x‚àí¬µ)TŒ£‚àí1(x‚àí¬µ), (30) Then the surface is extracted with Marching Cubes [66].
Recently, DUSt3R [54] reconstructs the scene as a point
wherexisanarbitraryposition.Œ£isformulatedwithascaling cloudgivenapairofun-calibratedandun-posedimages.After
matrix S and rotation matrix R as: extracting image token representations with Vision Trans-
former [181], two transformer decoders exchange informa-
Œ£=RSSTRT. (31)
tion of the token representations via cross-attention and then
In addition, each Gaussian contains the color c modeled by regress the per-pixel 3D points in the coordinate frame of the
Spherical Harmonics and an opacity Œ±. Different from NeRF first image. Many downstream tasks can be performed with
thatusesvolumerendering,3DGSefficientlyrendersthescene the point clouds, e.g., point matching, localization, intrinsic
viatile-basedrasterization.Afterprojecting3DGaussianG(x) and relative pose estimation.
into the 2D Gaussian G‚Ä≤(x) on the image plane [170], a tile-
basedrasterizerefficientlysortsthe2DGaussiansandemploys V. DISCUSSIONS
Œ±-blending for rendering:
In this section, we summarize and discuss the performance
(cid:88) i (cid:89)‚àí1 of learning-based MVS methods, including supervised online
C(x)= c œÉ (1‚àíœÉ ), œÉ =Œ± G‚Ä≤(x), (32)
i i j i i methods with depth estimation (Sec. V-A), supervised offline
i‚ààN j=1 methodswithdepthestimation(Sec.V-B),unsupervisedmeth-
where x is the pixel location, N is the number of sorted 2D ods with depth estimation (Sec. V-C) and methods without
Gaussians. During training, 3DGS minimizes the rendering depth estimation (Sec. V-D). Moreover, we discuss potential
loss like NeRF, as in Eq. (27). 3DGS can be initialized with directions for future research (Sec. V-E).16
TABLEII
QUANTITATIVERESULTSOFSUPERVISEDONLINEMVSMETHODSONSCANNET[26]AND7-SCENES[27].
ScanNet[26] 7-Scenes[27]
Methods
abs‚Üì abs-rel‚Üì abs-inv‚Üì Œ∑<1.25‚Üë abs‚Üì abs-rel‚Üì abs-inv‚Üì Œ∑<1.25‚Üë
MVDepthNet[25] 0.167 0.087 0.054 0.925 0.201 0.117 0.071 0.877
DPSNet[97] 0.219 0.119 0.071 0.868 0.249 0.149 0.085 0.826
Neural-RGBD[87] 0.236 0.122 0.075 0.850 0.214 0.131 0.076 0.865
GP-MVS[58] 0.149 0.076 0.049 0.940 0.174 0.100 0.064 0.903
Longetal.[94] 0.151 0.081 - 0.931 0.253 0.147 - 0.804
DeepVideoMVS[31] 0.119 0.060 0.038 0.965 0.145 0.038 0.054 0.938
MaGNet[88] 0.147 0.081 - 0.930 0.213 0.126 - 0.855
RIAV-MVS[111] 0.139 0.075 - 0.938 0.178 0.100 - 0.897
SimpleRecon[35] 0.089 0.043 - 0.981 0.105 0.058 - 0.974
TABLEIII
QUANTITATIVERESULTSOFPOINTCLOUDEVALUATIONONMVSBENCHMARKS[21]‚Äì[23]FORUNSUPERVISEDDEPTHMAP-BASEDMVS
METHODS.‚ÄúFINETUNEDWITHBLENDEDMVS‚ÄùDENOTESWHETHERTHEMETHODSAREFINETUNEDONBLENDEDMVS[24]BEFOREEVALUATINGON
TANKSANDTEMPLES[22].
DTU[21] Finetunedwith TanksandTemples[22]
Methods
Acc.‚Üì Comp.‚Üì Overall‚Üì BlendedMVS[24] IntermediateF1 ‚Üë AdvancedF1 ‚Üë
Unsup MVSNet[131] 0.881 1.073 0.977 ‚úó - -
MVS2 [129] 0.760 0.515 0.637 ‚úó 37.21 -
M3VSNet[130] 0.636 0.531 0.583 ‚úó 37.67 -
End-to-end JDACS-MS[42] 0.398 0.318 0.358 ‚úó 45.48 -
RC-MVSNet[43] 0.396 0.295 0.345 ‚úó 55.08 30.82
ElasticMVS[135] 0.374 0.325 0.349 ‚úó 57.88 37.81
CL-MVSNet[136] 0.375 0.283 0.329 ‚úó 59.39 37.03
Self-supervisedCVP-MVSNet[132] 0.308 0.418 0.363 ‚úó 43.48 -
Multi-stage U-MVSNet[44] 0.354 0.354 0.354 ‚úó 57.15 30.97
KD-MVS[45] 0.359 0.295 0.327 ‚úì 64.14 37.96
A. Supervised Online MVS with Depth Estimation the methods that use RNN and coarse-to-fine regularization
performmuchbetter.SinceRNNbasedmethodshavebadtime
Typically,onlineMVSmethodsaretrainedonScanNet[26]
efficiency, we can find that coarse-to-fine methods become
and then evaluated on ScanNet [26] and 7-Scenes [27]. We
main stream of the community because of their impres-
summarize the quantitative results of online MVS methods
sive performance and high efficiency in both memory and
in Tab. II. By incorporating temporal information in depth
run-time. In addition, iterative methods based on traditional
estimation, [31], [58], [94] achieve better performance than
PatchMatch [40], [103] or RAFT [41], [101], [102] also
MVDepthNet[25].However,thisusuallyincreasethenetwork
become popular since they achieve comparable performance
complexity. Instead of temporal fusion, SimpleRecon [35]
as state-of-the-art methods with more lightweight structure.
injects cheaply available metadata into the cost volume and
For benchmarks, most learning-based methods perform ex-
achieves state-of-the-art performance on both ScanNet and 7-
plicitly better than traditional methods on DTU and Tanks
Scenes.
andTemples.However,onETH3D,traditionalmethods[127],
[189] perform better than all existing learning-based methods.
B. Supervised Offline MVS with Depth Estimation
We conjecture that the strong viewpoint variations and large
Offline MVS methods are usually trained on DTU [21] and textureless regions in human-made environments still make
then evaluated on DTU [21], Tanks and Temples [22] and learning-based methods struggle. Therefore, it is valuable to
ETH3D [23]. Recently, many methods further finetune the pay more attention to ETH3D to evaluate the robustness of
DTU-pretrainedmodelonBlendedMVS[24]beforeevaluating learning-based methods in real-world scenes.
onTanksandTemplesandETH3Dsinceitcontainslarge-scale
scenes. 2) Memory Consumption and Run-time: Low run-time
1) Benchmark Performance: Tab. IV summarizes quan- and memory consumption are essential in most industrial
titative results of offline MVS methods. Compared with applications with limited computational power and storage,
those methods that use direct 3D CNN for regularization, e.g., autonomous driving and AR/VR. Because of the low-17
TABLEIV
QUANTITATIVERESULTSOFPOINTCLOUDEVALUATIONONMVSBENCHMARKS[21]‚Äì[23]FORSUPERVISEDOFFLINEMVSMETHODS.‚ÄúFINETUNED
WITHBLENDEDMVS‚ÄùDENOTESWHETHERTHELEARNING-BASEDMETHODSAREFINETUNEDONBLENDEDMVS[24]BEFOREEVALUATINGONTANKS
ANDTEMPLES[22]ANDETH3D[23].
DTU[21] Finetunedwith TanksandTemples[22] ETH3D[23]
Methods
Acc.‚Üì Comp.‚Üì Overall‚Üì BlendedMVS[24] IntermediateF1‚Üë AdvancedF1‚Üë TrainingF1‚Üë TestF1‚Üë
MVSNet[15] 0.396 0.527 0.462 ‚úó 43.48 - - -
P-MVSNet[182] 0.406 0.434 0.420 ‚úó 55.62 - - -
CIDER[36] 0.417 0.437 0.427 ‚úó 46.76 23.12 - -
Direct3DCNN PointMVSNet[95] 0.342 0.411 0.376 - - - - -
VA-Point-MVSNet[116] 0.359 0.358 0.359 ‚úó 48.70 - - -
PVA-MVSNet[90] 0.379 0.336 0.357 ‚úó 54.46 - - -
Fast-MVSNet[117] 0.336 0.403 0.370 ‚úó 47.39 - - -
R-MVSNet[37] 0.383 0.452 0.417 ‚úó 48.40 24.91 - -
D2HC-RMVSNet[38] 0.395 0.378 0.386 ‚úó 59.20 - - -
RNN
AA-RMVSNet[77] 0.376 0.339 0.357 ‚úì 61.51 - - -
BH-RMVSNet[183] 0.368 0.303 0.335 ‚úì 61.96 34.81 - 79.61
CasMVSNet[32] 0.325 0.385 0.355 ‚úó 56.84 - - -
CVP-MVSNet[75] 0.296 0.406 0.351 ‚úó 54.03 - - -
UCS-Net[39] 0.338 0.349 0.344 ‚úó 54.83 - - -
AttMVS[184] 0.383 0.329 0.356 ‚úó 60.05 37.34 - -
Vis-MVSNet[92] 0.369 0.361 0.365 ‚úì 60.03 - - -
EPP-MVSNet[100] 0.413 0.296 0.355 ‚úì 61.68 35.72 74.00 83.40
CDS-MVSNet[80] 0.351 0.278 0.315 ‚úì 61.58 - - -
TransMVSNet[79] 0.321 0.289 0.305 ‚úì 63.52 37.00 - -
GBi-Net[78] 0.315 0.262 0.289 ‚úì 61.42 37.32 - -
Coarse-to-fine UniMVSNet[114] 0.352 0.278 0.315 ‚úì 64.36 38.96 - -
NP-CVP-MVSNet[185] 0.356 0.275 0.315 ‚úì 59.64 - - -
MVSTER[99] 0.340 0.266 0.303 ‚úì 60.92 37.53 72.06 79.01
PVSNet[63] 0.337 0.315 0.326 ‚úì 59.11 35.51 76.57 82.62
MVSFormer[82] 0.327 0.251 0.289 ‚úì 66.37 40.87 - -
IS-MVSNet[186] 0.351 0.359 0.355 ‚úì 62.82 34.87 73.33 83.15
HR-MVSNet[187] 0.332 0.310 0.321 ‚úì 63.12 34.27 - -
EPNet[121] 0.299 0.323 0.313 ‚úì 63.68 40.52 79.08 83.72
GeoMVSNet[76] 0.331 0.259 0.295 ‚úì 65.89 41.52 - -
RA-MVSNet[188] 0.326 0.268 0.297 ‚úì 65.72 39.93 - -
DMVSNet[115] 0.338 0.272 0.305 ‚úì 64.66 41.17 - -
ET-MVSNet[86] 0.329 0.253 0.291 ‚úì 65.49 40.41 - -
PatchmatchNet[40] 0.427 0.277 0.352 ‚úó 53.15 32.31 64.21 73.12
PatchMatch-RL[103] - - - ‚úì 51.80 31.80 67.80 72.40
IterMVS[41] 0.373 0.354 0.363 ‚úì 56.94 34.17 71.69 80.09
Iterativeupdate
Effi-MVS[102] 0.321 0.313 0.317 ‚úó 56.88 34.39 - -
CER-MVS[101] 0.359 0.305 0.332 ‚úì 64.82 40.19 - -
IGEV-MVS[112] 0.331 0.326 0.324 - - - - -
resolution input and the simplicity of network structures, [102] further get rid of costly 3D convolution and use 2D
online MVS methods [25], [31], [35] usually achieve high convolution instead. It can be found many methods [40],
efficiency in both memory and run-time. In contrast, offline [41], [99], [102] can achieve near real-time estimation (about
MVS methods focus on high-resolution images and usually 10Hz)withrelativelyhighresolutionimages,e.g.,1152√ó864.
use computationally expensive network modules, e.g., 3D In conclusion, it is promising to further improve both the
convolutions,toimprovereconstructionquality.Theseincrease performance and efficiency for practical applications.
run-time and GPU memory. Recently, many researchers try to
improve the efficiency while maintaining the reconstruction
C. Unsupervised MVS with Depth Estimation
accuracy as other offline MVS methods. We compare and
summarize the efficiency of existing efficient methods [32], We summarize the results of unsupervised MVS methods
[39]‚Äì[41], [99], [102], [117] in Fig. 13. All the methods inTab.III.Basedontheconventionalphotometricconsistency
share similar coarse-to-fine structures to improve efficiency. loss (Sec. III-A), unsupervised MVS methods may have diffi-
To further reduce run-time and memory, [40], [41], [99], cultieswithmanychallengingsituations,e.g.,varyinglighting
conditions or occlusions, and suffer from downgraded recon-18
TABLEV
QUANTITATIVERESULTSOFVOXEL-BASEDMVSMETHODSON
0.6 SCANNET[26].
FastMVSNet
CasMVSNet
Methods Prec.‚Üë Recall‚Üë F-score‚Üë
0.5
UCSNet
Atlas[47] 0.675 0.605 0.636
PatchmatchNet
0.4 Effi-MVS NeuralRecon[48] 0.630 0.612 0.619
IterMVS
TransformerFusion[142] 0.728 0.600 0.655
MVSTER
0.3 VoRTX[143] 0.767 0.651 0.703
0.2
0.1
1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0
GPU Mem. (GB)
Fig. 13. GPU memory consumption and run-time of efficient offline meth-
ods [32], [39]‚Äì[41], [99], [102], [117] on DTU dataset [21]. The image
resolutionandthenumberofimagesaresetto1152√ó864and5respectively.
ExperimentsaredoneonaNVIDIA2080TiGPU. Reference NeuSNormal NeuSMesh VolSDFNormal VolSDFMesh
Fig. 14. Reconstruction results (rendered surface normal map and surface
struction results. Therefore, many methods propose to employ mesh)ofNeuS[50]andVolSDF[49]onDTUdataset[21].NeuSandVolSDF
different strategies to improve the robustness, e.g., segmen- applydifferentapproximationofdensitywithSDF.
tation consistency [42], optical flow consistency [44], neural
rendering consistency [43], pseudo-label generation with geo-
incorporating more explicit geometric supervision explicitly
metric filtering [45], [132], multi-iteration training [45], [132]
improves the reconstruction quality. Compared with depth
and featuremetric loss [45]. It is inspiring that KD-MVS [45]
map-based MVS methods [15], [25], the main drawback
alreadyoutperformsitsbackbone,CasMVSNet[32],andmany
of NeRF-based methods is that they are mostly per-scene
other fully supervised methods. However, since unsupervised
optimizationproblemsandneedlotsoftimetotrainthemodel
methods mainly test on relatively simple scenes [21], [22],
on each new scene. Recently, some methods [161]‚Äì[163] try
their scalability to more complex and large-scale scenes,
to make the pipeline generalizable so that there is no need to
e.g., ETH3D [23], remains a question. This good generaliza-
train the model for new scenes. However, they perform worse
tion capability across various scenes and datasets is crucial
than depth map-based methods [15]. Another way to improve
for practical applications. Therefore, evaluating unsupervised
training time efficiency is to employ hash grids [146], which
MVS methods on various scenes is an important task for
also improves surface details. However, these methods [157]‚Äì
future research. Most unsupervised methods use simple MVS
[160] need lots of GPU memory and space to train and store
backbones [15], [32], [75] and mainly focus on the training
the large model. In addition, the scalability of NeRF-based
strategy. Employing state-of-the-art architectures is also a
methodsremainsaproblemwhenattemptingtoapplyonlarge-
potential direction that can further boost the performance.
scale scenes [23].
3) 3D Gaussian Splatting-based Methods: Because of its
D. Learning-based MVS without Depth Estimation highly efficient rasterization, 3D Gaussian Splatting [170] be-
1) Voxel-based Methods: We summarize the quantitative comes popular in novel view synthesis and gradually replaces
results on ScanNet [26] in Tab. V. Since the voxel repre- NeRF [28] in the last half year. Recently, some researchers
sentation increases the computation overhead explicitly when start exploring the potential of 3D Gaussian Splatting in
the scale of scene increases, voxel-based methods mainly surface reconstruction. However, there are many challenges.
focus on indoor scenes, e.g., ScanNet [26]. We find that For example, the 3D Gaussians do not correspond well to
depth map-based MVS methods, e.g., SimpleRecon [35], fuse the actual surface of the scene since the 3D Gaussian has
depthmapswithtraditionalTSDFfusion[64]andcanachieve a 3D covariance matrix [52]. Moreover, the surface may
comparable performance as these voxel-based methods with contain noisy undulations [52]. However, it is worthwhile to
lower complexity. This further shows the advantages of depth further explore in this direction since 3D Gaussian Splatting
map-based methods over voxel-based methods. is much faster to train than those NeRF-based MVS methods.
2) NeRF-based Methods: The quantitative results of nerf- Moreover, instead of directly extracting the surface from 3D
based methods on DTU dataset [21] are summarized in space [52], rendering accurate depth maps and then fuse them
Tab. VI. We visualize the qualitative reconstructions of two like depth map-based MVS methods is a potential direction.
representative methods, NeuS [50] and VolSDF [49], in 4) Large Feed-forward Methods: As a new trend of 3D
Fig. 14. The most outstanding features of NeuS [50] and reconstruction, large feed-forward methods demonstrate their
VolSDF [49] are that they are self-supervised, i.e., without impressiveperformanceinchallengingsettingsthatotherMVS
depth supervision, and produce smooth and complete mesh methods cannot handle, e.g., single image input and un-posed
surfaces.However,manyrecentmethods[154]‚Äì[156]findthat sparse images. Employing a large model that is able to get
)s(
emit-nuR19
TABLEVI
QUANTITATIVERESULTSOFNERF-BASEDMVSMETHODSONDTU[21].
Methods VolSDF[49] NeuS[50] NeuralWarp[133] HF-NeuS[190] RegSDF[191] PET-NeuS[192]
Overall‚Üì 0.86 0.87 0.68 0.77 0.72 0.71
Methods GeoNeuS[156] Voxurf[193] NeuS2[157] PermutoSDF[159] Neuralangelo[158] UniSDF[160]
Overall‚Üì 0.51 0.72 0.70 0.68 0.61 0.64
scaled up, these methods manage to learn strong priors of cloud. BlendedMVS [24] introduces more large-scale scenes
multi-view geometry from large datasets [30]. However, these andimprovestheperformanceofmanyMVSmethodsonreal-
methods[53],[54],[179]facelimitationswhenitcomestoex- worldscenes,showninTab.IV.ArKitScenes[197]consistsof
tendingthescaleofthescene.Forexample,the3Drepresenta- 5,048 RGB-D sequences, which is more than three times the
tionemployedinLRM[53]isatri-planerepresentation[180], size of the current largest available indoor dataset, ScanNet.
which has a small resolution and struggles with scaling to As an extension of ScanNet, ScanNet++ [198] is a large-scale
larger sizes due to memory constraints. In addition, the tri- datasetthatcaptureshigh-qualitygeometryandcolorofindoor
planerepresentationisdifficulttomodelmorecomplexgeom- scenes with high-end laser scanner, DSLR camera, and RGB-
etry, e.g., with strong self-occlusions. In addition to tri-plane- D streams from an iPhone. Motivated by ImageNet [199] that
based3Drepresentations,newmethods[194],[195]havebeen drives a remarkable trend of learning from large-scale data
proposed that directly reconstruct 3D Gaussians in a feed- in 2D visual tasks, Yu et al. [200] propose MVImgNet, a
forward manner. These new methods demonstrate improved large-scaledatasetofmulti-viewimagescollectedbyshooting
capabilities in reconstructing both objects and scenes, but re- videos of real-world objects. MVImgNet demonstrates the
quireposedsparseimagesasinputs.Itisworthnotingthatthe potential of various 3D visual tasks, including radiance field
generalization ability of these methods is questionable since reconstruction, multi-view stereo, and view-consistent image
they learn priors of multi-view geometry completely from the understanding. Similarly, Objaverse [30] is a large dataset of
training dataset, while the depth map-based methods utilize objectswith800K+(andgrowing)3Dmodelswithdescriptive
feature matching from plane sweep algorithm from traditional captions, tags, and animations. Both MVImgNet [199] and
methods. Moreover, these methods typically need high-end Objaverse[30]canbeusedtotrainlargefeed-forwardmodels,
GPUs since they are usually GPU memory consuming. It is e.g., LRM [53], for reconstruction, since these models need
worth noting that, since large feed-forward methods usually massive data to learn the strong prior. However, both of them
do not enforce geometric constraint explicitly, e.g., epipolar mainly focus on object-level scenes. Large datasets for larger-
geometry, the reconstructed geometry is by default placed in scale scenes, e.g., rooms, are valuable to be explored.
a local coordinate system without absolute scales, making it
For evaluation, the main benchmarks [21]‚Äì[23], [26] are
not feasible for applications requiring precise measurement.
all introduced before 2018. Some of these benchmarks, e.g.,
[21], [22], are currently saturated and the learning-based
E. Future Research Directions methods are difficult to further improve the performance on
them. Therefore, it is meaningful to introduce new bench-
1) Datasets and Benchmarks: For learning-based MVS
markstoevaluatetherobustnessandperformanceoflearning-
methods, ScanNet [26] and DTU [21] are two main training
basedmethodsinchallengingreal-worldscenes.Forexample,
datasets,whileScanNet[26],DTU[21],Tanks&Temples[22]
LaMAR [201] is a recent large-scale dataset captured using
and ETH3D [23] are the main evaluation benchmarks.
multiple modern AR devices in diverse environments. It con-
For training, the scene scale of ScanNet and DTU is
tainschallengingshort-termappearanceandstructuralchanges
relatively small (room-scale for ScanNet and object-scale for
andhighqualityLiDARpointcloudgroundtruth.Thesemake
DTU) and their quality is not satisfactory. For example, the
LaMAR a potential benchmark for MVS.
camera calibration of ScanNet is not very accurate. For DTU,
the ground truth depth maps are rendered from mesh, which 2) ViewSelection: AsdiscussedinSec.I-B,viewselection
contain some outliers and holes. This is because the mesh is crucial for triangulation quality. Picking neighboring views
is reconstructed from the sparse ground truth point cloud that are suitable for triangulation can not only improve the
with Screened Poisson surface reconstruction [67] and thus reconstruction accuracy but also reduce useless computation
contains incomplete regions [184]. Therefore, improving the forthebadviews,e.g.,viewswithstrongocclusions.However,
scalability and quality of training datasets is an important view selection is often overlooked and not well studied. For
research direction. example,allofflinedepthmap-basedMVSmethods[32],[40],
Recently, there are many researchers trying to solve this [79], [99] follow MVSNet [15] and use the same simple
problem. TartanAir dataset [196] is a large-scale synthetic heuristic strategy to compute scores for neighboring views
dataset that is collected in photo-realistic simulation environ- and sort them. Online depth map-based MVS methods [31],
ments.Precisemulti-modalsensordataandgroundtruthlabels [35], [58] and voxel-based methods [48] also adopt heuristic
are provided, including the stereo RGB image, depth image, strategiestochooseviewswithenoughpose-distance.Though
segmentation, optical flow, camera poses, and LiDAR point itisintractabletoincorporatenon-differentiableviewselection20
into deep learning, it is worth exploring new view selection achieve real-time estimation with images of low resolutions,
strategies since it may improve the reconstruction without they may have issues with memory consumption since many
changing the model design of existing methods. Current view large backbones are usually used for feature extraction [31],
selection strategies simply pick the same set of neighboring [35]. In addition, the efficiency in both run-time and memory
views for all the pixels in the reference image, and it is will drop when image resolution increases. For offline MVS
probable that different reference pixels have different optimal methods, as discussed in Sec. V-B2, some recent methods
choices of neighboring views. If it is possible to choose the try to improve efficiency with carefully simplified network
best neighboring views for each pixel, reconstruction quality architectures. However, this usually limits the performance
canbefurtherimprovedandcomputationreduced.Thelearned whencomparedwithstate-of-the-artmethods.Therearemany
pixel-wiseviewweight[40],[41],[63]isonesolutionforthis other directions for further improvement. For example, using
since it weights the source views differently for each pixel model compression techniques and knowledge distillation are
based on its visibility across neighboring views. potentialdirectionstonotonlykeepthegoodperformanceand
3) Depth Fusion: As discussed in Sec. I-D, both online also improve efficiency.
and offline MVS methods with plane-sweep use traditional 6) Prior Assistance: MVS mainly relies on measurements
TSDF fusion or depth filtering methods to reconstruct mesh of local photometric consistency to find the optimal matching
or point cloud from the estimated depth maps. However, across reference and source images. Accordingly, it usually
TSDF fusion [64], [65] is not robust enough to the outliers encounters difficulties when estimating the geometry for re-
in the depth maps and may have memory issues due to gionswherethephotometricmeasurementbecomesunreliable
the dense volumetric representation. Depth filtering following or invalid, e.g., textureless areas and non-Lambertian sur-
Galliani et al. [18] introduces lots of hyper-parameters. Re- faces, which are common in human-made environments, e.g.,
searchers may carefully finetune these hyper-parameters for ETH3D[23].Therefore,leveragingpriorinformationtoguide
each scene since it may have a great impact on the evaluation theMVSalgorithminthesechallengingregionsisapromising
metrics. Therefore, it is valuable to improve the depth fusion research direction. We elaborate several typical examples of
stepsothatthereconstructionqualitycanbefurtherimproved. prior assistance as follows.
Recently, there have been some learning-based depth map ‚Ä¢ Surface Normal: It is a non-local representation of the
fusion methods [202], [203]. However, they are limited to geometry compared with the depth map. To enforce
smallscenesduetothedensevolumerepresentationandlarge the constraints of normal maps in depth estimation,
memory consumption. Kusupati et al. [206] integrate the multi-view normal
4) Features: One not well-studied topic is what kind estimation network into the MVS pipeline. Long et
of feature extractors are suitable for MVS, as mentioned al. [207] introduce a Combined Normal Map, estimated
in Sec. II-A. So far, most of the offline depth map-based with PlaneCNN [208] to enforce the normal consistency
methods [15], [32], [78] mainly apply simple 2D CNN and on the depth map. Liu et al. [209] perform graph-based
FPN [72] structure as a feature extractor. To improve the depthmapoptimizationaspost-processing.Sincenormal
receptive fields flexibly, Deformation Convolution [204] is monocular normal predictions [210] provide high-quality
used [40],[77]‚Äì[79]).Theattentionmechanismisalsoapplied priors for the full scene, [154], [155] use monocular
in feature learning, where works such as [77], [79], [90] use normal to improve the optimization of neural implicit
intra-attention or inter-attention to capture long-range feature surfaces.
dependencies.Furthermore,MVSFormer[82]digsdeeperinto ‚Ä¢ Shape Prior: For indoor scenes where common tex-
usingpatch-wiseViT[181]asafeatureextractorinMVS.The tureless areas, e.g., walls, planes are suitable choices
insight is that ViT works better to formulate global feature of the geometric primitives and are exploited in tra-
correlations, and FPN can learn detailed ones. In contrast, ditional methods [20]. Recently, PlaneMVS [211] and
online depth map-based methods [31], [35] usually adopt PlanarRecon [212] explicitly estimate plane parameters
efficient pretrained backbones [70], [73], [74]. Recently, large for depth refinement or holistic reconstruction. In ad-
feed-forward methods [53], [54], [175] adopt large pretrained dition, predicting object-level attributes simultaneously
feature encoders, e.g., ViT [181], DINO [83], to learn strong when estimating depth [213] is also a feasible solution
prior from large-amount of data. However, using a powerful for specific applications like urban modeling.
feature extraction network usually increases the computation ‚Ä¢ Semantic Segmentation: Intuitively, points assigned
overhead and reduces the efficiency. Finding a good balance with the same semantic labels may be more likely to
betweenperformanceandefficiencyisaninterestingdirection, lie on the same 3D plane. Some attempts [214], [215]
e.g., distilling the large ViT into a relatively small trans- employarule-basedprotocoltoutilizesemanticsegmen-
former [205]. tation for better matching and depth quality. Manhattan-
5) Real-timeandMemoryEfficientModels: Thoughrecon- SDF[216]adoptsManhattanworldpriorstohandlelow-
struction accuracy is considered the most important factor in textured planar regions corresponding to walls and floors
MVS, it is also critical to have models that can run in near with 2D segmentation.
real-timeandwithlowmemory,e.g.,AR/VRandautonomous
driving. Comparatively, depth map-based methods are most VI. CONCLUSION
suitable for achieving efficiency because of their conciseness, In this survey, we have provided a comprehensive re-
flexibility, and scalability. Though online MVS methods can view of the learning-based MVS methods until 2023, which21
are categorized into: depth map-based, voxel-based, NeRF- [23] T. Scho¬®ps, J. L. Scho¬®nberger, S. Galliani, T. Sattler, K. Schindler,
based, 3D Gaussian Splatting-based and large feed-forward M. Pollefeys, and A. Geiger, ‚ÄúA multi-view stereo benchmark with
high-resolutionimagesandmulti-cameravideos,‚ÄùinCVPR,2017.
methods. Particularly, we have devoted significant attention
[24] Y.Yao,Z.Luo,S.Li,J.Zhang,Y.Ren,L.Zhou,T.Fang,andL.Quan,
to depth map-based methods because of their conciseness, ‚ÄúBlendedmvs: A large-scale dataset for generalized multi-view stereo
flexibility and scalability. We explain key aspects such as networks,‚ÄùinCVPR,2020.
[25] K. Wang and S. Shen, ‚ÄúMvdepthnet: Real-time multiview depth esti-
datasets utilized, general working pipelines, and algorithmic
mationneuralnetwork,‚Äùin3DV,2018.
intricacies. Furthermore, we have summarized and compared [26] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
the quantitative performance of these methods across popular M. Nie√üner, ‚ÄúScannet: Richly-annotated 3d reconstructions of indoor
scenes,‚ÄùinCVPR,2017.
benchmarks, providing valuable insights into their efficacy
[27] B. Glocker, S. Izadi, J. Shotton, and A. Criminisi, ‚ÄúReal-time rgb-
and applicability. Finally, our discourse extends to an in- d camera relocalization,‚Äù in 2013 IEEE International Symposium on
depthexplorationofpotentialresearchdirectionsforthefuture MixedandAugmentedReality(ISMAR). IEEE,2013.
[28] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor-
of MVS, highlighting avenues for further investigation and
thi, and R. Ng, ‚ÄúNerf: Representing scenes as neural radiance fields
innovation in the field. forviewsynthesis,‚ÄùinECCV,2020.
[29] B.Kerbl,G.Kopanas,T.Leimku¬®hler,andG.Drettakis,‚Äú3dgaussian
splattingforreal-timeradiancefieldrendering,‚ÄùACMTransactionson
REFERENCES Graphics,vol.42,no.4,2023.
[30] M.Deitke,D.Schwenk,J.Salvador,L.Weihs,O.Michel,E.Vander-
[1] Y. Furukawa and C. Herna¬¥ndez, ‚ÄúMulti-view stereo: A tutorial,‚Äù Bilt,L.Schmidt,K.Ehsani,A.Kembhavi,andA.Farhadi,‚ÄúObjaverse:
Foundations and Trends¬Æ in Computer Graphics and Vision, vol. 9, Auniverseofannotated3dobjects,‚ÄùinCVPR,2023.
no.1-2,2015. [31] A. Duzceker, S. Galliani, C. Vogel, P. Speciale, M. Dusmanu, and
M.Pollefeys,‚ÄúDeepvideomvs:Multi-viewstereoonvideowithrecur-
[2] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, ‚ÄúA
rentspatio-temporalfusion,‚ÄùinCVPR,2021.
benchmarkfortheevaluationofrgb-dslamsystems,‚ÄùinIROS,2012.
[32] X.Gu,Z.Fan,S.Zhu,Z.Dai,F.Tan,andP.Tan,‚ÄúCascadecostvolume
[3] F.Endres,J.Hess,J.Sturm,D.Cremers,andW.Burgard,‚Äú3-dmapping
forhigh-resolutionmulti-viewstereoandstereomatching,‚ÄùinCVPR,
withanrgb-dcamera,‚ÄùIEEEtransactionsonrobotics,vol.30,no.1,
2020.
pp.177‚Äì187,2013.
[33] R.T.Collins,‚ÄúAspace-sweepapproachtotruemulti-imagematching,‚Äù
[4] T. Schops, T. Sattler, and M. Pollefeys, ‚ÄúBad slam: Bundle adjusted
inCVPR,1996.
directrgb-dslam,‚ÄùinCVPR,2019.
[34] J.L.Scho¬®nberger,E.Zheng,J.-M.Frahm,andM.Pollefeys,‚ÄúPixelwise
[5] Y.Pan,P.Xiao,Y.He,Z.Shao,andZ.Li,‚ÄúMulls:Versatilelidarslam
viewselectionforunstructuredmulti-viewstereo,‚ÄùinECCV,2016.
viamulti-metriclinearleastsquare,‚ÄùinICRA,2021.
[35] M. Sayed, J. Gibson, J. Watson, V. Prisacariu, M. Firman, and
[6] S.Izadi,D.Kim,O.Hilliges,D.Molyneaux,R.Newcombe,P.Kohli,
C.Godard,‚ÄúSimplerecon:3dreconstructionwithout3dconvolutions,‚Äù
J.Shotton,S.Hodges,D.Freeman,A.Davisonetal.,‚ÄúKinectfusion:
inECCV. Springer,2022,pp.1‚Äì19.
real-time 3d reconstruction and interaction using a moving depth
[36] Q.XuandW.Tao,‚ÄúLearninginversedepthregressionformulti-view
camera,‚ÄùinProceedingsofthe24thannualACMsymposiumonUser
stereowithcorrelationcostvolume,‚ÄùinAAAI,2020.
interfacesoftwareandtechnology,2011.
[37] Y.Yao,Z.Luo,S.Li,T.Shen,T.Fang,andL.Quan,‚ÄúRecurrentmvsnet
[7] M.Zollho¬®fer,M.Nie√üner,S.Izadi,C.Rehmann,C.Zach,M.Fisher,
forhigh-resolutionmulti-viewstereodepthinference,‚ÄùinCVPR,2019.
C.Wu,A.Fitzgibbon,C.Loop,C.Theobaltetal.,‚ÄúReal-timenon-rigid
[38] J.Yan,Z.Wei,H.Yi,M.Ding,R.Zhang,Y.Chen,G.Wang,andY.-
reconstructionusinganrgb-dcamera,‚ÄùACMTransactionsonGraphics
W. Tai, ‚ÄúDense hybrid recurrent multi-view stereo net with dynamic
(ToG),vol.33,no.4,2014.
consistencychecking,‚ÄùinECCV,2020.
[8] R. A. Newcombe, D. Fox, and S. M. Seitz, ‚ÄúDynamicfusion: Recon-
[39] S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, and
structionandtrackingofnon-rigidscenesinreal-time,‚ÄùinCVPR,2015.
H. Su, ‚ÄúDeep stereo using adaptive thin volume representation with
[9] S. M. Seitz and C. R. Dyer, ‚ÄúPhotorealistic scene reconstruction by
uncertaintyawareness,‚ÄùinCVPR,2020.
voxelcoloring,‚ÄùIJCV,vol.35,1999.
[40] F.Wang,S.Galliani,C.Vogel,P.Speciale,andM.Pollefeys,‚ÄúPatch-
[10] K.N.KutulakosandS.M.Seitz,‚ÄúAtheoryofshapebyspacecarving,‚Äù
matchnet:Learnedmulti-viewpatchmatchstereo,‚ÄùinCVPR,2021.
IJCV,vol.38,2000.
[41] F. Wang, S. Galliani, C. Vogel, and M. Pollefeys, ‚ÄúItermvs: Iterative
[11] I.Kostrikov,E.Horbert,andB.Leibe,‚ÄúProbabilisticlabelingcostfor
probabilityestimationforefficientmulti-viewstereo,‚Äù2022.
high-accuracymulti-viewreconstruction,‚ÄùinCVPR,2014.
[42] H.Xu,Z.Zhou,Y.Qiao,W.Kang,andQ.Wu,‚ÄúSelf-supervisedmulti-
[12] A.O.Ulusoy,M.J.Black,andA.Geiger,‚ÄúSemanticmulti-viewstereo: view stereo via effective co-segmentation and data-augmentation,‚Äù in
Jointlyestimatingobjectsandvoxels,‚ÄùinCVPR,2017. AAAI,2021.
[13] M. Lhuillier and L. Quan, ‚ÄúA quasi-dense approach to surface re- [43] D. Chang, A. BozÀáicÀá, T. Zhang, Q. Yan, Y. Chen, S. Su¬®sstrunk, and
constructionfromuncalibratedimages,‚ÄùIEEEtransactionsonpattern M. Nie√üner, ‚ÄúRc-mvsnet: unsupervised multi-view stereo with neural
analysisandmachineintelligence,vol.27,no.3,2005. rendering,‚ÄùinECCV,2022.
[14] Y. Furukawa and J. Ponce, ‚ÄúAccurate, dense, and robust multiview [44] H. Xu, Z. Zhou, Y. Wang, W. Kang, B. Sun, H. Li, and Y. Qiao,
stereopsis,‚ÄùIEEEtransactionsonpatternanalysisandmachineintel- ‚ÄúDigging into uncertainty in self-supervised multi-view stereo,‚Äù in
ligence,vol.32,no.8,2009. ICCV,2021.
[15] Y.Yao,Z.Luo,S.Li,T.Fang,andL.Quan,‚ÄúMvsnet:Depthinference [45] Y.Ding,Q.Zhu,X.Liu,W.Yuan,H.Zhang,andC.Zhang,‚ÄúKd-mvs:
forunstructuredmulti-viewstereo,‚ÄùinECCV,2018. Knowledge distillation based self-supervised learning for multi-view
[16] R.YangandM.Pollefeys,‚ÄúMulti-resolutionreal-timestereooncom- stereo,‚ÄùinECCV,2022.
moditygraphicshardware,‚ÄùinCVPR,2003. [46] T.Kim,J.Choi,S.Choi,D.Jung,andC.Kim,‚ÄúJustafewpointsare
[17] J.L.Scho¬®nbergerandJ.-M.Frahm,‚ÄúStructure-from-motionrevisited,‚Äù all you need for multi-view stereo: A novel semi-supervised learning
inCVPR,2016. methodformulti-viewstereo,‚ÄùinICCV,2021.
[18] S.Galliani,K.Lasinger,andK.Schindler,‚ÄúMassivelyparallelmulti- [47] Z. Murez, T. v. As, J. Bartolozzi, A. Sinha, V. Badrinarayanan, and
viewstereopsisbysurfacenormaldiffusion,‚ÄùinICCV,2015. A.Rabinovich,‚ÄúAtlas:End-to-end3dscenereconstructionfromposed
[19] Q. Xu and W. Tao, ‚ÄúMulti-scale geometric consistency guided multi- images,‚ÄùinECCV,2020.
viewstereo,‚ÄùinCVPR,2019. [48] J.Sun,Y.Xie,L.Chen,X.Zhou,andH.Bao,‚ÄúNeuralrecon:Real-time
[20] ‚Äî‚Äî, ‚ÄúPlanar prior assisted patchmatch multi-view stereo,‚Äù in AAAI, coherent3dreconstructionfrommonocularvideo,‚ÄùinCVPR,2021.
2020. [49] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, ‚ÄúVolume rendering of
[21] H.Aan√¶s,R.R.Jensen,G.Vogiatzis,E.Tola,andA.B.Dahl,‚ÄúLarge- neuralimplicitsurfaces,‚ÄùinAdvancesinNeuralInformationProcessing
scale data for multiple-view stereopsis,‚Äù IJCV, vol. 120, no. 2, pp. Systems,2021.
153‚Äì168,2016. [50] P.Wang,L.Liu,Y.Liu,C.Theobalt,T.Komura,andW.Wang,‚ÄúNeus:
[22] A.Knapitsch,J.Park,Q.-Y.Zhou,andV.Koltun,‚ÄúTanksandtemples: Learningneuralimplicitsurfacesbyvolumerenderingformulti-view
Benchmarkinglarge-scalescenereconstruction,‚ÄùACMTransactionson reconstruction,‚ÄùinAdvancesinNeuralInformationProcessingSystems,
Graphics(ToG),vol.36,no.4,pp.1‚Äì13,2017. 2021.22
[51] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, ‚ÄúDreamgaussian: [79] Y. Ding, W. Yuan, Q. Zhu, H. Zhang, X. Liu, Y. Wang, and X. Liu,
Generative gaussian splatting for efficient 3d content creation,‚Äù arXiv ‚ÄúTransmvsnet: Global context-aware multi-view stereo network with
preprintarXiv:2309.16653,2023. transformers,‚ÄùarXivpreprintarXiv:2111.14600,2021.
[52] A.Gue¬¥donandV.Lepetit,‚ÄúSugar:Surface-alignedgaussiansplatting [80] K. T. Giang, S. Song, and S. Jo, ‚ÄúCurvature-guided dynamic scale
forefficient3dmeshreconstructionandhigh-qualitymeshrendering,‚Äù networks for multi-view stereo,‚Äù arXiv preprint arXiv:2112.05999,
inCVPR,2024. 2021.
[53] Y.Hong,K.Zhang,J.Gu,S.Bi,Y.Zhou,D.Liu,F.Liu,K.Sunkavalli, [81] J. Liao, Y. Ding, Y. Shavit, D. Huang, S. Ren, J. Guo, W. Feng,
T.Bui,andH.Tan,‚ÄúLrm:Largereconstructionmodelforsingleimage andK.Zhang,‚ÄúWt-mvsnet:window-basedtransformersformulti-view
to3d,‚ÄùICLR,2024. stereo,‚ÄùAdvancesinNeuralInformationProcessingSystems,2022.
[54] S.Wang,V.Leroy,Y.Cabon,B.Chidlovskii,andJ.Revaud,‚ÄúDust3r: [82] C. Cao, X. Ren, and Y. Fu, ‚ÄúMvsformer: Learning robust image
Geometric3dvisionmadeeasy,‚ÄùinCVPR,2024. representationsviatransformersandtemperature-baseddepthformulti-
[55] R. Hartley and A. Zisserman, Multiple view geometry in computer viewstereo,‚ÄùarXivpreprintarXiv:2208.02541,2022.
vision. Cambridgeuniversitypress,2003. [83] M.Caron,H.Touvron,I.Misra,H.Je¬¥gou,J.Mairal,P.Bojanowski,and
[56] P. Moulon, P. Monasse, R. Perrot, and R. Marlet, ‚ÄúOpenmvg: Open A.Joulin,‚ÄúEmergingpropertiesinself-supervisedvisiontransformers,‚Äù
multiple view geometry,‚Äù in International Workshop on Reproducible inICCV,2021,pp.9650‚Äì9660.
ResearchinPatternRecognition,2016. [84] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and
[57] A.Dai,M.Nie√üner,M.Zollho¬®fer,S.Izadi,andC.Theobalt,‚ÄúBundle- C. Shen, ‚ÄúTwins: Revisiting the design of spatial attention in vision
fusion:Real-timegloballyconsistent3dreconstructionusingon-the-fly transformers,‚Äù Advances in Neural Information Processing Systems,
surfacereintegration,‚ÄùACMTransactionsonGraphics(ToG),vol.36, vol.34,pp.9355‚Äì9366,2021.
no.4,p.1,2017. [85] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
[58] Y. Hou, J. Kannala, and A. Solin, ‚ÄúMulti-view stereo by temporal B. Guo, ‚ÄúSwin transformer: Hierarchical vision transformer using
nonparametricfusion,‚ÄùinICCV,2019. shiftedwindows,‚ÄùinICCV,2021.
[59] R.Zhang,S.Li,T.Fang,S.Zhu,andL.Quan,‚ÄúJointcameraclustering [86] T.Liu,X.Ye,W.Zhao,Z.Pan,M.Shi,andZ.Cao,‚ÄúWhenepipolar
andsurfacesegmentationforlarge-scalemulti-viewstereo,‚ÄùinICCV, constraint meets non-local operators in multi-view stereo,‚Äù in ICCV,
2015. 2023.
[60] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, [87] C. Liu, J. Gu, K. Kim, S. G. Narasimhan, and J. Kautz, ‚ÄúNeural rgb
A.Bachrach,andA.Bry,‚ÄúEnd-to-endlearningofgeometryandcontext
(r)dsensing:Depthanduncertaintyfromavideocamera,‚ÄùinCVPR,
fordeepstereoregression,‚ÄùinICCV,2017. 2019.
[88] G. Bae, I. Budvytis, and R. Cipolla, ‚ÄúMulti-view depth estimation
[61] J.-R. Chang and Y.-S. Chen, ‚ÄúPyramid stereo matching network,‚Äù in
byfusingsingle-viewdepthprobabilitywithmulti-viewgeometry,‚Äùin
CVPR,2018.
CVPR,2022.
[62] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, ‚ÄúGroup-wise
[89] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, ‚ÄúGroup-wise
correlationstereonetwork,‚ÄùinCVPR,2019.
correlationstereonetwork,‚ÄùinCVPR,2019.
[63] Q.Xu,W.Su,Y.Qi,W.Tao,andM.Pollefeys,‚ÄúLearninginversedepth
[90] H.Yi,Z.Wei,M.Ding,R.Zhang,Y.Chen,G.Wang,andY.-W.Tai,
regression for pixelwise visibility-aware multi-view stereo networks,‚Äù
‚ÄúPyramid multi-view stereo net with self-adaptive view aggregation,‚Äù
IJCV,vol.130,no.8,pp.2040‚Äì2059,2022.
inECCV,2020.
[64] B. Curless and M. Levoy, ‚ÄúA volumetric method for building com-
[91] J.Yu,Z.Lin,J.Yang,X.Shen,X.Lu,andT.S.Huang,‚ÄúFree-form
plex models from range images,‚Äù in Proceedings of the 23rd annual
imageinpaintingwithgatedconvolution,‚ÄùinICCV,2019.
conferenceonComputergraphicsandinteractivetechniques,1996.
[92] J.Zhang,Y.Yao,S.Li,Z.Luo,andT.Fang,‚ÄúVisibility-awaremulti-
[65] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim,
viewstereonetwork,‚ÄùBMVC,2020.
A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon,
[93] S.Xingjian,Z.Chen,H.Wang,D.-Y.Yeung,W.-K.Wong,andW.-c.
‚ÄúKinectfusion:Real-timedensesurfacemappingandtracking,‚Äùin2011
Woo,‚ÄúConvolutionallstmnetwork:Amachinelearningapproachfor
10thIEEEinternationalsymposiumonmixedandaugmentedreality,
precipitationnowcasting,‚ÄùinNIPS,2015.
2011.
[94] X.Long,L.Liu,W.Li,C.Theobalt,andW.Wang,‚ÄúMulti-viewdepth
[66] W.E.LorensenandH.E.Cline,‚ÄúMarchingcubes:Ahighresolution
estimation using epipolar spatio-temporal networks,‚Äù in CVPR, 2021,
3d surface construction algorithm,‚Äù in Seminal graphics: pioneering
pp.8258‚Äì8267.
effortsthatshapedthefield,1998,pp.347‚Äì353.
[95] R. Chen, S. Han, J. Xu, and H. Su, ‚ÄúPoint-based multi-view stereo
[67] M.KazhdanandH.Hoppe,‚ÄúScreenedpoissonsurfacereconstruction,‚Äù
network,‚ÄùinICCV,2019.
ACMTransactionsonGraphics(ToG),vol.32,no.3,pp.1‚Äì13,2013.
[96] K.Luo,T.Guan,L.Ju,H.Huang,andY.Luo,‚ÄúP-MVSNet:Learning
[68] D. Eigen, C. Puhrsch, and R. Fergus, ‚ÄúDepth map prediction from a
patch-wisematchingconfidenceaggregationformulti-viewstereo,‚Äùin
single image using a multi-scale deep network,‚Äù Advances in neural
ICCV,2019.
informationprocessingsystems,2014.
[97] S.Im,H.-G.Jeon,S.Lin,andI.S.Kweon,‚ÄúDpsnet:End-to-enddeep
[69] A. Sinha, Z. Murez, J. Bartolozzi, V. Badrinarayanan, and A. Ra- planesweepstereo,‚ÄùinICLR,2018.
binovich, ‚ÄúDeltas: Depth estimation by learning triangulation and [98] K. Cho, B. V. Merrie¬®nboer, C. Gulcehre, D. Bahdanau, F. Bougares,
densificationofsparsepoints,‚ÄùinECCV,2020.
H. Schwenk, and Y. Bengio, ‚ÄúLearning phrase representations us-
[70] K.He,X.Zhang,S.Ren,andJ.Sun,‚ÄúDeepresiduallearningforimage ing RNN encoder-decoder for statistical machine translation,‚Äù arXiv
recognition,‚ÄùinCVPR,2016. preprintarXiv:1406.1078,2014.
[71] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional net- [99] X. Wang, Z. Zhu, G. Huang, F. Qin, Y. Ye, Y. He, X. Chi, and
worksforbiomedicalimagesegmentation,‚ÄùinInternationalConference X.Wang,‚ÄúMvster:epipolartransformerforefficientmulti-viewstereo,‚Äù
onMedicalimagecomputingandcomputer-assistedintervention,2015. inECCV. Springer,2022,pp.573‚Äì591.
[72] T.-Y. Lin, P. Dolla¬¥r, R. B. Girshick, K. He, B. Hariharan, and S. J. [100] X.Ma,Y.Gong,Q.Wang,J.Huang,L.Chen,andF.Yu,‚ÄúEpp-mvsnet:
Belongie, ‚ÄúFeature pyramid networks for object detection,‚Äù CVPR, Epipolar-assemblingbaseddepthpredictionformulti-viewstereo,‚Äùin
2017. ICCV,2021.
[73] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, [101] Z.Ma,Z.Teed,andJ.Deng,‚ÄúMultiviewstereowithcascadedepipolar
andQ.V.Le,‚ÄúMnasnet:Platform-awareneuralarchitecturesearchfor raft,‚ÄùarXivpreprintarXiv:2205.04502,2022.
mobile,‚ÄùinCVPR,2019,pp.2820‚Äì2828. [102] S. Wang, B. Li, and Y. Dai, ‚ÄúEfficient multi-view stereo by iterative
[74] M.TanandQ.Le,‚ÄúEfficientnetv2:Smallermodelsandfastertraining,‚Äù dynamiccostvolume,‚ÄùinCVPR,2022,pp.8655‚Äì8664.
inICML. PMLR,2021,pp.10096‚Äì10106. [103] J. Y. Lee, J. DeGol, C. Zou, and D. Hoiem, ‚ÄúPatchmatch-rl: Deep
[75] J. Yang, W. Mao, J. M. Alvarez, and M. Liu, ‚ÄúCost volume pyramid mvswithpixelwisedepth,normal,andvisibility,‚ÄùinICCV,2021,pp.
baseddepthinferenceformulti-viewstereo,‚ÄùinCVPR,2020. 6158‚Äì6167.
[76] Z.Zhang,R.Peng,Y.Hu,andR.Wang,‚ÄúGeomvsnet:Learningmulti- [104] C.Barnes,E.Shechtman,A.Finkelstein,andD.B.Goldman,‚ÄúPatch-
viewstereowithgeometryperception,‚ÄùinCVPR(CVPR),June2023, match: A randomized correspondence algorithm for structural image
pp.21508‚Äì21518. editing,‚ÄùACMTrans.Graph.,vol.28,no.3,2009.
[77] Z.Wei,Q.Zhu,C.Min,Y.Chen,andG.Wang,‚ÄúAa-rmvsnet:Adaptive [105] M. Bleyer, C. Rhemann, and C. Rother, ‚ÄúPatchmatch stereo - stereo
aggregationrecurrentmulti-viewstereonetwork,‚ÄùinICCV,2021. matchingwithslantedsupportwindows,‚ÄùinBMVC,2011.
[78] Z.Mi,D.Chang,andD.Xu,‚ÄúGeneralizedbinarysearchnetworkfor [106] Z. Teed and J. Deng, ‚ÄúRaft: Recurrent all-pairs field transforms for
highly-efficientmulti-viewstereo,‚ÄùinCVPR,2022. opticalflow,‚ÄùinECCV,2020.23
[107] L.Lipson,Z.Teed,andJ.Deng,‚ÄúRaft-stereo:Multilevelrecurrentfield [136] K.Xiong,R.Peng,Z.Zhang,T.Feng,J.Jiao,F.Gao,andR.Wang,
transformsforstereomatching,‚Äùin2021InternationalConferenceon ‚ÄúCl-mvsnet:Unsupervisedmulti-viewstereowithdual-levelcontrastive
3DVision(3DV). IEEE,2021,pp.218‚Äì227. learning,‚ÄùinICCV,2023.
[108] Z.TeedandJ.Deng,‚ÄúRaft-3d:Sceneflowusingrigid-motionembed- [137] F.Yang,Q.Sun,H.Jin,andZ.Zhou,‚ÄúSuperpixelsegmentationwith
dings,‚ÄùinCVPR,2021,pp.8375‚Äì8384. fullyconvolutionalnetworks,‚ÄùinCVPR,2020.
[109] X. Gu, W. Yuan, Z. Dai, C. Tang, S. Zhu, and P. Tan, ‚ÄúDro: [138] D.Sun,X.Yang,M.-Y.Liu,andJ.Kautz,‚ÄúPwc-net:Cnnsforoptical
Deep recurrent optimizer for structure-from-motion,‚Äù arXiv preprint flowusingpyramid,warping,andcostvolume,‚ÄùinCVPR,2018.
arXiv:2103.13201,2021. [139] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a
[110] C. Sormann, E. Santellani, M. Rossi, A. Kuhn, and F. Fraundorfer, neuralnetwork,‚ÄùarXivpreprintarXiv:1503.02531,2015.
‚ÄúDels-mvs:Deepepipolarlinesearchformulti-viewstereo,‚ÄùinWACV, [140] T. Furlanello, Z. Lipton, M. Tschannen, L. Itti, and A. Anandkumar,
2023. ‚ÄúBornagainneuralnetworks,‚ÄùinICML,2018.
[111] C. Cai, P. Ji, Q. Yan, and Y. Xu, ‚ÄúRiav-mvs: Recurrent-indexing an
[141] Q.Xie,M.-T.Luong,E.Hovy,andQ.V.Le,‚ÄúSelf-trainingwithnoisy
asymmetricvolumeformulti-viewstereo,‚ÄùinCVPR,2023.
studentimprovesimagenetclassification,‚ÄùinCVPR,2020.
[112] G.Xu,X.Wang,X.Ding,andX.Yang,‚ÄúIterativegeometryencoding
[142] A. Bozic, P. Palafox, J. Thies, A. Dai, and M. Nie√üner, ‚ÄúTrans-
volumeforstereomatching,‚ÄùinCVPR,2023,pp.21919‚Äì21928.
formerfusion:Monocularrgbscenereconstructionusingtransformers,‚Äù
[113] P. Knobelreiter, C. Sormann, A. Shekhovtsov, F. Fraundorfer, and
AdvancesinNeuralInformationProcessingSystems,vol.34,2021.
T.Pock,‚ÄúBeliefpropagationreloaded:Learningbp-layersforlabeling
[143] N. Stier, A. Rich, P. Sen, and T. Ho¬®llerer, ‚ÄúVortx: Volumetric 3d
problems,‚ÄùinCVPR,2020.
reconstruction with transformers for voxelwise view selection and
[114] R.Peng,R.Wang,Z.Wang,Y.Lai,andR.Wang,‚ÄúRethinkingdepth
fusion,‚Äùin3DV,2021.
estimation for multi-view stereo: A unified representation,‚Äù in CVPR,
[144] C. Sun, M. Sun, and H.-T. Chen, ‚ÄúDirect voxel grid optimization:
2022,pp.8645‚Äì8654.
Super-fast convergence for radiance fields reconstruction,‚Äù in CVPR,
[115] X. Ye, W. Zhao, T. Liu, Z. Huang, Z. Cao, and X. Li, ‚ÄúConstraining
2022.
depthmapgeometryformulti-viewstereo:Adual-depthapproachwith
saddle-shapeddepthcells,‚ÄùinICCV,2023. [145] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, ‚ÄúTensorf: Tensorial
[116] R. Chen, S. Han, J. Xu, and H. Su, ‚ÄúVisibility-aware point-based radiancefields,‚ÄùinECCV,2022.
multi-viewstereonetwork,‚ÄùIEEEtransactionsonpatternanalysisand [146] T.Mu¬®ller,A.Evans,C.Schied,andA.Keller,‚ÄúInstantneuralgraphics
machineintelligence,vol.43,no.10,2020. primitives with a multiresolution hash encoding,‚Äù ACM Transactions
[117] Z. Yu and S. Gao, ‚ÄúFast-mvsnet: Sparse-to-dense multi-view stereo onGraphics(ToG),vol.41,no.4,2022.
with learned propagation and gauss-newton refinement,‚Äù in CVPR, [147] J.T.Barron,B.Mildenhall,D.Verbin,P.P.Srinivasan,andP.Hedman,
2020. ‚ÄúMip-nerf 360: Unbounded anti-aliased neural radiance fields,‚Äù in
[118] T.-W. Hui, C. C. Loy, and X. Tang, ‚ÄúDepth map super-resolution by CVPR,2022.
deepmulti-scaleguidance,‚ÄùinECCV. Springer,2016,pp.353‚Äì369. [148] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and
[119] J. Xi, Y. Shi, Y. Wang, Y. Guo, and K. Xu, ‚ÄúRaymvsnet: Learning A. Kanazawa, ‚ÄúPlenoxels: Radiance fields without neural networks,‚Äù
ray-based1dimplicitfieldsforaccuratemulti-viewstereo,‚ÄùinCVPR, inCVPR,2022.
2022,pp.8595‚Äì8605. [149] Z.Chen,T.Funkhouser,P.Hedman,andA.Tagliasacchi,‚ÄúMobilenerf:
[120] Y. Shi, J. Xi, D. Hu, Z. Cai, and K. Xu, ‚ÄúRaymvsnet++: learning Exploiting the polygon rasterization pipeline for efficient neural field
ray-based 1d implicit fields for accurate multi-view stereo,‚Äù IEEE renderingonmobilearchitectures,‚ÄùinCVPR,2023.
TransactionsonPatternAnalysisandMachineIntelligence,2023. [150] J.T.Barron,B.Mildenhall,D.Verbin,P.P.Srinivasan,andP.Hedman,
[121] W. Su and W. Tao, ‚ÄúEfficient edge-preserving multi-view stereo net- ‚ÄúZip-nerf: Anti-aliased grid-based neural radiance fields,‚Äù in ICCV,
workfordepthestimation,‚ÄùinAAAI,2023. 2023.
[122] M. Poggi and S. Mattoccia, ‚ÄúLearning from scratch a confidence [151] C. Reiser, R. Szeliski, D. Verbin, P. Srinivasan, B. Mildenhall,
measure.‚ÄùinBMVC,2016. A. Geiger, J. Barron, and P. Hedman, ‚ÄúMerf: Memory-efficient radi-
[123] Z. Fu, M. Ardabilian, and G. Stern, ‚ÄúStereo matching confidence ance fields for real-time view synthesis in unbounded scenes,‚Äù ACM
learning based on multi-modal convolution neural networks,‚Äù in In- TransactionsonGraphics(TOG),vol.42,no.4,2023.
ternationalWorkshoponRepresentations,AnalysisandRecognitionof [152] Q.Gao,Q.Xu,H.Su,U.Neumann,andZ.Xu,‚ÄúStrivec:Sparsetri-
ShapeandMotionFromImagingData. Springer,2017. vectorradiancefields,‚ÄùinICCV,2023.
[124] F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia, ‚ÄúBeyond local [153] Y. Wei, S. Liu, Y. Rao, W. Zhao, J. Lu, and J. Zhou, ‚ÄúNerfingmvs:
reasoning for stereo confidence estimation with deep learning,‚Äù in Guided optimization of neural radiance fields for indoor multi-view
ECCV,2018,pp.319‚Äì334. stereo,‚ÄùinICCV,2021.
[125] S. Kim, S. Kim, D. Min, and K. Sohn, ‚ÄúLaf-net: Locally adaptive
[154] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, ‚ÄúMonosdf:
fusionnetworksforstereoconfidenceestimation,‚ÄùinCVPR,2019.
Exploringmonoculargeometriccuesforneuralimplicitsurfacerecon-
[126] Z.Li,W.Zuo,Z.Wang,andL.Zhang,‚ÄúConfidence-basedlarge-scale
struction,‚ÄùAdvancesinneuralinformationprocessingsystems,2022.
densemulti-viewstereo,‚ÄùTIP,vol.29,2020.
[155] J. Wang, P. Wang, X. Long, C. Theobalt, T. Komura, L. Liu, and
[127] A.Kuhn,C.Sormann,M.Rossi,O.Erdler,andF.Fraundorfer,‚ÄúDeepc-
W.Wang,‚ÄúNeuris:Neuralreconstructionofindoorscenesusingnormal
mvs:Deepconfidencepredictionformulti-viewstereoreconstruction,‚Äù
priors,‚ÄùinECCV,2022.
in3DV. IEEE,2020.
[156] Q.Fu,Q.Xu,Y.S.Ong,andW.Tao,‚ÄúGeo-neus:Geometry-consistent
[128] T.-Y.Lin,P.Goyal,R.Girshick,K.He,andP.Dolla¬¥r,‚ÄúFocallossfor
neural implicit surfaces learning for multi-view reconstruction,‚Äù Ad-
denseobjectdetection,‚ÄùinICCV,2017.
vancesinNeuralInformationProcessingSystems,2022.
[129] Y.Dai,Z.Zhu,Z.Rao,andB.Li,‚ÄúMvs2:Deepunsupervisedmulti-
[157] Y. Wang, Q. Han, M. Habermann, K. Daniilidis, C. Theobalt, and
viewstereowithmulti-viewsymmetry,‚Äùin3DV,2019.
L. Liu, ‚ÄúNeus2: Fast learning of neural implicit surfaces for multi-
[130] B. Huang, H. Yi, C. Huang, Y. He, J. Liu, and X. Liu, ‚ÄúM3vsnet:
viewreconstruction,‚ÄùinICCV,2023.
Unsupervisedmulti-metricmulti-viewstereonetwork,‚Äùin2021IEEE
InternationalConferenceonImageProcessing(ICIP),2021. [158] Z.Li,T.Mu¬®ller,A.Evans,R.H.Taylor,M.Unberath,M.-Y.Liu,and
[131] T.Khot,S.Agrawal,S.Tulsiani,C.Mertz,S.Lucey,andM.Hebert, C.-H.Lin,‚ÄúNeuralangelo:High-fidelityneuralsurfacereconstruction,‚Äù
‚ÄúLearning unsupervised multi-view stereopsis via robust photometric inCVPR,2023.
consistency,‚ÄùarXivpreprintarXiv:1905.02706,2019. [159] R.A.RosuandS.Behnke,‚ÄúPermutosdf:Fastmulti-viewreconstruction
[132] J.Yang,J.M.Alvarez,andM.Liu,‚ÄúSelf-supervisedlearningofdepth withimplicitsurfacesusingpermutohedrallattices,‚ÄùinCVPR,2023.
inferenceformulti-viewstereo,‚ÄùinCVPR,2021. [160] F.Wang,M.-J.Rakotosaona,M.Niemeyer,R.Szeliski,M.Pollefeys,
[133] F.Darmon,B.Bascle,J.-C.Devaux,P.Monasse,andM.Aubry,‚ÄúDeep and F. Tombari, ‚ÄúUnisdf: Unifying neural representations for high-
multi-viewstereogonewild,‚Äùin2021InternationalConferenceon3D fidelity 3d reconstruction of complex scenes with reflections,‚Äù arXiv
Vision(3DV). IEEE,2021. preprintarXiv:2312.13285,2023.
[134] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‚ÄúImage [161] X. Long, C. Lin, P. Wang, T. Komura, and W. Wang, ‚ÄúSparseneus:
qualityassessment:fromerrorvisibilitytostructuralsimilarity,‚ÄùIEEE Fastgeneralizableneuralsurfacereconstructionfromsparseviews,‚Äùin
transactionsonimageprocessing,vol.13,no.4,2004. ECCV,2022.
[135] J.Zhang,R.Tang,Z.Cao,J.Xiao,R.Huang,andL.Fang,‚ÄúElasticmvs: [162] Y.Ren,F.Wang,T.Zhang,M.Pollefeys,andS.Su¬®sstrunk,‚ÄúVolrecon:
Learning elastic part representation for self-supervised multi-view Volume rendering of signed ray distance functions for generalizable
stereopsis,‚ÄùAdvancesinNeuralInformationProcessingSystems,2022. multi-viewreconstruction,‚ÄùinCVPR,2023.24
[163] Y.Liang,H.He,andY.-C.Chen,‚ÄúRetr:Modelingrenderingviatrans- [187] Q.Zhu,Z.Wei,Z.Wang,Y.Chen,andG.Wang,‚ÄúHybridcostvolume
formerforgeneralizableneuralsurfacereconstruction,‚ÄùinAdvancesin regularization for memory-efficient multi-view stereo networks.‚Äù in
neuralinformationprocessingsystems,2023. BMVC,2022.
[164] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and [188] Y.Zhang,J.Zhu,andL.Lin,‚ÄúMulti-viewstereorepresentationrevist:
P.P.Srinivasan,‚ÄúRef-nerf:Structuredview-dependentappearancefor Region-awaremvsnet,‚ÄùinCVPR,2023,pp.17376‚Äì17385.
neuralradiancefields,‚ÄùinCVPR,2022. [189] Q. Xu, W. Kong, W. Tao, and M. Pollefeys, ‚ÄúMulti-scale geometric
[165] L.Yariv,P.Hedman,C.Reiser,D.Verbin,P.P.Srinivasan,R.Szeliski, consistencyguidedandplanarpriorassistedmulti-viewstereo,‚ÄùIEEE
J. T. Barron, and B. Mildenhall, ‚ÄúBakedsdf: Meshing neural sdfs TransactionsonPatternAnalysisandMachineIntelligence,2022.
for real-time view synthesis,‚Äù in ACM SIGGRAPH 2023 Conference [190] Y.Wang,I.Skorokhodov,andP.Wonka,‚ÄúHf-neus:Improvedsurface
Proceedings,2023. reconstruction using high-frequency details,‚Äù Advances in Neural In-
[166] Y. Liu, P. Wang, C. Lin, X. Long, J. Wang, L. Liu, T. Komura, and formationProcessingSystems,2022.
W.Wang,‚ÄúNero:Neuralgeometryandbrdfreconstructionofreflective [191] J.Zhang,Y.Yao,S.Li,T.Fang,D.McKinnon,Y.Tsin,andL.Quan,
objects from multiview images,‚Äù arXiv preprint arXiv:2305.17398, ‚ÄúCriticalregularizationsforneuralsurfacereconstructioninthewild,‚Äù
2023. inCVPR,2022.
[167] R.Liang,H.Chen,C.Li,F.Chen,S.Panneer,andN.Vijaykumar,‚ÄúEn- [192] Y.Wang,I.Skorokhodov,andP.Wonka,‚ÄúPet-neus:Positionalencoding
vidr:Implicitdifferentiablerendererwithneuralenvironmentlighting,‚Äù tri-planesforneuralsurfaces,‚ÄùinCVPR,2023.
inICCV,2023. [193] T.Wu,J.Wang,X.Pan,X.Xu,C.Theobalt,Z.Liu,andD.Lin,‚ÄúVox-
[168] W.Ge,T.Hu,H.Zhao,S.Liu,andY.-C.Chen,‚ÄúRef-neus:Ambiguity- urf:Voxel-basedefficientandaccurateneuralsurfacereconstruction,‚Äù
reducedneuralimplicitsurfacelearningformulti-viewreconstruction arXivpreprintarXiv:2208.12697,2022.
withreflection,‚ÄùinICCV,2023. [194] K.Zhang,S.Bi,H.Tan,Y.Xiangli,N.Zhao,K.Sunkavalli,andZ.Xu,
[169] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoor- ‚ÄúGs-lrm:Largereconstructionmodelfor3dgaussiansplatting,‚ÄùarXiv
thi, and R. Ng, ‚ÄúNerf: Representing scenes as neural radiance fields preprintarXiv:2404.19702,2024.
forviewsynthesis,‚ÄùCommunicationsoftheACM,vol.65,no.1,2021. [195] J.Tang,Z.Chen,X.Chen,T.Wang,G.Zeng,andZ.Liu,‚ÄúLgm:Large
[170] B.Kerbl,G.Kopanas,T.Leimku¬®hler,andG.Drettakis,‚Äú3dgaussian multi-view gaussian model for high-resolution 3d content creation,‚Äù
splattingforreal-timeradiancefieldrendering,‚ÄùACMTransactionson arXivpreprintarXiv:2402.05054,2024.
Graphics,vol.42,no.4,2023. [196] W.Wang,D.Zhu,X.Wang,Y.Hu,Y.Qiu,C.Wang,Y.Hu,A.Kapoor,
andS.Scherer,‚ÄúTartanair:Adatasettopushthelimitsofvisualslam,‚Äù
[171] N. Snavely, S. M. Seitz, and R. Szeliski, ‚ÄúPhoto tourism: exploring
inIROS,2020.
photo collections in 3d,‚Äù in ACM siggraph 2006 papers, 2006, pp.
835‚Äì846. [197] G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu,
T. Gebauer, B. Joffe, D. Kurz, A. Schwartz et al., ‚ÄúArkitscenes: A
[172] M. Kazhdan, M. Bolitho, and H. Hoppe, ‚ÄúPoisson surface recon-
diverse real-world dataset for 3d indoor scene understanding using
struction,‚Äù in Proceedings of the fourth Eurographics symposium on
mobilergb-ddata,‚ÄùarXivpreprintarXiv:2111.08897,2021.
Geometryprocessing,vol.7,no.4,2006.
[198] C.Yeshwanth,Y.-C.Liu,M.Nie√üner,andA.Dai,‚ÄúScannet++:Ahigh-
[173] H. Chen, C. Li, and G. H. Lee, ‚ÄúNeusg: Neural implicit surface
fidelitydatasetof3dindoorscenes,‚ÄùinICCV,2023.
reconstruction with 3d gaussian splatting guidance,‚Äù arXiv preprint
[199] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei,‚ÄúImagenet:
arXiv:2312.00846,2023.
Alarge-scalehierarchicalimagedatabase,‚ÄùinCVPR,2009.
[174] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
[200] X. Yu, M. Xu, Y. Zhang, H. Liu, C. Ye, Y. Wu, Z. Yan, C. Zhu,
Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
Z.Xiong,T.Liangetal.,‚ÄúMvimgnet:Alarge-scaledatasetofmulti-
Advancesinneuralinformationprocessingsystems,2017.
viewimages,‚ÄùinCVPR,2023.
[175] J. Han, F. Kokkinos, and P. Torr, ‚ÄúVfusion3d: Learning scalable
[201] P.-E. Sarlin, M. Dusmanu, J. L. Scho¬®nberger, P. Speciale, L. Gruber,
3d generative models from video diffusion models,‚Äù arXiv preprint
V. Larsson, O. Miksik, and M. Pollefeys, ‚ÄúLamar: Benchmarking
arXiv:2403.12034,2024.
localizationandmappingforaugmentedreality,‚ÄùinECCV,2022.
[176] D.Tochilkin,D.Pankratz,Z.Liu,Z.Huang,A.Letts,Y.Li,D.Liang,
[202] S. Weder, J. Schonberger, M. Pollefeys, and M. R. Oswald, ‚ÄúRout-
C. Laforte, V. Jampani, and Y.-P. Cao, ‚ÄúTriposr: Fast 3d object
edfusion: Learning real-time depth map fusion,‚Äù in CVPR, 2020, pp.
reconstructionfromasingleimage,‚ÄùarXivpreprintarXiv:2403.02151,
4887‚Äì4897.
2024.
[203] S.Weder,J.L.Schonberger,M.Pollefeys,andM.R.Oswald,‚ÄúNeu-
[177] J.Li,H.Tan,K.Zhang,Z.Xu,F.Luan,Y.Xu,Y.Hong,K.Sunkavalli,
ralfusion: Online depth fusion in latent space,‚Äù in CVPR, 2021, pp.
G. Shakhnarovich, and S. Bi, ‚ÄúInstant3d: Fast text-to-3d with sparse-
3162‚Äì3172.
viewgenerationandlargereconstructionmodel,‚ÄùICLR,2024.
[204] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,
[178] Y. Xu, H. Tan, F. Luan, S. Bi, P. Wang, J. Li, Z. Shi, K. Sunkavalli, ‚ÄúDeformableconvolutionalnetworks,‚ÄùinICCV,2017.
G. Wetzstein, Z. Xu et al., ‚ÄúDmv3d: Denoising multi-view diffusion
[205] K. Wu, J. Zhang, H. Peng, M. Liu, B. Xiao, J. Fu, and L. Yuan,
using3dlargereconstructionmodel,‚ÄùICLR,2024.
‚ÄúTinyvit:Fastpretrainingdistillationforsmallvisiontransformers,‚Äùin
[179] P.Wang,H.Tan,S.Bi,Y.Xu,F.Luan,K.Sunkavalli,W.Wang,Z.Xu, ECCV,2022.
andK.Zhang,‚ÄúPf-lrm:Pose-freelargereconstructionmodelforjoint [206] U. Kusupati, S. Cheng, R. Chen, and H. Su, ‚ÄúNormal assisted stereo
poseandshapeprediction,‚ÄùICLR,2024. depthestimation,‚ÄùinCVPR,2020.
[180] E.R.Chan,C.Z.Lin,M.A.Chan,K.Nagano,B.Pan,S.DeMello, [207] X.Long,L.Liu,C.Theobalt,andW.Wang,‚ÄúOcclusion-awaredepth
O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis et al., ‚ÄúEfficient estimationwithadaptivenormalconstraints,‚ÄùinECCV,2020.
geometry-aware3dgenerativeadversarialnetworks,‚ÄùinCVPR,2022. [208] C.Liu,K.Kim,J.Gu,Y.Furukawa,andJ.Kautz,‚ÄúPlanercnn:3dplane
[181] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, detectionandreconstructionfromasingleimage,‚ÄùinCVPR,2019.
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal., [209] H.Liu,X.Tang,andS.Shen,‚ÄúDepth-mapcompletionforlargeindoor
‚ÄúAnimageisworth16x16words:Transformersforimagerecognition scenereconstruction,‚ÄùPatternRecognition,vol.99,p.107112,2020.
atscale,‚ÄùarXivpreprintarXiv:2010.11929,2020. [210] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, ‚ÄúOmnidata: A scalable
[182] K. Luo, T. Guan, L. Ju, H. Huang, and Y. Luo, ‚ÄúP-mvsnet: Learning pipelineformakingmulti-taskmid-levelvisiondatasetsfrom3dscans,‚Äù
patch-wisematchingconfidenceaggregationformulti-viewstereo,‚Äùin inICCV,2021.
ICCV,2019,pp.10452‚Äì10461. [211] J. Liu, P. Ji, N. Bansal, C. Cai, Q. Yan, X. Huang, and Y. Xu,
[183] Z.Wei,Q.Zhu,C.Min,Y.Chen,andG.Wang,‚ÄúBidirectionalhybrid ‚ÄúPlanemvs:3dplanereconstructionfrommulti-viewstereo,‚ÄùinCVPR,
lstm based recurrent neural network for multi-view stereo,‚Äù IEEE 2022.
TransactionsonVisualizationandComputerGraphics,2022. [212] Y. Xie, M. Gadelha, F. Yang, X. Zhou, and H. Jiang, ‚ÄúPlanarrecon:
[184] K. Luo, T. Guan, L. Ju, Y. Wang, Z. Chen, and Y. Luo, ‚ÄúAttention- Real-time3dplanedetectionandreconstructionfromposedmonocular
awaremulti-viewstereo,‚ÄùinCVPR,2020. videos,‚ÄùinCVPR,2022.
[185] J.Yang,J.M.Alvarez,andM.Liu,‚ÄúNon-parametricdepthdistribution [213] A.OsmanUlusoy,M.J.Black,andA.Geiger,‚ÄúSemanticmulti-view
modellingbaseddepthinferenceformulti-viewstereo,‚ÄùinCVPR,2022, stereo:Jointlyestimatingobjectsandvoxels,‚ÄùinCVPR,2017.
pp.8626‚Äì8634. [214] E.-K. Stathopoulou and F. Remondino, ‚ÄúSemantic photogrammetry‚Äì
[186] L.Wang,Y.Gong,X.Ma,Q.Wang,K.Zhou,andL.Chen,‚ÄúIs-mvsnet: boosting image-based 3d reconstruction with semantic labeling,‚Äù The
Importance sampling-based mvsnet,‚Äù in ECCV. Springer, 2022, pp. International Archives of the Photogrammetry, Remote Sensing and
668‚Äì683. SpatialInformationSciences,vol.42,pp.685‚Äì690,2019.25
[215] E. K. Stathopoulou, R. Battisti, D. Cernea, F. Remondino, and
A.Georgopoulos,‚ÄúSemanticallyderivedgeometricconstraintsformvs
reconstructionoftexturelessareas,‚ÄùRemoteSensing,vol.13,no.6,p.
1053,2021.
[216] H.Guo,S.Peng,H.Lin,Q.Wang,G.Zhang,H.Bao,andX.Zhou,
‚ÄúNeural 3d scene reconstruction with the manhattan-world assump-
tion,‚ÄùinCVPR,2022.