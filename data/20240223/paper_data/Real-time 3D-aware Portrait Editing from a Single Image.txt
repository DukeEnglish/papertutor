Real-time 3D-aware Portrait Editing from a Single Image
QINGYANBAI,TheHongKongUniversityofScienceandTechnology,China
YINGHAOXU,StanfordUniversity,USA
ZIFANSHI,TheHongKongUniversityofScienceandTechnology,China
HAOOUYANG,TheHongKongUniversityofScienceandTechnology,China
QIUYUWANG,AntGroup,China
CEYUANYANG,ShanghaiAILaboratory,China
XUANWANG,AntGroup,China
GORDONWETZSTEIN,StanfordUniversity,USA
YUJUNSHENâˆ—,AntGroup,China
QIFENGCHENâˆ—,TheHongKongUniversityofScienceandTechnology,China
Edited 3D Portraits Efficiency Comparison
Input Novel-view Synthesis Input Ours (0.04s) IP2P+Live3D (10s)
Text Prompt: Make him resemble a young child. Input Ours (0.04s) PTI+CLIP (360s)
Fig.1. Photorealisticeditingresultsproducedbyourproposed3DPE,whichallowsuserstoperform3D-awareportraiteditingusingimageortextprompts.
Incomparisonwithbaselinemethods,suchasInstructPix2Pix[Brooksetal.2023]+Live3D[Trevithicketal.2023]andPTI[Roichetal.2022]+CLIP[Radford
etal.2021](detailsareillustratedinSec.4),ourapproachaccuratelyfollowstheguidancefromreferencepromptsandmaintainssufficientlybetterefficiency.
Thisworkpresents3DPE,apracticaltoolthatcanefficientlyeditaface touser-specifiednoveltypesofeditingduringinference(e.g.,withâˆ¼5min
imagefollowinggivenprompts,likereferenceimagesortextdescriptions,in fine-tuningpercase).Thecode,themodel,andtheinterfacewillbemade
the3D-awaremanner.Tothisend,alightweightmoduleisdistilledfroma3D publiclyavailabletofacilitatefutureresearch.
portraitgeneratorandatext-to-imagemodel,whichprovidepriorknowledge
AdditionalKeyWordsandPhrases:3D-awareportrait,efficientediting
offacegeometryandopen-vocabularyeditingcapability,respectively.Sucha
designbringstwocompellingadvantagesoverexistingapproaches.First,our 1 INTRODUCTION
systemachievesreal-timeeditingwithafeedforwardnetwork(i.e.,âˆ¼0.04s
perimage),over100Ã—fasterthanthesecondcompetitor.Second,thanks Inferringthegeometryandappearancefromasingle-viewportrait
tothepowerfulpriors,ourmodulecouldfocusonthelearningofediting- imagehasbecomematureandpractical[Gaoetal.2020;Koetal.
relatedvariations,suchthatitmanagestohandlevarioustypesofediting 2023;Linetal.2022;Sunetal.2022a;Trevithicketal.2023;Wang
simultaneouslyinthetrainingphaseandfurthersupportsfastadaptation etal.2022;Xieetal.2023],largelyattributedtotheutilizationof
âˆ—Correspondingauthors. priorsinvarious2D/3Dgenerativemodels.However,onlyperform-
inggeometryreconstructionisinsufficient.Thesignificanceof3D
Authorsâ€™addresses:QingyanBai,TheHongKongUniversityofScienceandTechnology, editing of portraits, driven by user intentions, and the need for
China;YinghaoXu,StanfordUniversity,USA;ZifanShi,TheHongKongUniversityof
ScienceandTechnology,China;HaoOuyang,TheHongKongUniversityofScience streamlinedefficiencyintheeditingprocesshasbeensteadilyin-
andTechnology,China;QiuyuWang,AntGroup,China;CeyuanYang,ShanghaiAI creasing.Thisisparticularlycrucialinreal-worldapplicationssuch
Laboratory,China;XuanWang,AntGroup,China;GordonWetzstein,StanfordUni-
asAR/VR,3Dtelepresence,andvideoconferencing,wherereal-time
versity,USA;YujunShen,AntGroup,China;QifengChen,TheHongKongUniversity
ofScienceandTechnology,China. editingisoftenessential.Consequently,akeyquestionarises:How
4202
beF
12
]VC.sc[
1v00041.2042:viXra
egamI
txeTâ€¢ Bai,Q.etal
canweeffectivelyaddressthechallengeofattaininghigh-fidelity â€¢ Ourmodelcanaccommodatevariouscontrolsignals,including
portraiteditingwhileensuringreal-timeefficiency? textandimageprompts.
Traditionaltoolsfor3Dportraitediting[Dengetal.2020]typi-
callyrelyontemplatefacialmodels[BlanzandVetter2023;Paysan
2 RELATEDWORK
etal.2009],whichhaslimitationsinhandlingthesubstantialgeome-
2.1 GenerativeFacePriors
trychangesbecauseitoftenoverlooksprecisefeatureslikehairand
beard.Recent3DGANs[Chanetal.2022,2021;Schwarzetal.2020; Generative models aim at modeling the underlying distribution
Shietal.2022a;XiaandXue2023]showremarkablecapabilitiesin ofthedata,containingawealthofpriorknowledge.Recently,3D
generatinghigh-fidelity3Dportraits.Theycanserveaspowerful GANs[Chanetal.2022,2021;Guetal.2022;Or-Eletal.2022;Pan
generativepriorsfor3DportraiteditingwhencoupledwithGAN etal.2021;Schwarzetal.2020,2022;Shietal.2023,2022b;Sko-
inversiontechniques[Abdaletal.2019,2020;Shenetal.2020b;Zhu rokhodovetal.2022;Wangetal.2023b;Xuetal.2022]aremostly
etal.2020,2016].However,thesemethodsmayencounterissues adopted to learn 3D faces from single-view image dataset. The
relatedtogeometrydistortionorexhibitslowspeed,andtheediting interiorrichdomain-specificgeometrypriorsenablevariousface-
isconstrainedbythelimitedlatentattributes.Besides,2Ddiffusion relatedapplicationssuchasimageediting[Caietal.2022;Sunetal.
modelscanofferastrongpriorwithSDSloss[Pooleetal.2022] 2022a,b;Trevithicketal.2023].Large-scalediffusionmodels[Rom-
foreditingpurposes[Hertzetal.2023].Nonetheless,theyoften bachetal.2022],incontrast,encodeknowledgeofhugedatasetsand
requirestep-by-stepoptimizationandbecomeamajorbottleneck thuscanprovidegeneralpriorinformation.Suchpriorsarebroadly
forreal-timeapplications. leveragedfortaskssuchasimageediting[Brooksetal.2023;Cao
Tothisend,wepresent3DPE,areal-time3D-awarePortrait etal.2023;Graikosetal.2022;Haqueetal.2023;Mengetal.2021;
Editingmethoddrivenbyuser-definedprompts.AsshowninFig.1, Zhangetal.2023],customization[Huetal.2021;Kumarietal.2023;
whenprovidedwithasingle-viewportraitimage,ourapproach Liuetal.2023a;Ruizetal.2023],andvideoediting[Ceylanetal.
empowersaversatilerangeofeditingthroughflexibleinstructions, 2023;Chaietal.2023;Geyeretal.2023;Liuetal.2023b;Ouyang
includingimagesandtext.Weleveragethepowerful3Dpriorfrom etal.2023;Qietal.2023;Wangetal.2023a;Yangetal.2023].In
a3D-awarefacegeneratorandachieveahigh-fidelity3Drecon- ourapproach,wecapitalizeontheadvantagesofboththegeometry
struction of the portrait image. Subsequently, we distill editing priorderivedfrom3DGANsandthebroadereditingprioroffered
knowledgefromanopen-vocabularytext-to-imagemodelintoa bylarge-scaletext-to-imagediffusionmodels,insteadofrelying
lightweightmoduleintegratedwiththe3D-awaregenerator.This solelyonasingletypeofprior.
module,characterizedbyitsminimalcomputationalcost,allows
oursystemtomaintainreal-timeinferenceandexcelinhandling
2.2 PortraitEditingfromaSingleImage
varioustypesofediting,allthewhileensuringgood3Dconsistency.
Anadditionaladvantageisthatourmodelsupportscustomization Althoughmanymethodsworkwellforreconstructing[Gaoetal.
throughuser-specifiedpromptswithfastadaptationspeed.This 2020;Guoetal.2022]orgenerating[Chanetal.2022]faces,editing
empowersuserstobuildtheirowneditingmodelataminimalcost, hasbecomeanecessaryinterfacetoconnectthesemethodswith
enablingoursystemtocatertoabroaderaudience. real-worldapplications.Previously,portraiteditinggivensingle-
Oursystemachievesreal-time3D-awareportraiteditingthrough viewimagewasmostlycompletedin2Dimagespacefacilitated
theutilizationofafeedforwardnetwork,withaprocessingtime withGANinversion[Abdaletal.2019,2020;Baietal.2022;HÃ¤rkÃ¶-
of40msonastandardcustomerGPU.Additionally,wepresenta nenetal.2020;Roichetal.2022;Shenetal.2020a,b;Xuetal.2021;
comprehensiveevaluationofourmethodusingvariousprompts Zhuetal.2020],whichenablesfasteditingbyexploringthetra-
bothquantitativelyandqualitatively.Ourdesignchoicesarealso jectoriesintheGANâ€™slatentspace.Recentdiffusion-basedportrait
validatedthroughcomparisonswithablatedvariantsofourmethod. editing[Brooksetal.2023;Zhangetal.2023]cansupportvarious
Comparedwithbaselinemethods,ourapproachdemonstratessu- editingtypeswithtextsasguidance.However,mostofthemare
perior3Dconsistency,precisetexturealignment,andasubstantial doneinthe2Dspace,andthusthereisnoguaranteefortheun-
improvementininferencetime,asreflectedbytheevaluationmet- derlying3Dconsistency.Therefore,3D-awareportraiteditingis
rics.Wedemonstratetheversatilityofourmethodbyshowcasing crucialtoachievethegoal.Somemethods[Changetal.2023;Jiang
itscapacitytoperformawidevarietyofedits,includingtextand etal.2023;Lietal.2023a;Linetal.2022;Sunetal.2022a,b;Wang
imageprompts,onportraitimages.Thecode,themodelandthe etal.2022;Xieetal.2023;Yinetal.2023]relyonthelatentspaceof
interfacewillbemadepubliclyavailable. 3DGANstoperformeditingthroughwalkinginthelatentspace,
Insummary,thecontributionsofourworkinclude: butarelimitedbythenumberofeditablelatentattributes.Afew
methods,suchasInstruct-NeRF2NeRF[Haqueetal.2023],Clip-
Face[Anejaetal.2023],andLENeRF[Hyungetal.2023],attempt
â€¢ Weproposealightweightmoduletodistillknowledgefrom3D toleveragethelarge-scaleopen-vocabularymodels[Radfordetal.
GANsanddiffusionmodelsfor3D-awareeditingfromasingle 2021;Rombachetal.2022]toediton3Drepresentationswithtexts
image.Duetotheminimalcostofthenewmodule,ourmodel asguidancebutrequireheavyiterativerefinementoftheedited
maintainsreal-timeperformance. results.Similarly,InstructPix2NeRF[Lietal.2023b]leveragestext-
â€¢ Our model supports fast adaptation to user-specified editing, to-imagemodelsandtextpromptstoeditthelatentspaceofthe
requiringonly10imagepairsand5minutesfortheadaptation. GAN-based3Dgenerator.However,itisheldbackbythemulti-stepReal-time3D-awarePortraitEditingfromaSingleImage â€¢
Distillation into Lightweight Module
E low Fusion E t
E Cross-Att
high
Prompt Geometry Constraint
Learnable
E Frozen 2D Constraint
p
Input Portrait
Diffusion Prior 3D GAN Prior Triplane
Generative Prior as Learning Guidance
Fig.2. Overviewofourmethod.Wedistillthepriorinthediffusionmodeland3DGANforreal-time3D-awareediting.Ourapproachisfine-tunedfrom
Live3D[Trevithicketal.2023],whereweextractfeaturesfromtheinputportraitIusingEâ„ğ‘–ğ‘”â„(Â·)andEğ‘™ğ‘œğ‘¤(Â·).Thepromptembeddingisgeneratedwith
Eğ‘(Â·)andinjectedwiththeinputfeaturesfromEâ„ğ‘–ğ‘”â„(Â·)throughacross-attentionmechanism.Ourmodelistrainedtomimictheoutputfromthediffusion
priortoacquireeditingknowledgeandenforcegeometryconstraintsthroughtriplane,multi-viewimages,anddepthsupervisionfromthe3Dprior.Inthis
context,InstructPix2Pix[Brooksetal.2023]andLive3Dserveasthediffusionand3Dprior,respectively.Itâ€™snoteworthythatonlyEğ‘(Â·)andEğ‘¡(Â·)are
learnableduringtraining,whileallotherparametersremainfrozen.
3.1 Preliminary
diffusioninferenceandtheheavystructureofaGAN-basedgen-
erator.Ours,incontrast,benefitsfromthelightweightdesignof
themoduleandachievesreal-timeeditingwithease.Withsucha 3DGANPriorforPortraitReconstruction.The3D-awareGANs
design,ourmodelcanalsoadapttonoveleditingpromptsfaster showcasetheabilitytosynthesizephotorealistic3Dimagesusinga
thanpreviousmethods.Moreover,textscannotalwaysillustratethe collectionofsingle-viewimages.Notably,EG3D[Chanetal.2022]
desiredeffectsprecisely,andsometimesanimagepromptservesas introducesanefficienttriplane3Drepresentation,demonstrating
abetterguidanceforimageediting.Ourmethodcansupportnot high-quality3D-awareimagerendering.Oncetrained,thegenerator
onlytextpromptsbutalsoimagepromptsforediting,providinga of EG3D can be applied for single-image 3D reconstruction via
moreflexibleanduser-friendlyinterface. GANinversion.However,existing3DGANinversionmethodsoften
encountergeometrydistortionorexhibitslowinferencespeed.To
tacklethesechallenges,weutilizeLive3D[Trevithicketal.2023],a
state-of-the-artsingle-imageportrait3Dreconstructionmodelbuilt
3 METHOD uponEG3D,preservinggeometryqualitywhileensuringreal-time
performance.Live3Demploysatwo-branchencoder,Eâ„ğ‘–ğ‘”â„(Â·)and
OurapproachtakesaportraitimageIanditscameraposec,obtained
Eğ‘™ğ‘œğ‘¤(Â·),toextractdifferentresolutionfeaturesfromtheinputimage.
viathefaceposeestimator[Dengetal.2019b].Additionally,theref-
ItthenutilizesaViT-basedEğ‘¡(Â·)decoder[Dosovitskiyetal.2020]
erencedpromptsP(imagesortexts)serveastheeditinginstructions.
totransformthefusedencoderfeaturesintothetriplane,T:
Theoutcomeofourmodelisaneditedversionofthe3Dportraits
characterizedbyNeRFinaccordancewiththeprompts.Theedited T=Eğ‘¡(Eâ„ğ‘–ğ‘”â„(I),Eğ‘™ğ‘œğ‘¤(I)) (1)
image is denoted as Iğ‘. Within our framework, we achieve this
objectivebyleveragingtheknowledgeof3DGANsandanopen- ThistriplaneTissubsequentlyusedinconjunctionwiththevolume
vocabularytext-to-imagemodel,whichisdistilledintoalightweight renderingmoduleandupsamplerofEG3Dtogeneratephotorealistic
module.Followingdistillation,ourmethodenablesreal-time3Dpor- viewsynthesisgiventhecameraposec.Forclarification,weuse
traiteditingandefficientadaptationtouser-specifiedprompts. R(Â·)todenotethisprocess.
InSec.3.1,weofferanillustrationof3DGANsanddiffusion DiffusionPriorforPortraitEditing.Onlyperforming3Dre-
models. The process of distilling these priors into a lightweight constructionisinsufficientasourgoalistoeditportraitimages.
moduleisoutlinedinSec.3.2.Finally,thedetailsformodeltraining Large-scalediffusionmodels[Rombachetal.2022],trainedonvast
andinference,alongwiththefastadaptationfornovelprompts,are text-imagepairs,cansynthesizerealisticphotoswithtextinput,
presentedinSec.3.3. offering a powerful editing prior. Despite their effectiveness forâ€¢ Bai,Q.etal
faceimages,theeditingprocessisoftenslowduetostep-by-step Input Rec Novel Views
optimizationormultipleinferencesindiffusionmodels.Moreover,
obtainingalargeamountofhigh-qualitypaireddatawithmulti-
viewconsistencyfromthediffusionmodelisextremelychallenging,
resultingintheapplicationof3D-awareeditingfromasingleimage
impractical.Therefore,ourgoalistodistilleditingknowledgefrom
thediffusionmodel,integrateitwiththe3DpriorinLive3Dintoa
lightweightmodule,andemployitforreal-timeportraitediting.
3.2 DistillingPriorsintoaLightweightModule
Oursystemaimstoperformreal-time3D-awareeditingforvari-
ouspromptsforthesingle-viewportrait.Thus,itneedspowerful
3Dknowledgeforgeometryreconstructionandeditingpriortohan-
dlingvariouscontrolsignals.Weleveragethestrengthsofboth3D
GANsanddiffusionmodels.Inthefollowing,weprovidedetailed Fig.3. DisentanglementinLive3Dfeatures.Weseparatelydisablethe
presentationsonhowtheknowledgeofthesetwotypesofmodels
featuresfromEâ„ğ‘–ğ‘”â„(Â·)andEğ‘™ğ‘œğ‘¤(Â·)toinferthereconstructedimage.With-
isdistilledintoalightweightmodule.
outEâ„ğ‘–ğ‘”â„(Â·),theoutputretainsthecoarsestructurebutlosestheappear-
FeatureRepresentationinLive3D.WecarefullystudytheLive3D
ance.Conversely,whenEğ‘™ğ‘œğ‘¤(Â·)isdeactivated,thereconstructedportraits
preservethetexture(suchastheblueandpurplereflectionontheglasses)
modelanddiscoverthat,asatwo-branchtriplane-based3Drecon-
butfailtocapturethegeometry.
structionmodel,thelow-resolutionandhigh-resolutionfeatures
fromtheLive3DencoderEğ‘™ğ‘œğ‘¤(Â·) andEâ„ğ‘–ğ‘”â„(Â·) tendtolearnvari-
ouslevelsofinformationwithoutexplicitguidance.Weconduct servesasthequeryandEğ‘(P)isemployedaskeyandvalue.The
astudybydisablingoneofthebranchesandinferringtherecon- updatedfeatureFğ‘andthegeometryfeatureFğ‘”arethenfeedinto
structedimages.AsillustratedinFig.3,whenthehigh-resolution thedecoderEğ‘¡(Â·)toinferthetriplaneTp:
encoderEâ„ğ‘–ğ‘”â„(Â·)isdisabled,theinferenceimageretainsasimilar
structurebutlosesitsdetailedappearance.Conversely,whenthe
Tp=Eğ‘¡(Fğ‘,Fğ‘”). (3)
low-resolutionencoderEğ‘™ğ‘œğ‘¤(Â·)isdisabled,thereconstructedpor- DistillingDiffusionand3DGANPrior.Unlikepreviousmethods
traitspreservesomeofthetexturefromtheinputbutstruggleto thatrequirelargeamountsof3Ddata,ourmodeloperateswithonly
capturethegeometry.Basedonthisanalysis,thefeaturesofthe 2Dpaireddatageneratedbythe2Deditingmodel[Brooksetal.2023]
twobranchesseparatelymodellow-frequencyandhigh-frequency duringtraining.Forvisualprompts,weperformcaptioning[Lietal.
information.Thisinsightmotivatesourmodeldesignintermsof 2022]onPandthenusethecaptionsastheinstructiontogenerate
howtodistillknowledgefromthediffusionmodeland3DGANs. aneditedimageIğ‘”ğ‘¡ with2Ddiffusionmodel[Brooksetal.2023]
GeometryPredictionforInputsI.Inourframework,weusethe asthepseudolabel.Fortextprompts,wefollowasimilarprocess
promptsPtorefinetheinputportraitIintotheeditedimageIğ‘. butskipthecaptioningstep.WiththepredictedtriplaneTp,we
TheIğ‘ inheritsasimilarstructuretotheinputI,providingcoarse canrendertheimageusingthepretrainedEG3Dmodelandthen
geometryorstructuralguidanceintheeditingprocess.Asdiscussed calculatethereconstructionlossasfollows:
intheabovesection,theencoderEğ‘™ğ‘œğ‘¤(Â·) consistentlygenerates
low-frequencyfeaturestorepresentgeometrycues,makingitwell- L 2ğ‘‘ =â„“ I(Iğ‘”ğ‘¡,R(Tp,c)), (4)
suitedforpreservingstructuredinformationintheinputportrait. whereR(Â·)istherenderingmoduleofEG3D,cisthecameraposeof
Asaresult,wefreezeEğ‘™ğ‘œğ‘¤(Â·)andleverageittoproducestructure inputportrait,andâ„“ 1(Â·)isanimagereconstructionlosspenalizing
featuresFğ‘” =Eğ‘™ğ‘œğ‘¤(I)forcoarsegeometryprediction. thedifferencebetweenthegroundtruthIğ‘”ğ‘¡ andtherenderingIğ‘ =
InjectingPromptsPasCondition.Incontrasttoinputimages, R(Tp,c)).Itâ€™simportanttonotethatL 2ğ‘‘,usedtoreconstructthe
promptstypicallyoffermorehigh-frequencyandtextureinforma- Iğ‘”ğ‘¡,essentiallydistilltheknowledgeofeditingfromthediffusion
tion to guide and control the editing process. Accordingly, it is model.
appropriatetoincorporatethepromptsintothehigh-levelbranch WeobservethatL 2ğ‘‘ helpsthemodeltoreconstructwellonthe
Eâ„ğ‘–ğ‘”â„(Â·) inLive3Dtoextractthehigh-frequencyfeatures.Toin- input camera view but suffers from geometry distortion during
corporateprompts,weutilizethepromptencoderEğ‘(Â·)tocreate novel-viewsynthesis.Tofullyexploitthe3DpropertiesofLive3D,
promptembeddingsthatarefusedintoourmodelthroughcross weproposetodistillthe3DknowledgeofLive3D.Specifically,we
attention,whichissimilartothestrategyinStableDiffusion[Rom- leveragethepretrainedLive3DtoinferthetriplaneTğ‘”ğ‘¡,andmulti-
bachetal.2022].Specifically,weaddacross-attentionlayerafter viewdepthsDğ‘”ğ‘¡ andimagesIğ‘”ğ‘¡ ofthepseudo-labelimageIğ‘”ğ‘¡:
Eâ„ğ‘–ğ‘”â„(Â·)toobtainthefeatureupdatedwiththepromptembeddings:
Tğ‘”ğ‘¡,Dğ‘”ğ‘¡ =G(Iğ‘”ğ‘¡,C), (5)
Fğ‘ =crossatt(Eâ„ğ‘–ğ‘”â„(I),Eğ‘(P)), (2)
whereG(Â·)representstheinferenceprocessofLive3D,whereC=
wheretheencoderEğ‘(Â·) isatransformer(MAE[Heetal.2022] {c1,..,cğ‘›}isthecameraset,andğ‘›denotesthenumberofcameras.
for image and CLIP [Radford et al. 2021] for text). The Eâ„ğ‘–ğ‘”â„(I) Wealsorenderthemulti-viewdepths Dğ‘ andimagesIğ‘ ofthe
lluF
hgihE
o/w
wolE
o/wReal-time3D-awarePortraitEditingfromaSingleImage â€¢
editedimageIğ‘ fromthetriplaneTğ‘ andthendefinetheobjective:
Table1. Quantitativecomparisons.Wecompareseveralbaselineson
the100imagesofFFHQdataset.Itâ€™simportanttonotethatweexclude
L 3ğ‘‘ =â„“ I(Iğ‘,Iğ‘”ğ‘¡)+â„“ T(Tğ‘,Tğ‘”ğ‘¡)+â„“ D(Dğ‘,Dğ‘”ğ‘¡), (6) ğ¶ğ¿ğ¼ğ‘ƒğ‘Ÿ forPTI+CLIPandLive3D+CLIPsincethesemodelsutilizeCLIPfor
whereâ„“ I(Â·),â„“ T(Â·)andâ„“ D(Â·)isthereconstructionlosspenalizing o impt pim roi vz ea mtio en n. tO (cu or mm po ad re el de wxc ie tl hs Iin P23 PD +q Lu iva eli 3t Dy )an ind ia nc fh ei re ev ne cs ea sr pe em edar ,k aa cb hl ie e2 v5 in0 gx
thedifferenceinimage,triplane,anddepthbetweenIğ‘ andIğ‘”ğ‘¡.
real-timeperformance.
3.3 TrainingandInference
Method ğ¼ğ·ğ‘¡ â†‘ ğ¶ğ¿ğ¼ğ‘ƒğ‘Ÿ â†‘ 3Dâ†‘ Timeâ†“
PTI+CLIP 0.11 - 0.73 360s
Training.Duringthetrainingphase,wesampleatripletconsisting
Live3D+CLIP 0.63 - 0.72 30s
oftheinputportraitIalongwithitscameraposec,promptsP,and
IP2P+Live3D 0.52 0.59 0.75 10s
thepseudo-labelimageIğ‘”ğ‘¡ generatedbythe2Ddiffusionmodel.
Ours 0.47 0.73 0.76 0.04s
WeleveragetheLive3Dmodelasapretrainedmodelandaddan
additionalpromptsencoderEğ‘ toextractpromptsembeddings.The
overalllearningobjectivecanbedescribedasfollows:
ğ¿=ğœ† 1ğ¿ 2ğ· +ğœ† 2ğ¿ 3ğ·, (7) forreconstructionandaugmentbothofthemwithCLIPloss[Rad-
fordetal.2021]forediting.Inthesecondcategory,weemployan
whereğœ† 1andğœ† 2arelossweights.Inoursetting,ğœ† 1andğœ† 2areboth instructivepixel-to-pixeleditingmethod[Brooksetal.2023]prior
setto1.0.Forthereconstructionloss,â„“ I(Â·),â„“ I(Â·)arecombinations toexecuting3DreconstructionviaLive3D.
ofL2lossandLPIPSloss[Zhangetal.2018],withlossweightsbeing
EvaluationCriteria.Weevaluateallmetricson100pairsofim-
1and2,respectively.â„“ D(Â·)andâ„“ T(Â·)areL1 loss.Notably,during
agesprocessedfromFFHQ.Weconductacomprehensiveevaluation
training,onlyEğ‘(Â·),Eğ‘¡(Â·)arelearnable,whileothermodulesare
oftheeditingperformanceinthefollowingfouraspects:identity
frozen.ItallowsourmodeltoleverageLive3Dknowledgeasmuch
preservation,referencealignment,3Dconsistency,andinference
aspossibleandconvergeataveryfastspeed.
speed.1)Identitypreservation(IDt)aimstomeasurethepreser-
Inference.Forinference,userscanprovideasingleportraitimage
vationoftheoriginalidentitybycalculatingthecosinesimilarity
andchoosepromptsusedduringourtraining.Ourmodelisable
betweentheidentityfeatureoftheinputimageIandthatofthe
to generate the edited 3D NeRF along with photorealistic view
editedimageIğ‘.WeuseArcFacemodel[Dengetal.2019a]toextract
synthesis.
identityfeatures.2)Referencealignment(CLIPr)targetsatassess-
CustomizedPromptsAdaptation.Toaccommodatecustomized
ingthealignmentoftheoutputeditingstylestothedesiredinput
stylesprovidedbyusers,weproposeamethodtoadaptourpre-
promptbycomputingthecosinesimilarityintheCLIP[Radford
trainedencodertonovelprompts.Weincreasethetuningefficiency
etal.2021]featurespace.3)3Dconsistency(3D)ontheeditedout-
byoptimizingonlyEğ‘(Â·)andthenormalizationlayersinEğ‘¡(Â·)with
putsismeasuredfollowingtheevaluationprotocolsestablishedby
thesamelearningobjectiveEq.7.Thismethodallowsustolimit
EG3D[Chanetal.2022].Thisinvolvescalculatingtheidentitysimi-
thetrainingdatatoonly10imagepairsandthelearningtimeto5
larityacrossmultipleviews.4)Inferencespeed(Time)ismeasured
minutesonasingleGPU.
onasingleNVIDIAA6000GPUwithanaverageof100samples.
4 EXPERIMENTS
4.2 Efficient3D-awarePortraitEditing
4.1 ExperimentalSetup
Wemakein-depthanalysisofourefficientportraiteditingsystem
TrainingSettings.Forrealfaces,weadopttheFFHQdataset[Kar- bothquantitativelyandqualitatively.Theresultsareincludedin
ras et al. 2019] at 512Ã—512 resolution with camera parameters Tab.1andFig.4.Forreferencealignment,wedonotreportğ¶ğ¿ğ¼ğ‘ƒ
ğ‘Ÿ
alignedbyEG3D[Chanetal.2022].Inordertoobtainthestylized formethodsthatleverageCLIPforoptimizationsinceitisevaluated
imagesaspseudolabelsandvisualprompts,weleverageInstruct- withCLIPaswell.Thestandoutfeatureofoursystemisitsefficiency.
Pix2Pix[Brooksetal.2023]toedittherealfaces.Specifically,given Ourmethodachievesaninferencespeedofmerely40ms,which
animageprompt,wefirstemployBLIP[Lietal.2022]forcaptioning improvesover100timescomparedtothefastestexistingbaselines,
andusetheobtainedtextastheinstructiontosynthesizeanedited whichrequirearound10seconds.Becauseoftheefficientknowledge
imagewithInstructPix2Pix.Wetotallyconductexperimentson20 distillation,ourapproachisalsogoodinpreservingtheidentity,
styles,andforeachstyle,wesynthesize1000images,resultingin adheremorecloselytothereferencepromptsandachievethebest
20,000imagepairsformodeltraining.Foreachstyle,weuse8tex- 3Dconsistency.AlthoughtheLive3D+CLIPachievethebestğ¼ğ·
ğ‘¡
tualpromptsfortrainingand5textualpromptsfortesting.Weadopt score,theeditedresultsarebasicallyunchangedcomparedtothe
alearningrateof5e-5andoptimizethemodelfor60kiterations input.WesuspectthatLive3Ddoesnothavealatentspaceandloses
withabatchsizeof32.Theentiretrainingprocedureiscompleted theeditingpriors,makingitchallengingfortheresultsofCLIP
overaperiodof40hoursutilizing8NVIDIAA100GPUs. optimizationtoalignwellwithprompts.ComparedwithPTI+CLIP
Baselines.Weconductcomparisonsagainsttwocategoriesofmeth- methods,ourmodelcanenableprecisealignmentwithinputand
odsthatachieveanalogousoutcomes:1)3Dconstructioncoupled prompts,whilethebaselinealwaysgenerateslow-fidelitytextures,
with3Dediting;2)2Deditingfollowedby3Dreconstruction.For andtheresultinggeometryhasmanyartifacts.Incontrasttothe2D
thefirstcategory,weimplementtwobaselineapproaches,wherein editingandsubsequent3Dreconstructionpipeline(IP2P+Live3D),
weleveragePTI[Roichetal.2022]orLive3D[Trevithicketal.2023] oursystemproducestexturesconsistentwiththepromptandalignsâ€¢ Bai,Q.etal
Recolor it to
a Gothic
color scheme.
Fig.4. Qualitativecomparisons.Wecomparetheresultsofseveralbaselineswithimagepromptsandtextprompts.Ineachcase,weincludetheedited
portraitsaswellastheirnovelviewrenderings.Ourmethodgenerateshigh-qualityeditedportraitswithbetter3Dqualityandalignmentwiththereferenced
prompts.
Novel Prompt Input 10s 1min 2min 5min Novel Views Novel Views
Fig.5. Novelpromptadaptation.Weshowtheintermediatetestingresultsat10s,1min,2minand5minduringadaptationwith10pairedtrainingimages.
betterwiththeinputstructure.Forexample,asthesecondsample orproducedusingtext-guidedimageeditingmodels.Theadaptation
inFig.4shows,ourmodelcanconsistentlyretainthehairstructure, processitselfisremarkablyfast,requiringonlyabout5minutesto
whileIP2P+Live3Dcannothandlethis.Thethirdsampleshowcases accomplishthelearningofthenovelprompts.InFig.5,wepresent
that IP2P + Live3D introduces texture artifacts on the face and theintermediatetestingresultsintheadaptationprocess.Ourmodel
cannotinheritthestyleofpromptsverywell. canquicklymasterthenovel-promptknowledgeinabout2minutes.
Withfurthertraining(e.g.,5minutes),theeditedresultsbecome
4.3 AdaptationtoCustomizedEditing
morestylized,demonstratingatrade-offbetweentheauthenticity
Withthetrainededitingnetwork,oursystemalreadysupportsa andthestylization.Uponcompletionoftheadaptation,thesystem
variety of 3D-aware editing styles. To expand the selection and allowsuserstoedittestinginputsinthesenewlylearnedstyles
betterconformtousersâ€™preferences,weofferanefficientmethod with a minimal inference time of 0.04s. This rapid performance
forfastadaptationtonewprompts.Userscanpersonalizetheediting indicatesthatthesystemiswell-optimizedforreal-timeapplications,
networkbyprovidingamodestsetof10referenceeditingpairs. providingaseamlessandefficientuserexperience.
Thesepairscanbeeitherhandpickedfromartist-createdexamples
tupnI
PILC+ITP
PILC+D3eviL
D3eviL+P2PI
sruO
ï¼‰s063ï¼ˆ
)s03(
)s01(
ï¼‰s40.0ï¼ˆReal-time3D-awarePortraitEditingfromaSingleImage â€¢
Input Novel Views Input w/o L2d w/ L2d
(a) (b)
Fig.6. Qualitativecomparisonforablationson(a)thedistillationlossof3DGAN(L 3ğ·)and(b)diffusionmodels(L 2ğ·).
Table2. Ablationstudyonthenumberofdatapairsusedfornovel
promptadaptation.WereportLPIPSscoreafter0.1min,1min,2min,and
5minfine-tuningforevaluation.
#Pair/Time 0.1min 1min 2min 5min
2 0.6191 0.5431 0.5213 0.5212
5 0.6148 0.5324 0.5064 0.4971
10 0.6076 0.5242 0.4983 0.4935
20 0.5981 0.5525 0.4954 0.4895
50 0.5891 0.5718 0.4922 0.4797
around2minutes.Asthenumberofprovideddatapairsincreases,
themodelcanadaptfasteratthebeginning.
4.5 Applications
Oneimportantapplicationofourpipelineistoeditstreamingvideos,
requiringaccuratereconstructionandefficienteditingsimultane-
ously.Wedemonstratetheeffectivenessofourpipelineontalking
Fig.7. Videoeditingresultswith3DPE.Weusetextprompts"Bronze facevideos,asillustratedinFig.7.Ourmethodaccuratelyrecon-
statue"toedittheinputvideo.Ourmethodcanaccuratelyreconstructchal- structsfacialexpressions,whileachievinghigh-qualityeditingre-
lengingfacialexpressionsandachievehigh-qualitynovelviewrenderings. sults.Sinceourmethodis3D-aware,wecannaturallyviewvideos
fromnovelperspectives.Thedemovideoisavailableinthesupple-
4.4 AblationStudies
mentarymaterials.Tobetterexperienceourmethod,wedesignan
Weanalyzeourmodelandvalidateourdesignchoicebyablating interactivesystemthatallowsusersforreal-timeediting,whichis
thecomponents. showninFig.8.Usersarerequiredtoprovideaprompttoindicate
L 3ğ· forDistillationofLive3D.Wecompareourmodeltrained thedesiredstyle,andaninputimagethatistobeedited.Byclicking
withandwithout L 3ğ·.AsshowninFig.6,weobservethatthe theâ€˜submitâ€™button,thesystemwillautomaticallyapplyourmethod
modelwithoutL 3ğ· resultsinveryflatgeometry,andthesynthesis onthegivenimageandprompt,andoutputreal-timeeditedimage
ofthesideviewlosesthenormalstructurewithmanyartifacts.This aswellasnovel-vieweditedresults.
highlights the importance of the 3D knowledge from Live3D in
5 DISCUSSION
trainingourmodelwith2Dpaireddata,enablingtheinferenceof
reasonablefacegeometrywithoutrelyingonany3Ddatasources. LimitationsandFuturework.Despiteachievingstate-of-the-art
L 2ğ· forDistillationofDiffusionModels.Wealsostudythe performanceintermsofqualityandefficiency,oursystemexhibits
effectofL 2ğ·.FromFig.6,weobservethatL 2ğ· iscriticalforour inconsistenciesinthedetailsfornovel-viewrendering.Thisoccurs
model, especially in preserving detailed texture. Without it, the becausetheEG3Dframeworkreliesonasuper-resolutionmodule.
editedimagetendstoloseappearanceinformationfromprompts Additionally,whenourmethodisappliedtovideoediting,itpresents
andexhibitsstructuralartifactsaroundthenose. flickeringartifactssinceourmodelisdesignedforper-frameediting.
AblationonNovelPromptAdaptations.Toinvestigatetheabil- Developingourmethodsforvideoeditingcouldbeaninteresting
ityofourmethodadaptedtonovelprompts,weconductanab- avenueoffutureresearch.
lationstudyonthenumberofnovel-promptdatapairsrequired Conclusion.Weintroduce3DPEforreal-time3D-awareportrait
fortraining.Wechoosethegoldenstatueasthenovelpromptfor editing.Bydistillingthepowerfulknowledgeofdiffusionmodels
demonstrationandperformadaptationwith2,5,10,20,or50pairs and3DGANsintoalightweightmodule,ourmodelsignificantly
ofnovel-promptdataonasingleA6000GPU.Weexperimentally reduceseditingtimewhileensuringquality.Additionally,ourmodel
findthatourmodelcanconvergeinabout5minutes,andtherefore, supportsfastadaptationtouser-specifiednovelprompts.Thead-
wereporttheperceptualdistancebetweenthemodeloutputand vantagesofourmethodempowerustogeneratephotorealistic3D
theprovidedgroundtruthat0.1min,1min,2min,and5minforcom- portraits,acapabilitycrucialforthevisualeffectsindustry,AR/VR
parison.AsshowninTab.2,ourmodelcanadapttothenewstylein systems,andteleconferencing,amongotherapplications.
tupnI
detidE
weiV
levoN
w/o
L3d
w/
L3dâ€¢ Bai,Q.etal
Fig.8. Theinteractiveinterfacethatallowsuserstocustomizetheirediting.
REFERENCES
2022.Efficientgeometry-aware3Dgenerativeadversarialnetworks.InIEEEConf.
RameenAbdal,YipengQin,andPeterWonka.2019.Image2StyleGAN:HowtoEmbed Comput.Vis.PatternRecog.
ImagesIntotheStyleGANLatentSpace?.InInt.Conf.Comput.Vis. EricRChan,MarcoMonteiro,PetrKellnhofer,JiajunWu,andGordonWetzstein.
RameenAbdal,YipengQin,andPeterWonka.2020.Image2StyleGAN++:HowtoEdit 2021.pi-gan:Periodicimplicitgenerativeadversarialnetworksfor3d-awareimage
theEmbeddedImages?.InIEEEConf.Comput.Vis.PatternRecog. synthesis.InIEEEConf.Comput.Vis.PatternRecog.
ShivangiAneja,JustusThies,AngelaDai,andMatthiasNieÃŸner.2023.Clipface:Text- SeunggyuChang,GihoonKim,andHayeonKim.2023.HairNeRF:Geometry-Aware
guidededitingoftextured3dmorphablemodels.InSIGGRAPH. ImageSynthesisforHairstyleTransfer.InInt.Conf.Comput.Vis.
QingyanBai,YinghaoXu,JiapengZhu,WeihaoXia,YujiuYang,andYujunShen.2022. JiankangDeng,JiaGuo,NiannanXue,andStefanosZafeiriou.2019a.Arcface:Additive
High-fidelityGANinversionwithpaddingspace.InEur.Conf.Comput.Vis. angularmarginlossfordeepfacerecognition.InIEEEConf.Comput.Vis.Pattern
VolkerBlanzandThomasVetter.2023. Amorphablemodelforthesynthesisof3D Recog.
faces.InSeminalGraphicsPapers:PushingtheBoundaries,Volume2. YuDeng,JiaolongYang,DongChen,FangWen,andXinTong.2020.Disentangledand
TimBrooks,AleksanderHolynski,andAlexeiAEfros.2023.Instructpix2pix:Learning controllablefaceimagegenerationvia3dimitative-contrastivelearning.InIEEE
tofollowimageeditinginstructions.InIEEEConf.Comput.Vis.PatternRecog. Conf.Comput.Vis.PatternRecog.
ShengquCai,AntonObukhov,DengxinDai,andLucVanGool.2022.Pix2NeRF:Unsu- YuDeng,JiaolongYang,SichengXu,DongChen,YundeJia,andXinTong.2019b.
pervisedConditionalp-GANforSingleImagetoNeuralRadianceFieldsTranslation. Accurate3dfacereconstructionwithweakly-supervisedlearning:Fromsingle
InIEEEConf.Comput.Vis.PatternRecog. imagetoimageset.InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang patternrecognitionworkshops.
Zheng.2023.MasaCtrl:Tuning-FreeMutualSelf-AttentionControlforConsistent AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xiaohua
ImageSynthesisandEditing.arXivpreprintarXiv:2304.08465(2023). Zhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,
DuyguCeylan,Chun-HaoPHuang,andNiloyJMitra.2023.Pix2video:Videoediting SylvainGelly,etal.2020.Animageisworth16x16words:Transformersforimage
usingimagediffusion.InProceedingsoftheIEEE/CVFInternationalConferenceon recognitionatscale.arXivpreprintarXiv:2010.11929(2020).
ComputerVision.23206â€“23217. ChenGao,YichangShih,Wei-ShengLai,Chia-KaiLiang,andJia-BinHuang.2020.
WenhaoChai,XunGuo,GaoangWang,andYanLu.2023. Stablevideo:Text-driven Portraitneuralradiancefieldsfromasingleimage.arXivpreprintarXiv:2012.05903
consistency-awarediffusionvideoediting.InProceedingsoftheIEEE/CVFInterna- (2020).
tionalConferenceonComputerVision.23040â€“23050. MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel.2023.Tokenflow:Consistent
EricRChan,ConnorZLin,MatthewAChan,KokiNagano,BoxiaoPan,Shalini diffusionfeaturesforconsistentvideoediting. arXivpreprintarXiv:2307.10373
DeMello,OrazioGallo,LeonidasJGuibas,JonathanTremblay,SamehKhamis,etal. (2023).Real-time3D-awarePortraitEditingfromaSingleImage â€¢
AlexandrosGraikos,NikolayMalkin,NebojsaJojic,andDimitrisSamaras.2022.Diffu- DanielRoich,RonMokady,AmitHBermano,andDanielCohen-Or.2022. Pivotal
sionmodelsasplug-and-playpriors.Adv.NeuralInform.Process.Syst.(2022). tuningforlatent-basededitingofrealimages.ACMTrans.Graph.(2022).
JiataoGu,LingjieLiu,PengWang,andChristianTheobalt.2022.Stylenerf:Astyle- RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
based3d-awaregeneratorforhigh-resolutionimagesynthesis.InInt.Conf.Learn. 2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.InIEEEConf.
Represent. Comput.Vis.PatternRecog.
JiaGuo,JinkeYu,AlexandrosLattas,andJiankangDeng.2022.Perspectivereconstruc- NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
tionofhumanfacesbyjointmeshandlandmarkregression.InEur.Conf.Comput. Aberman.2023.Dreambooth:Finetuningtext-to-imagediffusionmodelsforsubject-
Vis. drivengeneration.InIEEEConf.Comput.Vis.PatternRecog.
AyaanHaque,MatthewTancik,AlexeiAEfros,AleksanderHolynski,andAngjoo KatjaSchwarz,YiyiLiao,MichaelNiemeyer,andAndreasGeiger.2020.Graf:Generative
Kanazawa.2023. Instruct-nerf2nerf:Editing3dsceneswithinstructions. arXiv radiancefieldsfor3d-awareimagesynthesis.InAdv.NeuralInform.Process.Syst.
preprintarXiv:2303.12789(2023). KatjaSchwarz,AxelSauer,MichaelNiemeyer,YiyiLiao,andAndreasGeiger.2022.
ErikHÃ¤rkÃ¶nen,AaronHertzmann,JaakkoLehtinen,andSylvainParis.2020.Ganspace: Voxgraf:Fast3d-awareimagesynthesiswithsparsevoxelgrids.Adv.NeuralInform.
Discoveringinterpretablegancontrols.Adv.NeuralInform.Process.Syst.(2020). Process.Syst.(2022).
KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollÃ¡r,andRossGirshick. YujunShen,JinjinGu,XiaoouTang,andBoleiZhou.2020a. Interpretingthelatent
2022.Maskedautoencodersarescalablevisionlearners.InIEEEConf.Comput.Vis. spaceofGANsforsemanticfaceediting.InIEEEConf.Comput.Vis.PatternRecog.
PatternRecog. YujunShen,CeyuanYang,XiaoouTang,andBoleiZhou.2020b. InterFaceGAN:In-
AmirHertz,KfirAberman,andDanielCohen-Or.2023.Deltadenoisingscore.InInt. terpretingtheDisentangledFaceRepresentationLearnedbyGANs. IEEETrans.
Conf.Comput.Vis. PatternAnal.Mach.Intell.(2020).
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, Zifan Shi, Sida Peng, Yinghao Xu, Andreas Geiger, Yiyi Liao, and Yujun Shen.
LuWang,andWeizhuChen.2021.Lora:Low-rankadaptationoflargelanguage 2022a. Deepgenerativemodelson3drepresentations:Asurvey. arXivpreprint
models.arXivpreprintarXiv:2106.09685(2021). arXiv:2210.15663(2022).
JunhaHyung,SungwonHwang,DaejinKim,HyunjiLee,andJaegulChoo.2023.Local ZifanShi,YujunShen,YinghaoXu,SidaPeng,YiyiLiao,ShengGuo,QifengChen,
3DEditingvia3DDistillationofCLIPKnowledge.InIEEEConf.Comput.Vis.Pattern andDit-YanYeung.2023.Learning3d-awareimagesynthesiswithunknownpose
Recog. distribution.InIEEEConf.Comput.Vis.PatternRecog.
KaiwenJiang,Shu-YuChen,HongboFu,andLinGao.2023.NeRFFaceLighting:Implicit ZifanShi,YinghaoXu,YujunShen,DeliZhao,QifengChen,andDit-YanYeung.2022b.
andDisentangledFaceLightingRepresentationLeveragingGenerativePriorin Improving3d-awareimagesynthesiswithageometry-awarediscriminator.Adv.
NeuralRadianceFields.ACMTrans.Graph.(2023). NeuralInform.Process.Syst.(2022).
TeroKarras,SamuliLaine,andTimoAila.2019.Astyle-basedgeneratorarchitecture IvanSkorokhodov,SergeyTulyakov,YiqunWang,andPeterWonka.2022. Epigraf:
forgenerativeadversarialnetworks.InIEEEConf.Comput.Vis.PatternRecog. Rethinkingtrainingof3dgans.InAdv.NeuralInform.Process.Syst.
JaehoonKo,KyusunCho,DaewonChoi,KwangrokRyoo,andSeungryongKim.2023. JingxiangSun,XuanWang,YichunShi,LizhenWang,JueWang,andYebinLiu.2022a.
3dganinversionwithposeoptimization.InIEEEWinterConf.Appl.Comput.Vis. Ide-3d:Interactivedisentanglededitingforhigh-resolution3d-awareportraitsyn-
NupurKumari,BingliangZhang,RichardZhang,EliShechtman,andJun-YanZhu. thesis.ACMTrans.Graph.(2022).
2023.Multi-conceptcustomizationoftext-to-imagediffusion.InProceedingsofthe JingxiangSun,XuanWang,YongZhang,XiaoyuLi,QiZhang,YebinLiu,andJueWang.
IEEE/CVFConferenceonComputerVisionandPatternRecognition.1931â€“1941. 2022b. Fenerf:Faceeditinginneuralradiancefields.InIEEEConf.Comput.Vis.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi.2022. Blip:Bootstrapping PatternRecog.
language-imagepre-trainingforunifiedvision-languageunderstandingandgenera- AlexTrevithick,MatthewChan,MichaelStengel,EricChan,ChaoLiu,ZhidingYu,
tion.InInt.Conf.Mach.Learn. SamehKhamis,ManmohanChandraker,RaviRamamoorthi,andKokiNagano.2023.
JianhuiLi,JianminLi,HaojiZhang,ShilongLiu,ZhengyiWang,ZihaoXiao,Kaiwen Real-timeradiancefieldsforsingle-imageportraitviewsynthesis. ACMTrans.
Zheng,andJunZhu.2023a. PREIM3D:3DConsistentPreciseImageAttribute Graph.(2023).
EditingfromaSingleImage.InIEEEConf.Comput.Vis.PatternRecog. QiuyuWang,ZifanShi,KechengZheng,YinghaoXu,SidaPeng,andYujunShen.
JianhuiLi,ShilongLiu,ZidongLiu,YikaiWang,KaiwenZheng,JinghuiXu,JianminLi, 2023b.BenchmarkingandAnalyzing3D-awareImageSynthesiswithaModularized
andJunZhu.2023b.InstructPix2NeRF:Instructed3DPortraitEditingfromaSingle Codebase.arXivpreprintarXiv:2306.12423(2023).
Image.arXivpreprintarXiv:2311.02826(2023). WenWang,YanJiang,KangyangXie,ZideLiu,HaoChen,YueCao,XinlongWang,and
ConnorZLin,DavidBLindell,EricRChan,andGordonWetzstein.2022. 3dgan ChunhuaShen.2023a.Zero-shotvideoeditingusingoff-the-shelfimagediffusion
inversionforcontrollableportraitimageanimation.arXivpreprintarXiv:2203.13441 models.arXivpreprintarXiv:2303.17599(2023).
(2022). YoujiaWang,TengXu,YiwenWu,MinzhangLi,WenzhengChen,LanXu,andJingyi
ShaotengLiu,YuechenZhang,WenboLi,ZheLin,andJiayaJia.2023b. Video-p2p: Yu.2022.NARRATE:ANormalAssistedFree-ViewPortraitStylizer.arXivpreprint
Videoeditingwithcross-attentioncontrol.arXivpreprintarXiv:2303.04761(2023). arXiv:2207.00974(2022).
ZhihengLiu,RuiliFeng,KaiZhu,YifeiZhang,KechengZheng,YuLiu,DeliZhao, WeihaoXiaandJing-HaoXue.2023.ASurveyonDeepGenerative3D-awareImage
JingrenZhou,andYangCao.2023a.Cones:Conceptneuronsindiffusionmodels Synthesis.Comput.Surveys(2023).
forcustomizedgeneration.arXivpreprintarXiv:2303.05125(2023). JiaxinXie,HaoOuyang,JingtanPiao,ChenyangLei,andQifengChen.2023. High-
ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,and fidelity3DGANInversionbyPseudo-multi-viewOptimization.InIEEEConf.Comput.
StefanoErmon.2021.Sdedit:Guidedimagesynthesisandeditingwithstochastic Vis.PatternRecog.
differentialequations.arXivpreprintarXiv:2108.01073(2021). YinghaoXu,SidaPeng,CeyuanYang,YujunShen,andBoleiZhou.2022.3D-aware
Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira ImageSynthesisviaLearningStructuralandTexturalRepresentations.InIEEEConf.
Kemelmacher-Shlizerman.2022. Stylesdf:High-resolution3d-consistentimage Comput.Vis.PatternRecog.
andgeometrygeneration.InIEEEConf.Comput.Vis.PatternRecog. YinghaoXu,YujunShen,JiapengZhu,CeyuanYang,andBoleiZhou.2021.Generative
HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng, hierarchicalfeaturesfromsynthesizingimages.InIEEEConf.Comput.Vis.Pattern
XiaoweiZhou,QifengChen,andYujunShen.2023.Codef:Contentdeformation Recog.
fieldsfortemporallyconsistentvideoprocessing.arXivpreprintarXiv:2308.07926 ShuaiYang,YifanZhou,ZiweiLiu,andChenChangeLoy.2023. RerenderAVideo:
(2023). Zero-ShotText-GuidedVideo-to-VideoTranslation.arXivpreprintarXiv:2306.07954
XingangPan,XudongXu,ChenChangeLoy,ChristianTheobalt,andBoDai.2021. (2023).
Ashading-guidedgenerativeimplicitmodelforshape-accurate3d-awareimage FeiYin,YongZhang,XuanWang,TengfeiWang,XiaoyuLi,YuanGong,YanboFan,
synthesis.Adv.NeuralInform.Process.Syst.(2021). XiaodongCun,YingShan,CengizOztireli,etal.2023.3dganinversionwithfacial
PascalPaysan,ReinhardKnothe,BrianAmberg,SamiRomdhani,andThomasVetter. symmetryprior.InIEEEConf.Comput.Vis.PatternRecog.
2009.A3Dfacemodelforposeandilluminationinvariantfacerecognition.InIEEE LvminZhang,AnyiRao,andManeeshAgrawala.2023.Addingconditionalcontrolto
internationalconferenceonadvancedvideoandsignalbasedsurveillance. text-to-imagediffusionmodels.InInt.Conf.Comput.Vis.
BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.2022. Dreamfusion: RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
Text-to-3dusing2ddiffusion.arXivpreprintarXiv:2209.14988(2022). Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InIEEEConf.
ChenyangQi,XiaodongCun,YongZhang,ChenyangLei,XintaoWang,YingShan, Comput.Vis.PatternRecog.
andQifengChen.2023.Fatezero:Fusingattentionsforzero-shottext-basedvideo JiapengZhu,YujunShen,DeliZhao,andBoleiZhou.2020.In-domainGANinversion
editing.arXivpreprintarXiv:2303.09535(2023). forrealimageediting.InEur.Conf.Comput.Vis.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini Jun-YanZhu,PhilippKrÃ¤henbÃ¼hl,EliShechtman,andAlexeiAEfros.2016.Generative
Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021. visualmanipulationonthenaturalimagemanifold.InEur.Conf.Comput.Vis.
Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInt.Conf.
Mach.Learn.â€¢ Bai,Q.etal
Input Prompt Edited Novel Views
Add dark
Goth makeup
and clothes
Present it with
the solidity of a
bronze statue.
Fig.9. Additionalqualitativeresults.