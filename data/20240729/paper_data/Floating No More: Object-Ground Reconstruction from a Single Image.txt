Floating No More: Object-Ground Reconstruction from a Single Image
YunzeMan1,YichenSheng2,JianmingZhang3,Liang-YanGui1,Yu-XiongWang1
1UniversityofIllinoisUrbana-Champaign 2PurdueUniversity 3Adobe
{yunzem2,lgui,yxw}@illinois.edu, sheng30@purdue.edu, jianmzha@adobe.com
Figure 1. Our proposed ORG (Object Reconstruction with Ground) model simultaneously reconstructs a 3D object, estimates camera
parameters,andmodelstheobject-groundrelationshipfromamonocularimage.Duringshadowandreflectiongeneration,thepriordepth-
basedobjectgeometryestimationmethodcanresultinfloatingissueoranunnaturalshadowontheground,asdemonstratedinredboxes.
Ourmethod,ontheotherhand,achievessignificantlymorerealisticeditingandgeneration,asshowninblueboxes.
Abstract in the realm of image editing applications, where it influ-
ences key aspects like controllable shadow/reflection syn-
Recent advancements in 3D object reconstruction from
thesisandobjectviewmanipulation. Inthiswork, weaim
singleimageshaveprimarilyfocusedonimprovingtheac-
at predicting an accurate and grounded representation of
curacy of object shapes. Yet, these techniques often fail
objects in 3D space from a single image, specifically un-
toaccuratelycapturetheinter-relationbetweentheobject,
derunrestrictedcameraconditions. Recentsingle-viewap-
ground,andcamera. Asaresult,thereconstructedobjects
proaches have demonstrated considerable promise in tack-
oftenappearfloatingortiltedwhenplacedonflatsurfaces.
lingobjectreconstruction[26,34,39,56,60].However,due
This limitation significantly affects 3D-aware image edit-
tothelackofintegratedobject-groundmodeling,objectsre-
ingapplicationslikeshadowrenderingandobjectposema-
constructed using these methods often appear to be ‚Äúfloat-
nipulation. To address this issue, we introduce ORG (Ob-
ing‚Äù or tilted when placed on a flat surface, which greatly
ject Reconstruction with Ground), a novel task aimed at
hinderstherealisticrendering.
reconstructing3Dobjectgeometryinconjunctionwiththe
ground surface. Our method uses two compact pixel-level
Morespecifically,recentworksonmonoculardepthes-
representations to depict the relationship between camera,
timation[6,7,34,60]hasshowngreatperformance. They
object, and ground. Experiments show that the proposed
aim to recover the 3D information of an object from a
ORG model can effectively reconstruct object-ground ge-
single-view image by directly estimating the pixel-level
ometryonunseendata, significantlyenhancingthequality
depth values. Their models have been trained on large-
of shadow generation and pose manipulation compared to
scale datasets, and thus can generalize well on in-the-wild
conventionalsingle-image3Dreconstructiontechniques.
images. However, as pointed out by [60], to project the
depthmapinto3Dpointclouds, additionalcameraparam-
eters are needed. In some cases, off-the-shelf estimators
1.Introduction
can provide a rough estimate of these parameters, but this
The task of reconstructing an object in conjunction with a approach can limit the flexibility and effectiveness of ob-
physicallyplausibleground,whilenotextensivelyexplored, ject reconstruction in uncontrolled environments. More-
is of significant importance. This is particularly relevant over, the unknown shift in the depth or disparity map will
1
4202
luJ
62
]VC.sc[
1v41981.7042:viXraMulti-view Latent Depth NVS Ours
singleimage ‚úó ‚úì ‚úì ‚úì ‚úì
category-free ‚úì ‚úó ‚úì ‚úì ‚úì
camera-aware ‚úì ‚úó ‚úó ‚úó ‚úì
ground-aware ‚úó ‚úó ‚úó ‚úó ‚úì
Table 1. ORG processes multiple advantages from flexibility to
generalization against multi-view reconstruction work and other
single-view work including generation from latent embedding,
monocular depth estimation, and novel-view synthesis (NVS)
methods.
Figure 2. Without modeling object-ground correlation, existing
single-view3Destimationmethod[34]generates3Dmodelsfloat-
ingortiltedontheground. unseen datasets, including objects and humans, and show
qualitativeresultsonrandomunseenwebimages. Ourpro-
cause distortion in the3D reconstruction (see Figure 2 top posedmethodoutperformsexistingmethodsintermsofac-
row). Without an explicit modeling of the object-ground curacy,robustness,andefficiencyinvariousscenarios. Re-
relationship, recovered 3D objects are often hard to place sults show that our method achieves superior performance
on a flat support plane (see Figure 2 bottom row). These andprovidesamorecomprehensiveandlight-weightsolu-
challenges are also present in recent category-specific 2D- tion to the challenges of single-view object geometry esti-
to-3D methods that recover 3D shape from latent embed- mation. Insummary,ourmaincontributionsareasfollows.
ding space [9, 38, 39, 46, 52] and zero-shot novel-view
‚Ä¢ A novel framework ORG, for in-the-wild single-view
synthesis methods [25, 26, 29, 33, 43, 56], where they of-
object-ground 3D geometry estimation. To the best of
tenjustassumeasimpleorthographiccameramodel,oras-
our knowledge, this is the first method to jointly model
sume the camera parameter being given as input to avoid
object,camera,andgroundplanefromsingleimage.
over-complicationoftheproblem,whichontheotherhand
‚Ä¢ We propose a perspective field guided pixel height re-
limitstheirapplicationinunconstrainedscenarios.
projection module to efficiently convert our estimated
To address these challenges, we propose ORG (Object representations into common depth maps and point
Reconstruction with the Ground), a new formulation for clouds.
representingobjectsinrelationtotheground. Givenasin- ‚Ä¢ ORG achieves outstanding shadow generation and re-
gleimage,ourobjectiveistosimultaneouslydeducethe3D construction performance on unseen real-world images,
shape of the object, its positioning relative to the ground demonstratinggreatrobustnessandgeneralizationability.
plane,andthecameraparameters. Wecompareourmethod
withthreeexistingresearchstrands:depthestimation,latent
2.RelatedWork
embeddingreconstruction, anddiffusion-basednovel-view
synthesis, in addition to multi-view reconstruction tech- Single-viewDepthEstimation. Therehasbeensignificant
niques, as detailed in Table 1. Existing single-view meth- progress made in recenttimes in the estimation of monoc-
odsoftenfailtomaintaintheobject-groundrelationshipand ular depth [7, 8, 12, 34, 44, 60]. Given metric depth su-
usually presuppose known camera parameters or rely on pervision, someworkdirectlytrainstheirmodeltoregress
overlysimplisticcameramodels,leadingtosuboptimalper- the depth objective [12, 24, 59, 60]. While these methods
formancefortaskslikeefficientshadowgeneration.Instark achievegreatperformanceonvariousdatasets,thedifficulty
contrast, the output of our model supports the intricate in- ofobtainingmetricgroundtruthdepthhinderstheuseofdi-
terplaybetweenobject,ground,andcamera(seeFigure1), rectdepthsupervision. Instead,anotherlineofworkrelies
facilitatingsuperiorshadowgenerationandpose-awarege- on ranking losses, which evaluates relative depth [6, 53],
ometric reconstruction. To this end, we model the object orscale-andshift-invariantlosses[34,44]forsupervision.
as consisting of its front (visible) and back surfaces, and The latter methods produce particularly robust depth pre-
predict two pixel-level height map between the object and dictions without heavy annotation efforts, but the models
the ground [41], along with a dense camera parameter de- are not able to reason object-ground relationship and of-
scriptor[16]. Ourresultsdemonstratethatsuchasimplified ten produce unrealistic results when using depth map for
representationofobjectsisnotonlyadequateforgenerating downstream image editing tasks. In light of this, a recent
3D-realisticshadowsbutalsoyieldsconvincingreconstruc- work [41] proposes another annotation-friendly represen-
tionforawidearrayofcommonlyencounteredobjects. tation, pixel height, for better object shadow generation.
We create our training data from Objaverse [10], ren- However, this method has strict constraints on the camera
deringsiximagesforeachobjectwithdiversefocallength viewpoint. Werepurposetherepresentationformonocular
andcameraviewpoint. Weevaluateourmethodacrosstwo 3Dreconstructionandloosentheviewpointbyjointmodel-
2ingcamerawithobjectgeometry. byproposingaperspective-guidedpixelheightreprojection
method(Sec3.3).
Single-view3DGeometryReconstruction. Reconstruct-
ing object shapes from single-view image is a challeng-
3.1.Object,Ground,andCameraRepresentations
ing but well-established problem, with one of its semi-
nal work [37] optimizing for 6-DoF poses of objects with PixelHeightRepresentation. Proposedforsingle-image
known3Dmodels. Intheensuingdecades,learning-based shadow generation [41, 42], pixel height is a dense repre-
methodshavebeguntoproposecategory-specificnetworks sentation defined as the pixel distance between a point on
for3Destimationthatspanawiderangeofobjectswith[4, anobjectanditsgroundprojection,namelyitsverticalpro-
18] and without direct 3D supervision [14, 17, 20, 57], jection on the ground in the image, as we can see in Fig-
and using neural implicit representations [30, 58]. With ure3. Itisapixel-levelscalarwhichmeasuresthedistance
robust 3D supervision, recent methods have demonstrated betweenobjectanditssupportingplaneintheimagecoor-
the feasibility of learning 3D geometry with limited mem- dinate(numberofpixels, ratherthanmeters). Pixelheight
ory. Pixel2Mesh [46] offers a method to reconstruct the possesses many advantages over the depth representation
3D shape with mesh using a single image input. Mean- in modeling the object geometry. First, it is disentangled
while, PIFu[38,39]offersanefficientimplicitfunctionto from the camera model, and thus can be directly inferred
recoverhigh-resolutionsurfacesofhumans, includingpre- from the images context without additional camera infor-
viouslyunseenandoccludedregions.Whileachievinggreat mation. Moreover, it models the object and ground rela-
performance, some of these works rely on learning priors tionship,whichispivotalingeneratingrealistic3Dmodels
specific to a certain object category, hindering its general- forreal-worldimageapplications,asobjectsalmostalways
izabilty in the wild. Recently, advances in text-to-3D gen- haveacanonicalpositiononthegroundplane.
eration[5,22,32,49]alsoinspireimage-to-3Dgeneration While photo-realistic shadows can be generated from
using diffusion prior [25, 26, 29, 33, 43, 56]. Masked au- pixelheightmapwithprojectivegeometry,weseemorepo-
toencodersarealsousedtoobjectreconstructionfromsin- tential in this new representation. Constraining the object
gleimage[51]. Incomparison, ourmethodisthefirstone locationwithrespecttoa2Dplane, thepixelheightrepre-
tomodeltheobjectgeometrywithrespecttothegroundfor sentationplaysacriticalroleinreconstructing3Dshapeof
efficientimageeditingand3Dreconstruction. objectsontopoftheground. Moreover,strictrequirements
areenforcedoncameraviewpointsforpixelheight[41],and
Camera Parameter Estimation. An essential aspect of
onlythefrontsurfaceofaobjectisconsidered. Therefore,
single-view monocular 3D object comprehension is to re-
weproposetoloosenthisconditionbymodelingbothfront
trieve the focal length of a camera and the camera pose
andbacksurfacesoftheobject,andjointlypredictingcam-
relative to the object and the ground plane. Classic meth-
eraintrinsicandposerelativetotheground. Intheend,the
ods leverage reference image components, including cali-
field-of-view(FoV)isusedtoliftpixeldistancesintometric
brationgrids[61]orvanishingpoints[11],toestimatecam-
distance, and camera viewpoint helps align the object into
eraparameters.Recently,data-drivenapproacheshavebeen
thecanonicalposerelativetotheground.
proposed to use deep neural networks to infer the focal
length[15,50]andcameraposes[19,28,54]directlyfrom Perspective Field Representation. As shown in Fig-
in-the-wild images, or to use dense representation [16] to ure3,theperspectivefieldrepresentationofagivenimage
encodecameraparametersforamorerobustestimation. In iscomposedoftwodensefields,alatitudefieldrepresented
contrast, our method ORG jointly estimates intrinsic and by blue contour lines, and a up-vector field represented
extrinsiccameraparameterstogetherwithobjectgeometry by green arrows [16]. Specifically, assuming a camera-
and ground positions, achieving a self-contained pipeline centered spherical coordinate system where the zenith di-
for3D-awareimageeditingandreconstruction. rectionisoppositetogravity. ThecameramodelKprojects
a 3D position X ‚àà R3 in the spherical coordinate into the
3.Approach image frame x ‚àà R2. For each pixel location x, the up-
vectorisdefinedastheprojectionofthetangentialdirection
ORGconsiderssingle-viewobjectgeometryestimationby ofX alongthemeridiantowardsthenorthpole,thelatitude
jointpixelheight andperspectivefield prediction. Wepro- isdefinedastheanglebetweenthevectorpointingfromthe
vide an overview of our framework in Figure 3. Model- cameratoX andthegroundplane. Inotherwords,thelati-
ingobjectgeometryandcameraparametersasdensefields, tudefieldandtheup-vectorfieldencodetheelevationangle
we first introduce the background knowledge of the dense and the roll angle of the points on the object, respectively.
object-ground and dense camera representations (Sec 3.1). Bothperspectivefieldsandpixelheightmapareinvariantor
We learn a pyramid vision transformer (PVT) [47, 48] equivarianttoimageeditingoperationslikecropping,rota-
to predict the dense representation fields (Sec 3.2), and tionandtranslation. Asaresult,theyarehighlysuitablefor
prove that they can be repurposed for reconstruction task neuralnetworkmodelsdesignedfordensepredictiontasks.
33D Geometry (Pose Change)
Back
Front Surface
Surface
Pixel Height
d e d
iu
Gn o itc
e
jo
EstJ imoin att ion
d
le iF
e
ve R
thrp
-
itc
e p
sre
Pg
ie
H
le
x
iP
RGB Input
Perspective Field
Image Editing (Reflection, (Ground Aware) depth map
Shadow generation, etc.)
Figure3. ORGParadigm. Ourproposedmethodisabletotakeasingle-viewobject-centricimageasinput, andjointlyestimatetwo
denserepresentations,thepixelheightandperspectivefield,encodingtheobject-groundrelationshipandcameraparameters,respectively.
APerspectiveFieldGuidedPixelHeightRe-projectionmoduleisproposedtorepurposethetwopredicteddensefieldsintodepthmap
estimationandpointcloudgeneration.
3.2.DenseFieldEstimation fields. Wefurthermakemodificationstothedecoderhead,
enablingittoproducearegressionvalueforthepixelheight
We present a neural network model to estimate the two
map,upfieldmap,andlatitudefieldmap. WeusePVTv2-
dense fields from a single image. The per-pixel structure
b3pretrainedonCOCOdataset[23]asthebackboneofour
andtranslation-invariantnatureofthepixelheightandper-
architecture. The model is trained with AdamW [27] op-
spectivefieldrepresentationsmakethemhighlysuitablefor
timizer with learning rate 0.0005 and weight decay 1e-2
neural network prediction. Following [34, 60], we formu-
for60Kstepswithbatchsize8ona4-A100machine. We
latethedensefieldestimationtaskasaregressionproblem.
schedule the multi-step training stages at step 30K, 40K,
Specifically, for each image pixel of the pixel height field,
and 50K, with learning rate decreases by 10 time at each
assumingaraystartingfromacamerapointingtowardsthe
stage. Weresizetheimageto(512,512)beforeusinghor-
pixeltravelsthroughtheobject,therewillbeanentrypoint
izontal flipping, random cropping, and color jittering aug-
on the front surface of the object p and an exit point on
f mentationduringtraining.
thebacksurfaceoftheobjectp . Whentheraypassesthe
b
surfacesofobjectmultipletimes,weonlyconsiderthefirst 3.3.Perspective-GuidedPixelHeightReprojection
entry and last exit. The model is then asked to predict the
pixel height for both p and p . Moreover, we normalize After predicting two dense representations, we prove that
f b
they encode sufficient information to be efficiently con-
itwiththeheightoftheinputimage. Forlatitudefield,we
normalize the original [‚àíœÄ/2,œÄ/2] range into [0,1]. And verted into depth maps and point clouds for downstream
fortheup-vectorfield,eachangleŒ∏canrangefrom0to2œÄ, tasksandforfaircomparisonwithexistingmethods. First,
sincetheperspectivefieldcanbegeneratedfromcamerapa-
so direct normalization and regression pose ambiguity for
themodelsince0and2œÄrepresentthesameangle. Hence, rameters,wediscretizethecontinuousparameterrangeand
we represent each angle Œ∏ with a tuple (sinŒ∏,cosŒ∏), and use a grid search optimization strategy to estimate camera
field-of-viewŒ±andextrinsicrotationmatrixRasrowand
trainthemodeltoregresstoatwo-channelvectormap. All
regressiontasksaretrainedwithloss‚Ñì . pitchangles. Afterwards,thecamerafocallengthiscalcu-
2
lated as f = H , where H is the height of the input
Model Architecture and Training Details. We use the 2tanŒ±/2
image. ThentheintrinsicmatrixKisalsoestimatedas:
architecture of PVTv2-b3 [48] as our backbone to extract
joint feature map. We use SegFormer [55] with the Mix Ô£Æ f 0 c Ô£π
x
Transformer-B3 as our decoder. Residual connection is K =Ô£∞0 f c yÔ£ª, (1)
added before the decoder to include lower-level context 0 0 1
from the 2-layer CNN block. We find that transformer-
basedencoderissuitableforourtaskasiteffectivelymain- where(c ,c )istheprinciplepointoftheimageandisusu-
x y
tains global consistency in the two dense representation allyestimatedtobethecenteroftheimage.
4ùêë‚àí1ùêä‚àí1 d‚ãÖùê©im =ùêèworld=d‚ãÖ(X,Y,Z) (2) ùêèworld 4.Experiments
ùêë‚àí1ùêä‚àí1 d‡∑®‚ãÖùê©‡∑•im =ùêè‡∑©world=d‡∑®‚ãÖ(X‡∑©,Y‡∑©,Z‡∑®) (3)
Inthissection, weconductextensivequalitativeandquan-
Input pixel: ùê©im titative experiments to demonstrate the effectiveness and
Obtainablefrom PField and PixHt: ùêë,ùêä,ùê©‡∑•im generalizabilityofORG.Weevaluateourmodelwithclas-
ùê©im
Constraint 1: d‡∑®‚ãÖZ‡∑®=Constant sic depth estimation metric and point cloud reconstruction
Constraint 2: d‚ãÖ X,Y =d‡∑®‚ãÖ(‡∑©X,‡∑©Y) metriconbothobject-centricimagesandhuman-centricim-
Objective: Solve forùêè‡∑©world ages. We show that repurposing two dense representation
predictionsleadstoaveryrobust3Dreconstructionframe-
ùêè‡∑©world workfordiversecategoriesandviewpointsofimages.
4.1.DataRendering
ùê©‡∑•im
Figure4. Perspective-GuidedPixelHeightReprojection. PField Existingobject-centricdatasets[1,36]donotprovideaccu-
andPixHtareperspectivefieldandpixelheight,respectively. rate depth map and object-ground rotation information si-
multaneously. Hence,werenderalarge-scaledatasetfrom
Derivation. AnillustrationisprovidedinFigure4. Given Objaverse [10]. Objaverse is a large-scale object-centric
onepixelpim =(x,y)‚ààR2,weknownitsverticalprojec-
dataset consisting of over 800K high-quality 3D models.
tion point pÀúim = (xÀú,yÀú) ‚àà R2 on the ground in the image For each object in the dataset, we randomly sample 6 sets
frame,givenbytheestimatedpixelheightmap. Recallthat of camera intrinsic and extrinsic parameters (FoV and ro-
intrinsicandextrinsicmatricescanbeusedtoprojecta3D tation matrix), each is used to render an RGB image with
point P i in the world coordinate into an image pixel p i. pixel height and perspective field ground truth maps. The
Morespecifically,givenintrinsicmatrixKandextrinsicro- image dimension is (512,512). The camera always points
tationmatrixR,wehavethefollowingequationsdescribing at the center of the object and the z-axis of world coor-
the correspondence between a pixel pim on the object and dinates points orthogonally to the ground plane. We use
itscorresponding3DpointsPworldintheworldcoordinate:
a physically-based renderer Blender [3] to render realistic
object: R‚àí1K‚àí1(d¬∑pim)=Pworld =d¬∑(X,Y,Z) (2) surface appearance and develop a CUDA-based ray-tracer
to efficiently render front and back surface pixel heights.
ground: R‚àí1K‚àí1(dÀú¬∑pÀúim)=PÀúworld =dÀú¬∑(XÀú,YÀú,ZÀú) (3)
We conduct corrupt data filtering to remove images with
where d is the depth value of the point. Here, the point incorrect annotations and images with objects that are too
pÀúim in Eq. (3) is the vertical projection of the ground of smallonthecanvas. Thisresultsin3,364,052imagesinthe
pim in Eq. (2). For a given pixel pim, its corresponding dataset in total. We split the objects into train/val/test sets
pÀúim can be obtained from our estimated vertical direction in8:1:1. Wealsorandomizetheintensity,position,number
(perspectivefield)andtheestimatedpixelheight. Notethat of light sources, and distance between camera and object
the world coordinate has its Z axis pointing vertically up- to increase the dataset diversity. We will release our data
wards, and its XY plane parallel to the ground plane. The renderingscript andrendereddataset. Moredetailsofthe
objective is to obtain the location of the reconstructed 3D implementationandthedatasetareinthesupplementary.
pointPworld =d¬∑(X,Y,Z),andtoeliminatetheunknown
4.2.Baselines
depth d, we need two additional constraints with the help
of Eq. (3). The constraintone is that all 3D points PÀúworld We compare our method with single-view depth estima-
on the ground have a constant z-axis value. Without loss tion,image-to-3Dreconstruction,andcameraparameteres-
ofgenerality,weassumethattheconstantisone,toobtain timation work. For depth estimation work, we compare
a scale-invariant 3D point cloud. This gives us dÀú= 1/ZÀú, withLeReS[60],MiDaS[2,34,35],andMegaDepth[21],
whichthenleadstothenormalizedPÀúworld: which are single-view generic depth estimation methods
n
pretrainedonlarge-scaledatasets. Forimage-to-3Drecon-
PÀúw norld =(XÀú/ZÀú,YÀú/ZÀú,1)=(X n,Y n,1) (4) struction work, we compare with Zero-123 [26], a single-
imagenovel-viewsynthesisandreconstructionmethodalso
Thentheconstrainttwoisthatthe3DpointPworld andits
vertical ground projection PÀúworld have identical XY co- pretrained on Objaverse dataset [10]. For camera parame-
terestimation,wecomparewiththestate-of-the-artoff-the-
ordinates. With this, we know that d = Xn = Yn. We
X Y shelfcameraestimatorCTRL-C[19]andaheuristicmethod
calculated = XnYn fornumericalrobustness, andthefinal
XY weimplementedbyeyeballingaroughFoVandpitchangle
normalized3Dpointis
for all evaluation samples in the test set to get the camera
X Y
Pworld = n n ¬∑(X,Y,Z) (5) focal length and rotation matrix. Using estimated camera
n XY parameters, we can convert the predicted depth map into
whereX,Y,Z,X ,Y arecalculatedfromEqs.(2)to(4). pointclouds. Notethatinordertogeneratedepthmapand
n n
5Input RGB LeReS Depth Zero-123 + SJC ORG (Ours) Object-ground Reconstruction & Depth Map
Figure5.Qualitativeresultsofshadowandreflectiongenerationontheground,aswellasobject-groundreconstructionanddepthestima-
tion.Weshowcomparisonwiththedepth-basedestimationmethodLeReS[60]andmonocularnovelviewsynthesismethodZero-123[26].
ORGmaintainsgreatobject-groundrelationshipcomparedwithpriormethodswhichleadstomuchmorerealisticshadowandreflection
generation,asshownintheblueboxes.Ourmethodrunsveryfastandcaneasilyoutputrepresentationslikedepthmapandpointcloud.
pointcloudsforobjects,weuseimagemasktoremovethe small medium large
background region of our prediction, as well as for exist-
Baseline 0.23 0.37 0.72
ing methods, as we can see in Figure 5. More details are
ORG(Ours) 0.21 0.28 0.45
providedinthesupplementary.
diff ‚àí0.02 ‚àí0.09 ‚àí0.27
Metrics. Forafaircomparisonwithexistingmethods,we
Table 2. ORG achieves higher improvement against baseline
evaluate our method on depth estimation and point cloud
model(DPT-BeiT[2,35]+Ctrl-C[19])whenobjectshavelarger
reconstructiontasks. Inthemeanwhile,wevisualizethees-
viewpointdiversity. WereportresultsonpointcloudsLSIVmet-
timated ground plane together with reconstruction objects ricsonvalidationset.Small,medium,andlargestandfordifferent
to validate the object-ground correlation. For depth esti- levelsofviewpointdiversityofthesamples.
mation, following previous methods [34, 60], we use ab-
solute mean relative error (AbsRel) and the percentage of soluteerror(L1).
pixels with Œ¥ 1 = max( dd ‚àó ii,d d‚àó i i) < 1.25. We follow Mi- 4.3.Shadow,Reflection,andReconstruction
DaS [34] and LeReS [60] to align the scale and shift be-
foreevaluation. Forpointcloudestimation,followingprior Weshowresultsfor3Dreconstruction,shadowgeneration,
work[8,60],weuseLocallyScaleInvariantRMSE(LSIV) and reflection generation on unseen objects in Figure 5.
and Chamfer Distance (CD). In addition, we also evaluate We compare generation performance with the monocular
our direct estimation on pixel height, latitude-vector field depthestimationmethod[60]andthenovelviewsynthesis
andup-vectorfieldusingmean-squareerror(MSE)andab- method[26]. Forbothmethods,weuseCtrl-C[19]topre-
6Figure6. Reconstructedobjectintherealisticbackground. Blue:
novel view synthesis and realistic background composition with
ourmethod.Red:directbackgroundcomposition.
ObjectGeometry CameraParameters LSIV‚Üì diff
depth OFSestimator 1.25 0
depth perspectivefield 1.01 ‚àí0.24
pixelheight OFSestimator 0.98 ‚àí0.27
pixelheight perspectivefield 0.81 ‚àí0.44
Table3.Ourproposedjointestimationofpixelheightandperspec- Figure7.MorequalitativeresultsofORGindepthmapgeneration
tivefieldcontributetothefinalperformance. Wereportresultson andobject-groundreconstruction.Ourmethodgeneralizeswellto
pointcloudsLSIVmetrics. OFSstandsforoff-the-shelf,andwe unseenin-the-wildimages.
useCtrl-CastheOSFestimationinthisexperiment.
andgeneratephoto-realisticshadowfromtheestimatedob-
dictcameraparameters. Sincethesemethodsdonotmodel
jectshape,achievingbettervisualalignmentandrealism.
the ground explicitly, we use the estimated pitch angle to
obtainthegroundplanebyassumingthatitpassesthrough More Qualitative Results. Moreover, Figure 7 illus-
theobject‚Äôslowestpoint(pointwithsmallestheightvalue). tratesadditionalqualitativeresultsfromourstudy,focusing
Forthebaselinefornovelviewsynthesis,weuseSJC[45] ondepthmapgenerationandobject-groundreconstruction.
to reconstruct the shape of the object. As depicted in Fig- Ourmethodologyexhibitsremarkableproficiencyinrecon-
ure 5, notably, thereis a marked improvement in the qual- structingground-supportedobjectsacrossvarioustypes,un-
ityofshadowsandreflections,particularlyatcontactpoints derscoringtherobustnessofourapproach.
ontheground,ashighlightedinthedesignatedboxes. Our
research also includes object-ground reconstructions and
4.5.ObjectwithDiverseViewpoints
depthmapconversions. The3Dshapeofthereconstructed
models in our work is not only realistic but also maintains Wealsobreakdowntheevaluationintosubsetsofsamples
anaccurateverticalalignmentwiththegroundplane. This withdifferentrangeofcameraangles.Morespecifically,we
visualizationeffectivelydemonstratesourmodel‚Äôsversatil- dividethedifficultylevelbythepitchanglebecausenatural
ity, showcasing its exceptional performance across a wide imagesusuallyhavemorediversepitchanglesbutcloseto
arrayofobjectcategories,poses,andviewpoints. zerorollangles. Takingthemeanpitchangleoftheentire
dataset,sampleswithapitchanglesmallerthan10degrees
4.4.NovelViewSynthesisandImageComposition
difference than the mean angle are marked as small view-
We demonstrate applications such as object view manipu- point diversity. Samples with a pitch angle difference be-
lation, shadow generation, and image composition in Fig- tween10and30degreesaremarkedasmediumviewpoint
ure 6. In the red box, we show direct copy-and-paste diversity, and samples with pitch angle difference greater
compositionasacomparison, andperformanceofORGin than 30 degrees are marked as large viewpoint diversity.
shown in the blue box. We notice that the simple copy- Results in Table 2 show that ORG achieves a higher im-
pasting method does not match the camera perspective of provement compared to the baseline model (LeReS [60] +
thenewobjectanditssupportingplaneinthebackground, Ctrl-C[19])whenobjectshavegreaterviewpointdiversity.
creatingunrealisticvisualeffects. Ourmethod,ontheother This is because the traditional viewpoint estimation model
hand, estimatethebackgroundperspective, reconstructthe struggles for object-centric images, especially for samples
objectinto3Dandre-renderitfromthetargetperspective, withextremepitchangles.
7DepthMap PointClouds PixelHeight Lati-Vector Up-Vector
cameraparameters AbsRel‚Üì Œ¥ ‚Üë LSIV‚Üì CD‚Üì L1‚Üì L1‚Üì L1‚Üì
1
MegaDepth[21] 39.4 53.7 1.60 1.73 36.8
NDDepth[40] 35.8 54.2 1.49 1.65 30.9
MiDaS[2,34,35] heuristicconst. 22.7 77.9 1.31 1.45 26.0 8.77 3.02
LeReS[60] 30.0 63.1 1.11 1.34 24.5
ORG(Ours) 24.6 71.2 1.07 1.39 15.4
MegaDepth[21] 39.4 53.7 1.51 1.64 31.1
NDDepth[40] 35.8 54.2 1.46 1.60 28.3
MiDaS[2,34,35] Ctrl-C[19] 22.7 77.9 1.22 1.39 20.7 5.45 1.79
LeReS[60] 30.0 63.1 1.05 1.31 20.4
ORG(Ours) 21.1 76.0 0.99 1.27 15.4
ORG(Ours) Ours 19.1 81.2 0.93 1.26 15.4 4.94 1.45
Table4. ORGperformconsistentlythebestinbothdepthestimationandpointcloudestimationtasksofobject-centricimagesunderall
metrics. Weusetwooff-the-shelfcameraestimationalgorithmstomakeupfortheunknowncameraparameters. PixelHeightmetricis
reportedinabsoluteerrorofnumberofpixels,Latitude-vectorFieldandUp-VectorFieldarereportedindegrees.
4.6.ImportanceofJointEstimation Andwecanseethatusingthesameoff-the-shelfcameraes-
timator,ORGcanstilloutperformexistingmethodsonboth
The results in Table 3 show that the joint learning of pixel
two tasks. We make sure that no samples in the evalua-
heightandperspectivefieldleadstothebestreconstruction
tiondatasetareseenbypriormethodsorourmethodduring
performancecomparedtothedepthestimationandoff-the-
thetrainingphase,inordertocreateazero-shotevaluation
shelfcameraparameterestimator. Morespecifically, with-
scenario. ResultsshowthatORGachievesagreatgeneral-
outmodifyingthemodelarchitecture,wechangetheobjec-
izationabilityintheobject-centric3Dreconstructiontask.
tiveofourmodelfrompixelheightestimationtodepthesti-
Furthermore, we also break down the evaluation into
mationfollowingthelossusedinLeReS[60]. Trainedwith
pixel height, latitude vector, and up-vector estimation, and
the same dataset and scheduler, the pixel height represen-
evaluatewithmeanabsolutionerrorinthegenericspaceof
tation is able to achieve better point-cloud reconstruction
allthreepredictions(numberofpixelsforpixelheightand
than depth-based learning. We argue that this is because
degrees for two perspective fields). For prior methods, we
therepresentationfocusesmoreonobject-groundgeometry
useCtrl-Candtheheuristicconstant(bygridsearch)toes-
ratherthanobject-camerageometry, whichismorenatural
timateelevationangle,rollangleandcameraFoV,andcon-
andeasiertoinferfromobject-centricimages. Thisobser-
vertthemintoperspectivefieldrepresentationsforcompari-
vationfurthervalidatesthatthesuperiorgeneralizabilityof
son. Theirpixelheightestimationsarealsoconvertedusing
ORG comes from the better representation design and the
depthestimationandcameraparameterestimations. Aswe
jointtrainingstrategy,ratherthanthedataset.
can see in Table 4, our method outperforms the baselines
in all three tasks. These experiments demonstrate the ro-
4.7.QualitativeEvaluationonReconstruction
bustnessandgeneralizabilityofORGoverpriormethodsin
We compare the depth map estimation, point cloud gener- object3Destimationandreconstruction.
ation, and the prediction of our representations with four
state-of-the-art monocular depth estimation and 3D recon- 5.Conclusion
structionmethodsontheheld-outtestset. Weusethestate-
of-the-art camera parameter estimation model Ctrl-C [19] Inthispaper,weproposedORG,toourknowledge,thefirst
andaheuristicestimationtocompensateformissingintrin- data-driven architecture that simultaneously reconstructs
sic and extrinsic information from previous methods. We 3D object, estimates camera parameter, and models the
convert the raw output into depth map and point clouds object-ground relationship from a monocular image. To
for a fair comparison with existing methods. As shown achieve this, we propose a new formulation for represent-
in Table 4, our method performs consistently the best in ingobjectsinrelationtotheground. Qualitativeandquan-
both depth estimation and point cloud estimation tasks for titativeresultsonunseenobjectandhumandatasetsaswell
object-centric images under all metrics. We also try using aswebimagesdemonstratetherobustnessandflexibilityof
theothertwoalternativecameraparameterestimatorstore- our model, which marks a significant step towards in-the-
construct the point cloud from the pixel height estimation. wildsingle-imageobjectgeometryestimation.
8References [19] JinwooLee,HyunsungGo,HyunjoonLee,SunghyunCho,
MinhyukSung,andJunhoKim. Ctrl-c: Cameracalibration
[1] AdelAhmadyan,LiangkaiZhang,ArtsiomAblavatski,Jian-
transformerwithline-classification. InICCV,2021. 3,5,6,
ingWei,andMatthiasGrundmann.Objectron:Alargescale
7,8
datasetofobject-centricvideosinthewildwithposeanno-
[20] XuetingLi,SifeiLiu,KihwanKim,ShaliniDeMello,Varun
tations. InCVPR,2021. 5
Jampani,Ming-HsuanYang,andJanKautz. Self-supervised
[2] ReinerBirkl,DianaWofk,andMatthiasMu¬®ller. Midasv3.1
single-view 3d reconstruction via semantic consistency. In
‚Äìamodelzooforrobustmonocularrelativedepthestimation.
ECCV,2020. 3
arXivpreprintarXiv:2307.14460,2023. 5,6,8
[21] ZhengqiLiandNoahSnavely. Megadepth:Learningsingle-
[3] BlenderOnlineCommunity. Blender-a3dmodellingand
viewdepthpredictionfrominternetphotos. InCVPR,2018.
renderingpackage. 5,1
5,8
[4] ThomasJCashmanandAndrewWFitzgibbon. Whatshape
[22] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
are dolphins? building 3d morphable models from 2d im-
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
ages. TPAMI,2012. 3
Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolution
[5] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan- text-to-3dcontentcreation. InCVPR,2023. 3
tasia3d: Disentangling geometry and appearance for high-
[23] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
qualitytext-to-3dcontentcreation. InICCV,2023. 3
PietroPerona,DevaRamanan,PiotrDolla¬¥r,andCLawrence
[6] WeifengChen,ZhaoFu,DaweiYang,andJiaDeng. Single- Zitnick. MicrosoftCOCO:Commonobjectsincontext. In
imagedepthperceptioninthewild. NeurIPS,2016. 1,2 ECCV,2014. 4,1
[7] WeifengChen,ShengyiQian,andJiaDeng.Learningsingle- [24] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.
imagedepthfromvideosusingqualityassessmentnetworks. Learning depth from single monocular images using deep
InCVPR,2019. 1,2 convolutionalneuralfields. TPAMI,2015. 2
[8] WeifengChen,ShengyiQian,DavidFan,NoriyukiKojima, [25] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
MaxHamilton, andJiaDeng. Oasis: Alarge-scaledataset Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:
forsingleimage3dinthewild. InCVPR,2020. 2,6 Any single image to 3d mesh in 45 seconds without per-
[9] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan- shapeoptimization. InNeurIPS,2023. 2,3
derSchwing,andLiangyanGui. SDFusion:Multimodal3D [26] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
ShapeCompletion,Reconstruction,andGeneration. CVPR, makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
2023. 2 Zero-shot one image to 3d object. In Proceedings of the
[10] MattDeitke, DustinSchwenk, JordiSalvador, LucaWeihs, IEEE/CVF International Conference on Computer Vision,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana pages9298‚Äì9309,2023. 1,2,3,5,6
Ehsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: [27] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
Auniverseofannotated3dobjects. InCVPR,2023. 2,5 regularization. ICLR,2017. 4,1
[11] JonathanDeutscher,MichaelIsard,andJohnMacCormick. [28] YunzeMan,XinshuoWeng,XiLi,andKrisKitani.Ground-
Automatic camera calibration from a single manhattan im- net: Monocular ground plane normal estimation with geo-
age. InECCV,2002. 3 metricconsistency. InACMMM,2019. 3
[12] DavidEigen,ChristianPuhrsch,andRobFergus.Depthmap [29] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
predictionfromasingleimageusingamulti-scaledeepnet- AndreaVedaldi. Realfusion: 360reconstructionofanyob-
work. NeurIPS,2014. 2 jectfromasingleimage. InCVPR,2023. 2,3
[13] Daniel Gatis. Rembg. https://github.com/ [30] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
danielgatis/rembg,2023. 1 bastianNowozin,andAndreasGeiger.Occupancynetworks:
[14] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Learning 3D reconstruction in function space. In CVPR,
Shapeandviewpointwithoutkeypoints. InECCV,2020. 3 2019. 3
[15] YannickHold-Geoffroy,KalyanSunkavalli,JonathanEisen- [31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
mann,MatthewFisher,EmilianoGambaretto,SunilHadap, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
andJean-Franc¬∏oisLalonde. Aperceptualmeasurefordeep Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
singleimagecameracalibration. InCVPR,2018. 3 imperativestyle,high-performancedeeplearninglibrary. In
[16] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver NeurIPS,2019. 1
Wang,KevinMatzen,MatthewSticha,andDavidFFouhey. [32] BenPoole,AjayJain,JonathanTBarron,andBenMilden-
Perspective Fields for Single Image Camera Calibration. hall. Dreamfusion: Text-to-3dusing2ddiffusion. InICLR,
arXiv,2022. 2,3 2022. 3
[17] AngjooKanazawa,ShubhamTulsiani,AlexeiAEfros,and [33] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
JitendraMalik. Learningcategory-specificmeshreconstruc- Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
tionfromimagecollections. InECCV,2018. 3 rokhodov,PeterWonka,SergeyTulyakov,etal. Magic123:
[18] AbhishekKar, ShubhamTulsiani, JoaoCarreira, andJiten- One image to high-quality 3d object generation using both
dra Malik. Category-specific object reconstruction from a 2dand3ddiffusionpriors.arXivpreprintarXiv:2306.17843,
singleimage. InCVPR,2015. 3 2023. 2,3
9[34] Rene¬¥ Ranftl, Katrin Lasinger, David Hafner, Konrad [49] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
Schindler, and Vladlen Koltun. Towards robust monocular Li,HangSu,andJunZhu.Prolificdreamer:High-fidelityand
depthestimation:Mixingdatasetsforzero-shotcross-dataset diversetext-to-3dgenerationwithvariationalscoredistilla-
transfer. TPAMI,2020. 1,2,4,5,6,8 tion. InNeurIPS,2023. 3
[35] Rene¬¥ Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi- [50] Scott Workman, Connor Greenwell, Menghua Zhai, Ryan
siontransformersfordenseprediction. InICCV,2021. 5,6, Baltenberger,andNathanJacobs. Deepfocal: Amethodfor
8 directfocallengthestimation. InICIP,2015. 3
[36] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, [51] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
LucaSbordone,PatrickLabatut,andDavidNovotny. Com- Feichtenhofer, and Georgia Gkioxari. Prolificdreamer:
mon Objects in 3D: Large-scale learning and evaluation of High-fidelity and diverse text-to-3d generation with varia-
real-life3Dcategoryreconstruction. InICCV,2021. 5 tionalscoredistillation. InCVPR,2023. 3
[37] Lawrence G Roberts. Machine perception of three- [52] Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong
dimensionalsolids. PhDthesis,MIT,1963. 3 Zhang, William T Freeman, and Joshua B Tenenbaum.
[38] ShunsukeSaito,ZengHuang,RyotaNatsume,ShigeoMor- Learningshapepriorsforsingle-view3dcompletionandre-
ishima,AngjooKanazawa,andHaoLi. PIFu:Pixel-aligned construction. InECCV,2018. 2
implicitfunctionforhigh-resolutionclothedhumandigitiza- [53] KeXian,ChunhuaShen,ZhiguoCao,HaoLu,YangXiao,
tion. InICCV,2019. 2,3,1 Ruibo Li, and Zhenbo Luo. Monocular relative depth per-
[39] ShunsukeSaito,TomasSimon,JasonSaragih,andHanbyul ception with web stereo data supervision. In CVPR, 2018.
Joo.PIFuHD:Multi-levelpixel-alignedimplicitfunctionfor 2
high-resolution 3D human digitization. In Proceedings of [54] Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisen-
theIEEE/CVFConferenceonComputerVisionandPattern mann, Eli Shechtman, and Noah Snavely. Uprightnet:
Recognition,pages84‚Äì93,2020. 1,2,3 geometry-aware camera orientation estimation from single
[40] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, images. InICCV,2019. 3
and Zhengguo Li. Nddepth: Normal-distance assisted [55] EnzeXie,WenhaiWang,ZhidingYu,AnimaAnandkumar,
monoculardepthestimation. InICCV,2023. 8 JoseMAlvarez,andPingLuo. Segformer: Simpleandef-
[41] YichenSheng,YifanLiu,JianmingZhang,WeiYin,ACen- ficientdesignforsemanticsegmentationwithtransformers.
gizOztireli,HeZhang,ZheLin,EliShechtman,andBedrich NeurIPS,2021. 4,1
Benes. ControllableShadowGenerationUsingPixelHeight [56] DejiaXu,YifanJiang,PeihaoWang,ZhiwenFan,YiWang,
Maps. InECCV,2022. 2,3 andZhangyangWang.Neurallift-360:Liftinganin-the-wild
[42] YichenSheng,JianmingZhang,JulienPhilip,YannickHold- 2dphototoa3dobjectwith360¬∞views. InCVPR,2023. 1,
Geoffroy,XinSun,HeZhang,LuLing,andBedrichBenes. 2,3
Pixht-lab: Pixelheightbasedlighteffectgenerationforim- [57] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
age compositing. In Proceedings of the IEEE/CVF Con- supervisedmeshpredictioninthewild. InCVPR,2021. 3
ferenceonComputerVisionandPatternRecognition,pages [58] YufeiYe,AbhinavGupta,andShubhamTulsiani. What‚Äôsin
16643‚Äì16653,2023. 3 yourhands? 3dreconstructionofgenericobjectsinhands.
[43] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi, In Proceedings of the IEEE/CVF Conference on Computer
LizhuangMa,andDongChen.Make-it-3d:High-fidelity3d VisionandPatternRecognition,pages3895‚Äì3905,2022. 3
creationfromasingleimagewithdiffusionprior. InICCV, [59] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En-
2023. 2,3 forcinggeometricconstraintsofvirtualnormalfordepthpre-
[44] ChaoyangWang,SimonLucey,FedericoPerazzi,andOliver diction. InICCV,2019. 2
Wang. Web stereo video supervision for depth prediction [60] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus,
fromdynamicscenes. In3DV,2019. 2 Long Mai, Simon Chen, and Chunhua Shen. Learning to
[45] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, recover3dsceneshapefromasingleimage.InCVPR,2021.
andGregShakhnarovich. Scorejacobianchaining: Lifting 1,2,4,5,6,7,8
pretrained2ddiffusionmodelsfor3dgeneration. InCVPR, [61] ZhengyouZhang. Aflexiblenewtechniqueforcameracali-
2023. 7 bration. TPAMI,2000. 3
[46] NanyangWang,YindaZhang,ZhuwenLi,YanweiFu,Wei
Liu,andYu-GangJiang. Pixel2Mesh: Generating3Dmesh
modelsfromsingleRGBimages. InECCV,2018. 2,3
[47] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao
Song,DingLiang,TongLu,PingLuo,andLingShao.Pyra-
midvisiontransformer: Aversatilebackbonefordensepre-
dictionwithoutconvolutions. InICCV,2021. 3,1
[48] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,Kaitao
Song,DingLiang,TongLu,PingLuo,andLingShao. Pvt
v2: Improved baselines with pyramid vision transformer.
CVMJ,2022. 3,4
10Floating No More: Object-Ground Reconstruction from a Single Image
Supplementary Material
A.ImplementationDetails A100machine. Weschedulethemulti-steptrainingstages
at step 30K, 40K, and 50K, with learning rate decreases
Hereweprovidemoredetailsregardingtheimplementation
10√ó each time. We resize the images to (512,512) reso-
andtrainingofourmodel.
lution. We use horizontal flipping, random cropping, and
Backbone and Decoder. We use PVTv2-b3 [47] pre- color jittering augmentation during training. And because
trained on COCO dataset [23] as our encoder back- horizontal flipping, random cropping and resizing will af-
bone. And we use a decoder of a similar design as Seg- fectthevaluesofourrepresentations,weupdatetheground
Former[55],whichconsistsoffourmulti-layer-perceptron truth maps accordingly. The whole model is implemented
(MLP)layerstoextractfeaturemapsofdifferentscales.We usingthePyTorchframework[31].
predict two dense fields with five channels: two for front
B.MoreQualitativeAnalysis
and back surface pixel height map, one for latitude field,
andtwoforgravityfield.
HerewedemonstratemorevisualizationexamplesofORG.
DataNormalization. Forpixelheightestimation,wenor- We show more diverse categories of objects with different
malize the ground truth maps by dividing them with the camera viewpoints on random web images, and also full
height of the image, which roughly turns the range of the objectgeometryreconstructionresults.
pixelheightsinto[0,1]suchthatourmodelisnotaffected
DiverseCategories. InFigureA,weshowourdirectesti-
byobjectsatdifferentscale. Fortwoperspectivefields,we
mationofpixelheightandprospectivefields,andalsovisu-
normalizethelatitudefieldinto[0,1]andwerepresentthe
alize the reprojected depth maps and reconstructed object-
gravity (up-vector) field with a (sine, cosine) tuple as de-
ground point clouds of diverse categories of objects from
scribedinSection3.2inthemainpaper. Theestimationof
web images. The categories include common objects like
allthreerepresentationsareformulatedasregressionprob-
microphone, plant, car, and tripod, as well as cartoon fig-
lems and trained by MSE loss. Similar to existing meth-
ures. Theresultsshowgreatgeneralizabilityandrobustness
ods[34,38,39,60], duetotheestimationofanormalized
ofourmethodinthewild.
pixelheightrepresentation,ourreconstructedmodels(Sec-
Object-Ground Reconstruction. In addition to our pre-
tion 3.3) preserve the 3D geometry of the original objects
vious analyses, we present a detailed visualization of the
but are scale-ambiguous. We calibrate the objects recon-
complete3Dgeometryofreconstructedobjectsandground
structed by our methods and prior method using a linear
in Figure B. Here, the objects are represented using 3D
scalingfollowingLeReS[60].
point clouds. Despite employing a simplified geometric
ObjactMask. Allthedatasetsweutilizedfortrainingand
modelinourapproach,ourresultseffectivelyshowcasesu-
quantitativeevaluationcomewithobjectmasks, whichare
perior reconstruction quality, particularly for objects with
fromeitherhumanannotationoroff-the-shelfsegmentation
relativelystraightforwardgeometricstructures. Thisaspect
models. When evaluating on web images, we utilize seg-
of ORG highlights the balance between model simplicity
mentation model Rembg with u2net backbone [13] to get
andtheabilitytoachievehigh-fidelityreconstructions,even
theforegroundmask.
withlesscomplexgeometries.
DataGeneration. Weusethephysically-basedrendering
C.LimitationsandFutureWork
engineBlender[3]torenderrealisticRGBchannelresults.
Thefrontandbacksurfacepixelheightiscalculatedbyour
Primarily, ourapproachreliesonasimplifiedobjectshape
ray tracer. In detail, we shoot one ray to each pixel, find
assumption,optimizingforefficientimageediting(e.g.,re-
the first and last ray-object intersection points and calcu-
flection,shadowgeneration,andground-awareobjectpose
late their relevant 3D foot points (z=0). Then we project
change). However, this simplification may yield less than
theintersectionpointsandtheirfootpointsontothecamera.
satisfactory3Dreconstructionresultsforobjectswithintri-
Thepixelheightsarecalculatedbymeasuringthedistances
cate geometries, particularly in estimating their back sur-
oftheprojectedintersectionpointsandtheirprojectedfoot
faces. Additionally,ourmethodfocusessolelyonthegeo-
pointsinpixelunits.Ourpixelheightcalculationisefficient
metricaspectsofobjects,excludingconsiderationsofcolor
andcanbecomputedinrealtime.
and texture. We propose that leveraging our estimated ge-
Training and Scheduling. The model is trained with ometry as a conditioned prior could significantly enhance
AdamW[27]optimizerwithinitiallearningrate0.0005and image-inpainting processes, presenting a promising direc-
weight decay 1e-2 for 60K steps with batch size 8 on a 4- tionforfutureresearch.
1Object & Ground
Input RGB Pixel Height Perspective Field Depth Map Reconstruction
FigureA.VisualizationofORGonpixelheight,(foreground)perspectivefields,depthmap,andobject-groundreconstructionresults.The
resultsdemonstratethatourworkgeneralizestovariouscategoriesofobjects.
2FigureB.VisualizationofORGonthefullobjectgeometry(frontsurfaceandbacksurface)withthegroundplane.
3