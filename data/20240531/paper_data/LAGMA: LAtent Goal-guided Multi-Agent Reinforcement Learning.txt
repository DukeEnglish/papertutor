LAGMA: LAtent Goal-guided Multi-Agent Reinforcement Learning
HyunghoNa1 Il-ChulMoon12
Abstract asGoogleResearchFootball(GRF)(Kurachetal.,2020);
learningoptimalpolicytakeslongtime,andtrainedmod-
Incooperativemulti-agentreinforcementlearning
elsevenfailtoachieveacommongoal,suchasdestroying
(MARL),agentscollaboratetoachievecommon
allenemiesinSMACorscoringagoalinGRF.Thus,re-
goals, such as defeating enemies and scoring a
searchers focus on sample efficiency to expedite training
goal. However,learninggoal-reachingpathsto-
(Zhengetal.,2021)andencouragecommittedexploration
wardsuchasemanticgoaltakesaconsiderable
(Mahajanetal.,2019;Yangetal.,2019;Wangetal.,2019).
amountoftimeincomplextasksandthetrained
modeloftenfailstofindsuchpaths. Toaddress Toenhancesampleefficiencyduringtraining,statespace
this,wepresentLAtentGoal-guidedMulti-Agent abstractionhasbeenintroducedinbothmodel-based(Jiang
reinforcementlearning(LAGMA),whichgener- etal.,2015;Zhuetal.,2021;Hafneretal.,2020)andmodel-
atesagoal-reachingtrajectoryinlatentspaceand free settings (GrzesÂ´ & Kudenko, 2008; Tang & Agrawal,
providesalatentgoal-guidedincentivetotransi- 2020;Lietal.,2023). Suchsampleefficiencycanbemore
tions toward this reference trajectory. LAGMA importantinsparserewardsettingssincetrajectoriesinare-
consistsofthreemajorcomponents:(a)quantized playbufferrarelyexperiencepositiverewardsignals. How-
latentspaceconstructedviaamodifiedVQ-VAE ever,suchmethodshavebeenstudiedwithinasingle-agent
forefficientsampleutilization,(b)goal-reaching taskwithoutexpandingtomulti-agentsettings.
trajectorygenerationviaextendedVQcodebook,
Toencouragecommittedexploration,goal-conditionedrein-
and(c)latentgoal-guidedintrinsicrewardgenera-
forcementlearning(GCRL)(Kaelbling,1993;Schauletal.,
tiontoencouragetransitionstowardsthesampled
2015;Andrychowiczetal.,2017)hasbeenwidelyadopted
goal-reachingpath. Theproposedmethodiseval-
inasingleagenttask,suchascomplexpathfindingwitha
uatedbyStarCraftIIwithbothdenseandsparse
sparse reward(Nasiriany etal., 2019; Zhang et al., 2020;
reward settings and Google Research Football.
Chane-Saneetal.,2021;Kimetal.,2023;Leeetal.,2023).
Empiricalresultsshowfurtherperformanceim-
However, GCRLconcepthasalsobeenlimitedlyapplied
provementoverstate-of-the-artbaselines.
tomulti-agentreinforcementlearning(MARL)taskssince
there are various difficulties: 1) a goal is not explicitly
known,onlyasemanticgoalcanbefoundduringtrainingby
1.Introduction
rewardsignal;2)partialobservabilityanddecentralizedex-
Centralizedtraininganddecentralizedexecution(CTDE) ecutioninMARLmakesimpossibletoutilizepathplanning
paradigm(Oliehoeketal.,2008;Guptaetal.,2017)espe- with global information during execution, only allowing
ciallywithvaluefactorizationframework(Sunehagetal., suchplanningduringcentralizedtraining;3)mostMARL
2017;Rashidetal.,2018;Wangetal.,2020a)hasshown tasksseeknottheshortestpath,butthecoordinatedtrajec-
itssuccessonvariouscooperativemulti-agenttasks(Lowe tory,whichrenderssingle-agentpathplanninginGCRLbe
et al., 2017; Samvelyan et al., 2019). However, in more toosimplisticinMARLtasks.
complex tasks with dense reward settings, such as super
Motivatedbymethodsemployedinsingle-agenttasks,we
hardmapsinStarCraftIIMulti-agentChallenge(SMAC)
considerageneralcooperativeMARLproblemasfinding
(Samvelyanetal.,2019)orinsparserewardsettings,aswell
trajectoriestowardsemanticgoalsinlatentspace.
1Korea Advanced Institute of Science and Tech-
Contribution. This paper presents LAtent Goal-guided
nology (KAIST), Daejeon 34141, Republic of Korea.
Multi-Agentreinforcementlearning(LAGMA).LAGMA
2summary.ai, Daejeon, Republic of Korea. Correspondence
to: Hyungho Na <gudgh723@gmail.com>, Il-Chul Moon generatesagoal-reachingtrajectoryinlatentspaceandpro-
<icmoon@kaist.ac.kr>. videsalatentgoal-guidedincentivetotransitiontowardthis
referencetrajectoryduringcentralizedtraining.
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by â€¢ ModifiedVQ-VAEforquantizedembeddingspace
theauthor(s). construction: As one measure of efficient sam-
1
4202
yaM
03
]AM.sc[
1v89991.5042:viXraLAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
ple utilization, we use Vector Quantized-Variational states,particularlythoseinproximity. Inaddition,thanks
Autoencoder(VQ-VAE)(VanDenOordetal.,2017) tothediscretizedembeddings,thecount-basedestimation
whichprojectsstatestoaquantizedvectorspacesothat canbeadoptedtoestimatethevalueofstatesprojectedto
acommonlatentcanbeusedasarepresentativefora eachdiscretizedembedding. Then,wegenerateareference
widerangeofembeddingspace. However, statedis- orgoal-reachingtrajectorybasedonthisevaluationinquan-
tributionsinhighdimensionalMARLtasksarequite tizedvectorspaceandprovideanincentivefortransitions
limitedtosmallfeasiblesubspaceunlikeimagegener- thatoverlapwiththisreference.
ationtasks,whoseinputsorstatesoftenutilizeafull
IntrinsicincentiveinRL Inreinforcementlearning,bal-
statespace. Insuchacase,onlyafewquantizedvec-
ancing exploration and exploitation during training is a
torsareutilizedthroughouttrainingwhenadoptingthe
paramount issue (Sutton & Barto, 2018). To encourage
originalVQ-VAE.Tomakequantizedembeddingvec-
a proper exploration, researchers have presented various
torsdistributedproperlyovertheembeddingspaceof
formsofmethodsinasingle-agentcasesuchasmodified
feasiblestates,weproposeamodifiedlearningframe-
count-based methods (Bellemare et al., 2016; Ostrovski
workforVQ-VAEwithanovelcoverageloss.
etal.,2017;Tangetal.,2017),predictionerror-basedmeth-
â€¢ Goal-reachingtrajectorygenerationwithextended
ods (Stadie et al., 2015; Pathak et al., 2017; Burda et al.,
VQcodebook: LAGMAconstructsanextendedVQ
2018;Kimetal.,2018),andinformationgain-basedmeth-
codebooktoevaluatethestatesprojectedtoacertain
ods(Mohamed&JimenezRezende,2015;Houthooftetal.,
quantizedvectorandgenerateagoal-reachingtrajec-
2016). Inmostcases,anincentiveforexplorationisintro-
torybasedonthisevaluation. Specifically,duringtrain-
ducedasanadditionalrewardtoaTDtargetinQ-learningor
ing, we store various goal-reaching trajectories in a
aregularizertooveralllossfunctions. Recently,diverseap-
quantizedlatentspace. Then,LAGMAusesthemasa
proachesmentionedearlierhavebeenadoptedinthemulti-
referencetofollowduringcentralizedtraining.
agentenvironmenttopromoteexploration(Mahajanetal.,
â€¢ Latentgoal-guidedintrinsicrewardgeneration: To 2019;Wangetal.,2019;Jaquesetal.,2019;Mgunietal.,
encourage coordinated exploration toward reference 2021). Asanexample,EMC(Zhengetal.,2021)utilizes
trajectoriessampledfromtheextendedVQcodebook, episodiccontrol(Lengyel&Dayan,2007;Blundelletal.,
LAGMApresentsalatentgoal-guidedintrinsicreward. 2016)asregularizationforthejointQ-learning,inaddition
Theproposedlatentgoal-guidedintrinsicrewardaims to a curiosity-driven exploration by predicting individual
toaccuratelyestimateTD-targetfortransitionstoward Q-values. Learningwithintrinsicrewardsbecomesmore
goal-reachingpaths,andweprovideboththeoretical importantinsparserewardsettings. However,thisintrinsic
andempiricalsupport. rewardcanadverselyaffecttheoverallpolicylearningifit
is not properly annealed throughout the training. Instead
2.RelatedWorks of generating an additional reward signal solely encour-
agingexploration,LAGMAgeneratesanintrinsicreward
StatespaceabstractionforRL Stateabstractiongroups thatguaranteesamoreaccurateTD-targetforQ-learning,
stateswithsimilarcharacteristicsintoasinglecluster,and yieldingadditionalincentivetowardagoal-reachingpath.
ithasbeeneffectiveinbothmodel-basedRL(Jiangetal.,
Additional related works regarding goal-conditioned re-
2015;Zhuetal.,2021;Hafneretal.,2020)andmodel-free
inforcement learning (GCRL) and subtask-conditioned
settings(GrzesÂ´&Kudenko,2008;Tang&Agrawal,2020).
MARLarepresentedinAppendixC.
NECSA (Li et al., 2023) adopted the abstraction of grid-
based state-action pair for episodic control and achieved
state-of-the-art (SOTA) performance in a general single- 3.Preliminaries
RL task. This approach could relax the limitations of in-
DecentralizedPOMDP Ageneralcooperativemulti-agent
efficient memory usage in the conventional episodic con-
task with n agents can be formalized as the Decentral-
trol,butthisrequiresanadditionaldimensionalityreduction
izedPartiallyObservableMarkovDecisionProcess(Dec-
technique,suchasrandomprojection(Dasgupta,2013)in
POMDP)(Oliehoek&Amato,2016). DecPOMDPconsists
high-dimensionaltasks. Recently,EMU(Naetal.,2024)
ofatupleG=âŸ¨I,S,A,P,R,â„¦,O,n,Î³âŸ©,whereI isthefi-
presentedasemanticembeddingforefficientmemoryuti-
nitesetofnagents;sâˆˆSisthetruestateintheglobalstate
lization, but it still resorts to the episodic buffer, which
spaceS;Aistheactionspaceofeachagentâ€™sactiona form-
requiresstoringboththestatesandtheembeddings. This i
ingthejointactiona âˆˆ An; P(sâ€²|s,a)isthestatetransi-
additional memory usage could be burdensome in tasks
tionfunctiondeterminedbytheenvironment;Risareward
withlargestatespace. Incontrasttopreviousresearch,we
functionr =R(s,a,sâ€²)âˆˆR;Oistheobservationfunction
employVQ-VAEforstateembeddingandestimatetheover-
generatinganindividualobservationfromobservationspace
all value of abstracted states. In this manner, a sparse or
â„¦,i.e.,o âˆˆ â„¦;andfinally,Î³ âˆˆ [0,1)isadiscountfactor.
delayedrewardsignalcanbeutilizedbyabroadrangeof i
2LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
(a) VQ-VAE (e) Standard CTED framework
Train ğ‘“ ğœ™and ğ‘“ ğœ“via Eq. (6)
Replay
Environment
buffer
Mixing Network
ğ‘  ğ‘¡ ğ‘“ ğœ™ ğ‘“ ğœ“ ğ‘ Æ¸ ğ‘¡ ğœ ğ‘ ğ‘¡ ğ‘„ ğ‘¡ğ‘œğ‘¡=ğ‘“(ğ‘„ 1,ğ‘„ 2,â‹¯,ğ‘„ ğ‘›;ğœƒ)
Mixing Gradients
ğœ
Coverage ğœ’ğ‘¡
Loss ğ‘“ ğœ™ Controller
ğ‘„(âˆ™|ğ‘œ;ğœƒ)
ğ‘– ğ‘–
ğ‘… ğ‘¡ ğ‘¥ ğ‘¡ ğ‘¥ ğ‘,ğ‘¡ ğ‘¥ ğ‘,ğ‘¡ intrinsic reward ğ‘Ÿğ¼
VQ Codebook Goal-reaching trajectory generation Intrinsic reward generation for
ğ’Ÿ ğ‘‰ğ‘„ ğ’Ÿ ğ‘ ğ‘’ğ‘ with VQ Codebook desirable transition via Eq. (8)
ğ¶ ğ‘â‹® ,ğ‘¡ ğ‘¥â‹® ğ‘,ğ‘¡ ğ‘§ ğ‘¡ ğ’Ÿâ‹® ğ¶ğ‘,ğ‘¡ ğ’Ÿ ğœâ‹® ğœ’ğ‘¡ ğ‘¥ ğ‘,ğ‘¡ ğœ ğœ’âˆ— ğ‘¡~ğ’Ÿ ğœğœ’ğ‘¡ ğœ ğœ’âˆ— ğ‘¡
â‹® â‹® â‹® â‹®
(b) VQ Codebook generation (c) Goal-reaching trajectory generation (d) Intrinsic reward generation
Figure1: OverviewofLAGMAframework.(a)VQ-VAEconstructsquantizedvectorspacewithcoverageloss,while(b)
VQcodebookstoresgoal-reachingsequencesfromagivenx . Then,(c)thegoal-reachingtrajectoryiscomparedwiththe
q,t
currentbatchtrajectorytogenerate(d)intrinsicreward. MARLtrainingisdoneby(e)thestandardCTDEframework.
InageneralcooperativeMARLtask,anagentacquiresits asemanticgoal,suchasdefeatingallenemiesinSMACor
localobservationo ateachtimestep,andtheagentselects scoringagoalinGRF.Thus,wedefinegoalstatesandthe
i
anactiona âˆˆAbasedono . P(sâ€²|s,a)determinesanext goal-reachingtrajectoryincooperativeMARLasfollows.
i i
statesâ€²foragivencurrentstatesandthejointactiontaken
byagentsa. Foragiventupleof{s,a,sâ€²},Rprovidesan Definition3.1. (GoalStateandGoal-ReachingTrajectory)
ForagiventaskdependentR andanepisodicsequence
identicalcommonrewardtoallagents.Toovercomethepar- max
T := {s ,a ,r ,s ,a ,r ,...,s }, when Î£Tâˆ’1r =
tialobservabilityinDecPOMDP,eachagentoftenutilizesa 0 0 0 1 1 1 T t=0 t
R for r âˆˆ T, we define such an episodic sequence
localaction-observationhistoryÏ„ âˆˆ T â‰¡ (â„¦Ã—A)forits max t
i asagoal-reachingsequenceanddenoteasTâˆ—. Then,for
policyÏ€ (a|Ï„ ),whereÏ€ :T Ã—Aâ†’[0,1](Hausknecht&
i i âˆ€s âˆˆ Tâˆ—, Ï„âˆ— := {s ,s ,...s } is a goal-reaching tra-
Stone,2015;Rashidetal.,2018). Additionally,wedenotea t st t t+1 T
jectoryandwedefinethefinalstateofÏ„âˆ— asagoalstate
grouptrajectoryasÏ„ =<Ï„ 1,...,Ï„
n
>.
denotedbysâˆ—.
st
T
Centralized Training with Decentralized Execution
(CTDE) In fully cooperative MARL tasks, under the
4.Methodology
CTDEparadigm,valuefactorizationapproacheshavebeen
introduced by (Sunehag et al., 2017; Rashid et al., 2018; ThissectionintroducesLAtentGoal-guidedMulti-Agent
Son et al., 2019; Rashid et al., 2020; Wang et al., 2020a) reinforcementlearning(LAGMA)(Figure1). Wefirstex-
andachievedstate-of-the-artperformanceincomplexmulti- plainhowtoconstructaproper(1)quantizedembeddings
agent tasks such as SMAC (Samvelyan et al., 2019). In viaVQ-VAE.Tothisend,weintroduceanovellossterm
valuefactorizationapproaches,thejointaction-valuefunc- calledcoveragelosstodistributequantizedembeddingvec-
tion Qtot parameterized by Î¸ is trained to minimize the torsacrosstheoverallembeddingspace. Then,weelaborate
Î¸
followinglossfunction. onthedetailsof(2)goal-reachingtrajectorygeneration
with extended VQ codebook. Finally, we propose (3) a
L(Î¸)=E [(cid:0) rext+Î³Vtot(Ï„â€²)âˆ’Qtot(Ï„,a)(cid:1)2 ] latentgoal-guidedintrinsicreward whichguaranteesa
Ï„,a,rext,Ï„â€²âˆˆD Î¸âˆ’ Î¸
(1) betterTD-targetforpolicylearningandthusyieldsabetter
Here,Vtot(Ï„â€²)=max Qtot(Ï„â€²,aâ€²)bydefinition;Drep- convergenceonoptimalpolicy.
Î¸âˆ’ aâ€² Î¸âˆ’
resentsthereplaybuffer;rextisanexternalrewardprovided
bytheenvironment;Qtotisatargetnetworkparameterized 4.1.StateEmbeddingviaModifiedVQ-VAE
Î¸âˆ’
by Î¸âˆ’ for double Q-learning(Hasselt, 2010; Van Hasselt
Inthispaper,weadoptVQ-VAEasadiscretizationbottle-
et al., 2016); and Qtot and Qtot include both mixer and
Î¸ Î¸âˆ’ neck(VanDenOordetal.,2017)toconstructadiscretized
individualpolicynetwork.
low-dimensional embedding space. Thus, we first define
Goal State and Goal-Reaching Trajectory In general n -trainable embedding vectors (codes) e âˆˆ RD in the
c j
cooperative multi-agent tasks, undiscounted reward sum, codebookwherej ={1,2,...,n }. Anencodernetworkf
c Ï•
i.e.,R =Î£Tâˆ’1r ,ismaximizedasR ifagentsachieve inVQ-VAEprojectsaglobalstatestowardD-dimensional
0 t=0 t max
3LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
(a) TrainingwithoutL (Î» =0.0). (b) TrainingwithLall (Î» =0.2). (c) TrainingwithL (Î» =0.2).
cvr cvr cvr cvr cvr cvr
Figure2: VisualizationofembeddingresultsviaVQ-VAE.UnderSMAC5m vs 6mtask,thesizeofcodebookn =64,
c
thelatentdimensionD =8;thisillustratesembeddingsattrainingtimeatT=1.0M.ColoreddotsrepresentÏ‡,whichisa
statepresentationbeforequantization,andgraydotsarequantizedvectorrepresentationsbelongingtoVQcodebookderived
fromthestaterepresentations. Colorsfromredtopurple(rainbow)representfromsmalltolargetimestepwithinepisodes.
vector,x=f (s)âˆˆRD. Insteadofadirectusageoflatent bufferD,denotedasÏ‡ = {x âˆˆ RD : x = f (s),s âˆˆ D},
Ï• Ï•
vectorx,weuseadiscretizedlatentx bythequantization leavingonlyafeweclosetoxwithinanepisode.Toresolve
q
processwhichmapsanembeddingvectorxtothenearest thisissue,weintroducethecoveragelosswhichminimizes
embeddingvectorinthecodebookasfollows. theoveralldistancebetweenthecurrentembeddingxand
allvectorsinthecodebook,i.e.,e forallj ={1,2,...,n }.
j c
x q =e z,wherez =argmin j||xâˆ’e j|| 2 (2) 1 (cid:88)nc
Then, thequantizedvectorx isusedasaninputtoade-
La cl vl r(e)=
n
||sg[f Ï•(s)]âˆ’e j||2
2
(4)
q c j=1
coderf whichreconstructstheoriginalstates. Totrain
Ïˆ
anencoder,adecoder,andembeddingvectorsinthecode- AlthoughLall couldleadembeddingvectorstowardÏ‡,all
cvr
book,weconsiderthefollowingobjectivesimilarto(Van quantizedvectorstendtolocatethecenterofÏ‡ratherthan
DenOordetal.,2017;Islametal.,2022;Leeetal.,2023). densely covering whole Ï‡ space. Thus, we consider a
timestepdependentindexingJ(t)whencomputingthecov-
L (Ï•,Ïˆ,e)=||f ([x=f (s)] )âˆ’s||2
VQ Ïˆ Ï• q 2 erageloss. ThepurposeofintroducingJ(t)istomakeonly
+Î» ||sg[f (s)]âˆ’x ||2+Î» ||f (s)âˆ’sg[x ]||2 sequentiallyselectedquantizedvectorsclosetothecurrent
vq Ï• q 2 commit Ï• q 2
(3) embeddingx sothatquantizedembeddingsareuniformly
t
Here, [Â·] and sg[Â·] represent a quantization process and distributedacrossÏ‡accordingtotimesteps. Then,thefinal
q
stopgradient,respecitvely. Î» andÎ» arescalefactor formofcoveragelosscanbeexpressedasfollows.
vq commit
for correponsding terms. The first term in Eq. (2) is the
reconstructionloss,whilethesecondtermrepresentsVQ- L (e)= 1 (cid:88) ||sg[f (s)]âˆ’e ||2
objectivewhichmakesanembeddingvectoremovetoward cvr |J(t)| Ï• j 2 (5)
jâˆˆJ(t)
x=f (s).Thelasttermcalledacommitmentlossenforces
Ï•
an encoder to generate f Ï•(s) similar to x q and prevents We defer the details of J(t) construction to Appendix E.
itsoutputfromgrowingsignificantly. Toapproximatethe ByconsideringthecoveragelossinEq. (5), notonlythe
gradientsignalforanencoder,weadoptastraight-through nearestquantizedvectorbutalsoallvectorsinthecodebook
estimator(Bengioetal.,2013). movetowardsoveralllatentspaceÏ‡. Inthisway,Ï‡canbe
wellcoveredbyquantizedvectorsinthecodebook. Thus,
WhenadoptingVQ-VAEforstateembedding,wefoundthat
weconsidertheoveralllearningobjectiveasfollows.
onlyafewquantizedvectorseinthecodebookareselected
throughoutanepisode,whichmakesithardtoutilizesucha
Ltot (Ï•,Ïˆ,e)=L (Ï•,Ïˆ,e)+Î» L (e) (6)
methodformeaningfulstateembedding. Wepresumedthat VQ VQ cvr cvr
thereasonisthenarrowprojectedembeddingspacefrom
whereÎ» isascalefactorforL .
feasiblestatescomparedtoawholeembeddingspace,i.e., cov cvr
RD. Thus,mostrandomlyinitializedquantizedvectorse Figure2presentsthevisualizationofembeddingsbyprinci-
locatefarfromthelatentspaceofstatesinthecurrentreplay palcomponentanalysis(PCA)(Woldetal.,1987).InFigure
4LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
2,thetrainingwithoutL leadstoquantizedvectorsthat f (Ï„ ), respectively. Then, latent sequence after quanti-
cvr Ï• st
aredistantfromÏ‡. zation process can be expressed as Ï„ = [f (Ï„ )] =
Ï‡t Ï• xt q
{x ,x ,x ,...,x }. Toevaluatethevaluetheof
In addition, embedding space Ï‡ itself distributes q,t q,t+1 q,t+2 q,T
trajectoryÏ„ ,weuseC valueinthecodebookD of
around a few quantized vectors due to the commit- Ï‡t q,t VQ
aninitialquantizedvectorx inÏ„ .
ment loss in Eq. (3). Considering Lall makes q,t Ï‡t
cvr
quantized vectors close to Ï‡ but they majorly locate ToencouragedesiredtransitionscontainedinÏ„ withhigh
Ï‡t
around the center of Ï‡ rather than distributed properly. C value,weneedtokeepasequencedataofÏ„ . Fora
q,t Ï‡t
Ontheotherhand,thepro- givenstartingnodex ,wekeeptop-ksequencesinD
q,t seq
posedL resultsinwell- basedontheirC . Thus,D consistsoftwoparts;D
cvr q,t seq Ï„Ï‡t
distributedquantizedvec- storestop-ksequencesofÏ„ andD storestheircorre-
Ï‡t Cq,t
tors over Ï‡ space so that spondingC values. Updatingalgorithmforasequence
q,t
they can properly repre- bufferD andstructuraldetailsofD arepresentedin
seq seq
sentlatentspaceofsâˆˆD. AppendixD.
Figure 3 presents the oc-
AsinDefinition3.1,thehighestreturnincooperativemulti-
currenceofrecalledquan-
agenttaskscanonlybeachievedwhenthesemanticgoalis
tizedvectorsforstateem-
satisfied. Thus,onceagentshaveachievedacommongoal
Figure 3: Histogram of re- beddings in Fig. 2. We
duringtraining,goal-reachingtrajectoriesstartingfromvar-
calledquantizedvector. canseethattrainingwith
iousinitialpositionsarestoredinD . Afterweconstruct
Î» guaranteesquantized seq
cvr D ,areferencetrajectoryÏ„âˆ— canbesampledoutofD .
vectorswelldistributedacrossÏ‡. AppendixEpresentsthe seq Ï‡t Ï„Ï‡t
Foragiveninitialpositionx inthequantizedlatentspace,
trainingalgorithmfortheproposedVQ-VAE. q,t
werandomlysampleareferencetrajectoryorgoal-reaching
trajectoryfromD .
seq
4.2.Goal-ReachingTrajectoryGenerationwith
ExtendedVQCodebook
4.3.IntrinsicRewardGeneration
Afterconstructingquantizedvectorsinthecodebook,we
Withagoal-reachingtrajectoryÏ„âˆ— fromthecurrentstate,
need to properly estimate the value of states projected to Ï‡t
wecandeterminethedesiredtransitionsthatleadtoagoal-
each quantized vector. Note that the estimated value of
reachingpath,simplybycheckingwhetherthequantized
eachquantizedvectorisusedwhengeneratinganadditional
latent x at each timestep t is in Ï„âˆ— . However, before
incentivetodesiredtransitions,i.e.,transitiontowardagoal- q,t Ï‡t
quantizedvectorseinthecodebookwellcoverthelatent
reachingtrajectory. Thankstothequantizedvectorsinthe
distribution Ï‡, only a few e vectors are selected and thus
codebook,wecanresorttocount-basedestimationforthe
the same x will be repeatedly obtained. In such a case,
valueestimationofagivenstate. Foragivens , acumu- q
t stayinginthesamex willbeencouragedbyintrinsicre-
lative return from s denoted as R = Î£Tâˆ’1Î³iâˆ’tr , and q
t t i=t i ward if x âˆˆ Ï„âˆ— . To prevent this, we only provide an
x q,t = [x t = f Ï•(s t)] q,thevalueofx q,t canbecomputed incentiveq totheÏ‡ dt esiredtransitiontowardx suchthat
viacount-basedestimationas q,t+1
x âˆˆ Ï„âˆ— and x Ì¸= x . Aremainingproblemis
1
N (cid:88)xq,t hoq, wt+ m1 uchÏ‡ wt eincenq ti, vt+ iz1 esuchq, at desiredtransition. Instead
C q,t(x q,t)=
N
R tj(x q,t) (7) ofanarbitraryincentive,wewanttodesignanadditionalre-
xq,t
j=1 wardtoguaranteeabetterTD-target,toconvergeonoptimal
Here,N isthevisitationcountonx . However,asan policy.
xq,t q,t
e bn ec twod ee er nn ae stw peo cr ik ficf Ï• sti as teup sd aa nt ded xd =uri fn Ï•g (str )a cin ai nng b, reth ake .m Ta ht uc sh
,
P trr ao jep co ts oi rt yio an nd4.1 s.
â€²
âˆˆPro Ï„v âˆ—id ,ed anth ia nt triÏ„ nÏ‡âˆ—
st
icis rea wag ro dal r- Ire (a sâ€²c )hi :n =g
itbecomeshardtoaccuratelyestimatethevalueofsviathe Î³(C (sâ€²)âˆ’max Q
Ï‡t
(sâ€²,aâ€²))tothecurrentTD-target
q,t aâ€² Î¸âˆ’
count-basedvisitonx q,t.Toresolvethis,weadoptamoving y = r(s,a)+Î³V Î¸âˆ’(sâ€²) guarantees a true TD-target as
averagewithabuffersizeofmwhencomputingC q,t(x q,t) yâˆ— =r(s,a)+Î³Vâˆ—(sâ€²),whereVâˆ—(sâ€²)isatruevalueofsâ€².
andstoretheupdatedvalueintheextendedcodebook,D .
VQ
AppendixDpresentsstructuraldetailsofD .
VQ Proof. PleaseseeAppendixA.
AfterconstructingD ,nowweneedtodetermineagoal-
VQ
reaching trajectory Ï„âˆ—, defined in Definition 3.1, in the AccordingtoProposition4.1,whenÏ„âˆ— isagoal-reaching
latent space. This tras jt ectory is considered as a reference trajectory and sâ€² âˆˆ Ï„âˆ— , we can setÏ‡ at true TD-target by
Ï‡t
trajectory to incentivize desired transitions. Let the state addinganintrinsicrewardrI(sâ€²)tothecurrentTD-targety,
sequencefroms anditscorrespondinglatentsequencepro- yieldingabetterconvergenceonanoptimalpolicy. Inthe
t
jected by f as Ï„ = {s ,s ,s ,...,s } and Ï„ = casewhenareferencetrajectoryÏ„âˆ— isnotagoal-reaching
Ï• st t t+1 t+2 T xt Ï‡t
5LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Voronoi-cell
ğ‘Ÿğ¼
ğ‘Ÿğ¼
ğœ ğ‘¥ğ‘¡ ğœ ğœ’ğ‘¡ ğœ ğœ’ğ‘¡
ğœ
ğ‘¥ğ‘¡
ğœ ğœ’âˆ—
ğ‘¡
ğ‘’
(a) Trajectoryembedding. (b) Trajectoryinquantizedlatentspace. (c) Intrinsicrewardgeneration.
Figure4: Intrinsicrewardgenerationbycomparingthecurrenttrajectoryinquantizedlatentspace(Ï„ )withasampled
Ï‡t
goal-reachingtrajectory(Ï„âˆ— ).
Ï‡t
trajectory,rI(sâ€²)incentivizesthetransitiontowardthehigh- 4.4.OverallLearningObjective
returntrajectoryexperiencedsofar. Thus,wedefinealatent
ThispaperadoptsaconventionalCTDEparadigm(Oliehoek
goal-guidedintrinsicrewardrI asfollows.
etal.,2008;Guptaetal.,2017),andthusanyformofmixer
structure can be used for value factorization. We use the
rI(s )=Î³(C (s )âˆ’max Q (s ,aâ€²)), mixer structure presented in QMIX (Rashid et al., 2018)
t t+1 q,t t+1 aâ€² Î¸âˆ’ t+1 (8)
ifx âˆˆÏ„âˆ— andx Ì¸=x similarto(Yangetal.,2022;Wangetal.,2021;Jeonetal.,
q,t+1 Ï‡t q,t+1 q,t 2022)toconstructthejointQ-value(Qtot)fromindividual
Q-functions. Byadoptingthelatentgoal-guidedintrinsic
NotethatrI(s )isaddedtoy =r +Î³V noty . In rewardrI toEq. (1),theoveralllossfunctionforthepolicy
t t+1 t t Î¸âˆ’ t+1
addition,wecanmakesurethatrI becomesnon-negativeso learningcanbeexpressedasfollows.
thataninaccurateestimateofC (sâ€²)intheearlytraining
q,t
phasedoesnotadverselyaffecttheestimationofV Î¸âˆ’. Al- L(Î¸)=(cid:0) rext+rI +Î³max aâ€²Qt Î¸o âˆ’t(sâ€²,aâ€²)âˆ’Qt Î¸ot(s,a)(cid:1)2
gorithm1summarizestheoverallmethodforgoal-reaching (9)
trajectoryandanintrinsicrewardgeneration. Figure4illus-
NotethathererI doesnotincludeanyscalefactortocon-
tratestheschematicdiagramofquantizedtrajectoryembed-
trolitsmagnitude. ForanindividualpolicyviaQ-function,
dingsÏ„ viaVQ-VAEandintrinsicrewardgenerationby
Ï‡t GRUsareadoptedtoencodealocalaction-observationhis-
comparingitwithagoal-reachingtrajectory,Ï„âˆ— .
Ï‡t tory Ï„ to overcome the partial observability in POMDP
similartomostMARLapproaches(Sunehagetal.,2017;
Algorithm1Goal-reachingTrajectoryandIntrinsicReward Rashid et al., 2018; Son et al., 2019; Rashid et al., 2020;
Generation Wangetal.,2020a). However,inEq. (9),weexpressthe
Given: Sequences of the current batch [Ï„i ]B , a se- equationwithsinsteadofÏ„ fortheconcisenessandcoher-
quencebufferD ,anupdateintervaln Ï‡ ft ori= Ï„1 âˆ— ,and encewiththemathematicalderivation. Theoveralltraining
VQ-VAEcodebos oe kq
D
freq Ï‡t
algorithmforbothVQ-VAEtrainingandpolicylearningis
VQ
fori=1toBdo presentedinAppendixE.
ComputeRi
t
fort=0toT do 5.Experiments
Getindexz â†Ï„i
t Ï‡t Inthissection,wepresentexperimentsettingsandresults
ifmod(t,n )then
freq to evaluate the proposed method. We have designed our
RunAlgorithm2toupdateDzt withRi
seq t experimentswiththeintentionofaddressingthefollowing
SampleareferencetrajectoryÏ„âˆ— fromDzt
Ï‡t seq inquiriesdenotedasQ1-3.
else
ifz Gt eâˆˆ tCÏ„ Ï‡ qâˆ— ,t ta â†nd Dz t VztÌ¸= Q.z Ct qâˆ’ ,t1then â€¢ Q sta1 t. e-T oh f-e thp ee -r af ro tr Mm Aan Rc Le o frf amLA ewG oM rkA sii nn bc oo tm hp da er ni ss eon anto
d
(r tI âˆ’1)i â†Î³max(C q,tâˆ’max aâ€²Q Î¸âˆ’(s t,aâ€²),0) sparserewardsettings
endif
â€¢ Q2. Theimpactoftheproposedembeddingmethodon
endif
overallperformance
endfor
endfor â€¢ Q3. The efficiency of latent goal-guided incentive
comparedtootherrewarddesign
6LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure5: PerformancecomparisonofLAGMAagainstbaselinealgorithmsontwoeasyandhardSMACmaps: 1c3s5z,
5m vs 6m,andtwosuperhardSMACmaps: MMM2,6h vs 8z. (Denserewardsetting)
Figure6: PerformancecomparisonofLAGMAagainstbaselinealgorithmsonfourmaps: 3m,8m,2s3z,and2m vs 1z.
(Sparserewardsetting)
We consider complex multi-agent tasks such as SMAC to dense reward settings, LAGMA shows the best perfor-
(Samvelyan et al., 2019) and GRF (Kurach et al., 2020) manceinsparserewardsettingsthankstothelatentgoal-
asbenchmarkproblems. Inaddition,asbaselinealgorithm, guidedincentive. Sparserewardhardlygeneratesareward
we consider various baselines in MARL such as QMIX signalinexperiencereplay,thustrainingwiththeexperience
(Rashidetal.,2018),RODE(Wangetal.,2021)andLDSA oftheexactsamestatetakesalongtimetofindtheoptimal
(Yangetal.,2022)adoptingaroleorskillconditionedpolicy, policy. However,LAGMAconsidersthevalueofsemanti-
MASER(Jeonetal.,2022)presentingagent-wiseindivid- callysimilarstatesprojectedontothesamequantizedvector
ual subgoals from replay buffer, and EMC (Zheng et al., during training, so its learning efficiency is significantly
2021)adoptingepisodiccontrol. AppendixBpresentsfur- increased.
therdetailsofexperimentsettingsandimplementations,and
AppendixGillustratestheresourceusageandthecompu-
tationalcostrequiredfortheimplementationandtraining
ofLAGMA.Inaddition,additionalgeneralizabilitytestsof
LAGMAarepresentedinAppendixF.Ourcodeisavailable
at: https://github.com/aailabkaist/LAGMA.
5.1.PerformanceevaluationonSMAC
Denserewardsettings Fordenserewardsettings,wefol-
lowthedefaultsettingpresentedin(Samvelyanetal.,2019).
Figure 7: Performance comparison of LAGMA against
Figure 5 illustrates the overall performance of LAGMA.
baseline algorithms on two GRF maps: 3 vs 1WK and
Thankstoquantizedembeddingandlatentgoal-guidedin-
CounterAttack(CA) easy. (Sparserewardsetting)
centive,LAGMAshowssignificantperformanceimprove-
mentcomparedtothebackbonealgorithm,i.e.,QMIX,and
otherstate-of-the-art(SOTA)baselinealgorithms,especially 5.2.PerformanceevaluationonGRF
insuperhardSMACmaps.
Here,weconductexperimentsonadditionalsparsereward
Sparserewardsettings Forasparserewardsetting,we tasksinGRFtocompareLAGMAwithbaselinealgorithms.
followtherewarddesigninMASER(Jeonetal.,2022). Ap- Forexperiments,wedonotutilizeanyadditionalalgorithm
pendixBenumeratesthedetailsofrewardsettings. Similar forsampleefficiencysuchasprioritizedexperiencereplay
(Schauletal.,2015)forallalgorithms.
7LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure8: QualitativeanalysisonSMACMMM2(redteamsareRL-agents). Purplestarsrepresentquantizedembeddingsof
goalstatesinreplaybufferD. Yellowdotsindicatethequantizedembeddingsinasampledgoal-reachingtrajectorystarting
fromaninitialstatedenotedbyagreendot. GraydotsandtransparentdotsarethesameasFigure2. Blueandreddots
indicateterminalembeddingsoftwotrajectories,respectively.
EMC(Zhengetal.,2021)showscomparableperformance LAGMA(CL-All)trainedwithÎ»all consideringallquan-
cvr
byutilizinganepisodicbuffer, whichbenefitsingenerat- tizedvectorsateachtimestepandLAGMA(No-CL)trained
ing a positive reward signal via additional episodic con- withoutcoverageloss.
trol term. However, LAGMA with a modified VQ code-
Figure9illustratestheeffectoftheproposedcoverageloss
bookcouldguideascoringpolicywithoututilizinganaddi-
inmodifiedVQ-VAEontheoverallperformance. Asshown
tionalepisodicbufferasbeingrequiredinEMC.Therefore,
in Fig. 9, the performance decreases when the model is
LAGMAachievesasimilarorbetterperformancewithless
trainedwithoutcoveragelossortrainedwithÎ»all insteadof
memoryrequirement. cvr
Î» . Theresultsimplythat,withouttheproposedcoverage
cvr
loss,quantizedlatentvectorsmaynotcoverÏ‡properlyand
5.3.Ablationstudy
thusx canhardlyrepresenttheprojectedstates.Asaresult,
q
In this subsection, we conduct ablation studies to see agoal-reachingtrajectorythatconsistsofafewquantized
the effect of the proposed embedding method and latent vectorsyieldsnoincentivesignalinmosttransitions.
goal-guidedincentiveonoverallperformance. Wecom-
In addition, we conduct an ablation study on reward de-
pare LAGMA (ours) with ablated configurations such as
sign. Weconsiderasumofundiscountedrewards,C =
q0
Î£Tâˆ’1r ,fortrajectoryvalueestimationinsteadofC ,de-
t=0 t q,t
notedasLAMGA(Cq0). WealsoconsidertheLAMGA
configurationwithgoal-reachingtrajectorygenerationonly
attheinitialstatedenotedbyLAMGA(Cqt-No-Upd). Fig-
ure 10 illustrates the results. Figure 10 implies that the
reward design of C shows a more stable performance
q,t
thanbothLAMGA(Cq0)andLAMGA(Cqt-No-Upd).
5.4.Qualitativeanalysis
Figure9: Ablationstudyconsideringthecoverageloss(CL)
onfourSMACmaps:3mand2s3z.(Sparserewardsetting) In this section, we conduct a qualitative analysis to ob-
servehowthestatesinanepisodeareprojectedontoquan-
tizedvectorspaceandreceivelatentgoal-guidedincentive
comparedtogoal-reachingtrajectorysampledfromD .
seq
Figure8illustratesthequantizedembeddingsequencesof
two trajectories: one denoted by a blue line representing
abattle-wontrajectoryandtheotherdenotedbyaredline
representingalosingtrajectory. InFig. 8,alosingtrajectory
initiallyfollowedtheoptimalsequencedenotedbyyellow
dots but began to bifurcate at t = 20 by losing Medivac
Figure10: Performancecomparisonofgoal-guidedincen-
and two more allies. Although the losing trajectory still
tivewithotherrewarddesignchoicesontwoSMACmaps:
passedthroughgoal-reachingsequencesduringanepisode,
3mand2s3z. (Sparserewardsetting)
8LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
itultimatelyreachedaterminalstatewithoutachanceto A., Leibo, J. Z., Rae, J., Wierstra, D., and Hass-
defeattheenemiesatt=40,asindicatedbytheabsenceof abis, D. Model-free episodic control. arXiv preprint
apurplestar. Ontheotherhand,atrajectorythatachieved arXiv:1606.04460,2016.
victoryfollowedthegoal-reachingpathandreachedagoal
stateattheend,asindicatedbypurplestars. Sinceonlytran- Burda, Y., Edwards, H., Storkey, A., andKlimov, O. Ex-
sitionstowardthesequencesonthegoal-reachingpathare plorationbyrandomnetworkdistillation. arXivpreprint
incentivized,LAGMAcanefficientlylearnagoal-reaching arXiv:1810.12894,2018.
policy,i.e.,theoptimalpolicyincooperativeMARL.
Chane-Sane, E., Schmid, C., and Laptev, I. Goal-
conditionedreinforcementlearningwithimaginedsub-
6.Conclusions
goals. InInternationalConferenceonMachineLearning,
pp.1430â€“1440.PMLR,2021.
This paper presents LAGMA, a framework to generate a
goal-reaching trajectory in latent space and a latent goal-
Dasgupta,S. Experimentswithrandomprojection. arXiv
guidedincentivetoachieveacommongoalincooperative
preprintarXiv:1301.3849,2013.
MARL.Thankstothequantizedembeddingspace,theex-
perienceofsemanticallysimilarstatesissharedbystates
Ellis, B., Cook, J., Moalla, S., Samvelyan, M., Sun, M.,
projectedontothesamequantizedvector,yieldingefficient
Mahajan, A., Foerster, J., and Whiteson, S. Smacv2:
training. Theproposedlatentgoal-guidedintrinsicreward
Animprovedbenchmarkforcooperativemulti-agentre-
encourages transitions toward a goal-reaching trajectory.
inforcementlearning. AdvancesinNeuralInformation
Experimentsandablationstudiesvalidatetheeffectiveness
ProcessingSystems,36,2024.
ofLAGMA.
Ghosh,D.,Gupta,A.,andLevine,S. Learningactionable
Acknowledgements representations with goal-conditioned policies. arXiv
preprintarXiv:1811.07819,2018.
This research was supported by AI Technology Develop-
mentforCommonsenseExtraction,Reasoning,andInfer- GrzesÂ´,M.andKudenko,D. Multigridreinforcementlearn-
encefromHeterogeneousData(IITP)fundedbytheMin- ingwithrewardshaping. InInternationalConferenceon
istryofScienceandICT(2022-0-00077). ArtificialNeuralNetworks,pp.357â€“366.Springer,2008.
ImpactStatement Gupta,J.K.,Egorov,M.,andKochenderfer,M.Cooperative
multi-agent control using deep reinforcement learning.
ThispaperprimarilyfocusesonadvancingthefieldofMa- InInternationalconferenceonautonomousagentsand
chineLearningthroughmulti-agentreinforcementlearning. multiagentsystems,pp.66â€“83.Springer,2017.
Whiletherecouldbevariouspotentialsocietalconsequences
ofourwork,noneofwhichwebelievemustbespecifically Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas-
highlightedhere. teringatariwithdiscreteworldmodels. arXivpreprint
arXiv:2010.02193,2020.
References
Hasselt,H. Doubleq-learning. Advancesinneuralinforma-
Andrychowicz,M.,Wolski,F.,Ray,A.,Schneider,J.,Fong, tionprocessingsystems,23,2010.
R.,Welinder,P.,McGrew,B.,Tobin,J.,PieterAbbeel,O.,
Hausknecht,M.andStone,P. Deeprecurrentq-learningfor
andZaremba,W. Hindsightexperiencereplay. Advances
partiallyobservablemdps. In2015aaaifallsymposium
inneuralinformationprocessingsystems,30,2017.
series,2015.
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T.,
Saxton, D., and Munos, R. Unifying count-based ex- Houthooft,R.,Chen,X.,Duan,Y.,Schulman,J.,DeTurck,
plorationandintrinsicmotivation. Advancesinneural F., andAbbeel, P. Vime: Variationalinformationmax-
informationprocessingsystems,29,2016. imizing exploration. Advances in neural information
processingsystems,29,2016.
Bengio,Y.,LeÂ´onard,N.,andCourville,A. Estimatingor
propagatinggradientsthroughstochasticneuronsforcon- Islam,R.,Zang,H.,Goyal,A.,Lamb,A.,Kawaguchi,K.,
ditionalcomputation. arXivpreprintarXiv:1308.3432, Li, X., Laroche, R., Bengio, Y., and Combes, R. T. D.
2013. Discrete factorial representations as an abstraction for
goalconditionedreinforcementlearning. arXivpreprint
Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, arXiv:2211.00247,2022.
9LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega, Lowe,R.,Wu,Y.,Tamar,A.,Harb,J.,Abbeel,P.,andMor-
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social datch,I. Multi-agentactor-criticformixedcooperative-
influence as intrinsic motivation for multi-agent deep competitiveenvironments. NeuralInformationProcess-
reinforcementlearning. InInternationalconferenceon ingSystems(NIPS),2017.
machinelearning,pp.3040â€“3049.PMLR,2019.
Mahajan,A.,Rashid,T.,Samvelyan,M.,andWhiteson,S.
Jeon, J., Kim, W., Jung, W., and Sung, Y. Maser: Multi- Maven: Multi-agentvariationalexploration. Advancesin
agent reinforcement learning with subgoals generated NeuralInformationProcessingSystems,32,2019.
fromexperiencereplaybuffer. InInternationalConfer-
Mguni, D. H., Jafferjee, T., Wang, J., Perez-Nieves, N.,
ence on Machine Learning, pp. 10041â€“10052. PMLR,
Slumbers, O., Tong, F., Li, Y., Zhu, J., Yang, Y., and
2022.
Wang, J. Ligs: Learnable intrinsic-reward genera-
Jiang,N.,Kulesza,A.,andSingh,S. Abstractionselection tion selection for multi-agent learning. arXiv preprint
inmodel-basedreinforcementlearning. InInternational arXiv:2112.02618,2021.
ConferenceonMachineLearning,pp.179â€“188.PMLR,
Mohamed, S. and Jimenez Rezende, D. Variational in-
2015.
formation maximisation for intrinsically motivated re-
inforcement learning. Advances in neural information
Kaelbling,L.P. Learningtoachievegoals. InIJCAI,vol-
processingsystems,28,2015.
ume2,pp.1094â€“8.Citeseer,1993.
Na, H., Seo, Y., andMoon, I.-c. Efficientepisodicmem-
Kim,H.,Kim,J.,Jeong,Y.,Levine,S.,andSong,H.O.Emi:
oryutilizationofcooperativemulti-agentreinforcement
Exploration with mutual information. arXiv preprint
learning. arXivpreprintarXiv:2403.01112,2024.
arXiv:1810.01176,2018.
Nasiriany, S., Pong, V., Lin, S., and Levine, S. Planning
Kim, J., Seo, Y., Ahn, S., Son, K., and Shin, J. Imitat-
withgoal-conditionedpolicies. AdvancesinNeuralInfor-
inggraph-basedplanningwithgoal-conditionedpolicies.
mationProcessingSystems,32,2019.
arXivpreprintarXiv:2303.11166,2023.
Oliehoek, F.A.andAmato, C. Aconciseintroductionto
Kulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenen-
decentralizedPOMDPs. Springer,2016.
baum,J. Hierarchicaldeepreinforcementlearning: In-
tegrating temporal abstractionand intrinsicmotivation. Oliehoek,F.A.,Spaan,M.T.,andVlassis,N. Optimaland
Advancesinneuralinformationprocessingsystems,29, approximateq-valuefunctionsfordecentralizedpomdps.
2016. JournalofArtificialIntelligenceResearch,32:289â€“353,
2008.
Kurach,K.,Raichuk,A.,Stanczyk,P.,Zajkc,M.,Bachem,
O.,Espeholt,L.,Riquelme,C.,Vincent,D.,Michalski, Ostrovski,G.,Bellemare,M.G.,Oord,A.,andMunos,R.
M., Bousquet, O., et al. Google research football: A Count-basedexplorationwithneuraldensitymodels. In
novelreinforcementlearningenvironment. InProceed- Internationalconferenceonmachinelearning,pp.2721â€“
ings of the AAAI Conference on Artificial Intelligence, 2730.PMLR,2017.
volume34,pp.4501â€“4510,2020.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Lee,S.,Cho,D.,Park,J.,andKim,H.J. Cqm: Curriculum Curiosity-driven exploration by self-supervised predic-
reinforcement learning with a quantized world model. tion. InInternationalconferenceonmachinelearning,
arXivpreprintarXiv:2310.17330,2023. pp.2778â€“2787.PMLR,2017.
Lengyel,M.andDayan,P. Hippocampalcontributionsto Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G.,
control: thethirdway. Advancesinneuralinformation Foerster,J.,andWhiteson,S. Qmix: Monotonicvalue
processingsystems,20,2007. functionfactorisationfordeepmulti-agentreinforcement
learning. InInternationalconferenceonmachinelearn-
Li,Z.,Zhu,D.,Hu,Y.,Xie,X.,Ma,L.,Zheng,Y.,Song,Y., ing,pp.4295â€“4304.PMLR,2018.
Chen,Y.,andZhao,J. Neuralepisodiccontrolwithstate
abstraction. arXivpreprintarXiv:2301.11490,2023. Rashid, T., Farquhar, G., Peng, B., and Whiteson, S.
Weighted qmix: Expanding monotonic value function
Liu,Y.,Li,Y.,Xu,X.,Dou,Y.,andLiu,D. Heterogeneous factorisationfordeepmulti-agentreinforcementlearning.
skilllearningformulti-agenttasks. AdvancesinNeural Advancesinneuralinformationprocessingsystems,33:
InformationProcessingSystems,35:37011â€“37023,2022. 10199â€“10210,2020.
10LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Samvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Wang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson,
Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., S., andZhang, C. Rode: Learningrolestodecompose
Foerster,J.,andWhiteson,S. Thestarcraftmulti-agent multi-agent tasks. In Proceedings of the International
challenge. arXivpreprintarXiv:1902.04043,2019. ConferenceonLearningRepresentations(ICLR),2021.
Schaul,T.,Horgan,D.,Gregor,K.,andSilver,D. Universal Wold, S., Esbensen, K., and Geladi, P. Principal compo-
valuefunctionapproximators.InInternationalconference nentanalysis. Chemometricsandintelligentlaboratory
onmachinelearning,pp.1312â€“1320.PMLR,2015. systems,2(1-3):37â€“52,1987.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., andYi, Yang,J.,Borovikov,I.,andZha,H.Hierarchicalcooperative
Y. Qtran: Learningtofactorizewithtransformationfor multi-agentreinforcementlearningwithskilldiscovery.
cooperativemulti-agentreinforcementlearning. InInter- arXivpreprintarXiv:1912.03558,2019.
nationalconferenceonmachinelearning,pp.5887â€“5896.
PMLR,2019. Yang, M., Zhao, J., Hu, X., Zhou, W., Zhu, J., andLi, H.
Ldsa: Learningdynamicsubtaskassignmentincooper-
Stadie,B.C.,Levine,S.,andAbbeel,P. Incentivizingex- ative multi-agent reinforcement learning. Advances in
plorationinreinforcementlearningwithdeeppredictive NeuralInformationProcessingSystems,35:1698â€“1710,
models. arXivpreprintarXiv:1507.00814,2015. 2022.
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zam- Zhang, T., Guo, S., Tan, T., Hu, X., and Chen, F. Gen-
baldi,V.,Jaderberg,M.,Lanctot,M.,Sonnerat,N.,Leibo, erating adjacency-constrained subgoals in hierarchical
J. Z., Tuyls, K., et al. Value-decomposition networks reinforcementlearning. AdvancesinNeuralInformation
for cooperative multi-agent learning. arXiv preprint ProcessingSystems,33:21579â€“21590,2020.
arXiv:1706.05296,2017.
Zheng, L., Chen, J., Wang, J., He, J., Hu, Y., Chen, Y.,
Sutton,R.S.andBarto,A.G. Reinforcementlearning: An
Fan, C., Gao, Y., and Zhang, C. Episodic multi-agent
introduction. MITpress,2018.
reinforcementlearningwithcuriosity-drivenexploration.
AdvancesinNeuralInformationProcessingSystems,34:
Tang,H.,Houthooft,R.,Foote,D.,Stooke,A.,XiChen,O.,
3757â€“3769,2021.
Duan, Y., Schulman, J., DeTurck, F., andAbbeel, P. #
exploration: Astudyofcount-basedexplorationfordeep
Zhu,D.,Chen,J.,Shang,W.,Zhou,X.,Grossklags,J.,and
reinforcementlearning. Advancesinneuralinformation
Hassan,A.E. Deepmemory: model-basedmemorization
processingsystems,30,2017.
analysisofdeepneurallanguagemodels. In202136th
IEEE/ACMInternationalConferenceonAutomatedSoft-
Tang, Y. andAgrawal, S. Discretizingcontinuous action
wareEngineering(ASE),pp.1003â€“1015.IEEE,2021.
spaceforon-policyoptimization. InProceedingsofthe
aaaiconferenceonartificialintelligence,volume34,pp.
5981â€“5988,2020.
VanDenOord,A.,Vinyals,O.,etal. Neuraldiscreterep-
resentation learning. Advances in neural information
processingsystems,30,2017.
VanHasselt,H.,Guez,A.,andSilver,D. Deepreinforce-
mentlearningwithdoubleq-learning. InProceedingsof
theAAAIconferenceonartificialintelligence,volume30,
2016.
Wang, J., Ren, Z., Liu, T., Yu, Y., and Zhang, C. Qplex:
Duplexduelingmulti-agentq-learning. arXivpreprint
arXiv:2008.01062,2020a.
Wang, T., Wang, J., Wu, Y., and Zhang, C. Influence-
based multi-agent exploration. arXiv preprint
arXiv:1910.05512,2019.
Wang,T.,Dong,H.,Lesser,V.,andZhang,C. Roma:Multi-
agentreinforcementlearningwithemergentroles. arXiv
preprintarXiv:2003.08039,2020b.
11LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
A.MathematicalProof
Here,weprovidetheomittedproofofProposition4.1.
Proof. Lety =r(s,a)+Î³V bethecurrentTD-targetwiththetargetnetworkparameterizedwithÎ¸âˆ’. Addinganintrinsic
Î¸âˆ’
rewardrI(sâ€²)toyyieldsyâ€² =y+rI(sâ€²). Now,weneedtocheckwhetheryâ€²accuratelyestimatesyâˆ— =r+Î³Vâˆ—(sâ€²).
E[yâ€²]=E[r(s,a)+Î³V +rI(sâ€²)]
Î¸âˆ’
=E[r(s,a)+Î³max Q (sâ€²,aâ€²)+Î³(C (sâ€²)âˆ’max Q (sâ€²,aâ€²))]
aâ€² Î¸âˆ’ q,t aâ€² Î¸âˆ’
=E[r(s,a)+Î³(C (sâ€²))]
q,t
T (cid:88)âˆ’1 (10)
=E[r(s,a)+Î³(E[ Î³iâˆ’(t+1)r ])]
i
i=t+1
=r(s,a)+Î³E[r +Î³r +Â·Â·Â·+Î³Tâˆ’tâˆ’2r ]
t+1 t+2 Tâˆ’1
=r(s,a)+Î³Vâˆ—(sâ€²)
ThelastequalityinEq. (10)holdssincesâ€²isonagoal-reachingtrajectory,i.e.,sâ€² âˆˆÏ„âˆ— whosereturnismaximized,and
E[r +Î³r +Â·Â·Â·]isanunbiasedMonte-CarloestimateofVâˆ—(sâ€²).
Ï‡0
t+1 t+2
B.ExperimentDetails
In this section, we present details of SMAC (Samvelyan et al., 2019) and GRF (Kurach et al., 2020), and we also list
hyperparemetersettingsofLAGMAforeachtask. Tables1and2presentthedimensionsofstateandactionspacesandthe
maximumepisodiclength.
Table1: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofSMAC
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3m 48 9 60
8m 168 14 120
2s3z 120 11 120
2m vs 1z 26 7 150
1c3s5z 270 15 180
5m vs 6m 98 12 70
MMM2 322 18 180
6h vs 8z 140 14 150
Table2: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofGRF
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3 vs 1WK 26 19 150
CA easy 30 19 150
Inaddition,Table3presentsthetask-dependenthyperparametersettingsforallexperiments. AsseenfromTable3,weused
similarhyperparametersacrossvarioustasks. Foranupdateintervaln inAlgorithm1,weusethesamevaluen =5
freq freq
forallexperiments. Ïµ representsannealingtimeforexplorationrateofÏµ-greedy,from1.0to0.05.
T
Aftersomeparametricstudies,adjustinghyperparameterforVQ-VAEtrainingsuchasncd andnvq ,insteadofvaryingÎ»
freq freq
valueslistedasÎ» ,Î» ,andÎ» ,providesmoreefficientwayofsearchingparametricspace. Thus,weprimarilyadjust
vq commit cvr
ncd andnvq accordingtotasks,whilekeepingtheratiobetweenÎ»valuesthesame.
freq freq
12LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Forhyperparametersettings,werecommendtheefficientboundsforeachhyperparameterbasedonourexperimentsas
follows:
â€¢ Numberofcodebook,n : 64-512
c
â€¢ UpdateintervalforVQ-VAE,nvq : 10-40(underncd =10)
freq freq
â€¢ Updateintervalforextendedcodebook,ncd : 10-40(undernvq =10)
freq freq
â€¢ Numberofreferencetrajectory,k: 10-30
â€¢ Scalefactorofcoverageloss,Î» : 0.25-1.0(underÎ» =1.0andÎ» =0.5)
cvr vq commit
Notethatlargervaluesofn andk,andsmallervaluesofnvq andncd willincreasethecomputationalcost.
c freq freq
Table3: Hyperparametersettingsforexperiments.
task n D Î» Î» Î» Ïµ ncd nvq
c vq commit cvr T freq freq
3m 256 8 2.0 1.0 1.0 50K 10 40
8m 256 8 2.0 1.0 1.0 50K 20 10
SMAC(sparse)
2s3z 256 8 2.0 1.0 1.0 50K 10 40
2m vs 1z 256 8 2.0 1.0 1.0 500K 20 10
1c3s5z 64 8 1.0 0.5 0.5 50K 40 10
5m vs 6m 64 8 1.0 0.5 0.5 50K 40 10
SMAC(dense)
MMM2 64 8 1.0 0.5 0.5 50K 40 10
6h vs 8z 256 8 2.0 1.0 1.0 500K 40 10
3 vs 1WK 256 8 2.0 1.0 1.0 50K 20 10
GRF(sparse)
CA easy 256 8 2.0 1.0 1.0 50K 10 20
Table4presentstherewardsettingsforSMAC(sparse)whichfollowsthesparserewardsettingsfrom(Jeonetal.,2022).
Table4: RewardsettingsforSMAC(sparse)
Condition Sparsereward
Allenemiesdie(Win) +200
Eachenemydies +10
Eachallydies -5
13LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
C.AdditionalRelatedWorks
Goal-conditionedRLvs. SubtaskconditionedMARL Inasingleagentcase,Goal-conditionedRL(GCRL)which
aimstosolvemultipletaskstoreachgiventarget-goalshasbeenwidelyadoptedinvarioustasksincludingtaskswitha
sparsereward(Kaelbling,1993;Schauletal.,2015;Andrychowiczetal.,2017). GCRLoftenutilizesthegivengoalas
anadditionalinputtoactionpolicyinadditiontostates(Schauletal.,2015). Especially,goal-conditionedhierarchical
reinforcementlearning(HRL)(Kulkarnietal.,2016;Zhangetal.,2020;Chane-Saneetal.,2021)adoptshierarchicalpolicy
structurewhereanupper-tierpolicydeterminessubgoalorlandmarkandalower-tierpolicytakesactionbasedonbothstate
andselectedasubgoalorlandmark.
Asonetechnique,reachingtosubgoalsgeneratesarewardsignalviahindsightexperiencereplay(Andrychowiczetal.,
2017),andthusthegoal-conditionedpolicylearnpolicytoreachthefinalgoalwiththehelpoftheseintermediatesignals.
Thus,manyresearchers(Nasirianyetal.,2019;Zhangetal.,2020;Chane-Saneetal.,2021;Kimetal.,2023;Leeetal.,
2023)havestudiedonhowtogenerateintermediatesubgoalstoreachfinalgoals.
InthefieldofMARL,asubtask(Yangetal.,2022),role(Wangetal.,2020b;2021)orskill(Yangetal.,2019;Liuetal.,2022)
conditionedpolicyadoptedinahierarchicalMARLstructurehasastructuralcommonalitywithagoal-conditionedRLin
thatlower-tierpolicynetworkusedesignatedsubtaskbytheupper-tiernetworkasanadditionalinputwhendetermining
individualaction. InMARLtasks,suchsubtasks,roles,orskillsareabitdifferentfromsubgoalsinGCRL,astheyare
adoptedtodecomposeactionspaceforefficienttrainingorforsubtask-dependentcoordination. Anothermajordifferenceis
thatinageneralMARLtask,thefinalgoalisnotdefinedexplicitlyunlikeagoal-conditionedRL.MASER(Jeonetal.,
2022)adoptsthesubgoalgenerationschemefromgoal-conditionedRLwhenitgeneratesanintrinsicrewardbasedonthe
Euclideandistancebetweenactionablerepresentations(Ghoshetal.,2018)ofthecurrentandsubgoalobservation. However,
thissignaldoesnotguaranteetheconsistencywithlearningsignalforthejointQ-function. IncontrasttoMASER,weadopt
alatentgoal-guidedincentiveduringacentralizedtrainingphasebasedonwhethervisitingonthepromisingsubgoalsor
goalsinthelatentspace. Also,thegeneratedincentivebyLAGMAtheoreticallyguaranteesabetterTD-target,yielding
betterconvergenceontheoptimalpolicy.
D.StructureofExtendedVQCodebook
TocomputeC viaamovingaverage,dataisstoredinaFIFO(Firstin,FirstOut)styletothecodebookD ,similar
q,t VQ
to a replay buffer D. After computing C (x ) with the current R , we update the value of C (x ) in D as
q,t q,t t q,t q,t VQ
Dzt .C â†C (x )wherez isanindexofaquantizedvectorx .
VQ q,t q,t q,t t q,t
Extended VQ Codebook
ğ’Ÿ
ğ‘‰ğ‘„ ğ’Ÿ ğ’Ÿ ğ’Ÿ
ğ‘ ğ‘’ğ‘ ğ¶ğ‘,ğ‘¡ ğœğœ’ğ‘¡
ğ‘… ğ‘¡(1,1) â‹® ğ‘… ğ‘¡(1,ğ‘š) ğ‘’ 1 ğ’Ÿ ğ‘ 1 ğ‘’ğ‘ ğ¶ ğ‘1 ,ğ‘¡ =ğ¶ ğ‘m ,ğ‘¡in ğ‘¥ ğ‘1 ,ğ‘¡ ğ‘¥ ğ‘1 ,ğ‘¡+1 â‹®
ğ‘… ğ‘¡(2,1) â‹® ğ‘… ğ‘¡(2,ğ‘š) ğ‘’ 2 ğ’Ÿ ğ‘ 2 ğ‘’ğ‘ â‹® â‹® â‹® â‹®
â‹® â‹® â‹® â‹® ğ¶ ğ‘âˆ— ,ğ‘¡ â‹® â‹® â‹® ~ğœ ğœ’âˆ— ğ‘¡
ğ¶ ğ‘ğ‘§ ,ğ‘¡ ğ‘… ğ‘¡(ğ‘§,1) â‹® ğ‘… ğ‘¡(ğ‘§,ğ‘š) ğ‘’ ğ‘§ ğ‘¥ ğ‘ ğ’Ÿ ğ‘ ğ‘§ ğ‘’ğ‘ ={ğ’Ÿ ğ¶ğ‘,ğ‘¡,ğ’Ÿ ğœğœ’ğ‘¡}
â‹® â‹® â‹® â‹®
â‹® â‹® â‹® â‹®
ğ‘… ğ‘¡(ğ‘›ğ‘,1) â‹® ğ‘… ğ‘¡(ğ‘›ğ‘,ğ‘š) ğ‘’ ğ‘›ğ‘ ğ’Ÿ ğ‘ ğ‘› ğ‘’ğ‘ ğ‘ ğ¶ ğ‘ğ‘˜ ,ğ‘¡ =ğ¶ ğ‘m ,ğ‘¡ax ğ‘¥ ğ‘ğ‘˜ ,ğ‘¡ ğ‘¥ ğ‘ğ‘˜ ,ğ‘¡+1 â‹®
Figure11: VQcodebookstructure.
InAlgorithm2,heap pushandheap replaceadopttheconventionalheapspacemanagementrule,withacomputa-
tionalcomplexityofO(logk). ThedifferenceisthatweadditionallystorethesequenceinformationinD accordingto
Ï„Ï‡t
theirC values.
q,t
14LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Algorithm2UpdateSequenceBufferD
seq
1: D seq keeptopktrajectorysequencesbasedontheirC q,t
2: Input: AtotalrewardsumR tandasequenceÏ„ Ï‡t
3: Getaninitialindexz t â†Ï„ Ï‡t[0]
4: GetD Cq,t,D Ï„Ï‡t fromD sz et q
5: if|D Cq,t|<kthen
6: heap push(D Cq,t,D Ï„Ï‡t,C q,t,Ï„ Ï‡t)
7: else
8: C qm ,i tn â†D Cq,t[0]
9: ifR t >C qm ,i tnthen
10: heap replace(D Cq,t,D Ï„Ï‡t,R t,Ï„ Ï‡t)
11: endif
12: endif
E.ImplementationDetails
Inthissection,wepresentfurtherdetailsoftheimplementationforLAGMA.Algorithm3presentsthepseudo-codefora
timestepdependentindexingJ(t)usedinEq. (5). ThepurposeofatimestepdependentindexingJ(t)istodistributethe
quantizedvectorsthroughoutanepisode. Thus,Algorithm3triestouniformlydistributequantizedvectorsaccordingtothe
maximumbatchtimeofanepisode. Byconsideringthemaximumbatchtimeofeachepisode,Algorithm3canadaptively
distributequantizedvectors.
Algorithm3ComputeJ(t)
1: Input: ForgiventhemaximumbatchtimeT,thenumberofcodebookn c,andthecurrenttimestept
2: ift==0then
3: d=âŒŠn c/TâŒ‹
4: r =n c modT
5: i s =d nÃ—T
6: Keepthevaluesofd,r,i suntiltheendoftheepisode
7: endif
8: ifdâ‰¥1then
9: J(t)=dÃ—t:1:dÃ—(t+1)
10: ift<rthen
11: AppendJ(t)withi s+t
12: endif
13: else
14: J(t)=âŒŠtÃ—n c/TâŒ‹
15: endif
16: returnJ(t)
Forgiventhemaximumbatchtimet andthenumberofcodebookn ,Line#4andLine#5inAlgorithm3compute
max c
thequotientandremainder,respectively. Line#8computeanarraywithincreasingorderstartingfromtheindexdÃ—tto
dÃ—(t+1). Line#10additionallydesignatetheremainingquantizedvectorstotheearlytimeofanepisode.
Algorithm4presentstrainingalgorithmtoupdateencoderf ,decoderf ,andquantizedembeddingseinVQ-VAE.In
Ï• Ïˆ
Algorithm4,wealsopresentaseparateupdateforD ,whichestimatesthevalueofeachquantizedvectorinVQ-VAE.In
VQ
addition,theoveralltrainingalgorithmincludingtrainingforVQ-VAEispresentedinAlgorithm5.
15LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Algorithm4TrainingalgorithmforVQ-VAEandD
VQ
Parameter: learningrateÎ±andbatch-sizeB
Input: Bsampletrajectories[T]B fromreplaybufferD,thecurrentepisodenumbern ,anupdateintervalnvq for
i=1 epi freq
VQ-VAEandncd forD updateinterval.
freq VQ
fort=0toT do
fori=1toBdo
ifmod(n ,ncd )then
epi freq
GetRi andupdateD withEq. (7).
t VQ
endif
ifmod(n ,nvq )then
epi freq
Getcurrentstates âˆ¼[T] andcomputeJ(t)viaAlgorithm3
t i=1
ComputeLtot viaEq. (6)withf ,f ,ande.
VQ Ï• Ïˆ
endif
endfor
endfor
ifmod(n ,nvq )then
epi freq
âˆ‚Ltot âˆ‚Ltot âˆ‚Ltot
UpdateÏ•â†Ï•âˆ’Î± VQ,Ïˆ â†Ïˆâˆ’Î± VQ,eâ†eâˆ’Î± VQ
âˆ‚Ï• âˆ‚Ïˆ âˆ‚e
endif
Algorithm5TrainingalgorithmforLAGMA.
1: Parameter: BatchsizeBandthemaximumtrainingtimeT env
2: Input: Qi
Î¸
isindividualQ-networkofnagents,replaybufferD,extendedVQcodebookD VQ,andsequencebuffer
D
seq
3: InitializenetworkparametersÎ¸,Ï•,Ïˆ,e
4: whilet env â‰¤T env do
5: InteractwiththeenvironmentviaÏµ-greedypolicybasedon[Qi]n andgetatrajectoryT
Î¸ i=1
6: AppendT toD
7: GetBsampletrajectories[T]B âˆ¼D
i=1
8: Foragiven[T]B ,runMARLtrainingalgorithmandAlgorithm1toupdateÎ¸ withEq. (9),andAlgorithm4to
i=1
updateÏ•,Ïˆ,ewithEq. (6)
9: endwhile
F.GeneralizabilityofLAGMA
F.1.Policyrobustnesstest
Toassesstherobustnessofpolicylearnedbyourmodel,wedesignedtaskswiththesameunitconfigurationbuthighlyvaried
initialpositions,onesthatagentshadnotencounteredduringtraining,i.e.,unseenmaps. Withthesesettings,opponent
agentswillalsoexperiencetotallydifferentrelativepositionsandthuswillmakedifferentdecisions. Wesetthedifferent
initialpositionsforthisevaluationasfollows.
AsillustratedinFigure12,theinitialpositionofeachtaskissignificantlymovedfromthenominalpositionexperienced
duringthetrainingphase.
Forthecomparison,weconductthesameexperimentforotherbaselines,suchasQMIX(Rashidetal.,2018)andLDSA
(Yangetal.,2022). ThemodelbyeachalgorithmistrainedforT =1M innominalMMM2map(denotedasNominal)
env
andthenevaluatedundervariousproblemsettings,suchasNW(hard),NW,SW,andSW(hard).Eachevaluationisconducted
for5differentseedswith32testsandTable5showsthemeanandvarianceofwinrateofeachcase.
InTable5, LAGMAshowsnotonlythebestperformancebutalsotherobustperformanceinvariousproblemsettings.
The fast learning of LAGMA is attributed to the latent goal-guided incentive, which generates accurate TD-target by
utilizingvaluesofsemanticallysimilarstatesprojectedtothesamequantizedvector. BecauseLAGMAutilizesthevalueof
semanticallysimilarstatesratherthanthespecificstateswhenlearningQ-values,differentyetsemanticallysimilarstatestend
tohavesimilarQ-values,yieldinggeneralizablepolicies. Inthismanner,LAGMAwouldenablefurtherexploration,rather
16LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
Figure12: Problemsettingsforpolicyrobustnesstest. Team1representstheinitialpositionofRLagents,whileTeam2is
theinitialpositionofopponents.
Table5: PolicyrobustnesstestonSMACMMM2(superhard). AllmodelsaretrainedforT =1M. Thepercentage(%)
env
inparenthesesrepresentstheratiocomparedtoanominalmeanvalue.
NW(hard) NW Nominal SW SW(hard)
LAGMA 0.275Â±0.064(28.2%) 0.500Â±0.104(51.3%) 0.975Â±0.026 0.556Â±0.051(57.1%) 0.394Â±0.042(40.4%)
QMIX 0.050Â±0.036(13.1%) 0.138Â±0.100(36.1%) 0.381Â±0.078 0.194Â±0.092(50.8%) 0.156Â±0.058(41.0%)
LDSA 0.000Â±0.000(0.0%) 0.081Â±0.047(18.3%) 0.444Â±0.107 0.063Â±0.049(14.1%) 0.081Â±0.028(18.3%)
thansolelyenforcingexploitationofanidentifiedstatetrajectory. Thus,eventhoughthetransitiontowardagoal-reaching
trajectoryisencouragedduringtraining,thepolicylearnedbyLAGMAdoesnotoverfittospecifictrajectoriesandexhibits
robustnesstounseenmaps.
F.2.Scalabilitytest
LAGMAcanbeadoptedtolarge-scaleproblemswithoutanymodifications. VQ-VAEtakesaglobalstateasaninputto
projectthemintoquantizedlatentspace. Thus,inlarge-scaleproblems,onlytheinputsizewilldifferfromtaskswitha
smallnumberofagents. Inaddition,manyMARLtasksincludehigh-dimensionalglobalinputsizeaspresentedinTable1
inthemanuscript. ToassessthescalablityofLAGMA,weconductadditionalexperimentsin27m vs 30mSMACtask,
whosestatedimensionis1170. Figure13illustratestheperformanceofLAGMA.InFigure13,LAGMAmaintainsefficient
learning performance even when applied to large-scale problems, using identical hyperparameter settings as those for
small-scaleproblemssuchas5m vs 6m.
(a) Learningcurve.
Figure13: Performanceonlarge-scaleproblem(27m vs 30mSMAC).
17LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
F.3.Generalizabilitytesttoproblemswithdiversesemanticgoals
To show the generalizability of LAGMA further, we conducted additional experiments on another benchmark such as
SMACv2(Ellisetal.,2024),whichincludesdiversityininitialpositionsandunitcombinationswithintheidenticaltask.
Thus,fromtheperspectiveoflatentspace,SMACv2tasksmayencompassdistinctmultiplegoals,evenwithinthesametask.
Forevaluation,weadoptthesamehyperparametersasthosefor3 vs 1WKpresentedinTable3inthemanuscript,except
forD =4andncd =40.
freq
(a) protoss 5 vs 5 (b) protoss 10 vs 11
Figure14: PerformanceevaluationonSMACv2.
InFigure14,LAGMAshowscomparableorbetterperformancethanbaselinealgorithms,butitdoesnotexhibitdistinctively
strongperformance,unlikeotherbenchmarkproblems. Wedeemthatthisresultstemsfromcharacteristicsofthecurrent
LAGMAcapturingreferencetrajectoriestowardsasimilargoalintheearlytrainingphase.
Multi-objective(ormultiplegoals)tasksmayrequireadiversereferencetrajectorygeneration. ThecurrentLAGMAonly
considersthereturnofatrajectorywhenstoringreferencetrajectoriesinD . Thus,whentrajectoriestowarddifferentgoals
seq
bifurcatefromthesamequantizedvector,i.e.,semanticallysimilarstates,theymaynotbecapturedbythecurrentversionof
LAGMAalgorithmiftheirreturnisrelativelylowcomparedtothatofotherreferencetrajectoriesalreadystoredinD .
seq
Thus,LAGMAmaynotexhibitstrongeffectivenessinsuchtasksuntilvariousreferencetrajectoriestowarddifferentgoals
arestoredforagivenquantizedvector.
Toimprove,onemayalsoconsiderthediversityofatrajectorywhenstoringareferencetrajectoryinD . Inaddition,goal
seq
orstrategy-dependentagent-wiseexecutionwouldenhancecoordinationinsuchproblemcases,butitmayleadtodelayed
learningineasytasks. Thestudyregardingthistrade-offwouldbeaninterestingdirectionforfutureresearch.
18LAGMA:LAtentGoal-guidedMulti-AgentReinforcementLearning
G.Computationalcostanalysis
G.1.Resourceusage
TheintroductionofanextendedVQcodebookinLAGMArequiresadditionalmemoryusagetoanoverallMARLframework.
Memoryusagedependsonthecodebooknumber(n ),thenumberofareferencetrajectory(indexsequence)tosave(k)in
c
sequencebufferD ,itsbatchtimelength(T),thetotalnumberofdatasavedformovingaveragecomputation(m),and
seq
datatype. MemoryusageofD andD arecomputedasfollows.
Ï„Ï‡t VQ
â€¢ D :byte(dtype)Ã—n Ã—kÃ—T
Ï„Ï‡t c
â€¢ D :byte(dtype)Ã—n Ã—m
VQ c
Forexample,whenm=100,n =64,k =30,T =T =150,i.e.,themaximumtimestepdefinedbytheenvironment,
c max
and D and D use data type int64 and float32, respectively, resource usages by introducing extended VQ
Ï„Ï‡t VQ
codebookarecomputedasfollows:
â€¢ (D ) : 8(int64)Ã—64Ã—30Ã—150=2.19MiB
Ï„Ï‡t max
â€¢ D : 4(float32)Ã—64Ã—100=25.6KiB
VQ
Here,(D ) valuerepresentsthepossiblemaximumvalueandtheactualvaluemayvarybasedonthegoal-reaching
Ï„Ï‡t max
trajectoryofeachtask. Wecanseethatresourcerequirementduetotheintroductionoftheextendedcodebookismarginal
comparedtothatofthereplaybufferandtheGPUâ€™smemorycapacity,suchas24GiBinGeForceRTX3090. Notethatany
ofthesememoryusagesdonotdependonthedimensionofstatessinceonlytheindex(z)ofthequantizedvector(x )ofa
q
sequenceisstoredinD .
Ï„Ï‡t
G.2.Trainingtimeanalysis
InLAGMA,weneedtoconductanadditionalupdateforVQ-VAEandtheextendedcodebook. Thus,theupdatefrequency
ofVQ-VAEandtheextendedcodebookwouldaffecttheoveralltrainingtime. Inthemanuscript,weutilizetheidentical
updateintervalnvq = 10,indicatingtrainingonceevery10MARLtrainingiterationsforbothVQ-VAEandcodebook
freq
update. Table6representstheoveralltrainingtimetakenbyvariousalgorithmsfordiversetasks. GeForceRTX3090is
usedfor5m vs 6mandGeForceRTX4090for8m(sparse)andMMM2. Inthecaseof8m(sparse)task,thetraining
timevariesaccordingtowhetherthelearnedmodelfindspolicyachievingacommongoal. Thus,thetrainingtimeofthe
successfulcaseispresentedforeachalgorithm.
Table6: TrainingtimeforeachmodelinvariousSMACmaps(inhours).
Model 5m vs 6m(2M) 8m(sparse)(3M) MMM2(3M)
EMC 8.6 11.8 12.0
MASER 12.7 13.4 20.5
LDSA 5.6 11.0 9.8
LAGMA 10.5 12.6 17.7
Here,numbersinparenthesisrepresentthemaximumtrainingtime(T )accordingtotasks. InTable6,wecanseethat
env
trainingofLAGMAdoesnottakemuchtimecomparedtoexistingbaselinealgorithms. Therefore,wecanconcludethatthe
introductionofVQ-VAEandtheextendedcodebookinLAGMAimposesanacceptablecomputationalburden,withonly
marginalincreasesinresourcerequirements.
19