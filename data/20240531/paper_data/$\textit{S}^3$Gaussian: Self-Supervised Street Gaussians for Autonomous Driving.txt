3
S Gaussian: Self-Supervised Street Gaussians for
Autonomous Driving
NanHuang1,2,‚àó XiaobaoWei2 WenzhaoZheng1,3,‚Ä† PengjuAn2 MingLu2
WeiZhan1 MasayoshiTomizuka1 KurtKeutzer1 ShanghangZhang2,‚Ä°
https://wzzheng.net/S3Gaussian
1UCBerkeley 2PekingUniversity 3TsinghuaUniversity
wenzhao.zheng@outlook.com; shanghang@pku.edu.cn
Abstract
Photorealistic 3D reconstruction of street scenes is a critical technique for de-
velopingreal-worldsimulatorsforautonomousdriving. Despitetheefficacyof
NeuralRadianceFields(NeRF)fordrivingscenes,3DGaussianSplatting(3DGS)
emergesasapromisingdirectionduetoitsfasterspeedandmoreexplicitrepresen-
tation. However,mostexistingstreet3DGSmethodsrequiretracked3Dvehicle
boundingboxestodecomposethestaticanddynamicelementsforeffectiverecon-
struction,limitingtheirapplicationsforin-the-wildscenarios. Tofacilitateefficient
3Dscenereconstructionwithoutcostlyannotations,weproposeaself-supervised
streetGaussian(S3Gaussian)methodtodecomposedynamicandstaticelements
from4Dconsistency. Werepresenteachscenewith3DGaussianstopreservethe
explicitness and further accompany them with a spatial-temporal field network
tocompactlymodelthe4Ddynamics. Weconductextensiveexperimentsonthe
challengingWaymo-Opendatasettoevaluatetheeffectivenessofourmethod. Our
S3Gaussiandemonstratestheabilitytodecomposestaticanddynamicscenesand
achievesthebestperformancewithoutusing3Dannotations. Codeisavailableat:
https://github.com/nnanhuang/S3Gaussian/.
1 Introduction
Autonomousdrivinghasmadesignificantprogressinrecentyearsanddevelopedvarioustechniques
in each stage of its pipeline including perception [29, 67, 23, 56], prediction [18, 16, 31], and
planning[11,9,10]. Withtheemergenceofend-to-endautonomousdrivingwhichdirectlyoutputs
the control signal from sensor inputs [19, 20, 24], open-loop evaluation of autonomous driving
systems ceases to be effective and thus requires pressing improvement [65, 30]. As a promising
solution, real-world closed-loop evaluation requires sensor inputs for controllable views, which
motivatesthedevelopmentofhigh-qualityscenereconstructionmethods[53,59].
Despitenumerouseffortsonphoto-realisticreconstructiononsmall-scalescenes[35,36,7,25,55],
thelarge-scaleandhighlydynamiccharacteristicsofdrivingscenariosposenewchallengestothe
effective modeling of 3D scenes. To accommodate these, most existing works adopt tracked 3D
boundingboxestodecomposestaticanddynamicelements[60,58,53]. Still,thecostlyannotations
of 3D tracklets limit their applications for 3D modeling from in-the-wild data. EmerNerf [61]
addressedthisbysimultaneouslylearningthesceneflowandusingittoconnectcorrespondingpoints
in the 4D NeRF field for multi-frame reconstruction, enabling the emergence of decomposition
betweenstaticanddynamicobjectswithoutexplicitboundingboxes. However,3Ddrivingscene
modelinghasbeenundergoingashiftfromNeRF-basedreconstructionto3DGaussianSplattingdue
*WorkdoneduringaninternshipatUCBerkeley.‚Ä†Projectleader.‚Ä°Correspondingauthor.
Preprint.
4202
yaM
03
]VC.sc[
1v32302.5042:viXraGT RGB
3DGS
MARS
EmerNeRF
Ours
Front Left Front Front Right Front Left Front Front Right
Figure1: QualitativecomparisonoverWaymo-NOTRDatasets. Ontheleft,weshowcaseresults
fromnovelviewsynthesis;ontheright,resultsfromdynamicscenereconstructionaredisplayed.
Withtheproposedspatial-temporalnetworkfortheself-supervisedscenedecomposition,ourmethod
S3Gaussianproducesthebestrenderingqualitywithhighfidelityandsharpdetails.
toitsdesireforlowlatencyandexplicitrepresentation. ThoughEmerNerfdemonstratedpromising
results,itcanonlybeusedforNeRF-basedscenemodeling,whichtakesalongtimefortrainingand
rendering. Itisstillunclearhowtoachieve3DGaussianSplattingforurbanscenereconstruction
withoutexplicit3Dsupervision.
Toaddresstheaboveissues,weproposeaSelf-SupervisedStreetGaussiansnamedS3Gaussian,
offeringarobustsolutionfordynamicstreetsceneswithoutrequiring3Dsupervision. Specifically,to
handlethecomplexspatial-temporaldeformationsinherentindrivingscenes,S3Gaussianintroduces
a cutting-edge spatial-temporal field for scene decomposition in a self-supervised manner. This
spatial-temporalfieldincorporatesamulti-resolutionHexplanestructureencoderalongsideacompact
multi-headGaussiandecoder. TheHexplaneencoderisdesignedtodecomposethe4Dinputgridinto
multi-resolution,learnablefeatureplanes,efficientlyaggregatingtemporalandspatialinformation
from the dynamic street scenes. During the optimization process, the multi-resolution Hexplane
structureencodereffectivelyseparatestheentirescene,achievingacanonicalrepresentationforeach
scene. Dynamic-relatedfeaturesarestoredwithinthespatial-temporalplane,whilestatic-related
featuresareretainedinthespatial-onlyplane. Leveragingthedenselyencodedfeatures,themulti-
headGaussiandecoderscalculatethedeformationoffsetsfromthecanonicalrepresentations. These
deformationsarethenaddedtotheoriginal3DGaussians‚Äôattributes,includingpositionandspherical
harmonics,allowingforadynamicalterationofthescenerepresentationconditionedontimeseries.
Ourmaincontributionsaresummarizedasfollows:
‚Ä¢ WeproposeS3Gaussian,thefirstself-supervisedmethodthatmanagestodecomposethe
dynamicandstatic3DGaussiansinstreetsceneswithoutextramanuallyannotateddata.
‚Ä¢ Tomodelthecomplexchangesindrivingscenes,weintroduceanefficientspatial-temporal
decompositionnetworktoautomaticallycapturethedeformationof3DGaussians.
‚Ä¢ We conduct comprehensive experiments on challenging datasets, including NOTR and
Waymo. ResultsdemonstratethatS3Gaussianachievesstate-of-the-artrenderingqualityon
scenereconstructionandnovelviewsynthesistasks.
2 RelatedWork
3DGaussianSplatting. Recentbreakthroughsin3DGaussianSplatting(3DGS)[26]haverevo-
lutionizedscenemodelingandrendering. Harnessingthepowerofexplicit3DGaussians,3DGS
achievesoptimaloutcomesinnovelviewsynthesisandreal-timerenderingwhilealsosubstantially
reducingparametercomplexitycomparedtoconventionalrepresentationssuchasmeshesorvoxels.
2Thistechniqueseamlesslyintegratestheprinciplesofpoint-basedrendering[1]andsplatting[70],
facilitatingrapidrenderinganddifferentiablecomputationthroughsplat-basedrasterization.
Whiletheoriginal3DGSmodelisdesignedforstaticscenerepresentation,severalresearchershave
extendeditsapplicabilitytodynamicobjectsandscenes. Forinstance,Yangetal.[63]introduces
adeformationnetworkaimedatcapturingGaussianmotionfromaseriesofdynamicmonocular
images. Anotherapproach,detailedby[57],establishesconnectionsbetweenneighboringGaussians
using a HexPlane, thereby enabling real-time rendering. By optimizing point clouds containing
semanticlogitsand3DGaussiansfornoveldynamicscenerepresentation,Yanetal.[60]achieves
improvementsintrainingandrenderingspeed. SimilarlytoNeRF‚Äôsmethodology,Zhouetal.[68]
differentiatesstaticbackgroundsanddynamicobjectswithinthesceneandreconstructseachusing
distinctGaussianSplattingmethods. However,existingapproachesareconstrainedastheycanmodel
onlystaticordynamicscenesindividuallyandrequiresupervisedclassificationofscenetypes. Our
objectiveistoautonomouslylearnthedecompositionofstaticanddynamicscenesinaself-supervised
manner,therebyeliminatingtherelianceonrealannotations,suchasdynamicobjectboundingboxes.
StreetSceneReconstructionforAutonomousDrivingSimulation. Numerouseffortshavebeen
put into reconstructing scenes from autonomous driving data captured in real scenes. Existing
self-drivingsimulationenginessuchasCARLA[12]orAirSim[45]sufferfromcostlymanualeffort
tocreatevirtualenvironmentsandthelackofrealisminthegenerateddata. Therapiddevelopment
of Novel View Synthesis (NVS) techniques, including NeRF [35] and 3DGS [26], has attracted
considerableattentionwithinthearenaofautonomousdriving.Manystudies[8,17,33,34,40,43,42,
49,51,53,52,58,60,62,68]haveinvestigatedtheapplicationofthesemethodsforreconstructing
street scenes. Block-NeRF [49] and Mega-NeRF [52] propose segmenting scenes into distinct
blocksforindividualmodeling. UrbanRadianceField[42]enhancesNeRFtrainingwithgeometric
informationfrom LiDAR, whileDNMP [34]utilizes apre-traineddeformable meshprimitiveto
representthescene. Streetsurf[17]dividesscenesintoclose-range,distant-view,andskycategories,
yielding superior reconstruction results for urban street surfaces. For modeling dynamic urban
scenes,NSG[39]representsscenesasneuralgraphs,andMARS[58]employsseparatenetworksfor
modelingbackgroundandvehicles,establishinganinstance-awaresimulationframework. Withthe
introductionof3DGS[26],DrivingGaussian[68]introducesCompositeDynamicGaussianGraphs
andincrementalstaticGaussians,whileStreetGaussian[60]optimizesthetrackedposeofdynamic
Gaussiansandintroduces4Dsphericalharmonicsforvaryingvehicleappearancesacrossframes.
Theaforementionedmethodsnotonlysufferfromprolongedtrainingdurationsandsluggishrendering
speedsbutalsofailtoqualifytheabilitytodividedynamicandstaticscenesautomatically. Therefore,
we propose S3Gaussian to differentiate between dynamic and static scenes in a self-supervised
mannerwithouttheneedforadditionalannotations,andperformhigh-fidelityandreal-timeneural
renderingofdynamicurbanstreetscenes,whichiscrucialforautonomousdrivingsimulation.
3 ProposedApproach
Weaimtolearnaspatial-temporalrepresentationofthedynamicenvironmentofthestreetfroma
sequenceofimagescapturedbymovingvehicles. However,duetothelimitednumberofobservation
viewsandthehighcostofobtaininggroundtruthannotationsfordynamicandstaticobjects,weaim
tolearnthescenedecompositionofbothstaticanddynamiccomponentsinafullyself-supervised
manner,avoidingthesupervisionofextraannotationsincludingboundingboxesfordynamicobjects,
segmentationmasksforthescenedecomposition,andopticalflowforthemotionperception.
To achieve these objectives, we propose a novel scene representation named S3Gaussian. First,
in Sec. 3.1, we lift 3D Gaussians to 4D to better represent dynamic and complex scenes. Then,
in Sec. 3.2, we introduce a novel Spatial-temporal Field Network to integrate high-dimensional
spatial-temporalinformationanddecodethemtotransform4DGaussians. Finally,inSec.3.3,we
describetheentireoptimizationprocess,eliminatingextraannotations.
3.1 4DGaussianRepresentations
AsdepictedinFigure2,ourscenerepresentationsinclude3DGaussians[26]GandaSpatial-temporal
FieldNetworkF. Todepictstaticscenes,3DGaussiansarecharacterizedbyacovariancematrix
Œ£andapositionvectorX,referredtoasthegeometricattributes. Forastableoptimization,each
3Space-only Plane Space-time Plane Bilinear Multi-scale feature
Interpolation
ùìì ‚àÜùëÜùêª
ùë∫ùëØ
‚àÜùë•,
ùë∑ ùíöùíõ ùë∑ ùíôùíö ùë∑ ùíöùíï ùë∑ ùíôùíï ùìì ùíô ‚àÜ ‚àÜùë¶ ùëß,
ùë∑
ùë∑ ùíôùíõ ùíõùíï ùùì ùíé ùìì ùíî ùëì !
Dynamic Scene SurroundingNovel ViewSynthesis
Rendering Reconstruction
ùíô,ùíö,ùíõ,ùíï
Dynamic and StaticObject Decomposition
GaussianPrimitives Original Static Dynamic
Figure2: PipelineofS3Gaussian. Totacklethechallengesinself-supervisedstreetscenedecomposi-
tion,ourmethodconsistsofaMulti-resolutionHexplaneStructureEncodertoencode4Dgridinto
featureplanesandamulti-headGaussianDecodertodecodethemintodeformed4DGaussians. The
entirepipelineisoptimizedwithoutextraannotationsinaself-supervisedmanner,leadingtosuperior
scenedecompositionabilityandrenderingquality.
covariancematrixisfurtherfactorizedintoascalingmatrixS andarotationmatrixR:
Œ£=RSSTRT (1)
Inadditiontothepositionandcovariancematrices,eachGaussianisalsoassignedanopacityvalue
Œ±‚ààRandcolorC ‚ààR3(k+1)2,definedbysphericalharmonic(SH)coefficients,wherekrepresents
thedegreesofSHfunctions.
TheSpatial-temporalFieldNetworktakesthepositionofeachGaussianX andthecurrenttimestep
tasinput,producingspatial-temporalfeaturesf. Afterdecodingthesefeatures,thenetworkcan
predictthedisplacement‚ñ≥G ofeachpointrelativetocanonicalspacewhilealsoobtainingsemantic
informationf throughthesemanticfeaturedecoderD . WedetailitinSec.3.2.
s s
Following[64],weutilizeadifferentiable3DGaussiansplattingrendererRtoprojectthedeformed
3DGaussiansG‚Ä≤ =‚ñ≥G+G into2D[69]. Here,thecovariancematrixŒ£‚Ä≤incameracoordinatesis:
Œ£‚Ä≤ =JWŒ£WTJT (2)
whereJ istheJacobianmatrixoftheperspectiveprojection,andW istheviewingtransformmatrix.
ThecolorofeachpixeliscalculatedbyN orderedpointsusingŒ±-blending:
i‚àí1
(cid:88) (cid:89)
C = c Œ± (1‚àíŒ± ) (3)
i i i
i‚ààN j=1
Here,Œ± andc representtheopacityandcolorofonepoint,computedbyanoptimizableper-point
i i
opacityandSHcolorcoefficientswiththeviewdirection. Thesemanticmapcanberenderedsimply
bychangingthecolorcinEq.3tothesemanticfeaturef .
s
3.2 Spatial-temporalFieldNetwork
Theprimaryfocusofvanilla3DGaussiansSplattingisontasksinstaticscenes. However,thereal
worldisdynamic,especiallyincontextslikeautonomousdriving. Thismakesthetransitionfrom
3DGSto4Dacrucialandchallengingendeavor. Firstly,indynamicscenarios,theviewscapturedby
eachmovingcameraateachtimesteparesparserthaninstaticscenes,makingindividualmodeling
4ofeachtimestepexceptionallydifficultduetothissparsity. Therefore, itbecomesimperativeto
considerinformationsharingacrosstimesteps[14].
Moreover, modeling all Gaussian points in space and time is impractical for large-scale or long-
durationscenarioslikeautonomousdrivingduetosignificantmemoryoverhead. Hence,wepropose
leveraging an efficient Gaussian-based spatial-temporal network to model 3D Gaussian motion.
ThisnetworkcomprisesaMulti-resolutionHexplaneStructureEncoderandaminimalMulti-head
GaussianDecoder.Itonlyneedstomaintainasetofcanonical3DGaussiansandmodeladeformation
fieldforeachtimestep. Thisfieldpredictsdisplacementandcolorchangesrelativetothecanonical
space3DGaussians,thuscapturingGaussianmotion[57]. Additionally,weincorporateasimple
semanticfieldtoassistinautomaticallydecomposingstaticanddynamicGaussians.
Multi-resolution Hexplane Structure Encoder. To efficiently aggregate temporal and spatial
informationacrosstimesteps, consideringthatadjacentGaussiansoftensharesimilarspatialand
temporalcharacteristics,weemploytheMulti-resolutionHexplaneStructureEncoderE withatiny
MLPœï torepresentdynamic3Dsceneseffectivelyinspiredby[6,13,14,46]. Specifically,the
m
HexPlanedecomposesthe4Dspatial-temporalgridintosixmulti-resolutionlearnablefeatureplanes
spanningeachpairofcoordinateaxes,eachendowedwithanorthogonalaxis. Thefirstthreeplanes
P , P , P represent spatial-only dimensions, while the latter three P , P , P represent
xy xz yz xt yt zt
spatial-temporalvariations. Thisdecouplingoftimeandspaceisbeneficialforseparatingstaticand
dynamicelements. Dynamicobjectsbecomedistinctlyvisibleonthespatial-temporalplane,while
staticobjectssolelymanifestonthespatial-onlyplane.
Additionally,topromotespatialsmoothnessandcoherencewhilecompressingthemodelandreducing
thenumberoffeaturesstoredatthehighestresolution,inspiredbyInstant-NGP‚Äôsmulti-scalehash
encoding [? ], our hexplane encoder comprises multiple copies of different resolutions. This
representationeffectivelyencodesspatialfeaturesatvariousscales. Therefore,ourformulationis:
PœÅ ‚ààRd√óœÅri√óœÅrj,(i,j)‚àà{(x,y),(x,z),(y,z),(x,t),(y,t),(z,t)},œÅ‚àà{1,2} (4)
ij
wheredisthehiddendimensionoffeatures,œÅstandsfortheupsamplingscale,andrequalstothe
basicresolution. Givinga4Dcoordinate(x,y,z,t),wethenobtaintheneuralvoxelfeaturesand
mergeallthefeaturesusingatinyMLPœï asfollows:
m
(cid:91)(cid:89)
f(x,y,z,t)=œï ( œÄ(PœÅ,œàœÅ(x,y,z,t))) (5)
m ij ij
œÅ
whereœàœÅ projects4Dcoordinate(x,y,z,t)ontothecorrespondingplane,andœÄdenotesbilinear
ij
interpolation,usedforqueryingvoxelfeatureslocatedatthefourvertices. Wemergetheplanesusing
Hadamardproducttoproducespatiallylocalizedsignals,asdiscussedin[14].
Multi-headGaussianDecoder. WeuseseparateMLPheadsD = (D ,D ,D )todecodethe
SH x s
featuresobtainedinSec.3.2.Specifically,weemployasemanticfeaturedecodertocomputesemantic
featuresf =D (f(x,y,z,t)). Consideringthatmostautonomousdrivingscenariosinvolverigid
s s
motion,weonlyconsiderdeformationinthepositionoftheGaussians,thus‚ñ≥x=D (f(x,y,z,t)).
x
Additionally,consideringfactorslikeillumination,theappearanceofthescenevarieswithitsglobal
positionandtime. Therefore,wealsointroduceanSHcoefficientheadtomodelthe4Ddynamic
appearancemodel‚ñ≥SH =D (f(x,y,z,t)). Finally,ourdeformed4DGaussiansareformulated
SH
as: G‚Ä≤ ={X +‚ñ≥X,C+‚ñ≥C,s,r,œÉ,f }.
s
3.3 Self-supervisedOptimization
LiDARPriorInitialization. Toinitializethepositionsofthe3DGaussians,weleveragetheLiDAR
pointcloudcapturedbythevehicleinsteadofusingtheoriginalSFM[44]pointcloudtoprovide
abettergeometricstructure. Toreducemodelsize,wealsodownsampletheentirepointcloudby
voxelizingitandfilteringoutpointsoutsidetheimage. Forcolors,weinitializethemrandomly.
OptimizationObjective. Thelossfunctionofourmethodconsistsofsevenparts,andwejointly
optimizeourscenerepresentationandSpatial-temporalfieldusingit. L istheL1lossbetween
rgb
renderedandgroundtruthimagesandL measuresthesimilaritybetweenthem. L istheL2
ssim depth
lossbetweentheestimateddepthmapfromtheLiDARpointcloudandtherendereddepthmap,used
tosupervisetheexpectedpositionoftheGaussians[61,68]. Therendereddepthiscomputedusing
thepositionsoftheGaussians. L istheL2lossofsemanticfeature. Following[13,47,14],we
feat
5Final Warm-up Initial LiDAR Points Initial Warm-up Final
Figure3: Illustrationoftheoptimizationprocess. WiththeLiDARpointsinitializationandthestatic
3DGaussianWarm-upstrategy,ourmodelachieveshigh-quality4DGaussianrepresentationsofthe
complexdynamicscenes.
alsointroduceagrid-basedtotal-variationallossL . Giventhatmostelementsinthescenearestatic,
tv
weintroduceregularizationconstraintsintothespatial-temporalnetworktoenhancetheseparation
ofstaticanddynamiccomponents. WeachievethisbyminimizingtheexpectationofE(‚ñ≥X)and
E(‚ñ≥C),whichencouragesthenetworkonlytoproduceoffsetvalueswhennecessary. Then,thetotal
lossfunctioncanbeformulatedasfollows:
L=Œª L +Œª L +Œª L +Œª L +Œª L +Œªx Lx +Œªy Lc (6)
rgb rgb depth depth feat feat ssim ssim tv tv reg reg reg reg
where Œª = 1.0, Œª = 0.1, Œª = 0.1, Œª = 0.1, Œª = 0.1, Œªx = 0.01, and
rgb depth feat ssim tv reg
Œªy =0.01aretheweightsassignedtoeachlosscomponent.
reg
4 Experiments
Inthissection,weprimarilydiscusstheexperimentalmethodologyusedtoevaluatetheperformance
ofourS3Gaussian. Detailsofthedatasetsettings,baselinemethods,andimplementationspecifics
areprovidedinSec.4.1. InSec.4.2,wecompareourapproachwithstate-of-the-art(SOTA)methods
acrossvarioustasks. FurtherablationstudiesandanalysisaredetailedinSec.4.3.
4.1 ExperimentalSetup
Datasets.NOTRdatasetisasubsetoftheWaymoOpendataset[48]curatedby[61],whichcomprises
manychallengingdrivingscenarios: ego-static,high-speed,exposuremismatch,dusk/dawn,gloomy,
rainy, and night scenes. In contrast, many public datasets with LiDAR data suffer from a severe
imbalance, eg. nuScenes [4] and nuPlan [5], predominantly featuring simple scenes with few
dynamic objects. Therefore, we utilize NOTR‚Äôs dynamic32 (D32) and static32 (S32) datasets,
totaling64scenes,toobtainabalancedanddiversestandardforevaluatingourstaticanddynamic
reconstruction. Furthermore,sincemostbaselinemethodsareNeRF-based,toensureafairevaluation
ofourmethod‚Äôsperformance,weconductcomparisonswiththecurrentstate-of-the-artGaussian-
basedmethod,StreetGaussian[60]. WeadheretothedatasetconfigurationusedbyStreetGaussian,
employingthesixscenesselectedfromtheWaymoOpendataset[48],whicharecharacterizedby
complexenvironmentsandsignificantobjectmotion.
BaselineMethods.Weevaluateourapproachagainststate-of-the-artmethods,includingNeRF-based
modelsand3DGS-basedmodels. MARS[58]isamodular[50]simulatorbasedonNeRF,utilizing
2DboundingboxestotrainNeRFforstaticanddynamicobjectsrespectively. NSG[40]learnslatent
codestomodelmovingobjectswithashareddecoder. EmerNeRF[61]alsobuildsuponNeRFbut
self-supervisesthemodelingofdynamicscenesbyoptimizingflowfields,representingthecurrent
SOTAinself-supervisedlearningfordynamicdrivingscenerepresentations. The3DGS[26]model
employsanisotropic3DGaussianellipsoidsasanexplicit3Dscenerepresentation,achievingthe
strongestperformanceacrossvarioustasksinstaticscenes. StreetGaussian[60],thelatestGaussian-
basedmethod,introducestimeintoSHcoefficients,reachingSOTAperformanceaswell,albeitalso
utilizing2Dtrackedboxes. Forafaircomparison,wealsoapplyLiDARpointcloudinitializationto
3DGS,anddepthregularizationto3DGSandMARS,mirroringourapproach.
ImplementationDetails. Wetrainourmodelfor50,000iterationsusingtheAdamoptimizer[27],
followingthelearningrateconfigurationsof3DGaussians[26]. Additionally,weemploy5,000steps
ofpurestatic3DGaussiantraining[26]asawarm-upforthescene[57],asillustratedinFigure3.
Forthereconstructionoflongsequencescenes,wedividethesceneintomultipleclips. Specifically,
weuse50framesperclip, wheretheoptimizedSpatial-temporalfieldservesastheinitialization
fortheSpatial-temporalfieldofthenextsequencewith50steps. Thisapproachmaintainsspatial
and temporal consistency across sequences within the same scene. The basic resolution for our
6Table1: OverallperformanceofourmethodswithexistingSOTAapproachesontheWaymo-NOTR
dataset[61]. "PSNR*"and"SSIM*"denotethePSNRandSSIMofdynamicobjectsrespectively.
The best andthe secondbest resultsaredenotedbypinkandblue.
SceneReconstruction NovelViewSynthesis
Data Metrics
3DGS MARS EmerNeRF Ours 3DGS MARS EmerNeRF Ours
PSNR‚Üë 28.47 28.24 28.16 31.35 25.14 26.61 25.14 27.44
SSIM‚Üë 0.876 0.866 0.806 0.911 0.813 0.796 0.747 0.857
D32 LPIPS‚Üì 0.136 0.252 0.228 0.106 0.165 0.305 0.313 0.137
PSNR*‚Üë 23.26 23.37 24.32 26.02 20.48 22.21 23.49 22.92
SSIM*‚Üë 0.716 0.701 0.682 0.783 0.753 0.697 0.660 0.680
PSNR‚Üë 29.42 28.31 30.00 30.73 26.82 27.63 28.89 27.05
S32 SSIM‚Üë 0.891 0.879 0.834 0.883 0.836 0.848 0.814 0.825
LPIPS‚Üì 0.118 0.196 0.201 0.116 0.134 0.193 0.212 0.142
Table2: QuantitativeresultsonStreetGaussiandatasets[60]. Westrictlyfollowtheexperimental
settingofitandborrowresultsfromitsinceithasnotbeenopen-sourced.
Metrics 3DGS NSG MARS EmerNeRF StreetGaussian Ours
PSNR‚Üë 29.64 28.31 31.37 32.34 34.96 34.61
SSIM‚Üë 0.918 0.862 0.904 0.886 0.945 0.95z0
LPIPS‚Üì 0.117 0.346 0.246 0.142 0.068 0.050
PSNR*‚Üë 16.48 19.55 23.07 25.71 25.46 25.78
multi-resolutionHexPlaneencoderissetto64,thenupsampledby2and4as[57]. Thelearningrate
ofitissetas1.6√ó10‚àí3,decayedto1.6√ó10‚àí4attheendoftrainging.Eachdecoderinthemulti-head
decoderisasmallMLPwiththesamelearningrateastheHexPlaneencoder. Otherhyperparameters
arekeptconsistentwith3DGS[26]. IntheexperimentsconductedontheWaymo-NOTRdataset,we
strictlyadheredtotheexperimentalsettingsofEmerNeRF[61]. Similarly,fortheWaymo-Street
dataset,ourexperimentalsetupcloselyfollowedStreetGaussian[60].
4.2 ComparisonswiththeState-of-the-art
TheresultsontheWaymo-NOTRdatasetdemonstratethatourapproachconsistentlyoutperforms
othermethodsinscenereconstructionandnovelviewsynthesis,asshowninTable1. Forthestatic32
dataset,weutilizePSNR,SSIM,andLPIPS[66]asmetricstoevaluaterenderingquality. Forthe
dynamic32dataset,weadditionallyincludePSNR*andSSIM*metricsfocusingondynamicobjects.
Specifically,weprojectthe3Dboundingboxesofdynamicobjectsontothe2Dimageplaneand
calculatepixellossonlywithintheprojectedboxesas[61,60]. Ourmetricsoutperformthoseof
otherexistingmethods,indicatingthesuperiorperformanceofourapproachinmodelingdynamic
objects. Moreover,althoughstaticscenerepresentationisnotourprimaryfocus,ourmethodalso
performsexceptionallywellinthisaspect. Thus,ourapproachismoreversatileandgeneral.
We also conducted qualitative comparisons, as shown in Figure 1. We emphasized regions with
significantdifferencestoprovideaclearerdemonstration.Fromthefigure,itisevidentthatourmethod
surpassesthestate-of-the-art(SOTA)inboththesynthesisofnewviewpoints(leftsideofFigure1)
and reconstruction (right side of Figure 1) of static and dynamic scenes. Although 3DGS [26]
faithfullyreconstructsstaticobjects,itfailswhendealingwithdynamicobjectsandstruggleswith
reconstructingdistantskies. ThereconstructionqualityofMARS[58]ispoor,beingeffectiveonly
forveryshortsequences,anditstrugglestoreconstructfast-movingobjects. WhileEmerNeRF[61]
can self-supervise the reconstruction of static and dynamic objects, the reconstruction quality is
unsatisfactory, with issues such as ghosting, loss of plant texture details, missing lane markings,
andblurrydistantscenes. Fornovelviewsynthesis,ourmethodcangeneratehigh-qualityrendered
imagesandensureconsistencybetweenmultiplecameraviews. Indynamicscenereconstruction,we
accuratelysimulatedynamicobjectsinlarge-scalescenes,particularlydistantdynamicobjects,and
mitigateissuessuchasloss,ghosting,orblurrinessassociatedwiththesedynamicelements.
Table 2 presents the results on the dataset collected by StreetGaussian [60]. StreetGaussian is a
state-of-the-artmethodforGaussian-baseddynamicobjectrepresentation. Ourapproachperforms
7GT RGB Ours StreetGaussian EmerNeRF MARS 3DGS
Figure 4: Qualitative comparison over Waymo-Street Datasets [60]. All results are from novel
viewsynthesis. ComparedtoStreetGaussian[60], ourmethoddemonstratesastrongerabilityto
self-supervisedlyreconstructdistantdynamicobjectsandismoresensitivetochangesinscenedetails.
Table3: QuantitativeablationstudiesonWaymo-NOTRdynamic32datasets.
Task Metrics w/oPœÅ w/oD w/oD w/oD w/oWarm-up Ours
ij x SH s
PSNR‚Üë 18.702 29.861 31.458 31.605 31.390 32.135
Scene SSIM‚Üë 0.4793 0.8871 0.9157 0.9174 0.9173 0.9355
Reconstruct PSNR*‚Üë 16.800 24.626 26.420 26.556 26.628 27.046
SSIM*‚Üë 0.3627 0.7521 0.8162 0.8182 0.8213 0.8284
PSNR‚Üë 17.245 25.850 27.959 27.981 27.955 28.417
SSIM‚Üë 0.4499 0.8174 0.8616 0.8624 0.8641 0.8641
NVS
PSNR*‚Üë 15.613 21.385 21.385 23.402 23.681 23.974
SSIM*‚Üë 0.3118 0.6386 0.6386 0.7138 0.7117 0.7175
similarlytoStreetGaussian,butwiththedistinctionthatStreetGaussianusesadditionalbounding
boxestomodeldynamicobjects,whereasourapproachdoesnotrequireanyexplicitsupervision. As
showninFigure4,comparedtoStreetGaussian[60]whichusesexplicitsupervision,ourmethod
excelsinself-supervisedreconstructionofdistantdynamicobjects. Additionally,ourmethodismore
sensitivetochangesinscenedetails,suchasvariationsintrafficlights. Furthermore,StreetGaussian
exhibitsnoiseinthesky,resultinginadecreaseinrenderingquality.
4.3 AblationandAnalysis
Weinvestigatetheeffectivenessofourmethodanditsvariouscomponents. Duetotimeconstraints,
we select 20 sequences from NOTR dynamic32 [61] for analysis, and all models are trained for
a shorter duration of 30,000 iterations. Table 3 presents the quantitative results, while Figure 5
showcasesthevisualcomparisonresults.
Multi-resolutionHexplaneStructureEncoder. Comparedtopurelyexplicitmethods,theproposed
HexPlane encoder PœÅ allows for memory savings and enables retention of different dimensions
ij
ofspatial-temporalinformationinthescenethroughvariousresolutions. Discardingthismodule
and relying solely on a shallow MLP œï fails to accurately establish spatial-temporal fields and
m
cannotsimulateGaussiandeformations. BothTable3andFigure5demonstratethis,withoutthis
module,ourrenderingqualitysharplydeclines. Wealsoprovidevisualizationsofthefeaturesofthis
encoder,asshowninFigure6. Asanexplicitmodule,wecaneasilyoptimizeallGaussianfeatures
onasinglevoxelplane. FromFigure6,itisevidentthatthevoxelplanefeaturesmainlyconcentrate
onthemovingpartsofthescene. Thetrajectoriesofmovingvehiclesinthesceneextendfromthe
bottom-righttothetop-rightcorner. Asaresult,spatialplanefeaturesareprimarilyconcentratedin
thebottom-rightcorner,whereastemporalplanefeaturesarepredominantlyobservedontheright
side. Thesepatternsdemonstratethatourencodersuccessfullycapturesbothspatialandtemporal
information. Thiscapabilityallowsustoeffectivelyself-supervisethedecompositionofstaticand
dynamiccomponents,asillustratedinFigure6andFigure2.
Multi-head Gaussian Decoder. Our proposed multi-head Gaussian decoder can decode voxel
features. AsindicatedinTable3,disablingthiscomponentwouldimpactrenderingqualitygreatly.
Additionally,asshowninFigure5,disablingtheD decoderandonlytrainingGaussianincanonical
x
8Ours Complete Model W/O Semantic Decoder ùíü&‚Äô
W/O 3D-only Warm-up Strategy W/O SH Decoder ùíü%
W/O Hexplane Encoder ùí´ !#
"
W/O Position Decoder ùíü$
Figure5: VisualablationresultsontheWaymo-NOTRdynamic32dataset.
Rendered View 1 Rendered View 2
Temporal Grid Spatial-only Grid Static and Dynamic Object Decomposition
Figure6: VisualizationofHexPlanevoxelgrids,showcasingitscapabilitytodecomposestaticand
dynamicelements. Spatial-onlygridreferstothespatialvoxelparameters,whilethetemporalgrid
referstoitstimefeatures.
spacewouldintroducesignificantnoise. ThenoisestemsfromGaussianpointsinitializedbyLiDAR
pointclouds,resultinginaseriesofGaussianpointsalongamovingvehicle‚Äôstrajectory. Ifthese
pointsarenotdeformed,itbecomeschallengingtooptimizethemafterward. Furthermore,omitting
thesemanticfeaturedecoderD andcolordeformationdecoderD primarilyaffectsrendering
s SH
details. Forexample,thegeometricstructureofthetruckbecomesblurrierwithoutthesecomponents.
StaticGaussianWarm-up. AccordingtoFigure5,wefoundthatdirectlytrainingthe4DGaussians
withoutfirstoptimizing3DGaussiansforwarm-upnotonlyreducesconvergencespeedbutalso
affectsthefinalrenderingquality. Asshowninthe3,performingawarm-upstepalreadyyieldsbasic
staticscenereconstruction,whichalleviatesthepressureonthe4Dspatial-temporalnetworktolearn
large-scalescenesandallowsthenetworktofocusmoreondynamicparts. Additionally,itstabilizes
thenetworkbyavoidingearly-stagenumericalerrors[57].
5 Conclusion
Inthispaper,weproposeS3Gaussian,thefirstself-supervisedstreetGaussianmethodtodifferentiate
dynamicandstaticelementsincomplexdrivingscenes. S3GaussianemploysaSpatial-temporal
FieldNetworktoachievethescenedecompositionautomatically,whichconsistsofaMulti-resolution
HexplaneStructureEncoderandaMulti-headGaussianDecoder. Givena4Dgridinglobalspace,
theproposedHexplaneencoderaggregatesfeaturesintodynamicorstaticplanes. Thenwedecode
these features into the deformed 4D Gaussians. The entire pipeline is optimized without any
extraannotations. ExperimentsonchallengingdatasetsincludingNOTRandWaymoimprovethat
S3Gaussian show superior scene decomposition ability and obtain the state-of-the-art rendering
qualityacrossdifferenttasks. Abundantquantitativeresultsareimplementedtoshedlightonthe
effectivenessofeachcomponentinS3Gaussian.
9A Appendix
A.1 AdditionalImplementationDetails
Datasets Details. Our Waymo-NOTR dataset follows the setup of [61]. For camera images, we
utilizethreefrontalcameras: FRONTLEFT,FRONT,andFRONTRIGHT,adjustedtoaresolution
of640√ó960fortrainingandevaluation. Thelengthofallsequencesissetto100frames. Weselect
every10thframefromthesequencesasthetestframesandusetheremainingframesfortraining.
ForourWaymo-Streetdataset,consistentwith[60],weusefrontalcamerasanddownscaletheinput
imagesto1066√ó1600forevaluatingmonocularreconstructionandnovelviewsynthesiscapabilities.
ThelengthofallsequencesstrictlyfollowsthedatasetsettingreleasedbyStreetGaussian[60],with
eachsequenceapproximately100frameslong. Weselectevery4thframefromthesequencesasthe
testframesandusetheremainingframesfortraining.
FeatureExtraction. WeemploytheDINOv2[38]checkpointandthefeatureextractorimplementa-
tionby[2]. Specifically,weusetheViT-B/14variantandadjusttheimagedimensionsto644√ó966
withastrideof7. Giventhelargesizeofthefeaturemaps,following[61]weusePCAdecomposition
toreducethefeaturedimensionfrom768to3andnormalizethesefeaturestothe[0,1]range.
A.2 MoreRelatedWork(AdvancesinNeuralRadianceFields)
Inrecentyears,therehasbeenasurgeofinterestamongresearchersinleveragingneuralrendering
techniques for scene modeling. Among these techniques, Neural Radiance Fields (NeRF) have
garneredparticularattention. NeRF[35]utilizesdifferentiablevolumerenderingmethods,facilitating
the generation of novel scenes from a mere collection of planar images accompanied by their
respectivecameraposes. Moreover,NeRFdemonstratesthecapabilitytosegregatestreetviewsinto
staticanddynamicscenesbytrackingtheboundingboxesofvehicles. Despitetheextensiveresearch
effortsaimedatenhancingNeRF‚Äôsfunctionalities,whichhaveledtonotableadvancementsintraining
speed [14, 15, 37], pose optimization [3, 32, 54], scene editing [28, 43], object generation [21],
anddynamicscenerepresentation[22,41],challengespersist,particularlyregardingtrainingand
renderingspeed. ThesechallengesposesignificantobstaclestothewidespreadadoptionofNeRF
inautonomousdrivingscenarios. ComparedtoNeRF-basedmethods,S3Gaussiansproposed4D
Gaussianrepresentationsfordynamicscenes,significantlyboostingrenderingspeed.
A.3 Limitations
Similartoothermethods[61,58],oursceneencountersdifficultyinmodelingobjectsmovingathigh
speeds. Wesuspectthismaybeduetothedeformationfield‚Äôshighvariance, renderingitunable
tomodeltheirrapidmovementsaccurately. Moreover,viewsofrapidlymovingdynamicobjects
aretypicallysparse,withonlyafewviewsavailableforcapture,makingreconstructionevenmore
challenging. Howtoreconstructthesechallengingsceneswillbeafocusofourfutureresearch.
References
[1] Kara-AliAliev,DmitryUlyanov,andVictorS.Lempitsky. Neuralpoint-basedgraphics. ArXiv,
abs/1906.08240,2019.
[2] ShirAmir,YossiGandelsman,ShaiBagon,andTaliDekel. Deepvitfeaturesasdensevisual
descriptors. arXivpreprintarXiv:2112.05814,2(3):4,2021.
[3] WenjingBian,ZiruiWang,KejieLi,JiawangBian,andVictorAdrianPrisacariu. Nope-nerf:
Optimisingneuralradiancefieldwithnoposeprior. 2023IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages4160‚Äì4169,2022.
[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,
Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal
datasetforautonomousdriving.In2020IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2020.
[5] Holger Caesar, Juraj Kabzan, KokSeang Tan, FongWhye Kit, EricM. Wolff, AlexH. Lang,
LukeFletcher,OscarBeijbom,andSammyOmari. nuplan: Aclosed-loopml-basedplanning
10benchmarkforautonomousvehicles. arXiv: ComputerVisionandPatternRecognition,arXiv:
ComputerVisionandPatternRecognition,2021.
[6] AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InProceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages130‚Äì141,
2023.
[7] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf: Tensorialradiance
fields. InEuropeanConferenceonComputerVision,pages333‚Äì350.Springer,2022.
[8] YuruiChen,ChunGu,JunzheJiang,XiatianZhu,andLiZhang. Periodicvibrationgaussian:
Dynamicurbanscenereconstructionandreal-timerendering. ArXiv,abs/2311.18561,2023.
[9] JieCheng,YingbingChen,QingwenZhang,LuGan,ChengjuLiu,andMingLiu. Real-time
trajectoryplanningforautonomousdrivingwithgaussianprocessandincrementalrefinement.
InICRA,pages8999‚Äì9005,2022.
[10] Jie Cheng, Xiaodong Mei, and Ming Liu. Forecast-MAE: Self-supervised pre-training for
motionforecastingwithmaskedautoencoders. ICCV,2023.
[11] DanielDauner,MarcelHallgarten,AndreasGeiger,andKashyapChitta. Partingwithmiscon-
ceptionsaboutlearning-basedvehiclemotionplanning. InCoRL,2023.
[12] AlexeyDosovitskiy,Germ√°nRos,FelipeCodevilla,AntonioM.L√≥pez,andVladlenKoltun.
Carla: Anopenurbandrivingsimulator. InConferenceonRobotLearning,2017.
[13] JieminFang,TaoranYi,XinggangWang,LingxiXie,XiaopengZhang,WenyuLiu,Matthias
Nie√üner, andQiTian. Fastdynamicradiancefieldswithtime-awareneuralvoxels. InSIG-
GRAPHAsia2022ConferencePapers,pages1‚Äì9,2022.
[14] SaraFridovich-Keil,GiacomoMeanti,FrederikRahb√¶kWarburg,BenjaminRecht,andAngjoo
Kanazawa. K-planes: Explicitradiancefieldsinspace,time,andappearance. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages12479‚Äì12488,
2023.
[15] StephanJ.Garbin,MarekKowalski,MatthewJohnson,JamieShotton,andJulienP.C.Valentin.
Fastnerf: High-fidelityneuralrenderingat200fps. 2021IEEE/CVFInternationalConference
onComputerVision(ICCV),pages14326‚Äì14335,2021.
[16] JunruGu,ChenxuHu,TianyuanZhang,XuanyaoChen,YilunWang,YueWang,andHang
Zhao. Vip3d: End-to-end visual trajectory prediction via 3d agent queries. arXiv preprint
arXiv:2208.01582,2022.
[17] JianfeiGuo,NianchenDeng,XinyangLi,YeqiBai,BotianShi,ChiyuWang,ChenjingDing,
DongliangWang,andYikangLi. Streetsurf: Extendingmulti-viewimplicitsurfacereconstruc-
tiontostreetviews. ArXiv,abs/2306.04988,2023.
[18] AnthonyHu,ZakMurez,NikhilMohan,Sof√≠aDudas,JeffreyHawke,VijayBadrinarayanan,
RobertoCipolla,andAlexKendall. Fiery: Futureinstancepredictioninbird‚Äôs-eyeviewfrom
surroundmonocularcameras. InICCV,2021.
[19] ShengchaoHu,LiChen,PenghaoWu,HongyangLi,JunchiYan,andDachengTao. St-p3:
End-to-endvision-basedautonomousdrivingviaspatial-temporalfeaturelearning. InECCV,
2022.
[20] YihanHu,JiazhiYang,LiChen,KeyuLi,ChonghaoSima,XizhouZhu,SiqiChai,Senyao
Du,TianweiLin,WenhaiWang,etal. Planning-orientedautonomousdriving. InCVPR,pages
17853‚Äì17862,2023.
[21] NanHuang,TingZhang,YuhuiYuan,DongChen,andShanghangZhang. Customize-it-3d:
High-quality3dcreationfromasingleimageusingsubject-specificknowledgeprior,2024.
[22] XinHuang,QiZhang,FengYing,HongdongLi,XuanWang,andQingWang. Hdr-nerf: High
dynamicrangeneuralradiancefields. 2022IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages18377‚Äì18387,2021.
11[23] YuanhuiHuang,WenzhaoZheng,YunpengZhang,JieZhou,andJiwenLu. Tri-perspective
viewforvision-based3dsemanticoccupancyprediction. InCVPR,pages9223‚Äì9232,2023.
[24] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang,
Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for
efficientautonomousdriving. arXivpreprintarXiv:2303.12077,2023.
[25] BernhardKerbl,GeorgiosKopanas,ThomasLeimk√ºhler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4):1‚Äì14,
2023.
[26] BernhardKerbl,GeorgiosKopanas,ThomasLeimk√ºhler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),2023.
[27] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization,2017.
[28] YuanLi,ZhiLin,DavidW.Forsyth,Jia-BinHuang,andShenlongWang. Climatenerf: Extreme
weather synthesis in neural radiance field. 2023 IEEE/CVF International Conference on
ComputerVision(ICCV),pages3204‚Äì3215,2022.
[29] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and
JifengDai. Bevformer: Learningbird‚Äôs-eye-viewrepresentationfrommulti-cameraimagesvia
spatiotemporaltransformers. InECCV,2022.
[30] ZhiqiLi,ZhidingYu,ShiyiLan,JiahanLi,JanKautz,TongLu,andJoseMAlvarez. Isego
statusallyouneedforopen-loopend-to-endautonomousdriving? InCVPR,2024.
[31] MingLiang,BinYang,WenyuanZeng,YunChen,RuiHu,SergioCasas,andRaquelUrtasun.
Pnpnet: End-to-endperceptionandpredictionwithtrackingintheloop. InCVPR,2020.
[32] Chen-HsuanLin,Wei-ChiuMa,AntonioTorralba,andSimonLucey. Barf: Bundle-adjusting
neuralradiancefields. 2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),
pages5721‚Äì5731,2021.
[33] JeffreyYunfanLiu,YunChen,ZeYang,JingkangWang,SivabalanManivasagam,andRaquel
Urtasun. Real-timeneuralrasterizationforlargescenes. 2023IEEE/CVFInternationalConfer-
enceonComputerVision(ICCV),pages8382‚Äì8393,2023.
[34] Fan Lu, Yan Xu, Guang-Sheng Chen, Hongsheng Li, Kwan-Yee Lin, and Changjun Jiang.
Urbanradiancefieldrepresentationwithdeformableneuralmeshprimitives. 2023IEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages465‚Äì476,2023.
[35] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoor-
thi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis.
CommunicationsoftheACM,65(1):99‚Äì106,2021.
[36] ThomasM√ºller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMtransactionsongraphics(TOG),41(4):
1‚Äì15,2022.
[37] ThomasM√ºller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. ACMTransactionsonGraphics,page1‚Äì15,
2022.
[38] MaximeOquab,Timoth√©eDarcet,Th√©oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[39] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,andFelixHeide. Neuralscenegraphs
fordynamicscenes. 2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages2855‚Äì2864,2020.
[40] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,andFelixHeide.Neuralscenegraphsfor
dynamicscenes. In2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),2021.
12[41] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf:
Neuralradiancefieldsfordynamicscenes. 2021IEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR),pages10313‚Äì10322,2020.
[42] KonstantinosRematas,AnLiu,PratulP.Srinivasan,JonathanT.Barron,AndreaTagliasacchi,
ThomasA.Funkhouser,andVittorioFerrari.Urbanradiancefields.2022IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),pages12922‚Äì12932,2021.
[43] ViktorRudnev,MohamedA.Elgharib,WilliamH.B.Smith,LingjieLiu,VladislavGolyanik,
andChristianTheobalt.Nerfforoutdoorscenerelighting.InEuropeanConferenceonComputer
Vision,2021.
[44] JohannesL.SchonbergerandJan-MichaelFrahm. Structure-from-motionrevisited. In2016
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2016.
[45] S.Shah,DebadeeptaDey,ChrisLovett,andAshishKapoor. Airsim: High-fidelityvisualand
physicalsimulationforautonomousvehicles. InInternationalSymposiumonFieldandService
Robotics,2017.
[46] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu.
Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and
rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages16632‚Äì16642,2023.
[47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast
convergenceforradiancefieldsreconstruction. In2022IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),2022.
[48] PeiSun,HenrikKretzschmar,XerxesDotiwalla,AurelienChouard,VijaysaiPatnaik,PaulTsui,
JamesGuo,YinZhou,YuningChai,BenjaminCaine,VijayVasudevan,WeiHan,JiquanNgiam,
HangZhao,AlekseiTimofeev,ScottEttinger,MaximKrivokon,AmyGao,AdityaJoshi,Sheng
Zhao,ShuyangCheng,YuZhang,JonathonShlens,ZhifengChen,andDragomirAnguelov.
Scalabilityinperceptionforautonomousdriving: Waymoopendataset,2020.
[49] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P.
Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene
neuralviewsynthesis.2022IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages8238‚Äì8248,2022.
[50] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexan-
derKristoffersen, JakeAustin, KamyarSalahi, AbhikAhuja, DavidMcallister, JustinKerr,
andAngjooKanazawa. Nerfstudio: Amodularframeworkforneuralradiancefielddevelop-
ment. InSpecialInterestGrouponComputerGraphicsandInteractiveTechniquesConference
ConferenceProceedings.ACM,2023.
[51] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson,
and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. ArXiv,
abs/2311.15260,2023.
[52] HaithemTurki,DevaRamanan,andMahadevSatyanarayanan. Mega-nerf: Scalableconstruc-
tionoflarge-scalenerfsforvirtualfly-throughs. 2022IEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages12912‚Äì12921,2021.
[53] HaithemTurki,JasonYZhang,FrancescoFerroni,andDevaRamanan. Suds: Scalableurban
dynamicscenes. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages12375‚Äì12385,2023.
[54] ZiruiWang,ShangzheWu,WeidiXie,MinChen,andVictorAdrianPrisacariu. Nerf-: Neural
radiancefieldswithoutknowncameraparameters. ArXiv,abs/2102.07064,2021.
[55] XiaobaoWei,RenruiZhang,JiaruiWu,JiamingLiu,MingLu,YandongGuo,andShanghang
Zhang. Noc: High-qualityneuralobjectcloningwith3dliftingofsegmentanything. arXiv
preprintarXiv:2309.12790,2023.
13[56] YiWei,LinqingZhao,WenzhaoZheng,ZhengZhu,JieZhou,andJiwenLu. Surroundocc:
Multi-camera3doccupancypredictionforautonomousdriving. InICCV,pages21729‚Äì21740,
2023.
[57] GuanjunWu,TaoranYi,JieminFang,LingxiXie,XiaopengZhang,WeiWei,WenyuLiu,Qi
Tian,andXinggangWang. 4dgaussiansplattingforreal-timedynamicscenerendering. ArXiv,
abs/2310.08528,2023.
[58] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou,
Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang
Shi,YiyiLiao,andHaoZhao. Mars: Aninstance-aware,modularandrealisticsimulatorfor
autonomousdriving. CICAI,2023.
[59] ZiyangXie,JungeZhang,WenyeLi,FeihuZhang,andLiZhang. S-nerf: Neuralradiancefields
forstreetviews. arXivpreprintarXiv:2303.00749,2023.
[60] YunzhiYan,HaotongLin,ChenxuZhou,WeijieWang,HaiyangSun,KunZhan,Xianpeng
Lang, Xiaowei Zhou, and Sida Peng. Street gaussians for modeling dynamic urban scenes.
ArXiv,abs/2401.01339,2024.
[61] JiaweiYang,BorisIvanovic,OrLitany,XinshuoWeng,SeungWookKim,BoyiLi,TongChe,
DanfeiXu,SanjaFidler,MarcoPavone,andYueWang. Emernerf: Emergentspatial-temporal
scenedecompositionviaself-supervision,2023.
[62] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce
Yang,andRaquelUrtasun. Unisim: Aneuralclosed-loopsensorsimulator. 2023IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages1389‚Äì1399,2023.
[63] Ziyi Yang, Xinyu Gao, Wenming Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin.
Deformable3dgaussiansforhigh-fidelitymonoculardynamicscenereconstruction. ArXiv,
abs/2309.13101,2023.
[64] WangYifan,FeliceSerena,ShihaoWu,Cengiz√ñztireli,andOlgaSorkine-Hornung. Differen-
tiablesurfacesplattingforpoint-basedgeometryprocessing. ACMTransactionsonGraphics,
38(6):1‚Äì14,2019.
[65] Jiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu
Zhang,XiaoqingYe,andJingdongWang. Rethinkingtheopen-loopevaluationofend-to-end
autonomousdrivinginnuscenes. arXivpreprintarXiv:2305.10430,2023.
[66] RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,andOliverWang. Theunreason-
ableeffectivenessofdeepfeaturesasaperceptualmetric. In2018IEEE/CVFConferenceon
ComputerVisionandPatternRecognition,2018.
[67] YunpengZhang,ZhengZhu,WenzhaoZheng,JunjieHuang,GuanHuang,JieZhou,andJiwen
Lu. Beverse:Unifiedperceptionandpredictioninbirds-eye-viewforvision-centricautonomous
driving. arXivpreprintarXiv:2205.09743,2022.
[68] XiaoyuZhou,ZhiweiLin,XiaojunShan,YongtaoWang,DeqingSun,andMing-HsuanYang.
Drivinggaussian: Compositegaussiansplattingforsurroundingdynamicautonomousdriving
scenes. ArXiv,abs/2312.07920,2023.
[69] MatthiasZwicker,HanspeterPfister,JeroenVanBaar,andMarkusGross. Surfacesplatting. In
Proceedingsofthe28thannualconferenceonComputergraphicsandinteractivetechniques,
pages371‚Äì378,2001.
[70] MatthiasZwicker,HanspeterPfister,JeroenvanBaar,andMarkusH.Gross. Ewasplatting.
IEEETrans.Vis.Comput.Graph.,8:223‚Äì238,2002.
14