SEA: Supervised Embedding Alignment for
Token-Level Visual-Textual Integration in MLLMs
YuanyangYin1âˆ—,YaqiZhao2âˆ—,YajieZhang3,KeLin3,JiahaoWang3
XinTao3,PengfeiWan3,DiZhang3,BaoqunYin1â€ ,WentaoZhang2â€ 
1UniversityofScienceandTechnologyofChina
2PekingUniversity
3KuaishouTechnology
Abstract
Multimodal Large Language Models (MLLMs) have recently demonstrated re-
markableperceptualandreasoningabilities,typicallycomprisingaVisionEncoder,
anAdapter,andaLargeLanguageModel(LLM).Theadapterservesasthecritical
bridgebetweenthevisualandlanguagecomponents. However,trainingadapters
withimage-levelsupervisionoftenresultsinsignificantmisalignment,undermining
theLLMsâ€™capabilitiesandlimitingthepotentialofMultimodalLLMs. Toaddress
this,weintroduceSupervisedEmbeddingAlignment(SEA),atoken-levelalign-
mentmethodthatleveragesvision-languagepre-trainedmodels,suchasCLIP,to
alignvisualtokenswiththeLLMâ€™sembeddingspacethroughcontrastivelearning.
Thisapproachensuresamorecoherentintegrationofvisualandlanguagerepre-
sentations,enhancingtheperformanceandinterpretabilityofmultimodalLLMs
whilepreservingtheirinherentcapabilities. ExtensiveexperimentsshowthatSEA
effectivelyimprovesMLLMs,particularlyforsmallermodels,withoutaddingextra
dataorinferencecomputation. SEAalsolaysthegroundworkfordevelopingmore
generalandadaptablesolutionstoenhancemultimodalsystems.
1 Introduction
MultimodalLargeLanguageModels(MLLMs)rapidlydevelopedinrecentyears,showingimpressive
abilitiesinmultimodalperceptionandreasoning[2,5,33]. Thesemodelsbridgethegapbetween
differentmodalities,enablingmorecomprehensiveandcontext-awareinteractions. Theevolutionof
MLLMsrepresentasignificantadvancementinartificialgeneralintelligence(AGI),asitmimicshow
humansinteractwiththeworldandcomprehendcomplexinformationthroughdifferentsenses.
Traditional training paradigms in MLLMs typically involve two key phases: pre-training and
instruction-tuning[14,22,33,34,52]. Duringthepre-trainingphase,theadapterissolelytrainedto
enhanceitsabilitytotransformvisualrepresentationsintotext,therebyunderstandingtherelevance
ofimagecontentsandtheircorrespondingtextualdescriptions,andfacilitatingeffectivecross-modal
alignment. Theinstruction-tuningphasefurtherenhancesthemodelâ€™sadaptabilitytospecifictasks.
However, this pre-training paradigms face inherent limitations due to the mismatch between the
frozenlanguagemodel,trainedprimarilyontexttokens,andthevisualfeatures,whichoftenlack
directtextualequivalents. Moreover,thetrainingapproachprimarilyinvolvessimpleandimplicit
supervision,wherelossesarecomputedonlyforpredictinglanguageresponses. Inthissetup,visual
informationactsmerelyascontextualcueswithoutservingasdirectsupervisionforalignmentwith
âˆ—EqualContribution.
â€ CorrespondingAuthor.
Preprint.Underreview.
4202
guA
12
]VC.sc[
1v31811.8042:viXraLLaVA 1.5 bestow sunshine oriole SEA-LLaVA fruit sun bravely
heartthrobheartthrobc snap esa sciou- artfulnesse as llo yteric-sunflower heart heart m ista icsoch- fruit orange sun
d sle yxterou-c snap esa sciou-c snap esa sciou- heftiness m giy zetholo-sunshade fruit red blood topical g frr ua ip te- sun
u lin ne ea ssrth- f nr eu si stful- oriole fruit f nr eu si stful- heart
embezzlebluejeans c ioo nm ap tea ls ys- sunup oriole blueberrybluejeans sun sun bravely
c snap esa sciou-m sice ata llp yhy- e cs ao llt yeri- e cs ao llt yeri- u chn id ee vr ea- blueberry fruit orange orange sunshine
(a)LLaVA-1.5aligmentresult. (b)SEA-LLaVAaligmentresult.
Figure 1: Comparison of Alignment Results between LLaVA-1.5 and SEA-LLaVA. After 2
stages of training, we evaluate the alignment of visual tokens with the LLMâ€™s embedding space
bymeasuringthesimilaritybetweenvisualtokensandthewordsencodedbytheLLM.Theword
withthehighestsemanticrelevancetoeachvisualtokenisselectedasitsrepresentation. Correct
alignmentsaremarkedingreen,whileincorrectonesareinred. SEA-LLaVA,whichisanimproved
versionofLLaVA-1.5withoutintroducingnewdata,showssignificantlybetteralignment.
textualrepresentations. Inthispaper,wewillshowthatthisparadigmcannoteffectivelyguidethe
adapter in achieving accurate alignment between visual and textual features, a phenomenon we
refertoasmisalignment(SeeFigure1),leadingtoinconsistenciesinthemodelâ€™sunderstandingand
generationcapabilities(SeeFigure2). Thisissueisexacerbatedduringinstruction-tuning,wherethe
entireLLMmustbefine-tuned. Poorinitializationfromthepre-trainingphaseresultsinsuboptimal
adapterweights,makingthefine-tuningprocessmorechallengingandpotentiallydegradingboththe
modelâ€™slanguagecapabilitiesandoverallmultimodalperformance.
Several recent works have sought to improve alignment in MLLMs. For example, [39] uses an
optimaltransport-basedmethodtoalignpre-trainedvisionmodelembeddingswiththeLLMâ€™sspace,
thoughitislimitedbyitsrelianceonimage-captionpairsandisspecifictoencoder-decodermodels.
Similarly, [42]mapsvisualfeaturestoLLMvocabularythroughregression,butitstruggleswith
precisetoken-levelalignment. Additionally, [47]enhancesalignmentbyannotatingimageswith
alphanumeric tags, improving object-text alignment but requiring extensive data annotation and
addingcomplexity. Thesemethodsstillfacechallengeswithmisalignment,leadingtoinformation
lossandhinderingtheLLMâ€™sunderstandingofvisualcontent.
Inthiswork,weaddressacriticalissue: HowtoresolveinsufficientalignmentinMLLMs? Totackle
this,weproposeanoveltoken-levelsupervisedalignmentparadigmcalledSupervisedEmbedding
Alignment(SEA).SEAusesexplicitsupervisiontopreciselyalignvisualtokenswiththeLLMâ€™s
embeddingspace. Unliketraditionalimage-levelmethods,ourapproachensuresthateachvisual
tokenispreciselymatchedwithitscorrespondingsemanticrepresentationwithintheLLM,following
the trend of treating visual and textual tokens as equivalent inputs. Specifically, SEA improves
alignmentinmainstreamMLLMsthroughtwokeyaspects.
Token-LevelLabelingforFine-GrainedAlignment. Duringthepre-trainingstage,acaptioning
tasktrainsanadaptertoalignvisualpatcheswiththeLLMâ€™sembeddingspace,enablingvisualtokens
tobetreatedliketexttokens. However,currentimage-levelalignmentmethodsarecoarseandlack
fine-grainedguidance,leadingtosuboptimalresults.Therearetwomainchallengesimpedeachieving
token-levelfine-grainedalignment. First,sinceavisualtokenâ€™ssemanticrepresentationiscontinuous,
asinglevisualtokenoftenencapsulatesmultiplemeanings. Nocurrentmodelscangeneratemultiple
semantic labels for each visual token, making it impossible to provide fine-grained alignment
guidance. Second,visualencodersoftenproducesemanticshifts,whereatokenâ€™srepresentation
reflectssurroundingimageregions, complicatingtheaccuratemappingofvisualpatchestotheir
corresponding semantic labels. Existing methods highlight the suitability of vision encoders for
MLLMsbutoftenoverlookthevalueoftextencoders(pairedwithvisionencodersduringtraining).
Sincevisualrepresentationsfromthevisionencoderalignnaturallywithtextrepresentations,we
use the text encoder to infer semantic labels for each visual token. This approach addresses the
continuoussemantictoken-levellabelingchallengeandmitigatessemanticshifts,enablingeffective
token-levelalignment.
2ANovelSupervisedAlignmentParadigm. Mainstreamapproachesthatapplyimage-levelsupervi-
sionattheLLMâ€™soutputsdivergefromthegoaloftreatingvisualandtextualtokensequally,making
itdifficulttobridgethegapbetweenthem. Toaddressthis,weproposeanovelalignmentsupervision
paradigmthatdirectlyalignsvisualtokenswithtextualtokens,enablingtheLLMtoprocessvisual
tokensinamannersimilartotextualtokens. Specifically,byleveragingsemanticlabelsforeach
visualtokenandemployingcontrastivelearning,weensurethatvisualtokenrepresentationsclosely
matchtheircorrespondingsemanticlabelsintheLLMâ€™sembeddingspace. Bycombiningcontrastive
learninglosswithLLMpredictionlosstoupdatetheadapter,weeffectivelyenhanceitsalignment
capability,bridgingthegapbetweenthevisionencoderandtheLLM.
LeveragingourproposedSEA,wehavesignificantlyenhancedtheperformanceoftheLLaVA-1.5
across8benchmarks,withoutneedingextraannotations,data,orinferencecosts. Thismethodoffers
a universal, cost-effective training strategy for vision encoders trained on vision-language tasks,
showcasinghighreturnsandexceptionalinnovation.
2 RelatedWork
Vision-LanguagePre-Training. Theintegrationofcomputervisionandnaturallanguageprocess-
inghasresultedinVisionLanguageModels(VLMs). Thesemodelscombinevisualandlinguistic
components,typicallyleveragingimage-textpairs,toenrichvisualinformationwithsemanticcontent
and facilitate cross-modal understanding. Contrastive learning has become particularly influen-
tialintherealmofVision-Languagepre-trainingsincetheintroductionofmodelslikeCLIP[40],
ALIGN[21],SPARC[8]. Thesemethodsutilizesoftmaxcontrastivelearningonlarge-scaleimage-
textdatasets. Unlikeabovemethodsthatrequireaglobalviewofpairwisesimilaritiesfornormaliza-
tion,SigLIP[49]proposesasimpleapproachwithapairwiseSigmoidlossthatoperatessolelyon
image-textpairs. Thesemodelshavedemonstratedexceptionalzero-shottransfercapabilities,leading
toenhancedperformanceintasksthatrequirecomprehensiveunderstandingofbothmodalities.
Cross-Modal Alignment in MLLMs. The combination of image and text representations is
essentialforMLLMs,whichcanbecategorizedintotwomaintypesbasedontheirfusionapproach:
deepfusionandshallowfusion. Indeepfusion[4,6,26,46],imagefeaturesencodedbythevision
encoderareintegratedintothelanguagemodellayersviainteractionmodules,allowingtexttoattend
toimagefeatures. Conversely,shallowfusion[15,24,27,34,52]concatenatethevisionencoderâ€™s
outputdirectlywithtextembeddingsbeforepassingthemtothelanguagemodel. Thesemethods
show that vision models and LLMs, though independently trained, share significant information.
However, there are concerns that shallow fusion approaches fail to fully bridge the gap between
visualandlanguagemodelrepresentations,limitingtheLLMâ€™scomprehensionofvisualtokens. To
addressthis, [50]proposesassigningaspecialtokentodifferentimage-textpairsbasedonsimilarity
scores. Thisasymmetricallearningapproachenhancestheadapterâ€™salignmentcapability,improving
thelanguagemodelâ€™sunderstandingofvisualtokens. [32]enhancesvisualtokensbyreintroducing
informationextractedfromsegmentationandOCRmodelsbackintothevisualtokens. Whilethese
approacheshaveachievedsignificantbreakthroughs,theyfallshortoffundamentallyenhancingthe
adapterâ€™salignmentcapability. ThispaperintroducesSEA(SupervisedEmbeddingAlignment)asa
moreeffectivesolutiontoimprovethisalignment.
3 Preliminaries
Inthissection,wefirstintroducethetypicaladaptermodulethatbridgesthevisionencoderandLLM
inMLLMs. Followingthis,wehighlighttheissueofmisalignmentanditsimpacts,formingthebasis
forourmethodpresentedinSection4.
AdapterinMultimodalLLMs. MultimodalLargeLanguageModelsaimtointegratevisualand
textualinformationseamlessly. Acommonapproachinvolvesusinganadapter,typicallypositioned
betweenthevisionencoderandalargelanguagemodel. Theadaptercanbeasimplelinearlayer[34]
or a more complex multi-layer perceptron [33]. For clarity, we define the visual features output
bythevisionencoderasvisualpatchesandthefeaturesafterpassingthroughtheadapterasvisual
tokens. Duringpre-training,thismoduletransformsvisualpatchesintorepresentationscompatible
3Instructionï¼š SEA-LLaVA
Visual token Recalled Word: red
What color are the sockets in
the picture?
Assistant:
The sockets in the picture are red.
LLaVA 1.5
Visual token Recalled Word: referendum
Assistant:
The sockets in the picture are white.
Figure2: ImpactofVisualTokenAlignmentonModelPerformance. The"recalledword"refers
tothewordwiththehighestsimilarityscoreforagivenvisualtoken. Inthisexample,weaskeda
questionaboutthevisualelementsinthedashedgreenbox. LLaVA-1.5failedtoanswercorrectlydue
tomisalignmentofthevisualtoken,selecting"referendum"insteadofarelevantcolor. Incontrast,
SEA-LLaVA correctly aligned the visual token to the LLMâ€™s embedding space, resulting in the
correctresponse"red."
withtheLLMâ€™stextualrepresentations. Thegoalistorefinethesevisualtokenstocloselymatch
theirsemanticinformationintheLLMâ€™sembeddingspace.
Duringthepre-training,foragivenimage-textpair(X ,X ). TheLLMinputisconstructedas:
image text
X =g (f(X )) (1)
v Î¸ image
X =Î¨(X ) (2)
t text
X =[x ,...,x ,x ,...,x ], x âˆˆX x âˆˆX (3)
input v0 vm t0 tn vj v ti t
wheref representsforvisionencoder,grepresentsfortheadapter,andÎ¨isLLMâ€™sembeddinglayer.
Subsequently,X ispassedtotheLLMforgeneratingtheresponse,andtheparameterÎ¸oftheg
input
isupdatedbycalculatingtheauto-regressiveloss.
MisalignmentIssue. Despitethepre-trainingeffortstoalignvisualtokenswithtexttokensinthe
LLMâ€™sembeddingspace,currentmodelsexhibitsignificantmisalignmentissues. Theadapter,which
isimplicitlysupervisedthroughauto-regressiveloss,failstoensureaccuratesemanticrepresentations
ofeachvisualpatchduringpre-training. Asaresult,anotablediscrepancyarisesbetweenvisualand
textualtokensintheLLMâ€™sembeddingspaceafter2stagesoftrainingasshownin Figure1.
ImpactsofMisalignment. Themisalignmentafterpre-trainingleadstosignificantinformation
loss,profoundlyaffectingtheMLLMsâ€™comprehensionandperformance. Whenthelanguagemodel
cannotcomprehendthemisalignedtokens,itleadstoincorrectresponsesandseverehallucinations,
as depicted in Figure 2. Furthermore, if the language model is unfrozen during fine-tuning, the
divergencebetweenvisualandtextualrepresentationsisexacerbated, increasingthedifficultyof
trainingandseverelydamagingtheLLMâ€™slanguagecapabilities.
Toinvestigatetheseconcerns,weremovedthepre-trainingstageanddirectlyfine-tunedthemodel.
This approach led to a significant performance drop across multiple benchmarks, including pure
languagetasks(SeeFigure3).The"Vicuna-7B"baselinescored47.1,whilethe"LLaVAw/opretrain"
model,directlyfine-tunedwithoutpre-training,droppedto38.9,showinga17%performancedecrease
duetotheinterferencefromunalignedvisualtokens,leadingtocatastrophicforgetting. Whenpre-
trainingisincluded,theadapteralignsthevisualtokenswiththeLLMâ€™sembeddingspace,preventing
significantdegradationduringfine-tuning. Despitethealignmenteffectachievedduringpre-training,
the language modelâ€™s capabilities are still substantially impaired, indicating that cross-modality
alignment remains a challenging task. Notably, our proposed SEA scored 46.4, demonstrating
4(a)MMLUScoreoverFinetuningStage. (b)ComparisonofDifferentMethods.
Figure3: (a)Theimpactofremovingthepre-trainingstageonMMLUscoresduringthefine-tuning
phase. Theperformanceexhibitsasignificantdeclinewhenpre-trainingisomitted,highlightingits
crucialrole.Incontrast,theSEA-LLaVAmethod(redline)maintainsalmostthesameinitiallanguage
capabilities throughout the fine-tuning process, demonstrating its robustness. (b) A comparative
analysisofdifferentmethodsacrossvariousbenchmarksillustratestheeffectivenessoftheSEA-
LLaVAmethodinmaintainingsuperiorperformance.
thatourmethodeffectivelypreserveslanguagecapabilitiesduringfine-tuningandmaintainsrobust
performanceacrossmultimodaltasks.
We propose a novel token-level supervision alignment paradigm during pre-training to address
themisalignmentofvisualandtexttokensintheLLMâ€™sembeddingspace. Thisaimstomitigate
informationlossandimprovetheoverallrepresentationofbothmodalities.
4 SupervisedEmbeddingAlignment: Method
This section presents SEA, the first supervision paradigm to mitigate the issue of misalignment
betweenvisualandtexttokensinLLMâ€™sembeddingspaceduringpre-training(SeeFigure4). SEA
leverages the rich visual features extracted from vision-language pre-trained models, which are
alreadywell-alignedwiththetextencoderâ€™sspace. Asshowninthe Figure5,foreachpatchinthe
image,SEAfirstutilizesthevision-languagepre-trainedmodeltoselectwordsthatcanmaximizethe
cosinesimilaritybetweenvisualandtextfeaturesfromapre-definedlist,whichservesasthelabelfor
eachpatchintheimage. Subsequently,itguidesthealignmentbetweenvisualtokensandtexttokens
intheLLMâ€™sembeddingspacethroughcontrastivelearningduringpre-training. Wewillintroduce
eachstepofSEAindetailbelow.
4.1 ExtractSemanticLabelsforVisualPatches
Toachievefine-grainedsupervisionofthesemanticfeatureexpressionofeachvisualtokentrans-
formedbytheadapter,weneedtoobtainthecontinuoussemanticlabelsofeachpatchafterthevision
encoder. Foravision-languagepre-trainedvisionencoderf pairedwithacorrespondingtextencoder
h and a word list W, we extract the semantic information for each patch using Eqs. (4), (5), (6).
Wethenselectthetopnwordsbasedontheircosinesimilarityscoresforeachvisualpatch(See
Figure5(3)). Toensureonlythemostrelevantandpositivelycorrelatedwordsareconsidered,we
excludeanylabelswithsimilarityscoreslessthan0. Theremainingwordsarereferredtoasthe
semanticlabelsforeachvisualpatch. Thisapproachassignsmultiplesemanticlabelstoeachtoken,
preservingthecontinuoussemanticrepresentationofthealignedvisualtoken. Pairedtrainingofthe
visionandtextencodersalsoeffectivelypreventssemanticshift.
V =f(X )âˆˆRmÃ—d (4)
image
T =h(W)âˆˆRqÃ—d (5)
w ,s =argmax{âˆ’cos(v ,t )} (6)
i i i j
j
5The image is a delightful depiction of a kitten
and a puppy in a moment of mutual interest,
set against a backdrop of greenery that
enhances the serene and wholesome nature of
their interaction.
ğŸ§Š Large Language Model
ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ ğ‘“ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿğµğ‘™ğ‘œğ‘ğ‘˜Ã—ğ‘
ğŸ”¥MLP Adapter ğ‘” ğ‘¥ !! ğ‘¥ !" ğ‘¥ !# ğ‘¥ !$
ğŸ§Š Word embedding ğœ“
ğ‘¡
"
ğŸ§Š Vision Encoder ğ‘“
ğ‘¡ ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ ğ‘‹ "&â€™()*+(",&
#
Semantic labels Instruction
ğ‘¡
$
wolfhoundã€pupâ€¦â€¦
grassã€greenâ€¦â€¦ Describe the
ğ‘¡ % kittenã€catâ€¦â€¦ image in detail.
dewclaw ã€clawâ€¦â€¦
Figure4: OverviewoftheproposedSEA.Duringpre-training,LLMâ€™sembeddinglayerencodes
bothinstructionsandsemanticlabelssimultaneously. Theseencodedsemanticlabelsdirectlyguide
thealignmentofvisualtokenstotheLLMâ€™sembeddingspacethroughcontrastivelearning.Thevisual
tokensarethenconcatenatedwiththeinstructionsandinputintotheLLMtogeneratearesponse.
wherew ands aretheindicesandscoresofthetopnsemanticlabelsforthei-thvisualpatchv
i i i
respectively. v isthevisualfeatureofthepatchobtainedfromthevisionencoderf,andt isthe
i j
textembeddingofthej-thwordinthewordlistW,obtainedfromthetextencoderh. Thenegative
cosinesimilarityâˆ’cos(v ,t )iscomputedasdescribedinpreviousworks[30],wherethecosine
i j
similarityneedstobenegatedintheCLIPembeddingspace.
4.2 Token-LevelAlignment
TheuseofanadapteraimstoconvertvisualpatchesintoLLMâ€™sembeddingspace. However,the
currentimage-levelapproach fallsshortofachievingthisadequately asshown inFigure1a. We
suggestusingthesemanticlabelsofeachpatchtodirectlyguidetheadapterintransformingvisual
patchesintotheLLMâ€™sembeddingspace,therebyreducingmisalignment(See Figure1b).
Similarity-WeightedSamplingforContinuousSemanticRepresentation. Duetothesemantic
continuityofvisualtokens,weshouldidentifyanappropriatepositionforeachvisualtokenwithin
theLLMâ€™sembeddingspace,ensuringitretainsitscontinuoussemanticrepresentation. Specifically,
foragivenvisualpatchv withitscorrespondingsemanticlabelsL =[w ,...,w ]andsimilarity
i i 1 n
scoresS =[s ,...,s ],wefirstnormalizethesimilarityscorestogetthesamplingprobability,and
i 1 n
thensamplealabelforeachpatchbasedonSi inEq.(7).
norm
S
Si = i (7)
norm sum(S )
i
ALocalizedSamplingStrategy. Tofurtherenhancetheeffectivenessofcontrastivelearningand
mitigatetheissueofexcessivesimilaritybetweensamples,weadoptalocalizedsamplingstrategy.
Foreachimage,weperformsamplingwithinakÃ—kwindow,ensuringthatonlyonepatchissampled
fromeachwindow. Consequently,asingleimagewithN visualpatcheswillhaveN/(kÃ—k)patches
6ğŸ§Š Vision ğŸ”¥MLP â€¦ â€¦ ğŸ§Š LLM ğŸ§Š Vision ğŸ”¥MLP â€¦ ğŸ”¥ LLM
Encoder Adapter Encoder Adapter
(1) Stage 1: Pre-TrainingwithSEA (2) Stage 2: Instruction-Tuning
Text
Encoder
Ranked by cosine similarity Sample â€¦ â€¦
Vision
Encoder
Visual Patches with Semantic Label
(3)Semantic Labels Extraction in SEA
Figure 5: Overview of the SEA training pipeline. (1) Pre-Training with SEA. This stage uses
theSEAtoenhancemodalityalignmentbetweenvisionandlanguagemodels. Onlytheadapteris
optimized. (2)Instruction-Tuning. BoththeLLMandtheadapterarefine-tuned,ensuringimproved
multimodalperformance. (3)IllustrationofSemanticLabelExtractionforSEA.Duringpre-training,
semanticlabelsforvisualpatchesareextractedfromawordlistusingvision-languagepre-trained
models. Circularshapesrepresentcandidatelabels,andirregularhexagonsrepresentvisualpatches.
Theselabelsarerankedbycosinesimilarity,andthensampledaccordingtotheirsimilarity-weighted
rankingtoprovideexplicitsupervisionandenhancemodalityalignment.
participating in contrastive learning. For visual patches sharing the same label in one batch, we
randomlyretainonlyonepatchtoensuretheeffectivenessofcontrastivelearning. Wethenobtaina
seriesofvisualpatcheswithlabels,namely,{(x ,w ),...,(x ,w )},whereN isthenumberof
v1 1 vN N
tokensinonebatch.
Foreachlabelw ,wecomputethecorrespondingtextfeaturet asfollows:
i i
M
1 (cid:88)
t = Î¨(wk) (8)
i M i
k=1
whereÎ¨(wk)representstheencodedfeatureofthek-thtokenofw ,andM isthenumberoftokens
i i
afterencodingw .
i
Thelossofalignmentcanbecomputedas:
N (cid:32) (cid:33)
L =âˆ’
1 (cid:88)
log
exp(Ï•(x vi,t i)/Ï„)
+log
exp(Ï•(t i,x vi)/Ï„)
(9)
a 2N (cid:80)N exp(Ï•(x ,t )/Ï„) (cid:80)N exp(Ï•(t ,x )/Ï„)
i=1 j=1 vi j j=1 i vj
whereÏ•(x vi,t j)= âˆ¥xx vv ii
âˆ¥2
Â· âˆ¥tt jj âˆ¥2,andÏ„ isthetemperature,alearnableparameter.
Forgeneration,thepredictionofthenexttokenx(i)isconductedbasedonvisualtokensV ,prompt
i
P andprevioustokensx(<i). Thelosscanbecomputedas:
B
1 (cid:88) (cid:16) (cid:17)
L =âˆ’ logp x(i) |V,P,x(<i) (10)
g B Î¸ i
i=1
whereBisthebatchsize,Î¸isthetrainableparameters.
Duringthepre-trainingprocess,twolearningobjectivessimultaneouslysupervisetheadapter. We
obtainthefinallossLofpre-trainingbyaddingL andL ,aweightingfactorÎ»isintroducedto
a g
7balancethetwolosses.
L=L +Î»L (11)
g a
5 Experiments
Inthissection,wefirstprovideourexperimentalframeworkandevaluationresultson8common
benchmarkscomparedwithdifferentbackbonesin Table1. Subsequently,weempiricallyinvestigate
theeffectivenessandrobustnessofSEA.
Table1: Mainevaluationresultscomparedwithleadingbaselineson8popularbenchmarks.
VQAv2[17];VQAT:TextVQA[44];GQA[20];SQAI:ScienceQA-IMG[36];MMB:MMBench[35];
POPE[28];VizWiz[18];MM-Vet[48]. SEA-PRIMEoutperformsotheropen-sourcemodelsand
achieves competitive performance. All methods maintain the number of visual tokens without
doubling,andmodelsmarkedwith*areresultswereproduced. ColumnRes. istheimageresolution
ofvisionmodel.
Method LLM Res. VQAv2 VQAT GQA SQAI MMB POPE VizWiz MM-Vet
MobileVLM-3B[12] MLLaMA2.7B 336 â€“ 47.5 59.0 61.0 59.6 84.9 â€“ â€“
MobileVLM-V2-3B[13] MLLaMA2.7B 336 â€“ 57.5 61.1 70.0 63.2 84.7 â€“ â€“
LLaVA-Phi[53] Phi-2.7B 336 71.4 48.6 â€“ 68.4 59.8 85.0 35.9 28.7
TinyLLaVA[51] Phi-2.7B 384 79.9 59.1 62.0 69.1 66.9 86.4 â€“ 32.0
InstructBLIP[14] Vicuna-7B 224 â€“ 50.1 â€“ â€“ 30.6 â€“ 34.5 â€“
InstructBLIP[14] Vicuna-13B 224 â€“ 50.7 49.5 63.1 â€“ â€“ 33.4 â€“
Qwen-VL[7] Qwen-7B 448 79.5 63.8 59.3 67.1 38.2 â€“ 35.2 â€“
Qwen-VL-Chat[7] Qwen-7B 448 78.2 61.5 57.5 68.2 60.6 â€“ 38.9 â€“
LLaMA-VID[29] Vicuna-7B 336 79.3 â€“ 64.3 68.3 65.1 86.0 54.2 â€“
LLaMA-VID[29] Vicuna-13B 336 80.0 â€“ 65.0 70.0 66.6 86.0 54.3 â€“
LLaVA-1.5âˆ—[33] Vicuna-7B 336 78.8 58.3 62.0 67.9 66.2 86.5 45.7 30.7
LLaVA-1.5âˆ—[33] Vicuna-13B 336 80.0 60.8 63.3 71.6 67.7 87.6 53.6 35.1
ShareGPT4V[9] Vicuna-7B 336 80.6 â€“ â€“ 68.4 68.8 â€“ â€“ 37.6
Mini-Gemini[31] Gemma-2B 336+768 â€“ 56.2 â€“ â€“ 59.8 â€“ â€“ 31.1
Mini-Gemini[31] Vicuna-7B 336+768 â€“ 65.2 â€“ â€“ 69.3 â€“ â€“ 40.8
Mini-Gemini[31] Vicuna-13B 336+768 â€“ 65.9 â€“ â€“ 68.5 â€“ â€“ 46.0
S2âˆ’Wrapperâˆ—[43] Vicuna-7B 1008 79.7 60.3 63.2 â€“ 67.3 87.4 50.1 33.0
S2âˆ’Wrapper[43] Vicuna-13B 1008 80.9 63.1 â€“ â€“ 67.9 â€“ 56.0 35.4
AlignGPT[50] Vicuna-7B 336 79.1 58.4 62.9 68.5 67.3 86.0 54.2 30.8
AlignGPT[50] Vicuna-13B 336 80.0 60.2 63.6 70.3 69.5 86.2 56.4 35.6
VisualPrompt[32] Vicuna-7B 336 79.8 59.8 63.3 69.5 67.6 88.9 â€“ 34.9
OurModels
SEA-PRIME Gemma-2B 384 81.0 60.7 62.4 69.2 68.8 87.8 61.9 38.0
SEA-PRIME Phi3-3.8B 384 80.7 64.0 62.0 78.7 72.6 87.0 61.9 46.8
SEA-PRIME Vicuna-7B 384 81.4 67.2 63.1 73.9 75.6 88.4 63.8 44.2
SEA-PRIME Llama3-8B 384 83.1 68.0 65.1 79.0 76.0 87.4 64.7 46.0
SEA-PRIME Vicuna-13B 384 81.9 66.2 64.3 80.9 76.9 86.7 63.6 48.8
5.1 ExperimentalSetup
Architecture. WeconductedextensiveexperimentstoevaluatethegeneralizationofSEAacross
differentcomponentsoftheMLLMs. 1)VisionEncoders. WeexploredtheperformanceofSEA
withthreewidely-usedvisionencoders: CLIP-ViT-L@336px[40], SigLIP-ViT-SO@384px[49],
andS2-Wrapper@1008px[43]. 2)LLMs. TotestthescalabilityofSEA,weappliedittovarious
LLMs with parameters ranging from 2B to 13B. This included Gemma-2B [16], Phi-3-mini-4k-
instruct[1],Llama3-8B-Instruct[3]andVicuna-1.5-7B&13B[11]. 3)SEAImplementation. For
SEA,weselectedthetop10labelsbasedonsimilarity(i.e.,n=10),andsetthetemperatureÏ„ =0to
ensurerobustalignment. Meanwhile,weperformsamplingwitha2âˆ—2window. Thissetupallowed
ustocomprehensivelyevaluatetheeffectivenessofSEAinenhancingcross-modalalignment.
TrainingDetails. Weperformatwo-stagetrainingprocess. Inthefirststage,onlytheadapterwas
optimizedwhilethevisionencoderwasfixed.Inthesecondstage,boththeLLMandtheadapterwere
optimized. However,forSEA-PRIME,thevisionencoderwasalsotunedduringthesecondstage
withalearningrateof2e-6. Regardingthetrainingscheme,weoptimizeallthemodelsfor1epoch
withtheAdamWoptimizerandacosinelearningschedulefollowingLLaVAâ€™shyperparameters[33].
Thetrainingtimeformodelsin Table2rangesfrom6to10hoursusing8Ã—H800GPUs, which
isnearlyidenticaltoLLaVAâ€™strainingduration, withStage1requiringonlyanadditional10-20
minutes. ForSEA-PRIME,thetrainingtimeislessthan4dayswiththesameGPUconfiguration.
8Datasets. Forourmodelsin Table1,weusetheCambrian-1[45]trainingdata,whichconsistsof
2.5McaptionpairsformodalityalignmentandCambrian-7Mdataforinstructiontuning. Allablation
experimentsin Table2utilizethesamedataasLLaVA-1.5,specificallytheCC-595Kdataset[34]for
pre-traininganda656Kmixturedataset[33],whichincludesLLaVA-Instruct[34],TextVQA[44],
GQA[20],OCR-VQA[37],andVisualGenome[25]forinstruction-tuning.
(a)Similarityscoreofeachword. (b)Distributionofsimilarityscores.
Figure6: CosineSimilarityAnalysisofWordList. Basedoncosinesimilarityscores,weanalyzed
thesemanticlabelsforeachvisualpatchandtheusagepatternsofeverywordintheentirevocabulary.
In Figure6a,weobtainedsemanticlabelsforallvisualpatchesusingthemethodbelow,compiled
thosewithsimilarityscoresgreaterthan0,andplottedtheirdistribution. In Figure6b,wedepicted
thedistributionofaveragesimilarityscoresforeachwordacrosstheentirevocabulary.
WordList. Weusedthe2of12wordlistfromthe12dictionarycorpus,containing41,242words.
The LLaVA-Pretrain dataset was processed following the pipeline shown in Figure 5, assigning
relevant semantic labels to each visual patch. After defining the semantic labels as described in
Section4,wesetthesimilarityscoresofallotherwordsinthewordlistto0. Finally,weanalyzethe
distributionofthesimilarityscoresofthewordlist(SeeFigure6). Whilemanycandidatelabelsshow
lowcosinesimilaritiestotheircorrespondingimagepatches,anotablepeakinthedistributionaround
ahighersimilarityvalueof0.3suggestsaclusterofcandidatelabelsstronglyalignedsemantically
withtheimagepatches. Ithighlightsthesetâ€™srichsemanticcontentandabilitytocapturediverseand
meaningfulrepresentationsofvisualinformation. AsshowninFigure6b,thesimilarityscoresof
semanticlabelsclusteraround0.08,withabout32.7%oflabelshavingcosinesimilaritieslessthan0.
Notably,weachieveautilizationrateof61.2%forthe2of12wordlist,suggestingthatthe2of12list
offersamplevocabularytosupportoursemanticlabels.
5.2 ExperimentalResults
WeleveragetheSEAmethodtotrainafamilyofMLLMscalledSEA-PRIME.Ourmodelsutilize
LLMbackbonesofvariousscales. ThevisioncomponentemploysSigLIP-ViT-SO400M/14@384.
Wepre-traintheconnectorusing2.5MadapterdataandinstructiontuneusingCambrian-7Mdata.
Inourevaluationsonbenchmarks,SEA-PRIMEmodelsdemonstratesignificantimprovementsover
existingopen-sourcemethodsacrossvarioussettings,asshownin Table1. Specifically,ourSEA-
PRIMEmodelsexhibitsuperiorperformancewiththe2Band3.8Bparametermodelcomparedto
theefficientMobileVLM,achievingcompetitiveresultsdespiteitssmallerscale. Thescalabilityof
SEA-PRIMEisevidentwhenlargerLLMsareemployed. Notably,withtheLLaMA-3-Instruct-8B,
SEA-PRIMEachievesexceptionalresults,showcasingtheeffectivenessofourapproach.
Asshownin Figure1a,LLaVAâ€™sperformanceisunderwhelmingduetoitsunsupervisedpre-training,
which turns visual tokens into â€˜additional vocabularyâ€™. We introduced SEA during pre-training,
allowingtheadaptertobettertransformvisualrepresentations(SeeFigure1b)andprovidingbetter
initializationweightsforinstructiontuning. Tovalidatethelatter,weevaluatedthelanguagemodel
9Table2: ExploringtheCompatibilityandScalabilityofSEA.ScalingresultsonLLM,vision
encoder (VE) and resolution (Res.) are provided. "0.5M+0.6M" denotes the training data from
LLaVA-1.5[33]. ResultswithSEAaremarkedinâ– .
Method VE Res. PT+IT LLM VQAv2 VQAT GQA SQAI MMB POPE VizWiz MM-Vet
LLaVA[33] CLIP-L 336 0.5M+0.6M Vicuna-7B 78.8 58.3 62.0 67.9 66.2 86.5 45.7 30.7
SEA-LLaVA CLIP-L 336 0.5M+0.6M Vicuna-7B 79.1 58.9 63.2 69.4 66.8 87.6 48.8 31.9
ApplyingtoDifferentLLMs
LLaVA[33] CLIP-L 336 0.5M+0.6M Gemma-2B 72.5 43.7 56.0 61.3 54.0 84.4 38.7 23.9
+SEA CLIP-L 336 0.5M+0.6M Gemma-2B 76.6 49.7 60.9 62.5 59.5 87.0 39.5 27.6
LLaVA[33] CLIP-L 336 0.5M+0.6M Phi3-3.8B 77.4 54.6 60.8 73.0 68.7 86.5 37.1 35.4
+SEA CLIP-L 336 0.5M+0.6M Phi3-3.8B 77.5 55.3 61.0 74.2 69.4 87.0 39.0 34.7
LLaVA[33] CLIP-L 336 0.5M+0.6M LlaMA3-8B 79.4 57.7 63.7 76.0 72.5 87.0 48.1 34.0
+SEA CLIP-L 336 0.5M+0.6M LlaMA3-8B 79.6 58.0 63.8 76.6 72.0 87.0 45.2 36.3
LLaVA[33] CLIP-L 336 0.5M+0.6M Vicuna-13B 80.0 60.8 63.3 71.6 67.7 87.6 53.6 35.1
+SEA CLIP-L 336 0.5M+0.6M Vicuna-13B 79.8 60.4 63.8 71.7 68.0 87.6 57.3 35.8
Scalingtohigherresolution
LLaVA[33] SigLIP-SO 384 0.5M+0.6M Vicuna-7B 80.8 62.3 63.2 70.6 68.0 86.7 51.1 32.9
+SEA SigLIP-SO 384 0.5M+0.6M Vicuna-7B 80.9 62.6 63.4 71.3 68.4 87.3 52.4 34.6
S2âˆ’Wrapper[43] CLIP-L 1008 0.5M+0.6M Vicuna-7B 79.7 60.3 63.2 68.3 67.3 87.4 50.7 33.0
+SEA CLIP-L 1008 0.5M+0.6M Vicuna-7B 79.8 60.6 63.7 69.0 66.4 87.5 48.5 33.2
Table3: ExploringAblationforFine-tuningVisionEncoder. ThebaselineisLLaVA-1.5[33]with
Vicuna-7B,usingthesametrainingdataandstrategy. "FinetuneVE"referstothemethodwherethe
visionencoderisunfrozenduringinstructiontuning.
Method VQAv2 VQAT GQA SQA MMB VizWiz
Baseline 78.8 58.3 62.0 67.9 66.2 45.7
+FinetuneVE 80.3 +1.5 59.1 +0.8 63.4 +1.4 67.0 -0.9 66.1 -0.1 50.3 +4.6
+SEA 80.5 +0.2 59.5 +0.4 63.6 +0.2 69.5 +2.5 68.0 +1.9 51.6 +1.3
usingtheMMLUbenchmarkafterfine-tuning. Asshownin Figure3,SEAeffectivelypreserves
LLMâ€™slanguagecapabilities.
5.3 Ablationstudy
WeconductedanextensiveablationstudytoevaluatetheeffectivenessofourproposedSupervised
EmbeddingAlignment(SEA).Table2showcasestheevaluationresultsacross8popularbenchmarks.
TheseresultsdemonstratetheefficacyofourproposedSEAtechniqueinenhancingthealignment
betweenvisualrepresentationsandtheLLMâ€™sembeddingspaceandsignificantlyoutperforming
LLaVA-1.5,makingitahighlyadaptablesolutionfordiversemultimodalintegrationscenarios.
ApplyingtoDifferentLLMs. OurexperimentsexploretheapplicationofSEAacrossLLMsof
varyingsizes,includingtheGemma-2B[16],Phi-3-mini-4k-instruct[1],Vicuna-1.5-7B&13B[11],
andLlaMA3-8B-Instruct[3]. Notably,forthesmallermodel,SEAsignificantlyboostsperformance
across multiple tasks. This highlights SEAâ€™s ability to effectively address misalignment issues
thataremorepronouncedinsmallerLLMs,therebyenhancingtheirperformance. LargerLLMs,
whileinherentlybetterathandlingmisalignment,stillbenefitfromSEA,indicatingthatSEAoffers
additionalalignmentgainsregardlessofmodelsize.
ScalingtoHigherResolution. WealsoexaminedtheimpactofSEAwithdifferentvisionencoders.
ReplacingtheCLIP-ViT[40]withtheSigLIP-SO(400M)[49],SEAconsistentlyboostsperformance,
underscoring SEAâ€™s versatility and robustness across different encoder architectures. Using the
S2-Wrapper [43] at 1008px resolution, SEA again demonstrated its effectiveness by improving
performanceacrosstheboard. Theseresultsindicatethathigherresolutioninputs,whichprovide
moredetailedvisualinnformation,furtherenhanceSEAâ€™ssupervisedalignmentprocess.
Finetune Vision Encoders Based on SEA. [45] has demonstrated that unfreezing the vision
encoderduringthefine-tuningstageisalwaysbeneficial.Weconductedablationexperimentsbasedon
Vicuna-1.5-7B[11]. Theexperimentalresults,asshownin Table3,illustratethatalignmenttraining
10viaSEA,combinedwithunfreezingthevisionencoderduringthefine-tuningstage, consistently
yields positive benefits across all evaluated benchmarks. Compared to only using either SEA or
unfreezingthevisionencoderduringfine-tuning,thecombinedapproachisamoreeffectivetraining
strategy,indicatingthatSEAenhancesthebenefitsobtainedfromunfreezingthevisionencoder.
6 ConclusionandDiscussion
Inthispaper,wepresentSupervisedEmbeddingAlignment(SEA),astraightforwardandbroadly
applicablemethodbasedonvision-languagepre-trainedvisionencodersinmultimodalLLMs.Unlike
mostpriorstudiesthatfocusedonaligningvisionandlanguagerepresentationssolelythroughacus-
tomizedadapter,ourapproachexploresthenoveldirectionofenhancingalignmentwithsupervision
duringpre-trainingtoimprovetheperformanceofmultimodalLLMs. Ourexperimentsdemonstrate
thatSEAleadstosignificantimprovementsacrossmultiplebenchmarks. Additionally,SEAexhibits
robustnessandversatility,makingitapplicabletoavarietyofvisionencodersandlargelanguage
models. The proposed approach effectively utilizes the strengths of vision-language pre-trained
models,potentiallyunlockingnewfrontiersinmultimodalreasoningcapabilities.
However,despitethebenefitsSEAbringstotheperformanceofMultimodalLLMs,therearenotable
limitationsthatneedaddressing.Firstly,obtainingalignmentlabelsinSEAconstrainsitsapplicability,
asitcurrentlyreliesonpre-trainedvisionencodersalignedwithvision-languagepairs.ExtendingSEA
toeffectivelyalignandsuperviseautoregressivevisionencoders[10,19,38]orothertypesofvision
encoders[23,41]remainsanareaforfurtherresearch. Moreover,findinganoptimalrepresentation
foreachvisualtokenintheembeddingspaceremainsasignificantchallenge,impactinghowLLMs
interpretthesetokens. Thereisalsoconcernthatmanuallyguidingtheadapterforsemanticalignment
mightresultinthelossofadditionalinformationinthevisualtokens.Addressingtheseissuesrequires
thedevelopmentofmoreeffectivealignmentmethods,whichwillbecrucialforfurtherenhancing
thecapabilitiesandrobustnessofmultimodalLLMs.
References
[1] M.Abdin,S.A.Jacobs,A.A.Awan,J.Aneja,A.Awadallah,H.Awadalla,N.Bach,A.Bahree,
A.Bakhtiari,J.Bao,H.Behl,etal. Phi-3technicalreport: Ahighlycapablelanguagemodel
locallyonyourphone,2024.
[2] H.Agrawal,K.Desai,Y.Wang,X.Chen,R.Jain,M.Johnson,D.Batra,D.Parikh,S.Lee,
andP.Anderson. Nocaps: Novelobjectcaptioningatscale. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages8948â€“8957,2019.
[3] AI@Meta. Llama3modelcard. 2024.
[4] J.-B.Alayrac,J.Donahue,P.Luc,A.Miech,I.Barr,Y.Hasson,K.Lenc,A.Mensch,K.Millican,
M.Reynolds,etal. Flamingo: avisuallanguagemodelforfew-shotlearning. InNeurIPS,2022.
[5] S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,andD.Parikh. VQA:Visual
questionanswering. InIEEEICCV,pages2425â€“2433,2015.
[6] A.Awadalla,I.Gao,J.Gardner,J.Hessel,Y.Hanafy,W.Zhu,K.Marathe,Y.Bitton,S.Gadre,
S.Sagawa,etal. Openflamingo: Anopen-sourceframeworkfortraininglargeautoregressive
vision-languagemodels. arXivpreprintarXiv:2308.01390,2023.
[7] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: A
frontierlargevision-languagemodelwithversatileabilities. arXivpreprintarXiv:2308.12966,
2023.
[8] I.Bica,A.IlicÂ´,M.Bauer,G.Erdogan,M.BoÅ¡njak,C.Kaplanis,A.A.Gritsenko,M.Minderer,
C.Blundell,R.Pascanu,andJ.MitrovicÂ´. Improvingfine-grainedunderstandinginimage-text
pre-training,2024.
[9] L.Chen,J.Li,X.Dong,P.Zhang,C.He,J.Wang,F.Zhao,andD.Lin. Sharegpt4v: Improving
largemulti-modalmodelswithbettercaptions,2023.
11[10] X.Chen,S.Xie,andK.He. Anempiricalstudyoftrainingself-supervisedvisiontransformers.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages9640â€“9649,
2021.
[11] W.-L.Chiang,Z.Li,Z.Lin,Y.Sheng,Z.Wu,H.Zhang,L.Zheng,S.Zhuang,Y.Zhuang,J.E.
Gonzalez,I.Stoica,andE.P.Xing. Vicuna: Anopen-sourcechatbotimpressinggpt-4with
90%*chatgptquality. https://lmsys.org/blog/2023-03-30-vicuna/,2023.
[12] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, and
C.Shen. Mobilevlm: Afast,strongandopenvisionlanguageassistantformobiledevices,
2023.
[13] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, and
C.Shen. Mobilevlmv2: Fasterandstrongerbaselineforvisionlanguagemodel,2024.
[14] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning. arXiv:2305.06500,
2023.
[15] D.Driess,F.Xia,M.S.M.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,A.Wahid,J.Tompson,
Q.Vuong,T.Yu,W.Huang,Y.Chebotar,P.Sermanet,D.Duckworth,S.Levine,V.Vanhoucke,
K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. Palm-e: an
embodiedmultimodallanguagemodel. InProceedingsofthe40thInternationalConferenceon
MachineLearning,ICMLâ€™23.JMLR.org,2023.
[16] Google. Gemma: Introducingnewstate-of-the-artopenmodels. hhttps://blog.google/
technology/developers/gemma-open-models/,2024.
[17] Y.Goyal, T.Khot, D.Summers-Stay, D.Batra, andD.Parikh. Makingthevinvqamatter:
Elevatingtheroleofimageunderstandinginvisualquestionanswering. InCVPR,2017.
[18] D.Gurari,Q.Li,A.J.Stangl,A.Guo,C.Lin,K.Grauman,J.Luo,andJ.P.Bigham. Vizwiz
grandchallenge: Answeringvisualquestionsfromblindpeople,2018.
[19] K.He,X.Chen,S.Xie,Y.Li,P.DollÃ¡r,andR.Girshick. Maskedautoencodersarescalable
visionlearners. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages16000â€“16009,2022.
[20] D.A.HudsonandC.D.Manning. GQA:Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InIEEECVPR,pages6700â€“6709,2019.
[21] C.Jia,Y.Yang,Y.Xia,Y.-T.Chen,Z.Parekh,H.Pham,Q.Le,Y.-H.Sung,Z.Li,andT.Duerig.
Scalingupvisualandvision-languagerepresentationlearningwithnoisytextsupervision. In
ICML,2021.
[22] A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.delasCasas,F.Bressand,
G.Lengyel,G.Lample,L.Saulnier,L.R.Lavaud,M.-A.Lachaux,P.Stock,T.L.Scao,T.Lavril,
T.Wang,T.Lacroix,andW.E.Sayed. Mistral7b,2023.
[23] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,
A.C.Berg,W.-Y.Lo,etal. Segmentanything. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages4015â€“4026,2023.
[24] J.Y.Koh,R.Salakhutdinov,andD.Fried.Groundinglanguagemodelstoimagesformultimodal
inputsandoutputs,2023.
[25] R.Krishna,Y.Zhu,O.Groth,J.Johnson,K.Hata,J.Kravitz,S.Chen,Y.Kalantidis,L.-J.Li,
D.A.Shamma,etal. VisualGenome: Connectinglanguageandvisionusingcrowdsourced
denseimageannotations. IJCV,123:32â€“73,2017.
[26] H.LaurenÃ§on,L.Saulnier,L.Tronchon,S.Bekman,A.Singh,A.Lozhkov,T.Wang,S.Karam-
cheti,A.M.Rush,D.Kiela,M.Cord,andV.Sanh. OBELICS:Anopenweb-scalefiltered
datasetofinterleavedimage-textdocuments. InThirty-seventhConferenceonNeuralInforma-
tionProcessingSystemsDatasetsandBenchmarksTrack,2023.
12[27] J.Li,D.Li,S.Savarese,andS.Hoi. Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozenimageencodersandlargelanguagemodels. arXiv:2301.12597,2023.
[28] Y.Li,Y.Du,K.Zhou,J.Wang,W.X.Zhao,andJ.-R.Wen. Evaluatingobjecthallucinationin
largevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[29] Y.Li,C.Wang,andJ.Jia. Llama-vid: Animageisworth2tokensinlargelanguagemodels.
arXiv:2311.17043,2023.
[30] Y.Li,H.Wang,Y.Duan,andX.Li. Clipsurgeryforbetterexplainabilitywithenhancementin
open-vocabularytasks,2023.
[31] Y.Li,Y.Zhang,C.Wang,Z.Zhong,Y.Chen,R.Chu,S.Liu,andJ.Jia. Mini-gemini: Mining
thepotentialofmulti-modalityvisionlanguagemodels,2024.
[32] Y.Lin,Y.Li,D.Chen,W.Xu,R.Clark,P.Torr,andL.Yuan. Rethinkingvisualpromptingfor
multimodallargelanguagemodelswithexternalknowledge. arXivpreprintarXiv:2407.04681,
2024.
[33] H.Liu,C.Li,Y.Li,andY.J.Lee. Improvedbaselineswithvisualinstructiontuning. arXiv
preprintarXiv:2310.03744,2023.
[34] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. NeurIPS,36,2024.
[35] Y.Liu,H.Duan,Y.Zhang,B.Li,S.Zhang,W.Zhao,Y.Yuan,J.Wang,C.He,Z.Liu,etal.
MMBench: Isyourmulti-modalmodelanall-aroundplayer? arXivpreprintarXiv:2307.06281,
2023.
[36] P.Lu,S.Mishra,T.Xia,L.Qiu,K.-W.Chang,S.-C.Zhu,O.Tafjord,P.Clark,andA.Kalyan.
Learn to explain: Multimodal reasoning via thought chains for science question answering.
AdvancesinNeuralInformationProcessingSystems,35:2507â€“2521,2022.
[37] A.Mishra,S.Shekhar,A.K.Singh,andA.Chakraborty.OCR-VQA:Visualquestionanswering
byreadingtextinimages. InIEEEICDAR,pages947â€“952,2019.
[38] M.Oquab,T.Darcet,T.Moutakanni,H.Vo,M.Szafraniec,V.Khalidov,P.Fernandez,D.Haziza,
F.Massa, A.El-Nouby, etal. Dinov2: Learningrobustvisualfeatureswithoutsupervision.
arXivpreprintarXiv:2304.07193,2023.
[39] J.Park,J.Lee,andK.Sohn. Bridgingvisionandlanguagespaceswithassignmentprediction.
arXivpreprintarXiv:2404.09632,2024.
[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InICML,2021.
[41] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer.High-resolutionimagesynthesis
withlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10684â€“10695,2022.
[42] Y.Shang,M.Cai,B.Xu,Y.J.Lee,andY.Yan. Llava-prumerge: Adaptivetokenreductionfor
efficientlargemultimodalmodels. arXivpreprintarXiv:2403.15388,2024.
[43] B.Shi,Z.Wu,M.Mao,X.Wang,andT.Darrell. Whendowenotneedlargervisionmodels?,
2024.
[44] A.Singh,V.Natarajan,M.Shah,Y.Jiang,X.Chen,D.Batra,D.Parikh,andM.Rohrbach.
Towardsvqamodelsthatcanread. InCVPR,2019.
[45] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer,
X.Pan,A.Wang,R.Fergus,Y.LeCun,andS.Xie. Cambrian-1: Afullyopen,vision-centric
explorationofmultimodalllms,2024.
[46] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al.
Cogvlm: Visualexpertforpretrainedlanguagemodels. arXiv:2311.03079,2023.
13[47] A. Yan, Z. Yang, J. Wu, W. Zhu, J. Yang, L. Li, K. Lin, J. Wang, J. McAuley, J. Gao, and
L.Wang. Listitemsonebyone: Anewdatasourceandlearningparadigmformultimodalllms,
2024.
[48] W.Yu,Z.Yang,L.Li,J.Wang,K.Lin,Z.Liu,X.Wang,andL.Wang. Mm-vet: Evaluating
largemultimodalmodelsforintegratedcapabilities,2023.
[49] X.Zhai,B.Mustafa,A.Kolesnikov,andL.Beyer.Sigmoidlossforlanguageimagepre-training,
2023.
[50] F. Zhao, T. Pang, C. Li, Z. Wu, J. Guo, S. Xing, and X. Dai. Aligngpt: Multi-modal large
languagemodelswithadaptivealignmentcapability. arXivpreprintarXiv:2405.14129,2024.
[51] B.Zhou,Y.Hu,X.Weng,J.Jia,J.Luo,X.Liu,J.Wu,andL.Huang. Tinyllava: Aframework
ofsmall-scalelargemultimodalmodels,2024.
[52] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4: Enhancingvision-language
understandingwithadvancedlargelanguagemodels. arXiv:2304.10592,2023.
[53] Y.Zhu,M.Zhu,N.Liu,Z.Ou,X.Mou,andJ.Tang. Llava-phi: Efficientmulti-modalassistant
withsmalllanguagemodel,2024.
14