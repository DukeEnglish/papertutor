TechnicalReport
PUMA: EMPOWERING UNIFIED MLLM WITH
MULTI-GRANULAR VISUAL GENERATION
RongyaoFang1âˆ—â€  ChengqiDuan2âˆ— KunWang3 HaoLi1,4 HaoTian3 XingyuZeng3
RuiZhao3 JifengDai4,5 HongshengLi1â€¡ XihuiLiu2â€¡
1CUHKMMLab 2HKUMMLab 3SenseTime 4ShanghaiAILaboratory 5THU
a) The Diversity and Controllability Requirements of Different Visual Generative Tasks
Diverse Text-to-Image Generation Conditional Image Generation Image Manipulation
Generate images of a puma in different postures. Draw an image based on canny input. Remove the tractor from the field.
Diversity Controllability
b) PUMA for Various Visual Generation and Understanding Tasks
Image Inpainting Conditional Image Generation Diverse Text-to-Image Generation
The image should Produce a visual
convey On the withcannyedge
way to Eggum. Portray Preparing
Use inpainted for Examinations, Generate an
image as input to 1864 by Ilya image with
generate an image. Efimovich Repin. the caption:
a cute fox
Image Editing Image Understanding artwork by
James
W is th ha et I at d e vn ac no tau gra eg oe fs tp he eo sp el re v t io ce t ake G vii bll ri aa nr td c olors. Please remove the purpose without the need for prior
black cow statue of the scheduling, making it more
from the bench. sign? accessible and convenient
for potential customers.
Figure1: a)Diversityandcontrollabilitytradeoffinimagegenerationtasks: diversetext-to-image
generation requires high diversity and fidelity, while tasks like conditional generation and manip-
ulation requirehigh controllabilityon theimage. b) The introducedPUMA, aunified multimodal
largelanguagemodelthatprocessesandgeneratesmulti-granularvisualrepresentations,balancing
diversityandcontrollabilityacrossvisualgenerationtasks.Itexcelsinimageunderstanding,diverse
text-to-imagegeneration,editing,inpainting,colorization,andconditionalimagegeneration.
ABSTRACT
Recent advancements in multimodal foundation models have yielded significant
progress in vision-language understanding. Initial attempts have also explored
the potential of multimodal large language models (MLLMs) for visual content
generation. However, existing works have insufficiently addressed the varying
granularitydemandsofdifferentimagegenerationtaskswithinaunifiedMLLM
paradigmâ€”fromthediversityrequiredintext-to-imagegenerationtotheprecise
controllabilityneededinimagemanipulation. Inthiswork, weproposePUMA,
emPoweringUnifiedMLLMwithMulti-grAnularvisualgeneration. PUMAuni-
fiesmulti-granularvisualfeaturesasbothinputsandoutputsofMLLMs,elegantly
addressing the different granularity requirements of various image generation
taskswithinaunifiedMLLMframework. Followingmultimodalpretrainingand
task-specificinstructiontuning,PUMAdemonstratesproficiencyinawiderange
ofmultimodaltasks. Thisworkrepresentsasignificantsteptowardsatrulyuni-
fiedMLLMcapableofadaptingtothegranularitydemandsofvariousvisualtasks.
Thecodeandmodelwillbereleasedinhttps://github.com/rongyaofang/PUMA.
âˆ—EqualContribution
â€ ProjectLead
â€¡CorrespondingAuthors
1
4202
tcO
71
]VC.sc[
1v16831.0142:viXraTechnicalReport
1 INTRODUCTION
Unifying multimodal understanding and generation capabilities within a single model is a critical
milestonetowardartificialgeneralintelligence(AGI).Towardsthisgoal,recentadvancements(Liu
etal.,2024b;Zhuetal.,2023a)inmultimodallargelanguagemodels(MLLMs)havemadesignif-
icant progress in integrating visual reasoning and understanding with natural language interfaces.
However,developingaunifiedframeworkthatexcelsatbothcomprehendingandgeneratingmulti-
modalcontentremainsasignificantchallengeinthefieldofartificialintelligence.
Recentstudies(Sunetal.,2023;Geetal.,2024b)haveexploredMLLMâ€™spotentialforvisualgen-
eration, beyond the previously well-explored visual understanding and reasoning with MLLMs.
These approaches enable MLLMs to process image-text inputs and produce either textual outputs
or semantic-level visual tokens. In the case of image generation, these visual tokens are subse-
quently transformed into pixel-space images using diffusion-based decoders. Such unified frame-
works empower MLLMs to perform a wide spectrum of tasks within a single framework, ranging
fromdetailedvisualanalysistocreativeimagesynthesis.
However, existing MLLM-based methods (Sun et al., 2023; 2024b) face a common challenge in
thetrade-offbetweendiversity fortext-to-imagegenerationandhighcontrollabilityfor taskssuch
as image editing. Previous methods mostly rely on single-granular features extracted from a vi-
sualencoderandneglectthevaryinggranularityrequirementsofdifferenttasks. Ontheonehand,
generatingdiverseimagesreflectingtherealworldfromtextdescriptionsrequiresfeaturesthaten-
codecoarsesemanticconcepts. Suchfeaturesarefedasconditionsintothediffusion-basedimage
decoder, allowing the diffusion model to generate diverse images that semantically align with the
textprompt. Ontheotherhand,tasksdemandingprecisecontroloveroutputimages,suchasimage
editingandinpainting, requiretheLLMstopredictfine-grainedfeaturesthatencoderich, detailed
visualinformationfortheimagedecoder. Thisdichotomypresentsasignificantchallengeforcur-
rentMLLM-basedmethods,whichtypicallygeneratesingle-granularfeaturerepresentationsforall
tasks. Asaresult, modelsoptimizedfordiverseimagegenerationoftenlackthefine-grainedcon-
trollabilitynecessaryfordetaileddownstreamtaskssuchasediting,whilethosefocusedonprecise
controllability produce less varied outputs for the task of text-to-image generation. Although re-
cent work like SEED-X (Ge et al., 2024b) attempts to bypass this issue by leveraging condition
images directly input to the diffusion-based decoder for fine-grained control, a unified solution to
themulti-granularityproblemremainsunderexplored.
Towards the multi-granular feature demands of various tasks, we propose a novel paradigm
emPowering Unified MLLM with Multi-grAnular visual generation (PUMA). PUMA facilitates
seamless integration of image generation and understanding processes, while simultaneously han-
dling multiple feature granularities â€” from coarse-grained abstractions to fine-grained details â€”
withinasingleframework. Byleveragingmulti-scalefeatures,ourapproachempowersMLLMsto
excelindiverseimagegenerationandcontrollabledownstreamtasks,withinaunifiedframework.
Our method comprises three key modules: 1) An image encoder that extracts multi-granular rep-
resentations,whichserveasthefoundationforvisualgenerationandunderstanding; 2)Anautore-
gressiveMLLMthatprocessesandprogressivelygeneratesmulti-scaleimagefeatures;and3)Aset
of dedicated diffusion-based image decoders that decode images from MLLM-generated features
at multiple granularities. To optimize this framework, we employ a two-stage training strategy:
first fine-tuning the set of pre-trained diffusion models as our image decoders, where each model
reconstructs or generates images conditioned on the corresponding feature granularities from the
encoder;thentrainingtheautoregressiveMLLMwithregressionlosssupervisedbythemulti-scale
encoder features to process and generate multi-granular image features. PUMA leverages large-
scale pre-training followed by task-specific instruction tuning on a collection of linguistic-visual
datasets,enablingourmodeltohandlevarioustasksincludingimageunderstanding,text-to-image
generation,editing,inpainting,colorization,andconditionalgeneration.
In summary, we introduce a novel multi-granularity paradigm for MLLMs that addresses the lim-
itationsofexistingsingle-scalemethods. Bysimultaneouslyprocessingandgeneratingfeaturesat
multiple granularities, our approach enables a unified framework to handle a wide range of tasks,
from diverse image generation to precise editing and highly controllable generation. This unified
frameworkrepresentsasignificantadvancementtowardsmoreversatileandcapableMLLMs,con-
tributingtothebroadergoalofachievingAGIinmultimodaldomains.
2TechnicalReport
2 RELATED WORK
2.1 MULTIMODALUNDERSTANDING
Therapidadvancementoflargelanguagemodels(LLMs)hascatalyzedsignificantprogressinmul-
timodallargelanguagemodels(MLLMs)formultimodalunderstandingtasksDaietal.(2023);Li
etal.(2024b);Zhangetal.(2023a);Chenetal.(2024);Linetal.(2024);Zhangetal.(2024b);Li
et al. (2024a). Pioneering works such as LLaVA (Liu et al., 2024b) and MiniGPT-4 (Zhu et al.,
2023a) have demonstrated remarkable performance across diverse image understanding tasks, in-
cluding visual question answering (VQA), visual reasoning, optical character recognition (OCR),
and object grounding. These approaches typically employ visual encoders, such as the CLIP en-
coder(Radfordetal.,2021),toextractcontinuousimagefeatures,whicharethenprojectedintothe
LLMâ€™s embedding space for subsequent tasks. While successfully unifying various image under-
standingtaskswithinasinglemodel,thesemethodsmostlyadheretoamultimodal-input,text-output
paradigm.Consequently,theyexcelattext-basedresponsestovisualinputsbutcannotgeneratemul-
timodaloutputsbeyondtext,limitingtheirapplicabilityintasksrequiringvisualcontentgeneration.
2.2 UNIFIEDUNDERSTANDINGANDGENERATIONFORMLLMS
RecentresearchhasfocusedonequippingMLLMswithmultimodaloutputcapabilities(Wuetal.,
2023; Tang et al., 2024; Ye et al., 2024a; Zhu et al., 2023b). GILL (Koh et al., 2024) pioneered
theintegrationofimagegenerationabilitiesintoMLLMs. Subsequently,SEED-LLaMA(Geetal.,
2023)andEmu(Sunetal.,2023)furtheradvancedimagegenerationandunderstandingcapabilities
within MLLMs, while DreamLLM (Dong et al., 2023) proposed an end-to-end training approach
forenhancedperformance.
Morerecentworks,suchasSEED-X(Geetal.,2024b)andEmu2(Sunetal.,2024b),havescaledup
MLLMsforunifiedgeneration,adoptingcontinuousfeature-basedmethods. Theseapproachesuti-
lizepre-trainedvisionencoderstoextractcontinuoussemanticfeatures,whichMLLMsthenautore-
gressively regress. Specialized diffusion model-based decoders transform these MLLM-generated
featuresintopixel-spaceimages. However, thesingle-scaleimagefeaturegenerationpipelineem-
ployed by these methods struggles to address tasks with varying granularity demands, making it
challenging to balance diverse image generation with fine-grained control for manipulation tasks.
SEED-Xattemptstoaddressthemulti-granularityissuebyintroducingconditionalimageinputto
thediffusion-baseddecoderforfine-grainedcontrol. However,thisapproachlimitsitsapplicability
toimageeditingtasksencounteredduringdecodertraining. Consequently,aunifiedsolutiontothe
multi-granularity problem remains underexplored. In contrast, our work proposes a novel multi-
granularityparadigmthataddressestheselimitationsbysimultaneouslyhandlingmultiplelevelsof
featuregranularitywithinasingle,unifiedframework.
Alternative approaches have also been investigated. Chameleon (Team, 2024) explored using dis-
creteimagetokenstobridgeimageunderstandingandgeneration,butthevectorquantizationprocess
leads to information loss, hindering high-performance image understanding. TransFusion (Zhou
et al., 2024) and show-o (Ge et al., 2023) proposed transforming the MLLM backbone itself into
adenoiserinadiffusion-basedordemasking-basedapproach. However,thesemethodsrequirenu-
merousdenoisingstepsforeachimagegeneration,resultinginsubstantialcomputationalcostsgiven
thescaleofcurrentMLLMbackbones.VAR(Tianetal.,2024)isanothertrackofgenerationframe-
work that implements hierarchical autoregressive with discrete tokens for image generation, but it
onlydiscussesimagegenerationandcannotunifymultimodaltasks.
3 METHOD
Existingapproachestypicallyoptimizeforeitherfineorcoarse-grainedfeatures,resultinginatrade-
off between precise control and generation diversity. To overcome this limitation, we propose
PUMA,aunifiedmulti-granularMLLMparadigm. Ourapproachsimultaneouslyprocessesmulti-
plelevelsoffeaturegranularitywithinaunifiedMLLMframework,facilitatingseamlesstransitions
acrossawidespectrumofmultimodaltasks.
Our framework consists of three key components: an image encoder (Sec. 3.1), a set of image
decodersconditionedondifferentgranularfeatures(Sec.3.2), andamulti-granularautoregressive
MLLM(Sec.3.3). Thesecomponentsworksynergisticallytoextract,process,andgeneratemulti-
scale image features, adapting to various task-specific granularity requirements. To optimize our
3TechnicalReport
ExtractMulti-granularImageFeature
ğ‘“ğ‘ ğ‘“ğ‘âˆ’1 ğ‘“0
â€¦ â€¦ â€¦ [/IMG][TextInput] [IMG] â€¦ â€¦ â€¦
PUMA Multimodal Generative Model
â€¦ â€¦ â€¦ â€¦ â€¦ â€¦
[/IMG] [TextOutput] ğ‘“áˆ˜ ğ‘ ğ‘“áˆ˜ ğ‘âˆ’1 ğ‘“áˆ˜ 0
TextToken Multi-granularImageFeature
UnifiedMulti-granularAutoregressiveMLLM Classification Loss RegressionLoss
1.DiverseTexttoImageGeneration 2.ImageEditing
ğ‘“áˆ˜
ğ‘ â€¦
ğ‘“áˆ˜
0
[Prompt] PUMA Decoder PUMA Decoder
â€¦ ğ‘“áˆ˜
ğ‘âˆ’1
â€¦ ğ‘“áˆ˜
1
Coarse-granularImageFeature +[Instruction] Fine-granularImageFeature
3.ConditionalImageGeneration 4.ImageUnderstanding
â€¦
ğ‘“áˆ˜
0
PUMA Decoder PUMA [Understandingand
AnswertotheQuestion]
â€¦ ğ‘“áˆ˜
1
+[Prompt] Fine-granularImageFeature +[Question]
Figure 2: Upper: PUMAâ€™s unified multi-granular autoregressive pipeline for processing and gen-
erating text and multi-granular visual features. Lower: Illustration of PUMAâ€™s versatility across
various tasks: 1) diverse text-to-image generation, 2) image editing, 3) conditional image genera-
tion,and4)imageunderstanding,showcasingdifferentinput-outputconfigurations.
MLLM,weemployatwo-stageprocessofpretrainingandinstructiontuning(Sec.3.4),enablingit
toperformawiderangeoftasksincludingimageunderstanding,generation,editing,andconditional
imagegeneration.
3.1 IMAGEENCODINGANDMULTI-GRANULARFEATUREEXTRACTION
Our unified multi-granularity paradigm leverages a semantic image encoder to extract multi-scale
features, forming the foundation for diverse visual task processing. We employ a CLIP (Radford
etal.,2021)semanticimageencodertoprocessinputimagesxandgeneratetheinitialsetofhigh-
resolution features f âˆˆ RHÃ—WÃ—C, with H and W representing the spatial dimensions of the
0
highest resolution feature grid, and C denoting the channel dimension. In our setting, the feature
sizeisH =W =16,thusthehighestresolutionfeaturef has256visualtokens.
0
To obtain multi-granular representations, we derive lower resolution features through successive
applicationsof2Daveragepoolingwithkernelsize2andstride2:
f =AvgPool(f ), i=1,2,...,N (1)
i iâˆ’1
where N is the number of additional granular levels. This process generates a series of feature
grids at progressively coarser resolutions, ranging from fine-grained features preserving detailed
spatialinformationandlocaltextures,throughmid-levelfeaturescapturingobjectpartsandregional
structures,tofeaturesrepresentingcoarse-grainedsemanticconcepts. Thesefeaturesaredenotedas
f ,f ,f ,f ,andf ,whichhave256,64,16,4,and1visualtokensrespectively.
0 1 2 3 4
3.2 MULTI-GRANULARVISUALDECODING
Imagefeaturesatdifferentgranularitiesencodevaryinglevelsofinformation.Weemploydiffusion-
based models as decoders due to their flexible capability to handle multi-scale features. When
processing coarse-grained semantic features, the decoders can effectively synthesize missing fine-
grainedinformationwiththeirlearnedimagepriorsandgeneratediverse,semantics-alignedimages.
Ontheotherhand, whenhandlingfine-grainedfeatures, theyaccuratelyreconstructpreciseimage
details. Thisversatilityingeneratingorreconstructingimagesacrossdifferentgranularitiesmakes
diffusion-basedmodelssuitableforourmulti-granularityapproach.
4TechnicalReport
Original image ğ· 0 ğ‘“ 0 ğ· 1 ğ‘“ 1 ğ· 2 ğ‘“ 2 ğ· 3 ğ‘“ 3 ğ· 4 ğ‘“ 4
Fine-grained reconstruction Semantic-guided generation
Figure3: Multi-granularvisualdecodingfromfine-grainedtocoarse-grainedgranularity.
Wedevelopasetofdedicatediffusion-basedimagedecodersD ,D ,...,D correspondingtothe
0 1 N
featurescalesf ,f ,...,f .Thesedecodersenablethevisualdecodingofimagesatvariouslevelsof
0 1 N
granularity.WeformulatetheimagedecodingprocessforeachgranularityleveliasxË† =D (f ,z),
i i i
wherexË† isthedecodedimage,f isthefeaturemapatgranularityleveli,andz isarandomnoise
i i
vectorforthediffusionprocess.
Weleveragethepre-trainedSDXLmodels(Podelletal.,2023)asourdecodingframeworkandfine-
tune these pre-trained models to generate or reconstruct images conditioned on different granular
features. Bymodifyingtheconditionalinputmechanismthroughcross-attentioninSDXLtoaccept
ourmulti-granularfeaturesf ,weharnessthemodelsâ€™inherentabilitytodecodecoherentimages.
i
Fig. 4 shows the training process of
different granular image decoding,
during which the image encoder is
ğ‘“0
ğ‘“ğ‘âˆ’1
f Fr io gz .en 3to illp ur se trs ae tr ev ses the ema vn isti uc ap lro dp ece ort dy -.
Encoder
ğ»
â€¦ 2
1ğ‘“ğ‘
AvgPool AvgPool
ingcapabilitiesofmulti-granularde- 2 1
coders. The visualizations demon- ğ‘Š
strate the fidelity of decoded images
across different granularities, with Decoder â€¦ Decoder Decoder
finer-grainedfeaturesyieldingrecon-
ğ·0 ğ·ğ‘âˆ’1 ğ·ğ‘
structionsclosertotheoriginalinput, Multi-granular â€¦
and coarser-grained features leading VisualDecoding
to image generation guided by the
semantics of the input image. This
Figure4: Trainingphaseofmulti-granularvisualdecoding.
validates the effectiveness of our ap-
proach in preserving and utilizing
multi-granularvisualinformation.
This multi-granular decoding framework, in conjunction with our hierarchical feature extraction,
establishes a foundation for the subsequent stages of our MLLM architecture, paving the way for
diversevisualtasksinlatertrainingphases.
3.3 PROGRESSIVEMULTI-GRANULARIMAGEMODELINGINAUTOREGRESSIVEMLLM
Drivenbythegoalofutilizingaunifiedframeworkcapableofadaptingtoawiderangeofvisual-
linguistictaskswithvaryinggranularityrequirements,wedesignanautoregressiveMLLMtopro-
cessandgeneratebothtexttokensandmulti-granularimagefeatures.
Our autoregressive MLLM, denoted as M, processes text and multi-granular image features pro-
gressively, as illustrated in Fig. 2. The model processes features token by token, predicting each
token sequentially within each granularity level, and progressing from the coarsest level N to the
5TechnicalReport
finestlevel0. Thisapproachallowsthemodeltorefineitspredictionsasmoredetailedinformation
becomesavailable.
Westructuretheinputsequenceasaconcatenationoftexttokensandflattenedimagefeaturetokens
frommultiplegranularitylevels. Thisprogressiveapproachenablesthemodeltocapturedependen-
ciesacrossdifferentscales,fromcoarseglobalstructurestofinelocaldetails.
TheMLLMistrainedusinganautoregressivenexttokenpredictionobjective,combiningbothtext
andimagelosses:
L=âˆ’(cid:88) logP(t |t ,F
)+(cid:88)N
Î±
(cid:88)ki
|f âˆ’fË† |2 (2)
i <i <i i i,j i,j
i i=0 j=1
The first term represents the cross-entropy loss for text token prediction, where t are text tokens.
i
The second term is the regression loss for image feature prediction, where f and fË† are the
i,j i,j
groundtruthandpredictedfeaturetokens,respectively,atthei-thgranularitylevel. k isthenumber
i
ofvisualtokensatthei-thgranularitylevel. ThecoefficientÎ± allowsforadjustingtheimportance
i
ofeachgranularitylevelduringtraining.
3.4 MULTIMODALPRETRAININGANDINSTRUCTTUNING
To demonstrate the effectiveness of our unified multi-granularity paradigm, we implement a com-
prehensivetwo-stagetrainingpipelineforPUMA:multimodalpretrainingfollowedbytask-specific
instructtuning.Thisapproachallowsourmodeltofirstacquirebroadmultimodalcapabilitiesbefore
specializingintargetedvisual-linguistictasksduringthesubsequentinstructtuningstage.
Multimodal Pretraining: Our multimodal pretraining leverages a diverse set of large-scale
datasets: Laion-2B(Schuhmannetal.,2022),Laion-Aesthetics(Burger,2023),GRIT(Pengetal.,
2023), The Pile (Gao et al., 2020), OCR-VQA-200K (Mishra et al., 2019), and LLaVAR (Zhang
etal.,2023b). Thiscombinationofdatasetsprovidesarichmixtureofimage-textpairs,textualdata,
and specialized visual question-answering samples. To enhance the modelâ€™s bidirectional under-
standingofimage-textrelationships,weemployadynamictrainingstrategythatrandomlyalternates
betweentext-to-imageandimage-to-texttasksforeachimage-textpair.
Instruct Tuning: Following pretraining, we conduct targeted instruct tuning to adapt our model
tospecificvisual-linguistictasks. ToevaluatePUMAâ€™sperformanceacrossdifferenttasktypes,we
fine-tune four dedicated models for the four types of tasks, each initialized from the pretraining
checkpoint.
High-qualityText-to-ImageGeneration:WeutilizeLaion-Aesthetics(Burger,2023)andJourneyDB
(Sunetal.,2024a)tofocusongeneratingaestheticallypleasinganddiverseimages.
PreciseImageManipulation:TrainingontheSEED-Edit(Geetal.,2024a)datasetenablesaccurate
andcontrolledimageediting.
Conditional Image Generation: The subset of MultiGen-20M dataset (Qin et al., 2023) including
canny-to-image, inpainting, and colorization is employed to equip the model with the ability to
generateimagesunderspecificconditionsandconstraints.
ImageUnderstanding: Fine-tuningonthesubsetofLLaVA-OneVision(Lietal.,2024a)andCam-
brain (Tong et al., 2024) to enhance the modelâ€™s image comprehension capabilities. Data about
math/reasoningandcross-duplicateddatainthetwodatasetsareremoved.
4 EXPERIMENTS
Wepresentourexperimentalresultsasfollows: Sec.4.1detailsourexperimentalsetup. InSec.4.2,
we evaluate the effectiveness of our multi-granularity feature encoding and diffusion-based multi-
granularity image decoders. We then demonstrate PUMAâ€™s versatility across various tasks: di-
verse text-to-image generation (Sec. 4.3), image editing (Sec. 4.4), conditional image generation
(Sec.4.5),andvision-languageunderstanding(Sec.4.6).
6TechnicalReport
Original image SEED-LLaMA SEED-X Emu2 Ours (ğ‘“ scale)
0
Figure5:Fine-grainedimagereconstructionofSEED-LLaMA(Geetal.,2023),SEED-X(Geetal.,
2024b), Emu2(Sunetal.,2024b)andPUMA(f scale). Highqualityimagereconstructionisthe
0
foundationofpreciseimagemanipulationtasks.
Table1: ImagedecodingevaluationusingimageencoderanddecoderontheImageNetvalidation
set. PSNR and LPIPS measure the difference between reconstructed and ground truth images.
r r
PSNR andLPIPS measurethedifferencebetweentwoseparatereconstructionsofthesameimage,
d d
reflectingdecodingdiversity.
Model Encoderfoundation Tokennum. PSNR â†‘ LPIPS â†“ PSNR â†“ LPIPS â†‘
r r d d
SEED-LLaMA(2023) BLIP-2ViT(0.3B) 32 9.73 0.6756 10.45 0.6189
SEED-X(2024b) Qwen-VLEncoder(4B) 64 10.86 0.5152 11.60 0.4292
Emu2(2024b) EVA02-CLIP-E-plus(4B) 64 15.72 0.2532 16.07 0.2101
PUMA(f scale) CLIP-Large(0.3B) 1 10.76 0.6481 12.82 0.5751
4
PUMA(f scale) CLIP-Large(0.3B) 4 11.04 0.5971 12.61 0.5329
3
PUMA(f scale) CLIP-Large(0.3B) 16 12.35 0.4992 13.50 0.4354
2
PUMA(f scale) CLIP-Large(0.3B) 64 13.26 0.4325 14.12 0.3631
1
PUMA(f scale) CLIP-Large(0.3B) 256 18.16 0.2215 19.36 0.1559
0
4.1 SETUP
Our unified multi-granular MLLM employs LLaMA-3 8B (Touvron et al., 2023) as the language
model backbone and CLIP-Large (224Ã—224 input) (Radford et al., 2021) as the image encoder.
The image decoders are initialized from pretrained SDXL models (Podell et al., 2023). For more
detailsontheexperimentalsetup,pleaserefertotheAppendix.
4.2 MULTI-GRANULARVISUALDECODING
Weevaluatethemulti-granularvisualdecodingcapabilitiesofourmodelusingmulti-scalefeatures
fromtheencoder(Sec.3.1)anddedicatedvisualdecoders(Sec.3.2). Ouraimistwofold:toachieve
precisereconstructionusingfine-grainedfeaturescales(suchasf andf ),andtoimplementhigh
0 1
diversity semantics-guided image generation using coarse-grained features (such as f and f ). It
4 3
isworthmentioningthatinthissubsectionwevalidatethemulti-granularityencoderanddecoders
(Fig.4),whiletheMLLM(Sec.3.3)isnotleveragedfortheexperimentsinthissubsection.
4.2.1 FINE-GRAINEDIMAGERECONSTRUCTION
Fine-grainedimagereconstructioniscrucialforpreservingimagedetails,yetithasposedsignificant
challengesformodelslikeSEED-LLaMA(Geetal.,2023),SEED-X(Geetal.,2024b),andEmu2
(Sunetal.,2024b). WhileSEED-LLaMAandSEED-Xstrugglewithdetailedreconstruction,limit-
ingtheirpreciseimagemanipulationcapabilitieswithoutadditionaltechniquessuchasconditional
imageinput(asusedinSEED-X),Emu2attemptstoimprovereconstructionbyscalingupitsimage
7TechnicalReport
ğ‘“ scale-seed:1 ğ‘“ scale-seed:2 ğ‘“ scale-seed:1 ğ‘“ scale-seed:2 Emu2-seed:1 Emu2-seed:2
4 4 3 3
Generationprompt:Anthropomorphic rat, wearing harajuku street wear, decora kei, hyper realistic, clothing shops, shop
signs, barbie aesthetic, kidcore, graphic.
Figure6: Diversityvisualizationoftext-to-imagegenerationresultsfromPUMAfeaturescalesf
4
(1visualtoken),f (4visualtokens),andEmu2(Sunetal.,2024b).Thegeneratedfeaturesareinput
3
tocorrespondingdiffusion-baseddecoderswithdifferentrandomseeds.
Table 2: Diverse text-to-image generation evaluation on MSCOCO 30K validation set. CLIP-I
andCLIP-Tmeasurethesimilaritybetweengeneratedimagesandgroundtruthimagesorprompts.
LPIPS quantifies the difference between two images generated from the same prompt, reflecting
d
generation diversity. 5-scale Max denotes selecting the image with the highest score among the 5
outputsandcomputestheaveragemaximumvalue.
Model Tokennum. CLIP-Iâ†‘ CLIP-Tâ†‘ LPIPS â†‘
d
SD-v1.5(2022) - 0.667 0.302 0.692
DALL-E2(2022) - - 0.314 -
SDXL(2023) - 0.674 0.310 0.600
DALL-E3(2023) - - 0.320 -
SEED-LLaMA(2023) 32 0.682 - 0.652
Emu(2023) 64 0.656 0.286 0.700
Emu2(2024b) 64 0.686 0.297 0.329
SEED-X(2024b) 64 0.729 0.314 0.493
PUMA(f scale) 1 0.699 0.295 0.613
4
PUMA(f scale) 4 0.703 0.300 0.558
3
PUMA(5-scaleMax) - 0.736 0.317 -
encoderto4billionparameters. Ourapproachachievessuperiorreconstructionqualitywithamore
efficient architecture. We employ the CLIP-Large encoder (0.3 billion parameters), which is over
10timessmallerthanEmu2â€™s,andimplementfine-grainedlevelimageembeddingwith256tokens.
As demonstrated in Tab. 1, our method using f scale features achieves 18.16 PSNR and 0.2215
0 r
LPIPS (Zhangetal.,2018)ontheImageNetvalidationsetreconstruction.Theseresultsoutperform
r
Emu2â€™sreconstructionperformanceandsignificantlysurpassSEED-LLaMAandSEED-X(without
conditionalinput). Fig.5visuallyillustratesourmethodâ€™ssuperiorreconstructionquality.
4.2.2 SEMANTICS-GUIDEDGENERATION
Whilefine-grainedreconstructioniscrucialforpreciseimagemanipulation,tasksliketext-to-image
generationbenefitfromabalanceofsemanticfidelityandoutputdiversity. Ourapproachleverages
coarse-grainedfeatures(suchasf )toimplementsemantics-guidedimagegenerationthatpreserves
4
diversity in outputs. To quantify this semantics-guided diversity, we decode twice to obtain two
images from the same image input using different random seeds and measure their differences,
denoted as PSNR and LPIPS . Tab. 1 presents the diversity results for various visual decoding
d d
models and feature scales. Notably, our f and f scale decoders produce more diverse samples
3 4
compared to the decoders in SEED-X and Emu2, while still preserving the core semantics of the
input,asillustratedinFig.5. Thisdemonstratesourapproachâ€™seffectivenessinbalancingsemantic
accuracywithgenerativediversity,acrucialfactorintasksliketext-to-imagegeneration.
4.3 DIVERSETEXT-TO-IMAGEGENERATION
Ourmethodcangeneratediverseoutputsbyutilizingthecoarse-grainedfeature(f andf scales).
4 3
This capability enables our model to produce diverse images that correspond to text conditions.
Fig. 6 demonstrates that when generating images with a fixed text prompt utilizing feature scales
f andf ,ourmodelachieveshighgenerationdiversity. Italsoshowsthatf scaleoutputsexhibit
4 3 4
higher diversity, while f scale results demonstrate better consistency. In contrast, the generation
3
8TechnicalReport
e
la
c
s
ğ‘“4
A
M
U
P
e
la
c
s
ğ‘“3
A
M
U
P
prompt1 prompt2 prompt3 prompt4
prompt1:Painting the entire universe in nutshell, line art drawing, magical scene, highly detailed, soft
orange, mint green, soft blue, soft yellow, soft red, sharp outlines, sharp brush strokes, isolated.
prompt2:A beautiful blonde girl with futuristic wasp-inspired armour, compound eye, intricate design,
unreal engine, cinematic lighting.
prompt3:A girl with white hair holding a harfang owl in her arms, artwork by james gilleard,vibrant colours.
prompt4:Cluster of magic mushrooms in a dark lush green forest during a storm.
Figure7: Visualizationoftext-to-imagegenerationresultsfromPUMAfeaturescalesf (1visual
4
token)andf (4visualtokens).
3
Table3: ImageeditingevaluationonEmu-edittestbenchmark(Sheyninetal.,2024). 5-scaleMax
denotesselectingtheimagewiththehighestscoreamongthe5outputsandcomputestheaverage
maximumvalue.
Model CLIP-Iâ†‘ CLIP-Tâ†‘ DINOâ†‘
InstructPix2Pix(2023) 0.834 0.219 0.762
MagicBrush(2024a) 0.838 0.222 0.776
EMU-Edit(2024) 0.859 0.231 0.819
OmniGen(2024) 0.836 0.233 0.804
PUMA(f scale) 0.802 0.258 0.679
1
PUMA(f scale) 0.840 0.264 0.784
0
PUMA(5-scaleMax) 0.846 0.270 0.785
results of Emu2 (Sun et al., 2024b) show low diversity. For qualitative evaluation, Fig. 7 presents
visualizations of our modelâ€™s text-to-image generation with various prompts. For quantitative re-
sults,weevaluateourmodelontheMSCOCO30Kvalidationdataset(Linetal.,2014)andpresent
theCLIP-I,CLIP-T,andLPIPS inTab.2,whichtheformertwometricsmeasurestheconsistency
d
whileLPIPS measuresgenerationdiversity. Comparedwithrecentworks,ourmodeldemonstrates
d
superiorperformanceingenerationquality,diversity,andpromptrelevance.
4.4 IMAGEEDITING
To assess PUMAâ€™s image editing capabilities, we evaluated it on the Emu-Edit test benchmark
(Sheynin et al., 2024). Tab. 3 presents the results using CLIP-I, CLIP-T, and DINO (Caron et al.,
2021)scores. CLIP-IandDINOscoresmeasurethemodelâ€™sabilitytopreserveelementsfromthe
sourceimage, whileCLIP-Treflectstheconsistencybetweentheoutputimageandthetargetcap-
tion. Our results demonstrate that PUMA exhibits strong preservation ability, second only to the
current state-of-the-art model, EMU-Edit. Notably, PUMA achieves significantly better CLIP-T
9TechnicalReport
Inputimage PUMAğ‘“1scale Inputimage PUMAğ‘“0scale
åœ¨æ­¤å¤„é”®å…¥å…¬å¼ã€‚
Adda statue of a man. Turn tothe van gogh style. Replace the
woman with a child.
Figure8: Left: VisualizationsofPUMAâ€™simageeditingresult. Imageeditingutilizesf scalefea-
0
turetopreservethefine-graineddetailofinputimage. Right: VisualizationofPUMAâ€™sconditional
generationresults. â¶: canny-to-imagegeneration;â·: imageinpainting;â¸: imagecolorization.
Inputimage PUMAğ‘“ scale PUMAğ‘“ scale Inputimage PUMAğ‘“ scale PUMAğ‘“ scale
0 1 0 1
Replace the trees with palm trees. Turn the grayscale image into an image.
Figure9: Comparisonoff andf featurescalesfortasksrequiringprecisecontrollability.
0 1
Table4:Evaluationonmultimodalunderstandingbenchmarks.PUMAutilizesCLIP-Largeencoder
with224Ã—224input. Und. andGen. denoteâ€œunderstandingâ€andâ€œgenerationâ€,respectively.
Type Model #Params MMBâ†‘ MMEâ†‘ GQAâ†‘ VQAv2 â†‘ POPEâ†‘ Vizwizâ†‘
(test)
LLaVA-v1.5(2024a) 7B 64.3 1510.7 62.0 78.5 85.9 50.0
InstructBLIP(2023) 13B - 1212.8 49.5 - 78.9 33.4
Und.Only
Qwen-VL-Chat(2023) 7B - 1487.5 57.5 78.2 - 38.9
mPLUG-Owl2(2024b) 7B 64.5 1450.2 56.1 79.4 85.8 54.5
Emu(2023) 13B - - - 57.2 - -
NExT-GPT(2023) 7B 58.0 - - 66.7 - 48.4
SEED-X(2024b) 17B 75.4 1457.0 47.9 - 84.2 -
Und.andGen.
Chameleon(2024) 34B - - - 66.0 - -
Emu2-Chat(2024b) 40B - - 65.1 84.9 - 54.9
PUMA(Ours) 8B 68.9 1490.3 60.6 76.2 85.2 47.9
scores,evensurpassingthestate-of-the-artmodel. Thisindicatessuperioralignmentbetweenedited
imagesandtargetcaptions. Forqualitativeevaluation,Fig.8providesvisualizationsoftheediting
results,illustratingPUMAâ€™seffectivenessinimagemanipulationtasks.
4.5 CONDITIONALIMAGEGENERATION
We select a subset of canny-to-image, inpainting, and colorization tasks from the multigen-20M
dataset to train PUMAâ€™s conditional image generation ability. Fig. 8 demonstrates the conditional
generation results for these tasks. The f feature scale results provide the highest preservation of
0
imagedetails,particularlyfortaskslikeinpaintingandcolorization,whilethef scaleoffersbetter
1
overallvisualfidelitywithlimitedgenerationdiversity.
4.6 IMAGEUNDERSTANDING
WeevaluatePUMAâ€™simageunderstandingperformanceonseveralMLLMbenchmarks,including
MMB(Liuetal.,2023),MME(Fuetal.,2024),GQA(Hudson&Manning,2019),VQAv2(Antol
10TechnicalReport
et al., 2015), POPE (Li et al., 2023), and Vizwiz (Gurari et al., 2018). Tab. 4 presents the results
ofthisevaluation. DespitePUMAâ€™srelativelyfew8Bparametersandtheuseofanimageencoder
with224Ã—224resolutioninput,itdemonstratescompetitiveandoftensuperiorimageunderstand-
ingperformancecomparedtootherunifiedunderstandingandgenerationmodels.Notably,PUMAâ€™s
performance on some metrics even surpasses that of understanding-only baselines. This perfor-
mance can be attributed to PUMAâ€™s use of multi-granular continuous visual tokens as input to the
MLLM.Adetailedablationstudyexaminingtheimpactofdifferentscalefeaturesasinputonimage
understandingtasksisprovidedintheAppendix, offeringfurtherinsightsintotheeffectivenessof
PUMAâ€™smulti-granularapproach.
4.7 ABLATION
We conduct an ablation study to examine the impact of feature scale selection on tasks requiring
fine-grainedcontrollability.Fig.9comparestheoutputsoff andf featurescalesforimageediting
0 1
andcolorizationtasks. Theresultsdemonstratethatf scalefeaturesareinsufficientforpreserving
1
crucial image details, while f scale features maintain the necessary fine-grained information for
0
precisemanipulationtasks. MoreablationstudiesareintheAppendix.
5 CONCLUSION
Inthispaper,weintroducePUMA,anovelunifiedmulti-granularMLLMthatunifiesvariousgran-
ular tasks in visual generation and understanding. By leveraging multi-granular representations,
PUMAeffectivelyaddressesthechallengeofbalancingdiversityandcontrollabilityinimagegen-
eration tasks. Our approach demonstrates superior performance across a spectrum of visual tasks,
includingdiversetext-to-imagegeneration,imageediting,inpainting,colorization,conditionalgen-
eration, and understanding. PUMAâ€™s ability to adapt to varying granularity requirements within a
singleframeworkrepresentsasignificantadvancementinMLLMcapabilities. Thisworkopensup
newpossibilitiesformoreversatileandpowerfulmultimodalAIsystems,contributingtothebroader
goalofachievingartificialgeneralintelligenceinmultimodaldomains.
REFERENCES
StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZit-
nick,andDeviParikh. Vqa:Visualquestionanswering. InProceedingsoftheIEEEinternational
conferenceoncomputervision,pp.2425â€“2433,2015.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
JamesBetker,GabrielGoh,LiJing,TimBrooks,JianfengWang,LinjieLi,LongOuyang,Juntang
Zhuang,JoyceLee,YufeiGuo,etal. Improvingimagegenerationwithbettercaptions. Computer
Science.https://cdn.openai.com/papers/dall-e-3.pdf,2(3):8,2023.
TimBrooks,AleksanderHolynski,andAlexeiAEfros. Instructpix2pix: Learningtofollowimage
editinginstructions.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.18392â€“18402,2023.
LauraJannesBurger. Laion: Imagedata,ai,anddispossession. Masterâ€™sthesis,2023.
Mathilde Caron, Hugo Touvron, Ishan Misra, HerveÂ´ JeÂ´gou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of
theIEEE/CVFinternationalconferenceoncomputervision,pp.9650â€“9660,2021.
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.24185â€“24198,2024.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
BoyangLi,PascaleFung,andStevenHoi.Instructblip:Towardsgeneral-purposevision-language
modelswithinstructiontuning,2023.
11TechnicalReport
RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,Jianjian
Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and
creation. arXivpreprintarXiv:2309.11499,2023.
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,Xiawu
Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation
benchmarkformultimodallargelanguagemodels,2024.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang,HoraceHe,AnishThite,NoaNabeshima,etal. Thepile:An800gbdatasetofdiversetext
forlanguagemodeling. arXivpreprintarXiv:2101.00027,2020.
Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making
llamaseeanddrawwithseedtokenizer. arXivpreprintarXiv:2310.01218,2023.
Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: A
hybriddatasetforinstructionalimageediting. arXivpreprintarXiv:2405.04007,2024a.
YuyingGe,SijieZhao,JinguoZhu,YixiaoGe,KunYi,LinSong,ChenLi,XiaohanDing,andYing
Shan. Seed-x: Multimodalmodelswithunifiedmulti-granularitycomprehensionandgeneration.
arXivpreprintarXiv:2404.14396,2024b.
DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,and
Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.3608â€“3617,
2018.
DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.6700â€“6709,2019.
DiederikPKingma. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,2013.
JingYuKoh,DanielFried,andRussRSalakhutdinov.Generatingimageswithmultimodallanguage
models. AdvancesinNeuralInformationProcessingSystems,36,2024.
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei
Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint
arXiv:2408.03326,2024a.
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024b.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating
objecthallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
JiLin,HongxuYin,WeiPing,PavloMolchanov,MohammadShoeybi,andSongHan.Vila:Onpre-
trainingforvisuallanguagemodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.26689â€“26699,2024.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
DollaÂ´r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings,PartV13,pp.740â€“755.Springer,2014.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.26296â€“26306,2024a.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024b.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
12TechnicalReport
ILoshchilov. Decoupledweightdecayregularization. arXivpreprintarXiv:1711.05101,2017.
AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. Ocr-vqa: Visual
question answering by reading text in images. In 2019 international conference on document
analysisandrecognition(ICDAR),pp.947â€“952.IEEE,2019.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824,2023.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MuÂ¨ller, Joe
Penna,andRobinRombach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXivpreprintarXiv:2307.01952,2023.
Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Car-
los Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for
controllablevisualgenerationinthewild. arXivpreprintarXiv:2305.11147,2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748â€“8763.PMLR,2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684â€“10695,2022.
ChristophSchuhmann, RomainBeaumont, RichardVencu, CadeGordon, RossWightman, Mehdi
Cherti, TheoCoombes, AarushKatta, ClaytonMullis, MitchellWortsman, etal. Laion-5b: An
open large-scale dataset for training next generation image-text models. Advances in Neural
InformationProcessingSystems,35:25278â€“25294,2022.
ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,AmitZohar,OronAshual,DeviParikh,
andYanivTaigman.Emuedit:Preciseimageeditingviarecognitionandgenerationtasks.InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pp.8871â€“
8879,2024.
KeqiangSun,JuntingPan,YuyingGe,HaoLi,HaodongDuan,XiaoshiWu,RenruiZhang,Aojun
Zhou,ZipengQin,YiWang,etal. Journeydb: Abenchmarkforgenerativeimageunderstanding.
AdvancesinNeuralInformationProcessingSystems,36,2024a.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
JingjingLiu,TiejunHuang,andXinlongWang. Generativepretraininginmultimodality. arXiv
preprintarXiv:2307.05222,2023.
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context
learners. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.14398â€“14409,2024b.
ZinengTang,ZiyiYang,ChenguangZhu,MichaelZeng,andMohitBansal. Any-to-anygeneration
viacomposablediffusion. AdvancesinNeuralInformationProcessingSystems,36,2024.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024.
KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang.Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha
Akula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-1: Afullyopen,
vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024.
13TechnicalReport
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimotheÂ´e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-
modalllm. arXivpreprintarXiv:2309.05519,2023.
Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting
Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint
arXiv:2409.11340,2024.
HanrongYe,De-AnHuang,YaoLu,ZhidingYu,WeiPing,AndrewTao,JanKautz,SongHan,Dan
Xu,PavloMolchanov,etal. X-vila: Cross-modalityalignmentforlargelanguagemodel. arXiv
preprintarXiv:2405.19335,2024a.
QinghaoYe,HaiyangXu,JiaboYe,MingYan,AnwenHu,HaoweiLiu,QiQian,JiZhang,andFei
Huang. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwithmodalitycollabo-
ration.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.13040â€“13051,2024b.
KaiZhang, LingboMo, WenhuChen, HuanSun, andYuSu. Magicbrush: Amanuallyannotated
datasetforinstruction-guidedimageediting.AdvancesinNeuralInformationProcessingSystems,
36,2024a.
PanZhang,XiaoyiDong,YuhangZang,YuhangCao,RuiQian,LinChen,QipengGuo,Haodong
Duan,BinWang,LinkeOuyang,etal.Internlm-xcomposer-2.5:Aversatilelargevisionlanguage
modelsupportinglong-contextualinputandoutput. arXivpreprintarXiv:2407.03320,2024b.
RenruiZhang, JiamingHan, ChrisLiu, PengGao, AojunZhou, XiangfeiHu, ShilinYan, PanLu,
HongshengLi,andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-
initattention. arXivpreprintarXiv:2303.16199,2023a.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586â€“595,2018.
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.
Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint
arXiv:2306.17107,2023b.
ChuntingZhou,LiliYu,ArunBabu,KushalTirumala,MichihiroYasunaga,LeonidShamis,Jacob
Kahn, XuezheMa, LukeZettlemoyer, andOmerLevy. Transfusion: Predictthenexttokenand
diffuseimageswithonemulti-modalmodel. arXivpreprintarXiv:2408.11039,2024.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592,2023a.
JinguoZhu,XiaohanDing,YixiaoGe,YuyingGe,SijieZhao,HengshuangZhao,XiaohuaWang,
andYingShan. Vl-gpt:Agenerativepre-trainedtransformerforvisionandlanguageunderstand-
ingandgeneration. arXivpreprintarXiv:2312.09251,2023b.
14TechnicalReport
A PUMA TRAINING
A.1 VISUALDECODINGTRAINING
A.1.1 DATASETDETAILS
Fortrainingtheimagedecodingprocess,weleveragethreelarge-scaledatasets: Laion-2B(Schuh-
mannetal.,2022),Laion-Aesthetics(Burger,2023),andJourneyDB(Sunetal.,2024a). Toensure
high-quality generation capabilities, we apply a resolution-based filtering criterion, selecting only
imageswithresolutionsof512Ã—512pixelsorlarger. Weonlyusecentercropasthedataaugmen-
tationmethod.
A.1.2 TRAININGSETTINGS
Wetrainfivededicatedimagedecodersforthef ,f ,f ,f ,andf scalefeaturesrespectively. The
0 1 2 3 4
imageencoderisthefrozenCLIP-Limageencoder(Radfordetal.,2021). Eachimagedecoderis
initializedfromtheSDXLmodel. TheVAE(Kingma,2013)remainsfrozenthroughoutthetraining
process. The corresponding image features are input to the diffusion model through the cross-
attention mechanism, replacing the original text embedding input. We train the decoders using
AdamWoptimizer(Loshchilov,2017)withamaximumlearningrateof8e-5,usinglinearlearning
ratedecayandagradientclippingvalueof1.0. Thetrainingbatchsizeis1,024. Thetrainingsteps
for the five features are 40,000, 30,000, 20,000, 15,000, and 10,000 respectively, with features
containingmorevisualtokensusinglongertrainingsteps. Weusenoiseoffvalueof0.1andrandom
dropof10%oftheinputimagetoblankimageforclassifier-freeguidance.
A.2 MLLMTRAINING
A.2.1 TRAININGOBJECTIVE
PUMAemploysaunifiedframeworkwithsupervisiononbothtexttokensandimagefeatures. For
text tokens, we use cross-entropy classification loss, while for image features, we adopt MSE re-
gressionloss.Tobalancethecontributionoftextandimageoutputs,weapplyalossratioof0.02for
textand1.0forimagefeatures. Withintheimagefeatureregressionloss,weusedifferentratiosfor
theprogressivelygenerated5scalesofimagefeatures(f ,f ,f ,f ,andf ),withratiosof1024.0,
4 3 2 1 0
512.0, 64.0, 8.0, and 1.0 respectively. This scaling compensates for the varying number of tokens
at each feature scale, with larger ratios for scales with fewer tokens. The training loss objective
remainsconsistentacrossboththepretrainingandinstructiontuningphases.
A.2.2 PRETRAININGDATASETDETAILS
DuringPUMAâ€™spretrainingphase,weutilizeadiversesetofdatasetsincludingLaion-2B(Schuh-
mannetal.,2022),Laion-Aesthetics(Burger,2023),GRIT(Pengetal.,2023),ThePile(Gaoetal.,
2020),OCR-VQA-200K(Mishraetal.,2019),andLLaVAR(Zhangetal.,2023b). Fortheimage-
text pair data in Laion-2B, Laion-Aesthetics, and GRIT, we randomly assign 50% of the samples
to text-to-image training and 50% to image-to-text training, fostering both image generation and
understanding capabilities. We employ center crop as the primary image augmentation technique.
To train on the GRIT dataset for object grounding, we append 224 additional position tokens to
the MLLMâ€™s codebook, representing object positions with bounding box coordinates [x min,
y min, x max, y max]. We construct the training sequences by appending the tokens <s>
and </s> to denote the beginning and end of each sequence. At the beginning and end of each
image feature sequence, we include the special tokens [IMG] and [/IMG] to indicate the visual
position.
A.2.3 PRETRAININGSETTINGS
Weconductpretrainingfor100KstepsusingtheAdamWoptimizerwithabatchsizeof2048. The
maximum learning rates are set to 1e-4 for the projector and 3e-5 for the LLaMA backbone. We
employa2,000-stepwarm-upperiod,cosinelearningratedecay,andgradientclippingat5.0during
pretraining. Tooptimizememoryusageandcomputationalefficiency,trainingisacceleratedusing
DeepSpeedZeROStage3.Theentirepretrainingprocessiscarriedouton256NVIDIAV100GPUs
overaperiodof10days.
15TechnicalReport
A.2.4 INSTRUCTTUNINGSETTINGS
High-qualityText-to-ImageGeneration:WeutilizeLaion-Aesthetics(Burger,2023)andJourneyDB
(Sunetal.,2024a)withadataratio1:1toinstructtunethetext-to-imagegenerationmodelbasedon
thepreviouspretrainingcheckpoint. Weusetrainingbatchsize2048andtrainfor20,000stepswith
themaxlearningrate1e-5,warmup1,000steps,andcosinelearningratedecay. Randomcropwith
fixedaspectratioisadoptedastheimageaugmentation.
Precise Image Manipulation: We train the image manipulation task with SEED-Edit Ge et al.
(2024a). It contains seven different operations: background alteration, comprehensive image
changes,stylealteration,objectremoval,objectaddition,localizedmodifications,andcolor/texture
alterations. We train with batch size 1024 and train for 10,000 steps. The max learning rate is
1e-5, warm-up is 500 steps, and cosine learning rate decay is adopted. We apply random crop
withfixedaspectratioontheaccordinglyinputimageandoutputimage. Thesequenceoftheimage
manipulationsampleislikeâ€œ<s>[IMG]embedding of origin image[/IMG]instruct
editing prompt[IMG]embedding of edited image[/IMG]</s>â€.
Conditional Image Generation: We train on the subset of MultiGen-20M dataset (Qin et al.,
2023) including canny-to-image, image inpainting, and colorization. We use the training batch
size 1,024 and train for 20,000 steps. The max learning rate is 1e-5, warm-up is 500 steps, and
cosine learning rate decay is adopted. We apply center crop as the image augmentation. The
sequence of the conditional image generation is like â€œ<s>[IMG]embedding of origin
image[/IMG]instruct conditional generation prompt[IMG]embedding of
edited image[/IMG]</s>â€. Theâ€œinstruct conditional generation promptâ€
containsthecaptionofthetargetimageandwitha50%probabilitycontainthetaskinstructionlike
â€œPlease convert the canny image to a natural imageâ€.
ImageUnderstanding: Wetrain imageunderstandingtaskon thesubsetof LLaVA-OneVision (Li
et al., 2024a) and Cambrain (Tong et al., 2024). Data about math/reasoning and cross-duplicated
data in the two datasets are removed. We train with the batch size 512 and train all data for 1
epoch. The max learning rate is 1e-5 with the warm-up 500 steps. Cosine learning rate decay is
adopted. We apply resizing as the image augmentation. Supervision is only applied to the output
text tokens. We use the system message â€œA chat between a curious user and an
artificial intelligence assistant. The assistant gives helpful,
detailed, and polite answers to the userâ€™s questions.â€
B EVALUATION DETAILS
B.1 IMAGERECONSTRUCTIONEVALUATION
Toevaluatethereconstructionperformanceofdifferentscalesoffeaturesandourbaselines,weuse
the ImageNet validation set, comprising 50,000 images. Each image is resized to a rectangular
shapebeforebeinginputintoeachimageencoder. Weassessreconstructionprecisionbycomputing
PSNR andLPIPS ,whichmeasurethedifferencebetweenthereconstructedimageandtheoriginal
r r
image.
Giventheinherentrandomnessinthedecoders,wemeasurereconstructiondiversitybyreconstruct-
ingeachoriginalimagetwiceusingdifferentrandomseeds. WethencalculatePSNR andLPIPS
d d
toquantifythedifferencebetweenthesetworeconstructedimages. Higherdiversityisbeneficialfor
downstreamtaskssuchastext-to-imagegeneration.
For PSNR and LPIPS evaluations, we use a resolution of 256Ã—256 to align with the evaluation
settings in previous works. For LPIPS evaluation specifically, we employ AlexNet as the feature
extractor.
B.2 TEXT-TO-IMAGEGENERATIONEVALUATION
We evaluate text-to-image generation on the COCO 30K validation set (Lin et al., 2014). We use
CLIP-IandCLIP-Tscorestomeasuretheconsistencybetweenthegeneratedimageandtheground
truthimageandcaption,respectively.CLIP-Base-32servesasthefeatureextractorforthesemetrics.
Toassessgenerationdiversity,wecalculateLPIPS betweentwoimagesgeneratedusingthesame
d
inputpromptbutdifferentrandomseeds.TheLPIPS measurementdetailsareconsistentwiththose
d
describedinSec.B.1.
16TechnicalReport
Table 5: Ablation of different visual token input on image understanding. The experiments are
conductedonLLaVA-v1.5settingwithCLIP-Large-224visualencoder.
Visualtokentype Tokennumber MMBâ†‘ MMEâ†‘ GQAâ†‘ VQAv2 â†‘
(test)
f 1 56.8 1252.6 0.0 64.1
4
f 4 58.3 1285.5 0.0 67.0
3
f 16 61.5 1403.0 46.6 71.1
2
f 64 63.6 1400.8 58.4 74.4
1
f 256 65.4 1464.9 58.8 76.9
0
f -f 341 65.1 1445.5 61.0 76.9
4 0
B.3 IMAGEEDITINGEVALUATION
We evaluate image editing performance on the Emu-Edit benchmark Sheynin et al. (2024). To
assesseditingquality,weadoptCLIP-I,CLIP-T,andDINOscores. CLIP-IandDINOCaronetal.
(2021)scoresmeasurethemodelâ€™sabilitytopreserveelementsfromthesourceimage,whileCLIP-
Treflectstheconsistencybetweentheoutputimageandthetargetcaption. FortheDINOscore,we
employDINO-Small-16asthefeatureextractor.
B.4 IMAGEUNDERSTANDINGEVALUATION
For image understanding tasks, we use the same evaluation setting as LLaVA-v1.5 (Liu et al.,
2024a).Duringevaluation,weusethesystemmessageâ€œA chat between a curious user
and an artificial intelligence assistant. The assistant gives
helpful, detailed, and polite answers to the userâ€™s questions.â€
C ABLATION OF DIFFERENT SCALE FEATURES AS INPUT ON IMAGE
UNDERSTANDING TASK
Given that PUMA adopts a unified multi-granular image feature as both input and output for the
MLLMbackbone,weconductedanablationstudytoinvestigatetheinfluenceofdifferentscalesof
imagefeatureinputonimageunderstandingtasks. Forafaircomparison,weadoptedthestandard
LLaVA-1.5-7Bpretrainingandfinetuningsetting,onlychangingtheimageencodertoa224-input
CLIP-Largewithdifferentgranularitiesoffeatures.
Tab.5presentstheresultsofthisablationstudy.Thefindingsdemonstratethatfiner-grainedfeatures
generally lead to better performance in image understanding tasks. Notably, utilizing all image
featuresfromf tof (thePUMAsetting)achievescomparableperformancetousingall256visual
4 0
tokensofthefinestscale(f ). Theseresultsvalidatethattheunifiedvisualinputandoutputformat
0
ofPUMAprovidesarobustfoundationofvisualfeaturesforimageunderstandingtasks,effectively
balancingperformanceacrossdifferentgranularities.
D SELECTION OF 5 SCALE FEATURES IN TEXT-TO-IMAGE GENERATION
PUMAgeneratesimagesat5granularitylevels,allowinguserstoselecttheoutputthatbestmeets
theirrequirements. Inourevaluationofdiversetext-to-imagegeneration,weproduce5imageout-
putsforeachinputprompt,correspondingtothe5featurescales. Toassessperformance,weselect
theimagewiththehighestCLIP-IandCLIP-Tscoresamongthe5outputsandcomputetheaverage
maximumvalue. Tab.6presentstheCLIP-IandCLIP-Tscoresforeachofthe5featurescales.
The results demonstrate that different granularity levels excel in various aspects of image genera-
tion. Notably,theabilitytoselectthebestoutputfrommultiplescales(PUMA5-scaleMax)yields
significantly improved CLIP-I and CLIP-T scores compared to any single scale, highlighting the
advantageofPUMAâ€™smulti-granularapproach.
17TechnicalReport
PUMA(ğ‘“4)-1token PUMA(ğ‘“3)-4tokens PUMA(ğ‘“2)-16tokens PUMA(ğ‘“1)-64tokens PUMA(ğ‘“0)-256tokens
Generationprompt:Hyper realistic happy steampunk chibi girl wearing a pink hoodie with apeton white background.
Generationprompt:Beautiful portrait by J.c. Leyendecker, beautiful lighting, Victorian Female Hunter, Fantasy.
Figure10: VisualizationofPUMAtext-to-imageoutputsacrossfivescalefeaturesgiventhegener-
ationprompt.
Table6: CLIP-IandCLIP-TscoresonMSCOCO30Kvalidationsetwithdifferentfeaturescales.
Model Tokennum. CLIP-Iâ†‘ CLIP-Tâ†‘
PUMA(f scale) 1 0.699 0.295
4
PUMA(f scale) 4 0.703 0.300
3
PUMA(f scale) 16 0.703 0.301
2
PUMA(f scale) 64 0.693 0.299
1
PUMA(f scale) 256 0.621 0.280
0
PUMA(5-scaleMax) - 0.736 0.317
E QUALITATIVE RESULTS OF TEXT-TO-IMAGE GENERATION ON FIVE
SCALE FEATURES
Inthetext-to-imagegenerationtask,PUMAproducesfivedistinctimagescorrespondingtothefive
featurescales,allderivedfromasingleinputgenerationprompt.Fig.10presentssamplesofoutputs
acrossthesefivescalesforgivengenerationprompts.
F MORE QUALITATIVE RESULTS
Wepresentmorequalitativecasesforimagereconstruction,diversetext-to-imagegeneration,edit-
ing,andconditionalimagegeneration,asshowninFigures11to15.
18TechnicalReport
Original image ğ· 0 ğ‘“ 0 ğ· 1 ğ‘“ 1 ğ· 2 ğ‘“ 2 ğ· 3 ğ‘“ 3 ğ· 4 ğ‘“ 4
Fine-grained reconstruction Semantic-guided generation
Figure 11: More visualizations on multi-granular visual decoding from fine-grained to coarse-
grainedgranularity.
19TechnicalReport
Originalimage Reconstructedimage Originalimage Reconstructedimage
Figure12: Morevisualizationsonfine-grainedimagereconstructionwithf scalefeature.
0
20TechnicalReport
e
la
c
s
ğ‘“4
A
M
U
P
e
la
c
s
ğ‘“3
A
M
U
P
prompt1 prompt2 prompt3 prompt4
e
la
c
s
ğ‘“4
A
M
U
P
e
la
c
s
ğ‘“3
A
M
U
P
prompt5 prompt6 prompt7 prompt8
prompt1:Winter Queen princess baby girl, blond hair, blue eyes, pink lips, Walt Disney beautiful smiling, beautiful
character sweet and delicate, stickers, lovely frame, beautiful face, big eyes beautiful.
Prompt2: 1972 porsche, ginza, bright light, hyper realistic, magazine quality, cinematic lighting, neon ads in
background, vertical Japanese signs.
prompt3:a cute totoro like tortoise character, bold colors, amiga game, isometric, pixel art, 8K.
prompt4:Film still of rabbit sitting at the counter of an art-deco loungebar, drinking whisky from a tumbler glass, in
the style of "Blade Runner", velvety, soft lights, long shot, high quality photo.
prompt5: a container designed compound built for a group home styled living space. 6000 SQ ft with 7 bedrooms and
1 adult suite. give the view landscape style with a smilling pool in the front.
prompt6: Open valley from mountains, aspen, hyper-realistic.
prompt7: Cartoon, pixar style, the planet hamburger, line art drawing, magical scene, highly detailed, soft orange, soft
blue, soft pink, soft red, sharp outlines, sharp brush strokes.
prompt8: Beautiful colorful flower motif graphic, in the shape of an elegant flamingo in the style of Hayao Miyazaki,
front view.
Figure13: Morevisualizationsontext-to-imagegenerationutilizingf andf scales.
4 3
21TechnicalReport
Insert a small bird Eliminate the number 69 Replace the cactus Alter this photo to
perched on a tree branch. from the image. with palm trees. Ghibli Studio style.
Figure14: Morevisualizationsonimageediting.
Inputimage Generatedimage Inputimage Generatedimage
Figure15: Morevisualizationsonconditionalimagegeneration.
22