How Numerical Precision Affects Mathematical Reasoning Capabilities of
LLMs
GuhaoFeng*1,KaiYang*1,YuntianGu1,XinyueAi1,ShengjieLuo1,
JiachengSun2,DiHe1,ZhenguoLi2,LiweiWang1
1PekingUniversity 2HuaweiNoahâ€™sArkLab
Abstract address these challenges, various strategies have
been proposed, including carefully designed
DespitetheremarkablesuccessofTransformer- promptingstrategies(Weietal.,2022b;Yamauchi
basedLargeLanguageModels(LLMs)across
etal.,2023;Imanietal.,2023)andinference-based
various domains, understanding and enhanc-
searching method (Kang et al., 2024; Wu et al.,
ingtheirmathematicalcapabilitiesremainsa
2024a; Snell et al., 2024; Brown et al., 2024).
significant challenge. In this paper, we con-
However, a comprehensive understanding of the
ductarigoroustheoreticalanalysisofLLMsâ€™
mathematical abilities, with a specific focus intrinsiclimitationsthatrestrictthemathematical
ontheirarithmeticperformances. Weidentify reasoningcapabilitiesofLLMsremainselusive.
numericalprecisionasakeyfactorthatinflu- In principle, mathematical reasoning, built on
encestheireffectivenessinmathematicaltasks.
basicarithmeticaloperations,requirestheaccurate
OurresultsshowthatTransformersoperating
computationofintermediateresultsthroughoutthe
with low numerical precision fail to address
reasoningprocess(Bubecketal.,2023;Leeetal.,
arithmetictasks,suchasiteratedadditionand
2024). Thereexistworks(Fengetal.,2023;Yang
integer multiplication, unless the model size
growssuper-polynomiallywithrespecttothe et al., 2024) exploring the arithmetic capabilities
input length. In contrast, Transformers with ofLLMswithChainofThoughtprompting(Wei
standard numerical precision can efficiently et al., 2022b). However, unlike the tokenization
handle these tasks with significantly smaller methods of modern LLMs (Dubey et al., 2024;
modelsizes. Wefurthersupportourtheoretical
OpenAI, 2023), where each digit in a number is
findingsthroughempiricalexperimentsthatex-
tokenizedindividually,theseworkstreatawhole
ploretheimpactofvaryingnumericalprecision
numberasonetoken. Underthisassumption,each
onarithmetictasks,providingvaluableinsights
distinctnumberoccupiesauniquepositioninthe
forimprovingthemathematicalreasoningca-
pabilitiesofLLMs. vocabulary,leadingtoanessentialmismatchwith
practicalimplementations. Moreover,recentstud-
1 Introduction ies have demonstrated that LLMs operating with
reducednumericalprecision(e.g.,int4)exhibita
Transformer-based Large Language Models
significantdeclineinperformanceonmathematical
(LLMs), such as GPT (OpenAI, 2023), Claude
tasks(Jinetal.,2024;Marchisioetal.,2024).
(Anthropic, 2024), and LLaMA (Dubey et al.,
Inthispaper,weprovidearigoroustheoretical
2024), have achieved impressive performance
investigationofthearithmeticalabilitiesofLLMs
across a broad range of natural language tasks
under the autoregressive paradigm. Specifically,
(Zhu et al., 2024; Basyal and Sanghvi, 2023;
we follow the tokenization method of modern
Shao et al., 2023). Despite the great success,
LLMs, allowing the models to process and
significant challenges remain when applying
generate numbers digit by digit. Under these
LLMs to mathematical problem-solving. Unlike
assumptions, we identify numerical precision
many typical NLP tasks, which often depend on
as a key factor influencing their performance in
pattern recognition and statistical correlations
arithmetical tasks. Our analysis focuses on three
(Bleietal.,2003),mathematicalreasoningrequires
elementary arithmetic tasks: integer addition,
rigorous logical deduction in a specific order
iteratedaddition,andintegermultiplication,which
(Frieder et al., 2024; Bubeck et al., 2023). To
serve as elementary building blocks in solving
*Equalcontribution. complexreal-worldmathproblems.
1
4202
tcO
71
]GL.sc[
1v75831.0142:viXraArithmeticTasks StandardPrecision LowPrecision
IntegerAdditionADD(n,p) Constant O(n2)
IteratedAdditionIterADD(n,k,p) Constant Super-polynomial
IntegerMultiplicationMul(n,l,p) O(n2) Super-polynomial
Table1: Themodelsizew.r.t. theinputsizerequiredforvariousarithmetictasksonbounded-depthTransformers,
underbothstandardandlownumericalprecision. Bluedenotesthesmallmodelsize,andredrepresentsthelarge
modelsize.
Toelucidatetheroleofnumericalprecision,we tionedarithmetictasks,systematicallyexamining
first examine the expressiveness of Transformers howproblemsizeandnumericalprecisionimpact
operating under low precision, such as int8 and their capabilities. Furthermore, we also conduct
int4. Weestablishfoundationalimpossibilityre- experimentsonLLAMA3.1(Dubeyetal.,2024)
sultsforlow-precisionTransformers,demonstrat- to evaluate the performance of these arithmetic
ingthatsuchmodelsrequiresuper-polynomialsize tasksunderdifferentnumericalprecision. Ourem-
with respect to input length to solve iterated ad- piricalresultsdemonstratethatbothlow-precision
dition and integer multiplication (Theorems 4.2 andstandard-precisionTransformersperformade-
and 4.3). Our proofs, grounded in complexity quatelyontheintegeradditiontask. However,as
theory(AroraandBarak,2009;Razborov,1987), taskcomplexityincreasesâ€”particularlyiniterated
show that this limitation arises from the inability additionandintegermultiplicationâ€”thedecrease
ofindividualneuronstostoreintermediateresults inprecisioninTransformersresultsinsignificant
duringarithmeticcomputations. Asaresult,asig- performance degradation. These findings align
nificantlylargernumberofneuronsisrequiredto with our theoretical predictions and offer practi-
distributethecomputationandavoidoverflow. cal guidance for enhancing LLM performance in
mathematicalreasoningtasks.
Wefurtherdemonstratethatincreasingnumeri-
calprecisionisessentialtoovercomingthislimita-
tion: as the precision increases, the model size
required to solve these arithmetic problems de- 2 Preliminary
creases dramatically. In particular, we prove that
a bounded-depth Transformer with standard pre-
cision, suchasfloat32, cansolveallthreetasks An autoregressive Transformer, or decoder-only
efficiently,requiringconstantmodelsizeforinte- Transformer(Radfordetal.,2019;Daietal.,2019),
geranditeratedaddition(Theorems5.1and5.2), is a neural network designed to model sequence-
and quadric size for integer multiplication (The- to-sequence mappings. For an input sequence s
orem 5.3). At standard precision, it is sufficient of length n, each input token s i (for i âˆˆ [n]) is
(0)
forLLMstoaccuratelystorerelativelylargenum- transformed into a d-dimensional vector x =
i
berswithinindividualneurons,enablingtheTrans- Embed(s ) + p âˆˆ Rd, where Embed(Â·) repre-
i i
formertoperformarithmetictasksmoreeffectively. sentsthetokenembeddingfunction,andp denotes
i
Theseresultsnotonlyunderscorethecriticalroleof learnablepositionalembeddings. Themodelthen
numericalprecisioninenhancingthetheoreticalca- consistsofLTransformerblocks,eachfollowing
pabilitiesofLLMsformathematicaltasksbutalso theform:
offerpracticalinsights. Whilelow-precisionmod-
elsmayprovidecomputationalefficiency,ensuring
(cid:16) (cid:17)
adequatenumericalprecisionisessentialforappli- h(l) = x(lâˆ’1) +Attn(l) x(lâˆ’1) ;{x(lâˆ’1) : j â‰¤ i} ,
i i i j
cationsinvolvingcomplexmathematicalreasoning.
AsummaryofourresultsispresentedinTable1.
x(l)
=
h(l) +FFN(l)(h(l)
),
i i i
In addition to theoretical analysis, we conduct
extensiveexperimentstoempiricallyvalidateour
conclusions. First,weevaluatetheperformanceof where l âˆˆ [L]. Here, Attn(l) and FFN(l) denote
Transformerstrainedfromscratchontheaforemen- the multi-head self-attention layer and the feed-
2Integer Addition Iterated Addition Integer Multiplication
Input 1 (base ğ‘=2): Input 1 (base ğ‘=2): Input 1 (base ğ‘=2):
10+11= 10+1010+1110= 11Ã—11111=
Output 1: Output 1: Output 1:
101 11010 1011101
Input 2 (base ğ‘=10): Input 2 (base ğ‘=10): Input 2 (base ğ‘=10):
19+987= 44055+18754+905= 382Ã—3672=
Output 2: Output 2: Output 2:
1006 63714 1402704
Figure 1: Examples for three elementary arithmetic tasks we consider in this paper: integer addition, iterated
addition,andintegermultiplication.
forwardnetworkofthel-thTransformerblock: integerwithndigitsisexpressedas(x Â·Â·Â·x ) ,
nâˆ’1 0 p
where the sequence of digits x = [x ,Â·Â·Â· ,x ]
nâˆ’1 0
H
Attn(l)(x,S) =
(cid:88)(cid:16) W(l,h)(cid:17)âŠ¤
Â·H(l,h)(x,S),
istheinputtotheTransformermodel. Eachdigit
O x istreatedasanindividualtokenaccordingtothe
i
h=1
modelâ€™stokenizationscheme. Duringgeneration,
H(l,h)(x,S) =
the Transformer also produces the outcome digit
(cid:16) (cid:17)
softmax (W(l,h) z)âŠ¤(W(l,h) x) W(l,h) z, by digit, respecting the established tokenization
zâˆˆS K Q V
structure.
FFN(l)(x) = W(l) Ïƒ(W(l) x),
2 1 Unlikepriorworks(Fengetal.,2023;Yangetal.,
2024),whichrepresententireintegersassingleto-
where W Q(l,h) ,W K(l,h) ,W V(l,h) ,W O(l,h) âˆˆ RâŒˆ HdâŒ‰Ã—d
kens, we treat each digit of an integer as an indi-
are the query, key, value, and output matrices of
vidualtoken,allowingTransformerstoprocessand
the h-th head in the l-th layer. The weight ma-
generate numbers digit by digit. Given that most
tricesinthefeed-forwardnetworkaredenotedas
modernLLMstokenizelargenumbersinthisman-
W(l) ,W(l) âˆˆ RdÃ—d. TheactivationfunctionÏƒ is
1 2 ner, our approach not only aligns with practical
chosentobeGeLU(HendrycksandGimpel,2016),
tokenizationstrategiesbutalsomitigatestheissue
followingtheworkof(Radfordetal.,2019;Devlin
ofinflatedvocabularysize.
etal.,2019).
(M)
Thecomputedembeddingx isthenusedto IntegerAddition. Leta = (a Â·Â·Â·a ) and
n n1âˆ’1 0 p
predictthenexttokens ,whichisconcatenated b = (b Â·Â·Â·b ) denote two integers encoded
n+1 n2âˆ’1 0 p
to the input to continue the sequence generation in base-p. The input sequence is constructed by
process. This process terminates when an <EOS> concatenating the tokenized representations of a
tokenisgenerated. Furtherdiscussionsonrelated andb,separatedbytheadditionoperatorâ€˜+â€™. The
workarelistedinAppendixA. taskistogeneratethesums = (s Â·Â·Â·s ) ,which
n 0 p
containsn+1digitswheren = max(n ,n )and
1 2
3 ProblemSetup
representsthebase-psumofaandb,outputdigit-
by-digit.
Thispaperexploresthearithmeticreasoningcapa-
bilitiesofLLMsbyfocusingonthreeelementary
Iterated Addition. Consider k integers a =
arithmetictasks: integeraddition,iteratedaddition, 1
(a Â·Â·Â·a ) , Â·Â·Â· , a = (a Â·Â·Â·a )
andintegermultiplicationundertheautoregressive 1,n1âˆ’1 1,0 p k k,n kâˆ’1 k,0 p
wheren = max{n ,Â·Â·Â· ,n }. Theinputsequence
paradigm. Below,wedefinetheintegerrepresenta- 1 k
consistsofthetokenizedrepresentationsofthese
tionsusedthroughoutthestudyandprovideformal
integers, concatenated and separated by the addi-
descriptionsforeachtask.
tion operator â€˜+â€™. The Transformer model must
Integer Representations. We assume all inte- outputthesums = (s
n+âŒˆlog
kâŒ‰âˆ’1Â·Â·Â·s 0) p,which
p
gersarenon-negativeandrepresentedinthebase-p containsn+âŒˆlog kâŒ‰digitsandrepresentsthebase-
p
form,wherep â‰¥ 2isafixedbase. Specifically,an psumofthek integers.
3IntegerMultiplication. Intheintegermultiplica- han,1996)orfixedpointformats. Thisconfigura-
tiontask,thegoalistocomputetheproductoftwo tionmirrorsmanypracticaldeploymentscenarios,
integers,truncatedtoapre-specifiedlengthl. Let in which LLMs often employ reduced-precision
a = (a Â·Â·Â·a ) and b = (b Â·Â·Â·b ) be formatssuch as float8, int8, or even int4, par-
n1âˆ’1 0 p n2âˆ’1 0 p
twointegers,eachnolongerthanndigitsinbase-p. ticularlyduringinference(Hanetal.,2015). Given
Theinputsequenceisconstructedbyconcatenating thatthesemodelstypicallyprocessinputsequences
thetokenizedrepresentationsofaandb,separated comprising thousands of tokens, it is reasonable
bythemultiplicationoperatorâ€˜Ã—â€™. Theobjective andrealistictoassumethatthenumericalprecision
is to generate the lowest l digits (l â‰¤ 2n) of the remainsfixedatasmallconstant,independentof
products = (s Â·Â·Â·s ) inbase-parithmetic. sequencelength. Undertheconstant-precisionset-
lâˆ’1 0 p
ting,weexaminetheexpressivenessoftheTrans-
Remark 3.1. We consider a generalized case of
formermodelinelementaryarithmeticproblems.
integer multiplication where overflow may occur
if the result exceeds the given digit length. Stan- Theorem 4.1. For any fixed integers p, there ex-
dardintegermultiplicationisaspecialcaseofthis istconstant-precisionTransformerswithconstant
frameworkwhenl = 2n. depthLandhiddendimensiond = O(n2)thatcan
solvetheADD(n,p)task.
Figure1illustratesexamplesofthethreetasks.
Integeradditionisthesimplestoftheseoperations Theorem 4.1 suggests that the bounded-depth
and can be viewed as a specific instance of iter- Transformerswithreasonablehiddendimensions
atedaddition. Furthermore,integermultiplication are capable of solving the integer addition task.
inherentlyinvolvesthesummationofseveralinter- However, as we will show in subsequent theo-
mediateproducts. Consequently,wepresentthese rems,constant-precisionTransformersexhibitpro-
tasksinincreasingorderofcomplexity. Inthesub- nouncedlimitationswhenconsideringmorecom-
sequentsections,weusethenotationsADD(n,p) plexarithmeticproblems.
to denote n-digit addition in base-p arithmetic, Theorem 4.2. For any integers p and L, and for
IterADD(n,k,p)fortheiteratedadditionofkinte- anypolynomialf,thereexistproblemscalesnand
gerswithndigitseachinbase-p,andMUL(n,l,p) k such that no constant-precision autoregressive
for the multiplication of two n-digit integers in TransformerwithLlayersandhiddendimension
base-p,truncatedtoldigits. d < f(n,k)cancorrectlysolvetheIterADD(n,p)
task.
4 Low-PrecisionTransformersStruggle
Theorem 4.3. For any integers p and L, and for
withBasicArithmeticTasks
anypolynomialf,thereexistproblemscalesnand
Recent studies (Marchisio et al., 2024; Jin et al., l such that no constant-precision autoregressive
2024)haveshownthatLLMsoperatingunderlow- TransformerwithLlayersandhiddendimension
precision constraints encounter significant chal- d < f(n,l) can correctly solve the MUL(n,l,p)
lengesinperformingbasicmathematicaltasks. In task.
thissection,weexaminetheexpressivelimitations What accounts for this limitation? As pre-
ofTransformersundersuchconstraintsandseekto sentedinAppendixD,ourproofisgroundedincir-
explain the sharp decline in their arithmetical ca- cuitcomplexitytheory. Bymodelingtheconstant-
pabilities. Specifically,wedemonstratethatTrans- precisionTransformerasacomputationalcircuit,
formers restricted to low-precision arithmetic ex- we rigorously analyze its expressive limitations
hibitsubstantialdifficultyinsolvingevenelemen- throughthelensofcircuitcomplexity(Fengetal.,
taryarithmeticproblems. 2023;Merrilletal.,2022;MerrillandSabharwal,
Toformalizetheselimitations,webuildonthe 2023;Lietal.,2024). Specifically,Lietal.(2024)
frameworkintroducedbyLietal.(2024)andutilize provesthattheexpressivenessofconstant-precision
thesettingofaconstant-precisionTransformer Transformers with polynomial size and bounded
(SeeformaldefinitioninAppendixB).Inthisset- depthisupper-boundedbythecomputationcom-
ting,theinternalstatesofthemodelâ€™sneuronsare plexity class AC0. In contrast, we demonstrate
constrainedtorepresentrealnumbersusingonlyc that the complexity of tasks such as IterADD
bits,wherecisasmallconstantindependentofthe and MUL exceeds that of AC0, using reductions
inputsequencelength. Thesenumbersmayberep- from Majority, a well-established problem that
resentedbyfloatingpointinIEEE754formats(Ka- hasbeenprovablyunsolvablebythecircuitsinAC0
4(Razborov,1987;Smolensky,1987). Consequently, thelogarithmic-precisionsettingreflectspractical
these tasks are inherently hard for low-precision deploymentscenarios.
Transformers. Wefirstestablishthat,underlogarithmicpreci-
Practical Implications. Intuitively, when per- sion,aTransformerwithconstantdepthanddimen-
formingiteratedaddition,aTransformermustcom- sioncansolveboththeintegeradditionanditerated
putethesumofmultipledigitsatonetime. How- additiontasksforarbitrarilylargeinputlengths,as
ever,underlow-precisionconstraints,maintaining showninTheorems5.1and5.2.
accurateintermediateresultsbecomessignificantly
Theorem5.1. Foranyintegersnandp,thereexists
challenging. For example, for the model under
alogarithmic-precisionTransformerwithconstant
int4 precision, adding 8 and 9 in base-10 arith-
depthLandconstanthiddendimensiond(indepen-
meticwill requireseveral neuronstostore there-
dentofn)thatcangeneratethecorrectoutputfor
sults to avoid overflow. In such cases, a Trans-
anyinputontheADD(n,p)task.
formeroperatinginalow-precisionregimewould
Theorem5.2. Foranyintegersn,k,andp,there
eitherrequireadditionalstepstocomputethecor-
exists a logarithmic-precision Transformer with
rectsumorneedmuchmoreneuronstodistribute
constantdepthLandconstanthiddendimensiond
thecomputationtoavoidoverflow. Thiswillfinally
(independentofnandk)thatcangeneratethecor-
resultintheTransformersfailingtocompletethe
rectoutputforanyinputontheIterADD(n,k,p)
taskwithafixednumberoflayersandareasonable
task.
modelsize.
While low-precision Transformers can effec- We now turn to integer multiplication. As es-
tivelyhandlesomesimplestarithmetictasks,such tablishedinTheorem5.3,alogarithmic-precision
asbasicintegeraddition,theircapacityisseverely Transformerwith constant depth and polynomial
limited when addressing more complex tasks. hiddendimensionsiscapableofsolvingtheinteger
As demonstrated, low numerical precision, such multiplicationtask.
as int8 and float8, imposes fundamental con-
Theorem5.3. Foranyintegersn, l, andp, there
straints,preventingthesemodelsfromsolvingprob-
exists a logarithmic-precision Transformer with
lemsthatwouldrequireTransformerswithsuper-
constantdepthandhiddendimensionsO(n2)that
polynomialsize.
can generate the correct output for any input on
theMUL(n,l,p)task.
5 Standard-PrecisionTransformersAre
SufficientforArithmeticTasks Theorems5.1to5.3demonstratethat,understan-
dardprecision,abounded-depthTransformerwith
InSection4,wedemonstratedthatlow-precision reasonablesizecansolveallelementaryarithmetic
Transformers struggle with arithmetic tasks due tasks. Comparedtothetheoreticalresultsforlow-
totheirexpressivelimitations. Inthissection,we precisionTransformers(Theorems4.1to4.3),even
willshowthatincreasingnumericalprecisionises- amodestincreaseinnumericalprecisionleadsto
sentialtoovercomingthislimitation. Inparticular, a substantial improvement in expressiveness for
wefocusonstandard-precisionTransformersand arithmetictasks.
showthatsuchmodelscanovercometheselimita- TheReasonforIncreasedExpressiveness. In
tionsandsolvearithmeticproblemsefficiently. Section4,wehighlightedlow-precisionTransform-
To formalize the notion of standard precision ersâ€™ difficulties in performing elementary arith-
(e.g.,float32),wefollowFengetal.(2023)and metic operations. A critical insight is that low-
adoptthesettingofalogarithmic-precisionTrans- precisionTransformersstruggletostoreintermedi-
former(SeeformaldefinitioninAppendixB).In ateresults,evenforsimpleoperationslikeadding
thissetting,theTransformerâ€™sinternalneuronscan two digits. This issue is mitigated by using stan-
represent real numbers with up to O(logn) bits, dard precision. For instance, float32 can accu-
where n denotes the maximum input sequence ratelyrepresentintegerswithinasufficientlylarge
length. GiventhatmodernLLMsoftenlimittheir rangetostoreintermediateresultsduringcomputa-
context length to hundreds of thousands of to- tion. Thus,standardprecisionallowsTransformers
kens (OpenAI, 2023; Touvron et al., 2023; An- toperformarithmetictasksmoreeffectively.
thropic, 2024), it is natural to treat 32 as the log- PracticalImplications. Ourtheoreticalresults
arithmic scale corresponding to 100,000. Hence, underscore the critical importance of numerical
5Base-2 Iterated Addition (3 numbers)
1.0
0.5 float32, 3 layers
float32, 5 layers
bfloat16, 3 layers
0.0 bfloat16, 5 layers
2 3 4 5 6 7 8 9 10 11
Maximum Length of the Addends
Base-2 Integer Multiplication
1.0
0.5 float32, 3 layers
float32, 5 layers
bfloat16, 3 layers
0.0 bfloat16, 5 layers
2 3 4 5 6 7 8 9 10 11 12 13 14
Maximum Length of the Multiplicands
Figure 2: Model performance on different tasks in base-2. Within each sub-figure, the x-axis represents the
maximumdigitslengthandthey-axisrepresentstheaccuracygainedbyeachmodel. Thefigureindicatesthat,for
alltasks,Transformersutilizingfloat32(32-bitprecision)with3layersand5layersoutperformtheirbfloat16
(16-bitprecision)counterparts.
precisionwhendeployingTransformersforarith- 6.1 ExperimentalSetup
metictasks. Underlow-precisionsettings,aTrans-
former requires super-polynomial model size to Tasksanddatasets. Weevaluatethreeelemen-
solveevenelementaryarithmeticproblems,which taryarithmetictasks: integeraddition,iteratedad-
is impractical for real-world applications. While dition,andintegermultiplication,aspresentedin
low-precisionmodelsmayoffercomputationalef- Figure 1. Each task involves a series of experi-
ficiency,theyarelikelytofailinscenariosthatde- mentswithbasep = 2,10andvaryingchoicesof
mandaccuratenumericalreasoning,suchasmath- digit length n. For integer addition, we examine
ematicalproblem-solvingorscientificcomputing. the addition of integers in both base-2 and base-
However, a slight increase in precisionâ€”such as 10, with digit lengths n âˆˆ {4,8,16,32,64}. For
using float32â€”enables Transformers to handle iteratedaddition,weexaminetheadditionofthree
more complex arithmetic operations while main- numbersinbase-2, withdigitlengthsn âˆˆ [2,11],
tainingareasonablehiddendimension. Thus,em- aswellasinbase-10,withdigitlengthsn âˆˆ [1,4].
ployingsufficientnumericalprecisioniscrucialfor Similarly,forintegermultiplication,werunexperi-
ensuringbothaccuracyandrobustnessinarithmetic mentsinbase-2withdigitlengthsn âˆˆ [2,14],and
tasks,andshouldbeakeyconsiderationwhende- in base-10 with digit length n âˆˆ [2,5]. We dy-
signingordeployingLLMsforapplicationsinvolv- namicallygenerateddatasetsforbothtrainingand
ingcomplexarithmeticreasoning. testing,withfurtherdetailsondatasetconstruction
availableinAppendixF.
6 Experiments
Intheprecedingsections,weemployedcomplex- Modelconfigurations. Forallexperiments,we
itytheorytodemonstratethatlow-precisionTrans- use Transformer models with hidden dimension
formersfacesignificantchallengesinperforming d = 256, heads H = 4, and model depth L âˆˆ
elementaryarithmetictasks. Tovalidatethesetheo- {3,5}. Thecausalself-attentionlayeremploysro-
reticalinsights,weconductaseriesofexperiments tarypositionalembeddings(Suetal.,2024),which
tocomparetheperformanceofTransformermodels replacethetraditionalsinusoidalembeddings. For
underdifferentprecisions. Theresultsprovideem- activation, we choose NewGeLU, the variant of
piricalevidencethatthemodelâ€™sabilitytoexecute GeLU(HendrycksandGimpel,2016),andapply
arithmeticoperationsdropsasprecisiondecreases, Xiaverinitializationacrosstheparameters(Glorot
reinforcingourtheoreticalresults. andBengio,2010).
6
ycaruccA
ycaruccABase-10 Iterated Addition, 3 Layers Base-10 Iterated Addition, 5 Layers
1.0 float32 1.0 float32
bfloat16 bfloat16
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
1 2 3 4 1 2 3 4
Maximum Length of Addends Maximum Length of Addends
Base-10 Integer Multiplication, 3 Layers Base-10 Integer Multiplication, 5 Layers
1.0 float32 1.0 float32
bfloat16 bfloat16
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
2 3 4 5 2 3 4 5
Maximum Length of Multiplicands Maximum Length of Multiplicands
Figure3: Weevaluatemodelperformanceoniteratedadditiontasksinvolvingthreenumbersandintegermultipli-
cationtasks. Eachsub-figurepresentsacomparisonoftheperformancebetweenfloat32(32-bitprecision)and
bfloat16(16-bitprecision).
Model training and inference. We adopt the maximumdigitslengthincreases,acrossallmodel
AdamWoptimizer(LoshchilovandHutter,2019) depths. Specifically,the16-bitprecisionexhibitsa
with Î² = 0.9, Î² = 0.999, lr = 10âˆ’3 and markeddeclineinperformanceforlengthsranging
1 2
weightdecay = 0.01inallexperiments. Weutilize from 7 to 10 in base-2, whereas the 32-bit preci-
aschedulerwithlinearwarmupandcosinedecay sionmaintainsnear-completeaccuracyacrossthese
over 100k steps, with a warm-up period of 10k ranges. Inbase-10,32-bitcanachieve90%correct-
steps. We optimize the cross-entropy loss on an- nessincontrasttothe16-bitrepresentation,which
swertokens. Eachmodelistrainedfor100ksteps strugglestoyieldaccurateresults. Inthecontextof
with a batch size of 512. During inference, mod- multiplicationtasks,asthemaximumdigitlength
elsarerequiredtooutputtheexactanswerstothe increases, 32-bit precision achieves significantly
tasks. Wereporttheaccuracyastheevaluationmet- higheraccuracycomparedto16-bitprecision. No-
ric,andforeachtask,wetesttheaccuracyonabout tably, when the length reaches 13 in base-2, the
50ksamples. Toassesstheimpactofmodelpreci- accuracy of 16-bit drops to a very low level, in-
sion,weconductexperimentswithbothfloat32 dicatingtheirinabilitytogeneratecorrectresults.
andbfloat16numericalprecisions. Additionally,inbase-10,weobserveanoticeable
reductioninaccuracywhentransitioningfrom32-
6.2 ExperimentalResults bitto16-bitprecision,particularlyforinputswitha
maximumlengthof3in3-layermodelsandamax-
Integeradditionprovestobetooeasy,maintaining
imumlengthof4in5-layermodels. Thissuggests
more than 94% accuracy even as digit length in-
thatthedifferencesofprecisionforperformingel-
creasesto32inbothbase-2andbase-10withboth
ementaryarithmetictasksareconsistentwithour
float32 and bfloat16 (detailed results in Ap-
theoreticalresults.
pendixF).Theresultsofiteratedadditionandmul-
tiplicationinbase-2areshowninFigure2,while
6.3 FurtherExperimentsonLLMs
thecorrespondingresultsinbase-10areshownin
Figure 3. Each sub-figure corresponds to a task We further conducted an extensive set of exper-
withthex-axisrepresentingthemaximumlength iments on LLMs to empirically support our the-
oftheaddendsandthemultiplicandsrespectively, oretical findings. Specifically, we evaluated the
andthey-axisrepresentingthetestaccuracy. Inthe modelsâ€™performanceacrossthreeelementaryarith-
contextoftheiteratedadditiontask,theaccuracy metictasks,asillustratedinFigure1. Intheinteger
showsasignificantlymorepronounceddeclinein addition task, we tested the addition of two base-
16-bitprecisioncomparedto32-bitprecisionasthe 10 integers, varying their digit lengths from 1 to
7
ycaruccA
ycaruccA
ycaruccA
ycaruccAInteger Addition on LLMs Iterated Addition (3 Numbers) on LLMs
1.00 1.00
bfloat16 bfloat16
int4 int4
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
1 4 7 10 13 1 3 5 7 9
Length of Addends Length of Addends
Iterated Addition (5 Numbers) on LLMs Integer Multiplication on LLMs
1.00 1.00
bfloat16 bfloat16
0.75 int4 0.75 int4
0.50 0.50
0.25 0.25
0.00 0.00
1 2 3 4 5 1 2 3 4 5
Length of Addends Length of Multiplicands
Figure4: TheperformanceofLLAMA3.18BInstructonvarioustasksinbase-10. Withineachsub-figure,we
comparetheperformanceofbfloat16(16-bitprecision)andint4(4-bitprecision).
13. For the iterated addition task, we expanded reducingthenumericalprecisionfrom16-bitto4-
thesetuptoincludetheadditionofthreeandfive bithasnoobservableeffectonaccuracywhenthe
base-10 numbers, also with digit lengths ranging numberlengthislessthanorequalto7,andonlya
from1to9and1to5. Similarly,intheintegermul- slightreductioninaccuracyisobservedwhenthe
tiplicationtask,weexaminedthemultiplicationof numberlengthexceeds7. Thiscomparableperfor-
twobase-10numbers,withdigitlengthsspanning mancebetween16-bitand4-bitprecisionsupports
from1to5. Theprocessofgeneratingdataisthe ourtheorem,whichassertsthatbothlow-precision
sameasinpreviousexperiments. andstandard-precisionarithmeticcansuccessfully
handleintegeradditiontasks. Incontrast,foriter-
All experiments utilized the LLaMA 3.1 8B
atedadditionandintegermultiplicationtasks,the
Instruct model (Dubey et al., 2024) as the base
reduction in numerical precision results in a sig-
model, employing a few-shot learning approach
nificant decrease in accuracy. Specifically, in the
forinference. Detailedspecificationsoftheprompt
iterated addition task, accuracy drops by nearly
construction and generation parameters can be
50% as the length of the addends increases. The
found in the Appendix F. During inference, the
impactisevenmorepronouncedintheintegermul-
LLMsweretaskedwithproducingexactsolutions
tiplication task, where performance deteriorates
tothegivenarithmeticproblems. Foreachtask,we
further. These experimental findings support our
evaluatethemodelon1ksamplestocomputethe
theoreticalresultsthatnumericalprecisionisacrit-
accuracy serving as the evaluation metric. Since
ical factor in the success of iterated addition and
the LLaMA 3.1 model is trained with bfloat16
integer multiplication tasks. Overall, the results
precision, we adopted its performance under
underscoretheconsistencybetweentheprecision
bfloat16 as our baseline. To further investigate
requirementsfortheseelementaryarithmetictasks
the impact of reduced numerical precision, we
andourtheoreticalpredictions.
additionally evaluated the modelâ€™s performance
when quantized to 4-bit precision via the AWQ
7 Conclusion
algorithm(Linetal.,2024).
TheresultsoftheexperimentsareshowninFig- In this work, we have theoretically analyzed the
ure4. Eachsub-figurepresentstheresultsofatask, impactofnumericalprecisiononLLMsformath-
wherethex-axisdenotesthemaximumlengthof ematicalreasoning. Byfocusingonthreeelemen-
theaddendsormultiplicands,andthey-axisrepre- taryarithmetictasks,integeraddition,iteratedaddi-
sentsthetestaccuracy. Intheintegeradditiontask, tion,andintegermultiplication,weconstructively
8
ycaruccA
ycaruccA
ycaruccA
ycaruccAdemonstratethattheTransformersoperatingunder LochanBasyalandMihirSanghvi.2023. Textsumma-
standard precision can handle these tasks effec- rizationusinglargelanguagemodels: Acomparative
studyofmpt-7b-instruct,falcon-7b-instruct,andope-
tively. Incontrast,Transformerswithlowprecision
naichat-gptmodels. Preprint,arXiv:2310.10449.
strugglewithcomplexarithmetictasks,excelling
only at integer addition. Extensive experimental SatwikBhattamishra, KabirAhuja, andNavinGoyal.
resultscorroborateourtheoreticalfindings,show- 2020. On the ability and limitations of transform-
ers to recognize formal languages. arXiv preprint
ingthatstandardprecisionmodelsoutperformlow
arXiv:2009.11264.
precisionones. Webelievethisstudyoffersvalu-
ableinsightsforthedevelopmentofmorepowerful David M Blei, Andrew Y Ng, and Michael I Jordan.
LLMsinmathematicalreasoning. 2003. Latentdirichletallocation. Journalofmachine
Learningresearch,3(Jan):993â€“1022.
8 Limitations
BradleyBrown,JordanJuravsky,RyanEhrlich,Ronald
Clark,QuocVLe,ChristopherRÃ©,andAzaliaMirho-
Onelimitationofthisworkisthatwehavenotfully
seini.2024. Largelanguagemonkeys: Scalinginfer-
exploredallkeycomponentsofmathematicalrea- encecomputewithrepeatedsampling. arXivpreprint
soning. Whilethearithmetictasksconsideredare arXiv:2407.21787.
foundational,thereremainotheressentialelements
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
ofmathematicalreasoningwhosedependenceon Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
numerical precision is still unclear. Additionally, Neelakantan,PranavShyam,GirishSastry,Amanda
ourfocuswasexclusivelyonnumericalprecision, Askell,etal.2020. Languagemodelsarefew-shot
learners. InAdvancesinneuralinformationprocess-
butweacknowledgethatotherfactorsarelikelyto
ingsystems,volume33,pages1877â€“1901.
playasignificantroleinapplyingLLMstomathe-
maticalreasoning. Weleavetheseexplorationsfor SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen El-
futurework. dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg,etal.2023. Sparksofartificialgeneralintelli-
gence: Earlyexperimentswithgpt-4. arXivpreprint
References
arXiv:2303.12712.
Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
Zhang, and Wenpeng Yin. 2024. Large language
William W. Cohen. 2023. Program of thoughts
modelsformathematicalreasoning: Progressesand
prompting: Disentanglingcomputationfromreason-
challenges. InProceedingsofthe18thConference
ingfornumericalreasoningtasks. Transactionson
oftheEuropeanChapteroftheAssociationforCom-
MachineLearningResearch.
putationalLinguistics: StudentResearchWorkshop,
pages225â€“237,St.Julianâ€™s,Malta.Associationfor
VincentChengandZhangYu.2023. AnalyzingChat-
ComputationalLinguistics.
GPTâ€™smathematicaldeficiencies: Insightsandcon-
tributions. In Proceedings of the 35th Conference
Ekin AkyÃ¼rek, Dale Schuurmans, Jacob Andreas, onComputationalLinguisticsandSpeechProcessing
TengyuMa,andDennyZhou.2022. Whatlearning (ROCLING2023),pages188â€“193,TaipeiCity,Tai-
algorithmisin-contextlearning? investigationswith
wan.TheAssociationforComputationalLinguistics
linearmodels. arXivpreprintarXiv:2211.15661.
andChineseLanguageProcessing(ACLCLP).
SilasAlberti,NiclasDern,LauraThesing,andGittaKu- DavidChiang,PeterCholak,andAnandPillay.2023.
tyniok.2023. Sumformer: Universalapproximation Tighter bounds on the expressivity of transformer
forefficienttransformers. InTopological,Algebraic encoders. InProceedingsofthe40thInternational
andGeometricLearningWorkshops2023,pages72â€“ ConferenceonMachineLearning,pages5544â€“5562.
86.PMLR.
DamaiDai,YutaoSun,LiDong,YaruHao,Shuming
ShengnanAn,ZexiongMa,ZeqiLin,NanningZheng, Ma,ZhifangSui,andFuruWei.2023. Whycangpt
Jian-GuangLou,andWeizhuChen.2024. Learning learnin-context?languagemodelsimplicitlyperform
frommistakesmakesllmbetterreasoner. Preprint, gradientdescentasmeta-optimizers. InICLR2023
arXiv:2310.20689. Workshop on Mathematical and Empirical Under-
standingofFoundationModels.
Anthropic. 2024. The claude 3 model family: Opus,
sonnet,haiku. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
SanjeevAroraandBoazBarak.2009. Computational 2019. Transformer-xl: Attentive language mod-
complexity: amodernapproach. CambridgeUniver- els beyond a fixed-length context. arXiv preprint
sityPress. arXiv:1901.02860.
9Jacob Devlin, Ming-Wei Chang, Kenton Lee, and 2024. xval: Acontinuousnumberencodingforlarge
Kristina Toutanova. 2019. BERT: Pre-training of languagemodels.
deepbidirectionaltransformersforlanguageunder-
standing. InProceedingsofthe2019Conferenceof Sophia Gu. 2023. Llms as potential brainstorming
theNorthAmericanChapteroftheAssociationfor partners for math and science problems. Preprint,
ComputationalLinguistics: HumanLanguageTech- arXiv:2310.10677.
nologies,Volume1(LongandShortPapers),pages
4171â€“4186.AssociationforComputationalLinguis- Michael Hahn. 2020. Theoretical limitations of self-
tics. attentioninneuralsequencemodels. Transactionsof
theAssociationforComputationalLinguistics,8:156â€“
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey, 171.
AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
Akhil Mathur, Alan Schelten, Amy Yang, Angela SongHan,HuiziMao,andWilliamJDally.2015. Deep
Fan,etal.2024. Thellama3herdofmodels. arXiv compression: Compressing deep neural networks
preprintarXiv:2407.21783. withpruning,trainedquantizationandhuffmancod-
ing. arXivpreprintarXiv:1510.00149.
Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lor-
raine)Li,LiweiJiang,BillYuchenLin,SeanWelleck,
Yiding Hao, Dana Angluin, and Robert Frank. 2022.
Peter West, Chandra Bhagavatula, Ronan Le Bras,
Formallanguagerecognitionbyhardattentiontrans-
JenaHwang,SoumyaSanyal,XiangRen,AllysonEt-
formers:Perspectivesfromcircuitcomplexity. Trans-
tinger,ZaidHarchaoui,andYejinChoi.2023. Faith
actionsoftheAssociationforComputationalLinguis-
andfate: Limitsoftransformersoncompositional-
tics,10:800â€“810.
ity. InAdvancesinNeuralInformationProcessing
Systems,volume36,pages70293â€“70332.CurranAs-
Joy He-Yueya, Gabriel Poesia, Rose E. Wang, and
sociates,Inc.
NoahD.Goodman.2023. Solvingmathwordprob-
lemsbycombininglanguagemodelswithsymbolic
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom
solvers. Preprint,arXiv:2304.09102.
Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell,YuntaoBai,AnnaChen,TomConerly,etal.
Dan Hendrycks and Kevin Gimpel. 2016. Gaus-
2021. A mathematical framework for transformer
sian error linear units (gelus). arXiv preprint
circuits. TransformerCircuitsThread,1.
arXiv:1606.08415.
GuhaoFeng,BohangZhang,YuntianGu,HaotianYe,
Di He, and Liwei Wang. 2023. Towards revealing IEEE.2019. Ieeestandardforfloating-pointarithmetic.
themysterybehindchainofthought: Atheoretical IEEE Std 754-2019 (Revision of IEEE 754-2008),
perspective. InThirty-seventhConferenceonNeural pages1â€“84.
InformationProcessingSystems.
ShimaImani,LiangDu,andHarshShrivastava.2023.
GuhaoFengandHanZhong.2023. Rethinkingmodel- MathPrompter: Mathematicalreasoningusinglarge
based,policy-based,andvalue-basedreinforcement language models. In Proceedings of the 61st An-
learning via the lens of representation complexity. nualMeetingoftheAssociationforComputational
arXivpreprintarXiv:2312.17248. Linguistics (Volume 5: Industry Track), pages 37â€“
42,Toronto,Canada.AssociationforComputational
Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths,
Linguistics.
Tommaso Salvatori, Thomas Lukasiewicz, Philipp
Petersen,andJuliusBerner.2024. Mathematicalca-
RenrenJin,JiangcunDu,WuweiHuang,WeiLiu,Jian
pabilitiesofchatgpt. Advancesinneuralinformation
Luan,BinWang,andDeyiXiong.2024. Acompre-
processingsystems,36.
hensiveevaluationofquantizationstrategiesforlarge
languagemodels. arXivpreprintarXiv:2402.16775.
ShivamGarg,DimitrisTsipras,PercyLiang,andGre-
goryValiant.2022. Whatcantransformerslearnin-
William Kahan. 1996. Ieee standard 754 for binary
context? acasestudyofsimplefunctionclasses. In
floating-pointarithmetic. LectureNotesontheStatus
AdvancesinNeuralInformationProcessingSystems.
ofIEEE,754(94720-1776):11.
XavierGlorotandYoshuaBengio.2010. Understanding
thedifficultyoftrainingdeepfeedforwardneuralnet- JikunKang, XinZheLi, XiChen, AmirrezaKazemi,
works. InProceedingsofthethirteenthinternational QianyiSun,BoxingChen,DongLi,XuHe,QuanHe,
conference on artificial intelligence and statistics, Feng Wen, Jianye Hao, and Jun Yao. 2024. Mind-
pages 249â€“256. JMLR Workshop and Conference star: Enhancingmathreasoninginpre-trainedllms
Proceedings. atinferencetime. Preprint,arXiv:2405.16265.
SiavashGolkar,MarielPettee,AlbertoBietti,Michael NayoungLee,KartikSreenivasan,JasonD.Lee,Kang-
Eickenberg,MilesCranmer,GeraudKrawezik,Fran- wookLee,andDimitrisPapailiopoulos.2024. Teach-
cois Lanusse, Michael McCabe, Ruben Ohana, ingarithmetictosmalltransformers. InTheTwelfth
LiamHoldenParker,BrunoRÃ©galdo-SaintBlancard, International Conference on Learning Representa-
TiberiuTesileanu,KyunghyunCho,andShirleyHo. tions.
10ZhiyuanLi,HongLiu,DennyZhou,andTengyuMa. WilliamMerrill,AshishSabharwal,andNoahASmith.
2024. Chainofthoughtempowerstransformersto 2022. Saturated transformers are constant-depth
solveinherentlyserialproblems. InTheTwelfthIn- thresholdcircuits. TransactionsoftheAssociation
ternationalConferenceonLearningRepresentations. forComputationalLinguistics,10:843â€“856.
ZhenwenLiang,DianYu,XiaomanPan,WenlinYao, SwaroopMishra,MatthewFinlayson,PanLu,Leonard
Qingkai Zeng, Xiangliang Zhang, and Dong Yu. Tang,SeanWelleck,ChittaBaral,TanmayRajpuro-
2024. MinT:Boostinggeneralizationinmathemat- hit,OyvindTafjord,AshishSabharwal,PeterClark,
ical reasoning via multi-view fine-tuning. In Pro- andAshwinKalyan.2022. LILA:Aunifiedbench-
ceedingsofthe2024JointInternationalConference markformathematicalreasoning. InProceedingsof
onComputationalLinguistics,LanguageResources the2022ConferenceonEmpiricalMethodsinNat-
andEvaluation(LREC-COLING2024),pages11307â€“ uralLanguageProcessing,pages5807â€“5832,Abu
11318,Torino,Italia.ELRAandICCL. Dhabi,UnitedArabEmirates.AssociationforCom-
putationalLinguistics.
JiLin,JiamingTang,HaotianTang,ShangYang,Wei-
Ming Chen, Wei-Chen Wang, Guangxuan Xiao, RodrigoNogueira,ZhiyingJiang,andJimmyLin.2021.
Xingyu Dang, Chuang Gan, and Song Han. 2024. Investigatingthelimitationsoftransformerswithsim-
Awq: Activation-awareweightquantizationforon- plearithmetictasks. Preprint,arXiv:2102.13019.
devicellmcompressionandacceleration. Proceed-
ingsofMachineLearningandSystems,6:87â€“100. CatherineOlsson,NelsonElhage,NeelNanda,Nicholas
Joseph,NovaDasSarma,TomHenighan,BenMann,
AmandaAskell,YuntaoBai,AnnaChen,TomCon-
BingbinLiu,JordanT.Ash,SurbhiGoel,AkshayKr-
erly,DawnDrain,DeepGanguli,ZacHatfield-Dodds,
ishnamurthy,andCyrilZhang.2023. Transformers
DannyHernandez,ScottJohnston,AndyJones,Jack-
learnshortcutstoautomata. InTheEleventhInterna-
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
tionalConferenceonLearningRepresentations.
Amodei, Tom Brown, Jack Clark, Jared Kaplan,
SamMcCandlish,andChrisOlah.2022. In-context
Ilya Loshchilov and Frank Hutter. 2019. De-
learningandinductionheads. TransformerCircuits
coupled weight decay regularization. Preprint,
Thread. Https://transformer-circuits.pub/2022/in-
arXiv:1711.05101.
context-learning-and-induction-heads/index.html.
PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
OpenAI.2023. Gpt-4technicalreport. arXivpreprint
yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
arXiv:2303.08774.
WeiChang,MichelGalley,andJianfengGao.2024.
Mathvista: Evaluating mathematical reasoning of
Jorge PÃ©rez, Pablo BarcelÃ³, and Javier Marinkovic.
foundationmodelsinvisualcontexts. InTheTwelfth
2021. Attention is turing complete. The Journal
International Conference on Learning Representa-
ofMachineLearningResearch,22(1):3463â€“3497.
tions.
Jorge PÃ©rez, Javier MarinkovicÂ´, and Pablo BarcelÃ³.
ShengjieLuo,ShandaLi,ShuxinZheng,Tie-YanLiu,
2019. On the turing completeness of mod-
Liwei Wang, and Di He. 2022. Your transformer
ern neural network architectures. arXiv preprint
maynotbeaspowerfulasyouexpect. InAdvances
arXiv:1901.03429.
inNeuralInformationProcessingSystems.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
YujunMao,YoonKim,andYilunZhou.2024. Champ:
DarioAmodei,IlyaSutskever,etal.2019. Language
A competition-level dataset for fine-grained anal-
modelsareunsupervisedmultitasklearners. OpenAI
yses of llmsâ€™ mathematical reasoning capabilities.
blog,1(8):9.
Preprint,arXiv:2401.06961.
SyedRifatRaiyan,MdNafisFaiyaz,ShahMd.Jawad
KellyMarchisio,SaurabhDash,HongyuChen,Dennis Kabir, Mohsinul Kabir, Hasan Mahmud, and
Aumiller,AhmetÃœstÃ¼n,SaraHooker,andSebastian MdKamrulHasan.2023. Mathwordproblemsolv-
Ruder.2024. Howdoesquantizationaffectmultilin- ingbygeneratinglinguisticvariantsofproblemstate-
gualllms? arXivpreprintarXiv:2407.03211. ments. InProceedingsofthe61stAnnualMeetingof
theAssociationforComputationalLinguistics(Vol-
Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, ume4: StudentResearchWorkshop),pages362â€“378,
John Kirchenbauer, Brian R. Bartoldson, Bhavya Toronto,Canada.AssociationforComputationalLin-
Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi guistics.
Schwarzschild, and Tom Goldstein. 2024. Trans-
formerscandoarithmeticwiththerightembeddings. Alexander A Razborov. 1987. Lower bounds for the
Preprint,arXiv:2405.17399. size of circuits of bounded depth with basis fË†; g.
Math.notesoftheAcademyofSciencesoftheUSSR,
WilliamMerrillandAshishSabharwal.2023. Thepar- 41(4):333â€“338.
allelismtradeoff: Limitationsoflog-precisiontrans-
formers. TransactionsoftheAssociationforCompu- Ankit Satpute, Noah GieÃŸing, AndrÃ© Greiner-Petter,
tationalLinguistics. MoritzSchubotz,OlafTeschke,AkikoAizawa,and
11BelaGipp.2024. Canllmsmastermath? investigat- XuezhiWang,JasonWei,DaleSchuurmans,QuocVLe,
inglargelanguagemodelsonmathstackexchange. EdH.Chi,SharanNarang,AakankshaChowdhery,
In Proceedings of the 47th International ACM SI- andDennyZhou.2023. Self-consistencyimproves
GIR Conference on Research and Development in chainofthoughtreasoninginlanguagemodels. In
InformationRetrieval,SIGIRâ€™24,page2316â€“2320, TheEleventhInternationalConferenceonLearning
New York, NY, USA. Association for Computing Representations.
Machinery.
ColinWei,YiningChen,andTengyuMa.2022a. Sta-
David Saxton, Edward Grefenstette, Felix Hill, and tistically meaningful approximation: a case study
PushmeetKohli.2019. Analysingmathematicalrea- onapproximatingturingmachineswithtransformers.
soningabilitiesof neuralmodels. In International AdvancesinNeuralInformationProcessingSystems,
ConferenceonLearningRepresentations. 35:12071â€“12083.
PauloShakarian,AbhinavKoyyalamudi,NoelNgu,and JasonWei,XuezhiWang,DaleSchuurmans,Maarten
LakshmivihariMareedu.2023. Anindependenteval- Bosma,brianichter,FeiXia,EdH.Chi,QuocVLe,
uation of chatgpt on mathematical word problems andDennyZhou.2022b. Chainofthoughtprompt-
(mwp). Preprint,arXiv:2302.13814. ing elicits reasoning in large language models. In
AdvancesinNeuralInformationProcessingSystems.
YunfanShao,LinyangLi,JunqiDai,andXipengQiu.
2023. Character-LLM: A trainable agent for role-
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.
playing. InProceedingsofthe2023Conferenceon
Thinking like transformers. In International Con-
Empirical Methods in Natural Language Process-
ferenceonMachineLearning,pages11080â€“11090.
ing,pages13153â€“13187,Singapore.Associationfor
PMLR.
ComputationalLinguistics.
Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. 2024.
RuoqiShen,SebastienBubeck,RonenEldan,YinTat
Rnnsarenottransformers(yet): Thekeybottleneck
Lee, Yuanzhi Li, and Yi Zhang. 2024. Positional
onin-contextretrieval. Preprint,arXiv:2402.18510.
descriptionmattersfortransformersarithmetic.
YangzhenWu,ZhiqingSun,ShandaLi,SeanWelleck,
Roman Smolensky. 1987. Algebraic methods in the
andYimingYang.2024a. Anempiricalanalysisof
theoryoflowerboundsforbooleancircuitcomplex-
compute-optimalinferenceforproblem-solvingwith
ity. In Proceedings of the nineteenth annual ACM
languagemodels. arXivpreprintarXiv:2408.00724.
symposiumonTheoryofcomputing,pages77â€“82.
Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li,
CharlieSnell,JaehoonLee,KelvinXu,andAviralKu-
ErkangZhu,YueWang,YinTatLee,RichardPeng,
mar.2024. Scalingllmtest-timecomputeoptimally
QingyunWu,andChiWang.2024b. Mathchat: Con-
canbemoreeffectivethanscalingmodelparameters.
versetotacklechallengingmathproblemswithllm
arXivpreprintarXiv:2408.03314.
agents. Preprint,arXiv:2306.01337.
PragyaSrivastava,ManujMalik,VivekGupta,Tanuja
Ryutaro Yamauchi, Sho Sonoda, Akiyoshi San-
Ganu,andDanRoth.2024. EvaluatingLLMsâ€™math-
nai, and Wataru Kumagai. 2023. Lpml: Llm-
ematical reasoning in financial document question
prompting markup language for mathematical rea-
answering. InFindingsoftheAssociationforCom-
soning. Preprint,arXiv:2309.13078.
putationalLinguisticsACL2024,pages3853â€“3878,
Bangkok,Thailandandvirtualmeeting.Association
Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng,
forComputationalLinguistics.
Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He,
JianlinSu, MurtadhaAhmed, YuLu, ShengfengPan, and Liwei Wang. 2024. Do efficient transformers
Wen Bo, and Yunfeng Liu. 2024. Roformer: En- reallysavecomputation? InForty-firstInternational
hancedtransformerwithrotarypositionembedding. ConferenceonMachineLearning.
Neurocomputing,568:127063.
ShunyuYao,BinghuiPeng,ChristosPapadimitriou,and
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- KarthikNarasimhan.2021. Self-attentionnetworks
bert, Amjad Almahairi, Yasmine Babaei, Nikolay canprocessboundedhierarchicallanguages. InPro-
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti ceedings of the 59th Annual Meeting of the Asso-
Bhosale, et al. 2023. Llama 2: Open founda- ciationforComputationalLinguisticsandthe11th
tion and fine-tuned chat models. arXiv preprint InternationalJointConferenceonNaturalLanguage
arXiv:2307.09288. Processing (Volume 1: Long Papers), pages 3770â€“
3785.
JohannesVonOswald,EyvindNiklasson,EttoreRan-
dazzo,JoÃ£oSacramento,AlexanderMordvintsev,An- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-
dreyZhmoginov,andMaxVladymyrov.2023. Trans- hao Huang, Huan Sun, Yu Su, and Wenhu Chen.
formerslearnin-contextbygradientdescent. InIn- 2024. MAmmoTH:Buildingmathgeneralistmodels
ternationalConferenceonMachineLearning,pages throughhybridinstructiontuning. InTheTwelfthIn-
35151â€“35174.PMLR. ternationalConferenceonLearningRepresentations.
12Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
Rawat, Sashank J Reddi, and Sanjiv Kumar.
2019. Aretransformersuniversalapproximatorsof
sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077.
ChulheeYun, Yin-WenChang, SrinadhBhojanapalli,
AnkitSinghRawat,SashankReddi,andSanjivKu-
mar.2020. O(n)connectionsareexpressiveenough:
Universalapproximabilityofsparsetransformers. In
AdvancesinNeuralInformationProcessingSystems,
volume33,pages13783â€“13794.
AojunZhou,KeWang,ZimuLu,WeikangShi,Sichun
Luo,ZipengQin,ShaoqingLu,AnyaJia,LinqiSong,
MingjieZhan, andHongshengLi.2024a. Solving
challengingmathwordproblemsusingGPT-4code
interpreterwithcode-basedself-verification. InThe
TwelfthInternationalConferenceonLearningRepre-
sentations.
HattieZhou,ArwenBradley,EtaiLittwin,NoamRazin,
Omid Saremi, Joshua M. Susskind, Samy Bengio,
andPreetumNakkiran.2024b. Whatalgorithmscan
transformerslearn? astudyinlengthgeneralization.
InTheTwelfthInternationalConferenceonLearning
Representations.
YongchaoZhou,UriAlon,XinyunChen,XuezhiWang,
RishabhAgarwal,andDennyZhou.2024c. Trans-
formers can achieve length generalization but not
robustly. InICLR2024WorkshoponMathematical
andEmpiricalUnderstandingofFoundationModels.
WenhaoZhu,HongyiLiu,QingxiuDong,JingjingXu,
Shujian Huang, Lingpeng Kong, Jiajun Chen, and
LeiLi.2024. Multilingualmachinetranslationwith
largelanguagemodels: Empiricalresultsandanal-
ysis. In Findings of the Association for Computa-
tionalLinguistics: NAACL2024,pages2765â€“2781,
MexicoCity,Mexico.AssociationforComputational
Linguistics.
13A RelatedWork
A.1 LLMsforMathematicalReasoning
MathmeticalReasoning. RecentstudieshighlightthelimitationsofcurrentLLMsinmathematical
reasoning(Ahnetal.,2024;Srivastavaetal.,2024). Satputeetal.(2024)demonstratedthatadvanced
modelslikeGPT-4cangeneraterelevantanswers,buttheseanswersarenotalwaysaccurate. Additionally,
Maoetal.(2024)foundthatcurrentLLMsstruggleevenwithverifyingthesolutionstomathematical
problems. ToenhancethemathematicalcapabilitiesofLLMs,severalstudieshavecarefullydesigned
promptingstrategies(Shakarianetal.,2023;ChengandYu,2023;Gu,2023;Luetal.,2024)orfinetuned
LLMsonmathematics-relateddatasets(Anetal.,2024;Liangetal.,2024;Raiyanetal.,2023;Mishra
etal.,2022;Yueetal.,2024). Otherapproachesincludeinference-basedsearchingmethods(Kangetal.,
2024),theapplicationofexternaltools(Yamauchietal.,2023;He-Yueyaetal.,2023;Chenetal.,2023),
andtheintroductionofsimulatedinteractionprocesses(Wuetal.,2024b)orself-verificationmechanisms
(Wangetal.,2023;Zhouetal.,2024a).
ArithmeticalReasoning. Bubecketal.(2023)highlightedarithmeticalreasoningasakeycomponent
of true mathematical ability. However, Saxton et al. (2019); Dziri et al. (2023) identified significant
challengesthatLLMsencounterwhensolvingelementaryarithmetictasks,suchasmulti-digitaddition
andmultiplication. Acommonapproachtomitigatethesedifficultiesistoreversetheoutputdigitorder
(Shenetal.,2024),orboththeinputandoutputdigitordersimultaneously(Leeetal.,2024). Otherstudies
havefocusedondevelopingimprovedpositionalencodings(Golkaretal.,2024;McLeishetal.,2024)or
positionaltokens(Nogueiraetal.,2021)thataremoresuitableforarithmetictasks. Zhouetal.(2024b,c)
further examined the length extrapolation capabilities of LLMs in solving basic arithmetic problems,
emphasizingtheimportanceofdataformatsandpositionalembeddingsforbettergeneralization.
A.2 ComputationalPowersofTransformers
AnothermorerelevantlineofworkinvestigatesthetheoreticalexpressivepowerofTransformersfroma
computationalperspective.
UniversalApproximation. EarlytheoreticalworkonTransformersprimarilyfocusedontheirfunction
approximationcapabilities. Yunetal.(2019)demonstratedthatTransformerscanuniversallyapproximate
anycontinuoussequence-to-sequencefunctions,givensufficientsize. Thisuniversalityresulthassince
beenextendedtovariousTransformervariants,suchasSparseTransformers(Yunetal.,2020),Linear
Transformers (Alberti et al., 2023), and Transformers with relative positional encodings (RPE) (Luo
etal.,2022). Additionally,previousstudiesestablishedthatinfinite-precisionTransformersareTuring-
complete(PÃ©rezetal.,2019,2021),whileWeietal.(2022a)showedthatfinite-precisionTransformersare
approximatelyTuring-complete. AlthoughtheseresultshighlightTransformersâ€™computationalcapacity,
our work develops expressiveness results under more practical settings, exploring the differences in
expressivenessacrossvaryinglevelsofnumericalprecision.
FormalLanguageLearning. AnotherlineofresearchfocusesontheabilityofTransformerstolearn
formal languages. Liu et al. (2023) explored how Transformers simulate finite state automata, while
Bhattamishra et al. (2020); Yao et al. (2021) studied their ability to recognize counter languages and
Dyck languages, respectively. On the negative side, Hahn (2020) showed that Transformers are not
capableoflearningdistributionsoverlanguages. Inadditiontoaffirmativeresults,severalworkshave
characterizedthelimitationsofTransformersfromtheperspectiveofformallanguagemodeling(Hahn,
2020; Bhattamishra et al., 2020; Weiss et al., 2021; Yao et al., 2021; Chiang et al., 2023) or circuit
simulation(Haoetal.,2022;Merrilletal.,2022;MerrillandSabharwal,2023). However,fewofthese
studiesfocusontheautoregressiveTransformerscommonlyusedinLLMs,whichweinvestigateinthis
paper.
Chain-of-ThoughtandIn-ContextLearning. Chain-of-Thought(CoT)prompting(Weietal.,2022b)
playsacrucialroleintasksrequiringcomplexreasoningstructures,andseveralstudiesaimtounderstand
its underlying mechanisms. For instance, Feng et al. (2023); Li et al. (2024) analyzed CoT from an
14expressivenessperspective,andYangetal.(2024);Wenetal.(2024)examinedCoTacrossmoredifferent
model variants. In-context learning (Brown et al., 2020; Garg et al., 2022) is another powerful aspect
ofLLMs. Sometheoreticalworkhasshownthatin-contextlearningcanbeexplainedthroughgradient
descent(AkyÃ¼reketal.,2022;Daietal.,2023;VonOswaldetal.,2023),whileothersattributeittothe
inductionheadsmechanism(Elhageetal.,2021;Olssonetal.,2022).
B AdditionalBackgroundandPreliminary
B.1 CircuitComplexity
Circuitcomplexityclassescapturevariousaspectsofcomputationalcomplexity,typicallyboundingcircuit
widthanddepth. Foramoredetailedintroduction,werefertoAroraandBarak(2009).
We begin by defining Boolean circuits. A Boolean circuit over a basis of gates is represented as a
finite-sizedirectedacyclicgraph(DAG),whereeachvertexcorrespondstoeitherabasisfunction(orgate)
oraninputbit. Someinternalnodesaredesignatedasoutputs,andthefan-inofavertexisdefinedasits
in-degree. Buildingonthisdefinition,wecandefinethecomplexityclassesNCi,ACi,andTCi:
â€¢ NCi: Thisclassconsistsofconstantfan-in, polynomial-sizedcircuitsmadeupofAND,OR,and
NOTgates,withadepthofO(login).
â€¢ ACi: Thisclassincludesunboundedfan-in,polynomial-sizedcircuitscomposedofAND,OR,and
NOTgates(withNOTgatesallowedonlyoninputs),alsohavingadepthofO(login).
â€¢ TCi: ThisclassextendsACi byallowingmajoritygates.
TherelationshipsamongtheNC,AC,andTChierarchiesareasfollows:
NCi âŠ‚ ACi âŠ‚ TCi âŠ‚ NCi+1, NC0 âŠŠ AC0 âŠŠ TC0.
B.2 Constant-precisionTransformer
Previousworkhasinvestigatedtheexpressivenessofconstant-precisionTransformers(Lietal.,2024),
utilizingasimplifiedversionoftheIEEE754standards(IEEE,2019). Ourconstant-precisionsettingis
analogous,andwewillintroducethefloating-pointrepresentationsweconsiderhere.
A(e+2s+1)-floatingpointrepresentationincludeseexponentbits,2sprecisionbits,andonesign
bit. Thenumbersrepresentableunderthisrepresentationaredefinedasfollows:
F := {S Â·2âˆ’s+E | âˆ’2âˆ’2s+1 â‰¤ S â‰¤ 22sâˆ’1,âˆ’2eâˆ’1 â‰¤ E â‰¤ max(2eâˆ’1âˆ’1,0),S,E âˆˆ Z}.
e,s
For any x âˆˆ R, its representation under this floating-point format is determined by rounding to the
nearest value in F. In the event of a tie, we select the number with the smaller absolute value. In this
paper,wefocusonthecasewheree = 0,whichmeansallrepresentablenumberstaketheformS Â·2âˆ’s,
withS âˆˆ Zsuchthatâˆ’2âˆ’2s+1 â‰¤ S â‰¤ 22sâˆ’1. However,thisisnecessaryonlyforTheorem4.1,while
Theorems4.2and4.3donotdependonspecificnumericalrepresentations.
Lietal.(2024)demonstratedthatconstant-depthTransformerswithconstantprecisionbelongtothe
complexityclassAC0.
B.3 Logarithmic-precisionTransformer
Akeylimitationofconstant-precisionrepresentationisthatitfailstocapturetheinputsizenwithina
singleneuron. Toaddressthis,weconsiderlogarithmicprecision,allowingforO(logn)bitsfornumerical
representations. Logarithmic-precisionTransformerspossessseveraladvantageousproperties(Fengetal.,
2023;FengandZhong,2023):
â€¢ For floating-point representations with O(logn) bits, any real number x âˆˆ O(poly(n)) can be
representedwithO(poly(1/n))error.
15â€¢ EachneuronintheTransformercanonlystoreO(logn)bitsofinformation,whichmeansitcannot
retainallinputdata. Consequently,computationmustbedistributedacrossthenetwork,aligning
withtheoperationalprinciplesofTransformers.
Previouswork(Merrilletal.,2022;MerrillandSabharwal,2023)hasshownthatlogarithmic-precision
TransformersfallwithinthecomplexityclassTC0.
C TechnicalLemmas
C.1 TechnicalLemmasforLogarithmicPrecisionMLP
WewillfirstprovidesomebasicresultsforlogarithmicprecisionMLP,whichcomesfrom(Fengetal.,
2023). Weomittheproofshere. Thecompleteproofcanbefoundintheappendixof(Fengetal.,2023).
LemmaC.1(Fengetal.,2023,LemmaC.1). ForanyÏµ > 0,thereexistsatwo-layerMLPf : R2 â†’ R
with 4 hidden dimension and GeLU activation, such that for any a,b âˆˆ [âˆ’M,M], |f(a,b)âˆ’ab| â‰¤ Ïµ.
Moreover,theâ„“ normoff isboundedbyO(poly(M,1/Ïµ)).
âˆ
Lemma C.2 (Feng et al., 2023, Lemma C.2). For any two-layer MLP g : Rd1 â†’ Rd2 with ReLU
activation,andthel normboundedbyM. Then,foranyÏµ > 0,thereexistsatwo-layerMLPf withthe
âˆ
samesizeandGeLUactivation,suchthatforallx âˆˆ Rd1,wehaveâˆ¥f(x)âˆ’g(x)âˆ¥
âˆ
â‰¤ Ïµ. Moreover,the
â„“ normoff isboundedbyO(poly(M,1/Ïµ)).
âˆ
Lemma C.3 (Feng et al., 2023, Lemma C.4). Consider selection function g : Rd Ã—Rd Ã—R â†’ Rd:
g(x,y,t) = xift > 0,andg(x,y,t) = yotherwise. Then,foranyÏµ > 0,Î± > 0,andM > 0,thereexist
atwo-layerMLPwith2d+2hiddendimensionandGeLUactivation,suchthatforallx âˆˆ [âˆ’M,M]d,
y âˆˆ [âˆ’M,M]d,andt âˆˆ (âˆ’âˆ,âˆ’Î±]âˆª[Î±,+âˆ),âˆ¥f(x,y,t)âˆ’g(x,y,t)âˆ¥ â‰¤ Ïµ. Moreover,theâ„“ norm
âˆ âˆ
isboundedbyO(poly(M,1/Î±,1/Ïµ)).
C.2 TechnicalLemmasforLogarithmicPrecisionAttentionLayer
Fengetal.(2023)alsostudiedtheexpressivepowerofthestandardattentionlayerandintroducedtwo
basic operations: COPY and MEAN, showing that the standard attention layer with log-precision is
capable of these operations under some regularity assumptions. Here, we will provide the results and
furtherconsideraspecialoperationcalledSINGLECOPY.
Consider a sequence of vectors x ,x ,Â·Â·Â· ,x where x = (xËœ ,r ,1) âˆˆ [âˆ’M,M]d+2 and M is a
1 2 n i i i
constant. DenoteattentionmatricesK,Q,V âˆˆ Rdâ€²Ã—(d+2) andq = Qx ,k = Kx ,v = Vx . For
i i j j j j
anyscalars0 < Ï,Î´ < M,definethematchingsetS = {j â‰¤ i : |q Â·k | â‰¤ Ï}andconsiderthefollowing
i i j
operations:
â€¢ COPY:Theoutputisu ,Â·Â·Â· ,u withu = v ,wherepos(i) = argmax r . Theoutputu
1 n i pos(i) jâˆˆSi j i
isundefinedwhen|S | = 0.
i
â€¢ MEAN: The output is u ,Â·Â·Â· ,u with u = mean v . The output u is undefined when
1 n i jâˆˆSi j i
|S | = 0.
i
â€¢ SINGLECOPY:Theoutputisu ,Â·Â·Â· ,u withu = v ,wherepos(i)istheuniqueelementin
1 n i pos(i)
S . Theoutputisundefinedif|S | =Ì¸ 1.
i i
Wenextmakethefollowingregularityassumption:
AssumptionC.4. Foranyinputsequencesx ,x ,Â·Â·Â· ,x ,thematricesQ,K,V andscalarsÏ,Î´satisfy:
1 2 n
â€¢ Foranyi,j âˆˆ [n],|q Â·k | â‰¤ Ïorq Â·k â‰¤ âˆ’Î´ hold.
i j i j
â€¢ Foranyi,j âˆˆ [n],i = j or|r âˆ’r | â‰¥ Î´ hold.
i j
â€¢ âˆ¥Vâˆ¥ â‰¤ 1.
âˆ
Now,wewillshowalogarithmicprecisionattentionlayerwithO(d)embeddingdimensionandone
attentionheadcanperformtheoperationsdefinedabove.
16Lemma C.5 (Feng et al., 2023, Lemma C.7). Suppose Assumption C.4 holds and Ï â‰¤
Î´2
. For
8M
any Ïµ > 0, there exists an attention layer with a single attention head and O(d) embedding di-
mension that can approximate COPY operation, and the â„“ norm of the parameters is bounded by
âˆ
O(poly(M,1/Î´,log(n),log(1/Ïµ))). Formally,foranyinputsequencesx ,x ,...,x ,denotetheatten-
1 2 n
tionoutputaso ,o ,...,o . Thenforanyi âˆˆ [n]withS Ì¸= âˆ…,wehaveâˆ¥o âˆ’u âˆ¥ â‰¤ Ïµ.
1 2 n i i i âˆ
Lemma C.6 (Feng et al., 2023, Lemma C.8). Suppose Assumption C.4 holds and Ï â‰¤ Î´Ïµ .
16Mln(4Mn)
Ïµ
For any 0 < Ïµ â‰¤ M, there exists an attention layer with a single attention head and O(d) embedding
dimension that can approximate MEAN operation, and the â„“ norm of the parameters is bounded
âˆ
by O(poly(M,1/Î´,log(n),log(1/Ïµ))). Formally, for any input sequences x ,x ,...,x , denote the
1 2 n
attentionoutputaso ,o ,...,o . Thenforanyi âˆˆ [n]withS Ì¸= âˆ…,wehaveâˆ¥o âˆ’u âˆ¥ â‰¤ Ïµ.
1 2 n i i i âˆ
WeomittheproofsofLemmasC.5andC.6here. Thecompleteproofcanbefoundintheappendixof
(Fengetal.,2023).
Lemma C.7. Assume Assumption C.4 holds and Î´ âˆ’ Ï â‰¥ cÏ where c > 0. For any Ïµ > 0,
there exists an attention layer with a single attention head and O(d) embedding dimension that
can approximate SINGLE COPY operation, and the â„“ norm of the parameters is bounded by
âˆ
O(poly(M,1/Î´,1/c,log(n),log(1/Ïµ))). Formally,foranyinputsequencesx ,x ,...,x ,denotethe
1 2 n
attentionoutputaso ,o ,...,o . Then,foranyi âˆˆ [n]with|S | = 1,wehaveâˆ¥o âˆ’u âˆ¥ â‰¤ Ïµ.
1 2 n i i i âˆ
Proof. Weconstructthequery,keyandvalueasfollows:
â€¢ Query: Î»q âˆˆ Rd
i
â€¢ Key: k âˆˆ Rd
i
â€¢ Value: v âˆˆ Rd
i
whereÎ» > 0isaconstant. Denotea astheattentionscore,wehave
ij
exp(Î»(q Â·k )) exp(Î»(q Â·k ))
i j i j
a = = .
i,j (cid:80) (cid:80)
exp(Î»(q Â·k )) exp(Î»(q Â·k ))
jâ€² i jâ€² jâ€² i jâ€²
Since Î´ âˆ’ Ï â‰¥ cÏ, we have Î´ âˆ’ Ï â‰¥ c Î´. By setting Î» =
(c+1)ln(2n ÏµM)
(which is bounded by
c+1 cÎ´
O(poly(M,1/Î´,1/c,log(n),log(1/Ïµ)))),wehave
exp(âˆ’Î»Ï)
a â‰¥ (1)
i,pos(i) exp(âˆ’Î»Ï)+(nâˆ’1)exp(âˆ’Î»Î´)
1
=
1+(nâˆ’1)exp(âˆ’Î»(Î´âˆ’Ï))
â‰¥ 1âˆ’(nâˆ’1)exp(âˆ’Î»(Î´âˆ’Ï)) (2)
(cid:18) (cid:18) (cid:19)(cid:19)
2nM
â‰¥ 1âˆ’nexp âˆ’ln
Ïµ
Ïµ
= 1âˆ’ .
2M
Here, we use Assumption C.4 and |S | = 1 in (1), which implies that for jâ€² Ì¸= pos(i), we have
i
q Â·k â‰¤ âˆ’Î´;andin(2)weapplytheinequality 1 â‰¥ 1âˆ’xforallx â‰¥ 0. Thuswehave
i jâ€² 1+x
(cid:13) (cid:13) ï£« ï£¶
(cid:13) (cid:13)
(cid:13)(cid:88) (cid:13) (cid:88)
âˆ¥o iâˆ’u iâˆ¥ âˆ = (cid:13) a ijv j âˆ’v pos(i)(cid:13) â‰¤ Mâˆ¥Vâˆ¥ âˆÂ·ï£­1âˆ’a i,pos(i)+ a i,jï£¸
(cid:13) (cid:13)
(cid:13) j (cid:13) jÌ¸=pos(i)
âˆ
= Mâˆ¥Vâˆ¥ (2âˆ’2a ) â‰¤ Ïµ,
âˆ i,pos(i)
whichconcludestheproof.
17C.3 TechnicalLemmasforConstantPrecisionCalculation
Inthissection,weprovidesometechnicallemmasforconstantprecisioncalculationandconstantprecision
MLPs. Suppose we have 2s-bit precision bits and no exponent bits, and denote B = 2s âˆ’2âˆ’s. The
s
maximumnumberwecouldrepresentisB ,whiletheminimumisâˆ’B .
s s
LemmaC.8(Lietal.,2024,LemmaE.1andE.2). Foranys âˆˆ N ,exp(âˆ’B ) = 0andexp(B ) = B .
+ s s s
Proof. Noticethatexp(B ) â‰¥ eB > 2s+1,thusexp(âˆ’B ) â‰¤ 2âˆ’sâˆ’1. Thisimpliesthatexp(âˆ’B ) = 0.
s s s s
Ontheotherhand,exp(B ) â‰¥ B +1 > B ,whichimpliesthatexp(B ) = B .
s s s s s
LemmaC.9. Foranys âˆˆ N ,GeLU(âˆ’B ) = 0.
+ s
Proof. ItsufficestoproveB Î¦(âˆ’B ) â‰¤ 2âˆ’sâˆ’1 whereÎ¦istheCDFofGaussiandistribution.
s s
Fors = 1,wehaveB = 3,thus
s 2
3 3 1âˆ’0.68 1
B Î¦(âˆ’B ) â‰¤ Î¦(âˆ’1) â‰¤ Â· < .
s s
2 2 2 4
Fors â‰¥ 2,wehave
B sÎ¦(âˆ’B s) =
âˆšB s (cid:90) +âˆ eâˆ’x 22
dx â‰¤
âˆšB s (cid:90) +âˆ eâˆ’B 2sx
dx
2Ï€ 2Ï€
Bs
âˆš
Bsâˆš
(cid:114)
â‰¤
2 eâˆ’B 2s2
â‰¤ âˆš
2 2
â‰¤
âˆš2 2
â‰¤
1
,
Ï€ Ï€(B2+2) Ï€22s 2s+1
s
completingourproof.
D ProofsforSection4
Theorem4.1. Foranyfixedintegersp,thereexistconstant-precisionTransformerswithconstantdepthL
andhiddendimensiond = O(n2)thatcansolvetheADD(n,p)task.
For ease of reading, we first describe an algorithm to perform ADD(n,p) (Algorithm 1) and prove
thecorrectness,thenconstructaTransformerwiththegivenconfigurationsinTheorem4.1tosimulate
Algorithm1.
Algorithm1:ADD(n,p)Algorithm
Input :Twop-adicnumbersa,bwithlengthn ,n
1 2
Output:Thesumoftheinputso,containing(n+1)bitswheren = max(n ,n )
1 2
a = 0,b = 0;
1 n n
foreachi âˆˆ {0,Â·Â·Â· ,nâˆ’1}do
2
Computethecarry-onbitsc:
3
i = max{j â‰¤ i | a +b â‰¥ p};
4 âˆ§ i i
i = max{j â‰¤ i | a +b â‰¤ pâˆ’2};
5 âˆ¨ i i
c = 1 ;
6 i iâˆ§>iâˆ¨
end
7
Computetheoutcomeo: o = (a +b +c )modp;
8 i i i iâˆ’1
LemmaD.1(AnalgorithmtoperformADD(n,p)). Algorithm1outputso = a+bforallinputsa,b.
Proof. Giventwon-bitp-adicnumbersaandb,wecancomputethecarry-overbitsc = (c ,Â·Â·Â· ,c )as
n 1
follows:
c = 0,
âˆ’1
c = 1 ,
0 a0+b0â‰¥p
c = (c Â·1 )âˆ¨1 , (3)
1 0 a1+b1â‰¥pâˆ’1 a1+b1â‰¥p
Â·Â·Â· ,
c = (c Â·1 )âˆ¨1 .
i iâˆ’1 ai+biâ‰¥pâˆ’1 ai+biâ‰¥p
18Byremovingtherecursivecomputation,wecancomputethecarry-overbythefollowingformulas:
i = max{j â‰¤ i | a +b â‰¥ p},
âˆ§ i i
i = max{j â‰¤ i | a +b â‰¤ pâˆ’2}, (4)
âˆ¨ i i
c = 1 .
i iâˆ§>iâˆ¨
Orequivalently,
ï£® ï£¹
(cid:95) (cid:94)
c
i
= ï£°1 aj+bjâ‰¥pâˆ§ 1
a k+b
kâ‰¥pâˆ’2ï£». (5)
0â‰¤jâ‰¤i jâ‰¤kâ‰¤i
InEquation(4),i indicatestheclosestbitthatcancontributeanadditionalcarrytothehigherbitsupto
âˆ§
theithbit,whilei identifiesthehighestbitlowerthani,suchthatthecarrygeneratedbythebitsbelow
âˆ¨
i doesnotaffectthebitshigherthani . Therefore,thecarry-overc = 1ifandonlyifi < i .
âˆ¨ âˆ¨ i âˆ§ âˆ¨
Thenaftercomputingthecarryoverbits,wecancomputethesumoftheinputintegersasfollows:
o = (a +b )modp,
0 0 0
o = (a +b +c )modp,
1 1 1 0
Â·Â·Â· (6)
o = (a +b +c )modp,
i i i iâˆ’1
o = c .
n nâˆ’1
Therefore,theoutputoisexactlythesumoftheintegersandtheAlgorithm1outputsADD(a,b)for
alla,b âˆˆ {0,1}n.
Next,weprovidetheproofforTheorem4.1.
ProofforTheorem4.1. Now,wedemonstratethataconstant-precisiontransformer,withaconstantdepth
L,afixednumberofattentionheads,andO(n2)hiddendimensions,iscapableofsimulatingAlgorithm1.
Consequently,thismodelcanaccuratelygeneratecorrectoutputforanyinputintegersa,b.
InitialEmbeddings: Thetotallengthoftheinputsequenceisnolongerthan2(n+1). Wecategorize
the tokens into two classes: number tokens (0,1,Â·Â·Â· ,p âˆ’ 1) and auxiliary tokens (+, =, <SOS> and
<EOS>).
â€¢ Embeddingofinputtokena : u0 = (a e ,0,âˆ’1,0,0,1,1).
i a,i i i+1
â€¢ Embeddingofinputtokenb : u0 = (0,b e ,âˆ’1,0,0,2,1).
i b,i i i+1
â€¢ Embeddingofoutputtokeno : u0 = (0,0,o ,e ,0,3,âˆ’1).
i o,i i i+1
â€¢ Embeddingofâ€œ+â€token: u0 = (0,0,âˆ’1,0,0,4,âˆ’1).
+
â€¢ Embeddingofâ€œ=â€token: u0 = (0,0,âˆ’1,0,1,5,âˆ’1).
=
â€¢ Embeddingof<SOS>token: u = (0,0,âˆ’1,0,0,6,âˆ’1).
<SOS>
â€¢ Embeddingof<EOS>token: u = (0,0,0,0,0,3,âˆ’1).
<EOS>
wheree âˆˆ Rn+1 isone-hotvector. Supposewehave2s-bitprecisionanddefineB = 2sâˆ’2âˆ’s. Recall
i s
that,themaximumnumberwecanrepresentisB ,whiletheminimumisâˆ’B .
s s
Block1. ThefirstblockoftheTransformerwillCOPYthevalueofa ,b topositionofoutputs. We
i i
usetheattentionheadtofinishtheCOPYoperation. Wesetthequery,keyandvalueasfollows:
â€¢ Query: q = B .
s
â€¢ Key: k = u0[3n+6],i.e.,k = 1forinputnumbertokens,andk = âˆ’1otherwise.
19â€¢ Value: v = u0[1,Â·Â·Â· ,2n+2], i.e., v = (a e ,0) for input a, v = (0,b e ) for input b, and
i i+1 i i+1
v = 0otherwise.
Sincewehaveonlyconstantprecisionhere,weneedtochecktheattentionvaluescarefully. Theattention
value(withoutnormalization)willbeB fortokensa ,b ,andâˆ’B otherwise. ByLemmaC.8,wecan
s i i s
getexp(B ) = B ,exp(âˆ’B ) = 0,andthenormalizationtermis2nB = B ,indicatingtheattention
s s s s s
weights will be 1 for token x ,y , and 0 otherwise. Thus the attention output at the position of output
i i
tokensisalways(a ,Â·Â·Â· ,a ,b ,Â·Â·Â· ,b ).
0 n 0 n
Block2. ThesecondblockoftheTransformerusesMLPstocalculatetheoutputousingAlgorithm1.
Wewillfinishthesecalculationsfollowingthestepsbelow:
â€¢ Calculater = a +b fori = 0,Â·Â·Â· ,n. ThiscanbeimplementedbyaMLPwithconstanthidden
i i i
dimension. Here,werequireB â‰¥ 2ptoavoidoverflowofr .
s i
â€¢ Calculatef = 1 andg = 1 . ByLemmaC.9,wehave
i riâ‰¥p i riâ‰¥pâˆ’2
GeLU[B Â·(2r âˆ’2p+1)] GeLU[B Â·(2r âˆ’2p+5)]
s i s i
f = , g = .
i i
GeLU(B ) GeLU(B )
s s
Here,werequireB â‰¥ 4ptoavoidoverflowof2r âˆ’2p+1.
s i
â€¢ Calculatec bythefollowingformula:
i
ï£® ï£¹ ï£® ï£¹
(cid:95) (cid:94) (cid:95) (cid:94)
c i = ï£°1 aj+bjâ‰¥pâˆ§ 1 a k+b kâ‰¥pâˆ’2ï£» = ï£°f j âˆ§ g kï£».
0â‰¤jâ‰¤i jâ‰¤kâ‰¤i 0â‰¤jâ‰¤i jâ‰¤kâ‰¤i
Noticethat
(cid:95)
Î± =
GeLU[B
s((cid:80)Î³
i=1Î± i)]
,
(cid:94)
Î± = 1âˆ’
(cid:95)
(1âˆ’Î± ),
i i i
GeLU(B )
s
1â‰¤iâ‰¤Î³ 1â‰¤iâ‰¤Î³ 1â‰¤iâ‰¤Î³
whichimpliesthatâˆ¨,âˆ§canbeimplementedbyconstant-depth,constant-precisionMLPwithconstant
hiddendimension. Therefore,wecancalculatethevalueofc usingO(n)hiddendimension.
i
â€¢ Calculateo = a +b +c fori = 0,Â·Â·Â· ,n. ThiscanbeimplementedbyaMLPwithconstant
i i i iâˆ’1
hiddendimension. Again,werequireB â‰¥ 2ptoavoidoverflowofo .
s i
Sinceweneedtocalculater ,f ,g ,c foranyi,thisrequiresthehiddendimensionofO(n2).
i i i i
Block 3. Finally, this block filters out the token o from o. Letâ€™s consider the token o , where
i i+1
i âˆˆ {0,Â·Â·Â· ,nâˆ’1},wewanttopredictthenexttokeno . First,wecalculatee byusinge fromthe
i i+1 i+2
positionalembeddingofm0 . Then,wewillcalculateo by
o,i+1 i
o = âŸ¨e ,oâŸ©.
i i+1
Sincex = GeLU(x)âˆ’GeLU(âˆ’x),wehave
n+1 n+1
(cid:88) (cid:88)
o = e [j]o[j] = [GeLU(e [j]âˆ’B (2âˆ’2o[j]))âˆ’GeLU(âˆ’e [j]âˆ’B (2âˆ’2o[j]))],
i i+1 i+1 s i+1 s
j=1 j=1
whichimplieswecancalculatethevalueofo usingO(n)hiddendimension. Thefinaloutputfromthis
i
layerischaracterizedbytheequation:
(cid:40)
(o ,e ) ifi > 0,
e3 = i i+1
o,i+1
(0,0) ifi = 0.
PredictNextToken. GiventheoutputembeddingsofthelastTransformerlayere3 ,andtheword
o,i
embeddings,theTransformercansimplypredictthenexttokenbyfindingthenearestwordembeddings.
Inthisconstruction,weonlyrequireB â‰¥ 4p,whichimpliesconstantprecisionissufficient.
s
20Theorem4.2. ForanyintegerspandL,andforanypolynomialf,thereexistproblemscalesnandksuch
thatnoconstant-precisionautoregressiveTransformerwithLlayersandhiddendimensiond < f(n,k)
cancorrectlysolvetheIterADD(n,p)task.
Proof. Suppose there exists integers p â‰¥ 2,L and polynomial f, such that for any problem scales
n,k,thereexistsaconstant-precisionautoregressiveTransformerwithLlayersandhiddendimension
d â‰¤ f(n,k)thatcancorrectlysolvetheIterADD(k,n,p)task.
ConsiderMaj(b ,Â·Â·Â· ,b ),whereb âˆˆ {0,1}. HereweprovideareductionfromMaj(b ,Â·Â·Â· ,b )to
1 k i 1 k
IterADD(2,kâ€²,p)wherekâ€² = pâŒˆlog pkâŒ‰ â‰¤ pk. Leta
i
= b i(p2âˆ’1)fori = 1,Â·Â·Â· ,k,and
(cid:24) (cid:25)
k
a k+1+Â·Â·Â·+a
kâ€²
= pâŒˆlog pkâŒ‰+1âˆ’(p2âˆ’1) .
2
Thisisfeasiblesince
(cid:24) (cid:25)
k
pâŒˆlog pkâŒ‰+1âˆ’(p2âˆ’1) â‰¤ (pâŒˆlog pkâŒ‰âˆ’k)(p2âˆ’1)
2
forp â‰¥ 2. Thuswecanget
k (cid:24) (cid:25) kâ€²
(cid:88) k (cid:88)
Maj(b 1,Â·Â·Â· ,b k) = 1 â‡â‡’ b
i
â‰¥
2
â‡â‡’ a
i
â‰¥ pâŒˆlog pkâŒ‰+1 â‡â‡’ o
âŒˆlog pkâŒ‰+1
> 0
i=1 i=1
Sinceabounded-depth,fixed-precisionTransformerwithpolynomial-sizegeneratingasingletokenfalls
withinthecomplexityclassAC0,bythereductionabovewecangetMaj âˆˆ AC0,whichisacontradiction.
Theorem4.3. ForanyintegerspandL,andforanypolynomialf,thereexistproblemscalesnandlsuch
thatnoconstant-precisionautoregressiveTransformerwithLlayersandhiddendimensiond < f(n,l)
cancorrectlysolvetheMUL(n,l,p)task.
Proof. Suppose there exists integers p â‰¥ 2,L and polynomial f, such that for any problem scales
n,l, there exists a constant-precision autoregressive Transformer with L layers and hidden dimension
d â‰¤ f(n,l)thatcancorrectlysolvetheMUL(n,l,p)task.
ConsiderMaj(c ,Â·Â·Â· ,c ),wherec âˆˆ {0,1}. HereweprovideareductionfromMaj(c ,Â·Â·Â· ,c )to
1 k i 1 k
MUL(n,l,p)where
(cid:18) (cid:22) (cid:23)(cid:19)
n = (cid:0) âŒˆlog kâŒ‰+1(cid:1) pâŒˆlog pkâŒ‰+ k = O(klogk), l = n+âŒˆlog kâŒ‰ = O(klogk).
p 2 p
Let
(cid:22) (cid:23)
k
kâ€² = pâŒˆlog pkâŒ‰+ , c
k+1
= Â·Â·Â· = c
kâ€²
= 1,
2
and
a = c 0Â·Â·Â·0c 0Â·Â·Â·0Â·Â·Â·c 0Â·Â·Â·0, b = 10Â·Â·Â·010Â·Â·Â·0Â·Â·Â·10Â·Â·Â·0.
1 2 kâ€²
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
âŒˆlog nâŒ‰ âŒˆlog nâŒ‰ âŒˆlog nâŒ‰ âŒˆlog nâŒ‰ âŒˆlog nâŒ‰ âŒˆlog nâŒ‰
p p p p p p
Underthisconstruction,wecanget
(cid:24) (cid:25)
k
Maj(c 1,Â·Â·Â· ,c k) = 1 â‡â‡’ c 1+Â·Â·Â·+c
k
â‰¥ â‡â‡’ c 1+Â·Â·Â·+c
kâ€²
â‰¥ pâŒˆlog pkâŒ‰ â‡â‡’ o
lâˆ’1
> 0
2
Sinceabounded-depth,fixed-precisionTransformerwithpolynomial-sizegeneratingasingletokenfalls
withinthecomplexityclassAC0,bythereductionabovewecangetMaj âˆˆ AC0,whichisacontradiction.
21E ProofsforSection5
Theorem5.1. Foranyintegersnandp,thereexistsalogarithmic-precisionTransformerwithconstant
depthLandconstanthiddendimensiond(independentofn)thatcangeneratethecorrectoutputforany
inputontheADD(n,p)task.
Proof. Theorem 5.1 is a special case of Theorem 5.2. Taking k = 2 in Theorem 5.2 completes our
proof. Notice that m = âŒˆlog kâŒ‰ = 1, which implies that we donâ€™t need to combine neighboring bits
p
actually.
Theorem5.2. Foranyintegersn,k,andp,thereexistsalogarithmic-precisionTransformerwithconstant
depthLandconstanthiddendimensiond(independentofnandk)thatcangeneratethecorrectoutput
foranyinputontheIterADD(n,k,p)task.
For ease of reading, we also describe an algorithm to perform IterADD(n,k,p) (Algorithm 2) and
provethecorrectnessfirst. Then,wewillconstructaconstant-sizeTransformerwithlogarithmicprecision
tosimulateAlgorithm2.
Algorithm2:IterADD(n,k,p)Algorithm
Input : k p-adicnumbersa ,Â·Â·Â· ,a ,withmaximumlengthn
1 k
Output: Thesumoftheinputso
m = âŒˆlog kâŒ‰;
1 p
(cid:80)
Computethesumofeachbitasr = a forj = 0,Â·Â·Â· ,nâˆ’1;
2 j iâˆˆ[k] ij
Combineneighboringmbits:
3
mâˆ’1
(cid:88)
s = r pj
i ik+j
j=0
fori = 0,Â·Â·Â· ,âŒŠn/mâŒ‹;
Decomposes bys = b pm+q ,whereq âˆˆ [0,pmâˆ’1]andb ,q âˆˆ N;
4 i i i i i i i
c = 0;
5 0
foreachi = 0,Â·Â·Â· ,âŒŠn/mâŒ‹do
6
Computethecarry-onbitsc:
7
i = max{j â‰¤ i | q +b â‰¥ pm};
8 âˆ§ j jâˆ’1
i = max{j â‰¤ i | q +b â‰¤ pmâˆ’2};
9 âˆ¨ j jâˆ’1
c = 1 ;
10 i iâˆ§>iâˆ¨
end
11
Computethepm-adicoutcomeoËœ: oËœ = (q +b +c )modpm fori = 0,Â·Â·Â· ,âŒŠn/mâŒ‹+1;
12 i i iâˆ’1 iâˆ’1
Covertpm-adicoËœtop-adico:
13
(cid:36) (cid:37)
oËœ modp(l+1)
j
o =
i pl
fori = jk+lwherel âˆˆ {0,Â·Â·Â· ,kâˆ’1},j âˆˆ Z;
LemmaE.1(AnalgorithmtoperformIterADD(n,k,p)). Algorithm2outputso = a +Â·Â·Â·+a for
1 k
allinputsa ,Â·Â·Â· ,a .
1 k
Proof. In the initial four steps, we convert p-adic addition into pm-adic addition. This transformation
enablestheexpression(cid:80) s pim toaccuratelyrepresentthesumofk numbers.
i i
Atthisstage,s âˆˆ [0,kpm). Tocomputethefinalresultsaccurately,itiscrucialtoaccountforcarry-
i
overeffectstoensurethefinaloutputsoËœ rangefrom0topmâˆ’1. Wecandecomposeeachs usingthe
i i
formulas = b pm +q , whereq âˆˆ [0,pm âˆ’1]andb < k â‰¤ pm. Consequently, theoverflowb will
i i i i i i
22only directly overflow to the immediately subsequent bit q . And we have q +b â‰¤ 2(pm âˆ’1).
i+1 i+1 i
Therefore,letcdenotethevectortorecordthecarry-overeffectsonthepositioni,andwehave
c = 0,
âˆ’1
c = 1 (b := 0),
0 q0+bâˆ’1â‰¥pm âˆ’1
c = (c Â·1 )âˆ¨1 , (7)
1 0 q1+b0â‰¥pmâˆ’1 q1+b0â‰¥pm
Â·Â·Â·
c = (c Â·1 )âˆ¨1 .
i iâˆ’1 qi+biâˆ’1â‰¥pmâˆ’1 qi+biâˆ’1â‰¥pm
Byremovingtherecursivecomputation,wecancomputethecarry-overbythefollowingformulas:
i = max{j â‰¤ i | q +b â‰¥ pm},
âˆ§ j jâˆ’1
i = max{j â‰¤ i | q +b â‰¤ pmâˆ’2}, (8)
âˆ¨ j jâˆ’1
c = 1 .
i iâˆ§>iâˆ¨
Orequivalently,
ï£® ï£¹
(cid:95) (cid:94)
c
i
= ï£°1
qj+bjâˆ’1â‰¥pm
âˆ§ 1
q k+b
kâˆ’1â‰¥pmâˆ’2ï£». (9)
0â‰¤jâ‰¤i jâ‰¤kâ‰¤i
InEquation(8),i indicatesthenearestbitthatcancontributeanadditionalcarrytothehigherbitsup
âˆ§
tothei-thbit,whilei identifiesthehighestbitlowerthani,suchthatthecarrygeneratedbybitsbelow
âˆ¨
i doesnotaffectbitshigherthani . Therefore,thecarry-overc = 1ifandonlyifi > i .
âˆ¨ âˆ¨ i âˆ§ âˆ¨
Thenaftercomputingthecarry-overvectors,wecancomputethepm-adicsumoftheinputintegersas
follows:
oËœ = q ,
0 0
oËœ = (q +b +c )modpm,
1 1 0 0
(10)
Â·Â·Â·
oËœ = (q +b +c )modpm.
i i iâˆ’1 iâˆ’1
Finally, to convert the pm-adic number to the final p-adic number oËœ, we only need to perform the
modulusoperation:
(cid:36) (cid:37)
oËœ modp(l+1)
j
o =
i pl
for i = jk +l where l âˆˆ {0,Â·Â·Â· ,k âˆ’1}, j âˆˆ Z. Therefore, the output o is exactly the sum of the m
n-bitsintegersandtheAlgorithm2outputsIterADD(a ,Â·Â·Â· ,a )foralla ,Â·Â·Â· ,a .
1 m 1 m
Next,weprovidetheproofforTheorem5.2.
ProofforTheorem5.2. Now,wedemonstratethatalog-precisiontransformer,withaconstantdepth,a
fixednumberofattentionheads,andconstantembeddingdimensions,iscapableofsimulatingAlgorithm2.
Consequently,thismodelcanaccuratelygeneratecorrectoutputforanyinputintegersa ,...,a .
1 k
InitialEmbeddings: Thetotallengthoftheinputsequenceisnolongerthank(n+1). Wecategorize
the tokens into two classes: number tokens (0,1,Â·Â·Â· ,p âˆ’ 1) and auxiliary tokens (+, =, <SOS> and
<EOS>). Giventheparametersk,n,wedeterminetheparameterm = âŒˆlog kâŒ‰,asspecifiedinAlgorithm2.
p
Theembeddingsfortheseclassesaredefinedasfollows:
(cid:16) (cid:17)
â€¢ Embeddingofinputtokena : e0 = a ,0,0,i,j,jmodm,âŒŠj âŒ‹,pjmodm,pâˆ’(jmodm),ape .
i,j i,j i,j m i,j
â€¢ Embeddingofthei-thâ€œ+â€token: e0 = (0,1,0,i,âˆ’1,âˆ’1,âˆ’1,0,0,ape ).
i,+ i,+
â€¢ Embeddingoftheâ€œ=â€token: e0 = (0,1,0,k+1,âˆ’1,âˆ’1,âˆ’1,0,0,ape ).
= =
â€¢ Embeddingofthe<SOS>token: e0 = (0,1,0,0,âˆ’1,âˆ’1,âˆ’1,0,0,ape ).
<SOS> <SOS>
23â€¢ Embeddingofthe<EOS>token: e0 = (0,0,1,0,âˆ’1,âˆ’1,âˆ’1,0,0,ape ).
<EOS> <EOS>
â€¢ Embeddingofoutputtokeno : e0 = (o ,0,0,0,i,imodm,âŒŠ i âŒ‹,pimodm,pâˆ’(imodm),ape ).
i o,i i m o,i
whereape istheabsolutepositionalencoding. Inthisconstruction,thefirstthreedimensionsofeach
Â·Â·Â·
initialembeddingrepresentthewordembedding,whilethelastsixdimensionsaccountfortheposition
embedding.
Block1. ThefirstblockoftheTransformercalculatesthefollowingvalues:
1. l : thenumberofprevioustokens(inclusive)a whereiâ€² = iandâŒŠjâ€² âŒ‹ = âŒŠj âŒ‹. l isonlydefined
i,j iâ€²,jâ€² m m i,j
oninputnumbertokensa . Ifthevalueisundefined,wewillsetl = âˆ’1.
i,j
2. f : f = 1ifnoprevioustokens(exclusive)a withâŒŠjâ€² âŒ‹ = âŒŠj âŒ‹. f isonlydefinedoninput
i,j i,j iâ€²,jâ€² m m i,j
numbertokensa . Ifthevalueisundefined,wewillsetf = âˆ’1.
i,j
Tocalculatethefirstvalue,wecansetthequery,key,valueandr inAppendixC.2asfollows:
(cid:16) (cid:17)
â€¢ Query: q = âˆ’1,2i,âˆ’i2,âˆ’1,2âŒŠj âŒ‹,âˆ’âŒŠj âŒ‹2 .
i,j m m
(cid:16) (cid:17)
â€¢ Key: k =
iâ€²2,iâ€²,1,âŒŠjâ€² âŒ‹2,âŒŠjâ€²
âŒ‹,1 .
iâ€²,jâ€² m m
â€¢ Value: v = (ape ).
iâ€²,jâ€² iâ€²,jâ€²
â€¢ r: r = âˆ’ape .
iâ€²,jâ€² iâ€²,jâ€²
We can calculate the values required in query or key by previous MLPs using Lemma C.1. Thus
(cid:16) (cid:17)2
âŸ¨q ,k âŸ© = âˆ’ âŒŠjâ€² âŒ‹âˆ’âŒŠ j âŒ‹ âˆ’(iâ€² âˆ’i)2, which implies that âŸ¨q ,k âŸ© = 0 if âŒŠjâ€² âŒ‹ = âŒŠj âŒ‹ and
i,j iâ€²,jâ€² m m i,j iâ€²,jâ€² m m
i = iâ€², and âŸ¨q ,k âŸ© â‰¤ âˆ’1 otherwise. By Lemma C.5, we can use one attention head to copy the
i,j iâ€²,jâ€²
absolute position jâ€²â€² of the first token satisfying the conditions. Finally, the number of the tokens is
j âˆ’jâ€²â€²+1.
Tocalculatethesecondvalue,wecansetthequery,key,andr inAppendixC.2asfollows:
(cid:16) (cid:17)
â€¢ Query: q = âˆ’1,2âŒŠj âŒ‹,âˆ’âŒŠj âŒ‹2 .
i,j m m
(cid:16) (cid:17)
â€¢ Key: k =
âŒŠjâ€² âŒ‹2,âŒŠjâ€²
âŒ‹,1 .
iâ€²,jâ€² m m
â€¢ Value: v = (ape ).
iâ€²,jâ€² iâ€²,jâ€²
â€¢ r: r = âˆ’ape .
iâ€²,jâ€² iâ€²,jâ€²
(cid:16) (cid:17)2
Thus âŸ¨q ,k âŸ© = âˆ’ âŒŠjâ€² âŒ‹âˆ’âŒŠ j âŒ‹ , which implies that âŸ¨q ,k âŸ© = 0 if âŒŠjâ€² âŒ‹ = âŒŠj âŒ‹, and
i,j iâ€²,jâ€² m m i,j iâ€²,jâ€² m m
âŸ¨q ,k âŸ© â‰¤ âˆ’1 otherwise. By Lemma C.5, we can use one attention head to copy the absolute
i,j iâ€²,jâ€²
positionjâ€²â€² ofthefirsttokensatisfyingtheconditions. Finally,wehavef = 1ifjâ€²â€² = j. Itsufficesto
i,j
determinewhetherjâ€²â€² = j. Noticethat1 = ReLU[1âˆ’(j âˆ’jâ€²â€²)2],thuswecancalculatethevalueof
jâ€²â€²=j
f byaconstant-widthMLPusingLemmaC.1.
i,j
Finally,ifthevaluesareundefined,thosevalueswillbesetasâˆ’1intheMLPstagebyusingconditional
selection (Lemma C.3) using information in positional embeddings. To sum up, the new embeddings
generatedinthisblockcanbesummarizedas: e1 = (l,f). Alltheseembeddingswillbeconcatenated
withtheoriginalembeddings.
Block2. ThesecondblockoftheTransformeristailoredtoexecutethefirstthreelinesofAlgorithm2.
Inthisblock,themodelaggregatestheadjacentmbitstoderives viatheattentionmechanism. Foreach
i
tokena ,denotet asthenumberofprevioustokens(inclusive)a whereâŒŠj âŒ‹ = âŒŠjâ€² âŒ‹. Thesecond
i,j i,j iâ€²,jâ€² m m
blockoftheTransformercalculatesthefollowingvalues:
1. 1 wheret isdefinedabove. Ifthevalueisundefined,wewillsetthevalueasâˆ’1.
ti,j i,j
242. c : themeanvalueappliedacrossa pjâ€²modm forprevioustokens(inclusive)a whereâŒŠjâ€² âŒ‹ =
i,j iâ€²,jâ€² iâ€²,jâ€² m
âŒŠj âŒ‹. Ifthevalueisundefined,wewillsetthevalueasâˆ’1.
m
Thenwecangets = c t whereiisthemaximizevaluesuchthatthelengthofa isgreaterthan
w i,mw i,mw i
mk.
Tocalculatethefirstvalue,wefirstsetthequery,keyandvalueasfollows:
(cid:16) (cid:17)
â€¢ Query: q = âˆ’1,2âŒŠj âŒ‹,âˆ’âŒŠj âŒ‹2 .
i,j m m
(cid:16) (cid:17)
â€¢ Key: k =
âŒŠjâ€² âŒ‹2,âŒŠjâ€²
âŒ‹,1 .
iâ€²,jâ€² m m
â€¢ Value: v = (f ).
iâ€²,jâ€² iâ€²,jâ€²
(cid:16) (cid:17)2
These values can be calculated using Lemma C.1. Thus âŸ¨q ,k âŸ© = âˆ’ âŒŠjâ€² âŒ‹âˆ’âŒŠ j âŒ‹ , which
i,j iâ€²,jâ€² m m
impliesthatâŸ¨q ,k âŸ© = 0ifâŒŠjâ€² âŒ‹ = âŒŠj âŒ‹,andâŸ¨q ,k âŸ© â‰¤ âˆ’1otherwise. Bythedefinitionoff
i,j iâ€²,jâ€² m m i,j iâ€²,jâ€² i,j
andLemmaC.6,wecangettheattentionoutputis 1 ,asdesired.
ti,j
Tocalculatethesecondvalue,wecansetthequery,keyandvalueasfollows:
(cid:16) (cid:17)
â€¢ Query: q = âˆ’1,2âŒŠj âŒ‹,âˆ’âŒŠj âŒ‹2 .
i,j m m
(cid:16) (cid:17)
â€¢ Key: k =
âŒŠjâ€² âŒ‹2,âŒŠjâ€²
âŒ‹,1 .
iâ€²,jâ€² m m
â€¢ Value: v = (a pjâ€²modm).
iâ€²,jâ€² iâ€²,jâ€²
Similar to the calculation of the first value, these values can be calculated using Lemma C.1. Thus
(cid:16) (cid:17)2
âŸ¨q ,k âŸ© = âˆ’ âŒŠjâ€² âŒ‹âˆ’âŒŠ j âŒ‹ ,whichimpliesthatâŸ¨q ,k âŸ© = 0ifâŒŠjâ€² âŒ‹ = âŒŠj âŒ‹,andâŸ¨q ,k âŸ© â‰¤
i,j iâ€²,jâ€² m m i,j iâ€²,jâ€² m m i,j iâ€²,jâ€²
âˆ’1otherwise. ByLemmaC.6,theattentionoutputisc ,asdesired.
i,j
Finally,ifthevaluesareundefined,thosevalueswillbesetasâˆ’1intheMLPstagebyusingconditional
selection (Lemma C.3) using information in positional embeddings. To sum up, the new embeddings
generatedinthisblockcanbesummarizedas: e2 = (1,c). Alltheseembeddingswillbeconcatenated
t
withtheoriginalembeddings.
Block3. ThethirdblockoftheTransformercalculatesthevalueofc t . Wewillfirstcalculatet
i,j i,j i,j
byattentionlayerand 1 fromthelastblock,thencalculatec t usingLemmaC.1.
ti,j i,j i,j
Noticethatt willnotexceedtheabsolutepositionalofthecurrenttoken. Wesetthequery,keyand
i,j
valueasfollows:
(cid:18) (cid:19)
â€¢ Query: e = 1 ,âˆ’ 2 ,1 .
i,j t2
i,j
ti,j
(cid:16) (cid:17)
â€¢ Key: k = ape2 ,ape ,1 .
iâ€²,jâ€² iâ€²,jâ€² iâ€²,jâ€²
â€¢ Value: v = (ape ).
iâ€²,jâ€² iâ€²,jâ€²
ThesevaluescanbecalculatedusingLemmaC.1. ThusâŸ¨q ,k âŸ© =
âˆ’(cid:16)
ape iâ€²,jâ€²
âˆ’1(cid:17)2
,whichimplies
i,j iâ€²,jâ€² ti,j
that âŸ¨q ,k âŸ© = 0 if ape = t , and âŸ¨q ,k âŸ© â‰¤ âˆ’ 1 otherwise since t â‰¤ nk. By
i,j iâ€²,jâ€² iâ€²,jâ€² i,j i,j iâ€²,jâ€² n2k2 i,j
LemmaC.6,theattentionoutputist ,asdesired.
i,j
Finally,wecancalculatethevalueofc t usingthesubsequentMLP.Tosumup,thenewembeddings
i,j i,j
generatedinthisblockcanbesummarizedas: e3 = (ct). Alltheseembeddingswillbeconcatenatedwith
theoriginalembeddings.
Block4. ThisblockoftheTransformerexecutesthefourthlineofAlgorithm2,i.e.,decomposec t
i,j i,j
asb pm+q . Noticethatb â‰¤ i,thusb willnotexceedtheabsolutepositionalofthecurrenttoken.
i,j i,j i,j i,j
Wecansetthequery,keyandvalueasfollows:
25â€¢ Query: q = (cid:0) âˆ’(c t + 1)2,2pm(c t + 1),âˆ’p2m(cid:1) .
i,j i,j i,j 2 i,j i,j 2
â€¢ Key: k = (cid:0) 1,ape âˆ’ 1,(ape âˆ’ 1)2(cid:1) .
iâ€²,jâ€² iâ€²,jâ€² 2 iâ€²,jâ€² 2
â€¢ Value: v = ape .
iâ€²,jâ€² iâ€²,jâ€²
ThesevaluescanbecalculatedusingLemmaC.1. Thus
(cid:20) (cid:18) 1(cid:19) 1(cid:21)2
âŸ¨q ,k âŸ© = âˆ’ c t âˆ’ ape âˆ’ pm+ ,
i,j iâ€²,jâ€² i,j i,j iâ€²,jâ€² 2 2
(cid:16) (cid:17)2 (cid:16) (cid:17)2
which implies that |âŸ¨q ,k âŸ©| â‰¤
pmâˆ’1
if ape =
âŒŠci,jti,jâŒ‹,
and âŸ¨q ,k âŸ© â‰¤ âˆ’
pm+1
i,j iâ€²,jâ€² 2 iâ€²,jâ€² pm i,j iâ€²,jâ€² 2
otherwise. UsingLemmaC.7,
(pm+1)2âˆ’(pmâˆ’1)2 4
c = â‰¥ ,
(pmâˆ’1)2 pm
thus 1/c = O(pm) = O(k). This implies we can set the query, key and value, such that the attention
outputisâŒŠci,jti,jâŒ‹ = b . Finally,wecalculateq byq = c t âˆ’pmb usingthesubsequentMLP.
pm i,j i,j i,j i,j i,j i,j
Thenewembeddingsgeneratedinthisblockise4 = (b,q).
Block 5. This block of the Transformer computes q +b for s . Recall that, s = c t
w+1 w w w i,mw i,mw
whereiisthemaximizevaluesuchthatthelengthofa isgreaterthanmw. Wewanttocalculatethose
i
valuesatthecorrespondingpositions.
First,weuseattentionlayertoCOPYq fortokena definedabove. Noticethatwecanalwaysget
w+1 i
thecorrectvaluesincethepositionwithcorrectvalueofs isalwaysinfrontofthatwithcorrectvalue
w+1
ofs . Tofinishthis,weusetheattentionmechanismtoCOPYfromthepositionofthevalues . This
k w+1
canbeimplementedbysettingquery,key,valueandr inAppendixC.2:
(cid:16) (cid:17)
â€¢ Query: q = âˆ’1,2âŒŠj âŒ‹,âˆ’âŒŠj âŒ‹2,âˆ’1 .
i,j m m
(cid:16) (cid:17)
â€¢ Key: k = (âŒŠjâ€² âŒ‹âˆ’1)2,âŒŠjâ€² âŒ‹âˆ’1,1,(jâ€²modm)2 .
iâ€²,jâ€² m m
â€¢ Value: v = (q
,âŒŠjâ€²
âŒ‹).
iâ€²,jâ€² iâ€²,jâ€² m
â€¢ r: r = ape .
iâ€²,jâ€² iâ€²,jâ€²
We can calculate the values required in query or key by previous MLPs using Lemma C.1. Thus
(cid:16) (cid:17)2
âŸ¨q ,k âŸ© = âˆ’ âŒŠjâ€² âŒ‹âˆ’âŒŠ j âŒ‹âˆ’1 âˆ’ (jâ€²modm)2, which implies that âŸ¨q ,k âŸ© = 0 if âŒŠjâ€² âŒ‹ =
i,j iâ€²,jâ€² m m i,j iâ€²,jâ€² m
âŒŠj âŒ‹+1andjâ€²modm = 0,andâŸ¨q ,k âŸ© â‰¤ âˆ’1otherwise.
m i,j iâ€²,jâ€²
By Lemma C.5, we can use one attention head to copy the value of q and
âŒŠjâ€²
âŒ‹ from the last
iâ€²,jâ€² m
token satisfying the conditions. This indicates the first dimension of attention output is q if there
w+1
existsaninputnumberwithlengthgreaterthanm(w+1),asdesired. Otherwise,q shouldbezero,
w+1
whiletheattentionoutputisundefinedabove. Wecandistinguishbetweenthesetwocasesbychecking
the second dimension of attention output. This is because if no input numbers has the length greater
than m(k + 1), then the second dimension of the attention output will be at most âŒŠj âŒ‹. We can use
m
LemmaC.3todistinguishtwocasesandsetq = 0ifnecessary. Finally,wecanusethesubsequent
w+1
MLPtocalculatethecorrectvalueofq +b fors . Furthermore,wealsousetheMLPtocalculate
w+1 w w
1 , 1 , 1 , 1 by
qw+1+bwâ‰¥pm qw+1+bwâ‰¤pmâˆ’2 bwâ‰¥pm bwâ‰¤pmâˆ’2
1 = ReLU[q +b âˆ’(pmâˆ’1)]âˆ’ReLU[q +b âˆ’pm]
qw+1+bwâ‰¥pm w+1 w w+1 w
andLemmaC.2.
Tosumup,theembeddingsgeneratedinthisblockare:
26â€¢ Positionswiththecorrectvalueofs :
w
e5 = (q +b ,b ,1 , 1 , 1 , 1 ,w).
w+1 w w qw+1+bwâ‰¥pm qw+1+bwâ‰¤pmâˆ’2 bwâ‰¥pm bwâ‰¤pmâˆ’2
â€¢ Otherpositions: e6 = (âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1). (Thiscanbeimplementedbyfilteringoutthe
unfeasiblevaluesusingLemmaC.3).
Block6. ThisblockoftheTransformerfirstcalculatesthefollowingvaluesforthepositionswiththe
correctvalueofs :
w
â€¢ Theminimumw â‰¥ w suchthat1 = 1.
1 qw+1+bwâ‰¥pm
â€¢ Theminimumw â‰¥ w suchthat1 = 1.
2 qw+1+bwâ‰¤pmâˆ’2
BotharestandardCOPYoperation, thuscanbeimplementedbyLemmaC.5. Tomakesurethevalue
ofw ,w isvalid(i.e., theexistenceofw ,w ), wecanCOPY1 ,1 and
1 2 1 2 qw1+1+bw1â‰¥pm qw2+1+bw2â‰¤pmâˆ’2
checkwhethertheyare1,andfilterouttheinvalidvaluesbyMLPusingLemmaC.3. Theembeddings
generatedinthisblockare:
â€¢ Positionswiththecorrectvalueofs : e6 = (w ,w ).
w 1 2
â€¢ Otherpositions: e6 = (âˆ’1,âˆ’1). (Thiscanbeimplementedbyfilteringouttheunfeasiblevalues
usingLemmaC.3).
Block 7. The last block of the Transformer executes the final four steps of Algorithm 2. This layer
calculatesthecarry-overbitscandpm-adicrepresentationofthefinaloutputoviatheattentionmechanism
andtheMLP,subsequentlyconvertingthepm-adicnumberintoap-adicnumber.
The computation of carry-on bits, as described in Equation (8) within Algorithm 2, adheres to the
followingequations:
i = max{w â‰¤ i | q +b â‰¥ pm},
âˆ§ w wâˆ’1
i = max{w â‰¤ i | q +b â‰¤ pmâˆ’2}, (11)
âˆ¨ w wâˆ’1
c = 1 .
i iâˆ§>iâˆ¨
Intheattentionlayer,operationsarerestrictedtooutputtokensandothertokenswillmaintaintheembed-
dingsviatheresidualconnectionandthefilteroperationbyMLP.Letâ€™sconsiderthetokeno ,
(i+1)m+j+1
wherej âˆˆ {0,Â·Â·Â· ,mâˆ’1},wewanttopredictthenexttokeno . ThemodelexecutestheCOPY
(i+1)k+j
operation,duplicatingthepreviousembeddingstoextractq +b ,i ,andi . Theextractionissimilar
i+1 i âˆ§ âˆ¨
topreviousblocks,buthereweonlyneedtofocusonpositionswithcorrectvalueofs . Tofindoutthe
w
valueofi ,i ,wefirstCOPYtheembeddingofthepositionwiththecorrectvalueofs ,andfindthe
âˆ§ âˆ¨ i
minimumwâ€² whichsharesthesamevalueofw ,w withs . Again,thiscanbeimplementedbyseveral
1 2 i
COPYoperationwithLemmaC.5.
Thecarry-overbitc andthepm-adicresultsoËœ arethencomputedasfollows:
i i+1
c = 1 , oËœ = b +c +q .
i iâˆ§>iâˆ¨ i+1 i i i+1
Thiscomputationisfacilitatedbyaconstant-sizeMLP.Subsequently,fortheoutputtokenoËœ ,
(i+1)k+j
the result o = oËœ modpj+1 is required. We first calculate oËœ /pj+1 using the positional
(i+1)k+j i+1 i+1
embeddingandLemmaC.1,thencalculateâŒŠoËœ /pj+1âŒ‹usingthesimilarfashiontowhatwedidinBlock
i+1
4,andthencalculateoËœ modpj+1 usingMLP.Finally,wecangetthevalueofâŒŠoËœi+1modpj+1 âŒ‹usingthe
i+1 pj
similarfashiontowhatwedidinBlock4.
Uponoutputtingthetokeno ,themodelanticipatesthe<EOS>token,employinganMLPtofilterthe
0
hiddenembeddingsandoutputthewordembeddingfor<EOS>. Thus,thefinaloutputfromthislayeris
characterizedbytheequation:
(cid:40)
(o ,i,0) ifi > 0,
e7 = iâˆ’1
o,i
(âˆ’1,âˆ’1,1) ifi = 0.
27PredictNextToken. Giventheoutputembeddingsofthelasttransformerlayere7 ,andtheword
o,i
embeddings,thetransformercansimplypredictthenexttokenbyfindingthenearestwordembeddings.
Inthisconstruction,thenormoftheparametersisboundedbypoly(n,k),therefore,thisconstruction
canbeimplementedbyalog-precisiontransformerwitharbitrarilysmallerror.
Theorem 5.3. For any integers n, l, and k, there exists a polynomial f(n) such that a logarithmic-
precisionTransformerwithconstantdepthandhiddendimensionsf(n)cangeneratethecorrectoutput
foranyinputontheMUL(n,l,p)task.
Here,wefirstdescribeanalgorithmtoperformMul(n,l,p)(Algorithm3)andprovethecorrectness
ofAlgorithm3. Then,weconstructaTransformerwiththeconfigurationsinTheorem5.3capablefor
simulatingAlgorithm3.
Algorithm3:Mul(n,l,p)Algorithm
Input : Twop-adicnumbersa,bnolongerthannbits,truncatinglengthl
Output: o := abmodpl
m = âŒˆlog nâŒ‰+1;
1 p
Computetheproductofeachpairofbits: d = a b ;
2 i,j i j
Computeeachbitas
3
min(nâˆ’1,j)
(cid:88)
r = d
j k,jâˆ’k
k=max(0,jâˆ’(nâˆ’1))
forj = 0,Â·Â·Â· ,2nâˆ’1;
Combineneighboringmbits:
4
mâˆ’1
(cid:88)
s = r pj
i ik+j
j=0
fori = 0,Â·Â·Â· ,âŒŠ(2nâˆ’1)/mâŒ‹;
Decomposes bys = b pm+q ,whereq âˆˆ [0,pmâˆ’1]andb ,q âˆˆ N;
5 i i i i i i i
b = 0;
6 âˆ’1
foreachi = 0,Â·Â·Â· ,âŒŠ(2nâˆ’1)/mâŒ‹do
7
f = 1 ;
8 i qi+biâˆ’1â‰¥pm
g = 1 ;
9 i qi+biâˆ’1â‰¥pmâˆ’2
end
10
Computethecarry-onbitsc:
11
ï£« ï£¶
(cid:95) (cid:94)
c i = ï£­f j âˆ§ g kï£¸
0â‰¤jâ‰¤i jâ‰¤kâ‰¤i
fori = 0,Â·Â·Â· ,âŒŠ(2nâˆ’1)/mâŒ‹;
Computethepm-adicoutcomeoËœ: oËœ = (q +b +c )modpm fori = 0,Â·Â·Â· ,âŒŠ(2nâˆ’1)/mâŒ‹;
12 i i iâˆ’1 iâˆ’1
Covertpm-adicoËœtop-adico:
13
(cid:36) (cid:37)
oËœ modp(l+1)
j
o =
i pl
fori = jk+lwherel âˆˆ {0,Â·Â·Â· ,kâˆ’1},j âˆˆ Z;
LemmaE.2(AnalgorithmtoperformMul(n,l,p)). Algorithm3outputso = abmodpl forallinputs
a,b.
Proof. Itâ€™seasytoverify(cid:80) s pim accuratelyrepresentstheproductofa,b. Forthesubsequentsteps,
i i
theproofisthesameasthatofLemmaE.1sincetheysharethesameprocedures.
28Next,weprovidetheproofforTheorem5.3.
ProofforTheorem5.3. Now,wedemonstratethatalog-precisiontransformer,withaconstantdepth,a
fixednumberofattentionheads,andO(n2)embeddingdimensions,iscapableofsimulatingAlgorithm3.
Consequently,thismodelcanaccuratelygeneratecorrectoutputforanyinputintegersa,b.
InitialEmbeddings: Thetotallengthoftheinputsequenceisnolongerthan2(n+1). Wecategorize
the tokens into two classes: number tokens (0,1,Â·Â·Â· ,p âˆ’ 1) and auxiliary tokens (+, =, <SOS> and
<EOS>). Giventheparametersk,n,wedeterminetheparameterm = âŒˆlog kâŒ‰+1 â‰¥ 2,asspecifiedin
p
Algorithm3. Theembeddingsfortheseclassesaredefinedasfollows:
â€¢ Embeddingofinputtokena : u0 = (cid:0) a e ,0,âˆ’1,âˆ’1,0,1,i,0,ape (cid:1) .
i a,i i i+1 a,i
â€¢ Embeddingofinputtokenb : u0 = (cid:0) 0,b e ,âˆ’1,âˆ’1,0,2,i,0,ape (cid:1) .
i b,i i i+1 b,i
â€¢ Embeddingoftheâ€œÃ—â€token: u0 = (âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,4,âˆ’1,0,ape ).
Ã— Ã—
â€¢ Embeddingoftheâ€œ=â€token: u0 = (âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,5,âˆ’1,0,ape ).
= =
â€¢ Embeddingofthe<SOS>token: u0 = (âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,6,âˆ’1,0,ape ).
<SOS> <SOS>
â€¢ Embeddingofthe<EOS>token: u0 = (âˆ’1,âˆ’1,âˆ’1,âˆ’1,âˆ’1,7,âˆ’1,0,ape ).
<EOS> <EOS>
â€¢ Embeddingofoutputtokeno : u0 = (âˆ’1,âˆ’1,o ,e ,âˆ’1,3,i,pâˆ’(imodm),ape ).
i o,i i âŒŠi/mâŒ‹ o,i
wheree âˆˆ Rn isone-hotvector,andape isabsolutepositionalembedding. Inthisconstruction,the
i Â·Â·Â·
first 3n+3 dimensions of each initial embedding represent the word embedding, while the last three
dimensionsaccountsforthepositionembedding.
Block1. ThefirstblockoftheTransformerexecutesthefirstthreelinesofAlgorithm3. Tobespecific,
wefirstaggregatetheinputnumbera,btothepositionsofb ,andthencalculatethevaluesofr .
0 j
Toaggregatetheinputnumbera,btothepositionsofb ,wesetthequery,keyandvalueasfollows:
0
â€¢ Query: q = (e0[2n+2]),i.e.,q = (0)forinputnumbera,b,andq = (âˆ’1)otherwise.
â€¢ Key: k = (1).
â€¢ Value: v = e0[1,Â·Â·Â· ,2n].
ThusâŸ¨q,kâŸ© = 0forkeyofinputnumbertokens,andâŸ¨q,kâŸ© â‰¤ âˆ’1otherwise. ByLemmaC.6,theattention
outputis
1
(a ,Â·Â·Â· ,a ,b ,Â·Â·Â· ,b ).
0 nâˆ’1 0 nâˆ’1
ape âˆ’2
b,0
By Lemma C.1, we can use the subsequent MLP to get (a ,Â·Â·Â· ,a ,b ,Â·Â·Â· ,b ) given the value
0 nâˆ’1 0 nâˆ’1
of ape . Then we can calculate all d using the MLP, which requires O(n2) hidden dimension by
b,0 i,j
LemmaC.1.
Finally,wecalculate(r ,Â·Â·Â· ,r )by
2nâˆ’1 0
min(nâˆ’1,j)
(cid:88)
r = d .
j k,jâˆ’k
k=max(0,jâˆ’(nâˆ’1))
Block2. ThisblockoftheTransformerusesseveralMLPstoexecutesline4-12ofAlgorithm3. All
thecalculationsbelowarealsocalculatedatthepositionofb ,subsequenttowhatwedidinBlock1.
0
â€¢ Forthecalculationofs ,itâ€™seasytogetthevaluesvia(r ,Â·Â·Â· ,r ).
i 2nâˆ’1 0
29â€¢ Forthecalculationofb ,q ,noticethatb â‰¤ pm â‰¤ np2,thuswecanuse
i i i
np2
(cid:88)
b = ReLU(s âˆ’pm)
i i
j=0
foreachb ,whichrequiresO(n2)hiddendimensionintotalbyLemmaC.2. Thenq = s âˆ’b pm,
i i i i
whichcanbeeasilyimplementedbyMLPaswell.
â€¢ Forthecalculationoff ,g ,wecangetthosevaluesby
i i
f = ReLU[q +b âˆ’(pmâˆ’1)]âˆ’ReLU[q +b âˆ’pm],
i i iâˆ’1 i iâˆ’1
g = ReLU[q +b âˆ’(pmâˆ’2)]âˆ’ReLU[q +b âˆ’(pmâˆ’1)]
i i iâˆ’1 i iâˆ’1
andLemmaC.2,whichrequiresO(n)hiddendimensionintotal.
â€¢ Forthecalculationofc ,noticethat
i
(cid:32) Î³ (cid:33) (cid:32) Î³ (cid:33)
(cid:94) (cid:88) (cid:95) (cid:88)
Î± = ReLU Î± âˆ’Î³ +1 , Î± = 1âˆ’ReLU 1âˆ’ Î± .
i i i i
1â‰¤iâ‰¤Î³ i=1 1â‰¤iâ‰¤Î³ i=1
CombiningwithLemmaC.2,wecancalculatethevalueofeachc withO(n)hiddendimension.
i
â€¢ Finally, for the calculation of oËœ, we can use the similar fashion of the calculation of q . Since
i i
q +b +c < 2pm,wecancalculateeachoËœ usingconstanthiddendimension,whichimplies
i iâˆ’1 iâˆ’1 i
wecancalculateoËœusingO(n)hiddendimensionintotal.
Block3. ThelastblockoftheTransformerexecutesthelaststepofAlgorithm3. Letâ€™sconsiderthe
token o , where j âˆˆ {0,Â·Â·Â· ,mâˆ’1}, we want to predict the next token o . We first
(i+1)m+j+1 (i+1)k+j
COPYthevalueofoËœfromthepositionofb ,thenextractsoËœ byoËœ = âŸ¨oËœ,e âŸ©usingthepositional
0 i+1 i+1 i+1
embeddingofu0 .
o,i
Subsequently, for the output token o , the result o = oËœ modpj+1 is required. We
(i+1)k+j (i+1)k+j i+1
first calculate o /pj+1 using the positional embedding and Lemma C.1, then calculate âŒŠoËœ /pj+1âŒ‹
i+1 i+1
usingthesimilarfashiontowhatwedidwhencalculatings ,b inBlock2. SinceoËœ < 2pm â‰¤ np2,this
i i i+1
canbeimplementedbyaMLPwithO(n)hiddendimension. ThenwecancalculateoËœ modpj+1 using
i+1
MLP.Similarly,wecanfinallygetthevalueofâŒŠoËœi+1modpj+1
âŒ‹usingaMLPwithO(n)hiddendimension.
pj
Uponoutputtingthetokeno ,themodelanticipatesthe<EOS>token,employinganMLPtofilterthe
0
hiddenembeddingsandoutputthewordembeddingfor<EOS>. Thus,thefinaloutputfromthislayeris
characterizedbytheequation:
(cid:40)
(o ,i,3) ifi > 0,
e3 = iâˆ’1
o,i
(âˆ’1,âˆ’1,7) ifi = 0.
PredictNextToken. Giventheoutputembeddingsofthelasttransformerlayere3 ,andtheword
o,i
embeddings,thetransformercansimplypredictthenexttokenbysoftmax.
Inthisconstruction,thenormoftheparametersisboundedbyO(n2),therefore,thisconstructioncan
beimplementedbyalog-precisiontransformerwitharbitrarilysmallerror.
F ExperimentalDetails
Inthissection,wepresenttheexperimentaldetails.
F.1 Datasets
TheiteratedadditionandintegeradditiondataaregeneratedaccordingtoAlgorithm4. Themultiplication
dataaregeneratedaccordingtoAlgorithm5. Bothdatasetsareusedonlinefortrainingandtesting.
30Algorithm4:IteratedAdditionDataGeneration
Functionlarge_number_add(a,b,base):
1
Input: a: Listofdigitsofthefirstnumber
2
b: Listofdigitsofthesecondnumber
3
base: Thenumericalbase
4
Output: result: Listofdigitsofthesumofaandb
5
carryâ† 0,resultâ† []
6
max_lengthâ†max(length(a),length(b))
7
fori â† 0tomax_length-1do
8
sumâ†carry
9
ifi <length(a)then
10
sumâ†sum+a[i]
11
end
12
ifi <length(b)then
13
sumâ†sum+b[i]
14
end
15
carryâ†floor(sum/base)
16
result.append(summodbase)
17
end
18
ifcarryÌ¸= 0then
19
result.append(carry)
20
end
21
returnresult
22
Functionget_data(batch,length,num_count,base):
23
Input:
24
batch: Numberofsamples
25
length: Maximumlengthofaddends
26
num_count: Numberofaddends
27
base: Thenumericalbase
28
Output: tokenized_data: Tensorofgeneratedsequences
29
dataâ†randomintegersinrange[0,base) withshape (batch,length,num_count)
30
tokenized_dataâ† []
31
fori â† 0tobatchâˆ’1do
32
numbersâ†data[i,:,:]
33
stripleadingzerosofnumbersandgetstripped_numbers
34
fornuminnumbersdo
35
sum_digitsâ†large_number_add(sum_digits,num,base)
36
end
37
reversestripped_numbersandsum_digits
38
addtokenofâ€™+â€™andâ€™=â€™andâ€™<EOS>â€™toformsequencepadthesequenceintothesame
39
length
tokenized_data.append(sequence)
40
end
41
converttokenized_datatotensor
42
returntokenized_data
43
31Algorithm5:IntegerMultiplicationDataGeneration
Functionlarge_number_mult(a,b,base):
1
Input: a: Listofdigitsofthefirstnumber
2
b: Listofdigitsofthesecondnumber
3
base: Thenumericalbase
4
Output: result: Listofdigitsoftheproductofaandb
5
resultâ†[0]*(length(a)+length(b))
6
fori â† 0tolength(a)âˆ’1do
7
carryâ†0
8
forj â† 0tolength(b)âˆ’1do
9
productâ†a[i]âˆ—b[j]+result[i+j]+carry
10
carryâ†floor(product/base)
11
result[i+j]â†productmodbase
12
end
13
ifcarry>0then
14
result[i+length(b)]â†result[i+length(b)]+carry
15
end
16
end
17
stripleadingzerosfromresult
18
returnresult
19
Functionget_mult_data(batch,length,base):
20
Input:
21
batch: Numberofsamples
22
length: Maximumlengthofmultiplicands
23
base: Thenumericalbase
24
Output: tokenized_data: Tensorofgeneratedsequences
25
dataâ†randomintegersinrange[0,base) withshape (batch,length,2)
26
tokenized_dataâ† []
27
fori â† 0tobatchâˆ’1do
28
num_1â†data[i,:,0]
29
num_2â†data[i,:,1]
30
stripleadingzerosofnumbersandgetstripped_numbers
31
product_digitsâ†large_number_mult(num_1,num_2,base)
32
reversestripped_numbersandproduct_digits
33
addtokenofâ€™Ã—â€™andâ€™=â€™andâ€™<EOS>â€™toformsequencepadthesequenceintothesame
34
length
tokenized_data.append(sequence)
35
end
36
converttokenized_datatotensor
37
returntokenized_data
38
32Base2 Base10
Length float32Accuracy bfloat16Accuracy float32Accuracy bfloat16Accuracy
8 99.8% 99.6% 99.4% 99.0%
16 99.3% 98.4% 99.2% 98.1%
24 98.9% 96.3% 99.2% 97.4%
32 99.3% 95.9% 99.2% 94.1%
Table2: Evaluationofintegeradditionaccuracyacrossvariouslengthwithboth32-bitand16-bitprecision.
F.2 ModelTraining
We tried 3 different seeds and select the maximum accuracy for each task. All the experiments were
conductedonasingleNVIDIAGeForceRTX4090GPUoveradurationoftwoweeks.
F.3 IntegerAdditionResults
TheresultsoftheexperimentsarepresentedinTable2.
GenerationConfiguration
TopK 50
TopP 0.95
Temperature 0.1
Table3: GenerationConfigurationforLLAMA3.18BInstructinarithmetictasks.
F.4 PromptForLLM
DetailedpromptsforthethreeelementaryarithmetictasksarelistedintheTables4and5andgeneration
configurationcanbefoundintheTable3.
PromptforLLAMA3.18BInstructinIntegerAdditionandIteratedAdditiontasks.
Pleasedirectlycalculatethefollowingarithmeticexpressioninbasebasewiththefollowingformat:
<Expression>=<Result>
Itisimportantthatyoushouldnotshowanyintermediatestepsinyourcalculationprocess.
Thefinalanswershouldbecomputedinonestepandprovidedthefinalresultimmediatelywithoutany
explanation.
Herearesomeexamples
32+78=110
1234+4567+2134+4567=12502
2135+523+2135+523=5316
2314+4567+2314+4567=13762
ArithmeticExpression:
<Expression>
Table4: PromptforLLAMA3.18BInstructinIntegerAdditionandIteratedAdditiontasks.
33PromptforLLAMA3.18BInstructinIntegerMultiplicationtask.
Pleasedirectlycalculatethefollowingarithmeticexpressioninbasebase.
Itisimportantthatyoushouldnotshowanyintermediatestepsinyourcalculationprocess.
Thefinalanswershouldbecomputedinonestepandprovidedthefinalresultimmediatelywithoutany
explanation.
Herearesomeexamples
Examples:
32*56=1792
867*467=404889
123*456=56088
ArithmeticExpression:
<Expression>
Table5: PromptforLLAMA3.18BInstructinIntegerMultiplicationtask.
34