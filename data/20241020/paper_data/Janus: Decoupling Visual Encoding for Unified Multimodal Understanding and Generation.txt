Janus: Decoupling Visual Encoding for Unified
Multimodal Understanding and Generation
ChengyueWu1,2 XiaokangChen1,‚àó,‚Ä† ZhiyuWu1,3 YiyangMa1,3 XingchaoLiu1 ZizhengPan1
WenLiu1 ZhendaXie1 XingkaiYu1 ChongRuan1 PingLuo2,‚àó
1DeepSeek-AI 2TheUniversityofHongKong 3PekingUniversity
‚Ä†: Projectlead ‚àó: Correspondingauthors
ProjectPage: https://github.com/deepseek-ai/Janus
Abstract
Inthispaper,weintroduceJanus,anautoregressiveframeworkthatunifiesmultimodalunder-
standingandgeneration. Priorresearchoftenreliesonasinglevisualencoderforbothtasks,
suchasChameleon. However,duetothedifferinglevelsofinformationgranularityrequiredby
multimodalunderstandingandgeneration,thisapproachcanleadtosuboptimalperformance,
particularlyinmultimodalunderstanding. Toaddressthisissue,wedecouplevisualencoding
into separate pathways, while still leveraging a single, unified transformer architecture for
processing. Thedecouplingnotonlyalleviatestheconflictbetweenthevisualencoder‚Äôsrolesin
understandingandgeneration,butalsoenhancestheframework‚Äôsflexibility. Forinstance,both
themultimodalunderstandingandgenerationcomponentscanindependentlyselecttheirmost
suitableencodingmethods. ExperimentsshowthatJanussurpassespreviousunifiedmodel
andmatchesorexceedstheperformanceoftask-specificmodels. Thesimplicity,highflexibility,
andeffectivenessofJanusmakeitastrongcandidatefornext-generationunifiedmultimodal
models.
1. Introduction
Inrecentyears,multimodallargemodelshavemadesignificantadvancementsinbothunder-
standingandgenerationdomains[20,51]. Inthefieldofmultimodalunderstanding,researchers
followthedesignofLLaVA[51]byusingavisionencoderasabridgetoenablelargelanguage
models (LLMs) to understand images. In the field of visual generation, diffusion-based ap-
proaches [9, 20, 20, 67] have seen notable success. More recently, some works have explored
autoregressivemethodsforvisiongeneration[73,79], achievingperformancecomparableto
diffusionmodels. Tobuildmorepowerfulandgeneralistmultimodalmodels,researchershave
soughttocombinemultimodalunderstandingandgenerationtasks[75,77,94]. Forinstance,
somestudieshaveattemptedtoconnectmultimodalunderstandingmodelswithpretraineddif-
fusionmodels[27,28,75]. Forexample,Emu[75]usestheoutputoftheLLMasaconditionfora
pretraineddiffusionmodel,andthenreliesonthediffusionmodeltogenerateimages. However,
strictlyspeaking,thisapproachcannotbeconsideredatrulyunifiedmodel,becausethevisual
generationfunctionalityishandledbytheexternaldiffusionmodel,whilethemultimodalLLM
itselflacksthecapabilitytodirectlygenerateimages.
Otherapproaches[77,85,86,94]employasingletransformertounifybothmultimodalun-
4202
tcO
71
]VC.sc[
1v84831.0142:viXraPOPE
VQAv2 83.75 MME-Perception
73.75 1300.0
72.5
62.5 1100.0
61.25
GenEval 57.75 51.25 900.0 65.0MMBench
48.5 45.0
39.25 25.0
16.0 17.5
22.0 38.7540.0 25.0
28.0 32.5
47.5 50.0
MMMU MM-Vet
56.25 60.0 MobileVLM (1.4B)
LLaVA-Phi (2.7B)
InstructBLIP (7B)
Show-o (1.3B)
GQA SEED-Bench Janus (Ours, 1.3B)
(a) BenchmarkPerformance. (b) VisualGenerationResults.
Figure1|MultimodalunderstandingandvisiongenerationresultsfromourJanus. Janusoutperforms
thepreviousstate-of-the-artunifiedmultimodalmodelsaswellassometask-specificmultimodalunder-
standingmodels,whilealsodemonstratingstrongvisualgenerationcapabilities. Theimageresolutionis
384√ó384. Bestviewedonscreen.
derstandingandgenerationtasks,whichimprovesinstruction-followingforvisualgeneration,
unlockspotentialemergentabilities,andreducesmodelredundancy. Suchmethodstypically
useasinglevisionencodertoprocessinputsforbothtwotasks. However,therepresentations
requiredbymultimodalunderstandingandgenerationtasksdiffersignificantly. Inmultimodal
understandingtasks,thepurposeofthevisionencoderistoextracthigh-levelsemanticinforma-
tion(e.g.,objectcategoriesorvisualattributeswithinanimage). Theoutputofunderstanding
tasknotonlyinvolvesextractinginformationfromimagesbutalsoinvolvescomplexsemantic
reasoning. Therefore, the granularity of the vision encoder‚Äôs representation tends to mainly
focusonhigh-dimensionalsemanticrepresentation. Bycontrast,invisualgenerationtasks,the
mainfocusisongeneratinglocaldetailsandmaintainingglobalconsistencyintheimage. The
representationinthiscontextnecessitatesalow-dimensionalencodingthatiscapableoffine-
grainedspatialstructureandtexturaldetailexpression. Unifyingtherepresentationsofthese
two tasks within the same space will lead to conflicts and trade-offs. Consequently, existing
unified models for multimodal understanding and generation often compromise on multi-
modalunderstandingperformance,fallingmarkedlyshortofthestate-of-the-artsmultimodal
understandingmodels. Weexplorethisissuefurtherintheablationstudy.
Tosolvethisproblem,weproposeJanus1,aunifiedmultimodalframeworkthatdecouples
visual encoding for multimodal understanding and generation. Specifically, we introduce
twoindependentvisualencodingpathways: oneformultimodalunderstandingandonefor
multimodalgeneration,unifiedbythesametransformerarchitecture. Theproposedmethod
offerstwomainbenefits: (1)Janusalleviatestheconflictstemmingfromthedifferentgranular
needsofmultimodalunderstandingandgenerationandeliminatestheneedtomaketrade-offs
betweentwotaskswhenselectingvisualencoders. (2)Janusisflexibleandextensible. After
decoupling,boththeunderstandingandgenerationtaskscanadoptstate-of-the-artencoding
1InRomanmythology,Janusisthegodofdualityandtransitions,symbolizingthecoexistenceofcontradictory
forcesbyhavingtwofaces,eachlookinginoppositedirections.Similarly,ourmodelcapturestheinherenttension
between vision tasks: understanding demands abstract, high-level semantic representations, while generation
requiresconcrete,detailedinformation.Bydecouplingtheseprocessesintospecializedencoders,oursystemmirrors
Janus‚Äôsdualnature,resolvingthistensionwithinaunifiedarchitecture.
2techniquesspecifictotheirdomain. Moreover,itispossibleforJanustoaccommodateadditional
inputtypesinthefuture,suchaspointclouds,EEGsignals,oraudiodata,whereindependent
encoderscanextractfeaturesandthenuseaunifiedtransformertoprocessthem.
To the best of our knowledge, we are the first to highlight the importance of decoupling
visual encoding within the unified multimodal understanding and generation framework.
OurexperimentalresultsshowthatJanussurpassesexistingunifiedmodelswithcomparable
parameter sizes on both multimodal understanding and generation benchmarks, achieving
state-of-the-artresults. Notably,Janusevenoutperformssometask-specificmodelswhichhave
significantlymoreparameters(Figure1). Specifically,onmultimodalunderstandingbenchmarks
MMBench[54],SEED-Bench[42],andPOPE[48],Janus(1.3B)achievedscoresof69.4,63.7,and
87.0,respectively,outperformingLLaVA-v1.5(7B)[50]andQwen-VL-Chat(7B)[3]. Onvisual
generationbenchmarksMSCOCO-30K[11]andGenEval[30],JanusachievedanFIDscoreof
8.53andanaccuracyof61%,surpassingtext-to-imagegenerativemodelssuchasDALL-E2[66]
andSDXL[62]. Webelievethatthestrongperformance,coupledwiththehighflexibilityand
extensibilityofJanus,presentsitasastrongcandidatefornext-generationunifiedmultimodal
models.
2. Related Work
2.1. VisualGeneration
Visual generation is a rapidly evolving field that combines concepts from natural language
processingwithadvancementsintransformerarchitectures. Autoregressivemodels,influenced
bythesuccessinlanguageprocessing,leveragetransformerstopredictsequencesofdiscrete
visual tokens (codebook IDs) [24, 65, 75]. These models tokenize visual data and employ a
prediction approach similar to GPT-style [64] techniques. Additionally, masked prediction
models [7, 8] draw upon BERT-style [19] masking methods, predicting masked sections of
visualinputstoimprovesynthesisefficiency,andhavebeenadaptedforvideogeneration[89].
Concurrently,continuousdiffusionmodelshaveshowcasedimpressivecapabilitiesinvisual
generation[33,67,71],complementingdiscretemethodsbyapproachinggenerationthrougha
probabilisticlens.
2.2. MultimodalUnderstanding
Multimodal large language models (MLLMs) integrate both text and images [6, 80, 81]. By
leveraging pretrained LLMs, MLLMs [1, 2, 12, 51, 55, 82, 95] demonstrate a robust ability
to understand and process multimodal information. Recent advancements have explored
extendingMLLMswithpretraineddiffusionmodelstofacilitateimagegeneration[27,29,36,75,
76]. Thesemethodsfallunderthecategoryoftoolutilization,wherediffusionmodelsareused
togenerateimagesbasedontheconditionsoutputbytheMLLM,whiletheMLLMitselfdoes
nothavetheabilitytodirectlyperformvisualgeneration. Moreover,thegenerativeabilityof
theentiresystemisoftenconstrainedbytheexternaldiffusionmodel,makingitsperformance
inferiortodirectlyusingthediffusionmodelonitsown[27,75].
2.3. UnifiedMultimodalUnderstandingandGeneration
Unified multimodal understanding and generation models are considered powerful for fa-
cilitating seamless reasoning and generation across different modalities [77, 94]. Traditional
approachesinthesemodelstypicallyuseasinglevisualrepresentationforbothunderstanding
3Understanding LanguageResponse:X ImageGeneration GeneratedImage:X
# !
TextDe-Tokenizer ‚Ä¶‚Ä¶ ImageDecoder
Auto-Regressive Transformer
‚Ä¶‚Ä¶
Und.Encoder TextTokenizer TextTokenizer Gen.Encoder
Image:X ! LanguageInstruct:X " LanguageInstruct:X " Image:X !
Figure2 | ArchitectureofourJanus. Differentfrompreviousapproaches[77,85]thattypically
assume visual understanding and generation require the same visual encoder, our Janus de-
couplesvisualencodingforvisualunderstandingandvisualgeneration. ‚ÄúUnd. Encoder‚Äùand
‚ÄúGen. Encoder‚Äùareabbreviationsfor‚ÄúUnderstandingEncoder‚Äùand‚ÄúGenerationEncoder‚Äù,
respectively. Bestviewedincolor.
and generation tasks, regardless of whether they are based on autoregressive (AR) models
[77,85]or diffusionmodels[86,94]. Forexample, Chameleon [77]adopts aVQTokenizerto
encode images for both multimodal understanding and generation. However, this practice
may lead to suboptimal outcomes, as the vision encoder might face a trade-off between the
demandsofunderstandingandgeneration. Incontrast,ourJanuscanexplicitlydecouplethe
visualrepresentationsforunderstandingandgeneration,recognizingthatdifferenttasksmay
requirevaryinglevelsofinformation.
3. Janus: A Simple, Unified and Flexible Multimodal Framework
3.1. Architecture
The architecture of Janus is shown in Figure 2. For pure text understanding, multimodal
understanding,andvisualgeneration,weapplyindependentencodingmethodstoconvertthe
rawinputsintofeatures, whicharethenprocessedbyanunifiedautoregressivetransformer.
Specifically, for text understanding, we use the built-in tokenizer of the LLM to convert the
text into discrete IDs and obtain the feature representations corresponding to each ID. For
multimodalunderstanding,weusetheSigLIP[92]encodertoextracthigh-dimensionalsemantic
featuresfromimages. Thesefeaturesareflattenedfroma2-Dgridintoa1-Dsequence,andan
understandingadaptorisusedtomaptheseimagefeaturesintotheinputspaceoftheLLM.For
visualgenerationtasks,weusetheVQtokenizerfrom[73]toconvertimagesintodiscreteIDs.
AftertheIDsequenceisflattenedinto1-D,weuseagenerationadaptortomapthecodebook
embeddingscorrespondingtoeachIDintotheinputspaceoftheLLM.Wethenconcatenate
thesefeaturesequencestoformamultimodalfeaturesequence,whichissubsequentlyfedinto
theLLMforprocessing. Thebuilt-inpredictionheadoftheLLMisutilizedfortextpredictions
inboththepuretextunderstandingandmultimodalunderstandingtasks,whilearandomly
initialized prediction head is used for image predictions in the visual generation task. The
entiremodeladherestoanautoregressiveframeworkwithouttheneedforspeciallydesigned
4StageI: StageII: StageIII:
Training AdaptorsandImageHead UnifiedPretraining SupervisedFine-tuning
Understanding ImageGeneration Understanding ImageGeneration Understanding ImageGeneration
TextHead ‚ùÑ ImageHeadüî• TextHead üî• ImageHead üî• TextHead üî• ImageHead üî•
LLM LLM LLM
‚ùÑ üî• üî•
Und.Adaptor üî• Gen. Adaptorüî• Und.Adaptor üî• Gen. Adaptorüî• Und.Adaptor üî• Gen. Adaptorüî•
Und. Encoder ‚ùÑ Gen. Encoder ‚ùÑ Und. Encoder ‚ùÑ Gen. Encoder ‚ùÑ Und. Encoder üî• Gen. Encoder ‚ùÑ
Figure3|OurJanusadoptsathree-stagetrainingprocedure. Weuseflamesymbols/snowflake
symbolsinthediagramtoindicatethemoduleupdates/doesnotupdateitsparameters.
attentionmasks.
3.2. TrainingProcedure
ThetrainingofJanusisdividedintothreestages,asillustratedinFigure3. Detailsareprovided
inthebelow.
StageI:TrainingAdaptorsandImageHead. Themaingoalofthisstageistocreateaconceptual
connectionbetweenvisualandlinguisticelementswithintheembeddingspace,enablingthe
LLM to understand the entities shown in images and have preliminary visual generation
ability. WekeepthevisualencodersandtheLLMfrozenduringthisstage,allowingonlythe
trainableparameterswithintheunderstandingadaptor,generationadaptorandimageheadto
beupdated.
StageII:UnifiedPretraining. Inthisstage,weperformunifiedpretrainingwithmultimodal
corpustoenableJanustolearnbothmultimodalunderstandingandgeneration. Weunfreezethe
LLMandutilizealltypesoftrainingdata: puretextdata,multimodalunderstandingdata,and
visualgenerationdata. InspiredbyPixart[9],webeginbyconductingsimplevisualgeneration
trainingusingImageNet-1ktohelpthemodelgraspbasicpixeldependencies. Subsequently,we
enhancethemodel‚Äôsopen-domainvisualgenerationcapabilitywithgeneraltext-to-imagedata.
StageIII:SupervisedFine-tuning. Duringthisstage,wefine-tunethepretrainedmodelwith
instruction tuning data to enhance its instruction-following and dialogue capabilities. We
fine-tuneallparametersexceptthegenerationencoder. Wefocusonsupervisingtheanswers
while masking system and user prompts. To ensure Janus‚Äôs proficiency in both multimodal
understandingandgeneration,wedon‚Äôtfine-tuneseparatemodelsforacertaintask. Instead,
weuseablendofpuretextdialoguedata,multimodalunderstandingdataandvisualgeneration
data,ensuringversatilityacrossvariousscenarios.
3.3. TrainingObjective
Janusisanautoregressivemodel,andwesimplyadoptthecross-entropylossduringtraining:
‚àëÔ∏Å
L = ‚àí logùëÉ ùúÉ(ùë• ùëñ|ùë• <ùëñ) (1)
ùëñ=1
Here, ùëÉ(¬∑ | ¬∑) indicates the conditional probability modeled by the weights ùúÉ of Janus. For
puretextunderstandingandmultimodalunderstandingtasks,wecomputethelossonthetext
5sequence. Forvisualgenerationtasks,wecomputethelossonlyontheimagesequence. Tokeep
thedesignsimple,wehavenotassigneddifferentlossweightstodifferenttasks.
3.4. Inference
During inference, our model adopts a next-token prediction approach. For pure text under-
standingandmultimodalunderstanding,wefollowthestandardpracticeofsamplingtokens
sequentially from the predicted distribution. For image generation, we utilize classifier-free
guidance(CFG)2,similartopriorworks[8,26,73]. Specifically,foreachtoken,thelogit ùëô ùëî is
calculated as: ùëô ùëî = ùëô ùë¢+ùë†(ùëô ùëê‚àíùëô ùë¢), where ùëô ùëê is the conditional logit, ùëô ùë¢ is the unconditional logit,
andùë†isthescalefortheclassifier-freeguidance. Thedefaultnumberofùë†is5forthefollowing
evaluation.
3.5. PossibleExtensions
Itisimportanttonotethatourdesign,whichfeaturesseparateencodersforunderstandingand
generation,isstraightforwardandeasytoextend.
Multimodal Understanding. (1) For the multimodal understanding component, a stronger
visionencodercanbechosenwithoutworryingaboutwhethertheencoderiscapableofhandling
visiongenerationtasks,suchasEVA-CLIP[74],InternViT[13],etc. (2)Tohandlehigh-resolution
images,dynamichigh-resolutiontechniques[50]canbeused. Thisallowsthemodeltoscaleto
anyresolution,withoutperformingpositionalembeddinginterpolationforViTs. Tokenscanbe
furthercompressedtosavecomputationalcost,forinstance,usingpixelshuffleoperation[12].
Visual Generation. (1) For visual generation, finer-grained encoders can be chosen in order
to preserve more image details after encoding, such as MoVQGan [93]. (2) Loss functions
specificallydesignedforvisualgenerationcanbeemployed,suchasdiffusionloss[46]. (3)A
combinationofAR(causalattention)andparallel(bidirectionalattention)methodscanbeused
inthevisualgenerationprocesstoreduceaccumulatederrorsduringvisualgeneration[79].
SupportforAdditionalModalities. ThestraightforwardarchitectureofJanusallowsforeasy
integration with additional encoders, accommodating various modalities such as 3D point
cloud[53],tactile[88],andEEG[4]. ThisgivesJanusthepotentialtobecomeamorepowerful
multimodalgeneralistmodel.
4. Experiments
Inthissection,wepresentaseriesofcomprehensiveexperimentsdesignedtoassesstheperfor-
manceofourmethodacrossarangeofvisualunderstandingandgenerationtasks. Webeginby
detailingourexperimentalsetup,whichincludesthemodelarchitecture,trainingdatasets,and
evaluationbenchmarks. Next,wereporttheperformanceofJanus,followedbyacomparison
withotherstate-of-the-artmodelsonvariousbenchmarksformultimodalunderstandingand
generation. Wealsoconductextensiveablationstudiestoverifytheeffectivenessoftheproposed
method. Lastly,weprovidesomequalitativeresults.
2Duringtraining,wereplacethetextconditioninthetext-to-imagedatawithapadtokenataprobabilityof10%,
enablingthemodeltohaveunconditionalvisualgenerationcapability.
6Table1 | DetailedhyperparametersofourJanus. Dataratioreferstotheratioofmultimodal
understandingdata,puretextdata,andvisualgenerationdata.
Hyperparameters Stage1 Stage2 Stage3
Learningrate 1.0√ó10‚àí3 1√ó10‚àí4 2.0√ó10‚àí5
LRscheduler Cosine Constant Constant
Weightdecay 0.0 0.0 0.1
Gradientclip 1.0 1.0 1.0
Optimizer AdamW(ùõΩ =0.9,ùõΩ =0.95)
1 2
Warm-upsteps 300 5,000 0
Trainingsteps 10,000 180,000 24,000
Batchsize 256 512 256
DataRatio 1:0:1 2:3:5 7:3:10
4.1. ImplementationDetails
Inourexperiments,weutilizeDeepSeek-LLM(1.3B)[5]withamaximumsupportedsequence
lengthof4096asthebaselanguagemodel. Forthevisionencoderusedinunderstandingtasks,
weselectSigLIP-Large-Patch16-384[92]. Thegenerationencoderhasacodebookofsize16,384
anddownsamplesimagesbyafactorof16. Boththeunderstandingadaptorandthegeneration
adaptor are two-layer MLPs. The detailed hyperparameters for each stage are provided in
Table 1. All images are resized to 384√ó384 pixels. For multimodal understanding data, we
resizethelongsideoftheimageandpadtheshortsidewiththebackgroundcolor(RGB:127,
127,127)toreach384. Forvisualgenerationdata,theshortsideisresizedto384,andthelong
sideiscroppedto384. Weusesequencepackingduringtrainingtoimprovetrainingefficiency.
We mix all data types according to the specified ratios in a single training step. Our Janus is
trained and evaluated using HAI-LLM [32], which is a lightweight and efficient distributed
trainingframeworkbuiltontopofPyTorch. Thewholetrainingprocesstook7daysonacluster
of16nodes,eachequippedwith8NvidiaA100(40GB)GPUs.
4.2. DataSetup
Inthissection,weprovidedetailsofthepretrainingandsupervisedfinetuningdatasets.
StageI.Weuseadatasetthatincludes1.25millionimage-textpairedcaptionsfromShareGPT4V[10]
formultimodalunderstandingandapproximately1.2millionsamplesfromImageNet-1k[18]for
visualgeneration. TheShareGPT4Vdataisformattedas‚Äú<image><text>‚Äù. TheImageNetdata
isorganizedintoatext-to-imagedataformatusingthecategorynames: ‚Äú<category_name><image>‚Äù.
Here,the‚Äú<>‚Äùsymbolsrepresentplaceholders.
Stage II. We organize the data into the following categories. (1) Text-only data. We use pre-
training text copus from DeepSeek-LLM [5]. (2) Interleaved image-text data. We use Wiki-
How [39] and WIT [72] dataset. (3) Image caption data. We use images from [17, 18, 23, 38,
40, 45, 47, 49, 70]. Among them, we employ open-source multimodal model to re-caption
images in [17, 40]. The image caption data is formatted into question-answer pairs, for ex-
ample,‚Äú<image>Describe the image in detail.<caption>‚Äù. (4)Tableandchartdata.
We use corresponding table and chart data from DeepSeek-VL [55]. The data is formatted
as‚Äú<question><answer>‚Äù. (5)Visualgenerationdata. Weutilizeimage-captionpairsfrom
variousdatasetsincluding[17,38,40,57,58,60,63,70],alongwith2Min-housedata. Forimages
from[38,70],wefilterbasedonaestheticscoresandimagesizes,resultingin20%remaining.
Duringtraining,werandomlyuseonlythefirstsentenceofacaptionwitha25%probabilityto
7Table2|Comparisonwithstate-of-the-artsonmultimodalunderstandingbenchmarks. ‚ÄúUnd.‚Äù
and ‚ÄúGen.‚Äù denote ‚Äúunderstanding‚Äù and ‚Äúgeneration‚Äù, respectively. Models using external
pretraineddiffusionmodelaremarkedwith‚Ä†.
Type Model #LLMParamsPOPE‚ÜëMME-P‚ÜëMMB‚ÜëSEED‚ÜëVQAv2(ùë°ùëíùë†ùë°)‚ÜëGQA‚ÜëMMMU‚ÜëMM-Vet‚Üë
Und.Only LLaVA-v1.5-Phi-1.5[86] 1.3B 84.1 1128.0 - - 75.3 56.5 30.7 -
MobileVLM[14] 1.4B 84.5 1196.2 53.2 - - 56.1 - -
MobileVLM-V2[15] 1.4B 84.3 1302.8 57.7 - - 59.3 - -
MobileVLM[14] 2.7B 84.9 1288.9 59.6 - - 59.0 - -
MobileVLM-V2[15] 2.7B 84.7 1440.5 63.2 - - 61.1 - -
LLaVA-Phi[96] 2.7B 85.0 1335.1 59.8 - 71.4 - - 28.9
LLaVA[51] 7B 76.3 809.6 38.7 33.5 - - - 25.5
LLaVA-v1.5[50] 7B 85.9 1510.7 64.3 58.6 78.5 62.0 35.4 31.1
InstructBLIP[16] 7B - - 36.0 53.4 - 49.2 - 26.2
Qwen-VL-Chat[3] 7B - 1487.5 60.6 58.2 78.2 57.5 - -
IDEFICS-9B[41] 8B - - 48.2 - 50.9 38.4 - -
Emu3-Chat[83] 8B 85.2 - 58.5 68.2 75.1 60.3 31.6 -
InstructBLIP[16] 13B 78.9 1212.8 - - - 49.5 - 25.6
Und.andGen.DreamLLM‚Ä†[21] 7B - - - - 72.9 - - 36.6
LaVIT‚Ä†[36] 7B - - - - 66.0 46.8 - -
Emu‚Ä†[75] 13B - - - - 52.0 - - -
NExT-GPT‚Ä†[84] 13B - - - - 66.7 - - -
Show-o[86] 1.3B 73.8 948.4 - - 59.3 48.7 25.1 -
Gemini-Nano-1[78] 1.8B - - - - 62.7 - 26.3 -
LWM[52] 7B 75.2 - - - 55.8 44.8 - 9.6
VILA-U[85] 7B 85.8 1401.8 - 59.0 79.4 60.8 - 33.5
Chameleon[77] 7B - - - - - - 22.4 8.3
Janus(Ours) 1.3B 87.0 1338.0 69.4 63.7 77.3 59.1 30.5 34.3
encouragethemodeltodevelopstronggenerationcapabilitiesforshortdescriptions. ImageNet
samples[18]arepresentedonlyduringthefirst120Ktrainingsteps,whileimagesfromother
datasets appear in the later 60K steps. This approach helps the model first learn basic pixel
dependenciesbeforeprogressingtomorecomplexsceneunderstanding,assuggestedby[9].
Thevisualgenerationdataisprovidedintheformat: ‚Äú<caption><image>‚Äù.
StageIII.Fortextunderstanding,weusedatafrom [43]. Formultimodalunderstanding,we
useinstructtuningdatafrom[31,34,35,43,56,69]. Forvisualgeneration,weuseimage-text
pairsfrom[17,60,70](asubsetofthatinstageII)and4Min-housedata. Weutilizethefollowing
formatforinstructiontuning:‚ÄúUser:<Input Message>\n Assistant: <Response>‚Äù. For
multi-turndialogues,werepeatthisformattostructurethedata.
4.3. EvaluationSetup
MultimodalUnderstanding. Toassessmultimodalunderstandingcapabilities,weevaluateour
modelonwidelyrecognizedimage-basedvision-languagebenchmarks,whichincludeVQAv2
[31],GQA[35],POPE[48],MME[25],SEED[42],MMB[54],MM-Vet[90],andMMMU[91].
Visual Generation. For evaluating visual generation capabilities, we use the MSCOCO-30K
[11],MJHQ-30K[44],andGenEval[30]benchmarks. MSCOCO-30KandMJHQ-30Kemploy
theFr√©chetInceptionDistance(FID)metricongeneratedimagescomparedto30Khigh-quality
images, which indicates the overall efficacy of image generation. GenEval is a challenging
benchmark for image-to-text generation, designed to reflect the comprehensive generative
8Table 3 | Evaluation of text-to-image generation ability on GenEval benchmark. ‚ÄúUnd.‚Äù
and ‚ÄúGen.‚Äù denote ‚Äúunderstanding‚Äù and ‚Äúgeneration‚Äù, respectively. Models using external
pretraineddiffusionmodelaremarkedwith‚Ä†.
Type Method #Params SingleObj. TwoObj. Counting Colors Position ColorAttri. Overall‚Üë
LlamaGen[73] 0.8B 0.71 0.34 0.21 0.58 0.07 0.04 0.32
LDM[67] 1.4B 0.92 0.29 0.23 0.70 0.02 0.05 0.37
SDv1.5[67] 0.9B 0.97 0.38 0.35 0.76 0.04 0.06 0.43
PixArt-ùõº[9] 0.6B 0.98 0.50 0.44 0.80 0.08 0.07 0.48
Gen.Only
SDv2.1[67] 0.9B 0.98 0.51 0.44 0.85 0.07 0.17 0.50
DALL-E2[66] 6.5B 0.94 0.66 0.49 0.77 0.10 0.19 0.52
Emu3-Gen [83] 8B 0.98 0.71 0.34 0.81 0.17 0.21 0.54
SDXL[62] 2.6B 0.98 0.74 0.39 0.85 0.15 0.23 0.55
SEED-X‚Ä†[29] 17B 0.97 0.58 0.26 0.80 0.19 0.14 0.49
Show-o[86] 1.3B 0.95 0.52 0.49 0.82 0.11 0.28 0.53
Und.andGen. LWM[52] 7B 0.93 0.41 0.46 0.79 0.09 0.15 0.47
Chameleon[77] 34B - - - - - - 0.39
Janus(Ours) 1.3B 0.97 0.68 0.30 0.84 0.46 0.42 0.61
abilities of visual generation models by offering a detailed instance-level analysis of their
compositionalcapabilities.
4.4. ComparisonwithState-of-the-arts
MultimodalUnderstandingPerformance. Wecomparetheproposedmethodwithstate-of-the-
artunifiedmodelsandunderstanding-onlymodelsinTable2. Janusachievestheoverallbest
resultsamongmodelsofsimilarscale. Specifically,comparedtothepreviousbestunifiedmodel,
Show-o[86],weachieveperformanceimprovementsof41%(949 ‚Üí 1338)and30%(48.7 ‚Üí 59.1)
ontheMMEandGQAdatasets, respectively. ThiscanbeattributedtoJanusdecouplingthe
visualencodingformultimodalunderstandingandgeneration,mitigatingtheconflictbetween
thesetwotasks. Whencomparedtomodelswithsignificantlylargersizes,Janusremainshighly
competitive. Forinstance,JanusoutperformsLLaVA-v1.5(7B)onseveraldatasets,including
POPE,MMbench,SEEDBench,andMM-Vet.
VisualGenerationPerformance. WereportvisualgenerationperformanceonGenEval,COCO-
30KandMJHQ-30Kbenchmarks. AsshowninTable3,ourJanusobtains61%overallaccuracy
on GenEval, which outperforms the previous best unified model Show-o (53%) and some
populargeneration-onlymethods,e.g.,SDXL(55%)andDALL-E2(52%). Thisdemonstrates
that our approach has better instruction-following capabilities. As shown in Table 4, Janus
achievesFIDsof8.53and10.10ontheCOCO-30KandMJHQ-30Kbenchmarks,respectively,
surpassing unified models Show-o and LWM, and demonstrating competitive performance
comparedtosomewell-knowngeneration-onlymethods. Thisdemonstratesthattheimages
generatedbyJanushavegoodqualityandhighlightsitspotentialinvisualgeneration.
4.5. AblationStudies
WecarefullydesignablationstudiestoverifytheeffectivenessofJanus‚Äôsdesignconcept. First,
wedesignexperimentstovalidatetheimportanceandbenefitsofdecouplingvisualencoding.
Second, we investigate the impact of unified training on individual tasks like multimodal
understandingorvisualgeneration. ResultsarelistedinTable5.
9Table 4 | Evaluation of text-to-image generation ability on MSCOCO-30K and MJHQ-30K
benchmark. ‚ÄúUnd.‚Äù and ‚ÄúGen.‚Äù denote ‚Äúunderstanding‚Äù and ‚Äúgeneration‚Äù, respectively.
Modelsusingexternalpretraineddiffusionmodelaremarkedwith‚Ä†.
Type Model #Params COCO-30K‚Üì MJHQ-30K‚Üì
DALL¬∑E[65] 12B 27.50 -
GLIDE[59] 5B 12.24 -
LDM[67] 1.4B 12.64 -
DALL¬∑E2[66] 6.5B 10.39 -
Gen.Only SDv1.5[67] 0.9B 9.62 -
GigaGAN[37] 0.9B 9.09 -
PixArt-ùõº[9] 0.6B 7.32 -
Imagen[68] 34B 7.27 -
RAPHAEL[87] 3B 6.61 -
Emu‚Ä†[75] 13B 11.66 -
NExT-GPT‚Ä† [84] 13B 11.28 -
Und.andGen.
SEED-X‚Ä†[29] 17B 14.99 -
Show-o[86] 1.3B 9.24 15.18
LWM[52] 7B 12.68 17.77
VILA-U(256)[85] 7B - 12.81
VILA-U(384)[85] 7B - 7.69
Janus(Ours) 1.3B 8.53 10.10
Table 5 | Ablation studies. We verify the effectiveness of decoupling visual encoding and
compareunifiedtrainingwithtask-specifictraining. ‚ÄúUnd.‚Äù,‚ÄúGen.‚Äù and‚ÄúSE.Tokenizer‚Äùdenote
‚Äúunderstanding‚Äù,‚Äúgeneration‚Äùand‚Äúsemantictokenizer‚Äù,respectively.
ExpID VisualEncoder TrainingTask POPE‚Üë MMB‚Üë SEED‚Üë MMMU‚Üë COCO-FID‚Üì
A VQTokenizer Und. +Gen. 60.1 35.0 34.9 24.7 8.72
B SE.Tokenizer Und. +Gen. 82.4 52.7 54.9 26.6 7.11
C SE.Tokenizer Und. 83.9 62.1 60.8 27.5 -
D SigLIP+VQ(Ours) Und. +Gen. 87.0 69.4 63.7 30.5 8.53
E SigLIP Und. 85.9 70.6 64.8 28.8 -
F VQTokenizer Gen. - - - - 8.92
BaselineConstruction. Followingpreviouswork[77],weselectaVQtokenizer[73]toencode
imagesforbothmultimodalunderstandingandgenerationtasks,servingasthebaseline(Exp-A).
Considering that the VQ tokenizer in Exp-A might be weak in extract semantic information,
makingitlesseffectiveformultimodalunderstanding, wealsoconstructastrongerbaseline
Exp-B.WeadoptSigLIPtodistillanenhancedsemantictokenizer3 thatcanextracthigh-level
semanticinformationfromimageswhilealsohavetheabilitytoconvertimagesintodiscrete
IDs, which is similar to that in [85]. Details of the semantic tokenizer could be found in the
Appendix A.1.
Impact of Decoupling Visual Encoding. (1) From the results of Exp-A, we find the model
achievessatisfactoryperformanceonvisualgenerationbenchmark(8.72FIDonCOCO).How-
ever,thereisasignificantgaponunderstandingbenchmarksbetweenExp-Aandourmodel
(Exp-D).(2)WhencomparingExp-BtoExp-A,theresultsshowaclearimprovementinmulti-
3Thesemantictokenizerisonlyusedintheablationstudyasastrongerbaseline. Forsimplicity,weusethe
ordinaryVQtokenizer[73]inthemainexperiment.
10SDXL LlamaGen Janus(Ours) SDXL LlamaGen Janus(Ours)
A wise old owl with golden plumage perched on a luminous crystal
A close-up high-contrast photo of Sydney Opera House sitting next
tree in a magical forest. Radiant fireflies swirl around while ethereal
to Eiffel tower, under a blue night sky of roiling energy, exploding
mist rolls through the trees, illuminated by swirls of iridescent
yellow stars, and radiating swirls of blue.
moonlight and glistening emerald leaves.
A detailed portrait of the Roman god Janus, featuring his two faces looking in opposite A brave dog wearing a futuristic space suit, exploring an alien planet
directions. One face appears aged, with deep-set wrinkles and a wise, contemplative expression, amidst swirling dunes of stardust and meteor showers. The landscape
while the other face is youthful, exuding vigor and curiosity. His hair is styled in flowing curls,
framing both faces with a sense of divine symmetry. The artwork is rich in contrasting colors, is dotted with glowing crystal formations and ethereal terraforms,
with the left side dominated by cold blues and silvers, symbolizing winter and reflection, and the creating a surreal environment in which swirling vortexes in the sky
right side awash with warm golds and reds, representing spring and renewal. The background is
a celestial tapestry, adorned with stars and symbolic motifs of time and passage. depict the endless dance of distant galaxies.
Figure4|QualitativecomparisonsofvisualgenerationwithLlamaGenandSDXL.Theimages
generatedbyJanusshowbetterconsistencywiththeuser‚Äôsprompts. Theimageresolutionsfor
SDXL,LlamaGen,andoursare1024√ó1024,512√ó512,and384√ó384,respectively. Bestviewed
onscreen.
modalunderstanding,althoughthereisstillaconsiderablegapcomparedtoourmethod. In
terms of visual generation, Exp-B outperforms Exp-D. We hypothesize two possible reasons
forthis. First,thesemantictokenizerproducesdiscreteIDsthataremoresemanticallycoher-
ent,providingmorereasonablepredictiontargetsfortheLLM.Second,thevisualencoderin
Exp-B has significantly more parameters than the Gen. encoder in Exp-D. (3) To investigate
whetherusingasinglevisualencoderleadstoatrade-offbetweenmultimodalunderstanding
andgeneration,wefurtherdesignExp-CbasedonExp-B,whichfocusessolelyonmultimodal
understandingtraining. ThemultimodalunderstandingabilityofExp-Cissignificantlybetter
than that of Exp-B. This indicates that the visual encoder in Exp-B made trade-offs between
multimodalunderstandingandgeneration,ultimatelysacrificingitsmultimodalunderstanding
capability. Theaboveexperimentsillustratetheimportanceofdecouplingvisualencoding.
Unified Model vs. Pure Understanding & Pure Generation. We compare the performance
ofunifiedtraining(Exp-D)againstpureunderstanding(Exp-E)andpuregeneration(Exp-F)
training. For pure understanding, we omit visual generation data. For pure generation, we
exclude the understanding data. Please note that unified training and pure understanding
traininggothroughthesamestepsfortheunderstandingpart. Similarly,unifiedtrainingand
puregenerationtraininggothroughthesamestepsforthevisualgenerationpart. Experimental
resultsshowthattheperformanceofunifiedtrainingiscomparabletothatoftrainingsolely
for understanding or solely for visual generation. This demonstrates that our model, Janus,
is capable of incorporating strong generative abilities while minimally affecting multimodal
understandingperformance.
11User: What is the meaning of this meme from the top panel to the bottom?
Janus (Ours): The meme is a humorous take on the concept of a Nobel
Prize in Physics. The top panel shows a man in a red jacket with his hand
up, seemingly confused or surprised, while the bottom panel shows the
same man with a smile on his face, seemingly pleased or proud, as if he has
just won the Nobel Prize. The meme is a play on the idea that the man in
the top panel might be confused or surprised by the idea of winning a
Nobel Prize in Physics, while the man in the bottom panel seems to be
genuinely happy about it.
Chameleon: The meme is a visual representation of a conversation between two people, with the top
panel featuringthetext"I'mnotarguing,I'mjustexplainingwhyI'mright"and the bottom panel featuring
thetext"No,you'renot."The meme is using a technique called "irony" to convey the idea that even though
one person is presenting themselves as non-argumentative, they are actually being argumentative. The use
of the phrase "I'm not arguing, I'm just explaining why I'm right" is a common tactic used by people who
are trying to avoid an argument but are actually engaging in one. The meme is poking fun at this tactic by
showing the person's actions and words as being in direct contradiction to their claim.
Show-o: The meaning of the meme from the top panel to the bottom is that it is a humorous and light-
hearted depiction of a man wearing a yellow hat and a red jacket,with thecaption"I'm not a fan of the
sun." This suggests that the man is not a fan of the sun, and he is using the sun as a metaphor for his
feelings or emotions. The meme is likely meant to be humorous and entertaining, as it combines a serious
subject with a lighthearted and playful approach
User: Explain this meme.
Janus (Ours): The meme is comparing two
different types of neural networks used in
machine learning. The first type is called
"Decoupling Visual Encoding," and the second
type is called "Single Visual Encoder." The meme
is trying to convey that the first type is more
powerful and can handle more complex tasks,
while the second type is simpler and less powerful.
Chameleon: This image shows a cartoon ofapigandabearside-by-side,withthetext"When you're a
veganandyour friends still order sausageandbacon"above them. The image is a humorous take on the
challenges of being a vegan in a non-vegan environment, where one's friends or family may not share their
dietary preferences.
Show-o: The image features two dogs,onebrownandonewhite, standing side by side. The brown dog is
positioned on the left side of the image, while the white dog is on the right. Both dogs are looking at the
camera, seemingly posing for a picture. The brown dog is slightly larger than the white dog. The image is
captionedwiththephrase‚ÄúI‚Äòmnotadog,butI‚Äômadog.‚Äù This humorous statement is accompanied by the
image of the two dogs, emphasizing their‚Ä¶
Figure5 | Qualitativeresultsofmultimodalunderstandingonhumorousmemes. Wecompare
the response with Chameleon-7B [77] and Show-o [86]. We emphasize the key-points in the
response. Bestviewedonscreen.
4.6. QualitativeResults
VisualizationsofVisualGeneration. Figure4providesqualitativecomparisonsbetweenour
model,diffusion-basedmodelslikeSDXL[62],andtheautoregressivemodelLlamaGen[73].
12The results show that our model demonstrates superior instruction-following capabilities in
visualgeneration,accuratelycapturingmostofdetailsintheuser‚Äôsprompt. Thisindicatesthe
potential of the unified model in the realm of visual generation. More visualizations can be
foundintheAppendix B.
MultimodalUnderstandingonMEMEImages. Figure5showcasesthequalitativeresultsof
Janus‚Äôs multimodal understanding ability, compared with Chameleon [77] and Show-o [86].
Janusaccuratelyinterpretsthetextcaptionandcapturestheemotionconveyedinthememe. In
contrast,bothChameleonandShow-ostrugglewithaccuratelyrecognizingthetextintheimage.
Additionally,Chameleonfailstoidentifyobjectsinthememe,whileShow-omisinterpretsthe
dog‚Äôscolor. Theseexampleshighlightthatthedecoupledvisionencodersignificantlyenhances
Janus‚Äôsfine-grainedmultimodalunderstandingabilitycomparedtothesharedencoderused
by Chameleon and Show-o. More multimodal understanding exmples can be found in the
Appendix B.
5. Conclusion
Inthispaper,weintroducedJanus,asimple,unifiedandextensiblemultimodalunderstanding
andgenerationmodel. ThecoreideaofJanusistodecouplevisualencodingformultimodal
understanding and generation, which could alleviate the conflict arising from the differing
demands that understanding and generation place on the visual encoder. Extensive experi-
mentshavedemonstratedtheeffectivenessandleadingperformanceofJanus. Itisalsoworth
notingthatJanusisflexibleandeasytoextend. Inadditiontohavingsignificantpotentialfor
improvementinbothmultimodalunderstandingandgeneration,Janusisalsoeasilyextendable
toincorporatemoreinputmodalities. TheaboveadvantagessuggestthatJanusmayserveasan
inspirationforthedevelopmentofthenextgenerationofmultimodalgeneral-purposemodels.
13Appendix
A. Details of Semantic Tokenizer Mentioned in Ablation Study
A.1. ArchitectureofSemanticTokenizer
SemanticReconstructionLoss
PretrainedSigLIP
SemanticDecoder
19 97 822
LookupCodebook
CNN
Vector Quantization 96 701 100
Encoder
66 88 99
PixelDecoder
DiscreteVisualTokens
RGBReconstructionLoss
(a)ArchitectureofSemanticTokenizer
19 97 822
LookupCodebook
CNN
Vector Quantization 96 701 100 SemanticDecoder
Encoder
66 88 99
DiscreteVisualTokensafterVQ
Adaptor
19 97 822
PixelDecoder 96 701 100 LLM
66 88 99
DiscreteIDsfromLLMPrediction
(b)Architecture of LLM with Semantic Tokenizer Integration
Figure 6 | Architecture and usage of the semantic tokenizer. (a) Architecture used during
trainingofthesemantictokenizer. Weusepre-trainedSigLIP[92]tosupervisethereconstruction
ofsemanticinformation,whileusingrawimagetosupervisethereconstructionofRGBvalues.
(b) Integrating LLM with the semantic decoder. The semantic decoder outputs continuous
featureswithhigh-levelsemantics,whicharepassedthroughanadaptorandthenusedasinput
fortheLLM.Pleasenotethatthesemantictokenizerisonlyusedintheablationstudy,notin
themainexperiment.
Webuildthesemantictokenizerbasedonthetokenizerarchitectureproposedin[73],which
has a downsample rate of 16. In addition to the original CNN pixel decoder, we add an
additionalsemanticdecoderbranchafterVectorQuantization,asshowninFigure6(a). The
semanticdecoderisa12-layerViT[22],with12attentionheadsandahiddendimensionof768.
For the semantic decoder, we use a causal attention mask to facilitate next token prediction
whenintegratingitwithanLLM.
14A.2. Training
TrainingProcedure. Thesemantictokenizeristrainedfromscratchinatwo-stagemanner. In
thefirststage,wetrainthemodelontheImageNet-1k[18]datasetfor40epochs. Inthesecond
stage,wefine-tunethemodelfor1epochon50millionimages. Theseimagescomefromthe
visualgenerationdatausedduringtheJanuspretrainingprocess. Weuseaconstantlearning
rateof1ùëí‚àí4andabatchsizeof128.
TrainingLoss. Thetraininglossofthesemantictokenizerconsistsoftwoparts. Ononehand,
weusethelossforRGBreconstructionasdescribedin[73]. Ontheotherhand,weuseSigLIP-
Large-Patch16-384 as the teacher to supervise the semantic feature reconstruction results by
thesemanticdecoder. WeadoptthelossinBEiT-v2[61]. Specifically,wemaximizethecosine
similarity between the semantic feature predicted by the semantic decoder and the SigLIP
output. Theweightforthesemanticreconstructionlossissetto0.25.
A.3. IntegratingwithLLM
WepresenttheintegrationofthesemantictokenizerandtheLLMinFigure6(b). Theimageis
firsttransformedintocontinuousfeaturesthroughtheCNNencoder,vectorquantizationand
thesemanticdecoder. Then,theLLMprocessesthesefeaturesandgeneratespredictionsforthe
imageIDs. Finally,thepixeldecoderconvertsthesediscreteIDsintoRGBvalues.
B. Additional Qualitative Results
MoreVisualizationsofText-to-ImageGeneration. Wepresentmoretext-to-imagegeneration
resultsinFigure7. ItisevidentthatJanusiscapableofproducinghigh-qualityimagesthatad-
herecloselytothegivenprompts. Wefurtherexplorethemultilingualtext-to-imagecapabilities
ofourmodel,asshowninFigure8. Wearepleasantlysurprisedtofindthat,despiteourtraining
dataconsistingsolelyofEnglishtext-to-imagedata,Januscanstillprocesstext-to-imagetasksin
otherlanguages. Weattributethismultilingualabilitytotheoriginallargelanguagemodel‚Äôs
inherenttraits. TheLLMinitiallytranslatesvariouslanguagesintoaunifiedsemanticspace,
allowingJanustoperformtext-to-imagetasksnaturallywithoutadditionaltraining.
MoreMultimodalUnderstandingResults. Additionalresultsonmultimodalunderstanding
areshowninFigure9. Janusexhibitsimpressivecomprehensionabilitieswhenhandlinginputs
fromvariouscontexts,showcasingitspowerfulcapabilities.
15a young woman, looks like mix of Lana Del Rey and Real photo of a cup of hot steaming coffee and a Portrait of a beautiful, curvaceous, Pirate princess
grimes, flowing cool colored hair, marbled, iridescent, brass vase with a large bouquet of spring flowers by goddess babe, red hair, intricate ornate costume,
shoujo manga, pre-raphaelite, k-pop, gilded, pearl, an old oak window at sunrise, fine details, rich Caribbean background + outdoors + Ocean, painted
spun silk, clouds, ghost, glowing jellyfish, billowing colors taken with a nikon z6 camera and a nikon by ArtGerm, Alphonse Mucha, Roberto Ferri, Ross
gossamer cloth, Alexander McQueen, handmade lace, nikkor lens with 50 f5.6 iso 100 and a shutter speed Tran, Pixar, low angle shot, digital painting,
floral embroidery, snakeskin, dramatic lighting of 1400 knot. UHD dtm HDR 8k cinematic rim lighting, Unreal Engine 5, 8K
a cute fluffy chubby marmot sunbathing on a pile of epic 3d portrait of white King Kong wearing mech Tiny cute adorable mouse dressed as a king in a
rocks, snow mountains background, turquoise glacier armor made of black crystals, golden ornate around castle, anthropomorphic, Jean-Baptiste Monge, soft
lake afar, clear blue sky, highly detailed, golden hour, the armor, symmetrical body, hyperrealistic, intricate cinematic lighting, 8k, intricate details, portrait,
natural light, octane render, unreal engine details, shiny, cinematic, unreal engine, artstation, Pixar style character, old fashioned movie style
octane render,
a panda that has been cybernetically A stunning princess from kabul in red, white Theultimatewristwatchwatchtimemachine,
enhanced more cybernetics3d 4k unreal traditional clothing, blue eyes, brown hair. super advanced technology, holographic
engine chaos 20 display,intricatemechanism.
Tiny cute adorable fluffy baby raccoon with Architectural parametric pavilion made Beautiful surreal symbolism the mesmerizing vision
knitted blue scarf leaning at a table in a from wood and glass, with organic cavities, of a Cleopatra Queen of Egypt , full body ,
medieval pub holding a coffee cup, surrounded by a beautiful forest. Dramatic mesmerizing brown eyes, black hair and ethereal
a cin nt eh mro ap to icm o lr ip ghh tic in, gJ , ean 8- kB , ap it nis trte ic aM teo ng de e, t as io lsft , s race yn tre a, c ing ph reo ft lo er ce tia ol nis st ,i c, 8 k hh dy ,p e ir nr te ra il nis ct ai tc e, f d se hea aft i ru n pr i te i fs o o, n c, u r t sra ,u d e gia olt i li f dn e elg i n k e rc c ae o tl ie l oos ,rt i ,a spl o e fr t fa e ru c er t fa le e, x cp ts io ou s np u se r ,r e , b r oh a kzig eoh hr
portrait, Pixar style character, old fashioned detail in the style of Frank Lloyde Wright effect, fine art photography, cinematic compositing,
movie style authentic, professional by Rorianai style 36k s1000
Figure7 | Moretext-to-imagegenerationresults. Weupsampletheimagesto1024√ó1024for
bettervisualization.
16LlamaGen
Janus(Ours)
A realistic photo of ËøôÊòØ‰∏ÄÂº†Èõ™Â±±ÂÜôÂÆûÁÖßÁâáÔºåÈò≥ Unephotor√©alistedemontagnes ÊæÑ„ÅøÂàá„Å£„ÅüÈùíÁ©∫„ÅÆ‰∏ã„ÄÅÂ§™ÈôΩ„Å´ Arealisticphotoof‚ùÑ‚õ∞
s s un u nno dw l eit ry apm ce lao eku asn rt bga lli uin s es te snw ki yi nth g ÂÖâ Â§© ÁöÑ ‰ª§ÁÖß ‰∏ã Â∏∏ ‰∫∫ËÄÄ ÁÜ† Èùí ÊÉä‰∏ã ÁÜ† Ê†ë ÂèπÁöÑ Áîü Ë¢´ ÁöÑÂ±± Ëæâ ÁôΩ È´òÂ≥∞ „ÄÇ Èõ™ Â±±Âú® Ââç Ë¶Ü ÊôØÊô¥ ÊôØ Áõñ Ëâ≤Êúó ‰∏≠ Ôºå Â¢ûÁöÑ È´ò ‰∏∫ Ê∑ªËìù Â§ß Ëøô ‰∫Ü e √© u p p an c jn l e oal rn a un se ci i t,ri i s e√©g e t nds l√© a tee np bs t da l eger esrul aed ln s alo as dimn uo s plt p rpe o oai il d u fr obs e dle nc r. rs i e d√©n A s s et u uil √† rds leo p en f √†m r et e num cms ei eo l ie li tu gee tt ees ss r ÁÖß „É™ „Çè „Çí „Å´„Çâ Ê∑±„Ç¢ „Çå „ÅÆ „Åø„Åï „É´ „Åü „ÇÄ„Å™ ËÉå „Çí„Çå „ÇàÂÜô „ÅÆ ‰∏é„Åü „ÅÜÂ±± È´ò „ÅàÁúü „Å™„ÅÑ „Ç¢È†Ç „Å¶„ÄÇ„Åå Â∏∏ „É´ „ÅÑÂâçËºù Á∑ë „Éó „ÇãÊôØ„Åè „ÅÆ Ê®π „Çπ „ÄÇÈõ™ Èõ™ „Åå „ÅÆ„Å´ È¢®Â±± „ÄÅ„ÅÆ Ë¶Ü ÊÅØ ÊôØ w s i dnk ei yt ph . tt hhT‚òÄ ea tolu l,n f to hsd n r ee eor g bw ra ro- edc u al u n te hs da t tr e ad kb a il d nu üå≤ d ge
Ê∑±Â∫¶„ÄÇ sc√®nealpine√†couperlesouffle. alpinescene.
Figure 8 | Multilingual text-to-image generation samples compared to LlamaGen [73]. Note
thatweonlyuseEnglishtext-to-imagedataintraining,andthisisanemergentcapabilityof
ourmodel. Thelanguagesusedintheprompt,fromlefttoright,are: English,Chinese,French,
Japanese,andEnglishwithemoji.
Prompt
Prompt
What algorithm does the image depict?
Generate the latex code of this formula.
Response
A _ { n } = a_ 0 \left[ 1 + \frac{3}{4}
\sum_{ k=1}^{n} \left( \frac{4}{9} \right)^k \right]
Response
The image depicts a CLIP (Contrastive
Prompt
Language-Image Pre-training) model. The
Generate the matplotlib.pyplot code for this plot. CLIP model is a type of pre-training algorithm
that combines the power of language and
vision to learn a unified representation of the
world. The image encoder processes the input
image, and the text encoder processes the text
associated with the image. The output of the
text encoder is then used to predict the correct
pair of images, which is a key component of the
CLIP model.
Prompt
Why is this painting interesting?
Response
The image you've provided is a pie chart that
represents the distribution of different types of
animals based on their frequency. The chart is divided
into four segments, each representing a different type
of animal and its corresponding percentage.
Here's the code to generate the pie chart:
```python
import matplotlib.pyplot as plt
# Data
animals = [
{"name": "Hogs", "frequency": 30.0},
{"name": "Frog", "frequency": 15.0}, Response
{"name": "Logs", "frequency": 10.0},
{"name": "Dogs", "frequency": 45.0}
] This image is an interesting piece of art
# Create the pie chart because it combines a well-known painting, the
plt.pie(animals, labels=animals, autopct='%1.1f%%', "Mona Lisa," with a dog. The dog is depicted in
startangle=90) the same pose as the Mona Lisa, but with a
# Set the title and labels different expression and a different
plt.title('Distribution of Animals') background. The painting is a unique blend of
plt.title.set_text('Frequency of Animals') art styles and subjects, making it stand out.
# Show the plot
plt.show()
```
Figure9|Moremultimodalunderstandingresults. Janushasastrongmultimodalunderstanding
capabilityandcanhandleinputsfromvariouscontexts,suchasscientificcharts,artworkimages,
LaTeXformulaimages,andmore.
17References
[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774,2023.
[2] Anthropic. Theclaude3modelfamily: Opus,sonnet,haiku. https://www.anthropic.
com,2024.
[3] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou. Qwen-vl: Afron-
tierlargevision-languagemodelwithversatileabilities. arXivpreprintarXiv:2308.12966,
2023.
[4] Y. Bai, X. Wang, Y.-p. Cao, Y. Ge, C. Yuan, and Y. Shan. Dreamdiffusion: Generating
high-qualityimagesfrombraineegsignals. arXivpreprintarXiv:2306.16934,2023.
[5] X.Bi, D.Chen, G.Chen, S.Chen, D.Dai, C.Deng, H.Ding, K.Dong, Q.Du, Z.Fu, etal.
Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint
arXiv:2401.02954,2024.
[6] T. B. Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
2020.
[7] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. Maskgit: Masked generative
imagetransformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages11315‚Äì11325,2022.
[8] H.Chang,H.Zhang,J.Barber,A.Maschinot,J.Lezama,L.Jiang,M.-H.Yang,K.Murphy,
W.T.Freeman,M.Rubinstein,etal. Muse: Text-to-imagegenerationviamaskedgenerative
transformers. arXivpreprintarXiv:2301.00704,2023.
[9] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Y.Wu,Z.Wang,J.Kwok,P.Luo,H.Lu,etal. Pixart-
ùëéùëôùëù‚Ñéùëé: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
arXivpreprintarXiv:2310.00426,2023.
[10] L.Chen,J.Li,X.Dong,P.Zhang,C.He,J.Wang,F.Zhao,andD.Lin.Sharegpt4v: Improving
largemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,2023.
[11] X.Chen,H.Fang,T.-Y.Lin,R.Vedantam,S.Gupta,P.Doll√°r,andC.L.Zitnick. Microsoft
cococaptions: Datacollectionandevaluationserver. arXivpreprintarXiv:1504.00325,2015.
[12] Z.Chen,W.Wang,H.Tian,S.Ye,Z.Gao,E.Cui,W.Tong,K.Hu,J.Luo,Z.Ma,etal. How
fararewetogpt-4v? closingthegaptocommercialmultimodalmodelswithopen-source
suites. arXivpreprintarXiv:2404.16821,2024.
[13] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu,
et al. Internvl: Scaling up vision foundation models and aligning for generic visual-
linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages24185‚Äì24198,2024.
[14] X. Chu, L. Qiao, X. Lin, S. Xu, Y. Yang, Y. Hu, F. Wei, X. Zhang, B. Zhang, X. Wei, et al.
Mobilevlm: Afast,reproducibleandstrongvisionlanguageassistantformobiledevices.
arXivpreprintarXiv:2312.16886,2023.
18[15] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, et al.
Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint
arXiv:2402.03766,2024.
[16] W.Dai,J.Li,D.Li,A.M.H.Tiong,J.Zhao,W.Wang,B.Li,P.Fung,andS.Hoi. Instructblip:
Towardsgeneral-purposevision-languagemodelswithinstructiontuning,2023.
[17] dclure. Laion-aesthetics-umap. https://huggingface.co/datasets/dclure/laion
-aesthetics-12m-umap,2022.
[18] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei. Imagenet: Alarge-scalehierarchi-
calimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pages248‚Äì255.Ieee,2009.
[19] J.Devlin. Bert: Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.
arXivpreprintarXiv:1810.04805,2018.
[20] P.DhariwalandA.Nichol. Diffusionmodelsbeatgansonimagesynthesis. Advancesin
neuralinformationprocessingsystems,34:8780‚Äì8794,2021.
[21] R.Dong,C.Han,Y.Peng,Z.Qi,Z.Ge,J.Yang,L.Zhao,J.Sun,H.Zhou,H.Wei,etal.Dream-
llm: Synergisticmultimodalcomprehensionandcreation. arXivpreprintarXiv:2309.11499,
2023.
[22] A.Dosovitskiy. Animageisworth16x16words: Transformersforimagerecognitionat
scale. arXivpreprintarXiv:2010.11929,2020.
[23] Echo840. Detailedcaptiondataset. https://huggingface.co/datasets/echo840/
Detailed_Caption,2023.
[24] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image
synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition,pages12873‚Äì12883,2021.
[25] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, et al.
Mme: A comprehensive evaluation benchmark for multimodal large language models.
arXivpreprintarXiv:2306.13394,2023.
[26] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene:
Scene-based text-to-image generation with human priors. In European Conference on
ComputerVision,pages89‚Äì106.Springer,2022.
[27] Y.Ge, Y.Ge, Z.Zeng, X.Wang, andY.Shan. Plantingaseedofvisioninlargelanguage
model. arXivpreprintarXiv:2307.08041,2023.
[28] Y.Ge, S.Zhao, Z.Zeng, Y.Ge, C.Li, X.Wang, andY.Shan. Makingllamaseeanddraw
withseedtokenizer. arXivpreprintarXiv:2310.01218,2023.
[29] Y.Ge,S.Zhao,J.Zhu,Y.Ge,K.Yi,L.Song,C.Li,X.Ding,andY.Shan. Seed-x: Multimodal
models with unified multi-granularity comprehension and generation. arXiv preprint
arXiv:2404.14396,2024.
[30] D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for
evaluatingtext-to-imagealignment. AdvancesinNeuralInformationProcessingSystems,
36,2024.
19[31] Y.Goyal,T.Khot,D.Summers-Stay,D.Batra,andD.Parikh. Makingthevinvqamatter:
Elevatingtheroleofimageunderstandinginvisualquestionanswering. InProceedingsof
theIEEEconferenceoncomputervisionandpatternrecognition,pages6904‚Äì6913,2017.
[32] High-flyer. Hai-llm: Efficientandlightweighttrainingtoolforlargemodels,2023. URL
https://www.high-flyer.cn/en/blog/hai-llm.
[33] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems,33:6840‚Äì6851,2020.
[34] Y.-C.Hsiao,F.Zubach,M.Wang,etal. Screenqa: Large-scalequestion-answerpairsover
mobileappscreenshots. arXivpreprintarXiv:2209.08199,2022.
[35] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages6700‚Äì6709,2019.
[36] Y.Jin,K.Xu,L.Chen,C.Liao,J.Tan,B.Chen,C.Lei,A.Liu,C.Song,X.Lei,etal. Unified
language-vision pretraining with dynamic discrete visual tokenization. arXiv preprint
arXiv:2309.04669,2023.
[37] M.Kang,J.-Y.Zhu,R.Zhang,J.Park,E.Shechtman,S.Paris,andT.Park. Scalingupgans
for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pages10124‚Äì10134,2023.
[38] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,
A.C.Berg,W.-Y.Lo,etal.Segmentanything.InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages4015‚Äì4026,2023.
[39] M.KoupaeeandW.Y.Wang. Wikihow: Alargescaletextsummarizationdataset. arXiv
preprintarXiv:1810.09305,2018.
[40] A.Kuznetsova,H.Rom,N.Alldrin,J.Uijlings,I.Krasin,J.Pont-Tuset,S.Kamali,S.Popov,
M.Malloci,A.Kolesnikov,T.Duerig,andV.Ferrari. Theopenimagesdatasetv4: Unified
imageclassification,objectdetection,andvisualrelationshipdetectionatscale. IJCV,2020.
[41] H.Lauren√ßon,D.vanStrien,S.Bekman,L.Tronchon,L.Saulnier,T.Wang,S.Karamcheti,
A. Singh, G. Pistilli, Y. Jernite, and et al. Introducing idefics: An open reproduction of
state-of-the-artvisuallanguagemodel,2023. URLhttps://huggingface.co/blog/id
efics.
[42] B.Li,R.Wang,G.Wang,Y.Ge,Y.Ge,andY.Shan. Seed-bench: Benchmarkingmultimodal
llmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[43] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li.
Llava-onevision: Easyvisualtasktransfer. arXivpreprintarXiv:2408.03326,2024.
[44] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi. Playground v2. 5: Three
insightstowardsenhancingaestheticqualityintext-to-imagegeneration. arXivpreprint
arXiv:2402.17245,2024.
[45] L.Li,Y.Wang,R.Xu,P.Wang,X.Feng,L.Kong,andQ.Liu. Multimodalarxiv: Adataset
forimprovingscientificcomprehensionoflargevision-languagemodels. arXivpreprint
arXiv:2403.00231,2024.
20[46] T.Li,Y.Tian,H.Li,M.Deng,andK.He. Autoregressiveimagegenerationwithoutvector
quantization. arXivpreprintarXiv:2406.11838,2024.
[47] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan. Densefusion-1m: Merging
visionexpertsforcomprehensivemultimodalperception. arXivpreprintarXiv:2407.08303,
2024.
[48] Y.Li,Y.Du,K.Zhou,J.Wang,W.X.Zhao,andJ.-R.Wen. Evaluatingobjecthallucinationin
largevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[49] Z. Li, X. Yang, K. Choi, W. Zhu, R. Hsieh, H. Kim, J. H. Lim, S. Ji, B. Lee, X. Yan, et al.
Mmsci: A multimodal multi-discipline dataset for phd-level scientific comprehension.
arXivpreprintarXiv:2407.04903,2024.
[50] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages26296‚Äì26306,2024.
[51] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural
informationprocessingsystems,36,2024.
[52] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and
languagewithringattention. arXivpreprintarXiv:2402.08268,2024.
[53] M.Liu,R.Shi,K.Kuang,Y.Zhu,X.Li,S.Han,H.Cai,F.Porikli,andH.Su. Openshape:
Scaling up 3d shape representation towards open-world understanding. Advances in
neuralinformationprocessingsystems,36,2024.
[54] Y.Liu,H.Duan,Y.Zhang,B.Li,S.Zhang,W.Zhao,Y.Yuan,J.Wang,C.He,Z.Liu,etal.Mm-
bench: Isyourmulti-modalmodelanall-aroundplayer? arXivpreprintarXiv:2307.06281,
2023.
[55] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun,
et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint
arXiv:2403.05525,2024.
[56] P.Lu,L.Qiu,J.Chen,T.Xia,Y.Zhao,W.Zhang,Z.Yu,X.Liang,andS.-C.Zhu. Iconqa: A
newbenchmarkforabstractdiagramunderstandingandvisuallanguagereasoning. arXiv
preprintarXiv:2110.13214,2021.
[57] madebyollin. Megalith-huggingface. https://huggingface.co/datasets/madebyol
lin/megalith-10m,2024.
[58] mehdidc. Yfcc-huggingface. https://huggingface.co/datasets/mehdidc/yfcc15
m,2024.
[59] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and
M.Chen. Glide: Towardsphotorealisticimagegenerationandeditingwithtext-guided
diffusionmodels. arXivpreprintarXiv:2112.10741,2021.
[60] J.Pan, K.Sun, Y.Ge, H.Li, H.Duan, X.Wu, R.Zhang, A.Zhou, Z.Qin, Y.Wang, J.Dai,
Y.Qiao,andH.Li. Journeydb: Abenchmarkforgenerativeimageunderstanding,2023.
[61] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. Beit v2: Masked image modeling with
vector-quantizedvisualtokenizers. arXivpreprintarXiv:2208.06366,2022.
21[62] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.M√ºller,J.Penna,andR.Rom-
bach. Sdxl: Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023.
[63] ProGamerGov. Dalle3-high-quality-captions. https://huggingface.co/datasets/Pr
oGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions,2024.
[64] A.Radford. Improvinglanguageunderstandingbygenerativepre-training. 2018.
[65] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration.InInternationalconferenceonmachinelearning,pages
8821‚Äì8831.Pmlr,2021.
[66] A.Ramesh, P.Dhariwal, A.Nichol, C.Chu, andM.Chen. Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
[67] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pages10684‚Äì10695,2022.
[68] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-
tijoLopes,B.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodels
withdeeplanguageunderstanding. Advancesinneuralinformationprocessingsystems,
35:36479‚Äì36494,2022.
[69] S.Shah,A.Mishra,N.Yadati,andP.P.Talukdar. Kvqa: Knowledge-awarevisualquestion
answering. In Proceedings of the AAAI conference on artificial intelligence, volume 33,
pages8876‚Äì8884,2019.
[70] V. Singla, K. Yue, S. Paul, R. Shirkavand, M. Jayawardhana, A. Ganjdanesh, H. Huang,
A.Bhatele,G.Somepalli,andT.Goldstein. Frompixelstoprose: Alargedatasetofdense
imagecaptions. arXivpreprintarXiv:2406.10328,2024.
[71] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502,2020.
[72] K. Srinivasan, K. Raman, J. Chen, M. Bendersky, and M. Najork. Wit: Wikipedia-based
imagetextdatasetformultimodalmultilingualmachinelearning. InProceedingsofthe
44th international ACM SIGIR conference on research and development in information
retrieval,pages2443‚Äì2449,2021.
[73] P.Sun,Y.Jiang,S.Chen,S.Zhang,B.Peng,P.Luo,andZ.Yuan. Autoregressivemodelbeats
diffusion: Llamaforscalableimagegeneration. arXivpreprintarXiv:2406.06525,2024.
[74] Q.Sun,Y.Fang,L.Wu,X.Wang,andY.Cao. Eva-clip: Improvedtrainingtechniquesfor
clipatscale. arXivpreprintarXiv:2303.15389,2023.
[75] Q.Sun,Q.Yu,Y.Cui,F.Zhang,X.Zhang,Y.Wang,H.Gao,J.Liu,T.Huang,andX.Wang.
Generativepretraininginmultimodality. arXivpreprintarXiv:2307.05222,2023.
[76] Q.Sun,Y.Cui,X.Zhang,F.Zhang,Q.Yu,Y.Wang,Y.Rao,J.Liu,T.Huang,andX.Wang.
Generativemultimodalmodelsarein-contextlearners. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages14398‚Äì14409,2024.
22[77] C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024.
[78] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.
Dai,A.Hauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023.
[79] K.Tian,Y.Jiang,Z.Yuan,B.Peng,andL.Wang. Visualautoregressivemodeling: Scalable
imagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
[80] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re,
N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language
models. arXivpreprintarXiv:2302.13971,2023.
[81] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,
P.Bhargava,S.Bhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288,2023.
[82] W.Wang, Z.Chen,X.Chen, J.Wu,X.Zhu, G.Zeng, P.Luo, T.Lu, J.Zhou,Y.Qiao, etal.
Visionllm: Largelanguagemodelisalsoanopen-endeddecoderforvision-centrictasks.
AdvancesinNeuralInformationProcessingSystems,36,2024.
[83] X.Wang,X.Zhang,Z.Luo,Q.Sun,Y.Cui,J.Wang,F.Zhang,Y.Wang,Z.Li,Q.Yu,etal.
Emu3: Next-tokenpredictionisallyouneed. arXivpreprintarXiv:2409.18869,2024.
[84] S.Wu,H.Fei,L.Qu,W.Ji,andT.-S.Chua. Next-gpt: Any-to-anymultimodalllm. arXiv
preprintarXiv:2309.05519,2023.
[85] Y.Wu,Z.Zhang,J.Chen,H.Tang,D.Li,Y.Fang,L.Zhu,E.Xie,H.Yin,L.Yi,etal. Vila-u: a
unifiedfoundationmodelintegratingvisualunderstandingandgeneration. arXivpreprint
arXiv:2409.04429,2024.
[86] J.Xie,W.Mao,Z.Bai,D.J.Zhang,W.Wang,K.Q.Lin,Y.Gu,Z.Chen,Z.Yang,andM.Z.
Shou. Show-o: Onesingletransformertounifymultimodalunderstandingandgeneration.
arXivpreprintarXiv:2408.12528,2024.
[87] Z.Xue,G.Song,Q.Guo,B.Liu,Z.Zong,Y.Liu,andP.Luo. Raphael: Text-to-imagegen-
erationvialargemixtureofdiffusionpaths. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[88] F.Yang, C.Ma, J.Zhang, J.Zhu, W.Yuan, andA.Owens. Touchandgo: Learningfrom
human-collectedvisionandtouch. arXivpreprintarXiv:2211.12498,2022.
[89] L.Yu,Y.Cheng,K.Sohn,J.Lezama,H.Zhang,H.Chang,A.G.Hauptmann,M.-H.Yang,
Y.Hao,I.Essa,etal. Magvit: Maskedgenerativevideotransformer. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages10459‚Äì10469,
2023.
[90] W.Yu,Z.Yang,L.Li,J.Wang,K.Lin,Z.Liu,X.Wang,andL.Wang. Mm-vet: Evaluating
largemultimodalmodelsforintegratedcapabilities. arXivpreprintarXiv:2308.02490,2023.
[91] X.Yue,Y.Ni,K.Zhang,T.Zheng,R.Liu,G.Zhang,S.Stevens,D.Jiang,W.Ren,Y.Sun,etal.
Mmmu: Amassivemulti-disciplinemultimodalunderstandingandreasoningbenchmark
for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages9556‚Äì9567,2024.
23[92] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-
training. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages11975‚Äì11986,2023.
[93] C. Zheng, T.-L. Vuong, J. Cai, and D. Phung. Movq: Modulating quantized vectors for
high-fidelityimagegeneration. AdvancesinNeuralInformationProcessingSystems,35:
23412‚Äì23425,2022.
[94] C.Zhou,L.Yu,A.Babu,K.Tirumala,M.Yasunaga,L.Shamis,J.Kahn,X.Ma,L.Zettle-
moyer, and O. Levy. Transfusion: Predict the next token and diffuse images with one
multi-modalmodel. arXivpreprintarXiv:2408.11039,2024.
[95] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4: Enhancingvision-language
understanding with advanced large language models. arXiv preprint arXiv:2304.10592,
2023.
[96] Y. Zhu, M. Zhu, N. Liu, Z. Ou, X. Mou, and J. Tang. Llava-phi: Efficient multi-modal
assistantwithsmalllanguagemodel. arXivpreprintarXiv:2401.02330,2024.
24