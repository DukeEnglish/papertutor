(A Partial Survey of) Decentralized, Cooperative
Multi-Agent Reinforcement Learning
Christopher Amato, Northeastern University
May 13, 2024
Multi-agentreinforcementlearning(MARL)hasexplodedinpopularityinrecentyears. Many
approacheshavebeendevelopedbuttheycanbedividedintothreemaintypes: centralizedtraining
and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized
trainingandexecution(DTE).
CTE methods assume centralization during training and execution (e.g., with fast, free and
perfect communication) and have the most information during execution. That is, the actions of
each agent can depend on the information from all agents. As a result, a simple form of CTE can
beachievedbyusingasingle-agentRLmethodwiththecentralizedactionandobservationspaces
(maintainingacentralizedaction-observationhistoryforthepartiallyobservablecase). CTEmeth-
ods can potentially outperform the decentralized execution methods (since they allow centralized
control) but are less scalable as the (centralized) action and observation spaces can scale expo-
nentially with the number of agents. CTE is typically only used in the cooperative MARL case
since centralized control implies cooperation. CTDE methods are perhaps the most common as
theycanusecentralizedinformationduringtrainingbutexecuteinadecentralizedmanner‚Äîusing
only information available to that agent during execution. As a result, they can be more scalable
than CTE methods, do not require communication during execution, and can often perform well.
CTDE fits most naturally with the cooperative case, but can be potentially applied in competitive
or mixed settings depending on what information is assumed to be observed. Decentralized train-
ing and execution methods make the fewest assumptions and are often simple to implement. In
fact, as I‚Äôll discuss, any single-agent RL method can be used for DTE by just letting each agent
learn separately. Of course, there are pros and cons to such approaches as we discuss below. It is
worthnotingthatDTEisrequiredifnoofflinecoordinationisavailable. Thatis,ifallagentsmust
learn during online interactions without prior coordination, learning and execution must both be
decentralized. DTE methods can be applied in cooperative, competitive, or mixed cases but this
textwillfocusonthecooperativeMARLcase.
MARLmethodscanbefurtherbrokenupintovalue-basedandpolicygradientmethods. Value-
based methods (e.g., Q-learning) learn a value function and then choose actions based on those
values. Policy gradient methods learn an explicit policy representation and attempt to improve the
policyinthedirectionofthegradient. BothclassesofmethodsarewidelyusedinMARL.
In this text, I will first give a brief description of the cooperative MARL problem in the form
of the Dec-POMDP. Then, I will discuss value-based DTE methods starting with independent Q-
learninganditsextensionsandthendiscusstheextensiontothedeepcasewithDQN,theadditional
complications this causes, and methods that have been developed to (attempt to) address these
1
4202
yaM
01
]GL.sc[
1v16160.5042:viXrar
a
1
o
a1
Environment
n
o
n
Figure1: AdepictionofcooperativeMARL‚ÄîaDec-POMDP.
issues. Next, I will discuss policy gradient DTE methods starting with independent REINFORCE
(i.e.,vanillapolicygradient),andthenextendingtotheactor-criticcaseanddeepvariants(suchas
independentPPO).Finally,IwilldiscusssomegeneraltopicsrelatedtoDTEandfuturedirections.
The basics of reinforcement learning (in the single-agent setting) are not presented in this
text. Anyone interested in RL should read the book by Sutton and Barto [2018]. Similarly, for a
broaderoverviewofMARL,therecentbookbyAlbrecht,ChristianosandScha¬®ferisrecommended
[Albrechtetal.,2024].
It is worth noting that this survey is partial in two respects. First, it is from my viewpoint.
Second,itdoesnotcoverallworkindecentralized,cooperativeMARL.Thissubareahaslesswork
thansubareassuchasCTDEbutitisstillmoreextensivethanpresentedhere. Ihaveincludedwork
that I believe is important for understanding the main concepts in the subarea and apologize for
thosethatIhaveomitted.
1 The cooperative MARL problem: The Dec-POMDP
Thecooperativemulti-agentreinforcementlearning(MARL)problemcanberepresentedasaDec-
POMDP [Oliehoek and Amato, 2016, Bernstein et al., 2002]. Dec-POMDPs generalize POMDPs
[Kaelblingetal.,1998](andMDPs[Puterman,1994])tothemulti-agent,decentralizedsetting. As
depictedinFigure1,multipleagentsoperateunderuncertaintybasedonpartialviewsoftheworld,
withexecutionunfoldingovertime. Ateachstep,everyagentchoosesanaction(inparallel)based
purely on locally observable information, resulting in each agent obtaining an observation and the
team obtaining a joint reward. The shared reward function makes the problem cooperative, but
theirlocalviewsmeanthatexecutionisdecentralized.
Formally,aDec-POMDPisdefinedbytuple‚ü®I,S,{A },T,R,{O },O,H,Œ≥‚ü©,where
i i
‚Ä¢ Iisafinitesetofagents;
‚Ä¢ Sisasetofstateswithdesignatedinitialstatedistributionb ;
0
‚Ä¢ A isasetofactionsforeachagentiwithA = √ó A thesetofjointactions;
i i i
‚Ä¢ T is a state transition probability function, T : S √ó A √ó S ‚Üí [0,1], that specifies the
probability of transitioning from state s ‚àà S to s‚Ä≤ ‚àà S when the actions a ‚àà A are taken by
theagents(i.e.,T(s,a,s‚Ä≤) = Pr(s‚Ä≤|a,s));
‚Ä¢ R is a reward function: R : S√óA ‚Üí R, the immediate reward for being in state s ‚àà S and
takingtheactionsa ‚àà A;
2‚Ä¢ O isasetofobservationsforeachagent,i,withO = √ó O thesetofjointobservations;
i i i
‚Ä¢ O is an observation probability function: O : O√óA√óS ‚Üí [0,1], the probability of seeing
observations o ‚àà O given actions a ‚àà A were taken and resulting in state s‚Ä≤ ‚àà S (i.e.,
O(a,s‚Ä≤,o) = Pr(o|a,s‚Ä≤));
‚Ä¢ H isthenumberofstepsuntiltermination,calledthehorizon;
‚Ä¢ andŒ≥ ‚àà [0,1]isthediscountfactor.
AsolutiontoaDec-POMDPisajointpolicy,denotedœÄ‚Äîasetofpolicies,oneforeachagent,
each of which is denoted œÄ . Because the state is not directly observed, it is typically beneficial
i
for each agent to remember a history of its observations. Then, a local policy, œÄ , for an agent
i
is a mapping from local action-observation histories to actions, H ‚Üí A , where H is the set of
i i i
local observation histories, h = {a ,o ,...,a ,o }1, by agent i up to the current time step,
i i,1 i,1 i,t i,t
t. Many researchers just use observation histories (without including actions), which is sufficient
fordeterministicpoliciesbutmaynotbeforstochasticpolicies[OliehoekandAmato,2016]. Note
that histories have implicit time steps due to their length which we do not include in the notation
(i.e.,wealwaysassumeahistorystartsonthefirsttimestepandthelasttimestepisdefinedbythe
numberofaction-observationpairs). Wecandenotethehistoriesforallagentsatagiventimestep
as h = ‚ü®h ,...,h ‚ü©. Because one policy is generated for each agent and these policies depend
1 n
onlyonlocalobservations,theyoperateinadecentralizedmanner.
While there always exists an optimal deterministic joint policy in Dec-POMDPs, we can also
(cid:81)
use stochastic joint policies, overloading notation with: œÄ(a|h) = œÄ (a |h ), where œÄ (a |h )
i‚ààI i i i i i i
represents the local policy for agent i and the probability of choosing action a in history h . De-
i i
terministic policies will be used in the value-based methods in Section 2 while stochastic policies
willbeusedinthepolicygradientgradientmethodsinSection3.
The value of a joint policy, œÄ, at joint history h can be defined for the case of discrete states
andobservationsas
(cid:88) (cid:104) (cid:88) (cid:88) (cid:105)
VœÄ(h) = P(s|h,b ) R(s,œÄ(h))+Œ≥ P(s‚Ä≤|œÄ(h),s) P(o|œÄ(h),s‚Ä≤)VœÄ(hœÄ(h)o)
0
s s‚Ä≤ o
where P(s|h,b ) is the probability of state s after observing joint history h starting from state
0
distribution b and œÄ(h) is the joint action taken at the joint history. Also, hœÄ(h)o represents h‚Ä≤,
0
the joint history after taking joint action œÄ(h) in joint history h and observing joint observation o.
IntheRLcontext,algorithmsdonotiterateoverstatesandobservationstoexplicitlycalculatethis
expectation but approximate it through sampling. For the finite-horizon case, VœÄ(h) = 0 when
the length of h equals the horizon H, showing the value function includes the time step from the
history. The discount factor, Œ≥, is typically set to 1 in the finite-horizon case and Œ≥ ‚àà [0,1) in the
infinite-horizoncase(H = ‚àû)2.
Wewillalsoevaluatepoliciesstartingfromtheinitial,nullhistoryasVœÄ(h ). Anoptimaljoint
0
policy beginning at h is œÄ‚àó(h ) = argmax VœÄ(h ). That is, the optimal joint policy is the set of
0 0 œÄ 0
localpoliciesforeachagentthatprovidesthehighestvalue,whichisdenotedV‚àó.
1Sometimes,o isalsoincludedasanobservationgeneratedbytheinitialstatedistribution.
i,0
2The episodic case [Sutton and Barto, 2018] is typically indefinite-horizon with termination to a set of terminal
stateswithprobability1andtheinfinite-horizoncaseissometimescalled‚Äòcontinuing.‚Äô
3Reinforcement learning methods often use history-action values, Q(h,a), rather than just his-
tory values V(h). QœÄ(h,a) is the value of choosing joint action a at joint history h and then
continuingwithpolicyœÄ,
(cid:88) (cid:104) (cid:88) (cid:88) (cid:0) (cid:1)(cid:105)
QœÄ(h,a) = P(s|h,b ) R(a,s)+Œ≥ P(s‚Ä≤|a,s) P(o|a,s‚Ä≤)QœÄ hao,œÄ(hao) ,
0
s s‚Ä≤ o
while Q‚àó(h,a) is the value of choosing action a at history h and then continuing with the optimal
policy,œÄ‚àó.
Policiesthatdependonlyonasingleobservation Itissomewhatpopulartodefinepoliciesthat
depend only on a single observation rather than the whole observation history. That is, œÄ : O ‚Üí
i i
A , rather than H ‚Üí A . This type of policy is ofter referred to as reactive or Markov since it just
i i i
dependson(orreactsfrom)thepastobservation. Thesereactivepoliciesaretypicallynotdesirable
since they can be arbitrarily worse than policies that consider history information [Murphy, 2000]
but can perform well or even be optimal in simpler subclasses of Dec-POMDPs [Goldman and
Zilberstein, 2004]. In general, reactive policies reduce the complexity of finding a solution (since
many fewer policies need to be considered) but only perform well when the problem is not really
partiallyobservable.
2 Decentralized, value-based methods
Value-based methods (such as Q-learning [Watkins and Dayan, 1992]) learn a value function and
the policy is implicitly defined based on those values (i.e., choosing the action that maximizes the
value function). Some of the earliest MARL methods are based on applying Q-learning to each
agent. For instance, Claus and Boutilier [1998] decompose MARL approaches into independent
learners (IL) and joint-action learners (JAL). ILs learn Q-functions that depend on their own
information (ignoring other agents) while JALs learn joint Q-functions, which include the actions
of the other agents. These concepts were originally defined for the fully observable, bandit (i.e.,
stateless) case but can be extended to the partially observable, stateful case. For example, we
can define an IL Q-function as Q (h ,a ), which provides a value for agent i‚Äôs history h and
i i i i
action a . A JAL Q-function Q (h,a) for agent i would depend on the joint information h and
i i
a. All decentralized value-based MARL methods are ILs since they never have access to the joint
information,handa.
Inparticulardecentralizedvalue-basedMARLmethodsforDec-POMDPs:
‚Ä¢ assume at each step, each agent, i, observes the action it took, a , its resulting observation,
i
o ,aswellasthejointreward,r,and
i
‚Ä¢ estimate a value function based solely on this information (i.e., episodes consisting of se-
quencesofa ,o ,r).
i i
As a result, as we will discuss, any (partially observable) single-agent MARL method can be a
DTE MARL method by ignoring the fact that other agents exist and learning as if they were the
only agent in the problem. Such methods may have issues with convergence and performance (as
we‚Äôlldiscussbelow)butthisisexactlywhatisdonebyourfirstapproach: IndependentQ-learning.
42.1 Independent Q-learning (IQL)
IQL just applies Q-learning to each agent independently. That is, each agent learns its own local
Q-function, Q , using Q-learning on its own data. Pseudocode for a version of IQL for the Dec-
i
POMDP case is provided in Algorithm 1. Just like Q-learning in the single-agent case, that algo-
rithm uses a learning rate, Œ±, and some form of exploration, which in this case is œµ-greedy. Each
agent initializes their Q-function (e.g., to 0, optimistically, etc.), and then iterates over episodes
(either a fixed set or until convergence). During each episode, the agent chooses an action with
exploration,seesthejointrewardanditsownobservationandthenupdatestheQ-valueatthecur-
renthistory,h ,giventhecurrentQ-estimatesandtherewardusingthestandardQ-learningupdate
i
(line 9). The history is updated by appending the action taken and observation seen to the current
historyandtheepisodecontinuesuntilthehorizonisreached.
Note that the Q-functions are inherently timed as the history length defines the current time
step,makingthealgorithmfitwiththefinite-horizoncase. Regardless,thealgorithmcaneasilybe
extended to the episodic/indefinite-horizon case by including terminal states (or histories) instead
ofafixedhorizon.
Algorithm1IndependentQ-Learningforagenti(finite-horizon)
1: setŒ± andœµ
2: InitializeQ forallh ‚àà H ,a ‚àà A
i i i i i
3: forallepisodesdo
4: h ‚Üê ‚àÖ {Emptyinitialhistory}
i
5: fort = 1toH do
6: Choosea ath fromQ (h ,¬∑)withexploration(e.g.,œµ-greedy)
i i i i
7: Seejointrewardr,localobservationo {Dependsonjointactiona}
i
8: h‚Ä≤ ‚Üê h a o
i i i i
(cid:104) (cid:105)
9: Q (h ,a ) ‚Üê Q (h ,a )+Œ± r+Œ≥argmax Q (h‚Ä≤,a‚Ä≤)‚àíQ (h ,a )
i i i i i i a‚Ä≤ i i i i i i
i
10: h ‚Üê h‚Ä≤
i i
11: endfor
12: endfor
13: return Q
i
Important hidden information This algorithm runs in the underlying Dec-POMDP but the de-
tailsofthosedynamicsintheenvironmentarenotshown(astheyarenotvisibletotheagent). For
example,atthebeginningofeachepisode,thestate,s,issampledfromtheinitialstatedistribution
b , and when agents take joint action a the reward is generated from the reward function, R(s,a),
0
the next state s‚Ä≤ is sampled from the transition dynamics with probabilityPr(s‚Ä≤|s,a), and the joint
observation is sampled from the observation function Pr(o|s‚Ä≤,a). This process continues until the
endoftheepisode.
While independent Q-learning ignores the other agents, it still depends on them in order to
generate the joint reward r and the local observation o since both of these depend on the actions
i
of all agents. That is, from agent i‚Äôs perspective, it is learning in a (single-agent) POMDP, as
Algorithm 1 shows (the states of the POMDP would be the states of the Dec-POMDP plus the
5historiesoftheotheragents).3
In particular, as pointed out in previous papers [Lauer and Riedmiller, 2000]4, independent Q-
learningisactuallylearningaQ-functionforeachagentbasedontheactionsselectedbytheother
agents during training. That is, agent i would use data that depends on the actions taken by the
other agents (and observations other agents see) even if it doesn‚Äôt directly observe those actions
(andobservations). Inthepartiallyobservablecase,thefollowingQ-functionwouldbelearned:
(cid:34) (cid:35)
(cid:88) (cid:88)
Q (h ,a ) = PÀÜ (a,h|h ,a ) r+Œ≥ PÀÜ (o |h,a)maxQ (h‚Ä≤,a‚Ä≤) (1)
i i i i i i i i i
a‚Ä≤
a‚ààA oi i
where P(a,h|h ,a ) is the empirical probability that the joint action a and joint history h occurs
i i
when agent i selects its action a in h . This is precisely when independent Q-learning will update
i i
Q (h ,a )anditwillusethejointrewardr andagent‚ÄôsobservationfunctionP(o‚Ä≤|a,h),whichap-
i i i i
proximatesP(o‚Ä≤|a,s‚Ä≤)P(s‚Ä≤|a,s),whereP(o‚Ä≤|a,s‚Ä≤)marginalizesoutotheragentobservationsfrom
i i
the joint function P(o‚Ä≤|a,s‚Ä≤). PÀÜ (o |h,a) is the empirical probability based on observing o from
i i
h after taking action a. Therefore, IQL is assuming the other agents are part of the environment
and learning a Q-function for the resulting POMDP. If the other agents are not learning and do in
fact have fixed policies, the problem would just reduce to this POMDP. Unfortunately, the other
agents would also typically be learning, leading to nonstationarity in the underlying POMDP and
difficultywithcoordinatedactionselection.
Convergence and solutions In the case of IQL, the underlying POMDP that is being learned is
non-stationary since the other agents are also learning and thus changing their policies over time.
This may cause IQL to not converge [Tan, 1993]. Convergence of Q-learning in the multi-agent
caseisanactiveareaofresearch[ClausandBoutilier,1998,Tuylsetal.,2003,Wunderetal.,2010,
Hussain et al., 2023], it is still an open question what assumptions will allow convergence and to
what(unlikethepolicygradientcasewhereconvergencetoalocaloptimumisassuredundermild
assumptions[Peshkinetal.,2000,Lyuetal.,2023])5.
It is worth pointing out that even if algorithms can converge to an optimal Q-value (or any Q-
value),agentsmaynotbeabletoselectoptimalactionsiftherearemultipleoptimalpolicies[Lauer
and Riedmiller, 2000]. In this case, agents would need to coordinate on their actions to make sure
they choose actions from the same joint policy but this is not possible with only individual Q-
values (e.g., if Q (h ,a1) = Q (h ,a2) and Q (h ,a1) = Q (h ,a2) but Q(h ,h ,a1,a2) =
1 1 1 1 1 1 2 2 2 2 2 2 1 2 1 2
Q(h ,h ,a2,a1) < Q(h ,h ,a2,a2) = Q(h ,h ,a1,a1)‚Äîagentsmusttakethesameactionstobe
1 2 1 2 1 2 1 2 1 2 1 2
optimal).
IQL is very simple but can perform well [Tan, 1993, Sen et al., 1994, Tampuu et al., 2017]. It
isalsothebasisfortheothervalue-basedDTEMARLmethods.
3TheideaoffixingotheragentpoliciesandthensolvingtheresultingPOMDP(eitherbyplanningmethods[Nair
et al., 2003] or RL methods [Banerjee et al., 2012]) has had success and results in a best response for agent i. Fur-
thermore, iterating over agents, i, fixing other agent policies, ¬¨i, and calculating best responses will lead to a local
optimumoftheDec-POMDP[Banerjeeetal.,2012]. Unfortunately,suchmethodsrequirecoordinationtocommuni-
catewhichagent‚ÄôsturnitistolearnsotheyarenotappropriatefortheDTEcase.
4Note the paper considers the fully observable deterministic transition case but we extend the idea to the Dec-
POMDPcase.
5In fact, while Q-learning can provably converge to a global optimum in the single-agent case [Jaakkola et al.,
1993], there are no known MARL algorithms that have guaranteed convergence to a global optimum. This is an
interestingareaforfutureresearch!
62.2 Improving the performance of IQL
Several extensions of IQL have been developed to improve performance and better fit with the
MARL setting. While the below algorithms were developed for the fully observable case (the
multi-agent MDP [Oliehoek and Amato, 2016]), we extend them to the partially observable case
heretofitbetterwithourDec-POMDPsetting.
2.2.1 DistributedQ-learning
In order to promote coordination, Distributed Q-learning [Lauer and Riedmiller, 2000] makes
strong optimistic assumptions. In particular, extending to the partially observable case, it uses
thefollowingQ-update:
(cid:26) (cid:27)
Q (h ,a ) = max Q (h ,a ),r+Œ≥maxQ (h‚Ä≤,a‚Ä≤) (2)
i i i i i i i i i
a‚Ä≤
i
wherethecurrentQ-valueiskeptifitishigherthan(orequalto)thenewestimater+Œ≥max Q (h‚Ä≤,a‚Ä≤).
a‚Ä≤ i i i
i
Otherwise, the Q-value is updated to be the new estimate. The intuition behind this optimistic up-
dateisthatotheragentswillmakemistakesduringtrainingbecausetheyhavesuboptimalpolicies
or because they are exploring. Agents don‚Äôt necessarily want to learn from these mistakes. In-
stead, agents want to learn from the best policies of the other agents such as those that choose
actions that generate high-reward values, even if they don‚Äôt happen very often. In the special case
ofdeterministicMMDPs(themulti-agentfullyobservablecase,seeOliehoekandAmato[2016]),
thisapproachwilllearnthesameQ-functionascentralizedQ-learning(whichwillconvergetothe
optimal Q-values in the limit). This is unlike IQL which can get stuck in local optima or never
convergeatall.
Because Distributed Q-learning learns individual Q-values, agents may still not be able to
select optimal actions due to the coordination issue mentioned above. As a result, Distributed Q-
learningstoresthecurrentbestpolicyforeachagent(i.e.,iftheQ-valueisupdated,theactionthat
resulted in that update is stored for that state in the fully observable case or history in the partially
observable case). Including this coordination mechanism, the algorithm can converge to optimal
Q-functionsandoptimalpoliciesinthelimitforthefullyobservabledeterministiccase.
In the more general stochastic case, distributed Q-learning is not able to distinguish between
environmental stochasticity and agent stochasticity. That is, if getting a high-valued reward is
unlikely (due to reward or transition stochasticity) distributed Q-learning will assume it can be
gottenwithprobabilityoneusingmaxratherthantakingtheexpectation(overotheragentpolicies
andstochastictransitions)asinEquation1. Asaresult,distributedQ-learningisoverlyoptimistic
in the stochastic case and may perform poorly. Learning and incorporating the probabilities in
Equation 1 and then calculating the expectation over the next step Q-value could fix this problem
butwouldassumeagentsobservetheactionsofotheragentswhichisnotthecasefordecentralized
training.
2.2.2 HystereticQ-learning
Tomaintaintheideaofoptimismwhileaccountingforstochasticity,hystereticQ-learning[Matignon
et al., 2007] was developed. The idea is to use two learning rates, Œ± and Œ≤ with Œ± > Œ≤. That is,
agentsupdatetheirQ-valuesmoreduringapositiveexperiencethananegativeone.
7Inparticular,ifwedefinetheTDerroras:
Œ¥ ‚Üê r+Œ≥maxQ(h‚Ä≤,a‚Ä≤)‚àíQ (h ,a ),
i i i i i
a‚Ä≤
i
thenwecanrewritedistributedQ-learning(usingalearningrateŒ±)as:
(cid:40)
Q (h ,a )+Œ±Œ¥ if Œ¥ > 0
i i i
Q (h ,a ) = (3)
i i i
Q (h ,a ) else
i i i
highlighting the fact that positive experiences (in terms of TD error) cause updates to the Q-
functionwhilenegativeonesdonot.
HystereticQ-learning,incontrast,addsthesecondlearningrateŒ≤ tothenegativeexperiences:
(cid:40)
Q (h ,a )+Œ±Œ¥ if Œ¥ > 0
i i i
Q (h ,a ) = (4)
i i i
Q (h ,a )+Œ≤Œ¥ else
i i i
This ensures learning still occurs in the negative cases but the update is less than in the positive
cases because Œ± > Œ≤. Therefore, hysteretic Q-learning is still optimistic (hoping the positive
experiences are because of better action choices and not stochasticity) but is less optimistic than
distributedQ-learning. Theimplementationisalsoverysimple(althoughitdoesrequiretuningtwo
learning rates). Hysteretic Q-learning is very simple but can perform well in a range of domains
[Matignon et al., 2007]. Also, note that as Œ≤ approaches Œ±, hysteretic Q-learning will become
standardQ-learningandasŒ≤ approaches0,itwillbecomedistributedQ-learning.
2.2.3 LenientQ-learning
While it is possible to decay Œ≤ towards Œ± as learning continues, to be more optimistic at the
beginningoflearning(whenalotofexplorationishappening)andstillberobusttostochasticityas
learning converges, lenient Q-learning [Wei and Luke, 2016] allows agents to adjust this leniency
towards exploration on a history-action pair basis. Specifically, lenient Q-learning maintains a
temperature, T(h ,a ), per history-action pair that adjusts the probability that a negative update
i i
at a particular history-action pair will be ignored. These temperature values allow history-action
pairs that have not been visited often to not be updated yet, while frequently visited history-action
pairsareupdatedasinIQL.
ThelenientQ-learningupdateis:
(cid:40)
Q (h ,a )+Œ±Œ¥ if Œ¥ > 0 or rand ‚àº U(0,1) > 1‚àíe‚àíK‚àóT(hi,ai)
i i i
Q (h ,a ) = (5)
i i i
Q (h ,a ) else
i i i
T(h ,a )isinitializedtoamaximumtemperature,andisdecayedafteranupdatewith
i i
T(h ,a ) ‚Üê ŒªT(h ,a ) for Œª ‚àà (0,1) and leniency parameter K adjusts the impact of the
i i i i
temperatureontheprobabilitydistribution. TheresultingQ-updatewillprobabilisticallyupdateQ
based on how often history-action pairs are visited, with more frequently visited pairs being more
likelytobeupdatedwhentheTDerrorisnegative.
Lenient Q-learning can outperform hysteretic Q-learning since it can adjust the amount of
optimism in a more fine-grained way but it also requires maintaining (and updating) temperature
values as well as Q-values. Maintaining these values can be more problematic in the partially
observable case since the history-action space grows exponentially with the problem horizon (or
historylengthmoregenerally).
82.3 Deep extensions, issues, and fixes
In order to scale to larger domains, deep extensions of the above algorithms have been developed.
TheseapproachesaretypicallybasedondeepQ-networks(DQN)[Mnihetal.,2015]. WhileDQN
can scale to larger action and observation spaces, the decentralized multi-agent context can cause
problems. We first discuss the basics of DQN and its extension to the partially observable case,
DRQN, and then discuss a deep version of IQL, independent DRQN [Tampuu et al., 2017], the
coordinationissueswiththeapproach,andsomeproposedfixes.
2.3.1 DQNandDRQN
Deep Q-networks (DQN) [Mnih et al., 2015] is an extension of Q-learning [Watkins and Dayan,
1992] to include a neural net as a function approximator. That is, DQN learns Q (s,a), parame-
Œ∏
terizedwithŒ∏ (i.e.,Œ∏ representstheparametersoftheneuralnetwork),byminimizingtheloss:
(cid:104) (cid:105)
L(Œ∏) = E (cid:0) y ‚àíQ (s,a)(cid:1)2 ,where y = r+Œ≥argmaxQ (s‚Ä≤,a‚Ä≤) (6)
<s,a,r,s‚Ä≤>‚àºD Œ∏ Œ∏‚àí
a‚Ä≤
which is just the squared TD error‚Äîthe difference between the current estimated value, Q (s,a),
Œ∏
and the new value gotten from adding the newly seen reward to the previous Q-estimate at the
next state, Q (s‚Ä≤,a‚Ä≤). Because learning neural networks can be unstable, a separate target action-
Œ∏‚àí
value function, Q , and an experience replay buffer D [Lin, 1992] are implemented to stabilize
Œ∏‚àí
learning. ThetargetnetworkisanolderversionoftheQ-estimatorthatisupdatedperiodically,the
experiencereplaybufferstoress,a,s‚Ä≤,rsequencesandsingles,a,s‚Ä≤,rtuplesarei.i.d.sampledfor
updates. As shown in Figure 2(a), the neural network (NN) outputs values for all actions (a ‚àà A)
to make maxing at a state possible with a single forward pass (rather than iterating through the
actions).
Deep recurrent Q-networks (DRQN) [Hausknecht and Stone, 2015] extends DQN to handle
partialobservability,wheresomenumberofrecurrentlayers(e.g.,LSTM[HochreiterandSchmid-
huber, 1997]) are included to maintain an internal hidden state which is an abstraction of the his-
tory (as shown in Figure 2(b)). Because the problem is partially observable, o,a,o‚Ä≤,r sequences
are stored in the replay buffer during execution. The update equation is very similar to that of
DQN:
(cid:104) (cid:105)
L(Œ∏) = E (cid:0) y ‚àíQ (h,a)(cid:1)2 ,where y = r+Œ≥argmaxQ (h‚Ä≤,a‚Ä≤) (7)
<h,a,r,o>‚àºD Œ∏ Œ∏‚àí
a‚Ä≤
but since a recurrent neural network (RNN) is used, the internal state of the RNN can be thought
of as a history representation. That is, instead of learning Q-functions using full histories (i.e.,
h = o ,a ,o ,r ,...,o ,a ,o ,r ), h is the internal state of the RNN after sequentially inputing
0 1 1 1 t‚àí1 t t t
thehistorysequence(h = o ,a ,o ,...,o ,a ,o ). Similarly,h‚Ä≤ = hao‚Ä≤,whichcanbethoughtof
0 1 1 t‚àí1 t t
asthenewhistoryafterappendingo‚Ä≤ andatoh,whichisachievedbyinputingaando‚Ä≤ totheRNN
when its current state is h. Technically, a whole history (e.g., episode) should be sampled from
thereplaybuffertotraintherecurrentnetworkbutitiscommontouseafixedhistorylength(e.g.,
sample o,a,o‚Ä≤,r sequences of length 10). As mentioned above, many implementations just use
observation histories rather than full action-observation histories. With the popularity of DRQN,
it has become common to add recurrent layers to standard (fully observable) deep reinforcement
learningmethodswhensolvingpartiallyobservableproblems.
9(a) DQN (b) DRQN (c) IndependentDRQN
Figure2: DQN,DRQN,andindependentDRQNdiagrams.
2.3.2 IndependentDRQN(IDRQN)
Independent DRQN (IDRQN) [Tampuu et al., 2017]6 combines IQL and DRQN. As seen in Al-
gorithm 2, the basic structure is the same as IQL but Q is estimated using an RNN parameterized
i
by Œ∏ (shown in Figure 2(c)), along with using a target network, Œ∏‚àí, and a replay buffer, D. First,
episodes are generated by interacting with the environment (e.g., œµ-greedy exploration), which are
added to the replay buffer. This buffer typically has a fixed size and new data replaces old data
whenthebufferisfull. Episodescanthenbesampledfromthereplaybuffer(eithersingleepisodes
orasaminibatchofepisodes). Inordertohavethecorrectinternalstate,RNNsaretrainedsequen-
tially. Training is done from the beginning of the episode until the end7, calculating the internal
state and the loss at each step and updating Œ∏ using gradient descent. The target network Œ∏‚àí is
updatedperiodically(everyC episodesinthecode)bycopyingtheparametersfromŒ∏.
While the pseudocode uses a horizon, the resulting Q-functions no longer reflect the time step
based on the history since histories of different lengths may be combined into the same represen-
tation in the RNN. As a result, untimed/stationary Q-values are learned that just depend on the
RNN internal state, which we still call h, and not the time step. This stationary representation is
technically incorrect for the finite-horizon case (even though it is widely used in practice). Again,
the code can be adjusted to the episodic/indefinite-horizon case by replacing the horizon with ter-
minalstatesortheinfinite-horizoncasebyremovingtheloopoverepisodes(bothofwhichfixthe
issue with timed Q-functions as these cases do not require them). Lastly, the pseudocode, for this
approach and throughout the text, is written to be simple and highlight the main algorithmic com-
ponents. Therearemanyimplementationtricks,especiallyinthedeepcase,thatarenotincluded.
Issues with IDRQN Independent DRQN can perform well but has a number of issues. For
example, early methods disabled experience replay [Foerster et al., 2016]8. They found that per-
formance was worse with experience replay than without it. This is because agents are no longer
learning concurrently when using the replay buffer‚Äîexperience is generated from the behavior
of all agents (i.e., concurrently) but is then put into the buffer for learning. When learning, dif-
ferent agents will sample and learn from different experiences. Learning from this different data
6Again,theoriginalalgorithmonlyconsidersthefullyobservablecasebutweconsidertheextensiontotheDec-
POMDPcase.
7Itiscommoninpracticetouseafixedhistorylengthandsamplewindowsofthatlengthratherthantrainingwith
fullepisodes.
8OthermethodsforimprovingexperiencereplayinMARLexist,buttheytypicallyassumetheCTDEsetting(such
asFoersteretal.[2017]).
10Algorithm2IndependentDRQN(IDRQN)foragenti(finite-horizon*)
1: setŒ± andœµ
2: InitializenetworkparametersŒ∏ andŒ∏‚àí forQ
i
3: D ‚Üê ‚àÖ
i
4: e ‚Üê 1 {episodeindex}
5: forallepisodesdo
6: h ‚Üê Œ∏RNN {initialhistoryistheinitialRNNstateofthenetwork}
i
7: fort = 1toH do
8: Choosea ath fromQŒ∏(h ,¬∑)withexploration(e.g.,œµ-greedy)
i i i i
9: Seejointrewardr,localobservationo {Dependsonjointactiona}
i
10: appenda ,o ,r toDe
i i i
11: h ‚Üê h a o {updateRNNstateofthenetwork}
i i i i
12: endfor
13: sampleanepisodefromD
14: fort = 1toH do
15: h ‚Üê Œ∏RNN {initialhistoryistheinitialRNNstateofthenetwork}
i
16: a ,o ,r ‚Üê De(t)
i i i
17: h‚Ä≤ ‚Üê h a o
i i i i
18: y = r+Œ≥argmax QŒ∏‚àí(h‚Ä≤,a‚Ä≤)
a‚Ä≤ i i i
i
19: PerformgradientdescentonparametersŒ∏ withtheloss:
(cid:0)
y ‚àíQŒ∏(h ,a
)(cid:1)2
i i i
20: h ‚Üê h‚Ä≤
i i
21: endfor
22: ife mod C = 0then
23: Œ∏‚àí ‚Üê Œ∏
24: endif
25: e ‚Üê e+1
26: endfor
27: return Q
i
11ùë°
ùëñ
‚ãØ
ùëí
Figure3: Concurrentexperiencereplaytrajectories(CERTs)(from[Omidshafieietal.,2017])
makes performance noisy and unstable. This makes intuitive sense since agents will update their
Q-functions (and resulting policies) based on (potentially very) different experiences. We discuss
asimplefixnext.
2.3.3 Deephysteretic-Qandconcurrentbuffer
Hysteretic Q-learning has been extended to the deep case in the form of decentralized hysteretic
deep recurrent Q-networks (Dec-HDRQN) [Omidshafiei et al., 2017]. Dec-HDRQN makes two
main contributions: the use of hysteresis with D(R)QN, and a concurrent replay buffer to improve
performance9.
The inclusion of hysteresis in Dec-HDRQN just combines hysteresis and IDRQN. That is,
since y ‚àí QŒ∏(h ,a ) in the IDRQN loss is the TD error, Œ¥, the network parameters are updated
i i i
(usinggradientdescent)usinglearningrateŒ± ifŒ¥ > 0,andŒ≤ isusedotherwise.
Theconcurrentbufferisalsorelativelystraightforward. Concurrentexperiencereplaytrajecto-
ries (CERTs), are based on the idea of creating a joint experience replay buffer and then sampling
fromthatbuffer. Inparticular,wewouldliketohaveajointreplaybufferwithjointexperiencetu-
ples,ae,oe,re,...,ae,oe,re,wherethesuperscripthererepresentstheepisodenumber,e. Agents
1 1 1 t t t
could then sample from this joint buffer to get the local information from the same episode and
time step: h ,a ,o ,r (where h comes from the internal state of agent i‚Äôs RNN after passing the
i i i i
local history of actions and observations until the time step being sampled). Of course, in a de-
centralizedlearningcontext,agentswillnothaveaccesstosuchajointbuffer,butagentscanstore
their part of the joint buffer by indexing by episode e and timestep t. That is, as seen in Figure
3, agents store their local trajectories along axes for e and t. When an e and t is sampled, each
agentiwillsampledatafromthesameepisodeandtimestep,producingthesamedataaswouldbe
produced with the joint buffer. As long as agents have the same size buffers and coordinate on the
seedsfortherandomnumbergenerator,thisapproachcanbeusedfordecentralizedlearning.
2.3.4 DeeplenientQ-learning
LenientQ-learninghasbeenextendedtothefully-observabledeepcaseintheformoflenientDQN
[Palmer et al., 2018]. In this case, the leniency values are added to the replay buffer of DQN at
each step as: (s ,a ,r ,s ,l(s ,a )) where l(s ,a ) = 1‚àíe‚àíK‚àóT(œï(st),at). To scale to large state
t t t t+1 t t t t
spaces, the approach clusters states using an autoencoder which outputs a hash œï(s) for a given
9Thepaperalsodiscussesthemulti-taskcase,butwedon‚Äôtdiscussthataspecthere.
12states. Themethodalsointroducesatemperaturedecayscheduleafterreachingtheterminalstate
andanimprovedexplorationschemebasedonaveragetemperaturevalue.
Theapproachwasshowntoperformwellinasetofcoordinatedmulti-agentobjecttransporta-
tion problems (CMOTPs). It was not extended to the partially observable case but it should be
possiblebyusingamethodthatclustershistoriesratherthanstates.
2.3.5 LikelihoodQ-learning
Distributional Q-learning [Bellemare et al., 2023] has also been used to improve decentralized
MARL. Instead of just learning the expected Q-value as in traditional Q-learning, distributional
RL learns the distribution of returns that are possible due to stochasticity. Likelihood Q-learning
[Lyu and Amato, 2020] uses the return distribution to determine the likelihood of the experience
(i.e., h ,a ,o ,r) given the current estimate, which they call Time Difference Likelihood (TDL).
i i i
This information can be used to help determine if other agents are exploring (i.e., low likelihood
andlowvalue). Asaresult,theTDLcanbeusedasamultiplierofthelearningrate,causinglarger
updatesformorelikelyexperiences‚Äîwhenotheragentsarenotexploring.
But some low-likelihood experiences may be good to learn from, such as when agents have
found a high-quality joint action (i.e., low likelihood but high value). Distributional information
can then be used to estimate risk [Morimura et al., 2010]. In the multi-agent context risk-seeking
behaviorcanmeanbeingoptimisticasinhystereticorlenientlearning.
Each of these ideas can be used in isolation or in combination and they can be used with other
decentralized MARL methods to further improve performance. For instance, the combination of
Dec-HDRQN (see 2.3.3) with TDL was shown to outperform Dec-HDRQN and forms of lenient
Q-learning. Incorporatingriskcanfurtherimproveperformance.
3 Decentralized policy gradient methods
Single-agent policy gradient and actor-critic methods can similarly be extended to the decentral-
ized training multi-agent case by allowing each agent to learn separately. These approaches can
scaletolargeractionspacesandhavestronger(local)convergenceguaranteesthanthevalue-based
counterpartsabove.
Policy gradient methods can be used with continuous-action or stochastic policies. For all
approaches, we will assume that we have a stochastic policy for each agent parameterized by œà ,
i
where œÄœài(a |h ) represents the probability agent i will choose action a given the history h and
i i i i i
parameters, œà , Pr(a |h ,œà ). This is in contrast to the value-based methods in Section 2, where
i i i i
deterministic policies were generated based on the learned value functions. Like the value-based
methods, the decentralized policy gradient methods also assume agents observe their own action,
a ,theresultingobservation,o ,andthejointreward,r ateachtimestep.
i i
3.1 Decentralized REINFORCE
REINFORCE [Williams, 1992] is one of the oldest (and simplest) policy gradient methods. The
basic idea is to estimate the value of the policy using Monte Carlo rollouts and then update the
policy using gradient ascent. REINFORCE was extended to the cooperative multi-agent case by
Peshkin et al. [2000]. Importantly, Peshkin et al. [2000] also showed that the joint gradient can
13be decomposed into decentralized gradients. That is, decentralized gradient ascent will be equiv-
alent to joint gradient ascent. Like the value-based algorithms, ‚Äòindependent‚Äô means each agent
is learning independently but the data is generated by all agents from the given policies and all
agents make updates at the same time (i.e., concurrent learning). Convergence to a local optimum
is guaranteed when agents are learning concurrently because they will apply the same algorithm
to the same data at synchronized time steps. This is in contrast to the value-based methods, which
do not have strong convergence guarantees. These convergence guarantees extend to many other
policygradientmethods(asdiscussedbelow).
Pseudocode for a version of decentralized REINFORCE is in Algorithm 3. Here, a set of
stochastic policies, œÄ , that are parameterized by œà (e.g., parameters of a neural network), are
i i
learned. AlearningrateofŒ± isused(asusual). Themethoditeratesforsomenumberofepisodes,
choosingactionsfromthestochasticpolicy(whichhaspositiveprobabilityforallactionstoensure
proper exploration), observing the joint reward and local observation at each time step, and stor-
ing the episode. Then, the return is computed for each step of the episode and the parameters of
the policy are updated using the returns and gradient (just like the single-agent version of REIN-
FORCE).Timestepsareexplicitlyrepresentedinthecodetomakereturncalculationclear. Again,
the method can be extended to the episodic/indefinite-horizon case by replacing the horizon with
terminal states and it isn‚Äôt well-defined for the infinite-horizon case (since updating takes place at
theendofepisodes).
Algorithm3DecentralizedREINFORCEforagenti(finite-horizon)
Require: IndividualactormodelsœÄ (a ;h ),parameterizedbyœà
i i i i
1: setŒ±
2: forallepisodesdo
3: h ‚Üê ‚àÖ {Emptyinitialhistory}
i,0
4: ep ‚Üê ‚àÖ {Emptyepisode}
5: fort = 0toH‚àí1do
6: Choosea ath fromœÄ (a ;h )
i,t i,t i i i,t
7: Seejointrewardr ,localobservationo {Dependsonjointactiona}
t i,t
8: appenda ,o ,r toep
i,t i,t t
9: h ‚Üê h a o {Appendnewactionandobstoprevioushistory}
i,t+1 i,t i,t i,t
10: endfor
11: fort = 0toH‚àí1do
12: Computereturnattfromep: G ‚Üê
(cid:80)H‚àí1Œ≥k‚àítr
i,t k=t k
13: Updateparameters: œà ‚Üê œà +Œ±Œ≥tG ‚àálogœÄ (a |h )
i i i,t i i i,t
14: endfor
15: endfor
Independent REINFORCE isn‚Äôt widely used but serves as a basis for the later policy gradient
approachesaswellastheoreticalguarantees.
3.2 Independent actor critic (IAC)
Since REINFORCE is a Monte Carlo method, it must wait until the end of an episode to update.
Furthermore, it will typically be less sample efficient than methods that learn a value function.
14Actor-critic methods have been developed to combat these issues by learning a policy represen-
tation (the actor) as well as a value function (the critic) [Sutton and Barto, 2018]. Actor-critic
methods can use the critic to evaluate the actor without waiting until the end of the episode and
TD-learningtoupdatethecriticmoreefficiently.
Independent actor-critic (IAC) was first introduced by Foerster et al. [2018] and it can be
thought of as a combination of IQL (or IDRQN) and decentralized REINFORCE. Each agent
learns a policy, œÄ (a ;h ), as well as a value function such as Q (h ,a ). In the simplest case of
i i i i i i
IAC (where a Q-value is used for the critic), the gradient associated with the policy parameters œà
i
canbederivedas
(cid:104) (cid:105)
‚àá J = (1‚àíŒ≥)E Q (h ,a )‚àá logœÄœài(a |h ) , (8)
œài h,a‚àºœÅ(h,a) i i i œài i i i
assuming a stochastic policy for each agent i that is parameterized by œà , œÄœài(a |h ). Policies are
i i i i
sampledaccordingtotheon-policy(discounted)visitationprobabilityœÅ(whichisajointvisitation
probability since concurrent sampling is assumed). This gradient is very similar to the REIN-
FORCE one but uses a Q-value rather than the Monte Carlo return (and is shown here with the
expectationratherthanjustasample).
Intheon-policy,function-approximation(e.g.,deep)case,thecriticcanbeupdatedas:
(cid:104) (cid:105)
L(Œ∏ ) = E (cid:0) y ‚àíQŒ∏i(h ,a )(cid:1)2 ,where y = r+Œ≥QŒ∏i(h‚Ä≤,a‚Ä≤) (9)
i <hi,ai,r,oi>‚àºD i i i i i i
where h‚Ä≤ = h a o and a‚Ä≤ is the action sampled at the next time step using œÄœài(a‚Ä≤|h‚Ä≤) since it is an
i i i i i i i i
on-policyestimate(ratherthanthemaxactionusedinDQN).QŒ∏i(h ,a )representstheQ-function
i i i
estimateforagentiusingparametersŒ∏ (justlikeinSection2).
i
Pseudocode for IAC is given in Algorithm 4. Because the value function is no longer being
used to select actions but rather just to evaluate the policy, it is common to instead learn V (h ).
i i
ÀÜ
And because a model of V is learned, we denote it V. Like the REINFORCE case, the algorithm
loops for some number of episodes, choosing actions from the stochastic policy and seeing the
jointrewardandlocalobservationateachstep. Now,duringtheepisode,theTDerroriscalculated
and used to update the actor and the critic using the associated gradients described above. Since
ÀÜ ÀÜ
V-valuesarelearnedinsteadofQ-values,theTDerroriscalculatedwithr +Œ≥V (h )‚àíV (h )
t i i,t+1 i i,t
rather than Q . As a result, a value-based version of Equation 8 is used to update the actors. Also,
i
ÀÜ
a value-based version of Equation 9 is recovered by using y = r + Œ≥V (h ) and replacing
t i i,t+1
QŒ∏i(h ,a )withVÀÜ (h ).
i i i i i,t
As noted in the value-based (DRQN) case, using an RNN may merge histories of different
lengths into the same representation, resulting in incorrectly timed value functions in the finite-
horizon case. Like the other algorithms, IAC can be adapted to the episodic or infinite-horizon
casebyincludingterminalstatesorremovingtheloopoverepisodes. IACisasimplebaselinethat
isoftenusedandservesasthebasisofmanyotherdecentralizedMARLactor-criticmethods.
3.3 Other decentralized policy gradient methods
Anysingle-agentmethodcanbeextendedtothedecentralizedMARLcase. Typically,itjustentails
including recurrent layers to deal with partial observability and then training agents concurrently
togeneratejointdataandsynchronizeupdates. OnenotableexampleisIPPO[deWittetal.,2020,
15Algorithm4IndependentActor-Critic(IAC)(finite-horizon)
Require: IndividualactormodelsœÄ (a |h ),parameterizedbyœà
i i i i
ÀÜ
Require: IndividualcriticmodelsV (h),parameterizedbyŒ∏
i i
1: forallepisodesdo
2: h ‚Üê ‚àÖ {Emptyinitialhistory}
i,0
3: fort = 0toH‚àí1do
4: Choosea ath fromœÄ (a |h )
i,t i,t i i i,t
5: Seejointrewardr ,localobservationo {Dependsonjointactiona}
t i,t
6: h ‚Üê h a o {Appendnewactionandobstoprevioushistory}
i,t+1 i,t i,t i,t
ÀÜ ÀÜ
7: ComputevalueTDerror: Œ¥ ‚Üê r +Œ≥V (h )‚àíV (h )
i,t t i i,t+1 i i,t
8: Computeactorgradientestimate: Œ≥tŒ¥ ‚àálogœÄ (a |h )
i,t i i,t i,t
9: Update actor parameters œà using gradient estimate (e.g., œà ‚Üê œà +
i i i
Œ±Œ≥tŒ¥ ‚àálogœÄ (a |h ))
i,t i i,t i,t
ÀÜ
10: Computecriticgradientestimate: Œ¥ ‚àáV (h )
i,t i i,t
ÀÜ
11: UpdatecriticparametersŒ∏ usinggradientestimate(e.g.,Œ∏ ‚Üê Œ∏ +Œ≤Œ≥Œ¥ ‚àáV (h ))
i i i i,t i i,t
12: endfor
13: endfor
Yu et al., 2022]. IPPO is the extension of single-agent PPO [Schulman et al., 2017] to the multi-
agentcase. FormsofIPPOhavebeenshowntooutperformmanystate-of-the-artMARLmethods.
It is worth noting that the standard implementations of IPPO (and the impressive results) use pa-
rameter sharing (as discussed below) so they are not technically decentralized but decentralized
implementationscanstillperformwell[Yuetal.,2022].
4 Other topics
Concurrent learning It is very important to note that all of the methods in this section assume
agentsarerunningthesamealgorithmandperformsynchronousupdates(i.e.,atthesametimeon
the same data). Without these assumptions, methods that have convergence guarantees lose them
but may still work well in some domains. As discussed in relation to DQN (see Section 2.3.2),
breakingthis‚Äòconcurrent‚Äôlearningassumptioncancausemethodstoperformpoorly.
Parameter sharing Parameter (or weight) sharing, where agents share the same network for
estimating the value function in value-based methods or policies (and value functions) in policy
gradient methods, is common in cooperative MARL. The data from each agent can be used to
update a single value network for an algorithm such as DRQN. Agents can still perform differ-
entlyduetoobservingdifferenthistoriesandagentindicescanbeaddedtoincreasespecialization
[Gupta et al., 2017]. Since parameter sharing requires agents to share networks during training, it
isn‚ÄôtstrictlyDTEasitcouldn‚Äôtbeusedforonlinetraininginadecentralizedfashion. Nevertheless,
any of the algorithms in this text can be extended to use parameter sharing. The resulting algo-
rithmswouldbeaformofCTDEasthesharednetworkwouldrepresentacentralizedcomponent.
Nevertheless, these parameter-sharing implementations may be more scalable than other forms of
CTDE, are often simple to implement, and can perform well [Gupta et al., 2017, Foerster et al.,
2018,Yuetal.,2022].
16Relationship with CTDE Parameter sharing is one way to exploit a centralized training phase
but many others are possible (e.g., sharing a centralized critic [Lowe et al., 2017, Foerster et al.,
2018] or learning individual value functions from a joint one [Sunehag et al., 2017, Rashid et al.,
2018, Wang et al., 2021]). While CTDE methods are (by far) the most popular form of MARL,
they do not always perform the best. In fact, since DTE methods typically make the concurrent
learning assumption, they are actually quite close to CTDE methods. This is shown explicitly
in the policy gradient case as the gradient of the joint update is the same as the decentralized
gradient (as described above in Section 3.1)[Peshkin et al., 2000]. This phenomenon has been
alsobeenstudiedtheoreticallyandempiricallywithmodernactor-criticmethods[Lyuetal.,2021,
2023]. It turns out that while centralized critic actor-critic methods are often assumed to be better
than DTE actor-critic methods, they are theoretically the same (with mild assumptions) and often
empirically similar (and sometimes worse). Too much centralized information can sometimes be
overwhelming,harmingscalabilityofcentralized-criticmethods[Yuetal.,2022,Lyuetal.,2023].
Furthermore, some CTDE methods, such as using a state-based critic, are fundamentally unsound
andcanoftenhurtperformanceinpartiallyobservabledomains[Lyuetal.,2022,2023]. Ofcourse,
there are many different ways of performing centralized training for decentralized execution, and
studyingthedifferencesandsimilaritiesbetweenDTEandCTDEvariantsofalgorithmsseemslike
apromisingdirectionforunderstandingcurrentmethodsanddevelopingimprovedapproachesfor
bothcases.
Othersimilartypesoflearningnotconsideredhere Thereareanumberofpapersthatfocuson
‚Äòfully‚Äô decentralized MARL, such as Zhang et al. [2018]. These methods assume communication
between agents during training and execution. As a result, the assumptions are different than the
ones considered here. I would term these ‚Äòfully‚Äô decentralized as distributed or networked MARL
to make this point more clear. There are also methods that assume additional coordination. For
example,somemethodsassumeagentstaketurnslearningwhiletheotheragentsremainfixed[Su
et al., 2022, Banerjee et al., 2012]. Such methods require coordinating updates but can converge
tolocal(i.e.,Nash)equilibria.
5 Acknowledgements
I thank Roi Yehoshua, Andrea Baisero, and the members of my Lab for Learning and Planning in
Robotics(LLRP)forreadingmy(very)roughdraftsandprovidingcommentsthathelpedimprove
thedocument.
References
S. V. Albrecht, F. Christianos, and L. Scha¬®fer. Multi-Agent Reinforcement Learning: Foundations
andModernApproaches. MITPress,2024. https://www.marl-book.com.
B. Banerjee, J. Lyle, L. Kraemer, and R. Yellamraju. Sample bounded distributed reinforcement
learning for decentralized POMDPs. In Proceedings of the Twenty-Sixth AAAI Conference on
ArtificialIntelligence,2012.
17M. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. MIT
Press,2023. http://www.distributional-rl.org.
D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The complexity of decentralized
control of Markov decision processes. Mathematics of Operations Research, 27(4):819‚Äì840,
2002.
C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. In Proceedings of the National Conference on Artificial Intelligence, pages 746‚Äì752,
1998.
C. S. de Witt, T. Gupta, D. Makoviichuk, V. Makoviychuk, P. H. Torr, M. Sun, and S. Whiteson.
Is independent learning all you need in the StarCraft multi-agent challenge? arXiv preprint
arXiv:2011.09533,2020.
J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 29,
2016.
J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson. Stabilising
experience replay for deep multi-agent reinforcement learning. In Proceedings of the Interna-
tionalConferenceonMachineLearning,pages1146‚Äì1155,2017.
J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policygradients. InProceedingsoftheNationalConferenceonArtificialIntelligence,2018.
C. V. Goldman and S. Zilberstein. Decentralized control of cooperative systems: Categorization
andcomplexityanalysis. JournalofAIResearch,22:143‚Äì174,2004.
J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent control using deep rein-
forcementlearning. InAdaptiveandLearningAgentsWorkshopatAAMAS,2017.
M. Hausknecht and P. Stone. Deep recurrent Q-learning for partially observable MDPs. CoRR,
abs/1507.06527,2015.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì
1780,1997.
A. A. Hussain, F. Belardinelli, and G. Piliouras. Asymptotic convergence and performance of
multi-agent Q-learning dynamics. In Proceedings of the International Conference on Au-
tonomousAgentsandMultiagentSystems,2023.
T. Jaakkola, M. Jordan, and S. Singh. Convergence of stochastic iterative dynamic programming
algorithms. InAdvancesinNeuralInformationProcessingSystems,volume6,1993.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable
stochasticdomains. ArtificialIntelligence,101(1-2):99‚Äì134,1998.
M.LauerandM.A.Riedmiller. Analgorithmfordistributedreinforcementlearningincooperative
multi-agent systems. In Proceedings of the International Conference on Machine Learning,
pages535‚Äì542,2000.
18L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machinelearning,8(3-4):293‚Äì321,1992.
R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic
formixedcooperative-competitiveenvironments.InAdvancesinNeuralInformationProcessing
Systems,2017.
X. Lyu and C. Amato. Likelihood quantile networks for coordinating multi-agent reinforcement
learning. InProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiagent
Systems,2020.
X.Lyu,Y.Xiao,B.Daley,andC.Amato.Contrastingcentralizedanddecentralizedcriticsinmulti-
agent reinforcement learning. In Proceedings of the International Conference on Autonomous
AgentsandMultiagentSystems,2021.
X. Lyu, Y. Xiao, A. Baisero, and C. Amato. A deeper understanding of state-based critics in
multi-agent reinforcement learning. In Proceedings of the National Conference on Artificial
Intelligence,2022.
X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent rein-
forcementlearning. JournalofAIResearch,77:235‚Äì294,2023.
L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Hysteretic Q-learning: An algorithm for decen-
tralized reinforcement learning in cooperative multi-agent teams. In Proceedings of IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages64‚Äì69,2007.
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Ried-
miller,A.K.Fidjeland,G.Ostrovski,S.Petersen,C.Beattie,A.Sadik,I.Antonoglou,H.King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep rein-
forcementlearning. Nature,518(7540):529,2015.
T. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, and T. Tanaka. Nonparametric return dis-
tributionapproximationforreinforcementlearning. InProceedingsoftheInternationalConfer-
enceonMachineLearning,pages799‚Äì806,2010.
K. P. Murphy. A survey of POMDP solution techniques. Technical report, University of British
Columbia,2000.
R. Nair, M. Tambe, M. Yokoo, D. V. Pynadath, and S. Marsella. Taming decentralized POMDPs:
Towardsefficientpolicycomputationformultiagentsettings.InProceedingsoftheInternational
JointConferenceonArtificialIntelligence,pages705‚Äì711,2003.
F.A.OliehoekandC.Amato. AConciseIntroductiontoDecentralizedPOMDPs. Springer,2016.
S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-
agent reinforcement learning under partial observability. In Proceedings of the International
ConferenceonMachineLearning,pages2681‚Äì2690,2017.
G. Palmer, K. Tuyls, D. Bloembergen, and R. Savani. Lenient multi-agent deep reinforcement
learning. InProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiagent
Systems,pages443‚Äì451,2018.
19L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling. Learning to cooperate via policy search.
InProceedingsofUncertaintyinArtificialIntelligence,pages307‚Äì314,2000.
M. L. Puterman. Markov Decision Processes‚ÄîDiscrete Stochastic Dynamic Programming. John
Wiley&Sons,Inc.,1994.
T.Rashid,M.Samvelyan,C.Schroeder,G.Farquhar,J.Foerster,andS.Whiteson. QMIX:Mono-
tonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings
oftheInternationalConferenceonMachineLearning,pages4295‚Äì4304,2018.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
S.Sen,M.Sekaran,J.Hale,etal. Learningtocoordinatewithoutsharinginformation. InProceed-
ingsoftheNationalConferenceonArtificialIntelligence,volume94,pages426‚Äì431,1994.
K. Su, S. Zhou, J. Jiang, C. Gan, X. Wang, and Z. Lu. MA2QL: A minimalist approach to fully
decentralizedmulti-agentreinforcementlearning. arXivpreprintarXiv:2209.08244,2022.
P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot,
N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for coopera-
tivemulti-agentlearning. arXiv:1706.05296,2017.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (second edition). The
MITPress,2018.
A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente.
Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4),
2017.
M. Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
oftheInternationalConferenceonMachineLearning,pages487‚Äì494,1993.
K.Tuyls,K.Verbeeck,andT.Lenaerts. Aselection-mutationmodelforQ-learninginmulti-agent
systems. InProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiagent
Systems,pages693‚Äì700,2003.
J.Wang,Z.Ren,T.Liu,Y.Yu,andC.Zhang. QPLEX:Duplexduelingmulti-agentQ-learning. In
ProceedingsoftheInternationalConferenceonLearningRepresentations,2021.
C.J.C.H.WatkinsandP.Dayan. Q-learning. MachineLearning,8(3):279‚Äì292,May1992.
E. Wei and S. Luke. Lenient learning in independent-learner stochastic cooperative games. The
JournalofMachineLearningResearch,17(1):2914‚Äì2955,2016.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. MachineLearning,8(3-4):229‚Äì256,1992.
M. Wunder, M. L. Littman, and M. Babes. Classes of multiagent Q-learning dynamics with œµ-
greedyexploration. InProceedingsoftheInternationalConferenceonMachineLearning,pages
1167‚Äì1174,2010.
20C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness
ofPPOincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,
35:24611‚Äì24624,2022.
K. Zhang, Z. Yang, H.Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement
learning with networked agents. In Proceedings of the International Conference on Machine
Learning,pages5872‚Äì5881,10‚Äì15Jul2018.
21