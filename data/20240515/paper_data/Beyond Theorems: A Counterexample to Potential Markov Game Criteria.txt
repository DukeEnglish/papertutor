Beyond Theorems: A Counterexample to Potential Markov
Game Criteria
FATEMEHFARDNO,UniversityofWaterloo,Canada
SEYEDMAJIDZAHEDI,UniversityofWaterloo,Canada
Thereareonlylimitedclassesofmulti-playerstochasticgamesinwhichindependentlearningisguaranteed
toconvergetoaNashequilibrium.Markovpotentialgamesareakeyexampleofsuchclasses.Priorworkhas
outlinedsetsofsufficientconditionsforastochasticgametoqualifyasaMarkovpotentialgame.However,
theseconditionsoftenimposestrictlimitationsonthegameâ€™sstructureandtendtobechallengingtoverify.
Toaddress these limitations, Mguniet al. [12] introducea relaxed notionofMarkovpotentialgames and
offeranalternativesetofnecessaryconditionsforcategorizingstochasticgamesaspotentialgames.Under
these conditions, the authors claim that a deterministic Nash equilibriumcan be computed efficiently by
solvingadualMarkovdecisionprocess.Inthispaper,weofferevidencerefutingthisclaimbypresentinga
counterexample.
1 INTRODUCTION
Inrecentyears,therehasbeenagrowinginterestinapplyingmulti-agentreinforcementlearning
to find strategies thatconverge to a Nash equilibrium in multi-player stochastic games [2, 5, 8â€“
11,14].Insingle-agentenvironments,manylearningalgorithmsareguaranteedtoconvergetoop-
timalpoliciesundersomemildconditions[15â€“17].However,deployingindependentsingle-agent
learning algorithmsin a multi-agent environment does not guaranteefinding Nash equilibrium
policies[6].Themainreasonforthisisthat,fromtheperspectiveofalearningagent,theenviron-
ment undergoes constant changes as other agents concurrently learn, and these environmental
changespartlydependontheactionsofthelearningagent.
Anotherchallengeindeployingmulti-agentlearningisefficiency.Thecomputationofastation-
aryNashequilibriumforgeneralstochasticgamesisknowntobecomputationallyintractable[3].
Consequently,therearenoefficientmulti-agentlearningalgorithmsforlearningstationaryNash
equilibriumstrategiesingeneralstochasticgames.However,therearespecificclassesofstochas-
tic gameswhereNash equilibrium strategies couldbecomputedefficiently.One notableclassis
Markovpotentialgames(MPGs),withinwhichmulti-agentlearning,andspecificallyindependent
learning,exhibitspromisingconvergenceproperties[7,10,11].
AnMPGischaracterizedbytheexistenceofaglobalfunction,knownasthepotentialfunction,
whereachangeinanagentâ€™slong-termpayoffduetoaunilateralchangeintheagentâ€™sstrategy
equatestothechangeinthepotentialfunction.Thisuniquepropertyfacilitatestheuseofefficient
multi-agentlearningmethods,suchastheindependentpolicygradient,ensuringconvergenceto
a stationary Nash equilibrium strategy [10]. However, determining whether a stochastic game
qualifiesasanMPGbysearchingforsuchapotentialfunctionisnotalwaysstraightforward.Con-
sequently,giventhehighlydesirablepropertiesofMPGs,itbecomesessentialtoidentifysufficient
conditionsforcategorizinggamesasMPGs.
Existingresearchhasestablishedsetsofsufficientconditionsforidentifyingastochasticgame
asanMPG[10,11].However,theseconditionsoftenimposestringentrestrictionsonthegameâ€™s
structure andmaynot beeasy to verify. To address these limitations, ina recentwork[12],the
authorsintroducea relaxednotionofMPGsandoffer analternative setofnecessaryconditions
forcategorizingstochasticgamesaspotentialgames.Theauthorsclaimthatmeetingthesecon-
ditions ensures theexistence of a deterministic stationary Nash equilibrium in thegame,which
Authorsâ€™addresses:FatemehFardno,UniversityofWaterloo,Waterloo,Canada,ffardno@uwaterloo.ca;SeyedMajidZa-
hedi,UniversityofWaterloo,Waterloo,Canada,smzahedi@uwaterloo.ca.
4202
yaM
31
]TG.sc[
1v60280.5042:viXracorrespondstotheoptimalsolutionofasingle-agentMarkovdecisionprocess(MDP)constructed
basedontheoriginalstochasticgame.Essentially,thedeterministicstationaryNashequilibrium
of the original stochastic game can be efficiently computed by solving its (dual) MDP. This, in
turn,guaranteestheconvergenceofmulti-agentlearningmethodstoNashequilibriumstrategies.
In this paper, we scrutinize this key claim and present a counterexample to Theorem 1 in [12],
establishingacasewherethetheoremdoesnothold.
2 BACKGROUNDANDRELATEDWORK
In this section, we first briefly introduce stochastic games. We then provide some background
on Markovpotential games. We then discuss sufficient conditions for a stochastic gameto be a
Markovpotentialgame.
2.1 StochasticGames
WestartbydefiningMarkovdecisionprocesses(MDP)asatooltostudydecisionmakinginsingle-
agentenvironments:
Definition1 (MDP). AMarkov decision processis atuple (ğ‘†,ğ´,ğ‘Ÿ,ğ‘).ğ‘† is the state space.ğ´ is
theactionspace.ğ‘Ÿ : ğ‘† Ã—ğ´ â†¦â†’ Risthepayofffunction.Andğ‘ : ğ‘† Ã—ğ´Ã—ğ‘† â†¦â†’ [0,1] isthetransition
probabilityfunction.
Please note that in this definition, the action space is assumed to be the same across all states.
Relaxingthisassumptionintroducesadditionalnotation,butapartfromthat,itdoesnotposeany
significantdifficultyorinsight.
ForanyMDP,astationarystrategyğœ‹ :ğ‘†Ã—ğ´â†¦â†’ [0,1]mapseachstate-actionpairtoaprobability.
Wewriteğœ‹(ğ‘|ğ‘ )todenotetheprobabilityoftakingactionğ‘instateğ‘ understrategyğœ‹.Astrategy
ğœ‹ iscalleddeterministicifforallstatesğ‘  âˆˆğ‘†,thereexistsanactionğ‘ âˆˆğ´forwhichğœ‹(ğ‘ |ğ‘ ) =1.
Stochasticgames(a.k.a.Markovgames)extendMDPstomulti-agentenvironments:
Definition2(Stochasticgame). Anğ‘›-agentstochastic gameisatuple (ğ‘,ğ‘†,ğ‘¨,ğ’“,ğ‘)1.ğ‘ is
thesetofagents.ğ‘† isthestatespace.ğ‘¨=ğ´ Ã—Â·Â·Â·Ã—ğ´ ,whereğ´ istheactionspaceofagentğ‘– âˆˆ ğ‘.
1 ğ‘› ğ‘–
ğ’“ =ğ‘Ÿ Ã—Â·Â·Â·Ã—ğ‘Ÿ ,whereğ‘Ÿ :ğ‘†Ã—ğ‘¨â†¦â†’Risthepayofffunctionforagentğ‘– âˆˆğ‘.Andğ‘ :ğ‘†Ã—ğ‘¨Ã—ğ‘† â†¦â†’ [0,1]
1 ğ‘› ğ‘–
isthetransitionprobabilityfunction.
Noteagainthattheactionspaceofeachagentisassumedtobethesameacrossallstates.Similar
toDefinition1,thisassumptioncanbeeasilyremoved.
Weuseğœ‹ to denotethestrategy of agentğ‘–.Thejointstrategy profile ofall agentsis denoted
ğ‘–
by ğ… = ğœ‹ Ã— Â·Â·Â· Ã—ğœ‹ . And the joint strategy profile of all agents except agentğ‘– is denoted by
1 ğ‘›
ğ… =ğœ‹ Ã—Â·Â·Â·Ã—ğœ‹ Ã—ğœ‹ Ã—Â·Â·Â·Ã—ğœ‹ 2.
âˆ’ğ‘– 1 ğ‘–âˆ’1 ğ‘–+1 ğ‘›
Ininfinite-horizonstochasticgames,thelong-termvalueofstateğ‘  toagentğ‘– forstrategyğ… is
theexpectedsumofagentğ‘–â€™sdiscountedpayoffs:
âˆ
ğ‘‰ ğ‘–ğ… (ğ‘ ) ,E ğ… ğ›¾ğ‘¡ğ‘Ÿ ğ‘–(ğ‘  ğ‘¡,ğ’‚ ğ’•) ğ‘  0 =ğ‘  , (1)
" Ã•ğ‘¡=0
(cid:12)
#
whereE ğ…[Â·]denotestheexpectedvalueofarandomvariable(cid:12) (cid:12) giventhatagentsfollowjointstrategy
profileğ…,andğ›¾ isthediscountfactor3.Agentsareconsideredtobeself-interested. Eachagentâ€™s
objectiveistomaximizeitsownlong-termvalue.Abest-responsestrategyisastrategythatachieves
1Weuseboldfonttorepresentvectors.
2Weusethenotationâˆ’ğ‘–toindicateallagentsexceptagentğ‘–.
3ğ›¾determineshowmuchagentsdiscountfuturepayoffs.thehighestvalueforanagentgivenotheragentsâ€™strategies[1].Nashequilibriumisajointstrategy
whereeachagentâ€™sstrategyisabestresponsetoothersâ€™:
Definition3(ğ-Nasheqilibrium). Letğœ– â‰¥ 0.Theninanğ‘›-agentstochasticgame,anğœ–-Nash
equilibriumisastrategyprofileğ…âˆ— =ğœ‹âˆ—,...ğœ‹âˆ— suchthat:
1 ğ‘›
ğ‘‰ğ…âˆ—
(ğ‘ )
â‰¥ğ‘‰(ğœ‹ğ‘–,ğ… âˆ’âˆ— ğ‘–)
(ğ‘ )âˆ’ğœ–
ğ‘– ğ‘–
forallstatesğ‘  âˆˆğ‘†,allagentsğ‘– âˆˆğ‘,andallstrategiesğœ‹ âˆˆ Î  ,whereÎ  isthesetofallstrategiesfor
ğ‘– ğ‘– ğ‘–
agentğ‘–.
Whenğœ–
=0,wesimplycallthisaNashequilibrium.ANashequilibriumstrategyğ…âˆ—
isstationary
if ğœ‹âˆ— is stationary for all agentsğ‘– âˆˆ ğ‘. All stochastic games have at least one stationary Nash
ğ‘–
equilibrium[4].
Computing a stationary ğœ–-Nash equilibrium for general stochastic games is computationally
intractable[3]4.Thisimpliesthattherearenoefficientmulti-agentreinforcementlearningalgo-
rithmsforlearningstationaryNashequilibriumstrategiesingeneralstochasticgames.However,
therearespecificclassesofstochasticgamesforwhichNashequilibriumstrategiescouldbecom-
putedefficiently.AkeyexampleisMarkovpotentialgames(MPG).
2.2 MarkovPotentialGames
MondererandShapleyintroducetheconceptofpotentialgamesinthenormalform[13].Potential
gamesrepresent multi-agentcoordination,asall agentsâ€™ payoffsareperfectlyalignedwith each
other via a potential function. MPGs extend the concept of potential games from normal-form
gamestostochasticgames.AstochasticgamequalifiesasanMPGifthereexistsaglobalfunction,
calledthepotentialfunction,suchthatifanyagentunilaterallychangestheirstrategy,thechange
in their long-term value for each state mirrors precisely the change observed in the potential
functionatthatparticularstate:
Definition4(MPG). AstochasticgameisanMPGifthereexistsastrategy-dependentfunction
ğœ™ğ… :ğ‘† â†¦â†’Rforstrategiesğ… âˆˆ ğš·suchthat:
ğ‘‰ğ… (ğ‘ )âˆ’ğ‘‰(ğœ‹ğ‘–â€²,ğ… âˆ’ğ‘–) (ğ‘ ) =ğœ™ğ… (ğ‘ )âˆ’ğœ™(ğœ‹ğ‘–â€²,ğ… âˆ’ğ‘–)(ğ‘ )
ğ‘– ğ‘–
forallagentsğ‘– âˆˆğ‘,allstatesğ‘  âˆˆğ‘†,allstrategiesğ… = (ğœ‹ ,ğ… ) âˆˆ ğš·,andallstrategiesğœ‹â€² âˆˆ Î  .
ğ‘– âˆ’ğ‘– ğ‘– ğ‘–
AnyMPGhasatleastonestationary Nashequilibriumstrategyprofilethatisdeterministic [10].
Furthermore,independentlearningconvergestoanğœ–-NashequilibriumstrategyinMPGs:
Theorem1([10,Theorem1.1]). Inanğ‘›-agentMPG,ifallagentsrunindependentpolicygradi-
ent,thenforanyğœ– > 0,thelearningdynamicsreachesanğœ–-Nashequilibriumstrategyafterğ‘‚(1/ğœ–2)
iterations.
Themain idea behind the aforementioned theorem is that in MPGs, applying projected gradient
ascent (PGA) on thepotential functionğœ™ leads to the emergenceof anğœ–-Nash equilibrium. And
the key element in the proof of this theorem is the equality of the derivatives between value
functions and the potential function in MPGs. More recently, in [7], the authors show that in
anMPG,independentnaturalpolicygradientalsoconvergestoanequilibrium.
4Infact,theauthors showthat computing astationarycoarse-correlated equilibrium,whichisamore relaxednotion
comparedtoNashequilibrium,ingeneralstochasticgamesiscomputationallyintractable.2.3 FromStochasticGamestoMPG
InDefinition4,theconditionisfairlystronganddifficulttoverifyinpracticeforgeneralstochas-
tic games. Given the MPGsâ€™ desiderata, it becomes imperative to delineate the specific types of
stochasticgamesthatalignwiththecriteriaoutlinedinDefinition4.Tothisend,priorworkhas
providedsetsofsufficientconditions[10,11].Todiscusstheseconditions,wefirstneedtodefine
theclassofone-shotpotentialstochasticgames(OPSGs).WedefineastochasticgameasanOPSG
ifimmediatepayoffsatanystatearecapturedbyapotentialgameatthatstate:
Definition5(OPSG). Anğ‘›-aagentstochastic gameisOPSGifthereexists aone-shotpotential
functionÎ¦:ğ‘†Ã—ğ‘¨â†¦â†’Rsuchthat:
ğ‘Ÿ (ğ‘ ,ğ’‚)âˆ’ğ‘Ÿ (ğ‘ ,ğ‘â€²,ğ’‚ ) =Î¦(ğ‘ ,ğ’‚)âˆ’Î¦(ğ‘ ,ğ‘â€²,ğ’‚ )
ğ‘– ğ‘– ğ‘– âˆ’ğ‘– ğ‘– âˆ’ğ‘–
forallğ‘– âˆˆğ‘,allstatesğ‘  âˆˆğ‘†,allactionprofilesğ’‚ = (ğ‘ ,ğ’‚ ) âˆˆğ‘¨,andallactionsğ‘â€² âˆˆğ´ .
ğ‘– âˆ’ğ‘– ğ‘– ğ‘–
In[10],theauthorsshowthatanOPSGisMPGifeitherofthetwofollowingconditionshold:
(i)agent-independenttransitionsand(ii)equalityofindividualdummyterms.(i)holdsifthegameâ€™s
transitionprobabilityfunctiondoesnotdependonagentsâ€™jointaction:
Condition1(Agent-independenttransitions). AnOPSGhasagent-independenttransi-
tionsifforallstatesğ‘ ,ğ‘ â€² âˆˆğ‘† andactionprofilesğ’‚ âˆˆğ‘¨:
ğ‘(ğ‘ â€² |ğ‘ ,ğ’‚) =ğ‘(ğ‘ â€² |ğ‘ ).
And(ii)holdsifthedummytermsofeachagentâ€™simmediatepayoffsareequalacrossallstates:
Condition2(Eqalityof individual dummy terms). An OPSGwith one-shot potential
function Î¦ satisfies the equality of individualdummy terms iffor each agentğ‘– âˆˆ ğ‘, there exists a
functionğ‘£ :ğ‘†Ã—ğ‘¨ â†¦â†’Rsuchthat:
ğ‘– âˆ’ğ‘–
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ’‚ ) =Î¦(ğ‘ ,ğ‘ ,ğ’‚ )+ğ‘£ğ‘–(ğ‘ ,ğ’‚ ),
ğ‘– ğ‘– âˆ’ğ‘– ğ‘– âˆ’ğ‘– âˆ’ğ‘–
and
âˆ
âˆ‡ E ğ›¾ğ‘¡ğ‘£ (ğ‘  ,ğ’‚ğ‘¡ ) ğ‘  =ğ‘  =ğ‘ 1
ğœ‹ğ‘–(ğ‘ ) ğ‘– ğ‘¡ âˆ’ğ‘– 0 ğ‘ 
" Ã•ğ‘¡=0
(cid:12)
#
forallstatesğ‘ ,ğ‘ â€² âˆˆğ‘†,ğ‘
ğ‘ 
âˆˆR,and1 âˆˆR|ğ´ğ‘–|,whereğœ‹ ğ‘–(ğ‘ )(cid:12) (cid:12)isthestrategyofagentğ‘– atstateğ‘ .
In[12],the authorsarguethat Condition 1 and Condition 2 impose strong limitations onthe
structureofone-shotpotentialstochasticgames.Toavoidtheselimitations,theauthorspropose
analternativecondition:
Condition3(Statetransitivity). AnOPSGwithone-shotpotentialfunctionÎ¦satisfiesstate
transitivityifwehave:
ğ‘Ÿ (ğ‘ ,ğ’‚)âˆ’ğ‘Ÿ (ğ‘ â€²,ğ’‚) =Î¦(ğ‘ ,ğ’‚)âˆ’Î¦(ğ‘ â€²,ğ’‚)
ğ‘– ğ‘–
forallagentsğ‘– âˆˆğ‘,allstatesğ‘ ,ğ‘ â€² âˆˆğ‘†,andallactionprofilesğ’‚ âˆˆğ´.
Statetransitivityensurethatthedifferenceinimmediatepayoffsforchangingstateisthesamefor
eachagent.Theauthorsthenpresentatheoremthatclaimsthefollowing.
Claim1([12,Theorem1]). Letğº â‰” (ğ‘,ğ‘†,ğ‘¨,ğ’“,ğ‘)beanOPSGwithone-shotpotentialfunction
Î¦. Suppose that ğº satisfies Condition 3. Then ğº has a deterministic stationary Nash equilibrium
that correspondsto the optimal solution of the dual MDP definedasğºâ€² â‰” (ğ‘†,ğ‘¨,Î¦,ğ‘).That is, the
deterministicstationaryNashequilibriumofğº canbeefficientlycomputedbysolvingğºâ€².3 ANALYSIS
Inthissection,weprovideacounterexampleforwhichClaim1failstohold.
3.1 Counterexample
Since action space and state space are assumed to be continuous in [12], our counterexample
includescontinuousactionandstatespaces.Similarly,since[12]focusesoninfinite-horizongames,
ourcounterexampleconsidersaninfinite-horizongame.
Considergameğº,aninfinite-horizon,two-agentstochasticgamedefinedas:
â€¢ Thesetofagentsisğ‘ ={1,2}.
â€¢ Thecontinuousstatespaceisğ‘† = [0,1].
â€¢ Theactionspacesareğ´ =ğ´ = [0,1].
1 2
â€¢ Thepayofffunctionsforanystateğ‘  âˆˆğ‘† andanyactionprofileğ‘ = (ğ‘ ,ğ‘ ) are:
1 2
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ ) =ğ‘ âˆ’(ğ‘ âˆ’ğ‘ )2âˆ’4/(2âˆ’ğ‘ ),
1 1 2 2 2
and
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ ) =ğ‘ âˆ’(ğ‘ âˆ’ğ‘ )2.
2 1 2 2
â€¢ Thestatetransitionfunctiononlydependsontheactionofagent1andcanbewrittenas:
1 ifğ‘ â€² =ğ‘ , and
ğ‘(ğ‘ â€² |ğ‘ ,ğ‘ ,ğ‘ ) = 1
1 2
(0 otherwise.
It can be easily verified the aforementioned gameğº satisfies all the assumptions in [12]. In
particular,thepayofffunctionsarebounded,measurablefunctionsintheactions,Lipschitz, and
continuouslydifferentiableinthestateandactions.
Next,weshowthatğº isanOPSG.Wedothisbyshowingthatimmediatepayoffsatanystate
arecapturedbyapotentialgameatthatstate.Considerthefollowingpotentialfunction:
Î¦(ğ‘ ,ğ‘ ,ğ‘ ) =ğ‘ âˆ’(ğ‘ âˆ’ğ‘ )2.
1 2 2
Itiseasytoseethatğº isapotentialgameateachstateğ‘  âˆˆğ‘† withthepotentialfunctionÎ¦(ğ‘ ,Â·):
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ )âˆ’ğ‘Ÿ (ğ‘ ,ğ‘â€²,ğ‘ ) =Î¦(ğ‘ ,ğ‘ ,ğ‘ )âˆ’Î¦(ğ‘ ,ğ‘â€²,ğ‘ ) =0,
1 1 2 1 1 2 1 2 1 2
and
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ )âˆ’ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘â€²) =Î¦(ğ‘ ,ğ‘ ,ğ‘ )âˆ’Î¦(ğ‘ ,ğ‘ ,ğ‘â€²) = (ğ‘ âˆ’ğ‘â€²)2âˆ’(ğ‘ âˆ’ğ‘ )2.
2 1 2 2 1 2 1 2 1 2 2 2
ItisalsoeasytoseethatğºsatisfiesCondition3forallstatesğ‘ ,ğ‘ â€² âˆˆğ‘†andactionprofilesğ‘ = (ğ‘ ,ğ‘ ):
1 2
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ )âˆ’ğ‘Ÿ (ğ‘ â€²,ğ‘ ,ğ‘ ) =Î¦(ğ‘ ,ğ‘ ,ğ‘ )âˆ’Î¦(ğ‘ â€²,ğ‘ ,ğ‘ ) = (ğ‘ âˆ’ğ‘ â€²)âˆ’(ğ‘ âˆ’ğ‘ )2+(ğ‘ â€²âˆ’ğ‘ )2,
1 1 2 1 1 2 1 2 1 2 2 2
and
ğ‘Ÿ (ğ‘ ,ğ‘ ,ğ‘ )âˆ’ğ‘Ÿ (ğ‘ â€²,ğ‘ ,ğ‘ ) =Î¦(ğ‘ ,ğ‘ ,ğ‘ )âˆ’Î¦(ğ‘ â€²,ğ‘ ,ğ‘ ) = (ğ‘ âˆ’ğ‘ â€²)âˆ’(ğ‘ âˆ’ğ‘ )2+(ğ‘ â€²âˆ’ğ‘ )2.
2 1 2 2 1 2 1 2 1 2 2 2
Forcontradiction,letusassumethatClaim1holdsforğº.Thenwecanconstructğºâ€™sdualMDP,
ğºâ€²,asfollows.Theactionspaceisğ‘¨ =ğ´ Ã—ğ´ .Theactionateachstateisğ’‚ = (ğ‘ ,ğ‘ ) âˆˆ ğ‘¨.And
1 2 1 2
thepayofffunctionis:
ğ‘Ÿ(ğ‘ ,(ğ‘ ,ğ‘ )) =Î¦(ğ‘ ,ğ‘ ,ğ‘ ) =ğ‘ âˆ’(ğ‘ âˆ’ğ‘ )2.
1 2 1 2 2
Finally,ğºâ€²hasthesametransitionprobabilityfunctionasğº.ItcanbeeasilyshownthatthisMDP
hasthefollowingunique(deterministic)optimalstrategy:
1 if (ğ‘ ,ğ‘ ) = (1,ğ‘ ), and
ğ…âˆ—((ğ‘ ,ğ‘ ) |ğ‘ ) = 1 2 (2)
1 2
(0 otherwise.Thisoptimaljointstrategyprescribestakingğ‘ =1andğ‘ =ğ‘  inanystateğ‘  âˆˆğ‘†.
1 2
Next,weshowthatthisjointstrategyprofileisnotaNashequilibriumofğº.Toseethis,suppose
thatagent2â€™sstrategyistotakeğ‘ =ğ‘  ineverystateğ‘ .Byfixingagent2â€™sstationarystrategy,we
2
canfindagent1â€™sbestresponsebyconstructinganMDPwiththeimmediatepayofffunctionof:
ğ‘Ÿ (ğ‘ ,ğ‘ ) =ğ‘ âˆ’4/(2âˆ’ğ‘ ). (3)
1 1
InthisMDP,agent1â€™sactiondoesnotdirectlyaffecttheimmediatepayoffateachstate.However,
agent1â€™sactionsaffectthelong-termpayoffbydeterminingthenextstatesthroughthetransition
probability function.Given(3),agent1â€™slong-termpayoffis maximizedwhenğ‘  = 0.Hence,the
uniqueoptimalstrategyofagent1istotakeğ‘ =0ateverystateğ‘ .Thismeansthatğ…âˆ—in(2)does
1
notcorrespondtoğºâ€™sstationaryNashequilibrium,acontradiction!
WenotethatastationaryNashequilibriumofğº isforagent1and2torespectivelytakeğ‘ =0
1
andğ‘ = ğ‘  inallstatesğ‘  âˆˆ ğ‘†.Starting fromğ‘  = 0,theaveragepayoffofagent1underthisNash
2
equilibriumis-2,andtheaveragepayoffofagent2is0.
4 CONCLUSION
Inthispaper,wefirstintroducedstochasticgamesbriefly.Wethenprovidedbackgroundinforma-
tiononMarkovpotential gamesanddiscussedthesufficientconditionsforastochastic gameto
beclassifiedasaMarkovpotentialgame.Furthermore,weexaminedthemainclaimof[12]and
presented a counterexample to its Theorem 1, demonstrating that the theorem does not always
hold.
REFERENCES
[1] LawrenceEBlume.1995.Thestatisticalmechanicsofbest-responsestrategyrevision.GamesandEconomicBehavior
11,2(1995),111â€“145.
[2] VivekSBorkar.2002.ReinforcementlearninginMarkovianevolutionarygames.AdvancesinComplexSystems5,01
(2002),55â€“72.
[3] ConstantinosDaskalakis,NoahGolowich,andKaiqingZhang.2023. ThecomplexityofMarkovequilibriuminsto-
chasticgames.InThe36thAnnualConferenceonLearningTheory.4180â€“4234.
[4] ArlingtonMFink.1964. Equilibriuminastochasticn-persongame. JournalofScienceoftheHiroshimaUniversity,
seriesai(mathematics)28,1(1964),89â€“93.
[5] Jakob Foerster,Richard YChen,MaruanAl-Shedivat,ShimonWhiteson,PieterAbbeel,andIgor Mordatch.2018.
LearningwithOpponent-LearningAwareness.(2018),122â€“130.
[6] JakobFoerster,NantasNardelli,GregoryFarquhar,TriantafyllosAfouras,PhilipHSTorr,PushmeetKohli,andShimon
Whiteson.2017.Stabilisingexperiencereplayfordeepmulti-agentreinforcementlearning.InProceedingsofthe34th
InternationalConferenceonMachineLearning(ICML).1146â€“1155.
[7] RoyFox,StephenMMcaleer,WillOverman,andIoannisPanageas.2022. Independentnaturalpolicygradiental-
waysconvergesinMarkovpotentialgames.InProceedingsoftheInternationalConferenceonArtificialIntelligenceand
Statistics.4414â€“4425.
[8] Hongyi Guo, Zuyue Fu,Zhuoran Yang, and ZhaoranWang. 2021. Decentralizedsingle-timescale actor-critic on
zero-sumtwo-playerstochasticgames.InProceedingsoftheInternationalConferenceonMachine Learning(ICML).
3899â€“3909.
[9] JunlingHuandMichaelPWellman.2003. NashQ-learningforgeneral-sumstochasticgames. JournalofMachine
LearningResearch4(2003),1039â€“1069.
[10] StefanosLeonardos,WillOverman,IoannisPanageas,andGeorgiosPiliouras.2021. Globalconvergenceofmulti-
agentpolicygradientinMarkovpotentialgames.arXivpreprintarXiv:2106.01969(2021).
[11] SergioValcarcelMacua,JavierZazo,andSantiagoZazo.2018. Learningparametricclosed-looppoliciesforMarkov
potentialgames.arXivpreprintarXiv:1802.00899(2018).
[12] DavidHMguni,YutongWu,YaliDu,YaodongYang,ZiyiWang,MinneLi,YingWen,JoelJennings,andJunWang.
2021. Learninginnonzero-sumstochasticgameswithpotentials.InProceedingsoftheInternationalConferenceon
MachineLearning(ICML).7688â€“7699.
[13] DovMondererandLloydSShapley.1996.Potentialgames.GamesandEconomicBehavior14,1(1996),124â€“143.[14] JulienPÃ©rolat,FlorianStrub,BilalPiot,andOlivierPietquin.2017.LearningNashequilibriumforgeneral-sumMarkov
gamesfrombatchdata.InProceedingsofthe20thInternationalConferenceonArtificialIntelligenceandStatistics.232â€“
241.
[15] RichardSSuttonandAndrewGBarto.2018.Reinforcementlearning:Anintroduction. MITpress.
[16] ChristopherJCHWatkinsandPeterDayan.1992.Q-learning.MachineLearning8(1992),279â€“292.
[17] ChristopherJohnCornishHellabyWatkins.1989.Learningfromdelayedrewards.(1989).