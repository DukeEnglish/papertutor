Compositional Curvature Bounds for Deep Neural Networks
TahaEntesari*1 SinaSharifi*1 MahyarFazlyab1
Abstract L ‚â•0suchthat
f
‚à•f(x)‚àíf(y)‚à•‚â§L ‚à•x‚àíy‚à• ‚àÄx,y.
f
Akeychallengethatthreatensthewidespreaduse
ofneuralnetworksinsafety-criticalapplications This constant quantifies the sensitivity of the model f to
is their vulnerability to adversarial attacks. In inputperturbations,motivatingtheneedtoestimateL and
f
thispaper,westudythesecond-orderbehaviorof controlitthrougharchitectureorthetrainingprocess.
continuouslydifferentiabledeepneuralnetworks,
Forcontinuouslydifferentiablefunctions,L isatightupper
focusingonrobustnessagainstadversarialpertur- f
boundonthefirstderivative(‚à•Df(x)‚à•‚â§L ‚àÄx).However,
bations. First,weprovideatheoreticalanalysis f
onecangoonestepfurtherandleveragethesmoothnessof
ofrobustnessandattackcertificatesfordeepclas-
thefirstderivative,i.e.,boundsonthesecondderivativeto
sifiers by leveraging local gradients and upper
obtainamorerefinedmeasureofthefunction‚Äôssensitivity.
boundsonthesecondderivative(curvaturecon-
Indeed,themeritofsecond-orderinformationincharacter-
stant). Next, weintroduceanovelalgorithmto
izingandenhancingrobustnesshasbeenestablished,e.g.,
analyticallycomputeprovableupperboundson
(Singla&Feizi,2020).
the second derivative of neural networks. This
algorithmleveragesthecompositionalstructure OurContributions: Inthiswork,weseektocharacterize
of the model to propagate the curvature bound the adversarial robustness of continuously differentiable
layer-by-layer,givingrisetoascalableandmod- neuralnetworkclassifiersthroughtheLipschitzconstantof
ularapproach. Theproposedboundcanserveas theirfirstderivativedefinedas
a differentiable regularizer to control the curva-
‚à•Df(x)‚àíDf(y)‚à•‚â§L ‚à•x‚àíy‚à• ‚àÄx,y
tureofneuralnetworksduringtraining,thereby Df
enhancing robustness. Finally, we demonstrate
If f is twice differentiable, this constant is a tight upper
theefficacyofourmethodonclassificationtasks bound on the second derivative, ‚à•D2f(x)‚à• ‚â§ L ‚àÄx.
Df
usingtheMNISTandCIFAR-10datasets.
With a slight abuse of the formal definition, we denote
L asthe‚Äúcurvature‚Äùconstant. Ourcontributionsareas
Df
follows.
1.Introduction
‚Ä¢ Weprovideatheoreticalanalysisoftheinterplaybetween
Neuralnetworksareinfamouslypronetoadversariallyde- adversarialrobustnessandsmoothness. Specifically,for
signedperturbations(Szegedyetal.,2013). Toaddressthis classificationtasks,wederivelowerboundsonthemargin
vulnerability,manymethodshavebeenproposedtoquantify ofcorrectlyclassifieddatapointsusingthefirstderivative
andimprovetherobustnessofthesemodelsagainstadver- (the Jacobian) and its Lipschitz constant (the curvature
sarial attacks, such as adversarial training (Zhang et al., constant).Wethenshowthatthesecurvature-basedcertifi-
2019;Madryetal.,2018),regularization(Leinoetal.,2021; catesprovablyimproveuponLipschitz-basedcertificates,
Tsuzukuetal.,2018),randomizedsmoothing(Cohenetal., providedthecurvatureissufficientlysmall.
2019;Kumaretal.,2021),andmanyothers.Onemeasureof
‚Ä¢ Weproposeanovelalgorithmtoderiveanalyticalupper
robustnessistheLipschitzconstantdefinedasthesmallest
boundsonthecurvatureconstantofneuralnetworks.This
*Equalcontribution 1DepartmentofElectricalandComputer algorithm leverages the compositional structure of the
Engineering,JohnsHopkinsUniversity,Baltimore,UnitedStates modeltocomputetheboundinascalableandmodular
of America. Correspondence to: Mahyar Fazlyab <mahyarfa- fashion, improving upon previous works that only con-
zlyab@jhu.edu>.
siderscalar-valuednetworkswithaffine-then-activation
architectures. Thederivedboundisdifferentiableandcan
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by beusedasaregularizerfortraininglow-curvatureneural
theauthor(s). networks.
1
4202
nuJ
7
]GL.sc[
1v91150.6042:viXraCompositionalCurvatureBoundsforDeepNeuralNetworks
‚Ä¢ We introduce a relaxed notion of smoothness, the An- LipschitzConstantCalculation: Inrecentyears,there
chored Lipschitz constant, which significantly reduces hasbeenafocusonfindingaccurateboundsontheLipschitz
conservatism in terms of robustness certification. Suc- constantofneuralnetworks. Hereweonlydiscusstheones
cinctly,thisdefinitionfixesoneofthetwopointsinvolved thatcanhandlecontinuously-differentiablenetworks. One
in the definition of Lipschitz continuity to the point of oftheearlyworks,(Szegedyetal.,2013),providedabound
interest. on the Lipschitz constant using the norm of each layer,
whichisknowntobealoosebound. (Fazlyabetal.,2019)
‚Ä¢ Wealsopresentempiricalresultsdemonstratingtheper- formulatedtheproblemoffindingtheLipschitzconstantas
formanceofourmethodcomparedtopreviousworkson asemidefiniteprogram(SDP),providingaccuratebounds
calculatingcurvaturebounds,andweexaminetheimpact butattheexpenseoflimitedscalability. Later, (Hashemi
oflowcurvaturesontherobustnessofdeepclassifiers. et al., 2021) introduced a local version of LipSDP. Most
recently,(Fazlyabetal.,2023)proposedLipLT,ananalytic
Tothebestofourknowledge,thispaperisthefirsttode- methodforboundingtheLipschitzconstantthroughloop
velopamethodforobtainingprovableboundsonthesecond transformation,acontrol-theoreticconcept.Inthiswork,we
derivativeofgeneralsequentialneuralnetworks. Whilewe alsoleverageLipLTtoderiveupperboundsonthecurvature
consideradversarialrobustnessasanapplicationdomain, constant.
theproposedmethodisalsoofindependentinterestforother
Mostrelevanttooursetup,(Singla&Feizi,2020)develops
applicationsrequiringdifferentiableboundsonthesecond
amethodtoboundthecurvatureconstantofscalar-valued
derivativeofneuralnetworks,suchaslearning-basedcontrol
neural networks in the ‚Ñì norm and introduces a numer-
2
forsafety-criticalapplications(Robeyetal.,2020).
ical optimization scheme to provide curvature-based cer-
tificates. Incontrast,ourmethodboundsthecurvatureof
1.1.RelatedWork
arbitraryfunctioncompositions,inparticularvector-valued
Withrespecttothelargebodyofworkinthisfield,here,we feedforwardneuralnetworks,inany‚Ñì pnorm,andprovides
focusontheworksthataremorerelevanttooursetup. analyticalcurvature-basedcertificates.
1.2.PreliminariesandNotation
AdversarialRobustness: Therobustnessofdeepmodels
againstadversarialperturbationshasbeenatopicofinter- We denote the n-dimensional real numbers as Rn. For
estinrecentyears(Singla&Feizi,2021;2022;Xuetal., a vector x ‚àà Rn, x is its i-th element. For a matrix
i
2022;Zouetal.,2023). (Huangetal.,2021;Fazlyabetal., W ‚àà Rn√óm, W ‚àà R1√óm,W ‚àà Rn,W ‚àà R are
i,: :,j i,j
2023)usetheLipschitzconstantofthenetworkduringthe
thei-throw,j-thcolumn,andthej-thelementofW ,re-
i,:
training procedure to induce robustness by bounding the
spectively. Foravectorx,diag(x)isthediagonalmatrix
worst-caselogits. Toachieverobustness,insteadofpenaliz-
withdiag(x) = x andzerootherwise. Foranintegern
ii i
ingorconstrainingtheLipschitzconstantduringtraining,
let [n] = {1,¬∑¬∑¬∑ ,n}. Moreover, the operator norm of a
somemethodsdirectlyconstruct1-Lipschitznetworks. The
matrixAisdenotedas‚à•A‚à• = sup ‚à•Ax‚à• . For
use of Lipschitz bounded networks has been encouraged areal-valuedp‚â•1,wedenop te‚Üí iq tsH¬®olde‚à• rx c‚à• op n‚â§ ju1 gatewq ithp‚àó,
by many recentworks (Be¬¥thune etal., 2022)as they pro- i.e., 1 + 1 = 1. Foranyvectorx ‚àà Rn andnorm‚à•¬∑‚à• ,
videdesirablepropertiessuchasrobustnessandimproved p p‚àó p
wehave‚à•x‚à• =sup x‚ä§y.
generalization. AOL(Prach&Lampert,2022)providesa p‚àó ‚à•y‚à•p‚â§1
rescalingofthelayerweightsthatmakeseachlinearlayer Afunctionf : Rn ‚Üí Rm isLipschitzcontinuousonC ‚äÜ
1-Lipschitz. ToobtainLipschitzboundednetworks,many Rn if there exists a non-negative constant Lp,q such that
f
works have utilized LipSDP (Fazlyab et al., 2019) to pa- ‚à•f(x)‚àíf(y)‚à• ‚â§Lp,q‚à•x‚àíy‚à• ‚àÄx,y ‚ààC. Thesmallest
q f p
rameterize 1-Lipschitz layers. SLL (Araujo et al., 2022) such Lp,q is the Lipschitz constant, in the corresponding
f
proposes1-LipschitzresiduallayersbysatisfyingLipSDP, norms,whichisgivenby
and(Fazlyabetal.,2023)generalizesSLLbyproposinga
‚àö
œÅ-Lipschitzlayer. Mostrecently,(Wang&Manchester,
‚à•f(x)‚àíf(y)‚à•
2023) satisfies the LipSDP condition using Caley Trans- Lp,q = sup q.
f ‚à•x‚àíy‚à•
formsandproposesanon-residual1-Lipschitzlayer. x,y‚ààC,xÃ∏=y p
Otherworkslookbeyondthenetwork‚Äôsfirst-orderproper- Forbrevity,wedenoteLp,pasLp. Inthiswork,wedefine
tiesandcontrolthenetwork‚Äôscurvature(Moosavi-Dezfooli f f
anewnotionofLipschitzcontinuityataneighborhoodofa
et al., 2019; Singla et al., 2021). (Srinivas et al., 2022)
point.
proposesusingcentered-softplusactivationsandLipschitz-
boundedbatchnormalizationstocapthecurvatureandem- Definition1.1(AnchoredLipschitzconstant). Forafunc-
piricallyimproverobustness. tionf,theanchoredLipschitzconstantatapointx‚ààC is
2CompositionalCurvatureBoundsforDeepNeuralNetworks
definedas 2.1.RobustnessCertificatesforDeepClassifiers
Lp f,q(x)= sup ‚à•f( ‚à•x x) ‚àí‚àí yf ‚à•(y)‚à• q. C n Kons ci ld ae ssr ea s,c wla hss ei rfi ee frC :( Rx) n0:= ‚Üíar Rg nm Kax is1‚â§ ai‚â§ nen uK raf li( nx e) tww oi rth k
y‚ààC,xÃ∏=y p
that parameterizes the vector of logits. For a given input
x with correct label y ‚àà [n ], the condition for correct
K
Atanypointx,thisconstantisalowerboundontheLips- classificationis
chitzconstantasonecanconfirmLp,q = sup Lp,q(x).
f x‚ààC f
Figure1demonstratesthisconceptfurtherforthespecific f iy(x):=f i(x)‚àíf y(x)<0, ‚àÄiÃ∏=y.
caseofthetanhfunction.
Assumingthatxiscorrectlyclassifiedasy,thedistanceof
Inthefollowinglemma,weestablishtherelationbetween xtotheclosestdecisionboundarymeasurestheclassifier‚Äôs
theanchoredLipschitzconstantandthenormofthederiva- local robustness against additive perturbations. We can
tive. computethisdistancebysolvingthefollowingoptimization
problem,
Lemma1.2. Consideradifferentiablefunctionf :Rn ‚Üí
Rm andletLp(x)beacorrespondinganchoredLipschitz Œµ‚àó(x)=max Œµ
f
constant. Wehave (1)
s.t. sup f (x+Œ¥)‚â§0, ‚àÄiÃ∏=y.
iy
‚à•Œ¥‚à•p‚â§Œµ
‚à•Df(x)‚à• ‚â§Lp(x).
p f
For ReLU networks and p ‚àà {1,‚àû}, this optimization
problemcanbeencodedasaMixed-IntegerLinearprogram
SeeAppendixAfortheproof.
(MILP)byexploitingthepiece-wisenatureoftheactivation
GivenboundednumbersŒ± ‚â§ Œ≤,afunctionœï : R ‚Üí Ris functions(Duttaetal.,2018;Fischetti&Jo,2018;Tjeng
sloperestrictedin[Œ±,Œ≤]if et al., 2018). While these MILPs can be solved globally,
theysufferfrompoorscalability. Forneuralnetworkswith
œï(x)‚àíœï(y) differentiableactivationfunctions,eventhismixed-integer
Œ±‚â§ ‚â§Œ≤, ‚àÄx,y.
x‚àíy structure is absent, making the exact computation of dis-
tanceseffectivelyintractable. Therefore,wemustresortto
The Lipschitz constant of œï is then L œï = max(|Œ±|,|Œ≤|). findinglowerboundsonthecertifiedradiustogaintractabil-
Forsimplicity,andbasedoncommonly-useddifferentiable ity.
activationfunctionssuchassigmoidandtanh,weassume
thatœïismonotone,i.e.,Œ±‚â•0,implyingthatL œï =Œ≤. 2.1.1.LIPSCHITZ-BASEDCERTIFICATES
Supposethef ‚ÄôsareLipschitzcontinuous. Wecanthen
2.Curvature-basedRobustnessAnalysis iy
write
Consideracontinuouslydifferentiablefunctionf :Rn ‚Üí f iy(x+Œ¥)‚â§f iy(x)+Lp fiy(x)‚à•Œ¥‚à• p. (2)
Rm parameterizedbyaneuralnetwork. Inthiswork,our whereLp (x) > 0istheanchored Lipschitzconstantof
goal is to derive provable upper bounds on the Lipschitz f . Bysf uiy bstituting(2)intheconstraintsof(1),weobtain
iy
constantoftheJacobianDf :Rn ‚ÜíRm√ón,definedasthe
the following optimization problem to compute a zeroth-
smallestconstantLp D,q f suchthat order(gradient-free)lowerbound,
‚à•Df(x 1)‚àíDf(x 2)‚à•
q
‚â§Lp D,q f‚à•x 1‚àíx 2‚à• p, ‚àÄx 1,x 2. Œµ‚àó 0(x)=max Œµ
s.t. sup f (x)+Lp (x)‚à•Œ¥‚à• ‚â§0,‚àÄiÃ∏=y.
iy fiy p
Furthermore,wecanextendthistotheanchoredLipschitz ‚à•Œ¥‚à•p‚â§Œµ
constantoftheJacobian,Lp D,q f(x),atagivenpointx,as Wenotethatduetoconstrainttightening,wehaveŒµ‚àó 0(x)<
Œµ‚àó(x). Usingsimilarargumentsasin(Fazlyabetal.,2023),
‚à•Df(x+Œ¥)‚àíDf(x)‚à• ‚â§Lp,q(x)‚à•Œ¥‚à• , ‚àÄŒ¥. Œµ‚àó(x)hastheclosed-formexpression
q Df p 0
‚àíf (x)
Œµ‚àó(x)=min iy . (3)
Whileprovidingprovableupperboundsontheseconstants 0 iÃ∏=y Lp (x)
can be instrumental in various applications, in this work,
fiy
we primarily focus on the adversarial robustness of deep
2.1.2.CURVATURE-BASEDCERTIFICATES
classifiers. Wedevelopourmethodsandcertificatesbased
ontheLipschitzcontinuityoftheclassifieranditsJacobian. Whenthemodeliscontinuouslydifferentiable,wecanex-
Weelaboratemoreonthisinthefollowingsubsections. ploititscurvaturetoimprovethecertificatein(3). Specif-
3CompositionalCurvatureBoundsforDeepNeuralNetworks
Proposition 2.2. Suppose x is classified correctly, i.e.,
1.5 f iy(x) < 0 ‚àÄi Ã∏= y. Fix a p ‚â• 1, and define the zeroth-
Tanh
1.0
A Wn oc rh sto r Ce ad
s
eS l So lp oe
pe
orderŒµ‚àó 0(x)andfirst-orderŒµ‚àó 1(x)certifiedradiiasin(3)and
(6). Ifthefollowingconditionholds,
Anchor Point
0.5
0.0
Lp,p‚àó
(x)‚â§
‚àí2(‚à•‚àáf iy(x)‚à• p‚àóŒµ‚àó 0(x)+f iy(x))
, iÃ∏=y.
‚àáfiy Œµ‚àó(x)2
0
0.5
ThenŒµ‚àó(x)‚â•Œµ‚àó(x).
1.0 1 0
1.5 SeeAppendixAfortheproof.
3 2 1 0 1 2 3
Figure 1: Depiction of anchored Lipschitz constants for 2.2.AttackCertificatesforDeepClassifiers
f(x)=tanh(x). TheanchoredLipschitzconstantatx=2
Consideringthesamesetupasbefore,wenowaimtoobtain
islessthan0.582,whereastheglobalLipschitzconstantis
thesmallestperturbationbywhichacorrectlyclassifieddata
1.
pointcanprovablybemisclassified. Thiscomputationcan
beformulatedasthefollowingoptimizationproblem,
ically,supposethelogitdifferencef iscontinuouslydif-
iy ‚Ä≤‚àó
ferentiablewithLipschitzgradientsandletLp,p‚àó
(x)bean
Œµ (x)=min Œµ
anchored Lipschitz constant of ‚àáf
iy
at x. T‚àá hf eiy n, we can s.t. min inf f yi(x+Œ¥)<0. (7)
computeanupperboundonf (x+Œ¥)asfollows,
iÃ∏=y ‚à•Œ¥‚à•p‚â§Œµ
iy
Lp,p‚àó
(x) First,wenotethatproblems(1)and(7)areequivalent.
f (x+Œ¥)‚â§f (x)+‚àáf (x)‚ä§Œ¥+ ‚àáfiy ‚à•Œ¥‚à•2. (4)
iy iy iy 2 p Proposition 2.3. Suppose f correctly classifies the data
(cid:124) (cid:123)(cid:122) (cid:125)
fiy(x,Œ¥;Lp ‚àá, fp i‚àó y(x)) vp ao li un et ox fa ps roy b, lei. me. s, (f 1iy )( ax n) d< (7)0 arf eor eqi uÃ∏= al,y i. .eT .,h Œµe ‚àón (xt )he =o Œµp ‚Ä≤t ‚àóim (xa )l
.
SeeAppendixAforaderivationofthisinequality. Incon-
trasttothezeroth-orderbound,thisupperboundusesthe Usingthecurvature-basedupperbound,onecantightenthe
localfirstderivative,‚àáf iy(x),aswellasboundsonits(an- constraintsoftheproblemandachieveafirst-order(gradient-
chored) Lipschitz constant,
Lp,p‚àó
(x), to obtain a locally informed)attackcertificateasfollows,
‚àáfiy
moreaccurateapproximationoff (x+Œ¥). Bysubstituting
iy Œµ‚àó(x)=minŒµ
theupperbound(4)in(1),weobtainafirst-order(gradient- 1
informed)lowerboundonŒµ‚àó(x), s.t.min inf f (x,Œ¥;Lp,p‚àó (x))<0, (8)
iÃ∏=y ‚à•Œ¥‚à•p‚â§Œµ
yi ‚àáfyi
Œµ‚àó(x)=max Œµ
1
s.t. sup f
(x,Œ¥;Lp,p‚àó
(x))‚â§0, ‚àÄiÃ∏=y
(5)
Weanalyticallyacquiretheoptimalvalueofthisproblemin
iy ‚àáfiy
‚à•Œ¥‚à•p‚â§Œµ thefollowingproposition.
Proposition2.4(Curvature-basedattackcertificate). Sup-
Aswesummarizebelow,wecancomputethislowerbound
inclosedform,providedthatwecancomputeLp,p‚àó
(x).
posexisclassifiedcorrectly,i.e.,f iy(x) < 0‚àÄi Ã∏= y. Let
‚àáfiy I ={i|iÃ∏=y,2Lp,p‚àó (x)f (x)‚â§‚à•‚àáf (x)‚à•2 }. Assum-
Proposition 2.1 (Curvature-based certified radius). Sup- ‚àáfyi yi yi p‚àó
ingthatI isnon-empty,theoptimizationproblem(8)has
posexisclassifiedcorrectly,i.e.,f (x)<0‚àÄiÃ∏=y. The
iy theclosed-formsolutionŒµ‚àó(x)givenby
optimizationproblem(5)hastheclosed-formsolutionŒµ‚àó(x) 1
1
g miv inen ‚àíb ‚à•y
‚àáf iy(x)‚à• p‚àó+(‚à•‚àáf iy(x)‚à•2 p‚àó‚àí2Lp ‚àá,p fi‚àó y(x)f iy(x))21
.
m i‚àài
In‚à•‚àáf yi(x)‚à• p‚àó‚àí(‚à•‚àáf
y Li
p
‚àá( ,x
p
f) y‚àó‚à• i(2
p x‚àó
)‚àí2Lp ‚àá,p fy‚àó i(x)f yi(x))21
.
iÃ∏=y Lp,p‚àó (x) (9)
‚àáfiy
(6)
Given i‚àó ‚àà I minimizing (9), the perturbation real-
izing the attack certificate is obtained through solving
SeeAppendixAfortheproofofthisproposition.
sup ‚àáf (x)‚ä§Œ¥.
Inthefollowingproposition,weshowthatifthecurvature
‚à•Œ¥‚à•p‚â§Œµ i‚àóy
of the model is sufficiently small, we can certify a larger We note that while problem (7) is always feasible (for a
radiusthanLipschitz-basedcertificates. non-trivialclassifier),problem(8)canbeinfeasibledueto
4CompositionalCurvatureBoundsforDeepNeuralNetworks
the second derivative of h, assuming it exists, which is
Unsafe Radius
Safe Region =
0 athird-ordertensor(Srinivasetal.,2022)anddifficultto
Certified Safe Region ùëì!$ characterize. OurgoalistocomputetheLipschitzconstant
Decision Boundary ofDhdirectlywithoutresortingtoanytensorcalculus.
ùúÄ%‚àó safe radius against
ùúÄ#‚àó
ùëñ-th class
ùúÄ$‚àó
ùúÄ‚àó
v
ùúÄ!‚àó
ùúÄ‚àóCertifiable
Attack
Radius
Usingthechain Dru hl (e x, )th =eJ Dac fo (b gia (xn )o )f Dh gc (xan ).bewrittenas
Thefollowingtheoremestablishesarelationbetweenthe
Lipschitz constant of Dh and the Lipschitz constants of
ùúÄ‚àó=minùúÄ%‚àó =ùúÄ%‚àó f,g,Df,andDg.
ùúÄ‚àó‚â§% ùúÄ‚àó‚â§ùúÄ‚àó ùëì !#=
0
Theorem3.1(Compositionalcurvatureestimation). Given
functionsf,g,andhasdescribedabove,thefollowingin-
ùëì!"
equalityholds,
=
0
Figure2: Certified(Œµ‚àó)andattack(Œµ‚àó)radiiestimates. The Lp D,p h‚àó ‚â§Lp D,p g‚àó Lp f‚àó +Lp D,p f‚àó Lp gLp g‚àó , (10)
greentangentcircledenotesthecertifiedradiusŒµ‚àó.
whereLp,q denotestheLipschitzconstantofthefunction
s
s(¬∑).
thetighteningoftheconstraints,whichisequivalenttothe
Theorem3.1providesabasistorecursivelycalculateaLip-
setI beingempty. Figure2illustratesanexamplescenario
schitzconstantfortheJacobianofthecompositionofmul-
forŒµ‚àóandŒµ‚àó.
tiple functions. In the following, we adapt this result to
The derivation of Œµ‚àó(x) and Œµ‚àó(x) is significant in two anchoredLipschitzconstants.
ways. First, we established an analytical solution to the
Theorem3.2(Anchoredcompositionalcurvatureestima-
curvature-based certified radius in Proposition 2.1. Al-
tion). Consider functions f,g, and h as in Theorem 3.1.
though curvature-based certificates have been studied in
ThefollowinginequalityholdsfortheanchoredLipschitz
(Singla&Feizi,2020),theirmethodinvolvesaniterative
constantoftheJacobianofhatx
algorithm to solve an optimization problem numerically,
whereasourmethodyieldsclosed-formsolutions. Second, Lp,p‚àó (x)‚â§Lp‚àó Lp,p‚àó (x)+‚à•Dg(x)‚à• Lp,p‚àó (g(x))Lp(x),
Dh f Dg p‚àó Df g
weintroducedcurvature-basedattackcertificates,anovel
method for narrowing the certification gap of classifiers. whereLp,q(x)denotestheanchoredLipschitzconstantof
s
Thecertificationgapisthe(empirical)probabilityquantify- thefunctions(¬∑)atx.
ingcorrectlyclassifiedpointsthatlackthedesiredlevelof
certifieddefenseradiiandlackattackcertificatesatagiven ThestructureofTheorem3.1(andsimilarlyTheorem3.2)
perturbation budget œµ, i.e., P {Œµ(x) ‚â§ œµ ‚â§ Œµ(x)}. isofparticularinterestforsequentialneuralnetworksthat
(x,y)‚àºD
RefertoFigure6foranillustration. arethecompositionofindividuallayers. Inthefollowing
section,wewillinstantiateourframeworkforsuchmodels.
3.EfficientEstimationofCurvatureBounds Remark 3.3. We note that in Theorems 3.1 and 3.2, the
dualnormp‚àóischosentotailortheboundsspecificallyfor
Havingestablishedtheimportanceofcurvatureinproviding Proposition2.1andProposition2.4. Ingeneral,thesame
robustness and attack certificates, in this section, we pro- statementsholdifwereplacep‚àówithageneralq ‚â•1. See
poseourmethodtoderiveupperboundsontheLipschitz AppendixAformoredetails.
constantoftheJacobian(thecurvatureconstant)ofgeneral
sequentialmodels.Wethencurateouralgorithmforresidual 3.2.CurvatureBoundsforSequentialNeuralNetworks
neuralnetworksandexplorevariousLipschitzestimation
techniquestocalculatethecurvatureconstant. Considerasequentialresidualneuralnetwork
xk+1 =hk(xk)=Hkxk+GkŒ¶(Wkxk), (11)
3.1.CurvatureBoundsforComposedFunctions
Let h = f ‚ó¶ g be the composition of two continuously where k = 0,1,¬∑¬∑¬∑K ‚àí 1 and Wk ‚àà Rn‚Ä≤ k√ónk,Gk ‚àà
differentiable functions g : Rn1 ‚Üí Rn2 and f : Rn2 ‚Üí Rnk+1√ón‚Ä≤ k,andHk ‚ààRnk+1√ónk aregeneralmatrices. For
Rn3 with Lipschitz Jacobians. The Lipschitz constant of x ‚àà Rn, Œ¶(x) = [œï(x 1),¬∑¬∑¬∑ ,œï(x n)]‚ä§, where œï is a dif-
the Jacobian Dh ‚àà Rn3√ón1 of h is an upper bound on ferentiablemonotoneactivationfunctionslope-restrictedin
5CompositionalCurvatureBoundsforDeepNeuralNetworks
p
[Œ±,Œ≤](0‚â§Œ±‚â§Œ≤ <‚àû)withitsderivativeslope-restricted 3.2.2.COMPUTATIONOFL .
k
in[Œ±‚Ä≤,Œ≤‚Ä≤](‚àí‚àû<Œ±‚Ä≤ ‚â§Œ≤‚Ä≤ <‚àû). BysettingHk =0and
Lp istheLipschitzconstantofthemapx0 (cid:55)‚Üí xk defined
Gk =I,weobtainthestandardfeedforwardarchitecture. k
bythecomposedfunction(hk‚àí1‚ó¶¬∑¬∑¬∑‚ó¶h0)(x0). Anaive
LeveragingTheorem3.1,weproposearecursivealgorithm bound on Lp is the product of the Lipschitz constant of
k
tocomputeanupperboundontheJacobianoftheend-to- individuallayers,i.e.,Lp,naive =(cid:81)k‚àí1Lp
,wherewecan
endmapx0 (cid:55)‚Üí xK. WeestablishthisalgorithminCorol- upperboundeachLp frk om(15). Hi o= w0 eveh ri ,thisboundcan
lary3.4. hi
growquicklyasthedepthincreases. Tomitigatetheadverse
Corollary 3.4. Let Lp,p‚àó , k = 0,¬∑¬∑¬∑ ,K ‚àí1 be defined effectofdepth,weexploittheideaofLipLT.Specifically,
recursivelyas
Dk
wecanunroll(11)afterapplyinglooptransformationtoall
activationlayers,resultingin
p,p‚àó p,p‚àó p p‚àó p‚àó p,p‚àó
L =L L L +L L , (12)
Dk+1 Dhk k k hk Dk xk+1=HÀÜk¬∑¬∑¬∑HÀÜ0x0+(cid:88)k
HÀÜk¬∑¬∑¬∑HÀÜj+1GjŒ®(Wjxj).
p,p‚àó p p‚àó p p‚àó
withL D0 = 0, L 0 = L 0 = 1, whereL k,L k areLips- j=0
chitzconstantsforthemapx0 (cid:55)‚Üí xk, andLp D,p h‚àó k isaLip- This representation enables us to obtain all the constants
schitz constant for the Jacobian of hk. Then Lp,p‚àó is a Lp,LT ,¬∑¬∑¬∑ ,Lp,LT recursivelyasfollows,
Dk 1 K
LipschitzconstantfortheJacobianofthemapx0 (cid:55)‚Üíxk.
Lp,LT =‚à•HÀÜk¬∑¬∑¬∑HÀÜ0‚à•
k+1 p
p p‚àó
G piv ‚àóenuppe pr ,pb ‚àóoundsontheLipschitzconstantsasL k,L k , +Œ≤‚àíŒ±(cid:88)k
‚à•HÀÜk¬∑¬∑¬∑HÀÜj+1Gj‚à• ‚à•Wj‚à• Lp,LT .
(16)
L hk,andL Dhk,Corollary3.4presentsanalgorithmtocal- 2 p p j
culateanupperboundonthecurvatureconstantofresidual j=0
neuralnetworksinalayer-by-layerfashion. Asshownin(Fazlyabetal.,2023),thisboundprovablyim-
provesthenaiveboundobtainedbytheproductofLipschitz
Next,wewillcomputetheindividualconstantsappearing
in(12).
constantsofindividuallayers,i.e.,Lp,LT ‚â§(cid:81)k‚àí1Lp,naive
.
k i=0 hi
3.2.1.COMPUTATIONOFLp
hk
3.2.3.COMPUTATIONOFLp D,p h‚àó
k.
Lp hk istheLipschitzconstantofthek-thlayerhk. Starting Considerthek-thresidualblockhk in(11). TheJacobian
from(11),ananalyticalupperboundonthisconstantis ofthisblockisgivenas
Lp,naive =‚à•Hk‚à• +Œ≤‚à•Gk‚à• ‚à•Wk‚à• . (13) Dhk(xk)=Hk+Gkdiag(Œ¶‚Ä≤(Wkxk))Wk. (17)
hk p p p
This bound is relatively crude as it does not exploit the Thefollowingpropositionprovidesanupperboundonthe
monotonicityoftheactivations,i.e.,(13)isagnostictothe Lipschitzconstantofthisdifferentialoperator.
valueofŒ±. Asproposedin(Fazlyabetal.,2023),thisbound Proposition3.5. TheJacobianDhk definedin(17)isLip-
canbeimprovedbyapplyingalooptransformationonthe p,p‚àó
schitzcontinuouswithL beinganupperboundonthe
activationlayerŒ¶. Specifically,wecanrewritehk as Dhk
Lipschitzconstant,where
hk(xk)=HÀÜkxk+GkŒ®(Wkxk), (14) Lp,p‚àó
=L ‚à•Gk‚à• ‚à•Wk‚à• ‚à•Wk‚à• ,
Dhk œï‚Ä≤ p‚àó p‚àó p‚Üí‚àû
where HÀÜk = Hk+Œ±+Œ≤GkWk and œà(z) = œï(z)‚àí(Œ±+ whereL =max{|Œ±‚Ä≤|,|Œ≤‚Ä≤|}.
2 œï‚Ä≤
Œ≤)z/2isthelooptransformedactivationlayer.Asaresultof
thistransformation,Œ®isnowslope-restrictedin Œ≤‚àíŒ±[‚àí1,1], It is worth mentioning that the upper bound in Proposi-
2
implyingthatœàis(Œ≤‚àíŒ±)-Lipschitz. Anupperboundonthe tion3.5istractableandcanbecalculatedefficiently. Inpar-
Lipschitzconstantof2 hk,reformulatedasin(14),isthen ticular,forp = 2,thematrixnorms‚à•Gk‚à• p‚àó and‚à•Wk‚à• p‚àó
canbecalculatedviathepoweriterationforfullyconnected
Lp,LT =‚à•Hk+Œ±+Œ≤ GkWk‚à• + Œ≤‚àíŒ± ‚à•Gk‚à• ‚à•Wk‚à• . andconvolutionallayers. Furthermore,‚à•Wk‚à• p‚Üí‚àûissim-
hk 2 p 2 p p plythemaximumrow‚Ñì norm,whichisstraightforwardfor
p
(15) fullyconnectedlayersandalsoconvolutionallayerswith
respecttotherepetitivestructureoftheirToeplitzmatrices.
As shown in (Fazlyab et al., 2023), this bound, now in-
SeetheproofinAppendixAformoredetails.
formedbythemonotonicityconstantŒ±,isprovablybetter
than(13). Thiscanbeprovedbyapplyingthetrianglein- For the choice p = p‚àó = 2, we propose an alternative
equalityonthefirstterm. approachtoacquirebetterLipschitzestimatesforDhk. To
6CompositionalCurvatureBoundsforDeepNeuralNetworks
thisend,weproposetorewriteDhk asastandardnetwork Algorithm1CompositionalCurvatureEstimationofNeural
block. Networks
Input: K-layerneuralnetworkintheformof(11).
Lemma3.6(VectorizedJacobian). TheJacobianmatrixin
p p‚àó p,p‚àó
(17)canberewrittenasastandardneuralnetworklayer InitializeL 0 =L 0 =1,L D0 =0.
fork =0toK‚àí1do
p‚àó
dhk(xk):=vec(Dhk(xk))=bk+AkŒ¶‚Ä≤(Wkxk), CalculateL
hk
using(15).
p,p‚àó
CalculateaboundonL usingProposition3.5.
Dhk
where dhk(xk) ‚àà RnÀÜk with nÀÜ
k
= n
k+1
√ó n k. For all UpdateLp,p‚àó =Lp,p‚àó Lp Lp‚àó +Lp‚àó Lp,p‚àó
.
Ti h‚àà en[n k A+1 ‚àà],j R‚àà nÀÜk[ √ón k n]
‚Ä≤
k,a an nd dl b‚àà k[n ‚àà‚Ä≤ k] Rle nÀÜt km ar= e( gj iv‚àí e1 n)√ó byn k b+
k
m1+ =i.
endCa fl oc
rulateD Lk p k+ +1 1andD Lh p kk ‚àó +1k usk ing(16h )k
.
Dk
H ik j, Ak
ml
=Gk ilW lk j. ReturnLp,p‚àó
,theLipschitzconstantoftheJacobianof
DK
x0 (cid:55)‚ÜíxK.
Thefollowinglemmaestablishestherelationbetweenthe
Lipschitz constant of the Jacobian matrix (Lp ) and its
Dhk
vectorizedrepresentation(Lp )whenp=2.
dhk 3.2.5.COMPARISONWITHEXISTINGAPPROACHES
Lemma3.7. Letp=2,andsupposeLp istheLipschitz
dhk UnlikethepreviousworkbySinglaetal. (Singla&Feizi,
constantofthevectorizedJacobianfunctionxk (cid:55)‚Üídhk(xk)
2020),whichislimitedtoscalar-valuedandnon-residual
defined in Lemma 3.6. Then Lp is a valid Lipschitz
dhk architectures,ourframeworkaccommodatesvector-valued
constantfortheJacobianfunctionxk (cid:55)‚ÜíDhk(xk).
generalsequentialmodels. Additionally, althoughSingla
et al. (Singla & Feizi, 2020) could theoretically handle
WecanimprovetheLipschitzboundprovidedinProposi-
convolutional neural networks by expressing such layers
tion3.5usingthepreviouslemmas.
as equivalent fully connected layers using their Toeplitz
Theorem3.8. Forp = 2, Lp = L ‚à•Ak‚à• ‚à•Wk‚à• isa matrices(Chenetal.,2020),thisapproachwouldbecom-
dhk œï‚Ä≤ 2 2
Lipcshitz constant for the Jaocbian matrix Dhk. Further- putationally prohibitive. In contrast, our method readily
more,Lp ‚â§Lp . appliestoconvolutionallayers.
dhk Dhk
Moreover,ourmethoddoesnotrequiretwicedifferentiabil-
Leveragingthevectorizedrepresentationdhk oftheJaco-
ity. ThisisparticularlyrelevantforfunctionswithLipschitz
bian Dhk, we can utilize more advanced techniques for
continuousfirstderivativesbutundefinedsecondderivatives.
LipschitzestimationsuchasLipSDPtofurtherreducethe
Forinstance,considerthewell-knownExponentialLinear
conservatism. Specifically,fornon-residualbuildingblocks,
Unit(ELU):
i.e.,whenHk =0andGk =I,wecanextractafeasibleso-
lution(optimalwhenŒ±‚Ä≤ =‚àíŒ≤‚Ä≤)totheLipSDPformulation (cid:40)
z z ‚â•0
fordhk(x). f(z)=
Œ±(ez‚àí1) z ‚â§0
Theorem 3.9. Let hk(x) = Œ¶(Wkx). Define L2,SDP =
dhk The ELU has a Lipschitz continuous first derivative, but
L ‚à•TWk‚à• , where T is a diagonal matrix with T =
œï‚Ä≤ 2 ii
its second derivative is not defined at z = 0. Therefore,
‚à•Wk‚à• . ThenL2,SDP isavalidLipschitzconstantfordhk
i,: 2 dhk Hessian-basedanalysiswouldfailforthisfunction,whereas
in‚Ñì norm.
2 ourJacobianLipschitzanalysisisapplicabletonetworks
usingthisactivationfunction.
3.2.4.SUMMARYOFALGORITHM
In the following section, we will utilize Algorithm 1 to
Itnowremainstocombineallthecomponentsdeveloped
bound the Lipschitz constant of the Jacobian and exploit
thus far to obtain upper bounds on the curvature of the
itduringthetrainingphasetocontrolthecurvatureofthe
wholenetwork. ThisissummarizedinAlgorithm1. First,
neuralnetwork.
we use LipLT to calculate the Lipschitz constants of the
p‚àó p p‚àó
individuallayers(L hk)andthesubnetworks(L k andL k ). 3.3.Curvature-ControlledNetworks
Then,usingProposition3.5,weprovideanupperboundon
Lp,p‚àó
.
Finally,wecalculateLp,p‚àó
using(12).
AsestablishedinSection2,modelswithlowcurvaturecon-
Dhk Dk+1 stantscanelicitmorerobustbehavioragainstnorm-bounded
In the algorithm, we can easily swap the use of Proposi- perturbations. Drivenbythisobservation,wecandesigna
tion3.5withanyLipschitzconstantacquiredbasedonthe curvature-basedregularizerthatwouldpromoterobustness
theoretical ground of Lemma 3.7, such as that of Theo- duringtraining. Oneapproachistorewardlargecertified
rem3.8orTheorem3.9(ifthenetworkisnon-residual). radii in the objective similar to (Fazlyab et al., 2023; Xu
7CompositionalCurvatureBoundsforDeepNeuralNetworks
Table1: Comparisonofcertifiedaccuraciesobtainedfrom
state-of-the-artmethodsSLL(Araujoetal.,2022)andCRM
CCRC
1000 CRC (Fazlyabetal.,2023)onCIFAR-10.
800 CertifiedAccuracy(Œµ)
Model Methods Accuracy Parameters
36 72 108
255 255 255
600
Standard 79.95 0 0 0 0.7M
6C2F CRM 58.57 36.25 18.36 7.37 0.7M
400 CCRC(Ours) 61.15 49.53 33.36 16.95 0.7M
SLL 57.2 45.0 35.0 26.5 1M
Lip-3C1F SLL+CCRC(Ours) 53.2 46.6 39.3 31.6 1M
200
Standard 61.89 0 0 0 4M
6F CRM 60.63 42.73 24.75 12.6 4M
0 CCRC(Ours) 62.1 52.09 40.8 29.17 4M
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Certified Radius
Figure3: Certifiedradiuscomparisonona6-layerneural
etal.,1998). Next,tofurthermotivatetheuseofanchored
network.
Lipschitz constant estimation, we train several networks
withdifferentdepthstoportrayitseffectivenessonbothLip-
etal.,2022),givingrisetothetraininglossfunction schitzconstantestimationandJacobianLipschitzconstant
estimation. Finally, we compare our robustness certifica-
L(x,y;f)=L (f(x),u )+Œª1 g(Œµ‚àó(x)),
CE y {C(x)=y} 1 tion method with the state-of-the-art classification on the
whereL isthecross-entropyloss,u istheone-hotvec- CIFAR(Krizhevskyetal.,2009)datasetandprovideattack
CE y
tor corresponding to y, Œª > 0 is the regularization con- certificatesonthesamenetworks.
stant, and g: R ‚Üí R is a convex decreasing function
+
(e.g.,g(z)=exp(‚àíz)). Theroleoftheindicatorfunction ComparisonwithotherCurvature-basedMethods In
1 {C(x)=y}istorestricttheregularizertocorrectlyclassified thisexperiment,wecompareourproposedcompositional
points only. This regularizer is differentiable due to the curvaturecalculationmethodwiththepreviousworks. Con-
closed-formexpressionforŒµ‚àó(x)givenin(6). Nonetheless, sequently, we train a 6-layer fully connected network on
1
computingitforeachdatapointinthetrainingdatasetcan MNISTwithcurvatureregularizationandcomputethecer-
becomputationallycostlyforlarge-scaleinstances. Amore tified radii of the test data points for this network using
efficientapproachistoregularizetheboundontheglobal two curvature calculation algorithms. We denote (Singla
curvatureofthenetworkduringtraining, &Feizi,2020)asCurvature-basedRobustnessCertificate
(CRC),andourmethodasCompositionalCurvature-based
p,p‚àó
L(x,y;f)=L CE(f(x),u y)+ŒªL Df , RobustnessCertificate(CCRC).Next,tofocustheexperi-
mentoncomparingthecurvaturebounds,weusethemethod
To further improve the efficiency, we propose to use 1-
of(Singla&Feizi,2020)toobtainthecertifiedradiusfor
Lipschitzlayerstobuildthearchitecture. Specifically,when
p = 2, if hk in (11) is modified to be a 1-Lipschitz func- eachpoint.
p
tion (L hk = 1), the concatenation of all layers will be Figure3comparesthecertifiedradiiofthesemethodsand
p
1-Lipschitz (L = 1), and thus (12) yields the curvature confirms the superior performance of the compositional
k
bound
Lp
=
(cid:80)L‚àí1Lp
, which can be readily com- curvaturecalculationalgorithm.
Df k=0 Dhk
puted.
Anchored Lipschitz/Curvature Estimation Next, we
4.Experiments studytheimpactoflocalizingthecomputationsviathecon-
ceptofanchoredLipschitzconstantintroducedinthispaper.
In this section, we evaluate the performance of our pro-
Toachievethis,wetrainfullyconnectedneuralnetworksof
posed methods via a series of experiments. We contrast
varyingdepthsandcalculateupperboundsontheLipschitz
ourmethodsagainstthestate-of-the-artLipschitzconstant
andcurvatureconstantsofthenetwork,bothgloballyand
estimation,curvatureestimation,andneuralnetworkrobust-
inananchoredmanner. Figure4illustratestheresultson
ness certification algorithms. In our experiments we set
theMNISTdataset. Fortheanchoredbounds,weaverage
p = 2forallnormsandLipschitzcalculations. Wedefer
thevaluesoverthetestdataset. Theresultsdemonstratethat
thediscussionofhyperparameterstoAppendixD.1. Our
usingtheanchoredcounterpartssignificantlyimprovesthe
codeisavailableathttps://github.com/o4lc/Compositional-
bounds.
Curvature-Bounds-for-DNNs.
We first showcase the application and superiority of our AttackCertificationonCIFAR-10 Inthisexperiment,
JacobianLipschitzconstantestimationonMNIST(LeCun weprovideradiiforprovableattacksona6Fmodel. Fig-
8CompositionalCurvatureBoundsforDeepNeuralNetworks
1000
13 Global Lipschitz Global Curvature
Anchored Lipschitz 900 Anchored Curvature
12 800
11 700
600
10
500
9 400
8 300
3.0 3.5 4.0 4.5 5.0 5.5 6.0 3.0 3.5 4.0 4.5 5.0 5.5 6.0
Number of Layers Number of Layers
Figure 4: Comparison of global Lipschitz and Curvature
estimationagainsttheiranchoredcounterparts. Theshaded
areasdenotethestandarddeviationoverthewholedataset.
Figure 6: Illustration of the certificates provided by our
method. A andA arethecleanandverifiedaccuracies,
c v
respectively. ThecertificationradiusisŒµ= 36 .
255
100
80 with different architectures on CIFAR-10 with the state-
of-the-art. Wetraintwo6C2Fand6Fnon-residualanda
60
1-LipschitzneuralnetworkwithLip-3C1Farchitectureon
40 theCIFAR-10datasetwiththefollowinglossfunction
20
L(x,y;f)=LœÑ,ŒΩ(f(x),y)+ŒªLp
, (18)
0 CE Df
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Attack Radius
whereLœÑ,ŒΩ isamodifiedvariantofthecrossentropyloss
Figure 5: Histogram of the certified attack radii for a 6- CE
p
function(Prach&Lampert,2022)andL isthecurvature
layerneuralnetworktrainedviacurvatureregularizationon Df
boundacquiredthroughAlgorithm1.RefertoAppendixD.1
CIFAR-10.
formoreonthelossfunctiondetails.Table1showsthecom-
parisonbetweenthesemodels. Wefindthatincorporating
theadditionalregularizationtermleadstohighercertifiedac-
ure 5 shows the results. This model has an accuracy of
56.19%. Furthermore, with the perturbation budget 36 curacies,smallercertificationgaps,andoften,higherclean
255
themodelhascertifiedandPGDaccuracyof47.16%and accuracies.
48.46%,respectively. Byanalyzingtheattackcertificates
we find that our method is able to provide an attack cer- 5.Conclusion
tificate for a total of 808 samples, of which 645 require
aperturbationbudgetofatmost 36 . Usingthisinforma- Inthiswork,weproposedanovelmethodtocalculateprov-
255
ableupperboundsonthecurvatureconstantofsmoothdeep
tion,therobustaccuracyofthemodelwiththisperturbation
budgetisatmost56.19‚àí 645 √ó100=49.74%. Thisis neural networks, i.e., the Lipschitz constant of their first
10000
illustratedinFigure6,whereA isthecleanaccuracy,A‚àóis derivative. Ourmethodleveragesthecompositionalstruc-
c v
theverifiedaccuracy,andA andA ,arelowerandupper tureofthemodeltocomputethecurvatureboundinascal-
v v
ableandmodularfashion. Thegeneralityofourcurvature
boundsontheverifiedaccuracy,respectively.
estimationalgorithmcanenableitsuseforcompositional
Thishastwomainimplications. First,havingattackcertifi- functions beyond neural networks. Furthermore, we pro-
catesforanydataeliminatestheneedtoperformanattack videdanalyticalrobustnesscertificatesfordeepclassifiers
onthatdataastheexistenceofanattackwasverifiedbyour basedonthecurvatureofthemodel. Inthefuture,weaim
proposition. Second,theattackcertificatesfurthernarrow tofurthertightentheestimatedgapofthecurvaturebound,
downtheuncertaintyofthemodelaccuracy. Asthecertified enabling the algorithm to produce tighter bounds on the
accuracyisalowerboundontheactualcertifiedrobustness curvatureofevendeeperneuralnetworks.
ofthemodel,weconcludethattheactualcertifiedaccuracy
of this model is in the range [47.16,49.74], regardless of
ImpactStatement
thecertificationmethod.
Thispaperpresentsworkwhosegoalistoadvancethefield
RobustnessCertificationonCIFAR-10 Thefinalexper- ofMachineLearning. Wedonotforeseeanysocietalimpli-
iment aims to compare the certified accuracy of models cationsarisingsolelyfromourwork.
9CompositionalCurvatureBoundsforDeepNeuralNetworks
References LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P. Gradient-
basedlearningappliedtodocumentrecognition. Proceed-
Araujo,A.,Havens,A.J.,Delattre,B.,Allauzen,A.,and
ingsoftheIEEE,86(11):2278‚Äì2324,1998.
Hu,B. Aunifiedalgebraicperspectiveonlipschitzneural
networks. InTheEleventhInternationalConferenceon Leino,K.,Wang,Z.,andFredrikson,M. Globally-robust
LearningRepresentations,2022. neuralnetworks.InInternationalConferenceonMachine
Learning,pp.6212‚Äì6222.PMLR,2021.
Be¬¥thune, L., Boissin, T., Serrurier, M., Mamalet, F.,
Friedrich, C., and Gonzalez Sanz, A. Pay attention to Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
yourloss: understandingmisconceptionsaboutlipschitz Vladu, A. Towards deep learning models resistant to
neuralnetworks. AdvancesinNeuralInformationPro- adversarialattacks.InInternationalConferenceonLearn-
cessingSystems,35:20077‚Äì20091,2022. ingRepresentations,2018.
Chen,Y.,Xie,Y.,Song,L.,Chen,F.,andTang,T. Asur- Moosavi-Dezfooli, S.-M., Fawzi, A., Uesato, J., and
veyofacceleratorarchitecturesfordeepneuralnetworks. Frossard,P. Robustnessviacurvatureregularization,and
Engineering,6(3):264‚Äì274,2020. ISSN2095-8099. doi: viceversa. InProceedingsoftheIEEE/CVFConference
https://doi.org/10.1016/j.eng.2020.01.007. onComputerVisionandPatternRecognition,pp.9078‚Äì
9086,2019.
Cohen,J.,Rosenfeld,E.,andKolter,Z.Certifiedadversarial
robustnessviarandomizedsmoothing. Ininternational Prach,B.andLampert,C.H. Almost-orthogonallayersfor
conferenceonmachinelearning,pp.1310‚Äì1320.PMLR, efficientgeneral-purposelipschitznetworks. InEuropean
2019. ConferenceonComputerVision,pp.350‚Äì365.Springer,
2022.
Dutta, S., Jha, S., Sankaranarayanan, S., and Tiwari, A.
Outputrangeanalysisfordeepfeedforwardneuralnet- Robey,A.,Hu,H.,Lindemann,L.,Zhang,H.,Dimarogonas,
works. InNASAFormalMethodsSymposium,pp.121‚Äì D. V., Tu, S., and Matni, N. Learning control barrier
138.Springer,2018.
functionsfromexpertdemonstrations.In202059thIEEE
ConferenceonDecisionandControl(CDC),pp.3717‚Äì
Fazlyab,M.,Robey,A.,Hassani,H.,Morari,M.,andPap- 3724.IEEE,2020.
pas, G. Efficient and accurate estimation of lipschitz
Singla, S. and Feizi, S. Second-order provable defenses
constantsfordeepneuralnetworks. AdvancesinNeural
againstadversarialattacks. InInternationalconference
InformationProcessingSystems,32,2019.
onmachinelearning,pp.8981‚Äì8991.PMLR,2020.
Fazlyab,M.,Entesari,T.,Roy,A.,andChellappa,R. Cer-
Singla,S.andFeizi,S. Skeworthogonalconvolutions. In
tifiedrobustnessviadynamicmarginmaximizationand
InternationalConferenceonMachineLearning,pp.9756‚Äì
improvedlipschitzregularization,2023.
9766.PMLR,2021.
Fischetti,M.andJo,J. Deepneuralnetworksandmixed
Singla, S. and Feizi, S. Improved techniques for deter-
integerlinearoptimization. Constraints,23(3):296‚Äì309,
ministicl2robustness. AdvancesinNeuralInformation
2018.
ProcessingSystems,35:16110‚Äì16124,2022.
Hashemi, N., Ruths, J., and Fazlyab, M. Certifying in-
Singla,V.,Singla,S.,Feizi,S.,andJacobs,D. Lowcurva-
crementalquadraticconstraintsforneuralnetworksvia
tureactivationsreduceoverfittinginadversarialtraining.
convexoptimization. InLearningforDynamicsandCon-
InProceedingsoftheIEEE/CVFInternationalConfer-
trol,pp.842‚Äì853.PMLR,2021.
enceonComputerVision,pp.16423‚Äì16433,2021.
Huang, Y., Zhang, H., Shi, Y., Kolter, J. Z., and Anand- Srinivas, S., Matoba, K., Lakkaraju, H., and Fleuret, F.
kumar, A. Training certifiably robust neural networks Efficienttrainingoflow-curvatureneuralnetworks. Ad-
withefficientlocallipschitzbounds. AdvancesinNeural vances in Neural Information Processing Systems, 35:
InformationProcessingSystems,34:22745‚Äì22757,2021. 25951‚Äì25964,2022.
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,
offeaturesfromtinyimages. 2009. D.,Goodfellow,I.,andFergus,R.Intriguingpropertiesof
neuralnetworks. arXivpreprintarXiv:1312.6199,2013.
Kumar,A.,Levine,A.,andFeizi,S. Policysmoothingfor
provablyrobustreinforcementlearning. InInternational Tjeng,V.,Xiao,K.Y.,andTedrake,R. Evaluatingrobust-
ConferenceonLearningRepresentations,2021. nessofneuralnetworkswithmixedintegerprogramming.
10CompositionalCurvatureBoundsforDeepNeuralNetworks
InInternationalConferenceonLearningRepresentations,
2018.
Tsuzuku,Y.,Sato,I.,andSugiyama,M. Lipschitz-margin
training: Scalablecertificationofperturbationinvariance
fordeepneuralnetworks.Advancesinneuralinformation
processingsystems,31,2018.
Wang, R. and Manchester, I. Direct parameterization of
lipschitz-boundeddeepnetworks. InInternationalCon-
ferenceonMachineLearning,pp.36093‚Äì36110.PMLR,
2023.
Xu,Y.,Sun,Y.,Goldblum,M.,Goldstein,T.,andHuang,
F. Exploringandexploitingdecisionboundarydynamics
foradversarialrobustness. InTheEleventhInternational
ConferenceonLearningRepresentations,2022.
Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and
Jordan, M. Theoretically principled trade-off between
robustnessandaccuracy. InInternationalconferenceon
machinelearning,pp.7472‚Äì7482.PMLR,2019.
Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Uni-
versalandtransferableadversarialattacksonalignedlan-
guagemodels. arXivpreprintarXiv:2307.15043,2023.
11CompositionalCurvatureBoundsforDeepNeuralNetworks
A.TheoremsandProofs
ProofofLemma1.2
Proof. Foranydirectiond ‚àà Rn, letg (t) = f(x+td). Evidently, wehaveg‚Ä≤(t) = dg (t) = Df(x+td)d, where
d d dt d
Df(x)‚ààRm√ón. Moreover,wehave
g (t)‚àíg (0) ‚à•f(x+td)‚àíf(x)‚à•
‚à•Df(x)d‚à• =‚à•g‚Ä≤(0)‚à• =‚à•lim d d ‚à• = lim p ‚â§Lp(x)‚à•d‚à• ,
p d p t‚Üí0 t‚àí0 p t‚Üí0 t f p
wherethethirdequalityfollowsfromcontinuityof‚à•¬∑‚à• . Consequently,
p
‚à•Df(x)‚à• = sup ‚à•Df(x)d‚à• = sup ‚à•g‚Ä≤(0)‚à• ‚â§Lp(x).
p p d p f
‚à•d‚à•p‚â§1 ‚à•d‚à•p‚â§1
Corollary A.1. Consider a differentiable function f : Rn ‚Üí R and let Lp(x) be a corresponding anchored Lipschitz
f
constantinsomep-norm. Wehave
‚à•‚àáf(x)‚à• ‚â§Lp(x).
p‚àó f
Proof. TheprooffollowsfromthefactthatDf(x)=‚àáf(x)‚ä§andthatformatrixAandnorm‚à•¬∑‚à• ,wehave‚à•A‚ä§‚à• =
p p
‚à•A‚à• .
p‚àó
Derivationofequation(4)
Proof. ToprovethequadraticupperboundonthelogitgapwiththeanchoredLipschitzconstantweutilizethemeanvalue
theorem. ForanyxandŒ¥,wecanwrite
(cid:90) 1
f (x+Œ¥)=f (x)+‚àáf (x)‚ä§Œ¥+ (‚àáf (x+tŒ¥)‚àí‚àáf (x))‚ä§Œ¥dt.
iy iy iy iy iy
0
Wecanthenwrite
(cid:90) 1
|f (x+Œ¥)‚àíf (x)‚àí‚àáf (x)‚ä§Œ¥|‚â§ |(‚àáf (x+tŒ¥)‚àí‚àáf (x))‚ä§Œ¥|dt
iy iy iy iy iy
0
(cid:90) 1
H¬®older‚ÄôsInequality
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí‚â§ ‚à•(‚àáf (x+tŒ¥)‚àí‚àáf (x)‚à• ‚à•Œ¥‚à• dt
iy iy p‚àó p
p1 ‚àó+ p1=1 0
(cid:90) 1
‚â§ Lp,p‚àó (x)‚à•Œ¥‚à•2tdt
‚àáfiy p
0
Lp,p‚àó
(x)
= ‚àáfiy ‚à•Œ¥‚à•2.
2 p
Thus,wehave
Lp,p‚àó
(x)
f (x+Œ¥)‚â§f (x)+‚àáf (x)‚ä§Œ¥+ ‚àáfiy ‚à•Œ¥‚à•2.
iy iy iy 2 p
ProofofProposition2.1
Proof. Define‚àÜ = {Œ¥|f (x+Œ¥) ‚â§ 0}and‚àÜ = {Œ¥|f
(x,Œ¥;Lp,p‚àó
(x)) ‚â§ 0}. Itisclearthat‚àÜ ‚äÜ ‚àÜ. Consequently,
iy iy ‚àáfiy
{Œµ | sup f
(x,Œ¥;Lp,p‚àó
(x)) ‚â§ 0} ‚äÜ {Œµ | sup f (x+Œ¥) ‚â§ 0},andthus,(5)yieldsavalidlowerboundon
‚à•Œ¥‚à•p‚â§Œµ iy ‚àáfiy ‚à•Œ¥‚à•p‚â§Œµ iy
theoriginalradius(1).
12CompositionalCurvatureBoundsforDeepNeuralNetworks
Now,toacquiretheanalyticalresultwehave
Lp,p‚àó
(x)
Lp,p‚àó
(x)
sup f (x)+‚àáf (x)‚ä§Œ¥+ ‚àáfiy ‚à•Œ¥‚à•2 =f (x)+Œµ‚à•‚àáf (x)‚à• + ‚àáfiy Œµ2.
iy iy 2 p iy iy p‚àó 2
‚à•Œ¥‚à•p‚â§Œµ
ThisequalityholdsduetothefactthatfortheŒ¥‚àóachievingsup ‚àáf (x)‚ä§Œ¥wehave‚à•Œ¥‚àó‚à• =Œµ,implyingthatŒ¥‚àóis
‚à•Œ¥‚à•p‚â§Œµ iy p
Lp,p‚àó
(x)
alsoamaximizeroftheotherterm ‚àáfiy ‚à•Œ¥‚à•2.
2 p
Asaresult,weobtainthefollowingoptimizationproblemthatisequivalentto(5)
max Œµ
Lp,p‚àó
(x)
s.t. ‚àáfiy Œµ2+‚à•‚àáf (x)‚à• Œµ+f (x)‚â§0,‚àÄiÃ∏=y.
2 iy p‚àó iy
Usingelementarycalculations,weobtaintheoptimalsolution
min‚àí‚à•‚àáf iy(x)‚à• p‚àó+(‚à•‚àáf iy(x)‚à•2 p‚àó‚àí2Lp ‚àá,p fi‚àó y(x)f iy(x))21
.
iÃ∏=y Lp,p‚àó (x)
‚àáfiy
ProofofProposition2.2
Proof. Supposexiscorrectlyclassified,i.e.,f (x)<0foriÃ∏=y. ToenforcetheconditionŒµ‚àó(x)‚â§Œµ‚àó(x)wemusthave
iy 0 1
Œµ‚àó(x)‚â§
‚àí‚à•‚àáf iy(x)‚à• p‚àó+(‚à•‚àáf iy(x)‚à•2 p‚àó‚àí2Lp ‚àá,p fi‚àó y(x)f iy(x))21
, ‚àÄiÃ∏=y.
0 Lp,p‚àó (x)
‚àáfiy
Orequivalently,
Lp,p‚àó (x)Œµ‚àó(x)2+2‚à•‚àáf (x)‚à• Œµ‚àó(x)+2f (x)‚â§0.
‚àáfiy 0 iy p‚àó 0 iy
Theaboveinequalityholdsifandonlyif
Lp,p‚àó
(x)‚â§
‚àí2(‚à•‚àáf iy(x)‚à• p‚àóŒµ‚àó 0(x)+f iy(x))
. (19)
‚àáfiy Œµ‚àó(x)2
0
UtilizingCorollaryA.1,wenotethatŒµ‚àó(x)=min ‚àífiy(x) ‚â§ ‚àífiy(x) ‚â§ ‚àífiy(x) . ThisensuresthattheR.H.S.of
0 iÃ∏=y Lp fiy(x) Lp fiy(x) ‚à•‚àáfiy(x)‚à•p‚àó
(19)ispositive.
ProofofProposition2.3
Proof. Weprovethisintwosteps:
1. Œµ‚àó(x) ‚â§
Œµ‚Ä≤‚àó
(x): Supposethisdoesnothold. ThenŒµ‚àó(x) >
Œµ‚Ä≤‚àó
(x). However,havingasolutionfor(7)impliesthat
thereexistsapairŒ¥andj with‚à•Œ¥‚à•
‚â§Œµ‚Ä≤‚àó
(x)<Œµ‚àó(x)suchthatf (x+Œ¥)<0. Thisperturbation-indexpairisthen
p yj
aviolationfortheconstraintofproblem(1).
Thus,wemusthaveŒµ‚àó(x)‚â§Œµ‚Ä≤‚àó
(x).
2.
Œµ‚àó(x)‚â•Œµ‚Ä≤‚àó
(x):Similarly,ifthisdoesnothold,then‚àÄiÃ∏=y,Œ¥,‚à•Œ¥‚à•
‚â§Œµ‚àó(x)<Œµ‚Ä≤‚àó
(x)wehavesup f (x+
p ‚à•Œ¥‚à•p‚â§Œµ‚àó(x) iy
Œ¥)‚â§0. Buthavingasolutionfor(7)assertsthatthereexistsonesuchindexthatf (x+Œ¥)<0.
Thus,Œµ‚àó(x)‚â•Œµ‚Ä≤‚àó
(x)
yi
musthold.
Asaresult,wemusthaveŒµ‚àó(x)=Œµ‚Ä≤‚àó
(x).
13CompositionalCurvatureBoundsforDeepNeuralNetworks
ProofofProposition2.4
Proof. Toprovethisproposition,wefirstfindtheanalyticalsolutiontotheinneroptimizationproblems,namely
Lp,p‚àó
(x)
inf f (x,Œ¥;Lp,p‚àó (x))= inf inf f (x)+‚àáf (x)‚ä§Œ¥+ ‚àáfyi ‚à•Œ¥‚à•2. (20)
‚à•Œ¥‚à•p‚â§Œµ yi ‚àáfiy 0‚â§Œª‚â§1‚à•Œ¥‚à•p=ŒªŒµ yi yi 2 p
Let Œ¥‚àó = argmin ‚àáf (x)‚ä§Œ¥, which yields ‚àáf (x)‚ä§Œ¥‚àó = ‚àí‚à•‚àáf (x)‚à• . The inner optimization problem is
‚à•Œ¥‚à•p‚â§1 yi yi yi p‚àó
minimizedatŒ¥ =ŒªŒµŒ¥‚àó. Consequently,wecanframeproblem(20)equivalentlyas
Lp,p‚àó
(x)
g‚àó(Œµ)=g (Œµ,Œª‚àó)= min f (x)‚àíŒµ‚à•‚àáf (x)‚à• Œª+ ‚àáfyi Œµ2Œª2. (21)
i i 0‚â§Œª‚â§1 yi yi p‚àó 2
(cid:124) (cid:123)(cid:122) (cid:125)
gi(Œµ,Œª)
Therearethreepossiblesolutions
1. ŒªÀÜ =0. Inthisscenario
g (Œµ,ŒªÀÜ)=f (x).
i yi
‚à•‚àáf (x)‚à•
2. ŒªÀÜ = yi p‚àó . Inthisscenario
Lp,p‚àó (x)Œµ
‚àáfyi
‚à•‚àáf (x)‚à•2
g (Œµ,ŒªÀÜ)=f (x)‚àí yi p‚àó .
i yi 2Lp,p‚àó (x)
‚àáfyi
‚à•‚àáf (x)‚à•
ForthissolutionwemusthaveŒªÀÜ ‚â§1. Thisimposesthecondition yi p‚àó ‚â§Œµ.
Lp,p‚àó (x)
‚àáfyi
3. ŒªÀÜ =1. Inthisscenariowehave
Lp,p‚àó
(x)
g (Œµ,ŒªÀÜ)=f (x)‚àíŒµ‚à•‚àáf (x)‚à• + ‚àáfyi Œµ2.
i yi yi p‚àó 2
Puttingtogetherthedifferentconditions,wefindthat
Ô£± Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤f yi(x)‚àí ‚à• 2‚àá Lf py ,i p( ‚àóx () x‚à•2 p )‚àó ,‚à•‚àá Lf py ,pi ‚àó(x () x‚à• )p‚àó ‚â§Œµ
g i‚àó(Œµ)= Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥f yi(x)‚àíŒµ‚à•‚àá‚àá f yf iy (i x)‚à•
p‚àó
+ Lp ‚àá,p f 2y‚àó i(x) Œµ2 ,‚à•‚àá Lf‚àá py ,pf iy ‚àó(i x () x‚à• )p‚àó >Œµ
‚àáfyi
Thenextstepissolving
min Œµ
s.t. ming‚àó(Œµ)<0.
i
iÃ∏=y
ForeachiÃ∏=y,if ‚à•‚àáf yi(x)‚à• p‚àó ‚â§Œµtheng‚àó(Œµ)‚â§0requires2Lp,p‚àó (x)f (x)‚â§‚à•‚àáf (x)‚à•2 ,andif ‚à•‚àáf yi(x)‚à• p‚àó >Œµ
Lp,p‚àó (x) i ‚àáfyi yi yi p‚àó Lp,p‚àó (x)
‚àáfyi ‚àáfyi
thesmallestvalueofŒµyieldingg‚àó(Œµ)‚â§0is
i
(cid:113)
‚à•‚àáf (x)‚à• ‚àí ‚à•‚àáf (x)‚à•2 ‚àí2Lp,p‚àó (x)f (x)
yi p‚àó yi p‚àó ‚àáfyi yi
, (22)
Lp,p‚àó (x)
‚àáfyi
14CompositionalCurvatureBoundsforDeepNeuralNetworks
which similarly requires the condition 2Lp,p‚àó (x)f (x) ‚â§ ‚à•‚àáf (x)‚à•2 for realizability. Consequently, if
‚àáfyi yi yi p‚àó
2Lp,p‚àó (x)f (x) ‚â§ ‚à•‚àáf (x)‚à•2 holds for some i Ã∏= y, the smallest valid Œµ is given as in (22). Thus, we conclude
‚àáfyi yi yi p‚àó
that
(cid:113)
‚à•‚àáf (x)‚à• ‚àí ‚à•‚àáf (x)‚à•2 ‚àí2Lp,p‚àó (x)f (x)
Œµ‚àó(x)=min
yi p‚àó yi p‚àó ‚àáfyi yi
, (23)
1 i‚ààI Lp,p‚àó (x)
‚àáfyi
whereI ={i|iÃ∏=y,2Lp,p‚àó (x)f (x)‚â§‚à•‚àáf (x)‚à•2 }. IfI =‚àÖ,theproblemisinfeasible.
‚àáfyi yi yi p‚àó
ProofofTheorem3.1
Proof. WritingthedefinitionofLipschitzcontinuity,wehave
‚à•Dh(x)‚àíDh(y)‚à• =‚à•Df(g(x))Dg(x)‚àíDf(g(y))Dg(y)‚à•
q q
=‚à•Df(g(x))Dg(x)‚àíDf(g(x))Dg(y)+Df(g(x))Dg(y)‚àíDf(g(y))Dg(y)‚à•
q
‚â§‚à•Df(g(x))Dg(x)‚àíDf(g(x))Dg(y)‚à• +‚à•Df(g(x))Dg(y)‚àíDf(g(y))Dg(y)‚à•
q q
‚â§‚à•Df(g(x))‚à• ‚à•Dg(x)‚àíDg(y)‚à• +‚à•Df(g(x))‚àíDf(g(y))‚à• ‚à•Dg(y)‚à•
q q q q
‚â§Lp,q‚à•Df(g(x))‚à• ‚à•x‚àíy‚à• +Lq‚Ä≤,q‚à•g(x)‚àíg(y)‚à• ‚à•Dg(y)‚à•
Dg q p Df q‚Ä≤ q
‚â§Lp,q‚à•Df(g(x))‚à• ‚à•x‚àíy‚à• +Lq‚Ä≤,qLp,q‚Ä≤ ‚à•Dg(y)‚à• ‚à•x‚àíy‚à• .
Dg q p Df g q p
BasedonLemma1.2,wenotethatLq isanupperboundon‚à•Df(¬∑)‚à• andthatLq isanupperboundon‚à•Dg(¬∑)‚à• . Thus,
f q g q
wearriveat
‚à•Dh(x)‚àíDh(y)‚à• ‚â§(cid:0) Lp,qLq +Lq‚Ä≤,qLp,q‚Ä≤ Lq(cid:1) ‚à•x‚àíy‚à• .
q Dg f Df g g p
Finally,bysettingq =p‚àóandq‚Ä≤ =p,weobtainLp,p‚àó ‚â§Lp,p‚àó Lp‚àó +Lp,p‚àó LpLp‚àó.
Dh Dg f Df g g
ProofofTheorem3.2
Proof. TakingthesamestepsastheproofofTheorem3.1,wehave
‚à•Dh(x+Œ¥)‚àíDh(x)‚à• ‚â§‚à•Df(g(x+Œ¥))‚à• ‚à•Dg(x+Œ¥)‚àíDg(x)‚à• +‚à•Df(g(x+Œ¥))‚àíDf(g(x))‚à• ‚à•Dg(x)‚à•
q q q q q
‚â§Lp,q(x)‚à•Df(g(x+Œ¥))‚à• ‚à•Œ¥‚à• +Lq‚Ä≤,q(g(x))Lp,q‚Ä≤ (x)‚à•Dg(x)‚à• ‚à•Œ¥‚à•
Dg q p Df g q p
‚â§(cid:0) LqLp,q(x)+‚à•Dg(x)‚à• Lq‚Ä≤,q(g(x))Lp,q‚Ä≤ (x)(cid:1) ‚à•Œ¥‚à• .
f Dg q Df g p
Settingq =p‚àóandq‚Ä≤ =pyieldstheresult.
ProofofProposition3.5
Proof. Wedropthesuperscriptkforsimplicityhere. WritingoutthedefinitionofLipschitzcontinuitywehave
‚à•Dh(x)‚àíDh(y)‚à• =‚à•Gdiag(Œ¶‚Ä≤(Wx))W ‚àíGdiag(Œ¶‚Ä≤(Wy))W‚à•
q q
‚â§‚à•G‚à• ‚à•W‚à• ‚à•diag(Œ¶‚Ä≤(Wx))‚àídiag(Œ¶‚Ä≤(Wy))‚à•
q q q
=‚à•G‚à• ‚à•W‚à• max|Œ¶‚Ä≤(Wx) ‚àíŒ¶‚Ä≤(Wy) | (24)
q q i i
i
‚â§L ‚à•G‚à• ‚à•W‚à• max|W (x‚àíy)|,
œï‚Ä≤ q q i,:
i
whereL =max{|Œ±‚Ä≤|,|Œ≤‚Ä≤|}istheLipschitzconstantofœï‚Ä≤. Next,wecanwrite
œï‚Ä≤
max|W (x‚àíy)|=‚à•W(x‚àíy)‚à• ‚â§‚à•W‚à• ‚à•x‚àíy‚à• ,
i,: ‚àû p‚Üí‚àû p
i
Settingq =p‚àóyieldsthedesiredresult.
15CompositionalCurvatureBoundsforDeepNeuralNetworks
ProofofLemma3.6
Proof. Toperformtheconversiontoastandardlayer,weconsiderindividualentriesoftheoutput:
n‚Ä≤
(cid:88)k
Dhk(x) =Hk + GkWkŒ¶‚Ä≤(Wkx) .
ij ij il lj l
l=1
Thus,byflatteningthematrixDhk(x)intoavectordhk(x),wheretheij-thelementofDhk(x)ismappedtothe(cid:0)
(j‚àí
1)√ón +i(cid:1) -thelementofdhk(x),‚àÄi ‚àà [n ],j ‚àà [n ],weobtainthedesiredresult. Thevectorbk andmatrixAk
k+1 k+1 k
definedinthelemmayieldthecorrectmap. Importantly,wehavetheidentityAk =GÀÜkWÀúk,where
Ô£Æ Gk 0 ¬∑¬∑¬∑ 0 Ô£π Ô£Æ diag(Wk )Ô£π
:,1
GÀÜk
=Ô£Ø
Ô£Ø Ô£Ø Ô£∞
0
. . .
G
. .
.k ¬∑ .¬∑ .¬∑
.
0
. . .
Ô£∫
Ô£∫ Ô£∫ Ô£ª, WÀúk
=Ô£Ø
Ô£Ø Ô£Ø
Ô£∞diag(
. .
.W :k ,2)Ô£∫
Ô£∫ Ô£∫ Ô£ª.
0 0 ¬∑¬∑¬∑ Gk diag(Wk )
:,nk
Evidently,wehaveAk =(cid:0) Gkdiag(Wk )(cid:1) =GkWk.
ml :,j il il lj
ProofofLemma3.7
Proof. Byvectorization,wehave‚à•dhk(x)‚àídhk(y)‚à• =‚à•Dhk(x)‚àíDhk(y)‚à• ,where‚à•¬∑‚à• istheFrobeniusnorm. We
2 F F
canwrite
‚à•Dhk(x)‚àíDhk(y)‚à• ‚â§‚à•Dhk(x)‚àíDhk(y)‚à• =‚à•dhk(x)‚àídhk(y)‚à• ‚â§Lp ‚à•x‚àíy‚à• ,
2 F 2 dhk 2
wherewehaveusedthefactthatforagivenmatrixA,‚à•A‚à• =œÉ
(A)‚â§(cid:112)(cid:80)
œÉ2(A)=‚à•A‚à• .
2 max i i F
ProofofTheorem3.8
Proof. UsingidentityAk =GÀÜkWÀúk fromtheproofofLemma3.6,wehave
‚à•Ak‚à• ‚â§‚à•GÀÜk‚à• ‚à•WÀúk‚à• =‚à•Gk‚à• ‚à•WÀúk‚à• .
2 2 2 2 2
For‚à•WÀúk‚à• wehave(WÀúk‚ä§WÀúk) =(cid:80) WÀúkWÀúk. WithrespecttothesparsitypatternofthematrixWÀúk,WÀúk‚ä§WÀúk isonly
2 ij l li lj
non-zeroonitsdiagonalwith(WÀúk‚ä§WÀúk) = ‚à•W ‚à•2. Thus‚à•WÀúk‚à• = max ‚à•W ‚à• = ‚à•W‚à• . Thisconcludesthe
ii i,: 2 2 i i,: 2 2‚Üí‚àû
proof.
ProofofTheorem3.9
Proof. Using Lemma 3.6 to vectorize the Jacobian of the layer hk(x) = Œ¶(Wkx), we obtain Ak = WÀúk (see proof of
Lemma3.6). AsstatedintheproofofTheorem3.8,Ak‚ä§Ak isdiagonalwith(Ak‚ä§Ak) = ‚à•W ‚à• . Next,westatethe
ii i,: 2
semidefiniteprogramof(Fazlyabetal.,2019)forcalculatingtheLipschitzconstantofdhk =Akœï‚Ä≤(Wx). Define
(cid:34) (cid:35)
‚àíŒ±‚Ä≤Œ≤‚Ä≤Wk‚ä§TWk‚àíœÅI (Œ±‚Ä≤+Œ≤‚Ä≤)Wk‚ä§T
M(œÅ,T)= 2 ,
(Œ±‚Ä≤+Œ≤‚Ä≤)TWk ‚àíT +Ak‚ä§Ak
2
whereT isadiagonalnon-negativematrixofappropriatedimensions. Wehavethefollowingoptimizationproblem.
min œÅ
œÅ,T
s.t. M(œÅ,T)‚™Ø0.
‚àö
Asstatedin(Fazlyabetal.,2019),foranygivenfeasiblepair(œÅ,T), œÅisanupperboundontheLipschitzconstantofthe
desiredmap.
16CompositionalCurvatureBoundsforDeepNeuralNetworks
WefirstassumethatŒ±‚Ä≤ =‚àíŒ≤‚Ä≤,implyingthattheoff-diagonaltermsofM(œÅ,T)willbezero. Asaresult,thelinearmatrix
inequalityconstraintM(œÅ,T)‚™Ø0simplifiestosatisfyingtwosemidefiniteconditionsasfollows,
(cid:40)
Œ≤‚Ä≤2Wk‚ä§TWk ‚™ØœÅI,
Ak‚ä§Ak ‚™ØT.
Weclaimthattheoptimalsolutionforthissystemofconstraintsisgivenby
(cid:40)
T‚àó =Ak‚ä§Ak,
œÅ‚àó =Œ≤‚Ä≤2‚à•Wk‚ä§T‚àóWk‚à• .
2
ThechoiceofœÅ‚àó istrivial. Next,itiseasytoseethatifweinsteaduseT‚Ä≤ = T‚àó+E,whereE isanothernon-negative
diagonalmatrix,wewillhaveWk‚ä§T‚Ä≤Wk =Wk‚ä§(T‚àó+E)Wk =Wk‚ä§T‚àóWk+Wk‚ä§EWk ‚™∞Wk‚ä§T‚àóWk,wherethe
lastinequalityfollowsasWk‚ä§EWk isarealsymmetricpositivesemidefnitematrix.Thisconcludestheproofofoptimality
oftheproposedsolutionforthecaseinwhichŒ±‚Ä≤ =‚àíŒ≤‚Ä≤.
Next, weconsiderthescenarioinwhich|Œ±‚Ä≤| < Œ≤‚Ä≤, weobservethatafunctionthatisslope-restrictedin[Œ±‚Ä≤,Œ≤‚Ä≤]isalso
slope-restrictedin[‚àíŒ≤‚Ä≤,Œ≤‚Ä≤]. Consequently,theproposedœÅ‚àóisafeasiblepointinthiscase,althoughitmaynotbeoptimal.
Thecaseinwhich|Œ≤‚Ä≤|<|Œ±‚Ä≤|followsthesameargument,yieldingafeasiblesolutionœÅ‚àó =|Œ±‚Ä≤|‚à•WkT‚àóWk‚à• .
‚àö 2
Thus,wealwayshaveLp ‚â§L ‚à• T‚àóWk‚à• =Lp,SDP .
dhk œï‚Ä≤ 2 dhk
CorollaryA.2. Letp=2,andconsiderthemaphk(x)=Œ¶(Wkx)wherethederivativeoftheithactivationfunctionis
‚àö
slope-restrictedin[Œ±‚Ä≤,Œ≤‚Ä≤]. ThenLp (x)‚â§‚à•D T‚àóWk‚à• ,whereDisadiagonalmatrixwithD =max{|Œ±‚Ä≤|,|Œ≤‚Ä≤|}.
i i dhk 2 ii i i
B.CalculationofAnchoredLipschitz
Inthissection,weelaborateonsomeaspectsoftheanchoredLipschitzcalculationthatweintroducedinthemaintext.
Consideracontinuouslydifferentiablefunctionœï:R(cid:55)‚ÜíR. TheanchoredLipschitzconstantofœïataxisgivenby
|œï(y)‚àíœï(x)|
L (x)=max . (25)
œï yÃ∏=x |y‚àíx|
This optimization problem can be solved on a case-by-case basis. For example, for the case of the tanh function, the
maximizerof(25)isthepointfromwhichthetangentpassesthrough(x,œï(x)). Thisisgivenbysolvingthenonlinear
equation
|œï(t)‚àíœï(x)|
|œï‚Ä≤(y)|= lim . (26)
t‚Üíy |t‚àíx|
SeeFigure1forreference. Asimilarideafollowsforotherboundedactivationfunctions. Wenotethat(26)isingenerala
nonlinearequationwithoutaclosed-formsolution. Inpractice,weuseanumericalmethod(likebisection)tosolvethis
nonlinearequationatinitiationforagridofpointsofthereallineandthenquerythesevalueswhenevertheyareneededfor
Lipschitzcalculation.
Next,considerasingleresidualblockasin(11)
h(x)=Hx+GŒ¶(Wx).
WiththedefinitionofanchoredLipschitz,onecanusethelocalnaiveboundfortheLipschitzconstanttoobtainthefollowing
boundontheLipschitzconstantofh,
p,naive
L (x)=‚à•H‚à• +‚à•G‚à• ‚à•diag(L (x))W‚à• ,
h p p Œ¶ p
p,naive
whereL (x)=[L (x),¬∑¬∑¬∑ ,L (x)]. ThenL (x)wouldbeanupperboundontheanchoredLipschitzofh. This
Œ¶ œï1 œïn1 h
canbeadaptedtoLipSDP(Fazlyabetal.,2019)orLipLT(Fazlyabetal.,2023).
Theaboveanalysiscanextendedtomulti-layerresidualneuralnetworks. Forexample,multiplyingthelayer-wiseanchored
LipschitzboundswillyieldtheanchorednaiveLipschitzboundforthewholenetwork.
17CompositionalCurvatureBoundsforDeepNeuralNetworks
C.TimeComplexity
WeanalyzethetimecomplexityofAlgorithm1whenthe‚Ñì normisused(p=2). Weutilizethepowermethodtocalculate
2
thematrixnorms. Weassumethatweperformonlyasingleloopofpoweriterationtocalculatethematrixnorms. This
assumptionisjustifiedinpreviouswork(Fazlyabetal.,2023;Huangetal.,2021). Weprovidethecomplexityintermsof
thenumberofmultiplications. Forageneralneuralnetworkoftheformofequation(11)wehavethefollowingcalculations:
‚Ä¢ Lipschitzconstantofeachresidualblock,i.e. Lp :O(n n +n n‚Ä≤ +n‚Ä≤n ).
hk k+1 k k+1 k k k
‚Ä¢ LipschitzconstantoftheJacobianofeachresidualblock,i.e.,Lp :O(n n‚Ä≤ +2n‚Ä≤n )orLp :O(n n‚Ä≤ +
Dhk k+1 k k k dhk k+1 k
n n‚Ä≤n ).
k+1 k k
‚Ä¢ Lipschitzconstantofthesubnetworkfromthefirstlayertothe(k+1)-thlayer,i.e.,
k k k
Lp :O((cid:88)(cid:2) n n +n‚Ä≤n + (cid:88) (n n +n n‚Ä≤ +n‚Ä≤n )(cid:3) +(cid:88) (n n +n n‚Ä≤ +n‚Ä≤n )).
k+1 j+1 j j j i+1 i i+1 i i i i+1 i i+1 i i i
j=0 i=j+1 i=0
However,itisworthmentioningthatthecomputationalcomplexityofaforwardpassthroughthenetworkisalsoasumof
quadraticterms,i.e.,O((cid:80)K n n +n n‚Ä≤ +n‚Ä≤n ).Thus,themainbottleneckwouldbethecalculationofLp
k=0 k+1 k k+1 k k k k+1
p
andL . WeleveragethespecializedGPUimplementationofLipLT(Fazlyabetal.,2023),whichsubstantiallyreducesthe
dhk
p
timecomplexityofcalculatingL ,k =0,¬∑¬∑¬∑K.
k
p p
Furthermore,byusing1-Lipschitznetworksasdoneinsomeoftheexperiments,thetimecomplexityofL andL
k+1 hk
wouldbeO(1).
D.Experiments
Inthissection,weprovidethedetailsofourmethodsandprovidefurthersupportingexperiments.
D.1.ImplementationDetails
Weusedthreedifferentarchitecturesinourexperiments. WeshowconvolutionallayersintheformC(c,k,s,p),where
cisthenumberoffilters,k isthesizeofthesquarekernel,sisthestridelength,andpisthesymmetricpadding. Fully
connectedlayersareoftheformL(n),wherenisthenumberofoutputneuronsofthislayer. Furthermore,residuallayers
oftheform(11)isdenotedbyanextracharacter‚ÄòR‚Äô,i.e.,CRandLRforresidualconvolutionalandfully-connectedlayers,
respectively. Thedetailsofourarchitecturesareasfollows:
‚Ä¢ 6C2F:C(32,3,1,1),C(32,4,2,1),C(64,3,1,1),C(64,4,2,1),C(64,3,1,1)C(64,4,2,1),L(512),L(10).
‚Ä¢ 6F:L(1024),L(512),L(256),L(256),L(128),L(10).
‚Ä¢ Lip-3C1F:CR(15,3,1,1),CR(15,3,1,1),CR(15,3,1,1),LR(1024).
ForthehyperparameterŒªusedintheloss(18),weemployaprimal-dualapproach. Thatis,aftereachmini-batchweupdate
Œª+ =min{Œª+Œ∑(A ‚àíŒµ),Œª },
c min
where Œª is the current value of the regularizer, Œ∑ is a step size, A is a moving average of the training accuracy of the
c
mini-batches, Œµ is the minimum train accuracy that we expect from the model, and Œª is the smallest value for the
min
regularizer. FortrainingonCIFAR-10,wechoose(Œ∑,œµ,Œª )=(0.05,0.6,0.01).
min
Therestofthetrainingdetailsareasfollows. Weusethemodifiedcross-entropylossfunctionfrom(Prach&Lampert,
2022),
‚àö
f(x)‚àíŒΩ 2Lu
LœÑ,ŒΩ(f(x),y)=œÑ ¬∑CE( y,y), (27)
CE œÑ
whereœÑ isatemperatureconstant,u istheone-hotencodingofthevalueofy,andLisan‚Ñì Lipschitzconstantofthe
y 2
model. ŒΩ isazero-onevariablebasedonthearchitectureandmodeoftraining. Forthe6C2Fand6Fmodels,aswewant
18CompositionalCurvatureBoundsforDeepNeuralNetworks
Figure 7: Comparison of certified radii acquired via CRC/CCRC on a 6-layer neural network trained via curvature
regularizationonMNIST.(Left)Histogramofper-sampleradiusimprovementofourmethodover(Singla&Feizi,2020).
(Right)Plotofcertifiedradiiforcorrectlyclassifieddata. ThedataaresortedaccordingtotheCRCradii.
250 1400 Lipschitz Certificates
Curvature Certificates
1200
200
1000
150
800
100 600
400
50
200
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
Radius Difference Certified Radius
Figure8: ComparisonofcertifiedradiicalculatedthroughCurvatureCertificatesversusLipschitzCertificatesfora6-layer
neuralnetworktrainedviacurvatureregularizationonCIFAR-10. (Left)Histogramoftheper-sampleradiiimprovements.
(Right)Histogramofcertifiedradii.
regularizationdirectlythroughthecurvatureconstant,wesetŒΩ =0. FortheLip-3C1Fmodel,wesetŒΩ =1astheoriginal
(Araujoetal.,2022)work. WeuseœÑ =0.25. Furthermore,wetrainourmodelsfor1000epochswithabatchsizeof256
withacosineannealingstrategywithaninitiallearningrateof10‚àí4andafinallearningrate10‚àí5,andreporttheaverage
resultsontwoseedinTable1.
D.2.Per-sampleImprovement
Expandingonthe‚ÄúComparisonwithotherCurvature-basedMethods‚ÄùexperimentinSection4,weprovidetheper-sample
improvementsofthecertifiedradiiinFigure7,correspondingtoFigure3.
D.3.TrainingwithDirectCurvatureRegularization
We observed that for the models that are trained with direct regularization of the curvature, first-order certificates are
significantlybetterthanzeroth-ordercertificates,i.e.,byregularizingthemodel‚Äôscurvature,Proposition2.2wouldholdfor
allpointsofthetestdataset. ThisisshowninFigure8forthe6Fmodel.
D.4.AttackCertificateson1-Lipschitzmodels
Inthisexperiment,weprovideradiiforprovableattacksona1-LipschitzmodeltrainedontheCIFAR-10dataset. The
curvaturerequiredforthiscertificatewascalculatedusingAlgorithm1andutilizingthe1-Lipschitzstructure. Figure9
showsthebudgetrequiredforasubsetofthecorrectlyclassifieddatapointsthatcancertifiablybeattacked. Weconsiderthe
testsamplesoftheCIFAR-10dataset,whichincludes10,000samples. Themodel‚Äôsaccuracyonthetestsetisapproximately
50%,resultinginabout5,000correctlylabeledsamples. Ofthese5,000samples,wecanprovideanattackcertificatefor
19CompositionalCurvatureBoundsforDeepNeuralNetworks
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
Attack Radius
Figure9: Attackradiicertificatesfora1-Lipschitzstructure.
approximately150ofthem. Thistranslatestoa3%successrate(150/5000)fortheattackcertificateamongthecorrectly
classifiedtestsamples. Itisworthnotingthatthesesamplescanallbeprovablymisclassifiedwithanattackbudgetofless
than0.07,evenon1-Lipschitznetworks.
20