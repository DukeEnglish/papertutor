PublishedasaconferencepaperatICLR2024
FEDERATED CAUSAL DISCOVERY FROM HETEROGE-
NEOUS DATA
LokaLi1, IgnavierNg2, GongxuLuo1, BiweiHuang3,
GuangyiChen1,2, TongliangLiu1, BinGu1, KunZhang1,2
1MohamedbinZayedUniversityofArtificialIntelligence
2CarnegieMellonUniversity
3UniversityofCaliforniaSanDiego
{longkang.li, kun.zhang}@mbzuai.ac.ae
ABSTRACT
Conventionalcausaldiscoverymethodsrelyoncentralizeddata, whichisincon-
sistent with the decentralized nature of data in many real-world situations. This
discrepancyhasmotivatedthedevelopmentoffederatedcausaldiscovery(FCD)
approaches. However,existingFCDmethodsmaybelimitedbytheirpotentially
restrictive assumptions of identifiable functional causal models or homogeneous
data distributions, narrowing their applicability in diverse scenarios. In this pa-
per,weproposeanovelFCDmethodattemptingtoaccommodatearbitrarycausal
modelsandheterogeneousdata.Wefirstutilizeasurrogatevariablecorresponding
totheclientindextoaccountforthedataheterogeneityacrossdifferentclients.We
thendevelopafederatedconditionalindependencetest(FCIT)forcausalskeleton
discoveryandestablishafederatedindependentchangeprinciple(FICP)todeter-
minecausaldirections. Theseapproachesinvolveconstructingsummarystatistics
as a proxy of the raw data to protect data privacy. Owing to the nonparametric
properties,FCITandFICPmakenoassumptionaboutparticularfunctionalforms,
therebyfacilitatingthehandlingofarbitrarycausalmodels. Weconductextensive
experimentsonsyntheticandrealdatasetstoshowtheefficacyofourmethod.The
codeisavailableathttps://github.com/lokali/FedCDH.git.
1 INTRODUCTION
Causal discovery aims to learn the causal structure from observational data, attracting significant
attention from fields such as machine learning and artificial intelligence (Nogueira et al., 2021),
healthcare(Shenetal.,2020),economics(Zhang&Chan,2006),manufacturing(VukovicÂ´ &Thal-
mann,2022)andneuroscience(Tuetal.,2019). Recently,ithasbeenfacingnewopportunitiesand
challengesfromtherapidgrowthofdatavolume.Oneofthekeychallengesisdatadecentralization.
Traditionally, causal discovery is conducted at a centralized site where all data is gathered in one
location. However,inreal-worldscenarios,dataisoftendistributedacrossmultipleparties,suchas
the healthcare data across various hospitals (Kidd et al., 2022). Consequently, there has been in-
creasinginterestinfederatedcausaldiscovery(FCD),whichaimstouncovertheunderlyingcausal
structureofdecentralizeddatawithprivacyandsecurityconcerns.
Existing FCD methods from observational data can be generally classified as continuous-
optimization-based, constraint-based, and score-based methods. Some continuous-optimization-
basedmethodsextendNOTEARS(Zhengetal.,2018)withfederatedstrategies,suchasNOTEARS-
ADMM (Ng & Zhang, 2022) that relies on the ADMM (Boyd et al., 2011) optimization method,
FedDAG(Gaoetal.,2022)thatemploysFedAvg(McMahanetal.,2017)technique,andFED-CD
(Abyaneh et al., 2022) that utilizes belief aggregation. These methods might suffer from various
technicalissues,includingconvergence(Weietal.,2020;Ngetal.,2022),nonconvexity(Ngetal.,
2023), and sensitivity to data standardization (Reisach et al., 2021). As for score-based methods,
DARLIS (Ye et al., 2022) utilizes the distributed annealing (Arshad & Silaghi, 2004) strategy to
search for the optimal graph, while PERI (Mian et al., 2023) aggregates the results of the local
greedyequivalentsearch(GES)(Chickering,2002)andchoosestheworst-caseregretforeachitera-
tion. Oneconstraint-basedmethod,FEDC2SL(Wangetal.,2023),extendesÏ‡2testtothefederated
1
4202
beF
02
]GL.sc[
1v14231.2042:viXraPublishedasaconferencepaperatICLR2024
Table1:Thecomparisonofrelatedworksaboutfederatedcausaldiscoveryfromobservationaldata.
OurFedCDHmethodcouldhandlearbitraryfunctionalcausalmodelsandheterogeneousdata.
Data CausalModel Identifiability Federated
Method Category Distribution Assumption1 Requirement Strategy
NOTEARS-ADMM(Ng&Zhang,2022) Optimization-based Homogeneous LinearGaussian,EV Yes ADMM
NOTEARS-MLP-ADMM(Ng&Zhang,2022) Optimization-based Homogeneous NonlinearAdditiveNoise Yes ADMM
FedDAG(Gaoetal.,2022) Optimization-based Heterogeneous NonlinearAdditiveNoise Yes FedAvg
Fed-CD(Abyanehetal.,2022) Optimization-based Heterogeneous NonlinearAdditiveNoise Yes BeliefAggregation
DARLIS(Yeetal.,2022) Score-based Homogeneous GeneralizedLinear No DistributedAnnealing
PERI(Mianetal.,2023) Score-based Homogeneous LinearGaussian,NV No Voting
FedPC(Huangetal.,2022) Constraint-based Homogeneous LinearGaussian,NV No Voting
FedCDH(Ours) Constraint-based Heterogeneous ArbitraryFunctions No SummaryStatistics
version, however, this method is restrictive on discrete variables and therefore not applicable for
any continuous variables. Other constraint-based methods, such as FedPC (Huang et al., 2022),
aggregate the skeletons and directions of the Peter-Clark (PC) algorithm (Spirtes et al., 2000) by
each client via a voting mechanism. However, as shown in Table 1, most of these methods heav-
ily rely on either identifiable functional causal models or homogeneous data distributions. These
assumptions may be overly restrictive and difficult to be satisfied in real-world scenarios, limiting
theirdiverseapplicability. Forinstance,distributionshiftsmayoftenoccurintherealworldacross
differentclientsowingtodifferentinterventions,collectionconditions,ordomains,resultinginthe
presenceofheterogeneousdata.PleaserefertoAppendixA2forfurtherdiscussionofrelatedworks,
includingthoseofcausaldiscovery,heterogeneousdataandFCD.
In this paper, we propose FedCDH, a novel constraint-based approach for Federated Causal
Discovery from Heterogeneous data. The primary innovation of FedCDH lies in using summary
statistics as a proxy for raw data during skeleton discovery and direction determination in a fed-
erated fashion. Specifically, to address heterogeneous data, we first introduce a surrogate variable
corresponding to the client or domain index, allowing our method to model distribution changes.
UnlikeexistingFCDmethodsthatonlyleveragethedatafromdifferentclientstoincreasethetotal
samplesize,wedemonstratehowsuchdataheterogeneityacrossdifferentclientsbenefitstheidenti-
ficationofcausaldirectionsandhowtoexploitit. Furthermore,weproposeafederatedconditional
independence test (FCIT) for causal skeleton discovery, incorporating random features (Rahimi &
Recht,2007)toapproximatethekernelmatrixwhichfacilitatestheconstructionofthecovariance
matrix. Additionally, we develop a federated independent change principle (FICP) to determine
causaldirections,exploitingthecausalasymmetry. FICPalsoemploysrandomfeaturestoapprox-
imate the embeddings of heterogeneous conditional distributions for representing changing causal
models. It is important to note that FCIT and FICP are non-parametric, making no assumption
aboutspecificfunctionalforms,thusfacilitatingthehandlingofarbitrarycausalmodels. Toevalu-
ateourmethod, weconductextensiveexperimentsonsyntheticdatasetsincludinglinearGaussian
models and general functional models, and real-world dataset including fMRI Hippocampus (Pol-
dracketal.,2015)andHKStockMarketdatasets(Huangetal.,2020). Thesignificantperformance
improvementsoverotherFCDmethodsdemonstratethesuperiorityofourapproach.
2 REVISITING CAUSAL DISCOVERY FROM HETEROGENEOUS DATA
Inthissection,wewillfirstlyprovideanoverviewofcausaldiscoveryandsomecommonassump-
tions, then we will introduce the characterizations of conditional independence and independent
change. Thispaperaimsatextendingthosetechniquesfromthecentralizedtothefederatedsetting.
1)CausalDiscoverywithChangingCausalModels.
ConsiderdobservablerandomvariablesdenotedbyV=(V ,...,V )andK clients,andoneclient
1 d
correspondstooneuniquedomain. Inthispaper,wefocusonhorizontally-partitioneddata(Samet
& Miri, 2009), where each client holds a different subset of the total data samples while all the
clients share the same set of features. Let k be the client index, and â„§ be the domain index,
where k,â„§âˆˆ{1,...,K}. Each client has n samples, in total there are n samples, denoted by
k
1LinearGaussianmodelwithequalnoisevariance(EV)(Peters&BuÂ¨hlmann,2014)andnonlinearadditive
noisemodel(Hoyeretal.,2008)areidentifiable,whilelinearGaussianmodelwithnon-equalnoisevariance
(NV)isnotidentifiable.Generalizedlinearmodelandarbitraryfunctionalmodelarecertainlynotidentifiable.
2PublishedasaconferencepaperatICLR2024
n=(cid:80)K
n . The task of federated causal discovery is to recover the causal graph G given the
k=1 k
decentralizeddatamatrixVâˆˆRnÃ—d.
When the data is homogeneous, the causal process for each variable V can be represented by the
i
following structural causal model (SCM): V =f (PA ,Ïµ ), where f is the causal function, PA is
i i i i i i
theparentsofV ,Ïµ isanoisetermwithnon-zerovariance,andweassumetheÏµ â€™saremutuallyin-
i i i
dependent. Whenthedataisheterogeneous,theremustbesomecausalmodelschangingacrossdif-
ferentdomains. Thechangesmaybecausedbythevariationofcausalstrengthsornoisevariances.
Therefore,weformulatethecausalprocessforheterogeneousdataas:V =f (PA ,Ïµ ,Î¸ (â„§),ÏˆËœ(â„§)),
i i i i i
whereâ„§isregardedasanobservedrandomvariablereferredasthedomainindex, thefunctionf
i
orthedistributionofthenoiseÏµ isdifferentorchangingacrossdifferentdomains,bothÏˆËœ(â„§)and
i
Î¸ (â„§)areunobserveddomain-changingfactorsrepresentedasthefunctionsofvariableâ„§,ÏˆËœ(â„§)is
i
thesetofâ€pseudoconfoundersâ€thatinfluencethewholesetofvariablesandweassumethereareL
suchconfounders(ÏˆËœ(â„§)={Ïˆ (â„§)}L ,theminimumvalueforLcanbe0meaningthatthereisno
l l=1
suchlatentconfounderinthegraph,whilethemaximumvaluecanbeC2 = d(d+1),meaningthat
d 2
each pair of observed variables has a hidden confounder), Î¸ (â„§) denotes the effective parameters
i
ofV inthemodel,andweassumethatÎ¸ (â„§)isspecifictoV andisindependentofÎ¸ (â„§)forany
i i i j
iÌ¸=j. ÏˆËœ(â„§)andÎ¸ (â„§)inputâ„§whichisapositiveintegerandoutputarealnumber. LetG bethe
i obs
underlyingcausalgraphoverV,andG betheaugmentedgraphoverVâˆªÏˆËœ(â„§)âˆª{Î¸ (â„§)}d .For
aug i i=1
causaldiscoverywithchangingcausalmodels,wefollowpreviousworksuchasCD-NOD(Huang
etal.,2020)andmakethefollowingassumptions.
Assumption1(PseudoCausalSufficiency). Thereisnoconfounderinthedatasetofonedomain,
butweallowthechangesofdifferentcausalmodulesacrossdifferentdomainstobedependent.
Assumption2(MarkovandFaithfulness). ThejointdistributionoverV âˆªÏˆËœ(â„§)âˆª{Î¸ (â„§)}d is
i i=1
MarkovandfaithfultoG .
aug
Toremovethepotentialinfluencefromconfounders
and recover causal relations across different do-
â„§
mains, causal discovery could be conducted on the
augmented graph G instead of G . While â„§
aug obs
ÏˆËœ(â„§)âˆª{Î¸ i(â„§)}d i=1areunobservedvariables,thedo- !!(â„§) !"(â„§)
main index â„§ is observed variable. Therefore, â„§ is
$â„“(â„§)
introduced as the surrogate variable (Huang et al.,
! !
2020)forcausaldiscoveryfromheterogeneousdata. ! ! # $
# $
An illustration is given in Figure 1, where the aug-
(a) (b)
mentedgraphwiththeunobserveddomain-changing
variablesÏˆËœ(â„§)andÎ¸ i(â„§)couldbesimplifiedbyan Figure1: Anillustrationwherethecausalmod-
augmentedgraphwithjustasurrogatevariableâ„§. If elsofvariablesV andV arechangingacrossdo-
i j
there is an edge between surrogate variable â„§ and mains. (a) the graph with unobserved domain-
o cab us se arv le md ov da er lia rb ell ae teV di o ton VG a iu sg c, hth ae nn gii nt gm ae ca rn os ssth da it fft eh re
-
c sih man pg lii fin eg dfa gc rato pr hs wÏˆ iâ„“ t( hâ„§ t) h, eÎ¸ si u( râ„§ ro) ga an td evÎ¸ aj r( iâ„§ ab); le(b â„§) .the
i
entdomains,inotherwords,thedatadistributionof
V isheterogeneousacrossdomains.
i
2)CharacterizationofConditionalIndependence. LetX,Y,Z berandomvariablesorsetsof
randomvariables,withthedomainsX,Y,Z,respectively.Defineameasurableandpositivedefinite
kernelk ,anddenotethecorrespondingreproducingkernelHilbertspace(RKHS)H . Similarly,
X X
wedefinek ,H ,k andH . Oneofthemostusedcharacterizationsofconditionalindependence
Y Y Z Z
(CI)is: X âŠ¥âŠ¥ Y|Z ifandonlyifP = P P ,orequivalentlyP = P . Another
XY|Z X|Z Y|Z X|Y,Z X|Z
characterizationofCIisgivenintermsofthepartialcross-covarianceoperatoronRKHS.
Lemma1(CharacterizationofCIwithPartialCross-covariance(Fukumizuetal.,2007)). LetXÂ¨ â‰œ
(X,Z),k â‰œ k k ,andH betheRKHScorrespondingtok . AssumethatH âŠ‚ L2 ,H âŠ‚
XÂ¨ X Z XÂ¨ XÂ¨ X X Y
L2,H âŠ‚ L2. Further assume that k k is a characteristic kernel on (X Ã—Z)Ã—Y, and that
Y Z Z XÂ¨ Y
H
Z
+ R (the direct sum of two RHKSs) is dense in L2(P Z). Let Î£
XÂ¨Y|Z
be the partial cross-
covarianceoperator,then
Î£ =0 â‡â‡’ X âŠ¥âŠ¥Y|Z. (1)
XÂ¨Y|Z
3PublishedasaconferencepaperatICLR2024
Based on the above lemma, we further consider a different characterization of CI which enforces
the uncorrelatedness of functions in suitable spaces, which may be intuitively more appealing.
More details about the interpretation of Î£ , the definition of characteristic kernel, and the
XÂ¨Y|Z
uncorrelatedness-basedcharacterizationofCI,areputinAppendixA3.1.
3) Characterization of Independent Change. The Hilbert-Schmidt Independence Criterion
(HSIC)(Grettonetal.,2007)isastatisticalmeasureusedtoassesstheindependencebetweentwo
randomvariablesintheRKHS.WeusethenormalizedHSICtoevaluatetheindependenceoftwo
changingcausalmodels. ThevalueofthenormalizedHSICrangesfrom0to1,andasmallervalue
indicates that the two changing causal models are more independent. Let â–³Ë† be the normal-
Xâ†’Y
ized HSIC between P(X) and P(Y|X), and â–³Ë† be the normalized HSIC between P(Y) and
Yâ†’X
P(X|Y). Then,wecandeterminethecausaldirectionbetweenX andY withthefollowinglemma.
Lemma 2 (Independent Change Principle (Huang et al., 2020)). Let X and Y be two random
observed variables. Assume that both X and Y have changing causal models (both of them are
adjacenttoâ„§inG ). ThenthecausaldirectionbetweenX andY canbedeterminedaccording
aug
tothefollowingrules
i) Ifâ–³Ë† <â–³Ë† ,outputthedirectionXâ†’Y,
Xâ†’Y Yâ†’X
ii) Ifâ–³Ë† >â–³Ë† ,outputthedirectionYâ†’X.
Xâ†’Y Yâ†’X
Moredetailsaboutthedefinitionandformulationofâ–³Ë† andâ–³Ë† areinAppendixA3.2. It
Xâ†’Y Xâ†’Y
is important to note that: once the Gaussian kernel is utilized, the kernel-based conditional inde-
pendencetest(Zhangetal.,2012)andthekernel-basedindependentchangeprincipal(Huangetal.,
2020)assumesmoothnessfortherelationshipofcontinuousvariables.
3 FEDERATED CAUSAL DISCOVERY FROM HETEROGENEOUS DATA
Inthissection,wewillexplainourproposedFedCDHmethodindetails. Anoverallframeworkof
FedCDHisgiveninFigure2. Twokeysubmodulesofourmethodarefederatedconditionalinde-
pendenttest(FCIT;Theorem4andTheorem5)andfederatedindependentchangeprinciple(FICP;
Theorem 6), which are presented in Section 3.1 and Section 3.2, respectively. We then illustrate
howtoconstructthesummarystatisticsandhowtoimplementFCITandFICPwithsummarystatis-
tics(Theorem8)inSection3.3. Lastbutnotleast,wediscussthecommunicationcostsandsecure
computationsinSection3.4. Fortheproofsoftheoremsandlemmas,pleaserefertoAppendixA4.
3.1 FEDERATEDCONDITIONALINDEPENDENTTEST(FCIT)
1)NullHypothesis. Considerthenullandalternativehypotheses
H :X âŠ¥âŠ¥Y|Z, H :XâŠ¥Ì¸âŠ¥Y|Z. (2)
0 1
AccordingtoEq. 1,wecanmeasureconditionalindependencebasedontheRKHSs. Therefore,we
equivalentlyrewritetheabovehypothesismoreexplicitlyas
H :âˆ¥Î£ âˆ¥2 =0, H :âˆ¥Î£ âˆ¥2 >0. (3)
0 XÂ¨Y|Z HS 1 XÂ¨Y|Z HS
NotethatthecomputationformsofHilbert-SchmidtnormandFrobeniusnormarethesame,andthe
differenceisthattheHilbert-SchmidtnormisdefinedininfiniteHilbertspacewhiletheFrobenius
norm is defined in finite Euclidean space. We here consider the squared Frobenius norm of the
empiricalpartialcross-covariancematrixasanapproximationforthehypotheses,givenas
H :âˆ¥C âˆ¥2 =0, H :âˆ¥C âˆ¥2 >0, (4)
0 XÂ¨Y|Z F 1 XÂ¨Y|Z F
whereC =1 (cid:80)n [(AÂ¨ âˆ’E(AÂ¨|Z))T(B âˆ’E(B|Z))]correspondstothepartialcross-covariance
XÂ¨Y|Z n i=1 i i
matrix with n samples, C âˆˆRhÃ—h, AÂ¨=f(XÂ¨)âˆˆRnÃ—h, B=g(Y)âˆˆRnÃ—h, {fj(XÂ¨)|h }âˆˆF ,
XÂ¨Y|Z j=1 XÂ¨
{gj(Y)|h }âˆˆF . n and h denote the number of total samples of all clients and the number
j=1 Y
4PublishedasaconferencepaperatICLR2024
Summary Statistics
Total Sample Size & Global Covariance Tensor
Federated Conditional Federated Independent
Independence Test Change Principle
Server (FCIT) (FICP)
Sample Size & Local Sample Size & Local Sample Size & Local Skeleton Discovery Skeleton Discovery Direction Determination Direction Determination
Covariance Tensor Covariance Tensor Covariance Tensor Step 1 â„§ Step 2 â„§ Step 1 â„§ Step 2 â„§
â€¦ ğ‘‰# ğ‘‰! ğ‘‰" ğ‘‰# ğ‘‰! ğ‘‰" ğ‘‰# ğ‘‰! ğ‘‰" ğ‘‰# ğ‘‰! ğ‘‰"
Graph Initialization Ground-truth Graph
Local Data Local Data Local Data â„§ ğœ“(â„§)
Client 1 Client 2 Client K ğ‘‰# ğ‘‰! ğ‘‰" ğ‘‰# ğ‘‰! ğ‘‰"
Figure2: OverallframeworkofFedCDH. Left: Theclientswillsendtheirsamplesizesandlocal
covariancetensorstotheserver, forconstructingthesummarystatistics. Thefederatedcausaldis-
coverywillbeimplementedontheserver.RightTop:Relyingonthesummarystatistics,wepropose
two submodules: federated conditional independence test and federated independent change prin-
ciple,forskeletondiscoveryanddirectiondetermination. RightBottom: AnexampleofFCDwith
threeobservedvariablesisillustrated,wherethecausalmodulesrelatedtoV andV arechanging.
2 3
of hidden features or mapping functions, respectively. Since XÂ¨â‰œ(X,Z), then for each function
fj:XÂ¨(cid:55)â†’F , the input is XÂ¨âˆˆRnÃ—2 and the output fj(XÂ¨)âˆˆRn. For each function gk:Y(cid:55)â†’F , the
XÂ¨ Y
inputisYâˆˆRn andtheoutputgk(Y)âˆˆRn. NoticethatF andF arefunctionspaces,whichare
âˆš XÂ¨ Y
settobethesupportoftheprocess 2cos(wÂ·+b),wfollowsstandardGaussiandistribution,and
bfollowsuniformdistributionfrom[0,2Ï€]. Wechoosethesespecificspacesbecauseinthispaper
weuserandomfeaturestoapproximatethekernels. E(AÂ¨|Z)andE(B|Z)couldbenon-linearfunc-
tionsofZ whicharedifficulttoestimate. Therefore,wewouldliketoapproximatethemwithlinear
functions. Let q(Z)âˆˆRnÃ—h, {qj(Z)|h }âˆˆF , F shares a similar function space with F . We
j=1 Z Z Y
couldestimateE(fj|Z)withthelinearridgeregressionsolutionuTq(Z)andestimateE(gj|Z)with
j
vTq(Z)undermildconditions(Sutherland&Schneider,2015). Nowwegivethefollowinglemma.
j
Lemma3(CharacterizationofConditionalIndependence). Letfj andgj bethefunctionsdefined
forthevariablesXÂ¨ andY,respectively.ThenX âŠ¥âŠ¥Y|Zisapproximatedbythefollowingcondition
E(fËœgËœ)=0, âˆ€fËœâˆˆF and âˆ€gËœâˆˆF , (5)
XÂ¨|Z Y|Z
whereF ={fËœ|fËœj=fjâˆ’uTq(Z), fjâˆˆF }andF ={gËœ|gËœj=gjâˆ’vTq(Z), gjâˆˆF }.
XÂ¨|Z j XÂ¨ Y|Z j Y
LetÎ³ beasmallridgeparameter. AccordingtoEq. 1andEq. 5,byridgeregression,weobtain
C =C âˆ’C (C +Î³I)âˆ’1C . (6)
XÂ¨Y|Z XÂ¨Y XÂ¨Z ZZ ZY
2)TestStatisticandNullDistribution. Inordertoensuretheconvergencetoanon-degeneratedis-
tribution,wemultiplytheempiricalestimateoftheFrobeniusnormbyn,andsetitastheteststatistic
T =nâˆ¥C âˆ¥2. LetKËœ bethecentralizedkernelmatrix,givenbyKËœ â‰œHR RT H,
CI XÂ¨Y|Z F XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z
where H=Iâˆ’111T and R â‰œfËœ(XÂ¨)=f(XÂ¨)âˆ’uTq(Z) which can be seen as the residual after
n XÂ¨|Z
kernel ridge regression. Here, I refers to the nÃ—n identity matrix and 1 denotes the vector of n
ones. WenowdefineKËœ similarly. LetÎ» andÎ» betheeigenvaluesofKËœ andKËœ ,
Y|Z XÂ¨|Z Y|Z XÂ¨|Z Y|Z
respectively. Let {Î± ,...,Î± } denote i.i.d. standard Gaussian variables, and thus {Î±2,...,Î±2}
1 L 1 L
denotei.i.d. Ï‡2variables. Consideringni.i.d. samplesfromthejointdistributionP ,wehave
1 XÂ¨YZ
Theorem4(FederatedConditionalIndependentTest). UnderthenullhypothesisH (X andY are
0
conditionallyindependentgivenZ),theteststatistic
T â‰œnâˆ¥C âˆ¥2, (7)
CI XÂ¨Y|Z F
5PublishedasaconferencepaperatICLR2024
hastheasymptoticdistribution
L
TË† â‰œ 1 (cid:88) Î» Î» Î±2 .
CI n2 XÂ¨|Z,i Y|Z,j ij
i,j=1
Althoughthedefinedteststatisticsareequivalenttothatofkernel-basedconditionalindependence
test(KCI)(Zhangetal.,2012),theasymptoticdistributionsareindifferentforms. Pleasenotethat
thelargesamplepropertiesareneededwhenderivingtheasymptoticdistributionTË† above,andthe
CI
proofisshowninAppendixA4.2.
Given that X âŠ¥âŠ¥ Y|Z, we could introduce the independence between R and R , which
XÂ¨|Z Y|Z
leadstotheseparationbetweenÎ» andÎ» . Weshowthatthisseparatedformcouldhelpto
XÂ¨|Z,i Y|Z,j
approximatethenulldistributionintermsofadecomposablestatistic,suchasthecovariancematrix.
We approximate the null distribution with a two-parameter Gamma distribution, which is related
to the mean and variance. Under the hypothesis H and given the sample D, the distribution of
0
TË† can be approximated by the Î“(kË†,Î¸Ë†) distribution: P(t) = (tkË†âˆ’1 Â·eâˆ’t/Î¸Ë† )/(Î¸kË† Â·Î“(kË†)), where
CI
kË† = E2(TË† |D)/Var(TË† |D),andÎ¸Ë†= Var(TË† |D)/E(TË† |D). Weproposetoapproximatethe
CI CI CI CI
nulldistributionwiththemeanandvarianceinthefollowingtheorem.
Theorem5(NullDistributionApproximation). UnderthenullhypothesisH (X andY arecondi-
0
tionallyindependentgivenZ),wehave
E(TË† |D)=tr(C )Â·tr(C ),
CI XÂ¨|Z Y|Z
(8)
Var(TË† |D)=2âˆ¥C âˆ¥2 Â·âˆ¥C âˆ¥2,
CI XÂ¨|Z F Y|Z F
whereC =1RT HHR ,C =1RT HHR ,andtr(Â·)meansthetraceoperator.
XÂ¨|Z n XÂ¨|Z XÂ¨|Z Y|Z n Y|Z Y|Z
FortestingtheconditionalindependenceX âŠ¥âŠ¥ Y|Z,inthispaper,weonlydealwiththescenarios
where X and Y each contain a single variable while Z could contain a single variable, multiple
variables,orbeempty. WhenZ isempty,thetestbecomesthefederatedunconditionalindependent
test(FUIT),asaspecialcase. WeprovidemoredetailsaboutFUITinAppendixA5.
3.2 FEDERATEDINDEPENDENTCHANGEPRINCIPLE(FICP)
As described in Lemma 2, we can use independent change principle (ICP) to evaluate the depen-
dence between two changing causal models. However, existing ICP (Huang et al., 2020) heavily
relies on the kernel matrix to calculate the normalized HSIC. It may be challenging for decentral-
izeddatabecausetheoff-diagonalentriesofkernelmatrixrequiretherawdatafromdifferentclients,
whichviolatesthedataprivacyinfederatedlearning. Motivatedbythat,weproposetoestimatethe
normalizedHSICwiththefollowingtheorem.
Theorem6(FederatedIndependentChangePrinciple). Inordertocheckwhethertwocausalmodels
changeindependentlyacrossdifferentdomains,wecanestimatethedependenceby
âˆ¥Câˆ— âˆ¥2 âˆ¥Câˆ— âˆ¥2
â–³Ë† = X,YËœ F , â–³Ë† = Y,XËœ F , (9)
Xâ†’Y tr(Câˆ—)Â·tr(Câˆ—) Yâ†’X tr(Câˆ—)Â·tr(Câˆ—)
X YËœ Y XËœ
where YËœâ‰œ(Y|X), XËœâ‰œ(X|Y), Câˆ— and Câˆ— are specially-designed covariance matrices, and
X,YËœ Y,XËœ
Câˆ—,Câˆ—,Câˆ— andCâˆ— arespecially-designedvariancematrices.
X Y XËœ YËœ
3.3 IMPLEMENTINGFCITANDFICPWITHSUMMARYSTATISTICS
MoredetailsaregivenabouthowtoimplementFCITandFICPwithsummarystatistics. Theproce-
duresattheclientsandtheserverareshowninAlgorithm1. Eachclientneedstocalculateitslocal
samplesizeandcovariancetensor,whichareaggregatedintosummarystatisticsattheserver.
Thesummarystatisticscontaintwoparts: totalsamplesizenandcovariancetensorC . Withthe
T
summary statistics as a proxy, we can substitute the raw data at each client for FCD. The global
6PublishedasaconferencepaperatICLR2024
Algorithm1FedCDH: FederatedCausalDiscoveryfromHeterogeneousData
Input: datamatrixD
k
âˆˆRnkÃ—dateachclient,k,â„§âˆˆ{1,...,K}
Output: acausalgraphG
Clientexecutes:
1: (SummaryStatisticsCalculation) Foreachclientk,usethelocaldataD ktogetthesamplesize
n andcalculatethecovariancetensorC ,andsendthemtotheserver.
k Tk
Serverexecutes:
2: (SummaryStatisticsConstruction) Construct the summary statistics by summing up the local
samplesizesandthelocalcovariancetensors:
n=(cid:80)K
n ,C
=(cid:80)K
C .
k=1 k T k=1 Tk
3: (AugmentedGraphInitialization) BuildacompletelyundirectedgraphG 0ontheextendedvari-
ablesetV âˆª{â„§},whereV denotestheobservedvariablesandâ„§issurrogatevariable.
4: (FederatedConditionalIndependenceTest) Conduct the federated conditional independence
testbasedonthesummarystatistics,forskeletondiscoveryonaugmentedgraphanddirection
determinationwithonechangingcausalmodule. Intheend,getanintermediategraphG .
1
5: (FederatedIndependentChangePrinciple) Conduct the federated independent change princi-
plebasedonthesummarystatistics,fordirectiondeterminationwithtwochangingcausalmod-
ules. Ultimately,outputthecausalgraphG.
statisticsaredecomposablebecausetheycouldbeobtainedbysimplysummingupthelocalones,
suchasn=(cid:80)K
n andC
=(cid:80)K
C . Specifically,weincorporatetherandomFourierfeatures
k=1 k T k=1 Tk
(Rahimi & Recht, 2007), because they have shown competitive performances to approximate the
continuous shift-invariant kernels. According to the following Lemma, we could derive a decom-
posablecovariancematrixfromanindecomposablekernelmatrixviarandomfeatures.
Lemma7(EstimatingCovarianceMatrixfromKernelMatrix). Assumingthereareni.i.d. samples
forthecentralizedkernelmatricesKËœ ,KËœ ,KËœ andthecovariancematrixC ,wehave
x y x,y x,y
tr(KËœ )â‰ˆtr(Ï•Ëœ (x)Ï•Ëœ (y)T)=tr(Ï•Ëœ (y)TÏ•Ëœ (x))=ntr(C ),
x,y w w w w x,y
(10)
tr(KËœ KËœ )â‰ˆtr(Ï•Ëœ (x)Ï•Ëœ (x)TÏ•Ëœ (y)Ï•Ëœ (y)T)=âˆ¥Ï•Ëœ (x)TÏ•Ëœ (y)âˆ¥2 =n2âˆ¥C âˆ¥2,
x y w w w w w w x,y
wherex,yâˆˆRn,KËœ ,KËœ ,KËœ âˆˆRnÃ—n,C âˆˆRhÃ—h,Ï•Ëœ (x)âˆˆRnÃ—histhecentralizedrandomfea-
x y x,y x,y w
(cid:113)
ture,Ï•Ëœ (x)=HÏ• (x),Ï• (x)â‰œ 2[cos(w x+b ),...,cos(w x+b )]T andÏ• (x)âˆˆRnÃ—h,and
w w w h 1 1 h h w
similarlyforÏ•Ëœ (y)andÏ• (y). wisdrawnfromP(w)andbisdrawnuniformlyfrom[0,2Ï€].
w w
Inthispaper,weuserandomfeaturestoapproximatetheGaussiankernelforcontinuousvariables
and the delta kernel for discrete variables such as the surrogate variable â„§. It is important to note
thatthissurrogatevariableâ„§isessentiallyadiscretevariable(morespecifically,acategoricalvari-
able, with no numerical order among different values), and a common approach to deal with such
discrete variables is to use delta kernel. Notice that C denotes the covariance matrix for vari-
x,y
ablesetsxandy,whichissample-wisedecomposablebecauseC
=(cid:80)K
C ,whereC
x,y k=1 xk,yk xk,yk
corresponds to the local covariance matrix of variable sets x and y at the k-th client. Here,
k k
we have x k,y kâˆˆRnk,C xk,ykâˆˆRhÃ—h. In the augmented graph, there are dâ€²=d+1 variables (d ob-
served variables and one surrogate variable), thus we could construct a global covariance tensor
C âˆˆRdâ€²Ã—dâ€²Ã—hÃ—hbysummingupthelocalonesC âˆˆRdâ€²Ã—dâ€²Ã—hÃ—h.
T Tk
Theorem8(SufficiencyofSummaryStatistics). Thesummarystatistics,consistingoftotalsample
sizenandcovariancetensorC ,aresufficienttorepresentallthestatisticsforFCD,includingT
T CI
inEq. 7,E(TË† |D)andVar(TË† |D)inEq. 8,andâ–³Ë† andâ–³Ë† inEq. 9.
CI CI Xâ†’Y Yâ†’X
Accordingtotheabovetheorem, withthetotalsamplesizenandtheglobalcovariancetensorC
T
attheserver,itissufficienttoconductFCITandFICPintheFCDprocedures. Moredetailsabout
skeletondiscoveryanddirectiondeterminationruleswillbegiveninAppendixA6.
3.4 COMMUNICATIONCOSTSANDSECURECOMPUTATIONS
Weproposetoconstructsummarystatisticswithoutdirectlysharingtherawdata,whichhasalready
preservedthedataprivacytosomeextent. TheoriginalsamplesizeofrawdataisinRnÃ—dâ€²,where
7PublishedasaconferencepaperatICLR2024
Figure3:ResultsofsyntheticdatasetonlinearGaussianmodel. Byrows,weevaluatevaryingnum-
berofvariablesd, varyingnumberofclientsK, andvaryingnumberofsamplesn . Bycolumns,
k
weevaluateSkeletonF (â†‘),SkeletonSHD(â†“),DirectionF (â†‘)andDirectionSHD(â†“).
1 1
we assume n â‰« dâ€²,h. The constructed covariance tensor is in dimension Rdâ€²Ã—dâ€²Ã—hÃ—h, which
couldsignificantlyreducethecommunicationcostswhenthesamplesizenislargeenoughandthe
hiddendimensionhissmallenough. Furthermore,ifeachclientisrequiredtonotdirectlysharethe
localsummarystatistics,onecanincorporatesomestandardsecurecomputationtechniques,suchas
secure multiparty computation (Cramer et al., 2015), which allows different clients to collectively
computeafunctionovertheirinputswhilekeepingthemprivate,orhomomorphicencryption(Acar
et al., 2018), which enables complex mathematical operations to process encrypted data without
compromising the encryption. Please refer to Goryczka & Xiong (2015) for more about secure
computation. Itisworthnotingthatsomesecurecomputationtechniquescanintroducesignificant
computationoverhead.Tofurtherenhanceprivacyprotectionandcomputationalefficiency,itwould
bebeneficialtofurtherimproveourproposedmethodandweleaveitforfutureexplorations.
4 EXPERIMENTS
Toevaluatetheefficacyofourproposedmethod,weconductextensiveexperimentsonbothsynthetic
and real-world datasets. For the synthetic datasets, we consider the linear Gaussian model and
general functional model to show that our method can handle arbitrary functional causal models.
We ensure that all synthetic datasets have some changing causal models, meaning that they are
heterogeneousdata. Toshowthewideapplicabilityofourmethod,weruntworeal-worlddatasets,
fMRIHippocampus(Poldracketal.,2015)andHKStockMarketdatasets(Huangetal.,2020).
SyntheticDatasets. ThetrueDAGsaresimulatedbyErdoÂ¨s-ReÂ´nyimodel(ErdoËsetal.,1960)with
the number of edges equal to the number of variables. We randomly select 2 variables out of d
variablestobechangingacrossclients.Forthechangingcausalmodel,wegenerateaccordingtothe
SCM:V =(cid:80) ÏƒË†kf (V )+Î³Ë†kÏµ ,whereV âˆˆPA isthedirectcauseofV . Thecausalstrength
i VjâˆˆPAi ij i j i j i i
ÏƒË†k andtheparameterÎ³Ë†k changeacrossdifferentclientwithindexk,whichareuniformlysampled
ij
from U(0.5,2.5) and U(1,3), respectively. We separately generate the data for each domain with
different causal models and then combine them together. For the fixed causal model, we generate
(cid:80)
according to V = ÏƒË† f (V )+Ïµ . We consider the linear Gaussian model with non-equal
i VjâˆˆPAi ij i j i
8PublishedasaconferencepaperatICLR2024
noise variances and the general functional model. For linear Gaussian model, f (V )=V and Ïµ
i j j i
aresampledfromGaussiandistributionwithanon-equalvariancewhichisuniformlysampledfrom
U(1,2). For general functional model, f is randomly chosen from linear, square, sinc, and tanh
i
functions,andÏµ followsuniformdistributionU(âˆ’0.5,0.5)orGaussiandistributionN(0,1).
i
WecompareourFedCDHmethodwithotherFCDbaselines,suchasNOTEARS-ADMM(forlinear
case) (Ng & Zhang, 2022), NOTEARS-MLP-ADMM (for non-linear case) (Ng & Zhang, 2022),
FedDAG (Gao et al., 2022) and FedPC (Huang et al., 2022). We consider these baselines mainly
becauseoftheirpubliclyavailableimplementations. Weevaluateboththeundirectedskeletonand
the directed graph, denoted by â€œSkeletonâ€ and â€œDirectionâ€ in the Figures. We use the structural
Hammingdistance(SHD),F score,precision,andrecallasevaluationcriteria.Weevaluatevariable
1
d âˆˆ {6,12,18,24,30} while fixing other variables such as K=10 and n =100. We set client
k
Kâˆˆ{2,4,8,16,32} while fixing others such as d=6 and n =100. We let the sample size in one
k
clientn âˆˆ{25,50,100,200,400}whilefixingothervariablessuchasd=6andK=10. Following
k
the setting of previous works such as (Ng & Zhang, 2022), we set the sample size of each client
to be equal, although our method can handle both equal and unequal sample size per client. For
each setting, we run 10 instances with different random seeds and report the means and standard
deviations. TheresultsofF scoreandSHDaregiveninFigure3andFigureA3fortwomodels,
1
where our FedCDH method generally outperforms the other methods. Although we need large
sample properties in the proof of Theorem 4, in practice we only have finite samples. According
to the experiment of varying samples, we can see that with more samples the performance of our
method is getting better. More analysis including the implementation details, the results of the
precisionandrecall,theanalysisofcomputationaltime,andthehyperparameterstudy,thestatistical
significancetest,andtheevaluationongraphdensityareprovidedinAppendixA7.
Real-worldDatasets. Weevaluateourmethodandthebaselinesontworeal-worlddataset,fMRI
Hippocampus(Poldracketal.,2015)andHKStockMarketdatasets(Huangetal.,2020). (i)fMRI
Hippocampus dataset contains signals from d=6 separate brain regions: perirhinal cortex (PRC),
parahippocampalcortex(PHC),entorhinalcortex(ERC),subiculum(Sub),CA1,andCA3/Dentate
Gyrus (DG) in the resting states on the same person in 84 successive days. The records for each
daycanberegardedasonedomain,andthereare518samplesforeachdomain. Weselectn =100
k
samplesforeachdayandselectKâˆˆ{4,8,16,32,64}daysforevaluatingvaryingnumberofclients.
WeselectK=10daysandselectn âˆˆ{25,50,100,200,400}samplesforevaluatingvaryingnumber
k
ofsamples. (ii)HKStockMarketdatasetcontainsd=10majorstocksinHongKongstockmarket,
which records the daily closing prices from 10/09/2006 to 08/09/2010. Here one day can be also
seen as one domain. We set the number of clients to be Kâˆˆ{2,4,6,8,10} while randomly select
n =100 samples for each client. All other settings are following previous one by default. More
k
datasetinformation,implementationdetails,resultsandanalysisareprovidedinAppendixA8.
5 DISCUSSION AND CONCLUSION
Discussion. (i)Strengths: Firstofall,byformulatingoursummarystatistics,therequirementfor
communication between the server and clients is restricted to only one singular instance, thereby
substantially reducing the communication times. This is a marked improvement over other base-
linemethodsthatnecessitateiterativecommunications. Additionally, theutilizationofasurrogate
variable enhances our capability to handle heterogeneous data. Furthermore, leveraging the non-
parametriccharacteristicsofourproposedFCITandFICP,ourFedCDHmethodcanadeptlymanage
arbitraryfunctionalcausalmodels. (ii)Limitations: Firstly,theefficiencyofoursummarystatistics
inreducingcommunicationcostsmaynotbeconsiderablewhenthesamplesizenissmallorthehid-
dendimensionhislarge. Secondly,ourmethodisdesignedspecificallyforhorizontally-partitioned
federateddata,henceitcannotbedirectlyappliedtovertically-partitionedfederateddata.
Conclusion. Thispaperhasputforthanovelconstraint-basedfederatedcausaldiscoverymethod
called FedCDH, demonstrating broad applicability across arbitrary functional causal models and
heterogeneous data. We construct the summary statistics as a stand-in for raw data, ensuring the
protectionofdataprivacy. WefurtherproposeFCITandFICPforskeletondiscoveryanddirection
determination. The extensive experiments, conducted on both synthetic and real-world datasets,
underscorethesuperiorperformanceofourmethodoverotherbaselinemethods.Forfutureresearch,
wewillenhanceourmethodtoaddressmorecomplexscenarios,suchasvertically-partitioneddata.
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGEMENT
This project is partially supported by NSF Grant 2229881, the National Institutes of Health
(NIH)underContractR01HL159805,andgrantsfromAppleInc.,KDDIResearchInc., QurisAI,
MBZUAI,andInfiniteBrainTechnology.
REFERENCES
Amin Abyaneh, Nino Scherrer, Patrick Schwab, Stefan Bauer, Bernhard SchoÂ¨lkopf, and Arash
Mehrjou. Fed-cd: Federatedcausaldiscoveryfrominterventionalandobservationaldata. arXiv
preprintarXiv:2211.03846,2022.
Abbas Acar, Hidayet Aksu, A Selcuk Uluagac, and Mauro Conti. A survey on homomorphic en-
cryption schemes: Theory and implementation. ACM Computing Surveys (Csur), 51(4):1â€“35,
2018.
MuhammadArshadandMariusCSilaghi. Distributedsimulatedannealing. DistributedConstraint
ProblemSolvingandReasoninginMulti-AgentSystems,112,2004.
MarcusBendtsen. Regimeawarelearning. InConferenceonProbabilisticGraphicalModels, pp.
1â€“12.PMLR,2016.
StephenBoyd,NealParikh,EricChu,BorjaPeleato,JonathanEckstein,etal. Distributedoptimiza-
tionandstatisticallearningviathealternatingdirectionmethodofmultipliers. Foundationsand
TrendsÂ®inMachinelearning,3(1):1â€“122,2011.
Vince D Calhoun, Robyn Miller, Godfrey Pearlson, and Tulay AdalÄ±. The chronnectome: time-
varyingconnectivitynetworksasthenextfrontierinfmridatadiscovery. Neuron,84(2):262â€“274,
2014.
Chandler Squires. causaldag: creation, manipulation, and learning of causal models.
https://github.com/uhlerlab/causaldag,2018.
RongChen,KrishnamoorthySivakumar,andHillolKargupta. Learningbayesiannetworkstructure
fromdistributeddata.InProceedingsofthe2003SIAMInternationalConferenceonDataMining,
pp.284â€“288.SIAM,2003.
DavidMaxwellChickering.Optimalstructureidentificationwithgreedysearch.Journalofmachine
learningresearch,3(Nov):507â€“554,2002.
RonaldCramer,IvanBjerreDamgaËšrd,etal. Securemultipartycomputation. CambridgeUniversity
Press,2015.
JJDaudin. Partialassociationmeasuresandanapplicationtoqualitativeregression. Biometrika,67
(3):581â€“590,1980.
DoritDorandMichaelTarsi. Asimplealgorithmtoconstructaconsistentextensionofapartially
orientedgraph. TechnicialReportR-185,CognitiveSystemsLaboratory,UCLA,1992.
PaulErdoËs,AlfreÂ´dReÂ´nyi,etal. Ontheevolutionofrandomgraphs. Publ.Math.Inst.Hung.Acad.
Sci,5(1):17â€“60,1960.
KenjiFukumizu,ArthurGretton,XiaohaiSun,andBernhardSchoÂ¨lkopf. Kernelmeasuresofcondi-
tionaldependence. Advancesinneuralinformationprocessingsystems,20,2007.
ErdunGao,JunjiaChen,LiShen,TongliangLiu,MingmingGong,andHowardBondell. Feddag:
Federateddagstructurelearning. TransactionsonMachineLearningResearch,2022.
Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions
with differential privacy. IEEE transactions on dependable and secure computing, 14(5):463â€“
477,2015.
10PublishedasaconferencepaperatICLR2024
Kui Xiang Gou, Gong Xiu Jun, and Zheng Zhao. Learning bayesian network structure from dis-
tributed homogeneous data. In Eighth acis international conference on software engineering,
artificialintelligence,networking,andparallel/distributedcomputing(snpd2007),volume3,pp.
250â€“254.IEEE,2007.
Arthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard SchoÂ¨lkopf, and Alex Smola. A
kernel statistical test of independence. Advances in neural information processing systems, 20,
2007.
Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard SchoÂ¨lkopf. Nonlinear
causaldiscoverywithadditivenoisemodels. Advancesinneuralinformationprocessingsystems,
21,2008.
BiweiHuang,KunZhang,andBernhardSchoÂ¨lkopf. Identificationoftime-dependentcausalmodel:
Agaussianprocesstreatment. InTwenty-Fourthinternationaljointconferenceonartificialintel-
ligence,2015.
Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero, Clark Glymour,
andBernhardSchoÂ¨lkopf. Causaldiscoveryfromheterogeneous/nonstationarydata. TheJournal
ofMachineLearningResearch,21(1):3482â€“3534,2020.
Jianli Huang, Kui Yu, Xianjie Guo, Fuyuan Cao, and Jiye Liang. Towards privacy-aware causal
structurelearninginfederatedsetting. arXivpreprintarXiv:2211.06919,2022.
BrianKidd,KunboWang,YanxunXu,andYangNi. Federatedlearningforsparsebayesianmod-
els with applications toelectronic health records and genomics. In PACIFIC SYMPOSIUM ON
BIOCOMPUTING 2023: Kohala Coast, Hawaii, USA, 3â€“7 January 2023, pp. 484â€“495. World
Scientific,2022.
PhillipLippe,TacoCohen,andEfstratiosGavves.Efficientneuralcausaldiscoverywithoutacyclic-
ityconstraints. InternationalConferenceonLearningRepresentations,2022.
Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, and Bernhard SchoÂ¨lkopf. Amortized
inferenceforcausalstructurelearning. AdvancesinNeuralInformationProcessingSystems,35:
13104â€“13118,2022.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InArtificialintelli-
genceandstatistics,pp.1273â€“1282.PMLR,2017.
OsmanMian,DavidKaltenpoth,MichaelKamp,andJillesVreeken. Nothingbutregretsâ€”privacy-
preservingfederatedcausaldiscovery. InInternationalConferenceonArtificialIntelligenceand
Statistics,pp.8263â€“8278.PMLR,2023.
Yongchan Na and Jihoon Yang. Distributed bayesian network structure learning. In 2010 IEEE
InternationalSymposiumonIndustrialElectronics,pp.1607â€“1611.IEEE,2010.
IgnavierNgandKunZhang.Towardsfederatedbayesiannetworkstructurelearningwithcontinuous
optimization.InInternationalConferenceonArtificialIntelligenceandStatistics,pp.8095â€“8111.
PMLR,2022.
IgnavierNg,SeÂ´bastienLachapelle,NanRosemaryKe,SimonLacoste-Julien,andKunZhang. On
the convergence of continuous constrained optimization for structure learning. In International
ConferenceonArtificialIntelligenceandStatistics,2022.
Ignavier Ng, Biwei Huang, and Kun Zhang. Structure learning with continuous optimization: A
soberlookandbeyond. arXivpreprintarXiv:2304.02146,2023.
AnaRitaNogueira,JoaËœoGama,andCarlosAbreuFerreira. Causaldiscoveryinmachinelearning:
Theoriesandapplications. JournalofDynamics&Games,8(3):203,2021.
JonasPetersandPeterBuÂ¨hlmann. Identifiabilityofgaussianstructuralequationmodelswithequal
errorvariances. Biometrika,101(1):219â€“228,2014.
11PublishedasaconferencepaperatICLR2024
RussellAPoldrack, TimothyOLaumann, OluwasanmiKoyejo, BrendaGregory, AshleighHover,
Mei-YenChen,KrzysztofJGorgolewski,JeffreyLuci,SungJunJoo,RyanLBoyd,etal. Long-
term neural and physiological phenotyping of a single human. Nature communications, 6(1):
8885,2015.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in
neuralinformationprocessingsystems,20,2007.
AlexanderReisach,ChristofSeiler,andSebastianWeichwald.BewareofthesimulatedDAG!causal
discovery benchmarks may be easy to game. In Advances in Neural Information Processing
Systems,2021.
BasilSaeed,SnigdhaPanigrahi,andCarolineUhler. Causalstructurediscoveryfromdistributions
arisingfrommixturesofdags.InInternationalConferenceonMachineLearning,pp.8336â€“8345.
PMLR,2020.
SaeedSametandAliMiri.Privacy-preservingbayesiannetworkforhorizontallypartitioneddata.In
2009InternationalConferenceonComputationalScienceandEngineering,volume3,pp.9â€“16.
IEEE,2009.
XinpengShen,SisiMa,PrashanthiVemuri,andGyorgySimon. Challengesandopportunitieswith
causal discovery algorithms: application to alzheimerâ€™s pathophysiology. Scientific reports, 10
(1):1â€“12,2020.
ShoheiShimizu, PatrikOHoyer, AapoHyvaÂ¨rinen, AnttiKerminen, andMichaelJordan. Alinear
non-gaussianacyclicmodelforcausaldiscovery. JournalofMachineLearningResearch,7(10),
2006.
Peter Spirtes. An anytime algorithm for causal inference. In International Workshop on Artificial
IntelligenceandStatistics,pp.278â€“285.PMLR,2001.
PeterSpirtesandKunZhang. Causaldiscoveryandinference: conceptsandrecentmethodological
advances. InAppliedinformatics,volume3,pp.1â€“28.SpringerOpen,2016.
PeterSpirtes, ClarkNGlymour, RichardScheines, andDavidHeckerman. Causation, prediction,
andsearch. MITpress,2000.
EricVStrobl, KunZhang, andShyamVisweswaran. Approximatekernel-basedconditionalinde-
pendencetestsforfastnon-parametriccausaldiscovery. JournalofCausalInference,7(1),2019.
Danica J Sutherland and Jeff Schneider. On the error of random fourier features. arXiv preprint
arXiv:1506.02785,2015.
Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, and Cheng Zhang. Neuropathic pain
diagnosis simulator for causal discovery algorithm evaluation. Advances in Neural Information
ProcessingSystems,32,2019.
Matej VukovicÂ´ and Stefan Thalmann. Causal discovery in manufacturing: A structured literature
review. JournalofManufacturingandMaterialsProcessing,6(1):10,2022.
ZhaoyuWang,PingchuanMa,andShuaiWang. Towardspracticalfederatedcausalstructurelearn-
ing. arXivpreprintarXiv:2306.09433,2023.
DennisWei,TianGao,andYueYu. DAGswithnofears: Acloserlookatcontinuousoptimization
forlearningBayesiannetworks. InAdvancesinNeuralInformationProcessingSystems,2020.
RobertFWoolson. Wilcoxonsigned-ranktest. Wileyencyclopediaofclinicaltrials,pp.1â€“3,2007.
Eric P Xing, Wenjie Fu, and Le Song. A state-space mixed membership blockmodel for dynamic
networktomography. TheAnnalsofAppliedStatistics,4(2):535â€“566,2010.
Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu. Federated learning.
SynthesisLecturesonArtificialIntelligenceandMachineLearning,13(3):1â€“207,2019.
12PublishedasaconferencepaperatICLR2024
Qiaoling Ye, Arash A Amini, and Qing Zhou. Distributed learning of generalized linear causal
networks. arXivpreprintarXiv:2201.09194,2022.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural
networks. InInternationalConferenceonMachineLearning,pp.7154â€“7163.PMLR,2019.
YueYu,TianGao,NaiyuYin,andQiangJi. Dagswithnocurl: Anefficientdagstructurelearning
approach. InInternationalConferenceonMachineLearning,pp.12156â€“12166.PMLR,2021.
Kun Zhang and Lai-Wan Chan. Extensions of ica for causality discovery in the hong kong stock
market. InInternationalConferenceonNeuralInformationProcessing, pp.400â€“409.Springer,
2006.
KunZhangandAapoHyvarinen. Ontheidentifiabilityofthepost-nonlinearcausalmodel. arXiv
preprintarXiv:1205.2599,2012.
Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard SchoÂ¨lkopf. Kernel-based conditional
independencetestandapplicationincausaldiscovery. arXivpreprintarXiv:1202.3775,2012.
Kun Zhang, Biwei Huang, Jiji Zhang, Bernhard SchoÂ¨lkopf, and Clark Glymour. Discovery and
visualizationofnonstationarycausalmodels. arXivpreprintarXiv:1509.08056,2015.
XunZheng,BryonAragam,PradeepKRavikumar,andEricPXing.Dagswithnotears:Continuous
optimization for structure learning. Advances in Neural Information Processing Systems, 31,
2018.
FangtingZhou, KejunHe, andYangNi. Causaldiscoverywithheterogeneousobservationaldata.
InUncertaintyinArtificialIntelligence,pp.2383â€“2393.PMLR,2022.
13PublishedasaconferencepaperatICLR2024
Appendixfor
â€œFederatedCausalDiscoveryfromHeterogeneousDataâ€
Appendixorganization:
A1SummaryofSymbols 15
A2RelatedWorks 15
A3DetailsabouttheCharacterization 16
A3.1 CharacterizationofConditionalIndependence . . . . . . . . . . . . . . . . . . . . 16
A3.2 CharacterizationofIndependentChange . . . . . . . . . . . . . . . . . . . . . . . 18
A4Proofs 18
A4.1 ProofofLemma3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A4.2 ProofofTheorem4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A4.3 ProofofTheorem5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
A4.4 ProofofTheorem6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A4.5 ProofofLemma7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A4.6 ProofofTheorem8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
A5DetailsaboutFederatedUnconditionalIndependenceTest 24
A5.1 NullHypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A5.2 NullDistributionApproximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A6DetailsaboutSkeletonDiscoveryandDirectionDetermination 25
A6.1 SkeletonDiscovery. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
A6.2 DirectionDetermination. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A7DetailsabouttheExperimentsonSyntheticDatasets 28
A7.1 ImplementationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
A7.2 AnalysisofF andSHD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1
A7.3 ResultsofPrecisionandRecall . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A7.4 ResultsofComputationalTime . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A7.5 HyperparameterStudy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A7.6 StatisticalSignificanceTest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A7.7 EvaluationonDenseGraph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A7.8 Evaluationonthepowerofconditionalindependencetest . . . . . . . . . . . . . . 32
A7.9 Evaluationontheorderofdomainindices . . . . . . . . . . . . . . . . . . . . . . 32
A8DetailsabouttheExperimentsonReal-worldDataset 33
14PublishedasaconferencepaperatICLR2024
A8.1 DetailsaboutfMRIHippocampusDataset . . . . . . . . . . . . . . . . . . . . . . 33
A8.2 DetailsaboutHKStockMarketDataset . . . . . . . . . . . . . . . . . . . . . . . 34
A1 SUMMARY OF SYMBOLS
Inordertoimprovethereadabilityofourpaper,wesummarizethemostimportantsymbolsandtheir
meaningsthroughoutthepaper,asshowninTableA1.
TableA1: Summaryofsymbols
Symbol Meaning Symbol Meaning
d thenumberofobservedvariables. n thetotalsamplesizeofallclients.
K thenumberofclients. V thedatamatrix,VâˆˆRnÃ—d.
k theclientindex,kâˆˆ{1,...,K}. nk thesamplesizeofk-thclient.
â„§ domainindex,â„§âˆˆ{1,...,K}. Vi thei-thvariable,iâˆˆ{1,...,d}.
fi(Â·) thecausalfunctionofvariableVi. PAi theparentsofVi.
Ïµi thenoisetermofVi. ÏˆËœ asetofâ€œpseudoconfoundersâ€.
Î¸i theeffectiveparameterofVi. L thenumberofâ€œpseudoconfoundersâ€.
G,Gobs thecausalgraphwithdvariables. Gaug theaugmentedgraphwithd+1variables.
X,Y,Z asetofrandomvariables. kX,kY,kZ thepositivedefinitekernels.
X,Y,Z thedomainsforthevariables. HX,HY,HZ thereproducingkernelHilbertspaces.
XÂ¨ XÂ¨â‰œ(X,Z). â–³Ë† thenormalizedHSIC.
Î£ thecross-covarianceoperatorininfinitedimension. C thecross-covariancematrixinfinitedimension.
h thenumberofhiddenfeatures/mappingfunctions. f(Â·),g(Â·),q(Â·) themappingfunctions.
F thefunctionspaces. u,v theregressioncoefficients.
AÂ¨ AÂ¨=f(XÂ¨)âˆˆRnÃ—h. B B=g(Y)âˆˆRnÃ—h.
Î³ theridgeparameter. I theidentitymatrix.
KËœ thecentralizedkernelmatrix. H thematrixforcentralization,H=Iâˆ’111T.
n
TCI theteststatisticforconditionalindependence. R theresidualforridgeregression.
Î± thestandardGaussianvariable. Î±2 theÏ‡2variable.
1
Î» thenonzeroeigenvalues. TË† CI theasymptoticstatistic.
kË†,Î¸Ë† theparametersforGammadistributionÎ“(kË†,Î¸Ë†). Câˆ— thespecially-designedcovariancematrix.
tr(Â·) thetraceoperator. dâ€² dâ€²=d+1(plusonesurrogatevariable).
CT globalcovariancetensor,CT âˆˆRdâ€²Ã—dâ€²Ã—hÃ—h. CTk localcovariancetensor,CTkâˆˆRdâ€²Ã—dâ€²Ã—hÃ—h.
w thecoefficientsforrandomfeatures. b theinterceptsforrandomfeatures.
A2 RELATED WORKS
CausalDiscovery. Ingeneral,therearemainlythreecategoriesofmethodsforcausaldiscovery
(CD) from observed data (Spirtes & Zhang, 2016): constraint-based methods, score-based meth-
ods and function-based methods. Constraint-based methods utilize the conditional independence
test (CIT) to learn a skeleton of the directed acyclic graph (DAG), and then orient the edges upon
the skeleton. Such methods contain Peter-Clark (PC) algorithm (Spirtes & Zhang, 2016) and Fast
CausalInference(FCI)algorithm(Spirtes,2001). SometypicalCITmethodsincludekernel-based
independentconditionaltest(Zhangetal.,2012)andapproximatekernel-basedconditionalindepen-
denttest(Strobletal.,2019). Score-basedmethodsuseascorefunctionandagreedysearchmethod
tolearnaDAGwiththehighestscorebysearchingallpossibleDAGsfromthedata,suchasGreedy
Equivalent Search (GES) (Chickering, 2002). Within the score-based category, there is a continu-
ousoptimization-basesubcategoryattractingincreasingattention. NOTEARS(Zhengetal.,2018)
firstly reformulates the DAG learning process as a continuous optimization problem and solves it
using gradient-based method. NOTEARS is designed under the assumption of the linear relations
betweenvariables. SubsequentworkshaveextendedNOTEARStohandlenonlinearcasesviadeep
neural networks, such as DAG-GNN (Yu et al., 2019) and DAG-NoCurl (Yu et al., 2021). ENCO
(Lippeetal.,2022)presentsanefficientDAGdiscoverymethodfordirectedacycliccausalgraphs
utilizingbothobservationalandinterventionaldata. AVCI(Lorchetal.,2022)inferscausalstruc-
ture by performing amortized variational inference over an arbitrary data-generating distribution.
Thesecontinuous-optimization-basedmethodsmightsufferfromvarioustechnicalissues,including
convergence (Wei et al., 2020; Ng et al., 2022), nonconvexity (Ng et al., 2023), and sensitivity to
15PublishedasaconferencepaperatICLR2024
datastandardization(Reisachetal.,2021). Function-basedmethodsrelyonthecausalasymmetry
property, includingthelinearnon-Gaussionmodel(LiNGAM)(Shimizuetal.,2006), theadditive
noisemodel(Hoyeretal.,2008),andthepost-nonlinearcausalmodel(Zhang&Hyvarinen,2012).
Causal Discovery from Heterogeneous Data. Most of the causal discovery methods mentioned
aboveusuallyassumethatthedataisindependentlyandidenticallydistributed(i.i.d.). However,in
practical scenarios, distribution shift is possibly occurring across datasets, which can be changing
acrossdifferentdomainsorovertime,asfeaturedbyheterogeneousornon-stationarydata(Huang
etal.,2020). Totackletheissueofchangingcausalmodels, onemaytrytofindcausalmodelson
slidingwindowsfornon-stationarydata(Calhounetal.,2014),andthencomparethem. Improved
versionsincludetheregimeawarelearningalgorithmtolearnasequenceofBayesiannetworksthat
model a system with regime changes (Bendtsen, 2016). Such methods may suffer from high esti-
mationvarianceduetosamplescarcity, largetypeIIerrors, andalargenumberofstatisticaltests.
Some methods aim to estimate the time-varying causal model by making use of certain types of
smoothnessofthechange(Huangetal.,2015),buttheydonotexplicitlylocatethechangingcausal
modules. Several methods aim to model time-varying time-delayed causal relations (Xing et al.,
2010),whichcanbereducedtoonlineparameterlearningbecausethedirectionofthecausalrela-
tionsisgiven(i.e.,thepastinfluencesthefuture). Moreover,mostofthesemethodsassumelinear
causalmodels,limitingtheirapplicabilitytocomplexproblemswithnonlinearcausalrelations. In
particular, a nonparametric constraint-based method to tackle this causal discovery problem from
non-stationaryorheterogeneousdata,calledCD-NOD(Huangetal.,2020),wasrecentlyproposed,
wherethesurrogatevariablewasintroduced,writtenassmoothfunctionsoftimeordomainindex.
Thefirstmodel-basedmethodwasproposedforheterogeneousdatainthepresenceofcycliccausal-
ityandconfounders,namedCHOD(Zhouetal.,2022). Saeedetal. (Saeedetal.,2020)provideda
graphicalrepresentationviathemixtureDAGofdistributionsthatariseasmixturesofcausalDAGs.
FederatedCausalDiscovery. Atwo-stepprocedurewasadopted(Gouetal.,2007)tolearnaDAG
from horizontally partitioned data, which firstly estimated the structures independently using each
clientâ€™s local dataset, and secondly applied further conditional independence test. Instead of us-
ing statistical test in the second step, a voting scheme was used to pick those edges identified by
more than half of the clients (Na & Yang, 2010). These methods leverage only the final graphs
independently estimated from each local dataset, which may lead to suboptimal performance as
the information exchange may be rather limited. Furthermore, (Samet & Miri, 2009) developed a
privacy-preservingmethodbasedonsecuremultipartycomputation,butwaslimitedtothediscrete
case. Forverticallypartitioneddata,(Yangetal.,2019)constructedanapproximationtothescore
functioninthediscretecaseandadoptedsecuremultipartycomputation. (Chenetal.,2003)devel-
opedafour-stepprocedurethatinvolvestransmittingasubsetofsamplesfromeachclienttoacentral
site, which may lead to privacy concern. NOTEARS-ADMM (Ng & Zhang, 2022) and Fed-DAG
(Gaoetal.,2022)wereproposedforthefederatedcausaldiscovery(FCD)basedoncontinuousopti-
mizationmethods.Fed-PC(Huangetal.,2022)wasdevelopedasafederatedversionofclassicalPC
algorithm,however,itwasdevelopedforhomogeneousdata,whichmayleadtopoorperformanceon
heterogeneousdata. DARLIS(Yeetal.,2022)utilizesthedistributedannealing(Arshad&Silaghi,
2004)strategytosearchfortheoptimalgraph,whilePERI(Mianetal.,2023)aggregatestheresults
of the local greedy equivalent search (GES) (Chickering, 2002) and chooses the worst-case regret
for each iteration. Fed-CD (Abyaneh et al., 2022) was proposed for both observational and inter-
ventional data based on continuous optimization. FEDC2SL (Wang et al., 2023) extended Ï‡2 test
tothefederatedversion, however, thismethodisrestrictiveondiscretevariablesandthereforenot
applicableforanycontinuousvariables. Noticethatmostoftheseabove-mentionedmethodsheav-
ily rely on either identifiable functional causal models or homogeneous data distributions. These
assumptions may be overly restrictive and difficult to be satisfied in real-world scenarios, limiting
theirdiverseapplicability.
A3 DETAILS ABOUT THE CHARACTERIZATION
A3.1 CHARACTERIZATIONOFCONDITIONALINDEPENDENCE
Inthissection,wewillprovidemoredetailsabouttheinterpretationofÎ£ asformulatedinEq.
XÂ¨Y|Z
13,thedefinitionofcharacteristickernelasshowninLemma9,whichishelpfultounderstandthe
16PublishedasaconferencepaperatICLR2024
Lemma1inthemainpaper. Wethenprovidetheuncorrelatedness-basedcharacterizationofCIin
Lemma10.
Firstofall,fortherandomvector(X,Y)onX Ã—Y,thecross-covarianceoperatorfromH toH
Y X
isdefinedbytherelation
âŸ¨f,Î£ gâŸ© =E [f(X)g(Y)]âˆ’E [f(X)]E [g(Y)], (11)
XY HX XY X Y
forallf âˆˆH andg âˆˆH . Furthermore,wedefinethepartialcross-covarianceoperatoras
X Y
Î£ =Î£ âˆ’Î£ Î£âˆ’1Î£ . (12)
XY|Z XY XZ ZZ ZY
If Î£ is not invertible, use the right inverse instead of the inverse. We can intuitively interpret
ZZ
theoperatorÎ£ asthepartialcross-covariancebetween{f(X),âˆ€fâˆˆH }and{g(Y),âˆ€gâˆˆH }
XY|Z X Y
given{q(Z),âˆ€qâˆˆH }.
Z
Lemma 9 (Characteristic Kernel (Fukumizu et al., 2007)). A kernel K is characteristic, if the
X
conditionE Xâˆ¼P X[f(X)]=E Xâˆ¼Q X[f(X)](âˆ€fâˆˆH X)impliesP X=Q X,whereP X andQ X aretwo
probabilitydistributionsofX. GaussiankernelandLaplaciankernelarecharacteristickernels.
AsshowninLemma1,ifweusecharacteristickernelanddefineXÂ¨ â‰œ(X,Z),thecharacterization
ofCIcouldberelatedtothepartialcross-covarianceasÎ£ =0 â‡â‡’ X âŠ¥âŠ¥Y|Z,where
XÂ¨Y|Z
Î£ =Î£ âˆ’Î£ Î£âˆ’1Î£ . (13)
XÂ¨Y|Z XÂ¨Y XÂ¨Z ZZ ZY
Similarly, we can intuitively interpret the operator Î£ as the partial cross-covariance between
XÂ¨Y|Z
{f(XÂ¨),âˆ€fâˆˆH }and{g(Y),âˆ€gâˆˆH }given{q(Z),âˆ€qâˆˆH }.
XÂ¨ Y Z
Based on Lemma 1, we further consider a different characterization of CI which enforces the
uncorrelatedness of functions in suitable spaces, which may be intuitively more appealing. De-
note the probability distribution of X as P and the joint distribution of (X,Y) as P . Let
X XY
L2 be the space of square integrable functions of X and L2 be that of (X,Y). Specifically,
X XY
L2 ={f(X)|E(f2)<âˆ},andlikewiseforL2 . Particularly,considerthefollowingconstrained
X XY
L2spaces:
S â‰œ{f âˆˆL2 |E(f|Z)=0},
XÂ¨ XÂ¨
S â‰œ{g âˆˆL2 |E(g|Z)=0}, (14)
YÂ¨ YÂ¨
Sâ€² â‰œ{gâ€²|gâ€² =g(Y)âˆ’E(g|Z),g âˆˆL2}.
Y|Z Y
TheycanbeconstructedfromthecorrespondingL2spacesvianonlinearregression. Fromexample,
foranyfunctionf âˆˆL2 ,thecorrespondingfunctionfâ€²isgivenby:
XZ
fâ€²(XÂ¨)=f(XÂ¨)âˆ’E(f|Z)=f(XÂ¨)âˆ’Î²âˆ—(Z), (15)
f
whereÎ²âˆ—(Z)âˆˆL2 istheregressionfunctionoff(XÂ¨)onZ. Then,wecanthenrelatethedifferent
f Z
characterizationofCIfromLemma1totheuncorrelatednessinthefollowinglemma.
Lemma10(CharacterizationofCIbasedonPartialAssociation(Daudin,1980)). Eachofthefol-
lowingconditionsareequivalenttoX âŠ¥âŠ¥Y|Z
(i.)E(fg)=0,âˆ€f âˆˆS and âˆ€g âˆˆS ,
XÂ¨ YÂ¨
(ii.)E(fgâ€²)=0,âˆ€f âˆˆS and âˆ€gâ€² âˆˆSâ€² ,
XÂ¨ Y|Z
(16)
(iii.)E(fgËœ)=0,âˆ€f âˆˆS and âˆ€gËœâˆˆL2,
XÂ¨ YÂ¨
(iv.)E(fgËœâ€²)=0,âˆ€f âˆˆS and âˆ€gËœâ€² âˆˆL2.
XÂ¨ Y
When(X,Y,Z)arejointlyGaussian,theindependenceisequivalenttotheuncorrelatedness,inother
words,X âŠ¥âŠ¥Y|ZisequivalenttothevanishingofthepartialcorrelationcoefficientÏ . Wecan
XY|Z
regardtheLemma10asasageneralizationofthepartialcorrelationbasedcharacterizationofCI.
17PublishedasaconferencepaperatICLR2024
Forexample,condition(i)meansthatanyâ€residualâ€functionof(X,Z)givenZisuncorrelatedwith
thatof(Y,Z)givenZ.HerewecanobservethesimilaritybetweenLemma1andLemma10,except
theonlydifferencethatLemma10considersallfunctionsinL2spaces,whileLemma1exploitsthe
spacescorrespondingtosomecharacteristickernels. Ifwerestrictthefunctionf andgâ€²incondition
(ii)tothespacesH andH ,respectively,Lemma10isthenreducedtoLemma1.
XÂ¨ Y
BasedonthetwolemmasmentionedaboveplustheLemma1,wecouldfurtherderiveLemma3in
ourmainpaper.
A3.2 CHARACTERIZATIONOFINDEPENDENTCHANGE
InLemma2ofthemainpaper,weprovidetheindependentchangeprinciple(ICP)toevaluatethe
dependencebetweentwochangingcausalmodels. Here,wegivemoredetailsaboutthedefinition
andtheassignedvalueofnormalizedHSIC.Asmallervaluemeansbeingmoreindependent.
Definition1(NormalizedHSIC(Fukumizuetal.,2007)). GivenvariablesU andV,HSICprovides
ameasurefortestingtheirstatisticalindependence. AnestimatorofnormalizedHSICisgivenas
tr(MËœ MËœ )
HSICN = U V , (17)
UV tr(MËœ )tr(MËœ )
U V
where MËœ and MËœ are the centralized Gram matrices, MËœ â‰œ HM H, MËœ â‰œ HM H,
U V U U V V
H = I âˆ’ 111T, I is nÃ—n identity matrix and 1 is vector of n ones. How to construct M
n U
andM willbeexplainedinthecorrespondingcasesbelow. Tocheckwhethertwocausalmodules
V
changeindependentlyacrossdifferentdomains,thedependencebetweenP(X)andP(Y|X)andthe
dependencebetweenP(Y)andP(X|Y)onthegivendatacanbegivenby
tr(MËœ MËœ ) tr(MËœ MËœ )
X Y|X Y X|Y
â–³ = , â–³ = . (18)
Xâ†’Y tr(MËœ )tr(MËœ ) Yâ†’X tr(MËœ )tr(MËœ )
X Y|X Y X|Y
According to CD-NOD (Huang et al., 2020), instead of working with conditional distribution
P(X|Y) and P(Y|X), we could use the â€joint distributionâ€ P(X,Y), which is simpler, for esti-
mation. HereweuseY insteadofY toemphasizethatinthisconstructeddistributionX andY are
notsymmetric. Then,thedependencevalueslistedinEq. 18couldbeestimatedby
tr(MËœ MËœ ) tr(MËœ MËœ )
â–³Ë† = X YX , â–³Ë† = Y XY , (19)
Xâ†’Y tr(MËœ )tr(MËœ ) Yâ†’X tr(MËœ )tr(MËœ )
X YX Y XY
whereMËœ
X
â‰œHM XH,M
X
â‰œÂµË† X|â„§Â·ÂµË†T X|â„§.Similarly,wedefineMËœ Y,M
Y
andÂµË† Y|â„§.According
to(Huangetal.,2020),wehave
ÂµË† X|â„§ â‰œÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§X, (20)
where ÂµË† X|â„§ â‰œ Ï•(â„§)(Câ„§â„§ + Î³I)âˆ’1Câ„§X, ÂµË† X|â„§,Ï•(â„§) âˆˆ RnÃ—h, Î³ is a small ridge parameter, Ï•
represents the feature map, and â„§ is the surrogate variable indicating different domains or clients.
Similarly,wedefineMËœ Y,M
Y
andÂµË† Y|â„§.
ÂµË† Y|â„§ â‰œÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§Y. (21)
Moreover,MËœ
YX
â‰œHM YXH,M
YX
â‰œÂµË† YX|â„§Â·ÂµË†T YX|â„§. Similarly,wedefineMËœ XY,M
XY
and
ÂµË† .
XY
ÂµË† YX|â„§ â‰œÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§,(Y,X)
(22)
ÂµË† XY|â„§ â‰œÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§,(X,Y),
Eq. 19asformulatedaboveishelpfultofurtherderiveTheorem5inourmainpaper.
A4 PROOFS
Here,weprovidetheproofsofthetheoremsandlemmas,includingLemma3,Theorem4,Theorem
5,Theorem6,Lemma7,andTheorem8inourmainpaper.
18PublishedasaconferencepaperatICLR2024
A4.1 PROOFOFLEMMA3
Proof: We define the covariance matrix in the null hypothesis as C = 1 (cid:80)n [(AÂ¨ âˆ’
XÂ¨Y|Z n i=1 i
E(AÂ¨|Z))T(B âˆ’E(B|Z))]whichcorrespondstothepartialcross-covariancematrixwithnsamples,
i
C âˆˆRhÃ—h, AÂ¨=f(XÂ¨)âˆˆRnÃ—h, B=g(Y)âˆˆRnÃ—h, {fj(XÂ¨)|h }âˆˆF , {gj(Y)|h }âˆˆF . Notice
XÂ¨Y|Z j=1 XÂ¨ j=1 Y
thatF andF arefunctionspaces. nandhdenotethenumberoftotalsamplesofallclientsand
XÂ¨ Y
thenumberofhiddenfeaturesormappingfunctions,respectively.
Notice that E(AÂ¨|Z) and E(B|Z) could be non-linear functions of Z which may be difficult to
estimate. therefore, we would like to approximate them with linear functions. Let q(Z)âˆˆRnÃ—h,
{qj(Z)|h }âˆˆF . WecouldestimateE(fj|Z)withtheridgeregressionoutputuTq(Z)underthe
j=1 Z j
mildconditionsgivenbelow.
Lemma 11. (Sutherland & Schneider, 2015) Consider performing ridge regression of fj on Z.
Assume that (i) (cid:80)n fj = 0, fj is defined on the domain of XÂ¨; (ii) the empirical kernel matrix
i=1 i
of Z, denoted by K , only has finite entries (i.e., âˆ¥K âˆ¥ < âˆ); (iii) the range of Z is compact,
Z Z âˆ
Z âŠ‚RdZ. Thenwehave
(cid:104) (cid:105) c
P |EË†(fj|Z)âˆ’uTq(Z)|â‰¥Ïµ â‰¤ 0eâˆ’hÏµ2c1, (23)
j Ïµ2
whereEË†(fj|Z)istheestimateofE(fj|Z)byridgeregression,c andc arebothconstantsthatdo
0 1
notdependonthesamplesizenorthenumberofhiddendimensionsormappingfunctionsh.
Theexponentialratewithrespecttohintheabovelemmasuggestswecanapproximatetheoutput
ofridgeregressionwithasmallnumberofhiddenfeatures. Moreover,wecouldsimilarlyestimate
(cid:104) (cid:105)
E(gj|Z)withvTq(Z),becausewecouldguaranteethatP |EË†(gj|Z)âˆ’vTq(Z)|â‰¥Ïµ â†’ 0forany
j j
fixedÏµ>0atanexponentialratewithrespecttoh.
SimilartotheL2 spacesincondition(ii)ofLemma10,wecanconsiderthefollowingconditionto
approximateconditionalindependence:
E(fËœgËœ)=0,âˆ€fËœâˆˆFËœ and âˆ€gËœâˆˆFËœ , where
XÂ¨|Z Y|Z
FËœ ={fËœ|fËœj =fj âˆ’E(fj|Z),fj âˆˆF }, (24)
XÂ¨|Z XÂ¨
FËœ ={gËœ|gËœj =gj âˆ’E(gj|Z),gj âˆˆF }.
Y|Z Y
AccordingtoEq.23,wecouldestimateE(fj|Z)andE(gj|Z)byuTq(Z)andvTq(Z),respectively.
j j
Thus,wecanreformulatethefunctionspacesas
FËœ ={fËœ|fËœj =fj âˆ’uTq(Z),fj âˆˆF },
XÂ¨|Z j XÂ¨
(25)
FËœ ={gËœ|gËœj =gj âˆ’vTq(Z),gj âˆˆF }.
Y|Z j Y
Proofends. !(â„§) !(â„§)
! "
$(â„§)
â„“
A4.2 PROOFOFTHEOREM4
Proof: Assume"that there are n"i.i.d. samples for
! "
X,Y,Z. LetKËœ bethecentralizedkernelmatrix,
XÂ¨|Z
given by KËœ â‰œRËœ (a) RËœT =HR RT H, #(%)
XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z
where R â‰œfËœ(XÂ¨)=f(XÂ¨)âˆ’uTq(Z) which
XÂ¨|Z
can be seen as the residual after ridge
r Ke Ëœgress â‰œio RËœn.
RËœT
S =im Hila Rrly, RW Te Hcould defi ann de -,
$Ìˆ|"
'()Ìˆ) *(+) -,
!|"
Y|Z Y|Z Y|Z Y|Z Y|Z
R â‰œgËœ(Y)=g(Y)âˆ’(vbT)q(Z). Accordingly,
Y|Z
weletKËœ â‰œRËœ RËœT =HR RT H. We FigureA1: GiventhatX âŠ¥âŠ¥ Y|Z,wecould
XÂ¨Y|Z XÂ¨|Z Y|Z XÂ¨|Z Y|Z
introduce the independence between R
set the test statistic as T =nâˆ¥C âˆ¥2, where XÂ¨|Z
CI XÂ¨Y|Z F andR .
C â‰œRËœT RËœ =1RT HHR . Y|Z
XÂ¨Y|Z XÂ¨|Z Y|Z n XÂ¨|Z Y|Z
19PublishedasaconferencepaperatICLR2024
LetÎ» andÎ» betheeigenvaluesofKËœ and
XÂ¨|Z Y|Z XÂ¨|Z
KËœ , respectively. Furthermore, we define the EVD decomposition KËœ = V Î› VT ,
Y|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z
whereÎ› isthediagonalmatrixcontainingnon-negativeeigenvaluesÎ» .Similarly,wedefine
XÂ¨|Z XÂ¨|Z,i
KËœ = V Î› VT witheigenvaluesÎ» . LetÏˆ = [Ïˆ ,Ïˆ ,...,Ïˆ ] â‰œ
Y|Z Y|Z Y|Z Y|Z Y|Z,i XÂ¨|Z XÂ¨|Z,1 XÂ¨|Z,2 XÂ¨|Z,n
V Î›1/2 andÏ• =[Ï• ,Ï• ,...,Ï• ]â‰œV Î›1/2 .
Y|Z Y|Z Y|Z Y|Z,1 Y|Z,2 Y|Z,n Y|Z Y|Z
On the other hand, consider eigenvalues Î»âˆ— and eigenfunctions u of the kernel k
XÂ¨|Z,i XÂ¨|Z,i XÂ¨|Z
w.r.t. theprobablitymeasurewiththedensityP(xÂ¨), i.e., Î»âˆ— andu satisfy(cid:82) k (xÂ¨,xÂ¨â€²)Â·
XÂ¨|Z,i XÂ¨|Z,i XÂ¨|Z
u (xÂ¨)Â·P(xÂ¨)dxÂ¨ = Î»âˆ— Â·u (xÂ¨â€²), where we assume that u have unit variance, i.e.,
XÂ¨|Z,i XÂ¨|Z,i XÂ¨|Z,i XÂ¨|Z,i
E[u2 (XÂ¨)] = 1. Similarly,wedefinek ,Î»âˆ— ,anduâˆ— . Let{Î± ,...,Î± }denotei.i.d.
XÂ¨|Z,i Y|Z Y|Z,i Y|Z,i 1 n2
standardGaussianvariables,andthus{Î±2,...,Î±2 }denotei.i.d. Ï‡2variables.
1 n2 1
Lemma 12 (Kernel-based Conditional Independence Test (Zhang et al., 2012)). Under the null
hypothesisthatX andY areconditionalindependentgivenZ,wehavethattheteststatisticT â‰œ
CI
1 tr(KËœ KËœ ) have the same asymptotic distribution as TË† â‰œ 1 (cid:80)n2 Î»Ëœ Â· Î±2, where Î»Ëœ
n XÂ¨|Z Y|Z CI n k=1 k k k
are eigenvalues of wwT, w = [w ,...,w ], with the vector w obtained by stacking M =
1 n t t
[Ïˆ (XÂ¨ ),Ïˆ (XÂ¨ ),...,Ïˆ (XÂ¨ )]T Â·[Ï• (Y ),Ï• (Y ),...,Ï• (Y )].
XÂ¨|Z,1 t XÂ¨|Z,2 t XÂ¨|Z,n t Y|Z,1 t Y|Z,2 t Y|Z,n t
Intheabovelemma,theirteststatisticisequivalenttoours,duetothefactthat
1 1
tr(KËœ KËœ )= tr(RËœ (RËœT RËœ RËœT ))
n XÂ¨|Z Y|Z n XÂ¨|Z XÂ¨|Z Y|Z Y|Z
1
= tr((RËœT RËœ RËœT )RËœ )
n XÂ¨|Z Y|Z Y|Z XÂ¨|Z
= 1 âˆ¥RËœT RËœ âˆ¥2 (26)
n XÂ¨|Z Y|Z F
1
= âˆ¥nC âˆ¥2
n XÂ¨Y|Z F
=nâˆ¥C âˆ¥2.
XÂ¨Y|Z F
However,theirasymptoticdistributionisdifferentfromours.Basedontheirasymptoticdistribution,
wecouldgofurther. ThefirsttworowsofEq. 26holdtruebecauseofthecommutativepropertyof
trace,namely,tr(AB) = BA,refertoLemma6formoredetails. Accordingtotheformulationof
RËœ andRËœ ,wehave
XÂ¨|Z Y|Z
(cid:40)
f(XÂ¨)=uTq(Z)+R
XÂ¨|Z
(27)
g(Y)=vTq(Z)+R .
Y|Z
Based on the above formulations, we could easily draw the causal graph as shown in Fig. A1.
In particular, considering that X and Y are conditionally independent given Z, we could further
determinethatR andR areindependent,namely,wehave
XÂ¨|Z Y|Z
X âŠ¥âŠ¥Y|Z â‡â‡’ R âŠ¥âŠ¥R . (28)
XÂ¨|Z Y|Z
As f(XÂ¨) and g(Y) are uncorrelated, then E(w ) = 0. Furthermore, the covariance is Î£ =
t
Cov(w ) = E(w wT),wherew isdefinedinthesamewayasinLemma12. IfR âŠ¥âŠ¥ R ,
t t t XÂ¨|Z Y|Z
fork Ì¸=iorlÌ¸=j,wedenotethenon-diagonal(ND)entriesofÎ£ase ,where
ND
(cid:113)
e =E[ Î»âˆ— Î»âˆ— Î»âˆ— Î»âˆ— u u u u ]
ND XÂ¨|Z,i Y|Z,j XÂ¨|Z,k Y|Z,l XÂ¨|Z,i Y|Z,j XÂ¨|Z,k Y|Z,l
(cid:113)
= Î»âˆ— Î»âˆ— Î»âˆ— Î»âˆ— E[u u ]E[u u ] (29)
XÂ¨|Z,i Y|Z,j XÂ¨|Z,k Y|Z,l XÂ¨|Z,i XÂ¨|Z,k Y|Z,j Y|Z,l
=0.
WethendenotethediagonalentriesofÎ£ase ,where
D
e =Î»âˆ— Î»âˆ— E[u2 ]E[u2 ]
D XÂ¨|Z,i Y|Z,j XÂ¨|Z,i Y|Z,j
(30)
=Î»âˆ— Î»âˆ— ,
XÂ¨|Z,i Y|Z,j
20PublishedasaconferencepaperatICLR2024
which are eigenvalues of Î£. According to (Zhang et al., 2012), 1Î» converge in probability
n XÂ¨|Z,i
Î»âˆ— . Substituting all the results into the asymptotic distribution in Lemma 12, we can get the
XÂ¨|Z
updatedasymptoticdistribution
Î²
TË† â‰œ 1 (cid:88) Î» Î» Î±2 as Î² =nâ†’âˆ. (31)
CI n2 XÂ¨|Z,i Y|Z,j ij
i,j=1
whereÎ² isthenumberofnonzeroeigenvaluesÎ» ofthekernelmatricesKËœ .
XÂ¨|Z XÂ¨|Z
Consequently,T andTË† havethesameasymptoticdistribution. Proofends.
CI CI
A4.3 PROOFOFTHEOREM5
Proof: Firstofall,sinceÎ±2 followtheÏ‡2 distributionwithonedegreeoffreedom,thuswehave
ij
E(Î±2 ) = 1 and Var(Î±2 ) = 2. According to the asymptotic distribution in Theorem 4 and the
ij ij
derivationofLemma7,wehave
E(TË† |D)= 1 (cid:88) Î» Î»
CI n2 XÂ¨|Z,i Y|Z,j
i,j
1 (cid:88) (cid:88)
= Î» Î»
n2 XÂ¨|Z,i Y|Z,j
i j
1
= tr(KËœ )tr(KËœ ) (32)
n2 XÂ¨|Z Y|Z
1
= tr(RËœ RËœT )tr(RËœ RËœT )
n2 XÂ¨|Z XÂ¨|Z Y|Z Y|Z
1
= tr(nÂ·C )tr(nÂ·C )
n2 XÂ¨|Z Y|Z
=tr(C )tr(C ),
XÂ¨|Z Y|Z
where RËœ and RËœ are defined in the proof of Theorem 3 above. Therefore, E(TË† |D) =
XÂ¨|Z Y|Z CI
tr(C )tr(C ).
XÂ¨|Z Y|Z
Furthermore,Î±2 areindependentvariablesacrossiandj,andnoticethattr(KËœ2 )=(cid:80) Î»2 ,
ij XÂ¨|Z i XÂ¨|Z,i
andsimilarlytr(KËœ2 )=(cid:80) Î»2 . BasedontheasymptoticdistributioninTheorem4,wehave
Y|Z i Y|Z,i
Var(TË† |D)= 1 (cid:88) Î»2 Î»2 Var(Î±2 )
CI n4 XÂ¨|Z,i Y|Z,j ij
i,j
2 (cid:88) (cid:88)
= Î»2 Î»2 (33)
n4 XÂ¨|Z,i Y|Z,j
i j
2
= tr(KËœ2 )tr(KËœ2 ).
n4 XÂ¨|Z Y|Z
Additionally,accordingtothesimilarruleasinEq. 26,wehave
tr(KËœ2 )=tr(RËœ RËœT RËœ RËœT )
XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z
=tr(RËœT RËœ RËœT RËœ )
XÂ¨|Z XÂ¨|Z XÂ¨|Z XÂ¨|Z
=âˆ¥RËœT RËœ âˆ¥2 (34)
XÂ¨|Z XÂ¨|Z F
=âˆ¥nÂ·C âˆ¥2
XÂ¨|Z F
=n2âˆ¥C âˆ¥2.
XÂ¨|Z F
21PublishedasaconferencepaperatICLR2024
Similarly, we have tr(KËœ2 ) = n2âˆ¥C âˆ¥2. Substituting the results into the above formula-
Y|Z Y|Z F
tion about variance, we have 2 tr(KËœ2 )tr(KËœ2 ) = 2 Â· n2âˆ¥C âˆ¥2 Â· n2âˆ¥C âˆ¥2. Thus,
n4 XÂ¨|Z Y|Z n4 XÂ¨|Z F Y|Z F
Var(TË† |D)=2Â·âˆ¥C âˆ¥2 Â·âˆ¥C âˆ¥2. Proofends.
CI XÂ¨|Z F Y|Z F
A4.4 PROOFOFTHEOREM6
Proof: According to the above-mentioned formulations, we have MËœ X â‰œ HM XH = ÂµËœË† X|â„§ Â·
ÂµËœË†T X|â„§, ÂµËœË† X|â„§ â‰œ H Â·ÂµË† X|â„§. Basedontherulesofestimatingcovariancematrixfromkernelmatrix
inLemma6,wehave
tr(MËœ X)=tr(ÂµËœË† X|â„§Â·ÂµËœË†T X|â„§)
=tr(ÂµËœË†T X|â„§Â·ÂµËœË† X|â„§) (35)
=tr((HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§X)T(HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§X)) (36)
=tr(C Xâ„§(Câ„§â„§+Î³I)âˆ’1Ï•(â„§)TH Â·HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§X))
1
= ntr(C Xâ„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§X) (37)
1
= tr(Câˆ—). (38)
n X
Eq. 35isobtainedduetothetracepropertyoftheproductofthematrices, asshowninLemma6.
Eq. 36issubstitutingfromEq. 20. HereweuseEq. 38forsimplenotation. Wecanseethatitcan
berepresentedwithsomecombinationsofdifferentcovariancematrices. Similarly,wehave
1 1
tr(MËœ Y)= ntr(C Yâ„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§Y)= ntr(C Yâˆ—). (39)
RegardingthecentralizedGrammatricesforjointdistribution,similarlywehave
1 1
tr(MËœ YX)= ntr(C (Y,X),â„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§,(Y,X))= ntr(C Yâˆ— Ëœ),
(40)
1 1
tr(MËœ XY)= ntr(C (X,Y),â„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§,(X,Y))= ntr(C Xâˆ— Ëœ),
wheretr(MËœ )=tr(MËœ ). Furthermore,basedonLemma6andEq. 22,wehave
YX XY
tr(MËœ XMËœ YX)=tr(ÂµËœË† X|â„§ÂµËœË†T X|â„§Â·ÂµËœË† YX|â„§ÂµËœË†T YX|â„§)
=tr(ÂµËœË†T X|â„§ÂµËœË† YX|â„§ÂµËœË†T YX|â„§Â·ÂµËœË† X|â„§) (41)
=âˆ¥ÂµËœË†T X|â„§ÂµËœË† YX|â„§âˆ¥2
F
=âˆ¥(HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§X)T(HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§,(Y,X))âˆ¥2
F
(42)
=âˆ¥C Xâ„§(Câ„§â„§+Î³I)âˆ’1Ï•(â„§)TH Â·HÏ•(â„§)(Câ„§â„§+Î³I)âˆ’1Câ„§,(Y,X)âˆ¥2
F
1
=âˆ¥ nC Xâ„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§,(Y,X)âˆ¥2
F
(43)
1
= âˆ¥Câˆ— âˆ¥2. (44)
n2 X,YËœ F
Eq. 41isobtainedduetothetracepropertyoftheproductofthematrices, asshowninLemma6.
Eq. 41issubstitutingfromEq. 20andEq. 22. HereweuseEq. 44forsimplenotation. Wecansee
that it can be represented with some combinations of different covariance matrices. Similarly, we
have
1
tr(MËœ YMËœ XY)=âˆ¥ nC Yâ„§(Câ„§â„§+Î³I)âˆ’1Câ„§â„§(Câ„§â„§+Î³I)âˆ’1Câ„§,(X,Y)âˆ¥2
F
(45)
1
= âˆ¥Câˆ— âˆ¥2.
n2 Y,XËœ F
22PublishedasaconferencepaperatICLR2024
SubstitutingtheequationsaboveintoEq. 19,wehave
âˆ¥Câˆ— âˆ¥2 âˆ¥Câˆ— âˆ¥2
â–³Ë† = X,YËœ F , â–³Ë† = Y,XËœ F . (46)
Xâ†’Y tr(Câˆ—)Â·tr(Câˆ—) Yâ†’X tr(Câˆ—)Â·tr(Câˆ—)
X YËœ Y XËœ
Proofends.
A4.5 PROOFOFLEMMA7
Proof: Firstofall, weincorporaterandomFourierfeaturestoapproximatethekernels, because
theyhaveshowncompetitiveperformancestoapproximatethecontinuousshift-invariantkernels.
Lemma 13 (Random Features (Rahimi & Recht, 2007)). For a continuous shift-invariant kernel
K(x,y)onR,wehave:
(cid:90)
K(x,y)= p(w)ejw(xâˆ’y)dw =E [Î¶ (x)Î¶ (y)], (47)
w w w
R
whereÎ¶ (x)Î¶ (y)isanunbiasedestimateofK(x,y)whenwisdrawnfromp(w).
w w
Sinceboththeprobabilitydistributionp(w)andthekernelentryK(x,y)arereal,theintegralinEq.
47 converges when the complex exponentials are replaced with cosines. Therefore, we may get a
real-valuesmappingby:
K(x,y)â‰ˆÏ• (x)TÏ• (y),
w w
(cid:114)
2
Ï• (x)â‰œ [cos(w x+b ),...,cos(w x+b )]T,
w h 1 1 h h (48)
(cid:114)
2
Ï• (y)â‰œ [cos(w y+b ),...,cos(w y+b )]T,
w h 1 1 h h
wherewisdrawnfromp(w)andbisdrawnuniformlyfrom[0,2Ï€]. x,y,w,bâˆˆR,andtherandom-
izedfeaturemapÏ• : R â†’ Rh. Thepreciseformofp(w)reliesonthetypeoftheshift-invariant
w
kernelwewouldliketoapproximate. Hereinthispaper,wechoosetoapproximateGaussiankernel
as one of the characteristic kernels, and thus set the probability distribution p(w) to the Gaussian
one. BasedonEq. 48,wehave
tr(KËœ )â‰ˆtr(Ï•Ëœ (x)Ï•Ëœ (y)T), (49)
x,y w w
wherex,yâˆˆRn,KËœ âˆˆRnÃ—n,Ï•Ëœ (x)âˆˆRnÃ—h isthecentralizedrandomfeature,Ï•Ëœ (x)=HÏ• (x).
x,y w w w
Furthermore,benefitingfromthecommutativepropertyofthetraceoftheproductoftwomatrices,
wehave
tr(Ï•Ëœ (x)Ï•Ëœ (y)T)=tr(Ï•Ëœ (y)TÏ•Ëœ (x)), (50)
w w w w
Since each random feature is centralized, meaning the zero mean for each feature, therefore, we
have:
1 1
tr(Ï•Ëœ (y)TÏ•Ëœ (x))=tr( C )= tr(C ), (51)
w w n x,y n x,y
whereC isthecovariancematrixforvariablexandy,C âˆˆ RhÃ—h,histhenumberofhidden
x,y x,y
features.
Forthesecondformulation,wehave
tr(KËœ KËœ )=tr[Ï•Ëœ (x)Ï•Ëœ (x)TÏ•Ëœ (y)Ï•Ëœ (y)T]
x y w w w w
=tr[Ï•Ëœ (x)(Ï•Ëœ (x)TÏ•Ëœ (y)Ï•Ëœ (y)T)]
w w w w
=tr[(Ï•Ëœ (x)TÏ•Ëœ (y)Ï•Ëœ (y)T)Ï•Ëœ (x)]
w w w w
=tr[Ï•Ëœ (x)TÏ•Ëœ (y)Ï•Ëœ (y)TÏ•Ëœ (x)] (52)
w w w w
=âˆ¥Ï•Ëœ (x)TÏ•Ëœ (y)âˆ¥2
w w F
=âˆ¥nC âˆ¥2
x,y F
=n2âˆ¥C âˆ¥2.
x,y F
23PublishedasaconferencepaperatICLR2024
TogetherwithEq. 49,Eq. 50,Eq. 51andEq. 52formulatedabove,wecouldprovetheLemma7in
themainpaper. Proofends.
A4.6 PROOFOFTHEOREM8
Proof. Thesummarystatisticscontaintwoparts: totalsamplesizenandcovariancetensorC âˆˆ
T
Rdâ€²Ã—dâ€²Ã—hÃ—h. Let Cij âˆˆ RhÃ—h be the (i,j)-th entry of the covariance tensor, which denotes the
T
covariancematrixofthei-thandthej-thvariable.
With the summary statistics as a proxy, we can substitute the raw data at each client. During the
procedures of causal discovery, the needed statistics include T in Theorem 4, E(TË† |D) and
CI CI
Var(TË† |D)inTheorem5,andâ–³Ë† andâ–³Ë† inTheorem6.
CI Xâ†’Y Yâ†’X
1)BasedontheEq. (7)inthemainpaper,wehave
C =C âˆ’C (C +Î³I)âˆ’1C
XÂ¨Y|Z XÂ¨Y XÂ¨Z ZZ ZY
=C âˆ’C (C +Î³I)âˆ’1C (53)
(X,Z),Y (X,Z),Z ZZ ZY
=(C +C )âˆ’(C +C )(C +Î³I)âˆ’1C
XY ZY XZ ZZ ZZ ZY
Inthispaper,weconsiderthescenarioswhereX andY aresinglevariables,andZ maybeasingle
variable,asetofvariables,orempty. AssumingthatZ containsLvariables. Wehave
L L L L
(cid:88) (cid:88) (cid:88)(cid:88)
C = C , C = C , C = C , (54)
ZY ZiY XZ XZi ZZ ZiZj
i=1 i=1 i=1j=1
whereC ,C ,C ,andC aretheentriesofthecovariancetensorC . AccordingtoTheo-
XY ZiY XZi ZiZj T
rem3,T â‰œnâˆ¥C âˆ¥2. Therefore,thesummarystatisticsaresufficienttorepresentT .
CI XÂ¨Y|Z F CI
2)SimilartoEq. 53,wehave
C =(C +2C +C )(C +C )(C +Î³I)âˆ’1(C +C ) (55)
XÂ¨|Z XX XZ ZZ XZ ZZ ZZ XZ ZZ
C =C âˆ’C (C +Î³I)âˆ’1C . (56)
Y|Z YY YZ ZZ ZY
Substituting Eq. 54 into Eq. 55 and Eq. 56, we can also conclude that the covariance tensor
is sufficient to represent C and C . In other words, the summary statistics are sufficient to
XÂ¨|Z Y|Z
representE(TË† |D)andVar(TË† |D).
CI CI
3)AsshowninsectionA4.3,wehave
âˆ¥Câˆ— âˆ¥2 âˆ¥Câˆ— âˆ¥2
â–³Ë† = X,YËœ F , â–³Ë† = Y,XËœ F , (57)
Xâ†’Y tr(Câˆ—)Â·tr(Câˆ—) Yâ†’X tr(Câˆ—)Â·tr(Câˆ—)
X YËœ Y XËœ
whereeachcomponentscanberepresentedassomecombinationsofcovariancematrices,asshown
inEq. 37, Eq. 39, Eq. 40, Eq. 43, andEq. 45. Therefore, thesummarystatisticsaresufficientto
representâ–³Ë† andâ–³Ë† .
Xâ†’Y Yâ†’X
4) To sum up, we could conclude that: The summary statistics, consisting of total sample size n
and covariance tensor C , are sufficient to represent all the statistics needed for federated causal
T
discovery.
Proofends.
A5 DETAILS ABOUT FEDERATED UNCONDITIONAL INDEPENDENCE TEST
Here, weprovidemoredetailsaboutthefederatedunconditionalindependencetest(FUIT),where
the conditioning set Z is empty. Generally, this method follows similar theorems for federated
conditionalindependenttest(FCIT).
24PublishedasaconferencepaperatICLR2024
A5.1 NULLHYPOTHESIS
Considerthenullandalternativehypothesis
H :X âŠ¥âŠ¥Y, H :XâŠ¥Ì¸âŠ¥Y. (58)
0 1
SimilartoFCIT,weconsiderthesquaredFrobeniusnormoftheempiricalcovariancematrixasan
approximation,givenas
H :âˆ¥C âˆ¥2 =0, H :âˆ¥C âˆ¥2 >0. (59)
0 XÂ¨Y F 1 XÂ¨Y F
In this unconditional case, we set the test statistics as T â‰œ nâˆ¥C âˆ¥2, and give the following
UI XÂ¨Y F
theorem.
Theorem14(FederatedUnconditionalIndependentTest). UnderthenullhypothesisH (X andY
0
areindependent),theteststatistic
T â‰œnâˆ¥C âˆ¥2, (60)
UI XY F
hastheasymptoticdistribution
L
TË† â‰œ 1 (cid:88) Î» Î» Î±2 ,
UI n2 X,i Y,j ij
i,j=1
whereÎ» andÎ» aretheeigenvaluesofKËœ andKËœ ,respectively. Here,theproofissimilartothe
X Y X Y
proofofTheorem3,thuswereferthereaderstosectionA4.2formoredetails.
A5.2 NULLDISTRIBUTIONAPPROXIMATION
Wealsoapproximatethenulldistributionwithatwo-parameterGammadistribution,whichisrelated
tothemeanandvariance. UnderthehypothesisH andgiventhesampleD,thedistributionofTË†
0 CI
canbeapproximatedbytheÎ“(Îº,Î¸)distribution. Hereweprovidethetheoremfornulldistribution
approximation.
Theorem15(NullDistributionApproximation). UnderthenullhypothesisH (X andY areinde-
0
pendent),wehave
E(TË† |D)=tr(C )Â·tr(C ),
UI X Y
(61)
Var(TË† |D)=2âˆ¥C âˆ¥2 Â·âˆ¥C âˆ¥2,
UI X F Y F
Here, the proof is similar to the proof of Theorem 4, thus we refer the readers to section A4.3 for
moredetails.
A6 DETAILS ABOUT SKELETON DISCOVERY AND DIRECTION
DETERMINATION
In this section, we will introduce how we do the skeleton discovery and direction determination
duringtheprocessoffederatedcausaldiscovery. Allthosestepsareconductedontheserverside.
Ourstepsaresimilartothepreviousmethod, suchasCD-NOD(Huangetal.,2020), thecoredif-
ferencearethatwedevelopandutilizeourproposedfederatedconditionalindependenttest(FCIT)
andfederatedindependentchangeprinciple(FICP).
A6.1 SKELETONDISCOVERY.
Wefirstconductskeletondiscoveryontheaugmentedgraph. Theextrasurrogatevariableisintro-
ducedinordertodealwiththedataheterogeneityacrossdifferentclients.
Lemma16. GiventheAssumptions1,2and3inthemainpaper,foreachV âˆˆV,V andâ„§arenot
i i
adjacentinthegraphifandonlyiftheyareindependentconditionalonsomesubsetof{V |j Ì¸=i}.
j
25PublishedasaconferencepaperatICLR2024
Proof. IfV â€™scausalmoduleisinvariant,whichmeansthatP(V |PA )remainsthesameforevery
i i i
valueofâ„§,thenV âŠ¥âŠ¥ â„§|PA . Thus,ifV andâ„§arenotindependentconditionalonanysubsetof
i i i
other variables, V â€™s module changes with â„§, which is represented by an edge between V and â„§.
i i
Conversely,weassumethatifV â€™smodulechanges,whichentailsthatV andâ„§arenotindependent
i i
givenPA ,thenV andâ„§arenotindependentgivenanyothersubsetofV\{V }. Proofends.
i i i
Lemma17. GiventheAssumptions1,2and3inthemainpaper,foreveryV ,V âˆˆ V,V andV
i j i j
are not adjacent if and only if they are independent conditional on some subset of {V |l Ì¸= i,l Ì¸=
l
j}âˆª{â„§}.
Proof. The â€ifâ€ direction shown based on the faithfulness assumption on G and the fact that
aug
{Ïˆ (â„§)}L âˆª{Î¸ (â„§)}d is a deterministic function of â„§. The â€only ifâ€ direction is proven by
l l=1 i i=1
making use of the weak union property of conditional independence repeatedly, the fact that all
{Ïˆ (â„§)}L and{Î¸ (â„§)}d aredeterministicfunctionofâ„§, theabovethreeassumptions, andthe
l l=1 i i=1
propertiesofmutualinformation. Pleasereferto(Zhangetal.,2015)formorecompleteproof.
Withthegiventhreeassumptionsinthemainpaper,wecandoskeletondiscovery.
i) Augmentedgraphinitialization. Firstofall, buildacompletelyundirectedgraphonthe
extendedvariablesetV âˆª{â„§},whereV denotestheobservedvariablesandâ„§issurrogate
variable.
ii) Changing module detection. For each edge â„§âˆ’V , conduct the federated conditional
i
independence test or federated unconditional independent test. If they are conditionally
independentorindependent,removetheedgebetweenthem. Otherwise,keeptheedgeand
orientâ„§â†’V .
i
iii) Skeletondiscovery. Moreover,foreachedgeV âˆ’V ,alsoconductthefederatedindepen-
i j
dencetest orfederatedunconditionalindependent test. If theyareconditionally indepen-
dentorindependent,removetheedgebetweenthem.
Intheprocedures,howobservedvariablesdependonsurrogatevariableâ„§isunknownandusually
nonlinear,thusitiscrucialtouseageneralandnon-parametricconditionalindependenttestmethod,
whichshouldalsosatisfythefederatedlearningconstraints. Here,weutilizeourproposedFCIT.
A6.2 DIRECTIONDETERMINATION.
Afterobtainingtheskeleton,wecangoonwiththecausaldirectiondetermination. Byintroducing
thesurrogatevariableâ„§,itdoesnotonlyallowustoinfertheskeleton,butalsofacilitatethedirection
determinations. ForeachvariableV whosecausalmoduleischanging(i.e.,â„§âˆ’V ),insomeways
i i
wemightdeterminethedirectionsofeveryedgeincidenttoV . AssumeanothervariableV which
i j
isadjacenttoV ,thenwecandeterminethedirectionsviathefollowingrules.
i
i) Direction determination with one changing module. When V â€™s causal module is not
j
changing, wecanseeâ„§âˆ’V âˆ’V formsanunshieldedtriple. Forpracticepurposes, we
i j
cantakethedirectionbetweenâ„§andV asâ„§â†’V ,sinceweletâ„§bethesurrogatevariable
i i
to indicate whether this causal module is changing or not. Then we can use the standard
orientationrules(Spirtesetal.,2000)forunshieldedtriplestoorienttheedgebetweenV
i
andV . (1)Ifâ„§andV areindependentconditionalonsomesubsetof{V |lÌ¸=j}whichis
j i l
excludingV ,thenthetripleformsaV-structure,thuswehaveâ„§â†’V â†V . (2)Ifâ„§and
j i j
V areindependentconditionalonsomesubsetof{V |lÌ¸=i}âˆª{V }whichisincludingV ,
i l j j
thenwehaveâ„§â†’V â†’V . Intheprocedure,weapplyourproposedFCIT.
i j
ii) Directiondeterminationwithtwochangingmodules. WhenV â€™scausalmoduleischang-
j
ing,wecanseethereisaspecialconfounderâ„§betweenV âˆ’V . Firstofall,asmentioned
i j
above, we can still orient â„§ â†’ V and â„§ â†’ V . Then, inspired by that P(cause) and
i j
P(effect|cause) change independently, we can identify the direction between V and V
i j
accordingtoLemma1,andweapplyourproposedFICP.
26PublishedasaconferencepaperatICLR2024
(a) PrecisionandrecallonlinearGaussianmodel.
(b) Precisionandrecallongeneralfunctionalmodel.
FigureA2: Resultsofthesyntheticdataseton(a)linearGaussianmodeland(b)generalfunctional
model. Byrowsineachsubfigure, weevaluatevaryingnumberofvariablesd, varyingnumberof
clientsK,andvaryingnumberofsamplesn . Bycolumnsineachsubfigure,weevaluateSkeleton
k
Precision(â†‘),SkeletonRecall(â†‘),DirectionPrecision(â†‘)andDirectionRecall(â†‘).
27PublishedasaconferencepaperatICLR2024
A7 DETAILS ABOUT THE EXPERIMENTS ON SYNTHETIC DATASETS
Moredetailsaboutthesyntheticdatasetsareexplainedinthissection,includingtheimplementation
detailsinsectionA7.1,theresultsanalysisofF andSHDinsectionA7.2,thecompleteresultsof
1
precisionandrecallinsectionA7.3,thecomputationaltimeanalysisinsectionA7.4,thehyperpa-
rameterstudyonthenumberofhiddenfeatureshinsectionA7.5,thestatisticalsignificancetestfor
theresultsinsectionA7.6,andtheevaluationondensegraphinsectionA7.7.
A7.1 IMPLEMENTATIONDETAILS
Weprovidetheimplementationdetailsofourmethodandotherbaselinemethods.
â€¢ FedDAG(Gaoetal.,2022):Codesareavailableattheauthorâ€™sGithubrepositoryhttps:
//github.com/ErdunGAO/FedDAG. Thehyperparametersaresetbydefault.
â€¢ NOTEARS-ADMM and NOTEARS-MLP-ADMM (Ng & Zhang, 2022): Codes are
available at the authorâ€™s Github repository https://github.com/ignavierng/
notears-admm. Thehyperparametersaresetbydefault,e.g.,wesetthethresholdlevel
to0.1forpost-processing.
â€¢ FedPC (Huang et al., 2022): Although there is no public implementation provided by
the author, considering that it is the only constraint-based method among all the existing
worksforfederatedcausaldiscovery,westillcomparedwithit. Wereproduceditbasedon
the Causal-learn package https://github.com/py-why/causal-learn. Im-
portantly,wefollowthepaper,setthevotingrateas30%andsetthesignificancelevelto
0.05.
â€¢ FedCDH(Ours): Our method is developed based on the CD-NOD (Huang et al., 2020)
and KCI (Zhang et al., 2012) which are publicly available in the Causal-learn package
https://github.com/py-why/causal-learn. Wesetthehyperparameterhto
5, andsetthesignificancelevelforFCITto0.05. Oursourcecodehasbeenappendedin
theSupplementaryMaterials.
For NOTEARS-ADMM, NOTEARS-MLP-ADMM, and FedDAG, the output is a directed acyclic
graph (DAG), while FedPC and our FedCDH may output a completed partially directed acyclic
graph (CPDAG). To ease comparisons, we use the simple orientation rules (Dor & Tarsi, 1992)
implementedbyCausal-DAG(ChandlerSquires,2018)toconvertaCPDAGintoaDAG.Weeval-
uateboththeundirectedskeletonandthedirectedgraph,denotedbyâ€œSkeletonâ€andâ€œDirectionâ€as
shownintheFigures.
A7.2 ANALYSISOFF
1
ANDSHD
WehaveprovidedtheresultsofF andSHDinthemainpaperasshowninFigure3andFigureA3,
1
hereweprovidefurtherdiscussionsandanalysis.
The results of linear Gaussian model are given in Figure 3 and those of general functional model
areprovidedinFigureA3. Accordingtotheresults,weobservethatourFedCDHmethodgenerally
outperformsallotherbaselinesacrossdifferentcriteriaandsettings. Accordingtotheresultsofour
methodonbothofthetwomodels,whendincreases,theF scoredecreasesandtheSHDincreases
1
for skeletons and directions, indicating that FCD with more variables might be more challenging.
Onthecontrary,whenK andn increase,theF scoregrowsandtheSHDreduces,suggestingthat
k 1
morejointclientsorsamplescouldcontributetobetterperformancesforFCD.
InlinearGaussianmodel,NOTEARS-ADMMandFedPCgenerallyoutperformFedDAG.Therea-
son may be that the front two methods were proposed for linear model while the latter one was
speciallyproposedfornonlinearmodel. Ingeneralfunctionalmodel,FedPCobtainedtheworstper-
formancecomparedtoothermethodsindirectionF score,possiblyduetoitsstrongassumptionson
1
linearmodelandhomogeneousdata. FedDAGandNOTEARS-MLP-ADMMrevealedpoorresults
regardingSHD,thereasonsmaybetwo-fold: theyassumenonlinearidentifiablemodel,whichmay
notwellhandlethegeneralfunctionalmodel;andbothofthemarecontinuous-optimization-based
methods,whichmightsufferfromvariousissuessuchasconvergenceandnonconvexity.
28PublishedasaconferencepaperatICLR2024
Figure A3: Results of synthetic dataset on general functional model. By rows, we evaluate vary-
ing number of variables d, varying number of clients K, and varying number of samples n . By
k
columns,weevaluateSkeletonF (â†‘),SkeletonSHD(â†“),DirectionF (â†‘)andDirectionSHD(â†“).
1 1
A7.3 RESULTSOFPRECISIONANDRECALL
Inthemainpaper, wehaveonlyprovidedtheresultsofF scoreandSHD,duetothespacelimit.
1
Here, we provide more results and analysis of the precision and the recall. The results of average
andstandarddeviationareexhibitedinFigureA2. Accordingtotheresults,wecouldobservethat
ourFedCDHmethodgenerallyoutperformedallotherbaselinemethods,regardingtheprecisionof
bothskeletonanddirection.
Moreover, in the linear Gaussian model, NOTEARS-ADMM generally achieved the best perfor-
mance regarding the recall although it performed poorly in precision, the reason might be that
NOTEARS-ADMM assumed homogeneous data distribution, which might face challenges in the
scenarioswithheterogeneousdata. Inthegeneralfunctionalmodel,whenevaluatingvaryingnum-
bersofclientsK andsamplesn ,FedDAGperformedthebestwithrespecttotherecall,however,
k
neitherFedDAGnorNOTEARS-MLP-ADMMobtainedsatisfactoryresultsintheprecision,therea-
sonmightbethatbothofthemarecontinuous-optimization-basedmethods,whichmightpotentially
sufferfromvariousissuessuchasconvergenceandnonconvexity.
A7.4 RESULTSOFCOMPUTATIONALTIME
Existingworksaboutfederatedcausaldiscoveryrarelyevaluatethecomputationaltimewhencon-
ductingexperiments. Actually,itisusuallydifficulttomeasuretheexactcomputationaltimeinreal
life,becauseofsomefacts,suchastheparalleledcomputationforclients,thecommunicationtime
costs between the clients and the server, and so on. However, the computational time is a signif-
icant factor to measure the effectiveness of a federated causal discovery method to be utilized in
practical scenarios. Therefore, in this section, for making fair comparisons, we evaluate the com-
putationaltimeforeachmethod,assumingthatthereisnoparalleledcomputation(meaningthatwe
recordthecomputationaltimeateachclientandserverandthensimplyaddthemup)andnoextra
communicationcost(indicatingzerotimecostforcommunication).
Weevaluatedifferentsettingsasmentionedabove,includingvaryingnumberofvariablesd,varying
number of clients K, and varying number of samples n . We generate data according to linear
k
29PublishedasaconferencepaperatICLR2024
Table A2: Results of computational time for varying number of variables d, varying number of
clientsK,andvaryingnumberofsamplesn . Wereporttheaverageandstandarddeviationover10
k
runs. ThisisthesyntheticdatasetbasedonlinearGaussianmodel.
DataSizes Methods
d K n FedPC NOTEARS-ADMM FedDAG FedCDH(Ours)
k
6 3.87Â±1.97s 14.10Â±1.89s 136.92Â±21.50s 8.14Â±2.47s
12 32.01Â±3.54s 28.33Â±2.46s 321.84Â±65.94s 62.69Â±7.77s
18 10 100 39.58Â±4.75s 35.13Â±2.89s 398.27Â±149.51s 98.57Â±9.23s
24 84.05Â±7.64s 40.01Â±2.94s 715.80Â±268.93s 172.11Â±18.18s
30 94.03Â±9.48s 56.35Â±3.91s 1441.13Â±519.04s 232.35Â±26.67s
2 0.72Â±0.24s 7.04Â±0.64s 50.38Â±11.29s 3.88Â±1.49s
4 2.07Â±0.73s 9.07Â±0.77s 85.08Â±15.68s 5.24Â±1.74s
6 8 100 3.64Â±1.54s 10.80Â±0.78s 114.81Â±29.67s 8.01Â±2.32s
16 5.79Â±2.59s 19.40Â±2.51s 342.34Â±62.28s 12.60Â±2.98s
32 14.08Â±4.44s 30.56Â±2.88s 714.06Â±137.31s 20.30Â±4.37s
25 0.48Â±0.10s 13.06Â±1.91s 125.77Â±20.64s 3.75Â±1.29s
50 1.47Â±0.64s 13.75Â±2.51s 127.25Â±20.38s 5.74Â±1.61s
6 10 100 3.87Â±1.97s 14.10Â±1.89s 136.92Â±21.50s 8.14Â±2.47s
200 16.52Â±3.63s 14.68Â±2.23s 138.67Â±31.91s 13.78Â±3.75s
400 51.10Â±6.87s 15.90Â±2.54s 140.37Â±34.42s 22.86Â±4.55s
Gaussianmodel.Foreachsetting,werun10instances,reporttheaverageandthestandarddeviation
ofthecomputationaltime. TheresultsareexhibitedinTableA2.
Accordingtotheresults,wecouldobservethatamongthefourFCDmethods,FedDAGistheleast
efficient method with the largest time cost, because it uses a two-level structure to handle the het-
erogeneousdata: thefirstlevellearnstheedgesanddirectionsofthegraphandcommunicateswith
theservertogetthemodelinformationfromotherclients,whilethesecondlevelapproximatesthe
mechanismamongvariablesandpersonallyupdatesonitsowndatatoaccommodatethedatahetero-
geneity. Meanwhile, FedPC,NOTEARS-ADMMandourFedCDHarecomparable. Inthesetting
of varying variables, our method exhibited unsatisfactory performance among the three methods,
becausetheothertwomethods,FedPCandNOTEARS-ADMM,aremainlyforhomogeneousdata.
However, inthecaseofvaryingvariables, NOTEARS-ADMMisthemostineffectivemethod, be-
causewiththeincreasingofclients,moreparameters(oneclientcorrespondstoonesubadjacency
matrix which needs to be updated) should get involved in the optimization process, therefore, the
totalprocessingtimecanalsoincreasebyalargemargin. Inthescenarioofvaryingsamples,FedPC
istheslowestoneamongthethreemethods.
A7.5 HYPERPARAMETERSTUDY
Weconductexperimentsonthehyperparameter,suchasthenumberofmappingfunctionsorhidden
featuresh. Regardingtheexperimentsinthemainpaper,wesethto5bydefault. Hereinthissec-
tion,weseth âˆˆ {5,10,15,20,25,30},d = 6,K = 10,n = 100andevaluatetheperformances.
k
WegeneratedataaccordingtolinearGaussianmodel. WeusetheF score,theprecision,therecall
1
andtheSHDforbothskeletonanddirection. Wealsoreporttheruntime. Werun10instancesand
reporttheaveragevalues. TheexperimentalresultsaregiveninTableA3.
Accordingtotheresults,wecouldobservethatwiththenumberofhiddenfeatureshincreasing,the
performanceofthedirectionisobviouslygettingbetter,whiletheperformanceoftheskeletonmay
fluctuatealittlebit.
Theoretically, the more hidden features or a larger h we consider, the better performance of how
closelytherandomfeaturesapproximatethekernelsshouldbe.Whenthenumberofhiddenfeatures
approaches infinity, the performance of random features and that of kernels should be almost the
same. Andtheempiricalresultsseemtobeconsistentwiththetheory,wherealargehcanleadtoa
higherF scoreandprecisionforthedirectedgraph.
1
30PublishedasaconferencepaperatICLR2024
Table A3: Hyperparameter study on the number of hidden features h. We evaluate the F score,
1
precision,recall,andSHDofbothskeletonanddirection. Wereporttheaverageover10runs. This
isthesyntheticdatasetbasedonlinearGaussianmodel.
Metrics Skeleton Direction
Timeâ†“
h F 1â†‘ Precisionâ†‘ Recallâ†‘ SHDâ†“ F 1â†‘ Precisionâ†‘ Recallâ†‘ SHDâ†“
5 0.916 0.980 0.867 0.9 0.721 0.765 0.683 2.0 8.14s
10 0.916 0.980 0.867 0.9 0.747 0.810 0.700 2.0 8.87s
15 0.907 0.980 0.850 1.0 0.762 0.818 0.717 1.8 10.57s
20 0.889 0.980 0.833 1.2 0.767 0.833 0.717 1.8 12.72s
25 0.896 0.980 0.833 1.1 0.789 0.838 0.750 1.6 20.93s
30 0.896 0.980 0.833 1.1 0.825 0.873 0.783 1.4 37.60s
TableA4: TestresultofstatisticalsignificanceofourFedCDHmethodcomparedwithotherbase-
line methods. We report the p values via Wilcoxon signed-rank test (Woolson, 2007). This is the
syntheticdatasetbasedonlinearGaussianmodel.
Parameters [FedCDHvs.FedPC] [FedCDHvs.NOTEARS-ADMM] [FedCDHvs.FedDAG]
d k n S-F1 S-SHD D-F1 D-SHD S-F1 S-SHD D-F1 D-SHD S-F1 S-SHD D-F1 D-SHD
6 10 100 0.00 0.05 0.01 0.12 0.00 0.01 0.11 0.10 0.00 0.01 0.01 0.01
12 10 100 0.00 0.01 0.01 0.01 0.00 0.00 0.15 0.00 0.00 0.00 0.11 0.00
18 10 100 0.00 0.01 0.00 0.01 0.00 0.00 0.03 0.00 0.00 0.00 0.02 0.00
24 10 100 0.01 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.01 0.01 0.00
30 10 100 0.00 0.01 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00
6 2 100 0.00 0.00 0.01 0.01 0.01 0.00 0.21 0.01 0.00 0.00 0.03 0.00
6 4 100 0.00 0.01 0.00 0.01 0.01 0.01 0.01 0.00 0.00 0.01 0.01 0.00
6 8 100 0.00 0.00 0.01 0.02 0.02 0.01 0.03 0.02 0.00 0.00 0.09 0.00
6 16 100 0.00 0.01 0.01 0.02 0.00 0.00 0.10 0.03 0.00 0.00 0.07 0.00
6 32 100 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.01 0.00 0.00 0.03 0.00
6 10 25 0.00 0.00 0.01 0.01 0.01 0.01 0.26 0.02 0.00 0.00 0.03 0.00
6 10 50 0.00 0.01 0.01 0.00 0.01 0.00 0.99 0.03 0.00 0.00 0.02 0.00
6 10 200 0.00 0.01 0.01 0.02 0.00 0.00 0.03 0.02 0.00 0.00 0.11 0.01
6 10 400 0.00 0.01 0.01 0.01 0.01 0.00 0.03 0.01 0.00 0.01 0.01 0.00
Moreover, the computational time is also increasing. When h is smaller than 20, the runtime in-
creasessteadily. Whenhisgreaterthan20,theruntimegoesuprapidly. Importantly,wecouldsee
thatevenwhenhissmall,suchash=5,thegeneralperformanceofourmethodisstillrobustand
competitive.
A7.6 STATISTICALSIGNIFICANCETEST
Inordertoshowthestatisticalsignificanceofourmethodcomparedwithotherbaselinemethodson
thesyntheticlinearGaussianmodel,wereportthepvaluesviaWilcoxonsigned-ranktest(Woolson,
2007),asshowninTableA4. Foreachbaselinemethod,weevaluatefourcriteria: SkeletonF1(S-
F1),SkeletonSHD(S-SHD),DirectionF1(D-F1),andDirectionSHD(D-SHD).
Wesetthesignificancelevelto0.05. Thosepvalueshigherthan0.05areunderlined. Fromthere-
sults,wecanseethattheimprovementsofourmethodarestatisticallysignificantat5%significance
levelingeneral.
A7.7 EVALUATIONONDENSEGRAPH
AsshowninFigure3inthemainpaper,thetrueDAGsaresimulatedusingtheErdoÂ¨sâ€“ReÂ´nyimodel
(ErdoËsetal.,1960)withthenumberofedgesequaltothenumberofvariables. Hereweconsidera
moredensegraphwiththenumberofedgesaretwotimesthenumberofvariables.
we evaluate on synthetic linear Gaussian model and general functional model, and record the F
1
scoreandSHDforbothskeletonanddirectedgraphs. Allothersettingsarefollowingtheprevious
onesbydefault.
31PublishedasaconferencepaperatICLR2024
FigureA4:WeevaluateonsyntheticlinearGaussianmodel(TopRow)andgeneralfunctionalmodel
(BottomRow)whenthenumberofedgesaretwotimesthenumberofvariables. Bycolumns, we
evaluateSkeletonF (â†‘),SkeletonSHD(â†“),DirectionF (â†‘)andDirectionSHD(â†“).
1 1
AccordingtotheresultsasshowninFigureA4,wecanseethatourmethodsstilloutperformedother
baselinesinvaryingnumberofvariables. Interestingly,whenthegeneratedgraphismoredense,the
performanceofFedPCwillobviouslygodownforvariousnumberofvariables.
A7.8 EVALUATIONONTHEPOWEROFCONDITIONALINDEPENDENCETEST
Hereweaddedanewsetofexperimentstocomparethepowerofourproposedfederatedconditional
independencetestandthecentralizedconditionalindependencetest(i.e., kernel-basedconditional
independencetest(Zhangetal.,2012)).
We followed the previous paper (Zhang et al., 2012) and used the post-nonlinear model (Zhang
& Hyvarinen, 2012) to generate data. Assume there are four variables W,X,Y, and Z. X =
gË†(fË†(W) + Ïµ ), Y = gË†(fË†(W) + Ïµ ), and Z is independent from both X and Y. fË†and gË† are
X Y
functionsrandomlychosenfromlinear,square,sinandtanfunctions.Ïµ ,Ïµ ,W andZaresampled
X Y
from either uniform distribution U(âˆ’0.5,0.5) or Gaussian distribution N(0,1). Ïµ and Ïµ are
X Y
randomnoises. Inthiscase,XandYaredependentduetothesharedcomponentofW. SinceZ is
independentfrombothX andY,therefore,wehaveXâŠ¥Ì¸âŠ¥ Y|Z. Herewesetthesignificancelevel
to0.05,andthetotalsamplesizevariesfrom200,400,600,800to1000. ForfederatedCIT,weset
thenumberofclientsto10,therefore,eachclienthas20,40,60,80,or100samples. Werun1000
simulations and record the power of the two tests. From the result in Figure A5, we can see that
the power of our federated CIT is almost similar to that of centralized CIT. Particularly, when the
samplesizereaches1000,bothofthetwotestsachievepowerwithmorethan95%.
A7.9 EVALUATIONONTHEORDEROFDOMAININDICES
In this section, we aim to find out whether the order of domain indices will impact the results.
Theoretically, there should be no impact on the results when it takes different values because this
domainindexâ„§isessentiallyadiscretevariable(morespecifically,acategoricalvariable,withno
numericalorderamongdifferentvalues),acommonapproachtodealwithsuchdiscretevariableis
tousedeltakernel(basedonKroneckerdeltafunction),andthereforeitisreasonabletouserandom
featurestoapproximatethedeltakernelfordiscretevariables.
Empirically, we have added one new set of experiments to evaluate whether the order of domain
indiceswillimpacttheresults. WehaveonesetofdomainindicesandrunourFedCDHonthesyn-
thetic linear Gaussian model with varying number variables d âˆˆ {6,12,18,24,30} while keeping
K = 10andn = 100,othersettingsarethesameasthoseinourmainpaper. Then,werandomly
k
shuffletheindicesfordifferentdomains,denotedbyâ€œFedCDH+Shuffleâ€.
32PublishedasaconferencepaperatICLR2024
1.0
0.8
0.6
0.4
0.2 Federated CIT
Centralized CIT
0.0
200 300 400 500 600 700 800 900 1000
Number of samples n
FigureA5: Comparisonregardingthepoweroftestbetweenfederateconditionalindependencetest
andthecentralizedconditionalindependencetest.
1.00 30 1.00 30
0.75 0.75
20 20 0.50 0.50
10 10
0.25 0.25
0.00 0 0.00 0
10 20 30 10 20 30 10 20 30 10 20 30
Number of variables d Number of variables d Number of variables d Number of variables d
FedCDH+Shuffle FedCDH
FigureA6: EvaluationontheorderofdomainindicesonlinearGaussianmodel. Weevaluatevary-
ingnumberofvariablesd. Bycolumns,weevaluateSkeletonF (â†‘),SkeletonSHD(â†“),Direction
1
F (â†‘)andDirectionSHD(â†“).
1
As shown in Figure A6, the results turned out that: the performances between the two sets of dif-
ferentdomainindicesarequitesimilar,andwemayconcludethatithasnoobviousimpactonthe
resultswhenthedomainindicestakedifferentvalues.
A8 DETAILS ABOUT THE EXPERIMENTS ON REAL-WORLD DATASET
A8.1 DETAILSABOUTFMRIHIPPOCAMPUSDATASET
WeevaluateourmethodandthebaselinesonfMRIHippocampus(Poldracketal.,2015).Thedirec-
tionsofanatomicalgroundtruthare: PHCâ†’ERC,PRCâ†’ERC,ERCâ†’DG,DGâ†’CA1,CA1
â†’Sub,Subâ†’ERCandERCâ†’CA1. Generally,wefollowasimilarsettingastheexperimentson
syntheticdatasets. Foreachofthem,weusethestructuralHammingdistance(SHD),theF score
1
as evaluation criteria. We measure both the undirected skeleton and the directed graph. Here, we
considervaryingnumberofclientsK andvaryingnumberofsamplesineachclientn .
k
The results of F score and SHD is given in Figure A7. According to the results, we could ob-
1
serve that our FedCDH method generally outperformed all other baseline methods, across all the
criterialisted. Thereasoncouldbethatourmethodisspecificallydesignedforheterogeneousdata
whilesomebaselinemethodsassumehomogeneitylikeFedPCandNOTEARS-MLP-ADMM,fur-
thermore, ourmethodcanhandlearbitraryfunctionalcausalmodels, differentfromsomebaseline
methods that assume linearity such as FedPC. Compared with our method, FedDAG performed
muchworse,thereasonmightbeitsnatureofthecontinuousoptimization,whichmightsufferfrom
variousissuessuchasconvergenceandnonconvexity.
33
F
notelekS
1
rewoP
DHS
notelekS
F
noitceriD
1
DHS
noitceriDPublishedasaconferencepaperatICLR2024
Real data
FigureA7: Resultsofreal-worlddatasetfMRIHippocampus(Poldracketal.,2015). Byrows,we
evaluatevaryingnumberofclientsK andvaryingnumberofsamplesn . Bycolumns,weevaluate
k
SkeletonF (â†‘),SkeletonSHD(â†“),DirectionF (â†‘)andDirectionSHD(â†“).
1 1
15 15 0.6 0.6
10 10
0.4 5 0.4 5
0 0
2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0 2.5 5.0 7.5 10.0
Number of clients K Number of clients K Number of clients K Number of clients K
FedPC NOTEARS-MLP-ADMM FedDAG FedCDH
FigureA8:Resultsofreal-worlddatasetHKStockMarket(Huangetal.,2020).Weevaluatevarying
number of clients K, and we evaluate Skeleton F (â†‘), Skeleton SHD (â†“), Direction F (â†‘) and
1 1
DirectionSHD(â†“).
A8.2 DETAILSABOUTHKSTOCKMARKETDATASET
We also evaluate on HK stock market dataset (Huang et al., 2020) (See Page 41 for more details
about thedataset). The HKstock datasetcontains 10major stocks, which are dailyclosing prices
from 10/09/2006 to 08/09/2010. The 10 stocks are Cheung Kong Holdings (1), Wharf (Holdings)
Limited(2), HSBCHoldingsplc(3), HongKongElectricHoldingsLimited(4), HangSengBank
Ltd (5), Henderson Land Development Co. Limited (6), Sun Hung Kai Properties Limited (7),
Swire Group (8), Cathay Pacific Airways Ltd (9), and Bank of China Hong Kong (Holdings) Ltd
(10). Amongthesestocks,3,5,and10belongtoHangSengFinanceSub-index(HSF),1,8,and9
belong toHang SengCommerce andIndustry Sub-index(HSC), 2, 6, and7 belong toHang Seng
PropertiesSub-index(HSP),and4belongstoHangSengUtilitiesSub-index(HSU).
Hereonedaycanbealsoseenasonedomain. WesetthenumberofclientstobeKâˆˆ{2,4,6,8,10}
while randomly select n =100 samples for each client. All other settings are following previous
k
onesbydefault.TheresultsareprovidedinFigureA8.Accordingtotheresults,wecaninferthatour
FedCDHmethodalsooutperformedtheotherbaselinemethods,acrossthedifferentcriteria.Similar
totheanalysisabove,ourmethodistailoredforheterogeneousdata,incontrasttobaselinemethods
likeFedPCandNOTEARS-MLP-ADMM,whichassumehomogeneity. Additionally,ourapproach
is capable of handling arbitrary functional causal models, setting it apart from baseline methods
likeFedPCthatassumelinearity. Whencomparedtoourmethod,FedDAGexhibitedsignificantly
poorerperformance. Thiscouldbeattributedtoitsrelianceoncontinuousoptimization,whichmay
encounterchallengessuchasconvergenceandnonconvexity.
34
F
notelekS
1 DHS
notelekS
F
noitceriD
1 DHS
noitceriD