SMORE: Similarity-based Hyperdimensional Domain Adaptation
for Multi-Sensor Time Series Classification
JunyaoWang,MohammadAbdullahAlFaruque
{junyaow4,alfaruqu}@uci.edu
DepartmentofComputerScience,UniversityofCalifornia,Irvine,USA
ABSTRACT Distribution Shift
Manyreal-worldapplicationsoftheInternetofThings(IoT)employ ECG/EMG Multi-Sensor OOD Samples
machinelearning(ML)algorithmstoanalyzetimeseriesinforma- Sensor Time Series
Sweat Modeling
tioncollectedbyinterconnectedsensors.However,distributionshift, Sensor Pressure
afundamentalchallengeindata-drivenML,ariseswhenamodel Sensor Prediction
Training Inference
isdeployedonadatadistributiondifferentfromthetrainingdata
andcansubstantiallydegrademodelperformance.Additionally,in- Domain Adaptation Needed!
creasinglysophisticateddeepneuralnetworks(DNNs)arerequired (a) An Example of Distribution Shift in Multi-Sensor Time Series Data
tocaptureintricatespatialandtemporaldependenciesinmulti-
90 90
sensortimeseriesdata,oftenexceedingthecapabilitiesoftodayâ€™s 80 80
edgedevices.Inthispaper,weproposeSMORE,anovelresource- 6 60 0 6 60 0
efficientdomainadaptation(DA)algorithmformulti-sensortime
3 40 0 3 40 0 Standard ğ’Œ-fold
seriesclassification,leveragingtheefficientandparalleloperations
LODO
ofhyperdimensionalcomputing.SMOREdynamicallycustomizes 200
10 20 30 40 50
200
0.5k 1k 2k 4k 6k
test-timemodelswithexplicitconsiderationofthedomaincontext Iterations Dimensions
ofeachsampletomitigatethenegativeimpactsofdomainshifts.
(b) Comparing LODO CV and Standard k-fold CV of SOTA HDC
Ourevaluationonavarietyofmulti-sensortimeseriesclassifica-
tiontasksshowsthatSMOREachievesonaverage1.98%higher Figure1:MotivationofOurProposedSMORE
accuracythanstate-of-the-art(SOTA)DNN-basedDAalgorithms
amountofinformationnowadays,andthepotentialinstabilitiesof
with18.81Ã—fastertrainingand4.63Ã—fasterinference.
IoTsystems,alightweightandefficientdomain-adaptivelearning
approachformulti-sensortimeseriesdataiscriticallyneeded.
1 INTRODUCTION Brain-inspiredhyperdimensionalcomputing(HDC)hasbeen
With the emergence of the Internet of Things (IoT), many real- introducedasapromisingparadigmforedgeMLforitshighcompu-
worldapplicationsutilizeheterogeneouslyconnectedsensorsto tationalefficiencyandultra-robustnessagainstnoises[8,11,12].Ad-
collect information over the course of time, constituting multi- ditionally,HDCincorporateslearningcapabilityalongwithtypical
sensor time series data [1]. Machine learning (ML) algorithms, memoryfunctionsofstoring/loadinginformation,bringingunique
includingdeepneuralnetworks(DNNs),areoftenemployedto advantagesintacklingnoisytimeseries[8].Unfortunately,existing
analyzethecollecteddataandperformvariouslearningtasks.How- HDCsarevulnerabletoDS.Forinstance,asshowninFigure1(b),on
ever,distributionshift(DS),afundamentalchallengeindata-driven thepopularmulti-sensortimeseriesdatasetUSC-HAD[13],SOTA
ML,cansubstantiallydegrademodelperformance.Inparticular,the HDCsconvergeatnotablyloweraccuracyinleave-one-domain-out
excellentperformanceoftheseMLalgorithmsheavilyreliesonthe (LODO)cross-validation(CV)thaninstandardğ‘˜-foldCVregardless
criticalassumptionthatthetrainingandinferencedatacomefrom oftrainingiterationsandmodelcomplexity.LODOCVdevelops
thesamedistribution,whilethisassumptioncanbeeasilyviolated amodelwithalltheavailabledataexceptforonedomainleftout
asout-of-distribution(OOD)scenariosareinevitableinreal-world forinference,whilestandardğ‘˜-foldCVrandomlydividesdatainto
applications[2,3].Forinstance,asshowninFigure1(a),ahuman ğ‘˜ subsetswithğ‘˜ âˆ’1subsetsfortrainingandtheremainingone
activityrecognitionmodelcansystematicallyfailwhentestedon forevaluation.Suchperformancedegradationindicatesaverylim-
individualsfromdifferentagegroupsordiversedemographics. iteddomainadaptation(DA)capabilityofexistingHDCs.However,
Avarietyofinnovativedomainadaptationtechniqueshavebeen standardğ‘˜-foldCVdoesnotreflectreal-worldDSissuessincethe
proposedfordeeplearning(DL)[4â€“7].However,duetotheirlimited randomsamplingprocessintroducesdataleakage,enablingthe
capacityformemorization,DLapproachesoftenfailtoperformwell trainingphasetoincludeinformationfromallthedomainsand
onmulti-sensortimeseriesdatawithintricatespatialandtemporal thusinflatingmodelperformance.Toaddressthisproblem,wepro-
dependencies[1,8].Althoughrecurrentneuralnetworks(RNNs), poseSMORE,anovelHDC-basedDAalgorithmformulti-sensor
includinglongshort-termmemory(LSTM),haverecentlybeenpro- timeseriesclassification.Ourmaincontributionsarelistedbelow:
posedtoaddressthisissue,thesemodelsarenotablycomplicated â€¢ Tothebestofourknowledge,SMOREisthefirstHDC-based
andinefficienttotrain.Specifically,theircomplexarchitectures DAalgorithm.Byexplicitlyconsideringthedomaincontextof
requireiterativelyrefiningmillionsofparametersovermultiple eachsampleduringinference,SMOREprovidesonaverage1.98%
timeperiodsinpowerfulcomputingenvironments[9,10].Consid- higheraccuracythanSOTADL-basedDAalgorithmsonawide
eringtheresourceconstraintsofembeddeddevices,themassive rangeofmulti-sensortimeseriesclassificationtasks.
4202
beF
02
]GL.sc[
1v33231.2042:viXra
)%(
ycaruccA
)%(
ycaruccADACâ€™24,June23â€“27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
â€¢ Leveragingtheefficientandparallelhigh-dimensionaloperations, uniquepropertyofthehyperdimensionalspaceistheexistenceof
SMOREprovidesonaverage18.81Ã—fastertrainingand4.63Ã— largeamountsofnearlyorthogonalhypervectors,enablinghighly
fasterinferencethanSOTADL-basedDAalgorithms,providing parallel and efficient operations such as similarity calculations,
amoreefficientsolutiontotackletheDSchallenge. bundling,andbinding.Similarity(ğ›¿):calculationofthedistance
â€¢ WeevaluateSMOREacrossmultipleresource-constrainedhard- between two hypervectors. A common measure is cosine simi-
waredevicesincludingRaspberryPiandNVIDIAJetsonNano. larity.Bundling(+):element-wiseadditionofhypervectors,e.g.,
SMOREdemonstratesconsiderablylowerinferencelatencyand Hğ‘ğ‘¢ğ‘›ğ‘‘ğ‘™ğ‘’ =H1+H2,generatingahypervectorwiththesamedi-
energyconsumptionthanSOTADL-basedDAalgorithms. mensionasinputs.Bundlingprovidesanefficientwaytocheckthe
existenceofahypervectorinabundledset.Inthepreviousexam-
2 RELATEDWORKS ple,ğ›¿(Hğ‘ğ‘¢ğ‘›ğ‘‘ğ‘™ğ‘’,H1) â‰«0whileğ›¿(Hğ‘ğ‘¢ğ‘›ğ‘‘ğ‘™ğ‘’,H3)â‰ˆ0(H3â‰ H1,H2).
Bundlingmodelshowhumanbrainsmemorize inputs.Binding
2.1 DomainAdaptation
(âˆ—):element-wisemultiplicationoftwohypervectorstocreatean-
Distributionshift(DS)ariseswhenamodelisdeployedonadata othernear-orthogonalhypervector,i.e.Hğ‘ğ‘–ğ‘›ğ‘‘ = H1âˆ—H2 where
distributiondifferentfromwhatitwastrainedon,posingserious ğ›¿(Hğ‘ğ‘–ğ‘›ğ‘‘,H1) â‰ˆ0andğ›¿(Hğ‘ğ‘–ğ‘›ğ‘‘,H2) â‰ˆ0.Duetoreversibility,i.e.,
robustnesschallengesforreal-worldMLapplications[4,5].Var- Hğ‘ğ‘–ğ‘›ğ‘‘ âˆ—H1 = H2, information from both hypervectors can be
ious innovative methodologies have been proposed to mitigate preserved.Bindingmodelshowhumanbrainsconnectinputs.Per-
DS,andcanbeprimarilycategorizedasdomaingeneralizations mutation(ğœŒ):asinglecircularshiftofahypervectorbymoving
(DG)anddomainadaptations(DA).DGtypicallyseekstoconstruct thevalueofthefinaldimensiontothefirstpositionandshiftingall
modelsbyidentifyingdomain-invariantfeaturessharedacrossmul- othervaluestotheirnextpositions.Thepermutedhypervectoris
tiplesourcedomains[2].However,itcanbechallengingtoextract nearlyorthogonaltoitsoriginalhypervector,i.e.,ğ›¿(ğœŒH,H) â‰ˆ0.
commonfeatureswhenthereexistmultiplesourcedomainswith Permutationmodelshowhumanbrainshandlesequentialinputs.
distinctcharacteristics[6,8,14];thus,existingDGapproachesoften
failtoprovidecomparablehigh-qualityresultsasDA[7,15].In
contrast,DAgenerallyutilizesunlabeleddataintargetdomains 3.2 ProblemFormulation
toquicklyadaptmodelstrainedindifferentsourcedomains[4]. WeassumethatthereareK (K > 1)sourcedomains,i.e.,Dğ‘† =
Unfortunately,mostexistingDAtechniquesrelyonmultipleconvo- {D1,D2,...,DK},intheinputspaceI,andwedenotetheout-
S S S
lutionalandfully-connectedlayerstotraindomaindiscriminators, putspaceasY.I consistsoftime-seriesdatafromğ‘š (ğ‘š â‰¥ 1)
requiringintensivecomputationsanditerativerefinement.Wepro- interconnectedsensors,i.e.,I={I1,I2,...,Iğ‘š}.Ourobjectiveis
poseSMORE,thefirstHDC-basedDAalgorithm,toprovideamore toutilizetrainingsamplesinIandtheircorrespondinglabelsinY
resource-efficientDAsolutionforedgeplatforms. totrainaclassificationmodelğ‘“ :Iâ†’Ytocapturelatentfeatures
sothatwecanmakeaccuratepredictionswhengivensamplesfrom
2.2 HyperdimensionalComputing anunseentargetdomainD .Thekeychallengeisthatthejoint
T
distributionbetweensourcedomainsandtargetdomainscanbedif-
SeveralrecentworkshaveutilizedHDCasalightweightlearning
paradigmfortimeseriesclassification[16â€“18],achievingcompa- ferent,i.e,P (S I,Y) â‰ P (T I,Y),andthusthemodelğ‘“ canpotentially
rableaccuracytoSOTADNNswithnotablylowercomputational failtoadaptmodelstrainedonD tosamplesfromD .
S T
costs.However,existingHDCsdonotconsidertheDSchallenge. AsshowninFigure2,ourproposedSMOREstartswithmapping
Thiscanbeadetrimentaldrawbackasout-of-distribution(OOD) trainingsamplesfromtheinputspaceI toahyperdimensional
instancesareinevitableduringreal-worlddeploymentofMLap- spaceX withanencoderÎ© (A),i.e.,X = Î©(I),thatpreserves
plications.HyperdimensionalFeatureFusion[3]identifiesOOD thespatialandtemporaldependenciesinI.Wethenseparatethe
samplesbyfusingoutputsofseverallayersofaDNNmodeltoa encodeddataintoKsubsets(X1,Y1),(X2,Y2),...,(X K,Y K)(B)
commonhyperdimensionalspace,whileitsbackboneremainsrely- basedontheirdomains.Inthetrainingphase,wetrainKdomain-
ingonresource-intensivemulti-layerDNNs,andasystematicway specificmodelsM = {M1,M2,...,M K}suchthatMğ‘˜ : Xğ‘˜ â†’
totackleOODsampleshasyettobeproposed.DOMINO[8],anovel Yğ‘˜ (1 â‰¤ ğ‘˜ â‰¤ K) (C),andconcurrentlydevelopK expressive
HDC-basedDGmethod,constantlydiscardsandregeneratesbiased domaindescriptorsU = {U1,U2,...,U K}encodingthepattern
dimensionsrepresentingdomain-variantinformation;nevertheless, ofeachdomain(D).WhengivenainferencesampleI fromthe
T
itrequiressignificantlymoretrainingtimetoprovidereasonable targetdomainD ,wemapI tohyperdimensionalspacewiththe
T T
accuracy.OurproposedSMOREleveragesefficientoperationsof sameencoderÎ©usedfortraining,andidentifyOODsamples(E)
HDCtocustomizetest-timemodelsforOODsamples,providing withabinaryclassifierÎ¦utilizingthedomaindescriptorsU,i.e.,
accuratepredictionswithoutcausingsubstantialcomputational Î¦(Î©(I T),U).Finally,weconstructatest-timemodelM
T
based
overheadforbothtrainingandinference. ondomain-specificmodelsM(F)andwhetherthesampleisOOD
tomakeinferences(G)withexplicitconsiderationofthedomain
3 METHODOLOGY contextofI T,i.e.,YË†
I
=M T(Î¦(Î©(I T),U),M).
3.1 HDCPrelimnaries
Inspiredbyhigh-dimensionalinformationrepresentationinhu- 3.3 Multi-SensorTimeSeriesDataEncoding
man brains, HDC maps inputs onto hyperdimensional space as WeemploytheencodingtechniquesinFigure3tocaptureand
hypervectors, each of which contains thousands of elements. A preservespatialandtemporaldependenciesinmulti-sensortimeSMORE:Similarity-basedHyperdimensionalDomainAdaptationforMulti-SensorTimeSeriesClassification DACâ€™24,June23â€“27,2024,SanFrancisco,CA
E OOD Detection F Test-Time Modeling G Reasoning
Inference Data cosine ğ“œ cosine
A ğœ¹ ğŸ ğ“œ ğŸ ğ“œ ğŸ ğ“’ ğŸğ‰ğ“£ ğœ¹ ğŸ
â‹¯ğœ¹ ğŸ â‹¯ ğ“œ ğ“£ â‹¯ â‹¯ğ“’ ğŸğ‰ â‹¯ğœ¹ ğŸ
ğœ¹ ğ“· ğ“œ ğŸ‘ â‹¯ ğ“œ ğ“š ğ“’ ğ’ğ‰ ğœ¹ ğ“·
B Domains C Domain-Specific Modeling D Domain Descriptors
Similarity Model Update Models
Training Data ğ“§ ,ğ“¨ âˆ‘ğ’ğŸ ğ“—ğŸ ğ“¤
ğŸ ğŸ Modeling cosine ğ“œ ğ’Š(cid:2880)ğŸ ğ’Š ğŸ sensor I sensor II ğ“§ â‹¯ğŸ,ğ“¨ ğŸ â‹¯ğ“’ğ“’ ğŸğ’ŒğŸğ’Œ â‹¯ğœ¹ğœ¹ ğŸğŸ ğ“œ â‹¯ğŸğŸ âˆ‘ ğ’Šğ’ (cid:2880) â‹¯ğŸ ğŸğ“— ğ’ŠğŸ â‹¯ğ“¤ ğŸ
sensor m ğ“§ ğ“š,ğ“¨ ğ“š ğ“’ ğŸğ’Œ ğœ¹ ğ“· ğ“œ ğ“š âˆ‘ ğ’Šğ’ (cid:2880)ğ“š ğŸğ“— ğ’Šğ“š ğ“¤ ğ“š
Figure2:TheWorkflowofOurProposedSMORE
Sampling Window Vector Quantization Temporally Sorted Spatially Integrated
overallinformationfromsensorğ‘–,andğ‘šdenotesthetotalnumber
ğ“¨
ğ“¨ğ“¨
ğ’•
ğ’•ğ’•
ğŸ‘
ğŸğŸ
ğ’•ğŸ
S ğ’•ğŸens ğ’•o ğŸ‘r I
T
m ma inx ğ“—
ğ“—
ğ“—ğ’•
ğ’•
ğ’•ğŸ
ğŸ
ğŸ‘
ğ† ğ“—ğ†ğ†
ğ“—
ğ’•ğ“— ğŸ‘ğ’•ğŸğ’•ğŸ
ğ“¢ +âˆ— ğ“—
o
f
hrf
yo
ps me en
rS
vs eo
en
crs
ts
o.
o
rF
r
so
I
Gr ao anu ndr de Sx Gea
n
â€²m
s
aop nrl de
I
cIin
ab
lcF
y
uig
lr
au
a
tnr ine
d
go3 m, (w Glye âˆ—c
g
Ho em
n
)eb +ri an (te Gini â€²gn âˆ—f so Hir gm
n
â€²)aa .tt uio rn
e
ğ“¨ ğ“¨ğ“¨ â€² â€²â€² ğ’• ğ’•ğ’• ğŸ ğŸ‘ğŸ
ğ’•ğŸ
S ğ’•e ğŸns ğ’•o ğŸ‘r II
T
m ma inx ğ“—ğ“— ğ“— ğ’•(cid:4593)ğ’•(cid:4593) ğ’•(cid:4593) ğŸğŸ
ğŸ‘
ğ† ğ† ğ“—ğ† ğ“— ğ’•(cid:4593)ğ“— ğŸ‘ğ’•(cid:4593) ğŸğ’•(cid:4593) ğŸ ğ“¢â€² âˆ— ğ“—â€²
A
s3 p. s4
acsh eo
(D
w
Ano )m
,in
Sa MFi in
g
Ou- RrS Eep
2
se e,c pai aftfi
re
arc temM
sa
to
p
rapd
ii
nne igl ni gn dag
st aa mt po leh syp ine tr odim Ken susi bo sn ea tsl
basedontheirdomains(B),whereKrepresentsthetotalnumber
Figure3:HDCEncodingforMulti-SensorTimeSeriesData
ofdomains..WethenemployahighlyefficientHDCalgorithm
seriesdatawhenmappinglow-dimensioninputstohyperdimen- tocalculateadomain-specific modelforeverydomain(C),i.e.,
sionalspace.Wesampletimeseriesdatainğ‘›-gramwindows;in M = {M1,M2,...,M K}. Our approach aims to provide high-
eachsamplewindow,thesignalvalues(ğ‘¦-axis)storetheinforma- qualityresultswithfastconvergencebyidentifyingcommonpat-
tionandthetime(ğ‘¥-axis)representsthetemporalsequence.We ternsduringtrainingandeliminatingmodelsaturations.Webundle
firstassignrandomhypervectorsHğ‘šğ‘ğ‘¥ andHğ‘šğ‘–ğ‘›torepresentthe datapointsbyscalingaproperweighttoeachofthemdepending
maximumandminimumsignalvalues.Wethenperformvector onhowmuchnewinformationisaddedtoclasshypervectors.In
quantizationtovaluesbetweenthemaximumandminimumvalues particular,eachdomain-specificmodelMğ‘˜(1â‰¤ğ‘˜ â‰¤K)consists
to generate vectors with a spectrum of similarity to Hğ‘šğ‘ğ‘¥ and ofğ‘›classhypervectorsC 1ğ‘˜,C 2ğ‘˜,...,Cğ‘›ğ‘˜ (ğ‘›=thenumberofclasses),
Hğ‘šğ‘–ğ‘›. Forinstance,inFigure3,SensorIandSensorIIfollowa eachofwhichencodesthepatternofaclass.AnewsampleH in
timeseriesintrigram.SensorIhasthemaximumvalueatğ‘¡1and domain Dğ‘˜ updatesmodelMğ‘˜ basedonitscosinesimilarities,
theminimumvalueatğ‘¡2,andthusweassignrandomlygenerated denotedasS ğ›¿(H,Â·),withalltheclasshypervectorsinMğ‘˜,i.e.,
ğ‘¦h a ğ‘¡s â€²y 3sp i ige nr nv Se r eac nt n so d or ros m IH I.lğ‘¡ y W1 ga een tnd he eH r nağ‘¡ t a2 e st d so ighğ‘¦ nyğ‘¡1 p ha e yn r pvd eeğ‘¦ rc vğ‘¡ t2 eo, crr tse os H rp se ğ‘¡â€² t2c ot ai ğ‘¦v n ğ‘¡e d 3ly iH n.S ğ‘¡ Sâ€² 3i em t noi sl oa ğ‘¦r rğ‘¡â€²l 2y I, a aw n nd de ğ›¿(H,Cğ‘¡ğ‘˜ )= âˆ¥HH âˆ¥Â· Â·C âˆ¥Cğ‘¡ğ‘˜
ğ‘¡ğ‘˜âˆ¥
= âˆ¥HH
âˆ¥
Â· âˆ¥C Cğ‘¡ ğ‘¡ğ‘˜
ğ‘˜âˆ¥
âˆH Â·Norm(Cğ‘¡ğ‘˜ ) (1)
valueatğ‘¦â€² inSensorIIwithvectorquantization;mathematically,
ğ‘¡1 where1 â‰¤ğ‘¡ â‰¤ğ‘›.IfH hasthehighestcosinesimilaritywiththe
Hğ‘¡3 =Hğ‘¡2+ğ‘¦ ğ‘¦ğ‘¡ ğ‘¡3 1âˆ’ âˆ’ğ‘¦ ğ‘¦ğ‘¡ ğ‘¡2
2
Â·(Hğ‘¡1âˆ’Hğ‘¡2) c dl oa mss aih ny -p spe erv cie fic cto mr oC dğ‘–ğ‘˜ elw Mhi ğ‘˜le ui pt ds at tr eu se al sabel C ğ‘—ğ‘˜ (1 â‰¤ ğ‘–,ğ‘— â‰¤ ğ‘›),the
ğ‘¦â€² âˆ’ğ‘¦â€²
Hğ‘¡â€² 1 =Hğ‘¡â€² 3+ ğ‘¦ğ‘¡ ğ‘¡â€²1 2âˆ’ğ‘¦ğ‘¡ ğ‘¡â€²3
3
Â·(Hğ‘¡â€² 2âˆ’Hğ‘¡â€² 3). C ğ‘—ğ‘˜ â†C ğ‘—ğ‘˜ +ğœ‚Â·(cid:16) 1âˆ’ğ›¿(cid:0)H,C ğ‘—ğ‘˜(cid:1)(cid:17) Ã—H
(2)
Werepresentthetemporalsequenceofdatawiththepermutation Cğ‘˜ â†Cğ‘˜ âˆ’ğœ‚Â·(cid:16) 1âˆ’ğ›¿(cid:0)H,Cğ‘˜(cid:1)(cid:17) Ã—H,
ğ‘– ğ‘– ğ‘–
operationinsection3.1.ForSensorIandSensorIIinFigure3,we
performrotationshift(ğœŒ)twicetoHğ‘¡1 andH ğ‘¡â€² 1,oncetoHğ‘¡2 and whereğœ‚denotesalearningrate.Alargeğ›¿(H,Â·)indicatestheinput
H
sağ‘¡
mâ€² 2, pa lin nd gk we ie dp owHğ‘¡
b3
ya cn ad lcH
ulğ‘¡
aâ€²
3
tit nh ge Hsam =e ğœŒ.W ğœŒHe ğ‘¡b 1in âˆ—d ğœŒHda ğ‘¡t 2a âˆ—s Ham ğ‘¡3p ale ns din Ho â€²n =e d anat da tp ho ein mt oi ds em la isrg ui pn da all ty edm bis ym aa dt dc ih ne gd ao vr ea rl yre sa mdy ale lx pi ost rs tii on nth oe ftm heod ee nl -,
ğœŒğœŒHâ€² âˆ—ğœŒHâ€² âˆ—Hâ€².Finally,tospatiallyintegratedatafrommultiple codedvector(1âˆ’ğ›¿(H,Â·)â‰ˆ0).Incontrast,asmallğ›¿(H,Â·)indicates
sensoğ‘¡1
rs,
weğ‘¡2 geneğ‘¡3
rate a random signature hypervector for each anoticeablynewpatternandupdatesthemodelwithalargefactor
sensorandbindinformationas(cid:205)ğ‘š ğ‘–=1[Gğ‘–âˆ—Hğ‘–],whereGğ‘–denotesthe (1âˆ’ğ›¿(H,Â·)â‰ˆ1).Ourlearningalgorithmprovidesahigherchance
signaturehypervectorforsensorğ‘–,Hğ‘–isthehypervectorcontaining fornon-commonpatternstobeproperlyincludedinthemodel.
emiT
rosneS-itluM
gnidocnEseireS
rotceV
yreuQ
niamoD srotpircseD
ataD
hctaB
xam ?DOO )âˆ—ğœ¸â‰¤
ğœ¹(
ğ±ğšğ¦
ledoM elbmesnE
gnildnuB
xam noitciderPDACâ€™24,June23â€“27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
3.5 Out-of-DistributionDetection Algorithm1DomainAdaptiveHDCInference
3.5.1 DomainDescriptors. Inparalleltodomain-specificmodeling, Input: AnencodedtestingsamplesQ,adomainclassifierwithKclass
asshowninFigure2,weconcurrentlyconstructdomaindescriptors hypervectorsU1,U2,...,U K,athresholdforOODdetectionğ›¿âˆ—,K
(D) U = {U1,U2,...,U K} to encode the distinct pattern of domain-specificmodelsM1,M2,...,M K
eachdomain.Specifically,foreachdomainDğ‘˜ (1â‰¤ğ‘˜ â‰¤K),we Output: ApredictedlabelPforQ.
utilizethebundlingoperationtocombinethehS
ypervectorwithin
1: ğ›¿ğ‘šğ‘ğ‘¥ =max{ğ›¿(Q,U1),ğ›¿(Q,U2)...,ğ›¿(Q,U K)} âŠ²OODdetection
t dh ee scd ro ipm toa rin U, ğ‘˜i.e .. M,{ aH th1ğ‘˜ em,H at2ğ‘˜ ic, a. l. l. y, ,H Uğ‘›ğ‘˜
ğ‘˜ğ‘˜
=}, (cid:205)an
ğ‘›
ğ‘–d ğ‘˜c Ho ğ‘–n ğ‘˜s .t Gru ivc et nan the ex pp rr oe pss ei rv tye 432 ::: eif lsğ›¿ eğ‘š Mğ‘ Tğ‘¥ â†<ğ›¿ (cid:205)âˆ— ğ‘–Kt =h 1e ğ›¿n (Q,Uğ‘–)Â·Mğ‘– âŠ²Q âŠ²i msc oo âŠ²dn e Qs li ed ine ssr ne e od m tO b OlO i OnD g
D
ofthebundlingoperation(explainedinsection3.1),Uğ‘˜ iscosine- 5: forallğ›¿(Q,Uğ‘–) â‰¥ğ›¿âˆ—do âŠ²1â‰¤ğ‘– â‰¤ğ‘˜
similartoallthesamplesH ğ‘–ğ‘˜ (1â‰¤ğ‘– â‰¤ğ‘› ğ‘˜)withindomainDğ‘˜
S
as 6: MT â†(cid:205) ğ‘–K =1ğ›¿(Q,Uğ‘–)Â·Mğ‘– âŠ²partialmodelensembling
t sh amey pc leo sn tt hri ab tu at re et no ot th pe ab rtun ofdl ti hn eg bp ur no dc le es ,s i, .ea .n ,nd od ti is nsi dm oi mla ar it no Dal ğ‘˜lt .he 87 :: rP etâ† urnar Pgmax Cğ‘–T{ğ›¿(Q,C 1T),ğ›¿(Q,C 2T)...,ğ›¿(ğ‘„,Cğ‘›T)}
S
3.5.2 Out-of-DistributionDetection. AsshowninFigure2,akey
Table1:DetailedBreakdownsofDatasets
componentofourinferencephaseisthedetectionofOODsamples
(N:numberofdatasamples)
(E).Westartwithmappingthetestingsampletohyperdimensional
DSADS[19] USC-HAD[13] PAMAP2[20]
spacewiththesameencodingtechniqueasthetrainingphaseto
Domains N Domains N Domains N
obtainaqueryvectorQ(A).Then,asdetailedinAlgorithm1,we
Domain1 2,280 Domain1 8,945 Domain1 5,636
calculatethecosinesimilarityscoreofthequeryvectorQtoeachdo-
Domain2 2,280 Domain2 8,754 Domain2 5,591
maindescriptorU1,U2,...,U
K
(line1).Atestingsampleisidenti- Domain3 2,280 Domain3 8,534 Domain3 5,806
fiedasOODifitspatternissubstantiallydifferentfromallthesource Domain4 2,280 Domain4 8,867 Domain4 5,660
Domain5 8,274
domains.Therefore,whenthecosinesimilaritybetweenQandits
mostsimilardomain,i.e.,max(cid:8)ğ›¿(Q,U1),ğ›¿(Q,U2),...,ğ›¿(Q,U K)(cid:9), Total 9,120 Total 43,374 Total 22,693
issmallerthanathresholdğ›¿âˆ—,weconsiderQasanOODsample
(line2).Hereğ›¿âˆ—isatunableparameterandweanalyzetheimpactof (line7).Notethat,unliketheinferenceforOODsampleswherewe
ğ›¿âˆ—insection4.2.1.Wethendynamicallyconstructtest-timemodels ensembleallthedomain-specificmodelstoenhanceperformance,
(F)basedonwhetherthesampleisOOD(section3.6). thetest-timemodelM T foranin-distributionsampledoesnot
considerdomain-specificmodelsofthedomainswhereQ show
aminorsimilarityscorelowerthanğ›¿âˆ—.Inparticular,ifatesting
3.6 AdaptiveTest-TimeModeling
sampleQdoesnotshowhighsimilaritytoanyofthesourcedo-
3.6.1 InferenceforOODSamples. Foreachtestingsampleidenti-
mains,weincludeinformationfromallthedomainstoconstructa
fiedasOOD,wedynamicallyadaptdomain-specificmodelsM =
sufficientlycomprehensivetest-timemodeltomitigatethenegative
{M1,M2,...,Mğ‘˜}tocustomizeatest-timemodelM
T
thatbest
impactsofdistributionshift.Incontrast,whenQexhibitsconsider-
fitsitsdomaincontextandtherebyprovidesanaccurateprediction.
ablesimilaritytocertaindomains,addinginformationfromother
AsdetailedinAlgorithm1,foranOODsampleQ,weensemble
domainsiscomparabletointroducingnoisesandcanpotentially
eachdomain-specificmodelbasedonhowsimilarQistothedo-
misleadtheclassificationanddegrademodelperformance.
main(line3).Specifically,letğ›¿(Q,U1),ğ›¿(Q,U2),...,ğ›¿(Q,U K)
denotethecosinesimilaritybetweenQandeachdomaindescriptor
4 EXPERIMENTALEVALUATIONS
U1,U2,...,U K,weconstructthetest-timemodelM
T
forQas
4.1 ExperimentalSetup
M
T
=ğ›¿(Q,U1)Â·M1+ğ›¿(Q,U2)Â·M2+...+ğ›¿(Q,U K)Â·M K. (3)
WeevaluateSMOREonwidely-usedmulti-sensortimeseriesdatasets
Specifically,M T isofthesameshapeasM1,M2,...,Mğ‘˜;itcon- DSADS[19],USC-HAD[13],PAMAP2[20].Domainsaredefined
sistsofğ‘›classhypervectors(ğ‘› =numberofclasses),denotedas bysubjectgroupingchosenbasedonsubjectIDfromlowtohigh.
C 1T,C 2T,...,Cğ‘›T,formulatedwithexplicitconsiderationofthedo- Thedatasizeofeachdomainineachdatasetisdemonstratedin
maincontextofQ.Wethencomputethecosinesimilaritybetween TABLE1.WecompareSMOREwith(i)twoSOTACNN-basedDA
Qandeachoftheseclasshypervectors,andassignQtotheclass algorithms:TENT[4]andmultisourcedomainadversarialnetworks
towhichitachievesthehighestsimilarityscore(line7). (MDANs)[5],and(ii)twoHDCalgorithms:theSOTAHDCnot
3.6.2 InferenceforIn-DistributionSamples. Wepredictthelabelof consideringdistributionshifts[21](BaselineHD)andDOMINO[8],
anin-distributiontestingsample,i.e.,anon-OODsample,leverag- arecentlyproposedHDC-baseddomaingeneralizationframework.
ingdomain-specificmodelsofthedomainstowhichthesampleis TheCNN-basedDAalgorithmsaretrainedwithTensorFlow,and
highlysimilar.AsdemonstratedinAlgorithm1,forallthedomains weapplythecommonpracticeofgridsearchtoidentifythebest
Dğ‘– (1 â‰¤ ğ‘– â‰¤ K)wheretheencodedqueryvectorQ achievesa hyper-parametersforeachmodel.SinceDOMINOinvolvesdimen-
S
similarityscorehigherthanğ›¿âˆ—,weensembletheircorresponding sionregenerationineverytrainingiteration,forfairness,weinitiate
domain-specificmodelsincorporatingaweightoftheircosinesimi- itwithdimensionğ‘‘âˆ—=1ğ‘˜andmakeitstotaldimensionalities,i.e.,
larityscorewithQtoformulatethetest-timemodelM (line5-6). the sum of its initial dimension and all the regenerated dimen-
T
Similartosection3.6.1,M Tconsistsofğ‘›classhypervectors,andQ sionsthroughoutretraining,thesameasSMOREandBaselineHD
isassignedtotheclasswhereitobtainedthehighestsimilarityscore (ğ‘‘ =8ğ‘˜).Ourevaluationsincludeleave-one-domain-out(LODO)SMORE:Similarity-basedHyperdimensionalDomainAdaptationforMulti-SensorTimeSeriesClassification DACâ€™24,June23â€“27,2024,SanFrancisco,CA
TENT MDANs BaselineHD DOMINO SMORE (Our Work) 85
90
USC-HAD
80
75
75
60
45 70
Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 Average 00.4.5 00..56 00..67 00..78 00..89
ğœ¹âˆ—
95
DSADS Figure5:Imapctofğ›¿âˆ—onModelPerformance
80
4.2 Accuracy
TheaccuracyoftheLODOclassificationisdemonstratedinFigure
65
4.TheaccuracyofDomainğ‘˜indicatesthatthemodelistrainedwith
datafromalltheotherdomainsandevaluatedwithdatafromDo-
50
Domain 1 Domain 2 Domain 3 Domain 4 Average mainğ‘˜.Thisaccuracyscoreservesasanindicatorofthemodelâ€™sdo-
95 mainadaptationcapabilitywhenconfrontedwithunseendatafrom
PAMAP2
unseendistributions.SMOREachievescomparableperformanceto
80 TENTandonaverage1.98%higheraccuracythanMDANs.Addi-
tionally,SMOREprovides20.25%higheraccuracythanBaselineHD,
65 demonstratingthatSMOREsuccessfullyadaptstrainedmodelsto
fitsamplesfromdiversedomains.Additionally,SMOREprovides
50 onaverage4.56%higheraccuracythanDOMINO,indicatingDA
Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 Average
canmoreeffectivelyaddressthedomaindistribution(DS)challenge
Figure4:ComparingLODOAccuracyofSMOREandCNN- innoisymulti-sensortimeseriesdatacomparedtoDG.
basedDomainAdaptationAlgorithms
4.2.1 ImpactofHyperparameterğ›¿âˆ—. Theimpactofğ›¿âˆ— onclassi-
performance,learningefficiencyonbothserverCPUandresource-
fication results is demonstrated in Figure 5, where we evaluate
constraineddevices,andscalabilityusingdifferentsizesofdata.
SMOREonthedatasetUSC-HADasanexample.SMOREachieved
itsbestperformancewhenğ›¿âˆ—isaround0.65.Smallvaluesofğ›¿âˆ—are
4.1.1 Platforms. ToevaluatetheperformanceofSMOREonboth
morelikelytodetectin-distributionsamplesasOODsothatthe
high-performancecomputingenvironmentsandresource-limited
test-timemodelswouldincludenoisesfromthedomainswhere
edgedevices,weincluderesultsfromthefollowingplatforms:
thesampleonlyhasaminimalsimilarityscore,therebycausing
â€¢ ServerCPU:IntelXeonSilver4310CPU(12-core,24-thread,
notableperformancedegradation.Ontheotherhand,largeval-
2.10GHz),96GBDDR4memory,Ubuntu20.04,Python3.8.10,
uesofğ›¿âˆ—aremorelikelytoconsiderOODsamplesin-distribution.
PyTorch1.12.1,TDP120W.
Consequently,thetest-timemodelonlyincludesdomain-specific
â€¢ Embedded CPU: Raspberry Pi 3 Model 3+ (quad-core ARM modelsofthedomainsthatexhibitlimitedsimilaritytothegiven
A53@1.4GHz),1GBLPDDR2memory,Debian11,Python3.9.2,
sample;therefore,theensembledtest-timemodelisunlikelytobe
PyTorch1.13.1,TDP5W.
sufficientlycomprehensivetoprovideaccuratepredictions.
â€¢ EmbeddedGPU:JetsonNano(quad-coreARMA57@1.43GHz,
128-coreMaxwellGPU),4GBLPDDR4memory,Python3.8.10,
4.3 Efficiency
PyTorch1.13.0,CUDA10.2,TDP10W.
4.3.1 EfficiencyonServerCPU. Foreachdataset,eachdomaincon-
4.1.2 DataPreprocessing. Wedescribethedataprocessingforeach sistsofroughlysimilaramountsofdataasdetailedinTABLE1;
datasetprimarilyondatasegmentationanddomainlabeling. thus,weshowtheaverageruntimeoftrainingandinferenceforall
â€¢ DSADS[19]:TheDailyandSportsActivitiesDataset(DSADS) thedomains.AsdemonstratedinFigure6(a),SMOREexhibitson
includes19activitiesperformedby8subjects.Eachdatasegment average11.64Ã—fastertrainingthanTENTand18.81Ã—fastertraining
isanon-overlappingfive-secondwindowsampledat25Hz.Four thanMDANs.Additionally,SMOREdelivers4.07Ã—fasterinference
domainsareformedwithtwosubjectseach. thanTENTand4.63Ã—fasterinferencethanMDANs.Suchnotably
â€¢ USC-HAD[13]:TheUSChumanactivitydataset(USC-HAD) higherlearningefficiencyisthankstothehighlyparallelmatrix
includes12activitiesperformedby14subjects.Eachdatasegment operationsonhyperdimensionalspace.Additionally,SMOREpro-
isa1.26-secondwindowsampledat100Hzwith50%overlap.Five videsonaverage5.84Ã—fastertrainingcomparedtoDOMINO.In
domainsareformedwiththreesubjectseach. particular,DOMINOachievesdomaingeneralizationbyiteratively
â€¢ PAMAP2 [20]: The Physical Activity Monitoring (PAMAP2) identifyingandregeneratingdomain-variantdimensionsandthus
datasetincludes18activitiesperformedby9subjects.Eachdata requiresnotablymoreretrainingiterationstoprovidereasonable
segmentisa1.27-secondwindowsampledat100Hzwith50% performance.Ontheotherhand,duringeachretrainingiteration,
overlap.Fourdomains,excludingsubjectnine,areformedwith DOMINOonlykeepsdimensionsplayingthemostpositiverolein
twosubjectseach. theclassificationtask,andthusiteventuallyarrivesatamodelwith
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccADACâ€™24,June23â€“27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
TENT MDANs BaselineHD DOMINO SMORE (Our Work) increasingthetrainingdatasize,SMOREmaintainshighefficiency
100ğŸ00ğŸğŸ’ 12 inbothtrainingandinferencewithasub-lineargrowthinexecution
time.Incontrast,thetrainingandinferencetimeofCNN-based
8 DAalgorithmsincreasesconsiderablyfasterthanSMORE.This
1ğŸ00ğŸğŸ positionsSMOREasanefficientandscalableDAsolutionforboth
4 high-performanceandresource-constrainedcomputingplatforms.
ğŸ1ğŸğŸ 0
5 CONCLUSIONS
DASAD USCHADPAMAP2 DASAD USCHADPAMAP2
(a)EfficiencyofSMOREandCNN-based Algorithms on Server CPU WeproposeSMORE,anoveldomainadaptiveHDClearningframe-
workthatdynamicallycustomizestest-timemodelswithexplicit
10ğŸ0ğŸ0ğŸ‘ ğŸğŸğŸ’
considerationofthedomaincontextofeachsampleandtherebypro-
1ğŸ00ğŸğŸ videsaccuratepredictions.OurSMOREoutperformsSOTACNN-
basedDAalgorithmsintermsofbothaccuracyandtrainingand
ğŸğŸğŸ
ğŸ1ğŸ0ğŸ inferenceefficiency.SMOREalsoexhibitsnotablylowerinference
latencyandpowerconsumptiononedgeplatforms,makingita
scalableandefficientsolutiontoaddresstheDSchallenge.
ğŸ1ğŸğŸ
ğŸğŸğŸ
Raspberry Pi Jetson Nano Raspberry Pi Jetson Nano
(b)Efficiency ofSMOREandCNN-based Algorithms on Edge Platforms 6 ACKNOWLEDGEMENT
ThisworkwaspartiallysupportedbytheNationalScienceFounda-
Figure6:EfficiencyofSMOREandCNN-basedDAAlgorithms
tion(NSF)underawardCCF-2140154.
TENT MDANs SMORE (Our Work)
100ğŸ0ğŸ0ğŸ’ 9
REFERENCES
[1] HuihuiQiaoetal.Atime-distributedspatiotemporalfeaturelearningmethod
6 formachinehealthmonitoringwithmulti-sensortimeseries.Sensors,2018.
1ğŸ0ğŸ0ğŸ [2] IshaanGulrajanietal.Insearchoflostdomaingeneralization.InInternational
ConferenceonLearningRepresentations,2021.
3 [3] SamuelWilsonetal.Hyperdimensionalfeaturefusionforout-of-distribution
detection.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
ğŸğŸ1ğŸ 0
[4]
C Do em qup au nte WrV ai nsi gon et,2 a0 l.23 T.
ent:Fullytest-timeadaptationbyentropyminimization.
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
InInternationalConferenceonLearningRepresentations,2020.
Percentage of Training Data Percentage of Inference Data [5] HanZhaoetal.Multiplesourcedomainadaptationwithadversariallearning.In
Figure7:ComparingScalabilityUsingDifferentSizeofData
6thInternationalConferenceonLearningRepresentations,ICLR2018,2018.
[6] XinQinetal.Generalizablelow-resourceactivityrecognitionwithdiverseand
discriminativerepresentationlearning.InProceedingsofthe29thACMSIGKDD
compresseddimensionalityandprovidesaslightlyhigherinference ConferenceonKnowledgeDiscoveryandDataMining,page1943â€“1953,2023.
efficiency.Furthermore,comparedtoBaselineHD,SMOREprovides [7] ShioriSagawaetal.Distributionallyrobustneuralnetworks.InInternational
ConferenceonLearningRepresentations,2019.
significantlyhigheraccuracy(demonstratedinFig.4)withoutsub- [8] JunyaoWangetal.Domino:Domain-invarianthyperdimensionalclassification
stantiallyincreasingboththetrainingandinferenceruntimes. formulti-sensortimeseriesdata.In2023IEEE/ACMInternationalConferenceon
ComputerAidedDesign(ICCAD),pages1â€“9.IEEE,2023.
4.3.2 EfficiencyonEmbeddedPlatforms. Tofurtherunderstand [9] YaoQinetal.Adual-stageattention-basedrecurrentneuralnetworkfortime
theperformanceofSMOREonresource-constrainedcomputing s Ae rr ti ie fis cip ar le Id ni tc et lli io gn e. ncI en ,2P 0r 1o 7ce .edingsofthe26thInternationalJointConferenceon
platforms,weevaluatetheefficiencyofSMORE,TENT,MDANs, [10] SeppHochreiteretal.Longshort-termmemory.Neuralcomputation,1997.
andBaselineHDusingaRaspberryPi3ModelB+boardandan [11] JunyaoWangetal. Disthd:Alearner-awaredynamicencodingmethodfor
hyperdimensionalclassification.arXivpreprintarXiv:2304.05503,2023.
NVIDIAJetsonNanoboard.Bothplatformshaveverylimitedmem- [12] JunyaoWangetal. Hyperdetect:Areal-timehyperdimensionalsolutionfor
oryandCPUcores(andGPUcoresforJetsonNano).Figure6(b) intrusiondetectioniniotnetworks.IEEEInternetofThingsJournal,2023.
[13] MiZhangetal.Usc-had:Adailyactivitydatasetforubiquitousactivityrecog-
demonstratestheaverageinferencelatencyforeachalgorithmpro-
nitionusingwearablesensors. InProceedingsofthe2012ACMconferenceon
cessing each domain in the PAMAP2 dataset. On Raspberry Pi, ubiquitouscomputing,2012.
SMOREprovidesonaverage14.82Ã—speedupscomparedtoTENT [14] QiDouetal.Domaingeneralizationviamodel-agnosticlearningofsemantic
features.AdvancesinNeuralInformationProcessingSystems,2019.
and19.29Ã—speedupscomparedtoMDANsininference.OnJet- [15] YaroslavGaninetal.Domain-adversarialtrainingofneuralnetworks.Thejournal
son Nano, SMORE delivers 13.22Ã— faster inference than TENT ofmachinelearningresearch,2016.
[16] AbbasRahimietal.Hyperdimensionalbiosignalprocessing:Acasestudyfor
and17.59Ã—fasterinferencethanMDANs. Additionally,SMORE
emg-basedhandgesturerecognition.InICRC.IEEE,2016.
exhibitssignificantlylessenergyconsumption,providingamore [17] AliMoinetal.Awearablebiosensingsystemwithin-sensoradaptivemachine
resource-efficientdomainadaptationsolutionforthedistribution learningforhandgesturerecognition.NatureElectronics,2021.
[18] JunyaoWangetal. Robustandscalablehyperdimensionalcomputingwith
shiftchallengeonedgedevices.
brain-likeneuraladaptations.arXivpreprintarXiv:2311.07705,2023.
[19] BillurBarshanetal.Recognizingdailyandsportsactivitiesintwoopensource
4.4 Scalability machinelearningenvironmentsusingbody-wornsensorunits.TheComputer
Journal,2014.
WeevaluatethescalabilityofSMOREandSOTACNN-basedDA [20] AttilaReissetal.Introducinganewbenchmarkeddatasetforactivitymonitoring.
algorithmsusingvarioustrainingdatasizes(percentagesofthefull In16thinternationalsymposiumonwearablecomputers.IEEE,2012.
[21] AlejandroHernÃ¡ndez-Canoetal. Onlinehd:Robust,efficient,andsingle-pass
dataset)ofthePAMAP2dataset.AsdemonstratedinFigure7,with onlinelearningusinghyperdimensionalsystem.InDATE.IEEE,2021.
)s(
emiT
gniniarT
)s(
ycnetaL
ecnerefnI
)s(
emiT
gniniarT
ygrenE
)s(
ycnetaL
ecnerefnI
)J(
noitpmusnoC
)s(
emiT
ecnerefnI