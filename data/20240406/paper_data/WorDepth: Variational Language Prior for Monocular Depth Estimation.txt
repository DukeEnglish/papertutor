WorDepth: Variational Language Prior for Monocular Depth Estimation
ZiyaoZeng1 DanielWang1 FengyuYang1 HyoungseobPark1 YangchaoWu2
StefanoSoatto2 Byung-WooHong3 DongLao2 AlexWong1
1YaleUniversity 2UniversityofCalifornia,LosAngeles Chung-AngUniversity3
1{ziyao.zeng, daniel.wang.dhw33, fengyu.yang, hyoungseob.park, alex.wong}@yale.edu
2 wuyangchao1997@g.ucla.edu 2{soatto,lao}@cs.ucla.edu 3hong@cau.ac.kr
Abstract
Infinitely many 3D scenes
Three-dimensional(3D)reconstructionfromasingleim-
age is an ill-posed problem with inherent ambiguities, i.e.
scale. Predicting a 3D scene from text description(s) is
Metric-scale depth map
similarlyill-posed,i.e. spatialarrangementsofobjectsde-
scribed. Weinvestigatethequestionofwhethertwoinher-
ently ambiguous modalities can be used in conjunction to
produce metric-scaled reconstructions. To test this, we fo- Infinitely many 3D scenes
cusonmonoculardepthestimation,theproblemofpredict-
ing a dense depth map from a single image, but with an
additional text caption describing the scene. To this end,
webeginbyencodingthetextcaptionasameanandstan-
darddeviation;usingavariationalframework,welearnthe
distribution of the plausible metric reconstructions of 3D
Figure 1. Language as a prior for depth estimation. Depth
scenes corresponding to the text captions as a prior. To
estimationfromasingleimageisanill-posedproblem(i.e.,scale),
â€œselectâ€aspecificreconstructionordepthmap,weencode
andlikewisefromtextcaptions(i.e.,layout). Cantwoinherently
thegivenimagethroughaconditionalsamplerthatsamples
ambiguousmodalitiesresolvemetric-scaleddepthestimates?
fromthelatentspaceofthevariationaltextencoder,which
is then decoded to the output depth map. Our approach is
guity,suchasthescaleofthereconstruction. Consequently,
trainedalternatinglybetweenthetextandimagebranches:
inductionisnecessary,anddepthestimationbecomesdraw-
inoneoptimizationstep,wepredictthemeanandstandard
ing a scene with maximum likelihood from the distribu-
deviationfromthetextdescriptionandsamplefromastan-
tionofallpossiblescenes,conditionedontheimage. This
dard Gaussian, and in the other, we sample using a (im-
conditional scene distribution is learned by a deep neural
age)conditionalsampler. Oncetrained,wedirectlypredict
network on a chosen training set. While an ideal training
depthfromtheencodedtextusingtheconditionalsampler.
setshouldaccuratelyreflectthisdistribution,practicalchal-
Wedemonstrateourapproachonindoor(NYUv2)andout-
lenges arise due to the scarcity of well-established large-
door(KITTI)scenarios, whereweshowthatlanguagecan
scale depth datasets. A crucial question arises: Can any
consistentlyimproveperformanceinboth. Code: https:
priors, otherthanthetrainingset, beleveragedtocalibrate
//github.com/Adonis-galaxy/WorDepth.
thelearnedscenedistributiontotruereal-worldstatistics?
Thesepriorsmaycomeinmanyforms,fromgenericpri-
ors such as local smoothness and connectivity [19, 22, 67,
1.Introduction
102] or object orientation [15] that may be imposed as a
Theprocessofimagingisasurjectionfroma3Dscene partofthetrainingobjective(regularizer)tospecificinduc-
tothe2Dimagedomain, whereinfinitelymany3Dscenes tivebiasesrealizedasarchitecturaldesigns(layers)[65]or
canmaptothesameimage. Itsinverseproblem,estimating a collection object shapes [14]. While generic priors are
the3Dscenestructurefromasingleimage,i.e.,monocular suitable for a wide variety of scenes, they typically lack
depthestimation,isthereforeill-posedwithinherentambi- specificity, i.e., size or shape of objects within a specific
4202
rpA
4
]VC.sc[
1v53630.4042:viXra3Dscene. Ontheotherhand,specificnetworkdesignsmay processbetweenthe(text-)VAEandconditionalsampler.
backfire when the assumption motivating the design does Inonealternation,onewouldpredictthemeanandstan-
not hold, i.e., using specifics about camera parameters for darddeviationfromthetextcaptionandoptimizethetext-
reconstruction. We consider a more flexible source of pri- VAE branch for depth by minimizing a loss with respect
ors â€“ language â€“ that is closely tied to semantics, and of- togroundtruthonthedepthmapsampledusingastandard
ten shape (and functionality) [4, 31, 32]. Consider a text Gaussian (similar to traditional VAEs). In the other alter-
description of â€œA bedroom with a bed and a tableâ€ as in nation,onewouldstillusethemeanandstandarddeviation
Fig.1: Onecanimagineaprobable3Dscenecontaininga predictedbythetext-VAE,butinstead, usetheconditional
bed and a table as the primary objects. In fact, there exist sampler to â€œselectâ€ a specific depth map compatible with
infinitelymany3Dscenescompatiblewiththedescription, theimage,andagain,minimizealossontheoutputdepth.
asthereareambiguitiesintermsofthescenelayoutandthe Note:thatdependingonthealternation,eitherthetext-VAE
precise shape of the bed and table. Yet, one may surmise or the conditional sampler is frozen. At test-time, one no
that the scale of the scene is closely related to the objects longerneedstosamplefromtheGaussianandmaydirectly
(andtheirtypicalsizes)populatingit. Thislendstoaprior predictdepthusingthetext-VAEwiththeconditionalsam-
thatisspecificforagivenscene,yet,genericenoughwith- pler (see Fig. 2). In another mode, one may use the text-
outassumptionsonthecamerausedortheshapeswithinthe VAEalonetogenerateplausiblescenesforagivencaption.
imaged3Dscene. Ourcontributionsareasfollows:(i)Weproposeavari-
Hence,thequestionathandbecomeswhethertwoinher- ational framework that leverages complementary strengths
entlyambiguousmodalities(cameraimageandtextdescrip- of two inherently ambiguous modalities for monocular
tions) can be exploited for their complementary strengths: depthestimation;wetermourapproach,WorDepth.(ii)We
Intheimage,onecanobservethelayoutandobjectshapes introduce an image-based conditional sampler that models
populating the 3D scene; in a text caption, one has strong theuseoflanguageasaconditionalprior. (iii)Weachieve
priorsaboutthescale(andcoarseshapes)ofthescene. Our the state-of-the-art on indoor (NYU Depth V2 [58]) and
workaimstoresolvetherespectiveambiguitiesofthetwo outdoor (KITTI [20]) benchmarks. (iv) To the best of our
modalities by using language to reduce the solution space knowledge,wearethefirsttotreatlanguageasavariational
toyieldmetric-scaledreconstructionsas2.5Ddepthmaps. priorformonoculardepthestimation.
To test the feasibility of this approach, we consider the
2.RelatedWork
ill-posed inverse problem of monocular depth estimation,
where one predicts a depth map from a single image. In- Monoculardepthestimation trainsbyminimizingloss
steadofusingjustanimage,wealsoassumeatextdescrip- between depth predictions and ground-truth depth maps
tionorcaptiondescribingthe3Dscenecapturedwithinthe [2,7,17,35,46,52,54,61,66,78,80,84,86].Specifically,
image. Note that we do not make any assumption regard- DORN [16] employs a spacing-increasing discretization
ingthesourceofthedescription, i.e., itcanbedictatedby strategyfordepthestimationasanordinalregressionprob-
humans or generated by a model. But for practicality, we lem. AdaBins [2] introduces a transformer block that seg-
useanimagecaptioner(ExpansionNetv2[25])togenerate ments the depth range into adaptive bins. ASTransformer
abrief,concisedescriptionoftheimage. [7] incorporates an Attention-based Up-sample Block to
Toexploittheinherentambiguityoftextcaptions,where enhance detailed texture features. DepthFormer [40] em-
asingledescriptioncangenerateinfinitelymany3Dscenes, ploys hierarchical aggregation and heterogeneous interac-
we choose to encode the caption using a variational auto- tion modules for effective feature affinity and modeling.
encoder (VAE) as a mean and standard deviation of the RPSF [47] presents a differentiable model of the aper-
plausible scene layout distribution. By sampling a noise ture mask. However, deriving semantics solely from vi-
vector from a standard Gaussian and using the reparame- sualcuesischallengingbecauseofscaleambiguityandthe
terization trick customary in VAEs, we can draw from the limited size of fully annotated training datasets. We use
latent distribution and decode it into a metric-scaled depth language as a prior to ground predictions to metric scale.
map. Yet, to choose a particular depth map amongst the When ground-truth depth is not available, self-supervised
many possible, one must rely on the image. This is facili- approaches [3, 15, 27, 36, 51, 62â€“64, 70, 85, 94, 96, 100]
tatedbyaconditionalsamplerthatpredictsthenoisevector rely on geometric constraints, often established via from
from the given image in place of the one sampled from a various modalities, including lidar [44, 50, 67â€“69, 72, 79]
Gaussian to be used in the reparameterization step. Con- and radar [59], or through deliberate design. Arising from
sequently, thissubstitutionenablesonetosamplethemost training,ifdoneatalargescale,isaprioronthescenethat
probabledepthmap,adheringtothescenearrangementand canbeexploitedforsemantictasks[33]. Ontheotherhand,
objectshapesobservedintheimage, fromthelearneddis- weconsiderlanguageasasemanticpriortoenhancetheef-
tribution.Thisnaturallylendstoanalternatingoptimization fectivenessofmonoculardepthestimation.Variational and generative methods focus on the am- abletokensinplaceofdiscretehuman-languagewords.Ad-
biguous nature of monocular depth estimation, many in- ditionally,VPD[97]exploitsthehigh-fidelityembeddingof
volving Diffusion or VAE models for modeling this ambi- a pre-trained text-to-image diffusion model in monocular
guity[5,10,41,56,57,73,83].DepthGen[56]usesadepth depthestimation. However,existingmethodsusingvision-
pre-traineddiffusionmodel,whichgeneratesdepthestima- language models rely on implicit modeling. Conversely,
tions conditioned on images, and shows that the model is WorDepth explicitly models language as a prior for depth
capable of generating multiple plausible depth maps when estimation and exploits strong priors regarding the size of
depth is ambiguous. DDVM [57] uses a similar approach objectsdescribedintextcaptionstobettergroundmonocu-
anddesignedatrainingpipelinethatcanproducebothdepth lardepth(oftenscaleless)tometricscale.
mapsandopticalflowoutputswithadiffusionmodel. [73]
trainedaVAEmodelthatoutputsaprobabilitydistribution 3.Method
over scene depth given an image, which can then be com-
Given an RGB image x : â„¦ âŠ‚ R2 â†’ R3, monocu-
bined with additional inputs for more accurate depth esti-
lar depth estimation aims to infer a dense depth map y :
mations. VDN[10]modelsdepthasadistributionwithits
â„¦ âŠ‚ R2 â†’ R using a parameterized function h realized
varianceinterpretedasuncertainty. TheCodeSLAMmodel +
as a neural network, i.e., y := h(Â·). We consider a super-
[5] also employed a VAE conditioned on image intensities
vised dataset D = {x(m),t(m),yâˆ—(m)}M with M sam-
for depth estimation. However, although these work ex- m=1
ples, where yâˆ— : â„¦ âŠ‚ R2 â†’ R denotesthe ground-truth
ploredtheideaofuncertaintyindepthestimation,andeven +
depthmap,andtthetextcaptiondescribingtheimage.
combined other modalities of inputs [73], none have ex-
perimentedwithlanguagepriors,andmostVAE-basedap-
3.1.Textvariationalauto-encoder
proachesuseimagestoobtainthemeanofthemodeleddis-
tribution,whichisfundamentallydifferentfromWorDepth. To incorporate language priors to monocular depth es-
timation, we first design a variational auto-encoder (VAE)
Foundationmodels[6,21,23,37,38,48,49,53,77,98,
to learn the latent distribution of possible depth maps as
104]acquireacomprehensiveunderstandingoflanguages,
described by the text caption. This VAE is comprised of
images, and other data types through pre-training under
thetextencoderfromapre-trainedvision-languagemodel,
substantial and diverse datasets, thus forming an effective
CLIP [53], which by default offers a shared latent space
baseline for downstream tasks [2, 8, 12, 39, 42, 71, 74â€“
between vision and text embeddings, followed by a multi-
76, 81, 82, 89, 92, 93, 95]. To leverage foundation mod-
layer perceptron (MLP) to estimate the mean ÂµË† âˆˆ Rd and
els for monocular depth estimation, TADP [30] uses cap-
standard deviation ÏƒË† âˆˆ Rd of the latent distribution of
tionscreatedbyAItoenhancethecorrelationbetweentext
plausible scenes based on the text encoding. Note that the
and images in diffusion-based vision models. VPD [97]
CLIPtextencoderisfrozenatalltimesandneverupdated
leveragesadiffusion-basedpipelinewithcross-attentionbe-
when training WorDepth. Specifically, given a text cap-
tweentextandimages. Dinov2[48]trainsaViT[11]with
tiont = {t ,t ,...},wefirstencodeitusingtheCLIPtext
1B parameters using an automatically built image dataset 1 2
encoder and estimate the mean and standard deviation as
undercontrastivelearningobjectives. Unlikemethodsthat
(ÂµË†,ÏƒË†) = g (t) âˆˆ R2Ã—d using a multi-layer perceptron
relyonfoundationmodelsforfeatureextraction,WorDepth Ïˆ
(MLP). To sample from the distribution parameterized by
ispotentiallymoreefficientforindustrialapplications.
ÂµË† and ÏƒË†, we first draw a noise vector Ïµ âˆˆ Rd from a stan-
Vision-language models are designed to build connec- dardGaussianÏµâˆ¼N(0,1). Then,weuseÏµtosamplefrom
tions between visual and language inputs. CLIP [53] con-
thelatentdistributionviathereparameterizationtrick[29],
ducts contrastive learning between text-image pairs, em- zË†= ÂµË†+ÏµÂ·ÏƒË†. Werefertothismoduleasatextvariational
powering various tasks like few-shot image classification auto-encoder (text-VAE). To generate a depth map yË†from
[18,87,88,101],imagesegmentation [55,99],objectde- thesamplezË†âˆˆRd,wefirstduplicatezË†alongthehorizontal
tection [55,103], and3Dperception[26,90,91,105]. In andverticalaxestoyieldadÃ—hÃ—wlatent(choiceofde-
light of the powerful emerging ability brought by recent
signtobediscussedbelowinSec.3.2)andfeeditthrough
vision-language models, some works have tried to apply a depth decoder to yield yË† = h (zË†) âˆˆ RHÃ—W, where we
Ï• +
thevision-languagemodelformonoculardepthestimation. overloadzË†asthespatiallyduplicatedlatent,andH andW
DepthCLIP [91] leverages the semantic depth response of
denotetheheightandwidthofthedepthmap,presetashy-
CLIP[53]withadepthprojectionschemetoconductzero-
perparameterstomatchthedesiredimagedimensions.
shot adaptation from the semantic language response to
Totrainourtext-VAEanddepthdecoder,weminimize
monocular depth estimation. Furthermore, [26] extends
DepthCLIPwithlearnablepromptsanddepthcodebookto L =L (yâˆ—,yË†)+Î±Â·L (ÂµË†,ÏƒË†) (1)
VAE SI KL
narrowthedepthdomaingapamongdifferentscenes. Like-
wise,[1]modifiesDepthCLIP[91]usingcontinuouslearn- withrespecttoÏˆandÏ•,whereL isthescaleinvariantloss
SIAlternating witha ratiop Prior Depth
â€œA living room
with a couch and text-VAE ğœ‡Ì‚,ğœ+ âˆˆğ‘…! Ã— ğ‘…! ğ‘§Ì‚ =ğœ‡Ì‚ +ğœ–*ğœ+ Depth
Decoder
a ceiling fan.â€ Ground Truth
TextDescription
Detached KL-Divergence ğœ–âˆ¼N(0,1) Sharedweights Pixel-wise
gradient with N(0,1) depth loss
Conditional ğœ–Ìƒâˆˆğ‘…!Ã—#Ã—$ ğ‘§Ìƒ =ğœ‡Ì‚ +ğœ–Ìƒ*ğœ+ Depth
Sampler Decoder
RGBImage Predicted Depth
Figure2.TrainingWorDepth.Webeginwithoptimizingtext-VAEbypredictingthemeanandstandarddeviationofthelatentdistribution
ofdepthmapscorrespondingtoatextcaption.WethensamplezË†fromthedistributionusingthereparameterizationtrickwithÏµâˆ¼N(0,1)
anddecodeitintoadepthmapforlosscomputation.Wethenoptimizeaconditionalsamplerbypredictingpatch-wiseÏµËœfromanimageto
samplezËœfromthelatenttoyieldoutputdepthforthelosscomputation.Thedepthdecoderisupdatedinbothalternatingsteps.
(Eq.(3)), L theKLdivergenceloss(Eq.(4))asdetailed samesize,butpopulatedwithzerosinstead.
KL
inSection3.3,andÎ±theweightoftheKLdivergenceterm. Totraintheconditionalsampler,weminimizethesame
loss(Eq.(1))asthatoftext-VAE:
3.2.Image-basedconditionalsampler
L =L (yâˆ—,yËœ)+Î²Â·L (ÂµËœ,ÏƒËœ) (2)
CS SI KL
While our text-VAE can predict plausible metric-scaled
withrespecttoÏ†andÏ•. Withabatchsizeofb,thenumber
depth maps from text captions, we are interested in the
ofÏµËœisbÃ—hÃ—w,whileÂµËœ andÏƒËœ arethesamplemeanand
depthmapcorrespondingtoaspecificimage. Todoso,we
standard deviation of ÏµËœover a batch. We impose a KL di-
treattext-VAEasthelatentpriordistributionoftheplausi-
vergencelossasregularizationsothattheestimatedÏµËœdoes
ble scene layouts. Predicting depth yËœfor a specific image
not drift from the standard Gaussian, which also serves to
x requires sampling the latent corresponding to the depth
improvetrainingstability.
mapofthe3Dscenelayoutwiththehighestlikelihoodtobe
compatiblewiththeobservedimage,i.e.,priorconditioned 3.3.TrainingLoss
on the image. To this end, we introduce an image-based
Scaleinvariantloss. Weminimizeasupervisedlossus-
conditional sampler that will predict the sample ÏµËœin place
ing ground truth yâˆ—. To improve training stability over di-
of Ïµ âˆ¼ N(0,1) drawn from the standard Gaussian. Using
versescenes,weusethescale-invariantdepthloss[13]:
thereparameterizationtrickasbefore,wewilluseÏµËœtoselect
thelatentvectorzËœtobedecodedbythedepthdecoder. 1 (cid:88) Î³ (cid:88)
L (y,yâˆ—)= e(i,j)2âˆ’ ( e(i,j))2,
Specifically, our image-based conditional sampler uti- SI N N2
e e
lizes a Swin-L transformer backbone to encode an image (i,j)âˆˆâ„¦ (i,j)âˆˆâ„¦
(3)
x âˆˆ R3Ã—HÃ—W. We chose this design to exploit the local-
wheree(i,j)=logy(i,j)âˆ’logyâˆ—(i,j),â„¦denotestheim-
ityofthetokensproducedbySwin-L.Thetokensarethen
agespace, N thenumberofpixels, y thepredicteddepth,
encodedintohÃ—w numberoflocalsamplesÏµËœâˆˆ RdÃ—hÃ—w e
andÎ³thescalingfactortocontrolthesensitivityoftheloss.
tobeusedtosamplefromthelatentdistributionofourtext-
Kullback-Leibler (KL) divergence loss. Following
VAE; in other words, we perform â€œpatch-wiseâ€ selection
[29], we employ the KL Divergence loss as a regularizer,
from latent distribution for more granular predictions. To
which biases the predicted latent distribution (parameter-
doso, weadditionallyincludeÂµË† andÏƒË† aspartofitsinput.
ized by mean Âµ and standard deviation Ïƒ) towards a stan-
WenotethatÂµË†andÏƒË†havebeendetachedfromthecomputa-
dardGaussiandistribution. WeapplytheKullback-Leibler
tionalgraphandtreatedasinput. Werefertothismoduleas
divergencelosstoÂµandÏƒasfollows:
ourconditionalsamplerÏµËœ= f (x,ÂµË†,ÏƒË†),whichaimstoes-
Ï†
timatethemostprobablelatentvariableoftext-VAE.Thus, Ïƒ2+Âµ2 1
L (Âµ,Ïƒ)=âˆ’log(Ïƒ)+ âˆ’ . (4)
thescenelayoutlatentvectorisnowgivenbyzËœ=ÂµË†+ÏµËœÂ·ÏƒË†, KL 2 2
andthepredicteddepthyËœ = h (zËœ). Asanimplementation
Ï• 3.4.OptimizingWordepth
detail, we note that skip connections from the encoder f
Ï†
are injected into h by concatenation; when training text- Training Wordepth involves optimizing text-VAE with
Ï•
VAE(Sec.3.1),featuremapsofskipconnectionsareofthe our conditional sampler: One may choose to first trainImage Text Ours Ours Error Adabins Adabins Error Ground Truth
Depth
A store with Value (m)
chairs and
tables in it.
An unmade bed
in a bedroom
with a window.
A bathroom with a
toilet, a sink and a
shower curtain. Error
(Abs Rel)
A man standing
in a kitchen next
to a counter.
A classroom with
a group of desks.
Figure 3. Qualitative results on NYU Depth V2. We compare WorDepth with AdaBins [2]. Text descriptions are generated using
ExpansionNetv2[25]. Overall,WorDepthimprovesuniformlyacrosstheimage(darkerinerrormap),implyingbetterscale. WorDepth
alsopredictsmoreaccuratedepthinregionscorrespondingtoâ€œchairsâ€,â€œwindowâ€,â€œshowercurtainâ€,â€œmanâ€,andâ€œdesksâ€,whichareobjects
specifiedbytextdescriptions.Note:Zerosinthegroundtruthdepthmapindicatetheabsenceofvaliddepthvalues.
text-VAEuntilconvergence(i.e.,optimizeforÏˆâˆ—,Ï•âˆ—),then fromastandardGaussianinstead.
freezeÏˆâˆ—,Ï•âˆ—,andfinallytraintheimage-basedconditional
sample (i.e., optimize for Ï†âˆ—). However, we find that do- 4.Experiments
ingsooftenresultsintheconditionalsamplerbeingtrapped
inasuboptimallocalminimum. Moreover, thisintroduces Datasets. We evaluate our method on indoor (NYU
theinconvenienceofanextrastageoftraining. Instead,we DepthV2[58])andoutdoor(KITTI[20])scenarios. NYU
proposeanalternatingoptimizationschemetotrainthetext- DepthV2contains480Ã—640imageswithdepthvaluesfrom
VAEwithconditionalsampler. Inonealternatingstep, we 1Ã—10âˆ’3 to10meters. Wefollow[34]forthedatasetpar-
freeze the conditional sampler and train the text-VAE and tition,whichcontains24,231trainimagesand654testim-
depthdecoderfollowingtheprocedureinSec.3.1,i.e.,pre- ages.KITTIcontains352Ã—1216imageswheredepthvalues
dictingÂµË† andÏƒË† fromtextcaptiontandusingthereparam- from1Ã—10âˆ’3 to80meters. WeadopttheEigenSplit[13]
eterizationtrickwithanÏµdrawnfromastandardGaussian consisting of 23,488 training images and 697 testing im-
tosamplethelatentvector. Inthenextalternatingstep,we ages. Following[2,86],aftercleaningoutsampleswithout
freezetext-VAEandtraintheconditionalsamplerwiththe validgroundtruth,wehave652validimagesfortesting.
depth decoder following Sec. 3.2, i.e., predicting ÂµË† and ÏƒË† NetworkArchitecture. WeusetheResNet-50[24]ver-
using the frozen text-VAE and sample from the latent dis- sionofCLIP[53]textencodertoextracttextfeatures. We
tributionusingÏµËœpredictedfromtheimage. Thesealternat- use ExpansionNet-v2 [25] for captioning images for effi-
ingstepsarerepeatedwitharatioofp(foroptimizingtext- ciency. We set the dimension d of the latent space of the
VAE)to1âˆ’p(foroptimizingtheconditionalsampler). text-VAE and image-based conditional sampler to be 128.
Asfortheimage-basedconditionalsampler,weuseaSwin-
Inference. Oncetrained, wenolongerrequiredrawing LTransformerbackbone[45]pre-trainedonImageNet[9].
ÏµfromastandardGaussian. Instead,attesttime,theinfer- Forthetext-VAE,givenCLIPfeaturesofsize1024,weuse
encestepsimplyfollowsSec.3.2. Inanothermode,ifone a3-layerMLPwithhiddendimensionsof512,256,and128
wants to generate depth maps from text captions, one can to encode text features. For the depth decoder, there are 3
discardtheconditionalsamplerbranchanddirectlysample convolutionalup-samplingandrefinementlayers.FordepthMethod Backbone Î´<1.25â†‘ Î´<1.252 â†‘ Î´<1.253 â†‘ AbsRelâ†“ log â†“ RMSEâ†“
10
DepthCLIP[91] CLIP(zero-shot) 0.394 0.683 0.851 0.388 0.156 1.167
CLIPMDE[1] CLIP 0.465 0.776 0.922 0.319 0.139 0.970
GeoNet[52] ResNet-50 0.834 0.960 0.990 0.128 0.057 0.569
DORN[16] ResNet-101 0.828 0.965 0.992 0.115 0.051 0.509
Yinetal.[80] ResNeXt-101 0.875 0.976 0.994 0.108 0.048 0.416
TransDepth[78] ViT-B 0.900 0.983 0.996 0.106 0.045 0.365
ASN[46] HRNet-48 0.890 0.982 0.996 0.101 0.044 0.377
BigtoSmall[35] DenseNet-161 0.885 0.978 0.994 0.110 0.047 0.392
DPT-Hybird[54] ViT-B 0.904 0.988 0.998 0.110 0.045 0.357
ASTransformer[7] ViT-B 0.902 0.985 0.997 0.103 0.044 0.374
AdaBins[2] EffNet-B5+ViT-mini 0.903 0.984 0.997 0.103 0.044 0.364
NeWCRFs[86] Swin-L 0.922 0.992 0.998 0.095 0.041 0.331
Yuetal.[84] Swin-L 0.921 0.990 0.998 0.093 0.040 0.331
DepthFormer[40] Swin-L 0.923 0.989 0.997 0.094 0.040 0.329
Baseline Swin-L 0.910 0.990 0.998 0.098 0.043 0.351
WorDepth Swin-L 0.932 0.992 0.998 0.088 0.038 0.317
%Improvement - +2.42% +0.02% +0.00% -10.20% -11.63% -9.69%
Table1.QuantitativeresultsonNYUDepthV2.ThebaselinemethodistodirectlytrainaSwin-Limageencoderandthedepthdecoder
withoutthehelpoflanguageprior.ImprovementreferstotheperformanceenhancementrelativetotheBaseline.
prediction, we attach 3 skip connections from the condi- specific range. We note that while existing methods often
tionalsamplertothedepthdecoderbetweencorresponding produce high fidelity shapes (i.e., ordinal relationships of
layers. When optimizing for text-VAE by our alternating points)indepthmaps,thescaletendstobeoffâ€“leadingto
optimization scheme (Sec. 3.4), we sample Ïµ âˆ¼ N(0,1) lowerÎ´ < 1.25. OurgainintheÎ´ < 1.25metricindicates
from a standard Gaussian; as an implementation detail, all thatagreaterproportionofdepthestimationsalignclosely
valuespassedfromtheskipconnectionsaresettobezero. withthegroundtruth,thankstobetterscaleestimatedbased
Hyperparameters. We use the Adam [28] optimizer on objects that populate the scene, thereby yielding depth
without weight decay. The learning rate is reduced from valuesinrangesclosertothatofgroundtruth.
3 Ã— 10âˆ’5 to 1 Ã— 10âˆ’5 by a cosine learning rate sched- Tab.2showstheresultsontheKITTIdataset,usingthe
uler. The model is trained for 50 epochs for both KITTI Eigen Split [13] partition. WorDepth also achieves state-
and NYU Depth V2 under this scheduler. Î³ for scale- of-the-art performance. Like NYU Depth V2, WorDepth
invariantlossissetto0.85,andtheweightsÎ±andÎ²forKL- improvesthethresholdaccuracyÎ´ <1.25,however,therel-
Divergencearesetto1Ã—10âˆ’3. Wesettheprobabilitypto ativeperformancegainonthismetricisnotaspronounced
optimizingtext-VAEbranchto1%. Dataaugmentationin- as on NYU Depth V2. This difference can be due to the
cludesrandomgammawithin[0.9,1.1],randombrightness wider range of object sizes and shapes that may populate
within [0.75,1.25] for NYU Depth V2 [58] and [0.9,1.1] anoutdoorscenethatareattributedtothesameequivalence
forKITTI[20],randomcolorintensitywithin[0.9,1.1]for class of an object. For example, the term â€œcarâ€ may refer
eachchannel, randomhorizontalflippingwith50%proba- to a sedan, a coupe, or a hatchback â€“ all exhibit different
bility,andrandomrotationswithin[âˆ’2.5,2.5]degrees. sizes (coupes are smaller than sedans) and shapes (hatch-
Evaluation metrics. Following [7, 43], we evaluate backshaveanelevatedandconnecttrunk). Whiletextcap-
WorDepthandbaselinemethodsquantitativelyusingmean tionsgiveflexibilitybetweengeneralityandspecificityasa
absolute relative error (Abs Rel), root mean square error prior,incaseswherecaptionstendtobevague,theexplicit
(RMSE), absolute error in log space (log ), logarithmic reliance(bymodelingasaconditionalprior)onthemmay
10
root mean square error (RMSE ) and threshold accuracy backfire,leadingtoincorrectshapesandsizes.Nonetheless,
log
(Î´ ). The evaluation metrics are summarized in the Supp. conditioning on the image resolve such cases to a degree
i
Mat. Forqualitativeresultsandcomparisons,seeFig.3and andusageofthepriorleadstomorebenefitsthanharm.
4,wheretheerrormapshowstheabsoluterelativeerror. Qualitativecomparisons. Weshowrepresentativevi-
Quantitative results. We show results on NYU Depth sualexamplescomparingWorDepthwithabaselinemethod
V2 in Tab. 1, where we improve over the baseline and AdaBins [2] on the NYU Depth V2 dataset in Fig. 3, to
existing works across all evaluation metrics. We want to highlightthebenefitofthelanguageprior.
highlightthatWorDepthsignificantlyexcelsintermsofthe From the error map where brighter regions indicate
threshold accuracy Î´ < 1.25, which measures the propor- largererrors,itisevidentthatWorDepthpredictsmoreac-
tionofpredictionsdeviatingfromthegroundtruthwithina curate depth for objects mentioned in the text description,Depth
Image Value (m)
An empty street with trees A red truck is driving down a A group of cars parked in
Text
and a wall. road with trees. front of a large building.
Ours
Ours
Error
Error
(Abs Rel)
AdaBins
AdaBins
Error
Ground
Truth
Figure4. VisualizationofdepthestimationsonKITTI.ComparedwithAdaBins[2],WorDepthimprovesuniformlyacrosstheimage
(darker in error map), implying better scale. WorDepth also predicts more accurate depth in regions corresponding to â€œwallâ€, â€œtreesâ€,
â€œbuildingâ€,whichareobjectsspecifiedbytextdescriptions.Note:Zerosingroundtruthdepthindicatetheabsenceofvaliddepthvalues.
like â€œchairs and tablesâ€ in the first row, â€œa windowâ€ in the specificrangewhileignoringitwhenitfallswithinanother
second row, â€œa shower curtainâ€ in the third row, â€œa manâ€ range. Theresultingpriorembeddedinthetextdescription
in the fourth row, and â€œa group of desksâ€ in the last row. mayconveymorescaleinformationthaninitiallyapparent.
Note that errors maps of WorDepth shows improvement Optimizing with different alternation ratios. As a
uniformly across the image regions; this implies that our sensitivitystudy,weinvestigatehowdifferentratiosofalter-
methodestimatesabetterscalethanexistingones,thanksto natingoptimizationstepsbetweentext-VAEandconditional
priorsaboutobjectsize(andcoarseshapes)fromtextcap- sampler have an effect on the performance of WorDepth.
tions. Knowingthatacertainobjectexistswithinanimage We find that optimizing text-VAE with a lower ratio will
reducestheproblemtoâ€œplacingâ€theobjectinthe3Dscene lead to a more deterministic model, which is anticipated.
based on its shape and location in the image. We showed On the other hand, optimizing text-VAE more frequently
thatscalecanbeinferredfromlanguage,whichcannarrow enables the model to learn a better variational prior on the
downthesolutionspaceofdepthprediction,leadingtoim- depthmapscorrespondingtotextcaptions,whichfacilitates
provedaccuracy. the generation of diverse prior depth maps. However, this
A similar pattern is also evident in KITTI (Fig. 4). Ex- comes at the cost of training time as the conditional sam-
amplesincludeimprovedaccuracyforâ€œawallâ€showninthe pler takes more steps to converge and, given a fixed num-
firstcolumn,â€œtreesâ€inthesecondcolumn,andâ€œagroupof ber of steps, results in more blurry predictions. We iden-
carsâ€alongsideâ€œalargebuildingâ€inthelastcolumn. This tifytheratioat1%inupdatingtext-VAEtobethebestem-
observation is intriguing because, for example, the text â€œa pirically (Tab. 3). Ratios exceeding 10% notably degrades
wallâ€ is ambiguous by itself, especially in outdoor scenes, performancegivenafixedtraininglengthbecauseoffewer
wherethewallcouldbeanysizeordistanceawayfromthe updates to the sampler. Note that at 100%, where we do
camera, 1 or 100 meters. However, the text description of not condition the image, caption to depth generation still
a scene, either from a human annotator or a deep neural yieldsreasonableresultsasthetextcaptionsproduceplausi-
network, inherently carries biases that emphasize â€œa wallâ€ blestatisticsthatmatchthegroundtruthdepth.Ontheother
when its size (tall or wide enough) or depth falls within a hand, withoutthemodelinglanguageasavariationalpriorMethod Backbone Î´<1.25â†‘ Î´<1.252 â†‘ Î´<1.253 â†‘ AbsRelâ†“ RMSE â†“ RMSEâ†“
log
CLIPMDE[1] CLIP 0.550 0.830 0.938 0.303 0.119 6.322
DORN[16] ResNet-101 0.932 0.984 0.995 0.072 0.120 2.727
Yinetal.[80] ResNeXt-101 0.938 0.990 0.998 0.072 0.117 3.258
TransDepth[78] ViT-B 0.956 0.994 0.999 0.064 0.098 2.755
BigtoSmall[35] DenseNet-161 0.955 0.993 0.998 0.060 0.096 2.798
DPT-Hybird[54] ViT-B 0.959 0.995 0.999 0.062 0.092 2.573
ASTransformer[7] ViT-B 0.963 0.995 0.999 0.058 0.089 2.685
AdaBins[2] EffNet-B5+ViT-mini 0.964 0.995 0.999 0.058 0.089 2.360
NeWCRFs[86] Swin-L 0.974 0.997 0.999 0.052 0.079 2.129
Yuetal.[84] Swin-L 0.972 0.996 0.999 0.054 0.081 2.134
DepthFormer[40] Swin-L 0.975 0.997 0.999 0.052 0.079 2.143
Baseline Swin-L 0.969 0.996 0.999 0.054 0.085 2.343
WorDepth Swin-L 0.979 0.998 0.999 0.049 0.074 2.039
%Improvement - +1.03% +0.20% +0.00% -9.26% -12.94% -12.97%
Table2. QuantitativeresultsonKITTIEigenSplit. ThebaselinemethodistodirectlytrainaSwin-Limageencoderandthedepth
decoderwithoutthehelpoflanguageprior.ImprovementistherelativeperformancegaincomparedwiththeBaseline.
p Î´<1.25â†‘Î´<1.252â†‘Î´<1.253â†‘AbsRelâ†“log â†“RMSEâ†“ Method Î´<1.25â†‘Î´<1.252â†‘Î´<1.253â†‘AbsRelâ†“log â†“RMSEâ†“
10 10
0% 0.929 0.990 0.998 0.091 0.039 0.323 Adabins 0.771 0.944 0.983 0.159 0.068 0.476
1% 0.932 0.992 0.998 0.088 0.038 0.317 DepthFormer 0.815 0.970 0.993 0.137 0.059 0.408
10% 0.930 0.991 0.998 0.090 0.039 0.322 Baseline 0.803 0.965 0.990 0.141 0.062 0.427
50% 0.763 0.942 0.987 0.163 0.068 0.527 WorDepth 0.833 0.976 0.994 0.123 0.054 0.376
90% 0.642 0.906 0.975 0.211 0.089 0.687
100% 0.590 0.889 0.973 0.225 0.097 0.746 Table4. Zero-shotgeneralizationtoSUN-RGBD.Themodels
aretrainedontheNYUDepthV2andtestingontheSun-RGBD
Table3. Sensitivitytodifferentratiosofalternatingoptimiza- withoutanyfine-tuning.
tionstepsbetweentext-VAEandconditionalsampleronNYU
DepthV2. pdenotesprobabilityofoptimizingtext-VAE.While
morestepsspentontext-VAEwillyieldbettergenerativeresults, motionproblems.Theapproachisafirstinleveragingcom-
itcomesatthecostofslowerconvergenceforthesampler. plementary properties of two modalities with inherent am-
biguitiesforthe3Dreconstruction,toaddressthedeficitsin
(at0%,wherewetrainbothtext-VAEandconditionalopti- one another. We show that by exploiting the layout/scene
mizerjointlyasadirectmapfromsingleimageandcaption ambiguityinlanguageasastrengthviaourvariationalap-
todepth),performancedegradetodothelackoftheprior. proach, we can ground predictions to metric scale. This
Zero-shot Generalization. Given the smaller domain opens up new avenue in how one can address the issue of
gap in language descriptions across different scenes com- scalein3Dreconstructionaswellasprovideadirectframe-
pared to images, we conduct a zero-shot transfer experi- worktoextendingthemanyworksthatcurrentlyarelimited
ment to highlight our improved generalization ability. We torelativeorscalelessdepthpredictions.
train the model on the NYU Depth V2 [58] and test it
Limitations. Generic regularizers typically yield little
on the Sun-RGBD [60] without fine-tuning. As shown in
gains, but do little harm; specific regularizers can provide
Tab.4, WorDepthoutperformsbaselinemethodsbyasub-
larger boosts but are limited in their applications. While
stantial margin, indicating the transferability of language
usinglanguageasapriorgivesflexibilitybetweenthetwo,
priorswhichunderscorestherobustnessoftextdescriptions
specificity in the caption controls the degree of regulariza-
in handling scene variability. This suggests that language
tion imposed. Naturally, vague captions give little to no
descriptions may offer a more stable basis for generaliza-
information on object shape or size, so there is little to be
tionacrossdiversedatadomainsthandirectvisualsignals.
gained;specific,butincorrectcaptionsmaymisfire,barring
anymaliciousintent. AsWorDepthreliesonthequalityof
5.Discussion
thecaption,itissusceptibletoinaccuraciesstemmingfrom
descriptions provided by the image captioner. Its ease of
Inthisstudy,weseektoanswerthequestionofwhether
usealsoopensupvulnerabilitiesfrommalicioususerswho
languagecanbeusedtocalibratethelearnedscenedistribu-
maychoosecaptionstosteerpredictionsincorrectly.
tion to true real-world statistics. The answer is yes, which
is valuable for circumventing the long-standing problem Acknowledgments. This work was supported by NSF
of scale ambiguity in monocular depth or structure-from- 2112562.References [13] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale
[1] DylanAutyandKrystianMikolajczyk.Learningtoprompt
deepnetwork. Advancesinneuralinformationprocessing
clipformonoculardepthestimation:Exploringthelimitsof
systems,27,2014. 4,5,6,1
humanlanguage.InProceedingsoftheIEEE/CVFInterna-
[14] XiaohanFeiandStefanoSoatto. Visual-inertialobjectde-
tionalConferenceonComputerVision, pages2039â€“2047,
tectionandmapping. InProceedingsoftheEuropeancon-
2023. 3,6,8
ferenceoncomputervision(ECCV),pages301â€“317,2018.
[2] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka.
1
Adabins:Depthestimationusingadaptivebins.InProceed-
[15] Xiaohan Fei, Alex Wong, and Stefano Soatto. Geo-
ingsoftheIEEE/CVFConferenceonComputerVisionand
supervisedvisualdepthprediction. IEEERoboticsandAu-
PatternRecognition,pages4009â€“4018,2021. 2,3,5,6,7,
tomationLetters,4(2):1661â€“1668,2019. 1,2
8,1
[16] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
[3] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao
manghelich, and Dacheng Tao. Deep ordinal regression
Li,LeZhang,ChunhuaShen,Ming-MingCheng,andIan
networkformonoculardepthestimation.InProceedingsof
Reid. Unsupervised scale-consistent depth learning from
theIEEEconferenceoncomputervisionandpatternrecog-
video. International Journal of Computer Vision, 129(9):
nition,pages2002â€“2011,2018. 2,6,8
2548â€“2564,2021. 2
[17] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
[4] IrvingBiedermanandGinnyJu.Surfaceversusedge-based
manghelich, and Dacheng Tao. Deep ordinal regression
determinantsofvisualrecognition. Cognitivepsychology,
networkformonoculardepthestimation.InProceedingsof
20(1):38â€“64,1988. 2
theIEEEconferenceoncomputervisionandpatternrecog-
[5] Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan nition,pages2002â€“2011,2018. 2
Leutenegger,andAndrewJDavison. Codeslamâ€”learning
[18] PengGao, ShijieGeng, RenruiZhang, TeliMa, Rongyao
a compact, optimisable representation for dense visual
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
slam. InProceedingsoftheIEEEconferenceoncomputer
Clip-adapter: Better vision-language models with feature
visionandpatternrecognition,pages2560â€“2568,2018. 3
adapters. arXivpreprintarXiv:2110.04544,2021. 3
[6] MathildeCaron,HugoTouvron,IshanMisra,HerveÂ´JeÂ´gou,
[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian
Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Reid. Unsupervisedcnnforsingleviewdepthestimation:
Emergingpropertiesinself-supervisedvisiontransformers.
Geometrytotherescue. InComputerVisionâ€“ECCV2016:
InProceedingsoftheIEEE/CVFinternationalconference
14th European Conference, Amsterdam, The Netherlands,
oncomputervision,pages9650â€“9660,2021. 3
October 11-14, 2016, Proceedings, Part VIII 14, pages
[7] Wenjie Chang, Yueyi Zhang, and Zhiwei Xiong. 740â€“756.Springer,2016. 1
Transformer-based monocular depth estimation with
[20] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewe
attention supervision. In 32nd British Machine Vision
readyforautonomousdriving? thekittivisionbenchmark
Conference(BMVC2021),2021. 2,6,8,1 suite. In 2012 IEEE conference on computer vision and
[8] Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, patternrecognition,pages3354â€“3361.IEEE,2012.2,5,6,
ZiyaoZeng,andJianboShi. iquery:Instrumentsasqueries 1
for audio-visual sound separation. In Proceedings of the [21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
IEEE/CVF Conference on Computer Vision and Pattern
Singh,KalyanVasudevAlwala,ArmandJoulin,andIshan
Recognition,pages14675â€“14686,2023. 3 Misra. Imagebind: Oneembeddingspacetobindthemall.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, InProceedingsoftheIEEE/CVFConferenceonComputer
and Li Fei-Fei. Imagenet: A large-scale hierarchical im- VisionandPatternRecognition,pages15180â€“15190,2023.
agedatabase. In2009IEEEconferenceoncomputervision 3
andpatternrecognition,pages248â€“255.Ieee,2009. 5 [22] CleÂ´ment Godard, Oisin Mac Aodha, and Gabriel J Bros-
[10] Georgi Dikov and Joris van Vugt. Variational depth net- tow. Unsupervised monocular depth estimation with left-
works:Uncertainty-awaremonocularself-superviseddepth rightconsistency.InProceedingsoftheIEEEconferenceon
estimation. InEuropeanConferenceonComputerVision, computer vision and pattern recognition, pages 270â€“279,
pages43â€“60.Springer,2022. 3 2017. 1
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, [23] ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,Xi-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, anzhengMa,JiamingHan,KexinChen,PengGao,Xianzhi
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Li, Hongsheng Li, et al. Point-bind & point-llm: Align-
Sylvain Gelly, et al. An image is worth 16x16 words: ing point cloud with multi-modality for 3d understand-
Transformersforimagerecognitionatscale.arXivpreprint ing, generation, andinstructionfollowing. arXivpreprint
arXiv:2010.11929,2020. 3 arXiv:2309.00615,2023. 3
[12] YimingDou,FengyuYang,YiLiu,AntonioLoquercio,and [24] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
AndrewOwens.Tactile-augmentedradiancefields.InPro- Deepresiduallearningforimagerecognition. InProceed-
ceedingsoftheIEEE/CVFConferenceonComputerVision ingsoftheIEEEconferenceoncomputervisionandpattern
andPatternRecognition,2024. 3 recognition,pages770â€“778,2016. 5[25] JiaChengHu,RobertoCavicchioli,andAlessandroCapo- and Cewu Lu. From isolated islands to pangea: Unify-
tondi. Expansionnet v2: Block static expansion in fast ingsemanticspaceforhumanactionunderstanding. arXiv
end to end training for image captioning. arXiv preprint preprintarXiv:2304.00553,2023. 3
arXiv:2208.06551,2022. 2,5 [40] ZhenyuLi, ZehuiChen, XianmingLiu, andJunjunJiang.
[26] XuetingHu,CeZhang,YiZhang,BowenHai,KeYu,and Depthformer: Exploiting long-range correlation and local
ZhihaiHe. Learningtoadaptclipforfew-shotmonocular informationforaccuratemonoculardepthestimation.arXiv
depthestimation. arXivpreprintarXiv:2311.01034,2023. preprintarXiv:2203.14211,2022. 2,6,8
3 [41] Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-
[27] PanJi,RunzeLi,BirBhanu,andYiXu. Monoindoor: To- Phuoc, Douglas Lanman, James Tompkin, and Lei Xiao.
wardsgoodpracticeofself-supervisedmonoculardepthes- Gaufre:Gaussiandeformationfieldsforreal-timedynamic
timation for indoor environments. In Proceedings of the novelviewsynthesis,2023. 3
IEEE/CVF International Conference on Computer Vision, [42] Yiqing Liang, Eliot Laidlaw, Alexander Meyerowitz, Sri-
pages12787â€“12796,2021. 2 nathSridhar,andJamesTompkin. Semanticattentionflow
[28] DiederikPKingmaandJimmyBa. Adam: Amethodfor fields for monocular dynamic scene decomposition. In
stochastic optimization. arXiv preprint arXiv:1412.6980, ProceedingsoftheIEEE/CVFInternationalConferenceon
2014. 6 ComputerVision(ICCV),2023. 3
[43] Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte,
[29] DiederikPKingmaandMaxWelling. Auto-encodingvari-
ational bayes. arXiv preprint arXiv:1312.6114, 2013. 3, and Luc Van Gool. Va-depthnet: A variational ap-
proach to single image depth prediction. arXiv preprint
4
arXiv:2302.06556,2023. 6,1
[30] Neehar Kondapaneni, Markus Marks, Manuel Knott,
[44] Tian Yu Liu, Parth Agrawal, Allison Chen, Byung-Woo
RogeÂ´rioGuimaraËœes,andPietroPerona. Text-imagealign-
Hong,andAlexWong. Monitoreddistillationforpositive
ment for diffusion-based perception. arXiv preprint
congruent depth completion. In European Conference on
arXiv:2310.00031,2023. 3
ComputerVision,pages35â€“53.Springer,2022. 2
[31] BarbaraLandau, LindaBSmith, andSusanSJones. The
[45] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
importance of shape in early lexical learning. Cognitive
Zhang,StephenLin,andBainingGuo. Swintransformer:
development,3(3):299â€“321,1988. 2
Hierarchicalvisiontransformerusingshiftedwindows. In
[32] Barbara Landau, Linda Smith, and Susan Jones. Object
ProceedingsoftheIEEE/CVFinternationalconferenceon
shape,objectfunction,andobjectname.Journalofmemory
computervision,pages10012â€“10022,2021. 5
andlanguage,38(1):1â€“27,1998. 2
[46] XiaoxiaoLong,ChengLin,LingjieLiu,WeiLi,Christian
[33] DongLao,FengyuYang,DanielWang,HyoungseobPark,
Theobalt, Ruigang Yang, and Wenping Wang. Adaptive
SamuelLu,AlexWong,andStefanoSoatto. Ontheviabil-
surfacenormalconstraintfordepthestimation.InProceed-
ityofmonoculardepthpre-trainingforsemanticsegmenta-
ings of the IEEE/CVF international conference on com-
tion. arXivpreprintarXiv:2203.13987,2022. 2
putervision,pages12849â€“12858,2021. 2,6
[34] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and
[47] Mazen Mel, Muhammad Siddiqui, and Pietro Zanut-
IlHongSuh. Frombigtosmall: Multi-scalelocalplanar
tigh. End-to-end learning for joint depth and image
guidance for monocular depth estimation. arXiv preprint
reconstruction from diffracted rotation. arXiv preprint
arXiv:1907.10326,2019. 5
arXiv:2204.07076,2022. 2
[35] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and [48] MaximeOquab,TimotheÂ´eDarcet,TheÂ´oMoutakanni,Huy
IlHongSuh. Frombigtosmall: Multi-scalelocalplanar
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
guidance for monocular depth estimation. arXiv preprint
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.
arXiv:1907.10326,2019. 2,6,8
Dinov2: Learning robust visual features without supervi-
[36] Boying Li, Yuan Huang, Zeyu Liu, Danping Zou, and sion. arXivpreprintarXiv:2304.07193,2023. 3
Wenxian Yu. Structdepth: Leveraging the structural reg- [49] Zixuan Pan, Zihao Wei, and Andrew Owens. Efficient
ularities for self-supervised indoor depth estimation. In vision-language pre-training by cluster masking. In Pro-
ProceedingsoftheIEEE/CVFInternationalConferenceon ceedingsoftheIEEE/CVFConferenceonComputerVision
ComputerVision,pages12663â€“12673,2021. 2 andPatternRecognition,2024. 3
[37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. [50] Hyoungseob Park, Anjali Gupta, and Alex Wong. Test-
Blip: Bootstrapping language-image pre-training for uni- time adaptation for depth completion. In Proceedings of
fied vision-language understanding and generation. In theIEEE/CVFConferenceonComputerVisionandPattern
International Conference on Machine Learning, pages Recognition,2024. 2
12888â€“12900.PMLR,2022. 3 [51] RuiPeng,RonggangWang,YawenLai,LuyangTang,and
[38] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Yangang Cai. Excavating the potential capacity of self-
Blip-2: Bootstrapping language-image pre-training with supervisedmonoculardepthestimation. InProceedingsof
frozen image encoders and large language models. arXiv the IEEE/CVF International Conference on Computer Vi-
preprintarXiv:2301.12597,2023. 3 sion,pages15560â€“15569,2021. 2
[39] Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Yiming Dou, [52] XiaojuanQi,RenjieLiao,ZhengzheLiu,RaquelUrtasun,
YikunJi,JunyiZhang,YixingLi,JingruTan,XudongLu, andJiayaJia. Geonet: Geometricneuralnetworkforjointdepthandsurfacenormalestimation. InProceedingsofthe depthprediction. InProceedingsoftheIEEE/CVFConfer-
IEEEConferenceonComputerVisionandPatternRecog- ence on Computer Vision and Pattern Recognition, pages
nition,pages283â€“291,2018. 2,6 5644â€“5653,2019. 2
[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [65] AlexWongandStefanoSoatto. Unsuperviseddepthcom-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, pletionwithcalibratedbackprojectionlayers. InProceed-
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- ings of the IEEE/CVF International Conference on Com-
ingtransferablevisualmodelsfromnaturallanguagesuper- puterVision,pages12747â€“12756,2021. 1
vision. In International conference on machine learning, [66] AlexWong,SafaCicek,andStefanoSoatto. Targetedad-
pages8748â€“8763.PMLR,2021. 3,5 versarialperturbationsformonoculardepthprediction.Ad-
[54] ReneÂ´Ranftl,AlexeyBochkovskiy,andVladlenKoltun.Vi- vancesinneuralinformationprocessingsystems,33:8486â€“
siontransformersfordenseprediction. InProceedingsof 8497,2020. 2
the IEEE/CVF international conference on computer vi-
[67] Alex Wong, Xiaohan Fei, Stephanie Tsuei, and Stefano
sion,pages12179â€“12188,2021. 2,6,8 Soatto. Unsupervised depth completion from visual iner-
[55] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yan- tial odometry. IEEE Robotics and Automation Letters, 5
song Tang, Zheng Zhu, Guan Huang, Jie Zhou, and (2):1899â€“1906,2020. 1,2
Jiwen Lu. Denseclip: Language-guided dense pre-
[68] Alex Wong, Safa Cicek, and Stefano Soatto. Learning
diction with context-aware prompting. arXiv preprint
topologyfromsyntheticdataforunsuperviseddepthcom-
arXiv:2112.01518,2021. 3
pletion.IEEERoboticsandAutomationLetters,6(2):1495â€“
[56] SaurabhSaxena,AbhishekKar,MohammadNorouzi,and
1502,2021.
DavidJFleet. Monoculardepthestimationusingdiffusion
[69] AlexWong, XiaohanFei, Byung-WooHong, andStefano
models. arXivpreprintarXiv:2302.14816,2023. 3
Soatto. Anadaptiveframeworkforlearningunsupervised
[57] SaurabhSaxena,CharlesHerrmann,JunhwaHur,Abhishek
depthcompletion. IEEERoboticsandAutomationLetters,
Kar,MohammadNorouzi,DeqingSun,andDavidJFleet.
6(2):3120â€“3127,2021. 2
Thesurprisingeffectivenessofdiffusionmodelsforoptical
[70] Cho-Ying Wu, Jialiang Wang, Michael Hall, Ulrich Neu-
flowandmonoculardepthestimation. AdvancesinNeural
mann, andShuochenSu. Towardpracticalmonocularin-
InformationProcessingSystems,36,2024. 3
door depth estimation. In Proceedings of the IEEE/CVF
[58] NathanSilberman,DerekHoiem,PushmeetKohli,andRob
Conference on Computer Vision and Pattern Recognition,
Fergus. Indoor segmentation and support inference from
pages3814â€“3824,2022. 2
rgbdimages. InComputerVisionâ€“ECCV2012:12thEuro-
[71] ShaokaiWuandFengyuYang.Boostingdetectionincrowd
peanConferenceonComputerVision,Florence,Italy,Oc-
analysisviaunderutilizedoutputfeatures.InProceedingsof
tober7-13,2012,Proceedings,PartV12,pages746â€“760.
theIEEE/CVFConferenceonComputerVisionandPattern
Springer,2012. 2,5,6,8,1
Recognition(CVPR),pages15609â€“15618,2023. 3
[59] Akash Deep Singh, Yunhao Ba, Ankur Sarker, Howard
[72] Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano
Zhang,AchutaKadambi,StefanoSoatto,ManiSrivastava,
Soatto,DongLao,andAlexWong. Augundo: Scalingup
and Alex Wong. Depth estimation from camera image
augmentations for unsupervised depth completion. arXiv
and mmwave radar point cloud. In Proceedings of the
preprintarXiv:2310.09739,2023. 2
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages9275â€“9285,2023. 2 [73] ZhihaoXia,PatrickSullivan,andAyanChakrabarti. Gen-
erating and exploiting probabilistic monocular depth esti-
[60] ShuranSong, SamuelPLichtenberg, andJianxiongXiao.
mates. In Proceedings of the IEEE/CVF Conference on
Sunrgb-d: Argb-dsceneunderstandingbenchmarksuite.
Computer Vision and Pattern Recognition, pages 65â€“74,
InProceedingsoftheIEEEconferenceoncomputervision
2020. 3
andpatternrecognition,pages567â€“576,2015. 8
[61] RishiUpadhyay,HowardZhang,YunhaoBa,EthanYang, [74] FengyuYangandChenyangMa. Sparseandcompletela-
Blake Gella, Sicheng Jiang, Alex Wong, and Achuta tentorganizationforgeospatialsemanticsegmentation. In
Kadambi. Enhancing diffusion models with 3d perspec- ProceedingsoftheIEEE/CVFConferenceonComputerVi-
tivegeometryconstraints. ACMTransactionsonGraphics sionandPatternRecognition,pages1809â€“1818,2022. 3
(TOG),42(6):1â€“15,2023. 2 [75] Fengyu Yang, Chenyang Ma, Jiacheng Zhang, Jing Zhu,
[62] RuoyuWang, ZehaoYu, andShenghuaGao. Planedepth: WenzhenYuan,andAndrewOwens. Touchandgo:Learn-
Self-superviseddepthestimationviaorthogonalplanes. In ing from human-collected vision and touch. Neural In-
ProceedingsoftheIEEE/CVFConferenceonComputerVi- formation Processing Systems (NeurIPS) - Datasets and
sion and Pattern Recognition, pages 21425â€“21434, 2023. BenchmarksTrack,2022.
2 [76] FengyuYang,JiachengZhang,andAndrewOwens. Gen-
[63] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and eratingvisualscenesfromtouch. InternationalConference
HongkaiYu. Sqldepth: Generalizableself-supervisedfine- onComputerVision(ICCV),2023. 3
structured monocular depth estimation. arXiv preprint [77] FengyuYang,ChaoFeng,ZiyangChen,HyoungseobPark,
arXiv:2309.00526,2023. Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit
[64] AlexWongandStefanoSoatto. Bilateralcyclicconstraint Gangopadhyay, Andrew Owens, and Alex Wong. Bind-
and adaptive regularization for unsupervised monocular ingtouchtoeverything: Learningunifiedmultimodaltac-tilerepresentations. InProceedingsoftheIEEE/CVFCon- recognition with high-frequency fusion. arXiv preprint
ferenceonComputerVisionandPatternRecognition,2024. arXiv:2111.10332,2021. 3
3 [90] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
[78] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and pengMiao, BinCui, YuQiao, PengGao, andHongsheng
Elisa Ricci. Transformer-based attention networks for Li. Pointclip: Pointcloudunderstandingbyclip. InPro-
continuous pixel-wise prediction. In Proceedings of the ceedingsoftheIEEE/CVFConferenceonComputerVision
IEEE/CVF International Conference on Computer vision, andPatternRecognition,pages8552â€“8562,2022. 3
pages16269â€“16279,2021. 2,6,8 [91] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li.
[79] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense Can language understand depth? In Proceedings of the
depthposterior(ddp)fromsingleimageandsparserange. 30thACMInternationalConferenceonMultimedia,pages
InProceedingsoftheIEEE/CVFConferenceonComputer 6868â€“6874,2022. 3,6
VisionandPatternRecognition,pages3353â€“3362,2019. 2 [92] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
[80] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En- ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.
forcing geometric constraints of virtual normal for depth Llama-adapter: Efficient fine-tuning of language models
prediction. InProceedingsoftheIEEE/CVFInternational withzero-initattention. ICLR2024,2023. 3
ConferenceonComputerVision, pages5684â€“5693, 2019. [93] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin,
2,6,8 Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei
[81] ChenyuYou,WeichengDai,FenglinLiu,YifeiMin,Hao- Chang,PengGao,etal.Mathverse:Doesyourmulti-modal
ran Su, Xiaoran Zhang, Xiaoxiao Li, David A Clifton, llmtrulyseethediagramsinvisualmathproblems? arXiv
Lawrence Staib, and James S Duncan. Mine your own preprintarXiv:2403.14624,2024. 3
anatomy: Revisitingmedicalimagesegmentationwithex- [94] ChaoqiangZhao,YouminZhang,MatteoPoggi,FabioTosi,
tremely limited labels. arXiv preprint arXiv:2209.13476, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and
2022. 3 Stefano Mattoccia. Monovit: Self-supervised monocular
[82] Chenyu You, Weicheng Dai, Yifei Min, Lawrence Staib, depthestimationwithavisiontransformer. In2022Inter-
and James S Duncan. Implicit anatomical rendering for nationalConferenceon3DVision(3DV),pages668â€“678.
medicalimagesegmentationwithstochasticexperts. InIn- IEEE,2022. 2
ternationalConferenceonMedicalImageComputingand [95] HanbinZhao, FengyuYang, XingheFu, andXiLi. Rbc:
Computer-AssistedIntervention,pages561â€“571.Springer, Rectifying the biased context in continual semantic seg-
2023. 3 mentation. ECCV,2022. 3
[83] ChenyuYou,WeichengDai,YifeiMin,FenglinLiu,David [96] WangZhao,ShaohuiLiu,YezhiShu,andYong-JinLiu.To-
Clifton, S Kevin Zhou, Lawrence Staib, and James Dun- wardsbettergeneralization:Jointdepth-poselearningwith-
can. Rethinkingsemi-supervisedmedicalimagesegmenta- outposenet. InProceedingsoftheIEEE/CVFConference
tion:Avariance-reductionperspective.AdvancesinNeural onComputerVisionandPatternRecognition,pages9151â€“
InformationProcessingSystems,36,2024. 3 9161,2020. 2
[84] Shangbin Yu, Renyan Zhang, Shuaiye Ma, and Xinfang [97] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu,
Jiang. Monoculardepthestimationnetworkbasedonswin Jie Zhou, and Jiwen Lu. Unleashing text-to-image dif-
transformer.InJournalofPhysics:ConferenceSeries,page fusion models for visual perception. arXiv preprint
012019.IOPPublishing,2023. 2,6,8 arXiv:2303.02153,2023. 3
[85] ZehaoYu,LeiJin,andShenghuaGao.P2net:Patch-match [98] Chenhao Zheng, Jieyu Zhang, Aniruddha Kembhavi, and
andplane-regularizationforunsupervisedindoordepthes- Ranjay Krishna. Iterated learning improves composition-
timation. In European Conference on Computer Vision, ality in large vision-language models. In Proceedings of
pages206â€“222.Springer,2020. 2 theIEEE/CVFConferenceonComputerVisionandPattern
[86] WeihaoYuan,XiaodongGu,ZuozhuoDai,SiyuZhu,and Recognition,2024. 3
PingTan.Neuralwindowfully-connectedcrfsformonocu- [99] Chong Zhou, Chen Change Loy, and Bo Dai. Dense-
lardepthestimation.InProceedingsoftheIEEE/CVFCon- clip: Extract free dense labels from clip. arXiv preprint
ferenceonComputerVisionandPatternRecognition,pages arXiv:2112.01071,2021. 3
3916â€“3925,2022. 2,5,6,8 [100] JunshengZhou,YuwangWang,KaihuaiQin,andWenjun
[87] Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Zeng. Moving indoor: Unsupervised video depth learn-
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. ing in challenging environments. In Proceedings of the
Tip-adapter: Training-free clip-adapter for better vision- IEEE/CVF International Conference on Computer Vision,
language modeling. arXiv preprint arXiv:2111.03930, pages8618â€“8627,2019. 2
2021. 3 [101] KaiyangZhou,JingkangYang,ChenChangeLoy,andZi-
[88] RenruiZhang,LongtianQiu,WeiZhang,andZiyaoZeng. weiLiu. Learningtopromptforvision-languagemodels.
Vt-clip: Enhancing vision-language models with visual- arXivpreprintarXiv:2109.01134,2021. 3
guidedtexts. arXivpreprintarXiv:2112.02399,2021. 3 [102] Tinghui Zhou, Matthew Brown, Noah Snavely, and
[89] RenruiZhang,ZiyaoZeng,ZiyuGuo,XinbenGao,Kexue DavidGLowe. Unsupervisedlearningofdepthandego-
Fu, and Jianbo Shi. Dspoint: Dual-scale point cloud motionfromvideo. InProceedingsoftheIEEEconferenceon computer vision and pattern recognition, pages 1851â€“
1858,2017. 1
[103] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip
KraÂ¨henbuÂ¨hl, andIshanMisra. Detectingtwenty-thousand
classes using image-level supervision. arXiv preprint
arXiv:2201.02605,2022. 3
[104] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui,
HongFaWang,YatianPang,WenhaoJiang,JunwuZhang,
Zongwei Li, et al. Languagebind: Extending video-
language pretraining to n-modality by language-based se-
manticalignment. arXivpreprintarXiv:2310.01852,2023.
3
[105] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,Ziyao
Zeng,ZipengQin,ShanghangZhang,andPengGao.Point-
clipv2:Promptingclipandgptforpowerful3dopen-world
learning. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision, pages2639â€“2650, 2023.
3WorDepth: Variational Language Prior for Monocular Depth Estimation
Supplementary Material
A.Evaluationmetrics. Method Î´<1.25â†‘ Î´<1.252â†‘ Î´<1.253â†‘ AbsRelâ†“ log â†“ RMSEâ†“
10
d=32 0.925 0.990 0.998 0.093 0.039 0.327
Drawingon[7,43], ourevaluationofWorDepthalong- d=64 0.928 0.990 0.998 0.090 0.039 0.325
sidecomparisonmethodsinvolvesaquantitativeassessment d=128 0.932 0.992 0.998 0.088 0.038 0.317
d=256 0.930 0.991 0.998 0.089 0.039 0.323
throughseveralmetrics. Theseincludemeanabsoluterela-
d=512 0.929 0.990 0.998 0.089 0.039 0.324
tiveerror(AbsRel),rootmeansquareerror(RMSE),abso- d=1024 0.926 0.989 0.998 0.091 0.039 0.325
luteerrorinlogspace(log ),logarithmicrootmeansquare
10
error (RMSE ) and threshold accuracy (Î´ ). The evalua-
log i Table6.Sensitivitytodifferentnumbersofhiddenvariablesd.
tionmetricsaresummarizedinTable5fordetails.
ExperimentsareconductedonNYUDepthV2.disthenumberof
hiddenvariablesdofthetext-VAE.
B.AblationonModelArchitecture
C.AdditionalVisualizationonNYUDepthV2
Weevaluatedvaryinghiddenvariablesdoftext-VAEus-
Inthissection,asillustratedinFigure5,Wepresentad-
ing the NYU Depth V2 dataset[58], shown in Table 6. A
ditionalvisualizationscomparingWorDepthwithabaseline
key consideration was ensuring the hidden space was suf-
method AdaBins [2] on the NYU Depth V2 [58] dataset,
ficiently large to encode the necessary structural and ge-
emphasizing the advantages gained from integrating the
ometric features for reconstructing depth maps. This size
language prior. Compared with AdaBins, the error map,
requirementarisesfromtheneedtopreserveessentialfea-
with its brighter regions highlighting larger errors, clearly
turesaboutthesceneâ€™sobjectsandlayoutderivedfromtext
demonstrates that WorDepth achieves more precise depth
featuresencodedbytext-VAE.
predictionsforobjectsidentifiedinthetextdescription. For
However, itâ€™s equally crucial to avoid excessively large
instance: â€œa sink and a bath tubâ€ in the first row, â€œa white
hidden variables. A relatively constrained dimensionality
bathtubâ€inthesecondrow,â€œawoodendresserâ€inthethird
actsasaformofregularization,compellingthetext-VAEto
row, â€œa bedâ€ in the fourth row, â€œa bunk bedâ€ in the fifth
focusonextractingfeaturescrucialfordepthdecoding.Ad-
row,â€œanunmadebedwithclothesontopofitâ€inthesixth
ditionally, a limited hidden dimension prompts the model
row,â€œacouchandatableâ€intheseventhrow,â€œatableand
tolearnnotjustthedistributionmeanbutalsoitsvariance.
chairsâ€ in the eighth row, â€œa blender on a counterâ€ in the
This aspect is particularly important when mapping a text
ninthrow,â€œchairsâ€inthetenthrow,andâ€œmachineontopof
description to multiple scenes, such scenesâ€™ text features
awoodentableâ€inthelastrow.
are encoded with identical distribution means but exhibit
significantvariance.
We established hidden variables d of 32, 64, 128, 256,
512,and1024fortrainingWorDepth. Itwasobservedthat
D.AdditionalVisualizationonKITTI
the optimal hidden dimension is 128, striking a balance
between capturing sufficient geometric features of scenes
Thissection,depictedinFigure6,showcasesvisualiza-
whilemaintainingeffectiveregularization. Deviatingfrom
tions of Monocular Depth Estimation in outdoor scenarios
this optimal size, either too small or too large, adversely
withtheKITTIdataset[20]usingEigenSplit[13],compar-
impactsperformance.
ingwithAdabins[2]. Duetothelimitedvarietyofobjects
inoutdoorscenes,ourmethodcapturesfewerobjectscom-
Metric Formulation
paredtoindoorscenes. However,whensalientobjectsand
AbsRel N1 e(cid:80) (i,j)âˆˆâ„¦|yâˆ—(i y,j âˆ—) (âˆ’ i,jy )(i,j)| scenesarepresentoutdoors,ourmethodgainsapreliminary
RMSE (cid:113) 1 (cid:80) (yâˆ—(i,j)âˆ’y(i,j))2 understandingoftheirscale. Thisunderstandingaidsinen-
Ne (i,j)âˆˆâ„¦
hancingmonoculardepthestimationfortheseobjects. The
log 1 (cid:80) |log (yâˆ—(i,j))âˆ’log (y(i,j))|
10 Ne (i,j)âˆˆâ„¦ 10 10 errormapâ€™sbrighterregions,whichemphasizegreaterabso-
RMSElog (cid:113) N1 e(cid:80) (i,j)âˆˆâ„¦(ln(yâˆ—(i,j))âˆ’ln(y(i,j)))2 luterelativeerrors,unequivocallyshowthatWorDepthout-
Î´ %ofy(i,j)s.t. max(y(i,j),yâˆ—(i,j))<thrâˆˆ[1.25,1.252,1.253] performs AdaBins in making more accurate depth predic-
yâˆ—(i,j) y(i,j)
tions for objects and scenes mentioned in the text descrip-
Table5. Evaluationmetricformonoculardepthestimation. y tion. Forinstance: â€œtwowhitetrucksâ€intheupperright,â€œa
denotespredictionsandyâˆ—denotesgroundtruth. womanridingascooterâ€inthelowerleft,â€œbuildingsâ€inthe
lowermiddle,andâ€œforestwithtreeâ€inthelowerleft.Image Text Ours Ours Error Adabins Adabins Error Ground Truth
A bathroom
with a sink and
a bath tub.
A white bath tub
sitting in a
bathroom.
Depth
Value (m)
A television sitting
on top of a
wooden dresser.
A man standing
next to a bed in a
bedroom.
A bunk bed
with a table in
a room.
An unmade bed
with clothes on
top of it.
A living room
with a couch
and a table. Error
(Abs Rel)
A dining room
with a table
and chairs.
A kitchen with
a blender on a
counter.
A living room
with chairs
and a table.
A machine sitting
on top of a
wooden table.
Figure5.AdditionalvisualizationofmonoculardepthestimationonNYUDepthV2.Image
A group of cars parked on the A street with cars parked on Two white trucks driving down
Text
side of a road. the side of it. a street with a building.
Ours
Depth
Value (m)
Ours
Error
AdaBins
AdaBins
Error
Ground
Truth
Image
A woman riding a scooter
Text An empty street with buildings A forest with trees and a dirt road.
down a street.
and a car parked on it.
Error
Ours
(Abs Rel)
Ours
Error
AdaBins
AdaBins
Error
Ground
Truth
Figure6.AdditionalvisualizationofmonoculardepthestimationonKITTIEigensplit.