MonoOcc: Digging into Monocular Semantic Occupancy Prediction
Yupeng Zheng1,2‚àó, Xiang Li3‚àó, Pengfei Li3, Yuhang Zheng3,
Bu Jin1,2, Chengliang Zhong3, Xiaoxiao Long4‚Ä†, Hao Zhao3, and Qichao Zhang1,2‚Ä†(cid:66)
Abstract‚ÄîMonocular Semantic Occupancy Prediction aims
to infer the complete 3D geometry and semantic information
of scenes from only 2D images. It has garnered significant
attention, particularly due to its potential to enhance the 3D
perception of autonomous vehicles. However, existing methods
rely on a complex cascaded framework with relatively limited
information to restore 3D scenes, including a dependency
on supervision solely on the whole network‚Äôs output, single-
frame input, and the utilization of a small backbone. These
challenges, in turn, hinder the optimization of the framework
and yield inferior prediction results, particularly concerning
smaller and long-tailed objects. To address these issues, we
proposeMonoOcc.Inparticular,we(i)improvethemonocular
occupancy prediction framework by proposing an auxiliary
semantic loss as supervision to the shallow layers of the
framework and an image-conditioned cross-attention module
to refine voxel features with visual clues, and (ii) employ a
distillation module that transfers temporal information and
Fig. 1. Quantitative results of semantic occupancy prediction on Se-
richerknowledgefromalargerimagebackbonetothemonoc-
manticKITTI [16] test set compared with the state-of-the-art VoxFormer-
ular semantic occupancy prediction framework with low cost
S [17] and OccFormer [18]. Note that our method outperforms the latter
of hardware. With these advantages, our method yields state-
methods in the SSC mIoU, while also achieving a significant boost on
of-the-art performance on the camera-based SemanticKITTI bothsmallobjects(bicycle,motorcycle,traffic-sign)andlong-tailedobjects
Scene Completion benchmark. Codes and models can be (truck,other-vehicle,other-ground).ComparedwithVoxFormer-S,therel-
accessed at https://github.com/ucaszyp/MonoOcc. ative percentage increase of our method on average performance, small
objects and long-tailed objects are denoted by green, yellow and orange,
respectively.
I. INTRODUCTION
3D scene understanding serves as a foundation for au- The existing cutting-edge method, VoxFormer [17], pro-
tonomous driving systems, exerting a direct influence on poses a sparse-to-dense architecture, which aggregates 2D
downstream tasks such as planning, navigation, VR [1], [2], features to voxel space with depth-based queries and com-
map construction [3], [4]. The past years have witnessed pletes the entire scene conditioned on the feature of queries.
the rapid development and significant impact of lidar-based It should be emphasized that the inaccuracies of depth
algorithms [5]‚Äì[9] in outdoor 3D scene understanding. Nev- estimation affect the accuracy of the query, leading to an
ertheless, they are often considered expensive in terms of increase in the difficulty of subsequent completion and
hardwareforautonomousvehicles.Consequently,monocular semantic parsing. Besides, all of the existing methods such
scene understanding [10]‚Äì[15] has garnered considerable as [10], [17], [19] rely solely on the supervision of 3D
attention from the robotics community due to its cost- ground truth to train the deep cascaded architecture, includ-
efficiency and rich visual information. A popular topic in ingthe2Dimagebackbone,2D-3Dviewtransformer,and3D
this domain is Semantic Occupancy Prediction, also denoted completion network, bringing challenges to the optimization
as Semantic Scene Completion (SSC) [16]. Its objective is of heterogeneous sub-modules. Additionally, these methods
to predict the semantic occupancy of each voxel throughout only utilize visual information from a single frame input,
the entire scene, encompassing both visible and occluded resulting in poor performance, particularly causing failures
regions, while relying solely on monocular observations. in predictions for small objects and long-tailed objects (see
Fig.1).
1TheStateKeyLaboratoryofMultimodalArtificialIntelligenceSystems, To this end, we first propose an image-conditioned cross-
Institute of Automation, Chinese Academy of Sciences, Beijing 100190, attention module, aiming to refine inaccurate voxel features
China,{zhangqichao2014,zhengyupeng2022}@ia.ac.cn brought by depth estimation with the extra information from
2School of Artificial Intelligence, University of Chinese Academy of
image features and introduce an auxiliary semantic loss as
Sciences,Beijing,China,
3InstituteforAIIndustryResearch(AIR),TsinghuaUniversity,China, supervision to the shallow layers of the small framework,
4DepartmentofComputerScience,theUniversityofHongKong. facilitating more efficient optimization. Secondly, we em-
‚àóEqualcontribution.
ploy a large pre-trained image backbone instead of small
‚Ä†Projectleader.
(cid:66) Correspondingtozhangqichao2014@ia.ac.cn models trained on benchmark datasets as the former arts
4202
raM
31
]VC.sc[
1v66780.3042:viXraused to assist in SSC. Recent research (e.g., [20], [21]) has Prediction (synonymous with Semantic Scene Completion)
demonstrated that large image backbones can significantly techniques have emerged. MonoScene [10] is the pioneering
enhancetheadaptabilityandgeneralityof2Dimagesemantic methodformonocularsemanticoccupancyprediction,which
segmentation.However,howtoefficientlyutilizethesemod- proposes 2D-3D feature projections along the line of sight
els in the SSC task has not been explored. Considering the to generate voxel features and utilizes the 3D UNet to
efficiencyandresourceconstraintsinreal-worldapplications, process the volumetric data. TPVFormer [30] introduces a
weproposetousemodeldistillationtogetmorecompactyet simple yet efficient tri-perspective view representation as an
efficient models that can approximate the behavior of larger alternativetotheBEVrepresentation,enablingthecaptureof
models.AsshowninFig.2,wedenotethelargermodelasa 3D structural intricacies. OccFormer [18] devises dual-path
privilegedbranch,inspiredbytheideaofprivilegedlearning transformer blocks comprising local and global transformers
which is widely recognized in the robotics community [22], to decompose the 3D processing. VoxFormer [17] replaces
[23]. This branch is designed to take temporal image frames BEVquerieswithdepth-basedproposalqueriestoaggregate
as inputs, thus mitigating uncertainty in occluded areas. As features from images and introduces an MAE-like design to
illustrated in Fig. 1, the comparison between SOTAs‚Äô SSC achieve dense occupancy completion with sparse queries.
resultsandoursonSemanticKITTI[16]testsetdemonstrates
thatourmethodachievesasignificantgainongeneral,small
III. METHOD
and long-tailed objects. An overall framework of MonoOcc is illustrated in Fig. 2
Foreasyreference,wesummarizeourcontributionsbelow. We briefly describe the sparse-to-dense monocular 3D
‚Ä¢ We propose an image-conditioned cross-attention mod- semanticoccupancypredictionpipelineofabaselinemethod
ule and semantic auxiliary loss to improve the perfor- in section III-A. Two innovations, including an image-
mance of Monocular SSC. conditionedcrossattentionanda2Dsemanticauxiliaryloss,
‚Ä¢ We propose a privileged branch with pre-training a are proposed to improve the current framework in section
larger backbone and employing a cross-view trans- III-B. To promote the performance of small objects and
former to acquire more visual cues from temporal long-tailed objects, we further propose a privileged branch
frames. by pre-training a larger image backbone and introducing a
‚Ä¢ We propose a distillation module to transfer knowledge cross view transformer to enhance temporal view features
from the privileged branch to the monocular branch. in section III-C. Finally, we propose a distillation module
‚Ä¢ We achieve SOTA performance on SemanticKITTI to transfer the knowledge from the privileged branch to the
benchmark [16] and release our codes and models. monocular branch, making a trade-off between performance
and efficiency in section III-D.
II. RELATEDWORKS
A. Sparse-to-dense Monocular 3D Semantic Occupancy
Camera-based 3D Perception. In recent years, there
Prediction
has been a growing interest in camera-based 3D sensing
techniques [24]‚Äì[27], primarily driven by the advantages Image Feature Extractor. To extract 2D feature maps
of richer visual information, ease of deployment, and cost- F t2D ‚àà Rd√óh√ów from corresponding RGB images I t, an
effectivenessofferedbycameras.Recentresearchincamera- image feature extractor Œ¶ f is constructed by employing
based 3D perception focuses on constructing BEV fea- ResNet-50 [31] as backbone and FPN [32] as neck, where
ture representations and subsequently performing various d and (h,w) represent the dimension and resolution of the
downstream tasks in the BEV space. The Lift-Splat-Shoot imagefeature,respectively.Laterwewillleverageastronger
(LSS) method [28], along with its subsequent advancements image feature extractor pre-trained on a bunch of diverse
[26], [29], serves as the archetypal technique for forward autonomous driving datasets.
projection. LSS projects image features into 3D space and Depth-based Query. Following VoxFormer [17], we gen-
aggregates them into the BEV space, incorporating depth erate a total of N d queries Q d based on the depth map
uncertainty through predicted pixel-wise depth distributions. predicted by a pre-trained depth network. Specifically, we
BEVFormer [24] represents one of the backward projection utilize pixel-wise depth to unproject pixels into 3D space,
methods, utilizing deformable attention-based spatiotempo- andthenobtaininitialoccupancybyvoxelizingthesepoints.
ral transformers to construct BEV queries and aggregate Afterward, we acquire a tractable number of basically rea-
corresponding 2D features from multiple frames into the sonable initial queries Q d by correcting initial occupancy
BEVspace.Giventhat3Doccupancyrepresentationcontains with an occupancy prediction network (LMSCNet [33]).
richer spatial information compared to BEV representation, VoxelFeatureGenerator.FollowingVoxFormer[17],the
itplaysacrucialroleintheperceptionandplanningabilities process of generating voxel features FÀÜ3D ‚ààRx√óy√óz√ód with
S
of self-driving cars. Consequently, there is a noticeable shift resolution (x,y,z) can be divided into two steps:
towards employing camera-based solutions in 3D Semantic 1) We acquire O3D ‚àà RNd√ód, the feature of visible
S
Occupancy Prediction. regions, by utilizing Q to aggregate 2D feature F2D into
d t
Camera-based 3D Semantic Occupancy Prediction. 3Dspacewithdeformablecross-attentionmechanism(DCA)
After the introduction of SemanticKITTI dataset [16], an [34]:
abundance of outdoor Single-View 3D Semantic Occupancy O3D =DCA(cid:0) Q ,F2D(cid:1) . (1)
S d tùêπ‡∑†2ùê∑
{ùë°,ùë°‚àí1,‚Ä¶} Privileged Branch
Pre-trained Improved
ùêπ‡∑® ùëá3ùê∑
Cross View
ùêº{ùë°,ùë°‚àí1,‚Ä¶} Larger Œ¶ùëù
Transformer
Occupancy
Backbone Prediction
Distillation
Module
Improved Occupancy Prediction
ùêπ2ùê∑
ùë° ùêæ,ùëâ ùêπ‡∑® ùëÜ3ùê∑ ùëåùë°
ùêºùë° EF xe ta ratu cr toe rŒ¶ùëì OcS cp ua pr as ne c- yto P-D ree dn is ce
tion
ùëÑ Im Ca rg oe
s
sC -Aon ttd ei nti to ion ned Occ Hu ep aa dncy
ùëÜùëíùëö2ùê∑
ùë°
Semantic DecoderŒ¶ùë†
MMMooonnnooocccuuulllaaarrr BBBrrraaannnccchhh
2D Semantic Loss Distillation Loss SSC Loss
ùêπ2ùê∑
ùë° Updated QueryùëÇ ùëÜ3ùê∑
Concatenate
ùêπ ùëÜ3ùê∑ ùêπ‡∑† ùëÜ3ùê∑
Deformable Deformable
Cross-Attention Self-Attention
Sparse Depth QueryùëÑùëë Mask Token ùëÄ Sparse-to-Dense Occupancy Prediction
Fig.2. Thearchitectureofourproposedframework(seesectionIIIfordetails)
ForasetoftemporalviewfeaturesF2D ‚ààRN√ód√óh√ów semantic clues. Thus, we complete the scene conditioned on
{t,t‚àí1,...}
with a valid quantity of N, O3D is obtained by an average O3D and F2D:
T S t
of aggregated feature: FÀú3D =Complete(cid:0) F3D|O3D,F2D(cid:1) . (5)
S S S t
1 (cid:16) (cid:17)
O3D = DCA Q ,F2D . (2) Thiscanbenaturallyachievedthroughthedeformablecross-
T N d {t,t‚àí1,...}
attention mechanism. Specifically, we treat the FÀÜ3D as the
2) We acquire initial voxel features F S3D ‚ààRx√óy√óz√ód of query, F t2D as the key and value, and leverage dS eformable
the whole scene by filling the occluded regions with mask cross attention to obtain the corrected image-conditioned
tokenM ‚ààRdandthenupdateF S3D toFÀÜ S3D withdeformable voxel features from the refined feature:
self-attention mechanism (DSA) [34]: (cid:16) (cid:17)
FÀú3D =DCA FÀÜ3D,F2D . (6)
FÀÜ3D =DSA(cid:0) F3D,F3D(cid:1) . (3) S S t
S S S
2D Semantic Auxiliary Loss. The occupancy prediction
Semantic Voxel Map. The predicted semantic voxel map network is a long cascaded framework with components of
Y t ‚àà RX√óY√óZ√óC is obtained by up-sampling and linear different domains including a 2D feature extractor, 2D-3D
projectionofFÀÜ S3D,where(X,Y,Z)denotestheresolutionof
cross-attention,3Dcompletionself-attention,andoccupancy
the3Dvolume,C representsthenumberofclasses,including head,whichincreasesdifficultiesofoptimization.Toaddress
non-occupied. this problem, we propose a 2D auxiliary semantic loss as
deepsupervisiontothefeatureextractor.Itprovidesashorter
B. Improved Architecture for Monocular Semantic Occu-
pathforbackpropagation,enablingbetteroptimizationofthe
pancy Prediction
feature extractor, which serves as the source of features for
Image-Conditioned Cross-Attention. VoxFormer [17]
the entire framework.
treats semantic occupancy prediction as a generative task.
Regarding the implementation of 2D semantic loss, we
The MAE-like transformer hallucinates the occluded scene
first employ a semantic decoder Œ¶ composed of convolu-
s
conditioned on the visible scene. Mathematically, given the
tional layers and a fully connected layer to predict semantic
features of the visible region O3D and the initial voxel
S map Sem2D from image feature F2D:
feature F3D, the refined features of the entire scene FÀÜ3D t t
can be acS quired by: S Sem2 tD =Œ¶ s(cid:0) F t2D(cid:1) . (7)
FÀÜ3D =Complete(cid:0) F3D|O3D(cid:1) , (4) Then we project point clouds with semantic labels to corre-
S S S
spondingimagestogeneratesparsegroundtruth.Finally,we
whereComplete(¬∑|¬∑)meanscompletetheformerconditioned
employ cross-entropy loss L between ground truth and
sem
on the latter. Reviewing the generation of O S3D, the inac- Sem2D to directly optimize the feature extractor.
t
curacy of depth estimation introduces inaccurate geometric
C. Privileged Branch
information, bringing uncertainty to the completion of the
entire scene. We believe that the features from the input Pre-training Scaled-Up Feature Extractor. Increasing
image can help correct the inaccuracies as they can provide the size of the model is an effective strategy to improvethe accuracy of dense prediction tasks. However, due to the Proportion Loss from MonoScene [10], we use Kullback-
limited training samples in SemanticKITTI [16] which only Leibler Divergence as the loss function to provide the cues
contains about 12K images, using a larger image backbone from teacher to student:
wouldleadtooverfitting.Moreover,SemanticKITTInotonly
lacks dense semantic labels but also contains multiple long- L =KL(FÀú3D||FÀú3D). (9)
distill T S
tailed classes such as other-vehicle (0.20%), truck (0.16%),
E. Training Loss
andother-ground(0.56%)whichonlyhaveverylimitedlabel
points as supervision, making it ineffective to train a larger Reviewing the training process, we employ multiple loss
backbone. functions to supervise varying depths of the network.
To cope with these two problems, we pre-train the larger ‚Ä¢ For the feature extractor, we introduce loss L sem.
imagebackbonewithmoredataofthedrivingscenario.And ‚Ä¢ For the completion network, we propose temporal dis-
we employ the InternImage-XL [20] loading the pre-trained tillation loss L distill.
model as the visual backbone Œ¶ for the privilege branch to ‚Ä¢ For the final output semantic grid map, we utilize Loss
p
extract 2D features from multiple frames of images: functionsL ssc,Ls se cam l,andLg sce ao
l
fromMonoScene[10].
(cid:16) (cid:17) The total loss function can be represented as follows:
FÀÜ2D =Œ¶ I2D , (8)
{t,t‚àí1,...} p {t,t‚àí1,...} L=Œª L +Œª L +Œª L
1 sem 2 distill 3 ssc
(10)
where t represents the t-th frame image arranged in chrono- +Œª Lsem+Œª Lgeo,
4 scal 5 scal
logical order. More details about pre-training are elaborated
where Œª ,Œª ,Œª ,Œª , and Œª are hyper-parameters.
in section IV-B. 1 2 3 4 5
Cross View Transformer. Previous work [17], [35], [36] IV. EXPERIMENTS
has demonstrated that temporal information boosts down- A. Dataset
stream 3D scene perception. When aggregating 2D features
We evaluate our method on the SemanticKITTI dataset
into 3D with deformable cross-attention, only the centroid
[16], which provides dense semantic occupancy annotations
of voxels are projected to the image as reference points.
ofalllidarscansfromtheKITTIOdometryBenchmark[41].
Limited by the voxel resolution, the deviation between the
Each lidar scan of SemanticKITTI covers a range of [0 ‚àº
real position of objects in 3D space and the voxel centroid
51.2m,‚àí25.6 ‚àº 25.6m,‚àí2 ‚àº 4.4m] ahead of the ego car.
canaffecttheoccupancypredictionofasingleframe,which
Theground-truthsemanticoccupancyisrepresentedas256√ó
can be alleviated by involving more viewpoints. To acquire
256√ó32 3D voxel grids through voxelizing aggregated lidar
as much visual information as possible, we adopt multiple
scans with 0.2m resolution. Each voxel is annotated with
frames as the inputs of the privileged branch.
20classes(19semanticclassesand1free).Theofficialsplit
To further enhance the features of temporal views, we
fortrain,validation,andtestsetsisemployed.Wereportour
introduce the Cross View Transformer (CVT) [37] to inte-
main result (Table I) on the test set and do ablation studies
grate knowledge across multi-views, which is proved to be
(Table II, III) on the validation set.
effective on dense prediction tasks such as depth estimation
Evaluation metrics. Following common practices, we re-
[37],opticalflowestimation[38],[39],andmap-viewseman-
portthemeanintersectionoverunion(mIoU)of19semantic
tic segmentation [40]. In particular, we first add positional
classes for the Semantic Occupancy Prediction task.
encoding to the independently extracted temporal features.
Then, we input pairs of adjacent features in chronological B. Implementation Details
order into the CVT for feature enhancement. The enhanced WeprovidetwoversionsofMonoOcc,namelyMonoOcc-
features are lifted to the voxel space through the deformable SandMonoOcc-L.AsshowninFig.2,MonoOcc-Semploys
cross-attention mechanism (DCA). ResNet50 as the image backbone of the monocular branch,
while MonoOcc-L replaces the ResNet50 with our pre-
D. Distillation Module
trained larger backbone. For training, we set the hyper-
In the previous subsection, we adopt a temporal view as parameters as follows: Œª = 4.0, Œª = 3.0, Œª = 2.0,
1 2 3
input and scale up the image backbone to acquire visual Œª =1.0 and Œª =0.5. We train MonoOcc-S on 4 GeForce
4 5
clues as rich as possible. However, the usage of multiple 3090GPUsandMonoOcc-Lon4A100GPUsfor20epochs.
frames as input and the scaling up of the 2D backbone sig- To pre-train the larger backbone, we choose the
nificantlyincreasecomputationalcosts,affectingdeployment InternImage-XL [20](350M parameters) as our backbone,
in autonomous driving systems. To address this, inspired by andweprocessapproximately200Ktrainingdatafromopen-
privilege learning [23], we propose a distillation module, sourceautonomousdrivingdatasetsincludingMapillaryVis-
composed of a privileged teacher branch and a monocular tas [45], KITTI-360 [46], BDD100K [47], Cityscapes [48],
student branch. The module aims to transfer the knowl- and nuImages [49]. Based on the open-source pre-trained
edge from the privileged branch, which has richer clues of model of InternImage-XL, we first train on the Mapillary
temporal information and a larger backbone prior, to the Vistas dataset, which includes 124 semantic categories with
monocular branch, resulting in performance improvement detailed annotations of road elements and long-tailed ob-
forthemonocularbranch.Specifically,inspiredbyFrustums jects,effectivelyenhancingthemodel‚ÄôsunderstandingoftheTABLEI
SEMANTICSCENECOMPLETIONRESULTSONSEMANTICKITTI[16]TESTSET.
SemanticOccupancyPrediction
Method Pub Input mIoU
LMSCNet*[33] 3DV2020 Camera 46.70 19.50 13.50 3.10 10.30 14.30 0.30 0.00 0.00 0.00 10.80 0.00 10.40 0.00 0.00 0.00 5.40 0.00 0.00 7.07
3DSketch*[42] CVPR2020 Camera 37.70 19.80 0.00 0.00 12.10 17.10 0.00 0.00 0.00 0.00 12.10 0.00 16.10 0.00 0.00 0.00 3.40 0.00 0.00 6.23
AICNet*[43] CVPR2020 Camera 39.30 18.30 19.80 1.60 9.60 15.30 0.70 0.00 0.00 0.00 9.60 1.90 13.50 0.00 0.00 0.00 5.00 0.10 0.00 7.09
JS3C-Net*[44] AAAI2021 Camera 47.30 21.70 19.90 2.80 12.70 20.10 0.80 0.00 0.00 4.10 14.20 3.10 12.40 0.00 0.20 0.20 8.70 1.90 0.30 8.97
MonoScene[10] CVPR2022 Camera 54.70 27.10 24.80 5.70 14.40 18.80 3.30 0.50 0.70 4.40 14.90 2.40 19.50 1.00 1.40 0.40 11.10 3.30 2.10 11.08
TPVFormer[30] CVPR2023 Camera 55.10 27.20 27.40 6.50 14.80 19.20 3.70 1.00 0.50 2.30 13.90 2.60 20.40 1.10 2.40 0.30 11.00 2.90 1.50 11.26
VoxFormer-S[17] CVPR2023 Camera 53.90 25.30 21.10 5.60 19.80 20.80 3.50 1.00 0.70 3.70 22.40 7.50 21.30 1.40 2.60 0.00 11.10 5.10 4.90 12.20
VoxFormer-T‚Ä†[17] CVPR2023 Camera 54.10 26.90 25.10 7.30 23.50 21.70 3.60 1.90 1.60 4.10 24.40 8.10 24.20 1.60 1.10 0.00 13.10 6.60 5.70 13.41
OccFormer[18] ICCV2023 Camera 55.90 30.30 31.50 6.50 15.70 21.60 1.20 1.50 1.70 3.20 16.80 3.90 21.30 2.20 1.10 0.20 11.90 3.80 3.70 12.32
SurroundOcc[19] ICCV2023 Camera 56.90 28.30 30.20 6.80 15.20 20.60 1.40 1.60 1.20 4.40 14.90 3.40 19.30 1.40 2.00 0.10 11.30 3.90 2.40 11.86
MonoOcc-S(Ours) ICRA2024 Camera 55.20 27.80 25.10 9.70 21.40 23.20 5.20 2.20 1.50 5.40 24.00 8.70 23.00 1.70 2.00 0.20 13.40 5.80 6.40 13.80
MonoOcc-L(Ours) ICRA2024 Camera 59.10 30.90 27.10 9.80 22.90 23.90 7.20 4.50 2.40 7.70 25.00 9.80 26.10 2.80 4.70 0.60 16.90 7.30 8.40 15.63
*representstheresultsadaptedforRGBinputsandreportedinMonoScene[10].
‚Ä†representstheresultwithtemporalinputs.
Thebestandsecond-bestperformancesarerepresentedbyboldandunderlinerespectively.
TABLEII TABLEIII
ABLATIONSTUDYONTHEEFFECTIVENESSOFEACHPROPOSED ABLATIONSTUDYONTHEEFFECTIVENESSOFSCALINGUPAND
COMPONENTONIMPROVEDMONOCULARBRANCH(VALIDATIONSET). PRE-TRAINING(VALIDATIONSET).
2DSemantic Distillation ImageConditioned Train Scaling-up Pre-training TestMEM mIoU‚Üë
row mIoU‚Üë
AuxiliaryLoss Module CrossAttn. MEM
√ó √ó 8G 12.35
1 √ó √ó √ó 16G 12.35 ‚úì √ó 12G 14.09
2 ‚úì √ó √ó 16G 13.08 ‚úì ‚úì 12G 14.43
3 √ó ‚úì(L) √ó 16G 12.88
4 √ó √ó ‚úì 18G 12.70
5 ‚úì ‚úì(L) √ó 16G 13.26
6 ‚úì √ó ‚úì 18G 13.14 expected performance improvement on long-tailed objects
7 √ó ‚úì(L) ‚úì 18G 13.35 and small objects.
8 ‚úì ‚úì(S) ‚úì 18G 13.80
9 ‚úì ‚úì(L) ‚úì 18G 14.01 Our Method performs better on long-tailed objects.
As shown in Table I, MonoOcc-S shows a significant im-
provement in predicting long-tailed objects compared with
driving scenario. Then, we semantically align the KITTI-
VoxFormer-S, such as the other-ground (0.56%, 5.60 ‚Üí
360, BDD100K, Cityscapes, and nuImages datasets, further
9.70),other-vehicle(0.20%,3.70‚Üí5.40)andtruck (0.16%,
training on 19 common road elements specific to driving
3.50 ‚Üí 5.20).
scenarios. Finally, We pre-train the backbone on 8 A100
GPUs for a total of 20k iterations. Our Method performs better on small objects. As
shown in Table I, MonoOcc-S demonstrates a significant
C. Quantitative and Qualitative Results boostinpredictingsmallobjectscomparedwithVoxFormer-
S, such as the bicycle (1.00 ‚Üí 2.20), motorcycle (0.70 ‚Üí
In this section, we compare our method with competitive
1.50) and traffic-sign (4.90 ‚Üí 6.40).
baselines on the test split of SemanticKITTI in Table I and
demonstrate qualitative results in Fig. 3. Table I shows the Qualitative Results. To demonstrate the performance of
comparison results with some methods adapted for RGB our algorithm more intuitively, we also provide the quali-
input such as LMSCNet [33], JS3C-Net [44] and other tative visualization of the predicted semantic occupancy in
competitive camera-based semantic occupancy prediction Fig. 3. The four rows demonstrate the superiority of our
methods such as TPVFormer [30] VoxFormer [17], and method on long-tailed objects, small objects and road
OccFormer [18] on SemanticKITTI. Overall, our method segmentation. The red box in the first row shows that
achieves a significant improvement over the other baselines VoxFormer-S cannot estimate the instance of other-vehicle.
in nearly all classes and sets new SOTA. To be specific, The orange box in the second row, third row, and last row
MonoOcc-S and MonoOcc-L achieve a remarkable boost of show that our method performs better on small objects like
1.60 mIoU and 3.43 mIoU, respectively, compared to the person,bicycleandpole,respectively.Tofurtherdemonstrate
baseline method VoxFormer [17]. For the sake of fairness, the effectiveness of our method, we show the impressive
inthefollowing,wecompareMonoOcc-SwithVoxFormer-S prediction result of road, a crucial category for autonomous
to analyze the advantages of our methods. Delving into the vehicles to estimate the drivable area, within blue box in the
qualitative results, we find that our algorithm achieves the second row and last row.
)%03.51(daor‚ñ†
)%31.11(klawedis‚ñ†
)%21.1(gnikrap‚ñ†
)%65.0(dnuorg-rehto‚ñ†
)%1.41(gnidliub‚ñ†
)%29.3(rac‚ñ†
)%61.0(kcurt‚ñ†
)%30.0(elcycib‚ñ†
)%30.0(elcycrotom‚ñ†
)%02.0(elcihev-rehto‚ñ†
)%3.93(noitategev‚ñ†
)%15.0(knurt‚ñ†
)%71.9(niarret‚ñ† )%70.0(nosrep‚ñ† )%70.0(tsilcycib‚ñ†
)%50.0(tsilcycrotom‚ñ†
)%09.3(ecnef‚ñ† )%92.0(elop‚ñ†
)%80.0(ngis-cfifart‚ñ†Input Ground Truth MonoOcc-L MonoOcc-S VoxFormer-S
road sidewalk parking other-grnd building vegetation trunk terrain. traf.-sign pole
car person bicyclist motorcyclist bicyclist fence truck bicycle motorcycle other-veh.
Fig.3. QualitativeresultsofourmethodandVoxFormeronSemanticKITTIdataset
D. Ablation Study formance of the single-frame branch significantly increases.
In addition, by comparing row 6 with row 8 and row 6
WeprovideablationsonSemanticKITTIforthedesignsof
with row 9 of TableII, it is verified that knowledge from
each proposed component. Table II demonstrates the effec-
both multiple frames and large models can be introduced to
tiveness of each component individually and in combination
the single-frame branch through the distillation module. The
with other components on the single frame branch. In the
comparisonbetweenrow8androw9ofthetableshowsthat
columnoftheDistillationModule,(L)or(S)meansdistilling
scaling up the backbone of the privileged branch can also
the temporal branch with the large or small backbone to the
enhancetheperformanceofthesingle-framebranchthrough
single frame branch.
the distillation module.
2D Semantic Auxiliary Loss: The effectiveness of 2D
Table III shows the necessity of the distillation module.
Semantic Auxiliary Loss is shown in row 2 of Table II. It
Using a larger backbone results in a significant increase in
exceeds VoxFormer-S by 0.73 mIoU and scarcely increases
GPU memory usage during test time (8G ‚Üí 12G), while
GPU memory cost during training. Since auxiliary loss
the distillation module can transfer richer knowledge into
makesitpossibletooptimizethefeatureextractorinashorter
the single-frame branch at a low cost.
path, the performance is largely promoted.
Image Conditioned Cross-Attention: Row 4 of Table V. CONCLUSION
II shows the advantage of the image-conditioned cross- In this paper, we present MonoOcc, a high-performance
attention. The performance of the framework is improved and efficient framework for monocular semantic occupancy
by 0.35 mIoU with minimal extra memory cost (about 2G). prediction. We propose a semantic auxiliary loss and an
Row5androw9demonstratethatthecross-attentionsignif- image-conditionedcross-attentionmodule,improvingtheex-
icantlyimprovestheperformanceofthesingleframebranch isting3Dsemanticoccupancypredictionmethod.Bypropos-
combined with the other two components, by introducing ingadistillationmoduletotransfertemporalinformationand
visual cues for completing occluded regions. richer knowledge to the monocular branch from a privileged
Scaling-upAndPre-training:TableIIIshowsthepositive branch, we increase the performance of the framework
impacts of scaling-up and pre-training. According to the especially on small and long-tailed objects, while striking a
comparison between row 1 and row 2, it is clear that balancebetweenperformanceandefficiency.Benefitingfrom
increasing the parameters of the backbone can significantly these improvements, MonoOcc achieves SOTA performance
improve the performance of occupancy prediction. While on SemanticKITTI benchmark.
the results in row 2 and row 3 prove that pre-training the
backbone network on driving datasets further improves the VI. AKNOWLEDGEMENT
performance of occupancy prediction. This work is supported by the National Natural Science
Distillation Module: Row 3, 5 and 7 of Table II demon- Foundation of China (NSFC) under Grants No.62173324
strate the effectiveness of the distillation module. Thanks to and the CAS for Grand Challenges under Grants
thetransferofknowledgefromtheprivilegedbranch,theper- 104GJHZ2022013GC.REFERENCES [21] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,
Z.Zhang,L.Dongetal.,‚ÄúSwintransformerv2:Scalingupcapacity
[1] Z. Yan, X. Li, K. Wang, S. Chen, J. Li, and J. Yang, ‚ÄúDistortion
and resolution,‚Äù in Proceedings of the IEEE/CVF conference on
and uncertainty aware loss for panoramic depth completion,‚Äù in
computervisionandpatternrecognition,2022,pp.12009‚Äì12019.
International Conference on Machine Learning. PMLR, 2023, pp.
[22] D. Chen, B. Zhou, V. Koltun, and P. Kra¬®henbu¬®hl, ‚ÄúLearning by
39099‚Äì39109.
cheating,‚ÄùinConferenceonRobotLearning. PMLR,2020.
[2] Z.Yan,X.Li,K.Wang,Z.Zhang,J.Li,andJ.Yang,‚ÄúMulti-modal
[23] J. Lee, J. Hwangbo, L. Wellhausen, V. Koltun, and M. Hutter,
masked pre-training for monocular panoramic depth completion,‚Äù in
‚ÄúLearningquadrupedallocomotionoverchallengingterrain,‚ÄùScience
EuropeanConferenceonComputerVision. Springer,2022,pp.378‚Äì
robotics,vol.5,no.47,p.eabc5986,2020.
395.
[24] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao, and
[3] L.Wang,H.Ye,Q.Wang,Y.Gao,C.Xu,andF.Gao,‚ÄúLearning-based
J. Dai, ‚ÄúBevformer: Learning bird‚Äôs-eye-view representation from
3doccupancypredictionforautonomousnavigationinoccludedenvi-
multi-camera images via spatiotemporal transformers,‚Äù in European
ronments,‚Äùin2021IEEE/RSJInternationalConferenceonIntelligent
conferenceoncomputervision. Springer,2022.
RobotsandSystems(IROS),2020.
[25] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, ‚ÄúBevdet: High-
[4] M.Popovic,F.Thomas,S.Papatheodorou,N.Funk,T.Vidal-Calleja,
performancemulti-camera3dobjectdetectioninbird-eye-view,‚ÄùarXiv
andS.Leutenegger,‚ÄúVolumetricoccupancymappingwithprobabilis-
preprintarXiv:2112.11790,2021.
tic depth completion for robotic navigation,‚Äù in IEEE Robotics and
[26] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li,
AutomationLetters,2021,pp.5072‚Äì5079.
‚ÄúBevdepth: Acquisition of reliable depth for multi-view 3d object
[5] P.Li,R.Zhao,Y.Shi,H.Zhao,J.Yuan,G.Zhou,andY.-Q.Zhang,
detection,‚Äù in Proceedings of the AAAI Conference on Artificial
‚ÄúLode: Locally conditioned eikonal implicit scene completion from
Intelligence,2023.
sparse lidar,‚Äù 2023 IEEE International Conference on Robotics and
[27] A. Saha, O. Mendez, C. Russell, and R. Bowden, ‚ÄúTranslating im-
Automation(ICRA),pp.8269‚Äì8276,2023.
ages into maps,‚Äù in 2022 International conference on robotics and
[6] Z.Xia,Y.Liu,X.Li,X.Zhu,Y.Ma,Y.Li,Y.Hou,andY.Qiao,‚ÄúScp-
automation(ICRA),2022.
net:Semanticscenecompletiononpointcloud,‚ÄùinProceedingsofthe
[28] J. Philion and S. Fidler, ‚ÄúLift, splat, shoot: Encoding images from
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
arbitrary camera rigs by implicitly unprojecting to 3d,‚Äù in Computer
2023.
Vision‚ÄìECCV2020:16thEuropeanConference,Glasgow,UK,August
[7] Y. Chen, H. Li, R. Gao, and D. Zhao, ‚ÄúBoost 3-d object detection
23‚Äì28,2020,Proceedings,PartXIV16. Springer,2020.
via point clouds segmentation and fused 3-d giou-l1 loss,‚Äù IEEE
[29] A.Hu,Z.Murez,N.Mohan,S.Dudas,J.Hawke,V.Badrinarayanan,
TransactionsonNeuralNetworksandLearningSystems,vol.33,no.2,
R.Cipolla,andA.Kendall,‚ÄúFiery:Futureinstancepredictioninbird‚Äôs-
pp.762‚Äì773,2020.
eye view from surround monocular cameras,‚Äù in Proceedings of the
[8] Y.Chen,D.Zhao,L.Lv,andQ.Zhang,‚ÄúMulti-tasklearningfordan-
IEEE/CVFInternationalConferenceonComputerVision,2021.
gerousobjectdetectioninautonomousdriving,‚ÄùInformationSciences,
[30] Y. Huang, W. Zheng, Y. Zhang, J. Zhou, and J. Lu, ‚ÄúTri-perspective
vol.432,pp.559‚Äì571,2018.
viewforvision-based3dsemanticoccupancyprediction,‚ÄùinProceed-
[9] H.Li,Y.Chen,Q.Zhang,andD.Zhao,‚ÄúBifnet:Bidirectionalfusion
ings of the IEEE/CVF Conference on Computer Vision and Pattern
network for road segmentation,‚Äù IEEE transactions on cybernetics,
Recognition,2023.
vol.52,no.9,pp.8617‚Äì8628,2021.
[31] K.He,X.Zhang,S.Ren,andJ.Sun,‚ÄúDeepresiduallearningforimage
[10] A.-Q.CaoandR.deCharette,‚ÄúMonoscene:Monocular3dsemantic
recognition,‚Äù in Proceedings of the IEEE conference on computer
scene completion,‚Äù in Proceedings of the IEEE/CVF Conference on
visionandpatternrecognition,2016.
ComputerVisionandPatternRecognition,2022.
[32] T.-Y.Lin,P.Dolla¬¥r,R.Girshick,K.He,B.Hariharan,andS.Belongie,
[11] Y. Zheng, C. Zhong, P. Li, H. Gao, Y. Zheng, B. Jin, L. Wang,
‚ÄúFeaturepyramidnetworksforobjectdetection,‚ÄùinProceedingsofthe
H. Zhao, G. Zhou, Q. Zhang, and D. Zhao, ‚ÄúSteps: Joint self-
IEEEconferenceoncomputervisionandpatternrecognition,2017.
supervisednighttimeimageenhancementanddepthestimation,‚Äù2023
[33] L. Roldao, R. de Charette, and A. Verroust-Blondet, ‚ÄúLmscnet:
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
Lightweightmultiscale3dsemanticcompletion,‚Äùin2020International
2023.
Conferenceon3DVision(3DV). IEEE,2020.
[12] C.Godard,O.MacAodha,M.Firman,andG.J.Brostow,‚ÄúDigging
[34] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‚ÄúDeformable
into self-supervised monocular depth estimation,‚Äù in Proceedings of
detr: Deformable transformers for end-to-end object detection,‚Äù in
theIEEE/CVFinternationalconferenceoncomputervision,2019.
InternationalConferenceonLearningRepresentations,2020.
[13] B.Jin,X.Liu,Y.Zheng,P.Li,H.Zhao,T.Zhang,Y.Zheng,G.Zhou,
[35] Y.Chen,D.Zhao,H.Li,D.Li,andP.Guo,‚ÄúAtemporal-baseddeep
and J. Liu, ‚ÄúAdapt: Action-aware driving caption transformer,‚Äù 2023
learningmethodformultipleobjectsdetectioninautonomousdriving,‚Äù
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
in 2018 international joint conference on neural networks (IJCNN).
2023.
IEEE,2018,pp.1‚Äì6.
[14] Z.Yan,K.Wang,X.Li,Z.Zhang,J.Li,andJ.Yang,‚ÄúDesnet:Decom-
[36] X.Zhao,Y.Chen,J.Guo,andD.Zhao,‚ÄúAspatial-temporalattention
posedscale-consistentnetworkforunsuperviseddepthcompletion,‚Äùin
modelforhumantrajectoryprediction.‚ÄùIEEECAAJ.Autom.Sinica,
ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.37,
vol.7,no.4,pp.965‚Äì974,2020.
no.3,2023,pp.3109‚Äì3117.
[37] Y. Wei, L. Zhao, W. Zheng, Z. Zhu, Y. Rao, G. Huang, J. Lu,
[15] ‚Äî‚Äî, ‚ÄúRignet: Repetitive image guided network for depth comple-
andJ.Zhou,‚ÄúSurrounddepth:Entanglingsurroundingviewsforself-
tion,‚ÄùinEuropeanConferenceonComputerVision. Springer,2022,
supervised multi-camera depth estimation,‚Äù in Conference on Robot
pp.214‚Äì230.
Learning,2023.
[16] J.Behley,M.Garbade,A.Milioto,J.Quenzel,S.Behnke,C.Stach-
[38] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao, ‚ÄúGmflow:
niss, and J. Gall, ‚ÄúSemantickitti: A dataset for semantic scene un-
Learning optical flow via global matching,‚Äù in Proceedings of the
derstanding of lidar sequences,‚Äù in Proceedings of the IEEE/CVF
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
internationalconferenceoncomputervision,2019.
2022.
[17] Y.Li,Z.Yu,C.Choy,C.Xiao,J.M.Alvarez,S.Fidler,C.Feng,and
[39] H.Xu,J.Zhang,J.Cai,H.Rezatofighi,F.Yu,D.Tao,andA.Geiger,
A. Anandkumar, ‚ÄúVoxformer: Sparse voxel transformer for camera-
‚ÄúUnifying flow, stereo and depth estimation,‚Äù IEEE Transactions on
based3dsemanticscenecompletion,‚ÄùinProceedingsoftheIEEE/CVF
PatternAnalysisandMachineIntelligence,2023.
ConferenceonComputerVisionandPatternRecognition,2023.
[40] B. Zhou and P. Kra¬®henbu¬®hl, ‚ÄúCross-view transformers for real-time
[18] Y. Zhang, Z. Zhu, and D. Du, ‚ÄúOccformer: Dual-path transformer
map-view semantic segmentation,‚Äù in Proceedings of the IEEE/CVF
for vision-based 3d semantic occupancy prediction,‚Äù arXiv preprint
conferenceoncomputervisionandpatternrecognition,2022.
arXiv:2304.05316,2023.
[41] A. Geiger, P. Lenz, and R. Urtasun, ‚ÄúAre we ready for autonomous
[19] Y.Wei,L.Zhao,W.Zheng,Z.Zhu,J.Zhou,andJ.Lu,‚ÄúSurroundocc:
driving? the kitti vision benchmark suite,‚Äù in 2012 IEEE conference
Multi-camera3doccupancypredictionforautonomousdriving,‚ÄùarXiv
oncomputervisionandpatternrecognition,2012.
preprintarXiv:2303.09551,2023.
[42] X. Chen, K.-Y. Lin, C. Qian, G. Zeng, and H. Li, ‚Äú3d sketch-aware
[20] W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu, T. Lu,
semantic scene completion via semi-supervised structure prior,‚Äù in
L. Lu, H. Li et al., ‚ÄúInternimage: Exploring large-scale vision foun-
Proceedings of the IEEE/CVF Conference on Computer Vision and
dation models with deformable convolutions,‚Äù in Proceedings of the
PatternRecognition,2020.
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
2023.[43] J.Li,K.Han,P.Wang,Y.Liu,andX.Yuan,‚ÄúAnisotropicconvolutional no.3,2022.
networks for 3d semantic scene completion,‚Äù in Proceedings of the [47] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan,
IEEE/CVFConferenceonComputerVisionandPatternRecognition, andT.Darrell,‚ÄúBdd100k:Adiversedrivingdatasetforheterogeneous
2020. multitask learning,‚Äù in Proceedings of the IEEE/CVF conference on
[44] X.Yan,J.Gao,J.Li,R.Zhang,Z.Li,R.Huang,andS.Cui,‚ÄúSparse computervisionandpatternrecognition,2020.
single sweep lidar point cloud segmentation via learning contextual [48] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
shape priors from scene completion,‚Äù in Proceedings of the AAAI nenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset
ConferenceonArtificialIntelligence,2021. forsemanticurbansceneunderstanding,‚ÄùinProceedingsoftheIEEE
[45] G. Neuhold, T. Ollmann, S. Rota Bulo, and P. Kontschieder, ‚ÄúThe conferenceoncomputervisionandpatternrecognition,2016.
mapillaryvistasdatasetforsemanticunderstandingofstreetscenes,‚Äùin [49] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
ProceedingsoftheIEEEinternationalconferenceoncomputervision, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, ‚Äúnuscenes: A
2017. multimodal dataset for autonomous driving,‚Äù in Proceedings of the
[46] Y. Liao, J. Xie, and A. Geiger, ‚ÄúKitti-360: A novel dataset and IEEE/CVF conference on computer vision and pattern recognition,
benchmarks for urban scene understanding in 2d and 3d,‚Äù IEEE 2020.
Transactions on Pattern Analysis and Machine Intelligence, vol. 45,