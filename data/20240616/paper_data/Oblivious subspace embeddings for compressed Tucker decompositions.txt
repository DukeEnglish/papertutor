Oblivious subspace embeddings for compressed Tucker
decompositions
MatthewPietrosanu BeiJiang LinglongKong
DepartmentofMathematical& DepartmentofMathematical& DepartmentofMathematical&
StatisticalSciences,Universityof StatisticalSciences,Universityof StatisticalSciences,Universityof
Alberta Alberta Alberta
Edmonton,Canada Edmonton,Canada Edmonton,Canada
pietrosa@ualberta.ca bei1@ualberta.ca lkong@ualberta.ca
ABSTRACT 1 BACKGROUNDANDMOTIVATION
Emphasisinthetensorliteratureonrandomembeddings(tools Low-dimensionaldecompositionslieattheheartofmanytensor-
forlow-distortiondimensionreduction)forthecanonicalpolyadic basedmethodsforstatisticalinference,representation,andfeature
(CP)tensordecompositionhasleftanalogousresultsforthemore extraction[13].Tensor-specializedmethodsarethemselvesoften
expressiveTuckerdecompositioncomparativelylacking.Thiswork motivatedbyspecificappliedresearchquestionsandadvancements
establishesgeneralJohnsonâ€“Lindenstrauss(JL)typeguarantees indata-collectiontechnologies(e.g.,inpharmacology[6]andneu-
fortheestimationofTuckerdecompositionswhenanoblivious roimaging[21]).Inthesesettings,naivelyworkingwithvectorized
randomembeddingisappliedalongeachmode.Whentheseem- dataistypicallyneitherconceptuallysoundnorcomputationally
beddingsaredrawnfromaJL-optimalfamily,thedecomposition feasible.Dimension-reducingmapsthatpreservedatageometry
canbeestimatedwithinğœ€relativeerrorunderrestrictionsonthe haveconsequentlyfoundsubstantialapplicationintensorresearch,
embeddingdimensionthatareinlinewithrecentCPresults.Weim- wherecomputationalefficiencyandrepresentationqualityarepri-
plementahigher-orderorthogonaliteration(HOOI)decomposition maryconcernsduetoprohibitivesizeoftensordata.
algorithmwithrandomembeddingstodemonstratethepractical Thewell-knownJohnsonâ€“Lindenstrauss(JL)lemma[3]provides
benefitsofthisapproachanditspotentialtoimprovetheacces- atheoreticalbasisforsuchmaps.Wesaythatalineartransforma-
sibilityofotherwiseprohibitivetensoranalyses.Onmoderately tionğ‘¨isanğœ€-JLembeddingofasetS âŠ‚Rğ‘›intoRğ‘š if,forevery
largefaceimageandfMRIneuroimagingdatasets,empiricalre- ğ‘¥ âˆˆS,thereexistsğœ€ ğ‘¥ âˆˆ (âˆ’ğœ€,ğœ€)suchthat
s mu il nts imsh ao lw incth rea at ss eub insta ren ct oia nl sd trim uce tn ios nion err re od ru rc et li ao tn ivi es tp oos ts ri ab dl ie tiw onit ah
l
âˆ¥ğ‘¨ğ‘¥âˆ¥2 2=(1+ğœ€ ğ‘¥)âˆ¥ğ‘¥âˆ¥2 2.
HOOI(â‰¤5%largererror,50%â€“60%lowercomputationtimeforlarge JL-embeddingsaretypicallygeneratedrandomlyfromaclassof
modelswith50%dimensionreductionalongeachmode).Especially maps,independentof(orobliviousto)thedatatowhichitwillbe
forlargetensors,ourmethodoutperformstraditionalhigher-order applied.
singularvaluedecomposition(HOSVD)andrecentlyproposedTen- Withmuchemphasisonscalable,computationallyefficientten-
sorSketchmethods. sormethods,itisnotsurprisingthatthemajorityofdevelopments
intheliteraturefavorthecanonicalpolyadic(CP)decomposition
CCSCONCEPTS foritssparsity[9,16,18].Thishaslefttheoreticalresultsforthe
Tuckerdecompositioncomparativelylimited.Foragiventensor
â€¢Mathematicsofcomputingâ†’Dimensionalityreduction;â€¢
Theoryofcomputationâ†’Sketchingandsampling. ğ’€
âˆˆRğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘,theTuckerdecompositiontakestheform
ğ‘…1 ğ‘…ğ‘
KEYWORDS ğ’€ = âˆ‘ï¸ Â·Â·Â·âˆ‘ï¸ ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘Î“ 1,ğ‘Ÿ1â—¦Â·Â·Â·â—¦Î“ ğ‘,ğ‘Ÿğ‘
alternatingleastsquares,dimensionreduction,higher-orderorthog- ğ‘Ÿ1=1 ğ‘Ÿğ‘=1
onaliteration,Johnsonâ€“Lindenstrauss,low-rankapproximation, =: [ğš²| ğšª 1,...,ğšª ğ‘] (1)
neuroimaging,randomembedding,tensordecomposition
foraprespecifiedrank(ğ‘…1,...,ğ‘… ğ‘)(ğ‘… ğ‘— â‰¤ğ‘› ğ‘—),whereğš²=(ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘)
A MC atM theR wef Pe ier te rn oc sae nF uo ,r Bm eia Jit a:
ng,andLinglongKong.2024.Oblivioussubspace
âˆˆ Rğ‘…1Ã—Â·Â·Â·Ã—ğ‘…ğ‘ iscalledthecoretensor andğšª ğ‘— = [Î“ ğ‘—,1,...,Î“ ğ‘—,ğ‘…ğ‘—] âˆˆ
embeddingsforcompressedTuckerdecompositions.InProceedingsofConf
Rğ‘›ğ‘—Ã—ğ‘…ğ‘— iscalledtheğ‘—thfactormatrix.Briefly,theCPdecomposition
title(Abbr).ACM,NewYork,NY,USA,11pages. requiresğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘ =0unlessğ‘Ÿ1=Â·Â·Â·=ğ‘Ÿ ğ‘ [10].Thoughlesssparse
andcomputationallymorecomplex,Tuckerdecompositionsprovide
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor aricherclassofdecompositionsandarethesubjectofcontinued
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation researchinstatisticsandotherappliedfields[10,11,13].
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe Recent notable work on Tucker decompositions by
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Malik&Becker[15],Ma&Solomonik[14],andMinsteretal.[17]
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. incorporatedimensionreductionviaCountSketchoperatorsand
Abbr,MMMDDâ€“DD,YYYY,Location randomGaussianmatricesâ€”bothspecificJLclasses.Thelatterfo-
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. cusesonrandomizedhigher-orderSVD(HOSVD)foritscomputa-
ACMISBN978-1-4503-XXXX-X/18/06
tionalbenefits,whereashigher-orderorthogonaliteration(HOOI)
4202
nuJ
31
]LM.tats[
1v78390.6042:viXraAbbr,MMMDDâ€“DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
isknowntoprovidebetterdecompositions[4].Theworkâ€™sran- ğ‘¿[ğ‘—] âˆˆRğ‘›ğ‘—Ã—(cid:206)ğ‘ ğ‘˜=1,ğ‘˜â‰ ğ‘—ğ‘›ğ‘— bethemode-ğ‘—matricizationofğ‘¿.Vectoriza-
domizedHOSVDandsequentiallytruncatedHOSVDalgorithms tionandmatricizationaretakentobecompatibleinthesensethat
furthermoredonotrespecttheKroneckerstructureoftheTucker vecğ‘¿ =vecğ‘¿[1].LetÃ—ğ‘— denotemode-ğ‘— multiplicationofatensor
decompositionoronlysequentiallyapplyreductionsalongthedata byamatrix.Fornotationalconvenience,wewriteğ‘¿ Ã—ğ‘—âˆˆ[ğ‘] ğ‘¨ğ‘—
modes.Neitheroftheseworksprovidegeneraltheoreticalresults todenotetherepeatedmode ğ‘— multiplicationğ‘¿ Ã—1ğ‘¨1Â·Â·Â·Ã—ğ‘ğ‘¨ğ‘
thatapplybeyondaspecificclassofembeddings. (where order of operations is irrelevant as multiplication along
Similarcommentsapplytoearlierliteraturethatproposeran- differentmodescommute).Forfurtherdetailontensoroperations,
domizedTuckerdecompositionalgorithms(almostexclusivelyvia seeKolda&Bader[10].Fortensorsğ‘¿andğ’€ ofthesamesize,define
HOSVD)throughaspecificclassofJLembeddingortakeadifferent thetensorinnerproductâŸ¨ğ‘¿,ğ’€âŸ©=âŸ¨vecğ‘¿,vecğ’€âŸ©andtheassociated
approachentirely(e.g.,[2,5,19]).Foranoverviewandusefulclas- normâˆ¥ğ‘¿âˆ¥=âˆ¥vecğ‘¿âˆ¥2.
sificationofrandomizedalgorithms,see[1].Theseworkshighlight Anotationforbasiscoherence,introducedpreviouslyinIwenet
thewideinterestinTuckerdecompositionsandthepotentialfor al.[9]fortheCPdecomposition(andarguablygeneralizedhere),
applicationtoabreathofproblems,butagainemphasizethelack willbeconvenientwhenstudyingtheTuckerdecomposition(but
oftheoreticalguaranteesforgeneralJLembeddingframeworks. lessusefulconceptuallyduetotheorthogonalityrestrictiononthe
ThisarticleconsiderstheproblemofestimatingaTuckerde- ğšª ğ‘—s).Definethemodewisecoherenceofanydecompositionofthe
compositionofagiventensorğ’€ âˆˆRğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘ usingaâ€œcompressedâ€ forminEquation1asğœ‡
ğ’€
=max ğ‘—âˆˆ[ğ‘]ğœ‡ ğ’€,ğ‘—,where
versionğ’€ Ã—1ğ‘¨1Â·Â·Â·Ã—ğ‘ ğ‘¨ğ‘ âˆˆ Rğ‘š1Ã—Â·Â·Â·Ã—ğ‘šğ‘ ofthedata,wherethe
ğ‘¨ Tuğ‘— cs ka erre dea cr ob mitr pa or sy itiJ oL nse ,m nb amed ed lyin ,g ws h. eW ree tf ho ecu fas cs too rle mly ao trn ico er st hh aog vo en oa rl
-
ğœ‡ ğ’€,ğ‘— = ğ‘˜,â„m âˆˆa [ğ‘…x
ğ‘—]
âˆ¥| Î“âŸ¨ ğ‘—Î“ ,ğ‘˜ğ‘—,ğ‘˜ âˆ¥2,Î“ âˆ¥ğ‘— Î“,â„ ğ‘—,â„âŸ©|
âˆ¥ 2
thonormalcolumns(i.e.,eachğšª
ğ‘—
liesonaStiefelmanifold).The ğ‘˜â‰ â„
twoprimarycontributionsofthisworkareasfollows. iscalledthemode-ğ‘— coherence.Asinotherworks[9],owingtothe
nonuniquenessofCP/Tuckerdecompositions,wecalculatecoher-
â€¢ WeestablishJL-typetheoreticalresultsboundingtheerror
enceusingtherank-1termsofanexplicitlygivendecomposition.
introducedinexactandinexactTuckerdecompositionswhen
randomembeddingsfromaJL-optimalfamilyareapplied
2.2 ExactDecompositionsUnderJLEmbeddings
alongeachdatamode.Weemphasizethattheseresultsapply
generallytoJL-optimalfamiliesandnottoaspecifictypeof Webeginourtheoreticalanalysiswithanelementaryresulton
embedding,unlikeotherworks[15,17]. howTuckerdecompositionsareperturbedunderarbitrarymode-
â€¢ WeproposeanewHOOIalgorithmthatusesrandomem- ğ‘— multiplication. See the appendix for a proof of the following
beddingstoestimateTuckerdecompositions.Empirically, claim,whichreliesonroutinemanipulationandpropertiesoftensor
forlargemodels,ourapproachrequiressubstantiallyless matricization.
computationtimewithonlyasmallincreaseinreconstruc-
tionerrorrelativetotraditionalHOOIandcanmakelarge Lemma 2.1. Let ğ‘— âˆˆ [ğ‘] and ğ‘© âˆˆ Rğ‘šğ‘—Ã—ğ‘›ğ‘—. Suppose that ğ’€ âˆˆ
tensoranalysesfeasibleonmoderatecomputingresources.
Rğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘ hasarank-(ğ‘…1,...,ğ‘… ğ‘)Tuckerdecompositionğ’€ = [ğš² |
Unlikeotherworks[17],ourapproachtakesadvantageofthe
ğšª 1,...,ğšª ğ‘]andthatmin ğ‘Ÿâˆˆ[ğ‘…ğ‘—]âˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿâˆ¥2 >0.Then
KroneckerstructureoftheTuckerdecompositionanduses ğ’€â€² =ğ’€ Ã—ğ‘— ğ‘©= [ğš²â€² | ğšª 1â€²,...,ğšª ğ‘â€²],
(nearly)fullycompresseddatainallupdatesoftheestimated
decomposition.OurapproachoutperformsHOSVD[10]and whereğœ† ğ‘Ÿâ€² 1,...,ğ‘Ÿğ‘ = ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘âˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿğ‘—âˆ¥,ğšª ğ‘˜â€² = ğšª ğ‘˜ forğ‘˜ â‰  ğ‘—,andğšªâ€² ğ‘— has
recentlyproposedTensorSketchmethods[15]forTucker columnsÎ“ ğ‘—,ğ‘Ÿ/âˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿâˆ¥,ğ‘Ÿ âˆˆ [ğ‘… ğ‘—].Itfollowsthatğœ‡ ğ’€â€²,ğ‘˜ = ğœ‡ ğ’€,ğ‘˜ when
decomposition. ğ‘˜ â‰  ğ‘— and
meO ntu sr ba yp Ip wr eo nac eh tac ll .o [s 9e ]ly fof ro tl hlo ew Cs Pp da er ct oo mf pre oc se itn iot ns ,u wbs itt han at nia il md pe ov re tl ao np t- ğœ‡ ğ’€â€²,ğ‘— = ğ‘Ÿ,ğ‘ m âˆˆa [ğ‘…x
ğ‘—]
âˆ¥ğ‘©|âŸ¨ Î“ğ‘© ğ‘—Î“ ,ğ‘Ÿğ‘— âˆ¥,ğ‘Ÿ 2, âˆ¥ğ‘© ğ‘©Î“ Î“ğ‘—, ğ‘—ğ‘  ,ğ‘ âŸ© âˆ¥| 2.
ğ‘Ÿâ‰ ğ‘ 
distinctionasidefromthedifferentdecomposition.Theauthorsâ€™
remarkinSection3.2thatCPresultscanbeapplieddirectlyto Furthermore,
Tuckerdecompositionsisonlytruewhenthecoretensorhasa ğ‘…ğ‘— ğ‘…ğ‘—
specificpatternofhighsparsity(astheauthorsnote,throughan âˆ¥ğ’€â€²âˆ¥2 =âˆ‘ï¸âˆ‘ï¸ (ğš¿ ğ‘—ğš¿âŠ¤ ğ‘—)ğ‘Ÿ,ğ‘ âŸ¨ğ‘©Î“ ğ‘—,ğ‘Ÿ,ğ‘©Î“ ğ‘—,ğ‘ âŸ©,
appropriatechoiceoftensorbasis).Adirectapplicationofthesepre- ğ‘Ÿ=1ğ‘ =1
v suio mu ss )r ve is ou ll at ts et so ag ne in me pra ol rT tau nc tk be ar sd isec ino cm op ho es ri et nio cn es re(d qu ue irt eo mt eh ne tn .Tes ht ued s, withğš¿ ğ‘— =ğš² [ğ‘—](cid:0)(cid:203) ğ‘˜1 =ğ‘,ğ‘˜â‰ ğ‘—ğšª ğ‘˜(cid:1)âŠ¤ âˆˆRğ‘…ğ‘—Ã—ğ‘ğ‘— andğ‘ ğ‘— =(cid:206)ğ‘ ğ‘˜=1,ğ‘˜â‰ ğ‘—ğ‘› ğ‘˜.
themodifiedapproachtakeninthisworkisindeednecessary.
Whenğ‘©inLemma2.1isanğœ€-JLembeddingthatproperlyembeds
thecolumnspaceofthefactormatricesofaTuckerdecomposition,
2 THEORETICALRESULTS changes to the core tensor, coherence, and tensor norm can be
controlled.ThisnotionisformalizedinProposition2.2.
2.1 Notation
Asstandardoperations,letâ—¦denotethetensorouterproduct,âŠ—the Proposition2.2. Fix ğ‘— âˆˆ [ğ‘]andsupposethatğ’€ âˆˆ Rğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘
Kroneckerproduct,andâŠ¡theHadamardproduct.Letvecdenote permitstherank-(ğ‘…1,...,ğ‘… ğ‘)Tuckerdecompositionğ’€ = [ğš²| ğšª 1,...,
theusualvectorizationoperator.Foratensorğ‘¿ âˆˆRğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘,let ğšª ğ‘],whereğšª
ğ‘—
hascolumnsofunitlength.Supposethatğ‘¨âˆˆRğ‘šÃ—ğ‘›ğ‘—OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDDâ€“DD,YYYY,Location
isanğœ€-JLembeddingoftheset whereğ‘«ğ‘˜ =diag({âˆ¥ğ‘¨ğ‘˜,ğ‘Ÿğšª ğ‘˜,ğ‘Ÿâˆ¥2}ğ‘Ÿâˆˆ[ğ‘…ğ‘˜])andğ’€(0) =ğ’€.Sinceğšª ğ‘—(ğ‘—)
Sğ‘— = (cid:216) {Î“ ğ‘—,ğ‘Ÿ Â±Î“ ğ‘—,ğ‘ }âˆª (cid:216) {Î“ ğ‘—,ğ‘Ÿ}âŠ‚Rğ‘›ğ‘—. h apa ps lc ieo dlu tm on ps aiw rsit oh fu thn eit fn oo rmrm (, ğ’€th (ğ‘—e âˆ’r 1e )s ,u ğ’€l (ts ğ‘—)o )f .Proposition2.2canbe
ğ‘Ÿ,ğ‘ âˆˆ[ğ‘…ğ‘—] ğ‘Ÿâˆˆ[ğ‘…ğ‘—]
Towardthemainresult,
ğ‘Ÿ<ğ‘ 
L ğ’€e bt uğ’€ tâ€² w= ithğ’€ aÃ— cğ‘— oğ‘¨ re,w teh ni sc oh r( ğš²b â€²y wL ie tm hm elea m2 e.1 n) th sa ğœ†s
ğ‘Ÿâ€²
1t ,h ..e .,ğ‘Ÿs ğ‘am =e ğœ†d ğ‘Ÿ1e ,c ..o .,m ğ‘Ÿğ‘p âˆ¥o ğ‘¨si Î“ti ğ‘—o ,ğ‘Ÿn ğ‘—âˆ¥a 2s (cid:12) (cid:12) (cid:12)âˆ¥ğ’€âˆ¥2 âˆ’(cid:13) (cid:13)ğ’€ ğ‘—âˆˆÃ— [ğ‘]ğ‘¨ğ‘—(cid:13) (cid:13)2(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
ğ‘—âˆ‘ï¸ âˆˆ[ğ‘](cid:13) (cid:13)ğ’€(ğ‘—âˆ’1)(cid:13) (cid:13)2 âˆ’(cid:13) (cid:13)ğ’€(ğ‘—)(cid:13) (cid:13)2(cid:12) (cid:12)
(cid:12)
and (ia )ğ‘— |t ğœ†h
ğ‘Ÿâ€²
1f ,a ..c .,ğ‘Ÿto ğ‘r âˆ’m ğœ†a ğ‘Ÿt 1r ,.i .x .,ğ‘Ÿw ğ‘|it â‰¤hc ğœ€o |ğœ†lu ğ‘Ÿm 1,..n .,s ğ‘Ÿğ‘ğ‘¨ |;Î“ ğ‘—,ğ‘Ÿğ‘—/âˆ¥ğ‘¨Î“ ğ‘—,ğ‘Ÿğ‘—âˆ¥2.Then â‰¤ ğ‘—âˆ‘ï¸ âˆˆ[ğ‘](cid:12) (cid:12)1 ğ‘…âŠ¤ ğ‘—[ğ‘¬ğ‘— âŠ¡(ğš¿ ğ‘—ğš¿âŠ¤ ğ‘—)]1 ğ‘…ğ‘—(cid:12) (cid:12)
(( iii ii )) (cid:12) (cid:12)ğœ‡ âˆ¥ğ’€ ğ’€â€², â€²ğ‘— âˆ¥2â‰¤ âˆ’ğœ€/ âˆ¥( ğ’€1 âˆ¥âˆ’ 2(cid:12) (cid:12)ğœ€ â‰¤)a (cid:12) (cid:12)1n ğ‘…âŠ¤d ğ‘—ğœ‡ [ğ‘¬ğ’€ ğ‘—â€²,ğ‘˜ âŠ¡= (ğš¿ğœ‡ ğ’€ ğ‘—ğš¿,ğ‘˜ âŠ¤ ğ‘—fo )]r 1a ğ‘…l ğ‘—l (cid:12) (cid:12)ğ‘˜ , â‰  ğ‘—;and b By efc ol ra eim pro(i ci ei) eo dif nP gr ,o wp eos mit uio stn e2 x. a2 m,h ine ere thw eit gh enğ‘¬ eğ‘— raâˆˆ lt( eâˆ’ rmğœ€/ğ‘ o, fğœ€ t/ hğ‘ e)ğ‘… abğ‘—Ã— oğ‘… vğ‘— e.
whereğ‘¬ğ‘— âˆˆ (âˆ’ğœ€,ğœ€)ğ‘…ğ‘—Ã—ğ‘…ğ‘— andğš¿ ğ‘— isasdefinedinLemma2.1. sum:
Proof. We provide a sketch of the proof. To prove claim (i), (cid:12) (cid:12)1 ğ‘…âŠ¤ ğ‘—[ğ‘¬ğ‘— âŠ¡(ğš¿ ğ‘—ğš¿âŠ¤ ğ‘—)]1 ğ‘…ğ‘—(cid:12) (cid:12)
observetha |ğœ†t ğ‘Ÿâ€²
1,...,ğ‘Ÿğ‘
âˆ’ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘|=|ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘|(cid:12) (cid:12)âˆ¥ğ‘¨Î“ ğ‘—,ğ‘Ÿğ‘—âˆ¥2âˆ’1(cid:12)
(cid:12)
â‰¤âˆ‘ï¸ ğ‘Ÿğ‘… =ğ‘— 1âˆ‘ï¸ ğ‘ ğ‘… =ğ‘— 1ğ‘ğœ€ âˆ¥ğ‘’ ğ‘ŸâŠ¤ğš² [( ğ‘—ğ‘—) ]âˆ¥2âˆ¥ğ‘’ ğ‘ âŠ¤ğš² [( ğ‘—ğ‘—) ]âˆ¥2(cid:13) (cid:13)(cid:204) ğ‘˜=1
ğ‘
ğšª ğ‘˜(ğ‘—)âŠ¤ğšª ğ‘˜(ğ‘—)(cid:13) (cid:13)2.
â‰¤ğœ€|ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘|, ğ‘˜â‰ ğ‘—
where(cid:12) (cid:12)âˆ¥ğ‘¨Î“ ğ‘—,ğ‘Ÿâˆ¥2âˆ’1(cid:12) (cid:12)â‰¤ğœ€sinceÎ“ ğ‘—,ğ‘Ÿ âˆˆSğ‘— hasunitnorm. Wecalculatethefinaloperatornorm âˆ¥(cid:203) ğ‘˜1 =ğ‘,ğ‘˜â‰ ğ‘—ğšª ğ‘˜(ğ‘—)âŠ¤ğšª ğ‘˜(ğ‘—) âˆ¥2 =
To prove claim (ii), note that, for any distinct ğ‘Ÿ and ğ‘ , (cid:206)ğ‘ âˆ¥ğšª(ğ‘—) âˆ¥2intwocases.Whenğ‘˜ > ğ‘—,itisclearthatâˆ¥ğšª(ğ‘—) âˆ¥2 =
|âŸ¨ğ‘¨Î“ ğ‘—,ğ‘Ÿ,ğ‘¨Î“ ğ‘—,ğ‘ âŸ©âˆ’âŸ¨Î“ ğ‘—,ğ‘Ÿ,Î“ ğ‘—,ğ‘ âŸ©| â‰¤ğœ€byLemmaA.1since{Î“ ğ‘—,ğ‘Ÿ Â±Î“ ğ‘—,ğ‘ }âŠ‚ ğ‘˜=ğ‘,ğ‘˜â‰ ğ‘— ğ‘˜ 2 ğ‘˜ 2
Sğ‘—.Ontheotherhand, 1.Ontheotherhand,whenğ‘˜ â‰¤ ğ‘—,
âˆ¥ğ‘¨Î“ ğ‘—,ğ‘Ÿâˆ¥2âˆ¥ğ‘¨Î“ ğ‘—,ğ‘ âˆ¥2 â‰¥ ğ‘¡âˆˆm [i ğ‘…n ğ‘—]âˆ¥ğ‘¨Î“ ğ‘—,ğ‘¡âˆ¥2
2
â‰¥1âˆ’ğœ€ (ğšª ğ‘˜(ğ‘—)âŠ¤ğšª ğ‘˜(ğ‘—) )ğ‘Ÿ,ğ‘  = âˆ¥ğ‘¨âŸ¨ ğ‘˜ğ‘¨ Î“ğ‘˜ ğ‘˜Î“ ,ğ‘Ÿğ‘˜ âˆ¥,ğ‘Ÿ 2, âˆ¥ğ‘¨ ğ‘¨ğ‘˜ ğ‘˜Î“ Î“ğ‘˜ ğ‘˜,ğ‘  ,ğ‘ âŸ©
âˆ¥2
asÎ“
ğ‘—,ğ‘¡
âˆˆSğ‘—.Thus, forğ‘Ÿ,ğ‘  âˆˆ [ğ‘… ğ‘˜].Thisquantityisequalto1whenğ‘Ÿ =ğ‘ and,byclaim
(ii)ofProposition2.2,isabsolutelyboundedbyğœ€/(ğ‘âˆ’ğœ€)whenğ‘Ÿ â‰ ğ‘ .
ğœ‡ ğ’€â€²,ğ‘— = ğ‘Ÿ,ğ‘ m âˆˆa [ğ‘…x ğ‘—] âˆ¥ğ‘¨|âŸ¨ Î“ğ‘¨ ğ‘—Î“ ,ğ‘Ÿğ‘— âˆ¥,ğ‘Ÿ 2, âˆ¥ğ‘¨ ğ‘¨Î“ Î“ğ‘—, ğ‘—ğ‘  ,ğ‘ âŸ© âˆ¥| 2 â‰¤ 1âˆ’ğœ€ ğœ€. T abh su os l, uğšª teğ‘˜( lğ‘— y)âŠ¤ bğšª oğ‘˜ u(ğ‘— n) d= edğ‘°ğ‘… bğ‘˜ y+ ğœ€/( (1 ğ‘ğ‘…ğ‘˜ âˆ’1 ğ‘… ğœ€âŠ¤ )ğ‘˜ .âˆ’ Frğ‘° oğ‘… mğ‘˜) tâŠ¡ hiğ‘­ sğ‘˜ ,, itw ishe sr tre ağ‘­ igh ha tfs oe rl wem are dn tt os
ğ‘Ÿâ‰ ğ‘ 
Itisclearthatğœ‡ ğ’€â€²,ğ‘˜ =ğœ‡ ğ’€,ğ‘˜ forğ‘˜ â‰  ğ‘— sincetheğ‘˜thfactormatrixis sho Rw ett uh ra nt inâˆ¥ gğšª ğ‘˜( toğ‘—) tâˆ¥ h2 2 eâ‰¤ m1 ai+ nğœ€ r( eğ‘… sğ‘˜ ulâˆ’ t,w1) e/( cğ‘ anâˆ’ cğœ€ o) n. cludethat
thesamebetweenğ’€ andğ’€â€².
witF hin ğ‘©al =ly ğ‘¨,to anp dro ğ‘©ve =c ğ‘°l ğ‘›a ğ‘—im .C( oi mii) b, ic no in ngsi td he er sa ep rp epli rc ea st eio nn tas tio of nL se om fğ’€m â€²a a2 n. d1 (cid:12) (cid:12) (cid:12)âˆ¥ğ’€âˆ¥2 âˆ’(cid:13) (cid:13)ğ’€ ğ‘—Ã—ğ‘ =1ğ‘¨ğ‘—(cid:13) (cid:13)2(cid:12) (cid:12)
(cid:12)
ğ’€ y Pi re old ps osc il ta ii om n2(i .i 2i) ca on nd troco lsm thp ele Tte us ckth ee rdp er co oo mfo pf osth ite iop nr ro ep so us li tt ii no gn f. româ–¡
â‰¤ ğ‘ğœ€
âˆ‘ï¸ğ‘
ğ‘… ğ‘—(cid:16) 1+
ğ‘ğœ€(cid:17)2ğ‘—(cid:214)ğ‘—âˆ’1
(cid:104) 1+(cid:16) ğ‘âˆ’ğœ€ ğœ€(cid:17) (ğ‘… ğ‘˜ âˆ’1)(cid:105) âˆ¥ğš²âˆ¥2
ğ‘—=1 ğ‘˜=1
theapplicationofasingleembeddingğ‘¨alongmodeğ‘—.Proposition
ğ‘…Ëœğœ€ğ‘’ğœ€(2+ğ‘…Ëœ+2/ğ‘)
2.3repeatedlyappliesthisresulttoobtainaJL-typeboundforthe
â‰¤
âˆ¥ğš²âˆ¥2,
applicationofanembeddingğ‘¨ğ‘— alongeachmode. 1+ğœ€/(2ğ‘)
Proposition2.3. Letğœ€ âˆˆ (0,1)andsupposethatğ’€ âˆˆRğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘ wherethefinalinequalityholdssinceâˆ¥ğš²(ğ‘—)âˆ¥2 â‰¤ (1+ğœ€/ğ‘)2ğ‘—âˆ¥ğš²âˆ¥2
(byarecursiveapplicationofclaim(i)ofProposition2.2)andby
permitstherank-(ğ‘…1,...,ğ‘… ğ‘)orthogonalTuckerdecompositionğ’€ =
[ğš² | ğšª 1,...,ğšª ğ‘].Letğ‘¨ğ‘— âˆˆ Rğ‘šğ‘—Ã—ğ‘›ğ‘— beanğœ€/ğ‘-JLembeddingofSğ‘— other standard bounds. The result follows directly since âˆ¥ğ’€âˆ¥ =
(fromProposition2.2)foreachğ‘— âˆˆ [ğ‘].Then
âˆ¥ğš²Ã—ğ‘˜âˆˆ[ğ‘] ğšª ğ‘˜âˆ¥=âˆ¥ğš²âˆ¥. â–¡
(cid:12) (cid:12) (cid:12)âˆ¥ğ’€âˆ¥2 âˆ’(cid:13) (cid:13)ğ’€ ğ‘—Ã—ğ‘ =1ğ‘¨ğ‘—(cid:13) (cid:13)(cid:12) (cid:12) (cid:12)â‰¤ ğ‘…Ëœğœ€ 1ğ‘’ğœ€ +(2 ğœ€+ /ğ‘… (Ëœ+ 2ğ‘2/ )ğ‘) âˆ¥ğ’€âˆ¥2, Wac eB o sne af vo yer ne thiep anr to t ac te o fe o ad l mi fn o ig lr yt to h ofeth pte h rm oeo ba r ai en bt iir lc ie a ts ylu al dt n io a sf tly rt ih s bii s us tos ie f oc nJt L sio e Pn m, ğ‘šw b ,ğ‘›ee di odn nit nr Ro gd s ğ‘šu [ Ã—9c ğ‘›e ].
whereğ‘…Ëœ =max ğ‘—âˆˆ[ğ‘]ğ‘… ğ‘—. over (ğ‘š,ğ‘›) âˆˆ NÃ—Nisanğœ‚-optimalfamilyofJLembeddings if
thereexistsanabsoluteconstantğ¶ > 0suchthat,foranyğ‘š <ğ‘›
Proof. Weprovideasketchoftheproof.For ğ‘— âˆˆ [ğ‘],define and any set S âŠ‚ Rğ‘› with cardinality |S| â‰¤ ğœ‚exp(ğœ€2ğ‘š/ğ¶), the
ğ’€(ğ‘—) = ğ’€(ğ‘—âˆ’1) Ã—ğ‘— ğ‘¨ğ‘— = [ğš²(ğ‘—) | ğšª 1(ğ‘—),...,ğšª ğ‘(ğ‘—) ],whereğœ† ğ‘Ÿ( 1ğ‘— ,) ...,ğ‘Ÿğ‘ = randommatrixğ‘¨âˆ¼Pğ‘š,ğ‘›isanğœ€-JLembeddingofSintoRğ‘š with
ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘(cid:206) ğ‘˜ğ‘— =1âˆ¥ğ‘¨ğ‘—ğšª ğ‘˜,ğ‘Ÿğ‘˜âˆ¥2, p ofro JLba eb mili bt ey da dt inle ga sst (a1 sâˆ’ nuğœ‚ m.T eh rois usco on pc te impt ap lfe ar mm ii lt is esge en xe isr ta )l .discussion
ğšª(ğ‘—) =(cid:40) ğ‘¨ğ‘˜ğšª ğ‘˜ğ‘« ğ‘˜âˆ’1 ifğ‘˜ â‰¤ ğ‘— , (2) Theorem 2.4. Fix ğœ‚,ğœ€ âˆˆ (0,1) such that ğœ€ â‰¤ [ğ‘…Ëœâˆ’1 + 2âˆ’1 +
ğ‘˜ ğšª ğ‘˜ ifğ‘˜ > ğ‘— (ğ‘ğ‘…Ëœ )âˆ’1]âˆ’1ln2. Let L = span{âƒğ‘ ğ‘—=1Î“ ğ‘—,ğ‘Ÿğ‘— : ğ‘Ÿ ğ‘— âˆˆ [ğ‘… ğ‘—],ğ‘— âˆˆ [ğ‘]},Abbr,MMMDDâ€“DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
where each ğšª ğ‘— âˆˆ Rğ‘›ğ‘—Ã—ğ‘…ğ‘— has orthonormal columns. Draw ğ‘¨ğ‘— âˆˆ Lğ‘— = {[ğš² | ğšª 1,...,ğšª ğ‘] : ğšª ğ‘— âˆˆ Rğ‘›ğ‘—Ã—ğ‘…ğ‘—,ğšªâŠ¤ ğ‘— ğšª ğ‘— = ğ‘°ğ‘…ğ‘—}.Letğ‘¨ğ‘˜ âˆˆ
Rğ‘šğ‘—Ã—ğ‘›ğ‘— fromanğœ‚/ğ‘-optimalfamilyofJLdistributions,with Rğ‘šğ‘˜Ã—ğ‘›ğ‘˜,ğ‘˜ âˆˆ [ğ‘],with
ğ‘š ğ‘— â‰¥ ğ¶Ëœ ğ‘—ğ‘… ğœ€Ëœ 22ğ‘2 ln(cid:16)ğ‘… ğœ‚2 ğ‘—ğ‘ (cid:17) ğ‘š ğ‘˜ â‰¥ ğ¶Ëœ ğ‘—ğ‘ ğœ€22ğ‘Ëœ ğ‘— ln(cid:16)2ğ· ğ‘—( ğœ‚ğ‘âˆ’1)(cid:17) ,
andwhereğ¶Ëœ
ğ‘—
>0issomeabsoluteconstant.Thenwithprobability
bedrawnfromanğœ‚/(2ğ‘)-optimalfamilyofJLdistributions,where
atleast1âˆ’ğœ‚, (cid:12) (cid:12)âˆ¥ğ’€ ğ‘—Ã—ğ‘ =1ğ‘¨ğ‘—âˆ¥2 âˆ’âˆ¥ğ’€âˆ¥2(cid:12) (cid:12)â‰¤ğœ€âˆ¥ğ’€âˆ¥2 ğ¶ a ğ’Ëœ nğ‘— Ã—di
ğ‘
ğ‘˜s ğ‘Ëœ =a ğ‘— 1n = ğ‘¨a ğ‘˜db .is mo Wlu s itpt he anc po rLn obs ğ‘—t a.a bDn it e li, fi tğ· yneğ‘— attâ‰¥ h lee( a2 r sağ‘ tËœ nğ‘— 1d+ âˆ’om1 ğœ‚) ,(cid:0) li(cid:206) neğ‘˜ â„“ a=âˆ’ r11 mğ‘š aâ„“ p(cid:1) ğ¿(cid:0)(cid:206) viğ‘ â„“ a=ğ‘˜ ğ¿+ (1 ğ’ğ‘› )â„“ =(cid:1) ,
forallğ’€ âˆˆL.
(cid:12) (cid:12)âˆ¥ğ¿(ğ‘¿âˆ’ğ’€)âˆ¥2 âˆ’âˆ¥ğ‘¿âˆ’ğ’€âˆ¥2(cid:12) (cid:12)â‰¤ğœ€âˆ¥ğ‘¿âˆ’ğ’€âˆ¥2
Proof. Whendrawnfromanğœ‚/ğ‘-optimalfamilyofJLdistri-
butions, ğ‘¨ğ‘— is ağ›¿/ğ‘-JL embedding of Sğ‘— âŠ‚ Rğ‘›ğ‘— into Rğ‘šğ‘— with forallğ’€ âˆˆ Lğ‘—.Inparticular,if2ğ‘… ğ‘— <ğ‘› ğ‘—,asufficientconditionfor
probability1âˆ’ğœ‚/ğ‘‘provided theembeddingdimensionis
|Sğ‘—|=ğ‘…2 ğ‘— â‰¤ ğœ‚ ğ‘ exp(cid:16)(ğ›¿/ğ‘ ğ¶) ğ‘—2ğ‘š ğ‘—(cid:17) , ğ‘š ğ‘˜ â‰¥ ğ¶Ëœ ğ‘—(ğ‘ ğœ€+ 21)3ğ‘Ëœ ğ‘— ln(cid:16) ğ‘4 +âˆšğ‘› 1Ëœ ğœ‚(cid:17) ,
whereğ¶ >0issomeabsoluteconstant.Thissufficientcondition
isequivağ‘—
lenttoğ‘š
ğ‘—
â‰¥ğ¶ ğ‘—ğ‘2ğ›¿âˆ’2ln(ğ‘…2 ğ‘—ğ‘ğœ‚âˆ’1).
whereğ‘›Ëœ =max ğ‘˜âˆˆ[ğ‘]ğ‘› ğ‘˜.
ByProposition2.3(conditionalontheğ‘¨ğ‘—sbeingappropriate Proof. Wepresentasketchoftheproof.Consideranyğ’€ âˆˆLğ‘—.
JL-embeddings), ByProposition2.3andanargumentsimilartothatinTheorem2.4,
(cid:12) (cid:12) (cid:12)âˆ¥ğ’€âˆ¥2 âˆ’(cid:13) (cid:13)ğ’€ ğ‘—Ã—ğ‘ =1ğ‘¨ğ‘—(cid:13) (cid:13)(cid:12) (cid:12) (cid:12)â‰¤ ğ‘…Ëœğ›¿ 1ğ‘’ +ğ›¿( ğ›¿2+ /ğ‘… (Ëœ 2+ ğ‘2/ )ğ‘) âˆ¥ğ’€âˆ¥2. drawingtheğ‘¨ğ‘—sfro
(cid:12)
(cid:12)âˆ¥m ğ’€a âˆ¥n
2
âˆ’ğœ€/ âˆ¥( ğ¿4ğ‘… (Ëœ ğ’€ğ‘) )- âˆ¥o 2p
(cid:12)
(cid:12)t â‰¤im 2ğœ€al âˆ¥ğ’€JL âˆ¥2familyyields
Takingğ›¿ =ğœ€/(2ğ‘…Ëœ )yieldsthedesiredsufficientconditiononğ‘š
ğ‘—
and
allowstheRHSabovetobeboundedbyğœ€âˆ¥ğ’€âˆ¥2.Thefinalpartof withprobability1âˆ’ğœ‚/2.Thesufficientlowerboundonğ‘š ğ‘˜isomitted
asatighterboundwillbeintroducedshortly.
theclaim,the1âˆ’ğœ‚probabilitybound,holdsbyaunionboundover
Observethat
ğ‘— âˆˆ [ğ‘]. â–¡
1
2.3 InexactTuckerDecompositionsUnderJL vec(ğ’€ [âŠ¤ ğ‘—])=(cid:8) ğ‘°ğ‘›ğ‘— âŠ— (cid:2)(cid:0)(cid:204) ğšª ğ‘˜(cid:1)ğš²âŠ¤ [ğ‘—](cid:3)(cid:9) ğ‘ªğ‘—ğ›¾ ğ‘— =:ğ‘©ğ‘—ğ›¾ ğ‘—
Embeddings ğ‘˜=ğ‘
ğ‘˜â‰ ğ‘—
WheretheprevioussectionconcernedtensorswithanexactTucker
decomposition,thefollowingresultsconsiderapproximatedecom- whereğ¶ ğ‘—ğ›¾ ğ‘— = vec(ğšªâŠ¤ ğ‘— ) âˆˆ Rğ‘›ğ‘—ğ‘…ğ‘— representsvec(ğšªâŠ¤ ğ‘— ) âˆˆ Rğ‘›ğ‘—ğ‘…ğ‘— in
positions.ThefollowinglemmsisadirectmodificationofTheorem termsofğ›¾
ğ‘—
âˆˆRğ‘ğ‘— (withğ‘ªğ‘— âˆˆR(ğ‘›ğ‘—ğ‘…ğ‘—)Ã—ğ‘ğ‘— andğ‘
ğ‘—
<ğ‘› ğ‘—ğ‘… ğ‘—),whichis
5of[9],whichwepresentherewithoutproof. possibleduetoğšª ğ‘—â€™sorthogonalcolumns.Letğ‘Ëœ
ğ‘—
â‰¤ğ‘
ğ‘—
denotethe
whL ere em tm hea ğ‘»2 ğ‘Ÿ. s5 f. orL met ağ‘¿ noâˆˆ rthR oğ‘› n1 oÃ— rÂ· mÂ·Â· aÃ— lğ‘› sğ‘ eta in nd Rğ’€ ğ‘›1âˆˆ Ã—Â·Â·L Â·Ã—ğ‘›âŠ‚ ğ‘.s Lp ea tn P{ Lğ‘» âŠ¥ğ‘Ÿ} dğ‘Ÿ eâˆˆ n[ oğ‘… t] e, r s tha un ib sk s spo paf ac ceğ‘© eoğ‘— .. fI Rt ğ‘›is 1Ã—cl Â·e Â·Â·a Ã—r ğ‘›t ğ‘h .a Lt eL t{ğ‘— ğ‘»i ğ‘Ÿs }c ğ‘Ÿo âˆˆn [ğ‘t Ëœa ğ‘—i ]n be ed aw nit oh ri tn hoa nğ‘Ëœ oğ‘— r- mdi am le bn ass ii son foa rl
theorthogonalprojectionoperatorontoLâŠ¥.Fixğœ€ âˆˆ (0,1)andletğ¿
Nowconsiderthe2ğ‘Ëœ
ğ‘—
+1elementsof
bealinearoperatorsuchthat
(( iii )) ğ¿ ğ¿ani ids sa an nğœ€ ğœ€/ /2 (- 2J âˆšL ğ‘…e )m -Jb Led ed min bg edo df iL ngâˆª o{ fP LâŠ¥(ğ‘¿)}intoRğ‘š1Ã—Â·Â·Â·Ã—ğ‘šğ‘, S ğ‘—â€² ,0={P LâŠ¥(ğ‘¿)}âˆª ğ‘Ÿâˆˆ(cid:216)
[ğ‘Ëœ
ğ‘—](cid:110) âˆ¥P PL LâŠ¥ âŠ¥( (ğ‘¿ ğ‘¿)
)âˆ¥
Â±ğ‘»ğ‘Ÿ(cid:111) .
intoRğ‘š1Ã—Â·Â·S Â·Ã—â€² ğ‘š= ğ‘.ğ‘Ÿâˆˆ(cid:216) [ğ‘…](cid:110) âˆ¥PP LL âŠ¥âŠ¥ (( ğ‘¿ğ‘¿ ))
âˆ¥
Â±ğ‘»ğ‘Ÿ(cid:111) I
ğ‘¨
(cid:0)n (cid:206)ğ‘˜du
ğ‘
â„“isc =t ğ‘˜ai +v
n
1el ğ‘›ğœ€y
/
â„“d
(
(cid:1)2e mğ‘’fi
ğ‘
on
âˆšï¸
de eğ‘S
Ëœ -ğ‘—
ğ‘˜ğ‘—â€² ),ğ‘˜
- fiJL
b=
ee
r{
m
sğ’ 1bÃ— oeğ‘˜ fdğ‘¨
td
hiğ‘˜ eng: elğ’
o
emfâˆˆ
t
ehS
ne
tğ‘—â€² s, (ğ‘˜
2
oâˆ’
ğ‘
fËœ1
ğ‘—
S}.
+
ğ‘—â€²F ,ğ‘˜1o âˆ’)r 1(cid:0)e (cid:206)a wc
iğ‘˜
â„“h
t=âˆ’
hğ‘˜
11
pâˆˆ
ğ‘š
roâ„“[ bğ‘
(cid:1)Ã—
a] -,
bilityatleast1âˆ’ğœ‚/(2ğ‘)providedthat
Then(cid:12) (cid:12)âˆ¥ğ¿(ğ‘¿âˆ’ğ’€)âˆ¥2âˆ’âˆ¥ğ‘¿âˆ’ğ’€âˆ¥2(cid:12) (cid:12)â‰¤ğœ€âˆ¥ğ‘¿âˆ’ğ’€âˆ¥2 forallğ’€ âˆˆL.
ğ‘˜âˆ’1 ğ‘
Loosely,Lemma2.5providesconditionsunderwhichalinear (2ğ‘Ëœ
ğ‘—
+1)(cid:0)(cid:214) ğ‘š â„“(cid:1)(cid:0) (cid:214) ğ‘› â„“(cid:1)
operatorcanuniformly(overğ’€ insomesubspaceofinterestL)
â„“=1 â„“=ğ‘˜+1
e nm eeb de ed dğ‘¿ forâˆ’ thğ’€ ef mor as joo rm re esa ur lb ti it nra Try het oe rn es mor 2ğ‘¿ .6. bT eh lois wg 6e an ..eralresultis
â‰¤
ğœ‚ exp(cid:16)[ğœ€/(2ğ‘’ğ‘âˆšï¸ğ‘Ëœ ğ‘—)]2ğ‘š ğ‘—(cid:17)
.
2(ğ‘âˆ’1) ğ¶
ğ‘—
Theorem2.6. Forğ‘ â‰¥2,fixğ‘¿ âˆˆRğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘;ğœ‚,ğœ€ âˆˆ (0,1)withğœ€ â‰¤
[(2ğ‘…Ëœ )âˆ’1+4âˆ’1+(2ğ‘ğ‘…Ëœ )âˆ’1]âˆ’1ln2;andğ‘— âˆˆ [ğ‘].Alsofixğšª ğ‘˜ âˆˆRğ‘›ğ‘˜Ã—ğ‘…ğ‘˜, 1Amode-ğ‘˜ fiberofatensorğ‘¿ âˆˆ Rğ‘›1Ã—Â·Â·Â·Ã—ğ‘›ğ‘ (ğ‘ â‰¥ ğ‘˜)isavectoroftheform
ğ‘˜ âˆˆ [ğ‘]\{ğ‘—},withorthonormalcolumnsandğš²âˆˆRğ‘…1Ã—Â·Â·Â·Ã—ğ‘…ğ‘.Define (ğ‘¿ğ‘›1,...,ğ‘›ğ‘˜âˆ’1,ğ‘–,ğ‘›ğ‘˜+1,...,ğ‘›ğ‘)ğ‘–âˆˆ[ğ‘›ğ‘˜],whereğ‘Ÿğ‘— âˆˆ[ğ‘›ğ‘—]forğ‘—â‰ ğ‘˜areOblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDDâ€“DD,YYYY,Location
Whereğ·
ğ‘—
isanupperboundfortheLHSabove,asufficientcondi- Algorithm1OrthogonalTuckerdecompositionviaHOOIwith
tionforthisresultis randomembeddings(HOOI-RE)
ğ¶Ëœ ğ‘—ğ‘2ğ‘Ëœ ğ‘— (cid:16)2ğ· ğ‘—(ğ‘âˆ’1)(cid:17) Require: datağ‘-tensorğ‘¿;initialestimatesğšª,...,ğšª ğ‘,ğš²;maximum
ğ‘š ğ‘˜ â‰¥ ğœ€2 ln ğœ‚ . iterationsğ‘ iter;relativetoleranceğœ€ rel
Initializeğ‘«ğ‘—,ğ‘— âˆˆ [ğ‘]
anB ğœ€/y (( 2a âˆšï¸s ğ‘l Ëœi ğ‘—g )h -Jt Lm eo md bifi edca dt ii no gn oo ff S)L ğ‘—â€²e (m wim tha p9 roo bf a[ b9 i] l, iti ytf ao tll lo ew ass tt 1h âˆ’at ğœ‚ğ¿ /2is
).
ğ‘¿ rËœ epâ† eağ‘¿
t
Ã—ğ‘—âˆˆ[ğ‘] (ğ‘­ğ‘—ğ‘«ğ‘—) âŠ²Mix
Lemma2.5thusappliesandyieldsthedesiredresult.Thesecond
âˆš âˆš Generateğ‘ºğ‘—,ğ‘— âˆˆ [ğ‘] âŠ²Formembedding
boundfollowsunder2ğ‘… ğ‘— <ğ‘› ğ‘— bythebound ğ‘+1ğ‘ â‰¤ ğ‘’ğ‘’ â‰¤2. â–¡ forğ‘— âˆˆ [ğ‘]do âŠ²Updateğšª ğ‘—s
3 EMPIRICALEVALUATION
ğ‘¿ ğšªËœ ğ‘—ğ‘— â†â† ağ‘¿ rËœ gÃ— mğ‘˜â‰  inğ‘— ğšªğ‘º ğ‘—ğ‘˜
âˆ¥ğ‘¿Ëœ
ğ‘—
âˆ’ğš²Ã—ğ‘˜â‰ ğ‘— (ğ‘ºğ‘˜ğšª
ğ‘˜âŠ² )A Ã—p ğ‘—p ğšªl ğ‘—y âˆ¥embedding
3.1 DataandSetup endfor
Wenowpresenttworeal-worldapplicationsofrandomembed- ğ‘¿Ëœ â†ğ‘¿Ëœ Ã—ğ‘˜âˆˆ[ğ‘]ğ‘ºğ‘— âŠ²Applyembedding
dingstotheestimationofTuckerdecompositions.Ourmaingoal ğš²â†argmin ğš²âˆ¥ğ‘¿Ëœ ğ‘— âˆ’ğš²Ã—ğ‘˜âˆˆ[ğ‘] (ğ‘ºğ‘˜ğšª ğ‘˜)âˆ¥
istodemonstrateempiricallythatcompressedestimationcanoffer âŠ²Updateğš²
significantreductionsincomputationtimeinpractice,evenfor untilrelativefitdoesnotimprovebyatleastğœ€ reloruntilğ‘
iter
moderatelysizedproblems,withminimalimpactonreconstruction iterationsreached
error.AllanalyseswereperformedinMATLAB2023aandusethe forğ‘— âˆˆ [ğ‘]do âŠ²Unmix
implementationofmodewisetensormultiplicationintheTensor ğšª
ğ‘—
â†ğ‘«ğ‘—ğ‘­ğ‘—âŠ¤ğšª
ğ‘—
Toolboxpackage(v3.5)onanInteli7-8550UCPUwith16GBof
endfor
RAM. returnğšª 1,...,ğšª ğ‘,ğš²
ThefirstanalysisusestheORLdatabaseoffaces,2acollectionof
400grayscaleimagesofsize92Ã—112,featuring40subjectsunder
10lightingconditionsandwithdifferentfacialexpressions.We
formğ‘¨ğ‘— =ğ‘ºğ‘—ğ‘­ğ‘—ğ‘«ğ‘— whenupdatingeachfactormatrixğšª ğ‘— andthe
treatthedatasetasa92Ã—112Ã—400tensorandconsiderTucker coretensorğš².Specifically,ğ‘ºğ‘— âˆˆRğ‘šğ‘—Ã—ğ‘›ğ‘— isarow-samplingmatrix,
decompositionsofrank(ğ‘…,ğ‘…,ğ‘…)forğ‘… âˆˆ{5,15,30}.Theembedding ğ‘­ğ‘— âˆˆ Rğ‘›ğ‘—Ã—ğ‘›ğ‘— is someorthogonal matrix (herea discrete cosine
dimension(ğ‘š1,ğ‘š2,ğ‘š3)iscontrolledbyasingle(approximate)di- transformationmatrix),andğ‘«ğ‘— âˆˆRğ‘›ğ‘—Ã—ğ‘›ğ‘— isadiagonalRademacher
mensionreductionfactorDR(i.e.,DRâ‰ˆğ‘š ğ‘—/ğ‘› ğ‘— forallmodesğ‘— âˆˆ [ğ‘] matrix.Thethirdalgorithm,HOOI-RE*,isthesameasHOOI-REbut
towhichcompresionisapplied).
usesthefulldatatoestimatethecoretensor:specifically,thecore
Thesecondanalysisusesresting-statefMRIdata,specifically
forthehippocampus,obtainedfromtheAlzheimerâ€™sDiseaseNeu-
tensorupdateinHOOI-RE*seekstominimizeâˆ¥ğ‘¿âˆ’ğš²Ã—ğ‘˜âˆˆ[ğ‘] ğšª ğ‘˜âˆ¥.
roimagingInitiative(ADNI).3ThedataisasubsetofthatinWang
HOOI-REappliesğ‘­ğ‘—ğ‘«ğ‘— inaninitialpreprocessingstep:loosely,
thismixingstepâ€œspreadsâ€informationwithinthedatağ‘¿andmakes
etal.[20],wheredetailsregardingdataacquisition,extraction,and
subsequentupdateslesssensitivetoğ‘ºğ‘—.Decomposingthemixed
processingcanbefound.Thehippocampalsurfaceofeachofthe
datağ‘¿Ëœ isequivalenttodecomposingğ‘¿,soweneedonlyâ€œunmixâ€the
824subjectsisparameterizedasa100Ã—150Ã—2tensor(withmodes
estimatesandreturntotheoriginalğ‘¿ spaceattheend.Algorithm
correspondingtorotationaroundthehippocampalsurface,length
alongthesurface,andtheleft/righthippocampus,respectively,as 1usesclosed-formupdatesfortensorcomponents,namelyğšª ğ‘— â†
showninFigure6intheappendix).Foursurface-basedfMRImea-
ğ‘¼ğ‘—ğ‘½ğ‘—âŠ¤,whereğ‘¼ğ‘— andğ‘½ğ‘— containtheleftandrightsingularvectors
sures,radialdistanceandthreemTBMfeatures,areavailablefor fromathinSVDof[ğ‘¿Ëœ ğ‘— Ã—ğ‘˜â‰ ğ‘— (ğ‘ºğ‘˜ğšª ğ‘˜)] [ğ‘—]ğš²âŠ¤ [ğ‘—],andğš²â†ğ‘¿Ëœ Ã—ğ‘—âˆˆ[ğ‘]
eachsubject,fora100Ã—150Ã—2Ã—4Ã—824datatensor.Weconsider {[(ğ‘ºğ‘—ğšª ğ‘—)âŠ¤(ğ‘ºğ‘—ğšª ğ‘—)]âˆ’1(ğ‘ºğ‘—ğšª ğ‘—)âŠ¤} (whereapseudoinverseisusedin
Tuckerdecompositionsofrank(ğ‘…,ğ‘…,2,4,ğ‘…)forğ‘… âˆˆ{5,15,30}and theonecasewhereğ‘š ğ‘— < ğ‘… ğ‘—).Furthermore,theğ‘ºğ‘— matricesare
reductionsalongthefirst,second,andlastmodes.Noembeddings formedandappliedimplicitlyviasubsettingratherthanexplicit
areappliedtothethirdorfourthmodes. matrixmultiplication.HOOI,HOOI-RE,andHOOI-RE*useğœ€ rel=
Ineachanalysis,weconsidersixestimationmethods.Thefirstis 1Ã—10âˆ’5andğ‘ iter=100(wherethelatterisneverreached).
atraditionalalternatingleastsquares(ALS)algorithm,specifically, ThelastthreemethodsuseHOSVD[10]andtwoTensorSketch
higher-orderorthogonaliteration(HOOI)[10].Thesecond,presented algorithms(TUCKER-TSandTUCKER-TTMTSwithdefaultset-
inAlgorithm1asHOOI-RE,4appliesobliviousJLembeddingsofthe tings,asproposedandimplementedin[15]).Theyarepresentedfor
thesakeofcomparisontoother(traditionalandrecent)approaches
2AvailablefromAT&TLaboratoriesCambridgeathttp://cam-orl.co.uk/facedatabase. forestimatingTuckerdecompositions.
html
3DatausedinpreparationofthisarticlewereobtainedfromtheAlzheimerâ€™sDis-
easeNeuroimagingInitiative(ADNI)database(adni.loni.usc.edu).Assuch,thein- 3.2 Results
vestigatorswithintheADNIcontributedtothedesignandimplementationofADNI Figure1visualizesthetotalcomputationtimeandfinalreconstruc-
and/orprovideddatabutdidnotparticipateinanalysisorwritingofthisreport.A
completelistingofADNIinvestigatorscanbefoundat:http://adni.loni.usc.edu/wp- tionerrorfortheHOOI,HOOI-RE,andHOOI-RE*algorithmsover
content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf 100replications.Table1providesanumericalsummaryofthere-
4Ourimplementationandcodeforthefollowingnumericalstudiesareavailable
sultsforallsixalgorithms.Forlargedecompositions,improvements
inananonymizedrepositoryathttps://anonymous.4open.science/r/tucker_hooi-re-
0CE3/README.md incomputationtimeareclearwhenDR < 0.8forHOOI-REandAbbr,MMMDDâ€“DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
Face
HOOI
HOOIâˆ’RE*
HOOIâˆ’RE
R = 5 R = 15 R = 30 R = 5 R = 15 R = 30
DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Hippocampus
R = 5 R = 15 R = 30 R = 5 R = 15 R = 30
DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Figure1:TotalruntimeandFrobeniusreconstructionerrorfortheHOOI,HOOI-RE*,andHOOI-REalgorithmsintheface(left)
andhippocampus(right)analyses,summarizedover100replications.HorizontaldottedlinesindicatemedianHOOIresults.
SeeTable1forHOOI-REerrorsforğ‘…=30andDR=0.2,whichareomittedheretoavoidskewingtheplot.
HOOI-RE*:56%â€“73%reductionsinthefaceanalysisand40%â€“75% HOOI (R = 5) HOOI-RE (R = 5)
reductionsinthehippocampusanalysiswhenğ‘…=30.Atthesame
time,reconstructionerroronlysuffersslightly:a3%â€“11%increase
inFrobeniusreconstructionerrorinthesamesettingacrossboth
analyses.
NotableexceptionstothistrendoccurforHOOI-REwhenDR=
0.2orwhenDR=0.4andğ‘…=30â€”thatis,whereğ‘š
ğ‘—
issmallrelative
toğ‘… .Inthesesettings,HOOI-REyieldsamuchhigherreconstruc-
ğ‘—
tionerrorthanHOOI.AcomparisonoftheresultsforHOOI-RE
andHOOI-RE*suggestthattheincreasederrorisattributableto
theinstabilityoftheleast-squaresupdateofğš²whenğ‘š iscloseto
ğ‘— HOOI (R = 30) HOOI-RE (R = 30)
orlowerthanğ‘… .Insuchextremesettings,HOOI-RE*retainsgood
ğ‘—
performance.
AsshowninTable1,HOSVDandHOOI-REaregenerallycom-
parableinthefaceanalysis.However,withthemuchlargerhip-
pocampusdata,HOSVDtakessubstantiallylongerthanHOOI-RE
(evenunderminimalcompression)andyieldsestimateswithsub-
stantially larger reconstruction error. In both analyses, the two
TensorSketchmethodsyieldreconstructionerrorshigherthanthat
forHOOI-REwithDR=0.4.Inmostsettingsinthehippocampus
analysis,theTensorSketchmethodsrunoutofmemory(evenwhen
usingvariantsofthealgorithmsthatneverholdthefulldatatensor
inmemory). Figure2:HOOIandHOOI-RE(DR=0.5)reconstructionsfor
In the face analysis, for the same value ofğ‘… (which controls 16randomlyselectedfacesinthefaceanalysis.Theoriginal
thecoarsenessofthedecomposition),HOOI-REappearstoencode dataisvisualizedinFigure4.
thesamelevelofdetailasHOOI,albeitwithextranoise(Figure2).
HOOIandHOOI-REreconstructionsinthehippocampusanalysis
arealsocomparable(Figure3).Foranillustrationoftheeffectof
Forsmallmodelsorwhendimensionreductionisnotsubstantial
increaseddimensionreductioninthefaceanalysis,seeFigure4.
(e.g., whenğ‘… = 5 or DR = 0.8), HOOI-RE and HOOI-RE* tend
)s(
emiT
)s(
emiT
5.1
0.1
5.0
0.0
07
05
03
01
0
rorrE
rorrE
004
003
002
001
0
0006
0004
0002
0OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDDâ€“DD,YYYY,Location
Original DR 0.8
HOOI (RD) HOOI-RE (RD)
DR 0.6 DR 0.4
HOOI (mTBM3) HOOI-RE (mTBM3)
Figure4:EffectofdimensionreductiononHOOI-RErecon-
Figure3:HOOIandHOOI-REreconstructions(ğ‘…=30,DR= structionsinthefaceanalysis(ğ‘… = 30)for16randomlyse-
0.5)oftheradialdistance(RD)andmTBM3imagingmeasures lectedimages.
forasinglepatientinthehippocampusanalysis.
problemsizesubstantiallyandobtainhigh-qualityTuckerdecom-
positionsinafractionofthetime.RelativetotraditionalHOOI
to require more computation time and, as before, yield greater andHOSVD,ourmethodappearstoscalewellwithdatasizeğ‘›
reconstructionerrorthanHOOI.Thereasonforthisisclearfrom anddecompositionsizeğ‘….Theproposedmethodalsosubstantially
Figure5,whichsummarizesaveragetimeperiterationspentoneach outperformedrecentlydevelopedTensorSketchmethodsforHOOI
partoftheHOOIalgorithms(withnumericalresultsinTable2ofthe intermsofreconstructionerrorand,particularlyforlargertensors,
appendix).Briefly,usingembeddingsincursanoverheadcostthat computationtimeandcomputationalfeasibility.
mayormaynotbeoutweighedbytheimprovementincomputation While â€œcompressedâ€ tensor decompositions are not the only
timeneededtoupdatethefactormatricesorcoretensor.There general tool needed, decompositions and low-rank approxima-
isanetimprovementwhenthesizeofthedecompositionislarge tionsarearguablyanimportantpartofmanytensormethods[13].
or when the amount of dimension reduction is substantial (i.e., Ourresultsencourage furtherapplications totensorregression
largeğ‘…orsmallDR).HOSVD,likeHOOI,spendsalargemajority andotherspecializedmethods,particularlythoseseekingarich
ofitsruntimeonupdatestothefactormatrices(Table2inthe model/decompositionspacethroughTuckerrepresentations.There
appendix).Ourresultshighlighthowtheproposedapproachwith aresettingswhereTuckerdecompositionsmaybepreferredoverCP
JLembeddingscanreduceproblemdimensionalityandmitigate decompositionsforreasonsbeyondflexibility.Thelatterrequires
thiscomputationalbottleneckwhilepreservingtheintegrityofthe theğ‘… stobeequal,butitmaybemoreparsimonioustouseaTucker
ğ‘—
estimateddecompositioncomponents. decompositionwithgreatlyvaryingğ‘… s[11].Whenanalyzingour
ğ‘—
hippocampusdata,forexample,onemaydesiregreatervariability
4 SIGNIFICANCEANDIMPACT betweenpatient-levelreconstructions,soitmaybepreferableto
Theimportanceofefficientmethodsfortensoranalysisgrowsto-
haveğ‘…5(alongtheâ€œpatientâ€mode)largeandtheotherğ‘… ğ‘—ssmall.
ExpandedresultsforTuckerdecompositions,suchasthoseinthis
getherwiththesizeandrichnessofdataavailabletoresearchers.
work,canthussupportdomain-specificdevelopmentsevenifcor-
Thisisespeciallytrueinappliedfieldswheredomain-specificin-
respondingresultsforCPdecompositionsexist.5
sightistetheredtodataacquisitionandrelatedtechnology.Medical
WeconsideredanHOOIalgorithminthisworkpartlybecause
imagingisaprimeexampleofthis.Datasizemayinitiallyencour-
ofitsrarityintheliteraturerelativetoHOSVD.Morespecifically,
ageresearcherstoreduceoraltogethereliminatetensordatafrom
while HOSVD is typically favored for computational speed [4],
ananalysisplanâ€”e.g.,bysummarizingneuroimagingfeaturesover
HOOI(andthus,algorithmsthatwidentheapplicabilityofHOOIto
predefinedregionsofinterestorbydownsamplingtoamanageable
largetensordata)areofspecificinterestforthesakeforimproved
size,evenwhennotstatisticallyjustifiable.However,tensor-based
methodsfordimensionreductioncanmakelarge-scaleanalyses
5WhileTheorem2.4appearstoberestrictedtoğš²,Theorem2.6canbemodifiedto
feasibleonreadilyaccessiblecomputingresources.Theempirical
resultsinSection3showthatourapproachtoHOOIcanreduce a ac pc po lyun totf ğš²or bt yhi as d( jP uL stâŠ¥ in( gğ‘¿ L) ğ‘—= .W0) ec ha ase v. eS oim mi il ta tr el dy, tT heh se eor de em tai2 ls.6 foca rn brb ee ve ita ys .ilymodifiedtoAbbr,MMMDDâ€“DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
Figure5:AveragetimespentperiterationoneachpartoftheHOOI,HOOI-RE*,andHOOI-REalgorithmsintheface(left)and
hippocampus(right)analyses,averagedover100replications.Barsrepresentstandarderror.Preprocessingtimeisnotincluded,
butseeTable2intheappendixforquantitativesummaries.
decompositionquality.Ourresultsinfactshowthattheproposed quality.Wedidnotinvestigatethistheoreticallyorpresentrelated
HOOImethodcanoutperformHOSVDintermsofbothcompu- empiricalresultsinthiswork.Ourchoicetovarytheğ‘ºğ‘—swaspri-
tationtimeanddecompositionquality,withtheperformancegap marilymotivatedbyourinterestinthissourceofrandomness,from
growingwiththesizeandorderofthedata.Themajorityofthe theperspectiveofposteriorapproxmationinBayesiansettings[12].
improvementincomputationtimeperiterationappearstostem Specifically,workonBayesianhierarchicaltensormodelingiscur-
fromimprovementsinfactormatrixupdates,inturnduetofaster rently quite limited: while very few works consider dimension
SVDs.RandomizedSVDhasitselfreceivedmuchattentioninthe reductionforBayesianregressionwithnon-tensordata[7,8],none
broadertensorliterature[1]. considerthisinthecontextofjointtensormodels.Wearecurrently
Weacknowledgethatfurtherimprovementinruntimeispossible developingacompressedBayesianTuckerdecompositionthatwe
byfixingthesamplingmatrices(i.e.,theğ‘ºğ‘—s)acrossiterations(sim- ultimatelyaimtoincorporateintojointhierarchicalmodels.
ilartothefixedsketchesin[15]),butthisinourexperiencetends Wedidnotconsiderasecondstageofdimensionreduction(e.g.,
toincreasereconstructionerrorandvariationinreconstruction byvectorizingandembeddingthecompressedtensorintoalow-
dimensionalvectorspacesuchasRğ‘š)asinIwenetal.[9]fortheCP
decomposition.AsimilarresultfortheTuckerdecompositionmay
Table1:Mean(standarddeviation)runtime(s)andFrobenius followreadily,butwehavenotconsideredthathereandleavethe
reconstruction error for the HOOI, HOOI-RE, HOOI-RE*, developmentforfuturework.Inanothervein,theoreticalconver-
HOSVD,andTensorSketchalgorithms(withğ¾ =10)inboth genceguaranteesaregenerallydifficulttoobtainforALSalgorithms
numericalstudies,calculatedover100simulations. (includingHOOI,evenwithoutrandomembeddings)[10],sowe
havenotconsideredsuchresultshere,norhaveweconsidereda
Method DRTime Error formalruntimeanalysis.
ğ‘…=5 ğ‘…=15 ğ‘…=30 ğ‘…=5 ğ‘…=15 ğ‘…=30
Face
HOOI 0.47(0.02) 0.57(0.01) 1.13(0.04) 237.5(0.0) 186.4(0.0) 158.1(0.0)
HOOI-RE*0.8 0.58(0.14) 0.55(0.10) 0.60(0.12) 240.0(0.8) 189.6(0.5) 161.9(0.5) REFERENCES
0.6 0.44(0.08) 0.42(0.07) 0.47(0.07) 241.2(1.8) 190.6(0.6) 163.2(0.6) [1] SalmanAhmadi-Asl,StanislavAbukhovich,MaameG.Asante-Mensah,Andrzej
0.4 0.39(0.07) 0.34(0.06) 0.38(0.07) 242.9(1.7) 193.0(1.0) 166.1(1.1) Cichocki,AnhHuyPhan,TohishisaTanaka,andIvanOseledets.2021.Random-
0.2 0.32(0.06) 0.27(0.04) 0.31(0.06) 249.4(3.1) 201.5(2.3) 176.3(2.6)
izedalgorithmsforcomputationofTuckerdecompositionandhigherorderSVD
HOOI-RE0.8 0.68(0.16) 0.70(0.13) 0.78(0.15) 238.3(0.5) 188.0(0.2) 160.8(0.2)
0.6 0.49(0.12) 0.48(0.08) 0.50(0.09) 239.7(1.2) 190.4(0.4) 166.3(0.5)
(HOSVD).IEEEAccess9(2021),28684â€“28706. https://doi.org/10.1109/ACCESS.
2021.3058103
0.4 0.38(0.09) 0.34(0.06) 0.30(0.06) 242.4(1.7) 196.5(0.8) 197.3(5.1)
[2] MicheleN.daCosta,RenatoR.Lopes,andJoaÂ¯oMarcosT.Romano.2016.Random-
0.2 0.28(0.05) 0.24(0.04) 0.19(0.04) 252.5(3.2) 284.7(32.9) 831.9(42.5)
HOSVD 0.45(0.04) 0.45(0.04) 0.45(0.05) 241.0(0.0) 186.8(0.0) 158.2(0.0)
izedmethodsforhigher-ordersubspaceseparation.In201624thEuropeanSignal
TS 0.25(0.01)106.24(9.42) â€  262.9(4.0) 199.7(0.4) â€ 
ProcessingConference(EUSIPCO).215â€“219. https://doi.org/10.1109/EUSIPCO.
2016.7760241
TTMTS 0.19(0.01) 2.97(0.06)144.22(3.24) 480.7(26.4) 394.1(6.9) 370.7(2.8)
[3] Sanjoy Dasgupta and Anupam Gupta. 2003. An elementary proof
Hippocampus
HOOI 14.60(0.28) 34.01(0.28) 44.74(0.41) 4480.1(0.0) 3481.5(0.0) 3099.2(0.0) of a theorem of Johnson and Lindenstrauss. Random Structures &
HOOI-RE*0.8 33.90(7.41) 42.72(8.05) 46.37(6.62) 4483.1(1.8) 3486.3(1.2) 3106.2(1.4) Algorithms 22, 1 (2003), 60â€“65. https://doi.org/10.1002/rsa.10073
0.6 20.88(3.75) 24.81(4.02) 26.69(3.57) 4487.7(2.4) 3492.7(2.7) 3116.2(2.5) arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10073
0.4 13.07(2.17) 15.03(2.27) 16.43(2.36) 4499.5(9.8) 3504.8(4.6) 3135.4(4.7) [4] LievenDeLathauwer,BartDeMoor,andJoosVandewalle.2000. Onthe
0.2 9.23(1.20) 9.32(1.19) 10.85(1.48)4533.8(22.8) 3546.3(11.2)3200.5(12.9) bestrank-1andrank-(ğ‘…1,ğ‘…2,...,ğ‘…ğ‘)approximationofhigher-ordertensors.
HOOI-RE0.8 29.98(7.11) 40.31(5.96) 44.47(7.56) 4490.6(2.9) 3494.3(2.1) 3124.7(2.6) SIAMJ.MatrixAnal.Appl.21,4(2000),1324â€“1342. https://doi.org/10.1137/
0.6 18.14(3.69) 21.30(3.18) 22.35(3.71)4508.4(10.5) 3516.1(3.5) 3179.9(9.5) S0895479898346995
0.4 11.68(2.02) 12.26(1.91) 11.16(1.68)4550.7(25.1) 3574.7(11.1)3466.8(58.8) [5] PetrosDrineasandMichaelW.Mahoney.2007.Arandomizedalgorithmfora
0.2 7.28(0.57) 7.28(0.89) 6.68(0.72)4712.4(69.1)4462.7(292.7)9900.4(11.2) tensor-basedgeneralizationofthesingularvaluedecomposition.LinearAlgebra
HOSVD 42.35(1.03) 42.35(0.87) 42.94(1.18) 5856.1(0.0) 4446.4(0.0) 3936.4(0.0) Appl.420,2(2007),553â€“571. https://doi.org/10.1016/j.laa.2006.08.023
TS 42.52(1.78)â€  â€  â€ 4790.0(12.5) â€  â€  [6] MinaGachloo,YuxingWang,andJingboXia.2019. Areviewofdrugknowl-
TTMTS 6.63(0.11) â€  â€ 6421.4(87.5) â€  â€  edgediscoveryusingBioNLPandtensorormatrixdecomposition.Genomics&
â€ indicatesthattheTSorTTMTSalgorithmranoutofmemory.Aversionofthe Informatics17,2(2019),e18.
[7] RajarshiGuhaniyogiandDavidB.Dunson.2015.Bayesiancompressedregres-
algorithmthatdoesnotholddatainmemory[15]wasusedinsteadwherepossible,
sion.J.Amer.Statist.Assoc.110,512(2015),1500â€“1514. https://doi.org/10.1080/
butinmostcasesalsoranoutofmemory. 01621459.2014.969425OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDDâ€“DD,YYYY,Location
[8] RajarshiGuhaniyogiandAaronScheffler.2021. SketchinginBayesianhigh
dimensional regression with big data using Gaussian scale mixture priors.
arXiv:2105.04795[stat.ME]
[9] MarkA.Iwen,DeannaNeedell,ElizavetaRebrova,andAliZare.2021. Lower
memoryoblivious(tensor)subspaceembeddingswithfewerrandombits:Mode-
wisemethodsforleastsquares.SIAMJ.MatrixAnal.Appl.42,1(2021),376â€“416.
https://doi.org/10.1137/19M1308116
[10] TamaraG.KoldaandBrettW.Bader.2009.Tensordecompositionsandapplica-
tions.SIAMRev.51,3(2009),455â€“500. https://doi.org/10.1137/07070111X
[11] XiaoshanLi,DaXu,HuaZhou,andLexinLi.2018. Tuckertensorregression
andneuroimaginganalysis.StatisticsinBiosciences10,3(2018),520â€“545. https:
//doi.org/10.1007/s12561-018-9215-6
[12] JunS.LiuandYingNianWu.1999.Parameterexpansionfordataaugmentation.
J.Amer.Statist.Assoc.94,448(1999),1264â€“1274.
[13] YipengLiu.2022.TensorsforDataProcessing:Theory,MethodsandApplications.
Elsevier.
[14] LinjianMaandEdgarSolomonik.2021.Fastandaccuraterandomizedalgorithms
forlow-ranktensordecompositions.InAdvancesinNeuralInformationProcessing
Systems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(Eds.).
https://openreview.net/forum?id=B4szfz7W7LU
[15] OsmanAsifMalikandStephenBecker.2018.Low-rankTuckerdecompositionof
largetensorsusingTensorSketch.InAdvancesinNeuralInformationProcessing
Systems,S.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,and
R.Garnett(Eds.),Vol.31.CurranAssociates,Inc. https://proceedings.neurips.cc/
paper_files/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf
[16] OsmanAsifMalikandStephenBecker.2020.GuaranteesfortheKroneckerfast
Johnsonâ€“Lindenstrausstransformusingacoherenceandsamplingargument.
LinearAlgebraAppl.602(2020),120â€“137. https://doi.org/10.1016/j.laa.2020.05.004
[17] RachelMinster,ArvindK.Saibaba,andMishaE.Kilmer.2020. Randomized
algorithmsforlow-ranktensordecompositionsintheTuckerformat. SIAM
JournalonMathematicsofDataScience2,1(2020),189â€“215. https://doi.org/10.
1137/19M1261043
[18] BeheshtehRakhshanandGuillaumeRabusseau.2020.Tensorizedrandompro-
jections.InProceedingsoftheTwentyThirdInternationalConferenceonArtificial
IntelligenceandStatistics(ProceedingsofMachineLearningResearch,Vol.108),
SilviaChiappaandRobertoCalandra(Eds.).PMLR,3306â€“3316.
[19] Charalampos E. Tsourakakis. [n.d.]. MACH: Fast randomized tensor
decompositions. 689â€“700. https://doi.org/10.1137/1.9781611972801.60
arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9781611972801.60
[20] YalinWang,YangSong,PriyaRajagopalan,TuoAn,KrystalLiu,Yi-YuChou,
BorisGutman,ArthurW.Toga,andPaulM.Thompson.2011. Surface-based
TBMboostspowertodetectdiseaseeffectsonthebrain:AnN=804ADNIstudy.
NeuroImage56,4(2011),1993â€“2010. https://doi.org/10.1016/j.neuroimage.2011.
03.040
[21] Hua Zhou, Lexin Li, and Hongtu Zhu. 2013. Tensor regression with
applications in neuroimaging data analysis. J. Amer. Statist. Assoc.
108, 502 (2013), 540â€“552. https://doi.org/10.1080/01621459.2013.776499
arXiv:https://doi.org/10.1080/01621459.2013.776499Abbr,MMMDDâ€“DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
A ADDITIONALBACKGROUNDRESULTS Multiplyingthegeneraltermabovebyâˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿğ‘—âˆ¥/âˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿğ‘—âˆ¥(whichis
LemmaA.1. Ifğ‘¥,ğ‘¦ âˆˆRğ‘› andğ‘¨âˆˆRğ‘šÃ—ğ‘› isanğœ€-JLembeddingof possibleunderthehypothesisthatmin ğ‘Ÿâˆˆ[ğ‘…ğ‘—]âˆ¥ğ‘©Î“ ğ‘—,ğ‘Ÿâˆ¥2 >0)yields
{ğ‘¥Â±ğ‘¦}âŠ‚Rğ‘› intoRğ‘š
,then
thefirstpartoftheclaim.
ğœ€ Fromtheformofğ’€â€²above,itisclearthatğœ‡ ğ’€â€²,ğ‘˜ =ğœ‡ ğ’€,ğ‘˜ forğ‘˜ â‰  ğ‘—.
(cid:12) (cid:12)âŸ¨ğ‘¨ğ‘¥,ğ‘¨ğ‘¦âŸ©âˆ’âŸ¨ğ‘¥,ğ‘¦âŸ©(cid:12) (cid:12)â‰¤ 2(âˆ¥ğ‘¥âˆ¥2 2+âˆ¥ğ‘¦âˆ¥2 2) Ontheotherhand,
â‰¤ğœ€max{âˆ¥ğ‘¥âˆ¥2 2,âˆ¥ğ‘¦âˆ¥2 2}. ğœ‡ ğ’€â€²,ğ‘— = ğ‘Ÿ,ğ‘ m âˆˆa [ğ‘…x
ğ‘—]
âˆ¥ğ‘©|âŸ¨ Î“ğ‘© ğ‘—Î“ ,ğ‘Ÿğ‘— âˆ¥,ğ‘Ÿ 2, âˆ¥ğ‘© ğ‘©Î“ Î“ğ‘—, ğ‘—ğ‘  ,ğ‘ âŸ© âˆ¥| 2.
Proof. Theclaimfollowsbyroutinemanipulation.Observethat
ğ‘Ÿâ‰ ğ‘ 
Forthefinalpartoftheclaim,observethat
(cid:12) (cid:12)âŸ¨ğ‘¨ğ‘¥,ğ‘¨ğ‘¦âŸ©âˆ’âŸ¨ğ‘¥,ğ‘¦âŸ©(cid:12)
(cid:12) âˆ¥ğ’€â€²âˆ¥=âˆ¥ğ’€ Ã—ğ‘˜ ğ‘©âˆ¥2 =âˆ¥(ğ’€ Ã—ğ‘— ğ‘©) [ğ‘—]âˆ¥2
F
=(cid:12) (cid:12) (cid:12)1 4(cid:0)âˆ¥ğ‘¨ğ‘¥ âˆ’+
1
4ğ‘¨ (cid:0)ğ‘¦ âˆ¥âˆ¥ ğ‘¥2 2 +âˆ’ ğ‘¦âˆ¥ âˆ¥ğ‘¨
2
2ğ‘¥ âˆ’âˆ’ âˆ¥ğ‘¥ğ‘¨ âˆ’ğ‘¦âˆ¥ ğ‘¦2 2 âˆ¥(cid:1)
2 2(cid:1)(cid:12) (cid:12)
(cid:12)
=(cid:13)
(cid:13) (cid:13)ğ‘©ğšª ğ‘—ğš²
[ğ‘—](cid:16)(cid:204)
ğ‘˜ ğ‘˜=
â‰ 1
ğ‘
ğ‘—
ğšª
ğ‘˜(cid:17)âŠ¤(cid:13)
(cid:13)
(cid:13)2
F
= 1 4(cid:12) (cid:12) (cid:12)(cid:0)âˆ¥ğ‘¨ğ‘¥+ğ‘¨ğ‘¦âˆ¥2 2âˆ’âˆ¥ğ‘¥+ğ‘¦âˆ¥2 2(cid:1) =âˆ¥ğ‘©ğšª ğ‘—ğš¿ ğ‘—âˆ¥2 F.
â‰¤
41(cid:104)(cid:12)
(cid:12)âˆ¥ğ‘¨ğ‘¥âˆ’ +(cid:0) ğ‘¨âˆ¥ğ‘¨ ğ‘¦ğ‘¥
âˆ¥2
2âˆ’ âˆ’ğ‘¨ âˆ¥ğ‘¦ ğ‘¥âˆ¥ +2 2 ğ‘¦âˆ’ âˆ¥âˆ¥
2
2ğ‘¥
(cid:12)
(cid:12)
âˆ’ğ‘¦âˆ¥2 2(cid:1)(cid:12) (cid:12) (cid:12) Thus,bydir âˆ¥e ğ’€ct â€²âˆ¥c 2om =p ğ‘âˆ¥u
ğ‘©
âˆ’t ğ‘—a ğšªt ğ‘—io ğš¿n ğ‘—, âˆ¥2
F
+(cid:12) (cid:12)âˆ¥ğ‘¨ğ‘¥âˆ’ğ‘¨ğ‘¦âˆ¥2 2âˆ’âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥2 2(cid:12) (cid:12)(cid:105) =âˆ‘ï¸ ğ‘–=1(cid:13) (cid:13)ğ‘©ğšª ğ‘—Î¨ ğ‘–(cid:13) (cid:13)2 2
â‰¤ ğœ€1 4(cid:0)ğœ€âˆ¥ğ‘¥+ğ‘¦âˆ¥2 2+ğœ€âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥2 2(cid:1) =ğ‘ âˆ‘ï¸âˆ’ğ‘—(cid:13)
(cid:13)
(cid:13)ğ‘©âˆ‘ï¸ğ‘…ğ‘—
Î“ ğ‘—,ğ‘Ÿğœ“
ğ‘Ÿ,ğ‘–(cid:13)
(cid:13)
(cid:13)2
2
= 2(cid:0)âˆ¥ğ‘¥âˆ¥2 2+âˆ¥ğ‘¦âˆ¥2 2(cid:1) ğ‘ğ‘–= âˆ’1
ğ‘—
ğ‘Ÿ ğ‘…= ğ‘—1
ğ‘…ğ‘—
â‰¤ğœ€max{âˆ¥ğ‘¥âˆ¥2 2,âˆ¥ğ‘¦âˆ¥2 2}, =âˆ‘ï¸ âŸ¨ğ‘©âˆ‘ï¸ Î“ ğ‘—,ğ‘Ÿğœ“ ğ‘Ÿ,ğ‘–,ğ‘©âˆ‘ï¸ Î“ ğ‘—,ğ‘ ğœ“ ğ‘ ,ğ‘–âŸ©
ğ‘–=1 ğ‘Ÿ=1 ğ‘ =1
whichprovestheclaim.Above,thefirstequalityholdsbythepolar-
izationidentityandtheinequalitiesbythetriangleinequality,the âˆ‘ï¸ğ‘…ğ‘— âˆ‘ï¸ğ‘…ğ‘— (cid:16)ğ‘ âˆ‘ï¸âˆ’ğ‘— (cid:17)
hypothesisthatğ‘¨isanğœ€-JLembedding,andbytheparallelogram = ğœ“ ğ‘Ÿ,ğ‘–ğœ“ ğ‘ ,ğ‘– âŸ¨ğ‘©Î“ ğ‘—,ğ‘Ÿ,ğ‘©Î“ ğ‘—,ğ‘ âŸ©
ğ‘Ÿ=1ğ‘ =1 ğ‘–=1
law(equivalently,bybasicpropertiesofinnerproducts). â–¡
ğ‘…ğ‘— ğ‘…ğ‘—
ThefollowingisaproofofLemma2.1ofthemaintext.
=âˆ‘ï¸âˆ‘ï¸ (ğš¿ğš¿âŠ¤)ğ‘Ÿ,ğ‘ âŸ¨ğ‘©Î“ ğ‘—,ğ‘Ÿ,ğ‘©Î“ ğ‘—,ğ‘ âŸ©.
ğ‘Ÿ=1ğ‘ =1
Proof. Theclaimfollowsbyroutinemanipulationandtheprop-
Above,thesecondequalityholdssince,foranyarbitraryğ‘¨âˆˆRğ‘›Ã—ğ‘š,
ertiesoftensormatricization.Observethat âˆ¥ğ‘¨âˆ¥2
F
=(cid:205)ğ‘š ğ‘–=1âˆ¥ğ´ ğ‘–âˆ¥2 2.Thethirdandfinalequalitiessimplyusethe
definitionofmatrixmultiplication.Thiscompletestheproof. â–¡
ğ’€ [â€²
ğ‘—]
=ğ‘©ğ’€[ğ‘—]
ğ‘…1 ğ‘…ğ‘ ğ‘ ReceivedXXXXXXX20XX;revisedXXXXXXX20XX;acceptedXXXXXXX
=ğ‘©âˆ‘ï¸ Â·Â·Â·âˆ‘ï¸ ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘(cid:0) âƒ Î“ ğ‘˜,ğ‘Ÿğ‘˜(cid:1)
[ğ‘—]
20XX
ğ‘Ÿ1=1 ğ‘Ÿğ‘=1 ğ‘˜=1
âˆ‘ï¸ğ‘…1 âˆ‘ï¸ğ‘…ğ‘ (cid:16)(cid:204)1
(cid:17)âŠ¤
=ğ‘© Â·Â·Â· ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘Î“ ğ‘—,ğ‘Ÿğ‘— Î“ ğ‘˜,ğ‘Ÿğ‘˜
ğ‘Ÿ1=1 ğ‘Ÿğ‘=1 ğ‘˜=ğ‘
ğ‘˜â‰ ğ‘—
âˆ‘ï¸ğ‘…1 âˆ‘ï¸ğ‘…ğ‘ (cid:16)(cid:204)1
(cid:17)âŠ¤
= Â·Â·Â· ğœ† ğ‘Ÿ1,...,ğ‘Ÿğ‘(ğ‘©Î“ ğ‘—,ğ‘Ÿğ‘—) Î“ ğ‘˜,ğ‘Ÿğ‘˜ .
ğ‘Ÿ1=1 ğ‘Ÿğ‘=1 ğ‘˜=ğ‘
ğ‘˜â‰ ğ‘—
Thus,
ğ’€â€² =
âˆ‘ï¸ğ‘…1 Â·Â·Â·âˆ‘ï¸ğ‘…ğ‘
ğœ†
ğ‘Ÿ1,...,ğ‘Ÿğ‘(cid:16)ğ‘— âƒâˆ’1
Î“
ğ‘˜,ğ‘Ÿğ‘˜(cid:17)
â—¦(ğ‘©Î“ ğ‘—,ğ‘Ÿğ‘—)â—¦
ğ‘Ÿ1=1 ğ‘Ÿğ‘=1 ğ‘˜=1
(cid:16) ğ‘ (cid:17)
âƒ Î“ ğ‘˜,ğ‘Ÿğ‘˜ .
ğ‘˜=ğ‘—+1OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDDâ€“DD,YYYY,Location
Table2:Mean(standarddeviation)time(ms/iteration)spentoneachpartoftheHOOI,HOOI-RE*,HOOI-RE,andHOSVD
algorithmsinbothnumericalstudies,calculatedover100simulations.
Method DRğ‘…=5 ğ‘…=15 ğ‘…=30
ğ‘¨ ğšª ğš² ğ‘¨ ğšª ğš² ğ‘¨ ğšª ğš²
Face
HOOI 0.0(0.0) 18.5(1.3) 3.8(0.6) 0.0(0.0) 23.4(2.2) 5.2(0.8) 0.0(0.0) 33.6(3.4) 8.8(0.9)
HOOI-RE*0.8 21.9(3.6) 14.8(0.8) 4.2(0.1) 22.2(2.9) 14.4(0.4) 4.7(0.1) 23.7(2.3) 19.2(2.2) 9.9(2.0)
0.6 14.0(0.1) 8.1(0.5) 3.5(0.5) 14.8(0.3) 8.7(0.2) 4.8(0.1) 15.4(0.3) 10.3(0.1) 8.7(0.1)
0.4 9.2(0.3) 4.9(0.2) 3.6(0.5) 9.2(0.1) 5.1(0.0) 4.6(0.0) 10.0(0.4) 6.6(0.8) 10.3(2.1)
0.2 5.2(0.2) 2.7(0.1) 3.0(0.0) 5.0(0.1) 2.6(0.2) 4.7(0.5) 6.0(0.1) 4.0(0.0) 11.4(0.3)
HOOI-RE 0.8 23.2(3.4) 14.4(0.6) 8.6(0.2) 23.1(2.8) 14.6(0.2) 8.8(0.1) 24.9(3.1) 17.5(0.3) 10.7(0.1)
0.6 14.9(0.3) 7.9(0.3) 4.8(0.1) 15.1(0.3) 8.2(0.1) 5.2(0.1) 16.2(0.3) 11.1(1.1) 6.7(0.3)
0.4 9.1(0.1) 4.6(0.1) 2.6(0.0) 9.7(0.3) 5.1(0.1) 3.0(0.0) 9.9(0.3) 5.9(0.1) 3.5(0.1)
0.2 5.0(0.1) 2.6(0.2) 1.7(0.2) 5.4(0.1) 2.9(0.1) 1.8(0.0) 6.6(0.6) 4.4(0.1) 2.8(0.2)
HOSVD 0.0(0.0) 445.7(43.4) 3.4(0.4) 0.0(0.0) 442.4(39.5) 5.1(0.7) 0.0(0.0) 440.6(45.2) 8.7(1.6)
Hippocampus
HOOI 1.0 0.0(0.0) 565.0(44.8) 64.6(10.1) 0.0(0.0) 920.2(6.9) 121.3(1.3) 0.0(0.0) 1692.5(17.8) 255.9(4.8)
HOOI-RE*0.8 1676.0(16.9) 764.6(16.1) 57.9(6.3)1724.9(20.5) 802.8(18.3) 125.2(6.7)1727.3(13.5) 823.6(5.6) 242.4(1.5)
0.6 934.9(12.2) 379.2(8.0) 64.0(6.8) 967.6(17.7) 408.3(12.8) 126.6(9.3) 999.5(11.9) 417.4(6.3) 245.7(2.4)
0.4 407.2(6.4) 148.8(4.4) 61.5(3.8) 398.8(5.6) 150.2(5.5) 121.5(5.9) 420.0(5.6) 168.4(11.1) 246.9(2.3)
0.2 74.8(2.1) 22.8(1.2) 70.0(9.4) 59.9(1.1) 20.4(1.1) 122.5(3.0) 77.5(2.5) 29.4(2.6)274.0(14.4)
HOOI-RE 0.8 1581.2(18.1) 739.0(4.3) 268.3(4.5)1665.9(19.0) 786.0(19.8) 306.1(7.7)1838.3(34.9) 865.0(16.9)410.0(11.6)
0.6 887.1(9.6) 358.9(3.4) 136.0(1.9) 918.8(9.5) 402.8(19.1) 153.5(2.2)1052.1(17.4) 433.1(14.1) 216.7(3.4)
0.4 428.0(4.2) 160.7(7.1) 52.8(1.7) 395.2(3.7) 148.0(1.8) 57.5(1.7) 441.5(7.0) 182.1(10.5) 89.1(4.1)
0.2 60.5(2.7) 20.6(1.5) 7.8(0.4) 63.3(1.3) 23.1(1.1) 9.5(0.3) 80.2(4.6) 33.2(2.1) 17.3(0.6)
HOSVD 1.0 0.0(0.0)42242.4(1032.7)108.7(14.2) 0.0(0.0)42131.2(861.3)219.4(21.0) 0.0(0.0)42484.0(1171.8)454.8(10.9)
Algorithmparts:ğ‘¨,ğšª,andğš²denotetheapplicationofrandomembeddings,updatesforthefactormatrices,andupdatesforthecoretensor,respectively.
HOSVD:HOSVDisnotaniterativealgorithmandterminatesafterasingleupdatetoeachofthefactormatricesandthecorematrix.
Omissions:Timespentformingtherandommatricesisnotprovidedforthesakeofspace;meantimefortheHOOI-RE*andHOOI-REalgorithmsacrossallsettingsrangedfrom0.2
to0.4(withstandarddeviationsfrom0.0to0.1)ms/iteration.Timespentpreprocessingisalsonotincluded,butisaccountedforbyothertablesandfiguresinthemaintext.
Specifically,timespentâ€œmixingâ€thedatatookonaverage75.9(standarddeviation8.0)ms/replicationinthefaceanalysisand7868.3(standarddeviation746.7)ms/replicationinthe
hippocampusanalysis.
Tensor parameterization
3D template
Figure6:Visualizationofthe3Dtemplateand150Ã—100Ã—2parameterizationusedinthehippocampusanalysis.Thecolor
gradientsandsolidblacklinesindicatethecorrespondencebetweentherepresentations.