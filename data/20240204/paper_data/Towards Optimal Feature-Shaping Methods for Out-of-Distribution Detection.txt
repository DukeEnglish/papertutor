PublishedasaconferencepaperatICLR2024
TOWARDS OPTIMAL FEATURE-SHAPING METHODS FOR
OUT-OF-DISTRIBUTION DETECTION
QinyuZhao1,MingXu1,KartikGupta2,AkshayAsthana2,LiangZheng1,StephenGould1
1TheAustralianNationalUniversity
2SeeingMachinesLtd
{qinyu.zhao,mingda.xu,liang.zheng,stephen.gould}@anu.edu.au
{kartik.gupta,akshay.asthana}@seeingmachines.com
ABSTRACT
Featureshapingreferstoafamilyofmethodsthatexhibitstate-of-the-artperfor-
mance for out-of-distribution (OOD) detection. These approaches manipulate
thefeaturerepresentation, typicallyfromthepenultimatelayerofapre-trained
deeplearningmodel,soastobetterdifferentiatebetweenin-distribution(ID)and
OODsamples. However,existingfeature-shapingmethodsusuallyemployrules
manuallydesignedforspecificmodelarchitecturesandOODdatasets,whichcon-
sequentlylimittheirgeneralizationability. Toaddressthisgap,wefirstformulate
anabstractoptimizationframeworkforstudyingfeature-shapingmethods. Wethen
proposeaconcretereductionoftheframeworkwithasimplepiecewiseconstant
shapingfunctionandshowthatexistingfeature-shapingmethodsapproximatethe
optimalsolutiontotheconcreteoptimizationproblem.Further,assumingthatOOD
dataisinaccessible,weproposeaformulationthatyieldsaclosed-formsolution
forthepiecewiseconstantshapingfunction,utilizingsolelytheIDdata. Through
extensiveexperiments,weshowthatthefeature-shapingfunctionoptimizedbyour
methodimprovesthegeneralizationabilityofOODdetectionacrossalargevariety
ofdatasetsandmodelarchitectures. 1
1 INTRODUCTION
Out-of-distribution(OOD)detectionaimstoidentifytestsamplesthatfalloutsidetheinherenttraining
labelspace,givenadeeplearningmodelpre-trainedonanin-distribution(ID)trainingset. Todetect
OODsamples,OODscores,suchasmaximumsoftmaxprobability(MSP)(Hendrycks&Gimpel,
2016)andenergyscore(Liuetal.,2020)arecomputedusingthelogitsestimatedbythemodel,where
alowerscoreindicatesahigherprobabilitythatthesampleisOOD.
Featureshaping(Sunetal.,2021;Djurisicetal.,2022;Xu&Lian,2023;Songetal.,2022)refers
toafamilyofmethodsthatmanipulatetheunderlyingfeaturerepresentations,typicallyfromthe
penultimatelayerofapre-trainedmodel, suchthatOODscorescanmoreeffectivelydistinguish
betweenIDandOODdata.RepresentativemethodslikeReAct(Sunetal.,2021)andASH-P(Djurisic
etal.,2022)employelement-wisefeatureclippingandpercentile-basedfeaturepruning,respectively.
Despitetheirnotableachievements,existingfeature-shapingmethodsusuallyrelyonrulesmanu-
ally designed for specific OOD datasets and model architectures, which consequently limit their
generalizationability. OurobservationsindicatethatvariationsinOODdatasetsandmodelarchi-
tecturescancausesignificantperformancedegradationinthesemethods. Forinstance,whenusing
transformer-basedmodels,someapproaches,despiteachievingstate-of-the-artresultsonestablished
benchmarks,performnobetterthanrandomguessing. Theseobservationsemphasizethenecessity
foracomprehensiveunderstandingandcomparisonofthecurrentfeature-shapingmethodsandthe
developmentofamoregeneralizableapproach.
Tothisend,wefirstformulateanabstractoptimizationproblemthatencapsulatesallfeature-shaping
OOD detection methods. In a nutshell, our formulation entails finding the shaping function that
maximizessomeperformanceobjectiveforOODdetection.
1Ourcodeisavailableathttps://github.com/Qinyu-Allen-Zhao/OptFSOOD.
1
4202
beF
1
]VC.sc[
1v56800.2042:viXraPublishedasaconferencepaperatICLR2024
75 Ours
80 80 70 80 MSP ODINB RF eA Ac ct t
Energy
60 60 65 60 DICE
60
40 40 ASH-B 55 40
20 20
50
ASH-P VRA-P
0 0 45 20 ASH-S
ReAct BFAc At SH- AP SH- AB SH- VS RA-P Ours ReAct BFAc At SH- AP SH- AB SH- VS RA-P Ours ReAct BFAc At SH- AP SH- AB SH- VS RA-P Ours A8 U0
C (2
Conv8 N5
ets)
(a) (b) (c) (d)
Figure1: Comparingourmethodwithexistingfeature-shapingmethods. Thedashedlinesdenotethe
performanceofourmethodforcomparison. (a)ImageNet(ID)vs. iNaturalist(OOD)withViT-B-16;
(b) ImageNet (ID) vs. iNaturalist (OOD) with MLP-Mixer-B; (c) CIFAR100 (ID) vs. CIFAR10
(OOD)withMLP-Mixer-Nano;(d)AverageperformanceofdifferentmethodsacrosseightOOD
datasetswithtwoConvNetsandwithfourtransformer-basedmodels.
Tomaketheoptimizationproblemconcreteandtractable,weproposeapiecewiseconstantshaping
functionandformulateanobjectivefunctionthatmaximizestheexpecteddifferencebetweenthe
maximumlogitvaluesforIDandOODdata,assumingtheavailabilityofOODtrainingsamples. An
importantbenefitofthisformulationisthatithighlightstheoperationalmechanismsunderpinning
previousfeature-shapingmethods. Namely,theshapingfunctionsusedbyexistingfeature-shaping
methodsincludingReAct(Sunetal.,2021),BFAct(Kong&Li,2023),andVRA-P(Xu&Lian,
2023),approximatetheoptimalshapingfunctionfoundbyourapproach.
Usingourframework,wefurtherproposeafeature-shapingmethodthatcanbeoptimizedwithout
accesstoOODdata. Specifically, throughtheanalysisoftheeffectofOODdataontheoptimal
solution compared to ID data, we argue that it is feasible to remove the OOD-related term in
theobjectivefunction,therebyderivingaclosed-formsolutionbasedonIDdataonly. Empirical
observationsinexperimentssupportourclaim.
Ourexperimentscompareourproposedapproachtoexistingfeature-shapingmethods,utilizingamore
comprehensiveexperimentalsetupthanpreviousworks. WeuseCIFAR10,CIFAR100(Krizhevsky
etal.,2009),andImageNet-1k(Russakovskyetal.,2015)asIDdatasets. ForeachIDdataset,weuse
eightOODdatasetsforevaluation. Furthermore,weevaluateallmethodsusingawidevarietyof
models,includingconvolutionalnetworks(ConvNets),transformersandfullymulti-layerperceptron
(MLP)models,thelattertwoofwhichhavenotbeenextensivelystudiedinpriorworks. Asshown
inFig.1,comparedwithotherfeature-shapingmethods,oursdemonstratesmoregeneralizedOOD
detectionperformanceacrossabroadspectrumofdatasetsandmodelarchitectures.
Insummary,thispapercoversthreemainpoints. First,wedevelopageneraloptimizationformulation
for feature-shaping methods and show that a concrete instance of this problem provides insight
into the mechanisms of previous methods. Second, we propose a novel approach for training a
generalizablefeature-shapingmethodforOODdetectionwithoutaccesstoOODdata. Last, our
approachachievesmorereliableOODdetectionperformanceacrossvariousdatasetsandmodels.
2 A RECAP OF FEATURE SHAPING FOR OOD DETECTION
Inthissection, wedefinetheproblemofOODdetectionforimageclassification, withparticular
emphasis on feature-shaping approaches, and subsequently, introduce our general optimization
formulationoffeatureshapingforOODdetection.
2.1 OODDETECTIONFORIMAGECLASSIFICATION
AssumethatwehaveanIDdataset,definedasajointdistributionDID overinputobservationsX
X,Y
anddiscretelabelsY foragivenmodelM. Furthermore,assumewehaveanOODdatasetDOOD.
X,Yâ€²
Previousworks(Sunetal.,2021;Sun&Li,2022;Yangetal.,2021)assumeYâˆ©Yâ€² =âˆ…. Inherentin
thisassumptionisthatthemarginaldistributionsoverobservationsX forIDandOODdatasetsdo
notcoincide. Thatis,DID Ì¸=DOOD.
X X
2
CUA
)sremrofsnarT
4(
CUAPublishedasaconferencepaperatICLR2024
Themodel,usuallyaneuralnetwork,mapsobservationstologits,M:X â†’RC,whereC =|Y|
denotesthenumberofclasses. ItistrainedonadatasetS ={(x(i),y(i))}N ,comprisedofN
train i=1
i.i.d.samplesdrawnfromDID . Furthermore,fortypicalarchitecturesforimageclassification,we
X,Y
candecomposethemodelM=W â—¦Bintoabackbonemodel,B :X â†’RM,andlinearclassifier
W :RM â†’RC. Bextractspenultimatelayerfeaturerepresentationz âˆˆRM ofaninputobservation
x, and W is a linear layer that maps z to logits â„“ âˆˆ RC. Different from conventional machine
learningpractice,thetestsetS containsinputssampledfromthejointdistributionDIDâˆªDOOD,
test X X
whereDIDandDOODdenotethemarginaldistributionsofDID andDOOD,respectively.
X X X,Y X,Yâ€²
ThegoalofOODdetectionistodesignascorefunctionstoindicatewhetherasamplexisfrom
DIDorDOOD. MSP,maximumlogitscore(MLS),andenergyscoreserveasexamplesofscoresfor
X X
OODdetection. Theyaredefinedbasedonlogitsâ„“as:
C
s (â„“)= max
exp(â„“ i)
, s (â„“)= max â„“ , s
(â„“)=log(cid:88)
exp(â„“ ), (1)
MSP
i=1,...,C
(cid:80)C
exp(â„“ )
MLS
i=1,...,C
i Energy j
j=1 j j=1
whereâ„“ isthelogitforthei-thclass. Forthescoresdiscussedabove,alowervalueindicatesahigher
i
likelihoodofthesamplebeingOOD.Inpracticalapplications,athreshold,denotedbyt,isutilizedto
separateIDsamplesfromOODones. Ifthescoreofasamplesurpassesthisthreshold,thesample
willbeclassifiedasID,andOODotherwise.
2.2 FEATURE-SHAPING
A feature-shaping method uses a shaping function f : RM â†’ RM to manipulate features, i.e.,
zÂ¯ =f(z). ThegoalofdevelopingafeatureshapingmethodforOODdetectioncanbeformulatedas
solvingthefollowingoptimizationproblem,
(cid:104) (cid:110) (cid:111)(cid:105)
maximize(overf) E P
s(Â¯â„“ID ),s(Â¯â„“OOD
) ,
xIDâˆ¼DID,xOODâˆ¼DOOD
X X
subjectto Â¯â„“ID =W(zÂ¯ID), zÂ¯ID =f(B(xID)), (2)
Â¯â„“OOD =W(zÂ¯OOD), zÂ¯OOD =f(B(xOOD)),
whereP isagivenperformancemeasure. Forexample,acommonmeasureistheareaunderthe
receiver operating characteristic curve (AUC), defined as P (s ,s ) = 1{s >s }. Existing
AUC 1 2 1 2
shapingfunctionsf canbebroadlycategorizedintotwotypes: (i)element-wise,suchasReAct(Sun
etal.,2021)andVRA-P(Xu&Lian,2023),whichappliesaglobalelement-wisemappingtoeach
individualfeature,i.e.,zÂ¯ =f(z ),or(ii)whole-vectormappingssuchasASH(Djurisicetal.,2022),
i i
whichmanipulatethefullfeaturevectorz.
3 OPTIMAL FEATURE SHAPING
In this section, we propose a specific instance of the general formulation described in Section 2,
followingpreviouselement-wisefeature-shapingmethods(Sunetal.,2021;Xu&Lian,2023;Kong
&Li,2023). Tooptimizetheshapingfunction,weinitiallyexaminewhichfeaturevaluerangesare
effectiveforOODdetection. Asignificantchallengearisesfromthecontinuousnatureoffeature
values. Toaddressthis,wepartitionthedomainoffeaturesintoadisjointsetofintervals,which
allowsustoapproximatetheoptimalshapingfunctionwithapiece-wiseconstantfunction. With
thisapproximation,wecanformulateanoptimizationproblemtomaximizetheexpecteddifference
betweenthemaximumlogitvaluesofIDandOODdata. Subsequently,weshowthattheoptimized
shapingfunctionusingOODsamplesbearsastrikingresemblancetopriormethods,sheddinglight
onthemechanicsofthesemethods. Finally,weproposeaformulationthatdoesnotrequireanyOOD
datatooptimizeandadmitsasimple,closed-formsolution.
3.1 VALUE-ANDINTERVAL-SPECIFICFEATUREIMPACT
Givenasamplex,themaximumlogitâ„“max(disregardingthebiasterm)isthedotproductbetween
thecorrespondingweightvectorwmaxanditsfeaturerepresentationz,specifically,
M
(cid:88)
â„“max(z)=wmaxÂ·z = wmaxz . (3)
i i
i=1
3PublishedasaconferencepaperatICLR2024
Assuming an element-wise feature-shaping function f : R â†’ R, define Î¸(z ) â‰œ f(zi). Then
i zi
assumingtheweightswmaxdonotchange2,themaximumlogitaftershapingis:
M M
â„“Â¯max(zÂ¯)=(cid:88)
wmaxf(z
)=(cid:88)
Î¸(z )wmaxz . (4)
i i i i i
i=1 i=1
To design Î¸(z) and understand which feature value ranges are effective for OOD detection, we
compute the contribution of features with specific values to the maximum logit. Concretely, we
definethevalue-specificfeatureimpact(VSFI):V(z)=(cid:80)M 1{z =z}wmaxz . However,zisa
i=1 i i i
continuousvariable,andreal-worlddataonlyprovideacountablenumberoffeatures,whichcan
givesoverlysparsedistributionsofVSFIs. Toaddressthis,wepartitionthedomainofindividual
featuresintoadisjointsetofintervalsAthatcoverz . Formally,theinterval-specificfeatureimpact
i
(ISFI)onaninterval[a,b)âˆˆAisgivenby:
M
(cid:88) (cid:88)
I (z)= 1{aâ‰¤z <b}wmaxz = wmaxz . (5)
[a,b) i i i i i
i=1 i:aâ‰¤zi<b
Then,forasinglesample,themaximumlogitisthesummationoverallISFIs,givenby:
(cid:88)
â„“max(z)= I (z). (6)
[a,b)
[a,b)âˆˆA
Inourexperiments,wedividefeaturevaluesintoequal-widthintervals.Giventhatfeaturedistributions
typicallyexhibitlong-tailedcharacteristics(Sunetal.,2021),weselectthe0.1and99.9percentiles
ofallfeaturesinatrainingsetasthelowerÎ±andupperÎ² limitsoffeaturevalues,respectively. We
thenpartitiontherange[Î±,Î²)intoK =100equal-widthintervals. Thisresultsinasetofintervals
A={[a ,b )}K ,wherea =Î±+(kâˆ’1)Î´,b =Î±+kÎ´,andÎ´ = Î²âˆ’Î± fork =1,...,K. Then,
k k k=1 k k K
themaximumlogitcanbeexpressedas:
K
(cid:88) (cid:88)
â„“max(z)= I (z)= I (z), (7)
[a,b) [ak,bk)
[a,b)âˆˆA k=1
andforeachdatasample,wecangenerateaK-dimISFIvectordenotedas:
I(z)=(cid:0) I (z),...,I (z)(cid:1) âˆˆRK. (8)
[a1,b1) [aK,bK)
3.2 OPTIMIZINGTHERESHAPINGFUNCTION
WenowdescribehowtooptimizeÎ¸(z),assumingÎ¸(z)ispiecewiseconstantovertheintervalsA. For
aninterval[a ,b ),weestimateascalarÎ¸Ë† asaconstantapproximationtoÎ¸(z)withintheinterval.
k k k
Inthelimitastheintervalwidthsshrinkstozero,werecoverthecontinuous,element-wiseshaping
function. Wecanapproximatethemaximumlogitas,
K
â„“Â¯max(zÂ¯)=(cid:88) Î¸Ë† I (z). (9)
k [ak,bk)
k=1
Whilepreviousworks(Sunetal.,2021;Xu&Lian,2023)evaluatetheirmethodsbycomputingthe
expecteddifferenceinmaximumlogitsbetweenIDandOODdata,optimizingthiscriteriasolely
doesnotimproveOODdetectionperformance. Toseethis,observethatsimplyuniformlyscaling
featuresbyalargefactorincreasestheexpecteddifferenceinmaximumlogitswithoutenhancing
thedifferentiationbetweenIDandOODsamples. Thismotivatesintroducingregularizationonthe
shapingfunctionandleadsustothefollowingproblem3:
(cid:104) (cid:105)
maximize(overÎ¸) E Î¸TI(zID)âˆ’Î¸TI(zOOD)
xIDâˆ¼DâˆšXID,xOODâˆ¼D XOOD (10)
subjectto âˆ¥Î¸âˆ¥= K, zID =B(xID), zOOD =B(xOOD),
2WevalidatethisassumptioninSectionB.2.
âˆš
3Weconstrainâˆ¥Î¸âˆ¥tobeequaltoâˆ¥1âˆ¥= Kinsteadofothervalues.ReasonscanbefoundinSectionA.2.
4PublishedasaconferencepaperatICLR2024
1.0
0.5
0.0
0.5 Ours (w/ OOD) Ours (w/ OOD) Ours (w/ OOD) Ours (w/ OOD)
ReAct BFAct VRA-P Ours (w/o OOD)
(a) (b) (c) (d)
1.5
1.0
0.5
0.0
Ours (w/ OOD) Ours (w/ OOD) Ours (w/ OOD) Ours (w/ OOD)
0.5
ASH-P ASH-B ASH-S ASH-Rand
0 2 4 0 2 4 0 2 4 0 2 4
Feature value z Feature value z Feature value z Feature value z
(e) (f) (g) (h)
Figure2: Visualizationofshapingfunctions. Thebluelines(oursw/OOD)derivefromEq.10,while
thegreenline(oursw/oOOD)fromEq.14. Redlinesrepresentdifferentexistingmethods,while
shadedregionsindicateestimatedstandarddeviations. Î¸hasbeenrescaledforthebestvisualization.
whereduetothelinearityofexpectation,theobjectivefunctioncanbeexpressedas:
Î¸T(cid:16)
E
(cid:2) I(zID)(cid:3)
âˆ’E
(cid:2) I(zOOD)(cid:3)(cid:17)
. (11)
xIDâˆ¼DID xOODâˆ¼DOOD
X X
WeextractISFIvectorsfortheImageNet-1ktrainingset(ID)(Russakovskyetal.,2015)andfor
the iNaturalist dataset (OOD) (Van Horn et al., 2018), using a ResNet50 model pre-trained on
ImageNet-1ktrainingset. TheestimatedÎ¸Ë† isplottedinFig.2.
k
Forcompleteness,wedescribeexistingfeature-shapingmethods(Sunetal.,2021;Kong&Li,2023;
Xu&Lian,2023)usingourreparameterizationofthefeatureshapingproblemwithÎ¸(z),
ï£±
0 z <low
min(z,t) 1 ï£²
Î¸ (z)= , Î¸ (z)= , Î¸ (z)= 1 lowâ‰¤z <high
ReAct z BFAct (cid:113) 1+(z)2N VRA-P ï£³high highâ‰¤z,
t z
(12)
wheret,N,low,andhigharehyperparameters. WethenplottheresultantÎ¸(z)inFig.2(a-c). We
showthatthesepriorworksareempiricallyapproximatingouroptimalpiecewiseconstantshaping
function. A difference between our shaping function and VRA-P, is that VRA-P sets low-value
featurestozerowhileoursflipsthesignofthesefeaturestobetterutilizethemforOODdetection.
Then,weextendouranalysistootherfeature-shapingmethods. ASH-P,-B,-S,and-Randdenote
foursimplisticyeteffectivemethodsintroducedbyDjurisicetal.(2022). IncontrasttoReAct,which
employsaglobalthresholdtoclipfeatures,ASHmethodsincorporatetheentirefeaturevectorand
utilizealocalthresholdtoprocessthefeaturerepresentationofeachinstance. Theirshapingfunctions
arenotreducibletoelement-wisefunctions.
Weinsteadapproximatetheaverageshapingeffectwithineachfeaturevalueinterval. Specifically,
weutilizeImageNet-1kvalidationset(ID)andiNaturalist(OOD)withResNet50. Foraninterval
[a ,b ),weretrieveallfeaturesfromIDandOODdatasetsthatfallwithinthisrange,alongwith
k k
theirreshapedcounterparts. Followingthis,theratioofreshapedandoriginalvaluesarecomputedas
Î¸ foraspecificdatainstance. Subsequently,wecalculatethemeanvalueandstandarddeviationof
k
theseÎ¸ valuesoverallsamplesoneachinterval. TheresultsaredepictedinFig.2(e-h).
k
3.3 OPTIMIZINGTHESHAPINGFUNCTIONWITHOUTOODSAMPLES
5
)z(
)z(PublishedasaconferencepaperatICLR2024
Normally, we have access to the ID training set, en-
ðœ½ optimized ðœ½ optimized abling us to estimate E
xIDâˆ¼DID(cid:2) I(zID)(cid:3)
. However, a
w/ OOD data w/o OOD data significant challenge arises wX ith the unknown term
E
(cid:2) I(zOOD)(cid:3)
, which renders the optimization
xOODâˆ¼DOOD
X
ID problemdefinedinEq.10intractable.
TomaketheproblemtractablewithoutaccesstoOODdata,
Hard OOD weomittheOOD-relatedtermE
xOODâˆ¼DOOD(cid:2) I(zOOD)(cid:3)
by
X
thefollowingargument,whichaswewillseeisalsosup-
portedempiricallyinSection4.2.
A diagram to explain our intuition is shown in Fig. 3.
Easy OOD
Specifically, in scenarios where the OOD distribution
closelyresemblestheID(hardOOD),OODsampleslikely
Figure3: Diagramtoshowtheintuition havesimilarfeaturerepresentationstoIDsamples,imply-
inderivingEq.13. ingbothexpectationssharesimilardirections. Conversely,
incaseswhereOODsamplessignificantlydivergefrom
IDsamples(easyOOD),themaximumlogitsofOODsamplestendtobelower,therebyreducingthe
magnitudeofE
(cid:2) I(zOOD)(cid:3)
comparedtoID.
xOODâˆ¼DOOD
X
Thus,thecomponentofE
(cid:2) I(zOOD)(cid:3)
orthogonaltoIDexpectationissmallandwesuggest
xOODâˆ¼DOOD
omittingE
(cid:2) I(zOOD)(cid:3) ,X
resultinginthefollowingformulation:
xOODâˆ¼DOOD
X
maximize(overÎ¸) Î¸TE [I(z)]
xâˆ¼âˆšD XID (13)
subjectto âˆ¥Î¸âˆ¥= K, z =B(x).
Theoptimalsolutiontothisnewproblemcanbeexpressedinclosed-formas:
âˆš
K
Î¸â‹† = E [I(z)]. (14)
âˆ¥E xâˆ¼DID[I(z)]âˆ¥
2
xâˆ¼D XID
X
WeplotÎ¸â‹†calculatedbyEq.14inFig.2(d),notingthatitisincloseproximitytotheoptimalsolution
inthecasewhereOODsamplesareavailable. Inexperiments,wewillalsoshowthatourmethod
withonlyIDdataperformsalmostonparwithusingthetrueI(zOOD)calculatedfromOODdata.
At inference time, for each sample, we rescale its features within each interval [a ,b ) by the
k k
correspondingentryinÎ¸â‹†,andcalculateanOODscorebasedonthereshapedfeaturerepresentation.
4 EXPERIMENTS
Datasets. We evaluate our proposed method using a large scale benchmark where ImageNet-
1k(Russakovskyetal.,2015)istheIDdataset,aswellastwomoderatescalebenchmarkswhere
CIFAR10andCIFAR100(Krizhevskyetal.,2009)areusedastheIDdatasets. Weevaluateeight
OODdatasetsfortheImageNetbenchmark,comprisedofSpecies(Hendrycksetal.,2019b),iNatural-
ist(VanHornetal.,2018),SUN(Xiaoetal.,2010),Places(Zhouetal.,2017),OpenImage-O(Wang
etal.,2022),ImageNet-O(Hendrycksetal.,2021),Texture(Cimpoietal.,2014),andMNIST(Deng,
2012). When CIFAR10/100 is the ID dataset, eight OOD datasets are exploited, comprised of
TinyImageNet(Torralbaetal.,2008),SVHN(Netzeretal.,2011),Texture(Cimpoietal.,2014),
Places365(Zhouetal.,2017),LSUN-Cropped(Yuetal.,2015),LSUN-Resized(Yuetal.,2015),
iSUN(Xuetal.,2015),andfinallyCIFAR100/10.
Modelarchitectures. Weusepre-trainedmodelsprovidedbyPyTorchandpriorworksinour
experiments. ForImageNet-1k,weuseapre-trainedResNet50(Heetal.,2016)providedbyPyTorch
andaMobileNet-v2(Sandleretal.,2018)providedbyDjurisicetal.(2022). Additionally,weinclude
modelsbasedonthetransformerandfully-MLParchitectures,whichhavebeenlargelyoverlookedin
previousstudies. Specifically,weincludeViT-B-16,ViT-L-16(Dosovitskiyetal.,2020),SWIN-S,
SWIN-B (Liu et al., 2021), MLP-Mixer-B, and MLP-Mixer-L (Tolstikhin et al., 2021). For the
CIFAR10andCIFAR100benchmarks,weuseaDenseNet101(Huangetal.,2017)modelprovided
bySun&Li(2022),aViT-B-16finetunedonCIFAR10/100consistentwithFortetal.(2021),anda
MLP-Mixer-NanomodeltrainedonCIFAR10/100fromscratch.
6PublishedasaconferencepaperatICLR2024
Table1: OODdetectionresultsonImageNet-1k. â†‘indicateslargervaluesarebetterandâ†“indicates
smallervaluesarebetter. AllvaluesarepercentagesaveragedoverdifferentOODdatasets. Bold
numbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthirdbestresults.
ResNet50 MobileNetV2 ViT-B-16 ViT-L-16 SWIN-S SWIN-B MLP-B MLP-L Average
Method FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 69.30 76.26 72.58 77.41 69.84 77.40 70.59 78.40 65.12 80.89 69.45 77.26 70.66 80.31 71.63 79.93 69.90 78.48
ODINa 61.56 80.92 62.91 82.64 69.25 72.60 70.35 74.51 62.35 78.67 70.90 68.36 65.00 82.99 67.90 81.86 66.28 77.82
Energy 60.97 81.01 61.40 82.83 73.96 67.65 74.89 70.11 66.56 75.95 79.53 60.14 87.86 78.42 84.60 79.01 73.72 74.39
DICE 45.32 83.64 49.33 84.63 89.68 71.32 72.38 67.08 78.84 49.06 77.89 50.10 57.12 80.60 61.98 80.75 66.57 70.90
ReAct 42.29 86.54 54.19 85.37 73.82 76.86 76.16 81.07 58.07 85.10 70.23 80.10 90.13 77.18 89.03 78.32 69.24 81.32
BFAct 43.87 86.01 52.87 85.78 77.64 80.16 84.02 81.12 64.76 85.57 68.40 84.36 96.04 67.87 96.00 70.59 72.95 80.18
ASH-P 55.30 83.00 59.41 83.84 99.36 21.17 99.18 20.27 99.09 21.78 98.83 22.11 98.19 35.85 98.91 29.69 88.53 39.71
ASH-B 35.97 88.62 43.59 88.28 94.87 46.68 93.72 38.95 96.48 36.54 92.40 49.85 99.51 21.73 64.91 83.15 77.68 56.73
ASH-S 34.70 90.25 43.84 88.24 99.48 18.52 99.42 18.61 99.20 18.26 99.06 19.27 97.61 33.91 99.28 19.29 84.07 38.29
VRA-P 37.97 88.58 49.98 86.83 98.39 35.66 99.58 16.70 99.27 20.34 99.46 17.64 99.38 18.73 99.05 21.23 85.39 38.21
Ours(V) 41.84 88.11 53.31 86.36 69.33 82.59 72.17 83.23 66.89 84.53 65.96 83.71 76.59 78.46 78.13 78.85 65.53 83.23
Ours(E) 39.75 88.56 51.77 86.62 69.52 82.66 78.57 82.94 66.28 84.99 66.20 84.21 83.54 75.94 85.50 78.11 67.64 83.00
aChoiceofhyperparametersonImagenetmakesODINyieldidenticalresultstoMLS.
Table2: OODdetectionresultsonCIFAR10/100. â†‘indicateslargervaluesarebetterandâ†“indicates
smallervaluesarebetter. AllvaluesarepercentagesaveragedoverdifferentOODdatasets. Bold
numbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthirdbestresults.
CIFAR10 CIFAR100
DenseNet101 ViT-B-16 MLP-N Average DenseNet101 ViT-B-16 MLP-N Average
Method
FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 52.66 91.42 25.47 95.61 67.01 82.86 48.38 89.96 80.40 74.75 61.24 85.24 83.97 73.07 75.20 77.69
ODIN 32.84 91.94 24.93 92.84 69.20 69.53 42.32 84.77 62.03 82.57 56.91 80.86 78.71 66.47 65.88 76.63
MLS 31.93 93.51 12.79 97.16 64.50 82.97 36.41 91.21 70.71 80.18 52.31 88.63 82.41 74.52 68.48 81.11
Energy 31.72 93.51 12.40 97.22 63.95 82.84 36.02 91.19 70.80 80.22 51.93 88.82 79.90 75.30 67.54 81.45
DICE 29.67 93.27 26.84 92.41 96.64 52.17 51.05 79.28 59.56 82.26 95.18 42.05 95.78 44.48 83.51 56.26
ReAct 82.00 76.46 12.33 97.29 64.34 81.85 52.89 85.20 77.00 78.30 52.25 88.84 79.99 75.87 69.75 81.00
BFAct 84.40 74.39 12.34 97.21 78.02 72.68 58.25 81.43 80.27 73.36 51.23 89.04 80.05 76.58 70.52 79.66
ASH-P 29.39 93.98 20.20 95.31 84.39 66.93 44.66 85.41 68.21 81.11 53.69 87.66 86.73 65.27 69.54 78.01
ASH-B 28.21 94.27 82.10 69.54 93.93 53.00 68.08 72.27 57.45 83.80 91.74 56.39 93.63 57.20 80.94 65.80
ASH-S 23.93 94.41 39.93 91.06 82.57 68.02 48.81 84.50 52.41 84.65 60.44 84.25 89.39 59.63 67.41 76.18
VRA-P 38.41 92.77 13.73 96.86 100.00 65.95 50.71 85.19 67.75 82.72 52.21 88.49 87.19 66.03 69.05 79.08
Ours(V) 30.12 93.97 13.13 97.07 77.25 73.86 40.17 88.30 69.01 81.42 52.76 88.60 83.83 73.65 68.53 81.22
Ours(E) 28.90 94.12 12.79 97.14 83.87 71.83 41.85 87.70 65.2 82.39 51.67 88.78 81.33 74.67 66.07 81.95
Evaluationmetrics. Weutilizetwostandardevaluationmetrics,followingpreviousworks(Liang
etal.,2017;Sunetal.,2021;Sun&Li,2022): thefalsepositiverate(FPR95)whenthetruepositive
rateoftheIDsamplesis95%,andAUC.
Implementationdetails. Weevaluatetwovariantsofourmethod,thevanillamethoddescribedin
Eq.14denotedâ€œOurs(V)â€,basedonthemaximumlogitscore,andavariantbasedonenergyscores,
denotedâ€œOurs(E)â€.Forcomparison,weimplementfivebaselinemethods(toprowsinresultstables),
includingMSP(Hendrycks&Gimpel,2016),ODIN(Liangetal.,2017),MLS(Hendrycksetal.,
2019a), Energy(Liuetal.,2020), andDICE(Sun&Li,2022), andsixfeature-shapingmethods
(remainingrowsinresultstables),includingReAct(Sunetal.,2021),BFAct(Kong&Li,2023),
ASH-P, ASH-B, ASH-S (Djurisic et al., 2022), and VRA-P (Xu & Lian, 2023). All comparison
feature-shapingmethodsareevaluatedusingtheenergyscore. Hyperparametersaresetconsistent
withtheoriginalpapersandconcretehyperparametersettingscanbefoundinSectionA.1ofthe
Appendix. AllexperimentsarerunonasingleNVIDIAGeForceRTX4090GPU.
4.1 OODDETECTIONBENCHMARKRESULTS
ExperimentalresultsontheImageNet-1kbenchmarkareshowninTable1. Moredetailedresults
canbefoundinSectionsEandFoftheAppendix. Weobservethatthecomparisonmethodsfor
7PublishedasaconferencepaperatICLR2024
Base + Ours Base + Ours FPR95 AUC
80
69 90 88 89 87 8788 60 88
60 61 62 61 84 86
81 81
80 80 50 84
40 41 40 45 42 4240 76 82
70 40
M S P M L S E nergy DIC E R e A ct M S P M L S E nergy DIC E R e A ct 0 N1 umb3 er of1 0 inte3 r0 val1 s 0 K0 300
(a) (b) (c)
Figure4: Compatibilityandsensitivityanalysis. (a-b)OurmethodcanimproveotherOODscores
and methods. â€œBaseâ€ denotes using the original OOD score or method, while â€œ+Ourâ€ indicates
combiningthescoreormethodwithourfeature-shapingfunction. (c)Ourmethodâ€™sperformance
withdifferenthyperparametersettings,i.e.,numbersofintervalsK.
featureshapingperformextremelywellonthepretrainedResNet-50model,withsomeASHvariants
outperformingelement-wiseshapingmethodslikeReAct(Sunetal.,2021),andyieldingsimilar
performancetoourproposedmethods. Thisisnotsurprising,giventhatResNet-50wasthemodel
architectureusedintheoriginalpapers. AsweshowedinFig.2,theshapingfunctionsproposedby
thesemethodscloselyalignwiththeoptimalshapingfunctionlearnedgivenactualOODsamples.
However,weobservethatthesemethodsdonotgeneralizewelltoarchitecturesoutsideofResNet-50.
Weobservethatforfeature-shapingmethodsmoregenerally,strongerconvolutionalnetworkarchi-
tectureperformanceisnegativelycorrelatedtoperformanceonothernetworkarchitectures. However,
ourproposedmethodsyieldconsistentlystrongperformanceacrossallnetworkarchitectures,despite
onlyhavingaccesstoIDdataforparametertuning.
ResultsfortheCIFAR10andCIFAR100benchmarksareshowninTable2. Animportantobservation
regardingtheseresultsisthatfeature-shapingmethodsasawholeseemtounderperformthebaseline
methods. Thisindicatesthatfeatureshapingappearstobeineffectiveonsmaller-scaledatasetssuch
asCIFAR.Whileourmethodsdonotoutperformthebaselines,wearecompetitiveanddooutperform
allofthecomparisonfeatureshapingmethods,whichisconsistentwiththeImageNet-1kbenchmark.
4.2 FURTHERANALYSIS
WeconductadditionalexperimentsusingtheImageNet-1kbenchmarkandemployingResNet50.
ExtensiontootherOODscores. WhileourproposedmethodinEq.14isdesignedassuming
MLSastheOODscore,wealsoextendourmethodtouseotherOODscoressuchasenergyand
MSP.Furthermore,wecombineourmethodwithotherOODdetectionmethodsincludingDICEand
ReAct. ResultsareaveragedacrosstheeightOODdatasets. AsshowninFig.4(a-b),ourmethod
yieldsconsistentimprovementacrosstheseOODdetectionscoresandmethods.
Differentnumbersofintervals. Weevaluateourmethodswithdifferentnumberofintervals,i.e.,
varyingK. AsshowninFig.4(c),mostoftheimprovementinperformancecomesfromK â‰¤10.
Thisindicatesthattheoptimalfeatureshapingfunctionhasarelativelysimpleform.Thisisconsistent
withpriormethods(Sunetal.,2021;Xu&Lian,2023;Kong&Li,2023),whichassume2â€“3intervals.
Inaddition,therelativelylargeperformancegapoverthebaselinemotivatesfeatureshapingmore
generally. Interestingly,weobserveanextremelylargejumpbetweenK =0(â€œEnergyâ€baseline)
withK =1,whicheffectivelyprunesthetopandbottom0.1%offeaturevalues(usingthethresholds
computedbasedontheIDtrainingdata).
Empirical validation of Problem 13. In deriving Eq. 13, we remove the OOD-related term,
assumingOODdataisinaccessible. WenowempiricallyjustifyourintuitionshowninFig.3. We
estimatea = E
(cid:2) I(zID)(cid:3)
withImageNet-1kandb = E
(cid:2) I(zOOD)(cid:3)
withdifferent
xIDâˆ¼DID xOODâˆ¼DOOD
X X
OODdatasets. Wethencomputetheanglesandthenormratios, âˆ¥bâˆ¥,ofthetwoexpectations.
âˆ¥aâˆ¥
8
59RPF CUA 59RPF CUAPublishedasaconferencepaperatICLR2024
AsshowninTable3,challengingOODdatasets Table3:EmpiricalvalidationofProblem13.aand
like Species and ImageNet-O exhibit smaller bdenotetheexpectationsofI(z)forIDandOOD,
anglesbetweenthetwoexpectations,whilesim- respectively."AU(Real)"referstotheperformance
plerOODdatasetslikeiNaturalistandMNIST ofsolvingEq.10withOODdata.
demonstratesmallernormratios. Thus,Eq.13
providesareasonableapproximationtoÎ¸solved
OODdataset ||b||/||a||cos(a,b)AU(Real)AU(Ours)
byEq.10withaccesstoOODdata.
Species 0.72 0.99 78.69 78.59
ComparetheformulationutilizingOODdata iNaturalist 0.48 0.97 96.53 96.63
SUN 0.55 0.98 92.90 92.84
(Eq.10)andourmethod(Eq.14). Weopti- Places 0.59 0.99 90.06 90.15
mizeÎ¸bysolvingEq.10witheachOODdataset OpenImage-O 0.56 0.98 92.31 92.54
ImageNet-O 0.91 0.99 70.19 59.33
andgettheOODdetectionperformanceonthat
Texture 0.51 0.96 95.82 95.48
dataset. ThelasttwocolumnsofTable3com- MNIST 0.42 0.84 99.32 99.34
paretheperformanceofÎ¸ solvedfromEq.10
andthatfromEq.14(ours). Notably,ourmethodwithonlyIDdatayieldsperformancecomparable
totheformulationwithaccesstorealOODdata.
5 DISCUSSION
Herewefirstdiscussacorelimitationtoourframeworkthatourproposedmethodassumeselement-
wiseshapingfunctions,whichfailstocapturemethodslikeASH(Dosovitskiyetal.,2020). Wethen
connectourproposedmethodtoexistingworks.
General feature-shaping functions. As discussed previously, a feature-shaping function can
represent arbitrary mappings f : RM â†’ RM. While our proposed methods handle mappings
whichcanbedefinedelement-wiseoverindividualfeatures,theyfailtocapturemethodssuchas
ASH(Dosovitskiyetal.,2020)whichcanonlybedefinedoverthefullfeaturevector. Animportant
direction for future work is in designing a tractable optimization problem where the family of
functionsf includeswhole-vectormethodssuchasASH.Weexpectthattheresultantmappings
shouldalsoberelativelysimpleasintheelement-wisecase.
Feature contributions to the maximum logits. Prior research (Sun & Li, 2022; Dietterich &
Guyer,2022)hasexploredtheideaoffeaturecontributionstothemaximumlogits. Forexample,
DICE(Sun&Li,2022)identifiesthemostsignificantfeatureindicesforeachclassusingIDdata.
During test-time, it masks feature indices deemed insignificant before calculating OOD scores.
However,ourapproachdiffersfromtheirsinthatweconsiderfeaturevaluerangesinsteadofindices.
Wefindourfunctionyieldssignificantlybetterperformanceinpractice,partiallyattributedtothefact
thatitadmitsatractableoptimizationproblem. Investigatingthecombinationofthesetwoclassesof
methodsremainsapromisingdirectionforfuturework.
ClassifiertrainingforOODdetection. Priorstudies(Liangetal.,2017;Sun&Li,2022)have
leveragedsurrogateOODdatasets,suchasGaussianandUniformdistributions,totrainclassifiersfor
distinguishingbetweenIDandOODsamples. Liangetal.(2017)primarilyemploythesedatasets
toassesstheirmethodâ€™sperformance,whileSun&Li(2022)usethemforhyperparametertuning.
Despite these efforts, the approach is seldom applied in feature-shaping methods. Concurrently,
severalstudiesareprobingintooutliersynthesis(Duetal.,2022;Heetal.,2022)toapproximatereal
OODdistributionsandtrainclassifiersaccordingly. Ouroptimizationproblemcouldbenefitfrom
theseadvancements,asourparameterizedfeature-shapingfunctioncouldbeoptimizedbasedonan
IDdatasetandsynthesizedoutliers. WeexplorethispossibilityinSectionB.1oftheAppendix.
6 CONCLUSION
Thispaperformulatesageneraloptimizationproblemforfeature-shapingmethodsinOODdetection.
Using this formalism, we are able to better understand and explain the effectiveness of existing
state-of-the-artmethods. Inthecontextofourframework,weproposeanovelfeatureshapingmethod
whichcanbetrainedsolelyonIDdata. Experimentalresultsvalidatethesuperiorperformanceand
generalizationabilityofourapproachcomparedtopriorworks,andmotivatethedevelopmentofnew
methodsunderourframeworktofurtheradvanceOODdetectionmethods.
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGMENTS
We would like to extend our deepest appreciation to Weijian Deng, Yanbin Liu, Yunzhong Hou,
XiaoxiaoSun,andallourlabcolleaguesfortheirinvaluablesupportthroughoutthisproject. Their
collaborativeefforts,insightfuldiscussions,andconstructivefeedbackhavebeencrucialinshaping
andimprovingourpaper.
REFERENCES
MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi. Describ-
ingtexturesinthewild. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.3606â€“3613,2014.
LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestofthe
web]. IEEEsignalprocessingmagazine,29(6):141â€“142,2012.
ThomasGDietterichandAlexGuyer. Thefamiliarityhypothesis: Explainingthebehaviorofdeep
opensetmethods. PatternRecognition,132:108931,2022.
AndrijaDjurisic,NebojsaBozanic,ArjunAshok,andRosanneLiu. Extremelysimpleactivation
shapingforout-of-distributiondetection. arXivpreprintarXiv:2209.09858,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929,2020.
XuefengDu,ZhaoningWang,MuCai,andYixuanLi. Vos: Learningwhatyoudonâ€™tknowbyvirtual
outliersynthesis. arXivpreprintarXiv:2202.01197,2022.
StanislavFort,JieRen,andBalajiLakshminarayanan. Exploringthelimitsofout-of-distribution
detection. AdvancesinNeuralInformationProcessingSystems,34:7068â€“7081,2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pp.770â€“778,2016.
RundongHe,ZhongyiHan,XiankaiLu,andYilongYin. Ronf: reliableoutliersynthesisundernoisy
feature space for out-of-distribution detection. In Proceedings of the 30th ACM International
ConferenceonMultimedia,pp.4242â€“4251,2022.
DanHendrycksandKevinGimpel. Abaselinefordetectingmisclassifiedandout-of-distribution
examplesinneuralnetworks. arXivpreprintarXiv:1610.02136,2016.
DanHendrycks,StevenBasart,MantasMazeika,AndyZou,JoeKwon,MohammadrezaMostajabi,
JacobSteinhardt,andDawnSong. Scalingout-of-distributiondetectionforreal-worldsettings.
arXivpreprintarXiv:1911.11132,2019a.
DanHendrycks,StevenBasart,MantasMazeika,AndyZou,JoeKwon,MohammadrezaMostajabi,
JacobSteinhardt,andDawnSong. Scalingout-of-distributiondetectionforreal-worldsettings.
arXivpreprintarXiv:1911.11132,2019b.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-
sarialexamples. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.15262â€“15271,2021.
GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger. Denselyconnected
convolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.4700â€“4708,2017.
Haojia Kong and Haoan Li. Bfact: Out-of-distribution detection with butterworth filter rectified
activations. In Cognitive Systems and Information Processing: 7th International Conference,
ICCSIP 2022, Fuzhou, China, December 17-18, 2022, Revised Selected Papers, pp. 115â€“129.
Springer,2023.
10PublishedasaconferencepaperatICLR2024
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
ShiyuLiang,YixuanLi,andRayadurgamSrikant. Enhancingthereliabilityofout-of-distribution
imagedetectioninneuralnetworks. arXivpreprintarXiv:1706.02690,2017.
WeitangLiu,XiaoyunWang,JohnOwens,andYixuanLi. Energy-basedout-of-distributiondetection.
Advancesinneuralinformationprocessingsystems,33:21464â€“21475,2020.
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.10012â€“10022,2021.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BoWu,andAndrewYNg. Reading
digitsinnaturalimageswithunsupervisedfeaturelearning. 2011.
OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal. Imagenetlargescalevisualrecognition
challenge. Internationaljournalofcomputervision,115:211â€“252,2015.
MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-ChiehChen. Mo-
bilenetv2: Invertedresidualsandlinearbottlenecks. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.4510â€“4520,2018.
Yue Song, Nicu Sebe, and Wei Wang. Rankfeat: Rank-1 feature removal for out-of-distribution
detection. arXivpreprintarXiv:2209.08590,2022.
Yiyou Sun and Yixuan Li. Dice: Leveraging sparsification for out-of-distribution detection. In
ComputerVisionâ€“ECCV2022: 17thEuropeanConference,TelAviv,Israel,October23â€“27,2022,
Proceedings,PartXXIV,pp.691â€“708.Springer,2022.
YiyouSun,ChuanGuo,andYixuanLi. React: Out-of-distributiondetectionwithrectifiedactivations.
AdvancesinNeuralInformationProcessingSystems,34:144â€“157,2021.
IlyaOTolstikhin,NeilHoulsby,AlexanderKolesnikov,LucasBeyer,XiaohuaZhai,ThomasUn-
terthiner,JessicaYung,AndreasSteiner,DanielKeysers,JakobUszkoreit,etal. Mlp-mixer: An
all-mlparchitectureforvision. Advancesinneuralinformationprocessingsystems,34:24261â€“
24272,2021.
AntonioTorralba,RobFergus,andWilliamTFreeman. 80milliontinyimages: Alargedatasetfor
nonparametricobjectandscenerecognition. IEEEtransactionsonpatternanalysisandmachine
intelligence,30(11):1958â€“1970,2008.
GrantVanHorn,OisinMacAodha,YangSong,YinCui,ChenSun,AlexShepard,HartwigAdam,
PietroPerona,andSergeBelongie. Theinaturalistspeciesclassificationanddetectiondataset. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.8769â€“8778,
2018.
HaoqiWang,ZhizhongLi,LitongFeng,andWayneZhang. Vim: Out-of-distributionwithvirtual-
logitmatching. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.4921â€“4930,2022.
JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,andAntonioTorralba. Sundatabase:
Large-scalescenerecognitionfromabbeytozoo. In2010IEEEcomputersocietyconferenceon
computervisionandpatternrecognition,pp.3485â€“3492.IEEE,2010.
MingyuXuandZhengLian. Vra: Out-of-distributiondetectionwithvariationalrectifiedactivations,
2023.
PingmeiXu,KristaAEhinger,YindaZhang,AdamFinkelstein,SanjeevRKulkarni,andJianxiong
Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint
arXiv:1504.06755,2015.
JingkangYang,KaiyangZhou,YixuanLi,andZiweiLiu. Generalizedout-of-distributiondetection:
Asurvey. arXivpreprintarXiv:2110.11334,2021.
11PublishedasaconferencepaperatICLR2024
FisherYu,AriSeff,YindaZhang,ShuranSong,ThomasFunkhouser,andJianxiongXiao. Lsun:
Constructionofalarge-scaleimagedatasetusingdeeplearningwithhumansintheloop. arXiv
preprintarXiv:1506.03365,2015.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
millionimagedatabaseforscenerecognition. IEEEtransactionsonpatternanalysisandmachine
intelligence,40(6):1452â€“1464,2017.
12PublishedasaconferencepaperatICLR2024
A IMPLEMENTATION DETAILS
A.1 HYPERPARAMETERSETTINGSFOREXPERIMENTSINSECTION4
Forfeature-shapingmethods,weadheretothehyperparametersettingsreportedintheoriginalpapers
oftherespectivemethods.Thesesettingsusuallyoptimizetheperformanceofthemethodsonexisting
benchmarks. Foradditionalmethods,weprimarilyrelyonhyperparametersettingsdetailedinSunâ€™s
work(Sun&Li,2022). ThedetailedhyperparametersettingarelistedinTable4.
Table4: HyperparametersettingsforexperimentsinSection4
Method Hyperparameters ImageNet-1k CIFAR10 CIFAR100
MSP Nohyperparameters - - -
T,temperaturescaling
ODIN T =1000,Ïµ=0.0 T =1000,Ïµ=0.004 T =1000,Ïµ=0.004
Ïµ,noiselevel
MLS Nohyperparameters - - -
Energy Nohyperparameters - - -
DICE p,sparsityparameter p=0.7 p=0.9 p=0.9
ReAct p,thresholdpercentile p=90 p=90 p=90
N =2,thresh=95 N =2,thresh=95 N =2,thresh=95
BFAct thresh,N
percentileofthetrainingset percentileofthetrainingset percentileofthetrainingset
ASH-P p,pruningpercentile p=60 p=90 p=80
ASH-B p,pruningpercentile p=65 p=95 p=85
ASH-S p,pruningpercentile p=90 p=95 p=90
x1=60percentile x1=60percentile
ofthetrainingset ofthetrainingset
VRA-P x1,x2 x1=0.5,x2=1.0
x2=95percentile x2=95percentile
ofthetrainingset ofthetrainingset
K,thenumberof
Ours K=100 K=100 K=100
featurevalueintervals
A.2 THEMAGNITUDEOFRESHAPEDFEATURES
InSection3,wedescribeourmethodforlearningapiecewiseconstantreshapingfunctionsubjectto
normconstraints. ScalingÎ¸hasnoeffectonthevanillaversionofmethod. However,OODdetection
performanceissensitivetothescalingofÎ¸whenourmethodiscombinedwithotherOODscores
withnon-lineartransformationssuchasMSPandtheenergyscore.
Taketheenergyscoreasanexample. Ifwemultiplythefeaturerepresentationzbyasmallpositive
factorq(0<q <1),ignoringthebiasterminthelastfullyconnectedlayer,wehave:
C
(cid:88)
s (â„“)=log exp(qâ„“ ). (15)
Energy j
j=1
ThisisequivalenttoapplyatemperaturescalingT = 1/q totheenergyscore, whichwillaffect
the OOD detection performance, as analyzed in a previous paper (Liang et al., 2017). Besides,
consideringafullyconnectedlayerusuallycontainsabiasterm,thescalingofÎ¸willfurtheraffect
theinfluenceofthebias. Thus,weneedtoconsiderthescaleofÎ¸inouroptimizationproblem.
âˆš
WescaleÎ¸byafactorof K,motivatedbythefollowinganalysis. Consideringthatthemaximum
logitisthesumofallISFIs,wehave:
K
(cid:88)
â„“max(z)= I (z)=1TI(z), (16)
[ak,bk)
k=1
where1representsaK-dimensionalvectorofones,whichisatrivialfeature-shapingfunctionwith
âˆš
noeffect. Therefore,wesetâˆ¥Î¸âˆ¥=âˆ¥1âˆ¥= K.
13PublishedasaconferencepaperatICLR2024
Table5: OODdetectionresultsofvariantsofourmethodsonImageNet-1k. â†‘indicateslargervalues
arebetterandâ†“indicatessmallervaluesarebetter. Allvaluesarepercentagesaveragedoverdifferent
OODdatasets. Boldnumbersaresuperiorresultswhereasunderlinednumbersdenotethesecondand
thirdbestresults. â€œRealâ€meansusingarealOODdatasettoresolveProblem(Eq.10). â€œGaussianâ€
and â€œUniformâ€ indicate using Gaussian and Uniform distributions as surrogate OOD datasets in
resolvingProblem(Eq.10),respectively. â€œDynamicâ€denotessolvinganon-convexproblemwith
dynamicweightindicesdefinedinSectionB.2.
ResNet50 MobileNetV2 ViT-B-16 ViT-L-16 SWIN-S SWIN-B MLP-B MLP-L Average
Method FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 69.30 76.26 72.58 77.41 69.84 77.40 70.59 78.40 65.12 80.89 69.45 77.26 70.66 80.31 71.63 79.93 69.90 78.48
ODIN 61.56 80.92 62.91 82.64 69.25 72.60 70.35 74.51 62.35 78.67 70.90 68.36 65.00 82.99 67.90 81.86 66.28 77.82
Energy 60.97 81.01 61.40 82.83 73.96 67.65 74.89 70.11 66.56 75.95 79.53 60.14 87.86 78.42 84.60 79.01 73.72 74.39
DICE 45.32 83.64 49.33 84.63 89.68 71.32 72.38 67.08 78.84 49.06 77.89 50.10 57.12 80.60 61.98 80.75 66.57 70.90
ReAct 42.29 86.54 54.19 85.37 73.82 76.86 76.16 81.07 58.07 85.10 70.23 80.10 90.13 77.18 89.03 78.32 69.24 81.32
BFAct 43.87 86.01 52.87 85.78 77.64 80.16 84.02 81.12 64.76 85.57 68.40 84.36 96.04 67.87 96.00 70.59 72.95 80.18
ASH-P 55.30 83.00 59.41 83.84 99.36 21.17 99.18 20.27 99.09 21.78 98.83 22.11 98.19 35.85 98.91 29.69 88.53 39.71
ASH-B 35.97 88.62 43.59 88.28 94.87 46.68 93.72 38.95 96.48 36.54 92.40 49.85 99.51 21.73 64.91 83.15 77.68 56.73
ASH-S 34.70 90.25 43.84 88.24 99.48 18.52 99.42 18.61 99.20 18.26 99.06 19.27 97.61 33.91 99.28 19.29 84.07 38.29
VRA-P 37.97 88.58 49.98 86.83 98.39 35.66 99.58 16.70 99.27 20.34 99.46 17.64 99.38 18.73 99.05 21.23 85.39 38.21
Real 42.09 88.13 55.05 85.77 73.69 81.65 75.36 83.10 76.06 83.22 73.54 83.11 81.66 78.34 78.82 79.20 69.53 82.82
Gaussian 42.05 88.20 55.35 85.67 71.88 82.01 73.99 83.28 71.29 83.99 70.79 83.37 82.09 77.58 76.40 79.79 67.98 82.99
Uniform 42.36 88.02 55.12 85.73 73.14 81.82 74.30 83.25 72.68 83.74 70.57 83.43 80.84 76.88 76.41 79.74 68.18 82.83
Dynamic 41.81 88.13 53.29 86.35 69.33 82.59 72.18 83.23 66.90 84.52 65.98 83.71 76.87 78.35 78.10 78.85 65.56 83.22
Ours(V) 41.84 88.11 53.31 86.36 69.33 82.59 72.17 83.23 66.89 84.53 65.96 83.71 76.59 78.46 78.13 78.85 65.53 83.23
Ours(E) 39.75 88.56 51.77 86.62 69.52 82.66 78.57 82.94 66.28 84.99 66.20 84.21 83.54 75.94 85.50 78.11 67.64 83.00
B VARIANTS OF OUR METHOD
Inthissection,weexecuteaseriesofexperimentsonvariantsofourproposedmethods. Initially,we
employrealandsurrogateOODdatasetstoaddresstheoptimizationproblem(Eq.10)asalternatives
toourIDonlymethod. Subsequently,weevaluateanassumptionthatweightindicesofmaximum
logitwillnotchangeafterfeatureshaping. Wecalculateandreporttheratioofchangedweights
afterreshaping. Furthermore,wesolveanoptimizationproblemwhichallowsfordynamicweight
indices. Inthisscenario,itispossiblethattheweightindicesofthemaximumlogitscanundergo
changesafterthereshapingexercise. OurexperimentsarebasedontheImageNet-1kbenchmarkand
theresultsareillustratedinTable5.
B.1 EXPLOITREALANDSURROGATEOODDATASETS
We draw comparisons between our ID only method and a feature-shaping function optimization
approachthataccessesbothIDandOODdatasets.
First, we utilize real OOD datasets to address Problem (Eq. 10). For each model architecture,
ImageNet-1ktrainingset(ID)andiNaturalist(OOD)areleveragedtooptimzeÎ¸.
Second,weincorporateGaussianandUniformdistributionsassurrogateOODdatasetsinresolving
Problem(Eq.10),aligningwithpriorstudies(Liangetal.,2017;Sun&Li,2022). FortheGaussian
distribution,eachpixelisgeneratedwithameanof127.5andastandarddeviationof255. Forthe
uniform distribution, we directly generate random pixel values within the range [0, 255]. Every
generatedimageisofsize32Ã—32,andissubsequentlyclippedandroundedtoconformtothe[0,
255]valuerange. ForeachsurrogateOODdataset,wegenerateatotalof50,000samples.
AsillustratedinTable5,ourmethod,evenwithoutaccesstoOODdata,performscomparablyto
methodsthatoptimizeÎ¸utilizingbothIDandOODdatasets.Moreover,weobservethatamorerobust
objectivefunction,suchasLogLossorHingeLoss,couldpotentiallyoutperformourmethodwhen
givenaccesstorealorsurrogateOODdatasets. Thispresentsanintriguingavenueforexploration.
14PublishedasaconferencepaperatICLR2024
Table 6: Ratio of changed weight indices r on ImageNet-1k. All numbers are percentages. We
randomlysample10,000samplesfromImageNet-1ktrainingset,andrisdefinedinSectionB.2
ResNet50 MobileNetV2 ViT-B-16 ViT-L-16 SWIN-S SWIN-B MLP-B MLP-L Average
r 6.8 6.5 1.1 0.5 1.5 0.9 4.3 0.4 2.8
B.2 DYNAMICWEIGHTINDICES
Ourproposedmethod,aselucidatedinSections3.2and3.3,reliesonafundamentalsimplifying
assumption: theclassindexofthemaximumlogitremainsunchangedafterfeatureshaping.
Weinitiallyexaminethevalidityofthisassumptionbycalculatingtheratioofweightindicesthat
changeafterfeatureshaping.ForagivenIDtrainingsetS ofN samples,eachsamplex(i) âˆˆS
train train
hasweightindicescorrespondingtothemaximumlogitsbeforeandafterfeatureshaping,asfollows:
kËœ(i) =argmaxwTz(i) kÂ¯(i) =argmaxwTzÂ¯(i), (17)
k k
k k
where w denotes the weight vector corresponding to class k in the final fully-connected layer,
k
z(i) represents the feature representation for the sample x(i), and zÂ¯(i) is the reshaped feature.
Subsequently,wecomputetheratioofchangedweightindicesas:
(cid:110) (cid:111)
(cid:80)N 1 kËœ(i) Ì¸=kÂ¯(i)
i=1
r = Ã—100%. (18)
N
Werandomlysample10,000samplesfromImageNet-1ktrainingset. Theratiosofchangedweights
withdifferentmodelsarepresentedinTable6. Asobserved,theratiosaremarkedlysmall,which
furthersubstantiatesthecredibilityofourassumptionpertainingtofixedweights.
Furthermore,weproposeastraightforwardAlternatingOptimizationalgorithmfordynamicweight
indices. Eachiterationinvolvesthreeprimarysteps. Firstly,weoptimizeourfeature-shapingfunction
byresolvingtheoptimizationproblem(Eq.10)giventheweightsrelativetothemaximumlogits
foreachsample. Next,wereshapefeaturesutilizingouroptimizedfeature-shapingfunction. Lastly,
weupdatetheweightswithreshapedfeatures. Toexpeditetheprocess, werandomlysubsample
10,000samplesfromtheIDtrainingsetduringeachiterationandutilizethemforoptimization. Ten
iterationsarerunforeachmodelarchitecture. TheresultscanbeviewedinTable5.
Predictably,thisapproachyieldsperformancemetricsthatcloselymirrorourinitialmethod. Despite
theincorporationofdynamicweightindices,theoptimizedfeature-shapingfunctiondoesnotsignifi-
cantlydifferfromthatobtainedwithfixedweights. Thisobservationreinforcesthevalidityofour
originalassumption: theclassindexofthemaximumlogitremainsconstantfollowingreshaping.
C ROBUSTNESS OF OUR METHOD TO THE CHOSEN LOWER AND UPPER LIMITS
OF FEATURES
Given that feature distributions typically ex-
hibit long-tailed characteristics, we select the Table7: Performance of ourmethod with differ-
0.1and99.9percentilesofallfeaturesinatrain- ent choices of percentiles of all features on the
ingsetasthelowerandupperlimitsoffeature ImageNet-1kbenchmarkwithResNet50.
values, respectively, to optimize our shaping
function. Herewevalidatedifferentchoicesof
Lower Lower Upper Upper
percentilesontheperformanceofourmethod, percentile limit percentile limit FPR95 AUC
ontheImageNet-1kbenchmarkwithResNet50.
0.0 0.00 100.0 20.65 42.42 87.90
TheresultsareshowninTable7. 0.01 0.00 99.99 5.64 42.22 87.97
0.05 0.00 99.95 4.40 41.97 88.04
Our method exhibits a insensitivity to the 0.1 0.00 99.9 3.88 41.82 88.11
chooses of lower and upper limits. Although 0.5 0.00 99.5 2.74 41.01 88.44
1.0 0.00 99.0 2.28 41.00 88.50
inthemainexperimentsweconsistentlyusethe 5.0 0.03 95.0 1.35 47.85 86.41
0.1and99.9percentiles,exploringalternateset- 10.0 0.06 90.0 1.01 57.45 83.60
tingsrevealspotentialforslightimprovements.
15PublishedasaconferencepaperatICLR2024
D EMPIRICAL ANALYSIS OF THE OPTIMAL SHAPING FUNCTION
Here we try to provide empirical
insights into the specific form of
our optimal shaping function based
0.15 1.5
on a ResNet50 model pretrained
0.10 1.0 on ImageNet-1k, which is shown
in Fig. 2. Specifically, we feed
0.05 0.5 ImageNet-1kval(ID)andiNaturalist
(OOD)samplestothemodel,getting
0.00 0.0 the feature representation z âˆˆ RM
andtheweightsw.r.t. themaximum
0.05 ID samples 0.5 logit wmax âˆˆ RM for each sample.
OOD samples TheresultsareshowninFig.5. The
0.10 Our optimized (z) 1.0
redandgreenlinesrepresentthemean
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 weightsassignedtodifferentfeature
Feature value
Figure5: Empiricalanalysistoexplainaspecificformofthe values for ID and OOD samples, re-
optimalshapingfunction. spectively. Thecoloredregionsrepre-
sentstandarddeviations.
Aninterestingaspectisthatourapproachflipsthesignoflow-valuefeatures,contrarytoprevious
techniqueslikeReActandASH.Ourexperimentsrevealedthatlow-valuefeatures(z <0.5)often
alignwithnegativeweights(wmax <0). Intriguingly,IDsamplesgenerallyalignthesefeatureswith
smallerweightsthanOODsamples(onaverage,wmax <wmax <0). Byflippingthesignofthese
ID OOD
low-valuefeatures,wecanenhancethemaximumlogitsmoresignificantlyforIDsamplesthanfor
OODsamples,therebyboostingOODdetectionperformance.
However,notethattheshapeoftheoptimalshapingfunctionisinfluencedbymultiplefactors. For
instance, while ID samples are generally assigned larger weights for high-value features, OOD
samplestendtoexhibitagreaternumberofthesefeatures,asobservedinpriorstudies(Sunetal.,
2021). Thisaspectreducestheimpactofhigh-valuefeaturesindistinguishingbetweenIDandOOD
samples,leadingtothedecreaseofÎ¸forthosefeatures.
Besides,notethatouranalysisaboveisgroundedinthecontextofaResNet50modelpre-trained
onImageNet-1k. Theformoftheoptimalshapingfunctionmaydifferwhenappliedtoothermodel
architecturesordatasets.
E DETAILED OOD DETECTION PERFORMANCE
We report additional results in Tables 8, 9, and 10 to supplement the results provided in Table 1.
Specifically,weprovidedetailedperformanceforalleighttestOODdatasetsfortheImageNet-1k
benchmarkforthreerepresentativemodels,includingResNet50,ViT-B-16,andMLP-B.
F STANDARD IMAGENET-1K BENCHMARK COMPARISON
StandardImageNet-1kbenchmarkusesasubsetofOODdatasetsofours,andtheseresultscanbe
foundinTables8,9,and10.
Besides,wepicktheresultsofstandardImageNet-1kbenchmarkinourexperimentsandcalculated
thenewaverageperformance. Ourmethodstilloutperformspriorworksonthisstandardbenchmark.
ResultsareshowninTable11.
16
tigol
xam
eht
.t.r.w
sthgieW
)z(PublishedasaconferencepaperatICLR2024
Table8: OODdetectionresultswithResNet50onImageNet-1k. â†‘indicateslargervaluesarebetter
andâ†“indicatessmallervaluesarebetter. AllvaluesarepercentagesaveragedoverdifferentOOD
datasets. Boldnumbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthird
bestresults. â€œRealâ€meansusingarealOODdatasettoresolveProblem(Eq.10). â€œGaussianâ€and
â€œUniformâ€indicateusingGaussianandUniformdistributionsassurrogateOODdatasetsinresolving
Problem(Eq.10),respectively. â€œDynamicâ€denotessolvinganon-convexproblemwithdynamic
weightindicesdefinedinSectionB.2.
Species iNaturalist SUN Places OpenImage-O ImageNet-O Texture MNIST Average
Method
FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 79.53 75.15 52.77 88.42 68.58 81.75 71.57 80.63 63.58 84.98 100.00 28.64 66.13 80.46 52.25 90.02 69.30 76.26
ODIN 80.90 72.88 50.86 91.14 59.89 86.59 65.67 84.18 57.29 89.23 100.00 40.86 54.31 86.40 23.54 96.11 61.56 80.92
Energy 82.40 72.03 53.93 90.59 58.27 86.73 65.40 84.13 57.21 89.12 100.00 41.92 52.29 86.73 18.23 96.79 60.97 81.01
DICE 74.64 74.30 26.66 94.49 36.09 90.98 47.66 87.73 45.13 88.77 98.10 43.01 32.46 90.46 1.79 99.40 45.32 83.64
ReAct 68.64 77.41 19.55 96.39 24.00 94.41 33.43 91.93 43.68 90.53 98.05 52.43 45.83 90.45 5.11 98.76 42.29 86.54
BFAct 68.08 77.76 20.53 96.22 20.94 95.24 30.17 92.62 49.71 86.48 96.20 54.15 54.01 87.84 11.33 97.78 43.87 86.01
ASH-P 80.09 73.19 44.57 92.51 52.88 88.35 61.79 85.58 50.45 90.67 100.00 45.99 42.06 89.70 10.57 97.99 55.30 83.00
ASH-B 66.08 80.33 14.21 97.32 22.08 95.10 33.45 92.31 37.63 91.97 93.05 56.82 21.17 95.50 0.06 99.58 35.97 88.62
ASH-S 64.61 81.45 11.49 97.87 27.98 94.02 39.78 90.98 32.74 92.75 89.05 67.51 11.93 97.60 0.01 99.80 34.70 90.25
VRA-P 69.61 76.93 15.70 97.12 26.94 94.25 37.85 91.27 36.37 92.93 95.55 60.79 21.47 95.62 0.30 99.70 37.97 88.58
Real 68.91 78.58 18.64 96.53 37.76 92.69 47.31 89.97 39.63 92.33 97.45 60.01 24.15 95.57 2.90 99.34 42.09 88.13
Gaussian 69.39 78.31 18.88 96.49 37.54 92.68 47.43 89.88 39.60 92.32 97.05 60.85 23.24 95.76 3.25 99.30 42.05 88.20
Uniform 69.34 78.49 18.83 96.51 38.71 92.51 47.98 89.81 39.51 92.46 97.70 59.49 24.24 95.54 2.58 99.37 42.36 88.02
Dynamic 68.81 78.58 18.33 96.63 36.87 92.87 45.94 90.17 39.34 92.54 97.80 59.44 24.70 95.50 2.68 99.34 41.81 88.13
Ours(V) 68.83 78.59 18.33 96.63 37.03 92.84 45.97 90.15 39.29 92.54 97.80 59.33 24.80 95.48 2.67 99.34 41.84 88.11
Ours(E) 68.46 78.51 15.90 97.00 34.00 93.28 43.61 90.50 37.07 92.82 96.25 60.70 21.61 95.99 0.85 99.66 39.75 88.56
Table9: OODdetectionresultswithViT-B-16onImageNet-1k. â†‘indicateslargervaluesarebetter
andâ†“indicatessmallervaluesarebetter. AllvaluesarepercentagesaveragedoverdifferentOOD
datasets. Boldnumbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthird
bestresults. â€œRealâ€meansusingarealOODdatasettoresolveProblem(Eq.10). â€œGaussianâ€and
â€œUniformâ€indicateusingGaussianandUniformdistributionsassurrogateOODdatasetsinresolving
Problem(Eq.10),respectively. â€œDynamicâ€denotessolvinganon-convexproblemwithdynamic
weightindicesdefinedinSectionB.2.
Species iNaturalist SUN Places OpenImage-O ImageNet-O Texture MNIST Average
Method
FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 80.56 72.91 51.52 88.16 66.54 80.93 68.67 80.38 59.93 84.81 90.95 58.80 60.23 82.99 80.29 70.23 69.84 77.40
ODIN 82.18 67.96 52.26 85.24 66.89 76.35 69.14 75.06 58.75 81.55 89.05 54.30 56.68 81.69 79.03 58.65 69.25 72.60
Energy 86.54 61.88 64.08 79.24 72.77 70.24 74.31 68.44 64.93 76.46 87.00 52.71 58.48 79.30 83.61 52.92 73.96 67.65
DICE 88.09 65.08 90.43 74.49 94.27 65.76 92.81 65.19 82.61 77.51 85.85 70.86 83.67 77.63 99.68 74.02 89.68 71.32
ReAct 86.82 66.08 65.15 85.98 72.46 78.97 73.74 77.51 64.71 84.19 87.30 66.70 56.95 84.65 83.41 70.81 73.82 76.86
BFAct 90.52 64.43 80.01 84.62 77.86 81.10 77.55 79.61 69.66 85.45 87.15 73.84 57.61 85.67 80.76 86.53 77.64 80.16
ASH-P 98.38 28.99 99.95 10.42 99.69 19.83 99.66 21.02 99.57 18.91 99.15 30.70 98.49 29.92 100.00 9.57 99.36 21.17
ASH-B 94.34 53.78 92.36 48.20 95.30 52.35 95.75 52.18 92.74 52.73 94.60 51.53 93.90 43.84 99.96 18.86 94.87 46.68
ASH-S 98.11 28.50 99.99 6.71 99.73 16.70 99.66 18.33 99.84 14.07 99.40 28.51 99.11 24.15 100.00 11.20 99.48 18.52
VRA-P 98.14 39.19 99.78 23.06 98.83 30.67 98.89 32.59 98.28 37.46 98.65 43.67 94.57 52.22 100.00 26.40 98.39 35.66
Real 85.07 67.00 64.16 88.03 70.73 83.05 71.76 81.74 65.56 87.14 89.60 73.69 63.09 84.63 79.53 87.93 73.69 81.65
Gaussian 84.14 67.56 61.10 88.71 69.13 83.41 70.28 82.07 63.32 87.51 89.70 73.48 60.16 85.20 77.20 88.16 71.88 82.01
Uniform 84.69 67.25 62.56 88.44 70.60 83.27 71.37 81.93 64.54 87.41 90.20 73.59 61.60 84.94 79.54 87.71 73.14 81.82
Dynamic 83.20 69.47 56.06 89.91 66.63 84.17 68.48 82.69 60.21 88.17 89.50 71.82 56.81 86.43 73.75 88.08 69.33 82.59
Ours(V) 83.19 69.47 56.09 89.91 66.63 84.17 68.51 82.69 60.21 88.18 89.50 71.82 56.79 86.43 73.75 88.08 69.33 82.59
Ours(E) 84.56 68.70 60.09 89.55 68.02 83.92 69.42 82.36 61.77 88.07 87.95 72.74 54.61 86.72 69.72 89.21 69.52 82.66
17PublishedasaconferencepaperatICLR2024
Table10: OODdetectionresultswithMLP-BonImageNet-1k. â†‘indicateslargervaluesarebetter
andâ†“indicatessmallervaluesarebetter. AllvaluesarepercentagesaveragedoverdifferentOOD
datasets. Boldnumbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthird
bestresults. â€œRealâ€meansusingarealOODdatasettoresolveProblem(Eq.10). â€œGaussianâ€and
â€œUniformâ€indicateusingGaussianandUniformdistributionsassurrogateOODdatasetsinresolving
Problem(Eq.10),respectively. â€œDynamicâ€denotessolvinganon-convexproblemwithdynamic
weightindicesdefinedinSectionB.2.
Species iNaturalist SUN Places OpenImage-O ImageNet-O Texture MNIST Average
Method
FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 82.15 73.21 58.99 86.84 72.98 80.92 75.52 79.83 72.42 82.59 90.95 65.84 69.95 80.27 42.36 92.95 70.66 80.31
ODIN 80.99 73.39 53.31 89.53 67.47 84.24 71.67 82.44 68.06 85.51 90.45 68.69 59.75 84.40 28.33 95.73 65.00 82.99
Energy 89.04 71.84 84.31 83.65 86.85 79.35 87.15 78.42 82.30 81.24 88.40 67.53 85.25 78.41 99.58 86.91 87.86 78.42
DICE 71.40 75.65 44.44 90.19 64.69 80.84 70.31 78.08 63.89 80.26 89.90 56.33 52.20 84.33 0.12 99.11 57.12 80.60
ReAct 91.20 70.81 91.47 80.11 91.83 77.31 90.99 76.82 85.66 79.60 87.85 67.74 87.36 77.20 94.65 87.89 90.13 77.18
BFAct 95.94 64.38 98.60 65.76 97.16 67.50 96.66 68.74 94.69 70.00 90.85 66.57 94.61 67.83 99.85 72.19 96.04 67.87
ASH-P 97.25 46.50 99.22 32.82 98.72 40.24 97.74 43.07 98.67 38.34 94.55 52.65 99.38 31.38 100.00 1.81 98.19 35.85
ASH-B 99.67 25.98 100.00 15.86 99.71 19.84 99.74 21.36 99.82 24.19 97.25 46.39 99.86 19.48 100.00 0.74 99.51 21.73
ASH-S 96.50 42.89 98.42 32.35 98.03 38.00 97.90 38.63 97.65 37.03 93.50 50.19 98.90 30.41 100.00 1.80 97.61 33.91
VRA-P 98.88 29.11 99.92 11.57 99.73 17.54 99.41 20.71 99.77 16.82 97.80 37.15 99.50 16.49 100.00 0.44 99.38 18.73
Real 86.49 71.56 71.23 84.79 79.47 79.42 81.95 78.44 78.30 81.00 91.00 66.46 78.24 78.36 86.61 86.69 81.66 78.34
Gaussian 87.00 71.10 73.67 83.35 79.82 78.80 82.07 77.86 79.06 79.83 91.20 65.95 78.90 77.64 85.01 86.10 82.09 77.58
Uniform 86.92 70.35 75.45 81.81 80.47 77.69 82.25 77.07 79.29 78.56 90.70 65.63 78.05 76.45 73.59 87.51 80.84 76.88
Dynamic 87.24 70.88 76.95 82.17 83.83 77.26 84.75 76.12 78.65 80.31 90.70 65.68 76.76 79.17 36.06 95.19 76.87 78.35
Ours(V) 86.98 70.98 76.40 82.36 83.45 77.42 84.67 76.27 78.35 80.46 90.85 65.77 76.40 79.26 35.66 95.19 76.59 78.46
Ours(E) 91.67 69.17 92.48 76.83 91.83 73.88 91.86 73.34 87.14 77.34 90.40 66.02 89.29 75.33 33.61 95.61 83.54 75.94
Table11:OODdetectionresultsonImageNet-1kwhereOODdatasetsareiNaturalist,SUN,Places,
andTextures. â†‘indicateslargervaluesarebetterandâ†“indicatessmallervaluesarebetter. Bold
numbersaresuperiorresultswhereasunderlinednumbersdenotethesecondandthirdbestresults.
ResNet50 MobileNetV2 ViT-B-16 ViT-L-16 SWIN-S SWIN-B MLP-B MLP-L Average
Method FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘ FPâ†“ AUâ†‘
MSP 64.76 82.82 70.47 80.67 61.74 83.12 65.22 81.75 59.68 83.75 62.79 81.38 69.36 81.97 76.01 80.04 66.25 81.94
ODIN 57.68 87.08 60.42 86.39 61.24 79.59 64.06 78.65 57.30 80.99 64.36 73.77 63.05 85.15 73.17 82.29 62.66 81.74
Energy 57.47 87.05 58.87 86.59 67.41 74.31 68.43 74.65 62.82 77.65 75.32 64.87 85.89 79.96 84.44 79.38 70.08 78.06
DICE 35.72 90.92 41.93 89.60 90.30 70.77 71.77 67.12 88.68 39.90 87.92 40.71 57.91 83.36 66.23 81.53 67.55 70.49
ReAct 30.70 93.30 50.09 88.81 67.08 81.78 69.58 83.50 50.09 87.92 64.86 83.24 90.41 77.86 87.01 79.13 63.73 84.44
BFAct 31.41 92.98 48.35 89.19 73.26 82.75 81.16 82.69 57.20 88.21 65.44 86.62 96.76 67.46 96.36 72.16 68.74 82.76
ASH-P 50.33 89.04 57.15 87.34 99.45 20.30 99.42 18.37 99.11 19.21 99.08 20.59 98.77 36.88 99.04 29.20 87.79 40.11
ASH-B 22.73 95.06 35.66 92.13 94.33 49.14 94.08 37.89 96.42 30.00 91.47 47.38 99.83 19.14 66.05 84.31 75.07 56.88
ASH-S 22.80 95.12 38.67 90.95 99.62 16.47 99.55 16.72 99.36 14.98 99.35 17.75 98.31 34.85 99.36 18.65 82.13 38.18
VRA-P 25.49 94.57 45.53 89.85 98.02 34.64 99.62 14.99 99.38 18.15 99.65 15.51 99.64 16.58 99.23 19.49 83.32 37.97
Ours(V) 31.53 93.78 49.09 89.62 62.01 85.80 68.61 85.10 61.47 87.04 62.96 86.09 80.23 78.83 82.59 78.92 62.31 85.65
Ours(E) 28.78 94.19 46.92 89.90 63.04 85.64 74.96 84.69 60.67 87.50 63.50 86.55 91.37 74.85 88.49 78.22 64.71 85.19
18