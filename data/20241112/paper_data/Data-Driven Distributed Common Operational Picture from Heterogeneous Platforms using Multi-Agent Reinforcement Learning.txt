Data-Driven Distributed Common Operational Picture
from Heterogenous Platforms
using Multi-Agent Reinforcement Learning
Indranil Sur1â€  Aswin Raghavan1â€  Abrar Rahman1 James Z Hare2 Daniel Cassenti2 Carl Busart2
1SRI, Princeton, NJ, USA. 2DEVCOM Army Research Laboratory
{indranil.sur, aswin.raghavan, abrar.rahman} {james.z.hare.civ, daniel.n.cassenti.civ,
@sri.com carl.e.busart.civ} @army.mil
Abstract
The integration of unmanned platforms equipped with advanced sensors promises to enhance situational awareness and mitigate
the â€œfog of warâ€ in military operations. However, managing the vast influx of data from these platforms poses a significant challenge
for Command and Control (C2) systems. This study presents a novel multi-agent learning framework to address this challenge. Our
method enables autonomous and secure communication between agents and humans, which in turn enables real-time formation of
an interpretable Common Operational Picture (COP). Each agent encodes its perceptions and actions into compact vectors, which
are then transmitted, received and decoded to form a COP encompassing the current state of all agents (friendly and enemy) on the
battlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP models and agentâ€™s action selection policies. We
demonstrate resilience to degraded conditions such as denied GPS and disrupted communications. Experimental validation is
performed in the Starcraft-2 simulation environment to evaluate the precision of the COPs and robustness of policies. We report less
than 5% error in COPs and policies resilient to various adversarial conditions. In summary, our contributions include a method for
autonomous COP formation, increased resilience through distributed prediction, and joint training of COP models and multi-agent
RL policies. This research advances adaptive and resilient C2, facilitating effective control of heterogeneous unmanned platforms.
1 INTRODUCTION On the receiverâ€™s side, the message must be decoded to
recover the senderâ€™s perception and action. Furthermore,
The integration of unmanned platforms equipped with
the information should be integrated (aggregated over
advanced sensors holds promise for mitigating the â€œfog of
time) into a Common Operational Picture (COP). Like the
warâ€ and elevating situational awareness. However,
encoder, the decoder is also learned in a data-driven
managing and disseminating the influx of data from such
manner. In this paper, we simplify the definition of a COP
platforms poses a substantial challenge to the information
as the current state (position, health, shield, weapon, etc.)
processing capabilities of central Command and Control
of each friendly and enemy agents on the battlefield. We
(C2) nodes, particularly given the exponential growth in
argue that the COP is essential to decision-making agents.
data volume with increasing platform numbers. The
current manual processing methods are ill-suited for In recent years, AI/ML approaches that are trained end-
future C2 scenarios involving swarms of unmanned to-end in a data-driven manner have shown great
platforms. In this study, we present a framework utilizing promise. In the context of a data-driven autonomous COP,
a multi-agent learning approach to overcome this barrier. one advantage is that no modelling assumptions are made
about the noise in the sensors and actuators, the
We consider a framework where agents communicate
dynamics of the adversary, etc. With sufficient training,
with each other (and with humans) in an autonomous
our data-driven method will produce highly precise COPs.
fashion, and such communication functions are trained in
a data-driven manner. At each time step, each agent can However, ML models can be sensitive to deviations from
send/receive a real-valued message vector. The vector is the training data or training scenarios. This contrasts with
a learned encoding of the agentâ€™s perception or field of the DDIL (denied, disrupted, intermittent, and limited
view (FoV). The vectors are not easily interpretable by impact) environments, which are typically assumed in
adversaries, allowing for secure message transfer. army C2 scenarios. Our experiments emphasize
â€  Equal Contribution
2 ICCRTS 2024evaluation of the resilience to increased fog, denied GPS,
and disruption of communications (e.g., jamming).
Data-driven end-to-end training of our encoders and
decoders is achieved using deep learning of deep neural
networks (DNN). One challenge associated with the
application of DNNs to COP formation is the lack of human
interpretability in communications. Human
interpretability is crucial for a human operator to
effectively control the swarm. For example, by
interpreting the communication, the operator might
understand the features used by the swarm for
Figure 1: (Left) Example state from the Tigerclaw scenario.
(autonomous) decision-making. Our method is human-
(Right) Each agentâ€™s perception (local observation) and
machine interchangeable, meaning that a human
communication links between them.
operator can decode the incoming messages and encode
their perceptions to communicate with the swarm. The
resulting COP enables human directability of the swarm. DEVCOM Army Research Lab (ARL) and Army subject-
matter experts (SMEs) at the Captainâ€™s Career Course, Fort
In practice, the COP is heavily used in mission execution,
Moore, Georgia, US [2].
e.g., to ensure coordinated movements. We hypothesize
that incorporating the COP into autonomous decision- The COPs are evaluated for accuracy and hallucinations
making agents will produce resilient multi-agent policies that reveal interesting training dynamics. Our method
(e.g., resilience to changes in the enemy). Our produces highly accurate COPs with less than 5% error
experiments compare multi-agent policy learning with (compared to ground truth) over the entire simulation. To
and without the COP against multiple state-of-the-art test the robustness of policies, we compare our method
methods and validate the hypothesis. to multiple state-of-the-art multi-agent RL methods and
baselines. We show that our method produces policies
Next, we summarize our methodology. We first describe resilient to degraded visual range, degraded
our deep learning formulation in which each agent communication, denied GPS, and changes in the scenario.
encodes its perceptions and actions into compact vectors
In summary, this research enables the command and
and transmits them. The underlying embedding vector
control of heterogeneous autonomous platforms with
space is shared across agents to enable a shared
human-in-the-loop through data-driven COP formation
situational understanding. An encoder-decoder is trained
and advances the field of adaptive and resilient C2. The
per agent to produce local COPs. The local COP should be
contributions are as follows:
consistent with agent perceptions and should predict all
unitsâ€™ state (incl. position) over the area of operation. â€¢ A method to autonomously form an interpretable
Common Operational Picture (COP) in real-time,
End-to-end training of the COP is performed jointly with
including the prediction of enemy positions over the
agent policies using Deep Reinforcement Learning (DRL)
entire area of operation.
on a diverse set of simulated scenarios, initial force
â€¢ Demonstrate increased resiliency to denial of visual
configurations, and adversary actions. The output of
range and GPS because of the distributed COP
training is an encoder-decoder neural network (NN) and a
prediction using inter-agent communication.
policy NN shared across agents. The training can be
â€¢ Increased overall mission success by joint training of
configured in several ways: to minimize bandwidth,
COP models and multi-agent RL policies.
maximize resilience to disruption e.g., channel noise,
packet loss, jamming of GPS, etc. The method can be
2 PROBLEM FORMULATION
applied to coordinated information-gathering missions.
Consider a multi-agent system consisting of ğ‘ agents.
Experiments are performed in the Starcraft-2 (SC2) multi-
Each agent is defined as the tuple (ğ’ªğ‘–,ğ”„ğ‘–,ğ’ ,ğ’ ,ğ”“ğ‘–),
agent environment [1]. The effectiveness of our method is ğ‘–ğ‘› ğ‘œğ‘¢ğ‘¡
for agent ğ‘– = 1â€¦,ğ‘. The agentsâ€™ perception of the
empirically observed in multiple blue-vs.-red scenarios
environment is a mapping ğ’ªğ‘–:ğ”–Ã—ğ”“ğ‘– â†’ â„ğ‘‘ from the
that are modelled in SC2. Specifically, we test and
underlying state of the world (ğ‘  âˆˆ ğ”–) and agent-specific
evaluate our method on the challenging and realistic
capabilities ğ”“ğ‘– (e.g., field-of-view) to a ğ‘‘-dimensional
TigerClaw scenario (Figure 1) that was developed by the
observation vector ğ‘œğ‘– = ğ’ª(ğ‘ ,ğ‘ )âˆˆâ„ğ‘‘ (dropping the
ğ‘–
ICCRTS 2024 3Figure 2: Illustration of the challenge in COP formation in
the face of GPS denial. Colors represent different agents. Figure 3: Overview of our framework for COP prediction
from learned communication. The COP is determined and
dependence on time for brevity). Figure 1 shows an
used in the decision-making process. We use QMIX [4] as
example of state and agent observations. In this paper,
an example MARL method for COP integration.
agent observations and actions are in the agentâ€™s frame of
reference i.e., egocentric, whereas states are represented situational awareness and is necessary for developing a
in a global frame of reference. For example, agent Common Operational Picture (COP), i.e., a global
observations might include range, bearing, and health of understanding of the uncertainty in the underlying state
observed units, whereas the state will include ground as the battle evolves. In the distributed setting, each agent
truth about all quantities of all units (blue-vs-red). forms a local prediction of the COP via communication
and propagation of other agent's local COP, observation
At each time step, each agent receives a ğ¶-dimensional
and action, which allows for uncertainty reduction and
communication message from each agent from the
improved situational awareness.
previous time step. The messages are processed using the
function ğ’ :â„ğ¶ğ‘ â†’ â„ğ¶. In this paper, denied For example, a friendly agent could observe the range and
ğ‘–ğ‘›
bearing of enemy units in its local frame of reference (e.g.,
communication and out-of-range communication is
a tank at 1200 meters, 30 degrees north). Given this
represented as zero-valued vectors at the receiver ğ’ .
ğ‘–ğ‘›
information, a different friendly agent in another part of
We do not assume stable communication pathways to be
the battlefield needs to infer the tankâ€™s relative position
able to send and receive transmissions; rather, messages
to itself. This is challenging, especially when global
are â€œzero-edâ€ out in the communication channel
positioning is denied. This challenge is illustrated in Figure
unbeknownst to the sender.
2. Our COP enables a solution to this challenge using the
At each time step, each agent sends a ğ¶-dimensional
shared embedding space for communications.
communication message using the function ğ’ :ğ’ªğ‘– Ã—
ğ‘œğ‘¢ğ‘¡
Figure 3 shows the overall learning-to-communicate
ğ”„ğ‘–Ã—â„ğ¶ â†’ â„ğ¶ that maps the agentâ€™s local observation,
framework. All the components are represented with
action at the current time step and the result of ğ’ to an
ğ‘–ğ‘›
NNs, and all the weights are trained end-to-end in a data-
output message. Information to uniquely identify the
driven manner. Since the communication modules are
agent can also be transmitted by including a unique ID for
shared across agents and the decoder output is updated
each agent in the observation space.
in real-time using communication, we refer to the
The functions ğ’ ğ‘–ğ‘› and ğ’ ğ‘œğ‘¢ğ‘¡ are represented as neural predicted state as the Common Operational Picture (COP).
networks (NN) with learnable weights. The NN weights Future work can extend our data-driven COP framework
are shared across agents (and human operators). to incorporate future actions, such as agent intent.
Each agent can take an action ğ‘ğ‘– from its action set ğ”„ğ‘– that Since the communication is grounded in an interpretable
affects the evolution of the state. The action set can be state, human operators can use the learned black boxes
fixed or a function of capabilities ğ”“ğ‘– or observation ğ‘œ . We to receive and interpret the communication between
ğ‘–
train a policy shared across agents ğœ‹:ğ’ªğ‘– Ã—â„ğ¶ğ‘ â†’ ğ”„ğ‘– that autonomous platforms. Similarly, human operators can
maps its local observation and messages from other encode their observations and transmit them using ğ’ ğ‘œğ‘¢ğ‘¡.
agents to an action in its own action set. In the rest of this paper, we do not consider human
operators and assume the agents to be autonomous
In one sense, the multi-agent system forms a distributed
platforms.
mobile sensor network where each agent senses only a
part of the world. Inference of the underlying state The multi-agent system models a swarm performing a
(including friendly and enemy positions) corresponding to joint task, e.g., autonomous platforms in a C2 operation.
the agents' local observations is a key component of The joint task is captured as an overall system reward
4 ICCRTS 2024function â„›:ğ’®Ã—ğ”„1Ã—â€¦Ã—ğ”„ğ‘ â†’ [âˆ’1,+1]. For example, ğ‘–ğ‘›ğ‘“(ğ‘„ğ‘šğ‘–ğ‘¥(ğ‘  ,ğ‘1,â€¦,ğ‘ğ‘)
ğ‘¡ ğ‘¡ ğ‘¡
the reward can capture mission success rate, Blue Force ğ‘„,ğœƒ
2
casualties, attrition rate, task completion, etc.
âˆ’(ğ‘Ÿ +ğ‘„ğ‘šğ‘–ğ‘¥(ğ‘  ,ğ‘1 ,â€¦,ğ‘ğ‘ ))) (1)
ğ‘¡ ğ‘¡+1 ğ‘¡+1 ğ‘¡+1
The agents are trained to maximize the reward in a where ğ‘Ÿ is the system reward at time ğ‘¡ for taking actions
ğ‘¡
centralized manner within the framework of Centralized ğ‘ = (ğ‘1,ğ‘2,â€¦,ğ‘ğ‘) in state ğ‘  . We are significantly
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡
Training Decentralized Execution (CTDE). The result of
simplifying the exposition of QMIX [4] and deep Q-
training is a decentralized policy ğœ‹ğ‘–:ğ’ªğ‘– Ã—â„ğ¶ğ‘ â†’ ğ”„ğ‘–. learning [6] and leaving out key details like exploration,
During deployment, each agent executes the policy as a target network, double Q-learning [7], etc. Note that the
function of local observation and inter-agent state is available during centralized training only and not
communication. available during deployment. Similarly, the centralized Q-
function is used for training and not during deployment.
The overall decision-making problem is defined as a
QMIX does not use inter-agent communication.
Decentralized Partially Observable Markov Decision
Process (DecPOMDP) (ğ‘,ğ‘ƒ 0,ğ”–,{ğ’ªğ‘–},{ğ”„ğ‘–},ğ’¯,â„›,{ğ”“ğ‘–}), 3 COP MODEL AND TRAINING
ğ‘– = 1,â€¦,ğ‘. ğ‘ƒ defines the distribution over initial agent
0 In this section, we describe the neural network (NN)
positions or â€œlaydownâ€ (friendly and enemy). ğ’¯
architecture and training for the communication modules
represents the transition function over time ğ‘  =
ğ‘¡+1 ğ’ and ğ’ , decoder ğ’Ÿ, and the policy ğœ‹. We motivate
ğ‘–ğ‘› ğ‘œğ‘¢ğ‘¡
ğ’¯(ğ‘  ,ğ‘1,â€¦,ğ‘ğ‘) and ğ‘  âˆ¼ğ‘ƒ that captures the evolution
ğ‘¡ ğ‘¡ ğ‘¡ 0 0 the design choices that will be investigated in Section 4.
of state and the effect of the agentâ€™s actions.
Note that our COP model can be optimized with any MARL
We evaluate the robustness of the produced COPs and algorithm such as QMIX.
policies using a separate â€œtestâ€ DecPOMDP. In this paper,
We use the autoencoder (AE) concept as follows: an NN
we focus on varying the capabilities of agents ğ”“ğ‘– and encoder transforms an agent observation to a latent
laydown distribution ğ‘ƒ 0. The DecPOMDPs are vector ğ‘§. Paired with the encoder is an NN decoder that
incorporated into a simulator. In this paper we use the reconstructs the observation given the latent vector ğ‘§. An
Starcraft-2 real-time strategy game [3] to simulate AE is trained end-to-end to minimize reconstruction error,
Command-and-Control (C2) scenarios. e.g., mean squared error (MSE).
2.1 BACKGROUND: QMIX FOR MULTI-AGENT LEARNING Following [17], instead of communicating â€œrawâ€
observations, we can communicate the compact latent
QMIX [4] is a standard multi-agent Reinforcement
vector ğ‘§ and ensure that the embedding vectors
Learning (MARL) algorithm within the CTDE framework. It
correspond to the agent observations. The embeddings
extends Q-learning [5] to the multi-agent setting. QMIX
are not easily interpretable by the adversary allowing for
learns a Q-function for each agent ğ‘„:ğ’ªğ‘– Ã—ğ”„ğ‘– â†’ â„,
secure message transfer. However, there are two
mapping agent observation and action to a value. The Q-
significant challenges in the application of AE to COP.
function defines an agent policy ğœ‹ e.g., defined by the
action with maximum Q-value. ğ‘„(ğ‘œğ‘–,ğ‘ğ‘–) is the total Suppose each agent encodes its own observation ğ‘œğ‘– at the
ğ‘¡
reward of executing ğ‘ğ‘– and then following the policy. current time step and action ğ‘ğ‘– of the previous time
ğ‘¡âˆ’1
step and transmits the latent vector. Note that the
QMIX trains another centralized NN to model the Q-value
observation and action are in the agentsâ€™ frame of
of joint actions as a function of the Q-values of individual
reference. On the receiver side at time ğ‘¡+1, an agent
agent actions. In the simplest version, the centralized Q-
(that can receive the message) can use an NN decoder ğ·
function is a linear combination of agent Q-values. The
to decode the senderâ€™s observation. However, the
coefficients of the linear combination are learned as a
decoding will be in the sendersâ€™ frame of reference.
function of the global state ğ‘  (through a â€œhypernetâ€ NN).
ğ‘¡
Secondly, communication of agent observations alone is
ğ‘
ğ‘„ğ‘šğ‘–ğ‘¥(ğ‘ ,ğ‘1,â€¦,ğ‘ğ‘)= âˆ‘ğœƒ (ğ‘ )ğ‘„(ğ‘œğ‘–,ğ‘ğ‘–) insufficient. Consider agents in a linear chain where each
ğ‘–
agent can only communicate with its neighbor. For the
ğ‘–=1
first agentâ€™s observation to reach the last agent, each
In this model, the Q-function has only first-order terms,
agent must not only transmit its own (encoded)
but higher-order terms can be included to capture
observation, but also (re-)transmit the message received
correlations between agent Q-functions. The agent Q-
from its neighbor. In general, the number of messages can
functions, centralized Q-function and the hypernet are
grow exponentially (e.g., in tree-shaped connectivity
trained end-to-end with a temporal difference (TD) error:
structure). Rather, the agent must integrate the received
ICCRTS 2024 5observation (after decoding) and its own observation into What type of NN architecture should be used to integrate
a local COP. Then, only the latent vector corresponding to communication? All received communications, from all
the local COP needs to be transmitted. agents can be potentially informative. Thus, we choose an
attention-based architecture [8], which uses all-to-all
In this distributed setting, local integration of COP leads to
connections, over an architecture based on convolutions
predictions about agents outside any given agentsâ€™ visual
that capture local connections. The attention-based
range. The local COP, or an aggregation of local COPs,
architecture can learn to selectively attend to certain
should be interpreted as a density over the entire
messages as a function of inputs, which is more powerful
battlefield. Based on prior information, recency of
than fully connected architectures (like perceptron).
observation and communication, the COP should have
high density around likely agent positions. The spread or We use cross-attention [8] to process two sequences. Let
uncertainty should decrease with the frequency of ğ‘§ğ‘– be the integrated COP embedding vector. Let ğ’„=
ğ‘ 
observation of a given part of the battlefield by any agent. [ğ‘§1,â€¦,ğ‘§ğ‘] and ğ’„ğ’” = [ğ‘§1,â€¦,ğ‘§ğ‘] be the embeddings
ğ‘  ğ‘ 
received by the ğ‘–th agent (ğ‘§ğ‘– as in Eq. 2). Then,
We used the agent position attribute as an example to
motivate the challenges in COP prediction and our ğ‘§ğ‘– âˆ ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ¾ğ‘‡)ğ‘‰
ğ‘ 
distributed solution. In practice, our COP contains all
information relevant to the state of an operation, e.g. where ğ‘„ = ğ‘Š ğ’„, ğ¾ = ğ‘Š ğ’„ğ’”, ğ‘‰ = ğ‘Š ğ’„ğ’” where ğ‘Š are
ğ‘ ğ‘˜ ğ‘£
agent armor, health, weapon status etc., each with an learned matrices (in practice, multiple NN layers are
associated uncertainty (e.g., standard deviation). Our stacked to form ğ‘§ğ‘–). The incoming message processing
ğ‘ 
data-driven learning method, where ground truth is used module ğ’ (Section 2) is defined as ğ’ (ğ’„,ğ’„ğ’”)â‰œ ğ‘§ğ‘–. In
ğ‘–ğ‘› ğ‘–ğ‘› ğ‘ 
to train each agentâ€™s COP model, is general to all
practice, several rounds of communication are used to
attributes. We do not need to assume a sensor noise
propagate information and achieve consensus, by running
model or agent motion model in order to capture the
the communication modules for multiple iterations so
uncertainty in each attribute represented in the COP.
ğ‘§ = ğ¶ (ğ’ (â€¦(ğ’ (ğ‘š)))).
3.1 OBSERVATION ENCODING AND DECODING ğ‘  ğ‘–ğ‘› ğ‘œğ‘¢ğ‘¡ ğ‘–ğ‘›
We apply a standard AE to the observation and action. Next, we track the temporal evolution of the COP using a
Each agent encodes its own observation ğ‘œ ğ‘¡ğ‘– at the current recurrent NN (e.g., GRU [9]) that updates â„ ğ‘  = ğºğ‘…ğ‘ˆ(ğ‘§ ğ‘ )
time step and action ğ‘ ğ‘¡ğ‘– âˆ’1 of the previous time step using using the history of ğ‘§ ğ‘  values. It is this â„ ğ‘  that we pass to
NN encoder ğ¸ , and transmits the embedding latent the policy and the COP decoder.
ğ‘œğ‘ğ‘ 
vector ğ‘§ ğ‘¡ğ‘– = ğ¸ ğ‘œğ‘ğ‘ (ğ‘œ ğ‘¡ğ‘–,ğ‘ ğ‘¡ğ‘– âˆ’1). We use a ğ‘¡ğ‘ğ‘›â„ activation on We train a COP decoder NN ğ’Ÿ
ğ‘ğ‘œğ‘
with the ground truth
the encoder output so that each entry in the vector ğ‘§ ğ‘¡ğ‘– is egocentric state ğ‘ ğ‘– = ğ‘“(ğ‘ ,ğ‘–),ğ‘  âˆˆ ğ’®, where ğ‘“ is centering
bounded in the range [âˆ’1,+1]. the state on the agent. For example, centering translates
the positions of all units relative to the position of agent ğ‘–.
We consider two versions of the observation decoder:
ğ‘
1
â€¢ A ton (e ğ‘œÌ‚x ğ‘¡ğ‘–p ,ğ‘l Ì‚ic
ğ‘¡ğ‘–
âˆ’it
1
).d e Tc ho ed e or bsN eN
rv
atğ· ioğ‘œ nğ‘ ğ‘  AEt ha ist trm aia np es
d
ğ‘§ toğ‘–ğ‘¡ ğ·ğ‘–ğ‘› ğ‘ğ‘œğ‘“ ğ‘ğ‘âˆ‘ ğ‘–=1||ğ‘  ğ‘¡ğ‘– âˆ’ğ’Ÿ ğ‘ğ‘œğ‘(â„ ğ‘ )||
2
(3)
minimize â„ ğ‘  = ğºğ‘…ğ‘ˆ(ğ‘§ ğ‘ )
ğ‘§ = ğ’ (ğ’ )
ğ‘  ğ‘–ğ‘› ğ’•
ğ‘–ğ‘›ğ‘“ ||ğ‘œğ‘– âˆ’ğ‘œÌ‚ğ‘–|| +||ğ‘ğ‘– âˆ’ğ‘Ì‚ğ‘– || (2)
ğ¸ğ‘œğ‘ğ‘ ,ğ·ğ‘œğ‘ğ‘  ğ‘¡ ğ‘¡ 2 ğ‘¡âˆ’1 ğ‘¡âˆ’1 2 where ğ‘  ğ‘¡ is ground truth state provided by the simulator,
â€¢ An implicit decoder that is part of a COP decoder, ğ’ = [ğ’ (ğ‘œ1 ,ğ‘1 ),â€¦,ğ’ (ğ‘œğ‘ ,ğ‘ğ‘ )] is the
ğ’• ğ‘œğ‘¢ğ‘¡ ğ‘¡âˆ’1 ğ‘¡âˆ’1 ğ‘œğ‘¢ğ‘¡ ğ‘¡âˆ’1 ğ‘¡âˆ’1
where reconstruction (ğ‘œÌ‚ğ‘–,ğ‘Ì‚ğ‘– ) is extracted from the concatenation of the ğ¶-dimensional messages received
ğ‘¡ ğ‘¡âˆ’1
COP and the known agent capabilities (e.g., FoV). from the agents from the previous time step,
3.2 INTEGRATION OF COMMUNICATION ğ’ ğ‘œğ‘¢ğ‘¡(ğ‘œğ‘–,ğ‘ğ‘–)â‰œ [ğ‘§,ğ‘§ ğ‘ ] = [ğ¸ ğ‘œğ‘ğ‘ (ğ‘œ,ğ‘),ğ’ ğ‘–ğ‘›(ğ‘,ğ‘ ğ‘ )]
As described above, each agent must integrate received applied to the previous time step observation and action.
communications and produce an embedding vector
Note that the agents transmit both ğ‘§ and ğ‘§ to mitigate
ğ‘ 
corresponding to the COP. The embedding should be
two different sources of error: ğ‘§ controls the error in
informative as an additional input to agent policies. The
reconstruction of agentâ€™s observation (within field-of-
embedding space is shared across agents to enable a
view) and ğ‘§ controls the error in prediction of
ğ‘ 
shared situational understanding among agents and
unobserved enemies (outside the field-of-view).
human operators.
6 ICCRTS 2024ğ» is the predicted health predicted by the COP model.
ğ‘ğ‘œğ‘
The term is added for all agents and simulation steps.
3.5 INTEGRATING COP WITH POLICY LEARNING
As mentioned before, the COP â„ is provided as an
ğ‘ 
additional input to the policy via the GRU. Now we
describe the policy architecture. Following QMIX (Section
2.1), the policy first processes the observation and action
using linear layers. Then, we use a recurrent NN (e.g.,
GRU) to capture the observation and action history. The
hidden state of the policy GRU â„ and the hidden state of
ğœ‹
the COP GRU are concatenated. The agent Q-value
function takes the concatenated hidden state (denoted
[â„ ,â„ ]) and uses linear layers to map to action values.
ğ‘  ğœ‹
Figure 4 shows the NN architecture for COP and policy.
The overall training objective is a combination of policy
and COP training objectives from Eq. (1), (2), (3), (4):
Figure 4: Overview of the agent architecture. The output is
the agent action ğ‘ ğ‘¡ğ‘– and agent communication (ğ‘§,ğ‘§ ğ‘ ). The inf (ğ‘„ğ‘šğ‘–ğ‘¥(ğ‘  ,ğ‘1,â€¦,ğ‘ğ‘)
ğ‘¡ ğ‘¡ ğ‘¡
observation decoder ğ· is optional. ğ‘„,ğ¸ğ‘œğ‘ğ‘ ,ğ·ğ‘ğ‘œğ‘,ğ·ğ‘œğ‘ğ‘ 
ğ‘œğ‘ğ‘  2
âˆ’(ğ‘Ÿ +ğ‘„ğ‘šğ‘–ğ‘¥(ğ‘  ,ğ‘1 ,â€¦,ğ‘ğ‘ )))
ğ‘¡ ğ‘¡+1 ğ‘¡+1 ğ‘¡+1
(5)
3.3 INCORPORATING INITIAL STATE
1
ğ‘
+ âˆ‘||ğ‘ ğ‘– âˆ’ğ’Ÿ (â„ )||
In some scenarios, information about the initial state is ğ‘ ğ‘¡ ğ‘ğ‘œğ‘ ğ‘  2
ğ‘–=1
known (e.g. initial enemy positions gathered from ISR). +||ğ‘œğ‘– âˆ’ğ‘œÌ‚ğ‘–|| +||ğ‘ğ‘– âˆ’ğ‘Ì‚ğ‘– ||
ğ‘¡ ğ‘¡ ğ‘¡âˆ’1 ğ‘¡âˆ’1
Incorporating a known initial state can significantly 2 2
2
increase the accuracy of predicted COPs because it +ğœ† (1âˆ’ğ»).(ğ»âˆ’ğ» )
â„ ğ‘ğ‘œğ‘
provides ground truth positions especially for enemy units
4 EXPERIMENTS
outside the field-of-view of all friendly units. The COP
In this section, we evaluate the data-driven approach for
model can learn to track the changes in the state by
COP prediction and its impact on multi-agent policy
decoding the friendly actions and projecting the effect on
learning. While ML-driven methods such as ours can be
the state. By using different initial states, we can increase
highly precise, they can be fragile and may deviate from
the diversity of the training set.
the training scenarios or simulations. Therefore, we
When an initial state ğ‘  is provided (it is optional), we use
0 evaluate the resilience of the COP and the policy on
it to initialize the hidden state of the GRU (Figure 4). The
separate test scenarios [12].
initial state is projected to the dimensions of the GRU
hidden state using one linear layer. ğ‘Š is trained end- 4.1 OBSERVATION AND ACTION SPACES
ğ‘–ğ‘›ğ‘–ğ‘¡
to-end alongside the other parts of the model. As mentioned earlier, we use the Starcraft-2 (SC2) [3]
â„ = ğ‘Š .ğ‘  game environment to study Command-and-Control (C2)
0 ğ‘–ğ‘›ğ‘–ğ‘¡ 0
scenarios. We use the Starcraft Multi-Agent Challenge
3.4 HALLUCINATION
(SMAC [1] and SMACv2 [10]) that instruments the
Note that the COP prediction is over a generic set of simulator and provides a multi-agent interface. We use
attributes including position ğ’™ and health attributes â„, the pyMARL software library [1] and build on the
etc., over all the units. The loss function in Eq. (2) and Eq. algorithm implementations therein. To study
(3) gives uniform weightage to all attributes. We add a heterogenous agents, we extend pyMARL and add agent-
term to the training objective Eq. (4) to explicitly penalize specific capabilities:
hallucinations of agents that are present in the COP, but,
â€¢ Sight range: circular field-of-view in pixels.
in fact, not present in the simulation due to zero health.
â€¢ GPS: 2D (X,Y). Set to zeros when GPS is jammed.
2
ğœ† (1âˆ’ğ»).(ğ»âˆ’ğ» ) (4)
â„ ğ‘ğ‘œğ‘
â€¢ Shoot range: in pixels.
where ğ» is the ground truth health (from the state) and
ICCRTS 2024 7Figure 5: Air-Ground Recon ("1O10B-vs-1R") [11]. Figure 6: Withdrawing Attack ("3S-vs-5Z") [1].
In this paper, we work with fixed-size observation and enemy features (location, health, etc.).
state spaces. We assume that the maximum number of
We define a key metric for COP prediction called
agents is known beforehand. When the number of agents
hallucination. Hallucination refers to the prediction that
is lesser than the maximum (due to limited field-of-view,
an agent has non-zero health in the COP when the agent
or dead agents), the corresponding entries in the
has zero health (dead agent) in the ground truth
observation and state vectors are filled with zeros.
(simulation). Hallucination is calculated as the average
Geographical features are not observed. A given agent
error in the predicted health over dead agents.
observes non-zero entries for other agents within its field-
of-view. The real-valued observation vector contains: Finally, we evaluate the success of the policy w.r.t. a
clearly defined win condition. We report the average win
â€¢ Distance (in pixels) from the observing agent to the
rate (over 5000 episodes) for training and test scenarios.
observed agent.
4.3 SCENARIOS
â€¢ Relative coordinates of the observed agent to the
observing agent (egocentric, 2D). Air-Ground Recon: This scenario tests the ability of a
friendly aerial agent to track the movements of an enemy
â€¢ Health, shield, and type of the observed agent.
ground unit and communicate its position in a manner
â€¢ Whether observed agent is within shoot range. that friendly ground forces can decode. Upon decoding,
friendly forces must move to attack the enemy before the
â€¢ Observing agents own health, shield, unit type.
time expires. In the Starcraft-2 (SC2) simulation, we used
Each agent can execute movement actions (in four the scenario â€œ1O10B-vs-1Râ€ (shown in Figure 5 introduced
cardinal directions N, S, E, W by a fixed number of pixels) in [11]). It contains one friendly aerial unit, ten friendly
and attack actions (one action per enemy agent within ground units, and one enemy unit. The enemy must be
shoot range). The state vector contains the following attacked by all ten friendly units to be defeated. The win
information for each agent: condition is to kill the enemy unit before the timer expires.
â€¢ Absolute position (in global coordinates). Withdrawing Attack: In this scenario, the enemy force
outnumbers the friendly force five to three. However, the
â€¢ Health, shield, weapon status, unit type.
friendly agents have a speed advantage while in retreat.
Among these, the coordinates of agents require The friendly units must jointly attack and withdraw in a
transformation between egocentric and global frames. coordinated fashion to evenly distribute the damage from
enemy attacks across the friendly units by performing a
4.2 METRICS
â€œkitingâ€ micromanagement strategy. Precise coordination
We evaluate all these metrics on training and test in this scenario requires precise COPs. In SC2, we used the
scenarios. We evaluate if our method produces a COP that â€œ3S-vs-5Zâ€ scenario [1]. The win condition is to kill all
matches the ground truth in terms of the Mean Squared enemy agents before the timer expires.
Error (MSE). We compute the MSE for (1) the predicted
TigerClaw: The TigerClaw melee map [13] is a high-level
COP and the ground truth state (from the simulator), and
recreation of the TigerClaw combat scenario (Figure 7)
(2) the predicted field-of-view of all agents and the ground
developed using the StarCraft-2 map editor. The scenario
truth field-of-view. For each time step, the MSE is
(â€œTC_5B-vs-6Râ€) was developed by Army subject-matter
averaged over the agents and episode length, and the goal
experts (SMEs) at the Captainâ€™s Career Course, Fort
is to achieve a normalized MSE < 5% over the friendly and
8 ICCRTS 2024missiles (ATGM), and combat infantry. As seen in Figure 7,
the terrain is challenging in this scenario because there
are only four viable wadi crossing points. The win
condition is to kill 80% of the red agents. The red force
uses fixed behavior rules in all our scenarios.
4.4 TEST SCENARIOS
We evaluate our trained policies on modified laydowns
(change in the initial positions of friendly and enemy) in
all scenarios. These Out-of-Distribution (OOD) maps helps
Figure 7: TigerClaw: (Left) Original geographical map. understand the generalization capability of our method.
(Right) Corresponding designed map in StarCraft-2.
OOD maps for Air-Ground Recon ("1O10B-vs-1R") and
Withdrawing Attack (â€œ3S-vs-5Zâ€) are showing in Figure 8
and Figure 9 (for more details refer to [12]). In TigerClaw,
we change the blue force spawning region from defending
all crossings to either the south wadi or further north, as
shown in Figure 10. In training and testing, blue force is
randomly spawned in the corresponding blue region.
4.5 COMPARISON METHODS AND BASELINES
Our method of prediction of COPs is general and can be
learned with a fixed policy, or jointly learned with any
Figure 8: OOD maps for Air-Ground Recon (1O1B-vs-1R). MARL method. In this paper, we integrated COP learning
into the QMIX [4] MARL method. Future work can explore
integration with more recent MARL methods. Note that
QMIX does not use inter-agent communication. We built
two strong baselines on top of QMIX to compare against.
â€¢ QMIX w/ s : A version of QMIX leveraging the initial
0
state knowledge. This is a strong baseline because it
Figure 9: OOD maps for Withdrawing Attack (3S-vs-5Z).
alleviates the issue of partial observability. In this
baseline, each agent takes an additional input, i.e., the
initial state vector, in addition to the agent
observation input.
â€¢ QMIX w/ Cross Attention: A baseline that incorporates
inter-agent communication into QMIX. Each agent
receives an additional input, a message containing the
raw observations of all the agents. The agent
Figure 10: Tigerclaw OOD maps. Blue and red regions architecture is modified to process these observations
indicate the spawning areas for the blue and the red using Cross Attention.
forces, respectively.
We compare to recent multi-agent RL methods that learn
the inter-agent communication function: (1) MASIA [14]
Moore, Georgia. The Blue Force is an Armored Task Force predicts the state from other agentsâ€™ observations within
(TF), which consists of combat armor with M1A2 Abrams, a QMIX method, (2) NDQ [11] and (3) TarMAC [15] are
mechanized infantry with Bradley Fighting Vehicles (BFV), prior work on MARL where learned communication is not
mortar, armored recon cavalry with BFV, and combat grounded on state prediction.
aviation. The Red Force is a Battalion Tactical Group (BTG)
4.6 HYPERPARAMETERS
with attached artillery battery and consists of mechanized
infantry with BMP, mobile artillery, armored recon Training is performed end-to-end using the training
cavalry, combat aviation, anti-armor with anti-tank guided objective in Eq. (4). We perform training in simulation for
ICCRTS 2024 9Figure 12: COP MSE for health and XY prediction across all
scenarios.
prediction separately to evaluate the effect of denied GPS.
Note that XY prediction requires a change in the frame of
reference. In all scenarios, the COP model learns to predict
XY positions (< 0.05, bottom panel) more accurately than
the health attribute (~0.1, top panel).
In 3S-vs-5Z, while the MSE is low, there is a noticeable
increase in COP error over training (blue line), but not in
other scenarios where the COP error decreases
Figure 11: COP MSE over episode (color-coded by different
monotonically. The different dynamics reflect the
playout episodes) for different stages of training. Dots
differences between the scenarios. To succeed in the 3S-
mark the timestep that an agent was killed.
vs-5Z scenario, the policy needs to learn to evade,
withdraw, and attack. The training seems to tradeoff COP
20 million steps. We use the Adam optimizer with learning errors and focus on policy learning. In the other two
rate of 1e-3. Gradients are clipped at a norm of 20. Policy scenarios (green and orange lines), accurate prediction of
updates uses ğœ† = 0.3 and ğ›¾ = 0.99. We set ğ¶ = 32, enemy agentsâ€™ locations is required to win. Hence, we see
ğ‘¡ğ‘‘
(communication dimension) and â„ = 64 (hidden stable and accurate COP XY throughout training.
ğ‘ 
dimension) with four cross-attention heads. By default,
4.7.2 Human Interpretability
hallucination penalty ğœ† = 3. For exploration, we use ğœ–-
â„
A key feature of our method is that the COPs are human-
greedy exploration, annealing ğœ– from 1 to 0.05 over a
scenario-dependent number of steps, typically 105 steps, interpretable. Figure 13 shows a visualization of the state
except in TigerClaw, we use a schedule of 5ğ‘¥105 steps. and corresponding predicted COP for all three scenarios.
In the example from the 3S-vs-5Z scenario (first row, left
4.7 RESULTS
panel), three enemy agents are engaging one friendly unit
4.7.1 Convergence of COP under Denied GPS on the far east side of the map. We see that the friendly
can successfully communicate the positions of enemies
First, we evaluate the model without GPS capability for
(first row, right panel) to friendly agents on the far west
any agent. We track the MSE of the COP against the
side of the map. In the second example (middle row) from
ground truth state. As seen in Figure 11, in the â€œ3s-vs-5zâ€
the 1O10B-vs-1R scenario, we see that the position of the
scenario, the MSE is initially high (top panel), and with
enemy (unit marked â€œEOâ€ in the top left of the map) is
further training, the COP converges to low MSE (bottom
accurately communicated by the aerial unit and
panel), showing that the COP model produces highly
represented with low uncertainty, meaning that the
accurate COPs.
model can leverage heterogeneous agent capabilities. In
Figure 12 shows the convergence of COP in all three the third example (third row) from the TigerClaw scenario,
scenarios. We show the COP MSE for health and XY
we observe that the model can predict friendly positions
10 ICCRTS 2024Figure 14: Average Win Rate across training steps for Our
method and other baselines
Figure 13: (Left) State. Gray regions represent fog; clear
regions represent field-of-view. Lines connecting agents This explains the fast convergence of both the baselines
represent communication. (Right) Human interpretable to winning policies.
predicted COP. Ellipses represent uncertainty and
In comparison with methods that learn to communicate,
consensus: standard deviation in predicted agent position
our COP-based method outperforms TarMAC [15] (orange
over the COPs from all agents.
line), NDQ [11] (green line), and MASIA [14] (red line), in
all three scenarios. In two out of three tested scenarios,
accurately. Still, the enemy positions are not accurate our method is more than an order of magnitude faster to
beyond the visual range. converge. Faster convergence or lower sample complexity
reduces the time and effort required to produce such
To visualize the uncertainty in the COP, we averaged the
multi-agent policies for autonomous platforms. The
predicted XY positions across all agents and showed axis-
results show that our method provides an inductive bias
aligned ellipses with major and minor axes equal to the
based on situational awareness that facilitates learning. In
standard deviation in X and Y coordinates. A small ellipse
the next sections, policies are evaluated for robustness
means low uncertainty and high consensus among agents.
and resilience to variations between training and testing.
4.7.3 COP Improves Convergence Rate of Policy
4.7.4 Resilience to Fog
We trained the agent policies jointly with the COP models.
In this section, we test the trained policies in scenarios
The training results are shown in Figure 14. As expected,
where the agentsâ€™ sight range is reduced. Table 1 shows
QMIX is unable to solve the Air-Ground Recon problem
the results where the sight range of agentsâ€™ is reduced by
(second row) as it does not use communication. The
33%-66% of the training sight range. For example, in the
scenario timer expires before the enemy position is
TigerClaw scenario, the agents use a sight range of 30
discovered. The baseline QMIX w/ Cross Attention directly
pixels for training, i.e., they can observe enemies up to 30
uses the raw observations of all agents, it does not suffer
pixels away.
from the partial observability issue. Similarly, QMIX w/ s
0
has access to the initial state (including enemy position) When we reduce the sight range, we notice that the
which is highly informative in the 1O10B-vs-1R scenario. performance of QMIX and QMIX w/ s 0 (these do not use
ICCRTS 2024 11Table 1: Resilience of Policies trained in conjunction with Table 2: Resilience of Policies trained with GPS, to testing
COP to variation in the sight range of agents. with GPS denial. Performance of policies trained with total
GPS denial is shown in parenthesis as an upper bound.
Method Sight Range
Test GPS Config.
Train Test
Scenario All Partial Total
TC_5B-vs-6R 30 pixels 20 px 10 px
1O10B-vs-1R 0.97 0.68 0.68 (0.94)
QMIX 0.88 0.71 0.43
3s-vs-5z 0.93 0.83 0.7 (0.7)
QMIX w/ s 0.91 0.76 0.53
0
TC_5B-vs-6R 0.87 0.85 0.73 (0.88)
QMIX w/ Cross 0.88 0.74 0.65
Attn.
QMIX+COP 0.9 0.81 0.64 vs-1R), we deny GPS for all ground units, and only the
(Ours) aerial unit has GPS. In the Withdrawing Attack scenario
(3S-vs-5Z), we deny GPS for all agents except one selected
3Svs5Z 9 pixels 6 px 3 px
at random. In the TigerClaw scenario, we deny GPS for all
QMIX 1.0 0.79 0.0 agents except the helicopter and scout platoons. We
report the win rate degradation in Table 2.
QMIX w/ s 1.0 0.57 0.0
0
In the Air-Ground Recon scenario, we see a significant
QMIX w/ Cross 0.99 0.11 0.0
degradation (0.97 vs 0.68 win rate) compared to the upper
Attn.
bound win rate of 0.94. In the Withdrawing Attack
QMIX+COP 1.0 0.94 0.0 scenario, there is degradation in win rate (0.93 down to
(Ours) 0.83 and 0.7). However, the performance matches the
upper bound. That is, our method achieves the same
1O10B-vs-1R 9 pixels 6 px 3 px
performance without retraining the policies without GPS.
QMIX 0.47 0.47 0.43 Overall, in all three scenarios we see that our method can
mitigate the degradation even if one of the agents has GPS
QMIX w/ s 0.99 0.99 0.98
0
(e.g., win rate 0.85 vs 0.73 in TigerClaw).
QMIX w/ Cross 0.98 0.96 0.91
Attn. 4.7.6 Resilience to Change in Enemy Laydown
QMIX+COP 0.99 0.98 0.92 A potential vulnerability of MARL trained policies is when
(Ours) the initial state distribution is different to the training
configuration. Specifically, a change in the initial enemy
positions, aka laydown, can cause degradation in win rate.
communication) drops significantly (0.88/0.91 win rate We tested the trained policies on the OOD scenarios from
down to 0.43/0.53 win rate). In comparison, our method Section 4.4. The results are shown in Table 3.
is significantly more robust (0.9 at 30 pixels vs 0.64 at 10
Among methods that learn to communicate, our method
pixels), meaning the intelligent inter-agent
based on COP outperforms prior methods TarMAC, NDQ,
communication mitigates the degradation in the visual
and MASIA. In the TigerClaw scenario, our method retains
range. We also observe the performance of QMIX w/
the win rate on test laydowns as well. In the 1O10B-vs-1R
Cross Attention at visual range of 20 pixels is lower than
scenario (Test 3), our method is almost twice as effective
our method (0.74 vs 0.81 for our method). This difference
(0.78 vs 0.41 Win-Rate). The average win rate over
is significantly higher in the 3S-vs-5Z scenario (0.11 vs 0.94
scenarios and laydowns for our method is 0.837 vs the
for our method). We see that our method is the most
second-best 0.645 TarMAC.
robust in all three scenarios with variations in sight range.
4.8 MODEL ABLATIONS RESULTS
4.7.5 Resilience to Denied GPS
4.8.1 Hallucination
In this experiment, agents have GPS enabled during
training and are not denied. We compare the As mentioned in Section 3.4, hallucination of agents is an
performance to an upper bound of retraining without issue in our COP model. Figure 15 shows an example of
GPS. We test two scenarios: partial denial of GPS and total hallucination in the TigerClaw scenario. Four red agents
denial of GPS. In the Air-Ground Recon scenario (1O10B- are reported in the COP, including the artillery unit. Out of
12 ICCRTS 2024Table 3: Average win rate (5000 Episodes) on OOD laydowns for different scenarios. Boldface numbers represent best
performance among methods that learn to communicate (excluding baselines in the first group of rows).
Scenario 3S-vs-5Z 1O10B-vs-1R TC_5B-vs-6R
Laydown Train Test1 Test2 Test3 Train Test1 Test2 Test3 Train Test1 Test2
QMIX [4] 1.0 0.87 0.75 0.91 0.47 0.45 0.29 0.4 0.88 0.84 0.73
QMIX w/ s0 1.0 0.11 0.74 0.71 0.99 0.98 0.74 0.85 0.91 0.89 0.79
QMIX w/ Cross-Att. 0.99 0.85 0.94 0.8 0.98 0.96 0.56 0.25 0.88 0.89 0.71
TarMAC [15] 0.92 0.62 0 .93 0.46 0.62 0.6 0.39 0.2 0.82 0.76 0.77
NDQ [11] 0.64 0 . 2 0.4 0.39 0.89 0.85 0.35 0.34 0.76 0.77 0.72
MASIA [14] 0 0 0 0 0.95 0.87 0.47 0.41 0.71 0.73 0.71
QMIX + COP (Ours) 1.0 0.78 0.54 0.77 0.99 0.99 0.68 0.78 0.91 0.91 0.86
Table 4: Effect of three rounds of communication vs one
round of communication on the average win rate.
Win Rate Scenario
Rounds 1O10B-vs-1R 3s-vs-5z TC_5B-vs-6R
1 0.33 0.98 0.85
3 0.97 0.93 0.87
Figure 15: An example COP showing the problem of
hallucination, i.e. units are still tracked for a few steps four, two agents are dead in the simulation, while another
after they are dead in simulation. red agent (this is alive) is not captured in the COP.
In general, we observed from the rollouts that dead
agents are represented in the COP for a few time steps
before they are reflected as dead. Even though this issue
is reduced over training, there are still hallucinations
observed in the trained model. Therefore, we introduced
an explicit hallucination penalty as described in Section
3.4. We found that the increased weightage for health
predictions significantly reduces the hallucinations as
shown in Figure 16. We observed that the hallucination
penalty ğœ† = 3 significantly reduces the hallucination
â„
without reducing the win rate.
4.8.2 Number of rounds of communication
We use multiple communication rounds per simulation
step to refine the COP further and achieve a consensus
among agents. Table 4 shows the average win rate
comparing one vs three rounds of communication. The
table shows that three rounds of communication achieve
a higher win rate, especially in the 1O10B-vs-1R scenario.
Figure 16: Effect of hallucination penalty on hallucination 5 DISCUSSION
over training steps for different scenarios The experiments showed resilience to novel scenarios:
ICCRTS 2024 13GPS denial, communication denial (fog), and unknown created within the training loop.
enemy laydown. The experimental results show that the
In summary, this paper presents a data-driven method for
method produces precise COPs and highly resilient
common operational picture (COP) formation in a multi-
policies. We identified the issue of hallucination and
agent system. The method works with general
introduced a training regularizer to control it.
perceptions from heterogenous platforms in a GPS-
Currently, most COP formation is performed manually in denied environment. The COP is general, including but not
such challenging scenarios by collating communications at limited to unit positions (vs. methods that only estimate
a C2 node. This process is too slow to produce the COP, a positions, such as dead reckoning). The COP formation is
key data product for decision-making processes and fully autonomous, real-time, and human-interpretable.
autonomous policies that run on the platforms. The
ACKNOWLEDGEMENT
manual process is not scalable with the number of
This material is based upon work supported by the Army
platforms and the amount of data to be processed. The
Research Laboratory (ARL) under the Army AI Innovation
paper addresses this barrier to future C2.
Institute (A2I2) program Contract No. W911NF-22-2-
As shown in our experiments, this work also significantly
0137.
advances multi-agent reinforcement learning (MARL).
Existing MARL methods do not work well in our
challenging scenarios. Among MARL methods that learn REFERENCES
to communicate, existing methods suffer from a lack of
[1] M. Samvelyan et al., â€œThe starcraft multi-agent challenge,â€
grounding: it is unknowable what the swarm is
arXiv preprint arXiv:1902.04043, 2019.
communicating, whether it accurately captures an [2] V. G. Goecks et al., â€œOn games and simulators as a platform
evolving scenario, and how a human operator can be for development of artificial intelligence for command and
brought into the loop. This can produce unintended or control,â€ Journal of Defense Modeling & Simulation, vol.
undesired behaviors in new scenarios. 20, no. 4, pp. 495â€“508, Oct. 2023, doi:
10.1177/15485129221083278.
In terms of future directions, multi-task training is a
[3] O. Vinyals et al., â€œStarcraft ii: A new challenge for
straightforward extension to make the COPs and policies reinforcement learning,â€ arXiv preprint arXiv:1708.04782,
even more resilient. We can randomly vary the scenario, 2017.
sight range, communication range, laydown, capabilities, [4] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J.
etc., during training. Future work should explore sparse Foerster, and S. Whiteson, â€œMonotonic value function
communication (sparse in time). We did not vary the factorisation for deep multi-agent reinforcement learning,â€
Journal of Machine Learning Research, vol. 21, no. 178, pp.
bandwidth requirement of communications in this paper
1â€“51, 2020.
and future work must explore this as well.
[5] C. J. Watkins and P. Dayan, â€œQ-learning,â€ Machine learning,
Multi-agent exploration of an unknown dynamic vol. 8, pp. 279â€“292, 1992.
battlefield (e.g., ISR) is challenging as the number of [6] V. Mnih et al., â€œHuman-level control through deep
agents grows. A promising direction for future research is reinforcement learning,â€ nature, vol. 518, no. 7540, pp.
529â€“533, 2015.
to use our COPs for exploration. The COP can help identify
[7] H. Hasselt, â€œDouble Q-learning,â€ Advances in neural
areas of high uncertainty, and methods similar to active
information processing systems, vol. 23, 2010.
sensing can be derived to explore such regions of the
[8] A. Vaswani et al., â€œAttention is all you need,â€ Advances in
battlefield in a coordinated manner.
neural information processing systems, vol. 30, 2017.
A key unsolved modelling challenge is to support a [9] K. Cho et al., â€œLearning phrase representations using RNN
variable number of agents. Future work can consider encoder-decoder for statistical machine translation,â€ arXiv
preprint arXiv:1406.1078, 2014.
sequence-based models to allow messages of arbitrary
[10] B. Ellis et al., â€œSmacv2: An improved benchmark for
length without dependence on the number of agents. In
cooperative multi-agent reinforcement learning,â€
fact, the success of the cross-attention mechanism used
Advances in Neural Information Processing Systems, vol.
in this paper indicates the likelihood of success of such
36, 2024.
models for COP formation.
[11] T. Wang, J. Wang, C. Zheng, and C. Zhang, â€œLearning nearly
decomposable value functions via communication
Another key challenge is the comprehensive evaluation of
minimization,â€ arXiv preprint arXiv:1910.05366, 2019.
COPs and policies. So far, we have manually explored the
[12] P. Yu, B. Lee, A. Raghavan, S. Samarasekera, P. Tokekar, and
space of enemy laydowns to come up with challenging
J. Z. Hare, â€œEnhancing Multi-Agent Coordination through
scenarios. Future work can explore scenario co-design
Common Operating Picture Integration,â€ presented at the
methods (e.g., [16]) where challenging scenarios are First Workshop on Out-of-Distribution Generalization in
14 ICCRTS 2024Robotics at CoRL 2023, 2023.
[13] P. Narayanan et al., â€œFirst-year report of ARL directors
strategic initiative (FY20-23): artificial intelligence (AI) for
command and control (C2) of multi-domain operations
(MDO),â€ US Army Combat Capabilities Development
Command, Army Research Laboratory, 2021.
[14] C. Guan et al., â€œEfficient multi-agent communication via
self-supervised information aggregation,â€ Advances in
Neural Information Processing Systems, vol. 35, pp. 1020â€“
1033, 2022.
[15] A. Das et al., â€œTarmac: Targeted multi-agent
communication,â€ presented at the International
Conference on machine learning, PMLR, 2019, pp. 1538â€“
1546.
[16] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, â€œPaired
open-ended trailblazer (poet): Endlessly generating
increasingly complex and diverse learning environments
and their solutions,â€ arXiv preprint arXiv:1901.01753,
2019.
[17] T. Lin, J. Huh, C. Stauffer, S. N. Lim, and P. Isola, â€œLearning
to ground multi-agent communication with
autoencoders,â€ Advances in Neural Information Processing
Systems, vol. 34, pp. 15230â€“15242, 2021.
ICCRTS 2024 15