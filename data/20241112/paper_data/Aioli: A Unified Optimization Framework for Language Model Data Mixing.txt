Aioli: A unified optimization framework for language model data
mixing
MayeeF.Chen*1 MichaelY.Huâ‹†2 NicholasLourie3 KyunghyunCho2,3,4
ChristopherRÃ©1
1DepartmentofComputerScience,StanfordUniversity
2CenterforDataScience,NewYorkUniversity
3ComputerScienceDepartment,NewYorkUniversity
4PrescientDesign,Genentech
November11,2024
Abstract
Languagemodelperformancedependsonidentifyingtheoptimalmixtureofdatagroupstotrainon(e.g.,law,code,
math).Priorworkhasproposedadiversesetofmethodstoefficientlylearnmixtureproportions,rangingfromfitting
regressionmodelsovertrainingrunstodynamicallyupdatingproportionsthroughouttraining.Surprisingly,wefindthat
noexistingmethodconsistentlyoutperformsasimplestratifiedsamplingbaselineintermsofaveragetestperplexityper
group.Inthispaper,westudythecauseofthisinconsistencybyunifyingexistingmethodsintoastandardoptimization
framework.Weshowthatallmethodssetproportionstominimizetotalloss,subjecttoamethod-specificmixinglawâ€”an
assumptiononhowlossisafunctionofmixtureproportions.Wefindthatexistingparameterizationsofmixinglawscan
expressthetrueloss-proportionrelationshipempirically,butthemethodsthemselvesoftensetthemixinglawparameters
inaccurately,resultinginpoorandinconsistentperformance.Finally,weleveragetheinsightsfromourframeworktoderive
anewonlinemethodnamedAIOLI,whichdirectlyestimatesthemixinglawparametersthroughouttrainingandusesthem
todynamicallyadjustproportions.Empirically,AIOLIoutperformsstratifiedsamplingon6outof6datasetsbyanaverage
of0.28testperplexitypoints,whereasexistingmethodsfailtoconsistentlybeatstratifiedsampling,doingupto6.9points
worse.Moreover,inapracticalsettingwhereproportionsarelearnedonshorterrunsduetocomputationalconstraints,
AIOLIcandynamicallyadjusttheseproportionsoverthefulltrainingrun,consistentlyimprovingperformanceoverexisting
methodsbyupto12.01testperplexitypoints.
1 Introduction
Itiscriticaltodeterminewhatdatatotrainonforalanguagemodel(LM)toacquirearangeofcapabilities,fromgenerating
codetounderstandingscientificliteratureandconversingwithusers[3,34,39].Toachievethis,practitionersmixdatafrom
variousgroups(suchascodefiles,scientificpapers,andchatlogs)inspecificproportionstocomposeanoveralltraining
datasetâ€”aprocedureknownasdatamixing.IdentifyingtheoptimalmixtureproportionsiscriticaltoLLMperformance.
However,abrute-forcetrial-and-errorsearchovertheproportionsiscomputationallyexpensive,requiringmanytraining
runs.
Recentworkintroducestwotypesofdatamixingalgorithmsthatlearnmixtureproportions:offlineandonlinemethods.
Offlinemethodsconductmultipletrainingrunswithvaryingproportions,fitaregressionmodeltopredictperformance,and
usethismodeltodeterminetheoptimalstaticmixture[37,72].Onlinemethodsadjustthemixtureproportionsdynamically
throughouttrainingusinginformationfromthemodel,suchasitslossandgradients[2,13,21,70].Thesemixingmethods
requireatleastonetrainingruntolearntheproportionsbutaremoreefficientthanabrute-forcesearch.
Giventhewiderangeofmethodsavailable,itisimportanttodeterminewhichonesareeffective.However,whenwe
evaluatedexistingmethods,wefoundthatnomethodconsistentlyoutperformedstratifiedsamplingâ€”asimplebaseline
thatuniformlymixesgroupsandrequireszeroextratrainingrunsâ€”acrossallsetsofdatagroupsintermsofaveragetest
perplexity(Table2).Thissurprisingoutcomesuggeststhatallexistingmethodssufferfromsomecommonweaknesses.To
makeprogressindatamixing,weidentifythreeobjectivesforourwork:1)improveourunderstandingoftheunderlying
assumptionsofexistingmethods,2)assessthefidelityoftheseassumptionsinpracticetobetterunderstandperformance,
and3)applyourinsightstodevelopprinciplednewdatamixingmethods.
*Equalcontribution.Contact:mfchen@stanford.edu,michael.hu@nyu.edu
1
4202
voN
8
]GL.sc[
1v53750.1142:viXraUnified Framework:
1 2 Analyzing fidelity of existing methods 3 Our method: Aioli ğŸ§„
Linear Mixing Optimization
m
minimize pâˆˆâ–³TÃ—mâˆ‘LiT+1(pT) Aâ‹†(fitted) A(Ì‚existing)
i=1
lin
s.t. Lt+1(pt)=Ïƒ(Atpt) }
EstimateAAÌ‚ ioli
+
ADÌ‚ oReMi ADÌ‚ oGE â‹¯ ASÌ‚ kill-it
peÌ‚xisting
pAÌ‚ ioli
p*
Proportion of ArXiv Proportion of StackExchange Training steps
Data mixtures p for training language models
Figure1:Left:existingmethodscanbeexpressedinaunifiedoptimizationframework,inwhichtheyimplicitlyassumea
linearorlog-linearloss-proportionrelationship.Center:the(log)-linearparameterizationsarewell-specified,butexisting
methods set their parameters incorrectly. Right: AIOLI, an online mixing method that more accurately estimates the
parametersthatcapturethetrueloss-proportionrelationship.
In this paper, we improve our understanding of data mixing methods by showing that many existing methods can
beexpressedinaunifiedoptimizationframework,whichwecallLinearMixingOptimization(LMO)(Section3).These
methods solve an optimization problem that sets proportions to minimize the average loss per data group, subject to a
method-specificmixinglawâ€”aparticularassumptionrelatinglosspergroupandmixtureweights.Wefindthatallcurrent
mixinglawssharethesameparameterization:fortrainingroundtfrom1toT,
Lt+1(pt)=lin Ïƒ(Atpt),
wherept âˆˆâ–³m (thesimplex)aremixingproportionsovermgivendatagroupsattimet,Lt+1(pt):â–³m â†’(R+)m are
thelossespergroupatthenexttimestep,At âˆˆ RmÃ—m isaparametermatrix,Ïƒ = Idorexp,and=lin meansequalupto
linear transformation. Existing offline methods assume a static log-linear mixing law (T = 1, Ïƒ = exp), while online
methods assume a linear dynamic mixing law (T > 1, Ïƒ = Id). All methods set the parameters of their mixing laws
differently(Table1),andofflinemethodssolvetheoptimizationproblemdirectlywhileonlinemethodssolveitgreedily
usingexponentiatedgradientdescent.Ourframeworkrevealstheunderlyingassumptionsofeachmethodintermsofthe
mixinglawâ€™sparameterization,thevaluesoftheparameters,andhowtheoptimizationproblemissolved.
ApplyingtheLMOframework,wetestthefidelityofexistingmethodsassumptions,examiningiftheyholdinpractice
(Section4).Boththelog-linearstaticandlineardynamicparameterizationscapturethetrueloss-proportionrelationship
acrosspretrainingdatasets,achievinganaverageof0.0005MSEand0.969R2.Wethenshowthatalthoughexistingmixing
lawsarewell-specified,methodscansettheirparameters(At)inaccurately,causingpoorperformance.Wecompareeach
methodâ€™s parameters to the optimal parameters, which we approximate by fitting the mixing laws to training runs. We
findthatthemethodâ€™sparameterscandiffersignificantlyfromtheoptimalparameters,andtheextentofthesedeviations
iscorrelatedwithmethodperformancerelativetostratifiedsampling(Figure3),helpingexplainourinitialobservations.
Finally,wevalidatetheassumptionsmadeinsolvingtheoptimizationproblem,findingthatthegreedyapproximationin
onlinemethodsisareasonableproxyforthefullobjective.Ouranalysisshowsthatexistingmethodsâ€™parameterizationsand
solvingstrategiesareofhighfidelity,buttheirparametersarenot.
To validate these insights, we develop AIOLI, a simple new online data mixing method derived from the LMO
framework(Section5).Similartoexistingonlinemethods, AIOLI usesexponentiatedgradientdescenttominimizethe
averagelosspergroupateachtimestepsubjecttoalineardynamicmixinglaw,whichwehaveempiricallyshowntobe
well-specified.Unlikeexistingonlinemethods,AIOLIdirectlyestimatestheparametersAtfromthecurrenttrainingrunby
fittingthemixinglawonthehistoryoflossesanddynamicmixtureproportionssofar.AIOLIisthusabletodynamically
adjustproportionswithoutrequiringanyextratrainingruns.
WeevaluateAIOLIintwosettingsbytraining160Mparametermodelsonvariouscombinationsofdatasourcesfrom
SlimPajama[53](Section6).First,wecompareAIOLItoexistingdatamixingmethodsandfindthatAIOLIconsistently
outperformsstratifiedsamplingonall6datasets,byanaverageof0.280andupto0.439pointsintestperplexity.Onthe
otherhand,existingdatamixingmethodsdoworsethanstratifiedonatleastonedatasetbyupto6.9perplexitypoints,
despiteusingextratrainingruns.Asweexpect,theparametersofAIOLIarealsomoreconsistentlyclosetotheoptimal
parameters(Figure2).Second,weconsiderascenariowithlimitedadditionalcomputationalresources,inwhichpractitioners
cannotrunexperimentsforlearningmixtureproportionsforthefulltrainingduration.Inthissetting,mixtureproportions
learnedonashorterrunmaynotperformwellonthelongerfinalrun.WefindthatusingAIOLItodynamicallyadjustthese
learnedproportionsthroughoutthefinaltrainingruncanimproveperformancebyanaverageof1.202perplexitypointsin
28outof30cases,comparedtousingthelearnedproportionsdirectly.
2
viXrA
no
ssoL
egnahcxEkcatS
no
ssoL
ssoL
egarevA2 Problem Setup
We formalize the data mixing problem and establish notation. In data mixing, we have m data groups of text, such as
GitHub,BooksCorpus,andarXiv.Wearegiventrain,validation,andtestsetsforeachdatagroup,whichwedenoteas
Di ,Di ,Di fortheithgroup.DefineD ={D1 ,...,Dm },andsimilarlydefineD andD .
train val test train train train val test
Data&Mixing. Duringtraining,weshowthemodelatotalofN examplesfromD overS trainingsteps.Toexpress
train
howdataproportionscanchangethroughouttraining,wedividetrainingintoT equalrounds.Eachroundtusesamixture
proportion from the probability simplex: pt = [pt,...,pt ] âˆˆ â–³m. Static mixtures use only a single round (T = 1):
1 m
p=(cid:0) p1(cid:1)
,whiledynamicmixturesuseseveral(T
>1):p=(cid:0) p1,...,pT(cid:1)
.
Model&Loss. Letf(p,t)refertothelanguagemodel(LM),f,atthebeginningofroundtwherethemodelhasbeen
trainedondatasampledusingmixtureproportionsp1,Â·Â·Â· ,ptâˆ’1sofar.Givenamodelf,wecancomputeitslossoneach
groupusingthetrainingdata,L (f)=(L (f),...,L (f)),andsimilarlyusingthevalidationdata,L (f),and
train train,1 train,m val
testdata,L (f).Inthisnotation,thelossattheendoftrainingcanbeexpressedasL (f(p,T +1)).Whenthef being
test (Â·)
referredtoisobvious,wesimplywriteLt (p),andforstaticmixtureswedropthesuperscript:L (p).
(Â·) (Â·)
DataMixingProblem. Givenasetofdatagroups,anLMf totrainforS stepswithN samples,andT roundsoftraining
(i.e.,determiningwhetherweusestaticordynamicproportions),weaimtodeterminethepthatminimizesthetotaltestloss
acrossgroups:minimize(cid:80)m LT+1(p).
i=1 test,i
pâˆˆâ–³TÃ—m
Thisobjectiveaimstoproduceatrainedmodelthatdoeswellonmanydatagroups,whichcanserveasaproxyfor
downstreamperformance.However,withoutassumingadditionalstructureonLT+1(p),thisproblemcanonlybesolved
test
withabrute-forcesearchoverp,whichrequirestrainingmanydifferentmodels.Existingmethodsovercomethisbyimposing
animplicitconstraintontheproblem,therebysettingpwithoutsearching.Inthenextsection,ourLMOframeworkunifies
manyexistingmethodsusingasingleexplicitconstraint.
3 A Unified Optimization Framework for Data Mixing
WeintroducetheLMOframeworkbystatingthegeneraloptimizationproblem(Section3.1).Then,weexplainhowthis
frameworkcanexpressseveralexistingmethods(Section3.2,3.3),withasummaryofourinsightsregardingthesemethods
inSection3.3.3.
3.1 LinearMixingOptimization(LMO)
TheLMOframeworkconsistsofanoptimizationproblemthatisequivalenttothedatamixingproblem(Section2),subject
toanadditionalconstraint:
m
(cid:88)
minimize LT+1(p) (1)
pâˆˆâ–³TÃ—m val,i
i=1
m
(cid:16)(cid:88) (cid:17)
s.t.Lt+1(p)=ct+btÏƒ âˆ’At pt âˆ€iâˆˆ[m],tâˆˆ[T], (2)
val,i i i ij j
j=1
forsomeAt,bt,ct,andÏƒ.At âˆˆRmÃ—misamatrixthatencodescross-groupinteractions,whereAt intuitivelydescribes
ij
howmuchtrainingongroupj attimpactsgroupiâ€™sloss.bt,ct âˆˆRmaregroup-specificparameters.Ïƒ :Râ†’Riseither
theidentityfunction(Id)ortheexponentialfunction(exp).Werefertotheconstraintin(2)asamixinglawthatspecifiesthe
assumedrelationshipbetweenlossandproportions.
There are three components of this optimization problem that need to be specified to yield a way to set p: a) the
parameterizationofthemixinglaw,definedbyT andÏƒ;b)thevaluesoftheparametersAt,bt,andct;andc)howtosolve
theproblem.WeexpressexistingmethodsinLMObyspecifyingthesecomponents.
3.2 PreliminariesforexpressingmethodsintheLMOframework
We discuss preliminaries before presenting existing methods and explaining how they can be expressed in the LMO
framework.First,weformallydefinewhatitmeansforamethodtobeexpressedintheLMOframework.Then,wepresent
aresultthatallowsustoconvertbetweenlineardynamicmixinglaws(T >1,Ïƒ =Id)andawaytosetp,whichwewillto
usetoexpressonlinemethodsinourframeworkinSection3.3.
3Method 1)MixingLawParameterization 2)Parameters 3)Solver
DML Lval,i(p)=ci+biexp(cid:0)(cid:80)m j=1âˆ’Aijpj(cid:1) Fitfromâ‰¥m+1trainingruns Direct
Skill-It Lt va+ l,1 i(p)=Lt val,i(p)âˆ’bt(cid:80)m j=1At ijpt
j
At
ij
=Lt val,i(p)(LT val+ ,i1(1j)âˆ’L1 val,i(1j))/L1 val,i(1j) EGD
DoReMi Lt va+ l,1 i(p)=Lt val,i(p)âˆ’bt(cid:80)m j=1At ijpt
j
At ii=min{Lt train,i(p)âˆ’Ltrain,i(fref),0} EGD
DoGE Lt+1(p)=Lt (p)âˆ’bt(cid:80)m At pt At =âŸ¨â–½Lt (p),â–½Lt (p)âŸ© EGD
val,i val,i j=1 ij j ij val,i train,j
AIOLI Lt va+ l,1 i(p)=Lt val,i(p)âˆ’(cid:80)m j=1At ijpt
j
FitfromhistoryofLvalandp EGD
Table1:SummaryofhowexistingmethodsandAIOLIareexpressedintheLMOframework.
Definition1. AdatamixingmethodcanbeexpressedintheLMOframeworkifthewayitsetsproportionspandtrains
modelf intermsofpcanbeequivalentlyconstructedbyspecifyingamixinglawparameterization,parameters,andwayof
solvingtheLMOoptimizationproblem.
ThisdefinitionallowsustocastexistingmethodsasawayofsolvingtheLMOoptimizationproblembasedonhowthey
setpandtrainaccordingtop,evenifthemethodsthemselvesarenotoriginallydesignedtominimizeaveragevalidation
loss.
Convertingmixinglawsintoupdaterules. WhenT >1,anaturalwaytosolvetheLMOoptimizationproblemisvia
exponentiatedgradientdescent(EGD)[5,31],whichupdatesptgreedilywhileensuringthatitremainsontheprobability
simplex.ThefollowinglemmapresentstheEGDupdaterulefortheLMOoptimizationproblemwhenÏƒ =Id.
Lemma1. TheEGDupdaterulefor(1)subjecttoLt+1(p)=ctâˆ’bt(cid:80)m At pt âˆ€iâˆˆ[m]is
val,i i i j=1 ij j
(cid:18) m (cid:19)
1 (cid:88)
pt+1 = Â·ptexp Î· btAt âˆ€j âˆˆ[m], (3)
j Zt j i ij
i=1
whereÎ· >0isthestepsizeandZtisanormalizingconstantsuchthatpt+1 âˆˆâ–³m.
j
ThislemmashowshowtoadjustptdynamicallytosolvetheLMOoptimizationproblem.Notably,thisupdateruleis
definedintermsofthemixinglawparameters,Atandbt.Thisgivesusawaytoconvertbetweenhowamethodsetspand
theimplicitassumptionitmakesinitsmixinglaw.
3.3 Existingmethods
WediscussfourexistingdatamixingmethodsandexpressthemasspecificinstancesoftheLMOframework.Asummmary
ofourinsightsisprovidedinSection3.3.3andTable1.InAppendixB.1,wecommentonhowseveralotheronlineand
offlinedatamixingmethodsarerelatedtoourframework,andallproofsforthissectionareinAppendixB.2.
3.3.1 Offlinemethods
DataMixingLaws(DML). Yeetal.[72]proposeanofflinemethodusingastaticmixinglaw(T = 1):L (p) =
val,i
c +b
exp((cid:80)m
âˆ’A p )foriâˆˆ[m],withA,b,clearnedbysweepingtrainingrunsoverstaticproportions(atleastm+1
i i j=1 ij j
runstoavoidbeingunderdetermined).Theirmethodselectstheproportionthatminimizesthepredictedvalidationloss.This
lawcanbederivedfrom(2)withÏƒ =exp,showingthatLMOwitha)log-linearstaticmixinglaw,b)fittedparameters,and
c)directcomputationofpcanexpressDML.
3.3.2 OnlineMethods
We provide a colloquial description and an algorithmic description of three online methods. Then, in Theorem 1, we
demonstratehowtheyallareexpressedinLMOusingalineardynamicmixinglaw,theEGDupdaterule,andmethod-
specificmixinglawparameters.
Skill-It. Chenetal.[13]isanonlinemethodmotivatedbycurriculumlearningthatdynamicallyadjustsmixtureproportions.
Datagroupinteractionsareexpressedinaâ€œskillsgraph,â€whereeachedgedenoteshowmuchthelossononegroupchanges
whentrainedonanother.Theskillsgraphislearnedinadvanceusingmadditionaltrainingrunsandisthenusedtoupdate
proportionsptthroughouttraining.
Concretely,theskillsgraphmatrixASG hasentriesASG = (LT+1(1 )âˆ’L1 (1 ))/L1 (1 )indicatingtherelative
ij val,i j val,i j val,i j
decrease in loss on group i when training a model on group j only. This is used in the Skill-It update rule, pt+1 âˆ
j
4ptexp(Î·(cid:80)m ASGLt (p))forallj âˆˆ [m]andlearningrateÎ· > 0.Thisruledeterminespt+1,whichisthenusedto
j i=1 ij val,i
sampleD fortrainingf inthenextround.
train
DoReMi. Xieetal.[70]isanonlinemethodthatappliesideasfromdistributionallyrobustoptimizationtodatamixing,
wherethetrainingobjectiveminimizestheworst-groupexcesslossoveramodeltrainedwithstratifiedsampling.pt is
updateddynamicallytominimizethisexcesslossandthenaveragedforthefinalrun.DoReMirequirestwoadditionalruns
tolearnastaticp.
Concretely,letf =f(Unif(m),T+1)denoteaâ€œreferencemodelâ€thatisfirsttrainedusingstratifiedsampling.Then,a
ref
â€œproxymodelâ€usesdynamicproportionsaccordingtotheupdaterulept+1 âˆptexp(Î·max{Lt (p)âˆ’L (f ),0})
j j train,j train,j ref
forallj âˆˆ[m]andstepsizeÎ· >0.Thispt+1isusedtoweightthetrainingobjective,suchthattheproxymodelisupdated
tominimize(cid:80)m pt+1L (f)atthenexttimestep.Theaveragedstaticproportions 1 (cid:80)T ptarethenusedinthefinal
i=1 i train,i T t=1
run.
DoGE. Fanetal.[21]isanonlinemethodthatsolvesabi-leveloptimizationprobleminwhichptisupdatedtominimize
theaveragetraininglossateachstep.Byusingafirst-orderTaylorapproximationofthetrainingloss,ptisupdatedusingthe
gradientdotproductsacrossdatagroups.Thedynamicproportionsarethenaveragedforthefinalrun.DoGErequiresone
additionalruntolearnastaticp.
Concretely, a proxy model is trained using pt+1 âˆ ptexp(Î·âŸ¨â–½L (ft),(cid:80)m â–½L (ft)âŸ©), and f is updated to
j j train,j i=1 val,i
minimizethetraininglossweightedbypt,similartoDoReMi.Theaveragedstaticproportions 1 (cid:80)T ptareusedinthe
T t=1
finalrun.
Frameworkexpression. Allthreeonlinemethodsuseanupdaterulept+1 âˆ ptexp(Â·),whichissimilarto(3).This
j j
providesintuitionforourmaintheorem,whichexpressesthesemethodsinLMO.
Theorem1. Definethefollowingparametersforeachmethod:
â€¢ At,Skill-It âˆˆRmÃ—m,whereAt,Skill-It=Lt (p)(LT+1(1 )âˆ’L1 (1 ))/L1 (1 )foralli,j âˆˆ[m],
ij val,i val,i j val,i j val,i j
â€¢ At,DRM âˆˆRmÃ—m,whereAt,DRM =min{Lt (p)âˆ’L (f ),0}andAt,DRM =0foriÌ¸=j,
ii train,i train,i ref ij
â€¢ At,DoGE âˆˆRmÃ—m,whereAt,DoGE =âŸ¨â–½Lt (p),â–½Lt (p)âŸ©foralli,j âˆˆ[m].
ij val,i train,j
Instantiating the LMO framework (1) with a) a linear dynamic mixing law Lt+1(p) = Lt (p)âˆ’bt(cid:80)m At pt, b)
val,i val,i j=1 ij j
parameters At = At,Skill-It/DRM/DoGE, and c) EGD to solve for p allows for us to express Skill-It, DoReMi, and DoGE,
respectively.
3.3.3 SummaryofLMOFrameworkInsights
Table1summarizeshowexistingmethodsareexpressedintheLMOframework.LMOrevealstheassumptionseachmethod
makesthroughhowthecomponentsoftheframeworkarespecified.First,allmixinglawsareeitherlinearorlog-linear.
Second,themixinglawsdifferinthevaluesoftheparametersused.Forexample,Skill-Itâ€™sAtisthecurrentlosstimesa
matrixlearnedfromtrainingonstaticproportions,whileDoReMiâ€™sAtisdiagonal.Third,offlinemixingmethodssolvefor
pdirectlywhileonlinemixingmethodsuseEGD,whichusesagreedyapproximation.Inthenextsection,westudyifeach
oftheseassumptionsholdswellinpractice.
4 Analyzing Fidelity of Existing Methods with the LMO Framework
We examine the fidelity of the assumptions made by existing methods in terms of the three components of the LMO
framework:a)themixinglawparameterization,b)valuesofthemixinglawparameters,andc)howtosolvetheoptimization
problemforp.Afterprovidingexperimentdetails(Section4.1),wediscussthesethreecomponentsinorder(Section4.2-4.4).
4.1 ExperimentDetails
Datasettings. WeuseasampledversionofSlimPajama[53,73],apre-processedversionoftheRedPajamapretraining
dataset [62], which has been used to train open-source LMs [24, 59]. SlimPajama consists of 7 data groups: ArXiv,
Books, CommonCrawl, C4 [50], Github, StackExchange, and Wikipedia. To develop a fine-grained understanding of
data mixing, we create 6 settings by extracting combinations of these groups. We study three settings with m = 2:
Arxiv/StackExchange,Github/C4,andBook/StackExchange.Westudytwosettingswithm=3:Arxiv/Book/StackExchange
andCommonCrawl/Github/Wikipedia.Finally,westudymixingoverthefullSlimPajamadatasetwithm=7.
5Log-linear static mixing law on Arxiv/StackExchange Linear dynamic mixing law on Arxiv/StackExchange
100 3.3
4.0
3.2
3.1 3.8 101
3.0 3.6
101 2.9
3.4
2.8
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Proportion of arxiv Proportion of stackexchange Proportion of arxiv Proportion of stackexchange
Figure 2: Left: p vs log(L (p)âˆ’c ) with fitted static log-linear mixing law. Right: pt vs L (p) with fitted linear
i val,i i i val,i
dynamicmixinglaw.Colorsrepresentrandomseeds(left)andinitialp0 âˆˆP (right,blueis0.7,0.3).Bothlawsfitthetrue
loss-proportionrelationshipwell.
Models. Wetrain160MparameterGPT-styledecoder-onlyLMswithbatchsize8andcontextlength2048.Form=2,3,
wetrainfor5Ksteps,andform=7,wetrainfor40Ksteps.
Trainingsweeps. Toassessthetrueloss-proportionrelationshipandcompareittotheassumptionsmadebyexisting
methods, we conduct training sweeps over a set of various mixture proportions, denoted as P. For m = 2, we set
P ={[0.1,0.9],[0.2,0.8],...,[0.9,0.1]}.Form=3and7,wesetP equalto10pâ€™sdrawnfromtheDirichletdistribution
withÎ±=1.0and1.5,respectively.
4.2 Mixinglawparameterization
Weexaminewhetherexistingmethodsâ€™mixinglawparameterizationsâ€”log-linearstaticandlineardynamicâ€”capturethe
trueloss-proportionrelationship.Byempiricallyfittingthemtoloss-proportionpairs,wefindthatbothparameterizations
areindeedwell-specified.FullresultsforbothmixinglawsareinTable5inAppendixC.1.Wealsodiscussthegenerality
of these parameterizations across training scales and other SlimPajama subsets in Appendix C.1.1, and study if these
parameterizationsholdovermixturesofinstruction-tuningtasksinAppendixC.1.2.
Setup. For the log-linear static mixing law, we study if there exists A,b,c such that L (p) can be expressed as
val,i
c +b
exp((cid:80)m
âˆ’A p )foralli âˆˆ [m].WefittheparametersusingfulltrainingrunsonP.Forthelineardynamic
i i j=1 ij j
mixinglaw,westudyifthereexistsAtsuchthatLt+1(p)canbeexpressedasLt (p)âˆ’(cid:80)m At pt,foralliâˆˆ[m](btis
val,i val,i j=1 ij j
absorbedintoAt).TofitAt,weselectatimesteptandtrainonastaticproportionp0 âˆˆP untilroundt,andatt+1we
sweepthevaluesofpt+1 âˆˆP.
Results. Onaverageacrossour6datasettings,themeansquarederror(MSE)ofthefittedlog-linearstaticmixinglawis
8.9Ã—10âˆ’4,andtheR2coefficientofdeterminationis0.991.TheaverageMSEofthefittedlineardynamicmixinglawis
1.0Ã—10âˆ’4andtheR2is0.947.SeeFigure2forexamples.SincebothparameterizationshavehighR2andlowMSE,we
concludethattheycapturethetrueloss-proportionrelationshipwellandareofhighfidelity.
4.3 Valuesofmixinglawparameters
As shown in Table 1, each method sets the parameters of its mixing law differently. We study how close the method-
specific parameters are to the optimal parameters that are obtained when fitting the methodâ€™s mixing law to the true
loss-proportionrelationship,andiftheseparameterdisparitiesarereflectedinmethodperformance.Wefindthatexisting
methodsâ€™differencesinparametersarelargelyresponsiblefortheirperformance.WeomitstudyingDMLsinceitsparameters
arefittedfromfulltrainingrunsandhencedifferfromtheoptimalparametersinestimationerroronly.
Setup. Foreachonlinemethodâ€”Skill-It,DoReMi,andDoGEâ€”weselectasteptandobtainthemethod-specificAt.
WethensweepP forthenextroundt+1.ThissweepisusedtofitanapproximatelyoptimalAtâ‹† thatcapturesthetrue
loss-mixturerelationship,Lt+1(p)=Lt (p)âˆ’Atâ‹†pt,aswellasfitabt âˆˆRusedforscalingAt(detailsinAppendixC.2).
val val
WestudytherelationshipbetweenAËœt :=btAtandAtâ‹†,andhowthisrelationshipisconnectedtotheperformanceofthe
method.
ToexpresssimilaritybetweenAËœtandAtâ‹†inawaythatisreflectedinperformance,weobservethatfromLemma1,ptis
updatedusingthecolumnsumofAËœt,1âŠ¤AËœt.Moreover,themagnitudeofAËœtisnotcriticaltoperformancesincethestepsizeÎ·
6
vixra
no
)c -
ssoL(
goL
egnahcxekcats
no
)c
- ssoL(
goL
vixra
no
ssoL
pets-txeN
egnahcxekcats
no
ssoL
pets-txeN0.00
0.05
Skill-It
0.10 DoReMi
DoGE
Aioli (ours)
0.15
0.0 0.2 0.4 0.6 0.8 1.0
sim(At,At )
Figure3:ImprovementoverstratifiedsamplingversusoptimalityofAt.Eachdotrepresentsamethodappliedtoadataset.
Theredregionshowsthatexistingmethodsareworsethanstratifiedonatleast1dataset.Theverticaldashedlineservesasa
visualaid.
canalwaysbetunedtocontrolthis.Therefore,wecomparethevectorsaËœt =1âŠ¤AËœt/âˆ¥1âŠ¤AËœtâˆ¥ andatâ‹† =1âŠ¤Atâ‹†/âˆ¥1âŠ¤Atâ‹†âˆ¥ .
2 2
Finally, we note that the order of the elements of aËœt determines the update direction from pt to pt+1 in Lemma 1.
Therefore, we propose a similarity score that is an average of cosine similarity and the Spearman rank correlation,
sim(AËœt,Atâ‹†)=0.5cossim(aËœt,atâ‹†)+0.5Spearman(aËœt,atâ‹†).Thismetricisboundedbetweenâˆ’1and1,where1indicates
aËœt =atâ‹†andâˆ’1indicatesaËœt =âˆ’atâ‹†.
Results. InFigure3,weploteachmethodâ€™ssim(AËœt,Atâ‹†)versuseachmethodâ€™simprovementoverthestratifiedsampling
baseline,whichsetsp =1/mforalliâˆˆ[m],foreachdatasetinthem=2,3datasettings.Wefindthatnoexistingonline
i
methodworkswellacrossalldatasets(alsoseeTable2),andthatourmetricandlossimprovementhaveamoderatepositive
correlation(R2 =0.491).ThissuggeststhatAtâ€™sfidelitytoAtâ‹†clearlyinfluencestheperformanceofonlinemethods,and
thatexistingmethodsâ€™parametersarenotconsistentlyaccurateacrossthedatasets.InAppendixC.2.1,wegivemoredetails
onthepropertiesofAtâ‹†,suchashowAtâ‹†varieswithtandthesignificanceofitsoff-diagonalentries.Wefindthatrestricting
Attobestaticordiagonalâ€”assomeexistingmethodsâ€™parametersarestructuredâ€”canresultinlowersim(AËœt,Atâ‹†).
4.4 Solvingstrategy
WestudytheassumptionsmadeinhowexistingmethodssolvetheLMOoptimizationproblem.Wefindthatthegreedy
approximationusedbyEGD,minimize (cid:80)m Lt+1(p),doesnotsignificantlycompromiseperformancecomparedtofull
pt i=1 val,i
optimizationofdynamicproportions,whichhasanexponentiallylargesolutionspace.Inparticular,westudyifgreedily
selectingptfromP ateachtyieldstheoptimaldynamicproportionsin(P)T,andwefindthatthisholdsin2outof3data
settings(Table10).Thissuggeststhatthegreedyapproximationcansimplifyoptimizationwithoutsubstantialperformance
loss.WealsocommentonotherpossiblesolvingstrategiesinAppendixC.3.
5 AIOLI: a Method for Improved Data Mixing
TovalidateourinsightsfromSection4,wedevelopAIOLI,anonlinemethodderivedfromtheLMOframework.Wehave
threetakeawaysfromsection4:
a) Alineardynamicmixinglaw,Lt+1(p) = Lt (p)âˆ’(cid:80)m At pt foralli âˆˆ [m],cancapturetheloss-proportion
val,i val,i j=1 ij j
relationshipwithhighfidelity(Section4.2).
b) ExistingonlinemethodsoftensettheparametersAttobeverydifferentfromtrueAtâ‹†(Section4.3).
c) Exponentiatedgradientdescentcanrecovernear-optimalperformancewhilesimplifyingtheoptimizationproblem,
avoidinganexponentialsolutionspace(Section4.4).
WethusdirectlyspecifythelineardynamicmixinglawparameterizationandEGDastwooutofthreeLMOcomponents
ofAIOLIsincewefoundthattheirassumptionsgenerallyholdinpractice.AccordingtoLemma1,theupdaterulegiven
thesetwocomponentsispt+1 âˆptexp(Î·(cid:80)m At )(btisabsorbedintoAt).Thus,ourprimarymandateincreatingAIOLI
j j i=1 ij
7
ssol
dohtem
-
ssol
deifitartSistoconstructandutilizeanAtthatisanaccurateestimateofthetrueAtâ‹†inthelineardynamicmixinglaw,whichexisting
onlinemethodsfailtoachieve.
EstimatingAtâ‹† Tobuildintuition,notethatforeachgroupi âˆˆ [m],themvariablesAt ,...,At canbelearnedby
i1 im
solvingasystemofmequations,eachoftheform(cid:80)m At pt =Lt (p)âˆ’Lt+1(p)andwithdifferentp.Thatis,we
j=1 ij j val,i val,i
coulddoatrainingsweepofmdifferentproportionspt,1,pt,2,...,pt,m âˆˆâ–³mandobservehowLt+1 changestoobtain
val,i
At ,...,At .AIOLIusesthisintuitionwhileavoidingtrainingsweeps.InLEARNPARAMS(Algorithm2)atroundt,we
i1 im
definept,i = (1âˆ’Îµ)1 +ÎµUnif(m)foralli âˆˆ [m]tobeaone-hotvectorwithprobabilitymassÎµdistributeduniformly
i
across all groups. We allocate an initial Î´ âˆˆ [0,1] fraction of each round for estimating At (i.e., Î´S/T steps), and we
determinearandomlyinterleavedorderfortrainingonpt,1,...,pt,moverthisduration.Eachtimeafterwetrainaccording
tosomept,i,werecordtheresultingchangesinlosspergroup.Wethenaveragethelosschangesperpt,iacrosstheÎ´S/T
steps,effectivelysimulatingatrainingsweepateachroundwithoutneedingmtimesthecomputeandhavingtorollback
trainingtothebeginningoftheround.Finally,wesolvethelinearsystemofequationstoobtainAt.
AIOLI. Ineachroundoftraining,weestimateAtusingLEARNPARAMSandthennormalizetheentriesofAt,producing
AÂ¯t.Otherwise,Atdecreasesalongwithlossovertime,resultinginthefirstfewptupdatesbeingmuchlargerinmagnitude
thanothers.Weupdatetheproportionsusingpt âˆptâˆ’1exp(Î·(cid:80)m AÂ¯t ),andtrainfortheremainderofthatroundusing
j j i=1 ij
pt.
j
Finally,wedesignAIOLIsothatitcanbeusedtoimproveotherdatamixingmethods,whichwestudyinSection6.2.
MixtureproportionscanbeupdatedusingAIOLIeitherfromthestartoftrainingorfromthemiddleofarun.Inthelatter
case, we denote an initial static mixture pinit âˆˆ â–³m and initial number of steps S init. If S
init
is nonzero, AIOLI trains
accordingtop initforthefirstS initstepsbeforebeginningtoupdatethemixtureproportions.AIOLIispresentedinAlgorithm
1.
Algorithm1AIOLI
1: Input:dataD train,D val,modelf1.InitialstepsS init,initialproportionspinit âˆˆâ–³m.T roundsoverSâˆ’S initremaining
steps,Î´fractionperroundforlearningparameters,learningrateÎ·,one-hotsmoothingfactorÎµ.
2: IfS init Ì¸=0,trainf1onpinitforS initsteps.
3: Setp0 =Unif(m).
4: fort=1,...,T do
5: SetAt,ft+Î´ â†LEARNPARAMS(D train,D val,Î´,ft,Îµ)(Alg.2),andnormalizeAttogetAÂ¯t.
6: pt âˆptâˆ’1exp(Î·(cid:80)m AÂ¯t )forallj âˆˆ[m].
j j i=1 ij
7: Trainmodelft+Î´ with TS(1âˆ’Î´)stepsfrommixtureptoverD train.Obtainupdatedft+1.
Algorithm2LEARNPARAMS
1: Input:D train,D val,Î´,modelft,numberofsweepsk,one-hotsmoothingfactorÎµ.
2: SplitthefractionofatrainingroundÎ´intoK timesegments,whereK =mk.
3: SetÎ² =0 m,m.
4: Definept,i =(1âˆ’Îµ)1 i+ÎµUnif(m)foriâˆˆ[m],anddefineP =[pt,1,...,pt,m]âˆˆâ–³mÃ—m
5: Randomlyshufflekinstancesofeachiâˆˆ[m]tocreateanorderI âˆˆ[m]K overindicesofdatagroups.
6: fork =1,...,K do
7: Letj =I k.Trainmodelonmixturept,j ofD trainforonetimesegment,obtainft+kÎ´/K.
8: foriâˆˆ[m]do
9: UpdateÎ² ij â†Î² ij +L val,i(ft+(kâˆ’1)Î´/K)âˆ’L val,i(ft+kÎ´/K)withlossdifferenceonD vi al.
10: UpdateÎ² â† Î².
k
11: SetAt
i
=Pâˆ’1Î² iforeachiâˆˆ[m].
12: ReturnAt âˆˆRmÃ—m,ft+Î´
6 Experimental Results
We evaluate all methods in the LMO framework, including AIOLI, in two settings. First, we consider an unrestricted
additionaltrainingbudgetsettingtoassesshowAIOLIcomparestoothermethodsintheiroriginalform,sinceeachmethod
usesadifferentnumberofextratrainingrunstolearnproportions(Section6.1).Second,weconsiderarestrictedtraining
8Table2:Differenceinaveragetestperplexitycomparedtostratifiedsamplingintheunrestrictedsetting,whereallmeth-
ods can use â‰¤ 10 extra runs to learn p. Negative values (green) = improvement. A=Arxiv, B=Books, GH=GitHub,
SE=StackExchange,W=Wikipedia.
Method A/SE GH/C4 B/SE A/B/SE CC/GH/W SlimPajama #<stratified #extraruns
Stratified 16.532 35.991 47.192 35.114 41.583 26.426 - 0
GS âˆ’0.399 âˆ’0.407 âˆ’0.645 âˆ’0.247 0.298 0.176 4 10
DML âˆ’0.241 âˆ’0.110 âˆ’0.644 âˆ’0.599 0.242 0.175 4 10
Skill-It âˆ’0.326 0.551 âˆ’0.728 âˆ’0.568 âˆ’0.195 âˆ’0.184 5 m
DoReMi âˆ’0.307 5.303 âˆ’0.217 âˆ’0.393 6.898 0.123 3 2
DoGE 0.419 0.184 âˆ’0.678 1.843 0.604 0.809 1 1
AIOLI âˆ’0.205 âˆ’0.340 âˆ’0.439 âˆ’0.096 âˆ’0.196 âˆ’0.240 6 0
budgetsettingtoassessifAIOLIcanenhanceexistingmethodsinpractical,budget-constrainedconditions,whereexisting
methodshavelessthanafulltrainingruntolearnmixingproportions(Section6.2).Hyperparametersandexperimental
detailsareavailableinAppendixD,andablationsanddownstreamevaluationareinAppendixE.
Datasettingsandmodels. WeusethesamedatasettingsandmodelsasinSection4.1,wherewetrainforS =5Ksteps
form=2,3-groupsettingsandS =40KstepsforthefullSlimPajama.
Baselinesandevaluation. Weconsiderthreeonlinemethods(Skill-It,DoGE,DoReMi)andoneofflinemethod(DML).
Wealsoconsidergridsearch(GS),whichsweepstrainingrunsandselectspwiththelowestaveragevalidationloss,and
stratifiedsampling,whichsetsp = 1 foralliâˆˆ[m].Foreachmethod,wereporttheaveragetestperplexitypergroupof
i m
thetrainedmodel.Thismetricisconsideredaproxyfordownstreamperformance[21]andalsorepresentstheobjectivein
thedatamixingproblempresentedinSection2.
6.1 UnrestrictedSetting
Setup. Weallowmethodsupto10Sadditionaltrainingstepstolearnthemixtureproportions.Approacheslikegridsearch
andDMLcanusetheentirebudget(searchingandfittingover10fullruns),whileSkill-It,DoReMi,andDoGEusemS,2S,
andS extratrainingsteps,respectively(seeSection3.3).StratifiedsamplingandAIOLIusenoextratrainingsteps.We
evaluateAIOLIwithS
init
=0.
Results. InTable2,wefindthatAIOLIrobustlyoutperformsstratifiedsamplinginall6datasettingsbyanaverageof
0.280perplexitypoints,whileallothermethodsdoworsethanstratifiedsamplingonatleast1setofdatagroupsbyup
to6.9points.TheperformanceofAIOLIandotheronlinemethodsisadditionallyreflectedinFigure3,inwhichwefind
thatAIOLIâ€™sAtsimilaritywithAtâ‹†iscorrelatedwithperformance.WhileAIOLIâ€™sparametersimilarityisnotalwaysthe
highest,wenotethatitslowestsimilarityscoreismuchhigherthanthatofothermethods,providingevidencethatAIOLIâ€™s
parameterestimationprocedureismoreconsistentlyaccuratethanthatofothermethods.Lastly,regardingofflinemethods,
wehypothesizethattheirpoorperformanceonsettingswithlargermisduetothetrainingbudgetbeinglimitedto10S,and
thatincreasingthisbudgetwouldeventuallyallowthemtoperformwell.
6.2 RestrictedSetting
Motivation. Weintroducetherestrictedsettingbecausepractitionersmaynothavetheresourcesordesiretocomplete
multiplefulltrainingrunstolearnproportions,especiallyasrecentLLMsaretrainedforlongerandonmoredata[44].As
aresult,practitionersmayonlyusedatamixingmethodsonshortenedruns,producinglearnedproportionsthatmaybe
suboptimalonthefullrun.WestudyifAIOLIisabletoimproveperformancebydynamicallyadjustingpreviouslylearned
proportionsthroughoutthefulltrainingrun.
Setup. Weallowallexistingmethodsonly0.5S additionaltrainingstepstolearnthemixtureproportions.Thisrequires
methodstolearnpmethod overshorterrunsofS stepseach.Forinstance,gridsearchwillconduct10runsoflength
method
S/20(seeTable11).Weevaluateeachmethodbyusingpmethodlearnedfromshorterrunstotrainthemodelonthefullrun
ofS steps.WeuseAIOLItodynamicallyadjusteachpmethodthroughoutthefullrun.Thatis,foreachexistingmethod,we
runAIOLIwithpinit =pmethodandS
init
=S method,referringtothisasAIOLI+method.
Results. InTable3,wefindthataddingAIOLItoanyexistingmethodthatlearnsproportionsovershorterrunsimproves
average test perplexity per group in 28 out of 30 settings, by an average of 1.202 and a maximum of 12.012 points.
9Table3:Averagetestperplexityintherestrictedsetting,whereeachmethodlearnsponshortenedruns,andAIOLI+method
dynamicallyadjustspthroughouttraining.green=AIOLI+methodoutperformsmethod.
Method Arxiv/SE GH/C4 Books/SE Arxiv/Books/SE CC/GH/Wiki SlimPajama
GS 16.573 36.345 47.063 35.174 42.767 27.741
AIOLI+GS 16.388 35.925 46.667 34.705 41.378 25.654
DML 16.659 36.658 46.846 34.585 42.731 37.696
AIOLI+DML 16.277 35.856 46.710 34.529 41.595 25.654
Skill-it 16.246 37.255 46.667 34.539 42.069 26.734
AIOLI+Skill-it 16.261 36.153 46.586 34.565 41.732 26.073
DoReMi 16.522 37.812 46.489 34.934 42.738 28.762
AIOLI+DoReMi 16.347 35.626 46.163 34.770 41.800 26.587
DoGE 16.853 35.795 46.743 35.775 41.790 32.301
AIOLI+DoGE 16.473 35.632 46.145 34.771 41.378 26.073
Furthermore, AIOLI can help methods that initially underperform stratified sampling surpass it, such as DoGE across
allsettings.Insomesettings,suchasBooks/StackExchange,AIOLIimprovesmethodsthatalreadyoutperformstratified
sampling.ThisshowsthatAIOLIcanenhanceawidevarietyofmixtureproportions,regardlessoftheirinitialperformance.
ForthetwocaseswhereAIOLIunderperformsthebasemethod,thebasemethodalreadyoutperformsstratified,andadding
AIOLImaintainsthistrend,worseningperplexitybyatmost0.025points.
7 Related Work
Datamixing. Beyondthedatamixingmethodsexploredinourframework,Albalaketal.[2]framesonlinedatamixing
asamulti-armedbanditproblemwithlossastherewardfunction.Inconcurrentwork,Jiangetal.[28]alsosetdatamixtures
online and adaptively by using a credit assignment score that predicts how data from each domain affects loss on that
domain.Inourlanguage,Jiangetal.[28]useadiagonalAtmatrix,andthevaluesonthediagonalaredefinedbytheircredit
assignmentfunctionandtheper-grouplosses.Recentworkshavealsostudiedhowtomixdataonsmallermodelsanduse
theselearnedproportionsonlargermodels[23,30,37].Inasimilarvein,Naetal.[45]showthatonecansimulateamodel
trainedonaparticulardatamixturebyaveragingtogethermodelstrainedondifferent(possiblydisjoint)partitionsofdata
groups.Thrushetal.[60]mixesdatatooptimizeperformanceondownstreamtasks,constructinganAt-likeinteraction
matrixbyusingpretrainedmodelperplexities.
CurriculumLearning. Bengioetal.[6]initiallyintroducedcurriculumlearningastrainingmodelsoversamplesfrom
easiesttohardest.Whileearlyworkfocusedonmanuallydesignedcurricula,laterworkemphasizesmodel-drivenones
[20, 26, 41, 65]. Curricula can encourage skills-based generalization [27], or emphasize high quality data to improve
downstreamtaskperformance[10].Onlinemixingmethodscanbealsoviewedascurriculumlearningoverdatagroups.
DataSelection. Acommonwaytocuratedatasetsbesidesmixingistoselectdataattheper-samplelevel[3].Techniques
herecanbebroadlyclassifiedasdatafiltering,datamatching,anddatacondensation.Indatafiltering,low-qualitysamples
areremovedusingsimpleheuristics,suchasGitHubfilelengths[62,64],orviadeduplication[1,32,61].Indatamatching,
samplesthataremostsimilartoareferencedatasetareselected.Similaritycanbedefinedintermsofembeddings[71],
gradients[19,69],ordirectlyusingmachinelearningmodelstoscoresamples[11,25,43].Lastly,datacondensationaimsto
identifyasubsetofsamplesthatcapturesthefulltrainingdatasetâ€™sproperties.Selectionmechanismsincludeusinggradients,
modelpredictions,andembeddingdistances[49,55,63].
HyperparameterOptimizationandTruncationBias. Manydatamixingmethodsutilizeextratrainingrunstolearnthe
staticmixtureproportionsbeforethefinaltrainingrun.Thisallowsustoviewdatamixingasahyperparameteroptimization
probleminp.[72]and[37]mitigatetheinefficiencyofgridsearchinhigherdimensionsbycombiningitwithdatamixing
lawstoimposeadditionalstructure.However,bothgridsearchandtheseofflinemethodscanhavepoorperformancewhenp
issearchedfororfittedonshorterruns,asintherestrictedsetting.Tounderstandtheseresults,wenotethatmanypopular
hyperparameteroptimizationmethodscarefullycontroltruncation,andsomerunsareallowedtocontinuelongerthanothers
[18,35,56].Thus,generichyperparameteroptimizationmethodsmayeventuallyproveeffectivefortuningdatamixes.
108 Discussion
Weproposeanoptimizationframework,LMO,thatcanexpressseveralexistingdatamixingmethodsusingtheirimplicit
mixinglaw.UsingLMO,weshowthatlineardynamicmixinglawsarewell-specifiedincapturingthetrueloss-proportion
relationshipacrossdatasets,butexistingonlinemethodsstillperformpoorlyonsomedatasetsbecausetheirparametersare
inaccurate.ThisinsightinspiresAIOLI,whoseperformancegainsarerootedinitsabilitytoestimateparametersAtofthe
lineardynamicmixinglawthroughouttraining.
LimitationsandFutureWork. AIOLI introducesextrainferencecostduringtrainingviatherepeatedevaluationsin
LEARNPARAMS(Alg.2).Thisoverheadcanbereducedorcompletelymitigatedbyusingthetraininglossasaproxyforthe
validationloss,computingL overasubsetofD ,orbyusingeachAtforlonger(equivalently,decreasingT).
val val
TheLMOframeworkitselfisaninvitationforfuturework.WehopethatLMOidentifiesclearaxesofimprovementfor
datamixing(theparameterizationofthemixinglaw,parameterestimation,andhowtosolveforp)andinspiresthenext
generationofprincipleddatamixingmethods.Anotherdirectionforfutureworkisunderstandinghowtodefinedatagroups
andhowdatagrouppartitionsimpactdatamixing.Forexample,C4isasubsetofCommonCrawl,sorepartitioningthese
datasetsintomoredisjointgroupscouldresultinbetterdatamixingperformance.Furthermore,itisuncleariflinearmixing
lawsresultsfromsomespecificpropertyofthedatagroupswestudied.Itwouldbeinterestingtoconsiderinfuturework
howoftenlinearmixinglawshold,andhowtoexploitpotentialnon-linearitiesindatamixing.
8.1 ReproducibilityStatement
See Appendix B.2 for the full proofs on how to express Skill-it, DoReMi, and DoGE using the LMO framework. See
AppendixCfordetailsonhowtoreproduceouranalysesofmixinglawparametrizationvalidity,At parameterfit,and
assessingwhethergreedyoptimizationissufficientfordatamixing.Finally,toreproducetheexperimentalresults,pleasesee
AppendixD.
Coderelease. Codeforreproducingourresultsisavailableathttps://github.com/HazyResearch/aioli.
8.2 EthicsStatement
Ourworkfocusesonimprovingtheefficiencyandperformanceoflanguagemodeltraining.Whileourresearchdoesnot
directlyaddressethicalconcerns,itcancontributetomoreresponsibleAIdevelopmentbyoptimizingtraining,whichcan
reducecomputationalcostsandenergyconsumption.
9 Acknowledgments
WethankSabriEyuboglu,NeelGuha,BenViggiano,DanBiderman,DanFu,MichaelWornow,JonSaad-Falcon,Alyssa
Unell,OwenDugan,JerryLiu,andGautamMachirajufortheirfeedback.WethankNYUHPCandStanfordNLPfor
providingcomputeandresearchsupport.ThisresearchprojecthasbenefitedfromtheMicrosoftAccelerateFoundation
ModelsResearch(AFMR)grantprogram.
WegratefullyacknowledgethesupportofNIHunderNo.U54EB020405(Mobilize),NSFunderNos.CCF2247015
(Hardware-Aware),CCF1763315(BeyondSparsity),CCF1563078(VolumetoVelocity),1937301(RTML),and1922658
(NRT-HDR:FUTURE);USDEVCOMARLunderNos.W911NF-23-2-0184(Long-context)andW911NF-21-2-0251
(InteractiveHuman-AITeaming);ONRunderNos.N000142312633(DeepSignalProcessing);StanfordHAIunderNo.
247183;NXP,Xilinx,LETI-CEA,Intel,IBM,Microsoft,NEC,Toshiba,TSMC,ARM,Hitachi,BASF,Accenture,Ericsson,
Qualcomm,AnalogDevices,GoogleCloud,Salesforce,Total,theHAI-GCPCloudCreditsforResearchprogram,the
StanfordDataScienceInitiative(SDSI),theSamsungAdvancedInstituteofTechnology(undertheprojectNextGeneration
DeepLearning:FromPatternRecognitiontoAI),theNSFGraduateResearchFellowship(MYH),andmembersofthe
StanfordDAWNproject:Meta,Google,andVMWare.TheU.S.Governmentisauthorizedtoreproduceanddistribute
reprintsforGovernmentalpurposesnotwithstandinganycopyrightnotationthereon.Anyopinions,findings,andconclusions
orrecommendationsexpressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreflecttheviews,policies,or
endorsements,eitherexpressedorimplied,ofNIH,ONR,ortheU.S.Government.
11References
[1] AmroAbbas,KushalTirumala,DÃ¡nielSimig,SuryaGanguli,andAriSMorcos. Semdedup:Data-efficientlearningat
web-scalethroughsemanticdeduplication. arXivpreprintarXiv:2303.09540,2023.
[2] AlonAlbalak,LiangmingPan,ColinRaffel,andWilliamYangWang. Efficientonlinedatamixingforlanguagemodel
pre-training,2023. URLhttps://arxiv.org/abs/2312.02406.
[3] AlonAlbalak,YanaiElazar,SangMichaelXie,ShayneLongpre,NathanLambert,XinyiWang,NiklasMuennighoff,
BairuHou,LiangmingPan,HaewonJeong,ColinRaffel,ShiyuChang,TatsunoriHashimoto,andWilliamYangWang.
Asurveyondataselectionforlanguagemodels. arXivpreprintarXiv:2402.16827,2024. https://arxiv.org/
abs/2402.16827.
[4] AidaAmini,SaadiaGabriel,ShanchuanLin,RikKoncel-Kedziorski,YejinChoi,andHannanehHajishirzi. Mathqa:
Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019
Conference of the North, page 2357â€“2367. Association for Computational Linguistics, 2019. doi: 10.18653/v1/
n19-1245. URLhttp://dx.doi.org/10.18653/v1/N19-1245.
[5] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and
applications. Theoryofcomputing,8(1):121â€“164,2012.
[6] YoshuaBengio,JÃ©rÃ´meLouradour,RonanCollobert,andJasonWeston. Curriculumlearning. InProceedingsofthe
26thAnnualInternationalConferenceonMachineLearning,ICMLâ€™09,page41â€“48,NewYork,NY,USA,2009.
Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380. URL https:
//doi.org/10.1145/1553374.1553380.
[7] ChandraBhagavatula,RonanLeBras,ChaitanyaMalaviya,KeisukeSakaguchi,AriHoltzman,HannahRashkin,Doug
Downey,ScottWentauYih,andYejinChoi. Abductivecommonsensereasoning,2019.
[8] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle Oâ€™Brien, Eric Hallahan,
MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal. Pythia:Asuiteforanalyzing
largelanguagemodelsacrosstrainingandscaling.InInternationalConferenceonMachineLearning,pages2397â€“2430.
PMLR,2023.
[9] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical
commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. URL https://api.
semanticscholar.org/CorpusID:208290939.
[10] Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle. Does your data spark joy?
performancegainsfromdomainupsamplingattheendoftraining. InFirstConferenceonLanguageModeling,2024.
URLhttps://openreview.net/forum?id=vwIIAot0ff.
[11] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,
RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,Eric
Sigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,
IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020. URLhttps://arxiv.org/
abs/2005.14165.
[12] AngelicaChen,SadhikaMalladi,LilyH.Zhang,XinyiChen,QiuyiZhang,RajeshRanganath,andKyunghyunCho.
Preferencelearningalgorithmsdonotlearnpreferencerankings,2024. URLhttps://arxiv.org/abs/2405.
19534.
[13] MayeeChen,NicholasRoberts,KushBhatia,JueWANG,CeZhang,FredericSala,andChristopherRÃ©. Skill-it!a
data-drivenskillsframeworkforunderstandingandtraininglanguagemodels. InA.Oh,T.Naumann,A.Globerson,
K.Saenko,M.Hardt,andS.Levine,editors,AdvancesinNeuralInformationProcessingSystems,volume36,pages
36000â€“36040.CurranAssociates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/
paper/2023/file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf.
[14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,
YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:Anopen-sourcechatbotimpressinggpt-4
with90%*chatgptquality,March2023. URLhttps://lmsys.org/blog/2023-03-30-vicuna/.
12[15] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,andKristinaToutanova. BoolQ:
Exploringthesurprisingdifficultyofnaturalyes/noquestions. InJillBurstein,ChristyDoran,andThamarSolorio,
editors,Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924â€“2936, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https:
//aclanthology.org/N19-1300.
[16] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,andOyvindTafjord.
Thinkyouhavesolvedquestionanswering?tryarc,theAI2reasoningchallenge. CoRR,abs/1803.05457,2018. URL
http://arxiv.org/abs/1803.05457.
[17] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRÃ©. Flashattention:Fastandmemory-efficientexact
attentionwithio-awareness. AdvancesinNeuralInformationProcessingSystems,35:16344â€“16359,2022.
[18] TobiasDomhan,JostTobiasSpringenberg,andFrankHutter. Speedingupautomatichyperparameteroptimizationof
deepneuralnetworksbyextrapolationoflearningcurves. InTwenty-fourthinternationaljointconferenceonartificial
intelligence,2015.
[19] LoganEngstrom,AxelFeldmann,andAleksanderMadry. Dsdm:Model-awaredatasetselectionwithdatamodels. In
Forty-firstInternationalConferenceonMachineLearning,2024. URLhttps://openreview.net/forum?
id=GC8HkKeH8s.
[20] SiminFanandMartinJaggi. Irreduciblecurriculumforlanguagemodelpretraining. arXivpreprintarXiv:2310.15389,
2023.
[21] SiminFan,MatteoPagliardini,andMartinJaggi. Doge:Domainreweightingwithgeneralizationestimation,2024.
URLhttps://arxiv.org/abs/2310.15393.
[22] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noacâ€™h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason
Phang,LariaReynolds,HaileySchoelkopf,AviyaSkowron,LintangSutawika,EricTang,AnishThite,BenWang,
Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https:
//zenodo.org/records/12608602.
[23] CeGe,ZhijianMa,DaoyuanChen,YaliangLi,andBolinDing. Datamixingmadeefficient:Abivariatescalinglaw
forlanguagemodelpretraining,2024. URLhttps://arxiv.org/abs/2405.14908.
[24] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.
com/openlm-research/open_llama.
[25] EdouardGrave,PiotrBojanowski,PrakharGupta,ArmandJoulin,andTomasMikolov. Learningwordvectorsfor157
languages. InNicolettaCalzolari,KhalidChoukri,ChristopherCieri,ThierryDeclerck,SaraGoggi,KoitiHasida,
Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HÃ©lÃ¨ne Mazo, Asuncion Moreno, Jan Odijk, Stelios Piperidis,
andTakenobuTokunaga,editors,ProceedingsoftheEleventhInternationalConferenceonLanguageResourcesand
Evaluation(LREC2018),Miyazaki,Japan,May2018.EuropeanLanguageResourcesAssociation(ELRA). URL
https://aclanthology.org/L18-1550.
[26] GuyHacohenandDaphnaWeinshall. Onthepowerofcurriculumlearningintrainingdeepnetworks. InInterna-
tional Conference on Machine Learning, 2019. URL https://api.semanticscholar.org/CorpusID:
102350936.
[27] YunchengHuang,QianyuHe,YipeiXu,JiaqingLiang,andYanghuaXiao. Layingthefoundationfirst?investigating
thegeneralizationfromatomicskillstocomplexreasoningtasks,2024. URLhttps://arxiv.org/abs/2403.
09479.
[28] YidingJiang,AllanZhou,ZhiliFeng,SadhikaMalladi,andJ.ZicoKolter. Adaptivedataoptimization:Dynamic
sampleselectionwithscalinglaws,2024. URLhttps://arxiv.org/abs/2410.11820.
[29] ShamKakade. Lecture22:Exponentiatedgradientdescent. https://homes.cs.washington.edu/~sham/
courses/stat928/lectures/lecture22.pdf,n.d. Accessed:September29,2024.
[30] FeiyangKang,YifanSun,BingbingWen,SiChen,DawnSong,RafidMahmood,andRuoxiJia. Autoscale:Automatic
predictionofcompute-optimaldatacompositionfortrainingllms,2024. URLhttps://arxiv.org/abs/2407.
20177.
13[31] Jyrki Kivinen and Manfred K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.
InformationandComputation,132(1):1â€“63,1997. ISSN0890-5401. doi:https://doi.org/10.1006/inco.1996.2612.
URLhttps://www.sciencedirect.com/science/article/pii/S0890540196926127.
[32] KatherineLee,DaphneIppolito,AndrewNystrom,ChiyuanZhang,DouglasEck,ChrisCallison-Burch,andNicholas
Carlini. Deduplicating training data makes language models better, 2022. URL https://arxiv.org/abs/
2107.06499.
[33] MoshLevy,AlonJacoby,andYoavGoldberg. Sametask,moretokens:theimpactofinputlengthonthereasoning
performanceoflargelanguagemodels.InLun-WeiKu,AndreMartins,andVivekSrikumar,editors,Proceedingsofthe
62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages15339â€“15353,
Bangkok,Thailand,August2024.AssociationforComputationalLinguistics. doi:10.18653/v1/2024.acl-long.818.
URLhttps://aclanthology.org/2024.acl-long.818.
[34] JeffreyLi,AlexFang,GeorgiosSmyrnis,MaorIvgi,MattJordan,SamirGadre,HritikBansal,EtashGuha,Sedrick
Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen,
SuchinGururangan,MitchellWortsman,AlonAlbalak,YonatanBitton,MariannaNezhurina,AmroAbbas,Cheng-
Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal,
GabrielIlharco,GiannisDaras,KalyaniMarathe,AaronGokaslan,JieyuZhang,KhyathiChandu,ThaoNguyen,
IgorVasiljevic,ShamKakade,ShuranSong,SujaySanghavi,FartashFaghri,SewoongOh,LukeZettlemoyer,Kyle
Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini,
PangWeiKoh,JeniaJitsev,ThomasKollar,AlexandrosG.Dimakis,YairCarmon,AchalDave,LudwigSchmidt,and
VaishaalShankar. Datacomp-lm:Insearchofthenextgenerationoftrainingsetsforlanguagemodels,2024. URL
https://arxiv.org/abs/2406.11794.
[35] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel
bandit-basedapproachtohyperparameteroptimization. JournalofMachineLearningResearch,18(185):1â€“52,2018.
[36] HongLiu,SangMichaelXie,ZhiyuanLi,andTengyuMa. Samepre-trainingloss,betterdownstream:Implicitbias
matters for language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan
Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning,
volume 202 of Proceedings of Machine Learning Research, pages 22188â€“22214. PMLR, 23â€“29 Jul 2023. URL
https://proceedings.mlr.press/v202/liu23ao.html.
[37] QianLiu,XiaosenZheng,NiklasMuennighoff,GuangtaoZeng,LongxuDou,TianyuPang,JingJiang,andMinLin.
Regmix:Datamixtureasregressionforlanguagemodelpre-training,2024. URLhttps://arxiv.org/abs/
2407.01492.
[38] ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,QuocV.Le,BarretZoph,
JasonWei,andAdamRoberts. Theflancollection:Designingdataandmethodsforeffectiveinstructiontuning,2023.
[39] ShayneLongpre,GregoryYauney,EmilyReif,KatherineLee,AdamRoberts,BarretZoph,DennyZhou,JasonWei,
KevinRobinson,DavidMimno,andDaphneIppolito. Apretrainerâ€™sguidetotrainingdata:Measuringtheeffectsof
dataage,domaincoverage,quality,&toxicity. InKevinDuh,HelenaGomez,andStevenBethard,editors,Proceedings
ofthe2024ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human
LanguageTechnologies(Volume1:LongPapers),pages3245â€“3276,MexicoCity,Mexico,June2024.Associationfor
ComputationalLinguistics. doi:10.18653/v1/2024.naacl-long.179. URLhttps://aclanthology.org/2024.
naacl-long.179.
[40] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconductelectricity?anew
datasetforopenbookquestionanswering. InEllenRiloff,DavidChiang,JuliaHockenmaier,andJunâ€™ichiTsujii,
editors,Proceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2381â€“2391,
Brussels,Belgium,October-November2018.AssociationforComputationalLinguistics. doi:10.18653/v1/D18-1260.
URLhttps://aclanthology.org/D18-1260.
[41] SÃ¶renMindermann,JanMBrauner,MuhammedTRazzak,MrinankSharma,AndreasKirsch,WinnieXu,Benedikt
HÃ¶ltgen,AidanNGomez,AdrienMorisot,SebastianFarquhar,etal. Prioritizedtrainingonpointsthatarelearnable,
worthlearning,andnotyetlearnt. InInternationalConferenceonMachineLearning,pages15630â€“15649.PMLR,
2022.
[42] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural
languagecrowdsourcinginstructions. InACL,2022.
14[43] RobertC.MooreandWilliamLewis. Intelligentselectionoflanguagemodeltrainingdata. InJanHajicË‡,Sandra
Carberry,StephenClark,andJoakimNivre,editors,ProceedingsoftheACL2010ConferenceShortPapers,pages220â€“
224,Uppsala,Sweden,July2010.AssociationforComputationalLinguistics. URLhttps://aclanthology.
org/P10-2041.
[44] NiklasMuennighoff,AlexanderMRush,BoazBarak,TevenLeScao,NouamaneTazi,AleksandraPiktus,Sampo
Pyysalo,ThomasWolf,andColinRaffel. Scalingdata-constrainedlanguagemodels. InThirty-seventhConferenceon
NeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=j5BuTrEj35.
[45] ClaraNa,IanMagnusson,AnanyaHarshJha,TomSherborne,EmmaStrubell,JesseDodge,andPradeepDasigi.
Scalable data ablation approximations for language models through modular training and merging, 2024. URL
https://arxiv.org/abs/2410.15661.
[46] AvanikaNarayan,MayeeF.Chen,KushBhatia,andChristopherRÃ©. Cookbook:Aframeworkforimprovingllm
generativeabilitiesviaprogrammaticdatageneratingtemplates,2024.
[47] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Donâ€™t give me the details, just the summary! Topic-aware
convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical
MethodsinNaturalLanguageProcessing,Brussels,Belgium,2018.
[48] DenisPaperno,GermÃ¡nKruszewski,AngelikiLazaridou,QuanNgocPham,RaffaellaBernardi,SandroPezzelle,
MarcoBaroni,GemmaBoleda,andRaquelFernÃ¡ndez. TheLAMBADAdataset:Wordpredictionrequiringabroad
discoursecontext. CoRR,abs/1606.06031,2016. URLhttp://arxiv.org/abs/1606.06031.
[49] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Finding im-
portant examples early in training. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 20596â€“20607. Curran
Associates,Inc.,2021. URLhttps://proceedings.neurips.cc/paper_files/paper/2021/file/
ac56f8fe9eea3e4a365f29f0f1957c55-Paper.pdf.
[50] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
PeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. arXive-prints,2019.
[51] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine
comprehensionoftext. InJianSu,KevinDuh,andXavierCarreras,editors,Proceedingsofthe2016Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages2383â€“2392,Austin,Texas,November2016.Association
forComputationalLinguistics. doi:10.18653/v1/D16-1264. URLhttps://aclanthology.org/D16-1264.
[52] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande:Anadversarialwinograd
schemachallengeatscale.ProceedingsoftheAAAIConferenceonArtificialIntelligence,34(05):8732â€“8740,Apr.2020.
doi:10.1609/aaai.v34i05.6399. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/6399.
[53] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPa-
jama:A627BtokencleanedanddeduplicatedversionofRedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama,June2023.
URLhttps://huggingface.co/datasets/cerebras/SlimPajama-627B.
[54] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherD.Manning,AndrewNg,andChristopherPotts.
Recursivedeepmodelsforsemanticcompositionalityoverasentimenttreebank. InDavidYarowsky,TimothyBaldwin,
Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical
MethodsinNaturalLanguageProcessing,pages1631â€“1642,Seattle,Washington,USA,October2013.Association
forComputationalLinguistics. URLhttps://aclanthology.org/D13-1170.
[55] BenSorscher,RobertGeirhos,ShashankShekhar,SuryaGanguli,andAriS.Morcos. Beyondneuralscalinglaws:
beatingpowerlawscalingviadatapruning,2023. URLhttps://arxiv.org/abs/2206.14486.
[56] Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. arXiv preprint
arXiv:1406.3896,2014.
[57] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB.
Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/
stanford_alpaca,2023.
15[58] YiTay,MostafaDehghani,SamiraAbnar,HyungChung,WilliamFedus,JinfengRao,SharanNarang,VinhTran,
DaniYogatama,andDonaldMetzler. Scalinglawsvsmodelarchitectures:Howdoesinductivebiasinfluencescaling?
InHoudaBouamor,JuanPino,andKalikaBali,editors,FindingsoftheAssociationforComputationalLinguistics:
EMNLP2023,pages12342â€“12364,Singapore,December2023.AssociationforComputationalLinguistics. doi:10.
18653/v1/2023.findings-emnlp.825. URLhttps://aclanthology.org/2023.findings-emnlp.825.
[59] MosaicMLNLPTeam. Introducingmpt-7b:Anewstandardforopen-source,commerciallyusablellms,2023. URL
www.mosaicml.com/blog/mpt-7b. Accessed:2023-05-05.
[60] TristanThrush,ChristopherPotts,andTatsunoriHashimoto. Improvingpretrainingdatausingperplexitycorrelations,
2024. URLhttps://arxiv.org/abs/2409.05816.
[61] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via doc-
ument de-duplication and diversification. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and
S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 53983â€“53995. Curran
Associates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/2023/file/
a8f8cbd7f7a5fb2c837e578c75e5b615-Paper-Datasets_and_Benchmarks.pdf.
[62] Together.ai. Redpajama:anopendatasetfortraininglargelanguagemodels,October2023. URLhttps://github.
com/togethercomputer/RedPajama-Data.
[63] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J.
Gordon. Anempiricalstudyofexampleforgettingduringdeepneuralnetworklearning. InInternationalConference
onLearningRepresentations,2019. URLhttps://openreview.net/forum?id=BJlxm30cKm.
[64] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothÃ©eLacroix,Baptiste
RoziÃ¨re,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,andGuillaume
Lample. Llama:Openandefficientfoundationlanguagemodels,2023. URLhttps://arxiv.org/abs/2302.
13971.
[65] NeerajVarshney,SwaroopMishra,andChittaBaral. Letthemodeldecideitscurriculumformultitasklearning. In
ColinCherry,AngelaFan,GeorgeFoster,Gholamreza(Reza)Haffari,ShahramKhadivi,Nanyun(Violet)Peng,Xiang
Ren,EhsanShareghi,andSwabhaSwayamdipta,editors,ProceedingsoftheThirdWorkshoponDeepLearningfor
Low-Resource Natural Language Processing, pages 117â€“125, Hybrid, July 2022. Association for Computational
Linguistics. doi:10.18653/v1/2022.deeplo-1.13. URLhttps://aclanthology.org/2022.deeplo-1.13.
[66] Cunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. Semeval-2020 task 4:
Commonsense validation and explanation. In Proceedings of the Fourteenth Workshop on Semantic Evaluation.
InternationalCommitteeforComputationalLinguistics,2020. doi:10.18653/v1/2020.semeval-1.39. URLhttp:
//dx.doi.org/10.18653/v1/2020.semeval-1.39.
[67] YizhongWang,SwaroopMishra,PegahAlipoormolabashi,YeganehKordi,AmirrezaMirzaei,AnjanaArunkumar,
ArjunAshok,ArutSelvanDhanasekaran,AtharvaNaik,DavidStap,etal. Super-naturalinstructions:generalizationvia
declarativeinstructionson1600+tasks. InEMNLP,2022.
[68] MengzhouXia,MikelArtetxe,ChuntingZhou,XiVictoriaLin,RamakanthPasunuru,DanqiChen,LukeZettlemoyer,
andVeselinStoyanov. Trainingtrajectoriesoflanguagemodelsacrossscales. InAnnaRogers,JordanBoyd-Graber,
andNaoakiOkazaki,editors,Proceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1:LongPapers),pages13711â€“13738,Toronto,Canada,July2023.AssociationforComputationalLinguistics.
doi:10.18653/v1/2023.acl-long.767. URLhttps://aclanthology.org/2023.acl-long.767.
[69] MengzhouXia,SadhikaMalladi,SuchinGururangan,SanjeevArora,andDanqiChen. Less:Selectinginfluentialdata
fortargetedinstructiontuning. arXivpreprintarXiv:2402.04333,2024.
[70] SangMichaelXie,HieuPham,XuanyiDong,NanDu,HanxiaoLiu,YifengLu,PercyLiang,QuocVLe,Tengyu
Ma,andAdamsWeiYu. Doremi:Optimizingdatamixturesspeedsuplanguagemodelpretraining. InThirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
lXuByUeHhd.
[71] SangMichaelXie,ShibaniSanturkar,TengyuMa,andPercyLiang. Dataselectionforlanguagemodelsviaimportance
resampling,2023. URLhttps://arxiv.org/abs/2302.03169.
[72] JiashengYe,PeijuLiu,TianxiangSun,YunhuaZhou,JunZhan,andXipengQiu. Datamixinglaws:Optimizingdata
mixturesbypredictinglanguagemodelingperformance,2024. URLhttps://arxiv.org/abs/2403.16952.
16[73] Dongkeun Yoon. Slimpajama-6b. https://huggingface.co/datasets/DKYoon/SlimPajama-6B,
2023. Accessed:September24,2024.
[74] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. HellaSwag:Canamachinereallyfinishyour
sentence? InAnnaKorhonen,DavidTraum,andLluÃ­sMÃ rquez,editors,Proceedingsofthe57thAnnualMeetingofthe
AssociationforComputationalLinguistics,pages4791â€“4800,Florence,Italy,July2019.AssociationforComputational
Linguistics. doi:10.18653/v1/P19-1472. URLhttps://aclanthology.org/P19-1472.
[75] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,PingYu,LiliYu,
SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,andOmerLevy. Lima:Lessismoreforalignment,2023.
17Appendix
InAppendixA,weprovideaglossaryofnotationusedinthepaper.InAppendixB,wediscusshowadditionaldatamixing
methodsarerelatedtotheLMOframeworkandprovideproofsthatexistingmethodscanbeexpressedinourframework.In
AppendixC,weprovideadditionalresultsonouranalysisofexistingdatamixingmethods.InAppendixDweprovide
additional details for our results in Section 6, and in Appendix E we provide additional results, including downstream
evaluationandablations.
A Notation
TheglossaryisgiveninTable4below.
Symbol Usedfor
m Thenumberofdatagroups.Examplesofdatagroupsincludeapre-trainingdomainoraninstruction-tuningtask.
D Training,validation,andtestdatasetscomprisedofmgroups,whereDi isgroupiâ€™straining/validation/testdata.
train/val/test (Â·)
N TotalnumberofsamplesfromD totrainon.
train
S Numberofstepstotrainfor(i.e.,S =N Ã—batchsize).
T Numberofroundstodividetraininginto,whereeachroundis S steps.
T
p Mixtureproportionsarep=(p1)forT =1(static)andp=(p1,...,pT)forT >1(dynamic),
wherept =[pt,...,pt ]âˆˆâ–³misaprobabilitydistribution.
1 m
f Alanguagemodel(canbeeitherpre-trainedorinitializedfromscratch).
f(p,t) Themodelf atthebeginningofroundtafterbeingtrainedonp1,...,ptâˆ’1sofar.
L (f) L (f)=(L (f),...,L (f))isthevectoroffâ€™straininglossesovereachdatagroup;
train/val/test train train,1 train,m
similarlydefinedforvalidationandtestlosses.
Lt (p) ShorthandforL (f(p,t)).Whendealingwithstaticmixtures,wealsouseL (p).
(Â·) (Â·) (Â·)
At ParametermatrixAt âˆˆRmÃ—musedinmixinglaws(2),capturingcross-groupinteractions.
SeeTable1forinstantiations.
bt,ct Group-specificparametersbt,ct âˆˆRmusedinmixinglaws2.Notethatthevalueofctdoesnotimpactthe
LMOframework,andneitherdoesbtwhenallbtareequal.
i
Ïƒ EitherÏƒ:Râ†’R=Idorexp.
Zt Usedfornormalizationinproportionupdaterule.
Î· StepsizeÎ·>0usedinproportionupdaterule.
P Thesetofmixtureproportionsthatcomprisesatrainingsweep.
Atâ‹† ApproximatelyoptimalAtforthelineardynamicmixinglaw,obtainedbyfitting
Lvalt+1(p)=Lvalt(p)âˆ’Atâ‹†povertrainingsweeps.
AËœt Method-specificAËœt =btAt,whereAtisobtaineddirectlyfromthemethodand
bt âˆˆRislearnedfromtrainingsweeps.
sim(AËœt,Atâ‹†) Similaritybetweenmethod-specificandoptimalAt,definedasanaverageofcosinesimilarityand
SpearmanrankcorrelationoverAtâ€™snormalizedcolumnsums.
Îµ one-hotsmoothingfactorusedtodefinept,i =(1âˆ’Îµ)1 +ÎµUnif(m),smoothedone-hotdistributions
i
weusetolearnAtinAIOLI.
Î´ ThefractionperrounddedicatedtolearningAtinAIOLI.
k NumberofsweepspergrouptoaverageAtestimatesoverinAIOLI.
pinit Initialmixturepinit âˆˆâ–³mthatAIOLIcandynamicallyadjust.
S Numberofstepstotrainaccordingtopinit.
init
Table4:Glossaryofvariablesandsymbolsusedinthispaper.
B LMO framework details
B.1 Additionalexistingmethods
Wecommentontwootherpopulardatamixingmethods,OnlineDataMixing(ODM)[2]andRegMix[37].
InODM[2],datamixingisframedasamulti-armedbanditproblem,whereeacharmisadatagroupthatabatchis
trainedon,andtherewardfunctionisdefinedintermsofthetraininglossofeachgroup.ODMusestheEXP3algorithmto
exploretrainingondifferentdatagroups.pt,whichisusedtodeterminewhichgrouptheentiretrainingbatchiscomprised
of,isupdatedaccordingtopt+1 =(1âˆ’mÎµ ) exp(Îµtâˆ’1R jt) +Îµ .Îµ isanexplorationrate,andtherewardfunctionis
j t (cid:80)m i=1exp(Îµtâˆ’1R it) t t
18Rt =Î±Rtâˆ’1+(1âˆ’Î±)Lt train,j(p) ifthejthgroupisselectedattimet;otherwise,Rt =Rtâˆ’1.Whiletheexplorationand
j j pt j j
j
thesmoothingofptandRtmakethismethodnotdirectlyexpressibleinourframework,wenotethattheupdaterulecan
belooselyinterpretedasallocatinglargerproportionstogroupsthathavehighloss.Thisupdateruledoesnotconsider
cross-groupinteractionsandisthussimilartoDoReMiâ€™supdaterule,whichutilizesadiagonalAtdefinedintermsofcurrent
loss.
RegMix[37]conductsmanytrainingrunsonsmallermodelsatshorterscales.SimilartoDML[72],aregressionmodel
isfittotheserunsandusedtopredictmixtureproportionsforalongerrunonalargermodel.Theyconsiderusingalinear
regressionmodel,i.e.,themixinglawL (p)=c âˆ’(cid:80)m A pt,butfindthattheR2isrelativelylow(0.87).Instead,
val,i i j=1 ij j
theirmainapproachusesLightGBM,atree-basedgradientboostingapproach,i.e.,usinganensembleofnon-lineardecision
treesasamixinglaw.Wenotethat AIOLI couldbeusedinconjunctionwithRegMixtofacilitatebettertransferacross
modelsizeandtrainingdurationintheirsettings,anexcitingdirectionforfuturework.
B.2 Proofsforsection3.3
B.2.1 BackgroundonExponentiatedGradientDescent
Weprovidebackgroundonexponentiatedgradientdescent(EGD)takenfromKakade[29].InEGD,wehaveasequenceof
decisionsw1,...,wT,wherewt =[wt,...,wt ]âˆˆâ–³m.Wealsohaveasequenceofcostfunctionsc1,...,cT :â–³m â†’R.
1 m
To minimize the total cost (cid:80)T ct(wt), the EGD update rule sets w0 = Unif(m), and updates according to wt+1 =
t=1 j
w jtexp(âˆ’Î·â–½ jct(wt)) .Z ensuresthatwt+1 âˆˆ â–³m,Î· isastepsize,andâ–½ ct(wt)denotes âˆ‚ct(wt).EGDisknowntohave
Zt t j âˆ‚w jt
certainregretguaranteesonthevalueofcostsincurredbyplayingw1,...,wT versusalwaysplayingthebestfixedpointin
hindsight,i.e.,thequantity(cid:80)T ct(wt)âˆ’inf (cid:80)T ct(w).WenowarereadytoproveLemma1.
t=1 wâˆˆâ–³m t=1
Lemma1. TheEGDupdaterulefor(1)subjecttoLt+1(p)=ctâˆ’bt(cid:80)m At pt âˆ€iâˆˆ[m]is
val,i i i j=1 ij j
(cid:18) m (cid:19)
1 (cid:88)
pt+1 = Â·ptexp Î· btAt âˆ€j âˆˆ[m], (3)
j Zt j i ij
i=1
whereÎ· >0isthestepsizeandZtisanormalizingconstantsuchthatpt+1 âˆˆâ–³m.
j
Proof. Thecostfunctionateachtimestepinoursettingis(cid:80)m Lt+1(p),andthedecisionwemakeispt.Themixinglaw
i=1 val,i
constraintin(2)withÏƒ =IdisLt+1(p)=ctâˆ’bt(cid:80)m At pt foralliâˆˆ[m],soourobjectivefunction(1)canbewritten
val,i i i j=1 ij j
as
m (cid:18) m (cid:19)
(cid:88) (cid:88)
ctâˆ’bt At pt . (4)
i i ij j
i=1 j=1
Thegradientofthisexpressionwithrespecttopt forj âˆˆ[m]isâˆ’(cid:80)m btAt .PluggingthisintotheEGDupdaterule,
j i=1 i ij
weobtaintheupdatept+1 = 1 ptexp(Î·(cid:80)m btAt ).
j Zt j i=1 i ij
B.2.2 ProofofTheorem1
ToproveTheorem1,wewriteoutindividualpropositions1,2,3forexpressingeachonlinemethodintheLMOframework.
ByourdefinitionofwhatitmeanstoexpressamethodinLMO,wemustconsiderhoweachmethod1)trainsf and2)
setspt.WemustseeifthisprocedurecanbereplicatedbysolvingsomespecificationoftheLMOoptimizationproblemin
ourdatamixingsetup.
Critically,notethatthisdefinitionofâ€œexpressionâ€doesnotclaimthattheoptimizationproblemsproposedinexisting
methodsareexactlythesameastheLMOoptimizationproblem.Instead,wearestatingthatthetrainingproceduresusedin
theirmethodscanbeequivalentlyviewedasawayofsolvingtheLMOoptimizationproblemsubjecttocertainassumptions
ontheloss-proportionrelationship.
Proposition1(Skill-ItDerivation). Usinga)alineardynamicparameterizationLt+1(p)=Lt (p)âˆ’bt(cid:80)m At pt,b)
val,i val,i j=1 ij j
parametersAt =Lt (p)Â·(LT+1(1 )âˆ’L1 (1 ))/L1 (1 ),andc)exponentiatedgradientdescent(EGD)tosolve
ij val,i val,i j val,i j val,i j
forp,theLMOframework(1)canexpressSkill-It.
19Proof. TheSkill-ItalgorithmsetsptineachroundandthensamplesfromD accordingtopttotrainf foraround.This
train
trainingprocedureisdirectlyspecifiedinourdatamixingproblemsetup(Section2).Therefore,wesimplyneedtoshow
thattheSkill-Itupdaterulecanbeconvertedintoalineardynamicmixinglaw.BycomparingLemma1andtheSkill-It
updaterulept+1 = 1 Â·ptexp(cid:0) Î·(cid:80)m ASGLt (p)(cid:1) ,wecanmatchAt inthelemmawithASG inSkill-It,andwecan
matchbtinthj elemmZ at witj hLt (p).i= T1 heri ej forv ea ,l, Li emma1tellsusthatui sj ingLt+1(p)=ctâˆ’btij (cid:80)m Lt (p)ASGpt in
i val,i val,i i j=1 val,i ij j
theLMOframeworkwithexponentiatedgradientdescentrecoversSkill-It(sincethebtandctcanbedroppedandareonly
i
usedforscalingAt).
UsingthedefinitionofASG,wecanrewritethemixinglawasLt+1(p)=ctâˆ’bt(cid:80)m At,Skill-Itpt whereAt,Skill-It =
ij val,i i j=1 ij j ij
Lt (p)(LT+1(1 )âˆ’L1 (1 ))/L1 (1 ).Lastly,notethatwecanreplacectwithanyothervalue,includingLt (p),
val,i val,i j val,i j val,i j i val,i
duetothefactthatpthasmâˆ’1degreesoffreedom(seeLemma2).
Wenotethat[13]explicitlyspecifiestheirmixinglawinequation2oftheirpaper,alongwiththesameobjectivefunction
asoursintheLMOframework.
Proposition2(DoReMiDerivation). Usinga)alineardynamicparameterizationLt+1(p)=Lt (p)âˆ’bt(cid:80)m At pt,
val,i val,i j=1 ij j
b)parametersAt =min{Lt (p)âˆ’L (f ),0}fori=j andA =0otherwise,andc)EGDtosolveforp,the
ij train,i train,i ref ij
LMOframework(1)canexpressDoReMiâ€™sproxymodel.
Proof. WhentrainingtheproxymodelforDoReMi,ptissetineachround,andthenfisupdatedtominimize(cid:80)m ptL (f).
i=1 i train,i
UsingLemma3,weestablishthatDoReMiâ€™sweightedtrainingobjectiveateachtimestepisequalinexpectationtotheob-
jectiveoftrainingondatasampledfrompt,whichiswhatourproblemsetupfocuseson.Havingestablishedthatthetraining
procedureisthesameinexpectation,wenowneedtoshowthattheDoReMiptupdaterulecanbeconvertedintoalineardy-
namicmixinglaw.BycomparingLemma1andtheDoReMiupdaterulept+1 âˆptexp(Î·max{Lt (p)âˆ’L (f ),0}),
j j train,j train,j ref
wecanmatchAt inthelemmawith0foriÌ¸=j,andAt withmax{Lt (p)âˆ’L (f ),0}.Therefore,Lemma1tells
ij ii train,j train,j ref
usthatusingLt+1 = ctâˆ’bt(cid:80)m At pt withAt = max{Lt (p)âˆ’L (f ),0}canexpresstheDoReMiproxy
val,i i j=1 ij j ii train,j train,j ref
modeltraining.WeincludebttoallowforscalingAt,butsincethisdoesnotimpacttheoptimalp,itisnotintheupdate
rule.Lastly,applyingLemma2letsuswritethemixinglawasLt+1 =Lt (p)âˆ’bt(cid:80)m At pt.
val,i val,i j=1 ij j
WecommentonthefactthatDoReMiâ€™sproxymodelistrainedwithaDRO(distributionallyrobustoptimization)min-max
objective,namely,minimize maximize (cid:80)m p LT+1(f).Thisobjective,whichdiffersfromourdatamixingobjective,
f p i=1 i train,i
yieldsptgradientascentandftgradientdescentupdates.However,wearestillabletoexpressthistrainingprocedurein
theLMOframework,sinceourclaimis:ifweassumethattheLt+1 =Lt (p)âˆ’bt(cid:80)m At,DRMpt mixinglawcaptures
val,i val,i j=1 ij j
therelationshipbetweenLt andpt,thentrainingaccordingtotheDoReMiproxyrunshouldnotonlyguidef andpto
val
optimizetheDROobjective,butalsotooptimizetheaveragevalidationlosspergroup.
Proposition3(DoGEDerivation). Usinga)alineardynamicparameterizationLt+1(p) = Lt (p)âˆ’bt(cid:80)m At pt,
val,i val,i j=1 ij j
b)parametersAt =âŸ¨â–½Lt (p),â–½Lt (p)âŸ©foralli,j âˆˆ[m],andc)EGDtosolveforp,theLMOframework(1)can
ij val,i train,j
expressDoGEâ€™sproxymodel.
Proof. WhentrainingtheproxymodelforDoGE,ptissetineachround,andthenfisupdatedtominimize(cid:80)m ptL (f).
i=1 i train,i
Using Lemma 3, we establish that DoGEâ€™s weighted training objective at each timestep is equal in expectation to the
objectiveoftrainingondatasampledfrompt.Next,weshowthattheDoGEupdaterulecanbeconvertedintoalineardy-
namicmixinglaw.BycomparingLemma1andtheDoGEupdaterulept+1 âˆptexp(Î·âŸ¨â–½L (ft),(cid:80)m â–½L (ft)âŸ©),
j j train,j i=1 val,i
we can see that At in the Lemma can be matched with âŸ¨â–½L (ft),â–½L (ft)âŸ©. Therefore, using the mixing law
ij train,j val,i
Lt+1 =ctâˆ’bt(cid:80)m At pt withAt =âŸ¨â–½L (ft),â–½L (ft)âŸ©allowsLMOtoexpressDoGEproxymodeltraining.
val,i i j=1 ij j ij train,j val,i
Again, bt is included for scaling but does not impact optimization, and by applying Lemma 2, we can replace ct with
i
Lt (p).
val,i
Lemma 2. Let Lt+1(p) = ct âˆ’ (cid:80)m At pt for some ct and At. Then, there exists an Bt such that Lt+1(p) =
i i j=1 ij j ij i
Lt(p)âˆ’(cid:80)m Bt pt.
i j=1 ij j
20Proof. Sincept âˆˆâ–³m,wecanwritetheprobabilitypt as1âˆ’(cid:80)mâˆ’1pt.Then,thefirstequationcanbewrittenas
m j=1 j
mâˆ’1 (cid:18) mâˆ’1 (cid:19)
(cid:88) (cid:88)
Lt+1(p)=ctâˆ’ At pt âˆ’At 1âˆ’ pt (5)
i i ij j im j
j=1 j=1
mâˆ’1
(cid:88)
=ctâˆ’ (At âˆ’At )pt âˆ’At
i ij im j im
j=1
mâˆ’1
(cid:88)
=Lt(p)âˆ’ (At âˆ’At )pt âˆ’(At âˆ’ct+Lt(p))
i ij im j im i i
j=1
mâˆ’1 mâˆ’1
(cid:88) (cid:88)
=Lt(p)âˆ’ (At âˆ’At +At âˆ’ct+Lt(p))pt âˆ’(At âˆ’ct+Lt(p))(1âˆ’ pt)
i ij im im i i j im i i j
j=1 j=1
mâˆ’1 mâˆ’1
(cid:88) (cid:88)
=Lt(p)âˆ’ (At âˆ’ct+Lt(p))pt âˆ’(At âˆ’ct+Lt(p))(1âˆ’ pt).
i ij i i j im i i j
j=1 j=1
LetBt =At âˆ’ct+Lt(p)forallj âˆˆ[m].Then,thisequationbecomes
ij ij i i
mâˆ’1 mâˆ’1
(cid:88) (cid:88)
Lt+1(p)=Lt(p)âˆ’ Bt pt âˆ’Bt (1âˆ’ pt) (6)
i i ij j im j
j=1 j=1
m
(cid:88)
=Lt(p)âˆ’ Bt pt.
i ij j
j=1
Lemma3. LetLt (f,p)bethetotaltraininglossoff onabatchofsizeB sampledfromD accordingtop âˆˆ â–³m,
B train
andletLt (f,p)bethetotaltraininglossonsamplesfromgroupiinthatbatch.Then,theaveragelossoverauniformly
B,i
sampledbatchweightedbyptisequalinexpectationtotheaveragelosspergroupoverabatchsampledaccordingtopt:
(cid:34) (cid:88)m (cid:35) (cid:20) Lt (f,pt)(cid:21)
E ptLt (f,Unif(m)) =E B (7)
i B,i m
i=1
Proof. LeteachgroupiconsistofsamplesxfromthedistributionD ,whereD âˆ¼D andletLËœ (f)=E [â„“(f,x)]
i train i train,i xâˆ¼Di
bethepopulation-levellossongroupi,whereâ„“(f,x)isfâ€™slossonsamplex.
Ifabatchisuniformlysampled,eachgrouphasB/msamples.WecanthenwriteLt (f,Unif(m))=(cid:80)B/mâ„“(f,xi),
B,i k=1 k
wherexi isthekthsampleofgroupiinthebatch.Then,
k
E(cid:34) (cid:88)m
pt iLt
B,i(f,Unif(m))(cid:35) =Eï£® ï£°(cid:88)m
pt
iB (cid:88)/m
â„“(f,xi
k)ï£¹ ï£»=(cid:88)m p mt iB
LËœ train,i(f). (8)
i=1 i=1 k=1 i=1
Next,ifabatchissampledaccordingtopt,thengroupihasBptsamplesinthebatch.WecanthenwriteLt (f,pt)=
i B
(cid:80)m (cid:80)pt iB â„“(f,xi).Then,
i=1 k=1 k
ï£® ï£¹
E(cid:20) Lt B( mf,pt)(cid:21)
=E
ï£°(cid:88)m (cid:88)pt iB â„“(f m,xi k) ï£»=(cid:88)m p mt iB
LËœ train,i(f). (9)
i=1k=1 i=1
Thishenceestablishestheequivalenceinexpectationbetweenaweightedtrainingobjectiveandtrainingondatasampled
accordingtop.
21Table5:Comparisonoflog-linearstaticandlineardynamicmixinglawparameterizationsacrossdifferentdatasettingswith
MSEandR2metrics.Bothlog-linearandlineardynamicmixinglawsfittherelationshipbetweenmixingproportionsand
losseswell.
Arxiv/SE GH/C4 Books/SE
Parameterization
MSE R2 MSE R2 MSE R2
Log-linearstatic 2e-4 0.990 5e-4 0.989 6e-4 0.987
Lineardynamic 2e-4 0.936 1e-4 0.948 4e-5 0.926
Arxiv/Books/SE CC/GH/Wiki SlimPajama
MSE R2 MSE R2 MSE R2
Log-linearstatic 6e-4 0.991 0.001 0.989 0.002 0.997
Lineardynamic 6e-5 0.957 1e-4 0.975 5e-6 0.938
C Analysis Details and Additional Results
C.1 MixingLawParameterization
Wedescribehowweperformedthelinearandlog-linearparameterizationexperiments.Forthelog-linearstaticparame-
terizations,wetrainourmodelonp âˆˆ P sweepsandfittheparametersusingcodeprovidedinYeetal.[72](i.e.,using
PyTorchandL-BFGStominimizetheHuberlossofthemixinglaw).Wedothisover5randomseedsfork =2,3andover
3seedsforthefullSlimPajama.
Forthelineardynamicparameterizations,fork = 2,3wetrainthemodelfor2000stepsaccordingtosomep0 âˆˆ P,
andthensweepoverP forthenext100steps.Wedothisforonerandomseed,performing|P|2 totalruns.Forthefull
SlimPajamasetting,wetrainthemodelfor10000stepsusingstratifiedsampling,andthensweepoverP forthenext5000
steps.WefittheparametersusingPytorchandL-BFGS.
C.1.1 Additionalparameterizationexperiments
Parameterizationacrosscheckpoints.Weinvestigatewhetherthelog-linearstaticandlineardynamicmixinglawsremain
well-specifiedinlaterstagesoftrainingandonotherdatasets.Todoso,wetakevariousPythia160Mcheckpoints [8],
sweepmixingproportions,andfitthelineardynamicandlog-linearstaticmixinglaws.Wetrainfor2000stepsaccording
tothelearningratesandlearningrateschedulerreportedin[8].Wefitthestaticmixinglawonfullrunsof2000steps,
andthelineardynamicmixinglawatt = 500,afterwhichwedoatrainingsweepoverthenext500steps.InTables6
and7,wefindthatthestrongfitforlog-linearstaticmixinglawscontinuestoholdduringpre-trainingatcheckpoint72K
(roughlyhalfwaythroughtrainingPythia-160M)andafterpre-training,withanaverageR2of0.982and0.991,respectively.
However,thelineardynamicmixinglawâ€™sR2 coefficientislower,averaging0.815atcheckpoint72Kand0.830atthe
endofpre-training.Itmaybeinterestingtofurtherstudyifthedynamicsoftheloss-proportionrelationshipevolveina
structuredwaythroughouttraining,oriftheseresultsareduetomorenoiseinhowmodelslearnatlaterstagesoftraining.
Parameterizationacrossothersetsofdatagroups.InFigure4,weidentifyanexamplesetofdatagroupsthatexhibits
anon-linearrelationshipbetweenlossandproportion:Books/C4fromSlimPajama.Forthesetwodatagroups,weseethat
astheproportionofBooksincreaseswhileC4decreases,thelossonBooksstartsincreasingpastacertainp,suggesting
quitecounterintuitivelythatperformanceonBooksisoptimizedbyallocatingsomeproportiontoC4.Inthiscase,neither
log-linearstaticorlineardynamicmixinglawshavegoodfittotheproportion-lossrelationship,asneithercanrepresentthe
non-linearity.Inparticular,theaverageMSEandR2forthelog-linearstaticmixinglawis0.003and0.558,respectively,
andtheaverageMSEandR2forthelineardynamicmixinglawis0.0002and0.721.
Fortunately,becausethesenonlinearitiesexistontheboundaryofthesimplexandtendtoincurhighloss,theytendto
havelittleimpactontheoptimizationofp,whichstrivestominimizetheaveragelossforthestandarddatamixingproblem.
Forinstance,wefoundthattheoptimalproportionaccordingtoYeetal.[72]â€™slog-linearstaticmixinglawononerandom
seedwas[0.176,0.824],andthetrueoptimalfromgridsearchwas[0.2,0.8].However,itisimportanttofurtherinvestigate
thisnon-linearphenomenononadditionaldatagroupsandtrainingregimes,whichwedefertofuturework.
C.1.2 Parameterizationoninstruction-tuningmixtures
Previously,westudiediftrainingonSlimPajama(fromscratch,atapre-trainingcheckpoint,andattheendofpre-training)
exhibitedlineardynamicorlog-linearstaticmixing.Wenowstudyifsupervisedfine-tuningonamixtureoftasktypes
22Table6:Comparisonoflog-linearstaticandlineardynamicmixinglawparameterizationswhentrainingfromthe72K
Pythia-160Mcheckpoint.
Arxiv/SE GH/C4 Books/SE
Parameterization
MSE R2 MSE R2 MSE R2
Log-linearstatic 2e-4 0.975 7e-5 0.992 2e-4 0.981
Lineardynamic 4e-4 0.834 7e-4 0.815 6e-4 0.796
Table7:Comparisonoflog-linearstaticandlineardynamicmixinglawparameterizationswhentrainingfromthepre-trained
Pythia-160M.
Arxiv/SE GH/C4 Books/SE
Parameterization
MSE R2 MSE R2 MSE R2
Log-linearstatic 3e-6 0.994 4e-6 0.992 6e-6 0.986
Lineardynamic 5e-5 0.896 8e-5 0.824 1e-4 0.769
exhibitssimilarmixinglaws.Thedatamixinggroupsweconsiderareinstruction-followingtasks.Itisimportanttoknow
howtooptimallymixthesegroupssothatthemodelcanfollowavarietyofinstructions,asshownbyhowexistingdatasets
consistofadiversesetofcommands[14,38,46,57,67,75].
Weselectm=9tasksfromNaturalInstructions[42,67]:AbductiveNLI,BoolQ,HellaSwag,MathQA,PIQA,SemEval,
SQuAD1.1,SST2,andXSum.Weselectedtaskswithmanysamples,prioritizingdiversityofcapabilitiesandformats.We
constructvalidationandtestsplitsthatare100samplespergroup.MoreinformationisprovidedinTable8.
Table8:OverviewofInstructionTasks
Task TasknumberinNaturalInstructions #Samples OutputFormat
AbductiveNLI[7] task067 6499 Open-ended
BoolQ[15] task380 6500 Yes/No
HellaSwag[74] task1389 6494 Multiplechoice
MathQA[4] task1420 6452 Multiplechoice
PIQA[9] task080 6500 Open-ended
SemEval[66] task295 5996 Multiplechoice
SQuAD1.1[51] task075 6498 Open-ended
SST2[54] task363 6495 Pos/Neg
XSum[47] task1290 6493 Open-ended
Toconductthesweeps,wesetP tobe50mixingproportionsdrawnfromtheDirichletdistributionwithÎ±=1.5.Forthe
staticparameterization,weconduct50trainingrunsoverP,for1000stepseach,andwedothisover5randomseeds.For
thedynamicparameterization,wetrainon10proportionsfromP for500stepsandthensweepovertheentireP forthe
next100steps.Wedothisover1randomseed.Weensuretherearenorepeatedsamplesintraining.Weuseapre-trained
Pythia-160Mmodel[8],consistentwiththerestofourexperiments,andusealinearschedulerwithlearningrate1e-5and
100warmupsteps.
Our results are in Table 9. In addition to displaying the averaged MSE and R2 across all 9 groups, we also display
per-groupresults.Wefindthatthelog-linearstaticmixinglawattainsanaverageR2of0.888overtheseinstructiontasks.
However,thelineardynamicmixinglawonlyattainsanaverageR2of0.419.Interestingly,weobservethatthe4instruction
tasksthatinvolveopen-endedgenerationhavehigherR2(averageof0.73)whilethebinaryandmultiplechoicetaskshave
a lower R2 (average of 0.17) for the linear dynamic law. We hypothesize that this is because tasks that do not require
open-endedgenerationareeasiertolearnandmoresusceptibletooverfitting.Weobservedthattheirvalidationlossesoften
plateaubefore500steps,andincreasingtheproportionsafterthispointdoesnotconsistentlydecreaseloss.Finally,wealso
includealog-lineardynamicmixinglawâ€”thatis,log(Lt (p))=log(Ltâˆ’1(p))âˆ’(cid:80)m At pt.Thiscanbethoughtofas
val,i val,i j=1 ij j
apiecewiseversionofthelog-linearstaticmixinglaw,andwefindthatthisslightlyimprovesMSEandR2comparedtothe
lineardynamicmixinglaw.
23100
101
102
103
104
101
105
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Proportion of book Proportion of c4
4.50 5.5
4.45 5.4
4.40 5.3
4.35 5.2
4.30 5.1
4.25 5.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Proportion of book Proportion of c4
0.4 prior 0.3 prior 0.6 prior 0.8 prior
0.1 prior 0.5 prior 0.7 prior 0.9 prior
0.2 prior
Figure4:Top:Log-linearstaticmixinglawfitonBooks/C4across5randomseeds(validationloss).Bottom:Lineardynamic
mixinglawfitonBooks/C4on1randomseed.Eachcolorisadifferentinitialmixturep0 âˆˆP trainedfor2000steps,and
thefittingsweepsaredoneover100additionalsteps.
C.2 Valuesofmixinglawparameters
Weexplainhowtocomparemethod-specificAtâ€™stoanapproximationofthetrueAtâ‹†.First,afterperformingmethod-specific
initialization,suchastrainingreferencemodels,weruneachonlinemethod(Skill-It,DoReMiâ€™sproxymodelDoGEâ€™sproxy
model,Skill-it,andAIOLI)fortsteps.ForSkill-It,DoReMi,andDoGE,weusetheunrestrictedsettingconfigurationof
hyperparameterspresentedinSectionD.ForAIOLI,weanalyzetheparametersofAIOLI+GSfromtherestrictedsetting,
sincewefoundthatthishadlessnoisyfluctuationintheweightsthanintheunrestrictedsetting.Form=2,wesett=1000
forSkill-Itandt=500forDoGE,DoReMi,andAIOLIsinceSkill-Itisupdatedlessfrequently.Form=3,wesett=1000
forDoGE,DoReMiandSkill-It,andt=1500forAIOLI.Wethencheckpointthelanguagemodelandthemethodâ€™sAt.For
DoGEandDoReMi,wecomputeasmoothedAt = 1 (cid:80)100 Atâˆ’100+i becauseeachAt iscomputedatthebatchlevel,
100 i=1
andcanthusbenoisy.ForAIOLI,wealsosmooththeAtbyaveragingtheprevioustimestepparameters.
ToapproximateAtâ‹†,werunatrainingsweepoverP forthenext100stepsafterthecheckpoint.Weusethistraining
sweeptofitAtâ‹†fromthedynamicmixinglawLt+1(p)=Lt (p)âˆ’(cid:80)m Atâ‹†pt.
val,i val,i j=1 ij j
Before we compare parameters, we must scale At by some bt where Lt+1(p) = Lt (p)âˆ’bt(cid:80)m At pt for all
val,i val,i j=1 ij j
i âˆˆ [m]. This is allowed since bt does not influence the optimal p and does not need to be in the update rule. We
fit a single bt across all groupsâ€™ mixing laws and set AËœt = btAt. We can then compare At and Atâ‹† using the metric
sim(AËœt,Atâ‹†)=0.5cossim(aËœt,atâ‹†)+0.5Spearman(aËœt,atâ‹†),whichweproposedinSection4.3.
C.2.1 PropertiesofAtâ‹†
WediscusssomepropertiesofAtâ‹†,findingthat1)Atâ‹†canvarysignificantlyacrosstime,and2)Atâ‹†needstobemodeledas
afullmatrix.InAppendixE.2,wealsopresentablationsonAIOLIthattesthowcapturingthesetwopropertiesinAIOLIare
importanttoperformance.
Foreachinitialmixturep0 âˆˆP,wetrainfort=2000stepsandthensweepoverP forthenext100steps.Werepeatthis
setupfort=4000toobtainA2000â‹†andA4000â‹†.WedothisexperimentforArxiv/StackexchangeandGithub/C4.
ExtentoftimevariationofAt.WefindthatthecolumnsumsofAtcanchangeorderovertime,meaningthatthept
â€œchangesdirectionâ€intermsofwhichgrouphasthelargestproportion.Inparticular,forp0 =[0.5,0.5]andGithub/C4,we
havethat
(cid:20) (cid:21) (cid:20) (cid:21)
0.148 0.011 0.015 0.001
A2000â‹† = A4000â‹† = (10)
âˆ’0.013 0.087 0.001 0.015
24
koob
no
)c
-
ssoL(
goL
koob
no
ssoL
pets-txeN
4c
no
)c
-
ssoL(
goL
4c
no
ssoL
pets-txeNTable 9: Comparison of log-linear static, linear dynamic, and log-linear dynamic mixing law parameterizations over
instruction-tuningtasksintermsofMSEandR2.
Log-linearstatic Lineardynamic Log-lineardynamic
Task
MSE R2 MSE R2 MSE R2
AbductiveNLI 3e-4 0.939 4e-4 0.586 4e-5 0.599
BoolQ 1e-3 0.941 8e-2 0.215 2e-2 0.276
HellaSwag 6e-4 0.848 6e-3 0.225 2e-3 0.256
MathQA 8e-4 0.787 6e-3 0.090 2e-3 0.115
PIQA 5e-4 0.916 3e-4 0.754 2e-5 0.761
SemEval 9e-4 0.974 4e-3 0.239 3e-3 0.254
SQuAD1.1 8e-3 0.947 4e-3 0.742 9e-4 0.766
SST2 3e-3 0.662 2e-2 0.082 4e-2 0.118
XSum 1e-4 0.977 1e-4 0.838 1e-5 0.841
Average 2e-3 0.888 1e-2 0.419 8e-3 0.443
The column sums are 1âŠ¤A2000â‹† = [0.135,0.098] and 1âŠ¤A4000â‹† = [0.016,0.017], showing that the ordering of
proportionsofthegroupschanges.Thissuggeststhattheoptimalptcanchangesignificantlyacrosstime,prioritizingGithub
initiallyandlaterC4,whichisalsoreflectedforGithub/C4inthegreedyrowofTable10.
However,forArxiv/Stackexchange,wefoundthatthecolumnsumsofA2000â‹†andA4000â‹†neverchangeintermsofthe
orderingofproportionsofthedatagroups,acrossallp0 âˆˆ P.Asaresult,theoptimalpt neverchangesdirection.This
suggeststhathowmuchAtvariesinorderingovertimedependsonthedatagroups.Asaresult,methodslikeSkill-It,which
useatime-invariantASGmultipliedbyvalidationloss,maynotbeabletomatchthetrueAtâ‹†ifthegroupsâ€™validationlosses
changeinrankingacrosstime,whichweobserveinGithub/C4.
ModelingAtâ‹†asafullvsdiagonalmatrix.Wefindthatmodelingtheoff-diagonalentriesofAtâ‹†isimportant.Foreach
sweep,wefitbothAtâ‹†asdescribedaboveandadiagonalmatrixAtâ‹†.WecompareifthecolumnsumsofAtâ‹†andAtâ‹†differ
d d
intheorderofelements.
WefindthatforArxiv/StackExchange,p0 =0.4,andbotht=2000andt=4000,settingpt basedonthefullmatrix
wouldputalargerproportiononStackExchange,whilesettingptbasedonthediagonalmatrixwouldputalargerweighton
ArXiv.Inparticular,thefullanddiagonalmatricesfort=2000are
(cid:20) (cid:21) (cid:20) (cid:21)
0.249 0.058 0.284 0
A2000â‹† = A2000â‹† = (11)
0.025 0.224 d 0 0.238
ThesecondcolumnsumislargerforA2000â‹† andsmallerforA2000â‹†.WealsohavesimilarfindingsonGithub/C4;for
d
p0 =0.6andt=2000,wehave
(cid:20) (cid:21) (cid:20) (cid:21)
0.119 0.027 0.135 0
A2000â‹† = A2000â‹† = (12)
âˆ’0.010 0.104 d 0 0.098
UsingthediagonalmatrixforGithub/C4wouldresultinprioritizingtrainingonGithub,eventhoughthefullmatrix
suggeststhatC4shouldbeprioritized.Therefore,itisimportanttomodelAtâ‹†asafullmatrix.MethodslikeDoReMi,which
useadiagonalAt,canperformsuboptimally.
C.3 Solvingstrategy
WepresentourresultsonexaminingtheassumptionsmadeinhowexistingmethodssolvetheLMOoptimizationproblem.
Allonlinemethodsuseexponentiatedgradientdescent,whichupdatesptusingthegradientatthecurrenttimestep.This
involvesagreedyapproximationoftheobjectivefunction.Westudyifthegreedyapproximationyieldsapisclosetothe
trueoptimalp.
Form=2datasettings,wetakeourS =5000stepsandsplititintoT =2rounds.Weperformabrute-forcesweepat
eachroundoverP,whichsweepsp =0.1,0.2,...,0.9.Intotaloveronerandomseed,weconduct81trainingrunsfor
1
eachofArxiv/Stackexchange,Github/C4,andBooks/Stackexchange.
253.4
4.2
3.3
4.0 3.2
3.1 3.8
3.0
3.6
2.9
3.4
2.8
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Proportion of arxiv Proportion of stackexchange
0.7 prior 0.3 prior 0.5 prior 0.8 prior
0.1 prior 0.4 prior 0.6 prior 0.9 prior
0.2 prior
Figure5:ThelineardynamicparameterizationresultsfromFigure2,withpt =[0,1]and[1,0]alsoplotted.Weseethatthe
lineardynamicsaremisspecifiedatpt =0forbothi.
i
Wedeterminethegreedy-approximatepbyselectingthebestp1.Then,conditioningonthisp1,weselectthebestp2.We
reportwhatthegreedypanditsperformanceisinthefirstrowofTable10,andwereporttheoptimalpanditsperformance
inthesecondrow.Notethatthisprotocoldoesnotdependonthemixinglaworamethodforsettingp.
WefindthatforArxiv/StackExchangeandBooks/StackExchange,thegreedyproportionsandtheoptimalproportionsare
identical.However,forGithub/C4,thegreedyapproximationfailstorecovertheoptimalproportions.Therefore,thegreedy
approximationrecoverstheoptimaldynamicproportionsin2outof3cases.
Table10:Comparisonofthegreedilyselectedp1, p2versustheoptimalp1, p2foraT =2roundsdatamixingproblem.
On2outof3datasets,thegreedilyselectedproportionsmatchtheoptimalproportions.
Arxiv/SE GH/C4 Books/SE
Solving
p1,p2 AvgtestPPL p1,p2 AvgtestPPL p1,p2 AvgtestPPL
1 1 1 1 1 1
Greedy 0.4,0.4 16.039 0.6,0.4 36.525 0.3,0.6 45.513
Optimal 0.4,0.4 16.039 0.3,0.6 34.709 0.3,0.6 45.513
Beyondexponentiatedgradientdescent,onemaywonderifexactlysolvingthegreedyobjectivecouldsuffice.Forthe
lineardynamicmixinglawLt+1(p)=Lt(p)âˆ’Atpt,theoptimalptis1 ,wherej =argmax(cid:80)m At .However,wefind
j i=1 ij
inFigure5thattheloss-proportionrelationshipcanbenonlinearattheedgeofthesimplexwherept =1 .Exponentiated
j
gradientdescent,whichusesentropyregularization,ishenceabletoimplicitlyavoidextremepwherethelinearmixinglaw
ismisspecifiedandthusisapracticaltechniqueforLMO.
D Experimental Details
Trainingdetails. ToobtaintheSlimPajamatestsets,weshuffleandsplitthevalidationsetfromSlimPajama-6B[53,73]
inhalf.Next,wediscussthetrainingsetupsfortherestrictedandunrestrictedsettings.Forthem=2,3settings,wetrain
a160MmodelusingPythia-160Mâ€™sconfigurationforS =5000stepsandresultsareaveragedover5randomseeds.For
m=7,wetraina160MmodelusingPythia-160Mâ€™sconfigurationforS =40000stepsresultsareaveragedover3random
seeds.AllsettingsuseFlashAttention[17],batchsizeof8,contextsizeof2048,andcosinelearningratedecayfroma
startinglearningrateof5e-5to1e-5with500stepsoflearningratewarmup.
Forthem=2,3settings,experimentswererunonaNVIDIARTX6000AdaGenerationGPU.Forthem=7setting,
experimentswererunonaNVIDIAA10080GBGPU.
Restrictedversusunrestricted. Boththerestrictedandunrestrictedsettingssharethesamelengthofthefinaltraining
runs(5000and40000steps,asabove).Theunrestrictedsettinggivesallmethodsupto10trainingrunstoinitializemixing
algorithmparameters,or10S steps,whiletherestrictedsettinggive0.5S steps.SeeTable11fortrainingbudgetallocations
ineachsetting.AIOLIandstratifiedsamplingdonotuseextratrainingruns.
Mixingalgorithmdetails. SeeTable12foraglossaryofhyperparameters.m=2:restrictedTable13,unrestrictedTable
14;m=3:restrictedTable15,unrestrictedTable16;m=7:restrictedTable17,unrestrictedTable18.
26
vixra
no
ssoL
pets-txeN
egnahcxekcats
no
ssoL
pets-txeNTable11:Trainingbudgetallocationsforrestrictedandunrestrictedsettings.
Setting m Method Runswithintrainingbudget
Unrestricted 2 DML 10runs,5000steps
Skill-it 2runs,5000steps
DoReMi 2runs,5000steps
DoGE 1run,5000steps
3 DML 10runs,5000steps
Skill-it 3runs,5000steps
DoReMi 2runs,5000steps
DoGE 1run,5000steps
7 DML 10runs,40000steps
Skill-it 7runs,40000steps
DoReMi 2runs,40000steps
DoGE 1run,40000steps
Restricted 2 DML 10runs,250steps
Skill-it 2runs,1250steps
DoReMi 2runs,1250steps
DoGE 1run,2500steps
3 DML 10runs,250steps
Skill-it 3runs,833steps
DoReMi 2runs,1250steps
DoGE 1run,2500steps
7 DML 10runs,2000steps
Skill-it 7runs,2814steps
DoReMi 2runs,10000steps
DoGE 1run,20000steps
InTable19,weprovidethemixtureproportionsforeachmethod(averagedacrosstrainingsteps)foreachdatasetonone
randomseed.InFigure6,weprovideallofAIOLIâ€™sproportiontrajectoriesthroughouttraininginboththeunrestrictedand
restrictedsettingsononerandomseedforthem=2settings.
Toperformtrainingsweepsandemulategridsearchesinstaticsettingsform=3,7,weoversampledfromtheDirichlet
withÎ±=1by4xthenumberofpointsandthenhierarchicallymergedclosestpointsintoacentroiduntilweobtainedx
points.Forexample,toobtain10pointsinthe7-dimensionalsimplexforSlimPajama,wewouldsample40pointsinthe
simplexandhierarchicallymergeclosestpointsuntil10pointsremain.Thisistoensurethatnear-duplicatepâ€™sarenot
includedinthesweep.ThisprocedureisusedinGridSearch(GS)andDMLinSection6andinouranalysisinSection4
AIOLI-specifichyperparameters Intheunrestrictedsetting,wefounditsometimeshelpfultouseanexponentialmoving
average with proportion Î³ over At for AIOLI. Formally, the standard pt update rule in Algorithm 1 can be unrolled as
pt+1 âˆp0exp(Î·(cid:80)t (cid:80)m AÂ¯Ï„ ),whichplacesequalweightoneveryAÂ¯Ï„ .ToincorporatetheEMA,wedefineA1 =AÂ¯1
j j Ï„=1 i=1 ij ij ema
and At = (1âˆ’Î³)AÂ¯t +Î³Atâˆ’1. We then use the update rule pt+1 âˆ p0exp(Î·At ). This allows AIOLI to decay the
ema ema j j ema
contributionsofAt,suchthatthevalueofptislessdependentonearlierproportionsinthetraining.
27Table12:Hyperparametersineachoftheonlinemixingmethods.HyperparametersforgridsearchandDMLarethenumber
ofrunsandnumberofstepsperrun,whicharedescribedabove.
Method Hyperparameter Type
AIOLI Â·numberofroundsT int
Â·sweepsk int
Â·proportionofroundÎ´dedicatedtoLEARNPARAMS float
Â·EGDlearningrateÎ· float
Â·Îµone-hotsmoothingfactor float
Â·EMAparameterÎ³ floatorNone
Skill-it Â·numberofroundsT int
Â·EGDlearningrateÎ· float
Â·multiplicativeweightswindow int
DoReMi Â·EGDlearningrateÎ· float
DoGE Â·EGDlearningrateÎ· float
Â·proportionoftrainingbatchthatconsistsofvalidation float
set.DoGEcomputesgradientsonthevalidationset
28Table13:Restrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=2(correspondingto
Table3results).
Datagroups Method Hyperparameter Value
arXiv/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.25
GitHub/C4 AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.1
Â·proportionofbatchforvalidationset 0.25
Books/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.25
29Table14:Unrestrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=2(corresponding
toTable2results).
Datagroups Method Hyperparameter Value
arXiv/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ 0.1
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.25
GitHub/C4 AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.3
Â·EMAparameterÎ³ 0.5
Skill-it Â·numberofroundsT 5
Â·EGDlearningrateÎ· 0.1
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.1
Â·proportionofbatchforvalidationset 0.25
Books/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.128
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.1
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.8
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.25
30Table15:Restrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=3(correspondingto
Table3results).
Datagroups Method Hyperparameter Value
arXiv/Books/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.288
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.5
CommonCrawl/GitHub/Wiki AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.288
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ None
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.5
31Table16:Unrestrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=3(corresponding
toTable2results).
Datagroups Method Hyperparameter Value
arXiv/Books/SE AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.288
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.1
Â·EMAparameterÎ³ 0.5
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.5
CommonCrawl/GitHub/Wiki AIOLI Â·numberofroundsT 20
Â·sweepsk 4
Â·proportionofroundÎ´ 0.288
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.3
Â·EMAparameterÎ³ 0.5
Skill-it Â·numberofroundsT 10
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.01
Â·proportionofbatchforvalidationset 0.5
Table17:Restrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=7(correspondingto
Table3results).
Datagroups Method Hyperparameter Value
SlimPajama,full AIOLI Â·numberofroundsT 20
Â·sweepsk 2
Â·proportionofroundÎ´ 0.07
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.2
Â·EMAparameterÎ³ 0.1
Skill-it Â·numberofroundsT 20
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.03
Â·proportionofbatchforvalidationset 0.5
32Table18:Unrestrictedhyperparametervaluesforeachdatamixingalgorithmforexperimentswherem=7(corresponding
toTable2results).
Datagroups Method Hyperparameter Value
SlimPajama,full AIOLI Â·numberofroundsT 20
Â·sweepsk 2
Â·proportionofroundÎ´ 0.07
Â·Îµone-hotsmoothingfactor 0.75
Â·EGDlearningrateÎ· 0.3
Â·EMAparameterÎ³ 0.1
Skill-it Â·numberofroundsT 20
Â·EGDlearningrateÎ· 0.2
Â·Multiplicativeweightswindow. 3
DoReMi Â·EGDlearningrateÎ· 0.01
DoGE Â·EGDlearningrateÎ· 0.1
Â·proportionofbatchforvalidationset 0.5
33Table19:Averageproportionsovertheentiretrainingtrajectoryfortheunrestrictedsetting,ononerandomseed.
Datagroups Method AverageProportions
arXiv/SE Gridsearch [0.4,0.6]
DML [0.404,0.596]
Skill-it [0.437,0.563]
DoReMi [0.37,0.63]
DoGE [0.624,0.376]
AIOLI [0.507,0.493]
GitHub/C4 Gridsearch [0.3,0.7]
DML [0.46,0.54]
Skill-it [0.583,0.417]
DoReMi [0.858,0.142]
DoGE [0.352,0.648]
AIOLI [0.505,0.495]
Books/SE Gridsearch [0.3,0.7]
DML [0.381,0.619]
Skill-it [0.316,0.684]
DoReMi [0.286,0.714]
DoGE [0.325,0.675]
AIOLI [0.456,0.544]
arXiv/Books/SE Gridsearch [0.291,0.306,0.403]
DML [0.245,0.277,0.477]
Skill-it [0.292,0.238,0.469]
DoReMi [0.318,0.180,0.502]]
DoGE [0.592,0.132,0.276]
AIOLI [0.331,0.336,0.333]
CC/GitHub/Wiki Gridsearch [0.291,0.306,0.403]
DML [0.157,0.472,0.371]
Skill-it [0.275,0.3,0.425]
DoReMi [0.101,0.714,0.185]]
DoGE [0.536,0.220,0.244]
AIOLI [0.3320.3340.333]
SlimPajama,full Gridsearch [0.202,0.022,0.28,0.038,0.018,0.376,0.064]
(A/B/C4/CC/G/SE/W) DML [0.042,0,0,0.579,0,0.249,0.013]
Skill-it [0.098,0.111,0.204,0.103,0.138,0.266,0.076]
DoReMi [0.08,0.047,0.057,0.11,0.467,0.078,0.157]
DoGE [0.056,0.162,0.343,0.28,0.038,0.067,0.051]
AIOLI [0.138,0.138,0.149,0.162,0.138,0.137,0.138]
34Arxiv/Stackexchange Github/C4 Book/Stackexchange
0.55
0.55 0.70
0.50
0.65
0.50
0.60 0.45
0.45
Aioli Unrestricted 0.55 0.40
0.40 Aioli+GS
Aioli+DML 0.50
0.35
0.35 A Ai io ol li i+ +S Dk oi Rll e-i Mt i 0.45
Aioli+DoGE 0.30
0.30 0.40
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
checkpoint checkpoint checkpoint
Figure6: AIOLIâ€™sproportionsthroughouttrainingforbothunrestrictedandrestrictedsettingsonArxiv/Stackexchange,
Github/C4,andBook/Stackexchange.ThesetrajectoriesshowthatAIOLImeaningfullyaltersthemixtureproportionsover
time.
E Additional Experiments
E.1 DownstreamTasks
Wefindthatlowerperplexityispositivelycorrelatedwithworseperformanceondownstreamtasks.Weevaluatedallmodels
trainedonSlimPajamaonARC-Challenge,ARC-Easy[16],BoolQ[15],HellaSwag[74],LAMBADA[48],OpenBookQA
[40], PiQA [9], and WinoGrande [52] using the Language Model Evaluation Harness [22] (Table 20). The correlation
betweenperplexityandthemacroaverageofourdownstreamtasksis0.529,indicatingthatlowerperplexityispredictiveof
worsedownstreamperformance.Infact,DMLobtainsthebestoverallperformance,eventhoughitomitsthreeoutofseven
datasetsinSlimPajama(seetheaverageproportionsinTable19).
Table20:DownstreamevaluationmetricsforvariousdatamixingmethodsaftertrainingonSlimPajamaacrossthreerandom
seedsintheunrestrictedsetting.
Method Average ARC-C ARC-E BoolQ HellaSwag LAMBADA OpenBookQA PiQA WinoGrande
Stratified 0.305 0.176 0.314 0.394 0.261 0.116 0.117 0.563 0.499
AIOLI 0.311 0.172 0.315 0.447 0.264 0.114 0.111 0.559 0.504
GS 0.322 0.176 0.329 0.502 0.262 0.117 0.124 0.568 0.500
DML 0.333 0.181 0.330 0.608 0.261 0.109 0.128 0.554 0.490
Skill-it 0.316 0.182 0.322 0.462 0.261 0.124 0.122 0.559 0.492
DoReMi 0.324 0.177 0.323 0.507 0.264 0.127 0.122 0.574 0.499
DoGE 0.314 0.173 0.313 0.471 0.262 0.116 0.115 0.557 0.504
Onepotentialreasonforthisdisparityisthedistributionshiftbetweenpre-trainingdataanddownstreamevaluationdata;
forexample,theDMLresultssuggestthattrainingonBooks,C4,andGithubisnotneededtodowellontheaboveselection
ofdownstreamtasks.Manyrecentworkshavealsonotedthatperplexityanddownstreamperformanceareuncorrelated
[36,58,68].Furthermore,Levyetal.[33]proposesaquestionansweringdatasetwheretheperplexityofthepretrained
model is positively correlated with performance, similar to our results. This mismatch between training objective and
downstreamevaluationsalsoextendstopost-training,wherebetterlearningofhumanpreferencesdoesnottranslatetobetter
win-rateagainstotherpost-trainedmodels[12].
Resolvingthedisconnectbetweentrainingobjectiveanddownstreamevaluationsisanareaofactiveresearch.Inthecase
ofdatamixing,AIOLIremainstheonlyalgorithminourteststhatrobustlyminimizesaveragetestperplexityâ€“essentially,
AIOLIachieveswhatitsetsouttoachieveintheLMOframeworkin(1).Conversely,otherdatamixingalgorithmsmight
beimplicitlydoingsomethingelsewithrespecttominimizingdownstreamevaluations.Consideringhowtoincorporate
downstreamevaluationsintodatamixingisafruitfulareaforfuturework.
E.2 Ablations
WeablateAIOLIbystudyingperformancewhentwokeypropertiesofAt(AppendixC.2.1)arechanged:whenT =1(i.e.,
At isonlylearnedonceatthebeginningoftrainingandusedthroughout),andwhenAt isassumedtobediagonal.We
evaluatethesetwoablationsintheunrestrictedsettingpresentedinSection6.1andTable2:
35
)viXrA(
noitroporp
)buhtiG(
noitroporp
)kooB(
noitroporpâ€¢ AIOLI-STATIC:WesetT =1inAlgorithm1.Thatis,welearnA1atthebeginningoftraining.WeusethisA1toset
p1,andusethisp1 fortheremainderofthetrainingrun.ThisapproachtestsifAt needstobeadjustedthroughout
training.
â€¢ AIOLI-DIAGONAL: We assume that each At is diagonal in this ablation. In particular, in LEARNPARAMS we do
At =Î² /pt,iratherthanAt =Pâˆ’1Î² foreachiâˆˆ[m]inline11.Thisapproachtestsifitissufficienttonotmodel
ii ii i i
cross-groupinteractionsandinsteadonlycapturehowmuchgroupiâ€™sperformanceimproveswhentrainedongroupi
itself.
For both AIOLI-STATIC and AIOLI-DIAGONAL, we use the same set of hyperparameters as AIOLI as described in
Appendix D. For AIOLI-STATIC, we additionally sweep over EGD learning rates {Î·,2Î·,3Î·,4Î·} where Î· is the EGD
learningrateusedbyAIOLI.
OurresultsareinTable21.WefindthatAIOLIoutperformsbothablationsin3outof6settings,andobtainsthelowest
testperplexityonaverageoverthesesettings.ThissuggeststhatbothT >1andmodelingoff-diagonalentriesareimportant
toAIOLIâ€™sconsistentperformanceacrossdatasets.
Table21:AblationsonAIOLI.Thetablereportsthedifferenceinaveragetestperplexitycomparedtostratifiedsampling.
Negativevalues(green)=improvement,andbolded=bestperformingmethodforgivendatasetting.A=Arxiv,B=Books,
GH=GitHub,SE=StackExchange,W=Wikipedia.AIOLIoutperformsablationsin3outof6settingsandattainsthelowest
testperplexityonaverage.
Method A/SE GH/C4 B/SE A/B/SE CC/GH/W SlimPajama Average
Stratified 16.532 35.991 47.192 35.114 41.583 26.426 33.806
AIOLI âˆ’0.205 âˆ’0.340 âˆ’0.439 âˆ’0.096 âˆ’0.196 âˆ’0.240 âˆ’0.253
AIOLI-STATIC âˆ’0.065 âˆ’0.333 âˆ’0.226 âˆ’0.213 0.092 âˆ’0.330 âˆ’0.179
AIOLI-DIAGONAL âˆ’0.182 âˆ’0.178 âˆ’0.354 âˆ’0.163 âˆ’0.215 âˆ’0.202 âˆ’0.216
F Why the method is called AIOLI
Anaioliisanemulsion,whereindividualcomponentsremainchemicallyseparatefromeachother,despitebeingcombined
intoonemixture.Similarly,ourAtmatrixisformedfromseparatetestruns(thept,1,...,pt,minSection5),despitebeing
combinedintooneupdateforpt.
36