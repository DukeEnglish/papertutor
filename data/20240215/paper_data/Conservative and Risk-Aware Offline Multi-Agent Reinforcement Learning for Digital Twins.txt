1
Conservative and Risk-Aware Offline Multi-Agent
Reinforcement Learning for Digital Twins
Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, and Hirley Alves
Abstract‚ÄîDigital twin (DT) platforms are increasingly re-
DigitalTwin
garded as a promising technology for controlling, optimizing,
and monitoring complex engineering systems such as next- Policies
ùíü ùùÖ
generationwirelessnetworks.Animportantchallengeinadopting
DT solutions is their reliance on data collected offline, lacking
direct access to the physical environment. This limitation is Offline Training
particularlysevereinmulti-agentsystems,forwhichconventional
multi-agent reinforcement (MARL) requires online interactions
with the environment. A direct application of online MARL ùë†,ùëü ùë†,ùëü
ùùÖùüè ùùÖùüè
schemes to an offline setting would generally fail due to the ùú∑
epistemicuncertaintyentailedbythelimitedavailabilityofdata.
In this work, we propose an offline MARL scheme for DT- ‚ãÆ Training data set ‚ãÆ
based wireless networks that integrates distributional RL and ùùÖùë∞ ùíü= ùë†ùë°,ùëéùë°,ùëüùë°,ùë†ùë°+1 ùùÖùë∞
conservative Q-learning to address the environment‚Äôs inherent ùëé ùú∑ ùëé
aleatoric uncertainty and the epistemic uncertainty arising from Data Collection Physical Twin Deployment
limited data. To further exploit the offline data, we adapt
the proposed scheme to the centralized training decentralized
execution framework, allowing joint training of the agents‚Äô
policies.TheproposedMARLscheme,referredtoasmulti-agent Fig. 1: A digital twin of a physical system may only have access to data
conservative quantile regression (MA-CQR) addresses general
collectedofflinefollowingsomefixedandunknownpoliciesœÄ
Œ≤
={œÄ Œ≤i}I
i=1
atthephysicaltwinconsistingofI agents.Basedonthisdataset,thedigital
risk-sensitive design criteria and is applied to the trajectory
twinwishestooptimizepoliciesœÄ={œÄi}I forthephysicalsystemwhile
planningproblemindronenetworks,showcasingitsadvantages. i=1
ensuringrobustnesstotheuncertaintyarisingfromthestochasticenvironment,
Index Terms‚ÄîDigital twins, offline multi-agent reinforcement fromthelimiteddata,andfromthelackofinteractionswiththeenvironment.
learning, distributional reinforcement learning, conservative Q-
learning, UAV networks
physical systems: digital twin (DT) platforms [2], [3]. A
I. INTRODUCTION DT system consists of a digital mirror of the physical twin
A. Context and Motivation (PT) counterpart built and maintained using data from the
PT. Originating in manufacturing, DTs have emerged as a
Recent advances in machine learning (ML), high-
promising solution for fields such as healthcare and next-
performance computing, cloudification, and simulation intel-
generation wireless networks, allowing algorithms and ML
ligence [1] have supported the development of a novel data-
modelstobeoptimizedandtestedvirtuallybeforedeployment
driven paradigm for the design, monitoring, and control of
onthePT[4],[5].Inthecontextofwirelessnetworks,DTsare
Eslam Eldeeb and Hirley Alves are with Centre for Wireless Communi- particularly well suited as a component in open radio access
cations (CWC), University of Oulu, Finland. (e-mail: eslam.eldeeb@oulu.fi; network architectures (RANs) [6], [7].
hirley.alves@oulu.fi). Houssem Sifaou and Osvaldo Simeone are with the
The DT generally updates its internal representation of the
King‚Äôs Communications, Learning & Information Processing (KCLIP) lab
within the Centre for Intelligent Information Processing Systems (CIIPS), PTandtheMLmodelsbeingtrainedonbehalfofthePTbased
Department of Engineering, King‚Äôs College London, WC2R 2LS London, on data received from the PT. The process of synchronizing
U.K. (e-mail: houssem.sifaou@kcl.ac.uk; osvaldo.simeone@kcl.ac.uk). Mo-
the DT is limited by the communication bottleneck between
hammad Shehab is with CEMSE Division, King Abdullah University of
ScienceandTechnology(KAUST),Thuwal23955-6900,SaudiArabia(email: DT and PT [7], [8]. As a result, an important challenge
mohammad.shehab@kaust.edu.sa). in adopting DT solutions is their reliance on data collected
The work of E. Eldeeb and H. Alves was partially supported by the
offline, lacking direct access to the physical system. This
Research Council of Finland (former Academy of Finland) 6G Flagship
Programme (Grant Number: 346208) and by the European Commission limitation is particularly severe in multi-agent systems, for
throughtheHexa-X-II(GAno.101095759).TheworkofH.SifaouandO. which conventional multi-agent reinforcement (MARL) re-
Simeone was partially supported by the European Union‚Äôs Horizon Europe
quires online interactions with the environment [9].
project CENTRIC (101096379). O. Simeone was also supported by the
Open Fellowships of the EPSRC (EP/W024101/1) by the EPSRC project A direct application of online MARL schemes to an offline
(EP/X011852/1),andbyProjectREASON,aUKGovernmentfundedproject
setting would generally fail due to the epistemic uncertainty
undertheFutureOpenNetworksResearchChallenge(FONRC)sponsoredby
theDepartmentofScienceInnovationandTechnology(DSIT). entailed by the limited availability of data. In particular, even
The second author has contributed to the problem definitions and the in the case of a single agent, offline reinforcement learning
experiments.Thethirdauthorhashadanactiveroleindefiningtheproblems,
(RL) may over-estimate the quality of given actions that
aswellasinwritingthetext,whilethelasttwoauthorshavehadasupervisory
roleandhaverevisedthetext. happened to perform well during data collection due to the
4202
beF
31
]GL.sc[
1v12480.2042:viXra2
inherent stochasticity and outliers of the environment [10]. A popular risk measure for use in DRL is the conditional
This problem can be addressed in online RL via exploration, value at risk (CVaR) [20]‚Äì[23], which evaluates the average
trying actions, and modifying return estimates based on envi- performance by focusing only on the lower tail of the return
ronmental feedback. However, exploration is not feasible in distribution. Furthermore, a state-of-the-art DRL strategy is
offline RL, as policy design is based solely on the offline quantile-regression deep Q-network (QR-DQN), which ap-
dataset. Furthermore, in multi-agent systems, this problem is proximates the return distribution by estimating N uniformly
exacerbated by the inherent uncertainty caused by the non- spaced quantiles [19].
stationarybehaviorofotheragentsduringtraining[11,Chapter
11]. C. Related Work
Inthispaper,weproposeanovelofflineMARLstrategythat Offline MARL: Offline MARL solutions have been proposed
addressestheoveralluncertaintyattheDTabouttheimpactof inseveralpreviousworksbyadaptingtheideaofconservative
actionsonthePT.Theintroducedapproachesaretermedmulti- offline learning to the context of multi-agent systems [24]‚Äì
agent conservative independent quantile regression (MA- [28]. Specifically, conservative estimates of the value function
CIQR) via independent learning and multi-agent conservative in a decentralized fashion are obtained in [24] via value
centralized quantile regression (MA-CCQR) via centralized deviation and transition normalization. Several other works
training and decentralized execution. These aproaches inte- proposed centralized learning approaches. The authors in [25]
grate distributional RL [11] and offline RL [12] to support leveraged first-order policy gradients to calculate conserva-
a risk-sensitive multi-agent design that mitigates impairments tive estimates of the agents‚Äô value functions. The work [26]
arising from access to limited data from the PT. We showcase presented a counterfactual conservative approach for offline
the performance of MA-CIQR and MA-CCQR by focusing MARL, while [27] introduced a framework that converts
on the problem of designing trajectories of unmanned aerial global-level value regularization into equivalent implicit local
vehicles (UAVs) used to collect data from sensors in an value regularization. The authors in [28] addressed the over-
Internet-of-Things (IoT) scenario [13]‚Äì[15] (see Fig. 1). estimation problem using implicit constraints.
Overall, all of these works focused on risk-neutral objec-
B. Offline RL and Distributional RL
tives,hencenotmakinganyprovisionstoaddressrisk-sensitive
OfflineRLhasgainedincreasinginterestinrecentyearsdue criteriasuchasCVaR.Inthisregard,thepaper[23]combined
to its wide applicability to domains where online interaction distributional RL and conservative Q-learning to develop a
with the environment is impossible or presents high costs and risk-sensitive algorithm, but only for single-agent settings.
risks. Offline RL relies on a static offline transition dataset Regarding applications of offline RL to wireless systems,
collected from the PT using some behavioral policy. The therecentwork[29]investigatedaradioresourcemanagement
behavioralpolicyisgenerallysuboptimalandmaybeunknown problembycomparingtheperformanceofseveralsingle-agent
to the designer [10]. The reliance on a suboptimal policy offline RL algorithms.
for data collection distinguishes offline RL from imitation Applications of MARL to wireless systems: Due to the multi-
learning, in which the goal is reproducing the behavior of objectiveandmulti-agentnatureofmanycontrolandoptimiza-
an expert policy [16]. The discrepancy between the behavior tion problems in wireless networks, MARL has been adopted
and optimized policies creates a distributional shift between asapromisingsolutioninrecentyears.Forinstance,relatedto
trainingdataanddesignobjective.Thisshiftcouldberesolved our contribution, the work in [30] proposed an online MARL
by collecting more data, but this is not possible in an offline algorithmtojointlyminimizetheage-of-information(AoI)and
setting. Therefore, the distributional shift contributes to the the transmission power in IoT networks with traffic arrival
epistemic uncertainty of the agent. prediction, whereas the authors in [31] leveraged MARL for
Several approaches have been proposed to address this AoI minimization in UAV-to-device communications. More-
problem in offline RL. One class of methods constrains the over, MARL was used in [32] for resource allocation in UAV
differencebetweenthelearnedandbehaviorpolicies[17].An- networks. The work [33] developed a MARL-based solution
otherpopularapproachistolearnconservativeestimatesofthe for optimizing power allocation dynamically in wireless sys-
action-value function or Q-function. Specifically, conservative tems.Theauthorsin[34]usedMARLfordistributedresource
Q-learning (CQL), proposed in [12], penalizes the values of managementandinterferencemitigationinwirelessnetworks.
the Q-function for out-of-distribution (OOD) actions. OOD ApplicationsofdistributionalRLtowirelesssystems:Distribu-
actions are those whose impact is not sufficiently covered by tional RL has been recently leveraged in [35] to carry out the
the data set. optimizationforadownlinkmulti-usercommunicationsystem
Apart from offline RL via CQL, the proposed scheme with a base station assisted by a reconfigurable intelligent re-
builds on distributional RL (DRL), which is motivated by the flector(IR).Meanwhile,reference[36]focusedonthecaseof
inherent aleatoric uncertainty caused by the stochasticity of mmWave communications with IRs on a UAV. Distributional
the environment [18]‚Äì[20]. Rather than targeting the average RL has also been used in [37] for resource management in
return as in conventional RL, DRL maintains an estimate of network slicing.
the distribution of the return. This supports the design of All in all, to the best of our knowledge, our work in this
risk-sensitive policies that disregard gains attained via risky paper is the first to integrate conservative offline RL and
behavior, favoring policies that ensure satisfactory worst-case distributional MARL, and it is also the first to investigate the
performance levels instead. application of offline MARL to wireless systems.3
TABLEI:Abbreviations
D. Main Contributions
AoI Age-of-information
This work introduces MA-CQR, a novel offline MARL CDF Cumulativedistributionfunction
scheme that supports optimizing risk-sensitive design criteria CQL ConservativeQ-learning
CTDE Centralizedtraininganddecentralizedexecution
such as CVaR. MA-CQR is evaluated on the relevant problem
CVaR Conditionalvalueatrisk
of UAV trajectory design for IoT networks. The contributions DQN DeepQ-networks
of this paper are summarized as follows. DRL Distributionalreinforcementlearning
DT Digitaltwin
‚Ä¢ WeproposeMA-CQR,anovelconservativeanddistribu- MA-CCQL Multi-agentconservativecentralizedQ-learning
tional offline MARL solution. MA-CQR leverages quan- MA-CCQR Multi-agentconservativecentralizedquantileregression
MA-CIQL Multi-agentconservativeindependentQ-learning
tile regression (QR) to support the optimization of risk-
MA-CIQR Multi-agentconservativeindependentquantileregression
sensitive design criteria and CQL to ensure robustness MA-CQR Multi-agentconservativequantileregression
to OOD actions. As a result, MA-CQR addresses both MARL Multi-agentreinforcementlearning
OOD Out-of-distribution
the epistemic uncertainty arising from the presence of
PT Physicaltwin
limited data and the aleatoric uncertainty caused by the QR-DQN Quantile-regressionDQN
randomness of the environment. UAV Unmannedaerialvehicles
‚Ä¢ We present two versions of MA-CQR with different
TABLEII:Notations
levels of coordination among the agents. In the first
version,referredtoasMA-CIQR,theagents‚Äôpoliciesare I Numberofagents
optimized independently. In the second version, referred
st OverallstateofthePTsystemattimestept
at Jointactionofallagentsattimestept
to as MA-CCQR, we leverage value decomposition tech- ai Actionofagentiattimestept
t
niques that allow centralized training and decentralized rt Immediaterewardattimestept
Œ≥ Discountfactor
execution [38].
Q(s,a) Q-function
‚Ä¢ To showcase the proposed schemes, we consider atra- Z(s,a) Returnstartingfrom(s,a)
jectory optimization problem in UAV networks [30]. As P(st+1|st,at) Transitionprobability
illustratedinFig.1,thesystemcomprisesmultipleUAVs œÄi(ai t|st) Policyofagenti
P Distributionofthereturn
collecting information from IoT devices. The multi- Z(s,a)
R(rt|st,at) Stationaryrewarddistribution
objective design tackles the minimization of the AoI for Œæ Risktolerancelevel
data collected from the devices and the overall transmit JCVaR CVaRriskmeasure
Œæ
power consumption. We specifically exploit MA-CQR to F Z‚àí œÄ1(cid:0) Œæ(cid:1) InverseCDFofthereturn
D OfflinedatasetcollectedatthePT
design risk-sensitive policies that avoid excessively risky
trajectories in the pursuit of larger average returns.
Œ∏ ji(s,a) QuantileestimateofthedistributionP Zi(s,a)(Œ∏i)
Œ∂ (u) QuantileregressionHuberloss
œÑ
‚Ä¢ Numerical results demonstrate that MA-CIQR and MA- ‚àÜi(k) TDerrorsevaluatedwiththequantileestimates
jj‚Ä≤
CCQR versions yield faster convergence and higher re- ofagenti
turns than the baseline algorithms. Furthermore, both Œ± CQLhyperparameter
M Numberofdevicesinthesystem
schemes can avoid risky trajectories and provide the
Am AoIofdevicemattimestept
t
best worst-case performance. Experiments also depict gi,m Channelgainbetweenagentianddevicem
t
that centralized training provides faster convergence and attimestept
requires less offline data. P ti,m Transmissionpowerfordevicemtocommunicatewith
agentiattimestept
The rest of the paper is organized as follows. Section II de- p risk Riskprobability
scribestheMARLsettingandthedesignobjective.SectionIII P risk Riskpenalty
introduces distributional RL and conservative Q-Learning. In
section IV, we present the proposed MA-CQR algorithm. In
setting and formulate the offline learning problem. Tables I
Section VI, we provide numerical experiments on trajectory
and II summarize the list of abbreviations and notations.
optimization in UAV networks. Section VII concludes the
paper.
A. Multi-Agent Setting
II. PROBLEMDEFINITION
Consider an environment characterized by a time-variant
WeconsiderthesettingillustratedinFig.1,whereI agents state s t, where t = 1,2,... is the discrete time index. At
act in a physical environment that evolves in discrete time time step t, each agent i takes action ai ‚àà Ai within some
t
as a function of the agents‚Äô actions and random dynamics. discrete action space Ai. We denote by a t = (cid:2) a1 t,¬∑¬∑¬∑ ,aI t(cid:3)
The design of the agents‚Äô policies œÄ = {œÄi}I i=1 is carried the vector of actions of all agents at timestep t. The state s t
out at a central unit ‚Äì the digital twin (DT) in Fig. 1 ‚Äì that evolves according to a transition probability P(s t+1|s t,a t) as
has only access to a fixed dataset D, while not being able to a function of the current state s t and of the action vector a t.
interact with the physical system. The dataset D is collected The transition probability P(s t+1|s t,a t) is stationary, i.e., it
offline by allowing the agents to act in the environment does not vary with time index t.
according to arbitrary, fixed, and generally unknown policies We focus on a fully observable multi-agent reinforcement
œÄ = {œÄi}I . In this section, we describe the multi-agent learning setting, in which each agent i has access to the full
Œ≤ Œ≤ i=14
system state s and produces action ai by following a policy
t t
œÄi(ai |s ).
t t
B. Design Goal
As illustrated in Fig. 1, the DT aims at finding the optimal
policiesœÄ (a|s)={œÄi(a|s)}I thatmaximizeariskmeasure CVaR[Z]
‚àó ‚àó i=1
œÅ(¬∑) of the return ZœÄ =(cid:80)‚àû Œ≥tr , which we write as
t=0 t
J (œÄ)=œÅ[ZœÄ], (1)
œÅ
where 0 < Œ≥ < 1 is a given discount factor. The distribution
of the return ZœÄ depends on the policies œÄ through the
distribution of the trajectory T = (s ,a ,r ,s ,a ,r ,...),
0 0 0 1 1 1
which is given by
‚àû
(cid:89)
P(T)=P(s ) œÄ(a |s )R(r |s ,a )P(s |s ,a ),
0 t t t t t t+1 t t Fig. 2: Illustration of the conditional value-at-risk (CVaR). The quantile
t=0 function F‚àí1(Œæ) is plotted as a function of the risk tolerance level Œæ. The
(2) shadedareaZ representingthelowertailofthedistributiondepictstheŒæ-level
with œÄ(a |s )=(cid:81)I œÄi(ai |s ) being the joint conditional CVaR.
t t i=1 t t
distribution of the agents‚Äô actions; P(s ) being a fixed initial
0
distribution; and R(r | s ,a ) being the stationary reward
t t t
distribution. C. Offline Multi-Agent Reinforcement Learning
The standard choice for the risk measure in (1) is the
Conventional MARL [39] assumes that agents optimize
expectation œÅ[¬∑]=E[¬∑], yielding the standard criterion theirpoliciesœÄ ={œÄi(a|s)}I viaanonlineinteractionwith
i=1
theenvironment,allowingfortheexplorationofnewactionsa
Javg(œÄ)=E(cid:2) ZœÄ(cid:3)
. (3)
t
asafunctionthestates .Inthispaper,asillustratedinFig.1,
t
we assume that the design of policies is carried out at the
The average criterion in (3) is considered to be risk neutral,
DT on the basis solely of the availability of an offline dataset
as it does not directly penalize worst-case situations, catering D = {(s,a,r,s‚Ä≤)} of transitions (s,a,r,s‚Ä≤). Each transition
only to the average performance.
follows the stationary marginal distribution from (2), with
In stochastic environments where the level of aleatoric policyœÄ(a|s)givenbythefixedandunknownbehaviorpolicy
uncertainty caused by the transition probability and/or the œÄ (a|s)=(cid:81)I œÄi(ai|s).
Œ≤ i=1 Œ≤
reward distribution is high, maximizing the expected return
may not be desirable since the return ZœÄ has high variance.
In such scenarios, designing risk-sensitive policies may be III. BACKGROUND
preferabletoenhancetheworst-caseoutcomeswhilereducing
the average performance (3). In this section, we present a brief review of distributional
RL, as well as of offline RL via CQL for a single agent
A common risk-sensitive measure is the conditional value-
model [12]. This material will be useful to introduce the
at-risk (CVaR) [21], which is defined as the conditional mean
proposedmulti-agentofflineDRLsolutioninthenextsection.
JCVaR(œÄ)=E(cid:2) ZœÄ |ZœÄ ‚â§F‚àí1(cid:0) Œæ(cid:1)(cid:3) , (4)
Œæ ZœÄ
where
F‚àí1(cid:0) Œæ(cid:1)
is the inverse cumulative distribution function
A. Distributional Reinforcement Learning
ZœÄ
(CDF) of the return ZœÄ for some Œæ ‚àà [0,1], i.e., the Œæ-th
Distributional RL aims at optimizing the agent‚Äôs policy,
quantileofthedistributionofthereturn.TheCVaR,illustrated œÄ, while accounting for the inherent aleatoric uncertainty
in Fig. 2, focuses on the lower tail of the return distribution
associated with the stochastic environment. To this end, it
by neglecting values of the return that are larger than the Œæ-
tracks the return‚Äôs distribution, allowing the minimization of
th quantile
F‚àí1(cid:0) Œæ(cid:1)
. Accordingly, the probability Œæ represents
ZœÄ an arbitrary risk measure, such as the CVaR.
the risk tolerance level, with Œæ =1 recovering the risk-neutral
Toelaborate,letusdenotetherandomvariablerepresenting
objective (3). The CVaR can also be written as the integral of
the return starting from a given state-action pair (s,a) as
the quantile function F Z‚àí œÄ1(Œæ) as ZœÄ(s,a). Taking the expectation of the return ZœÄ(s,a) over
distribution (2) yields the state-action value function, also
1(cid:90) Œæ
JCVaR(œÄ)= F‚àí1(u)du. (5) known as Q-function, as
Œæ Œæ ZœÄ
0
QœÄ(s,a)=E[ZœÄ(s,a)]. (6)
nruteR5
Classical Q-learning algorithms learn the optimal policy œÄ‚àó B. Conservative Q-Learning
by finding the optimal Q-function Q(s,a) as the unique fixed Conservative Q-learning (CQL) is a Q-learning variant that
point of the Bellman optimality operator [40]
addresses epistemic uncertainty in offline RL. Specifically, it
(cid:20) (cid:21)
tackles the uncertainty arising from the limited available data,
Q(s,a)=E r+Œ≥ maxQ(s‚Ä≤,a‚Ä≤) , (7)
a‚Ä≤‚ààA which may cause some actions to be OOD due to the lack of
exploration. This way, CQL is complementary to QR-DQN,
with average evaluated with respect to the random variables
(r,s‚Ä≤) ‚àº R(r|s,a)P(s‚Ä≤|s,a). The optimal policy œÄ‚àó for the which,instead,targetstheinherentaleatoricuncertaintyinthe
stochastic environment.
average criterion (3) is directly obtained from the optimal Q-
To introduce CQL, let us first review conventional offline
function as
(cid:26) (cid:27) DQN [10], which approximates the solution of the Bellman
œÄ‚àó(a|s)=1 a=arg max Q(s,a) , (8) optimalitycondition(7)byiterativelyminimizingtheBellman
a‚ààA
loss
with 1{¬∑} being the indicator function. (cid:34)(cid:18) (cid:19)2(cid:35)
Similarly, for any risk measure œÅ[¬∑], one can define the L(Q,QÀÜ(k))=EÀÜ r+Œ≥maxQÀÜ(k)(s‚Ä≤,a‚Ä≤)‚àíQ(s,a) ,
distributional Bellman optimality operator for the random a‚Ä≤‚ààA
return Z(s,a) as [18], [19] (13)
(cid:18) (cid:19)
Z(s,a)=D r+Œ≥Z s‚Ä≤,arg max œÅ[Z(s‚Ä≤,a‚Ä≤)] , (9) where EÀÜ[¬∑] is the empirical average over samples (s,a,r,s‚Ä≤)
a‚Ä≤‚ààA from the offline dataset D; QÀÜ(k) is the current estimate of the
where equality holds regarding the distribution of the random optimal Q-function Q at iteration k; and the optimization is
variables on the left- and right-hand sides, and the random over function Q(s,a), which is typically modeled as a neural
variables (r,s‚Ä≤) are distributed as in (7). The optimal policy network.Thetermr+Œ≥max QÀÜ(k)(s‚Ä≤,a‚Ä≤)‚àíQ(s,a)isalso
a‚Ä≤‚ààA
for the general criterion (1) can be expressed directly as a known as the TD-error.
function of the optimal Z(s,a) in (9) as [18], [19] The maximization over the actions in the TD error in
(cid:26) (cid:27) (13) may yield over-optimistic return estimates when the Q-
œÄ(a|s)=1 a=arg maxœÅ[Z(s,a)] . function is estimated using offline data. In fact, a large value
a‚ààA
of the estimated maximum return max Q(s,a) may be
Quantile regression DQN (QR-DQN) [19] estimates the
a‚Ä≤‚ààA
obtained based purely on the randomness in the environment
distribution P of the optimal return Z(s,a) by approx-
Z(s,a) during data collection. This uncertainty could be resolved by
imating it via a uniform mixture of Dirac functions centered
collecting additional data. However, this is not possible in an
at N values {Œ∏ (s,a)}N , i.e.,
j j=1 offline setting, and hence one should consider such actions as
PÀÜ Z(s,a)(Œ∏)=
N1 (cid:88)N
Œ¥ Œ∏j(s,a). (10)
O thO eD ep[ is1 t0 e] m, i[ c41 u] n, ca en rd tac ino tu yn ot fth te here Dsu Tl .ting uncertainty as part of
j=1 To account for this issue, the CQL algorithm adds a
Each value Œ∏ (s,a) in (10) is an estimate of the quantile regularization term to the objective in (13) that penalizes
j
F‚àí1 (œÑÀÜ ) of distribution P corresponding to the quan- excessively large deviations between the maximum estimated
Z(s,a) j Z(s,a)
tile target œÑÀÜ j =(œÑ j‚àí1‚àíœÑ j)/2, with œÑ j =j/N for 1‚â§j ‚â§N. return max a‚Ä≤ (cid:80)‚ààAQ(s,a) (cid:0), approxi (cid:1)mated with the differentiable
Note that {Œ∏ (s,a)}N are estimated via quantile regression, quantity log exp Q(s,aÀú) , and the average value of
j j=1 aÀú‚ààA
which is achieved by modeling the function mapping (s,a) to Q(s,a) in the data set D as
tt ah ke eN
s
av sa tl au te es a{ sŒ∏ ij n( ps u, ta ,) a} nN j= d1 oa us tpa utsne tu hr eal esn te imtw ao terk
d
[ Œ∏1 j9 (] s, ,aw )hi fc oh
r
L CQL(Q,QÀÜ(k))= 21 L(Q,QÀÜ(k)) (14)
all actions a‚ààA . (cid:20) (cid:21)
The neural network is trained by minimizing the loss +Œ±EÀÜ log(cid:88) exp(cid:0) Q(s,aÀú)(cid:1) ‚àí Q(s,a) ,
aÀú‚ààA
N N
1 (cid:88)(cid:88) Œ∂ (cid:0) ‚àÜ (cid:1) , where Œ±>0 is a hyperparameter [12].
N2 œÑÀÜj jj‚Ä≤
j=1j‚Ä≤=1 AcombinationofQR-DQNandCQLwasproposedin[23]
for a single-agent setting to address risk-sensitive objectives
where ‚àÜ are the temporal difference (TD) errors corre-
jj‚Ä≤
inofflinelearning.Thisapproachappliesaregularizationterm
sponding to the quantile estimates, i.e.,
as in (14) to the distributional Bellman operator (9). The next
‚àÜ =r+Œ≥Œ∏ (s‚Ä≤,a‚Ä≤)‚àíŒ∏i(s,ai), (11)
jj‚Ä≤ j‚Ä≤ j section will introduce an extension of this approach for the
with a‚Ä≤ = arg max 1 (cid:80)N Œ∏ (s‚Ä≤,a), and Œ∂ is the multi-agent scenario under study in this paper.
a‚ààA N j‚Ä≤=1 j‚Ä≤ œÑ
quantile regression Huber loss defined as
(cid:40) IV. OFFLINECONSERVATIVEDISTRIBUTIONALMARL
‚àí1u2|œÑ ‚àí1{u<0}|, if |u|‚â§1
Œ∂ (u)= 2 (12) WITHINDEPENDENTTRAINING
œÑ (cid:0) |u|‚àí 1(cid:1) |œÑ ‚àí1{u<0}|, otherwise.
2 This section proposes a novel offline conservative distribu-
We refer the reader to [19] for more details about the tional independent Q-learning approach for MARL problems.
theoretical guarantees and practical implementation of QR- The proposed method combines the benefits of distributional
DQN. RL and CQL to address the risk-sensitive objective (1) in6
Algorithm1:ConservativeIndependentQ-learningfor the inherent stochasticity of the environment. This section
Offline MARL (MA-CIQL) introduces a risk-sensitive Q-learning algorithm for offline
Input: Discount factor Œ≥, learning rate Œ∑, conservative MARL to address the more general design objective (4) for
penalty constant Œ±, number of agents I, number of some risk tolerance level Œæ.
training iterations K, number of gradient steps G, The proposed approach, which we refer to as multi-agent
and offline dataset D conservative independent quantile regression (MA-CIQR),
Output: Optimized Q-functions Qi(s,ai) for maintains an estimate of the lower tail of the distribution of
i=1,...,I the return Zi(s,a), up to the risk tolerance level Œæ, for each
Initialize network parameters agent i. This is done in a manner similar to (10) by using N
for iteration k in {1,...,K} do estimated quantiles, i.e.,
for gradient step g in {1,...,G} do N
Sample a batch B from the dataset D PÀÜ (Œ∏i)= 1 (cid:88) Œ¥ . (16)
for agent i in {1,...,I} do
Zi(s,a) N Œ∏ ji(s,a)
j=1
Estimate the MA-CIQL loss L
MA-CIQL Generalizing(10),however,thequantityŒ∏i(s,a)isanestimate
in (15) j
ofthequantileF‚àí1 (œÑÀÜ ),withœÑÀÜ = œÑj‚àí1‚àíœÑj andœÑ =Œæj/N
Perform a stochastic gradient step based on Zi(s,a) j j 2 j
for 1‚â§j ‚â§N. This way, only the quantiles of interest cover
the estimated loss
end the return distribution up to the Œæ-th quantile.
end At each iteration k, for each agent, i, the DT updates the
end distribution (16) by minimizing a loss function that combines
Return Qi(s,ai)=QÀÜi(K)(s,ai) for i=1,...,I the quantile loss used by QR-DQN and the conservative
penalty introduced by CQL. Specifically, the loss function of
MA-CIQR is given by
m inu Flt ii g-a .g 1e .n Tt hsy est ae pm prs ob aa cs he ed so sn tuo df ifl edine heo rp eti am pi pz la yti ao nna int dth epe eD ndT ea ns
t L MA-CIQR(Œ∏i,Œ∏ÀÜi(k))=
2N1 2EÀÜ(cid:88)N (cid:88)N
Œ∂
œÑÀÜj(cid:16)
‚àÜi j( jk
‚Ä≤)(cid:17)
(17)
Q-learning approach, whereby learning is done separately for j=1j‚Ä≤=1
e ma ec th hoa dg sen bt a. seT dhe onne tx ht es ce ec nti to ran lizw ei dll trs atu ind iy ngm ao nr de dso ecp eh nis trti ac la izte ed
d
+Œ±EÀÜ(cid:34) N1 (cid:88)N (cid:34)
log
(cid:88) exp(cid:0)
Œ∏
ji(s,aÀúi)(cid:1)
‚àíŒ∏
ji(s,ai)(cid:35)(cid:35)
,
execution (CTDE) framework. j=1 aÀúi‚ààAi
where Œ∂ (u) is the quantile regression Huber loss defined in
œÑ
A. Multi-Agent Conservative Independent Q-Learning (12) and ‚àÜi(k) are the TD errors evaluated with the quantile
jj‚Ä≤
estimates as
Wefirstpresentamulti-agentversionofCQL,referredtoas
multi-agentconservativeindependentQ-learning(MA-CIQL), ‚àÜi(k) =r+Œ≥Œ∏ÀÜi(k)(s‚Ä≤,a‚Ä≤i)‚àíŒ∏i(s,ai), (18)
jj‚Ä≤ j‚Ä≤ j
for the offline MARL problem. As in its single-agent version
described in the previous section, MA-CIQL addresses the where a‚Ä≤i = arg max ai‚ààAi N1 (cid:80)N j‚Ä≤=1Œ∏ÀÜ ji( ‚Ä≤k)(s‚Ä≤,ai). Note that
averagecriterion(3),aimingtomitigatetheeffectofepistemic the TD error ‚àÜi(k) is obtained by using the j‚Ä≤-th quantile
jj‚Ä≤
uncertainty caused by OOD actions. of the current k-th iteration? to estimate the return as r +
To this end, for each agent i, the DT maintains a separable Œ≥Œ∏ÀÜi(k)(s‚Ä≤,a‚Ä≤i), while considering the j-th quantile Œ∏i(s,ai) as
j‚Ä≤ j
Q-function Qi(s,ai), which is updated at each iteration k by the quantity to be optimized.
approximately minimizing the loss The corresponding optimized policy is finally obtained as
Ô£± Ô£º
L (Qi,QÀÜi(k))= 1 L(Qi,QÀÜi(k)) (15) Ô£≤ 1 (cid:88)N Ô£Ω
MA-CIQL 2 œÄi(ai|s)=1 ai =arg max Œ∏i(s‚Ä≤,ai) . (19)
+Œ±EÀÜ(cid:20) log(cid:18) (cid:88) exp(Qi(s,aÀúi))(cid:19) ‚àíQi(s,ai)(cid:21)
,
Ô£≥ ai‚ààAi N j=1 j Ô£æ
By (5), the objective in (19) is an estimate of the CVaR at
aÀúi‚ààAi
the risk tolerance level Œæ. The pseudocode of the MA-CIQR
which is the multi-agent version of (14) over the Q-function
algorithmisprovidedinAlgorithm2.AsforMA-CIQL,MA-
Qi, where L(Qi,QÀÜi(k)) is the offline DQN loss in (13) and
CIQR applies separately across all agents.
QÀÜi(k) is the estimate of the Q-function of agent i at the k-th
iteration. Algorithm 1 summarizes the MA-CIQL algorithm V. OFFLINECONSERVATIVEDISTRIBUTIONALMARL
for offline MARL. Note that the algorithm applies separately WITHCENTRALIZEDTRAINING
toeachagentandisthusanexampleofindependentper-agent
The independent learning strategies studied in the previous
learning.
section may fail to yield coherent policies across different
agents. This section addresses this issue by introducing meth-
B. Multi-Agent Conservative Independent Quantile-
odsbasedonvaluedecomposition[38].Specifically,weadopt
Regression
the CTDE framework, which supports the optimization of
MA-CIQL can only target the average criterion (3), thus the agents‚Äô policies based on a global loss, along with the
not accounting for risk-sensitive objectives that account for decentralized execution of the optimized policies.7
Algorithm 2: Conservative Independent Quantile Re- where the function QÀúi(s,ai) indicates the contribution of the
gression for Offline MARL (MA-CIQR) i-th agent to the overall Q-function. For conventional offline
Input: Discount factor Œ≥, learning rate Œ∑, number of DQN, the Bellman loss (13) is minimized over the functions
quantiles N, conservative penalty constant Œ±, number {QÀúi(s,ai)}I i=1.Thisproblemcorrespondstotheminimization
of agents I, number of training iterations K, number of the global loss
of gradient steps G, offline dataset D, and CVaR (cid:34)(cid:32) I
parameter Œæ L({QÀúi}I ,{QÀÜi(k)}I )=EÀÜ r+Œ≥(cid:88) max QÀÜi(k)(s‚Ä≤,ai)
i=1 i=1
Output: Optimized quantile estimates {Œ∏ ji(s,ai)}N
j=1
i=1aÀúi‚ààAi
for all i=1,...,I I (cid:33)2(cid:35)
Define œÑ =Œæi/N, i=1,...,N ‚àí(cid:88) QÀúi(s,ai) , (21)
i
Initialize network parameters for each agent i=1
for iteration k in {1,...,K} do whereQÀÜi(k) isthecurrentestimateofthecontributionofagent
for gradient step g in {1,...,G} do i. In practice, every function QÀúi(s,ai) is approximated using
Sample a batch B from the dataset D
a neural network. Furthermore, the policy of each agent is
for agent i in {1,...,I} do
obtained from the optimized function QÀúi(s,ai) as
for j in {1,...,N} do
for j‚Ä≤ in {1,...,N} do (cid:26) (cid:27)
œÄi(ai|s)=1 ai =arg max QÀúi(s,ai) . (22)
Calculate TD errors ‚àÜ ji( jk ‚Ä≤) using ai‚ààAi
(18)
The same approach can be adopted to enhance MA-CIQL
end
by using (20) in the loss (15). This yields the loss
end
Estimate the MA-CIQR loss L MA-CIQR in L ({QÀúi}I ,{QÀÜi}I )= 1 L({QÀúi}I ,{QÀÜi}I )
(17) MA-CCQL i=1 i=1 2 i=1 i=1
Perform a stochastic gradient step based on +Œ±EÀÜ(cid:88)I (cid:20) log(cid:18) (cid:88) exp(QÀúi(s,aÀúi))(cid:19) ‚àíQÀúi(s,ai)(cid:21)
, (23)
the estimated loss
end i=1 aÀúi‚ààAi
end
with L({QÀúi}I ,{QÀÜi}I ) defined in (21). The obtained
end i=1 i=1
scheme, whose steps are detailed in Algorithm 3, is referred
Return {Œ∏i(s,ai)}N ={Œ∏ÀÜi(K)(s,ai)}N for all
j j=1 j j=1 to as multi-agent conservative centralized Q-learning (MA-
i=1,...,I
CCQL). The optimized policy is given in (22).
Algorithm 3: Conservative Centralized Q-learning for B. Multi-AgentConservativeCentralizedQuantile-Regression
Offline MARL (MA-CCQL) TheCTDEapproachbasedonthevaluedecomposition(20)
Input: Discount factor Œ≥, learning rate Œ∑, conservative canalsobeappliedtoMA-CIQRtoobtainacentralizedtrain-
penalty constant Œ±, number of agents I, number of ing version referred to as multi-agent conservative centralized
training iterations K, number of gradient steps G, quantile regression (MA-CCQR).
and offline dataset D To this end, we first recall that the lower tail of the
Output: Optimized Q-functions Qi(s,ai) for distribution of Z(s,a) is approximated by MA-CIQR as in
i=1,...,I (16) using the estimates of the quantiles F‚àí1 (œÑÀÜ ), with
Z(s,a) j
Initialize network parameters for each agent œÑÀÜ = œÑj‚àí1‚àíœÑj and œÑ = Œæj/N for 1 ‚â§ j ‚â§ N. To jointly
j 2 j
for iteration k in {1,...,K} do
optimize the agents‚Äô policies, we decompose each quantile
for gradient step g in {1,...,G} do Œ∏ (s,a) as
j
Sample a batch B from the dataset D
I
Estimate the MA-CCQL loss L MA-CCQL in (23) Œ∏ j(s,a)=(cid:88) Œ∏Àú ji(s,ai), (24)
Perform a stochastic gradient step to update the
i=1
network parameters of each agent
where Œ∏Àúi(s,ai) represents the contribution of agent i. The
end j
end functions {{Œ∏Àú ji(s,ai)}I i=1}N j=1 are jointly optimized using a
Return QÀúi(s,ai)=QÀÜi(K)(s,ai), for i=1,...,I loss obtained by plugging the decomposition (24) into (17) to
obtain
L ({{Œ∏Àúi}I }N ,{{Œ∏ÀÜi(k)}I }N )=
MA-CCQR j i=1 j=1 j i=1 j=1
A. Multi-Agent Conservative Centralized Q-Learning
N N
In the CTDE framework, it is assumed that the global Q-
1 EÀÜ(cid:88)(cid:88)
Œ∂
(cid:16) ‚àÜ(k)(cid:17)
(25)
2N2 œÑÀÜj jj‚Ä≤
function can be decomposed as [38]
j=1j‚Ä≤=1
Q(s,a)=(cid:88)I
QÀúi(s,ai), (20)
+Œ±EÀÜ(cid:88)I (cid:34) 1 (cid:88)N (cid:34) log(cid:88) exp(cid:16) Œ∏Àúi(s,aÀúi)(cid:17) ‚àíŒ∏Àúi(s,ai)(cid:35)(cid:35)
,
N j j
i=1 i=1 j=1 aÀú‚ààA8
Algorithm 4: Conservative Centralized Quantile Re-
gression for Offline MARL (MA-CCQR)
Input: Discount factor Œ≥, learning rate Œ∑, number of
quantiles N, conservative penalty constant Œ±, number
of agents I, number of training iterations K, number
of gradient steps G, offline dataset D, and CVaR
parameter Œæ
Output: Optimized functions {Œ∏Àúi(s,ai)}N for all
j j=1
i=1,...,I
Define œÑ =Œæi/N, i=1,...,N
i
Initialize network parameters for each agent
for iteration k in {1,...,K} do
Fig. 3: Multiple UAVs serve limited-power sensors to minimize power
for gradient step g in {1,...,G} do expenditure while also minimizing the age of information for data retrieval
Sample a batch B from the dataset D from the sensors. The environment is characterized by a riskier region for
for j in {1,...,N} do navigationoftheUAVsinthemiddleofthegridworld(coloredarea).
for j‚Ä≤ in {1,...,N} do
Calculate global TD error ‚àÜ(k)
jj‚Ä≤ grid world. The devices report their observations to I fixed-
using (26)
velocity rotary-wing UAVs flying at height h and starting
end
from positions selected randomly on the grid. The grid world
end
containsnormalcells,representedaswhitesquares,andarisk
Estimate the MA-CCQR loss L in (25)
MA-CCQR region of special cells, colored in the figure. Whenever UAVs
Perform a stochastic gradient step to update the
cross the risk region, there is a chance of failure, e.g., due
network parameters of each agent
to a higher collision probability. The current position at each
end
time t of each UAV i is projected on the plane as coordinates
end
Return {Œ∏ÀÜi(K)(s,ai)}N , for i=1,...,I (cid:0) xi t,y ti(cid:1) . The DT monitoring of this system aims to determine
j j=1 trajectories for the UAVs on the grid that jointly minimize the
AoI and the transmission powers across all the IoT devices.
TheAoImeasuresthefreshnessoftheinformationcollected
where {{Œ∏ÀÜ ji(k)}I i=1}N j=1 represents the current estimate of the by the UAVs from the devices [42]. For each device m, the
contribution of agent i and ‚àÜ(k) is given by AoIisdefinedasthetimeelapsedsincethelasttimedatafrom
jj‚Ä≤
the device was collected by a UAV [43], [44]. Accordingly,
I I
‚àÜ(k) =r+Œ≥(cid:88) Œ∏ÀÜi(k)(s‚Ä≤,a‚Ä≤i)‚àí(cid:88) Œ∏Àúi(s,ai), (26) the AoI of device m at time t is updated as follows
jj‚Ä≤ j‚Ä≤ j
(cid:40)
i=1 i=1 1, if Vm =1,
Am = t (27)
with a‚Ä≤i = arg max 1 (cid:80)N Œ∏ÀÜi(k)(s‚Ä≤,ai). The individ- t min{A ,Am +1}, otherwise;
ai‚ààAi N j‚Ä≤=1 j‚Ä≤ max t‚àí1
ual policies of the agent are finally obtained as
where A is the maximum AoI, and Vm =1 indicates that
Ô£± Ô£º max t
œÄi(ai|s)=1Ô£≤
ai =arg max
1 (cid:88)N Œ∏Àúi(s,ai)Ô£Ω
.
device m is served by a UAV at time step t. The maximum
Ô£≥ ai‚ààAi N j Ô£æ value A max determines the maximum penalty assigned to the
j=1 UAVs for not collecting data from a device at any given time.
For each agent, the function that maps (s,ai) to the N values For the sake of demonstrating the idea, we assume line-of-
{Œ∏Àúi(s,a)}N ismodeledasaneuralnetworkandthestepsof sight (LoS) communication links and write the channel gain
j j=1
the MA-CCQR scheme are provided in Algorithm 4. between agent i and device m at time step t as
g
gi,m = 0 , (28)
VI. APPLICATION:TRAJECTORYLEARNINGINUAV t h2+(Li,m)2
t
NETWORKS
where g is the channel gain at a reference distance of 1 m
Inthissection,weconsidertheapplicationofofflineMARL 0
andLi,m isthedistancebetweenUAVianddevicemattime
to the trajectory optimization problem in UAV networks. t
t. Using the standard Shannon capacity formula, for device
Following [42], as illustrated in Fig. 3, we consider multiple
m to communicate to UAV i at time step t, the transmission
UAVs acting as BSs to receive uplink updates from limited-
power must be set to [45]
power sensors.
(cid:16) (cid:17)
2BE‚àí1 œÉ2
A. Problem Definition and Performance Metrics Pi,m = , (29)
t gi,m
Consider a grid world, as shown in Fig. 3, where each t
cell is a square of length L . The system comprises a set where E is the size of the transmitted packet, B is the
c
M of M uplink IoT devices deployed uniformly in the bandwidth, and œÉ2 is the noise power.9
TABLEIII:Simulationparametersandhyperparameters
Parameter Value Parameter Value
‚àí1
g0 30dB Œ± 1
B 1MHz Œ≥ 0.99
h 100m Œæ 0.15
‚àí2
E 5Mb œÉ2 ‚àí100dBm
MA-CIQR
Œ≥ 0.99 Amax 100
Batchsize 128 Œª 500 MA-CIQR-CVaR
IterationsK 150 p risk 0.1 ‚àí3 MA-CIQL
Lc 100m Optimizer Adam MA-DIQN
MA-QR-DIQN
‚àí4
If all the UAVs are outside the risk region, the reward
function is given deterministically as a weighted combination
of the sums of AoI and powers across all agents ‚àí5
1 (cid:88)M (cid:88)M 0 20 40 60 80 100 120 140
r t =‚àí M Am t ‚àíŒª P tim,m, (30) Epochs
m=1 m=1
where Œª>0 is a parameter that controls the desired trade-off
between AoI and power consumption. In contrast, if any of Fig.4:Averagetestreturnasafunctionofthenumberoftrainingepochsat
the UAVs is within the risk region, with probability p , the
theDTusingP risk=100and16%offlinedatasetforasystemof2UAVs
risk serving10sensors.Thereturnisaveragedover100testepisodesattheend
reward is given by (30) with the addition of a penalty value ofeachtrainingepochandshownupondivisionby1000.
P >0, while it is equal to (30) otherwise.
risk
To complete the setting description, we define state and
all its variants), the learning rate is set to 10‚àí5, while for all
actions as follows. The global state of the system at
other schemes, we use a learning rate of 10‚àí4.
each time step t is the collection of the UAVs‚Äô posi-
The offline dataset D is collected using online independent
tions and the individual AoI of the devices, i.e., s =
(cid:2) x1,y1,¬∑¬∑¬∑ ,xI,yI,A1,A2,¬∑¬∑¬∑ ,AM(cid:3) . At each time t,t the DQN agents. In particular, we train the UAVs using an online
t t t t t t t MA-DQN algorithm until convergence and use 6% and 16%
action ai = [wi,di] of each UAV i includes the direction
t t t ofthetotalnumberoftransitionsfromtheobservedexperience
wi ‚àà {north,south,east,west,hover}, where ‚Äúhover‚Äù repre-
t as the offline datasets1.
sents the decision of staying in the same cell, while the other
actions move the UAV by one cell in the given direction. It
C. Numerical Results
also includes the identity di ‚ààM‚à™{0} of the device served
t First,weshowthesimulationresultsoftheproposedmodel
at time t, with di = 0 indicating that no device is served by
t viaindependentQ-learningcomparedtothebaselineschemes.
UAV i.
Then, we investigate the benefits of the CTDE approach
compared to independent training.
B. Implementation and Dataset Collection 1) Independent Learning: Fig. 4 shows the average test
Weconsidera10√ó10gridworldwithI =2UAVsserving return, evaluated online using 100 test episodes, for the
M = 10 limited-power sensors and a 5 √ó 4 risk region in policies obtained after a given number of training epochs at
the DT. The figure thus reports the actual return obtained by
the middle of the grid world as illustrated in Fig. 3. We use
the system as a function of the computational load at the DT,
a fully connected neural network with two hidden layers of
size 256 and ReLU activation functions to represent the Q- which increases with the number of training epochs.
We first observe that both MA-DIQN and MA-QR-DIQN,
function and the quantiles. The experiments are implemented
designed for online learning, fail to converge in the offline
using Pytorch on a single NVIDIA Tesla V100 GPU. Ta-
setting at hand. This well-known problem arises from overes-
ble III shows the UAV network parameters and the proposed
timatingQ-valuescorrespondingtoOODactionsintheoffline
schemes‚Äô hyperparameters. We compare the proposed method
dataset [10]. In contrast, conservative strategies designed for
MA-CQR to baseline offline MARL schemes, namely MA-
offline learning, namely MA-CIQL, MA-CIQR, and MA-
DQN[46],MA-CQL(seeSec.IV-A),andMA-QR-DQN.MA-
CIQR-CVaR,exhibitanincreasingaveragereturnasafunction
DQN corresponds to MA-CQL when no conservative penalty
of the training epochs. In particular, the proposed MA-CIQR
for OOD is applied, i.e., Œ± = 0 in (15), whereas MA-QR-
andMA-CIQR-CVaRprovidethefastestconvergence,needing
DQN corresponds to MA-CQR when Œ±=0 and Œæ =1. Both
around 30 training epochs to reach the maximum return. In
the independent and centralized training frameworks apply to
contrast,MA-CIQLshowsslowerconvergence.Thishighlights
MA-DQN and MA-QR-DQN.
thebenefitsofdistributionalRLinhandlingtheinherentuncer-
For the proposed MA-CQR, we consider two settings for
tainties arising in multi-agent systems from the environment
the risk tolerance level Œæ, namely Œæ = 1 and Œæ = 0.15, with
and the actions of other agents [11].
the former corresponding to a risk-neutral design. We refer to
theformerasMA-CQRandthelatterasMA-CQR-CVaR.For
1The code and datasets are available at https://github.com/Eslam211/
alldistributionalRLschemes(MA-QR-DQNandMA-CQRin Conservative-and-Distributional-MARL
nr
ter
tset
egarevA10
120 In the next experiment, we investigate the capacity of
the proposed risk-sensitive scheme MA-CIQR-CVaR to avoid
risky trajectories. As a first illustration of this aspect, Fig. 6
100
shows two examples of trajectories obtained via MA-CIQR
and MA-CIQR-CVaR. It is observed that the risk-neutral
80
policies obtained by MA-CIQR take shortcuts through the
risky area, while the risk-sensitive trajectories obtained via
60 MA-CIQR-CVaR avoid entering the risky area.
2) CentralizedLearning: Here,wecomparethecentralized
40 MA-QR-DIQN training approach with independent learning. Fig. 7 shows the
MA-DIQN
average test return as a function of training epochs for an
MA-CIQL
20 MA-CIQR-CVaR environment of 2 agents and 10 sensors. We use two offline
MA-CIQR datasets with different sizes, equal to 6% and 16% of the
Idle
total transitions from the observed experience of online DQN
0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 agents. We increase the value of the risk region penalty to
Sum-power P =300comparedtotheprevioussubsectionexperiments.
risk
Fig. 7a elucidates that the performance of the independent
learning schemes is affected by increasing the risk penalty
Fig.5:Sum-AoIasafunctionofthesum-powerusingP risk=Œª/4and16% P .Specifically,MA-CIQLfailstoreachconvergence,while
offlinedatasetforasystemof2UAVsserving10sensors. risk
MA-CIQR and MA-CIQR-CVaR reach near-optimal perfor-
mance but with a slower and less stable convergence than
x their centralized variants. However, as in Fig. 7b, a significant
x performance gap is observed between the proposed indepen-
x x dent schemes and their centralized counterpart for reduced
x x dataset size. This illustrates the merits of the CTDE approach
in coordinating between agents during training, requiring less
x data to obtain effective policies.
x In a manner similar to Fig. 5, Fig. 8 shows the trade-
off between sum-AoI and sum-power consumption for both
x
independent and centralized training approaches for a system
x
of 3 agents serving 15 sensors. Increasing the parameter Œª
in (30) reduces the total power consumption at the expense
Risk-neutral Risk-sensitive
of AoI. Here again, we observe a significant gain in per-
formance for MA-CCQR and MA-CCQR-CVaR compared to
their independent variants. In contrast, the non-distributional
Fig. 6: A comparison between the trajectories of two UAVs using risk-
neutral and risk-sensitive policies obtained via MA-CIQR and MA-CIQR- schemes, MA-CIQL and MA-CCQL, show similar results, as
CVaR,respectively.Crossesrepresentthepositionsofthedevices. both perform poorly in this low data regime.
Finally,togainfurtherinsightsintothecomparisonbetween
MA-CCQRandMA-CCQR-CVaR,weleveragetwometricsas
InFig5,wereporttheoptimalachievabletrade-offbetween in[23],namelythepercentageofviolationsandtheCVaR
0.15
sum-AoI and sum-power consumption across the devices. return. The former is the percentage of timesteps at which
This region is obtained by training the different schemes one of the UAVs enters the risk region with respect to the
while sweeping the hyperparameter values Œª. We recall that total number of timesteps. In contrast, the CVaR metric is
0.15
the hyperparameter Œª controls the weight of the power as the average return of the 15% worst episodes.
comparedtotheAoIintherewardfunction(30).Inparticular,
In Table IV, we report these two metrics, as well as the
setting Œª ‚Üí 0 minimizes the AoI only, resulting in a round-
averagereturn,withallreturnsnormalizedby1000.Thanksto
robinoptimalpolicy.Attheotherextreme,settingalargevalue
the ability of MA-CCQR-CVaR to learn how to avoid the risk
of Œª causes the UAV never to probe the devices, achieving
region, this scheme has the lowest percentage of violations
the minimum power equal to zero, and the maximum AoI
among all the schemes. In addition, it achieves the largest
A =100. This point is denoted as ‚Äúidle point‚Äù the figure.
max CVaR return, with a small gain in terms of average return
0.15
The other curves represent the minimum sum-AoI achievable
ascomparedtoMA-CCQR.Thisdemonstratestheadvantages
as a function of the sum-power.
of the risk-sensitive design of policies.
From Fig. 5, we observe that the proposed MA-CIQR
alwaysachievesthebestage-powertrade-offwiththeleastage
VII. CONCLUSIONS
andsum-powerconsumptionwithinalthecurves.AsinFig.4,
MA-DIQNandMA-QR-DIQNprovidetheworstperformance Inthispaper,wedevelopedadistributionalandconservative
duetotheirfailuretohandletheuncertaintyarisingfromOOD offline MARL scheme for DT-based wireless systems. We
actions. considered optimizing the CVaR of the cumulative return to
IoA-muS11
MA-CIQR-CVaR MA-CIQR MA-CIQL 100
MA-CCQR-CVaR MA-CCQR MA-CCQL
‚àí1 80
‚àí2 60
MA-CIQL
40 ‚àí3 MA-CCQL
MA-CIQR-CVaR
MA-CCQR-CVaR
20 MA-CIQR
‚àí4
MA-CCQR
Idle
0
‚àí5 0 1 2 3 4 5
Sum-power
0 20 40 60 80 100 120 140
Epochs
Fig.8:Sum-AoIasafunctionofthesum-powerforpenaltyP risk=Œª/2.5
(a)16%datasetsize inasystemof3UAVsand15sensors(datasetsizeequalto6%).
networks. Numerical results illustrate that the learned policies
‚àí1
avoid risky trajectories more effectively and yield the best
performance compared to the baseline MARL schemes. The
proposedapproachcanbeextendedbyconsideringonlinefine-
‚àí2
tuningofthepoliciesinthePTtohandlethepossiblechanges
inthedeploymentenvironmentcomparedtotheonegenerating
the offline dataset.
‚àí3
REFERENCES
‚àí4 [1] A.Lavin,D.Krakauer,H.Zenil,J.Gottschlich,T.Mattson,J.Brehmer,
A.Anandkumar,S.Choudry,K.Rocki,A.G.Baydinetal.,‚ÄúSimulation
intelligence: Towards a new generation of scientific methods,‚Äù arXiv
preprintarXiv:2112.03235,2021.
‚àí5 [2] R. Saracco, ‚ÄúDigital twins: Bridging physical space and cyberspace,‚Äù
0 20 40 60 80 100 120 140 Computer,vol.52,no.12,pp.58‚Äì64,2019.
[3] M. Raza, P. M. Kumar, D. V. Hung, W. Davis, H. Nguyen, and
Epochs
R. Trestian, ‚ÄúA digital twin framework for industry 4.0 enabling next-
(b)6%datasetsize genmanufacturing,‚Äùin20209thinternationalconferenceonindustrial
technologyandmanagement(ICITM). IEEE,2020,pp.73‚Äì77.
Fig.7:Averagetestreturnasafunctionofthenumberoftrainingepochsat [4] P. Coveney and R. Highfield, Virtual You: How Building Your Digital
theDTforpenaltyP risk=300inasystemof3UAVsand15sensorswith Twin Will Revolutionize Medicine and Change Your Life. Princeton
datasetsizeequalto(a)16%and(b)6%.Thereturnisaveragedover100 UniversityPress,2023.
testepisodesattheendofeachtrainingepoch,andshownupondivisionby [5] L.U.Khan,W.Saad,D.Niyato,Z.Han,andC.S.Hong,‚ÄúDigital-twin-
1000. enabled 6g: Vision, architectural trends, and future directions,‚Äù IEEE
CommunicationsMagazine,vol.60,no.1,pp.74‚Äì80,2022.
[6] J.Mirzaei,I.Abualhaol,andG.Poitau,‚ÄúNetworkdigitaltwinforopen
TABLEIV:Performanceevaluationover100testepisodesafter150training
RAN:Thekeyenablers,standardization,andusecases,‚ÄùarXivpreprint
iterationsforpenaltyP risk=Œª/2.5inasystemof3UAVsand15sensors
arXiv:2308.02644,2023.
(datasetsizeequalto6%).
[7] D. Villa, M. Tehrani-Moayyed, C. P. Robinson, L. Bonati, P. Johari,
M. Polese, S. Basagni, and T. Melodia, ‚ÄúColosseum as a digital twin:
Algorithm Averagereturn CVaR0.15 return Violations
Bridging real-world experimentation and wireless network emulation,‚Äù
arXivpreprintarXiv:2303.17063,2023.
MA-DQN(online) ‚àí1.5633 ‚àí1.9611 11.83%
[8] J.Zheng,T.H.Luan,Y.Zhang,R.Li,Y.Hui,L.Gao,andM.Dong,
MA-DCQN ‚àí4.8993 ‚àí5.4930 8.29%
‚ÄúDatasynchronizationinvehiculardigitaltwinnetwork:Agametheo-
MA-QR-DCQN ‚àí4.2987 ‚àí4.5743 6.85%
reticapproach,‚ÄùIEEETransactionsonWirelessCommunications,2023.
MA-CCQL ‚àí3.8518 ‚àí4.4695 17.7%
[9] S.V.Albrecht,F.Christianos,andL.Scha¬®fer,Multi-AgentReinforcement
MA-CCQR ‚àí1.4028 ‚àí2.1775 8.56%
Learning: Foundations and Modern Approaches. MIT Press, 2024.
MA-CCQR-CVaR ‚àí1.3641 ‚àí1.8513 5.83%
[Online].Available:https://www.marl-book.com
[10] S. Levine, A. Kumar, G. Tucker, and J. Fu, ‚ÄúOffline reinforcement
learning: Tutorial, review, and perspectives on open problems,‚Äù arXiv
preprintarXiv:2005.01643,2020.
obtain risk-sensitive policies. We introduce two variants of [11] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Rein-
the proposed scheme depending on the level of coordination forcementLearning. MITPress,2023,http://www.distributional-rl.org.
[12] A.Kumar,A.Zhou,G.Tucker,andS.Levine,‚ÄúConservativeQ-learning
between the agents during training. The proposed algorithms
for offline reinforcement learning,‚Äù Advances in Neural Information
were applied to the trajectory optimization problem in UAV ProcessingSystems,vol.33,pp.1179‚Äì1191,2020.
nruter
tset
egarevA
nruter
tset
egarevA
IoA-muS12
[13] M.A.Abd-Elmagid,A.Ferdowsi,H.S.Dhillon,andW.Saad,‚ÄúDeep [37] Y. Hua, R. Li, Z. Zhao, X. Chen, and H. Zhang, ‚ÄúGan-powered
reinforcement learning for minimizing age-of-information in UAV- deep distributional reinforcement learning for resource management in
assisted networks,‚Äù in 2019 IEEE Global Communications Conference networkslicing,‚ÄùIEEEJournalonSelectedAreasinCommunications,
(GLOBECOM). IEEE,2019,pp.1‚Äì6. vol.38,no.2,pp.334‚Äì349,2019.
[14] M. Samir, C. Assi, S. Sharafeddine, D. Ebrahimi, and A. Ghrayeb, [38] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
‚ÄúAge of information aware trajectory planning of UAVs in intelligent M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls et al.,
transportation systems: A deep learning approach,‚Äù IEEE Transactions ‚ÄúValue-decomposition networks for cooperative multi-agent learning,‚Äù
onVehicularTechnology,vol.69,no.11,pp.12382‚Äì12395,2020. arXivpreprintarXiv:1706.05296,2017.
[15] E. Eldeeb, J. M. de Souza Sant‚ÄôAna, D. E. Pe¬¥rez, M. Shehab, N. H. [39] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
Mahmood,andH.Alves,‚ÄúMulti-UAVpathlearningforageandpower ‚ÄúMulti-agent actor-critic for mixed cooperative-competitive environ-
optimizationinIoTwithUAVbatteryrecharge,‚ÄùIEEETransactionson ments,‚Äù2020.
VehicularTechnology,vol.72,no.4,pp.5356‚Äì5360,2022. [40] R.Bellman,‚ÄúDynamicprogramming,‚ÄùScience,vol.153,no.3731,pp.
[16] K. Ciosek, ‚ÄúImitation learning by reinforcement learning,‚Äù in 34‚Äì37,1966.
InternationalConferenceonLearningRepresentations,2022.[Online]. [41] H.Xu,L.Jiang,J.Li,Z.Yang,Z.Wang,V.W.K.Chan,andX.Zhan,
Available:https://openreview.net/forum?id=1zwleytEpYx ‚ÄúOfflineRLwithnoOODactions:In-samplelearningviaimplicitvalue
[17] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, ‚ÄúTrust regularization,‚Äù2023.
region policy optimization,‚Äù in International conference on machine [42] E.Eldeeb,D.E.Pe¬¥rez,J.MicheldeSouzaSant‚ÄôAna,M.Shehab,N.H.
learning. PMLR,2015,pp.1889‚Äì1897. Mahmood, H. Alves, and M. Latva-Aho, ‚ÄúA learning-based trajectory
[18] M.G.Bellemare,W.Dabney,andR.Munos,‚ÄúAdistributionalperspec- planning of multiple UAVs for AoI minimization in IoT networks,‚Äù in
tiveonreinforcementlearning,‚ÄùinInternationalconferenceonmachine 2022JointEuropeanConferenceonNetworksandCommunications&
learning. PMLR,2017,pp.449‚Äì458. 6GSummit(EuCNC/6GSummit),2022,pp.172‚Äì177.
[19] W.Dabney,M.Rowland,M.Bellemare,andR.Munos,‚ÄúDistributional [43] E.Eldeeb,M.Shehab,A.E.Kal√∏r,P.Popovski,andH.Alves,‚ÄúTraffic
reinforcementlearningwithquantileregression,‚ÄùinProceedingsofthe predictionandfastuplinkforhiddenmarkoviotmodels,‚ÄùIEEEInternet
AAAIConferenceonArtificialIntelligence,vol.32,no.1,2018. ofThingsJournal,vol.9,no.18,pp.17172‚Äì17184,2022.
[20] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, ‚ÄúImplicit quantile [44] A. Kosta, N. Pappas, and V. Angelakis, ‚ÄúAge of information: A new
networks for distributional reinforcement learning,‚Äù in International concept,metric,andtool,‚ÄùFoundationsandTrendsinNetworking,Now
conferenceonmachinelearning. PMLR,2018,pp.1096‚Äì1105. Publishers,Inc.,2017.
[21] R.T.Rockafellar,S.Uryasevetal.,‚ÄúOptimizationofconditionalvalue- [45] E. Eldeeb, M. Shehab, and H. Alves, ‚ÄúAge minimization in massive
at-risk,‚ÄùJournalofrisk,vol.2,pp.21‚Äì42,2000. iotviauavswarm:Amulti-agentreinforcementlearningapproach,‚Äùin
[22] S.H.LimandI.Malik,‚ÄúDistributionalreinforcementlearningforrisk- 2023 IEEE 34th Annual International Symposium on Personal, Indoor
sensitivepolicies,‚ÄùAdvancesinNeuralInformationProcessingSystems, andMobileRadioCommunications(PIMRC),2023,pp.1‚Äì6.
vol.35,pp.30977‚Äì30989,2022. [46] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
[23] Y. Ma, D. Jayaraman, and O. Bastani, ‚ÄúConservative offline distribu- J. Aru, and R. Vicente, ‚ÄúMultiagent cooperation and competition with
tional reinforcement learning,‚Äù Advances in Neural Information Pro- deep reinforcement learning,‚Äù PloS one, vol. 12, no. 4, p. e0172395,
cessingSystems,vol.34,pp.19235‚Äì19247,2021. 2017.
[24] J. Jiang and Z. Lu, ‚ÄúOffline decentralized multi-agent reinforcement
learning,‚Äù2023.
[25] L. Pan, L. Huang, T. Ma, and H. Xu, ‚ÄúPlan better amid conservatism:
Offline multi-agent reinforcement learning with actor rectification,‚Äù in
International Conference on Machine Learning. PMLR, 2022, pp.
17221‚Äì17237.
[26] J.Shao,Y.Qu,C.Chen,H.Zhang,andX.Ji,‚ÄúCounterfactualconser-
vativeQlearningforofflinemulti-agentreinforcementlearning,‚Äù2023.
[27] X.Wang,H.Xu,Y.Zheng,andX.Zhan,‚ÄúOfflinemulti-agentreinforce-
mentlearningwithimplicitglobal-to-localvalueregularization,‚ÄùarXiv
preprintarXiv:2307.11620,2023.
[28] Y. Yang, X. Ma, C. Li, Z. Zheng, Q. Zhang, G. Huang, J. Yang, and
Q.Zhao,‚ÄúBelievewhatyousee:Implicitconstraintapproachforoffline
multi-agentreinforcementlearning,‚Äù2021.
[29] K. Yang, C. Shen, J. Yang, S. ping Yeh, and J. Sydir, ‚ÄúOffline
reinforcement learning for wireless network optimization with mixture
datasets,‚Äù2023.
[30] E. Eldeeb, M. Shehab, and H. Alves, ‚ÄúTraffic learning and proactive
UAV trajectory planning for data uplink in markovian IoT models,‚Äù
2023.
[31] F. Wu, H. Zhang, J. Wu, L. Song, Z. Han, and H. V. Poor, ‚ÄúAoI
minimization for UAV-to-device underlay communication by multi-
agentdeepreinforcementlearning,‚ÄùinGLOBECOM2020-2020IEEE
GlobalCommunicationsConference,2020,pp.1‚Äì6.
[32] J.Cui,Y.Liu,andA.Nallanathan,‚ÄúMulti-agentreinforcementlearning-
based resource allocation for UAV networks,‚Äù IEEE Transactions on
WirelessCommunications,vol.19,no.2,pp.729‚Äì743,2020.
[33] Y. S. Nasir and D. Guo, ‚ÄúMulti-agent deep reinforcement learning
for dynamic power allocation in wireless networks,‚Äù IEEE Journal on
Selected Areas in Communications, vol. 37, no. 10, pp. 2239‚Äì2250,
2019.
[34] N.Naderializadeh,J.J.Sydir,M.Simsek,andH.Nikopour,‚ÄúResource
management in wireless networks via multi-agent deep reinforcement
learning,‚Äù IEEE Transactions on Wireless Communications, vol. 20,
no.6,pp.3507‚Äì3523,2021.
[35] Q.Zhang,W.Saad,andM.Bennis,‚ÄúMillimeterwavecommunications
withanintelligentreflector:Performanceoptimizationanddistributional
reinforcement learning,‚Äù IEEE Transactions on Wireless Communica-
tions,vol.21,no.3,pp.1836‚Äì1850,2021.
[36] ‚Äî‚Äî, ‚ÄúDistributional reinforcement learning for mmwave communica-
tions with intelligent reflectors on a uav,‚Äù in GLOBECOM 2020-2020
IEEEGlobalCommunicationsConference. IEEE,2020,pp.1‚Äì6.