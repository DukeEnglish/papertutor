Graph Mamba: Towards Learning on Graphs with State Space Models
AliBehrouz*1 FarnooshHashemi*1
Abstract
GraphNeuralNetworks(GNNs)haveshownpromisingpotentialingraphrepresentationlearning. Themajority
ofGNNsdefinealocalmessage-passingmechanism,propagatinginformationoverthegraphbystackingmultiple
layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor
capturingoflong-rangedependencies. Recently,GraphTransformers(GTs)emergedasapowerfulalternativeto
Message-PassingNeuralNetworks(MPNNs). GTs,however,havequadraticcomputationalcost,lackinductive
biasesongraphstructures,andrelyoncomplexPositional/StructuralEncodings(SE/PE).Inthispaper,weshow
thatwhileTransformers,complexmessage-passing,andSE/PEaresufficientforgoodperformanceinpractice,
neitherisnecessary. MotivatedbytherecentsuccessofStateSpaceModels(SSMs),suchasMamba,wepresent
GraphMambaNetworks(GMNs),ageneralframeworkforanewclassofGNNsbasedonselectiveSSMs. We
discussandcategorizethenewchallengeswhenadoptingSSMstograph-structureddata,andpresentfourrequired
andoneoptionalstepstodesignGMNs,wherewechoose(1)NeighborhoodTokenization,(2)TokenOrdering,
(3)ArchitectureofBidirectionalSelectiveSSMEncoder,(4)LocalEncoding,anddispensable(5)PEandSE.
WefurtherprovidetheoreticaljustificationforthepowerofGMNs. Experimentsdemonstratethatdespitemuch
lesscomputationalcost,GMNsattainanoutstandingperformanceinlong-range,small-scale,large-scale,and
heterophilicbenchmarkdatasets. Thecodeisinthislink.
1.Introduction
Recently,graphlearninghasbecomeanimportantandpopularareaofstudyduetoitsimpressiveresultsinawiderange
of applications, like neuroscience (Behrouz et al., 2023), social networks (Fan et al., 2019), molecular graphs (Wang
etal.,2021),etc. Inrecentyears,Message-PassingNeuralNetworks(MPNNs),whichiterativelyaggregateneighborhood
informationtolearnthenode/edgerepresentations,havebeenthedominantparadigminmachinelearningongraphs(Kipf
&Welling,2016;VelicË‡kovicÂ´ etal.,2018;Wuetal.,2020;Gutteridgeetal.,2023). They,however,havesomeinherent
limitations,includingover-squashing(DiGiovannietal.,2023),over-smoothing(Ruschetal.,2023),andpoorcapturingof
long-rangedependencies(Dwivedietal.,2022). WiththeriseofTransformerarchitectures(Vaswanietal.,2017)andtheir
successindiverseapplicationssuchasnaturallanguageprocessing(Wolfetal.,2020)andcomputervision(Liuetal.,2021),
theirgraphadaptations,so-calledGraphTransformers(GTs),havegainedpopularityasthealternativesofMPNNs(Yun
etal.,2019;Kimetal.,2022;RampaÂ´sË‡eketal.,2022).
Graphtransformershaveshownpromisingperformanceinvariousgraphtasks,andtheirvariantshaveachievedtopscores
inseveralgraphlearningbenchmarks(Huetal.,2020;Dwivedietal.,2022). ThesuperiorityofGTsoverMPNNsisoften
explainedbyMPNNsâ€™biastowardsencodinglocalstructures(MuÂ¨lleretal.,2023),whileakeyunderlyingprincipleof
GTsistoletnodesattendtoallothernodesthroughaglobalattentionmechanism(Kimetal.,2022;Yunetal.,2019),
allowing direct modeling of long-range interactions. Global attention, however, has weak inductive bias and typically
requiresincorporatinginformationaboutnodesâ€™positionstocapturethegraphstructure(RampaÂ´sË‡eketal.,2022;Kimetal.,
2022). Tothisend,variouspositionalandstructuralencodingschemesbasedonspectralandgraphfeatureshavebeen
introduced(Kreuzeretal.,2021;Kimetal.,2022;Limetal.,2023a).
DespitethefactthatGTswithproperpositionalencodings(PE)areuniversalapproximatorsandprovablymorepowerful
*Equalcontribution 1CornellUniversity,Ithaca,USA.Correspondenceto:AliBehrouz<ab2947@cornell.edu>,FarnooshHashemi
<sh2574@cornell.edu>.
Preprint.UnderReview.
1
4202
beF
31
]GL.sc[
1v87680.2042:viXraGraphMamba
thananyWLtest(Kreuzeretal.,2021),theirapplicabilitytolarge-scalegraphsishinderedbytheirpoorscalability. That
is,thestandardglobalattentionmechanismonagraphwithnnodesincursbothtimeandmemorycomplexityofO(n2),
quadraticintheinputsize,makingtheminfeasibleonlargegraphs. Toovercomethehighcomputationalcost,inspired
bylinearattentions(Zaheeretal.,2020),sparseattentionmechanismsongraphsattractsattention(RampaÂ´sË‡eketal.,2022;
Shirzadetal.,2023). Forexample,Exphormer(Shirzadetal.,2023)suggestsusingexpandergraphs,globalconnectors,and
localneighborhoodsasthreepatternsthatcanbeincorporatedinGTs,resultinginasparseandefficientattention. Although
sparseattentionspartiallyovercomethememorycostofglobalattentions,GTsbasedonthesesparseattentions(RampaÂ´sË‡ek
etal.,2022;Shirzadetal.,2023)stillmightsufferfromquadratictimecomplexity. Thatis,theyrequirecostlyPE(e.g.,
Laplacianeigen-decomposition)andstructuralencoding(SE)toachievetheirbestperformance,whichcantakeO(n2)to
compute.
AnotherapproachtoimproveGTsâ€™highcomputationalcostistousesubgraphtokenization(Chenetal.,2023;Zhaoetal.,
2021;Kuangetal.,2021;Baeketal.,2021;Heetal.,2023),wheretokens(a.k.apatches)aresmallsubgraphsextracted
withapre-definedstrategy. Typically,thesemethodsobtaintheinitialrepresentationsofthesubgraphtokensbypassing
themthroughanMPNN.Givenkextractedsubgraphs(tokens),thetimecomplexityofthesemethodsisO(k2),whichis
moreefficientthantypicalGTswithnodetokenization. Also,thesemethodsoftendonotrelyoncomplexPE/SE,astheir
tokens(subgraphs)inherentlycarryinductivebias. Thesemethods,however,havetwomajordrawbacks: (1)Toachieve
highexpressivepower,givenanode,usuallyrequireatleastasubgraphpereachremainingnode(Zhangetal.,2023a;
Bar-Shalometal.,2023),meaningthatk âˆˆO(n)andsothetimecomplexityisO(n2). (2)EncodingsubgraphsviaMPNNs
can transmit all their challenges of over-smoothing and over-squashing, limiting their applicability to heterophilic and
long-rangegraphs.
Recently,SpaceStateModels(SSMs),asanalternativeofattention-basedsequencemodelingarchitectureslikeTransformers
havegainedincreasingpopularityduetotheirefficiency(Zhangetal.,2023b;Nguyenetal.,2023). They,however,donot
achievecompetitiveperformancewithTransformersduetotheirlimitsininput-dependentcontextcompressioninsequence
models,causedbytheirtime-invarianttransitionmechanism. Tothisend,Gu&Dao(2023)present,Mamba,aselective
statespacemodelthatusesrecurrentscansalongwithaselectionmechanismtocontrolwhichpartofthesequencecanflow
intothehiddenstates. Thisselectioncansimplybeinterpretedasusingdata-dependentstatetransitionmechanism(SeeÂ§2.3
foradetaileddiscussion). Mambaoutstandingperformanceinlanguagemodeling,outperformingTransformersofthesame
sizeandmatchingTransformerstwiceitssize,motivatesseveralrecentstudiestoadaptitsarchitecturefordifferentdata
modalities(Liuetal.,2024b;Yangetal.,2024;Zhuetal.,2024;Ahamed&Cheng,2024).
Mambaarchitectureisspecificallydesignedforsequencedataandthecomplexnon-causalnatureofgraphsmakesdirectly
applyingMambaongraphschallenging. Further,naturalattemptstoreplaceTransformerswithMambainexistingGTs
frameworks (e.g., GPS (RampaÂ´sË‡ek et al., 2022), TokenGT (Kim et al., 2022)) results in suboptimal performance in
botheffectivenessandtimeefficiency(SeeÂ§5forevaluationandÂ§3foradetaileddiscussion). Thereasonis,contraryto
Transformersthatallowseachnodetointeractwithalltheothernodes,Mamaba,duetoitsrecurrentnature,onlyincorporates
informationaboutprevioustokens(nodes)inthesequence. ThisintroducesnewchallengescomparedtoGTs: (1)Thenew
paradigmrequirestokenorderingthatallowsthemodeltakeadvantageoftheprovidedpositionalinformationasmuchas
possible. (2)Thearchitecturedesignneedtobemorerobusttopermutationthanapuresequentialencoder(e.g.,Mamba).
(3)WhilethequadratictimecomplexityofattentionscandominatethecostofPE/SEinGTs,complexPE/SE(withO(n2)
cost)canbeabottleneckforscalingGraphMambaonlargegraphs.
Contributions. Toaddressalltheabovementionedlimitations,wepresentGraphMambaNetworks(GMNs),anewclassof
machinelearningongraphsbasedonstatespacemodels(Figure1showstheschematicoftheGMNs). Insummaryour
contributionsare:
â€¢ RecipeforGraphMambaNetworks. WediscussnewchallengesofGMNscomparedtoGTsinarchitecturedesign
and motivate our recipe with four required and one optional steps to design GMNs. In particular, its steps are (1)
Tokenization,(2)TokenOrdering,(3)LocalEncoding,(4)BidirectionalSelectiveSSMEncoderanddispensable(5)PE
andSE.
â€¢ AnEfficientTokenizationforBridgingFrameworks. Literaturelacksacommonfoundationaboutwhatconstitutesa
goodtokenization. Accordingly,architecturesarerequiredtochooseeithernode-orsubgraph-leveltokenization,while
eachofwhichhasitsown(dis)advantages,dependingonthedata. Wepresentagraphtokenizationprocessthatnot
onlyisfastandefficient,butitalsobridgesthenode-andsubgraph-leveltokenizationmethodsusingasingleparameter.
2GraphMamba
Tokenization PE/SE Local Encoding Token Ordering Bidirectional Mamba
Random Walk For each node and ð‘š!=1,â€¦,ð‘š, PE/SE (1) Sum over the rows of MPNNs To vectorize ð’Žâ‰¥ðŸ Tokens have Robust to Permutation We
we sample ð‘€ walks with length ð‘š! and consider non-diagonal elements of the each token one can use implicit order due to scan the sequence of tokens
their induced subgraph as a token. For each ð‘š!, random walk matrix. message-passing to hierarchical structure.in two directions.
we repeat the process ð‘  times. (2) Eigenvectors of the Laplacian. incorporate local âŠ•
(3) Anonymous random walk information.
ð’Ž = ðŸŽ: Each node is an independent token. encoding, i.e., counting the ð’Ž=ðŸŽ Sort nodes
number of times a node appears at RWF Given the walks based on PRR/Degree.
A tol klo ew nis z s aw tii ot nc h ui sn ig n gb e at w sie ne gn le n po ad re a a mn ed t es ru b ð‘šg ,r aph a certain position. t sh ua bt g rc ao prr he ,s op no en d cas nt o u a s e Implicit order +
m hya pk ei rn pg a t rh ae m c eh teo ric de uo rf i nto gk te rn ai iz na it ni gon . a tunable R die ffl ea rt ei nv ce e P ofE P/S EE /S EU s ai sn eg d p ga ei r fe-w ati use r es. l oo fc na ol did ese n tt oi t vy e cr te ol ra it zi eo n it . (ð‘š$=ð‘š) (ð‘š$=1) Ã— Ã—
â€¦ â€¦ â€¦
For each Token:
ðœŽ
s s
ðœŽ ðœŽ
â€¦ â€¦ â€¦ â€¦ ð’Ž=ðŸŽ
Concatenation
â€¦
s subgraphs s subgraphs s subgraphs small large
(ð‘š!=1) (ð‘š!=2) (ð‘š!=ð‘š) PRR/Degree/â€¦ MPNN
âŠ• PNA
Gated GCN
Long Sequence Mamba shows performance Optional PE/SE When using Features When node Domain Knowledge âŠ• Sum/Concatenation GINE
improvement with longer sequences, and so we subgraph tokens (i.e., ð‘š!â‰¥1), PE/SE or edge features are One can use domain
use parameter s to control the length of the is optional. That is, tokens have available, one can knowledge (when ðœŽ Activation Function
subgraph sequence. Based on the dataset, one their own inductive bias, and do not concatenate them with adapting GMs to Linear Layer
can tune ð‘  to achieve better results. need additional information about the PE/SE, before the specific domain) or 1-d Convolution
the graph structure. local encoding step. structural properties Selective SSM
like Personalized Required Step
PageRank or degree.
Optional Step
Figure1.SchematicoftheGMNswithfourrequiredandoneoptionalsteps: (1)Tokenization: thegraphismappedintoasequence
oftokens(m â‰¥ 1: subgraphandm = 0: nodetokenization)(2)(OptionalStep)PE/SE:inductivebiasisaddedtothearchitecture
usinginformationaboutthepositionofnodesandthestrucutreofthegraph. (3)LocalEncoding: localstructuresaroundeachnode
areencodedusingasubgraphvectorizationmechanism.(4)TokenOrdering:thesequenceoftokensareorderedbasedonthecontext.
(Subgraphtokenization(mâ‰¥1)hasimplicitorderanddoesnotneedthisstep).(5)(Stackof)BidirectionalMamba:itscansandselects
relevantnodesorsubgraphstoflowintothehiddenstates.â€ Inthisfigure,thelastlayerofbidirectionalMamba,whichperformsasa
readoutonallnodes,isomittedforsimplicity.
Moreover,thepresentedtokenizationhasimplicitorder,whichisspeciallyimportantforsequentialencoderslikeSSMs.
â€¢ NewBidirectionalSSMsforGraphs. InspiredbyMamba,wedesignaSSMarchitecturethatscanstheinputsequence
intwodifferentdirections,makingthemodelmorerobusttopermutation,whichisparticularlyimportantwhenwedo
notuseimplicitlyorderedtokenizationongraphs.
â€¢ TheoreticalJustification. WeprovidetheoreticaljustificationforthepowerofGMNsandshowthattheyareuniversal
approximatorofanyfunctionsongraphs. WefurthershowthatGMNsusingproperPE/SEismoreexpressivethanany
WLtest,matchingGTsinthismanner.
â€¢ Outstanding Performance and New Insights. Our experimental evaluations demonstrate that GMNs attain an
outstandingperformanceinlong-range,small-scale,large-scale,andheterophilicbenchmarkdatasets,whileconsuming
lessGPUmemory. TheseresultsshowthatwhileTransformers,complexmessage-passing,andSE/PEaresufficientfor
goodperformanceinpractice,neitherisnecessary. Wefurtherperformablationstudyandvalidatethecontributionof
eacharchitecturalchoice.
2.RelatedWorkandBackgrounds
TosituateGMNsinabroadercontext,wediscussfourrelevanttypesofmachinelearningmethods:
2.1.Message-PassingNeuralNetworks
Message-passingneuralnetworksareaclassofGNNsthatiterativelyaggregatelocalneighborhoodinformationtolearnthe
node/edgerepresentations(Kipf&Welling,2016).MPNNshavebeenthedominantparadigminmachinelearningongraphs,
andattractsmuchattention,leadingtovariouspowerfularchitectures,e.g.,GAT(VelicË‡kovicÂ´ etal.,2018),GCN(Henaff
3
stnioP
yeKGraphMamba
etal.,2015;Kipf&Welling,2016),GatedGCN(Bresson&Laurent,2017),GIN(Xuetal.,2019),etc. SimpleMPNNs,
however,areknowntosufferfromsomemajorlimitationsincluding:(1)limitingtheirexpressivitytothe1-WLisomorphism
test(Xuetal.,2019),(2)over-smoothing(Ruschetal.,2023),and(3)over-squashing(Alon&Yahav,2021;DiGiovanni
etal.,2023). VariousmethodshavebeendevelopedtoaugmentMPNNsandovercomesuchissues,includinghigher-order
GNNs(Morrisetal.,2019;2020),graphrewiring(Gutteridgeetal.,2023;Arnaiz-RodrÂ´Ä±guezetal.,2022),addaptiveand
cooperativeGNNs(Erricaetal.,2023;Finkelshteinetal.,2023),andusingadditionalfeatures(Satoetal.,2021;Murphy
etal.,2019).
2.2.GraphTransformers
WiththeriseofTransformerarchitectures(Vaswanietal.,2017)andtheirsuccessindiverseapplicationssuchasnatural
languageprocessing(Wolfetal.,2020)andcomputervision(Liuetal.,2021),theirgraphadaptationshavegainedpopularity
asthealternativesofMPNNs(Yunetal.,2019;Kimetal.,2022;RampaÂ´sË‡eketal.,2022). Usingafullglobalattention,
GTs consider each pair of nodes connected (Yun et al., 2019) and so are expected to overcome the problems of over-
squashingandover-smoothinginMPNNs(Kreuzeretal.,2021). GTs,however,haveweakinductivebiasandneedsproper
positional/structuralencodingtolearnthestructureofthegraph(Kreuzeretal.,2021;RampaÂ´sË‡eketal.,2022). Tothisend,
variousstudieshavefocusedondesigningpowerfulpositionalandstructuralencodings(Wangetal.,2022;Yingetal.,2021;
Kreuzeretal.,2021;Shiv&Quirk,2019).
SparseAttention. WhileGTshaveshownoutstandingperformanceindifferentgraphtasksonsmall-scaledatasets(up
to10Knodes),theirquadraticcomputationalcost,causedbytheirfullglobalattention,haslimitedtheirapplicabilityto
large-scalegraphs(RampaÂ´sË‡eketal.,2022). Motivatedbylinearattentionmechanisms(e.g.,BigBird(Zaheeretal.,2020)
andPerformer(Choromanskietal.,2021)),whicharedesignedtoovercomethesamescalabilityissueofTransformerson
longsequences,usingsparseTransformersinGTarchitectureshasgainedpopularity(RampaÂ´sË‡eketal.,2022;Shirzadetal.,
2023;Kongetal.,2023;Liuetal.,2023;Wuetal.,2023). ThemainideaofsparseGTsmodelsistorestricttheattention
pattern,i.e.,thepairsofnodesthatcaninteractwitheachother. Asanexample,Shirzadetal.(2023)presentExphormer,
thegraphadaptionofBigBirdthatusesthreesparsepatternsof(1)expandergraphattention,(2)localattentionamong
neighbors,and(3)globalattentionbyconnectingvirtualnodestoallnon-virtualnodes.
SubgraphTokenization. AnothermethodtoovercomeGTsâ€™highcomputationalcostistousesubgraphtokenization(Chen
et al., 2023; Zhao et al., 2021; Baek et al., 2021; He et al., 2023), where tokens are small subgraphs extracted with a
pre-definedstrategy. Thesesubgraphtokenizationstrategiesusuallyarek-hopneighborhood(givenafixedk)(Nguyenetal.,
2022a;Hussainetal.,2022;Parketal.,2022),learnablesampleofneighborhood(Zhangetal.,2022),ego-networks(Zhao
etal.,2021),hierarchicalk-hopneighborhoods(Chenetal.,2023),graphmotifs(Rongetal.,2020),graphpartitions(He
etal.,2023). Tovectorizeeachtoken,subgraph-basedGTmethodstypicallyrelyonMPNNs,makingthemvulnerableto
over-smoothingandover-squashing. Mostofthemalsouseafixedneighborhoodofeachnode,missingthehierarchical
structureofthegraph.TheonlyexceptionisNAGphormer(Chenetal.,2023)thatusesallk =1,...,K-hopneighborhoods
ofeachnodeasitscorrespondingtokens. Althoughthistokenizationletsthemodellearnthehierarchicalstructureofthe
graph,byincreasingthehopoftheneighborhood,itstokensbecomeexponentiallylarger,limitingitsabilitytoscaletolarge
graphs.
2.3.StateSpaceModels
StateSpaceModels(SSMs),atypeofsequencemodels,areusuallyknownaslineartime-invariantsystemsthatmapinput
sequence x(t) âˆˆ RL to response sequence y(t) âˆˆ RL (Aoki, 2013). Specifically, SSMs use a latent state h(t) âˆˆ RN,
evolutionparameterAâˆˆRNÃ—N,andprojectionparametersBâˆˆRNÃ—1,CâˆˆR1Ã—N suchthat:
hâ€²(t)=Ah(t)+Bx(t),
y(t)=Ch(t). (1)
Duetothehardnessofsolvingtheabovedifferentialequationindeeplearningsettings,discretespacestatemodels(Gu
etal.,2020;Zhangetal.,2023b)discretizetheabovesystemusingaparameterâˆ†:
h =AÂ¯ h +BÂ¯ x ,
t tâˆ’1 t
y =Ch , (2)
t t
4GraphMamba
where
AÂ¯ =exp(âˆ†A),
BÂ¯ =(âˆ†A)âˆ’1(exp(âˆ†Aâˆ’I)) .âˆ†B. (3)
Guetal.(2020)showsthatdiscrete-timeSSMsareequivalenttothefollowingconvolution:
KÂ¯ =(cid:0) CÂ¯BÂ¯,CÂ¯AÂ¯BÂ¯,...,CÂ¯AÂ¯Lâˆ’1BÂ¯(cid:1) ,
y =xâˆ—KÂ¯, (4)
andaccordinglycanbecomputedveryefficiently. Structuredstatespacemodels(S4),anothertypeofSSMs,areefficient
alternativesofattentionsandhaveimprovedefficiencyandscalabilityofSSMsusingreparameterization(Guetal.,2022;
Fuetal.,2023;Nguyenetal.,2023). SSMsshowpromisingperformanceontimeseriesdata(Zhangetal.,2023b;Tang
etal.,2023),Genomicsequence(Nguyenetal.,2023),healthcaredomain(Guetal.,2021),andcomputervision(Guetal.,
2021;Nguyenetal.,2022b). They,however,lackselectionmechanism,causingmissingthecontextasdiscussedbyGu&
Dao(2023). Recently,Gu&Dao(2023)introduceanefficientandpowerfulselectivestructuredstatespacearchitecture,
calledMAMBA,thatusesrecurrentscansalongwithaselectionmechanismtocontrolwhichpartofthesequencecanflow
into the hidden states. The selection mechanism of Mamba can be interpreted as using data-dependent state transition
mechanisms,i.e.,makingB,C,andâˆ†asfunctionofinputx . Mambaoutstandingperformanceinlanguagemodeling,
t
outperformingTransformersofthesamesizeandmatchingTransformerstwiceitssize,motivatesseveralrecentstudiesto
adaptitsarchitecturefordifferentdatamodalitiesandtasks(Liuetal.,2024b;Yangetal.,2024;Zhuetal.,2024;Ahamed&
Cheng,2024;Xingetal.,2024;Liuetal.,2024a;Maetal.,2024).
3.Challenges&Motivations: TransformersvsMamba
Mambaarchitectureisspecificallydesignedforsequencedataandthecomplexnon-causalnatureofgraphsmakesdirectly
applyingMambaongraphschallenging. BasedonthecommonapplicabilityofMambaandTransformersontokenized
sequentialdata,astraightforwardapproachtoadaptMambaforgraphsistoreplaceTransformerswithMambainGTs
frameworks,e.g.,TokenGT(Kimetal.,2022)orGPS(RampaÂ´sË‡eketal.,2022). However,thisapproachmightnotfully
takeadvantageofselectiveSSMsduetoignoringsomeoftheirspecialtraits. Inthissection,wediscussnewchallengesfor
GMNscomparedtoGTs.
Sequencesvs2-DData. Itisknownthattheself-attentivearchitecturecorrespondstoafamilyofpermutationequivariant
functions(Leeetal.,2019;Liuetal.,2020).Thatis,theattentionmechanisminTransformers(Vaswanietal.,2017)assumes
aconnectionbetweeneachpairoftokens,regardlessoftheirpositionsinthesequence,makingitpermutationequivariant.
Accordingly,Transformerslackinductivebiasandsoproperlypositionalencodingiscrucialfortheirperformance,whenever
theorderoftokensmatter(Vaswanietal.,2017;Liuetal.,2020). Ontheotherhand,Mambaisasequentialencoderand
scanstokensinarecurrentmanner(potentiallylesssensitivetopositionalencoding). Thus,itexpectscausaldataasaninput,
makingitchallengingtobeadaptedto2-D(e.g.,images)(Liuetal.,2024b)orcomplexgraph-structureddata. Accordingly,
whileingraphadaptionofTransformersmappingthegraphintoasequenceoftokensalongwithapositional/structural
encodingswereenough,sequentialencoders,likeSSMs,andmorespecificallyMamba,requireanorderingmechanismfor
tokens.
AlthoughthissensitivitytotheorderoftokensmakestheadaptionofSSMstographschallenging,itcanbemorepowerful
whenevertheordermatters. Forexample,learningthehierarchicalstructuresintheneighborhoodofeachnode(k-hops
for k = 1,...,K), which is implicitly ordered, is crucial in different domains (Zhong et al., 2022; Lim et al., 2023b).
Moreover,itprovidestheopportunitytousedomainknowledgewhentheordermatters(Yuetal.,2020). Inourproposed
framework,weprovidetheopportunityforbothcases:(1)usingdomainknowledgeorstructuralproperties(e.g.,Personalized
PageRank(Pageetal.,1998))whentheordermatters,or(2)usingimplicitlyorderedsubgraphs(noorderingisneeded).
Furthermore, our bidirectional encoder scans nodes in two different directions, being capable of learning equivariance
functionsontheinput,wheneveritisneeded.
Long-rangeSequenceModeling. Ingraphdomain,thesequenceoftokens,eithernode,edge,orsubgraph,canbecounted
as the context. Unfortunately, Transformer architecture, and more specifically GTs, are not scalable to long sequence.
Furthermore,intuitively,morecontext(i.e.,longersequence)shouldleadtobetterperformance;however,recentlyithas
beenempiricallyobservedthatmanysequencemodelsdonotimprovewithlongercontextinlanguagemodeling(Shietal.,
5GraphMamba
2023).Mamba,becauseofitsselectionmechanism,cansimplyfilterirrelevantinformationandalsoresetitsstateatanytime.
Accordingly,itsperformanceimprovesmonotonicallywithsequencelength(Gu&Dao,2023). Tothisend,andtofully
takeadvantageofMamba,onecanmapagraphornodetolongsequences,possiblybagsofvarioussubgraphs. Notonlythe
longsequenceoftokenscanprovidemorecontext,butitalsopotentiallycanimprovetheexpressivepower(Bevilacqua
etal.,2022).
Scalability. Duetothecomplexnatureofgraph-structureddata,sequentialencoders,includingTransformersandMamba,
requireproperpositionalandstructuralencodings(RampaÂ´sË‡eketal.,2022;Kimetal.,2022). ThesePEs/SEs,however,often
havequadraticcomputationalcost,whichcanbecomputedoncebeforetraining. Accordingly,duetothequadratictime
complexityofTransformers,computingthesePEs/SEswasdominatedandtheyhavenotbeenthebottleneckfortraining
GTs. GMNs,ontheotherhand,havelinearcomputationalcost(withrespecttobothtimeandmemory),andsoconstructing
complexPEs/SEscanbetheirbottleneckwhentrainingonverylargegraphs. ThisbringanewchallengeforGMNs,asthey
needtoeither(1)donotusePEs/SEs,or(2)usetheirmoreefficientvariantstofullytakeadvantageofSSMsefficiency.
OurarchitecturedesignmaketheuseofPE/SEoptionalandourempiricalevaluationshowsthatGMNswithoutPE/SEcan
achievecompetitiveperformancecomparedtomethodswithcomplexPEs/SEs.
NodeorSubgraph? Inadditiontotheabovenewchallenges,thereisalackofcommonfoundationaboutwhatconstitutesa
goodtokenization,andwhatdifferentiatesthem,eveninGTframeworks. Existingmethodsuseeithernode/edge(Shirzad
etal.,2023;RampaÂ´sË‡eketal.,2022;Kimetal.,2022),orsubgraphtokenizationmethods(Chenetal.,2023;Zhaoetal.,
2021;Heetal.,2023). Whilemethodswithnodetokenizationaremorecapableofcapturinglong-rangedependencies,
methodswithsubgraphtokenshavemoreabilitytolearnlocalneighborhoods,arelessrelyonPE/SE(Chenetal.,2023),
andaremoreefficientinpractice. Ourarchitecturedesignletsswitchingbetweennodeandsubgraphtokenizationusinga
singleparameterm,makingthechoiceoftokenizationatunablehyperparameterduringtraining.
4.GraphMambaNetworks
Inthissection,weprovideourfive-steprecipeforpowerful,flexible,andscalableGraphMambaNetworks. Following
thediscussionabouttheimportanceofeachstep,wepresentourarchitecture. TheoverviewoftheGMNframeworkis
illustratedinFigure1.
Throughoutthissection,weletG=(V,E)beagraph,whereV ={v ,...,v }isthesetofnodesandE âŠ†V Ã—V isthe
1 n
setofedges. Weassumeeachnodev âˆˆV hasafeaturevectorx(0) âˆˆX,whereXâˆˆRnÃ—disthefeaturematrixdescribing
v
theattributeinformationofnodesanddisthedimensionoffeaturevectors. Givenv âˆˆV,weletN(v)={u|(v,u)âˆˆE}
bethesetofvâ€™sneighbors. GivenasubsetofnodesS âŠ†V,weuseG[S]todenotetheinducedsubgraphconstructedby
nodesinS,andX todenotethefeaturematrixdescribingtheattributeinformationofnodesinS.
S
4.1.TokenizationandEncoding
Tokenization, which is the process of mapping the graph into a sequence of tokens, is an inseparable part of adapting
sequentialencoderstographs. Asdiscussedearlier,existingmethodsuseeithernode/edge(Shirzadetal.,2023;RampaÂ´sË‡ek
etal.,2022;Kimetal.,2022),orsubgraphtokenizationmethods(Chenetal.,2023;Zhaoetal.,2021;Heetal.,2023),
eachofwhichwithitsown(dis)advantages. Inthispart,wepresentanewsimplebutflexibleandeffectiveneighborhood
samplingforeachnodeanddiscussitsadvantagesoverexistingsubgraphtokenization. Themainandhigh-levelideaofour
tokenizationistofirst,samplesomesubgraphsforeachnodethatcanrepresentthenodeâ€™sneighborhoodstructureaswellas
itslocal,andglobalpositionsinthegraph. Thenwevectorize(encode)thesesubgraphstoobtainthenoderepresentations.
NeighborhoodSampling. Givenanodev âˆˆV,andtwointegersm,M â‰¥0,foreach0â‰¤mË† â‰¤m,wesampleM random
walksstartedfromvwithlengthmË†. LetT (v)fori=0,...,M bethesetofvisitednodesinthei-thwalk. Wedefinethe
mË†,i
tokencorrespondstoallwalkswithlengthmË† as:
(cid:34)M (cid:35)
(cid:91)
G[T (v)]=G T (v) , (5)
mË† mË†,i
i=1
which is the union of all walks with length mË†. One can interpret G[T (v)] as the induced subgraph of a sample of
mË†
mË†-hopneighborhoodofnodev. Attheend,foreachnodev âˆˆV wehavethesequenceofG[T (v)],...,G[T (v)]asits
0 m
correspondingtokens.
6GraphMamba
Usingrandomwalks(withfixedlength)ork-hopneighborhoodofanodeasitsrepresentativetokenshasbeendiscussedin
severalrecentstudies(Dingetal.,2023;Zhangetal.,2022;Chenetal.,2023;Zhaoetal.,2021). Thesemethods,however,
sufferfromasubsetoftheselimitations: (1)theyuseafixed-lengthrandomwalk(Kuangetal.,2021),whichmissesthe
hierarchicalstructureofthenodeâ€™sneighborhood. Thisisparticularlyimportantwhenthelong-rangedependenciesofnodes
areimportant. (2)theyuseallnodesinallk-hopneighborhoods(Chenetal.,2023;Dingetal.,2023),resultinginatrade-off
betweenlong-rangedependenciesandover-smoothingorover-squashingproblems. Furthermore,thek-hopneighborhoodof
awell-connectednodemightbethewholegraph,resultinginconsideringthegraphasatokenofanode,whichisinefficient.
Ourneighborhoodsamplingapproachaddressesalltheselimitations. Itsampledthefixednumberofrandomwalkswith
differentlengthsforallnodes,capturinghierarchicalstructureoftheneighborhoodwhileavoidingbothinefficiency,caused
byconsideringtheentiregraph,andover-smoothingandoversquashing,causedbylargeneighborhoodaggregation.
WhyNotMoreSubgraphs? Asdiscussedearlier,empiricalevaluationhasshownthattheperformanceofselectivestate
spacemodelsimprovesmonotonicallywithsequencelength(Gu&Dao,2023). Furthermore,theirlinearcomputational
costallowustousemoretokens,providingthemmorecontext. Accordingly,tofullytakeadvantageofselectivestatespace
models,givenanintegers>0,werepeattheaboveneighborhoodsamplingprocessforstimes. Accordingly,foreachnode
v âˆˆV wehaveasequenceof
G[T (v)],G[T1(v)],...,G[Ts(v)],...,G[T1(v)],...,G[Ts(v)]
0 1 1 m m
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
stimes stimes
asitscorrespondingsequenceoftokens. Here, wecanseeanotheradvantageofourproposedneighborhoodsampling
comparedtoChenetal.(2023);Dingetal.(2023). WhileinNAGphormer(Chenetal.,2023)thesequencelengthofeach
nodeislimitedbythediameterofthegraph,ourmethodcanproducealongsequenceofdiversesubgraphs.
Theorem4.1. WithlargeenoughM,m,ands>0,GMNsâ€™neighborhoodsamplingisstrictlymoreexpressivethank-hop
neighborhoodsampling.
Structural/PositionalEncoding. TofurtheraugmentourframeworkforGraphMamba,weconsideranoptionalstep,when
weinjectstructuralandpositionalencodingstotheinitialfeaturesofnodes/edges. PEismeanttoprovideinformationabout
thepositionofagivennodewithinthegraph. Accordingly,twoclosenodeswithinagraphorsubgrapharesupposedtohave
closePE.SE,ontheotherhand,ismeanttoprovideinformationaboutthestructureofasubgraph. FollowingRampaÂ´sË‡ek
etal.(2022),weconcatenateeithereigenvectorsofthegraphLaplacianorRandom-walkstructuralencodingstothenodesâ€™
feature,wheneverPE/SEareneeded: i.e.,
x(new) =x ||p , (6)
v v v
wherep isthecorrespondingpositionalencodingtov. Forthesakeofconsistency,weusex insteadofx throughoutthe
v v v
paper.
NeighborhoodEncoding. Givenanodev âˆˆV anditssequenceoftokens(subgraphs),weencodethesubgraphviaencoder
Ï•(.). Thatis,weconstructx1,x2,...,xmsâˆ’1,xms âˆˆRdasfollows:
v v v v
(cid:16) (cid:17)
x((iâˆ’1)s+j) =Ï• G[Tj(v)],X , (7)
v i Tj(v)
i
where1â‰¤iâ‰¤mand1â‰¤j â‰¤s. Inpractice,thisencodercanbeanMPNN,(e.g.,Gated-GCN(Bresson&Laurent,2017)),
orRWF(ToÂ¨nshoffetal.,2023)thatencodesnodeswithrespecttoasampledsetofwalksintofeaturevectorswithfour
parts: (1)nodefeatures,(2)edgefeaturesalongthewalk,and(3,4)localstructuralinformation.
TokenOrdering. ByEquation7,wecancalculatetheneighborhoodembeddingsforvarioussampledneighborhoodsofa
nodeandfurtherconstructasequencetorepresentitsneighborhoodinformation,i.e.,x1,x2,...,xmsâˆ’1,xms. Asdiscussed
v v v v
inÂ§3,adoptionofsequencemodelslikeSSMstograph-structureddatarequiresanorderonthetokens. Tounderstandwhat
constitutesagoodordering,weneedtorecallselectionmechanisminMamba(Gu&Dao,2023)(wewilldiscussselection
mechanismmoreformallyinÂ§4.2). MambabymakingB,C,andâˆ†asfunctionsofinputx (seeÂ§2.3fornotations)letsthe
t
modelfilterirrelevantinformationandselectimportanttokensinarecurrentmanner,meaningthateachtokengetsupdated
basedontokensthatcomebeforetheminthesequence. Accordingly,earliertokenshavelessinformationaboutthecontext
ofsequence,whilelatertokenshaveinformationaboutalmostentiresequence. Thisleadsustoordertokensbasedoneither
theirneedsofknowinginformationaboutothertokensortheirimportancetoourtask.
7GraphMamba
Whenmâ‰¥1: For the sake of simplicity first let s = 1. In the case that m â‰¥ 1, interestingly, our architecture design
providesuswithanimplicitlyorderedsequence. Thatis,givenv âˆˆV,thei-thtokenisasamplesfromi-hopneighborhood
ofnodev,whichisthesubgraphofallj-hopneighborhoods,wherej â‰¥i. Thismeans,givenalargeenoughM (numberof
sampledrandomwalks),ourT (v)hasenoughinformationaboutT (v),notviceversa. Tothisend,weusethereverseof
j i
initialorder,i.e.,xm,xmâˆ’1,...,x2,x1. Accordingly,innersubgraphscanalsohaveinformationabouttheglobalstructure.
v v v v
Whensâ‰¥2,weusethesameprocedureasabove,andreversetheinitialorder,i.e.,xsm,xsmâˆ’1,...,x2,x1. Tomakeour
v v v v
modelrobusttothepermutationofsubgraphswiththesamewalklengthmË†,werandomlyshufflethem. Wewilldiscussthe
orderinginthecaseofm=0later.
4.2.BidirectionalMamba
AsdiscussedinÂ§3,SSMsarerecurrentmodelsandrequireorderedinput,whilegraph-structureddatadoesnothaveany
orderandneedspermutationequivariantencoders. Tothisend,inspiredbyVimincomputervision(Zhuetal.,2024),we
modifyMambaarchitectureandusetworecurrentscanmodulestoscandataintwodifferentdirections(i.e.,forwardand
backward). Accordingly,giventwotokenst andt ,wherei>j andindicesshowtheirinitialorder,inforwardscant
i j i
comesaftert andsohastheinformationaboutt (whichcanbeflownintothehiddenstatesorfilteredbytheselection
j j
mechanism). In backward pass t comes after t and so has the information about t . This architecture is particularly
j i i
importantwhenm=0(nodetokenization),whichwewilldiscusslater.
More formally, in forward pass module, let Î¦ be the input sequence (e.g., given v, Î¦ is a matrix whose rows are
xsm,xsmâˆ’1,...,x1,calculatedinEquation7),Abetherelativepositionalencodingoftokens,wehave:
v v v
Î¦ =Ïƒ(Conv(W LayerNorm(Î¦))),
input input
B=W Î¦ , C=W Î¦ , âˆ†=Softplus(W Î¦ ),
B input C input âˆ† input
AÂ¯ =Discrete (A,âˆ†),
A
BÂ¯ =Discrete (B,âˆ†),
B
y =SSM AÂ¯,BÂ¯,C(Î¦ input),
y =W (yâŠ™Ïƒ(W LayerNorm(Î¦))), (8)
forward forward,1 forward,2
where W,W ,W ,W ,W and W are learnable parameters, Ïƒ(.) is nonlinear function (e.g., SiLU),
B C âˆ† forward,1 forward,2
LayerNorm(.)islayernormalization(Baetal.,2016),SSM(.)isthestatespacemodeldiscussedinEquations2and4,and
Discrete(.)isdiscretizationprocessdiscussedinEquation3. Weusethesamearchitectureasaboveforthebackward
pass(withdifferentweights)butinsteadweuseÎ¦ astheinput,whichisamatrixwhoserowsarex1,x2,...,xsm. Let
inverse v v v
y betheoutputofthisbackwardmodule,weobtainthefinalencodingsas
backward
y =W (y +y ). (9)
output out forward backward
Inpractice,westacksomelayersofthebidirectionalMambatoachievegoodperformance. Notethatduetoourordering
mechanism,thelaststateoftheoutputcorrespondstothewalkwithlengthmË† =0,i.e.,thenodeitself. Accordingly,thelast
staterepresentstheupdatednodeencoding.
AugmentationwithMPNNs. WefurtheruseanoptionalMPNNmodulethatsimultaneouslyperformsmessage-passing
andaugmentstheoutputofthebidirectionalMambaviaitsinductivebias. Particularlythismoduleisveryhelpfulwhen
therearerichedgefeaturesandsoanMPNNcanhelptotakeadvantageofthem. Whileinourempiricalevaluationweshow
thatthismoduleisnotnecessaryforthesuccessofGMNsinseveralcases,itcanbeusefulwhenweavoidcomplexPE/SE
andstronginductivebiasisneeded.
HowDoesSelectionWorkonSubgraphs? Asdiscussedearlier,theselectionmechanismcanbeachievedbymaking
B,C,andâˆ†asthefunctionsoftheinputdata(Gu&Dao,2023). Accordingly,inrecurrentscan,basedontheinput,the
modelcanfiltertheirrelevantcontext. TheselectionmechanisminEquation9isimplementedbymakingB,C,andâˆ†as
functionsofÎ¦ ,whichismatrixoftheencodingsofneighborhoods. Therefore,asmodelscansthesampledsubgraphs
input
fromthei-hopneighborhoodsindescendingorderofi,itfiltersirrelevantneighborhoodstothecontext(laststate),whichis
thenodeencoding.
LastLayer(s)ofBidirectionalMamba. Tocapturethelong-rangedependenciesandtoflowinformationacrossthenodes,
weusethenodeencodingsobtainedfromthelaststateofEquation9astheinputofthelastlayer(s)ofbidirectionalMamba.
8GraphMamba
Therefore,therecurrentscanofnodes(inbothdirections)canflowinformationacrossnodes. Thisdesignnotonlyhelps
capturinglong-rangedependenciesinthegraph,butitalsoisakeytotheflexibilityofourframeworktobridgenodeand
subgraphtokenization.
4.3.TokenizationWhenm=0
Inthiscase,foreachnodev âˆˆV weonlyconsidervitselfasitscorrespondingsequenceoftokens.Basedonourarchitecture,
inthiscase,thefirstlayersofbidirectionMambabecomesimpleprojectionasthelengthofthesequenceisone. However,
thelastlayers,whereweusenodeencodingsastheirinput,treatsnodesastokensandbecomeanarchitecturethatusea
sequentialencoder(e.g.,Mamba)withnodetokenization. Morespecifically,inthisspecialcaseofframework,themodelis
theadaptionofGPS(RampaÂ´sË‡eketal.,2022)framework,whenwereplaceitsTransformerwithourbidirectionalMamba.
Thisarchitecturedesignletsswitchingbetweennodeandsubgraphtokenizationusingasingleparameterm,makingthe
choiceoftokenizationatunablehyperparameterduringtraining. Notethatthisflexibilitycomesmorefromourarchitecture
ratherthanthemethodoftokenization. Thatis,inpracticeonecanuseonly0-hopneighborhoodinNAGphormer(Chen
etal.,2023),resultinginonlyconsideringthenodeitself. However,inthiscase,thearchitectureofNAGphormerbecomesa
stackofMLPs,resultinginpoorperformance.
TokenOrdering. Whenm=0: Oneremainingquestionishowonecanordernodeswhenweusenodetokenization. As
discussedinÂ§4.1,tokensneedtobeorderedbasedoneither(1)theirneedsofknowinginformationaboutothertokens
or(2)theirimportancetoourtask. Whendealingwithnodesandspecificallywhenlong-rangedependenciesmatter,(1)
becomesamustforallnodes. Ourarchitectureovercomesthischallengebyitsbidirectionalscanprocess. Therefore,we
needtoordernodesbasedontheirimportance. Thereareseveralmetricstomeasuretheimportanceofnodesinagraph.
Forexample,variouscentralitymeasures(Latora&Marchiori,2007;Ruhnau,2000),degree,k-core(Lick&White,1970;
Hashemietal.,2022),PersonalizedPageRankorPageRank(Pageetal.,1998),etc. Inourexperiments,forthesakeof
efficiencyandsimplicity,wesortnodesbasedontheirdegree.
HowDoesSelectionWorkonNodes? Similartoselectionmechanismonsubgraphs,themodelbasedontheinputdatacan
filterirrelevanttokens(nodes)tothecontext(downstreamtasks).
4.4.TheoreticalAnalysisofGMNs
Inthissection,weprovidetheoreticaljustificationforthepowerofGMNs. Morespecifically,wefirstshowthatGMNsare
universalapproximatorofanyfunctionongraphs. Next,wediscussthatgivenproperPEandenoughparameters,GMNs
aremorepowerfulthananyWLisomorphismtest,matchingGTs(withthesimilarassumptions). Finally,weevaluatethe
expressivepowerofGMNswhentheydonotuseanyPEorMPNNandshowthattheirexpressivepowerisunbounded
(mightbeincomparable).
Theorem 4.2 (Universality). Let 1 â‰¤ p < âˆž, and Ïµ > 0. For any continues function f : [0,1]dÃ—n â†’ RdÃ—n that is
permutationequivariant,thereexistsaGMNwithpositionalencoding,g ,suchthatâ„“p(f,g)<Ïµ.
p
Theorem4.3(ExpressivePowerw/PE). Giventhefullsetofeigenfunctionsandenoughparameters,GMNscandistinguish
anypairofnon-isomorphicgraphsandaremorepowerfulthananyWLtest.
WeprovetheabovetwotheoremsbasedontherecentworkofWang&Xue(2023), wheretheyprovethatSSMswith
layer-wisenonlinearityareuniversalapproximatorsofanysequence-to-sequencefunction.
Theorem4.4(ExpressivePowerw/oPEandMPNN). Withenoughparameters,foreveryk â‰¥1therearegraphsthatare
distinguishablebyGMNs,butnotbyk-WLtest,showingthattheirexpressivepowerisnotboundedbyanyWLtest.
WeprovetheabovetheorembasedontherecentworkofToÂ¨nshoffetal.(2023),wheretheyproveasimilartheoremfor
CRaWl(ToÂ¨nshoffetal.,2023). NotethatthistheoremdoesnotrelyontheMambaâ€™spower,andtheexpressivepowercomes
fromthechoiceofneighborhoodsamplingandencoding.
5.Experiments
Inthissection,weevaluatetheperformanceofGMNsinlong-range,small-scale,large-scale,andheterophilicbenchmark
datasets.Wefurtherdiscussitsmemoryefficiencyandperformablationstudytovalidatethecontributionofeacharchitectural
choice. Thedetailedstatisticsofdatasetsandadditionalexperimentsareavailableintheappendix.
9GraphMamba
Table1.BenchmarkonLong-RangeGraphDatasets(Dwivedietal.,2022).Highlightedarethetopfirst,second,andthirdresults.
COCO-SP PascalVOC-SP Peptides-Func Peptides-Struct
Model
F1scoreâ†‘ F1scoreâ†‘ APâ†‘ MAEâ†“
GCN 0.0841 0.1268 0.5930 0.3496
Â±0.0010 Â±0.0060 Â±0.0023 Â±0.0013
GIN 0.1339 0.1265 0.5498 0.3547
Â±0.0044 Â±0.0076 Â±0.0079 Â±0.0045
Gated-GCN 0.2641 0.2873 0.5864 0.3420
Â±0.0045 Â±0.0219 Â±0.0077 Â±0.0013
CRaWl 0.3219 0.4088 0.6963 0.2506
Â±0.00106 Â±0.0079 Â±0.0079 Â±0.0022
SAN+LapPE 0.2592 0.3230 0.6384 0.2683
Â±0.0158 Â±0.0039 Â±0.0121 Â±0.0043
NAGphormer 0.3458 0.4006 - -
Â±0.0070 Â±0.0061
GraphViT - - 0.6855 0.2468
Â±0.0049 Â±0.0015
GPS 0.3774 0.3689 0.6575 0.2510
Â±0.0150 Â±0.0131 Â±0.0049 Â±0.0015
GPS(BigBird) 0.2622 0.2762 0.5854 0.2842
Â±0.0008 Â±0.0069 Â±0.0079 Â±0.0130
Exphormer 0.3430 0.3446 0.6258 0.2512
Â±0.0108 Â±0.0064 Â±0.0092 Â±0.0025
GPS+Mamba 0.3895 0.4180 0.6624 0.2518
Â±0.0125 Â±0.012 Â±0.0079 Â±0.0012
GMN- 0.3618 0.4169 0.6860 0.2522
Â±0.0053 Â±0.0103 Â±0.0012 Â±0.0035
GMN 0.3974 0.4393 0.7071 0.2473
Â±0.0101 Â±0.0112 Â±0.0083 Â±0.0025
5.1.ExperimentalSetup
Dataset. Weusethreemostcommonlyusedbenchmarkdatasetswithlong-range,small-scale,large-scale,andheterophilic
properties. Forlong-rangedatasets,weuseLongeRangeGraphBenchmark(LRGB)dataset(Dwivedietal.,2022). For
smallandlarge-scaledatasets, weuseGNNbenchmark(Dwivedietal.,2023). ToevaluatetheGMNsonheterophilic
graphs,weusefourheterophilicdatasetsfromtheworkofPlatonovetal.(2023). Finally,weusealargedatasetfromOpen
GraphBenchmark(Huetal.,2020). WeevaluatetheperformanceofGMNsonvariousgraphlearningtasks(e.g.,graph
classification,regression,nodeclassificationandlinkclassification). Also,foreachdatasetsweusetheproposemetrics
intheoriginalbenchmarkandreportthemetricacrossmultipleruns,ensuringtherobustness. Wediscussdatasets,their
statisticsandtheirtasksinAppendixA.
Baselines. We compare our GMNs with (1) MPNNs, e.g., GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), and
Gated-GCN(Bresson&Laurent,2017),(2)RandomwalkbasedmethodCRaWl(ToÂ¨nshoffetal.,2023),(3)state-of-the-art
GTs, e.g., SAN (Kreuzer et al., 2021), NAGphormer (Chen et al., 2023), Graph ViT (He et al., 2023), two variants of
GPS(RampaÂ´sË‡eketal.,2022),andExphormer(Shirzadetal.,2023),and(4)ourbaselines(i)GPS+Mamba: whenwe
replacethetransformermoduleinGPSwithbidirectionalMamba. (ii)GMN-: whenwedonotusePE/SEandMPNN.We
usethesametraining,validation,andtestingforallthebaselinestomakesureafaircomparison.
5.2.LongRangeGraphBenchmark
Table1reportstheresultsofGMNsandbaselinesonlong-rangegraphbenchmark. GMNsconsistentlyoutperformbaselines
inalldatasetsthatrequireslong-rangedependenciesbetweennodes. Thereasonforthissuperiorperformanceisthree
folds: (1)GMNsbasedonourdesignuselongsequenceoftokenstolearnnodeencodingsandthenuseanotherselection
mechanismtofilterirrelevantnodes. TheprovidedlongsequenceoftokensenablesGMNstolearnlong-rangedependencies,
withoutfacingscalabilityorover-squashingissues. (2)GMNsusingtheirselectionmechanismarecapableoffilteringthe
neighborhoodaroundeachnode. Accordingly,onlyinformativeinformationflowsintohiddenstates. (3)Therandom-walk
basedneighborhoodsamplingallowGMNstohavediversesamplesofneighborhoods,whilecapturingthehierarchical
natureofk-hopneighborhoods. Also,itisnotablethatGMNconsistentlyoutperformsourbaselineGPS+Mamba,which
showstheimportanceofpayingattentiontothenewchallenges. Thatis,replacingthetransformermodulewithMamba,
whileimprovestheperformance,cannotfullytakeadvantageoftheMambatraits. Interestingly,GMN-,avariantofGMNs
without Transformer, MPNN, and PE/SE that we use to evaluate the importance of these elements in achieving good
performance,canachievecompetitiveperformancewithothercomplexmethods,showingthatwhileTransformers,complex
message-passing,andSE/PEaresufficientforgoodperformanceinpractice,neitherisnecessary.
10GraphMamba
Table2.BenchmarkonGNNBenchmarkDatasets(Dwivedietal.,2023).Highlightedarethetopfirst,second,andthirdresults.
MNIST CIFAR10 PATTERN MalNet-Tiny
Model
Accuracyâ†‘ Accuracyâ†‘ Accuracyâ†‘ Accuracyâ†‘
GCN 0.9071 0.5571 0.7189 0.8100
Â±0.0021 Â±0.0038 Â±0.0033 Â±0.0000
GIN 0.9649 0.5526 0.8539 0.8898
Â±0.0025 Â±0.0152 Â±0.0013 Â±0.0055
Gated-GCN 0.9734 0.6731 0.8557 0.9223
Â±0.0014 Â±0.0031 Â±0.0008 Â±0.0065
CRaWl 0.9794 0.6901 - -
Â±0.050 Â±0.0259
NAGphormer - - 0.8644 -
Â±0.0003
GPS 0.9811 0.7226 0.8664 0.9298
Â±0.0011 Â±0.0031 Â±0.0011 Â±0.0047
GPS(BigBird) 0.9817 0.7048 0.8600 0.9234
Â±0.0001 Â±0.0010 Â±0.0014 Â±0.0034
Exphormer 0.9843 0.7413 0.8670 0.9422
Â±0.0004 Â±0.0050 Â±0.0003 Â±0.0024
GPS+Mamba 0.9821 0.7341 0.8660 0.9311
Â±0.0004 Â±0.0015 Â±0.0007 Â±0.0042
GMN 0.9839 0.7576 0.8714 0.9415
Â±0.0018 Â±0.0042 Â±0.0012 Â±0.0020
Table3.Benchmarkonheterophilicdatasets(Platonovetal.,2023).Highlightedarethetopfirst,second,andthirdresults.
Roman-empire Amazon-ratings Minesweeper Tolokers
Model
Accuracyâ†‘ Accuracyâ†‘ ROCAUCâ†‘ ROCAUCâ†‘
GCN 0.7369 0.4870 0.8975 0.8364
Â±0.0074 Â±0.0063 Â±0.0052 Â±0.0067
Gated-GCN 0.7446 0.4300 0.8754 0.7731
Â±0.0054 Â±0.0032 Â±0.0122 Â±0.0114
NAGphormer 0.7434 0.5126 0.8419 0.7832
Â±0.0077 Â±0.0072 Â±0.0066 Â±0.0095
GPS 0.8200 0.5310 0.9063 0.8371
Â±0.0061 Â±0.0042 Â±0.0067 Â±0.0048
Exphormer 0.8903 0.5351 0.9074 0.8377
Â±0.0037 Â±0.0046 Â±0.0053 Â±0.0078
GOAT 0.7159 0.4461 0.8109 0.8311
Â±0.0125 Â±0.0050 Â±0.0102 Â±0.0104
GPS+Mamba 0.8310 0.4513 0.8993 0.8370
Â±0.0028 Â±0.0097 Â±0.0054 Â±0.0105
GMN 0.8769 0.5407 0.9101 0.8452
Â±0.0050 Â±0.0031 Â±0.0023 Â±0.0021
5.3.ComparisononGNNBenchmark
We further evaluate the performance of GMNs in small and large datasets from the GNN benchmark. The results of
GMNsandbaselineperformancearereportedinTable2. GMNandExphormerachievecompetitiveperformanceeach
outperformstheothertwotimes. Ontheotherhandagain,GMNconsistentlyoutperformsGPS+Mambabaseline,showing
theimportanceofdesigninganewframeworkforGMNsratherthenusingexistingframeworksofGTs.
5.4.HeterophilicDatasets
ToevaluatetheperformanceofGMNsontheheterophilcdataaswellasevaluatingtheirrobustnesstoover-squashing
andover-smoothing,wecomparetheirperformancewiththestate-of-the-artbaselinesandreporttheresultsinTable3.
OurGMNoutperformsbaselinesin3outof4datasetsandachievethesecondbestresultintheremainingdataset. These
resultsshowthattheselectionmechanisminGMNcaneffectivelyfilterirrelevantinformationandalsoconsiderlong-range
dependenciesinheterophilicdatasets.
5.5.AblationStudy
ToevaluatethecontributionofeachcomponentofGMNsinitsperformance,weperformablationstudy. Table4reportsthe
results. Thefirstrow,reportstheperformanceofGMNswithitsfullarchitecture. Thenineachrow,wemodifyonethe
elementswhilekeepingtheotherunchanged: Row2removethebidirectionalMambaanduseasimpleMamba. Row3
removetheMPNNandRow4usePPRordering. FinallythelastrowremovePE.Resultsshowthatalltheelementsof
GMNcontributestoitsperformancewithmostcontributionfrombidirectionMamba.
11GraphMamba
Table4. AblationstudyonGMNarchitecture.
Roman-empire Amazon-ratings Minesweeper
Model
Accuracyâ†‘ Accuracyâ†‘ ROCAUCâ†‘
GMN 0.8769 0.5407 0.9101
Â±0.0050 Â±0.0031 Â±0.0023
w/obidirectionalMamba 0.8327 0.5016 0.8597
Â±0.0062 Â±0.0045 Â±0.0028
w/oMPNN 0.8620 0.5312 0.8983
Â±0.0043 Â±0.0044 Â±0.0031
PPRordering 0.8612 0.5299 0.8991
Â±0.0019 Â±0.0037 Â±0.0021
w/oPE 0.8591 0.5308 0.9011
Â±0.0054 Â±0.0026 Â±0.0025
Figure2.EfficiencyevaluationandaccuracyofGMNsandbaselinesonOBGN-Arxivand GPS
1500 GMN
MalNet-Tiny.Highlightedarethetopfirst,second,andthirdresults.OOM:OutofMemory.
Method Gated-GCN GPS NAGphormer Exphormerâ€  GOAT Ours 1000
GPS+Mamba GMN
OGBN-Arxiv 500
Training/Epoch(s) 0.68 OOM 5.06 1.97 13.09 1.18 1.30
Memory(GB) 11.09 OOM 6.24 36.18 8.41 5.02 3.85 0
Accuracy 0.7141 OOM 0.7013 0.7228 0.7196 0.7239 0.7248 1 300 600 900 1200
Average number of nodes
MalNet-Tiny
Training/Epoch(s) 10.3 148.99 - 57.24 - 36.07 41.00
Figure3.MemoryofGPSandGMNon
Accuracy 0.9223 0.9234 - 0.9224 - 0.9311 0.9415
â€ Wefollowtheoriginalpaper(Shirzadetal.,2023)anduseonevirtualnodeinefficiencyevaluation. MalNet-Tinydataset.
5.6.Efficiency
Aswediscussedearlier,oneofthemainadvantagesofourmodelisitsefficiencyandmemoryusage. Weevaluatethisclaim
onOGBN-Arxiv(Huetal.,2020)andMalNet-Tiny(Dwivedietal.,2023)datasetsandreporttheresultsinTable2. Our
variantsofGMNsarethemostefficienctmethodswhileachievingthebestperformance. Toshowthetrendofscalability,we
useMalNet-TinyandplotthememoryusageofGPSandGMNinFigure3. WhileGPS,asagraphtransformerframework,
requireshighcomputationalcost(GPUmemoryusage),GMNsâ€™smemoryscaleslinearlywithrespecttotheinputsize.
6.Conclusion
Inthispaper,wepresentGraphMambaNetworks(GMNs)asanewclassofgraphlearningbasedonStateSpaceModel.
WediscussandcategorizethenewchallengeswhenadoptingSSMstograph-structureddata,andpresentfourrequiredand
oneoptionalstepstodesignGMNs,wherewechoose(1)NeighborhoodTokenization,(2)TokenOrdering,(3)Architecture
ofBidirectionalSelectiveSSMEncoder,(4)LocalEncoding,anddispensable(5)PEandSE.Wefurtherprovidetheoretical
justificationforthepowerofGMNsandconductseveralexperimentstoempiricallyevaluatetheirperformance.
PotentialBroaderImpact
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal
consequencesofourwork,noneofwhichwefeelmustbespecificallyhighlightedhere.
12
)BM(
yromeM
UPGGraphMamba
References
Ahamed, M. A. and Cheng, Q. Mambatab: A simple yet effective approach for handling tabular data. arXiv preprint
arXiv:2401.08867,2024.
Alon,U.andYahav,E.Onthebottleneckofgraphneuralnetworksanditspracticalimplications.InInternationalConference
onLearningRepresentations,2021. URLhttps://openreview.net/forum?id=i80OPhOCVH2.
Aoki,M. Statespacemodelingoftimeseries. SpringerScience&BusinessMedia,2013.
Arnaiz-RodrÂ´Ä±guez,A.,Begga,A.,Escolano,F.,andOliver,N.M. Diffwire: InductivegraphrewiringviathelovaÂ´szbound.
InTheFirstLearningonGraphsConference,2022. URLhttps://openreview.net/forum?id=IXvfIex0mX6f.
Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization,2016.
Baek,J.,Kang,M.,andHwang,S.J.Accuratelearningofgraphrepresentationswithgraphmultisetpooling.InInternational
ConferenceonLearningRepresentations,2021. URLhttps://openreview.net/forum?id=JHcqXGaqiGn.
Bar-Shalom,G.,Bevilacqua,B.,andMaron,H. Subgraphormer: SubgraphGNNsmeetgraphtransformers. InNeurIPS
2023Workshop: NewFrontiersinGraphLearning,2023. URLhttps://openreview.net/forum?id=e8ba9Hu1mM.
Behrouz,A.,Delavari,P.,andHashemi,F. Unsupervisedrepresentationlearningofbrainactivityviabridgingvoxelactivity
andfunctionalconnectivity. InNeurIPS2023AIforScienceWorkshop,2023. URLhttps://openreview.net/forum?id=
HSvg7qFFd2.
Bevilacqua,B.,Frasca,F.,Lim,D.,Srinivasan,B.,Cai,C.,Balamurugan,G.,Bronstein,M.M.,andMaron,H. Equivariant
subgraphaggregationnetworks. InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=dFbKQaRk15w.
Bresson,X.andLaurent,T. Residualgatedgraphconvnets. arXivpreprintarXiv:1711.07553,2017.
Chen,J.,Gao,K.,Li,G.,andHe,K. NAGphormer: Atokenizedgraphtransformerfornodeclassificationinlargegraphs.
InTheEleventhInternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.net/forum?id=
8KYeilT3Ow.
Choromanski,K.M.,Likhosherstov,V.,Dohan,D.,Song,X.,Gane,A.,Sarlos,T.,Hawkins,P.,Davis,J.Q.,Mohiuddin,
A.,Kaiser,L.,Belanger,D.B.,Colwell,L.J.,andWeller,A. Rethinkingattentionwithperformers. InInternational
ConferenceonLearningRepresentations,2021. URLhttps://openreview.net/forum?id=Ua6zuk0WRH.
DiGiovanni,F.,Giusti,L.,Barbero,F.,Luise,G.,Lio,P.,andBronstein,M.M. Onover-squashinginmessagepassing
neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning, pp.
7865â€“7885.PMLR,2023.
Ding, Y., Orvieto, A., He, B., andHofmann, T. Recurrentdistance-encodingneuralnetworksforgraphrepresentation
learning. arXivpreprintarXiv:2312.01538,2023.
Dwivedi,V.P.,RampaÂ´sË‡ek,L.,Galkin,M.,Parviz,A.,Wolf,G.,Luu,A.T.,andBeaini,D. Longrangegraphbenchmark.
InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.(eds.),AdvancesinNeuralInformation
ProcessingSystems,volume35,pp.22326â€“22340.CurranAssociates,Inc.,2022. URLhttps://proceedings.neurips.cc/
paper files/paper/2022/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets and Benchmarks.pdf.
Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio,Y.,andBresson,X. Benchmarkinggraphneuralnetworks.
JournalofMachineLearningResearch,24(43):1â€“48,2023.
Errica,F.,Christiansen,H.,Zaverkin,V.,Maruyama,T.,Niepert,M.,andAlesiani,F. Adaptivemessagepassing: Ageneral
frameworktomitigateoversmoothing,oversquashing,andunderreaching. arXivpreprintarXiv:2312.16560,2023.
Fan,W.,Ma,Y.,Li,Q.,He,Y.,Zhao,E.,Tang,J.,andYin,D. Graphneuralnetworksforsocialrecommendation. InThe
worldwidewebconference,pp.417â€“426,2019.
13GraphMamba
Finkelshtein, B., Huang, X., Bronstein, M., and Ceylan, Ë™I. Ë™I. Cooperative graph neural networks. arXiv preprint
arXiv:2310.01267,2023.
Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language
modelingwithstatespacemodels. InTheEleventhInternationalConferenceonLearningRepresentations,2023. URL
https://openreview.net/forum?id=COZDy0WYGg.
Gu,A.andDao,T. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXivpreprintarXiv:2312.00752,
2023.
Gu,A.,Dao,T.,Ermon,S.,Rudra,A.,andReÂ´,C. Hippo:Recurrentmemorywithoptimalpolynomialprojections. Advances
inneuralinformationprocessingsystems,33:1474â€“1487,2020.
Gu,A.,Johnson,I.,Goel,K.,Saab,K.,Dao,T.,Rudra,A.,andReÂ´,C. Combiningrecurrent,convolutional,andcontinuous-
timemodelswithlinearstatespacelayers. Advancesinneuralinformationprocessingsystems,34:572â€“585,2021.
Gu,A.,Goel,K.,andRe,C. Efficientlymodelinglongsequenceswithstructuredstatespaces. InInternationalConference
onLearningRepresentations,2022. URLhttps://openreview.net/forum?id=uYLFoz1vlAC.
Gutteridge,B.,Dong,X.,Bronstein,M.M.,andDiGiovanni,F. Drew: Dynamicallyrewiredmessagepassingwithdelay.
InInternationalConferenceonMachineLearning,pp.12252â€“12267.PMLR,2023.
Hashemi,F.,Behrouz,A.,andLakshmanan,L.V. Firmcoredecompositionofmultilayernetworks. InProceedingsof
theACMWebConference2022,WWWâ€™22,pp.1589â€“1600,NewYork,NY,USA,2022.AssociationforComputing
Machinery. ISBN9781450390965. doi: 10.1145/3485447.3512205. URLhttps://doi.org/10.1145/3485447.3512205.
He, X., Hooi, B., Laurent, T., Perold, A., LeCun, Y., andBresson, X. Ageneralizationofvit/mlp-mixertographs. In
InternationalConferenceonMachineLearning,pp.12724â€“12745.PMLR,2023.
Henaff, M., Bruna, J., and LeCun, Y. Deep convolutional networks on graph-structured data. arXiv preprint
arXiv:1506.05163,2015.
Hu,W.,Fey,M.,Zitnik,M.,Dong,Y.,Ren,H.,Liu,B.,Catasta,M.,andLeskovec,J. Opengraphbenchmark: Datasetsfor
machinelearningongraphs. Advancesinneuralinformationprocessingsystems,33:22118â€“22133,2020.
Hussain, M. S., Zaki, M. J., and Subramanian, D. Global self-attention as a replacement for graph convolution. In
Proceedingsofthe28thACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining,pp.655â€“665,2022.
Kim, J., Nguyen, D., Min, S., Cho, S., Lee, M., Lee, H., andHong, S. Puretransformersarepowerfulgraphlearners.
AdvancesinNeuralInformationProcessingSystems,35:14582â€“14595,2022.
Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint
arXiv:1609.02907,2016.
Kong,K.,Chen,J.,Kirchenbauer,J.,Ni,R.,Bruss,C.B.,andGoldstein,T. Goat: Aglobaltransformeronlarge-scale
graphs. InInternationalConferenceonMachineLearning,pp.17375â€“17390.PMLR,2023.
Kreuzer,D.,Beaini,D.,Hamilton,W.,LeÂ´tourneau,V.,andTossou,P. Rethinkinggraphtransformerswithspectralattention.
AdvancesinNeuralInformationProcessingSystems,34:21618â€“21629,2021.
Kuang,W.,Zhen,W.,Li,Y.,Wei,Z.,andDing,B. Coarformer: Transformerforlargegraphviagraphcoarsening. 2021.
Latora,V.andMarchiori,M. Ameasureofcentralitybasedonnetworkefficiency. NewJournalofPhysics,9(6):188,2007.
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based
permutation-invariantneuralnetworks. InInternationalconferenceonmachinelearning,pp.3744â€“3753.PMLR,2019.
Lick, D. R. and White, A. T. k-degenerate graphs. Canadian Journal of Mathematics, 22(5):1082â€“1096, 1970. doi:
10.4153/CJM-1970-125-1.
14GraphMamba
Lim,D.,Robinson,J.D.,Zhao,L.,Smidt,T.,Sra,S.,Maron,H.,andJegelka,S. Signandbasisinvariantnetworksfor
spectralgraphrepresentationlearning. InTheEleventhInternationalConferenceonLearningRepresentations,2023a.
URLhttps://openreview.net/forum?id=Q-UHqMorzil.
Lim,H.,Joo,Y.,Ha,E.,Song,Y.,Yoon,S.,Lyoo,I.K.,andShin,T. Brainagepredictionusingmulti-hopgraphattention
module(MGA)withconvolutionalneuralnetwork. InMedicalImagingwithDeepLearning,shortpapertrack,2023b.
URLhttps://openreview.net/forum?id=brK-VVoDpqo.
Liu,C.,Zhan,Y.,Ma,X.,Ding,L.,Tao,D.,Wu,J.,andHu,W. Gapformer: Graphtransformerwithgraphpoolingfor
nodeclassification. InProceedingsofthe32ndInternationalJointConferenceonArtificialIntelligence(IJCAI-23),pp.
2196â€“2205,2023.
Liu, J., Yang, H., Zhou, H.-Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al. Swin-umamba:
Mamba-basedunetwithimagenet-basedpretraining. arXivpreprintarXiv:2402.03302,2024a.
Liu,X.,Yu,H.-F.,Dhillon,I.,andHsieh,C.-J. Learningtoencodepositionfortransformerwithcontinuousdynamical
model. InInternationalconferenceonmachinelearning,pp.6327â€“6335.PMLR,2020.
Liu,Y.,Tian,Y.,Zhao,Y.,Yu,H.,Xie,L.,Wang,Y.,Ye,Q.,andLiu,Y. Vmamba: Visualstatespacemodel. arXivpreprint
arXiv:2401.10166,2024b.
Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,andGuo,B. Swintransformer: Hierarchicalvisiontransformer
usingshiftedwindows. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pp.10012â€“10022,
2021.
Ma, J.,Li, F., andWang, B. U-mamba: Enhancinglong-rangedependencyforbiomedicalimagesegmentation. arXiv
preprintarXiv:2401.04722,2024.
Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,J.E.,Rattan,G.,andGrohe,M. Weisfeilerandlemangoneural:
Higher-ordergraphneuralnetworks. InProceedingsoftheAAAIconferenceonartificialintelligence,volume33,pp.
4602â€“4609,2019.
Morris,C.,Rattan,G.,andMutzel,P. Weisfeilerandlemangosparse: Towardsscalablehigher-ordergraphembeddings.
AdvancesinNeuralInformationProcessingSystems,33:21824â€“21840,2020.
MuÂ¨ller,L.,Galkin,M.,Morris,C.,andRampaÂ´sË‡ek,L. Attendingtographtransformers. arXivpreprintarXiv:2302.04181,
2023.
Murphy, R., Srinivasan, B., Rao, V., and Ribeiro, B. Relational pooling for graph representations. In International
ConferenceonMachineLearning,pp.4663â€“4673.PMLR,2019.
Nguyen, D. Q., Nguyen, T. D., and Phung, D. Universal graph transformer self-attention networks. In Companion
ProceedingsoftheWebConference2022,pp.193â€“196,2022a.
Nguyen,E.,Goel,K.,Gu,A.,Downs,G.,Shah,P.,Dao,T.,Baccus,S.,andReÂ´,C. S4nd: Modelingimagesandvideosas
multidimensionalsignalswithstatespaces. Advancesinneuralinformationprocessingsystems,35:2846â€“2861,2022b.
Nguyen, E., Poli, M., Faizi, M., Thomas, A. W., Wornow, M., Birch-Sykes, C., Massaroli, S., Patel, A., Rabideau,
C.M.,Bengio,Y.,Ermon,S.,Re,C.,andBaccus,S. HyenaDNA:Long-rangegenomicsequencemodelingatsingle
nucleotide resolution. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=ubzNoJjOKj.
Page,L.,Brin,S.,Motwani,R.,andWinograd,T. Thepagerankcitationranking: Bringordertotheweb. Technicalreport,
Technicalreport,stanfordUniversity,1998.
Park,J.,Yun,S.,Park,H.,Kang,J.,Jeong,J.,Kim,K.-M.,Ha,J.-w.,andKim,H.J. Deformablegraphtransformer. arXiv
preprintarXiv:2206.14337,2022.
Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evaluation of
GNNs under heterophily: Are we really making progress? In The Eleventh International Conference on Learning
Representations,2023. URLhttps://openreview.net/forum?id=tJbbQfw-5wv.
15GraphMamba
RampaÂ´sË‡ek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf,G.,andBeaini,D. Recipeforageneral,powerful,scalable
graphtransformer. AdvancesinNeuralInformationProcessingSystems,35:14501â€“14515,2022.
Rong,Y.,Bian,Y.,Xu,T.,Xie,W.,Wei,Y.,Huang,W.,andHuang,J. Self-supervisedgraphtransformeronlarge-scale
moleculardata. AdvancesinNeuralInformationProcessingSystems,33:12559â€“12571,2020.
Ruhnau,B. Eigenvector-centralityâ€”anode-centrality? Socialnetworks,22(4):357â€“365,2000.
Rusch, T.K., Bronstein, M.M., andMishra, S. Asurveyonoversmoothingingraphneural networks. arXivpreprint
arXiv:2303.10993,2023.
Sato,R.,Yamada,M.,andKashima,H. Randomfeaturesstrengthengraphneuralnetworks. InProceedingsofthe2021
SIAMinternationalconferenceondatamining(SDM),pp.333â€“341.SIAM,2021.
Shi,F.,Chen,X.,Misra,K.,Scales,N.,Dohan,D.,Chi,E.H.,SchaÂ¨rli,N.,andZhou,D. Largelanguagemodelscanbe
easilydistractedbyirrelevantcontext. InInternationalConferenceonMachineLearning,pp.31210â€“31227.PMLR,
2023.
Shirzad,H.,Velingker,A.,Venkatachalam,B.,Sutherland,D.J.,andSinop,A.K. Exphormer: Sparsetransformersfor
graphs. arXivpreprintarXiv:2303.06147,2023.
Shiv, V. and Quirk, C. Novelpositional encodingsto enabletree-based transformers. Advances inneural information
processingsystems,32,2019.
Tang,S.,Dunnmon,J.A.,Liangqiong,Q.,Saab,K.K.,Baykaner,T.,Lee-Messer,C.,andRubin,D.L.Modelingmultivariate
biosignalswithgraphneuralnetworksandstructuredstatespacemodels. InMortazavi,B.J.,Sarker,T.,Beam,A.,andHo,
J.C.(eds.),ProceedingsoftheConferenceonHealth,Inference,andLearning,volume209ofProceedingsofMachine
LearningResearch,pp.50â€“71.PMLR,22Junâ€“24Jun2023. URLhttps://proceedings.mlr.press/v209/tang23a.html.
ToÂ¨nshoff,J.,Ritzert,M.,Wolf,H.,andGrohe,M. Walkingoutoftheweisfeilerlemanhierarchy: Graphlearningbeyond
messagepassing. TransactionsonMachineLearningResearch,2023. ISSN2835-8856. URLhttps://openreview.net/
forum?id=vgXnEyeWVY.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,Å.,andPolosukhin,I. Attentionisall
youneed. Advancesinneuralinformationprocessingsystems,30,2017.
VelicË‡kovicÂ´,P.,Cucurull,G.,Casanova,A.,Romero,A.,Lio`,P.,andBengio,Y. Graphattentionnetworks. InInternational
ConferenceonLearningRepresentations,2018. URLhttps://openreview.net/forum?id=rJXMpikCZ.
Wang,H.,Yin,H.,Zhang,M.,andLi,P.Equivariantandstablepositionalencodingformorepowerfulgraphneuralnetworks.
InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.net/forum?id=e95i1IHcWj.
Wang,S.andXue,B.State-spacemodelswithlayer-wisenonlinearityareuniversalapproximatorswithexponentialdecaying
memory. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023. URLhttps://openreview.net/
forum?id=i0OmcF14Kf.
Wang,Y.,Min,Y.,Shao,E.,andWu,J. Moleculargraphcontrastivelearningwithparameterizedexplainableaugmentations.
In2021IEEEInternationalConferenceonBioinformaticsandBiomedicine(BIBM),pp.1558â€“1563.IEEE,2021.
Wolf,T.,Debut,L.,Sanh,V.,Chaumond,J.,Delangue,C.,Moi,A.,Cistac,P.,Rault,T.,Louf,R.,Funtowicz,M.,etal.
Transformers: State-of-the-artnaturallanguageprocessing. InProceedingsofthe2020conferenceonempiricalmethods
innaturallanguageprocessing: systemdemonstrations,pp.38â€“45,2020.
Wu,Y.,Xu,Y.,Zhu,W.,Song,G.,Lin,Z.,Wang,L.,andLiu,S. Kdlgt: alineargraphtransformerframeworkviakernel
decompositionapproach. InProceedingsoftheThirty-SecondInternationalJointConferenceonArtificialIntelligence,
pp.2370â€“2378,2023.
Wu,Z.,Pan,S.,Chen,F.,Long,G.,Zhang,C.,andPhilip,S.Y. Acomprehensivesurveyongraphneuralnetworks. IEEE
transactionsonneuralnetworksandlearningsystems,32(1):4â€“24,2020.
16GraphMamba
Xing,Z.,Ye,T.,Yang,Y.,Liu,G.,andZhu,L. Segmamba: Long-rangesequentialmodelingmambafor3dmedicalimage
segmentation. arXivpreprintarXiv:2401.13560,2024.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerfularegraphneuralnetworks? InInternationalConferenceon
LearningRepresentations,2019. URLhttps://openreview.net/forum?id=ryGs6iA5Km.
Yang, Y., Xing, Z., and Zhu, L. Vivim: a video vision mamba for medical video object segmentation. arXiv preprint
arXiv:2401.14168,2024.
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,andLiu,T.-Y. Dotransformersreallyperformbadlyforgraph
representation? AdvancesinNeuralInformationProcessingSystems,34:28877â€“28888,2021.
Yu,Z.,Cao,R.,Tang,Q.,Nie,S.,Huang,J.,andWu,S. Ordermatters: Semantic-awareneuralnetworksforbinarycode
similaritydetection. InProceedingsoftheAAAIconferenceonartificialintelligence,volume34,pp.1145â€“1152,2020.
Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. Graph transformer networks. Advances in neural information
processingsystems,32,2019.
Zaheer,M.,Guruganesh,G.,Dubey,K.A.,Ainslie,J.,Alberti,C.,Ontanon,S.,Pham,P.,Ravula,A.,Wang,Q.,Yang,L.,
etal. Bigbird: Transformersforlongersequences. Advancesinneuralinformationprocessingsystems,33:17283â€“17297,
2020.
Zhang,B.,Feng,G.,Du,Y.,He,D.,andWang,L. AcompleteexpressivenesshierarchyforsubgraphGNNsviasubgraph
weisfeiler-lehman tests. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),
Proceedingsofthe40thInternationalConferenceonMachineLearning,volume202ofProceedingsofMachineLearning
Research,pp.41019â€“41077.PMLR,23â€“29Jul2023a. URLhttps://proceedings.mlr.press/v202/zhang23k.html.
Zhang,M.,Saab,K.K.,Poli,M.,Dao,T.,Goel,K.,andRe,C. Effectivelymodelingtimeserieswithsimplediscretestate
spaces. InTheEleventhInternationalConferenceonLearningRepresentations,2023b. URLhttps://openreview.net/
forum?id=2EpjkjzdCAa.
Zhang,Z.,Liu,Q.,Hu,Q.,andLee,C.-K. Hierarchicalgraphtransformerwithadaptivenodesampling. AdvancesinNeural
InformationProcessingSystems,35:21171â€“21183,2022.
Zhao,J.,Li,C.,Wen,Q.,Wang,Y.,Liu,Y.,Sun,H.,Xie,X.,andYe,Y. Gophormer: Ego-graphtransformerfornode
classification. arXivpreprintarXiv:2110.13094,2021.
Zhong,W.,He,C.,Xiao,C.,Liu,Y.,Qin,X.,andYu,Z. Long-distancedependencycombinedmulti-hopgraphneural
networksforproteinâ€“proteininteractionsprediction. BMCbioinformatics,23(1):1â€“21,2022.
Zhu,L.,Liao,B.,Zhang,Q.,Wang,X.,Liu,W.,andWang,X. Visionmamba: Efficientvisualrepresentationlearningwith
bidirectionalstatespacemodel. arXivpreprintarXiv:2401.09417,2024.
17GraphMamba
Table5. DatasetStatistics.
Setup
Dataset #Graphs Average#Nodes Average#Edges #Class Metric
InputLevel Task
Long-rangeGraphBenchmark(Dwivedietal.,2022)
COCO-SP 123,286 476.9 2693.7 81 Node Classification F1score
PascalVOC-SP 11,355 479.4 2710.5 21 Node Classification F1score
Peptides-Func 15,535 150.9 307.3 10 Graph Classification AveragePrecision
Peptides-Struct 15,535 150.9 307.3 11(regression) Graph Regression MeanAbsoluteError
GNNBenchmark(Dwivedietal.,2023)
MNIST 70,000 70.6 564.5 10 Graph Classification Accuracy
CIFAR10 60,000 117.6 941.1 10 Graph Classification Accuracy
Pattern 14,000 118.9 3,039.3 2 Node Classification Accuracy
MalNet-Tiny 5,000 1,410.3 2,859.9 5 Graph Classification Accuracy
HeterophilicBenchmark(Platonovetal.,2023)
Roman-empire 1 22,662 32,927 18 Node Classification Accuracy
Amazon-ratings 1 24,492 93,050 5 Node Classification Accuracy
Minesweeper 1 10,000 39,402 2 Node Classification ROCAUC
Tolokers 1 11,758 519,000 2 Node Classification ROCAUC
VeryLargeDataset(Huetal.,2020)
OGBN-Arxiv 1 169,343 1,166,243 40 Node Classification Accuracy
A.DetailsofDatasets
ThestatisticsofallthedatasetsarereportedinTable5. Foradditionaldetailsaboutthedatasets,werefertotheLong-range
graphbenchmark(Dwivedietal.,2022),GNNBenchmark(Dwivedietal.,2023),HeterophilicBenchmark(Platonovetal.,
2023),andOpenGraphBenchmark(Huetal.,2020).
B.ExperimentalSetup
B.1.Hyperparameters
Weusetherecommendedsettingsintheofficialimplementationsandperformhyperparametertuningforeachbaseline.
Also,weusegridsearchtotunehyperparameters. Followingpreviousstudies,weusethesamesplitoftraning/test/validation
as(RampaÂ´sË‡eketal.,2022). Wereporttheresultsoverthe4randomseeds.
C.DetailsofGMNArchitecture: Algorithms
Algorithm1showstheforwardpassoftheGraphMambaNetworkwithonelayer. Foreachnode,GMNfirstsamplesM
walkswithlengthmË† = 1,...,mandconstructsitscorrespondingtokens,eachofwhichastheinducedsubgraphofM
walkswithlengthmË†. Werepeatthisprocessstimestohavelongersequenceandmoresamplesfromeachhierarchyofthe
neighborhoods. Thispartofthealgorithm,canbecomputedbeforethetrainingprocessandinCPU.Next,GMNsforeach
nodeencodeitstokensusinganencoderÏ•(.),whichcanbeanMPNN(e.g.,gated-GCN(Bresson&Laurent,2017))or
RWFencoding(proposedbyToÂ¨nshoffetal.(2023)). WethenpasstheencodingstoaBidirectionalMambablock,which
wedescribeinAlgorithm2(ThisalgorithmissimpletwoMambablock(Gu&Dao,2023)suchthatweuseoneofthe
backwardorforwardorderingofinputsforeachofthem). Attheendofline15,wehavethenodeencodingsobtainedfrom
subgraphtokenization. Next,wetreateachnodeasatokenandpasstheencodingtoanotherbidirectionalMamba,witha
specificorder. Wehaveuseddegreeorderinginourexperiments,buttherearesomeotherapproachesthatwehavediscussed
inthemainpaper.
D.AdditionalExperimentalResults
D.1.ParameterSensitivity
TheeffectofM. ParameterM isthenumberofwalksthatweaggregatetoconstructasubgraphtoken. Toevaluateitseffect
ontheperformanceoftheGMN,weusetwodatasetsofRoman-empireandPascalVOC-SP,fromtwodifferentbenchmarks,
18GraphMamba
Table6. Searchspaceofhyperparametersforeachdatasetâ€ .
Dataset M s #Layers Max#Epochs LearningRate
Long-rangeGraphBenchmark(Dwivedietal.,2022)
COCO-SP {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
PascalVOC-SP {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
Peptides-Func {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
Peptides-Struct {1,2,4,8,16,32} {0,1,2,4,8,16} {4,5} 300 0.001
GNNBenchmark(Dwivedietal.,2023)
MNIST {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
CIFAR10 {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Pattern {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
MalNet-Tiny {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
HeterophilicBenchmark(Platonovetal.,2023)
Roman-empire {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Amazon-ratings {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Minesweeper {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
Tolokers {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
VeryLargeDataset(Huetal.,2020)
OGBN-Arxiv {1,2,4,8,16,32} {0,1,2,4,8,16} {3,4,6} 300 0.001
â€ Thisspaceisnotfullysearchedandpreliminaryresultsarereportedbasedonitssubspace.Wewillupdatetheresultsaccordingly.
Roman-empire
0.8 0.8 0.8 PascalVOC-SP
Roman-empire Roman-empire
PascalVOC-SP PascalVOC-SP
0.6 0.6
0.6
0.4 0.4
0.4
1 2 4 6 8 10 1 5 10 20 30 40 50 60 5 10 15
M m s
Figure4. Theeffectof(Left)M,(Middle)m,and(Right)sontheperformanceofGMNs.
andvarythevalueofM fromXtoY.TheresultsarereportedinFigure4(Left). Theseresultsshowthatperformancepeaks
atcertainvalueofM andtheexactvaluevarieswiththedataset. Themainreasonis,parameterM determinesthathow
manywalkscanbeagoodrepresentativeoftheneighborhoodofanode,andsodependsonthedensity,homophilyscore,
andnetworktopologythisvaluecanbedifferent.
The effect of m. Similar to the above, we use two datasets of Roman-empire and PascalVOC-SP, from two different
benchmarks, and vary the value of m from X to Y. The results are reported in Figure 4 (Middle). The performance is
non-decreasingwithrespecttothevalueofm. Thatis,increasingthevalueofm,i.e.,consideringfarneighborsinthe
tokenizationprocess,doesnotdamagetheperformance(mightleadtobetterresults). Intuitively,usinglargevaluesofmis
expectedtodamagetheperformanceduetotheover-smoothingandover-squashing;however,theselectionmechanism
inBidirectionalMambacanselectinformativetokens(i.e.,neighborhood),filteringinformationthatcauseperformance
damage.
Theeffectofs. Usingthesamesettingastheabove,wereporttheresultswhenvaryingthevalueofsinFigure4(Right).
Result show that increasing the value of s can monotonically improve the performance. As discussed earlier, longer
sequencesoftokenscanprovidemorecontextforourmodelandduetotheselectionmechanisminMamba(Gu&Dao,
2023),GMNscanselectinformativesubgraphs/nodesandfilterirrelevanttokens,resultinginbetterresultswithlonger
sequences.
19
CCA/1F CCA/1F CCA/1FGraphMamba
Algorithm1GraphMambaNetworks(withonelayer)
Input: AgraphG=(V,E),inputfeaturesXâˆˆRnÃ—d,orderedarrayofnodesV ={v ,...,v },andhyperparametersM,m,ands.
1 n
Optional:MatrixP,whoserowsarepositional/structuralencodingscorrespondtonodes,and/oraMPNNmodelÎ¨(.).
Output: TheupdatednodeencodingsX .
new
1: forvâˆˆV do â–·Thisblockcanbedonebeforethetraining.
2: formË† =0,...,mdo
3: forsË†=1,...,sdo
4: TsË†(v)â†âˆ…;
mË†
5: forMË† =1,...,M do
6: Wâ†SamplearandomwalkwithlengthmË† startingfromv;
7: TsË†(v)â†TsË†(v)âˆª{u|uâˆˆW};
mË† mË†
8:
9: â–·Startthetraining:
10: Initializealllearnableparameters;
11: forvâˆˆV do
12: forj =1,...,sdo
13: fori=1,...,mdo
(cid:16) (cid:17)
14: x(iâˆ’1)s+j â†Ï• G[Tj(v)],X ||P ;
v i Tj(v) Tj(v)
i i
Î¦ â†âˆ¥sm xi; â–·Î¦ isamatrixwhoserowsarexi.
v i=1 v v v
15: y (v)â†BiMamba(Î¦ ); â–·UsingAlgorithm2.
output v
16: â–·Eachnodeisatoken:
17: Y â†âˆ¥sm y (v); â–·yisamatrixwhoserowsarey (v).
i=1 output output
18: Y â†BiMamba(Y)+Î¨(G,Xâˆ¥P);
output
Table7. ComparisonwithGREDModel.
MNIST CIFAR10 PATTERN Peptides-func Peptides-struct
Model
Accuracyâ†‘ Accuracyâ†‘ Accuracyâ†‘ APâ†‘ MAEâ†“
GREDâ€  0.9822 0.7537 0.8676 0.7041 0.2503
Â±0.0095 Â±0.6210 Â±0.0200 Â±0.0049 Â±0.0019
GMN 0.9839 0.7576 0.8714 0.7071 0.2473
Â±0.0018 Â±0.0042 Â±0.0012 Â±0.0083 Â±0.0025
â€ ResultsarereportedbyDingetal.(2023).
D.2.ComparisonwithGRED(Dingetal.,2023)
GRED(Dingetal.,2023)isarecentworkonArXivthatusesanRNNonthesetofneighborswithdistancek =1,...,K
toanodeofinterestforthenodeclassificationtask. SincethecodeormodelsofGREDarenotavailable,forthecomparison
ofGMNsandGRED,werunGMNsonthedatasetsusedintheoriginalpaper(Dingetal.,2023). Theresultsarereported
inTable7. GMNsconsistentlyoutperformsGRED(Dingetal.,2023)inalldatasets. Thereasonistwofolds: (1)GMNs
usesampledwalksinsteadofallthenodeswithink-hopneighborhood. AsdiscussedinTheorem4.1,thisapproachwith
largeenoughlengthandsamplesismoreexpressivethanconsideringallnodeswithintheneighborhood. (2)GREDuses
simpleRNNtoaggregatetheinformationaboutallthedifferentneighborhoodsofanode,whileGMNsuseMamba,which
haveaselectionmechanism. Thisselectionmechanismhelpthemodeltochooseneighborhoodsthataremoreinformative
andimportantthanothers.
E.ComplexityAnalysisofGNMs
mâ‰¥1. Foreachnodev âˆˆV,wegenerateM Ã—swalkswithlengthmË† =1,...,m,whichrequiresO(M Ã—sÃ—(m+1))
time. Given K tokens, the complexity of bidirectional Mamba is 2Ã— of Mamba (Gu & Dao, 2023), which is linear
withrespecttoK. Accordingly,sincewehaveO(M Ã—sÃ—m)tokens,thefinalcomplexityforagivennodev âˆˆ V is
O(M Ã—sÃ—(m+1)). Repeatingtheprocessforallnodes, thetimecomplexityisO(M Ã—sÃ—(m+1)Ã—|V|+|E|),
whichislinearintermsof|V|and|E|(graphsize). TocomparetothequadratictimecomplexityofGTs,evenforsmall
networks,notethatinpractice,M Ã—sÃ—(m+1)â‰ª|V|,andinourexperimentsusuallyM Ã—sÃ—(m+1)â‰¤200. Also,
notethatusingMPNNasanoptionalstepcannotaffectthetimecomplexityastheMPNNrequiresO(|V|+|E|)time.
m=0. Inthiscase,eachnodeisatokenandsotheGMNrequiresO(|V|)time. UsingMPNNinthearchitecture,thetime
20GraphMamba
Algorithm2BidirectionalMamba
Input: AsequenceÎ¦(Orderedmatrix,whereeachrowisatoken).
Output: TheupdatedsequenceencodingsÎ¦.
1: â–·ForwardScan:
2: Î¦ =Ïƒ(Conv(W LayerNorm(Î¦)));
f input,f
3: B =W Î¦;
f Bf f
4: C =W Î¦;
f Cf f
5: âˆ† =Softplus(W Î¦);
6:
AÂ¯f
=Discrete
(Aâˆ† ,âˆ†f );f
A
7: BÂ¯ =Discrete (B,âˆ†);
f Bf f
8 9:
:
y Yf = =S WSM AÂ¯, (BÂ¯ yf,C âŠ™f( ÏƒÎ¦ (f W);
LayerNorm(Î¦)));
f f,1 f f,2
10: â–·BackwardScan:
11: Î¦â†Reverse-rows(Î¦); â–·Reversetheorderofrowsinthematrix.
12: Î¦ =Ïƒ(Conv(W LayerNorm(Î¦)));
b input,b
13: B =W Î¦ ;
b Bb b
14: C =W Î¦ ;
b Cb b
15: âˆ† =Softplus(W Î¦ );
16:
AÂ¯b
=Discrete
(A,âˆ† âˆ†b );b
A
17: BÂ¯ =Discrete (B ,âˆ†);
b Bb b
18: y b =SSM AÂ¯,BÂ¯ b,Cb(Î¦ b);
19: Y =W (y âŠ™Ïƒ(W LayerNorm(Î¦)));
b b,1 b b,2
20: â–·Output:
21: y â†W (Y +Reverse-row(Y ));
output out f b
22: returny ;
output
complexitywouldbeO(|V|+|E|),dominatingbytheMPNNtimecomplexity.
Asdiscussedabove,basedonthepropertiesofMambaarchitecture,longersequenceoftokens(largervalueofsâ‰¥1)can
improvetheperformanceofthemethod. Basedontheabovementionedtimecomplexitywhenmâ‰¥1,wecanseethatthere
isatrade-offbetweentimecomplexityandtheperformanceofthemodel. Thatis,whilelargersresultinbetterperformance,
itresultsinslowermodel.
F.TheoreticalAnalysisofGMNs
TheoremF.1. WithlargeenoughM,m,ands>0,GMNsâ€™neighborhoodsamplingisstrictlymoreexpressivethank-hop
neighborhoodsampling.
Proof. Wefirstshowthatinthiscondition,therandomwalkneighborhoodsamplingisasexpressiveask-hopneighborhood
sampling. Tothisend,givenanarbitrarysmallÏµ>0,weshowthattheprobabilitythatk-hopneighborhoodsamplingis
moreexpressivethanrandomwalkneighborhoodsamplingislessthanÏµ. Letm=k,s=1,andp betheprobabilitythat
u,v
wesamplenodevinawalkwithlengthm=kstartingfromnodeu. Thisprbobalityiszeroiftheshortestpathofuandv
ismorethank. ToconstructthesubgraphtokencorrespondstomË† =k,weuseM samplesandsotheprobabilityofnot
seeingnodevinthesesamplesisq =(1âˆ’p )M â‰¤1. NowletM â†’âˆžandv âˆˆN (u)(i.e.,p Ì¸=0),wehave
u,v,M u,v k u,v
lim q =0. Accordingly,withlargeenoughM,wehaveq â‰¤Ïµ. ThismeansthatwithalargeenoughM
Mâ†’âˆž u,v,M u,v,M
whenm=kands=1,wesampleallthenodeswithinthek-hopneighborhood,meaningthatrandomwalkneighborhood
samplingatleastprovideasmuchinformationask-hopneighborhoodsamplingwitharbitrarylargeprobability.
Next,weprovideanexamplethatk-hopneighborhoodsamplingisnotabletodistinguishtwonon-isomorphismgraphs,
whilerandomwalksamplingcan. LetS = {v ,v ,...,v }beasetofnodessuchthatallnodeshaveshortestpathless
1 2 â„“
thanktou. Usinghyperparamtersm=kandarbitraryM,lettheprobabilitythatwegetG[S]asthesubgraphtokenbe
1>q >0. Usingssamples,theprobabilitythatwedonothaveG[S]asoneofthesubgraphtokensis(1âˆ’q )s. Now
S S
usinglargesâ†’âˆž,wehavelim (1âˆ’q )s =0andsoforanyarbitraryÏµ>0thereisalarges>0suchthatwesee
sâ†’âˆž S
allnon-emptysubgraphsofthek-hopneighborhoodwithprobabilitymorethan1âˆ’Ïµ,whichismorepowerfulthanthe
neighborhooditself.
Notethattheaboveproofdoesnotnecessarilyguaranteeanefficientsampling,butitguaranteestheexpressivepower.
Theorem F.2 (Universality). Let 1 â‰¤ p < âˆž, and Ïµ > 0. For any continues function f : [0,1]dÃ—n â†’ RdÃ—n that is
21GraphMamba
permutationequivariant,thereexistsaGMNwithpositionalencoding,g ,suchthatâ„“p(f,g)<Ïµ.
p
Proof. Recently,Wang&Xue(2023)showthatSSMswithlayer-wisenonlinearityareuniversalapproximatorsofany
sequence-to-sequencefunction. Weletm = 0,meaningweusenodetokenization. UsingtheuniversalityofSSMsfor
sequence-to-sequence function, the rest of the proof is the same as Kreuzer et al. (2021), where they use the padded
adjacencymatrixofGasapositionalencodingtoprovethesametheoremforTransformers. Infact,theuniversalityfor
sequence-to-sequencefunctionsisenoughtoshowtheuniversalityongraphswithastrongpositionalencoding.
TheoremF.3(ExpressivePowerw/PE). Giventhefullsetofeigenfunctionsandenoughparameters,GMNscandistinguish
anypairofnon-isomorphicgraphsandaremorepowerfulthananyWLtest.
Proof. DuetotheuniversalityofGMNsinTheoremF.3,onecanuseaGMNwiththepaddedadjacencymatrixofGas
positionalencodingandlearnafunctionthatisinvariantundernodeindexpermutationsandmapsnon-isomorphicgraphsto
differentvalues.
TheoremF.4(ExpressivePowerw/oPEandMPNN). Withenoughparameter,foreveryk â‰¥1therearegraphsthatare
distinguishablebyGMNs,butnotbyk-WLtest,showingthattheirexpressivitypowerisnotboundedbyanyWLtest.
Proof. TheproofofthistheoremdirectlycomesfromtherecentworkofToÂ¨nshoffetal.(2023),wheretheyproveasimilar
theoremforCRaWl(ToÂ¨nshoffetal.,2023). Thatis,usingRWFasÏ•(.)inGMNs(withoutMPNN),makestheGMNsas
powerfulasCRaWl. ThereasonisCRaWluses1-dCNNontopofRWF,whileGMNsuseBidirectionalMambablockon
topofRWF:usingabroadcastSMM,thisblockbecomessimilarto1-dCNN.
22