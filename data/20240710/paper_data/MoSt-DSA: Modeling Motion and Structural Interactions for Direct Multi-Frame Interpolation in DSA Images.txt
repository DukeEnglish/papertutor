MoSt-DSA: Modeling Motion and Structural Interactions
for Direct Multi-Frame Interpolation in DSA Images
ZiyangXua,HuangxuanZhaob,ZiweiCuia,WenyuLiua,ChuanshengZhengb andXinggangWanga;*
aInstituteofAI,SchoolofEIC,HuazhongUniversityofScienceandTechnology
bUnionHospital,TongjiMedicalCollege,HuazhongUniversityofScienceandTechnology
Abstract. Artificialintelligencehasbecomeacrucialtoolformed- Interpolating1( ), 2( ),3( ) frames
icalimageanalysis.Asanadvancedcerebralangiographytechnique,
DigitalSubtractionAngiography(DSA)posesachallengewherethe
MoSt-DSA-ğ“›ğŸ
FILM-â„’1
radiationdosetohumansisproportionaltotheimagecount.Byre-
Memory Usage
ducingimagesandusingAIinterpolationinstead,theradiationcan MoSt-DSA
be cut significantly. However, DSA images present more complex FILM-â„’ğ‘ ğ‘¡ğ‘¦ğ‘™ğ‘’ 2.20 G 3.58 G
motion and structural features than natural scenes, making inter-
polation more challenging. We propose MoSt-DSA, the first work
thatusesdeeplearningforDSAframeinterpolation.Unlikenatural
sceneVideoFrameInterpolation(VFI)methodsthatextractunclear SoftSplat-â„’ğ¹
orcoarse-grainedfeatures,wedeviseageneralmodulethatmodels
motionandstructuralcontextinteractionsbetweenframesinanef-
EMA-VFI
ficient full convolution manner by adjusting optimal context range
andtransformingcontextsintolinearfunctions.Benefitingfromthis,
MoSt-DSAisalsothefirstmethodthatdirectlyachievesanynum-
ber of interpolations at any time steps with just one forward pass
duringbothtrainingandtesting.Weconductextensivecomparisons
with 7 representative VFI models for interpolating 1 to 3 frames,
Figure1: SSIM-Time-Memory comparison of different methods
MoSt-DSA demonstrates robust results across 470 DSA image se-
for direct interpolating 1 to 3 frames on our DSA dataset. Our
quences(eachtypically152images),withaverageSSIMover0.93,
MoSt-DSA-L achieves94.62,94.35,93.58SSIM,0.024s,0.070s,
1
average PSNR over 38 (standard deviations of less than 0.030 and
0.117sinferencetime,and2.59G,2.61G,2.61Gmemoryusagefor
3.6,respectively),comprehensivelyachievingstate-of-the-artperfor-
interpolating1to3frames,respectively,outperformingSOTAEMA-
manceinaccuracy,speed,visualeffect,andmemoryusage.Ourcode
VFI[37]inallaspects.DetailsinTab.1,2,3.
isavailableathttps://github.com/ZyoungXu/MoSt-DSA.
capturing images from multiple angles [38]. 4D DSA adds a time
1 Introduction dimension,formingasequencethatcapturesdynamicbloodflowâ€™s
changesovertime[12].
Frame interpolation, a class of fundamental tasks in computer vi-
Moreover, frame interpolation for DSA images differs signifi-
sion,aimstodeduceintermediateframesfromgivenprecedingand
cantlyfromnaturalimages.AscomparingFig.2withFig.3,DSA
succeedingones[11,21].Thesetasksareclassifiedintosingle-frame
imagespresentmorecomplexstructuralandmotiondetails[9].Cur-
andmulti-frameinterpolationbasedonthenumberofframesinferred
rently,nospecificinterpolationsolutionsforDSAimagesexist.
[14,16,29].Traditionally,multi-frameinterpolationisachievedre-
Aswemovefrom2Dto4DDSA,frameinterpolationcomplexity
cursively.Forinstance,anintermediateframeI isinferredfirst,and
b
increases. Our research targets direct multi-frame interpolation for
thenusedwiththegroundtruthsofadjacentframestodeduceaddi-
4DDSA.Hereafter,DSArefersspecificallyto4DDSAunlessstated
tionalframesI andI [23,24,26].However,thisapproachneither
a c
otherwise.FrameinterpolationforDSAimagesconfrontschallenges
supportsdirectmulti-frameinterpolationnorallowsflexibledetermi-
fromcomplexstructuresandmotions.First,thevascularstructureis
nationofframecount(typicallyodd).
complex: vessels are irregular, dense, and varied in size, like Fig.
DSAisanadvancedmedicalimagingtechnologywidelyusedin
3(a).Second,theimagingcapturesthecontrastagentâ€™sdiffusion,a
interventionalsurgery[25].Itiscrucialfordiagnosingandtreating
non-rigidandcomplexmotiondepictedinFig.3(b).Third,vessels
variousvasculardiseases,includingbrain,heart,andlimbs.DSAop-
rotateduringimaging,causingocclusionsandoverlapsthatcompli-
erates by injecting a contrast agent, usually iodine-based, into the
catemotionanalysis,asshowninFig.3(c).
patientandcapturingvascularimageswithX-rays.DSAtechnology
Toaddresstheabovechallenges,extractingfine-grainedandpre-
varies: 2D DSA provides basic two-dimensional images. 3D DSA,
cise motion and structural features is critical. However, existing
âˆ—CorrespondingAuthor.Email:xgwang@hust.edu.cn frameinterpolationmethodsaretailoredfornaturalscenes,resulting
4202
luJ
9
]VC.sc[
1v87070.7042:viXraMotion Artifact Structural Dissipation Blurring
T
G
e
c
n
e
re
fn
I
Figure2:Variousmotionsinnaturalscenes.Motionsubjectshave
Figure4:Existingframeinterpolationmethodsaretailoredfornat-
simplestructureandcoarsetexturefeaturegranularity,alsoeasyto
ural scenes, and commonly exhibit issues such as motion artifacts,
predictthemotiontrajectory.
structuraldissipation,andblurringinDSAframeinterpolation.
Inthiswork,weproposeanetworkforflexible,efficient,anddi-
rect multi-frame interpolation in DSA images. Initially, we extract
multi-scale features from input frames and enhance them through
cross-scale fusion. Inspired by the EMA-VFI [37], we propose a
general module named MSFE that extracts motion and structural
featuresbetweenenhancedframesbycross-attention.UnlikeEMA-
VFI,MSFEdoesnâ€™trelyonexpensiveattentionmapsandcanflexibly
(a) Complex structure adjustcontext-awaregranularity.Specifically,byadjustingtheopti-
mal context range and transforming contexts into linear functions,
MSFEcalculatescross-attentionbetweeninputframesinafullycon-
volutionalmanner,whichreducesthestoragecostandincreasesthe
computingspeed.Afterextractinggeneralmotionandstructuralfea-
turesthroughMSFE,wemapthemotionfeaturesatdifferenttimest
anddecodethemtogetherwiththestructuralfeaturestoobtainflows
and masks. Finally, a simplified UNet [28] refines features at dif-
ferent scales, decoding the flows, masks, and structural features to
(b) Complex diffuse motion of contrast agents producethecorrespondingintermediateframeI t.Akeyadvantage
isthatbyextractinggeneralmotionandstructuralfeaturesonlyonce,
ourMoSt-DSAcaninterpolateanyintermediateframebycombining
differenttimetduringbothtrainingandtesting.Thisismoreflexible
thanmethodsinterpolatingforfixedt[24],moreefficientthanmeth-
odsextractingdifferentfeaturesformultiplet[16],andmoredirect
thanmethodsinterpolatingmultiframesrecursively[23,26].
Insummary,ourworkoffersthesemaincontributions:
(c) Complex rotational motion â€¢ To our knowledge, MoSt-DSA is the first work that uses deep
learning for DSA frame interpolation, and also the first method
Figure3:VariouschallengesforframeinterpolationinDSAimages.
that directly achieves any number of interpolations at any time
stepswithjustoneforwardpassduringbothtrainingandtesting.
inunclearorcoarseextractionofmotionandstructuralfeaturesfor â€¢ We propose a general module named MSFE that models mo-
DSAimages.Commonapproachesfallintothreecategories.Thefirst tionandstructuralcontextinteractionsbetweenframesbycross-
usesasinglemoduletomixandextractbothmotionandstructural attention. Significantly, by adjusting the optimal context range
features,resultinginambiguityinbothaspects[1,5,16,17,21].The andtransformingcontextsintolinearfunctions,MSFEcalculates
second designs multiple modules to sequentially extract structural cross-attentioninafullyconvolutionalmanner,whichreducesthe
featuresofeachframeandmotionfeaturesbetweenframes,although storagecostandincreasesthecomputingspeed.
clearmotionfeaturesareobtained,thecorrespondingstructuralrela- â€¢ We conduct extensive comparisons with 7 representative VFI
tionshipsbetweenframesarelacking[4,6,13,22,24,26,32,35,36,39]. modelsforinterpolating1to3frames,MoSt-DSAdemonstrates
The third designs a single module to extract relative motion and robust results across 470 DSA image sequences (each typically
structural features from frames simultaneously, but due to coarse 152 images), with average SSIM over 0.93, average PSNR over
context granularity, it fails to adapt to the fine-grained, complex 38(standarddeviationsoflessthan0.030and3.6,respectively),
structures of DSA images [37]. These methods commonly exhibit comprehensivelyachievingstate-of-the-artperformanceinaccu-
issuessuchasmotionartifacts,structuraldissipation,andblurringin racy,speed,visualeffect,andmemoryusage.Ifappliedclinically,
DSAframeinterpolation,asshowninFig.4. MoSt-DSA can significantly reduce the DSA radiation dose re-ceivedbydoctorsandpatients,loweringitby50%,67%,and75% Enhanced Structural
wheninterpolating1to3frames,respectively. Features of ğ¼0 ğœ†ğ¶,ğœ†ğ‘ƒ
Cross-Scale ğ‘ v n
2 RelatedWork Fusion Lambda Layer o C Relative
2 In. t1 erpoD lati ir oe nc tt aM sksu fl oti r- cF or na tim nue ouIn st ie mr ap go el sa et qi uo en
nces,knownasVideo
Fe Ca rt
o
Fu
s
ur
s
se
-
iSs
o
c
no af
l e
ğ¼0
ğ‘˜,ğ‘£
Lambda Layer
n o
itce
jo rP
&
se
R &
v n
F
b
ğ¼M
0e ea
t
ao
wt
nt ui deo
r
e
en
ğ¼n
1s
FrameInterpolation(VFI),aimtogenerateoneormultipleinterme- Features of ğ¼1 o C (ğ‘€0,ğ‘€1)
diateframesbetweeninputframes[14,16].ConsideringthatinDSA Enhanced Structural ğœ†ğ¶,ğœ†ğ‘ƒ
imaging, where the radiation dose correlates with image count, re- Features of ğ¼1
ducing frames and using AI interpolation instead can cut radiation Figure5:AnillustrationoftheMoStAttentionintheMSFEmod-
significantly. Further, if multi-frame interpolation could be rapidly uleforcalculatingmotionandstructuralfeatures.Theenhanced
achievedwithjustoneforwardpass,itwouldnotonlyfurtherreduce structuralfeaturesofI andI areinvolvedinsubsequentcalcula-
0 1
radiation dose but also shorten the time consumed, securing more tions in the MSFE module to generate the final structural features,
precioustimeforpatienttreatment.However,advancedVFImethods seeFig.7fordetails.
primarilyfocusedonsingle-frameinterpolation,withmulti-framein-
terpolationoftenreliantonrecursion[23,24,26,37].Duringtraining,
these methods are limited as they neither directly complete multi-
frameinterpolationnorallowflexibleframenumberdetermination,
leading to a significant decrease in accuracy for direct multi-frame
interpolation during testing. Unlike these methods, our MoSt-DSA
candirectlyachieveanynumberofframeinterpolationsatanytime
stepswithjustoneforwardpassduringbothtrainingandtesting.
2.2 ModelingMotionandStructuralInteractions
Modeling the motion and structural interactions is essential for
extracting motion and structural features. Existing frame interpo-
lation methods are tailored for natural scenes, and the modeling = * = * = *
of motion and structural interactions could divided into three cat-
egories. The first category concatenates input frames to a back- Features = Lambda * Query
bone network that extracts mixed features of motion and structure
Figure6:AnillustrationofhowweuseLambdaLayertocalculate
[1,5,16,17,21]. While straightforward to implement, these meth-
featuresinourMoStAttention.LambdaLayersummarizescontex-
ods lack clear motion information, leading to restrictions in inter-
tualinformation(withinascoper)intoafixed-sizelinearfunction
polating frames with various numbers and time steps [8,11,31].
(i.e.amatrix)appliedtothecorrespondingquery,thusbypassingthe
The second category utilizes multiple modules to sequentially ex-
needformemory-intensiveattentionmaps.
tract the structural features of each frame and the motion features
between frames [4,6,13,22,24,26,32,35,36,39]. Although these
3 Method
methodsprovideexplicitmotionfeatures,theyrequiremoduleswith
high computational costs, such as cost volume [13,24,26]. More- Presentingagroundbreakingapproachtodirectmulti-frameinterpo-
over, capturing structural features from individual frames does not lation in DSA images, Fig. 7 delineates the overall network archi-
adequatelyidentifythestructuralcorrespondencebetweenframes,a tecture of our method. Briefly, it is divided into five key modules.
criticalaspectnotedby[13]forVFItasks.Thethirdcategory,repre- Initially,itemploystheMulti-ScaleFeatureExtractor(FE),Cross-
sentedby[37],utilizesasinglemoduleforconcurrentextractionof ScaleFeatureFusion(CSFF),andMotion-StructureFeatureExtrac-
relativemotionandstructuralfeaturesfromframes.Thisapproachâ€™s tor(MSFE)toextractgeneralmotionandstructuralfeatures.Subse-
advantagesincludepreservingandenhancingthedetailedstructural quently, theFlow-Mask Estimator (FME) andRefiner decode and
featuresofinputframeswithoutinterferencefrommotionfeatures, refinethesefeaturesfordifferentmomentsttogenerateframeI .
t
mapping motion features to any moment for arbitrary intermediate
frame generation, and significantly lowering training costs. How-
3.1 ExtractingGeneralMotionandStructural
ever, due to coarse context granularity, it fails to adapt to the fine-
Features
grained, complex structures of DSA images. These methods com-
monlyexhibitissuessuchasmotionartifacts,structuraldissipation, Multi-ScaleFeatureExtractor(FE).Toexcavatefoundationalfea-
and blurring in DSA frame interpolation, as shown in Fig. 4. Our turesofbloodvesselsofvarioussizesbeforeextractingmotionand
method,aligningwiththethirdcategory,introducesageneralmod- structural features, we first employ the FE to derive three different
ulenamedMSFEthatmodelsmotionandstructuralcontextinterac- scalesofneurovascularfeatures.ForinputframesI andI ,weini-
0 1
tionsbetweenframesbycross-attention.Differingfrom[37],MSFE tially compute the third layer of low-level features L0 and L0, re-
0 1
doesnâ€™t rely on expensive attention maps and can flexibly adjust spectively,using3x3convolutionsfollowedbyPReLU[10].Subse-
context-aware granularity. By adjusting the optimal context range quently,throughdownsamplingandthesameconvolutionandactiva-
andtransformingavailablecontextsintolinearfunctions,MSFEcal- tionconfiguration,wecalculatethesecondlayeroflow-levelfeatures
culatescross-attentioninafullyconvolutionalmanner,whichfurther L1andL1,aswellasthefirstlayeroflow-levelfeaturesL2andL2
0 1 0 1
reducesthestoragecostandincreasesthecomputingspeed. forI andI respectively.Mathematically,
0 1Motion-Structure
ğ¼0 Ã— Multiplication C Concat Ï„ Warp + Add Feature Extractor
ğ¼1 ğ‘‹à·ª ğ‘§ğ‘¦is formed by ğ‘‹ğ‘§ğ‘¦being warped by flow. Structure Feature for ğ¼0and ğ¼1
Motion Feature for ğ¼0and ğ¼1
H * W ğ¿0 0 v n o
1
41 2 HH **
1
41 2 WW ğ¿ ğ¿1 20 0ğ¿ğ¿ 10 11 su
it
o e lur lt aAC
Sc -
1 8H * 1 8W n e tta lF &
ra e n iL
m ro N re
y a L
ğ‘¥âˆ’ğ‘¥
ğ‘“ğ‘™ğ‘–ğ‘
n o itn tSe ott MA + m ro N re
y a L
P L M +
M
Fea Ft eu ar te us
r
efo
s
r
f
oğ¼0
r ğ¼1
ğ¿2
1 Fus Fi uo sn
i
ofe na ft eu ar te us
re
f so fr
o
ğ¼ r0
ğ¼1
ğ‘€0,ğ‘€M 1otion & Structure features
ğ‘†0,ğ‘†1
Multi-Scale Feature Extractor Cross-Scale Feature Fusion
ğ‘¡ vac la un
e
b be
e
ta wny
ee n
ğ‘¡ Ã— ğ¼0, ğ¼1
[0,1], and
2x (Conv33 + PReLU) M.S.A.C. For ğ‘–-th feature, using 2ğ‘–âˆ’1atrous multiple ğ‘¡can be C C
convs(same k but different s/p/d) specified at one
DownSample+ 2x (Conv33 + PReLU) L. & F. Using1x1 conv, then flatten to 1D time.
PixelShuffle DownSample
+ Conv C ğ¼0, ğ¼1 Concat& Conv Sequence
ğ¼t
Refiner (UNet)
C C ğ¿ ğ¿à·ª à·ª10 0 0, ,ğ¿ ğ¿à·ª à·ª10 1
1
ta c n o
C
Ï„ MFl ao sw
k UpSample
D 3(o sw =n 2S )a +m Pp Rl ee L, UCo +n Cv o3 C C ğ¿à·ª2 0,ğ¿à·ª2 1 Ã— Flow and mask for calculating ğ¼t
nv33(s=1)+PReLU C C ğ‘†à·ª 0,ğ‘†à·© 1
Flow-Mask Estimator
U 4(p sS =a 2m )p +le P,
R
d ee LC Uonv4 C ğ‘†à·ª 0â€²,ğ‘†à·© 1â€²
Conv Sequence 3x Conv33(s=1)
Figure7:OverallnetworkarchitectureofourMoSt-DSA.First,theMulti-ScaleFeatureExtractor(FE)processestheinputframesI 0andI
1
toobtainfeaturesatthreedifferentscalesthroughcontinuousconvolutionanddown-sampling.Next,Cross-ScaleFeatureFusion(CSFF)uses
multi-scaleatrousconvolutions[3]togeneratefusedfeaturesforI andI ,whicharelinearlymappedandnormalized.TheMotion-Structure
0 1
FeatureExtractor(MSFE)thencalculatesmotionandstructuralfeaturesfromthesefusedfeatures.Subsequently,fortheintermediatetimet,
motionfeaturesaremapped.Thesemappedmotionfeatures,alongwithstructuralfeaturesandtheoriginalframes,feedintotheFlow-Mask
Estimator (FME) to predict flow and mask. Finally, the Refiner combines the various scale features from FE and structural features from
MSFE,alongwithflowandmask,refiningthemintotheimageofintermediatetimet.
ï£± L0 =H(I )
ï£² Lj 1
j
=D(cid:0) Lj 0 j(cid:1) , (1) F j =T (cid:2) C(cid:0) F (cid:0) L0 j(cid:1) ,F (cid:0) L1 j(cid:1) ,F (cid:0) L2 j(cid:1)(cid:1)(cid:3) , (3)
ï£³ L2 =D(cid:0) L1(cid:1)
j j
where T represents the linear mapping, with C indicates the con-
where H is a stack of convolution and activation functions, while
catenation operation. Finally, we flatten F and then normalize it,
j
DrepresentsanintegrationofH withanadditionaldownsampling
preparingforsubsequentprocessingbytheMSFE.
operation,andjis0or1.
Motion-Structure Feature Extractor (MSFE). We propose
Cross-ScaleFeatureFusion(CSFF).Tofuseneurovascularfea-
MoStAttentiontocalculaterelativemotionfeatureswhileenhancing
tures of different scales and enhance the representation of founda-
structuralfeaturesbetweenframes,asshowninFig.5and6indetail.
tional features, we further employ the CSFF for I and I to per-
0 1 Tofacilitateandsimplifyunderstanding,thefollowingformulaswe
formcross-scalefeaturefusion.Specifically,forthei-thlayerlow-
giveisbasedonouractualcodeimplementation.Wefirstconcatenate
levelfeaturesLi 0andLi 1,weuse2iâˆ’1atrousconvolutions[3](with
F andF toobtainF âˆˆ R|n|Ã—d,andthenacquireFâ€² âˆˆ R|n|Ã—d
afixedkernelsizeof3,strideof2i,andforthen-thatrousconvolu- 0 1 a a
throughreverseconcatenation,as:
tion,bothpaddinganddilationsizearen).Mathematically,
F (cid:16) Li j(cid:17) =(cid:16) A 1(cid:16) Li j(cid:17) ,...,A n(cid:16) Li j(cid:17)(cid:17) , (2) (cid:26) F Fa
â€²
= =C C( (F F0, ,F F1)
)
. (4)
a 1 0
whereF signifiesfeaturefusion,Aindicatesatrousconvolution.The
variablen,representingthenumberofatrousconvolutions,takesa Furthermore,weemployaLambdaLayer[2]tosimulatecontent-
value of 2iâˆ’1 for i equal to 0, 1, or 2. Moreover, by merging the basedandposition-basedcontextualinteractionsinafullyconvolu-
fusedfeaturesfromvariousscalesandimplementingalinearmap- tionalmanner.Specifically,wedenotethedepthofqueryandvalue
ping,weobtainthecross-scalefusedfeaturesF andF forI and as|k|and|v|,respectively,anddenotethepositioninformationwith
0 1 0
I respectively,as: P âˆˆR|n|Ã—d.Thequeries,keys,andvaluesarecalculatedasfollows:
13.3 LossFunctions
ï£± Q=F W âˆˆR|n|Ã—|k|
ï£² a Q
K =Fâ€²W âˆˆR|n|Ã—|k| . (5) Tofurtherenhancetheinferencequality,weemployedacombination
a K
ï£³ V =Fâ€²W âˆˆR|n|Ã—|v| ofthreetypesoflossfunctions,asfollows:
a V
ThenwerepresentrelativepositionembeddingsasE âˆˆR|n|Ã—|k|. L=w 1L 1+w VGGL VGG+w StyleL Style, (11)
Bynormalizingthekeys,weobtainKÂ¯ = softmax(K,axis= n).
Next,wecomputethecontent-basedcontextualinteractionsÎ» and whereL 1 denotestheL1reconstructionloss,whichminimizesthe
c
position-basedcontextualinteractionsÎ» ,as: pixel-wise RGB difference. Additionally, L VGG employs the L1
p
normoftheVGG-19featurestoenhancefinerimagedetailsandtex-
ture quality [33]. The style loss L utilizes the L2 norm of the
Style
(cid:26) Î» =KÂ¯TV âˆˆR|k|Ã—|v| auto-correlation of the VGG-19 features [7,18,27]. This approach
c . (6)
Î»
p
=ETV âˆˆR|k|Ã—|v| aimstofurtherleveragethebenefitsofL VGGbycapturingandrepli-
catingstylepatternsandtexturesmoreeffectively.Regardingthese-
Finally,byapplyingcontextualinteractionstothequeriesaswell lectionoftheweights(w ,w ,w ),wereferenced[26].
1 VGG Style
asP,weobtainthegeneralmotionandstructuralfeaturesnecessary
forinferringanyintermediateframe,as:
4 Experiments
(cid:26)
S =QÎ» c+QÎ» p =C(S 0,S 1) . (7) 4.1 Datasets
M =PÎ» +PÎ» =C(M ,M )
c p 0 1
Wecollected470headDSAimagesequencesfrom8hospitals,each
3.2 DecodingandRefiningforMultiIntermediate fromadifferentpatient,typicallycontaining152imagesof489x489
Frames resolution.Theseweresplitinto329fortrainingand141fortesting,
maintaininga7:3ratio.Foreachsequencetargetingn-frameinter-
To further obtain motion features corresponding to multiple differ- polation, we arrange it into several groups, each with consecutive
entintermediatetimest,wemultiplyeachtwiththegeneralmotion n+2frames.Adjacentgroupsstartoneframeapart.Fordetailsre-
featurestomapandobtainM andM . gardingdataacquisition,weuseNeuAngio33C,NeuAngio43C,and
0â†’t 1â†’t
Flow-Mask Estimator (FME). For a specific t, M and NeuAngio-CTequipment,followingtheSpinDSAprotocol.
0â†’t
M areconcatenatedwithS andS ,respectively,andthiscom-
1â†’t 0 1
binationservesaspartaoftheinputfortheFME,whilepartbisthe
4.2 ImplementationDetails
concatenationofI andI .AsshowninFig.7,FME(denotebyF)
0 1
appliesPixelShuffle[30]upsamplingtoparta,anddownsamplingto
part b. Subsequently, parts a and b merge and undergo continuous ModelConfiguration.Forinterpolating1to3frames,time(t)se-
convolutionoperations,eventuallyleadingtothegenerationofbidi- quences are set to [0.5], [0.33, 0.67], and [0.25, 0.50, 0.75], re-
rectionalopticalflowÏ• andmaskÂµ correspondingtothespecific spectively.Forsimulatingcontextualinteractions,contextmodeling
t t
tthroughupsampling,as: scope(r)sizesare29,29,and21.Effectsofvaryingrarecompared
intheablationstudy.
TrainingDetails.Wetrainedon4A100GPUs,andfortasksin-
Ï• ,Âµ =F(C(M ,M ,S ,S ),C(I ,I )). (8) terpolating 1 to 3 frames, we set the batch sizes to 10, 10, and 6,
t t 0â†’t 1â†’t 0 1 0 1
withwarm-upstepssetto9000,12000,and16000,respectively.We
Next,weinitiallyemployÏ• towarpI ,I ,aswellasthelow- usetheAdamW[20]optimizerwithÎ² = 0.9,Î² = 0.999,anda
t 0 1 1 2
levelfeaturesLi fromdifferentlayersextractedbyFE,andthegen- weightdecayof1eâˆ’4.Thelearningrateiswarmedupto2eâˆ’4and
j
eralstructuralfeaturesS andS extractedbyMFSE.Forinstance, thendecaysfollowingacosineschedule[19],decreasingto2eâˆ’5
0 1
forX zy,theresultafterwarpingisdenotedasX(cid:103)zy.Subsequently,we over300epochs.Wecropeachframetoaresolutionof320Ã—320
concatenateI 0,I 1,I(cid:101)0,I(cid:101)1,Ï• t,andÂµ ttogether,referredtoasO t. andapplyrandomflipandrotationforaugmentation.Regardingthe
Refiner.Finally,throughtheRefiner(asimplifiedUNet[28]),by selectionoflossweights(w ,w ,w ),wereferenced[26],
1 VGG Style
integratingandrefiningfeaturesofdifferentscalesintoO layerby assigningweightsof(1.0,1.0,0.0)forthefirstepochandweightsof
t
layer,andthenutilizingskipconnection,weobtaintheintermediate (1.0,0.25,40.0)forthesubsequentepochs.
frameI(cid:98)tcorrespondingtot,asdepictedinFig.7.Mathematically, TestingDetails.Tohighlightourmethodâ€™sadvantages,wecom-
paredMoSt-DSAwithrepresentativeVFImethods.ForABME[24]
andSoftSplat[23],wetestedonreleasedpre-trainedweightsdueto
I(cid:98)t =I(cid:101)t+R(O t,L,S), (9) theabsenceoftrainingcodes.ForEMA-VFI(state-of-the-art)[37]
and FILM [26], we retrained them on our dataset following their
whereRsignifiestheRefiner,LdenotesthecollectionofLi j,andS originalsetups.AlltestswereperformedonasingleRTX3090GPU.
referstothecollectionofS 0 andS 1.ThesymbolâŠ™representsthe ComparisonDetails.Wetrainedtwoversions:one(MoSt-DSA-
Hadamardproduct,andI(cid:101)tisdeterminedasfollows: L 1) using only the L
1
loss, which achieves higher test scores; the
other(MoSt-DSA)usingourproposedcombinedlossL,whichben-
I(cid:101)t =Âµ tâŠ™backwarp(I 0,Ï• tâ†’0) efits image quality (see supplementary materials for proof). When
(10)
+(1âˆ’Âµ )âŠ™backwarp(I ,Ï• ). comparingvisualeffects,weusetheversionofthemodelthatyields
t 1 tâ†’1
highimagequality[7,26],i.e.,FILM-L ,andSoftSplat-L .
Style FBlend (mixing two frames by overlapping) Our EMA-VFIâ˜… FILM -â„’ ABME SoftSplat -â„’
ğ‘†ğ‘¡ğ‘¦ğ‘™ğ‘’ ğ¹
e
c
n
e
re
fn
I
m ğµ2 ğ‘†7 ğ‘†11 ğ‘†8 ğ‘†12 ğ‘†9 ğµ3 ğ‘†10
o
o
Z
.fn
I
ğ‘†1 ğ‘€1
ğ‘†5
ğ‘†2 ğ‘€2
ğ‘†6
ğ‘†3 ğ‘€3
ğµ1
ğ‘†4 ğ‘€4
ğ‘€5
0
1
Ã—
la
u
d
is
e
R
m
o o ğµ2 ğ‘†7 ğ‘†11 ğ‘†8 ğ‘†12 ğ‘†9 ğµ3 ğ‘†10
Z
0
1
ğ‘†1
ğ‘€1
ğ‘†2
ğ‘€2
ğ‘†3
ğ‘€3
ğ‘†4
ğ‘€4
Ã—
.R
ğ‘†5 ğ‘†6 ğµ1 ğ‘€5
Figure8:VisualcomparisonforinterpolatingoneframewithmethodsinVFI."â‹†"indicatesSOTA.Ontherightside,thefirstandthird
rowscorrespondtothegreenboxinblend,andthesecondandfourthrowscorrespondtotheblueboxinblend.M standsformotionartifact,
Sforstructuraldissipation,andBforblurring.Comparingtheresultsofvariousmethods,itcanbeprovedthatthenaturalsceneVFImethod
hasmanyproblemsofmotionartifact,structuraldissipationandblurring,whileMoSt-DSArelativelyobviouslyalleviatestheseproblems.
Table 1: Quantitative comparison with VFI methods on single- Table 3: Quantitative comparison with VFI methods on three
frame interpolation. Best scores for color losses in blue, and for frames interpolation. The meanings of blue, red, green, and "â‹†"
perceptually-sensitivelossesinred.Thesecondlowestmemoryus- arethesameasthoseinTable1.EMAisshortforEMA-VFI.
ageingreen."â‹†"indicatesSOTA.EMAisshortforEMA-VFI.
SSIM(%) PSNR Time(s) Memory
SSIM(%) PSNR Time(s) Memory Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 3frameâ†“ 3frameâ†“
Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 1frameâ†“ 1frameâ†“ FILM-L1[26] 91.94 3.52 37.21 3.91 0.548 3.58G
EMA-small[37] 90.48 4.52 36.31 4.24 0.122 2.10G
ABME[24] 94.02 2.28 39.83 3.39 0.383 2.66G
EMAâ‹†[37] 90.57 4.50 36.35 4.24 0.165 2.64G
EF MIL AM -s- mL a1 ll[2 [36 7]
]
99 44 .. 11 91 2 2. .3 27
1
3 49 0. .8 06
7
33 .. 44 73 0 0. .2 00 21
7
23 .. 15 01 GG MoSt-DSA-L1 93.58 2.69 38.85 3.56 0.117 2.61G
EMAâ‹†[37] 94.33 2.13 40.13 3.40 0.056 2.63G SoftSplat-LF[23] 90.07 3.87 37.24 3.53 0.137 2.20G
MoSt-DSA-L1 94.62 2.12 40.32 3.35 0.024 2.59G FILM-LVGG[26] 90.63 3.83 36.75 3.74 0.548 3.58G
SoftSplat-LF[23] 91.78 3.07 38.59 3.51 0.035 2.20G FILM M- oL SS t-t Dy Sle A[26] 9 90 3. .5 04
3
3 2. .9 90
4
33 86 .. 66 66 3 3. .8 52
9
00 .. 15 14 78 23 .. 65 18 GG
FILM-LVGG[26] 93.10 2.67 39.27 3.45 0.201 3.51G
FILM-LStyle[26] 93.05 2.72 39.25 3.48 0.201 3.51G
MoSt-DSA 93.65 2.61 39.55 3.45 0.024 2.59G
SSIM-Mean PSNR-Mean
Table 2: Quantitative comparison with VFI methods on two
frames interpolation. The meanings of blue, red, green, and "â‹†"
Time (s) Memory (G)
arethesameasthoseinTable1.EMAisshortforEMA-VFI.
SSIM(%) PSNR Time(s) Memory
SSIM-STD PSNR-STD
Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 2frameâ†“ 2frameâ†“
FILM-L1[26] 92.62 3.13 37.93 3.82 0.388 3.58G
EMA-small[37] 91.82 3.68 37.37 4.07 0.074 2.10G
EMAâ‹†[37] 91.90 3.63 37.41 4.08 0.112 2.64G
MoSt-DSA-L1 94.35 2.29 39.78 3.44 0.070 2.61G
SoftSplat-LF[23] 90.86 3.49 37.84 3.43 0.084 2.20G
FILM-LVGG[26] 91.42 3.42 37.47 3.67 0.388 3.58G Figure9:Intuitivecomparisonofmetricsforeachmethodinter-
FILM-LStyle[26] 91.31 3.50 37.39 3.74 0.388 3.58G
polating1to3frames.OurMoSt-DSA-L leadsSOTAEMA-VFI
MoSt-DSA 93.14 2.84 38.94 3.38 0.070 2.61G 1
inallrespects,whileshowingsuperiorrobustnesswithalowerSTD.
4.3 Single-FrameInterpolation
tion,andB forblurring.Bycomparingtheresultsofvariousmeth-
We visualized the single-frame interpolation results of each model ods, it can be proved that the natural scene VFI method has many
andcomparedthemwiththegroundtruthbycalculatingresiduals. problemsofmotionartifact,structuraldissipationandblurring,while
AsshowninFig.8,thefirstandthirdrowscorrespondtothegreen MoSt-DSArelativelyobviouslyalleviatestheseproblems.
boxinblend,andthesecondandfourthrowscorrespondtotheblue The quantitative comparison for single-frame interpolation, as
boxinblend.M standsformotionartifact,S forstructuraldissipa- showninTab.1,demonstratesourMoSt-DSAâ€™ssuperiorityinSSIM,0.94 40 0.94 40 0.94 40
SSIM SSIM (Average of two frames) SSIM (Average of three frames)
PSNR 0.935 PSNR (Average of two frames) 39.5 0.935 PSNR (Average of three frames) 39.5
0.935 39.5 0.93 39
0.93 39 0.925 38.5
0.93 39 0.925 38.5 0.92 38
0.925 38.5 0.92 38 0.915 37.5
0.91 37
0.915 37.5
0.92 38 0.905 36.5
0.91 37
0.9 36
0.915 37.5 0.905 36.5 0.895 35.5
3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31
r (scope size of context modeling) r (scope size of context modeling) r (scope size of context modeling)
0.05 5 0.05 5 0.05 5
SSIM (STD) SSIM (STD, Average of two frames) SSIM (STD, Average of three frames)
0.045 PSNR (STD) 4.5 0.045 PSNR (STD, Average of two frames) 4.5 0.045 PSNR (STD, Average of three frames) 4.5
0.04 4 0.04 4 0.04 4
0.035 3.5 0.035 3.5 0.035 3.5
0.03 3 0.03 3 0.03 3
0.025 2.5 0.025 2.5 0.025 2.5
0.02 2 0.02 2 0.02 2
0.015 1.5 0.015 1.5 0.015 1.5
0.01 1 0.01 1 0.01 1
0.005 0.5 0.005 0.5 0.005 0.5
0 0 0 0 0 0
3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31
r (scope size of context modeling) r (scope size of context modeling) r (scope size of context modeling)
Figure10:Impactofcontextmodelingscope(r)sizesforDSAframeinterpolationtasks:from1to3frames.Thefirsttothirdcolumns
correspondtointerpolating1to3frames,andthefirsttosecondrowsrepresentthemeanandSTD,respectively.ThestableSTDprovesthe
robustnessofMSFE,andtheMeanindicatesthatthebestrforinterpolatingframes1to3is29,29,and21,respectively.
PSNR, and inference time over all competitors. Furthermore, our multi-framesupervisiontomodelmotionandstructuralinteractions
modelalsoboastsmoreefficientmemoryusagethanEMA-VFIdur- accurately,andhighlightstheimportanceofmulti-framesupervision
inginference.Notably,thescoredifferencesamongSOTAmethods trainingfordirectmulti-frameinterpolationtasks.
intheVFIdomainareminimal.Forinstance,ontheUCF101dataset
[34], the top-performing EMA-VFI surpasses the second-best [15]
4.5 3DReconstructionShowcasefromSingleFrame
byonly0.01%inSSIMand0.01inPSNR,andthethird-best[39]
Interpolation
by0.04%inSSIMand0.01inPSNR.Thus,itisasignificantmar-
ginthatourMoSt-DSA-L â€™sleadovertheEMA-VFI,by0.29%in Weconducted3Dreconstructionsusingboththesingle-frameinter-
1
SSIMand0.19inPSNR,asshowninTab.1. polatedsequences(interpolatingeveryotherimage)andtheoriginal
DSAsequences.Ourresultsarevirtuallyindistinguishabletothere-
constructionfromoriginaldata.Detailsinsupplementarymaterials.
4.4 DirectMulti-FrameInterpolation
We further compared our method with representative VFI methods
4.6 AblationStudy
intasksofdirectinterpolating2and3frames.
For each method, we set t=[0.33, 0.67] and t=[0.25, 0.50, 0.75] Impactofcontextmodelingscope(r).Fig.10demonstratesthatthe
forinterpolating2to3frames,respectively.ConsideringthatABME impactofronSTDisminimal,highlightingMSFEâ€™srobustness.râ€™s
[24]couldnâ€™tinterpolateatarbitrarytimesteps,weexcludeditfrom influence is slightly more pronounced on Mean-of-SSIM (no more
thecomparison.Wegivetheaveragevaluesacrossmetricsforeach than1.8%)thanonMean-of-PSNR(nomorethan1.1).Moredetailed
framecount.Forinstance,ifinterpolating2framesresultsinSSIM numericalresultsareavailableinthesupplementarymaterials.
valuesof[0.8,0.9],thentheaverageis0.85. LossfunctioncomparisononourMoSt-DSA.Weprovethatour
Thequantitativeevaluationresultsforinterpolating2and3frames proposed loss function significantly improves image quality, in the
arepresentedinTab.2and3.OurMoSt-DSAcontinuestooutper- supplementarymaterials.
formothermethods,intermsofSSIM,PSNR,andinferencetime,
alsoexhibitingalowerstandarddeviation(STD).Memoryusagedur-
5 Conclusion
ing inference also remains more efficient than the EMA-VFI. This
conclusivelydemonstratesthesuperiorrobustnessofourmethod. We have proposed MoSt-DSA, the first work that uses deep learn-
Wefurtherintuitivelycomparedthemetricsforinterpolating1to ing for DSA frame interpolation, to reduce radiation dose in DSA
3 frames, Fig. 9 shows a clear pattern: MoSt-DSAâ€™s superiority in imagingsignificantly.Inparticular,wedevisedageneralmodulethat
SSIM, PSNR, and stability grows with the increase in interpolated models motion and structural context interactions between frames
frames. Compared to EMA-VFI, MoSt-DSA-L â€™s SSIM is higher in a fully convolutional manner, by adjusting the optimal context
1
by0.29%,2.45%,and3.01%forinterpolating1,2,and3frames. rangeandtransformingavailablecontextsintolinearfunctions.Ex-
IntermsofPSNR,theincreaseis0.19,2.37,and2.50.Furthermore, perimentresultsshowthatourMoSt-DSAoutperformsthestate-of-
MoSt-DSA-L â€™s STD for SSIM is lower by 0.4%, 37%, and 40%, the-artVideoFrameInterpolationmethodsinaccuracy,speed,visual
1
and for PSNR, it is 1.49%, 16%, and 16% lower. We believe this effect, and memory usage for interpolating 1 to 3 frames, and can
significant lead reflects the advantages of MoSt-DSA trained with alsoassistphysiciansin3Ddiagnosisandtreatment.
MISS
fo naeM
MISS
fo
DTS
RNSP
fo naeM
RNSP
fo
DTS
MISS
fo naeM
MISS
fo
DTS
RNSP
fo naeM
RNSP
fo
DTS
MISS
fo naeM
MISS
fo
DTS
RNSP
fo naeM
RNSP
fo
DTSReferences [21] LiyingLu,RuizhengWu,HuaijiaLin,JiangboLu,andJiayaJia,â€˜Video
frameinterpolationwithtransformerâ€™,inProceedingsoftheIEEE/CVF
[1] WenboBao,Wei-ShengLai,ChaoMa,XiaoyunZhang,ZhiyongGao, Conference on Computer Vision and Pattern Recognition, pp. 3532â€“
and Ming-Hsuan Yang, â€˜Depth-aware video frame interpolationâ€™, in 3542,(2022).
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPat- [22] SimonNiklausandFengLiu,â€˜Context-awaresynthesisforvideoframe
ternRecognition,pp.3703â€“3712,(2019). interpolationâ€™,inProceedingsoftheIEEEconferenceoncomputervi-
[2] Irwan Bello, â€˜Lambdanetworks: Modeling long-range interactions sionandpatternrecognition,pp.1701â€“1710,(2018).
withoutattentionâ€™,arXivpreprintarXiv:2102.08602,(2021). [23] SimonNiklausandFengLiu,â€˜Softmaxsplattingforvideoframeinter-
[3] Liang-ChiehChen,GeorgePapandreou,IasonasKokkinos,KevinMur- polationâ€™,inProceedingsoftheIEEE/CVFConferenceonComputer
phy,andAlanLYuille,â€˜Deeplab:Semanticimagesegmentationwith VisionandPatternRecognition,pp.5437â€“5446,(2020).
deepconvolutionalnets,atrousconvolution,andfullyconnectedcrfsâ€™, [24] Junheum Park, Chul Lee, andChang-Su Kim, â€˜Asymmetric bilateral
IEEEtransactionsonpatternanalysisandmachineintelligence,40(4), motionestimationforvideoframeinterpolationâ€™,inProceedingsofthe
834â€“848,(2017). IEEE/CVFInternationalConferenceonComputerVision,pp.14539â€“
[4] Duolikun Danier, Fan Zhang, and David Bull, â€˜St-mfnet: A spatio- 14548,(2021).
temporalmulti-flownetworkforframeinterpolationâ€™,inProceedings [25] AlessandroPosa,AlessandroTanzilli,PierluigiBarbieri,LorenzoSteri,
oftheIEEE/CVFConferenceonComputerVisionandPatternRecog- FrancescoArbia,GiuliaMazza,ValentinaLongo,andRobertoIezzi,
nition,pp.3521â€“3531,(2022). â€˜Digitalsubtractionangiography(dsa)technicalanddiagnosticaspects
[5] Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov, â€˜Cdfi: inthestudyoflowerlimbarteriesâ€™,Radiation,(2022).
Compression-drivennetworkdesignforframeinterpolationâ€™,inPro- [26] FitsumReda,JanneKontkanen,EricTabellion,DeqingSun,Caroline
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern Pantofaru,andBrianCurless,â€˜Film:Frameinterpolationforlargemo-
Recognition,pp.8001â€“8011,(2021). tionâ€™,arXivpreprintarXiv:2202.04901,(2022).
[6] PanGao,HaoyueTian,andJieQin,â€˜Videoframeinterpolationwith [27] FitsumAReda,GuilinLiu,KevinJShih,RobertKirby,JonBarker,
flowtransformerâ€™,arXivpreprintarXiv:2307.16144,(2023). DavidTarjan,AndrewTao,andBryanCatanzaro,â€˜Sdc-net:Videopre-
[7] LeonAGatys,AlexanderSEcker,andMatthiasBethge,â€˜Imagestyle diction using spatially-displaced convolutionâ€™, in Proceedings of the
transfer using convolutional neural networksâ€™, in Proceedings of the Europeanconferenceoncomputervision(ECCV),pp.718â€“733,(2018).
IEEE conference on computer vision and pattern recognition, pp. [28] OlafRonneberger,PhilippFischer,andThomasBrox,â€˜U-net:Convo-
2414â€“2423,(2016). lutionalnetworksforbiomedicalimagesegmentationâ€™,inInternational
[8] ShuruiGui,ChaoyueWang,QihuaChen,andDachengTao,â€˜Feature- ConferenceonMedicalimagecomputingandcomputer-assistedinter-
flow:Robustvideointerpolationviastructure-to-texturegenerationâ€™,in vention,pp.234â€“241.Springer,(2015).
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPat- [29] Wei Shang, Dongwei Ren, Yi Yang, Hongzhi Zhang, Kede Ma, and
ternRecognition,pp.14004â€“14013,(2020). Wangmeng Zuo, â€˜Joint video multi-frame interpolation and deblur-
[9] Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, and Jin- ringunderunknownexposuretimeâ€™,inProceedingsoftheIEEE/CVF
manKim,â€˜Aspatiotemporalvolumetricinterpolationnetworkfor4d ConferenceonComputerVisionandPatternRecognition(CVPR),pp.
dynamicmedicalimageâ€™,inProceedingsoftheIEEE/CVFConference 13935â€“13944,(June2023).
onComputerVisionandPatternRecognition(CVPR),(June2020). [30] WenzheShi,JoseCaballero,FerencHuszÃ¡r,JohannesTotz,AndrewP
[10] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun,â€˜Delving Aitken,RobBishop,DanielRueckert,andZehanWang,â€˜Real-timesin-
deepintorectifiers:Surpassinghuman-levelperformanceonimagenet gleimageandvideosuper-resolutionusinganefficientsub-pixelcon-
classificationâ€™,inProceedingsoftheIEEEInternationalConferenceon volutionalneuralnetworkâ€™,inProceedingsoftheIEEEconferenceon
ComputerVision(ICCV),(December2015). computervisionandpatternrecognition,pp.1874â€“1883,(2016).
[11] ZheweiHuang,TianyuanZhang,WenHeng,BoxinShi,andShuchang [31] ZhihaoShi,XiangyuXu,XiaohongLiu,JunChen,andMing-Hsuan
Zhou, â€˜Rife: Real-time intermediate flow estimation for video frame Yang,â€˜Videoframeinterpolationtransformerâ€™,inProceedingsofthe
interpolationâ€™,arXivpreprintarXiv:2011.06294,(2020). IEEE/CVFConferenceonComputerVisionandPatternRecognition,
[12] ShuichiIto,MitsunoriKanagaki,NaoyaYoshimoto,YoichiroHijikata, pp.17482â€“17491,(2022).
MarinaShimizu,andHiroyukiKimura,â€˜Cerebralproliferativeangiopa- [32] HyeonjunSim,JihyongOh,andMunchurlKim,â€˜Xvfi:Extremevideo
thydepictedbyfour-dimensionalcomputedtomographicangiography: frame interpolationâ€™, in Proceedings of the IEEE/CVF International
Acasereportâ€™,RadiologyCaseReports,(2022). ConferenceonComputerVision,pp.14489â€“14498,(2021).
[13] Zhaoyang Jia, Yan Lu, and Houqiang Li, â€˜Neighbor correspondence [33] Karen Simonyan and Andrew Zisserman, â€˜Very deep convolu-
matchingforflow-basedvideoframesynthesisâ€™,inProceedingsofthe tional networks for large-scale image recognitionâ€™, arXiv preprint
30thACMInternationalConferenceonMultimedia,(2022). arXiv:1409.1556,(2014).
[14] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik [34] KhurramSoomro,AmirRoshanZamir,andMubarakShah,â€˜Ucf101:
Learned-Miller,andJanKautz,â€˜Superslomo:Highqualityestimation Adatasetof101humanactionsclassesfromvideosinthewildâ€™,arXiv
ofmultipleintermediateframesforvideointerpolationâ€™,inProceedings preprintarXiv:1212.0402,(2012).
oftheIEEEconferenceoncomputervisionandpatternrecognition,pp. [35] TianfanXue,BaianChen,JiajunWu,DonglaiWei,andWilliamTFree-
9000â€“9008,(2018). man,â€˜Videoenhancementwithtask-orientedflowâ€™,InternationalJour-
[15] XinJin,LonghaiWu,JieChen,YouxinChen,JayoonKoo,andCheul- nalofComputerVision,(2019).
heeHahm,â€˜Aunifiedpyramidrecurrentnetworkforvideoframein- [36] Zhiyang Yu, Yu Zhang, Dongqing Zou, Xijun Chen, Jimmy S Ren,
terpolationâ€™,inProceedingsoftheIEEE/CVFConferenceonComputer and Shunqing Ren, â€˜Range-nullspace video frame interpolation with
VisionandPatternRecognition,pp.1578â€“1587,(2023). focalizedmotionestimationâ€™,inProceedingsoftheIEEE/CVFConfer-
[16] TarunKalluri,DeepakPathak,ManmohanChandraker,andDuTran, enceonComputerVisionandPatternRecognition,pp.22159â€“22168,
â€˜Flavr: Flow-agnostic video representations for fast frame interpola- (2023).
tionâ€™, inProceedings ofthe IEEE/CVF WinterConference on Appli- [37] GuozhenZhang,YuhanZhu,HaonanWang,YouxinChen,Gangshan
cationsofComputerVision(WACV),pp.2071â€“2082,(January2023). Wu, and Limin Wang, â€˜Extracting motion and appearance via inter-
[17] LingtongKong,BoyuanJiang,DonghaoLuo,WenqingChu,Xiaoming frameattentionforefficientvideoframeinterpolationâ€™,inProceedings
Huang,YingTai,ChengjieWang,andJieYang,â€˜Ifrnet:Intermediate oftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
featurerefinenetworkforefficientframeinterpolationâ€™,inProceedings nition,pp.5682â€“5692,(2023).
oftheIEEE/CVFConferenceonComputerVisionandPatternRecog- [38] Huangxuan Zhao, Zhenghong Zhou, Feihong Wu, Dongqiao Xiang,
nition,pp.1969â€“1978,(2022). Hui Zhao, Wei Zhang, Lin Li, Zhong Li, Jia Huang, Hongyao Hu,
[18] GuilinLiu,FitsumAReda,KevinJShih,Ting-ChunWang,Andrew etal.,â€˜Self-supervisedlearningenables3ddigitalsubtractionangiogra-
Tao,andBryanCatanzaro,â€˜Imageinpaintingforirregularholesusing phyreconstructionfromultra-sparse2dprojectionviews:amulticenter
partial convolutionsâ€™, in Proceedings of the European conference on studyâ€™,CellReportsMedicine,(2022).
computervision(ECCV),pp.85â€“100,(2018). [39] ChangZhou,JieLiu,JieTang,andGangshanWu,â€˜Videoframein-
[19] IlyaLoshchilovandFrankHutter,â€˜Sgdr:Stochasticgradientdescent terpolationwithdenselyqueriedbilateralcorrelationâ€™,arXivpreprint
withwarmrestartsâ€™,arXivpreprintarXiv:1608.03983,(2016). arXiv:2304.13596,(2023).
[20] IlyaLoshchilovandFrankHutter.Decoupledweightdecayregulariza-
tion,2019.SupplementaryMaterials
Table6:Impactofcontextscope(r)onthreeframesinterpolation.
Thebestscoresandthesecondbestareinredandbluerespectively.
S.1.DetailedImpactofContextModelingScope(r)
SSIM(%) PSNR Time(s) Memory
DetailednumericalresultsareasTab.4,5,and6,consistentwiththe r Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 3framesâ†“ 3framesâ†“
conclusionsofÂ§4.6inthemaindocument. 3 91.77 3.57 37.70 3.58 0.118 2.61G
Wewouldliketohighlightthefollowingpoints: 5 91.55 3.65 37.72 3.61 0.120 2.61G
7 91.24 3.70 37.62 3.62 0.118 2.61G
9 91.25 3.71 37.60 3.56 0.118 2.61G
â€¢ OurMoSt-DSAdoesnotsignificantlyincreasememoryusagedur- 11 92.49 3.12 38.30 3.55 0.117 2.61G
13 91.37 3.68 37.66 3.60 0.117 2.61G
ing multi-frame interpolation inference (consuming 2.59, 2.61, 15 92.13 3.32 38.07 3.56 0.117 2.61G
and2.61Gforinterpolating1to3frames,respectively),enabling 17 91.47 3.60 37.70 3.58 0.118 2.61G
19 91.96 3.46 37.95 3.60 0.117 2.61G
convenient offline deployment on memory-constrained devices 21 93.03 2.94 38.67 3.60 0.117 2.61G
andassistingphysiciansinreducingDSAradiationexposure. 23 93.01 2.94 38.66 3.58 0.118 2.61G
25 91.67 3.64 37.82 3.64 0.118 2.61G
â€¢ Furthermore, our MoSt-DSA maintains low computation times
27 91.25 3.76 37.62 3.63 0.118 2.61G
(0.024,0.070,and0.117sforinterpolating1to3frames,respec- 29 91.57 3.62 37.77 3.60 0.118 2.61G
tively),savingvaluabletimeforpatienttreatment. 31 92.14 3.35 38.03 3.54 0.117 2.61G
Ground Truth
Table4:Impactofcontextscope(r)onsingleframeinterpolation. â„’1+â„’ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰+â„’ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†
Thebestscoresandthesecondbestareinredandbluerespectively.
SSIM(%) PSNR Time(s) Memory
r
Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 1frameâ†“ 1frameâ†“
3 93.37 2.68 39.29 3.46 0.024 2.59G
5 92.93 2.85 39.03 3.39 0.025 2.59G
7 93.49 2.67 39.44 3.44 0.025 2.59G
9 93.49 2.68 39.46 3.44 0.024 2.59G
11 93.26 2.83 39.24 3.48 0.025 2.59G
13 93.04 2.94 39.11 3.42 0.025 2.59G
15 93.60 2.66 39.52 3.46 0.025 2.59G
17 92.71 3.02 38.91 3.45 0.025 2.59G
19 93.60 2.64 39.51 3.46 0.025 2.59G
21 93.57 2.69 39.50 3.45 0.025 2.59G
2 23
5
9 93 3. .4 50
0
2 2. .7 75
2
3 39 9. .3 49
7
3 3. .4 43
6
0 0. .0 02 24
5
2 2. .5 59 9G
G
â„’1 â„’1+â„’ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰
27 93.64 2.69 39.55 3.46 0.025 2.59G
29 93.65 2.61 39.55 3.45 0.024 2.59G
31 93.49 2.73 39.46 3.47 0.024 2.59G
Table5:Impactofcontextscope(r)ontwoframesinterpolation.
Thebestscoresandthesecondbestareinredandbluerespectively.
SSIM(%) PSNR Time(s) Memory
r
Meanâ†‘ STDâ†“ Meanâ†‘ STDâ†“ 2framesâ†“ 2framesâ†“
3 92.60 3.07 38.55 3.40 0.071 2.61G Figure11:LossfunctioncomparisononourMoSt-DSA.Ourloss
5 92.84 3.05 38.73 3.53 0.071 2.61G
7 92.88 2.83 38.73 3.38 0.071 2.61G functionstrategyshowsasignificantimprovement(greenboxes).
9 92.95 2.85 38.78 3.37 0.071 2.61G
11 93.13 2.83 38.84 3.36 0.071 2.61G
13 92.66 3.02 38.58 3.38 0.071 2.61G S.3.3DReconstructionShowcasefromSingleFrame
15 93.12 2.83 38.93 3.33 0.071 2.61G Interpolation
17 93.11 2.84 38.94 3.36 0.071 2.61G
19 93.15 2.85 38.66 3.38 0.071 2.61G
WeprovideashowcaseasFig.12,consistentwiththeconclusionsof
21 91.94 3.15 38.20 3.25 0.071 2.61G
23 92.89 3.01 38.78 3.53 0.071 2.61G Â§4.5inthemaindocument.AsshowninFig.12,ourresultsarevir-
25 93.12 2.84 38.92 3.31 0.070 2.61G
tuallyindistinguishabletothereconstructionfromoriginaldata,with
27 93.14 2.85 38.96 3.39 0.070 2.61G
29 93.14 2.84 38.94 3.38 0.070 2.61G differencesonlyatthemostdelicatepartsofthebloodvessels(blue
31 92.40 3.23 38.41 3.40 0.070 2.61G box).Also,ourresultsaccuratelymaintainlesionsizesanddetails,
aligningwiththeoriginaldata(greenarrows).Theaboveconclusion
indicatesthatourMoSt-DSAcanalsoassistphysiciansin3Ddiag-
nosisandtreatment.
S.2.LossFunctionComparisononOurMoSt-DSA
AsshowninFig.11,solelyusingL ,orcombiningL withL ,
1 1 VGG
resultsinblurredimagedetails(redboxes),whereasourcombined
lossfunctionnotonlyeliminatestheblurrinessbutalsocloselyap-
proximatesthegroundtruth(greenboxes).Figure12:3Dreconstructionshowcase:basedonoursingle-frameinterpolationresult(below)vs.originaldata(above).Ourresultsare
virtuallyindistinguishabletothereconstructionfromoriginaldata,withdifferencesonlyatthemostdelicatepartsofthebloodvessels(blue
boxes).Also,ourresultsaccuratelymaintainlesionsizesanddetails,aligningwiththeoriginaldata(greenarrows).