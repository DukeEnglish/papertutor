ConceptExpress: Harnessing Diffusion Models for
Single-image Unsupervised Concept Extraction
Shaozhe Hao, Kai Han‚ãÜ, Zhengyao Lv
Shihao Zhao, and Kwan-Yee K. Wong‚ãÜ
The University of Hong Kong
{szhao,shzhao,kykwong}@cs.hku.hk kaihanx@hku.hk cszy98@gmail.com
Abstract. Whilepersonalizedtext-to-imagegenerationhasenabledthe
learning of a single concept from multiple images, a more practical yet
challenging scenario involves learning multiple concepts within a single
image. However, existing works tackling this scenario heavily rely on
extensive human annotations. In this paper, we introduce a novel task
named Unsupervised Concept Extraction (UCE) that considers an un-
supervisedsettingwithoutanyhumanknowledgeoftheconcepts.Given
an image that contains multiple concepts, the task aims to extract and
recreateindividualconceptssolelyrelyingontheexistingknowledgefrom
pretraineddiffusionmodels.Toachievethis,wepresentConceptExpress
that tackles UCE by unleashing the inherent capabilities of pretrained
diffusion models in two aspects. Specifically, a concept localization ap-
proach automatically locates and disentangles salient concepts by lever-
aging spatial correspondence from diffusion self-attention; and based on
the lookup association between a concept and a conceptual token, a
concept-wiseoptimizationprocesslearnsdiscriminativetokensthatrep-
resenteachindividualconcept.Finally,weestablishanevaluationproto-
col tailored for the UCE task. Extensive experiments demonstrate that
ConceptExpress is a promising solution to the UCE task. Our code and
data are available at: https://github.com/haoosz/ConceptExpress
Keywords: Unsupervised concept extraction ¬∑ Diffusion model
1 Introduction
After observing an image containing multiple concepts, a skilled painter can
recreateeachindividualconceptwithinthecomplexscene.Thisremarkablecog-
nitiveabilitypromptsustoraiseanintriguingquestion:Do text-to-image gener-
ative models also possess the capability to extract and recreate concepts? In this
paper, we try to provide an answer to this question by harnessing the potential
of Stable Diffusion [54] in concept extraction.
Diffusion models [23,45,50,54,57,62] have exhibited unprecedented per-
formance in photorealistic text-to-image generation. Although diffusion models
‚ãÜ Corresponding authors
4202
luJ
9
]VC.sc[
1v77070.7042:viXra2 S. Hao et al.
Conceptual tokens
[V1]‚ãØ[Vi]‚ãØ[VN]‚ãØ
Given image
Unsupervised:
‚úó No concept descriptors
‚úó No object masks Frozen Pretrained
Diffusion Model
‚úó No instance numbers
‚úì Diffusion model
Extraction
Extracted concepts
Fig.1: Unsupervised concept extraction. We focuson the unsupervisedproblem
of extracting multiple concepts from a single image. Given an image that contains
multiple concepts (e.g., Star Wars characters C-3PO, R2-D2, and desert), we aim to
harness a frozen pretrained diffusion model to automatically learn the conceptual to-
kens. Using the learned conceptual tokens, we can regenerate the extracted concepts
withhighquality,asshownintherightmostcolumn.Inthisprocess,nohumanknowl-
edgeoraidsareavailable,andweonlyrelyontheinherentcapabilitiesofthepretrained
Stable Diffusion [54].
are trained solely for the purpose of text-to-image generation, extensive evi-
dencesuggeststheirunderlyingcapabilitiesinvarioustasks,includingclassifica-
tion[37],segmentation[28,44,66,69,76],andsemanticcorrespondence[22,39,81].
This indicates that diffusion models embed significant world knowledge, poten-
tially enabling them to perceive and recreate concepts akin to skilled painters.
Motivated by this insight, we delve into this problem and explore the untapped
potentialofStableDiffusion[54]inconceptextraction.Whilerecentresearch[2,
26]hasmadeinitialattemptsinexploringconceptextractionusingStableDiffu-
sion, existing approaches heavily rely on external human knowledge for supervi-
sion during the learning process. For example, Break-A-Scene [2] demands pre-
annotated object masks, while MCPL [26] requires accurate concept-descriptive
captions.However,thesehumanaidsarebothcostlyandofteninaccessible.This
criticalconstraintrendersexistingapproachesinfeasible,asnoneofthemextract
concepts without using any prior knowledge of the concepts.
To bridge this gap, we introduce a novel and challenging task named Unsu-
pervisedConceptExtraction (UCE).Givenanimagecontainingmultipleobjects,
UCE aims to automatically extract the object concepts such that they can be
used to generate new images. In UCE, we consider a strict and realistic ‚Äúunsu-
pervised‚Äù setting, in which there is no prior knowledge about the image or the
concepts present within it. Specifically, ‚Äúunsupervised‚Äù emphasizes (1) no con-
cept descriptors for proper word embedding initialization, (2) no object masks
for concept localization and disentanglement, and (3) no instance number for a
definite number of concepts to be extracted. We illustrate UCE in Fig. 1.
To tackle this problem, we introduce ConceptExpress, the first method
designedforunsupervisedconceptextraction.ConceptExpressunleashesthein-
herentcapabilitiesofpretrainedStableDiffusion,enablingittodisentangleeach
concept in the compositional scene and learn discriminative conceptual tokens
that represent each individual concept. ConceptExpress presents two major in-
novations. (1) For concept disentanglement, we propose a concept localization
approach that automatically locates salient concepts within the image. This ap-
proachinvolvesclusteringspatialpointsontheself-attentionmap,buildingupon
theobservationthatStableDiffusionhaslearnedgoodunsupervisedspatialcor-ConceptExpress 3
respondence in the self-attention layers [66]. Our approach has three sequential
steps,namelypre-clustering,filtering,andpost-clustering,seamlesslyintegrating
aparameter-freehierarchicalclusteringmethod[58].Byutilizingtheend-of-text
cross-attentionmapasamagnitudefilter,wefilteroutnon-salientbackgrounds.
Additionally, our approach automatically determines the number of concepts
based on self-adaptive clustering constraints. (2) For conceptual token learning,
weemployconcept-wisemaskeddenoisingoptimizationbyreconstructingthelo-
catedconcept.Thisoptimizationisbasedonatokenlookuptablethatassociates
eachlocatedconceptwithitscorrespondingconceptualtoken.Toaddresstheis-
sueofabsenceofinitialwords,whichcandetrimentallyimpactoptimization[16],
we introduce a split-and-merge strategy for robust token initialization, mitigat-
ing performance degradation. To prevent undesired cross-attention activation
with the wrong concept, we incorporate regularization to align cross-attention
maps with the desired concept activation exhibited in self-attention maps.
To evaluate the new UCE task, we construct a new dataset that contains
various multi-concept images, and introduce an evaluation protocol including
two metrics tailored for unsupervised concept extraction. We use concept simi-
larity, including identity similarity and compositional similarity, to measure the
absolute similarity between the source and the generated concepts. We also use
classificationaccuracytoassessthedegreeofconceptdisentanglement.Through
comprehensive experiments, our results demonstrate that ConceptExpress suc-
cessfully tackles the challenge of unsupervised concept extraction, as evidenced
by both qualitative and quantitative evaluations.
2 Related Work
Text-to-image synthesis IntherealmofGANs[8,20,29‚Äì31],plentyofworks
havegainedremarkableadvancementsintext-to-imagegeneration[52,64,77,78,
80,86] and text-driven image manipulation [1,18,47,75], significantly pushing
forward image synthesis conditioned on plain text. Content-rich text-to-image
generationisachievedbyauto-regressivemodels[51,79]thataretrainedonlarge-
scale text-image data. Based on the pretrained CLIP [49], Crowson et al. [14]
optimizes the generated image at test time using CLIP similarity without any
training. Diffusion-based methods [23] have pushed the boundaries of text-to-
image generation to a new level, e.g., DALL¬∑E 2 [50], Imagen [57], GLIDE [45],
and LDM [54]. Based on the implementation of LDMs [54], Stable Diffusion
(SD), large-scale trained on LAION-5B [59], achieves unprecedented text-to-
image synthesis performance. Diffusion models are widely used for various tasks
such as controllable generation [82,85], global [9,67] and local editing [3,5,13,
32,46,70], video generation [24,61,74] and editing [43,83], inpainting [41], and
scene generation [4,6].
Generative concept learning Recently, many works [12,16,17,21,25,36,38,
42,48,55,60,65,73] have emerged, aiming to learn a generative concept from
multiple images. For example, Textual Inversion [16] learns an embedding vec-
tor that represents a concept in the textual embedding space. Liu et al. [40]4 S. Hao et al.
extended it to multi-concept discovery using composable diffusion models [15].
Theirworkoperatesinanunsupervisedsettinglikeours.However,thereisama-
jor difference: they extract concepts from multiple images, each containing only
one concept, whereas our focus is on extracting multiple concepts from a single
image. Our work is closely related to Break-A-Scene [2] which relies heavily on
human-annotated masks that are not available in our setting. The concurrent
works,MCPL[26]andDisenDiff[84],addressasimilarproblem,buttheyrequire
either a concept-descriptive text caption or specific class names, which renders
theminfeasibleforourtask.Therearealsoworksrelatedtogenerativeconcepts
include concept erasing [19,35], decomposition [11,68], manipulation [72], and
creative generation [53].
Attention-based segmentation Pre-trained Stable Diffusion [54] possesses
highly informative semantic representations within its attention layers. This
property effectively enables its cross-attention layers to indicate the interrela-
tionsbetweentextandimagetokens[63],anditsself-attentionlayerstocapture
thespatialcorrespondenceamongimagetokens.Consequently,priorworks[7,28,
44,66,69,76]haveexploredtheutilizationofthepre-trainedStableDiffusionfor
semantic segmentation, showing remarkable performance in unsupervised zero-
shot segmentation. Diffsegmenter [69] and FTM [76] use cross-attention to ini-
tialize segmentation maps, and then extract affinity weights from self-attention
for further refinement. DiffSeg [66] achieves unsupervised zero-shot segmenta-
tionbyclusteringaggregatedself-attentionmaps.Theirinvestigationoftheself-
attention property inspires our concept localization approach.
3 Unsupervised Concept Extraction
We aim to learn discriminative tokens that can represent multiple instance-level
concepts from a single image in an unsupervised manner. Specifically, given an
imageI containingmultiplesalientinstances,weuseapretrainedtext-to-image
diffusion model to discover a set of conceptual tokens S = {[V ]}N and their
i i=1
corresponding embedding vectors V = {v }N , which capture discriminative
i i=1
concepts from I. The concept number N is automatically determined in the
discovery process. By prompting the i-th token [V ] ‚àà S, we can recreate the
i
corresponding concept extracted from I. We present ConceptExpress to tackle
this problem. Fig. 2 gives an overview of ConceptExpress.
3.1 Preliminary
Text-to-image diffusion model[54]iscomposedofapretrainedautoencoder
with an encoder E to extract latent codes and a corresponding decoder D to
reconstruct images, a CLIP [49] text encoder that extracts text embeddings,
and a denoising U-Net œµ with text-conditional cross-attention blocks. Textual
Œ∏
inversion[16]representsaparticularconceptusingalearnableembeddingvector
v , which is optimized using a standard latent denoising loss with œµ frozen,
‚ãÜ Œ∏
written as
L=E (cid:2) ‚à•œµ‚àíœµ (z ,t,c (y))‚à•2(cid:3) , (1)
z‚àºE(I),y,œµ‚àºN(0,1),t Œ∏ t v‚ãÜ 2ConceptExpress 5
‚àÖ [V]‚ãØ[V]‚ãØ[V]‚ãØ‚ãØ
1 i N
üî•Trainable
üî•
‚ùÑFrozen
grad
StableDiffusion
ùìõùëµ
+ ùúñ~ùí©(0,1)
Self- Cross-
ùìõ
Attention Attention ùíä
ùìõùüè
‚ùÑ
Token Lookup Table (ùì£)
‚ãØ ‚ãØ
m m m
1 i N
Image ùêº
LoC co an lic ze ap tit
o n
{ [V 1]:m 1, ‚ãØ , [V i]:m i, ‚ãØ , [V N]:m N }
Fig.2:OverviewofConceptExpress.ConceptExpresstakesamulti-conceptimage
I asinputandlearnsasetofconceptualtokens.ConceptExpressconsistsofthreekey
components. First, it leverages self-attention maps from the unconditional token ‚àÖ to
locate the latent concepts. Second, it constructs a token lookup table that associates
each concept mask with its corresponding conceptual token [V ]. Finally, it optimizes
i
each conceptual token using a masked denoising loss. The learned conceptual tokens
canthenbeusedtogenerateimagesthatrepresenteachindividualconcept.SeeSec.3
for more details of the method.
where t is the timestep, z is the latent code at timestep t, œµ is the randomly
t
sampled Gaussian noise, y is the text prompt, and c is the text encoder pa-
v‚ãÜ
rameterized by the learnable v . ConceptExpress advances further by learning
‚ãÜ
multiple embedding vectors in an unsupervised setting.
FINCH[58]isanefficientparameter-freehierarchicalclusteringmethod.Given
a set of n sample points in d dimensions, denoted as S = {s | s ‚àà Rd}n , we
i i i=1
construct an adjacent matrix G for paired samples as
(cid:40)
1 if Œ∫ =j or Œ∫ =i or Œ∫ =Œ∫
G(i,j)= i j i j , (2)
0 otherwise
whereŒ∫ representstheindexoftheclosestsampletos ‚àà S underaspecificdis-
i i
tancemetric.Toobtainasamplepartition,wegrouptheconnectedcomponents
withintheundirectedgraphdefinedbytheadjacencymatrixG.Eachconnected
component in the graph represents a cluster, and the centroids of the clusters
are treated as super sample points for constructing a new adjacent matrix. This
processenablesiterativehierarchicalclusteringuntilallsamplesaregrouped.As
a result, multiple clustering levels of varying granularity are generated.
3.2 Automatic Latent Concept Localization
We begin by locating instance-level concepts within the diffusion latent space.
In pretrained diffusion models, self-attention possesses good properties of spa-
tialcorrespondencewhichofferstheinherentbenefitasanunsupervisedsemantic
‚ãØ
‚ãØ6 S. Hao et al.
Given ùêºi mage m!!$ "! # Cro os fs - [a EOtte T]n ùíÜtion Filtered masks m!!$ "# f!!$ "# [Vi] [V1] [V2]
ùë£!
m!
f!
Pre-clustering Filtering Post-clustering Token Lookup Table
Fig.3: Visualization. Left: we visualize the concept localization process, which in-
volves: (1) pre-clustering that groups together semantically related regions; (2) filter-
ing that removes non-salient regions that are not visually significant; and (3) post-
clustering that integrates salient regions into instance-level concepts. Right: we visu-
alize the token lookup table, which establishes a one-to-one correspondence between
the conceptual token [V ] and the learnable embedding vector v , the latent mask m ,
i i i
and the attention map f .
i
segmenter [66]. With this insight, we propose an approach to automatically lo-
cating concepts by subtly leveraging self-attention.
Let A
l
‚àà R(hl√ówl)√ó(hl√ówl) denote the self-attention map from the l-th layer
oftheU-Net,wherethefeaturemaphasaspatialresolutionh √ów .Toaggregate
l l
self-attention maps from different layers into an identical resolution h√ów, we
follow the practice in [66] to interpolate the last two dimensions, duplicate the
first two dimensions, and average all maps. The aggregated attention, denoted
asA‚ààR(h√ów)√ó(h√ów),canberepresentedasasetofh√ów spatialsamples,each
of which is an h√ów dimensional distribution, i.e., A = {a | a ‚àà Rh√ów}h√ów.
i i i=1
By clustering on A, we can naturally derive latent masks that align with the
semantic segmentation of the original image. This is because latent patches
sharing similar semantics tend to possess consistent self-attention activations.
Themasksareformedbycombiningspatialsamplesbelongingtothesameclus-
ter, effectively representing specific segments in the image. However, accurately
locating instance-level concepts and effectively filtering out the background re-
main challenging when our goal is to disentangle multiple instances rather than
solely segmenting semantics. To tackle this challenge, we adapt the hierarchical
clusteringalgorithmFINCH[58]togeneratelatentmasksthatsatisfyourneeds.
Pre-clustering WefirstapplyFINCHalgorithmonA.Sincea isnormalized
i
andtreatedasadistribution,Œ∫ canbedeterminedusingadistributiondistance
i
metric, specifically the mean KL divergence, i.e.,
d(a ,a )=(D (a ,a )+D (a ,a ))/2, (3)
i j KL i j KL j i
Œ∫ =argmin {d(a ,a ) | a ‚ààA}. (4)
i i j j
j
We set the upper limit of the number of discovered concepts to N . We then
max
identify the clustering level with the cluster number N‚Ä≤ closest to but greater
than N . At this level, we construct a mask for each cluster from all spa-
max
tial points within the cluster. We denote the resulting masks as {m | m ‚àà
i i
{0,1}h√ów}N‚Ä≤ . Since spatial samples within the same cluster share consistent
i=1
semantics, the distribution distance between them serves as an effective indi-
cator for distinguishing between different semantic instances. Therefore, we useConceptExpress 7
the largest intra-cluster distance at this level, denoted as Œ¥, as a self-adaptive
threshold to determine the final clustering level in the post-clustering phase.
Filtering The obtained masks cover all areas on the latent map, encompass-
ing both the foreground instances with clear semantics and the indistinct back-
groundregions.Indiffusionmodels,thecross-attentionmapoftheend-of-textto-
ken([EOT])demonstratesrobustforegroundlocalizationcapabilities[21],where
salient regions exhibit higher magnitudes and vice versa. This characteristic
makes it suited for automatically distinguishing between distinct instances and
indistinct backgrounds. Let e‚ààRh√ów denote the cross-attention map of [EOT].
Based on e, we discard masks whose masked regions satisfy
‚à•vec(m ‚äôe)‚à• ‚à•vec(e)‚à•
i 1 < 1 (5)
‚à•vec(m )‚à• h√ów
i 1
where vec(¬∑), ‚à•¬∑‚à• , and ‚äô denote matrix vectorization, ‚Ñì norm, and Hadamard
1 1
productrespectively.Byapplyingthiscriterion,wefilteroutthosemaskswhose
masked regions show magnitudes lower than the average level, indicating that
they correspond to indistinct regions. This criterion helps identify and exclude
masks that correspond to indistinct regions in the [EOT] cross-attention map.
Post-clustering After filtering, we reapply FINCH to the remaining clusters
iteratively. Additionally, we introduce two extra constraints to determine the
stopping point in the clustering procedure. (1) To enhance the proximity of
semantic relationships within the same mask, we set G(i,j)=0 if the distance
d(a ,a ) exceeds Œ¥, which is determined in the level with N‚Ä≤ clusters of pre-
i j
clustering. By removing such connections, we hinder the grouping of strong se-
manticvariationswithinthesamemask.(2)Weforbidnon-adjacentmasksfrom
grouping together, i.e., masks that are not spatially adjacent to each other can-
not be clustered together, regardless of their connectivity in G. With these two
constraints, the clustering will automatically terminate and yield N masks that
locate the latent spaces corresponding to the N target concepts. The mean at-
tentionactivationsofeachconceptregionispreciselythecentroidofeachcluster,
given by
f =m1√ó(h√ów)¬∑A(h√ów)√ó(h√ów) / ‚à•vec(m )‚à• (6)
i i i 1
where the centroid f ‚ààR1√ó(h√ów) represents the average attention activations of
i
i-th masked latent region to the entire h√ów latent space. The latent masks and
their corresponding attention activations are ready for token optimization. The
concept localization process is visualized in Fig. 3 (left).
3.3 Concept-wise Masked Denoising
We construct a token lookup table
T :={[V ]:(v ,m ,f ) | i=1,2,¬∑¬∑¬∑ ,N} (7)
lookup i i i i
where the i-th conceptual token [V ] corresponds to a learnable embedding vec-
i
tor v , a latent mask m ‚àà {0,1}h√ów, and a mean attention map f ‚àà Rh√ów.
i i i8 S. Hao et al.
We visualize the token lookup table in Fig. 3 (right). We employ the masked
denoising loss [2] to optimize each token [V ]‚ààT :
i lookup
(cid:104) (cid:105)
L =E ‚à•[œµ‚àíœµ (z ,t,c (y ))]‚äôm ‚à•2 (8)
i z‚àºE(I),yi,œµ,t Œ∏ t vi i i 2
where y is the text prompt ‚Äúa photo of [V ]" and v is the only trainable
i i i
parameter. Masked denoising forces the new token to learn exclusively within
specific latent regions that contain concept-wise information.
Robust token initialization To
learn the concept embedding above, Training Process
[V1]1
one may think to directly apply tex-
[V1]2 1/g Œ£ [V1]
tual inversion [16]. However, the use [V1]3
of suitable words for initializing the [V2]1
conceptual tokens is crucial for suc- [V2]2 1/g Œ£ [V2]
cessful textual inversion. In the case
[V2]3
ofanunsupervisedsetting,wherespe- Initialization Split (g=3) & Training Merge Finetuning
Fig.4: Split-and-merge. During the
cific words for initializing concept to-
training process, we sequentially initialize
kens are not available, the perfor- conceptual tokens, train the split tokens,
mance of [16] will deteriorate sharply. mergethetokensbyaveraging,andfurther
To resolve this problem, we propose fine-tune the merged tokens. Finally, the
a split-and-merge strategy that ran- merged tokens are well-learned and effec-
domly initializes multiple tokens for tively represent individual concepts.
each concept, which are later merged into a single token after several warm-
up steps. Multiple tokens can explore a broader concept space, providing a
greater opportunity for convergence into an embedding vector that can more
precisely represent the underlying concept. Formally, we randomly initialize g
tokens {[V ]j}g for each concept and extend the token lookup table as
i j=1
(cid:110) (cid:111)
Tsplit := [V ]j :(vj,m ,f ) | i=1,...,N;j=1,...,g , (9)
lookup i i i i
wherevj isthej-thrandomlyinitializedembeddingvectorcorrespondingtothe
i
conceptualtoken[V ]j.Attheearlytrainingsteps,weoptimizethelossinEq.(8)
i
on [V ]j ‚àà Tsplit , i.e., L , to learn the g√óN tokens. In addition, leveraging
i lookup i,j
the constraint that embeddings for the same concept should exhibit a closer
embedding distance, we incorporate a contrastive loss for each token [V ]j as
i
Lcon =‚àí 1 log(cid:80) v iq‚ààVi\{v ij}exp(v ij ¬∑v iq/œÑ) , (10)
i,j g√óN (cid:80) exp(vj ¬∑vn/œÑ)
vn‚ààV\{vj} i m
m i
where œÑ is the temperature, V is the full set of embedding vectors, and V is
i
the subset of embedding vectors that correspond to the i-th concept. Eq. (10)
enforces tokens representing the same concept to be closer to each other, induc-
ing these randomly initialized embedding vectors to converge to a shared space
during several warm-up training steps. Afterward, we merge the tokens by com-
puting the mean value of the g embeddings associated with each concept. The
token lookup table is reset to Eq. (7), where the token embeddings are good
initializers to robustly represent the corresponding concepts. In the subsequentConceptExpress 9
training steps, we use the denoising loss described in Eq. (8) to optimize the
merged tokens that represent each concept on a one-to-one basis. We depict the
training process using split-and-merge in Fig. 4.
Attention alignment Althougheachconceptualtokenisoptimizedtorecon-
struct the masked region, there is a lack of direct alignment between the tokens
and individual concepts within a compositional scene. This absence of align-
ment leads to inaccurate cross-attention activation for the learned conceptual
tokens, which hinders the performance of compositional generation. To address
thisproblem,foreachtokeninthelookuptable,wealignitscross-attentionmap
with the mean attention f of the correspondingmaskedregion using a location-
i
aware earth mover‚Äôs distance (EMD) regularization. The earth moving cost is
computed as the Euclidean distance between the 2D locations on two attention
maps. Let the cross attention map of the token [V ]j be c ‚àà Rh√ów, where j
i [Vi]j
can be omitted after token merging. The regularization loss is formulated as
Lreg =EMD(c ,f ) (11)
i,j [Vi]j i
which softly guides the cross-attention map to match the desired concept acti-
vations exhibited in the self-attention map.
3.4 Implementation Details
We train the tokens in two phases for a total of 500 steps, with a learning rate
of 5e-4. In the first 100 steps, we optimize the tokens [V ]j ‚ààTsplit using
i lookup
N g
1 (cid:88)(cid:88)
L= (L +Œ±Lcon+Œ≤Lreg). (12)
g√óN i,j i,j i,j
i=1j=1
We then merge the tokens, deriving [V ] ‚àà T , and optimize them in the
i lookup
subsequent 400 steps using
N
1 (cid:88)
L= (L +Œ≤Lreg). (13)
N i i
i=1
We use Stable Diffusion v2-1 [54] as our base model. We set Œ±=1e-3, Œ≤=1e-5,
œÑ=0.07, and g=5. All experiments are conducted on a single RTX 3090 GPU.
In our implementation, self-attention used in concept localization is computed
using the unconditional text prompt ‚àÖ at timestep 0, which induces minimal
textual intervention and maximal denoising of the given image.
4 Experiments
4.1 Dataset and Baseline
Dataset In our work, we do not rely on predefined object masks or manually
selected initial words for training images. This allows us to gather high-quality
imagesfromtheInternetwithouthumanannotationstoformourdataset.Specif-
ically, we collect a set D of 961 images from Unsplash2, ensuring that each
1
1 96 is considerably large compared to the dataset sizes in the previous works, such
as 30 in DreamBooth [55], 50 in Break-A-Scene [2], and 10 in DisenDiff [84].
2 https://unsplash.com/10 S. Hao et al.
image contains at least two distinct instance-level concepts. The collected im-
agesencompassawiderangeofobjectcategories,includinganimals,characters,
toys, accessories, containers, sculptures, buildings, landscapes, vehicles, foods,
and plants. For a fair comparison, we construct a set D using the 7 images
2
provided by [2]. For evaluation, we generate 8 testing images for each training
image using the prompt ‚Äúa photo of [V ]‚Äù.
i
Baseline To the best of our knowledge, Break-A-Scene [2] is the only work
that is closely related to the problem in this paper. However, Break-A-Scene
operates under a strongly supervised setting, which requires significant prior
knowledgeofthetrainingimage,includingthenumberofconcepts,objectmasks,
andproperlyselectedinitialwords.Toensureafairandmeaningfulcomparison,
weadaptBreak-A-Scenetoourunsupervisedsetting.Specifically,wedisablethe
use of manually picked initial words and instead apply random initialization.
Additionally, we leverage the instance masks identified by our method as the
annotated masks for Break-A-Scene. Finally, we exclusively train the learnable
tokens without fine-tuning the diffusion model. We use this adapted version of
Break-A-Scene, denoted as BaS‚Ä†, as the baseline method for comparison.
4.2 Evaluation Metric
We establish an evaluation protocol for the UCE problem, which includes two
tailored metrics described as follows.
Concept similarity To quantify how well the model is able to recreate the
concepts accurately, we evaluate the concept similarity, including identity sim-
ilarity (SIMI) and compositional similarity (SIMC). Identity similarity mea-
suresthesimilaritybetweeneachconceptinthetrainingimageandtheconcept-
specific generated images. We employ CLIP [49] and DINO [10] to compute the
similarities. To ensure that the similarity is computed specifically for the i-th
concept,weobtainconcept-wisemaskswithSAM[33]byidentifyingthespecific
SAMmaskassociatedwithourextractedconcept.Specifically,foreachconcept,
we prompt SAM with 3 randomly sampled points on our extracted mask to
produce SAM masks. The training image is then masked with the SAM mask
corresponding to the i-th concept. The metric of identity similarity provides a
crucial criterion for evaluating the intra-concept performance of unsupervised
concept extraction. Compositional similarity measures the CLIP or DINO sim-
ilarity between the source image and the generated image, conditioned on the
prompt ‚Äúa photo of [V ] and [V ] ... [V ]‚Äù. This metric quantifies the degree to
1 2 N
which the source image can be reversed using the extracted concepts.
Classification accuracy Toassesstheextentofdisentanglementachievedfor
eachconceptwithinthefullsetofextractedconcepts,weestablishabenchmark
that evaluates concept classification accuracy. Specifically, we first employ a
visionencoder,suchasCLIP[49]orDINO[10],toextractfeaturerepresentations
for each concept from the SAM-masked training images. In total, we obtain
264 concepts in D and 19 concepts in D . We use these concept features as
1 2
prototypes to construct a concept classifier. We then employ the same visionConceptExpress 11
Source BaS‚Ä† Ours Source BaS‚Ä† Ours Source BaS‚Ä† Ours
image image image
1 1 1
2 2 2
3 3 3
1 1 1
2 2 2
Fig.5: Comparison with BaS‚Ä† [2]. We compare the concept extraction results of
BaS‚Ä† andConceptExpressin6examples.Foreachexample,weshowthesourceimage
andthegeneratedconceptimages.Weannotateconceptsinserialnumbersforlegibility.
Table 1: Quantitative comparison. For reference, we also provide the results of
original Break-A-Scene [2] on D (marked in grey), by using mask and initializer su-
2
pervision (BaS) and further finetuning (BaS f.t.).
(a) EvaluationusingCLIP[49]. (b) EvaluationusingDINO[10].
D1 D2 D1 D2
Method SIMISIMCACC1ACC3SIMISIMCACC1ACC3 Method SIMISIMCACC1ACC3SIMISIMCACC1ACC3
BaS[2] ‚Äì ‚Äì ‚Äì ‚Äì 0.686 0.696 0.467 0.599 BaS[2] ‚Äì ‚Äì ‚Äì ‚Äì 0.316 0.474 0.559 0.704
BaSf.t.[2] ‚Äì ‚Äì ‚Äì ‚Äì 0.693 0.789 0.526 0.697 BaSf.t.[2] ‚Äì ‚Äì ‚Äì ‚Äì 0.411 0.696 0.697 0.737
BaS‚Ä†[2] 0.627 0.773 0.174 0.282 0.613 0.653 0.368 0.487 BaS‚Ä†[2] 0.254 0.510 0.202 0.315 0.231 0.417 0.329 0.559
Ours 0.6890.7840.2630.3850.7150.7370.5660.783 Ours 0.3190.5680.3240.4700.3710.5350.8030.934
encoder to extract query features for all generated images, each associated with
aspecificconceptcategory.Finally,weevaluatethetop-k classificationaccuracy
ofthequeryfeaturesusingourconceptclassifier.Wereportclassificationresults
fork=1,3,denotingthetop-kaccuracyasACCk.Thismetriceffectivelyassesses
the inter-concept performance of unsupervised concept extraction.
4.3 Performance
Quantitative comparison WecompareConceptExpresswithBaS‚Ä† basedon
conceptsimilarityandclassificationaccuracymetrics.Thequantitativecompar-
ison results on the two datasets are reported in Tabs. 1a and 1b, respectively
with CLIP [49] and DINO [10] as the visual encoder. Notably, ConceptExpress
outperforms BaS‚Ä† by a significant margin on all evaluation metrics. It achieves
higher concept similarity SIMI and SIMC, indicating a closer alignment with
the source concepts. It also achieves higher classification accuracy ACC1 and
ACC3,indicatingamoresignificantlevelofdisentanglementamongtheindivid-
ually extracted concepts. These results highlight the limitations of the existing
concept extraction approach [2] and establish ConceptExpress as the state-of-
the-art method for the UCE problem.
Qualitative comparison We show several generation samples of Concept-
Express and BaS‚Ä† in Fig. 5. ConceptExpress presents overall better generation12 S. Hao et al.
w/o SnM
g=5
w/ SnM merge
Source image (ours)
Training step
0-100 (split) 100 (merge) 200 500 (end)
Fig.6: Generation results of Split-and-
Source image ùêæ-means FINCH Ours
merge (SnM) ablation. We show the gen- Fig.7: Comparison on self-
erated concept ‚Äútraffic light‚Äù throughout the attention clustering. Each located
training process, with (bottom) and without concept is enclosed within a distinct
(top) SnM that utilizes g=5 diverse tokens. colored region.
fidelity and quality than BaS‚Ä†. We observe some defects in the generations of
BaS‚Ä†. For example, in the top left ‚ù∏ and the top center ‚ù∑, the generation of
BaS‚Ä† deviates from the source concept. In addition, BaS‚Ä† fails to preserve the
characteristics of the source concept in the top center ‚ù∏ and the bottom left ‚ù∂
‚ù∑. ConceptExpress effectively overcomes the defects of wrong identity and poor
preservation observed in the generations in BaS‚Ä†. ConceptExpress consistently
generates high-quality images that precisely align with the source concepts.
4.4 Ablation Study
WeconductaquantitativeablationstudyonthetrainingcomponentsinTab.2.
Effectiveness of split-and-merge strategy (SnM) By comparing Rows
(0)and(1),wevalidatethebenefitofthesplit-and-mergestrategytoinitializer-
absent training. The split-and-merge strategy effectively improves identity sim-
ilarity and classification accuracy while slightly sacrificing compositional sim-
ilarity due to its strong focus on a single concept. In Fig. 6, we present the
generated images at different training steps, which reveals how SnM rectifies
the training direction. The results illustrate that SnM effectively expands the
concept space, allowing learnable tokens to explore a wider range of concepts,
ultimately resulting in a more faithful concept indicator.
Effectiveness of regularization By comparing Rows (0) and (2) in Tab. 2,
we observe that regularizing the attention map can enhance the generation per-
formance of individual concepts. Row (3) is our full method which further im-
proves the performance regarding all metrics compared to incorporating each
component in Rows (1) and (2). The thorough ablation study indicates the ef-
fectiveness of each training component in ConceptExpress.
4.5 Concept Localization Analysis
Self-attention clustering To validate the significance of our three-phase
method for concept localization, we compare it with k-means and our base
method FINCH [58]. Since k-means requires a predefined cluster number and
FINCH requires a stopping point, we set a proper cluster number of 7 for them.
Afterclustering,weapplytheproposedfilteringmethodforfaircomparison.WeConceptExpress 13
Table 2: Ablation study. Concept- Table 3: Comparison of concept lo-
wiseoptimization,split-and-mergestrat- calization. ‚Äú#clust‚Äù predefines the clus-
egy, and regularization are respectively ter number for k-kmeans and FINCH. The
abbreviatedasCwO,SnM,andReg.The best value is highlighted in bold, while the
resultsareevaluatedonD usingDINO. second-best value is underlined.
2
CwO SnM Reg SIMI SIMC ACC1 ACC3 Method k-means FINCH Ours MC[71]
(0) ‚úì 0.344 0.549 0.625 0.776 #clust 4 5 6 7 4 5 6 7 auto given
(1) ‚úì ‚úì 0.362 0.519 0.750 0.901 IoU 52.8 51.4 48.7 47.6 50.7 54.8 54.6 51.5 57.3 58.1
(2) ‚úì ‚úì 0.364 0.490 0.724 0.895 Recall 70.3 90.1 95.5 95.7 83.7 93.4 96.9 97.9 89.1 97.3
(3) ‚úì ‚úì ‚úì 0.371 0.535 0.803 0.934 Precision 92.7 88.0 81.5 75.4 98.7 90.7 85.8 77.6 93.7 77.0
comparetheconceptlocalizationresultsusingk-means,FINCH,andourmethod
inFig.7.Wecanobservethatusingk-meansorFINCHwillmisssomeconcepts
(the 1st row) or split a single concept (the 2nd row). In contrast, our method
effectively locates complete and intact concepts, automatically determining a
reasonable concept number.
Conceptlocalizationbenchmark Toquantitativelyevaluatetheconceptlo-
calization performance, we establish a benchmark by (1) building a test dataset
of multi-concept images along with ground-truth concept masks, and (2) devis-
ing tailored metrics for concept localization. The test dataset is sourced from
CLEVR[27],asyntheticdatasetfeaturingcleanbackgroundsandclear,distinct
objects.Inthisdataset,eachobjectexplicitlyrepresentsaconcept,therebyelim-
inatingpotentialdiscrepanciesinhuman-definedconceptsinnaturalimages.By
comparing the predicted masks with the ground-truth masks in the test set us-
ingtheHungarianalgorithm[34],wecanevaluatethreemetrics:(1)Intersection
over Union (IoU) that assesses segmentation accuracy, (2) Recall that evalu-
ates the proportion of true concepts the model can discover, and (3) Precision
that evaluates the correctness of the discovered concepts. We provide additional
details of this evaluation benchmark for concept localization in Appendix B.
Quantitative evaluation Based on the established benchmark, we evaluate
our method compared to k-means and FINCH with various predefined cluster
numbers in Tab. 3. We also report the results of a training-free segmentation
method MaskCut (MC) introduced in CutLER [71] for reference, which per-
forms comparably to our method. When using k-means and FINCH, the prede-
finedclusternumbercansignificantlyimpactperformance,andnofixednumber
can consistently achieve desired performance across all metrics. In contrast, our
methodperformswellintermsofIoUandPrecision,withaslighttrade-offinRe-
call.Onepossiblereasonisthat,unlikespecifyingtheclusternumber,ourmodel
automatically determines it. Therefore, there may be cases where two concepts
aremergedintoone,resultinginoneconceptbeingunabletobematchedtothe
ground truth, potentially reducing recall. Nevertheless, as the only method ca-
pableofautomaticallydeterminingthenumberofconcepts,ourmethodachieves
the best overall performance compared to all other clustering techniques.
4.6 Unsupervised vs. Supervised
Although ConceptExpress is an unsupervised model, it would be intriguing to
compare ConceptExpress to some supervised methods. Motivated by this, we14 S. Hao et al.
Unsupervised Adding supervision
1
1
2 2
Source image
3
[Vi] [Vi] in the jungle [Vi] in the snow [Vi] with a sunset
Source image
4
Ours BaS‚Ä† + Initializer+ GT mask + Both [V1] wan hd e a[ tV f2i] e w ldith a amo[V n1g] sa kn yd s c[ rV a2p] ers in [ aV m1] o a vn ied t[ hV e2a] ter [V1o] na n tod p [ V o2f ] w fl ao ta et ring c[ oV b1b] la en sd to [ nV e2 ] s to rn e ea t
Fig.8: Comparison with supervised Fig.9:Text-promptedgeneration.We
methods. We compare ConceptExpress show the generation results prompted by
and BaS‚Ä† to the supervised methods of various text contexts using a single con-
Break-A-Scene [2] with added initializers, ceptual token (top) and multiple concep-
ground truth masks, and both of them. tual tokens (bottom).
experiment by providing initial words and ground-truth object masks (obtained
by SAM [33]) for the supervised method Break-A-Scene [2]. We compare our
method, BaS‚Ä†, and three supervised methods by adding different supervision to
Break-A-Scene, as shown in Fig. 8. We can observe that adding initial words
guides the generation towards the specified category, while adding ground-truth
object masks enhances the preservation of texture details. However, even with
these two settings, the generated results of Break-A-Scene still fall short com-
pared to our unsupervised model. Despite being trained in a completely unsu-
pervised manner, our model performs on par with the fully supervised setting,
where both types of supervision are used. This result further highlights the ef-
fectiveness of our model in addressing the concept extraction problem.
4.7 Text-prompted Generation
With the extracted generative concepts, we can perform text-prompted gener-
ation. In Fig. 9, we showcase the results conditioned on various text prompts
using both individual concepts and compositional concepts. The results demon-
strate that the learned conceptual tokens can generate images with high text
fidelity, aligning faithfully with the text prompt. Furthermore, the images gen-
erated with theconceptual tokens also preserve consistent concept identitywith
thesourceconceptsinbothindividualandcompositionalgeneration.Pleaserefer
toAppendixFforadditionalphotorealisticresultsoftext-promptedgeneration.
5 Conclusion
In this paper, we introduce Unsupervised Concept Extraction (UCE) that aims
to leverage diffusion models to learn individual concepts from a single image in
an unsupervised manner. We present ConceptExpress to tackle the UCE prob-
lem by harnessing the capabilities of pretrained diffusion models to locate con-
cepts and learn their corresponding conceptual tokens. Moreover, we establish
an evaluation protocol for the UCE problem. Extensive experiments highlight
ConceptExpress as a promising solution to the UCE task.ConceptExpress 15
Acknowledgement This work is partially supported by the Hong Kong Re-
search Grants Council - General Research Fund (Grant No.: 17211024).
References
1. Abdal,R.,Zhu,P.,Femiani,J.,Mitra,N.,Wonka,P.:CLIP2StyleGAN:Unsuper-
vised extraction of stylegan edit directions. In: ACM SIGGRAPH (2022)
2. Avrahami,O.,Aberman,K.,Fried,O.,Cohen-Or,D.,Lischinski,D.:Break-a-scene:
Extracting multiple concepts from a single image. In: SIGGRAPH Asia (2023)
3. Avrahami, O., Fried, O., Lischinski, D.: Blended latent diffusion. In: ACM SIG-
GRAPH (2023)
4. Avrahami,O.,Hayes,T.,Gafni,O.,Gupta,S.,Taigman,Y.,Parikh,D.,Lischinski,
D.,Fried,O.,Yin,X.:Spatext:Spatio-textualrepresentationforcontrollableimage
generation. In: CVPR (2023)
5. Avrahami, O., Lischinski, D., Fried, O.: Blended diffusion for text-driven editing
of natural images. In: CVPR (2022)
6. Bar-Tal,O.,Yariv,L.,Lipman,Y.,Dekel,T.:Multidiffusion:Fusingdiffusionpaths
for controlled image generation. In: ICML (2023)
7. Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V., Babenko, A.: Label-
efficient semantic segmentation with diffusion models. In: ICLR (2022)
8. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high fidelity
natural image synthesis. In: ICLR (2019)
9. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023)
10. Caron, M., Touvron, H., Misra, I., J√©gou, H., Mairal, J., Bojanowski, P., Joulin,
A.: Emerging properties in self-supervised vision transformers. In: ICCV (2021)
11. Chefer,H.,Lang,O.,Geva,M.,Polosukhin,V.,Shocher,A.,Irani,M.,Mosseri,I.,
Wolf,L.:Thehiddenlanguageofdiffusionmodels.arXivpreprintarXiv:2306.00966
(2023)
12. Chen, W., Hu, H., Li, Y., Rui, N., Jia, X., Chang, M.W., Cohen, W.W.: Subject-
driven text-to-image generation via apprenticeship learning. In: NeurIPS (2023)
13. Couairon,G.,Verbeek,J.,Schwenk,H.,Cord,M.:Diffedit:Diffusion-basedseman-
tic image editing with mask guidance. In: ICLR (2022)
14. Crowson, K., Biderman, S., Kornis, D., Stander, D., Hallahan, E., Castricato, L.,
Raff,E.:VQGAN-CLIP:Opendomainimagegenerationandeditingwithnatural
language guidance. In: ECCV (2022)
15. Du, Y., Li, S., Mordatch, I.: Compositional visual generation with energy based
models. In: NeurIPS (2020)
16. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G.,
Cohen-Or, D.: An image is worth one word: Personalizing text-to-image gener-
ation using textual inversion. In: ICLR (2023)
17. Gal, R., Arar, M., Atzmon, Y., Bermano, A.H., Chechik, G., Cohen-Or, D.:
Encoder-based domain tuning for fast personalization of text-to-image models.
arXiv preprint arXiv:2302.12228 (2023)
18. Gal, R., Patashnik, O., Maron, H., Bermano, A.H., Chechik, G., Cohen-Or, D.:
Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Trans-
actions on Graphics (TOG) (2022)
19. Gandikota, R., Materzynska, J., Fiotto-Kaufman, J., Bau, D.: Erasing concepts
from diffusion models. In: ICCV (2023)16 S. Hao et al.
20. Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,
Courville, A., Bengio, Y.: Generative adversarial nets. In: NeurIPS (2014)
21. Hao,S.,Han,K.,Zhao,S.,Wong,K.Y.K.:Vico:Detail-preservingvisualcondition
forpersonalizedtext-to-imagegeneration.arXivpreprintarXiv:2306.00971(2023)
22. Hedlin, E., Sharma, G., Mahajan, S., Isack, H., Kar, A., Tagliasacchi, A., Yi,
K.M.:Unsupervisedsemanticcorrespondenceusingstablediffusion.arXivpreprint
arXiv:2305.15581 (2023)
23. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020)
24. Ho, J., Salimans, T., Gritsenko, A.A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. In: NeurIPS (2022)
25. Jia,X.,Zhao,Y.,Chan,K.C.,Li,Y.,Zhang,H.,Gong,B.,Hou,T.,Wang,H.,Su,
Y.C.:Tamingencoderforzerofine-tuningimagecustomizationwithtext-to-image
diffusion models. arXiv preprint arXiv:2304.02642 (2023)
26. Jin, C., Tanno, R., Saseendran, A., Diethe, T., Teare, P.: An image is worth mul-
tiple words: Learning object level concepts using multi-concept prompt learning.
arXiv preprint arXiv:2310.12274 (2023)
27. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick,
C., Girshick, R.: CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning. In: CVPR (2017)
28. Karazija, L., Laina, I., Vedaldi, A., Rupprecht, C.: Diffusion models for zero-shot
open-vocabulary segmentation. arXiv preprint arXiv:2306.09316 (2023)
29. Karras, T., Aittala, M., Laine, S., H√§rk√∂nen, E., Hellsten, J., Lehtinen, J., Aila,
T.: Alias-free generative adversarial networks. In: NeurIPS (2021)
30. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: CVPR (2019)
31. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing
and improving the image quality of stylegan. In: CVPR (2020)
32. Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., Irani,
M.:Imagic:Text-basedrealimageeditingwithdiffusionmodels.In:CVPR(2023)
33. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
34. Kuhn, H.W.: The hungarian method for the assignment problem. Naval research
logistics quarterly (1955)
35. Kumari,N.,Zhang,B.,Wang,S.Y.,Shechtman,E.,Zhang,R.,Zhu,J.Y.:Ablating
concepts in text-to-image diffusion models. In: ICCV (2023)
36. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept cus-
tomization of text-to-image diffusion. In: CVPR (2023)
37. Li,A.C.,Prabhudesai,M.,Duggal,S.,Brown,E.,Pathak,D.:Yourdiffusionmodel
is secretly a zero-shot classifier. In: ICCV (2023)
38. Li,D.,Li,J.,Hoi,S.C.: Blip-diffusion:Pre-trainedsubjectrepresentationforcon-
trollable text-to-image generation and editing. In: NeurIPS (2023)
39. Li, X., Lu, J., Han, K., Prisacariu, V.: Sd4match: Learning to prompt stable dif-
fusion model for semantic matching. arXiv preprint arXiv:2310.17569 (2023)
40. Liu,N.,Du,Y.,Li,S.,Tenenbaum,J.B.,Torralba,A.:Unsupervisedcompositional
concepts discovery with text-to-image generative models. In: ICCV (2023)
41. Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., Van Gool, L.: Re-
paint: Inpainting using denoising diffusion probabilistic models. In: CVPR (2022)ConceptExpress 17
42. Ma, Y., Yang, H., Wang, W., Fu, J., Liu, J.: Unified multi-modal latent dif-
fusion for joint subject and text conditional image generation. arXiv preprint
arXiv:2303.09319 (2023)
43. Molad,E.,Horwitz,E.,Valevski,D.,Acha,A.R.,Matias,Y.,Pritch,Y.,Leviathan,
Y., Hoshen, Y.: Dreamix: Video diffusion models are general video editors. arXiv
preprint arXiv:2302.01329 (2023)
44. Ni, M., Zhang, Y., Feng, K., Li, X., Guo, Y., Zuo, W.: Ref-diff: Zero-shot refer-
ringimagesegmentationwithgenerativemodels.arXivpreprintarXiv:2308.16777
(2023)
45. Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models. In: ICML (2022)
46. Patashnik, O., Garibi, D., Azuri, I., Averbuch-Elor, H., Cohen-Or, D.: Localizing
object-levelshapevariationswithtext-to-imagediffusionmodels.In:ICCV(2023)
47. Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: StyleCLIP:
Text-driven manipulation of stylegan imagery. In: ICCV (2021)
48. Qiu, Z., Liu, W., Feng, H., Xue, Y., Feng, Y., Liu, Z., Zhang, D., Weller, A.,
Sch√∂lkopf, B.: Controlling text-to-image diffusion by orthogonal finetuning. In:
NeurIPS (2023)
49. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021)
50. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125
(2022)
51. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,
Sutskever, I.: Zero-shot text-to-image generation. In: ICML (2021)
52. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative
adversarial text to image synthesis. In: ICML (2016)
53. Richardson,E.,Goldberg,K.,Alaluf,Y.,Cohen-Or,D.:Conceptlab:Creativegen-
eration using diffusion prior constraints. arXiv preprint arXiv:2308.02669 (2023)
54. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
55. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation.
In: CVPR (2023)
56. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nition challenge. IJCV (2015)
57. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-
to-image diffusion models with deep language understanding. In: NeurIPS (2022)
58. Sarfraz, S., Sharma, V., Stiefelhagen, R.: Efficient parameter-free clustering using
first neighbor relations. In: CVPR (2019)
59. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti,
M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open
large-scale dataset for training next generation image-text models. In: NeurIPS
(2022)
60. Shi, J., Xiong, W., Lin, Z., Jung, H.J.: Instantbooth: Personalized text-to-image
generation without test-time finetuning. arXiv preprint arXiv:2304.03411 (2023)18 S. Hao et al.
61. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H.,
Ashual,O.,Gafni,O.,etal.:Make-a-video:Text-to-videogenerationwithouttext-
video data. In: ICLR (2022)
62. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:ICLR(2021)
63. Tang,R.,Pandey,A.,Jiang,Z.,Yang,G.,Kumar,K.,Lin,J.,Ture,F.:Whatthe
daam: Interpreting stable diffusion using cross attention. In: ACL (2023)
64. Tao,M.,Tang,H.,Wu,F.,Jing,X.Y.,Bao,B.K.,Xu,C.:DF-GAN:Asimpleand
effective baseline for text-to-image synthesis. In: CVPR (2022)
65. Tewel,Y.,Gal,R.,Chechik,G.,Atzmon,Y.:Key-lockedrankoneeditingfortext-
to-image personalization. In: ACM SIGGRAPH (2023)
66. Tian,J.,Aggarwal,L.,Colaco,A.,Kira,Z.,Gonzalez-Franco,M.:Diffuse,attend,
and segment: Unsupervised zero-shot segmentation using stable diffusion. arXiv
preprint arXiv:2308.12469 (2023)
67. Tumanyan, N., Geyer, M., Bagon, S., Dekel, T.: Plug-and-play diffusion features
for text-driven image-to-image translation. In: CVPR (2023)
68. Vinker, Y., Voynov, A., Cohen-Or, D., Shamir, A.: Concept decomposition for
visual exploration and inspiration. arXiv preprint arXiv:2305.18203 (2023)
69. Wang, J., Li, X., Zhang, J., Xu, Q., Zhou, Q., Yu, Q., Sheng, L., Xu, D.: Diffu-
sion model is secretly a training-free open vocabulary semantic segmenter. arXiv
preprint arXiv:2309.02773 (2023)
70. Wang, S., Saharia, C., Montgomery, C., Pont-Tuset, J., Noy, S., Pellegrini, S.,
Onoe, Y., Laszlo, S., Fleet, D.J., Soricut, R., et al.: Imagen editor and editbench:
Advancing and evaluating text-guided image inpainting. In: CVPR (2023)
71. Wang, X., Girdhar, R., Yu, S.X., Misra, I.: Cut and learn for unsupervised object
detection and instance segmentation. In: CVPR (2023)
72. Wang,Z.,Gui,L.,Negrea,J.,Veitch,V.:Conceptalgebrafortext-controlledvision
models. arXiv preprint arXiv:2302.03693 (2023)
73. Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual
concepts into textual embeddings for customized text-to-image generation. arXiv
preprint arXiv:2302.13848 (2023)
74. Wu, J.Z., Ge, Y., Wang, X., Lei, W., Gu, Y., Hsu, W., Shan, Y., Qie, X., Shou,
M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-to-video
generation. arXiv preprint arXiv:2212.11565 (2022)
75. Xia, W., Yang, Y., Xue, J.H., Wu, B.: Tedigan: Text-guided diverse face image
generation and manipulation. In: CVPR (2021)
76. Xiao, C., Yang, Q., Zhou, F., Zhang, C.: From text to mask: Localizing en-
tities using the attention of text-to-image diffusion models. arXiv preprint
arXiv:2309.04109 (2023)
77. Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan:
Fine-grainedtexttoimagegenerationwithattentionalgenerativeadversarialnet-
works. In: Proceedings of the IEEE conference on computer vision and pattern
recognition (2018)
78. Ye, H., Yang, X., Takac, M., Sunderraman, R., Ji, S.: Improving text-to-image
synthesis using contrastive learning. arXiv preprint arXiv:2107.02423 (2021)
79. Yu, J., Xu, Y., Koh, J.Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,
A., Yang, Y., Ayan, B.K., et al.: Scaling autoregressive models for content-rich
text-to-image generation. Transactions on Machine Learning Research (2022)
80. Zhang, H., Koh, J.Y., Baldridge, J., Lee, H., Yang, Y.: Cross-modal contrastive
learningfortext-to-imagegeneration.In:ProceedingsoftheIEEE/CVFconference
on computer vision and pattern recognition (2021)ConceptExpress 19
81. Zhang,J.,Herrmann,C.,Hur,J.,Cabrera,L.P.,Jampani,V.,Sun,D.,Yang,M.H.:
A tale of two features: Stable diffusion complements dino for zero-shot semantic
correspondence. In: NeurIPS (2023)
82. Zhang, L., Agrawala, M.: Adding conditional control to text-to-image diffusion
models. arXiv preprint arXiv:2302.05543 (2023)
83. Zhang, Y., Wei, Y., Jiang, D., ZHANG, X., Zuo, W., Tian, Q.: Controlvideo:
Training-free controllable text-to-video generation. In: ICLR (2024)
84. Zhang, Y., Yang, M., Zhou, Q., Wang, Z.: Attention calibration for disentangled
text-to-image personalization. In: CVPR (2024)
85. Zhao, S., Chen, D., Chen, Y.C., Bao, J., Hao, S., Yuan, L., Wong, K.Y.K.: Uni-
controlnet:All-in-onecontroltotext-to-imagediffusionmodels.In:NeurIPS(2023)
86. Zhu, M., Pan, P., Chen, W., Yang, Y.: DM-GAN: Dynamic memory generative
adversarial networks for text-to-image synthesis. In: CVPR (2019)ConceptExpress 1
A More Details on Implementation
Attention aggregation Theself-attentionmapsindifferentlayershavevary-
ing resolutions. To aggregate them into a single map for further processing, we
follow the approach used in [66]. Each self-attention map A [I,J,:,:] represents
l
the correlation between the location (I,J) and all spatial locations. As a result,
the last two dimensions of the self-attention maps have spatial consistency, and
weinterpolatethemtoensureuniformity.Ontheotherhand,thefirsttwodimen-
sions of the self-attention maps indicate the locations to which attention maps
refer. Therefore, we duplicate these dimensions. By interpolation and duplica-
tion,wealigntheself-attentionmapsinalllayerstoacommonlatentresolution
(i.e., 64√ó64). Finally, we compute the average of all attention maps to obtain
theaggregatedmap.Thisaggregationstepallowsustocreateaunifiedattention
map combining all maps from different layers.
Conceptualtokenlearning Weutilizethesplit-and-mergestrategyintrain-
ingconceptualtokens.Inthetrainingaftersplitting,wenotonlysampleprompts
ofindividualtokensbutalsosampleacompositionalprompt‚Äúaphotoof[V ]and
1
[V ] ... [V ]‚Äù for training. This approach enhances the compositionality of the
2 N
learnabletokens.However,unlike[2],werefrainfromusingallpossiblecomposi-
tions and instead only use the full composition. This decision is made to ensure
that the proportionof using single tokens remains high during training. This, in
turn, facilitates effective learning of each individual conceptual token.
Earth mover‚Äôs distance (EMD) Wepenalizeattentionalignmentusingthe
location-awareEMD.TheEMDisformulatedasanoptimaltransportationprob-
lem.Supposewehavesuppliesofn sourcesS ={s }ns anddemandsofn des-
s i i=1 d
tinations D = {d }nd . Given the moving cost from the i-th source to the j-th
j j=1
destinationc ,anoptimaltransportationproblemaimstofindtheminimal-cost
ij
flow f from sources to destinations:
ij
minimize
(cid:88)ns (cid:88)nd
c f (14)
ij ij
fij i=1 j=1
subject to f ‚©æ0, i=1,...,n , j =1,...,n (15)
ij s d
(cid:88)nd
f =s , i=1,...,n (16)
ij i s
j=1
(cid:88)ns
f =d , j =1,...,n (17)
ij j d
i=1
where the optimal flow fÀú is computed by the moving cost c , the supplies
ij ij
s , and the demands d . The EMD can be further formulated as (1‚àíc )fÀú .
i j ij ij
In our problem, the cross-attention map c represents the supply, while the
[Vi]j
target mean attention f represents the demand. The moving cost is calculated
i
as the Euclidean distance between spatial locations. Unlike the MSE, the EMD
considers differences not only between elements at the same location but also
between elements at different locations. This means that the EMD takes into
account both spatial alignment and the magnitude of differences, providing a
more comprehensive measure of dissimilarity.2 S. Hao et al.
Source image Extracted concepts GT concepts
IoU2 IoU3=0
IoU
yellow metal cylinder gray metal cube green metal cylinder 1
Discovered concepts
blue metal cube purple metal sphere cyan rubber cube
ùëã+ùëå
ùëπùíÜùíÑùíÇùíçùíç=
ùëã+ùëå+ùëç
ùëã+ùëå
ùë∑ùíìùíÜùíÑùíäùíîùíäùíêùíè=
ùëã+ùëå+ùëç
ùë∞ùíêùëº=avg(ùêºùëúùëà!,ùêºùëúùëà",ùêºùëúùëà#)
purple rubber cube yellow rubber cylinder green metal cylinder
Fig.10: Concept localization benchmark.Left:Wepresentsomeexamplesofthe
sourceimagesinthedatasetandtheircorrespondingconcept-wiseimagesgeneratedby
ConceptExpress.Weprovidethevisualpropertiesofeachdistinctconceptunderneath
thegeneratedimage.Right:Wevisualizethematchingprocessbetweenground-truth
(GT)conceptsanddiscoveredconcepts.Here,  and  denotethetruepositivematches,
denotes the wrongly discovered concept, and ? denotes the missed true concept.
B Concept Localization Benchmark
In the main paper, we present a novel benchmark to evaluate concept local-
ization performance. This benchmark effectively assesses our model‚Äôs concept
localization capability in two aspects: (1) concept discovery accuracy and (2)
concept segmentation efficacy. Here, we offer further details on the benchmark,
including the dataset and evaluation metrics.
Datasetcuration Toassessconceptlocalization,thebenchmarkdatasetmust
meet two criteria: (1) clear and distinct concept definition, and (2) accurate
ground-truth concept masks. Natural images lack these characteristics. Instead,
we source our images from CLEVR [27], a dataset known for its well-defined
objects,diverseincolors,materials,andshapes,setagainstauniformgreyback-
ground. We collect a total of 25 images, each containing 3 to 5 concepts, along
withtheircorrespondingground-truthsegmentationmasks.Weshowimagesam-
ples from the dataset alongside the generated images of our extracted concepts
in Fig. 10 (left).
Evaluationmetrics Wefurtherintroduceevaluationmetricstailoredforcon-
cept localization. The process of concept localization incorporates two parts:
(1) concept discovery and (2) concept segmentation. For these two parts, we
devise three metrics: recall and precision to assess concept discovery, and av-
erageintersectionoverunion(IoU)toassessconceptsegmentation.Specifically,
let P = {m }N denote the set of the N concept segments discovered by the
i i=1
model, and let Q={¬µ }M denote the set of the M ground-truth concept seg-
j j=1ConceptExpress 3
ments.Tomatchthediscoveredconceptsandtheground-truthconcepts,weaim
to maximize their average inter-instance IoU, which is given by
M‚Ä≤
1 (cid:88) (cid:16) (cid:17)
Avg. IoU= max IoU ¬µ ,m (18)
Œª Œª1 2‚àà ‚ààŒõ Œõ( (M N,, MM ‚Ä≤‚Ä≤ )) N i=1 Œª1(i) Œª2(i)
where M‚Ä≤ = min(M,N) and Œõ(M,M‚Ä≤) denotes the set of all M‚Ä≤-permutations
of integers ranging from 1 to M. Similarly, Œõ(N,M‚Ä≤) represents the set of all
M‚Ä≤-permutations of integers ranging from 1 to N. We use Hungarian optimal
assignmentalgorithm[34]tosolvethemaximizationproblemofEq.(18)overthe
set of permutations. The maximum value of the average IoU reflects the overall
segmentation proficiency of our localized concepts. The permutations Œª and
1
Œª together give the matching correspondence between ground-truth concepts
2
and discovered concepts. We further examine the count of true positive concept
matches that yield non-zero IoUs by
M‚Ä≤
(cid:88) (cid:110) (cid:16) (cid:17) (cid:111)
R= 1 IoU ¬µ ,m Ã∏=0 (19)
Œª1(i) Œª2(i)
i=1
Therefore, we can compute recall and precision by
R R
recall= precision= (20)
M N
Withrecallandprecision,wecanevaluateconceptdiscoveryperformancebased
on whether there are missed true concepts or wrongly discovered concepts. The
computation of all three evaluation metrics is visualized in Fig. 10 (right).
C User Study
Toensuretheassessmentofgenerationqualityalignswithhumanpreference,we
conduct a user study comparing the generated results from BaS‚Ä† and Concept-
Express. We asked 14 users to vote between our method and BaS‚Ä† by viewing
the generated images of 19 concepts from 7 images in D . For each concept,
2
we presented the users with 8 images, randomly generated by ConceptExpress
andBaS‚Ä† respectively,alongwiththemaskedimageofthesourceconcept.They
werethenaskedtoindicatewhichmodelproducedimagesthatbetterresembled
the source concept. Finally, we collected a total of 266 user votes, representing
human preference. Among all the votes, 18.8% of the votes favored BaS‚Ä† while
81.2% preferred our model. Detailed statistics of the votes for each concept are
present in Fig. 11. The user study further indicates that ConceptExpress out-
performs BaS‚Ä† in generating concept images that align with human judgment.4 S. Hao et al.
15.0
BaS
12.5
Ours
10.0
7.5
5.0
2.5
0.0 I1-C1 I1-C2 I1-C3 I1-C4 I2-C1 I3-C1 I3-C2 I4-C1 I4-C2 I5-C1 I5-C2 I6-C1 I6-C2 I6-C3 I6-C4 I7-C1 I7-C2 I7-C3 I7-C4
Concepts
Fig.11: User study statistics.Wereportthehumanvotesofatotalof19concepts
extracted from 7 source images, comparing BaS‚Ä† and our method. On the horizontal
axis,In-Cmdenotesthem-thconceptinthen-thimage,whiletheverticalaxisdisplays
the number of human votes for each method, represented by different colors.
D Additional Ablation Studies
D.1 Self-attention Clustering
In Fig. 12, we present additional self-attention clustering results, comparing our
approach with k-means and FINCH [58]. Upon observation, we note that k-
means and FINCH may separate a single concept (as seen in the 1st, 2nd, and
3rd rows) or include background regions (as seen in the 4th row) within their
clusters. In contrast, our approach consistently demonstrates high accuracy in
locating each concept, ensuring precise concept localization within the image.
D.2 Split-and-merge Strategy
Visualization InFig.13,weprovidetwoadditionalexamplesillustratinghow
the split-and-merge strategy rectifies the token learning process. The split-and-
mergestrategyexpandsthesearchspaceforconceptsduringthesplittingphase,
allowing the merged token to exhibit concept characteristics that closely align
withthesourceconcept.Thisimprovestheabilitytolearnandrepresentunseen
concepts without initialization, ultimately enhancing image generation quality.
Effect of contrastive loss We further investigate the impact of the con-
trastive loss during the splitting phase. The contrastive loss encourages split
tokens representing the same concept to be closer together, promoting better
conceptagreementacrossalltokens.InFig.14,weemployPCAtovisualizethe
embedding vectors with and without the contrastive loss. When the contrastive
lossisused,theembeddingvectorsrepresentingthesameconceptexhibitamore
compactdistribution,aligningwithourgoalofenhancingconceptrepresentation.
WealsoreportthequantitativecomparisoninTab.4.Theuseofcontrastiveloss
canenhancetheperformanceonallmetrics,especiallyonclassificationaccuracy.
Effectoftokenmerging Tovalidatethenecessityofmergingmultipletokens
midwaythroughoptimization,weadditionallyevaluateavariantthatoptimizes
themultiplerandomlyinitializedtokensseparatelyandmergesthemattheend.
We compare the results in Tab. 5. Our method significantly outperforms the
variant, especially in compositional similarity and classification accuracy.
setoV
namuHConceptExpress 5
Source image ùêæ-means FINCH Ours
Fig.12: Conceptlocalizationresultsusingdifferentmethodsforself-attentioncluster-
ing.
Effect of the number of split tokens To observe the impact of the num-
ber of split tokens, i.e., the value of g, on the training process, we evaluate the
performance of our model using different numbers of split tokens. The results
are reported in Tab. 6. From the table, we can make the following observations:
(1)Usingonlyasingletoken(g=1),i.e.,excludingthesplit-and-mergestrategy,
yields poor performance, again demonstrating the significance of the split-and-
mergestrategy;(2)g=3performscomparablytooursetting(g=5),withaslight
improvement in identity similarity and a moderate drop in classification accu-
racy; (3) A larger number (e.g., g=7) may decrease the performance across all
metrics to some extent. In the main paper, we set g=5 to balance all metrics.
This experiment underscores the significance of our split-and-merge strategy for
robust token initialization, which can greatly impact performance.
D.3 Attention Alignment
We compare training with EMD attention alignment (ours) to training with
MSE attention alignment and training without attention alignment, as present
in Fig. 15. From the observation, we can see that when attention alignment is
not used, some concepts in the compositional generated image may be missed.6 S. Hao et al.
w/o SnM
g=5
w/ SnM merge
Source image (ours)
Training step
0-100 (split) 100 (merge) 200 500 (end)
w/o SnM
g=5
w/ SnM merge
Source image (ours)
Training step
0-100 (split) 100 (merge) 200 500 (end)
Fig.13: Generation results of split-and-merge ablation.
Table 4: Contrastive loss ablation on D using DINO.
1
Method SIMI SIMC ACC1 ACC3
w/o contrast. 0.316 0.567 0.320 0.456
w/ contrast. (ours) 0.319 0.568 0.324 0.470
On the other hand, using mask MSE can lead to unsatisfactory single-concept
generations. In contrast, our EMD attention alignment strikes a balance be-
tween these two extremes and performs well in both aspects. It ensures that
the model captures and represents all the desired concepts in the compositional
image while also generating high-quality single-concept images. Our attention
alignment approach achieves a favorable trade-off.
E Additional Quantitative Analysis
E.1 Unsupervised vs. Supervised
We present quantitative results for unsupervised methods, namely ConceptEx-
pressandBaS‚Ä†,aswellasmethodsaugmentedwithdifferenttypesofsupervision.
ThecomparisonresultsarepresentedinTab.7.Bycomparing(5)to(1)-(3),weConceptExpress 7
w/o contrastive loss w/ contrastive loss
Fig.14: Visualization of token embedding vectors. We randomly initialize 5
embedding vectors marked with the same color to represent each of the 5 concepts,
resulting in a total of 25 embedding vectors. After the splitting phase, specifically at
step 100, we use PCA to visualize the learned embedding vectors. We compare the
results with and without the contrastive loss.
Table 5: We present the results of com- Table 6: Resultsvarywithdifferentnum-
parison with the variant (Var) that opti- bersofsplittokens.Usingonlyasingleto-
mizes multiple split tokens separately and ken(g=1)yieldspoorperformance.Exper-
merges them at the end. Experiments are iments are conducted on D using DINO.
2
conducted on D using DINO.
2
#tokens SIMI SIMC ACC1 ACC3
SIMI SIMC ACC1 ACC3 g=1 0.319 0.486 0.651 0.789
Var 0.369 0.400 0.730 0.868 g=3 0.381 0.535 0.750 0.914
Ours 0.371 0.535 0.803 0.934 g=5(ours) 0.371 0.535 0.803 0.934
g=7 0.367 0.525 0.743 0.882
can observe that unsupervised ConceptExpress outperforms supervised versions
of BaS‚Ä†. We provide supervision of ground-truth SAM masks for our model in
(6)whichslightlyimprovesSIMI,SIMC,andACC3 whileloweringACC1.This
result indicates that our identified masks exhibit performance characteristics
largelycomparabletoSAMmasks.Wefurtherfinetunethefullysupervisedver-
sion of BaS‚Ä† in (4), which is the original implementation of Break-A-Scene [2].
We also finetune our unsupervised model in (7). The results clearly show that,
withfinetuning,ourunsupervisedmodelsignificantlyoutperformstheoriginally
implemented BaS‚Ä† that is fully supervised by masks and initial words.
E.2 Text Guidance
We also explore the performance of subject-driven text-to-image generation. To
do this, we utilize a set of prompts to generate text-conditioned images with all
extracted concepts and their compositions. We expand the set of prompts used
in [2] from 10 to 15 in Tab. 8. We evaluate the generated images by measuring
theirCLIPimagesimilaritywiththemaskedsourceimage,aswellastheirCLIP
textsimilaritywiththecorrespondingtextpromptinTab.8(withthelearnable
token removed).8 S. Hao et al.
w/o attention alignment w/ mask MSE w/ attention EMD (ours)
1 2
3
1
+
Source image 2
+
3
Fig.15: Individual and compositional generation results of attention comparison.
Table 7: Unsupervised vs. supervised. We compare unsupervised settings, i.e.,
ConceptExpress and BaS‚Ä†, and supervised settings by adding different supervision
to Bas‚Ä†, including the ground-truth SAM masks (+Mask) and the human-annotated
initialwords(+Init.).Weadditionallyreporttheresultsoffinetuning(+FT)thewhole
diffusionmodelforreference.Row(4)representstheoriginalimplementationofBreak-
A-Scene [2]. Experiments are conducted on D using DINO.
2
Method +Mask +Init. +FT SIMI SIMC ACC1 ACC3
(1) 0.231 0.417 0.329 0.559
(2) ‚úì 0.266 0.430 0.388 0.618
BaS‚Ä†
(3) ‚úì ‚úì 0.316 0.474 0.559 0.704
(4) ‚úì ‚úì ‚úì 0.411 0.696 0.697 0.737
(5) 0.371 0.535 0.803 0.934
(6) Ours ‚úì 0.396 0.564 0.757 0.974
(7) ‚úì 0.490 0.785 0.888 0.980
InFig.16,wepresenttheaverageimageandtextsimilaritiesforallcompared
methods. We observe that as supervision is gradually added to BaS‚Ä†, the im-
agesimilarityincreaseswhilethetextsimilaritydecreases.Thisisexpectedsince
theunsupervisedBaS‚Ä† maystruggletolearncertainconceptsrepresentedbythe
learnable tokens, resulting in the text prompt dominating the text embedding
spaceandexcessivelyguidingtheimagegenerationprocess.However,assupervi-
sionisintroduced,BaS‚Ä† canbetterlearntheconceptualtokensandprioritizethe
subjectconceptinthegeneratedimage,therebyreducingtherelianceontextin-
formation. The lower text similarity indicates the text information becomes less
pronounced rather than completely absent. ConceptExpress also exhibits high
image similarity and slightly lower text similarity. Notably, it performs closely
to the fully supervised BaS‚Ä†, indicating that ConceptExpress effectively learns
reliable generative conceptual tokens for subject-driven image generation.ConceptExpress 9
Table 8: Text prompt set. ‚Äú{}‚Äù represents the conceptual token.
Prompts
aphotoof{}inthejungle
aphotoof{}inthesnow
aphotoof{}atthebeach
aphotoof{}ontopofpinkfabric
aphotoof{}ontopofawoodenfloor
aphotoof{}withacityinthebackground
aphotoof{}withamountaininthebackground
aphotoof{}floatingontopofwater
aphotoof{}withatreeandautumnleavesinthebackground
aphotoof{}withtheEiffelTowerinthebackground
aphotoof{}ontopofthesidewalkinacrowdedstreet
aphotoof{}withaJapanesemoderncitystreetinthebackground
aphotoof{}ontopofadirtroad
aphotoof{}amongtheskyscrapersinNewYorkCity
aphotoof{}inadreamofadistantgalaxy
Table 9: Classification accuracy under a larger classifier. We use * to denote
theresultsevaluatedwiththelargerclassifierthatcontains1,000additionalprototypes
sampledfromImageNet-1k.ExperimentsareconductedonD evaluatedusingDINO.
2
ACC1 *ACC1 ACC3 *ACC3
BaS‚Ä† 0.329 0.138 0.559 0.203
Ours 0.803 0.395 0.934 0.546
E.3 Larger Classifier
In the evaluation of classification accuracy, we can increase the number of pro-
totypes in the classifier by including a large codebook of concepts besides the
concepts in the datasets. Specifically, we randomly sample one image per class
fromImageNet-1k[56],obtaining1,000imagesintotal.Weencodethemasaddi-
tional concept prototypes in the classifier. We report the results of classification
accuracybyusingthelargerclassifierinTab.9.Wenotethattheaccuracyvalues
under the larger classifier are considerably lower compared to those under the
original classifier for both models. Nevertheless, our model consistently demon-
strates significant superiority over BaS‚Ä† across all evaluation criteria, regardless
of whether the larger or the original classifier is employed.
E.4 Initializer Analysis
In our unsupervised setting, initial words for training each concept are inacces-
siblebecausetheconceptsareautomaticallyextracted,andhumanexamination
ofeachconceptrequirescostlylabor.Weintroducethesplit-and-mergestrategy
to address this problem.
Despite this, we still aim to explore the performance of models trained with
different types of initializers in supervised settings and compare them with10 S. Hao et al.
0.66
Ours BaS‚Ä†+Mask & Init.
0.64 Source image BaS‚Ä†+Init.
BaS‚Ä†+Mask
0.62
0.60 BaS‚Ä†
0.58
0.56
0.54
0.52
0.50
Prompt only
0.20 0.22 0.24 0.26 0.28
Average Text Similarity
‚Üë
Fig.16: Quantitative evaluation of subject-driven text-to-image generation.
The bounding circle size represents the normalized mean value of the main metrics
reported in Tab. 7. We also include evaluation results for two additional scenarios:
(1) Prompt only: we evaluate all metrics when generating images using only the text
prompt; (2) Source image: we evaluate the text similarity between the masked source
images and all prompts.
our unsupervised approach. We consider two types of initializers: (1) human-
annotated initializers and (2) model-annotated initializers. For human annota-
tion,wedirectlyassignasuitablewordtoanextractedconceptbasedonhuman
preference. For model annotation, we use the vision-language model CLIP [49]
to retrieve a word from CLIP text tokenizer‚Äôs vocabulary. This word is selected
based on the highest similarity to the extracted concept, determined by com-
paring features between each word in the vocabulary set and the masked image
part of the concept. We apply these two types of initializers to BaS‚Ä† and our
model, and compare the experimental results in Tab. 10.
We observe the following: (1) Our model consistently outperforms BaS‚Ä†, re-
gardless of the type of initializer; (2) Human annotation generally yields better
performancethanmodelannotationforbothmodels;(3)Althoughourunsuper-
vised approach slightly lags behind our model with the use of human-annotated
initializersintermsofidentitysimilarity,itstilloutperformsallothersupervised
methods in all metrics. These observations demonstrate the effectiveness of the
split-and-merge strategy in resolving the challenge of inaccessible initializers in
unsupervised concept extraction.
F Additional Comparison and Our Results
As a supplement to the main paper, we provide additional comparison results
betweenConceptExpressandBas‚Ä† inFig.19.Furthermore,wepresentabroader
range of generation examples from ConceptExpress in Figs. 20 and 21, showcas-
ytiralimiS
egamI
egarevA
‚ÜëConceptExpress 11
Table 10: Comparison of using different types of initializers. We initialize
conceptualtokensusinghumanannotation(Humaninit.)andmodelannotation(CLIP
init.)byCLIP[49]vocabularyretrievalforBaS‚Ä† andourmodel,incomparisontoour
unsupervised method. Experiments are conducted on D using DINO.
2
SIMI SIMC ACC1 ACC3
BaS‚Ä† w/ Human Init. 0.312 0.506 0.586 0.730
BaS‚Ä† w/ CLIP Init. 0.304 0.470 0.507 0.658
Ours w/ Human Init. 0.390 0.515 0.803 0.914
Ours w/ CLIP Init. 0.330 0.514 0.638 0.862
Ours (unsupervised) 0.371 0.535 0.803 0.934
Point prompt Box prompt
Fig.17: Human interaction with SAM through point or box prompts. By
leveragingSAM,thehuman-desiredentityinthegivenimagecanbeexplicitlyspecified
and the corresponding concept can be effectively extracted and learned by Concept-
Express.
ing individual, compositional, and text-guided generation results. These results
fully demonstrate the effectiveness of ConceptExpress in the UCE problem.
G Human Interaction with SAM
Althoughourtaskdoesnotdemandannotatedconceptmasks,wecanalsoseam-
lesslyincorporateSAM[33]intoourmodeltoenableinteractiveconceptextrac-
tion. We showcase human interaction with SAM through point or box prompts
in Fig. 17. This experiment demonstrates that our model can be seamlessly
integrated with SAM in practice, enabling human interaction in the concept
extraction process through explicit point or box prompts.
MAS
htiw
tcaretnI
stpecnoc
detareneG12 S. Hao et al.
Source image Concept localization Generated images of the specific concept
Case 1
Case 2
Fig.18: Unsatisfactory cases.Case 1:bothinstancesbelongtothesamecategory
(bird). Case 2: the discovered concept has a small occurrence in the image. In the
figure,weonlyvisualizetheconceptofinterest,whiletheotherconceptsarenotplotted.
H Unsatisfactory Cases
In our analysis, we have noticed two limitations of ConceptExpress. The first
limitation is its difficulty in distinguishing instances from the same semantic
category. The second limitation is its struggle to accurately learn concepts for
instanceswitharelativelysmalloccurrence.Wehavediscussedtheselimitations
in Sec. 5 in the main paper.
Tofurtherillustratetheselimitations,weprovideexamplesofunsatisfactory
cases in Fig. 18. The first case arises because similar patches of instances
from the same category (such as bird wings) exhibit close distributions in self-
attentionmaps,leadingtoearlygroupinginthepre-clusteringphase.Asaresult,
we localize the two birds as a single concept, which may affect the number of
instances shown in the generated image. We considered including spatial bias
to address this, but we found it might impede the normal grouping of complete
concepts. Therefore, we opt for our current approach. It is important to note
that this limitation does not significantly impact the overall generation quality.
Ourmodelpreservestheintegrityofcompleteconcepts,ensuringtheyreflectbird
characteristicsratherthangeneratingcreatureswithmultipleheads.Despitethe
segmentationcontainingmultipleinstances,themodelcanstillgenerateasingle
instance,asshowninrow1column3.Thesecondcaseiscausedbythelimited
occurrenceofthetargetconcept.Duetothelowresolution(64√ó64)ofthelatent
space, the small region captures limited information, making it challenging to
trainanaccurateconceptrepresentation.Asaresult,thesmall‚Äúmarblepedestal‚Äù
islearnedthroughthediffusionprocessandistransformedintoarepresentation
resemblinga‚Äúmarblechurch‚Äù.Webelievethatfutureresearchwillaimtoaddress
these limitations.ConceptExpress 13
I Broader Impact
This paper presents a convenient and efficient concept extraction method that
does not require external manual intervention. The extracted concepts can be
used to generate new images. On one hand, this greatly facilitates the effort-
less disentanglement of instances from an image and enables the generation of
personalized images. It even allows for the creation of a vast concept library by
processing a large batch of images, which can be archived for the use of swift
generation.Ontheotherhand,however,thistechnologycanalsobemeticulously
exploitedtohandleandgeneratesensitiveimages,suchasviolent,pornographic,
or privacy-compromising content. Moreover, due to its unsupervised nature, or-
ganizations with massive image datasets can easily perform concept extraction
and build extensive concept libraries, which may include a substantial amount
of harmful or sensitive content. We believe it is crucial to continuously observe
and regulate this technology to ensure its responsible use in the future.
Limitation ConceptExpress has the following limitations that remain to be
addressed in future research. The first limitation is related to processing im-
agescontainingmultipleinstancesfromthesamesemanticcategory,suchastwo
instances of a bird. In this case, self-attention correspondence struggles to dis-
entangle these instances and tends to identify them as a single concept, rather
than recognizing them as separate instances. The second limitation is that cer-
tainconceptswitharelativelysmalloccurrenceintheimagemaybediscovered.
Thiscanresultinpoorconceptlearningduetothelackofsufficientinformation
for reconstructing the concept in the latent space with a resolution of 64√ó64.
The third limitation is that our model requires a certain level of input image
quality. In the future, it can be further enhanced to robustly handle uncurated
natural data, making it more applicable to real-world scenarios.14 S. Hao et al.
1 2 1 2
BaS‚Ä† BaS‚Ä†
Ours Ours
1 2 1 2
BaS‚Ä† BaS‚Ä†
Ours Ours
1 2 3 4 5
BaS‚Ä†
Ours
1 2 3 4
BaS‚Ä†
Ours
Fig.19: Additional results comparing between ConceptExpress and BaS‚Ä†.ConceptExpress 15
[V1] [V1]w ai rt oh u s nu dn f il towers [V1 d] io rn t rt oo ap d of a [V b1 e] aa un td if u[V l 3 su] nw si eth t a [[ VV 31 ] l] i via n in n ad g l [ ruV ox2 ou m] r ia on ud s
[V2] Tow[V e2 r ] inw ti hth e t bh ae c kE gif rf oe ul nd [V2] in the snow [V m1] o b a u an cnd kt a g[ iV rn o2 ui] n n w dthit eh a [ [V V1 3] ] a in nd t h[V e 2 s] n a on wd
[V3] [V3] in the jungle [V3] at the beach [V2 ba] a a c cn i ktd y g [ riV n o u3 t nh] e dw ith o[V n1 a] ca on bd b [ leV s2 t] o a nn ed s t[ rV e3 e] t
[V1] [V1]floa wti an tg e ron top of [V1] bw ai cth k ga r oci ut ny din the [V1]and [V2]
[V2] [V2]wi st uh n a s b eteautiful [V2] on fa t bo rp ic of pink [V f1 ie] la dn id n [ tV h2 e ] bw acit kh g ra o w unh deat
[V1] [V1]floa wti an tg e ron top of [V1]a im n o Nn eg w t h Ye o rs kk y cs itc yrapers [V1]and [V2]
[V2] [V2] in the snow [V l2 ea] vw ei st h in a tt hr ee e b aa cn kd g a rou utu nm dn [V b1 e] aa un td if u[V l 2 su] nw si eth t a
Fig.20: Additional generated results of ConceptExpress (part 1).16 S. Hao et al.
[V1] [V l1 ea] vw ei st h in a tt hr ee e b aa cn kd g a rou utu nm dn [V1]in the jungle [V1]and [V2]
[V2] [V2]on top of a dirt road [V2] in a theater [V1]and [V2]in the snow
[V1] [V1]on a cobblestone street [V1]in the snow [V1]and [V2]in the snow
[V2] [V2] at the beach [V2] wit bh a a c km go rou un nta din in the mou[ nV t1 a] ina n ind t[ hV e2 b] aw ci kt gh r a o und
[V1] [V1] at the beach Tow[V e1 r ] i nw ti hth e t bh ae c kE gif rf oe ul nd [V2]in [V1]
[V2] [V l2 ea] vw ei st h in a tt hr ee e b aa cn kd g a rou utu nm dn [V2] wi st uh n a s b eteautiful [V2]in green [V1]
Fig.21: Additional generated results of ConceptExpress (part 2).