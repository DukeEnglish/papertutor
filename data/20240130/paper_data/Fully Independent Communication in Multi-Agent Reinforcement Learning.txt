Fully Independent Communication in Multi-Agent Reinforcement
Learning
RafaelPina VarunaDeSilva
LoughboroughUniversityLondon LoughboroughUniversityLondon
London,UnitedKingdom London,UnitedKingdom
r.m.pina@lboro.ac.uk v.d.de-silva@lboro.ac.uk
CorentinArtaud XiaolanLiu
LoughboroughUniversityLondon LoughboroughUniversityLondon
London,UnitedKingdom London,UnitedKingdom
c.artaud2@lboro.ac.uk xiaolan.liu@lboro.ac.uk
ABSTRACT knowledgeofcertainoccurrencescanhelptopreventcatastrophic
Multi-AgentReinforcementLearning(MARL)comprisesabroad events.
areaofresearchwithinthefieldofmulti-agentsystems.Several InconventionalMARLapproaches,communicationcanoften
recentworkshavefocusedspecificallyonthestudyofcommunica- be applied by adding an additional network that learns how to
tionapproachesinMARL.Whilemultiplecommunicationmethods producemessagesthatfollowacertaincommunicationprotocol
havebeenproposed,thesemightstillbetoocomplexandnoteasily [7,20,40].Thisnetworkcanbeintegratedintothelearningprocess
transferabletomorepracticalcontexts.Oneofthereasonsforthat oftheagentsandimprovetheperformanceofthestandardMARL
isduetotheuseofthefamousparametersharingtrick.Inthis algorithm.Insimpleterms,eachagentproducesacertainmessage
paper,weinvestigatehowindependentlearnersinMARLthatdo thatrepresentsitsownknowledgeorexperienceatacertainmo-
notshareparameterscancommunicate.Wedemonstratethatthis ment,andthenthismessageisbroadcastedtotheotheragents.
settingmightincurintosomeproblems,towhichweproposea Afteritisdelivered,themessageisusedasanadditionalinputto
newlearningschemeasasolution.Ourresultsshowthat,despite thestandardnetworksoftheagents,meaningthatnowtheyhave
thechallenges,independentagentscanstilllearncommunication someknowledgeaboutsomethingelsethatwassentbytheothers,
strategiesfollowingourmethod.Additionally,weusethismethod althoughitisencoded.
toinvestigatehowcommunicationinMARLisaffectedbydifferent InmostMARLapproaches,itisadoptedaconfigurationwhere
networkcapacities,bothforsharingandnotsharingparameters. theparametersofthelearningnetworksareshared[10,16,27,32,
Weobservethatcommunicationmaynotalwaysbeneededand 37].Thissettingisoftenreferredtosimplyasparametersharing
thatthechosenagentnetworksizesneedtobeconsideredwhen and,asthenamesuggests,approachesthatadoptthisconfiguration
usedtogetherwithcommunicationinordertoachieveefficient useonlyasinglenetwork(ortwo,ifthereisamixernetworkor
learning. acommunicationnetwork,forinstance)thatissharedbyallthe
agentsoftheteam.However,whenwelookatpracticalapplications,
sharingparametersbecomesunrealistic[39].Withinthemultiple
KEYWORDS
proposedcommunicationmethods,whensharingparametersis
Multi-AgentReinforcementLearning,Communication,Indepen-
notfeasibleaquestionnaturallyarises:cancommunicationstillbe
dentLearning,DeepLearning
conductedsuccessfullywhentheagentsdonotshareparameters?
Inthispaper,weinvestigatecommunicationamongindependent
learnersinMARLwhodonotsharetheparametersoftheirnet-
1 INTRODUCTION
worksandconsideragentsthathavedistinctnetworksfortheir
CommunicationinMulti-AgentReinforcementLearning(MARL)
policyandforgeneratingcommunicationmessages.Wedemon-
hasbeenanimportanttopicofresearchinthebroadfieldofMARL
stratethatthiskindofcommunicationcanbechallengingtoachieve
[6,8,15,16,18].Usually,inthestandardapproaches,agentslearn
duetotheparametersofthenetworksnotbeingshared,butitstill
thetasksbyobservingcertainpartsoftheenvironmentaround
ispossiblethankstoanewlearningschemethatweproposeinthis
them,andthenmakeadecisionbasedonwhattheysee.However,
paper.Additionally,inthecourseoftheexperiments,wearguethat
iftheagentsarecarriersofcommunicationcapabilities,theycan
communicationinMARLmightnotalwaysbenecessaryand,ifit
useotherinformationbesidesonlytheirownobservationstomake
isusednaivelywithoutconsideringtheenvironment,itendsup
abetterdecision.Forinstance,theycanreceiveobservationsfrom
bringingoverheadtothelearningprocess,withoutanybenefits.
theotheragents,ortheiractions[6,18].
Tofurtheranalysetheeffectofcommunicationinthelearning
Fromtheperspectiveofpracticalapplications,communicationis
processofMARL,wealsoinvestigatewhethersimplyincreasing
alsoseenasafeasiblewayofimprovinglearningduetoprogresses
thecapacityofthenetworksoftheagentscancompensateforthe
indiversefields[3,9,22].Forexample,inscenariosofautonomous
absenceofacommunicationnetwork.Thisisbecause,byincreasing
vehiclesorfactorieswithmultipleagents,communicationcanbe
thecapacityoftheagentnetwork,theamountofinformationthat
keytolearnbetterresponsestoeventsthatsomeagentscansee
atacertainmoment,buttheotherscannot[23].Havingthisprior
4202
naJ
62
]GL.sc[
1v95051.1042:viXrathisnetworkcanrepresentalsoincreases,whichcouldaccommo-
datefortheabsenceofcommunication.Ontheotherhand,commu-
nicationenablestheflowofinformationacrossagents,which,at
firstglance,shouldalwaysbebeneficial,posinganotherinteresting
question.
Overall,inthispaper,weintendtostudythechallengesofcom-
municationinindependentMARLwhenparametersarenotshared,
whichisanunderstudiedsettinginMARLthatcanbringbene-
(a)ParameterSharing (b)NoParameterSharing
fitsforpracticalapplications[39].Inthissense,weproposeaway
ofsuccessfullycommunicatingundertheseconditions.Wealso
showthatcommunicationmightnotalwaysbeuseful,bringing Figure1:Simpleoverviewoftheconfigurationsofsharing
uselessoverhead,andshowhowitaffectslearningwhentheagent (left)andnotsharing(right)parameters.Intheformer,a
networkshavehigherorlowercapacities. jointactionisproducedbypoliciesthatarepartofthesame
network(sharedbyalltheagents),whileinthelattereach
2 BACKGROUND agenthasitsownseparatepolicy.
2.1 DecentralisedPartiallyObservableMarkov
DecisionProcesses(Dec-POMDPs) whereeachagentmaintainsallthesecomponentsonitsown.For
In this work we formalise the treated scenarios following brevity,inthispaperwerefertothismethodalsoasIndependent
Decentralised Partially Observable Markov Decision Processes Q-learning(IQL),andusedeeprecurrentQ-networks,asintroduced
(Dec-POMDPs) [25]. These can be represented by the tuple in[12],toaccommodateforpartialobservability.Overall,IQLis
ğº=âŸ¨ğ‘†,ğ´,ğ‘Ÿ,ğ‘‚,ğ‘,ğ‘ƒ,ğ‘,ğ›¾âŸ©. At each state ğ‘  âˆˆ ğ‘†, each agent ğ‘– âˆˆ trainedtominimisetheloss
Nâ‰¡{1,...,ğ‘} chooses an action ğ‘ âˆˆ ğ´, forming a joint action (cid:20) (cid:21)
â‰¡= {ğ‘1,...,ğ‘ ğ‘}thatisexecutedasawholeintheenvironment,
L(ğœƒ)=E
ğ‘âˆ¼ğµ
(cid:0)ğ‘Ÿ+ğ›¾m ğ‘a â€²xğ‘„(ğœâ€²,ğ‘â€²;ğœƒâˆ’)âˆ’ğ‘„(ğœ,ğ‘;ğœƒ)(cid:1)2 , (2)
resultinginatransitiontoanextstateğ‘ â€² accordingtoaproba-
forasampleğ‘sampledfromareplaybufferofexperiencesğµ,and
bility ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘) : ğ‘† Ã—ğ´ Ã—ğ‘† â†’ [0,1]. This results in a reward
whereğœƒ andğœƒâˆ’ aretheparametersofthelearningnetworkanda
ğ‘Ÿ(ğ‘ ,ğ‘) : ğ‘† Ã—ğ´ â†’ Rthatissharedbytheteam.Becauseofpar-
targetnetwork,respectively.
tial observability, at each state each agent receives only a local
observationğ‘œ ğ‘– âˆˆ ğ‘‚(ğ‘ ,ğ‘–) : ğ‘† Ã—N â†’ ğ‘,andeachagentholdsan
action-observationhistoryğœ ğ‘– âˆˆ T â‰¡ (ğ‘ Ã—ğ´)âˆ—.Inthecontextof 2.3 SharingParametersandNotSharing
communication,eachagentgeneratesacertainmessageğ‘š ğ‘– âˆˆM Parameters
thatisthenbroadcastedtotheothersandwillalsoconditiontheir ParametersharingisapopularstrategyadoptedbymostMARL
policies.Thus,ifasetofincomingmessagestoagentğ‘–excepttheir approachesthatallowsalltheagentsofateamtosharethesame
ownisrepresentedbyğ‘š âˆ’ğ‘–,thenitspolicycanberepresentedas learningnetworks[10].Onekeyaspectofthisapproachistheuse
ğœ‹ ğ‘–(ğ‘ ğ‘–|ğœ ğ‘–,ğ‘š âˆ’ğ‘–).Thejointpolicyaimstooptimiseajointaction-value ofanagentIDtogetherwitheachagentâ€™sinputwhentheseare
functionğ‘„ ğœ‹(ğ‘  ğ‘¡,ğ‘ ğ‘¡)=E ğœ‹[ğ‘… ğ‘¡|ğ‘  ğ‘¡,ğ‘ ğ‘¡],whereğ‘… ğ‘¡ =(cid:205) ğ‘˜âˆ =0ğ›¾ğ‘˜ğ‘Ÿ ğ‘¡+ğ‘˜ isthe fedtothenetwork.Asstatedandsupportedbymultipleprevious
discountedreturnandğ›¾ âˆˆ [0,1)isadiscountfactor. works,thistrickwillallowthesamenetworktotreattheagentsas
independentunits,despitetheinputsofallagentsbeinggivenat
2.2 IndependentDeepQ-Learning(IQL) thesametimeandtothesamenetwork.Accordingtotheliterature,
Asoneofthefirstproposedapproachesformulti-agentlearning,In- theultimatebenefitofthisstrategyismainlyseeninthetraining
dependentQ-learning(IQL)canbeseenasthemoststraightforward timethattheagentstaketoreachconvergenceinthetasksand
MARLmethod[34].Insimplerterms,IQLconsistsofgeneralising henceonthesampleefficiency[4,27].
theconceptsfromsingle-agentreinforcementlearningtomulti- Not using parameter sharing is empirically not beneficial in
agentsettings,i.e.,eachagentproducesanindividualQ-function simulatedenvironments.However,itallowsustohaveadifferent
thatisupdatedfollowingtheequation perspectiveonMARLandlookintothelearningagentsasfully
self-containedentities(likehumans),andthatlearnbythemselves.
(cid:20) (cid:21)
ğ‘„(ğ‘ ,ğ‘)=(1âˆ’ğ›¼)ğ‘„(ğ‘ ,ğ‘)+ğ›¼ ğ‘Ÿ+ğ›¾maxğ‘„(ğ‘ â€²,ğ‘â€²) , (1) Importantly,whenweconsiderpracticalapplications,itisoften
ğ‘â€² unfeasibletokeepanetworkthatissharedbyalltheagents[39].
whereğ›¼ isalearningrate.FollowingtheintroductionofDeepQ- InFigure1,wecanseethekeydifferencesbetweenthetwode-
Networks(DQN)[24],intheworkof[33]theauthorsusetogether scribedconfigurations.Asitisillustrated,intheparametersharing
theadvancesfromdeepreinforcementlearningandindependentQ- setting,allthepoliciesarecontrolledbythesamenetworkwith
learningtoproposeanimprovedindependentQ-learningwherethe parametersğœƒ,producingajointactionthatcontainsalltheactions
agentsarenowcontrolledbyindividualdeepQ-networksinstead oftheagents.Instead,withthenon-parametersharingconfigu-
offollowingonlythesimplertabularcase.Importantly,inDQN ration,eachagentiscontrolledbyanindependentnetworkwith
theauthorsintroduceanexperiencereplaybufferthatiskeptby parametersğœƒ thatproducestheactiononlyforthisparticularagent.
ğ‘–
thelearningagentandtheuseofatargetnetworkthatstabilises Afteralltheactionsarecomputed,theseareputtogetherintoa
learning.Logically,thiscanbereplicatedinthemulti-agentcase, jointactionthatisexecutedintheenvironment.implicationsofnotsharingparameters.Importantly,whilesharing
parameters can be easily done in simulated environments, it is
somethingverydifficulttoachieveinpracticalscenarios[39].Thus,
itisimportanttostudytheimplicationsofnotsharingparameters.
Inthispaper,weintendtoanalysehowcommunicationcanbe
integratedwithindependentlearnersthatdonotshareparameters.
Inotherwords,theseagentshavetheirownnetworksandmust
learnhowtogeneratemessageswithoutreceivingdirectfeedback
fromthepoliciesoftheotheragents.In[8],despitetheyhavenot
shownit,theauthorsdiscusshowRIALcouldbeextendedtothe
(a)CommunicationwithParameterShar- (b) CommunicationwithNoParameter
casewhereparametersarenotshared.Inthecaseof[8]itwouldbe
ing(PS) Sharing(NPS)
simplesincethemessagesaretreatedasiftheywereactionsthatare
sent,andthataregeneratedfromthesamepolicynetwork,leading
Figure2:Illustrationofthemaindifferencesintheprocess tolowerqualitymessages[1,13].Thus,therearenoproblemswhen
ofgeneratingandbroadcastingmessagesbetweensharing thelossesarepropagatedinthenetworkbecausetheestimated
andnotsharingparametersofthelearningnetworks.Inthe Q-values ensure that the links to the policy networks are kept.
firstcase,boththepoliciesandcommunicationnetworks However,trainingapolicyandlearningmessagesatthesametime
arecontrolledbythesameparametersğœƒ andğœ‡,whileinthe
usingthesamenetworkmightrepresentadifficultlearningtrade-
secondcase,thesehavedistinctparametersğœƒ ğ‘– andğœ‡ ğ‘–. off[26].Iftheagentshaveinsteadapolicyandadistinctnetwork
specialisedforcommunicationtheycanlearnstrongerbehaviours,
buttheprocessbecomeschallenging.Wenowformallydescribethis
2.4 CommunicationinMARL processandhighlightthemainproblemsthatneedtobeaddressed.
Asithasbeendiscussedthroughoutthispaper,communication We consider agents that are controlled by a standard policy
inMARLconsistsontheabilityoftheagentstosharesomeof network and also have a distinct network whose purpose is to
theirexperienceoftheenvironmentwiththeteammates.Thiscan generatemessagesforcommunication.Letusfirstconsiderthecase
includeelementssuchastheobservations,theactions,andevena ofIQLwithparametersharingandcommunication.Forsimplicity
fingerprint.Thus,byusingthisadditionallayerofinformationin ofnotation,inthedemonstration,weusetheobservationsğ‘œ ğ‘–instead
thelearningprocess,theagentswilllearnaQ-functionthatcondi- ofthehistoryğœ ğ‘–.Letalso ğ‘“ ğ‘– andğ‘” ğ‘– denotetwocertainfunctions
tionsnotonlyontheaction-observationhistoryasintraditional suchthatğ‘“ ğ‘– â†’ğ‘„andğ‘” ğ‘– â†’M,forasetofallQ-valuesğ‘„andaset
approachesbutalsoinasetofincomingmessagesfromtheothers, ofallmessagesM.Wehavethat
ğ‘„ ğ‘–( F[ iğœ gğ‘– u,ğ‘š reâˆ’ 2ğ‘–] d, eğ‘ pğ‘– i) c.
tsanoverviewofthemessagegenerationprocess
{ğ‘„ ğ‘–} ğ‘–ğ‘ =1={ğ‘“ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘ ğ‘–;ğœƒ)} ğ‘–ğ‘ =1, (3)
inMARL.InFigure2(a),wecanseethat,whentheagentssharepa- whereğ‘š âˆ’ğ‘– correspondstothemessagesfromallagentsexceptğ‘–,
rametersofthelearningnetworks,theprocessisrelativelystraight- thatisproducedbyaneuralnetworkdenotedbyafunctionğ‘” ğ‘— with
forward:acommunicationnetworkcontrolledbytheparametersğœ‡ parametersğœ‡
w tioil nl sg )e ,n ae nr dat te ht eh ne tm hee ss esa ag re es s( ei nn tth tois thca es oe ta hn eren ac go ed ni tn sg anof dt fh ee do tb os te hrv eia r- ğ‘š
âˆ’ğ‘–
={ğ‘” ğ‘—(ğ‘œ ğ‘—;ğœ‡)}ğ‘
ğ‘—=1,ğ‘—â‰ ğ‘–
âˆ§ğ‘š
ğ‘–
=ğ‘” ğ‘–(ğ‘œ ğ‘–;ğœ‡). (4)
AsperEq.(2),wecandefinethelossfunctionforthelearning
policynetworktogetherwiththeobservations.Figure2(b)now
problemas
depictsthecommunicationprocessbutwhenparametersarenot
wsh oa rr ke sd. bL oo thgi fc oa rll ty h, ein pt oh li is cyca cs oe n, te ra oc lh lea dg be ynt ğœƒco an nt dro fl os rin cod mep men ud ne in cat tn ioe nt- Lğ‘–(ğœƒ,ğœ‡)=ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥
ğ‘
ğ‘–â€²ğ‘„ ğ‘–(ğ‘œ ğ‘–â€²,ğ‘šâ€² âˆ’ğ‘–,ğ‘ ğ‘–â€²;ğœƒâˆ’)âˆ’ğ‘„ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘ ğ‘–;ğœƒ)
controlledbyğœ‡
ğ‘–.Inthiscase,eachagentğ‘–
independentlygenerates
=ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥
ğ‘
ğ‘–â€²ğ‘“ ğ‘–(ğ‘œ ğ‘–â€²,{ğ‘” ğ‘—(ğ‘œâ€² ğ‘—;ğœ‡âˆ’)} ğ‘—ğ‘— == 1ğ‘ ,ğ‘—â‰ ğ‘–,ğ‘ ğ‘–â€²;ğœƒâˆ’)
a simm pes lesa ag te fit rh sa t,t ii ts bth ree an ks se tn ht eto linth ke inot th he ers c. oW mh puil te at th iois nam li gg rh at ps hee fom
r
âˆ’ğ‘“ ğ‘–(ğ‘œ ğ‘–,{ğ‘” ğ‘—(ğ‘œ ğ‘—;ğœ‡)} ğ‘—ğ‘— == 1ğ‘ ,ğ‘—â‰ ğ‘–,ğ‘ ğ‘–;ğœƒ). (5)
thecommunicationnetworkswhenthelossesarebackpropagated,
Fromtheabove,wecanwriteLğ‘–(ğœƒ,ğœ‡)â‰¡Lğ‘–(ğ‘“ ğ‘–(Â·;ğœƒ,ğœ‡),ğ‘” ğ‘–(Â·;ğœ‡)),and
thenwecanalsowritethebackpropagationrulesforthegradients
sincetheparametersofthenetworksarenotsharedandthefinal
as
Q-valuesthatresultfromthepoliciesareconditionedonlyonthe
incomingmessages.Wediscussthisphenomenonfurtherahead. âˆ‡ğœƒLğ‘– = ğœ• ğœ•L ğ‘“ğ‘– ğœ• ğœ•ğ‘“ ğœƒğ‘– + ğœ• ğœ•L ğ‘”ğ‘– ğœ• ğœ•ğ‘” ğœƒğ‘– = ğœ• ğœ•L ğ‘“ğ‘– ğœ• ğœ•ğ‘“ ğœƒğ‘– , (6)
ğ‘– ğ‘– ğ‘–
3 METHODS âˆ‡ğœ‡Lğ‘– = ğœ•Lğ‘– ğœ•ğ‘“ ğ‘– + ğœ•Lğ‘– ğœ•ğ‘” ğ‘– , (7)
ğœ•ğ‘“ ğœ•ğœ‡ ğœ•ğ‘” ğœ•ğœ‡
ğ‘– ğ‘–
3.1 CommunicationinMARLforFully andfromthis,itfollowsthattheparametersofthenetworksare
IndependentLearners updatedas
I cn omth mis us ne ic ct ai to in on,w we its hta fr ut llb yy info dr em pea nll dy ed ne ts ac gr eib ni tn sg inth Me Aim RLpl ti hca at ti do ons no otf ğœƒ =ğœƒâˆ’ğ›¼âˆ‡ğœƒLğ‘– =ğœƒâˆ’ğ›¼ğœ• ğœ•L
ğ‘“
ğ‘–ğ‘– ğœ• ğœ•ğ‘“ ğœƒğ‘– , (8)
s Mh Aar Re Lpa ar pa pm roe ate cr hs e. sS ,h aa nr din tg hp ua sr ia tm iset oe fr ts enis ft oa rk ge on ttf eo nrg tora cn ote nd sii dn erm to hs et ğœ‡=ğœ‡âˆ’ğ›¼âˆ‡ğœ‡Lğ‘– =ğœ‡âˆ’ğ›¼(cid:18)ğœ• ğœ•L ğ‘“ğ‘– ğœ• ğœ•ğ‘“ ğœ‡ğ‘– + ğœ• ğœ•L ğ‘”ğ‘– ğœ• ğœ•ğ‘” ğœ‡ğ‘–(cid:19) . (9)
ğ‘– ğ‘–Figure 3: Illustration of how our proposed scheme for independent communication without parameter sharing
(NPS+IQL+COMM)workswhencomparedtosharingparameters(PS).Thefigureshowsthatagentsthatdonotsharepa-
rametersalsoneedtoreceivetheirownmessageasinputtokeepthelinktothecomputationalgraphoftheircommunication
networkduringbackpropagation.Ontheotherhand,whenparametersaresharedthistrickisnotneededsinceallofthemuse
thesamenetworkandtherearenogradientpropagationproblemsbylosingthelinkstothecommunicationnetworksinthe
computationgraph.
ThisisthestandardprocedureforIQLwithparametershar- order to update ğœ‡
ğ‘–
the corresponding gradient rule in this case
ing.However,whenwedonotshareparametersofthenetworks, wouldhavetobe
the case can be very different. We consider now the setting of
f au nl dly wi in thde cp oe mn mde un nt icl ae ta ir on ne .r Is n, ti h.e i. s, cI oQ nL figw ui rt ah tin ono ,p tha era am gee nte tsr as rh ea fr uin llg
y âˆ‡ğœ‡ğ‘–Lğ‘— =
ğœ• ğœ•L ğ‘“ğ‘— ğœ•ğœ• ğœ‡ğ‘“ ğ‘—
+
ğœ•ğœ• ğ‘”Lğ‘— ğœ•ğ‘” ğœ‡âˆ’ğ‘—
, (13)
ğ‘— ğ‘– âˆ’ğ‘— ğ‘–
self-containedanddonotshareanyparameters.However,weal-
lowthemtocommunicate.Inthiscase,ifwefollowanequivalent
communicationschemeasinthepreviouscase(i.e.,learningfrom whichisanabsurd,because ğ‘— doesnotshareparameterswithğ‘–,
theincomingmessagesfromtheothers),wenowhavethat andthus ğœ‡ ğ‘– willneverbeupdatedâˆ€ğ‘– âˆˆ {1,...,ğ‘}.Thiscanbe
summarisedasthefollowingremark.
{ğ‘„ ğ‘–} ğ‘–ğ‘ =1={ğ‘“ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘ ğ‘–;ğœƒ ğ‘–)} ğ‘–ğ‘ =1, (10)
whereğ‘š correspondsonceagaintothemessagesfromallagents Remark1. Theparametersofacommunicationnetwork ğœ‡ ğ‘– of
âˆ’ğ‘–
exceptğ‘–,thatareproducedbyaneuralnetworkdenotedbyafunc-
agentğ‘–willneverbeupdatediffullyindependentlearnersthatdonot
tionğ‘” withparametersğœ‡ sharetheparametersğœƒ ğ‘– andğœ‡ ğ‘– learnonlyfromtheirobservationsand
ğ‘— ğ‘—
incomingmessagesfromtheothers.
ğ‘š
âˆ’ğ‘–
={ğ‘” ğ‘—(ğ‘œ ğ‘—;ğœ‡ ğ‘—)}ğ‘
ğ‘—=1,ğ‘—â‰ ğ‘–
âˆ§ğ‘š
ğ‘–
=ğ‘” ğ‘–(ğ‘œ ğ‘–;ğœ‡ ğ‘–). (11)
As a solution to the problem stated in Remark 1 that occurs
SimilarlytoEq.(2),wecandefinethelossfunctionforthelearn-
inindependentcommunicationwithoutsharingparameterswhen
ingproblemas
updatingthecommunicationnetworks,weproposeinsteadthe
L(ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–)=ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥ ğ‘ ğ‘–â€²ğ‘„ ğ‘–(ğ‘œ ğ‘–â€²,ğ‘šâ€² âˆ’ğ‘–,ğ‘ ğ‘–â€²;ğœƒ ğ‘–âˆ’)âˆ’ğ‘„ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘ ğ‘–;ğœƒ ğ‘–) followinglearningschemeforindependentcommunication:
âˆ’=ğ‘Ÿ
ğ‘“
ğ‘–+ (ğ‘œğ›¾ ğ‘–,ğ‘š {ğ‘”ğ‘ğ‘¥ ğ‘—(ğ‘ ğ‘œğ‘–â€²ğ‘“ ğ‘—ğ‘– ;( ğœ‡ğ‘œ ğ‘—ğ‘–â€² ), }{
ğ‘—
ğ‘—ğ‘”
=
=ğ‘— 1ğ‘( ,ğ‘œ ğ‘—â‰ â€² ğ‘— ğ‘–; ,ğœ‡ ğ‘âˆ’ ğ‘— ğ‘–;) ğœƒ} ğ‘–ğ‘— ğ‘— )= = .1ğ‘ ,ğ‘—â‰ ğ‘–,ğ‘ ğ‘–â€²;ğœƒ ğ‘–âˆ’)
(12)
{ğ‘„ ğ‘–} ğ‘–ğ‘ =1={ğ‘“ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘š ğ‘–,ğ‘ ğ‘–;ğœƒ ğ‘–)} ğ‘–ğ‘ =1, (14)
Becausenownetworksarenotshared,fromtheabovewecanwrite whereğ‘š correspondsonceagaintothemessagesfromallagents
âˆ’ğ‘–
Lğ‘–(ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–)â‰¡Lğ‘–(ğ‘“ ğ‘–(Â·;ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–),ğ‘” âˆ’ğ‘–(Â·;ğœ‡ âˆ’ğ‘–)).Thus,hypothetically,in exceptğ‘–,thatareproducedbyafunctionğ‘”
ğ‘—
withparametersğœ‡ ğ‘—,inthesamewayasinEq.(11).Lğ‘– cannowbewrittenas Algorithm1FullyIndependentCommunication
1: InitialiseemptyreplaybufferDandtheparametersofpolicy
Lğ‘–(ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–,ğœ‡ ğ‘–)= andcommunicationnetworksforeachagentğ‘–
=ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥ ğ‘ ğ‘–â€²ğ‘„ ğ‘–(ğ‘œ ğ‘–â€²,ğ‘šâ€² âˆ’ğ‘–,ğ‘š ğ‘–â€²,ğ‘ ğ‘–â€²;ğœƒ ğ‘–âˆ’)âˆ’ğ‘„ ğ‘–(ğ‘œ ğ‘–,ğ‘š âˆ’ğ‘–,ğ‘š ğ‘–,ğ‘ ğ‘–;ğœƒ ğ‘–) 2 3:
:
for wst hep il= ee0 pt io som da ex ii sm nu om tds ot nep es dd oo
=ğ‘Ÿ+ğ›¾ğ‘šğ‘ğ‘¥ ğ‘ ğ‘–â€²ğ‘“ ğ‘–(ğ‘œ ğ‘–â€²,{ğ‘” ğ‘—(ğ‘œâ€² ğ‘—;ğœ‡âˆ’ ğ‘— )}ğ‘— ğ‘—= =1ğ‘ ,ğ‘—â‰ ğ‘–,ğ‘” ğ‘–(ğ‘œ ğ‘–â€²;ğœ‡ ğ‘–âˆ’),ğ‘ ğ‘–â€²;ğœƒ ğ‘–âˆ’) 4: Executeactionğ‘=(ğ‘1,...,ğ‘ ğ‘)andgetğ‘Ÿ andğ‘ â€²
âˆ’ğ‘“ ğ‘–(ğ‘œ ğ‘–,{ğ‘” ğ‘—(ğ‘œ ğ‘—;ğœ‡ ğ‘—)}ğ‘— ğ‘—= =1ğ‘ ,ğ‘—â‰ ğ‘–,ğ‘” ğ‘–(ğ‘œ ğ‘–;ğœ‡ ğ‘–),ğ‘ ğ‘–;ğœƒ ğ‘–), (15) 5 6:
:
endU wpd ha ilt eebufferDwith(ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)
7: ifDisnotemptythen
and now, we have that Lğ‘–(ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–,ğœ‡ ğ‘–) â‰¡ 8: foreachepisodeeinğµâˆ¼Ddo
Lğ‘–(ğ‘“ ğ‘–(Â·;ğœƒ ğ‘–,ğœ‡ âˆ’ğ‘–,ğœ‡ ğ‘–),ğ‘” âˆ’ğ‘–(Â·;ğœ‡ âˆ’ğ‘–),ğ‘” ğ‘–(Â·;ğœ‡ ğ‘–)),andwecannowwritethe 9: foreachagentido âŠ²Messagegeneration
rulesas 10: Sendğ‘š ğ‘– â†ğ‘ğ‘œğ‘šğ‘š_ğ‘›ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜(ğœ ğ‘–)toothersâ‰ ğ‘–
11: endfor
âˆ‡ğœƒğ‘–Lğ‘– = ğœ• ğœ•L
ğ‘“
ğ‘–ğ‘– ğœ•ğœ• ğœƒğ‘“ ğ‘–
ğ‘–
+ ğœ•ğœ• ğ‘”L âˆ’ğ‘–
ğ‘–
ğœ• ğœ•ğ‘” ğœƒâˆ’ ğ‘–ğ‘– + ğœ• ğœ•L
ğ‘”
ğ‘–ğ‘– ğœ•ğœ•ğ‘” ğœƒğ‘–
ğ‘–
= ğœ• ğœ•L
ğ‘“
ğ‘–ğ‘– ğœ•ğœ• ğœƒğ‘“ ğ‘– ğ‘–, (16) 1 12 3:
:
for ğ‘šea âˆ’c ğ‘–h â†age inn ct oi md io ngmessagesfroâŠ² mT or ta hin erin sg â‰ s ğ‘–tage
14: ğ‘šğ‘‘ âˆ’ğ‘– â†detachedcopyofğ‘š âˆ’ğ‘–
Inâˆ‡ tğœ‡ uğ‘– iL tivğ‘– e= ly,ğœ• ğœ• tL hğ‘“ ğ‘– iğ‘– sğœ• sğœ• tğœ‡ğ‘“ eğ‘– ğ‘– p+ soğœ•ğœ• lğ‘” vL eâˆ’ğ‘– sğ‘– tğœ• hğœ•ğ‘” eğœ‡âˆ’ ğ‘– pğ‘– ro+ blğœ• ğœ• eL ğ‘” mğ‘–ğ‘– sğœ•ğœ• tağ‘” ğœ‡ tğ‘– ğ‘– ed= inğœ• ğœ•L Rğ‘“ ğ‘– eğ‘– mğœ•ğœ• ağœ‡ğ‘“ ğ‘– rğ‘– k+ 1.ğœ• ğœ• HL ğ‘” oğ‘–ğ‘– wğœ•ğœ• eğ‘” ğœ‡
(
v1ğ‘– ğ‘– e7.
r)
,
1 1 15 6 7: :
:
ğ‘„ C Î”ğ‘„ağ‘– lâ‰¡ ğ‘–cu =ğ‘„ la ğ‘Ÿğ‘– t( e +[ğœ t ğ›¾ağ‘– ğ‘š, rğ‘š g ğ‘eğ‘‘ âˆ’ ğ‘¥t ğ‘ğ‘– s, ğ‘–â€²ğ‘„ğ‘š ğ‘„ğ‘–â€²ğ‘– ğ‘–â€²] u âˆ’, sğ‘ i ğ‘„ğ‘– n) ğ‘–gtargetnetworks
18: Updateallğœƒ ğ‘– parameters
nowwhendoingthefinalrule,itisimpliedthat 19: Updatetargetnetworkseverytargetinterval
20: endfor
âˆ‡ğœ‡ âˆ’ğ‘–Lğ‘– = ğœ• ğœ•L
ğ‘“
ğ‘–ğ‘– ğœ•ğœ• ğœ‡ğ‘“ âˆ’ğ‘–
ğ‘–
+ ğœ•ğœ• ğ‘”L âˆ’ğ‘–
ğ‘–
ğœ•ğœ•ğ‘” ğœ‡âˆ’ âˆ’ğ‘–
ğ‘–
+ ğœ• ğœ•L
ğ‘”
ğ‘–ğ‘– ğœ•ğœ• ğœ‡ğ‘” âˆ’ğ‘–
ğ‘–
2 21 2:
:
ende in fdfor
=
ğœ•Lğ‘– ğœ•ğ‘“ ğ‘–
+
ğœ•Lğ‘– ğœ•ğ‘” âˆ’ğ‘–
. (18)
23: endfor
ğœ•ğ‘“ ğœ•ğœ‡ ğœ•ğ‘” ğœ•ğœ‡
ğ‘– âˆ’ğ‘– âˆ’ğ‘– âˆ’ğ‘–
FromEq.(18),wenotetheexistenceofasecondproblem(thatis
independentofoursolutiontotheprobleminRemark1),sinceğœ‡ 3.2 Setting
âˆ’ğ‘–
wouldbeupdatedas Toconducttheexperimentsinthispaper,weconsiderasetofdiffer-
entalgorithmicconfigurationstoenabletheanalysisoftheeffects
ğœ‡ âˆ’ğ‘– =ğœ‡ âˆ’ğ‘– âˆ’ğ›¼(cid:18)ğœ• ğœ•L
ğ‘“
ğ‘–ğ‘– ğœ•ğœ• ğœ‡ğ‘“ âˆ’ğ‘–
ğ‘–
+ ğœ•ğœ• ğ‘”L âˆ’ğ‘–
ğ‘–
ğœ•ğœ•ğ‘” ğœ‡âˆ’ âˆ’ğ‘– ğ‘–(cid:19) , (19) o uf seco am lwm ayu sni inca dt eio pn enw deit nh ta Qn -d lew ari nth ino gut (Ip Qa Lra wm ite hte dr es eh par Qin -ng e. tW we oro kp st ,ato
s
describedinsection2.2,duetoitsknownsimplicity,allowingfora
forğ‘ times,causinglossesofgradientwhenpropagatingthrough
fairanalysisoftheimplicationsofdifferentconfigurationsinvolv-
thesamevaluesseveraltimes.Wecanwriteasecondimportant
ingcommunicationandvaryinglevelsofinformationexchange.
remark
Inaddition,itfacilitatestheanalysisoftheimpactofsharingand
Remark2. Iffullyindependentagentsthatdonotsharethepa- notsharingparameters.Notethatthescopeofthispaperisnotto
rametersğœƒ ğ‘– andğœ‡ ğ‘– oftheirnetworkslearnfromthemessagesofthe proposecomplexcommunicationarchitectures,asinotherworks
suchas[7,16,20].Weconsiderthefollowingconfigurations:
others,thentheincomingmessageswillbeusedforbackpropagation
ğ‘ times,causingproblemsinthecomputationalgraph. â€¢ PS+IQL:referstotheuseofIQLandtheagentssharethe
parametersofthesamenetwork(parametersharing).
ToovercometheproblemdescribedinRemark2,foreachagent
â€¢ NPS+IQL:referstotheuseofIQLandtheagentsdonot
ğ‘–,wedetachğ‘š fromthecomputationalgraph,ensuringthatall
âˆ’ğ‘– sharetheparametersoftheirnetworks(noparametershar-
ğœƒ
ğ‘–
âˆ§ğœ‡ ğ‘–,ğ‘– âˆˆ{1,...,ğ‘}areupdatedexactlyonce,accordingto
ing).
ğœƒ
ğ‘–
=ğœƒ
ğ‘–
âˆ’ğ›¼ğœ• ğœ•L ğ‘“ğ‘– ğœ•ğœ• ğœƒğ‘“ ğ‘– , asperEq.(16), (20) â€¢ P shS a+ rI iQ ngL+ paC rO amM eM te: rsre (f se ar ms eto asth Pe S+u Is Qe Lo )f ,bIQ utL nw owith wt eh ie nca lg ue dn et as
ğ‘– ğ‘–
communicationmodulethatallowstheagentstobroadcast
messages;weprovidemoredetailsahead.
ğœ‡ ğ‘– =ğœ‡ ğ‘–
âˆ’ğ›¼(cid:18)ğœ•Lğ‘– ğœ•ğ‘“ ğ‘–
+
ğœ•Lğ‘– ğœ•ğ‘” ğ‘–(cid:19)
, asperEq.(17). (21) â€¢ NPS+IQL+COMM(thatcorrespondstotheproposedlearn-
ğœ•ğ‘“ ğœ•ğœ‡ ğœ•ğ‘” ğœ•ğœ‡
ğ‘– ğ‘– ğ‘– ğ‘– ingschemein3.1):referstotheuseofIQLwiththeagents
Withthislearningscheme,whichcanbesummarizedbyEq.(14), notsharingparameters(sameasNPS+IQL),butnowthey
wesolvebothproblemsdescribedinRemarks1and2thatoccur useacommunicationmodule(sameasinPS+IQL+COMM)
infullyindependentlearningwithcommunicationandwithout thatallowstheagentstobroadcastmessages;notethathere,
parametersharing.Thisschemeallowsalltheparameterstobeup- sincewedonotconsiderparametersharing,thecommuni-
dated,enablinglearningwithcommunication.Intheresultssection cationshouldalsobeself-containedandeachagentshould
ahead,weshowthattheagentsthatfollowthisconfigurationare encode its own messages (with its own communication
stillabletolearncommunicationstrategies. network).Algorithm1describeshowthismethodworks.(a)PS+IQL (b)PS+IQL+COMM
(a)3s_vs_5z (b)PredatorPrey
Figure4:Environmentsusedintheexperiments.Ontheleft,
3s_vs_5z,ascenariofromtheSMACcollection[29],andon
therightaPredatorPreygame,where4predatorsmustcatch
twomovingprey[19].
Figure 3 shows the general architecture of the communica-
(c)NPS+IQL (d)NPS+IQL+COMM
tion approaches. The figure depicts directly the architecture of
NPS+IQL+COMM(asdescribedinsubsection3.1).Forbrevity,we
donotshowthearchitecturesoftheotherconfigurations,although Figure5:Winratesachievedbytheattemptedmethodsin
thesecanbededuceddirectlyfromthisfigure.Ifthecommunica- 3s_vs_5z.Thedashedline(optimum)representstheoptimal
tionmodulesareremoved,thenitrepresentsNPS+IQL,andthen valuethatcouldbeachievedbytheagents,i.e.,awinrateof
bothconfigurationscanbeextendedtothePScaseifweassume 1.Forcompleteness,weinclude,inthesupplementary,the
thenetworkstoberepresentationsofthesamenetwork.Nonethe- correspondingrewardsofthesewinrates.
less,inthesupplementary,westillincludethearchitecturewithout
communication.
whenapreyiscaught,andthereisasteppenaltyofâˆ’0.1Ã—ğ‘.
3.3 NetworkCapacity Mostimportantly,thereisateampunishmentfornon-cooperative
Inthispaper,tocarryoutadeeperanalysisofhowcommunication
behavioursofâˆ’0.75Ã—ğ‘ everytimeanagentattemptstocapture
apreyalone.ThispunishmentinPredatorPreytaskshasshown
affectslearning,wealsoinvestigatehowchangingthedimension
tobeimportantinpreviousworkstoevaluatetheimportanceof
numberofthehiddenlayersoftheagentsâ€™networksaffectstheir
communicationapproaches[2,16,20,40],sincethesepunishments
learningprocessinthepresenceofcommunication.Whenwein-
willmakethetaskconfusingfortheagents.
creasethesizeofthehiddenlayers,itmeansthatthepolicynetwork
Withtheresultsfromtheexperimentscarriedoutinthispaper,
canrepresentmoreinformationthanwhenthissizeisdecreased.
weintendtoanswerthefollowingquestions:
Thismeansthat,ifagentscanlearnusingnetworksthathavelower
capacity,theseagentscanstillperformunderlightweightnetworks. â€¢ (Q1)Howdoessharingparametersaffectlearningwhen
Importantly,thesenetworkscanstillextracttheneededinforma- comparedtonotsharingparameters?
tionfromtheinputstothenetworktolearnthetask,whilekeeping â€¢ (Q2)Canagentsstillcommunicatewhentheydonotshare
therepresentationsoftheinputsmuchsmaller.Thiscanbecrucial parameters?
whentherearecertaincomputationalconstraints.Atthesametime, â€¢ (Q3)Iscommunicationalwaysnecessary(sinceitisoften
itmeansthatlotsofcapacitymightbeuselessandthereiswasteof naivelyapplied,bringinguselesscomplexitytothelearning
informationifthesameinformationcouldberepresentedwitha networks)?
lowercapacity. â€¢ (Q4)Howiscommunicationaffectedbydifferentsizesof
networkcapacityforwhenwebothshareandnotshare
4 EXPERIMENTSANDRESULTS1 parameters?
Inthissection,wepresenttheresultsofthediscusseddifferentcon- Inthesupplementary,wedescribethehyperparametersusedin
figurations.Importantly,oneofthekeypointsoftheseexperiments theexperiments,alongsideotherimplementationdetails.
istoevaluatewhethertheproposedmethodinsection3.1enables
successfulcommunicationforindependentlearnerswhodonot 4.1 CommunicationwhenSharingorNot
shareparameters.Weevaluateourhypothesesintheenvironments SharingParameters
3s_vs_5zandPredatorPrey(Figures4(a)and4(b)).3s_vs_5zisone
Aswehavediscussedpreviouslyinthispaper,oneofthekeypoints
oftheenvironmentsoftheSMACcollection[29],where3stalker
hereistostudytheimplicationsofsharingandnotsharingparam-
units(meleeunits)mustdefeat5zealots(rangedunits).Intheused
eters.Fromthepresentedexperiments,weobservethatsharing
versionofPredatorPrey,4agentsmustcapture2randomlymoving
parameters naturally brings advantages to the agentsâ€™ learning
preyina7Ã—7gridworld.Theteamrewardthatisreceivedis5Ã—ğ‘
process.Inlinewithotherworksthathavearguedthatsharing
1Codesusedcanbefoundathttps://github.com/rafaelmp2/marl-indep-comm parametersworksmostlyasawayofspeedinguplearningandcanbeproblematic.Figures5(a)and5(b)showtheperformances
ofPS+IQLandPS+IQL+COMMinthe3s_vs_5zenvironment,re-
spectively.Inthiscase,wecanseethatcommunicationdoesnot
seemtoplayanimportantroleinthetask.Inbothcases,theagents
achieveoptimalperformances,meaningthatcommunicationbe-
comesredundantandonlybringsoverheadtotheagents,sincean
additionalnetworkisbeingused(andsimplyincreasingthecapac-
ityoftheagentnetworkswouldbeenough).Whenparametersare
(a)PS+IQL (b)PS+IQL+COMM notshared,NPS+IQLandNPS+IQL+COMMinFigures5(c)and5(d),
weobserveasimilarscenario,wheretheeffectofcommunication
isalmostunnoticeable.Ontheotherhand,whenwelookatthe
secondtestedenvironmentPredatorPrey,wecanseethatcommu-
nicationhasaverystrongimpact(Figure6).Whencommunication
isnotused,theagentscannotsolvethistaskatall.However,when
theycommunicate,theycanbreakthebarrierofnegativereturns,
bothwhentheyshareanddonotshareparameters.Thus,inthis
case,communicationisnecessaryforlearning,asopposedtothe
(c)NPS+IQL (d)NPS+IQL+COMM previousenvironment.Theseresultsurgefortheneedtoanalyse
whencommunicationisneededornotbeforeapplyingitnaively
Figure6:Rewardsachievedbytheattemptedmethodsinthe
asitmightcauseawasteofcomputationalresources(Q3).
PredatorPreytask,withapunishmentfornon-cooperative
behavioursof-0.75. 4.2 TheImpactofDifferentNetworkCapacity
Inordertostudytheamountofnetworkcapacityneededforlearn-
ingandhowcommunicationhelpswiththisinformation,wehave
savingcomputationalresources,hereweenhanceinFigure5(a)
alsoexperimentedwithdifferentsizesforthehiddenlayersofthe
that,whentheagentsshareparameters,theysolve3s_vs_5zmuch
agentnetworkbutfixedthecommunicationnetworkhiddendi-
faster,despitenotsharingparameterswillalsosolvetheenviron-
mensionsto64.Asitwasexpected,whentheagentshaveahigher
ment,butwhiletakingmuchlonger.Whenwestudytheeffect
networkcapacity,theirperformanceisdrasticallyimproved.Onthe
ofcommunication,itispossibletoseethattheimpactofcommu-
otherhand,whenthenetworkcapacityisnotenough,itmighttake
nicationisalsomoreevidentwhenparametersareshared.This
themlongertolearnthetasks.Thismeansthatnetworkswitha
isduetothefactthatthefeedbackresultingfromthemessages
highernetworkcapacityhaveahighersampleefficiencyastheycan
producedandthepoliciesoftheagentsisbackpropagatedtothe
learnfasterwiththesameamountofsamples.Thus,wecanseethat,
samenetworks.Inthecaseofnotsharingparameters,theproblem
whilewithsize32theagentsstruggletolearnin3s_vs_5z,when
becomesmuchmorecomplexbecausethereisnolinkbetween
weincreaseto64and128theylearnmoreeasilyandmuchfaster.
networksofdifferentagents,andthusthecommunicationnetwork
Onceagain,thisverifiesbothforPS+IQLandNPS+IQL(Figures
doesnotreceivedirectfeedbackofthemessagesproducedbyitself.
5(a)and5(c)).
Instead,ithastounderstandtheimpactofthesemessagesbyunder-
Whenwelookatthecommunicationside(Figures5(b)and5(d)),
standinghowtheyareaffectingtheteamrewardglobally.However,
we observe once again that, in the case of 3s_vs_5z, following
despitethislimitationinherenttothefactthatparametersarenot
ourhypothesisthatcommunicationmaynotbealwaysnecessary,
shared,wecanstillseetheimprovementsofcommunicationin
addingcommunicationwillnotyieldanysignificantimprovement
fullyindependentlearnersinFigure5(d).Theseimprovementsof
oversimplychangingthesizesofthenetworkcapacity.However,
communicationforfullyindependentagentsareevenstrongerin
whenwelookatthePredatorPreycase(Figure6),wecanseethatthe
PredatorPrey(Figure6(d)),wheretheagentsmanagetoachieve
agentscanonlysolveitwithcommunicationand,whileincreasing
positiverewardsinthetask,asopposedtowhentheydonotcom-
thenetworkcapacitywithoutcommunicationdoesnothaveany
municate(Figure6(c)).Thisdemonstratesthatourframeworkfor
impact,increasingittogetherwithaddingcommunicationmakesa
communicationwhenparametersarenotsharedenableslearning
bigdifference.Herewecanseethatcommunicationalliedtothe
inthischallengingconfiguration.Thisisanimportantobservation
rightnetworksizewillleadtobetterperformance.Thus,byusing
whenweconsiderscenarioswhereparameterscannotbeshared
acommunicationnetwork,wecanspareresourcesregardingthe
(Q1,Q2).
standardagentnetworks.Insummary,notetheimportantremark
ItiscommontoseeintheliteratureoncommunicationinMARL
thatwhileincreasingthenetworkcapacitymightbeenoughfor
methodswherecommunicationisnaivelyappliedtoanarbitrary
somecases,whencommunicationisnecessarysimplyincreasing
numberofvariedenvironments.Inmostcases,themethodsusing
thenetworkcapacityisnotenough,andbothareneeded(Q4).
communicationendupperformingbetterinthetestedenviron-
ments,buttherearecaseswheretheperformancesachievedarevery
closetobenchmarkswithoutcommunicationbeingused.Insuch 5 RELATEDWORK
cases,oneoftenwonderswhethercommunicationisreallyneces- RecentworksinMARLhavedevelopedstrongmethodstotackle
sary,orifitisjustbringinguselessoverheadtothenetworks,which complexproblemsbasedonneuralnetworkarchitecturesthatcangivegoodvaluefunctionestimations.Tonameafew,QMIXpro- 6 CONCLUSION
posesawayofmixingindividuallyestimatedaction-valuefunctions CommunicationisstillanopenareaofresearchinMARL.While
intoaglobalfunction,fromwhichoptimaljointpoliciescanbe remarkable progress has been made in the field, there are still
extracted[27].VDN,orQTRAN[30,32]areothersuchmethods aspectsthathavenotbeeninvestigatedindetail.Inthiswork,we
thatlearnadifferenttypeofmixoftheseindividualfunctions.Ul- haveparticularlyshownthatthereareseveraladvantageswhen
timately,thegoalisalwaystolearnanoptimaljointaction-value agentsareallowedtoshareparametersofthelearningnetworks,
function.Thesemethodscomposeawayoflearningwithoutcom- assupportedbytheliterature.However,thisconfigurationmay
munication,i.e.,theagentsdonotdirectlybroadcastinformation notalwaysbefeasible(forinstance,inpracticalapplications),and
toeachother,althoughtheyhaveindirectaccesstothepoliciesof thusthereisaneedtodeepenthestudyoftheimplicationsofnot
theothersduetothemixingduringtraining. sharingparameters.Wehaveproposedawayofcommunicating
Whilethesemethodsarecommunication-free,latelymultiple inMARLforindependentlearnersthathavedistinctnetworksfor
otherworkshavetargetedcommunicationinMARLfromdiffer- policyandmessagegenerationanddonotshareparameters.The
entangles.In[8]theauthorsdemonstratehowacommunication resultsachievedshowthatagentscanstilllearncommunication
protocolcanbelearnedwhenthemessageisgeneratedbythepol- strategiesunderthissetting.
icyoftheagentsandtreatedasanaction.In[31]itisproposed Takingintoaccountthenetworkcapacitycanalsobeadeal
anothermethodofcommunicationthataggregatesthemessages breakerinlearning.Evenwhenconsideringcommunicationamong
ofalltheagentsintoacumulativemessagethatisthensent.In agents,differentnetworkcapacitiesmayleadtodifferentoutcomes.
[5]theauthorsproposeamethodthatminimisesinformationloss Infact,itisimportanttoevaluatewhethercommunicationorextra
ofmessageaggregation,butrequiresaccesstoglobalinformation network capacity in MARL is really needed before jumping to
during training. Other recent approaches have proposed strate- theseoptionsnaively,resultinginunnecessaryoverhead.Inthe
giesforcommunicatingbasedonspecificfactorssuchaswhom, future,weintendtoextendourfindingstootherdifferentscenarios
when,andwhattocommunicate[6,15].In[11],theauthorsusea anddigdeeperintothelearningprocessofcommunicationthat
centralagentthatcontrolsmessagesandlearnswhatneedstobe resultsfromtheproposedlearningschemeforfullyindependent
senttotheagents.Othertypesofcommunicationhavebeenused communication.
togetherwithmethodsthatdonotinitiallyusecommunication.
Forinstance,methodssuchas[20,40]proposecommunicationar- REFERENCES
chitecturesthatcanbeusedtoimprovetheperformanceofother
[1] SushrutBhalla,SriramG.Subramanian,andMarkCrowley.2019. Training
non-communicativeapproaches.Amongallthementionedmeth- CooperativeAgentsforMulti-AgentReinforcementLearning.InProceedingsof
ods,notethat,duringthecommunicationprocess,thereisalways the18thInternationalConferenceonAutonomousAgentsandMultiAgentSystems
(MontrealQC,Canada)(AAMASâ€™19).InternationalFoundationforAutonomous
somesortofinformationexchange(whensharingmessages),and AgentsandMultiagentSystems,Richland,SC,1826â€“1828.
thereisalwayssomesortoflossofinformation(duringencoding [2] WendelinBÃ¶hmer,VitalyKurin,andShimonWhiteson.2019.DeepCoordination
oraggregation).Thistypeofinformationconditioningisstudied Graphs.CoRRabs/1910.00091(2019).arXiv:1910.00091 http://arxiv.org/abs/1910.
00091
inworkssuchas[38],wheretheauthorsproposeamethodthat [3] Kwang-ChengChenandHsuan-ManHung.2019.WirelessRoboticCommunica-
canlearnmessagesthatfollowcertainbandwidthconstraints.In tionforCollaborativeMulti-AgentSystems.InICC2019-2019IEEEInternational
[17],theauthorsalsouseadrop-outstrategybasedontheaverage
ConferenceonCommunications(ICC).1â€“7. https://doi.org/10.1109/ICC.2019.
8761140
weightsofseveralmessagenetworksduringtraining. [4] FilipposChristianos,GeorgiosPapoudakis,ArrasyRahman,andStefanoV.Al-
Aspectssuchastheimplicationsofcommunicatingunderlimited brecht.2021.ScalingMulti-AgentReinforcementLearningwithSelectiveParam-
eterSharing. arXiv:2102.07475[cs.MA]
bandwidthchannels,orthemeasurementofthequalityofcommu- [5] TianshuChu,SandeepChinchali,andSachinKatti.2020.Multi-agentreinforce-
nicationhavebeenstudiedinworkssuchas[21,28,35,38].Instead, mentlearningfornetworkedsystemcontrol. arXivpreprintarXiv:2004.01339
inthisworkweintendtoanalysewhethercommunicationisreally (2020).
[6] AbhishekDas,ThÃ©ophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,
necessarywhentheagentnetworksalreadyhaveenoughcapacity. MichaelRabbat,andJoellePineau.2019.TarMAC:TargetedMulti-AgentCommu-
Inworkssuchas[14]itisshownhowincreasingthenetworkca- nication.InProceedingsofthe36thInternationalConferenceonMachineLearning,
Vol.97.1538â€“1546. arXiv:1810.11187.
pacitycanimprovelearning,buthowdoesitaffectcommunication
[7] ZiluoDing,WeixinHong,LiwenZhu,TiejunHuang,andZongqingLu.2022.
methods?Furthermore,notethatin[36]itisanalysedcommunica- SequentialCommunicationinMulti-AgentReinforcementLearning. https:
tionamongcriticsthatapproximateadvantageutilities.However, //openreview.net/forum?id=xzeGP-PtPMI
[8] JakobN.Foerster,YannisM.Assael,NandodeFreitas,andShimonWhiteson.
theauthorsdonotfocusondirectagent-to-agentcommunication. 2016.LearningtoCommunicatewithDeepMulti-AgentReinforcementLearning.
Importantly,wealsonotethat,whilein[8]theauthorshavebriefly InAdvancesinNeuralInformationProcessingSystems,Vol.29. arXiv:1605.06676.
mentionedcommunicationforindependentagents,itisstillnot [9] AndreiFurda,LaurentBouraoui,MichelParent,andLjuboVlacic.2010. Im-
provingSafetyforDriverlessCityVehicles:Real-TimeCommunicationand
clearhowcommunicationcanbeachievedinfullyindependent DecisionMaking.In2010IEEE71stVehicularTechnologyConference.1â€“5. https:
agentsthatdonotshareparametersincomplexscenarios.Sharing //doi.org/10.1109/VETECS.2010.5494179
[10] JayeshK.Gupta,MaximEgorov,andMykelKochenderfer.2017.Cooperative
parametersisoftentakenforgrantedinMARL,leaningresearch
Multi-agentControlUsingDeepReinforcementLearning.InAutonomousAgents
towardsoblivionregardingtheunderstandingoftheimplications andMultiagentSystems,GitaSukthankarandJuanA.Rodriguez-Aguilar(Eds.).
ofeithersharingornotsharingparameters,asdiscussedinworks SpringerInternationalPublishing,Cham,66â€“83.
[11] NikunjGupta,GSrinivasaraghavan,SwarupMohalik,NishantKumar,and
like[4]. MatthewETaylor.2023.Hammer:Multi-levelcoordinationofreinforcement
learningagentsvialearnedmessaging.NeuralComputingandApplications(2023),
1â€“16.
[12] MatthewHausknechtandPeterStone.2017. DeepRecurrentQ-Learningfor
PartiallyObservableMDPs. arXiv:1507.06527[cs.LG][13] OmarHouidi,SihemBakri,DjamalZeghlache,JulienLesca,PhamTranAnh [34] MingTan.1993.Multi-AgentReinforcementLearning:Independentvs.Coop-
Quang,JÃ©rÃ©mieLeguay,andPaoloMedagliani.2023.AMAC:Attention-based erativeAgents.InProceedingsoftheTenthInternationalConferenceonMachine
Multi-AgentCooperationforSmartLoadBalancing.InNOMS2023-2023IEEE/IFIP Learning.330â€“337.
NetworkOperationsandManagementSymposium.1â€“7. https://doi.org/10.1109/ [35] MycalTucker,RogerLevy,JulieAShah,andNogaZaslavsky.2022. Trading
NOMS56928.2023.10154214 offUtility,Informativeness,andComplexityinEmergentCommunication.In
[14] JianHu,SiyangJiang,SethAustinHarding,HaibinWu,andShihweiLiao. AdvancesinNeuralInformationProcessingSystems,S.Koyejo,S.Mohamed,
2023. RethinkingtheImplementationTricksandMonotonicityConstraintin A.Agarwal,D.Belgrave,K.Cho,andA.Oh(Eds.),Vol.35.CurranAssociates,
CooperativeMulti-AgentReinforcementLearning. arXiv:2102.03479[cs.LG] Inc.,22214â€“22228. https://proceedings.neurips.cc/paper_files/paper/2022/file/
[15] JiechuanJiangandZongqingLu.2018.LearningAttentionalCommunication 8bb5f66371c7e4cbf6c223162c62c0f4-Paper-Conference.pdf
forMulti-AgentCooperation.arXiv:1805.07733[cs](Nov.2018). http://arxiv.org/ [36] SimonVanneste,AstridVanneste,TomDeSchepper,SiegfriedMercelis,Peter
abs/1805.07733arXiv:1805.07733. Hellinckx,andKevinMets.2023.Distributedcriticsusingcounterfactualvalue
[16] DaewooKim,SangwooMoon,DavidHostallero,WanJuKang,TaeyoungLee, decompositioninmulti-agentreinforcementlearning.InAdaptiveandLearning
KyunghwanSon,andYungYi.2019.LearningtoScheduleCommunicationin AgentsWorkshop(ALA),collocatedwithAAMAS,29-30May,2023,London,UK.
Multi-agentReinforcementLearning. arXiv:1902.01554[cs.AI] 1â€“9.
[17] WoojunKim,MyungsikCho,andYoungchulSung.2019. Message-dropout: [37] JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang.2021.
Anefficienttrainingmethodformulti-agentdeepreinforcementlearning.In QPLEX:DuplexDuelingMulti-AgentQ-Learning. arXiv:2008.01062[cs.LG]
ProceedingsoftheAAAIconferenceonartificialintelligence,Vol.33.6079â€“6086. [38] RundongWang,XuHe,RunshengYu,WeiQiu,BoAn,andZinoviRabinovich.
[18] WoojunKim,JongeuiPark,andYoungchulSung.2021.CommunicationinMulti- 2019.LearningEfficientMulti-agentCommunication:AnInformationBottleneck
AgentReinforcementLearning:IntentionSharing.InInternationalConference Approach.CoRRabs/1911.06992(2019).arXiv:1911.06992 http://arxiv.org/abs/
onLearningRepresentations. https://openreview.net/forum?id=qpsl2dR9twy 1911.06992
[19] AnuragKoul.2019.ma-gym:Collectionofmulti-agentenvironmentsbasedon [39] AnnieWong,ThomasBÃ¤ck,AnnaVKononova,andAskePlaat.2023.Deepmul-
OpenAIgym.https://github.com/koulanurag/ma-gym. tiagentreinforcementlearning:Challengesanddirections.ArtificialIntelligence
[20] ZeyangLiu,LipengWan,Xuesui,KewuSun,andXuguangLan.2021.Multi-Agent Review56,6(2023),5023â€“5056.
IntentionSharingviaLeader-FollowerForest.TechnicalReportarXiv:2112.01078. [40] Sai Qian Zhang, Qi Zhang, and Jieyu Lin. 2019. Efficient Communica-
arXiv. http://arxiv.org/abs/2112.01078arXiv:2112.01078[cs]type:article. tion in Multi-Agent Reinforcement Learning via Variance Based Control.
[21] RyanLowe,JakobFoerster,Y.-LanBoureau,JoellePineau,andYannDauphin. arXiv:1909.02682[cs.LG]
2019.OnthePitfallsofMeasuringEmergentCommunication.TechnicalReport
arXiv:1903.05168.arXiv. http://arxiv.org/abs/1903.05168arXiv:1903.05168[cs,
stat]type:article.
[22] HangyuMao,ZhiboGong,ZhengchaoZhang,ZhenXiao,andYanNi.2019.
LearningMulti-agentCommunicationunderLimited-bandwidthRestrictionfor
InternetPacketRouting. arXiv:1903.05561[cs.MA]
[23] FedericoMason,FedericoChiariotti,AndreaZanella,andPetarPopovski.2023.
Multi-AgentReinforcementLearningforPragmaticCommunicationandControl.
arXivpreprintarXiv:2302.14399(2023).
[24] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiA.Rusu,JoelVeness,
MarcG.Bellemare,AlexGraves,MartinA.Riedmiller,AndreasKirkebyFidjeland,
GeorgOstrovski,StigPetersen,CharlieBeattie,AmirSadik,IoannisAntonoglou,
HelenKing,DharshanKumaran,DaanWierstra,ShaneLegg,andDemisHassabis.
2015.Human-levelcontrolthroughdeepreinforcementlearning.Nature518
(2015),529â€“533. https://api.semanticscholar.org/CorpusID:205242740
[25] FransA.OliehoekandChristopherAmato.2016. AConciseIntroductionto
DecentralizedPOMDPs.Springer.
[26] MarieOssenkopf,MackenzieJorgensen,andKurtGeihs.2019.HierarchicalMulti-
AgentDeepReinforcementLearningtoDevelopLong-TermCoordination.In
Proceedingsofthe34thACM/SIGAPPSymposiumonAppliedComputing(Limassol,
Cyprus)(SACâ€™19).AssociationforComputingMachinery,NewYork,NY,USA,
922â€“929. https://doi.org/10.1145/3297280.3297371
[27] TabishRashid,MikayelSamvelyan,ChristianSchroederdeWitt,GregoryFar-
quhar,JakobFoerster,andShimonWhiteson.2018. QMIX:MonotonicValue
FunctionFactorisationforDeepMulti-AgentReinforcementLearning.InProceed-
ingsofthe35thInternationalConferenceonMachineLearning,Vol.80.4295â€“4304.
arXiv:1803.11485.
[28] CinjonResnick,AbhinavGupta,JakobFoerster,AndrewM.Dai,andKyunghyun
Cho.2020.Capacity,Bandwidth,andCompositionalityinEmergentLanguage
Learning.arXiv:1910.11424[cs,stat](April2020). http://arxiv.org/abs/1910.11424
arXiv:1910.11424.
[29] MikayelSamvelyan,TabishRashid,ChristianSchrÃ¶derdeWitt,GregoryFar-
quhar,NantasNardelli,TimG.J.Rudner,Chia-ManHung,PhilipH.S.Torr,
JakobN.Foerster,andShimonWhiteson.2019.TheStarCraftMulti-AgentChal-
lenge.CoRRabs/1902.04043(2019).arXiv:1902.04043 http://arxiv.org/abs/1902.
04043
[30] KyunghwanSon,DaewooKim,WanJuKang,DavidHostallero,andYungYi.
2019.QTRAN:LearningtoFactorizewithTransformationforCooperativeMulti-
AgentReinforcementLearning.CoRRabs/1905.05408(2019).arXiv:1905.05408
http://arxiv.org/abs/1905.05408
[31] SainbayarSukhbaatar,arthurszlam,andRobFergus.2016.LearningMultiagent
CommunicationwithBackpropagation.InProceedingsofthe30thInternational
ConferenceonNeuralInformationProcessingSystems,D.D.Lee,M.Sugiyama,
U.V.Luxburg,I.Guyon,andR.Garnett(Eds.).2252â€“2260.
[32] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,Karl
Tuyls,andThoreGraepel.2017.Value-DecompositionNetworksForCooperative
Multi-AgentLearning. arXiv:1706.05296[cs.AI]
[33] ArdiTampuu,TambetMatiisen,DorianKodelja,IlyaKuzovkin,KristjanKorjus,
JuhanAru,JaanAru,andRaulVicente.2015. MultiagentCooperationand
CompetitionwithDeepReinforcementLearning. arXiv:1511.08779[cs.AI]A SMACREWARDPLOTS
Forcompletenesspurposes,inFigure7wealsoshowtheresultingrewardsfor3s_vs_5zthatcorrespondtothewinratesintheexperiments
sectionofthemainpaper.Wecanseethattheplotsoftherewardstellthesamestoryasthecorrespondingwinrates.However,insome
cases,therewardsforsmallernetworkcapacitiesareclosetotheotherswithhighercapacity,meaningthattheseagentswithlowercapacity
arelikelytobeclosetobetterperformances.Forinstance,inFigure7(a)wecanseethattheperformancewithsize32isgettingverycloseto
theonewith64.AlsoinFigure7(d),wecanseethat,attheendoftraining,theagentswithsize32areveryclosetotheoneswithsize64and
eventheoneswithsize128.Thissuggeststhatitislikelythattheywouldreachthesameoptimalperformanceiftheyweretrainedfor
longer.Thisleadstotheinsightthatagentswithlowercapacitywillrequiremoretrainingtime,buttheymightreachthesameoptimal
performanceiftrainedforlongenough.Theseobservationscannotbeeasilyseenintheplotsofthewinrates.
(a)PS+IQL (b)PS+IQL+COMM (c)NPS+IQL (d)NPS+IQL+COMM
Figure7:Rewardsachievedbytheattemptedmethodsin3s_vs_5z.Thedashedline(optimum)representsthemaximumvalue
achievedamongallthemethodsinthisscenario.Itisplottedsimplyforbettervisualisationpurposes.
B ARCHITECTUREOFINDEPENDENTLEARNINGWITHOUTCOMMUNICATION
Figure8:ArchitectureforNPS+IQLasdescribedinthemaintextofthispaper.WhencomparedtoNPS+IQL+COMM,nowthere
isnocommunicationnetworkandtheactionsoftheagentsaretakenbasedonlyontheobservations,andnotonthemessages
aswell,asshownforNPS+IQL+COMMinsection3.1inthemainpaper.
Forcomparisonwiththearchitectureinsection3.1ofthemainpaper,inFigure8weshowthearchitectureofindependentlearning
withoutparametersharingthatwasusedintheexperimentsinthispaper.Intuitively,independentlearningwithparametersharingcanbe
deducedfromthisarchitecture,ifwelookatthenetworksoftheagentsasiftheyrepresentthesamenetworkthatreceivesanadditional
agentIDasinput.
C HYPERPARAMETERSANDIMPLEMENTATIONDETAILS
Intheexperimentspresentedinthemainpaper,alltheresultsrepresenttheaverageof3independentruns.Inourconfigurationofdeep
independentQ-learning,alltheagentsarecontrolledbyrecurrentdeepneuralnetworkswithaGatedRecurrentUnit(GRU)cell.Thedefault
widthoftheGRUis64.However,asdescribedintheexperiments,weincreaseordecreasethisvaluetoconductouranalysis.Weuseboth
parametersharingandnoparametersharingintheexperiments.Whenparametersarenotshared,eachindividualagentiscontrolledbya
separatenetwork,liketheonedescribed.Intheexperimentswithcommunication,weuseacommunicationneuralnetworkbasedonlinear
transformationsthatencodestheobservationsoftheagents.Wefixthedimensionofthehiddenlayersto64.Importantly,whenweuseparametersharing,anagentIDisincludedintheinputsoftheagents.However,thisisnotusedanymorewhenparametersarenotshared,
sinceitbecomesredundant.
Theexploration-exploitationtradeoffoftheagentsismadeaccordingtotheepsilon-greedymethod,withaninitialepsilonofğœ– =1that
annealsdowntotaminimumof0.05throughout50000trainingepisodes.WeusetheRMSpropoptimisertotrainallthenetworks,witha
learningrateğ›¼ =5Ã—10âˆ’4.Thediscountfactorusedisğ›¾ =0.99,andthemaximumsizeofthereplaybufferis5000,fromwhichminibatches
of32episodesaresampled.Every200trainingepisodes,thetargetnetworksareupdated.