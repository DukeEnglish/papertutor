EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
YuhuiLi‚ô† FangyunWei‚Ä° ChaoZhang‚ô† HongyangZhang‚ô£‚Ä†
‚ô†PekingUniversity ‚Ä°MicrosoftResearch ‚ô£UniversityofWaterloo ‚Ä†VectorInstitute
hongyang.zhang@uwaterloo.ca
https://github.com/SafeAILab/EAGLE
Figure 1: Speedup ratio of Vicuna and LLaMA2-Chat inference latency on the MT-bench for greedy (temperature=0)
settings.SpeedupratioofMedusaandLookaheadarecopiedfromtheiroriginaltechnicalreports.Withspeculativesampling,
thereisalackofsuitabledraftmodelstoacceleratethe7Bmodel. Employinga7Bmodelasthedraftmodelfora13B
modelresultsinslowspeedsduetothehighoverheadofthe7Bmodel,renderingitlessefficientthanvanillaauto-regressive
decoding. ThesescenariosaremarkedasN/A.Inthispaper,weonlycomparemethodsthatdonotneedtofinetunethe
backbonemodels,ensuringtheoutputtextdistributionremainsconstant.
Abstract coding,2xfasterthanLookahead,and1.6xfaster
thanMedusa. Usinggpt-fast,EAGLEattainson
Auto-regressivedecodingmakestheinferenceof average160tokens/swithLLaMA2-Chat13Bon
LargeLanguageModels(LLMs)time-consuming. asingleRTX3090GPU,comparedto24tokens/s
Weproposeasimpleframework,EAGLE(Extrap- ofHuggingface‚Äôsimplementations.
olation Algorithm for Greater Language-model
Efficiency),forlosslessacceleration. Unliketra-
ditionalspeculativesamplingmethods,EAGLE 1.Introduction
operatesthedraftingprocessauto-regressivelyat
themoreregular(second-top-layer)featurelevel Auto-regressivedecodinghasbecomethedefactostandard
andaddressesthesamplinguncertaintyissuesin for large language models (LLMs). This process gener-
thenext-featurepredictionproblemsbyintegrat- atesoutputtokensoneatatime,whichmakesthegenera-
ingtokensfromonetimestepahead. Theacceler- tionbyLLMsbothcostlyandslow. Speculativesampling
ationprovidedbyEAGLEislossless: itinvolves (Leviathanetal.,2023;Chenetal.,2023a)basedmethods
nofine-tuningofthetargetLLM,andthegener- offerasolutiontothischallenge.Theydividethegeneration
atedtextmaintainsthesamedistributionasthat process of LLMs into two stages: the draft stage, where
ofvanillaauto-regressivedecoding.Asofthesub- potentialtokensareconjecturedatalowcost,andtheveri-
missionofthispaper,EAGLEisthefastestknown ficationstage,wherethesetokensarevalidatedinparallel
frameworkwithinthespeculativesamplingfamily. throughasingleforwardpassoftheLLM.Theparalleliza-
OnMT-bench,EAGLEis3xfasterthanvanillade- tion in speculative sampling facilitates the generation of
1
4202
naJ
62
]GL.sc[
1v77051.1042:viXraEAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
multiplepost-checktokensperLLMforwardpass,signifi-
cantlyenhancingspeed. Moreimportantly,theverification
stageinspeculativesamplingensuresthatthetextdistribu-
tionalignspreciselywiththedecodingresultsoftheoriginal
LLM,maintainingtheintegrityofthegeneratedcontent.
The key to applying speculative sampling is to identify a
draftmodelthatissufficientlysimilartotheoriginalLLM
yethaslowerlatency. Speculativesamplingtypicallyem-
ploysalower-parameterversionofaLLMfromthesame
series as the draft model. For instance, in the LLaMA2 Figure2: SpeedupratioontheMT-benchfornon-greedy
(Touvronetal., 2023)serieswhich includesmodels with (temperature=1)settings. Lookaheadisconfinedtogreedy
7B, 13B, and 70B parameters, using the 7B model as a decoding,andthenon-greedygenerationofMedusadoes
proxy ofthe 70Bmodel is valid, while finding asuitable notguaranteelosslessperformance. Therefore,EAGLEis
draftmodelforthesmallest7Bvariantistricky. Analter- notcomparedwiththesemethods.
native could be to use TinyLLaMA (Zhang et al., 2024),
butthisisnotfeasibleforinstruct-tunedmodelsduetothe
inconsistencyininstructiontemplatesbetweenLLaMA2- lize this benchmark as it has been employed by the cur-
ChatandTinyLLaMA-Chat. For13Band70Bmodels,a7B rentstate-of-the-art,includingLookaheadandMedusa,to
modelwithinthesameseriescouldserveasthedraftmodel. demonstrate their speedup ratios. This choice facilitates
However, due to the high overhead of the 7B model, the a fair and direct comparison between our approach and
accelerationeffectofspeculativesamplingisnotoptimal. these benchmarks. Under a greedy decoding setting, for
Traininganew,appropriatelysizeddraftmodelspecifically Vicuna-13BandLLaMA2-Chat13B,70B,EAGLEoffersa
forspeculativesamplingisnotanidealsolutioneitherdue 3xaccelerationthatistheoreticallyguaranteedtomaintain
tothehighcost: TinyLLaMAistrainedon3,000Btokens, text distribution of the original LLM and is ready to use
whereasEAGLEistrainedon2-4Btokens. rightout‚Äìof-the-box. Comparedtotherecentlyproposed
speculative sampling-based frameworks, Lookahead and
Thekeytoenhancingaccelerationinspeculativesampling
Medusa, EAGLE achieves 2x and 1.6x speedups, respec-
lies in reducing the time overhead and improving the ac-
tively. Whileenhancingspeed,EAGLEalsoincreasesthe
ceptancerateofthedraftbytheoriginalLLM(Chenetal.,
throughput of LLM systems by 2x. EAGLE operates in
2023b;Xiaetal.,2023;Santillietal.,2023). Numerousap-
parallel with other acceleration or throughput-improving
proachesfocusonreducingtheoverheadofthedraftphase.
methods,suchasquantization,compilation,etc. Combining
Lookahead(Fuetal.,2023)employsn-gramandJacobiiter-
EAGLEwiththesetechniquescouldfurtherreducetheoper-
ation,whileMedusa(Caietal.,2023)utilizesasetofMLPs
ationalcostsofLLMsystems. Forexample,withgpt-fast1,
thatpredicttokensbasedonthesecond-top-layerfeatureof
EAGLEacceleratesLLaMA2-Chat7Bdecodingfrom24.5
the original LLM. These strategies significantly decrease
tokens/sto160.4tokens/sonasingleRTX3090GPU.
thelatencyingeneratingdrafts,leadingtoimprovedaccel-
eration. However,theultimatepotentialofthesemethodsis EAGLEboastslowtrainingcosts. FortheLLaMA2-Chat
cappedduetotheloweraccuracyofthedraftstheyproduce. 70Bmodel,EAGLEtrainsadecoderlayerwithfewerthan
1Bparametersusingnomorethan70kdialoguesfromthe
Building uponthe observation thatauto-regression atthe
ShareGPTdataset. Thetrainingiscompletedin1-2days
feature level is more manageable than at the token level,
on4xA100(40G)GPUs. Inpracticalapplications,EAGLE
weintroduceasimpleframework,EAGLE(Extrapolation
requiresonlyasingletrainingsessiontoprovideaccelera-
AlgorithmforGreaterLanguage-modelEfficiency),which
tionforeachquery. Asthenumberofqueriesincreases,the
divergesfromdirecttokenpredictionandinsteadperforms
amortizedtrainingcostofEAGLEbecomesnegligible.
auto-regressiveoperationsatthefeaturelevel. Byincorpo-
ratingatokensequenceadvancedbyonetimestep,EAGLE Comparedwithexistingspeculativesampling-basedtech-
circumvents the uncertainty associated with feature-level niques,theadvantagesofEAGLEinclude:
auto-regression. Withthedraftmodelcomprisingmerelya
transformerdecoderlayer,EAGLEachievesadraftaccuracy
‚Ä¢ Simplicity: EAGLEisanewframeworktoaccelerate
ofapproximately0.8,substantiallysurpassingMedusa‚Äôs0.6.
auto-regressivedecodingandisapplicabletoanyauto-
WeevaluatedEAGLEontheMT-bench(Zhengetal.,2023), regressiveLLMs. Themethodaddsonlyalightweight
ahighlyrealisticbenchmarksimulatingactualapplications plug-in (a single transformer decoder layer) to the
andreal-worldscenarios,includingmulti-turninstructions LLM, which can be easily deployed in a production
akin to dialogues with ChatGPT. We have chosen to uti-
1https://github.com/pytorch-labs/gpt-fast
2EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
environment. Inthispaper,wehaveappliedEAGLEto instance,T denotesthesequence(t ,t ,...,t ). Con-
i:j i i+1 j
LLaMA2-Chat(7B,13B,70B),Vicuna(7B,13B,33B) sidering a LLM, given an input token sequence T , the
1:j
andMixtral8x7BInstruct-v0.1inazero-shotwayon LLMacquirescorrespondingembeddingsE throughthe
1:j
theMT-benchandGSM8Kdatasets. embeddinglayer. Theseareprocessedthroughmultiplede-
coderlayerstoyieldfeaturesF . TheLMHeadthenmaps
1:j
‚Ä¢ Reliability: EAGLEdoesnotinvolveanyfine-tuning f onto a distribution p = LMHead(f ), from which
j j+1 j
oftheoriginalLLM,andthepreservationoftheoutput the next token t is sampled. As an example, vanilla
j+1
distributionbyEAGLEistheoreticallyguaranteedfor auto-regressionatthetokenlevelcanbeexpressedbythe
boththegreedyandnon-greedysettings. Thisensures process (T ‚Üí E ‚Üí f ‚Üí p ‚Üí t ) for any
1:j 1:j j j+1 j+1
noriskofdegradationinedgecases,whereacceleration integerj ‚â•1.
mightleadtoerroneousorevenharmfulLLMoutputs.
Speculativesampling. Speculativesamplingalternatesbe-
This is in sharp contrast to Lookahead and Medusa
tweenthedraftandverificationphases.Considerthecurrent
whichfocusesongreedysettingsonly.
sequence T . During the draft phase, speculative sam-
1:j
pling employs a model, smaller than the target LLM, to
‚Ä¢ Speed: EAGLE stands out as the fastest framework
auto-regressivelygenerateŒ≥ tokensTÀÜ ,2andrecords
within the family of speculative sampling as of the j+1:j+Œ≥
submissionofthispaper. Notably,ontheVicunaand the corresponding distributions PÀÜ j+1:j+Œ≥. In the verifica-
LLaMA2-Chatforgreedyinference(seeFigure1),EA- tionphase,asingleforwardpassofthetargetLLMyields
GLEsurpassesvanilladecodingspeedsbyafactorof the probabilities for the corresponding positions. During
3, outpaces Lookahead decoding by 2 times, and is the verification phase, a single forward pass of the target
1.6timesfasterthanMedusadecoding. Usinggpt-fast, LLMprovidestheprobabilitiesforthecorrespondingpo-
EAGLEattainsonaverage160tokens/swithLLaMA2- sitions. Tokensarethensequentiallyevaluatedforaccep-
Chat 13B on a single RTX 3090 GPU, compared to tance, with token tÀÜ j+i having an acceptance probability
24tokens/sofHuggingface‚Äôsimplementations. Inthe min(1,p j+i/pÀÜ j+i). UpontherejectionofatokentÀÜ j+i,all
MoE(MixtureofExperts)model,specificallytheMix- subsequenttokensarediscarded, andthistokenisresam-
tral8x7BInstruct-v0.1(Jiangetal.,2024),theEAGLE pled based on a distribution norm(max(0,p j+i ‚àípÀÜ j+i)).
algorithmachievesaspeedincreaseof1.5times. Appendix A.1 of speculative sampling (Leviathan et al.,
2023)provesthattheaforementionedsamplingmethodis
equivalenttodirectlysamplingfromthedistributionofthe
In addition to introducing the simple yet efficient frame-
targetLLM.EAGLEalsoemploysthissamplingapproach,
workofEAGLE,wealsoprovideananalysisofthefactors
ensuring that the distribution of the generated text re-
contributingtoitseffectiveness,whichmightbeofindepen-
mainsunchangedforboththegreedyandnon-greedy
dentinteresttootherspeculativesamplingmethods(Section
settings.
5.3.2). EAGLEisbuiltuponthefollowingtwoobservations:
3.Observations
‚Ä¢ Firstly,utilizingtop-layerfeaturesprovestobemore
effectivethanemployingbottom-layertokenembed- Webeginwithtwoobservationsthatdirectlymotivatethe
dingswhenusingthesamelightweightnetwork. designofEAGLE.
‚Ä¢ Secondly,theuncertaintyinherentinthesamplingpro-
3.1.Auto-regressingfeaturesiseasierthantokens
cesssignificantlyconstrainstheperformanceofdraft
modelsthatonlyinputtop-layerfeatures. Hence,in- Tokensequencesrepresentsimpletransformationsofnat-
corporatingthesamplingresults(token)intothedraft urallanguageandareinherentlycomplex. Incontrast,the
modeliscrucial. second-top-layerfeaturesofLLMs,afterundergoingalin-
eartransformation(LMHead),canpredictthenexttoken,
making the feature sequences more structured and regu-
2.Preliminaries
lar. Consequently,performingauto-regressionatthefeature
Notations. Inthispaper,werefertotheLargeLanguage levelisexpectedtobemoremanageablethanatthetoken
Modelintendedforaccelerationasthe‚ÄútargetLLM‚Äùandthe level. Wehypothesizethisisbecausehigh-levelfeature‚Äôs
modelusedforrapiddraftgenerationasthe‚Äúdraftmodel‚Äù. evolutionpathovertimeissimplerthanthatoftokens,thus
Unlessspecifiedotherwise,‚Äúfeature‚Äùreferstothesecond- the former can be characterized by a smaller model. To
to-top-layerfeatureofLLM,thatis,thehiddenstatebefore validatethishypothesis,wedesignedexperimentsonMT-
theLMHead. Wedenotetokensbylowercaseletterst,their
2Wewillfrequentlyusethehatnotationstodenotethecorre-
embeddings by e, and features by f, and the distribution
spondingtermsforthedraftmodels.
byp. Weuseuppercaseletterstorepresentsequences. For
3EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
ùëù always ùëù I ùëù am
ùëù(begin)=0.8 ùëù(am)=0.6 ùëù(excited)=0.3
ùëù(look)=0.2 ùëù(always)=0.4 ùëù(ready)=0.7
ùëì always ùëì I ùëì am
sampling sampling
ùëí always ùëí I ùëí am
Figure3: Accuracyandspeedupratioofdraftmodelsbased
ontokensandfeatures,testedonMT-benchwithVicuna7B always I am
asthetargetLLM.
Figure4: Uncertaintyinfeaturesequences. Thenextfea-
ture following f is contingent on the sampling outcome
I
andcannotbedeterminedsolelybasedonf ,whereboth
benchwithVicuna7B(Chiangetal.,2023)bysettingthe I
‚Äúalways‚Äùand‚Äúam‚Äùarepossibletofollowthetoken‚ÄúI‚Äùand
draftmodelsasaone-layertransformerdecoder,forboth
leadtotwobranches.
thefeatureandtokenauto-regression. AsillustratedinFig-
ure3,auto-regressivefeatureprocessingfollowedbytoken
generation((f ,f )‚Üíf ‚Üít )usingthefrozenLMHead
1 2 3 4
yieldsamuchhigheraccuracycomparedtoauto-regressive
tokenprocessing((t ,t ,t )‚Üít ),withanimprovement
1 2 3 4
aslargeas30%. Thisresultsinamuchhigherspeedupratio
too(1.5x‚Üí1.9x).
3.2.Uncertaintymattersinnext-featurepredictions
Figure5: Accuracyandspeedupratioofdraftmodelsbased
When generating text, the target LLM does not directly onfeatureandfeature&shifted-token. Thefeature&shifted-
predict tokens; instead, it forecasts the probability distri- token approach effectively resolves the uncertainty. The
bution of tokens and then samples based on this distribu- modelsweretestedonMT-bench,employingVicuna7Bas
tion, thereby introducing randomness into the generation thetargetLLM.
process. Unliketokens,featuresarehigh-dimensionaland
continuous,precludingtheuseofthesamesampling-based consistentwithotherspeculativesamplingbasedmethods.
approach. ConsidertheexampleillustratedinFigure4. If
‚Äúam‚Äùissampled,thefeaturesequencebecomes(f I,f am). 4.1.Draftingphase
Conversely, if ‚Äúalways‚Äù is sampled, the feature sequence
TheprimarydistinctionbetweenEAGLEandothermethods
becomes(f ,f ). Thisintroducesambiguityintothe
I always
liespredominantlyinthedraftingphase. Figure6illustrates
auto-regressivemodeloffeaturesequences: itisuncertain
a schematic of the drafting phase for different methods.
whether the next feature following f should be f or
I am
Speculativesampling(Leviathanetal.,2023;Chenetal.,
f . Medusaalsoconfrontsthisissue. Itsobjectiveis
always
2023a)andLookahead(Fuetal.,2023)predicttokensbased
topredicttheprobabilityoftokensspacedbyaninterval,
ontokens. Medusa(Caietal.,2023)independentlypredicts
suchasusingf topredictp orp inthisexample,
I am always
t andt usingthefeaturef fromthetargetLLM.EAGLE
correspondingtotheprobabilityofthenexttokenfollowing 4 5 2
predictsf usingthefeaturesequence(f ,f )andthetoken
‚ÄúIam‚Äùor‚ÄúIalways‚Äù,respectively. Thisinherentuncertainty 3 1 2
sequence(t ,t ),advancedbyonetimestep. Fromp =
rendersitimpossibleforMedusa‚Äôsdraftmodeltoperfectly 2 3 4
LMHead(f ),t issampled. Subsequently,f andt are
emulatethetargetLLM.Toaddressthisissue,EAGLEin- 3 4 3 4
concatenated into the input sequence to predict the next
putsthetokensequencefromonetimestepahead,which
featuref andsamplethesubsequenttokent .
includesthesamplingoutcomes, intothedraftmodel. In 4 5
theexampleillustratedinFigure4,thisinvolvespredicting AsillustratedinFigure7,EAGLE‚Äôsdraftmodelcomprises
f alwaysbasedonf I andt always,andpredictingf ambased threemodules: theEmbeddinglayer,LMHead,andAuto-
onf I andt am. AsillustratedinFigure5,byaddressingthe regressionHead. TheEmbeddinglayerandLMHeadem-
uncertainty,thespeedupratiofurtherincreasesfrom1.9xto ploytheparametersoftheoriginalLLManddonotneces-
2.8x. sitateadditionaltraining. Thedraftmodeltakesasinputa
4.EAGLE featuresequenceofshape(bs,seq len,hidden dim)andan
advancedtokensequenceofshape(bs,seq len). Itthencon-
EAGLEisbuiltupontheabovetwoobservations. Itencom- vertsthetokensequenceintoatokenembeddingsequence
passesbothadraftingphaseandaverificationphase,tobe ofshape(bs,seq len,hidden dim),andconcatenatesitto
4EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
make/ a/ with/ the/ to/
Speculative Sampling Lookahead can I help our you your feel
Sampling Sampling multiple times
ùë°1 ùë°2 ùë°3 Smaller LLM ùë°4 ùë°3 2-Gram, Jacobi ùë°4
ùë°1 ùë°2 ùë°3 ùë°4 Smaller LLM ùë°5 ùë°4 2-Gram, Jacobi ùë°5 LM Head LM Head
Medusa EAGLE ùëìhow ùëìcan ùëìI ùëìmake ùëìhelp ùëìwith ùëìyou
ùëì2 Medusa Head1 ùë°4 ùëìùë° 12 ùëìùë° 23 AuEm tob -re ed gd ri en sg
s
il oa ny e Hr e&
ad
ùëì3 ùë°4
Transformer One Auto-regression Head
Layers
Medusa Head2 ùë°5
ùëìùë° 12 ùëìùë° 23 ùëìùë° 34 AuEm tob -re ed gd ri en sg s il oa ny e Hr e& ad ùëì4 ùë°5
ùëíhow ùëícan
ùëì ùëíh co aw
n
ùëìc ùëía In ùëímùëì aI
ke
ùëíhùëì eI
lp
ùëíùëì whe il tp
h
ùëì ùëíh ye ol up
Figure 6: A comparison of the methods for drafting the Embedding Embedding
fourth and fifth tokens, t and t , where (t ,t ) is the
4 5 1 2
How can can I make help with you
prompt. t(representedbyblueblocks)denotestokens,and
Forward 1 Forward 1 Forward 2 Forward 3
f (orangeblocks)signifiesthefeatures,withsubscriptsin-
dicating their positions in the sequence. The red border target LLM Draft model
indicatesthepredictionsofthedraftmodel. Forsimplicity,
theninthen-gramforLookahead,asshowninthefigure,
hasbeensetto2. Query How can
Forward 1
Sampling using Original LLM I
Forward 1
Draftingusing make help
FeatExtrapolator Forward 2
formafusedsequenceofshape(bs,seq len,2√óhidden dim).
a our with you
TheAuto-regressionHeadconsistingofanFClayeranda Forward 3
decoderlayer. TheFClayerreducesthedimensionalityof the your to feel
thefusedsequenceto(bs,seq len,hidden dim)andthenwe
Figure7: PipelineofEAGLE.Theuppersectionillustrates
utilizethedecoderlayertopredictthenextfeature. TheLM
thecomputationalprocess,whilethelowersectiondisplays
Headcalculatesthedistributionbasedonthefeature,from
thecorrespondinggenerationresultsforeachstep. Inthe
whichthenexttokenissampled. Finally,thepredictedfea-
upper section, green blocks represent token embeddings,
tureandthesampledtokenareconcatenatedintotheinput,
orangeblockssignifythefeaturesfromthesecond-top-layer
facilitatingthecontinuationoftheauto-regressiveprocess.
of the LLM, red boxes indicate features predicted by the
EAGLEgeneratesatree-structureddraft. Toenhanceeffi-
Auto-regression Head, and blue modules with snowflake
ciency,weimplementtreeattention,enablingthecreation
iconsrepresenttheuseoforiginalLLMparameters,which
ofadrafttreewithadepthofmthroughmforwardpasses,
arenotsubjecttotraining.
therebyencompassingmorethanmtokens. Intheexample
illustrated in Figure 7, EAGLE drafts a tree of 10 tokens
through3forwardpasses.
p =Softmax(LMHead(f )),
i+2 i+1
4.2.Trainingofthedraftmodels pÀÜ i+2 =Softmax(LMHead(fÀÜ i+1)),
L =CrossEntropy(p ,pÀÜ ).
Predictingthenextfeatureconstitutesaregressiontask,for cls i+2 i+2
whichweemploySmoothL1loss(seeFigure6EAGLE):
By integrating regression loss and classification loss, we
train the Auto-regression Head using the following com-
binedlossfunction:
E =TokenEmbedding(T ),
2:i+1 2:i+1
fÀÜ =Auto-regressionHead(E ,F ), L=L +w L .
i+1 2:i+1 1:i reg cls cls
L =SmoothL1(f ,fÀÜ ).
reg i+1 i+1 Typically, theclassificationlossisanorderofmagnitude
largerthantheregressionlossinnumericalterms. Conse-
quently,wesetw to0.1.
Predictingfeaturesisanintermediaryobjectiveofthedraft cls
model,withtheultimategoalbeingthepredictionoftokens TheoptimalapproachfortrainingtheAuto-regressionHead
to generate a sequence of tokens. Consequently, we also involvesgeneratingtextauto-regressivelyusingthetarget
employclassificationlosstodirectlyoptimizetowardsthis LLM.However,suchdatagenerationiscostly. Fortunately,
finalobjective: EAGLEexhibitslowsensitivitytotrainingdata(ablation
5EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
studyinSection5.3.3). Insteadofemployingtextgenerated
bythetargetLLM,weutilizeafixeddataset,substantially
reducingtheoverhead. Duringthedraftingphase,EAGLE
auto-regressively processes features. Inaccuracies in fea-
tures can lead to error accumulation. To mitigate this is-
sue,weemploydataaugmentationbyaddingrandomnoise
sampled from a uniform distribution U(‚àí0.1,0.1) to the
second-top-layerfeaturevectorsofthetargetLLMduring
training(Jainetal.,2023).
4.3.Verificationphase
Employing tree attention, the target LLM computes the
probabilityofeachtokeninthetree-structureddraftthrough
asingleforwardpass. Ateverylevelofthedrafttree,we
recursivelyapplyspeculativesamplingalgorithmstosample
Figure8: SpeedupratiosofEAGLEacrossvarioustasks.
oradjustthedistribution,consistentwithSpecInfer(Miao
et al., 2023) and SpecTr (Sun et al., 2023), ensuring that
the distribution of the output text aligns with that of the
originalLLM.Simultaneously,werecordtheacceptedto-
kensandtheircorrespondingfeatures,whichserveasthe ‚Ä¢ AcceptancerateŒ±: Theratioofthenumberoftokens
startingpointforauto-regressivefeatureextrapolationinthe acceptedtothenumberoftokensgeneratedduringthe
subsequentdraftphase. draftphase. Theacceptanceratemetricmeasuresthe
accuracyofdrafts. Fortreedraft,multipletokensmay
5.Experiments besampledatthesamelocation, yetonlyonetoken
isaccepted,renderingtheacceptanceratemetricun-
Modelsandtasks. WeconductedexperimentsonVicuna suitableforassessmentinthiscontext. Therefore,for
models(withsizesof7B,13B,33B),LLaMA2-chatmodels theexperimentsreportedinthispaperconcerningac-
(7B, 13B, 70B) and Mixtral 8x7B Instruct-v0.1, encom- ceptancerate,wedonotusetreedraftbutratherchain
passing the common sizes of current mainstream Large draft,similartospeculativesamplingandDistillSpec.
LanguageModels(LLMs). Ourevaluationsspandialogue ThedraftmodelofEAGLEtakesfeaturesequenceand
andmathematicalreasoningtasks. Fordialoguetasks,we a token sequence as input. The auto-regressive pro-
utilizedtheMT-bench,whichcompriseshighlyrealisticsit- cessingoffeaturesinthedraftphasecanleadtothe
uationalquestionsdesignedtotestamodel‚Äôscapabilitiesin propagationoferrors. WedefinetheAcceptancerate
multi-turnconversationsandadherencetoinstructions. For when the input contains n features predicted by the
mathematical reasoning tasks, we employed the GSM8K draftmodel,whichmayincludeerrors,asn-Œ±.
(Cobbeetal.,2021)dataset,consistingofhigh-qualitygrade
schoolmathproblemscreatedbyhumanproblemwriters.
Speculative sampling (Leviathan et al., 2023) conducted
experimentswithabatchsizeof1,asettingsubsequently AccelerationofEAGLEtheoreticallyguaranteesthepreser-
adopted by other works such as DistillSpec (Zhou et al., vation of the original LLMs‚Äô output distribution. Conse-
2023)andBiLD(Kimetal.,2023). Similarly,themajority quently,evaluatingthequalityofEAGLE‚Äôsgeneratedresults
ofourexperimentsalsoadoptedthissetting. Experiments isbothunnecessaryandmeaningless.
withabatchsizegreaterthan1arepresentedinSection5.4.
Training.WefixedthetargetLLMs.EAGLEwastrainedon
Metrics. Similar to other methods based on speculative theShareGPTplatform,utilizing68,000dialogueiterations
sampling,EAGLEprimarilyfocusesonlatencyratherthan withalearningratesetat3e-5. WeemployedtheAdamW
throughput. FollowingthemethodologyofDistillSpec,we optimizer with beta values (Œ≤ ,Œ≤ ) set to (0.9, 0.95) and
1 2
assessaccelerationeffectsusingthefollowingmetrics: implementedgradientclippingof0.5. Thetrainableparame-
tersofEAGLEcorrespondingtothe7B,13B,33B,and70B
‚Ä¢ Walltimespeedupratio: Theactualtestspeedupratio modelsare0.24B,0.37B,0.56B,and0.99B,respectively.
relativetovanillaauto-regressivedecoding. ThetrainableparametersofEAGLEforMoEmodelMixtral
8x7Bis0.28B.EAGLEischaracterizedbyitslowtraining
‚Ä¢ AverageacceptancelengthœÑ: Theaveragenumberof cost;theAuto-regressionHeadistrainablewithin1-2days
tokensacceptedperforwardpassoftheoriginalLLM. onanA10040Gserverforthe70Bmodels.
6EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
5.1.Effectiveness Table 2: Speedup ratio, average acceptance length œÑ and
acceptance rate Œ± on GSM8K. T denotes temperature, V
Figure1illustratesthespeedupratiosofEAGLE,Medusa,
representsVicuna,andLCstandsforLLaMA2-Chat.
andLookaheadontheMT-bench,withtemperature=0. On
theVicuna13BandLLaMA2-Chat13B,70Bmodels,EA-
Model Speedup œÑ 0-Œ± 1-Œ± 2-Œ± 3-Œ± 4-Œ±
GLE achieves a speed that is 3x faster than vanilla auto-
V7B 3.01x 4.00 0.79 0.71 0.70 0.71 0.70
regressive decoding, 2x faster than Lookahead, and 1.6x
V13B 3.08x 3.97 0.79 0.71 0.67 0.68 0.64
faster than Medusa. Figure 2 and Table 2 showcase the V33B 3.25x 3.94 0.79 0.71 0.67 0.67 0.67
accelerationratiosofEAGLEundervarioussettings. Fig- T=0 LC7B 2.91x 3.82 0.75 0.69 0.64 0.65 0.63
LC13B 3.20x 4.03 0.80 0.70 0.70 0.68 0.66
ure 8 presents the speedup ratios of EAGLE across vari-
LC70B 3.03x 3.93 0.77 0.71 0.66 0.64 0.60
ous tasks. The coding task, which involves a substantial
V7B 2.34x 3.29 0.72 0.65 0.66 0.62 0.62
number of fixed templates, exhibits the most significant
V13B 2.57x 3.60 0.73 0.69 0.65 0.64 0.64
speedup effect. EAGLE also demonstrates notable effec- V33B 2.77x 3.60 0.77 0.69 0.66 0.64 0.62
tivenessinothertasks. Tables1displaytheAverageaccep- T=1 LC7B 2.40x 3.52 0.72 0.68 0.63 0.64 0.62
LC13B 2.82x 3.67 0.78 0.70 0.67 0.67 0.66
tancelengthandacceptancerateofEAGLE.Theoriginal
LC70B 2.74x 3.58 0.76 0.69 0.64 0.63 0.62
LLMgeneratesnearlyfourtokensperforwardpass,which
is a significant efficiency improvement compared to the
vanillaauto-regressivedecodingthatgeneratesonetoken
Table3: Speedupratio,averageacceptancelengthœÑ,and
per pass. The acceptance rate with completely accurate
acceptance rate Œ± on MT-bench at temperature=0. The
feature sequences, 0-Œ±, is notably higher than that with
targetLLMisMixtral8x7BInstruct-v0.1.
a single erroneous feature, 1-Œ±, indicating that errors in
featurescanimpactperformanceofEAGLE.However,the
minimaldifferencesbetween1-Œ±,2-Œ±,3-Œ±,and4-Œ±demon- Speedup œÑ 0-Œ± 1-Œ± 2-Œ± 3-Œ± 4-Œ±
strateEAGLE‚Äôsrobustnesstofeatureerrorsanditsability 1.50x 3.25 0.67 0.62 0.61 0.64 0.63
toeffectivelymanagetheaccumulationofsucherrors.
Table1: AverageacceptancelengthœÑ andacceptancerateŒ±
onMT-bench. Tdenotestemperature,VrepresentsVicuna, basedmethods,whichinvolvesforwardingmultipletokens,
andLCstandsforLLaMA2-Chat. mayrequirereadingtheweightsofmorethantwo,oreven
all,experts. Conversely,standarddecoder-onlymodelsne-
Model œÑ 0-Œ± 1-Œ± 2-Œ± 3-Œ± 4-Œ± cessitatereadingallweights,regardlessofwhetheroneor
V7B 3.94 0.79 0.74 0.72 0.73 0.67 multipletokensareforwarded.
V13B 3.98 0.79 0.74 0.72 0.74 0.70
V33B 3.68 0.74 0.69 0.67 0.67 0.66 5.2.Casestudy: EAGLE+gpt-fast
T=0 LC7B 3.62 0.76 0.69 0.67 0.68 0.68
EAGLEiscompatiblewithotheraccelerationtechnologies.
LC13B 3.90 0.77 0.69 0.69 0.70 0.71
LC70B 3.81 0.75 0.69 0.65 0.64 0.64 WeconductedexperimentscombiningEAGLEwithgpt-fast,
whichemploysquantizationandcompilationforaccelera-
V7B 3.17 0.71 0.68 0.66 0.66 0.65
tion. As shown in Figure 4, by integrating EAGLE with
V13B 3.20 0.73 0.68 0.68 0.67 0.69
gpt-fast, we increased the generation speed of LLaMA2-
V33B 3.22 0.71 0.67 0.64 0.64 0.64
T=1 LC7B 3.30 0.71 0.66 0.66 0.66 0.64 Chat7BonasingleRTX3090to160.4tokens/s,compared
LC13B 3.45 0.73 0.69 0.66 0.67 0.67 to24.5tokens/susingHuggingface‚Äôsimplementations.
LC70B 3.46 0.73 0.67 0.64 0.66 0.65
Table4: GenerationspeedofcombiningEAGLEwithgpt-
As shown in Table 3, for the Mixtral 8x7B Instruct-v0.1
fast. TheevaluationdatasetisMT-bench,thetargetLLMis
model,EAGLEachieveda1.5xacceleration. Thecompara-
LLaMA2-Chat13B,andthetemperatureissetto0.
tivelylesseracceleration,asopposedtomodelslikeLLaMA,
canbeattributedpartlytoaloweraverageacceptancelength
Precision FP16 int4
andpartlytotheinherentchallengesinacceleratingMixture
ofExperts(MoE)modelsusingspeculativesampling-based Vanilla(Huggingface) 24.5tokens/s N/A
methods. MoEmodelsselectexpertsonaper-tokenbasis. gpt-fast 55.1tokens/s 106.9tokens/s
Duringvanillaauto-regressivedecoding,forwardingasin- EAGLE+gpt-fast 100.2tokens/s 160.4tokens/s
gletokenentailsreadingtheweightsofonlytwoexperts.
Incontrast,theverificationphaseofspeculativesampling-
7EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
onVicuna7B,assessingdraftmodelswithvaryinginputs.
Wetestedfourtypesofinputs: feature&shifted-token(EA-
GLE),feature&unshifted-token,token,andfeature. Both
feature&shifted-token (EAGLE) and feature&unshifted-
tokenintegratesemanticinformationatdifferentlevels. The
distinctionliesinthefactthatfeature&shifted-token(EA-
GLE)inputstokensadvancedbyonetimestep,equipping
ittoaddressrandomnesseffectively. Apartfromtheuseof
aFClayertoreducedimensionalityforthefeature&token
Figure9: SpeedupratiosofEAGLEwithandwithoutthe input,thestructureofthedraftmodelremainsentirelycon-
useoftreeattention. TheevaluationdatasetisMT-bench, sistent. Figure10presentstheexperimentaloutcomeson
withthetemperatureparametersetto0. the MT-bench with Vicuna 7B as the target LLM. Three
observationscanbedrawn.
Table5: AverageacceptancelengthœÑ ofEAGLEwithand
withouttheuseoftreeattention. Theevaluationdatasetis ‚Ä¢ First, when the number of parameters of the draft
MT-bench,withthetemperatureparametersetto0. modelislimited,utilizingfeaturesyieldsslightlybetter
resultsthantokens.
Vicuna LLaMA2-Chat
‚Ä¢ Second, integrating features and tokens slightly en-
Size Chain Tree Size Chain Tree
hancesperformance.Thisisprimarilybecausediscrete,
7B 3.20 3.94(+0.74) 7B 3.00 3.62(+0.62) error-freetokensmitigatetheaccumulationoferrorsin
13B 3.23 3.98(+0.75) 13B 3.18 3.90(+0.68) features(asevidencedbythenear-equivalenceofthe
33B 2.97 3.68(+0.71) 70B 3.12 3.81(+0.69)
acceptancerateofthefeature&unshifted-tokendraft
modeltothatofthefeature-onlymodel,0-Œ±,butwith
amarkedlyhigher1-Œ±).
5.3.Ablationstudy
‚Ä¢ Third,addressingtherandomnessinherentinthesam-
5.3.1.TREEATTENTION plingprocessresultsinthemostsignificantimprove-
ment. Thefeature&shifted-tokenscheme,compared
EAGLE, similar to SpecInfer and Medusa, employs tree
to feature&unshifted-token, adds no complexity yet
attention, where both the generation and validation of
markedlyenhancesthedraftmodel‚Äôscapabilitybysim-
draftsaretree-structured. Incontrast,methodslikespecula-
plyadvancingthetokenbyonetimestep,allowingthe
tivesamplingdonotusetreeattention,resultinginchain-
draftmodeltoaccountfortherandomnessinsampling.
structureddraftgenerationandvalidation. Table5andFig-
ure9presentcomparativeresultsindicatingtheimpactof
usingtreeattention. Theimplementationoftreedraftand
5.3.3.TRAININGDATA
verificationinEAGLEresultsinanapproximateincreaseof
0.6-0.8intheaverageacceptancelengthandabout0.3-0.5 EAGLEutilizesafixeddatasetfortraining. Employingthe
inthespeedupratio. Comparedtochaindraftandverifica- targetLLMforauto-regressivedecodingondatasetqueries
tion,treedraftandverificationdonotincreasethenumber togeneratetrainingdatamayyieldenhancedperformance
offorwardpassesinthemodel(boththetargetLLMand butwouldsubstantiallyincreasetrainingoverhead. Wecon-
thedraftmodel),buttheydoincreasethenumberoftokens ductedablationexperimentsontheMT-benchdataset. As
processedperforwardpass.Consequently,theimprovement showninTable6,trainingthedraftmodelwithdatagener-
in the speedup ratio is less pronounced than the increase atedbythetargetLLMleadstoonlyaslightimprovement
in average acceptance length. Notably, even without em- inperformance. ThissuggeststhatEAGLEisnotsensitive
ployingtreedraftandverification,EAGLEdemonstratesa to the training data, allowing for the adoption of a fixed
significantaccelerationeffect,approximatelyintherange dataset approach. Consequently, this choice significantly
of2.2x-2.7x. reducesthetrainingcostsofEAGLE.
5.3.2.INPUTSOFDRAFTMODELS 5.4.Batchsizeandthroughput
Compared to other speculative sampling-based methods, Inference in LLMs is memory-bound (Patterson, 2004;
thekeyinnovationofEAGLEliesinitsutilizationoffea- Shazeer,2019),leavingGPUcomputationalresourcesun-
tures computed by the target LLM and the incorporation derutilized. Theprinciplebehindthespeculativesampling-
of sampling outcomes into the input of the draft model basedapproachinenhancinggenerationspeedliesinmore
to address randomness. We conducted an ablation study effectivelyutilizingGPUcomputationalresources. Asthe
8EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
feature&shifted-token feature&unshifted-token token feature
Speedup 0- 1-
0.8
2.5 0.6
3 0.6
2.0
0.4
0.4
1.5 2
0.2
2.5 5.0 2.5 5.0 2.5 5.0 2.5 5.0
Epoch Epoch Epoch Epoch
2.0 3.0 0.6
0.6
2.5
0.4
1.5 0.4
2.0
0.2
1.5 0.2
1.0
2.5 5.0 2.5 5.0 2.5 5.0 2.5 5.0
Epoch Epoch Epoch Epoch
Figure10: Performanceofdraftmodelswithvaryinginputs. ThetargetLLMisVicuna7B,andthetestdatasetisMT-bench.
Speedreferstothewalltimespeedupratio,œÑ denotestheaverageacceptancelength,0-Œ±representstheacceptanceratewith
entirelypreciseinputs,1-Œ±indicatestheacceptanceratewhentheinputincludesoneimprecisefeature,andT referstothe
temperature.
Table6:ThespeedupratiosandaverageacceptancelengthœÑ Although speculative sampling-based methods predomi-
usingdifferenttrainingdatasetsevaluatedontheMT-bench, nantly focus on latency, we also investigated EAGLE‚Äôs
withthetargetLLMbeingLLaMA2-Chat7Bandthetem- throughputforbatchsize>1,anotherkeymetricforLLM
peraturesetto0. ‚ÄúFixeddataset‚Äùreferstobothquestions systems. Compared to vanilla auto-regressive decoding,
andanswersoriginatingfromtheShareGPTdataset. ‚ÄúData EAGLE requires roughly the same CUDA memory. For
generatedbytargetLLM‚Äùdenotesthatwhilequestionsare Vicuna7BasthetargetLLM,operatingunderamemory
sourcedfromtheShareGPTdataset,theanswersaregener- constraintofasingleRTX3090with24GofCUDAmem-
atedbythetargetLLM. ory,themaximumbatchsize(bs)forvanillaauto-regressive
decoding and EAGLE are 8 and 7, respectively. In the
Trainingdata Speedup œÑ caseofLLaMA2-Chat70B,constrainedby4A100(40G)
GPUstotaling160GofCUDAmemory,themaximumbs
Fixeddataset 2.78x 3.62
forvanillaauto-regressivedecodingandEAGLEare5and
DatageneratedbytargetLLM 2.88x 3.75
4, respectively. All evaluations were conducted at FP16
precision. Wecalculatedthethroughputfordifferentbsand
selectedthemaximumvalue. Bothvanillaauto-regressive
decodingandEAGLEachievemaximumthroughputattheir
batchsizeincreases,theavailablecomputationalcapacity
respective maximum bs. Tree attention consumes more
of the GPU decreases, leading to a reduction in the ac-
computationalresources. Atabatchofbs=7,thecomputa-
celerationeffect. Inthissection,wepresentexperimental
tionalresourcesarelessabundant,makingthenon-useof
results for scenarios where the batch size exceeds 1. As
treeattentionmoreadvantageous. AsillustratedinTable7,
demonstratedinTable7,thespeedupratiodiminisheswith
EAGLEachievesa2xincreaseinthroughput.
increasingbatchsize. WhenusingVicuna7Basthetarget
LLM,thespeedupratioatbs=4ishigherthanatbs=3. This
isattributedtothefactthat, duringtheverificationphase 6.RelatedWork
ofEAGLE,thetargetLLMprocessesmultipletokensina
Therehasbeenconsiderableresearchintoacceleratinglan-
singleforwardpass,andtheprocessingatbs=4isfasterthan
guagemodels,involvingtechniquessuchasdistillation(Hin-
atbs=3. Incontrast,withvanillaauto-regressivedecoding
tonetal.,2015),quantization(Hubaraetal.,2018). These
wherethetargetLLMprocessesonetokenperforwardpass,
methodsaimtoreducethelatencyperforwardpass.
thespeedsatbs=3andbs=4arenearlyidentical.
9
0=T
1=TEAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
Table7: Speedupratiosatdifferentbatchsizesandthrough- outputdistributionoftheLLMwhilesignificantlyenhancing
putofEAGLE.TheevaluationdatasetisMT-bench,with generationspeed. OnMT-bench,EAGLEis3xfasterthan
thetemperatureparametersetto0. vanillaauto-regressivedecoding,2xfasterthanLookahead,
and1.6xfasterthanMedusa.
Batchsize 1 2 3 4 Throughput
Acknowledgements. Weacknowledgeusefuldiscussions
Vicuna7B 2.90x 2.87x 2.65x 2.76x 1.97x
withtheMedusa‚ÄôsteamleaderTianleCai,theLookahead‚Äôs
LLaMA2-Chat70B 3.01x 2.81x 2.50x 2.40x 1.99x
teamleaderHaoZhang,interactionswiththegpt-fastteam
leadersHoraceHeandSoumithChintalaonX,andYihan
Wu.
Similartoourapproachareframeworksbasedonspecula-
tivesampling. Earlyworks(Sternetal.,2018;Sunetal.,
2021)acceleratedgreedydecoding,whilespeculativesam- References
pling(Leviathanetal.,2023;Chenetal.,2023a)extendedit
Cai, T., Li, Y., Geng, Z., Peng, H., and Dao, T. Medusa:
tonon-greedysampling,provablymaintainingtheoriginal
SimpleframeworkforacceleratingLLMgenerationwith
outputdistribution. Ensuringunchangedoutputdistribution
multiple decoding heads. https://github.com/
makes acceleration more challenging; many studies have
FasterDecoding/Medusa,2023.
explored lossy acceleration as a trade-off. For instance,
DistillSpec(Zhouetal.,2023)modifiesacceptanceproba-
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,
bilitiesusingaleniencefunction,BiLD(Kimetal.,2023)
L., and Jumper, J. Accelerating large language model
acceptsdraftsifthedistancemetricfromtheoriginalLLM
decoding with speculative sampling. arXiv preprint
distributionisbelowacertainthreshold,andMedusa(Cai
arXiv:2302.01318,2023a.
et al., 2023) uses a minimum of a hard threshold and an
entropy-dependentthresholdfortruncation. Incontrast,EA- Chen,Z.,Yang,X.,Lin,J.,Sun,C.,Huang,J.,andChang,K.
GLE does not employ any relaxations and maintains the C.-C. CascadespeculativedraftingforevenfasterLLM
outputdistributionoftheLLMunchanged. inference. arXivpreprintarXiv:2312.11462,2023b.
Theprimarydifferencesamongspeculativesampling-based
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
methodsmanifestpredominantlyinthedraftphase. Specu-
H.,Zheng,L.,Zhuang,S.,Zhuang,Y.,Gonzalez,J.E.,
lativesampling(Leviathanetal.,2023;Chenetal.,2023a)
Stoica, I., and Xing, E. P. Vicuna: An open-source
utilizesalower-parameterversionoftheoriginalLLMas
chatbot impressing gpt-4 with 90%* chatgpt quality,
thedraftmodel. Self-SpeculativeDecoding(Zhangetal., March 2023. URL https://lmsys.org/blog/
2023)skipssomelayersoftheoriginalLLMduringdraft 2023-03-30-vicuna/.
generation. SpecInfer(Miaoetal.,2023)employsasetof
smallmodelstogeneratedraftsinparallel. CascadeSpecu- Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H.,
lativeDrafting(Chenetal.,2023b)andStagedSpeculative Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano,
Decoding(Spector&Re,2023)cascadedifferentoverhead R.,etal. Trainingverifierstosolvemathwordproblems.
draftmodels.OnlineSpeculativeDecoding(Liuetal.,2023) arXivpreprintarXiv:2110.14168,2021.
trainsthedraftmodelonadistributionofqueries. Methods
(Hooperetal.,2023;Fuetal.,2023;Yangetal.,2023b)such Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break-
asMedusa(Caietal.,2023)donotemployaseparatetarget ing the sequential dependency of LLM infer-
LLM;instead,theygeneratedraftsbyutilizingfeaturesor ence using lookahead decoding, November
weightsfromthetargetLLM.REST(Heetal.,2023)gener- 2023. URL https://lmsys.org/blog/
atesdraftsbasedonretrievalmethods. LLMA(Yangetal.,
2023-11-21-lookahead-decoding/.
2023a), usedfortaskslikegrammaticalcorrectionwhere
He, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest:
inputandoutputoverlap,retrievesdraftsdirectlyfromthe
Retrieval-based speculative decoding. arXiv preprint
input.
arXiv:2311.08252,2023.
7.Conclusion Hinton, G., Vinyals, O., and Dean, J. Distilling
the knowledge in a neural network. arXiv preprint
Inthispaper,weintroduceEAGLE,anefficientframework arXiv:1503.02531,2015.
for speculative sampling. EAGLE conducts the drafting
process auto-regressively at the more structured (second- Hooper, C., Kim, S., Mohammadzadeh, H., Genc, H.,
top-layer)featurelevelandmitigatessamplinguncertainty Keutzer, K., Gholami, A., and Shao, S. Speed: Spec-
inpredictingthenextfeaturebyincorporatingtokensfrom ulativepipelinedexecutionforefficientdecoding. arXiv
onetimestepahead. EAGLEisguaranteedtopreservethe preprintarXiv:2310.12072,2023.
10EAGLE:SpeculativeSamplingRequiresRethinkingFeatureUncertainty
Hubara,I.,Courbariaux,M.,Soudry,D.,El-Yaniv,R.,and Stern,M.,Shazeer,N.,andUszkoreit,J. Blockwiseparallel
Bengio,Y. Quantizedneuralnetworks: Trainingneural decodingfordeepautoregressivemodels. Advancesin
networkswithlowprecisionweightsandactivations.jour- NeuralInformationProcessingSystems,31,2018.
nalofmachinelearningresearch,18(187):1‚Äì30,2018.
Sun,X.,Ge,T.,Wei,F.,andWang,H. Instantaneousgram-
Jain, N., Chiang, P.-y., Wen, Y., Kirchenbauer, J., Chu, maticalerrorcorrectionwithshallowaggressivedecoding.
H.-M.,Somepalli,G.,Bartoldson,B.R.,Kailkhura,B., arXivpreprintarXiv:2106.04970,2021.
Schwarzschild,A.,Saha,A.,etal. NEFTune: Noisyem-
Sun, Z., Suresh, A. T., Ro, J. H., Beirami, A., Jain, H.,
beddingsimproveinstructionfinetuning. arXivpreprint
andYu,F. Spectr: Fastspeculativedecodingviaoptimal
arXiv:2310.05914,2023.
transport. arXivpreprintarXiv:2310.15141,2023.
Jiang,A.Q.,Sablayrolles,A.,Roux,A.,Mensch,A.,Savary,
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
B.,Bamford,C.,Chaplot,D.S.,Casas,D.d.l.,Hanna,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
E. B., Bressand, F., et al. Mixtral of experts. arXiv
Bhosale,S.,etal. LlAMA2: Openfoundationandfine-
preprintarXiv:2401.04088,2024.
tuned chat models. arXiv preprint arXiv:2307.09288,
2023.
Kim, S., Mangalam, K., Moon, S., Malik, J., Mahoney,
M.W.,Gholami,A.,andKeutzer,K. Speculativedecod- Xia, H., Ge, T., Wang, P., Chen, S.-Q., Wei, F., and Sui,
ingwithbiglittledecoder. InThirty-seventhConference Z. Speculativedecoding: Exploitingspeculativeexecu-
onNeuralInformationProcessingSystems,2023. tionforacceleratingseq2seqgeneration. InFindingsof
theAssociationforComputationalLinguistics: EMNLP
Leviathan,Y.,Kalman,M.,andMatias,Y. Fastinference
2023,pp.3909‚Äì3925,2023.
from transformers via speculative decoding. In Inter-
nationalConferenceonMachineLearning,pp.19274‚Äì Yang, N., Ge, T., Wang, L., Jiao, B., Jiang, D., Yang, L.,
19286.PMLR,2023. Majumder, R., and Wei, F. Inference with reference:
Lossless acceleration of large language models. arXiv
Liu, X., Hu, L., Bailis, P., Stoica, I., Deng, Z., Cheung, preprintarXiv:2304.04487,2023a.
A.,andZhang,H. Onlinespeculativedecoding. arXiv
preprintarXiv:2310.07177,2023. Yang, S., Lee, G., Cho, J., Papailiopoulos, D., and
Lee, K. Predictive pipelined decoding: A compute-
Miao,X.,Oliaro,G.,Zhang,Z.,Cheng,X.,Wang,Z.,Wong, latencytrade-offforexactllmdecoding. arXivpreprint
R.Y.Y., Chen, Z., Arfeen, D., Abhyankar, R., andJia, arXiv:2307.05908,2023b.
Z. SpecInfer: AcceleratinggenerativeLLMservingwith
Zhang,J.,Wang,J.,Li,H.,Shou,L.,Chen,K.,Chen,G.,
speculativeinferenceandtokentreeverification. arXiv
andMehrotra,S. Draft&verify: Losslesslargelanguage
preprintarXiv:2305.09781,2023.
modelaccelerationviaself-speculativedecoding. arXiv
Patterson,D.A. Latencylagsbandwith. Communications preprintarXiv:2309.08168,2023.
oftheACM,47(10):71‚Äì75,2004.
Zhang, P., Zeng, G., Wang, T., and Lu, W. TinyLlama:
Anopen-sourcesmalllanguagemodel. arXivpreprint
Santilli, A., Severino, S., Postolache, E., Maiorca, V.,
arXiv:2401.02385,2024.
Mancusi, M., Marin, R., and Rodola, E. Accelerat-
ingtransformerinferencefortranslationviaparallelde-
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
coding. In Rogers, A., Boyd-Graber, J., and Okazaki,
Zhuang,Y.,Lin,Z.,Li,Z.,Li,D.,Xing,E.,etal. Judging
N. (eds.), Proceedings of the 61st Annual Meeting of
llm-as-a-judgewithmt-benchandchatbotarena. arXiv
theAssociationforComputationalLinguistics(Volume
preprintarXiv:2306.05685,2023.
1: Long Papers), pp. 12336‚Äì12355, Toronto, Canada,
July 2023. Association for Computational Linguistics. Zhou, Y., Lyu, K., Rawat, A. S., Menon, A. K., Ros-
doi: 10.18653/v1/2023.acl-long.689. URL https: tamizadeh,A.,Kumar,S.,Kagy,J.-F.,andAgarwal,R.
//aclanthology.org/2023.acl-long.689. DistillSpec: Improvingspeculativedecodingviaknowl-
edgedistillation. arXivpreprintarXiv:2310.08461,2023.
Shazeer,N. Fasttransformerdecoding: Onewrite-headis
allyouneed. arXivpreprintarXiv:1911.02150,2019.
Spector, B. and Re, C. Accelerating LLM inference
with staged speculative decoding. arXiv preprint
arXiv:2308.04623,2023.
11