Rate Analysis of Coupled Distributed Stochastic Approximation for Misspecified
âœ©
Optimization
YaqunYanga,JinlongLeib,âˆ—
aDepartmentofControlScienceandEngineering,TongjiUniversity,Shanghai,201804,China
bDepartmentofControlScienceandEngineering,TongjiUniversity,Shanghai,201804,China;ShanghaiInstituteofIntelligentScienceand
Technology,TongjiUniversity,Shanghai,200092,China
Abstract
Weconsiderannagentsdistributedoptimizationproblemwithimperfectinformationcharacterizedinaparamet-
ricsense,wheretheunknownparametercanbesolvedbyadistinctdistributedparameterlearningproblem. Though
eachagent onlyhas accessto itslocal parameterlearning andcomputationalproblem, theymean tocollaboratively
minimizetheaverageoftheirlocalcostfunctions. Toaddressthespecialoptimizationproblem,weproposeacoupled
distributedstochasticapproximationalgorithm, in whicheveryagentupdatesthecurrentbeliefs ofitsunknownpa-
rameteranddecisionvariablebystochasticapproximationmethod;thenaveragesthebeliefsanddecisionvariablesof
itsneighborsovernetworkinconsensusprotocol. Ourinterestliesintheconvergenceanalysisofthisalgorithm. We
quantitativelycharacterizethefactorsthataffectthealgorithmperformance,andprovethatthemean-squarederrorof
(cid:18) (cid:19)
thedecisionvariableisboundedbyO( n1 k)+O âˆš n(11
âˆ’Ïw)
k11
.5
+O(cid:0) (1âˆ’1 Ïw)2(cid:1) k1 2,wherekistheiterationcountand(1âˆ’Ï w)
isthespectralgapofthenetworkweightedadjacencymatrix.Itrevealsthatthenetworkconnectivitycharacterizedby
(1âˆ’Ï )onlyinfluencesthehighorderofconvergencerate,whilethedomainratestillactsthesameasthecentralized
w
algorithm.Inaddition,weanalyzethatthetransientiterationneededforreachingitsdominantrateO(1)isO( n ).
NumericalexperimentsarecarriedouttodemonstratethetheoreticalresultsbytakingdifferentCPUsan sk agents( ,1 wâˆ’Ï hw) i2
ch
ismoreapplicabletoreal-worlddistributedscenarios.
Keywords: DistributedCoupledOptimization,StochasticApproximation,Misspecification,ConvergenceRate
Analysis
1. Introduction
Inrecentyears,distributedoptimizationhasdrawnmuchresearchattentioninvariousfieldsincludingeconomic
dispatch[1,2],smartgrids[3,4,5],automaticcontrols[6,7,8]andmachinelearning[9,10]. Indistributedscenarios,
each agent only preserves its local information, while they can exchange information with others over a connected
network to cooperatively minimize the average of all agentsâ€™ cost functions [11, 12]. There are several approaches
for resolving distributed optimization problems such as (primary) consensus-based, duality-based, and constraint
exchangemethods,wheretheprimalapproachescharacterizedbygradient-basedalgorithmshaveattractedthemost
researchattentionduetotheirsatisfactoryperformanceandwell-scalablenature[13]. Thedistributeddualapproaches
based on Lagrange method regularly use dual decomposition like the alternating direction method of multipliers
(ADMM)[14]. Constraintexchangemethodisanotherprevalentschemewheretheinformationexchangedbyagents
amountstoconstraints[15].
âœ©ThepaperwassponsoredbytheNationalKeyResearchandDevelopmentProgramofChinaunderNo2022YFA1004701,theNationalNatural
ScienceFoundationofChinaunderNo.72271187andNo.62373283,andpartiallybyShanghaiMunicipalScienceandTechnologyMajorProject
No.2021SHZDZX0100,andNationalNaturalScienceFoundationofChina(GrantNo.62088101).
âˆ—Correspondingauthor
Emailaddress:yangyaqun@tongji.edu.cn, leijinlong@tongji.edu.cn(JinlongLei)
4202
rpA
12
]CO.htam[
1v96631.4042:viXraHowever, among various formulations in distributed optimization, a crucial assumption is that we need precise
objective functions (or problem information), i.e., all parameters in the model are precisely known. Yet in many
economicandengineeringproblems,parametersofthefunctionsareunknownbutwemayhaveaccesstoobservations
thatcanaidinresolvingthismisspecification. Forexample,intheMarkowitzprofileproblem,itisroutinelyassumed
thattheexpectationorcovariancematricesassociatedwithacollectionofstocksareaccuratelyavailable,butinreality,
itneedsempiricalestimatesviapastdata[16].
This paper is devoted to proposing distributed algorithms for resolving optimization problems with parametric
misspecification, and quantitatively characterizing the influence of network properties, the heterogeneity of agents,
initialstates,etc. onthealgorithmperformance. Thisworkisprimarilycenteredaroundconductingacomprehensive
theoreticalanalysisofconvergence. Webeginbyinitiatingtheproblemformulation.
1.1. ProblemFormulation
Inthisarticle,weconsiderastaticmisspecifieddistributedoptimizationproblemdefinedasfollows:
1(cid:88)n
C (Î¸ ): min f(x,Î¸ )= f(x,Î¸ ), (1)
x âˆ— xâˆˆRp âˆ— n i=1 i âˆ—
where f(x,Î¸ ) â‰œ E[fËœ(x,Î¸,Î¾)]isthelocalcostfunctiononlyaccessibleforagenti âˆˆ N â‰œ {1,2,...,n}. Supposethat
i âˆ— i i
foranyi âˆˆ N,Î¾ : â„¦ â†’ Rd aremutuallyindependentrandomvariablesdefinedonaprobabilityspace(â„¦ ,F ,P ).
i x x x x
Here,Î¸ âˆˆRqdenotestheunknownparameter,whichisasolutiontoadistinctconvexproblem.
âˆ—
1(cid:88)n
L : minh(Î¸)= h(Î¸), (2)
Î¸ Î¸âˆˆRq n i=1 i
whereh(Î¸)â‰œE[hËœ (Î¸,Î¶)]isthelocalparameterlearningfunctiononlyaccessibleforagentiâˆˆN,andforanyiâˆˆN,
i i i
Î¶ : â„¦ â†’ Rm are mutually independent random variables defined on a probability space (â„¦ ,F ,P ). Problems in
i Î¸ Î¸ Î¸ Î¸
theformeq.(1)andeq.(2)jointlyformulateanunknowncoupleddistributedoptimizationschemeconsistingofboth
computationalproblemandlearningproblem,wherethelearningproblemisindependentofthecomputationalone.
Wehavedepictedtheproblemsettinginfig.1.
(ğ’™,ğœ½)âˆˆâ„ğ’‘Ã—ğ’’
ğ’‡ (ğ’™,ğœ½)
ğ’Š
ğœ½âˆˆâ„ğ’’
ğ’‰ (ğœ½)
ğ’Š
agent ğ‘–
Figure 1: The problem setup: a connected network of communicating agents, where each agent preserving a local learning problem hi and
computationalproblem ficorrelatedwithhithroughtheunknownparameterÎ¸,whiletheycooperatetosolvethedistributedcoupledoptimization
problem.
1.2. PriorWork
Wenowgiveareviewofpriorworkforresolvingoptimizationproblemswithunknownparameters.
2Robust optimization approach. Robust optimization considers the optimization problem where the parameter Î¸
is unavailable but one can have access to its uncertainty set, say U [17]. The key idea is to optimize against the
Î¸
worst-caserealizationwithinthisset,i.e.,
minmax f(x,Î¸).
xâˆˆRpÎ¸âˆˆU
Î¸
Robustoptimizationisshowntobeausefultechniqueintheresolutionofproblemsarisingfromcontrol,design,and
optimization [18]. However, it usually produces conservative solutions and sometimes is intractable when poor set
U ischosen(e.g. thesetisgivenbyunexplicitsystemsofnon-convexinequalities)[19].
Î¸
Stochasticoptimization. Unlikerobustoptimization,inastochasticoptimizationscenarioonemayobtainstatis-
tical or distributional information about the unknown parameter. For example, Î¸ follows a probability distribution
D[20],theoptimalsolutionisgainedbyminimizingtheexpectationofcostfunctions,
minE [f(x,Î¸)].
Î¸âˆ¼D
xâˆˆRp
Stochasticoptimizationhasbeenwidelyinvestigatedintelecommunication,financeandmachinelearning[21].In
thescenarioofamulti-agentnetworkdealingwithlargedatasets,stochasticoptimizationhasbecomepopularsinceit
ischallengingtocalculatetheexactgradientwhilethestochasticgradientismucheasiertoobtain.Akeyshortcoming
inusingstochasticoptimizationmodelsforresolvingoptimizationproblemswithunknownparametersliesinthatit
needsthedistributionofÎ¸,whichmightbeastringentrequirementwhentheavailabledataforestimatingislimitedor
noisy. Insuchcases,theresultingdistributionestimatesmaybeunreliableorbiased,leadingtosuboptimalsolutions
oreveninfeasiblesolutions[22]. Alternatively,supposethatÎ¸ canbelearntbyasuitablydefinedestimationproblem,
âˆ—
thenitbringsaboutthefollowingapproach.
Data-driven learning approach. As data availability reaches hitherto unseen in recent years, we can use data-
drivenapproachestolessenoreveneliminatetheimpactofmodeluncertainty. Forexample,themodelparameterÎ¸
canbeobtainedbysolvingasuitablydefinedlearningprobleml(Î¸)(seee.g.,[23]),
(cid:40) (cid:41)
min f(x,Î¸ ):Î¸ âˆˆargminl(Î¸) . (3)
âˆ— âˆ—
xâˆˆRp Î¸âˆˆRq
Computationalevidenceinportfoliomanagementandqueueingconfirmthatdata-drivensetssignificantlyoutperform
traditionalrobustoptimizationtechniques[19].
Anaturalquestioniswhetherthisproblemcouldbesolvedinasequentialmethod,i.e.,firstaccomplishestimating
Î¸ with high accuracy and then solve the core computational optimization problem with the achieved estimation Î¸Ë†.
âˆ—
However,theyhavesomedisadvantagesdiscussedin[23,24,25]:ontheonehand,thelarge-scaleparameterlearning
problemwillleadtolongtimewaitingforsolvingtheoriginalproblem. Ontheotherhand,thisschemeprovidesan
approximatesolutionÎ¸Ë†,thenthecorrupterrormightpropagatesintothecomputationalproblem. Assuch,sequential
methods cannot provide asymptotically accurate convergence. Therefore, an alternative simultaneous approach is
designed(seee.g., [23,26]), whichuseobservationstogetanestimationÎ¸ ofunknownparametersÎ¸âˆ— ateachtime
k
instantk;thenupdatetheupperoptimizationproblembytakingtheestimatedparameterÎ¸ asâ€œtrueâ€parameter. This
k
simultaneousapproachcangenerateasequence{(x ,Î¸ )}thatconvergestoaminimizerof f(x,Î¸ )andl(Î¸)respectively
k k âˆ—
[23].
Suchdata-drivenlearningapproachesforunknownparameterhasgraduallyattractedresearchattentionrecently.
Forexample,theauthorsof[23]presentedacentralizedcoupledstochasticoptimizationschemetosolveproblem(3)
andshowedtheconvergencepropertiesinregimeswhenthefunctioniseitherstronglyconvexormerelyconvex.Then
[25]extendeditsmoothornonsmoothfunctions f andpresentedanaveraging-basedsubgradientapproach,butitis
stillacentralizedscheme. Inaddition,theauthorsof[24]dividedtheoptimizationproblemwithuncertaintyintotwo
paradigms: robustoptimizationandjointestimationoptimization,andtheyexploitedthesetwoproblemstructuresin
onlineconvexoptimizationandgaveregretanalysisunderdifferentconditions. Therecentwork[16]investigatedthe
misspecified conicconvex programs, anddeveloped acentralizedfirst-order inexactaugmented Lagrangianscheme
forcomputingtheoptimalsolutionwhilesimultaneouslylearningtheunknownparameters.Theaforementionedwork
[16,24,23,25]allinvestigatedcentralizedmethods,whiletherearesomeotherworkexploitdistributedapproaches.
Forexample,[27]consideredthedistributedstochasticoptimizationwithimperfectinformation,whileitonlyshowed
3thatthegeneratediteratesconvergealmostsurelytotheoptimalsolution.Thoughthework[28]presentedadistributed
problem with a composite structure consisting of an exact engineering part and an unknown personalized part, it
exhibitsaboundedregretundercertainconditions.
1.3. GapsandMotivation
Recallingtheproblemsetupinsection1.1,ourresearchfallsintodistributeddata-drivenstochasticoptimization
scenario. Takingintoaccounttheresearchthatismostpertinenttothispaper, themajorityofpreviousstudieshave
primarily concentrated on centralized inquiries (see e.g. [16, 24, 23, 25]), while the distributed schemes [27, 28]
mainlyinvestigatedtheasymptoticconvergence. Itremainsunknownhowtodesignanefficientdistributedalgorithm,
how does the network connectivity influence the algorithm performance, and whether the rate can reach the same
order as the centralized scheme? To be specific, this paper is motivated by the following gaps: (i) previous work
on unknown parameter learning problems focused on the centralized scheme, the distributed data-driven stochastic
approximationmethodislessstudied; (ii)thediscussionofconvergenceanalysisespeciallyhowfactorssuchasthe
numberofagents,thenetworkconnectivity,andtheheterogeneityofagentsinfluencetherateofalgorithmisrarely
studiedindetails;(iii)thegapbetweencentralizedanddistributedalgorithmunderimperfectinformationneedtobe
specified,orinotherwordscanwefindthetransienttimewhentherateofdistributedalgorithmsreachthesameorder
asthatofthecentralizedscheme?
1.4. OutlineandContributions
Toaddressthesegaps,weproposeadata-drivencoupleddistributedstochasticapproximationmethodtoresolve
thisspecialoptimizationproblemandgiveapreciseconvergencerateanalysisofouralgorithm. Themaincontribu-
tionsaresummarizedasfollows,andthecomparisonwithpreviousworksisshownintable1.
Table1:Workcomparationwithpreviousstudies
Paper Distributed ImperctectInformation Stochastic Rate FactorInfluence
(cid:16) (cid:17)
[24,23,25] (cid:37) (cid:33) (cid:37) O 1 (cid:37)
(cid:16)k(cid:17)
[29,30] (cid:33) (cid:37) (cid:33) O 1 (cid:33)
k
[28] (cid:33) (cid:33) (cid:37) \ (cid:33)
[27] (cid:33) (cid:33) (cid:33) \ (cid:37)
(cid:18) (cid:19)
[31] (cid:33) (cid:37) (cid:37) O âˆš1 (cid:37)
(cid:16) k(cid:17)
OurWork (cid:33) (cid:33) (cid:33) O 1 (cid:33)
k
(1)Weproposeacoupleddistributedstochasticapproximationalgorithmthatgeneratesiterates{(xxx(k),Î¸Î¸Î¸(k))}for
the distributed stochastic optimization problem (1) with the unknown parameter learning prescribed by a separate
distributedstochasticoptimizationproblem(2). Ourmodelframeworkbuildsuponpreviousresearchinvolvingdeter-
ministicandstochasticgradientschemes. Thisisparticularlyrelevantforcertainstudieswherewaitingforparameter
learningtocompleteoveranextendedperiodisnotfeasible,orforreal-worldproblemsinwhichparameterlearning
andobjectiveoptimizationareintertwined.
(2) We characterize the convergence rate of the presented algorithm that combined the distributed consensus
protocol with stochastic gradient descent methods. On the one hand, we prove that the upper bound of expected
consensuserrorforeveryagentdecayatrateO(1);ontheotherhand,wealsoshowthattheupperboundedofexpected
k2
optimizationerrorisO(1). Wethengivethesublinearconvergencerateandquantitativelycharacterizesomefactors
k
affectingtheconvergencerate,suchasthenetworksize,spectralgapoftheweightedadjacencymatrix,heterogenous
of individual function, and initial values. We emphasize that the mean-squared error of the decision variable is
(cid:18) (cid:19)
bounded by O(1)+O âˆš 1 1 +O(cid:0) 1 (cid:1) 1, which indicates that the network connectivity characterized by
nk n(1âˆ’Ïw) k1.5 (1âˆ’Ïw)2 k2
(1âˆ’Ï ) only influences the high order of convergence rate, while the domain rate O(1) still acts the same as the
w k
centralizedalgorithm.
(3) We analyze the transient time K for the proposed algorithm, namely, the number of iterations before the
T
algorithmreachesitsdominantrate. Specially,weshowthatwhentheiteratekâ‰¥ K ,thedominantfactorinfluencing
T
4the convergence rate is related to stochastic gradient descent, while for small k < K , the main factor influencing
T
theconvergencerateoriginatesfromthedistributedaverageconsensusmethod. Finally,weshowthatthealgorithm
asymptoticallyachievesthesamenetwork-independentconvergencerateasthecentralizedscheme.
The paper is organized as follows. We present the algorithm and the related assumptions in section 2. In sec-
tion 3, the auxiliary results supporting the convergence rate analysis is proved. Our main results are in section 4.
Experimentalresultsareimplementedinsection5,whiletheconcludingremarksaregiveninsection6.
Notation. Allvectorsinthispaperarecolumnvectors. Thestructureofthecommunicationnetworkismodeled
byanundirectedweightedgraphG=(N,E,W)inwhichN ={1,2,...,n}representsthesetofvertices. EâŠ†N Ã—N
is the set of edges. W = [w ] âˆˆ RnÃ—n denotes the weighted adjacency matrix, w > 0 if and only if agent i and
ij nÃ—n ij
agent jareconnected, w = w = 0otherwise. Eachagent(vertice)hasasetofneighborsN = {j|(i, j) âˆˆ E}. The
ij ji i
graphisconnectedmeansforeverypairofnodes(i, j)thereexistsapathofedgesthatgoesfromito j. ||Â·||denotes
L -normforvectorsandEuclideannormformatrices. Theoptimalsolutiondenoteas(x ,Î¸ ).
2 âˆ— âˆ—
2. AlgorithmandAssumptions
To solve this special optimization problem consisting of the computational problem eq. (1) and the learning
problem eq. (2), we will propose a Coupled Distributed Stochastic Approximation (CDSA) Algorithm and impose
someconditionsforrateanalysisinthissection.
2.1. AlgorithmSetUp
As mentioned previously, each agent i only knows its local core computational function f(x,Î¸) and parameter
i
learningfunctionh(Î¸),whiletheyareconnectedbyanetworkG=(N,E,W)inwhichagentsmaycommunicateand
i
exchangeinformationwiththeirneighborsN ={j|(i, j)âˆˆE}.Ateachstepkâ‰¥0,everyagentiholdsanestimateofthe
i
decisionvariableandunknownparameter,denotedbyx(k)andÎ¸(k),respectively.Supposethateveryagenthasaccess
i i
to a stochastic first-order oracle that can generate stochastic gradients g(x(k),Î¸(k),Î¾(k)) â‰œ âˆ‡ f(x(k),Î¸(k),Î¾(k))
i i i i x i i i i
and Ï•(Î¸(k),Î¶(k)) â‰œ âˆ‡ h(Î¸(k),Î¶(k)) respectively (where Î¾,Î¶,i = 1,2,...,n are independent random variables).
i i i Î¸ i i i i i
Then, every agent updates its parameters through stochastic gradient descent method to obtain temporary estimates
(cid:101)x i(k)and(cid:101)Î¸ i(k).Next,eachagentcommunicateswithitslocalneighborsandgatherstemporaryparametersinformation
over a static connected network to renew the iterates x(k+1) and Î¸(k+1) based on the consensus protocol. We
i i
summarizethepseudo-codeisinalgorithm1.
Algorithm1CoupledDistributedStochasticApproximation(CDSA)
Initialization: W =[w ] ;(x(0),Î¸(0)),âˆ€iâˆˆN
ij nÃ—n i i
Evolution: fork=0,1,2,...;âˆ€iâˆˆN
Compute: stochasticgradientÏ• (Î¸(k),Î¶(k))andg(x(l),Î¸(k),Î¾(k))
i i i i i i i
Choose: stepsizeÎ± andÎ³ (Tobeintroducedinsection3.3)
k k
Update accordingtothefollowingstochasticgradientdescentmethod.
(cid:101)x i(k)= x i(k)âˆ’Î± kg i(x i(k),Î¸ i(k),Î¾ i(k))
(cid:101)Î¸(k)=Î¸(k)âˆ’Î³ Ï• (Î¸(k),Î¶(k))
i i k i i i
Gather information(cid:101)x j(k),(cid:101)Î¸ j(k) from its neighbors j âˆˆ N
i
and renew the iterates by the consensus protocol
below.
(cid:88)
x i(k+1)= w ij(cid:101)x j(k)
(cid:88)jâˆˆN
i
Î¸(k+1)= w (cid:101)Î¸ (k)
i ij j
jâˆˆN
i
5WecanrewriteAlgorithm1inamorecompactformasfollows.
(cid:88)
x(k+1)= w (x (k)âˆ’Î± g (x(l),Î¸(k),Î¾(k))), (4)
i ij j k j i i i
(cid:88)jâˆˆN
i (cid:16) (cid:17)
Î¸(k+1)= w Î¸ (k)âˆ’Î³ Ï• (Î¸(k),Î¶(k)) . (5)
i ij j k j i i
jâˆˆN
i
Define
xxxâ‰œ[x ,x ,Â·Â·Â· ,x ]T âˆˆRnÃ—p,Î¸Î¸Î¸â‰œ[Î¸ ,Î¸ ,Â·Â·Â· ,Î¸ ]T âˆˆRnÃ—q, (6)
1 2 n 1 2 n
Î¾ â‰œ[Î¾ ,Î¾ ,Â·Â·Â· ,Î¾ ]T âˆˆRn, Î¶ â‰œ[Î¶ ,Î¶ ,Â·Â·Â· ,Î¶ ]T âˆˆRn, (7)
1 2 n 1 2 n
g(xxx,Î¸Î¸Î¸,Î¾Î¾Î¾)â‰œ[g (x ,Î¸ ,Î¾ ),g (x ,Î¸ ,Î¾ ),Â·Â·Â· ,g (x ,Î¸ ,Î¾ )]T âˆˆRnÃ—p, (8)
1 1 1 1 2 2 2 2 n n n n
Ï•(Î¸Î¸Î¸,Î¶Î¶Î¶)â‰œ[Ï• (Î¸ ,Î¶ ),Ï• (Î¸ ,Î¶ ),Â·Â·Â· ,Ï• (Î¸ ,Î¶ )]T âˆˆRnÃ—q. (9)
i 1 1 i 2 2 i n n
Thenequation(4)and(5)canbereformulatedinthefollowingvectorformula.
xxx(k+1)=W(xxx(k)âˆ’Î± g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))), (10)
k
Î¸Î¸Î¸(k+1)=W(Î¸Î¸Î¸(k)âˆ’Î± Ï•(Î¸Î¸Î¸(k),Î¶Î¶Î¶(k))). (11)
k
2.2. Assumptions
In this subsection, we will specify the conditions for rate analysis of the CDSA algorithm. We need to make
some assumptions about the properties of objective functions in both learning and computation metrics to get the
globaloptimalsolution. Besides,weimposesomeconstraintsonconditionalfirstandsecondmomentsofâ€œstochastic
gradientâ€. Lastbutnotleast,weinheritthetypicalassumptionsaboutcommunicationnetworksasthatofdistributed
algorithms.
Assumption2.2.1(Functionproperties) (i)ForeveryÎ¸ âˆˆ Rq, f(x,Î¸),i = 1,Â·Â·Â· ,nisstronglyconvexandLipschitz
i
smoothinxwithconstantsÂµ andL ,i.e.
x x
(âˆ‡ f(xâ€²,Î¸)âˆ’âˆ‡ f(x,Î¸))T(xâ€²âˆ’x)â‰¥Âµ ||xâ€²âˆ’x||2,âˆ€x,xâ€² âˆˆRp,
x i x i x
||âˆ‡ f(xâ€²,Î¸)âˆ’âˆ‡ f(x,Î¸)||â‰¤ L ||xâ€²âˆ’x||,âˆ€x,xâ€² âˆˆâˆˆRp.
x i x i x
(ii)Forevery x âˆˆ Rp, f(x,Î¸),i = 1,Â·Â·Â· ,nisstronglyconvexandLipschitzsmoothinÎ¸withconstantsÂµ and L
i Î¸ Î¸
respectively,i.e.
(âˆ‡ f(x,Î¸â€²)âˆ’âˆ‡ f(x,Î¸))T(Î¸â€²âˆ’Î¸)â‰¥Âµ ||Î¸â€²âˆ’Î¸||2,âˆ€Î¸,Î¸â€² âˆˆRq,
x i x i Î¸
||âˆ‡ f(x,Î¸â€²)âˆ’âˆ‡ f(x,Î¸)||â‰¤ L ||Î¸â€²âˆ’Î¸||,âˆ€Î¸,Î¸â€² âˆˆRq.
x i Î¸ Î¸
(iii)The learning metric h(Î¸) for every i âˆˆ {1,2,...,n} is strongly convex andLipschitz smooth with constants Î½
i Î¸
andC ,i.e.
Î¸
(h(Î¸)âˆ’h(Î¸â€²))T(Î¸âˆ’Î¸â€²)â‰¥Î½ ||Î¸âˆ’Î¸â€²||2,âˆ€Î¸,Î¸â€² âˆˆRq,
Î¸
||âˆ‡h(Î¸)âˆ’âˆ‡h(Î¸â€²)||â‰¤C ||Î¸âˆ’Î¸â€²||,âˆ€Î¸,Î¸â€² âˆˆRq.
Î¸
Strongconvexityassumptionsindicatethatbothcomputationalproblemandlearningproblemhaveauniqueoptimal
solution x âˆˆ Rp and Î¸ âˆˆ Rq [32]. The Lipschitz continuity of gradient functions ensure that the gradient doesnâ€™t
âˆ— âˆ—
changearbitrarilyfastconcerningthecorrespondingparametervector. Itiswidelyusedintheconvergenceanalyses
of most gradient-based methods, without it, the gradient wouldnâ€™t provide a good indicator for how far to move to
decrease the objective function[32]. These assumptions are satisfied for many machine learning problems, such as
logisticregression,linearregression,andsupportvectormachine(SVM).
Next,wedefineanewprobabilityspace(Z,F,P),whereZâ‰œâ„¦ Ã—â„¦ ,F â‰œF Ã—F andPâ‰œP Ã—P . Weuse
x Î¸ x Î¸ x Î¸
F(k)todenotetheÏƒ-algebrageneratedby{(x(0),Î¸(0)),(x(1),Î¸(1)),...,(x(k),Î¸(k))|iâˆˆN}. Thengivethefollowing
i i i i i i
assumptions related to the stochastic gradient estimator, which assume that the stochastic gradient is an unbiased
estimatorofthetruegradient,andthevarianceofthestochasticgradientisrestricted.
6Assumption2.2.2(Conditionalfirstandsecondmoments) For all k â‰¥ 0 and i âˆˆ N,there exist Ïƒ > 0,Ïƒ >
x Î¸
0,M >0,M >0,suchthat
x Î¸
(a) E [g(x(k),Î¸(k),Î¾(k))|F(k)]=âˆ‡ f(x(k),Î¸(k)), a.s.,
Î¾i(k) i i i i x i i i
(b) E [Ï•(Î¸(k),Î¶(k))|F(k)]=âˆ‡h(Î¸(k)), a.s.,
Î¶i(k) i i i i i
(c) E [||g(x(k),Î¸(k),Î¾(k))âˆ’âˆ‡ f(x(k),Î¸(k))||2|F(k)],
Î¾i(k) i i i i x i i i
â‰¤Ïƒ2+M ||âˆ‡ f(x(k),Î¸(k))||2 a.s.,
x x x i i i
(d) E [||Ï•(Î¸(k),Î¶(k))âˆ’âˆ‡h(Î¸(k))||2|F(k)]â‰¤Ïƒ2+M ||âˆ‡h(Î¸(k))||2, a.s.,
Î¶i(k) i i i i i Î¸ Î¸ i i
Next, weimposethe connectivityconditiononthe graph, whichindicatesthataftermultiple roundsofcommu-
nication,informationcanbeexchangedbetweenanytwoagents. Thisinheritsthetypicalassumptionsonconsensus
protocols[33].
Assumption2.2.3(Graphandweightedmatrix) The graph G is static, undirected, and connected. The weighted
adjacencymatrixW isnonnegativeanddoublystochastic,i.e.,
W1=1,1TW =1T (12)
where1isthevectorofallones.
Next, we state two lemmas that partially explain the practicability of algorithm 1 based on the aforementioned
assumptions.
Lemma2.2.1 [34, Lemma 10] For any x âˆˆ Rp, define x+ = xâˆ’Î±âˆ‡f(x). Suppose that f is strongly convex with
constantÂµanditsgradientfunctionisLipschitzcontinuouswithconstantL. IfÎ±âˆˆ(0,2/L),wethenhave||x+âˆ’x ||â‰¤
âˆ—
Î»||xâˆ’x ||,whereÎ»â‰œmax(|1âˆ’Î±Âµ|,|1âˆ’Î±L|).
âˆ—
Itcanbeobservedfromtheabovelemmathataslongaswechooseaproperstepsize(0<Î±<2/L),thedistanceto
optimizershrinksbyaratioÎ»<1ateachstepforstronglyconvexandsmoothfunctions. Whilethefollowinglemma
reveals that under distributed algorithm with linear iteration, the gap between the current iteration and consensus
optimalsolutionisdecreasedbyaratioÏ <1comparedtothelastiteration.
w
Lemma2.2.2 [33,Theorem1]LetAssumption2.2.3hold,andÏ denotethespecturalnormofmatrixWâˆ’111111T. Thus
w n
Ï <1. DefineÏ‰Ï‰Ï‰+ =WÏ‰Ï‰Ï‰foranyÏ‰Ï‰Ï‰âˆˆRnÃ—p. Wethenhave||Ï‰Ï‰Ï‰+âˆ’111Ï‰Â¯||â‰¤Ï ||Ï‰Ï‰Ï‰âˆ’111Ï‰Â¯||,whereÏ‰Â¯ â‰œ 1111TÏ‰Ï‰Ï‰.
w w n
Theaforementionedlemmasshowthatboththegradientdescentmethodanddistributedlineariterationcanmove
thedecisionvariabletowardstheoptimalsolutionwithlineardecayingrates. Thus,ouralgorithmconsistingofboth
approachesmightfindtheoptimalsolutionefficiently. Wewillrigorouslyprovetheconvergencerateofalgorithm1
inthefollowingtwosections.
3. AuxiliaryResults
In this section, we will present some results to assist subsequent convergence rate analysis. We first give some
preliminaryboundwhichwillbeusedforlaterproof,thenpresentthesupportinglemmasconcerningrecursionsfor
expectedoptimizationerrorandexpectedconsensuserror,andfinally,weprovethatunderdiminishingstepsize,the
mean-squareddistancebetweenthecurrentiterateandtheoptimalsolutionisuniformlybounded.
3.1. PreliminaryBound
Forsimplicity,wedenote
1(cid:88)n 1(cid:88)n
xÂ¯(k)â‰œ x(k), Î¸Â¯(k)â‰œ Î¸(k), (13)
n i=1 i n i=1 i
1(cid:88)n
gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))â‰œ g(x(k),Î¸(k),Î¾(k)), (14)
n i=1 i i i i
âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))â‰œ
1(cid:88)n
âˆ‡ f (x(k),Î¸(k)). (15)
x n i=1 x i i i
7We will show in the following lemma that with Assumptions 2.2.1 and 2.2.2, the conditional squared distance
betweenthegradientâˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))anditsestimatecanbeboundedbylinearcombinationsofsquarederrors||xxx(k)âˆ’
x
111xT||2and||Î¸Î¸Î¸(k)âˆ’111Î¸T||2. Forcompleteness,itsproofisgiveninappendixAppendix A.
âˆ— âˆ—
Lemma3.1.1 LetAssumption2.2.1and2.2.2hold. Thenforanykâ‰¥0,
E[||gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2|F(k)]
x
â‰¤
3M xL2
x||xxx(k)âˆ’111xT||2+
3M xL Î¸2
||Î¸Î¸Î¸(k)âˆ’111Î¸T||2+
MÂ¯
, (16)
n2 âˆ— n2 âˆ— n
3M
(cid:80)n
||âˆ‡ f(x ,Î¸ )||2
where MÂ¯ = x i=1 x i âˆ— âˆ— +Ïƒ2. (17)
n x
Thefollowinglemmashowsthegapbetweenthegradientofobjectivefunctionattheconsensualpoints(xÂ¯(k),Î¸Â¯(k)),
denotedby 1(cid:80)n âˆ‡ f(xÂ¯(k),Î¸Â¯(k)),andatcurrentiterates 1(cid:80)n âˆ‡ f(x(k),Î¸(k))canalsobeboundedbylinearcom-
n i=1 x i n i=1 x i i i
binationsof||xxx(k)âˆ’111xT||2and||Î¸Î¸Î¸(k)âˆ’111Î¸T||2. ThepreciseproofisinappendixAppendix B.
âˆ— âˆ—
Lemma3.1.2 LetAssumption2.2.1hold. Thenforanykâ‰¥0,
L L
||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||â‰¤ âˆšx ||xxx(k)âˆ’111xÂ¯(k)T||+ âˆšÎ¸ ||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||. (18)
x x
n n
The above two lemmas, providing the related upper bounds of functions, are derived by virtue of the Lipschitz
smoothassumption. Theyareessentialforthesubsequentconvergenceanalysis.
3.2. SupportingLemmas
Inthissubsection,wepresentsomeresultsconcerningexpectedoptimizationerror E[||xÂ¯(k)âˆ’x ||2]andexpected
âˆ—
consensus error E[||xxx(k) âˆ’111xÂ¯(k)T||2] for core computational problem, while the discussion of parameter learning
problemcanbefoundin[30]. Foreaseofpresentation,wedenote
U (k)â‰œE[||xÂ¯(k)âˆ’x ||2],V (k)â‰œE[||xxx(k)âˆ’111xÂ¯(k)T||2], (19)
1 âˆ— 1
U (k)â‰œE[||Î¸Â¯(k)âˆ’Î¸ ||2],V (k)â‰œE[||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||2]. (20)
2 âˆ— 2
NextwewillboundU (k+1)andV (k+1)byerrortermsatiterationk. ThepreciseproofofLemma3.2.1isin
1 1
appendixAppendix C.
Lemma3.2.1 LetAssumption2.2.1âˆ¼2.2.3hold,underalgorithm1,
(AAA)SupposingstepsizeÎ± â‰¤ 1,wehave
k Lx
Î±2L2 Î±2L2
U (k+1)â‰¤(1âˆ’Î± Âµ )2U (k)+ k x V (k)+ k Î¸V (k)
1 k x 1 n 1 n 2
2L L Î±2
+ x Î¸ kE[||xxx(k)âˆ’111xÂ¯(k)T||||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||]
n
2Î± L
+ âˆšk x(1âˆ’Î± Âµ )E[||xÂ¯(k)âˆ’x ||||xxx(k)âˆ’111xÂ¯(k)T||]
k x âˆ—
n
2Î± L
+ âˆšk Î¸(1âˆ’Î± Âµ )E[||xÂ¯(k)âˆ’x ||||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||]
k x âˆ—
n
ï£« ï£¶
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 xE[||xxx(k)âˆ’111x âˆ—T||]+ 3M nx 2L Î¸2 E[||Î¸Î¸Î¸(k)âˆ’111Î¸ âˆ—T||]+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸, (21)
8(BBB)SupposingstepsizeÎ± â‰¤min{1, 1 },wehave
k Lx 3Âµx
U (k+1)â‰¤(1âˆ’
3
Î± Âµ )U (k)+
6Î± kL2
xV (k)+
6Î± kL Î¸2
V (k)
1 2 k x 1 nÂµ 1 nÂµ 2
ï£« x x ï£¶
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 xE[||xxx(k)âˆ’111x âˆ—T||2]+ 3M nx 2L Î¸2 E[||Î¸Î¸Î¸(k)âˆ’111Î¸ âˆ—T||2]+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸. (22)
ResultBBBrestrictsthestepsizetosmalleronethanthatofResultAAA.ItthussimplifiesResultAAAeq.(21)byremoving
the cross term to facilitate the later analysis. We can revisit Inequality eq. (22) and reformulate it as U (k +1) â‰¤
1
(1âˆ’ 3Î± Âµ )U (k)+error(Î± ),whereerror(Î±)meansanerrorfunctionthatisproportionaltoÎ±. Weshouldmention
2 k x 1 k
that,sinceÎ± >0andÂµ >0,expectedoptimizationerrorU (k)roughlyshrinksbyaratio(1âˆ’ 3Î± Âµ )<1. Though
k x 1 2 k x
thereisanerrortermrelatedtoÎ± ,whenwechoosediminishingstepsizepolicyandtheconsensuserrorsV (k),V (k)
k 1 2
aswellasE(||xxx(k)âˆ’111xT||2),E(||Î¸Î¸Î¸(k)âˆ’111Î¸T||2)arebounded,theerrormaydecreaseto0,whichindicatestheconvergence
âˆ— âˆ—
ofU (k).
1
Wedefine
âˆ‡ F(xxx,Î¸Î¸Î¸)â‰œ[âˆ‡ f (x ,Î¸ ),âˆ‡ f (x ,Î¸ ),Â·Â·Â· ,âˆ‡ f (x ,Î¸ )]T âˆˆRnÃ—p. (23)
x x 1 1 1 x 2 2 2 x n n n
In the next lemma, we will show the recursive formulation of expected consensus error V (k), which is critical for
1
convergenceanalysis. Forcompleteness,wegiveitsproofinappendixAppendix D.
Lemma3.2.2 LetAssumption2.2.1âˆ¼2.2.3hold,andconsideralgorithm1. Thenforanykâ‰¥0,wehave
3+Ï2 (cid:32) 3 (cid:33) (cid:16)
V (k+1)â‰¤ wV (k)+Î±2Ï2nÏƒ2+3Î±2Ï2 +M L2E[||xxx(k)âˆ’111xT||2]
1 4 1 k w x k w 1âˆ’Ï2 x x âˆ—
w(cid:17)
+L2E[||Î¸Î¸Î¸(k)âˆ’111Î¸T||2]+||âˆ‡ F(111xT,111Î¸T)||2 . (24)
Î¸ âˆ— x âˆ— âˆ—
TherecursionofexpectedconsensuserrorcanbereformulateasV (k+1)â‰¤ 3+Ï2 wV (k)+error(Î±2Ï2). Itisworth
1 4 1 k w
mentioningthatV (k)canroughlyshrinkby
3+Ï2
w < 1sinceÏ < 1. Notethattheextraerrortermintheconsensus
1 4 w
error is proportional to Î±2, compared to U (k) with an error term proportional to Î± . We might obtain a qualitative
k 1 k
conclusionthatexpectedconsensuserrordecreasefasterthanexpectedoptimizationerror. Wewillpresenttheprecise
proofinthenextpartthatconsensuserrordecreaseto0atanorderO(1)whileoptimizationerroratO(1).
k2 k
Remark1 Recalling recursion of U (k) in (22) and recursion of V (k) in (24), we could notice that the expected
1 1
consensuserrorismorerelatedtothenetworkconnectivityÏ ,whichisnaturalbecauseâ€œconsensusâ€isinducedfrom
w
thedistributedalgorithm, whileâ€œoptimizationâ€mainlycomesfromoriginaloptimizationmethodsuchasstochastic
gradientdescent.
3.3. UniformBound
Fromnowon,weconsiderstepsizepolicyasfollows
Î² Î²
Î± â‰œ ,Î³ â‰œ , âˆ€k, (25)
k Âµ (k+K) k Âµ (k+K)
x Î¸
wheretheÎ²isapositiveconstant,and
K
â‰œ(cid:38) maxï£± ï£´ï£²3Î²(1+M x)L2 x,3Î²(1+M Î¸)L Î¸2ï£¼ ï£´ï£½(cid:39)
(26)
ï£´ï£³
Âµ2 Âµ2
ï£´ï£¾
x Î¸
withâŒˆÂ·âŒ‰denotingtheceilingfunction.
Next, We present a lemma that derives a uniform bound on the iterates {Î¸Î¸Î¸(k)},{xxx(k)} generated by algorithm 1.
Sucharesultishelpfulforboundingtheexpectedoptimizationerrorandexpectedconsensuserror.
9Lemma3.3.1 LetAssumption2.2.1âˆ¼2.2.3hold. Consideralgorithm1withstepsizepolicy(25). Wethenobtainfrom
[30,Lemma8]thatforallkâ‰¥0,
E[âˆ¥Î¸(k)âˆ’Î¸ âˆ¥2]â‰¤Î˜Ë†
â‰œmaxï£± ï£´ï£²âˆ¥Î¸(0)âˆ’Î¸ âˆ¥2,9âˆ¥âˆ‡h i(Î¸ âˆ—)âˆ¥2
+
Ïƒ2
Î¸
ï£¼ ï£´ï£½
. (27)
i âˆ— i ï£´ï£³ i âˆ— Âµ (1+M )L2ï£´ï£¾
Î¸ Î¸ Î¸
Basedon(27),wecanobtainthefollowingresultwithÎ˜Ë† â‰œ(cid:80)n Î˜Ë† ,
i=1 i
E[||xxx(k)âˆ’111xT||2]â‰¤ XË†, where (28)
âˆ—
(cid:40) 11L2Î˜Ë† 11||âˆ‡ F(111xT,111Î¸T)||2 7nÏƒ2 (cid:41)
XË† â‰œmax ||xxx(0)âˆ’111xT||2, Î¸ + x âˆ— âˆ— + x . (29)
âˆ— Âµ2 Âµ2 9(1+M )L2
x x x x
We will give the proof of (28) in appendix Appendix E. Lemma 3.3.1 indicates that although the problem we
consider is unconstrained, the gap between the iterates generated by algorithm CDSA and the optimal solution is
uniformly bounded. It is critical for the analysis of sublinear convergence rates of U (k) and V (k). Then based on
1 1
thislemma,wewillprovideuniformupperboundsfortheexpectedoptimizationerrorandexpectedconsensuserror.
Lemma3.3.2 LetAssumption2.2.1âˆ¼2.2.3hold.Consideralgorithm1withstepsizepolicy(25).WethenhaveU (k)â‰¤
1
XË†,V (k)â‰¤4XË†.
n 1
Proof Byrecalling(28)andusingCauchy-Schiwazinequality,weobtainthat
U 1(k)=E[||xÂ¯(k)âˆ’x
âˆ—||2]=E(cid:34)(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)n1(cid:88)n
i=1x i(k)âˆ’
1 n(cid:88)n
i=1x
âˆ—(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
= 1 E(cid:20)(cid:13) (cid:13) (cid:13)(cid:88)n (x(k)âˆ’x )(cid:13) (cid:13) (cid:13)2(cid:21) â‰¤ 1 Ã—nE[||xxx(k)âˆ’111xT||2]â‰¤ XË† ,
n2 (cid:13) i=1 i âˆ— (cid:13) n2 âˆ— n
V (k)=E[||xxx(k)âˆ’111xÂ¯(k)T||2]=E[||xxx(k)âˆ’111xT +111xT âˆ’111xÂ¯(k)T||2]
1 âˆ— âˆ—
â‰¤2E[||xxx(k)âˆ’111xT||2]+2E[||111(x âˆ’xÂ¯(k)||2)]
âˆ— âˆ—
XË†
â‰¤2XË† +2nÃ— â‰¤4XË†.
n
â–¡
4. MainResults
In this section, we will make full use of previous results and then give a precise convergence rate analysis of
algorithm 1. The elaboration will be divided into three parts. Firstly, we respectively establish the O(1) and O(1)
k k2
convergence rate of U (k) = E[||xÂ¯(k) âˆ’ x ||2] and V (k) = E[||xxx(k) âˆ’111xÂ¯(k)T||2] based on two supporting lemmas
1 âˆ— 1
in section 3.2. Secondly, we show that the convergence rate, measured by the mean-squared error of the decision
variables,isasfollows.
(cid:18) (cid:19)
(cid:16) (cid:17)
1(cid:88)n E[||x(k)âˆ’x ||2]â‰¤ Î²2MÂ¯ + O âˆš n(11 âˆ’Ïw) + O (1âˆ’1 Ïw)2 ,
n i=1 i âˆ— (2Î²âˆ’1)nÂµ2(k+K) (k+K)1.5 (k+K)2
x
where the first term is only concerned with the stochastic gradient descent method which is network independent,
whilethehigher-orderdependson(1âˆ’Ï ). Finally,wecharacterizethetransienttimeneededforCDSAtoapproach
(cid:16) w (cid:17)
theasymptoticconvergencerateisO n .
(1âˆ’Ïw)2
4.1. SublinearConvergence
WefirstprovethattheexpectedconsensuserrorV (k)=E[||xxx(k)âˆ’111xÂ¯(k)T||2]decayswithrateV (k)=O(1).
1 1 k2
10Lemma4.1.1 LetAssumption2.2.1âˆ¼2.2.3hold. Consideralgorithm1withstepsize(25). RecallthedefinitionsofK.
Define
âˆ‡HHH(Î¸)â‰œ[âˆ‡h (Î¸ ),âˆ‡h (Î¸ ),Â·Â·Â·âˆ‡h (Î¸ )]âˆˆRnÃ—q, (30)
1 1 2 2 n n
(cid:108) (cid:110) 16 (cid:111)(cid:109)
K â‰œ max 2K, . (31)
1 1âˆ’Ï2
w
Wethenobtainfrom[30,Lemma10]thatforanykâ‰¥ K âˆ’K,
1
VË† (cid:110) 8Î²2Ï2câ€² (cid:111)
V (k)â‰¤ 2 withVË† â‰œmax K2Î˜Ë†, w 4 , (32)
2 (k+K)2 2 1 Âµ2(1âˆ’Ï2)
(cid:32) (cid:33) Î¸ w
3 (cid:16) (cid:17)
wherecâ€² â‰œ2 +M L2Î˜Ë† +âˆ¥âˆ‡HHH(111Î¸T)âˆ¥ +nÏƒ2. (33)
4 1âˆ’Ï2 Î¸ Î¸ âˆ— Î¸
w
Furthermore,weachievethat
(cid:40) (cid:41)
VË† 8Î²2Ï2c
V (k)â‰¤ 1 withVË† â‰œmax 4K2XË†, w 4 , (34)
1 (k+K)2 1 1 Âµ2(1âˆ’Ï2)
(cid:32) (cid:33) x w
3 (cid:16) (cid:17)
wherec â‰œ3 +M L2XË† +L2Î˜Ë† +||âˆ‡ F(111xT,111Î¸T)||2 +nÏƒ2. (35)
4 1âˆ’Ï2 x x Î¸ x âˆ— âˆ— x
w
Proof Wenowprove(34). FromLemma3.2.2and3.3.1itfollowsthat
3+Ï2
V (k+1)â‰¤ wV (k)+Î±2Ï2c , âˆ€kâ‰¥0. (36)
1 4 1 k w 4
We use induction method to show that (34) holds for any k â‰¥ K âˆ’K. Recall from Lemma 3.3.2 that V (k) â‰¤ 4XË†.
1 1
Thenfork= K âˆ’K,V (k)â‰¤ 4K 12XË† = 4K 12XË† â‰¤ VË† 1 bythedefinitionofVË† in(34). Supposethat(34)holdsfork=kËœ.
1 1 K2 (k+K)2 (k+K)2 1
1
Itsufficestoshowthat(34)holdsfork=kËœ+1.
Notefrom(31)thatkËœ+K â‰¥ 16 foranykËœ â‰¥ K âˆ’K. Wethenhave
1âˆ’Ï2
w
1
(cid:32) kËœ+K (cid:33)2 3+Ï2 2 1 3+Ï2
âˆ’ w =1âˆ’ + âˆ’ w
kËœ+K+1 4 kËœ+K+1 (kËœ+K+1)2 4
1âˆ’Ï2 2 1âˆ’Ï2
â‰¥ w âˆ’ â‰¥ w.
4 kËœ+K 8
Dividebothsidesofaboveinequalityby Î²2Ï2 wc4. RecallingthedefinitionofVË† in(34),wehave
Âµ2
x
1
Î²2Ï Âµ2
w
2c
4
ï£« ï£¬ï£¬ï£¬ï£¬ï£¬ï£­(cid:32) kËœ+kËœ+ KK +1(cid:33)2
âˆ’
3+ 4Ï2 wï£¶ ï£·ï£·ï£·ï£·ï£·ï£¸âˆ’1
â‰¤
Âµ8 2(Î² 12Ï âˆ’2
w
Ïc
4
2)
â‰¤VË† 1. (37)
x x w
Thisimpliesthat
3+Ï2 VË† Î²2Ï2c 1 VË†
w 1 + w 4 â‰¤ 1 . (38)
4 (kËœ+K)2 Âµ2 (kËœ+K)2 (kËœ+K+1)2
x
Thenbyusing(36)andthedefinitionofÎ± in(25),wederivethatV (kËœ+1) â‰¤ VË† 1 ,i.e.,(34)holdsfork =kËœ+1.
k 1 (kËœ+K+1)2
Thenthelemmaisproved. â–¡
InlightofLemma4.1.1andotherauxiliaryresults,weestablishtheO(1)convergencerateofexpectedoptimiza-
k
tionerrorU (k)=E[||xÂ¯(k)âˆ’x ||2]inthefollowinglemma.
1 âˆ—
11Lemma4.1.2 LetAssumption2.2.1âˆ¼2.2.3hold. Consideralgorithm1withstepsize(25),whereÎ²>2. Wethenhave
Î²2c (K +K)1.5Î² XË†
U (k)â‰¤ 5 + 1
1 (1.5Î²âˆ’1)nÂµ2(k+K) (k+K)1.5Î² n
+ï£® ï£¯ï£¯ï£¯ï£¯ï£°3 (Î² 12 .( 51 Î².5 âˆ’Î² 2x âˆ’ )n1 Âµ)c 25 + (1.1 52 Î²Î² âˆ’L2 x 2VË† )n1
Âµ2
+ (1.1 52 Î²Î² âˆ’L Î¸2 2VË† )n2 Âµ2ï£¹ ï£ºï£ºï£ºï£ºï£»Â· (k+1
K)2
x x x
foranykâ‰¥ K âˆ’K,where
1
3M L2 3M L2
c â‰œ x xXË† + x Î¸Î˜Ë† +MÂ¯, (39)
5 n n
XË†,K ,VË† ,VË† ,MÂ¯ aredefinedby(29)(31)(32)(34)(17)respectively.
1 2 1
Proof SinceÎ± = Î² by(25),recallingthedefinitionofK andK in(26)and(31),wecanseethatÎ± â‰¤ Î² â‰¤
k Âµx(k+K) 1 k ÂµK1
Î² â‰¤ Âµx â‰¤min{ 1 , 1}. ThenLemma3.2.1(B)holds. Togetherwith3.3.1itfollowsthatforanykâ‰¥ K âˆ’K,
2ÂµK 6(1+Mx)L2
x
3Âµx Lx 1
U (k+1)â‰¤(1âˆ’ 3 Î± Âµ )U (k)+ 6Î± kL2 xV (k)+ 6Î± kL Î¸2 V (k)+ Î±2 kc 5 . (40)
1 2 k x 1 nÂµ 1 nÂµ 2 n
x x
RecallingthedefinitionofÎ± = Î² ,wehave
k Âµx(k+K)
U (k+1)â‰¤(1âˆ’
3Î²
)U (k)+
6Î²L2 xV 1(k)
+
6Î²L Î¸2V 2(k)
+
Î²2c
5 Â·
1
. (41)
1 2(k+K) 1 nÂµ2(k+K) nÂµ2(k+K) nÂµ2 (k+K)2
x x x
Thus
(cid:89)k+Kâˆ’1 3Î²
U (k)â‰¤ (1âˆ’ )U (K )
1 t=K1+K 2t 1
ï£«
1
ï£¶
+(cid:88)k t=+ KK 1âˆ’ +1 K(cid:89)k j=+ tK +âˆ’ 11 (1âˆ’ 3 2Î² j)ï£¬ï£¬ï£¬ï£¬ï£­6 nÎ² ÂµL
2
x2 x Â· V 1(t tâˆ’K) + 6 nÎ² ÂµL
2
xÎ¸2 Â· V 2(t tâˆ’K) + Î² n2 Âµc
2
x5 Â· t1 2ï£·ï£·ï£·ï£·ï£¸.
Recallfrom[30,lemma11]thatforanyâˆ€1< j<k, jâˆˆNand1<Î³â‰¤ j/2,(cid:81)kâˆ’1(1âˆ’ Î³)â‰¤ jÎ³ . Thenweachieve
t=j t kÎ³
(K +K)1.5Î²
U (k)â‰¤ 1 U (K )
1 (k+K)1.5Î² 1 1
ï£« ï£¶
+(cid:88)k t=+ KK 1âˆ’ +1
K
(( kt ++ K1) )1 1.5 .5Î²
Î²
ï£¬ï£¬ï£¬ï£¬ï£­6 nÎ² ÂµL
2
x2 x Â· V 1(t tâˆ’K) + 6 nÎ² ÂµL
2
xÎ¸2 Â· V 2(t tâˆ’K) + Î² n2 Âµc
2
x5 Â· t1 2ï£·ï£·ï£·ï£·ï£¸
=
(K 1+K)1.5Î²
U (K )+
6Î²L Î¸2 (cid:88)k+Kâˆ’1 (t+1)1.5Î²V 2(tâˆ’K)
+
(k+K)1.5Î² 1 1 nÂµ2 x(k+K)1.5Î² t=K1+K t
6Î²L2
x
(cid:88)k+Kâˆ’1 (t+1)1.5Î²V 1(tâˆ’K)
+
Î²2c
5
(cid:88)k+Kâˆ’1 (t+1)1.5Î²
.
nÂµ2 x(k+K)1.5Î² t=K1+K t nÂµ2 x(k+K)1.5Î² t=K1+K t2
InlightofLemma4.1.1,wehaveV (kâˆ’K)â‰¤ VË† 1 andV (kâˆ’K)â‰¤ VË† 2 foranykâ‰¥ K âˆ’K. Hence
1 k2 2 k2 1
U (k)â‰¤
Î²2c
5
(cid:88)k+Kâˆ’1 (t+1)1.5Î²
+
(K 1+K)1.5Î²
U (K )
1 nÂµ2 x(k+K)1.5Î² t=K1+K t2 (k+K)1.5Î² 1 1
+
6Î²L2 xVË†
1
(cid:88)k+Kâˆ’1 (t+1)1.5Î²
+
6Î²L Î¸2VË†
2
(cid:88)k+Kâˆ’1 (t+1)1.5Î²
. (42)
nÂµ2 x(k+K)1.5Î² t=K1+K t3 nÂµ2 x(k+K)1.5Î² t=K1+K t3
Bytheproofin[30,lemma12],whenb>aâ‰¥ K ,wehave
1
(cid:88)b (t+1)1.5Î² b1.5Î²âˆ’1 3(1.5Î²âˆ’1)b1.5Î²âˆ’2 (cid:88)b (t+1)1.5Î² 2b1.5Î²âˆ’2
â‰¤ + , â‰¤ . (43)
t=a t2 1.5Î²âˆ’1 1.5Î²âˆ’2 t=a t3 1.5Î²âˆ’2
12Thus
Î²2c 3Î²2(1.5Î²âˆ’1)c 1 (K +K)1.5Î²
U (k)â‰¤ 5 + 5 Â· + 1 U (K )
1 (1.5Î²âˆ’1)nÂµ2(k+K) (1.5Î²âˆ’2)nÂµ2 (k+K)2 (k+K)1.5Î² 1 1
x x (44)
+ 12Î²L2 xVË† 1 Â· 1 + 12Î²L Î¸2VË† 2 Â· 1 .
(1.5Î²âˆ’2)nÂµ2 (k+K)2 (1.5Î²âˆ’2)nÂµ2 (k+K)2
x x
RecallingLemma3.3.2yieldsthedesiredresult. â–¡
4.2. RateEstimate
In this subsection, we will discuss the factors that affect the convergence rate of the algorithm, especially the
networksizen,thespectralgap(1âˆ’Ï
),thesummationofinitialoptimizationerrors(cid:80)n
||x(0)âˆ’x ||2andconsensus
errors
(cid:80)n
||Î¸(0)âˆ’Î¸ ||2, and the
hetew
rogenous of computational functions and
learnii n= g1 fui nctionsâˆ—
characterized by
(cid:80)n
||âˆ‡
i f=1
(x
,i
Î¸ )||2
anâˆ— d(cid:80)n
||âˆ‡h(Î¸ )||2. Firstly,weboundtheconstantsappearinginLemmas4.1.1and4.1.2bythe
i=1 x i âˆ— âˆ— i=1 i âˆ—
aforementionedfactors. Wethenutilizethemtosimplifythesublinearrateoftheexpectedoptimizationerror,based
onwhich,wecanimprovetheconvergencerateandderivethemainresultforAlgorithm1.
Lemma4.2.1 DenoteA
â‰œ(cid:80)n
||x(0)âˆ’x ||2,B
â‰œ(cid:80)n
||âˆ‡ f(x ;Î¸ )||2,
1 i=1 i âˆ— 1 i=1 x i âˆ— âˆ—
A â‰œ(cid:80)n ||Î¸(0)âˆ’Î¸ ||2,andB â‰œ(cid:80)n ||âˆ‡h(Î¸ )||2. Thentheordersofconstantsâ€™XË† (29),Î˜Ë† (27),VË† (34),VË† (32),c
2 i=1 i âˆ— 2 i=1 i âˆ— 1 2 4
(35)andc (39)areasfollow.
5
XË† =O(A +A +B +B +n), Î˜Ë† =O(A +B +n),
1 2 1 2 2 2
(cid:32) A +A +B +B +n(cid:33) (cid:32) A +B +n(cid:33)
VË† =O 1 2 1 2 , VË† =O 2 2 ,
1 (1âˆ’Ï )2 2 (1âˆ’Ï )2
(cid:32) A +A +Bw +B +n(cid:33) (cid:18)A +A w +B +B +n(cid:19)
c =O 1 2 1 2 , c =O 1 2 1 2 .
4 1âˆ’Ï 5 n
w
Proof TheupperboundofÎ˜Ë† andVË† dealonlywithunknownparameterÎ¸,whichcanbeinheriteddirectlyfrom[30,
2
lemma13]. AsforXË†,recalling(29)wehave
11L2Î˜Ë† 11||âˆ‡ F(111xT,111Î¸T)||2 7nÏƒ2
XË† â‰¤||xxx(0)âˆ’111xT||2+ Î¸ + x âˆ— âˆ— + x
âˆ— Âµ2 Âµ2 9(1+M )L2 (45)
x x x x
=O(A +A +B +B +n).
1 2 1 2
Fromthedefinitionofc in(35),itfollowsthat
4
(cid:32) 3 (cid:33) (cid:32) A +A +B +B +n(cid:33)
c =3 +M (L2XË† +L2Î˜Ë† +||âˆ‡ F(111xT,111Î¸T)||2)+nÏƒ2 =O 1 2 1 2 (46)
4 1âˆ’Ï2 x x Î¸ x âˆ— âˆ— x 1âˆ’Ï
w w
(cid:16) (cid:17)
Notefrom(31)and(26)thatK =O 1 . Thenby(34),weobtain
1 1âˆ’Ïw
(cid:40) 8Î²2Ï2c (cid:41) (cid:32) A +A +B +B +n(cid:33)
VË† =max 4K2XË†, w 4 =O 1 2 1 2 . (47)
1 1 Âµ2(1âˆ’Ï2) (1âˆ’Ï )2
x w w
Inlightofequation(39),wecanachieve
c =
3M xL2
xXË† +
3M xL Î¸2
Î˜Ë† +MÂ¯
=O(cid:18)A 1+A 2+B 1+B 2+n(cid:19)
. (48)
5 n n x n
â–¡
Thesimplificationoftheseconstantsmakesitconvenientforlateranalysis. Inlightofrelation(34),sinceVË† isthe
1
onlyconstant, theconvergenceresultofexpectedconsensuserrorV (k)canbeeasilyobtained. Whiletheexpected
1
optimizationerrorU (k)needstobereformulatedmoreconcisely.
1
13Corollary4.1 LetAssumption2.2.1âˆ¼2.2.3hold. Consideralgorithm1withstepsizepolicy(25),whereÎ² > 2. Then
weobtainfrom[30,Corollary1]that
Î²2câ€² 1 câ€²
U (k)â‰¤ 5 Â· + 6 , âˆ€kâ‰¥ K âˆ’K,
2 (1.5Î²âˆ’1)nÂµ2 (k+K) (k+K)2 1
x
wherecâ€² â‰œ 2MÎ¸L Î¸2Î˜Ë† +2MÎ¸(cid:80)n i=1âˆ¥âˆ‡hi(Î¸âˆ—)âˆ¥2+Ïƒ2,câ€² =O(cid:16) A2+B2+n(cid:17) .Basedonwhich,wefurtherhavethatforanykâ‰¥ K âˆ’K,
5 n n Î¸ 6 n(1âˆ’Ïw)2 1
Î²2c 1 c
U (k)â‰¤ 5 Â· + 6 , (49)
1 (1.5Î²âˆ’1)nÂµ2 (k+K) (k+K)2
x
(cid:16) (cid:17)
wherec isdefinedin(39),andc =O A1+A2+B1+B2+n .
5 6 n(1âˆ’Ïw)2
Proof InlightofLemma4.1.2andLemma4.2.1,wecanobtainthat
Î²2c (K +K)1.5Î²âˆ’2 XË† 1
U (k)â‰¤ 5 + 1 Â·
1 (1.5Î²âˆ’1)nÂµ2(k+K) (k+K)1.5Î²âˆ’2 n (k+K)2
+ï£® ï£¯ï£¯ï£¯ï£¯ï£°3 (Î² 12 .( 51 Î².5 âˆ’x Î² 2âˆ’ )n1 Âµ)c 25 + (1.1 52 Î²Î² âˆ’L2 x 1VË† )n1
Âµ2
+ (1.1 52 Î²Î² âˆ’L Î¸2 1VË† )n2 Âµ2ï£¹ ï£ºï£ºï£ºï£ºï£»Â· (k+1
K)2
x x x
Î²2c 1 (cid:18)A +A +B +B +n(cid:19) 1
= 5 Â· +O 1 2 1 2
(1.5Î²âˆ’1)nÂµ2 (k+K) n (k+K)2
(cid:34) (cid:18)A +Ax +B +B +n(cid:19) (cid:32) A +A +B +B +n(cid:33) (cid:32) A +B +n(cid:33)(cid:35) 1
+ O 1 2 1 2 +O 1 2 1 2 +O 2 2
n2 n(1âˆ’Ï )2 n(1âˆ’Ï )2 (k+K)2
Î²2c 1 (cid:32) A +A +B +B +w n(cid:33) 1 w
â‰¤ 5 Â· +O 1 2 1 2 .
(1.5Î²âˆ’1)nÂµ2 (k+K) n(1âˆ’Ï )2 (k+K)2
x w
â–¡
Based on this corollary, together with Lemma 3.2.1, we further elaborate the convergence result of Algorithm
1. Especially, we give an upper bound of
1(cid:80)n E(cid:104)
âˆ¥x(k)âˆ’x
âˆ¥2(cid:105)
and formulate it in a way to make an intuitive
n i=1 i âˆ—
comparisonwiththecentralizedalgorithm.
Theorem4.1 LetAssumption2.2.1âˆ¼2.2.3hold. Consideralgorithm1withstepsizepolicy(25),whereÎ² > 2. Then
foranykâ‰¥ K âˆ’K,wehave
1
1(cid:88)n Î²2MÂ¯
E[||x(k)âˆ’x ||2]â‰¤
n i=1 i âˆ— (2Î²âˆ’1)nÂµ2(k+K)
(cid:32) A +A +B +B +n(cid:33) 1x (cid:32) A +A +B +B +n(cid:33) 1
+O 1 âˆš2 1 2 +O 1 2 1 2 , (50)
n n(1âˆ’Ï w) (k+K)1.5 n(1âˆ’Ï w)2 (k+K)2
whereMÂ¯ isdefinedin(17).
Proof Fork â‰¥ K âˆ’K,byrecallingLemma3.2.1(AAA)andthedefinitionofU (k),V (k)andU (k),V (k)in(19)and
1 1 1 2 2
14(20),wehave
U (k+1)â‰¤(1âˆ’Î± Âµ )2U (k)+
Î±2 kL2
x V (k)+
Î±2 kL Î¸2
V (k)+
2L xL Î¸Î±2
k
(cid:112)
V (k)V (k)
1 k x 1 n 1 n 2 n 1 2
2Î± L (cid:112) 2Î± L (cid:112)
+ âˆšk x U (k)V (k)+ âˆšk Î¸ U (k)V (k)
1 1 1 2
n n
ï£« ï£¶
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 x(nU 1(k)+V 1(k))+ 3M nx 2L Î¸2 (nU 2(k)+V 2(k))+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸
(cid:32) 3M L2(cid:33) 3M L2
=(1âˆ’2Î± Âµ )U (k)+Î±2 Âµ2+ x x U (k)+Î±2Â· x Î¸U (k)
k x 1 k x n 1 k n 2
+
Î±2 kL2
x
(cid:32)
1+
3M x(cid:33)
V (k)+
Î±2 kL Î¸2 (cid:32)
1+
3M x(cid:33)
V (k)+
2L xL Î¸Î±2
k
(cid:112)
V (k)V (k)
n n 1 n n 2 n 1 2
2Î± L (cid:112) 2Î± L (cid:112)
Î±2MÂ¯
+ âˆšk x U (k)V (k)+ âˆšk Î¸ U (k)V (k)+ k ,
n 1 1 n 1 2 n
wherethefirstinequalityfollowstheCauchy-Schwarzinequalityintheprobabilisticformandthefactthat
(cid:104) (cid:105)
E[||xxx(k)âˆ’111xT||2]=E âˆ¥x(k)âˆ’111xÂ¯T +111xÂ¯T âˆ’111xTâˆ¥2
âˆ— âˆ—
(cid:104) (cid:105) (cid:104) (cid:105)
â‰¤2E âˆ¥x(k)âˆ’111xÂ¯Tâˆ¥2 +2E âˆ¥111xÂ¯T âˆ’111xTâˆ¥2 (51)
âˆ—
(cid:104) (cid:105) (cid:104) (cid:105)
=2E âˆ¥x(k)âˆ’111xÂ¯Tâˆ¥2 +2nE âˆ¥xÂ¯âˆ’x âˆ¥2 =2V (k)+2nU (k).
âˆ— 1 1
Thus,duetoÎ± = Î² ,wehave
k Âµx(k+K)
U
(k+1)â‰¤(cid:32)
1âˆ’
2Î² (cid:33)
U (k)+
Î²2U 1(k) (cid:32)
1+
3M xL2 x(cid:33)
+
3M xL Î¸2Î²2U 2(k)
1 k+K 1 (k+K)2 nÂµ2 nÂµ2(k+K)2
x x
Î²2L2 (cid:32) 3M (cid:33) V (k) Î²2L2 (cid:32) 3M (cid:33) V (k)
+ x 1+ x 1 + Î¸ 1+ x 2
nÂµ2 n (k+K)2 nÂµ2 n (k+K)2
xâˆš âˆš x âˆš
2L L Î²2 V (k)V (k) 2Î²L U (k)V (k) 2Î²L U (k)V (k) Î²2MÂ¯ 1
+ x Î¸ 1 2 + âˆš x 1 1 + âˆš Î¸ 1 2 +
nÂµ2 (k+K)2 nÂµ k+K nÂµ k+K nÂµ2 (k+K)2
x x x x
Denotebyc =1+ 3MxL2 x andc =1+ 3Mx . Theninlightof[30,Lemma11],weobtainthat
7 nÂµ2
x
8 n
(cid:32) (cid:33) (cid:32) (cid:32) (cid:33)(cid:33)(cid:34)
U
(k)â‰¤(cid:89)k+Kâˆ’1
1âˆ’
2Î²
U (K
)+(cid:88)k+Kâˆ’1 (cid:89)k+Kâˆ’1
1âˆ’
2Î² Î²2MÂ¯
1 t=K1+K t 1 1 t=K1+K i=t+1 i nÂµ2 xt2
âˆš
+
3M xL Î¸2Î²2U 2(tâˆ’K)
+
Î²2L2 xc 8V 1(tâˆ’K)
+
Î²2L Î¸2c 8V 2(tâˆ’K)
+
2L xL Î¸Î²2 V 1(tâˆ’K)V 2(tâˆ’K)
nÂµ2 t2 nÂµ2 t2 nÂµ2 t2 nÂµ2 t2
x âˆš x x âˆš x (cid:35)
Î²2c U (tâˆ’K) 2Î²L U (tâˆ’K)V (tâˆ’K) 2Î²L U (tâˆ’K)V (tâˆ’K)
+ 7 1 + âˆš x 1 1 + âˆš Î¸ 1 2
t2 nÂµ t nÂµ t
â‰¤
(K 1+K)2Î²
U (K
)+(cid:88)kx +Kâˆ’1 (t+1)2Î² (cid:34) Î²2MÂ¯
+
Î²2c 7Ux 1(tâˆ’K) (52)
(k+K)2Î² 1 1 t=K1+K (k+K)2Î² nÂµ2 xt2 âˆšt2
+
Î²2L2 xc 8V 1(tâˆ’K)
+
Î²2L Î¸2c 8V 2(tâˆ’K)
+
2L xL Î¸Î²2 V 1(tâˆ’K)V 2(tâˆ’K)
nÂµ2 t2 nÂµ2 t2 nÂµ2 t2
+x 3M xL Î¸2Î²2U 2(tâˆ’K)x
+
2Î²L xâˆš U 1( âˆštâˆ’K)Vx 1(tâˆ’K)
+
2Î²L Î¸âˆš U 1( âˆštâˆ’K)V 2(tâˆ’K)(cid:35)
.
nÂµ2t2 nÂµ t nÂµ t
x x x
15AccordingtoCorollary4.1andLemma4.1.1,wehave
U (k)â‰¤
(K 1+K)2Î²
U (K )+
1
Â·
Î²2MÂ¯ (cid:88)k+Kâˆ’1 (t+1)2Î²
1
+
Î²2( ck 7+K (cid:88))2Î² k+K1
âˆ’1
1 (t+1( )k 2Î²+ (cid:34)K)2Î² Î²2n cÂµ 52
x
Â·t= 1K1 ++K
c
6(cid:35)
t2
+
( nk Âµ3+
2
xM
(kK
x
+L)2 Î¸2Î²
KÎ²2
)2Î²t (cid:88)=K1
k
t+ =+K
KK 1âˆ’ +1
Kt (2
t+
t21)2( Î²1.
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°5 (Î² 1.âˆ’ 5Î²1 Î²) âˆ’2n cÂµ
1â€²
52
x
)nÂµ2
Î¸t
Â· 1
tt2
+ c t2â€² 6ï£¹ ï£ºï£ºï£ºï£ºï£»
+
Î²2L2 xc
8
(cid:88)k+Kâˆ’1 (t+1)2Î²
Â·
VË†
1
nÂµ2 x(k+K)2Î² t=K1+K t2 t2
(cid:112)
+ Î²2L Î¸2c 8 (cid:88)k+Kâˆ’1 (t+1)2Î² Â· VË† 2 + 2L xL Î¸Î²2 (cid:88)k+Kâˆ’1 (t+1)2Î² VË† 1VË† 2
nÂµ2 x(k+K)2Î² t=K1+K t2 t (cid:115)2 nÂµ2 x(k+K)2Î² t=K1+ (cid:115)K t2 t2
+ âˆš
2Î²L
x
(cid:88)k+Kâˆ’1 (t+1)2Î² Î²2c
5 Â·
1
+
c
6
VË†
1
nÂµ x(k+K)2Î² t=K1+K t (1.5Î²âˆ’1)nÂµ2
x
t t2 t2
(cid:115) (cid:115)
+ âˆš
2Î²L
Î¸
(cid:88)k+Kâˆ’1 (t+1)2Î² Î²2c
5 Â·
1
+
c
6
VË†
2.
nÂµ x(k+K)2Î² t=K1+K t (1.5Î²âˆ’1)nÂµ2
x
t t2 t2
âˆš âˆš âˆš
Since a+bâ‰¤ a+ b,wecanachieve
(cid:115) (cid:115) (cid:115) (cid:112)
Î²2c 1 c VË† c VË† 1 c VË†
5 Â· + 6 Â· 1 â‰¤Î² 5 1 Â· + 6 1, (53)
(1.5Î²âˆ’1)nÂµ2 t t2 t2 (1.5Î²âˆ’1)nÂµ2 t1.5 t2
x x
then
U 1(k)â‰¤ Î²2M (Â¯
k
ï£®(cid:80) +k t=+ KKK )1âˆ’ 2+1 Î²K nÂµ(t+
2
xt1 2)2Î² + (K 1+ (kK +)2 KÎ²U )2Î²1(K 1) + 2Î²2âˆš (cid:112)c 5( (cid:112)L 1x .(cid:112) 5Î²VË† âˆ’1+ (cid:113)1L Ã—Î¸ n(cid:112) ÂµV
2
xË†
(
ï£¹2 k) +(cid:80) tk K=+ KK )21âˆ’ + Î²1 K (t+ t21 .5)2Î²
+ (k+1
K)2Î²
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°(1.5Î² Î²4 âˆ’c 5 1c 7
)nÂµ2
x
+ (1.53 Î²M âˆ’xÎ² 14 )L nÎ¸2 2c Âµâ€² 5
2 xÂµ2
Î¸
+ 2Î²(L x c 6V âˆšË† 1 n+
Âµ
xL Î¸ câ€² 6VË† 2)ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»(cid:88) tk =+ KK 1âˆ’ +1
K
(t+ t31)2Î²
+ (k+1
K)2Î²
ï£® ï£¯ï£¯ï£¯ï£¯ï£¯ï£°Î²2c 6c 7+ 3M x nÎ² Âµ2
2
xL Î¸2câ€² 6 + Î²2(L2 xVË† 1 n+
Âµ2
xL Î¸2VË† 2)c 8 + 2L xL Î¸ nÎ² Âµ2
2
x(cid:112) VË† 1VË† 2ï£¹ ï£ºï£ºï£ºï£ºï£ºï£»(cid:88) tk =+ KK 1âˆ’ +1
K
(t+ t41)2Î² .
Recall(43)andnotethat
(cid:88)b (t+1)2Î² â‰¤(cid:88)b 2(t+1)2Î² â‰¤(cid:90) b+1
2t2Î²âˆ’2.5dtâ‰¤
2(b+1)2Î²âˆ’1.5
,
(cid:88)t b=a (t+t2. 15
)2Î²
â‰¤(cid:88)t b=a 2(t (t+ +1 1)2 )2.5
Î²
â‰¤(cid:90)a+ b1
+1
2t2Î²âˆ’4dtâ‰¤
2(b2 +Î² 1âˆ’ )2Î²1 âˆ’.5
3
, âˆ€aâ‰¥16.
(54)
t=a t4 t=a (t+1)4
a+1
2Î²âˆ’4
Thenbynoticingthatc =c =O(1)andusingLemma4.2.1,wehave
7 8
Î²2MÂ¯ (cid:32) A +A +B +B +n(cid:33) 1 (cid:32) A +A +B +B +n(cid:33) 1
U (k)â‰¤ +O 1 âˆš2 1 2 +O 1 2 1 2
1
(cid:32)
A(2 +Î² Aâˆ’1 +)n BÂµ2
x
+(k B+K +)
n(cid:33)
1
n n( (cid:32)1 Aâˆ’Ï +w)
A
+B( +k+ BK +)1 n.5
(cid:33)
1
n(1âˆ’Ï w)2 (k+K)2
+O 1 2 1 2 +O 1 2 1 2
n(1âˆ’Ï )2 (k+K)3 n(1âˆ’Ï )2Î² (k+K)2Î²
Î²2MÂ¯ w (cid:32) A +A +B +B +n(cid:33) 1w (cid:32) A +A +B +B +n(cid:33) 1
= +O 1 âˆš2 1 2 +O 1 2 1 2 .
(2Î²âˆ’1)nÂµ2 x(k+K) n n(1âˆ’Ï w) (k+K)1.5 n(1âˆ’Ï w)2 (k+K)2
16Byrecalling(51),wehave 1(cid:80)n E[||x(k)âˆ’x ||2]â‰¤2U (k)+ 2V1(k).Thistogetherwith(34)andtheestimateofVË† in
n i=1 i âˆ— 1 n 1
Lemma4.2.1provetheresult. â–¡
Inlightofrelation(50),byrecallingthedefinitionsofA ,A ,B ,B inLemma4.2.1,wecanseethattheconver-
1 2 1 2
gencerateisproportionaltoinitialerrorsforbothcomputationalproblem(cid:80)n âˆ¥x(0)âˆ’x âˆ¥2 andparameterlearning
problem(cid:80)n âˆ¥Î¸(0)âˆ’Î¸ âˆ¥2. Itisworthnotingthattheheterogeneityofagentsâ€™i= i1 ndivi idualcâˆ— ostfunctions,measuredby
B = (cid:80)n |i |= âˆ‡1 fi (x ;Î¸ )|âˆ— |2, B = (cid:80)n ||âˆ‡h(Î¸ )||2,alsoinfluencetheconvergencerateinasimilarway. ThoughÎ¸ ,x
ar1 erespi e= c1 tivex lyi thâˆ— eoâˆ— ptimal2 solutioi= n1 stomi inâˆ— 1(cid:80)n h(Î¸)andmin 1(cid:80)n f (x;Î¸âˆ—), theyareusuallynottheoptiâˆ— maâˆ— l
Î¸ n i=1 i x n i=1 i
solutiontoeachlocalfunctionh(Î¸), f(x,Î¸).Therefore,thebiggerthedifferencebetweenthelocalcosts,theslower
i i
theconvergencerateofthealgorithm.
Remark2 Herewegivesomecommentsregeradingtheinfluenceofthenetworksizenandthespectralgap(1âˆ’Ï )
w
ontheconvergencerate. SinceA ,A ,B andB areallO(n),wecansimplifytherelation(50)asfollow.
1 2 1 2
(cid:18) (cid:19)
(cid:16) (cid:17)
1(cid:88)n E[||x(k)âˆ’x ||2]â‰¤ Î²2MÂ¯ + O âˆš n(11 âˆ’Ïw) + O (1âˆ’1 Ïw)2 . (55)
n i=1 i âˆ— (2Î²âˆ’1)nÂµ2(k+K) (k+K)1.5 (k+K)2
x
Itisnoticedthatthealgorithmconvergesfasterforbetternetworkconnectivity(i.e.,smallerÏ ). Forexample,afully
w
connectedgraphisthemostefficientconnectiontopologysinceÏ = 0. Incontrast,itholds1âˆ’Ï â†’ 0asn â†’ âˆ
w w
for the cycle graph, which indicates that the algorithm will converge very slowly for large-scale cycle graphs. The
following table taken from [35, Chapter 4] characterizes the relation between network size n and the spectral gap.
Consideringpluggingtheorderconcerningnfromthetableintorelation(55),wemayobtainthequantitiveinfluence
ofthenetworksizeontheconvergencerate.
Table2:Relationbetweenthenetworksizenandthespectralgap1âˆ’Ïw
NetworkTopology SpectralGap(1âˆ’Ïw) NetworkTopology SpectralGap(1âˆ’Ïw)
PathGraph O(1) 2D-meshGraph O(1)
n2 n
CycleGraph O(1) CompleteGraph 1
n2
ThereareotherfactorssuchasthestrongconvexityandLipschitzsmoothnessparameters,aswellasthevariance
ofthestochasticgradient,allofwhichcanalsoaffecttheconvergencerate. Wewillnotincludeaquantitativeanalysis
ofthesefactorssincethebigOconstantintheconvergencerateisalreadyquitecomplexandweoftenusetherelation
likeÂµ â‰¤ L forsimplicity. Whilesomeintuitivepropertycanbenaturallyobtainedfrom(55): thelargerconvexity
x x
and Lipschitz smoothness parameters can lead to the faster rate; the higher variance of stochastic gradient descent
leadstoalowerconvergenceratesincetermMÂ¯ definedby(17)getsbigger.
4.3. TransientTime
Inthissubsection,wewillestablishthetransientiterationneededfortheCDSAalgorithmtoreachitsdominant
rate.
Firstly,werecalltheconvergenceratefrom[30,Theorem2]forthecentralizedstochasticgradientdescent,
(cid:32) (cid:33)
(cid:104) (cid:105) Î²2MÂ¯ 1 1
E âˆ¥x(k)âˆ’x âˆ¥2 â‰¤ +O . (56)
âˆ— (2Î²âˆ’1)nÂµ2k n k2
Comparingitto(55),wemayconcludethatourdistributedalgorithmconvergestotheoptimalsolutionatacomparable
rate to the centralized algorithm, since they are both of the same order O(1). Besides, our work demonstrates that
k
thenetworkconnectivityÏ doesnotinfluencethetermO(1),itonlyappearsinhigher-ordertermsO( 1 )andO(1).
w k k1.5 k2
Thoughourdistributedalgorithmasymptoticallyreachesthesameorderofconvergencerateasthatofthecentralized
algorithm, itâ€™sunclearhowmanyiterationsittakestoreachthedominateorderO(1)sincetherearetwoextraerror
k
terms O( 1 ) and O(1) induced by averaging consensus. We refer to the number of iterations before distributed
k1.5 k2
stochasticapproximationmethodreachesitsdominantrateastransientiterations,i.e.,wheniterationkisrelatively
17small, the terms other than n and k still dominate the convergence rate[36, Section 2]. The next theorem state the
iterationsneededforAlgorithm1toreachitsdominantrate.
Theorem4.2 LetAssumption2.2.1âˆ¼2.2.3hold,andsetstepsizeas(25),whereÎ²>2. IttakesK =O(cid:0) n (cid:1) itera-
tioncountsforalgorithm1toreachtheasymptoticrateofconvergence,i.e. whenk â‰¥ K
,wehaT
ve
1(cid:80)n(1âˆ’Ï
Ew
[)2
||x(k)âˆ’
T n i=1 i
x ||2]â‰¤ Î²2MÂ¯ O(1).
âˆ— (2Î²âˆ’1)nÂµ2 xk
(cid:16) (cid:17)
Proof Recallingrelationineq.(55),weseethatforanykâ‰¥O n ,
(1âˆ’Ïw)2
Î²2MÂ¯ 1 1
â‰¥O(âˆš ) ,
(2Î²âˆ’1)nÂµ2(k+K) n(1âˆ’Ï ) (k+K)1.5
x (cid:32) w(cid:33)
Î²2MÂ¯ 1 1
â‰¥O .
(2Î²âˆ’1)nÂµ2(k+K) (1âˆ’Ï )2 (k+K)2
x w
â–¡
5. Experiments
Inthissection,wewillprovidenumericalexamplestoverifyourtheoreticalfindings,andcarryoutexperiments
byBluefog1. ItisapythonlibrarythatcanbeconnectedtotheNVIDIACollectiveCommunicationsLibrary(NCCL)
formulti-GPUcomputingorMessagePassingInterface(MPI)libraryformulti-CPUcomputing[37],i.e.,eachagent
inourdistributedexperimentscenarioisCPU.
5.1. RidgeRegression
Considerthefollowingridge-distributedregressionproblemwithanunknownregularizationparameterÎ¸ ,
âˆ—
C (Î¸
):min(cid:88)n
E
(cid:20)(cid:16) uTxâˆ’v(cid:17)2+Î¸ ||x||2(cid:21)
,
x âˆ— xâˆˆRp i=1 ui,vi i i âˆ—
whereÎ¸ canbeobtainedbythedistributedlearningproblembelow,
âˆ—
(cid:88)n
L :Î¸ =argmin (Î¸âˆ’Î±)2.
Î¸ âˆ— i=1 i
Specially,foragentiâˆˆN â‰œ{1,Â·Â·Â· ,n},itslocalobjectivefunctionsarespecifiedas
f(x;Î¸)=minE
(cid:20)(cid:16) uTxâˆ’v(cid:17)2+Î¸||x||2(cid:21)
,h(Î¸)=min(Î¸âˆ’Î±)2.
i
x
ui,vi i i i
Î¸
i
Here(u,v)aredatasamplecollectedbyeachagenti,whereu âˆˆ Rp arethesamplefeatures,whilev âˆˆ Rrepresent
i i i i
theobservedoutputs.
Parameter settings. Set p = 5 and suppose that for all i âˆˆ N, each component of u âˆˆ Rp is an independent
i
identical distribution in U(âˆ’0.5,0.5), and v
i
is drawn according to v
i
= uT
i
(cid:101)x
i
+Ïµ i, where Ïµ
i
is an gaussian random
v eaa sri ia lybl ce as lcp ue lc ai tfi ee dd thb ay tN th( e0, o0 p. t0 i1 m) a, la sn od lu(cid:101)x
ti
io= ns( a1 re3 Î¸5 4 =9 0) .0is 05a (npr +ed 1e )fi ,n ae nd dp xaram =et (cid:104)e (cid:80)r.
n
S Eet (Î±
ui
u= T)0 +.0 n1 Î¸Ã— I(cid:105)i E. I (t uc ua Tn )b =e
âˆ— âˆ— i=1 ui i i âˆ— ui i i
11 2( 11
2
+Î¸ âˆ—)âˆ’1 n1(cid:80)n i=1(cid:101)x i.
We compare the performance of Algorithm 1 under the path graph and complete graph topology with different
networksizen. Inlightoftheresultsintable2ofthepathgraphandcompletegraph,convergencerateestimationcan
bereformulated.
1(cid:88)n Î²2MÂ¯ O(n3/2) O(n2)
Path: E[||x(k)âˆ’x ||2]â‰¤ + + , (57)
n i=1 i âˆ— (2Î²âˆ’1)nÂµ2(k+K) (k+K)1.5 (k+K)2
x âˆš
1(cid:88)n Î²2MÂ¯ O(1/ n) 1
Complete: E[||x(k)âˆ’x ||2]â‰¤ + + . (58)
n i=1 i âˆ— (2Î²âˆ’1)nÂµ2(k+K) (k+K)1.5 (k+K)2
x
1https://github.com/Bluefog-Lib/bluefog
18WerunAlgorithm1,wheretheinitialvaluesaresetas(x(0),Î¸(0))=(000 ,1)âˆ€i,andtheweightedadjacencymatrix
i i 5
ofthecommunicationnetworkisbuiltaccordingtotheMetropolis-Hastingsrule[12]. Accordingto(25),wechoose
thestepsizesasÎ± =Î³ = 20 foranykâ‰¥0.
k k k+20
(a.1)n=10pathgraphtopology (b.1)n=10completegraphtopology
n=10 n=10
102 n=15 102 n=15
n=20 n=20
n=25 n=25
101 n=30 101 n=30
100 100
101 101
102 102
103 103
0 10000 20000 30000 40000 50000 60000 70000 0 10000 20000 30000 40000 50000 60000 70000
 N  N
(a.2)Theperformanceofpathgraph (b.2)Theperformanceofcompletegraph
Figure2:TheperformanceofCDSAbetweenpathgraphandcompletegraphtopology.Theresultsareaveragedover200MonteCarlosampling.
We demonstrate the empirical results in Fig. 2, where the empirical mean-squared error 1(cid:80)n E[||x(k)âˆ’ x ||2]
n i=1 i âˆ—
is calculated by averaging through 200 sample paths. We can see from the Subfigure (a.2) that for the path graph,
whentheiteratekissmall,thelargernetworksizenwillleadtothehighermean-squarederror 1(cid:80)n E[||x(k)âˆ’x ||2].
n i=1 i âˆ—
However,withtheincreaseofk,weobserveaphasetransitionthatalargernetworksizenwillleadtoasmallermean-
squarederror 1(cid:80)n E[||x(k)âˆ’x ||2](namelyfasterconvergencerate).Thisphenomenonmatchesthetheoreticalresult
n i=1 i âˆ—
(57): when k is small, the main factor influencing the convergence rate is the second and third term concerning the
networksizenviathedistributedconsensusprotocol,whilewhenkislarge,thefirstterminheritedfromcentralized
stochasticgradientdescentdominatestheconvergencerate.
Comparedittotheempiricalperformanceofthecompletegraphshowninsubfigure(b.2),wecanfindthatfrom
thebeginningtotheend,alargernetworkngeneratessmallererrors,whichalsomatches(58).
19
n
]2
*x
)k(ix
[
1 n
1=i
n
]2
*x
)k(ix
[
1 n
1=i5.2. LogisticRegression
Wefurtherconsiderconvexbutnotstronglyconvexproblem,anduselogisticregressiontodemonstratethatour
algorithmcanalsoleadstoasymptoticconvergence.
ConsiderthebinaryclassificationvialogisticregressionwithunknownregularizationparameterÎ¸ ,
âˆ—
C Î·(Î¸ âˆ—): m
Î·in(cid:88)n i=1(cid:88)m
j=i
1ln(cid:16) 1+eâˆ’Î·Txijlij(cid:17)
+
Î¸
2âˆ—||Î·||2,
whereÎ¸ canbeobtainedbyadistributedparameterlearningproblemasfollow,
âˆ—
(cid:88)n
L : Î¸ =argmin (Î¸âˆ’Î±)2.
Î¸ âˆ— i=1 i
Asforagenti,itsitsownlocalcomputationalproblemandparameterlearningproblemareasfollows.
f i(Î·;Î¸)=m
Î·in(cid:88)m
j=i
1ln(cid:16) 1+eâˆ’Î·Txijlij(cid:17)
+
2Î¸
nâˆ—||Î·||2, h i(Î¸)=m Î¸in(Î¸âˆ’Î± i)2.
In this scenario, we set Î± = 0.01Ã—i and let each agent i âˆˆ N possess dataset D â‰œ {(x ,l ) : j = 1,Â·Â·Â·m},
i i ij ij i
where x representsathree-dimensionalsamplefeaturewherethefirstdimensionis1andtheothertwodimension
ij
areselectedfromN((1,0)T,III)orN((0,1)T,III),whilel istherelatedsamplelabel1orâˆ’1respectively. Supposethat
ij
everyagentholdsanumberofpositivesamplesandnegativesampleswhichonlyaccessibletoitself.
100
Cycle Graph
Complete Graph
Path Graph
2D-mesh Graph
10 1
10 2
10 3
10 4
0 1000 2000 3000 4000 5000
 N
Figure3:TheperformanceofCDSAof25agentsunderfourtopologiesintable2forbinaryclassificationvialogisticregression.Theresultsare
averagedover200MonteCarlosampling.
WenowcomparetheempiricalperformanceofAlgorithm1underfourclassesofgraphtopologies, pathgraph,
cyclegraph,2D-meshgraph,andcompletegraph.Wesetn=25andrunAlgorithm1withinitialvalues(Î·(0),Î¸(0))=
i i
000 foralliâˆˆN,wherethestepsizeandweightedadjacencymatrixaresetthesameasRidgeRegression.Theempirical
4
resultsareshowninfig.3,whichshowsthatthecompletegraphhasbestperformance,2D-meshgraphhasthesecond-
bestperformance,whilethepathgraphdisplaystheworstperformance. Theseempiricalfindingsmatchthatlistedin
table2,wherethe2D-meshgraphhasalargerspectralgapthanthepathgraphandcyclepath,henceleadstoalower
mean-squarederror.
20
n
]2
*x
)k(ix
[
1 n
1=i6. Conclusions
Inthiswork,weconsiderthedistributedoptimizationproblemmin 1(cid:80)n f(x;Î¸âˆ—)withtheunknownparameter
Î¸âˆ—
collaborativelysolvedbyadistributedparameterlearningproblemmx in
n
1i= (cid:80)1 ni
h(Î¸). Eachagentonlyhasaccess
Î¸ n i=1 i
toitslocalcomputationalproblem f(x,Î¸)anditsparameterlearningproblemh(Î¸). Weproposeacoupleddistributed
i i
stochastic approximation algorithm for resolving this special distributed optimization, where agents can exchange
informationaboutdecisionvariablesxandlearningparameterÎ¸withneighborsoveraconnectednetwork.Wequanti-
tativelycharacterizethefactorsthatinfluencetherateofconvergence,andvalidatesthatthealgorithmasymptotically
achievestheoptimalnetwork-independentconvergenceratecomparedtothecentralizedalgorithmscheme. Inaddi-
tion, we analyze the transient time K , and show that when the iterate k â‰¥ K , the dominate factor influencing the
T T
convergence rate is related to stochastic gradient descent, while for small k < K , the main factor influencing the
T
convergence rate originates from the distributed average consensus method. Future work will consider more gen-
eral problems under weakened assumptions. It is of interests to explore the accelerated algorithm to obtain a faster
convergencerate.
References
[1] G.Binetti,A.Davoudi,D.Naso,B.Turchiano,F.L.Lewis,Adistributedauction-basedalgorithmforthenonconvexeconomicdispatch
problem,IEEETransactionsonIndustrialInformatics10(2)(2014)1124â€“1132.doi:10.1109/TII.2013.2287807.
[2] P.Yi,Y.Hong,F.Liu,Initialization-freedistributedalgorithmsforoptimalresourceallocationwithfeasibilityconstraintsandapplicationto
economicdispatchofpowersystems,Automatica74(2016)259â€“269.doi:https://doi.org/10.1016/j.automatica.2016.08.007.
[3] A.CorteÂ´s,S.MartÂ´Inez,Aprojection-baseddecompositionalgorithmfordistributedfastcomputationofcontrolinmicrogrids,SIAMJournal
onControlandOptimization56(2)(2018)583â€“609.doi:10.1137/15M103889X.
URLhttps://doi.org/10.1137/15M103889X
[4] S.Sahyoun, S.M.Djouadi, K.Tomsovic, S.Lenhart, OptimalDistributedControlforContinuumPowerSystems, pp.416â€“422. doi:
10.1137/1.9781611974072.57.
URLhttps://epubs.siam.org/doi/abs/10.1137/1.9781611974072.57
[5] L.-N.Liu,G.-H.Yang,Distributedoptimaleconomicenvironmentaldispatchformicrogridsovertime-varyingdirectedcommunicationgraph,
IEEETransactionsonNetworkScienceandEngineering8(2)(2021)1913â€“1924.doi:10.1109/TNSE.2021.3076526.
[6] V.Krishnan,S.MartÂ´Ä±nez,Distributedcontrolforspatialself-organizationofmulti-agentswarms,SIAMJournalonControlandOptimization
56(5)(2018)3642â€“3667.doi:10.1137/16M1080926.
URLhttps://doi.org/10.1137/16M1080926
[7] T.Skibik,M.M.Nicotra,Analysisoftime-distributedmodelpredictivecontrolwhenusingaregularizedprimalâ€“dualgradientoptimizer,
IEEEControlSystemsLetters7(2022)235â€“240.doi:10.1109/LCSYS.2022.3186631.
[8] Y.-L.YangTao, XuLei, Event-triggereddistributedoptimizationalgorithms, ActaAutomaticaSinica48(1)(2022)133â€“143. doi:10.
16383/j.aas.c200838.
[9] A.Nedic,Distributedgradientmethodsforconvexmachinelearningproblemsinnetworks:Distributedoptimization,IEEESignalProcessing
Magazine37(3)(2020)92â€“101.doi:10.1109/MSP.2020.2975210.
[10] S.A.Alghunaim, A.H.Sayed, Distributedcoupledmultiagentstochasticoptimization, IEEETransactionsonAutomaticControl65(1)
(2020)175â€“190.doi:10.1109/TAC.2019.2906495.
[11] B.Touri,B.Gharesifard,Aunifiedframeworkforcontinuous-timeunconstraineddistributedoptimization,SIAMJournalonControland
Optimization61(4)(2023)2004â€“2020.doi:10.1137/21M1442711.
URLhttps://doi.org/10.1137/21M1442711
[12] G.Notarstefano,I.Notarnicola,A.Camisa,Distributedoptimizationforsmartcyber-physicalnetworks,FoundationsandTrendsinSystems
andControl7(3)(2020)253â€“383.doi:10.1561/2600000020.
[13] X. Meng, Q. Liu, A consensus algorithm based on multi-agent system with state noise and gradient disturbance for distributed convex
optimization,Neurocomputing519(2023)148â€“157.doi:https://doi.org/10.1016/j.neucom.2022.11.051.
[14] N.S.Aybat,E.Y.Hamedani,Adistributedadmm-likemethodforresourcesharingovertime-varyingnetworks,SIAMJournalonOptimiza-
tion29(4)(2019)3036â€“3068.doi:10.1137/17M1151973.
URLhttps://doi.org/10.1137/17M1151973
[15] L.Carlone,V.Srivastava,F.Bullo,G.C.Calafiore,Distributedrandomconvexprogrammingviaconstraintsconsensus,SIAMJournalon
ControlandOptimization52(1)(2014)629â€“662.doi:10.1137/120885796.
URLhttps://doi.org/10.1137/120885796
[16] N.S.Aybat,H.Ahmadi,U.V.Shanbhag,Ontheanalysisofinexactaugmentedlagrangianschemesformisspecifiedconicconvexprograms,
IEEETransactionsonAutomaticControl67(8)(2021)3981â€“3996.
[17] D.Bertsimas,D.B.Brown,C.Caramanis,Theoryandapplicationsofrobustoptimization,SIAMreview53(3)(2011)464â€“501.
[18] A.Ben-Tal,L.ElGhaoui,A.Nemirovski,Robustoptimization,Princetonuniversitypress,2009.
[19] D.Bertsimas,V.Gupta,N.Kallus,Data-drivenrobustoptimization,MathematicalProgramming167(2018)235â€“292.
[20] C.Jie,L.Prashanth,M.Fu,S.Marcus,C.SzepesvaÂ´ri,Stochasticoptimizationinacumulativeprospecttheoryframework,IEEETransactions
onAutomaticControl63(9)(2018)2867â€“2882.
[21] A.Shapiro,D.Dentcheva,A.Ruszczynski,Lecturesonstochasticprogramming:modelingandtheory,SIAM,2021.
21[22] C.Wilson,V.V.Veeravalli,A.NedicÂ´,Adaptivesequentialstochasticoptimization,IEEETransactionsonAutomaticControl64(2)(2018)
496â€“509.
[23] H.Jiang, U.V.Shanbhag, Onthesolutionofstochasticoptimizationandvariationalproblemsinimperfectinformationregimes, SIAM
JournalonOptimization26(4)(2016)2394â€“2429.
[24] N.Ho-Nguyen,F.KÄ±lÄ±ncÂ¸-Karzan,Exploitingproblemstructureinoptimizationunderuncertaintyviaonlineconvexoptimization,Mathemat-
icalProgramming177(1-2)(2018)113â€“147.doi:10.1007/s10107-018-1262-8.
[25] H.Ahmadi,U.V.Shanbhag,Ontheresolutionofmisspecifiedconvexoptimizationandmonotonevariationalinequalityproblems,Compu-
tationalOptimizationandApplications77(1)(2020)125â€“161.
[26] N.Liu,L.Guo,Stochasticadaptivelinearquadraticdifferentialgames,arXivpreprintarXiv:2204.08869(2022).
[27] A.Kannan,A.NedicÂ´,U.V.Shanbhag,Distributedstochasticoptimizationunderimperfectinformation,in:201554thIEEEConferenceon
DecisionandControl(CDC),IEEE,2015,pp.400â€“405.
[28] I.Notarnicola,A.Simonetto,F.Farina,G.Notarstefano,Distributedpersonalizedgradienttrackingwithconvexparametricmodels,IEEE
TransactionsonAutomaticControl68(1)(2023)588â€“595.doi:10.1109/TAC.2022.3147007.
[29] J.Du,Y.Liu,Y.Zhi,H.Gao,Computationalconvergencerateanalysisofdistributedoptimizationalgorithm,in:InternationalConferenceon
Guidance,NavigationandControl,Springer,2022,pp.5288â€“5299.
[30] S.Pu,A.Olshevsky,I.C.Paschalidis,Asharpestimateonthetransienttimeofdistributedstochasticgradientdescent,IEEETransactionson
AutomaticControl67(11)(2021)5900â€“5915.
[31] S.Liang, L.Wang, G.Yin, Distributedquasi-monotonesubgradientalgorithmfornonsmoothconvexoptimizationoverdirectedgraphs,
Automatica101(2019)175â€“181.
[32] L.Bottou,F.E.Curtis,J.Nocedal,Optimizationmethodsforlarge-scalemachinelearning,SIAMReview60(2)(2018)223â€“311. doi:
10.1137/16M1080173.
URLhttps://doi.org/10.1137/16M1080173
[33] L.Xiao,S.Boyd,Fastlineariterationsfordistributedaveraging,Systems&ControlLetters53(1)(2004)65â€“78.
[34] G.Qu,N.Li,Harnessingsmoothnesstoacceleratedistributedoptimization,IEEETransactionsonControlofNetworkSystems5(3)(2017)
1245â€“1260.
[35] F.Bullo,LecturesonNetworkSystems,1.6Edition,KindleDirectPublishing,2022.
URLhttp://motion.me.ucsb.edu/book-lns
[36] B.Ying,K.Yuan,Y.Chen,H.Hu,P.Pan,W.Yin,Exponentialgraphisprovablyefficientfordecentralizeddeeptraining,AdvancesinNeural
InformationProcessingSystems34(2021)13975â€“13987.
[37] B.Ying,K.Yuan,H.Hu,Y.Chen,W.Yin,Bluefog: Makedecentralizedalgorithmspracticalforoptimizationanddeeplearning,arXiv
preprintarXiv:2111.04287(2021).
Appendix A. ProofofLemma3.1.1
Proof ByusingAssumption2.2.2,weobtainthat
E[||gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2|F(k)]
x
=E(cid:34)(cid:13) (cid:13) (cid:13)1(cid:88)n
g(x(k),Î¸(k),Î¾(k))âˆ’
1(cid:88)n
âˆ‡
f(x(k),Î¸(k))(cid:13) (cid:13) (cid:13)2 |F(k)(cid:35)
(cid:13) n i=1 i i i i n i=1 x i i i (cid:13)
1 (cid:88)n (cid:104) (cid:105)
= E ||g(x(k),Î¸(k),Î¾(k))âˆ’âˆ‡ f(x(k),Î¸(k))||2|F(k)
n2 i=1 i i i i x i i i
â‰¤
1 (cid:88)n (cid:16)
Ïƒ2+M ||âˆ‡
f(x(k),Î¸(k))||2(cid:17)
â‰¤
Ïƒ2
x +
M
x(cid:80)n
i=1||âˆ‡ xf i(x i(k),Î¸ i(k))||2
, (A.1)
n2 i=1 x x x i i i n n2
wherethesecondequalityusethefactthatÎ¾,âˆ€iareindependentrandomvariables. Byrecallingassumption2.2.1,we
i
achieve
||âˆ‡ f(x(k),Î¸(k))||2 =||âˆ‡ f(x(k),Î¸(k))âˆ’âˆ‡ f(x ,Î¸(k))
x i i i x i i i x i âˆ— i
+âˆ‡ f(x ,Î¸(k))âˆ’âˆ‡ f(x ,Î¸ )+âˆ‡ f(x ,Î¸ )||2
x i âˆ— i x i âˆ— âˆ— x i âˆ— âˆ—
â‰¤3||âˆ‡ f(x(k),Î¸(k))âˆ’âˆ‡ f(x ,Î¸(k))||2+3||âˆ‡ f(x ,Î¸(k))
x i i i x i âˆ— i x i âˆ— i
âˆ’âˆ‡ f(x ,Î¸ )||2+3||âˆ‡ f(x ,Î¸ )||2
x i âˆ— âˆ— x i âˆ— âˆ—
â‰¤3L2||x(k)âˆ’x ||2+3L2||Î¸(k)âˆ’Î¸ ||2+3||âˆ‡ f(x ,Î¸ )||2. (A.2)
x i âˆ— Î¸ i âˆ— x i âˆ— âˆ—
Combining(A.2)and(A.1)yieldstheresult(16). â–¡
22Appendix B. ProofofLemma3.1.2
Proof ByrecallingthedefinitionofxÂ¯(k),Î¸Â¯(k)andâˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))in(13)and(15),usingAssumption2.2.1,wehave
x
||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||
x x
1(cid:88)n 1(cid:88)n
=|| âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’ âˆ‡ f(x(k),Î¸(k))||
n i=1 x i n i=1 x i i i
1(cid:88)n
â‰¤ ||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡ f(x(k),Î¸(k))||
n i=1 x i x i i i
1(cid:88)n
= ||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡ f(x(k),Î¸Â¯(k))+âˆ‡ f(x(k),Î¸Â¯(k))âˆ’âˆ‡ f(x(k),Î¸(k))||
n i=1 x i x i i x i i x i i i
1(cid:88)n (cid:104) (cid:105)
â‰¤ ||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡ f(x(k),Î¸Â¯(k))||+||âˆ‡ f(x(k),Î¸Â¯(k))âˆ’âˆ‡ f(x(k),Î¸(k))||
n x i x i i x i i x i i i
i=1
1(cid:16) (cid:88)n (cid:88)n (cid:17)
â‰¤ L ||xÂ¯(k)âˆ’x(k)||+L ||Î¸Â¯(k)âˆ’Î¸(k)||
n x i=1 i Î¸ i=1 i
L L
â‰¤âˆšx ||xxx(k)âˆ’111xÂ¯(k)T||+ âˆšÎ¸ ||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||,
n n
wherethelastrelationfollowsfromCauchy-Schwarzinequality. â–¡
Appendix C. ProofofLemma3.2.1
Proof Accordingtothedefinitionsof xÂ¯(k)andgÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))in(13)and(14),togetherwith(cid:80)n w = 1form
i=1 ij
Assumption2.2.3,wehave
1(cid:88)n (cid:18)(cid:88)n (cid:16) (cid:17)(cid:19)
xÂ¯(k+1)= w x (k)âˆ’Î± g (x (k),Î¸ (k),Î¾ (k))
n i=1 j=1 ij j k j j j j
1(cid:88)n 1(cid:88)n
= x (k)âˆ’Î± Â· g (x (k),Î¸ (k),Î¾ (k))= xÂ¯(k)âˆ’Î± gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)). (C.1)
n j k n j j j j k
j=1 j=1
Thus,
||xÂ¯(k+1)âˆ’x ||2 =||xÂ¯(k)âˆ’Î± gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’x ||2
âˆ— k âˆ—
=||xÂ¯(k)âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’x +Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’Î± gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2
k x âˆ— k x k
=||xÂ¯(k)âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’x ||2+Î±2||âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2
k x âˆ— (cid:16)k x (cid:17)
+2Î± (xÂ¯(k)âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’x )T âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)) .
k k x âˆ— x
InlightofAssumption2.2.2andLemma3.1.1,bytakingconditionalexpectationonbothsidesofaboveequation,we
have
E[|xÂ¯(k+1)âˆ’x ||2|F(k)]|â‰¤||xÂ¯(k)âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’x ||2
âˆ— k x âˆ—
ï£« ï£¶
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 x||xxx(k)âˆ’111x âˆ—T||2+ 3M nx 2L Î¸2 ||Î¸Î¸Î¸(k)âˆ’111Î¸ âˆ—T||2+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸. (C.2)
Next,weboundthefirsttermontherightsideof(C.2).
||xÂ¯(k)âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’x ||2
k x âˆ—
=||xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k);Î¸Â¯(k))âˆ’x +Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’Î± âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
k x âˆ— k x k x
=||xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’x ||2+Î±2||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
k x âˆ— k x x
+2Î± (xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’x )T(âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k)))
k k x âˆ— x x
â‰¤||xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’x ||2+Î±2||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)âˆ—(cid:32)(cid:32)(cid:32) k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Term1 Term2
+2Î± ||xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’x ||Ã—||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||. (C.3)
(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)âˆ—(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)x(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term3
AsforTerm1,byleveragingthefactthatÎ± â‰¤ 1,Lemma2.2.1indicates
k L
||xÂ¯(k)âˆ’Î± âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’x ||2 â‰¤(1âˆ’Î± Âµ )2||xÂ¯(k)âˆ’x ||2. (C.4)
k x âˆ— k x âˆ—
23ByLemma3.1.2,Term2canbeboundedasfollow
(cid:32)
Î± L ||xxx(k)âˆ’111xÂ¯(k)T|| Î± L
||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||(cid:33)2
Î±2||âˆ‡ f(xÂ¯(k),Î¸Â¯(k))âˆ’âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2 â‰¤ k x âˆš + k Î¸ âˆš
k x x n n
Î±2L2||xxx(k)âˆ’111xÂ¯(k)T||2 Î±2L2||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||2 2L L Î±2
â‰¤ k x + k Î¸ + x Î¸ k||xxx(k)âˆ’111xÂ¯(k)T||Ã—||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||. (C.5)
n n n
Finally,Term3canbeboundedbyinvokingthesametransformationapprochesusedinTerm1andTerm2:
(cid:32) (cid:33)
L L
Term3â‰¤2Î± (1âˆ’Î± Âµ )||xÂ¯(k)âˆ’x || âˆšx ||xxx(k)âˆ’111xÂ¯(k)T||âˆšÎ¸ ||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||
k k x âˆ—
n n
2Î± L (1âˆ’Î± Âµ )||xÂ¯(k)âˆ’x ||||xxx(k)âˆ’111xÂ¯(k)T|| 2Î± L (1âˆ’Î± Âµ )||xÂ¯(k)âˆ’x ||||Î¸Î¸Î¸(k)âˆ’111Î¸Â¯(k)T||
â‰¤ k x k x âˆš âˆ— + k Î¸ k x âˆš âˆ— . (C.6)
n n
Inlightofrelation(C.3)âˆ¼(C.6),takingfullexpectationonbothsideofrelation(C.2)yieldstheresult(AAA).Furthermore
byusingmeanvalueinequality2abâ‰¤a2+b2,werearrange(21)andobtainthat
Î±2L2 Î±2L2 Î±2L2 Î±2L2
U (k+1)â‰¤(1âˆ’Î± Âµ )2U (k)+ k x V (k)+ k Î¸V (k)+ k x V (k)+ k Î¸V (k)
1 k x 1 n 1 n 2 n 1 n 2
Î±2L2 1 Î±2L2 1
+(1âˆ’Î± Âµ )2c U (k)+ k x Â· V (k)+(1âˆ’Î± Âµ )2c U (k)+ k Î¸ Â· V (k)
k x 1 1 n c 1 k x 2 1 n c 2
ï£« 1 ï£¶ 2
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 xE[||xxx(k)âˆ’111x âˆ—T||]+ 3M nx 2L Î¸2 E[||Î¸Î¸Î¸(k)âˆ’111Î¸ âˆ—T||]+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸
1 Î±2L2 1 Î±2L2
â‰¤(1+c +c )(1âˆ’Î± Âµ )2U (k)+(2+ ) k x V (k)+(2+ ) k Î¸V (k)
1 2 k x 1 c n 1 c n 2
ï£« 1 2ï£¶
+Î±2 kï£¬ï£¬ï£¬ï£¬ï£­3M nx 2L2 xE[||xxx(k)âˆ’111x âˆ—T||]+ 3M nx 2L Î¸2 E[||Î¸Î¸Î¸(k)âˆ’111Î¸ âˆ—T||]+ M nÂ¯ ï£·ï£·ï£·ï£·ï£¸, (C.7)
wherec ,c >0. Takec =c = 3Î± Âµ ,thenc +c = 3Î± Âµ . NoticingthatÎ± â‰¤ 1 ,i.e. Î± Âµ â‰¤ 1,wehave
1 2 1 2 16 k x 1 2 8 k x k 3Âµx k x 3
13 1 3
(1+c +c )(1âˆ’Î± Âµ )2 =1âˆ’ Î± Âµ + Î±2Âµ2+ Î±3Âµ3
1 2 k x 8 k x 4 k x 8 k x
(C.8)
13 1 3 1 3
â‰¤1âˆ’ Î± Âµ + Î± Âµ + Ã— Î± Âµ =1âˆ’ Î± Âµ ,
k x k x k x k x
8 12 8 9 2
and(2+ 1 )Î± â‰¤ 6,m=1,2. Plugtheminto(C.7)yeildstheresultBBB. â–¡
cm k Âµx
Appendix D. ProofofLemma3.2.2
Proof Recallingthedefinitionofg(xxx,Î¸Î¸Î¸,Î¾Î¾Î¾)in(8)andrelationxÂ¯(k+1)= xÂ¯(k)âˆ’Î± gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))ineq.(C.1),and
k
using(10),wehave
xxx(k+1)âˆ’111xÂ¯(k+1)=W(xxx(k)âˆ’Î± g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)))âˆ’111(xÂ¯(k)âˆ’Î± gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)))
k k
=(Wâˆ’111111T
)(cid:2)
(xxx(k)âˆ’111xÂ¯(k))âˆ’Î±
(g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)))(cid:3)
. (D.1)
n k
ThusbyLemma2.2.2,weobtain
||xxx(k+1)âˆ’111xÂ¯(k+1)||2
â‰¤Ï2||(xxx(k)âˆ’111xÂ¯(k))âˆ’Î± (g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2
w k
=Ï2(cid:2) ||xxx(k)âˆ’111xÂ¯(k)||2+Î±2||g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2
w (cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term4
âˆ’2Î±
(xxx(k)âˆ’111xÂ¯(k))T(g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))(cid:3)
(D.2)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)k(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
Term5
Inthefollowing,wewillseparatelyconsiderTerm4andTerm5. Notethat
âˆ¥Iâˆ’11T/nâˆ¥â‰¤1. (D.3)
24ThebyusingAssumptions2.2.2(a)and2.2.2(b),wederive
(cid:104) (cid:105)
E Î±2 ||g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2|F(k)
k
(cid:104)
=Î±2E ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))
k x x x
(cid:105)
+111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))+g(xxx(k),Î¸Î¸Î¸(k)),Î¾Î¾Î¾(k)âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2|F(k)
x
=Î±2||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2+Î±2E[||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))
k x x k x
âˆ’g((xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111(âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))âˆ’gÂ¯(xxx(k),Î¸Î¸Î¸(k)))||2|F(k)]
x
(D.3) (cid:104)
â‰¤ Î±2 ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
k x x
(cid:105)
+E[||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))||2|F(k)]
x
â‰¤Î±2||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2+Î±2nÏƒ2+Î±2M ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2. (D.4)
k x x k x k x x
RecallingAssumption2.2.2(a),weobtainthat
E(cid:2)
âˆ’2Î±
(xxx(k)âˆ’111xÂ¯(k))T(g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’111gÂ¯(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k)))|F(k)(cid:3)
k
(cid:16) (cid:17)
=âˆ’2Î± (xxx(k)âˆ’111xÂ¯(k))T âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k)) , (D.5)
k x x
Inlightof(D.2),(D.4)and(D.5),wehave
1
E[||xxx(k+1)âˆ’111xÂ¯(k+1)||2|F(k)]
Ï2
w
â‰¤||xxx(k)âˆ’111xÂ¯(k)||2+Î±2||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
k x x
+Î±2nÏƒ2+Î±2M ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2
k x k x x
âˆ’2Î± (xxx(k)âˆ’111xÂ¯(k))T(âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k)))
k x x
â‰¤||xxx(k)âˆ’111xÂ¯(k)||2+Î±2||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
k x x
+Î±2nÏƒ2+Î±2M ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2
k x k x x
1
+c ||xxx(k)âˆ’111xÂ¯(k)||2+ Î±2||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’111âˆ‡Â¯ F(xxx(k),Î¸Î¸Î¸(k))||2
3 c k x x
3 (cid:32) (cid:33)
1
â‰¤(1+c )||xxx(k)âˆ’111xÂ¯(k)||2+Î±2nÏƒ2+Î±2 1+M + ||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2, (D.6)
3 k x k x c x
3
wherec >0isarbitrary,andthelastinequalityalsousestheporpertyin(D.3).
3
Wethenconsidertheupperboundof||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2asfollow,
x
||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))||2 =||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’âˆ‡ F(111xT,Î¸Î¸Î¸(k))
x x x âˆ—
+âˆ‡ F(111xT,Î¸Î¸Î¸(k))âˆ’âˆ‡ F(111xT,111Î¸T)+âˆ‡ F(111xT,111Î¸T)||2
x âˆ— x âˆ— âˆ— x âˆ— âˆ—
â‰¤3||âˆ‡ F(xxx(k),Î¸Î¸Î¸(k))âˆ’âˆ‡ F(111xT,Î¸Î¸Î¸(k))||2
x x âˆ—
+3||âˆ‡ F(111xT,Î¸Î¸Î¸(k))âˆ’âˆ‡ F(111xT,111Î¸T)||2+3||âˆ‡ F(111xT,111Î¸T)||2
x âˆ— x âˆ— âˆ— x âˆ— âˆ—
â‰¤3L2||xxx(k)âˆ’111xÂ¯(k)||2+3L2||Î¸Î¸Î¸(k)âˆ’111Î¸T||2+3||âˆ‡ F(111xT,111Î¸T)||2. (D.7)
x Î¸ âˆ— x âˆ— âˆ—
Letc = 1âˆ’Ï2 w. Combining(D.6)and(D.7),weobtainthat
3 2
1
E[||xxx(k+1)âˆ’111xÂ¯(k+1)||2|F(k)]
Ï2
w (cid:32) (cid:33)
3âˆ’Ï2 3 (cid:16)
â‰¤ w||xxx(k)âˆ’111xÂ¯(k)||2+3Î±2 +M L2||xxx(k)âˆ’111xÂ¯(k)||2
2 k 1âˆ’Ï2 x x
w (cid:17)
+L2||Î¸Î¸Î¸(k)âˆ’111Î¸T||2+||âˆ‡ F(111xT,111Î¸T)||2 +Î±2nÏƒ2. (D.8)
Î¸ âˆ— x âˆ— âˆ— k x
NotethatÏ2(3âˆ’Ï2 w) â‰¤ 3+Ï2 w byÏ âˆˆ (0,1). Thenbytakingfullexpectationonbothsidesof(D.8)andmultiplyingÏ2
w 2 4 w w
leadstotheresult(24). â–¡
Appendix E. ProofofLemma3.3.1
Proof Foranykâ‰¥0,inordertoboundE[||xxx(k)âˆ’111xT||2],wefirstlyconsiderboundingE[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’
âˆ— i k i i i i
25x ||2]foralliâˆˆN. ByusingAssumption2.2.1(i)andAssumption2.2.2(c),wehave
âˆ—
E[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2|F(k)]=||x(k)âˆ’x âˆ’Î± âˆ‡ f(x(k),Î¸(k))||2
i k i i i i âˆ— i âˆ— k x i i i
(cid:104) (cid:105)
+Î±2E ||âˆ‡ f(x(k),Î¸(k))âˆ’g(x(k),Î¸(k),Î¾(k))||2|F(k)
k x i i i i i i i
â‰¤||x(k)âˆ’x ||2âˆ’2Î± âˆ‡ f(x(k),Î¸(k))T(x(k)âˆ’x )
i âˆ— k x i i i i âˆ—
+Î±2||âˆ‡ f(x(k),Î¸(k))||2+Î±2(Ïƒ2+M ||âˆ‡ f(x(k),Î¸(k))||2)
k x i i i k x x x i i i
â‰¤||x(k)âˆ’x ||2âˆ’2Î± Âµ ||x(k)âˆ’x ||2+2Î± ||âˆ‡ f(x ,Î¸(k))||||x(k)âˆ’x ||
i âˆ— k x i âˆ— k x i âˆ— i i âˆ—
+Î±2(1+M )||âˆ‡ f(x(k),Î¸(k))||2+Î±2Ïƒ2, (E.1)
k x x i i i k x
Considertheupperboundoftheterm||âˆ‡ f(x(k),Î¸(k))||2ontherightsideofaboveinequality. UsingAssumption
x i i i
2.2.1(i)and(ii),wehave
||âˆ‡ f(x(k),Î¸(k))||2 =||âˆ‡ f(x(k),Î¸(k))âˆ’âˆ‡ f(x ,Î¸(k))+âˆ‡ f(x ,Î¸(k))
x i i i x i i i x i âˆ— i x i âˆ— i
âˆ’âˆ‡ f(x ,Î¸ )+âˆ‡ f(x ,Î¸ )||
x i âˆ— âˆ— x i âˆ— âˆ—
â‰¤3L2||x(k)âˆ’x ||2+3L2||Î¸(k)âˆ’Î¸ ||2+3||âˆ‡ f(x ,Î¸ )||2. (E.2)
x i âˆ— Î¸ i âˆ— x i âˆ— âˆ—
Wecansimilarlyobtain||âˆ‡ f(x ,Î¸(k))||2 â‰¤2L2||Î¸(k)âˆ’Î¸ ||2+2||âˆ‡ f(x ,Î¸ )||2.Combining(E.2)and(E.1),itproduces
x i âˆ— i Î¸ i âˆ— x i âˆ— âˆ—
E[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2|F(k)]â‰¤||x(k)âˆ’x ||2âˆ’2Î± Âµ ||x(k)âˆ’x ||2
i k i i i i âˆ— i âˆ— k x i âˆ—
(cid:113)
+Î±2Ïƒ2+2Î± 2L2||Î¸(k)âˆ’Î¸ ||2+2||âˆ‡ f(x ,Î¸ )||2||x(k)âˆ’x ||
k x k Î¸ i âˆ— x i âˆ— âˆ— i âˆ—
+Î±2(1+M )(3L2||x(k)âˆ’x ||2+3L2||Î¸(k)âˆ’Î¸ ||2+3||âˆ‡ f(x ,Î¸ )||2)
k x x i âˆ— Î¸ i âˆ— x i âˆ— âˆ—
â‰¤(1âˆ’2Î± Âµ +3Î±2(1+M )L2)||x(k)âˆ’x ||2
(cid:113)k x k x x i âˆ—
+2Î± 2L2||Î¸(k)âˆ’Î¸ ||2+2||âˆ‡ f(x ,Î¸ )||2||x(k)âˆ’x ||
k Î¸ i âˆ— x i âˆ— âˆ— i âˆ—
+Î±2[3(1+M )L2||Î¸(k)âˆ’Î¸ ||2+3(1+M )||âˆ‡ f(x ,Î¸ )||2+Ïƒ2]. (E.3)
k x Î¸ i âˆ— x x i âˆ— âˆ— x
Fromthedefinitionof K in(26),forallk â‰¥ 0,wehaveÎ± â‰¤ Âµx . RecallthefactthatE[||Î¸(k)âˆ’Î¸ ||2] â‰¤ Î˜Ë† in
k 3(1+Mx)L2
x (cid:112)
i âˆ— i
(27). Bytakingfullexpectationonbothsidesof(E.3)andusingE[||x(k)âˆ’x ||]â‰¤ E[||x(k)âˆ’x ||2],wehave
i âˆ— i âˆ—
E[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2]â‰¤(1âˆ’Î± Âµ )E[||x(k)âˆ’x ||2]
i k i i i i âˆ— k x i âˆ—
(cid:113)
(cid:112)
+2Î± 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2 E[||x(k)âˆ’x ||2]
k Î¸ i x i âˆ— âˆ— i âˆ—
ï£® ï£¹
+Î± kï£¯ï£¯ï£¯ï£¯ï£°Âµ LxL 2Î¸2 Î˜Ë† i+ LÂµ 2x||âˆ‡ xf i(x âˆ—,Î¸ âˆ—)||2+ 3(1Âµ +xÏƒ M2 x )L2ï£ºï£ºï£ºï£ºï£»
x x (cid:20) x x
â‰¤E[||x(k)âˆ’x ||2]âˆ’Î± Âµ E||x(k)âˆ’x ||2
i âˆ— k x i âˆ—
(cid:113)
(cid:112)
âˆ’2 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2 E[||x(k)âˆ’x ||2]
Î¸ i x i âˆ— âˆ— i âˆ—
âˆ’ï£« ï£¬ï£¬ï£¬ï£¬ï£­Âµ LxL 2Î¸2
Î˜Ë† i+
LÂµ
2x||âˆ‡ xf i(x âˆ—,Î¸ âˆ—)||2+
3(1Âµ +xÏƒ M2
x
)L2ï£¶ ï£·ï£·ï£·ï£·ï£¸(cid:21)
. (E.4)
x x x x
Next,weconsiderthefollowingset:
(cid:26) (cid:113) âˆš
X â‰œ qâ‰¥0:Âµ qâˆ’2 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2 q
i x Î¸ i x i âˆ— âˆ—
Âµ (cid:32) Ïƒ2 (cid:33) (cid:27)
âˆ’ x 3L2Î˜Ë† +3||âˆ‡ f(x ,Î¸ )||2+ x â‰¤0 . (E.5)
3L2 Î¸ i x i âˆ— âˆ— 1+M
x x
ItcanbeseenthatX isnon-emptyandcompact. IfE[||x(k)âˆ’x ||2] (cid:60) X,inlightof(E.4)weknownthatE[||x(k)âˆ’
i i âˆ— i i
26Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2]â‰¤E[||x(k)âˆ’x ||2]. WhileforE[||x(k)âˆ’x ||2]âˆˆX,byusingÎ± â‰¤ Âµx ,wederive
k i i i i âˆ— i âˆ— i âˆ— i k 3(1+Mx)L2
x
E[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2]
i k i i i i âˆ—
(cid:40) Âµ (cid:104) (cid:113) âˆš
â‰¤max qâˆ’ x Âµ qâˆ’2 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2 q
qâˆˆX
i
(cid:32)3(1+M x)L2
x
x Î¸ i (cid:33)x (cid:41)i âˆ— âˆ—
Âµ Ïƒ2 (cid:105)
âˆ’ x 3L2Î˜Ë† +3||âˆ‡ f(x ,Î¸ )||2+ x â‰œR. (E.6)
3L2 Î¸ i x i âˆ— âˆ— 1+M i
x x
Basedonpreviousarguments,weconcludethatforallk>0,
(cid:110) (cid:111)
E[||x(k)âˆ’Î± g(x(k),Î¸(k),Î¾(k))âˆ’x ||2]â‰¤max E[||x(k)âˆ’x ||2],R . (E.7)
i k i i i i âˆ— i âˆ— i
InlightofW1=1,bynotingfrom(10)that
âˆ¥xxx(k+1)âˆ’1xTâˆ¥2 â‰¤âˆ¥Wâˆ¥2âˆ¥xxx(k)âˆ’Î± g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’1xTâˆ¥2
âˆ— k âˆ—
â‰¤âˆ¥xxx(k)âˆ’Î± g(xxx(k),Î¸Î¸Î¸(k),Î¾Î¾Î¾(k))âˆ’1xTâˆ¥2. (E.8)
k âˆ—
Thistogetherwith(E.7)produces
(cid:26) (cid:88)n (cid:27)
E[||xxx(k)âˆ’111xT||2]â‰¤max E[||xxx(0)âˆ’111xT||2], R (E.9)
âˆ— âˆ— i=1 i
Inthefollowing,wewillgiveanupperboundofR. FromthedefinitionofX in(E.5),weknowthattherightzero
i i
oftheupwardopeningparabolais
âˆš 1 (cid:34) (cid:113)
q = 2 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2
i 2Âµ Î¸ i x i âˆ— âˆ—
x
(cid:115)
(cid:32) (cid:33)(cid:35)
4Âµ2 Ïƒ2
+ 4(2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2)+ x 3L2Î˜Ë† +3||âˆ‡ f(x ,Î¸ )||2+ x .
Î¸ i x i âˆ— âˆ— 3L2 Î¸ i x i âˆ— âˆ— 1+M
x x
ThenbyusingÂµ â‰¤ L ,weachieve
x x
(cid:34)
q â‰¤ 1 2Ã—4(cid:0) 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2(cid:1)
i 4Âµ2 Î¸ i x i âˆ— âˆ—
x (cid:33)(cid:35)
(cid:16) 4Âµ2Ïƒ2
+2 8L2Î˜Ë† +8||âˆ‡ f(x ,Î¸ )||2+4L2Î˜Ë† +4||âˆ‡ f(x ,Î¸ )||2+ x x
Î¸ i x i âˆ— âˆ— Î¸ i x i âˆ— âˆ— 3L2(1+M )
x x
â‰¤ 10L Î¸2Î˜Ë† i + 10||âˆ‡ xf i(x âˆ—,Î¸ âˆ—)||2 + 2Ïƒ2 x â‰œqâˆ—.
Âµ2 Âµ2 3(1+M )L2 i
x x x x
Thus,X =[0,q]âŠ‚[0,qâˆ—]. Hencefrom(E.6)itfollowsthat
i i i
Âµ (cid:34) (cid:113) âˆš
R â‰¤qâˆ—âˆ’ x Âµ qâˆ’2 2L2Î˜Ë† +2||âˆ‡ f(x ,Î¸ )||2 q
i i 3(1+M )L2 x Î¸ i x i âˆ— âˆ—
âˆ’ Âµ x (cid:32) 3L2x Î˜Ë† x +3||âˆ‡ f(x ,Î¸ )||2+ Ïƒ2 x (cid:33)(cid:35)(cid:12) (cid:12) (cid:12) (cid:12) âˆš
3L2 x Î¸ i x i âˆ— âˆ— 1+M x (cid:12) (cid:12) q= 2LÎ¸2Î˜Ë†i+2||âˆ‡xfi(xâˆ—,Î¸âˆ—)||2
Âµx
â‰¤ 10L Î¸2Î˜Ë† i + 10||âˆ‡ xf i(x âˆ—,Î¸ âˆ—)||2 + 2Ïƒ2 x
Âµ2 Âµ2 3(1+M )L2
x (cid:34)x (cid:32) x x (cid:33)
Âµ Âµ Ïƒ2
+ x x 3L2Î˜Ë† +3||âˆ‡ f(x ,Î¸ )||2+ x
3(1+M )L2 3L2 Î¸ i x i âˆ— âˆ— 1+M
x x x ï£¹ x
+2L Î¸2Î˜Ë† i+2|| Âµâˆ‡ xf i(x âˆ—,Î¸ âˆ—||2)ï£ºï£ºï£ºï£ºï£ºï£»
x
â‰¤ 11L Î¸2Î˜Ë† i + 11||âˆ‡ xf i(x âˆ—,Î¸ âˆ—)||2 + 7Ïƒ2 x , (E.10)
Âµ2 Âµ2 9(1+M )L2
x x x x
wherethelastinequalityhasusedÂµ â‰¤ L .
x x
Combing(E.10)and(E.9),thelemmaholds. â–¡
27