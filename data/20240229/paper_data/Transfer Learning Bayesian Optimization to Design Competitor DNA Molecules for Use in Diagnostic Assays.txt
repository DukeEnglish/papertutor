Transfer Learning Bayesian Optimization to Design Competitor DNA
Molecules for Use in Diagnostic Assays
Ruby Sedgwick1,2, John P. Goertz1, Molly M. Stevens1,3, Ruth Misener2, and Mark van der
Wilk2
1Department of Materials, Department of Bioengineering and Institute of Biomedical
Engineering, Imperial College London, London
2Department of Computing, Imperial College London, London
3Department of Physiology, Anatomy and Genetics, Department of Engineering Science, and
Kavli Institute for Nanoscience Discovery, University of Oxford, Oxford
February 28, 2024
Abstract
With the rise in engineered biomolecular devices, there is an increased need for tailor-made
biologicalsequences. Often, manysimilarbiological sequencesneedto bemadefora specific
application meaning numerous, sometimes prohibitively expensive, lab experiments are nec-
essary for their optimization. This paper presents a transfer learning design of experiments
workflow to make this development feasible. By combining a transfer learning surrogate
model with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the reduction
in the number of experiments using data from the development of DNA competitors for use
in an amplification-based diagnostic assay. We use cross-validation to compare the predic-
tive accuracy of different transfer learning models, and then compare the performance of the
models for both single objective and penalized optimization tasks.
1 Introduction
Tailoring biological sequences, such as oligonucleotides or proteins, for specific applications is a common
challenge in bioengineering. These engineered molecules have a variety of uses including in biosensors
(Hua et al., 2022; Deng et al., 2023; Goertz et al., 2023), medical therapeutics (Badeau et al., 2018;
Blakney et al., 2019; Ebrahimi and Samanta, 2023) and bio-computing (Siuti et al., 2013; Qian et al.,
2011; Lv et al., 2021). However, development often requires expensive or time consuming experiments,
meaninggoodexperimentaldesignisnecessarytooptimizethebiologicalsequenceswithintheexperimen-
tal budget (Cox and Reid, 2000). This also leads to better analysis, especially when there are interaction
effects between input factors, which is common in biological experiments (Kreutz and Timmer, 2009;
Politis et al., 2017; Papaneophytou, 2019; Fellermann et al., 2019; Narayanan et al., 2020; Gilman et al.,
2021).
Iterative experimental designs have the advantage of using information from previous experiments to
inform future ones. Bayesian optimization is an iterative global black-box optimization strategy (Snoek
et al., 2012; Shahriari et al., 2016) which has proven effective for the design of biomolecular experiments
including antibody development (Khan et al., 2023), extracellular vesicle production (Bader et al., 2023),
1
4202
beF
72
]MQ.oib-q[
1v40771.2042:viXradesign and manufacturing of proteins and tissues (Romero et al., 2013; Mehrian et al., 2018; Narayanan
et al., 2021; Gamble et al., 2021), validation of molecular networks (Sedgwick et al., 2020) and vaccine
production (Rosa et al., 2022). In Bayesian optimization, a surrogate model, usually a Gaussian process,
of the system is built using data and an acquisition function decides which data point to collect next.
Gaussian processes are a powerful tool for designing biological experiments in low data regimes due to
their uncertainty estimates (Hie et al., 2020).
When many similar biological sequences need to be designed, it can be even harder to optimize
all the sequences within the experimental budget. Optimizing each sequence from scratch discards
useful information from previous tasks, meaning more experiments are required. An alternative is to
use transfer learning — a technique that improves the learning of new sequences by sharing information
between optimization tasks (Zhuang et al., 2021).
Aswerequireoursurrogatemodeltobedataefficientandhaveuncertaintyquantification,weconsider
four transfer learning Gaussian process models: an average Gaussian process (AvgGP), the multi-output
Gaussian process (MOGP), the linear model of coregionalisation (LMC) and the latent variable multi-
output Gaussian process (LVMOGP). The key difference between these Gaussian process models lies in
theirhandlingofcorrelationsbetweenoutputs: fromnocorrelationintheMOGPtonon-linearcorrelation
in the LVMOGP.
We apply these surrogate model in conjunction with Bayesian optimization for efficient optimization
of bio-molecules, as shown in Figure 1. We focus specifically on the development of a new modular
diagnostic assay, based on competitive polymerase chain reaction (PCR), for measuring expression of
multiple genes simultaneously, giving a single end point readout (Goertz et al., 2023). This diagnostic
requires many competitor DNA sequences to be optimized to have the correct amplification properties
in PCR reactions, and we believe the relationship between the responses of the competitors may be
non-linear. For optimal results, these competitors should have a predefined amplification curve rate; and
a nuisance drift factor should ideally be below a certain threshold to allow for a more stable readout.
We use synthetic data experiments to compare transfer learning Gaussian process models in different
settings. We then use cross-validation to verify the benefit of the LVMOGP for modeling the response of
thecompetitors, usingdatafromDNAamplificationexperiments. WeconfirmthataLVMOGPsurrogate
model in conjunction with the design of experiments workflow speeds up optimization of the competitors
for both a single objective case, where only rate is optimized, and an optimization case with a penalty
on drift over a given threshold.
2 Materials and Methods
2.1 Gaussian Process Regression
AGaussianprocessisastochasticprocessrepresentinganinfinitecollectionofrandomvariables,thejoint
distributionofanysubsetofwhichisamulti-dimensionalGaussiandistribution(RasmussenandWilliams,
2006). A Gaussian process is fully defined by its mean m : RD (cid:55)→ R and covariance k : RD ×RD (cid:55)→ R
functions:
f(x) ∼ GP` m(x);k(x;x′)´ ; (1)
where x ∈ RD. For a full nomenclature see Appendix 8. The output data y(x) ∈ R is assumed to be
noisy evaluations of f(x) ∈ R:
y(x) = f(x)+›; (2)
where › ∼ N(0;ff2) and ff2 is the noise variance.
n n
2Collect Data Calculate Rate and Drift
A B
continuous variables
Drift
% guanine-cytosine
number of base pairs Rate
categorical variable
primer and reporter
combination
RFU = relative fluorescence units
Bayesian Optimization Fit Transfer Learning Model
D C
LVMOGP latent space
distance indicates
similarity
points indicate unique primer and reporter combinations
Figure 1: Design of experiments workflow for optimizing the competitor DNA molecules. (A) Data is
collected in the lab using a DNA amplification reaction assay. (B) The rate and drift are then calculated
by fitting them to the amplification curves. (C) Transfer learning surrogate models use the data to
predict the rate and drift for each of the given competitors. (D) A Bayesian optimization algorithm
selects the experiment to run for each competitor. This process is repeated until all optimal competitor
sequences are found or the experimental budget is exhausted.
Prior beliefs about the data can be expressed in the selection of the mean and covariance functions.
Oftenthisimpliessettingthemeanfunctiontozero,whichiswhatwedohere. Acommonkernelfunction
is the squared exponential, which is a stationary kernel that assumes the data-generating function is
smooth:
k(x;x′) = ff2exp
−XD
(x
d
−x
d′)2!
; (3)
k 2‘2
d=1 d
where ff2 is the kernel variance and ‘ is the lengthscale of dimension d. Given a set of N training data
k d
D = {(x ;y )|i = 1;:::;N}, the training inputs {x }N can be aggregated into the matrix X ∈ RN×D
i i n i=1
and the training observations {y }N aggregated into the vector y ∈ RN. It is then possible to write
n i=1
a joint distribution of the training observations y and predicted function value f at prediction locations
∗
X . Thus, the mean and covariance of the Gaussian process at the prediction points can be calculated
∗
respectively:
—(X ) = E[f¯|X;y;X ] = K(X ;X)[K(X;X)+ff2I]−1y (4)
∗ ∗ ∗ ∗ n
3ff(X ) = K(X ;X )−K(X ;X)[K(X;X)+ff2I]−1K(X;X ): (5)
∗ ∗ ∗ ∗ n ∗
Thehyperparameters„ = {ff2;ff2;‘ }areoptimizedbymaximizingthemarginallikelihoodp(y|X;„),
n k d
which is calculated in closed form (Rasmussen and Williams, 2006).
2.2 Transfer Learning Gaussian Processes
2.2.1 Independent Gaussian Processes with Shared Kernel
A simple way of transferring information is through the kernel hyperparameters. In the multi-output
Gaussian process (MOGP), the outputs are assumed to be multi-dimensional such that y ∈ RN×P
(Álvarez et al., 2012). All outputs have the same kernel function and hyperparameters but function
values on different outputs are uncorrelated. This means the kernel of the MOGP is a block diagonal
with k(X ;X′) = k(X ;X′) if p = p′ and k(X ;X′) = 0 if p ̸= p′ where p is the output index. The
p p p p p p
joint distribution for two outputs f and f evaluated at points X and X is given by:
1 2 1 2
» – „ » –«
f K(X ;X ) 0
1 ∼ N 0; 1 1 : (6)
f 0 K(X ;X )
2 2 2
2.2.2 Linear Model of Coregionalization
The linear model of coregionalization (LMC) extends the MOGP to model linear correlations between
output surfaces by assuming they are linear combinations of Gaussian process latent functions:
f (x) = W g(x)+» fl (x): (7)
p p p p
where W ∈ RP×Q is a vector of weights g(x) = {g (x)}Q are shared latent functions, fl (x) is a
q q=1 p
latent function that shares the kernel of g(x) and allows for some independent behavior and » is a
p
learned constant (Álvarez et al., 2012; Bonilla et al., 2007).
This leads to a kernel structured in such a way that the joint distribution between two functions f
1
and f is given by:
2
»
f
– "PQ
b k (X ;X )
PQ
b k (X ;X
)#!
1 ∼ N 0; q=1 11 q 1 1 q=1 12 q 2 2 ; (8)
f 2 PQ q=1b 21k q(X 1;X 1) PQ q=1b 22k q(X 2;X 2)
where b is an element of B = WWT +diag(»), a P×P matrix determining the similarity between
pp′
functions and there are Q different covariance functions k (x;x′). If Q = 1, this is known as the intrinsic
q
coregionalization model (Álvarez et al., 2012).
Coregionalization methods have successfully been used for Bayesian optimization (Cao et al., 2010;
Swerskyetal.,2013;Tighineanuetal.,2022)andappliedtotheoptimizationofsyntheticgenes(González
et al., 2015) and chemical reactions (Taylor et al., 2023). However, coregionalization methods assume
the response surfaces are linear combinations of a small number of latent functions, so they can fail to
fit and predict well on data with non-linear similarity between surfaces.
2.2.3 Latent Variable Multi-output Gaussian Process
The latent variable multi-output Gaussian process (LVMOGP) introduced by Dai et al. (2017) can model
non-linear similarities. It does so by augmenting the input domain of a Gaussian process with a Q
H
dimensional latent space H. Each output function has a latent variable, such that the latent variables
are denoted by H = [h 1;:::;h P]T ∈ RP×QH. The LVMOGP assumes output y
p
is generated by:
y (x) = f(x;h )+›; (9)
p p
4where › ∼ N(0;ff2). The latent space allows the LVMOGP to automatically transfer learn between
n
output functions as it will cluster similar output functions together and place widely different ones far
apart on the latent space. The distance on the latent space and the latent space lengthscale then
determine the amount of correlation between different output functions. To account for uncertainty in
the placement of the latent variables, they are treated as distributions rather than point estimates, such
that h ∼ N(— ;Σ ). For more details on the implementation of the LVMOGP see Appendix 8.1.
p hp hp
Similar latent variable models have been used for Bayesian optimization of material development
(Zhang et al., 2020) and for transfer learning across cell lines (Hutter et al., 2021). However, these
methods treat the latent variables as point estimates rather than distributions as in the LVMOGP, which
can cause poor uncertainty estimates, especially at low data regimes.
2.2.4 Comparison of Gaussian Process Models
In our comparisons, we include a fourth model called the average Gaussian process (AvgGP), which
treats all the data as if it has come from the same response surface. Figure 2 shows predictions on
a toy data set of the four Gaussian process models we consider. As the AvgGP doesn’t differentiate
between surfaces, it doesn’t fit any response surface well. The MOGP only shares hyperparameters but
no information about function values between response surfaces, meaning it makes worse predictions and
has more uncertainty on new response surfaces. The LMC has a better mean prediction than the MOGP
as it shares information between response surfaces. The LVMOGP similarly has better mean prediction
than the MOGP as it shares information across response surfaces through the latent space. If Q = 1
and B is the identity matrix, then the LMC recovers the MOGP. If a linear kernel is applied to the latent
dimensions of the LVMOGP, the LMC is recovered, and by making the distance between latent variables
large relative to the lengthscale, the MOGP can be recovered too. The fact there are hyperparameter
settings for the LMC and LVMOGP that recover the MOGP is promising for preventing negative transfer,
as in the case where there is no correlation between response surfaces they can just revert to the MOGP.
However, this is only true for large data sets — in low data regimes, we may expect some negative
transfer in the no correlation case, due to uncertainty in the hyperparameter values and, in the case of
the LVMOGP, a prior on the existence of correlations.
2.2.5 Gaussian Process Implementation
All coding was done in Python using version 3.9. The Gaussian process models were implemented using
GPFlow 2.3.0 (Matthews et al., 2017). GPFlow has implementations of the standard Gaussian process,
MOGP and the LMC. Our LVMOGP was implemented as a new GPflow model class, which can be
accessed via the Github links in Appendix 8.2. Other packages used include PyMC3 3.11.4 (Salvatier
etal.,2016)forBayesianparameterestimation, Numpy1.21.4(Harrisetal.,2020), Scipy1.7.1(Virtanen
et al., 2020) and Pandas 1.3.4 (The pandas development team, 2023) for data processing and Matplotlib
3.4.3 (Droettboom et al., 2015) for visualization.
2.3 Bayesian Optimization
Bayesian optimization is a sequential experimental design strategy for finding the global minimum (or
maximum) of an objective function (Shahriari et al., 2016; Snoek et al., 2012). As the objective function
is unknown, a surrogate model is used to represent the posterior belief of the objective function and
updated every time a new data point is observed. An acquisition function is then used to select the
next data point to collect. A common acquisition function is the expected improvement which trades
off exploration of regions with little data and exploitation of regions which are expected to be optimal
5AvgGP MOGP LMC LVMOGP
all data on same latent variables
surface
shared
Kronecker kernel
hyperparameters
Figure 2: Predictions of the four Gaussian process models fitted to a toy dataset. MOGP: multioutput
Gaussian process, AvgGP: average Gaussian process, LMC: linear model of coregionalization, LVMOGP:
latentvariablemulti-outputGaussianprocess. Thedotsarethedata, thedashedlineisthetruefunction,
the solid line is the Gaussian process mean prediction and the shaded region is 2 times the predicted
standard deviation, meaning around 95% of the data points should lie within the shaded region. The
bottom row explains how data is transferred between the surfaces by each model.
(Jones et al., 1998; Garnett, 2023). This process is repeated until the optimum has been found or the
experimental budget exhausted.
2.3.1 Acquisition Function
Rather than maximizing or minimizing the rate, as is usual in Bayesian optimization, we wish to minimize
the difference between the rate, f , and the target rate, T :
rate rate
q
argmin (f −T )2 (10)
rate rate
BP;GC
Therefore, we use the target vector optimization acquisition function, that extends the expected im-
provement acquisition function to minimize the Euclidean distance between a target vector and a vector
of the current predicted values (Uhrenholt and Jensen, 2019). As we are only optimizing the rate, we use
their formulation with scalars instead of vectors. In this formulation, a stochastic variable is defined as
‹|x = ∥y(x)−yt∥2 where y(x) is the output value at input x and yt is our target value. The distribu-
2
tion of p(‹|x) is modeled with the aim of minimizing ‹. If the response surfaces are Gaussian processes,
then p(‹|x) can be approximated using a non-central ffl2 distribution. The expected improvement for
this non-central ffl2 distribution is expressed as:
¸ = ‹ G (‹ =‚2)−‚2E[t|t < ‹ =‚2]G (‹ =‚2); (11)
EI min – min min – min
6
1
noitcnuf
2
noitcnuf
3
noitcnuf
refsnart
fo
dohtemwhere‹ istheminimum‹ observedsofar, ‚ isrootmeanofthevariancesofeachoutputevaluatedat
min
the training points, t = ‹‚−2, and G is an approximate cumulative ffl2 distribution with non-centrality
–
parameter – defined in the paper (Uhrenholt and Jensen, 2019).
2.3.2 Bayesian Optimization with Drift Penalty
To ensure the drift value remains below, or close to the threshold, we use the probability of feasibility to
encourage the algorithm to select points that have a high chance of being below the threshold (Schonlau
et al., 1998):
PF(x) = p(f (x) ≤ T ); (12)
drift drift
where f (x) is the value of drift function at x, and T is the drift threshold.
drift drift
Wethenmultiplytheexpectedimprovementbytheprobabilityoffeasibilitytogetourfinalacquisition
function:
¸ = PF(x)¸ (x): (13)
p EI
The probability of feasibility has been used for optimization applications including analog circuits (Lyu
et al., 2018) and materials design (Sharpe et al., 2018).
2.3.3 Performance Metrics
For both the synthetic experiments and the cross-validation experiments we assessed the fit of Gaussian
process models with two performance metrics: root mean squared error (RMSE):
s
PN∗ (—(x∗)−y∗)2
RMSE = i=0 i i ; (14)
N∗
and negative log predictive density (NLPD):
1
XN∗
1
XN∗ (y∗−—(x∗))2!
NLPD = logp(y∗|x∗;X;y;„) = − −log(2ıff(x∗)2)− i i : (15)
N∗ i i 2N∗ i ff(x∗)2
i=0 i=0 i
These are both calculated on a test set of input locations X∗ of length N∗. The RMSE is useful
for comparing the mean predictions of the Gaussian processes, while the NLPD also indicates how good
the uncertainty estimate is, both of which are important for effective exploration and exploitation. For
assessing the Bayesian optimization algorithm, we use cumulative regret:
„q «
regret = min —(x )−y )2 +max(0;f (x )−T ) ; (16)
∗i best drift i drift
xi∈X
where y is the data point closest to the target out of both training and candidate sets for that
best
surface. max(0;f (x )−T ) is a penalty for exceeding the drift threshold.
drift i drift
2.4 Data Collection
Each competitor has predefined primers and fluorescent probes and a design region where the sequence
can be altered. Rather than tackling the difficult combinatorial problem of optimizing the sequence
directly, we reduce the problem to two key input variables: the number of base pairs (BP) and guanine-
cytosine content (GC) as in Figure 3. This converts the design space into a more manageable continuous
form and reduces the input dimensions, which is beneficial when data is limited. For each BP-GC
7fluorescent probe primer
primer
design region
rate drift
Figure 3: Schematic of the competitor design space. For a given competitor DNA molecule, the primers
and fluorescent probe regions are fixed. We can edit the design region to ensure the sequence has a given
number of base pairs and guanine-cytosine content. Changing the number of base pairs and guanine-
cytosine-content affects the rate and drift of the competitor, allowing us to fine-tune to the rate and
drift required for the diagnostic assay.
combination, chosen by an expert researcher, a polymerase chain reaction (PCR) assay generates an
amplification curve, from which rate and drift are calculated. In total, we have data on 34 different
competitors and wish to optimize 16 of these. Across the 34 competitors, we have 592 data points at
327 unique input locations, with 1 to 6 repeats at each location. See Appendix 8.3 for a summary of the
data.
The rate and drift for each amplification curve were calculated using the following equations:
‌
F = ; (17)
T
1+ (‌−F0) ·e−r·fi
F0
„ «
F
signal = F · 1+ T ·m·(ln(F )=r) ; (18)
T 0
‌
where F and F are the end point and starting fluorescence, ‌ is carrying capacity, r is the rate, m is
T 0
the drift and fi is cycle number.
2.4.1 Polymerase Chain Reactions
To perform the PCR reactions, we used an Applied Biosystems QuantStudio 6 Flex using Applied Biosys-
tems MicroAmp EnduraPlate Optical 384-well plates (Thermo Fisher Scientific, Waltham, MA, USA).
The theromcycling stages consisted of a melt step at 95°C for 3 seconds and an annealing step at 60°C.
All reactions were performed at 10 µL and used Applied Biosystems TaqMan Fast Advanced Master Mix.
Either fluorescent probes or EvaGreen dye (Biotium, Fremont, CA, USA) were used as reporters.
82.4.2 DNA Sequences
For each BP-GC combination for a given competitor, NUPACK (Zadeh et al., 2011) was used to create a
DNA sequence with the correct number of base pairs and guanine-cytosine content, as well as the correct
sequences for the primer and probes. These sequences, alongside synthetic natural target analogs, were
purchased from Twist Biosciences (San Francisco, CA) or as eBlock Gene Fragments from Integrated
DNA Technologies (“IDT”, Coralville, IA, USA). Primers and probes were also purchased from IDT.
3 Results
3.1 Synthetic Data Experiments
To explore the performance of the MOGP, AvgGP, LMC and LVMOGP, we ran experiments on synthetic
data sets representing three test cases: uncorrelated, linearly correlated and horizontally offset response
surfaces. All synthetic experiments had two response surfaces each with 30 points observed and 10 new
response surfaces with no points observed initially. We added one random point to each new response
surface every iteration and recorded the RMSE and NLPD for the Gaussian process models’ predictions.
Figure 4 shows the RMSEs and NLPDs of the Gaussian process models for these test settings.
Fortheuncorrelatedtestcase,responsesurfacesweregeneratedasindependentsamplesofaGaussian
process prior with a ‘ = 0:3 and ff2 = 2. This test case was to check for negative transfer, where the
k
sharing of information hinders rather than aids the learning process. In Figure 4, the MOGP outperforms
the other Gaussian process models for RMSE and NLPD until approximately 10 data points. We expect
the LMC and LVMOGP to have some negative transfer at very low data regimes as they have a prior
expectation of correlations between response surfaces. However, with enough data, they should perform
the same at the MOGP, which is corroborated by the results in Figure 4.
The response surfaces for the linearly-correlated test case were created as linear combinations of two
latent functions, both generated as independent samples of a Gaussian process with ‘ = 0:3 and ff2 = 2.
k
The LMC outperforms the other two Gaussian process models except at very low data regimes, which is
likely due to overconfidence of the LMC when it has little data. The LMC and LVMOGP outperform the
MOGP even at high data regimes, showing the advantage of transfer learning.
The horizontally offset test case was chosen as a simple example where the LMC struggles to fit the
data. The response surfaces were generated by offsetting a sigmoid function horizontally by a random
constant. In this case, the LVMOGP outperforms the other Gaussian process models for both RMSE and
NLPD. This is because the LVMOGP can learn new surfaces with very few data points, as all it needs
to do is to correctly predict where the sloped region is. The LMC performs worse than the LVMOGP
because the offset cannot be represented by a linear combination of its latent functions, meaning it
requires more data to perform as well.
Across all the test cases, the LMC has poor NLPD at low data regimes. This is likely because it
cannot express uncertainty in the deterministic B matrix.
3.2 Prediction of DNA Amplification Experiments
Theperformanceoftheproposeddesignofexperimentsworkflowwasvalidatedusingdatafromcompeti-
tor DNA amplification experiments. This was done in three parts: first cross-validation was performed to
compare the predictive accuracy of the Gaussian process models; then a Bayesian optimization procedure
was used to optimize only the rate; finally the Bayesian optimization with drift penalty procedure was
applied.
9Example Functions Performance Metrics
Figure 4: Results of experiments with synthetically-generated data. The plots on the left show example
data-generating functions used for the synthetic experiments. The plots on the right show the RMSE
and NLPD for the three different test response surface types for each of the Gaussian process models.
New points are added randomly, and each line is the mean of 5 different randomly generated data sets,
all generated from the same test functions.
In cross-validation, the training set consisted of all the data from the two competitors that had the
most observations as well as a random subset of the remaining data, but ensuring all competitors had at
least one data point. This was repeated 150 times for each percentage of data in the training set. Figure
5 shows the RMSE and NLPD of the Gaussian process models’ predictions. The LVMOGP outperforms
the other Gaussian process models for both RMSE and NLPD for both rate and drift. The LMC has poor
NLPD in comparison to the other Gaussian process models, suggesting it has poor uncertainty estimates.
The AvgGP model shows little improvement with increased amounts of training data. This shows the
limitations of averaging the surfaces and justifies modeling each response surface separately.
3.3 Optimization of DNA Amplification Experiments
Ideally, fortheBayesianoptimizationexperimentswewouldintegratethealgorithmintotheexperimental
loop, collecting new data with each new recommendation of each Gaussian process model. However, due
tothecostofexperiments,thiswasinfeasible. Instead,weperformedretrospectiveBayesianoptimization
using the existing competitive DNA amplification dataset. The data was split into training and candidate
10
ylraenil
detalerrocnu
tesffo
detalerroc
ESMR
ESMR
ESMR
DPLN
DPLN
DPLNFigure 5: Results of cross-validation on the DNA amplification data for both rate and drift. For each
cross-validation run, the training set consisted of all the data from two competitors and a random subset
of the data on the remaining competitors, ensuring all competitors had at least one data point. This is
repeated for different percentages of data in the training set, and for each percentage, it is repeated 200
times.
sets, with the design of experiments algorithm only allowed to choose the next point out of the candidate
set. Bayesian optimization was run iteratively until all points had been selected or up to a maximum
number of iterations, whichever happened first.
Two learning scenarios were tested: the "Learning Many" scenario where all data from two competi-
tors were fully observed to begin with and then 16 competitors optimized in parallel; and the "One at a
Time" where each of the 16 competitors was optimized individually, with the 33 remaining competitors
included in the training set. These scenarios replicate likely wet lab experimentation scenarios — the first
for when many competitors need to be optimized at once, and the second for when many competitors are
already optimized and we want to add an extra one. The maximum number of iterations was 15 for the
rate-only optimization and 20 or 10 for the penalized optimization, depending on the learning scenario.
We also considered two methods for choosing the first experiment for a new competitor with no
previously observed data. Choosing the most central data point ("Center" in Figure 6) offers both
maximum reduction in variance across the response surface and ensures all competitor response surfaces
have a comparable point, which may help the transfer learning methods determine their similarities. It is
also a reasonable approximation to what a human experimenter might do if they had no prior knowledge
of the response surface. The second method is to let the Gaussian process model choose the first point
("Model’s Choice" in Figure 6) for a new competitor. For the AvgGP and the LVMOGP, this is possible
as they can make posterior predictions on new response surface. For the LVMOGP, the latent variable of
the new surface is determined as a weighted average of the latent variables of the response surfaces with
data that have the same probe and at least one matching primer. If there are no surfaces with matching
primers, we use a weighted average of the surfaces with the same probe. For the LMC and MOGP we
have no posterior, so the first point is selected randomly.
113.3.1 Single Objective Bayesian Optimization
Single Objective With Drift Penalty
Learning Many One at a Time Learning Many One at a Time
no. iterations no. iterations
Figure 6: Cumulative regret of each of the Gaussian process models for single objective (left) and
penalized (right) Bayesian optimization. Each line indicates the mean across 24 random seeds and all
competitors, while the shaded regions indicate the upper and lower 5% quantiles by random seed. The
top row is when the first point on each new surface is selected as being the center point, and the bottom
is when the model is allowed to choose the first point. The "Learning Many" scenario is when many
competitors are being optimized at the same time, and the "One at a Time" scenario is when one
competitor is being optimized, with all others being in the training set.
The left panel of Figure 6 shows the results of optimizing rate without considering the drift penalty.
The variance in the results comes from three sources. The first is the random selection of the next point
when two points have the same expected improvement — this causes unavoidable variation. The second
is due to the Gaussian process models optimizing to different hyperparameter values due to different
initializations. The different values arise because the optimization of the non-convex hyperparameter loss
surfaces is difficult. The final source of variation is the random starting point for the MOGP and LMC.
In all cases, the LVMOGP has much lower cumulative regret than the other models. The "Center"
start point allows us to compare the performance of the Gaussian process models without being skewed
by the first point. In this case the LMC and LVMOGP have the lowest cumulative regret. The ordering
changes between the "Center" and "Model’s choice" scenario, as in the latter the AvgGP and LVMOGP
are able to predict on new surfaces, giving them an advantage over the LMC and MOGP when choosing
the first point. See Appendix 8.4.3 for a table of the mean regrets of the first points for a quantification
of this improvement.
As the "One at a Time" scenario includes the data from all other competitors, the Gaussian process
models start with far more data than the "Learning Many" scenario. This means the AvgGP, LMC and
LVMOGP all have less regret in the "One at a Time" scenario, as they are able to transfer information
about the function values of competitors to improve prediction of the target competitor behavior. The
MOGP does not transfer information about function values, so performs relatively worse than the others
for the "One at a Time" scenario.
12
tergeR
evitalumuC
retneC
eciohC
s'ledoM
terger
evitalumuc
ertneC
eciohC
s'ledoM3.3.2 Bayesian Optimization with Drift Penalty
Rate Drift expected
mean uncertainty mean uncertainty improvement
Figure 7: Predictions for the rate and drift for each of the Gaussian process models. The BP and GC
axes are in log and logit scales respectively. These plots show the mean of the Gaussian process model
predictions and the uncertainty which here is 2×standard deviation. The expected improvement with
probability of feasibility is then plotted in the final column. This is for the case where we are optimizing
competitor FP005-FP004-EvaGreen and have observed one data point so far, with the models able to
choose the first point. The black contour lines on the mean plots indicate the target rate and threshold
drift values.
The right-hand panel of Figure 6 shows the cumulative regret for optimization of the rate with a
penalty on the drift. In all cases, the LVMOGP has the lowest cumulative regret at the end. In the
"Learning Many" scenario the AvgGP again benefits from selecting the first point for the "Model’s
Choice" starting point, but the LVMOGP actually performs slightly worse than it did for the "Center"
start point. This may be due to negative transfer in the drift predictions at very low data regimes making
the selection of the first point sub-optimal.
The ordering of the Gaussian process models is different for the "Learning Many" and "One at a
Time", probablybecausetheincreasedamountofdataallowstheLMCtopredictcomparativelybetterin
the"OneataTime"scenariothanthe"LearningMany". TheLVMOGPoutperformstheotherGaussian
process models the most in the "One at a Time" "Model’s Choice" experiment, which is likely due to
the large amounts of data on all competitors, except the target, and effective transfer of information
between them.
Figure 7 shows the rate and drift predictions and expected improvement for one iteration. Most
notably, the MOGP has no transfer learning, so has almost equal expected improvement for most of the
candidate points. The other three models transfer information across the competitors, meaning even
with one data point, they have much more complex predictions than the MOGP. We can also see how
the AvgGP, MOGP and LMC fit the drift poorly. This is because the drift is of a different order of
13
PGgvA
PGOM
CML
PGOMVLRate Drift
Figure8: LatentspaceoftheLVMOGPfortherateanddrift. Thecrossesindicatecompetitorswithprobe
primers and the dots indicate those with EvaGreen primers. The shaded circles indicate the uncertainty
in the latent positions.
magnitude depending on the fluorescent probe used. Most of the Gaussian process models are unable
to detect this, meaning they end up with a poor fit to the data. The LVMOGP, however, does identify
this —Figure 8 shows how it clusters the two probe types at different sides of the latent space. This
indicates it has recognized there are two regimes for drift, despite not being explicitly told which probe
a competitor uses.
See Appendix 8.4 for further Bayesian optimization results for both the single objective and penalized
optimizations. These results show the LVMOGP reaches the best point on the surfaces faster and with
less cumulative regret, more often than the other models for most test cases.
4 Discussion
Expensive and time consuming experiments require an intelligent design of experiments strategy. This
studydemonstrateshowatransferlearningsurrogatemodelcanbeusedinconjunctionwithBayesianop-
timizationtooptimizebiologicalsequences. ForthespecificcaseofdesigningcompetitorDNAmolecules
for a new diagnostic, reducing the number and therefore cost of experiments can help it reach the af-
fordability criteria for point of care settings (Land et al., 2019).
In Bayesian optimization, we need a surrogate function with reliable mean and uncertainty estimates
toensureabalancebetweenexplorationandexploitationwhenselectingnewpoints. Ourcross-validation
results in Section. 3.2 show the LVMOGP has better predictive accuracy than the other Gaussian process
models for both rate and drift. These results also demonstrate one of the limitations of the LMC: the
LMC has very high NLPD at low data regimes. This implies the LMC has poor uncertainty estimates
and is overfitting, a result which has been previously observed (Dai et al., 2017).
To replicate a real-life iterative design of experiments regime, we performed Bayesian optimization
on DNA amplification experimental data, but only allowing the models to select new points from existing
data. For the single objective optimization case, the LVMOGP has lower cumulative regret than the
other Gaussian process models for all test cases and starting points. This shows the LVMOGP transfer
learning approach is useful both when optimizing multiple competitors at a time, and when using the
data from all previous competitors to optimize a new one. The superior performance of the LMC and
LVMOGP for the "Center" starting point shows transfer learning speeds up the learning process. These
results also demonstrate the advantage of a surrogate model that can predict unseen surfaces — both
the LVMOGP and the AvgGP see a large improvement in regret when they are allowed to select the first
14point, both outperforming the MOGP and LMC where the first point is chosen at random.
When optimizing new biological sequences, there are often factors we wish to keep within a certain
range such as purity (Degerman et al., 2006) or biophysical properties (Khan et al., 2023). While these
can be treated as constraints, sometimes we may be willing to violate them slightly if it leads to a large
improvement in the objective function. In these scenarios, we can add a penalty. To apply a penalty on
the nuisance drift factor, we used probability of feasibility to penalize any point predicted to be above the
threshold drift value. In the penalized optimization, the LVMOGP had less cumulative regret than the
othermodelsbutthedifferenceinperformancewassmallerthanthatofthesingleobjectiveoptimization.
This could be due to the added challenge of dealing with the penalized on drift.
There was variation in the performance of the Gaussian process models’ across random seeds due to
the hyperparameter initialization. The LVMOGP has more variation due to its training being a harder
optimization problem. While smart initialization and random restarts helped with this issue, future work
couldsimplifytheoptimizationprocedures. TheoptimizationoftheGaussianprocessmodelsisdiscussed
in Appendix 8.5.
WhiletheworkflowoutlinedherewillbeusefulfortheoptimizationofnewcompetitorDNAmolecules,
it is not specific to this application and could be used for other applications where it is necessary to
optimize many similar tasks, such as engineering DNA probes (Lopez et al., 2018; Wadle et al., 2016),
exploringproteinfitnesslandscapes(Huetal.,2023), optimizingconditionsfordifferentcelllines(Hutter
et al., 2021), or inferring psuedotime for cellular processes (Campbell and Yau, 2015). With the rise in
lab automation, this workflow can be integrated into a design build test pipeline similar to Carbonell
et al. (2018) and HamediRad et al. (2019) which can greatly reduce the time required to optimize
new biomolecular components, speeding up the creation of new devices. This method could also be
incorporated into hybrid models in bio-processing and chemical engineering, for decision making for
systems with many similar components (Narayanan et al., 2023; Mowbray et al., 2021; Schweidtmann
et al., 2021).
This workflow could also be extended to multi-output optimization problems by using a multi-output
acquisition function or by finding Pareto optimal solutions (Belanger et al., 2019; Selega and Campbell,
2022; Jablonka et al., 2021; Schweidtmann et al., 2018). Similarly, the surrogate functions needed for
multi-fidelity learning, where we have multiple sources of information about an optimization task with
some sources being cheaper but less informative than others, are similar to those for transfer learning,
making it an easy extension (Folch et al., 2023; Sun et al., 2022).
5 Conclusion
We have shown how a transfer learning design of experiments workflow can be used to optimize many
competitor DNA molecules for an amplification-based diagnostics device. We used cross-validation to
demonstrate that the latent variable multi-output Gaussian process has the best predictive accuracy and
have shown it has the least regret when Bayesian optimization is performed on the DNA amplification
data. Future improvements to the optimization of the model hyperparameters would lead to faster and
more consistent performance of the algorithm. Despite this, we believe this workflow is applicable to
many other biotechnology applications and should be used to reduce the experimental load when there
are many similar tasks to be optimized but their similarity is a priori unknown.
6 Funding Information
This work was supported by the UKRI CDT in AI for Healthcare Grant No. EP/S023283/1, UK Re-
search and Innovation Grant No. EP/P016871/1, the BASF / RAEng Research Chair in Data-Driven
15Optimization, the US NIH Grant No. 5F32GM131594, the EPSRC IRC Next Steps Plus grant No.
EP/R018707/1 and the RAEng Chair in Emerging Technologies award No. CiET2021/ 94. For the
purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to
any Author Accepted Manuscript version arising.
7 Conflict of Interest
JPG and MMS are co-founders at Signatur Biosciences, Inc., a company which seeks to commercialize
the medical diagnostic technology this paper focuses on as a use-case, and they are co-inventors in a
patentdescribingamethodforamplification-basedquantificationofnucleicacids. Theremainingauthors
declare no conflict of interest.
16References
Badeau, B. A., Comerford, M. P., Arakawa, C. K., Shadish, J. A. and DeForest, C. A. (2018) Engineered
modular biomaterial logic gates for environmentally triggered therapeutic delivery. Nature chemistry,
10, 251–258. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5822735/.
Bader, J., Narayanan, H., Arosio, P. and Leroux, J.-C. (2023) Improving extracellular vesicles production
through a Bayesian optimization-based experimental design. European Journal of Pharmaceutics and
Biopharmaceutics, 182, 103–114. URL: https://www.sciencedirect.com/science/article/
pii/S0939641122002983.
Belanger, D., Vora, S., Mariet, Z., Deshpande, R., Dohan, D., Angermueller, C., Murphy, K., Chapelle,
O. and Colwell, L. (2019) Biological Sequences Design using Batched Bayesian Optimization.
Blakney, A. K., McKay, P. F., Ibarzo Yus, B., Hunter, J. E., Dex, E. A. and Shattock, R. J. (2019)
The Skin You Are In: Design-of-Experiments Optimization of Lipid Nanoparticle Self-Amplifying RNA
Formulations in Human Skin Explants. ACS Nano, 13, 5920–5930. URL: https://pubs.acs.org/
doi/10.1021/acsnano.9b01774.
Bonilla, E. V., Chai, K. and Williams, C. (2007) Multi-task Gaussian Process Prediction. Advances
in Neural Information Processing Systems, 20. URL: https://proceedings.neurips.cc/paper/
2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html.
Campbell, K. and Yau, C. (2015) Bayesian Gaussian Process Latent Variable Models for pseudotime
inference in single-cell RNA-seq data.
Cao, B., Pan, S. J., Zhang, Y., Yeung, D.-Y. and Yang, Q. (2010) Adaptive Transfer Learning. Proceed-
ings of the AAAI Conference on Artificial Intelligence, 24. URL: https://ojs.aaai.org/index.
php/AAAI/article/view/7682. Number: 1.
Carbonell,P.,Jervis,A.J.,Robinson,C.J.,Yan,C.,Dunstan,M.,Swainston,N.,Vinaixa,M.,Hollywood,
K. A., Currin, A., Rattray, N. J. W., Taylor, S., Spiess, R., Sung, R., Williams, A. R., Fellows,
D., Stanford, N. J., Mulherin, P., Le Feuvre, R., Barran, P., Goodacre, R., Turner, N. J., Goble,
C., Chen, G. G., Kell, D. B., Micklefield, J., Breitling, R., Takano, E., Faulon, J.-L. and Scrutton,
N.S.(2018)AnautomatedDesign-Build-Test-Learnpipelineforenhancedmicrobialproductionoffine
chemicals. CommunicationsBiology, 1, 1–10. URL:https://www.nature.com/articles/s42003-
018-0076-9. Number: 1 Publisher: Nature Publishing Group.
Cox, D. R. and Reid, N. (2000) The Theory of the Design of Experiments. CRC Press.
Dai, Z., Álvarez, M. and Lawrence, N. (2017) Efficient Modeling of Latent Information in Super-
vised Learning using Gaussian Processes. In Advances in Neural Information Processing Systems,
vol. 30. Curran Associates, Inc. URL: https://proceedings.neurips.cc/paper/2017/file/
1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf.
Degerman, M., Jakobsson, N. and Nilsson, B. (2006) Constrained optimization of a preparative ion-
exchange step for antibody purification. Journal of Chromatography A, 1113, 92–100. URL: https:
//www.sciencedirect.com/science/article/pii/S0021967306003013.
Deng, F., Pan, J., Liu, Z., Zeng, L. and Chen, J. (2023) Programmable DNA biocomputing circuits for
rapid and intelligent screening of SARS-CoV-2 variants. Biosensors and Bioelectronics, 223, 115025.
URL: https://www.sciencedirect.com/science/article/pii/S095656632201065X.
17Droettboom, M., Hunter, J., Firing, E., Caswell, T. A., Elson, P., Dale, D., Lee, J.-J., McDougall,
D., Root, B., Straw, A., Seppänen, J. K., Nielsen, J. H., May, R., Varoquaux, Yu, T. S., Moad,
C., Gohlke, C., Würtz, P., Hisch, T., Silvester, S., Ivanov, P., Whitaker, J., Cimarron, Hobson, P.,
Giuca, M., Thomas, I., mmetz bn, Evans, J., dhyams and NNemec (2015) matplotlib: v1.4.3. URL:
https://zenodo.org/record/15423.
Ebrahimi, S. B. and Samanta, D. (2023) Engineering protein-based therapeutics through structural and
chemical design. Nature Communications, 14, 2411. URL: https://www.nature.com/articles/
s41467-023-38039-x. Number: 1 Publisher: Nature Publishing Group.
Fellermann, H., Shirt-Ediss, B., Kozyra, J., Linsley, M., Lendrem, D., Isaacs, J. and Howard, T. (2019)
Design of experiments and the virtual PCR simulator: An online game for pharmaceutical scientists
and biotechnologists. Pharmaceutical Statistics, 18, 402–406. URL: https://www.ncbi.nlm.nih.
gov/pmc/articles/PMC6767770/.
Folch, J. P., Lee, R. M., Shafei, B., Walz, D., Tsay, C., van der Wilk, M. and Misener, R. (2023)
Combining multi-fidelity modelling and asynchronous batch Bayesian Optimization. Computers &
Chemical Engineering, 172, 108194. Publisher: Elsevier.
Gamble, C., Bryant, D., Carrieri, D., Bixby, E., Dang, J., Marshall, J., Doughty, D., Colwell, L., Berndl,
M., Roberts, J. and Frumkin, M. (2021) Machine Learning Optimization of Photosynthetic Microbe
Cultivation and Recombinant Protein Production. preprint, Bioengineering. URL: http://biorxiv.
org/lookup/doi/10.1101/2021.08.06.453272.
Garnett, R. (2023) Bayesian Optimization. 127–129. Cambridge University Press.
Gilman, J., Walls, L., Bandiera, L. and Menolascina, F. (2021) Statistical Design of Experiments for
SyntheticBiology. ACSSyntheticBiology,10,1–18. URL:https://doi.org/10.1021/acssynbio.
0c00385. Publisher: American Chemical Society.
Goertz, J. P., Sedgwick, R., Smith, F., Kaforou, M., Wright, V. J., Herberg, J. A., Kote-Jarai, Z., Eeles,
R., Levin, M., Misener, R., Wilk, M. v. d. and Stevens, M. M. (2023) Competitive Amplification Net-
works enable molecular pattern recognition with PCR. URL: https://www.biorxiv.org/content/
10.1101/2023.06.29.546934v1.
González, J., Longworth, J., James, D. C. and Lawrence, N. D. (2015) Bayesian Optimization for
SyntheticGeneDesign. arXiv:1505.01627[stat]. URL:http://arxiv.org/abs/1505.01627. ArXiv:
1505.01627.
HamediRad, M., Chao, R., Weisberg, S., Lian, J., Sinha, S. and Zhao, H. (2019) Towards a fully au-
tomated algorithm driven platform for biosystems design. Nature Communications, 10, 5150. URL:
https://www.nature.com/articles/s41467-019-13189-z. Number: 1 Publisher: Nature Pub-
lishing Group.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser,
E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M.,
Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T.,
Weckesser, W., Abbasi, H., Gohlke, C. and Oliphant, T. E. (2020) Array programming with NumPy.
Nature,585,357–362.URL:https://www.nature.com/articles/s41586-020-2649-2.Number:
7825 Publisher: Nature Publishing Group.
18Hie, B., Bryson, B. D. and Berger, B. (2020) Leveraging Uncertainty in Machine Learning Accelerates
Biological Discovery and Design. Cell Systems, 11, 461–477.e9. URL: https://www.cell.com/
cell-systems/abstract/S2405-4712(20)30364-1. Publisher: Elsevier.
Hu, R., Fu, L., Chen, Y., Chen, J., Qiao, Y. and Si, T. (2023) Protein engineering via Bayesian
optimization-guided evolutionary algorithm and robotic experiments. Briefings in Bioinformatics, 24,
bbac570. URL: https://doi.org/10.1093/bib/bbac570.
Hua, Y., Ma, J., Li, D. and Wang, R. (2022) DNA-Based Biosensors for the Biochemical Analysis: A Re-
view. Biosensors, 12, 183. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8945906/.
Hutter, C., von Stosch, M., Cruz Bournazou, M. N. and Butté, A. (2021) Knowledge transfer across
cell lines using hybrid Gaussian process models with entity embedding vectors. Biotechnology and
Bioengineering, 118, 4389–4401. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/
bit.27907.
Jablonka, K. M., Jothiappan, G. M., Wang, S., Smit, B. and Yoo, B. (2021) Bias free multiobjective
active learning for materials design and discovery. Nature Communications, 12, 2312. URL: https:
//www.nature.com/articles/s41467-021-22437-0. Number: 1 Publisher: Nature Publishing
Group.
Jones, D. R., Schonlau, M. and Welch, W. J. (1998) Efficient Global Optimization of Expensive Black-
Box Functions. Journal of Global Optimization, 13, 455–492. URL: https://doi.org/10.1023/A:
1008306431147.
Khan, A., Cowen-Rivers, A. I., Grosnit, A., Deik, D.-G.-X., Robert, P. A., Greiff, V., Smorodina, E.,
Rawat, P., Akbar, R., Dreczkowski, K., Tutunov, R., Bou-Ammar, D., Wang, J., Storkey, A. and
Bou-Ammar, H. (2023) Toward real-world automated antibody design with combinatorial Bayesian
optimization. Cell Reports Methods, 3, 100374.
Kreutz, C. and Timmer, J. (2009) Systems biology: experimental design. The FEBS journal, 276,
923–942.
Land, K. J., Boeras, D. I., Chen, X.-S., Ramsay, A. R. and Peeling, R. W. (2019) REASSURED diag-
nostics to inform disease control strategies, strengthen health systems and improve patient outcomes.
Nature Microbiology, 4, 46–54. URL: https://www.nature.com/articles/s41564-018-0295-3.
Number: 1 Publisher: Nature Publishing Group.
Lopez, R., Wang, R. and Seelig, G. (2018) A molecular multi-gene classifier for disease diagnostics.
NatureChemistry,10,746–754. URL:https://www.nature.com/articles/s41557-018-0056-1.
Number: 7 Publisher: Nature Publishing Group.
Lv, H., Li, Q., Shi, J., Fan, C. and Wang, F. (2021) Biocomputing Based on DNA Strand Displacement
Reactions. ChemPhysChem,22,1151–1166. URL:https://onlinelibrary.wiley.com/doi/abs/
10.1002/cphc.202100140.
Lyu, W., Xue, P., Yang, F., Yan, C., Hong, Z., Zeng, X. and Zhou, D. (2018) An Efficient Bayesian
OptimizationApproachforAutomatedOptimizationofAnalogCircuits. IEEETransactionsonCircuits
and Systems I: Regular Papers, 65, 1954–1967. Conference Name: IEEE Transactions on Circuits and
Systems I: Regular Papers.
19Matthews, A. G., Van Der Wilk, M., Nickson, T., Fujii, K., Boukouvalas, A., León-Villagrá, P., Ghahra-
mani, Z. and Hensman, J. (2017) GPflow: a Gaussian process library using tensorflow. The Journal
of Machine Learning Research, 18, 1299–1304.
Mehrian, M., Guyot, Y., Papantoniou, I., Olofsson, S., Sonnaert, M., Misener, R. and Geris, L. (2018)
Maximizing neotissue growth kinetics in a perfusion bioreactor: An in silico strategy using model
reduction and Bayesian optimization. Biotechnology and Bioengineering, 115, 617–629. URL: https:
//onlinelibrary.wiley.com/doi/abs/10.1002/bit.26500.
Mowbray, M., Savage, T., Wu, C., Song, Z., Cho, B. A., Del Rio-Chanona, E. A. and Zhang, D.
(2021)Machinelearningforbiochemicalengineering: Areview. BiochemicalEngineeringJournal,172,
108054. URL: https://www.sciencedirect.com/science/article/pii/S1369703X21001303.
Narayanan, H., Dingfelder, F., Condado Morales, I., Patel, B., Heding, K. E., Bjelke, J. R., Egebjerg, T.,
Butté, A., Sokolov, M., Lorenzen, N. and Arosio, P. (2021) Design of Biopharmaceutical Formulations
Accelerated by Machine Learning. Molecular Pharmaceutics, 18, 3843–3853. URL: https://pubs.
acs.org/doi/10.1021/acs.molpharmaceut.1c00469.
Narayanan, H., Luna, M. F., von Stosch, M., Cruz Bournazou, M. N., Polotti, G., Morbidelli, M.,
Butté, A. and Sokolov, M. (2020) Bioprocessing in the Digital Age: The Role of Process Models.
Biotechnology Journal, 15, 1900172. URL: https://onlinelibrary.wiley.com/doi/abs/10.
1002/biot.201900172.
Narayanan, H., von Stosch, M., Feidl, F., Sokolov, M., Morbidelli, M. and Butté, A. (2023) Hybrid
modeling for biopharmaceutical processes: advantages, opportunities, and implementation. Frontiers
in Chemical Engineering, 5. URL: https://www.frontiersin.org/articles/10.3389/fceng.
2023.1157889.
Papaneophytou, C. (2019) Design of Experiments As a Tool for Optimization in Recombinant Protein
Biotechnology: From Constructs to Crystals. Molecular Biotechnology, 61, 873–891. URL: https:
//doi.org/10.1007/s12033-019-00218-x.
Politis, S. N., Colombo, P., Colombo, G. and Rekkas, D. M. (2017) Design of experiments (DoE)
in pharmaceutical development. Drug Development and Industrial Pharmacy, 43, 889–901. URL:
https://doi.org/10.1080/03639045.2017.1291672.
Qian, L., Winfree, E. and Bruck, J. (2011) Neural network computation with DNA strand displacement
cascades. Nature, 475, 368–372. URL: https://www.nature.com/articles/nature10262.
Rasmussen, C. E. and Williams, C. K. I. (2006) Gaussian processes for machine learning. Adaptive
computation and machine learning. Cambridge, Mass: MIT Press.
Romero, P. A., Krause, A. and Arnold, F. H. (2013) Navigating the protein fitness landscape with
Gaussian processes. Proceedings of the National Academy of Sciences, 110, E193–E201. URL:
https://www.pnas.org/content/110/3/E193. Publisher: National Academy of Sciences Section:
PNAS Plus.
Rosa, S. S., Nunes, D., Antunes, L., Prazeres, D. M. F., Marques, M. P. C. and Azevedo, A. M. (2022)
MaximizingmRNAvaccineproductionwithBayesianoptimization. BiotechnologyandBioengineering,
119, 3127–3139. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/bit.28216.
20Salvatier, J., Wiecki, T. V. and Fonnesbeck, C. (2016) Probabilistic programming in Python using
PyMC3. PeerJ Computer Science, 2, e55. URL: https://peerj.com/articles/cs-55. Publisher:
PeerJ Inc.
Schonlau, M., Welch, W. J. and Jones, D. R. (1998) Global versus local search in constrained optimiza-
tionofcomputermodels. InNewdevelopmentsandapplicationsinexperimentaldesign,vol.34,11–26.
Institute of Mathematical Statistics. URL: https://projecteuclid.org/ebooks/institute-
of-mathematical-statistics-lecture-notes-monograph-series/New-developments-
and-applications-in-experimental-design/chapter/Global-versus-local-search-in-
constrained-optimization-of-computer-models/10.1214/lnms/1215456182.
Schweidtmann, A. M., Clayton, A. D., Holmes, N., Bradford, E., Bourne, R. A. and Lapkin, A. A.
(2018)Machinelearningmeetscontinuousflowchemistry: AutomatedoptimizationtowardsthePareto
front of multiple objectives. Chemical Engineering Journal, 352, 277–282. URL: https://www.
sciencedirect.com/science/article/pii/S1385894718312634.
Schweidtmann, A. M., Esche, E., Fischer, A., Kloft, M., Repke, J.-U., Sager, S. and Mitsos, A. (2021)
Machine Learning in Chemical Engineering: A Perspective. Chemie Ingenieur Technik, 93, 2029–2039.
URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cite.202100083.
Sedgwick, R., Goertz, J., Stevens, M., Misener, R. and van der Wilk, M. (2020) Design of Experiments
for Verifying Biomolecular Networks. arXiv:2011.10575 [cs, q-bio, stat]. URL: http://arxiv.org/
abs/2011.10575. ArXiv: 2011.10575.
Selega, A. and Campbell, K. R. (2022) Multi-objective Bayesian Optimization with Heuristic Objectives
for Biomedical and Molecular Data Analysis Workflows. Transactions on Machine Learning Research.
URL: https://openreview.net/forum?id=QspAcsAyis.
Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. and de Freitas, N. (2016) Taking the Human Out of
the Loop: A Review of Bayesian Optimization. Proceedings of the IEEE, 104, 148–175.
Sharpe, C., Seepersad, C. C., Watts, S. and Tortorelli, D. (2018) Design of Mechanical Metama-
terials via Constrained Bayesian Optimization. In Volume 2A: 44th Design Automation Confer-
ence, V02AT03A029. Quebec City, Quebec, Canada: American Society of Mechanical Engineers.
URL: https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2018/
51753/Quebec%20City,%20Quebec,%20Canada/273625.
Siuti, P., Yazbek, J. and Lu, T. K. (2013) Synthetic circuits integrating logic and memory in living cells.
Nature Biotechnology, 31, 448–452. URL: https://www.nature.com/articles/nbt.2510.
Snoek, J., Larochelle, H. and Adams, R. P. (2012) Practical Bayesian Optimization of
Machine Learning Algorithms. In Advances in Neural Information Processing Systems,
vol. 25. Curran Associates, Inc. URL: https://proceedings.neurips.cc/paper/2012/hash/
05311655a15b75fab86956663e1819cd-Abstract.html.
Sun, Y., Nathan-Roberts, W., Pham, T. D., Otte, E. and Aickelin, U. (2022) Multi-fidelity Gaussian
Process for Biomanufacturing Process Modeling with Small Data. URL: http://arxiv.org/abs/
2211.14493. ArXiv:2211.14493 [cs].
Swersky,K.,Snoek,J.andAdams,R.P.(2013)Multi-TaskBayesianOptimization. InAdvancesinNeural
Information Processing Systems 26 (eds. C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani
and K. Q. Weinberger), 2004–2012. URL: http://papers.nips.cc/paper/5086-multi-task-
bayesian-optimization.pdf.
21Taylor, C. J., Felton, K. C., Wigh, D., Jeraal, M. I., Grainger, R., Chessari, G., Johnson, C. N. and
Lapkin, A. A. (2023) Accelerated Chemical Reaction Optimization Using Multi-Task Learning. ACS
Central Science, 9, 957–968. URL: https://pubs.acs.org/doi/10.1021/acscentsci.3c00050.
The pandas development team, T. p. d. (2023) pandas-dev/pandas: Pandas. URL: https://zenodo.
org/record/7979740.
Tighineanu, P., Skubch, K., Baireuther, P., Reiss, A., Berkenkamp, F. and Vinogradska, J. (2022)
Transfer Learning with Gaussian Processes for Bayesian Optimization. In Proceedings of The 25th
International Conference on Artificial Intelligence and Statistics, 6152–6181. PMLR. URL: https:
//proceedings.mlr.press/v151/tighineanu22a.html. ISSN: 2640-3498.
Titsias,M.(2009)VariationalLearningofInducingVariablesinSparseGaussianProcesses.InProceedings
of the Twelth International Conference on Artificial Intelligence and Statistics, 567–574. PMLR. URL:
https://proceedings.mlr.press/v5/titsias09a.html. ISSN: 1938-7228.
Titsias,M.andLawrence,N.D.(2010)BayesianGaussianProcessLatentVariableModel. InProceedings
of the Thirteenth International Conference on Artificial Intelligence and Statistics, 844–851. JMLR
WorkshopandConferenceProceedings. URL:https://proceedings.mlr.press/v9/titsias10a.
html. ISSN: 1938-7228.
Uhrenholt, A. K. and Jensen, B. S. (2019) Efficient Bayesian Optimization for Target Vector Estimation.
In Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics
(eds. K. Chaudhuri and M. Sugiyama), vol. 89 of Proceedings of Machine Learning Research, 2661–
2670. PMLR. URL: https://proceedings.mlr.press/v89/uhrenholt19a.html.
Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski,
E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman,
K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I., Feng,
Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero,
E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F. and van Mulbregt, P. (2020)
SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17, 261–272.
URL: https://www.nature.com/articles/s41592-019-0686-2. Number: 3 Publisher: Nature
Publishing Group.
Wadle, S., Lehnert, M., Rubenwolf, S., Zengerle, R. and von Stetten, F. (2016) Real-time PCR probe
optimization using design of experiments approach. Biomolecular Detection and Quantification, 7,
1–8. URL: https://www.sciencedirect.com/science/article/pii/S2214753515300139.
Zadeh, J. N., Steenberg, C. D., Bois, J. S., Wolfe, B. R., Pierce, M. B., Khan, A. R., Dirks, R. M.
and Pierce, N. A. (2011) NUPACK: Analysis and design of nucleic acid systems. Journal of Compu-
tational Chemistry, 32, 170–173. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/
jcc.21596.
Zhang, Y., Tao, S., Chen, W. and Apley, D. W. (2020) A Latent Variable Approach to Gaussian Process
Modeling with Qualitative and Quantitative Factors. Technometrics, 62, 291–302. URL: https:
//www.tandfonline.com/doi/full/10.1080/00401706.2019.1638834.
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H. and He, Q. (2021) A Comprehensive
Survey on Transfer Learning. Proceedings of the IEEE, 109, 43–76. Conference Name: Proceedings
of the IEEE.
22Álvarez, M. A., Rosasco, L. and Lawrence, N. D. (2012) Kernels for Vector-Valued Functions: A Review.
Foundations and Trends® in Machine Learning, 4, 195–266. URL: https://www.nowpublishers.
com/article/Details/MAL-036. Publisher: Now Publishers, Inc.
238 Appendix
Nomenclature
Acronyms
AvgGP Average Gaussian Process
BP Number of Base Pairs
DNA Deoxyribonucleic Acid
ELBO Evidence lower bound to marginal likelihood for LVMOGP
GC Percentage Guanine-Cytosine Content
LMC Linear Model of Coregionalization
LVMOGP Latent Variable Multi-output Gaussian Process
MOGP Multi-output Gaussian Process
NLPD Negative Log Predictive Density
PCR Polymerase Chain Reaction
RMSE Root Mean Squared Error
Functions
¸ (·) Expected improvement acquisition function
EI
¸ (·) Acquisition function including probability of feasibility
p
GP Gaussian Process
f(·) Function of x
f (·) Rate function in competitor amplification
rate
f (·) Drift function
drift
g(·) Latent Gaussian processes in the linear model of coregionalisation
k(·;·) Gaussian Process covariance function
K (·;·) Covariance function of the data
ii
K (·;·) Cross covariance function between the data and inducing points
iu
K (·;·) Covariance function of the inducing points
uu
m(·) Gaussian Process mean function
PF(·) Probability of feasibility
Parameters and Variables
24‹ stochastic variable defined as the squared difference between observed outputs and the target
value
f Predictions at locations X
∗ ∗
h Latent variable of the pth output function
p
I Identity matrix
u Inducing variables
W Vector of weights of the latent functions in the linear model of coregionalisation
x Input location such that x ∈ RD
‘ Lengthscale of dimension d
d
› Noise added to y where › ∼ N(0;ff2I))
n
– Non-centrality parameter of target vector optimization expected improvement
—(X ) Predicted mean at locations X
∗ ∗
— Mean of the pth latent variable
hp
‌ Carrying capacity
ff(X ) Predicted covariance at locations X
∗ ∗
ff2 Kernel variance
k
ff2 Noise variance of Gaussian process
n
Σ Variance of the pth latent variable
hp
fi Cycle number
„ Gaussian Process hyperparameters
B Coregionalization matrix in the LMC
D Dimensions of x
F Fluorescence in DNA amplification reaction
F Fluorescence at the beginning of the DNA amplification reaction
0
F Fluorescence at the end of the DNA amplification reaction
T
H Latent variables such that H = [h 1;:::;h p]T ∈ RQH×P
M Mean of the variational distribution on Z
P Number of output functions in multi-output Gaussian Process
q(·) Variational distribution
Q Number of covariance matrices in the LMC
25S Variance of the variational distribution on Z
t = ‹‚−2
T Target rate
rate
T Drift threshold
drift
X Training inputs of Gaussian Process X = {x ;:::;x } ∈ RN×D
1 N
X Locations to be evaluated
∗
y Noisy evaluations of x
y Data point which is closest to the target out of the train and test datasets for a given surface
best
Z Inducing points
Miscellaneous
H The latent space in the LVMOGP
G An approximation to the cumulative non-central ffl2 distribution function
268.1 Latent Variable Multi-output Gaussian Process Implementation
Gaussian processes are normally trained by maximizing the log marginal likelihood. However, the pres-
ence of the latent variable distributions in the LVMOGP means the log marginal likelihood is no longer
tractable. Instead, Dai et al. (2017) used variational inference to approximate a lower bound to this log
marginal likelihood, following the method proposed by Titsias (2009) and Titsias and Lawrence (2010).
In variational inference, the aim is to minimize the Kullback-Leibler divergence between an approximate
posterior and a true posterior.
OurimplementationoftheLVMOGPtakesaconcatenationoftheinputdataandtheircorresponding
latent variables X˜ = [X;H :] ∈ RN×(D+QH) where H
:
to denotes the vector of latent inputs for each
observed data point. All inputs X for the same output dimension will have the same latent variable, h .
p p
For the LVMOGP this variational lower bound is given as:
N »
1 X 1 1
ELBO = − log(2ıff2)+ − yTy + y ⟨K ⟩ K−1M
2 n 2ff2 :i :i ff2 :i iu q(H:i) uu
n n
i=1
1
− Tr(K−1⟨KTK ⟩ K−1(MMT +S))
2ff2 uu iu iu q(H:i) uu
n
–
1
− (Tr(⟨K ⟩ )−Tr(K−1⟨KTK ⟩ )
2ff2 ii q(H:i) uu iu iu q(H:i)
n
N
X
−KL[q(u)||p(u)]− KL[q(H )||p(H )] (19)
:i :i
i=1
where ⟨K⟩ denotes a kernel expectation over the variational distribution of the latent variable of
q(hi)
datapointi. K andK arethecovariancefunctionsofthedataandtheinducingpointsZ respectively,
ii uu
while K is the cross covariance function between the two. Tr is the trace of a matrix. M and S are the
iu
mean and covariance of the variational distribution over inducing points q(Z) ∼ N(M;S). The second
term in this expression can be viewed as a data fit term, while the last term can be seen as a complexity
penalty.
Two types of prediction are relevant using the LVMOGP. The first is when we have new input points
X and new position on the latent space h . In this case, the posterior prediction can be calculated in
∗ ∗
closed form. The second, and more likely, prediction case is when we want to predict a new point X
∗
at a point on the latent space where we already have data with latent variable h . This integration is
p
intractable, but following Titsias and Lawrence (2010), the first and second moments can be computed
in closed form if using a squared exponential kernel.
8.2 Data Availability
Raw data is available on request from rdm-enquiries@imperial.ac.uk. Code for the synthetic experiments
can be found at the following link: https://github.com/RSedgwick/TLGPs Code for the DNA am-
plification experiments Bayesian optimization can be found here: https://github.com/RSedgwick/
TL_DOE_4_DNA.
8.3 Data Summary
Eachcompetitorisdefinedbyitsprimer-reportercombination. Foreachoftheseprimer-paircombinations
we then have data at different guanine-cytosine content and no. of base pairs combinations. Table 1
gives a summary of the number of unique locations on each of the competitors.
27Not To Be Optimized To Be Optimized
Primer Reporter Combination No. Unique Primer Reporter Combination No. Unique
Locations Locations
FP004-RP004-EvaGreen 28 FP004-RP004-Probe 53
FP002-RP002x-Probe 12 FP001-RP001x-EvaGreen 24
FP004-RP004x-Probe 12 FP001-RP001x-Probe 20
FP001-RP001-Probe 9 RP001x-FP002-Probe 19
FP001-RP005-Probe 8 FP002-RP002x-EvaGreen 15
FP004-RP004x-EvaGreen 8 FP005-FP001-EvaGreen 14
FP003-RP008-Probe 5 FP004-FP005-Probe 8
FP006-RP006-Probe 5 FP005-FP001-Probe 8
FP005-RP005-Probe 5 FP005-FP004-EvaGreen 8
FP002-RP002-EvaGreen 4 RP002x-FP005-Probe 8
FP002-RP006-Probe 4 RP008x-FP001-EvaGreen 8
FP057.1.0-RP003x-Probe 3 RP008x-FP005-Probe 8
FP003-RP008x-EvaGreen 3 FP001-RP004-EvaGreen 7
FP003-RP008-EvaGreen 3 RP002x-FP004-EvaGreen 6
FP002-RP002-Probe 3 FP002-RP004-EvaGreen 3
FP001-RP001-EvaGreen 2 RP002x-FP002-EvaGreen 2
FP003-RP003-Probe 1
FP057.1.0-RP003x-EvaGreen 1
Table 1: Summary of the amount of data we have for each competitor design surface. Each unique
location refers to a unique GC-BP combination.
8.4 Extra Bayesian Optimization Results
The following tables contain extra results for the Bayesian optimization experiments. The first table in
each section, Tables 2 and 5, shows counts of the first model to get to the best point on a surface for
all competitors and seeds. If two models get to the best point on the same iteration, they are both
counted as "winners". The second table, Tables 3 and 6 shows counts of the models with the lowest
cumulative regret for each competitor and seed. The same thing applies if two models have the same
cumulative regret. For the single objective optimization, Table 4 shows the average number of iterations
for each model to get within tolerance of the target rate (+/- 0.05). For the penalized optimization
Table 7 shows the average number of iterations for each model to get either within tolerance of the rate
target with no drift penalty, or to the best point (which may have a drift penalty). For some of the runs
with the drift penalty, some of the models failed to get to the best point for some surfaces within the
experimental budget. In these cases, those surfaces were discarded and the average was taken for the
surfaces where all the models had managed to get to the best point within the experimental budget.
8.4.1 Single Objective Optimization
ExtraresultsforthesingleobjectiveBayesianoptimization. TheseresultsdemonstratethattheLVMOGP
gets to the best point more often (Table 2) and has has the lowest cumulative regret (Table 3) more
oftenthantheothermodels. TheLVMOGPalsoreachesthebestpointinthelowestnumberofiterations
for all the learning scenarios (Table 4).
28learning scenario starting point MOGP Avg GP LMC LVMOGP
center 124 121 144 255
learning many
model’s choice 107 119 97 147
center 140 140 156 215
one at a time
model’s choice 86 118 87 191
Table2: TableshowingcountsofthefirstGaussianprocessmodeltoreachthebestpointonasurfacefor
the single objective Bayesian optimization experiments. The counts are the number of times a Gaussian
process model did the best on a competitor for each seed. If two Gaussian process models performed the
same for a given instance, they are both counted. This is for 16 competitors and 25 random seeds.
learning scenario starting point MOGP Avg GP LMC LVMOGP
center 182 80 140 197
learning many
model’s choice 85 94 83 117
center 129 140 131 206
one at a time
model’s choice 99 106 87 159
Table 3: Table showing counts of the first Gaussian process model had the lowest cumulative regret
on a surface for the single objective Bayesian optimization experiments. The counts are the number of
times a Gaussian process model did the best on a competitor for each seed. If two Gaussian process
models performed the same for a given instance, they are both counted. This is for 16 competitors and
25 random seeds.
learning scenario starting point MOGP Avg GP LMC LVMOGP
center 3.13 3.25 3.11 2.58
learning many
model’s choice 3.08 2.63 3.09 2.44
center 2.94 3.06 2.85 2.15
one at a time
model’s choice 2.94 2.63 2.63 1.81
Table 4: Table showing the mean number of iterations need for the models to get within tolerance of the
target rate (+/- 0.05) for the single objective optimization. This is for 16 competitors and 25 random
seeds.
8.4.2 Bayesian Optimization with Drift Penalty
Extra results for the Bayesian optimization with a penalty on drift. These results demonstrate that the
LVMOGP gets to the best point more often (Table 5) and has has the lowest cumulative regret (Table 6)
more often than the other models for most of the learning scenarios. The LVMOGP also reaches the
best point in the lowest number of iterations for all the learning scenarios (Table 7).
learning scenario starting point MOGP Avg GP LMC LVMOGP
center 142 157 123 165
learning many
model’s choice 89 122 101 111
center 141 137 153 217
one at a time
model’s choice 75 102 79 164
Table 5: Table showing counts of the first Gaussian process model to reach the best point on a surface
for the penalized Bayesian optimization experiments. The counts are the number of times a Gaussian
process model did the best on a competitor for each seed. If two Gaussian process models performed the
same for a given instance, they are both counted. This is for 16 competitors and 24 random seeds.
29learning scenario starting point MOGP Avg GP LMC LVMOGP
center 180 118 100 163
learning many
model’s choice 85 103 84 111
center 173 118 139 204
one at a time
model’s choice 83 70 65 156
Table 6: Table showing counts of the first Gaussian process model had the lowest cumulative regret on
a surface for the penalized Bayesian optimization experiments. The counts are the number of times a
Gaussian process model did the best on a competitor for each seed. If two Gaussian process models
performed the same for a given instance, they are both counted. This is for 16 competitors and 24
random seeds.
learning scenario starting point MOGP Avg GP LMC LVMOGP
center 2.47 3.26 2.95 2.38
learning many
model’s choice 3.39 3.13 3.23 2.13
center 3.00 3.20 2.82 2.47
one at a time
model’s choice 2.70 2.69 2.44 1.41
Table 7: Table showing the mean number of iterations need for the models to either get within tolerance
of the target rate (+/- 0.05) without drift penalty or reach the best point (which may have a penalty)
for the penalized optimization. For some runs, one or more of the models would not achieve this within
the experimental budget. In these cases, the affected competitors were removed and the mean taken of
the remaining. This is for 16 competitors and 24 random seeds.
8.4.3 Comparison of Choice of First Point
Table 8 shows the average regret of the first data point chosen by each of the models for each of the
learning scenarios. From this table, it is clear to see the AvgGP and the LVMOGP improve on the regret
of the central point, and outperform the random selection of the MOGP and LMC. This demonstrates
that having a principled method of selecting the first point is useful for reducing regret.
learning scenario starting point MOGP Avg GP LMC LVMOGP
center 0.588 0.588 0.588 0.588
learning many
model’s choice 0.651 0.499 0.703 0.464
center 0.588 0.588 0.588 0.588
one at a time
model’s choice 0.675 0.308 0.623 0.309
Table 8: Table of the mean regret of the first data point for each of the learning scenarios for each of
the models.
8.5 Optimization of Gaussian Process Models
We used gradient descent to optimize the Gaussian process hyperparameters. The optimization of the
hyperparameters of the Gaussian process models are non-convex problems, meaning gradient descent
algorithms will only find local optima. To improve the hyperparameter optimization procedure, we used
principled methods of initialization along side random restarts to fit the same Gaussian process model
multiple times, and then select the hyperparameter configuration with the best log marginal likelihood.
These regimes differ slightly for the different models.
30For all model, unless otherwise states, we initialize the lengthscale randomly as ‘ ∼ Uniform(0;1),
noise variance randomly as ff ∼ Uniform(0;0:1) and kernel variance ff = 1. For the MOGP and AvgGP
n k
we did nine random restarts with these settings.
For the LMC we used three different methods for initializing W and », with three random restarts
for each:
• Both W and » random. In this initialization, we initialize W ∼ Uniform(0:1;1) and » ∼
Uniform(0:1;1).
• W randomand» = 0. InthisinitializationW ∼ Uniform(0:1;1)and» = 10−6. Thisinitialization
was chosen as we thought it would favor solutions with small » so it would better fit the linear
correlation case, where the test functions are generated as linear combinations of some linear
functions.
• W random and » = 1. In this initialization W ∼ Uniform(0:1;1) and » = 1. We chose this
initialization to favor large », which is useful for the uncorrelated test case, as it would encourage
the output functions to behave independently of each other.
The random initialisations for W helped the initialisations for two reasons: firstly, in the GPflow
implementation if W is not initialized it defaults to a rank of 1, and secondly by initializing to random
values rather than all one value we avoid saddle points on the optimization surface.
For the LVMOGP we used three different initialization procedures, again with three random restarts
for each:
• Random. In this initialization all hyperparameters and variational parameters were initialized ran-
domly. the means of the latent variables were initialized as — ∼ Uniform(−1;1).
H
• GPy. This is the method used in the GPy implementation of the LVMOGP (Dai et al., 2017), that
has following three steps:
1. A sparse MOGP is fitted to the data using a set of inducing points Z which are common
to all outputs. The mean predictions —(Z) ∈ RNU×P of the output function values at these
inducing inputs is then calculated:
—(Z) = K(Z;Z)[K(Z;Z)+ff2I]−1Y: (20)
n
The sparse MOGP is used is ensure all output functions are observed at the same input
locations for the functional PCA, which is necessary when data is observed at different loca-
tions on different surfaces. It also serves the purpose of smoothing the data plus the trained
lengthscalesareusedtoinitialisethelengthscalesoftheobserveddimensionsoftheLVMOGP.
2. The mean predictions —(Z) ∈ RNU×P are then used as inputs to functional PCA. The first
Q
H
eigenvectors V ∈ RNU×QH and eigenvalues {– q}Q q=H
1
of —(Z)T—(Z) are calculated and
used to project —(Z) into latent space
H = —(Z)TV; (21)
where H ∈ RP×QH. The relative contributions of each of the eigenvalues is also calculated
as:
–˜ –
& = q –˜ = q (22)
q max{–˜ }QH q PQH –
i i=1 i=1 i
313. The latent variables H from the functional PCA are used to initialize the latent variables of a
Bayesian Gaussian process latent variable model. The lengthscales of the Bayesian Gaussian
process latent variable model are initialized to {1}QH . Once the Bayesian Gaussian process
&q q=1
latent variable model is trained, the latent variables and hyperparameters of the Bayesian
Gaussian process latent variable model are used to initialize those of the LVMOGP.
• PCA. In this initialization, the first two steps of the GPy initialization are followed. This means
fitting a sparse MOGP to the data and performing principle component analysis (PCA) on the
posterior predictions at inducing point locations. The MOGP hyperparameters were then used to
initialize the LVMOGP observed lengthscale, kernel variance and noise variance. The output of the
PCA was used to initialize the latent variable means and the lengthscale of the latent dimensions.
This initialization was chosen as a simplified version of the GPy initialization.
See the github repositories in Appendix 8.2 for more details.
In the synthetic experiments, we found the method of initializing the hyperparamters affected the end
log marginal likelihood, with no initialization outperforming all others for each model. Therefore, we
decided to continue with all initializations for the PCR data experiments. For the PCR data experiments
we did 10 random restarts for each initialization, due to the randomness of some of the initializations.
32