Coresets for Multiple â„“ Regression
ğ‘
David P. Woodruff Taisuke Yasuda
Carnegie Mellon University Carnegie Mellon University
dwoodruf@cs.cmu.edu taisukey@cs.cmu.edu
June 5, 2024
Abstract
Acoreset ofadatasetwithğ‘›examplesandğ‘‘featuresisaweightedsubsetofexamplesthatissufficient
forsolvingdownstreamdataanalytictasks. Nearlyoptimalconstructionsofcoresetsforleastsquaresand
â„“ linear regression with a single response are known in prior work. However, for multiple â„“ regression
ğ‘ ğ‘
where there can be ğ‘š responses, there are no known constructions with size sublinear in ğ‘š. In this
work, we construct coresets of size ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘) for ğ‘<2 and ğ‘‚Ëœ(ğœ€âˆ’ğ‘ğ‘‘ğ‘/2) for ğ‘>2 independently of ğ‘š (i.e.,
dimension-free) that approximate the multiple â„“ regression objective at every point in the domain up ğ‘
to (1Â±ğœ€) relative error. If we only need to preserve the minimizer subject to a subspace constraint, we
improve these bounds by an ğœ€ factor for all ğ‘>1. All of our bounds are nearly tight.
We give two application of our results. First, we settle the number of uniform samples needed to
approximate â„“ Euclidean power means up to a (1+ğœ€) factor, showing that Î˜Ëœ(ğœ€âˆ’2) samples for ğ‘=1,
ğ‘
Î˜Ëœ(ğœ€âˆ’1) samples for 1<ğ‘<2, and Î˜Ëœ(ğœ€1âˆ’ğ‘) samples for ğ‘>2 is tight, answering a question of Cohen-
Addad, Saulpic, and Schwiegelshohn. Second, we show that for 1 < ğ‘< 2, every matrix has a subset
of ğ‘‚Ëœ(ğœ€âˆ’1ğ‘˜) rows which spans a (1+ğœ€)-approximately optimal ğ‘˜-dimensional subspace for â„“ subspace
ğ‘
approximation, which is also nearly optimal.
4202
nuJ
4
]SD.sc[
1v23420.6042:viXra1 Introduction
Least squares linear regression and â„“ linear regression are some of the most fundamental and practically
ğ‘
valuable computational problems in statistics and optimization. In this problem, our input is an ğ‘›Ã—ğ‘‘ matrix
AâˆˆRğ‘›Ã—ğ‘‘ and a response vector bâˆˆRğ‘›, and our goal is to output an approximate minimizer xË† âˆˆRğ‘‘ such
that
â€–AxË†âˆ’bâ€–ğ‘ â‰¤(1+ğœ€) minâ€–Axâˆ’bâ€–ğ‘. (1)
ğ‘ ğ‘
xâˆˆRğ‘‘
Amongthevastliteratureonâ„“ regression, samplingalgorithmsandcoresets, whicharealgorithmsthatselect
ğ‘
a weighted subset of the rows of A and b that suffice to solve â„“ regression, have played major roles in the
ğ‘
development of efficient algorithms. That is, we seek a diagonal matrix SâˆˆRğ‘›Ã—ğ‘› with few non-zero entries,
i.e., nnz(S)â‰ªğ‘›, such that the weighted subset of rows SA and Sb are sufficient to compute a solution xË†
satisfying (1). We will often refer to nnz(S) as the sample complexity. We focus on approaches that construct
S by i.i.d. sampling of each of the ğ‘› rows:
Definition 1.1 (â„“ sampling matrix). Let ğ‘ â‰¥ 1. A random diagonal matrix S âˆˆ Rğ‘›Ã—ğ‘› is a random â„“
ğ‘ ğ‘
samplingmatrixwithsamplingprobabilities{ğ‘ }ğ‘› if for each ğ‘–âˆˆ[ğ‘›], the ğ‘–th diagonal entry is independently
ğ‘– ğ‘–=1
set to be
{ï¸ƒ
1/ğ‘1/ğ‘ with probability ğ‘
S = ğ‘– ğ‘–
ğ‘–,ğ‘–
0 otherwise
Two well-studied guarantees for S are strong coresets and weak coresets. Strong coresets refer to coresets
that preserve the value of the objective function at every point in the domain, while weak coresets only
guarantee that the unconstrained minimizer is preserved. If we only care about solving the unconstrained â„“
ğ‘
regression problem, then weak coresets are sufficient to solve this problem, and it is known that weak coresets
can be substantially smaller than strong coresets in certain settings [MMWY22]. On the other hand, strong
coresets are necessary when the objective function must be evaluated at points away from the optimum, for
example for constrained optimization problems.
Definition 1.2 (Strong coreset). We say that S is a strong coreset if â€–S(Axâˆ’b)â€–ğ‘ =(1Â±ğœ€)â€–Axâˆ’bâ€–ğ‘
ğ‘ ğ‘
simultaneously for every xâˆˆRğ‘‘.
Definition 1.3 (Weak coreset). We say that S is a weak coreset if
â€–AxË†âˆ’bâ€–ğ‘ â‰¤(1+ğœ€) minâ€–Axâˆ’bâ€–ğ‘
ğ‘ ğ‘
xâˆˆRğ‘‘
for xË† =argmin â€–S(Axâˆ’b)â€–ğ‘.
xâˆˆRğ‘‘ ğ‘
The efficient construction of coresets for â„“ regression has been studied in a long line of work [Cla05,
ğ‘
DMM06a, DMM06b, DDH+09] culminating in the â„“ Lewis weight sampling algorithm [Lew78, BLM89,
ğ‘
Tal90, LT91, Tal95, SZ01, CP15, WY23b], which gives an algorithm that constructs a strong coreset S with
{ï¸ƒ
ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘) ğ‘â‰¤2
nnz(S)= .
ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘ğ‘/2) ğ‘>2
A related line of work in the active â„“ regression setting shows that weak coresets for â„“ regression with
ğ‘ ğ‘
â§ ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘) ğ‘=1
âªâªâªâ¨ğ‘‚Ëœ(ğœ€âˆ’1ğ‘‘)
1<ğ‘<2
nnz(S)=
âªâªâªâ©ğ‘‚ğ‘‚ Ëœ(( ğœ€ğœ€ âˆ’âˆ’ (1 ğ‘ğ‘‘ âˆ’)
1)ğ‘‘ğ‘/2)
ğ‘ğ‘= >22
can be constructed even without knowing b [CP19, CD21, PPP21, MMWY22, WY23a]. Note that these
bounds strictly improve over the strong coreset guarantees of â„“ Lewis weight sampling for 1<ğ‘<3.
ğ‘
11.1 Multiple â„“ regression
ğ‘
It is often the case that we are interested in more than just one target b to predict, and in general, we may
wish to simultaneously fit ğ‘š target vectors that are given by a matrix BâˆˆRğ‘›Ã—ğ‘š and solve the minimization
problem
ğ‘š
min â€–AXâˆ’Bâ€–ğ‘ = min âˆ‘ï¸ â€–AXe âˆ’Be â€–ğ‘
XâˆˆRğ‘‘Ã—ğ‘š ğ‘,ğ‘ XâˆˆRğ‘‘Ã—ğ‘š ğ‘— ğ‘— ğ‘
ğ‘—=1
This is known as the multiple response â„“ regression problem, or simply the multiple â„“ regression problem,
ğ‘ ğ‘
and is the focus of the present work.
1.1.1 Coreset constructions for ğ‘=2
For ğ‘=2, the construction of strong coresets for the multiple response problem follows almost immediately
from strong coresets for the single response problem due to orthogonality and the Pythagorean theorem, and
we can construct S such that
â€–S(AXâˆ’B)â€–2 =(1Â±ğœ€)â€–AXâˆ’Bâ€–2
ğ¹ ğ¹
with nnz(S)=ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘) samples. Indeed, assume without loss of generality that A has orthogonal columns,
and suppose that S satisfies
â€¢ â€–SAxâ€–2 =(1Â±ğœ€)â€–Axâ€–2 for every xâˆˆRğ‘‘ (i.e., S is a subspace embedding)
2 2
â€¢ â€–S(AX*âˆ’B)â€–2 =(1Â±ğœ€)â€–AX*âˆ’Bâ€–2 where X* is the optimal minimizer
ğ¹ ğ¹
â€¢ â€–AâŠ¤SâŠ¤S(AX*âˆ’B)â€–2 â‰¤(ğœ€2/ğ‘‘)â€–Aâ€–2â€–AX*âˆ’Bâ€–2 =ğœ€2â€–AX*âˆ’Bâ€–2
ğ¹ ğ¹ ğ¹ ğ¹
Then, the following argument of Section 7.5 of [CW13] shows that S is a strong coreset. Indeed,
â€–S(AXâˆ’B)â€–2 =â€–SA(Xâˆ’X*)â€–2 +â€–S(AX*âˆ’B)â€–2 +2tr(ï¸€ (Xâˆ’X*)âŠ¤AâŠ¤SâŠ¤S(AX*âˆ’B))ï¸€
ğ¹ ğ¹ ğ¹
by expanding the square, and the inner product term is bounded by
âƒ’ âƒ’tr(ï¸€ (Xâˆ’X*)âŠ¤AâŠ¤SâŠ¤S(AX*âˆ’B))ï¸€âƒ’ âƒ’â‰¤â€–Xâˆ’X*â€– ğ¹â€–AâŠ¤SâŠ¤S(AX*âˆ’B)â€–
ğ¹
â‰¤ğœ€â€–A(Xâˆ’X*)â€– â€–AX*âˆ’Bâ€–
ğ¹ ğ¹
â‰¤ğœ€â€–AXâˆ’Bâ€–2
ğ¹
and S also preserves the quantities â€–SA(Xâˆ’X*)â€–2 and â€–S(AX*âˆ’B)â€–2 up to (1Â±ğœ€) relative error. A
ğ¹ ğ¹
similar trick is available in the weak coreset setting (see, e.g., Section 3.1 of [CNW16]), which gives a bound
of nnz(S)=ğ‘‚Ëœ(ğœ€âˆ’1ğ‘‘) for this guarantee. Unfortunately, almost every step in the above argument uses special
properties of the â„“ norm that are not available for the â„“ norm, and thus we will need completely different
2 ğ‘
arguments to handle ğ‘Ì¸=2.
1.1.2 Challenges for ğ‘Ì¸=2
If we desire only weak coresets, then prior results on active â„“ regression in fact almost immediately provide
ğ‘
a solution. These results show that a weak coreset S for the single response â„“ regression problem can
ğ‘
be constructed independently of b, and with the dependence of nnz(S) on the failure probability ğ›¿ being
polylogarithmic. Thus by setting the failure rate to ğ›¿ =1/10ğ‘š, we can simultaneously solve every column of
B independently with overall probability at least 9/10.
For strong coresets, however, such a column-wise strategy must be implemented carefully. If we consider
constructing a strong coreset for a single column ğ‘— âˆˆ[ğ‘š], then the sampling probabilities now depend on the
target vector Be , so the sampling complexity would need to scale as ğ‘š rather than polylog(ğ‘š) as in the
ğ‘—
previous upper bound weak coresets. On the other hand, another natural strategy is to mimic the strategy
for the ğ‘=2 case and take the sampling probabilities to only guarantee an â„“ subspace embedding for the
ğ‘
column space of A and that ğ‘ â‰¥ â€–eâŠ¤B*â€–ğ‘/â€–B*â€–ğ‘ for B* := AX* âˆ’B. This is a reasonable choice of
ğ‘– ğ‘– ğ‘ ğ‘,ğ‘
sampling probabilities, and indeed it is not hard to see that
â€–S(AXâˆ’B)â€–ğ‘ =(1Â±ğœ€)â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
2for any fixed XâˆˆRğ‘‘Ã—ğ‘š with only nnz(S)=ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘) samples for ğ‘<2 and nnz(S)=ğ‘‚Ëœ(ğœ€âˆ’2ğ‘‘ğ‘/2) samples for
ğ‘>2 via a Bernstein tail bound. However, it is unclear how to extend a guarantee for any single XâˆˆRğ‘‘Ã—ğ‘š
toaguaranteesimultaneouslyforall XâˆˆRğ‘‘Ã—ğ‘š. Althoughthedependenceonthefailurerateğ›¿ islogarithmic,
a net argument, or even more sophisticated chaining arguments, over the possible choices of XâˆˆRğ‘‘Ã—ğ‘š seem
to require a union bound over sets of size exp(ğ‘‘ğ‘š), thus again introducing a linear dependence on ğ‘š in the
sample complexity nnz(S). As we show, a careful blend of these two ideas will be necessary to obtain our
strong coreset result.
1.2 Strong coresets for multiple â„“ regression
ğ‘
Our first main result is the first construction of strong coresets for multiple â„“ regression that is independent
ğ‘
of ğ‘š.
Theorem 1.4 (Strong coresets for multiple â„“ regression). Let AâˆˆRğ‘›Ã—ğ‘‘, BâˆˆRğ‘›Ã—ğ‘š, and ğ‘â‰¥1. There is
ğ‘
an algorithm which constructs S with
â§ [ï¸‚ ]ï¸‚
ğ‘‚(ğ‘‘) ğ‘‘ 1
âªâªâªâ¨
ğœ€2
(logğ‘‘)2log
ğœ€
+log
ğ›¿
1â‰¤ğ‘<2
nnz(S)=
âªâªâªâ©ğ‘‚(ğ‘‘
ğœ€ğ‘ğ‘/2)[ï¸‚
(logğ‘‘)2logğ‘‘
ğœ€
+log1
ğ›¿]ï¸‚
ğ‘>2
such that with probability at least 1âˆ’ğ›¿,
â€–S(AXâˆ’B)â€–ğ‘ =(1Â±ğœ€)â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
simultaneously for every XâˆˆRğ‘‘Ã—ğ‘š. Furthermore, S can be constructed in ğ‘‚Ëœ(nnz(A)+nnz(B)+poly(ğ‘‘))
time.
We achieve a nearly optimal dependence on ğ‘‘ and ğœ€, as we show that â„¦(ğ‘‘ğ‘/2/ğœ€ğ‘) rows are necessary for
strong coresets in Theorem 5.1 for ğ‘>2, while it is known that â„¦Ëœ(ğ‘‘/ğœ€2) rows are necessary even for ğ‘š=1
for ğ‘ < 2 [LWW21]. We note that our upper bound shows that multiple â„“ regression is as easy as single
ğ‘
response â„“ regression for ğ‘<2, while our lower bound demonstrates an interesting separation between the
ğ‘
two for ğ‘>2.
1.2.1 Initial logğ‘š bound
Our main technique is to generalize the â€œpartition by sensitivityâ€ technique introduced in the active â„“
ğ‘
regression work of [MMWY22] and show how this can be applied to the strong coreset setting. We describe
the idea for the case of ğ‘<2, as the case of ğ‘>2 is analogous.
In the active â„“ regression setting, we must show that we can design sampling algorithms that preserve
ğ‘
the objective function value, even if we do not know the target vector b. In this setting, one of the main
observationsof[MMWY22]isthateventhoughwecannotpreserveâ€–Axâˆ’bâ€–ğ‘ itself, wecanactuallypreserve
ğ‘
the difference â€–Axâˆ’bâ€–ğ‘âˆ’â€–bâ€–ğ‘, if â€–bâ€–ğ‘ =ğ‘‚(OPTğ‘) which is without loss of generality. To see this idea,
ğ‘ ğ‘ ğ‘
assume (without loss of generality due to [DDH+09]) that we restrict our attention to â€–Axâ€–ğ‘ =ğ‘‚(OPTğ‘).
ğ‘
Then, the analysis of [MMWY22] proceeds by partitioning the coordinates of b into two sets, those such
that |b(ğ‘–)|ğ‘ is larger than ğœ€âˆ’ğ‘w OPTğ‘ and those that are smaller than this threshold, where w is the ğ‘–-th â„“
ğ‘– ğ‘– ğ‘
Lewis weight of A. It is known that w bounds the sensitivities of A, that is, |[Ax](ğ‘–)|ğ‘ â‰¤w â€–Axâ€–ğ‘ so it
ğ‘– ğ‘– ğ‘
follows that for any |b(ğ‘–)|ğ‘ â‰¥ğœ€âˆ’ğ‘w OPTğ‘, we have that
ğ‘–
||[Axâˆ’b](ğ‘–)|ğ‘âˆ’|b(ğ‘–)|ğ‘|=ğ‘‚(ğœ€)|b(ğ‘–)|ğ‘
for any xâˆˆRğ‘‘ with â€–Axâ€–ğ‘ =ğ‘‚(OPTğ‘). On the other hand, if |b(ğ‘–)|ğ‘ â‰¤ğœ€âˆ’ğ‘w OPTğ‘, then we have by the
ğ‘ ğ‘–
triangle inequality that
||[Axâˆ’b](ğ‘–)|ğ‘âˆ’|b(ğ‘–)|ğ‘|â‰¤ğ‘‚(ğœ€âˆ’ğ‘)w OPTğ‘.
ğ‘–
Thus, up to an additive ğ‘‚(ğœ€)(â€–Sbâ€–ğ‘ + â€–bâ€–ğ‘) error, ||[Axâˆ’b](ğ‘–)|ğ‘âˆ’|b(ğ‘–)|ğ‘| has sensitivities which are
ğ‘ ğ‘
controlled by the â„“ Lewis weights of A. This allows one to show that sampling by the â„“ Lewis weights of A
ğ‘ ğ‘
preserves â€–Axâˆ’bâ€–ğ‘âˆ’â€–bâ€–ğ‘ for all â€–Axâ€–ğ‘ =ğ‘‚(OPTğ‘).
ğ‘ ğ‘ ğ‘
3In order to apply this idea to the strong coreset setting, we generalize the above argument to multiple
scales. That is, we replace OPTğ‘ by an arbitrary scale ğ‘…â‰¥â€–bâ€–ğ‘, and show that for every â€–Axâ€–ğ‘ â‰¤ğ‘‚(ğ‘…)
ğ‘ ğ‘
that
âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–Sbâ€–ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘âˆ’â€–bâ€–ğ‘)ï¸€âƒ’ âƒ’â‰¤ğœ€(ğ‘…+â€–Sbâ€–ğ‘)
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
Finally, we can generalize this to the following guarantee by union bounding over finitely many scales ğ‘…,
which holds for every xâˆˆRğ‘‘:
âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–Sbâ€–ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘âˆ’â€–bâ€–ğ‘)ï¸€âƒ’ âƒ’â‰¤ğœ€(ï¸€ â€–bâ€–ğ‘+â€–Sbâ€–ğ‘+â€–Axâ€–ğ‘)ï¸€
(2)
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
This guarantee is in a form that can be summed over the ğ‘š columns of B. Thus, if a logğ‘š dependence
is admissible, then we can apply the above result with failure probability 1/10ğ‘š, union bound over the ğ‘š
columns, and sum the results to obtain
âƒ’ âƒ’(â€–S(AXâˆ’B)â€–ğ‘ âˆ’â€–SBâ€–ğ‘ )âˆ’(â€–AXâˆ’Bâ€–ğ‘ âˆ’â€–Bâ€–ğ‘ )âƒ’ âƒ’â‰¤ğœ€(ï¸€ â€–Bâ€–ğ‘ +â€–SBâ€–ğ‘ +â€–AXâ€–ğ‘ )ï¸€ .
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
Now suppose that we additionally have
â€¢ â€–SBâ€–ğ‘ =(1Â±ğœ€)â€–Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
â€¢ â€–Bâ€–ğ‘ =ğ‘‚(OPTğ‘) (which is without loss of generality by subtracting an ğ‘‚(1)-optimal solution)
ğ‘,ğ‘
Then, we have
â€–S(AXâˆ’B)â€–ğ‘ =â€–AXâˆ’Bâ€–ğ‘ âˆ’â€–Bâ€–ğ‘ +â€–SBâ€–ğ‘ Â±ğ‘‚(ğœ€)(ï¸€ â€–Bâ€–ğ‘ +â€–AXâ€–ğ‘ )ï¸€
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
=â€–AXâˆ’Bâ€–ğ‘ Â±ğœ€â€–Bâ€–ğ‘ Â±ğ‘‚(ğœ€)(ï¸€ â€–Bâ€–ğ‘ +â€–AXâ€–ğ‘ )ï¸€
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
=â€–AXâˆ’Bâ€–ğ‘ Â±ğ‘‚(ğœ€)â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
so we indeed have a strong coreset as desired.
1.2.2 Removing the ğ‘š dependence
Next, we show how to completely remove the ğ‘š dependence, which requires additional ideas. When applying
(2) to each of the ğ‘š columns, suppose that we set the failure probability to poly(ğœ€ğ›¿) instead of ğ‘‚(1/ğ‘š).
Then, this guarantee will hold for a 1âˆ’poly(ğœ€ğ›¿) fraction of â€œgoodâ€ columns, for which we can obtain
(1Â±ğœ€) approximations. On the remaining poly(ğœ€ğ›¿) fraction of â€œbadâ€ columns, note that the mass of B
on these columns is at most poly(ğœ€ğ›¿)â€–Bâ€–ğ‘ with probability 1âˆ’ğ›¿ by Markovâ€™s inequality. Then on these
ğ‘,ğ‘
columns, â€–S(AXâˆ’B)e â€– is just â€–SAXe â€– up to a small total additive error of poly(ğœ€ğ›¿)â€–Bâ€–ğ‘ . In
ğ‘— ğ‘ ğ‘— ğ‘ ğ‘,ğ‘
turn, we have that â€–SAXe â€– = (1Â±ğœ€)â€–AXe â€– by using that S is an â„“ subspace embedding. Thus,
ğ‘— ğ‘ ğ‘— ğ‘ ğ‘
by combining with the (1Â±ğœ€) approximation on the rest of the â€œgoodâ€ columns, we can still ensure that
â€–S(AXâˆ’B)â€– =(1Â±ğœ€)â€–AXâˆ’Bâ€– .
ğ‘,ğ‘ ğ‘,ğ‘
1.3 Weak coresets for multiple â„“ regression
ğ‘
In the weak coreset setting, we consider a generalized multiple â„“ regression problem, where we are given
ğ‘
a design matrix A âˆˆ Rğ‘›Ã—ğ‘‘, an â€œembeddingâ€ G âˆˆ Rğ‘¡Ã—ğ‘š, and a target matrix B âˆˆ Rğ‘›Ã—ğ‘š, and we wish to
approximately minimize the objective function â€–AXGâˆ’Bâ€– .
ğ‘,ğ‘
As noted previously, for multiple â„“ regression without an embedding (i.e., G=I ) the construction of
ğ‘ ğ‘¡
weak coresets follows relatively straightforwardly by applying active â„“ regression results along each column.
ğ‘
However, this strategy fails when we must additionally handle the embedding matrix G, as this constraint
couples the columns of AX together. Furthermore, we argue that handling the embedding G is substantially
moreinterestingthattheunconstrainedcase. Indeed,asweseelaterinSections1.4and1.5,theincorporation
of the embedding G will allow us to handle interesting extensions of our results to settings beyond the
entrywise â„“ norm via the use of a linear embedding into this norm. We will denote the optimal value as
ğ‘
OPT:= min â€–AXGâˆ’Bâ€–
ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
and let X* denote the matrix achieving this optimum unless otherwise noted. We will prove the following
result:
4Theorem 1.5 (Weak coresets for multiple â„“ regression). Let A âˆˆ Rğ‘›Ã—ğ‘‘, G âˆˆ Rğ‘¡Ã—ğ‘š, B âˆˆ Rğ‘›Ã—ğ‘š, and
ğ‘
1â‰¤ğ‘<âˆ. There is an algorithm which constructs S independently of B with
ğ‘‚(ğ‘‘)[ï¸‚
ğ‘‘
1]ï¸‚(ï¸‚ 1)ï¸‚2
nnz(S)= (logğ‘‘)2log +log loglog
ğœ€2ğ›¿2 ğœ€ ğ›¿ ğœ€
for ğ‘=1,
ğ‘‚(ğ‘‘)[ï¸‚
ğ‘‘
1]ï¸‚(ï¸‚ 1)ï¸‚2
nnz(S)= (logğ‘‘)2log +log loglog
ğœ€ğ›¿2 ğœ€ ğ›¿ ğœ€
for 1<ğ‘<2, and
ğ‘‚(ğ‘‘ğ‘/2)[ï¸‚
ğ‘‘
1]ï¸‚(ï¸‚ 1)ï¸‚ğ‘
nnz(S)= (logğ‘‘)2log +log loglog
ğœ€ğ‘âˆ’1ğ›¿ğ‘ ğœ€ ğ›¿ ğœ€
for ğ‘>2 such that with probability at least 1âˆ’ğ›¿, for any XË† âˆˆRğ‘‘Ã—ğ‘¡ such that
â€–S(AXË†Gâˆ’B)â€–ğ‘ â‰¤(1+ğœ€) min â€–S(AXGâˆ’B)â€–ğ‘ ,
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
we have
â€–AXË†Gâˆ’Bâ€–ğ‘ â‰¤(1+ğ‘‚(ğœ€)) min â€–AXGâˆ’Bâ€–ğ‘ .
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
Conditioned on the event that â€–S(AX*Gâˆ’B)â€–ğ‘ =ğ‘‚(â€–AX*Gâˆ’Bâ€–ğ‘ ) for the global optimizer X*, the
ğ‘,ğ‘ ğ‘,ğ‘
dependence on ğ›¿ can be replaced by a single log1 factor and the poly(loglog1) factor can be removed.
ğ›¿ ğœ€
Furthermore, S can be constructed in ğ‘‚Ëœ(nnz(A)+ğ‘‘ğœ”) time.
We achieve a nearly optimal dependence on ğ‘‘ and ğœ€, as we show that â„¦(ğ‘‘ğ‘/2/ğœ€ğ‘âˆ’1) rows are necessary
for weak coresets in Theorem 5.2 for ğ‘>2. Our weak coreset upper bound result together with our strong
coreset lower bound of Theorem 5.1 shows a tight ğœ€ factor separation between the two coreset guarantees.
Note that in the statement of Theorem 1.5, the dependence on the failure rate ğ›¿ is polynomial. This
is in fact necessary if we restrict our algorithm to be of the form of â€œsample-and-solveâ€ algorithms whose
sampling matrices S do not depend on B, as demonstrated in a lower bound result of Theorem 12.8 of
[MMWY22]. The only reason why this dependence becomes necessary in the analysis of the upper bound
is that â€–S(AX*Gâˆ’B)â€–ğ‘ may be as large as ğ‘‚(ï¸€1)ï¸€ â€–AX*Gâˆ’Bâ€–ğ‘ with probability at least ğ›¿, and this
ğ‘,ğ‘ ğ›¿ ğ‘,ğ‘
is the source of the hardness result of Theorem 12.8 of [MMWY22] as well. This is a mild problem and
can be easily circumvented in one of two ways. The first is to simply allow the algorithm to incorporate
the row norms of B into the sampling probabilities just as in Theorem 1.4. However, this would not give
an active regression algorithm that makes only polylogarithmic in ğ›¿ many queries. If we wish for such an
active regression algorithm, then we can follow [MMWY22] and consider the following two-stage procedure.
First, we can obtain a constant factor solution XË† with a polylogarithmic dependence on ğ›¿ by employing a
â€œmedianâ€-likeprocedure(seeSection3.1of[MMWY22]). Then, wecanrunlog1 copiesofthealgorithm, each
ğ›¿
of which succeeds with probability 1âˆ’ğ›¿. Then, we can sort the runs by their estimates â€–S(AXË†Gâˆ’B)â€–ğ‘
ğ‘,ğ‘
anddiscardhalfoftherunswiththehighestvaluesofâ€–S(AXË†Gâˆ’B)â€–ğ‘ . Thisguaranteesthattheremaining
ğ‘,ğ‘
runs have â€–S(AX*Gâˆ’B)â€–ğ‘ =ğ‘‚(1)â€–AX*Gâˆ’Bâ€–ğ‘ with probability at least 1âˆ’ğ›¿, which is enough for
ğ‘,ğ‘ ğ‘,ğ‘
the rest of the argument to go through with only a polylogarithmic dependence on ğ›¿.
1.4 Applications: sublinear algorithms for Euclidean power means
Our first application of our results on coresets for multiple â„“ regression is on designing coresets for the
ğ‘
Euclidean power means problem. In this problem, we are given as input a set of ğ‘› points {b }ğ‘› âŠ†Rğ‘¡, and
ğ‘– ğ‘–=1
we wish to find a center xË† âˆˆRğ‘¡ that minimizes the sum of the Euclidean distances to xË†, raised to the power
ğ‘. That is, we seek to minimize the objective function given by
ğ‘›
âˆ‘ï¸
â€–xâˆ’b â€–ğ‘ =â€–1xâŠ¤âˆ’Bâ€–ğ‘
ğ‘– 2 ğ‘,2
ğ‘–=1
5where 1 is the ğ‘›Ã—1 matrix of all ones, B âˆˆ Rğ‘›Ã—ğ‘¡ is the matrix with b in its ğ‘› rows, and â€–Â·â€– is the
ğ‘– ğ‘,2
(ğ‘,2)-norm of a matrix given by the â„“ norm of the Euclidean norm of the rows. This is a fundamental
ğ‘
problem which generalizes the well-studied problems of the mean (ğ‘=2), geometric median (ğ‘=1), and
minimum enclosing balls (ğ‘=âˆ). Coresets and sampling algorithms for this problem were recently studied
by [CSS21], who showed that a uniform sample of ğ‘‚Ëœ(ğœ€âˆ’(ğ‘+3)) points suffices to output a center xË† âˆˆRğ‘¡ such
that
â€–1xË†âŠ¤âˆ’Bâ€–ğ‘ â‰¤(1+ğœ€)minâ€–1xâŠ¤âˆ’Bâ€–ğ‘ =(1+ğœ€)OPTğ‘.
ğ‘,2 xâˆˆRğ‘¡ ğ‘,2
In comparison to the upper bounds, the lower bounds given by [CSS21] was â„¦(ğœ€âˆ’(ğ‘âˆ’1)) which is off by an
ğœ€4 factor compared to the upper bound, which was improved to â„¦(ğœ€âˆ’1) for 1<ğ‘<2 by [MMWY22] and
â„¦(ğœ€âˆ’2) for ğ‘=1 by [CD21, PPP21].
One of the main open questions highlighted by the work of [CSS21] is to obtain tight bounds for this
problem: how many uniform samples are necessary and sufficient to output a (1+ğœ€)-approximate solution to
the Euclidean power means problem. Our main contribution is a nearly optimal algorithm which matches the
lower bounds of [CD21, PPP21, CSS21, MMWY22].
Theorem 1.6. Let {b }ğ‘› âŠ†Rğ‘‘. Then, there is a sublinear algorithm which uniformly samples at most
ğ‘– ğ‘–=1
â§ ğ‘‚(ğœ€âˆ’2)(ï¸€ log1 +log1)ï¸€ log1 ğ‘=1
âªâ¨ ğœ€ ğ›¿ ğ›¿
ğ‘ = ğ‘‚(ğœ€âˆ’1)(ï¸€ log1 +log1)ï¸€ log1 1<ğ‘â‰¤2
ğœ€ ğ›¿ ğ›¿
âªâ©ğ‘‚(ğœ€1âˆ’ğ‘)(ï¸€ log1 +log1)ï¸€ log1 2<ğ‘<âˆ
ğœ€ ğ›¿ ğ›¿
rows b and outputs a center xË† such that
ğ‘–
ğ‘› ğ‘›
âˆ‘ï¸ âˆ‘ï¸
â€–xË†âˆ’b â€–ğ‘ â‰¤(1+ğœ€) min â€–xâˆ’b â€–ğ‘
ğ‘– 2 ğ‘– 2
xâˆˆRğ‘‘
ğ‘–=1 ğ‘–=1
with probability at least 1âˆ’ğ›¿.
To apply the techniques developed in this work to the Euclidean power means problem, we need to embed
the (ğ‘,2)-norm into the entrywise â„“ norm. To make this reduction, we use a classic result of Dvoretzky and
ğ‘
Milman [Dvo61, Mil71], which shows that a random subspace of a normed space is approximately Euclidean.
We will need the following version of this result for â„“ norms:
ğ‘
Theorem 1.7 (Dvoretzkyâ€™s theorem for â„“ norms [FLM77, PVZ17]). Let ğ‘ â‰¥ 1 and 0 < ğœ€ < 1/ğ‘. Let
ğ‘
ğ‘›â‰¥ğ‘‚(max{ğœ€âˆ’2ğ‘˜,ğœ€âˆ’1ğ‘˜ğ‘/2), and let GâˆˆRğ‘›Ã—ğ‘˜ be an i.i.d. random Gaussian matrix. Then, with probability
at least 2/3, â€–Gxâ€–ğ‘ =(1Â±ğœ€)ğ‘›â€–xâ€–ğ‘ for every xâˆˆRğ‘˜.
ğ‘ 2
Note then that if G is an appropriately scaled random Gaussian matrix, then we have that
â€–1xâŠ¤âˆ’Bâ€–ğ‘ =(1Â±ğœ€)â€–1xâŠ¤Gâˆ’BGâ€–ğ‘
ğ‘,2 ğ‘,ğ‘
by the above result. We may now note that the latter optimization problem is exactly of the form of an
embedded â„“ regression problem, and thus our weak coreset results immediately apply to this problem. In
ğ‘
fact, handling this Dvoretzky embedding is our main motivation for studying the â„“ regression problem with
ğ‘
the embedding. We also note that similar reductions are possible by making use of other linear embeddings
between â„“ norms [WW19, LWY21, LLW23]. The full argument is given in Appendix D.1.
ğ‘
In addition to sharpening the bound of [CSS21] to optimality, we note that our techniques, both
algorithmically and in the analysis, are simpler than the prior work of [CSS21]. The previous algorithm
required partitioning the dataset into â€œringsâ€ of points with similar costs and preprocessing these rings.
Furthermore, the analysis uses a specially designed chaining argument with custom net constructions that
require terminal Johnsonâ€“Lindenstrauss embeddings. On the other hand, our algorithm simply runs multiple
instancesofaâ€œsample-and-solveâ€algorithm,wheretherunwithlowestsampledmassiskept. Furthermore,the
analysis largely builds on existing net constructions for â„“ regression, and does not need terminal embeddings.
ğ‘
In fact, our proof for the power means problem only need â„“ regression net constructions in ğ‘‘=1 dimensions
ğ‘
due to our use of Dvoretzkyâ€™s theorem, which avoids the sophisticated constructions of [BLM89] for large ğ‘‘
6and only needs a standard volume argument (Remark B.15). Our partition of sensitivity can also be thought
of as a coarse notion of rings, where we only consider two classes of costs, â€œbigâ€ and â€œsmallâ€, whereas prior
work requires finer a classification of points into rings of points whose costs are related up to a constant
factor.
1.5 Applications: spanning coresets for â„“ subspace approximation
ğ‘
As a second application of our results, we give the first construction of spanning coresets for â„“ subspace
ğ‘
approximation with nearly optimal size. The â„“ subspace approximation is a popular generalization of the
ğ‘
classic Frobenius norm low rank approximation problem, where the input is a set of ğ‘› points {a }ğ‘› in ğ‘‘
ğ‘– ğ‘–=1
dimensions, and we wish to compute a rank ğ‘˜ subspace ğ¹ âŠ†Rğ‘‘ that minimizes
ğ‘›
âˆ‘ï¸
â€–aâŠ¤(I âˆ’P )â€–ğ‘
ğ‘– ğ‘‘ ğ¹ 2
ğ‘–=1
where P denotes the orthogonal projection matrix onto ğ¹. Equivalently, we can write this as
ğ¹
min â€–A(I âˆ’P )â€–ğ‘ .
ğ‘‘ ğ¹ ğ‘,2
rank(ğ¹)â‰¤ğ‘˜
While strong and weak coresets for this problem have attracted much attention [FL11, SV12, SW18,
HV20, FKW21, WY23a], our main contribution to this line of research is on a different coreset guarantee,
which we call spanning coresets. Spanning coresets are subsets of the points a which span a (1+ğœ€)-optimal
ğ‘–
rank ğ‘˜ subspace, and is another popular guarantee in this literature [DV07, SV12, CW15]. In addition to
being an interesting object in its own right [SV12], the existence of small spanning coresets have found
applications to constructions for strong and weak coresets for â„“ subspace approximation [HV20].
ğ‘
Definition 1.8 (Spanning coreset). Let {a }ğ‘› âŠ†Rğ‘‘. A subset ğ‘† âŠ†[ğ‘›] is a (1+ğœ€)-spanning coreset if the
ğ‘– ğ‘–=1
points {a } span a ğ‘˜-dimensional subspace ğ¹Ë† such that
ğ‘– ğ‘–âˆˆğ‘†
â€–A(I âˆ’P )â€–ğ‘ â‰¤(1+ğœ€) min â€–A(I âˆ’P )â€–ğ‘ .
ğ‘‘ ğ¹^ ğ‘,2 ğ‘‘ ğ¹ ğ‘,2
rank(ğ¹)â‰¤ğ‘˜
Our main result is the following upper bound on the size of spanning coresets.
Theorem 1.9. Let {a }ğ‘› âŠ†Rğ‘‘, 1â‰¤ğ‘<âˆ, ğ‘˜ âˆˆN, and 0<ğœ€<1. Then, there exists a (1+ğœ€)-spanning
ğ‘– ğ‘–=1
coreset ğ‘† of size at most
â§
ğ‘‚(ğœ€âˆ’2ğ‘˜)(log(ğ‘˜/ğœ€))3 ğ‘=1
âªâ¨
|ğ‘†|= ğ‘‚(ğœ€âˆ’1ğ‘˜)(log(ğ‘˜/ğœ€))3 1<ğ‘â‰¤2
âªâ©ğ‘‚(ğœ€1âˆ’ğ‘ğ‘˜ğ‘/2)(log(ğ‘˜/ğœ€))3
2<ğ‘<âˆ
In particular, we improve the previous best result of ğ‘‚(ğœ€âˆ’1ğ‘˜2log(ğ‘˜/ğœ€)) due to Theorem 3.1 of [SV12] in
the ğ‘˜ dependence for all 1â‰¤ğ‘<4. The proof of this result is given in Section D.2. Furthermore, we give the
first lower bounds on the size of spanning coresets by generalizing an argument of [DV06] for ğ‘=2, showing
that spanning coresets must have size at least â„¦(ğœ€âˆ’1ğ‘˜) in Theorem 5.3. Together, our results settle the size of
spanning coresets up to polylogarithmic factors for 1<ğ‘<2. To obtain this result, we again use Dvoretzkyâ€™s
theorem to embed the problem to an embedded entrywise â„“ norm problem, and then apply our weak coreset
ğ‘
results.
Finally, we note that our spanning coreset lower bound implies other interesting lower bounds for coresets.
First, we note that weak coresets for â„“ subspace approximation are automatically spanning coresets, so our
ğ‘
lower bound for spanning coresets also gives the first nontrivial lower bound on the size of weak coresets for
â„“ subspace approximation. Secondly, we note that our proof of Theorem 1.9 in fact shows that any upper
ğ‘
bound on weak coresets for â„“ regression with an embedding implies upper bounds for spanning coresets of
ğ‘
the same size. Thus, our spanning coreset lower bound in fact implies an â„¦(ğ‘‘/ğœ€) lower bound on the size of
weak coresets for â„“ regression with an embedding, which establishes that our weak coreset upper bound for
ğ‘
â„“ regression (Theorem 1.5) is also nearly optimal for 1<ğ‘<2 up to polylogarithmic factors.
ğ‘
On the other hand, for ğ‘>2, our weak coreset lower bound of Theorem 5.2 shows that our technique of
reducing spanning coresets to weak coresets cannot prove a better upper bound than the result of Theorem
1.9, andthusnewideasarerequiredtoimproveupontheğ‘‚Ëœ(ğœ€âˆ’1ğ‘˜2)spanningcoresetupperboundofTheorem
3.1 of [SV12]. This is an interesting open problem.
71.6 Open directions
We conclude with several potential directions for future research. One interesting question is to improve our
understanding of upper bounds and lower bounds for coresets for single response â„“ regression.
ğ‘
Question 1.10. How many rows are necessary and sufficient for strong and weak coresets for single response
â„“ regression?
ğ‘
For strong coresets, this questions is already nearly optimally settled for ğ‘<2 with Î˜Ëœ(ğœ€âˆ’2ğ‘‘) rows known
to be necessary and sufficient [LWW21]. For ğ‘>2, however, there is still a gap in our understanding, with
the best known upper bound being Î˜Ëœ(ğœ€âˆ’2ğ‘‘ğ‘/2) via â„“ Lewis weight sampling while the best known lower
ğ‘
bound is only â„¦(ğœ€âˆ’1ğ‘‘ğ‘/2+ğœ€âˆ’2ğ‘‘). It is an interesting question to determine whether the lower bound can be
improved to match the â„“ Lewis weight sampling upper bound or not.
ğ‘
For weak coresets, the deficiencies are much more glaring. There are currently no known nontrivial lower
bounds for weak coresets, while the best known algorithms are the better of the two upper bounds given by
active â„“ regression and strong coresets, both of which are substantially more restricted settings than weak
ğ‘
coresets.
Finally, we highlight the question of obtaining a nearly optimal upper bound on spanning coresets for â„“
ğ‘
subspace approximation for ğ‘>2.
Question 1.11. How many rows are necessary and sufficient for spanning coresets for â„“ subspace approxi-
ğ‘
mation?
We conjecture that our lower bound of â„¦(ğ‘˜/ğœ€) is tight, while the best known upper bound is the better of
our Theorem 1.9 and ğ‘‚Ëœ(ğœ€âˆ’1ğ‘˜2) [SV12].
2 Preliminaries
2.1 â„“ Lewis weights
ğ‘
Definition 2.1 (One-sided â„“ Lewis weights [JLS22, WY22]). Let AâˆˆRğ‘›Ã—ğ‘‘ and ğ‘âˆˆ(0,âˆ). Let ğ›¾ âˆˆ(0,1].
ğ‘
Then, weights wâˆˆRğ‘› are ğ›¾-one-sided â„“ Lewis weights if w â‰¥ğ›¾Â·ğœ (W1/2âˆ’1/ğ‘A), where W:=diag(w). If
ğ‘ ğ‘– ğ‘–
ğ›¾ =1, we just say that w are one-sided â„“ Lewis weights.
ğ‘
The following theorem collects the results of [CP15, JLS22] on the fastest known algorithms for approxi-
mating one-sided â„“ Lewis weights:
ğ‘
Theorem 2.2. Let AâˆˆRğ‘›Ã—ğ‘‘ and ğ‘>0. There is an algorithm which computes one-sided â„“ Lewis weights
ğ‘
(Def. 2.1) w such that ğ‘‘â‰¤â€–wâ€– â‰¤2ğ‘‘ in ğ‘‚Ëœ(nnz(A)+ğ‘‘ğœ”) time.
1
3 Strong coresets
Theorem 3.1 (Strong coresets for multiple â„“ regression). Let XË† âˆˆRğ‘‘Ã—ğ‘š satisfy
ğ‘
â€–AXË† âˆ’Bâ€–ğ‘ â‰¤ğ‘‚(1) min â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘š
and let BË† := AXË† âˆ’ B. Let S be the â„“ sampling matrix (Definition 1.1) with sampling probabilities
ğ‘
ğ‘ â‰¥min{1,w /ğ›¼+v /ğ›½} for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘›, v =â€–eâŠ¤BË†â€–ğ‘/â€–BË†â€–ğ‘ ,
ğ‘– ğ‘– ğ‘– ğ‘ ğ‘– ğ‘– ğ‘ ğ‘,ğ‘
â§ [ï¸‚ 1]ï¸‚âˆ’1
âªâªâªâ¨ğ‘‚(ğ›¾)ğœ€2 (logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘<2
ğ›¼=
âªâªâªâ©ğ‘‚ â€–w(ğ›¾ â€–ğ‘ ğ‘/ /2 2) âˆ’ğœ€ 1ğ‘[ï¸‚ (logğ‘‘)2logğ‘›+log1 ğ›¿]ï¸‚âˆ’1
ğ‘>2
1
8and ğ›½ =ğ‘‚(ğœ€âˆ’2log1). Then with probability at least 1âˆ’ğ›¿,
ğ›¿
â€–S(AXâˆ’B)â€–ğ‘ =(1Â±ğœ€)â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
simultaneously for every XâˆˆRğ‘‘Ã—ğ‘š.
Our main technical lemma is the following result which generalizes the sampling results of [MMWY22,
WY23a] on preserving differences. The proof can be found in Appendix A.
Theorem 3.2. LetSbetheâ„“ samplingmatrix(Definition1.1)withsamplingprobabilities ğ‘ â‰¥min{1,w /ğ›¼}
ğ‘ ğ‘– ğ‘–
for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘› and
ğ‘
â§ ğ‘‚(ğ›¾)ğœ€2[ï¸‚ 1]ï¸‚âˆ’1
âªâªâªâ¨
ğœ‚2/ğ‘
(logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘<2
ğ›¼= .
âªâªâªâ© ğœ‚ğ‘‚ â€–( wğ›¾ â€–ğ‘/ ğ‘2 /) 2ğœ€ âˆ’ğ‘ 1[ï¸‚ (logğ‘‘)2logğ‘›+log1 ğ›¿]ï¸‚âˆ’1
ğ‘>2
1
For each x* âˆˆRğ‘‘ and b* =Ax*âˆ’b, with probability at least 1âˆ’ğ›¿,
(ï¸‚ )ï¸‚
âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–Sb*â€–ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘âˆ’â€–b*â€–ğ‘)ï¸€âƒ’ âƒ’â‰¤ğœ€ â€–b*â€–ğ‘+â€–Sb*â€–ğ‘+ 1 â€–Axâˆ’Ax*â€–ğ‘
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğœ‚ ğ‘
simultaneously for every xâˆˆRğ‘‘.
Given Theorem 3.2, the proof of Theorem 3.1 proceeds as described in the introduction.
Proof of Theorem 3.1. By replacing B by BË† âˆ’AXË†, we assume that â€–Bâ€– =ğ‘‚(OPT). We apply Theorem 3.2
ğ‘
with failure probability at ğœ€ğ‘ğ›¿2. Now let ğ‘† âŠ†[ğ‘š] be the set of columns for which the guarantee of Theorem
3.2 fails. Note then that by Markovâ€™s inequality,
âˆ‘ï¸
â€–Be â€–ğ‘ =ğ‘‚(ğœ€ğ‘ğ›¿)â€–Bâ€–ğ‘
ğ‘— ğ‘ ğ‘,ğ‘
ğ‘—âˆˆğ‘†
with probability at least 1âˆ’ğ›¿. We also have that
âˆ‘ï¸ 1âˆ‘ï¸
â€–SBe â€–ğ‘ â‰¤ â€–Be â€–ğ‘ =ğ‘‚(ğœ€ğ‘)â€–Bâ€–ğ‘
ğ‘— ğ‘ ğ›¿ ğ‘— ğ‘ ğ‘,ğ‘
ğ‘—âˆˆğ‘† ğ‘—âˆˆğ‘†
with probability at least 1âˆ’ğ›¿, again by Markovâ€™s inequality. Then,
ğ‘‚(1)
â€–S(AXâˆ’B)e â€–ğ‘ =(1Â±ğœ€)â€–SAXe â€–ğ‘Â± â€–SBe â€–ğ‘
ğ‘— ğ‘ ğ‘— ğ‘ ğœ€ğ‘âˆ’1 ğ‘— ğ‘
ğ‘‚(1)
=(1Â±ğœ€)2â€–AXe â€–ğ‘Â± â€–SBe â€–ğ‘
ğ‘— ğ‘ ğœ€ğ‘âˆ’1 ğ‘— ğ‘
by using that S is a subspace embedding. Similarly, we have that
ğ‘‚(1)
â€–(AXâˆ’B)e â€–ğ‘ =(1Â±ğœ€)â€–AXe â€–ğ‘Â± â€–Be â€–ğ‘.
ğ‘— ğ‘ ğ‘— ğ‘ ğœ€ğ‘âˆ’1 ğ‘— ğ‘
Then summing over ğ‘— âˆˆğ‘† gives that
âˆ‘ï¸ âˆ‘ï¸
â€–S(AXâˆ’B)e â€–ğ‘ = â€–(AXâˆ’B)e â€–ğ‘Â±ğ‘‚(ğœ€)â€–Bâ€–ğ‘ .
ğ‘— ğ‘ ğ‘— ğ‘ ğ‘,ğ‘
ğ‘—âˆˆğ‘† ğ‘—âˆˆğ‘†
On the other hand, for ğ‘— âˆˆ/ ğ‘†, Theorem 3.2 succeeds so we have
â€–S(AXâˆ’B)e â€–ğ‘ =â€–(AXâˆ’B)e â€–ğ‘âˆ’â€–Be â€–ğ‘+â€–SBe â€–ğ‘Â±ğœ€(ï¸€ â€–Be â€–ğ‘+â€–SBe â€–ğ‘+â€–AXe â€–ğ‘)ï¸€
ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— ğ‘ ğ‘— ğ‘
Summing the guarantee over the ğ‘š columns ğ‘— gives
â€–S(AXâˆ’B)â€–ğ‘ =â€–AXâˆ’Bâ€–ğ‘ âˆ’â€–Bâ€–ğ‘ +â€–SBâ€–ğ‘ Â±ğ‘‚(ğœ€)(ï¸€ â€–Bâ€–ğ‘ +â€–AXâ€–ğ‘ )ï¸€
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
=â€–AXâˆ’Bâ€–ğ‘ Â±ğœ€â€–Bâ€–ğ‘ Â±ğ‘‚(ğœ€)(ï¸€ â€–Bâ€–ğ‘ +â€–AXâ€–ğ‘ )ï¸€
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
=â€–AXâˆ’Bâ€–ğ‘ Â±ğ‘‚(ğœ€)â€–AXâˆ’Bâ€–ğ‘ .
ğ‘,ğ‘ ğ‘,ğ‘
94 Weak coresets
We sketch the proof of the following result in this section. Full proofs can be found in Appendix C.
Theorem 4.1 (Weak coresets for multiple â„“ regression). Let S be the â„“ sampling matrix (Definition 1.1)
ğ‘ ğ‘
with sampling probabilities ğ‘ â‰¥min{1,w /ğ›¼} for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘› and
ğ‘– ğ‘– ğ‘
[ï¸‚ 1]ï¸‚âˆ’1[ï¸‚ 1]ï¸‚âˆ’2
ğ›¼=ğ‘‚(ğ›¾)ğœ€ğ›¿2 (logğ‘‘)2logğ‘›+log loglog
ğ›¿ ğœ€
for ğ‘<2 and
ğ‘‚(ğ›¾ğ‘/2)ğœ€ğ‘âˆ’1ğ›¿ğ‘[ï¸‚ 1]ï¸‚âˆ’1[ï¸‚ 1]ï¸‚âˆ’ğ‘
ğ›¼= (logğ‘‘)2logğ‘›+log loglog
â€–wâ€–ğ‘/2âˆ’1 ğ›¿ ğœ€
1
for ğ‘>2. Then, for any XË† âˆˆRğ‘‘Ã—ğ‘¡ such that
â€–S(AXË†Gâˆ’B)â€–ğ‘ â‰¤(1+ğœ€) min â€–S(AXGâˆ’B)â€–ğ‘ ,
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
we have
â€–AXË†Gâˆ’Bâ€–ğ‘ â‰¤(1+ğ‘‚(ğœ€)) min â€–AXGâˆ’Bâ€–ğ‘ .
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
We first establish lemmas that relate approximation quality to the closeness of solutions to the optimum
in Section 4.1, and we use this in an iterative argument in Section 4.2.
4.1 Closeness of nearly optimal solutions
The following lemma uses strong convexity for ğ‘<2 and a Bregman divergence bound for ğ‘>2 to quantify
the difference between the â„“ norms of two vectors.
ğ‘
Lemma 4.2. For any y,yâ€² âˆˆRğ‘›, we have
ğ‘âˆ’1
â€–yâ€²â€–2 â‰¥â€–yâ€–2âˆ’2â€–yâ€–2âˆ’ğ‘âŸ¨yâˆ˜(ğ‘âˆ’1),yâˆ’yâ€²âŸ©+ â€–yâˆ’yâ€²â€–2
ğ‘ ğ‘ ğ‘ 2 ğ‘
if 1<ğ‘<2 (Lemma 8.1 of [BMN01]) and
ğ‘âˆ’1
â€–yâ€²â€–ğ‘ â‰¥â€–yâ€–ğ‘âˆ’ğ‘âŸ¨yâˆ˜(ğ‘âˆ’1),yâˆ’yâ€²âŸ©+ â€–yâˆ’yâ€²â€–ğ‘
ğ‘ ğ‘ ğ‘2ğ‘ ğ‘
if 2â‰¤ğ‘<âˆ (Lemmas 3.2 and 4.6 of [AKPS19]).
We need the following elementary computation.
Lemma 4.3 (Gradients of multiple â„“ regression). The gradient âˆ‡ â€–AXGâˆ’Bâ€–ğ‘ is given by the formula
ğ‘ X ğ‘,ğ‘
ğ‘› ğ‘š
âˆ‘ï¸âˆ‘ï¸
ğ‘[AXGâˆ’B](ğ‘–,ğ‘—)âˆ˜(ğ‘âˆ’1)(AâŠ¤e )(eâŠ¤GâŠ¤)
ğ‘– ğ‘—
ğ‘–=1ğ‘—=1
The following lemma uses Lemmas 4.2 and 4.3 to show that if X achieves a nearly optimal value, then X
must be close to the optimal solution X*.
Lemma4.4(Closenessofnearlyoptimalsolutions). Letğ‘>1. ForanyXâˆˆRğ‘‘Ã—ğ‘¡ suchthatâ€–AXGâˆ’Bâ€– â‰¤
ğ‘,ğ‘
(1+ğœ‚)OPT with ğœ‚ âˆˆ(0,1), we have that
{ï¸ƒ
ğ‘‚(ğœ‚1/2)OPT ğ‘<2
â€–AXGâˆ’AX*Gâ€– â‰¤
ğ‘,ğ‘ ğ‘‚(ğœ‚1/ğ‘)OPT ğ‘>2
where X* :=argmin â€–AXGâˆ’Bâ€– .
XâˆˆRğ‘‘Ã—ğ‘¡ ğ‘,ğ‘
104.2 Iterative size reduction argument
We now sketch the proof of Theorem 4.1.
We will need the following initial result to seed our iterative argument. Note that the dependence on ğœ€ is
suboptimal by an ğœ€ factor for every 1<ğ‘<âˆ.
Lemma 4.5. Let S be the â„“ sampling matrix (Definition 1.1) with sampling probabilities ğ‘ â‰¥min{1,w /ğ›¼}
ğ‘ ğ‘– ğ‘–
for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘› and
ğ‘
[ï¸‚ 1]ï¸‚âˆ’1
ğ›¼=ğ‘‚(ğ›¾)(ğœ€ğ›¿)2 (logğ‘‘)2logğ‘›+log
ğ›¿
for 1â‰¤ğ‘<2 and
ğ‘‚(ğ›¾ğ‘/2)(ğœ€ğ›¿)ğ‘[ï¸‚ 1]ï¸‚âˆ’1
ğ›¼= (logğ‘‘)2logğ‘›+log
â€–wâ€–ğ‘/2âˆ’1 ğ›¿
1
for 2<ğ‘<âˆ. Then, for any XË† âˆˆRğ‘‘Ã—ğ‘¡ such that
â€–S(AXË†Gâˆ’B)â€–ğ‘ â‰¤(1+ğœ€) min â€–S(AXGâˆ’B)â€–ğ‘ ,
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
we have
â€–AXË†Gâˆ’Bâ€–ğ‘ â‰¤(1+ğ‘‚(ğœ€)) min â€–AXGâˆ’Bâ€–ğ‘ .
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘‘Ã—ğ‘¡
Starting from this initial solution bound of Lemma 4.5, we can proceed via an iterative argument similar
to those of [MMWY22, WY23a] which alternates between using a bound on the closeness of the solution to
the optimal solution to improve the approximation (Theorem 3.2), and using a bound on the approximation
to improve the closeness to the optimum (Lemma 4.4). More specifically, we can show that for 1<ğ‘<2, a
bound of ğ¶/ğœ€ğ›½ on the sample complexity implies that a bound of ğ¶/ğœ€2ğ›½/(1+ğ›½) is sufficient as well. Iterating
this argument starting from ğ›½ =2 due to Lemma 4.5 for ğ‘‚(loglog1) iterations yields the desired bound of
ğœ€
ğ¶/ğœ€, as claimed. Similarly, for ğ‘>2, a bound of ğ¶/ğœ€ğ›½ implies a bound of ğ¶/ğœ€ğ‘ğ›½/(1+ğ›½), which results in a
final bound of ğ¶/ğœ€ğ‘âˆ’1, as claimed. The full details can be found in Appendix C.
5 Lower bounds
In this section, we complement our various upper bounds with matching lower bounds. In the interest of
space, the proofs are given in Appendix E.
Theorem 5.1. Let 2<ğ‘<âˆ be fixed. Let ğœ€âˆˆ(0,1) be less than some sufficiently small constant. Then, a
strong coreset S for multiple â„“ regression requires nnz(S)=â„¦(ğœ€âˆ’ğ‘ğ‘‘ğ‘/2) non-zero rows.
ğ‘
Theorem 5.2. Let 2<ğ‘<âˆ be fixed. Let ğœ€âˆˆ(0,1) be less than some sufficiently small constant. Then, a
weak coreset S for multiple â„“ regression requires nnz(S)=â„¦(ğœ€1âˆ’ğ‘ğ‘‘ğ‘/2) non-zero rows.
ğ‘
Theorem 5.3. Let 1â‰¤ğ‘<âˆ and
{ï¸ƒ
1/6 ğ‘â‰¤2
ğ‘ =
ğ‘ 1/(6Â·5ğ‘/2âˆ’1) ğ‘>2
Letğ‘˜ âˆˆN. Then,thereisamatrixBâˆˆRğ‘›Ã—(ğ‘›+1) suchthatforeveryğœ€â‰¥ğ‘˜/ğ‘›andanysubsetofğ‘ â‰¤(ğ‘ /4)ğœ€âˆ’1ğ‘˜
ğ‘
rows, any rank ğ‘˜ subspace ğ¹â€² spanned by the ğ‘  rows must have
â€–BP âˆ’Bâ€–ğ‘ >(1+ğœ€) min â€–BP âˆ’Bâ€–ğ‘ .
ğ¹â€² ğ‘,2 ğ¹ ğ‘,2
rank(ğ¹)â‰¤ğ‘˜
Acknowledgements
We thank the anonymous reviewers for useful feedback on improving the presentation of this work. David P.
Woodruff and Taisuke Yasuda were supported by a Simons Investigator Award.
11References
[AKPS19] Deeksha Adil, Rasmus Kyng, Richard Peng, and Sushant Sachdeva. Iterative refinement for
â„“ -norm regression. In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-
ğ‘
SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January
6-9, 2019, pages 1405â€“1424. SIAM, 2019. 4.2
[BLM89] J. Bourgain, J. Lindenstrauss, and V. Milman. Approximation of zonoids by zonotopes. Acta
Math., 162(1-2):73â€“141, 1989. 1, 1.4, B.3.2, B.15
[BMN01] Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The ordered subsets mirror descent
optimization method with applications to tomography. SIAM J. Optim., 12(1):79â€“108, 2001. 4.2
[CD21] Xue Chen and Michal Derezinski. Query complexity of least absolute deviation regression via
robust uniform convergence. In Mikhail Belkin and Samory Kpotufe, editors, Conference on
Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado, USA, volume 134 of
Proceedings of Machine Learning Research, pages 1144â€“1179. PMLR, 2021. 1, 1.4, A
[Cla05] Kenneth L. Clarkson. Subgradient and sampling algorithms for â„“ regression. In Proceedings of
1
the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA â€™05, pages 257â€“266,
USA, 2005. Society for Industrial and Applied Mathematics. 1
[CNW16] Michael B. Cohen, Jelani Nelson, and David P. Woodruff. Optimal approximate matrix product
in terms of stable rank. In Ioannis Chatzigiannakis, Michael Mitzenmacher, Yuval Rabani,
and Davide Sangiorgi, editors, 43rd International Colloquium on Automata, Languages, and
Programming,ICALP2016,July11-15,2016,Rome,Italy,volume55ofLIPIcs,pages11:1â€“11:14.
Schloss Dagstuhl - Leibniz-Zentrum fuÂ¨r Informatik, 2016. 1.1.1
[CP15] Michael B. Cohen and Richard Peng. Lp row sampling by lewis weights. In Rocco A. Servedio
and Ronitt Rubinfeld, editors, Proceedings of the Forty-Seventh Annual ACM on Symposium
on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pages 183â€“192.
ACM, 2015. 1, 2.1, A, A
[CP19] XueChenandEricPrice. Activeregressionvialinear-samplesparsification. InAlinaBeygelzimer
andDanielHsu,editors,ConferenceonLearningTheory, COLT2019, 25-28June2019, Phoenix,
AZ, USA, volume 99 of Proceedings of Machine Learning Research, pages 663â€“695. PMLR, 2019.
1
[CSS21] VincentCohen-Addad,DavidSaulpic,andChrisSchwiegelshohn.Improvedcoresetsandsublinear
algorithms for power means in euclidean spaces. In Marcâ€™Aurelio Ranzato, Alina Beygelzimer,
Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural
Information Processing Systems 34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 21085â€“21098, Virtual, 2021.
1.4, 1.4
[CW13] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input
sparsity time. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum, editors, Symposium on
Theory of Computing Conference, STOCâ€™13, Palo Alto, CA, USA, June 1-4, 2013, pages 81â€“90.
ACM, 2013. 1.1.1
[CW15] Kenneth L. Clarkson and David P. Woodruff. Input sparsity and hardness for robust subspace
approximation. InVenkatesanGuruswami,editor,IEEE56thAnnualSymposiumonFoundations
of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 310â€“329.
IEEE Computer Society, 2015. 1.5
[DDH+09] Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Mahoney.
Sampling algorithms and coresets for â„“ regression. SIAM J. Comput., 38(5):2060â€“2078, 2009. 1,
ğ‘
1.2.1
12[DMM06a] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for â„“
2
regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006, pages 1127â€“
1136. ACM Press, 2006. 1
[DMM06b] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Subspace sampling and relative-
error matrix approximation: Column-row-based methods. In Yossi Azar and Thomas Erlebach,
editors,Algorithms-ESA2006, 14thAnnualEuropeanSymposium, Zurich, Switzerland, Septem-
ber 11-13, 2006, Proceedings, volume 4168 of Lecture Notes in Computer Science, pages 304â€“314.
Springer, 2006. 1
[DV06] Amit Deshpande and Santosh S. Vempala. Adaptive sampling and fast low-rank matrix
approximation. In Josep DÂ´Ä±az, Klaus Jansen, JosÂ´e D. P. Rolim, and Uri Zwick, editors,
Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques,
9th International Workshop on Approximation Algorithms for Combinatorial Optimization
Problems, APPROX 2006 and 10th International Workshop on Randomization and Computation,
RANDOM 2006, Barcelona, Spain, August 28-30 2006, Proceedings, volume 4110 of Lecture
Notes in Computer Science, pages 292â€“303. Springer, 2006. 1.5, E.3
[DV07] AmitDeshpandeandKasturiR.Varadarajan. Sampling-baseddimensionreductionforsubspace
approximation. In David S. Johnson and Uriel Feige, editors, Proceedings of the 39th Annual
ACM Symposium on Theory of Computing, San Diego, California, USA, June 11-13, 2007,
pages 641â€“650. ACM, 2007. 1.5
[Dvo61] Aryeh Dvoretzky. Some results on convex bodies and Banach spaces. In Proc. Internat.
Sympos. Linear Spaces (Jerusalem, 1960), pages 123â€“160. Jerusalem Academic Press, Jerusalem;
Pergamon, Oxford, 1961. 1.4
[FKW21] Zhili Feng, Praneeth Kacham, and David P. Woodruff. Dimensionality reduction for the sum-of-
distancesmetric. InMarinaMeilaandTongZhang,editors,Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research, pages 3220â€“3229. PMLR, 2021. 1.5
[FL11] Dan Feldman and Michael Langberg. A unified framework for approximating and clustering
data. In Lance Fortnow and Salil P. Vadhan, editors, Proceedings of the 43rd ACM Symposium
on Theory of Computing, STOC 2011, San Jose, CA, USA, 6-8 June 2011, pages 569â€“578.
ACM, 2011. 1.5
[FLM77] T. Figiel, J. Lindenstrauss, and V. D. Milman. The dimension of almost spherical sections of
convex bodies. Acta Math., 139(1-2):53â€“94, 1977. 1.7
[HV20] LingxiaoHuangandNisheethK.Vishnoi. Coresetsforclusteringineuclideanspaces: importance
sampling is nearly optimal. In Konstantin Makarychev, Yury Makarychev, Madhur Tulsiani,
Gautam Kamath, and Julia Chuzhoy, editors, Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020, pages
1416â€“1429. ACM, 2020. 1.5, D.2
[JLS22] Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Improved iteration complexities for
overconstrained p-norm regression. In Stefano Leonardi and Anupam Gupta, editors, STOC
â€™22: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 -
24, 2022, pages 529â€“542. ACM, 2022. 2.1
[Lew78] D. R. Lewis. Finite dimensional subspaces of ğ¿ . Studia Mathematica, 63(2):207â€“212, 1978. 1
ğ‘
[LLW23] Yi Li, Honghao Lin, and David P. Woodruff. â„“ -regression in the arbitrary partition model
ğ‘
of communication. In Gergely Neu and Lorenzo Rosasco, editors, The Thirty Sixth Annual
Conference on Learning Theory, COLT 2023, 12-15 July 2023, Bangalore, India, volume 195 of
Proceedings of Machine Learning Research, pages 4902â€“4928. PMLR, 2023. 1.4
13[LT91] Michel Ledoux and Michel Talagrand. Probability in Banach spaces. Classics in Mathematics.
Springer-Verlag, Berlin, 1991. Isoperimetry and processes, Reprint of the 1991 edition. 1, B.8
[LWW21] Yi Li, Ruosong Wang, and David P. Woodruff. Tight bounds for the subspace sketch problem
with applications. SIAM J. Comput., 50(4):1287â€“1335, 2021. 1.2, 1.6
[LWY21] YiLi,DavidP.Woodruff,andTaisukeYasuda. Exponentiallyimproveddimensionalityreduction
for â„“ : Subspace embeddings and independence testing. In Mikhail Belkin and Samory Kpotufe,
1
editors, Conference on Learning Theory, COLT 2021, 15-19 August 2021, Boulder, Colorado,
USA, volume 134 of Proceedings of Machine Learning Research, pages 3111â€“3195. PMLR, 2021.
1.4
[Mil71] V. D. Milman. A new proof of A. Dvoretzkyâ€™s theorem on cross-sections of convex bodies.
Funkcional. Anal. i PriloË‡zen., 5(4):28â€“37, 1971. 1.4
[MMR19] Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-
lindenstrauss transform for k-means and k-medians clustering. In Moses Charikar and Edith
Cohen, editors, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 1027â€“1038. ACM, 2019.
B.2
[MMWY22] Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Active linear
regression for â„“ norms and beyond. In 63rd IEEE Annual Symposium on Foundations of
ğ‘
Computer Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022, pages
744â€“753. IEEE, 2022. 1, 1, 1.2.1, 1.3, 1.4, 3, 4.2, D.1
[PPP21] AdityaParulekar,AdvaitParulekar,andEricPrice.L1regressionwithLewisweightssubsampling.
InMaryWoottersandLauraSanita`,editors,Approximation, Randomization, and Combinatorial
Optimization. Algorithms and Techniques, APPROX/RANDOM 2021, August 16-18, 2021,
University of Washington, Seattle, Washington, USA (Virtual Conference), volume 207 of
LIPIcs, pages 49:1â€“49:21. Schloss Dagstuhl - Leibniz-Zentrum fuÂ¨r Informatik, 2021. 1, 1.4
[PTB13] Udaya Parampalli, Xiaohu Tang, and Serdar Boztas. On the construction of binary sequence
families with low correlation and large sizes. IEEE Trans. Inf. Theory, 59(2):1082â€“1089, 2013.
E.1
[PVZ17] Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of Dvoretzkyâ€™s theorem in â„“ğ‘›.
ğ‘
Stochastic Process. Appl., 127(10):3187â€“3227, 2017. 1.7
[SV12] Nariankadu D. Shyamalkumar and Kasturi R. Varadarajan. Efficient subspace approximation
algorithms. Discret. Comput. Geom., 47(1):44â€“63, 2012. 1.5, 1.5, 1.6
[SW18] Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspace approx-
imation: Goodbye dimension. In Mikkel Thorup, editor, 59th IEEE Annual Symposium on
Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9, 2018, pages 802â€“813.
IEEE Computer Society, 2018. 1.5
[SZ01] Gideon Schechtman and Artem Zvavitch. Embedding subspaces of ğ‘™ into ğ‘™ğ‘›, 0 < ğ‘ < 1.
ğ‘ ğ‘
Mathematische Nachrichten, 227(1):133â€“142, 2001. 1
[Tal90] Michel Talagrand. Embedding subspaces of ğ¿ into ğ‘™ğ‘. Proc. Amer. Math. Soc., 108(2):363â€“369,
1 1
1990. 1
[Tal95] MichelTalagrand. Embeddingsubspacesofğ¿ inğ‘™ğ‘. InGeometric aspects of functional analysis
ğ‘ ğ‘
(Israel, 1992â€“1994), volume 77 of Oper. Theory Adv. Appl., pages 311â€“325. BirkhÂ¨auser, Basel,
1995. 1
[Ver18] Roman Vershynin. High-dimensional probability, volume 47 of Cambridge Series in Statistical
and Probabilistic Mathematics. Cambridge University Press, Cambridge, 2018. B.9
14[WW19] Ruosong Wang and David P. Woodruff. Tight bounds for â„“ oblivious subspace embeddings.
ğ‘
In Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019, pages
1825â€“1843. SIAM, 2019. 1.4
[WY22] David P. Woodruff and Taisuke Yasuda. High-dimensional geometric streaming in polynomial
space. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022,
Denver, CO, USA, October 31 - November 3, 2022, pages 732â€“743. IEEE, 2022. 2.1
[WY23a] David P. Woodruff and Taisuke Yasuda. New subset selection algorithms for low rank approxi-
mation: Offline and online. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the
55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June
20-23, 2023, pages 1802â€“1813. ACM, 2023. 1, 1.5, 3, 4.2
[WY23b] David P. Woodruff and Taisuke Yasuda. Online Lewis weight sampling. In Nikhil Bansal and
Viswanath Nagarajan, editors, Proceedings of the 2023 ACM-SIAM Symposium on Discrete
Algorithms, SODA 2023, Florence, Italy, January 22-25, 2023, pages 4622â€“4666. SIAM, 2023. 1,
A
[WY23c] David P. Woodruff and Taisuke Yasuda. Sharper bounds for â„“ sensitivity sampling. In
ğ‘
Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,
pages 37238â€“37272. PMLR, 2023. B.1.2, B.3.2
A â„“ Lewis weight sampling for differences
ğ‘
Throughout this section, we fix the following notation:
Definition A.1.
â€¢ Let 1â‰¤ğ‘<âˆ.
â€¢ Let ğœ€âˆˆ(0,1) be an accuracy parameter and let ğ›¿ âˆˆ(0,1) be a failure probability parameter.
â€¢ Let AâˆˆRğ‘›Ã—ğ‘‘ and bâˆˆRğ‘›.
â€¢ Let wâˆˆRğ‘› be ğ›¾-one-sided â„“ Lewis weights for A such that maxğ‘› w â‰¤ğ‘¤.
ğ‘ ğ‘–=1 ğ‘–
â€¢ Let x* âˆˆ Rğ‘‘ any center, let ğœ‚ âˆˆ (0,1) be a proximity parameter, and let ğ‘… â‰¥ â€–Ax*âˆ’bâ€–ğ‘ be a scale
ğ‘
parameter.
â€¢ For each ğ‘–âˆˆ[ğ‘›] and xâˆˆRğ‘‘, let
âˆ† (x):=|[Axâˆ’b](ğ‘–)|ğ‘âˆ’|[Ax*âˆ’b](ğ‘–)|ğ‘
ğ‘–
Our main result of the section is the following:
TheoremA.2. LetSbetheâ„“ samplingmatrix(Definition1.1)withsamplingprobabilitiesğ‘ â‰¥min{1,w /ğ›¼}
ğ‘ ğ‘– ğ‘–
for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘› and
ğ‘
â§ ğœ€2 [ï¸‚ 1]ï¸‚âˆ’1
âªâªâªâ¨ğ‘‚(ğ›¾)
ğœ‚2/ğ‘
(logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘<2
ğ›¼= .
âªâªâªâ©ğ‘‚(ğ›¾ğ‘/2) ğœ‚â€–wğœ€ â€–ğ‘ ğ‘/2âˆ’1[ï¸‚ (logğ‘‘)2logğ‘›+log1 ğ›¿]ï¸‚âˆ’1
ğ‘>2
1
Then for each x* âˆˆRğ‘‘ and ğ‘…â‰¥â€–Ax*âˆ’bâ€–ğ‘, with probability at least 1âˆ’ğ›¿,
ğ‘
sup âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–S(Ax*âˆ’b)â€–ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘âˆ’â€–Ax*âˆ’bâ€–ğ‘)ï¸€âƒ’ âƒ’â‰¤ğœ€(ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘)
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘
â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…
15We will prove Theorem A.2 throughout this section. Before doing so, we state the following more
convenient form of the result:
Theorem 3.2. LetSbetheâ„“ samplingmatrix(Definition1.1)withsamplingprobabilities ğ‘ â‰¥min{1,w /ğ›¼}
ğ‘ ğ‘– ğ‘–
for ğ›¾-one-sided â„“ Lewis weights wâˆˆRğ‘› and
ğ‘
â§ ğ‘‚(ğ›¾)ğœ€2[ï¸‚ 1]ï¸‚âˆ’1
âªâªâªâ¨
ğœ‚2/ğ‘
(logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘<2
ğ›¼= .
âªâªâªâ© ğœ‚ğ‘‚ â€–( wğ›¾ â€–ğ‘/ ğ‘2 /) 2ğœ€ âˆ’ğ‘ 1[ï¸‚ (logğ‘‘)2logğ‘›+log1 ğ›¿]ï¸‚âˆ’1
ğ‘>2
1
For each x* âˆˆRğ‘‘ and b* =Ax*âˆ’b, with probability at least 1âˆ’ğ›¿,
(ï¸‚ )ï¸‚
âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–Sb*â€–ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘âˆ’â€–b*â€–ğ‘)ï¸€âƒ’ âƒ’â‰¤ğœ€ â€–b*â€–ğ‘+â€–Sb*â€–ğ‘+ 1 â€–Axâˆ’Ax*â€–ğ‘
ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğœ‚ ğ‘
simultaneously for every xâˆˆRğ‘‘.
Proof. We apply Theorem A.2 with ğ›¿ set to ğ›¿/ğ¿ for ğ¿=ğ‘‚(log(1/ğ›¿ğœ€)) and ğ‘… set to 2ğ‘™â€–Ax*âˆ’bâ€–ğ‘ for ğ‘™âˆˆ[ğ¿].
ğ‘
By a union bound, the conclusion holds simultaneously for every ğ‘™ âˆˆ [ğ¿] with probability at least 1âˆ’ğ›¿.
Furthermore, by Markovâ€™s inequality, â€–S(Ax*âˆ’b)â€–ğ‘ =ğ‘‚(1/ğ›¿)â€–Ax*âˆ’bâ€–ğ‘ with probability at least 1âˆ’ğ›¿.
ğ‘ ğ‘
If â€–Axâˆ’Ax*â€–ğ‘ â‰¤ 2ğ¿â€–Ax*âˆ’bâ€–ğ‘ = poly(1/ğ›¿ğœ€)â€–Ax*âˆ’bâ€–ğ‘, then the result follows immediately from
ğ‘ ğ‘ ğ‘
applying the conclusion of Theorem A.2 at the appropriate scale ğ‘™ âˆˆ[ğ¿]. Otherwise, we have that â€–Axâˆ’
Ax*â€–ğ‘ â‰¥poly(1/ğ›¿ğœ€)â€–Ax*âˆ’bâ€–ğ‘, in which case
ğ‘ ğ‘
â€–S(Axâˆ’Ax*)â€–ğ‘ â‰¥â„¦(1)â€–Axâˆ’Ax*â€–ğ‘ â‰¥poly(1/ğ›¿ğœ€)â€–Ax*âˆ’bâ€–ğ‘
ğ‘ ğ‘ ğ‘
so
(1+ğœ€)ğ‘âˆ’1
â€–S(Axâˆ’b)â€–ğ‘âˆ’â€–S(Ax*âˆ’b)â€–ğ‘ =(1Â±ğœ€)â€–S(Axâˆ’Ax*)â€–ğ‘Â± â€–S(Ax*âˆ’b)â€–ğ‘
ğ‘ ğ‘ ğ‘ ğœ€ğ‘âˆ’1 ğ‘
(1+ğœ€)ğ‘âˆ’1
=(1Â±ğœ€)â€–S(Axâˆ’Ax*)â€–ğ‘Â± â€–Ax*âˆ’bâ€–ğ‘
ğ‘ ğ›¿ğœ€ğ‘âˆ’1 ğ‘
=(1Â±ğ‘‚(ğœ€))â€–S(Axâˆ’Ax*)â€–ğ‘
ğ‘
and similarly,
â€–Axâˆ’bâ€–ğ‘âˆ’â€–Ax*âˆ’bâ€–ğ‘ =(1Â±ğ‘‚(ğœ€))â€–Axâˆ’Ax*â€–ğ‘.
ğ‘ ğ‘ ğ‘
Thus it suffices to have that
âƒ’ âƒ’ ğœ€
âƒ’â€–S(Axâˆ’Ax*)â€–ğ‘âˆ’â€–Axâˆ’Ax*â€–ğ‘âƒ’â‰¤ â€–Axâˆ’Ax*â€–ğ‘.
âƒ’ ğ‘ ğ‘âƒ’ ğœ‚ ğ‘
In fact, standard â„“ Lewis weight sampling guarantees give
ğ‘
â§ ğœ€
âƒ’ âƒ’
âªâªâ¨ğœ‚1/ğ‘â€–Axâˆ’Ax*â€–ğ‘
ğ‘
ğ‘<2
âƒ’â€–S(Axâˆ’Ax*)â€–ğ‘âˆ’â€–Axâˆ’Ax*â€–ğ‘âƒ’â‰¤
âƒ’ ğ‘ ğ‘âƒ’ ğœ€ğ‘/2
âªâªâ© ğœ‚1/2â€–Axâˆ’Ax*â€–ğ‘
ğ‘
ğ‘>2
which is stronger.
Throughout our proof of Theorem A.2, we will assume without loss of generality that Sğ‘ >1, that is we
ğ‘–,ğ‘–
only consider rows that are sampled with probability ğ‘ <1, since rows that are kept with probability ğ‘ =1
ğ‘– ğ‘–
do not contribute towards the sampling error. Note first that we can write
âƒ’ âƒ’
âƒ’ âƒ’(ï¸€ â€–S(Axâˆ’b)â€–ğ‘ ğ‘âˆ’â€–S(Ax*âˆ’b)â€–ğ‘ ğ‘)ï¸€ âˆ’(ï¸€ â€–Axâˆ’bâ€–ğ‘ ğ‘âˆ’â€–Ax*âˆ’bâ€–ğ‘ ğ‘)ï¸€âƒ’ âƒ’=âƒ’ âƒ’ âƒ’âˆ‘ï¸ğ‘› (Sğ‘ ğ‘–,ğ‘–âˆ’1)âˆ† ğ‘–(x)âƒ’ âƒ’ âƒ’.
âƒ’ âƒ’
ğ‘–=1
Thesupremumofthisquantity,normalizedby(ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘)ğ‘™,over{ï¸€ â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…}ï¸€ isarandom
ğ‘ ğ‘
variable. We will bound the ğ‘™-th moment of this random variable for ğ‘™=ğ‘‚(log1 +logğ‘›).
ğ›¿
We start with a standard symmetrization procedure (see, e.g., [CP15, CD21]).
16Lemma A.3 (Symmetrization).
â¡
1
âƒ’
âƒ’âˆ‘ï¸ğ‘›
âƒ’ âƒ’ğ‘™â¤
E Sâ£ (ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘ ğ‘)ğ‘™ â€–Axâˆ’Asu xp
*â€–ğ‘
ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
âƒ’
ğ‘–=1(Sğ‘ ğ‘–,ğ‘–âˆ’1)âˆ† ğ‘–(x)âƒ’ âƒ’
âƒ’
â¦
â¡
1
âƒ’
âƒ’âˆ‘ï¸ğ‘›
âƒ’ âƒ’ğ‘™â¤
â‰¤ 2ğ‘™ ğœ€âˆ¼{Â±E 1}ğ‘›,Sâ£ (ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘ ğ‘)ğ‘™ â€–Axâˆ’Asu xp
*â€–ğ‘
ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
âƒ’
ğ‘–=1ğœ€ ğ‘–Sğ‘ ğ‘–,ğ‘–âˆ† ğ‘–(x)âƒ’ âƒ’
âƒ’
â¦
Next, we replace the Rademacher process on the right hand side of Lemma A.3 by one which â€œremovesâ€
Sğ‘ , that is, one of the form
ğ‘–,ğ‘–
â¡ âƒ’
âƒ’âˆ‘ï¸ğ‘›
âƒ’ âƒ’ğ‘™â¤
E â£ sup âƒ’ âƒ’ ğœ€ ğ‘–âˆ† ğ‘–(x)âƒ’ âƒ’ â¦. (3)
ğœ€âˆ¼{Â±1}ğ‘› â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
ğ‘–=1
âƒ’
ThisisroughlydonebynotingthatifwetakeSAtobeaâ€œpartofâ€A,thenthedomain{ï¸€ â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…}ï¸€
ğ‘
only dilates by a constant factor as S preserves â„“ norms in the column space of A. More formally, we have
ğ‘
the following lemma:
Lemma A.4. Let BâˆˆRğ‘šÃ—ğ‘‘ satisfy â€–Bxâ€–ğ‘ â‰¤ğ¶â€–Axâ€–ğ‘ for every xâˆˆRğ‘‘. For every fixing of S, let
ğ‘ ğ‘
(ï¸ƒ )ï¸ƒ
SA
B :=
S
B
be the concatenation of SA and B, and let
ğ¹
S
= sup âƒ’ âƒ’â€–SAxâ€–ğ‘ ğ‘âˆ’â€–Axâ€–ğ‘ ğ‘âƒ’ âƒ’.
â€–Axâ€–ğ‘ ğ‘â‰¤1
Suppose that for every fixing of S and ğ‘…â€² â‰¥ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘, we have that
ğ‘
âƒ’ âƒ’
âƒ’âˆ‘ï¸ğ‘› âƒ’
E sup âƒ’ ğœ€ Sğ‘ âˆ† (x)âƒ’â‰¤ğœ€ğ‘™ğ›¿ğ‘…â€²ğ‘™
âƒ’ ğ‘– ğ‘–,ğ‘– ğ‘– âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–BSxâˆ’BSx*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…â€²âƒ’
ğ‘–=1
âƒ’
Then,
1
âƒ’ âƒ’âˆ‘ï¸ğ‘› âƒ’ âƒ’ğ‘™
(ï¸ )ï¸
E E sup âƒ’ ğœ€ Sğ‘ âˆ† (x)âƒ’ â‰¤(2ğœ€)ğ‘™ğ›¿ (1+ğ¶)ğ‘™+E[ğ¹ğ‘™]
S (ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘ ğ‘)ğ‘™ ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
ğ‘–=1
ğ‘– ğ‘–,ğ‘– ğ‘– âƒ’ âƒ’ S S
Proof. Note that
â€–B (xâˆ’x*)â€–ğ‘ =â€–SA(xâˆ’x*)â€–ğ‘+â€–B(xâˆ’x*)â€–ğ‘ â‰¤(1+ğ¹ +ğ¶)â€–A(xâˆ’x*)â€–ğ‘
S ğ‘ ğ‘ ğ‘ S ğ‘
so
âƒ’ âƒ’âˆ‘ï¸ğ‘› âƒ’ âƒ’ğ‘™ âƒ’ âƒ’âˆ‘ï¸ğ‘› âƒ’ âƒ’ğ‘™
E sup âƒ’ ğœ€ Sğ‘ âˆ† (x)âƒ’ â‰¤ E sup âƒ’ ğœ€ Sğ‘ âˆ† (x)âƒ’
âƒ’ ğ‘– ğ‘–,ğ‘– ğ‘– âƒ’ âƒ’ ğ‘– ğ‘–,ğ‘– ğ‘– âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
ğ‘–=1
âƒ’ ğœ€âˆ¼{Â±1}ğ‘›â€–BSxâˆ’BSx*â€–ğ‘ ğ‘â‰¤(1+ğ¹S+ğ¶)ğœ‚ğ‘…âƒ’
ğ‘–=1
âƒ’
â‰¤ğœ€ğ‘™ğ›¿(1+ğ¹ +ğ¶)ğ‘™(ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘)ğ‘™
S ğ‘
â‰¤ğœ€ğ‘™ğ›¿2ğ‘™âˆ’1((1+ğ¶)ğ‘™+ğ¹ğ‘™)(ğ‘…+â€–S(Ax*âˆ’b)â€–ğ‘)ğ‘™ Fact B.1
S ğ‘
Taking expectations on both sides proves the lemma.
NotethatifSistheâ„“ Lewisweightsamplingmatrix,thenE[|ğ¹ |ğ‘™]inLemmaA.4isknowntobebounded
ğ‘ S
by ğ‘‚(1)ğ‘™ (that is, S is an ğ‘‚(1)-approximate â„“ subspace embedding) by standard results on â„“ Lewis weight
ğ‘ ğ‘
sampling [CP15, WY23b].
17Furthermore, we can design B such that the â„“ Lewis weights of B are uniformly bounded by ğ›¼, where
ğ‘ S
ğ›¼ is the oversampling parameter such that S samples the ğ‘–th row with probability min{1,w /ğ›¼}. For ğ‘<2,
ğ‘–
this simply follows by taking B to be a flattening of A where every row is duplicated 1/ğ›¼ times due to the
monotonicity of â„“ Lewis weights [CP15]. For ğ‘ > 2, monotonicity of â„“ Lewis weights does not hold, but
ğ‘ ğ‘
Theorem 5.2 of [WY23b] nonetheless shows that ğ›¾-one-sided â„“ Lewis weights can be constructed for B
ğ‘ S
with ğ›¾ =â„¦(1) that makes a similar argument go through.
Finally, it remains to bound the Rademacher process of the form of (3), where A has ğ›¾-one-sided â„“ Lewis
ğ‘
weights uniformly bounded by ğ‘¤ = ğ›¼. We will prove the following in Section B. Assuming this theorem,
Theorem A.2 follows by setting ğ‘¤ =ğ›¼ as stated.
Theorem A.5. For all ğ‘™âˆˆN, we have
E sup
âƒ’ âƒ’ âƒ’âˆ‘ï¸ğ‘›
ğœ€ âˆ†
(x)âƒ’ âƒ’ âƒ’ğ‘™
â‰¤(ğœ€ğ‘…)ğ‘™ (4)
âƒ’ ğ‘– ğ‘– âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
ğ‘–=1
âƒ’
where
â§ âªâ¨ğ‘‚(ğ‘¤ğœ‚2/ğ‘)1/2ğ›¾âˆ’1/2[ï¸(ï¸€ (logğ‘‘)2logğ‘›)ï¸€1+1/ğ‘™ +ğ‘™]ï¸1/2
ğ‘<2
ğœ€= .
âªâ©ğ‘‚(ğ‘¤ğœ‚â€–wâ€–ğ‘/2âˆ’1)1/ğ‘ğ›¾âˆ’1/2[ï¸(ï¸€ (logğ‘‘)2logğ‘›)ï¸€1+1/ğ‘™ +ğ‘™]ï¸1/ğ‘
ğ‘>2
1
B Rademacher process bounds
We continue to fix our notation from Definition A.1. We will prove Theorem A.5 in this section.
We split the sum in (4) into two parts: the part that is bounded by the ğ›¾-one-sided Lewis weights of A,
and the part that is not. To this end, define a threshold
â§ ğœ‚
ğ‘<2
âªâªâ¨ğ›¾ğ‘/2ğœ€ğ‘
ğœ :=
ğœ‚â€–wâ€–ğ‘/2âˆ’1
âªâªâ© 1 ğ‘>2
ğ›¾ğ‘/2ğœ€ğ‘
where ğœ€ will be determined later, and define the set of â€œgoodâ€ entries ğºâŠ†[ğ‘›] as
ğº:={ğ‘–âˆˆ[ğ‘›]:|[Ax*âˆ’b](ğ‘–)|â‰¤ğœw ğ‘…} (5)
ğ‘–
We then bound
âƒ’ âƒ’âˆ‘ï¸ğ‘› âƒ’ âƒ’ğ‘™ âƒ’ âƒ’âˆ‘ï¸ âƒ’ âƒ’ğ‘™
E sup âƒ’ ğœ€ âˆ† (x)âƒ’ â‰¤2ğ‘™âˆ’1 E sup âƒ’ ğœ€ âˆ† (x)âƒ’
âƒ’ ğ‘– ğ‘– âƒ’ âƒ’ ğ‘– ğ‘– âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
ğ‘–=1
âƒ’ ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
ğ‘–âˆˆğº
âƒ’
âƒ’ âƒ’ğ‘™
âƒ’ âƒ’
+2ğ‘™âˆ’1 E sup âƒ’ âƒ’ âˆ‘ï¸ ğœ€ ğ‘–âˆ† ğ‘–(x)âƒ’ âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’
âƒ’ğ‘–âˆˆ[ğ‘›]âˆ–ğº
âƒ’
âƒ’
using the Fact B.1, and separately estimate each term. We can think of the first term as the â€œsensitivityâ€
term, where each term in the sum is bounded by the Lewis weights of A, and the latter term as the â€œoutlierâ€
term, where each term in the sum is much larger than the corresponding Lewis weights.
B.1 Preliminaries
We repeatedly use the following inequalities.
Fact B.1. For any ğ‘â‰¥1 and any ğ‘,ğ‘âˆˆR, |ğ‘+ğ‘|ğ‘ â‰¤2ğ‘âˆ’1(|ğ‘|ğ‘+|ğ‘|ğ‘)=ğ‘‚(|ğ‘|ğ‘+|ğ‘|ğ‘).
Fact B.2 (Corollary A.2, [MMR19]). For any ğ‘ â‰¥ 1, ğœ€ > 0, and any ğ‘,ğ‘ âˆˆ R, |ğ‘+ğ‘|ğ‘ â‰¤ (1+ğœ€)|ğ‘|ğ‘ +
(1+ğœ€)ğ‘âˆ’1 |ğ‘|ğ‘.
ğœ€ğ‘âˆ’1
18Fact B.3. For any ğ‘â‰¥1 and any ğ‘,ğ‘âˆˆR, |ğ‘|ğ‘âˆ’|ğ‘|ğ‘ â‰¤ğ‘|ğ‘âˆ’ğ‘|(|ğ‘|ğ‘âˆ’1+|ğ‘|ğ‘âˆ’1).
We will need the notion of weighted â„“ norms â€–Â·â€– :
ğ‘ w,ğ‘
Definition B.4. Let wâˆˆRğ‘› be non-negative weights. Then for yâˆˆRğ‘›, we define
(ï¸ƒ ğ‘› )ï¸ƒ1/ğ‘
âˆ‘ï¸
â€–yâ€– := w |y(ğ‘–)|ğ‘ .
w,ğ‘ ğ‘–
ğ‘–=1
B.1.1 â„“ Lewis weights
ğ‘
Lemma B.5 (One-sided Lewis weights bound sensitivities). Let AâˆˆRğ‘›Ã—ğ‘‘ and 0<ğ‘<âˆ. Let wâˆˆRğ‘› be
ğ›¾-one-sided â„“ Lewis weights. Then,
ğ‘
{ï¸ƒ
|[Ax](ğ‘–)|ğ‘ ğ›¾âˆ’ğ‘/2â€–wâ€–ğ‘/2âˆ’1Â·w ğ‘>2
sup â‰¤ 1 ğ‘–
â€–Axâ€–ğ‘ ğ›¾âˆ’1Â·w ğ‘<2
xâˆˆrowspan(A)âˆ–{0} ğ‘ ğ‘–
Lemma B.6. Let AâˆˆRğ‘›Ã—ğ‘‘ and let w be ğ›¾-one-sided â„“ Lewis weights for A. Then,
ğ‘
{ï¸ƒ
âƒ¦ âƒ¦ â€–wâ€–1/2âˆ’1/ğ‘â€–Axâ€– ğ‘>2
âƒ¦W1/2âˆ’1/ğ‘Axâƒ¦ â‰¤ 1 ğ‘
âƒ¦ âƒ¦ 2 ğ›¾1/2âˆ’1/ğ‘â€–Axâ€– ğ‘ ğ‘<2
Lemma B.7. Let AâˆˆRğ‘›Ã—ğ‘‘ and let 0<ğ‘<âˆ. The following hold: Let wâˆˆRğ‘› be ğ›¾-one-sided â„“ Lewis
ğ‘
weights, and let R be a change of basis matrix R such that W1/2âˆ’1/ğ‘AR is an orthonormal matrix. Then,
for each ğ‘–âˆˆ[ğ‘›],
w
ğ‘–
â‰¥ğ›¾ğ‘/2Â·âƒ¦ âƒ¦eâŠ¤
ğ‘–
ARâƒ¦ âƒ¦ğ‘ 2.
B.1.2 Gaussian processes
Theorem B.8 (Gaussian comparison, Equation 4.8, [LT91]). Let ğ¹ :R â†’R be convex and let {x }ğ‘› be
+ + ğ‘– ğ‘–=1
a finite sequence in a Banach space. Then,
(ï¸ƒâƒ¦ âƒ¦âˆ‘ï¸ğ‘› âƒ¦ âƒ¦)ï¸ƒ (ï¸ƒ (ï¸ğœ‹)ï¸1/2âƒ¦ âƒ¦âˆ‘ï¸ğ‘› âƒ¦ âƒ¦)ï¸ƒ
E ğ¹ âƒ¦ ğœ€ x âƒ¦ â‰¤ E ğ¹ âƒ¦ g x âƒ¦ .
ğœ€âˆ¼{Â±1}ğ‘› âƒ¦ âƒ¦
ğ‘–=1
ğ‘– ğ‘–âƒ¦ âƒ¦ gâˆ¼ğ’©(0,Iğ‘›) 2 âƒ¦ âƒ¦
ğ‘–=1
ğ‘– ğ‘–âƒ¦ âƒ¦
Theorem B.9 (Dudleyâ€™s entropy integral, Theorem 8.1.6, [Ver18]). Let (ğ‘‹ ) be a Gaussian process with
ğ‘¡ ğ‘¡âˆˆğ‘‡
pseudo-metric ğ‘‘ (ğ‘ ,ğ‘¡):=â€–ğ‘‹ âˆ’ğ‘‹ â€– . Let ğ¸(ğ‘‡,ğ‘‘ ,ğ‘¢) denote the minimal number of ğ‘‘ -balls of radius ğ‘¢
ğ‘‹ ğ‘  ğ‘¡ 2 ğ‘‹ ğ‘‹
required to cover ğ‘‡. Then, for every ğ‘§ â‰¥0, we have that
{ï¸‚ [ï¸‚âˆ«ï¸ âˆ ]ï¸‚}ï¸‚
âˆšï¸€
Pr sup|ğ‘‹ âˆ’ğ‘‹ |â‰¥ğ¶ logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¢) ğ‘‘ğ‘¢+ğ‘§Â·diam(ğ‘‡) â‰¤2exp(âˆ’ğ‘§2)
ğ‘  ğ‘¡ ğ‘‹
ğ‘ ,ğ‘¡âˆˆğ‘‡ 0
Integrating the tail bound gives moment bounds. The following is taken from Lemma 6.8 of [WY23c].
Lemma B.10 (Moment bounds). Let Î› be a Gaussian process with domain ğ‘‡ and distance ğ‘‘ . Let
ğ‘‹
â„° :=âˆ«ï¸€âˆâˆšï¸€ logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¢) ğ‘‘ğ‘¢ and ğ’Ÿ =diam(ğ‘‡). Then, for ğ‘™âˆˆN,
0 ğ‘‹
âˆš
E [|Î›|ğ‘™]â‰¤(2â„°)ğ‘™(â„°/ğ’Ÿ)+ğ‘‚( ğ‘™ğ’Ÿ)ğ‘™
gâˆ¼ğ’©(0,Iğ‘›)
B.2 Estimates on the outlier term
We first bound the outlier terms (ğ‘–âˆˆ/ ğº), which is much easier.
Lemma B.11. With probability 1, we have that
âˆ‘ï¸
sup |âˆ† (x)|â‰¤ğ‘‚(ğœ€)ğ‘….
ğ‘–
â€–Axâˆ’Ax*â€–ğ‘â‰¤ğœ‚ğ‘…
ğ‘ ğ‘–âˆˆ[ğ‘›]âˆ–ğº
19Proof. For each ğ‘–âˆˆ[ğ‘›]âˆ–ğº, we have that
|[Axâˆ’b](ğ‘–)|âˆˆ|[Ax*âˆ’b](ğ‘–)|Â±|[Ax*âˆ’Ax](ğ‘–)|
âˆˆ|[Ax*âˆ’b](ğ‘–)|Â±ğ›¾âˆ’1/2â€–wâ€–1/2âˆ’1/ğ‘w1/ğ‘â€–Ax*âˆ’Axâ€– Lemma B.5
1 ğ‘– ğ‘
âˆˆ|[Ax*âˆ’b](ğ‘–)|Â±ğ›¾âˆ’1/2ğœ‚1/ğ‘â€–wâ€–1/2âˆ’1/ğ‘w1/ğ‘ğ‘…1/ğ‘
1 ğ‘–
âˆˆ|[Ax*âˆ’b](ğ‘–)|Â±ğœ€|[Ax*âˆ’b](ğ‘–)| ğ‘–âˆˆ[ğ‘›]âˆ–ğº
Thus,
|âˆ† (x)|â‰¤ğ‘‚(ğœ€)|[Ax*âˆ’b](ğ‘–)|ğ‘
ğ‘–
so
ğ‘›
âˆ‘ï¸ |âˆ† (x)|â‰¤âˆ‘ï¸ ğ‘‚(ğœ€)|[Ax*âˆ’b](ğ‘–)|ğ‘ =ğ‘‚(ğœ€)â€–Ax*âˆ’bâ€–ğ‘ â‰¤ğ‘‚(ğœ€)ğ‘….
ğ‘– ğ‘
ğ‘–âˆˆ[ğ‘›]âˆ–ğº ğ‘–=1
B.3 Estimates on the sensitivity term
Next, we estimate the sensitivity term (ğ‘–âˆˆğº),
âƒ’ âƒ’ğ‘™
âƒ’âˆ‘ï¸ âƒ’
E sup âƒ’ ğœ€ âˆ† (x)âƒ’ .
âƒ’ ğ‘– ğ‘– âƒ’
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
ğ‘ ğ‘–âˆˆğº
To estimate this moment, we obtain a subgaussian tail bound via the tail form of Dudleyâ€™s entropy integral,
and then integrate it. We will crucially use that |âˆ† (x)| for ğ‘–âˆˆğº is bounded over all â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…,
ğ‘– ğ‘
which gives the following sensitivity bound:
Lemma B.12. For all ğ‘–âˆˆğº, and xâˆˆRğ‘‘ with â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…, we have |[Axâˆ’b](ğ‘–)|ğ‘ â‰¤ğ‘‚(ğœw ğ‘…) and
ğ‘ ğ‘–
|âˆ† (x)|â‰¤ğ‘‚(ğœw ğ‘…).
ğ‘– ğ‘–
Proof. We have
|[Axâˆ’b](ğ‘–)|ğ‘ â‰¤2ğ‘âˆ’1(|[Ax*âˆ’b](ğ‘–)|ğ‘+|[Axâˆ’Ax*](ğ‘–)|ğ‘) Fact B.1
â‰¤2ğ‘âˆ’1ğœw ğ‘…+2ğ‘âˆ’1ğ›¾âˆ’ğ‘/2ğœ‚â€–wâ€–0âˆ¨(ğ‘/2âˆ’1)w ğ‘… ğ‘–âˆˆğº (see (5)) and Lemma B.5
ğ‘– 1 ğ‘–
â‰¤ğ‘‚(ğœw ğ‘…)
ğ‘–
The bound on âˆ† (x) follows easily from the above calculation.
ğ‘–
B.3.1 Bounding low-sensitivity entries
We now separately handle entries ğ‘–âˆˆğº with small Lewis weight. To do this end, define
{ï¸ ğœ€ }ï¸
ğ½ := ğ‘–âˆˆğº:w â‰¥ .
ğ‘– ğœğ‘›
We then bound the mass on the complement of ğ½:
Lemma B.13. For all â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…, we have that
ğ‘
âˆ‘ï¸
|âˆ† (x)|â‰¤ğ‘‚(ğœ€ğ‘…)
ğ‘–
ğ‘–âˆˆ[ğ‘›]âˆ–ğ½
Proof. We have that for each ğ‘–âˆˆ[ğ‘›]âˆ–ğ½, w â‰¤ğœ€/ğœğ‘› so by Lemma B.12,
ğ‘–
âˆ‘ï¸ âˆ‘ï¸ âˆ‘ï¸ ğ‘‚(ğœ€)
|âˆ† (x)|â‰¤ ğ‘‚(ğœw ğ‘…)â‰¤ ğ‘…â‰¤ğ‘‚(ğœ€ğ‘…)
ğ‘– ğ‘– ğ‘›
ğ‘–âˆˆ[ğ‘›]âˆ–ğ½ ğ‘–âˆˆ[ğ‘›]âˆ–ğ½ ğ‘–âˆˆ[ğ‘›]âˆ–ğ½
20B.3.2 Bounding high-sensitivity entries: Gaussian processes
Finally, it remains to bound the Rademacher process only on the entries indexed by ğ‘–âˆˆğ½. By a Gaussian
comparison theorem (Theorem B.8), we may bound the Rademacher process above by a Gaussian process
instead, that is,
âƒ’ âƒ’ğ‘™ âƒ’ âƒ’ğ‘™
âƒ’âˆ‘ï¸ âƒ’ (ï¸ğœ‹)ï¸ğ‘™/2 âƒ’âˆ‘ï¸ âƒ’
E sup âƒ’ ğœ€ âˆ† (x)âƒ’ â‰¤ E sup âƒ’ g âˆ† (x)âƒ’ . (6)
ğœ€âˆ¼{Â±1}ğ‘›â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
ğ‘–âˆˆğ½
ğ‘– ğ‘– âƒ’ âƒ’ 2 gâˆ¼ğ’©(0,Iğ‘›)â€–Axâˆ’Ax*â€–ğ‘ ğ‘â‰¤ğœ‚ğ‘…âƒ’ âƒ’
ğ‘–âˆˆğ½
ğ‘– ğ‘– âƒ’ âƒ’
We can now appeal to the theory of Gaussian processes to bound this quantity. Define a Gaussian process by
âˆ‘ï¸
ğ‘‹ := g âˆ† (x)
x ğ‘– ğ‘–
ğ‘–âˆˆğ½
with pseudo-metric
(ï¸‚ )ï¸‚1/2 (ï¸ƒ )ï¸ƒ1/2
ğ‘‘ (x,xâ€²):= E|ğ‘‹ âˆ’ğ‘‹â€²|2 = âˆ‘ï¸ (âˆ† (x)âˆ’âˆ† (xâ€²))2
ğ‘‹ x x ğ‘– ğ‘–
g
ğ‘–âˆˆğ½
We will use Dudleyâ€™s entropy integral (Theorem B.9) to bound the tail of this quantity, and then integrate to
obtain moment bounds.
Using the sensitivity bound of Lemma B.12, we obtain a bound on the pseudo-metric ğ‘‘ .
ğ‘‹
Lemma B.14. Let ğ‘ =ğ‘‚(log(ğœğ‘›/ğœ€)). For x,xâ€² âˆˆğ‘‡ for ğ‘‡ ={â€–Axâˆ’Ax*â€–ğ‘ â‰¤ğœ‚ğ‘…}, we have that
ğ‘
{ï¸ƒ
ğ‘‚(ğ‘¤1/2)ğœ‚1/ğ‘âˆ’1/2â€–Wâˆ’1/ğ‘A(xâˆ’xâ€²)â€–ğ‘/2ğ‘…1/2 ğ‘<2
ğ‘‘ (x,xâ€²)â‰¤ w,ğ‘
ğ‘‹
ğ‘‚(ğ‘¤1/2)ğœ1/2âˆ’1/ğ‘â€–Wâˆ’1/ğ‘A(xâˆ’xâ€²)â€– ğ‘…1âˆ’1/ğ‘ ğ‘>2
w,ğ‘
and
{ï¸ƒ
ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘ğ›¾âˆ’1/2ğ‘…) ğ‘<2
diam(ğ‘‡)= sup ğ‘‘ (x,xâ€²)â‰¤
ğ‘‹
x,xâ€²âˆˆğ‘‡ ğ‘‚(ğœ€ğ‘¤1/2ğœ1/2ğ‘…) ğ‘>2
Proof. Let y=Axâˆ’b and yâ€² =Axâ€²âˆ’b. Note then that
âˆ‘ï¸ âˆ‘ï¸
ğ‘‘ (x,xâ€²)2 = (âˆ† (x)âˆ’âˆ† (xâ€²))2 = (|y(ğ‘–)|ğ‘âˆ’|yâ€²(ğ‘–)|ğ‘)2
ğ‘‹ ğ‘– ğ‘–
ğ‘–âˆˆğ½ ğ‘–âˆˆğ½
âˆ‘ï¸
â‰¤ğ‘2 |y(ğ‘–)âˆ’yâ€²(ğ‘–)|2(|y(ğ‘–)|ğ‘âˆ’1+|yâ€²(ğ‘–)|ğ‘âˆ’1)2 Fact B.3
ğ‘–âˆˆğ½
For ğ‘<2, we have that
âˆ‘ï¸
ğ‘‘ (x,xâ€²)2 â‰¤ğ‘2â€–(yâˆ’yâ€²)| â€–ğ‘ (|y(ğ‘–)âˆ’yâ€²(ğ‘–)|)2âˆ’ğ‘(|y(ğ‘–)|ğ‘âˆ’1+|yâ€²(ğ‘–)|ğ‘âˆ’1)2
ğ‘‹ ğ½ âˆ
ğ‘–âˆˆğ½
âˆ‘ï¸
â‰¤2ğ‘2â€–(yâˆ’yâ€²)| â€–ğ‘ (|y(ğ‘–)âˆ’yâ€²(ğ‘–)|)2âˆ’ğ‘(|y(ğ‘–)|2ğ‘âˆ’2+|yâ€²(ğ‘–)|2ğ‘âˆ’2)
ğ½ âˆ
ğ‘–âˆˆğ½
â‰¤2ğ‘2â€–(yâˆ’yâ€²)| â€–ğ‘ â€–yâˆ’yâ€²â€–2âˆ’ğ‘(â€–yâ€–2ğ‘âˆ’2+â€–yâ€²â€–2ğ‘âˆ’2) HÂ¨olderâ€™s inequality
ğ½ âˆ ğ‘ ğ‘ ğ‘
â‰¤ğ‘‚(ğœ‚2/ğ‘âˆ’1)â€–(yâˆ’yâ€²)| â€–ğ‘ ğ‘….
ğ½ âˆ
where HÂ¨olderâ€™s inequality is applied with exponents ğ‘ >1 and ğ‘ >1. For ğ‘>2, we have that
2âˆ’ğ‘ 2ğ‘âˆ’2
ğ‘›
ğ‘‘ (x,xâ€²)2 â‰¤2ğ‘2â€–(yâˆ’yâ€²)| â€–2 âˆ‘ï¸ |y(ğ‘–)|2ğ‘âˆ’2+|yâ€²(ğ‘–)|2ğ‘âˆ’2
ğ‘‹ ğ½ âˆ
ğ‘–=1
ğ‘›
â‰¤2ğ‘2max{â€–y| â€– ,â€–yâ€²| â€– }ğ‘âˆ’2â€–(yâˆ’yâ€²)| â€–2 âˆ‘ï¸ |y(ğ‘–)|ğ‘+|yâ€²(ğ‘–)|ğ‘
ğ½ âˆ ğ½ âˆ ğ½ âˆ
ğ‘–=1
21â‰¤ğ‘‚(1)(ğœğ‘¤ğ‘…)1âˆ’2/ğ‘â€–(yâˆ’yâ€²)| â€–2 ğ‘… Lemma B.12
ğ½ âˆ
Furthermore, we have that
â€–(yâˆ’yâ€²)| â€– =â€–(Axâˆ’Axâ€²)| â€–
ğ½ âˆ ğ½ âˆ
=â€–W1/ğ‘(Wâˆ’1/ğ‘Axâˆ’Wâˆ’1/ğ‘Axâ€²)| â€–
ğ½ âˆ
â‰¤ğ‘¤1/ğ‘â€–(Wâˆ’1/ğ‘Axâˆ’Wâˆ’1/ğ‘Axâ€²)| â€–
ğ½ âˆ
â‰¤2ğ‘¤1/ğ‘â€–Wâˆ’1/ğ‘Axâˆ’Wâˆ’1/ğ‘Axâ€²â€–
w,ğ‘
where the last step follows from the fact that w â‰¥ğœ€/ğœğ‘› for ğ‘–âˆˆğ½ and ğ‘ =ğ‘‚(log(ğœğ‘›/ğœ€)). Combining these
ğ‘–
bounds gives the claimed bound on ğ‘‘ (x,xâ€²).
ğ‘‹
Finally, we have by Lemma B.5 that
â€–Wâˆ’1/ğ‘A(xâˆ’x*)â€– =mğ‘›
ax|[A(xâˆ’x*)](ğ‘–)| â‰¤{ï¸ƒ ğ›¾âˆ’1/ğ‘â€–A(xâˆ’x*)â€–
ğ‘
ğ‘<2
âˆ ğ‘–=1 w ğ‘– ğ›¾âˆ’1/2â€–wâ€–1 1/2âˆ’1/ğ‘â€–A(xâˆ’x*)â€– ğ‘ ğ‘>2
so we have the claimed diameter bound for the set {â€–A(xâˆ’x*)â€–ğ‘ â‰¤ğœ‚ğ‘…}.
ğ‘
The following entropy bounds are obtained from [WY23c], which in turn largely follow [BLM89].
Remark B.15. The following entropy bounds are net necessary if we only need this result for ğ‘‘ = 1, for
example for applications to Euclidean power means. In this case, standard volume arguments suffice (see, e.g.,
Lemma 2.4 of [BLM89]).
Lemma B.16. Let 1 â‰¥ w âˆˆ Rğ‘› be non-negative weights. Let 2 â‰¤ ğ‘ < âˆ and let A âˆˆ Rğ‘›Ã—ğ‘‘ be such that
W1/2A is orthonormal. Let ğœ â‰¥maxğ‘› ğ‘–=1âƒ¦ âƒ¦eâŠ¤
ğ‘–
Aâƒ¦ âƒ¦2 2. Let ğµ wğ‘(A):={x:â€–Axâ€–
w,ğ‘
â‰¤1}. Then,
ğ‘›2/ğ‘ğ‘Â·ğœ
logğ¸(ğµ2(A),ğµğ‘(A),ğ‘¡)â‰¤ğ‘‚(1)
w w ğ‘¡2
and
(ï¸‚ )ï¸‚
1 logğ‘‘
logğ¸(ğµğ‘(A),ğµğ‘(A),ğ‘¡)â‰¤ğ‘‚(1) +logğ‘›+ğ‘›2/ğ‘ğ‘ ğœ.
w w ğ‘¡ğ‘ 2âˆ’ğ‘
for ğ‘<2.
We may now evaluate Dudleyâ€™s entropy integral.
Lemma B.17 (Entropy integral bound for ğ‘<2). We have that
âˆ«ï¸ âˆ âˆšï¸€ (ï¸ ğœğ‘›)ï¸1/2
logğ¸(ğµğ‘(A),ğ‘‘ ,ğ‘¡) ğ‘‘ğ‘¡â‰¤ğ‘‚(ğ‘¤1/2ğ›¾âˆ’1/2ğœ‚1/2ğ‘…) log logğ‘‘
ğ‘‹ ğœ€
0
Proof. Note that it suffices to integrate the entropy integral to diam(ğ‘‡), which is bounded in Lemma B.14.
Note also that ğ‘‡ is just a translation of (ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A), so we have
logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¡)=logğ¸((ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A),ğ‘‘ ,ğ‘¡)
ğ‘‹ ğ‘‹
=logğ¸((ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A),ğ¾â€–Wâˆ’1/ğ‘A(Â·)â€–ğ‘/2,ğ‘¡) Lemma B.14
w,ğ‘
=logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡2/ğ‘/ğ¾2/ğ‘(ğœ‚ğ‘…)1/ğ‘)
w w
where ğ¾ =ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘âˆ’1/2ğ‘…1/2).
For small radii less than ğœ† for a parameter ğœ† to be chosen, we use a standard volume argument, which
shows that
ğ‘›
logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡)â‰¤ğ‘‚(ğ‘‘)log
w w ğ‘¡
so
âˆ«ï¸ ğœ† âˆšï¸€ âˆ«ï¸ ğœ†âˆšï¸‚ ğ‘›ğ¾2/ğ‘(ğœ‚ğ‘…)1/ğ‘
logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¡) ğ‘‘ğ‘¡â‰¤ ğ‘‘log ğ‘‘ğ‘¡
ğ‘‹ ğ‘¡2/ğ‘
0 0
22âˆšï¸ âˆš âˆ«ï¸ ğœ†âˆšï¸‚ ğ‘…2/ğ‘
â‰¤ğœ† ğ‘‘log(ğ‘›(ğœ‚2/ğ‘ğ‘¤)1/ğ‘)+ ğ‘‘ log ğ‘‘ğ‘¡
ğ‘¡2/ğ‘
0
âˆšï¸ âˆš âˆšï¸‚ ğ‘…
â‰¤ğœ† ğ‘‘log(ğ‘›(ğœ‚2/ğ‘ğ‘¤)1/ğ‘)+ ğ‘‘Â·ğ‘‚(ğœ†) log
ğœ†
âˆšï¸‚
ğ‘›(ğœ‚2/ğ‘ğ‘¤)1/ğ‘ğ‘…
â‰¤ğ‘‚(ğœ†) ğ‘‘log
ğœ†
On the other hand, for large radii larger than ğœ†, we use the bounds of Lemma B.16. Note that the entropy
bounds do not change if we replace A by AR, where R is the change of basis matrix such that W1/2âˆ’1/ğ‘AR
is orthonormal. Then by the properties of ğ›¾-one-sided â„“ Lewis weights (Lemma B.7), we have
ğ‘
â€–eâŠ¤Wâˆ’1/ğ‘ARâ€–2 =wâˆ’2/ğ‘â€–eâŠ¤ARâ€–2 â‰¤ğ›¾âˆ’1.
ğ‘– 2 ğ‘– ğ‘– 2
Then, Lemma B.16 gives
ğ‘‚(ğ‘¤ğœ‚2/ğ‘ğ‘…2) ğœğ‘›
logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡2/ğ‘/ğ¾2/ğ‘(ğœ‚ğ‘…)1/ğ‘)= log
w w ğ›¾ğ‘¡2 ğœ€
so the entropy integral gives a bound of
ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘ğ‘…)(ï¸ ğœğ‘›)ï¸1/2âˆ«ï¸ diam(ğ‘‡) 1 ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘ğ‘…)(ï¸ ğœğ‘›)ï¸1/2 diam(ğ‘‡)
log ğ‘‘ğ‘¡= log log .
ğ›¾1/2 ğœ€ ğ‘¡ ğ›¾1/2 ğœ€ ğœ†
ğœ†
âˆš
We choose ğœ†=diam(ğ‘‡)/ ğ‘‘, which yields the claimed conclusion.
An analogous result and proof holds for ğ‘>2.
Lemma B.18 (Entropy integral bound for ğ‘>2). Let 2<ğ‘<âˆ. Let AâˆˆRğ‘›Ã—ğ‘‘ and let 0â‰¤w âˆˆRğ‘› be
ğ›¾-one-sided â„“ Lewis weights. Let ğ‘¤ =max w . Then,
ğ‘ ğ‘–âˆˆ[ğ‘›] ğ‘–
âˆ«ï¸ âˆ âˆšï¸€ (ï¸ ğœğ‘›)ï¸1/2
logğ¸(ğµğ‘(A),ğ‘‘ ,ğ‘¡) ğ‘‘ğ‘¡â‰¤ğ‘‚(ğ‘¤1/2ğœ€ğœ1/2ğ‘…) log logğ‘‘
ğ‘‹ ğœ€
0
Proof. Note that it suffices to integrate the entropy integral to diam(ğ‘‡), which is bounded in Lemma B.14.
Note also that ğ‘‡ is just a translation of (ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A), so we have
logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¡)=logğ¸((ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A),ğ‘‘ ,ğ‘¡)
ğ‘‹ ğ‘‹
=logğ¸((ğœ‚ğ‘…)1/ğ‘Â·ğµğ‘(A),ğ¾â€–Wâˆ’1/ğ‘A(Â·)â€– ,ğ‘¡) Lemma B.14
w,ğ‘
=logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡/ğ¾(ğœ‚ğ‘…)1/ğ‘)
w w
where ğ¾ =ğ‘‚(ğ‘¤1/2ğœ1/2âˆ’1/ğ‘ğ‘…1âˆ’1/ğ‘).
For small radii less than ğœ† for a parameter ğœ† to be chosen, we use a standard volume argument, which
shows that
ğ‘›
logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡)â‰¤ğ‘‚(ğ‘‘)log
w w ğ‘¡
so
âˆ«ï¸ ğœ† âˆšï¸€ âˆ«ï¸ ğœ†âˆšï¸‚ ğ‘›ğ¾(ğœ‚ğ‘…)1/ğ‘
logğ¸(ğ‘‡,ğ‘‘ ,ğ‘¡) ğ‘‘ğ‘¡â‰¤ ğ‘‘log ğ‘‘ğ‘¡
ğ‘‹ ğ‘¡
0 0
âˆšï¸ âˆš âˆ«ï¸ ğœ†âˆšï¸‚ ğ‘…
â‰¤ğœ† ğ‘‘log(ğ‘›ğ‘¤1/2ğœ‚1/ğ‘ğœ1/2âˆ’1/ğ‘)+ ğ‘‘ log ğ‘‘ğ‘¡
ğ‘¡
0
âˆšï¸ âˆš âˆšï¸‚ ğ‘…
â‰¤ğœ† ğ‘‘log(ğ‘›ğ‘¤1/2ğœ‚1/ğ‘ğœ1/2âˆ’1/ğ‘)+ ğ‘‘Â·ğ‘‚(ğœ†) log
ğœ†
âˆšï¸‚
ğ‘›ğ‘¤1/2ğœ‚1/ğ‘ğœ1/2âˆ’1/ğ‘ğ‘…
â‰¤ğ‘‚(ğœ†) ğ‘‘log
ğœ†
23On the other hand, for large radii larger than ğœ†, we use the bounds of Lemma B.16. Note that the entropy
bounds do not change if we replace A by AR, where R is the change of basis matrix such that W1/2âˆ’1/ğ‘AR
is orthonormal. Then by the properties of ğ›¾-one-sided â„“ Lewis weights (Lemma B.7), we have
ğ‘
â€–eâŠ¤Wâˆ’1/ğ‘ARâ€–2 =wâˆ’2/ğ‘â€–eâŠ¤ARâ€–2 â‰¤ğ›¾âˆ’1.
ğ‘– 2 ğ‘– ğ‘– 2
Then, Lemma B.6 and Lemma B.16 give
logğ¸(ğµğ‘(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡/ğ¾(ğœ‚ğ‘…)1/ğ‘)
w w
â‰¤ logğ¸(ğµ2(Wâˆ’1/ğ‘A),ğµğ‘(Wâˆ’1/ğ‘A),ğ‘¡/ğ¾(ğœ‚ğ‘…)1/ğ‘â€–wâ€–1/2âˆ’1/ğ‘)
w w 1
ğ¾2(ğœ‚ğ‘…)2/ğ‘â€–wâ€–1âˆ’2/ğ‘ ğœğ‘›
â‰¤ 1 log
ğ›¾ğ‘¡2 ğœ€
ğ‘‚(ğ‘¤)ğœ€2ğœğ‘…2 ğœğ‘›
â‰¤ log
ğ‘¡2 ğœ€
so the entropy integral gives a bound of
(ï¸ ğœğ‘›)ï¸1/2âˆ«ï¸ diam(ğ‘‡) 1 (ï¸ ğœğ‘›)ï¸1/2 diam(ğ‘‡)
ğ‘‚(ğ‘¤1/2ğœ€ğœ1/2ğ‘…) log ğ‘‘ğ‘¡=ğ‘‚(ğ‘¤1/2ğœ€ğœ1/2ğ‘…) log log .
ğœ€ ğ‘¡ ğœ€ ğœ†
ğœ†
âˆš
We choose ğœ†=diam(ğ‘‡)/ ğ‘‘, which yields the claimed conclusion.
We are now ready to prove Theorem A.5.
Proof of Theorem A.5. We have by Lemma B.10 that the Gaussian process of (6) is bounded by
âˆš
(2â„°)ğ‘™(â„°/ğ’Ÿ)+ğ‘‚( ğ‘™ğ’Ÿ)ğ‘™
where
â§ (ï¸ ğœğ‘›)ï¸1/2
âªâ¨ğ‘‚(ğ‘¤1/2ğ›¾âˆ’1/2ğœ‚1/ğ‘ğ‘…) log
ğœ€
logğ‘‘ ğ‘<2
â„° â‰¤
(ï¸ ğœğ‘›)ï¸1/2
âªâ©ğ‘‚(ğœ€ğ‘¤1/2ğœ1/2ğ‘…)
log logğ‘‘ ğ‘>2
ğœ€
by Lemmas B.17 and B.18 and
{ï¸ƒ
ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘ğ›¾âˆ’1/2ğ‘…) ğ‘<2
ğ’Ÿ â‰¤
ğ‘‚(ğœ€ğ‘¤1/2ğœ1/2ğ‘…) ğ‘>2
by Lemma B.14. This gives a bound of (ğ›¼ğ‘…)ğ‘™ on (6), where
â§ [ï¸ƒ(ï¸‚(ï¸ ğœğ‘›)ï¸1/2 )ï¸‚1+1/ğ‘™ âˆš ]ï¸ƒ
âªâªâªâªâ¨ğ‘‚(ğ‘¤1/2ğœ‚1/ğ‘ğ›¾âˆ’1/2) log
ğœ€
logğ‘‘ + ğ‘™ ğ‘<2
ğ›¼=
âªâªâªâªâ©ğ‘‚(ğœ€ğ‘¤1/2ğœ1/2ğ‘…)[ï¸ƒ(ï¸‚(ï¸ logğœ ğœ€ğ‘›)ï¸1/2 logğ‘‘)ï¸‚1+1/ğ‘™ +âˆš ğ‘™]ï¸ƒ
ğ‘>2
We now set ğ›¼=ğœ€ and solve for the ğœ€ that we can obtain. From this, we see that we can set
â§ âªâ¨ğ‘‚(ğ‘¤ğœ‚2/ğ‘)1/2ğ›¾âˆ’1/2[ï¸(ï¸€ (logğ‘‘)2logğ‘›)ï¸€1+1/ğ‘™ +ğ‘™]ï¸1/2
ğ‘<2
ğœ€= .
âªâ©ğ‘‚(ğ‘¤ğœ‚â€–wâ€–ğ‘/2âˆ’1)1/ğ‘ğ›¾âˆ’1/2[ï¸(ï¸€ (logğ‘‘)2logğ‘›)ï¸€1+1/ğ‘™ +ğ‘™]ï¸1/ğ‘
ğ‘>2
1
24C Missing proofs for weak coresets
C.1 Proof of the closeness lemma
Proof of Lemma 4.4. First note that
ğ‘› ğ‘š
âŸ¨ âŸ© âˆ‘ï¸âˆ‘ï¸
(AX*Gâˆ’B)âˆ˜(ğ‘âˆ’1),AX*Gâˆ’AXG = [AX*Gâˆ’B](ğ‘–,ğ‘—)âˆ˜(ğ‘âˆ’1)[A(X*âˆ’X)G](ğ‘–,ğ‘—)
ğ‘–=1ğ‘—=1
ğ‘› ğ‘š
=âˆ‘ï¸âˆ‘ï¸ [AX*Gâˆ’B](ğ‘–,ğ‘—)âˆ˜(ğ‘âˆ’1)âŸ¨ï¸€ (AâŠ¤e )(eâŠ¤GâŠ¤),X*âˆ’XâŸ©ï¸€
ğ‘– ğ‘—
ğ‘–=1ğ‘—=1
âŸ¨ ğ‘› ğ‘š âŸ©
âˆ‘ï¸âˆ‘ï¸
= [AX*Gâˆ’B](ğ‘–,ğ‘—)âˆ˜(ğ‘âˆ’1)(AâŠ¤e )(eâŠ¤GâŠ¤),X*âˆ’X .
ğ‘– ğ‘—
ğ‘–=1ğ‘—=1
The left term in the product is the gradient of the objective at the optimum by Lemma 4.3, so this is just 0
for any X. Then for ğ‘<2, we have by Lemma 4.2 that
ğ‘âˆ’1
â€–AX*Gâˆ’Bâ€–2 + â€–AXGâˆ’AX*Gâ€–2 â‰¤â€–AXGâˆ’Bâ€–2 â‰¤(1+ğœ‚)2â€–AX*Gâˆ’Bâ€–2
ğ‘,ğ‘ 2 ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
which rearranges to
â€–AXGâˆ’AX*Gâ€– â‰¤ğ‘‚(ğœ‚1/2)OPT.
ğ‘,ğ‘
and for ğ‘>2, we have by Lemma 4.2 that
ğ‘âˆ’1
â€–AX*Gâˆ’Bâ€–ğ‘ + â€–AXGâˆ’AX*Gâ€–ğ‘ â‰¤â€–AXGâˆ’Bâ€–ğ‘ â‰¤(1+ğœ‚)ğ‘â€–AX*Gâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘2ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
which rearranges to
â€–AXGâˆ’AX*Gâ€– â‰¤ğ‘‚(ğœ‚1/ğ‘)OPT.
ğ‘,ğ‘
C.2 Proof of the initial weak coreset bound
Proof of Lemma 4.5. We first show that
(ï¸‚ )ï¸‚
1
â€–AXË†Gâˆ’AX*Gâ€–ğ‘ â‰¤ğ‘‚ OPTğ‘
ğ‘,ğ‘ ğ›¿
with probability at least 1âˆ’ğ›¿. By using the fact that S is an ğ‘‚(1)-approximate â„“ subspace embedding, we
ğ‘
have that
â€–AXË†Gâˆ’AX*Gâ€–ğ‘ â‰¤â€–S(AXË†Gâˆ’AX*G)â€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
(ï¸ )ï¸
â‰¤2ğ‘âˆ’1 â€–S(AXË†Gâˆ’B)â€–ğ‘ +â€–S(AX*Gâˆ’B)â€–ğ‘ Fact B.1
ğ‘,ğ‘ ğ‘,ğ‘
â‰¤2ğ‘+1â€–S(AX*Gâˆ’B)â€–ğ‘ Approximate optimality of XË†
ğ‘,ğ‘
Thelatterquantityisatmostğ‘‚(1)OPTğ‘ withprobabilityatleast1âˆ’ğ›¿ byMarkovâ€™sinequality. Thus,wemay
ğ›¿
replace the optimization of XË† over all XâˆˆRğ‘‘Ã—ğ‘¡ with optimization over the ball {X:â€–AXGâˆ’AX*Gâ€–ğ‘ =
ğ‘,ğ‘
ğ‘‚(1)OPTğ‘}.
ğ›¿
The rest of the proof now mimics the proof of Theorem 3.1. We apply Theorem 3.2 with accuracy
parameter ğœ€ set to ğœ€ğ›¿, failure parameter set to (ğœ€ğ›¿)ğ‘ğ›¿2, and proximity parameter ğœ‚ set to 1. Let ğ‘† âŠ†[ğ‘š] be
the set of columns for which Theorem 3.2 fails. Then by applying Markovâ€™s inequality twice as in the proof
of Theorem 3.1, we have that
âˆ‘ï¸
â€–S(AX*Gâˆ’B)e â€–ğ‘ =ğ‘‚((ğœ€ğ›¿)ğ‘)OPTğ‘
ğ‘— ğ‘
ğ‘—âˆˆğ‘†
25and
âˆ‘ï¸
â€–(AX*Gâˆ’B)e â€–ğ‘ =ğ‘‚((ğœ€ğ›¿)ğ‘)OPTğ‘
ğ‘— ğ‘
ğ‘—âˆˆğ‘†
and thus it follows that
âˆ‘ï¸ â€–S(AXGâˆ’B)e â€–ğ‘ =âˆ‘ï¸ â€–(AXGâˆ’B)e â€–ğ‘Â±ğ‘‚(ğœ€ğ›¿)(ï¸€ â€–A(Xâˆ’X*)Gâ€–ğ‘+OPTğ‘)ï¸€ .
ğ‘— ğ‘ ğ‘— ğ‘ ğ‘
ğ‘—âˆˆğ‘† ğ‘—âˆˆğ‘†
Summing this result with the rest of the columns ğ‘— âˆˆ/ ğ‘† gives that
âƒ’ âƒ’(ï¸€ â€–S(AXGâˆ’B)â€–ğ‘ âˆ’â€–S(AX*Gâˆ’B)â€–ğ‘ )ï¸€ âˆ’(ï¸€ â€–AXGâˆ’Bâ€–ğ‘ âˆ’â€–AX*Gâˆ’Bâ€–ğ‘ )ï¸€âƒ’
âƒ’
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
â‰¤ ğœ€ğ›¿(ï¸€ â€–AX*Gâˆ’Bâ€–ğ‘ +â€–S(AX*Gâˆ’B)â€–ğ‘ +â€–AXGâˆ’AX*Gâ€–ğ‘ )ï¸€ â‰¤ğ‘‚(ğœ€)OPTğ‘
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
Thus, in the ball {X:â€–AXGâˆ’AX*Gâ€–ğ‘ =ğ‘‚(1)OPTğ‘}, we have that
ğ‘,ğ‘ ğ›¿
â€–S(AXGâˆ’B)â€–ğ‘ =â€–AXGâˆ’Bâ€–ğ‘ +(â€–S(AX*Gâˆ’B)â€–ğ‘ âˆ’â€–AX*Gâˆ’Bâ€–ğ‘ )Â±ğ‘‚(ğœ€)OPTğ‘.
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
It follows that XË† must minimize â€–AXGâˆ’Bâ€–ğ‘ up to an additive ğ‘‚(ğœ€)OPTğ‘.
ğ‘,ğ‘
C.3 Proof of the weak coreset construction
Proof of Theorem 4.1. Let
â§ [ï¸‚ 1]ï¸‚
âªâªâ¨ğ‘‚(ğ›¾âˆ’1)ğ›¿âˆ’2â€–wâ€–
1
(logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘<2
ğ¶ =
[ï¸‚ ]ï¸‚
1
âªâªâ©ğ‘‚(ğ›¾âˆ’ğ‘/2)ğ›¿âˆ’ğ‘â€–wâ€–ğ‘ 1/2 (logğ‘‘)2logğ‘›+log
ğ›¿
ğ‘>2
We will make use of the fact that â€–S(AX*Gâˆ’B)â€–ğ‘ =ğ‘‚(1)â€–S(AX*Gâˆ’B)â€–ğ‘ with probability at least
ğ‘,ğ‘ ğ›¿ ğ‘,ğ‘
1âˆ’ğ›¿ by Markovâ€™s inequality.
We will first give the argument for ğ‘<2. Suppose that ğ¶/ğœ€ğ›½ rows are needed for a (1+ğœ€)-approximate
weak coreset. Now choose ğ‘ such that ğ‘ âˆ’ 2 = âˆ’ğ‘ğ›½, that is, ğ‘ = 2/(1 + ğ›½). Then for ğœ‚2/ğ‘ = ğœ€ğ‘,
ğ¶ğœ‚2/ğ‘/(ğœ€ğ›¿)2 =ğ¶/ğœ‚(2/ğ‘)ğ›½ rows yields a (1+ğœ‚2/ğ‘)-approximate weak coreset. Then, a (1+ğœ‚2/ğ‘)-approximate
minimizer X satisfies
â€–AXGâˆ’AX*Gâ€–ğ‘ â‰¤ğ‘‚(ğœ‚)â€–AX*Gâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
by Lemma 4.4. For all such X, an argument as done in Theorem 3.1 and Lemma 4.5 shows that â€–S(AXGâˆ’
B)â€–ğ‘ âˆ’â€–S(AX*Gâˆ’B)â€–ğ‘ and â€–AXGâˆ’Bâ€–ğ‘ âˆ’â€–AX*Gâˆ’Bâ€–ğ‘ are close up to an additive error of
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
(ï¸‚ )ï¸‚
1
ğœ€ğ›¿ â€–AX*Gâˆ’Bâ€–ğ‘ +â€–S(AX*Gâˆ’B)â€–ğ‘ + â€–AXGâˆ’AX*Gâ€–ğ‘ =ğ‘‚(ğœ€)â€–AX*Gâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘ ğœ‚ ğ‘,ğ‘ ğ‘,ğ‘
Thus, ğ¶/ğœ‚(2/ğ‘)ğ›½ rows in fact gives a (1+ğ‘‚(ğœ€))-approximate minimizer. That is, if ğ¶/ğœ€ğ›½ rows is sufficient for
(1+ğœ€)-approximation, then ğ¶/ğœ‚(2/ğ‘)ğ›½ =ğ¶/ğœ€ğ‘ğ›½ =ğ¶/ğœ€2ğ›½/(1+ğ›½) rows is sufficient for (1+ğœ€)-approximation as
well. We may now iterate this argument. Consider the sequence ğ›½ given by
ğ‘–
2ğ›½
ğ›½ =2, ğ›½ = ğ‘– .
0 ğ‘–+1 1+ğ›½
ğ‘–
The solution to this recurrence is given by the following lemma, with ğ‘=2:
Lemma C.1. Let ğ‘>1 and let {ğ›½ }âˆ be defined by the recurrence relation ğ›½ =ğ‘ and ğ›½ =ğ‘ğ›½ /(1+ğ›½ ).
ğ‘– ğ‘–=0 0 ğ‘–+1 ğ‘– ğ‘–
Then,
1
ğ›½ =
ğ‘– ğ‘âˆ’ğ‘–(ğ‘âˆ’1âˆ’(ğ‘âˆ’1)âˆ’1)+(ğ‘âˆ’1)âˆ’1
26Proof. Note that 1 = 1 1 + 1 so the sequence {ğ‘ }âˆ given by ğ‘ =1/ğ›½ satisfies the linear recurrence
ğ›½ğ‘–+1 ğ‘ğ›½ğ‘– ğ‘ ğ‘– ğ‘–=0 ğ‘– ğ‘–
ğ‘ = 1ğ‘ + 1. Note that this recurrence has the fixed point ğ‘ = 1/(ğ‘âˆ’1), so the sequence ğ‘â€² = ğ‘ âˆ’ğ‘
ğ‘–+1 ğ‘ ğ‘– ğ‘ ğ‘– ğ‘–
satisfies ğ‘â€² = 1ğ‘â€², which gives, ğ‘â€² =ğ‘âˆ’ğ‘–ğ‘â€². Thus, ğ‘ âˆ’ğ‘=ğ‘âˆ’ğ‘–(ğ‘ âˆ’ğ‘) so
ğ‘–+1 ğ‘ ğ‘– ğ‘– 0 ğ‘– 0
1 1
ğ›½ = =
ğ‘– ğ‘ ğ‘âˆ’ğ‘–(ğ‘ âˆ’ğ‘)+ğ‘
ğ‘– 0
1
= .
ğ‘âˆ’ğ‘–(ğ‘âˆ’1âˆ’(ğ‘âˆ’1)âˆ’1)+(ğ‘âˆ’1)âˆ’1
Thus, applying this argument ğ‘‚(loglog1) times yields that ğ›½ â‰¤ 1+ğ‘‚(1/log(1)) which means that
ğœ€ ğ‘– ğœ€
reading only ğ‘‚(1)ğ¶/ğœ€ entries suffices. Union bounding over the success of the ğ‘‚(loglog1) rounds completes
ğœ€
the argument.
Next, let ğ‘ > 2. Suppose that ğ¶/ğœ€ğ›½ rows are needed for a (1+ğœ€)-approximate weak coreset. Now
choose ğ‘ such that ğ‘âˆ’ğ‘ = âˆ’ğ‘ğ›½, that is, ğ‘ = ğ‘/(1+ğ›½). Then for ğœ‚ = ğœ€ğ‘, ğ¶ğœ‚/ğœ€ğ‘ = ğ¶/ğœ‚ğ›½ rows yields a
(1+ğœ‚)-approximate weak coreset. Then, a (1+ğœ‚)-approximate minimizer X satisfies
â€–AXGâˆ’AX*Gâ€–ğ‘ â‰¤ğ‘‚(ğœ‚)â€–AX*Gâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğ‘,ğ‘
by Lemma 4.4. For all such X, an argument as done in Theorem 3.1 and Lemma 4.5 shows that â€–S(AXGâˆ’
B)â€–ğ‘ âˆ’â€–S(AX*Gâˆ’B)â€–ğ‘ and â€–AXGâˆ’Bâ€–ğ‘ âˆ’â€–AX*Gâˆ’Bâ€–ğ‘ are close up to an additive error of
ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
(ï¸‚ )ï¸‚
1
ğœ€ â€–AX*Gâˆ’Bâ€–ğ‘ + â€–AXGâˆ’AX*Gâ€–ğ‘ =ğ‘‚(ğœ€)â€–AX*Gâˆ’Bâ€–ğ‘
ğ‘,ğ‘ ğœ‚ ğ‘,ğ‘ ğ‘,ğ‘
Thus, ğ¶/ğœ‚ğ›½ rows in fact gives a (1+ğ‘‚(ğœ€))-approximate minimizer. That is, if ğ¶/ğœ€ğ›½ rows is sufficient for
(1+ğœ€)-approximation, then ğ¶/ğœ‚ğ›½ =ğ¶/ğœ€ğ‘ğ›½ =ğ¶/ğœ€ğ‘ğ›½/(1+ğ›½) rows is sufficient for (1+ğœ€)-approximation as well.
We may now iterate this argument. Consider the sequence ğ›½ given by
ğ‘–
ğ‘ğ›½
ğ›½ =ğ‘, ğ›½ = ğ‘– .
1 ğ‘–+1 1+ğ›½
ğ‘–
Then by Lemma C.1, applying this argument ğ‘‚(loglog1) times yields that ğ›½ â‰¤(ğ‘âˆ’1)+ğ‘‚(1/log(1)) which
ğœ€ ğ‘– ğœ€
means that reading only ğ‘‚(1)ğ¶/ğœ€ğ‘âˆ’1 entries suffices. Union bounding over the success of the ğ‘‚(loglog1)
ğœ€
rounds completes the argument.
D Missing proofs for applications
D.1 Sublinear algorithm for Euclidean power means
Theorem 1.6. Let {b }ğ‘› âŠ†Rğ‘‘. Then, there is a sublinear algorithm which uniformly samples at most
ğ‘– ğ‘–=1
â§ ğ‘‚(ğœ€âˆ’2)(ï¸€ log1 +log1)ï¸€ log1 ğ‘=1
âªâ¨ ğœ€ ğ›¿ ğ›¿
ğ‘ = ğ‘‚(ğœ€âˆ’1)(ï¸€ log1 +log1)ï¸€ log1 1<ğ‘â‰¤2
ğœ€ ğ›¿ ğ›¿
âªâ©ğ‘‚(ğœ€1âˆ’ğ‘)(ï¸€ log1 +log1)ï¸€ log1 2<ğ‘<âˆ
ğœ€ ğ›¿ ğ›¿
rows b and outputs a center xË† such that
ğ‘–
ğ‘› ğ‘›
âˆ‘ï¸ âˆ‘ï¸
â€–xË†âˆ’b â€–ğ‘ â‰¤(1+ğœ€) min â€–xâˆ’b â€–ğ‘
ğ‘– 2 ğ‘– 2
xâˆˆRğ‘‘
ğ‘–=1 ğ‘–=1
with probability at least 1âˆ’ğ›¿.
Proof. We will assume without loss of generality that by reading ğ‘‚(log1) rows of B, we can identify an
ğ›¿
ğ‘‚(1)-approximate solution xË† (see, e.g., Section 3.1 of [MMWY22]). Thus by subtracting off this solution, we
may assume that â€–Bâ€–ğ‘ =ğ‘‚(OPTğ‘).
ğ‘,2
27We then use Dvoretzkyâ€™s thoerem to embed this problem into the entrywise â„“ norm, so that
ğ‘
â€–1xâŠ¤âˆ’Bâ€–ğ‘ =(1Â±ğœ€)â€–1xâŠ¤Gâˆ’BGâ€–ğ‘
ğ‘,2 ğ‘,ğ‘
for every center xâˆˆRğ‘‘. This is now in a form where we may apply our weak coreset results for multiple â„“
ğ‘
regression of Theorem 1.5. Note that in this particular setting, the A matrix corresponds to the ğ‘›Ã—ğ‘‘ all
ones matrix with ğ‘‘=1, and the â„“ Lewis weights can be taken to be uniform.
ğ‘
Now consider running ğ¿=ğ‘‚(log1) independent instances of the weak coreset algorithm, each which has
ğ›¿
the property that the algorithm makes at most
(ï¸‚ )ï¸‚
1 1
ğ‘‚(ğœ€âˆ’ğœŒ) log +log (7)
ğœ€ ğ›¿
queries for ğœŒ = 2 for ğ‘ = 1, ğœŒ = 1 for 1 < ğ‘ < 2, and ğœŒ = ğ‘âˆ’1 for 2 < ğ‘ < âˆ, and that if â€–S(1(x*)âŠ¤Gâˆ’
BG)â€–ğ‘ =ğ‘‚(â€–1(x*)âŠ¤Gâˆ’BGâ€–ğ‘ ) for the optimal solution x*, then it succeeds with probability at least
ğ‘,ğ‘ ğ‘,ğ‘
1âˆ’ğ›¿/ğ¿. By a union bound, this holds for all ğ¿ instances.
By Markovâ€™s inequality, each instance satisfies â€–SBGâ€–ğ‘ =ğ‘‚(â€–BGâ€–ğ‘ ) with probability at least 9/10,
ğ‘,ğ‘ ğ‘,ğ‘
so at least 2/3 of the ğ¿ instances must satisfy this bound with probability at least 1âˆ’ğ›¿. By Dvoretzkyâ€™s
theorem, this means that â€–SBâ€–ğ‘ =ğ‘‚(â€–Bâ€–ğ‘ ). Then, if we restrict our attention to the (2/3)ğ¿ instances
ğ‘,2 ğ‘,2
with the smallest values of â€–SBâ€–ğ‘ , then all of these instances must output a correct (1+ğœ€)-approximately
ğ‘,2
optimal solution, simultaneously with probability 1âˆ’ğ›¿. This gives a query bound of ğ¿ times (7).
D.2 Spanning coresets for â„“ subspace approximation
ğ‘
We show that weak coreset construction imply spanning sets for â„“ subspace approximation.
ğ‘
Theorem 1.9. Let {a }ğ‘› âŠ†Rğ‘‘, 1â‰¤ğ‘<âˆ, ğ‘˜ âˆˆN, and 0<ğœ€<1. Then, there exists a (1+ğœ€)-spanning
ğ‘– ğ‘–=1
coreset ğ‘† of size at most
â§
ğ‘‚(ğœ€âˆ’2ğ‘˜)(log(ğ‘˜/ğœ€))3 ğ‘=1
âªâ¨
|ğ‘†|= ğ‘‚(ğœ€âˆ’1ğ‘˜)(log(ğ‘˜/ğœ€))3 1<ğ‘â‰¤2
âªâ©ğ‘‚(ğœ€1âˆ’ğ‘ğ‘˜ğ‘/2)(log(ğ‘˜/ğœ€))3
2<ğ‘<âˆ
Proof. By first computing a strong coreset of size poly(ğ‘˜/ğœ€) [HV20], we can assume that ğ‘›,ğ‘‘=poly(ğ‘˜/ğœ€).
Let P=VVâŠ¤ be the rank ğ‘˜ projection that minimizes â€–APâˆ’Aâ€–ğ‘ . Note then that
ğ‘,2
min â€–AVXâˆ’Aâ€–ğ‘ =â€–APâˆ’Aâ€–ğ‘ .
ğ‘,2 ğ‘,2
XâˆˆRğ‘˜Ã—ğ‘‘
We then use Dvoretzkyâ€™s theorem to embed this problem into the entrywise â„“ norm, so that
ğ‘
â€–AVXâˆ’Aâ€–ğ‘ =(1Â±ğœ€)â€–AVXGâˆ’AGâ€–ğ‘
ğ‘,2 ğ‘,ğ‘
for every X âˆˆ Rğ‘˜Ã—ğ‘‘, for some fixed G âˆˆ Rğ‘‘Ã—ğ‘š with ğ‘š = poly(ğ‘‘/ğœ€). Then by our weak coreset result for
multiple â„“ regression (Theorem 4.1), there is a diagonal matrix S with
ğ‘
â§
ğ‘‚(ğœ€âˆ’2ğ‘˜)(log(ğ‘˜/ğœ€))3 ğ‘=1
âªâ¨
nnz(S)â‰¤ ğ‘‚(ğœ€âˆ’1ğ‘˜)(log(ğ‘˜/ğœ€))3 1<ğ‘â‰¤2
âªâ©ğ‘‚(ğœ€1âˆ’ğ‘ğ‘˜ğ‘/2)(log(ğ‘˜/ğœ€))3
2<ğ‘<âˆ
such that any (1+ğœ€)-approximate minimizer XË† of â€–S(AVXGâˆ’AG)â€–ğ‘ satisfies
ğ‘,ğ‘
â€–AVXË†Gâˆ’AGâ€–ğ‘ â‰¤(1+ğœ€) min â€–AVXGâˆ’AGâ€–ğ‘ .
ğ‘,ğ‘ ğ‘,ğ‘
XâˆˆRğ‘˜Ã—ğ‘‘
We will take XË† to be
XË† =arg min â€–S(AVXâˆ’A)â€–ğ‘
ğ‘,2
XâˆˆRğ‘˜Ã—ğ‘‘
28which is indeed a (1+ğœ€)-approximate minimizer of â€–S(AVXGâˆ’AG)â€–ğ‘ by Dvoretzkyâ€™s theorem. Then,
ğ‘,ğ‘
again by Dvoretzkyâ€™s theorem, we then have for this XË† that
â€–AVXË† âˆ’Aâ€–ğ‘ â‰¤(1+ğ‘‚(ğœ€)) min â€–AVXâˆ’Aâ€–ğ‘
ğ‘,2 ğ‘,2
XâˆˆRğ‘˜Ã—ğ‘‘
=(1+ğ‘‚(ğœ€))â€–APâˆ’Aâ€–ğ‘ .
ğ‘,2
Finally, note that XË† has row span contained in the row span of SA, since otherwise â€–S(AVXâˆ’A)â€–ğ‘
ğ‘,2
can be reduced by projecting the rows of X onto rowspan(SA). Then, if P is the projection matrix onto
ğ¹
ğ¹ =rowspan(XË†), then for each row ğ‘–âˆˆ[ğ‘›] of A,
â€–P a âˆ’a â€– =minâ€–xâˆ’a â€– â‰¤â€–XË†âŠ¤VâŠ¤a âˆ’a â€–
ğ¹ ğ‘– ğ‘– 2 ğ‘– 2 ğ‘– ğ‘– 2
xâˆˆğ¹
so
â€–AP âˆ’Aâ€–ğ‘ â‰¤â€–AVXË† âˆ’Aâ€–ğ‘ .
ğ¹ ğ‘,2 ğ‘,2
We thus conclude that there is a rank ğ‘˜ subspace in the row span of SA that is (1+ğœ€)-approximately
optimal.
E Missing proofs for coreset lower bounds
We provide missing proofs from Section 5.
We will use the following lemma from coding theory.
Theorem E.1 ([PTB13]). For any ğ‘â‰¥1 and ğ‘‘=2ğ‘˜âˆ’1 for some integer ğ‘˜, there exists a set ğ‘† âŠ†{âˆ’1,1}ğ‘‘
and a constant ğ¶ depending only on ğ‘ which satisfy
ğ‘
â€¢ |ğ‘†|=ğ‘‘ğ‘
âˆš
â€¢ For any ğ‘ ,ğ‘¡âˆˆğ‘† such that ğ‘ Ì¸=ğ‘¡, |âŸ¨ğ‘ ,ğ‘¡âŸ©|â‰¤ğ¶ ğ‘‘
ğ‘
E.1 Strong coresets
Proof of Theorem 5.1. Let ğ‘  = ğ‘‘ğ‘/2 and let ğ‘† âŠ† {Â±1}ğ‘‘ be a set of |ğ‘†| = ğ‘  points given by Theorem E.1
âˆš âˆš
such that âŸ¨a,aâ€²âŸ© â‰¤ ğ¶ ğ‘‘ = ğ‘‚( ğ‘‘) for some ğ¶ğ‘ â‰¥ 1, for every distinct a,aâ€² âˆˆ ğ‘†. Let ğ‘š = ğ‘ ğœ€âˆ’ğ‘, let
ğ‘/2 ğ‘/2
Aâˆˆ{Â±1}ğ‘šÃ—ğ‘‘ be the matrix with ğœ€âˆ’ğ‘ copies of a in its rows for each aâˆˆğ‘†, and let B=ğ‘‘Â·I be the ğ‘šÃ—ğ‘š
ğ‘š
identity matrix scaled by ğ‘‘. For each row ğ‘–âˆˆ[ğ‘š], we say that ğ‘–â€² âˆˆ[ğ‘ ] is its group number if eâŠ¤A is the ğ‘–â€²-th
ğ‘–
point in ğ‘†.
Suppose for contradiction that S is a strong coreset with nnz(S)â‰¤ğ‘š/16 such that
(ï¸ƒ )ï¸ƒ
ğœ€
â€–S(AXâˆ’B)â€–ğ‘ = 1Â± â€–AXâˆ’Bâ€–ğ‘
ğ‘,ğ‘ 12ğ¶ğ‘ ğ‘,ğ‘
ğ‘/2
for every XâˆˆRğ‘‘Ã—ğ‘š. Then, there is a subset ğ‘‡ âŠ†[ğ‘š] with |ğ‘‡|=ğ‘š/16 such that S is supported on ğ‘‡. For
eachğ‘–â€² âˆˆ[ğ‘ ],letğ‘‡ âŠ†ğ‘‡ denotetherowsofğ‘‡ whoserowsinAwithgroupnumberğ‘–â€² âˆˆ[ğ‘ ],soâˆ‘ï¸€ğ‘  |ğ‘‡ |=|ğ‘‡|.
ğ‘–â€² ğ‘–â€²=1 ğ‘–â€²
Then by averaging, there are at least (3/4)ğ‘  groups ğ‘–â€² âˆˆ[ğ‘ ] such that |ğ‘‡ |â‰¤ğœ€âˆ’ğ‘/2. Thus, we may assume
ğ‘–â€²
without loss of generality that |ğ‘‡ |=ğœ€âˆ’ğ‘ for the first (1/4)ğ‘  groups, |ğ‘‡ |=ğœ€âˆ’ğ‘/2 for the last (3/4)ğ‘  groups,
ğ‘–â€² ğ‘–â€²
and |ğ‘‡|=(5/8)ğ‘š.
Let ğ‘Š := âˆ‘ï¸€ğ‘š |S |ğ‘ denote the total weight mass of S. Note then that by querying X = 0, we must
ğ‘–=1 ğ‘–,ğ‘–
have that
(ï¸ƒ )ï¸ƒ
ğœ€
â€–SBâ€–ğ‘ =ğ‘Š =(1Â±ğœ€)â€–Bâ€–ğ‘ = 1Â± ğ‘š.
ğ‘,ğ‘ ğ‘,ğ‘ 12ğ¶ğ‘
ğ‘/2
Let ğ‘Š denote the sum of |S |ğ‘ on the first (1/4)ğ‘  groups, and let ğ‘Š denote the sum of |S |ğ‘ on the last
1 ğ‘–,ğ‘– 2 ğ‘–,ğ‘–
(3/4)ğ‘  groups. We will assume that ğ‘Š â‰¤ğ‘š/4, since the case of ğ‘Š â‰¥ğ‘š/4 is symmetric.
1 1
29We now construct a query XâˆˆRğ‘‘Ã—ğ‘š with the ğ‘—-th column given by
{ï¸ƒ
ğœ€Â·eâŠ¤A ğ‘— âˆˆğ‘‡
Xe = ğ‘—
ğ‘—
0 ğ‘— âˆˆ/ ğ‘‡
Note then that for each ğ‘–,ğ‘— âˆˆ[ğ‘š],
â§
ğœ€ğ‘‘ eâŠ¤A=eâŠ¤A,ğ‘— âˆˆğ‘‡
âªâ¨ âˆš ğ‘– ğ‘—
eâŠ¤AXe = ğœ€ğ¶ ğ‘‘ eâŠ¤AÌ¸=eâŠ¤A,ğ‘— âˆˆğ‘‡
ğ‘– ğ‘— ğ‘/2 ğ‘– ğ‘—
âªâ©0
ğ‘— âˆˆ/ ğ‘‡
Let ğ‘–âˆˆ[ğ‘š] and let ğ‘–â€² âˆˆ[ğ‘ ] be its group number. Then the cost of row ğ‘– if ğ‘–âˆˆğ‘‡ is
ğ‘š
â€–eâŠ¤
ğ‘–
AXâˆ’eâŠ¤
ğ‘–
Bâ€–ğ‘
ğ‘
=âˆ‘ï¸ ğ‘—=1âƒ’ âƒ’eâŠ¤
ğ‘–
AXe
ğ‘—
âˆ’B(ğ‘–,ğ‘—)âƒ’ âƒ’ğ‘ = âŸ(1âˆ’
ğ‘–
=ğœ€
â
ğ‘—)ğ‘ğ‘‘ğ‘ +(|ğ‘‡ ğ‘–â€²|âˆ’1)Â·
eâŠ¤ ğ‘–
AâŸğœ€ğ‘ =ğ‘‘
â
eğ‘
âŠ¤
ğ‘—A+(|ğ‘‡|âˆ’|ğ‘‡ ğ‘–â€²|)Â·ğœ€
âŸ
eğ‘ âŠ¤ğ¶ Ağ‘ğ‘
Ì¸=/ â2
eğ‘‘ âŠ¤ğ‘ A/ 2
ğ‘– ğ‘—
=(1âˆ’ğ‘ğœ€+|ğ‘‡ |ğœ€ğ‘+(5/8)ğ¶ğ‘ +ğ‘œ(ğœ€))ğ‘‘ğ‘
ğ‘–â€² ğ‘/2
while the cost of row ğ‘–âˆˆ[ğ‘š] if ğ‘–âˆˆ/ ğ‘‡ is
ğ‘š
â€–eâŠ¤
ğ‘–
AXâˆ’eâŠ¤
ğ‘–
Bâ€–ğ‘
ğ‘
=âˆ‘ï¸âƒ’ âƒ’eâŠ¤
ğ‘–
AXe
ğ‘—
âˆ’B(ğ‘–,ğ‘—)âƒ’ âƒ’ğ‘ = âŸğ‘‘ âğ‘ +|ğ‘‡ ğ‘–â€²|Â· ğœ€ âŸğ‘ ğ‘‘ âğ‘ +(|ğ‘‡|âˆ’|ğ‘‡ ğ‘–â€²|)Â·ğœ€ğ‘ğ¶ ğ‘ğ‘ /2ğ‘‘ğ‘/2
ğ‘—=1 ğ‘–=ğ‘— eâŠ¤A=eâŠ¤A âŸ â
ğ‘– ğ‘— eâŠ¤AÌ¸=eâŠ¤A
ğ‘– ğ‘—
=(1+|ğ‘‡â€²|ğœ€ğ‘+(5/8)ğ¶ğ‘ +ğ‘œ(ğœ€))ğ‘‘ğ‘.
ğ‘– ğ‘/2
Let
ğ‘ =(1âˆ’ğ‘ğœ€+1+(5/8)ğ¶ğ‘ +ğ‘œ(ğœ€))ğ‘‘ğ‘
1 ğ‘/2
ğ‘ =(1âˆ’ğ‘ğœ€+(1/2)+(5/8)ğ¶ğ‘ +ğ‘œ(ğœ€))ğ‘‘ğ‘
2 ğ‘/2
ğ‘ =(1+(1/2)+(5/8)ğ¶ğ‘ +ğ‘œ(ğœ€))ğ‘‘ğ‘
3 ğ‘/2
Then, the total true cost is at least
ğ‘š 3ğ‘š 3ğ‘š
â€–AXâˆ’Bâ€–ğ‘ = ğ‘ + ğ‘ + ğ‘
ğ‘,ğ‘ 4 1 8 2 8 3
ğ‘š 3ğ‘š 3ğ‘š
= ğ‘ + ğ‘ + (ğ‘ âˆ’ğ‘ )
4 1 4 2 8 3 2
ğ‘š 3ğ‘š 3ğ‘š
â‰¥ ğ‘ + ğ‘ + Â·(ğœ€âˆ’ğ‘œ(ğœ€))ğ‘‘ğ‘
4 1 4 2 4
while the strong coreset estimate is at most
â€–S(AXâˆ’B)â€–ğ‘ =ğ‘Š ğ‘ +ğ‘Š ğ‘
ğ‘,ğ‘ 1 1 2 2
=ğ‘Š (ğ‘ âˆ’ğ‘ )+(ğ‘Š +ğ‘Š )ğ‘
1 1 2 1 2 2
(ï¸ƒ )ï¸ƒ
ğ‘š ğœ€
â‰¤ (ğ‘ âˆ’ğ‘ )+ 1+ ğ‘šğ‘
4 1 2 12ğ¶ğ‘ 2
ğ‘/2
ğ‘š 3ğ‘š ğœ€
â‰¤ ğ‘ + ğ‘ + ğ‘šğ‘‘ğ‘.
4 1 4 2 4
Furthermore,
(ï¸‚ )ï¸‚
ğœ€ ğ‘š 3ğ‘š ğœ€ ğœ€
ğ‘ + ğ‘ + ğ‘šğ‘‘ğ‘ â‰¤ ğ‘šğ‘‘ğ‘
12ğ¶ğ‘ 4 1 4 2 4 4
ğ‘/2
so (1+ ğœ€ )â€–S(AXâˆ’B)â€–ğ‘ < â€–AXâˆ’Bâ€–ğ‘ and thus S fails to be a strong coreset. Rescaling ğœ€ by
12ğ¶ğ‘ ğ‘,ğ‘ ğ‘,ğ‘
ğ‘/2
constant factors gives the desired result.
30E.2 Weak coresets
Proof of Theorem 5.2. Our hard instance is identical to the one of Theorem 5.1, except that each group has
ğœ€1âˆ’ğ‘/2ğ¶ğ‘ copies rather than ğœ€âˆ’ğ‘ copies.
ğ‘/2
Note that if S does not sample some row ğ‘–âˆˆ[ğ‘š], then the ğ‘–-th column of SB is all zeros, so the solution
obtained by the weak coreset is Xe =0, which has objective function value â€–Be â€–ğ‘ =ğ‘‘ğ‘. On the other hand,
ğ‘– ğ‘– ğ‘
the optimal value is at most (1âˆ’ğœ€)ğ‘ğ‘‘ğ‘ since we can set Xe =ğœ€AâŠ¤e so that
ğ‘– ğ‘–
ğœ€1âˆ’ğ‘ ğœ€1âˆ’ğ‘
â€–(AXâˆ’B)e â€–ğ‘ â‰¤(1âˆ’ğœ€)ğ‘ğ‘‘ğ‘+ Â·ğœ€ğ‘ğ‘‘ğ‘+ğ‘‘ğ‘/2 Â·ğ¶ğ‘ ğœ€ğ‘ğ‘‘ğ‘/2
ğ‘– ğ‘ 2ğ¶ğ‘ 2ğ¶ğ‘ ğ‘/2
ğ‘/2 ğ‘/2
ğœ€ ğœ€
â‰¤(1âˆ’ğœ€)ğ‘ğ‘‘ğ‘+ Â·ğ‘‘ğ‘+ Â·ğ‘‘ğ‘
2 2
â‰¤((1âˆ’ğœ€)ğ‘+ğœ€)ğ‘‘ğ‘
which is a (1+ğœ€) factor smaller for all ğœ€ sufficiently small. Thus, if nnz(S)â‰¤ğ‘š/2, then the solution X that
minimizes â€–S(AXâˆ’B)â€–ğ‘ must be at least an additive ğœ€ğ‘‘ğ‘Â·ğ‘š/2 more expensive than the optimal solution,
ğ‘,ğ‘
and thus it fails to be a (1+ğœ€/2)-optimal solution.
E.3 Spanning coresets
We generalize an argument of Section 4 of [DV06].
Lemma E.2. Let 1â‰¤ğ‘<âˆ and
{ï¸ƒ
1/6 ğ‘â‰¤2
ğ‘ =
ğ‘ 1/(6Â·5ğ‘/2âˆ’1) ğ‘>2
Then, there is a matrix A âˆˆ Rğ‘›Ã—(ğ‘›+1) such that for every ğœ€ â‰¥ 1/ğ‘› and any subset of ğ‘  â‰¤ ğ‘ ğœ€âˆ’1 rows, any
ğ‘
rank 1 subspace ğ¹â€² spanned by the ğ‘  rows must have
â€–AP âˆ’Aâ€–ğ‘ >(1+ğœ€) min â€–AP âˆ’Aâ€–ğ‘ .
ğ¹â€² ğ‘,2 ğ¹ ğ‘,2
rank(ğ¹)â‰¤1
Proof. Let ğ‘›â‰¤ğœ€âˆ’1 and let A be the ğ‘›Ã—(ğ‘›+1) matrix given by [ğ‘…Â·1 ,I ] for some large enough ğ‘…>0.
ğ‘› ğ‘›
That is, A is ğ‘… along the first column and the ğ‘›Ã—ğ‘› identity for the last ğ‘› columns. Note that the optimal
value is upper bounded by
ğ‘›((1âˆ’ğœ€)2+ğœ€2Â·(ğ‘›âˆ’1))ğ‘/2 =ğ‘›(1âˆ’2ğœ€+ğœ€2ğ‘›)ğ‘/2 =ğ‘›(1âˆ’ğœ€)ğ‘/2.
LetxâˆˆRğ‘ bethecoefficientsofalinearcombinationofğ‘ rowsofA. Wemayassumethecoefficientsarenon-
negative, since making the coefficients negative can only increase the cost. Note first that 1/2â‰¤â€–xâ€– â‰¤3/2
1
since otherwise
ğ‘›Â·|ğ‘…âˆ’ğ‘…â€–xâ€– |ğ‘ â‰¥ğ‘›Â·ğ‘…/2
1
which cannot be (1+ğœ€)-approximately optimal for ğ‘…â‰¥2.
The cost of the ğ‘–-th row is
(ï¸€
(1âˆ’x
)2+â€–xâ€–2âˆ’x2)ï¸€ğ‘/2 =(ï¸€
1âˆ’2x
+â€–xâ€–2)ï¸€ğ‘/2
. If â€–xâ€– â‰¥2, then
ğ‘– 2 ğ‘– ğ‘– 2 2
(ï¸€ 1âˆ’2x +â€–xâ€–2)ï¸€ğ‘/2 â‰¥(1âˆ’2â€–xâ€– +â€–xâ€–2)ğ‘/2 =(â€–xâ€– âˆ’1)ğ‘ â‰¥1
ğ‘– 2 2 2 2
so this cannot produce a (1+ğœ€)-approximately optimal solution. Thus, assume â€–xâ€– â‰¤2. Then,
2
(ï¸€
1âˆ’2x
+â€–xâ€–2)ï¸€ğ‘/2 =(ï¸€
1+â€–xâ€–2)ï¸€ğ‘/2(ï¸‚
1âˆ’
2
x
)ï¸‚ğ‘/2
â‰¥(ï¸€
1+â€–xâ€–2)ï¸€ğ‘/2(ï¸‚
1âˆ’
ğ‘
x
)ï¸‚
ğ‘– 2 2 1+â€–xâ€–2 ğ‘– 2 1+â€–xâ€–2 ğ‘–
2 2
so summing over the rows gives a cost of
(ï¸‚ )ï¸‚
(ï¸€ 1+â€–xâ€–2)ï¸€ğ‘/2 ğ‘›âˆ’ ğ‘ â€–xâ€– =(ï¸€ 1+â€–xâ€–2)ï¸€ğ‘/2 ğ‘›âˆ’ğ‘(1+â€–xâ€–2)ğ‘/2âˆ’1â€–xâ€–
2 1+â€–xâ€–2 1 2 2 1
2
31â‰¥(ï¸€ 1+â€–xâ€–2/ğ‘ )ï¸€ğ‘/2 ğ‘›âˆ’ğ‘(1+â€–xâ€–2)ğ‘/2âˆ’1â€–xâ€– since 1/2â‰¤â€–xâ€– â‰¤3/2
1 2 1 1
â‰¥(1+1/2ğ‘ )ğ‘/2ğ‘›âˆ’(3/2)ğ‘(1+â€–xâ€–2)ğ‘/2âˆ’1
2
â‰¥(1+ğ‘/4ğ‘ )ğ‘›âˆ’(3/2)ğ‘(1+â€–xâ€–2)ğ‘/2âˆ’1
2
{ï¸ƒ
(1+ğ‘/4ğ‘ )ğ‘›âˆ’(3/2)ğ‘ ğ‘â‰¤2
â‰¥
(1+ğ‘/4ğ‘ )ğ‘›âˆ’(3/2)ğ‘Â·5ğ‘/2âˆ’1 ğ‘>2
Thus, this fails to be a (1+ğœ€)-approximately optimal solution for
{ï¸ƒ
(3/2)ğ‘ ğ‘â‰¤2
(ğ‘/4ğ‘ )ğ‘›â‰¥
(3/2)ğ‘Â·5ğ‘/2âˆ’1 ğ‘>2
that is,
{ï¸ƒ
ğ‘›/6 ğ‘â‰¤2
ğ‘ â‰¤ .
ğ‘›/(6Â·5ğ‘/2âˆ’1) ğ‘>2
We now extend Lemma E.2 to a general rank ğ‘˜ lower bound.
Proof of Theorem 5.3. Let ğ‘›=ğœ€âˆ’1 and let B be a ğ‘˜ğ‘›Ã—ğ‘˜(ğ‘›+1) block diagonal matrix with the ğ‘›Ã—(ğ‘›+1)
matrix construction AâˆˆRğ‘›Ã—(ğ‘›+1) of Lemma E.2 on the block diagonal. Consider any set ğ‘† of ğ‘  rows of B,
and let ğ‘† denote the set of |ğ‘† | = ğ‘  rows supported on the ğ‘–-th block for each ğ‘– âˆˆ [ğ‘˜]. Let ğ¹ denote the
ğ‘– ğ‘– ğ‘– ğ‘–
optimal subspace spanned by the rows ğ‘† on the ğ‘–th block.
ğ‘–
Let ğ‘‡ âŠ†[ğ‘˜] denote the set of ğ‘–âˆˆ[ğ‘˜] such that ğ‘  â‰¤ğ‘ ğ‘›. If ğ‘–âˆˆğ‘‡, then we by Lemma E.2 that
ğ‘– ğ‘
(ï¸‚ )ï¸‚
ğ‘
â€–AP âˆ’Aâ€–ğ‘ > 1+ ğ‘ min â€–AP âˆ’Aâ€–ğ‘
ğ¹ğ‘– ğ‘,2 ğ‘  ğ‘– rank(ğ¹)â‰¤ğ‘˜ ğ¹ ğ‘,2
Then, the additive error from these rows is bounded below by
âˆ‘ï¸ğ‘
ğ‘ min â€–AP âˆ’Aâ€–ğ‘ â‰¥|ğ‘‡|Â·
ğ‘ ğ‘|ğ‘‡|
min â€–AP âˆ’Aâ€–ğ‘ AM-HM
ğ‘–âˆˆğ‘‡
ğ‘  ğ‘– rank(ğ¹)â‰¤ğ‘˜ ğ¹ ğ‘,2 âˆ‘ï¸€ ğ‘–âˆˆ[ğ‘˜]:ğ‘ ğ‘–â‰¤ğ‘ğ‘ğ‘›ğ‘  ğ‘– rank(ğ¹)â‰¤ğ‘˜ ğ¹ ğ‘,2
ğ‘ |ğ‘‡|
â‰¥|ğ‘‡|Â· ğ‘ min â€–AP âˆ’Aâ€–ğ‘
ğ‘  rank(ğ¹)â‰¤ğ‘˜ ğ¹ ğ‘,2
ğ‘ |ğ‘‡|2
â‰¥ ğ‘ min â€–BP âˆ’Bâ€–ğ‘
ğ‘˜ğ‘  rank(ğ¹)â‰¤ğ‘˜ ğ¹ ğ‘,2
Note that |ğ‘‡|â‰¥ğ‘˜/2 by averaging, so
ğ‘ |ğ‘‡|2 ğ‘ ğ‘˜
ğ‘ â‰¥ ğ‘ â‰¥ğœ€
ğ‘˜ğ‘  4ğ‘ 
which proves the theorem.
F Experimental evaluation
We show that empirically, we indeed see that the trade-off between the number of uniform samples and the
approximationqualityisindependentofthedimensionğ‘šinthesettingofEuclideanpowermeans. Wedothis
by plotting the sample size against the resulting relative error for ğ‘šâˆˆ{100,500}, where an ğ‘š-dimensional
dataset is constructed by sampling ğ‘š random features from the MNIST dataset. The results are shown in
Figure 1 and the experiment code is provided in Section F.1.
32Sample size vs relative error for 1-mean estimation
0.010
m=100
m=500
0.008
0.006
0.004
0.002
0.000
0 2000 4000 6000 8000 10000
Sample size
Figure 1: Sample size vs relative error for 1-mean estimation
F.1 Experiment code
We provide the code snippet for the experimental evaluation below.
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
np.random.seed(2024)
(train_X, train_y), (test_X, test_y) = mnist.load_data()
train_X = train_X.reshape(len(train_X), -1)
train_X = train_X / np.max(train_X)
n, d = train_X.shape
def power_mean_loss(train_ds, x, p=1):
x = np.expand_dims(x, axis=0)
x = np.repeat(x, repeats=n, axis=0)
e = train_ds - x
e = np.linalg.norm(e, axis=-1)
e = np.power(e, p)
return np.sum(e) / n
def run(train_ds, max_iter=200, p=1):
n, d = train_ds.shape
x0 = np.zeros(d)
x = tf.Variable(initial_value=x0)
opt = tf.keras.optimizers.Adam(learning_rate=0.5)
x.assign(x0)
def power_mean_loss_tf():
e = train_ds - x
e = tf.norm(e, axis=-1)
e = tf.math.pow(e, p)
33
rorre
evitaleRreturn tf.reduce_sum(e) / n
losses = []
while opt.iterations < max_iter:
opt.minimize(power_mean_loss_tf, var_list=[x])
loss = power_mean_loss_tf().numpy()
if np.isnan(loss):
print(x.numpy())
losses.append(loss)
return x.numpy(), losses
n, d = train_X.shape
sample_sizes = [100, 500, 1000, 5000, 10000]
for m in [100, 500]:
cols = np.random.choice(d, m)
train_m = train_X[:, cols]
x, losses = run(train_m)
OPT = losses[-1]
estimates = []
for sample_size in sample_sizes:
train_sample = np.random.choice(n, sample_size)
train_sample = train_m[train_sample, :]
x, losses = run(train_sample)
estimates.append(power_mean_loss(train_m, x))
relative_errors = [(e / OPT) - 1 for e in estimates]
print(â€™relative errorsâ€™, relative_errors)
34