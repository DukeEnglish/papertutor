Parrot: Multilingual Visual Instruction Tuning
Hai-LongSun1,2,3âˆ— Da-WeiZhou1,2 YangLi3 ShiyinLu3 ChaoYi1,2
Qing-GuoChen3 ZhaoXu3 WeihuaLuo3 KaifuZhang3 De-ChuanZhan1,2 Han-JiaYe1,2â€ 
1SchoolofArtificialIntelligence,NanjingUniversity
2NationalKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity
3AIBusiness,AlibabaGroup
Abstract
TherapiddevelopmentofMultimodalLargeLanguageModels(MLLMs)likeGPT-
4Vhasmarkedasignificantsteptowardsartificialgeneralintelligence. Existing
methodsmainlyfocusonaligningvisionencoderswithLLMsthroughsupervised
fine-tuning(SFT)toendowLLMswithmultimodalabilities,makingMLLMsâ€™in-
herentabilitytoreacttomultiplelanguagesprogressivelydeteriorateasthetraining
processevolves. WeempiricallyfindthattheimbalancedSFTdatasets,primarily
composedofEnglish-centricimage-textpairs,leadtosignificantlyreducedperfor-
manceinnon-Englishlanguages. Thisisduetothefailureofaligningthevision
encoderandLLMwithmultilingualtokensduringtheSFTprocess. Inthispaper,
weintroducePARROT,anovelmethodthatutilizestextualguidancetodrivevisual
tokenalignmentatthelanguagelevel. PARROTmakesthevisualtokenscondition
on diverse language inputs and uses Mixture-of-Experts (MoE) to promote the
alignmentofmultilingualtokens. Specifically,toenhancenon-Englishvisualto-
kensalignment,wecomputethecross-attentionusingtheinitialvisualfeaturesand
textualembeddings,theresultofwhichisthenfedintotheMoEroutertoselectthe
mostrelevantexperts. Theselectedexpertssubsequentlyconverttheinitialvisual
tokensintolanguage-specificvisualtokens. Moreover,consideringthecurrentlack
ofbenchmarksforevaluatingmultilingualcapabilitieswithinthefield,wecollect
andmakeavailableaMassiveMultilingualMultimodalBenchmarkwhichincludes
6languages,15categories,and12,000questions,namedasMMMB.Ourmethod
notonlydemonstratesstate-of-the-artperformanceonmultilingualMMBenchand
MMMB,butalsoexcelsacrossabroadrangeofmultimodaltasks. Boththesource
codeandthetrainingdatasetofPARROTwillbemadepubliclyavailable.
1 Introduction
TherapiddevelopmentofLargeLanguageModels(LLMs),suchasGPT-4[52;8;45;46;53;59],
hasgainedsignificantattention. However,LLMsarelimitedtoprocessingasingletextualmodality.
TheexpansionintovisualmodalitieshasendowedLLMswithmultimodalcapabilities[64;2;63;74;
17;27;32;65;71;7],therebyacceleratingthedevelopmentofMultimodalLargeLanguageModels
(MLLMs)andfurtherbringingusclosertotherealizationofArtificialGeneralIntelligence(AGI).
CurrentMLLMsmainlyrelyonpre-trainedLLMsandvisionencoders,focusingonbridgingthe
modalitygapbyaligningvisualfeatureswithlanguageembeddingtokens. Existingresearchpre-
dominantlyemployseitheraQ-Former[31;17;6]oranMLPprojector[38;35;12;34;37;42]to
alignvisionencoderswithLLMs. Thetrainingdata[38]mainlyconsistsofEnglish-centricdatafrom
âˆ—WorkdoneduringaninternshipatAIBusiness,AlibabaGroup.
â€ Correspondingauthor,email:yehj@lamda.nju.edu.cn.
Preprint.Underreview.
4202
nuJ
4
]VC.sc[
1v93520.6042:viXrağŸ¤– Chinese-CLIP+Qwen1.5-Chat7B ğŸƒ OpenAI-CLIP + Qwen1.5-Chat 7B
ğŸ‘¨ğŸ’» è¯·é—®å›¾ä¸­çš„äººæ‰‹ä¸Šæ‹¿äº†ä»€ä¹ˆä¸œè¥¿ï¼Ÿ å›¾ä¸­çš„äººæ‰‹é‡Œæ‹¿ç€ä¸€ä¸ªå¥–æ¯ã€‚ ğŸ¤–
å›¾ä¸­çš„äººæ‰‹é‡Œæ‹¿ç€ä¸€ä¸ªå¥–æ¯ã€‚ ğŸƒ
ğŸ‘¨ğŸ’» å›¾ä¸­æè¿°çš„æ˜¯ä»€ä¹ˆå†…å®¹ï¼Ÿ å›¾ä¸­æè¿°çš„æ˜¯ä¸€ä¸ªå¹´è½»ç”·å­åœ¨æ¯”èµ›åœºä¸Šï¼Œæ‰‹æŒå¥–æ¯ï¼Œ
åº†ç¥èƒœåˆ©ã€‚ä»–ç©¿ç€è¿åŠ¨æœè£…ï¼Œè‚Œè‚‰å‘è¾¾ï¼Œè¡¨ç°å‡ºäº† ğŸ¤–
èƒœåˆ©è€…çš„è‡ªè±ªå’Œå–œæ‚¦ã€‚
Thepersonintheimageisayoungmanwhoisstandinginastadium,holdingatrophy.Heis ğŸƒ
wearingaredandwhitesportsuniform,whichsuggestthathe is a member of a sports team.
ğŸ‘¨ğŸ’» å›¾ä¸­çš„äººåœ¨å¹²ä»€ä¹ˆï¼Ÿ è¿™ä¸ªäººåœ¨åº†ç¥èƒœåˆ©ï¼Œå› ä¸ºä»–æ­£åœ¨ä¸¾ç€ä¸€ä¸ªå¥–æ¯ã€‚ä»–å¯èƒ½
æ˜¯ä¸€ä¸ªè¿åŠ¨å‘˜ï¼Œå› ä¸ºå¥–æ¯é€šå¸¸ç”¨äºå¥–åŠ±èƒœåˆ©è€…ã€‚ä»–å¯èƒ½ ğŸ¤–
åœ¨åº†ç¥æ¯”èµ›èƒœåˆ©ï¼Œæˆ–è€…åœ¨åº†ç¥å›¢é˜Ÿçš„æˆå°±ã€‚
Theimageportraysamanwhoisstandinganarena,holdingatrophy. ğŸƒ
Figure1:TheoutputofOpenAI-CLIP-basedandChinese-CLIP-basedmodelsusingthesameChineseprompts.
WecanobservethattheOpenAI-CLIP-basedmodelexhibitsconfusionbetweenChineseandEnglishresponses.
imagecaptionsandmultimodalconversations. Duringtraining,thealignmentcomponentconverts
thevisualfeaturesintolanguageembeddingtokens. Withencodedvisualfeatures, theLLMcan
integratevisualinformationtorespondtomultimodalinputs.
MultilingualcapabilityinMLLMsentailstheabilitytogenerateresponsesinthesamelanguage
astheinput,cateringtothelinguisticdiversityinherentinconversation. Therefore,itiscritically
importantforprocessinglanguage-specificcontentandculturaldifferences,ensuringequitableaccess
totechnologicalbenefitsforindividualsacrossdiverseregionsandnations[13;24]. ManyLLMs
possessmultilingualcapabilities[60;5;46],enablingdiverselanguageresponsesaccordingtouser
input. However,afterthealignmenttrainingofMLLMs,themodelmayloseitsabilitytounderstand,
process,orgenerateinnon-Englishlanguages,andwecallthisphenomenonmultilingualerosion.
Forexample,LLaVA[38]usuallyrespondsinEnglish,regardlessoftheinputlanguage. Therefore,it
isessentialtoenhanceMLLMâ€™smultilingualcapabilitiesduringmultimodalalignment.
Themainreasonformultilingualerosionisthatthedatausedformultimodalalignmentishighly
imbalancedatthelanguagelevel. DuetothedominanceofEnglish-centricdata,whilethemodel
alignsvisualandtextualtokenswellinEnglish,itperformspoorlyinotherlanguages. Hence,it
iscrucialtoalignvisualandtextualtokenscompatiblyatthelanguagelevel. Wehypothesizethat
multilingualerosionmayarisefromthelackofalignmentbetweenvisualtokensandtextualtokens
inotherlanguages. Fromtheperspectiveofpre-traineddatasets,OpenAI-CLIP[51]istrainedonthe
large-scaleimage-textpairsthroughcontrastivelearning,withthetextcorpusbeingmostlyinEnglish,
potentiallybiasingimageencodingtowardsanEnglishsemanticspace.AsshowninFigure1,wetrain
twoseparatemodelsusingthesamedata: onewithOpenAI-CLIPvisionencoderandtheotherwith
Chinese-CLIP[62]visionencoder. Interestingly,themodelequippedwithOpenAI-CLIPstruggles
togeneratesuitableoutputsaccordingtoChineseinputswhiletheothermodelwithChinese-CLIP
cannotonlyunderstandthequeriesbutalsogenerateappropriateoutputsinChinese. Furthermore,
weobservedanimprovementinperformance,from66.4to68.3,ontheMMBench-CN[39]dataset
whenusingChinese-CLIP.Therefore,thechallengearises: howtouseEnglish-centricmultilingual
image-textdatatobridgethemodalitygapwhileenhancingtheMLLMâ€™smultilingualcapabilities.
Duetothescarcityofnon-Englishmultimodaldata(e.g.,lackoflarge-scale,high-qualityimage-text
data), we require almost the same amount of image-text data as LLaVA to enhance the modelâ€™s
multilingualcapabilities.Moreover,motivatedbypreliminaryexperiments,itisnecessarytocondition
thevisualtokensondiverselanguageinputs. Inthispaper,weintroducePARROT,anovelmethod
that utilizes textual guidance to drive visual token alignment at the language level and converts
visualtokensintolanguage-specificembeddingsusingaMixture-of-Experts(MoE)module[25;56].
Specifically,wefirstcalculatethecross-attentionbetweentheclasstokenofvisualfeaturesextracted
bythevisionencoderandthetextembeddingsderivedfromwordtokenembeddings. Theresult
is then passed through the router of MoE to obtain the activated probability distribution of each
languageexpert. Subsequently,demandingtheinputlanguage,theEnglish-biasedvisualtokensare
convertedintolanguage-specificembeddingsusingtheselectedexperts. ThisenablesPARROTnot
onlytoenhanceitsmultilingualcapabilitiesbutalsotobridgethemultimodalgapeffectively.
Toaddressthescarcityofcurrentmultilingualbenchmarks,weintroduceanewbenchmarkencom-
passingsixlanguages: English,Chinese,Portuguese,Arabic,Turkish,andRussian. Thisincludes
an extension of the MMBench-DEV dataset to these six languages and a Massive Multilingual
2MultimodalBenchmark(MMMB)featuring2,000evaluationquestionsperlanguage,totaling12,000
questions. Wecompareseveralopen-sourcemultimodalmethodsandevaluatesomeprivatemodels.
ExtensiveexperimentsvalidatethePARROTâ€™sstate-of-the-artperformanceacrosstwomultilingual
benchmarks. SpecificallyinTurkishandArabic,ourmethodevenoutperformsLLaVA-NeXT[37]
bymorethan10percentagepointsinbothbenchmarks. Additionally,weevaluateourmodelacrossa
broadrangeofmultimodalbenchmarks(e.g.,MME[22],ScienceQA-IMG[41],andSEED-Bench-
IMG[28]),demonstratingitscompetitiveperformanceindiversitytasks.
2 RelatedWork
MultimodalLargeLanguageModels. ThedomainofMLLMshaswitnessedsignificantadvances,
particularly in the enhancement of visual and language processing. Current MLLM is usually a
combinationofvisualencoders[51;57;21;69;48;67],LLMs,andfusionmodules. Innovationslike
Flamingo[2]andOpenFlamingo[4]haveadvancedvisualrepresentationbyintegratingaPerceiver
Resamplerwithvisionencoders. BLIP-2[31]andInstructBLIP[17]employQ-Formertoconnect
thefrozenLLMandvisionencoder. InternVL[14]trainshugeViTandQFormertointegratevisual
modalitiesthroughamulti-stagetrainingmethod. MiniGPT4[74]leveragesbothaQ-Formeranda
linearprojectortobridgethegapbetweenthevisionmoduleandLLM.Furthermore,LLaVA[38]
adopts a simple MLP projector to promote the alignment between the LLM and vision encoder.
mPLUG-Owl[64]introducesanapproachthatbeginstofinetunethevisionencoderandalignvisual
features, followed by tuning the LLM using LoRA [23]. Qwen-VL [6] improves visual module
resolutionto448,aimingtorefinethemodelâ€™svisualprocessingcapabilities. Fuyu-8B[7]directly
projectsimagepatchesbeforeintegrationwithLLM.MM1[43]hasconductedablativestudieson
connectordesignchoices,revealingthatthemodalityadaptertypeislesscriticalthanthenumberof
visualtokensandtheresolution. MiniGemini[34]utilizeshigh-resolutionvisualtokensandhigh-
qualitydatatonarrowtheperformancegapwithGPT-4andGemini. Withtherapidadvancements
inopen-sourcemodels,proprietarymodelssuchasGPT-4V/4o[46;47],Gemini[58;54],Qwen-
VL-Plus/MAX[6],andClaude3[3]haveachievedoutstandingresultsinevaluationsandpractical
applications. Inthiswork,owingtothesimplicityoftheLLaVAarchitecture,weadoptaframework
similartoLLaVAtodesignourmodel.
MultilingualMultimodalModels. Recentyearshavewitnessedrapidprogressintheexpansion
of multimodal models to include a wider variety of languages. M3P [44] leverages English as a
pivot and alternates between English-only vision-language pre-training and multilingual masked
languagemodeling. Incontrast,UC2[73]translatesEnglishcaptionsintovariouslanguagesanduses
imagesastheanchor. mCLIP[10]enhancestheCLIPmodelbyaligningitwithamultilingualtext
encoderthroughknowledgedistillation. Thankstotheexpansionoftheoverallcapabilitiesoflarge
languagemodels[1;5;26;66],theirmultilingualcapacitieshavesignificantlyimproved. Integrating
multilingualLLMswithvisualabilitieshasincreasinglybecomearesearchfocus. Inthedomainof
LLMs,PaLI[13]developsa17Bmultilinguallanguage-imagemodelthatspansover100languages.
Ying-VLM [33] discovers that instruction tuning in English can extend its applicability to other
languages. Ziya-Visual[40]illustratesthetranslationofEnglishimage-textdatasetsintoChinese,
usingin-contextlearningforinstruction-responsegeneration. VisCPM[24]introducesatraining
paradigmthatfine-tunestheMLLMinaquasi-zero-shotmannerbasedonastrongmultilinguallarge
languagemodel. Despitetheseadvancements,theyareprimarilyconfinedtotwolanguagesorrely
onthemassivetranslatedcorpus. Ontheotherhand,thereisnosuitablemultilingualbenchmark
forMLLMstoevaluatetheperformanceofmultiplelanguages. Therearealsosomemultilingual
researchstudiesinotherdomains,suchasmultilingualmachinetranslation[72;49;50;68].
3 MMMB:AMassiveMultilingualMultimodalBenchmark
Inthissection,wefirstdiscussthelimitationsofexistingbenchmarksandthenpresentthecharacter-
isticsthatanidealmultilingualbenchmarkshouldpossess. Furthermore,wedesignandconstructa
newbenchmarkandprovideitscorrespondingevaluationstrategy.
3Istheoutputofthecodeâ€˜â€˜b
islowerthan50â€™â€™? What is the capital of Michigan?
A. Pierre
B. Charleston
C. Grand Rapids
D. Lansing
a)Codereasoning b)Logicalreasoning c)Lowrelevancebetweenimageandtext
Figure2: Somebadcasesformultilingualbenchmarkperceive. Left: codereasoningisstronglyrelatedto
English.Middle:logicalreasoningistoochallenging.Right:lackrelevancebetweenimageandtext.
3.1 LimitationsofExistingBenchmarks
There are several existing multilingual benchmarks (e.g., Multi30K [20], M3Exam [70], MM-
Bench[39],andLLaVA-Bench[38;24])forMLLMs,buttheyhavesomelimitations: 1)Outdated
Benchmarks. Multi30kisdesignedforimage-textretrievaltasks,andtheperformancehasnearly
reachedtheupperboundduetotherelativelyeasyproblems. 2)Non-StandardizedEvaluations.
Otherbenchmarks,likeLLaVA-Bench,relyonevaluationsusingGPT-4. DependenceonGPT-4asa
defactoâ€œGroundTruthâ€mayhinderreproducibility. Meanwhile,sinceLLaVAusesadeprecated
version (GPT-4-0314), using other different versions could result in unfair comparisons. On the
otherhand,becauseM3Examdoesnotofferconsistenttestsamplesacrossdifferentlanguages,it
cannotensurewhetherpoorperformanceisduetotheproblemâ€™sdifficultyorthemodelâ€™slackof
multilingual capabilities. 3) Limited Languages. MMBench and LLaVA-Bench are limited in
EnglishandChinese,whichcannotmeasurethemultilingualcapabilitiesacrossabroadspectrum.
3.2 ConstructionoftheMultilingualBenchmark
TomoresuitablyevaluatethemultilingualcapabilitiesofMLLMs,anidealbenchmarkshouldexhibit
thefollowingcharacteristics:
1)LanguageswithSignificantDifferences. Itshouldcoveradiversearrayoflanguagefamilies,
selectinglanguagesthatareasdistinctandnon-repetitiveaspossible.Thisensuresabroadassessment
ofMLLMsâ€™abilitytoadaptacrosslinguisticvariances.
2)ProblemswithMediumLevelofDifficulty. Theproblemsshouldnotbetoodifficult(e.g.,logical
reasoning)becausetheaimistoassessthemultilingualunderstanding,processing,andgenerating
capabilitiesofMLLMs,notlogicalreasoningskills.
3)TaskswithMultilingualandMultimodal. AsshowninFigure2,datawithindatasetsshouldnot
bestronglyrelatedtoEnglish(e.g.,codereasoning). Itcannotbeinherentlytransformedintomultiple
languagessincetheyarecomposedofEnglishwords. Moreover,imagesshouldbeanindispensable
partwhenMLLMsanswerthequestion. Forinstance,ifgivenamapoftheUnitedStatesandasked
toidentifyitscapital,MLLMsonlyrequirethetext-onlyabilitytoanswerthisquestion. Therefore,it
isessentialthatquestionshighlightasignificantcorrelationbetweenimagesandtexts.
4)ContentConsistencyacrossLanguages.Thegoalofthisbenchmarkistoevaluatethemultilingual
capabilitiesofMLLMs,andweaimtoshowthediscrepanciesacrossdifferentlanguagesfairly. For
example,ifEnglishquestionsmainlyfocusonadditionwithinonehundredwhileChinesequestions
mainlyconcerncalculuscomputation,itbecomesdifficulttoascertainwhetherpoorperformancein
ChinesearisesfromthecomplexityoftheproblemorthelimitedmultilingualcapabilitiesofMLLMs.
Hence,itiscrucialtoensurecontentconsistencyacrosslanguagesforafaircomparison.
Weselectedsixlanguagesforinclusion: English(en),Chinese(zh),Portuguese(pt),Arabic(ar),
Turkish(tr),andRussian(ru).Theselanguagesrepresentadiverserangeoflinguisticfamilies,andwe
listthedetailedinformationandsomemultilingualcasesinFigure3. Intermsofdatasetrequirements
andconsistency,ourbenchmarkincorporatesdatasetsintwomainrespects: 1)SinceMMBench[39]
officiallyincludesEnglishandChineseversions,weextendittotheotherfourlanguages. 2)Forthe
massivemultilingualmultimodalbenchmark,denotedasMMMB,weselectandcleanthesuitable
datafromScienceQA[41],MME[22],andSEED-Bench[28]datasetswithestablishedguidelines.
ThesedatasetsarethenprocessedintoaVisualQuestionAnswering(VQA)format,resultingina
totalof12,000samplesacrossallsixlanguages.
4Can you see any clouds in the sky? Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ±ĞµĞ»Ñ‹Ñ… ÑÑ‚ÑƒĞ»ÑŒĞµĞ² Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞµ? Qual das seguintes opÃ§Ãµes estÃ¡ na terceira
A B.. N Tho e, ri et' s i sc no om sp kle yt e vl iy s ic bl le ear A B..1 2 c Ao .l au n ba i? blioteca
C D. . Y Ye es s, , a bn ud t tt hh ee yy aa rr ee sd pe an rs se e C D. .3 4 B C. .o o r de es pt aa ru tr aa mnt ee nto de polÃ­cia
English Russian D.a mercearia Portuguese
A BØŸï»² Ø§Ø±ï»¨ .Ù‹ .ï»¨ ïºï»œ ï»»ï»¢ïº´ï»¤ ï»Œï¯¾ï¯¾ ï»§ Ùï»Ú¾ ïº ï»„ØŒï»Š ï»Œï»§Ø· ï»»ïº Ø§ï»˜ïº˜ï»ŸØ§ ï»²ï»“ ïº”ïº˜ï»“ï»» ïºªïºŸï»®ï¯¾ ï»Ú¾ å›¾ A B C. ..ä¸­ é»‘ æ£•ç°æ‰€ è‰² è‰²è‰²ç¤º çš„ çš„çš„çš„é•¿å‡³æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ G t A B CÃ¼ . .Ã¶ .r r i YSBs ne Ã¶ Ã¼s al ra l ze fÄ±n
Ã¼
Ã§kg l cÃ¼Ã§aÃ¶ Ã¼lÄ±r elr
l
ae g err rÃ¶s ru Ã¼d la Ã¼ yÃ¶ on rc ?elikli olarak ne
Arabic D. ç™½è‰²çš„ Chinese D.TÃ¼plÃ¼ dalgÄ±Ã§lar Turkish
English Chinese L Ea nn gg lu ia sg he L Ia nn dg ou -a Eg ue rof pam eai nly Lan Gg eu ra mge ang ir coup Geography
Maps
126 ,0L 0a 0ng su aa mg pe ls
es
P ortuguese PoC Arh
t
rui an
g
be
u
is cee
se
InS Adi fn
o
ro
-
oE- -T Aui
r
sb
o
ie apt tea
ia
cn
n R
SS
o
ei
m
mni
a
it
n
ti ic
c ce 15categoriesÂ·Â·Â·
Arabic Turkish RTu ur sk sii ash
n
IndoA -Elt ua ri oc
pean
T Su lar vk ii cc Count Color
Figure3:OverviewofMMMB.Itincorporates6languages,15categories,and12,000questions.
3.3 EvaluationStrategy
Sincerandomguessingcanleadtoâˆ¼25%Top-1accuracyfor4-choicequestions,potentiallyreducing
the discernible performance differences between various MLLMs. Additionally, MLLMs may
prefer to predict a certain choice among all given choices [39], which further amplifies the bias
in evaluation. To address these issues, we implement a circular validation strategy inspired by
MMBench. Specifically,MMMBisadaptedtotheformatofYes/Noquestions,whereeachimageis
pairedwithtwoquestions,demandingâ€˜Yesâ€™andâ€˜Noâ€™answers,respectively. AsshowninFigure8,
ananswerisconsideredaccurateonlyifbothquestionsareansweredcorrectly;failingeitherresults
in marking the entire instance as incorrect. This strategy ensures a more rigorous evaluation of
MLLMs,reducingthelikelihoodofrandomguessingandpromotingmorevalidatedcomparisons
acrossdifferentmodels.
4 Methods
4.1 Preliminaries: VisualInstructionTuning
ArepresentativeworkinMLLMsisLLaVA[38],whichintroducesasimpleyeteffectivemethodfor
achievingalignmentbetweenthevisionencoderandthepre-trainedLLM.Specifically,foragiven
inputimageX ,LLaVAutilizesthepre-trainedCLIPvisionencoderViT-L/14[51]toextractthe
v
visualfeaturesZ =g(X ).ItthenemploysVicuna[15]astheLLMtoobtainthetextualembeddings
v v
H . ToalignthevisionencoderwiththeLLM,aprojectorintheformofamulti-layerperceptron
t
(MLP)denotedasWislearned. ThisprojectorconvertsZ intolanguageembeddingtokensH ,
v v
effectivelyfacilitatingtheintegrationofmultimodalinformationwithintheLLMâ€™sframework.
H =WÂ·Z , with Z =g(X ). (1)
v v v v
Finally, we input H and H into LLM to generate the modelâ€™s responses. However, after the
v t
modalityalignmenttraining,LLaVAlosesitsabilitytoprocessinnon-Englishlanguages.
4.2 PilotStudy
To address the challenge of multilingual erosion in MLLMs due to the dominance of English in
image-textdata,wehypothesizethatthereisaninherentmismatchbetweenvisualtokensH and
v
textualtokensH ,whichtendstobiasthemtowardsEnglishsemantics,makingthemodelmorelikely
t
togenerateoutputsinEnglish. Specifically,thewidely-usedvisionencoderofOpenAI-CLIP[51]is
pre-trainedonavastcorpusofEnglish-centricimage-textpairs,resultinginvisualrepresentations
morealignedwithEnglish.
Toexplorethisphenomenon,wetraintwodistinctmodelsusingthesamedata: oneincorporating
OpenAI-CLIPvisionencoderandtheotherintegratingChinese-CLIP[62]visionencoder. Asshown
inFigure1,themodelequippedwithOpenAI-CLIPstrugglestogeneratesuitableoutputsaccording
5
naissuR setubirttA1.The animal in the picture is a rabbit Pre-training
2.å›¾ä¸­çš„åŠ¨ç‰©æ˜¯ä¸€åªå…”å­
3. Resimdeki hayvan bir tavÅŸandÄ±r. Weighted-Sum LLM
MoE
LargeLanguageModel
MLP1 MLP2 MLP3 Â·Â·Â· MLPn Projector
Â·Â·Â· Â·Â·Â· VisionEncoder
MultilingualMoE
InstructionTuning
Linear
Cross-Attention LLM
SiLU
Projection WordEmbedding Router Linear MoE
Vision MLPExpert Projector
1.What animal is in the picture?
Encoder 2.å›¾ç‰‡ä¸­çš„åŠ¨ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ MultilingualMoEblock VisionEncoder
3. FotoÄŸraftaki hayvan nedir?
Figure4:TheoverallarchitectureofPARROT.ItconvertsEnglish-biasedfeaturestolanguage-specificfeatures
basedonthemultilingualMoEmodule,aimingtoimprovethemultilingualcapabilities.Thetrainingdetails
withineachstagearepresentedontheright.
totheChineseinputs,whereasthemodelusingChinese-CLIPnotonlyunderstandsthequeriesbut
alsogeneratesappropriateoutputsinChinese. Moreover,tofurtherevaluatethemodelâ€™sChinese
capability,wetestitonChinesedatasetsandobserveimprovedperformance: from66.4to68.3on
MMBench-CNandfrom62.4to66.1onMMMB-zhwhenutilizingChinese-CLIP.
4.3 TextualGuidancetoDriveVisualTokenAlignment
Due to the low-resource nature of non-English multimodal data (e.g., lack of large-scale, high-
qualityimage-textdata),weneedtousenearlythesameamountofdataasLLaVAtoenhancethe
modelâ€™smultilingualcapabilities. Furthermore,motivatedbytheseinterestingfindingsandaiming
toalignvisualtokenswithtextualembeddingsatthelanguagelevel,weproposePARROT,anovel
approachthatleveragestextualguidancetofacilitatethemultilingualalignmentofvisualfeatures.
PARROTenablesthetransitionofEnglish-biasedvisualfeaturesacquiredthroughtheOpenAI-CLIP
toaccommodateotherlanguages. Thisapproachensuresthatitcanprovidelanguage-specificvisual
tokenstoLLMbasedonthemultiplelanguageinputs,therebyenhancingitsmultilingualcapabilities.
First, we extract visual features through the vision encoder and transform them into language
embedding tokens H using a projector. We obtain the embeddings H âˆˆ RNÃ—C derived from
v t
text inputs via the word embedding table. Subsequently, to convert the English-biased features
into language-specific features using textual guidance, we employ a cross-modal cross-attention
mechanismtoobtainHâ€² âˆˆRC:
v
(cid:18) HclsHT(cid:19)
Hâ€² =Attention(Q,K,V)=Softmax âˆšv t H , (2)
v t
C
whereQequalsthematrixH ,KandVareequivalenttoH . Hcls âˆˆRC isthe[CLS]tokenofH .
v t v v
Thisprocessallowsthevisualfeaturestobedynamicallyadjustedbasedonthemultilingualinputs.
SincetheprojectedlanguageembeddingtokensH areEnglish-biased,weneedtoconvertthem
v
tolanguage-specificembeddingsfordifferentlanguages. Tothisend,weintroducealightweight
Mix-of-Experts(MoE)module,whichincludesarouterandseverallanguagetransformationexperts.
TherouterofMoEisalinearlayerthatgeneratesaprobabilitydistributionoverthesetofexperts
E =[e ,e ,Â·Â·Â· ,e ],effectivelypredictingtheprobabilityofselectingandactivatingeachexpert.
1 2 E
Each expert is an MLP designed to convert English-biased embeddings into language-specific
embeddings. TheinputstoexpertsE isH ,andtheoutputshavethesamedimensionsastheinputs.
v
Subsequently,toobtainanormalizedprobabilitydistributionforactivatinglanguage-specificexperts,
Hâ€² is fed as input to the router. The router network contains a linear layer that computes the
v
normalizedweightmatrixusingHâ€² forvoting,producingP âˆˆRE:
v
P =Softmax(Linear(Hâ€²)), (3)
v
whichselectsandactivatesthespecificexperts. Moreover,weprocesstheEnglish-biasedembeddings
H throughtheselectedexpertstoconvertthemintolanguage-specificvisualrepresentations:
v
k
(cid:88)
MoE(H )= P[i]Â·E(H ) . (4)
v v i
i=1
6ThisapproacheffectivelyalignsEnglish-biasedembeddingswithmultiplelanguages,ensuringamore
accurateandcomprehensiverepresentationacrossdifferentlinguisticcontexts. Tostabilizetraining
andconvertvisualembeddingswithlessvarianceinvisual-semanticinformation,ensuringthemodel
performswellintasksbeyondthemultilingualmultimodaldomain,weutilizeMoEreweightingto
obtainthefinallanguage-specificvisualembeddingsG :
v
G =H +Î±MoE(x), (5)
v v
whereÎ±isthetrade-offparameter. Inconclusion,wefirstfusethevisualandtextualinputsviaEq.2
totransformthevisualembeddingswithtextualguidance. Moreover,thefusedresultisinputtedinto
theMoEmoduletoselectandactivatethemostrelevantlanguageexpertsviaEq.3andthenobtain
thelanguage-specificembeddingsviaEq. 4. Lastly,weemployMoEreweightingtoconvertvisual
embeddingswithlessvarianceinoriginalvisual-semanticinformation5. Thisapproachenablesusto
endowtheMLLMwithmultilingualcapabilitiesusingaslittlemultilingualdataaspossible. Figure4
illustratesthearchitecture,thedetailedMoEmodule,andthetrainingstagesofPARROT.
4.4 TrainingStage
Our goal is to utilize as little multilin- Table1: Detailsonthe PARROTâ€™strainingdata, derivedfrom
gualdataaspossibletoenhancethemul- publiclyavailabledatasetsandourin-housemultilingualdata.
tilingual capabilities of MLLMs. The
wholetrainingprocedureisdividedinto TrainingStage Datasets Samples Total
twodistinctstages:
LLaVA-1.5-pretrain[38] 558K
Stage1 Laion-Captionâˆ—[55] 12K 1.2M
Stage1: ModalityAlignment. Inthis
CC12M-Captionâˆ—[9] 645K
stage,wekeepboththevisionencoder
andtheLLMweightsfrozen,focusing LLaVA-1.5-finetune[38] 665K
solely on optimizing the projectors to ShareGPT4V-zhâˆ—[12] 71K
align the visual features H v with the Stage2 S Sh ha ar re eG GP PT T4 4V V- -p art âˆ—âˆ— [[ 11 22 ]] 11 24 KK 793K
pre-trainedLLMwordembedding. This
ShareGPT4V-trâˆ—[12] 17K
stage can be likened to training a vi-
ShareGPT4V-ruâˆ—[12] 14K
sual tokenizer that is compatible with
thefrozenLLM.Toenhancethediver-
sityofimages,weextractaportionofdatafromLAION[55]andCC12M[9]datasetsandconstruct
thein-housecaptiondatathroughGPT-4V.
Stage2: InstructionTuningforMultilingualAlignment. Westillkeepthevisionencoderweights
frozenwhilecontinuingtotraintheprojector,MoE,andLLM.DuetothedesignoftheMoEmodule,
PARROTcanrapidlylearntoalignvisualrepresentationsacrossmultiplelanguagesbyusingasmall
amount of multilingual image-text data. As shown in Table 1, we only use nearly 10K training
dataforeachlanguageinstage2. Thisapproachisparticularlybeneficialgiventhescarcityofdata
resourcesinlow-resourcelanguages.
Toaddressthechallengeoflimiteddatainnon-Englishlanguages,weuseasemi-automaticapproach
toacquireimage-textdata. Initially,wepartitiontheShareGPT4Vdataset[12]randomlyforeach
language,extractingaselectionofnon-duplicate,non-parallelimage-textdatafortraining. Subse-
quently,weimplementatranslationandcalibrationschemeusingGPT-4toconvertEnglishtextsinto
textsofotherlanguages. Recognizingthatthisstepmayintroducenoiseandpotentialtranslation
errors,weapplyamanualcalibrationprocesstofurtherfine-tuneandcleanthedata,therebyobtaining
high-qualitymultilingualimage-textdata.
Thistwo-stagetrainingapproachensureseffectivemodalityandmultilingualalignment,evenwith
limitedNon-Englishdata,aligningwellwiththerealitiesofdatascarcityinlow-resourcelanguages.
5 Experiments
Inthissection,webeginwithanoverviewoftheexperimentalframework,providingdetailsonspecific
implementations,evaluationbenchmarks,andMLLMsusedforcomparativeevaluation. Following
this, we conduct a comprehensive comparison of PARROT with the state-of-the-art approaches
usingmultilingualbenchmarks. Additionally,wecomparePARROTwithleadingmodelsacrossa
rangeofmultimodaltasks. Finally,thissectionconcludeswithablationstudiesandvisualizationof
multilingualcases,highlightingtheexceptionalabilityofPARROTinhandlingmultilingualtasks.
75.1 ExperimentalSetup
Implementation Details: In this study, we configure PARROT with the pre-trained CLIP ViT-
L/14[51]asthevisionencoderandQwen1.5-Chat[5]asthebackboneforLLM.Theinitiallearning
ratesforthetwostagesaresetat1eâˆ’3 and2eâˆ’5,respectively,withthebatchsizeof256and128.
Theentiretrainingprocessisnotablyoptimizedto21hoursonthe16Ã—A100GPUssetup,attributed
totheuseoftherelativelysmalltrainingdatasets. Additionally,BF16andTF32precisionformatsare
employedtometiculouslybalancespeedandaccuracythroughoutthetrainingprocess. Asdefinedin
Eq.4,wesetthenumberofexpertstosixtomatchthenumberoflanguages. EachexpertisanMLP
composedoftwolinearlayerswithSiLU[19]activationfunction. MoredetailsareshowninTable5.
EvaluationBenchmark: Ourevaluationisdividedintotwoparts: oneevaluatesthemultilingual
capabilitiesofMLLMs,whiletheotherassessesitsoverallperformance. Thefirstevaluationisper-
formedontwodatasets: MMBench[39]andanewlydevelopedbenchmarkMMMB.ForMMBench,
weexpandittoincludesixlanguagesthroughtranslationviaGPT-4,followedbymanualverification.
For MMMB, we construct it following the methodology described in Section 3. We present the
accuracyforeachlanguageinTable2. Furthermore,thesecondevaluationcoversawidebroadrange
ofmultimodaltasks,suchasMME[22],MMStar[11],ScienceQA[41],RealWorldQA[61]and
SEED-Bench[28],withperformancereportedusingaradarchartinFigure5b.
Comparison Models: For comprehensive comparisons, we select leading open-source models
in MLLMs, including LLaVA-1.5 [29], LLaVA-NeXT [37], Qwen-VL [6], Monkey [35], Visu-
alGLM [18], VisCPM [24], MiniGPT-4-v2 [74], shareGPT4V [12], InstructBLIP [17], mPLUG-
Owl2[64],Mini-Gemini[34].Furthermore,weincorporateclosed-sourcemethodsinourbenchmarks,
includingGPT-4V[12],Qwen-VL-MAX[6],andGeminiPro[54],todemonstratetheirremarkable
performance.Fortheevaluationprocess,weemploytheVLMEvalKitinOpenCompass[16],ensuring
consistentconfigurationsettingsacrossallmethodstomaintainfairnessincomparison. Formostof
thementionedmethods,wedirectlyusetheVLMEvalKitimplementation. Alternatively,weintegrate
othermethodsnotofficiallyprovidedintothisframeworktoensureconsistencyinevaluation.
5.2 MainResults
In this section, we present the results of the multilingual experiment in Table 2 and the overall
experiment in Figure 5b. According to Table 2, PARROT-14B achieves state-of-the-art (SOTA)
performanceinalllanguagesontheMMBenchbenchmarkandalsoachievestheSOTAperformance
infivelanguagesontheMMMBbenchmark,withEnglishinthesecondplace. Themultilingual
capabilities of PARROT-14B closely reach that of GPT-4V, demonstrating the exceptional ability
ofourproposedarchitecture. Notably,PARROT-7BalsovalidatestheSOTAperformanceonboth
benchmarksacrossalllanguages,evensurpassingtheLLaVA-NeXT-13B.Additionally,asshown
inFigure5b,thisevaluationaimstoshowthatPARROTnotonlypossessesexcellentmultilingual
capabilitiesbutalsoprovidesanoverallunderstandingofPARROTâ€™scapabilitiesinhandlingvarious
complexmultimodaltasks(e.g.,MME[22],MMStar[11],andSEED-Bench[28]). Additionally,
asdepictedinFigure5c,wevisualizetheexpertdistributionswithintheMoE.Itisevidentthatthe
secondexpertispredominantlyactivatedwhenusingtheChineseprompt,indicatingthatdifferent
expertsareutilizedforvariouslanguageprompts. Inexistingmultilingualbenchmarks, PARROT
alsodemonstratescompetitiveperformancewhileusinglessthan1%ofthedatacomparedtoother
multilingualMLLMs,asillustratedinFigure6.
5.3 AblationStudy
Ablationstudyoneachcomponent. Weconductanablationexperimentonthemultilingualdata
and the MoE module. As shown in Figure 5a, using multilingual data improves performance in
eachlanguage. Moreover,theMoEmodulesignificantlyimprovesperformance,demonstratingthe
effectivenessofourproposedmethod.
Ablationstudyondifferentdatasets.AsshowninTable3,itisevidentthattheinclusionofdifferent
multilingualdatasetscontinuallyimprovesperformanceontheMMBenchbenchmark,andallmodels
with7Bparametersareusedforthisexperiment. Thishighlightstherobustnessandscalabilityofour
approachtohandlingmultiplelanguageseffectively.
8Table2:Accuracyperformancecomparisononmultilingualbenchmarks.Wereportallcomparedmethodswith
VLMEvalKit[16].Thebestandsecondresultsareshowninboldandunderline,respectively.
MMMB MMBench
Method LLM
en zh pt ar tr ru en zh pt ar tr ru
Open-sourcemodels
LLaVA-1.5[36] Vicuna-7B 67.07 58.83 59.76 43.50 46.43 59.06 65.37 58.33 59.02 36.16 43.90 56.95
LLaVA-1.5[36] Vicuna-13B 69.76 62.86 60.76 45.49 54.44 62.69 68.98 63.23 62.97 46.56 53.17 61.59
LLaVA-NeXT[37] Vicuna-7B 70.87 61.57 61.81 42.74 46.95 63.85 67.95 60.56 60.39 38.40 45.36 59.62
LLaVA-NeXT[37] Vicuna-13B 74.44 67.19 63.21 45.36 53.09 68.24 70.87 64.51 64.08 45.36 52.92 61.85
Qwen-VL[6] Qwen-7B 52.63 36.37 38.65 36.54 37.42 40.70 42.26 22.25 25.08 18.72 26.37 28.17
Qwen-VL-Chat[6] Qwen-7B 56.02 57.77 46.37 43.04 41.05 48.65 54.29 56.52 43.12 35.73 39.17 42.86
MiniGPT-4-v2[74] LLaMA2-13B 38.71 30.05 31.52 26.60 26.02 29.23 23.88 11.76 14.26 2.49 6.78 12.54
ShareGPT4V[12] Vicuna-v1.5-7B 69.24 60.23 60.29 43.57 45.26 61.23 69.59 61.6 59.62 37.37 43.38 59.45
InstructBLIP[17] Vicuna-7B 39.47 32.92 35.67 23.80 28.36 36.37 27.83 18.81 27.14 3.26 8.50 20.87
mPLUG-Owl2[64] LLaMA2-7B 67.25 60.99 59.70 45.78 45.43 62.63 66.15 59.36 58.24 37.88 47.68 60.39
Monkey[35] Qwen-VL-7B 66.02 58.18 46.31 38.83 37.66 48.59 58.07 53.52 49.57 31.01 31.35 45.18
Monkey-chat[35] Qwen-VL-7B 71.63 66.54 60.35 48.77 46.31 58.59 70.79 65.72 65.03 46.90 48.10 59.36
VisualGLM[18] ChatGLM-6B 31.05 18.07 19.42 15.38 22.81 19.77 23.2 17.18 11.43 2.92 6.62 5.33
VisCPM-Chat[24] CPM-Bee-10B 53.10 47.54 28.19 26.90 26.78 26.84 45.88 46.39 15.81 1.46 9.19 1.20
PARROT Qwen1.5-7B 70.00 68.13 67.31 62.69 58.01 66.26 70.70 70.36 65.12 57.82 58.43 64.00
PARROT Qwen1.5-14B 73.92 71.64 69.82 68.13 64.33 70.18 74.40 72.25 69.16 66.15 64.52 69.33
Closed-sourcemodels
GPT-4V[46] Private 74.97 74.21 71.46 73.51 68.95 73.10 77.60 74.40 72.51 72.34 70.53 74.83
GeminiPro[58] Private 75.03 71.87 70.64 69.94 69.59 72.69 73.63 72.08 70.27 61.08 69.76 70.45
Qwen-VL-MAX[6] Private 77.19 75.26 72.16 70.82 66.02 74.21 76.80 77.58 74.57 75.00 69.07 75.00
MMBench-CN
70 Baseline
w/ Multilingual Data
w/ Multilingual Data and MoE RealWorldQA MME
Layer 2
65
60
Layer 1
55 SQA-IMG SEED-IMG
50 en zh pt ar tr ru MMStar 0% 25% 50% 75% 100%
Mini-Gemini7B LLaVA-NeXT7B Monkey9.8B Expert 1 Expert 2 Expert 3
Languages Qwen-VL-Chat7B mPLUG-Owl2 7B Parrot7B(Ours) Expert 4 Expert 5 Expert 6
(a)Ablationstudy. (b)Multiplemultimodaltasks. (c)Expertdistributions.
Figure5:Left:TheablationstudyofmultilingualdataandtheMoEmoduleusingtheMMBenchbenchmark.
Middle:TheperformanceofPARROTonabroadrangeofmultimodaltaskscomparedwithexistingmodels.
Modelswith7Bparametersareusedforthetwoexperiments.Right:ExpertdistributionsofMoE.Wesummarize
theactivatedexpertsduringthefeed-forwardprocessusingChinesePrompts.
Ablation study on monolingual fine-tuning datasets. The ablation study presented in Table 9
evaluates the performance of different monolingual datasets added incrementally to the baseline
dataset LLaVA-1.5-finetune. It highlights the significant impact of adding different multilingual
datasets to a baseline model. Each dataset incrementally improves performance in its respective
languageand,whencombined,leadstooverallenhancedperformanceacrossallevaluatedlanguages.
Thisindicatestherobustnessandeffectivenessoftheproposedmethodinhandlingmultilingualdata,
makingitascalablesolutionformultilingualtasks.
5.4 VisualizationofMultilingualConversations
To enhance the intuitive understanding of the PARROTâ€™s multilingual capability, we prepare a
comprehensivecasestudyaccompaniedbyillustrativevisuals. Forinstance,asdepictedinFigure6,
ourframeworkdemonstratesremarkablemultilingualcapabilities. ThisunderscoresthePARROTâ€™s
versatility in navigating different languages and presents its potential in bridging linguistic gaps
across diverse domains. Through careful analysis and visualization, we aim to provide a deeper
insightintothemechanismdrivingthiscapability,illustratingitspracticalimplicationsandpotential
applicationsinreal-worldscenarios. ThisvisualizationservesasastrongindicatorofthePARROTâ€™s
solidarchitectureanditsexceptionalabilitytounderstand,process,andgeneratemultiplelanguages
withremarkableefficiency. MoremultilingualconversationcasesareshowninAppendixE.
9
)%(
ycaruccATable3:AblationstudyondifferentmultilingualtrainingdatasetsinMMBenchbenchmark.Modelswith7B
parametersareusedforthisablation.
Dataset English Chinese Portuguese Arabic Turkish Russian
LLaVA-1.5-finetune 69.4 66.6 60.3 55.3 52.1 60.7
+zh 69.2 -0.2 68.6 +2.0 64.1 +3.8 59.1 +3.8 50.9 -1.2 61.6 +0.9
+zhpt 71.1 +1.7 70.4 +3.8 65.4 +5.1 57.9 +2.6 52.1 +0.0 62.9 +2.2
+zhptar 71.0 +1.6 68.6 +2.0 65.7 +5.4 58.6 +3.3 52.2 +0.1 62.2 +1.5
+zhptartr 70.4 +1.0 68.7 +2.1 64.9 +4.6 61.2 +5.9 59.7 +7.6 62.0 +1.3
+zhptartrru 70.7 +1.3 70.4 +3.8 65.1 +4.8 57.8 +2.5 58.4 +6.3 64.0 +3.3
Arabic:ØŸØ©Ø±ï»®ïº¼ï»ŸØ§ ï»²ï»“ ïººïº¨ïº¸ï»ŸØ§ ïºï®­ïº³Ø±ïºï»¤ï¯¾ ï»²ïº˜ï»ŸØ§ ïº”ïº¿ïºï¯¾ïº®ï»ŸØ§ ï»²Ú¾ ïºï»£
[Arabic-EnglishTrans:What sport is the person in the picture doing?]
Output: .ïº²ï»¨ïº˜ï»ŸØ§ ïºï»Œï» ï¯¾ Ø©Ø±ï»®ïº¼ï»ŸØ§ ï»²ï»“ ïººïº¨ïº¸ï»ŸØ§
[Arabic-EnglishTrans:The person in the picture is playing tennis.]
Turkish:Sporcu hangi etkinliÄŸi kazandÄ± ve gÃ¶rsele gÃ¶re kaÃ§ Grand Slam ÅŸampiyonluÄŸu kazandÄ±?
[Turkish-EnglishTrans:What event did the athlete win and how many Grand Slam titles has the athlete won according to the image?]
Output:Sporcu, 2023 ABD AÃ§Ä±k'Ä± kazandÄ± ve gÃ¶rsele gÃ¶re 24 Grand Slam ÅŸampiyonluÄŸu kazandÄ±.
[Turkish-EnglishTrans:The athlete won the US Open 2023, and according to the image, he has won 24 Grand Slam titles.]
Portuguese:Descreva as emoÃ§Ãµes ou expressÃµes capturadas no rosto do atleta na imagem. O que isso sugere sobre os sentimentos dele em relaÃ§Ã£o Ã  sua vitÃ³ria?
[Portuguese-EnglishTrans:Describe the emotions or expressions captured in the athlete's face in the image. What does this suggest about their feelings regarding their victory?]
Output:O jogador de tÃªnis, que estÃ¡ em pÃ© e segurando uma raquete de tÃªnis, parece estar expressando felicidade e orgulho em relaÃ§Ã£o Ã  sua vitÃ³ria. Ele estÃ¡ sorrindo, o que
sugere que ele estÃ¡ animado e feliz com o resultado de seu esforÃ§o. A imagem captura um momento de realizaÃ§Ã£o e conquista, com o jogador no centro do quadro, destacado
por uma cor vibrante e uma fonte proeminente que diz "CampeÃ£o". No geral, a imagem transmite uma sensaÃ§Ã£o de orgulho, realizaÃ§Ã£o e realizaÃ§Ã£o.
[Portuguese-EnglishTrans:The tennis player, standing and holding a tennis racket, appears to be expressing happiness and pride about his victory. He is smiling, which
suggests he is excited and pleased with the outcome of his effort. The image captures a moment of achievement and conquest, with the player at the center of the frame,
highlighted by a vibrant color and a prominent font that says "Champion". Overall, the image conveys a sense of pride, accomplishment, and fulfillment.]
English:How are the overall colors and composition of the image?
Output:The image features a painting of a deer and a swan standing near a river. The deer is positioned on the right side of the
painting, while the swan is on the left side. The river is surrounded by lush greenery, with many different types of flowers
scattered throughout the scene. The painting has a vibrant and colorful composition, with various shades of green, blue, and
white. The painting is done in a watercolor style, which adds to its artistic and artistic qualities.
Chinese:å›¾ä¸­æœ‰å“ªäº›åŠ¨ç‰©ï¼Ÿ
[Chinese-EnglishTrans:What animals are present in the image?]
Output:è¿™å¼ å›¾ç‰‡ä¸­æœ‰ä¸¤ç§åŠ¨ç‰©ï¼šä¸€åªç™½å¤©é¹…åœ¨æ°´ä¸­æ¸¸æ³³ï¼Œä»¥åŠä¸€åªé¹¿ç«™åœ¨å²¸è¾¹ã€‚
[Chinese-EnglishTrans:The picture features two animals: a swan swimming in the water and a deer standing on the shore.]
Russian:ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ -ÑÑ‚Ğ¾ ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ.
[Russian-EnglishTrans:What is the weather shown in this picture?]
Output:ĞŸĞ¾Ğ³Ğ¾Ğ´Ğ° Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞµ Ğ²Ñ‹Ğ³Ğ»ÑĞ´Ğ¸Ñ‚ Ğ¾Ñ‡ĞµĞ½ÑŒ ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹. ĞĞµĞ±Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾Ğµ, Ñ Ğ³Ğ¾Ğ»ÑƒĞ±Ñ‹Ğ¼ Ğ½ĞµĞ±Ğ¾Ğ¼ Ğ¸ Ğ±ĞµĞ»Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸, ÑÑ€ĞºĞ¸Ğ¼
ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ²ĞµÑ‚Ğ¾Ğ¼, Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµĞ¹Ğ·Ğ°Ğ¶ ÑÑ€ĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ½ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ’ÑÑ Ğ¾Ğ±ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° ĞºĞ°Ğ¶ĞµÑ‚ÑÑ Ğ¾Ñ‡ĞµĞ½ÑŒ ÑĞ¿Ğ¾ĞºĞ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¸ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾Ğ¹,
Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ´Ñ‹Ñ…Ğ° Ğ½Ğ° ÑĞ²ĞµĞ¶ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ´ÑƒÑ…Ğµ.
[Russian-EnglishTrans:The weather in this picture looks very sunny. The sky is clear, with blue sky and white clouds, bright
sunlight, and the surrounding scenery is bright and full of life. The overall environment appears very peaceful and beautiful,
suitable for outdoor activities.]
Figure6:MultimodalconversationcasesofPARROTinmultiplelanguages.
6 Conclusion
ThispaperaddressesthecriticalchallengeofenhancingthemultilingualcapabilitiesofMLLMs. We
introducePARROT,anovelmethodleveragingtextualguidancetodrivevisualtokenalignmentat
thelanguagelevel,thusenablingthetransitionofEnglish-biasedvisualembeddingsintolanguage-
specificonesusinganMoEmodule.ExtensiveexperimentsconductedonanewlyintroducedMassive
MultilingualMultimodalBenchmark(MMMB)acrosssixlanguagesdemonstratethestate-of-the-art
performanceofPARROTcomparedtoexistingmethods,particularlypresentingremarkableimprove-
mentsinTurkishandArabic. Furthermore, ourmodelexhibitscompetitiveresultsacrossawide
rangeofdiversemultimodalbenchmarks,emphasizingitsefficacyinaddressingbothmultilingual
andmultimodalchallenges. PARROTnotonlyadvancesthefrontierofMLLMsbutalsounderscores
theimportanceofequitableaccesstotechnologicalbenefitsacrosslinguisticandculturaldiversities.
10References
[1] MetaAI. Buildthefutureofaiwithmetallama3. Technicalreport,MetaAI,2024.
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. NeurIPS,2022.
[3] anthropic. Introducingthenextgenerationofclaude. Technicalreport,anthropic,2024.
[4] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe,YonatanBitton,SamirGadre,ShioriSagawa,etal. Openflamingo: Anopen-source
frameworkfortraininglargeautoregressivevision-languagemodels. arXiv:2308.01390,2023.
[5] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,
YuHan,FeiHuang,etal. Qwentechnicalreport. arXiv:2309.16609,2023.
[6] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXiv:2308.12966,2023.
[7] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,
andSagË˜nakTasÂ¸Ä±rlar. Introducingourmultimodalmodels,2023.
[8] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,2020.
[9] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m: Pushing
web-scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
[10] GuanhuaChen,LuHou,YunChen,WenliangDai,LifengShang,XinJiang,QunLiu,JiaPan,
andWenpingWang. mclip: Multilingualclipviacross-lingualtransfer. InACL,2023.
[11] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,Jiaqi
Wang,YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-language
models? arXivpreprintarXiv:2403.20330,2024.
[12] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua
Lin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXiv:2311.12793,
2023.
[13] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz,
SebastianGoodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. Pali: Ajointly-scaled
multilinguallanguage-imagemodel. arXivpreprintarXiv:2209.06794,2022.
[14] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. arXiv:2312.14238,2023.
[15] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%chatgptquality,2023.
[16] OpenCompassContributors. Opencompass: Auniversalevaluationplatformforfoundation
models. https://github.com/open-compass/opencompass,2023.
[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning. arXiv:2305.06500,2023.
[18] ZhengxiaoDu, YujieQian, XiaoLiu, MingDing, JiezhongQiu, ZhilinYang, andJieTang.
Glm: Generallanguagemodelpretrainingwithautoregressiveblankinfilling. InProceedingsof
the60thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),pages320â€“335,2022.
11[19] StefanElfwing,EijiUchibe,andKenjiDoya. Sigmoid-weightedlinearunitsforneuralnetwork
functionapproximationinreinforcementlearning. Neuralnetworks,107:3â€“11,2018.
[20] Desmond Elliott, Stella Frank, Khalil Simaâ€™an, and Lucia Specia. Multi30k: Multilingual
english-germanimagedescriptions. arXivpreprintarXiv:1605.00459,2016.
[21] YuxinFang,QuanSun,XinggangWang,TiejunHuang,XinlongWang,andYueCao. Eva-02:
Avisualrepresentationforneongenesis. arXiv:2303.11331,2023.
[22] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,
Wei Lin, Zhenyu Qiu, Wei Lin, et al. Mme: A comprehensive evaluation benchmark for
multimodallargelanguagemodels. arXiv:2306.13394,2023.
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.
arXiv:2106.09685,2021.
[24] JinyiHu,YuanYao,ChongyiWang,ShanWang,YinxuPan,QianyuChen,TianyuYu,Hanghao
Wu, Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal
learningacrosslanguages. arXiv:2308.12038,2023.
[25] RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton.Adaptivemixtures
oflocalexperts. Neuralcomputation,3(1):79â€“87,1991.
[26] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXiv:2310.06825,2023.
[27] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXiv:2305.03726,2023.
[28] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:
Benchmarkingmultimodalllmswithgenerativecomprehension. InCVPR,2024.
[29] ChunyuanLi,CliffWong,ShengZhang,NaotoUsuyama,HaotianLiu,JianweiYang,Tristan
Naumann,HoifungPoon,andJianfengGao. Llava-med: Trainingalargelanguage-and-vision
assistantforbiomedicineinoneday. arXiv:2306.00890,2023.
[30] JiachenLi,XinyaoWang,SijieZhu,Chia-WenKuo,LuXu,FanChen,JiteshJain,Humphrey
Shi,andLongyinWen. Cumo: Scalingmultimodalllmwithco-upcycledmixture-of-experts.
arXivpreprintarXiv:2405.05949,2024.
[31] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. arXiv:2301.12597,2023.
[32] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-image
pre-trainingforunifiedvision-languageunderstandingandgeneration. InICML,2022.
[33] LeiLi,YuweiYin,ShichengLi,LiangChen,PeiyiWang,ShuhuaiRen,MukaiLi,Yazheng
Yang,JingjingXu,XuSun,LingpengKong,andQiLiu. M3it: Alarge-scaledatasettowards
multi-modalmultilingualinstructiontuning. arXiv:2306.04387,2023.
[34] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels. arXiv:2403.18814,2024.
[35] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,Yuliang
Liu,andXiangBai. Monkey: Imageresolutionandtextlabelareimportantthingsforlarge
multi-modalmodels. arXiv:2311.06607,2023.
[36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning. arXiv:2310.03744,2023.
[37] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
12[38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
arXiv:2304.08485,2023.
[39] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXiv:2307.06281,2023.
[40] JunyuLu,DixiangZhang,XiaojunWu,XinyuGao,RuyiGan,JiaxingZhang,YanSong,and
PingjianZhang. Ziya-visual: Bilinguallargevision-languagemodelviamulti-taskinstruction
tuning. arXivpreprintarXiv:2310.08166.,2023.
[41] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. NeurIPS,2022.
[42] ShiyinLu,YangLi,Qing-GuoChen,ZhaoXu,WeihuaLuo,KaifuZhang,andHan-JiaYe.
Ovis: Structuralembeddingalignmentformultimodallargelanguagemodel. arXivpreprint
arXiv:2405.20797,2024.
[43] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,Philipp
Dufter,DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,etal. Mm1: Methods,analysis&
insightsfrommultimodalllmpre-training. arXiv:2403.09611,2024.
[44] MinhengNi,HaoyangHuang,LinSu,EdwardCui,TaroonBharti,LijuanWang,Dongdong
Zhang, andNanDuan. M3p: Learninguniversalrepresentationsviamultitaskmultilingual
multimodalpre-training. InCVPR,2021.
[45] OpenAI. Chatgpt: Alanguagemodelforconversationalai. Technicalreport,OpenAI,2023.
[46] OpenAI. Gpt-4technicalreport. arXiv:2303.08774,2023.
[47] OpenAI. Gpt-4o: Hellogpt-4o. Technicalreport,OpenAI,2024.
[48] MaximeOquab,TimothÃ©eDarcet,ThÃ©oMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal.Dinov2:Learning
robustvisualfeatureswithoutsupervision. arXiv:2304.07193,2023.
[49] TelmoPessoaPires,RobinMSchmidt,Yi-HsiuLiao,andStephanPeitz. Learninglanguage-
specificlayersformultilingualmachinetranslation. arXivpreprintarXiv:2305.02665,2023.
[50] TaidoPurasonandAndreTÃ¤ttar. Multilingualneuralmachinetranslationwiththerightamount
ofsharing. InProceedingsofthe23rdAnnualConferenceoftheEuropeanAssociationfor
MachineTranslation,pages91â€“100,2022.
[51] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
[52] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training,2018.
[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[54] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
arXiv:2403.05530,2024.
[55] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: An open large-scale dataset for training next generation image-text models. NeurIPS,
2022.
13[56] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton,
andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts
layer. arXivpreprintarXiv:1701.06538,2017.
[57] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao. Eva-clip: Improvedtraining
techniquesforclipatscale. arXiv:2303.15389,2023.
[58] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[59] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
thÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXiv:2302.13971,2023.
[60] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXiv:2307.09288,2023.
[61] x.ai. Realworldqadataset. https://x.ai/blog/grok-1.5v,2024.
[62] An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang
Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint
arXiv:2211.01335,2022.
[63] Han-Jia Ye, Yi Shi, and De-Chuan Zhan. Identifying ambiguous similarity conditions via
semanticmatching. InCVPR,pages16610â€“16619,2022.
[64] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXiv:2304.14178,2023.
[65] ChaoYi,De-ChuanZhan,andHan-JiaYe. Bridgethemodalityandcapacitygapsinvision-
languagemodelselection. arXivpreprintarXiv:2403.13797,2024.
[66] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.
arXiv:2403.04652,2024.
[67] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for
languageimagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages11975â€“11986,2023.
[68] BiaoZhang,AnkurBapna,RicoSennrich,andOrhanFirat. Shareornot? learningtoschedule
language-specificcapacityformultilingualtranslation. InICLR,2021.
[69] HaoZhang,FengLi,ShilongLiu,LeiZhang,HangSu,JunZhu,LionelMNi,andHeung-
YeungShum. Dino: Detrwithimproveddenoisinganchorboxesforend-to-endobjectdetection.
arXiv:2203.03605,2022.
[70] WenxuanZhang,MahaniAljunied,ChangGao,YewKenChia,andLidongBing. M3exam: A
multilingual,multimodal,multilevelbenchmarkforexamininglargelanguagemodels.Advances
inNeuralInformationProcessingSystems,36,2024.
[71] Yi-KaiZhang,Ting-JiHuang,Yao-XiangDing,De-ChuanZhan,andHan-JiaYe. Modelspider:
Learningtorankpre-trainedmodelsefficiently. NeurIPS,36,2024.
[72] XinyuZhao,XuxiChen,YuCheng,andTianlongChen. Sparsemoewithlanguageguided
routingformultilingualmachinetranslation. InICLR,2024.
[73] MingyangZhou,LuoweiZhou,ShuohangWang,YuCheng,LinjieLi,ZhouYu,andJingjing
Liu. Uc2: Universalcross-lingualcross-modalvision-and-languagepre-training. InCVPR,
2021.
[74] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. Minigpt-4: Enhanc-
ingvision-languageunderstandingwithadvancedlargelanguagemodels. arXiv:2304.10592,
2023.
14A TrainingDatasetsinLLaVA
Inthissection,weanalyzethemultilingualdatainLLaVA[38]. FromTable4andFigure7,itis
evidentthatduringthepre-trainstage,LLaVAsolelyutilizesmultimodalimage-textpairsdatafor
training,comprising558KofEnglishdata. DuringtheSFTstage,bothmultimodalandtext-onlydata
areincorporatedintothetrainingprocess.Multilingualdataappearonlyinthetext-onlydataset.Apart
fromEnglish,themostprominentnon-EnglishdataisChinese,amountingtojust3.1K,constituting
0.25%ofthetotaldataset. Therefore, itisevidentthatLLaVAâ€™sdatasetsareEnglish-centricand
imbalanced. The specific language and abbreviation are as follows: English (en), Chinese (zh),
Korean (ko), Spanish (es), French (fr), Japanese (ja), German (de), Portuguese (pt), Traditional
Chinese(zh-tw),Italian(it).
Table4:ThedetailedinformationaboutLLaVAâ€™sdatasets.
(a)Thedetailedinformationintwostages.
TrainingStage Type TotalSize English OtherLanguages
Multimodal 558K 558K -
Stage1(Pre-train)
Text-only - - -
Multimodal 624K 558K -
Stage2(SFT)
Text-only 41K 31K 10K
(b)Thetop-10multilingualinformation
Language en zh ko es fr ja de pt zh-tw it
Size 31K 3192 1219 1123 1049 551 435 422 305 234
TotalSize
en zh ko es fr ja de pt zh-tw it
Figure7:ThepiechartofLLaVAâ€™smultilingualdata.
B TrainingDetails
AsshowninTable5,weprovidethetraininghyperparametersforPARROT. Throughoutallstages
oftraining,weconsistentlytrainforoneepoch,withabatchsizeof256forthefirststageand128
15Table5:Traininghyperparameters.
Config Stage1 Stage2
Experts - 6
MLPexpertnetwork 2LinearlayerswithSiLU
Deepspeed Zero2 Zero3
Imageresolution 336Ã—336
Imageencoder Clip-ViT-L/14-336
Featureselectlayer -2
Imageprojector 2LinearlayerswithGeLU
Epoch 1
Optimizer AdamW
Learningrate 1e-3 2e-5
Learningratescheduler Cosine
Weightdecay 0.0
Textmaxlength 2048
BatchsizeperGPU 16 8
GPU 16Ã—A100-80G
Precision Bf16
Gradientcheckpoint True
forthesecondstage. Wemaintainanimageresolutionof336x336foralltwostagesandenablethe
gradientcheckpointmodeforeachtrainingstage.
C ExtendedExperiments
Inthissection,wefurtherprovidemoreexperimentsandablationstudiestovalidatethegenerality
andcapabilityofPARROTacrossvarioustasks. Additionally,wepresentmoretrainingdetailsabout
Figure1toofferaclearerunderstandingforreaders.
C.1 BilingualEvaluationonLLaVA-Bench
VisCPM[24]extendstheLLaVA-BenchdatasettotheChineseversionforbilingualevaluation. To
comprehensively compare PARROT with other multilingual models, we conduct experiments on
thisbenchmark. DuetothedeprecationoftheGPT-4-0314versionbyOpenAI,wetestPARROTin
LLaVA-BenchfollowingtheversionofGPT-4-1106-previewforcomparison. AsshowninTable6,
PARROTnotonlydemonstratesexceptionalabilityintheEnglishversionofthisbenchmarkbutalso
presentscompetitiveperformanceintheChineseversion.
Notably,asshowninTable7,VisCPMuses140MEnglishdataand1MChinesedatatotrainthe
model,whileQwen-VL-Chatuses1.1BEnglishdataand300MChinesedata,whereasPARROTonly
utilizesapproximately2Mdataintotal. Despiteusinglessthan1%ofthetrainingdata, PARROT
achievesremarkableperformanceinboththeEnglishandChineseversionsonLLaVA-Bench. Owing
tothearchitectureweproposed,significantimprovementinthemodelâ€™smultilingualcapabilitycan
beachievedwithminimaldatausage.
C.2 RadarChartsonMMBenchandMMMB
ForamoreintuitivedemonstrationofthemultilingualcapabilitiesofPARROT,wepresentradarcharts
forthemultilingualMMBenchandMMMBbenchmarks. AsdepictedinFigure9aandFigure9b,
ourproposedmethodPARROTexhibitssignificantlybetterperformancecomparedtoothermodels.
C.3 MoreExperimentalDetailsaboutDifferentBackbones
Inthissection,weprovidedetailedinformationtoexplainFigure1.Firstly,toensureafaircomparison
betweentheOpenAI-CLIP-basedmodelandtheChinese-CLIP-basedmodel,wetraindistinctmodels
usingthesametrainingdataasLLaVA,asshowninTable4a. Thehyperparametersarelistedin
Table5withouttheMoEhyperparameters. AsdepictedinFigure1,theOpenAI-CLIP-basedmodel
strugglestogenerateChineseoutputswhengivenChinesepromptsduetotheEnglish-centrictraining
16Table 6: Experimental results on LLaVA Test Set accessed by GPT-4. Con: Conversation, DD: Detailed
Description,CR:ComplexReasoning,AVG:theaveragescoreofthreetasks. Thebest/secondbestresults
aremarkedinboldandunderlined,respectively.Thesymbolâˆ—denotesthatthedataarejudgedfollowingthe
versionofGPT-4-1106-previewbecausetheGPT-4-0314versionisdeprecatedbyOpenAI.
LLM English Chinese
Model
Backbone
Con DD CR AVG Con DD CR AVG
MiniGPT-4 Vicuna-13B 65.0 67.3 76.6 69.7 - - - -
English
InstructBLIP Vicuna-13B 81.9 68.0 91.2 80.5 - - - -
Model
LLaVA Vicuna-13B 89.5 70.4 96.2 85.6 - - - -
mPLUG-OWL BLOOMZ-7B 64.6 47.7 80.1 64.2 76.3 61.2 77.8 72.0
En-Zh
VisualGLM ChatGLM-6B 62.4 63.0 80.6 68.7 76.6 87.8 83.6 82.7
Bilingual
Qwen-VL-Chat Qwen-7B 82.4 76.9 91.9 83.8 82.3 93.4 89.5 88.2
Model
VisCPM-Balance CPM-Bee-10B 75.5 64.7 91.3 77.3 85.4 81.4 96.6 88.0
Multilingual
PARROTâˆ— Qwen1.5-7B 82.5 71.0 89.3 81.1 82.1 88.6 92.3 87.7
Model
Table7:Comparisonofvisionencoders,LLMs,andtrainingdata.
Model visionencoder LLM TrainingData
mPLUG-Owl ViT-L/14(0.3B) BLOOMZ-7B -
VisualGLM Q-Former(1.6B) ChatGLM-6B English:300M;Chinese30M
Qwen-VL-Chat ViT-bigG(1.9B) Qwen-7B English:1.1B;Chinese:300M
VisCPM Muffin(0.7B) CPM-Bee-10B English:140M;Chinese:1M
PARROT ViT-L/14(0.3B) Qwen1.5-Chat-7B English:1.8M;Chinese:71K
data. Incontrast,despitetheextremelyscarceamountofChinesetrainingdata,theChinese-CLIP-
basedmodelnaturallyacquireszero-shotcapabilitytounderstand,process,andgenerateChinese
texts. Furthermore,wecomparebothmodelsonMMBench-CNandMMMB-zhtoevaluatetheir
Chinese capability. As shown in Table 8, the performance of the Chinese-CLIP-based model is
significantlyhigherthanthatoftheOpenAI-CLIP-basedmodel. Ontheotherhand,weempirically
findthatdifferentLLMshaveasignificantimpactonperformance. Qwen[5]demonstratessuperior
ChinesecapabilitycomparedtoVicuna[15],yetitsEnglishcapabilityremainscompetitive.
D BroaderImpactandLimitations
BroaderImpact. PARROTleveragingMoEtoenhancemultilingualalignmentpresentsapositive
social impact by promoting linguistic diversity and inclusivity. To address the challenge of the
No
Is this an apple?
Yes
Yes
Is this a pear?
The answer is wrong.
No
The answer is correct.
Figure8:Anexampleofcircularevaluationstrategy.
17Table8:TheperformanceofdifferentvisionencodersandLLMsonMMBenchandMMMB.MMBrefersto
MMBench.â€œEn/enâ€representstheEnglishversion,andâ€œCN/zhâ€representstheChineseversion.
Method Visionencoder LLM MMB-EN MMB-CN MMMB-en MMMB-zh
LLaVA OpenAI-CLIPViT-L/14 Vicuna7B 65.4 58.3 67.1 58.8
LLaVA OpenAI-CLIPViT-L/14 Qwen1.5-Chat7B 68.8 66.4 68.2 62.4
LLaVA Chinese-CLIPViT-L/14 Qwen1.5-Chat7B 68.1 68.3 67.6 66.1
PARROT OpenAI-CLIPViT-L/14 Qwen1.5-Chat7B 70.7 70.4 70.0 68.1
imbalancedlanguagedatainSFTdatasetsandimprovenon-Englishvisualtokensalignment,this
approachcontributestobreakinglanguagebarriersandfacilitatingcross-culturalcommunication,
therebyfosteringunderstandingandcollaborationacrossdiverselinguisticcommunities. Addition-
ally,thecreationoftheMassiveMultilingualMultimodalBenchmark(MMMB)fillsacrucialgap
in evaluating multilingual capabilities, enabling researchers to assess and improve upon modelsâ€™
performanceacrossdifferentlanguagesandcultures. However,itâ€™scrucialtoacknowledgepotential
negativesocialimpacts,suchastheriskofhallucination. Thiscouldpotentiallyresultinthemodel
generatingmisleadingorincorrectinformation,whichisacommonchallengeobservedinMLLMs.
Limitations.Despiteadvancements,MLLMsmaystillexhibitlimitationsinaccuratelyunderstanding
andrespondingtocomplexlanguage-specificcontexts,leadingtomisinformationormisinterpretation
ofmultilingualinputs. Ontheotherhand,duetothevisualcomponentofPARROTbeingbasedon
CLIP,thereareinherentlimitationsinitsabilitytoprocesshigh-resolutionimages,resultinginthe
inabilitytorecognizeextremelydetailedcontentinsomeimages. Hence,enhancingPARROTâ€™sability
tohandlehigh-resolutionprocessingwillbepartoffuturework.
E MoreVisualizationResults
Inthissection,weincludeadditionalvisualizationresultsbetweenusersâ€™questionsandPARROTâ€™s
responsesusingmultiplelanguages. ThesepicturesareselectedfromLLaVA[38]andCuMo[30].
AsdepictedinFiguresFigures10to15,itisevidentthatPARROTpossessessuperiormultilingual
capabilities for understanding, processing, and generating multilingual texts. In certain specific
cases,PARROTmayalsoexperiencehallucinations. AsdepictedintheuppercaseofFigure10,it
misidentifiesXiaomiSU7asaPorscheTaycan.
18English Monkey-Chat 9.8B English Monkey-Chat 9.8B
72.0 Qwen-VL-Chat 7B 72.0 Qwen-VL-Chat 7B
LLaVA-NeXT 7B LLaVA-NeXT 7B
mPLUG-Owl2 7B mPLUG-Owl2 7B
58.0 Parrot 7B (Ours) 58.0 Parrot 7B (Ours)
Russian Chinese Russian Chinese
72.0 44.0 72.0 72.0 44.0 72.0
58.0 58.0 58.0 58.0
44.0 30.0 44.0 44.0 30.0 44.0
30.0 30.0 30.0 30.0
30.0 30.0 30.0 30.0
44.0 30.0 44.0 44.0 30.0 44.0
58.0 58.0 58.0 58.0
72.0 44.0 72.0 72.0 44.0 72.0
Turkish Portuguese Turkish Portuguese
58.0 58.0
72.0 72.0
Arabic Arabic
(a)MMBench. (b)MMMB.
Figure9:TheradarchartsforthemultilingualMMBenchandMMMBbenchmarks.
Table9: Ablationstudyonmonolingualfine-tuningdatasetinMMMBbenchmark. Thetableshowsan
effectofperformanceonsixlanguageswhenusingfine-tuningdatafromdifferentlanguages.Modelswith7B
parametersareusedforthisablation.
Dataset English Chinese Portuguese Arabic Turkish Russian
LLaVA-1.5-finetune 72.69 67.60 65.61 57.72 48.30 63.80
+zh71k 69.18 69.06 63.92 58.13 48.95 63.63
+pt14k 69.94 68.83 65.67 58.65 51.11 63.04
+ar12k 70.47 68.36 64.39 60.79 51.11 63.16
+tr17k 70.82 69.01 64.85 60.76 60.70 64.39
+ru14k 69.59 68.07 64.27 60.35 53.92 64.15
+zhptartrru 70.00 68.13 67.31 62.69 58.01 66.26
19English:What car is it in this image?
Output:The turquoise blue car isPorsche Taycan.
English:What animals are in this image?
Output:A cat and a dog are in the image.
Figure10:MorevisualizationresultsbetweentheuserandPARROTusingEnglishprompts.Wehighlightthe
hallucinationsfromtheresponsesofPARROT.
Chinese:ä½ çŸ¥é“è¿™å¹…ç”»æ˜¯è°ç”»çš„å—?
[Chinese-EnglishTrans:Doyouknowwhodrewthispainting]
Output:è±æ˜‚çº³å¤šÂ·è¾¾èŠ¬å¥‡
[Chinese-EnglishTrans:Leonardo da Vinci]
Chinese:è¯·æè¿°è¿™å¹…ç”»çš„ä¸»è¦å†…å®¹
[Chinese-EnglishTrans:Please describe the main content of this painting.]
Output:è¿™å¹…ç”»æ˜¯ä¸€å¹…è‚–åƒç”»ï¼Œæç»˜äº†ä¸€ä¸ªå¹´è½»å¥³å­çš„å¤´éƒ¨å’Œè‚©è†€ã€‚å¥¹ç©¿ç€ä¸€ä»¶é»‘è‰²è¿è¡£è£™ï¼Œä¸Š
é¢æœ‰é‡‘è‰²çš„è£…é¥°ã€‚å¥¹çš„å¤´å‘æ˜¯æ·±æ£•è‰²çš„ï¼Œè¢«æ¢³æˆä¸€ä¸ªå‘é«»ã€‚å¥¹çš„è„¸ä¸Šæœ‰è¤è‰²çš„æ–‘ç‚¹ï¼Œçœ¼ç›æ˜¯æ£•
è‰²çš„ã€‚å¥¹æ­£åœ¨å¾®ç¬‘ï¼Œè¡¨æ˜å¥¹å¿ƒæƒ…æ„‰å¿«ã€‚è¿™å¹…ç”»çš„é£æ ¼æ˜¯è‚–åƒç”»ï¼Œå®ƒä»¥æç»˜äººç‰©çš„å¤´éƒ¨å’Œè‚©è†€ä¸º
ç‰¹å¾ã€‚è‚–åƒç”»é€šå¸¸ä½¿ç”¨é€¼çœŸçš„æŠ€å·§ï¼Œä»¥æ•æ‰äººç‰©çš„ç‰¹å¾å’Œè¡¨æƒ…ã€‚è¿™å¹…ç”»ä¸­äººç‰©çš„æœè£…å’Œç¯å¢ƒæš—
ç¤ºç€å®ƒå¯èƒ½æç»˜çš„æ˜¯æ–‡è‰ºå¤å…´æ—¶æœŸçš„è‰ºæœ¯å®¶ã€‚
[Chinese-EnglishTrans:This painting is a portrait, depicting the head and shoulders of a young woman.
She is dressed in a black dress with gold embellishments. Her hair is dark brown and styled into a bun.
There are brown spots on her face, and her eyes are brown. She is smiling, indicating she is in a good
mood. The style of the painting is portraiture, characterized by the depiction of the subject's head and
shoulders. Portraits often use realistic techniques to capture the features and expressions of the person.
The attire and setting of the figure in this painting suggest it may be depicting an artist from the
Renaissance period.]
Figure11:MorevisualizationresultsbetweentheuserandPARROTusingChineseprompts.
20Portuguese:Qual Ã© o final deste filme?
[Portuguese-EnglishTrans:What's the ending of this movie?]
Output: O filme "Titanic" Ã© um romance histÃ³rico de 1997 que apresenta uma narrativa fictÃ­cia de uma
mulher chamada Rose e um homem chamado Jack. A narrativa se passa durante o final do sÃ©culo XIX,
quando o navio "Titanic" estÃ¡ se preparando para sua viagem de luxo para Nova York. Jack e Rose se
enamoram e se casam, mas a viagem Ã© marcada por um desastre inesperado.
[Portuguese-EnglishTrans:The movie "Titanic" is a 1997 historical romance that features a fictional
narrative about a woman named Rose and a man named Jack. The story is set during the late 19th century,
as the ship "Titanic" is preparing for its luxury voyage to New York. Jack and Rose fall in love and get
married, but the journey is marked by an unexpected disaster.]
Figure12:MorevisualizationresultsbetweentheuserandPARROTusingPortugueseprompts.
Arabic: ØŸïºï»¨Ú¾ ï»²ïº—Ø±ïºï¯¾Ø² ïºªï»¨ï»‹ ïºï®­ï»¨ï»£ Ø§Ø±Ù‹ïº¬ïº£ Ù†ï»®ï»›Ø£ Ù†Ø£ ïºïº ï¯¾ ï»²ïº˜ï»ŸØ§ Ø¡ïºï¯¿ïº·ï»·Ø§ ïºï»£
[Arabic-EnglishTrans:What's the ending of this movie?]
Output: ï»–ïº‹Ø§ï»®ï»‹ ÙŠØ£ ÙˆØ£ ØŒïº”ï¯¿ïº‹ïºï»¤ï»ŸØ§ Ø¡ïºï¯¿ïº·ï»·Ø§ ÙˆØ£ ØŒïº”ï» ï» ïº’ï»¤ï»ŸØ§ ïº¢ï»„ïº³ï»·Ø§ ï»ïºœï»£ ØŒïº”ï» ï»¤ïº˜ïº¤ï»£ ïº®Ø·ïºïº¨ï»£ ÙŠØ£ ï»¦ï»£ Ø§Ø±Ù‹ïº¬ïº£ Ù†ï»®ï»œï¯¾ Ù†Ø£ ïºïº ï¯¾ ØŒï»Šï»—ï»®ï»¤ï»ŸØ§ Ø§ïº¬Ú¾ Ø©Ø±ïºï¯¾Ø² Ø¡ïºï»¨ïº›Ø£
ï»ïºœï»£ ØŒïº”ï¯¿ïºŒï¯¿ïº’ï»ŸØ§ ï»ï»£Ø§ï»®ï»Œï»Ÿïºïº‘ ï»¢ï» ï»‹ ï»°ï» ï»‹ Ø§ï»®ï»§ï»®ï»œï¯¾ Ù†Ø£ Ø±Ø§Ùˆïº°ï»ŸØ§ ï»°ï» ï»‹ ïºïº ï¯¾ ØŒï»šï»ŸØ° ï»°ï»ŸØ¥ ïº”ï»“ïºïº¿ï»¹ïºïº‘ .ïº”ï¯¿ïº‹ïºï»¤ï»ŸØ§ Ø¡ïºï¯¿ïº·ï»·Ø§ ÙˆØ£ Ø±ï»®ïº¨ïº¼ï»ŸØ§ ÙˆØ£ Ø±ï»®ïº¨ïº¼ï»ŸØ§ ï»ïºœï»£ ØŒïº”ï» ï»¤ïº˜ïº¤ï»£
.ïº”ï¯¿ïº¿ïº®ï»£Ùˆ ïº”ï»¨ï»£Ø¢ ïº”ïº‘ïº®ïº ïº— Ù†ïºï»¤ï»€ï»Ÿ ØŒïº”ï¯¿ïº‹ïºï»¤ï»ŸØ§ ÙÙˆïº®ï»ˆï»ŸØ§ ÙˆØ£ ØŒØ©Ø±Ø§ïº®ïº¤ï»ŸØ§ ØªïºïºŸØ±Ø¯ ÙˆØ£ ØŒïº²ï»˜ï»„ï»ŸØ§ ØªØ§ïº®ï¯¿ï»ïº—
[Arabic-EnglishTrans:During the visit to this site, one should be cautious of any potential hazards, such
as wet surfaces, or water-related objects, or any potential obstacles, like rocks or boulders or aquatic
objects. Additionally, visitors should be aware of environmental factors, such as changes in weather,
temperature, or water conditions, to ensure a safe and satisfying experience.]
Figure13:MorevisualizationresultsbetweentheuserandPARROTusingArabicprompts.
21Turkish:Bu malzemelerle hangi yemekleri yapabilirim?
[Turkish-EnglishTrans: What are the meals that I can cook with these?]
Output: BuzdolabÄ±ndaki Ã§eÅŸitli malzemelerle, Ã§ilekli bir salata, havuÃ§lu bir sandviÃ§, meyveli bir yemek
ve sÃ¼t ve yoÄŸurtla bir yemek gibi Ã§eÅŸitli yemekler yapÄ±labilir. BuzdolabÄ±, havuÃ§, Ã§ilek ve yoÄŸurt gibi
diÄŸer gÄ±da maddelerini de iÃ§erir; bunlar Ã§eÅŸitli yemekler yaratmak iÃ§in kullanÄ±labilir. Birden fazla
yiyecek maddesi ve kaplarÄ±n olmasÄ±, buzdolabÄ±nÄ±n iyi stoklandÄ±ÄŸÄ±nÄ± ve Ã§eÅŸitli yemek seÃ§eneklerine
olanak tanÄ±dÄ±ÄŸÄ±nÄ± gÃ¶sterir.
[Turkish-EnglishTrans:With the various items in the refrigerator, one can cook a variety of meals, such
as a salad with strawberries, a sandwich with carrots, a meal with berries, and a meal with milk and
yogurt. The refrigerator also contains other food items, such as carrots, strawberries, and yogurt, which
can be used to create a diverse range of dishes. The presence of multiple food items and containers
suggests that the refrigerator is well-stocked, allowing for a variety of meal options.]
Figure14:MorevisualizationresultsbetweentheuserandPARROTusingTurkishprompts.
Russian:ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ¸Ñ‚Ğµ Ğ¼Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¾ Ğ¼ĞµÑÑ‚Ğ¾ Ğ½Ğ°
Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸.
[Russian-EnglishTrans: Please help me introduce the place in this
picture.]
Output:ĞœĞµÑÑ‚Ğ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞµ, -ÑÑ‚Ğ¾ ÑĞ¾Ğ±Ğ¾Ñ€ Ğ’Ğ°ÑĞ¸Ğ»Ğ¸Ñ Ğ‘Ğ»Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾, Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾
Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ°Ğº ÑĞ¾Ğ±Ğ¾Ñ€ ĞŸĞ¾ĞºÑ€Ğ¾Ğ²Ğ° ĞŸÑ€ĞµÑĞ²ÑÑ‚Ğ¾Ğ¹ Ğ‘Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¸Ñ†Ñ‹ Ğ½Ğ° Ñ€Ğ²Ñƒ. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğµ ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ²
ĞœĞ¾ÑĞºĞ²Ğµ, Ğ Ğ¾ÑÑĞ¸Ñ, Ğ½Ğ° ĞšÑ€Ğ°ÑĞ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸.
[Russian-EnglishTrans:The place depicted in the picture is the Saint Basil's Cathedral, officially known
as the Cathedral of the Intercession of the Most Holy Theotokos on the Moat. This iconic structure is
located in Moscow, Russia, on Red Square. ]
Figure15:MorevisualizationresultsbetweentheuserandPARROTusingRussianprompts.
22