Enhancing Temporal Consistency in Video Editing by
Reconstructing Videos with 3D Gaussian Splatting
InkyuShin‚àó QihangYu XiaohuiShen InSoKweon
KAIST ByteDance ByteDance KAIST
Kuk-JinYoon Liang-ChiehChen
KAIST ByteDance
https://video-3dgs-project.github.io
Video-3DGS
Video Reconstruction
GT Video Deformable-3DGS
3DGS Video-3DGS
Video Editing
Original Video Original Video
Text2Vid-Zero Text prompt: ‚Äúneon lights‚Äù TokenFlow Text prompt: ‚Äúthrough an obstacle on Mars‚Äù
Text2Vid-Zero + Video-3DGS TokenFlow + Video-3DGS
Figure 1: The proposed Video-3DGS expands the capabilities of 3D Gaussian Splatting
(3DGS)(Kerbletal.,2023)todynamicmonocularvideoscenes,enhancingtemporalconsistencyin
bothvideoreconstructionandvideoediting. Forinstance,itconsistentlycapturesandreconstructs
dynamicobjectssuchasridersandhorses(uppersection),whilealsoenrichingstylesmoothnessin
scenarioslikedrift-carsequences(bottomleft)andensuringstructureconsistency(bottomright).
Regionsofinterestarehighlightedbywhitedashedrectangles.
‚àóWorkdoneduringinternshipatByteDance.
Preprint.Underreview.
4202
nuJ
4
]VC.sc[
1v14520.6042:viXraAbstract
Recentadvancementsinzero-shotvideodiffusionmodelshaveshownpromise
fortext-drivenvideoediting, butchallengesremaininachievinghightemporal
consistency. Toaddressthis,weintroduceVideo-3DGS,a3DGaussianSplatting
(3DGS)-basedvideorefinerdesignedtoenhancetemporalconsistencyinzero-shot
videoeditors. Ourapproachutilizesatwo-stage3DGaussianoptimizingprocess
tailored for editing dynamic monocular videos. In the first stage, Video-3DGS
employsanimprovedversionofCOLMAP,referredtoasMC-COLMAP,which
processesoriginalvideosusingaMaskedandClippedapproach. Foreachvideo
clip,MC-COLMAPgeneratesthepointcloudsfordynamicforegroundobjects
andcomplexbackgrounds. Thesepointcloudsareutilizedtoinitializetwosets
of3DGaussians(Frg-3DGSandBkg-3DGS)aimingtorepresentforegroundand
backgroundviews. Bothforegroundandbackgroundviewsarethenmergedwith
a 2D learnable parameter map to reconstruct full views. In the second stage,
weleveragethereconstructionabilitydevelopedinthefirststagetoimposethe
temporal constraints on the video diffusion model. This approach ensures the
temporalconsistencyintheeditedvideoswhilemaintaininghighfidelitytothe
editingtextprompt. Wefurtherproposearecursiveandensembledrefinementby
revisitingthedenoisingstepandguidancescaleusedinvideodiffusionprocess
with Video-3DGS. To demonstrate the efficacy of Video-3DGS on both stages,
weconductextensiveexperimentsacrosstworelatedtasks: VideoReconstruction
andVideoEditing. Video-3DGStrainedwith3kiterationssignificantlyimproves
videoreconstructionquality(+3PSNR,+7PSNRincrease)andtrainingefficiency
(√ó1.9,√ó4.5timesfaster)overNeRF-basedand3DGS-basedstate-of-artmethods
onDAVISdataset,respectively. Moreover,itenhancesvideoeditingbyensuring
temporalconsistencyacross58dynamicmonocularvideos.
1 Introduction
Advancementsindiffusion-basedgenerativemodels (Hoetal.,2020;Rombachetal.,2022)have
significantlyimprovedtext-drivenimageeditingcapabilities(Brooksetal.,2023). Buildingonthis
success,therehasbeenconsiderableefforttoextendthesetechnologiestothevideodomain(Ho
etal.,2022;Singeretal.,2022),whichholdspracticalpotentialacrossabroadrangeofapplications,
includingFilm/EntertainmentandAR/VR.However,trainingavideodiffusionmodelfromscratch
using video datasets is not effective for two main reasons: (i) it lacks a well-constructed video
trainingdatasettohandlethediversedistributionofvideosinthewild,and(ii)discardingpre-trained
imagediffusionmodelswouldincurapenaltyingeneratinghigh-qualityeditedframesforvideos. To
addressthelimitations,zero-shottraining-freevideoeditingmethods(Khachatryanetal.,2023;Geyer
etal.,2023;Karaetal.,2023)thatarebuiltonpre-trainedimagediffusionmodelhaverecentlybeen
introducedtonotonlyincreasethevideoeditingqualitybutalsoimproveefficiency. Nevertheless,
thesezero-shotvideodiffusionmodelsstillexhibitlowtemporalconsistencyindynamicvideosdue
to their limited understanding of individual video scenes. A natural question thus emerges: Is it
possibletodesignasimpleyeteffectiveplug-and-playmoduletoenhancethetemporalconsistency
ofeacheditedvideofromanyzero-shotvideoeditors?
Toanswerthequestion, wecarefullydesignVideo-3DGS,aninnovativeapproachthatleverages
per-scenerepresentationpowerof3DGaussianSplatting(3DGS)(Kerbletal.,2023)toenhance
temporalconsistencyabilityforzero-shotvideodiffusionmodels. Ourapproachaimstoutilizethe
structuralpreservationabilityof3DGS,whichisknownforexplicitlyrepresentingandassociating
multipleviewswithunified3DGaussians. Asillustratedin Figure2,Video-3DGSemploysatwo-
stage3DGSoptimizationprocesstoreconstructdynamicmonocularvideosandrefinetheinitially
editedones,ensuringenhancedtemporalconsistencyandvisualcoherence.
Thefirststagebeginsbyaddressingthelimitationsof3DGSinrepresentingdynamicmonocularvideo
scenes,wheremultipleobjectsmoveagainstcomplexbackgrounds. Tosolvethisissue,newmethods
adopting3DGSfordynamicmonocularvideosareemerging. Deformable-3DGS(Yangetal.,2023)
employedthedeformationnetworktocondition3DGSwithtime-variable,while4DGS(Wuetal.,
2023a)additionallyaddedHexPlane(Cao&Johnson,2023)encoderbeforethedeformationnetwork.
Despitetheseattemptstoextend3DGSformonocularvideoscenes,itisimportanttoacknowledgethe
2wild video Video-3DGS (1st stage) Video-3DGS (2nd stage)
Frg-3DGS zero-shot
video editor
foreground points
video
recon
background points video
edit
MC-COLMAP
Bkg-3DGS
Figure2: TheoverallpipelineofVideo-3DGS.Weaimtodesignavideo-level3DGaussianSplatting
frameworktoreconstructthevideoscenes(1ststage),whichenableshightemporalconsistencyin
videoediting(2ndstage). Specifically,Video-3DGSisempoweredbytheproposedMC-COLMAP
thateffectivelyobtains3Dpointsforforegroundmovingobjects. Thebackground3Dpointsare
modeled with spherical-shaped random points, surrounding the foreground points. Video-3DGS
utilizestwosetsof3DGaussians(Frg-3DGSandBkg-3DGS)torepresentforegroundandbackground
3Dpoints,respectively. A2Dlearnableparametermapmergestheforegroundandbackgroundviews,
renderedfromeachsetof3DGaussians. Themergedviewsenablehigh-fidelityvideoreconstruction.
Then, we leverage this reconstruction capability into zero-shot video editor to enhance temporal
consistencywhilemaintaininghighfidelitytotextprompt.
persistinglimitations,whicharesummarizedasfollows. (i)Thefragiledependencyontheunderlying
SfMmethod,e.g.,COLMAP(Sch√∂nberger&Frahm,2016)forderiving3Dpointsfromtheentire
dynamicvideos. (ii)Thedifficultyofaccuratelyrepresentingmonocularvideoswithasingleset
of3DGaussians. Videoframes,particularlythosewithdynamicmotionandintricatebackground,
areinfeasibletoberepresentedbyasinglesetof3DGaussians,evenwiththedeformationnetwork.
Aswecanobservefrom Figure1,Deformable-3DGS(Yangetal.,2023)showsunsatisfyingvideo
reconstructionresultsfordynamicmovingobjects.Therefore,wefirstdesigntheframeworkofVideo-
3DGStoaddressthecomplexitiesofdynamicmonocularvideoscenes. Oneofthekeydistinctionsof
theproposedmethodliesinitsabilitytoexcelin‚Äúwilder‚Äùmonocularvideoscenarios(e.g.,videoswith
largerobjectmotion). Tothisend,weinitiallysimplifythevideosequenceswithtwodecomposition
strategies.First,spatialdecomposition,poweredbyanoff-the-shelfopenvocabularyvideosegmentor,
mitigates background clutter 2. Second, temporal decomposition breaks down the entire video
sequenceintomultipleshortervideoclipswithoverlappingframesbetweenneighboringclips. Given
thesedecompositions,wecaneffectivelyextract3Dpointsofmaskedforegroundmovingobjects
ineachvideoclipbyintroducingMC-COLMAP.Meanwhile,theclutteredbackgroundismodeled
with spherical-shaped random 3D points surrounding the pre-extracted 3D points of foreground
movingobjects. Foreachvideoclip, Video-3DGSutilizestwosetsof3DGaussians(Frg-3DGS
andBkg-3DGS)torepresentforegroundandbackground3Dpoints,respectively. Weadditionally
implementmulti-resolutionhashencoding(M√ºlleretal.,2022)withdeformationnetworks(Yang
etal.,2023)onbothFrg-3DGSandBkg-3DGS,whichcanboostperformanceandefficiency. Lastbut
notleast,toobtainthefinalrendered2Doutputsfromboth3DGaussians,weadoptastraightforward
andeffective2Dlearnableparametermaptomergethetworenderedimages(renderedviewsfrom
Frg-3DGSandBkg-3DGS),resultinginfaithfulvideoframerepresentations.
Thesubsequentstagefocusesonseamlesslyintegratingapre-optimizedVideo-3DGSintoexisting
zero-shotvideoeditors.Theprimaryadvantageofusingapre-optimizedVideo-3DGSliesinitsability
toapplystructuredconstraintsof3DGaussiansacrossmultiplevideoframes. Morespecifically,we
maintainthestructuralcomponentsofFrg-3DGSandBkg-3DGSinafixedstatewhileselectively
fine-tuningthecolorparameters,suchassphericalharmoniccoefficients,alongsidea2Dlearnable
parametermap. Thisfine-tuningprocessisdesignedtocaptureandreplicatethestyleoftheinitially
editedvideoframes. Byapplyingfixed3DGaussianstructureswithstyleupdatesineachclipand
smoothingtransitionsbetweenoverlappingframesacrossneighboringclips,weeffectivelyminimize
styleflickeringthroughouttheentirevideo,enhancingtemporalconsistency. Wehavealsoobserved
thatexistingzero-shotvideoeditorsexhibitsensitivitytovariationsinparameters,suchasthenumber
ofdenoisingstepsandthescaleofimageortextguidance. Thesevariationscansignificantlyimpact
2backgroundcontainingcomplexregionsthatcluttertheforegroundobjects(e.g.,treesinthebackground).
3theeditingoutputs,asdemonstratedin Figure3. Tooptimizeeffectivenessandreduceparameter
sensitivity,weintroducearecursiveandensembledvideoeditingstrategy. Thisinvolvesinterspersing
Video-3DGSbetweensplitdenoisingstepsandupdatingstylesusingmultiplevideoseditedunder
different guidance scales. Thisfurther explorationhelps to stabilize the editingoutcomes across
varyingparametersettings.
Tovalidatetheproposedtwo-stageVideo-3DGSoptimization,wetestedoncorrespondingvideo
tasks: video reconstruction and video editing. For video reconstruction, our approach surpasses
both NeRF-based and 3DGS-based state-of-the-art methods across 28 DAVIS videos. We then
demonstratetheapplicabilityofVideo-3DGS‚Äôsvideoscenerepresentationtothevideoeditingtaskin
58challengingmonocularvideossourcedfromtheCVPR2023LOVEUText-GuidedVideoEditing
(TGVE)challenge(Wuetal.,2023b). Video-3DGSconsistentlyenhanceseditingqualityacrossthree
off-the-shelfvideoeditors(Text2Video-Zero(Khachatryanetal.,2023),TokenFlow(Geyeretal.,
2023),andRAVE(Karaetal.,2023)).
2 RelatedWork
Inthispart,webrieflyreviewtheliteratureonText-to-Imageediting(Section2.1),andZero-shot
Text-to-Videoediting(Section2.2). Morerelatedworksaboutneuralrenderingformonocularvideos
areintroducedinAppendixA.
2.1 Text-to-ImageEditing
Text-to-imageeditinghassignificantlyadvancedwiththedevelopmentofgenerativemodels,particu-
larlylatentdiffusionmodels(Rombachetal.,2022). Thesemodelshavedemonstratedremarkable
capabilitiesingeneratinghigh-qualityimagesguidedbytextualprompts. MethodssuchasDream-
Booth (Ruiz et al., 2022) and Textual Inversion (Gal et al., 2022) have achieved notable success
by fine-tuning pre-trained models for specific editing tasks. Recent approaches, like Prompt-to-
Prompt(Hertzetal.,2022)andDiffEdit(Couaironetal.,2022),leverageattentionmechanismswithin
thesemodelstoenablelocalizedanddetailedimageeditswithoutextensiveretraining. Techniques
likeSDEdit(Mengetal.,2022)utilizestochasticdifferentialequationstoguideimagesynthesis,en-
ablingprecisecontroloverthegeneratedcontent. Inadditiontothese,BlendedDiffusion(Avrahami
etal.,2022)presentsamethodtoblendnewlygeneratedcontentseamlesslyintoexistingimages.
Thistechniqueensuresthatthesynthesizedadditionsarecoherentwiththeoriginalimagecontext,
thusmaintainingahighlevelofrealism. AnothernotableapproachisPaintbyWord(Andonianetal.,
2021),whichintroducesaframeworkwhereuserscaninteractivelyeditimagesbyprovidingtextual
descriptionsforspecificregions. Thismethodutilizesacombinationofmaskpredictionandimage
generationtoaccuratelyreflecttheuser‚Äôsintent. Despitesignificantadvancementsintext-to-image
editing,theprogressintext-to-videoeditingtechnologyhasbeenrelativelyslow.
2.2 Zero-shotText-to-VideoEditing
Zero-shottext-to-videoeditingleveragespre-trainedtext-to-imagemodelstoeditvideoswithout
requiringextensiveretraining. Thisapproachaimstoovercomethelimitationsoftraditionalvideo
editingmethods,suchasvideo-specifictraining(Hoetal.,2022)andatlaslearning(Kastenetal.,
2021),whichareoftentime-consumingandrequiresignificantmanualeffort. Recentadvancements
inthisdomainhaveintroducedvarioustechniquestoachievehigh-qualityandtemporallyconsistent
videoedits. Forinstance,RAVE(Karaetal.,2023)employsanoiseshufflingstrategytoenhance
temporalconsistencybyleveragingspatio-temporalinteractionsbetweenframes. Thismethodinte-
gratesspatialguidancethroughControlNet(Zhangetal.,2023)andoperatesefficientlyintermsof
memoryrequirements,makingitsuitableforeditinglongervideos. RAVE‚Äôsapproachsignificantly
reducesprocessingtimecomparedtoexistingmethods,achievingroughly25%fastereditingrate
whilemaintaininghighvisualquality. Othernotablezero-shotmethodsincludePix2Video(Ceylan
etal.,2023)andFateZero(Qietal.,2023),whichutilizesparse-causalattentionandattentionfea-
turepreservation,respectively,tomaintainmotionandstructuralconsistencyacrossvideoframes.
Text2Video-Zero(Khachatryanetal.,2023)synthesizesandeditsvideosbyintegratingcross-frame
attentionandcontrollingthefidelitytostructureoforiginalvideowithimageguidancescale. Token-
Flow(Geyeretal.,2023)enforcesconsistencybypropagatingdiffusionfeaturesacrossframesbased
on inter-frame correspondences in zero-shot manner. These techniques collectively demonstrate
4video editing video editing
‚ÄúTrucks drive on a racetrack, after ùëÅùëë/ 2 after ùëÅùëë
autumn, fall colors.‚Äù
guidance scale:
1.0
guidance scale:
1.5
video editor
(e.g., Text2Video-Zero)
Figure3: Werevisitthekeyhyperparametersinthevideodiffusionprocess: thedenoisingstep(N )
d
andtheguidancescale. Employingahigherdenoisingstepcombinedwithalowerguidancescale
(e.g., similar to the image guidance scale in Text2Video-Zero (Khachatryan et al., 2023)) results
ingreaterfidelitytotheeditingpromptbutcompromisesstructuralandtemporalconsistency,and
viceversa. Thisanalysisconfirmsthatthezero-shotvideoeditormodelishighlysensitivetothese
hyperparameters.
the potential of zero-shot video editing in producing temporally coherent and visually appealing
videoswithouttheneedforextensivetrainingonvideo-specificdatasets. Despitetherecognized
improvements in efficiency and editing capabilities offered by zero-shot video editors, they still
sufferfromsubpartemporalconsistencyandoverallvideoeditingqualityduetoalackofper-scene
understanding. Thislimitationmotivatesustodesignaplug-and-playrefinerforzero-shotvideo
editors(we selectrepresentativethree editors: Text2Video-Zero/ TokenFlow/ RAVE), aimedat
enhancingper-sceneunderstandingandimprovingthequalityoftheeditedvideos.
3 Method
The meta-architecture of Video-3DGS aims to design a video-specific 3DGS, which serves as a
plug-and-playrefinerforinitiallyeditedvideofromzero-shotvideoeditors. Thisprocessisstructured
into two seamless stages. The first stage of Video-3DGS (Section 3.2) aims to represent and
reconstruct original videos with two integrated components: MC-COLMAP (Section 3.2.1) and
Foreground/Background3DGS(Section3.2.2). MC-COLMAPplaysapivotalroleingenerating
masked clip-level foreground and background 3D points. Subsequently, for each clip, two sets
of3DGaussians,Frg-3DGSandBkg-3DGS,areinitializedandoptimizedbasedonthesepoints.
Additionally,a2Dlearnableparametermapisemployedtomergetheforegroundandbackground
viewsrenderedfromFrg-3DGSandBkg-3DGS.Theresultingviewsaccuratelyrepresentthevideo
frames,facilitatingthevideoreconstruction. Section3.3detailsthesecondstage,wherewetransform
theoptimizedVideo-3DGSintoaplug-and-playtemporalrefinerforvideoediting.
3.1 Preliminary
Diffusion Model Diffusion probabilistic models (DPM), introduced by (Sohl-Dickstein et al.,
2015)andfurtherdevelopedin (Hoetal.,2020)representaclassofgenerativemodelsdesignedto
approximateadatadistributionq throughaprogressivedenoisingprocess. Themodelstartswith
aGaussiani.i.dnoisyimagex ‚àºN(0,I)andthediffusionmodelœµ incrementallydenoisesthis
T Œ∏
image until it reaches a clean image x drawn from the target distribution q. The Deterministic
0
Samplingalgorithm(DDIM)describedin(Songetal.,2020)initializesthenoiseprocess,termed
DDIM inversion, by starting from a clean image x . This approach effectively facilitates image
0
editingthroughdiffusionmodels. Subsequently,thetechniquehasbeenadaptedforvideoeditingby
incorporatingtemporalconstraints. Thisextensionemploysseveralstrategiestoensureconsistency
acrossframes,includingtheuseofa3DUNetwithcross-frameattentionasexploredinText2Video-
Zero(Khachatryanetal.,2023),diffusionfeaturepropagationasdiscussedinTokenFlow(Geyer
etal.,2023),andthegridtrickdetailedinRAVE(Karaetal.,2023).
3DGaussianSplatting Wealsopresentanoverviewof3DGaussianSplatting(3DGS)(Kerbl
et al., 2023). The method involves the initialization of a set of 3D Gaussians upon 3D point
clouds. ThepointcloudsarederivedfromStructurefromMotion(SfM)techniques,exemplifiedby
COLMAP(Sch√∂nberger&Frahm,2016)withtheinputV ofaseriesofN consecutiveframesx
i
5Clip #1 Clip #2 . . .
Monocular
video If failed,
SfMwith more frames
Open-vocabulary MC-COLMAP MC-COLMAP
VOS
ùë≥ùëπùíÜùíÑùíêùíè ùë≥ùëπùíÜùíÑùíêùíè
. . .
Frg-3DGS Bkg-3DGS Frg-3DGS Bkg-3DGS
clip-level
ùëÆùüèùë≠ùíìùíà ùëÆùüèùë©ùíåùíà ùëÆùüêùë≠ùíìùíà ùëÆùüêùë©ùíåùíà
optimization rendering rendering rendering rendering
ùë∞ùüèùë≠ùíìùíà ùë∞ùüèùë©ùíåùíà ùë∞ùüêùë≠ùíìùíà ùë∞ùüêùë©ùíåùíà
ùë∞ ùüèùíéùíÜùíìùíàùíÜùíÖ ùë∞ ùüêùíéùíÜùíìùíàùíÜùíÖ
Figure4: TheproposedVideo-3DGS(1ststage)comprisestwokeycomponents. First,toeffectively
capture3Dpointcloudsandcorrespondingframeviewpointsfromdynamicmonocularvideos,we
introduceMaskedandClippedCOLMAP(MC-COLMAP).Thismodulespatiallyandtemporally
decomposes video frames, facilitating the extraction of clip-level foreground points through pro-
gressivelyprocessingclips. Additionally,weinitializespherical-shapedrandombackgroundpoints
conditionedontheforegroundpoints. Second,withthesetwosetsofpointclouds,weintroducetwo
distinctsetsof3DGaussianSplatting(3DGS):Frg-3DGSandBkg-3DGS,optimizedseparatelyfor
foregroundandbackgroundpoints,respectively. Subsequently,weemployastraightforwardmerging
operationtocombinetherenderedoutputsofFrg-3DGSandBkg-3DGS.Weoptimizethemerged
renderedoutputsforeachclipusingthereconstructionloss.
(i.e.,V ={x }N )inamonocularvideosequence. Then,3DGaussiansaretrainedwiththefunction
i i=1
ofG(x,r,s,œÉ,SH)containingcenterpositionx,3Dcovariancematrixobtainedfromquaternionr
andscalings,opacityœÉ,andsphericalharmoniccoefficientSH.
3.2 Video-3DGS(1stStage): ReconstructingVideoswith3DGS
3.2.1 MC-COLMAP
WedenotetheconventionalCOLMAPfunctionasR,whichisdesignedtoderivea3Dpointcloud
p, along with the associated camera data c (encompassing both intrinsic (cin) and extrinsic (cex)
parameters). Furthermore,itoutputsafeedbacksignalstatus,whichservesasanindicatorofthe
COLMAPsystem‚Äôssuccessinreconstructing3DpointsfromtheentirevideoframesV. Thatis,
p,c,status=R(V) (1)
However,thepresenceofforegroundmovingobjectsandclutteredbackgroundinvideoframesposes
asignificantchallengeinextractingconsistentlymatched3Dpointsacrosstheentirevideoframes.
Thisdifficultyleadstoinaccurateandsparsereconstructionsinvideooutputs,asisevidentinour
experiments. Totacklethesechallenges,weproposeMC-COLMAP,arevisedversionofCOLMAP,
specificallydesignedtoprogressivelyprocessvideoframes. Iteffectivelyminimizesmotionand
complexbackgroundthroughtwokeystrategies: spatialdecompositionandtemporaldecomposition.
Spatial Decomposition To reduce the background cluttering effect, we adopt an off-the-shelf
open-vocabularyvideoobjectsegmentationnetworkS (e.g.,DEVA(Chengetal.,2023))toextract
thesegmentationmasksforforegroundmovingobjectsinthevideoV asfollows:
Vf =S(V,class) (2)
whereVf andclassdenotetheextractedsegmentationmasksofforegroundmovingobjectsWang
etal.(2021);Yuetal.(2022)andtheuser-guidedtextprompt(arequiredinputforthesegmentation
networktospecifythetargetobject(Yuetal.,2023a,b)),respectively.
6TemporalDecomposition ThesegmentationmasksVf forforegroundobjects,whichspanthe
entirevideosequence,presentaprocessingchallengeforCOLMAPduetotheirintricatemotion
dynamics. Tomitigatethiscomplexity,stemmingfromdiversemotionpatternsthroughoutthevideo
sequence,wepartitionthevideosequenceintomultipleshortervideoclips(Kimetal.,2022;Shin
et al., 2024; He et al., 2023). This division ensures that objects exhibit reduced motion within
eachclip,thusfacilitatingmoremanageableprocessingforCOLMAP.Formally,weaddressthisby
splittingVf intomultipleM clips{Vf}M .
j j=1
RatherthanevenlydividingthevideosequenceintoM clips,eachcontainingkframes,weintroduce
aprogressiveschemetoaddresspotentialfailurecases,suchaswhentheforegroundobjectremains
static within a clip, making the k frames inadequate for point cloud extraction or registration in
SfM.Specifically,webeginwiththefirstclipinitializedwiththefirstkframes(i.e.,Vf ={x }k ).
1 i i=1
If the COLMAP function R fails to process the clip (i.e., status Ã∏= ‚ÄòSuccess‚Äô), we iteratively
1
includeoneadditionalconsecutiveframeintothecurrentclipuntilCOLMAPreturnsa‚ÄòSuccess‚Äô
status. Subsequently,thenextclipcommenceswiththelastframeofpreviousclip,alsoinitialized
withk frames. Thisprocesscontinuesuntiltheentirevideosequenceisprocessed. Wetermthe
resultingpipelineasMC-COLMAP,which appliesCOLMAPinaMaskedand Clippedmanner.
OurMC-COLMAPsystem(denotedasR )yieldsmultiplesetsofmaskedandclipped3Dpoint
MC
clouds,alongwiththeircorrespondingviewsfromM clips:
{(p ,c )}M =R (S(V,class)) (3)
j j j=1 MC
3.2.2 ForegroundandBackground3DGS
Foreground3DGaussians Foreachvideoclip,the3Dpointcloudsforforegroundmovingobjects,
derived from Equation (3), serve as initialization for optimizing foreground 3D Gassuians. This
processyieldsasetof3DGaussians{GFrg}M ,tailoredforthoseforegroundpointcloudswith
j j=1
thecorrespondingcameraviews,{(p ,c )}M . Withthis,wesuccessfullyrepresenttheforeground
j j j=1
objectsusing3DGaussians,andthesubsequentstepinvolvesmodelingtheclutteredbackground.
Background3DGaussians Foreachvideoclip,tosimplymodeltheclutteredbackground,we
utilize spherical-shaped random point clouds {pBkg}M , surrounding the previously extracted
j j=1
foreground3Dpoints.Thesebackgroundrandompointcloudsserveastheinitializationforoptimizing
background3DGaussians,yieldingasetof3DGaussians{GBkg}M . Notably,thespherical-shaped
j j=1
randompointcloudsaredefinedbytwohyper-parameters: numberofpointsn anditsradiusr .
Bkg i
Ourmethodisempiricallyfoundrobusttothosehyper-parameters,andwefixn =60kpointsand
Bkg
r to3timeslargerthantheforegroundpoints‚ÄôdistancethatismeasuredbythemaximumEuclidean
i
distancebetweenforegroundpoints.
Deformable3DGaussians Following(Yangetal.,2023),boththeforegroundandbackground3D
Gaussiansareenhancedwiththedeformablenetwork,whicheffectivelyleveragethetimeinformation,
butweextendthemforclip-levelprocessing,resultingin:
Frg-3DGS:{GFrg({x ,r ,s }+Œ¥Frg,œÉ ,SH )}M
j j j j j j j j=1
(4)
Bkg-3DGS:{GBkg({x ,r ,s }+Œ¥Bkg,œÉ ,SH )}M
j j j j j j j j=1
where Œ¥ is the deformation within the j-th clip to transform center x , rotation r , and scale s ,
j j j j
accordingtoeachclipcenterandnormalizedtime. ThesuperscriptFrgandBkgdenoteforeground
andbackground,respectively. Weimplementthedeformationnetworkwith4Dmulti-resolutionhash
encoding(M√ºlleretal.,2022). GiventhosetwosetsofFrg-3DGSandBkg-3DGS,wereformulate
themasClip-3DGS,whereeachclipcontainsitscorrespondingFrg-3DGSandBkg-3DGS.Thatis,
Clip-3DGS={GFrg,GBkg}M . Wethenprocessthose3DGaussiansinaclip-by-clipmanner.
j j j=1
Merging Foreground and Background Views with 2D Learnable Parameters For the j-th
clip,weleveragethedifferentiablepoint-basedrenderingtechnique(Wilesetal.,2020),asoutlined
in(Kerbletal.,2023),toprocessthetwosetsof3DGaussiansGFrg andGBkg. Thisenablesusto
j j
generatetwodistinctrenderedimagesforeachframewithinthej-thclip. Specifically,renderingthe
i-thvideoframeproducestwoimages,{IFrg,IBkg},derivedfromGFrg andGBkg,respectively.
i i j j
To seamlessly merge these images, both of dimensions height H and width W, we introduce a
straightforwardyetpowerfulmergingtechnique. Thismethodemploysa2DlearnableparameterŒ±
7recursive editing
ensemble editing
SH update
guidance optimized
scale:
Video-3DGS
video editor 1.0
guidance video editor
scale:
1.5 append
denoising step:
ùëÅùëë / ùëÅùëü
guidance denoising step:
scale: ùëÅùëë/ ùëÅùëü
2.0
x ùëÅùëü
Figure 5: The overview of Video-3DGS (2nd stage) as a plug-and-play refiner for video editing
beginswithfine-tuningthesphericalcoefficientoftheoptimizedVideo-3DGSonaninitiallyedited
video, whichisproducedusinganoff-the-shelfvideoeditorwiththedefaulthyperparameters: a
denoisingstepN andaguidancescale. Thismethod,referredtoasthesingle-phaserefinerwith
d
Video-3DGS,isfurtherenhancedbyourfindingsin Figure3. WesplitN intoarecursivenumber
d
N andfine-tunethesphericalcoefficientparametersagainstmultipleoutputsfromvariedguidance
r
scales,aimingforimprovedtemporalconsistencyandhighfidelitytotheeditingtext. Thisadvanced
approachisnamedtheRecursiveandEnsembled(RE)refinement.
‚ààRH√óW,facilitatingpixel-wisemergingwithcorrespondinglearnableparametersinitializedtoa
valueof0.5. Formally,wehave:
Imerged =Œ± √óIFrg+(1‚àíŒ± )√óIBkg (5)
i i i i i
whereImergedisthemergedresultforthei-thvideoframe.
i
Through the merging operation, we derive merged images for all video frames by optimizing N
differentŒ±values(N representsthetotalnumberofframesinthevideo).
TrainingLosses TooptimizeFrg-3DGSandBkg-3DGS,weadoptthereconstructionloss,denoted
asL ,whichcomprisestwocomponents:L andL ,akintotheapproachoutlinedin(Kerbl
recon 1 SSIM
etal.,2023). Thesecomponentsarecalculatedbycomparingthreerenderedimages-theforeground
IFrg,thebackgroundIBkg,andthemergedimageImerged -againsttheirrespectivegroundtruth
images.
VideoReconstruction Thereconstructionofavideocanbeachievedbysequentiallyrenderingthe
Frg-3DGSandBkg-3DGSforeachclip. Subsequently,therenderedimagesfromeachframeare
mergedusingapre-trainedalphaparameter(Equation(5)).
3.3 Video-3DGS(2ndStage): Plug-and-PlayRefinerforEditingVideos
Single-phaseRefiner Theutilizationofpre-trained3DGaussiansshowcasesremarkableprofi-
ciencyinpreservingstructureandensuringtemporalconsistencyacrossvideoframes,makingthem
highlysuitableforvideoeditingtasks. Initiallyeditedvideoframes,denotedas{Iedited}N ,canbe
i i=1
sourcedfromanyzero-shotvideoeditors. Toaddressinconsistenciesinstyleandobjectpresence
withintheseeditedvideos,weleveragepre-trained3DGaussians.Thisapproachinvolvesmaintaining
theoriginalstructuralcontextbyfixingthepositional(x,r,s)anddeformationparameters(Œ¥)ofboth
Frg-3DGSandBkg-3DGS.Simultaneously,weadjustthecolorvalue(SH)andopacity(œÉ)parame-
terstoalignwiththestyleoftheeditedframes,accomplishedbyminimizingthereconstructionloss
betweenrenderedimages{Imerged}N andeditedframes{Iedited}N forupdatingcorresponding
i i=1 i i=1
SH andœÉasfollows:
N
(cid:88)
min L (Imerged,Iedited) (6)
recon i i
SH,œÉ
i=1
Alteringthecolorvalueof3DGaussiansenablescorrespondingareasofrenderedimagestomaintain
consistencywithineachclip. Moreover,refiningoneditedoverlappingframesbetweenneighboring
clipscanensuresmoothstyletransitionacrossclips,ultimatelyenhancingthetemporalconsistency
8Metrics
successrate PSNR SSIM Time
Method
Robust-Dyn (Liuetal.,2023) 28/28 26.8 0.795 746m
NLA(Kastenetal.,2021) 28/28 27.3 0.795 309m
NeRF-based
CoDeF(Ouyangetal.,2023) 28/28 29.4 0.869 28m
Nerv(Chenetal.,2021b) 28/28 34.6 0.980 20m
3DGS(Kerbletal.,2023) 20/28 24.8 0.832 10m
3DGS-based
Deform-3DGS(Yangetal.,2023) 20/28 30.6 0.919 50m
Video-3DGS(3k) 28/28 37.6 0.980 11m
3DGS-based(ours) Video-3DGS(5k) 28/28 41.2 0.989 22m
Video-3DGS(10k) 28/28 45.8 0.995 56m
Table 1: Average quantitative results of DAVIS videos for Video Reconstruction. ‚Äòsuccess rate‚Äô
indicatestheproportionofvideos,outofthetotal28videos,thatcanbesuccessfullyreconstructed.
Ourmethod,Video-3DGS,demonstratesefficienttrainingandsignificantlyoutperformsNeRF-based
and3DGS-basedmethods. WereportVideo-3DGSresultstrainedwith3k,5k,and10kiterations.
overwholevideoframes. GiventhatweadoptVideo-3DGSfollowingasingle-phaseofvideoediting,
Video-3DGS(2ndstage)canbeappropriatelyreferredtoasasingle-phaserefiner.
RecursiveandEnsembledRefiner Inthesingle-phaserefiner,weupdatethesphericalharmonic
coefficients(SH)andopacity(œÉ)ofpre-trainedVideo-3DGSbyminimizingthereconstructionloss
betweenrenderedimagesandeditedvideoframes. However,weencounteredinstanceswheresome
initialeditedframeshadalreadylostthestructuresoftheoriginalvideoduetoprolongedinference
steps(numberofdenoisingsteps)implementedbytheoff-the-shelfvideoeditor. Toaddressthisissue,
thenumberofdenoisingsteps(N )isdividedintomultiplephases(N ,fixedto2forefficiency).
d r
Weperformthesingle-phaserefinementoneditedframesfromthefirstphase(e.g.,videoediting
afterN /2),resultinginintermediaterefinedframes. Theserefinedframesarethenusedasinput
d
forDDIMinversion(asutilizedinTokenFlowandRAVE)ortheimageencoder(asemployedin
Text2Video-Zero)toinitializethenextphase. Subsequently,theremainingdenoisingsteps(N /2)
d
arecarriedoutforthenextphaseofvideoediting. Thisisfollowedbyafinalsingle-phaserefinement
usingVideo-3DGSonthe editedframesfromthissubsequentphase. Werefertothisprocessas
recursiverefinement. Anothercriticalhyperparameterinthequalityofeditedvideosisguidance
parametersinthediffusionprocess,suchasimageguidanceinText2Video-Zero,textguidancein
TokenFlow,andcontrolnetguidanceinRAVE.Asshownin Figure3,differentguidancescaleslead
tosignificantchanges. Tomitigatesensitivitytoscalechangesandintegratetheadvantagesofeach
scale,weconductanensembleupdateofSH andœÉwithmultipleeditedvideosfromthreedifferent
scalesduringeachrecursivephase. Forthenextphaserefinement,weincorporatethosemultiple
editedvideosfromthepreviousphaseintonextones.Itallowseachphasetobenefitfromtheprevious
edits. Forsimplicity,werefertoVideo-3DGS(2ndstage)withRecursiveandEnsembledrefinement
asVideo-3DGS(RE)andillustrateitinFigure5.
4 ExperimentalResults
In this section, we evaluate Video-3DGS on two tasks: Video Reconstruction (Section 4.1) and
VideoEditing(Section4.2). Weprovidethedetailsofdatasetsandmetricsusedforthosetwotasks
(AppendixB),ablationstudies(AppendixD),andqualitativeresults(AppendixE)intheAppendix.
4.1 VideoReconstruction
Baselines AsVideo-3DGSaimstoreconstructvideosbylearningscenerepresentations,wecom-
pareourapproachwithtworepresentativemethods: NeRF-basedand3DGS-basedrepresentations.
ForNeRF-basedmethods,weselectfourstate-of-the-artbaselines: 1)NLA(Kastenetal.,2021),
whichproposesavideoreconstructionandeditingmethodusinglayeredatlasespoweredbytheNeRF
framework. 2)CoDEF(Ouyangetal.,2023),whichleveragesacanonicalspaceofdeformationfields
toreconstructandeditvideos. 3)RobustDyn(Liuetal.,2023),anadvancedviewreconstructionand
synthesismodelthatestimatescameraposesindiversesettings. 4)NervChenetal.(2021b),which
proposesneuralrepresentationtoencodevideosinneuralnetworks. For3DGS-basedmethods,we
firstconsidertheoriginal3DGSmethod(Kerbletal.,2023),whichlacksmodulesspecifictovideo
scenes. Additionally,weselectDeformable-3DGS(Yangetal.,2023),astate-of-the-artapproach
thatutilizesadeformationnetworkon3DGaussian,servingasanotherstrongbaseline.
9Method Text2Vid-Zero +Video-3DGS +Video-3DGS(RE)
Dataset WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë
DAVIS 0.691 20.1 0.827(+13.6%) 21.0(+0.9%) 0.899(+20.8%) 22.3(+2.2%)
Videovo 0.773 21.9 0.902(+12.9%) 22.1(+0.2%) 0.926(+15.3%) 23.1(+1.2%)
Youtube 0.701 19.8 0.885(+18.4%) 20.4(+0.6%) 0.922(+22.1%) 21.1(+1.3%)
Method TokenFlow +Video-3DGS +Video-3DGS(RE)
Dataset WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë
DAVIS 0.855 22.9 0.909(+5.4%) 23.9(+1.0%) 0.912(+5.7%) 24.8(+1.9%)
Videovo 0.897 22.6 0.933(+4.6%) 23.5(+0.9%) 0.937(+4.9%) 23.8(+1.2%)
Youtube 0.848 22.1 0.923(+7.5%) 23.1(+1.0%) 0.923(+7.5%) 24.2(+2.1%)
Method RAVE +Video-3DGS +Video-3DGS(RE)
Dataset WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë WarpSSIM‚Üë Qedit‚Üë
DAVIS 0.872 23.4 0.908(+3.6%) 24.1(+0.7%) 0.913(+4.1%) 24.8(+1.5%)
Videovo 0.872 22.4 0.913(+4.1%) 23.5(+1.1%) 0.923(+5.1%) 23.6(+1.2%)
Youtube 0.855 21.7 0.918(+6.3%) 22.9(+1.2%) 0.921(+6.6%) 23.5(+1.8%)
Table2: QuantitativeresultsofVideoEditingonthreedifferentdatasets: DAVIS,Videvo,Youtube.
Each dataset contains four editing categories: style change, object change, background change,
multiplechange. WepresenttheresultsforaverageWarpSSIMandQ offourcategories,withtext
edit
colorindicatingtheeffectofVideo-3DGS.OurVideo-3DGSincreasesbothtemporalconsistency
andoverallvideoeditingperformanceacrossallinitialvideoeditors. ‚Üë: thehigher,thebetter.
1 1
0.8 0.8
0.72 0.69
0.6 0.55 0.6 0.53
0.4 0.40.37 0.4 0.350.38
0.20.18 0.23 0.2 0.25 0.20.21 0.27 0.25 0.22
0.1 0.1
0 0
Text2Video-Zero TokenFlow RAVE Text2Video-Zero TokenFlow RAVE
Baseline w/Video-3DGS Equalpreference w/Video-3DGS w/Video-3DGS(RE) Equalpreference
Figure6: UserstudyofVideo-3DGSonthreevideoeditors.
QuantitativeResults AsillustratedinTable1,ourcomprehensivevideoreconstructionexperiments
encompass28videossourcedfromDAVIS.WenotethatthefourNeRF-basedmethodsgenerally
exhibit limitations in both reconstruction quality (measured by PSNR and SSIM) and efficiency
(trainingtime),primarilyattributedtotheirimplicitneuralrepresentation. Conversely,the3DGS
method(Kerbletal.,2023)demonstratesasignificantreductionintrainingtime(lessthan10minutes
inaverageof20videos),owingtoitsexplicit3DGaussianrepresentationandefficientrasterization.
However,the3DGSmethodnotablyexhibitsperformancedegradation(averagePSNRof20videos:
24.8),asitistailoredforstaticscenesandlacksdesignconsiderationsfordynamicvideoscenes. On
theotherhand,thestate-of-the-artbaselineDeformable-3DGS,whichemploysadeformationfieldfor
timedimensionon3DGS,showsimprovedvideoreconstructionquality(averagePSNRof20videos:
30.6);nevertheless,thisenhancementcomesattheexpenseofcompromisedtrainingefficiency(50
minutesinaverageof20videos).Furthermore,duetothefundamentalissueinCOLMAP,theycannot
conductreconstructionon8videos,whichhinders3DGSfrombeingusedforwildvideodatasets. On
theotherhand,poweredbytheproposedMC-COLMAPandframeworkofFrg-3DGS/Bkg-3DGS,
Video-3DGScanreconstructall28videosinhighqualitywithshortertrainingtime(iteration3k:37.6
PSNRwith11minutes;iteration5k:41.2PSNRwith22minutes;iteration10k:45.8PSNRwith56
minutes;allresultsaremeasuredbytakingaverageover28videos). Upontrainingfor3kiterations,
Video-3DGSsignificantlysurpassesboththeNeRF-basedSoTA,Nerv(Chenetal.,2021b),andthe
3DGS-basedSoTA,Deform-3DGS(Yangetal.,2023),intermsofvideoreconstructionqualityand
trainingefficiency. Specifically,itachievesanimprovementinPSNRby+3and+7overNervand
Deform-3DGS, respectively. Furthermore, Video-3DGS demonstrates a notable improvement in
trainingtimeefficiency,being1.9timesfasterthanNervand4.5timesfasterthanDeform-3DGS.
4.2 VideoEditing
Baselines Wemeticulouslychoosethreezero-shotvideoeditingmethodsasbaselinecomparisons.
1)Text2Video-Zero(Khachatryanetal.,2023),whichextendsInstruct-pix2pix(Brooksetal.,2023)
10
ecnereferPresU ecnereferPresUtothevideodomainbyinsertingtemporalattentionwithinthediffusionmodel. 2)TokenFlow(Geyer
etal.,2023),whichachievestemporalsmoothnessthroughthepropagationofdiffusionfeaturesusing
inter-framecorrespondences. 3)RAVE(Karaetal.,2023),whichemploysanoise-shufflingstrategy
andgridtrickforenhancingvideoeditingcapabilities.
QuantitativeResults WeassessedVideo-3DGS‚Äôseditingcapabilityontopofthosethreevideo
editors in Table 2. Video-3DGS significantly enhances temporal consistency (WarpSSIM score)
acrossvariouseditingscenariosspanningthreedifferentdatasets. Consequently,thisimprovement
generallyyieldssuperiorfinalvideoeditingresults(Q ). ItprovesthatVideo-3DGSgenerally
edit
providesrobusttemporaleditingguidancetoexistingvideoeditors.Wealsoobservethatourproposed
recursiveandensembledrefinementcanfurtherimprovebothtemporalconsistencyandvideoediting
resultsonallofvideoeditorsanddatasets.
UserStudy Toexploreuserpreferencesforvideoeditingoutcomes,weconductedauserstudy
detailedinFigure6. Weenlisted20participantsandpresentedthemwith30randomlypairedvideos
(10pairsforeachoftheadoptedoff-the-shelfvideoeditors)derivedfromtheLOVEUvideoresults.
Participantsweretaskedwithselectingtheirpreferrededitedvideo,takingintoaccountfidelitytothe
providedtextprompt(forediting)andtemporalconsistency. Consistentwiththefindingsintheleft
panelofFigure6,weobservedthatusersgenerallyfavoredVideo-3DGSguidedvideoeditingresults
overtheoriginaleditedvideos. Moreover,intherightpanelofFigure6,weconductedanotheruser
studytoshowtheeffectivenessofrecursiveandensembledrefinementineditingwithVideo-3DGS.
5 Conclusion
WeintroducedVideo-3DGS,anoveltwo-stageapproachtailoredtoeditdynamicmonocularvideo
scenesbyreconstructingthemwith3DGaussianSplatting(3DGS).ThefirststageofVideo-3DGS
liesitscapacitytospatiallyandtemporallydecomposevideos,streamliningmotionrepresentation
acrossclips. Itsurpassespreviousstate-of-the-artmethodsinvideoreconstruction. Thesecondstage
ofVideo-3DGSleveragesthereconstructionabilitytorefinetheeditedvideosfromthreedifferent
zero-shotvideoeditors,whichenhancesbothtemporalconsistencyandeditingresults. Weanticipate
significantpositivesocietalimpactsfromourmethod,asitconsistentlyperformswellinrepresenting
diversevideosandrefiningoutputsfromzero-shotvideoeditors. Weexpectittobenefitvarious
applications,suchasEntertainmentwithvideosynthesisandAR/VR.
11References
Andonian,A.,Osmany,S.,Cui,A.,Park,Y.,Jahanian,A.,Torralba,A.,andBau,D. Paintbyword.
arXivpreprintarXiv:2103.10951,2021.
Avrahami, O., Lischinski, D., and Fried, O. Blended diffusion for text-driven editing of natural
images. InCVPR,2022.
Barron, J. T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., and Srinivasan, P. P.
Mip-nerf: Amultiscalerepresentationforanti-aliasingneuralradiancefields. InCVPR,2021.
Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing
instructions. InCVPR,2023.
Cao,A.andJohnson,J. Hexplane: Afastrepresentationfordynamicscenes. InCVPR,2023.
Ceylan,D.,Huang,C.-H.,andMitra,N.J. Pix2video: Videoeditingusingimagediffusion. InICCV,
2023.
Chen,A.,Xu,Z.,Zhao,F.,Zhang,X.,Xiang,F.,Yu,J.,andSu,H. Mvsnerf: Fastgeneralizable
radiancefieldreconstructionfrommulti-viewstereo. arXivpreprintarXiv:2103.15595,2021a.
Chen,H.,He,B.,Wang,H.,Ren,Y.,Lim,S.N.,andShrivastava,A. Nerv: Neuralrepresentations
forvideos. NeurIPS,2021b.
Cheng,H.K.,Oh,S.W.,Price,B.,Schwing,A.,andLee,J.-Y. Trackinganythingwithdecoupled
videosegmentation. InICCV,2023.
Chng,S.-F.,Ramasinghe,S.,Sherrah,J.,andLucey,S. Garf: Gaussianactivatedradiancefieldsfor
highfidelityreconstructionandposeestimation. arXivpreprintarXiv:2204.05735,2022.
Couairon,G.,Verbeek,J.,Schwenk,H.,andCord,M. Diffedit: Diffusion-basedsemanticimage
editingwithmaskguidance. arXivpreprintarXiv:2210.11427,2022.
Fan,Z.,Wang,K.,Wen,K.,Zhu,Z.,Xu,D.,andWang,Z. Lightgaussian: Unbounded3dgaussian
compressionwith15xreductionand200+fps. arXivpreprintarXiv:2311.17245,2023.
Gal,R.,Alaluf,Y.,Atzmon,Y.,Patashnik,O.,Bermano,A.H.,Chechik,G.,andCohen-Or,D. An
imageisworthoneword: Personalizingtext-to-imagegenerationusingtextualinversion. arXiv
preprintarXiv:2208.01618,2022.
Garbin,S.J.,Kowalski,M.,Johnson,M.,Shotton,J.,andValentin,J. Fastnerf: High-fidelityneural
renderingat200fps. arXivpreprintarXiv:2103.10380,2021.
Geyer, M., Bar-Tal, O., Bagon, S., and Dekel, T. Tokenflow: Consistent diffusion features for
consistentvideoediting. arXivpreprintarXiv:2307.10373,2023.
Gu,J.,Liu,L.,Wang,P.,andTheobalt,C. Stylenerf: Astyle-based3d-awaregeneratorforhigh-
resolutionimagesynthesis. arXivpreprintarXiv:2110.08985,2021.
He,J.,Yu,Q.,Shin,I.,Deng,X.,Yuille,A.,Shen,X.,andChen,L.-C. Maxtron: Masktransformer
withtrajectoryattentionforvideopanopticsegmentation. arXivpreprintarXiv: 2311.18537,2023.
Hertz,A.,Mokady,R.,Tenenbaum,J.,Aberman,K.,Pritch,Y.,andCohen-Or,D. Prompt-to-prompt
imageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,2022.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionprobabilisticmodels. NeurIPS,2020.
Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,andFleet,D.J. Videodiffusionmodels.
NeurIPS,2022.
Jeong,Y.,Ahn,S.,Choy,C.,Anandkumar,A.,Cho,M.,andPark,J. Self-calibratingneuralradiance
fields. arXivpreprintarXiv:2108.13826,2021.
12Kara,O.,Kurtkaya,B.,Yesiltepe,H.,Rehg,J.M.,andYanardag,P.Rave:Randomizednoiseshuffling
for fast and consistent video editing with diffusion models. arXiv preprint arXiv:2312.04524,
2023.
Karnewar,A.,Ritschel,T.,Wang,O.,andMitra,N. Relufields: Thelittlenon-linearitythatcould. In
SIGGRAPH,2022.
Kasten,Y.,Ofri,D.,Wang,O.,andDekel,T. Layeredneuralatlasesforconsistentvideoediting.
ACMTransactionsonGraphics(TOG),40(6):1‚Äì12,2021.
Kerbl,B.,Kopanas,G.,Leimk√ºhler,T.,andDrettakis,G. 3dgaussiansplattingforreal-timeradiance
fieldrendering. ACMTransactionsonGraphics,42(4),2023.
Khachatryan,L.,Movsisyan,A.,Tadevosyan,V.,Henschel,R.,Wang,Z.,Navasardyan,S.,andShi,
H. Text2video-zero:Text-to-imagediffusionmodelsarezero-shotvideogenerators. arXivpreprint
arXiv:2303.13439,2023.
Kim,D.,Xie,J.,Wang,H.,Qiao,S.,Yu,Q.,Kim,H.-S.,Adam,H.,Kweon,I.S.,andChen,L.-C.
Tubeformer-deeplab: Videomasktransformer. InCVPR,2022.
Kundu,A.,Genova,K.,Yin,X.,Fathi,A.,Pantofaru,C.,Guibas,L.,Tagliasacchi,A.,Dellaert,F.,
andFunkhouser,T. Panopticneuralfields: Asemanticobject-awareneuralscenerepresentation.
arXivpreprintarXiv:2205.04334,2022.
Lee,B.,Lee,H.,Sun,X.,Ali,U.,andPark,E. Deblurring3dgaussiansplatting. arXivpreprint
arXiv:2401.00834,2024.
Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with
frozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023.
Lin,C.-H.,Ma,W.-C.,Torralba,A.,andLucey,S. Barf: Bundle-adjustingneuralradiancefields.
arXivpreprintarXiv:2104.06405,2021.
Liu,Y.-L.,Gao,C.,Meuleman,A.,Tseng,H.-Y.,Saraf,A.,Kim,C.,Chuang,Y.-Y.,Kopf,J.,and
Huang,J.-B. Robustdynamicradiancefields. InCVPR,2023.
Ma, L., Li, X., Liao, J., Zhang, Q., Wang, X., Wang, J., and Sander, P. V. Deblur-nerf: Neural
radiancefieldsfromblurryimages. arXivpreprintarXiv:2111.14292,2022.
Martin-Brualla,R.,Radwan,N.,Sajjadi,M.S.M.,Barron,J.T.,Dosovitskiy,A.,andDuckworth,
D. Nerfinthewild: Neuralradiancefieldsforunconstrainedphotocollections. arXivpreprint
arXiv:2008.02268,2021.
Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image
synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073,
2022.
Meuleman,A.,Liu,Y.-L.,Gao,C.,Huang,J.-B.,Kim,C.,Kim,M.H.,andKopf,J. Progressively
optimizedlocalradiancefieldsforrobustviewsynthesis. InCVPR,2023.
Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf:
Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,65
(1):99‚Äì106,2021.
M√ºller, T., Evans, A., Schied, C., and Keller, A. Instant neural graphics primitives with a
multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1‚Äì102:15, July 2022. doi:
10.1145/3528223.3530127. URLhttps://doi.org/10.1145/3528223.3530127.
Ouyang,H.,Wang,Q.,Xiao,Y.,Bai,Q.,Zhang,J.,Zheng,K.,Zhou,X.,Chen,Q.,andShen,Y.
Codef: Content deformation fields for temporally consistent video processing. arXiv preprint
arXiv:2308.07926,2023.
Paszke,A.,Gross,S.,Massa,F.,Lerer,A.,Bradbury,J.,Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,
N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library.
NeurIPS,2019.
13Pont-Tuset,J.,Perazzi,F.,Caelles,S.,Arbel√°ez,P.,Sorkine-Hornung,A.,andVanGool,L. The2017
davischallengeonvideoobjectsegmentation. arXivpreprintarXiv:1704.00675,2017.
Qi,C.,Cun,X.,Zhang,Y.,Lei,C.,Wang,X.,Shan,Y.,andChen,Q. Fatezero: Fusingattentionsfor
zero-shottext-basedvideoediting. arXivpreprintarXiv:2303.09535,2023.
Reiser, C., Peng, S., Liao, Y., and Geiger, A. Kilonerf: Speeding up neural radiance fields with
thousandsoftinymlps. arXivpreprintarXiv:2103.13744,2021.
Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,andOmmer,B. High-resolutionimagesynthesis
withlatentdiffusionmodels. InCVPR,2022.
Ruiz,N.,Li,Y.,Jampani,V.,Pritch,Y.,Rubinstein,M.,andAberman,K. Dreambooth: Finetuning
text-to-imagediffusionmodelsforsubject-drivengeneration. arXivpreprintarxiv:2208.12242,
2022.
Sch√∂nberger,J.L.andFrahm,J.-M. Structure-from-motionrevisited. InCVPR,2016.
Shin,I.,Kim,D.,Yu,Q.,Xie,J.,Kim,H.-S.,Green,B.,Kweon,I.S.,Yoon,K.-J.,andChen,L.-C.
Video-kmax: Asimpleunifiedapproachforonlineandnear-onlinevideopanopticsegmentation.
InWACV,2024.
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O.,
Gafni,O.,etal. Make-a-video: Text-to-videogenerationwithouttext-videodata. arXivpreprint
arXiv:2209.14792,2022.
Sohl-Dickstein,J.,Weiss,E.,Maheswaranathan,N.,andGanguli,S. Deepunsupervisedlearning
usingnonequilibriumthermodynamics. InICML,2015.
Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502,2020.
Teed,Z.andDeng,J. Raft: Recurrentall-pairsfieldtransformsforopticalflow. InECCV,2020.
Wang,D.,Cui,X.,Salcudean,S.,andWang,Z.J. Generalizableneuralradiancefieldsfornovelview
synthesiswithtransformer. arXivpreprintarXiv:2206.05375,2022.
Wang, H., Zhu, Y., Adam, H., Yuille, A., and Chen, L.-C. Max-deeplab: End-to-end panoptic
segmentationwithmasktransformers. InCVPR,2021.
Wiles, O., Gkioxari, G., Szeliski, R., andJohnson, J. Synsin: End-to-endviewsynthesisfroma
singleimage. arXivpreprintarXiv:1912.08804,2020.
Wu,G.,Yi,T.,Fang,J.,Xie,L.,Zhang,X.,Wei,W.,Liu,W.,Tian,Q.,andWang,X. 4dgaussian
splattingforreal-timedynamicscenerendering. arXivpreprintarXiv:2310.08528,2023a.
Wu,J.Z.,Li,X.,Gao,D.,Dong,Z.,Bai,J.,Singh,A.,Xiang,X.,Li,Y.,Huang,Z.,Sun,Y.,He,R.,
Hu,F.,Hu,J.,Huang,H.,Zhu,H.,Cheng,X.,Tang,J.,Shou,M.Z.,Keutzer,K.,andIandola,F.
Cvpr2023textguidedvideoeditingcompetition. arXivpreprintarXiv:2310.16003,2023b.
Yang,Z.,Gao,X.,Zhou,W.,Jiao,S.,Zhang,Y.,andJin,X.Deformable3dgaussiansforhigh-fidelity
monoculardynamicscenereconstruction. arXivpreprintarXiv:2309.13101,2023.
Yu,A.,Li,R.,Tancik,M.,Li,H.,Ng,R.,andKanazawa,A. Plenoctreesforreal-timerenderingof
neuralradiancefields. arXivpreprintarXiv:2103.14024,2021.
Yu,Q.,Wang,H.,Qiao,S.,Collins,M.,Zhu,Y.,Adam,H.,Yuille,A.,andChen,L.-C.kmax-deeplab:
k-meansmasktransformer. arXivpreprintarXiv:2207.04044,2022.
Yu, Q., He, J., Deng, X., Shen, X., and Chen, L.-C. Convolutions die hard: Open-vocabulary
segmentationwithsinglefrozenconvolutionalclip. NeurIPS,2023a.
Yu,Q.,Shen,X.,andChen,L.-C. Towardsopen-endedvisualrecognitionwithlargelanguagemodel.
arXivpreprintarXiv:2311.08400,2023b.
14Yu,Z.,Chen,A.,Huang,B.,Sattler,T.,andGeiger,A.Mip-splatting:Alias-free3dgaussiansplatting.
arXivpreprintarXiv:2311.16493,2023c.
Zhang,L.,Rao,A.,andAgrawala,M. Addingconditionalcontroltotext-to-imagediffusionmodels.
InICCV,2023.
Zhi,S.,Laidlow,T.,Leutenegger,S.,andDavison,A.J. In-placescenelabellingandunderstanding
withimplicitscenerepresentation. InICCV,2021.
Zhou,A.,Kim,M.J.,Wang,L.,Florence,P.,andFinn,C. Nerfinthepalmofyourhand: Corrective
augmentationforroboticsvianovel-viewsynthesis. arXivpreprintarXiv:2301.08556,2023.
15Appendix
Intheappendix,weprovideadditionalinformationaslistedbelow:
‚Ä¢ AppendixAprovidesmorerelatedworks.
‚Ä¢ AppendixBprovidesthedetailsofuseddatasetsandmetrics.
‚Ä¢ AppendixCprovidestheimplementationdetails.
‚Ä¢ AppendixDprovidesextensiveablationstudyontheproposedVideo-3DGS.
‚Ä¢ AppendixEprovidesqualitativeresultsonvideoreconstructionandvideoediting.
‚Ä¢ AppendixGdiscussesourmethod‚Äôslimitations.
‚Ä¢ AppendixFprovidesthelicensesoftheassetsused.
‚Ä¢ AppendixHdiscussesbroaderimpact.
A RelatedWork
Inthispart,wefurtherintroducetheliteraturerelatedtothecorepartofVideo-3DGS(1ststage):
neuralrenderingforscenerepresentation(AppendixA.1),3DGaussianSpatting-basedrepresentation
(AppendixA.2),dynamicscenerepresentation(AppendixA.3),andspatialtemporaldecomposition
inneuralrendering(AppendixA.4).
A.1 NeuralRenderingforSceneRepresentation
Theneuralradiancefield(NeRF) (Mildenhalletal.,2021)isoneoftheadvancedmethodologies
thatrepresentscenesusingimplicitneuralrenderingwithMLP.Ithasseensignificantadvancement
recently,withmajorimprovementsinseveralkeyareas. Firstly,thespeedofbothtraining(Karnewar
etal.,2022)andinference(Garbinetal.,2021)hasbeensubstantiallyincreased,allowingthecurrent
modelstobetrainedanddeployedwithinminutes(Barronetal.,2021;Yuetal.,2021;Reiseretal.,
2021;Martin-Bruallaetal.,2021). Furtheradvancementshaveaddressedimageartifacts,including
motionblur(Maetal.,2022),exposureandlensdistortion(Jeongetal.,2021),enhancingthequality
and realism of rendered scenes. The necessity for precise camera calibrations is becoming less
stringent,thankstonewmethodsthatallowforlocalcamerarefinement(Chngetal.,2022;Linetal.,
2021). MorerecentresearchhasalsoextendedNeRFapplicationsbeyondtraditionaluses,exploring
areassuchasgeneralization(Wangetal.,2022;Chenetal.,2021a),semanticunderstanding(Zhi
etal.,2021;Kunduetal.,2022),robotics(Zhouetal.,2023),and3Dimagestyletransfer(Guetal.,
2021). ThesedevelopmentsindicateabroadeningscopeandincreasingversatilityofNeRFmodelsin
capturingandrenderingcomplexscenes.
A.2 3DGaussianSpatting-basedRepresentation
BuildingupontheadvancementsinNeuralRadianceFields(NeRF)(Kerbletal.,2023)forscene
representation,anotherinnovativeapproachthathasgainedattentionistheutilizationof3DGaussian
distributionstomodelscenes. Thismethodology,oftenreferredtoas‚ÄúGaussianSceneRepresen-
tation‚Äù, leveragesthemathematicalpropertiesofGaussiandistributionstoefficientlycapturethe
volumetricdensityandcolorinformationof3Dscenes. SimilartothetrendinNeRF,therehave
beenseveraladvancementsinperformanceandefficiency. Forexample, someofthemapplythe
moduleusedinNeRFto3DGaussian-basedviewsynthesis. (Leeetal.,2024)leveragesacompact
MLPtoadjustthecovarianceof3DGaussians,whichcaneffectivelyreconstructsharpdetailsfrom
blurryimagesandMip-splatting(Yuetal.,2023c)addressaliasissuein3DGSassimilarto (Barron
etal.,2021). Besidestheadvancementsinquality, (Fanetal.,2023)enhancestheefficiencyand
compactnessof3DGaussianSplattingforneuralrenderingbypruninginsignificantGaussians.
A.3 HandlingDynamicSceneRepresentation
DespitethesignificantprogressinNeRFand3DGSforstaticscenerepresentation,bothapproaches
encountersubstantialchallengeswhenappliedtodynamicscenes. Thecorelimitationstemsfrom
theirinherentdesign,whichisprimarilytailoredforstaticenvironments,thusstrugglingtoadaptto
16changesinscenegeometryandappearanceovertime. Toaddressthesechallenges,recentresearch
endeavorshavebegunexploringextensionsandalternativestoNeRFand3DGaussianSplatting
thatincorporatetemporalinformationintothescenemodelingprocess. Forinstance,deformation
networkhasbeenwidelyusedforencodingtimedimensionbothintheframeworkofNeRFand
3DGS, (Fanetal.,2023)and (Yangetal.,2023)respectively. Moredevelopmentsfordynamic
scenerepresentationareintroducedinNeRF.Robust-Dyn(Liuetal.,2023)enhancestherobustness
ofdynamicradiancefieldreconstructionbyjointlyestimatingstaticanddynamicscenecomponents
along with camera parameters. NeRV Chen et al. (2021b) utilizes neural implicit representation
to encode videos by simply fitting a neural network to input video frames. Yet, despite their
advancements,duetothefundamentallimitationintheNeRFframework,theseapproachescontinue
tograpplewithchallengesrelatedtoachievingfinedetail,maintainingtemporalconsistencyacross
frames,andrequiringextensiveoptimizationtimes. Drivenbythesechallenges,theproposedVideo-
3DGSseekstoinvestigatetheapplicationof3DGSfordynamicscenescapturedthroughmonocular
video,exploringitsefficacyandpotentialimprovements.
A.4 SpatialTemporalDecomposition
OurproposedVideo-3DGSdecomposesthedynamicvideospatiallyandtemporallytorepresent
originalvideoeffectively. Similarly,ProgressiveNeRF(Meulemanetal.,2023)proposesmethod
ofdecomposingvideointospatialandtemporalcomponentsthroughprogressivelyoptimizedlocal
radiancefields. Itrepresentsasignificantadvancementinthefieldofrobustviewsynthesisandlarge-
scalescenereconstruction. TheprimarydistinctionbetweenourmethodandtheProgressiveNeRF
approach lies in how we manage camera poses and frame integration across video clips. While
ProgressiveNeRF dynamically updates camera poses and continues to append frames until the
camera‚Äôsposeextendstotheboundaryofahigh-resolution,uncontractedspace,ourmethodtreats
eachclipindependently. Weutilizeoverlappingframessolelytoensureconsistencyof3DGaussians
correspondingtooverlappingframesacrossneighboringclips,witheachclipmaintainingitsunique
cameraposes. However,allclipscanstillberenderedontothesameimageplaneviaadifferentiable
renderingwitheachdistinctive3DGaussianandcamerapose. Additionally,ourprocessrepeatsuntil
MC-COLMAPsignalsa‚ÄòSuccess‚Äôstatusmarkingacleardifferenceinthecriteriausedtodetermine
thesufficiencyofframesforeachclip. Furthermore,ProgressiveNeRFusesseveraldependencies
(e.g.,depthestimationandopticalflow)andmanylossesforbettertemporaldecomposition. However,
weonlyneedsegmentationmasksandsimplereconstructionlosses(L1andSSIM)asintheoriginal
3DGS paper. We also provide extensive visual comparison with ProgressiveNeRF in this link:
https://video-3dgs-progressivenerf.github.io/.
B DatasetsandMetrics
B.1 Datasets
VideoReconstruction Toassessthequalityofvideoreconstructioninreal-worldmonocularvideo
scenes,wecuratedadatasetcomprising28representativevideosfromtheDAVISdataset(Pont-Tuset
et al., 2017), each with a resolution of 480p. These videos feature a varied array of foreground
movingobjects,encompassinghumans,vehicles,andanimals,togetherwithclutteredbackgrounds.
BelowsarethekeyfeaturesofDAVISdataset:
‚Ä¢ Resolution: videosareavailableintworesolutions,480pand1080p. Followingpriorworks,
weutilized480p.
‚Ä¢ Eachvideohasadifferentnumberofframesrangingfrom40to80
‚Ä¢ FPS:24
‚Ä¢ Videosequences: thedatasetincludes50videosequences. Amongthem,weselected28
videos that feature a varied array of foreground-moving objects, encompassing humans,
vehicles,andanimals.
VideoEditing Forvideoediting,weadoptthedatasetsandtextpromptsusedintheCVPR2023
workshop,LOVEUText-GuidedVideoEditing(TGVE)challenge(Wuetal.,2023b). Theycurateda
datasetconsistingof76videos(480p)in-the-wildfromDAVIS,Youtube,andVidevoVideos. Each
videoiseditedaccordingto4differenttypesoftextpromptsacquiredfromBLIP-2(Lietal.,2023),
17Type COLMAP MaskedCOLMAP MCCOLMAP
COLMAP successrate 20/28 25/28 28/28
processingtime 12m30s 2m10s 2m3s
Type Deformable-3DGS Frg-3DGS Frg-3DGS+Bkg-3DGS Frg-3DGS+Bkg-3DGS
3DGS iteration/trainingtime 40k(50m) 10k(4m) 30k(15m) 60k(34m) 10k(8m) 30k(34m) 3k(11m) 5k(21m)
PSNR 30.6 32.9 35.3 37.3 35.1 38.0 37.6 41.2
Table3: AblationstudyonMC-COLMAP.Thecoloredcellsindicatethebest,secondbest,andthird
bestperformances(bestviewedincolor). OurMC-COLMAPnotonlysuccessfullygenerates3D
pointsforall28videos,butalsoenablesthehighreconstructionquality.
Scene blackswan boat drift-turn horsejump-high
Clip#1 17(1) 11(1) 10(1) 10(1)
Clip#2 11(1) 13(1) 11(1) 11(1)
Clip#3 11(1) 17(1) 11(1) 11(1)
Clip#4 11(1) 20(5) 11(1) 11(1)
Clip#5 4 22 11(1) 11
Clip#6 - - 11(1) -
Clip#7 - - 8 -
Table4: AnalysisonstatisticsofMC-COLMAP.Weshowthenumberofframesforeachclipin
differentscenes. Thenumberintheparenthesesreferstothenumberofoverlappingframesbetween
neighboringclips.
including: stylechange,objectchange,backgroundchange,andmultiplechange. Asnotedinthe
mainpaper,wetestedourframeworkin58videoswith4editingprompts,totaling232videoediting
scenarios. Belowarekeydetails.
‚Ä¢ Resolution: 480x480
‚Ä¢ Framenumber: DAVIS:32/Youtube:128/Videvo: 32
‚Ä¢ FPS:24
B.2 EvaluationMetrics
VideoReconstruction Toevaluatevideoreconstructionquality,twoprincipalmetricsareutilized:
PeakSignal-to-NoiseRatio(PSNR)andStructuralSimilarityIndexMeasure(SSIM).Thesemetrics
collectivelyappraisethefidelityofthereconstructedvideoincomparisontotheoriginal.Additionally,
efficiencyisgaugedthroughthemetricofTrainingTime,whichquantifiestherequiredtrainingtime,
thusindicatingthecomputationalefficiencyofthereconstructionalgorithm.
VideoEditing Toassessvideoeditingquality,weutilizeWarpSSIM,whichcomputesthemean
SSIM score between the edited video warped by the optical flow (Teed & Deng, 2020) (derived
fromthesourcevideo)andcorrespondingoriginaleditedvideo. Thismetricoffersvaluableinsight
intothetemporalconsistencyofpost-editing. Furthermore,weemployQ (Karaetal.,2023),a
edit
comprehensivevideoeditingmetricthatcombinestheWarpSSIMscorewiththeCLIPScore_text,
providingamultiplicativeassessmentoftheoverallvideoeditingperformance.
C ImplementationDetails
OurframeworkisbasedonPytorch(Paszkeetal.,2019)and3DGaussianSplatting(Kerbletal.,2023),
whileadoptingdepthincorporateddifferentiableGaussianrasterization(Yangetal.,2023). During
training,twosetsof3DGaussians(bothFrg-3DGSandBkg-3DGS)ineachcliparesequentially
optimizedwiththreedifferenttotalnumbersoftrainingiterations: atotalof3k,5kand10kiterations,
eachtargetingadistincttime-accuracytrade-off(e.g.,10kiterationsforthebestqualitybuttheslowest
training time). We employ the same training hyper-parameters (e.g., learning rate, deformation
networksetting)asspecifiedby(Yangetal.,2023). Withoptimized3DGaussiansforeachclip,video
reconstructionproceedsthroughsequentialdifferentialrendering(Kerbletal.,2023)oftheclips. For
videoediting,3DGaussiansoptimizedfororiginalvideosundergofine-tuningfor1kiterationsto
updatethesphericalharmoniccoefficients(SH)andopacity(œÉ),guidedbytheinitialeditedvideos
(obtainedfromanoff-the-shelfzero-shotvideoeditor). EachexperimentusesasingleA100GPU.
18Metrics
PSNR SSIM
Method
Frg-3DGS(w/fore) 35.1 0.923
Frg-3DGS(w/rdn) 36.5 0.951
Frg-3DGS(w/fore+rdn) 37.5 0.957
Frg-3DGS+Bkg-3DGS 41.4 0.987
Table5: AnalysisonFrg-3DGSandBkg-3DGS.‚Äòfore‚Äôand‚Äòrnd‚Äôdenotetheforegroundandrandom
3Dpoints,respectively. UtilizingasinglesetofFrg-3DGScanbenefitfrombothtypesof3Dpoints.
However,performancecanbeenhancedfurtherbyemployingtwosetsofFrg-3DGSandBkg-3DGS,
tailoredspecificallyforforegroundandbackgroundrandom3Dpoints,respectively.
D AblationStudy
D.1 Video-3DGS(1stStage)
Analysis on MC-COLMAP We present a comprehensive analysis of MC-COLMAP. Table 3
comparestwobaselines: conventionalCOLMAP,whichprocessesfullvideoframes,andMasked
COLMAP,whichemploysonlyspatialdecomposition. ConventionalCOLMAPencountersmore
failurecase(8outof28videosfailtoobtain3Dpoints)andexhibitsslowerprocessingtime(12m30s).
Among the successful cases (20 videos), reconstruction using the state-of-the-art Deformable-
3DGS (Yang et al., 2023) performs suboptimally in both training time and PSNR compared to
ourFrg-3DGS,poweredbymulti-resolutionhashencoding-baseddeformationnetwork. Itisworth
notingthatthisstudyutilizesonlyonesetof3DGaussians,withoutspatialdecompositionforfore-
groundandbackgroundpoints. MaskedCOLMAPdemonstratesimprovedsuccessrate(only3out
of28videosfailtoobtain3Dpoints)andenhancestheprocessingspeedbymitigatingtheeffects
of cluttered backgrounds through spatial decomposition. Additionally, reconstruction quality, as
measuredbyPSNR,isimproved. Finally,ourproposedMC-COLMAP,whichincorporatesboth
spatialandtemporaldecomposition,successfullycaptures3Dpointsfromall28videos. Furthermore,
it enables the best reconstruction quality of 41.2 PSNR with 5k training iterations, significantly
outperformingtheothersettings.
MC-COLMAPStatistics AsshowninTable4,toenhancecomprehensionofMC-COLMAP,we
presentstatisticsthatillustratethedistributionofcliplengthacrossfourrepresentativescenesfrom
thechallengingDAVISdataset. Wehaveselectedadefaultvalueofk=10forourexperimentsand
foundoutthatalteringktoeither5or15doesnotmarkedlyimpactourresults. Wesynchronize
different3DGSsetsacrossvideoclipsthroughtheuseofoverlappingframes,whichareindicatedin
theabovetableasnumberswithinparentheses.Initially,wedesignatethecountofoverlappingframes
tobeone,althoughthisnumbercanbeadjustedbasedonthenumberofframesaddedtoeachclip.
Italsoshowsbehaviorsforsloworfastmotioninvideo. Forexample,thescenes‚Äúblackswan‚Äùand
‚Äúboat‚Äùfeaturecomparativelystaticandslowmovements,whereas‚Äúdrift-turn‚Äùand‚Äúhorsejump-high‚Äù
arecharacterizedbyhighlydynamicactions. Eachscenebeginswithaninitialsetof10framesfor
MC-COLMAPprocessing. Itisnotedthatfast-movingscenesaretypicallydecomposedinto10or
11frames. Incontrast,sceneswithslowermotiondemandagreaternumberofframestoaccurately
extract3DpointsforMC-COLMAPanalysis.
AnalysisonFrg-3DGSandBkg-3DGS InTable5,wedemonstratethenecessityofemployingtwo
setsofFrg-3DGSandBkg-3DGSforvideoreconstruction. WebeginwithasinglesetofFrg-3DGS
thatleveragesonlytheforeground3Dpoints(denotedasFrg-3DGSw/fore),resultingin35.1PSNR
and0.923SSIM.ByinitializingthesinglesetofFrg-3DGSwiththespherical-shapedrandompoints
(denoted as Frg-3DGS w/ rdn), the performance improves to 36.5 PSNR and 0.951 SSIM. The
inclusion of random 3D points uniformly captures both foreground and background points, thus
enhancingperformance. Furthermore,employingbothforeground3Dpointsandspherical-shaped
random3Dpoints(Frg-3DGSw/fore+rdn)yieldsfurtherimprovement. Finally,utilizingtwosets
ofGaussians, Frg-3DGSandBkg-3DGS,eachspecifictoforegroundandbackground3Dpoints
respectively,significantlyenhancesperformanceto41.4PSNRand0.987SSIM.
2DLearnableParameterforMergingForegroundandBackgroundViews Wevisualizedthe
2Dlearnableparameter(i.e.,Œ±inEquation(5))indifferentvideoscenariosinthefollowinglink:
https://video-3dgs-2d-learnable-mask.github.io/. Asshowninthevideos,thelearned
Œ±focusesontheforegroundregions,whilethelearned(1-Œ±)focusesonthebackgroundregions.
19Text2Video-Zero +Video-3DGS TokenFlow +Video-3DGS RAVE +Video-3DGS
Editingtime 1m11 +6m7s 2m50s +6m7s 7m20s +6m5s
Table6: AblationstudyontimecostofVideo-3DGS(‚Äòstylechange‚ÄôinDAVIS).
Dataset
Method WarT pe Sx St2 IMVi ‚Üëd-Ze Qro
edit‚Üë
Recu+ rsV ivid eeo- E3D nsG emS
bled
WarpSSIM‚Üë Qedit‚Üë
0.827 21.0
‚úì 0.842 21.4
DAVIS 0.691 20.1 ‚úì 0.897 21.4
‚úì ‚úì 0.899 22.3
Table7: AnalysisonthecomponentsofrecursiveandensembledVideo-3DGS.
ModelSize Theoverallsizeofthemodelisdeterminedbyboththenumberofclipsandthesize
oftheFrg-3DGSandBkg-3DGSforeachclip. Forexample,considerthe‚Äúblackswan‚Äùscene. the
Video-3DGS(3kiteration)necessitates,onaverage,a15MBFrg-3DGSanda60MBBkg-3DGS
perclip. Consequently,thecumulativemodelsizeamountsto375MB.Incontrast,theconventional
3DGS(30kiteration)employsasingular,substantial3DGSmodel,approximately300MBinsize,
whichisaresultofthedensificationtechniqueemployedduringlongiterations. Despitetherelatively
bigmodelsizerequiredfor3DGS,sinceitrepresentsandstoresthesceneinanexplicitmanner(point
cloud),wecantakeadvantageoftheefficienttrainingandinferencecapabilityof3DGSinvideos.
ItshouldbenotedthattheaggregatesizeoftheVideo-3DGSmodelescalateswithanincreasing
numberofclips. Therefore,apivotalobjectiveforfutureresearchistominimizetheoverallsize
requisiteforVideo-3DGSimplementation. Oneofthenaivewaysofachievingthisisoptingfor
alargerkvaluesothatwecaneffectivelyreducethefinalmodelsize,asthisleadstoadecreased
numberofclips.
D.2 Video-3DGS(2ndStage)
TimeCostAnalysis SimilartoTable1,wealsoexplorethetimeefficiencyofvideoeditingwhen
incorporatingVideo-3DGSatopexistingzero-shotvideoeditingframeworksinthissectioninTable6.
Thisassessmentspansthreecriticalphases: optimizingFrg-3DGSandBkg-3DGSwiththeinitial
video, updating SH and œÉ for the initially edited video, and rendering to produce a refined edit.
WecandemonstratethatintegratingVideo-3DGSasanenhancementmoduleconsistentlyrequires
approximately6minutes,irrespectiveoftheunderlyingeditingtechniques,affirmingitsefficiencyas
areasonableinvestmentinprocessingtime.
AnalysisonRecursiveandEnsembledRefinement AfterobservingtheeffectivenessofVideo-
3DGS(RE)inTable2,weconductedanablationstudyonitscomponents,asshowninTable7. Each
componentindividuallyimprovesperformancecomparedtoasingle-phaserefiner. The‚ÄòRecursive‚Äô
componentincreasesWarpSSIMby1.5%andQ by0.4%. Similarly,the‚ÄòEnsembled‚Äôcomponent
edit
enhancestherefiner,resultingina7%increaseinWarpSSIManda0.4%increaseinQ . Our
edit
Video-3DGS(RE),whichcombinesbothrecursiveandensembledstrategies,achievesthehighest
scoresinbothWarpSSIM(0.899)andQ (22.3).
edit
D.3 RelationshipbetweenVideo-3DGS(1stStage)andVideo-3DGS(2ndStage)
WeexaminethecorrelationbetweenvideoreconstructionandeditingqualityinTable8. Weintroduce
twoversionsofVideo-3DGS:modelA,whereallclipsutilizethesamedeformationfield(thusreduc-
ingtherepresentationcapacity),andmodelB,ourdefaultsetting. Asdepictedinthetable,modelB
achievessuperiorreconstructionqualitycomparedtomodelA,resultinginbetterperformanceacross
allthreeoff-the-shelfvideoeditors. Wealsoconductedqualitativecomparisonwith3DGS(Kerbl
etal.,2023)tofurtherprovethepositiverelationshipbetweenvideoreconstructionandvideoediting
inhttps://video-3dgs-recon-edit.github.io/.
E QualitativeResults
Inthissection,weprovidemorequalitativeresultsforbothvideoreconstruction(AppendixE.1)and
videoediting(AppendixE.2).
20Text2Video-Zero +Video-3DGS TokenFlow +Video-3DGS RAVE +Video-3DGS
Editing Reconstruction Editing Editing Reconstruction Editing Editing Reconstruction Editing
WarpSSIM/Qedit Model PSNR WarpSSIM/Qedit WarpSSIM/Qedit Model PSNR WarpSSIM/Qedit WarpSSIM/Qedit Model PSNR WarpSSIM/Qedit
0.791/21.1 A B 43 11 .. 31 0 0. .8 84 42 1/ /2 21 2. .7 8 0.869/23.2 A B 3 41 1.. 31 0 0. .8 98 14 6/ /2 22 4. .9 4 0.874/24.8 A B 3 41 1. .1 3 0 0. .8 97 01 5/ /2 23 5. .3 6
Table 8: The relationship between video reconstruction and video editing on DAVIS. Across all
threeoff-the-shelfvideoeditors(Text2Video-Zero,TokenFlow,andRAVE),modelB(withbetter
reconstructionability)yieldsbettervideoeditingresultsthanmodelA.
E.1 VideoReconstruction
Forvideoreconstruction, wevisualizequalitativeresultsinFigure7, Figure8, andFigure9. As
shown in the figures, the proposed Video-3DGS consistently demonstrates higher reconstruction
quality. Videocomparisonswith3DGS(Kerbletal.,2023)andDeformable-3DGS(Yangetal.,2023)
areprovidedinthefollowinglink: https://video-3dgs-additional-recon.github.io/
E.1.1 SpatialDecomposition
Wealsoprovidevisualanalysisonthespatialdecompositioninthefollowingvideolink: https:
//video-3dgs-spatial.github.io/. Ourmethodsuccessfullyextractsthecorresponding3D
pointsofmovingobjectsusingMC-COLMAP.Wealsofoundthatthespatialdecompositioncan
workwellinascenewithmultipleforegroundobjects.
E.2 VideoEditing
Forvideoediting,weprovidethevisualizationresultsforthe‚ÄòSingle-phaseRefiner‚Äôand‚ÄòRecursive
andEnsembledRefiner‚Äô.
E.2.1 Single-phaseRefiner
To show the visual effectiveness of Video-3DGS as single-phase refiner, we visualize qualita-
tive results in Figure 10, Figure 11, and Figure 12, which adopt Text2Video-Zero (Khachatryan
et al., 2023), TokenFlow (Geyer et al., 2023), and RAVE (Kara et al., 2023) as the underlying
videoeditor, respectively. Asshowninthefigures, Video-3DGSeffectivelyenhancesthetempo-
ral consistency in the edited results across all three video editors. We further provide the video
analysisofVideo-3DGSasaplug-and-playrefinertoshowbettervisualcomparisoninthislink:
https://video-3dgs-additional-edit.github.io/.
E.2.2 RecursiveandEnsembledRefiner
WefurtherconductthevisualanalysisonVideo-3DGSasRecursiveandEnsembled(RE)refiner.
First,similartoFigure3,weprovidemorevideoresultsshowingthesensitivityofvideoeditorsto
hyperparameterchangesinthislink:https://video-3dgs-re-hyperparameters.github.io/.
Itconfirmsthatdifferenthyperparametervaluesinthedeployedvideoeditorsresultsindifferent
outputs. Then,weshowtheeditingvisualimprovementbroughtbyVideo-3DGS(RE)inhttps:
//video-3dgs-re.github.io/.
F AssetLicenses
Thelicensesoftheassetsusedintheexperimentsaredenotedasfollows:
Datasets:
‚Ä¢ DAVIS2017(Pont-Tusetetal.,2017):https://davischallenge.org/index.html
‚Ä¢ LOVEU-TGVE-2023(Wuetal.,2023b):https://github.com/showlab/loveu-tgve-2023
Codes:
‚Ä¢ 3D Gaussian Splatting (Kerbl et al., 2023): https://github.com/graphdeco-inria/
gaussian-splatting
21‚Ä¢ Text2Video-Zero (Khachatryan et al., 2023): https://github.com/Picsart-AI-Research/
Text2Video-Zero
‚Ä¢ TokenFlow(Geyeretal.,2023):https://github.com/omerbt/TokenFlow
‚Ä¢ RAVE(Karaetal.,2023):https://github.com/RehgLab/RAVE
Models:
‚Ä¢ Stable Diffusion v 1.5 Models (Rombach et al., 2022): https://huggingface.co/runwayml/
stable-diffusion-v1-5
G Discussion&Limitations
WhiletheproposedVideo-3DGSexhibitsremarkableperformanceinbothvideoreconstructionand
editingtasks,certainchallengespersist. Invideoreconstruction,faithfulreconstructionbecomes
challengingwhenforegroundobjectsexhibitextremelylargemotion. Invideoediting,unsatisfactory
resultsmayariseiftheunderlyingvideoeditorfailsorifsignificantchangestoobjectshapesare
required,whichcontradictstheinherentpropertyof3DGaussiansprioritizingthepreservationof
original structure. Nevertheless, our work represents a pioneering effort in extending 3DGS to
enhance the temporal consistency of initially edited videos from any zero-shot video editor. We
envisionthatthiswillinspirefurtherresearchinazero-shotvideoeditingcommunity. Unlikeother
conventionalneuralrepresentationpapers,ourworkdoesnotexplorethetaskofnovelviewsynthesis.
Instead,ourmethodfocusesonvideoreconstructionandeditingbyencodingandfittingvideoswith
3DGS.WeanticipatethatourVideo-3DGSwillalsoserveasafundamentalframeworkfor4Dnovel
viewsynthesisinfutureresearch.
H BroaderImpact
ThispaperpresentsVideo-3DGS,whichleverages3DGaussianSplattingtoreconstructandedit
dynamicmonocularvideos. Weanticipatesignificantpositivesocietalimpactsfromourmethod,asit
consistentlyperformswellinrepresentingdiversevideosandrefiningoutputsfromzero-shotvideo
editors. Weexpectittobenefitvariousapplications,suchasEntertainmentwithvideosynthesisand
AR/VR.
22GT video
NLA
CoDeF
Robust
-Dyn
3DGS
Deform-
3DGS
Video-
3DGS
(3k)
Video-
3DGS
(10k)
Figure7: Qualitativeresultsforvideoreconstruction.
23GT video
NLA
CoDeF
Robust
-Dyn
3DGS
Deform-
3DGS
Video-
3DGS
(3k)
Video-
3DGS
(10k)
Figure8: Qualitativeresultsforvideoreconstruction.
24GT video
NLA
CoDeF
Robust
-Dyn
3DGS
Deform-
3DGS
Video-
3DGS
(3k)
Video-
3DGS
(10k)
Figure9: Qualitativeresultsforvideoreconstruction.
25Original video
Initial edited video
(Text2Video-Zero)
+Video-3DGS
Text prompt: Text prompt:
A truck drifts on a muddy off-road track, sepia-toned. A man is dancing on a wave.
Original video
Initial edited video
(Text2Video-Zero)
+Video-3DGS
Text prompt: Text prompt:
Man wearing wet suit surfing on the ocean, golden hour lighting. An Audi Q7 goes on a snow trail, anime style.
Figure10: Qualitativeresultsofsingle-phaserefinerforvideoeditingbyadoptingText2Video-Zero.
26Original video
Initial edited video
(TokenFlow)
+Video-3DGS
Text prompt: Text prompt:
A car drifts on a snowy road. A man is surfing on a wave made of aurora borealis.
Original video
Initial edited video
(TokenFlow)
+Video-3DGS
Text prompt:
Text prompt:
The squirrel is climbing down a tree
Several goldfish swim in a tank, impressionist style.
in a wooded park, Van Gogh style
Figure11: Qualitativeresultsofsingle-phaserefinervideoeditingbyadoptingTokenFlow.
27Original video
Initial edited video
(RAVE)
+Video-3DGS
Text prompt: Text prompt:
A car driving on a birthday cake Eric Jones Santana & Ivana Ariel dancing salsa among people on the
roof of a skyscraper with city lights in the background.
Original video
Initial edited video
(RAVE)
+Video-3DGS
Text prompt: Text prompt:
A Delta Airlines aircraft descends onto the runaway A car drifts on a racetrack, neon lights.
during a cloudless morning, sepia toned.
Figure12: Qualitativeresultsofsingle-phaserefinerforvideoeditingbyadoptingRAVE.
28