IMAGE CONDUCTOR: PRECISION CONTROL FOR INTER-
ACTIVE VIDEO SYNTHESIS
YaoweiLi1,2, XintaoWang2, ZhaoyangZhang2:, ZhouxiaWang2,3,
ZiyangYuan2,4, LiangbinXie2,5,6, YuexianZou1(cid:0), YingShan2
1PekingUniversity2ARCLab,TencentPCG 3NanyangTechnologicalUniversity
4TsinghuaUniversity 5UniversityofMacau 6ShenzhenInstituteofAdvancedTechnology
ProjectPage: https://liyaowei-stu.github.io/project/ImageConductor/
ğŸª„
(a) Camera Transitions
A corgi dog.
A girlwithwavy hair.
lush greenery.
ğŸª„
(b) Object Movements
A couple.
A burningrose.
A jellyfish.
Figure1: OrchestratedResultsofImageConductor. ImageConductorenables
fine-grainedandaccurateimage-to-videomotioncontrol,includingbothcamera
transitionsandobjectmovements. Colorfullinesdenotemotiontrajectories.
ABSTRACT
Filmmakingandanimationproductionoftenrequiresophisticatedtechniquesfor
coordinatingcameratransitionsandobjectmovements,typicallyinvolvinglabor-
intensivereal-worldcapturing. DespiteadvancementsingenerativeAIforvideo
creation,achievingprecisecontrolovermotionforinteractivevideoassetgener-
ationremainschallenging. Tothisend,weproposeImageConductor,amethod
forprecisecontrolofcameratransitionsandobjectmovementstogeneratevideo
assets from a single image. An well-cultivated training strategy is proposed to
separatedistinctcameraandobjectmotionbycameraLoRAweightsandobject
LoRAweights. Tofurtheraddresscinematographicvariationsfromill-posedtrajec-
tories,weintroduceacamera-freeguidancetechniqueduringinference,enhancing
objectmovementswhileeliminatingcameratransitions. Additionally,wedevelop
atrajectory-orientedvideomotiondatacurationpipelinefortraining. Quantitative
andqualitativeexperimentsdemonstrateourmethodâ€™sprecisionandfine-grained
controlingeneratingmotion-controllablevideosfromimages,advancingtheprac-
ticalapplicationofinteractivevideosynthesis.
(cid:0)Correspondingauthor.:Projectlead.
1
4202
nuJ
12
]VC.sc[
1v93351.6042:viXra1 INTRODUCTION
Filmmakingandanimationproductionareessentialformsofvisualart. Duringthecreativeprocessof
videomedia,professionaldirectorsoftenrequireadvancedcinematographytechniquestometiculously
plan and coordinate camera transitions and object movements, ensuring storyline coherence and
refinedvisualeffects. Toachieveprecisecreativeexpression,thecurrentworkflowforvideomedia
orchestrationandproductionheavilyreliesonreal-worldcapturingand3Dscanmodeling,whichare
labor-intensiveandcostly.
Recentwork(Hoetal.,2022;Blattmannetal.,2023b;Girdharetal.,2023;Xingetal.,2023;Chen
etal.,2023;Blattmannetal.,2023a;Bar-Taletal.,2024;Brooksetal.,2024)exploresanAIGC-
basedfilmmakingpipelinethatleveragesthepowerfulgenerativecapabilitiesofdiffusionmodelsto
generatevideoclipassets. Despitetheseadvancements,generatingdynamicvideoassetsallowing
creators precisely express their ideas remains unusable, for: (1) Lacking of efficient generating
controlinterface. (2)Lackingoffine-grainedandaccuratecontrolovercameratransitionsandobject
movements.
Although several works have attempted to introduce motion control signals to guide the video
generationprocess(Yinetal.,2023;Wangetal.,2023b;2024;Wuetal.,2024),noneoftheexisting
methodssupportaccurateandfine-grainedcontroloverbothcameratransitionsandobjectmovements
(seeFig.1).
Infact, dataavailableontheinternetoftenmixesbothcameratransitionsandobjectmovements,
leadingtoambiguitiesbetweenthetwotypesofmotion. AlthoughMotionCtrl(Wangetal.,2023b)
usesadata-drivenapproachtodecouplecameratransitionsfromobjectmotion,itstilllacksprecision
andeffectiveness. Cameraparametersareneitherintuitivenorstraightforwardtoobtainforcinemato-
graphicvariations. Forobjectmovements,MotionCtrlusesParticleSfM(Zhaoetal.,2022),amotion
segmentationnetworkbasedonopticalflowestimation,whichintroducessignificanterrors. Addi-
tionally,groundtruthvideosannotatedbasedonmotionsegmentationnetworksstillcontaincamera
transitions,causinggeneratedvideostoexhibitunintendedcinematographicvariations. Decoupling
cinematographicvariationsfromobjectmovementsthroughdatacurationisinherentlychallenging.
Obtainingvideodatafromafixedcameraviewpoint,i.e.,videoswithonlyobjectmovements,is
difficult. Opticalflow-basedmotionsegmentationmethods(Teed&Deng,2020;Xuetal.,2022;
Zhaoetal.,2022;Yinetal.,2023;Wangetal.,2023b)struggletoaccuratelytrackmovingobjects
withouterrorsandfailtoeliminateintrinsiccameratransitionsinrealisticvideos. Overall,existing
methodsareeithernotfine-grainedornotsufficientlyeffective.
Inthispaper,weproposeImageConductor,aninteractivemethodforfine-grainedobjectmotion
andcameracontroltogenerateaccuratevideoassetsfromasingleimage. Effectivefine-grained
motioncontrolrequiresrobustmotionrepresentation. Trajectories,beingintuitiveanduser-friendly,
allowuserstocontrolmotioninvideocontentbydrawingpaths. However,alarge-scale,high-quality
open-sourcetrajectory-basedtrackingvideodatasetiscurrentlylacking. Toaddressthis, weuse
CoTracker(Karaevetal.,2023)toannotateexistingvideodataanddesignadatafilteringworkflow,
resultinginhigh-qualitytrajectory-orientedvideomotiondata.
Toaddressthecouplingofcinematographicvariationsandobjectmovementsinreal-worlddata,we
firsttrainavideoControlNet(Zhangetal.,2023)usingannotateddatatoconveymotioninformationto
theUNetbackboneofthediffusionmodel. Wethenproposeacollaborativeoptimizationmethodthat
appliesdistinctsetsofLow-RankAdaptation(LoRA)weights(Huetal.,2021a)ontheControlNetto
distinguishvarioustypesofmotion. Inadditiontothedenoisinglosscommonlyusedindiffusion
models, we introduce an orthogonal loss to ensure the independence of different LoRA weights,
enablingaccuratemotiondisentanglement.
Toflexiblyeliminatecinematographicvariationscausedbyill-posedtrajectories,whicharedifficult
to distinguish in LoRA, and to enhance object movement, we also introduce a new camera-free
guidance. Thistechniqueiterativelyexecutesanextrapolationfusionbetweendifferentlatentsduring
thesamplingprocessofdiffusionmodels,similartotheclassifier-freeguidancetechnique(Ho&
Salimans,2022).
Inbrief,ourmaincontributionsareasfollows:
â€¢ Weconstructahigh-qualityvideomotiondatasetwithprecisetrajectoryannotations,ad-
dressingthelackofsuchdataintheopen-sourcecommunity.
2ğ¼ğ‘šğ‘ğ‘”ğ‘’ ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡
elegant Conv Layers Sum 1. Video Collection
koi fish
gliding CLIP Transformers Trainable Â·Â·Â·
through (Spatial + Temporal) Freeze mixed camera
the dark.
2. CutsDetection & Selection
Â·Â·Â·
avoid cuts
3. MotionEstimation & Filtering
motion
Â·Â·Â· Â·Â·Â· Â·stÂ·reÂ·ngth
ğ‘! 4. Cropping & Tracking
ğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘  Â·Â·Â·
LoRA 5. Sampling & Gaussian Filter
LoRA
ğ‘€ğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆ’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘ğ‘ğ‘™ğ‘’
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡ Â·Â·Â· sd Â·Â· pe Â·Â· an rÂ·Â·s se
e
(a) (b)
Figure2: a)FrameworkofImageConductor. 3DUNetservesasthediffusionbackbone,while
imageControlNetandmotion-controllableControlNet(anditsLoRAweights)conveyappearance
andmotioninformation,respectively. Weprogressivelyfine-tunedifferentmodulesduringtrarning
phase (see Sec 2.4). b) Trajectory-oriented video motion data construction workflow. We
carefullycuratethedatatoensuredynamicandconsistentvideocontent,aswellasprecisetrajectory
annotations(seeSec2.2).
â€¢ WeintroduceamethodtocollaborativelyoptimizeLoRAweightsinmotionControlNet,
effectivelyseparatingandcontrollingcameratransitionsandobjectmovements
â€¢ Weproposecamera-freeguidancetoheuristicallyeliminatecameratransitionscausedby
multipletrajectoriesthatarechallengingtoseparatewithLoRAweights.
â€¢ Extensiveexperimentsdemonstratethesuperiorityofourmethodinpreciselyandfinely
motioncontrol,enablingthegenerationofvideosfromimagesthatalignwithuserdesires.
2 APPROACH
2.1 OVERVIEW
ImageConductoraimstoanimateastaticimagebypreciselydirectingcameratransitionsandobject
movementsaccordingtouserspecifications,producingcoherentvideoassets. Ourworkflowincludes
trajectory-orientedvideodataconstruction(Sec.2.2),amotion-awareimage-to-videoarchitecture
(Sec.2.3),controllablemotionseparation(Sec.2.4),andcamera-freeguidance(Sec.2.5).
Weuseuser-friendlytrajectoriestodefinetheintensityanddirectionofcameratransitionsandobject
movements. Toaddressthelackoflarge-scaleannotatedvideodata,wedesignadataconstruction
pipelinetocreateaconsistentvideodatasetwithappropriatemotion.
Usingthisdata,wetrainVideoControNet(Zhangetal.,2023)tosynthesizemotion-controllablevideo
content. Toeliminateambiguitiesbetweencameratransitionsandobjectmovements,weemploy
separatesetsofLoRAweights. First,wetrainwithcamera-onlyLoRAweightstocontrolcamera
transitions. Then,weloadtheseweightsanduseanewsetofobjectLoRAweightstodecoupleobject
movement,ensuringprecisecontrol. Wealsointroducealossfunctionwithorthogonalconstraintsto
maintainindependencebetweendifferentLoRAweights.
Toseamlesslyblendcameratransitionsandobjectmovements,weproposeacamera-freeguidance
techniquethatiterativelyextrapolatesbetweencameraandobjectmotionlatentsduringinference.
Fig.2(a)showsourframework,Fig.2(b)illustratesourdatacurationpipeline,andFig.3presents
thecoreideaofImageConductor.
32.2 TRAJECTORY-ORIENTEDVIDEOMOTIONDATACONSTRUCTION.
Since Image Conductor relies on trajectories to guide motion, we need a dataset with trajectory
annotationstotrackdynamicinformationinvideos. Existinglarge-scalevideodatasetstypically
lack such annotations. While some methods use motion estimators to annotate video data, these
approachesoftensufferfrominaccuracies(Yinetal.,2023;Wangetal.,2023b;Wuetal.,2024)or
lackgenerality(Wuetal.,2024). Moreover,almostallannotateddatasetswithtrajectoryannotations
arenotpubliclyavailable. Toaddressthis,weintroduceacomprehensiveandgeneralpipelinefor
generatinghigh-qualityvideodatawithappropriatemotionandconsistentscenes,asillustratedin
Fig.2(b).
VideoCollection. WeutilizetheWebViddataset(Bainetal.,2021),alarge-scalemixeddatasetwith
textualdescriptions,andtheRealestate10Kdataset(Zhouetal.,2018),acamera-onlydataset,forour
research. TheImageConductoraimstodecoupleobjectmovementsfrommixeddata,requiringscene
consistencyandhighmotionquality. Toensuretemporalquality,weprocesstheWebViddataset
bydetectingcutsandfilteringmotion. FortheRealestate10Kdataset,wefocusonthediversityof
cameratransitionsandgeneratevideocaptionsusingBLIP2(Lietal.,2023)byextractingframesat
specificintervalsandconcatenatingtheirdescriptions.
Cuts Detection and Selection. In videos, cuts refer to transitions between different shots, and
generativevideomodelsaresensitivetosuchmotioninconsistencies(Blattmannetal.,2023a). To
avoidcutsandabruptscenechanges,whichcancausethemodeltooverfitthesephenomena,we
firstuseacutdetectiontool1 toidentifycutswithinthevideodataset. Wethenselectthelongest
consistentscenesasourvideoclips,ensuringsceneconsistency.
Motion Estimation and Filtering. o ensure the dataset exhibits good dynamics, we use
RAFT(Teed&Deng,2020)tocomputetheopticalflowbetweenadjacentframesandcalculatethe
Frobeniusnormasamotionscore. Wefilteroutthelowest25%ofvideosamplesbasedonthisscore.
Toreducecomputationalcost,weresizetheshortersideofthevideosto256pixelsandrandomly
samplea32-framesequencewithatemporalintervalof1to16frames. These32framesareusedas
thetrainingdataset,andtheirmotionscoresarecomputedforsamplefiltering.
CroppingandTracking. Tostandardizethedimensionsofthetrainingdata,weperformcenter
croppingonthepreviouslyobtaineddata, resultinginvideoframesofsize 384Ë†256Ë†32. We
then employ CoTracker (Karaev et al., 2023), a tracking method towards dense point, to record
motionwithinthevideousinga16Ë†16grid. Comparedtoopticalflow-basedpointcorrespondence
methods(Teed&Deng,2020;Xuetal.,2022),trackingavoidsdrift-inducederroraccumulation,
providingamoreaccuraterepresentationofmotion. Aftertracking,weaccumulatepointtrajectories
bycalculatingthedifferencesbetweenadjacentpointswithinthesametrajectory. Thisresultsin
stackedflowmapscompatiblewiththeinputformatofControlNet(Zhangetal.,2023).
SamplingandGaussianFilter. oenhanceuserinteractionandusability,weusesparsetrajectories
formotionguidance. WeheuristicallysamplenPr1,8strajectoriesfromthedenseset,with8being
theupperlimit. Thevalueofnisrandomlyselected,andthenormalizedmotionintensityofeach
trajectoryisusedasthesamplingprobability. Theaccumulatedflowmapfromthesetrajectories
formsasparsematrix. Toavoidtraininginstabilitycausedbythesparsematrix,weapplyaGaussian
filtertothetrajectories,similartopreviousmethods(Yinetal.,2023;Wangetal.,2023b;Wuetal.,
2024). Throughthisdataprocessingpipeline, weconstructedatrajectory-orientedvideomotion
datasetcontaining130kmixedvideoswithcameratransitionsandobjectmovements,and62kvideos
withonlycameratransitions.
2.3 MOTION-AWAREIMAGE-TO-VIDEOARCHITECTURE
Image-to-VideoBackbone. AsillustratedinFig.2(a),weutilizeAnimatediff(Guoetal.,2023b)
equippedwithSparseCtrl(Guoetal.,2023a)forimagesasourpre-trainedimage-to-videofounda-
tionalmodel. ThismodelusestheCLIP(Radfordetal.,2021)textencodertoextracttextembeddings
1https://github.com/Breakthrough/PySceneDetect.
4Training Workflow GT Inference Workflow
Update Camera LoRA
Camera LoRA
Loss
Camraâˆ’ğ‘œğ‘›ğ‘™ğ‘¦ ğ‘€ğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆ’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘ğ‘ğ‘™ğ‘’ ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡ Pan Left Camera LoRA
A sunflower. ğ‘ğ‘œğ‘œğ‘š ğ‘‚ğ‘¢ğ‘¡
GT
Rocky coastline with
Camera LoRA Loss crashing waves.
Object LoRA
Camra&ğ‘‚ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘€ğ‘œğ‘¡ğ‘–ğ‘œğ‘›âˆ’ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘ğ‘ğ‘™ğ‘’ ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘™ğ‘ğ‘’ğ‘¡
Pan Left & ObjectMoveRight
Object LoRA
A herd of alpacas. Update Object LoRA ObjectMove
Camera Transitions Object Movements Camera Pose Freeze Freeze and Stopgrad Trainable
Figure3: Fine-grainedMotionSeparationMethod. a)Thetrainingprocessisdividedintotwo
stages. Initially,camera-onlydataisusedtoempowerthecameraLoRAwiththeabilitytocontrol
cameratransitions. Afterloadingthewell-trainedcameraLoRA,mixedmotiondataisusedtotrain
theobjectLoRA,refiningobjectmotioninformation. b)Duringinference,loadingdifferentLoRAs
providesthemodelwithvariouscontrolcapabilities.
c P R1Ë†d, which are then passed to the UNet (Ronneberger et al., 2015) backbone via cross-
txt
attentionmechanism. Theinputimage,servingasthefirstframe,isconcatenatedwithanall-zero
frame matrix and a mask identifier channel-wise to form c P RTË†4Ë†HË†W. Next, the video
img
SparseCtrl,avariantoftheControlNet(Zhang&Agrawala,2023)thatremovestheskip-connections
betweentheControlNetâ€™sandtheUNetencoderâ€™sinputlatents,isusedtoextractsimageinformation
fromc .
img
Motion-ControllableControlNet. Toextractmotioninformationfromtheannotatedtrajectory
inputc PRTË†2Ë†HË†W forcompositionofcameratransitionsandobjectmovementsinvideos,
trajs
we use ControlNet as the motion encoder to capture multi-level motion representations. This
ControlNetincorporatesdifferenttypesofLoRAweightstoguidetheimage-to-videogeneration
with user-desired camera transitions and object movements. Consistent with the observations of
SparseCtrl(Guoetal.,2023a),wefindthatremovingtheskipconnectionsbetweenthemainbranchâ€™s
andtheconditionalbranchâ€™sinputlatentsspeedsupconvergenceduringtraining.
2.4 CONTROLLABLEMOTIONSEPARATION
Theaimofourapproachistopreciselyseparatecameratransitionsandobjectmovementsinvideos,
enablingfine-grainedcontroloverthegenerationofvideoclipassertsthatmeetsuserexpectations.
Tothisend,weintroducedcameraLoRAÎ¸ andobjectLoRAÎ¸ intothemotionControlNetto
cam obj
guidethesynthesisofdifferenttypesofmotion. AsshownintheFig.3,duringthetrainingprocess,
weemployedacollaborativeoptimizationstrategy. First,weoptimizedthecameraLoRA,andthen,
weoptimizedtheobjectLoRAbasedontheloadedcameraLoRA.Duringtheinferencestage,the
modelloadsdifferentLoRAtocontrolcameratransitions(e.g.,zoomingout)andobjectmovements
(e.g.,twowavesadvancinginaspecifieddirection).
CameraTransitions. Sinceitisavailabletoobtaindatawithcamera-onlytransition,westraight-
forwardlytraincameraLoRAÎ¸ â€œÎ¸ `âˆ†Î¸ usingourourcarefullycultivatedcameramotion
cam 0 cam
dataset,endowingtheControNetwiththeabilitytodirectcinematographicvariations. Thestandard
diffusiondenoisingtrainingobjectiveisutilized:
â€œ â€°
L â€œE âˆ¥ÏµÂ´Ïµ ,t,c ,c ,c qâˆ¥2 , (1)
cam z0,cam,ctxt,cimg,ctrajs,Ïµâ€Np0,Iq,t Î¸campzt,cam txt img trajs 2
whereÎ¸ isthedenoiserwhereControlNetwithcameraLoRAloaded,z isthenoisylatentof
cam t,cam
videoswithonlycameratransitionattimestept,c ,c andc refertothetextprompt,image
txt img trajs
prompt,andconditionaltrajectory,respectively.
ObjectMovements. Duetothescarcityoffixed-camera-viewvideodatawithoutcinematographic
variations,weneedtodecoupleobjectmotionfrommixeddatawherebothcameratransitionsand
objectmovementsareexsist.Observingthatdistincttypesofmotionsharethesametrajectory,wecan
5furthertraintheobjectLoRAÎ¸ â€œÎ¸ `âˆ†Î¸ afterloadingthewell-trainedcameraLoRAweights,
obj 0 obj
i.e.,targetingthereconstructionofcameratransitionsandobjectmovementsintheoriginalvideo
contentfrommixeddata. Formally,weloadboththecameraLoRAandobjectLoRAsimultaneously
duringtrainingphase,andpreventgradientflowtothecameraLoRAviastopgradsgrÂ¨s:
Î¸ â€œÎ¸ `sgrâˆ†Î¸ s`âˆ†Î¸ . (2)
mixed 0 cam obj
Similarly,weoptimizetheobjectLoRAusingthestandarddiffusiondenoisingobjective:
â€œ â€°
L â€œE âˆ¥ÏµÂ´Ïµ ,t,c ,c ,c qâˆ¥2 , (3)
mixed z0,mixed,ctxt,cimg,ctrajs,Ïµâ€Np0,Iq,t Î¸mixedpzt,mixed txt img trajs 2
whereÎ¸ isthedenoiserwhereControlNetwithallLoRAloadedasinEq.2,z isthenoisy
mixed t,cam
latentofvideoswithcameratransitionandobjectmovementsattimestept.
OrthogonalLoss. ToencouragetheobjectLoRAtolearnconceptsdistinctfromthecameraLoRA
andtoacceleratetheconvergenceofthemodel,weproposeanorthogonallossasajointoptimization
objective. Specifically,weextractalllinearlayerweightsW andW fromthedifferentLoRAs
cam traj
andimposeanorthogonalityconstraintonthem:
â€œ â€°
L â€œE âˆ¥IÂ´W WT âˆ¥2 (4)
ortho Wi,camPWcam,Wi,trajPWtraj i,cam i,traj 2
whereI representstheidentitymatrix,W andW refertotheweightsofthei-thlinearlayer
i,cam i,traj
ofthecameraLoRAandobjectLoRA,respectively.
Inall,theoptimizationprocessisincremental. WefirstoptimizethecameraLoRAusingL ,and
cam
thenoptimizetheobjectLoRAusingL andL .
mixed ortho
2.5 CAMERA-FREEGUIDANCE
Whenusersaimtocontrolmultipleobjects,multipletrajectoriesoftenintroducecameratransitions.
Inspired by classifier-free guidance (Ho & Salimans, 2022), we propose a camera-free guidance
techniquetoflexiblyandseamlesslyenhancemotionintensitywhileeliminatingcameratransitions.
ÏµË† px ,cqâ€œÏµ px ,âˆ…q
Î¸0,Î¸trajs t Î¸0 t
`Î» pÏµ px ,cqÂ´Ïµ px ,âˆ…qq (5)
cfg Î¸0 t Î¸0 t
`Î» pÏµ px ,cqÂ´Ïµ px ,cqq,
trajs Î¸trajs t Î¸0 t
where Î¸ refer to the model with object LoRA and Î¸ is the model with pre-trained motion
trajs 0
ControlNet. Thefinaloutputlatentisderivedbyextrapolatingtheoutputsofthesetwocomponents.
3 EXPERIMENTS
3.1 COMPARISONSWITHSTATE-OF-THE-ARTMETHODS
We compare Image Conductor with exsisting state-of-the-art image-based or text-based motion
controllablevideogenerationmethods,namelyDragNUWA(Yinetal.,2023),DragAnything(Wu
etal.,2024)andMotionCtrl(Wangetal.,2023b).
Evaluationdatasets. Toindependentlyevaluatecameratransitionsandobjectmovements,weuse
twodistinctdatasets: 1)Camera-OnlyMotionEvaluationDataset: Weselect10cameratrajectories,
e.g.panleft,panright,panup,pandown,zoomin,zoomout,toevaluatecontrolovercinematographic
variations. 2)Object-OnlyMotionEvaluationDataset: Wedesign10variedtrajectories,including
straightlines,curves,shakinglines,andtheircombinations.
Qualitative Evaluation Fig. 4 displays some of our qualitative results. Compared to previous
methods(Yinetal.,2023;Wuetal.,2024;Wangetal.,2023b),ourapproachcaneffectivelycontrol
camera transitions and object movements. In terms of camera transitions, both DragNUWA and
DragAnythingfailtoachievethecameratransitionofpanningdownandthenupinthegenerated
video. Although Motionctrl-SVD is capable of generating the specified camera movement, it is
unabletodefinenaturalcontentchangesviatextprompts. Additionally,itcannotaccuratelydefine
theintensityofcamerachanges,andsometimesintroducesdistortionartifact.
6ğŸª„
(a) Camera Transitions
Pan Up then Down
Sunflowers.
ğŸª„
(b) Object Movements
Twocars.
Figure4: QualitativeComparisonsoftheproposedImageConductor. (a)CameraTransitions.
Our method can simultaneously utilize text, image, and trajectory prompts as control signals to
achievemorenaturalcontentandcameratransitions. (b)ObjectMovements. Apartfromourmethod,
otherapproachesincorrectlyconfuseobjectmovementswithcameratransitions.
Intermsofobjectmovements,bothDragNUWAandDragAnythingincorrectlyinterpretobjectmove-
mentascameratransition,resultingingeneratedvideosthatdonotmeetuserintentions. Inaddition,
themotiontrajectoriesoftheirgeneratedvideosareoftenpoorlymatchedtothedesiredtrajectories
preciselyduetotheerrorsintroducedbythelabeleddataset. Astrajectory-basedMotionCtrlrelies
onthetext-to-videomodel,wedirectlyusetextandtrajectorypromptstocontrolthegenerationof
thevideounderdifferentseed. Theresultsdemonstratethatitlacksfine-grainedcontroloverthe
generatedcontentduetoitsinabilitytouseimagesasconditions. Additionally, itstillexhibitsa
significantamountofcameratransitionratherthanobjectmovement. Inall,ourmethodiscapableof
accuratelyandfinelycontrollingvarioustypesofmotionutilizingtheseparatedLoRA.
Quantitative Evaluation As shown in the Tab. 1, compared to other methods, our proposed
Image Conductor achieves state-of-the-art quantitative performance. We measure our alignment
with the given trajectoies via the CamMC and ObjMC metrics, surpassing the baseline models
and demonstrating our precise motion control capabilities. At the same time, the FID and FVD
metricsillustratethatourgenerationqualitysurpassesothermodels,capableofproducingrealistic
videos. Furthermore,weinvite31participantstoassesstheresultsofDragNUWA,DragAnything
andImageConductor. Theassessmentincludesvideoquality,motionsimilarity. Participantsare
7
AWUNgarD
gnihtynAgarD
lrtCnoitoM
sruO
AWUNgarD
gnihtynAgarD
lrtcnoitoM
sruOğŸª„
(a) Personalized and Controllable Video Synthesis
A cat.
A mountain.
A tank34.
A jellyfish.
Figure5: ResultsofPersonalizedandControllableVideoSynthesis. Thepre-trainedbasemodel
andLoRAweightsaresourcedfromTuSun2,HelloObject3,andCardosAnime4checkpoint.
Table1: QuantitativeComparisonswithSOTAMethods. Weutilizeautomaticmetrics(i.e.,FID,
FVD,CamMC,ObjMC)andhumanevaluation(i.e.,overallperformance,samplequality,motion
similarity) to evaluate the performance. DN and DA denotes DragNUWA (Wu et al., 2024) and
DragAnything(Yinetal.,2023),respectively.
AutomaticMetrics HumanEvaluation
Method
FIDÃ“ FVDÃ“ CamMCÃ“ ObjMCÃ“ OverallÃ’ QualityÃ’ MotionÃ’
DN(Yinetal.,2023) 237.26 1283.85 48.72 51.24 31.8% 37.1% 27.7%
DA(Wuetal.,2024) 243.17 1287.15 66.54 60.97 6.5% 8.1% 6.3%
ImageConductor 209.74 1116.17 33.49 42.38 61.7% 54.8% 66.0%
alsoaskedtogiveanoverallpreferenceforeachcomparedpair. Thestatisticalresultsconfirmthat
ourgeneratedvideosnotonlyappearmorerealisticandvisuallyappealingbutalsoexhibitsuperior
motionadherencecomparedtothoseproducedbyothermodels.
3.2 PERSONALIZEDANDCONTROLLABLEVIDEOSYNTHESIS
SincethebaseT2Vmodelisnotfine-tuned,ourmethodnaturallypossessestheabilityforpersonalized
generationwhilemaintainingcontrollability.InFig.5,weloadedsomepersonalizedmodelstosample
videosusingtheprovidedprompt,guidancescaleanduser-specifiedtrajectories.Theresultsshowthat
ourmethodcanseamlesslyintegratewithopen-sourcecustomizationcommunities(e.g.,CIVITAI5)
andhaspowerfulcapabilitiesforgeneratingcontrollablevideocontentassets.
3.3 ABLATIONSTUDIES
EffectofDistinctLoRAWeights Tovalidatethatourcarefullydesignedinteractiveoptimization
strategycanseparatecameratransitionsandobjectmovementsthroughdistinctLoRAweights,we
usethesametrajectoryasinputtoguidedifferentLoRAtogeneratevideos. AsshowninFig.6,
loadingvariousLoRAweightsendowsthemodelwithdifferentcapabilities. Forinstance,avertically
2https://civitai.com/models/33194/leosams-pallass-catmanul-lora.
3https://civitai.com/models/121716/helloobjects.
4https://civitai.com/models/25399/cardos-anime.
5https://civitai.com/.
8Rocks and coastline.
Figure6: EffectofdistinctLoRAweights. Imageconductorenablesuserstoindependentlycontrol
cameraandobjectmotioninteractively.
Figure7: EffectofCamera-freeGuidance. Thecamera-freeguidanceapproachflexiblyenhances
objectmovementsduringinference.
upwardtrajectorycausesthevideotopanupwhenthecameraLoRAisloaded,anditcreatesupward
waveswhentheobjectLoRAisloaded.
EffectofCamra-freeGuidance AsshowninFig.7,usingcamera-freeguidancecanfacilitate
theseparationofobjectmovementsfromcameratransitionsinseveralchallengingexamples. When
camera-freeguidanceÎ» issetto1,i.e.,camera-freeguidanceisnotyetused,thegeneratedvideo
trajs
exhibits a unexpected pan left transformation. When the Î» is set to 1.1, the generated videos
trajs
exhibitreasonableobjectmovements,yetsomeartifactsstillremain. Astheguidanceincreases,the
movementsoftheobjectbecomesmoreapparentandclear.
4 CONCLUSION
Inconclusion,thispaperintroducesImageConductor,anovelapproachforpreciseandfine-grained
control of camera transitions and object movements in interactive video synthesis. We design
a training strategy and utilized distinct LoRA weights to decouple camera transition and object
movements. Additionally,weproposeacamera-freeguidancetechniquetoenhanceobjectmovement
control. Extensiveexperimentsdemonstratetheeffectivenessofourmethod,markingasignificant
steptowardspracticalapplicationsinvideo-centriccreativeexpression.
9
ARoL
aremaC
ARoL
tcejbO
ARoL
aremaC
ARoL
tcejbO
1=%$#"!ğœ†
1.1=%$#"!ğœ†
0.2=%$#"!ğœ†REFERENCES
MaxBain,ArshaNagrani,GÃ¼lVarol,andAndrewZisserman. Frozenintime: Ajointvideoand
imageencoderforend-to-endretrieval. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.1728â€“1738,2021.
OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur, YuanzhenLi, TomerMichaeli, etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXivpreprintarXiv:2401.12945,2024.
Georgios Batzolis, Jan Stanczuk, Carola-Bibiane SchÃ¶nlieb, and Christian Etmann. Conditional
imagegenerationwithscore-baseddiffusionmodels. arXivpreprintarXiv:2111.13606,2021.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023a.
AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
22563â€“22575,2023b.
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe
Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video
generation models as world simulators. 2024. URL https://openai.com/research/
video-generation-models-as-world-simulators.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo
Xing,YaofangLiu,QifengChen,XintaoWang,etal. Videocrafter1: Opendiffusionmodelsfor
high-qualityvideogeneration. arXivpreprintarXiv:2310.19512,2023.
Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao,
ByungEunJeon,YuweiFang,Hsin-YingLee,JianRen,Ming-HsuanYang,etal. Panda-70m:
Captioning70mvideoswithmultiplecross-modalityteachers. arXivpreprintarXiv:2402.19479,
2024.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,andDaniel
Cohen-Or. Animageisworthoneword: Personalizingtext-to-imagegenerationusingtextual
inversion. arXivpreprintarXiv:2208.01618,2022.
RohitGirdhar,MannatSingh,AndrewBrown,QuentinDuval,SamanehAzadi,SaiSakethRambhatla,
AkbarShah,XiYin,DeviParikh,andIshanMisra.Emuvideo:Factorizingtext-to-videogeneration
byexplicitimageconditioning. arXivpreprintarXiv:2311.10709,2023.
YuweiGuo,CeyuanYang,AnyiRao,ManeeshAgrawala,DahuaLin,andBoDai.Sparsectrl:Adding
sparsecontrolstotext-to-videodiffusionmodels. arXivpreprintarXiv:2311.16933,2023a.
YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Animatediff:
Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning. arXivpreprint
arXiv:2307.04725,2023b.
HaoHe,YinghaoXu,YuweiGuo,GordonWetzstein,BoDai,HongshengLi,andCeyuanYang.Cam-
eractrl: Enablingcameracontrolfortext-to-videogeneration. arXivpreprintarXiv:2404.02101,
2024.
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840â€“6851,2020.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: Highdefinition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
10EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021a.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021b.
NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
Rupprecht. Cotracker: Itisbettertotracktogether. arXivpreprintarXiv:2307.07635,2023.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,
2023.
AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels.
InInternationalConferenceonMachineLearning,pp.8162â€“8171.PMLR,2021.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,2021.
Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing
catastrophicforgettinginparameterefficienttuning. arXivpreprintarXiv:2402.18865,2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMICCAI,2015.
TimSalimansandJonathanHo. Progressivedistillationforfastsamplingofdiffusionmodels,2022.
MaximilianSeitzer. pytorch-fid: FIDScoreforPyTorch. https://github.com/mseitzer/
pytorch-fid,2020.
JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsupervised
learningusingnonequilibriumthermodynamics. InInternationalconferenceonmachinelearning,
pp.2256â€“2265.PMLR,2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020a.
YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
Advancesinneuralinformationprocessingsystems,32,2019.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020b.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020c.
ZacharyTeedandJiaDeng. Raft: Recurrentall-pairsfieldtransformsforopticalflow. InComputer
Visionâ€“ECCV2020: 16thEuropeanConference,Glasgow,UK,August23â€“28,2020,Proceedings,
PartII16,pp.402â€“419.Springer,2020.
ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,and
SylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges. arXiv
preprintarXiv:1812.01717,2018.
11Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang
Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint
arXiv:2402.01566,2024.
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang,YujunShen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion
controllability. arXivpreprintarXiv:2306.02018,2023a.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying
Shan. Motionctrl: Aunifiedandflexiblemotioncontrollerforvideogeneration. arXivpreprint
arXiv:2312.03641,2023b.
WejiaWu,ZhuangLi,YuchaoGu,RuiZhao,YefeiHe,DavidJunhaoZhang,MikeZhengShou,
YanLi, TingtingGao, andDiZhang. Draganything: Motioncontrolforanythingusingentity
representation. arXivpreprintarXiv:2403.07420,2024.
JinboXing,MenghanXia,YongZhang,HaoxinChen,XintaoWang,Tien-TsinWong,andYing
Shan. Dynamicrafter: Animatingopen-domainimageswithvideodiffusionpriors. arXivpreprint
arXiv:2310.12190,2023.
HaofeiXu,JingZhang,JianfeiCai,HamidRezatofighi,andDachengTao. Gmflow: Learningoptical
flowviaglobalmatching. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pp.8121â€“8130,2022.
ShengmingYin,ChenfeiWu,JianLiang,JieShi,HouqiangLi,GongMing,andNanDuan.Dragnuwa:
Fine-grainedcontrolinvideogenerationbyintegratingtext,image,andtrajectory. arXivpreprint
arXiv:2308.08089,2023.
LvminZhangandManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusionmodels.
arXivpreprintarXiv:2302.05543,2023.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.3836â€“3847,2023.
WangZhao,ShaohuiLiu,HengkaiGuo,WenpingWang,andYong-JinLiu. Particlesfm: Exploiting
densepointtrajectoriesforlocalizingmovingcamerasinthewild. InECCV,2022.
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnification:
Learningviewsynthesisusingmultiplaneimages. arXivpreprintarXiv:1805.09817,2018.
12A RELATED WORKS
VideoSynthesis. Withtheemergenceofmassivedata(Bainetal.,2021;Chenetal.,2024)and
thegradualperfectionofdiffusionmodeltheory(Hoetal.,2020;Songetal.,2020c;Batzolisetal.,
2021),deepgenerativemodelshavemaderemarkableprogress(Rombachetal.,2022;Galetal.,
2022;Zhang&Agrawala,2023;Wangetal.,2023a;Chenetal.,2023;Brooksetal.,2024). Despite
the significant achievements, current video generation methods (Wang et al., 2023a; Chen et al.,
2023;Brooksetal.,2024;Blattmannetal.,2023a;Girdharetal.,2023)stillexhibitrandomnessand
facechallengesingeneratinghigh-qualityvideoswithcontrollability,whichhindersthepractical
applicationofAIGC-basedvideogenerationmethods.
MotionControlinVideos. Recently,somestudieshaveintroducedadditionalcontrolsignals,such
astrajectories(Yinetal.,2023;Wuetal.,2024;Wangetal.,2023b),cameraparameters(Wangetal.,
2023b;Heetal.,2024),andboundingboxes(Wangetal.,2024),tocontrolvisualelementsinvideos,
i.e.,cameratransitionsandobjectmovements,thusachievinginteractivevideoassertsgeneration.
However,theylackthecapabilitytopreciselyandfinelymanipulatevisualelements,especiallywhen
itcomestoobjectmovements(Yinetal.,2023;Wangetal.,2023b). Inthispaper,wemeticulously
designatrainingstrategythatutilizesexistingdatatoachieveflexibleandprecisemotionseparation
andcontrol.
B PRELIMINARY
B.1 CONDITIONALVIDEODIFFUSIONMODEL
Formally, diffusion models consists of a forward process and a reverse process (Sohl-Dickstein
etal.,2015;Hoetal.,2020;Songetal.,2020c). TheforwardprocessisdefinedasaMarkovchain
thatprogressivelyaddsdistinctlevelsofGaussiannoisetothesignalx overaseriesoftimesteps
0
tPr0,Ts,untilthex iscompletelycorruptedtox â€Np0,Iq:
0 T
? ?
qpx |x qâ€œNp Î± x ,p1Â´Î± qIq, and qpx |x qâ€œNp Î±s x ,p1Â´Î±s qIq, (6)
t tÂ´1 t tÂ´1 t t 0 t 0 t
Here wÅ›e consider the variance-preserving setting (Song et al., 2020b) with 0 Äƒ Î±
t
Äƒ 1 and
Î±s â€œ t Î± whereÎ± isadecreasingsequence. Thereverseprocessisaparameter-containing
t iâ€œ1 i t
processdesignedtoiterativelydenoisethecorruptedsequencex :
T
ppx |x qâ€œNpÂµ px q,Ïƒ2Iq. (7)
tÂ´1 t t t t
Themeanandvarianceofthereverseprocesscanbedefinedas:
? ?
Î± p1Â´Î±s q Î±s p1Â´Î± q
Âµ px ,x qâ€œ t tÂ´1 x ` tÂ´1 t x , (8)
t t 0 1Â´Î±s t 1Â´Î±s 0
Ë†t Ë™ t
1Â´Î±s
Ïƒ2 â€œp1Â´Î± q tÂ´1 . (9)
t t 1Â´Î±s
t
HereweconsiderÏƒ2isanuntrainedtimedependentconstants(Hoetal.,2020;Nichol&Dhariwal,
t
2021),andx canbereparameterizedusingEq.6andestimatedusingv-prediction(Salimans&Ho,
0
2022)orÏµ-predictiontechniques(Hoetal.,2020).
Givenaninputconditionc,thegoaloftheconditionalvideodiffusionmodelistosampleavideo
sequence x â€œ tx1,x2,Â¨Â¨Â¨ ,xLu with L frames from the conditional probability distribution
0 0 0 0
ppx |cq. Specifically,Âµ px ,t,cqcanbecalculatedusingtheÏµ-prediction:
0 Î¸ t
Ë† Ë™
1 1Â´Î±
Âµ Î¸px t,t,cqâ€œ ?
Î±
x tÂ´ ? 1Â´Î±st Ïµ Î¸px t,t,cq , (10)
t t
whereÏµ isadenoisingUNetnetwork. Inthiscase,theÏµ isoptimizedviadenoisingscorematch-
Î¸ Î¸
ing(Song&Ermon,2019):
â€œ ? ? â€°
minE }ÏµÂ´Ïµ p Î± x ` 1Â´Î± Ïµ,t,cq}2 . (11)
Î¸
px0,cqâ€qpx0,cq,Ïµâ€Np0,Iq,t Î¸ t 0 t 2
13Figure8: Inherentvideocontentinconsistencybetweenthefirstframeandsubsequentframesinthe
basemodel.
B.2 LOW-RANKADAPTATION
Low-RankAdaptation(LoRA)(Huetal.,2021b)isaparameter-efficienttuningapproachusedto
acceleratemodelfine-tuningonincomingdata,whichcanpreventcatastrophicforgetting(Renetal.,
2024). Unliketrainingtheentiremodel,LoRAaddsapairofrank-decompositionmatricestothe
linearlayerweights, whichoptimizesonlythenewlyintroducedparametersandensuresthatthe
otherparametersarefixed. Mathematically,thenewweightsW1 PRmË†ncanbedefinedas:
W1 â€œW `âˆ†W â€œW `ABT, (12)
whereAPRmË†r andB PRnË†r areapairoflearnablematricesandr !minpm,nqistherankto
reducethecostoffine-tuning.
C EXPERIMENTAL DETAILS
C.1 IMPLEMENTATIONDETAILS.
WeuseAnimatediffv3(Guoetal.,2023b)combinedwithRGBSparseCtrl(Guoetal.,2023a)as
ourbasemodelforimage-to-videogeneration. WetrainonlythemotionControlNetwhilekeeping
theUNetbackboneweightsfrozen. ThemotionControlNetistrainedonourcultivatedsampled
16-framevideosequenceswitharesolutionof384Ë†256(Section2.2). BothcameraLoRAand
objectLoRAisoptimizedwithAdam(Kingma&Ba,2014)on8NVIDIATeslaV100GPUsfora
weekwithabatchsizeof64andalearningrateof1Ë†10Â´4. WeinitiallytrainthemotionControlNet
usingmixeddata. Subsequently,weutilizecamera-onlydataandmixeddatatoextractthecamera
LoRAandobjectLoRAweightsrespectively. Tofacilitateuserinput,wefollowastrategyoftraining
ondensetrajectoriesfirst,andthenfine-tuningthemodelonsparsetrajectories. Duringtheinference
phase,weuse25stepsofDDIMsampler(Songetal.,2020a). Unlessotherwisenoted,thescaleof
classifier-freeguidance(Ho&Salimans,2022)issetto8.5.
C.2 EVALUATIONMETRICS.
Tothoroughlyevaluatetheeffectivenessofourmethod,wefollowingMotionCtrl(Wangetal.,2023b)
toassessedtwotypesofmetrics: 1)Videocontentqualityevaluation. WeemployFrÃ©chetInception
Distance(FID)(Seitzer,2020),FrÃ©chetVideoDistance(FVD)(Unterthineretal.,2018)tomeasure
the visual quality and temporal coherenceand. The reference videos of FID and FVD are 5000
videosrandomlyselectedfromWebVid(Bainetal.,2021). 2)Videomotionqualityevaluation. The
Euclideandistancebetweenthepredictedandgroundtruthtrajectories,i.e.,CamMCandObjMC,
is used to evaluate the motion control. Unlike MotionCtrl, which uses particleSFM (Zhao et al.,
2022)toestimatethecameraposesofthepredictedvideoforcalculatingCamMC,wedirectlyextract
pixel-levelmovementtrajectoriestocomputeCamMCsimilartoObjMC.
14
1emarF
2
emarFD LIMITATIONS
Despiteourmodelcanfaithfullyproducemotioninformationbasedonuser-inputtrajectories,the
generatedqualityofcontentisconstrainedbythebasemodel. Forexample,asshowninFig.8,we
observethatalthoughAnimatediff(Guoetal.,2023b)withimageSparseCtrl(Guoetal.,2023a)
imposes strong constraints on the first frame, subsequent frames exhibit some inconsistencies in
coloranddetailcomparedtothefirstframe. Onepossiblesolutionistoconcatenatenoisyimage
latentstotheinputnoiseinadditiontousingtheimageconditioninginjectionmechanism,similarto
SVD(Blattmannetal.,2023a)andDynamiCrafter(Xingetal.,2023).
Anotherlimitationisthatdespitetextandimagepromptsgenerallycomplementingeachotherin
mostscenariosduringthevideogenerationprocess,iftheyconveydifferentmeanings,thequalityof
theoutputmaybecompromised.
15