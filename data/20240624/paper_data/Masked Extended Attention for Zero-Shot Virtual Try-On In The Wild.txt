MaX4Zero: Masked Extended Attention for
Zero-Shot Virtual Try-On In The Wild
NadavOrzech YotamNitzan UlysseMizrahi DovDanon AmitH.Bermano
TelAvivUniversity
Figure1:MaX4ZeroperformsVirtualTry-Onin-the-wild(forunseentargetimagesandgarments)withoutanyfine-tuning.
Givenatargetimage(top),andgarmentimage(bottom),andimageisgeneratedusingadiffusion-basedpriorthatreplacesthe
inputgarmentwiththeonealreadyworninthetarget(right).
ABSTRACT begeneratedpersuasively,itisdifficulttocontrolexactlywhich
VirtualTry-On(VTON)isahighlyactivelineofresearch,with jacketthepandashouldbewearingwithoutdriftingawayfromthe
increasingdemand.Itaimstoreplaceapieceofgarmentinanim- naturalimagemanifold.Aprominentexampleforwhereprecise
agewithonefromanother,whilepreservingpersonandgarment identityandhighrealismarecrucialisVirtualTry-On(VTON).
characteristicsaswellasimagefidelity.Currentliteraturetakes VTON, where a garment taken from one image replaces a gar-
asupervisedapproachforthetask,impairinggeneralizationand mentinanother,isahighlyactivefieldwithdirectandimmediate
imposingheavycomputation.Inthispaper,wepresentanovel commercialapplicationsforthefashion,retailande-commerce
zero-shottraining-freemethodforinpaintingaclothinggarment industries.State-of-the-artVTONliteratureexclusivelyemploys
byreference.Ourapproachemploysthepriorofadiffusionmodel supervision,employingasetofreference-target-resultimagesto
withnoadditionaltraining,fullyleveragingitsnativegeneralization finetunethegenerativepriortobetteradheretotheVTONtask.As
capabilities.Themethodemploysextendedattentiontotransfer wedemonstrate,thisapproachdoesnotgeneralizewelltounseen
imageinformationfromreferencetotargetimages,overcoming garments,impairingthecapabilitiesofthepowerfulbackboneprior
twosignificantchallenges.Wefirstinitiallywarpthereferencegar- throughcatastrophicforgetting.Instead,wearguecontemporary
mentoverthetargethumanusingdeepfeatures,alleviating"texture priorsalreadypossestheknowledgetohandlethetask,andonly
sticking".Wethenleveragetheextendedattentionmechanismwith requireproperguidance.Hence,weturntoimagecontrollitera-
carefulmasking,eliminatingleakageofreferencebackgroundand ture.Sincegarmentidentityisfundamentalinourcase,common
unwantedinfluence.Throughauserstudy,qualitative,andquanti- generativebasedimageeditingtechniques[Mengetal.2022]are
tativecomparisontostate-of-the-artapproaches,wedemonstrate unsuitable,astheydonotpreservethepatternsandnuancesofthe
superiorimagequalityandgarmentpreservationcomparedunseen fabricwell,aswedemonstrate.Personalizationmethods[Araretal.
clothingpiecesorhumanfigures.Codewillbeavailableatour 2023;Galetal.2022,2023;Nitzanetal.2022;Ruizetal.2023;Tewel
projectpage:https://nadavorzech.github.io/max4zero.github.io/ etal.2023;Yeetal.2023]fitthetaskcloser,howevertheytypically
requireper-garmenttraining,whichisunfeasibleforreal-world
scenarios.Inaddition,aswedemonstrate,theyareinferiorinterms
1 INTRODUCTION ofqualityforthespecificVTONcase.
In this paper we present MaX4Zero â€” the first Zero-Shot In-
Currentgenerativemodelsshowcaseremarkablerealismandun-
the-WildVirtualTry-Onapproach,realisticallyandfaithfullyaug-
precedented understanding of visual concepts, but at the same
mentingunseenreferenceitemswithouttraining.Followingrecent
time,theylackcontrol.Whileevenamotorcycleridingpandacan
1
4202
nuJ
12
]VC.sc[
1v13351.6042:viXraOrzech,etal.
personalizationandconsistencyliterature,weproposeemploying techniqueadditionallyintroducesanewpenaltytermtothediffu-
ExtendedAttention[Alalufetal.2023;Caoetal.2023;Geyeretal. sionlossbasedonattentionmaptotalvariation,aimingatfurther
2023]â€”anattentionmechanismmixinginformationbetweenim- preservinggarmentidentitythroughsharperattentionmaps.
agesduringthegenerationprocess,encouragingdataflowbetween
thereferenceandtargetimages,resultinginsemanticsimilarity 2.2 Imageeditingwithdiffusionmodels
betweenthetwo.Indoingso,weobservetwoprimarychallenges.
Diffusionmodels[Hoetal.2020;Nicholetal.2022;Rameshetal.
First,weseeaphenomenonthathasbeenreferredtointheliter-
2022;Rombachetal.2022a;Songetal.2022]haverapidlyadvanced
atureasTextureSticking[Karrasetal.2021];asstrongastheprioris,
thefieldofimageediting,demonstratingremarkableflexibilityand
somedetailsfromthereferenceimagearenotunderstoodcorrectly
qualityacrossvariousapplications.Perhapsthesimplesttechnique
andtreatedastexture,presenting"sticking"behavior,whereposi-
forimageeditingisinpainting[Lugmayretal.2022;Nitzanetal.
tionandorientationoffeaturesarenotdeterminedsemantically,
2024;Rombachetal.2022b;Wangetal.2023;Xieetal.2023]â€“the
butratherfromfeaturesinthetargetimage.Inordertomitigate
taskofcompletingamasked,ormissing,imageregion.
this,wewarpthereferenceimagetomatchthetargetinaninitial
In the context of modern diffusion models inpainting is per-
registrationstep.Thisencouragestheelementstobepositioned
formedbyrunningthedenoisingprocessconditionedonthenon-
correctlyduringtransfer.Thecorrespondencesforthedeformation
maskedpartsoftheimageandatextprompt
areextractedfromofthesameprior,andgapsbetweentheoriginal
Thisapproachisrestrictedtolocaledits,asonlythemasked
garmentandthedeformedonearealsoinpaintedusingtheprior.
regionmaybemodified,andcannotreuseinformationcontained
Thesecondchallengeisleakageofbackgrounddetailsfromthe
withinthemask.
referencegarmentimage,whichareclearlyirrelevanttotheVTON
Anotherangletoperformwhole-imageeditingrevolvesaround
process.Toalleviateleakage,weproposemaskingtheattentionof
inversiontechniques.Givenadiffusionmodelandanimage,inver-
unrelatedregionsintheextendedattention,preventingbackground
sionreferstotheprocessofrecoveringtheinputnoisewhichwould
regionsfromleakingintothemaingeneration.Together,thesesteps
leadtothegenerationoforiginalimageafterdenoising.Thesemeth-
balance the preservation of garment identity and details, while
ods,usuallyreferredtoundertheumbrellatermofDDIMInversion
leveragingthepriortoensuretheresultingimageisonthenatural
[DhariwalandNichol2021;Songetal.2022],requirecarefulse-
imagemanifold.
lectionoftherenoisingprocessinordertorecovertrulyinverted
Weevaluateourapproachagainststate-of-the-artVTON[Kim
noise,withexamplessuchasNull-textInversion[Mokadyetal.
etal.2023;Morellietal.2023a],imageediting[Yeetal.2023],and
2022],LEdits++[Bracketal.2023]orReNoise[Garibietal.2024].
paint-by-referencemethod[Chenetal.2024],anddemonstrate,
Somealsoperforminversiondirectlyinimage-spaceratherthan
throughqualitativeandquantitativeexperimentsandauserstudy,
latent-space,asseenin[InbarHuberman-Spiegelglas2023].All
therightbalancebetweenidentityandgarmentpreservationand
thesemethodsprimarilyutilizeglobaleditingtechniques.How-
resultingrealism.
ever,fortheVirtualTry-Ontask,maintainingtheintegrityofthe
backgroundisparamount.
Anotheravenueforeditingisthatofobjecttransfer,i.e.editing
animagebyaddingorreplacingobjectsintheimageusingasource
2 RELATEDWORK
imagecontainingtheobjects.Thisbynaturepreservestheidentity
2.1 Virtualtry-on ofthetransferedobjectentirely.Arecenttechniqueshowinggreat
promise,Anydoor[Chenetal.2024],achievesthisbybothencoding
Virtualtry-onhasalonghistoryofresearch[Shilkrotetal.2013],yet
thesourceobjectthroughanidentityencoder,andmergingseveral
hasonlyknownrecentleapsinqualitythroughtheuseofgenerative
levelsofdetailfrequenciestoobtainperturbationswhicharefed
models.Weheregooversomeofthemostrecentdevelopmentsand
intotheUNetactivations.
state-of-the-art.TryOnDiffusion[Zhuetal.2023],trainsaspecific
"Parallel-UNet",whichisabletoinparalleldiffuseaninputgarment
2.3 Identity-preservinggeneration
andaninputmaskedpersontogetherwithaposeestimateintoa
lowresolutiontryonimage,whichisthenupscaledwithanother Thetopicofpreservingtheidentityofasubjectduringgenera-
standarddiffusionmodel.Whiletransferqualityisveryhigh,the tionisanaturalonewhengeneratingimagesorvideoswherea
methodlosestheidentityonthetargetperson,asittendstoreplace specificsubjectisdesiredtobeseen.Typically,applicationshave
thebodyentirelyandalterdetailssuchastattoos,bodyshape,etc. revolvedaroundpreservingtheidentityofpeople,inparticular
LaDI-VTON[Morellietal.2023a]usesacombinationofTextual facialfeatures.
Inversion[Galetal.2022]onthegarmentimage,andthefeeding Themostnaivewaytoperformidentitypreservationis,given
ofbothimagestoadiffusionmodeltogetherwith"EnhancedMask- acollectionofimagesrepresentingasubject,tofine-tuneapre-
Aware Skip Connections" which directly send inputs the UNet traineddiffusionmodelusingsaidcollection[Nitzanetal.2022;
outputsinordertoadjustfinedetailsinthefinaloutput. Ruizetal.2023].Asthisisacostlyprocess,manytechniqueshave
StableVITON[Kimetal.2023],ontheotherhand,mergesthe beendevelopedtoavoidtrainingontheentiremodel,suchasLoRA
personandgarmentimages(togetherwithdenseposeestimation [Huetal.2021],FedPara[NamHyeon-Woo2021],OFT[Qiuetal.
andmasks)intotwoseparateencoderswhicharethenmergedinto 2023],CustomDiffusion[adnBingliangZhangetal.2022],CONES
theUNetdecoder,usingthegarmentencodingaskeysandval- [Liuetal.2023]whichtotrainlowparametercountperturbations
uesforattention,whilepersonencodingsareusedasqueries.The ortransformationsoftheoriginalweights.Themaindownsideis
2MaX4Zero
Initial Registration
Semantic âˆ’ =
Segmentation
ğ‘¥
ğ‘ Deformed mask
Inpainting
Unet Fringe mask
MLS
Deform
Inpainting
DIFT Unet
ğ‘¥ Features
ğ‘”
FringeAssignment ğ‘¥ ğ‘”,ğ‘
Inpainting Unet
â€¦ Conv. Cross- â€¦
ğ‘¥ ğ‘” DP ia ffr uti sa el block MEA attn.
ğ¾
ğ¾ğ‘€ğ‘”
ğ‘ ğ‘”
ğ‘¥ Partial
ğ‘”,ğ‘ Diffuse
âˆ™ ğ‘£ ğ‘ Output
ğ‘£
ğ‘”
Gray long-sleeve top Masked Attention
ğ‘„ğ‘€ğ‘
ğ‘
Consistent Inpainting Masked Extended Attention
Figure2:OverviewoftheproposedMaX4Zeromethod.Top:theInitialRegistrationstage,wherethereferencegarmentis
warpedtomatchthetargetpersonusingextracteddeepfeaturesfrombothimages[Tangetal.2023].Theremaininggaps
betweenthetargetgarmentandwarpedonearefilledbytheFringeAssignmentmodule(seeFigure4).Bottom:theConsistent
Inpaintingstage,whereutilizingtheMaskedExtendedAttentionmechanismfortransferringthereferencefine-detailsthrough
stroke-basedinpainting.
thattheyrequiremanyimagesofthesubject,andmaybevery text-to-imagemodel(StableDiffusion[Rombachetal.2022b]in
limitedinthescopeofsuccessfulgenerations. ourcase).Ourzero-shotmethodMaX4Zero,iscomposedof2stages
A more advanced way of preserving identity is the use of a aspresentedinFig2-(1)InitialRegistrationwherethegarmentis
pretrainedidentity-preservingmodulewhichtweaksthediffusion placedontopofthepersonwhilefillingthecreatedfringes,provid-
modelâ€™sweightsoractivationsconditionaltoanidentity-providing ingpowerfulquesforfinegeneration(2)ReferencebasedConsistent
image.Oneofthemostpopular,IP-adapter[Yeetal.2023],uses Inpaintingforinsertingfine-graineddetailsandincreasingfidelity.
atraineddecoupledcross-attentionmodulewhichinjectspertur-
bationsderivedfromanidentityimageintothediffusionUNetin
3.1 Preliminaries
ordertoconditionthediffusion,muchlikeinAnyDoor(seeabove).
IP-adapterishoweverpretrainedwithastrongemphasisonthe StableDiffusionisanopensourcediffusionmodelvariantwhich
preservationoftheidentityofhumanfaces,andstruggleswith initiatesitsdenoisingprocessinalatentspace.Eachimageisfirst
othertypesofconditionings,especiallygarments.Whileoverall fed forward through a pretrained encoder, transforming it into
appearanceispreserved,mostdetailsdonottransfercorrectly. thelatentspace,andinthefinalstepoftheprocessapretrained
decodermapsthegeneratedoutputbacktoRGBspace.Ateach
timestep,theencodedinputundergoesgradualdenoisingwithin
3 METHOD
thestablediffusionmodelâ€™sU-Netarchitecture,whichincorporates
Given a person image ğ‘¥ ğ‘ âˆˆ Rğ»ğ‘Ã—ğ‘Šğ‘Ã—3 and a garment image cross-attentionandself-attentionblocks.Theself-attentionblocks
ğ‘¥ ğ‘” âˆˆ Rğ»ğ‘”Ã—ğ‘Šğ‘”Ã—3,ourgoalistosynthesizethegarmentobjecton focusonimagedetailsandcross-attentionblocksareutilizedto
topoftheperson,whileremovingpreviousexistinggarmentand incorporatetextprompts.
preservingallotherdetails.Inthisworkweaddressthetaskof Ineachself-attentionblock,theintermediatefeaturesarepro-
virtual-tryonthroughareference-basedimageinpaintingapproach. jectedintoqueriesğ‘„,keysğ¾,andvaluesğ‘‰ withdimensionğ‘‘.Each
Inotherwords,weinpainttheareawithinthepreviousgarment selfattentionlayerfirstcalculatessimilarityscoresbetweenallğ‘„
regionofthepersonâ€™simageğ‘¥ ğ‘,usingthedesiredgarmentasrefer- andğ¾ vectors.Thenthenormalizedvaluesvectorsareprojected
enceğ‘¥ ğ‘”.Inordertoperformthisinpainting,weutilizeapre-trained ontheattentionmatrixcreatingtheblockoutputğœ™ asaweighted
3Orzech,etal.
sumofthevaluesvectors.Formally: Inthewarpingphase,weinitiallyemploytheforwardprocessof
(cid:18)ğ‘„Â·ğ¾âŠ¤(cid:19) thediffusionmodel,byaddingnoisecorrespondingtotimestep
ğ´=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ âˆš ğ‘¡ tobothimages.Then,wepasstheimagesthroughthedenois-
ğ‘‘
ingprocess,whileextractingthedeepfeaturesfromthemodelâ€™s
ğœ™ =ğ´Â·ğ‘‰ transformerblocksofboththetargetpreviousgarmentandthe
whereğ´isthecomputedattentionmapandğœ™istheblockoutput. requiredreferencedgarment.Accordingtotheextractedfeatures,
Thisprocessisappliedtoeachqueryindependentlyallowingnon- wecancreateabi-linearcorrespondencematrixbetweenthetwo
restrictedreceptivefieldbycapturingcorrespondencesacrossthe garmentsbyidentifyingthebestsemanticmatches,whichreferred
entireimage. toastwovectorsexhibitingtheminimumdistancebetweenthem.
Thiscorrespondencematrixisthenusedinordertodeformthe
3.2 InitialRegistration referencegarmentontopofthetargetpersonusinganon-rigid
deformationschemenamedMoving-Least-Squares(MLS)[Schaefer
Inordertofaithfullyinpaintthegivengarmentontopofthetar-
etal.2006].
getperson,transferringthegarmenttextureorappearanceisnot
FringeAssignment.Afterwarpingthereferencegarmentthere
enough,aspresentedinFig3.Ascanbeseen,weobservethat
explicitspatialfeaturesinğ‘¥ ğ‘”,sensitivetoscales,rotations,andeven are,however,somefringesleftunwrappedbecausethetwogar-
mentsarenotidentical,andthepostureofthetargetpersonis
translations,canbefoundinthegeneratedresult,eventhough
arbitrary.Wefillthesefringesinbyapplyinginpainting,askingto
theycarrynosemanticmeaning.Wepostulatethisisbecausesome
completetheregionsleftbytheoldgarmentandarenotcoveredby
featurescannotbefullyavoidedortranslatedduetolimitedca-
thecurrentone.Previouswork[Avrahamietal.2023]showsthat
pacityoftheprior,inaphenomenasimilartothatcalledTexture
inpaintingmodelsfailtoproperlyaltersmallmasks.Thuswechose
Stickingintheliterature[Karrasetal.2021].Wehencefirstalign
to use a double-mask strategy, consisting of the original fringe
thegarmentonthetargetimageandrelevantmaskedarea.thistask
maskandadilatedone.AsdepictedinFig4,attimestepğ‘¡ ofthe
becomesmorecomplicatedwhenworkingwithrealworldimages
inpaintingprocessweusethedilatedmasktopredictthedefused
incontrarytogeneratedones,sincewedonâ€™thavecontrolonthe
generatedimagelayout.
noiseğœ– ğ‘¡âˆ’1.However,theiterationoutputğ‘§ ğ‘¡âˆ’1isconstructedfrom
thediffusedimageğ‘§ ğ‘¡âˆ’1,wherethemodelpredictionisappliedonly
totheoriginalthinfringemask.
Figure4:Overviewofthedouble-maskinpaintingstrategy.At
eachtimestep,thedilatedmaskisusedfornoiseprediction,
butonlytheregionsinthethinmaskareactuallytakento
thenextdiffusioniteration.
Ourexperimentsshowthatbyusingthedouble-masksstrategy
webenefitfrombothworlds-thediffusionmodelisabletogenerate
validinformationononehand,andontheother,theinformation
stillcomplieswiththebackgroundseamlessly.
Figure3:Garmentgenerationcomparedincludingorexclud- 3.3 MaskedExtendedAttention(MEA)
ingtheinitialregistrationstage.Thisdemonstratesthe"tex-
Oncewehavethecoarsefeaturesofthegarmentontopofthe
turesticking"phenomenon,wherethespatialfeaturesofthe
personimage,placedinthecorrectlocation,wecanturntofine
transformedshirtareapparentintheresultinggeneration.
detailenhancement.Previousworks[Alalufetal.2023;Caoetal.
2023;Geyeretal.2023]exploredtheself-attentionlayerswithin
GarmentWarping.Toovercomethisissue,wedevelopadeep thedenoisingnetworkofatext-to-imagediffusionmodel.More
featuresbaseddeformationblockinspiredby[Tangetal.2023]. specifically,ithasbeendemonstratedthatitispossibletoutilizethe
4MaX4Zero
queries,keys,andvaluesfromtheseself-attentionlayersinorder distributesitsfocusacrosstwoimagesasweexpanditsreceptive
totransfersemanticinformationbetweendifferentandunrelated fieldbeyondjustthetargetimage.Thisresultsinamoreunified
images. attentionmapsasasoftmaxisappliedtoalargersetoffeatures.
MaskedExtendedAttention(MEA).Priorresearch[Alaluf Thus,thiscanalsointroducenoisyinformationintotheblockand
etal.2023]hasshownthatqueriescontrolthesemanticmeaningof potentiallyoverlookthedesiredfinerdetails.Inordertoencourage
eachspatiallocation,whilethekeysprovidecontextualinformation theattentionmapstoconcentrateitsfocusonmorespecificregions
foreachquery.Thisenablesthemodeltoassessthesignificanceof withinthereferenceimage,following[Alalufetal.2023]weuseda
variousimagesegmentsforagivenquerypositionbymultiplying contrastoperationtoenhancethevariabilityoftheattentionmaps
therelevantqueryandkey.Then,thevaluesrepresentthecontent whereweamplifytheemphasisonthepeaksoftheattentionmap,
tobegeneratedandspecifytheinformationusedtodeterminethe effectivelynullifyinganynoisyinformation.Thecontrastoperator
relevantfeaturesofeachqueryposition. definedby[Alalufetal.2023]as:
Giventhoseobservations,andinspiredby[Caoetal.2023;Geyer
ğ¸ğ‘›â„ğ‘ğ‘›ğ‘ğ‘’(ğ´)=(ğ´âˆ’ğœ‡(ğ´))ğ›½+ğœ‡(ğ´)
etal.2023],ourapproachasdemonstratedinFig.2istoexpandthe
self-attentionkeysandvaluesduringthegenerationofthetarget whereğœ‡isameanoperationandğ›½isthecontrastoperator
image,allowingittoincludeinformationfromthereferenceimage, Stroke-Based Inpainting Finally, we employed the stroke-
yetthequeriesremainsfixed.Inotherwords,duringgenerationof basedinpaintingconcept(SDEdit),asintroducedin[Mengetal.
thetargetgarment,weallowthenewgeneratedgarmentvaluesto 2022].Byusingtheregisteredgarmentasainitialimage,wedenoise
bedeterminednotonlybythesameimagecontext,butalsofrom theimageusingclassifier-freeguidance(CFG)[HoandSalimans
thegarmentexample. 2022].Ateachdenoisingtimestep,weusetwoparallelforward
Weobservethatusingthisextendedattentionresultsinback- passesthroughthedenoisingnetwork.Thefirstpassusingthe
groundinformationleakagefromthereferencegarmentintothe conventionalself-attentionlayersofthenetworktoenhancethe
targetresult.Toovercomethischallenge,inspiredby[Caoetal. realism of the generated garment while preserving the context
2023],wecarefullymaskedtherelevantpartswithintheattention receivedfromthegarmentdescriptionprompt,yieldinginğ‘§ğ‘ğ‘ğ‘ ğ‘’
masks.Theuseofmasksaspartoftheextendedattentionmecha- andğ‘§ğ‘¡ğ‘’ğ‘¥ğ‘¡ respectively.Meanwhile,thesecondpassleveragesour
nismallowsustoestablishcontrolonwhichinformationpropagate MaskedExtendedAttentionlayerstoreproducethereferencegar-
betweentheimagesandthroughdifferentpatcheswithinthesame mentbycapturingitsfinedetails,yieldinginğ‘§ğ‘€ğ¸ğ´ .ByCFGscheme
image.WenamedthismechanismMaskedExtendAttentionor thepredictednoisedefinedas:
MEA.
Inmoredetails,theinputtotheMEAblockaretheself-attention ğ‘§ ğ‘¡âˆ’1=ğ‘§ ğ‘¡ğ‘ğ‘ğ‘ ğ‘’ +ğ›¼ ğ‘€ğ¸ğ´Â·(ğ‘§ ğ‘¡ğ‘€ğ¸ğ´ âˆ’ğ‘§ ğ‘¡ğ‘ğ‘ğ‘ ğ‘’ )+ğ›¼ ğ‘¡ğ‘’ğ‘¥ğ‘¡ Â·(ğ‘§ ğ‘¡ğ‘¡ğ‘’ğ‘¥ğ‘¡ âˆ’ğ‘§ ğ‘¡ğ‘ğ‘ğ‘ ğ‘’ )
featuresfrombothimages.Namely,thequeries,keysandvalues
fromboththetargetpersonimageğ‘„ ğ‘,ğ¾ ğ‘,ğ‘‰ ğ‘,andthereference 4 EXPERIMENTALRESULTS
garmentğ‘„ ğ‘”,ğ¾ ğ‘”,ğ‘‰ ğ‘”. As mention, we expand the input of the ex- Datasets.Weconductourexperimentsusingthreepubliclyavail-
tendedattentionblockbyutilizetheinpaintingmasksğ‘€ ğ‘ andğ‘€ ğ‘” abledatasets.Theclothinggarmentimagestakenfromthevirtual-
throughtheprocess.Thekeysofbothimagesareconcatenatedand tryondatasetsDressCode[Morellietal.2022]andVITON-HD
themaskedextendedattentiondefinedas: [Choietal.2021],whilethedescriptionforeachgarmentextract
fromtheworkof[Baldratietal.2023].Thetargetpersonimages
ğ‘€ğ¸ğ´(ğ‘„ ğ‘,[ğ¾ ğ‘,ğ¾ ğ‘”],ğ‘€ ğ‘,ğ‘€ ğ‘”)=
weretakenfromtheworkof[Liangetal.2015],alargedatasetfor
(cid:32) (1âˆ’ğ‘€ ğ‘)Â·ğ‘“ğ‘ğ‘”+ğ‘€
ğ‘
Â·ğ‘“ğ‘“ğ‘”(cid:33) humanin-the-wildparsing.Toensureaccurateutilizationofour
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ âˆš (1) benchmarks,werigorouslyfollowedthepre-processingguidelines
ğ‘‘
ofVITON-HDdatasettoobtainthenecessaryinputconditions.
where Baselines.Wecompareourmethodwithtwodiffusion-based
ğ‘“ğ‘ğ‘” =ğ‘„ ğ‘ Â·ğ¾ ğ‘âŠ¤ virtualtry-onmethodsStableVITON[Kimetal.2023]andLaDI-
VTON[Morellietal.2023b],bothtrainedontargetperson-clothing
ğ‘“ğ‘“ğ‘” =ğ‘„
ğ‘
Â·[ğ¾ ğ‘,ğ‘€ ğ‘”Â·ğ¾ ğ‘”]âŠ¤
garmentpairsextractedfromDressCodeandVITON-HD.Weaddi-
Theblockoutputfortargetpersongenerationisdefinedas: tionallyassessedourmethodagainsttwoexample-basedmodels:
ğœ™ğ‘€ğ¸ğ´ =ğ‘€ğ¸ğ´(ğ‘„ ğ‘,[ğ¾ ğ‘,ğ¾ ğ‘”],ğ‘€ ğ‘,ğ‘€ ğ‘”)Â·[ğ‘‰ ğ‘,ğ‘‰ ğ‘”] IP-adapter[Yeetal.2023]asamaskorientedimageeditingtech-
nique,andAnyDoor[Chenetal.2024]aninpaint-by-reference
Intuitively,generationofatargetgarmentpixeltakesintoac- method.Bothofthesemethodsareextensivelytrainedonlarge
counttherelevantinformationfromthereferenceimage,aswell datasetscontainingdiverseobjects,withtheobjectiveofeffectively
asinformationfromthesamegeneratedgarmentwiththescene reconstructingtheobjectwithinthescene.
context.NotethattheMaskedExtendedattentionprocessesthe EvaluationMetrics.Inordertoquantifyourmethodperfor-
originalandun-deformedgarment,inordertotransfertheauthen- mance, we initially conducted a user study survey with 57 par-
ticandan-distortedattributestotheirgroundedplace. ticipants.Eachparticipantwasaskedtoassessimagecoherence,
DetailsPropagationEnhancement.Priorresearch[Alaluf aswellasthepreservationofboththeclothinggarmentandthe
etal.2023]hasshownthatconventionalself-attentionblockstend targetpersonâ€™sidentity.MoreoverweusedFrenchetInceptionDis-
to concentrate their attention on a concentrated region around tance(FID)score[Heuseletal.2018]tomeasurethefidelityof
theimagepatch.Incontrast,themaskedextendedattentionblock thegeneratedimages,andinadditionLPIPS[Zhangetal.2018]
5Orzech,etal.
Virtual Try-On Methods Image-Based Inpainting Methods
Person & Garment LaDI-VTON StableVITON IP-Adapter AnyDoor Ours
Figure5:QualitativecomparisonofMaX4Zeroandcompetitorsoninthewildtargetimages.WecompareagainstLaDI-VTON
[Morellietal.2023a]andstableVTON[Kimetal.2023],whicharededicatedVTONapproaches,andIP-adapter[Yeetal.2023]
andAnydoor[Chenetal.2024],whicharepersonalizedimageeditingandpaint-by-referencemethodsrespectively.Ascanbe
seen,garmentidentityispreservedbetterusingourmethodforunseengarments.
6MaX4Zero
andDreamSim[Fuetal.2023]forevaluatinggarmentattributes Table 1: Quantitative comparison for in the wild settings.
transferandpersonidentitypreservation. ResultsinBoldandUnderlinedreferstobestandsecondbest
results,respectively.Wemeasuregarmentidentityusingtwo
4.1 QualitativeResults semanticsimilaritymethods(LPIPS[Zhangetal.2018]and
DreamSim(DrSm)[Fuetal.2023])onthegarmentregionof
AswedemonstrateinFig.5,MaX4Zerogeneratesrealisticimages,
theimagealone,personpreservationusingthesamemethods
astheimagebackgroundremainsunchangedduetotheadvan-
on the background region, and overall realism using FID
tagesoftheinpaintingscheme.Moreover,MaX4Zerosuccessfully
[Heuseletal.2018]scores.Forallmetrics,lowerisbetter.As
maintainsthein-the-wildtargetpersonâ€™sidentitywhileaccurately
canbeseen,ourapproachoffersabalancebetweenrealism
transferringthetextureandcharacteristicsofthereferencedgar-
andidentitypreservation,supportingusersâ€™preferencesin
ment,comparedtothefourbaselinemethods.Specifically,Anydoor
theuserstudy.
preservethesemanticfeaturesofthereferencegarment,butmisses
therotationorspatiallocationofthose,ascouldbeseenonsecond
row.Thesecondimage-basedinpaintingmethod,IP-adapterableto GarmentID PersonPres.
inpaintagarmentwhichfitsthesemanticmeaningoftherequested ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ ğ¿ğ‘ƒğ¼ğ‘ƒğ‘† ğ·ğ‘Ÿğ‘†ğ‘š ğ¿ğ‘ƒğ¼ğ‘ƒğ‘† ğ·ğ‘Ÿğ‘†ğ‘š ğ¹ğ¼ğ·
referencebutmissesthefinedetailsofit.Ontheotherhand,virtual
LaDI-VTON 0.679 0.619 0.146 0.047 74.78
try-onmethodssuchasStableVITONandLaDI-VTONwhichbeen
Stable-VITON 0.686 0.639 0.071 0.013 67.56
trainedonvirtualtry-onspecificdomainareabletocreatefitand
harmonizedclothingandappropriatelypositionedinmostcases, IP-Adapter 0.687 0.626 0.016 0.001 64.33
butontheexpenseoffailingtocapturethecharacteristicsofthe AnyDoor 0.619 0.434 0.059 0.059 76.80
garmentandtransferittothearbitraryin-the-wildperson,ascan Ours 0.643 0.518 0.021 0.009 66.27
beseeninthelefttwocolumns.Moreover,incontrarytoMaX4Zero,
thosebenchmarksalterthebackgroundofthetargetgarmentor
thepersonpostureascanbenoticedinallthreerows. identity preservation by LPIPS [Zhang et al. 2018] and Dream-
AdditionaloutcomesproducedbyMaX4ZeroaredepictedinFigures Sim[Fuetal.2023]againassimilarityscore,butinthiscasewe
1,6and11.Furthermore,MaX4Zeromethodallowustotry-onout evaluatedthegeneratedimagewiththeclothinggarmentcropped
ofdomainreferencegarmentsortargetfigure,asdepictinFigures outagainsttheoriginalpersonfigureimage.Lastly,weusedFID
12and13.Seethesupplementarymaterialformorequalitative [Heuseletal.2018]scorebetweenthegeneratedimagesandthe
results. originalin-the-wildhumanimagesinordertoevaluatethefidelity
andrealismofthegeneratedimages.
InTheWildEvaluation.Ourobjectivehereistoassessour
methodagainstbenchmarkmodelsusingarbitrarytargetperson
images,whichmayvaryintermsofpostureandbackground.This
isincontrasttotheVITON-HDandDressCodedatasets,where
thebackgroundsareconstantandthehumanfigureshavesimilar
poses.AsshowninTable1,regardingproperlytransfergarment
identity,ourtraining-freemethodrankedasthesecondbestbe-
tweenthefinetunedmethodscloselytrailingbehindAnyDoor,
accordingtotheexplainedsimilaritymetrics.Therelativesucces-
sionofAnyDoorinthistermcouldbeattributedtotheextensive
resourcesthatAnyDoorallocatestoacquirereferencedetailsaspre-
sentedin[Chenetal.2024].However,AnyDoorexhibitedinferior
resultsinpersonandbackgroundpreservationcomparedtoour
methodandotherbenchmarks.Intermsofpreservingtheperson
andbackground,ourmethodsecuredthesecond-bestposition,with
anarrowmarginbehindIP-Adapter,asbothfirstmethodsusing
theinpaintingapproach.Moreover,theresultspresentedinTable
1,demonstratesimilartrendregardingoverallimagefidelity,as
Figure6:ResultsGallery.GeneratedresultsbyMaX4Zero.
indicatedbytheFIDscores.Ontheotherhand,IP-adapterstrug-
Bestviewedwhenzoomedin.
gledtoaccuratelytransferthegarmentattributes,asitrankednear
thebottomaccordingtothegarmentsimilaritymetrics.Notethat
bothpretrainedvirtualtry-onmethoddidnotevaluatedastop2
4.2 QuantitativeResults
methodsinbothmetrics.Inoverallperspectiveofbothtasks,our
Metrics. In effort to properly demonstrate the success of each trainingfreeandzero-shotmethodwasfoundcomparabletothe
methodweusedseveralevaluationmetrics.First,toevaluateprop- task-specificfine-tunedmodels.
erlyreferencegarmenttransferweusedLPIPSandDreamSimas UserStudy.Inordertoquantifyfurtherourresultsagainstthe
similarityscores,betweenthegeneratedgarmentandtherefer- othermethodsonin-the-wildtargetpersonimagesweconducteda
encedone.Usingthesamereasoning,weaddressedtheperson userstudyinvolving57participants.Eachparticipantwaspresented
7Orzech,etal.
withthetargetpersonimage,thereferencegarment,andtworesults: oftiltedtextureasinthethirdrow.However,thisissueissolved
onegeneratedbyourmodelandtheotherbyabenchmarkmethod. whenusingtheinitialregistrationstage,asthedeformationofthe
Theneachonewasaskedtodeterminewhichresultexcelledin referencealignstheattributestothesameproperlocationinall
threeaspects:(1)Preservingthetargetpersonidentityandposture, cases.WebelievethatthisphenomenonoccursbecausetheMEA,
(2)Accuratelycapturinggarmentattributes,and(3)Overallimage eventhoughisanattentionmechanism,stilloperatesonthepatch
fidelity.AsdepictedinFig.7,ourmethodsurpassedallotherfine- level,givingsignificantbiastoimmediatesurroundings..
tuned benchmarks across all three aspects. As shown in 7, the ExtendedAttentionTheeffectivenessoftheMEAmechanism
comparisoninassessinggarmentattributeswascloselycontested isshowcasedinFig8.Here,weexploretheimpactofvaryingthe
when compared with AnyDoor. Furthermore, the StableVITON MEAguidancescale,rangingfromğ›¼ ğ‘€ğ¸ğ´ =0whenthemechanism
methodprovesformidableintermsofpreservingtheidentityofthe isinactive,throughourselectedhyperparameterğ›¼ ğ‘€ğ¸ğ´ =15,toan
personandoverallfidelity,owingtoitscapabilitytomaintainthe exceptionallyhighvalueofğ›¼ ğ‘€ğ¸ğ´ =40.
originalfeaturesoftheimage.Notethatthetrendobservedinour Intheleftcolumn,wheretheMEAmechanismisinactive,itcan
userstudyresemblestothequantitativeresultspresentedearlier. benoticedthatthefine-detailsofthereferencegarmentfailtoex-
However,whencomparingthetworesultssidebyside,ourmethod tendintothetargetimage.Thisisduetothefactthatduringinpaint-
wasfoundtobeoutperformedinhumanperception. ingtheinpaintedarealackscontextualinformationfrombeyond
thetargetimageboundaries.Meaning,theoutcomeisstroke-based
inpaintingofthewarpedgarmentarea,utilizingtheinpainting
modelpriorknowledgealone.Ontheotherhand,asshownon
therightcolumn,whenincreasingexcessivelytheMEAguidance
factor,fine-graineddetailsoverpropagatesintothetargetimage,
whetheritreflectedinextrabuttonsasseeninthesecondrow,or
generationofshelf-likegarment,disruptingtheharmonizedtarget
image.
Figure 7: User Study results, comparing three aspects of
MaX4Zeroresultsagainstfourbenchmarkmethods.Based
on57participants.
Figure8:AblationstudyoftheConsistentInpaintingstage
usingdifferentMEAguidancefactors.Wesetğ›¼ ğ‘€ğ¸ğ´ =15for
4.3 AblationStudy allourexperiments
Inordertodemonstratetheeffectivenessofourmethodcomponents
weevaluatedtheeffectofeachcomponentonthefinalresult. ExtendedAttentionwithMasking(MEA)Wehighlighted
InitialRegistration.First,weinvestigatedtheeffectivenessof thesignificantofemployingmasksintheMEAmechanisminFig
thewrappingmoduleintheinitialregistrationstageandhowit 9.Asdiscussedearlier,applyingmaskstotheextendedattention
influencesthefinalresults.InFig.3,wevisualizetheimpactonthe processenablesustoregulatetheflowofinformationbetween
finalgarment,bothwithandwithoutinitialregistration,usingthe thereferenceandtargetimages.Intheleftcolumn,weobserve
samereferenceclothingpiecewithdifferentaugmentations.Inboth backgroundelementsfromthereferenceimageseepingintothe
casesweinpaintedtherequiredmaskedareaofthetargetimage, inpaintedsegments.Thisresultsintwodistinctartifacts:firstly,the
whileaddressingeachaugmentedreferencegarmentordeformed emergenceofwhitepatchesalongthegeneratedgarmentboundary,
versionofit.Ascanbeseeninthefigure,whenneglectingthe asevidentinthefirstrow;secondly,anincreaseinthebackground
initialregistrationstage,theaugmentationonthereferenceimage colorâ€™sinfluencewithinthegeneratedtextureoftheclothingpiece
effectsthegeneratedimage,whetheritisamoredensetexture relativetothereferenceimage,asreflectedinmorenotablewhite
whenusingsmall-scalegarmentinthesecondrow,orpropagation flowersinthesecondrow.
8MaX4Zero
Figure10:Limitations:Ourpriorstillsufferswhenrecon-
structingtextandelaborateprints(a)andissensitivetomask
placementasitmightalterobjectwithinthemask(b),(c).
5.1 Limitations
CurrentStableDiffusion-basedarchitecturesstillexhibitshortcom-
ingsinaccuratelyreproducingreadableandcoherenttextualele-
mentssuchastextorlogos.Insomefailcasesourmodelisable
toreproducetheoveralllooksorfinedetailsofthetextualobject,
Figure9:AblationstudyonthemaskingoftheExtended butstrugglestodefinepreciseandhighlycomprehensiblelettersor
Attentionmechanism.Ascanbeseen,maskingmitigatesthe numbers,asshowninFig10(a).Anotherissueourmodelstruggle
leakageofthewhitebackgroundintothegeneration. withisthesensitivityforthemaskplacement.Ascanbeseenin
Fig10(b),oncethemaskincludesotherhumanbodysegments(in
thiscasethelefthand),ourmethodmightalteritpositionorlooks,
fittingittoanoverallauthenticimage,howevernotidenticaltothe
originalone.Forthesamereason,itisevidentfromFig10(c)that
5 DISCUSSION
whentheinpaintingmaskextendsbeyonditsintendedboundaries
Inthisworkweintroducedanovelzero-shotin-the-wildapproach
intootherobjects,suchashair,palms,orinthisinstance,abook
totheimage-basedVirtualTry-Ontask.Ourpurposemethodcom-
heldbythehumanfigure,ourmethodmightalterit,resultingin
prises two stages. Initially we ground the coarse feature of the
disruptionstotheoriginalimage.Aninterestingavenueforfuture
referencegarmentontothetargetpersonâ€™sbodybywrappingthe
researchistoaccommodatethismaskinamoresemanticorlatent
garmentarounditandfillinginanygapscreatedbyincomplete
manner,ratherthenthroughexplicitspatialarrangement.
matching.Asshowninourwork,thisstageprovidesstrongcues
forthenextstep,ensuringtherightfeaturesaregeneratedinthe
6 ACKNOWLEDGEMENTS
rightpositions,diminishingthebiasoriginatingfromthereference
ThisworkwassupportedinpartbytheIsraelScienceFoundation
garmentimage.Oncewehavethegroundedgarment,weintroduce
underGrantNo.1337/22.
aMaskedExtendedAttentionmechanism.Thismechanismpours
thefinedetailsfromthereferenceontothegroundedlocationonthe
REFERENCES
targetthroughaninpaintingdiffusionprocess.Inourexperiments
wedemonstratedthatourmethodoutperformsothersupervised- Nupur Kumari adn Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-
Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion.
trainedvirtual-try-onbenchmarksonin-the-wildhumanimages.
arXiv:2212.04488[cs.CV]
Furthermore,weemphasizedthecriticalroleofeachcomponentin Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel
contributingtothesuccessofourentirepipeline. Cohen-Or.2023. Cross-ImageAttentionforZero-ShotAppearanceTransfer.
arXiv:2311.03335[cs.CV]
SocietalImpact.Misuseofthistechnology,andothervirtual
MoabArar,RinonGal,YuvalAtzmon,GalChechik,DanielCohen-Or,ArielShamir,and
try-ontechniques,includedisinformation,fakenews,andbody- AmitH.Bermano.2023.Domain-AgnosticTuning-EncoderforFastPersonalization
relatedembarrassmentonpublicchannelssuchassocialmedia.Our ofText-To-ImageModels.InSIGGRAPHAsia2023ConferencePapers(<conf-loc>,
<city>Sydney</city>,<state>NSW</state>,<country>Australia</country>,</conf-
workcontributestothedemocratizationofVTONtools,potentially loc>)(SAâ€™23).AssociationforComputingMachinery,NewYork,NY,USA,Article
makingthemmoreconvincingandapproachable.Viablesolutions 72,10pages. https://doi.org/10.1145/3610548.3618173
OmriAvrahami,OhadFried,andDaniLischinski.2023.BlendedLatentDiffusion.ACM
include model watermarking and real-time detection on public
TransactionsonGraphics42,4(July2023),1â€“11. https://doi.org/10.1145/3592450
platforms. We call potential users of our code not to cross the AlbertoBaldrati,DavideMorelli,GiuseppeCartella,MarcellaCornia,MarcoBertini,
lineofcausingdiscomfortorviolatingnationalregulations,and andRitaCucchiara.2023. MultimodalGarmentDesigner:Human-CentricLa-
tentDiffusionModelsforFashionImageEditing.InProceedingsoftheIEEE/CVF
theresearchcommunitytocontinuedevelopingin-placereliable
InternationalConferenceonComputerVision.
detectionmethods. Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick
Wehopeourworkwillcontributetoresolvingthevirtual-try- Schramowski,KristianKersting,andApolinÃ¡rioPassos.2023.LEDITS++:Limitless
ImageEditingusingText-to-ImageModels. arXiv:2311.16711[cs.CV]
ontaskwithgreatgenerality,speed,andresourcedemands.We MingdengCao,XintaoWang,ZhongangQi,YingShan,XiaohuQie,andYinqiang
encouragefurtherexplorationofthesetuningfreezero-shottwo- Zheng.2023.MasaCtrl:Tuning-FreeMutualSelf-AttentionControlforConsistent
ImageSynthesisandEditing. arXiv:2304.08465[cs.CV]
stage methods to tackle image-based inpainting across various
XiChen,LianghuaHuang,YuLiu,YujunShen,DeliZhao,andHengshuangZhao.2024.
domains. AnyDoor:Zero-shotObject-levelImageCustomization. arXiv:2307.09481[cs.CV]
9Orzech,etal.
SeunghwanChoi,SunghyunPark,MinsooLee,andJaegulChoo.2021.VITON-HD: ZejuQiu,WeiyangLiu,HaiwenFeng,YuxuanXue,YaoFeng,ZhenLiu,DanZhang,
High-ResolutionVirtualTry-OnviaMisalignment-AwareNormalization.InProc. AdrianWeller,andBernhardSchÃ¶lkopf.2023.ControllingText-to-ImageDiffusion
oftheIEEEconferenceoncomputervisionandpatternrecognition(CVPR). byOrthogonalFinetuning. arXiv:2306.07280[cs.CV]
PrafullaDhariwalandAlexNichol.2021. DiffusionModelsBeatGANsonImage Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
Synthesis. arXiv:2105.05233[cs.LG] 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents.
StephanieFu,NetanelTamir,ShobhitaSundaram,LucyChai,RichardZhang,Tali arXiv:2204.06125[cs.CV]
Dekel,andPhillipIsola.2023.DreamSim:LearningNewDimensionsofHuman RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOm-
VisualSimilarityusingSyntheticData. arXiv:2306.09344[cs.CV] mer.2022a. High-ResolutionImageSynthesiswithLatentDiffusionModels.
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitH.Bermano,GalChechik, arXiv:2112.10752[cs.CV]
andDanielCohen-Or.2022.AnImageisWorthOneWord:PersonalizingText-to- RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.
ImageGenerationusingTextualInversion. arXiv:2208.01618[cs.CV] 2022b.High-resolutionimagesynthesiswithlatentdiffusionmodels.InProceedings
RinonGal,MoabArar,YuvalAtzmon,AmitH.Bermano,GalChechik,andDaniel oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.10684â€“10695.
Cohen-Or.2023.Encoder-basedDomainTuningforFastPersonalizationofText- NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
to-ImageModels.ACMTrans.Graph.42,4,Article150(jul2023),13pages. https: Aberman.2023. DreamBooth:FineTuningText-to-ImageDiffusionModelsfor
//doi.org/10.1145/3592133 Subject-DrivenGeneration.InProceedingsoftheIEEE/CVFConferenceonComputer
Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel VisionandPatternRecognition(CVPR).22500â€“22510.
Cohen-Or. 2024. ReNoise: Real Image Inversion Through Iterative Noising. ScottSchaefer,TravisMcPhail,andJoeWarren.2006.Imagedeformationusingmoving
arXiv:2403.14602[cs.CV] leastsquares.InACMSIGGRAPH2006Papers(Boston,Massachusetts)(SIGGRAPH
MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel.2023.TokenFlow:Consistent â€™06).AssociationforComputingMachinery,NewYork,NY,USA,533â€“540. https:
DiffusionFeaturesforConsistentVideoEditing. arXiv:2307.10373[cs.CV] //doi.org/10.1145/1179352.1141920
MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSepp RoyShilkrot,DanielCohen-Or,ArielShamir,andLigangLiu.2013.GarmentPersonal-
Hochreiter.2018.GANsTrainedbyaTwoTime-ScaleUpdateRuleConvergetoa izationviaIdentityTransfer.IEEEComputerGraphicsandApplications33,4(2013),
LocalNashEquilibrium. arXiv:1706.08500[cs.LG] 62â€“72. https://doi.org/10.1109/MCG.2012.90
JonathanHo,AjayJain,andPieterAbbeel.2020. DenoisingDiffusionProbabilistic JiamingSong,ChenlinMeng,andStefanoErmon.2022.DenoisingDiffusionImplicit
Models. arXiv:2006.11239[cs.LG] Models. arXiv:2010.02502[cs.LG]
Jonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance. LumingTang,MenglinJia,QianqianWang,ChengPerngPhoo,andBharathHariharan.
arXiv:2207.12598[cs.LG] 2023.EmergentCorrespondencefromImageDiffusion. arXiv:2306.03881[cs.CV]
EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, YoadTewel,RinonGal,GalChechik,andYuvalAtzmon.2023. Key-LockedRank
LuWang,andWeizhuChen.2021.LoRA:Low-RankAdaptationofLargeLanguage OneEditingforText-to-ImagePersonalization.InACMSIGGRAPH2023Conference
Models. arXiv:2106.09685[cs.CV] Proceedings(<conf-loc>,<city>LosAngeles</city>,<state>CA</state>,<coun-
TomerMichaeliInbarHuberman-Spiegelglas,VladimirKulikov.2023.AnEditFriendly try>USA</country>,</conf-loc>)(SIGGRAPHâ€™23).AssociationforComputing
DDPMNoiseSpace:InversionandManipulations. arXiv:2304.06140[cs.CV] Machinery,NewYork,NY,USA,Article12,11pages. https://doi.org/10.1145/
TeroKarras,MiikaAittala,SamuliLaine,ErikHÃ¤rkÃ¶nen,JanneHellsten,JaakkoLehti- 3588432.3591506
nen,andTimoAila.2021. Alias-FreeGenerativeAdversarialNetworks.InProc. SuWang,ChitwanSaharia,CesleeMontgomery,JordiPont-Tuset,ShaiNoy,Stefano
NeurIPS. Pellegrini,YasumasaOnoe,SarahLaszlo,DavidJFleet,RaduSoricut,etal.2023.Im-
JeonghoKim,GyojungGu,MinhoPark,SunghyunPark,andJaegulChoo.2023.Sta- ageneditorandeditbench:Advancingandevaluatingtext-guidedimageinpainting.
bleVITON:LearningSemanticCorrespondencewithLatentDiffusionModelfor InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.
VirtualTry-On. 18359â€“18369.
XiaodanLiang,SiLiu,XiaohuiShen,JianchaoYang,LuoqiLiu,JianDong,LiangLin, ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKunZhang.2023.Smartbrush:
andShuichengYan.2015.DeepHumanParsingwithActiveTemplateRegression. Textandshapeguidedobjectinpaintingwithdiffusionmodel.InProceedingsofthe
IEEETransactionsonPatternAnalysisandMachineIntelligence37,12(Dec.2015), IEEE/CVFConferenceonComputerVisionandPatternRecognition.22428â€“22437.
2402â€“2414. https://doi.org/10.1109/tpami.2015.2408360 HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang.2023.IP-Adapter:TextCompatible
ZhihengLiu,RuiliFeng,KaiZhu,YifeiZhang,KechengZheng,YuLiu,DeliZhao, ImagePromptAdapterforText-to-ImageDiffusionModels.(2023).
JingrenZhou,andYangCao.2023.Cones:ConceptNeuronsinDiffusionModels Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang.
forCustomizedGeneration. arXiv:2303.05125[cs.CV] 2018. TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.
AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,and arXiv:1801.03924[cs.CV]
LucVanGool.2022. Repaint:Inpaintingusingdenoisingdiffusionprobabilistic LuyangZhu,DaweiYang,TylerZhu,FitsumReda,WilliamChan,ChitwanSaharia,
models.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern MohammadNorouzi,andIraKemelmacher-Shlizerman.2023.TryOnDiffusion:A
recognition.11461â€“11471. TaleofTwoUNets. arXiv:2306.08276[cs.CV]
ChenlinMeng,YutongHe,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,and
StefanoErmon.2022.SDEdit:GuidedImageSynthesisandEditingwithStochastic
DifferentialEquations. arXiv:2108.01073[cs.CV]
RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or.2022.
Null-text Inversion for Editing Real Images using Guided Diffusion Models.
arXiv:2211.09794[cs.CV]
DavideMorelli,AlbertoBaldrati,GiuseppeCartella,MarcellaCornia,MarcoBertini,
andRitaCucchiara.2023a. LaDI-VTON:LatentDiffusionTextual-InversionEn-
hancedVirtualTry-On. arXiv:2305.13501[cs.CV]
DavideMorelli,AlbertoBaldrati,GiuseppeCartella,MarcellaCornia,MarcoBertini,
andRitaCucchiara.2023b. LaDI-VTON:LatentDiffusionTextual-InversionEn-
hancedVirtualTry-On.InProceedingsoftheACMInternationalConferenceon
Multimedia.
DavideMorelli,MatteoFincato,MarcellaCornia,FedericoLandi,FabioCesari,and
RitaCucchiara.2022.DressCode:High-ResolutionMulti-CategoryVirtualTry-On.
InProceedingsoftheEuropeanConferenceonComputerVision.
Tae-HyunOhNamHyeon-Woo,MoonYe-Bin.2021.FedPara:Low-RankHadamard
ProductforCommunication-EfficientFederatedLearning.arXiv:2108.06098[cs.CV]
AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,
Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE: Towards Pho-
torealisticImageGenerationandEditingwithText-GuidedDiffusionModels.
arXiv:2112.10741[cs.CV]
YotamNitzan,KfirAberman,QiuruiHe,OrlyLiba,MichalYarom,YossiGandelsman,
InbarMosseri,YaelPritch,andDanielCohen-Or.2022.Mystyle:Apersonalized
generativeprior.ACMTransactionsonGraphics(TOG)41,6(2022),1â€“10.
YotamNitzan,ZongzeWu,RichardZhang,EliShechtman,DanielCohen-Or,Taesung
Park,andMichaÃ«lGharbi.2024.LazyDiffusionTransformerforInteractiveImage
Editing.arXivpreprintarXiv:2404.12382(2024).
10MaX4Zero
APPENDIX sizeof32Ã—32and64Ã—64duringtheentiredenoisingprocess.
DetailsPropagationEnhancement-wechosetoenhanceour
A IMPLEMENTATIONDETAILS maskedextendedattentionmapsusingcontrastfactorofğ›½ =1.5.
Inourwork,weadoptedthepipelineofStableDiffusionv2using Stroke-BasedInpainting-thestrokebasedphaseusingtheMEA
thedefaulthyperparametersandinputdimensionsof512Ã—512. mechanismsettothelast35%ofthedenoisingprocess.
Fornon-squareimageswecroppedouttherelevantmaskedarea
B ADDITIONALQUALITATIVERESULTS
and patch it back at the end of the process. For the inpainting
process,weemploythestandardDDIMschedulerfor50denoising Inthefollowingfigures,youwillfindadditionalqualitativeresults.
steps.GarmentWarping-weextractedthedeepfeaturesfromthe Table-formattedresultsareshowninFig.11.Furtherresultsusing
secondlayerofthedecoderpartofthedenoisingU-Net.Forthethe out-of-domainreferencegarmentsortargetfigurescanbefound
deformationweusedthebi-linearcorrespondencematrix,while inFig.12andFig.13.SupplementaryresultsareavailableinFig.
omittingoutliermatcheswhichcausedistortionsinthewarped 14,Fig.15andFig.16.Theseresults,generatedusingmentioned
garment.MaskedExtendedAttention-Forthesecondstage,we hyperparameters,encompassexamplesfeaturingdiversehuman
replacetheconventionalself-attentionlayerswithourMEAlayers figuresinnaturalsettingsandvariousgarmenttypes.
withinthedenoisingU-Netonlyinthedecoderpartandatattention
11Orzech,etal.
Garment
Person
Figure11:In-the-wildvirtual-try-onresultsgeneratedbyMaX4Zero.Bestviewedwhenzoomedin.
12MaX4Zero
Garment
Person
Figure12:In-the-wildresultsgeneratedbyMaX4Zerousingavarietyofreferencegarmentsbeyondthenativevirtualtry-on
task.Bestviewedwhenzoomedin.
13Orzech,etal.
Garment
Person
Figure13:In-the-wildresultsgeneratedbyMaX4Zerousingunseentargetpersonimageswhichareoutofthedomainofthe
nativevirtualtry-ontask.Bestviewedwhenzoomedin.
14MaX4Zero
Figure14:ResultsGallery.AdditionalgeneratedresultsbyMaX4Zero.Bestviewedwhenzoomedin.
15Orzech,etal.
Figure15:ResultsGallery.AdditionalgeneratedresultsbyMaX4Zero.Bestviewedwhenzoomedin.
16MaX4Zero
Figure16:ResultsGallery.AdditionalgeneratedresultsbyMaX4Zero.Bestviewedwhenzoomedin.
17