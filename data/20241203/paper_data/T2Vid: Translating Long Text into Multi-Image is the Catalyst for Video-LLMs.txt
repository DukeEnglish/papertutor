T2Vid: Translating Long Text into Multi-Image
is the Catalyst for Video-LLMs
ShukangYin1‚Ä†, ChaoyouFu2‚Ä†‚Ä°*, SiruiZhao1‚Ä°‚Ä†, YunhangShen3, ChunjiangGe4, YanYang2
ZuweiLong3, YuhanDai1,TongXu1, XingSun3, RanHe5, CaifengShan2, EnhongChen1‚Ä°
1USTC,2NJU,3TencentYouTuLab,4THU,5CAS
Abstract modelsalsoexcelinfollowinguserinstructionsandgeneral-
izingtonewtasks.Amainstreamparadigmfordeveloping
The success of Multimodal Large Language Models such models takes a two-stage training strategy. The first
(MLLMs) in the image domain has garnered wide atten- stage, pretraining, mainly serves to align vision modality
tion from the research community. Drawing on previous withtextandinjectvariouskindsofvisualknowledgeinto
successfulexperiences,researchershaverecentlyexplored themodel.Inthisstage,large-scaledatasetsoftext-image
extending the success to the video understanding realms. pairsareoftenused,suchasLAION[30]andCC[32],com-
Apartfromtrainingfromscratch,anefficientwayistouti- prisingalargeproportionofthetotalcomputeandinjecting
lizethepre-trainedimage-LLMs,leadingtotwomainstream abundantvisionknowledgeintomodels.Somemethodsalso
approaches,i.e.zero-shotinferenceandfurtherfine-tuning incorporateOCRanddetection-relateddatatoimprovefoun-
withvideodata.Inthiswork,ourstudyoftheseapproaches dational capabilities [1, 6]. The second stage, instruction
harvests an effective data augmentation method. We first fine-tuning, adapts models to accommodate various tasks
makeadeeperinspectionofthezero-shotinferencewayand and helps generalize to new instructions. Training in this
identifytwolimitations,i.e.limitedgeneralizationandlack stagetypicallyinvolvesinstructiondataobtainedfromself-
of temporal understanding capabilities. Thus, we further instructionoradaptationoftask-specificdatasets(e.g.VQA
investigate the fine-tuning approach and find a low learn- andchartunderstandingdatasets).
ingefficiencywhensimplyusingallthevideodatasamples, Researchershaverecentlyshiftedtheirfocusfromsingle-
which can be attributed to a lack of instruction diversity. image models to more advanced ones that support video
Aiming at this issue, we develop a method called T2Vid understanding.Borrowingsuccessfulexperiencefromdevel-
to synthesize video-like samples to enrich the instruction opingimagemodels,somevideocounterpartsaretypically
diversityinthetrainingcorpus.Integratingthesedataen- trainedfromscratch,followingasimilartwo-stagetraining
ablesasimpleandefficienttrainingscheme,whichachieves paradigm [7, 18]. Apart from this path, some researchers
performancecomparabletoorevensuperiortousingfull utilizepre-trainedimage-LLMsinstead.Typicalapproaches
video datasets by training with just 15% the sample size. include zero-shot inference [12, 16, 35] and further fine-
Meanwhile, we find that the proposed scheme can boost tuning[7,15,18,22].Nevertheless,afurtherinspectionof
theperformanceoflongvideounderstandingwithouttrain- both approaches is still lacking, leaving many underlying
ingwithlongvideosamples.Wehopeourstudywillspark factorsthatinfluencemodelperformancelargelyunexplored.
morethinkingaboutusingMLLMsforvideounderstanding Tounveilthesefactors,thisworkinvestigatesthesetwoap-
andcurationofhigh-qualitydata.Thecodeisreleasedat proaches.Throughachainofexplorations,wefinallyharvest
https://github.com/xjtupanda/T2Vid. anefficientschemeforfine-tuningimage-LLMsforvideo
understanding,dubbedT2Vid.
We start with the probing of zero-shot inference with
1.Introduction Image-LLMs. Through quantitative and qualitative evalu-
ation, we find that: (1) Due to a limited effective context
ThepastfewyearshaveseentherapidprogressofMulti-
window, it is challenging for image-LLMs to generalize
modalLargeLanguageModels(MLLMs)[1,24,40].Apart
tolongervideosbysimplyusingmoreframes;(2)Instead
fromsolvingtraditionalvisiontasks(suchasVQA),these
ofunderstandingthetemporalcorrelationsamongframes,
image-LLMsmightadoptan‚Äúattend-aggregate‚Äùpatternfor
*Projectleader.
‚Ä†Equalcontribution. videounderstanding.Thus,weswitchtoinvestigatingthe
‚Ä°Correspondingauthors. fine-tuningapproachsincefurtherenhancingtemporalun-
1
4202
ceD
2
]VC.sc[
2v15991.1142:viXraderstandingcapabilitiescanboosttheperformanceofvideo throughfine-tuning.
understanding. Nevertheless, we find a low learning effi-
2.2.Zero-shotInferenceforVideoUnderstanding
ciencyproblemthroughasimpledatascalingexperiment.
Aninspectionofdatacharacteristicssuggeststhatthismight Apartfromfurthertraining,analternativewayistodesign
beduetoalackofinstructiondiversityinthetrainingcorpus. training-freeapproachestoperformzero-shotinferencewith
Toaddressthisissue,weproposeanefficientschemetosyn- image-LLMs.Forexample,IG-VLM[16]arrangesmultiple
thesizevideo-likesamples.Specifically,weuseexistingtext video frames into a single image grid and designs corre-
instruction data whose sample comprises a (long-context, spondingpromptstohelpmodelsunderstandthegridformat.
instruction, answer) triplet. The long-context part is split Given a limited total resolution of image input, there is a
into multiple segments and then further transformed into trade-off between resolutions and the number of sampled
images,andtheinstructionandanswerstayintact.Processed videoframes.SF-LLaVA[35]scalesupthenumberofin-
inthisway,thesyntheticsampleshavethesamestructure putframesbyintroducingadual-streamdesign,wherethe
asvideoinstructiondataandcanbeincorporatedseamlessly slow branch utilizes more spatial tokens extracted with a
intothetrainingcorpus.Moreover,theproposedsynthetic lowerframerate,whilethefastbranchistheopposite.Free
methodcaneffectivelyenrichtheinstructiondiversityinan Video-LLM [12] focuses on efficient inference and intro-
economicalway,thusboostinglearningefficiency. duces independent prompt-guided sampling strategies for
Comprehensiveexperimentsdemonstratethatourmeth- spatialandtemporaldimensions.
ods can facilitate efficient fine-tuning of image-LLMs for However,thesemethodsaregenerallyevaluatedonmore
generalvideounderstandingandassistmodelsinthecom- traditionalbenchmarkslikeMSVD-QA[33],TGIF-QA[14]
prehensionoflongvideos.Wehopethatourfindingsand andActivityNet-QA[41]).Thesebenchmarksaregenerally
proposedschemecanmotivatemorethinkingaboutMLLMs domain-specific and focus on certain basic skills, such as
invideounderstandingtasksandsparkthecurationofhigher- actionrecognitionandrepetitioncount,whichlackcompre-
qualityvideodata. hensiveness in both length coverage (especially in longer
videos)andskillcoverage.Moreover,thequestionsasked
2.RelatedWork ofteninvolveshallowperceptionwithoutdeeperreasoning.
Recently, with the rise of benchmarks specifically de-
2.1.MultimodalLargeLanguageModels
signedforMLLMs[9,18,26],amorein-depthandcompre-
hensiveevaluationhasbecomemoreaccessible.Compared
The research on Multi-modal large language models
toprevioustraditionalbenchmarks,thesenewlydeveloped
(MLLMs)hasgarneredwideattentionfrombothindustry
benchmarksaregenerallymorechallenging,oftenentailing
andacademia.Stimulatedbytheextraordinarycapabilities
composite skills and a finer-grained understanding of the
thatGPT-4[29]serieshaveshown,researchershavedelved
video(e.g.theplotinthemovieorcausalrelationshipsbe-
intodevelopingopen-sourcedmodelsthatcancompetewith
tweenevents),andcanbemuchlongerinduration(e.g.up
orsurpassthisamazingclose-sourcedproduct.
to60minutesintheVideo-MMEbenchmark).Inthiswork,
Image-LLMs. To develop image-LLMs, the mainstream
weidentifythepotentiallimitationsofzero-shotinference
approachistobuilduponpowerfulpre-trainedLLMsand
usingthesenewlydevelopedvideobenchmarks.
extendLLMswiththecapabilitytoperceiveandreasonwith
images [1, 24]. Based on a two-stage training recipe, i.e.
3.ModelFormulation
image-text alignment training and instruction tuning, the
developed model can fulfill a wide range of multimodal WefocusonMLLMsofthemainstreamtoken-levelfu-
userqueriesandpresentitsanswersinuser-friendlynatural sion architecture [40], which typically comprises a vision
languagesentences. encoder,aconnector,andanLLMbackbone.
Video-LLMs.Followingthesuccessofimage-LLMs,subse- Givenauserqueryandvisualinput,thevisionencoder
quentendeavorsaimtoexpandthetriumphofimagecom- extracts features from the visual input, and the connector
prehension to more intricate video understanding. Works further projects the vision features to align with text em-
likeVideo-ChatGPT[27],VTimeLLM[13],PLLaVA[34] beddingspace.Then,theprocessedvisionfeaturesaresent
and LLaVA-NeXT-Video [44] attempt further fine-tune togetherwiththetextembeddingsofuserqueriesintothe
image-LLMs to enhance video understanding capability. LLMbackbone tooutput naturallanguage responsein an
Other research [7, 15, 18, 22] explores training from pre- auto-regressiveway:
trainedLLM,followingthebasicalignment-then-finetuning
L
paradigmsimilartoimage-LLM.Theseapproachesusually (cid:89)
p(w |w ,w )‚àº P(w |w ,w ,w ) (1)
involve joint training that mixes image and video data in o V T t <t V T
t=1
thetrainingcorpus.Inthisstudy,webuilduponpre-trained
image-LLMsandenhancevideounderstandingcapabilities wherew = {w }L istheoutputwordtokensequence
o o,t t=1
2oflengthL,w representsthevisualtokensprocessedbya otherbenchmarks.
V
visionencoderandaconnector,w isthetextembeddings Toensurerobustandefficientjudgingofmodelanswers,
T
oftheuserquery. weuseacombinationofexactmatchingandLLMmatching
Duringourexploration,wemainlyutilizethreeimage- forassessment.Moredetailsabouttheimplementationof
LLMs,includingMini-InternVL-Chat-4B-V1.5[6](termed thisevaluationschemeareavailableintheAppendix.
as InternVL hereafter), MiniCPM-Llama3-8B-V2.5 [37]
(termed as MiniCPM-8B hereafter), and Idefics3-8B- 5.Motivation
Llama3 [17] (termed as Idefics3-8B hereafter). These
5.1.TheLimitationsofZero-shotReasoning
instruction-tuned models are trained with massive image
dataandequippedwithstrongfoundationalcapabilities.To 5.1.1 LackofFrameCountGeneralization
supporthigher-resolutionvisioninput,thesemodelsadopt
Alookatzero-shotperformance.Togainaninitialgrasp
thepatchifyingtechnique[21,23,38]withadynamicreso-
of zero-shot performance, we compare the results with
lutionscheme,whereanimagecanbecroppedintomultiple
some representative video-LLMs of similar LLM param-
sub-imagesaccordingtodifferentaspectratios.Specifically,
eter size, including Video-LLaVA [22], VideoChat2 [18],
InternVLsupportsupto13sub-imageswhereeachoneis
Chat-UniVi-v1.5[15],VideoLLaMA2[7],VITA[10],and
convertedinto256visualtokens;MiniCPM-8Bslicesimages
Kangaroo[25],assummarizedinTab.1.
intoamaximumof10patches,whereeachisrepresentedby
The table shows that, through zero-shot inference, the
96visualtokens;Idefics3-8Bsupportsupto26sub-images,
image-LLMmodelalreadyoutperformsavarietyofvideo-
eachcomprising169tokens.
LLMsoflargerLLMparametersizes.Thissuggeststhata
4.EvaluationSetup well-trainedimage-LLMcansomehowgeneralizetovideo
understanding.Thezero-shotvideounderstandingcapabil-
Toevaluatethemodelcapabilitiesinanefficientandcom- itycanbeattributedtothewidelyusedtrainingtechnique
prehensive way, we use Video-MME [9], MVBench [18], ‚Äúpatchifying‚Äù,whichcropsalargeimageintomultiplesub-
andTempCompass[26]asourbenchmarks.Wemainlyuse imagesandexemptstheneedforlarge-scalefine-tuningof
Video-MMEinourprobingsinceitcomprehensivelycovers vision encoders. Processed in this way, an image can be
different visual domains and temporal ranges. We do not viewedasaseriesofsub-images,andthemodelprocesses
usetraditionalvideo-QAbenchmarks(e.g.MSVD-QA[33], allthesesub-imagestogethertounderstandtheoverallimage.
TGIF-QA [14], ActivityNet-QA [41]) since these bench- Similarly,avideoisnaturallyasequenceofimageframes.
marks are generally domain-specific and focus on certain Duetothissimilarityininputstructure,itisreasonablethat
skills,suchasactionrecognitionandrepetitioncount,which image-LLMssomehowgeneralizetomoreframeswithout
lackcomprehensivenessinbothlengthcoverageandskill furtherfine-tuning.
coverage.Moreover,thequestionsaskedofteninvolveshal- Theinfluenceofdifferentframecounts.Nowthatimage-
lowperceptionwithoutdeeperreasoningsinceearlymod- LLMs can achieve competitive performance, we further
elsgenerallylackreasoningcapacity,whereasrecentLLM- probethegeneralizationpotentialw.r.t.inputframes.Specif-
based models excel. We illustrate more about the bench- ically,weexperimentwithdifferentframecountsandvisual-
marksusedasfollows: izethechangesinperformance,asshowninFig.1.Inthe
Video-MMEisacomprehensivebenchmarkdesignedforthe figure,wecanobservethattheperformancegenerallyfirst
evaluationofvideo-LLMs.Fortemporalcoverage,videos improves as more frames are available (around the green
ofshortlength(upto2minutes),mediumlength(4‚Äì15min- dottedline),thenstaysrelativelystableorgraduallydrops
utes),andlongerduration(30‚Äì60minutes)areincluded.The (beforehittingthereddottedline).Thephenomenonindi-
videosandannotationsaremanuallycollectedandfiltered. catesthatthoughimage-LLMscangeneralizetomoreimage
Weonlyusetherawframeswithoutthesubtitlestoevaluate numbersthanseeninthetrainingprocess,thereispotentially
videounderstandingcapabilities. anupperlimitofframenumberthatmodelscaneffectively
MVBenchdesignsasetof20videotasksthatcoverbothper- process.Therefore,zero-shotinferenceforlongervideoun-
ceptionandcognition,suchasscenetransitionandepisodic derstandingcanbechallenging.Moreresultsandanalyses
reasoning.ComparedtoVideo-MME,thevideosaresourced inthisregardareavailableintheAppendix.
fromexistingbenchmarks,andtheQAsareautomatically
generatedforthe20pre-definedtasks.
5.1.2 LackofTemporalUnderstandingCapabilities
TempCompass focuses on fine-grained temporal aspects,
suchasaction,speed,andattributechange.Thevideosand Sinceimage-LLMsarenottrainedwithanyvideodata,
metainformationaremanuallycollected,afterwhichannota- itisintriguingtoinvestigatehowthesemodelscanreason
tionsaregeneratedbyLLMswiththeaidofmetainformation. withvideos.Thispromptsustofurtherprobeiftheimage-
Weusethemultiple-choiceQA(MCQ)formattoalignwith LLMreallyunderstandsavideoandthetemporalstructure
3LLM
Methods ShortVideo MediumVideo LongVideo Overall
Params(B)
Video-LLaVA[22] 7 45.3 38.0 36.2 39.9
VideoChat2[18] 7 48.3 37.0 33.2 39.5
Chat-UniVi-v1.5[15] 7 45.7 40.3 35.8 40.6
VideoLLaMA2[7] 8 56.0 45.4 42.1 47.9
InternVL(Zero-shot)[6] 3.8 61.3 51.8 44.3 52.5
VITA[10] 8x7 65.9 52.9 48.6 55.8
Kangaroo[25] 8 66.1 55.3 46.6 56.0
InternVL(200Kfulldata) 3.8 66.7 54.2 48.1 56.3
InternVL(30Khybriddata) 3.8 67.0 53.7 49.3 56.7
Table 1. Accuracy comparisons of different methods on the Video-MME benchmark, ranking in ascending order regarding overall
performance. Ourmethods useonly15%ofthetotalsamplesizecomparedtothefullvolumevideodatasets(200K)forfine-tuningand
achievecomparableperformance.30Khybriddatacomprise20Kdatasampledfromthefullvideodatasetsand10Ksynthesizedfromtext
datausingourT2Vidmethod.Bolddigitsindicatethebestperformance. Lightgray denoteszero-shotinferencewithimage-LLM.
Asshowninthefigure,thoughthemodelanswersina
toneasifitisviewingavideo,itdoesnotreallyunderstand
  
the temporal relationships between frames but instead ag-
 6 K R U W  P D [  W U D L Q L Q Q J  I U D P H V
 0 L G  P D [  W U D L Q L Q J  F R Q W H [ W gregates information from different frames in a relatively
    / R Q J  P D [  / / 0  F R Q W H [ W independentway.Morespecifically,thetextingreen(cor-
 2 Y H U D O O
responding to the 4th and 8th frames) mixes together the
   subtitle information. However, the two descriptions actu-
allyhaveinherenttemporalstructure,wherethe4-thframe
givesanoverallintroduction(ofthetown‚ÄúHallstatt‚Äù)
  
andthe8-thframeintroducesspecificscenicspots.Forthe
purplepart(the5thframe),themodelmistakenlytakesthe
  
descriptionoftransportationintownasthevideo‚Äôstitle.No-
tably,thoughpartoftheupperhalfofthetexthasnotshown
   upcompletely,themodelsuccessfullyinfersthecomplete
                  
   I U D P H V textusinglanguageprior,demonstratingthemodel‚Äôsstrong
Figure1.Zero-shotperformanceonVideo-MMEwheninference OCRcapabilities.Then,inthesecondparagraph,themodel
withadifferentnumberofframes.Image-LLMscangeneralizeto goesbacktodescribetheearlier2-thand3-thframe,respec-
inputmoreframesthanseeninthetrainingprocessbuttoarather tively, using phrases like ‚Äúin the background‚Äù and
limitedextent,makingithardtotrulyunderstandlongervideos.The ‚Äúanother image‚Äùasifitisprocessingmultipleimages
consistentincreaseofinputframesbringslimitedgainstotheperfor- rather than a holistic video. This is expected since, when
manceorcanevenbedetrimental.‚Äúmax training frames‚Äù
dealingwithapatchifiedimage,themodeloftenattendsto
denotesthemaxnumberofpatchesusedforasingleimagewhen
multiplesub-images,extractsinformationfromthem,and
training; ‚Äúmax training context‚Äù denotes the maximum
finallyassemblesafinalanswerratherthanfocusingonthe
context window set when training; ‚Äúmax LLM context‚Äù de-
temporalstructurewithinthesepatches.
notesthecontextwindowoftheoriginalLLMbackbonebefore
furthermultimodaltraining. In contrast, the fine-tuned video model learns more
temporal relationships and thus presents a more coherent
or just attends to multiple video frames and reasons with description of frame sequence, using correct phrases as
theinformationextractedfromtheseframes,justlikedeal- connectionssuchas‚Äúthe video starts with‚Äùand
ingwithapatchifiedimage.Ourcasestudyindicatesthat ‚Äútransitions to‚Äù.Nevertheless,wenoticethathalluci-
thelattercase(whichwecall‚Äúattend-aggregate‚Äùpat- nations[19,39]alsoincrease.Besidesinherentnoiseinthe
tern)ismorelikely.Specifically,wesamplesomevideosand trainingcorpus,thephenomenonishighlycorrelatedwith
promptthemodelinazero-shotway,suchas‚ÄúDescribe thedatabias.Forexample,manyanswersofsamplesinthe
this video.‚Äù.Atypicalqualitativecase,includingthe ShareGeminidatasetstartwithphraseslike‚ÄúThe video
answers of zero-shot inference and fine-tuned model, is begins with a black screen‚Äù, contributing to a
showninFig.2. strongpriorinthefine-tunedmodel.Welistmorequalitative
4
  F F $1 2 3 4
5 6 7 8
9 10 11 12
Zero-shot:
The video showcases the picturesque town of Hallstatt, Austria (Trends Tours), known for its stunning alpine scenery, historic architecture, and vibrant local
culture. Through the images, viewers can explore various attractions such as the Gothic-style Catholic Church of Hallstatt and the Gothard Ossuary (No.8), which
are recognized as UNESCO World Heritage Sites (No.4). The video's title "Hallstatt is accessible through road and rail transport" (No.5) highlights the town's
easy-to-reach nature via different modes of transport. In the background, one scene features an individual taking selfies at the edge of a boat (No.3), emphasizing
Hallstatt's scenic lake and tranquil atmosphere. Meanwhile, another image focuses on the quaint charm and bustling activities alongside Hallstatt's lake (No.2),
demonstrating a harmonious blend of tradition and modern tourism.
Fine-tuned:
The video starts with a black screen and then transitions to a collage of images of Hallstatt, Austria, featuring boats on the water, buildings, boats on the
water, and a sign reading "Hallstatt." (No.3,4,6), A town square with a large statue is shown, along with a train (No.5), in which a woman is sitting. The video
then transitions to more images of Hallstatt, including a lake with swans and a boat, a mountain with a church (No.11-12), and a train that stops and several
people disembark. The video ends with a town square with people walking around. The overall narrative of the video is to show the scenery of Hallstatt, including
the lake, town square, train stop, and mountain. The video uses a variety of images to show the different parts of Hallstatt and highlights its scenic beauty.
Figure2.Aqualitativecaseusingtheprompt‚ÄúDescribethisvideo.‚Äù,wherethevideocentersaroundtheintroductionofatriptoatown.
Comparedwiththefine-tunedmodel,thezero-shotcasereasonswithanddescribesvideoframesinarelativelyindependentwaywithout
understandingthetemporalcorrelations.Thispatternfollowsthecommonbehavioroftheimagemodel.Thekeyframesarehighlightedwith
colors(exceptforred,whichisusedtoindicatehallucinations).Thenumbersontheupper-leftcornerofeachframeindicateframeorder.
Zoominforbettervisibility.
casesandanalysesintheAppendix. 5.2.ThePitfallsinFine-tuning
In light of the limitations of zero-shot inference with
image-LLMs,weturntoinvestigatethefine-tuningapproach
8000 forimprovingthevideounderstandingcapabilitiesofmodels.
30000 7000 In this section, we first illustrate the training settings and
25000 6000 thentheexperimentalfindings.
20000 5000
4000
15000
3000
10000 5.2.1 TrainingDatasets
2000
5000 1000
0 0 With8NVIDIAA800GPUs,wetrainend-to-end(except
0 102030405060708090100 0 50 100 150 200 250 300
# Frames # Frames forInternVL-4B,wherewefreezethevisionencoder)with
(a)Share-Gemini (b)Video-ChatGPT
alearningrateof5e-6andaglobalbatchsizeof64.Dur-
Figure 3. Video length statistics of ShareGemini and Video- ingourinvestigation,weutilizetworepresentativetypesof
ChatGPTdatasets.Bothdatasetsmostlycovervideosshorterthan datasets,i.e.,video-captionpairsandvideoinstructiondata.
3.5minutes.WeextractvideoframesatanFPSof1.Forbetter
Specifically,wechoosetheShareGemini[31]datasetandthe
visibility,wepicksampleswithframenumberslowerthan99.9
Video-ChatGPT[27]datasetascaptionandinstructiondata,
percentileforvisualization.
respectively.WeextractvideoframesatanFPSof1foreach
video.Inconsiderationofefficiency,weuseupto64frames
5
soediV
#
soediV
#forInternVL-4Band24framesforbothMiniCPM-8Band thetrainingcorpusaremostlyshorterthan3.5minutes,ren-
Idefics3-8B.Whenthetotalnumberofframesexceedsthe deringalargegapinthetemporalrangebetweentraining
thresholds,weuniformlydownsamplethevideoframes.We and testing (in the evaluation phase, video lengths range
showthestatisticsofvideolengthsinFig.3andprovidea from4‚Äì15minutesand30‚Äì60minutesformediumandlong
moredetailedintroductionofthetwodatasetsbelow. videos,respectively).
ShareGemini-Webvid-core100k. It is a video caption Moreimportantly,theadditionalgainofscalinguptrain-
datasetwith100Ksamplesintotal.Thevideosarecurated ingvolumefrom30Kto200Kdatasamplesislimited(+0.6
fromWebVid[2],aweb-scalevideo-captiondatasetcover- overall). In view of the huge difference in data samples
ingopendomainsandgeneralsubjects.Regardingduration, used,thelearningefficiencyoffine-tuningwiththesevideo
thedatasetmainlycontainsshortvideoswithlengthsshorter datasetscanbequitelimited.Thephenomenonalsosuggests
than30seconds. thattherecouldbehighredundancyinthetrainingcorpus.
ThecaptionsareannotatedbycallingthestrongGemini-
1.5-Pro[11]API.Toensurethediversityofvideocontent,
an advanced clustering algorithm [3] is used to filter out 5.2.3 ALookatInstructionDiversity
highlysimilarvideos.Forsimplicity,werefertothisdataset
asShareGeminiinthefollowingpartsofthepaper.
Video-ChatGPT. The video instruction dataset contains
100Kvideo-instructionpairs.Thevideosinthiscollection ShareGemini
arederivedfromActivityNet[4].Thedataset‚Äôsdurationis 60 Video-ChatGPT
morediverseanduniform,yettheaveragevideolengthis
40
nomorethan3.5minutes.Therearebroadlythreetypesof
instructions: video summarization, questions about video
20
content,andcreative/generativetasks.
The dataset is annotated in a semi-automatic manner. 0
Some of the data samples are manually annotated by hu-
manannotatorsbyrefiningandenrichingthevideocaptions. 20
OtherinstructiondataaregeneratedbyGPT-3.5withtheaid
40
ofoff-the-shelfdensepredictionandcaptioningmodels.
60
5.2.2 LowLearningEfficiencyofFine-tuning 80 60 40 20 0 20 40 60
Figure4.t-SNEplotofinstructiondistributionofvideodatasets-
ShareGeminiandVideo-ChatGPT.5,000instructionsaresampled
Setting S M L Overall
fromeachdataset.ShareGeminiexhibitssomeclearclusters,while
Zero-shot 61.3 51.8 44.3 52.5 Video-ChatGPTisrelativelymorediverse.
30Ksampleddata 66.2(+4.9) 53.3(+1.5) 47.4(+3.1) 55.7(+3.2)
Prior studies have underscored the importance of in-
200Kfulldata 66.7(+0.5) 54.2(+0.9) 48.1(+0.7) 56.3(+0.6)
struction diversity for fine-tuning LLMs [46] and image-
LLMs[42].Thesefindingspromptustoconductaninspec-
Table2.AccuracycomparisonsofdifferentsettingsontheVideo-
MMEbenchmark,rankinginascendingorderregardingoverallper- tion of data in this aspect. Thus, we follow previous ap-
formance.S,M,andLindicateshort,medium,andlongvideos,re- proaches[36,45]tovisualizethediversityofinstructionsin
spectively.Zero-shotindicatesdirectinferencewiththeimage- thetrainingcorpus.Specifically,werandomlysample5,000
LLM,andfulldatasettingindicatesfine-tuningwithallthevideo instructionsfromShareGeminiandVideo-ChatGPT,respec-
datasamplesinthetrainingcorpus.Theperformancegainsofthe tively.Then,theinstructionsareembeddedandvisualizedus-
lattertwosettingsarecalculatedrelativetothepreviousrow. ingthet-SNEtechnique,asshowninFig.4.Inthefigure,the
distributionofShareGeminiexhibits9clearclusters.Thisis
We experiment with fine-tuning with only parts of the becausethedatasetsamplesfromafixedpoolof9templates
video data and with the full volume. Specifically, we ran- asinstructions,eachofwhichisavariantof‚ÄúDescribe
domly sample 15% of the total video samples from each this video in detail.‚ÄùIncontrast,thedistribution
videodatasetandcomparetheperformanceasinTab.2.As ofVideo-ChatGPTismorediverse,asitincludesnotonly
showninthetable,comparedwiththeshortvideoset,the videosummarizationbutalsoquestionsrelatedtospecific
gainsinmediumandlongvideosetsarerelativelysmaller. videocontent.Overall,theinstructionsforthesevideosam-
This can be attributed to the fact that the video lengths in plesarenotdiverseenough.
66.OurMethods
75
Since currently available video data are limited in in-
structiondiversity,andannotatinghigh-qualityvideodata
50
iscostly,weaimtoexpandtheinstructiondiversityusing
syntheticdata.
25
Long-context Text seg. 1 Text seg. N Image 1 Image N 0
Split Transcribe
¬∑¬∑¬∑ ¬∑¬∑¬∑
25
Inst: What are the main Inst: What are the main
limitations of the paper? Copy limitations of the paper?
Ans: The paper assumes‚Ä¶ Ans: The paper assumes‚Ä¶ 50
LongAlpaca
Text Sample Video Sample
LongQLoRA
75 ShareGemini
Figure5.Aconceptualillustrationofourschemeforsynthesizing
Video-ChatGPT
video-likesamplesfromtextdata.Foreach(long-context,instruc-
100 75 50 25 0 25 50 75 100
tion,answer)triplet,wesplitthelongcontextintosegmentsand
transcribethesesegmentsintoaseriesofimages,simulatingthe
Figure6.t-SNEplotofinstructiondistributionafterapplyingour
structureofavideo.Zoominforbettervisibility.
proposedmethods.Byincorporatingtextdata,i.e.LongAlpaca[5]
Arichsourceofinstructiondataliesinthetextdomain, andLongQLoRA[5],theoveralldistributionbecomesmorediverse.
andtheycaneffectivelyenrichthediversityofinstruction. 5,000instructionsaresampledfromeachdataset.
Nevertheless, there is inherently a modality gap between
thetextandvisualdomain.Tobetterutilizethesedata,we
trainingformat.
bridge the modality gap by synthesizing images with the
text.Fig.5illustratesouroverallidea.Specifically,foreach
DataMix S M L Overall
(long-context,question,answer)triplet,wedividethecontext
30KShare-Gemini 65.7 52.8 46.1 54.9
informationintomultiplesegmentsandembedthetextof
eachsegmentintoasequenceofimages.Forexample,the 30KVideo-ChatGPT 66.3 53.0 47.3 55.6
context can be a section of a book or an academic paper, 15KShare-Gemini
66.2 53.3 47.4 55.7
whiletheinstructionandtheanswerarecenteredaroundthe 15KVideo-ChatGPT
context,e.g.aninquirytosummarizethegivenparagraphs 10KShare-Gemini
and the corresponding synopsis. After the transformation, 10KVideo-ChatGPT 67.0 53.7 49.3 56.7
thestructureoftheseinstructiondataisexactlythesameas 10Ksynthetic
thevideocounterparts,andwecanincorporatethesynthetic 10KShare-Gemini
dataintothevideotrainingcorpuswithoutextraprocessing. 10KVideo-ChatGPT 67.3 52.4 47.7 55.8
Ourproposedschemeenjoysthreebenefits:(1)Mixing 10Kpuretext
intextdatacaneffectivelyenrichtheinstructiondiversity
Zero-shot 61.3 51.8 44.3 52.5
(Fig.6),thusimprovingthelearningefficiencyforvideofine-
200Kfulldata 66.7 54.2 48.1 56.3
tuning;(2)Imagessynthesizedfromtextcanemulatethe1D
temporalstructureofvideoframessincetextsegmentsare
Table 3. Accuracy comparisons of different data compositions
generallycorrelatedinthecontext,thusmitigatingthegap
ontheVideo-MMEbenchmark. Ourproposedscheme achieves
betweencommonvideosamplesandsyntheticones;(3)Text
anoverallperformancesuperiortootherdatamixesofthesame
dataareeasiertocollectthanvideosamples.Thus,utilizing
amount(30K)andevenmoredata(200K).The Zero-shot and
syntheticdatacanbeeconomical.
200Kfulldata fine-tuningsettingsarelistedforcomparison.
Onarelatednote,LLaMA-VID[20]andLongVA[43]
alsoutilizetextdatatoassistvideounderstanding,andwe
7.EvaluationonProposedMethods
underscore the differences here: (1) Different objectives:
LLaMA-VIDandLongVAusetextdatainthefine-tuning
7.1.AblationonDifferentDataCompositions
and continued pre-training stage, respectively, in order to
expandthecontextwindowforlongvideounderstanding.In Inordertoexaminetheimpactofdifferentdatacomposi-
contrast,ourmethodsmainlyutilizethediversityofinstruc- tionsandvalidatetheeffectivenessoftheproposedmethod,
tions in the text data to facilitate efficient fine-tuning. (2) we conduct an ablation study and construct the following
Differentprocessingmethods:LLaMA-VIDandLongVA settingswiththesameamountoftotaldatasamples:
directlyusethepuretextdataintraining,whileourmethods ‚Ä¢ 30KvideosamplesfromShareGemini.
transformtextsamplesintovideo-likesamplesandunifythe ‚Ä¢ 30KvideosamplesfromVideo-ChatGPT.
7Video-MME
MVBench TempCompass Avg.
Short Medium Long Overall
TEXT-AIDEDMODELS
LLaMA-VID[20] 40.6 40.2 34.9 38.6 41.9 45.6 42.0
LongVA[43] 61.1 48.8 45.4 51.8 50.7 56.9 53.1
MINICPM-8B[37]
Zero-shot 58.3 46.1 40.1 48.2 42.9 49.1 46.7
200Kdata 60.3 47.3 44.7 50.8 48.0 54.7 51.2
30Kdata 61.9 51.7 45.6 53.0 48.4 56.8 52.7
IDEFICS3-8B[17]
Zero-shot 61.3 47.0 45.1 51.2 49.6 55.9 52.2
200Kdata 62.0 53.1 44.9 53.3 50.7 62.9 55.6
30Kdata 67.1 53.8 48.0 56.3 51.6 62.3 56.7
Table4.Theperformanceofdifferentmodels/settingsonthreevideobenchmarks. Ourmethods useonly15%ofthetotalsamplesize
comparedtothefullvideosets(200K)forfine-tuningandachievesimilarorevensuperiorperformance.Thehighestperformanceofa
modelamongallsettingsishighlightedinbold.LLaMA-VIDandLongVAalsousepuretextdatabutwithouttransformation,whichis
differentfromourmethodsandobjectives.
‚Ä¢ 15K video samples from ShareGemini and 15K from thatis1.2pointshigherthanthefulldatatraining(asshown
Video-ChatGPT,respectively. inthe4throwcomparedto200KfulldatainTab.3).This
‚Ä¢ Our proposed scheme: 10K samples each from resultsuggeststhatfine-tuningwithalongmultimodalcon-
ShareGeminiandVideo-ChatGPT,plus10Ksamplessyn- text can enhance the comprehension of longer videos. In
thesizedfromtextdata(5KfromLongAlpacaand5Kfrom thefollowingsection,wewillpresentadditionalresultsto
LongQLora,respectively). furtherillustratethispoint.
‚Ä¢ Same video samples as above (20K in total), plus 10K
7.2.GeneralizationAbilityofOurProposedScheme
samplesofcorrespondingpuretextdata.
The effectiveness of our proposed methods. As shown We further verify the generalization ability of the pro-
inTab.3,comparingthefirstthreerows,wecanfindthat posed scheme and evaluate our methods on more image-
whenusingthesameamountofvideosamples,trainingonly LLMsoflargerparametersizes,includingMiniCPM-8Band
withShareGeminiisnotaseffectiveasusingmorediverse Idefics3-8B,acrossdifferentbenchmarks,asshowninTab.4.
datacompositions.Meanwhile,usingthesameamountof Generaleffectivenessoftheproposedscheme.Theresults
data,ourproposedscheme(the4throw)achievesthebest inthetablesuggestthatonbothmodels,thoughfine-tuning
performance.Moreover,whencomparedwiththefull200K withfewersamples,ourproposedschemeachievesperfor-
data fine-tuning setting, our proposed scheme uses much manceonparwithorbetterthanfine-tuningwiththecom-
fewerdatasamples(only15%)toachievecomparableper- pletevideodatasets.Wealsocomparewithrelatedmethods,
formance, and the training cost reduces from 276.8 GPU LLaMA-VIDandLongVA,whichalsoutilizetextdatabut
hoursto33.6GPUhours,makingan8.2xspeedup.Theover- intheoriginalformofpuretext,inordertoexpandthecon-
allresultsdemonstratetheimportanceofinstructiondiversity textwindow.Incontrast,ourmethodsexcelinenrichingthe
andtheeffectivenessofourproposedmethods. instructionsviasynthesizingvideo-likedatafromtextdata.
Boostinlongvideounderstanding.Onthelongvideosets
Notably, replacing the synthetic data with the original
oftheVideo-MMEbenchmark,ourmethodsprovidecon-
pure text counterpart achieves an overall inferior perfor-
sistent gains compared to the full data fine-tuning setting,
mance.Wehypothesizethatthisisduetotheinherentdo-
thoughweusevideosamples(mostnolongerthan3.5min-
main gap between vision and text. Thus, to simulate the
utesinthetrainingsets)oflengthsmuchshorterthanthetest
structureofvideosequences,transcribinglongtextintoim-
sets(30-60minutes).Theresultsindicatethatourproposed
agesmightbenecessary.
methodscanassistmodelsinunderstandinglongvideos.
Can synthetic data help models understand longer
videos?Interestingly,inthetrainingstage,weonlyutilize
8.Conclusion
synthetic samples of long multimodal context instead of
authenticlongvideosamples.However,onthelongvideo This paper revisits the two mainstream approaches of
benchmarkset,ourproposedmethodstillachievesascore utilizing image-LLMs to perform video understanding. A
8chainofinvestigationsrevealspossibledrawbacksinboth Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong
approaches,underscoringthebottlenecksofzero-shotinfer- Chen,RongrongJi,andXingSun.Video-mme:Thefirst-ever
enceandtheeasilyignoredaspectsoffine-tuning.Among comprehensiveevaluationbenchmarkofmulti-modalllmsin
these discoveries, the limited instruction diversity of cur- videoanalysis. arxiv:2405.21075,2024. 2,3
rentvideodataisnotable,leadingtodowngradedlearning [10] ChaoyouFu,HaojiaLin,ZuweiLong,YunhangShen,Meng
Zhao,YifanZhang,XiongWang,DiYin,LongMa,Xiawu
efficiency.Aimedatthisspecificissue,wedevelopaneffi-
Zheng, et al. Vita: Towards open-source interactive omni
cientmethod,dubbedT2Vid,toenrichtheinstructionsin
multimodalllm. arXiv:2408.05211,2024. 3,4
thetrainingcorpus.Comprehensiveevaluationsdemonstrate
[11] GeminiTeam.Gemini1.5:Unlockingmultimodalunderstand-
theeffectivenessoftheproposedmethod.Wehopethefind-
ingacrossmillionsoftokensofcontext. arxiv:2403.05530,
ingsfromthispapercansparkdeeperthinkingaboutusing
2024. 6
MLLMsforvideounderstandingandhowtocuratevideo
[12] KaiHan,JianyuanGuo,YehuiTang,WeiHe,EnhuaWu,and
dataofhigherquality.
YunheWang. Freevideo-llm:Prompt-guidedvisualpercep-
tionforefficienttraining-freevideollms. arXiv:2410.10441,
References
2024. 1,2
[13] BinHuang,XinWang,HongChen,ZihanSong,andWenwu
[1] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,
Zhu. Vtimellm:Empowerllmtograspvideomoments. In
PengWang,JunyangLin,ChangZhou,andJingrenZhou.
CVPR,2024. 2
Qwen-vl:Aversatilevision-languagemodelforunderstand-
ing,localization,textreading,andbeyond. arxiv:2308.12966, [14] YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,and
GunheeKim. Tgif-qa:Towardspatio-temporalreasoningin
2023. 1,2
visualquestionanswering. InCVPR,2017. 2,3
[2] MaxBain,ArshaNagrani,Gu¬®lVarol,andAndrewZisserman.
Frozenintime:Ajointvideoandimageencoderforend-to- [15] PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao,
endretrieval. InICCV,2021. 6 andLiYuan. Chat-univi:Unifiedvisualrepresentationem-
powerslargelanguagemodelswithimageandvideounder-
[3] DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,
standing. arxiv:2311.08046,2023. 1,2,3,4
ChristophFeichtenhofer,andJudyHoffman. Tokenmerging:
Yourvitbutfaster. InICLR,2023. 6 [16] WonkyunKim,ChanginChoi,WonseokLee,andWonjong
Rhee. Animagegridcanbeworthavideo:Zero-shotvideo
[4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
questionansweringusingavlm. arXiv:2403.18406,2024. 1,
andJuanCarlosNiebles. Activitynet:Alarge-scalevideo
2
benchmarkforhumanactivityunderstanding.InCVPR,2015.
6 [17] Hugo Laurenc¬∏on, Andre¬¥s Marafioti, Victor Sanh, and Le¬¥o
Tronchon.Buildingandbetterunderstandingvision-language
[5] YukangChen,ShengjuQian,HaotianTang,XinLai,Zhijian
models:insightsandfuturedirections. arXiv:2408.12637,
Liu,SongHan,andJiayaJia. Longlora:Efficientfine-tuning
2024. 3,8,11
oflong-contextlargelanguagemodels. arxiv:2309.12307,
2024. 7 [18] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,
[6] ZheChen,WeiyunWang,HaoTian,ShenglongYe,Zhangwei Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.
Gao,ErfeiCui,WenwenTong,KongzhiHu,JiapengLuo, Mvbench:Acomprehensivemulti-modalvideounderstanding
ZhengMa,JiMa,JiaqiWang,XiaoyiDong,HangYan,Hewei benchmark. InCVPR,2024. 1,2,3,4
Guo,ConghuiHe,BotianShi,ZhenjiangJin,ChaoXu,Bin [19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao,
Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, andJi-RongWen. Evaluatingobjecthallucinationinlarge
PinlongCai,LichengWen,XiangchaoYan,MinDou,Lewei vision-languagemodels. InEMNLP,2023. 4
Lu,XizhouZhu,TongLu,DahuaLin,YuQiao,JifengDai, [20] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid:An
andWenhaiWang. Howfararewetogpt-4v?closingthegap imageisworth2tokensinlargelanguagemodels. InECCV,
tocommercialmultimodalmodelswithopen-sourcesuites. 2024. 7,8
arxiv:2404.16821,2024. 1,3,4 [21] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,
[7] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin JingxuYang,YaboSun,YuliangLiu,andXiangBai.Monkey:
Li,GuanzhengChen,YongxinZhu,WenqiZhang,Ziyang Imageresolutionandtextlabelareimportantthingsforlarge
Luo,DeliZhao,andLidongBing. Videollama2:Advancing multi-modalmodels. InCVPR,2024. 3
spatial-temporalmodelingandaudiounderstandinginvideo- [22] BinLin,YangYe,BinZhu,JiaxiCui,MunanNing,PengJin,
llms. arxiv:2406.07476,2024. 1,2,3,4 andLiYuan. Video-llava:Learningunitedvisualrepresenta-
[8] HaodongDuan,JunmingYang,YuxuanQiao,XinyuFang, tionbyalignmentbeforeprojection. arXiv:2311.10122,2023.
LinChen,YuanLiu,XiaoyiDong,YuhangZang,PanZhang, 1,2,3,4
Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for [23] ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,
evaluatinglargemulti-modalitymodels. InACMMM,2024. Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen,
11 JiamingHan,SiyuanHuang,YichiZhang,XumingHe,Hong-
[9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai shengLi,andYuQiao. Sphinx:Thejointmixingofweights,
Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang tasks,andvisualembeddingsformulti-modallargelanguage
Shen,MengdanZhang,PeixianChen,YanweiLi,Shaohui models. arxiv:2311.07575,2023. 3
9[24] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. [39] ShukangYin,ChaoyouFu,SiruiZhao,TongXu,HaoWang,
Visualinstructiontuning. InNeurIPS,2024. 1,2 DianboSui,YunhangShen,KeLi,XingSun,andEnhong
[25] JiajunLiu,YibingWang,HanghangMa,XiaopingWu,Xi- Chen. Woodpecker:Hallucinationcorrectionformultimodal
aoqiMa,andJieHu. Kangaroo:Apowerfulvideo-language largelanguagemodels. arxiv:2310.16045,2023. 4
model supporting long-context video input. https:// [40] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,
kangaroogroup.github.io/Kangaroo.github. TongXu,andEnhongChen. Asurveyonmultimodallarge
io/,2024. Accessed:2024-07-30. 3,4 languagemodels. arxiv:2306.13549,2024. 1,2
[26] YuanxinLiu,ShichengLi,YiLiu,YuxiangWang,Shuhuai [41] ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,Yueting
Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
Tempcompass: Do video llms really understand videos? understandingcomplexwebvideosviaquestionanswering.
arXiv:2403.00476,2024. 2,3 InAAAI,2019. 2,3
[27] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and [42] YanZeng,HanboZhang,JianiZheng,JiangnanXia,Guo-
Fahad Shahbaz Khan. Video-chatgpt: Towards detailed qiangWei,YangWei,YuchenZhang,TaoKong,andRuihua
videounderstandingvialargevisionandlanguagemodels. Song. Whatmattersintrainingagpt4-stylelanguagemodel
arxiv:2306.05424,2023. 2,5 withmultimodalinputs? InNAACL,2024. 6
[28] Meta. Introducingllama3.1:Ourmostcapablemodelsto [43] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng,
date. https://ai.meta.com/blog/meta-llama- JingkangYang,YuanhanZhang,ZiyueWang,HaoranTan,
3-1/,2024. Accessed:2024-10-28. 11 Chunyuan Li, and Ziwei Liu. Long context transfer from
[29] OpenAI. Gpt-4technicalreport. arxiv:2303.08774,2023. 2 languagetovision. arXiv:2406.16852,2024. 7,8
[30] ChristophSchuhmann,RomainBeaumont,RichardVencu, [44] YuanhanZhang,BoLi,HaotianLiu,YongJaeLee,Liangke
CadeGordon,RossWightman,MehdiCherti,TheoCoombes, Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li.
Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Llava-next:Astrongzero-shotvideounderstandingmodel.
Laion-5b:Anopenlarge-scaledatasetfortrainingnextgener- https://llava-vl.github.io/blog/2024-04-
ationimage-textmodels. InNeurIPS,2022. 1 30-llava-next-video/,2024. Accessed:2024-07-09.
[31] Share14. Sharegemini: Scaling up video caption data for 2
multimodal large language models. https://github. [45] WentingZhao,XiangRen,JackHessel,ClaireCardie,Yejin
com/Share14/ShareGemini,2024. Accessed:2024- Choi,andYuntianDeng. Wildchat:1mchatgptinteraction
07-09. 5 logsinthewild. InICLR,2024. 6
[32] PiyushSharma,NanDing,SebastianGoodman,andRadu [46] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,Jiao
Soricut. Conceptualcaptions:Acleaned,hypernymed,image Sun,YuningMao,XuezheMa,AviaEfrat,PingYu,LiliYu,
alt-textdatasetforautomaticimagecaptioning.InACL,2018. etal. Lima:Lessismoreforalignment. InNeurIPS,2024. 6
1
[33] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,
XiangnanHe,andYuetingZhuang.Videoquestionanswering
viagraduallyrefinedattentionoverappearanceandmotion.
InACMMM,2017. 2,3
[34] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong
Ng, and Jiashi Feng. Pllava: Parameter-free llava exten-
sion from images to videos for video dense captioning.
arXiv:2404.16994,2024. 2
[35] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen,
ZhengfengLai,HaimingGang,KaiKang,andAfshinDe-
hghan. Slowfast-llava: A strong training-free baseline for
videolargelanguagemodels. arXiv:2407.15841,2024. 1,2
[36] ZhangchenXu,FengqingJiang,LuyaoNiu,YuntianDeng,
RadhaPoovendran,YejinChoi,andBillYuchenLin.Magpie:
Alignmentdatasynthesisfromscratchbypromptingaligned
llmswithnothing. arxiv:2406.08464,2024. 6
[37] YuanYao,TianyuYu,AoZhang,ChongyiWang,JunboCui,
Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui
He,etal. Minicpm-v:Agpt-4vlevelmllmonyourphone.
arXiv:2408.01800,2024. 3,8,11
[38] JiaboYe,AnwenHu,HaiyangXu,QinghaoYe,MingYan,
GuohaiXu,ChenliangLi,JunfengTian,QiQian,JiZhang,
et al. Ureader: Universal ocr-free visually-situated lan-
guageunderstandingwithmultimodallargelanguagemodel.
arXiv:2310.05126,2023. 3
10A.ImplementationDetails
  D   5 H V X O W V  R Q  0 L Q L & 3 0   %
A.1.AnswerJudging   
We notice that MiniCPM-8B [37] often fails to follow   
instructions properly when we explicitly ask the model
to ‚ÄúAnswer with the option‚Äôs letter from   
the given choices directly‚Äù,makingsimpleex-
act matching inaccurate. Specifically, the model often   
prepends or appends additional text other than the option
  
letters, e.g. ‚ÄúAnswer: B. Pink.‚Äù, or gives additional
                                   
explanationsapartfromtheanswer.    I U D P H V
To cope with these issues, we adopt a combination of  6 K R U W  / R Q J  P D [  W U D L Q L Q Q J  I U D P H V  P D [  W U D L Q L Q J  F R Q W H [ W
 0 L G  2 Y H U D O O  P D [  / / 0  F R Q W H [ W
exactmatchingandLLMmatchingforassessment.Specif-
ically, we strip the prefixes such as ‚ÄúAnswer:‚Äù from the   E   5 H V X O W V  R Q  , G H I L F V    %
predictionandtrytouseregularexpressionmatchingtofind   
theoptionletter.Whentheexactmatchingschemefails,we
useanLLM(Llama-3.1-8B-Instruct[28])tofindanoption   
closesttothemodelprediction.WhentheLLMmatching
fails, a placeholder outside of the available options (such   
as‚ÄúZ‚Äù)isreturnedtodenoteawronganswer.Ourjudging
  
promptfortheLLMismodifiedfromVLMEvalKit[8],as
showninTab.5.
  
                                 
B.AdditionalResultsinExperiments    I U D P H V
B.1.ResultsonDifferentFrameCounts Figure7.AccuracyofMiniCPM-8BandIdefics3-8BontheVideo-
MMEbenchmarkwheninferencewithdifferentnumbersofvideo
Wepresenttheperformanceofdifferentinputframenum- frames.Asinputframesincrease,theperformanceofbothmod-
bers of MiniCPM-8B [37] and Idefics3-8B [17] in Fig. 7. els stays relatively stable and does not improve much. ‚Äúmax
Asinputframesincrease,theperformancewithinthecon- training frames‚Äùdenotesthemaxnumberofpatchesused
textwindowoftheLLMbackboneismorestablethanthe forasingleimagewhentraining;‚Äúmax training context‚Äù
InterVL-4B, which is of a smaller parameter size. Never- denotesthemaximumcontextwindowsetwhentraining;‚Äúmax
LLM context‚ÄùdenotesthecontextwindowoftheoriginalLLM
theless, the performance still does not improve much as
backbonebeforefurthermultimodaltraining.
moreframesareavailable,againindicatingthattheeffective
context is limited. Thus, simply expanding the LLM con- coherentway.
textwindowforimage-LLMstofacilitatelongvideounder- In case 2 (Fig. 9), we ask the two models to identify
standingwouldnotbeverypractical.Asinthemainpaper, thetemporalchangesinobjectcounts.Thefine-tunedmodel
weattributethisphenomenontothelackofcapabilitiesof correctlyrecognizesthechangesinthenumberofpolarbears
image-LLMstotrulyunderstandthetemporalrelationships as scenes change in the video clip, while the image-LLM
amongframes.Additionalcasestudiesrelatedtothistopic fails to do so and seems to give the answer according to
areavailableinthefollowingsection. thelastfewframes.Thisisexpectedsinceimage-LLMsare
trainedwithstaticimagesratherthandynamicvideos,where
B.2.CaseStudy
thelatterinjecttemporalconceptsintomodels.
In the main paper, we identify the drawback of image-
LLMsgenerallyfailingtounderstandthetemporalrelation-
shipsamongvideoframes.Inthissection,wesupplement
morecasesandanalysesonthistopic.
Incase1(Fig.8),welistacasewhereweinstructmodels
todescribethevideo,whiletheimage-LLMfailstounder-
standtheseparatescenesandtemporalrelationshipsbutin-
steadfocusesonseveralspecificframesanddescribesrather
independently.Incontrast,themodelfine-tunedwithvideo
datacanmoreaccuratelydistinguishtheactionshappening
inthevideoandunderstandtheconnectionsinatemporally
11
  F F $
  F F $1 2 3 4
5 6 7 8
9 10 11 12
Prompt: Describe this video.
Zero-shot:
The image presented depicts a penguin colony in a snowy landscape. Six penguins are seated closely together, each exhibiting unique postures. Their bodies
vary in size, suggesting different stages of development, with the larger ones likely being older or mature. (No.1-3) Each penguin's head is inclined slightly to one
side and exhibits a distinctive, streamlined shape, characteristic of penguins. The penguins' feathered coats are mostly white, possibly to provide camouflage in
their natural habitat. A notable feature in the image is that six adult penguins are gathered around a single small, new hatchling. (No.5) The adult penguins seem
to be taking turns incubating the hatchling, a behavior common in penguins, where a group of adults often take turns keeping the egg warm until it hatches. Only
one adult penguin is clearly visible, with the rest obscured by the others and the surrounding snow.
Fine-tuned:
The video begins with a wide shot of a snowy landscape. A large bird with black and white feathers is standing on the snow, flapping its wings. (No.1)
Several small, gray birds with black heads are standing in the snow nearby. (No.2) The large bird approaches the small birds and begins to interact with them.
(No.3,4) The small birds move around the large bird, sometimes touching it with their beaks. The large bird continues to flap its wings and move around the small
birds. The video ends with a shot of the large bird and the small birds standing in the snow. (No.11,12)
Figure8.Acasestudyusingtheprompt‚ÄúDescribethisvideo.‚Äù,wherethevideoisadocumentaryoftheArcticenvironment.Compared
withthefine-tunedmodel,thezero-shotcasereasonswithanddescribesvideoframeswithoutunderstandingthetemporalcorrelations.
Thispatternfollowsthecommonbehavioroftheimagemodel.Thekeyframesarehighlightedwithcolors(exceptforred,whichisusedto
indicatehallucinations).Thenumbersontheupper-leftcornerofeachframeindicateframeorder.Zoominforbettervisibility.
1 2 3 4
5 6 7 8
9 10 11 12
Prompt: How does the number of polar bears change in the video?
Zero-shot:
The number of polar bears does not change in the video; there are always two polar bears in each frame.
Fine-tuned:
Initially, there are three polar bears in the video. One polar bear is sniffing around in the snow while the other two are crouching in the distance. (No.1-4)
The scene then changes, and only two polar bears are visible. One of the bears is lying down in the snow while the other has its head resting on top. (No.9-12)
Figure9.Acasestudydesignedtotestthetemporalreasoningcapabilitiesofmodels.Inthevideo,threepolarbearsshowupatthebeginning,
andintheend,thescenechangesintoavideoshotoftwopolarbears.Theimage-LLMfailstorecognizethechangethroughzero-shot
prompting.Notethatwehaveverifiedthattheimage-LLMcancorrectlyrecognizethenumberbyinputtingasingleframeseparately.
12Systemmessage
YouareanAIassistantwhowillhelpmetomatchananswerwithseveraloptionsofasingle-choicequestion.
Prompt
Youareprovidedwithaquestion,severaloptions,andananswer,andyouneedtofindwhichoptionismostsimilarto
theanswer.
Ifthemeaningofalloptionsaresignificantlydifferentfromtheanswer,outputZ.Yourshoulddirectlyoutputasingle
uppercasecharacter,suchasA,B,C,D(iftheyarevalidoptions)andZ,andnothingelse.Herearetwoexamples.
Example1:
Question:Whatisthemainobjectinimage?
Options:A.teddybear.
B.rabbit.
C.cat.
D.dog.
Answer:acuteteddybear
Output:A
Example2:
Question:Whatisthemainobjectinimage?
Options:A.teddybear.
B.rabbit.
C.cat.
D.dog.
Answer:Spider
Output:Z
Nowhereisthequestion,options,andtheanswer,youshouldmatchandgivemetheoptionletter:
Question:{Question}
Options:{Options}
Answer:{ModelAnswer}
Output:
Table5.TemplateforpromptingLLMtoperformoptionmatching.{Question} isthespecificquestionofabenchmarksample,and
{Options}arecorrespondingchoicesofthequestion.{ModelAnswer}istherawpredictionofMLLMs.
13