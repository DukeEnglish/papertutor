Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos
QiruiChen,ShangzheDi,WeidiXie
ShanghaiJiaoTongUniversity
{chen qirui,dishangzhe,weidi}@sjtu.edu.cn
https://qirui-chen.github.io/MultiHop-EgoQA
Abstract
ThispaperconsiderstheproblemofMulti-HopVideoQues-
tionAnswering(MH-VidQA)inlong-formegocentricvideos.
This task not only requires to answer visual questions, but
also to localize multiple relevant time intervals within the
0s 45s 90s 135s 180s
videoasvisualevidences.Wedevelopanautomatedpipeline
Question: What ingredients did I cut with the knife?
to create multi-hop question-answering pairs with associ-
ated temporal evidence, enabling to construct a large-scale Answer: You cut onion and potato on the cutting mat with the knife.
dataset for instruction-tuning. To monitor the progress of
Evidence: 0-13s, 26-32s, 44-55s, 145-168s
this new task, we further curate a high-quality benchmark,
MULTIHOP-EGOQA, with careful manual verification and
Figure 1: We introduce the problem of Multi-Hop Video
refinement. Experimental results reveal that existing multi-
modal systems exhibit inadequate multi-hop grounding and Question Answering for long-form egocentric video under-
reasoning abilities, resulting in unsatisfactory performance. standing. This task requires the model to answer questions
Wethenproposeanovelarchitecture,termedasGrounding bygatheringandreasoningacrossscatteredvisualclues,ne-
Scattered Evidence with Large Language Model (GeLM), cessitatingthegroundingofmultiplerelevanttimespansas
thatenhancesmulti-modallargelanguagemodels(MLLMs) supportingevidence.
byincorporatingagroundingmoduletoretrievetemporalev-
idencefromvideosusingflexiblegroundingtokens.Trained Recently, the introduction of Ego4D dataset (Grauman
on our visual instruction-tuning data, GeLM demonstrates
et al. 2022) has enabled a series of research in visual-
improved multi-hop grounding and reasoning capabilities,
language understanding in egocentric videos, e.g., question
settinganewbaselineforthischallengingtask.Furthermore,
answering that focuses on summarizing entire video con-
whentrainedonthird-personviewvideos,thesamearchitec-
tent(Mangalam,Akshulakov,andMalik2023);naturallan-
turealsoachievesstate-of-the-artperformanceonthesingle-
hopVidQAbenchmark,ActivityNet-RTL,demonstratingits guagequery(NLQ)thatrequirestemporallocalizationbased
effectiveness. on a given query (Ramakrishnan, Al-Halah, and Grauman
2023);groundedquestionansweringthatconsidersanswer-
ing the query, while localizing the query-related time span
1 Introduction
simultaneously (Ba¬®rmann and Waibel 2022). However, the
Withtherapiddevelopmentofcomputervision,thecommu- above-mentionedsettingstendtofallintoanover-simplistic
nityhaswitnessedasignificantinterestindeployingvision scenario,wherequestionsaretypicallyanswerablebasedon
systemswithinembodiedagents,suchasautonomousvehi- visualcuesfromasingletimepoint,oronlyonetimespanis
clesandhumanoidrobots.Insuchscenarios,theinputsare annotated among multiple relevant spans. For instance, the
typicallylong,continuousvideostreamsfromafirst-person question ‚ÄúHow many shirts did I pack in my suitcase?‚Äù is
perspective,capturingtheworldthroughtheeyesofanagent deliberately excluded if the packing process occurs across
activelyinteractingwithitsenvironment.Forthevirtualas- multiple,non-contiguoustimespans,asdescribedinthean-
sistants or physical robots to be useful, the ability to per- notationprocessoftheNLQtaskinEgo4D.
form egocentric video question answering (VidQA) is cru- Asaconsequence,VidQAsystemsbuiltupontheabove-
cial,stemmingfromtwoaspects:first,VidQAleverageslan- mentionedtaskscanhardlybeappliedinmulti-hopscenar-
guageasanaturalinterfaceforhuman-machineinteraction, ios,duetotwoprimarylimitations:thescarcityofdatasup-
therebyenhancingtheusabilityandaccessibilityforthegen- porting multi-hop reasoning, the deficiency in architecture
eral public; second, it can encompass various vision tasks designtosupportmulti-hoptemporalperception.First,there
aboutthe‚Äòwho‚Äô,‚Äòwhen‚Äô,‚Äòwhere‚Äô,and‚Äòwhat‚Äôofanindivid- is a notable insufficiency of training data on questions that
ual‚Äôsdailylife,e.g.,actionrecognition,objectdetection,and requirereasoningacrossmultipletemporalspans,especially
sceneunderstanding,thusactingasarobustandcomprehen- in long-form egocentric videos; Second, existing architec-
sivebenchmarkforvideounderstanding. turesthattreattemporalgroundingasalanguagemodeling
4202
guA
62
]VC.sc[
1v96441.8042:viXratask (Ren et al. 2024; Huang et al. 2024a,b), e.g., directly Avg. Time Multi-
Dataset Annotation Ego?
Duration(s) Labels Spans
incorporatingthetimestampasthetargetofauto-regressive
Conventional VidQA Benchmarks
prediction, which is less effective than task-specific mod-
MovieQA Manual 211.4 ‚úó ‚úì ‚úó
els(Linetal.2023;Mu,Mo,andLi2024). MSRVTT-QA Auto 15 ‚úó ‚úó ‚úó
To bridge the gap, this paper introduces the problem of MSVD-QA Auto 10 ‚úó ‚úó ‚úó
TVQA Manual 76.2 ‚úó ‚úì ‚úó
Multi-HopVideoQuestion-Answering(MH-VidQA).Asil- How2QA Manual 60 ‚úó ‚úì ‚úó
lustratedinFig.1,thistaskrequiresthemodeltosimultane- NeXT-QA Manual 44 ‚úó ‚úó ‚úó
iVQA Manual 18.6 ‚úó ‚úó ‚úó
ouslyanswerquestionsthatinvolvevisualinformationfrom
EgoSchema Auto+Manual 180 ‚úì ‚úó ‚úó
multipletimeintervalsandlocalizethesetimespansasevi-
Grounded VidQA Benchmarks
dencewithinlong,egocentricvideos. QAEgo4D Manual 498 ‚úì ‚úì ‚úó
Toacquirethedatanecessaryforvisualinstructiontuning, NExT-GQA Manual 39.5 ‚úó ‚úì ‚úó
ActivityNet-RTL Auto+Manual 180 ‚úó ‚úì ‚úó
wehavedevelopedanautomatedpipelinetoconstructlarge-
MULTIHOP-EGOQA Auto+Manual 180 ‚úì ‚úì ‚úì
scale question-answer-evidence triplets from the narrations
ofEgo4D(Graumanetal.2022).Specifically,webuildac- Table 1: Comparison of VidQA benchmarks. Our pro-
tionscenegraphs(Jietal.2020)byextractingsyntaxtrees posedbenchmarkfocusesonassessingmulti-hopreasoning
from narrations, allowing us to analyze the temporal pro- andgroundingabilitieswithinlong-formegocentricvideos.
gressionofactions,objects,andtheirrelationships,thereby
identifyingpotentialquestionsthatrequireinformationfrom
area.However,mostofitsquestionscanbeansweredbased
multiple time points to answer. We then utilize large lan-
on subtitles alone, with few relying on visual cues (Jasani,
guagemodels(LLMs)togenerateeligibletripletsacrosssix
Girdhar, and Ramanan 2019). ActivityNet-QA (Yu et al.
different question types, that encompass real-world scenar-
2019) and How2QA (Sanabria et al. 2018) have focused
ios with an emphasis on the interactions between the indi-
on visual understanding in daily life and instructional
vidualandtheexternalenvironment,aswellasthelong-term
videos. More recent datasets like NeXT-QA (Xiao et al.
temporalrelationofevents.Thecategoriesincluderepeated
2021),PerceptionTest(Patrauceanetal.2024),STAR(Wu
activities, multiple actions, multiple objects, multiple loca-
et al. 2021), and AGQA (Grunde-McLaughlin, Krishna,
tions/people,eventcomposition,andeventcomparison.
andAgrawala2021)focusondesigningquestionsrequiring
Leveraging our automatically constructed data for vi-
spatio-temporal reasoning and causal relations. Addition-
sualinstructiontuning,despitetheexistingVideoLLM(Ren
ally,EgoSchema(Mangalam,Akshulakov,andMalik2023)
etal.2024)hasdemonstratedimprovedmulti-hopreasoning
proposes to generate questions through LLMs and manual
abilities,itstillstrugglestolocalizetherelevanttimespans,
effortsforlong-formegocentricvideos.
primarilyduetothelimitationsinpredictingtimestampsac-
curately. We further propose a novel architecture, termed Multi-HopQAwithGrounding.InNaturalLanguagePro-
as Grounding Scattered Evidence with Large Language cessing, multi-hop question-answering involves reasoning
Model (GeLM). This architecture incorporates grounding acrossmultiplepiecesofinformation,oftenrequiringthere-
tokensintothevocabularyofamulti-modallargelanguage trieval of evidence from various sources (Yang et al. 2018;
model, that are generated within the responses and then Ho et al. 2020; Xiong et al. 2021; Trivedi et al. 2022;
fused with visual features in a temporal grounding module Zhang et al. 2024a). In video understanding, conventional
to provide corresponding evidences, thereby enhancing the VidQA benchmarks do not necessitate models to explicitly
interpretabilityoftheanswers. localizeorreasonovertemporallyscatteredevidence.How-
TotrackthedevelopmentprogressonMH-VidQAtask,we ever, recent works like EGOTIMEQA (Di and Xie 2024),
have established a new benchmark, termed as MULTIHOP- NExT-GQA(Xiaoetal.2024),andREXTIME(Chenetal.
EGOQA, that involves participants for validating and refin- 2024a)emphasizetheimportanceofthegroundingevidence
ingthegeneratedtriplets.Comprehensiveevaluationsshow inVidQA.Thesebenchmarks,though,assumethatevidence
that both proprietary and open-source large multi-modal is confined to a single time span, overlooking the need
modelslargelyfallbehindhumanperformance,highlighting forlong-termtemporalmodellingandmulti-stepreasoning,
thesubstantialchallengepresentedby MULTIHOP-EGOQA. whichcanbeanoversimplificationinvideounderstanding.
Our architecture, trained on the automatically constructed Multi-modalLargeLanguageModels.Withtherecentad-
instruction-tuningdata,hasshownsignificantimprovement vancements in Large Language Models (LLMs) (Achiam
in multi-hop reasoning and grounding. We also evaluate etal.2023;Chiangetal.2023;AI@Meta2024;Jiangetal.
ourarchitectureonanotherpublicsingle-hopVidQAbench- 2024),researchersareendeavouringtodevelopMulti-modal
mark,ActivityNet-RTL(Huangetal.2024b),outperforming Large Language Models (MLLMs) by aligning visual and
existingapproachesbyalargemargin. linguistic modalities through visual instruction tuning. For
image understanding, several studies (Alayrac et al. 2022;
2 RelatedWork
Li et al. 2023a; Zhu et al. 2023; Liu et al. 2023a, 2024)
VideoQuestionAnsweringDatasets.VideoQuestionAn- haveshownstrongperformanceacrossvariousVQAbench-
swering(VidQA)isavideounderstandingtaskthatinvolves marks (Lu et al. 2023; Liu et al. 2023b; Yue et al. 2024).
answering natural language queries using visual-only or Invideounderstanding,whilesomeworks(Lietal.2023b;
multi-modal information from videos. MovieQA (Tapaswi Ataallahetal.2024;Zhangetal.2024b)havemadeprogress
et al. 2016) proposes one of the earliest datasets in this on traditional VidQA benchmarks, they are generally de-Stage I: Stage II:
100% Segments 50%
Raw Video Cropping Scattered Recurrence Mining
Untrimmed Video with Complete Narrations Syntax Trees over Time
cut peel off
0:05 C puts cellphone on the counter dobj dobj
on with with in
0:17 C peels off carrot with peeler knife
carrots Scattered pumpkin
0:32 C removes carrot peel from peeler
recurrence
‚Ä¶‚Ä¶ Rule-based board knife knife board
33:22 C puts tablet down on the sofa Filtering
t t ‚â´t
1 2 1
Stage IV: Stage III:
16% 20%
Manual Check & Refinement QA Generation & Filtering
Grounded Multi-Hop QA Categorized Narrations about knife (pobj)
In-Context Examples
Question: What tasks did I use the knife for? 0:17 - 0:47 C cuts carrot with knife on
chopping board
Answer: You <T1> cut carrots </T1> and <T2>
peeled off pumpkin skin </T2> with the knife. LLM 0:47 - 0:50 C puts knife on the chopping board
Filtering 1:28 - 2:20 C peels off pumpkin skin with knife
Evidence: 10s - 47s, 88s - 180s
in kitchen
Category: Composition Grounded
2:23 - 2:30 C moves knife
QA Triplets
Figure 2: Illustration of our data curation pipeline. To collect large-scale multi-hop VidQA data, we have developed an
automated pipeline. We begin by using action scene graphs to identify potential multi-hop reasoning questions based on the
syntaxtreesofannotatednarrations.Next,weuseGPT-4otogeneratedatasamplesthatincludequestions,answers,andrelevant
timespans.Finally,weperformmanualvalidationandrefinementtocreatethenewbenchmark.
signedforshortvideos.Recenteffortstoimprovetemporal acrossvariouscategoriesusingLLMs;(iv)filteringthegen-
awareness(Renetal.2024;Huangetal.2024a;Qianetal. eratedsampleswithLLMs,thenfollowedbymanualreview
2024)stilllagbehindtask-specificmodels(Linetal.2023; andrefinement.
Mu,Mo,andLi2024)inthetemporalgroundingability.To
addressthisgap,ourproposeddatasetsupportsbothinstruc- 4.1 RawVideoCropping&Selection
tion tuning and the evaluation of multi-hop reasoning and
We start with the 9,611 untrimmed egocentric videos (24-
grounding in long-form egocentric videos, thereby advanc-
minute duration on average), accompanied with a total of
ingthedevelopmentofvideo-languagemodels.
3.85MtimestampednarrationsfromEgo4D(Graumanetal.
2022). Since these timestamps indicate the occurrence of a
3 ProblemFormulation
new action (i.e., the start time), to estimate the duration of
Given a video stream and a question in the format of free-
eachaction,wetakethetimestampofthesubsequentnarra-
formtext,e.g.,V andQrespectively,theobjectiveistogen-
tionastheendtime.Specifically,wesegmenttherawvideos
eratetheanswerandlocalisethetemporalevidence:
intonon-overlapping3-minuteclips.Eachclipandthecor-
[AÀÜ,TÀÜ]=Œ¶(V,Q), (1) respondingnarrationsaredenotedasV andN ={N }|N|.
i i=1
whereTÀÜ ={[s ,e ],[s ,e ],...,[s ,e ]}referstoasetof
1 1 2 2 n n 4.2 MiningMulti-HopQAfromNarrations
non-overlappingstart-endtimeintervals,inwhichthevideo
contentisnecessaryforderivingtheanswerAÀÜ. Weproposetominethemulti-hopVidQAtripletsfromac-
To develop the vision systems that address our consid- tion scene graphs for each long-form video, which provide
eredMH-VidQAtask,itisessentialtocollectdataintriplet temporallyevolvingobjectdescriptions,human-objectrela-
form,i.e.,(Q,A,T),totrainthearchitecturethatcansimul- tionships,andtheprogressionofactionsovertime.(Jietal.
taneouslyanswerquestions,andgroundthemacrossmulti- 2020;Yangetal.2023;Rodinetal.2024).
ple time spans. In the following sections, we will detail an To build action scene graphs, we use the syntax tree of
automated pipeline for constructing visual instructions and each narration to identify the specific nodes, involving ac-
trainingourproposedarchitecture. tions, objects, locations and people. We then search for
structureswhereasinglenoderecursovertime,butconnects
4 MULTIHOP-EGOQA:CurationPipeline with different neighbouring nodes across various scenes,
sincethesestructuresarelikelytocontainthemulti-hoprea-
Thecurationpipelineinvolvesfourstages:(i)croppingand
soningqueries.Thedetailedprocedureisoutlinedbelow.
selecting video clips from untrimmed Ego4D dataset; (ii)
mining potential questions that demand multi-hop reason- Narration Syntax Tree ‚Üí Action Scene Graph. We use
ing based on narrations; (iii) producing (Q,A,T) triplets spaCy (Honnibal et al. 2020) to parse each narration andextract the basic nodes, including actions (verb), direct ob- visual features. The given question is tokenized and trans-
jects(dobj),andprepositionalobjects(pobj)alongwiththeir formed into textual embeddings, while the visual features
modifiers. For instance, ‚ÄúC puts the cooking pot on the areprojectedintovisualembeddingswiththesamedimen-
counter top‚Äù can be parsed into {put, cooking pot, counter sionthroughalinearprojector:
top}asthreenodeswithdistinctsyntacticattributes.
x =œï (Œ¶ (V)), x =œï (Q) (3)
Searching Scattered Recurrence in Graph. We then fo- v proj v-enc q emb
cusonanodeuthatrecurssporadicallythroughouttheen- wherex ‚àà RL√óD,x ‚àà RQ√óD denotethecomputedem-
v q
tiresetofnarrationsN.Theminimumandmaximumrecur- beddingsforvisualandquestionrespectively.
rencetimesfortheselectednodeuaredenotedast and
min
t max,respectively,whichdeterminesthenumberoftimein- 5.2 Multi-modalLargeLanguageModel
tervals involved in the question. We extract the narrations
Thevisualandtextualembeddingsarethenfedintoamulti-
relatedtothespecificnodeufromN,denotedasN .These
u modallargelanguagemodel:
narrations,thoughfocusedonnodeu,describedifferentac-
tionscenes,makingthempotentialcandidatesformulti-hop {h ,h ,h }=MLLM([x :x ]) (4)
v q a v q
tripletgenerationinStageIII.
where h ,h ,h represent the hidden states of the input
v q a
4.3 QAGeneration&FilteringwithLLM frames,question,andtheoutputresponserespectively.The
Based on the nodes‚Äô syntactic attributes, we use different answertextsAÀÜarethendecodedusingalinearheadonh a.
in-context learning examples to guide the LLM-based QA Grounding Tokens. Inspired by approaches that enable
generationprocesses.Theresultingmulti-hopquestionsare MLLMs to segment visual entities (Lai et al. 2024; Zhang
divided into six categories, involving repeated activities, etal.2024c;Yanetal.2024),weexpandthevocabularyby
multiple actions, multiple objects, multiple locations/peo- adding grounding token pairs, i.e., <T> </T>, which indi-
ple,eventcomposition,andeventcomparison.Thedetailed cate the start-end time span. As illustrated in Fig. 3, when
promptsandquestionexamplesarepresentedintheSupple- the MLLM needs to ground the temporal evidence for its
mentaryMaterial.Formally,fortheselectednarrationsN u, response, the relevant part of the response is enclosed by
thetripletisgeneratedwithprompt(P),denotedas: <T>and</T>.Weconcatenatethelast-layerhiddenstatesof
each pair, i.e., <T1> and </T1>, ..., <TK> and </TK> along
{(Q,A,T)}=LLM(P;N ) (2)
u thechanneldimensiontoformasinglegroundingqueryvec-
Aftertheautomatedgeneration,weuseanLLMtofilterout tor, resulting in K grounding queries H g ‚àà RK√ó2D. Note
unreasonableQApairs,resultingin4,412clipswith14,397 that,thevalueofK canvaryfordifferentresponses.These
triplets.WeutilizeGPT-4oforbothgenerationandfiltration queries are then processed through the grounding module,
processesduetoitssuperiorcapabilities. whichinteractswiththevisualhiddenstatesh v ‚ààRL√óD.
4.4 ManualCheck&Refinement 5.3 EvidenceGroundingModule
To construct a benchmark, we select 380 clips with 1,208 To ground the time spans that support the answer, we de-
tripletsandhire12graduatestudentsmajoringincomputer signanevidencegroundingmodulethatprocessesavariable
vision,tovalidatetheclarityofthedataandfurtherrefinethe numberofgroundingqueriesandpredictsthecorresponding
temporalannotations.Asaresult,weobtain360clipswith temporalproposalsinthevideo:TÀÜ ={[s ,e ]}|TÀÜ|.
i i i=1
1,080 triplets, which form the final benchmark, termed as Webeginbyprojectingthehiddenstatesofframesh and
v
MULTIHOP-EGOQA.Theannotationdetailsandbenchmark groundingqueriesH intothesamedimensionC:
g
statisticsareprovidedintheSupplementaryMaterial.
w =œï (h )‚ààRL√óC, w =œï (H )‚ààRK√óC (5)
v v v g g g
5 GeLM:ABaselineMethodforMH-VidQA
Following this, two separate branches are used to predict
Existingmodelsforvideoquestionansweringtypicallypro- the temporal evidence for the answer: the saliency branch
vide answers without supporting temporal evidence, or are andthesimilaritybranch,asdepictedinFig.3.Thesaliency
restricted to identifying a single time interval. Here, we branch utilizes a self-attention mechanism across all visual
proposeanovelarchitecture,termedasGeLM:Grounding features and grounding tokens to identify all temporal evi-
ScatteredEvidencewithLargeLanguageModelforMulti- denceforthequestionholistically.Thesimilaritybranchcal-
HopVideoQuestion-Answering.AsdepictedinFig.3,our culatesthevisual-textualsimilaritybetweeneachgrounding
model primarily comprises a multi-modal large language queryandallvisualfeatures,todeterminethetimespansfor
model anda grounding module,with special grounding to- eachpartoftheresponseinafragmentedmanner.
kens (<T></T>) indicating the time span of the enclosed
Saliency branch. As illustrated in Fig. 3, the saliency
keyinformationintheresponse.
branchutilizesthreeTransformerEncoderlayersasthetem-
5.1 Visual-languageEncodingModule poral aggregator, to fuse information between grounding
queriesandvisualhiddenstates:
GiventhevideoclipwithLframesandtheassociatedques-
tion, we first adopt a frozen visual encoder to extract the {o ,o }=Œ¶ ([w :w ]) (6)
v g temp-agg v gGeLM Grounding Grounding Module
üî•
Module LM Head
t t t t
Hg 1 2 3 4 packet
hv hq * ha two eggshells
yÃÇ SÃÇ
Multi-modal A: You <T1> moved a packet into the
üî• bin </T1>, and later <T2> dropped the Saliency Similarity
LLM Head Head
eggshell into the bin twice </T2>.
Transformer Projection
Q: What items did I dispose of into Encoder Head
the bin, and in what order?
Visual Encoder wv wg
Visual Tokens
Video Textual Tokens MLP MLP
Frames
Grounding Tokens
hv Hg
Figure3:Overviewoftheproposedarchitecture.GeLMcangenerategroundingtokenpairs,i.e., <T></T>,intheresponse
ofamulti-modallargelanguagemodel,whichdenotethestartandendtimesoftheenclosedstatement.Thesegroundingtokens
arethenprocessedwithvisualhiddenstatestothegroundmultipletimespansthatprovideevidencesupportingtheanswer.
ThepredictedsaliencyscoreyÀÜ ‚àà RL isderivedthroughthe thresholding method to each row and take the union of the
saliencyheadœï (¬∑)andthesigmoidfunctionœÉ(¬∑): resultstoobtainasetofproposals.
saliency
yÀÜ =œÉ(œï (o )) (7) Training objective. For question answering, the cross en-
saliency v
tropy loss L (AÀÜ,A) is utilized for next token predic-
whereahigherscoreindicatesahigherprobabilitythateach CE
tion.Forevidencegrounding,giventhegroundtruthbinary
frameservesasvisualevidenceforthequestion-answering.
saliencylabelsy ‚àà {0,1}L,weusebinarycrossentropyas
Thesaliencyheadœï (¬∑)consistsoftwoConv1Dlayers
saliency lossfunction:L = 1 (cid:80)L ‚àíy logyÀÜ .Withtheground
withReLUactivation. BCE L i=1 i i
truthbinarysimilaritymatrixS ‚àà {0,1}K√óL,weadoptthe
Similarity branch. Apart from predicting the saliency, we
Multiple Instance Learning NCE (MIL-NCE) loss (Miech
alsoaimtodeterminethetimespansofkeyinformationen-
etal.2019)forcontrastivelearning:
closedbyeachgroundingtokenpairseparately,represented
qas uea riesi sm ai nla dri Lty fm raa mtr eix s.SÀÜ Sp‚àà eciR ficK a√ó llL
y,
b we etw pe re on jecK
t
tg hr eou hn idd din eng
L =‚àí
1 (cid:88)K log(cid:80)L j=1S ijexp(SÀÜ ij/œÑ)
(10)
NCE K (cid:80)L exp(SÀÜ /œÑ)
stateswithalinearlayer, i=1 j=1 ij
zv =œà (w ), zg =œà (w ) (8) whereœÑ denotestemperature.i,jcorrespondtoi-thground-
v-proj v g-proj g
ing query and j-th frame, respectively. The final loss is a
andcomputethecosinesimilaritymatrix:
weightedsumoftheabovelosses:L=L +Œª L +
CE BCE BCE
SÀÜ =
zg
i
¬∑zv
j ‚àà[‚àí1,1] (9)
Œª NCEL NCE.
ij ‚à•zg‚à•¬∑‚à•zv‚à•
6 Experiments
where a higher value of value of SÀÜ ij indicates that the j-th In this section, we first describe the metrics for our bench-
frameismorerelevanttothei-thgroundingquery. mark, MULTIHOP-EGOQA, and then evaluate the perfor-
Proposal generation strategy. During inference, to gener- manceofexistingapproaches.Next,weemployinstruction
ate temporal proposals (TÀÜ), we apply the following post- tuning with the automatically constructed dataset, to estab-
processing. Utilizing the saliency score vector yÀÜ ‚àà RL, lishastrongbaselineforthemulti-hopVidQAtask.Lastly,
we set a threshold at 70% of the maximum saliency score. weshowthatourmethodalsoachievesstate-of-the-artper-
Timestamps with scores above this threshold are merging formanceontheexistingpublicsingle-hopVidQAtask.
intotimespans.
6.1 EvaluationMetrics
Leveraging the similarity matrix SÀÜ ‚àà RK√óL, we apply
anaveragepoolingkernelwithasizeof3andastrideof1 Weevaluatetheperformanceofquestionansweringandev-
to smooth the values. Then we perform a softmax function idencegroundingseparatelyonMULTIHOP-EGOQA.
alongeachrowtogetpositivescores.Sinceeachrowvector Question answering. To evaluate open-ended answers, we
SÀÜ ‚àà RL in the matrix represents the predicted temporal useGPT-4oastheprimaryevaluatorforscoring,asitmore
k,:
relevance for the k-th grounding query, we apply the same closely aligns with human judgment and is widely adoptedTemporalGrounding QuestionAnswering
Methods Mode Input
mIoP mIoG IoU@0.3 mIoU Sent.Sim. Score(10‚Üë)
Human - - 71.8 81.0 87.0 61.8 74.3 7.5
GPT-4o(OpenAI2024) z.s. 60frames 18.9 24.4 12.0 12.2 73.7 5.4
End-to-EndMLLMs
InternVL2-8B(Chenetal.2024b) z.s. 30frames 11.8 24.0 6.3 6.6 71.9 4.5
LLaVA-NeXT-Video-7B(Zhangetal.2024b) z.s. 32frames - - - - 62.1 4.2
TimeChat-7B(Renetal.2024) z.s. 96frames 10.2 5.6 3.0 3.6 58.9 3.3
VTimeLLM-7B(Huangetal.2024a) z.s. 100frames 12.4 28.2 8.8 9.2 70.5 4.3
Pipeline:CaptionModule‚ÜíLLM(QA+Grounding)
LLaVa-NeXT-7B‚ÜíLlama-3.1-8B z.s. 180frames 21.4 22.3 10.1 9.7 63.6 3.5
GeLM-7B(Ours) f.t. 180frames 23.7 41.0 18.2 16.7 75.0 4.8
Table 2: Performance of various multi-modal models on MULTIHOP-EGOQA. The best and second-best performances
of the metrics are highlighted. ‚Äòz.s.‚Äô and ‚Äòf.t.‚Äô refer to zero-shot and fine-tuning, respectively. Existing approaches of various
typesfallshortofhumanperformanceonthischallengingtask.Tobridgethisgapandsetanewbaseline,wehavetrainedour
proposedarchitectureusingautomaticallyconstructedvisualinstructiontuningdata.
forassessmentpurposes(ChiangandLee2023;Zhengetal. sampled frames from the video clip, to perform answering
2024).WealsoreporttheaverageSentenceSimilarity(Sent. andgrounding.
Sim.) between the ground truth answers and the predicted
End-to-end models. We conduct investigations across var-
answers (Reimers and Gurevych 2019). For time-related
ious popular MLLMs, including Image LLM (InternVL2-
questions, we exclude them from metrics of answering, as
8B), Short Video LLM (LLaVA-NeXT-Video-7B), and
theycanbeaccuratelyevaluatedwithlocalizationmetrics.
LongVideoLLMs(TimeChat-7B,VTimeLLM-7B).
Evidence localization. Given the m predicted non-overlap
Multi-stagepipeline.Toexploretheeffectivenessofdense
time spans: TÀÜ = {TÀÜ 1,TÀÜ 2,...,TÀÜ m} and the ground truth captioningforMH-VidQA,weadoptamulti-stagepipeline,
consistingofnspans:T ={T 1,T 2,...,T n}foreachvideo, consisting of an image caption module, followed by an
IoU(IntersectionoverUnion)iscomputedasfollows: LLM.Thecaptionsofsampledframeswithtimestampswill
beutilizedbytheLLMforansweringandgrounding.
(cid:80)m (cid:80)n |TÀÜ ‚à©T |
IoU(T,TÀÜ)= (cid:12) i=1 j=1 i j (cid:12) (11) Overall Results. From experiments presented in Tab. 2,
(cid:12)(cid:83)m TÀÜ ‚à™(cid:83)n T (cid:12) we can draw the following observations: 1) Both the pro-
(cid:12) i=1 i j=1 j(cid:12)
prietary model and open-source multi-modal LLMs signif-
This can be seen as an extension of the IoU between two icantly lag behind human performance, underscoring the
intervals,measuringtheJaccardDistancebetweentwosets current limitations in multi-hop reasoning and grounding
oftimespans.Subsequently,themeanIoU(mIoU)isdeter- capabilities within multi-modal systems. 2) Reasoning and
minedbyaveragingtheIoUvaluesacrosstheentiretestset. grounding abilities are disentangled in existing visual sys-
Additionally,wecalculatetheproportionofvideoswithan tems.Forinstance,LLaVA-NeXT-Videoisunabletohandle
IoU exceeding 0.3, designated as IoU@0.3. Similar to pre- requestsinvolvingtemporalgrounding,butcanstillanswer
cision and recall, we compute mIoP and mIoG by averag- part of questions that do not involve temporal grounding.
ing Intersection over Prediction (IoP) and Intersection over 3) Instruction-tuning with single-hop data does not guar-
Ground Truth (IoG), replacing the denominator in Eq. (11) anteesuperiorityinmulti-hopgrounding.Forexample,de-
with(cid:12) (cid:12)(cid:83)m TÀÜ(cid:12) (cid:12)and(cid:12) (cid:12)(cid:83)n T (cid:12) (cid:12),respectively. spite TimeChat and VTimeLLM have been fine-tuned with
(cid:12) i=1 i(cid:12) (cid:12) j=1 j(cid:12) temporallyawareinstructionsandmulti-turnconversations,
the ability to ground multiple intervals for a single query
6.2 EvaluationonMULTIHOP-EGOQA
remains limited. 4) Dense captions do indeed help tempo-
Inthissection,weevaluateseverallatestmulti-modalmod- ral grounding, but errors may cascade. Although caption-
elsonMULTIHOP-EGOQA,exploringtheirabilitiesofmulti- ing at per second provides explicit temporal information
hopreasoningandtemporalgrounding. for grounding, errors in the captioning process are difficult
to correct through the subsequent stages. We recommend
Humanandadvancedproprietarymodel.Initially,wein-
thatreadersrefertotheevaluationdetailsandadditional
vite participants (different from annotators in the curation
qualitativeresultsintheSupplementaryMaterial.
pipeline)toassesshumanperformanceonthistask.Weran-
domlysample10%ofthetestsplitandrequestparticipants
6.3 ABaselineMethodforMULTIHOP-EGOQA
to answer the questions and localise relevant time spans.
Additionally, we evaluate the advanced proprietary model, Inthefollowingsection,weproposeanewbaselineforthis
GPT-4o,byleveragingitsvisualcapabilitieswithuniformly challengingtaskandconductablationexperimentstoevalu-TemporalGrounding QA TemporalGrounding QA
TrainingLoss Strategy Method TrainingData
IoU@0.3 mIoU Score‚Üë IoU@0.3 mIoU Score‚Üë
L - - - 4.7 ‚úó 3.0 3.6 3.3
CE
TimeChat
+L Saliency 13.8 14.2 4.7 100% 8.6 8.1 4.4
BCE
+L NCE Similarity 14.1 13.4 4.6 25% 13.0 11.3 4.6
Saliency 19.2 14.7 4.7 Ours 50% 16.7 16.1 4.7
+L +L
NCE BCE
Similarity 18.2 16.7 4.8 100% 18.2 16.7 4.8
Table3:Ablationofthetrainingobjectiveandinference Table4:Effectoftheinstruction-tuningdata.Weexplore
strategy.The gray shadingindicatesthedefaultsetting. onboththepre-trainedmodelandourproposedarchitecture.
ate the effectiveness of the automatically constructed train- 6.4 OnExistingSingle-HopVidQABenchmark
ingdata,andourarchitecturaldesignforfutureresearch.
Dataset and metrics. In addition to our multi-hop bench-
ImplementationDetails mark, we validate the effectiveness of our method on
the public single-hop VidQA benchmark (Huang et al.
Training data. We utilize the triplets generated in our
2024b), which contains 229 question-answer pairs across
automated pipeline to train the multi-modal LLM and the
160 videos. For this benchmark, the temporal grounding
grounding module. These triplets have been filtered by the
metricsaremIoUandPrecision@0.5(P@0.5),withthelat-
LLM, but not manually refined in Stage IV, consisting of
termeasuringthepercentageofpredictionswithanIoUover
3,156clipswithatotalof10,414samples.
0.5. Additionally, the GPT-4 Relative Score (R. Score) is
Architecture.ThevisualfeaturesofMULTIHOP-EGOQA are computedforevaluatingthepredictedexplanations.
extractedwiththeInternVideo-MM-L-14(Wangetal.2022)
Comparison.Intheexistingstate-of-the-artmethod,forex-
from 8 frames per second. The large language model em-
ample,LITA(Huangetal.2024b)addsspecialtimetokens
ployedisVicuna-7Bv1.3(Chiangetal.2023).Thedimen-
intothevocabularytoprocesstemporalgroundingasanext-
sions of the hidden states for the LLM and the grounding
tokenpredictiontaskonthisbenchmark.AsshowninTab.5,
moduleare4096and1024,respectively.
our architecture significantly exceeds LITA with both tem-
Training setup. The experiments are conducted using 4 poralgroundingbranchesafterfine-tuning.
NVIDIA H800 (80GB) GPUs, with a batch size of 32 per
device. The model is trained for 10 epochs with a learning TemporalGrounding QA
rateof2√ó10‚àí5,employingawarmupcosinedecaystrategy. Model Strategy
mIoU P@0.5 R.Score‚Üë
AblationStudies LITA-7B TimeToken 24.1 21.2 44.0
Effect of training objective and inference strategy. We LITA-13B TimeToken 28.6 25.9 46.3
explore the role of each training loss and the effect of the Ours-7B Saliency 31.8 28.2 45.3
two branches on generating temporal proposals. As shown
Ours-7B Similarity 35.4 ‚Üë11.3 31.0 ‚Üë9.8 45.1 ‚Üë1.1
in Tab. 3, although the saliency branch is unable to distin-
guishthetimeintervalofeachgroundingtokenpair,thebi- Table5:Comparisonwiththestate-of-the-artmethodon
narycrossentropylosstendstobenefitthetemporalground- ActivityNet-RTL,apublicsingle-hopVidQAbenchmark.
ing, improving the performance of the similarity branch,
withIoU@0.3increasingfrom14.1to18.2,andmIoUfrom
13.4to16.7.Correspondingly,thesimilaritybranchalsoen- 7 Conclusion
hancestheinferenceresultsofthesaliencybranch,demon-
Toconclude,wehaveinitiatedtheMH-VidQAtaskforlong-
stratingthecomplementarityofbothbranches.
formegocentricvideounderstanding.Toacquiretheassoci-
Effect of the visual instruction-tuning data. To validate ateddataset,wehavedevisedanautomatedpipelinetomine
the effectiveness of our data curation pipeline for mining large-scalemulti-hopQAtriplets,asubsetofwhicharesub-
large-scale multi-hop VidQA data, we utilize the automat- sequentlyvalidatedandrefinedmanually,resultinginanew
icallycollectedinstructionstofine-tuneapre-trainedVideo benchmark. Existing multi-modal systems demonstrate im-
LLM, e.g., TimeChat, and evaluate on MULTIHOP-EGOQA. provementinmulti-hopreasoningabilitiesaftertrainingon
As Tab. 4 shows, visual instruction tuning enhances the the automatically collected data, but they still struggle to
multi-hop reasoning and grounding abilities of TimeChat, groundtemporalevidencefortheirresponseseffectivelydue
demonstrating the effectiveness of constructed data. Addi- to weak temporal perception. To bridge this gap, we have
tionally, we trained our GeLM model by varying percent- proposed a novel model capable of answering multi-hop
ages of data, while maintaining the same number of itera- questionsandconcurrentlygroundingscatteredvisualclues,
tionstoexploretheeffectofthedatascale.Thecontinuous which establishes a baseline for this challenging task after
performance improvement from the increased data volume visualinstructiontuning.Ourmethodalsoachievesstate-of-
demonstratesthepotentialofourautomatedpipelinetocol- the-artperformanceonthepublicsingle-hopVidQAbench-
lectlarge-scaledataandenhancemodelcapabilities. mark,furtherunderscoringitseffectiveness.Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos
SupplementaryMaterial
A BenchmarkDetails
This section provides additional statistical details about the proposed benchmark, MULTIHOP-EGOQA, outlines the question
typescategorizedbyhumanannotators,detailstheautomatedpipelinerulesandpromptsforgeneratingandfilteringquestion-
answer-evidencetriplets,andintroducestheuserinterfacesforhumanannotationandparticipantevaluation.
A.1 AdditionalStatistics
As illustrated in Figure 4, we present a statistical analysis of MULTIHOP-EGOQA, encompassing the duration of temporal
evidence,thenumberoftimespansinvolvedinthequestions,andthedistributionofwordcountsinbothquestionsandanswers.
Our calculations reveal that the average duration of temporal evidence is 19.5 seconds, with half of the instances lasting less
than15seconds.Theaveragenumberoftimespans,or‚Äúhops‚Äù,is2.1,withamaximumof6hopsobserved.Furthermore,the
averagewordcountsforquestionsandanswersare10.3and14.4,respectively,reflectinganotablelevelofcomplexity.
30 27.3% 50 49.8%
40
20 19.4% 18.6%
30
23.8%
12.2%
20 18.8%
10 8.2% 8.0%
6.3%
10
5.6%
1.5% 0.3%
0 0
[0, 5) [5, 10) [10, 15) [15, 20) [20, 25) [25, 30) [30, 180) 1 2 3 4 5 6
Evidence Duration (s) Number of Time Spans
(a)Histogramoftemporalevidenceduration. (b)Histogramoftimespancounts.
53.3% 42.3%
50 40
40
30
30
20 18.2% 18.6%
20.6%
20 17.0%
12.0%
10 8.5% 10 7.2%
0.0% 0.6% 1.4%
0 0
[0, 4) [4, 8) [8, 12) [12, 15) [15, 20) [20, 50) [0, 5) [5, 10) [10, 15) [15, 20) [20, 25) [25, 50)
Number of Words Number of Words
(c)Histogramofquestionwordcounts. (d)Histogramofanswerwordcounts.
Figure4:VisualizationofMULTIHOP-EGOQAstatistics.
A.2 Multi-HopQuestionCategories
AsshowninTable6,thequestionsin MULTIHOP-EGOQA aremanuallycategorizedintosixgroups:repeatedactivities,mul-
tiple actions, multiple objects, multiple locations/people, event composition, and event comparison. Notably, all these visual
questions require gathering information from multiple time intervals and reasoning across them to provide accurate answers.
Thetemporaldistributionofthesetimespanscanbescatteredanddistant.ThequestionformatfollowstheNLQtaskstylein
Ego4Dbutcoversabroaderrangeofscenarios.
)%(
egatnecreP
)%(
egatnecreP
)%(
egatnecreP
)%(
egatnecrePID Category Question Answer
You<T1>openedthefridge</T1>three
A(39.6%) RepeatedActivities HowmanytimesdidIopenthefridge?
times.
WhatitemdidItakeoutofthefridgeand You<T1>tookoutaconicalflask</T1>
B(22.4%) MultipleActions
laterputbackduringthevideo? andthen<T2>putitback</T2>.
Yourinsed<T1>thecarrot</T1>,<T2>
C(17.9%) MultipleObjects WhatitemsdidIrinsebesidesmyhand? thetomato</T2>,and<T3>thepotato
peeler</T3>.
Youputthephone<T1>onthekitchen
WheredidIputthephone? counter</T1>andlater<T2>onthe
D(6.7%) MultipleLocations/People diningtable</T2>.
HowmanypeopledidItalkto?
Two.Youtalkedto<T1>ladyX</T1>
and<T2>ladyY</T2>.
You<T1>putthetoysin</T1>,andthe
E(10.3%) EventComposition WhatdidIdoafteropeningthebox?
secondtime<T2>tookoutcandles</T2>.
No.You<T1>rinsedthewashingpad
DidIwashmyhandsfirstorrinsethe
F(3.1%) EventComparison </T1>between<T2>washingyourhands
washingpad?
twice</T2>.
Table6:ExamplesofeachcategoryinMULTIHOP-EGOQA.Eachpairofspecialtokens<T>...</T>representsthetimeintervals
oftheenclosedreferent.‚ÄòEventComposition‚Äôreferstoquestionsthatinvolvebothmultipleactionsandmultipleobjects.
A.3 DetailsoftheAutomatedPipeline
Rule-based Filtering. As described in the main paper, we filter video clips based on the following criteria: (i) those with
thenumberofnarrationsgreaterthan60orsmallerthan30,(ii)thosewherethetimespanbetweenthefirstnarrationandlast
narrationislessthan150s.Thisfilteringensuresthatthenarrationsofclipsaredetailedandhavebalancedtemporalgranularity.
We select nodes u with recurrence times smaller than t = 5 and greater than t = 2. Additionally, the time span of the
max min
selectednarrationsN shouldexceed10stoensurethedistinctnessofmultipletimeintervals.
u
ThepromptforLLM-basedGenerationandFiltration. InTable7andTable8,wepresentthepromptsusedforgenerating
andfilteringmulti-hopVidQAtriplets,utilizinggpt-4o-2024-05-13,respectively.Inpractice,weemploythreedifferent
generationpromptsforthenodeu,correspondingtotheattributesverb,dobj(directobjects),andpobj(prepositionalobjects).
Weempiricallydesignsevenin-contextlearningexamples,toencompassasmanyspecificformsoftheselectednarrationN
u
aboutthenodeuaspossible.
A.4 DetailsoftheAnnotationProcedure
Annotation Interfaces. In Figure 5a, we present the annotation interface developed for human annotators to validate the
tripletsgeneratedandfilteredbyGPT-4o,aswellastorefinethetimeintervalsforreasonablequestion-answerpairs.Figure5b
showstheinterfacedesignedforparticipantsdifferentfrompreviousannotators,towatchthevideoclipwiththeaccompanying
questions. These participants are required to answer the question and localise the time spans that support their answers. The
resultsarethenusedtoassessthehumanperformanceofansweringandgroundingontheMulti-HopVidQAtask.
AnnotationGuidelines. Thedetailedguidelines,asillustratedintheinterfaceofFigure5a,areoutlinedasfollows:
Annotation Background: For a 3-minute video, there are several QAs to be annotated. Each QA includes a question, an
answer,thecorrespondingtimerangeinthevideofortheanswer,andthequestiontype.EachQAalreadyhasannotations
basedonGPT-4o,andthetaskistomodifyorfiltertheseannotations.Specifically:
1. ForeachQA,firstcheckifthequestionandanswerarereasonablebasedonthevideocontent.
2. Each key action or information mentioned in the answer should be enclosed with a pair of delimiters <T></T>,
representingthetimeintervalitoccursinthevideo.Notethateachactionmayappearinasingletimeperiod[s,e]or
repeatedlyinmultipletimeperiods[[s ,e ],...[s ,e ]](unitsinseconds,0‚â§s‚â§e‚â§179).
1 1 n n
3. AnnotatethetypeofQAbasedontheexamplesinthetable(A,B,... ,F).Ifthereisasignificantdiscrepancybetween
thequestionandanswer,markitdirectlyasU,representing‚ÄòUnusable‚Äô.
4. Whenthequestiondirectlyasksforthetimingofanevent(WhendidI...?),theanswershouldalwaysbe<T></T>.
Forthiscase,onlyensurethatthetemporalannotationiscorrect.
Thankyouforparticipating!SystemPrompt
YouareahelpfulassistantdesignedtooutputJSON.
User:
Iwantyoutoactasateacherinaclasscalled‚ÄòVideoUnderstanding‚Äô.Iwillprovidevideoactiondescriptionswithtimestamps,
andyourtaskistogenerateonequestion-and-answer(QA)pairforyourstudents.
-Createquestionsthatrequireintegratinginformationfrommorethanoneprovidedactiondescriptiontoanswer.
-Ensurethequestionisspecificandconcrete,notgeneral.Ensurethequestiondoesnotcontainanytimestamp.
-Ensuretheanswerisundisputedlyrighttoyourquestion.Provideconcisesentencesfortheanswer.
- Enclose the mentioned action or objects in your answer between two paired special tokens (e.g., <T1> and </T1>,
<T2>and</T2>,...),andnextoutputthetimespanofeachenclosedinformationaccordingtodescriptionswithtimes-
tamps.
-Whentheinputactiondescriptionsaresimilar(likeonlythehandusedisdifferent),directlyaskaboutthetimespansofthe
action(WhendidI...?).Theanswershouldincludeonlyspecialtokenswhendirectlyaskingabouttimespans(e.g.,Example
4).
-Mergethesame,similarorcontinuousactioninyouranswerandtheirtimespan.(e.g.,Example3).
Hereareexamplesoftheinputdescriptionswithtimestamps,andtheoutputQApair:
Example1:
User:
start,end,description
92,94,CtalkswithladyB.
96,97,CtalkswithladyB.
167,173,CtalkswithladyX.
176,179,CtalkswithladyX.
Assistant:
{‚ÄòQuestion‚Äô:‚ÄòWhodidItalkwith?‚Äô,
‚ÄòAnswer‚Äô:‚ÄòYou<T1>talkedwithtwoladiesincluding<T1>ladyB</T1>and<T2>ladyX</T2>.‚Äô,
‚ÄòTimespan‚Äô:{‚Äò<T1>‚Äô:[92,97],‚Äò<T2>‚Äô:[167,179]}}
Example2:
User:
start,end,description
22,23,Cplacesthesyringeonthetable.
27,28,Cplacesthecontainerinthemachine.
149,150,Cplacesthesyringeonthecounter.
163,174,Cplacesthecontaineronthecounter.
Assistant:
{‚ÄòQuestion‚Äô:‚ÄòWheredidIplacethesyringeonduringthevideo?‚Äô,
‚ÄòAnswer‚Äô:‚ÄòYou<T1>placedthesyringeonthetable</T1>and<T2>onthecounter</T2>.‚Äô,
‚ÄòTimespan‚Äô:{‚Äò<T1>‚Äô:[22,23],‚Äò<T2>‚Äô:[149,150]}}
Example3:
...
Now,it‚Äôsyourturnandlet‚Äôsthinkstepbystep.Basedonthefollowingdescriptionswithtimestampstogenerateonerequired
QApair:
User:
start,end,description
<start>,<end>,<narration>
...
Assistant:
Table 7: Prompt for GPT-4o to generate multi-hop VidQA triplets. This prompt is designed for the action node (e.g.,
talkofexample1,andplaceofexample2).Theremainingfiveexamplesareomittedduetospaceconstraints.SystemPrompt
YouareahelpfulassistantdesignedtooutputJSON.
User:
IwillprovideaQApairthattestsstudents‚Äôunderstandingofanegocentricvideo,focusingonactivitiesfeaturedinthevideo.
Thequestionrequirestudenttointegrateinformationfrommorethanonetimespantoanswer.Yourtaskistoevaluatewhether
theQAisreasonable.Specifically,analyzeifanyofthefollowingissuesarepresent:
1.Thequestionistoovague,ambiguousorgeneral.
2.Giventheanswer,thequestionisnotnaturalorwell-defined.
3.Thequestioninvolvesambiguousactionslike‚ÄòwhatdidItouch,‚Äôwhichcanhavevariousdegreesofinterpretation(e.g.,
holdingvs.brieflytouching).Thesimilarverbsinclude‚Äòcheck‚Äô,‚Äòinspect‚Äô,‚Äòadjust‚Äô,etc.
ForeachinputQA,youfirstneedtooutputwhetheritisreasonable,andthenprovidetherationaleforyourjudgement.In
youroutput,0indicatestheQAisreasonable,and1indicatestheQAisunreasonable.
ExamplesofreasonableQAs:
Example1:
Input:{‚ÄòQ‚Äô:‚ÄòWheredidIplacethesyringeduringthevideo?‚Äô,‚ÄòA‚Äô:‚ÄôYouplacedthesyringeonthetableandonthecounter.‚Äô}
Output:{‚ÄòJudgement‚Äô:0,‚ÄòRationale‚Äô:‚ÄòTherearenoobviousproblems.Theanswercanbedeterminedonlybywatchingthe
video.‚Äô}
Example2:
Input:{‚ÄòQ‚Äô:‚ÄòDidIremovethepansupportfirstorremovedirtfromthebrush?‚Äô,‚ÄòA‚Äô:‚ÄòYouremovedthepansupportafter
removingdirtfromthebrush.‚Äô}
Output:{‚ÄòJudgement‚Äô:0,‚ÄòRationale‚Äô:‚ÄòThequestionandanswerareclearandspecific,allowingforawell-definedresponse
basedonthevideocontent.‚Äô}
Example3:
Input:{‚ÄòQ‚Äô:‚ÄòWhatliquiddidIpouronthesponge?‚Äô,‚ÄòA‚Äô:‚ÄòYoupouredwateranddetergentonthesponge.‚Äô}
Output: {‚ÄòJudgement‚Äô: 0, ‚ÄòRationale‚Äô: ‚ÄòThe question is well-defined and specific. The answer can be determined only by
watchingthevideoandispossiblyrighttothequestion.‚Äô}
Example4:
...
ExamplesofunreasonableQAs:
Example1:
Input:{‚ÄòQ‚Äô:‚ÄòWhatdidIremovebeforethepansupport?‚Äô,‚ÄòA‚Äô:‚ÄòYouremoveddirtfromthebrushbeforeremovingthepan
support.‚Äô}
Output: {‚ÄòJudgement‚Äô: 1, ‚ÄòRationale‚Äô: ‚ÄòRemoving dirt and removing the pan support are two distinct actions despite both
involvingtheverb‚Äòremove.‚ÄôGroupingtheminonequestionmakesitdifficultforstudentstoformulatethecorrectanswer.‚Äô}
Example2:
Input:{‚ÄòQ‚Äô:‚ÄòWhatobjectsdidIopen?‚Äô,‚ÄòA‚Äô:‚ÄòYouopenedafile,adoor,andabucket.‚Äô}
Output:{‚ÄòJudgement‚Äô:1,‚ÄòRationale‚Äô:‚ÄòThequestionistoobroadandvagueregardingtheverb‚Äòopen.‚ÄôItischallengingfor
studentstocategorizeopeningadoorandopeningafileasthesametypeofaction.‚Äô}
Example3:
...
Now,it‚Äôsyourturnandlet‚Äôsthinkstepbystep.Basedonthefollowingdescriptionswithtimestampstogenerateonerequired
QApair:
Input:<QASample>
Assistant:
Table 8: Prompt for GPT-4o to preliminarily filter the unreasonable multi-hop VidQA samples, before further manual
validationandrefinement.Theremainingexamplesareomittedduetospaceconstraints.(a)Humanannotationinterface.
(b)Humanevaluationinterface.
Figure5:UserinterfacesforthemanualannotationandtheevaluationofhumanperformanceonMULTIHOP-EGOQA.B EvaluationDetails
Inthissection,webeginbyintroducingthebaselinemodelswhichareevaluatedinMULTIHOP-EGOQA,includingmulti-modal
models and a multi-stage pipeline. Subsequently, we provide the prompts and detailed settings employed in the inference
processwiththesesystems.Furthermore,wereporttheirQAperformanceonadditionalopen-endedansweringmetrics,some
ofwhicharenotutilizedinMULTIHOP-EGOQA,astheymaybedeemedunsuitableforevaluatinglong-formanswers.
B.1 BaselinesandInferenceSettings
GPT-4o (OpenAI 2024). We uniformly sample one frame every three seconds from the video clip and leverage the visual
perception abilities of gpt-4o-2024-05-13 to simultaneously answer the given question and ground the supporting evi-
dence.Additionally,weemployregularexpressionstoextracttimeintervalsfromtheresponse.The‚Äòdetail‚Äôparameterforthe
APIfunctionissetto‚Äòlow‚Äô.ThepromptusedispresentedinTable10.
InternVL2(Chenetal.2024b). InternVL2isanopen-sourceMulti-modalLargeLanguageModel(MLLM),whichsupports
diverseinputmodalitiesandmultitaskoutputs.Weutilizethemulti-turnconversationalabilityofInternVL2toassessitsper-
formanceinbothansweringandgrounding.Leveragingitscapabilityofmulti-imageinput,weuniformlysampled30frames
fromthevideoclip.ThespecificcheckpointweusedisOpenGVLab/InternVL2-8BfromHuggingFace.Thepromptused
ispresentedinTable11.
LLaVA-NeXT-Video(Zhangetal.2024b). LLaVA-NeXT-Videoisamodelthatexcelsinvideounderstandingtasks,lever-
agingtechniqueslikeAnyResforrepresentinghigh-resolutionimagesandlengthgeneralizationforhandlinglongvideos.We
utilizethecheckpointllava-hf/LLaVA-NeXT-Video-7B-hffromHuggingFaceforevaluation.Despitetestingmulti-
pleprompts,LLaVA-NeXT-Videoencountersdifficultiesingeneratingtime-relatedresponses.Asaresult,wereportonlythe
question-answeringmetricsforthismodel.ThedetailedpromptispresentedinTable12.
TimeChat(Renetal.2024). TimeChatisamultimodallargelanguagemodeldesignedforlongvideounderstanding,featur-
ingatimestamp-awareframeencoderandaslidingvideoQ-Former,supportedbytheTimeITdatasetwith125Kinstancesfor
improvedinstruction-following.GiventhatTimeChathasdesignedspecificinstructiontemplatesfortemporalgrounding,we
leverage its multi-turn conversational capabilities to first answer the question and then follow its predefined prompt styles to
groundtheanswer.Thenumberofinputframesis96asdefault.ThedetailedpromptispresentedinTable12andthecheckpoint
isdownloadedfromtheofficialGitHubrepository.
VTimeLLM (Huang et al. 2024a). VTimeLLM is a Video LLM designed for precise video moment understanding and
temporal reasoning, employing a three-stage training strategy, namely, feature alignment, temporal-boundary awareness, and
enhancement of temporal understanding. We adhere to the multi-turn conversational templates of VTimeLLM, starting with
questionanswering,thenfollowedbygroundingtheresponse.Bydefault,100framesaresampledasvisualinput.Thedetailed
promptisprovidedinTable14.
Multi-stagePipeline. Toconstructamulti-stagepipelineformulti-hopVidQA,wefirstemployanimagecaptioningmodule,
llava-hf/llava-v1.6-mistral-7b-hffromHuggingFace,togeneratecaptionsforframesuniformlysampledfrom
thevideopersecond,withpromptsshowninTable15.Thisprocessyields180captionspervideo.Forthereasoningmodule,we
utilizemeta-llama/Meta-Llama-3.1-8B-Instruct.ThepromptsusedaresimilartothoseemployedforGPT-4o,as
showninTable16,withthedistinctionbeingtheinput,whichconsistsofeithervideoframesorframecaptions.
B.2 AdditionalMetricsforEvaluatingOpen-EndedResponses
QuestionAnswering
Methods Input
BLEU-4 METEOR ROUGE-L CIDEr Sent.Sim Score‚Üë
GPT-4o 60frames 17.9 21.8 42.8 172.6 73.7 5.4
End-to-EndMLLMs
InternVL2-8B 30frames 21.0 24.1 43.6 173.6 71.9 4.5
LLaVA-NeXT-Video-7B 32frames 8.6 21.3 32.0 78.1 62.1 4.2
TimeChat-7B 96frames 10.3 16.3 32.0 79.7 58.9 3.3
VTimeLLM-7B 100frames 20.9 24.0 47.3 176.8 70.5 4.3
Pipeline:CaptionModule‚ÜíLLM(QA+Grounding)
LLaVa-NeXT-7B‚ÜíLlama-3.1-8B 180frames 6.4 16.0 36.8 123.3 63.6 3.5
Table9:Zero-shotperformancecomparisononadditionalopen-endedquestionansweringmetrics.
Asintroducedinthemaintext,weutilizeGPT-4oastheprimaryevaluatortoscoretheopen-endedresponsesbasedonthe
givenquestionsandthecorrespondinggroundtruthanswers.ThespecificpromptusedforevaluationispresentedinTable17.Priorstudies(Ba¬®rmannandWaibel2022)adoptBLEU-4(Papinenietal.2002),METEOR(BanerjeeandLavie2005)and
ROUGE(Lin2004)asmetricsforevaluatingshortanswerswithin5words,whicharenotsuitableforlonganswerstypicalofour
benchmark.InTable9,wereportthezero-shotperformanceofmulti-modalmodelsontheseevaluationmetrics.Itisimportant
tonotethat,‚Äòzero-shot‚Äômeansthemodelshavenotbeenexplicitlyfine-tunedonthetrainingsetofMULTIHOP-EGOQA,though
Ego4D videos or other egocentric videos might be involved in training some of the evaluated models. We observe that only
SentenceSimilarity(Sent.Sim.)alignswiththejudgementsofGPT-4o,comparedwithBLEU-4,METEOR,andROUGE-L.
CIDErtendstofavourresponsesthatparaphrasethequestionbeforeanswering.Specifically,weutilizeall-MiniLM-L6-v2
fromtheSentenceTransformerslibrarytoextractsentenceembeddings,following(DiandXie2024).
User:
Hereisa3-minuteegocentricvideorecordingmyactivities.Iwillprovideyouwithframessampledevery3secondsfromthe
video.Yourtaskistoansweraspecificquestionbasedontheseframesaccurately.Ensureyouranswerisconcise.
Afteranswering,listthetimeintervalsrelatedtothequestionasevidence.Eachintervalshouldhaveastartandendtimestamp
inseconds(e.g.,[[9,15],[120,135]]).Notethatthedurationofthevideois180sandtheframesaresampledat0s,3s,6s,...,
180s.
Finally,explainyouranswerinonesentence.
Here‚Äôsanexampleoftheresponseformat:
###Question:
HowmanytimesdidIopenthetap?
###Answer:
Youopenedthetaptwice.
###Evidence:
[[9,15],[120,135]]
###Rationale:
Accordingtotheframessampledfrom9sto15s,andfrom120sto135s,youopenedthetaptwice.
Yourresponseshouldstrictlyfollowtheexampleformat,includingthreeparts:Answer,Evidence,andRationale.Donotadd
anyextracontent.Hereisthequestionyouneedtoanswer:
###Question:
<Question>
###Frames:
Theframesampledat<Second>s:
<Frame>
...
Assistant:
Table10:PromptforGPT-4otoperformansweringandgroundingonMULTIHOP-EGOQA.
User:
Frame<Number>:<Image>.
...
Youaregivenanego-centricvideo.Pleasewatchthevideoandanswerthefollowingquestion:<Question>
Assistant:
<Answer>
User:
Theframesaresampleduniformlyfrom0sto180s,namelyat6s,12s,...,180s.Localizethetimespansthatsemantically
matchandsupportyouranswerofthequestion.Forexample,theanswercanbededucedfrom10sto25s,andfrom140sto
150s.
Assistant:
<Evidence>
Table11:PromptforInternVL2toperformansweringandgroundingonMULTIHOP-EGOQA.User:
Youaregivenframesofavideo.Accordingtotheinputframes,answerthefollowingquestionconcisely:<Question>.
Table 12: Prompt for LLaVa-NeXT-Video to perform only answering on MULTIHOP-EGOQA. The video inputs are im-
plicitlyincorporatedwithtextpromptsthroughfunctioncalls.
SystemPrompt
Youareabletounderstandthevisualcontentthattheuserprovides.Followtheinstructionscarefullyandexplainyouranswers
indetail.
User:
Frame<Number>:<Image>.
...
Youaregivenanegocentricvideo.Pleasewatchthevideoandanswerthefollowingquestion:<Question>
Assistant:
<Answer>
User:
Detectandreportthestartandendtimestampsofthevideosegmentthatsemanticallymatchesthetextualquery<Answer>
Assistant:
<Evidence>
Table13:PromptforTimeChattoperformansweringandgroundingonMULTIHOP-EGOQA.
User:
Thisisavideowith100frames:<Video>
<Question>
Assistant:
<Answer>
User:
Duringwhichframescanwesee<Answer>happeninginthevideo?
Assistant:
<Evidence>
Table14:PromptforVTimeLLMtoperformansweringandgroundingonMULTIHOP-EGOQA.
User:
[INST]<image>
Describetheactioninthisvideoframeinoneconcisesentence.[/INST]
Assistant:
Table15:Promptforthecaptionmoduleinthemulti-stagepipelinetocaptionthevisualcontentofper-secondframes.User:
Hereisavideoshotfromafirst-personperspective,recordingmyactivities.Iwillprovidedescriptionsofeachsecondof
thevideo.YourtaskistoanswerthequestionIgiveyoubasedonthesevideodescriptions.Ensureyouranswerisconcise.
Afteranswering,providethetimeintervalsthatrelatedtothequestion-answeringasevidence.Evidenceislistofintervals
(e.g.,[[s1,e1],...,[s ,e ],...])andeachtimeinterval(e.g.,[s ,e ],s <= e )consistsofonestarttimestampandoneend
i i i i i i
timestamp.Notethatevidencemayincludemultipletimeintervals.Finally,explainyouranswerinonesentence.
Hereisanexampleformatforyourresponse.
###Question
WhendidIopenthetap?
###Answer
Youopenedthetapfrom10sto20sand60sto70s.
###Evidence
[[10,20],[60,70]]
###Rationale
Accordingtotheactiondescriptionswithtimestamps,youopenedthetapfortwotimesfrom10sto20sandfrom60sto70s.
Hereisthevideoandquestionyouneedtoreview:
###Captions
timestamp,caption
<timestamp>,<caption>
...
###Question
<Question>
Yourresponseshouldstrictlyfollowtheformatofgivenexample,includingthreeparts:###Answer,###Evidence,and###Ra-
tionale.Donotaddanyextracontent.
Assistant:
Table16:Promptforthereasoningmoduleinthemulti-stagepipelinetoperformansweringandgrounding,basedonthe
captionswithtimestampsacquiredfromthepreviousstage.
SystemPrompt
YouareahelpfulassistantdesignedtooutputJSON.
User:
Astheinstructorofavideounderstandingcourse,youhaveassignedyourstudentstowatchafirst-personperspectivevideo
andansweraquestionrelatedtoitscontent.Yourtaskistogradestudents‚Äôanswerbasedonthequestionandreferenceanswer.
Yourscoreshouldbebetween1and10,withahigherscoreindicatingaclosermatchbetweenthestudent‚Äôsanswerandthe
referenceanswer.Whengrading,considerthefollowingaspectsofthestudent‚Äôsanswer:helpfulness,relevance,accuracy,and
levelofdetail.Provideabriefrationaleforyourscore.
Ensureyourresponseisadictwith‚Äòscore‚Äôand‚Äòrationale‚Äôaskeys.Forexample,{‚Äòscore‚Äô:5,‚Äòrationale‚Äô:‚Äò<rationale>‚Äô}.
###Question
<Question>
###ReferenceAnswer
<GroundTruthAnswer>
###StudentAnswer
<PredictedAnswer>
Assistant:
Table17:PromptforGPT-4otoscoreopen-endedresponsesbasedonquestionsandtheassociatedgroundtruthanswers.C AdditionalExperiments
Inthissection,wepresentseveralextendedablationstudiesandqualitativeanalysisoftheresultsproducedbyvariousmodels.
C.1 ExtendedAblationStudies
Ablationofthresholdsforgeneratingtemporalproposals. Asoutlinedinthemainpaper,weutilizeathresholdingmethod
togeneratetemporalproposalsbasedonboththesaliencyscorevectorandthesimilarityscorematrix.Thethresholdvalueis
determined by multiplying a coefficient by the maximum activation value along the time axis. In Table 18a, we analyze the
impactofthiscoefficient,notingthatitvariesbetweenthesaliencyscoreandthesimilarityscore,becausethesimilarityscore
undergoestemperaturescalingbeforetheapplicationofthesoftmaxfunction,leadingtodifferingcoefficientvalues.
Trainingonbothegocentricandthird-viewdatasets. Wetrainourarchitectureusingbothautomaticallyconstructedmulti-
hoptripletsandthetrainingsplitofActivityNet-RTL.Subsequently,weevaluatethemodelonMULTIHOP-EGOQA,aspresented
inTable18b,andonthetestsplitofActivityNet-RTL,asshowninTable18c.
Ourfindingssuggestthatunifiedtrainingleadstoaslightperformancedecreaseacrossbothbenchmarkscomparedtotrain-
ing them separately. Despite this decline, the unified model outperforms existing methods on both MULTIHOP-EGOQAand
ActivityNet-RTL. This reduction in performance may be attributed to the distribution gap between QA samples and the dif-
fering perspectives ofthe two datasets. Incorporatingadditional grounded QA trainingdata from more diversesources could
potentiallyenhancethemodel‚Äôsgeneralizationcapabilities.
TemporalGrounding TemporalGrounding
Strategy Coefficient TrainData #Frames
mIoP mIoG IoU@0.3 mIoU IoU@0.3 mIoU
0.6 24.0 26.7 19.1 15.3 Ego 180 18.2 16.7
Saliency 0.7 24.3 24.3 19.2 14.7 Ego+Third 180 15.3 15.1
0.8 24.8 21.9 17.3 14.0 (b)EffectoftrainingonmixeddataforMULTIHOP-EGOQA.
0.05 22.2 45.0 17.4 16.3
Similarity 0.10 23.7 41.0 18.2 16.7 TemporalGrounding
TrainData #Frames
0.15 24.8 32.9 16.7 15.8 mIoU P@0.5
(a) Ablation of thresholds when inferring on MULTIHOP-EGOQA. As Third 100 35.4 35.1
detailedinthemaintext,wedeterminethethresholdforgeneratingtem- Third 180 33.0 31.6
poralproposalsbytakingthemaximumvalueofthesaliencyorsimilarity Ego+Third 180 32.1 27.1
scoreforeachrowvectorandmultiplyingitbyacoefficient.
(c)EffectoftrainingonmixeddataforActivityNet-RTL.
Table18:ExtendedablationexperimentsoftrainingandinferringusingGeLM.
C.2 QualitativeAnalysis
Visualizationofoutputsfromthesaliencybranchandsimilaritybranch. Givenatestvideoin MULTIHOP-EGOQA with
theassociatedquery,‚ÄúWhatorderdidIopenthefridgeandthedrawerduringthevideo?‚Äù,theresponseprovidedbyourmodel
is, ‚ÄúYou <T1> opened the fridge </T1> before <T2> opening the drawer </T2>‚Äù, which involves two distinct time spans. As
depictedinFigure6,weillustratethegroundtruthtemporalevidence T = {[71,78],[89,92]}inseconds,thesaliencyscore
vectoryÀÜ ‚àà RL alongwitheachrowvectorSÀÜ ofthesimilarityscorematrixSÀÜ ‚àà RK√óL.Thesaliencybranchhasgenerated
k,:
temporal proposals consisting of two windows globally, while the similarity branch is capable of pinpointing the time spans
delineatedbyeachpairofgroundingtokenswithineachrowofthesimilaritymatrix.
1 1.0
0.3 <T1></T1>
0.8 <T2></T2>
0.6 0.2
0.4
0.1
0.2
0 0.0 0.0
0 30 60 90 120 150 180 0 30 60 90 120 150 180 0 30 60 90 120 150 180
Timestamp (seconds) Timestamp (seconds) Timestamp (seconds)
Figure6:Visualizationofthegroundtruthtemporalproposals(left),alongwiththeresultsfromthesaliencybranch(middle)
andsimilaritybranch(right)oftheGeLMmodeltoprovidetemporalevidencefora2-hopquestion.
hturT
dnuorG
erocS
ycneilaS
erocS
ytiralimiSVisualizationoftheevaluationresultson MULTIHOP-EGOQA. Wepresentadditionalmulti-hopVidQAevaluationresults
ofvariousmodelsalongwithourproposedmethod.
AsillustratedinFigure7,whenasked,‚ÄùWhattypesofcabinetsdidIopenduringthevideo?‚Äù,ourGeLMmodelgenerates
theresponse,‚ÄùYouopenedadrawercabinetandadoorcabinet‚Äù,matchingthevisualcontent.Moreover,itpreciselyidentifies
the temporal boundaries of both events as visual evidence respectively. In contrast, TimeChat and VTimeLLM offer only a
single,imprecisetimeintervalandanambiguousanswer.
0s 36s 72s 108s 144s 180s
ùí¨: What types of cabinets did I open during the video?
ùíú: You opened a cabinet underneath the slab and opened sections of the wall cabinet. ùíØ: 15 - 18s, 88 - 93s
GeLM: You opened a drawer cabinet and a door cabinet. ùíØÃÇ: 16 - 18s, 88 - 91s
TimeChat: You opened a kitchen cabinet and a cupboard in the video. ùíØÃÇ: 5 - 30s
VTimeLLM: You opened a white cabinet and a brown cabinet during the video. ùíØÃÇ: 59 - 90s
Figure7:Theevaluationexampleaboutthe‚ÄòMultipleObject‚Äô.
AsshowninFigure8,whenaskedthequestion,‚ÄúHowmanytimesdidIcloseadraweronthedrawercabinettoolboxduring
thevideo?‚Äù,ourmodelprovidestheresponse‚ÄúYouclosedthedrawerthreetimes‚Äù,whichalignswiththegroundtruth:‚ÄúYou
closedadraweronthedrawercabinettoolboxthreetimes‚Äù.Additionally,itaccuratelyidentifiesthethreespecifictimespans
duringwhichthisactivityoccurs.However,InternVL2generatesanincorrectanswerandtimespan,evenwhenpromptedthat
multipletimeintervalsmayexist.AlthoughVTimeLLMcorrectlyidentifiesthenumberoftimes,itcanonlyprovideasingle
timespan,alimitationimposedbyitsinstructionsduringthefine-tuningstage.
0s 36s 72s 108s 144s 180s
ùí¨: How many times did I close a drawer on the drawer cabinet tool box during the video?
ùíú: You closed a drawer on the drawer cabinet tool box three times. ùíØ: 38 - 39s, 90 - 98s, 133 - 135s
GeLM: You closed the drawer three times. ùíØÃÇ: 36 - 39s, 89 - 98s, 130 - 135s
InternVL2: During the video, you closed a drawer on the drawer cabinet tool box for two times. ùíØÃÇ: 20 - 30s
VTimeLLM: You closed the drawer on the drawer cabinet tool box three times during the video. ùíØÃÇ: 30 - 59s
Figure8:Theevaluationexampleaboutthe‚ÄòRepeatedActivities‚Äô.D LimitationsandEthicalConcerns
Despitethepromisingresults,ourproposedmethodcanbeimprovedfromthreeaspects:Firstly,ourautomatedpipelinehasthe
potential for further enhancement, such as integrating additional visual models to improve the accuracy of data construction.
Moreover,wesuggestextendingtheapplicationofourautomatedpipelinetolongervideos,andthird-personperspectivevideos,
whichwouldbettermeettheevolvingdemandsofVideoLLMs.Inaddition,similartotheapproachofcalculatingmeanAverage
Precision(mAP)separatelyforsmallandlargeobjectsinobjectdetection,wemayconsiderdifferentiatingtheIntersectionover
Union(IoU)calculationsforlongerandshorterintervalstomorepreciselyassessthetemporalgroundingcapabilitiesofvarious
models.Finally,incorporatinggenerallanguagegroundingdataduringpre-trainingmayfurtherstrengthenthegeneralization
capabilitiesofourbaselinemethod.Withregardtoethicalconsiderations,werecognizethattheknowledgeembeddedinlarge
languagemodelsmaycarrybiasesrelatedtogender,age,geography,orculture.
References
Achiam,J.;Adler,S.;Agarwal,S.;Ahmad,L.;Akkaya,I.;Aleman,F.L.;Almeida,D.;Altenschmidt,J.;Altman,S.;Anadkat,
S.;etal.2023. GPT-4technicalreport. arXiv:2303.08774.
AI@Meta.2024. Llama3ModelCard.
Alayrac,J.-B.;Donahue,J.;Luc,P.;Miech,A.;Barr,I.;Hasson,Y.;Lenc,K.;Mensch,A.;Millican,K.;Reynolds,M.;etal.
2022. Flamingo:avisuallanguagemodelforfew-shotlearning. InNeurIPS.
Ataallah,K.;Shen,X.;Abdelrahman,E.;Sleiman,E.;Zhu,D.;Ding,J.;andElhoseiny,M.2024.MiniGPT4-Video:Advancing
MultimodalLLMsforVideoUnderstandingwithInterleavedVisual-TextualTokens. arXiv:2404.03413.
Banerjee,S.;andLavie,A.2005. METEOR:AnautomaticmetricforMTevaluationwithimprovedcorrelationwithhuman
judgments. InACLWorkshop.
Ba¬®rmann, L.; and Waibel, A. 2022. Where did i leave my keys?-episodic-memory-based question answering on egocentric
videos. InCVPRW.
Chen, J.-J.; Liao, Y.-C.; Lin, H.-C.; Yu, Y.-C.; Chen, Y.-C.; and Wang, Y.-C. F. 2024a. ReXTime: A Benchmark Suite for
Reasoning-Across-TimeinVideos. arXiv:2406.19392.
Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong, W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to
gpt-4v?closingthegaptocommercialmultimodalmodelswithopen-sourcesuites. arXiv:2404.16821.
Chiang,C.-H.;andLee,H.-y.2023. CanLargeLanguageModelsBeanAlternativetoHumanEvaluations? InACL.
Chiang,W.-L.;Li,Z.;Lin,Z.;Sheng,Y.;Wu,Z.;Zhang,H.;Zheng,L.;Zhuang,S.;Zhuang,Y.;Gonzalez,J.E.;Stoica,I.;and
Xing,E.P.2023. Vicuna:AnOpen-SourceChatbotImpressingGPT-4with90%*ChatGPTQuality.
Di,S.;andXie,W.2024. GroundedQuestion-AnsweringinLongEgocentricVideos. InCVPR.
Grauman,K.;Westbury,A.;Byrne,E.;Chavis,Z.;Furnari,A.;Girdhar,R.;Hamburger,J.;Jiang,H.;Liu,M.;Liu,X.;Martin,
M.;Nagarajan,T.;Radosavovic,I.;Ramakrishnan,S.K.;Ryan,F.;Sharma,J.;Wray,M.;Xu,M.;Xu,E.Z.;Zhao,C.;Bansal,
S.;Batra,D.;Cartillier,V.;Crane,S.;Do,T.;Doulaty,M.;Erapalli,A.;Feichtenhofer,C.;Fragomeni,A.;Fu,Q.;Gebreselasie,
A.;Gonzalez,C.;Hillis,J.;Huang,X.;Huang,Y.;Jia,W.;Khoo,W.;Kolar,J.;Kottur,S.;Kumar,A.;Landini,F.;Li,C.;Li,
Y.;Li,Z.;Mangalam,K.;Modhugu,R.;Munro,J.;Murrell,T.;Nishiyasu,T.;Price,W.;Puentes,P.R.;Ramazanova,M.;Sari,
L.;Somasundaram,K.;Southerland,A.;Sugano,Y.;Tao,R.;Vo,M.;Wang,Y.;Wu,X.;Yagi,T.;Zhao,Z.;Zhu,Y.;Arbelaez,
P.;Crandall,D.;Damen,D.;Farinella,G.M.;Fuegen,C.;Ghanem,B.;Ithapu,V.K.;Jawahar,C.V.;Joo,H.;Kitani,K.;Li,
H.;Newcombe,R.;Oliva,A.;Park,H.S.;Rehg,J.M.;Sato,Y.;Shi,J.;Shou,M.Z.;Torralba,A.;Torresani,L.;Yan,M.;and
Malik,J.2022. Ego4D:AroundtheWorldin3,000HoursofEgocentricVideo. InCVPR.
Grunde-McLaughlin, M.; Krishna, R.; and Agrawala, M. 2021. AGQA: A Benchmark for Compositional Spatio-Temporal
Reasoning. InCVPR.
Ho, X.; Nguyen, A.-K. D.; Sugawara, S.; and Aizawa, A. 2020. Constructing A Multi-hop QA Dataset for Comprehensive
EvaluationofReasoningSteps. InACL.
Honnibal,M.;Montani,I.;VanLandeghem,S.;Boyd,A.;etal.2020. spaCy:Industrial-strengthnaturallanguageprocessing
inpython.
Huang,B.;Wang,X.;Chen,H.;Song,Z.;andZhu,W.2024a. Vtimellm:Empowerllmtograspvideomoments. InCVPR.
Huang,D.-A.;Liao,S.;Radhakrishnan,S.;Yin,H.;Molchanov,P.;Yu,Z.;andKautz,J.2024b. LITA:LanguageInstructed
Temporal-LocalizationAssistant. InECCV.
Jasani,B.;Girdhar,R.;andRamanan,D.2019. AreweaskingtherightquestionsinMovieQA? InICCVW.
Ji, J.; Krishna, R.; Fei-Fei, L.; and Niebles, J. C. 2020. Action genome: Actions as compositions of spatio-temporal scene
graphs. InCVPR.Jiang,A.Q.;Sablayrolles,A.;Roux,A.;Mensch,A.;Savary,B.;Bamford,C.;Chaplot,D.S.;Casas,D.d.l.;Hanna,E.B.;
Bressand,F.;etal.2024. Mixtralofexperts. arXiv:2401.04088.
Lai,X.;Tian,Z.;Chen,Y.;Li,Y.;Yuan,Y.;Liu,S.;andJia,J.2024. Lisa:Reasoningsegmentationvialargelanguagemodel.
InCVPR.
Li,J.;Li,D.;Savarese,S.;andHoi,S.2023a. Blip-2:Bootstrappinglanguage-imagepre-trainingwithfrozenimageencoders
andlargelanguagemodels. InICML.
Li,K.;Wang,Y.;He,Y.;Li,Y.;Wang,Y.;Liu,Y.;Wang,Z.;Xu,J.;Chen,G.;Luo,P.;etal.2023b.Mvbench:Acomprehensive
multi-modalvideounderstandingbenchmark. arXiv:2311.17005.
Lin,C.-Y.2004. Rouge:Apackageforautomaticevaluationofsummaries. InACL.
Lin,K.Q.;Zhang,P.;Chen,J.;Pramanick,S.;Gao,D.;Wang,A.J.;Yan,R.;andShou,M.Z.2023. Univtg:Towardsunified
video-languagetemporalgrounding. InICCV.
Liu,H.;Li,C.;Li,Y.;Li,B.;Zhang,Y.;Shen,S.;andLee,Y.J.2024. LLaVA-NeXT:Improvedreasoning,OCR,andworld
knowledge.
Liu,H.;Li,C.;Wu,Q.;andLee,Y.J.2023a. VisualInstructionTuning. InNeurIPS.
Liu,Y.;Duan,H.;Zhang,Y.;Li,B.;Zhang,S.;Zhao,W.;Yuan,Y.;Wang,J.;He,C.;Liu,Z.;etal.2023b. Mmbench:Isyour
multi-modalmodelanall-aroundplayer? arXiv:2307.06281.
Lu,P.;Bansal,H.;Xia,T.;Liu,J.;Li,C.;Hajishirzi,H.;Cheng,H.;Chang,K.-W.;Galley,M.;andGao,J.2023. Mathvista:
Evaluatingmathematicalreasoningoffoundationmodelsinvisualcontexts. arXiv:2310.02255.
Mangalam,K.;Akshulakov,R.;andMalik,J.2023.EgoSchema:ADiagnosticBenchmarkforVeryLong-formVideoLanguage
Understanding. InNeurIPSDatasetsandBenchmarksTrack.
Miech, A.; Zhukov, D.; Alayrac, J.-B.; Tapaswi, M.; Laptev, I.; and Sivic, J. 2019. HowTo100M: Learning a Text-Video
EmbeddingbyWatchingHundredMillionNarratedVideoClips. InICCV.
Mu,F.;Mo,S.;andLi,Y.2024. SnAG:ScalableandAccurateVideoGrounding. InCVPR.
OpenAI.2024. GPT-4o. https://openai.com/index/hello-gpt-4o/.
Papineni,K.;Roukos,S.;Ward,T.;andZhu,W.-J.2002. Bleu:amethodforautomaticevaluationofmachinetranslation. In
ACL.
Patraucean,V.;Smaira,L.;Gupta,A.;Recasens,A.;Markeeva,L.;Banarse,D.;Koppula,S.;Malinowski,M.;Yang,Y.;Doer-
sch,C.;etal.2024. Perceptiontest:Adiagnosticbenchmarkformultimodalvideomodels. InNeurIPS.
Qian,L.;Li,J.;Wu,Y.;Ye,Y.;Fei,H.;Chua,T.-S.;Zhuang,Y.;andTang,S.2024.Momentor:Advancingvideolargelanguage
modelwithfine-grainedtemporalreasoning. InICML.
Ramakrishnan, S. K.; Al-Halah, Z.; and Grauman, K. 2023. Naq: Leveraging narrations as queries to supervise episodic
memory. InCVPR.
Reimers,N.;andGurevych,I.2019. Sentence-bert:Sentenceembeddingsusingsiamesebert-networks. InEMNLP.
Ren,S.;Yao,L.;Li,S.;Sun,X.;andHou,L.2024.Timechat:Atime-sensitivemultimodallargelanguagemodelforlongvideo
understanding. InCVPR.
Rodin,I.;Furnari,A.;Min,K.;Tripathi,S.;andFarinella,G.M.2024. ActionSceneGraphsforLong-FormUnderstandingof
EgocentricVideos. InCVPR.
Sanabria,R.;Caglayan,O.;Palaskar,S.;Elliott,D.;Barrault,L.;Specia,L.;andMetze,F.2018. How2:ALarge-scaleDataset
forMultimodalLanguageUnderstanding. InNeurIPS.
Tapaswi, M.; Zhu, Y.; Stiefelhagen, R.; Torralba, A.; Urtasun, R.; and Fidler, S. 2016. MovieQA: Understanding Stories in
MoviesthroughQuestion-Answering. InCVPR.
Trivedi,H.;Balasubramanian,N.;Khot,T.;andSabharwal,A.2022. MuSiQue:MultihopQuestionsviaSingle-hopQuestion
Composition. InTACL.
Wang,Y.;Li,K.;Li,Y.;He,Y.;Huang,B.;Zhao,Z.;Zhang,H.;Xu,J.;Liu,Y.;Wang,Z.;etal.2022. Internvideo:General
videofoundationmodelsviagenerativeanddiscriminativelearning. arXiv:2212.03191.
Wu, B.; Yu, S.; Chen, Z.; Tenenbaum, J. B.; and Gan, C. 2021. STAR: A Benchmark for Situated Reasoning in Real-World
Videos. InNeurIPSDatasetsandBenchmarksTrack.
Xiao, J.; Shang, X.; Yao, A.; and Chua, T.-S. 2021. NExT-QA:Next Phase of Question-Answering to Explaining Temporal
Actions. InCVPR.
Xiao,J.;Yao,A.;Li,Y.;andChua,T.-S.2024. Canitrustyouranswer?visuallygroundedvideoquestionanswering. InCVPR.
Xiong,W.;Li,X.;Iyer,S.;Du,J.;Lewis,P.;Wang,W.Y.;Mehdad,Y.;Yih,S.;Riedel,S.;Kiela,D.;etal.2021. Answering
ComplexOpen-DomainQuestionswithMulti-HopDenseRetrieval. InICLR.Yan, C.; Wang, H.; Yan, S.; Jiang, X.; Hu, Y.; Kang, G.; Xie, W.; and Gavves, E. 2024. VISA: Reasoning Video Object
SegmentationviaLargeLanguageModels. InECCV.
Yang,J.;Peng,W.;Li,X.;Guo,Z.;Chen,L.;Li,B.;Ma,Z.;Zhou,K.;Zhang,W.;Loy,C.C.;andLiu,Z.2023. PanopticVideo
SceneGraphGeneration. InCVPR.
Yang,Z.;Qi,P.;Zhang,S.;Bengio,Y.;Cohen,W.W.;Salakhutdinov,R.;andManning,C.D.2018. HotpotQA:Adatasetfor
diverse,explainablemulti-hopquestionanswering. arXiv:1809.09600.
Yu,Z.;Xu,D.;Yu,J.;Yu,T.;Zhao,Z.;Zhuang,Y.;andTao,D.2019. ActivityNet-QA:ADatasetforUnderstandingComplex
WebVideosviaQuestionAnswering. InAAAI.
Yue, X.; Ni, Y.; Zhang, K.; Zheng, T.; Liu, R.; Zhang, G.; Stevens, S.; Jiang, D.; Ren, W.; Sun, Y.; et al. 2024. Mmmu: A
massivemulti-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertagi. InCVPR.
Zhang,J.;Zhang,H.;Zhang,D.;Yong,L.;andHuang,S.2024a. End-to-EndBeamRetrievalforMulti-HopQuestionAnswer-
ing. InNAACL.
Zhang,Y.;Li,B.;Liu,h.;Lee,Y.j.;Gui,L.;Fu,D.;Feng,J.;Liu,Z.;andLi,C.2024b. LLaVA-NeXT:AStrongZero-shot
VideoUnderstandingModel.
Zhang,Y.;Ma,Z.;Gao,X.;Shakiah,S.;Gao,Q.;andChai,J.2024c. Groundhog:Groundinglargelanguagemodelstoholistic
segmentation. InCVPR.
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2024. Judging
llm-as-a-judgewithmt-benchandchatbotarena. InNeurIPS.
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023. Minigpt-4: Enhancing vision-language understanding with ad-
vancedlargelanguagemodels. arXiv:2304.10592.