JointPPO: Diving Deeper into the Effectiveness of PPO in Multi-Agent
Reinforcement Learning
ChenxingLiu and GuizhongLiu
SchoolofInformationandCommunicationsEngineering,Xiâ€™anJiaotongUniversity
lcx459455791@stu.xjtu.edu.cn,liugz@xjtu.edu.cn
Abstract control [Schrittwieser et al., 2020; Wu et al., 2023], apply-
ing RL to multi-agent systems, known as Multi-Agent Re-
While Centralized Training with Decentralized
inforcementLearning(MARL),emergesasapromisingand
Execution (CTDE) has become the prevailing
challenging research area. In MARL, all the agents consis-
paradigm in Multi-Agent Reinforcement Learning
tentlyinteractwiththeenvironment,optimizingtheirpolicies
(MARL), it may not be suitable for scenarios in
in a trial-and-error manner, with the goal of maximizing the
whichagentscanfullycommunicateandshareob-
expectedcumulativerewards.
servationswitheachother. Fullycentralizedmeth-
The existence of more than one learning agent in MARL
ods, also know as Centralized Training with Cen-
incurs greater uncertainty and learning instability, making
tralized Execution (CTCE) methods, can fullyuti-
it a challenging problem [Nguyen et al., 2020]. To tackle
lize observations of all the agents by treating the
thischallenge,existingMARLalgorithmsprimarilyfallinto
entire system as a single agent. However, tradi-
three categories: fully independent methods, fully central-
tionalCTCEmethodssufferfromscalabilityissues
izedmethods,andcentralizedtrainingwithdecentralizedex-
due to the exponential growth of the joint action
ecution (CTDE) methods. In fully independent methods, as
space. To address these challenges, in this paper
depicted in Figure 1(a), each agent trains and acts indepen-
we propose JointPPO, a CTCE method that uses
dently, making decisions solely based on its own observa-
ProximalPolicyOptimization(PPO)todirectlyop-
tion[Tampuuetal.,2017;deWittetal.,2020]. Thisallows
timize the joint policy of the multi-agent system.
fordirectimplementationofsingle-agentRLalgorithms,but
JointPPO decomposes the joint policy into con-
still suffers from learning instability. CTDE methods, on
ditional probabilities, transforming the decision-
the other hand, allow agents to access the environment state
making process into a sequence generation task.
or all the observations during training while still maintain-
A Transformer-based joint policy network is con-
ing decentralized execution during interactions, as depicted
structed, trained with a PPO loss tailored for the
in Figure 1(c) [Lowe et al., 2017; Foerster et al., 2018;
joint policy. JointPPO effectively handles a large
Rashid et al., 2020b]. While both of these methods have
joint action space and extends PPO to multi-agent
achievedgoodperformance,theyarelimitedtodecentralized
settingwiththeoreticalclarityandconciseness.Ex-
execution, which means that agents share little information
tensive experiments on the StarCraft Multi-Agent
witheachotherandmakedecisionindependently. Suchlim-
Challenge (SMAC) testbed demonstrate the supe-
itation makes them less appropriate for scenarios with suf-
riorityofJointPPOoverstrongbaselines. Ablation
ficient communication, as the information available in other
experimentsandanalysesareconductedtoexplores
agentsâ€™observationsisignored. Additionally,thewidespread
thefactorsinfluencingJointPPOâ€™sperformance.
useofparameter-sharingtechniquesinthesemethodsleadsto
suboptimal outcome and homogeneous behavior, potentially
1 Introduction causingfailureincomplexscenarios[Kubaetal.,2021].
Multi-AgentSystems(MAS)arecomplexsystemscomposed Alternatively, fully centralized methods, also known as
ofmultipleagentsthatcooperatewitheachotherinteracting centralizedtrainingwithcentralizedexecution(CTCE)meth-
with a common environment [Dorri et al., 2018]. Such sys- ods, mitigate these limitations by treating the entire multi-
tems are ubiquitous in various real-world scenarios, includ- agent system as a single agent and making full use of all
ingtrafficlightcontrol[Wuetal.,2020],finance[Leeetal., theagentsâ€™observations[Liuetal.,2021;Chenetal.,2022].
2007], and robots coordination [Han et al., 2020]. In this Figure1(b)illustratesCTCEmethodsthatbehaveasacentral
paper, we aim at developing an intelligent decision-making controller which integrates all agentsâ€™ observation and des-
method for fully cooperative MAS, in which agents act as a ignatestheiractions. Currentcentralizedmethods, however,
unifiedteamtotacklecomplextasks. facechallengesduetotheexponentialgrowthofjointaction
While Reinforcement Learning (RL) has shown remark- spaceswithincreasingnumberofagents.
able success in achieving intelligent decision-making and In this paper, we address those challenges in two steps:
4202
rpA
81
]AM.sc[
1v13811.4042:viXra(ğ‘ğ‘¡1,ğ‘ğ‘¡2,...,ğ‘ğ‘¡ğ‘›) ~ ğ…(âˆ™|ğ‘œğ‘¡1,ğ‘œğ‘¡2,...,ğ‘œğ‘¡ğ‘›)
Central Controller Centralized Critic
Training Phase
ğ‘ğ‘¡1~ğœ‹1(âˆ™|ğ‘œğ‘¡1) ğ‘ğ‘¡2~ğœ‹2(âˆ™|ğ‘œğ‘¡2) ğ‘ğ‘¡ğ‘›~ğœ‹ğ‘›(âˆ™|ğ‘œğ‘¡ğ‘›) ğ‘ğ‘¡1 ğ‘œğ‘¡1 ğ‘ğ‘¡2 ğ‘œğ‘¡2 ğ‘ğ‘¡ğ‘› ğ‘œğ‘¡ğ‘›
Agent 1 Agent 2
...
Agent n
ğ‘ğ‘¡1~ğœ‹1(âˆ™|ğ‘œğ‘¡1) ğ‘ğ‘¡2~ğœ‹2(âˆ™|ğ‘œğ‘¡2) ğ‘ğ‘¡ğ‘›~ğœ‹ğ‘›(âˆ™|ğ‘œğ‘¡ğ‘›)
... ...
Agent 1 Agent 2 Agent n Agent 1 Agent 2 Agent n
ğ‘ğ‘¡1 ğ‘œğ‘¡1 ğ‘ğ‘¡2 ğ‘œğ‘¡2 ğ‘ğ‘¡ğ‘› ğ‘œğ‘¡ğ‘›
ğ‘ğ‘¡1 ğ‘œğ‘¡1 ğ‘ğ‘¡2 ğ‘œğ‘¡2 ğ‘ğ‘¡ğ‘› ğ‘œğ‘¡ğ‘› Execution Phase
Environment Environment Environment
(a) Fullyindependent(DTDE)paradigm.(b) Fullycentralized(CTCE)paradigm. (c) CTDEparadigm.
Figure1:DifferentlearningparadigmsinMARL.
First, we decompose the joint policy of the multi-agent sys- 2 RelatedWorks
temintoconditionalprobabilities,transformingthedecision-
Inrecentyears,therehasbeensignificantprogressinMARL.
makingprocessintoasequencegenerationtask,andpropose
FullyindependentmethodIQL[Tampuuetal.,2017]firstex-
a general framework that solves MARL with any sequence
ploredextendingRLtomulti-agentsettingbyapplyingDQN
generation model. Then we propose JointPPO, a CTCE
to each agent independently. Fully centralized methods, on
method designed to directly optimize the joint policy. As
the other hand, have received less attention since they suf-
aninstanceoftheproposedframework,JointPPOcontainsa
fer from scalability issues due to the exponential growth of
jointpolicynetworkwhichactsasacentralcontroller,taking
jointactionspace. Existingfullycentralizedmethodsusually
all the agentsâ€™ observations as input and generating agentsâ€™
adoptaninformationexchangemechanismtohandlethelarge
actions sequentially at each decision time. A PPO loss tai-
action space [Liu et al., 2021; Chen et al., 2022]. However,
loredforthejointpolicyisdesignedforthenetworktraining.
in practice they do not exhibit the expected stronger perfor-
Consideredthatthenetworkarchitecturedesignisnotthepri-
mancethanCTDEmethods.
maryfocusofthispaper,weadopttheTransformerstructure
TheCTDEparadigmreachesacompromisebetweenfully
introducedinthestate-of-the-artalgorithmMAT[Wenetal.,
independent and centralized approaches, attracting great at-
2022] with some modifications as our joint policy network.
tention of the MARL community [Oliehoek et al., 2008;
In this way, JointPPO simplifies MARL to single-agent RL
Kraemer and Banerjee, 2016]. Numerous works have
andeffectivelysolvesitbyleveragingtheTransformermodel.
emerged within the CTDE paradigm, encompassing both
Mostimportantly,JointPPObringstheadvantagesofPPOto
value factorization methods and policy gradient methods.
MARLwiththeoreticalclarityandconciseness.
Mostvaluefactorizationmethodsweredesignedtosatisfythe
We extensively evaluate JointPPO on StarCraft Multi-
IGM (Indicidual-Global-Max) condition [Son et al., 2019].
Agent Challenge (SMAC) testbed [Samvelyan et al., 2019]
VDN [Sunehag et al., 2017] first conducted value factoriza-
across various maps, encompassing both homogeneous and
tion by approximating the joint value function as a sum of
heterogeneous scenarios. JointPPO consistently demon-
individual ones. QMIX [Rashid et al., 2020b] extended this
stratessuperiorperformanceanddataefficiencycomparedto
withamonotonicityassumptionandanon-negative-weighted
strong baselines. It achieves nearly 100% win rates across
mixer network. Subsequent efforts usually built upon the
all the test maps and exhibits an remarkable advantage in
structureintroducedinQMIX,furtherapproachingtheIGM
terms of cost for victory such as killed allies. Comprehen-
condition or introducing additional components [Son et al.,
siveablationexperimentsandanalysesarefurtherconducted
2019;Mahajanetal.,2019;Wangetal.,2020a;Rashidetal.,
to explore the elements influencing JointPPOâ€™s training pro-
2020a]. However, these value factorization methods face a
cessandfinalperformance.
commonchallengecausedbythemismatchbetweentheop-
Tosumup,thecontributionsofthisworkareasfollows:
timaljointvaluefunctionandindividualvaluefunctionsdur-
â€¢ We explicitly decompose the joint policy of the multi- ing training. Such mismatch necessitates more iterations to
agent system into conditional probabilities, and sum- recoverthesatisfactionofIGM,resultinginlowsampleeffi-
mariseageneralframeworkofsolvingMARLusingany ciency.
sequencegenerationmodel. Among the various policy gradient methods, trust region
methods, represented by Trust Region Policy Optimization
â€¢ We propose JointPPO as an instance of the proposed
(TRPO)[Schulmanetal.,2015a]andProximalPolicyOpti-
framework. JointPPO effectively handles the high-
mization (PPO) [Schulman et al., 2017], stand out for their
dimensionaljointactionspacebyleveragingtheTrans-
supreme performance with theoretically-justified monotonic
formermodel.
policy improvement [Kakade and Langford, 2002]. Numer-
â€¢ As a CTCE method, JointPPOâ€™s performance demon- ous studies have tried to extend this advantage to the multi-
strates the feasibility of addressing MARL by simpli- agent setting. While IPPO [de Witt et al., 2020] applied
fying it into single-agent RL, which facilitates the ef- PPOindependentlytoeachagent,MAPPO[Yuetal., 2022]
fectiveintegrationoftechniquesandresearchoutcomes introduced centralized critics and comprehensively explored
fromsingle-agentRLintothedomainofMARL. factors that influences its performance. HAPPO [Kuba etal., 2021] presented an Multi-Agent Advantage Decompo- 3.2 Multi-AgentTransformer
sition Theorem and proposed a sequential update scheme.
The state-of-the-art algorithm, Multi-Agent Transformer
MAT[Wenetal., 2022], themostrelevantworkstothispa- (MAT)[Wenetal.,2022],firsteffectivelysolveMARLprob-
per,wasderivedfromtheMulti-AgentAdvantageDecompo-
lem by transforming it into a sequence generation problem
sitionTheoremandintroducedanovelapproachofleveraging
andleveragingtheTransformermodeltomaptheinputofthe
thesequencemodeltogenerateagentsâ€™actionssequentially. agentsâ€™observations(cid:0) o1,...,on(cid:1)
totheoutputoftheagentsâ€™
t t
There are also lots of recent work following the sequential
actions
(cid:0) a1,...,an(cid:1)
. It consists of an encoder and a de-
updateschemeortheaction-dependentscheme[Wangetal., t t
coder, separately used to learn a valid representation of the
2023b; Kuba et al., 2022; Li et al., 2021; Bertsekas, 2021;
jointobservationsandtooutputtheagentsâ€™actionsinanauto-
Li et al., 2023]. However, those methods either suffer from
regressivemanner. MATâ€™straininglossisdesignedbasedon
sampleinefficiencyorrequiresophisticatedtheoreticalanal-
the Multi-Agent Advantage Decomposition Theorem, which
yses,whicharesusceptibletovulnerabilities. Incontrast,we
decomposes the joint advantage function of the multi-agent
startfromtheperspectiveofCTCE,proposeapracticalalgo-
systemintoindividualadvantagefunctionsandimpliesase-
rithm JointPPO that uses PPO to directly optimize the joint
quential update scheme. However, we argue that the loss in
policy. SinceJointPPOisderivedwithoutanyassumptionof
itsimplementationdoesnotstrictlyadheretothementioned
valuedecompositionorcreditassignment,itnaturallyinher-
theorem,asitgeneratesagentsâ€™theactionssequentiallyrather
itsthetheoreticalguaranteeofmonotonicimprovementfrom
thanupdatingtheagentsâ€™actionssequentially. Besides,MAT
PPOforthejointpolicy.
remainsatthestageofoptimizingindividualpolicies,without
considering a direct optimization for the joint policy. All of
3 Preliminaries
those lead to a mismatch between its theoretical foundation
3.1 PODMP andpracticalimplementation.
Despitethoseconcerns,MATâ€™suseoftheTransformerhas
Weconsiderthedecision-makingprobleminthefullycoop-
beenasignificantcontributionandsuccess. Therefore,inthis
erative multi-agent systems described by Partially Observ-
paper,weadoptitsTransformerarchitecturewithsomemod-
ableMarkovDecisionProcesses(POMDP)[Kaelblingetal.,
ificationstoconstructourjointpolicynetwork. Mostimpor-
1998]. An n-agent POMDP can be formalized as a tuple
tantly,weredesignthePPOlossfromafullycentralizedper-
âŸ¨N,S,O,A,P,R,Î³âŸ©, where N = {1,...,n} is the set of
spective, aiming to provide theoretical clarity and concise-
agentsandS istheglobalstatespaceoftheenvironment. We
ness.
denotethelocalobservationandactionspaceofagentibyOi
andAi respectively,andsubsequently,theCartesianproduct
4 Method
O = O1Ã—,...,Ã—On represents the joint observation space
of the system and A = A1Ã—,...,Ã—An represents the joint In this section, we present details of JointPPO in three sub-
actionspace. P : S Ã—AÃ—S â†’ [0,1]isthetransitionfunc- sections: problem modeling, Transformer-based joint policy
tiondenotingthestatetransitionprobability. R:SÃ—Aâ†’R network, and joint PPO loss. In problem modeling, we dis-
representstherewardfunctionwhichgivesrisetotheinstan- cussthedecompositionofthejointpolicyandproposeagen-
taneousrewardandÎ³ âˆˆ[0,1)isthediscountfactorthatgives eralframeworkthatsolvesMARLwithsequencegeneration
smallerweightstofuturerewards. model. In the next two subsections, we present the detail of
InPOMDP,eachagenti âˆˆ N onlyhaveaccesstotheob- JointPPO in term of its network architecture and loss func-
servation oi âˆˆ Oi to the environment rather than the global tion,asaninstanceoftheproposedframework.
t
state s âˆˆ S. At each time step t, all agents i âˆˆ N choose
t
their actions ai âˆˆ Ai which may be discrete or continu- 4.1 ProblemModeling
t
ous, and all the actions together forming the joint action Asmentionedearlier,thegoalforMARListolearnanopti-
a t = (cid:0) a1 t,...,an t(cid:1) âˆˆ A. Executing the joint action a t, the mal joint policy Ï€âˆ—(a t|o t) that maximizes the expected ac-
agentsstimulatetheenvironmentintothenextstateaccording cumulatedreturn(Eq.(1)). Mostexistingmethodshandlethe
to the transition function P and, at the same time, receive a jointpolicyÏ€bydecomposingitintoindependentindividual
scalarteamrewardr t =R(s t,a t). Repeatingtheabovepro- policies:
cess, agents consistently interact with the environment and
collect the rewards. We define the joint policy Ï€(a t|o t) as
Ï€(a |o )=Ï€(cid:0) a1,a2,...,an|o (cid:1)
=(cid:89)n
Ï€i(cid:0) ai|o (cid:1) . (2)
a conditional probability of the joint action a t given all the t t t t t t t t
agentsâ€™ observations o =
(cid:0) o1,...,on(cid:1)
âˆˆ O, and return i=1
t t t
G wh( eÏ„ r) e=
Ï„
d(cid:80) enâˆ t o= t0 esÎ³ tt hr et a ss amth pe lea dcc tu ram jeu cl ta ot re yd .d Ti hsc eo gu on ate ld ofre Mw Aar Rds L, N vio dt ua ab ll py, olt ih ce yr ,e suar ce hs ae sv Ï€er ia (cid:0)l ad i|i off ie (cid:1)re on rt Ï€f io (cid:0)r am iu |Ï„la it (cid:1)i ,o wns hio cf ht dh ie ffi en rd ii n-
t t t t
istolearnanoptimaljointpolicyÏ€âˆ— thatmaximizestheex- theavailableinformationusedfordecisionmaking.However,
pectedreturn: the underlying assumption remains unchanged: the agentsâ€™
actionsareindependentofeachother.
Ï€âˆ— =argmaxE [G(Ï„)]
Ï€ Whilesuchindependenceassumptionfacilitatesdecentral-
Ï€
(cid:34) âˆ (cid:35) (1) ized execution, it has some shortcomings. First, as high-
=argmaxE (cid:88) Î³tR(cid:0) s ,a | (cid:1) . lightedintheintroduction,decentralizedexecutionisnotuni-
Ï€
Ï€ t t atâˆ¼Ï€(at|ot)
versally suitable. It is restricted to scenarios where commu-
t=0ğ‘‰ğ‘‰(ğ’ğ’ğ‘¡ğ‘¡) ğœ‹ğœ‹ğ‘šğ‘š (ï¿½|ğ’ğ’ğ’•ğ’•,ğ‘ğ‘ğ‘¡ğ‘¡1:ğ‘šğ‘šâˆ’1 )
average pooling
critic feed forward add & norm
feed forward
feed forward
ğ’ğ’ï¿½ğ‘¡ğ‘¡=(ğ‘œğ‘œï¿½ğ‘¡ğ‘¡1 ,ğ‘œğ‘œï¿½ğ‘¡ğ‘¡2 ,...,ğ‘œğ‘œï¿½ğ‘¡ğ‘¡ğ‘›ğ‘› ) add & norm
add & norm
cross-attention multi-head attention
feed forward slef-attention block
block
add & norm ğ’ğ’ï¿½ğ‘¡ğ‘¡
add & norm
masked
multi-head attention observation action multi-head attention
embedding embedding
1 2 ğ‘›ğ‘› 1 2 ğ‘šğ‘šâˆ’1
ğ’ğ’ğ‘¡ğ‘¡=(ğ‘œğ‘œğ‘¡ğ‘¡,ğ‘œğ‘œğ‘¡ğ‘¡,...,ğ‘œğ‘œğ‘¡ğ‘¡) ğ‘ğ‘ğ‘¡ğ‘¡,ğ‘ğ‘ğ‘¡ğ‘¡,...,ğ‘ğ‘ğ‘¡ğ‘¡
Figure2:ArchitectureoftheTransformer-basedpolicynetwork.
nicationamongagentsislimited. Incontrast,real-worldsit- State transition
uations often involve agents with robust communication ca-
pabilities, enabling them to freely share observations. This
ğ‘ ğ‘ ğ‘¡ğ‘¡+1~ğ‘ƒğ‘ƒ(ï¿½|ğ‘ ğ‘ ğ‘¡ğ‘¡,ğ‘ğ‘ğ‘¡ğ‘¡)
sharing of observations enhances agentsâ€™ awareness to the ğ‘ ğ‘ ğ‘¡ğ‘¡ ğ‘ ğ‘ ğ‘¡ğ‘¡+1
environmentandcanleadtobettercooperation,whiledecen-
tralizedexecutionneglectsthis. Second,thereexistsituations
ğ’ğ’ğ‘¡ğ‘¡ ğ’ğ’ğ‘¡ğ‘¡+1
where the agentsâ€™ actions exhibit interdependence. Agents 1 1
in a system can sometimes be divided into dominant agents
ğ‘ğ‘ğ‘¡ğ‘¡~ğœ‹ğœ‹ (ï¿½|ğ’ğ’ğ’•ğ’•)
1 Action
andassistantagents[Wangetal.,2023a;Zhangetal.,2022; ğ‘ğ‘ğ‘¡ğ‘¡ 2 2 1 generation
Ruanetal.,2022b;Duetal.,2022]. Theactionsofthedom- ğ‘ğ‘ğ‘¡ğ‘¡~ğœ‹ğœ‹ (ï¿½|ğ’ğ’ğ’•ğ’•,ğ‘ğ‘ğ‘¡ğ‘¡)
2
inantagentscarrygreaterimportance,grantingthempriority ğ‘ğ‘ğ‘¡ğ‘¡
indecision-making. Subsequently,theassistantagentsmake â€¦ ğ‘›ğ‘› ğ‘›ğ‘› 1:ğ‘›ğ‘›âˆ’1
ğ‘ğ‘ğ‘¡ğ‘¡~ğœ‹ğœ‹ (ï¿½|ğ’ğ’ğ’•ğ’•,ğ‘ğ‘ğ‘¡ğ‘¡ )
decisions based on the dominant agentsâ€™ actions, playing a ğ‘›ğ‘›
supportiverole.Suchacooperativepatternisalsocommonin Joint ağ‘ğ‘cğ‘¡ğ‘¡tion
humansociety,whereindividualactionsarenotindependent ğ’‚ğ’‚ğ‘¡ğ‘¡=(ğ‘ğ‘ğ‘¡ğ‘¡1 ,ğ‘ğ‘ğ‘¡ğ‘¡2 ,...,ğ‘ğ‘ğ‘¡ğ‘¡ğ‘›ğ‘› )
Figure3:Actiongenerationprocess.
but rather correlated, challenging the independence assump-
tion.
Therefore, we propose an alternative decomposition also contributes to handling the exponential growth of the
methodforthejointpolicythatdoesnotrelyontheassump- jointactionspace.
tion of independence among agentsâ€™ actions. Formally, we Whileourproposedframeworkdoesnotnecessitatethein-
decomposethejointpolicyintoconditionalprobabilities: dependence assumption, it does require a pre-specified or-
Ï€(a |o )=Ï€(cid:0) a1,a2,...,an|o (cid:1) (3) der for the actions generation, implying some dependencies
t t t t t t among the agents. We view this order as prior knowledge
n
=(cid:89) Ï€i(cid:0) ai|o ,a1:iâˆ’1(cid:1) , (4) aboutthesystemandwewilldiscussthisfurtherintheabla-
t t t tionexperiments.
i=1
where Ï€i(cid:0) ai|o ,a1:iâˆ’1(cid:1) , called the conditional individual 4.2 Transformer-BasedJointPolicyNetwork
t t t
policy,istheconditionalprobabilityoftheith agentâ€™saction Having transformed the decision-making process into a
giventhejointobservationandtheprecedingagentsâ€™actions. sequence generation task, we can take advantage of
In this way, the decision-making process of MAS is explic- any state-of-the-art sequence models. The Transformer
itly transformed into a sequence generation task. Given the model [Vaswani et al., 2017], originally designed for ma-
joint observation o at each time step, the actions of agents chine translation tasks, has exhibited strong sequence mod-
t
aregeneratedsequentially,asillustratedinFigure3. elingabilities. Hence,weadopttheTransformerarchitecture
Anysequencegenerationmodelhasthepotentialtotackle introducedinMATtoconstructourjointpolicynetwork.
this task. Consequently, we propose a generalized frame- AsillustratedinFigure2,ourTransformer-basedjointpol-
work, outlined in Algorithm 1, designed to solves MARL icy network consists of an encoder, a decoder, and a cen-
withanysequencegenerationmodel. Thisframeworkoffers tralized critic. The encoder, whose parameters are denoted
the benefit of simplifying MARL into single-agent RL. Be- by Ï•, plays a vital role of learning an effective representa-
sides,theapplicationofpowerfulsequencegenerationmodel tion of the original observations oË† =
(cid:0) oË†1,...,oË†n(cid:1)
. This is
t t tAlgorithm1AGeneralFrameworkofSolvingMARLUsing joint observation value function V (oË†), which is approxi-
Ïˆ t
SequenceGenerationModel mated by the centralized critic, to estimate the joint advan-
Input: Number of agents n, the action generation order tagefunction, followingtheGeneralizedAdvantageEstima-
(i ,...,i ). tion(GAE)[Schulmanetal.,2015b]as:
1 n
Initialize: Acentralizedcritic,asequencegenerationmodel, h
andareplaybuffer. A(o ,a )=(cid:88) (Î³Î»)lÎ´ , (5)
t t t+l
1: repeat l=0
2: Consistently interact with the environment using the whereÎ´ =r +Î³V (oË† )âˆ’V (oË†)istheTDerrorattime
t t Ïˆ t+1 Ïˆ t
sequencegenerationmodelasthejointpolicynetwork, step t and h is GAE steps. Similar to IPPO [de Witt et al.,
whichtakesallagentsâ€™observationasinputandgener- 2020],weformulatethecriticâ€™slossastheerrorbetweenthe
atesnactionssequentiallyintheorderofi 1,i 2,...,i n. predicted joint observation value V Ïˆ(oË† t) and its estimated
Collect interaction data and input the data into replay valuebasedontherealcollectedrewards:
3:
b Tu raff ie nr t.
hecentralizedcriticusingthesampleddatafrom L =
1 T (cid:88)âˆ’1 min(cid:20)(cid:16)
V
(oË†)âˆ’VË†(cid:17)2 ,(cid:16)
V (oË†)+
thereplaybuffertoapproximatethevaluefunction. critic T Ïˆ t t Ïˆold t
t=0
4: Trainthejointpolicynetworkusingthesampleddate (cid:17)2(cid:21)
fromthereplaybufferandthevaluefunctionapproxi- clip(V (oË†)âˆ’V (oË†),âˆ’Ïµ,+Ïµ)âˆ’VË† ,
Ïˆ t Ïˆold t t
matedbycentralizedcritic.
(6)
5: untilThedesiredperformanceisachieved
where VË† = A +V (oË†) and Ïˆ are the old parameters
Output: Atrainedjointpolicynetwork. t t Ïˆ t Î¸old
beforetheupdate. Eq.(6)restrictstheupdateofthecentral-
izedvalue functiontowithina trustregion, preventingfrom
overfitting to previous data as the data distribution continu-
achieved through a self-attention block consisting of a self-
ally changes with the evolving policy. Since the critic takes
attention mechanism, Multi-Layer Perceptrons (MLPs), and
theencodedobservationoË† asinput,thislossalsocontributes
residualconnections. Suchcomputationalblocktakesallthe t
agentsâ€™originalobservationso =
(cid:0) o1,...,oi(cid:1)
asinput, in-
tothetrainingoftheencodertolearnanexpressiverepresen-
t t t tationoftheobservations.
tegratestask-relatedinformationandoutputstheencodedob-
servations oË† =
(cid:0) oË†1,...,oË†n(cid:1)
. Then those coded observa-
As for policy training, we employ PPO on the generated
t t t joint policy. The joint policy Ï€ (a |oË†) is computed fol-
tionsarepassedthroughthecentralizedcritic,whoseparam- Î¸ t t
lowing Eq. (4) by multiplying the generated conditional lo-
etersaredenotedbyÏˆ,tocalculatethejointobservationvalue
cal policies. Formally, the joint PPO loss is constructed as
V (oË†),whichisthenusedtocalculatethejointPPOloss.
Ïˆ t follows:
The decoder, whose parameters are denoted by Î¸, acts
Tâˆ’1
as a sequence generator that generates the agentâ€™s action 1 (cid:88) (cid:16) (cid:17)
a t =(cid:0) a1 t,...,ai t(cid:1) inanauto-regressionmanner.Specifically, L policy =âˆ’ T min Î± tA t,clip(Î± t,1Â±Ïµ)A t , (7a)
thisprocessbeginswithaninputofainitialtokenaswellas t=0
the encoded observation oË† , and output of the first agentâ€™s where
t
conditional individual policy Ï€ Î¸1(cid:0) a1 t|oË† t(cid:1) , which is actually
Î± =
Ï€ Î¸(a t|oË† t)
(7b)
the probability distribution over possible actions. The first t Ï€ (a |oË†)
agentâ€™s action a1
t
is sampled from this distribution, encoded (cid:81)Î¸o nld Ï€t i(cid:0)t ai|oË†,a1:iâˆ’1(cid:1)
asaone-hotvector,andthenfedbackintothedecoderasthe = i=1 Î¸ t t t .
second token. Subsequently, the second agentâ€™s action a2
t
is (cid:81)n i=1Ï€ Î¸i old(cid:0) ai t|oË† t,a t1:iâˆ’1(cid:1)
sampledaccordingtotheoutputconditionalindividualpolicy Eq. (7a) presents a direct use of PPO on the joint policy
Ï€ Î¸2(cid:0) a2 t|oË† t,a1 t(cid:1) . This process continues until all the agentsâ€™ Ï€ Î¸(a t|oË† t), whose update is restricted to within the trust re-
actionsaregenerated,togetherformingthejointaction. The gion. ThetheoreticaladvantageofPPOguaranteesamono-
decoderblockconsistsofamaskedself-attentionmechanism, tonicimprovementofthejointpolicy. Inthiswayweextend
amaskedcross-attentionmechanism,MLPsandsomeresid- PPOtomulti-agentsettingsmoothly. Havingconstructedthe
ual connections. The masked cross-attention mechanism is loss function for both the critic and the policy, the overall
used to integrate the encoded observation, where â€œmaskedâ€ learninglosscanbecomputedby:
indicatesthattheattentionmatrixisanuppertriangularma-
n
ptr ri ex ce en ds inu grin gg ent eh ra at tee dac ah cta iog ne snt aâ€™s j,ja <ct ii .on ai t depends only on its L(Î¸,Ï•,Ïˆ)=L critic+Î» 1L policy+Î» 2(cid:88) H(Ï€ Î¸i), (8)
t i=1
whereH(Ï€i)denotestheentropyoftheconditionalindivid-
4.3 JointPPOLoss Î¸
ual policy Ï€i, serving to prevent from early convergence to
Î¸
For network training, we use PPO algorithm to directly op- suboptimal solutions, and Î» ,Î» are the weight parameters.
1 2
timize the joint policy. To achieve this, an accurate ap- JointPPO is a fully centralized method as it operates on all
proximationofthejointobservationvaluefunctionV (o )is variables at the joint level, and that is also the origin of its
t
necessary, so we build the loss functions for the critic and name. ThecompletepseudocodeforJointPPOisprovidedin
the policy separately. For the criticâ€™s loss, We first use the supplementarymaterials.Table1:PerformanceEvaluationsofwinrateandstandarddeviationontheSMACtestbed.
Task Type Difficulty JointPPO MAT MAPPO HAPPO Steps
5m vs 6m Homogeneous Hard 89.06(0.03) 72.81(0.17) 85.62(0.05) 61.56(0.07) 1e7
8m vs 9m Homogeneous Hard 98.44(0.01) 97.81(0.02) 97.50(0.01) 65.31(0.03) 5e6
10m vs 11m Homogeneous Hard 99.69(0.00) 98.12(0.02) 93.75(0.03) 75.31(0.09) 5e6
27m vs 30m Homogeneous SuperHard 100.00(0.00) 95.63(0.04) 91.25(0.04) 0.00(0.00) 1e7
6h vs 8z Homogeneous SuperHard 92.5(0.04) 97.81(0.01) 77.50(0.17) 0.08(0.05) 1e7
MMM Heterogeneous Easy 99.69(0.01) 96.88(0.00) 97.50(0.02) 0.00(0.00) 2e6
3s5z Heterogeneous Hard 96.88(0.01) 92.19(0.05) 96.25(0.02) 31.56(0.18) 3e6
MMM2 Heterogeneous SuperHard 97.19(0.02) 88.44(0.08) 89.06(0.07) 0.01(0.01) 1e7
3s5z vs 3s6z Heterogeneous SuperHard 91.56(0.03) 95.63(0.03) 62.81(0.04) 88.12(0.07) 2e7
(a) Learningcurvesofwinrate.
(b) Learningcurvesofthenumberofkilledallies.
Figure4:ComparisonofJointPPOagainstbaselinesonfourSMACmaps.
5 Experiments in [Wang et al., 2020b], we compute the win rates over 32
evaluation games after each training iteration and take the
Inthissection,weevaluateJointPPOacrossvarioustasksin
median of the final ten evaluation win rates as the perfor-
theStarCraftMulti-AgentChallenge(SMAC)testbed.
manceforeachseed. However,evaluatingalgorithmssolely
5.1 SMACTestbed basedonwinratesisnotenough,asthereexitsituationsthat
twoalgorithmwithsamewinratesmaydifferintermsofthe
SMAC(StarCraftMulti-AgentChallenge)[Samvelyanetal.,
costpaidforthevictory, suchasthenumberofkilledallies.
2019]isatestbedforMARLthatoffersadiversesetofStar-
Therefore,wefurtherrecordthenumberofkilledalliesinthe
Craft II unit micromanagement tasks of varying difficulties.
evaluation game as an additional performance metric. More
In these tasks, a collaborative team of algorithm controlled
detailsarepresentedinsupplementarymaterials.
unitsneedtodefeatanenemyteamofunitscontrolledbythe
built-inAI.TheunitsinSMACarealsodiverse. Inhomoge-
5.2 JointPPOâ€™sPerformance
neoustasks,theunitscomprisingateamareofthesametype,
whereas heterogeneous tasks mean the opposite. Successful WepresenttheperformanceofJointPPOonseveralrepresen-
strategiesoftenrequireprecisecoordinationamongtheunits, tative tasks, covering both homogeneous and homogeneous
executingtacticssuchasfocusedattackorkitingtogainpo- settings. We compare JointPPO with PPO-based algorithms
sitionaladvantages. MAPPO, HAPPO, and SOTA algorithm MAT. We use the
Forourexperiments,weusegameversion4.6,andallthe samehyperparametersofthesebaselinealgorithmsfromtheir
evaluation results are averaged over 5 random seeds. For originalpaperstoensuretheirbestperformanceandfaircom-
eachrandomseed, followingtheevaluationmetricproposed parisons. The evaluation results and learning curves of win 0 0 0   0 0 0   0 0 0   0 0 0 
               
               
   
           
   
           
     H S R F K   
     H  H  0  0 S  S  H  H R  R  D  G F  F  Q  L D K  K  Q            /     L Q H           -  0  0 R  $  $ L Q  7  7 W  B 3  Z 3  L 2  W K B V D P H B S D U D P H W H U           Q       R          B       H       Q       W       U R S \ B O R V V           '  ,  5 Q H  D Y I  Q H D  G U X  V  R O  H W  P    2  2   2 U  U G  G  U H  G H U  H U  U
         &   O L S S L Q J  3 D U D P H W H   U                 ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
         ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
         ( Q   Y    L U R Q P H Q W  6   W H  
 S V
          H 
 
(a) (b) (c) (d)
Figure5:Resultsofablationexperiments.(a):Effectoftrainingepochsandclippingparameter.(b):ComparisonbetweenJointPPO,original
MATandMATwiththesamehyperparameterasJointPPO.(c):ComparisonofJointPPOwithdifferentweightedentropyloss.(d):Compar-
isonofJointPPOwithdifferentactiongenerationorder.
ratesarepresentedinTable1andFigure4(a). JointPPOex- rameter Î» (Eq. 8). A well tuned Î» is supposed to strike a
2 2
hibits competitive performance with baseline algorithms in balance between exploration and exploitation. Here, we in-
termsoffinalwinratesandsampleefficiency. Remarkably,it vestigatehowentropylosswithdifferentweightsaffectsthe
achievesnearly100%testwinratesacrossalltestmaps, in- trainingprocess.Figure5(c)showsthat,asmallerweightcan
cludingthesuperhardheterogeneoustaskMMM2,whichis result in faster convergence while a larger one does the op-
noteasilysolvedbyexistingmethods. Additionally,wesur- posite. However,whenitâ€™ssettoosmall,suchasanextreme
prisinglyobservethatJointPPOdemonstratesalowercostof case, zero, it brings a lower final win rate which indicates a
killedalliedforvictoryinmosttasks(Figure4(b)),indicating suboptimalconvergence. Therefore,inallofourexperiment,
betteroptimality,whichweattributetoitsdirectoptimization theweightÎ» issetasancompromiseas0.1.
2
ofthejointpolicy.
ActionGenerationOrder
5.3 AblationStudies Asmentionedearlierthatourproposedframeworkrequiresa
designatedactiongenerationorder.Intheaboveexperiments,
We conduct ablation experiments and analyses on factors
forconvenience, weusethedefaultordergivenbytheenvi-
thatâ€™s important in JointPPOâ€™s implementation including:
ronment. HerewestudyhowthisorderinfluencesJointPPO.
PPO training epochs and clipping parameter, entropy loss,
Weconductexperimentswithinverseorderandrandomorder
and action generation order. Each factor is studied through
compared to the default order. Figure 5(d) shows that these
aseriesofexperimentsonthesuperhardheterogeneoustasks
threesettingshavelittledifferenceinperformance,whichval-
MMM2.
idatesthatJointPPOisquiterobusttotheorderofagents.De-
PPOTrainingEpochsandClippingParameter spitethis,weareinterestedintheideaofintroducinglearned
Thesetwoparametersareamongthemostinfluentialhyper- dependency such as [Ruan et al., 2022a], so that the algo-
parametersaffectingJointPPOâ€™straining. Weconductexper- rithmcanautomaticallyadjustthegenerationorderbasedon
iments with different combination of these two parameters. thesituation,whichweleaveforfuturework.
TheresultsarepresentedinFigure5(a). Generally, thefinal
winratesarepositivelycorrelatedwithtrainingepochs,while 6 Conclusion
negatively correlated with the clipping parameter. However,
In this paper, we decompose the joint policy of the multi-
as the training epoch further increases, the training process
agent system into conditional probabilities and introduce a
will crush, which was observed during experiments. This
framework that solves MARL using sequence generation
trendvalidatestheexplanationin[Yuetal.,2022]thatgreater
model. By leveraging the Transformer model, the pro-
trainingepochsbringhighersampleefficiency,butmayhurt
posed CTCE algorithm, JointPPO, effectively handles high-
the optimality of convergence due to an overfitting on old
dimensionaljointactionspacesanddemonstratecompetitive
data. A larger clipping parameter may cause instability, as
performanceonSMAC.Forfuturework,weareinterestedin
seenintheexperimentwithepoch=5,clippingparameter=0.2.
extending this work with learned agentsâ€™ dependencies and
WefurtherconductexperimentsusingMATwiththesame
integratingwiththeprosperousdevelopmentsinsingle-agent
trainingepochsandclippingparameterasJointPPO.Wecom-
RLforfurtheradvancements.
pareitsperformancewithJointPPOandoriginalMATinFig-
ure5(b). TheresultsshowthatMATwiththesameparame-
References
tersperformworsethantheitsoriginalsettings,therebyelim-
inatingtheimpactofparametertuningontheJointPPOâ€™sim- [Bertsekas,2021] Dimitri Bertsekas. Multiagent reinforce-
provedperformance. ment learning: Rollout and policy iteration. IEEE/CAA
JournalofAutomaticaSinica,8(2):249â€“272,2021.
EntropyLoss
The entropy loss is a crucial component contributing to the [Chenetal.,2022] Yiqun Chen, Wei Yang, Tianle Zhang,
learning loss. Its strength is determined by the weight pa- ShiguangWu,andHongxingChang.Commander-soldiers
 H W D 5  Q L :  H W D 5  Q L :  H W D 5  Q L :  H W D 5  Q L :reinforcement learning for cooperative multi-agent sys- in marl via trust-region decomposition. arXiv preprint
tems. In 2022 International Joint Conference on Neural arXiv:2102.10616,2021.
Networks(IJCNN),pages1â€“7.IEEE,2022.
[Lietal.,2023] Chuming Li, Jie Liu, Yinmin Zhang,
[deWittetal.,2020] Christian Schroeder de Witt, Tarun Yuhong Wei, Yazhe Niu, Yaodong Yang, Yu Liu, and
Gupta, Denys Makoviichuk, Viktor Makoviychuk, Wanli Ouyang. Ace: Cooperative multi-agent q-learning
Philip HS Torr, Mingfei Sun, and Shimon Whiteson. with bidirectional action-dependency. In Proceedings of
Is independent learning all you need in the starcraft theAAAIconferenceonartificialintelligence,volume37,
multi-agentchallenge? arXivpreprintarXiv:2011.09533, pages8536â€“8544,2023.
2020.
[Liuetal.,2021] Bo Liu, Qiang Liu, Peter Stone, Ani-
[Dorrietal.,2018] AliDorri, SalilSKanhere, andRajaJu- mesh Garg, Yuke Zhu, and Anima Anandkumar. Coach-
rdak. Multi-agent systems: A survey. Ieee Access, player multi-agent reinforcement learning for dynamic
6:28573â€“28593,2018. team composition. In International Conference on Ma-
[Duetal.,2022] Wei Du, Shifei Ding, Chenglong Zhang, chineLearning,pages6860â€“6870.PMLR,2021.
andZhongzhiShi.Multiagentreinforcementlearningwith [Loweetal.,2017] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean
heterogeneous graph attention network. IEEE Transac- Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
tionsonNeuralNetworksandLearningSystems,2022. agentactor-criticformixedcooperative-competitiveenvi-
[Foersteretal.,2018] Jakob Foerster, Gregory Farquhar, ronments. Advancesinneuralinformationprocessingsys-
Triantafyllos Afouras, Nantas Nardelli, and Shimon
tems,30,2017.
Whiteson. Counterfactualmulti-agentpolicygradients. In [Mahajanetal.,2019] Anuj Mahajan, Tabish Rashid,
Proceedings of the AAAI conference on artificial intelli- Mikayel Samvelyan, and Shimon Whiteson. Maven:
gence,volume32,2018. Multi-agent variational exploration. Advances in neural
[Hanetal.,2020] RuihuaHan,ShengduoChen,andQiHao. informationprocessingsystems,32,2019.
Cooperative multi-robot navigation in dynamic environ- [Nguyenetal.,2020] Thanh Thi Nguyen, Ngoc Duy
ment with deep reinforcement learning. In 2020 IEEE Nguyen, and Saeid Nahavandi. Deep reinforcement
International Conference on Robotics and Automation learning for multiagent systems: A review of challenges,
(ICRA),pages448â€“454.IEEE,2020. solutions, and applications. IEEE transactions on
[Kaelblingetal.,1998] Leslie Pack Kaelbling, Michael L cybernetics,50(9):3826â€“3839,2020.
Littman, and Anthony R Cassandra. Planning and acting [Oliehoeketal.,2008] Frans A Oliehoek, Matthijs TJ
inpartiallyobservablestochasticdomains. Artificialintel- Spaan, and Nikos Vlassis. Optimal and approximate q-
ligence,101(1-2):99â€“134,1998. valuefunctionsfordecentralizedpomdps. JournalofAr-
[KakadeandLangford,2002] ShamKakadeandJohnLang- tificialIntelligenceResearch,32:289â€“353,2008.
ford. Approximately optimal approximate reinforcement [Rashidetal.,2020a] TabishRashid,GregoryFarquhar,Bei
learning. In Proceedings of the Nineteenth International Peng, and Shimon Whiteson. Weighted qmix: Expand-
ConferenceonMachineLearning,pages267â€“274,2002. ingmonotonicvaluefunctionfactorisationfordeepmulti-
[KraemerandBanerjee,2016] Landon Kraemer and agent reinforcement learning. Advances in neural infor-
Bikramjit Banerjee. Multi-agent reinforcement learning mationprocessingsystems,33:10199â€“10210,2020.
as a rehearsal for decentralized planning. Neurocomput- [Rashidetal.,2020b] Tabish Rashid, Mikayel Samvelyan,
ing,190:82â€“94,2016.
ChristianSchroederDeWitt,GregoryFarquhar,JakobFo-
[Kubaetal.,2021] Jakub Grudzien Kuba, Ruiqing Chen, erster, and Shimon Whiteson. Monotonic value function
Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, factorisationfordeepmulti-agentreinforcementlearning.
and Yaodong Yang. Trust region policy optimisation The Journal of Machine Learning Research, 21(1):7234â€“
in multi-agent reinforcement learning. arXiv preprint 7284,2020.
arXiv:2109.11251,2021. [Ruanetal.,2022a] Jingqing Ruan, Yali Du, Xuantang
[Kubaetal.,2022] Jakub Grudzien Kuba, Xidong Feng, Xiong,DengpengXing,XiyunLi,LinghuiMeng,Haifeng
Shiyao Ding, Hao Dong, Jun Wang, and Yaodong Zhang,JunWang,andBoXu.Gcs:graph-basedcoordina-
Yang. Heterogeneous-agent mirror learning: A contin- tionstrategyformulti-agentreinforcementlearning.arXiv
uum of solutions to cooperative marl. arXiv preprint preprintarXiv:2201.06257,2022.
arXiv:2208.01682,2022. [Ruanetal.,2022b] Jingqing Ruan, Linghui Meng, Xuan-
[Leeetal.,2007] Jae Won Lee, Jonghun Park, O Jangmin, tangXiong,DengpengXing,andBoXu. Learningmulti-
JongwooLee,andEuyseokHong. Amultiagentapproach agent action coordination via electing first-move agent.
to q-learning for daily stock trading. IEEE Transactions In Proceedings of the International Conference on Auto-
on Systems, Man, and Cybernetics-Part A: Systems and mated Planning and Scheduling, volume 32, pages 624â€“
Humans,37(6):864â€“877,2007. 628,2022.
[Lietal.,2021] WenhaoLi,XiangfengWang,BoJin,Junjie [Samvelyanetal.,2019] Mikayel Samvelyan, Tabish
Sheng,andHongyuanZha. Dealingwithnon-stationarity Rashid, Christian Schroeder De Witt, Gregory Farquhar,Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, [Wangetal.,2023b] Xihuai Wang, Zheng Tian, Ziyu Wan,
Philip HS Torr, Jakob Foerster, and Shimon Whiteson. Ying Wen, Jun Wang, and Weinan Zhang. Order mat-
The starcraft multi-agent challenge. arXiv preprint ters: Agent-by-agent policy optimization. arXiv preprint
arXiv:1902.04043,2019. arXiv:2302.06205,2023.
[Schrittwieseretal.,2020] Julian Schrittwieser, Ioannis [Wenetal.,2022] Muning Wen, Jakub Kuba, Runji Lin,
Antonoglou, Thomas Hubert, Karen Simonyan, Laurent WeinanZhang,YingWen,JunWang,andYaodongYang.
Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Multi-agent reinforcement learning is a sequence model-
Demis Hassabis, Thore Graepel, et al. Mastering atari, ingproblem. AdvancesinNeuralInformationProcessing
go, chess and shogi by planning with a learned model. Systems,35:16509â€“16521,2022.
Nature,588(7839):604â€“609,2020.
[Wuetal.,2020] Tong Wu, Pan Zhou, Kai Liu, Yali Yuan,
[Schulmanetal.,2015a] John Schulman, Sergey Levine, Xiumin Wang, Huawei Huang, and Dapeng Oliver Wu.
PieterAbbeel,MichaelJordan,andPhilippMoritz. Trust Multi-agent deep reinforcement learning for urban traffic
regionpolicyoptimization. InInternationalconferenceon lightcontrolinvehicularnetworks. IEEETransactionson
machinelearning,pages1889â€“1897.PMLR,2015. VehicularTechnology,69(8):8243â€“8256,2020.
[Schulmanetal.,2015b] John Schulman, Philipp Moritz, [Wuetal.,2023] PhilippWu,AlejandroEscontrela,Danijar
SergeyLevine,MichaelJordan,andPieterAbbeel. High- Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer:
dimensional continuous control using generalized advan- Worldmodelsforphysicalrobotlearning. InConference
tageestimation. arXivpreprintarXiv:1506.02438,2015. onRobotLearning,pages2226â€“2240.PMLR,2023.
[Schulmanetal.,2017] John Schulman, Filip Wolski, Pra- [Yuetal.,2022] ChaoYu,AkashVelu,EugeneVinitsky,Ji-
fulla Dhariwal, Alec Radford, and Oleg Klimov. Prox- axuanGao,YuWang,AlexandreBayen,andYiWu. The
imal policy optimization algorithms. arXiv preprint surprisingeffectivenessofppoincooperativemulti-agent
arXiv:1707.06347,2017. games. Advances in Neural Information Processing Sys-
tems,35:24611â€“24624,2022.
[Sonetal.,2019] Kyunghwan Son, Daewoo Kim, Wan Ju
Kang,DavidEarlHostallero,andYungYi. Qtran: Learn- [Zhangetal.,2022] Feiye Zhang, Qingyu Yang, and Dou
ingtofactorizewithtransformationforcooperativemulti- An. A leader-following paradigm based deep reinforce-
agentreinforcementlearning. InInternationalconference mentlearningmethodformulti-agentcooperationgames.
onmachinelearning,pages5887â€“5896.PMLR,2019. NeuralNetworks,156:1â€“12,2022.
[Sunehagetal.,2017] PeterSunehag,GuyLever,Audrunas
Gruslys, WojciechMarianCzarnecki, ViniciusZambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z
Leibo, Karl Tuyls, et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296,2017.
[Tampuuetal.,2017] Ardi Tampuu, Tambet Matiisen, Do-
rian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru,
Jaan Aru, and Raul Vicente. Multiagent cooperation and
competition with deep reinforcement learning. PloS one,
12(4):e0172395,2017.
[Vaswanietal.,2017] AshishVaswani,NoamShazeer,Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advancesinneuralinformationprocessingsystems,
30,2017.
[Wangetal.,2020a] JianhaoWang,ZhizhouRen,TerryLiu,
Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,
2020.
[Wangetal.,2020b] Tonghan Wang, Tarun Gupta, Anuj
Mahajan, Bei Peng, Shimon Whiteson, and Chongjie
Zhang. Rode: Learning roles to decompose multi-agent
tasks. arXivpreprintarXiv:2010.01523,2020.
[Wangetal.,2023a] JiaoWang,MingruiYuan, YunLi,and
Zihui Zhao. Hierarchical attention masterâ€“slave for het-
erogeneous multi-agent reinforcement learning. Neural
Networks,162:359â€“368,2023.A PseudocodeofJointPPO Table3:Differenthyper-parametersusedforJointPPOintheexper-
iments.
Algorithm2JointPPO
PPO PPO policyloss lrdecay
maps
Input: Number of agents N, actions generation order epochs clip coefficient strategy
(i ,...,i ),batchsizeB,episodesK,stepsperepisodeT.
1 N
5m vs 6m 15 0.05 5 linear
Initialize: Encoderâ€™s parameters Ï•, decoderâ€™s parameters Î¸,
8m vs 9m 15 0.1 5 linear
replaybufferB.
10m vs 11m 15 0.1 5 linear
1: fork =0,1,...,Kâˆ’1do
27m vs 30m 15 0.1 5 linear
2: Initializetheenvironmentandstartanepisode.
6h vs 8z 15 0.05 2 linear
3: fort=0,1,...,T âˆ’1do
MMM 15 0.2 5 linear
4: (//TheInteractionPhase)
5: Collectobservationso t =(cid:8) oi t(cid:9)n i=1. M3 Ms5 Mz 2 1 10 5 00 .. 02 5 5 5 expl oin ne ea nr tial
6: Input the collected observations to the joint pol-
3s5z vs 3s6z 10 0.1 2 linear
icy network. Get the predicted joint observation
value V (o ) and generated conditional local poli-
Ï• t
cies (cid:8) Ï€i(cid:0) ai|o ,a1:iâˆ’1(cid:1)(cid:9) according to the or- C DetailsofExperimentalResults
t t t i=1:n
der.
In this section, we present details of the experiment results,
7: Sampleagentsâ€™actionsandexecutethejointaction
a =
(cid:0) a1,...,ai(cid:1)
totheenvironment. Receivethe
including the training curves of win rates and number of
t t t killedalliesacrossalltestmapsinFigure6andFigure7. We
teamrewardr andstimulatetheenvironmenttothe
t alsopresentthedetailedresultsofablationstudyontheinflu-
nextstate.
8:
Inserttuple(cid:0)
o t,V Ï•(o
t),(cid:8) Ï€i(cid:9)
,a t,r
t(cid:1)
intoB.
e nn ac le wo infp rp ao tee ap no dc ah va en rd agc eli wpp inin rg ap tear fa om ree ate cr h. sW ete or fe pc ao rr ad mt eh te erfi s-
,
9: endfor
seeninTable4. Thefinalwinrateisthewinratedescribed
10: (//TheTrainingPhase)
abovewhichreflectstheoptimalityoftheconvergence,while
11: SamplearandomminibatchofBfromB.
theaveragewinrate,herewerefertothewinrateaveraged
12: CalculatethelossaccordingtoEq.(8)andupdatenet-
over the entire training process from scratch, which reflects
workparametersÏ•andÎ¸withgradientdescent.
thelearningrateandthesampleefficiency. Bothkindsofwin
13: endfor
rateareaveragedover5randomseeds.
Output: AtrainedTransformer-basedjointpolicynetwork.
Table4:PPOEpochsandClippingParameterAblations
Clipping
B Hyper-parameterSettingsforExperiments 0.05 0.1 0.15 0.2
Epochs
5 90.0(46.8) 85.3(55.6) 88.1(64.6) 73.1(51.4)
During experiments, the implementations of MAT, MAPPO
10 93.4(53.6) 94.1(66.9) 92.2(66.2) 91.3(68.1)
and HAPPO are consistent with their official repositories. 15 96.9(62.7) 92.8(67.1) 90.6(61.6) 88.1(60.7)
Here we list the hyper-parameter adopted in the implemen-
tationofJointPPOfordifferenttasksinTable2andTable3,
especiallyintermsoftheppoepochs,ppoclip,learningrate
decay strategy, and the coefficient parameter Î» of the PPO
1
loss,whichcorelatestoitsproportionintheoveralllearning
loss.
Table2:Commonhyper-parametersusedforJointPPOintheexper-
iments.
hyper-parameters value
learningrate 5e-4
batchsize 3200
discountfactor 0.99
entropycoef 0.01
hiddenlayernum 1
hiddenlayerdim 64
attentionblocknum 1
optimizer Adam
learningratedecay True
usevaluenormalization TrueFigure6:PerformancecomparisononSMACtasksintermsofwinrate.Figure7:PerformancecomparisononSMACtasksintermsofthenumberofkilledallies.