MAxPrototyper: A Multi-Agent Generation System for Interactive
User Interface Prototyping
MingyueYuan JieshanChen AaronQuigley
UniversityofNewSouthWales CSIROâ€™sData61 CSIROâ€™sData61
Australia Australia Australia
mingyue.yuan@unsw.edu.au Jieshan.Chen@data61.csiro.au Aaron.Quigley@data61.csiro.au
ABSTRACT UIdesigntodayhasadvancedsignificantly,forexamplewiththe
Inautomateduserinteractivedesign,designersfacekeychallenges, useoflow-fidelityvisualmodelssuchaswireframestohelpcreate
including accurate representation of user intent, crafting high- high-fidelityfunctionalprototypes[8].Inaddition,techniquessuch
quality components, and ensuring both aesthetic and semantic asSwire[16],Screen2Vec[20],andVINS[6]incorporatesketches
consistency.Addressingthesechallenges,weintroduceMAxPro- orscreenshotstoretrieveGUIs,whileGuigle[4]andRaWi[17]
totyper,ourhuman-centered,multi-agentsystemforinteractive furtherenhanceretrievability,thussimplifyingtheprototyping
designgeneration.ThecoreofMAxPrototyperisathemedesign process.However,theseretrievedGUIimagesoftensufferfrom
agent.Itcoordinateswithspecializedsub-agents,eachresponsible limitationsintermsofvisualfidelity,creativityandreusability.
forgeneratingspecificpartsofthedesign.Throughanintuitiveon- WidelyusedGUIprototypingtools,suchasSketch[28],Adobe
lineinterface,userscancontrolthedesignprocessbyprovidingtext XD[1],Figma[11],typicallyofferacombinationoffundamental
descriptionsandlayout.Enhancedbyimprovedlanguageandim- GUIcomponentsandtemplates.However,theirinabilitytogen-
agegenerationmodels,MAxPrototypergenerateseachcomponent eratecustomizedresultsalsoaffectstheirsupportforthecreative
withcarefuldetailandcontextualunderstanding.Itsmulti-agent processandefficiencyoftheoveralldesignprocess.Additionally,
architectureenablesamulti-roundinteractioncapabilitybetween whilegenerativemethodssuchaslayout2image[5],VAE[25],Mid-
thesystemandusers,facilitatingpreciseandcustomizeddesign Journey[22]andstablediffusion[27]showcaseremarkablecreative
adjustmentsthroughoutthecreationprocess. potential,theirpracticalimplementationishinderedbydifficultto
control.resultinginunstructuredandnon-editableoutcomes.The
CCSCONCEPTS needformanualreconstructionandthevaguenessofcomponents
occupyingsmallareaswithintheoverallimagegeneration,suchas
â€¢Human-centeredcomputingâ†’SystemsandtoolsforInter-
â€œtextâ€,â€œtextbuttonâ€,andâ€œiconâ€,furthercomplicatetheseissues.
actiondesign;Computingmethodologies.
Toaddressthelimitationsofnon-editableoutputs,enhancethe
KEYWORDS creativepotentialofretrieval-basedmethods,andguaranteehigh-
quality prototype generation, we present our novel interactive
Interactivedesign,userinterface,conversationalagents,largelan-
designsystemMAxPrototyperâ€”Multi-AgentCollaborationfor
guagemodels,imagegeneration
ExplainableUIPrototypeGeneration.Thissystemempowersde-
ACMReferenceFormat: signerstocrafthigh-fidelityprototypesthatarebothcustomizable
MingyueYuan,JieshanChen,andAaronQuigley.2024.MAxPrototyper:A andcomprehensibleateachstageofthegenerationprocess.
Multi-AgentGenerationSystemforInteractiveUserInterfacePrototyping. MAxPrototyperinitiatesthedesignjourneywithuser-provided
InExtendedAbstractsoftheCHIConferenceonHumanFactorsinComputing
inputssuchastextdescriptionsandawireframelayout.Thetext
Systems(CHIEAâ€™24).ACM,NewYork,NY,USA,5pages.
providesthegeneraldesignrequirement(e.g.,â€œStartingpagefor
1 INTRODUCTION MAxPrototyper:aintelligentdesignassistant.â€),andthewireframe
givesapreliminarylayoutoftheintendedUIdesign.
User interface (UI) design is an essential aspect of the modern
Specifically, this process draws inspiration from a top-down
softwareindustry,asitplaysasignificantroleinshapingtheoverall
designphilosophy.Itallowsoursystem,MAxPrototyper,tofirst
userexperience.WhiletraditionalUIdesignapproachesrequire
generateanoverallthemedesign,followedbytheiterativecreation
extensiveeffortandexpertise,numerousstudies[8,20,29,30,35]
ofeachindividualcomponents.Thisprocessensuresaestheticcon-
haveexploredandvalidatedthattheuseoftechniques,suchas
sistency throughout the design generation, thus maintaining a
deeplearning,canalleviatetheworkloadassociatedwithUIdesign.
coherentvisualstyleacrossallelementsoftheprototype.
Despitetheseadvancements,thereisstilladisconnectbetweenthe
Acorefeatureofoursystemistheabilityofacentralagent
toolsavailableandthedailypracticeofdesigners.
toutilizeacachepool,whichblendsaccumulatedresultstoeffec-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor tivelyguidethesub-agentsâ€™actions,thusmaintainingtheintegrity
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed ofthedesigncontext.Thisstrategyenablesaccuratemulti-round
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
interaction capabilities. Designers are thus provided with an it-
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, erativeframeworkthatsupportsanin-depthunderstandingand
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora customizableadjustmentsateachgenerationphase.
fee.Requestpermissionsfrompermissions@acm.org.
Bybalancingautomatedgenerationandcustomization,weaim
CHIEAâ€™24,May11â€“16,2024,Honolulu,HI,USA
Â©2024AssociationforComputingMachinery. tosupportastreamlineddesignprocessandtoolthatempowers
4202
yaM
21
]CH.sc[
1v13170.5042:viXraCHIEAâ€™24,May11â€“16,2024,Honolulu,HI,USA MingyueYuan,JieshanChen,andAaronQuigley
designerswithdeeperinsightsandgreatercontrol.Theresulting thisdatathroughScreen2Words[32],whichaugmentsRicotopro-
prototypescanbeconvenientlysavedinSVGorJSONformat,fur- vide112khigh-leveltextualdescriptionsforits22kUIscreenshots.
therempoweringdesignerswithpracticalefficiency. Throughthisdataset,weobtained<UIdescription>.
BeyondUIfunctionalitysemantics,designgenerationnecessi-
2 METHOD tatesthoughtfulconsiderationofthemes,colors,andthetarget
audience. We adopt Blip2 [19], which is a zero-shot visual lan-
guagemodel,togeneratethemedescriptionsviaavisualquestion-
answering approach. We identify four key attributes for theme
design: theme color, primary color, theme description, and app
category,andcreateaspecificquestion,pairingitwithaUIim-
age,andtheninputitintoBlip2toderivetheanswer,whichis
showninTable1.Finally,thesethreedescriptionsareconcatenated
together,andformUISemanticKnowledge:<textcontent/iconde-
scriptions><highleveldescription><themedesigndescription>.
2.1.2 IconKnowledgeBase. WecollectedthedatafromGoogle
Figure1:Overview:usinguserpromptandUILayoutasin- MaterialDesignIcons[14],ahigh-qualityrepositorythatstores
put,MAxPrototyperutilizesamulti-agentmethodforUI over900diverseicons(inSVGformatandwithtextdescription).
prototypecreation.Itintegratesfourprimaryagentsâ€”Theme WeobtainourIconKnowledgeBase:<iconSVGcode><semantic
Design,TextContent,ImageContent,andIconâ€”withTheme description>.
Designasthecentralguidingthesub-agents. Table1:Blip2â€™sVQAinstructiontemplatesforscreenshots
AsillustratedinFig.1,oursystemconsistsofknowledgebases
andfourkeyagents:ThemeDesignAgentğ´ ğ‘¡â„ğ‘’ğ‘šğ‘’,TextContent Attributes Blip2InstructionTemplates
Agentğ´ ğ‘¡ğ‘’ğ‘¥ğ‘¡,ImageContentAgentğ´ ğ‘–ğ‘šğ‘”andIconAgentğ´ ğ‘–ğ‘ğ‘œğ‘›.We T Prh ie mm ae ryC Col oo lr
or
â€œ â€œW Beh sia dt ei ss tt hh ee bb aa cc kk gg rr oo uu nn dd ,wco hlo ar tâ€™o sf thth eis dosc mre inen as nh to ct o? lâ€
orinthisimage?â€
collectknowledgebasestoimportrelevantdesignknowledge.Build- ThemeDescription â€œCanyoudescribethisscreenshotindetail?â€
AppCategory â€œWhichcategorydoesthisappbelongto?â€
inguponthisfoundation,MAxPrototyperprocessesuserinputs,
comprisingbothapromptandaUIlayout.Thesysteminitiates
2.2 ThemeDesignAgent
itsworkflowwiththeThemeDesignAgent,whichformulatesa
high-levelthemedescriptionandgeneratesacorrespondingtheme ğ´ ğ‘¡â„ğ‘’ğ‘šğ‘’ actsasthesupervisor,directingtheUIdesignprocess.Pow-
imagetoestablishacoherentandconsistentdesignnarrative. eredwithUIknowledgebasecollectedinSection2.1,itsetsthe
Servedasthemaindirector,ğ´ ğ‘¡â„ğ‘’ğ‘šğ‘’subsequentlyexecutespecial- generalstylebygeneratingtheglobalthemedescriptionandtheme
izedtasksthroughğ´ ğ‘¡ğ‘’ğ‘¥ğ‘¡,ğ´ ğ‘–ğ‘šğ‘”andğ´ ğ‘–ğ‘ğ‘œğ‘›forthedetailedcreation image.TheseinformationcanmakesurethegeneratedUIdesign
ofeachcomponent.Seamlesscoordinationbetweenthemainagent isofhighquality,coherentandconsistent.
andsupportingagentsisvitalforeffectiveness.Utilizingablend
2.2.1 KnowledgeRetrieval. Domain-specificknowledgeenhances
ofpreviousresultsandacachepool,thecentralagenteffectively
theaccuracyofLLM-generatedcontent.RecognizingtheLLMâ€™s
informsthesub-agents,ensuringthedesigncontextismaintained.
token input limitations and the complexities of fine-tuning, we
introduceaknowledgeretrievalphase,infusingdomain-specific
2.1 KnowledgeBaseConstruction
knowledgeintoourgenerationprocess.
Employingdomain-specificknowledgeenhancesthecreativityand Basedontheuserprompt(ğ¼ğ‘› ğ‘)andUIlayout(ğ¼ğ‘› ğ‘™),wewantto
outputqualityofLargeLanguageModels(LLMs).[21,33].Wecol- retrievethemostrelevantknowledgefromourlargeknowledge
lecttwoknowledgebases,namelyUIknowledge(pairsoftheme base. To do so, we concatenate these two information together,
descriptionswithlocalcomponentdescriptions)andIconKnowl- andencodethemintoavectorEmb(ğ¼ğ‘›)asthequeryvector,where
edge(PairsofSVGcodewithdescriptions)forğ´ ğ‘¡â„ğ‘’ğ‘šğ‘’ andğ´ ğ‘–ğ‘ğ‘œğ‘›. ğ¼ğ‘›=ğ¼ğ‘› ğ‘+ğ¼ğ‘› ğ‘™.WeusetheTEXT-EMBEDDING-ADA-002embedding
model[23]).Similarly,wealsoembedeachpieceofUIknowledge
2.1.1 UIKnowledgeBase.
relatedtoUICompositionandSemanticintoavector(Emb(ğ‘˜ğ‘ ğ‘—))
1)UICompositionKnowledge.RicoDataset[10]isoneofthe
aswell.Thenwecomputethecosinedistancebetweenqueryand
most comprehensive open-source UI datasets, including the UI
eachknowledge,andretrievethetop-kresultstoinstructourmulti-
screenshots,theirviewhierarchyinformationandtheirattributes
agentsystem1.Wedenotetheretrievedknowledgeasğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿ ğ‘–.
liketext,boundsandclass,andthecompositionoftheseUIelements.
Ourpreliminaryexperiments,alongsidefindingsbyBryan[31],
Weextractedclassandboundsfromthesemetadata,andformthe
indicatethatwhenemployingrelatedknowledgeasfew-shotprompt-
UIcompositionknowledge(<componenttypes><boundingboxes>)
ing,theinitialexampletendstobethemostinfluential.Subsequent
foreachUI.
examplesoftenprovidediminishingreturnsinfocusingthemodelâ€™s
2)UISemanticKnowledge.Thefine-grainedcomponentde-
output.Furthermore,giventheinputlengthlimitationsoflanguage
scriptioncanbeobtainedbyparsingcontent-descriptionfromthe
models,whichrestrictthenumberofexemplarsintheprompt,we
Ricodatasetmetadata.Weobtain<textcontent/icondescriptions>.
limitthenumberofreferencesto2.
WhileRicocontainsthemetadataofthecompositionandtext
contentsofUI,itlacksthehigh-levelUIdescription.Weobtain 1Wesetğ‘˜=2inourexperiments.MAxPrototyper:AMulti-AgentGenerationSystemforInteractiveUserInterfacePrototyping CHIEAâ€™24,May11â€“16,2024,Honolulu,HI,USA
2.2.2 Theme Description Generation. Given a user prompt thedesignatedpositionat[bbox].â€ Inalignmentwithequation2,the
ğ¼ğ‘› ğ‘, a UI layout ğ¼ğ‘› ğ‘™, and the top ğ‘˜ retrieved knowledge items executionpromptoftheğ´ ğ‘¡ğ‘’ğ‘¥ğ‘¡ obtainsitsvaluefromthecentral
w{(cid:205) itğ‘˜ ğ‘– h=0 thğ‘Ÿ eğ‘’ğ‘“ syğ‘’ğ‘Ÿ sğ‘– te} m(w â€™sh te hr ee m,ğ‘˜ e= de2 s) c, rt iph te is oe nc po rm ompo pn te ğ‘ƒn ğ‘¡â„ts ğ‘’ğ‘šar ğ‘’e tc oo fn oc ra mte un laa tt eed
a
a ng ae ten dtâ€™ wsc ita hch ğ‘e ğ‘¡ğ‘’, ğ‘¥d ğ‘¡e ,n toot ee nd sa us reğ¶ cğ‘ oğ‘ nâ„ğ‘’ siğ‘¡ sâˆ’ t1 e, na cn yd inis ss yu stb es meq .uentlyconcate-
comprehensiveinputforouragent.
Uponcompletingthethemedescriptiongeneration,theresultant 2.4 ImageContentAgent
themedescriptionisdenotedbyğ‘…ğ‘’ğ‘  ğ‘¡â„ğ‘’ğ‘šğ‘’. Toenhancethegenerationqualityoflocalimage-associatedcompo-
nentsandmaintaintheconsistency,wealsodeployouradaptively
2.2.3 ThemeImageGeneration. Theobjectiveofgeneratinga
themeimageistovisuallyguidetheoveralldesign,allowingthe
fine-tunedstablediffusionmodelinğ´ ğ‘–ğ‘šğ‘”,butdisablingControlNet
module.weusetheimagedescriptionfromthegeneratedthemede-
sub-agentstogeneratecoherentandconsistentdesign.
scriptionastheuserprompt(a).Ratherthanusingthelatentimage
WeconsiderStableDiffusionmodel[26],astate-of-the-arttext-
generatedfromGaussiannoise,weextracttheareaoftheimage
to-imagegenerationasourmainmodel.However,asthismodel
componentfromthethemeimageastheinput(b).Byharmonizing
onlyconsidersthetextcondition,itsuffersfromlimitedcontrol
bothtextualandvisualsignals,wecanguaranteethattheproduced
overthespatialcompositionoftheimage,acrucialaspectforour
contentalignsseamlesslywiththeprimarythemedesignintent.
UIdesigngeneration.Toaddressthis,weintegrateControlNet[34],
whichaugmentsthediffusionmodelbyprovidingenhancedspatial
2.5 IconAgent
controlovereachmodule.ControlNetcontrolsthegenerationby
manipulatingthedenoisingmodulebyimportingadditionalspatial ğ´ ğ‘–ğ‘ğ‘œğ‘› is crucial for selecting appropriate icons and integrating
conditioninUNet.WeemployUIlayoutasthespatialcondition. them into the graphical user interface components. In addition
Inaddition,asthestablediffusionmodelfacesobstacleswhen toactingasintuitivevisualcues,well-designediconscanimprove
generatingUIimages,whichrequiresadifferentdomainknowledge comprehensionandtheoveralluserexperience.Thesystemprompt
fromgeneralimages[9].Wefurtherfinetunethemodelusingthe forğ‘ ğ‘–ğ‘ğ‘œğ‘›,is:â€œInreferencetorelevantinformationandtakinginto
datasets of UI screenshots and their complementary high-level accountitspositioningat[bbox],andbasedonthethemedescription,
descriptionscollectedinSection2.1. proposeanindicativephraselikeâ€œmsgâ€fortheâ€œIconâ€..Asshown
inequation(2),executionpromptisbasedonthecentralagentâ€™s
2.2.4 Sub-agentExecution. Duringsub-agentexecutionphase,
cache,ğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡âˆ’1,combinedwithğ‘ ğ‘–ğ‘ğ‘œğ‘›.Thisapproachensuresthe
ThemeDesignAgentidentifiestheoptimalsub-agentcorrespond-
iconsselectedmatchtheGUIdesignsemanticallyandvisually.
ingtothecomponenttype.Weconsidered13componenttypes,as
detailedbytheRicodataset.Toelaborate,ğ´ ğ‘¡ğ‘’ğ‘¥ğ‘¡ handlesâ€œTextBut-
3 EXPERIMENT
tonâ€andâ€œTextâ€.ğ´ ğ‘–ğ‘šğ‘”isentrustedwithâ€œImageâ€andâ€œBackground
Imageâ€,andtheğ´ ğ‘–ğ‘ğ‘œğ‘› focusesonâ€œIconâ€components.Forother 3.1 ExperimentsSetup
componenttypes,weseektorenderthemeditable,drawingin-
FromSection2.1.1,weintotalcollectedasetof3,738UItextual
sightsfromRaWi[18].Thecolorforanycomponentisdetermined
descriptions,theircorrespondingwireframesandUIscreenshots.
byidentifyingthedominantRGBcolorfromtheimageregionâ€™s
Forvalidationpurpose,theUIscreenshotsaretreatedastheground
histogramandthenrepresentingitinHTMLcode.
truth.Itisessentialtonotethatthisdataisreservedwithinthe
Thedynamicbetweenthecentralagentandthesub-agentsises-
testsetfolderofRicoandhasnotbeenutilizedduringthemodelâ€™s
sentialtooursystemâ€™sfunctionality.Whenacentralagentengages fine-tuningphase.Imagesareresizedtodimensionsof512Ã—512.
asub-agent,itusesacombinationofpastresultsandacachepool
Toassessthequalityanddiversityofgeneratedresults,weutilize
toinformthesub-agentâ€™sprompts:
twometrics:FrÃ©chetInceptionDistance(FID)[15]andGeneration
ğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡ =ğ‘…ğ‘’ğ‘  ğ‘¡âˆ’1+ğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡âˆ’1 (1) Diversity(GD)[7],whichareusedintheimagegenerationtask[7,
12].FIDevaluatesthesimilaritybetweengeneratedresultsandreal
ğ‘ ğ‘¡+1=ğ‘ ğ‘ ğ‘¢ğ‘+ğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡ (2)
ones,whileGDassessesthediversityofthegeneratedprototypes.
Inthiscontext,ğ‘…ğ‘’ğ‘  ğ‘¡âˆ’1istheoutputfromthesub-agentforthe
ğ‘¡âˆ’1ğ‘¡â„ component.Theğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡ representsacachepoolthatinte- 3.2 ResultsandDiscussion
gratesthepreviousresultwithaccumulatedknowledgefromearlier
3.2.1 RQ1:Howdoesourfine-tunedmodelperformagainst
iterations.Thiscachepoolservesasanessentialmemoryfunction,
baselinemodelsinbothqualityanddiversity?
retainingthedesigncontextandfacilitatingmulti-turninteractions.
Baselines.ForRQ1,weconsidertwostate-of-the-artimagegen-
Meanwhile,ğ‘ ğ‘¡+1functionsasthepromptfortheğ‘¡âˆ’1ğ‘¡â„ componentâ€™s erationmodels:stable-diffusion-1-5[2]andstable-diffusion-2-
sub-agentinteraction,incorporatingboththespecificpromptğ‘
ğ‘ ğ‘¢ğ‘ 1[3]asthesetwobaselinesdonotincorporateControlNetmodule,
forthecurrentsub-agentandthecumulativeknowledgeinğ¶ğ‘ğ‘â„ğ‘’ ğ‘¡. weconsidervariantsofbothbyintegratingControlNet,denotedas
stable-diffusion-1-5(withControlNet),stable-diffusion-2-1
2.3 TextContentAgent
(withControlNet).Inaddition,wealsoemployanablatedversion
Theprimaryroleofğ´ ğ‘¡ğ‘’ğ‘¥ğ‘¡istogeneratetextualinformationtailored ofourapproach,MAxProtytper(w/oControlNet)asabaseline.
tospecificGUIcomponents.WeuseGPT-4[24].Thesystemprompt Results.AsseeninTable2,Forgenerationquality,weconsis-
forthisagent,representedasğ‘ ğ‘¡ğ‘’ğ‘¥ğ‘¡,is:â€œBasedonthethemedescrip- tentlyoutperformsbothbaselinemodelsintermsofFIDscores,
tionandrelevantdetails,provideatextcontentrecommendationfor regardlessofwhetherControlNetisutilized.Specifically,alowerCHIEAâ€™24,May11â€“16,2024,Honolulu,HI,USA MingyueYuan,JieshanChen,andAaronQuigley
Table2:ComparativeanalysisofFIDandGDscoresamong â€œKnowledgeRetrievalâ€moduleisessentialfordataimportation,
variousmodelswithandwithouttheControlNet withitsremovalincreasingtheFIDscorefrom23.76to42.56,sig-
nificantlyimpactingdesignquality.â€œThemeImageGenerationâ€is
Model FID GD vitalforlinkingtextandvisuals,seestheFIDscoreriseto33.08
whenomitted,underscoringitssignificanceinvisualintegration.
stable-diffusion-1-5(w/oControlNet) 69.48 15.93
Theâ€œSub-agentExecutionâ€phase,withspecializedagentsforeach
stable-diffusion-2-1(w/oControlNet) 67.15 15.42
designelement,demonstratesthevalueofadetailedgeneration
MAxPrototyper(w/oControlNet) 33.08 15.95
approachthroughminorFIDscorevariations.
stable-diffusion-1-5(withControlNet) 54.42 11.48 Inconclusion,â€œKnowledgeRetrievalâ€andâ€œThemeImageGen-
stable-diffusion-2-1(withControlNet) 57.23 11.14 erationâ€arethemostinfluential,butallmodulescollaboratively
MAxPrototyper 23.76 13.98 enhancethefinaloutput.
Table3:Resultsofablationstudyfordifferentmodules 4 DISCUSSIONANDFUTUREWORK
Ourcurrentwork,MAxPrototyper,focusesonimprovingthedesign
Method FID GD generation process and enhancing the quality of prototypes. It
MAxPrototyper 23.76 13.98 drawsinspirationfromarichknowledgebaseandprimarilyrelies
onhigh-leveluserdescriptionsandwireframelayoutstotranslate
-RetrievedKnowledgeItems 42.56 12.14
userintentintotangibledesignprototypes.Lookingahead,there
-ThemeDescriptionGeneration 28.43 11.77
areseveraldirectionsofimprovementsandexpansionsthatwecan
-ThemeImageGeneration 33.08 12.95
dointhefuture:
-TextContentAgent 24.06 13.78
1)AutomatedWireframeGeneration. Tofurtherstreamline
-ImageContentAgent 24.71 13.38
theuserexperience,wecouldofferautomaticgenerationofrelevant
-IconContentAgent 24.32 13.41
wireframesbasedontheuserâ€™shigh-leveldescriptions,thusremov-
ingoneextrastepfromtheuserâ€™sside.Inthecurrentiterationof
ourwork,wechosenottoincludethisfeature,becausewewantto
FIDscoresuggeststhatthedistributionofgeneratedimagesmore
focusonrefiningthegenerationprocessofthedesignprototypes
closelymatchesthatofrealimages.ThisindicatesthatMAxPro-
fromprovidedwireframesandhigh-leveldescriptions.Integrating
totyperâ€™soutputsaremorerealistic,evidentfromitssignificantly
automatedwireframecreationpresentsitsowndistinctchallenges,
reducedFIDscores:33.08withoutControlNetandanevenlower
particularlyinensuringthatthecreatedwireframesaccuratelyand
23.76withControlNet.
satisfactorilyreflecttheuserâ€™sdesignintent.However,recognizing
Fordiversity,Notably,GDscoresreflectthemodelâ€™sabilityto
itspotentialbenefits,weregarditasapotentialaspectforfuture
producevariedyetdetailedUIdesigns.AhigherGDindicatesmore
enhancement.
detail,suggestingthatthedesignsarediverseandintricateintheir
2)DynamicComponentIntegration.Currently,oursystem
presentation.MAxPrototyperâ€™sGDscores,bothwithandwithout
mainlydealswithstaticcomponents,whichwesimplyclassifyinto
ControlNet,surpassthoseofthebaselinemodels.Thisincreasein
text,images,andicons.Ourfutureworkcandiscussaboutexpand-
GDvaluesemphasizesourmodelâ€™ssuperiorcapabilityinproducing
ingthistoincludemoredynamicandinteractiveelements,such
designsthatarevariedandenrichedwithdetailscomparedtoits
ascheckboxesandpickers,thusmakingourgeneratedprototypes
peers.
morefunctionalandinteractive.
ForControlNet,IncorporatingControlNetresultsinnoticeable
3)ToolIntegrationandDesign-to-Code.Withanaimtoseam-
improvementsinFIDscoresforallmodels.ForourMAxPrototyper,
lesslyincorporateoursystemintodesignersâ€™workflow,weimagine
theFIDshowsanenhancementof10.68,representingasubstantial
MAxPrototyper could be developed as a plugin that integrates
32%improvement.
smoothly with popular wireframe sketching software, such as
Inconclusion,OurMAxPrototyperoutperformsthebaseline
Figma[11]andSketch[28].Thiswouldwidenoursystemâ€™sfunc-
modelsintermsofbothqualityanddiversity.TheadditionofCon-
tionalitybyprovidingdesignerswithrobusteditingcapabilities
trolNetoptimizesthisperformancefurtherbyintroducinglayout
directlywithintheseplatforms.Additionally,futureworkcouldalso
constraints,reinforcingitspotentialforgeneratingrealisticand
explorewaystoautomatedesign-to-codeconversiontoprovide
detail-orientedUIdesigns.
designersanddeveloperswithaend-to-endsolution.
3.2.2 RQ2:HowdoMAxPrototyperâ€™sindividualmodules 4) Usability and Accessibility. As we continue to improve
influenceitsperformanceinqualityanddiversity? MAxPrototyper,wearecommittedtoaugmentingoursystemwith
BaselinesandAblationStrategy.Tobettercomprehendthe standardmobiledeviceuserinterfaceguidelineslikematerialde-
interactionandindividualimpactofMAxPrototyperâ€™scomponents sign[13].Theseimprovementswillfurtherenhanceoursystemâ€™s
ontheendresult,weperformanablationstudy,sequentiallyre- usabilityandaccessibility,ensuringourprototypesnotonlylook
movingeachmoduleandevaluatingtheeffect.AsseeninTable3, appealingbutalsoofferintuitiveanduser-friendlyinteractions.
wecarefullycraftedsixablations. Byfocusingonthesepotentialdirectionsandcontinuallyrefin-
Results.MAxPrototyperâ€™sanalysishighlightseachmoduleâ€™s ingourprocessesbasedonuserfeedback,wecanbettercombine
critical role in achieving high-quality, diverse UI designs. The automationefficiencywithcreativefreedom.MAxPrototyper:AMulti-AgentGenerationSystemforInteractiveUserInterfacePrototyping CHIEAâ€™24,May11â€“16,2024,Honolulu,HI,USA
REFERENCES
[31] BryanWang,GangLi,andYangLi.2023.Enablingconversationalinteractionwith
[1] Adobe.2023.AdobeXD.https://adobexdplatform.com/. mobileuiusinglargelanguagemodels.InProceedingsofthe2023CHIConference
[2] StabilityAI.2022.stable-diffusion-v1-5.https://huggingface.co/runwayml/stable- onHumanFactorsinComputingSystems.1â€“17.
diffusion-v1-5. [32] B.Wang,G.Li,X.Zhou,Z.Chen,andY.Li.2021. Screen2Words:Automatic
[3] StabilityAI.2023.stable-diffusion-2-1.https://huggingface.co/stabilityai/stable- MobileUISummarizationwithMultimodalLearning.(2021).
diffusion-2-1. [33] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,
[4] C.Bernal-Cardenas,K.Moran,M.Tufano,Z.Liu,L.Nan,Z.Shi,andD.Poshy- LinxiFan,andAnimaAnandkumar.2023.Voyager:Anopen-endedembodied
vanyk.2019.Guigle:AGUISearchEngineforAndroidApps.ACM(2019). agentwithlargelanguagemodels.arXivpreprintarXiv:2305.16291(2023).
[5] SanketBiswas,PauRiba,JosepLladÃ³s,andUmapadaPal.2021. Docsynth:a [34] LvminZhangandManeeshAgrawala.2023. AddingConditionalControlto
layoutguidedapproachforcontrollabledocumentimagesynthesis.InDocument Text-to-ImageDiffusionModels. arXiv:2302.05543[cs.CV]
AnalysisandRecognitionâ€“ICDAR2021:16thInternationalConference,Lausanne, [35] Tianming Zhao, Chunyang Chen, Yuanning Liu, and Xiaodong Zhu. 2021.
Switzerland,September5â€“10,2021,Proceedings,PartIII.Springer,555â€“568. GUIGAN:LearningtoGenerateGUIDesignsUsingGenerativeAdversarialNet-
[6] S.Bunian,K.Li,C.Jemmali,C.Harteveld,andM.S.El-Nasr.2021.VINS:Visual works. arXiv:2101.09978[cs.HC]
SearchforMobileUserInterfaceDesign.(2021).
[7] N.Cao,X.Yan,Y.Shi,andC.Chen.2019. AI-Sketcher:ADeepGenerative
ModelforProducingHigh-QualitySketches.AssociationfortheAdvancementof
ArtificialIntelligence(AAAI)(2019).
[8] JieshanChen,ChunyangChen,ZhenchangXing,XinXia,LimingZhu,John
Grundy,andJinshuiWang.2020.Wireframe-BasedUIDesignSearchthrough
ImageAutoencoder. 29,3,Article19(jun2020),31pages. https://doi.org/10.
1145/3391613
[9] JieshanChen,MulongXie,ZhenchangXing,ChunyangChen,XiweiXu,Liming
Zhu,andGuoqiangLi.2020.ObjectDetectionforGraphicalUserInterface:Old
FashionedorDeepLearningoraCombination? CoRRabs/2008.05132(2020).
arXiv:2008.05132 https://arxiv.org/abs/2008.05132
[10] BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,
YangLi,JeffreyNichols,andRanjithaKumar.2017.Rico:AMobileAppDataset
forBuildingData-DrivenDesignApplications.InProceedingsofthe30thAnnual
SymposiumonUserInterfaceSoftwareandTechnology(UISTâ€™17).
[11] Figma.2023.Figma.https://www.figma.com/.
[12] S.Ge,V.Goswami,C.L.Zitnick,andD.Parikh.2020.CreativeSketchGeneration.
(2020).
[13] Google.2023.MaterialDesign.https://material.io/.
[14] Google.2023.MaterialIcons.https://github.com/google/material-design-icons.
[15] M.Heusel,H.Ramsauer,T.Unterthiner,B.Nessler,andS.Hochreiter.2017.GANs
TrainedbyaTwoTime-ScaleUpdateRuleConvergetoaLocalNashEquilibrium.
(2017).
[16] F.Huang,J.F.Canny,andJ.Nichols.2019.Swire:Sketch-basedUserInterface
Retrieval.Inthe2019CHIConference.
[17] KristianKolthoff,ChristianBartelt,andSimonePaoloPonzetto.2023.Correction
to:Data-drivenprototypingvianatural-language-basedGUIretrieval.Automated
SoftwareEngineering30,1(2023).
[18] KristianKolthoff,ChristianBartelt,andSimonePaoloPonzetto.2023. Data-
drivenprototypingvianatural-language-basedGUIretrieval.AutomatedSoftware
Engineering30,1(2023),13.
[19] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023.Blip-2:Bootstrapping
language-imagepre-trainingwithfrozenimageencodersandlargelanguage
models.arXivpreprintarXiv:2301.12597(2023).
[20] J.J.Li,L.Popowski,T.Mitchell,andB.A.Myers.2021.Screen2Vec:Semantic
EmbeddingofGUIScreensandGUIComponents.(2021).
[21] PanLu,BaolinPeng,HaoCheng,MichelGalley,Kai-WeiChang,YingNianWu,
Song-ChunZhu,andJianfengGao.2023.Chameleon:Plug-and-playcomposi-
tionalreasoningwithlargelanguagemodels. arXivpreprintarXiv:2304.09842
(2023).
[22] MidJourney.2023.MidJourney.https://www.midjourney.com.
[23] OpenAi.2023. Embeddings-OpenAIAPI. https://platform.openai.com/docs/
guides/embeddings.
[24] OpenAi.2023.GPT4-OpenAIAPI.https://platform.openai.com/docs/models/gpt-
4.
[25] AliRazavi,AaronVandenOord,andOriolVinyals.2019.Generatingdiverse
high-fidelityimageswithvq-vae-2.Advancesinneuralinformationprocessing
systems32(2019).
[26] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rn
Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.
10684â€“10695.
[27] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rn
Ommer.2022. High-resolutionimagesynthesiswithlatentdiffusionmodels.
2022IEEE.InCVFConferenceonComputerVisionandPatternRecognition(CVPR).
10674â€“10685.
[28] Sketch.2023.Sketch.https://sketch.io/.
[29] Y.Su,Z.Liu,C.Chen,J.Wang,andQ.Wang.2021. OwlEyes-Online:AFully
AutomatedPlatformforDetectingandLocalizingUIDisplayIssues.
[30] AmandaSwearnginandYangLi.2019. Modelingmobileinterfacetappability
usingcrowdsourcinganddeeplearning.InProceedingsofthe2019CHIConference
onHumanFactorsinComputingSystems.1â€“11.