GraSSRep: Graph-Based Self-Supervised Learning for Repeat
Detection in Metagenomic Assembly
Ali Azizpour1, Advait Balaji2, Todd J. Treangen2, and Santiago Segarra1
1 Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA
{aa210,segarra}@rice.edu
2 Department of Computer Science, Rice University, Houston, TX, USA
{advait,treangen}@rice.edu
Abstract. RepetitiveDNA(repeats)posessignificantchallengesforaccurateandefficientgenomeas-
semblyandsequencealignment.Thisisparticularlytrueformetagenomicdata,wheregenomedynamics
such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome as-
sembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these
challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly
graph‚Äôs structure through graph neural networks (GNNs) within a self-supervised learning framework
toclassifyDNAsequencesintorepetitiveandnon-repetitivecategories.Specifically,weframethisprob-
lemasanodeclassificationtaskwithinametagenomicassemblygraph.Inaself-supervisedfashion,we
rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of
thenodes.Wethenusethosepseudo-labelstotrainaGNNembeddingandarandomforestclassifierto
propagatethelabelstotheremainingnodes.Inthisway,GraSSRepcombinessequencingfeatureswith
pre-definedandlearnedgraphfeaturestoachievestate-of-the-artperformanceinrepeatdetection.We
evaluateourmethodusingsimulatedandsyntheticmetagenomicdatasets.Theresultsonthesimulated
data highlight our GraSSRep‚Äôs robustness to repeat attributes, demonstrating its effectiveness in han-
dlingthecomplexityofrepeatedsequences.Additionally,ourexperimentswithsyntheticmetagenomic
datasets reveal that incorporating the graph structure and the GNN enhances our detection perfor-
mance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with
respect to precision and recall.
Keywords: Metagenomics ¬∑ Repeat detection ¬∑ Graph neural network ¬∑ Self-supervised learning
This work was supported by the NSF under award EF-2126387.
4202
beF
41
]GL.sc[
1v18390.2042:viXra2 A. Azizpour et al.
1 Introduction
Metagenomicsisascientificdisciplinethatinvolvesanalyzinggeneticmaterialobtainedfromcomplexuncul-
tured samples housing DNA from diverse organisms [36]. This field utilizes high-throughput sequencing and
bioinformatictechniquestocharacterizeandcomparethegenomicdiversityandfunctionalpotentialofentire
microbial communities without the need for isolating and culturing individual organisms [38]. The result-
ing data can provide insights into the ecological roles and evolutionary relationships of the microorganisms
present in the sample [29].
However,thesequencingofDNAfromsuchsamplesposesuniquechallenges.Oneofthemajorchallenges
inthemetagenomicassemblyisthepresenceofrepeats[11,25],whicharesequencesofDNAthataresimilar
or identical to sequences elsewhere in the genome [35]. The challenges posed by repeats in isolated genomes
haveprimarilybeenaddressedthroughtheuseoflong-readtechnologies[21].However,metagenomicspresents
a more complex problem as microbial mixtures often contain multiple closely related genomes that differ
in just a few locations due to structural variants, such as horizontal gene transfer [33], gene duplication,
and gene loss/gain [17]. Reads spanning the length of individual strains are required to fully resolve these
genome-scale repeats present in microbiomes.
These repetitive elements, while natural and abundant in genomes, complicate the process of genome
assembly and comparison [34]. They intricately tangle the assembly graph, making it difficult to distinguish
theorder,orientation,andcopynumbervariationofgenomescomprisingthemicrobiomeunderstudy,result-
ing in fragmented assemblies. Moreover, repeats introduce ambiguities for comparative genomics, hindering
differentiation between identical or similar regions and complicating the understanding of gene functions,
regulatory elements, and their role in genetic disorders [35]. To overcome these obstacles, precise identifica-
tion and annotation of repeated sequences is necessary. Unraveling the complexities of repeated sequences
is not only crucial for enhancing genome assembly but also essential for deciphering intricate regulatory
mechanisms and evolutionary processes. Indeed, identifying these repeats is foundational for understanding
genome stability, gene expression, and disease susceptibility, making the development of accurate repeat
detection methods vital for advancing genomic research [12].
Graphs are powerful tools for visualizing complex relationships
between various objects, such as DNA sequences. Graph-based al-
gorithms can effectively represent the interconnections and overlap-
ping patterns within genomes [23], where the nodes in the graph
represent unique DNA sequences. Due to the tangled nature of re-
peatedsequenceswithintheassemblygraph,exploitinggraphstruc-
ture becomes particularly advantageous. As an illustrative example,
Figure 1 portrays the assembly graph obtained from a simulated
metagenome with two organisms. In this scenario, three random se-
quences are generated. Two of these sequences are inserted as intra-
genome repeats in each organism, while the third one is inserted in
both organisms, serving as an inter-genome repeat. The graph re-
veals that repeats (especially inter-genome repeats) are represented
by central and well-connected nodes, indicating the potential of uti-
lizing the inherent graph structure in genomic data for identifying
repeated sequences. A node labeled as a repeat represents a unique Fig.1. Repeat positions within the as-
DNA sequence that occurs in several positions of the metagenomic sembly graph.
sample. Notice, however, that graph structure is not enough to tell
apart the yellow nodes from some of the blue ones. This motivates
an approach that combines graph features with sequencing information such as read coverage or length of
the DNA sequence.
Previous studies have employed pre-specified graph features in combination with machine learning tech-
niques to address the challenge of detecting repeats, treating it as a node classification problem [9, 10]. In
this context, the nodes of the graph represent DNA sequences, and the objective is to classify them into
repeatsandnon-repeats.However,giventhevastamountofgenomicdata,thereremainsampleopportunity
for enhancement through learning discriminative graph features. One of the promising ways to achieve this
is by employing graph neural networks (GNNs) [37]. GNNs have the unique ability to learn distinctive andGraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 3
valuable features for the nodes within the graphs. Unlike predefined features, GNNs generate these charac-
teristicsthroughtrainableiterativecomputations,makingthemadaptivetothespecificdata.Thesefeatures
haveshownpromisingresultsinmanyotherfields[13,5,40,4],emphasizingtheefficiencyofutilizingGNNs
to classify nodes accurately and uncover the complexities within large graphs [15].
However, one of the primary challenges in genomic data analysis is the fact that most of the data is
unlabeled, particularly in distinguishing between repeat and non-repeat sequences. This characteristic of
the data prevents the application of supervised or semi-supervised learning techniques for classifying DNA
sequences [20]. In the absence of labeled data points offering insights into each class, these conventional
methods become ineffective. To overcome this issue, self-supervised learning emerges as a natural and pow-
erful alternative to leverage the vast unsupervised data [18]. In self-supervised learning, specific data points
(nodes) are initially given (potentially noisy) labels. Subsequently, machine learning algorithms are em-
ployed, coupled with fine-tuning steps, to refine the model‚Äôs performance. This approach ensures the ability
to classify data points without requiring access to their true labels.
In this paper, we propose GraSSRep, a novel graph-based algorithm to identify and detect the repeated
sequences in the metagenomic assembly in a self-supervised manner. Our contributions are threefold:
1) By leveraging GNNs, we devise the first method that learns (rather than pre-specifies) graph features for
repeat detection.
2) We establish the first algorithm that uses self-supervised learning for repeat detection. In this way, we
leverage existing methods to generate noisy labels that we then refine and expand using our learnable
architecture.
3) Through numerical experiments, we demonstrate the robustness of our methodology, the value of its
constituent parts, and the performance gain compared with the state of the art.
An implementation of GraSSRep, along with code to reproduce our results in Section 4, can be found at
https://github.com/aliaaz99/GraSSRep.
2 Methods
Given paired-end reads, our goal is to identify repeated DNA sequences in the metagenome (see Section 3.2
fortheprecisecriteriausedtodefinearepeat).Anoverviewofourmethodspecificallydesignedforthistask
is illustrated in Figure 2. In the subsequent sections, we provide a detailed explanation of each step involved
in the pipeline.
2.1 Step 1: Unitig graph construction
In the initial step, we construct a unitig graph in order to leverage graph features for repeat detection.
Inspired by KOMB [2], we generate our own graph instead of using existing assemblers. We do this due
to the tendency of most assemblers to simplify assembly graphs for improved assembly quality, potentially
leading to the loss of some sequences. Our approach retains all sequences on the graph, facilitating the
detection of potential repeats.
First,allreadsareassembledintounitigsbyfeedingthemintothedeBruijngraphconstructorABySS[32].
UnitigsareobtainedbytraversingthedeBruijngraph,andthemaximalconsensussequencesthatterminate
at branches caused by repeats or variants are regarded as unitigs. We consider these unitigs as the nodes V
of our unitig graph, where |V|=N.
To form the edges, we map the reads back to the unitigs using Bowtie2 [24]. Based on this, we define
two types of edges between the unitigs using the information from the read-mapping process. The first set,
referredtoasadjacencyedges,capturespotentiallyneighboringunitigsinthegenome.Thesecondset,which
we call repeat edges, provides additional relational information for repeat detection. Specifically, for a given
forward and reverse read pair, denoted as r and r respectively, where r is mapped to a set of unitigs
1 2 1
called U , and r is mapped to a set of unitigs called U , we connect all unitigs in U to all unitigs in U
1 2 2 1 2
using adjacency edges. Additionally, we establish connections between all unitigs within each set of U and
1
U using repeat edges. Furthermore, we assign weights to the edges, given by the number of reads mapping
2
tothecorrespondingunitigs.Wemergethesetwoedgetypestocreateasingleedgeconnectingtheunitigsin
our graph. The sum of the weights from both repeat and adjacency edges determines the final edge weight.4 A. Azizpour et al.
(a) Steps 1 and 2 (b) Step 3
1 3 5 7 1 3 5 7
0 9 0 9
Assembly 2 4 6 8 2 4 6 8
Repeats ( )
Non-repeats( )
Read pairs Pseudo-labels Unlabeled ( )
(c) Step 4 (d) Step 5
Embeddings learned by ‚Ñé!!
0 0 2 3 0 2
1 7 3
6
2 8 6 7
9 8
3
‚Ñé!! ùëç ùëì !" 4 R Fa on rd eo sm t 9
5
:ùëî 6 1 4 1 4
" 7 5 5
8
9
Fig.2.OverviewofGraSSRep.(a)Readsareassembledintounitigsformingthenodesoftheunitiggraph.Edgesare
constructed based on the read mapping information. Also, feature vectors are computed for each unitig. (b) Unitigs
with distinctive sequencing features are selected as training nodes and labeled. (c) The unitig graph is input into a
GNN. Embeddings are generated for each unitig and combined with the initial features. A random forest classifier
predictslabelsforallunitigsbasedontheaugmentedfeaturevectors.(d)Sequencingfeaturesareemployedtoidentify
outliers within each predicted class, leading to the reassignment of their class labels.
Wethendeletetheedgeswithweightsinthelowestquartiletofocusonlyonthoserelationsbetweenunitigs
that have a strong presence in the reads. For the remainder of our method, we ignore the weights of the
non-deleted edges so that we work with an unweighted graph (except for the computation of the weighted
degree in Step 2). We denote by A ‚àà {0,1}N√óN the adjacency matrix of the corresponding unweighted
graph, where A =1 if there is an edge between unitigs i and j, and A =0 otherwise.
ij ij
2.2 Step 2: Feature extraction
We compute features of the unitigs that are informative in determining which unitigs are repetitive. We
consider two types of features: sequencing and graph-based. Sequencing features (unitig length and mean
coverage) are obtained during the sequencing process before constructing the unitig graph and used in
Steps 3 and 5. In addition, we incorporate five graph-based features that are widely used in the literature:
betweennesscentrality,k-corevalue,degree,weighteddegree,andclusteringcoefficient.Previousstudieshave
emphasized the significance of betweenness centrality [30] in identifying repeats [9]. Additionally, KOMB [2]
has underscored the crucial role of the k-core value in anomaly detection within unitigs. Furthermore, the
simple and weighted degree of nodes indicates their connectivity strength with other unitigs, aiding in the
identification of repeated regions. We also consider the clustering coefficient due to its substantial impact
on node classification tasks, as well as its demonstrated positive effects and favorable outcomes in various
related domains [39]. We store the graph-based features in a matrix X ‚àà RN√ó5, where every row contains
the five graph-based features of a given node (unitig) in the graph. Thus, we define our featured graph of
interest G =(V,A,X).
2.3 Step 3: Selection of the training nodes
Recall that we do not have any prior information (labels) on whether any unitig is a repeat or not. In this
context, the idea of self-supervised learning is first to do a high-confidence classification of a subset of the
unitigs (assigning potentially noisy labels, denominated pseudo-labels, to a subset of the nodes) and then
use those nodes as a training set for a machine learning model that can classify the remaining unitigs. WeGraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 5
generate this set of pseudo-labels using the sequencing features from Step 2. In generating pseudo-labels, it
is important only to consider those for which we have a high level of confidence, so that the training process
based on these pseudo-labels is reliable.
Indefiningourpseudo-labels,werelyonthefactthatshorterunitigswithhighercoveragearehighlylikely
toberepetitive,whileverylongunitigswithlowercoveragearemorelikelytobenon-repeatunitigs[9].More
precisely,letusdefineasxlen andxcov thelength(numberofbasepairs)andcoverage(meannumberofreads
i i
mappedtothebasepairsintheunitig)ofnodei,respectively.Wesetapercentilep(with0‚â§p‚â§50)based
on which we define the following thresholds: œÑlen is the p-th percentile of the lengths among all unitigs in V,
low
œÑlen is the (100‚àíp)-th percentile of the lengths among all unitigs, and œÑcov is the (100‚àíp)-th percentile of
high
the coverages among all unitigs. Based on these thresholds, we divide the unitigs into three sets, the repeats
R, the non-repeats N, and the unlabeled U, as follows
R={i‚ààV|xlen <œÑlen ‚àß xcov >œÑcov}, N ={i‚ààV|xlen >œÑlen ‚àß xcov <œÑcov}, (1)
i low i i high i
and U =V\(R‚à™N). In (1), unitigs shorter than the lower length threshold and with a coverage surpassing
the coverage threshold are included in the training set with a repeat pseudo-label (R). Conversely, unitigs
exceeding the higher length threshold and having a coverage below the coverage threshold are added to the
trainingsetwithanon-repeatpseudo-label(N).Ifaunitigdoesnotmeetanyoftheseconditions,itsuggests
that sequencing features alone are not sufficient to determine its classification. Consequently, these unitigs
are not included in the training set (U).
2.4 Step 4: Unitig classification via self-supervised learning
Weleverageself-supervisedlearningbytrainingagraph-basedmodelonR(binarylabelof1)andN (binary
label of 0) and use that model to classify the nodes in U.
Consider the graph G =(V,A,X) generated in Steps 1 and 2 and denote by g a GNN parameterized by
Œ∏
Œ∏ [37]. This GNN takes the graph structure A and the node features X as input and produces labels yÀÜ
GNN
for the nodes at the output. To generate these labels, g can be viewed as an end-to-end network that is
Œ∏
structured as follows
yÀÜ =g (X,A)=f (h (X,A)), (2)
GNN Œ∏ Œ∏2 Œ∏1
where h consists of graph convolutional layers followed by an activation function [1].1 Each layer in h
Œ∏1 Œ∏1
generates new observations for every node based on its neighboring nodes. These convolutional layers are
succeeded by f , which represents a fully connected neural network [16]. The purpose of this network is to
Œ∏2
predict the final label for each node based on the features derived from the last layer of h .
Œ∏1
We denote the output of the convolutional layers by Z = h (X,A) ‚àà RN√ód, where d is a pre-specified
Œ∏1
embedding dimension. The i-th row z of Z represents new features for unitig i, learned in such a way that
i
the final linear layer, f , can predict the class of the unitigs based on these features. These embeddings
Œ∏2
enableustoachieveourobjectiveofunderstandingthegraph-basedcharacteristicsofrepeatandnon-repeat
unitigs. Notice that the features in z not only depend on graph features of node i but also on the features
i
of its local neighborhood through the aggregation of the trainable convolutional layers in h .
Œ∏1
In order to learn the parameters Œ∏ ={Œ∏ ‚à™Œ∏ }, the GNN undergoes an end-to-end training based on the
1 2
pseudo-labels R and N identified in Step 3. This training process involves minimizing a loss function that
compares the predicted labels yÀÜ with the pseudo-labels
GNN
(cid:88) (cid:88)
Œ∏‚ãÜ =argmin L([yÀÜ (Œ∏)] ,1)+ L([yÀÜ (Œ∏)] ,0), (3)
GNN i GNN i
Œ∏
i‚ààR i‚ààN
where L represents a classification loss (such as cross-entropy loss [7]) and we have made explicit the depen-
dence of yÀÜ with Œ∏. In essence, in (3) we look for the GNN parameters Œ∏‚ãÜ such that the predicted labels
GNN
for the nodes in R are closest to 1 while the predicted labels for the nodes in N are closest to 0. Intuitively,
the intermediate embeddings Z obtained using the optimal parameters Œ∏‚ãÜ encode learning-based features
1 We provide here a generic functional description of our methodology whereas in Section 3.3 we detail the specific
architecture used in the experiments.6 A. Azizpour et al.
relevant for the classification beyond the pre-defined ones in Step 2. Thus, we construct the augmented fea-
ture matrix X¬Ø =[X,Z]‚ààRN√ó(5+d) by concatenating the initial graph-based features with those generated
by the GNN.
Arandomforest(RF)classifieristhentrainedonthepseudo-labelsR‚à™N havingtheaugmentedfeatures
X¬Ø as input. The RF is trained by creating multiple decision trees from different subsets of the dataset (a
processknownasbootstrapping),witheachtreeusingarandomsubsetoffeatures.Whenmakingpredictions,
theindividualtrees‚Äôoutputsarecombinedthroughmajorityvoting,producingareliableandpreciseensemble
model [3]. The RF classifier combines the explanatory power of the original graph-based features X found
to be relevant in previous works with the learning-based features Z to generate the predicted labels yÀÜ .
RF
Notice that the sequencing features xlen and xcov are not used in computing yÀÜ other than in the
RF
generation of the pseudo-labels. If we were to include these features as inputs to the RF, then the classifier
can simply learn the conditions in (1) and obtain zero training error by ignoring all the graph features. This
would directly defeat the purpose of our self-supervised framework. Instead, the current pipeline can distill
thegraph-basedattributesassociatedwithrepeatsandnon-repeats,enablingustogeneralizethisknowledge
to classify other unitigs effectively.
2.5 Step 5: Fine-tuning the labels
Inthefinalstepofourmethod,weenhancetheperformanceofourpredictionsthroughafine-tuningprocess.
Wefirstassignthepseudo-labelsofthetrainingnodesinRandN astheirfinalpredictedlabels.Ourprimary
focus is then directed toward the non-training unitigs in U. These unitigs have been classified by the RF
in Step 4 relying solely on their graph-based features and embeddings learned by the GNN. At this point,
reconsidering sequencing features becomes crucial, as they hold valuable information that can significantly
contribute to determining the accurate labels of the unitigs.
To do so, we divide the unitigs in U into two disjoint sets: those predicted as repeats (label 1) by yÀÜ
RF
form the set U1 and those predicted as non-repeats (label 0) by yÀÜ form the set U0. Within each set, our
RF
objectiveistoidentifyoutliersusingthesequencingfeaturesxlenandxcovandmodifytheirlabelsaccordingly,
similar to Step 3. Within each set, specific thresholds are computed based on the distribution of sequencing
features of the unitigs in that set. More precisely, for U1 we define œÅlen as the (100‚àíp)-th percentile of the
high
unitigs‚Äô lengths and œÅcov and the p-th percentile of the coverage. Conversely, for U0 we define œÅlen as the
low low
p-th percentile of the unitigs‚Äô lengths and œÅcov and the (100‚àíp)-th percentile of the coverage. Based on
high
these thresholds, we identify outliers based on the following criteria
U1‚Üí0 ={i‚ààU1|xlen >œÅlen ‚àß xcov <œÅcov}, U0‚Üí1 ={i‚ààU0|xlen <œÅlen ‚àß xcov >œÅcov }. (4)
i high i low i low i high
In(4),wechangethelabelfromrepeattonon-repeat(U1‚Üí0)forthoseunitigsthatarelongerthanathreshold
and have low coverage. Similarly, we change the label from non-repeat to repeat (U0‚Üí1) for short unitigs
with high coverage. Notice that we used the same percentile p to compute the thresholds œÅ here as that one
used to compute the thresholds œÑ in Step 3. Naturally, we could select a different percentile here, but we use
the same one as this shows good empirical results and reduces the number of hyperparameters.
Summarizing, the final labels yÀÜ predicted by our model are given by
(cid:40)
1 for all i‚ààR ‚à™ (U1\U1‚Üí0) ‚à™ U0‚Üí1,
[yÀÜ] = (5)
i 0 for all i‚ààN ‚à™ (U0\U0‚Üí1) ‚à™ U1‚Üí0.
In (5), we see that the unitigs deemed as repeats ([yÀÜ] = 1) by our method are those i) assigned a repeat
i
pseudo-label in Step 3 (R), ii) classified as repeats by our RF in Step 4 and not deemed as outliers in Step
5 (U1 \U1‚Üí0), or iii) classified as non-repeats in Step 4 but later deemed as outliers in Step 5 (U0‚Üí1).
Conversely, unitigs classified as non-repeats are those i) assigned a non-repeat pseudo-label in Step 3 (N),
ii) classified as non-repeats by our RF in Step 4 and not deemed as outliers in Step 5 (U0 \U0‚Üí1), or iii)
classified as repeats in Step 4 but later deemed as outliers in Step 5 (U1‚Üí0).
3 Experimental setup
3.1 Datasets
We test GraSSRep in three types of datasets.GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 7
Simulated data: To represent distinct organisms, we generate two random backbone genomes with
an equal probability of observing each base. Subsequently, a random sequence of length L is generated
for each backbone and integrated into the genome with a copy number of C, serving as an intra-genome
repeat.Additionally,aninter-genomerepeatoflengthLisrandomlygeneratedandinsertedC timesinboth
genomes, representing an inter-genome repeat. Unlike the backbone genomes, repeats exhibit a non-uniform
distribution of bases, resulting in distinctive characteristics unique to each repeat, setting them apart from
the backbone genome. Consequently, we have two genomes, both containing a repeat content of 2√óL√óC
within a fixed length of 5 million base pairs for each organism. As a result, the characteristics of the repeats
within the genomes can be controlled by adjusting the values of L and C. Finally, simulated reads, each 101
base pairs in length, are generated using wgsim [26] with default values for error (2%) and mutation (0.1%).
Shakya 1: In this dataset, we analyze the reference genomes of a synthetic metagenome called Shakya,
whichconsistsof64organisms,including48bacteriaand16archaea[31].Basedonthesereferencegenomes,
read pairs are generated using wgsim, akin to the previous dataset. However, unlike the simulated data, all
thebackbonegenomesinthisdatasetarerealorganisms,containingintricaterepeatpatternsthatarebeyond
our control. The generated reads are 101 base pairs long with a high coverage (‚âÉ 50), and are produced
without any errors or mutations, in order to identify exact repeats in the data.
Shakya 2: Read pairs from the Shakya [31] study were obtained from the European Nucleotide Archive
(ENA ‚Äì Run:SRR606249), all with a length of 101. We have no influence over coverage or read errors in
this set of reads, mirroring real-world settings. This characteristic enables us to evaluate GraSSRep under
realistic scenarios.
3.2 Assembly
In all experiments, unitigs are assembled using a k-mer size of k = 51. During the read-mapping step, all
reads are trimmed so that only the first k nucleotides are mapped to the unitigs. Given that the shortest
unitighasalengthofk,trimmingtheendofthereadsensuresthatunitigswithlengthsshorterthantheread
length can still be mapped by some reads. Therefore, all unitigs are incorporated into the unitig graph, each
connecting to at least one other unitig. This ensures that our node classification problem comprehensively
considers all unitigs generated during the assembly process.
Toassessourmodelaccurately,itiscrucialtohavethegroundtruthlabelsfortheunitigs.Toidentifythese
labels, all unitigs are aligned to the reference genomes using NUCmer [27] (with the --maxmatch option).
Unitigsaremarkedasrepeatsiftheymeetspecificcriteria.Generally,thiscriterionincludesaligningatmore
than one location with at least 95% identity and 95% alignment length. However, in the error-free cases,
the criterion is aligning at more than one location with 100% identity and alignment length, which indicates
exact repeats through the reference genomes (Shakya 1 dataset).
3.3 Method design and hyperparameter choices
To select and label the training nodes, a threshold value p ranging between 25 and 35 is employed in Step
3, depending on the presence of noise in the data. Specifically, p = 35 in instances where noise is present
(simulateddataandShakya2),ensuringrobustnessinthepresenceofdatairregularities.Fornoiselesscases
(Shakya 1), we set p = 25, leading to a stricter definition of repeat pseudo-labels. Previous studies have
demonstrated that this choice yields effective repeat detection [10].
In Step 4, the first component of the GNN, h , consists of two consecutive GraphSAGE convolutional
Œ∏1
layers, each followed by a ReLU activation function [14]. The node representation update in these layers can
be mathematically defined as follows:
(cid:16) (cid:16)(cid:110) (cid:111)(cid:17) (cid:17)
z(l+1) =ReLU W ¬∑Mean z(l),‚àÄu‚ààNeigh(v) ,B z(l) , ‚àÄv ‚ààV,
v k u k v
wherez(l) representsthenodeembeddingofthenodev atlayerl,Neigh(v)representsthesetofneighboring
v
nodes of node v, and Mean is an aggregation function that combines the embeddings of neighboring nodes.
Moreover, B and W represent the linear transformation matrix for the self and neighbor embeddings,
k k
respectively. In this equation, z(l+1) represents the updated embedding of the node v at the next layer
v
(l+1). The first and second convolutional layers have 16 and 8 hidden channels, respectively. This results8 A. Azizpour et al.
in d = 8 new features being generated for each node, represented as Z ‚àà RN√ó8. Since h has two graph
Œ∏1
convolutional layers, the final embeddings combine the features within the 2-hop neighborhoods of each
node. Additionally, the second component of the GNN, f , comprises a single fully connected layer that
Œ∏2
transformsthenewlylearnedfeatures,Z,intobinaryclassesusingalineartransformationmatrixT‚ààR8√ó2.
The GNN is trained for 2000 epochs, utilizing cross-entropy as the loss function and employing the Adam
optimizer [19] with a learning rate of 0.01.
TheRFclassifierutilizes100treesintheforesttogenerateitsresults.Thesplitcriterionforeachdecision
tree is determined using the Gini impurity measure, ensuring the creation of optimal splits at each node.
Finally, to account for the randomness inherent in the training process, both the training and testing steps
arerepeatedfor10iterationsineachcase.Thereportedresultsareaveragedacrosstheseiterations,providing
a robust and reliable evaluation. As figures of merit, we report the classification accuracy, precision, recall,
and F1-score (harmonic mean of precision and recall).
4 Results
We present a comprehensive analysis of our algorithm‚Äôs performance across various settings.
4.1 Evaluation on varying repeat characteristics
We leverage the simulated dataset to examine the effect of three crucial characteristics that are beyond our
control within the real datasets:
A) Length of the repeats. To measure the impact of repeat length, we fix the copy number of both
inserted intra-genome and inter-genome repeats at C =25 and vary their length from L=150 to L=1000
base pairs, leading to a copy content ranging from 0.15% to 1% in the reference genomes.
B) Copy number of the repeats. We set the length of the inserted repeats to L=400 base pairs and
adjust their copy number from C =20 to C =150, increasing the complexity of the dataset. This results in
a copy content ranging between 0.32% and 2.4% in the reference genomes.
C) Coverage. We generate backbone data by inserting repeats of L = 400 base pairs in length with a
copy number of C =25 to have 0.4% copy content in the reference genomes. The number of generated read
pairs is varied, ranging from 0.5 to 2.5 million base pairs. Consequently, the coverage ranges from 10 to 50,
allowing us to analyze the algorithm‚Äôs performance under different coverage levels.
These adjustments enable a detailed evaluation of our algorithm‚Äôs robustness and adaptability across
a spectrum of repeat characteristics and coverage scenarios. Note that due to errors and mutations in the
generated reads, our analysis considers a repeat as having at least 95% identity over 95% of the length.
Consequently, more than just three unitigs are identified as repeats in this context, each with copy numbers
that may differ from the exact number of inserted repeats.
As illustrated in Figure 3(a), our approach demonstrates resilience to variations in repeat length, with
all metrics remaining stable as the repeat length increases. This robustness is particularly evident when the
repeat length surpasses the outer distance of the read pairs, which is 500 in our case. This phenomenon
occursbecausetherepeatunitigspredominantlyconnectwithneighboringsegmentsinthereferencegenome
within the graph structure. However, when the repeat length falls short of the outer distance, the unitigs
correspondingtoDNAsegmentstotheleftandrightoftherepeatsinthereferencegenomemightbedirectly
connected by an edge in the unitig graph, leading to less precise repeat detection.
Figure 3(b) shows the performance attained when varying the copy number. Our method achieves an
F1-score constantly exceeding 90% and, with copy numbers greater than 50, the precision also surpasses
90%. Given the simulated data scenario, with a 400-length repeat within a 5 million base pairs backbone, it
is logical for repeat copy numbers to fall within this range with at most 2% repeat content. This outcome
underscores the robustness of GraSSRep to logical copy numbers and demonstrates its resilience not only to
repeat length but also to copy number variations. This ensures its applicability across various datasets and
scenarios.
AsdemonstratedinFigure3(c),themodel‚Äôsperformanceexhibitsaconstantimprovementwithincreased
coverage, particularly in terms of enhanced precision. Notably, when the coverage reaches 40 (corresponding
to2millionreads),themodelachievesanF1-scoregreaterthan90.ItisimportanttonotethatacrossalltheGraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 9
Fig.3. Assessing the method across various repeat characteristics. (a) The model remains stable in metrics even
withincreasingrepeatlength,especiallybeyondtheouterdistanceofreadpairs(500basepairs).(b)Themethodis
robust to the copy number variation, consistently achieving an F1-score above 90%. (c) Higher sequencing coverage
improves the model‚Äôs performance.
different scenarios in the preceding three cases, our approach consistently achieves an almost perfect recall
rate of nearly 100%, highlighting its effectiveness in detecting almost all repeats.
4.2 Ablation study of the steps of the algorithm
We focus on the behavior of our method across different steps using the Shakya 1 dataset. After assembling
andconstructingthegraph,wehaveN =59808unitigsasthenodesofthegraph,outofwhich11900unitigs
are exact repeats (total length of the unitig repeated with 100% identity).
To begin, our evaluation involves assessing the method across various steps of the pipeline. Specifically,
we examine the outcomes relative to the baseline, the results produced by the GNN (yÀÜ ), the outputs
GNN
generatedbyRF(yÀÜ ),andfinally,afterthefine-tuningstep(yÀÜ).Inthiscontext,theterm‚Äúbaseline‚Äùrefers
RF
to a straightforward heuristic used to classify the unitigs. This heuristic relies on Step 3 and labels nodes
according to the following criteria
(cid:40)
1 for all i‚ààR,
[yÀÜ ] = (6)
base i
0 for all i‚ààN ‚à™ U.
This approach allows us to test the effectiveness of sequencing features in node labeling in the absence of
graph-based features.
In Figure 4(a), it is evident that the F1-score consistently rises throughout the pipeline, emphasizing the
importance of each step in achieving optimal results. The baseline method exhibits high precision (86.2%)
but low recall (26.5%), indicating appropriate node selection for determining pseudo-labels but an inability
to identify most repeats. This observation underscores that sequencing features alone are insufficient for
detecting repeats. This limitation is modified by the GNN, which significantly boosts the recall to 71.1%,
effectivelyidentifyingmorerepeats,whichsuggeststhatgraphstructureissignificantindetectingtherepeats.
Subsequent application of the RF further amplifies this increase in recall to 79.7%. However, this enhanced
recall comes at the cost of reduced precision compared to the baseline. To address this precision loss, the
fine-tuning step effectively identifies outliers, leading to a precision increase from 50.8% at the output of the
RF to 59.3% for the final estimation. In summary, our approach yields a 68.2% F1-score without any prior
labels on the unitigs, representing a substantial improvement of 27.6% over the baseline method.
Moreover,weinvestigatetheimpactoftheGNNandtheembeddingsitgenerates.Toassessthis,weper-
formtwoanalyses.First,weexcludeZfromthefeaturematrixfedtotheRF,resultinginX¬Ø = [X] ‚àà RN√ó5
aiming to observe the method‚Äôs performance only based on the initial graph-based features. As depicted in
Figure 4(a) under ‚ÄòExcluding GNN‚Äô, this exclusion leads to a decrease in both F1-score and precision. This
decline suggests that embeddings play a crucial role in enhancing the reliability of repeat detection. Second,10 A. Azizpour et al.
Fig.4. (a) Progression of the method‚Äôs performance throughout the different steps, highlighting the effectiveness
of each step in improving repeat detection. We also test the impact of excluding the GNN embeddings. (b) High
importance of GNN-generated embeddings in RF classification.
we calculate the importance of the features fed to the RF by averaging the impurity decrease from each
feature across trees. The more a feature decreases the impurity, the more important it is. These importance
values are then plotted in Figure 4(b). The plot indicates that nearly all learned embeddings (labeled z1
through z8) exhibit higher importance compared to the pre-specified graph features, except for betweenness
centrality. This finding emphasizes the utility of the embeddings generated by the GNN in improving the
overall performance of the method.
Lastly, we perform an ablation study on the percentile value p used to define the thresholds in Steps 3
and5.TheanalysisinAppendixA.1revealsthatourapproachisrobusttothishyperparameter,particularly
within the range of 25 to 35, which corresponds to the range used in our experiments.
4.3 Comparison with existing repeat detection methods
Wepresentacomprehensivecomparisonofourmethodwithseveralexistingrepeatdetectionmethodsusing
unitigs assembled from the reads downloaded from ENA (Shakya 2).
Fig.5. GraSSRep compared to the other repeat detection methods, demonstrating superior performance.
We consider five widely recognized methods for this comparison. Opera [8] and SOPRA [6] identify
repetitive unitigs by filtering out those with coverage 1.5 and 2.5 times higher than the average coverage of
all unitigs, respectively, without considering any graph structure. Similarly, the MIP scaffolder [28] utilizes
both high coverage (more than 2.5 times the average) and a high degree (‚â• 50) within the unitig graph to
detect the repeats. Additionally, Bambus2 [22] categorizes a unitig as a repeat if the betweenness centrality,GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 11
divided by the unitig length, exceeds the upper bound of the range within c standard deviations above
the mean on this feature. Here, c represents a hyperparameter of this method, and the optimal outcome
on our dataset was achieved with c = 0. Lastly, Metacarvel [10] employs four more complex graph-based
featuresalongsidecoverageinatwo-stepprocess.First,anyunitigwithahighbetweennesscentrality(‚â•three
standarddeviationsplusthemean)ontheunitiggraphismarkedasarepeat.Moreover,aunitigisidentified
as a repeat if it falls within the upper quartile for at least three of these features: mean coverage, degree,
ratio of skewed edges (based on coverage), and ratio of incident edges invalidated during the orientation
phase of the unitigs; see [10] for details. Notably, since we utilize a unitig graph instead of a scaffold graph,
we do not incorporate the latest feature and adjust the flag threshold from three to two in the second step.
AsillustratedinFigure5,GraSSRepoutperformsallothermethods,particularlydemonstratingsuperior
capabilityindetectingrepeatswithahigherrecallrate(55%versusthenextbestalternativeat29.8%).This
superiority comes from the combined value of incorporating learnable graph features (through the GNN)
and considering a self-supervised framework. Notice that, even if we fix the embedding dimension at d=8,
the graph features learned by the GNN depend on the specific dataset under consideration. In this way, our
trainablearchitecturecandistillthekeygraphfeaturesthatcharacterizerepeatsinthespecificmetagenomic
sample. This adaptive approach stands in contrast to other methods, which often rely on fixed features.
Moreover, since the RF is not pre-trained but rather trained based on the pseudo-labels, different features
may vary in importance based on context. For instance, if we repeat the analysis in Figure 4(b) for this
dataset (not shown here), we observe that the clustering coefficient of the unitigs holds greater significance
indetectingrepeatscomparedtobetweennesscentralityordegree.Inthisway,ourself-supervisedframework
allowsustoadapttothemetagenomicdataathand,andwedonothavetoworryaboutgeneralizationissues
of pre-trained models.
Thus far, we have focused on the practical unsupervised setting where no repeat labels are available.
For completeness, we now consider the case where repeat labels for some unitigs are available. This setting
might arise, e.g., if we have knowledge about specific organisms present in the metagenomic sample and
their corresponding reference genomes are accessible. GraSSRep can seamlessly accommodate this case. In
our pipeline, we can leverage this prior knowledge to substitute Step 3. Instead of pseudo-labels, we employ
the known node labels as our training set, leading to a semi-supervised (instead of self-supervised) setting.
Our analysis in Appendix A.2 shows that performance can be markedly improved in the case where labels
are available for a fraction of the unitigs.
5 Conclusion
We tackled the challenging task of detecting repetitive sequences (repeats) in metagenomics data when we
only have access to paired-end reads. We introduced GraSSRep, a novel method that leverages the inherent
structure of the assembly graph by employing GNNs to extract specific features for the unitigs. Moreover,
adopting a self-supervised learning framework, we generated noisy pseudo-labels for a subset of the unitigs,
which were then used to train a graph-based classifier on the rest of the unitigs. Experimental studies using
both simulated and synthetic metagenomic datasets demonstrated the robustness of our method across
diverse repeat characteristics, the value of every step in our algorithm in enhancing repeat detection, and
the performance gain compared to existing repeat detection tools.
Anaturalextensionofourapproachisitsintegrationintowidelyusedassemblers.Thisintegrationwould
replacetheirexistingrepeatdetectionmoduleswithGraSSRep,yieldingpotentialimprovementsinassembly
quality. We also intend to apply our method to real datasets, particularly in environments like hot springs
wherewidelyaccessiblereferencegenomesarescarce.Lastly,theoverallpipelineofGraSSRepcanpotentially
address other problems in genomics where graph structures can be used to identify specific genetic markers
in the absence of prior knowledge. For instance, we intend to leverage our approach for the identification of
transposable elements, which play important roles in eukaryotic/mammalian genomes.Bibliography
[1] Agarap, A.F.: Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 (2018)
[2] Balaji, A., Sapoval, N., Seto, C., Elworth, R.L., Fu, Y., Nute, M.G., Savidge, T., Segarra, S., Trean-
gen, T.J.: KOMB: K-core based de novo characterization of copy number variation in microbiomes.
Computational and Structural Biotechnology Journal 20, 3208‚Äì3222 (2022)
[3] Breiman, L.: Random forests. Machine learning 45, 5‚Äì32 (2001)
[4] Chowdhury, A., Verma, G., Rao, C., Swami, A., Segarra, S.: Unfolding WMMSE using graph neural
networks for efficient power allocation. IEEE Transactions on Wireless Communications 20(9), 6004‚Äì
6017 (2021)
[5] CÀáutura, G., Li, B., Swami, A., Segarra, S.: Deep demixing: Reconstructing the evolution of epidemics
using graph neural networks. In: European Signal Processing Conference (EUSIPCO). pp. 2204‚Äì2208
(2021)
[6] Dayarian, A., Michael, T.P., Sengupta, A.M.: SOPRA: Scaffolding algorithm for paired reads via sta-
tistical optimization. BMC bioinformatics 11, 1‚Äì21 (2010)
[7] De Boer, P.T., Kroese, D.P., Mannor, S., Rubinstein, R.Y.: A tutorial on the cross-entropy method.
Annals of operations research 134, 19‚Äì67 (2005)
[8] Gao, S., Sung, W.K., Nagarajan, N.: Opera: reconstructing optimal genomic scaffolds with high-
throughput paired-end sequences. Journal of Computational Biology 18(11), 1681‚Äì1691 (2011)
[9] Ghurye, J., Pop, M.: Better identification of repeats in metagenomic scaffolding. In: Algorithms in
Bioinformatics: 16th International Workshop, WABI 2016, Aarhus, Denmark, August 22-24, 2016. Pro-
ceedings 16. pp. 174‚Äì184. Springer (2016)
[10] Ghurye, J., Treangen, T., Fedarko, M., Hervey, W.J., Pop, M.: MetaCarvel: linking assembly graph
motifs to biological variants. Genome biology 20(1), 1‚Äì14 (2019)
[11] Ghurye, J.S., Cepeda-Espinoza, V., Pop, M.: Metagenomic Assembly: Overview, Challenges and Appli-
cations. The Yale journal of biology and medicine 89(3), 353 (2016)
[12] Girgis,H.Z.:Red:anintelligent,rapid,accuratetoolfordetectingrepeatsde-novoonthegenomicscale.
BMC bioinformatics 16(1), 1‚Äì19 (2015)
[13] Glaze, N., Bayer, A., Jiang, X., Savitz, S., Segarra, S.: Graph representation learning for stroke re-
currence prediction. In: IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). pp. 1‚Äì5 (2023)
[14] Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large graphs. Advances in
neural information processing systems 30 (2017)
[15] Hamilton, W.L., Ying, R., Leskovec, J.: Representation learning on graphs: Methods and applications.
arXiv preprint arXiv:1709.05584 (2017)
[16] Haykin, S.: Neural networks: a comprehensive foundation. Prentice Hall PTR (1998)
[17] Iranzo,J.,Wolf,Y.I.,Koonin,E.V.,Sela,I.:Genegainandlosspushprokaryotesbeyondthehomologous
recombinationbarrierandaccelerategenomesequencedivergence.Naturecommunications10(1), 5376
(2019)
[18] Jaiswal,A.,Babu,A.R.,Zadeh,M.Z.,Banerjee,D.,Makedon,F.:Asurveyoncontrastiveself-supervised
learning. Technologies 9(1), 2 (2020)
[19] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980
(2014)
[20] Kipf,T.N.,Welling,M.:Semi-supervisedclassificationwithgraphconvolutionalnetworks.arXivpreprint
arXiv:1609.02907 (2016)
[21] Koren, S., Phillippy, A.M.: One chromosome, one contig: complete microbial genomes from long-read
sequencing and assembly. Current opinion in microbiology 23, 110‚Äì120 (2015)
[22] Koren, S., Treangen, T.J., Pop, M.: Bambus 2: scaffolding metagenomes. Bioinformatics 27(21), 2964‚Äì
2971 (2011)
[23] Koutrouli, M., Karatzas, E., Paez-Espino, D., Pavlopoulos, G.A.: A guide to conquer the biological
network era using graph theory. Frontiers in bioengineering and biotechnology 8, 34 (2020)
[24] Langmead,B.,Salzberg,S.L.:Fastgapped-readalignmentwithBowtie2.Naturemethods9(4),357‚Äì359
(2012)GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly 13
[25] Lapidus,A.L.,Korobeynikov,A.I.:Metagenomicdataassembly‚Äìthewayofdecodingunknownmicroor-
ganisms. Frontiers in Microbiology 12, 613791 (2021)
[26] Li, H.: wgsim-Read simulator for next generation sequencing. Github repository (2011)
[27] Mar¬∏cais, G., Delcher, A.L., Phillippy, A.M., Coston, R., Salzberg, S.L., Zimin, A.: MUMmer4: A fast
and versatile genome alignment system. PLoS computational biology 14(1), e1005944 (2018)
[28] Salmela,L.,M¬®akinen,V.,V¬®alim¬®aki,N.,Ylinen,J.,Ukkonen,E.:Fastscaffoldingwithsmallindependent
mixed integer programs. Bioinformatics 27(23), 3259‚Äì3265 (2011)
[29] Schatz,M.C.,Delcher,A.L.,Salzberg,S.L.:Assemblyoflargegenomesusingsecond-generationsequenc-
ing. Genome research 20(9), 1165‚Äì1173 (2010)
[30] Segarra,S.,Ribeiro,A.:Stabilityandcontinuityofcentralitymeasuresinweightedgraphs.IEEETrans-
actions on Signal Processing 64(3), 543‚Äì555 (2015)
[31] Shakya, M., Quince, C., Campbell, J.H., Yang, Z.K., Schadt, C.W., Podar, M.: Comparative metage-
nomic and rRNA microbial diversity characterization using archaeal and bacterial synthetic communi-
ties. Environmental microbiology 15(6), 1882‚Äì1899 (2013)
[32] Simpson,J.T.,Wong,K.,Jackman,S.D.,Schein,J.E.,Jones,S.J.,Birol,I.:ABySS:aparallelassembler
for short read sequence data. Genome research 19(6), 1117‚Äì1123 (2009)
[33] Soucy,S.M.,Huang,J.,Gogarten,J.P.:Horizontalgenetransfer:buildingtheweboflife.NatureReviews
Genetics 16(8), 472‚Äì482 (2015)
[34] Treangen, T.J., Abraham, A.L., Touchon, M., Rocha, E.P.: Genesis, effects and fates of repeats in
prokaryotic genomes. FEMS microbiology reviews 33(3), 539‚Äì571 (2009)
[35] Treangen, T.J., Salzberg, S.L.: Repetitive DNA and next-generation sequencing: com-
putational challenges and solutions. Nature Reviews Genetics 13(1), 36‚Äì46 (Jan 2012).
https://doi.org/10.1038/nrg3117, https://doi.org/10.1038/nrg3117
[36] Wooley,J.C.,Godzik,A.,Friedberg,I.:Aprimeronmetagenomics.PLOSComputationalBiology6(2),
1‚Äì13 (02 2010). https://doi.org/10.1371/journal.pcbi.1000667, https://doi.org/10.1371/journal.
pcbi.1000667
[37] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Philip, S.Y.: A comprehensive survey on graph neural
networks. IEEE transactions on neural networks and learning systems 32(1), 4‚Äì24 (2020)
[38] Yang, C., Chowdhury, D., Zhang, Z., Cheung, W.K., Lu, A., Bian, Z., Zhang, L.: A review of com-
putational tools for generating metagenome-assembled genomes from metagenomic sequencing data.
Computational and Structural Biotechnology Journal 19, 6301‚Äì6314 (2021)
[39] Zaki,N.,Efimov,D.,Berengueres,J.:Proteincomplexdetectionusinginteractionreliabilityassessment
and weighted clustering coefficient. BMC bioinformatics 14(1), 1‚Äì9 (2013)
[40] Zhao, Z., Verma, G., Rao, C., Swami, A., Segarra, S.: Link scheduling using graph
neural networks. IEEE Transactions on Wireless Communications 22(6), 3997‚Äì4012 (2023).
https://doi.org/10.1109/TWC.2022.322278114 A. Azizpour et al.
A Appendix
A.1 Ablation study on the percentile value p
A critical hyperparameter of GraSSRep is the percentile value p, which determines the thresholds in Steps
3 and 5. In Figure 6, we plot the achieved F1-score for different choices of p, ranging from 1 to 50.
As illustrated, GraSSRep consistently attains an F1-
score exceeding 50% for the values of p greater than 10,
indicating the method‚Äôs robustness to this hyperparame-
ter.
A.2 Incorporating prior knowledge
In certain scenarios, prior information about unitigs can
be available, allowing for their labeling without relying
on sequencing features. For instance, if we have knowl-
edgeaboutspecificorganismspresentinthemetagenomic
Fig.6.F1-scoreperformanceacrossvaryingvaluesof
sampleandtheircorrespondingreferencegenomesareac-
the hyperparameter p, demonstrating the method‚Äôs
cessible,wecanidentifyrepetitiveunitigsassociatedwith
stability.
those organisms. In our pipeline, we leverage this prior
knowledgetosubstituteStep3.Instead,weemploynodes
labeled with this prior information as the training set.
This approach prompted us to explore semi-supervised
learning.
Todothis,weinitiallydeterminethenumberofunitigsthatwouldhavebeenincludedinthetrainingset
using Step 3. Subsequently, we randomly select the same number of nodes with their labels as our training
nodes. Additionally, instead of fine-tuning the labels predicted by RF with sequencing features in Step 5,
we integrate the length and mean coverage of the unitigs into their initial node features within the unitig
graph. The remainder of our method remains unchanged.
Fig.7. Semi-supervised learning in comparison to self-supervised learning.
WeutilizethereferencegenomesintheShakyadatasettoapplyoursemi-supervisedlearningmethodon
Shakya 2, comparing it with the self-supervised approach. As shown in Figure 7, semi-supervised learning
outperforms the self-supervised method in all of the metrics. This experiment highlights two important
features of our method. First, even though we present the method for the more challenging case where no
information is given about the metagenomic sample other than the reads, whenever reference genomes are
available, this information can be seamlessly introduced. Second, this ground-truth information about the
repetitive nature of some unitigs can drastically help in the repeat detection process, so it must be used
whenever it is available.