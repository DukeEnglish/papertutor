IFCap: Image-like Retrieval and Frequency-based Entity Filtering
for Zero-shot Captioning
SoeunLeeâˆ— Si-WooKimâˆ— TaewhanKim Dong-JinKimâ€ 
HanyangUniversity,SouthKorea.
{soeun, boreng0817, taewhan, djdkim}@hanyang.ac.kr
Abstract etal.,2019b,2024). Toovercomethis,recentstud-
ies have explored text-only training methods for
Recentadvancementsinimagecaptioninghave
imagecaptioning,aimingtosolvetheproblemus-
explored text-only training methods to over-
ingonlytextualdata(Nukraietal.,2022;Lietal.,
comethelimitationsofpairedimage-textdata.
2023; Fei et al., 2023; Zeng et al., 2024; Wang
However,existingtext-onlytrainingmethods
often overlook the modality gap between us- etal.,2023;Liuetal.,2023;Maetal.,2023).
ing text data during training and employing Text-onlytrainingintroducesanewdirectionin
imagesduringinference. Toaddressthisissue, which models are trained solely using text data.
weproposeanovelapproachcalledImage-like Recent existing works have studied what to use
Retrieval,whichalignstextfeatureswithvisu-
as extra cues, such as extracted nouns (Fei et al.,
allyrelevantfeaturestomitigatethemodality
2023),generatedsyntheticimages(Liuetal.,2023;
gap. Our method further enhances the accu-
Ma et al., 2023) for training, and extracted tags
racyofgeneratedcaptionsbydesigningaFu-
sionModulethatintegratesretrievedcaptions fromobjectdetectors(Liuetal.,2023). However,
withinputfeatures. Additionally,weintroduce existing methods that rely on object information
aFrequency-basedEntityFilteringtechnique are sensitive to incorrect data, and utilizing large
thatsignificantlyimprovescaptionquality. We external models (e.g., stable diffusion Rombach
integrate these methods into a unified frame-
etal.,2022orobjectdetectorsCarionetal.,2020)
work,whichwerefertoasIFCap(Image-like
incurs additional costs. Thus, we aim to address
RetrievalandFrequency-basedEntityFiltering
theproblembyacquiringdiverseinformationcost-
forZero-shotCaptioning). Throughextensive
effectivelywithoutadditionalmodels.
experimentation,ourstraightforwardyetpow-
erful approach has demonstrated its efficacy, The retrieval task involves finding relevant in-
outperformingthestate-of-the-artmethodsby formationinadatabaseforagivenquery. Initially
asignificantmargininbothimagecaptioning rooted in NLP (Lewis et al., 2020), the field has
and video captioning compared to zero-shot expandedintoCVandintomulti-modalretrieval.
captioningbasedontext-onlytraining.1
Dependingontheinputdataanddatabase,various
retrieval methods are possible, such as image-to-
1 Introduction
text(Ramosetal.,2023)andtext-to-textretrieval
Thetaskofimagecaptioninggeneratesappropriate (Wangetal.,2023). Intheexistingtext-onlytrain-
textualdescriptionsforimagesbycombiningcom- ingstudy,therehavebeenattemptstousethetext-
putervision(CV)andnaturallanguageprocessing to-textretrievalmethod. However,existingworks
(NLP). With the emergence of Large Language canâ€™taddressthemodalitygapinherentintext-only
Models (LLMs) and Vision and Language Mod- trainingsettings,wheretrainingisperformedwith
els (VLMs), various works have studied efficient textandinferencewithimages. Inaddition,such
training methods for image captioning (Mokady worksrelytoomuchonretrievedcaptionswithout
etal.,2021;Luoetal.,2023;Ramosetal.,2023). consideringvisualinformation. Thismodalitygap
Theseapproachesdevelopeffectivecaptioningby
andtheuseofanarrowscopeofinformationmay
using pre-trained models with few parameters or leadtoperformancedegradation.
lightweightnetworks. However,theseworksrely Toverifythis,wevisualizetheanalysisresultof
on paired image-text data, which is costly (Kim theCLIPembeddingfeatureofretrievedcaptions
thatthemodelusesintrainingviat-SNEinFig.2.
âˆ—Equalcontribution.â€ Correspondingauthor.
1Code:https://github.com/boreng0817/IFCap The analysis is done on the COCO (Chen et al.,
4202
peS
62
]VC.sc[
1v64081.9042:viXraTrainingtimeretrieval
Text-to-text Retrieval Inferencetimeretrieval Image-like Retrieval (Ours)
Retrieved sentences in training
Embedding Space Retrieved sentences in inference Embedding Space
Modality gap
Text Image Text Image
CLIP Classifier Entity Retrieval Frequency-based Entity Filtering (Ours)
(Softmax) Vocabulary "A ma1nis taking a ride on his motor1cyclenear the country side."
"A ma2nsitting on a motor2cyclenear the edge of a moun1tain."
man "A person riding a motor3cycleon a narrow road."
backpack â‹®
X
motorcycle (Frequency) : 2 : 3 : 1
Threshold â‰¥0.3, (wrong) Threshold â‰¥2,
â†’Hard Prompt:â€œThere are man, backpackin image.â€ â†’Hard Prompt: â€œThere are motorcycle, manin image.â€
Figure 1: (Top) The previous text-to-text retrieval approach overlooks the modality gap, leading to different
informationusebetweentrainingandinference. Ourapproachaddressesthisbyaligningtextfeatureswiththe
imageembeddingspaceduringretrieval. (Bottom)ThetraditionalCLIPclassifier-basedentityretrievalmethod
struggles with entity detection as vocabulary size grows. Our approach detects frequently occurring words in
retrievedcaptions,extractingentitiesmoreaccuratelywithoutrelyingonalimitedvocabulary.
2015) validation split, and the CLIP similarity- the original input and additional representations.
basedKNNalgorithmisusedforretrieval. Inthe Additionally, as shown by numerous studies (Fei
figure,thereisalargedifferencebetweenthedistri- etal.,2023;Ramosetal.,2023),promptscanclar-
butionoffeaturesusedafterimage-to-textretrieval ifytheinformationprovidedtothelanguagemodel.
andtext-to-textretrieval,whichshowsthatamodal- Weextractkeywordsfromtheinputcaptiontocon-
itygapexistsbetweenimageandtext. struct a hard prompt, which is fed to the LLM,
offering explicit guidance. This approach maxi-
Totacklethisissue,weproposeanovelapproach
mizestheutilityoftextdata,guidingthemodelto
called â€œImage-like Retrieval,â€ that addresses the
generateaccurateandrelevantcaptions.
modality gap between image and text data. We
inject a noise into the CLIP text feature to act as
Guidingcaptiondecoderwithextractedentities
a query in image feature distribution. Visualiza-
fromanimagehelpsthemodelgenerateanaccu-
tion results for this approach are shown in Fig. 2
rate description of the image. However, we find
right,demonstratingthatourmethodexhibitsadis-
thatthepreviousworks (Feietal.,2023;Liuetal.,
tributionhighlysimilartothatofimage-to-retrieval
2023) show low entity detection precision, espe-
resultsandgroundtruthcaptions,unliketraditional
cially when the vocabulary is large as shown in
text-to-text retrieval methods. Indeed, when our
Fig.3. Therefore,weproposeaFrequency-based
methodisappliedtotheexistingresearch(Wang
Entity Filtering technique precisely utilizing en-
et al., 2023), performance improvements are ob-
tityinformationwithoutrelyingonthevocabulary.
served,asshowninTable 12.
During inference, we utilize retrieved sentences
Priorresearch(Wangetal.,2023)reliessolely from images, parsing them into nouns and calcu-
onretrievedcaptions,whichmayincludewrongin- latingtheirfrequency. Then,wefilternounswith
formationintheinputcaption,potentiallyleading pre-definedthresholdsandcuratehardpromptsfor
to inaccurate outputs. To address this, we design the text decoder. This simple method yields re-
a Fusion Module that effectively integrates both markableperformanceimprovements.Entity Filtering (Ours)
ViECap
(39.5%) DETR
231 / 585
(85.1%)
1,374 / 1,614
(15.5%)
354 / 2,281
(22.1%)
227 / 1,029
(78.4%)
1,277 / 1,628
(40.0%)
5,464 / 13,659
(68.8%)
3,052 / 4,438
(86.1%)
Figure2: ThedistributionofCLIPembeddingfeatures 8,678 / 10,083
correspondingtoimagesâ– ,pairedcaptions(cid:8),retrieved
0 20% 40% 60% 80%
captions(cid:8)foraspecificimage,andtheresultoftext-to-
Figure3: PrecisionofextractedentitiesintheCOCO
textretrieval(cid:8)andourImage-likeRetrieval(cid:8).
test set, total 5,000 images. If an extracted entity ex-
ists in the ground-truth caption, it counts as correct
Insummary,ourcontributionsareasfollows: or else wrong. Three methods (Ours, ViECap2023,
â€¢ We propose a novel approach, Image-like Re- DETR2020)arecomparedwiththreedifferentsettings.
trieval,whichachieveseffectssimilartoimage- Ourmethodisillustratedin3.3,andViECapusesCLIP
based classifier with the source domainâ€™s vocabulary
to-textretrievalintext-onlytraining. Then,we
list. WefollowthewaySynTIC(Liuetal.,2023)uses
introduce a Fusion Module for interaction be-
DETRandemploytheCOCOvocabularylist. Dueto
tweenexistingandadditionalrepresentations.
the inaccessible vocabulary list of Flickr30k, DETR
â€¢ Weproposeanentityfilteringtechniqueininfer-
canâ€™tbecompared,andViECapusestheVGOI(Zhang
ence,Frequency-basedEntityFiltering,enhanc- etal.,2021)vocabularylistinFlickr30k. Ourmethod
ing the language model by filtering frequently dominatestheprecisionscoreandquantityofentitiesin
appearingentitiesinretrievedcaptions. everysetting.
â€¢ Extensive evaluations show IFCap achieves
state-of-the-art performance in various bench- ever,ithasbeenshownthattheseembeddingsare
marks,includingvideocaptioning. locatedintwoseparateregions,withasignificant
gap between the modalities (Liang et al., 2022).
2 Relatedwork
Thismodalitygaphinderstheinteractionbetween
2.1 Text-onlyCaptioning visionandtextmodalitiesandlimitsthequalityof
generatedcaptions. Amongthenotableapproaches
TheadvantageofCLIP(Radfordetal.,2021)has
addressingthisissue,CapDec(Nukraietal.,2022)
been utilized in a variety of tasks, such as image
assumes that the image embeddings paired with
captioning,imagegeneration,andobjectdetection.
textembeddingsarelocatedwithinasmallradius
Intherealmofimagecaptioning,text-onlytraining
aroundthetextembeddingsandmitigatesthegap
research is emerging that uses only text data for
withnoiseinjection. CLOSE(Guetal.,2022)high-
learningwithoutimagedata,takingadvantageof
lightsthelowcosinesimilaritybetweenimagesand
theCLIPcharacteristicthatimageembeddingsand
theirpairedtextsandusesahyper-parameter-scaled
textembeddingsarelearnedtobeclose. DeCap(Li
noiseinjectiontechniquetobridgethegap.
etal.,2023)trainsatextdecoderusingonlytextual
Wefocusonthemodalitygapforretrievalfrom
dataandintroducesasupportmemorymechanism
a new perspective. Our goal is to perform text
to project input images into the text embedding
retrievalsimilartoimage-to-textretrieval,consider-
spaceduringinference,facilitatingthegeneration
ingthemodalitygap. Thedistinctionfromexisting
ofcaptions. ViECap(Feietal.,2023)recognizes
methodscanbeobservedinFig.2left.
themainentityoftextdatathatcomesasinputand
configuresitasaprompt,allowingLLMtoperform
2.3 RetrievalAugmentedGeneration
object-agnosticlearningbasedonopenvocabulary
retrievalusingCLIP. RetrievalhasbeenusedindiversewaysinNLP.Im-
agecaptioningalsobenefitsfromretrievalmodules
2.2 ModalityGap
by incorporating novel objects and new informa-
VisionlanguagemodelssuchasCLIPaimtoembed tionintocaptions,allowingaccesstonewdomains
images and text closely in a shared space. How- without additional training. Retrieval is applied
k03rkcilF
k03rkcilF
>-
OCOC
OCOCImage-like Top-ğ’Œ Text Fusion Module
e
Retrieval sentences Encoder
Fusion
ğ‘»ğ’† ğ‘¹ğ’†
m ğ’‡ ğ’‡
iT Module ğ’ğŸ ğ’ğŸ
g â€œA man is in a kitchen Text ğ‘¸ ğ‘²,ğ‘½
n in making pizzas.â€ Encoder ğ’‡
ia Object parsing ğ~ğ‘µ ğŸ,ğˆğŸ ğ‘¨ğ’•ğ’•
r
T
man There are man,
ğœ½ğ’’
kitchen kitchen, pizza
pizza in the image.
Mapping Network
GPT-2
Image-to-text Top-ğ’Œ Text
Retrieval sentences Encoder
e m Fusion
iT Module
e c Image Frozen parameter
n Encoder
e r Learnable parameter
e
fn
I Entity There are dog, Concatenation
sandin the
Filtering image. Noise injection
Figure 4: Theoverview of IFCap. During training, we extractnouns from theinput text and retrieve k similar
sentencesusingourImage-likeRetrievalmethod. Extractednounsareincorporatedintoaprompttemplatetoform
ahardprompt. Boththeinputtextandretrievedsentencesareencodedusingthetextencoder. Theseembeddings
interactandcombinethroughourFusionModulebeforebeingfedintotheLLMforsentencegeneration. During
inference,weretrievelsentencessimilartotheinputimageandconstructahardpromptbyextractingentitiesvia
Frequency-basedEntityFilteringfromtheretrievedsentences. Thesentencesareencodedusingatextencoder,and
theinputimageisencodedusinganimageencoder,followedbyinputintotheFusionModule. Thesubsequent
processfollowsaproceduresimilartothetrainingphase.
invariouswaysinimagecaptioningmodels. For Our IFCap utilizes a simple yet powerful re-
instance,Smallcap(Ramosetal.,2023)retrieves trievalmechanismandaddressesthemodalitygap
captionsrelevanttotheinputimageandusesthem betweenimageandtextwithImage-likeRetrieval
asinstructionsforthetextdecoder. Intext-onlyim- (Section 3.1). After performing Image-like Re-
agecaptioning,ViECap(Feietal.,2023)retrieves trieval,weemployaFusionModule(Section3.2)
novelobjectsfromtheinputimageandusesthem tomergeinputembeddingswiththeretrievedfea-
asprompts,whileKnight(Wangetal.,2023)uses tures. Duringinference,weusetheretrievedcap-
retrievedcaptionsastextfeatures. tionsfromtheimagetofindaccurateanddetailed
Most retrieval methods are based on image-to- entitieswithFrequency-basedEntityFiltering(Sec-
text retrieval, but text-only captioning performs tion3.3).
text-to-text retrieval. However, during inference,
themodalitygapcausedbytheinputimageleadsto 3.1 Image-likeRetrieval(ILR)
poorperformance. Ourmethodcarefullyaddresses
Whiletext-to-textretrievalcanbeeffectivelyper-
thisissuetoimproveperformancebyconsidering
formed during training, it is likely to suffer from
thegapbetweenimageandtext.
performance degradation during inference when
animageisprovidedasinputduetothemodality
3 Methods
gap. Therefore, Image-like Retrieval (ILR) aims
We propose a new text-only image captioning to perform text-to-text retrieval in a manner that
model,IFCap,whichisillustratedin Fig.4. Dur- resemblesimage-to-textretrievaloutcomes,given
ing training, the model only utilizes text data, as text input. For this, we propose an approach that
isstandardfortext-onlytrainingmodels. First,we insertsnoiseintothefeaturespaceoftheinputtext,
embedtheinputtextusingatextencoder. Thetext bringingitclosertotheimagefeaturespace. The
embeddingsarethenfedintoamappingnetworkto augmentationprocessisasfollows:
closethegapbetweendifferentmodalities. Finally, First,weutilizetheCLIPtoembedtheinputtext
the processed embeddings go through a caption t andthetextcorpusT = {t}Nc withatextencoder
i i i=1
decodertogeneratetheoutputcaption. E . Then, we introduce noise Ïµ âˆ¼ N(0,Ïƒ2) into
T r rtheembeddingofinputtextT ,aimingtoadjustthe The noun implies intuitive and explicit informa-
i
textfeaturestoalignmorecloselywiththeimage tionaboutobjectsintheimage. Foremployingthe
featurespace: propertyofnouns,weextractentitiesineachtrain-
ingtextcorpusandinputimages. Webuildahard
T = E (t), TÏµ = T +Ïµ . (1) prompthwithextractedentitiesE = {e ,e ,...,e }
i T i i i r 1 2 n
tomakethemodelawareofexistingentitiesinthe
Next,theretrievalstepisperformedusingthenoise- image. Withretrievedcaptionsandhardprompts
injectedinputtextTÏµ. Toidentifythedescriptions with entities, the model can learn the ability to
i
mostrelevanttoTÏµ, thetop-k descriptionsarere- generatepropercaptionswithoutimages. Weuse
i
trievedbycalculatingthecosinesimilaritybetween auto-regressivelosstooptimizeourprojectorand
TÏµ andallsentenceembeddingsinthetextcorpus. captiondecoder. (DetailsabouttheFusionModule
i
Thisprocesscloselyfollowspreviousmethodsin arein Sec.4.1).
image-to-textretrieval(Ramosetal.,2023),with
thedistinctionthatweperformretrievalbasedon 1
(cid:88)N
L = âˆ’ log(y|F;h;y ;Î¸). (5)
TÏµ insteadofimages. Î¸ N i <i
i i=1
By utilizing this approach during training, we
3.3 Frequency-basedEntityFiltering(EF)
canenhancetheabilityofamodeltoprovideimage-
likeinformationeveninatext-onlytrainingsetting, After retrieving l captions from an image, we
therebynarrowingthemodalitygapandimproving use grammar parser tools (e.g., NLTK Bird and
performance. Loper, 2004) to extract nouns from the retrieved
sentencesandcalculatethefrequencyoftheseex-
3.2 FusionModule(FM) tractednounsas F = [f , f ,..., f ]. Wethenselect
1 2 n
Intext-onlyimagecaptioning,choosingwhichad- nouns that have a frequency larger than a prede-
ditional information to inject into the model and finedthresholdandplacethemintoahardprompt.
dealingwithnewrepresentationswithgivendata Heuristicthreshold: Since frequency is dis-
appropriatelyareimportantissues. Tohandlethis crete,wecanmanuallyfindthebestthresholdby
problem,weusetheattentionmechanism(Vaswani conductingexperimentswitheverypossiblethresh-
etal.,2017)tofuseinputtextfeaturesandretrieved old. Thisallowsustodeterminetheglobaloptimal
captionsfeaturestoextracttheirmeaningfulinter- threshold.
action. Theattentionmechanismemphasizescer- Adaptivethreshold: We can use a heuristic
tainimportantfeatures,andduetoitseffectiveness, threshold,butthesethresholdsareoftenunsuitable
ithasbeenwidelyutilizedinthefieldofcaptioning
fordifferentenvironments,andperformingexten-
(Xuetal.,2015). siveexperimentsincursunnecessarycosts. Instead,
wecanestimatethecommondistributionofnoun
Wefirstencodeinputtextandretrievedcaptions
frequenciesascertainprobabilitydistributions. We
using CLIP (Radford et al., 2021) text encoder,
theninjectaGaussiannoiseÏµ âˆ¼ N(0,Ïƒ2)toinput canassumefrequenciesfollowN(Âµ F,Ïƒ2 F).
textfeatureforrelievingthemodalitygapbetween
Ï„ = Âµ +Ïƒ . (6)
adap F F
imageandtext. Thenweadjustthedimensionof
the input text feature and retrieved captions fea- Any nouns with a frequency larger than Ï„ ,
adap
ture to the embedding space of caption decoder whichplacesthemintheupper15%, canbecon-
withlinearlayers f l1 and f l2 respectively,andapply sideredoutliers. Usingthisadaptivethreshold,we
cross-attention f Att withT e asqueryandR e askey, canimplementaflexiblethresholdthatfitsvarious
thencreatefusionrepresentationF e containingin- settings. However, it does not guarantee global
put text and retrieved captions. Finally, F
e
is fed optima,leadingtoatrade-offrelationshipbetween
intoatrainableMappingNetwork,whichencodes heuristicthresholdsandadaptivethresholds.
the overall contents of the given input. We can
summarizethisprocesswithequations. 4 Experiments
4.1 ImplementationDetails
T = T +Ïµ, R = E (ILR(T )), (2)
e i e T i
While verifying the state-of-the-art performance
F = f (f (T ), f (R )), (3)
e Att l1 e l2 e ofourmodel,weuseCLIP(ViT-B/32)astheim-
F = Map(F ;Î¸ ). (4)
e q age encoder and GPT2 (Radford et al., 2019)
baseImage Text COCO Flickr30k
Method
Encoder Decoder B@4 M C S B@4 M C S
CapDec(2022) RN50x4 GPT-2 26.4 25.1 91.8 11.9 17.7 20.0 39.1 9.9
Large
DeCap(2023) ViT-B/32 Transformer 24.7 25.0 91.2 18.7 21.2 21.8 56.7 15.2
Base
CLOSE(2022) ViT-L/14 T5 - - 95.3 - - - - -
base
ViECap(2023) ViT-B/32 GPT-2 27.2 24.8 92.9 18.2 21.4 20.1 47.9 13.6
Base
MeaCap (2024) ViT-B/32 GPT-2 27.2 25.3 95.4 19.0 22.3 22.3 59.4 15.6
InvLM Base
Knight(2023) RN50x64 GPT-2 27.8 26.4 98.9 19.6 22.6 24.0 56.3 16.3
Large
ICSDâ™  (2023) ViT-B/32 BERT 29.9 25.4 96.6 - 25.2 20.6 54.3 -
Base
SynTICâ™ â€  (2023) ViT-B/32 TransformerL=4 29.9 25.8 101.1 19.3 22.3 22.4 56.6 16.6
H=4
IFCap ViT-B/32 GPT-2 30.8 26.7 108.0 20.3 23.5 23.0 64.4 17.0
Base
Table1: ResultontheIn-domaincaptioningincludingCOCOtestsplitandFlickr30ktestsplit. Everyresultis
copiedfromtheoriginalpapers. â™ : Utilizestext-to-imagegenerationmodelinthetrainingtime,â€ : Utilizesobject
detectorduringthetrainingandinferencetime. IFCapachievesstate-of-the-artinmostmetrics. Thebestnumber
overallisinboldandthesecondbestinunderline.
COCO=â‡’Flickr Flickr=â‡’COCO COCO=â‡’NoCapsVal
Method
B@4 M C S B@4 M C S Method In Near Out Entire
DeCap(2023) 16.3 17.9 35.7 11.1 12.1 18.0 44.4 10.9 C S C S C S C S
ViECap(2023) 17.4 18.0 38.4 11.2 12.6 19.3 54.2 12.5
DeCap(2023) 65.2 - 47.8 - 25.8 - 45.9 -
Knight(2023) 21.1 22.0 48.9 14.2 19.0 22.8 64.4 15.1
CapDec(2022) 60.1 10.2 50.2 9.3 28.7 6.0 45.9 8.3
SynTIC(2023) 17.9 18.6 38.4 11.9 14.6 19.4 47.0 11.9
ViECap(2023) 61.1 10.4 64.3 9.9 65.0 8.6 66.2 9.5
SynTIC-TT 19.4 20.2 43.2 13.9 20.6 21.3 64.4 14.3
IFCapâ‹† 17.8 19.4 47.5 12.7 14.7 20.4 60.7 13.6 IFCapâ‹† 70.1 11.2 72.5 10.9 72.1 9.6 74.0 10.5
IFCap-TT 21.2 21.8 59.2 15.6 19.0 23.0 76.3 17.3
Table 3: Results on the NoCaps validation split. â‹†:
Table2: ResultsontheCross-domaincaptioning. âˆ’TT: withoutEntityFilteringmoduleintheinferencetime.
models can access to target domainâ€™s corpus during IFCapachievesstateoftheartineverymetric.
inferencetime. â‹†: withoutEntityFilteringmodulein
the inferencetime. IFCap achieves state-of-the-artin
mostmetrics.
theMappingNetwork,whichconsistsof8layered
transformers(Vaswanietal.,2017).
as the text decoder. Parameters in the image en- Frequency-basedEntityFiltering: Fromthein-
coder are frozen during training, and the text de- put image, we retrieve l sentences and extracted
coder and Fusion Module are trained. We train a nounstoobtainfrequencyF. Withthepredefined
total of 5 epochs, learning rate of 2 Ã— 10âˆ’5, use threshold,wefilterentitiesandbuildhardprompt
schedulerforlearningratescheduler,AdamWopti- h,providingmoreaccurateanddiverseentitiesto
mizer(KingmaandBa,2014), andsetbatchsize thecaptiondecoder.
80. WeuseasingleNVIDIARTX4090with24GB Datasets, metrics We evaluate our model in
VRAM;ittakesaboutanhouranduses12GBof human-annotateddatasets. Forin-domaingeneral-
VRAMduringtraining. ization, we test our model on MS-COCO (Chen
Image-like Retrieval: We first discover ade- et al., 2015), Flickr30k (Young et al., 2014),
quate Ïƒ r for Image-like Retrieval. Based on our and utilize Karpathy split (Karpathy and Fei-
experiment(Fig.5),wechooseÏƒ r as0.04inmost Fei, 2015). Also, to check the modelâ€™s perfor-
cases. Weretrieveksentenceswithnoise-injected mance in the unseen scenarios, we use the No-
inputtextfeatureT e. Caps (Agrawal et al., 2019) validation set. For
Fusion Module: We project T âˆˆ Rd and metrics,weusecommonimagecaptioningmetrics
e
R
e
âˆˆ RdÃ—k with f l1, f
l2
into Rdgpt, RdgptÃ—k respec- CIDEr(Vedantametal.,2015),SPICE(Anderson
tively where d is the CLIP dimension and d is etal.,2016),BLEU@n(Papinenietal.,2002),and
gpt
thedimensionofGPT-2embeddingspace. Weuse METEOR (Banerjee and Lavie, 2005). More de-
projected T as query and R as key in f layer. tailsaboutdatasetsandmetricsareincludedinthe
e e Att
Finally, F and Î¸ are concatenated and fed into appendix(Sec.A).
e qMSR-VTT MSVD DesignChoice COCO
Method Pre-Ïµ Post-Ïµ Retrieval
B@4 M C S B@4 M C S Reference B@4 M C S
ZeroCap(2022b) 2.3 12.9 5.8 - 2.9 16.3 9.6 - ViECap 27.2 24.8 92.9 18.2
MAGIC(2022) 5.5 13.3 7.4 4.2 6.6 16.1 14.0 2.9 Smallcap âœ“ 23.5 24.2 88.5 18.2
Knight âœ“ âœ“ 26.0 24.6 92.9 18.3
CLMs(2022) 6.2 17.8 10.1 6.5 7.0 16.4 20.0 3.1
Knight+ILR âœ“ âœ“ âœ“ 27.2 25.0 93.9 18.3
CapDec(2022) 8.9 23.7 11.5 5.9 7.9 23.3 34.5 3.2
IFCap âœ“ âœ“ 28.5 26.0 102.0 20.0
EPT(2022a) 3.0 14.6 11.3 - 3.0 17.8 17.4 -
Knight(2023) 25.4 28.0 31.9 8.5 37.7 36.1 63.8 5.0
Table6: ImportanceofnoiseinjectiontimingofImage-
IFCap 27.1 25.9 38.9 6.7 40.6 34.2 83.9 6.3
like Retrieval. Pre-Ïµ refers to noise injection before
retrieval,andPost-Ïµreferstonoiseinjectiontoretrieved
Table 4: Results on the Video captioning including
features.
MSR-VTTandMSVD.IFCapachievesstate-of-the-art
inmostmetrics.
kretrieved COCO
sentences B@4 M C S
Image-like Fusion Entity COCO
Retrieval Module Filtering B@4 M C S 3 28.1 25.7 100.0 19.5
âœ“ âœ“ âœ“ 30.8 26.7 108.0 20.3 5 28.5 26.0 102.0 20.0
âœ“ âœ“ 29.2 26.0 104.0 19.9 7 28.2 26.0 101.7 19.8
âœ“ âœ“ 28.5 26.0 102.0 20.0
âœ“ 27.2 24.7 97.3 18.5 Table 7: Ablation studies of the number of retrieved
âœ“ 27.7 25.6 99.0 19.4 captionskforFusionModule.
27.2 24.8 92.9 18.2
ingtime(Maetal.,2023;Liuetal.,2023). Also,
Table 5: Ablation studies of the key components of
IFCap. inFlickr30k,IFCapshowsdecentperformancein
BLEU@4 and METEOR and achieves the best
scoresinCIDErandSPICE.
4.2 Text-onlyCaptioning
4.4 Cross-domainCaptioning
We compare our model with other state-
of-the-art text-only image captioning models. WevalidateIFCapâ€™stransferabilitythroughdiverse
CapDec(Nukraietal.,2022)andViECap(Feietal., domains,includingtheNoCapsvalidationsetand
2023)arebasedonClipcap(Mokadyetal.,2021). cross-domainfromCOCOâ†’Flickr30kandvice
They use predefined Gaussian noise for aligning versa. InNoCaps,weusethesamemodeltrained
text and image features. Similarly, CLOSE (Gu intheCOCOdomaintotesthowthemodelrecog-
et al., 2022) uses various noise settings, and De- nizesunseenobjectsduringtraining. IntheNoCaps
Cap (Li et al., 2023) uses a memory bank. And validationsplit,ourIFCapperformsthebestinev-
a recent approach to text-only image captioning, erymetricandeverydomaincomparedtoprevious
Knight (Wang et al., 2023) only utilizes text state-of-the-arttext-onlyimagecaptioningmodels
features with a retrieval mechanism, also Mea- (Lietal.,2023;Nukraietal.,2022;Feietal.,2023).
Cap (Zeng et al., 2024) processes retrieved sen- Also,incross-domainsettingsbetweenCOCOand
tences into Subject-Predicate-Object triplets and Flickr30k,IFCapwinsstate-of-the-artinmostmet-
employsthemasadditionalinformation. ICSD(Ma ricsandthesecondbestinsomemetrics.
etal.,2023)andSynTIC(Liuetal.,2023)utilize
text-to-imagegenerationmodelslikeStableDiffu- 4.5 VideoCaptioning
sion(Rombachetal.,2022)toclosethegap.
Invideocaptioning,wetrainourmodelinthesame
manneraspreviousexperiments. First,weperform
4.3 In-domainCaptioning
Image-likeRetrievalonthecorpusfromeachvideo
We benchmark our IFCap on in-domain settings captioning dataset MSVD (Wu et al., 2017) and
in Table 1 including COCO and Flickr30k. We MSR-VTT (Xu et al., 2016). For inference time,
compareourmethodswithpreviousstate-of-the-art we sample 5 images from input video and calcu-
in text-only image captioning. Our IFCap domi- latetheaverageoftheirCLIPimagefeatures. We
nateseverymetricintheCOCOdatasetcompared also retrieve 5 sentences from each sampled im-
to models that utilize larger models (Gu et al., age, 25intotal, andalsocalculatetheaverageof
2022;Wangetal.,2023)andhavecomplextrain- CLIP text features per image. Most of the met-Transformer Cross-Attention COCO COCO Flickr30k
Ï„
#Layers #Layers B@4 M C S B@4 M C S B@4 M C S
1 23.9 24.6 86.9 17.8 1 6.5 18.7 6.4 17.0 6.8 18.9 3.9 15.4
1
4 26.2 24.4 92.8 18.0 2 21.4 26.5 80.3 21.0 18.9 23.4 52.2 17.9
1 27.4 24.9 95.0 18.5 3 28.1 26.8 103.6 21.1 23.5 23.0 64.4 17.0
2
4 26.4 24.9 95.5 18.4 4 30.2 26.7 107.7 20.7 23.8 22.3 61.1 15.9
5 30.8 26.7 108.0 20.3 23.8 21.9 59.1 15.3
1 27.4 25.5 99.7 19.1
4 6 30.4 26.4 106.2 19.9 23.6 21.7 57.3 15.0
4 27.9 25.8 99.1 19.4
7 30.0 26.1 104.6 19.6 23.6 21.6 56.5 14.8
1 28.3 26.0 102.0 20.0
8 8 29.8 26.0 103.4 19.4 23.7 21.6 55.9 14.7
4 28.4 25.7 100.6 19.5
Table10: AblationstudiesofheuristicthresholdÏ„of
Table8: Ablationstudiesofthenumberoftransformer
EntityFiltering.
layersandcross-attentionlayersoftheFusionModule.
COCO Flickr30k
lretrieved COCO Flickr Ï„ adap B@4 M C S B@4 M C S
sentences B@4 M C S B@4 M C S
Lognormal(Âµ,Ïƒ2)
5 29.9 26.4 106.1 20.2 23.5 22.2 61.9 16.0
Âµ 22.0 26.6 83.8 21.1 19.0 23.4 52.7 17.9
7 30.3 26.5 107.2 20.3 23.5 23.0 64.4 17.0 Âµ+Ïƒ 29.1 26.7 106.6 20.7 22.0 22.9 63.0 17.2
9 30.8 26.7 108.0 20.3 23.4 22.6 62.9 16.6
Âµ+2Ïƒ 29.6 26.1 103.5 19.6 23.3 21.8 58.1 15.3
Table 9: Ablation studies of the number of retrieved
N(Âµ,Ïƒ2)
Âµ 24.9 26.7 95.9 21.1 19.2 23.2 55.6 17.7
sentenceslforEntityFiltering.
Âµ+Ïƒ 30.1 26.6 107.5 20.4 22.3 22.5 62.3 16.4
Âµ+2Ïƒ 29.8 26.2 104.7 19.7 23.4 21.9 58.5 15.5
ricsinbothdatasets,IFCap,fulfillsstate-of-the-art Best(H) 30.8 26.7 108.0 20.3 23.5 23.0 64.4 17.0
performance,exceptMETEOR.
Table11: AblationstudiesofadaptivethresholdÏ„ of
adap
4.6 AblationStudy EntityFiltering.
We conduct extensive experiments to identify
the impact of each key component in IFCap, videocaptioning,refertoTable14.
Image-likeRetrieval(ILR),FusionModule(FM), Image-like Retrieval: It is crucial to identify
and Frequency-based Entity Filtering(EF). Also, adequate timing for injecting noise into text fea-
for each component, we search the best hyper- tures for successful text-to-text retrieval that imi-
parameterintheCOCOtestsplitwithanin-domain tatesimage-to-textretrieval. Wecansplitinjecting
setting. timingintoPre-Ïµ andPost-Ïµ. Wefindthatourset-
Key Components: We check the strength of tingwhichinjectsnoisebeforeperformingretrieval
eachcomponentbydetachingfromourbestmodel, is the best among all possible combinations. We
whichconsistsofall3componentsTable5. First, canverifythisinTable6. Thefirstcolumnofthe
removingFM,wesimplyconcatenatetheinputtext table indicates how the model performs retrieval,
featureandretrievedfeaturesafterapplyingdimen- just for easy understanding of noise injection in
sionmappinglayers f and f andpassingittothe retrieval.
l1 l2
captiondecoder. RemovingEFissimplyapplying Fusion Module: We utilize a cross-attention
entityextractionviaCLIPclassifierlike(Feietal., layer and transformer layer for mapping the net-
2023) does. Demounting ILR makes inaccessi- work. In Table 8, we try multiple combinations
bletoretrievalfeaturessolelyusinginputfeatures; ofthenumberofeachlayer. Themorelayerswe
henceFMcanâ€™texistwithoutILR.Addingmore use, the more performance gain we can get until
components into the baseline, we can explicitly the number of transformer layers is 4. The per-
notice performance improvement. So, using all formance gain is also observed when we use 8
threekeycomponentsconstitutesastate-of-the-art transformer layers but it is so slight. Increasing
model,whichisIFCap. NotethatIFCaphas2vari- the number of cross-attention layers is effective
ants,IFCapandIFCapâ‹†,withEFandwithoutEF when the transformer layer is small, but the ten-
respectively. To see a full comparison of various dency does not last while the transformer layer
datasets,includingin-domain,cross-domain,and grows. Weconcludeusing8transformerlayersand5 Conclusion
B@4
1.05
M
1.00 C In this paper, we propose a zero-shot caption-
S ing method, IFCap, through text-only training.
0.95
IFCap performs Image-like Retrieval to address
0.90
the gap between image-to-text retrieval and text-
0.85
to-textretrieval,FusionModuleforinteractionbe-
tweenexistingandadditionalrepresentations,and
0.30
Frequency-basedEntityFilteringduringinference
0.25 timetoextractfrequentlyoccurringentitiesfrom
0.20 the retrieved sentences. Our method can be eas-
0.15 ilyappliedtovarioustasksandprovidesvaluable
0.10 guidanceforretrieval-basedmethodsinatext-only
0 0.00010.001 0.005 0.01 0.016 0.02 0.03 0.04 0.05 0.1 0.5 1
setting. It offers clear and precise information
Figure5: Hyper-parametersearchforfindingbestÏƒ
r
usedinImage-likeRetrieval. Allexperimentsarecon- toLLMswithoutrelyingonalimitedvocabulary.
ducted with the COCO test set. The X-axis denotes ThesimplicityandrobustnessofIFCaparedemon-
Ïƒ2, and the Y-axis denotes scores of commonly used stratedthroughstate-of-the-artperformanceacross
r
captioningmetricsBLEU@4(B@4), METEOR(M), various datasets in image and video captioning.
CIDEr(C),andSPICE(S).
The future direction of our method includes the
extensionofourmethodonmorecomplexdatasets,
suchasregion-basedcaptioning(Kimetal.,2019a,
asinglecross-attentionlayershowsthebestperfor-
2021) or visual question answering (Cho et al.,
mance. For a fair comparison, we detach the EF
2023a,b),whichsufferfromdataissues.
module. Also,thenumberofretrievedcaptionsis
crucial. Weconductablationstudiestofindoptimal
6 Limitations
k,whichcanbefoundinTable7.
Frequency-basedEntityFiltering: Weneedto WedemonstratethatIFCapexhibitssuperiorperfor-
choose1)howmanyretrievedsentencesl,touse manceacrossvariousimagecaptioningandvideo
and 2) the threshold Ï„, for filtering nouns for EF captioning datasets compared to other zero-shot
toextractaccurateanddiverseentities. Theformer image captioning models with text-only training.
canbefoundinTable9, notethatindifferentdo- However, the optimal value of Ïµ for Image-like
r
mains,optimallmayvary. FortheCOCOdomain, Retrieval currently requires a heuristic approach
usinglas9showsthebestperformance,while7is todetermine. Weleavethetaskoffindingamore
thebestinFlickr30k. convenientmethodfordeterminingtheoptimalÏµ
r
Wefindthebestthresholdsettinginaheuristic asfutureworktofurtherimproveimagecaptioning
andadaptiveway. IntheformercaseTable10,we modelswithtext-onlytraining.
set Ï„ ranging from 1 to 8, which is the minimum
and maximum value of the given setting. Above Acknowledgements
8, performance freeze due to none of the entities
ThiswaspartlysupportedbytheInstituteofInfor-
being retrieved. In the COCO test, we use l = 9
mation&CommunicationsTechnologyPlanning&
andl = 7intheFlickr30ktestsplit. Wenoticethat
Evaluation(IITP)grantfundedbytheKoreangov-
each domain has different optimal Ï„, COCO at 5
ernment(MSIT)(No.RS-2020-II201373,Artificial
andFlickr30kat3fortheCIDErscore. Incontrast
Intelligence Graduate School Program(Hanyang
totheheuristicway,wecanassumesuchdistribu-
University))andtheNationalResearchFoundation
tion exists from frequencies F. We try Gaussian
ofKorea(NRF)grantfundedbytheKoreagovern-
distribution and Log-normal distribution with Âµ,
ment(MSIT)(No. RS-2023-00245661).
Âµ+Ïƒ, and Âµ+2Ïƒ, capturing upper 50%, 15.8%,
and2.2%basedonthefrequencyofentity. InTa-
ble11,weobserveÏ„ adap = Âµ+Ïƒalmostreproduces References
theperformanceofglobaloptimalintheheuristic
HarshAgrawal,KaranDesai,YufeiWang,XinleiChen,
threshold. If ground truth does not exist or com-
Rishabh Jain, Mark Johnson, Dhruv Batra, Devi
puting resource is limited, the adaptive threshold
Parikh,StefanLee,andPeterAnderson.2019. No-
becomesattractive. caps: Novelobjectcaptioningatscale. InProceed-ings of the IEEE/CVF international conference on Dong-JinKim,JinsooChoi,Tae-HyunOh,andInSo
computervision,pages8948â€“8957. Kweon.2019a. Denserelationalcaptioning: Triple-
stream networks for relationship-based captioning.
PeterAnderson,BasuraFernando,MarkJohnson,and InProceedingsoftheIEEEConferenceonComputer
Stephen Gould. 2016. Spice: Semantic proposi- VisionandPatternRecognition,pages6271â€“6280.
tionalimagecaptionevaluation. InComputerVisionâ€“
ECCV2016: 14thEuropeanConference,Amsterdam, Dong-JinKim,JinsooChoi,Tae-HyunOh,andInSo
TheNetherlands,October11-14,2016,Proceedings, Kweon.2019b. Imagecaptioningwithveryscarce
PartV14,pages382â€“398.Springer. superviseddata: Adversarialsemi-supervisedlearn-
ingapproach. InProceedingsofthe2019Conference
SatanjeevBanerjeeandAlonLavie.2005. Meteor: An onEmpiricalMethodsinNaturalLanguageProcess-
automaticmetricformtevaluationwithimprovedcor- ing and the 9th International Joint Conference on
relationwithhumanjudgments. InProceedingsof NaturalLanguageProcessing(EMNLP-IJCNLP).
theaclworkshoponintrinsicandextrinsicevaluation
measuresformachinetranslationand/orsummariza- Dong-JinKim,Tae-HyunOh,JinsooChoi,andInSo
tion,pages65â€“72. Kweon. 2021. Dense relational image captioning
viamulti-tasktriple-streamnetworks. IEEETransac-
StevenBirdandEdwardLoper.2004. NLTK:Thenatu- tionsonPatternAnalysisandMachineIntelligence,
rallanguagetoolkit. InProceedingsoftheACLIn- 44(11):7348â€“7362.
teractivePosterandDemonstrationSessions,pages
Dong-JinKim,Tae-HyunOh,JinsooChoi,andInSo
214â€“217,Barcelona,Spain.AssociationforCompu-
Kweon.2024. Semi-supervisedimagecaptioningby
tationalLinguistics.
adversariallypropagatinglabeleddata. IEEEAccess.
Nicolas Carion, Francisco Massa, Gabriel Synnaeve,
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
Nicolas Usunier, Alexander Kirillov, and Sergey
methodforstochasticoptimization. arXivpreprint
Zagoruyko.2020. End-to-endobjectdetectionwith
arXiv:1412.6980.
transformers. InEuropeanconferenceoncomputer
vision,pages213â€“229.Springer.
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Petroni,VladimirKarpukhin,NamanGoyal,Hein-
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
richKÃ¼ttler, MikeLewis, Wen-tauYih, TimRock-
ishna Vedantam, Saurabh Gupta, Piotr DollÃ¡r, and
tÃ¤schel,etal.2020. Retrieval-augmentedgeneration
C.LawrenceZitnick.2015. MicrosoftCOCOcap-
forknowledge-intensivenlptasks. AdvancesinNeu-
tions: Datacollectionandevaluationserver. arXiv
ralInformationProcessingSystems,33:9459â€“9474.
preprintarXiv:1504.00325.
Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang.
Jae Won Cho, Dawit Mureja Argaw, Youngtaek Oh,
2023. Decap: Decoding clip latents for zero-shot
Dong-JinKim,andInSoKweon.2023a. Empirical
captioning via text-only training. arXiv preprint
studyonusingadaptersfordebiasedvisualquestion
arXiv:2303.03032.
answering. ComputerVisionandImageUnderstand-
ing,237:103842.
VictorWeixinLiang, YuhuiZhang, YongchanKwon,
SerenaYeung,andJamesYZou.2022. Mindthegap:
Jae Won Cho, Dong-Jin Kim, Hyeonggon Ryu, and
Understandingthemodalitygapinmulti-modalcon-
In So Kweon. 2023b. Generative bias for robust
trastiverepresentationlearning. AdvancesinNeural
visual question answering. In Proceedings of the
InformationProcessingSystems,35:17612â€“17625.
IEEE/CVFConferenceonComputerVisionandPat-
ternRecognition,pages11681â€“11690.
Zhiyue Liu, Jinyuan Liu, and Fanrong Ma. 2023.
Improving cross-modal alignment with synthetic
Junjie Fei, Teng Wang, Jinrui Zhang, Zhenyu He, pairs for text-only image captioning. Preprint,
Chengjie Wang, and Feng Zheng. 2023. Transfer- arXiv:2312.08865.
abledecodingwithvisualentitiesforzero-shotim-
age captioning. In Proceedings of the IEEE/CVF Ziyang Luo, Zhipeng Hu, Yadong Xi, Rongsheng
InternationalConferenceonComputerVision,pages Zhang,andJingMa.2023. I-tuning: Tuningfrozen
3136â€“3146. languagemodelswithimageforlightweightimage
captioning. In ICASSP 2023-2023 IEEE Interna-
SophiaGu,ChristopherClark,andAniruddhaKemb- tionalConferenceonAcoustics,SpeechandSignal
havi.2022. Icanâ€™tbelievethereâ€™snoimages! learn- Processing(ICASSP),pages1â€“5.IEEE.
ing visual tasks using only language supervision.
arXivpreprintarXiv:2211.09778. FeipengMa,YizhouZhou,FengyunRao,YueyiZhang,
andXiaoyanSun.2023. Imagecaptioningwithmulti-
Andrej Karpathy and Li Fei-Fei. 2015. Deep visual- contextsyntheticdata. Preprint,arXiv:2305.18072.
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference on RonMokady,AmirHertz,andAmitHBermano.2021.
computervisionandpatternrecognition,pages3128â€“ Clipcap: Clip prefix for image captioning. arXiv
3137. preprintarXiv:2111.09734.DavidNukrai,RonMokady,andAmirGloberson.2022. Junyang Wang, Ming Yan, Yi Zhang, and Jitao Sang.
Text-onlytrainingforimagecaptioningusingnoise- 2023. From association to generation: Text-only
injectedclip. arXivpreprintarXiv:2211.00575. captioning by unsupervised cross-modal mapping.
arXivpreprintarXiv:2304.13273.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu- JunyangWang,YiZhang,MingYan,JiZhang,andJitao
ationofmachinetranslation. InProceedingsofthe Sang.2022. Zero-shotimagecaptioningbyanchor-
40thannualmeetingoftheAssociationforComputa- augmentedvision-languagespacealignment. arXiv
tionalLinguistics,pages311â€“318. preprintarXiv:2211.07275.
ZuxuanWu,TingYao,YanweiFu,andYu-GangJiang.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
2017. Deep learning for video classification and
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
captioning,page3â€“29. AssociationforComputing
try, Amanda Askell, Pamela Mishkin, Jack Clark,
MachineryandMorgan&Claypool.
etal.2021. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconfer-
JunXu,TaoMei,TingYao,andYongRui.2016. Msr-
enceonmachinelearning,pages8748â€“8763.PMLR.
vtt: A large video description dataset for bridging
videoandlanguage. InProceedingsoftheIEEEcon-
AlecRadford,JeffreyWu,RewonChild,DavidLuan,
ferenceoncomputervisionandpatternrecognition,
DarioAmodei,IlyaSutskever,etal.2019. Language
pages5288â€“5296.
modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
AaronCourville,RuslanSalakhudinov,RichZemel,
RitaRamos,BrunoMartins,DesmondElliott,andYova and Yoshua Bengio. 2015. Show, attend and tell:
Kementchedjhieva.2023. Smallcap: lightweightim- Neural image caption generation with visual atten-
age captioning prompted with retrieval augmenta- tion. InInternationalconferenceonmachinelearn-
tion. In Proceedings of the IEEE/CVF Conference ing,pages2048â€“2057.PMLR.
onComputerVisionandPatternRecognition,pages
2840â€“2849. PeterYoung,AliceLai,MicahHodosh,andJuliaHock-
enmaier. 2014. From image descriptions to visual
RobinRombach,AndreasBlattmann,DominikLorenz, denotations: Newsimilaritymetricsforsemanticin-
Patrick Esser, and BjÃ¶rn Ommer. 2022. High- ferenceovereventdescriptions. Transactionsofthe
resolutionimagesynthesiswithlatentdiffusionmod- AssociationforComputationalLinguistics,2:67â€“78.
els. In Proceedings of the IEEE/CVF conference
Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen,
oncomputervisionandpatternrecognition, pages
Zhengjue Wang, and Bo Chen. 2024. Meacap:
10684â€“10695.
Memory-augmented zero-shot image captioning.
arXivpreprintarXiv:2403.03715.
Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani
Yogatama, Yan Wang, Lingpeng Kong, and Nigel
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei
Collier.2022. Languagemodelscansee: Plugging
Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and
visual controls in text generation. arXiv preprint
Jianfeng Gao. 2021. Vinvl: Revisiting visual rep-
arXiv:2205.02655.
resentations in vision-language models. Preprint,
arXiv:2101.00529.
YoadTewel,YoavShalev,RoyNadler,IdanSchwartz,
and Lior Wolf. 2022a. Zero-shot video caption-
ing with evolving pseudo-tokens. arXiv preprint A Image-likeRetrieval
arXiv:2207.11100.
YoadTewel,YoavShalev,IdanSchwartz,andLiorWolf.
COCO
2022b. Zerocap: Zero-shotimage-to-textgeneration Method
for visual-semantic arithmetic. In Proceedings of B@4 M C S
theIEEE/CVFConferenceonComputerVisionand
Knight 27.8 26.4 98.9 19.6
PatternRecognition,pages17918â€“17928.
Knight+ILR 29.8 25.6 102.7 19.7
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Table12: EffectofImage-likeRetrievalonKnight.
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing
systems,30.
HyperParameters COCO Flickr30k NoCaps MSVD MSR-VTT
RamakrishnaVedantam,CLawrenceZitnick,andDevi Epochs 5 30 - 10 10
Parikh. 2015. Cider: Consensus-based image de- l 9 7 7 7 7
Ï„ 5 3 3 5 6
scription evaluation. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecogni-
Table13: Hyperparametertable.
tion,pages4566â€“4575.Inâˆ’domain Crossâˆ’domain VideoCaptioning
COCO Flickr
COCO=â‡’NoCapsVal
COCO=â‡’Flickr Flickr=â‡’COCO MSR-VTT MSVD
Method In Near Out Entire
C S C S C S C S C S C S C S C S C S C S
ViECap 92.9 18.2 47.9 13.6 61.1 10.4 64.3 9.9 65.0 8.6 66.2 9.5 38.4 11.2 54.2 12.5 - - - -
Knight 98.9 19.6 56.3 16.3 - - - - - - - - 48.9 14.2 64.4 15.1 31.9 8.5 63.8 5.0
IFCapâ‹† 102.0 20.0 59.8 15.8 70.1 11.2 72.5 10.9 72.1 9.6 74.0 10.5 47.5 12.7 60.7 13.6 20.8 4.1 40.2 3.4
IFCap 108.0 20.3 64.4 17.0 75.8 12.4 72.3 11.6 60.2 8.9 70.5 10.8 59.2 15.6 76.3 17.3 38.9 6.7 83.9 6.3
Table14: OverallcomparisonamongbaselinesandIFCap. â‹†: withoutEntityFilteringmoduleintheinferencetime.
WeobservethatImage-likeRetrievalisalsoap-
plicable to other models that employ text-to-text
retrieval(Wangetal.,2023). BasedonFig.5,we
perform ILR with Ïµ = 0.04 in the training time
r
ofKnight. IntheCOCOtestset,everymetricex-
cept METEOR is improved compared to vanilla
Knight(Wangetal.,2023),verifyingtheeffective-
nessofourILR.
B Hyperparameter
We include the details about our experiments in
eachdatasetinTable13.
C ComparisonwithBaselines
Wecomparebaselines(Feietal.,2023;Wangetal.,
2023) with IFCap and IFCapâ‹† in every domain,
includingin-domaincaptioning,cross-domaincap-
tioning,andvideocaptioning. Resultscanbefound
inTable14.
D QualitativeResults
WeshowadditionalqualitativeresultsinFig.6.Knight: Asilverpassengertraintraveling Knight: Amanwithabeardand adogona Knight: Aviewof amountainrangewith
downatracknexttoanelevatedwalkway. couch. anairplaneinthebackground.
ViECap: Acarisshowninfrontof alarge ViECap: Amanstandingnexttoabrown ViECap: Alargeairplaneflyingthrougha
billboard. and whitedog. bluesky.
ViECap entity: [] ViECap entity: [dog] ViECap entity: [airplane]
IFCap: Amonorailtraintravelingdown IFCap: Amanand adogaresmilinginfront IFCap: The wingof anairplanewith
tracksnexttoabuilding. of aChristmastree. mountainsinthebackground.
IFCap entity: [monorail, train] IFCap entity: [man, dog] IFCap entity: [mountain, wing, airplane]
GT:Amonorailmakingit'swaydownthe GT:Amaninfrontof aChristmastreewith GT: The viewoutof anairplanewithpartof
trackaboveabunchof cars. hisdog. thewing.
Knight: Agiraffestandingnexttoalarge Knight: Agroupof menracingeachother Knight: Amotorcycleisparkedontheside
tree. onacourse. of theroadnexttoatree.
ViECap: Twogiraffesstandingnexttoeach ViECap: Askierinaredjacketisskiing ViECap: Abasketfullof bananashanging
otherinagrassyarea. downahill. fromatree.
ViECap entity: [giraffe] ViECap entity: [skis] ViECap entity: []
IFCap: Agiraffestandingnexttoatreein IFCap: Twocrosscountryskiersracing IFCap: Amotorcyclethatissittingontopof
thewater. downahill. afence.
IFCap entity: [giraffe, tree, water] IFCap entity: [country] IFCap entity: [motorcycle]
GT: Agiraffeinafieldnexttotreeand body GT: Twocrosscountryskiersheadingonto GT: Amotorcyclesittingontopof afence
of water. thetrail. asdÃ©cor.
Knight: Agroupof trafficlightssittingon Knight: Abowlof fruitsittingontopof a Knight: Ablackvasewithawhiteflowerin
topof aroad. counter. it.
ViECap: Astreetfilledwithtrafficlights ViECap: Acloseupof fruitsand vegetables ViECap: Ablackand silverspoonwitha
nexttoatallbuilding. onatable. toothbrushinit.
ViECap entity: [trafficlight] ViECap entity: [] ViECap entity: [spoon]
IFCap: Abunchof trafficlightsatan IFCap: Acloseupof abowlof orangesand IFCap: Ablackand whitevasewitha
intersection. applesonacounter. flowerinit.
IFCap entity: [light, intersection] IFCap entity: [bowl, apple, orange, counter] IFCap entity: [vase, flower]
GT: A photo taken from one vehicle of GT: abowlof applesand abowlof oranges. GT: Thin black and white vase with black
another at an intersection. flowers.
Figure6: QualitativeresultontheCOCOtestset. Wehighlighttheretrievedentitiesandtheirappearanceinthe
generatedcaptionswith IFCap, ViECap and Intersection.