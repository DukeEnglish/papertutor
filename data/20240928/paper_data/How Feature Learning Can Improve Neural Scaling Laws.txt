Preprint
HOW FEATURE LEARNING CAN IMPROVE NEURAL
SCALING LAWS
BlakeBordelonâˆ—,AlexanderAtanasovâˆ—,CengizPehlevan
ABSTRACT
Wedevelopasolvablemodelofneuralscalinglawsbeyondthekernellimit. The-
oretical analysis of this model shows how performance scales with model size,
training time, and the total amount of available data. We identify three scaling
regimes corresponding to varying task difficulties: hard, easy, and super easy
tasks. For easy and super-easy target functions, which lie in the reproducing
kernelHilbertspace(RKHS)definedbytheinitialinfinite-widthNeuralTangent
Kernel(NTK),thescalingexponentsremainunchangedbetweenfeaturelearning
and kernel regime models. For hard tasks, defined as those outside the RKHS
oftheinitialNTK,wedemonstratebothanalyticallyandempiricallythatfeature
learningcanimprovescalingwithtrainingtimeandcompute,nearlydoublingthe
exponentforhardtasks.Thisleadstoadifferentcomputeoptimalstrategytoscale
parametersandtrainingtimeinthefeaturelearningregime. Wesupportourfind-
ingthatfeaturelearningimprovesthescalinglawforhardtasksbutnotforeasy
and super-easy tasks with experiments of nonlinear MLPs fitting functions with
power-lawFourierspectraonthecircleandCNNslearningvisiontasks.
1 INTRODUCTION
Deeplearningmodelstendtoimproveinperformancewithmodelsize,trainingtimeandtotalavail-
abledata. Thedependenceofperformanceontheavailablestatisticalandcomputationalresources
areoftenregularandwell-capturedbyapower-law(Hestnessetal.,2017;Kaplanetal.,2020). For
example,theChinchillascalinglaw(Hoffmannetal.,2022)forthelossL(t,N)ofaN-parameter
modeltrainedonlinefortsteps(orttokens)follows
L(t,N)=c tâˆ’rt +c Nâˆ’rN +L , (1)
t N âˆ
where the constants c ,c and exponents r ,r are dataset and architecture dependent and L
t N t N âˆ
representsthelowestachievablelossforthisarchitectureanddataset. Thesescalinglawsenablein-
telligentstrategiestoachieveperformanceunderlimitedcomputebudgets(Hoffmannetal.,2022)or
limiteddatabudgets(Muennighoffetal.,2023). Abetterunderstandingofwhatpropertiesofneural
networkarchitectures,parameterizationsanddatadistributionsgiverisetotheseneuralscalinglaws
couldbeusefultoselectbetterinitializationschemes,parameterizations,andoptimizers(Yangetal.,
2021;Achiametal.,2023;Everettetal.,2024)anddevelopbettercurriculaandsamplingstrategies
(Sorscheretal.,2022).
Despitesignificantempiricalresearch, apredictivetheoryofscalinglawsfordeepneuralnetwork
models is currently lacking. Several works have recovered data-dependent scaling laws from the
analysis of linear models (Spigler et al., 2020; Bordelon et al., 2020; Bahri et al., 2021; Maloney
etal.,2022;Simonetal.,2021;Bordelonetal.,2024a;Zavatone-Veth&Pehlevan,2023;Paquette
et al., 2024; Lin et al., 2024). However these models are fundamentally limited to describing the
kernelorlazylearningregimeofneuralnetworks(Chizatetal.,2019). Severalworkshavefound
thatthisfailstocapturethescalinglawsofdeepnetworksinthefeaturelearningregime(Fortetal.,
2020;Vyasetal.,2022;2023a;Bordelonetal.,2024a). Atheoryofscalinglawsthatcancapture
consistentfeaturelearningeveninaninfiniteparameterN â†’ âˆlimitisespeciallypressinggiven
the success of mean field and Âµ-parameterizations which generate constant scale feature updates
acrossmodelwidthsanddepths(Meietal.,2019;Geigeretal.,2020;Yang&Hu,2021;Bordelon
&Pehlevan,2022;Yangetal.,2022;Bordelonetal.,2023;2024b). Thetrainingdynamicsofthe
âˆ—EqualContribution
1
4202
peS
62
]LM.tats[
1v85871.9042:viXraPreprint
infinite width/depth limits in such models can significantly differ from the lazy training regime.
InfinitelimitswhichpreservefeaturelearningarebetterdescriptorsofpracticalnetworksVyasetal.
(2023a). Motivatedbythis,weaskthefollowing:
Question: Under what conditions can feature learning improve the scaling law exponents of
neuralnetworkscomparedtolazytrainingregime?
1.1 OURCONTRIBUTIONS
Inthiswork,wedevelopatheoreticalmodelofneuralscalinglawsthatallowsforimprovedscaling
exponentscomparedtolazytrainingundercertainsettings. Ourcontributionsare
1. Weproposeasimpletwo-layerlinearneuralnetworkmodeltrainedwithaformofprojected
gradientdescent. Weshowthatthismodelreproducespowerlawscalingsintrainingtime,
model size and training set size. The predicted scaling law exponents are summarized in
termsoftwoparametersrelatedtothedataandarchitecture(Î±,Î²),whichwerefertoasthe
sourceandcapacityexponentsfollowingCaponnetto&Vito(2005).
2. Weidentifyaconditiononthedifficultyofthelearningtask, measuredbythesourceex-
ponentÎ², underwhichfeaturelearningcanimprovethescalingofthelosswithtimeand
withcompute. Foreasytasks,whichwedefineastaskswithÎ² >1wheretheRKHSnorm
of the target is finite, there is no improvement in the power-law exponent while for hard
tasks(Î² < 1)thatareoutsidetheRKHSoftheinitiallimitingkernel,therecanbeanim-
provement. Forsuper-easytasksÎ² > 2âˆ’ 1,whichhaveverylowRKHSnorm,variance
Î±
from stochastic gradient descent (SGD) can alter the scaling law at large time. Figure 1
summarizestheseresults.
3. Weprovideanapproximatepredictionofthecomputeoptimalscalinglawsforhard,easy
tasksandsuper-easytasks. Eachoftheseregimeshasadifferentexponentforthecompute
optimalneuralscalinglaw. Table1summarizestheseresults.
4. Wetestourpredictedfeaturelearningscalingsbytrainingdeepnonlinearneuralnetworks
fitting nonlinear functions. In many cases, our predictions from the initial kernel spectra
accuratelycapturethetestlossofthenetworkinthefeaturelearningregime.
Dynamics, ğœ¸â†’ğŸ Dynamics, ğœ¸â‰«ğŸ
Rich Regime
1.2 Lazy Limit
1.0 ğ‘¡âˆ’1+1/ğ›¼
â†’ğ‘âˆ’ğ›¼ğ›½
ğ‘¡âˆ’1+1/ğ›¼
â†’ğ‘âˆ’ğ›¼ğ›½
0.8 ğ‘ ğ‘ ğœ¶ ğ‘¡âˆ’ğ›½â†’ğ‘âˆ’ğ›¼ğ›½ ğœ¶ ğ‘¡âˆ’2ğ›½/1+ğ›½ â†’ğ‘âˆ’ğ›¼ğ›½
0.6
SGD Noise SGD Noise
0.4
ğ‘¡âˆ’2+1/ğ›¼ ğ‘¡âˆ’1+1/ğ›¼ ğ‘¡âˆ’2+1/ğ›¼ ğ‘¡âˆ’1+1/ğ›¼
0.2 1 ğµ â†’ ğ‘ 1 ğµ â†’ ğ‘
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 1 0 1
Hard ğœ· Easy Hard ğœ· Easy
(a) lim L(t,N)âˆ¼tâˆ’Ï‡(Î²) (b) LazyLimit (c) RichRegime
Nâ†’âˆ
Figure 1: Our model changes its scaling law exponents for hard tasks, where the source is suf-
ficiently small Î² < 1. (a) The exponent Ï‡(Î²) which appears in the loss scaling L(t) âˆ¼ tâˆ’Ï‡(Î²)
of our model. (b)-(c) Phase plots in the Î±,Î² plane of the observed scalings that give rise to the
compute-optimaltrade-off. Arrows(â†’)representatransitionfromonescalingbehaviortoanother
astâ†’âˆ,wherethebalancingofthesetermsatfixedcomputeC =Ntgivesthecomputeoptimal
scalinglaw. InthelazylimitÎ³ â†’ 0,werecoverthephaseplotforÎ± > 1ofPaquetteetal.(2024).
AtnonzeroÎ³,however,weseethatthesetofâ€œhardtasksâ€,asgivenbyÎ² <1exhibitsanimproved
scalingexponent. ThecomputeoptimalcurvesfortheeasytaskswithÎ² >1areunchanged.
1.2 RELATEDWORKS
Our work builds on the recent results of Bordelon et al. (2024a) and Paquette et al. (2024) which
analyzedtheSGDdynamicsofastructuredrandomfeaturemodel. Staticsofthismodelhavebeen
analyzedbymanypriorworks(Atanasovetal.,2023;Simonetal.,2023;Zavatone-Veth&Pehlevan,
2023; Bahri et al., 2021; Maloney et al., 2022). These kinds of models can accurately describe
2
)
(Preprint
networksinthelazylearningregime. However,theempiricalstudyofVyasetal.(2022)andsome
experimentsinBordelonetal.(2024a)indicatethatthepredictedcomputeoptimalexponentswere
smallerthanthosemeasuredinnetworksthatlearnfeaturesonrealdata.Theselatterworksobserved
thatnetworkstrainfasterintherichregimecomparedtolazytraining. Wedirectlyaddressthisgap
inperformance betweenlazyandfeature learningneuralnetworks byallowingthekernel features
toadapttothedata. WerevisitthecomputervisionsettingsofBordelonetal.(2024a)andshowthat
ournewexponentsmoreaccuratelycapturethescalinglawinthefeaturelearningregime.
Otherworkhasinvestigatedwhenneuralnetworksoutperformkernels(Ghorbanietal.,2020). Ba
etal.(2022)andAbbeetal.(2023)haveshownhowfeaturelearningneuralnetworkscanlearnlow
rankspikesinthehiddenlayerweights/kernelstohelpwithsparsetaskswhilelazynetworkscannot.
Targetfunctionswithstaircaseproperties,wherelearningsimplercomponentsaidlearningofmore
complexcomponentsalsoexhibitsignificantimprovements(withrespecttoalargeinputdimension)
due to feature learning (Abbe et al., 2021; Dandi et al., 2023; Bardone & Goldt, 2024). Here, we
consideradifferentsetting. Weaskwhetherfeaturelearningcanleadtoimprovementsinpowerlaw
exponentsfortheneuralscalinglaw. TheworkofPaccolatetal.(2021)asksasimilarquestionin
thecaseofasimplestripemodel. Hereweinvestigatewhetherthepowerlawscalingexponentcan
be improved with feature learning in a model that only depends on properties of the initial kernel
andthetargetfunctionspectra.
2 SOLVABLE MODEL OF SCALING LAWS WITH FEATURE LEARNING
Westartbymotivatinganddefiningourmodel. Ourgoalsistobuildasimplemodelthatexhibits
feature learning in the infinite-width limit but also captures finite network size, finite batch SGD
effects,andsamplesizeeffectsthatcansignificantlyalterscalingbehavior.
Following the notation of Bordelon et al. (2024a), we introduce our model from the perspective
of kernel regression. We first assume a randomly initialized neural network in an infinite-width
limitwheretheneuraltangentkernel(NTK)concentrates. Wethendiagonalizetheinitialinfinite-
width NTK. The resulting eigenfunctions Ïˆ (x) âˆˆ RM have an inner product that define the
âˆ
infinite-widthNTKK (x,xâ€²)=Ïˆ (x)Â·Ïˆ (xâ€²). Theseeigenfunctionsareorthogonalunderthe
âˆ âˆ âˆ
probabilitydistributionofthedatap(x)with
(cid:10)
Ïˆ (x)Ïˆ
(x)âŠ¤(cid:11)
=Î›=diag(Î» ,...,Î» ). (2)
âˆ âˆ xâˆ¼p(x) 1 M
WewilloftenconsiderthecasewhereM â†’âˆfirstsothatthesefunctionsÏˆ (x)formacomplete
âˆ
basisforthespaceofsquareintegrablefunctions.
WenextconsiderafinitesizedmodelwithN parameters.Weassumethismodelâ€™sinitialparameters
aresampledfromthesamedistributionastheinfinitemodelandthattheN â†’âˆlimitrecoversthe
samekernelK (x,xâ€²). ThefiniteN-parametermodel,atinitializationt=0,hasN eigenfeatures
âˆ
ÏˆËœ(x,0)âˆˆRN. Inthefeaturelearningregime,thesefeatureswillevolveduringtraining.
The finite networkâ€™s learned function f is expressed in terms of the lower dimensional features,
while the target function y(x) can be decomposed in terms of the limiting (and static) features
Ïˆ (x)withcoefficientswâ‹†. TheinstantaneousfinitewidthfeaturesÏˆËœ(x,t)canalsobeexpanded
âˆ
asalinearcombinationofthebasisfunctionsÏˆ (x)withcoefficientmatrixA(t) âˆˆ RNÃ—M. We
âˆ
canthereforeviewourmodelasthefollowingstudent-teachersetup
1
f(x,t)= w(t)Â·ÏˆËœ(x,t), ÏˆËœ(x,t)=A(t)Ïˆ (x)
N âˆ (3)
y(x)=wâ‹†Â·Ïˆ (x).
âˆ
IfthematrixA(t)israndomandstaticthengradientdescentonthisrandomfeaturemodelrecovers
thestochasticprocessanalyzedbyBordelonetal.(2024a);Paquetteetal.(2024). Inthiswork,we
extendtheanalysistocaseswherethematrixA(t)isalsoupdated. Weconsideronlinetrainingin
themaintextbutdiscussandanalyzethecasewheresamplesarereusedinAppendixD.
Wealloww(t)toevolvewithstochasticgradientdescent(SGD)andA(t)evolvebyprojectedSGD
onameansquareerrorwithbatchsizeB. LettingÎ¨ (t) âˆˆ RBÃ—M representarandomlysampled
âˆ
batch of B points evaluated on the limiting features {Ïˆ (x )}B and Î· to be the learning rate,
âˆ Âµ Âµ=1
3Preprint
ourupdatestaketheform
(cid:18) (cid:19)
1 1
w(t+1)âˆ’w(t)=Î·A(t) Î¨ (t)âŠ¤Î¨ (t) v0(t), v0(t)â‰¡w âˆ’ A(t)âŠ¤w(t)
B âˆ âˆ â‹† N
(cid:18) (cid:19)(cid:18) (cid:19)
1 1
A(t+1)âˆ’A(t)=Î·Î³ w(t)v0(t)âŠ¤ Î¨ (t)âŠ¤Î¨ (t) A(0)âŠ¤A(0) . (4)
B âˆ âˆ N
Thefixedrandomprojection(cid:0)1A(0)âŠ¤A(0)(cid:1)
presentinA(t)â€™sdynamicsensurethatthefeatures
N
cannot have complete access to the infinite width features Ïˆ but only access to the initial N-
âˆ
dimensionalfeaturesA(0)Ïˆ .Ifthistermwerenotpresentthentherewouldbenofiniteparameter
âˆ
bottlenecksinthemodelandevenamodelwithN =1couldfullyfitthetargetfunction,leadingto
trivialparameterscalinglaws1. Inthissense, thevectorspacespannedbythefeaturesÏˆËœ doesnot
changeoverthecourseoftraining,butthefinite-widthkernelHilbertspacedoeschangeitskernel:
ÏˆËœ(x,t)Â·ÏˆËœ(xâ€²,t). Featurelearninginthisspaceamountstoreweighingthenormsoftheexisting
features. WehavechosenA(t)tohavedynamicssimilartothefirstlayerweightmatrixofalinear
neuralnetwork. Aswewillsee,thisisenoughtoleadtoanimprovedscalingexponent.
The hyperparameter Î³ sets the timescale of Aâ€™s dynamics and thus controls the rate of feature
evolution. The Î³ â†’ 0 limit represents the lazy learning limit Chizat et al. (2019) where features
arestaticandcoincideswitharandomfeaturemodeldynamicsofBordelonetal.(2024a);Paquette
etal.(2024). ThetesterroraftertstepsonaN parametermodelwithbatchsizeBis
(cid:28)(cid:104) (cid:105)2(cid:29)
L(t,N,B,Î³)â‰¡ Ïˆ âˆ(x)Â·wâˆ—âˆ’ÏˆËœ(x,t)Â·w(t) =v0(t)âŠ¤Î›v0(t). (5)
xâˆ¼p(x)
In the next sections we will work out a theoretical description of this model as a function of the
spectrumÎ›andthetargetcoefficientswâ‹†. Wewillthenspecializetopower-lawspectraandtarget
weightsandstudytheresultingscalinglaws.
3 DYNAMICAL MEAN FIELD THEORY OF THE MODEL
WecanconsiderthedynamicsforrandomA(0)andrandomdrawsofdataduringSGDinthelimit
of M â†’ âˆ and N,B â‰« 12. This dimension-free theory is especially appropriate for realistic
traceclasskernelswhereâŸ¨K (x,xâ€²)âŸ© = (cid:80) Î» < âˆ(equivalenttoÎ± > 1),whichisourfocus.
âˆ x k k
Define wâ‹†,v (t) to be respectively the components of wâ‹†,v0(t) in the kth eigenspace of Î›. The
k k
errorvariablesv0(t)aregivenbyastochasticprocess,andyielddeterministicpredictionfortheloss
k
L(t,N,B,Î³),analogoustotheresultsofBordelonetal.(2024a).
Sincetheresultingdynamicsforv0(t)atÎ³ > 0arenonlinearandcannotbeexpressedintermsof
k
a matrix resolvent, we utilize dynamical mean field theory (DMFT), a flexible approach for han-
dling nonlinear dynamical systems driven by random matrices (Sompolinsky & Zippelius, 1981;
Helias & Dahmen, 2020; Mannelli et al., 2019; Mignacco et al., 2020; Gerbelot et al., 2022;
Bordelon et al., 2024a). Most importantly, the theory gives closed analytical predictions for
L(t,N,B,Î³). We defer the derivation and full DMFT equations to the Appendix C. The full
set of closed DMFT equations are given in Equation equation 26 for online SGD and Equation
equation 41 for offline training with data repetition. Informally, this DMFT computes a closed
setofequationsforthecorrelationandresponsefunctionsforacollectionoftime-varyingvectors
V ={v0(t),v1(t),v2(t),v3(t),v4(t)} including,
tâˆˆ{0,1,...}
1 (cid:20) Î´v0(t) (cid:21)
C (t,s)=v0(t)âŠ¤Î›v0(s), C (t,s)= v3(t)Â·v3(s), R (t,s)=TrÎ› ,
0 3 N 0,2 Î´v2(s)âŠ¤
where Î´vk(t) representstheresponseofvk attimettoasmallperturbationtovâ„“ attimes. The
Î´vâ„“(s)âŠ¤
testlosscanbecomputedfromL(t) = C (t,t). Thistheoryisderivedgenerallyforanyspectrum
0
Î» andanytargetwâ‹†. Inthecomingsectionswewillexamineapproximatescalingbehaviorofthe
k k
losswhenthespectrumfollowsapowerlaw.
1Wecouldalsosolvethisproblembytrainingamodeloftheformf = w(t)âŠ¤B(t)AÏˆwherew(t)and
B(t)aredynamicalwithinitialconditionB(0) = I andAfrozenandthematrixB(t)followinggradient
descent.WeshowthatthesetwomodelsareactuallyexhibitequivalentdynamicsinAppendixB.
2Alternatively,wecanoperateinaproportionallimitwithN/M,B/Mapproachingconstants.Thisversion
ofthetheorywouldbeexactwithnofinitesizefluctuations.
4Preprint
4 POWER LAW SCALINGS FROM POWER LAW FEATURES
We consider initial kernels that satisfy source and capacity conditions as in (Caponnetto & Vito,
2005; Pillaud-Vivien et al., 2018; Cui et al., 2021; 2023). These conditions measure the rate of
decayofthespectrumoftheinitialinfinitewidthkernelK (x,xâ€²)andtargetfunctiony(x)inthat
âˆ
basis. Concretely,weconsidersettingswiththefollowingpowerlawscalings:
(cid:88)
Î» âˆ¼kâˆ’Î±, Î» (wâˆ—)2 âˆ¼Î»âˆ’Î² âˆ¼kâˆ’Î±Î². (6)
k â„“ â„“ k
â„“>k
TheexponentÎ±iscalledthecapacityandmeasurestherateofdecayoftheinitialkerneleigenvalues.
WewillassumethisexponentisgreaterthanunityÎ± > 1sincethelimitingN â†’ âˆkernelshould
be trace class. The exponent Î² is called the source and quantifies the difficulty of the task under
kernelregressionwithK .3 TheRKHSnorm|Â·|2 ofthetargetfunctionisgivenby:
âˆ H
(cid:40)
(cid:88) (cid:88) 1 Î² >1
|y|2 = (wâ‹†)2 = kâˆ’Î±(Î²âˆ’1)âˆ’1 â‰ˆ Î±(Î²âˆ’1) (7)
H k âˆ Î² <1.
k k
While the case of finite RKHS norm (Î² > 1) is often assumed in analyses of kernel methods that
relyonnorm-basedbounds, suchas(Bartlett&Mendelson,2002;Bach,2024), theÎ² < 1caseis
actuallymorerepresentativeofrealdatasets. Thiswaspointedoutin(Weietal.,2022). Thiscanbe
seenbyspectraldiagonalizationsperformedonrealdatasetsin(Bahrietal.,2021;Bordelonetal.,
2024a)aswellasinexperimentsinSection5.2. Westressthispointsincethebehavioroffeature
learningwithÎ² >1andÎ² <1willbestrikinglydifferentinourmodel.
General Scaling Law in the Lazy Limit For the purposes of deriving compute optimal scaling
laws, the works of Bordelon et al. (2024a) and Paquette et al. (2024) derived precise asymptotics
for the loss curves under SGD. For the purposes of deriving compute optimal scaling laws, these
asymptoticscanbeapproximatedasthefollowingsumofpowerlawsatlarget,N
1 Î·
lim L(t,N,B,Î³)â‰ˆ tâˆ’Î² +Nâˆ’Î±min{2,Î²}+ tâˆ’(1âˆ’ Î±1) + tâˆ’(2âˆ’ Î±1). (8)
Î³â†’0 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N B
LimitingGradientFlow ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
FiniteNTransient SGDTransient
The first terms represent bottleneck/resolution-limited scalings in the sense that taking all other
quantities to infinity and studying scaling with the last (Bahri et al., 2021). The first term gives
the loss dynamics of population gradient flow (N,B â†’ âˆ) while the second (model bottleneck)
termdescribest â†’ âˆlimitofthelosswhichdependsonN. Thethirdandfourthtermsaremixed
transientsthatarisefromtheperturbativefinitemodelandbatchsizeeffects. WhileBordelonetal.
(2024a)focusedonhardtaskswhereÎ² < 1wherethefirsttwotermsdominatewhenconsidering
computeoptimalscalinglaws,Paquetteetal.(2024)alsodiscussedtwootherphasesoftheeasytask
regimewherethefinaltwotermscanbecomerelevantforthecomputeoptimalscaling.
GeneralScalingLawintheFeatureLearningRegime ForÎ³ >0,approximationstoourprecise
DMFTequationsunderpowerlawspectragivethefollowingsumofpowerlaws
1 Î·
L(t,N,B,Î³)â‰ˆ tâˆ’Î²max{1, 1+2 Î²} +Nâˆ’Î±min{2,Î²}+ tâˆ’(1âˆ’ Î±1)max{1, 1+2 Î²} + tâˆ’(2âˆ’ Î±1)max{1, 1+2 Î²} .
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N B
LimitingGradientFlow ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
FiniteNTransient SGDTransient
(9)
Weseethatintherichregime,allexponentsexceptforthemodelbottleneckareeitherthesameor
areimproved. Foreasytasksandsuper-easytaskswhereÎ² > 1,werecoverthesameapproximate
scaling laws as those computed in the linear model of Bordelon et al. (2024a) and Paquette et al.
(2024). For hard tasks, Î² < 1, all exponents except for the model bottleneck term are improved.
BelowwewillexplainwhyeachofthesetermscanexperienceanimprovementintheÎ² < 1case.
Weexhibitaphasediagramallofthecaseshighlightedinequation8,equation9inFigure1.
3Thesourceexponentrusedin(Pillaud-Vivienetal.,2018)andotherworksisgivenby2r=Î².
5Preprint
Accelerated Training in Rich Regime The key distinction between our model and the random
feature model (Î³ = 0) is the limiting gradient flow dynamics, which allow for acceleration due
to feature learning. For nonzero feature learning Î³ > 0, our theory predicts that in the N â†’ âˆ
limit,thelossscalesasapowerlawL(t) âˆ¼ tâˆ’Ï‡ wheretheexponentÏ‡satisfiesthefollowingself-
consistentequation
(cid:34) (cid:35) (cid:26) (cid:27)
Ï‡(Î²)=âˆ’ lim 1 ln (cid:88) (wâ‹†)2Î» exp(cid:0) âˆ’Î» (cid:2) t+Î³t2âˆ’Ï‡(cid:3)(cid:1) =Î²max 1, 2 . (10)
tâ†’âˆlnt k k k 1+Î²
k
WederivethisequationinAppendixE.WeseethatifÎ² >1thenwehavethesamescalinglawasa
lazylearningmodelL(t)âˆ¼tâˆ’Î². However,ifthetaskissufficientlyhard(Î² <1),thentheexponent
isincreasedtoÏ‡= 2Î² >Î².
1+Î²
This acceleration is caused by the fact that the effective dynamical kernel K(t) defined by the
dynamicsofourfeaturesÏˆËœ(x,t)divergesasapowerlawK(t) âˆ¼ t1âˆ’Ï‡ whenÎ² < 1(seeAppendix
E).Asaconsequence,attimet,themodelislearningmodek (t)âˆ¼t(2âˆ’Ï‡)/Î±whichgivesaloss
â‹†
L(t)âˆ¼ (cid:88) (w kâ‹†)2Î»
k
âˆ¼tâˆ’Î²(2âˆ’Ï‡) =tâˆ’Î²max{1, 1+2 Î²} . (11)
k>kâ‹†
WhileourmodelpredictsthatthescalingexponentonlychangesforhardtaskswhereÎ² <1,italso
predictsanoveralldecreaseintraininglossasÎ³ increasesforeithereasyorhardtasks. InFigure2
(a)-(b)weshowthetheN,B â†’âˆlimitofourtheoryatvaryingvaluesofÎ³. ForeasytasksÎ² >1,
themodelswillalwaysfollowLâˆ¼tâˆ’Î² atlatetime,butwithapotentiallyreducedconstantwhenÎ³
islarge. Forhardtasks(Fig. 2(b))thescalingexponentimprovesLâˆ¼tâˆ’ 12 +Î² Î² forÎ³ >0.
ModelBottleneckScalings OurtheorycanbeusedtocomputefiniteN effectsintherichregime
duringSGDtraining. Inthiscase,thedynamicssmoothlytransitionbetweenfollowingthegradient
descenttrajectoryatearlytimetoanasymptotethatdependsonN ast â†’ âˆ. InFigure2(c)-(d)
weillustratetheselearningcurvesfromourtheoryandfromfiniteN simulations,showingagood
matchofthetheorytoexperiment.
We derive the asymptotic scaling of Nâˆ’Î±min{2,Î²} in Appendix E.3. Intuitively, at finite N, the
dynamics only depend on the filtered signal
(cid:0)1A(0)âŠ¤A(0)(cid:1)
w . Thus the algorithm can only
N â‹†
estimate,atbest,thetopN componentsofw ,resultinginthefollowingtâ†’âˆloss
â‹†
(cid:88)
L(N)âˆ¼ kâˆ’Î±Î²âˆ’1 âˆ¼Nâˆ’Î±Î². (12)
k>N
SGD Noise Effects The variance in the learned model predictions due to random sampling of
minibatchesduringSGDalsoaltersthemeanfieldpredictionoftheloss. InFigure3,weshowSGD
noiseeffectsfromfinitebatchsizeBforhardÎ² <1andsupereasyÎ² >2âˆ’1/Î±tasks.
Compute Optimal Scaling Laws in Feature Learning Regime At a fixed compute budget
C = Nt, one can determine how to allocate compute towards training time t and model size N
using our derived exponents from the previous sections. Choosing N,t optimally, we derive the
followingcomputeoptimalscalinglawsL (C)inthefeaturelearningregimeÎ³ >0. Thesearealso
â‹†
summarizedinFigure1. 4
1. Hard task regime (Î² < 1): the compute optimum balances the population gradient flow
termtâˆ’ 12 +Î²
Î²
andthemodelbottleneckNâˆ’Î±Î².
2. Easytasks(1 < Î² < 2âˆ’ 1): thecomputeoptimumcomparesgradientflowtermtâˆ’Î² to
Î±
finiteN transientterm 1tâˆ’1+1/Î±
N
3. Supereasytasks(Î² >2âˆ’ 1):computeoptimumbalancesthefiniteNtransient 1tâˆ’1+1/Î±
Î± N
andSGDtransientterms 1tâˆ’2+ Î±1.
B
4The three regimes of interest correspond to Phases I,II,III in Paquette et al. (2024). These are the only
(cid:80)
relevantregimesfortrace-classâŸ¨K (x,x)âŸ© = Î» <âˆ(finitevariance)kernels(equivalenttoÎ±>1).
âˆ x k k
6Preprint
100
100
10 1
10 2 =0
=0 10 1 =2 3
10 3 =2 3 =2 2
=2 2 =2 1
10 4 =2 1 =20
=20 =21
10 5 =21 10 2 t
t t 2/(1+ )
10 6
100 101 102 103 104 100 101 102 103 104
t t
(a) EasyTaskÎ² =1.2withN,B â†’âˆ (b) HardTaskÎ² =0.4withN,B â†’âˆ
100 100
N=24 N=24
N=25 N=25
10 1
N=26 N=26
N=27 N=27
N=28 10 1 N=28
10 2 N=29 N=29
N=210 N=210
N=211 N=211
t t 2/(1+ )
10 3 Theory Theory
10 2
100 101 102 100 101 102
t t
(c) EasyTaskÎ² =1.2withfiniteN (d) HardTaskÎ² =0.4withfiniteN
Figure2: Thelearningdynamicsofourmodelunderpowerlawfeaturesexhibitspowerlawscaling
with an exponent that depends on task difficulty. Dashed black lines represent solutions to the
dynamicalmeanfieldtheory(DMFT)whilecoloredlinesandshadedregionsrepresentmeansand
errorbars over 32 random experiments. (a) For easy tasks with source exponent Î² > 1, the loss
is improved with feature learning but the exponent of the power law is unchanged. We plot the
approximationL âˆ¼ tâˆ’Î² inblue. (b)ForhardtaskswhereÎ² < 1, thepowerlawscalingexponent
improves. An approximation of our learning curves predicts a new exponent L âˆ¼ tâˆ’ 12 +Î² Î² which
matchestheexactN,B â†’âˆequations. (c)-(d)Themeanfieldtheoryaccuratelycapturesthefinite
N effectsinboththeeasyandhardtaskregimes. AsN â†’âˆthecurveapproachestâˆ’Î²max{1, 1+2 Î²}.
TaskDifficulty HardÎ² <1 Easy1<Î² <2âˆ’1/Î± Super-EasyÎ² >2âˆ’1/Î±
Lazy(Î³ =0) Î±Î² Î±Î² 1âˆ’ 1
Î±+1 Î±Î²+1 2Î±
Rich(Î³ >0) 2Î±Î² Î±Î² 1âˆ’ 1
Î±(1+Î²)+2 Î±Î²+1 2Î±
Table 1: Compute optimal scaling exponents r
C
for the loss L â‹†(C) âˆ¼ Câˆ’rC for tasks of varying
difficulty in the feature learning regime. For Î² > 1, the exponents coincide with the lazy model
analyzedbyBordelonetal.(2024a);Paquetteetal.(2024),whileforhardtaskstheyareimproved.
We work out the complete compute optimal scaling laws for these three settings by imposing the
constraintC = Nt,identifyingtheoptimalchoiceofN andtatfixedtandverifyingtheassumed
dominantbalance. WesummarizethethreepossiblecomputescalingexponentsinTable1.
InFigure4wecomparethecomputeoptimalscalinglawsinthehardandeasyregimes. Weshow
thatthepredictedexponentsareaccurate.InFigure3weillustratetheinfluenceofSGDnoiseonthe
learningcurveinthesupereasyregimeanddemonstratethatthelargeC computeoptimalscaling
lawisgivenbyCâˆ’1+ 21 Î±.
7
)t(
)t(
)t(
)t(Preprint
100 100
B = 1 B = 1
B = 2 101 B = 2
B = 4 B = 4
B = 8 102 B = 8
B = 16 B = 16
101 B B = = 3 62 4 103 B B = = 3 62 4
t Th2 eo/(1 ry+) 104 t t 2+1/
t 1+1/
105 Theory
102 106
100 101 102 100 101 102
t t
(a) SGDTransientsHardÎ² =0.5 (b) SGDTransientsSuper-EasyÎ² =3.6
Figure3: SGDTransientsinfeaturelearningregime. (a)Inthehardregime,theSGDnoisedoes
notsignificantlyalterthescalingbehavior, butdoesaddsomeadditionalvariancetothepredictor.
AsB â†’ âˆ,thelossconvergestothetâˆ’2Î²/(1+Î²) scaling. (b)Inthesuper-easyregime,themodel
transitionsfromgradientflowscalingtâˆ’Î² toaSGDnoiselimitedscaling 1tâˆ’2+1/Î±andfinallytoa
B
finiteN transientscaling 1tâˆ’1+1/Î±.
N
100 C + 100 C +1 100 C +1
Theory Theory C1+21
Theory
101 101
102 102
101
103
103
104
102 103 104 105 102 103 104 105 106 102 103 104 105 106
Compute C Compute C Compute C
(a) HardTaskScalingÎ² =0.5 (b) EasyTaskÎ² =1.25 (c) SuperEasyÎ² =1.75>2âˆ’ 1.
Î±
Figure 4: Compute optimal scalings in the feature learning regime (Î³ = 0.75). Dashed black
lines are the full DMFT predictions. (a) In the Î² < 1 regime the compute optimal scaling law
is determined by a tradeoff between the bottleneck scalings for training time t and model size N,
giving L â‹†(C) âˆ¼ Câˆ’ Î±Î± Î²Î² +Ï‡ Ï‡ where Ï‡ = 12 +Î²
Î²
is the time-exponent for hard tasks inthe rich-regime.
(b) In the easy task regime 1 < Î² < 2âˆ’ 1, the large C scaling is determined by a competition
Î±
between the bottleneck scaling in time t and the leading order 1/N correction to the dynamics
L â‹†(C) âˆ¼ Câˆ’ Î±Î± Î²+Î² 1. (c)Inthesuper-easyregime, thescalingexponentatlargecomputeisderived
bybalancingtheSGDnoiseeffectswiththe1/N transients.
5 EXPERIMENTS WITH DEEP NONLINEAR NEURAL NETWORKS
Whileourtheoryaccuratelydescribessimulationsofoursolvablemodel,wenowaimtotestifthese
newexponentsarepredictivewhentrainingdeepnonlinearneuralnetworks.
5.1 SOBOLEVSPACESONTHECIRCLE
We first consider training multilayer nonlinear MLPs with nonlinear activation function Ï•(h) =
[ReLU(h)]qÏ• inthemeanfieldparameterization/ÂµP(Geigeretal.,2020;Yang&Hu,2021;Borde-
lon&Pehlevan,2022)withdimensionless(width-independent)featurelearningparameterÎ³ . We
0
considerfittingtargetfunctionsy(x)withx=[cos(Î¸),sin(Î¸)]âŠ¤onthecircleÎ¸ âˆˆ[0,2Ï€].Theeigen-
functionsforrandomlyinitializedinfinite widthnetworksaretheFourierharmonics. Weconsider
targetfunctionsy(Î¸)withpower-lawFourierspectrawhilethekernelsatinitializationK(Î¸,Î¸â€²)also
admitaFouriereigenexpansion
âˆ âˆ
(cid:88) (cid:88)
y(Î¸)= kâˆ’qcos(kÎ¸), K(Î¸,Î¸â€²)= Î» cos(k(Î¸âˆ’Î¸â€²)). (13)
k
k=1 k=1
8
)C(
)t(
)C(
)t(
)C(Preprint
WeshowthattheeigenvaluesofthekernelatinitializationdecayasÎ»
k
âˆ¼kâˆ’2qÏ• intheAppendixA.
ThecapacityandsourceexponentsÎ±,Î² requiredforourtheorycanbecomputedfromqandq as
Ï•
2qâˆ’1
Î±=2q , Î² = (14)
Ï• 2q
Ï•
Thus task difficulty can be manipulated by altering the target function or the nonlinear activation
function of the neural network. We show in Figure 5 examples of online training in this kind
of network on tasks and architectures of varying Î². In all cases, our theoretical prediction of
tâˆ’Î²max{1, 1+2 Î²}providesaveryaccuratepredictionofthescalinglaw.
100 100
10 1
10 1
10 2 =0.10
10 3 = =0 0. .2 55 0 10 2
=0.75 =0.50
10 4 =1.00 10 3 =0.75
=1.25 =1.00
10 5 =1.50 =1.25
t max{1,2/(1+ )} 10 4 t max{1,2/(1+ )}
10 6
101 102 103 104 101 102 103 104
t t
(a) ReLU(q =1.0)withvaryingÎ² = 2qâˆ’1 (b) Varyingq withfixedtarget(q=1.4)
Ï• 2qÏ• Ï•
Figure5: Changingthetargetfunctionâ€™sFourierspectrumordetailsoftheneuralarchitecturecan
changethescalinglawinnonlinearnetworkstrainedonline. (a)Ourpredictedexponentsarecom-
paredtoSGDtraininginaReLUnetwork. TheexponentÎ² isvariedbychangingq,thedecayrate
for the target functionâ€™s Fourier spectrum. The scaling laws are well predicted by our toy model
tâˆ’Î²max{1, 1+2 Î²}. (b) The learning exponent for a fixed target function can also be manipulated by
changingpropertiesofthemodelsuchastheactivationfunctionq .
Ï•
5.2 COMPUTERVISIONTASKS(MNISTANDCIFAR)
We next study networks trained on MNIST and CIFAR image recognition tasks. Our motivation
is to study networks training in the online setting over several orders of magnitude in time. To
this end, we adopt larger versions of these datasets: â€œMNIST-1Mâ€ and CIAFR-5M. We generate
MNIST-1Musingthedenoisingdiffusionmodel(Hoetal.,2020)inPearce(2022). WeuseCIFAR-
5MfromNakkiranetal.(2021). EarlierresultsinRefinettietal.(2023)showthatnetworkstrained
onCIFAR-5MhaveverysimilartrajectoriestothosetrainedonCIFAR-10withoutrepetition. The
resulting scaling plots are provided in Figure 6. MNIST-1M scaling is very well captured by the
ourtheoreticalscalingexponents. TheCIFAR-5MscalinglawexponentatlargeÎ³ firstfollowsour
0
predictions,butlaterentersaregimewithascalingexponentlargerthanwhatourtheoreticalmodel
predicts.
6 DISCUSSION
Weproposedasimplemodeloflearningcurvesintherichregimewheretheoriginalfeaturescan
evolve as a linear combination of the initial features. While the theory can give a quantitatively
accuratefortheonlinelearningscalingexponentintherichregimeforhardtasks, theCIFAR-5M
experimentsuggeststhatadditionaleffectsinnonlinearnetworkscanoccuraftersufficienttraining.
However,therearemanyweakerpredictionsofourtheorythatwesuspecttoholdinawidersetof
settings,whichweenumeratebelow.
Source Hypothesis: Feature Learning Only Improves Scaling For Î² < 1. Our model makes
a general prediction that feature learning does not improve the scaling laws for tasks within the
9
)t( )t(Preprint
10 1
0=1e-02
0=3e-02
0=1e-01
0=3e-01 Linearized
0=1e+00 0=0.25
10 2 t t 2/(1+ ) 10 1 0 0= =0 0. .3 53 0
t
t 2/(1+ )
100 101 102 103 100 101 102 103 104 105
t Steps
(a) CNNsonMNIST-1M,Î² =0.30 (b) CNNsonCIFAR-5MÎ² =0.075
Figure 6: The improved scaling law with training time gives better predictions for training deep
networks on real data, but still slightly underestimate improvements to the scaling law for CNNs
trained on CIFAR-5M, especially at large richness Î³ . (a)-(b) Training on MNIST-1M is well de-
0
scribedbythenewpowerlawexponentfromourtheory. (c)CNNtrainingonCIFAR-5Misinitially
welldescribedbyournewexponent,buteventuallyachievesabetterpowerlaw.
RKHS of the initial infinite width kernel. Our experiments with ReLU networks fitting functions
in different Sobolev spaces with Î² > 1 support this hypothesis. Since many tasks using real data
appear to fall outside the RKHS of the initial infinite width kernels, this hypothesis suggests that
lazy learning would not be adequate to describe neural scaling laws on real data, consistent with
empiricalfindings(Vyasetal.,2022).
InsignificanceofSGDforHardTasks RecentempiricalworkhasfoundthatSGDnoisehaslittle
impactinonlinetrainingofdeeplearningmodels(Vyasetal.,2023b;Zhaoetal.,2024). Ourtheory
suggests this may be due to the fact that SGD transients are always suppressed for realistic tasks
which are often outside the RKHS of the initial kernel. The regime in which feature learning can
improvethescalinglawinourmodelispreciselytheregimewhereSGDtransientshavenoimpact
onthescalingbehavior.
Ordering of Models in Lazy Limit Preserved in Feature Learning Regime An additional in-
terestingpredictionofourtheoryisthattheorderingofmodelsbyperformanceinthelazyregime
ispreservedisthesameastheorderingofmodelsinthefeaturelearningregime. IfmodelAoutper-
formsmodelBonataskinthelazylimit(Î² > Î² ),thenmodelAwillalsoperformbetterinthe
A B
richregimeÏ‡(Î² )>Ï‡(Î² )(seeFigure1).Thissuggestsusingkernellimitsofneuralarchitectures
A B
for fast initial architecture search may be viable, despite failing to capture feature learning (Park
etal.,2020). Thispredictiondeservesagreaterdegreeofstresstesting.
Limitations and Future Directions There are many limitations to the current theory. First, we
study mean square error loss with SGD updates, while most modern models are trained on cross-
entropy loss with adaptive optimizers (Everett et al., 2024). Understanding the effect of adaptive
optimizers or preconditioned updates on the scaling laws represents an important future direction.
In addition, our model treats the learned features as linear combinations of the initial features, an
assumptionwhichmaybeviolatedinfinitewidthneuralnetworks. Lastly,whileourtheoryisvery
descriptive of nonlinear networks on several tasks, we did identify a noticeable disagreement on
CIFAR-5M after sufficient amounts of training. Versions of our model where the learned features
arenotwithinthespanoftheinitialfeaturesorwherethematrixAundergoesdifferentdynamics
mayprovideapromisingavenueoffutureresearchtoderiveeffectivemodelsofneuralscalinglaws.
ACKNOWLEDGEMENTS
We would like to thank Jacob Zavatone-Veth, Jascha Sohl-Dickstein, Courtney Paquette, Bruno
Loureiro,andYasamanBahriforusefuldiscussions.
10
ssoL
tseT
ssoL
tseTPreprint
B.B. is supported by a Google PhD Fellowship. A.A. is supported by a Fannie and John Hertz
Fellowship. C.P. is supported by NSF grant DMS-2134157, NSF CAREER Award IIS-2239780,
andaSloanResearchFellowship. ThisworkhasbeenmadepossibleinpartbyagiftfromtheChan
Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and
ArtificialIntelligence.
REFERENCES
EmmanuelAbbe,EnricBoix-Adsera,MatthewSBrennan,GuyBresler,andDheerajNagaraj. The
staircaseproperty: Howhierarchicalstructurecanguidedeeplearning. AdvancesinNeuralIn-
formationProcessingSystems,34:26989â€“27002,2021.
EmmanuelAbbe,EnricBoixAdsera,andTheodorMisiakiewicz. Sgdlearningonneuralnetworks:
leapcomplexityandsaddle-to-saddledynamics.InTheThirtySixthAnnualConferenceonLearn-
ingTheory,pp.2552â€“2623.PMLR,2023.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:
The silent alignment effect. In International Conference on Learning Representations, 2022.
URLhttps://openreview.net/forum?id=1NvflqAdoom.
Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. The onset of
variance-limited behavior for networks in the lazy and rich regimes. In The Eleventh Interna-
tional Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=JLINxPOVTh7.
AlexanderBAtanasov,JacobAZavatone-Veth,andCengizPehlevan. Scalingandrenormalization
inhigh-dimensionalregression. arXivpreprintarXiv:2405.00592,2024.
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-
dimensionalasymptoticsoffeaturelearning: Howonegradientstepimprovestherepresentation.
AdvancesinNeuralInformationProcessingSystems,35:37932â€“37946,2022.
FrancisBach. Learningtheoryfromfirstprinciples. MITpress,2024.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scalinglaws. arXivpreprintarXiv:2102.06701,2021.
Lorenzo Bardone and Sebastian Goldt. Sliding down the stairs: how correlated latent variables
acceleratelearningwithneuralnetworks. arXivpreprintarXiv:2404.08602,2024.
PeterLBartlettandShaharMendelson. Rademacherandgaussiancomplexities: Riskboundsand
structuralresults. JournalofMachineLearningResearch,3(Nov):463â€“482,2002.
BlakeBordelonandCengizPehlevan. Self-consistentdynamicalfieldtheoryofkernelevolutionin
wideneuralnetworks. arXivpreprintarXiv:2205.09653,2022.
BlakeBordelon,AbdulkadirCanatar,andCengizPehlevan. Spectrumdependentlearningcurvesin
kernelregressionandwideneuralnetworks. InInternationalConferenceonMachineLearning,
pp.1024â€“1034.PMLR,2020.
Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise
hyperparametertransferinresidualnetworks: Dynamicsandscalinglimit,2023.
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Adynamicalmodelofneuralscaling
laws. arXivpreprintarXiv:2402.01092,2024a.
BlakeBordelon,HamzaTahirChaudhry,andCengizPehlevan. Infinitelimitsofmulti-headtrans-
formerdynamics. arXivpreprintarXiv:2405.15712,2024b.
AndreaCaponnettoandErnestoDeVito. Fastratesforregularizedleast-squaresalgorithm. 2005.
11Preprint
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming.
Advancesinneuralinformationprocessingsystems,32,2019.
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka ZdeborovaÂ´. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
InformationProcessingSystems,34:10131â€“10143,2021.
HugoCui,BrunoLoureiro,FlorentKrzakala,andLenkaZdeborovaÂ´. Errorscalinglawsforkernel
classificationundersourceandcapacityconditions. MachineLearning: ScienceandTechnology,
4(3):035033,2023.
YatinDandi,FlorentKrzakala,BrunoLoureiro,LucaPesce,andLudovicStephan. Howtwo-layer
neural networks learn, one (giant) step at a time. In NeurIPS 2023 Workshop on Mathemat-
ics of Modern Machine Learning, 2023. URL https://openreview.net/forum?id=
iBDcaBLhz2.
SimonSDu,WeiHu,andJasonDLee. Algorithmicregularizationinlearningdeephomogeneous
models: Layersareautomaticallybalanced. Advancesinneuralinformationprocessingsystems,
31,2018.
Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu,
Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, et al. Scaling expo-
nentsacrossparameterizationsandoptimizers. arXivpreprintarXiv:2407.05872,2024.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometryandthetimeevolutionoftheneuraltangentkernel. InHugoLarochelle,Marcâ€™Aurelio
Ranzato,RaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin(eds.),AdvancesinNeuralIn-
formationProcessingSystems33:AnnualConferenceonNeuralInformationProcessingSystems
2020,NeurIPS2020,December6-12,2020,virtual,2020.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
trainingindeepneuralnetworks. JournalofStatisticalMechanics:TheoryandExperiment,2020
(11):113301,2020.
CedricGerbelot,EmanueleTroiani,FrancescaMignacco,FlorentKrzakala,andLenkaZdeborova.
Rigorous dynamical mean field theory for stochastic gradient descent methods. arXiv preprint
arXiv:2210.06591,2022.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networksoutperformkernelmethods? AdvancesinNeuralInformationProcessingSystems,33:
14820â€“14830,2020.
MoritzHeliasandDavidDahmen.Statisticalfieldtheoryforneuralnetworks,volume970.Springer,
2020.
JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXivpreprintarXiv:1712.00409,2017.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840â€“6851,2020.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.Train-
ingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXivpreprintarXiv:2001.08361,2020.
LicongLin,JingfengWu,ShamMKakade,PeterLBartlett,andJasonDLee.Scalinglawsinlinear
regression: Compute,parameters,anddata. arXivpreprintarXiv:2406.08466,2024.
12Preprint
AlexanderMaloney,DanielARoberts,andJamesSully. Asolvablemodelofneuralscalinglaws.
arXivpreprintarXiv:2210.16859,2022.
StefanoSaraoMannelli,FlorentKrzakala,PierfrancescoUrbani,andLenkaZdeborova. Passed&
spurious: Descentalgorithmsandlocalminimainspikedmatrix-tensormodels. Ininternational
conferenceonmachinelearning,pp.4333â€“4342.PMLR,2019.
SongMei, TheodorMisiakiewicz, andAndreaMontanari. Mean-fieldtheoryoftwo-layersneural
networks:dimension-freeboundsandkernellimit. InConferenceonLearningTheory,pp.2388â€“
2464.PMLR,2019.
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka ZdeborovaÂ´. Dynamical
mean-fieldtheoryforstochasticgradientdescentingaussianmixtureclassification. Advancesin
NeuralInformationProcessingSystems,33:9540â€“9550,2020.
Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-
maneTazi,SampoPyysalo,ThomasWolf,andColinRaffel. Scalingdata-constrainedlanguage
models. arXivpreprintarXiv:2305.16264,2023.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
onlinelearnersaregoodofflinegeneralizers. InInternationalConferenceonLearningRepresen-
tations,2021.
Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric
compressionofinvariantmanifoldsinneuralnetworks. JournalofStatisticalMechanics: Theory
andExperiment,2021(4):044001,2021.
CourtneyPaquette,KiwonLee,FabianPedregosa,andElliotPaquette. Sgdinthelarge: Average-
caseanalysis,asymptotics,andstepsizecriticality. InConferenceonLearningTheory,pp.3548â€“
3626.PMLR,2021.
ElliotPaquette,CourtneyPaquette,LechaoXiao,andJeffreyPennington. 4+3phasesofcompute-
optimalneuralscalinglaws. arXivpreprintarXiv:2405.15074,2024.
Daniel S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards nngp-
guidedneuralarchitecturesearch. arXivpreprintarXiv:2011.06006,2020.
Tim Pearce. Conditional diffusion mnist. https://github.com/TeaPearce/
Conditional_Diffusion_MNIST,2022. Accessed: 2024-05-14.
LoucasPillaud-Vivien,AlessandroRudi,andFrancisBach. Statisticaloptimalityofstochasticgra-
dientdescentonhardlearningproblemsthroughmultiplepasses.AdvancesinNeuralInformation
ProcessingSystems,31,2018.
MariaRefinetti,AlessandroIngrosso,andSebastianGoldt. Neuralnetworkstrainedwithsgdlearn
distributions of increasing complexity. In International Conference on Machine Learning, pp.
28843â€“28863.PMLR,2023.
AndrewMSaxe,JamesLMcClelland,andSuryaGanguli. Exactsolutionstothenonlineardynam-
icsoflearningindeeplinearneuralnetworks. arXivpreprintarXiv:1312.6120,2013.
JamesBSimon,MadelineDickens,DhruvaKarkada,andMichaelRDeWeese. Theeigenlearning
framework:Aconservationlawperspectiveonkernelregressionandwideneuralnetworks.arXiv
preprintarXiv:2110.03922,2021.
James B Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern
machine learning: when infinite overparameterization is optimal and overfitting is obligatory.
arXivpreprintarXiv:2311.14646,2023.
HaimSompolinskyandAnnetteZippelius.Dynamictheoryofthespin-glassphase.PhysicalReview
Letters,47(5):359,1981.
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neu-
ral scaling laws: beating power law scaling via data pruning. Advances in Neural Information
ProcessingSystems,35:19523â€“19536,2022.
13Preprint
StefanoSpigler,MarioGeiger,andMatthieuWyart. Asymptoticlearningcurvesofkernelmethods:
empirical data versus teacherâ€“student paradigm. Journal of Statistical Mechanics: Theory and
Experiment,2020(12):124001,2020.
NikhilVyas,YaminiBansal,andPreetumNakkiran. Limitationsofthentkforunderstandinggen-
eralizationindeeplearning. arXivpreprintarXiv:2206.10012,2022.
NikhilVyas,AlexanderAtanasov,BlakeBordelon,DepenMorwani,SabarishSainathan,andCen-
giz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. arXiv
preprintarXiv:2305.18411,2023a.
Nikhil Vyas, Depen Morwani, Rosie Zhao, Gal Kaplun, Sham Kakade, and Boaz Barak. Beyond
implicitbias:Theinsignificanceofsgdnoiseinonlinelearning.arXivpreprintarXiv:2306.08590,
2023b.
AlexanderWei,WeiHu,andJacobSteinhardt.Morethanatoy:Randommatrixmodelspredicthow
real-worldneuralrepresentationsgeneralize. InInternationalConferenceonMachineLearning,
pp.23549â€“23588.PMLR,2022.
BlakeWoodworth,SuriyaGunasekar,JasonDLee,EdwardMoroshko,PedroSavarese,ItayGolan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
ConferenceonLearningTheory,pp.3635â€“3673.PMLR,2020.
GregYangandEdwardJHu.Tensorprogramsiv:Featurelearningininfinite-widthneuralnetworks.
InInternationalConferenceonMachineLearning,pp.11727â€“11737.PMLR,2021.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via
zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https:
//openreview.net/forum?id=Bx6qKuBM2AD.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ry-
der, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural
networksviazero-shothyperparametertransfer. arXivpreprintarXiv:2203.03466,2022.
JacobA.Zavatone-VethandCengizPehlevan. Learningcurvesfordeepstructuredgaussianfeature
models,2023.
RosieZhao,DepenMorwani,DavidBrandfonbrener,NikhilVyas,andShamKakade. Deconstruct-
ingwhatmakesagoodoptimizerforlanguagemodels. arXivpreprintarXiv:2407.07972,2024.
14Preprint
APPENDIX
A ADDITIONAL EXPERIMENTS
100 100
10 2 10 1
10 4 q =0.75 10 2
q =1.0
=1.0
q =1.25 =1.2
10 6 q =1.5 10 3 =1.5
q =1.75 =1.8
k 2q 10 4 t
10 8
100 101 102 100 101 102 103 104
k t
(a) KerneleigenspectraÎ±=2q (b) EasySobolevTasks(ReLUq =1)
Ï• Ï•
Figure7: Additionalexperimentsinthesettingwithdatadrawnfromthecircle. (a)Thespectraof
kernels across different nonlinearities Ï•(h) = ReLU(h)qÏ• which scale as Î»
k
âˆ¼ kâˆ’2qÏ•. (b) More
experimentsintheeasytaskregime,showthatfeaturelearningdoesnotalterthelongtimescaling
behaviorforÎ² >1.
8Ã—100
7Ã—100
=0.0001
=0.01
6Ã—100 =0.05
=0.1
t
t (2 )
105 106 107 108 109
Tokens
Figure 8: A depth L = 4 decoder-only transformer (16 heads with d = 128) trained on next-
head
wordpredictionwithSGDontheC4datasettokenizedwiththeSentencePiecetokenizer. Weplot
cross entropy loss and fit a powerlaw tâˆ’Î² to lazy learning curve Î³ = 10âˆ’4 over the interval from
106 to 3Ã—109 tokens. We then compute the new predicted exponent tâˆ’Î²(2âˆ’Î²) and compare to a
simulationatÎ³ =0.1. Thoughourtheoreticalpredictionofadoublingofthescalingexponentwas
derivedinthecontextofMSE,thenewscalingexponentfitsthedatasomewhatwellforthissetting
atearlytimes.
B FURTHER DISCUSSION OF MODELS
Weseekamodelthatincorporatesboththebottle-neckingeffectsoffinitewidthobservedinrecent
linear random feature models of scaling laws while still allowing for a notion of feature learning.
Exactlysolvablemodelsoffeaturelearningnetworksarerelativelyrare. Here,wetakeinspiration
fromthelinearneuralnetworkliteratureSaxeetal.(2013).Linearneuralnetworksexhibitbothlazy
andrichregimesoflearningWoodworthetal.(2020),inwhichtheycanlearnusefultask-relevant
featuresinawaythatcanbeanalyticallystudiedAtanasovetal.(2022). Inourwork,wegobeyond
thesemodelsoflinearneuralnetworksandshowthatlinearneuralnetworkstrainedondataunder
source and capacity conditions can improve the convergence rate compared to that predicted by
kerneltheory.
15
k
ssoL
yportnE
ssorC
)t(Preprint
ğ‘“ ğ’™ ğ‘“ ğ’™ ğ‘“ ğ’™
ğ‘ ğ‘ ğ‘ ğ‘
ğ´0 ğ‘¤ ğ‘¡ ğ´ğ‘¡ ğ‘¤ ğ‘¡ ğ´0 ğµ ğ‘¡ ğ‘¤ ğ‘¡
ğ‘€ ğ‘€ ğ‘€
(a) (b) (c)
Figure 9: Three different models studied in this work and prior work. Black weights are frozen
whileorangeweightsaretrainable. a)Alinearrandomfeaturemodelwithonlythereadoutweights
trainable. ThismodelwasstudiedinMaloneyetal.(2022);Bordelonetal.(2024a);Paquetteetal.
(2024)asasolvablemodelofneuralscalinglaws. b)Atwolayerlinearnetworkwithbothweights
trainable. Thismodeldoesnotincurabottleneckduetofinitewidthbutundergoesfeaturelearning,
whichimprovesthescalingofthelosswithtime. WestudypurelinearneuralnetworksinAppendix
G.Inthemaintext,wetrainthismodelwithaprojectedversionofgradientdescent. Thisisequiv-
alent to c) and gives both finite-parameter bottlenecks as well as improvements to scaling due to
featurelearning.
Themodelintroducedinsection2isgivebyatwo-layerlinearnetworkactingontheÏˆ (x):
âˆ
1
f(x)= wâŠ¤AÏˆ (x). (15)
N âˆ
There, weconstrainedittoupdateitsweightsbyaformofprojectedgradientdescentasgivenby
Equation4. Here,weshowthatrunningthisprojectedgradientdescentisequivalenttorunningor-
dinarygradientdescentonatwolayerlinearnetworkafterpassingÏˆ throughrandomprojections.
âˆ
DefineA =A(0)andB(t)=A(t)A+where+denotestheMoore-Penrosepsuedoinverse.Then
0 0
assumingN <M andA isrankN,wehave
0
B(t)A =A(t), B(0)=I . (16)
0 NÃ—N
NowconsidertakingÏˆ andpassingitthroughA ,andthentrainingB,wwithordinarygradient
âˆ 0
descent. Wehaveupdateequations:
(cid:18) (cid:19)
1
w(t+1)âˆ’w(t)=Î·B(t)A Î¨âŠ¤Î¨ v0(t)
0 B âˆ âˆ
(17)
(cid:18) (cid:19)
1 1
B(t+1)âˆ’B(t)=Î·w(t)v0(t)âŠ¤ Î¨âŠ¤Î¨ AâŠ¤
B âˆ âˆ N 0
MultiplyingthesecondequationbyA ontherightrecoversequation4. Here,Î³ actsasarescaling
0
ofthelearningratefortheBupdateequations.Weillustratethismodel,aswellasthelinearrandom
featureandlinearneuralnetworkmodelsinFigure9.
SeveralpapersMaloneyetal.(2022);Atanasovetal.(2023);Bordelonetal.(2024a);Atanasovetal.
(2024)havestudiedthemodelgiveninequation15withfrozenAunderthefollowinginterpretation.
ThesamplesÏˆ âˆˆRD correspondtothedatasetasexpressedinthespaceofaninfinitelywideNTK
atinitialization. ThesearepassedthroughasetoffrozenrandomweightsW ,whicharethoughtof
1
astheprojectionfromtheinfinitewidthnetworktothefinite-widthempiricalNTK,corresponding
to a lazy network. From there, the final layer weights are not frozen and perform the analog of
regressionwiththefinite-widthNTK.InBordelonetal.(2024a),thismodelwasshowntoreproduce
many of the compute optimal scaling laws observed in practice. It was also however shown there
that the scaling laws for lazy networks are very different from those observed for feature-learning
networks.
16Preprint
Ourmotivationistodevelopasimpleandsolvablemodelofhowthefinite-widthnetworkfeatures
ÏˆËœ(x;t) = A(t)Ïˆ (x) might evolve to learn useful features. The projected linear model defined
âˆ
above states that the ÏˆËœ recombine themselves in such a way so that the empirical neural tangent
kernelÏˆËœ(x;t)Â·ÏˆËœ(xâ€²;t)isbetteralignedtothetask. Thesimplemodelofalinearneuralnetworkis
richenoughtoyieldanimprovedpowerlaw,whilestillbeinganalyticallytractable.
C DERIVATION OF THE MEAN FIELD EQUATIONS
Inthissetting,wederivethemeanfieldequationsforthetypicaltestlossL(t,N,B)asafunctionof
trainingtime. Toaccomplishthis,wehavetoperformdisorderaveragesovertherandommatrices
A(0)and{Î¨(t)}âˆ . Westartbydefiningthefollowingcollectionoffields
t=0
1
v0(t)=wâ‹†âˆ’ A(t)âŠ¤w(t)
N
1
v1(t)=Î¨(t)v0(t), v2(t)= Î¨(t)âŠ¤v1(t)
B
1
v3(t)=A(0)v2(t), v4(t)= A(0)âŠ¤v3(t)
N
1
vw(t)= A(0)âŠ¤w(t) (18)
N
Fromtheseprimitivefields,wecansimplifythedynamicsofA,w(t)
(cid:88)
A(t)=A(0)+Î·Î³ w(s)v4(s)âŠ¤
s<t
w(t+1)=w(t)+Î·A(t)v2(t)
=w(t)+Î·v3(t)+Î·2Î³(cid:88) w(s)(cid:2) v4(s)Â·v2(t)(cid:3)
s<t
(cid:88)
=w(t)+Î·v3(t)+Î·2Î³ w(s)C (t,s) (19)
3
s<t
whereweintroducedthecorrelationfunctionC (t,s)â‰¡ 1v3(t)Â·v3(s)=v2(t)Â·v4(s). Similarly
3 N
forv0(t)andvw(t)wehave
(cid:88)
v0(t)=wâ‹†âˆ’vw(t)âˆ’Î·Î³ v4(s)C (t,s)
w
s<t
(cid:88)
vw(t+1)=vw(t)+Î·v4(t)+Î·2Î³ vw(t)C (t,s) (20)
3
s<t
whereweintroducedC (t,s)â‰¡ 1w(t)Â·w(s). Weseethat,conditionalonthecorrelationfunction
w N
C (t,s), the vector vw(t) can be interpreted as a linear filtered version of {v4(s)} and is thus
3 s<t
redundant. Inaddition,wenolongerhavetoworkwiththerandommatrixA(t)butcanrathertrack
projections of this matrix on vectors of interest. Since all dynamics only depend on the random
variablesV ={v0,v1,v2,v3},wethereforeonlyneedtocharacterizethejointdistributionofthese
variables.
Disorder Averages We now consider the averages over the random matrices which appear in
the dynamics {Î¨(t)} tâˆˆN and A(0). This can be performed with either a path integral or a cavity
derivationfollowingthetechniquesofBordelonetal.(2024a).Afteraveragingover{Î¨(t)}âˆ ,one
t=0
obtainsthefollowingprocessforv1(t)andv2(t)
k
v1(t)=u1(t), u1(t)âˆ¼N(0,Î´(tâˆ’s)C (t,t))
0
v2(t)=u2(t)+Î» v0(t), u2(t)âˆ¼N (cid:0) 0,Bâˆ’1Î´(tâˆ’s)Î» C (t,t)(cid:1) . (21)
k k k k k k 1
17Preprint
wherethecorrelationfunctionsC andC havetheforms
0 1
C
(t,s)=(cid:88)
Î»
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:10) v1(t)v1(s)(cid:11)
(22)
0 k k k 1
k
TheaverageoverthematrixA(0)couplesthedynamicsforv3(t),v4(t)resultinginthefollowing
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)âˆ¼N(0,C (t,s))
N 2,4 2
s<t
v4(t)=u4(t)+(cid:88) R (t,s)v2(s), u4 âˆ¼N (cid:0) 0,Nâˆ’1C (t,s)(cid:1) (23)
k k 3 k k 3
s<t
where
C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
, C
(t,s)=(cid:10) v3(t)v3(s)(cid:11)
(24)
2 k k 3
k
Lastly,wehavethefollowingsinglesiteequationforw(t)whichcanbeusedtocomputeC (t,s)
w
(cid:88)
w(t+1)âˆ’w(t)=Î·v3(t)+Î·2Î³ C (t,s)w(s). (25)
3
s<t
FinalDMFTEquationsforourModelforOnlineSGD Thecompletegoverningequationsfor
thetestlossevolutionafteraveragingovertherandommatricescanbeobtainedfromthefollowing
stochasticprocesseswhicharedrivenbyGaussiannoisesources{u2(t),u3(t),u4(t)}. LettingâŸ¨Â·âŸ©
k k
representaveragesoverthesesourcesofnoise,theequationscloseas
(cid:88)
v0(t)=wâ‹†âˆ’vw(t)âˆ’Î·Î³ C (t,s)v4(s)
k k k w k
s<t
(cid:88)
vw(t+1)=vw(t)+Î·v4(t)+Î·2Î³ C (t,s)vw(s)
k k k 3 k
s<t
v2(t)=u2(t)+Î» v0(t), u2(t)âˆ¼N (cid:0) 0,Bâˆ’1Î» Î´(tâˆ’s)C (t,t)(cid:1)
k k k k k k 0
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)âˆ¼N(0,C (t,s))
N 2,4 2
s<t
(cid:88)
w(t+1)=w(t)+Î·v3(t)+Î·2Î³ C (t,s)w(s)
3
s<t
(cid:88)
v4(t)=u4(t)+ R (t,s)v2(s), u4(t)âˆ¼N(0,Nâˆ’1C (t,s))
k k 3 k k 3
s<t
(cid:88)(cid:28) âˆ‚v2(t)(cid:29) (cid:28) âˆ‚v3(t)(cid:29)
R (t,s)= k , R (t,s)=
2,4 âˆ‚u4(s) 3 âˆ‚u3(s)
k k
C
(t,s)=(cid:88)
Î»
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
(26a)
0 k k k 2 k k
k k
Closing the DMFT Correlation and Response as Time Ã— Time Matrices We can try using
these equations to write a closed form expression for v0(t) which determines the generalization
k
error. First, we start by solving the equations for v kw = Vec{v kw(t)} tâˆˆN. We introduce a step
functionmatrixwhichisjustlowertriangularmatrix[Î˜] =Î·Î˜(tâˆ’s)
t,s
vw =[Iâˆ’Î·Î³Î˜C ]âˆ’1Î˜v4 â‰¡Hwv4
k 3 k k k
Hw â‰¡[Iâˆ’Î·Î³Î˜C ]âˆ’1Î˜ (27)
k 3
Now,combiningthiswiththeequationforv0 =Vec{v0(t)}wefind
k k
v0 =H0(cid:2) wâ‹†1âˆ’(Hw+Î·Î³C )(cid:0) u4 +R u2(cid:1)(cid:3)
k k k k w k 3 k
H0 =[I+Î» (Hw+Î·Î³C )R ]âˆ’1 (28)
k k k w 3
18Preprint
ThekeyresponsefunctionisR withentries[R ] =R (t,s)satisfiesthefollowingequation
3 3 t,s 3
M
1 (cid:88)
R =I+ R R , R =âˆ’ Î» H0(Hw+Î·Î³C )
3 N 2,4 3 2,4 k k k w
k=1
M
1 (cid:88)
=â‡’R =Iâˆ’ Î» H0(Hw+Î·Î³C )R
3 N k k k w 3
k=1
M
=Iâˆ’ 1 (cid:88) Î» [I+Î» (Hw+Î·Î³C )R ]âˆ’1(Hw+Î·Î³C )R (29)
N k k k w 3 k w 3
k=1
Lastly,wecancomputethecorrelationmatrixC fromthecovarianceC
w 3
C =[Iâˆ’Î·Î³C ]âˆ’1Î˜C Î˜âŠ¤[Iâˆ’Î·Î³C ]âˆ’1âŠ¤ (30)
w 3 3 3
Theremainingcorrelationfunctionsaredefinedas
(cid:20) (cid:18) (cid:19) (cid:21)
C =(cid:88) Î» H0 (wâ‹†)211âŠ¤+(Hw+Î·Î³C ) 1 C + Î» kR diag(C )RâŠ¤ (Hw+Î·Î³C )âŠ¤ [H0]âŠ¤
0 k k k k w N 3 B 3 0 3 k w k
k
C = 1 (cid:88) Î» (cid:0) Iâˆ’Î» H0(Hw+Î·Î³C )R (cid:1) diag(C )(cid:0) Iâˆ’Î» H0(Hw+Î·Î³C )R (cid:1)âŠ¤
2 B k k k k w 3 0 k k k w 3
k
(cid:20) (cid:21)
+(cid:88) H0 11âŠ¤(wâ‹†)2+ 1 (Hw+Î·Î³C )C (Hw+Î·Î³C )âŠ¤ (cid:2) H0(cid:3)âŠ¤
k k N k w 3 k w k
k
C =R C RâŠ¤ (31)
3 3 2 3
D OFFLINE TRAINING: TRAIN AND TEST LOSS UNDER SAMPLE REUSE
Our theory can also handle the case where samples are reused in a finite dataset Î¨ âˆˆ RPÃ—M. To
simplify this setting we focus on the gradient flow limit (this will preserve all of the interesting
finite-P effectswhilesimplifyingtheexpressions)
(cid:18) (cid:19)
d 1
w(t)=A(t) Î¨âŠ¤Î¨ v0(t)
dt P
(cid:18) (cid:19)(cid:18) (cid:19)
d 1 1
A(t)=Î³ w(t)v0(t)âŠ¤ Î¨âŠ¤Î¨ A(0)âŠ¤A(0) (32)
dt P N
Weintroducethefields
1
v1(t)=Î¨v0(t), v2(t)= Î¨âŠ¤v1(t) (33)
P
1
v3(t)=A(0)v2(t), v4(t)= A(0)âŠ¤v3(t) (34)
N
sothatthedynamicscanbeexpressedas
d
w(t)=A(t)v2(t)
dt
d
A(t)=Î³w(t)v4(t)âŠ¤ (35)
dt
Asbeforewealsointroducethefollowingfieldwhichshowsupinthev0(t)dynamics
1
vw(t)= A(0)âŠ¤w(t) (36)
N
DataAverage TheaverageoverthefrozendatamatrixÎ¨âˆˆRPÃ—M
(cid:90) t
v2(t)=u2(t)+Î» dsR (t,s)v0(s), u2(t)âˆ¼N(0,Pâˆ’1Î» C (t,s)) (37)
k k k 1 k k k 1
0
1 (cid:90) t
v1(t)=u1(t)+ dsR (t,s)v1(s), u1(t)âˆ¼N(0,C (t,s)) (38)
P 0,2 0
0
19Preprint
FeatureProjectionAverage NextweaverageoverA(0)âˆˆRNÃ—M withN/M =Î½ whichyields
(cid:90) t
v4(t)=u4(t)+ dsR (t,s)v2(s), u4(t)âˆ¼N(0,Nâˆ’1C (t,s)) (39)
k k 3 k k 3
0
1 (cid:90) t
v3(t)=u3(t)+ dsR (t,s)v3(s), u3(t)âˆ¼N(0,C (t,s)) (40)
N 2,4 2
0
Wecannowsimplyplugtheseequationsintothedynamicsofw(t),vw(t),v0(t)toobtainthefinal
DMFTequations.
FinalDMFTEquationsforDataReuseSetting Thecompletegoverningequationsforthetest
lossevolutionafteraveragingovertherandommatrices{Î¨,A}canbeobtainedfromthefollowing
stochasticprocesseswhicharedrivenbyGaussiannoisesources{u2(t),u3(t),u4(t)}. LettingâŸ¨Â·âŸ©
k k
representaveragesoverthesesourcesofnoise,theequationscloseas
(cid:90) t
v0(t)=wâ‹†âˆ’vw(t)âˆ’Î³ dsC (t,s)v4(s)
k k k w k
0
(cid:90) t
âˆ‚ vw(t)=v4(t)+Î³ dsC (t,s)vw(s)
t k k 3 k
0
1 (cid:90) t
v1(t)=u1(t)+ dsR (t,s)v1(s), u1(t)âˆ¼N(0,C (t,s))
P 0,2 0
0
(cid:90)
v2(t)=u2(t)+Î» dsR (t,s)v0(t), u2(t)âˆ¼N (cid:0) 0,Pâˆ’1Î» C (t,s)(cid:1)
k k k 1 k k k 1
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)âˆ¼N(0,C (t,s))
N 2,4 2
s<t
(cid:88)
w(t+1)=w(t)+Î·v3(t)+Î·2Î³ C (t,s)w(s)
3
s<t
(cid:88)
v4(t)=u4(t)+ R (t,s)v2(s), u4(t)âˆ¼N(0,Nâˆ’1C (t,s))
k k 3 k k 3
s<t
(cid:88)
(cid:28) âˆ‚v0(t)(cid:29) (cid:28) âˆ‚v1(t)(cid:29)
R (t,s)= Î» k , R (t,s)=
0,2 k âˆ‚u2(s) 1 âˆ‚u1(s)
k k
(cid:88)(cid:28) âˆ‚v2(t)(cid:29) (cid:28) âˆ‚v3(t)(cid:29)
R (t,s)= k , R (t,s)=
2,4 âˆ‚u4(s) 3 âˆ‚u3(s)
k k
C
(t,s)=(cid:88)
Î»
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:10) v1(t)v1(s)(cid:11)
0 k k k 1
k
C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
, C
(t,s)=(cid:10) v3(t)v3(s)(cid:11)
(41a)
2 k k 3
k
The Î³ â†’ 0 limit of these equations recovers the DMFT equations from Bordelon et al. (2024a)
whichanalyzedtherandomfeature(staticA)case.
E BOTTLENECK SCALINGS FOR POWER LAW FEATURES
Inthissetting,weinvestigatethescalingbehaviorofthemodelunderthesourceandcapacitycon-
ditionsdescribedinthemaintext:
Î» âˆ¼kâˆ’Î±, (wâ‹†)2Î» âˆ¼kâˆ’Î²Î±âˆ’1 (42)
k k k
E.1 TIMEBOTTLENECK
InthissectionwecomputethelossdynamicsinthelimitofN,B â†’âˆ.Westartwithaperturbative
argument that predicts a scaling law of the form L(t) âˆ¼ tâˆ’Î²(2âˆ’Î²) for Î² < 1. We then use this
20Preprint
approximationtobootstrapmoreandmorepreciseestimatesoftheexponent. Thefinalpredictionis
thelimitofinfinitelymanyapproximationstepswhichrecoversL(t) âˆ¼ tâˆ’ 12 +Î² Î². Wethenprovidea
self-consistencyderivationofthisexponenttoverifythatitisthestablefixedpointoftheexponent.
E.1.1 WARMUP: PERTURBATIONEXPANSIONOFTHEDMFTODERPARAMETERS
First, in this section, we investigate the N,B â†’ âˆ limit of the DMFT equations and study what
happensforsmallbutfiniteÎ³. ThisperturbativeapproximationwillleadtoanapproximationL âˆ¼
tâˆ’Î²(2âˆ’Î²). In later sections, we will show how to refine this approximation to arrive at our self-
consistentlycomputedexponent 2Î² . IntheN,B â†’âˆlimit,theDMFTequationssimplifyto
1+Î²
R â†’I , u4 â†’0, u2 â†’0, v4 â†’Î» v0 , C â†’C . (43)
3 k k k k k 3 2
Thedynamicsinthislimithavetheform
(cid:88)
v0(t)=wâ‹†âˆ’vw(t)âˆ’Î·Î³Î» C (t,s)v0(s)
k k k k w k
s<t
(cid:88)
vw(t+1)âˆ’vw(t)=Î·Î» v0(t)+Î·Î³ C (t,s)vw(s)
k k k k 2 k
s<t
(cid:88)
w(t+1)âˆ’w(t)=Î·v3(t)+Î·Î³ C (t,s)w(s)
2
s<t
C
(t,s)=(cid:88) Î»2(cid:10) v0(t)v0(s)(cid:11)
, C (t,s)=âŸ¨w(t)w(s)âŸ© (44)
2 k k k w
k
TheseexactdynamicscanbesimulatedaswedoinFigure2.However,wecanobtainthecorrectrate
of convergence by studying the following Markovian continuous time approximation of the above
dynamicswhereweneglecttheextraO(Î³)terminthevw(t)dynamics
k
d
v0(t)â‰ˆÎ» v0(t)âˆ’Î³Î» C (t)v0(t), C (t)â‰¡C (t,t)
dt k k k k w k w w
âˆ‚ C (t)â‰ˆC (t)+O(Î³), C (t)â‰¡C (t,t) (45)
t w 2 2 2
Thesolutionfortheerroralongthek-theigenfunctionwilltaketheform
(cid:18) (cid:90) t (cid:19)
v0(t)â‰ˆexp âˆ’Î» tâˆ’Î³Î» dsC (s) wâ‹† (46)
k k k w k
0
We next solve for the dynamics of C (t) and C (t) in the leading order Î³ â†’ 0 limit (under the
2 w
lineardynamics)
(cid:90)
C 2(t)âˆ¼(cid:88) Î»2 k(w kâ‹†)2eâˆ’2Î»kt âˆ¼ dkkâˆ’Î±âˆ’Î²Î±âˆ’1eâˆ’kâˆ’Î±t âˆ¼tâˆ’Î²
k
(cid:26) t1âˆ’Î² Î² <1
C (t)âˆ¼ (47)
w C (âˆ) Î² >1
w
whereC (âˆ)isalimitingfinitevalueofC (t).
w w
v0(t) (cid:26) exp(cid:0) âˆ’Î» tâˆ’Î³Î» t2âˆ’Î²(cid:1) Î² <1
k â‰ˆ k k (48)
wâ‹† exp(âˆ’Î» [1+Î³C (âˆ)]t) Î² >1
k k w
For Î² < 1, the feature learning term will eventually dominate. The mode k (t) which is being
â‹†
learnedattimetsatisfies
k (t)âˆ¼t(2âˆ’Î²)/Î± (49)
â‹†
whichimpliesthatthe
(cid:88)
Lâ‰ˆ (wâ‹†)2Î» âˆ¼tâˆ’Î²(2âˆ’Î²) (50)
k k
k>kâ‹†
However,thissolutionactuallyover-estimatestheexponent. Toderiveabetterapproximationofthe
exponent,weturntoaMarkovianperspectiveonthedynamicswhichholdsasN,B â†’âˆ.
21Preprint
E.1.2 MATRIXPERTURBATIONPERSPECTIVEONTHETIMEBOTTLENECKSCALINGWITH
MARKOVIANDYNAMICS
The limiting dynamics in the N,B â†’ âˆ limit can also be expressed as a Markovian system in
termsofthevectorv0(t)andamatrixM(t)whichpreconditionsthegradientflowdynamics
(cid:20) (cid:21)
d 1 Î³
v0(t)=âˆ’M(t)Î›v0(t), M(t)= A(t)âŠ¤A(t)+ |w(t)|2I
dt N N
d M(t)=Î³(cid:0) w âˆ’v0(t)(cid:1) v0(t)âŠ¤Î›+Î³Î›v0(t)(cid:0) w âˆ’v0(t)(cid:1)âŠ¤ +2Î³(w âˆ’v0(t))âŠ¤Î›v0(t)I
dt â‹† â‹† â‹†
(51)
Wecanrewritethissystemintermsofthefunctionâˆ†(t) = Î›1/2v0(t)withy = Î›1/2w andthe
â‹†
HermitiankernelmatrixK =Î›1/2M(t)Î›1/2then
d
âˆ†(t)=âˆ’K(t)âˆ†(t).
dt
d
K(t)=Î³(yâˆ’âˆ†(t))âˆ†(t)âŠ¤Î›+Î³Î›âˆ†(t)(yâˆ’âˆ†(t))âŠ¤+2Î³(yâˆ’âˆ†(t))Â·âˆ†(t)Î› (52)
dt
ThetestlosscanbeexpressedasL(t)=|âˆ†(t)|2.
Loss Dynamics Dominated by the Last Term in Kernel Dynamics We note that the loss dy-
namicssatisfythefollowingdynamicsatlargetimet
d
L(t)=âˆ’âˆ†(t)âŠ¤K(t)âˆ†(t)
dt
(cid:90) t
=âˆ’âˆ†(t)Î›âˆ†(t)âˆ’2Î³ ds[(yâˆ’âˆ†(s))Â·âˆ†(t)]âˆ†(t)âŠ¤Î›âˆ†(s)
0
(cid:90) t
âˆ’2Î³(âˆ†(t)âŠ¤Î›âˆ†(t)) ds(yâˆ’âˆ†(s))Â·âˆ†(s)
0
(cid:90) t (cid:90) t
âˆ¼âˆ’âˆ†(t)âŠ¤Î›âˆ†(t)âˆ’2Î³[yÂ·âˆ†(t)] dsâˆ†(t)âŠ¤Î›âˆ†(s)âˆ’2Î³(âˆ†(t)âŠ¤Î›âˆ†(t)) dsyÂ·âˆ†(s)
(cid:124) (cid:123)(cid:122) (cid:125) 0 0
LazyLimit (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Subleading Dominant
(53)
(cid:20) (cid:90) t (cid:21)
â‰ˆâˆ’âˆ†(t)âŠ¤Î›âˆ†(t) 1+2Î³ dsyÂ·âˆ†(s) . (54)
0
One can straightforwardly verify that the middle term is subleading compared to the final term is
thatundertheansatzthatâˆ† (t) âˆ¼
exp(cid:0)
âˆ’Î»
t2âˆ’Ï‡(cid:1)
whereÎ² â‰¤ Ï‡ â‰¤ 1forÎ² < 1. Wecantherefore
k k
focusonthelasttermwhenderivingcorrectionstothescalinglaw.
IntuitionPump: PerturbativeLevel1Approximation InthelazylimitÎ³ â†’ 0,K(t) = Î›for
all t. However, for Î³ > 0 this effective kernel matrix K(t) evolves in a task-dependent manner.
TocomputeK willapproximateM(t)withitsleadingorderdynamicsinÎ³,whichareobtainedby
evaluatingthev0(t)dynamicswiththelazylearningÎ³ â†’0solution. Wecanthusapproximatethe
kernelmatrixK(t)dynamicsas
K(t)â‰ˆÎ›+Î³yyâŠ¤(Iâˆ’exp(âˆ’Î›t))âŠ¤+Î³(Iâˆ’exp(âˆ’Î›t))yyâŠ¤
+2Î³(cid:2) yâŠ¤Î›âˆ’1(Iâˆ’exp(âˆ’Î›t))y(cid:3)
Î› (55)
Fromthisperspectiveweseethatthekernelhastwodynamicalcomponents. First,alowrankspike
growsinthekernel,eventuallyconvergingtotherankonematrixyyâŠ¤. Inaddition,thereisascale
growthoftheexistingeigenvaluesduetothelastterm(cid:2) yâŠ¤Î›âˆ’1(Iâˆ’exp(âˆ’Î›t))y(cid:3)
Î›,whichwill
approachthevalueoftheRKHSnormofthetargetfunctionastâ†’âˆ.Theeigenvalues{K (t)}âˆ
k k=1
ofthekernelK(t)evolveatleadingorderasthediagonalentries. AssumingthatÎ² <1theseterms
22Preprint
increasewithtas
K k(t)âˆ¼Î» k+2Î³y k2(cid:0) 1âˆ’eâˆ’Î»kt(cid:1) +2Î³Î» k(cid:88)y Î»â„“2 (1âˆ’eâˆ’Î»â„“t)
â„“
â„“
âˆ¼Î» k+2Î³y k2(cid:0) 1âˆ’eâˆ’Î»kt(cid:1) +2Î³Î» kt1âˆ’Î² (56)
Thedynamicsfortheerrorscanbeapproximatedas
âˆ‚
âˆ† (t)âˆ¼âˆ’K (t)âˆ† (t)
âˆ‚t k k k
(cid:18) (cid:90) t (cid:19)
(cid:112)
=â‡’ âˆ† (t)âˆ¼exp âˆ’ dsK (s) Î» wâ‹†
k k k k
0
âˆ¼exp(cid:0) âˆ’Î» tâˆ’2Î³Î» (wâ‹†)2tâˆ’2Î³Î» t2âˆ’Î²(cid:1)(cid:112) Î» wâ‹† (57)
k k k k k k
Forsufficientlylarget,thefinaltermdominatesandthemodek (t)whichisbeinglearnedattime
â‹†
tis
2âˆ’Î²
k â‹†(t)âˆ¼t Î± (58)
Thetestlossissimplythevarianceintheunlearnedmodes
(cid:88)
L(t)âˆ¼ (wâ‹†)2Î» âˆ¼tâˆ’Î²(2âˆ’Î²). (59)
k k
k>kâ‹†
Bootstrapping a More Accurate Exponent From the previous argument, we started with the
lazylearninglimitingdynamicsforv0(t) âˆ¼ eâˆ’Î»ktwâ‹† andusedthesedynamicstoestimatetherate
k k
at which M(t) (or equivalently K(t)) changes. This lead to an improved rate of convergence for
the mode errors v0(t), which under this next order approximation decay as v0(t) âˆ¼ eâˆ’Î»kt2âˆ’Î²wâ‹†.
k k k
Supposingthattheerrorsdecayatthisrate,wecanestimatethedynamicsofM(t)
dd tK k(t)â‰ˆÎ» k(cid:88) (w â„“â‹†)2Î» â„“eâˆ’Î»â„“t2âˆ’Î² â‰ˆÎ» ktâˆ’Î²(2âˆ’Î²), (60)
â„“
(cid:16) (cid:17)
=â‡’ v0(t)âˆ¼exp âˆ’Î» t2âˆ’Î²(2âˆ’Î²) wâ‹† , (Level2Approximation) (61)
k k k
Wecanimaginecontinuingthisapproximationschemetohigherandhigherlevelswhichwillyield
aseriesofbetterapproximationstothepowerlaw
ï£±
tâˆ’Î² Level0Approximation
ï£´ï£´ï£´ï£´ï£´ï£²tâˆ’Î²(2âˆ’Î²)
Level1Approximation
L(t)âˆ¼ tâˆ’Î²[2âˆ’Î²(2âˆ’Î²)] Level2Approximation (62)
ï£´ï£´ï£´ï£´ï£´ï£³.t .âˆ’ .Î²[2âˆ’Î²(2âˆ’Î²(2âˆ’Î²))] Level3Approximation
WeplotthefirstfewoftheseinFigure10,showingthattheyapproachalimitasthenumberoflevels
diverges. Asnâ†’âˆ,thisgeometricserieswilleventuallyconvergetotâˆ’ 12 +Î² Î².
E.2 SELF-CONSISTENTDERIVATIONOFTHEINFINITELEVELSCALINGLAWEXPONENT
Fromtheaboveargument,itmakessensetowonderwhetherornotthereexistsafixedpointtothis
seriesofapproximationsthatwillactuallyyieldthecorrectexponentinthelimitofinfinitelymany
steps. Indeedinthissection,wefindthatthislimitcanbecomputedself-consistently
d (cid:88)
M(t)â‰ˆÎ³Î» wâ‹†Î» v0(t) (63)
dt k â„“ â„“ â„“
â„“
(cid:90) t
(cid:88)
=â‡’ M(t)=1+Î³ ds wâ‹†Î» v0(t) (64)
â„“ â„“ â„“
0 â„“
(cid:18) (cid:90) t (cid:19)
v (t)âˆ¼exp âˆ’Î» dtâ€²M(tâ€²) wâ‹† (65)
k k k
0
23Preprint
1.2
1.0
0.8 Level 0
Level 1
0.6 Level 2
Level 3
0.4
Level 4
Level 5
0.2
Level 6
Level
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Figure10: PredictionsforthelossscalingexponentÏ‡ (Î²)atvaryinglevelsnoftheapproximation
n
scheme. Our final prediction is the infinite level limit which gives 2 . This agrees with a self-
1+Î²
consistencyargument.
Wecandefinetheintermediatevariable
(cid:88)
B(t)= wâ‹†Î» v0(t) (66)
k k k
k
whichhasself-consistentequation
(cid:32) (cid:90) t (cid:34) (cid:90) tâ€² (cid:35)(cid:33)
(cid:88)
B(t)= Î» (wâ‹†)2exp âˆ’Î» dtâ€² 1+Î³ dsB(s) (67)
k k k
k 0 0
WecanseekasolutionoftheformB(t)âˆ¼tâˆ’Ï‡. Thisyields
(cid:26) (cid:27)
2
tâˆ’Ï‡ â‰ˆtâˆ’max{Î²,Î²(2âˆ’Ï‡)} =â‡’ Ï‡=Î²max 1, . (68)
Î²+1
UsingthesolutionforB(t),wecanalsoderivethescalingforthelosswhichisidenticalL(t)âˆ¼tâˆ’Ï‡.
Wenotethatthisargumentalsoleadstoanapproximatedoublingoftheexponentcomparedtothe
lazy case for Î² < 1, however this is slightly disagreement with the perturbative approach which
yieldsÎ²(2âˆ’Î²)forÎ² <1.
ThisargumentiswherewedevelopedthegeneralexpressionthatdeterminesÏ‡whichwasprovided
inthemaintext
(cid:34) (cid:35)
Ï‡=âˆ’ lim 1 ln (cid:88) (wâ‹†)2Î» exp(cid:0) âˆ’Î» (cid:2) t+Î³t2âˆ’Ï‡(cid:3)(cid:1) . (69)
tâ†’âˆlnt k k k
k
E.3 FINITEMODELSIZEBOTTLENECK
Inthelimitoft â†’ âˆ,thedynamicsfor{v0(t)}willconvergetoafixedpointthatdependsonN.
k
Toascertainthevalueofthisfixedpoint,wefirstmustcomputetheasymptotics. First,wenotethat
24
)
(
nPreprint
correlationandresponsefunctionsreachthefollowingfixedpointbehaviors
(cid:90) t
lim dtâ€²R (tâ€²,s)âˆ¼ r Î´(tâˆ’s)
3 3
t,sâ†’âˆ
0
lim R (t,s)=r Î˜(tâˆ’s)
2,4 2,4
t,sâ†’âˆ
lim C (t,s)=c
w w
t,sâ†’âˆ
(cid:90) t
lim dtâ€²C (tâ€²,s)=0 (70)
3
t,sâ†’âˆ
0
whichgivesthefollowinglongtimebehavior
(cid:90) t (cid:90) t (cid:90) tâ€²
v0(t)âˆ¼wâ‹†âˆ’ dtâ€²u4(tâ€²)âˆ’Î» dtâ€² dsR (tâ€²,s)v0(s) (71)
k k k k 3 k
0 0 0
(cid:90) t
âˆ¼wâ‹†âˆ’ dtâ€²u4(tâ€²)âˆ’Î» r v0(t) (72)
k k k 3 k
0
=â‡’ r âˆ¼âˆ’(cid:88) Î» k (73)
2,4 1+Î» r
k 3
k
Usingtheasymptoticrelationshipbetween 1r r = Î½,wearriveatthefollowingself-consistent
N 3 2,4
equationforr
3
1= 1 (cid:88) Î» kr 3 (74)
N 1+Î» r
k 3
k
Forpowerlawfeaturesthisgives
(cid:90) kâˆ’Î±r
N â‰ˆ dk 3 â‰ˆ[r ]1/Î± =â‡’ r âˆ¼NÎ± (75)
kâˆ’Î±r +1 3 3
3
whichrecoversthecorrectscalinglawwithmodelsizeN.
lim C
(t,s)=(cid:88) Î» k(w kâ‹†)2
t,sâ†’âˆ 0 (1+Î» kNÎ±)2
k
(cid:90) N (cid:90) âˆ
â‰ˆNâˆ’2Î± dkkâˆ’Î±Î²âˆ’1+2Î±+ dkkâˆ’Î²Î±âˆ’1 âˆ¼Nâˆ’Î±min{2,Î²}
1 N
(76a)
E.4 FINITEDATABOTTLENECK(DATAREUSESETTING)
ThedatabottleneckwhentrainingonrepeatedP trainingexamplesisverysimilar. Inthiscase,the
relevantresponsefunctionstotrackareR (t,s)andR (t,s)whichhavethefollowinglargetime
1 0,2
propertiesast,sâ†’âˆ
(cid:90) t
dtâ€²R (tâ€²,s)âˆ¼ r Î´(tâˆ’s)
1 1
0
R (t,s)âˆ¼r Î˜(tâˆ’s)
0,2 0,2
Underthisansatz,wefindthefollowingexpressionforr astâ†’âˆ
1
1âˆ¼ 1 (cid:88) Î» kr 1 . (77)
P 1+Î» r
k 1
k
25Preprint
Followinganidenticalargumentabovewefindthatr âˆ¼PÎ±,resultinginthefollowingasymptotic
1
testloss
lim C
(t,s)=(cid:88) Î» k(w kâ‹†)2
t,sâ†’âˆ 0 (1+Î» kPÎ±)2
k
(cid:90) P (cid:90) âˆ
â‰ˆPâˆ’2Î± dkkâˆ’Î±Î²âˆ’1+2Î±+ dkkâˆ’Î²Î±âˆ’1 âˆ¼Pâˆ’Î±min{2,Î²}.
1 P
(78a)
F TRANSIENT DYNAMICS
To compute the transient 1/N and 1/B effects, it suffices to compute the scaling of a response
function/Volterrakernelatleadingorder.
F.1 LEADINGBIASCORRECTIONATFINITEN
Atleadingorderin1/N thebiascorrectionsfromfinitemodelsizecanbeobtainedfromthefollow-
ingleadingorderapproximationoftheresponsefunctionR (t,s)
3
R 3(t,s)âˆ¼Î´(tâˆ’s)âˆ’
N1 (cid:88)
Î»
keâˆ’Î»k(tÏ‡/Î²âˆ’sÏ‡/Î²)+O(Nâˆ’2)
k
1
âˆ¼Î´(tâˆ’s)+ (tÏ‡/Î² âˆ’sÏ‡/Î²)âˆ’1+1/Î± (79)
N
(cid:110) (cid:111)
whereÏ‡=Î²max 1, 2 .FollowingPaquetteetal.(2024),wenotethatthescalingofR (t,s)âˆ’
1+Î² 3
Î´(tâˆ’s) determines the scaling of the finite width transient. Thus the finite width effects can be
approximatedas
1
L(t,N)âˆ¼tâˆ’Î²max{1, 1+2 Î²}+Nâˆ’Î±min{2,Î²}+ tâˆ’(1âˆ’1/Î±)max{1, 1+2 Î²}. (80)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N
LimitingDynamics ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125)
FiniteModelTransient
InthecasewhereÎ² >1thisagreeswiththetransientderivedinPaquetteetal.(2024). Howeverfor
Î² <1,thefeaturelearningdynamicsacceleratethedecayrateofthisterm.
F.2 LEADINGSGDCORRECTIONATFINITEB
WestartbycomputingtheN â†’âˆlimitoftheSGDdynamicscanbeapproximatedas
C (t)â‰ˆ(cid:88) (wâ‹†)2Î» exp(cid:0) âˆ’2Î» (t+Î³t2+Ï‡)(cid:1) + Î· (cid:90) t dsK(t,s)C (s) (81)
0 k k k B 0
k 0
(cid:110) (cid:111)
whereÏ‡=Î²max 1, 2 andK(t,s)istheVolterra-kernelforSGDPaquetteetal.(2021;2024),
1+Î²
whichinourcasetakestheform
K(t,s)=(cid:88)
Î»
exp(cid:0)
âˆ’2Î»
(cid:2) (t+Î³t2âˆ’Ï‡)âˆ’(sâˆ’Î³s2âˆ’Ï‡)(cid:3)(cid:1)
k k
k
(cid:16)
max(1,2âˆ’Ï‡)
max(1,2âˆ’Ï‡)(cid:17)âˆ’(Î±âˆ’1)
âˆ¼ t Î± âˆ’s Î± (82)
Since the transient dynamics are again generated by the long time behavior of K(t,0), we can
approximatetheSGDdynamicsas
Î·
L(t,B)â‰ˆtâˆ’Î²max{1, 1+2 Î²}+ tâˆ’(1âˆ’1/Î±)max{1, 1+2 Î²} . (83)
(cid:124) (cid:123)(cid:122) (cid:125) B
GradientFlow (cid:124) (cid:123)(cid:122) (cid:125)
SGDNoise
26Preprint
Source Cap a Mc =ity 1 L 0i 0n 0e 0a ,r NN =etw 10o 0rk , , B == 12 2. 800, =1.20 Source Cap a Mc =ity 1 L 0i 0n 0e 0a ,r NN =etw 10o 0rk , , B == 12 2. 800, =0.20
11 00 10 t0 0 0 0= = = =1 1 1 1e e e e+ - - -0 0 00 1 2 30 100
t
t0 0 0 0 2= = = = /(1 1 1 1 1e e e e ++ - - -0 0 0 )0 1 2 30
102
103
100 101 102 103 104 100 101 102 103 104
Steps Steps
(a) (b)
Figure11: LinearNetworksa) Î² > 1, whereacrossvaluesofÎ³, weobservethesameasymptotic
scalinggoingastâˆ’Î² aspredictedbykerneltheory. b)Î² <1,wherefeaturelearninglinearnetworks
achieveanimprovedscalingaspredictedbyourtheory.
Asbefore,theÎ² > 1caseisconsistentwiththeestimatefortheVolterrakernelscalinginPaquette
etal.(2024).
G LINEAR NETWORK DYNAMICS UNDER SOURCE AND CAPACITY
Inthissection,weshowhowinasimplelinearnetwork,theadvantageinthescalingpropertiesof
the loss due to larger Î³ is evident. Here, we consider a simple model of a two-layer linear neural
network trained with vanilla SGD online. We explicitly add a feature learning parameter Î³. We
study when this linear network can outperform the rate of tâˆ’Î² given by linear regression directly
from input space. Despite its linearity, this setting is already rich enough to capture many of the
powerlawsbehaviorsobservedinrealisticmodels.
G.1 MODELDEFINITION
FollowingChizatetal.(2019),thenetworkfunctionisparameterizedas:
1
f(x;t)= (fËœ(x;t)âˆ’fËœ(x;0)), fËœ(x;t)=wâŠ¤Ax, (84)
Î³
WeletxâˆˆRM andtakehiddenlayerwidthN sothatAâˆˆRNÃ—M,w âˆˆRN,asinthemaintext.
Wetrainthenetworkonpowerlawdataofthefollowingform
xâˆ¼N(0,Î›), y =wâˆ—Â·x. (85)
WeimposetheusualsourceandcapacityconditionsonÎ›andwâˆ—asinequation6.
G.2 IMPROVEDSCALINGSONLYBELOWÎ² <1
WeempiricallyfindthatwhenÎ² > 1, largeÎ³ networksdonotachievebetterscalingthansmallÎ³
ones. By contrast, when Î² < 1 we see an improvement to the loss scaling. We illustrate both of
thesebehaviorsinFigure11.Empirically,weobservearateoftâˆ’2Î²/(1+Î²)fortheselinearnetworks.
Thisisthesameimprovementderivedfortheprojectedgradientdescentmodelstudiedinthemain
text.
G.3 TRACKINGTHERANKONESPIKE
In the linear network setting, one can show that this improved scaling is due to the continued the
growthofarankonespikeinthefirstlayerweightsAofthelinearnetwork. Bybalancing,asinDu
27
ssoL ssoLPreprint
Linear Network, =2.00, =1.20 Linear Network, =2.00, =0.20
M=10000, N=100, B=128 M=10000, N=100, B=128
102
0=1e-03
103
0=1e-03
0=1e-02 0=1e-02
0=1e-01 0=1e-01
0=1e+00
102
t10=1e+00
t1 2/(1+ )
101
101
100 100
100 101 102 103 104 100 101 102 103 104
t t
(a) (b)
Figure12: Westudythegrowthofthespikeasmeasuredby|w|2. (a)Foreasytasks,wherewâˆ— is
finite,thespikegrowstofinitesize,andthenplateaus. Thisleadstoamultiplicativeimprovementin
theloss,butdoesnotchangethescalingexponent. (b)Whenthetaskishard,wcontinuestogrow
withoutbound. Boththeperturbativescalingoft1âˆ’Î² andthescalingt1âˆ’2Î²(1+Î²) obtainedfromthe
self-consistentequation10areplotted. Weseeexcellentagreementwiththelatterscaling.
etal.(2018),thisismatchedbythegrowthof|w|2. Thiswhichwillcontinuetogrowextensivelyin
timeDonlywhenÎ² <1. WeillustratethesetwocasesinFigure12.
28
2|w| 2|w|