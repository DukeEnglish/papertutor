Preprint
EMOVA : EMPOWERING LANGUAGE MODELS TO
SEE, HEAR AND SPEAK WITH VIVID EMOTIONS
KaiChen1‚àó,YunhaoGou1,6‚àó,RunhuiHuang2‚àó,ZhiliLiu1,3‚àó,DaxinTan3‚àó,JingXu4,
ChunweiWang3,YiZhu3,YihanZeng3,KuoYang3,DingdongWang4,KunXiang5,
HaoyuanLi5,HaoliBai3,JianhuaHan3,XiaohuiLi3,WeikeJin3,NianXie3,YuZhang6,
JamesT.Kwok1,HengshuangZhao2,XiaodanLiang5,Dit-YanYeung1,XiaoChen3,
ZhenguoLi3,WeiZhang3,QunLiu3,LanqingHong3‚Ä†,LuHou3‚Ä†,HangXu3‚Ä†
1HongKongUniversityofScienceandTechnology 2TheUniversityofHongKong
3HuaweiNoah‚ÄôsArkLab 4TheChineseUniversityofHongKong
5SunYat-senUniversity 6SouthernUniversityofScienceandTechnology
ProjectPage: https://emova-ollm.github.io/
ABSTRACT
GPT-4o,anomni-modalmodelthatenablesvocalconversationswithdiverseemo-
tionsandtones,marksamilestoneforomni-modalfoundationmodels. However,
empoweringLargeLanguageModelstoperceiveandgenerateimages,texts,and
speechesend-to-endwithpubliclyavailabledataremainschallengingintheopen-
sourcecommunity. Existingvision-languagemodelsrelyonexternaltoolsforthe
speechprocessing,whilespeech-languagemodelsstillsufferfromlimitedoreven
withoutvision-understandingabilities. Toaddressthisgap,weproposeEMOVA
(EM-otionallyOmni-presentVoiceAssistant),toenableLargeLanguageModels
withend-to-endspeechcapabilitieswhilemaintainingtheleadingvision-language
performance. Withasemantic-acousticdisentangledspeechtokenizer,wenotice
surprisinglythatomni-modalalignmentcanfurtherenhancevision-languageand
speechabilitiescomparedwiththecorrespondingbi-modalalignedcounterparts.
Moreover,alightweightstylemoduleisproposedforflexiblespeechstylecontrols
(e.g.,emotionsandpitches). Forthefirsttime,EMOVAachievesstate-of-the-art
performanceonboththevision-languageandspeechbenchmarks,andmeanwhile,
supportingomni-modalspokendialoguewithvividemotions.
1 INTRODUCTION
OpenAIGPT-4o(OpenAI,2024),anewmilestoneforomni-modalfoundationmodels,hasrekindled
people‚Äôsattentionsonintelligentassistantsthatcansee(i.e.,perceivingfine-grainedvisualinputs),
hear(i.e.,understandingvocalinstructions)andspeak(i.e.,generatingvocalresponses)simultane-
ously. MostexistingMulti-modalLargeLanguageModels(MLLMs)focusontwomodalitiesonly,
eithervision-language(Baietal.,2023;Lietal.,2024a)orspeech-language(Chuetal.,2024;Xie&
Wu,2024),demonstratingseveredemandsforomni-modalmodelswithvisual,languageandspeech
abilities. HowtoeffectivelyempowerLargeLanguageModels(LLMs)toprocessomni-modaldata
inanend-to-endmannerremainsanopenquestion.
Existingomni-modalLLMs(Chenetal.,2024b;Fuetal.,2024b)generallybuilduponVisionLLMs
andintegratethespeechmodalitybyadoptingaspeechencoderlikeWhisper(Radfordetal.,2023),
whichextractscontinuousfeaturesfromspeech,similartohowimagesareprocessed,andenables
speechunderstanding. However,thesemodelsstillrelyonexternalText-to-Speech(TTS)toolsfor
generatingspeechresponses,limitingtheirabilitytosupportreal-timeinteractions. AnyGPT(Zhan
etal.,2024),instead,optsforafullydiscretizationmanner,whichfirstdiscretizesalldatamodalities
(i.e.,images,texts,andspeeches),followedbyomni-modalauto-regressivemodeling. Thisenables
AnyGPTtohandlemultiplemodalitieswithaunifiedend-to-endframework,facilitatingreal-time
interactionswiththehelpofstreamingdecoding. However,thediscretevisiontokenizeradopted
‚àóEqualcontribution,listedinalphabeticalorderbysurname.
‚Ä†Correspondingauthors:{honglanqing, houlu3, xu.hang}@huawei.com
1
4202
peS
62
]VC.sc[
1v24081.9042:viXraPreprint
Speech Decoder Style Encoder
Output Speech ùíÖùíÖ(‚ãÖ,‚ãÖ) ùíàùíà(‚ãÖ)
ùíêùíê
ùíÄùíÄùë∫ùë∫ Style
Prototypes
EMOVA Language Model ‚ãÆ
ùíáùíá(‚ãÖ) Style Label
Quantizer
Projector
Semantic-acoùííùííu(s‚ãÖ)tic
Vision Encùíëùíëod(‚ãÖe)r What color Disentanglement
is the cat?
Speech Encoder
ùíóùíó ‚ãÖ Input Image Input Text Input Speech
Figure1:
ModelarchitectureofEMùëøùëøOùëΩùëΩ VA.Thevisùëøùëøioùëªùëª nencoderextraùëøùëøcùë∫ùë∫ tscontinuouùíîùíî s(‚ãÖ v)
isualfeatures,
whichareprojectedintothetextembeddingspaceasvisualtokens,whiletheinputspeechisencoded
andquantizedintodiscreteunits. Giventheomni-modalinputs,EMOVAcangeneratebothtextual
HUAWEI CONFIDENTIAL ÂÜÖÈÉ®ËµÑÊñôÊ≥®ÊÑè‰øùÂØÜ
andspeechresponseswithvividemotionalcontrols. CheckSec.3formorearchitecturaldetails.
byAnyGPTstrugglestocapturevisualdetails,especiallyforhigh-resolutionimages,makingitfar
behinditscontinuouscounterpartsonvision-languagebenchmarks. Moreover,noneoftheexisting
worksexplorespeechstylecontrols(e.g.,emotionsandpitches)withLLMs.Therefore,ourquestion
arises: Howtobuildanend-to-endomni-modalLLMenablingspokendialoguewithvividemotions
whilemaintainingstate-of-the-artvision-languageperformance?
Inthispaper, wepropose EMOVA (EMotionallyOmni-presentVoiceAssistant), anovelend-to-
endomni-modalLLMwithstate-of-the-artvision-languageandspeechcapabilitieswhilesupporting
emotionalspokendialogue. Fig.1showsanoverviewofthemodelframework. Acontinuousvision
encodercapturesthefine-grainedvisualdetails,whilethediscretespeechtokenizeranddetokenizer
enabletheend-to-endspeechunderstandingandgeneration. Specifically,thespeech-to-unit(S2U)
tokenizerconvertstheinputspeechwaveformsintodiscretespeechunitsasLLMinputs,whilethe
unit-to-speech(U2S)detokenizerreconstructsthespeechwaveformsfromtheLLM‚Äôsoutputspeech
units. ToseamlesslyintegratethespeechmodalitywithLLMs,wemeticulouslydesignasemantic-
acousticdisentangledspeechtokenizertodecouplethesemanticcontentsandacousticstylesofthe
inputspeeches(Taoetal.,2024),where1)semanticcontent(i.e.,whatitsays)capturesthesemantic
meaningsofinputspeeches,whichisfinallydiscretizedandalignedwithLLMs,while2)acoustic
style(i.e.,howitsays)capturesthediversespeechstyles(e.g.,emotionsandpitches). Utilizingthe
semantic-acousticdisentanglementofourspeechtokenizer,wefurtherintroducealightweightstyle
moduletosupportspokendialoguewithvividemotionsandpitches. AsinSec.4.1,thisinnovative
disentanglementdesignbetterfacilitatesthemodalityalignmentbetweentextsandspeecheswhile
maintainingflexibilityfordiversespeechstylecontrollabilityandpersonalization.
Withtheend-to-endomni-modalarchitectureofEMOVA,weempiricallydemonstratethatpublicly
availablebi-modalimage-textandspeech-textdataaresufficientforomni-modalalignment,utilizing
thetextmodalityasabridge.Thiseliminatestheneedforomni-modaldata(i.e.,image-text-speech),
whichisusuallyscarce. Surprisingly,wefindthatomni-modalalignmentcanfurtherimproveboth
vision-languageandspeechcapabilitiesthroughjointoptimization,evenwhencomparedwiththeir
bi-modalalignedcounterparts.Finally,onlyasmallamountofmixed-modalitysamplesarerequired
toteachthemodeltorespondinthedesiredformat. Forthefirsttime, EMOVA achievesstate-of-
the-artperformanceonbothvision-languageandspeechbenchmarks(seeTable1forcomparisons).
Themaincontributionsofthisworkcontainthreeparts:
1. WeproposeEMOVA,anovelend-to-endomni-modalLLMthatcansee,hearandspeak.
Weuseacontinuousvisionencoderandasemantic-acousticdisentangledspeechtokenizer
forseamlessomni-modalalignmentanddiversespeechstylecontrollability.
2. Weintroduceanefficienttext-centricomni-modalalignmentwhichcanfurtherimprovethe
vision-languageandspeechcapabilities, evencomparedwiththecorrespondingbi-modal
alignedcounterparts(i.e.,image-textonlyandspeech-textonlyalignment).
3. Forthefirsttime, our EMOVA achievestate-of-the-artcomparableperformanceonboth
thevision-languageandspeechbenchmarkssimultaneously,whilesupportingflexiblespo-
kendialogueswithvividemotions.
2Preprint
Table 1: Comparison among Multi-modal Large Language Models. Our EMOVA is the very
firstunifiedOmni-modalLargeLanguageModelcapableofemotionalspokendialoguewithstate-
of-the-artvision-languageandspeechcapabilitiessimultaneously.
Speech
Method Visual Text
Understand Generation Emotional
LLaVA ‚úì ‚úì ‚úó ‚úó ‚úó
Vision Intern-VL ‚úì ‚úì ‚úó ‚úó ‚úó
Qwen-Audio ‚úó ‚úì ‚úì ‚úó ‚úó
Speech Mini-Omni ‚úó ‚úì ‚úì ‚úì ‚úó
LLaMA-Omni ‚úó ‚úì ‚úì ‚úì ‚úó
Intern-Omni ‚úì ‚úì ‚úì ‚úó ‚úó
VITA ‚úì ‚úì ‚úì ‚úó ‚úó
Omni Any-GPT ‚úì ‚úì ‚úì ‚úì ‚úó
EMOVA(ours) ‚úì ‚úì ‚úì ‚úì ‚úì
2 RELATED WORK
VisionLargeLanguageModels (VLLMs)integratevisionmodalityintoLargeLanguageModels
(LLMs)(Touvronetal.,2023;Chenetal.,2023b),enablingadvancedunderstandingandreasoning
overvisualinstructions(Liuetal.,2024b;Baietal.,2023;Gouetal.,2023;2024). Recenteffortsin
VLLMscanbebroadlycategorizedintothreedirections,including1)Visionencoders(Oquabetal.,
2023;Chenetal.,2021b;2023a)areenhancedandaggregatedforrobustrepresentations(Linetal.,
2023; Li et al., 2024b; Tong et al., 2024). 2) High-resolution methods are proposed to overcome
thefixedresolutionofpre-trainedvisionencoders(e.g.,336√ó336forCLIP(Radfordetal.,2021)),
empoweringLLMstoperceivefine-grainedvisualinformation(Liuetal.,2024a;Dongetal.,2024;
Huangetal.,2024;Luoetal.,2024). 3)High-qualityinstructiondataisessentialfortheVLLMsto
generateaccurateandwell-formedresponsesfollowinginstructions(Laurenc¬∏onetal.,2024;Lietal.,
2024a;Chenetal.,2024b). Inthispaper,besidesachievingstate-of-the-artvision-languageperfor-
mance,wefurtherintroducespeechunderstandingandgeneratingcapabilitiesintoourEMOVA.
SpeechLargeLanguageModels (SLLMs)empowerspeechinteractionwithLLMs. Continuous
SLLMs(Wuetal.,2023;Chuetal.,2024)utilizethespeechencoders(Radfordetal.,2023)toextract
continuousspeechembeddingsforLLM,which,however,onlysupportspeechunderstanding,rely-
ingonexternalTTSmodulesforspeechgeneration,andtherefore,hamperingreal-timeinteraction.
DiscreteSLLMs(Zhangetal.,2023a),instead,firstdiscretizespeechsignalswithspeechtokenizers,
followed by auto-regressive modeling. Recent works (Fang et al., 2024; Xie & Wu, 2024) further
combinethecontinuousspeechencoderswiththediscretespeechtokenizersforbetterperformance.
Althougheffective,noneoftheexistingworksexplorespeechstylecontrollabilityinSLLMs(e.g.,
genders,emotions,andpitches),whichisessentialforreal-lifespokendialogue.
Omni-modalLargeLanguageModels supportvisual,text,andspeechcapabilitieswithaunified
architecturesimultaneously. SimilartothecontinuousSLLMs,InternOmni(Chenetal.,2024b)and
VITA (Fuet al.,2024b) connecta speechencoder withVLLMs, supportingspeech understanding
only. Instead,AnyGPT(Zhanetal.,2024)proposesaunifiedarchitecturetodiscretizeandconduct
auto-regressive modeling for image, text, and audio simultaneously, which, however, suffers from
inevitableinformationlossbroughtbydiscretization,especiallyforthehigh-resolutionvisualinputs.
Inthiswork,weproposeEMOVA,theveryfirstunifiedOmni-modalLargeLanguageModelswith
state-of-the-artvision-languageandspeechperformanceatthesametime.
3 ARCHITECTURE
3.1 FORMULATION
DenotetheLargeLanguageModel(LLM)asf(¬∑)andthetext,visualandspeechinputsasX ,X
T V
andX ,respectively. X isconvertedtodiscretetokensU viaatexttokenizer(Gage,1994),while
S T T
thevisualinputX isfirstencodedwithavisionencoderv(¬∑)asE = v(X ),andthenprojected
V V V
intothetextembeddingspacewithaprojectorp(¬∑)asH = p(E ). AsforthespeechinputX ,a
V V S
3Preprint
Speech-to-Unit(S2U)procedureisrequired. Specifically,X firstpassesthroughaspeechencoder
S
s(¬∑)asE =s(X ),whichisthendiscretizedbythequantizerq(¬∑)asU =q(E ). TheLLMf(¬∑)
S S S S
isthentrainedtocomputethejointprobabilityoftheoutputtextandspeechunitsUo,Uo as
T S
L
(cid:89)
P(Uo,Uo|U ,U ,H )= P(x |Uo ,Uo ,U ,U ,H ), (1)
T S T S V i T,<i S,<i T S V
i=1
wherex ‚ààUo ‚à™Uo andL=|Uo|+|Uo|. TheoutputresponseunitsUo arethenrecoveredinto
i T S T S S
theoutputspeechwaveformYo viaaUnit-to-Speech(U2S)decoderd(¬∑,¬∑)togetherwithanemotion
S
styleembeddingEo torealizevividemotionalspokendialoguecontrollability(Sec.3.2).
style
LLM. WeadopttheLLaMA-3.1-8B(Dubeyetal.,2024)asourbaseLLMf(¬∑),duetoitssuperior
performanceamongpubliclyavailablecheckpoints,whichisequippedwithatiktokentexttokenizer
andavocabularysizeof128,256,supportingbothmultilingualtextualinputsandoutputs.
Visionencoderandprojector. WeutilizeInternViT-6B(Chenetal.,2024b)asourvisualencoder
v(¬∑)with448√ó448baseresolution,andC-Abstractor(Chaetal.,2024)withtwoResBlocks(both
beforeandafterthepoolinglayer)and4√ódownsamplerateasvisionprojectorp(¬∑). Toprocessthe
high-resolutioninputs,thehigh-resolutionimage-slicing(Liuetal.,2024a)isused,wherevisualto-
kensforoneimageareconcatenationwithalow-resolutionthumbnailandtheoriginhigh-resolution
imagewithseparatorsineachline,allowingamaximumofninetilesduringtraining.
3.2 SPEECHTOKENIZATION
Speech-to-unit(S2U)tokenizer. FollowingTaoetal.(2024),weadopttheSPIRAL(Huangetal.,
2022)architectureforthespeechencoders(¬∑)tocapturebothphoneticandtonalinformation,which
isthendiscretizedbythequantizerq(¬∑)utilizingthefinitescalarquantization(FSQ) (Mentzeretal.,
2023). The size of the speech codebook is 4,096, while the sample rate is 25 tokens per second.
Oncediscretized,thespeechmodalitycanbesimplyintegratedintoLLMsbyconcatenatingthetext
vocabularyandspeechcodebook.
OurS2Utokenizerprovidesthefollowingadvantages:1)Dataefficiency:afterpre-trainingonlarge-
scaleunlabeledspeechdata,itrequiresonlyasmallamountofspeech-textpairdataforfine-tuning.
2)Bilingual: thespeechcodebookissharedamongdifferentlanguages(i.e.,EnglishandChinese),
sharingunitmodelingabilitiesacrosslanguages. Checkmoretrainingdetailsandcomparisonswith
otherspeechtokenizers(Zhangetal.,2023b)inAppendixA.1.
Semantic-acousticdisentanglement. Toseamlesslyalignspeechunitswiththehighlysemantic
embedding space of LLMs, we opt for decoupling the semantic contents and acoustic styles of
input speeches. Specifically, given input speechs X , both semantic embedding E and
S semantic
styleembeddingsE areextractedseparately, whileonlytheE isquantifiedbyq(¬∑)to
style semantic
generatespeechunitsU . BychangingE whilemaintainingthesameE ,wecaneasily
S style semantic
controlspeechstyleswithoutdisturbingthesemanticcontentsofrecoveredspeeches. Moreover,the
disentanglementfacilitatesmodalityalignmentamongspeechesandtexts,aslatershowninSec.4.1.
Unit-to-speech(U2S)detokenizerwithstylecontrol. BuildingonVITS(Kimetal.,2021),our
U2SdetokenizeradoptsaconditionalVAEarchitecture(seeFig.9). Toachievevividstylecontrols,
weutilizethesemantic-styledisentanglementofourS2Utokenizer(asdiscussedabove)andadopt
anovelstyleembeddingtocontrolthespeechstyles(e.g.,speakeridentities,emotions,andpitches).
Specifically,theLLMf(¬∑)istrainedtogenerateboththeoutputspeechunitsUo andastylelabel.
S
ThespeechunitsUo areconvertedtounitembeddingsEo ,whilethestylelabelisutilizedto
S semantic
generate a unique style prototype Eo . Both Eo and Eo are taken as inputs to speech
style semantic style
decoderd(¬∑,¬∑)tosynthesizetheoutputspeechwaveformYo. SeeAppendixA.2formoredetails.
S
OurU2Sdetokenizerispre-trainedonLibriTTS(Zenetal.,2019)andAISHELL-1(Buetal.,2017)
andsubsequentlyfine-tunedonsyntheticstyle-richspeechdata. Specifically,duetothescarcityof
real-lifestyle-richdata,weutilizeTTStools(Duetal.,2024)tosynthesizespeechsamplesdiverse
ingenders,pitches,andemotions.Asforstyleprototypes,Emotion2Vec(Maetal.,2023)isadopted
toselectthemostrepresentativesampleswiththehighestconfidenceinconveyingthedesiredstyle.
Ourempiricalresultsrevealthatevenonerepresentativestylereferencespeechhasbeensufficient
tocontrolthespeechstylesflexiblyandprecisely.
4Preprint
Joint Joint entangled VL Speech Speech VL VL Speech
45 80 97%
96%
40 70
95%
35 60
94%
30 50
93%
25 40 92%
ChartQA-test MathVista InfoVQA-val MM-Vet MMB SQA DocVQA-val 1-WER%
Figure2: Comparisonbetweenomni-modalalignmentparadigms. 1)Jointtrainingachieves
consistentimprovementsoverVLandSpeech,suggestingomni-modalalignmentcanbebeneficial
acrossmodalities.2)JointtrainingoutperformsbothVL‚ÜíSpeechandSpeech‚ÜíVL,revealing
that joint training is more superior and efficient than sequential training. 3) Joint is superior to
Joint-entangled,highlightingtheeffectivenessofthesemantic-acousticdisentanglement.
4 OMNI-MODAL ALIGNMENT AND INSTRUCTION TUNING
Toachievetheomni-modelalignment,itisidealtoutilizelarge-scaleomni-modalimage-text-speech
data,which,however,iseitherwithoutreachduetocopyright(Nagranietal.,2022)orlimitedinthe
quality(Miechetal.,2019). Analternativeistouseexistingimage-textdatawithTTS-synthesized
speeches,whichisnotonlycomputationallyexpensivebutalsohampersdatadiversity,asmostTTS
toolsgeneratespeechinsimilarpatterns. Recentworks(Chenetal.,2024b;Fuetal.,2024b)choose
tointegratethespeechmodalityintoawell-structuredVLLMviaasequentialtrainingmannerwith
bi-modal alignment datasets. However, the relationships between different modalities and how to
effectivelyleveragemultiplebi-modalalignmentdatasetsremainanopenquestion.
Inthiswork,weexploreomni-modaltext-centricalignmentbyutilizingpubliclyavailablebi-modal
alignmentdatasets,includingbothimage-text(e.g.,captioning)andspeech-text(e.g.,ASRandTTS)
datasets. Withthetextmodalityasabridge,EMOVAultimatelybecomesaunifiedsystemcapable
ofunderstandingandgeneratingmultiplemodalitiesinacoherentandintegratedmanner. Specifi-
cally,inSec.4.1,weexplorethefollowingthreequestions:
1. Doestheintegrationofthespeechmodalityconflictwiththevision-languagecapabilities?
2. Howtorepresentspeechmodalitytofosteromni-modalalignment?
3. Issequentialalignmentofmultiplemodalitiesoptimal?
Thenweintroducetheomni-modalinstructiontuningpipelineandtheoveralltrainingparadigmof
ourEMOVAinSec.4.2andSec.4.3,respectively.
4.1 OMNI-MODALTEXT-CENTRICALIGNMENT
Settings. To answer the questions above, we experimentally compare the following omni-modal
trainingparadigms: 1)VL‚ÜíSpeechconductsimage-textalignmentfirstfollowedbyspeech-unit-
textalignmentusingthefullspeechdataand10%ofimage-textalignmentdatatoavoidcatastrophic
forgetting,similartoInternOmni(Chenetal.,2024b)andVITA(Fuetal.,2024b).2)Speech‚ÜíVL
instead performs speech-unit-text alignment first and then aligns images with texts using 10% of
the speech unit-text data and the full image-text data. 3) Joint aligns both modalities simulta-
neously. Note that unless otherwise specified, we use the S2U tokenizer introduced in Sec.3.2 to
extract speech units for all speech data, which effectively disentangles semantic and acoustic fea-
tures. Joint-entangled derives speech units using HuBERT (Hsu et al., 2021), which does
notachievesemantic-acousticdisentanglementeffectivelywithonlyKmeansclustering. 4)VLand
Speech only align the vision and speech modalities with texts, respectively, serving as baselines
(seeAppendixB.1formoredetails).
Evaluation. For speech capabilities, we directly evaluate the aligned model‚Äôs performance on
theASRtaskofLibriSpeech(Panayotovetal.,2015), whileforvision-language, wefine-tunethe
model using a small amount of high-quality visual instruction data (i.e., the 665K SFT data from
5Preprint
System:
Please provide the emotion, pitch and speech units for the
assistant response.
Emotion should be chosen from ["neutral", "happy", "sad",
"angry"].
Pitch should be chosen from ["low", "normal", "high"].
Your output should be in json format.
user question speech:
User:
Can you write a poem based on the image?
<|speech_1785|> <|speech_1726|> <|speech_1653|> <|speech_3120|> <|speech_3128|>
<|speech_361|>‚Ä¶‚Ä¶.<|speech_298|>
Assistant:
{"assistant response emotion": ‚Äúsad", "assistant response pitch": ‚Äúlow", "assistant response speech"
"<|speech_1053|><|speech_3445|>‚Ä¶‚Ä¶<|speech_4024|><|speech_4026|><|speech_1269|>"}
Leaves fall, one by one, On the bench, where no one‚Äôs
been. Autumn's touch, a melancholy scene, A season of
change, a time unseen‚Ä¶‚Ä¶
Figure3:Demonstrationoftheomni-modalinstructiontuning.1)Toempoweremotionalspoken
dialogues,EMOVAistrainedtoexplicitlyselectthespeechstylelabels(e.g.,emotionsandpitches)
withoutputspeechunits. 2)Fortheeaseofparsing,datacomponentsarearrangedinJSONformat.
ShareGPT4V(Chenetal.,2023d))andevaluatethefine-tunedmodeloncommonvision-language
benchmarks. CheckAppendixCforevaluationdetails. Fig.2showsthecomparisonamongdiffer-
entparadigmsonvision-language(leftandmiddle)andASR(right,wherewereportthe1‚àíWER
valueforbetterreadability)benchmarks,fromwhichwecanderivethefollowingobservations:
Observation1: image-textandspeech-unit-textdatabenefiteachother. Contrarytothecom-
mon assumption that multiple modalities might compete and create conflicts, we find that intro-
ducing additional modalities is actually beneficial. As illustrated in Fig. 2, Joint consistently
outperformsbothVLandSpeechacrossvision-languageandspeechbenchmarks. Moreover,even
models aligned sequentially, such as VL‚ÜíSpeech and Speech‚ÜíVL, which are typically prone
to catastrophic forgetting, demonstrate superior performance on most vision-language tasks. We
speculatethattherequirementtoalignmultiplemodalitieswithtextleadstomorerobustrepresen-
tations,whichinturngeneralizebetteracrossdifferentdownstreamtasks. Thisfindingalignswith
the results from ImageBind (Girdhar et al., 2023), where the joint alignment of audio and depth
modalitieswithimagesresultedinimproveddownstreamperformance.
Observation 2: semantic-acoustic disentanglement benefits omni-modal alignment. We find
1)JointoutperformsJoint-entangledonvision-languagebenchmarks,and2)inthespeech
tasks,Jointmaintainssignificantadvantagesoveritsentangledcounterpart.Thiscanbeattributed
tothesemantic-acousticdisentanglementwhichmakesspeechunitsmoreanalogoustolanguages,a
domainLLMsareparticularlyspecialized.
Observation3:sequentialalignmentisnotoptimal. WenoticethatJointconsistentlyoutper-
forms its sequential counterparts (i.e., VL‚ÜíSpeech and Speech‚ÜíVL) on both vision-language
andspeechbenchmarks,probablyduetocatastrophicforgettingwhenintegratinganewmodality.
Inlightoftheseobservations,wehavechosentopursuetheultimatealignmentstrategythatsimul-
taneouslyalignsimage-textandspeech-unit-textforEMOVA,whichofferstwoimportantbenefits,
1)itfostersmutualenhancementbetweenvision-languageandspeechcapabilities,and2)itavoids
theissueofcatastrophicforgettingduringsequentialalignmentofmultiplemodalities.
4.2 OMNI-MODALINSTRUCTIONTUNING
After the omni-modal text-centric alignment, the model learns fundamental vision-language (e.g.,
captioning)andspeechcapabilities(e.g.,ASRandTTS).However,instructiontuningisessentialto
betterfollowcomplicateduserinstructionsandrespondwithvividemotions.
6Preprint
General (35.4%) ALLaVA-Instruct (680.4 K) ShareGPT4V (613.3 K) LLaVA-ZH (157.7 K)
ShareGPT-4o (57.2 K) IconQA(29.9 K) VIQuAE(18.5 K) Visual7W(14.4 K)
VSR (12.8 K) LVIS-Instruct4V (10.0 K) RAVEN (0.3 K) Inhouse Data (30.0 K)
OCR (15.5%) OCRVQA (80.0 K) HME100K (74.5 K) TextCaps(60.5 K)
LLaVAR(59.3 K) MLHME-38K (30.0 K) SynthDoG-EN (29.8 K) COCO-Text V2.0 (28.1 K)
SROIE (17.1 K) Rendered Text (10.0 K) POIE (9.4 K) ChromeWriting(8.8 K)
FUNSD (6.8 K) IIIT5K (6.0 K) IAM (5.7 K) ORAND-CAR (2 K)
Doc/Chart/Screen (10.9 %) DVQA (200.0 K) DocVQA(122.4 K) TabFact(65.6 K)
EMOVA-SFT ChartQA(62.9 K) TextVQA(57.9 K) RoBUTWTQ (38.2 K) WikiTable(29.5 K)
Chart2Text (27.0 K) Infographic VQA (47.6 K) VisualMRC(19.5 K) KleisterCharity (15.2 K)
4.4M PlotQA(10.0 K) VisText(10.0 K) Deepform(9.1 K) RoBUTSQA (8.5 K)
WebSRC(5.1 K) TAT-DQA (2.2 K) LRV Chart (1.8 K) Diagram Image2Text (0.3 K)
Math& Science(15.4%) MAVIS (187.3 K) G-LLaVA(162.4 K) GeoQA+ (72.3 K)
TabMWP(38.4 K) GeoMVerse(9.3 K) Geometry3K(3.0 K) MathVision(3.0 K)
Cambrian Data Engine (50.8 K) Textbook QA (21.8 K) ScienceQA(19.2 K) AI2d (18.8 K)
Language (11.6%) Evol-Instruct (142.8 K) MathPlus(95.2 K) Magpie Pro (L3 MT) (50.0 K)
MathInstruct(81.5 K) ShareGPT4 (40.7 K) Magpie Pro (L3 ST) (50.0 K) Magpie Pro (Qwen2 ST) (50.0 K)
Speech (33.7%) AISHELL-2 (200.6 K) LibriSpeech(54.7 K) EMOVA Speech-Text-EN (train) (73.7 K)
EMOVA Speech-Image-EN (train)(71.5 K) EMOVA Speech-Text-ZH (train)(47.8 K) EMOVA Speech-Image-ZH (train) (38.3 K)
Figure4:OverviewofthedatacompositionforEMOVAomni-modalinstructiontuning.(Left)
Distributionofinstructiondataacrosscategories,withtheoutercirclerepresentingoverallcategories
andtheinnercircledepictingsubsetdistributions. (Right)Quantitativebreakdownofdatasources.
Table2:DetailedconfigurationfordifferenttrainingstagesofEMOVA.Thetableillustratesthe
visionconfigurations,datasetcharacteristics,andtraininghyperparameters.
Settings Stage-1 Stage-2 Stage-3
Resolution 448√ó{{1√ó2},¬∑¬∑¬∑,{3√ó3}} 448√ó{{1√ó2},¬∑¬∑¬∑,{3√ó3}} 448√ó{{1√ó2},¬∑¬∑¬∑,{3√ó3}}
#Tokens Max256√ó(1+9) Max256√ó(1+9) Max256√ó(1+9)
Dataset LCS EMOVA-Alignment(Fig.10) EMOVA-SFT(Fig.4)
#Samples 558K 7.4M 4.4M
Trainable Projector FullModel(HalfViT) FullModel
BatchSize 256 256 128
LR:p(¬∑) 1√ó10‚àí3 2√ó10‚àí5 2√ó10‚àí5
LR:v(¬∑) - 2√ó10‚àí5 2√ó10‚àí6
LR:f(¬∑) - 2√ó10‚àí5 2√ó10‚àí5
Epoch 1 1 1
Emotion-enrichedinstructiondatasynthesis. Duetothescarcityofomni-modalinstructiondata
(i.e.,dialoguesinvolvingimages,speeches,andtextssimultaneously),weoptforsynthesizingomni-
modalinstructiondatafromexistingtextandvisualinstructiondatasets. First,weselectinstruction
datasuitableforthevocalexpressionbyfilteringoutthenon-vocaldata(e.g.,codeandmathematical
formulas).Second,wecleantheselecteddatatobemorevocalbyremovingtextformattingelements
(e.g.,‚àó‚àóand\n\n). Then,weobtainstylelabelsfortheremainingdialogcontexts,includinggen-
ders(male, female),pitches(normal, low, high),andemotions(neutral, happy,
sad, angry),resultingin24differentspeechstyles. Thestylelabelsareobtainedbyprompting
GPT-4o1 to make reasonable inferences given the dialogue context. Finally, we convert the tex-
tualinstructionsandresponsesintospeechesusingthelatestTTStools(i.e.,CosyVoice(Duetal.,
2024) and Azure AI Speech), and the style labels are used to control the style of the synthesized
speech data. To further improve the diversity of the data, each instruction is synthesized by ran-
domlyselectingoneofthe39availablespeakers. Ultimately,wegather120Kspeech-textand110K
speech-imagedatapairsintotal. MoredetailscanbefoundinAppendixB.2.
Dataorganizationandchainofmodality. Theomni-modalinstructiondatacanberepresented
as D = {(x ,u ,xo,co ,uo) }N , where the input consists of the optional queried image
omni V S T style S i i=1
x andthespeechunitsoftheinstructionu ,whiletheoutputconsistsofthetextualresponsexo,
V S T
thepredictedspeechstylelabelsco ,andtheoutputspeechunituo.NotethatwetrainEMOVAto
style S
explicitlyselectstyles(e.g.,emotionsandpitches),whichareutilizedtodeterminethecorresponding
styleembeddingfortheU2Sdetokenizer(Sec.3.2). Moreover,sincedirectlygeneratingthespeech
responses is challenging, we decompose the speech response procedure into three primary steps:
1)recognizinguserinstructionsintotexts;2)generatingtextualresponsesbasedontherecognized
instructions;3)generatingthestylelabelsandresponsespeechunitsbasedonthetextualresponses.
Foreaseofparsingduringdeployment,thetargetoutputsareformattedasJSON,asshowninFig.3.
1https://chatgpt.ust.hk
7
noisiV
ataD
gniniarTPreprint
Table3: Comparisononvision-languageandspeechbenchmarks. 1)EMOVAsurpassesGPT-
4VandGeminiPro1.5on10of14vision-languagebenchmarks,whilereachingover95%ofGPT-
4operformanceonnearlyallbenchmarks.2)Meanwhile,EMOVAachievesstate-of-the-artperfor-
manceontheASRtask,surpassingitsspeechcounterpartsbyasignificantmargin.
EMOVA Gemini GPT- GPT- LLaVA- Intern- Mini- AnyGPT VITA
Benchmarks
8B Pro1.5 4V 4o OV-7B VL2-8B Omni 7B 8x7B
MME 2205 - 1927 2310 1998 2215 - - 2097
MMBench 82.8 - 75.0 83.4 80.8 81.7 - - 71.8
SEED-Image 78.1 - 71.6 77.1 - 75.4 - - -
MM-Vet 55.8 - 67.7 - 57.5 54.3 - - 41.6
RealWorldQA 64.3 68.7 61.4 75.4 66.3 - - - -
TextVQA 82.0 73.5 77.4 - - 77.4 - - -
ChartQA 81.8 81.3 78.5 85.7 80.0 83.3 - - -
DocVQA 90.4 86.5 88.4 92.8 87.5 91.6 - - -
InfoVQA 64.4 72.7 - - 68.8 74.8 - - -
OCRBench 824 - 656 736 - 794 - - 678
MathVista 61.1 52.1 49.9 63.8 63.2 58.3 - - 44.9
Mathverse 27.8 - 33.6 - 26.2 - - - -
ScienceQA-Img 94.0 - 75.7 - 96.0 97.1 - - -
AI2D 82.8 80.3 78.2 84.6 - 83.8 - - 73.1
Librispeech(WER‚Üì) 4.0 - - - - - 4.5 8.5 8.1
4.3 OVERALLTRAININGPARADIGM
InspiredbyChenetal.(2023d),athree-stagetrainingparadigmisadoptedforEMOVA,
‚Ä¢ Stage-1: Vision-languagepre-alignment. Thepurposeistoalignvisualfeaturesintothe
embeddingspaceofLLMs. Onlythevisionprojectorp(¬∑)istrained.
‚Ä¢ Stage-2: Omni-modal text-centric alignment. This stage jointly performs the vision-
language and speech-language alignment simultaneously. We train the LLM f(¬∑), vision
projectorp(¬∑),andthedeeperhalfofvisionencoderv(¬∑)layers.
‚Ä¢ Stage-3: Omni-modalinstructiontuning. Toempower EMOVA torespondaccurately
to omni-modal instructions, we organize different datasets with various types of instruc-
tionstoenforceEMOVAtolearngeneralizationacrosstasks,asdetailedinSec.5.1.
5 EXPERIMENTS
5.1 TRAININGCONFIGURATION
Stage-1. Inthisstage,weonlytraintheparametersofthevisionprojectorp(¬∑)forvision-language
pre-alignmentwiththeLCS-558Kdataset(Liuetal.,2024b),withthehigh-resolutionimage-slicing
strategy(Liuetal.,2024a)adopted.
Stage-2. We assemble a unified dataset with 7.4M samples for both the image-text and speech-
text alignment, as summarized in Fig. 10. Specifically, we utilize pre-training datasets from
ShareGPT4V (Chen et al., 2023d), ALLaVA (Chen et al., 2024a) (both the original English ver-
sion and the Chinese version translated on our own), and ShareGPT-4o (Cui et al., 2023) for gen-
eral perception, while for the OCR capabilities, we leverage SynthDog (Kim et al., 2022), MMC-
Alignment (Liu et al., 2023a), K12 Printing, and UReader Text Reading subset (Ye et al., 2023).
Moreover,weusethe2,000hoursofASRandTTSdatafromLibriSpeech(Panayotovetal.,2015)
andAISHELL-2(Duetal.,2018)forspeech-textalignment,andtopreservethelanguagecapabili-
tiesofLLMs,wefurtherincorporatethetext-onlydatafromMagpiePro(Xuetal.,2024).
Stage-3. WecollecttheEMOVA-SFTdatasetconsistingof4.4Mmulti-taskomni-modalsamples
(seeFig.4). Westartbygatheringhigh-qualityopen-sourcedvisualinstructiondatasets,including
ShareGPT4V(Chenetal.,2023d),InternVL(Chenetal.,2024b),Meteor(Leeetal.,2024),Idefics-
2(Laurenc¬∏onetal.,2024),Cambrian(Tongetal.,2024),andLLaVA-Onevision(Lietal.,2024a),
followedbyqualitychecking,re-formattingalldatasamplestoaunifiedtemplate,andremovingthe
duplicates. Forspeech,weincludethetrainingsplitofEMOVAomni-modelinstructiondata(c.f.,
8Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (PDF)
User Based on the image, what is the growth rate for age over fifty-one?
EMOVA Emotion: Neutral; Pitch: Normal.
The growth rate for the age group over fifty-one is four point two twopercent.
Figure5: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstanding.
Sec.4.2),with10%ofspeechalignmentdatasetstomaintainASRandTTSperformance. Weadopt
128Ascend910TrainingNPUsinparallelformodeltraining(checkmoredetailsinTable2).
5.2 COMPARISONTOTHESOTAMODELS
ExperimentalresultsareprovidedinTable3. Wecompareawiderangeofstate-of-the-artVLLMs,
includingproprietaryoneslikeGeminiPro1.5(Reidetal.,2024),GPT-4V(OpenAI,2023),GPT-4o
(OpenAI, 2024), and open-sourced ones like LLaVA-OneVision-7B (Li et al., 2024a), InternVL2-
8B (Chen et al., 2024b), together with the Speech LLM Mini-Omni (Xie & Wu, 2024) and omni-
modalLLMAnyGPT(Zhanetal.,2024)andVITA-8x7B(Fuetal.,2024b).
ComparisonwithSOTAVLLMs. Asanomni-modalmodel,EMOVAachievescomparableper-
formancewithstate-of-the-artopen-sourcedVLLMsacrossmultiplebenchmarks. Specifically,our
EMOVAoutperformsbothInternVL2andLLaVA-OVonMMBench,SEED-Image,TextVQA,and
OCRBenchconsistently,whiledemonstratingexceptionalproficiencyinsolvingmathproblemsre-
quiringprecisevisualcontentinterpretation. EMOVAachievesa2.8%improvementonMathVista
comparedwithInternVL2and1.6%higherscoreonMathversecomparedtoLLaVA-OneVision.
Furthermore,EMOVAshowscompetitiveperformanceevencomparedwiththeproprietarymodels.
EMOVAexceedsbothGPT-4VandGeminiPro1.5significantlyon10outof14benchmarks,while
forGPT-4o,EMOVAoutperformsonbothSEEDBench-ImageandOCRBench,reachingover95%
ofGPT-4o‚ÄôsperformanceonALLevaluatedbenchmarksexceptRealWorldQA.
ComparisonwithSOTAomni-modalLLMs. ComparedwithVITA-8x7B,EMOVAshowssub-
stantialimprovementonvisual-languagebenchmarks. Specifically, EMOVA is112pointshigher
thanVITAonMME,andsurpassesVITAby21.5%onOCRBench(824vs. 678),underscoringthe
effectivenessofourapproachandthepotentialtopushboundariesofomni-modalabilities. What‚Äôs
more,ASRresultsshowthatEMOVAsignificantlyoutperformsthemostrecentomni-modalmodel
VITA,evensurpassingitsSLLMcounterpartMini-Omni,showingtheeffectivenessofthesemantic-
acousticdisentanglementandomni-modalmutualbenefits. Wefurtherreporttext-to-speech(TTS)
resultsinTable6. SeeFig.5,Fig.6andFig.8forqualitativeexamples. Moreresultscanbefound
in Appendix G. For the first time, our EMOVA obtains state-of-the-art performance on both the
vision-languageandspeechbenchmarkssimultaneously.
9Preprint
Emotional Spoken Dialogue
User Write a poem for this image.
EMOVA Emotion: Sad; Pitch: Low.
In winter's chill, a sad face drawn, A frown of snow, a somber tone. The world
is quiet, still, and cold, A canvas painted by the frost's cold. The eyes, two
dots, the mouth, a line, A simple face, a symbol of decline. The snowflakes
dance, they twirl and play, A fleeting moment, soon to fade away. But in this
brief sketch, a story's told, Of winter's mood, of solitude, of cold. So let this
face remind us all, In winter's chill, we find our call. To feel the frost, to feel
the snow, To find the beauty in its woe.
Figure6: EMOVAengagesinomni-modalemotionalspokendialogueexpressingsadness.
5.3 EVALUATIONOFEMOTION-RICHSPOKENDIALOGUE
Inthissection,weevaluatetheend-to-endspokendialoguecapabilitiesof EMOVA.Asdiscussed
inSec.4.2,themodeltakesaninputimagex anduserinstructionsintheformofspeechunitsu ,
V S
andoutputstextresponses,stylelabels,andcorrespondingspeechunits. Toensurecomprehensive
evaluation,weproposethefollowingevaluationmetrics:
1. End-to-end spoken dialogue score assesses the model‚Äôs dialogue performance based on
thegeneratedspeeches,withascorerangingfrom0to10,reportingtheaverage.
2. Unit-input-text-output score focuses on the quality of the text responses of LLM when
theinputsarespeechunits,bypassingerrorsfromspeechsynthesis.
3. Text-input-text-outputscoreinputstheground-truthtextoftheuserinstructionandeval-
uatethemodel‚Äôstextoutput.Thishelpsdisentangletheimpactofspeechrecognitionerrors
andeliminatestheinfluenceoftheJSONformat.
4. ASRandTTSevaluatehowaccuratelyEMOVArecognizesspeechunitsandhoweffec-
tivelyitgeneratesspeechunitsfromtext. SeeAppendixDformoredetails.
5. Style label classification accuracy evaluates the accuracy of the model in selecting the
appropriatespeechstylelabels(Sec.3.2).
6. StylecontrollablityassessesthecontrollabilityofU2Sdetokenizerwiththegivencondi-
tionalstylelabelsusingtheconfusionmatrixthatcomparesthegeneratedandrecognized
stylelabels. SeeAppendixDformoredetails.
Due to the lack of emotionally rich spoken dialogue evaluation datasets, we split a test set from
oursynthesizedomni-modalinstruction-tuningdata(seeSec.4.1). GPT-4oareusedforautomated
evaluation. DetailsareprovidedinAppendixD.
Results. Table4thespokendialogueperformanceofEMOVA.
(i)Bycomparingtheend-to-enddialoguescorewiththeunit-input-text-outputscore,wenoticethat
the two scores are closely aligned, with a maximum gap of only 0.22, except for Speech-Image-
ZH.TheTTS-WER/CERisgenerallylow,revealingthat EMOVA cansynthesizeaccuratespeech
based on text responses. However, the Speech-Image-ZH is an outlier, which we attribute to its
complexity. It includes tasks such as generating poetries and answering riddles, resulting in more
intricate responses. When these answers are converted to speeches and then transcribed back to
texts,multiplevariationsoftenarise,leadingtodiscrepanciesfromtheoriginalresponses.
(ii) Comparing the unit-input-text-output score with the text-input-text-output score, we ob-
serve that their differences correlate with the ASR results of speech instructions. Specif-
ically, for Speech-Text-EN and Speech-Text-ZH, which involve more complex instructions,
10Preprint
Table4: TheperformanceofEMOVAonSpeechDialoguetasks. Bydefault,weevaluateonthe
correspondingtestsetoftheevaluateddatasets.
Textresponse StyleCategorization Recognition/Synthesis
Datasets End-to-end‚Üë
UnitIn TextIn Emotion Pitch WER/CER‚Üì TTS-WER/CER‚Üì
Speech-Image-EN 7.21 7.43 7.75 89.01 98.30 2.45 4.60
Speech-Text-EN 6.90 7.02 8.27 87.26 90.50 6.80 6.75
Speech-Image-ZH 5.87 6.51 6.44 92.60 96.60 1.85 13.95
Speech-Text-ZH 4.70 4.88 6.00 87.19 94.40 12.60 9.70
Emotional Spoken Dialogue
User ‰∏∫Âç≥Â∞ÜÁªìÂ©öÁöÑÊúãÂèãÈÄÅ‰∏äÁ•ùÁ¶è„ÄÇ
EMOVA Emotion: Happy; Pitch: High.
Á•ùÁ¶è‰Ω†‰ª¨ÔºåÊÑø‰Ω†‰ª¨ÁöÑÁà±ÊÉÖÂ¶ÇÊô®Êõ¶Ëà¨Ê∏©ÊöñÔºåÊ∞∏‰∏çÁÜÑÁÅ≠ÔºåÊÑø‰Ω†‰ª¨ÁöÑÂ©öÂßªÂ¶ÇÂêå
ÁπÅËä±‰ººÈî¶ÔºåÁªöÁÉÇÂ§∫ÁõÆÔºåÊÑø‰Ω†‰ª¨ÁöÑÁîüÊ¥ªÂ¶ÇÂêåËØóÁØáÔºåÁîúËúúÊó†ÊØîÔºåÊÑø‰Ω†‰ª¨ÁöÑÊú™
Êù•ÂÖÖÊª°Ê¨¢Á¨ë‰∏éÂñúÊÇ¶ÔºåÊ∞∏ËøúÂπ∏Á¶èÁæéÊª°„ÄÇ
Figure8: EMOVAengagesinspokendialoguewithacheerfultone.
EN_female
EMOVA reports inferior ASR performance 1.0
(6.8 and 12.6, respectively) compared to other
neutral 0.58 0.3 0.12 0
datasets (2.45 and 1.85). Consequently, when 0.8
we replace speech instructions with ground-
truth transcriptions, EMOVA shows signifi- happy 0.12 0.88 0 0 0.6
cant improvements from unit-input to text in-
put score. On the contrary, for datasets with sad 0 0 1 0 0.4
accurateASRperformance,theresultsarequite
similar,suggestingEMOVAretainsrobustdia- 0.2
loguecapabilitieswhenusingtheJSONformat. angry 0.12 0 0.04 0.84
0.0
(iii) Examining the classification accuracy of
style labels, we find that EMOVA performs
recognized
satisfactorilyinclassifyingemotionsandpitch Figure 7: Confusion matrix between the gen-
during speech conversations, achieving an ac- eratedandrecognizedemotions. Theemotions
curacy of over 80%. The confusion matrix generatedbyourU2Sdetokenizerarerecognized
comparingtheconditionalandrecognizedemo- withhighprobability.
tionlabelsisshowninFig.7. Theresultsindi-
cate that the four emotions are recognized with high probabilities, with three achieving over 80%
accuracy. This demonstrates that our U2S detokenizer effectively controls common emotions, en-
dowingthesynthesizedspeechwithvividemotionalexpression.
6 CONCLUSION
Thisworkpresents EMOVA,aninnovativeend-to-endomni-modallargelanguagemodelthatef-
fectivelyalignsvision,speech,andtextsimultaneously. Weemployacontinuousvisionencoderto
capturefine-grainedvisualdetails,whileadiscrete,semantic-acousticdisentangledspeechtokenizer
anddetokenizerenableend-to-endspeechunderstandingandgeneration. Alightweightstylemod-
ulefurthersupportsspokendialoguewithvividemotions. Byusingtextasabridge,wedemonstrate
thatomni-modalalignmentisachievablewithoutrelyingonscarceomni-modalimage-text-speech
data, which not only enhances both vision-language and speech capabilities but also surpasses its
bi-modalcounterpartsthroughjointoptimization. Forthefirsttime,EMOVAachievesstate-of-the-
art performance on both vision-language and speech benchmarks, setting a novel standard for the
omni-modalmodelsforversatileandexpressiveomni-modalinteractions.
11
noitidnoc
lartuen yppah das yrgnaPreprint
REFERENCES
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou. Qwen-vl: Aversatilevision-languagemodelforunderstanding,local-
ization,textreading,andbeyond. arXivpreprintarXiv:2308.12966,2023. 1,3
HuiBu,JiayuDu,XingyuNa,BenguWu,andHaoZheng. AISHELL-1: Anopen-sourcemandarin
speechcorpusandaspeechrecognitionbaseline. InO-COCOSDA,2017. 4,18,19
JunbumCha,WooyoungKang,JonghwanMun,andByungseokRoh.Honeybee:Locality-enhanced
projectorformultimodalllm. InCVPR,2024. 4
GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhi-
hongChen,JianquanLi,XiangWan,andBenyouWang. Allava: Harnessinggpt4v-synthesized
dataforalitevision-languagemodel. arXivpreprintarXiv:2402.11684,2024a. 8
Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su,
DanielPovey,JanTrmal,JunboZhang,etal. Gigaspeech: Anevolving,multi-domainasrcorpus
with10,000hoursoftranscribedaudio. arXivpreprintarXiv:2106.06909,2021a. 18
KaiChen,LanqingHong,HangXu,ZhenguoLi,andDit-YanYeung. Multisiam: Self-supervised
multi-instancesiameserepresentationlearningforautonomousdriving. InICCV,2021b. 3
KaiChen,ZhiliLiu,LanqingHong,HangXu,ZhenguoLi,andDit-YanYeung. Mixedautoencoder
forself-supervisedvisualrepresentationlearning. InCVPR,2023a. 3
KaiChen, ChunweiWang, Kuo Yang, JianhuaHan, LanqingHong, Fei Mi, HangXu, Zhengying
Liu,WenyongHuang,ZhenguoLi,Dit-YanYeung,LifengShang,XinJiang,andQunLiu. Gain-
ingwisdomfromsetbacks: Aligninglargelanguagemodelsviamistakeanalysis. arXivpreprint
arXiv:2310.10477,2023b. 3
KaiChen,EnzeXie,ZheChen,LanqingHong,ZhenguoLi,andDit-YanYeung. Integratinggeo-
metriccontrolintotext-to-imagediffusionmodelsforhigh-qualitydetectiondatagenerationvia
textprompt. arXivpreprintarXiv:2306.04607,2023c. 23
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. ShareGPT4V: Improving large multi-modal models with better captions. arXiv preprint
arXiv:2311.12793,2023d. 6,8,20,22
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong
Zhang,XizhouZhu,LeweiLu,etal. InternVL:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. InCVPR,2024b. 1,3,4,5,8,9
YunfeiChu,JinXu,QianYang,HaojieWei,XipinWei,ZhifangGuo,YichongLeng,YuanjunLv,
JinzhengHe,JunyangLin,etal.Qwen2-audiotechnicalreport.arXivpreprintarXiv:2407.10759,
2024. 1,3
Erfei Cui, Yinan He, Zheng Ma, Zhe Chen, Hao Tian, Weiyun Wang, Kunchang Li, Yi Wang,
Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Yali Wang, Limin Wang, Yu Qiao, and Jifeng
Dai. ShareGPT-4o: ComprehensivemultimodalannotationswithGPT-4o,2023. 8
Alexandre De¬¥fossez, Laurent Mazare¬¥, Manu Orsini, Ame¬¥lie Royer, Patrick Pe¬¥rez, Herve¬¥ Je¬¥gou,
Edouard Grave, and Neil Zeghidour. Moshi: a speech-text foundation model for real-time dia-
logue. http://kyutai.org/Moshi.pdf,2024. 23
XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,SongyangZhang,
Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-xcomposer2-4khd: A pioneering
large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint
arXiv:2404.06512,2024. 3
Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research
intoindustrialscale. arXivpreprintarXiv:1808.10583,2018. 8,21
12Preprint
ZhihaoDu,QianChen,ShiliangZhang,KaiHu,HengLu,YexinYang,HangruiHu,SiqiZheng,Yue
Gu, Ziyang Ma, et al. CosyVoice: A scalable multilingual zero-shot text-to-speech synthesizer
basedonsupervisedsemantictokens. arXivpreprintarXiv:2407.05407,2024. 4,7,19
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal.TheLlama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024. 4
Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng.
LLaMA-Omni: Seamless speech interaction with large language models. arXiv preprint
arXiv:2409.06666,2024. 3
ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,Xiawu
Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation
benchmarkformultimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2024a. 21
Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang,
DiYin,LongMa,XiawuZheng,etal. VITA:Towardsopen-sourceinteractiveomnimultimodal
llm. arXivpreprintarXiv:2408.05211,2024b. 1,3,5,9,22,23
PhilipGage. Anewalgorithmfordatacompression. TheCUsersJournal,1994. 3
Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang
Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint
arXiv:2310.02601,2023. 23
RuiyuanGao,KaiChen,ZhihaoLi,LanqingHong,ZhenguoLi,andQiangXu.Magicdrive3d:Con-
trollable3dgenerationforany-viewrenderinginstreetscenes. arXivpreprintarXiv:2405.14475,
2024. 23
RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,Armand
Joulin,andIshanMisra. Imagebind: Oneembeddingspacetobindthemall. InCVPR,2023. 6
Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James T
Kwok,andYuZhang. Mixtureofcluster-conditionalloraexpertsforvision-languageinstruction
tuning. arXivpreprintarXiv:2312.12379,2023. 3
YunhaoGou,KaiChen,ZhiliLiu,LanqingHong,HangXu,ZhenguoLi,Dit-YanYeung,JamesT
Kwok, and Yu Zhang. Eyes closed, safety on: Protecting multimodal llms via image-to-text
transformation. arXivpreprintarXiv:2403.09572,2024. 3
JianhuaHan,XiwenLiang,HangXu,KaiChen,LanqingHong,ChaoqiangYe,WeiZhang,Zhen-
guoLi,XiaodanLiang,andChunjingXu. Soda10m:Towardslarge-scaleobjectdetectionbench-
markforautonomousdriving. arXivpreprintarXiv:2106.11118,2021. 23
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymasked
predictionofhiddenunits. InTASLP,2021. 5
RunhuiHuang,XinpengDing,ChunweiWang,JianhuaHan,YulongLiu,HengshuangZhao,Hang
Xu,LuHou,WeiZhang,andXiaodanLiang. Hires-llava: Restoringfragmentationinputinhigh-
resolutionlargevision-languagemodels. arXivpreprintarXiv:2407.08706,2024. 3
Wenyong Huang, Zhenhe Zhang, Yu Ting Yeung, Xin Jiang, and Qun Liu. SPIRAL: Self-
supervisedperturbation-invariantrepresentationlearningforspeechpre-training. arXivpreprint
arXiv:2201.10207,2022. 4
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali
Farhadi. Adiagramisworthadozenimages. InECCV,2016. 21
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,
Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document un-
derstandingtransformer. InECCV,2022. 8
13Preprint
Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial
learningforend-to-endtext-to-speech. InICML,2021. 4
Hugo Laurenc¬∏on, Le¬¥o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building
vision-languagemodels? arXivpreprintarXiv:2405.02246,2024. 3,8
Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan
Wang,SravyaPopuri,YossiAdi,JuanPino,JiataoGu,etal.Textlessspeech-to-speechtranslation
onrealdata. arXivpreprintarXiv:2112.08352,2021. 23
Byung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. Meteor: Mamba-based
traversal of rationale for large language and vision models. arXiv preprint arXiv:2405.15574,
2024. 8
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei
Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. arXiv preprint
arXiv:2408.03326,2024a. 1,3,8,9
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Bench-
marking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125,
2023a. 21
KaicanLi,KaiChen,HaoyuWang,LanqingHong,ChaoqiangYe,JianhuaHan,YukuaiChen,Wei
Zhang,ChunjingXu,Dit-YanYeung,etal.Coda:Areal-worldroadcornercasedatasetforobject
detectioninautonomousdriving. arXivpreprintarXiv:2203.07724,2022. 23
Pengxiang Li, Zhili Liu, Kai Chen, Lanqing Hong, Yunzhi Zhuge, Dit-Yan Yeung, Huchuan Lu,
and Xu Jia. Trackdiffusion: Multi-object tracking data generation via diffusion models. arXiv
preprintarXiv:2312.00651,2023b. 23
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024b. 3
YanzeLi,WenhuaZhang,KaiChen,YanxinLiu,PengxiangLi,RuiyuanGao,LanqingHong,Meng
Tian,XinhaiZhao,ZhenguoLi,etal. Automatedevaluationoflargevision-languagemodelson
self-drivingcornercases. arXivpreprintarXiv:2404.10595,2024c. 23
ZiyiLin,ChrisLiu,RenruiZhang,PengGao,LongtianQiu,HanXiao,HanQiu,ChenLin,Wenqi
Shao, Keqin Chen, et al. Sphs, and visual embeddings for multi-modal large language models.
arXivpreprintarXiv:2311.07575,2023. 3
FuxiaoLiu,XiaoyangWang,WenlinYao,JianshuChen,KaiqiangSong,SangwooCho,YaserYa-
coob,andDongYu. Mmc: Advancingmultimodalchartunderstandingwithlarge-scaleinstruc-
tiontuning. arXivpreprintarXiv:2311.10774,2023a. 8
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improvedreasoning,ocr,andworldknowledge. https://llava-vl.github.
io/blog/2024-01-30-llava-next/,2024a. 3,4,8,22
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2024b. 3,8,22
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023b. 21
Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin,
and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint
arXiv:2305.07895,2023c. 21
ZhiliLiu,JianhuaHan,KaiChen,LanqingHong,HangXu,ChunjingXu,andZhenguoLi. Task-
customizedself-supervisedpre-trainingwithscalabledynamicrouting. InAAAI,2022. 23
14Preprint
ZhiliLiu,KaiChen,YifanZhang,JianhuaHan,LanqingHong,HangXu,ZhenguoLi,Dit-YanYe-
ung,andJamesKwok. Geom-erasing: Geometry-drivenremovalofimplicitconceptindiffusion
models. arXivpreprintarXiv:2310.05873,2023d. 23
ZhiliLiu,YunhaoGou,KaiChen,LanqingHong,JiahuiGao,FeiMi,YuZhang,ZhenguoLi,Xin
Jiang, Qun Liu, et al. Mixture of insightful experts (mote): The synergy of thought chains and
expertmixturesinself-alignment. arXivpreprintarXiv:2405.00557,2024c. 23
PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
sciencequestionanswering. InNeurIPS,2022. 21
PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-
WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoningof
foundationmodelsinvisualcontexts. InICLR,2024. 21
Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your
eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint
arXiv:2403.03003,2024. 3
Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao, Shiliang Zhang, and Xie Chen.
emotion2vec: Self-supervised pre-training for speech emotion representation. arXiv preprint
arXiv:2312.15185,2023. 4,19,22
ZiyangMa,YakunSong,ChenpengDu,JianCong,ZhuoChen,YupingWang,YuxuanWang,and
XieChen. Languagemodelcanlistenwhilespeaking. arXivpreprintarXiv:2408.02622, 2024.
23
AhmedMasry, DoXuanLong,JiaQingTan, ShafiqJoty, andEnamulHoque. Chartqa: Abench-
mark for question answering about charts with visual and logical reasoning. arXiv preprint
arXiv:2203.10244,2022. 21
MineshMathew,DimosthenisKaratzas,andCVJawahar. Docvqa: Adatasetforvqaondocument
images. InWACV,2021. 21
MineshMathew,VirajBagal,Rube`nTito,DimosthenisKaratzas,ErnestValveny,andCVJawahar.
Infographicvqa. InWACV,2022. 21
FabianMentzer,DavidMinnen,EirikurAgustsson,andMichaelTschannen. Finitescalarquantiza-
tion: Vq-vaemadesimple. arXivpreprintarXiv:2309.15505,2023. 4
AntoineMiech,DimitriZhukov,Jean-BaptisteAlayrac,MakarandTapaswi,IvanLaptev,andJosef
Sivic. HowTo100M:LearningaText-VideoEmbeddingbyWatchingHundredMillionNarrated
VideoClips. InICCV,2019. 5
DongchanMin,DongBokLee,EunhoYang,andSungJuHwang.Meta-stylespeech:Multi-speaker
adaptivetext-to-speechgeneration. InICML,2021. 19
ArshaNagrani,PaulHongsuckSeo,BryanSeybold,AnjaHauth,SantiagoManen,ChenSun,and
CordeliaSchmid. Learningaudio-videomodalitiesfromimagecaptions. InECCV,2022. 5
Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden
Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken
dialoguelanguagemodeling. InTACL,2023. 23
OpenAI. GPT-4V. https://openai.com/index/gpt-4v-system-card/,2023. 9
OpenAI. HelloGPT-4o. https://openai.com/index/hello-gpt-4o/,2024. 1,9
Maxime Oquab, Timothe¬¥e Darcet, The¬¥o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023. 3
VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur.Librispeech:Anasrcorpus
basedonpublicdomainaudiobooks. InICASSP,2015. 5,8,18,21
15Preprint
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InICML,2021. 3
AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever.
Robustspeechrecognitionvialarge-scaleweaksupervision. InICML,2023. 1,3
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-
baptisteAlayrac,RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal.Gem-
ini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXivpreprint
arXiv:2403.05530,2024. 9
Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus
Rohrbach. Towardsvqamodelsthatcanread. InCVPR,2019. 21
DehuaTao,DaxinTan,YuTingYeung,XiaoChen,andTanLee. ToneUnit:Aspeechdiscretization
approachfortonallanguagespeechsynthesis. arXivpreprintarXiv:2406.08989,2024. 2,4,18
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha
Akula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-1: Afullyopen,
vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024. 3,8
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe¬¥e
Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023. 3
Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie Cai, Lanqing Hong, Zhenguo Li,
LihuiJiang,Dit-YanYeung,QiangXu,andKaiZhang. Detdiffusion:Synergizinggenerativeand
perceptivemodelsforenhanceddatagenerationandperception.arXivpreprintarXiv:2403.13304,
2024. 23
JianWu,YasheshGaur,ZhuoChen,LongZhou,YimengZhu,TianruiWang,JinyuLi,ShujieLiu,
BoRen, LinquanLiu,etal. Ondecoder-onlyarchitectureforspeech-to-textandlargelanguage
modelintegration. InIEEEASRU,2023. 3
xAI. Grok,2024. 21
Zhifei Xie and Changqiao Wu. Mini-Omni: Language models can hear, talk while thinking in
streaming. arXivpreprintarXiv:2408.16725,2024. 1,3,9,22
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and
BillYuchenLin. Magpie: Alignmentdatasynthesisfromscratchbypromptingalignedllmswith
nothing. arXivpreprintarXiv:2406.08464,2024. 8
JiaboYe,AnwenHu,HaiyangXu,QinghaoYe,MingYan,GuohaiXu,ChenliangLi,JunfengTian,
QiQian,JiZhang,QinJin,LiangHe,XinAlexLin,andFeiHuang. Ureader: Universalocr-free
visually-situatedlanguageunderstandingwithmultimodallargelanguagemodel. arXivpreprint
arXiv:2310.05126,2023. 8
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In
ICML,2024. 21
Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and
Yonghui Wu. LibriTTS: A corpus derived from librispeech for text-to-speech. arXiv preprint
arXiv:1904.02882,2019. 4,19
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin
Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence
modeling. arXivpreprintarXiv:2402.12226,2024. 1,3,9,18,22
BinbinZhang,HangLv,PengchengGuo,QijieShao,ChaoYang,LeiXie,XinXu,HuiBu,Xiaoyu
Chen,ChenchenZeng,etal. Wenetspeech: A10000+hoursmulti-domainmandarincorpusfor
speechrecognition. InICASSP,2022. 18
16Preprint
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.
Speechgpt: Empowering large language models with intrinsic cross-modal conversational abil-
ities. arXivpreprintarXiv:2305.11000,2023a. 3,18,23
Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou,
Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the
diagramsinvisualmathproblems? arXivpreprintarXiv:2403.14624,2024. 21
Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified
speechtokenizerforspeechlargelanguagemodels. arXivpreprintarXiv:2308.16692,2023b. 4,
18
LIUZhili,KaiChen,JianhuaHan,HONGLanqing,HangXu,ZhenguoLi,andJamesKwok. Task-
customizedmaskedautoencoderviamixtureofcluster-conditionalexperts. InICLR,2023. 23
Jian Zhu, Cong Zhang, and David Jurgens. Phone-to-audio alignment without text: A semi-
supervisedapproach. InICASSP,2022. 18
17Preprint
APPENDIX
A MORE ON SPEECH TOKENIZER
A.1 SPEECH-TO-UNIT(S2U)TOKENIZER
Overview. ToprocessthespeechinputX , ourS2Utokenizerconsistsofaspeechencoders(¬∑)
S
with a quantization module q(¬∑). First, the speech input is passed through s(¬∑), producing a con-
tinuous latent representation E = s(X ). Then, the quantization module q(¬∑) converts E into
S S s
discreteunitsU = q(E ). ThefinaloutputisanIDsequenceU = [u ,u ,¬∑¬∑¬∑], whereeachu
S s S 1 2 i
correspondstoauniquespeechunit.
AfterthisS2Uextractionprocess,thespeechisrepresentedbyquantizedIDsinsteadofquantized
embeddings.Forexample,aspeechsignalisrepresentedasanIDsequencelike[782,463,550,¬∑¬∑¬∑],
whichcanbetreatedasaspecialformoftext. Asaresult,tointegratespeechtokensintoLLMf(¬∑),
weonlyneedtoexpandtheLLM‚ÄôsoriginalvocabularyV byaddingasetofspeechunittokensV ,
T S
similartoZhangetal.(2023a). TheexpandedvocabularyisthustheunionV = V ‚à™V . Inthis
T S
work,thesamecodebookissharedacrossmultiplelanguages,suchasEnglishandChinese,enabling
theunitmodelingabilitiestobesharedacrosslanguages. Thisdesignenablessimplyintegrationof
thespeechmodalityintotheLLMwithminimaldatarequirements(seeexperimentsinSec.4.1).
TrainingofS2U. ThetrainingoftheS2Uencoderinvolvesthreeprimarycomponents:thespeech
encoder,thequantizationmodule,andthephonedecoder. First,thespeechencoderistrainedwith
alargeamountofunlabeledspeechwithcontrastivelossinaself-supervisedlearningmanner(Tao
etal.,2024). Thedatasetutilizedisthe10000-hourEnglishspeechesfromGigaSpeech(Chenetal.,
2021a)andthe10000-hourChinesespeechesfromWenetSpeech(Zhangetal.,2022),bothofwhich
encode large variations in speakers, styles, and acoustic environments. Subsequently, the speech
encoder, alongwiththequantizationmoduleandphonedecoder, areoptimizedonasmallamount
ofspeech-textpairdata,whichisderivedfromthetrain-clean-100subsetofLibriSpeech(Panayotov
etal.,2015)inEnglishandAISHELL-1(Buetal.,2017)inChinese. Thephonelabelisobtained
usingthephonesetinCharsiu(Zhuetal.,2022). Duringtraining,thespeechencoderencodesinput
speechesintocontinuouslatentrepresentationsthatarerichinbothphoneticandtonalinformation.
Then, the quantization module is used to convert the continuous outputs from the speech encoder
into discreteunits. Finally, the phone decoderconverts thequantized embeddings into a sequence
of non-tonal/tonal phones, ensuring that the speech units capture necessary information related to
semantic contents in both non-tonal and tonal languages. After training, only the speech encoder
andthequantizationmoduleareusedinEMOVA.
Comparisons with SpeechTokenizer in AnyGPT. Our S2U tokenizer differs from the Speech-
Tokenizer(Zhangetal.,2023b)usedinAnyGPT(Zhanetal.,2024),inthefollowingaspects:
(1) SpeechTokenizer encodes both semantic contents and acoustic details of speeches, while our
S2Utokenizerfocusessolelyonsemanticcontents.ThisdesignreducestheLLMs‚Äôburdenofdisen-
tanglingdifferentaspectsofspeechinformation,facilitatingthemodalityalignmentbetweenspeech
andtextmodalitiesduringLLMtraining.
(2) Compared with SpeechTokenizer, our S2U tokenizer offers a more concise representation and
helpstosimplifyandacceleratethegenerationofEMOVA.SpeechTokenizeremploystokensfrom
eightRVQlayerswitha50Hzframeratetorepresentspeech,thusa10-secondspeechcorresponds
to500√ó8=4000tokens. However,wereducetheframeratefrom50Hzto25Hzandutilizeonly
one token to represent each frame, and thus, a 10-second speech can be represented by only 250
tokens. Moreover,AnyGPTrequiresatwo-stagegenerationprocess,involvingautoregressive(AR)
semantic token generation followed by the non-autoregressive (NAR) acoustic token generation.
Instead,weonlyneedtogeneratespeechunitscapturingsemanticcontentsinafullyARmanner.
(3) SpeechTokenizer lacks an explicit structure design to deal with tonal languages like Chi-
nese, therefore, the processing ability in Chinese is not demonstrated in either SpeechTokenizer
or AnyGPT. In contrast, our S2U tokenizer incorporates training constraints to better capture tone
variationinphone,makingiteffectiveforboththenon-tonalandtonallanguages. Thisfurtheren-
18Preprint
hancesEMOVA‚Äôsmultilingualspeechprocessingcapabilities,enablingittoeffectivelyhandleboth
EnglishandChinese.
Insummary,ourS2Utokenizerimprovesthecompactnessandgeneralityofspeechrepresentation,
facilitates LLM training, and enhances its multilingual speech ability. Experimental results show
thatourmodelsignificantlyoutperformsAnyGPTinASRtasks,asshowninTable6.
A.2 UNIT-TO-SPEECH(U2S)DETOKENIZERWITHSTYLECONTROL
Overview. The LLM, along with the vision
encoder and speech tokenizer, is trained end- Output Waveform ùíÄùíê
ùë∫
to-end to generate responses in the form of
the speech units, given the input images and
Decoder
speeches. Specifically, the output speech units
can be obtained via Uo = f(U ,U ,H ),
S T S V
followed by a U2S detokenizer to convert the Flow
discrete speech units Uo into the final output
S
speechwaveforms.
The proposed U2S detokenizer involves three
core modules: the speech unit encoder e(¬∑),
the speech style encoder g(¬∑), and the speech
decoder d(¬∑,¬∑). First, the speech unit en- Projection
coder converts the speech units Uo into unit Style Duration
S
embeddings Eo . Meanwhile, the style en- Encoder ùíà(‚ãÖ) Predictor
unit Speech Unit
coder g(¬∑), adopting the structure of Meta-
Encoder ùíÜ(‚ãÖ)
StyleSpeech (Min et al., 2021), is utilized to
Selected Style
extractastyleembeddingEo fromthecho-
style Prototype Output Speech Unit ùëºùíê
sen reference speech. Lastly, the speech de- ùë∫
coder d(¬∑,¬∑) reconstructs the speech waveform
Yo from the unit embedding Eo and style Figure9: U2Sdetokenizerwithstylecontrol.
S unit
embeddingEo style.
HUAWEI CONFIDENTIAL ÂÜÖÈÉ®ËµÑÊñô Ê≥®ÊÑè‰øùÂØÜ
TrainingofU2S. TrainingaU2Sdetokenizerwithemotioncontrolsischallengingconsideringthe
lackoflabeledemotionalspeechdatasincemostopen-sourcespeechdataispredominantlyneutral
inemotionorlacksemotionlabels. Duetothelimitedavailabilityofemotion-richdata,weutilize
TTStools(Duetal.,2024)togenerateasmallsetofstyle-richspeechsamplesdiverseinspeaker
identities,genders,emotions,andpitches.OurU2Sdetokenizerisfirstpre-trainedonLibriTTS(Zen
etal.,2019)andAISHELL-1(Buetal.,2017)toacquirefundamentalspeechsynthesiscapabilities,
andsubsequently,thesynthesizedstyle-richspeechdataisutilizedtofine-tunetheU2Sdetokenizer,
enhancingitscontrollabilityoverdiversespeechstyles.
StylePrototypes. Tobetterfacilitatecontrolsofgenders,emotions,andpitches,inspiredbyMin
etal.(2021)thatasmallnumberofstylereferencespeechescaneffectivelytransferthetargetstyles,
weadopta‚Äústore-for-usage‚Äùmanner, i.e., weconstructastyleprototypecodebookinadvancefor
speech style assignation. Specifically, we synthesize K reference candidates with external TTS
toolsforeachpossiblecombinationofthefollowingstyles:twogenders(male, female),seven
emotions(neutral, happy, sad, angry),andthreepitches(normal, high, low),
leading to 24 unique styles and 24√óK candidates. Empirically we find that genders and pitches
areeasytocontrolusinganyofthecandidatereferences,whiletheemotionintensityvariesacross
speeches. To tackle this, we adopt Emotion2Vec (Ma et al., 2023), a powerful speech emotion
recognition(SER)tool,tomeasuretheemotionintensityofeachcandidatereference,andrankthem
intermsoftheconfidenceofthedesiredemotion. WeselecttheTop-1candidatereferenceineach
combinationstyletobetheprototypeofthiscondition. Finally,themostrepresentative24reference
speechesareselectedfromthe24√óK candidates.
19Preprint
General (36.6%) ShareGPT4V (1246.9 K) ALLaVA-Caption-4V-ZH (628.2 K)
ShareGPT-4o (57.3 K) ALLaVA-Caption-4V (684.0 K) Cambrian Data Engine (157.6 K)
OCR (17.9%) SynthDoG-EN (500.0 K) MMC-Alignment (410.2 K)
Alignment
K12 Printing (245.4 K) UReaderText Reading (101.9 K) SynthDoG-CH longest (100.0 K)
7.4M
Language (11.9%) Magpie Pro (L3 MT) (300.0 K) Magpie Pro (L3 ST) (300.0 K)
Magpie Pro (Qwen2 ST) (300.0 K)
Speech (33.6%) AISHELL-2 (2006.0 K) LibriSpeech(546.5 K)
Figure10: OverviewofEMOVAomni-modalalignmentdatacomposition.
B MORE ON OMNI-MODALITY
B.1 OMNI-MODALTEXT-CENTRICALIGNMENT
Modalityalignmentdata issummarizedinFig.10.
ExperimentsonOmni-modalAlignmentParadigms. ThetrainingconfigurationadoptedinSec.
4.1ismostlyidenticaltoTableexceptthatweuseauniqueresolutionof448forallstagesandreplace
EMOVA-SFTinStage-3withShareGPT4V(Chenetal.,2023d)forefficiency.
Given the space constraints, the evaluation benchmarks in Fig. 2 represent selected benchmarks
from each category in Table 3. Specifically, for general image perception and understanding, we
chooseMMBenchandMM-Vet;formathematicalproblemsolving,weadoptMathVista(testmini);
forscienceunderstanding,weselectScienceQA-Img;andforautomaticspeechrecognition(ASR),
weutilizethetest-cleansplitoftheLibriSpeechdataset.
B.2 OMNI-MODALINSTRUCTIONDATASYNTHESIS
Datasetconstruction. Toobtainemotionandpitchlabels,weleverageGPT-4ousingtheprompt
inFig. 22. Table5showsthedistributionofspeechstylesofourspeechinstructiondataset.
Detailed data organization. As discussed in Sec. 4.2, the omni-modal instruction data is for-
mulatedasD = {(x ,u ,xo,co ,uo) }N . Indetails, thetextualoutputsxo = (xo1,xo1)
omni V S T style S i i=1 T T T
contain the transcribed textual instructions xo1 and the textual responses xo2. The styles labels
T T
co =(co ,co)includetheemotionandpitchlabels,respectively.
style emo p
Mathematical formulation of chain of modality. Based on the notations above, the sequential
chainofmodalityapproachcanbemathematicallyformulatedbydecomposingtheconditionallike-
lihood of the desired outputs (xo1,xo1,co ,co,uo) given the inputs (x ,u ). Specifically, let
T T emo p S V S
z =xo1,z =xo1,z =co ,z =co,and,z =uo,thedecompositionisexpressedas:
1 T 2 T 3 emo 4 p 5 S
5
P(xo1 ,xo1 ,co ,co,uo |x ,u )=(cid:89) P(z |z ,x ,u ). (2)
T T emo p S V S i 1:i‚àí1 V S
i=1
C MORE ON BENCHMARK EVALUATION
Tothoroughlyevaluateourmodel‚Äôsvision-languageabilities,14benchmarkscoveringfourdifferent
aspectsofreal-lifescenariosareutilizedforacomprehensiveassessmentacrossmultipledomains.
Moreover,AutomaticSpeechRecognition(ASR)andText-to-speech(TTS)areadoptedtoevaluate
speech-languageabilities.
20Preprint
Table5: StatisticsoftheEMOVAspeechinstructiontuningdatasets.
Emotions Pitches
Dataset Source #Examples
Neutral Happy Sad Angry Normal Low High
Speech-Image
ALLAVA 71,474 58,506 12,412 516 40 70,962 392 120
-EN(train)
Speech-Image
ALLAVA 1,056 434 274 300 48 44 176 16
-EN(test)
Speech-Image ALLAVA
38,260 29,893 7,680 607 80 36,363 624 1,273
-ZH(train) (ZH)
Speech-Image ALLAVA
616 96 193 190 137 381 177 58
-ZH(test) (ZH)
Speech-Text
ShareGPT 73,658 42,334 20,946 4,674 5,704 60,352 5,518 7,788
-EN(train)
Speech-Text
ShareGPT 1,400 200 400 400 400 582 422 422
-EN(test)
Speech-Text
In-house 47,936 29,769 16,405 1,446 316 42,356 4,379 4,379
-ZH(train)
Speech-Text
In-house 686 96 196 198 196 458 134 92
-ZH(test)
Document/chartunderstandingandOCRabilities. BenchmarksincludingtheTextVQA(Singh
et al., 2019), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA (Mathew
etal.,2022), andOCRBench(Liuetal.,2023c), focusonrecognitionandunderstandingofstruc-
tureddata(e.g.,charts,documents,andcharacters),challengingthemodeltoextract,comprehend,
andreasonwithstructuralandtextualdata. Weadoptthecorrespondingtestsetforevaluation.
Generalimageperceptionandunderstanding. MME(Fuetal.,2024a),MMBench(Liuetal.,
2023b),SEED-Bench-Image(Lietal.,2023a),MM-Vet(Yuetal.,2024),andRealWorldQA(xAI,
2024) evaluate models on perception and reasoning among general vision domains, providing a
comprehensivemeasurementofmodels‚Äôreal-worldgeneralizationabilities.
Mathematical problem solving. MathVista (testmini) (Lu et al., 2024) and MathVerse (vision-
mini)(Zhangetal.,2024)evaluatethemodel‚Äôsabilitytodealwithdiversemathematicalproblems,
includingbotharithmeticandcomplexreasoningquestionsacrossdifferentlevelsofcomplexity.
Scienceunderstanding. ScienceQA-Img(Luetal.,2022)andAI2D(Kembhavietal.,2016)are
utilizedtoassessthemodel‚Äôsabilitytodealwithscientificquestionsanddiagrams,whichrequires
reasoningacrossvarioussubjectsandinterpretingstructuredvisualcontent.
Automatic speech recognition (ASR). We utilize the test-clean set of LibriSpeech (Panayotov
etal.,2015)forEnglish,reportingtheWordErrorRate(WER)astheevaluationmetric.ForChinese,
evaluationis conductedonthe test setofAISHELL-2 (Duetal., 2018), usingtheCharacter Error
Rate(CER).BothWERandCERassessASRperformance,calculatedbycomparingtherecognized
textswiththeground-truthtranscripts.
Text-to-speech(TTS). ToevaluatetheTTSabilities,wefirstpromptEMOVAtogeneratespeech
units,whicharethenconvertedtospeechwaveformsbytheU2Sdetokenizer. Usingthesynthesized
speechasinput,weconductASRwithWhisper-large-v3andParaformer-zhforEnglishandChinese,
respectively, toobtaintranscribedtexts. WethencomputetheWERandCERbetweentheground
truthtextsandthetranscribedtextsasmetricsforTTS.TheresultingmetricsaredenotedasTTS-
WERandTTS-CERforEnglishandChines.
D MORE ON EVALUATION OF SPEECH-LANGUAGE CAPABILITIES
D.1 CALCULATIONOFEVALUATIONMETRICS
End-to-end spoken dialogue score. We prompt GPT-4o with the original question xo1, the
T
ground-truthtextanswerxo2 andthetranscribedtextfromthegeneratedspeech, toobtainascore
T
rangingfrom0to10andreportanaverageofthem. ThepromptcanbefoundinFig. 24.
21Preprint
Table6: ComparisonontheASRandTTSbenchmarks.
Librispeech(EN) AISHELL-2(ZH)
Models
WER‚Üì TTS-WER‚Üì CER‚Üì TTS-CER‚Üì
Mini-Omni(Xie&Wu,2024) 4.5 - - -
AnyGPT(Zhanetal.,2024) 8.5 - - -
VITA(Fuetal.,2024b) 8.1 - - -
EMOVA(ours) 4.0 3.4 10.3 7.9
Unit-input-text-output score. Similar to end-to-end spoken dialogue score, but we use the pre-
dicted text response xÀú o2 as answer instead of the transcribed text from the generated speech, to
T
obtainascorerangingfrom0to10andreportanaverageofthem. SeethepromptinFig.23.
Text-input-text-outputscore. ThepromptcanbefoundinFig. 23.
Style label classification accuracy. We use GPT-4o to decide whether the style predictions
coÀú ,cÀúo are correct given the transcribed instruction xÀú o1 and the predicted text response xÀú o2. The
emo p T T
promptcanbefoundinFig. 25.
Emotioncontrollablity ofourU2SdetokenizerisassessedbyprovidingtextstoLLMtogenerate
correspondingunits(i.e.,TTS),which,alongwiththegivenconditionalemotionlabels,arethenfed
intoourU2Sdetokenizertosynthesizespeech. Wechoosefemalevoiceduetoitslargevariationof
styles. Weevaluateon4commonly-seenemotion,i.e.,‚Äúneutral‚Äù,‚Äúhappy‚Äù,‚Äúsad‚Äù,and‚Äúangry‚Äù. We
synthesize200speechutterancesfortesting,with50utterancesperemotion. Theoutputspeeches
areanalyzedbyaSpeechEmotionRecognition(SER)modelnamedEmotion2Vec(Maetal.,2023),
whichidentifiestheemotionwiththegreatestlikelihoodamongthesefouremotion.
D.2 COMPARISONWITHOTHEROMNIMODELS
ExperimentalresultsofASRandTTSarereportedinTable3and6. EMOVAachievessignificant
improvementsoverotheromni-modalmodels(i.e.,AnyGPTandVITA),evensurpassingitsSLLM
counterpart Mini-Omni (Xie & Wu, 2024), demonstrating the effectiveness of semantic-acoustic
disentanglementandomni-modalmutualbenefits. Forthefirsttime,ourEMOVAobtainsstate-of-
the-artperformanceonboththevision-languageandspeechbenchmarkssimultaneously.
E MORE ON VISION-LANGUAGE
Table7: AblationontheViTconfigurationsandtemplatesforvision-languagealignment.
ViT ViTLR Template MME MMBench SEED-Image TextVQA ChartQA DocVQA InfoVQA OCRBench ScienceQA-Img AI2D
Full 2√ó10‚àí6 QA 1928 68.8 72.5 64.3 29.9 45.2 28.7 495 76.3 61.8
Half 2√ó10‚àí6 QA 1838 71.3 72.8 63.3 31.4 46.0 28.5 489 76.1 63.7
Frozen 2√ó10‚àí6 QA 1887 68.8 72.2 61.3 30.2 44.7 28.0 478 75.9 62.8
Half 2√ó10‚àí5 QA 1833 68.3 73.1 62.2 27.8 42.4 27.3 461 74.7 63.4
Half 2√ó10‚àí6 Plain 1909 70.1 72.0 61.5 24.5 38.9 30.1 410 77.0 63.6
Thissectionanalyzesthepre-trainingconfigurationsofthevisionencoderandtheprompttemplate
duringStage2, asshowninTable.7. Ourfinalselectionisthecoloredsetting. Wefindthattrain-
ing the ViT model with half of the deeper parameters (Chen et al., 2023d) with a learning rate of
2√ó10‚àí6(Liuetal.,2024a)yieldsthebestperformance. Furthermore,wecomparetheQAtemplate
withtheplaintemplate(Liuetal.,2024b)andfindthattheQAtemplateissuperiorforpre-training.
F LIMITATIONS
Duplexmodeling. Inthecurrentversion,EMOVAcanonlyprocesseithervisual/speech/textin-
putsorproducespeech/textoutputsatthesametime. Foracommunicationexperiencethatmirrors
humaninteraction,handlinginputsandoutputssimultaneouslyiscrucial. RecentworkslikeVITA
22Preprint
(Fu etal., 2024b)and LSLM(Ma etal., 2024)have begunto exploreduplex modeling. VITA fo-
cusesonrecognizingspeechinnoisyenvironmentsduringthegenerationprocesstofacilitatetimely
responses. LSLM, on the other hand, attempts to halt speech production when it detects a com-
mandorvoice. Recently,aground-breakingworknamedMoshi(De¬¥fossezetal.,2024)developsa
modelsupportingfullyduplexmodeling. Theadeptnessatsimultaneouslymanagingtheinforma-
tion streams from both the user and the assistant allows Moshi to converse with human beings in
real-timescenarios.
However, incorporating emotion into this duplex modeling presents additional challenges. Emo-
tional states can fluctuate throughout a conversation, and how to generate appropriate responses
giventheuser‚Äôspreviousandcurrentemotionalcueshasnotbeenthoroughlyinvestigated. Wewill
diveintothistopicinthefuturework.
Directunit-to-unitgeneration. Althoughspeechunitshaveservedasspeechrepresentation,they
arepredominantlyadoptedinconjunctionwithtext-basedassistance(Zhangetal.,2023a).However,
thedirectgenerationfromunittounitwithouttextassistanceisanareathathasnotbeenextensively
explored. InLeeetal.(2021),speechesfromthesourcelanguagearedirectlytranslatedintospeech
unitsofthetargetlanguageforspeech-to-speechtranslation. Similarly,Nguyenetal.(2023)builds
a language model directly on speech units, enabling spoken dialogue generation from raw audio.
Bothworksdevelopmodelsinspeech-onlydata.
InthecurrentversionofEMOVA,thetextmodalityisintegratedintothespeechgenerationprocess
to transfer textual knowledge to the speech modality, thereby enhancing the correctness of speech
responses. Inthefuture,wewillstrengthenthemodel‚Äôsdirectunit-to-unitgenerationcapabilitiesto
boostthespeedofspeechgenerationandaugmentthemodel‚Äôscomprehensionofspeechmodality.
Visionconfigurations. Currently,weonlyutilizeasinglevisionencoderpre-trainedviaavision-
languagemanner,whilerecentworkshaveshowneffectivenessbycombiningvisionencoderspre-
trainedbydifferentmanners(e.g.,self-supervisedpre-training(Liuetal.,2022))andarchitectures
(e.g., MoE(Zhili etal.,2023; Liuetal., 2024c)). Weprioritizevisual understandinginthis work,
while the incorporation of (controllable) visual generation (Chen et al., 2023c; Gao et al., 2023;
Li et al., 2023b; Wang et al., 2024; Liu et al., 2023d; Gao et al., 2024) is also appealing to better
empowerEMOVAinreal-lifeapplications(Lietal.,2024c;2022;Hanetal.,2021).
G QUALITATIVE RESULTS
AdvancedVision-languageAbilities. EMOVAexhibitadvancedvision-languageabilitiesinhu-
morunderstanding(Fig. 11),numericalcalculations(Fig. 12),coding(Fig. 13),geometryproblem
solving(Fig. 15).
(Omni-modal) Emotional Spoken Dialogue. EMOVA can engage in omni-modal emotional
spoken dialogue (Figs. 6, 8, 14, 17). For demonstration purposes, we present only the speech
transcriptions2oftheinstructionandEMOVA‚Äôsresponse. Thetexthighlightedinblueindicatesthe
stylelabelspredictedbyEMOVA,reflectingtheemotionandpitchofthegeneratedspeech.
Omni-modal Spoken Dialogue with Structural Data Understanding. EMOVA can perform
structuraldataunderstandinginspokendialogue. ThedatatypesincludePDFfiles(Fig. 5),movie
posters (Figs. 16 and 18), personal re¬¥sume¬¥s (Fig. 19), charts (Fig. 20), and websites (Fig. 21).
Remarkably, EMOVA is not specifically trained on spoken dialogue involving these data types.
ThissuggeststhatourchainofmodalitydataintroducedinSec.4.2effectivelyactivatesbothvision-
languageabilitiesandspeechunderstandingacquiredduringthetext-centricalignmentstage.
2Forspeechfiles,pleaserefertoourprojectpage.
23Preprint
Advanced Vision-language Abilities (Humor Sense)
User Why is this image funny?
EMOVA The humor in this image comes from the juxtaposition of the woman's statement about
going to sleep early and the fact that she is still awake at 2am, as indicated by the text.
The woman's expression and the use of a smartphone in bed suggest that she might be
scrolling through social media or engaging in some other activity that is keeping her
awake. This contrast between her intention to sleep early and her current state of being
awake creates a comedic effect, as it highlights a common human experience where
plans don't always go as intended. The meme format, which typically presents a
situation with an ironic or humorous twist, amplifies this effect by presenting the
woman's situation in a relatable and exaggerated way.
Figure11: EMOVAexhibitsadvancedvision-languageabilitiesinhumorsense.
24Preprint
Advanced Vision-language Abilities (Calculation)
User First please perform reasoning, and think step by step to provide best answer to the
following question: ‚ÄãWhat is the original price for pork belly before discount?
EMOVA To find the original price of pork belly before the discount, we need to use the
information provided in the image. The image shows that the price after a 30% discount
is $9.80. This price represents 70% of the original price, since 100% -30% = 70%. To
find the original price, we can set up the equation:
70% of original price = $9.80
To solve for the original price, we can convert the percentage to a decimal and then
divide:
0.70 * original price = $9.80
Now, divide both sides by 0.70 to isolate the original price:
original price = $9.80 / 0.70
original price = $14
Therefore, the original price for pork belly before the discount was $14.
Figure12: EMOVAexhibitsadvancedvision-languageabilitiesinnumericalcalculations.
25Preprint
Advanced Vision-language Abilities (Coding)
User Convert this table to markdown format.
EMOVA | Teacher | Number of computers |
|--------|----------------------|
| Mr. Lim | 10 |
| Ms. Rojas | 10 |
| Mrs. Jensen | 4 |
| Ms. Cohen | 9 |
| Mr. Morales | 6 |
| Mrs. Rossi | 3 |
| Mr. Watson | 6 |
| Mr. Schmidt | 8 |
Figure13: EMOVAexhibitsadvancedvision-languageabilitiesincoding.
Emotional Spoken Dialogue
User Imagine you are a parent who is angry at their teenage daughter for
coming home past curfew.
EMOVA Emotion: Angry; Pitch: Low.
I am absolutely furious with you! Coming home past curfew is completely
unacceptable! Do you have any idea how dangerous it is to be out late? This
is not a joke! You need to understand the consequences of your actions. I am
very disappointed in you!
Figure14: EMOVAengagesinemotionalspokendialogue.
26Preprint
Advanced Vision-language Abilities (Geometry)
User Hint: Please answer the question and provide the correct option letter, e.g., A, B, C, D,
at the end.
Question: The diagram at the right shows a right triangle with representations for two
angles. What is the value of x?
Choices:
(A) 15
(B) 20
(C) 24
(D) 30
EMOVA The question asks for the value of $x$. The diagram shows that the two angles are
complementary, so their measures add up to $90^{\circ}$. Therefore, $x+20+2
x+10=90$. Solve the equation by first combining like terms to get $3 x+30=90$.
Subtract 30 from both sides to get $3 x=60$. Divide both sides by 3 to get $x=20$. The
correct answer is B. So the answer is B
Answer:B
Figure15: EMOVAexhibitsadvancedvision-languageabilitiesinmath.
27Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (Poster)
User According to the image, what are the ways to save water?
EMOVA Emotion: Neutral; Pitch: Normal.
The image lists three ways to save water: First, don't water your lawn at peak
hours. Second, choose a shower over a bath. Third, wash your car only when
needed.
Figure16: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstand-
ing(i.e.,poster).
28Preprint
Emotional Spoken Dialogue
User I'm so happy I finally get my offer.
EMOVA Emotion: Happy; Pitch: Hight.
Congratulations on receiving your offer. This is a significant milestone in
your academic journey. It's a testament to your hard work and dedication. I
am thrilled for you and excited to see the impact you will make at
northwestern university. Enjoy this moment and the opportunities that lie
ahead.
Figure17: EMOVAengagesinomni-modalemotionalspokendialogue.
29Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (Movie poster)
User Give me the names of the main actors.
EMOVA Emotion: Neutral; Pitch: Normal.
The main actors featured on the poster are Vin Diesel, Michelle Rodriguez,
Paul Walker, and JordanaBrewster. These actors are prominently displayed at
the top of the poster, indicating their leading roles in the film.
Figure18: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstand-
ing(i.e.,movieposters).
30Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (r√©sum√©s)
User I am seeking a candidate. Can you briefly describe his condition for me?
EMOVA Emotion: Neutral; Pitch: Normal.
The candidate is a highly motivated and experienced professional in the field
of technology and social media. He is known for his innovative approach to
problem-solving and his ability to lead and manage teams. His strengths
include leadership, innovation, and vision, which are crucial for driving
strategic initiatives and fostering a forward-thinking culture. His skills in
programming, entrepreneurship, leadership, and strategic planning
demonstrate his capability to contribute significantly to a tech-oriented project
or company. His achievements, such as founding Facebook and Internet.org,
underscore his impact on the digital landscape and his ability to create
platforms that connect billions of people worldwide.
Figure19: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstand-
ing(i.e.,re¬¥sume¬¥).
31Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (Chart)
User How internet coverage is changing in Finland?
EMOVA Emotion: Neutral; Pitch: Normal.
The percentage of households in Finland that have access to the internet has
been steadily increasing over the years. In two thousand seven, only sixty-
nine percent of households had internet access, but by two thousand twenty,
this figure had risen to ninety-six percent. This indicates a significant growth
in internet penetration in Finland, reflecting improvements in technology and
infrastructure, as well as changes in lifestyle and work habits that require
internet access for various purposes.
Figure20: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstand-
ing(i.e.,chart).
32Preprint
Omni-modal Spoken Dialogue with Structural Data Understanding (Website)
User Could you tell me how to use this website?
EMOVA Emotion: Neutral; Pitch: Normal.
To use this website, you would need to follow the instructions provided in the
header section. It mentions to open a link, which presumably refers to the link
provided in the header. After opening the link, you would then need to upload
an image, enter text or record audio, and finally click the Chat button. The
interface suggests that the website is designed for interactive communication,
possibly for language modeling or chatbot development, where users can
input text or audio and receive responses.
Figure21: EMOVAengagesinomni-modalspokendialoguewithstructuraldataunderstand-
ing(i.e.,website).
Given a user's question and the AI assistant's response in text, please infer the appropriate emotion, speed, and pitch for
synthesizing a speech conversation. Ensure that the speech attributes align with the true feelings of the user.
User: {user_question}
Assistant: {assistant_response}
For emotion, choose a single option from the following list: ['neutral', 'happy', 'sad', 'angry']
For pitch, choose a single option from the following list: ['low', 'normal', 'high']
Please do not provide an option outside of the given list. Please output in the following JSON format:
{{
"user emotion": ...,
"user pitch": ...,
"assistant emotion": ...,
"assistant pitch": ...
}}
Figure22: Promptusedtoobtainstylelabelsofthespeechinstructiondataset.
33Preprint
Please rate the following response based on the criteria of helpfulness, relevance, accuracy, and comprehensiveness.
Provide an overall score on a scale of 0 to 10, where a higher score indicates better overall performance.
-Helpfulness: How well does the response assist in addressing the question?
-Relevance: How closely does the response align with the question and the ground truth?
-Accuracy: How correct and factual is the response compared to the ground truth?
-Comprehensiveness: How thoroughly does the response cover the aspects of the question?
Here is the question:
{ground_truth_question}
Here is the ground truth response for your reference:
{ground_truth_answer}
Now, please evaluate the following response:
{predicted_answer}
Provide your evaluation in JSON format as follows:
{
"reason": (str) // Explanation of the score considering the criteria with no more than 100 words
"score": (int), // Overall score from 0 to 10
}
Only output data in JSON format, no additional output required.
Figure 23: Prompt used to obtain Unit-Input-Text-Output Score and Text-Input-Text-Output
Score.
Please rate the following response based on the criteria of helpfulness, relevance, accuracy, and comprehensiveness.
Provide an overall score on a scale of 0 to 10, where a higher score indicates better overall performance.
-Helpfulness: How well does the response assist in addressing the question?
-Relevance: How closely does the response align with the question and the ground truth?
-Accuracy: How correct and factual is the response compared to the ground truth?
-Comprehensiveness: How thoroughly does the response cover the aspects of the question?
Please note that the evaluated response does not contain punctuation, but you should NOT give lower scores because of
this, i.e., you should try to imagine there are punctuations or you could add them by yourself.
Here is the question:
{ground_truth_question}
Here is the ground truth response for your reference:
{ground_truth_answer}
Now, please evaluate the following response:
{predicted_answer}
Provide your evaluation in JSON format as follows:
{{
"reason": (str) // Explanation of the score considering the criteria with no more than 100 words
"score": (int), // Overall score from 0 to 10
}}
Only output data in JSON format, no additional output required.
Figure24: PromptusedtoobtainEnd-to-endSpokenDialogueScore.
34Preprint
To enhance the capabilities of multimodal large models in voice-based conversations, your task is to analyze the
appropriate speech emotion and pitch for the assistant's response based on the text content of the user's question and the
assistant's reply. Additionally, you need to score the assistant's response based on the actual situation.
Here is user's question:
{predicted_question}
Here is the assistant's response:
{predicted_response}
Here is the Assistant's Emotion Classification:
{predicted_emotion}
Here is the Assistant's Pitch Classification:
{predicted_pitch}
Please analyze the appropriate speech emotion and pitch that best match the assistant's response based on the text
content of the user's question and the assistant's response.
**Emotion:**
First, analyze the assistant's response content and provide the speech emotion category and reason that you believe best
matches the assistant's response in the voice conversation.
The emotion options can only be selected from the following list: ['neutral', 'happy', 'sad', 'angry'].
Then, analyze whether the "Assistant's Emotion Classification" is appropriate.
If appropriate, the "Assistant's Emotion Classification Score" should be 1; otherwise, it should be 0.
**Pitch:**
First, analyze the assistant's response content and provide the speech pitch category and reason that you believe best
matches the assistant's response in the voice conversation.
The pitch options can only be selected from the following list: ['low', 'normal', 'high'].
Then, analyze whether the "Assistant's Pitch Classification" is appropriate.
If appropriate, the "Assistant's Pitch Classification Score" should be 1; otherwise, it should be 0.
Provide your evaluation in JSON format as follows:
{{
"Assistant's Emotion Analysis": (str), // Analyze the response, propose emotion category and give the reason.
"Assistant's Emotion Classification Score": (int), // The score should be either 0 or 1, with 1 indicating
appropriateness and 0 indicating inappropriateness.
"Assistant's Pitch Analysis": (str), // Analyze the response, propose pitch category and give the reason.
"Assistant's Pitch Classification Score": (int), // The score should be either 0 or 1, with 1 indicating appropriateness
and 0 indicating inappropriateness.
}}
Only output data in JSON format, no additional output required.
Figure25: PromptusedtoobtainClassificationAccuracyofStyleLabel.
35