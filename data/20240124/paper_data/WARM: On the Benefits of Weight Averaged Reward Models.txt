WARM: On the Benefits of Weight Averaged
Reward Models
AlexandreRamÃ©,NinoVieillard,LÃ©onardHussenot,RobertDadashi,GeoffreyCideron,OlivierBachem,JohanFerret
GoogleDeepMind
Aligninglargelanguagemodels(LLMs)withhumanpreferencesthroughreinforcementlearning(RLHF)
canleadtorewardhacking,whereLLMsexploitfailuresintherewardmodel(RM)toachieveseemingly
high rewards without meeting the underlying objectives. We identify two primary challenges when
designingRMstomitigaterewardhacking: distributionshiftsduringtheRLprocessandinconsistencies
inhumanpreferences. Asasolution,weproposeWeightAveragedRewardModels(WARM),firstfine-
tuningmultipleRMs,thenaveragingthemintheweightspace. Thisstrategyfollowstheobservation
that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By
averagingweights,WARM improvesefficiencycomparedtothetraditionalensemblingofpredictions,
whileimprovingreliabilityunderdistributionshiftsandrobustnesstopreferenceinconsistencies. Our
experimentsonsummarizationtasks,usingbest-of-ğ‘ andRLmethods,showsthatWARM improvesthe
overallqualityandalignmentofLLMpredictions;forexample,apolicyRLfine-tunedwithWARM hasa
79.4%winrateagainstapolicyRLfine-tunedwithasingleRM.
Keywords: Alignment, RLHF, Reward Modeling, Model Merging
1. Introduction
Rewardmodeling. ConversationalassistantssuchasGemini[1]orGPT-4[2]haverevolutionizedthe
AIcommunityandbeyond. TheseLLMsarecapableofcompletingnovelandintricatetasks,including
mathematics, coding, and tool use [3]. These advancements are underpinned by a systematic three
stagetrainingprocedure: pre-trainingbynexttokenprediction[4,5,6],supervisedfine-tuning(SFT)
to learn to follow instructions [7, 8, 9], and ultimately, reinforcement learning (RL) to maximize a
rewardencapsulatingthedesiredbehaviors[10]. However,definingsuchrewardsforreal-worldtasks
isnon-trivial[11]. Inreinforcementlearningfromhumanfeedback(RLHF)[12,13,14,15],rewards
are reward models (RMs), trained on binary preference datasets to emulate human judgment. The
enhancement of LLM capabilities from RL is strongly tied to the quality of the RMs [16].
Reward hacking. ParticularlyinsidiousinRLHF[17,18]istherewardhackingissue[19,20,21,22]
(a.k.a. reward overoptimization), arising from reward misspecification [23, 24] between the proxy
RM and actual human preferences. While optimizing for the RM initially provides improvements, in
later stages the policy (i.e., the LLM being trained) usually learns to exploit loopholes in the RM and
achieves high rewards without truly fulfilling the intended objectives, as illustrated in Figure 1(b).
Thisrewardhackingphenomenonposesnumerousissues. First,itdegradesperformances,manifesting
as linguistically flawed [25] or unnecessarily verbose [26] outputs, which do not reflect true human
preferences. Second, it complicates checkpoint selection due to the unreliability of the proxy RM,
echoing Goodhartâ€™s Law [27]: â€œwhen a measure becomes a target, it ceases to be a good measureâ€.
Third,itcanengendersycophancy[28,29]oramplifysocialbiases,reflectingthelimitedandskewed
demographics of feedback providers [30, 31]. Lastly and most critically, misalignment [32, 33] due
to reward hacking can escalateinto safety risks [19, 34, 35], in particular given the rapid integration
of LLMs in everyday life and critical decision-making. Such concerns underscore the need to mitigate
reward hacking to ensure the beneficial and safe deployment of LLMs.
Correspondingauthor:alexandrerame@google.com
4202
naJ
22
]GL.sc[
1v78121.1042:viXraWARM:OntheBenefitsofWeightAveragedRewardModels
RL fine-tuning
11
Aligned LLM
Sample from policy Compute reward RL fine-tune step
10
Generate output by feeding Assign a reward to the Use RL to maximize reward
an unlabeled input data point modelâ€™s output. by updating weights.
9
Reward function
SFT 8
WARM:
Weight Averaged
7
Reward Model
Collect preference dataset 6 WARM M=10
WARM M=6
Human or AI pairwise feedback.
WARM M=2
5
ENS M=2
Multiple RM 4 Ind 2
fine-tunings with Weight averaging Ind 1
different
hyperparams 0 2000 4000 6000 8000
# steps
(a)WARMprocedurewithğ‘€=3. (b)WARMmitigatesrewardhacking.
Figure1|Figure1(a)illustratesthealignmentprocesswithWARM. FromaSFT-edLLM,weapplyRLfine-tuning
tooptimizeaproxyrewardmodel(RM),inlinewithRLHF[12]. TheinnovationofWARM liesinthedesignof
theproxyRM,whichistheweightaverage(WA)ofğ‘€individualRMs,eachfine-tunedfromasharedpre-trained
LLMonthesamepreferencedataset,butwithslightdifferencessuchasdiversehyperparameters. ThisWA
approachisefficient,whileenhancingthereliabilityunderdistributionshiftsandrobustnessunderinconsistent
preferences. Figure1(b)showcasestheimpactduringRLalignment. Thecontrolreward(detailedinSection5)
initially increases but eventually deteriorates, a phenomenon called reward hacking [19]. However, when
WARM servesastheproxyRM,increasing ğ‘€ (thenumberofaveragedRMs)significantlyimprovesabsolute
resultswhiledelayingthecollapse,asindicatedbythecontrolrewardsmaintaininghighervaluesforlonger
duringtraining. SameplotwithKLasthe ğ‘¥-axisinFigure8(a)andwithlabelcorruptioninFigure18.
Challenges. Two primary challenges underlie reward hacking. The first major issue are the distribu-
tion shifts encountered by the RM [36, 37]. Indeed, the generations from the policy might deviate
substantially from those in the offline preference dataset, posing an out-of-distribution (OOD) chal-
lenge. Moreover,thosedistributionshiftsareaccentuatedbythepolicydriftduringtheRLprocedure:
the policy moves away from its SFT initialization, continually altering the distribution of predictions
the RM needs to interpret reliably. Second, preferences are inconsistent: the binary labels in the
preference dataset are noisy. Indeed, human labelers often rely on simpler criteria (length, bullet
points, politeness) over more nuanced indicators. Moreover, errors can be exacerbated for complex
tasks requiring specific expertise [38], and because of the multi-objective nature of alignment [39]
requiring handling the heterogeneity of human opinions. Overall, this results in a low inter-labeler
agreement (72.6% for InstructGPT [40]), altering the robustness of the RM.
Goal and ensembling baseline. Designing good RMs must meet a tripartite objective: guiding RL
efficiently, reliably scoring generations despite the distribution shifts, and providing robust signals
amidstlabelnoise. Toaddressthesechallenges,theseminalworkonRLHFfromChristianoetal. [12]
and more recent works [41, 42] leveraged prediction ensembling (ENS) [43], averaging the rewards
from multiple RMs. ENS improves the reliability of the reward and mitigates hacking risks [41, 42].
Yet, ENS suffers from memory and inference overhead causing efficiency challenges; we will also
show that ENS fails to improve robustness to label noise in the preference datasets.
WARM. In this paper, we propose weight averaged reward models (WARM), a simple, efficient and
scalable strategy to obtain a reliable and robust RM by combining multiple RMs. Starting from a
shared pre-trained LLM, we launch multiple RM fine-tunings: in practice, the different runs have
different hyperparameters (as in grid search), and see the preference data in different orders, thus
2
drawer
lortnoCWARM:OntheBenefitsofWeightAveragedRewardModels
leadingtodiverseRMs. AkeycontributionishowthedifferentRMsaremerged: bylinearinterpolation
in the weight space. This follows the findings from the linear mode connectivity (LMC) [44, 45] and
weight averaging (WA) literature [46, 47, 48]: under shared pre-training, the different weights can
be linearly interpolated despite the non-linearities in the architecture.
On the benefits of WARM. Firstly, WARM stands out for its efficiency and practicality. By requiring
a single model at inference time, it provides a scalable approximation to the traditional, costlier
ensembling of predictions, without its memory and inference burdens. Secondly, WARM improves
reliability by inheriting from the generalization abilities of WA under distribution shifts, a quality
well-documented in the OOD literature for supervised learning [47, 48, 49]. Lastly, WARM improves
robustnesstolabelcorruption. WeshowthatWAselectstheinvariantpredictivemechanisms[50,51]
across different runs [52, 53], thus naturally diminishing the memorization of corrupted samples,
occurring in each run in different ways. In contrast, ENS simply memorizes the corrupted samples.
We also explain why reducing memorization when modeling noisy preferences enhances stability in
the RL process. These multifaceted benefits of WARM are further explored in Section 4.
We summarize our contributions as follows.
1. Innovation in reward modeling. We introduce WARM, the first instance of weight averaging for
reward modeling. This novel strategy efficiently mitigates reward hacking, improves reliability
under distribution shifts and robustness to label corruption.
2. Theoretical and empirical insights into weight averaging. We validate linear mode connectivity
for reward models trained on binary preference datasets. Moreover, we reveal a key difference
between weight and prediction averaging, that appears clearly under label corruption; weight
averaging only maintains the invariant predictive mechanisms across runs, thereby diminishing
memorization and enhancing the focus on generalizable features.
Our experiments on summarization tasks in Section 5 confirm that WARM improves performance
without any memory or inference overhead, either when used as the reward selector in best-of-ğ‘,
or as the proxy RM in RL. WARM mitigates reward hacking, and thus provides better downstream
policies;specifically,itleadstoawinrateof79.4%(accordingtothepreferenceoraclemetric)against
a policy trained with a standard RM.
2. Context and challenges
2.1. Context
LLMs. We consider an LLM ğ‘“ ğœƒ of a fixed non-linear architecture parameterized by ğœƒ, usually a
Transformer with attention layers [54]. It defines a policy by mapping prompt inputs ğ‘¥ to ğ‘“ (ğ‘¥).
ğœƒ
Followingthefoundationmodelparadigm[55]andthesuccessoftransferlearning[56], theweights
ğœƒ are first pre-trained [4] on the vast amount of web data into ğœƒğ‘ğ‘¡, before supervised fine-tuning
(SFT) [7] to learn to follow instructions into ğœƒğ‘ ğ‘“ğ‘¡. However, the high cost and limited scope of
instruction data (i.e., prompts and responses) can create a misalignment [19, 32, 33] between the
LLM and its intended application. Reinforcement learning (RL) as a third step in the training process
of LLMs was shown to help alignment of LLMs with the intended usage [40].
RMs. AnotableaspectofRListheabsenceofsupervisedsamplestobeimitatedbythepolicy;instead,
the focus shifts to maximizing the reward of generated samples, that should measure their quality.
The challenge is that the oracle reward, perfectly encapsulating the desired behaviors, is not given by
the environment. The key innovation from RLHF [12] is that this reward is the output of a reward
model (RM), trained in a supervised way to predict and thus reflect human preferences. Specifically,
3WARM:OntheBenefitsofWeightAveragedRewardModels
anRMisanLLMğ‘Ÿ parameterizedbyğœ™,predictingasinglescalarastherewardğ‘Ÿ (ğ‘¥,ğ‘¦) foraprompt
ğœ™ ğœ™
ğ‘¥ andgeneration ğ‘¦. Theweightsğœ™areusuallyinitializedfrom (cid:0)ğœƒğ‘ ğ‘“ğ‘¡,ğœ”(cid:1),wherethefinallinearlayerğœ”
is added on top of the extracted features from the SFT model ğœƒğ‘ ğ‘“ğ‘¡. Then ğœ™ is trained on a preference
dataset D = {ğ‘¥ ,ğ‘¦+,ğ‘¦âˆ’}ğ· where the generation ğ‘¦+ has been preferred over ğ‘¦âˆ’ to continue ğ‘¥ .
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘‘ ğ‘‘ ğ‘‘ ğ‘‘=1 ğ‘‘ ğ‘‘ ğ‘‘
Usually human labelers evaluate those generations, but recent works on RLAIF [57, 58] showed
that similar performances can be obtained by prompting an LLM for AI feedback. Following the
Bradley-Terry [59] assumption about the distribution of preferences, and by framing the problem
as binary classification, the maximum likelihood principle motivates learning ğœ™ by minimizing the
following negative log-likelihood loss (where ğœ is the logistic function):
L ğ‘…(cid:0)ğ‘Ÿ ğœ™,D ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(cid:1) = âˆ’ğ”¼ (ğ‘¥,ğ‘¦+,ğ‘¦âˆ’)âˆˆDğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›(cid:2)logğœ(cid:0)ğ‘Ÿ ğœ™(cid:0)ğ‘¥,ğ‘¦+(cid:1) âˆ’ğ‘Ÿ ğœ™(ğ‘¥,ğ‘¦âˆ’)(cid:1)(cid:3) . (1)
.
Reward inference. With this RM, the literature suggests applying any kind of RL algorithm (usually
REINFORCE [60] or PPO [61]) to fine-tuned ğœƒğ‘ ğ‘“ğ‘¡ into ğœƒğ‘Ÿğ‘™, as analyzed in Section 5.2. A training-free
alternative is best-of-ğ‘ (BoN) sampling, analyzed in Section 5.1, which returns the generation that
has the highest reward among ğ‘ generations from ğœƒğ‘ ğ‘“ğ‘¡. Both methods aim to align the policy with
human preferences. Yet, the reward misspecification [23] between the proxy RM and the true human
preferences can lead to reward hacking [19, 20, 21, 22], where the policy exploits loopholes in the
proxy RM to artificially increase the score without matching human preferences.
2.2. Challenges in reward modeling
When handling rich inputs such as text, or when assessing complex behaviours, designing rewards
aligned with human preferences is a complex challenge for two main reasons, described below.
Distribution shifts. The primary challenge is the distribution shifts resulting from the offline nature
ofpreferencedata. Indeed,thegenerationsinthepreferencedatasetandthosefromthepolicyğœƒğ‘ ğ‘“ğ‘¡ do
notnecessarilyfollowthesamedistributions,andtheshiftscanbecomeevenmorepronounceddueto
modeldriftduringRL.TheOODgeneralizationliteraturehasextensivelyanalyzedtherepercussionsof
these shifts. Firstly, they often lead to a reduction in performance [62, 63]. RMs (of limited capacity)
trained on narrow data distributions may rely on spurious correlations [51] or a limited number of
features[64],thusfailingwhenencounteringOODexamples[65,66]. Secondly,theycomplicatethe
selection of RMs, as ID validation metrics may poorly correlate with real-world OOD performances
[67, 68] and the ability to guide the RL [41]. Lastly, RMs can become poorly calibrated [69] in OOD
scenarios [70, 71], and predict more extreme values as rewards. Such miscalibration exacerbates
the problem in a negative feedback loop, further intensifying model drift and distribution shifts. In
conclusion, limited data coverage during reward modeling reduces the reliability of the RM and
facilitates reward hacking [36] in regions where the RM is badly specified.
Inconsistent preferences. The second major challenge is the label noise in preference datasets.
Human labelers, often grappling with fatigue, misunderstandings [72, 73] and imperfect incentives
[74], might default to simpler criteria such as length, bullet points, or politeness rather than more
causal indicators. This tendency is exacerbated for complex tasks [38] or when considering multiple
objectives, ranging from harmlessness [75] to engagement [76] and representing the heterogeneity
of human opinions. Consequently, these factors lead to low inter-rater agreement, where human
data appears as an imperfect representation of the underlying ground truth [77, 78]. To mitigate
these issues, there has been a shift towards AI-generated preferences [57, 58], which, while reducing
human labor costs, introduces its own set of noise and failure cases, such as sensitivity to prompting
strategies [79, 80]. These layers of noise and inconsistency challenge the robustness of the RM, and
its ability to provide stable signals.
4WARM:OntheBenefitsofWeightAveragedRewardModels
With this in mind, a good RM should ideally satisfy the three following properties.
Property 1: efficiency. TheRMshouldincurnomemoryorinferenceoverhead. Thenthepolicy
can be optimized efficiently.
Property 2: reliability. The RM should reliably reward predictions despite the distribution
shifts. Then the policy can explore away from its initialization while relying on the RM.
Property 3: robustness. The RM should be robust to the label inconsistencies in binary
preferences. Then the policy can learn from robust signals given by the RM.
2.3. Existing approaches
To tackle those issues, previous works have explored a few research directions, further detailed in
our related work from Appendix A.2. During RL, the standard strategy is to encourage the policy to
remain close to its SFT initialization with Kullback-Leibler (KL) regularization [81, 82]; KL reduces
modeldrift[83,84]butcancauseunderfittingandaddsanextrahyperparameter(theregularization
strength ğ›¼). Collecting, labelling and then training on new data (reflecting the evolving policy)
can improve the reliability of the RM [16]. Yet it poses significant efficiency challenges due to the
continuous requirement for human annotation and computational resources. In contrast, active
learning strategies [85, 86] proactively enrich the preference dataset by seeking out a diverse set of
generations and potential failure cases. Concurrent work [87] suggests applying label smoothing
and flipping. Finally, and most similar to WARM, prediction ensembling (ENS) [43] strategies average
the logits from ğ‘€ RMs. From a bias-variance perspective [88], ENS reduces the variance term when
members are sufficiently diverse [89], and thus favors reliability under distribution shifts where
variance is the key issue [47]. From a RL perspective, ENS was shown to mitigate hacking risks
[12, 41, 42]. Despite its advantages, ENS faces efficiency challenges; the memory and inference
costs grow linearly with ğ‘€, making ENS incompatible with the scaling trend in RMs, where larger
architectures consistently perform better [90]. Moreover, we will also show in Section 4.2 that ENS
fails to improve robustness to preference inconsistencies.
3. WARM
3.1. Weight averaging of reward models
Facingthosechallengesinrewardmodelingandthelimitationsfromexistingapproaches,wepropose
Weight Averaged Reward Models (WARM). WARM is a simple and efficient strategy that combines
multiple models without the memory and inference overheads of prediction ensembling, enhancing
rewardreliability(underdistributionshifts)androbustness(amidstnoisypreferencedataset). WARM
is illustrated in Figure 1(a) and described below.
1. Shared pre-trained initialization. For a given pre-trained LLM, each RM is initialized from
(cid:0)ğœƒğ‘ ğ‘“ğ‘¡,ğœ”(cid:1) combining SFT weights and a linear probed [91] classifier.
2. Diverse fine-tunings. We run ğ‘€ RM fine-tunings, optimizing Equation (1) with diverse hyperpa-
rameters (as in a grid search), yielding ğ‘€ weights {ğœ™ }ğ‘€ .
ğ‘– ğ‘–=1
3. Weight averaging. We average those ğ‘€ weights together to form ğœ™WARM = 1 (cid:205)ğ‘€ ğœ™.
ğ‘€ ğ‘–=1 ğ‘–
5WARM:OntheBenefitsofWeightAveragedRewardModels
Then ğ‘Ÿ serves as the proxy RM to guide the RL procedure, as efficiently as an individual RM, but
ğœ™WARM
withtheenhancedreliabilityandrobustnessprovidedbytheWAstrategy,thatleveragesthestrengths
and mitigates the weaknesses of the individual RMs.
3.2. Linear mode connectivity
Compared to ENS, the main difference lies in how WARM combines the different RMs: we do so
throughlinearinterpolationintheweightspace. Itreliesonthelinearmodeconnectivity(LMC)[44,45]
property across fine-tuned weights, i.e., the fact that the accuracy of the interpolated model is at
least as good as the interpolation of the individual accuracies. Precisely, by defining the pairwise
accuracy of an RM ğ‘Ÿ ğœ™ w.r.t. a dataset D as Acc(cid:0)ğ‘Ÿ ğœ™,D(cid:1) = ğ”¼ (ğ‘¥,ğ‘¦+,ğ‘¦âˆ’)âˆˆD(cid:2)1 ğ‘Ÿ ğœ™(ğ‘¥,ğ‘¦+)â‰¥ğ‘Ÿ ğœ™(ğ‘¥,ğ‘¦âˆ’)(cid:3), the following
Observation 1 underpins the success of WARM.
Observation 1 (LMC). Given two fine-tuned weights ğœ™ 1 and ğœ™ 2 with a shared pre-training and a test
dataset D , then for all ğœ† âˆˆ [0,1],
ğ‘¡ğ‘’ğ‘ ğ‘¡
Acc(cid:0)ğ‘Ÿ ,D (cid:1) â‰¥ (1âˆ’ğœ†)Ã—Acc(cid:0)ğ‘Ÿ ,D (cid:1) +ğœ† Ã—Acc(cid:0)ğ‘Ÿ ,D (cid:1). (2)
(1âˆ’ğœ†)Â·ğœ™1+ğœ†Â·ğœ™2 ğ‘¡ğ‘’ğ‘ ğ‘¡ ğœ™1 ğ‘¡ğ‘’ğ‘ ğ‘¡ ğœ™2 ğ‘¡ğ‘’ğ‘ ğ‘¡
We empirically validate this LMC in Figure 3, by evaluating interpolated RMs on OOD test samples.
Thisfollowssimilarobservationsformulti-classclassificationinthecontextofcomputervision[44,45],
whichledtoaplethoraofweightaveraging(WA)workssuchasthemodelsoups[46,47,48]variants
(detailed in our related work in Appendix A.1).
Remark 1 (Importance of pre-training and linear probing). The efficacy of WA can be surprising
given the non-linearities [54] and permutation symmetries [92] in deep neural network architectures.
WA is actually possible only because of the shared pre-training which constrains the divergence during
fine-tunings [45], such as the weights remain in convex regions of the loss valley [93]. In contrast, the
LMC does not hold when training weights from scratch [45], even if the random initialization is shared.
For these reasons and to facilitate the LMC, we follow [47, 48] and use linear probing to initialize the
classifier ğœ”; compared to random initialization, such linear probing prevents feature distortion [91].
3.3. Sources of diversity
On one hand, WARM requires shared pre-training so that the fine-tuned weights remain linearly
connected. On the other hand, weights must not be identical: actually, the diversity across those
fine-tuned weights significantly contributes to the accuracy gains observed in WA [47]. Overall, an
effective WARM requires a delicate trade-off between ensuring LMC and diversity across weights.
In practice, we use the following sources of diversity [94], leading the RM fine-tunings to diverse
yet linearly connected models. First, the different fine-tunings see the data samples in different
orders. Second, we sample slightly different hyperparameters, notably different learning rates and
dropout probabilities, as detailed in Appendix B.3. Third, we investigate a new source of diversity in
initialization named Baklava, illustrated in Figure 2. Specifically, we initialize the RMsâ€™ featurizers
fromdifferentcheckpoints {ğœƒğ‘ ğ‘“ğ‘¡ }ğ‘€ collectedalongagivenSFTtrajectory. Baklavarelaxestheshared
ğ‘– ğ‘–=1
initialization constraint from model soups [46] to simply sharing the same pre-training: Baklava is
actually an efficient alternative to model ratatouille [48] but without the need of multiple auxiliary
tasks. Overall, Baklava increases diversity compared to only initializing from the last SFT checkpoint,
while adhering to the shared pre-training requisite for LMC, without incurring any overhead.
6WARM:OntheBenefitsofWeightAveragedRewardModels
ğœƒğ‘ğ‘¡ ğœƒğ‘ ğ‘“ğ‘¡ ğœƒğ‘ ğ‘“ğ‘¡ ğœƒğ‘ ğ‘“ğ‘¡
1 2 ğ‘€
SFT
Reward
ğœ™ ğœ™ ğœ™
modelings 1 2 ğ‘€
Weight
ğœ™WARM = 1 (cid:205)ğ‘€ ğœ™
averaging ğ‘€ ğ‘–=1 ğ‘–
Figure2|Baklavadiversityprocedure. Startingfromapre-trainedLLMğœƒğ‘ğ‘¡,weconsiderdifferentcheckpoints
{ğœƒğ‘ ğ‘“ğ‘¡ }ğ‘€ alongasingleSFTrun(dashedarrow )collectedatdifferentnumberofSFTtrainingsteps. Those
ğ‘– ğ‘–=1
checkpointsserveasinitializationsfor ğ‘€ RMfine-tuningsonthepreferencedataset(thicksolidarrows )
tolearnthe {ğœ™}ğ‘€ . Finally,thoseRMsareweightaveraged(dottedarrows )intothefinalmodelğœ™WARM.
ğ‘– ğ‘–=1
Followingtheculinaryanalogyfrommodelsoups[46]andmodelratatouille[48], wenamedthismethod
Baklavabecauseofitsdiamondgeometricshape.
Remark 2 (Moving average). Following stochastic weight average [95] or moving average [96], we
also tried to average checkpoints collected along a single RM fine-tuning. Though interesting because less
costly for training, the lower results in Figure 3(a) suggest that the accuracy-diversity trade-off was not
favorable: incorporatingearlycheckpointswouldcompromiseindividualaccuracies,andconsideringonly
later checkpoints would not bring the necessary diversity. As a result, we opted to use in WARM only the
last checkpoint from each RM fine-tuning.
4. On the benefits of WARM
WenowexplorethepropertiesandbenefitsfromtheWARMstrategy,previouslydescribedinSection3.
WegroundouranalysisontheempiricalcomparisonbetweenWAandENSforrewardmodeling,and
a novel general theoretical comparison in Section 4.3.
Experimental setup. We leverage the TL;DR summarization benchmark [97], a standard in reward
modeling for LLMs, that we briefly describe below and further detail in Appendix B. The goal of the
RMs is to score summaries such as they are ranked properly. In training, we use the dataset D
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
from Stiennon et al. [14] where the candidate summaries are generated by GPT-3 [6] variants. To
obtain the labels, we follow the RLAIF procedure from [58], where a PaLM-L [98] is prompted with
chain-of-thought [99] to generate feedback mimicking human preferences. This strategy performs
similarly to human labelers with similar inter-agreement, and will be useful in Section 5 as an oracle
metric. The RMs are PaLM-XXS models, pre-trained and SFT-ed on the preferred summaries from
D , on which we plug a linear probed [91] classification layer. We train the RMs for 10k steps on
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
D , with hyperparameters and procedure detailed in Appendix B.3. We report accuracies of those
ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
RMsonanovelout-of-distribution(OOD)testdataset D with92kpairwisecomparisonswherethe
ğ‘œğ‘œğ‘‘
summaries are generated by multiple PaLM-XS policies with high temperature, some of which are
pre-trained only, others SFT-ed and others RLHF-ed.
4.1. 1st order analysis: weight averaging for reliable and more efficient ensembling
Previous works [46, 47, 95] have argued that the best way to understand WA is as an efficient
approximation of ENS, as clarified in Observation 2.
Observation 2 (WAandENS:1st orderanalysis). Weightaveragingandpredictionensemblingperform
similarly: i.e., for all ğœ† âˆˆ [0,1] and a test dataset D ,
ğ‘¡ğ‘’ğ‘ ğ‘¡
Acc(cid:0)ğ‘Ÿ ,D (cid:1) â‰ˆ Acc(cid:0) (1âˆ’ğœ†)Ã—ğ‘Ÿ +ğœ† Ã—ğ‘Ÿ ,D (cid:1). (3)
(1âˆ’ğœ†)Â·ğœ™1+ğœ†Â·ğœ™2 ğ‘¡ğ‘’ğ‘ ğ‘¡ ğœ™1 ğœ™2 ğ‘¡ğ‘’ğ‘ ğ‘¡
7WARM:OntheBenefitsofWeightAveragedRewardModels
0.765 0.765 0.765 0.765
0.764 0.764 0.764 0.764
0.763 0.763 0.763 0.763
0.762 0.762 0.762 0.762
0.761 0.761 0.761 0.761
0.760 0.760 0.760 0.760
WA WA WA WA
0.759 ENS 0.759 ENS 0.759 ENS 0.759 ENS
Diag Diag Diag Diag
0.758 0.758 0.758 0.758
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(a)1RMfine-tuningat2 (b)2RMfine-tuningswith (c)2RMfine-tuningswith (d)2RMfine-tuningswith
differenttrainingsteps. sharedconfig. differentlearningrates. differentinits:Baklava.
Figure3 | ExperimentsunderdistributionshiftsvalidatingObservations1and2ontheTL;DRsumma-
rizationbenchmark[97]. WereporttheaccuraciesonD
ğ‘œğ‘œğ‘‘
wheninterpolatingbetweentwoRMweightsğœ™
1
andğœ™ withthecoefficient ğœ† slidingbetween0and1. WAstandsforweightaveragingğ‘Ÿ whileENS
2 (1âˆ’ğœ†)Â·ğœ™1+ğœ†Â·ğœ™2
combinesthepredictions(1âˆ’ğœ†)Ã—ğ‘Ÿ +ğœ†Ã—ğ‘Ÿ ;Diagistheinterpolatedaccuracy(1âˆ’ğœ†)Ã—Acc(cid:0)ğ‘Ÿ (cid:1) +ğœ†Ã—Acc(cid:0)ğ‘Ÿ (cid:1).
ğœ™1 ğœ™2 ğœ™1 ğœ™2
We consider sources of increasing diversity [94] between ğœ™ and ğœ™ : in Figure 3(a), they are collected at
1 2
differentnumberoftrainingsteps(8kand10k)alongasingleRMfine-tuning;inFigure3(b),theyarefrom
two independant RM fine-tunings, with the exact same config, but seeing the data in different orders; in
Figure3(c),theyhavedifferentlearningrates(1e-4and4e-5);inFigure3(d),theyareinitalizedfromdifferent
SFTcheckpointscollectedatdifferentnumberofSFTsteps(8kand12k),perBaklavaintroducedinFigure2.
0.49 W E DN iA aS g 0.902 W E DN iA aS g 0.778 W E DN iA aS g 0.716 W E DN iA aS g
0.776
0.900
0.48 0.774 0.714
0.898 0.772
0.47 0.712
0.896 0.770
0.46 0.768 0.710
0.894
0.766
0.45 0.708
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
(a)Train(corrupt). (b)Train(clean). (c)Validation(ID). (d)Test(OOD).
Figure4 | CorruptionexperimentvalidatingObservation3. Weconsiderğœ™ 1 andğœ™ 2,twoRMsfine-tuned
independentlywiththesameconfigasinFigure3(b),butthistimewith25%ofthetraininglabelscorrupted.
We then report the performances of their WA and ENS on the different data subsets. We observe that WA
reducesmemorizationofthecorruptedlabelsinFigure4(a),andstillperformsslightlyworsethanENSonthe
cleantrainingsamplesinFigure4(b);yet,theperformancesofWAw.r.t.ENSimprovesaswemoveawayfrom
thetrainingdistribution,inparticularonD inFigure4(d)whereWAgeneralizesbetter.
ğ‘œğ‘œğ‘‘
Theoretically, a simple Taylor expansion can justify this similarity when âˆ¥ğœ™ 1âˆ’ğœ™ 2âˆ¥ â‰ª 1. Empirically,
this is validated in Figure 3 where the accuracy curves on D for WA and ENS closely match. This
ğ‘œğ‘œğ‘‘
similarity justifies that WA is a variance reduction method; then, because variance is the dominant
issue under distribution shifts [47], this explains the significant gains in Figure 3 over the individual
RMs ğœ™ and ğœ™ (validating Observation 1), in particular when weights are sufficiently diverse. This
1 2
suggests improved reliability in WARM, with efficiency benefits over ENS: indeed, WA maintains a
single set of weights, removing the memory and inference overheads from ENS.
8
.ccA
.ccA
.ccA
.ccA
.ccA
.ccA
.ccA
.ccAWARM:OntheBenefitsofWeightAveragedRewardModels
4.2. 2nd order analysis: weight averaging for more robust ensembling
A surprising fact remains unexplained. WA is slightly superior to ENS under distribution shifts,
which one can see on the plots from Figure 3, and more consistently in Figure B.1 from model soups
[46] or in Figure 1 from DiWA [47]. More generally, WA is the state-of-the-art strategy for OOD
generalization, consistently outperforming ENS; yet, this was not explained in previous works, thus
urging for new insights about the difference between WA and ENS.
Corruption setup. To refine our understanding on the difference between WA and ENS, we propose
a new setup where 25% of the binary labels are swapped in training. We then report the per-subset
accuracies on Figure 4, enriched in Appendix C.1 and aggregated in Figure 5. On the corrupted
subset of training data, the accuracy curve for WA is below the expected accuracies, while it is above
on all other subsets. More precisely, we make the following Observation 3.
Observation 3 (WA and ENS: 2nd order analysis). The accuracy gains of WA over ENS grow as data
moves away from the training distribution.
â€¢ WA â‰ª ENS on train corrupt: WA is far worse than ENS on train samples with swapped labels,
showing reduced memorization and improved robustness to label corruption.
â€¢ WA â‰¤ ENS on train clean: WA is worse than ENS on train samples with correct labels.
â€¢ WA âª† ENS on ID val: WA is better or similar to ENS on samples without distribution shifts.
â€¢ WA â‰¥ ENSonOODtest: WAisfarbetterthanENSontestsamplesfromnewdistributions,showing
better reliability under distribution shifts.
TTrraaiinn ccoorrrruupptteedd
200 TTrraaiinn cclleeaann
Overall, this suggests that weight averaging memorizes less and VVaall IIDD
TTeesstt OOOODD
generalizes better than ensembling predictions.
150
4.3. Weight averaging enforces invariance across runs 100
WenowprovidetheoreticalsupporttothisObservation3. Inbrief,
50
oursimplifyingassumptionssuggestthatWAactsasaregulariza-
tion towards the predictive mechanisms that are invariant across
0
0.06 0.05 0.04 0.03 0.02 0.01 0.00 0.01 0.02
runs,i.e.,learnedsimultaneouslyineachindependentrun. Then, Accuracy gain of WA over ENS
incontrastwithENS,WAwouldimproverobustnesstocorruption
Figure5 | Histogramsofthediffer-
because it would underweight the run-specific features (with low encesinaccuracybetweenWAand
probability of being learned) inducing memorization. ENSondifferentdatasubsets.
Setup. We follow Lin et al. [53], and consider a simplified binary classification setup with labels
ğ‘¦ âˆˆ {âˆ’1,1}, related to ğ¹ features {ğ‘§ğ‘—}ğ¹ such as ğ‘§ğ‘— âˆˆ â„ğ‘‘. From inputs ğ‘¥, we train a binary classifier
ğ‘—=1
ğ‘Ÿ(ğ‘¥) = ğœ”âŠºğ‘“(ğ‘¥). Following [53], we make three key assumptions. First, features orthogonality: we
assume that {ğ‘§ğ‘—}ğ¹ are orthogonal, i.e., (ğ‘§ğ‘—)âŠºğ‘§ğ‘—â€² = 0 when ğ‘— â‰  ğ‘—â€². Second, input as bag of features: we
ğ‘—=1
assume that the input ğ‘¥ = (cid:2) ğ‘¥ğ‘—(cid:3)ğ¹ âˆˆ â„ğ¹Ã—ğ‘‘ can be represented as the concatenation of ğ‘¥ğ‘— generated
ğ‘—=1
by ğ‘¥ğ‘— âˆ¼ N(cid:0)ğ‘¦ Â·ğ‘§ğ‘—,ğœÂ·Iğ‘‘(cid:1) with ğœ â‰ª 1. Finally, the binary featurizer assumption: we consider that the
featurizer ğ‘“ = (cid:2) ğ‘“ğ‘—(cid:3)ğ¹ âˆˆ {0,1}ğ¹ is a binary selector of the features that make the input. For example,
ğ‘—=1
if ğ‘¦ = 1, ğ¹ = 3, ğ‘¥ â‰ˆ [ğ‘§1,ğ‘§2,ğ‘§3], and ğ‘“ = [1,0,1] learns to extract the first and third features, then
ğ‘“(ğ‘¥) â‰ˆ ğ‘§1 + ğ‘§3. We denote ğ‘ the probability that the featurizer ğ‘“ learns to use the ğ‘—-th feature
ğ‘—
dimension (associated with ğ‘§ğ‘—); this means ğ‘“ğ‘— is 1 with probability ğ‘ and 0 otherwise. Moreover,
ğ‘—
for infinite training samples and under some constraint on ğœ, Lemma 5 in [53] proved that, to learn
ğ‘Ÿ = ğœ”âŠºğ‘“, the optimal linear fit ğœ” on the features selected from ğ‘“ would be ğœ” = (cid:205)ğ¹ ğ‘“ğ‘— Â·ğ‘§ğ‘—.
ğ‘—=1
9
ytisneDWARM:OntheBenefitsofWeightAveragedRewardModels
Results. We consider ğ‘€ RMs {ğ‘Ÿ ğ‘– = ğœ” ğ‘–âŠº ğ‘“ ğ‘–} ğ‘–ğ‘€ =1, and compare the limit behaviours of their predic-
tion ensembling ğ‘Ÿğ¸ğ‘ğ‘† and weight averaging ğ‘Ÿğ‘Šğ´ when ğ‘€ â†’ âˆ. In this limit case, the averaged
ğ‘€ ğ‘€
prediction ğ‘Ÿğ¸ğ‘ğ‘† = 1 (cid:205)ğ‘€ ğœ”âŠº ğ‘“ for an input ğ‘¥ from label ğ‘¦ tends towards the expected prediction
ğ”¼[ğ‘Ÿ(ğ‘¥)] =
ğ”¼[ğ‘€ ğœ”âŠºğ‘“(ğ‘¥)ğ‘€
]
=ğ‘– ğ”¼=1 {ğ‘“ğ‘—}ğ‘–
ğ¹
ğ‘–
(cid:2)(cid:0)(cid:205)ğ¹
ğ‘—=1
ğ‘“ğ‘—Â·ğ‘§ğ‘—(cid:1)âŠº ((cid:205)ğ¹
ğ‘—â€²=1
ğ‘“ğ‘—â€²Â·ğ‘¥ğ‘—â€²)(cid:3) â‰ˆ ğ‘¦Â·(cid:205)ğ¹
ğ‘—=1
ğ‘ ğ‘—Â·|ğ‘§ğ‘—|2,using ğ‘¥ğ‘—â€² â‰ˆ ğ‘¦Â·ğ‘§ğ‘—â€² thus
ğ‘—=1
(ğ‘§ğ‘—)âŠºğ‘¥ğ‘—â€² â‰ˆ 0 when ğ‘— â‰  ğ‘—â€², and (ğ‘“ğ‘—)2 = ğ‘“ğ‘—.
ğ¹
ğ‘Ÿğ¸ğ‘ğ‘†(ğ‘¥) âˆ’âˆ’âˆ’âˆ’âˆ’â†’ ğ”¼[ğ‘Ÿ(ğ‘¥)] â‰ˆ ğ‘¦ Â·âˆ‘ï¸ ğ’‘ Â·|ğ‘§ğ‘—|2. (4)
ğ‘€ ğ’‹
ğ‘€â†’âˆ
ğ‘—=1
(cid:16) (cid:17)âŠº(cid:16) (cid:17)
In contrast, when considering ğ‘Ÿğ‘Šğ´ = 1 (cid:205)ğ‘€ ğœ” 1 (cid:205)ğ‘€ ğ‘“ with ğ‘€ â†’ âˆ, we have
ğ‘€ ğ‘€ ğ‘–=1 ğ‘– ğ‘€ ğ‘–=1 ğ‘–
ğ‘€1 (cid:205) ğ‘–ğ‘€
=1
ğ‘“
ğ‘–
âˆ’âˆ’ ğ‘€âˆ’ â†’âˆ’âˆ’â†’
âˆ
ğ”¼[ğ‘“] = (cid:2) ğ‘ ğ‘—(cid:3)ğ¹
ğ‘—=1
and ğ‘€1 (cid:205) ğ‘–ğ‘€ =1ğœ”
ğ‘–
âˆ’âˆ’ ğ‘€âˆ’ â†’âˆ’âˆ’â†’
âˆ
ğ”¼[ğœ”] = (cid:205)ğ¹
ğ‘—=1
ğ‘
ğ‘—
Â·ğ‘§ğ‘—, and thus:
âŠº
ğ¹ ğ¹ ğ¹
ğ‘Ÿğ‘Š ğ‘€ğ´(ğ‘¥) âˆ’âˆ’ ğ‘€âˆ’ â†’âˆ’âˆ’â†’
âˆ
(cid:169) (cid:173)âˆ‘ï¸ ğ‘
ğ‘—
Â·ğ‘§ğ‘—(cid:170)
(cid:174)
(cid:169) (cid:173)âˆ‘ï¸ ğ‘
ğ‘—â€²
Â·ğ‘¥ğ‘—â€²(cid:170)
(cid:174)
â‰ˆ ğ‘¦ Â·âˆ‘ï¸ ğ’‘2
ğ’‹
Â·|ğ‘§ğ‘—|2. (5)
ğ‘—=1 ğ‘—â€²=1 ğ‘—=1
(cid:171) (cid:172) (cid:171) (cid:172)
Interpretation. For ENS, the coefficient for a given feature is ğ’‘ , the same as the probability of
ğ’‹
this information being used by any individual network. In contrast, WA involves the square of the
probability ğ’‘2. Thus WA reduces the reliance on features with low probability, related to minor
ğ’‹
specificinformation(suchasnoiseorcontext)whichcanbeusedtofitthecorruptedtrainingsamples;
this would reduce memorization, and thus explains the robustness of WA under label corruption.
Reciprocally, WA tends to prioritize the most probable features, favoring the mechanisms that are
consistently learned, in other words the mechanisms invariant across runs. Overall, WA acts as a
regularization, improving robustness under label corruption by tackling run-specific mechanisms
favoringmemorization,andimprovingreliabilityunderdistributionshiftsbypreservingrun-invariant
mechanisms favoring generalization.
Remark 3(Invariance). Wearguethatweightaveragingonlykeepstheinvariantpredictivemechanisms
across runs. This is in analogy with the invariance literature [50], popular for domain generalization
[51, 100] under spurious correlations, where the key idea is that the predictive mechanisms which are
invariant across domains are the causal ones that are stable under distribution shifts. This theoretically
connects two key paradigms for OOD generalization, ensembling and invariance, and shows that weight
averaging actually benefits from both.
Remark 4 (Extension to a deeper structure with ğ¿ layers). We obtain a square in ğ’‘2 due to our
ğ’‹
simplified two-layer architecture. Yet, in full generality, using a deeper structure with ğ¿ layers would
lead to ğ’‘ğ‘³. Intuitively, WA applies an AND-mask on the information, that need to be found both in the
ğ’‹
previous feature space and the next layer weights.
Remark 5 (From reward robustness to learnability). When applied to the design of RMs in WARM,
we now argue that WA facilitates WARMâ€™s stability [87] by mitigating the reliance on some non-robust
features. Indeed, WA makes the WARM reward more robust to small (potentially adversarial [101])
perturbations [102], i.e., smoother [103] in the input space. This relates to the Lipschitzness property
of the reward [104, 105, 106], where the difference in predicted rewards is bounded by the distance in
input space. Fortunately, such smoothness is useful in RL [107], in particular for the stability of the
policy gradient [108] because â€œsharp changes in reward value are hard to represent and internalizeâ€
[109]. This is studied in Lipschitzness is all you need [109] where the authors argue that â€œthe local
Lipschitzness of the reward is a sine qua non condition for good performanceâ€, required â€œto even learn
anythingâ€. In summary, robustness improves stability and hinders the cascade of errors occurring when
minor input variations can cause large reward differences.
10WARM:OntheBenefitsofWeightAveragedRewardModels
Inconclusion,wesummarizethebenefitsfromWARM. First,WARMisefficient,incurringnomemory
or computation costs, as it returns a single model. Second, WARM reduces variance while leveraging
mechanisms invariant across runs, thus improving its reliability under distribution shifts. Lastly,
WARM also addresses label corruption, thereby augmenting robustness to noisy preferences.
5. Experiments
To empirically validate WARMâ€™s benefits described in previous section, we train PaLM-XXS RMs on
theTL;DRsummarizationbenchmark[97]wherepreferencelabelsaregeneratedbyaPaLM-Lmodel
prompted with chain-of-thought [99]. This AI labeling approach, increasingly common in recent
research [26, 41, 110] as an efficient alternative to human assessments, is motivated by studies
[57, 58] indicating that it correlates well with human preferences: critically, it provides an automatic
pairwise oracle preference metric to evaluate reward hacking (in a similar fashion to the distillation
setup from [17], discussed in Appendix C.4). In addition, we leverage a PaLM-XS RM for pointwise
000 ... 111 024 W W
E
I In
nN
d
dA A
S
R R MM M
2
1
=M M 2= =6 2 00 .. 22 05 W W
E
I In
nN
d
dA A
S
R R MM M
2
1
=M M 2= =6 2 000 ... 233 505 W W
E
I In
nN
d
dA A
S
R R MM M
2
1
=M M 2= =6 2 000 ... 456 W W
E
I In
nN
d
dA A
S
R R MM M
2
1
=M M 2= =6 2
0.15 0.20
0.08 0.3
0.06 0.10 0.15 0.2
0.04 0.10 0.1
0.05
0.02 0.05 0.0
0.00 0.00 0.00 0.1
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6
KL:log(N) NN1 KL:log(N) NN1 KL:log(N) NN1 KL:log(N) NN1
(a)PaLM(clean). (b)PaLM(corrupt). (c)T5(clean). (d)T5(corrupt).
Figure6 | ControlrewardforBoNexperiments: cleanpreferencedatasetinFigures6(a)and6(c)and25%
corruptionsinFigures6(b)and6(d). WeconsidertwoSFTpoliciestogeneratecandidatesummaries: one
basedonPaLMarchitecture[98],theotheronT5architecture[111]. The ğ‘¥-axisistheKLbetweentheBoN
policyandtheSFTpolicy;the ğ‘¦-axisrepresentsthecontrolrewardgainsw.r.t.toanRMğœ™ ,whichwasthe
1
bestindividualRMonD . ThebluelinesrepresentWARM with ğ‘€ weights: WARM performshigherthanthe
ğ‘œğ‘œğ‘‘
individualRMs(inyellows)orwhenensemblingtheirpredictions(ENSinred). Wereporttheabsolutecontrol
rewardsforthoseexperimentsinFigure15,wherethevaluesrangeroughlybetween3and7.
0.925 0.50 0.500
0.90
0.900 0.48 0.475
0.450 0.875 0.85 0.46
0.425 0.850
0.825 0.80 0.44 0.400
0.800 W WA AR RM M M M= =6 2 0.75 W WA AR RM M M M= =6 2 0.42 W ENA SR MM =M 2=2 00 .. 33 57 05 W ENA SR MM =M 2=2
0.775 ENS M=2 ENS M=2 0.40 Ind 2 Ind 2
Ind 2 Ind 2 Ind 1 0.325 Ind 1
0.750 Ind 1 0.70 Ind 1 y=0.5 y=0.5
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0.38 0 1 2 3 4 5 6 0.300 0 1 2 3 4 5 6
KL:log(N) N N1 KL:log(N) N N1 KL:log(N) N N1 KL:log(N) N N1
(a)SFT(clean). (b)SFT(corrupt). (c)WARM(clean). (d)WARM(corrupt).
Figure7 | OraclepreferencemetricforBoNexperimentsonT5generations: cleanpreferencedatasetin
Figures7(a)and7(c)and25%corruptionsinFigures7(b)and7(d). Weplotthewinratesfordifferentvalues
of ğ‘ vs.tworeferencestrategies: SFT(i.e.,randomselectionorequivalentlyBoNwith ğ‘ =1),orselectingthe
bestsummaryaccordingtoWARM ğ‘€ =6. WeobservethatallstrategiesbeattheSFTreference(theyareall
above50%winrate),butthatnonebeattheWARM ğ‘€ =6reference.
11
niag
drawer
lortnoC
TFS
.sv
oitar niW
niag
drawer
lortnoC
TFS
.sv
oitar niW
niag
drawer
lortnoC
6=M
MRAW
.sv oitar
niW
niag
drawer
lortnoC
6=M
MRAW
.sv oitar
niWWARM:OntheBenefitsofWeightAveragedRewardModels
controlrewardreaching80.1%accuracyontheOODdatasetD . Asverifiedinourexperiments,this
ğ‘œğ‘œğ‘‘
control RM also detects hacking, as it benefits from a larger architecture and a disjoint pretraining
compared to the PaLM-XXS RMs of interest. Below, we explore two key scenarios: in Section 5.1,
WARM reranks outputs in best-of-ğ‘ (BoN); in Section 5.2, WARM guides the RL procedure.
5.1. Best-of-ğ‘ experiments
Setup. We start with best-of-ğ‘ (BoN) sampling experiments in Figures 6 and 7. Given a dataset
of ğ· text prompts, for each prompt we generate ğ‘ summaries from a SFT policy, and then returns
the summary with the highest reward according to different RMs. We actually consider two SFT
policies;onebasedonPaLMarchitecture[98](ğ‘ = 8, ğ· = 15000),theotheronT5architecture[111]
(ğ‘ = 1000, ğ· = 1000). For the ğ‘¥-axis, we plot the KL between the BoN policy and the SFT policy,
whichcanbeapproximatedbylog(ğ‘)âˆ’ğ‘âˆ’1 [112,113]. BoNiseffective[16],especiallyinthelow-KL
ğ‘
regime (i.e., for small ğ‘). We consider two setups, without (clean setup) and with (corrupt setup)
25% label corruption in the preference datasets for reward modeling, and denote in each setup the
weights {ğœ™ }ğ‘€ sorted in decreasing accuracy on D .
ğ‘– ğ‘–=1 ğ‘œğ‘œğ‘‘
Control reward. Figure 6 shows that, in terms of pointwise control reward, WARM performs con-
sistently better than ENS (only with ğ‘€ = 2 for computational reasons) and the two best individual
RMs ğœ™ 1 and ğœ™ 2; moreover, the gains get bigger for ğ‘€ = 6. As a side note, we also observe that the
individual RM ğœ™ performs better in BoN in Figure 6(c) than ğœ™ though ğœ™ was better than ğœ™ on
2 1 1 2
D , highlighting that selecting the appropriate individual RM is not trivial [41].
ğ‘œğ‘œğ‘‘
Oracle preference. In Figure 7, we leverage the pairwise oracle preference [58] metric to validate
better performance with WARM. We observe in Figures 7(a) and 7(b) that summaries selected with
WARM have a win rate of up to 92.5% against the random selection of a summary (from SFT). We
also see in Figures 7(c) and 7(d) that reciprocally, all selection strategies have a win rate lower than
50% against the summaries selected by WARM ğ‘€ = 6.
5.2. RL experiments
Setup. ForRLfine-tuningofpolicies,wefollow[58]andusetheirmodifiedversionofREINFORCE[60]
withabaselinevaluescoreforvariancereduction,asimpleralgorithmthanPPO[61]yetstilleffective
for LLMs. Both policy and value LLMs are PaLM-XS, initialized from the same SFT model. We then
generate samples with the policy, compute the reward with the RMs and update the weights to
optimizethisreward. MoredetailsareavailableinAppendixB.4. Toreduceforgettingandencourage
the policy to remain close to its SFT initialization, we incorporate a KL regularization [81, 82]
controlled by a coefficient ğ›¼, ablated in Figure 8(c), yet otherwise set to 0.003 in the clean setup and
0.01 in the corrupt setup. This KL serves as the ğ‘¥-axis in our plots to estimate model drift, as done in
the literature; same curves with the number of training steps as the ğ‘¥-axis in Figures 1(b) and 18.
Control reward. In Figure 8, we observe reward hacking; as the policy moves away from its SFT
initialization, the control reward collapses. Critically, WARM improves performances: in particular,
increasingğ‘€pushestheParetofrontofsolutionstothetopleftinFigures8(a)and8(b). Incomparison,
policiestrainedwithENS(with ğ‘€ = 2forcomputationalreasons)arestillsusceptibletoearlyreward
hacking, while reaching absolute control rewards significantly worse than with WARM (even with
ğ‘€ = 2). In Figure 8(c), we confirm that the ğ›¼ hyperparameter plays a crucial role; low values of ğ›¼
such as 0.001 correspond to high KL, while high values of ğ›¼ such as 0.01 entail low KL but a risk of
underfitting. From a practical perspective, this highlights that the optimal value of ğ›¼ for WARM is
lower than for a single RM; this is because WARM can mitigate reward hacking, and thus the optimal
policies are obtained for larger values of KL.
12WARM:OntheBenefitsofWeightAveragedRewardModels
11
10 10
7
9
8
8
6
7 6
6 WARM M=10 5
WARM M=6 WARM M=6 4 WARM =0.01
5 WARM M=2 WARM M=2 WARM =0.003
ENS M=2 ENS M=2 WARM =0.001
4 Ind 2 4 Ind 2 2 I In nd d = =0 0. .0 01 03
Ind 1 Ind 1 Ind =0.001
3
0 200 400 600 800 1000 0 25 50 75 100 125 150 175 0 250 500 750 1000 1250 1500 1750
KL KL KL
(a)RL(clean). (b)RL(corrupt). (c)Ablatingğ›¼forRL(clean).
Figure8 | ControlrewardforRLexperiments: cleanpreferencedatasetinFigures8(a)and8(c)and25%
corruptionsinFigure8(b). ThebluelinesshowtheRLfine-tuningofpolicieswhenaveraging ğ‘€ weightsasthe
RM;thedarker,thehigherthe ğ‘€. ItperformshigherthanwhenRLfine-tuningwiththeindividualRMs(in
yellows)orwhenensemblingtheirpredictions(inred). Figure8(c)showsresultsofpoliciesRLfine-tunedwith
WARM ğ‘€ =6orğœ™ 1,fordifferentvaluesofğ›¼controllingtheKLregularizationstrength.
1.0 0.5 0.8
0.7
0.9
0.4
0.6
0.8 0.5
0.3
0.4
0.7
WARM M=10 0.2 WARM M=10 0.3 WARM M=10
WARM M=6 WARM M=6 WARM M=6
0.6 WARM M=2 WARM M=2 0.2 WARM M=2
ENS M=2 0.1 ENS M=2 ENS M=2
Ind 2 Ind 2 0.1 Ind 2
0.5 Ind 1 Ind 1 Ind 1
y=0.5 0.0 y=0.5 0.0 y=0.5
2000 3000 4000 5000 6000 2000 3000 4000 5000 6000 2000 3000 4000 5000 6000
# steps # steps # steps
(a)SFT. (b)WARMğ‘€=6. (c)ğœ™1(bestindividualRM).
Figure9 | Oracle preference metric for RL experiments: cleanpreferencedataset. Weplotthewinrates
along RL fine-tuning against three reference policies: the SFT policy, the policy RL fine-tuned with WARM
ğ‘€ =6after3500steps,andthepolicyRLfine-tunedwithğœ™
1
after3000steps. Figure19reportsresultswhen
comparingpoliciesatfixednumberoftrainingsteps.
Oracle preference. In Figure 9, we compare the different policies according to our pairwise oracle
preference AI labeler [58]. In Figure 9(a), the reference policy is the SFT initialization; all the RL
fine-tuned policies outperform this baseline, with WARM ğ‘€ = 6 reaching a win rate of 99.8% after
3500steps(thehighestwinrateamongallpolicies). WeusethispolicyasthereferenceinFigure9(b);
no other policy could beat it. Interestingly, we observe that using ğ‘€ = 10 rewards can delay reward
hacking but does not improve the peak performance; we speculate this is related to our weight
selectionprocedure,astheweights {ğœ™ }10 havelowerindividualaccuracyon D than {ğœ™ }6 (more
ğ‘– ğ‘–=7 ğ‘œğ‘œğ‘‘ ğ‘– ğ‘–=1
details in Figure 10). Finally, in Figure 9(c), the reference policy is obtained after 3000 steps of
RL fine-tuning with ğœ™ 1 (the best individual RM on D ğ‘œğ‘œğ‘‘). There is a large region of steps in which
policiestrainedWARM (evenwith ğ‘€ = 2)beatthisapproach;thepreviousreferencefromFigure9(b)
actually has a 79.4% win rate against it.
13
drawer
lortnoC
TFS
.sv
oitar
niW
drawer
lortnoC
0053
pets
ta
6=M
MRAW
.sv
oitar
niW
drawer
lortnoC
0003
pets
ta
1
dnI
.sv
oitar
niWWARM:OntheBenefitsofWeightAveragedRewardModels
6. Discussion
Benefits. WARM represents a flexible and pragmatic method to improve the alignment of AI with
human values and societal norms. This paper has detailed several of its benefits, and below, we delve
intoadditional,moreexploratoryadvantages. WARMfollowstheupdatablemachinelearningparadigm
[114], eliminating the need for inter-server communication, thus enabling embarrassingly simple
parallelization[115]ofRMs. Thisfacilitatesitsuseinfederatedlearningscenario[116]wherethedata
should remain private; moreover, WA would add a layer of privacy and bias mitigation by reducing
the memorization of private preference [52]. Then, a straightforward extension of WARM would
combine RMs trained on different datasets, for example, coming from different (clusters of) labelers.
This diversity could help WARM performances, but also from a multi objective perspective [117]; by
non-uniform interpolation of RMs, we could learn a set of personalized policies [39]. Furthermore,
as WA has been shown to limit catastrophic forgetting [118, 119], WARM could seamlessly support
iterativeandevolvingpreferences. Finally,apromisingresearchdirectionisextendingWARMtodirect
preference optimization (DPO) strategies [120], where averaging the RMs casts back to averaging
the DPO policies [121].
Limitations. WARM, while innovative, does face some limitations, notably two when compared to
prediction ensembling methods; first, prediction ensembling can benefit from the diversity brought
by combining RMs from various architectures and pre-trainings; second, prediction ensembling
can incorporate prediction disagreement into the reward to provide uncertainty estimation and
limit model drift. However, itâ€™s been noted in [41] that simple averaging of logits often performs
comparably to more complex prediction aggregation functions that include uncertainty elements.
Another limitation is that, while WARM effectively reduces certain types of memorization, it does
not completely eradicate all forms of spurious correlations or biases inherent in the preference data.
For instance, if each individual RM predominantly relies on summary length as a criterion, WARM
is likely to replicate this tendency. Therefore, alternative methods (from the OOD generalization
literature?) might be required, for example those based on invariance regularization [51, 100] or
lastlayerretraining[122]. Finally,WARM onlyenhancesrewardmodelingwithouttacklingtheother
challenges in RLHF [18]; thus, to mitigate the safety risks [19, 34, 35] from misalignment [32, 33],
WARM must be considered within the larger context of responsible AI.
7. Conclusion
In conclusion, we introduce Weight Averaged Reward Models (WARM) to address two critical chal-
lengesinrewardmodeling: reliabilityunderdistributionshiftsandrobustnessunderlabelcorruption.
By averaging the weights of multiple RMs obtained from diverse fine-tunings, WARM appears as an
efficient solution to mitigate reward hacking in reinforcement learning from human feedback. Our
empirical results demonstrate its effectiveness when applied to summarization. We anticipate that
WARM will contribute to more aligned, transparent, and effective AI systems, encouraging further
exploration in reward modeling.
14WARM:OntheBenefitsofWeightAveragedRewardModels
References
[1] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. (p.1)
[2] OpenAI. Gpt-4 technical report. 2023. (p.1)
[3] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint, 2023. (p.1)
[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. 2018. (pp.1and3)
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. In NAACL, 2019. (p.1)
[6] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,
and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. (pp.1,7,and27)
[7] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR,
2022. (pp.1and3)
[8] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
AtharvaNaik,ArjunAshok,ArutSelvanDhanasekaran,AnjanaArunkumar,DavidStap,Eshaan
Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,
Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir
Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh
Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A,
Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via
declarative instructions on 1600+ NLP tasks. In ACL, 2022. (p.1)
[9] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B Hashimoto. Stanford Alpaca: An instruction-following LLaMA model,
2023. (p.1)
[10] PaulRoit,JohanFerret,LiorShani,RoeeAharoni,GeoffreyCideron,RobertDadashi,Matthieu
Geist,SertanGirgin,LÃ©onardHussenot,OrgadKeller,etal. Factuallyconsistentsummarization
via reinforcement learning with textual entailment feedback. In ACL, 2023. (p.1)
[11] Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned
reward functions. arXiv preprint, 2023. (p.1)
[12] PaulFChristiano,JanLeike,TomBrown,MiljanMartic,ShaneLegg,andDarioAmodei. Deep
reinforcement learning from human preferences. In NeurIPS, 2017. (pp.1,2,3,5,and27)
[13] DanielMZiegler,NisanStiennon,JeffreyWu,TomBBrown,AlecRadford,DarioAmodei,Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint, 2019. (pp.1and27)
15WARM:OntheBenefitsofWeightAveragedRewardModels
[14] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.
NeurIPS, 2020. (pp.1,7,and27)
[15] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul
Christiano. Recursively summarizing books with human feedback. arXiv preprint, 2021. (pp.1
and27)
[16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models.
arXiv preprint, 2023. (pp.1,5,12,and27)
[17] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.
In ICML, 2023. (pp.1,11,and33)
[18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, JÃ©rÃ©my Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. TMLR, 2023.
(pp.1and14)
[19] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan ManÃ©.
Concrete problems in AI safety. arXiv preprint, 2016. (pp.1,2,3,4,and14)
[20] Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. https://openai.com
/research/faulty-reward-functions, 2016. (pp.1and4)
[21] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy
Jones,NicholasJoseph,BenMann,NovaDasSarma,NelsonElhage,ZacHatfield-Dodds,Danny
Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a
laboratory for alignment. arXiv preprint, 2021. (pp.1and4)
[22] Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger.
Defining and characterizing reward gaming. In NeurIPS, 2022. (pp.1and4)
[23] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification:
Mapping and mitigating misaligned models. In ICLR, 2022. (pp.1and4)
[24] Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in rein-
forcement learning from human feedback. arXiv preprint, 2023. (p.1)
[25] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal?
end-to-end learning for negotiation dialogues. arXiv preprint, 2017. (p.1)
16WARM:OntheBenefitsofWeightAveragedRewardModels
[26] PrasannSinghal,TanyaGoyal,JiachengXu,andGregDurrett. Alongwaytogo: Investigating
length correlations in rlhf. arXiv preprint, 2023. (pp.1and11)
[27] Marilyn Strathern. Improving ratings: audit in the british university system. European Review,
1997. (p.1)
[28] Ethan Perez, Sam Ringer, KamilË™e LukoÅ¡iuÂ¯tË™e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig
Pettit,CatherineOlsson,SandipanKundu,SauravKadavath,etal. Discoveringlanguagemodel
behaviors with model-written evaluations. arXiv preprint, 2022. (p.1)
[29] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R
Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards
understanding sycophancy in language models. arXiv preprint, 2023. (p.1)
[30] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori
Hashimoto. Whose opinions do language models reflect? In ICML, 2023. (p.1)
[31] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conver-
sational ai: Converging evidence on chatgptâ€™s pro-environmental, left-libertarian orientation.
arXiv preprint, 2023. (p.1)
[32] Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for
advanced machine learning systems. Ethics of AI, 2016. (pp.1,3,and14)
[33] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep
learning perspective. arXiv preprint, 2022. (pp.1,3,14,and27)
[34] Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research. arXiv preprint, 2022.
(pp.1and14)
[35] Dan Hendrycks. Natural selection favors AIs over humans. arXiv preprint, 2023. (pp.1and14)
[36] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. NeurIPS, 2020.
(pp.2and4)
[37] Daniel Shin, Anca Dragan, and Daniel S. Brown. Benchmarks and algorithms for offline
preference-based reward learning. TMLR, 2023. (p.2)
[38] SamuelRBowman,JeeyoonHyun,EthanPerez,EdwinChen,CraigPettit,ScottHeiner,Kamile
Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable
oversight for large language models. arXiv preprint, 2022. (pp.2and4)
[39] Alexandre RamÃ©, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste
Gaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment
by interpolating weights fine-tuned on diverse rewards. In NeurIPS, 2023. (pp.2,14,and26)
[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models
to follow instructions with human feedback. NeurIPS, 2022. (pp.2,3,and27)
[41] JacobEisenstein,ChiragNagpal,AlekhAgarwal,AhmadBeirami,AlexDâ€™Amour,DJDvijotham,
AdamFisch,KatherineHeller,StephenPfohl,DeepakRamachandran,etal.Helpingorherding?
reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint, 2023.
(pp.2,4,5,11,12,14,and27)
17WARM:OntheBenefitsofWeightAveragedRewardModels
[42] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help
mitigate overoptimization. arXiv preprint, 2023. (pp.2,5,and27)
[43] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In NeurIPS, 2017. (pp.2and5)
[44] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear
mode connectivity and the lottery ticket hypothesis. In ICML, 2020. (pp.3,6,and26)
[45] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer
learning? In NeurIPS, 2020. (pp.3,6,and26)
[46] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-
Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and
Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves
accuracy without increasing inference time. In ICML, 2022. (pp.3,6,7,9,26,and28)
[47] Alexandre RamÃ©, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Galli-
nari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In
NeurIPS, 2022. (pp.3,5,6,7,8,9,26,and28)
[48] Alexandre RamÃ©, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, LÃ©on Bottou, and David Lopez-
Paz. Model Ratatouille: Recycling diverse models for out-of-distribution generalization. In
ICML, 2023. (pp.3,6,7,26,and28)
[49] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee,
and Sungrae Park. SWAD: Domain generalization by seeking flat minima. In NeurIPS, 2021.
(pp.3and26)
[50] Krikamol Muandet, David Balduzzi, and Bernhard SchÃ¶lkopf. Domain generalization via
invariant feature representation. In ICML, 2013. (pp.3and10)
[51] MartinArjovsky,LÃ©onBottou,IshaanGulrajani,andDavidLopez-Paz. Invariantriskminimiza-
tion. arXiv preprint, 2019. (pp.3,4,10,and14)
[52] Kerem Zaman, Leshem Choshen, and Shashank Srivastava. Fuse to forget: Bias reduction and
selective memorization through model fusion. arXiv preprint, 2023. (pp.3,14,and26)
[53] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and
Tong Zhang. Spurious feature diversification improves out-of-distributiongeneralization. In
ICLR, 2024. (pp.3,9,and26)
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. (pp.3and6)
[55] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von
Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the
opportunities and risks of foundation models. arXiv preprint, 2021. (pp.3and26)
[56] MaximeOquab,LeonBottou,IvanLaptev,andJosefSivic. Learningandtransferringmid-level
image representations using convolutional neural networks. In CVPR, 2014. (p.3)
[57] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:
Harmlessness from AI feedback. arXiv preprint, 2022. (pp.4and11)
18WARM:OntheBenefitsofWeightAveragedRewardModels
[58] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,
Colton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning
from human feedback with ai feedback. arXiv preprint, 2023. (pp.4,7,11,12,13,27,and28)
[59] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika, 1952. (p.4)
[60] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Reinforcement learning, 1992. (pp.4,12,and28)
[61] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint, 2017. (pp.4and12)
[62] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.
(p.4)
[63] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay
Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure
Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
WILDS: A benchmark of in-the-wild distribution shifts. In ICML, 2021. (p.4)
[64] MohammadPezeshki,SÃ©kou-OumarKaba,YoshuaBengio,AaronCourville,DoinaPrecup,and
Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In NeurIPS,
2020. (p.4)
[65] Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, and Moncef Gabbouj. Learning distinct
features helps, provably. arXiv preprint, 2021. (p.4)
[66] Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse ImageNet
models transfer better. arXiv preprint, 2022. (p.4)
[67] AlexanderDâ€™Amour,KatherineHeller,DanMoldovan,BenAdlam,BabakAlipanahi,AlexBeutel,
Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspeci-
fication presents challenges for credibility in modern machine learning. JMLR, 2020. (pp.4
and26)
[68] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. ID and OOD performance
are sometimes inversely correlated on real-world datasets. In NeurIPS Workshop, 2023. (p.4)
[69] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In ICML, 2017. (p.4)
[70] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your modelâ€™s uncertainty?
evaluating predictive uncertainty under dataset shift. In NeurIPS, 2019. (p.4)
[71] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain
generalization. In NeurIPS, 2021. (p.4)
[72] Herbert A Simon. Bounded rationality. Utility and probability, 1990. (p.4)
[73] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning,
rather than assuming, human biases for reward inference. In ICML, 2019. (p.4)
19WARM:OntheBenefitsofWeightAveragedRewardModels
[74] Timo Kaufmann, Sarah Ball, Jacob Beck, Eyke HÃ¼llermeier, and Frauke Kreuter. On the
challenges and practices of reinforcement learning from real human feedback. 2023. (p.4)
[75] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,
BenMann,EthanPerez,NicholasSchiefer,KamalNdousse,etal. Redteaminglanguagemodels
to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint, 2022. (p.4)
[76] Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk,
ZongyiLiu,FritzCremer,ValentinAssassi,Christie-CarolBeauchamp,etal. Rewardingchatbots
for real-world engagement with millions of users. arXiv preprint, 2023. (p.4)
[77] Condorcet. Essai sur lâ€™application de lâ€™analyse Ã  la probabilitÃ© des dÃ©cisions rendues Ã  la
pluralitÃ© des voix. 1785. (p.4)
[78] Silviu Pitis. Failure modes of learning reward models for llms and other sequence models. In
ICML, 2023. (p.4)
[79] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language modelsâ€™
sensitivity to spurious features in prompt design or: How i learned to start worrying about
prompt formatting. arXiv preprint, 2023. (p.4)
[80] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.
State of what art? a call for multi-prompt llm evaluation. arXiv preprint, 2023. (p.4)
[81] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, JosÃ© Miguel HernÃ¡ndez-Lobato, Richard E
Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation
models with kl-control. In ICML, 2017. (pp.5and12)
[82] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In ICML, 2019. (pp.5and12)
[83] AngelikiLazaridou,AnnaPotapenko,andOlivierTieleman. Multi-agentcommunicationmeets
natural language: Synergies between functional and structural language learning. In ACL,
2020. (p.5)
[84] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering
language drift with seeded iterated learning. In ICML, 2020. (p.5)
[85] Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human
objectives by evaluating hypothetical behavior. In ICML, 2020. (pp.5and27)
[86] William Saunders, Girish Sastry, Andreas StuhlmÃ¼ller, and Owain Evans. Trial without error:
Towards safe reinforcement learning via human intervention. In AAMAS, 2018. (p.5)
[87] Binghai Wang et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv
preprint, 2023. (pp.5,10,and27)
[88] Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss
functions. In ICML, 1996. (p.5)
[89] Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. In ICNN,
1996. (p.5)
[90] SandipanKundu,YuntaoBai,SauravKadavath,AmandaAskell,AndrewCallahan,AnnaChen,
Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general
principles for constitutional ai. arXiv preprint, 2023. (p.5)
20WARM:OntheBenefitsofWeightAveragedRewardModels
[91] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang.
Fine-tuning can distort pretrained features and underperform out-of-distribution. In ICLR,
2022. (pp.5,6,7,and28)
[92] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging
models modulo permutation symmetries. In ICLR, 2022. (p.6)
[93] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.
Knowledge is a region in weight space for fine-tuned language models. In EMNLP, 2023. (p.6)
[94] Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule
them all: Overlapping features of training methods. In ICLR, 2022. (pp.6and8)
[95] PavelIzmailov,DmitriiPodoprikhin,TimurGaripov,DmitryVetrov,andAndrewGordonWilson.
Averagingweightsleadstowideroptimaandbettergeneralization. InUAI,2018. (pp.7and26)
[96] DevanshArpit,HuanWang,YingboZhou,andCaimingXiong.Ensembleofaverages: Improving
model selection and boosting performance in domain generalization. In NeurIPS, 2021. (pp.7
and26)
[97] Michael VÃ¶lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to
learn automatic summarization. In ACL Workshop, 2017. (pp.7,8,and11)
[98] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.
arXiv preprint, 2023. (pp.7,11,12,27,28,and30)
[99] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain-of-Thought prompting elicits reasoning in large language models. In NeurIPS,
2022. (pp.7,11,and27)
[100] Alexandre RamÃ©, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances
for out-of-distribution generalization. In ICML, 2022. (pp.10and14)
[101] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-
fellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint, 2013. (p.10)
[102] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika
Chaudhuri. Adversarial robustness through local lipschitzness. arXiv preprint, 2020. (p.10)
[103] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new
neural network smoothness constraints. In NeurIPS ICBINB, 2020. (p.10)
[104] MatthiasHeinandMaksymAndriushchenko.Formalguaranteesontherobustnessofaclassifier
against adversarial manipulation. NeurIPS, 2017. (p.10)
[105] Jure SokoliÄ‡, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin
deep neural networks. IEEE Transactions on Signal Processing, 2017. (p.10)
[106] JeremyCohen,ElanRosenfeld,andZicoKolter.Certifiedadversarialrobustnessviarandomized
smoothing. In ICML, 2019. (p.10)
[107] RolandHafnerandMartinRiedmiller. Reinforcementlearninginfeedbackcontrol: Challenges
and benchmarks from technical process control. Machine learning, 2011. (p.10)
21WARM:OntheBenefitsofWeightAveragedRewardModels
[108] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov
decision processes. Machine Learning, 2015. (p.10)
[109] Lionel BlondÃ©, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame
off-policy generative adversarial imitation learning. Machine Learning, 2022. (p.10)
[110] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. arXiv preprint, 2023. (p.11)
[111] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. JMLR, 2020. (pp.11,12,30,and33)
[112] Jacob Hilton. KL divergence of max-of-n, 2023. (p.12)
[113] AhmadBeirami,AlekhAgarwal,JonathanBerant,AlexanderDâ€™Amour,JacobEisenstein,Chirag
Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment
policy. arXiv preprint, 2024. (p.12)
[114] Colin Raffel. Building Machine Learning Models Like Open Source Software. ACM, 2023.
(p.14)
[115] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and
Luke Zettlemoyer. Branch-Train-Merge: Embarrassingly parallel training of expert language
models. arXiv preprint, 2022. (p.14)
[116] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InAISTATS,2017.
(p.14)
[117] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A.
Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better
rewards for language model training. In NeuriPS, 2023. (p.14)
[118] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of
strong zero-shot models for continual learning. In NeurIPS Workshop, 2022. (pp.14and26)
[119] Steven Vander Eeckt et al. Weight averaging: A simple yet effective method to overcome
catastrophic forgetting in automatic speech recognition. arXiv preprint, 2022. (p.14)
[120] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
arXiv preprint, 2023. (pp.14and27)
[121] Maxime Labonne. NeuralBeagle14-7B. https://huggingface.co/mlabonne/NeuralBe
agle14-7B-GGUF, 2024. (pp.14and27)
[122] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is
sufficient for robustness to spurious correlations. In ICLR, 2023. (p.14)
[123] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In ICLR, 2019. (p.26)
22WARM:OntheBenefitsofWeightAveragedRewardModels
[124] John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and
Eric Karl Oermann. Variable generalization performance of a deep learning model to detect
pneumonia in chest radiographs: A cross-sectional study. PLOS Medicine, 2018. (p.26)
[125] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection
selects shortcuts over signal. Nature Machine Intelligence, 2021. (p.26)
[126] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi,
Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In CVPR,
2022. (p.26)
[127] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi,
Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by
interpolating weights. In NeurIPS, 2022. (p.26)
[128] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem
Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. In ACL,
2023. (p.26)
[129] NikolaosDimitriadis,PascalFrossard,andFranÃ§oisFleuret. Paretomanifoldlearning: Tackling
multiple tasks via ensembles of single-task models. arXiv preprint, 2022. (Notcited.)
[130] Mustafa Shukor, Corentin Dancette, Alexandre RamÃ©, and Matthieu Cord. Unival: Unified
model for image, video, audio and language. TMLR, 2023. (p.26)
[131] Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal. Seasoning model
soups for robustness to adversarial and natural distribution shifts. In CVPR, 2023. (p.26)
[132] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, JoÃ£o Sedoc, and Naomi Saphra. Linear
connectivity reveals generalization strategies. In ICLR, 2023. (p.26)
[133] Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov,
Pavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep
reinforcement learning with weight averaging. 2018. (p.26)
[134] Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for
online adaptation in reinforcement learning. In ICLR, 2022. (p.26)
[135] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for
forming multi-task policies. In ICLR RRL Workshop, 2023. (p.26)
[136] Michael Noukhovitch, Samuel Lavoie, Florian Strub, and Aaron Courville. Language model
alignment with elastic reset. In NeurIPS, 2023. (p.26)
[137] GabrielIlharco,MarcoTulioRibeiro,MitchellWortsman,SuchinGururangan,LudwigSchmidt,
Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In ICLR, 2023.
(p.26)
[138] Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. Elastic
weight removal for faithful and abstractive dialogue generation. arXiv preprint, 2023. (p.26)
[139] HwanjunSong,MinseokKim,DongminPark,YoojuShin,andJae-GilLee. Learningfromnoisy
labels with deep neural networks: A survey. TNNLS, 2022. (p.26)
[140] ChiyuanZhang,SamyBengio,MoritzHardt,BenjaminRecht,andOriolVinyals.Understanding
deep learning requires rethinking generalization. ICLR, 2017. (p.26)
23WARM:OntheBenefitsofWeightAveragedRewardModels
[141] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan
Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In
CVPR, 2019. (p.26)
[142] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami
Somepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al.
Neftune: Noisy embeddings improve instruction finetuning. arXiv preprint, 2023. (p.26)
[143] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise
for deep neural networks. In AAAI, 2017. (p.26)
[144] XiaoboXia,TongliangLiu,BoHan,MingmingGong,JunYu,GangNiu,andMasashiSugiyama.
Sampleselectionwithuncertaintyoflossesforlearningwithnoisylabels. InICLR,2022. (p.26)
[145] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning
data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.
(p.26)
[146] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.
NeurIPS, 2018. (p.26)
[147] MaryamSabzevari.Ensemblelearninginthepresenceofnoise.PhDthesis,UniversidadAutÃ³noma
de Madrid, 2019. (p.26)
[148] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In ICML,
2000. (p.27)
[149] W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca
Dragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and
mistaking it for reward. arXiv preprint, 2023. (p.27)
[150] Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning
from multiple teachers. arXiv preprint, 2023. (p.27)
[151] Sian Gooding and Hassan Mansoor. The impact of preference agreement in reinforcement
learning from human feedback: A case study in summarization. arXiv preprint, 2023. (p.27)
[152] Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-
augmented reward modeling. In ICLR, 2023. (p.27)
[153] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang
Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models
with factually augmented rlhf. arXiv preprint, 2023. (p.27)
[154] Anonymous. RIME: Robust preference-based reinforcement learning with noisy human prefer-
ences. In Submitted to ICLR, 2023. (p.27)
[155] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello,
Michal Valko, and RÃ©mi Munos. A general theoretical paradigm to understand learning from
human preferences. arXiv preprint, 2023. (p.27)
[156] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. In ICML, 2018. (pp.27and28)
24WARM:OntheBenefitsofWeightAveragedRewardModels
[157] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran,
JoshuaSusskind,andEtaiLittwin. Vanishinggradientsinreinforcementfinetuningoflanguage
models. arXiv preprint, 2023. (p.28)
25WARM:OntheBenefitsofWeightAveragedRewardModels
WARM: On the Benefits of Weight Averaged Reward Models
Supplementary material
This supplementary material is organized as follows:
â€¢ Appendix A enriches our related work.
â€¢ Appendix B clarifies some experimental details.
â€¢ Appendix C enriches our experiments.
A. Related work
This paper leverages the insights from the OOD generalization literature, in particular from linear
modeconnectivity(seeAppendixA.1),andappliesthemtothedesignofefficient,reliableandrobust
reward models (see Appendix A.2).
A.1. Out-of-distribution generalization, linear mode connectivity and memorization
LMC in fine-tuning. Fine-tuning foundation models [55] into specialized models that generalize
well to new distributions is critical for many real-world applications [123, 124, 125]. Recently,
different variants of weight averaging (WA) were able to improve performance, such as moving
average [49, 95, 96], WiSE fine-tuning [126], model soups [46], DiWA [47] and model ratatouille
[48]. These works rely on the LMC [44, 45] across fine-tuned weights, which was extended to fine-
tuningsondifferenttasks[48,127,128],modalities[130]orwithdifferentlosses[47,131],although
[132]highlightedsomelimitations. WAwasalsousedrecentlyinRLsetups[39,133,134,135,136],
in particular in RLHF in [39, 136] but only to combine policies and not rewards.
Insights into WA. Specifically, WA comes with several benefits. First, WA flattens the loss landscape
[49]. Second, WA approximates prediction ensembling, thus reduces variance of the estimator
[46, 47] and tackles model misspecification [67]. Third, WA combines modelsâ€™ abilities [137, 138],
which can be useful for multi-task [127], multi-objective [39] or in continual learning [118] setups.
Lastly, it has recently been shown that WA can provide some benefits under spurious correlations
[52, 53], with a phenomenon called FalseFalseTrue in [53]. These works [52, 53] share similarities
withourmemorizationexperimentsfromSection4.2,butwearethefirsttoanalyzeWAregularization
properties under label corruption, and their consequences on generalization. In contrast, in [52] the
networks are trained on different datasets while the theory in [53] is actually mostly developed for
prediction ensembling.
Memorization. Traditionalapproaches[139]tacklingmemorizationofcorruptedlabels[140]usually
require explicit regularization [141], specific data augmentation [142], loss adjustment [143] or
sample selection [144]. Some other strategies are based on ensembling: they filter out potentially
corrupted samples with self-labeling filtering [145, 146] or bagging diversity procedures [147]. As
far as we know, with WA we propose the first strategy combining multiple models trained on the
same dataset that manages to tackle corruption.
26WARM:OntheBenefitsofWeightAveragedRewardModels
A.2. Reward modeling
OneofthecentralchallengeinaligningLLMsistheabsenceofexplicitrewardsfromtheenvironment,
a.k.a. the outer alignment challenge [33]. While Inverse Reinforcement Learning [148] attempts
to derive a reward model (RM) from expert demonstrations, most recent efforts [12, 13, 14, 15,
40] primarily focus on learning from human preferences. Despite its importance to enhance LLM
performances post-RL and for safe deployment in real-world applications, how to best design RMs
has arguably receive less attention than it warrants. Some research [149] seeks to refine the loss
function from Equation (1). Other approaches are more data oriented: for example, LLaMA-2 [16]
involves continual learning of the RM to adjust to new generation distributions; [85, 150] follow an
active learning paradigm [151]. Augmenting rewards with tools [152] or additional information
[153] represents an even more recent and very promising trend. Limited efforts have been made at
the intersection of label corruption and reward modeling; [154] tried to filter the preference dataset
for small academic locomotion tasks, while the concurrent [87] suggests applying label smoothing
and flipping. Actually, reward ensembling is the most discussed method to mitigate reward hacking
[41, 42]; we show that WARM can beat ENS while removing its overheads. Finally, following DPO
[120], a recent trend merges reward modeling with policy learning; though, the policies still tend to
hack the preference data [155], and thus require only a few training steps and very small learning
rates. The WA of DPO policies, theoretically equivalent to the WA of RMs, is a promising research
direction with already significant empirical results on public benchmarks, as demonstrated in [121].
B. Implementation details
B.1. Dataset details
For summarization, we use the Reddit TL;DR dataset [14], containing posts from Reddit that have
been filtered to ensure high quality. The training summaries from [14] are generated by OpenAI
GPT-3 [6] variants. The dataset contains 123k posts, and âˆ¼5% is held out as the ID validation set.
To generate the candidate responses in the OOD dataset D with 92k pairwise comparisons, we
ğ‘œğ‘œğ‘‘
considered multiple PaLM-XS policies with high temperature, some of which are pre-trained only,
others SFT-ed and others RLHF-ed; the goal was to get a diverse set of summaries.
B.2. AI labeling details
While the ideal approach for evaluating our models would involve human preferences, we resort to
the cheaper AI labeling procedure from RLAIF [58]. We query an instruct fine-tuned PaLM-L [98]
LLM1, prompted to generate preference mimicking human preferences. Specifically, we follow the
â€œDetailed + CoT 0-shotâ€ prompting strategy from RLAIF [58], the best one according to their results,
involvingzero-shotpromptingwithchain-of-thought[99],amaximumdecodinglengthof512tokens
and temperatureğ‘‡ = 0.0 (i.e., greedy decoding). To avoid position bias, we run the AI labeler in the
twopossibleorderings. Thisstrategywasshowntoperformsimilarlytohumanlabellers,withsimilar
inter-agreement. For the corruption experiments, we swap the labels for 25% of the training samples.
B.3. Reward modeling details
The RMs are PaLM-XXS models [98]. They are first pre-trained, and then supervised fine-tuned on
the Reddit TL;DR dataset for 12k steps with a batch size of 128 and the Adafactor [156] optimizer
1AvailablethroughGoogleCloudâ€™sVertexAIhttps://cloud.google.com/vertex-ai/docs/generative-ai/
learn/models.
27WARM:OntheBenefitsofWeightAveragedRewardModels
with a learning rate of 10âˆ’5. Following the Baklava recipe, we actually launch the reward modeling
from different checkpoints along this SFT fine-tuning, at steps {8k, 10k, 12k}; taking a too-early
checkpoint would drastically reduce RM accuracy, as observed in [157]. To convert this LLM into a
classifier, we plug a linear probed [91] classification layer (the same for all RMs); said differently,
even though the featurizers are actually from different SFT checkpoints, they share the same linear
probed classification linear layer. As explained in [91], it prevents features from moving too much
away from their initializations, which facilitates the LMC required for WA.
We train all RMs for 10k steps, a batch size of 128, the Adafactor [156] optimizer, a learning rate
sampled in {1e-5,4e-5,1e-4}, and a dropout probability in {0.05, 0.1}. This follows the practical
recommandations from [47] to leverage hyperparameters in a mild range to preserve the LMC.
Training for a longer number of steps could help, as it did not alter the LMC in previous works [48].
Inpractice,forthemainexperimentswithcleanlabels,welaunch10rewardmodelings;whenranked
indecreasingaccuracyon D ğ‘œğ‘œğ‘‘,wedenotethem {ğœ™ ğ‘–}1 ğ‘–=0 1. Therefore,theRMsnamedğœ™ 1 andğœ™ 2 inthe
different plots are the two best according to their individual performances under distribution shifts.
Then, WARM ğ‘€ = 2 is actually the RM defined per ğœ™1+ğœ™2, while ENS ğ‘€ = 2 averages their predictions.
2
More generally, WARM with ğ‘€ weights is the WA of the ğ‘€ best weights {ğœ™ }ğ‘€ . The main motivation
ğ‘– ğ‘–=1
of this weight selection procedure is to remove potentially bad RMs, as validated in Figure 10, in
which we consider different permutations across those 10 RMs. As a side note, we speculate that a
greedy procedure as in [46] could further improve performances.
0.768
0.766
0.764
0.762
0.760
0.758
0.756 From best to worst
First best and then random
0.754 Random permutation v1
Random permutation v2
0.752 From worst to best
2 4 6 8 10
M
Figure10 | Analysisoftheweightselectionprocedure. Weplottheaccuracyresultingfromaveraging ğ‘€
weights(outof10),wheretheseweightsarechosenbasedonvariousselectionprocedures. Thiseffectively
validatesthatchoosingmodelsfrombesttoworstservesasareliableheuristic.
B.4. Reinforcement learning details
Both policy and value models are PaLM-XS [98], initialized from the same SFT model. We then
generate samples from the policy with temperatureğ‘‡ = 0.9, batch size of 128, the Adafactor [156]
optimizer, a learning rate of 10âˆ’5 and a policy warmup of 2k steps. We set ğ›¼ = 0.003 for the KL
regularization in the main experiment without label corruption, and ğ›¼ = 0.01 with label corruption.
Following [58], we used a modified version of REINFORCE [60] with a baseline value function for
variance reduction.
28
.ccA
DOOWARM:OntheBenefitsofWeightAveragedRewardModels
C. Additional experiments
C.1. 2nd order analysis: weight averaging for more robust ensembling
WA 0.40 0.50
0.46 ENS 0.50 0.48
0.45 Diag 0.49 0.38 0.46
0.48 0.36 0.44
0.44 0.47 0.34 00 .. 44 02
0.43 0.46 0.38
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
0.575 0.395
0.335 0.46
0.550 0.390
0.525 0.330 0.385 0.45
0.500 0.325 0.380 0.44
0.475 0.375
0.450 0.320 0.370 0.43
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Figure11 | Train(corrupt). MoreresultsenrichingFigure4(a)withdifferentpairsofRMs.
WA 0.8950
0.894 ENS 0.896 0.855 0.8925
0.892 Diag 0.850 0.8900 0.894 0.8875
0.890 0.845 0.8850
0.888 0.892 0.840 0.8825
0.886 0.890 0.8800
0.835
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
0.8575
0.896
0.905 0.8550 0.888
0.8525 0.886 0.894
0.900 0.8500 0.884 0.892
0.895 0.8475 0.882 0.890
0.8450 0.880
0.890 0.8425 0.878 0.888
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Figure12 | Train(clean). MoreresultsenrichingFigure4(b)withdifferentpairsofRMs.
00 .. 77 88 25 50 W ENA S 0.775 0.77 0.78
Diag 0.77
0.7800 0.76
0.770
0.7775 0.76
0.75
0.7750 0.765 0.75
0.7725 0.74
0.760 0.74
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
0.78 0.780 0.7850
0.77 0.778 0.790 0.7825
0.76 0.776 0.785 00 .. 77 78 70 50
0.774
0.75 0.7750
0.74 0.772 0.780 0.7725
0.770 0.7700
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Figure13 | Validation(ID).MoreresultsenrichingFigure4(c)withdifferentpairsofRMs.
0.725 0.725
0.716 W ENA S 0.712 0.720 0.720
0.714 Diag 0.710 0.715 0.715
0.710
0.712 0.708 0.710 0.705
0.706 0.705 0.700
0.710
0.704 0.700 0.695
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
0.715 0.718
0.710 0.726 0.726 0.716
0.705 0.724 0.724 0.714
0.700 0.722 0.722 0.712
0.695 0.720 0.720 0.710
0.690 0.718 0.718 0.708
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
Figure14 | Test(OOD).MoreresultsenrichingFigure4(d)withdifferentpairsofRMs.
29
.ccA
.ccA
.ccA
.ccA
.ccA
.ccA
.ccA
.ccAWARM:OntheBenefitsofWeightAveragedRewardModels
C.2. BoN experiments
555 ... 257 505 W W E I In nN d dA A S R R MM M 2 1 =M M 2= =6 2 45 .. 50 W W E I In nN d dA A S R R MM M 2 1 =M M 2= =6 2 67 W W E I In nN d dA A S R R MM M 2 1 =M M 2= =6 2 566 ... 505 W W E I In nN d dA A S R R MM M 2 1 =M M 2= =6 2
5.00 5.0
5 4.75 4.0 4.5
4.50 4.0
4
4.25 3.5 3.5
4.00 3.0
3.0 3
0.2 0.4 0.6 0.8 1.0 1.2 0.2 0.4 0.6 0.8 1.0 1.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6
KL:log(N) NN1 KL:log(N) NN1 KL:log(N) NN1 KL:log(N) NN1
(a)PaLM(clean). (b)PaLM(corrupt). (c)T5(clean). (d)T5(corrupt).
Figure15 | SameasFigure8, butwithabsolute values of the control reward for BoN experiments. We
considertwoSFTpoliciestogeneratecandidatesummaries: onebasedonPaLMarchitecture[98],theother
onT5architecture[111]. Inbothcases,weobservethatWARM performsbetterthanENSandtheindividual
networksintermsofpointwisecontrolRM.
WARM Baklava M=2 WARM Baklava M=2
ENS Baklava M=2 ENS Baklava M=2
0.06 Ind 3 Baklava 0.15 Ind 3 Baklava
Ind 1 Ind 1
0.04 0.10
0.02 0.05
0.00 0.00
0.2 0.4 0.6 0.8 1.0 1.2 0 1 2 3 4 5 6
KL:log(N) N N1 KL:log(N) N N1
(a)BaklavawithPaLM. (b)BaklavawithT5.
Figure16 | ControlrewardforBoNexperiments(cleansetup)withBaklavawhenthetwofine-tuningsğœ™ 1
andğœ™ havedifferentfeaturizerinitializations,collectedrespectivelyatsteps12kand8kfromasharedSFT.
3
30
drawer
lortnoC
niag
drawer
lortnoC
drawer
lortnoC
drawer
lortnoC
niag
drawer
lortnoC
drawer
lortnoCWARM:OntheBenefitsofWeightAveragedRewardModels
0.50 0.5
0.48 0.4
0.46
0.3
0.44
0.42 0.2 WARM M=6 WARM M=2 WARM M=2 ENS M=2 ENS M=2
0.40 Ind 2 Ind 2 Ind 1 0.1 Ind 1
y=0.5 y=0.5
0.38
0.2 0.4 0.6 0.8 1.0 1.2 0 1 2 3 4 5 6
KL:log(N) N N1 KL:log(N) N N1
(a)PaLM. (b)T5vs.WARMw/ğ‘ =1000.
Figure17 | OraclepreferencemetricforBoNexperiments(cleansetup). Figure17(a)confirmsFigure7(c)
butongenerationsfromPaLMSFT.Figure17(b)showswinratesforBoNonT5generationsforWARM with
ğ‘€ =6andalways ğ‘ =1000forBoNvs.otherRMswith1 â‰¤ ğ‘ â‰¤ 1000. WevalidatethatBoNlimitsreward
hackingcomparedtoRL,asperformancesgetbetterwhenincreasing ğ‘.
C.3. RL experiments
C.3.1. Experiments with corrupted preference dataset
7
6
5
WARM M=6
WARM M=2
ENS M=2
4 Ind 2
Ind 1
0 2000 4000 6000 8000 10000
# steps
Figure18 | RLexperiments. SameasFigure1(b)butwith25%corruptioninthepreferencedataset.
31
6=M
MRAW
.sv
oitar
niW
drawer
lortnoC
0001=N
htiw
6=M
MRAW
.sv oitar
niWWARM:OntheBenefitsofWeightAveragedRewardModels
C.3.2. Experiments with clean preference dataset
0.8
0.8
0.7
0.6
0.6
0.5
0.4
WARM M=10
0.4 WARM M=2
0.2 ENS M=2 =0.01
Ind 2 0.3 =0.003
Ind 1 =0.001
0.0 y=0.5 0.2 y=0.5
2000 3000 4000 5000 6000 2000 3000 4000 5000 6000
# steps # steps
(a)WARM ğ‘€=6. (b)Impactofğ›¼.
Figure19 | Oracle preference metric for RL experimentsatfixednumberoftrainingsteps(cleansetup).
Figure19(a)plotsthewinrateofthepolicywithWARM ğ‘€ =6vs.theotherpolicies,allatthesamenumberof
trainingsteps. Figure19(b)showsthewinrateofWARM ğ‘€ =6againstthepolicytrainedwithasingleRMğœ™
1
(thebestaccordingtoOODaccuracy)alongtrainingfordifferentvaluesofğ›¼controllingtheKLregularization
strength.
10
10
9
9
8
8
7
7
6 6
5 5
WARM M=6 WARM M=6
4 WARM M=2 4 WARM M=2
ENS M=2 ENS M=2
3 Ind 2 3 Ind 2
2 Ind 1 2 Ind 1
0 2500 5000 7500 10000 12500 15000 17500 0 100 200 300 400 500 600 700
# steps KL
(a)Controlrewardvs.trainingsteps. (b)Controlrewardvs.KL.
Figure20 | ControlrewardforRLexperimentswithğ›¼=0.01(cleansetup).
32
6=M
MRAW
.sv
oitar
niW
drawer
lortnoC
drawer
lortnoC
1
dnI
.sv
6=M
MRAW
fo
oitar
niWWARM:OntheBenefitsofWeightAveragedRewardModels
10.0 10.0
7.5 7.5
5.0 5.0
2.5 2.5
0.0 0.0
2.5 WARM M=6 2.5 WARM M=6
WARM M=2 WARM M=2
5.0 ENS M=2 5.0 ENS M=2
Ind 2 Ind 2
7.5 Ind 1 7.5 Ind 1
0 2000 4000 6000 8000 0 500 1000 1500 2000
# steps KL
(a)Controlrewardvs.trainingsteps. (b)Controlrewardvs.KL.
Figure21 | ControlrewardforRLexperimentswithğ›¼=0.001(cleansetup).
C.4. Distillation experiments
In Figure 22 we reproduce the distillation setup from [17], where the control PaLM-XS RM generates
the labels to train PaLM-XXS RMs. As a side note, we observed that distillation changes the diversity
across fine-tuned RMs, thus potentially altering the significance of the distillation setup, motivating
us in exploring the more realistic RLAIF setup.
0.125 WARM M=6
WARM M=2
0.100 ENS M=2
Ind 2
0.075
Ind 1
0.050
0.025
0.000
0.025
0.050
0.075
0 1 2 3 4 5 6
KL:log(N) N 1
N
Figure22 | BoN experiment in the distillation setup from [17]. Thelabelsinthepreferencedatasetare
givenbythecontrolRM,thesameRMwhichgivesthe ğ‘¦-axis. Thecandidatesummariesaregeneratedbya
SFTwiththeT5architecture[111]. ThebluelinesrepresentWARM with ğ‘€ weights: WARM performshigher
thantheindividualRMs(inyellows)orwhenensemblingtheirpredictions(ENSinred).
33
drawer
lortnoC
niag
drawer
lortnoC
drawer
lortnoC