Text Embedding Inversion Attacks on Multilingual Language Models
YiyiChen HeatherLent JohannesBjerva
DepartmentofComputerScience,AalborgUniversity,Denmark
{yiyic, hcle, jbjerva}@cs.aau.dk
Abstract be secure, recent work has demonstrated that ac-
cess to the embeddings is no more safe than raw
Representing textual information as real-
text, as models can learn to decode these embed-
numberedembeddingshasbecomethenormin
dings(SongandRaghunathan,2020;Morrisetal.,
NLP.Moreover,withtheriseofpublicinterest
2023; Zhou et al., 2023). As such, there is a sub-
inlargelanguagemodels(LLMs),Embeddings
asaService(EaaS)hasrapidlygainedtraction stantialthreattoprivacyifmaliciousactorsareable
asabusinessmodel. Thisisnotwithoutout- toeavesdroponcommunicationchannelsbetween
standing security risks, as previous research EaaSprovidorsandcustomers,andaccesstheem-
hasdemonstratedthatsensitivedatacanbere- beddingsintheprocess.
constructed from embeddings, even without
Decodingthecontentoftheseembeddingscan
knowledgeoftheunderlyingmodelthatgen-
bedoneviainversionattacks. Aftergainingaccess
erated them. However, such work is limited
byitssolefocusonEnglish,leavingallother toembeddingsandtheblack-boxembedderviathe
languagesvulnerabletoattacksbymalicious EaaSAPI,themaliciousactorcantrainanexternal
actors. Tothisend,thisworkinvestigatesLLM model,whichapproximatestheinversionfunction
security from the perspective of multilingual thatreconstructsthetextfromtheembeddings. Pre-
embeddinginversion.Concretely,wedefinethe
vious work has proven has demonstrated that an
problemofblack-boxmultilingualandcross-
exactmatchfordatarecreationcanbeobtainedin
lingualinversionattacks,withspecialattention
specificsettings,albeitwiththelimitationofassum-
to a cross-domain scenario. Our findings re-
ingmonolingualEnglishmodelsandembeddings
veal that multilingual models are potentially
morevulnerabletoinversionattacksthantheir (Morrisetal.,2023).
monolingual counterparts. This stems from Inareal-worldscenariohowever,aneavesdrop-
the reduced data requirements for achieving
permaynotnecessarilyknowthelanguageofthe
comparableinversionperformanceinsettings
textencodedwithintheembedding. Forinstance,
wheretheunderlyinglanguageisnotknowna-
aSpanishEaaSprovidermighthostitsdatainGer-
priori. Toourknowledge,thisworkisthefirst
many,foraFrench-speakingcompany. Thusinthis
todelveintomultilingualitywithinthecontext
ofinversionattacks,andourfindingshighlight work we investigate three research questions: (i)
theneedforfurtherinvestigationandenhanced To what extent are inversion attacks feasible in a
defensesintheareaofNLPSecurity. multilingualsetting?;(ii)Areattacksfeasibleand
effectivewhenthelanguageisunknowna-priori?;
1 Introduction
(iii)Doescross-lingualtransferallowinformation
Industrial applications of Natural Language Pro- to be leaked across the languages included in a
cessing (NLP) typically utilize Large Language multilingualmodel?
Models (LLMs) and frequently rely on vector
databasesviaframeworkssuchasEmbeddingsasa Contributions Inthiswork,wedefinetheprob-
Service(EaaS).Inthiscontext,ratherthanstoring lem of black-box multilingual and cross-lingual
dataasstrings,highqualitysentenceembeddings inversionattacks,withspecialattentiontoacross-
arestoredinaremotedatabaseinstead. Thisallows domainscenario. Whilepreviousresearchhassuc-
end-users to efficiently search across these con- ceeded in reconstruction of tokens with bag-of-
densed representations, which are seemingly im- words approach (Song and Raghunathan, 2020)
pervioustoprivacybreaches. However,whilesuch and sequences with informative words (Li et al.,
EaaSworkflowshavepreviouslybeenassumedto 2023),Morrisetal.(2023)hasproventhepotential
4202
naJ
22
]LC.sc[
1v29121.1042:viXraand effectiveness of embedding inversion to the make assumptions about the imagined attacker‚Äôs
extentofexacttextreconstructioninEnglish. We levels of access to the victim model. White-box
approachmultilingualinversionbyextendingthe scenariosassumeattackeraccesstothefullmodel
methodologyintroducedbyMorrisetal.(2023)to (Wallace et al., 2019; Tsymboi et al., 2023), re-
amultilingualsettingoverEnglish,French,Span- sultinginmanypossibleattacksurfaces. Previous
ish,andGerman. worksinNLPhaveshownthatitispossibletore-
Inthisstudy,wearethusthefirsttoinvestigate trievesensitivetrainingdatabyattackingmodels
multilingualinversionattacksandpotentialofex- directly(Fredriksonetal.,2014,2015),attacking
acttextualreconstructioninamultilingualsetting, gradients(Zhuetal.,2019;Dengetal.,2021),as
inparticularwhenthelanguageofatargetembed- wellasthroughleveragingleakedhiddenstates(Li
dingisunknown. Concretely,weexperimentusing etal.,2022). Meanwhile,black-boxattacksassume
astate-of-the-artmultilingualblack-boxencoder, an attacker has no knowledge of the underlying
wherethetrainedmultilingualinversionmodelre- modelitself,andcanonlyinteractwithmodelsat
constructs texts in certain languages outperform- the most abstracted level (e.g., provide input and
ing their monolingual counterpart without extra registeroutputthroughanAPI).Forexample,Car-
stepsofcorrections. Furthermore,weconductex- linietal.(2020)areabletoextractsensitivetraining
perimentsoncross-lingualandcross-domaintext data(e.g.,namesandphonenumbers)fromGPT-2
reconstruction,andproposeastraightforwardAd (Radfordetal.,2019),byfirstgeneratingdatafrom
hoc Translation method to counteract the short- the model and then using membership inference
comingsofthecurrentstandardofstring-matching attacks to filter utterances likely to be part of the
metricsinthisspecificsetting. originaltrainingdata.
Finally,weopen-sourceallofourtrainedinver-
sionmodelsduetothecomputationalcostoftheir In the case of embedding inversion attacks,
training.1 Whileopensourcingourmodelcomes whereby an imagined attacker aims to recreate
withsuchriskasprovidingmodelstoattackers,the thetextencodedbythedistributedrepresentation,
underlyingattackmechanismpresentedinthispa- SongandRaghunathan(2020)firstdemonstrated
perisalreadyestablished,togetherwithadefense that 50%‚Äì70% percent of tokens could be recov-
in(Morrisetal.,2023). Webelievethesemodels eredfromanembedding. Sincethen,thesuccessof
will be useful to the research community, allow- subsequentattackshasonlyimproved,withnewer
ing for the development of multilingual defense approachesnowabletoretrieveentiresentencesof
mechanisms,withoutneedingtospendresourceon encodedtext(H√∂hmannetal.,2021;Hayetetal.,
trainingthemodelswepresent. 2022;Morriset al.,2023;Liet al.,2023). Mean-
while,thedevelopmentofcounter-measurestoem-
2 RelatedWork bedding inversion attacks is an area of ongoing
investigation. For example, Zhou et al. (2023)
Modelsarewellknowntomemorizetrainingdata,
proposeadefensemethodwhichmakesrandomly
andarethereforesusceptibletoleakingprivatein-
perturbed embeddings after an initial clustering
formation(Shokrietal.,2016;Carlinietal.,2018;
step, such that the embeddings are still semanti-
Nasr et al., 2019). As such, there is increased re-
callymeaningfulandusefulfordownstreamtasks,
search interest in exploring this vulnerability to
whileremainingresistantagainstinversionattacks.
inversion attacks from the perspective of cyber-
Parameter-efficientfine-tuninghasalsobeenfound
security,simulatingattacksagainstmodelstorecre-
toprotectmodelsagainstwhite-box(viagradients)
ate sensitive training data. Work in this direc-
inversionattacksinthesettingoffederatedlearn-
tionhasbeenconductedacrossvariousdomainsof
ing (Zhang et al., 2023). Beyond direct defenses
machinelearning,suchascomputationalgenetics
against inversion attacks, other methods for pro-
(Fredriksonetal.,2014),computervision(Fredrik-
ducingmoresecureembeddingshavemadeuseof
sonetal.,2015),andmorerecentlyNLP(Songand
existing encryption methods (Huang et al., 2020;
Raghunathan,2020). Generally,suchworksatthe
XieandHong,2021)aswellasdifferentialprivacy
intersectionofmachinelearningandcyber-security
(Lyu et al., 2020). However, until privacy can be
(e.g., on inversion attacks or adversarial attacks)
guaranteedforembeddings,inversionattackswill
continuetoposeathreat,andthusrequirecontin-
1The trained inversion models are available at: https:
//huggingface.co/yiyic uedinvestigation.Finally,toourknowledge,previousworksinem- dialoguesystems(Moslemetal.,2022;Sunetal.,
beddinginversionareallconductedinamonolin- 2022). Inourwork,weapproachtheinversionat-
gualsettingoverEnglish(SongandRaghunathan, tacksinthecontextoftextgeneration. Inthiscase,
2020;Lyuetal.,2020;Hayetetal.,2022;Parikh agenerationmodelœà determineshowmuchinfor-
etal.,2022;Kimetal.,2022;Morrisetal.,2023; mationcanbeencodedanddecoded,anddownthe
Zhouetal.,2023;Lietal.,2023). Thisisasignifi- line,howwelltextcanbeconstructed. Forexam-
cantshortcoming,asitrisksleavingdefencesfor ple,ifœà issolelypre-trainedonLatinscript,then
non-Englishlanguagesunexplored,withimplica- SanskritorCyrillicdatacannotbeencodedorde-
tions for LLM Security for all languages. For in- coded. Hence,itisnotfeasibletoreconstructtext
stance,thiscouldleadtoimplementationsofLLMs in unknown scripts, and it is unexplored whether
innon-Englishbeingconsiderablylesssecurethan text in unknown languages can be reconstructed.
theirEnglishcounterparts. Inthisstudy,weinvestigatetextreconstructionin
unknownlanguagesbutinthesamescript. More
3 Methodology
specifically,howwellagenerationmodelcangen-
eralizeacrosslanguagesinthesamescriptisalsoa
Text embeddings can be generated through the
determinantfactorforinversionattacks.
encoding of text using a language model, or
Moreover, to recover the text from œï(x), it is
throughdedicatedtextvectorizationtechniqueslike
implicitthatthedataspacetowhichxbelongsis
Word2Vec(Mikolovetal.,2013)orGLoVE(Pen-
unknown. In practice, to build an attacker model
ningtonetal.,2014). AsEaaStypicallydealswith
basedontheeavesdroppedembeddings,atraining
embeddingsofsentencesorphrases,thisrequires
datasetDisusedsothattheattackerdirectlylearns
ustoexplorethisrelativelymorechallengingset-
œà from pairs (œï(y),y), where y ‚àà D. Moreover,
ting,asopposedtoword-levelembeddings. Inthis
D has a strong impact on inversion performance.
work, we consider a black-box embedding inver-
Inreality,theinversionattacksareessentiallycross-
sion attack scenario. To exemplify such attacks,
domainproblem,sinceD mostlikelydonotrepre-
envisionmaliciousactorseavesdroppingoncom-
sentthedataspaceofx.
municationchannelsbetweenEaaSprovidersand
customers,ultimatelygainingtheaccesstotheem-
Multilingual Inversion Attacks Compared to
beddingsduringtheprocess. Whilepreviouswork
monolingual embedding inversion, to investigate
hasassumedthatthisentireprocesscanbeassumed
thepotentialandeffectsofmultilingualinversion
totakeplaceinEnglish,wehereconsiderthecon-
attacks,thecomplexityofexperimentationscales
siderablymoredifficultsettingwheretheunderly-
uprapidly,aseachlanguagespaceofœà,œï,xand
inglanguageisunknown. Thatis,wespecifically
D plays a vital role. For example, defining the
aimatmultilingualembeddinginversion.
investigatedlanguagesasasetL = {l ,l ,...l }.
1 2 n
3.1 Black-boxEmbeddingInversion Thescaleoftrainingattackermodelsmultipliesby
languagesandothercontrolledparameters,suchas
Toformalizetheattackscenario,assumethatasen-
maximal sequence length for text generation (cf.
sitivetextsequencexandablack-boxencoderœï
Section4). Moreover,thecomplexityofanexhaus-
aregiven. Theembeddinginversionattackisthen
tivemultilingualcross-evaluationhasacomplexity
definedasusinganexternalattackermodelœà tore-
of O(|L|), since each monolingual and multilin-
covertextualinputxfromtheembeddingobtained
gualmodelshouldbeevaluatedonalllanguages.
viaœï(x). However,thearchitectureandparameters
Weinvestigatethepotentialofmultilingualem-
ofœïarebothinaccessible,wecansolelyaccessœï
bedding inversion under the assumption that we
viaanEaaSAPI.Theattackermodelœà isbuiltto
can directly send almost unlimited queries to the
learntheinversemappingœï‚àí1,whichweformulate
black-box œï, and obtain embeddings œï(y) for
asanapproximationproblem(Lietal.,2023):
y ‚àà D. Following the same approximation ap-
proach from Morris et al. (2023), assuming that
œà(œï(x)) ‚âà œï‚àí1(œï(x)) = x (1)
e = œï(y), the search for text yÀÜ with the embed-
Textgenerationmodelshaveproventobeeffec- dingsthatclosesttothetargetembeddingeunder
tiveingeneratingcontextuallycoherenttexts(Shen œïisoptimizedbythefollowingformula,utilizing
etal.,2019),theyhaveawiderangeofapplications, Cosinesimilaritytomeasurethesemanticsimilar-
such as machine translation, summarization and ityintheembeddingspace:Vec2Text
ùë¶%(")
Target text ùë¶ Correction ùë¶"(#)
Trump once asked then- Trump einmal fragte damals
acting FBI director Andrew ùùì ùëí ùùã FBI Director Andrew Mccabe ùùì ùëíÃÇ("#$)
Mccabe about his 2016-vote w√§hrend seiner 2016-Vote
ùëíÃÇ(")
AdTrans(ùë¶")
Trump once asked then-FBI
Director Andrew Mccabe
during his 2016-vote Ad hoc Translation
Figure1: Overviewofthemethod,Vec2Text(Morrisetal.,2023)plusAdhocTranslation. Thetextsareexamples
ofcrosslingualtextreconstructionevaluation. EnglishtextisevaluatedontheinversionmodeltrainedonGerman
texts. Assumingaccesstoatargetembeddinge(blue),andqueryaccesstotheembedderœï(bluemodel)viaEaaS
API,theinversionmodelœà (orange)aimstoiterativelygeneratehypothesiseÀÜ(pink)toreachthetarget. During
cross-lingualevaluationonEnglishtextwithinversionmodeltrainedonGermandata,thegeneratedtextyÀÜisin
German,andtranslatedtoEnglish(AdTrans(yÀÜ)),tobecomparedwiththeinputy. Exampleinputisfromtestdata
inMTG-EN.
text in l . To investigate this aspect, as shown in
x
yÀÜ= argmaxcos(œï(y),e) (2) Fig. 1, we propose a post-inversion strategy, i.e.,
y
AdhocTranslation(AdTrans),wherethegenerated
Specifically,asshowninFig.1,following (Mor- textistranslatedfroml inl ,furtherthetranslated
y x
risetal.,2023), theinversionmodeltrainingand text is evaluated against the target text, to verify
inference is conditioned on the previous output, whethertheinvertedtextinl leakinformationof
y
that at correction step t+1, the model takes the thetargettextinl (cf. Section5.3).
x
concatenationofpreviousoutputyÀÜ(t) andhypothe- Previousresearchhassolelyfocusedoninverting
sisembeddingeÀÜ(t),plustargetembeddinge.
embeddings for English texts, taking for granted
We assume that the attacker (1) has access to theknowledgeoflanguageoftheinputtextfrom
the black-box œï via EaaS API and also (2) has targetembedding. Ourstudyexpandsonthisline
theknowledgeofthelanguagescriptoftheinput of research by expanding the language space of
textforthetargetembeddings. Thenmultilingual eachessentialcomponents,withoutassumingprior
embedding inversion attack is composed of the knowledgeofthelanguages.
following steps: (a) build an attacker model œà,
basedonatextgenerationmodel,pre-trainedonthe 4 ExperimentalSetup
samelanguagescripts,ideallythesamelanguage;
(b)[AttackModelTraining]trainœà byiteratively
queryingtheblack-boxembeddingmodelœïwith MultilingualEmbeddings WeleverageT5-base
texty ‚àà D,andresultinginyÀÜoptimizedwithEq.2 (Raffeletal.,2023)asourgenerationmodel, fol-
(correction step 1); (c) [Inference] having tested lowing Morris et al. (2023). We train the mul-
soundgeneralizabilityofœà,theembeddingsœï(x) tilingual inversion models œà on a state-of-the-
canbeinverted,andreconstructtextx,andfurther art multilingual encoder œï: multilingual-e5-base
stepsofoptimizations(correctionsteps>1)with (ME5-base)2 (Wang et al., 2022), a pre-trained
Eq.2andimplementbeamsearchatthesequence transformer based on XLM-R based (Conneau
levelwhereanewgenerationistakenonlywhenit etal.,2020),viaweakly-supervisedcontrastivepre-
isclosertothetargetembeddingscomparedtothe trainingonamixtureofmultilingualdatasets. The
previousstep. modelischosenasitisoneofthebestperforming
multilingualmodelsontheMTEBtextembeddings
AdhocTranslation Withoutpriorknowledgeof benchmark (Muennighoff et al., 2023). Further-
thelanguagel xoftargettext,thelanguagel y ofthe more,wealsoreproducetheresultsfrom(Morris
trainingdatasetD canbedifferentfroml x,which etal.,2023)bytraininginversionmodelsonGTR-
mayresultinthetrainedinvertermodeldecoding baseusingEnglishdatasets,asourbaselines.
textsonlyinl . However,thegeneratedtext,albeit
y
inl y,canconveythesameinformationasthetarget 2transformers:intfloat/multilingual-e5-baseDatasets Previousresearch(Morrisetal.,2023) the true embedding and the embedding of the re-
trains text inversion models on natural ques- constructedtextintheembeddingspaceoftrained
tions and question-answer pairs, such as MS- œï. Such metrics fall short in terms of evaluating
Marco(Bajajetal.,2018)andNaturalQuestions whetherthesemanticcontent,e.g.,specificprivate
(NQ) (Kwiatkowski et al., 2019) datasets. While information,isrecovered. Thelimitationisparticu-
thesedatasetsarelarge,theyarelimitedtoEnglish. larlyevidentincross-lingualsettings,forexample,
Thusforourexperiments,wetrainandevaluatethe wherethegeneratedGermantextconveyssimilar
multilingualinversionmodelsontheMTGdataset, meaning as the input English text, a nuance that
abenchmarksuitespecificformultilingualtextgen- word-matchmetricsfailtocapture(cf. Fig 1)
erationtrainingandevaluation(Chenetal.,2022),
Experiments Following the setup from (Mor-
withparallelexamplesacrossalllanguages. MTG
ris et al., 2023), there are two stages of model
iscuratedfromdifferentdomains,includingnews,
trainingforembeddinginversion: (1)Baseinver-
daily life and Wikipedia. In order to ensure the
sion model, learning text distributions given em-
validityofourexperiments,andtestgeneralizabil-
beddings,(2)Vec2Textcorrectormodel,initialized
ity, we exclude the data curated from Wikipedia,
with the trained Base model and training using
sincethisdomaindataisusedtotrainboththeT5-
Eq. 2. To evaluate the potential of multilingual
base and ME5-base models. For each language,
andcross-lingualembeddinginversionattacks,we
thisresultsin123kpassagestobeusedastraining
trainBasemodelsandVec2Textmodelsforeach
data. Passagesrefertoparagraphsorsectionsofa
language and MTG-MULTI, and evaluate exten-
document. Weobtain3-5Msentencesineachlan-
sivelyinmultilingualsettings. Incomparisonwith
guagefortrainingdatainMTGusingNLTK(Bird
previous research, we train and evaluate English
and Loper, 2004) sentence tokenization. This is
inversionmodelsonNQandMTG-EN.
considerablyfewersamplesascomparedto (Mor-
The Adam optimizer with a learning rate of
risetal.,2023),inwhichtheGTR-basemodelis
trainedon5MpassagesfromNQ.3 Meanwhile,we 0.001 with 625 warmup steps is used. We train
each base model and corrector model for 100
trainandevaluateondatainEnglish,French,Ger-
epochseach. Weuseabatchsizeof512forinver-
man and Spanish, noted as MTG-EN, MTG-FR,
sionmodelsand256forcorrectormodelstrained
MTG-DE, and MTG-ES, respectively. We also
on data with 32 tokens, while the batch sizes are
compose a 5M-sentence multilingual dataset in-
halvedformodelstrainedondatatruncatedto64
cluding1.2Msentencesfromeachlanguage,noted
tokens, accordingly. All models are trained on 4
asMTG-MULTI.
AMDMI250GPUswithdistributedtraining.4 Un-
EvaluationMetrics Wemeasuremodelperfor- derthesecircumstances,trainingourslowestmodel
mance using two types of metrics, to compare takesabout8days.
with the results from previous research (Morris
et al., 2023). First, for evaluating text recon- 5 ResultsandAnalysis
struction, word-match metrics are used: BLEU
5.1 MonolingualEnglishTextReconstruction
score (Post, 2018), where n-gram similarities be-
tweenthetrueandreconstructedtextaremeasured; In-Domain Tohaveaproofofconcept,werepli-
ROUGEscore(Lin,2004),whererecallofoverlap- catetheexperimentfromMorrisetal.(2023),by
pingwordsofreconstructedtextisreported;Token training inversion models using GTR-base and
F1,themulti-classF1scoresbetweenthesetofpre- ME5-base as embedders on the NQ dataset. The
dictedtokensandthesetoftruetokens,considering Base and Vec2Text model with 1 correction step
eachwordasaclass;Exact-match,thepercentage trainedonME5-basehasaperformanceonparwith
ofreconstructedtextsmatchingperfectlythetrue GTR-base. Moreover,thetextembeddingstrained
texts. Additionally,thecosinesimilaritybetween onME5-basearemorecloserinembeddingspace
than embeddings trained on GTR-base, i.e., with
3Themodelstruncatetextsinto32tokensand64tokens,
higher cosine similarities. However, with more
toevaluatehowsequencelengthaffectstheperformanceof
steps of correction and beam search, the perfor-
embeddingsinversion. EachpassageinNQissignificantly
longerthan32and64tokens. Toobtainmoretrainingdata mance is boosted to 0.9244 in BELU score with
samplesfromMTGdataset,weimplementNLTKsentence
tokenizationonMTGdataset,whichresultsinabout3-5M 4Distributed Training with Accelerate: https://
sentencesforeachlanguage. huggingface.co/docs/transformers/accelerate#Tokens #PredTok. BLEU ROUGE TF1 Exact COS
GTR ME5 GTR ME5 GTR ME5 GTR ME5 GTR ME5 GTR ME5 GTR ME5
Base(0Steps) 32 32 32 32 0.2718 0.2877 0.6286 0.6368 63.74 65.9 0.4 0.4 0.8793 0.9738
Vec2Text(1Step) 32 31 32 32 0.4862 0.4792 0.7839 0.7703 78.44 78.35 8 4.8 0.9210 0.9588
(20Steps) 32 32 32 32 0.8330 0.7447 0.9512 0.8957 95.11 90.3 58 21.8 0.9862 0.9920
(20Steps) 32 32 32 32 0.8431 0.7503 0.9549 0.8976 95.6 90.56 58.4 21.8 0.9862 0.9920
(50Steps+4sbeam) 32 32 32 32 0.9018 0.7887 0.9726 0.9111 97.15 91.55 74.4 32.6 0.9853 0.9902
(50Steps+8sbeam) 32 32 32 32 0.9244 0.8086 0.9776 0.9189 97.78 92.42 82 35 0.9921 0.9926
(100Steps) 32 32 32 32 0.9245 0.8082 0.9775 0.9183 97.79 92.37 82 35 0.9921 0.9926
(100Steps+4sbeam) 32 32 32 32 0.9017 0.7882 0.9725 0.9111 97.15 91.53 74.4 32.8 0.9824 0.9902
(100Steps+8sbeam) 32 32 32 32 0.9245 0.8082 0.9775 0.9183 97.79 92.37 82 35 0.9921 0.9926
Table1: EvaluationofEnglishTextReconstruction. Thebestperformancesforeachmodelreachedintheearliest
stagesareinbold. TheunderlinedresultsarewhereME5-basemodeloutperformsGTR-basemodel.
82%exactmatchforGTR-basemodel,whilethe English text reconstruction, with solely training
bestperformanceforME5-baseis0.8086inBLEU thefirststageofinversionmodel,themultilingual
scorewith35%exactmatch. Theperformancedif- embeddings model yields better word-matching
ferencecouldbeduetothefactthattheGTR-base performancesandtheembeddingsarecloserinthe
ist5-basedmodel,thesamestructureasthegener- embeddingspace. However,theadaptedapproxi-
ationmodelœà. However,utilizingME5-basesets mation approach Eq. 2 boosts performance more
upamorerealisticattackscenarioofblack-boxem- onmonolingualembeddingmodel.
beddinginversion,asthestructureoftheembedder
5.2 MultilingualTextReconstruction
œïisunknown.
NQ‚ÜíMTG-EN MTG-EN‚ÜíNQ MTG-MULTI‚ÜíNQ ME5_MTG-EN BLEU ME5_MTG-EN Runtime
GTR ME5_MTG-FR BLEU ME5_MTG-FR Runtime
Base 0.0581(0.7334) - - ME5_MTG-DE BLEU ME5_MTG-DE Runtime
ME5_MTG-ES BLEU ME5_MTG-ES Runtime
Vec2Text 0.3623(0.9767)
100
ME5
Base 0.0589(0.9272) 0.0715(0.9511) 0.0671(0.9553) 20k
Vec2Text 0.2119(0.9440) 0.1535(0.9669) 0.1079(0.9708)
80
Table 2: Cross-Domain English Text Reconstruction
15k
Evaluation,BLEUscoresandCOSarereported. Hori- s)
d
Vz oo
n
en ctt 2wa Tl
o
ec
e
xo
m
tm mbp ea odr ddis eeo
lr
sn
s
ato
r
rn
a
eiM
n ee
vE
d
a5
lo
u-
n
aB tta ehs dee bsm
a
ymo 5d
e
0el Ns s,
tQ
ea pn
d
sd
at
ov
a
fe ser ct
t
oi .c rTa rel hl cy
e
- EU
Score 60
10k ME
(secon
tion with sequence beam search width 8. The arrow
BL
40
NTI
U
R
‚Üíindicatesthecross-domainevaluationdirection. For
5k
example, NQ‚Üí MTG-EN indicates that the model is
20
trainedonNQandevaluatedonMTG-EN.
0
Cross-Domain Toevaluatetheperformanceof 0
e dm atb ase ed tdi in ng Ei nn gv le ir ss hi ,on tha ett mac ok ds elo sn tro au int- eo df-d oo nm Nai Qn 1 Step 20 Steps50 Steps50 Steps 5 +0 4 S ste bp es a m1 +0 80 sS bte ep as m100 Steps1 0 +0 4 S ste bp es a m+ 8 sbeam
andMTG-ENdatasetsarecross-evaluatedonboth
Figure2: BLEUscorevs. RuntimebyEvaluationfor
datasets, respectively, as shown in Table 2. The
InversionModelsinEnglish,French,GermanandSpan-
results on MTG-EN are similar in BLEU scores ish.
for both Base models trained on GTR-Base and
ME5-Base, while GTR model outperforms ME5 Toexplorethepotentialofmultilingualembed-
bymorethan0.15inBLEUscores,andthecosine ding inversion, we train ME5-base embedder on
similarity of reconstructed and true text embed- MTG datasets in English, German and French,
dings are boosted by over 0.24 . In comparison, Spanish, noted as ME5_MTG-EN, ME5_MTG-FR,
thecosinesimilarityforME5modelsarenotmuch ME5_MTG-DE and ME5_MTG-ES, respectively,
variedandconstantlyhigh(‚â• 0.92)acrossstages andthecomposedmultilingualdatasetofallfour
of evaluations and across domains. From the ob- languages,notedasME5_MTG-MULTI,andtested
servations of both in-domain and out-of-domain on each language for both experimental settings.#Tokens #PredTok. BLEU ROUGE TF1 Exact COS
MONO MULTI MONO MULTI MONO MULTI MONO MULTI MONO MULTI MONO MULTI MONO MULTI
MTG-EN
Base(0Steps) 32 32 31.94 31.95 0.1157 0.1079 0.4598 0.4439 44.97 43.71 0 0 0.9381 0.9215
Vec2Text(1Step) 32 32 31.95 31.96 0.183 0.1338 0.5874 0.4895 56.37 48.22 0.4 0.2 0.9236 0.8637
(20Steps) 32 32 31.99 31.98 0.4148 0.2372 0.7905 0.6253 75.15 59.74 8.8 3 0.9441 0.8433
(50Steps) 32 32 31.99 31.97 0.4305 0.2527 0.802 0.6414 76.29 61.39 9.4 3.2 0.9464 0.9296
(50Steps+4sbeam) 32 32 31.99 31.98 0.4587 0.2989 0.827 0.6817 78.24 65.27 10.8 5 0.9372 0.9487
(50Steps+8sbeam) 32 32 31.98 31.98 0.4849 0.3204 0.8351 0.6938 79.16 66.67 12 7.4 0.9277 0.9303
(100Steps) 32 - 31.98 - 0.4853 - 0.8351 - 79.12 - 12 - 0.9277 -
(100Steps+4sbeam) 32 - 31.99 - 0.459 - 0.8271 - 78.24 - 10.8 - 0.9372 -
(100Steps+8sbeam) 32 - 31.98 - 0.4853 - 0.8351 - 79.12 - 12 - 0.9277 -
MTG-FR
Base[0Steps] 32 32 32 32 0.1864 0.1981 0.5286 0.552 52.93 55.68 0 0.2 0.9408 0.9511
Vec2Text(1Step) 32 32 32 31.98 0.291 0.2832 0.6358 0.6308 63.36 63.1 2.6 2 0.9655 0.9271
(20Steps) 32 32 31.98 32 0.6239 0.5878 0.8412 0.8132 83.48 81.02 36 32 0.9752 0.9492
(50Steps) 32 32 31.98 32 0.6404 0.6075 0.8518 0.8301 84.51 82.49 36.8 33 0.9754 0.9252
(50Steps+4sbeam) 32 32 32 32 0.7196 0.6872 0.8829 0.867 87.91 86.22 50.4 45.2 0.9643 0.942
(50Steps+8sbeam) 32 32 32 32 0.7454 0.73 0.8912 0.8938 88.83 88.84 54.4 49.6 0.9757 0.942
(100Steps) 32 - 32 - 0.7444 - 0.891 - 88.77 - 54.4 - 0.9757 -
(100Steps+4sbeam) 32 - 32 - 0.7193 - 0.8826 - 87.89 - 50.4 - 0.9643 -
(100Steps+8sbeam) 32 - 32 - 0.7444 - 0.891 - 88.77 - 54.4 - 0.9757 -
MTG-DE
Base(0Steps) 32 32 32 31.98 0.133 0.137 0.4313 0.4524 44.6 46.14 0 0 0.9599 0.9642
Vec2Text(1step) 32 32 31.93 31.98 0.22 0.1808 0.5555 0.5195 56 52.07 1.2 0.2 0.9699 0.9516
(20Steps) 32 32 31.95 32 0.566 0.4137 0.8095 0.7041 79.84 69.81 30.2 16.6 0.9573 0.9232
(50Steps) 32 32 31.95 32 0.5736 0.4359 0.8233 0.7228 81.4 71.54 30.4 17.4 0.9687 0.9278
(50Steps+4sbeam) 32 32 31.98 31.98 0.6579 0.5248 0.8584 0.767 84.56 75.75 42.4 28.2 0.9778 0.9321
(50Steps+8sbeam) 32 32 32 32 0.695 0.5408 0.878 0.7757 86.46 76.44 47.4 29.6 0.9671 0.9646
(100Steps) 32 - 32 - 0.6955 - 0.878 - 86.47 - 47.4 - 0.9791 -
(100Steps+4sbeam) 32 - 31.98 - 0.6561 - 0.8573 - 84.46 - 42.2 - 0.9778 -
(100Steps+8sbeam) 32 - 32 - 0.6955 - 0.878 - 86.47 - 47.4 - 0.9791 -
MTG-ES
Base(0steps) 32 32 31.95 32 0.2321 0.2709 0.5515 0.6054 56.75 62.07 1.6 1.8 0.938 0.9501
Vec2Text(1step) 32 32 32 32 0.3518 0.3692 0.6621 0.6804 67.76 68.92 8 9.6 0.9549 0.9423
(20Steps) 32 32 32 32 0.6661 0.6443 0.8559 0.8461 85.78 84.73 44.8 38.4 0.9632 0.9563
(50Steps) 32 32 32 32 0.6785 0.6593 0.8661 0.8525 86.67 85.46 45.4 38.8 0.9697 0.9582
(50Steps+4sbeam) 32 32 32 32 0.7729 0.7452 0.9041 0.8945 90.47 89.23 60.8 53.6 0.9697 0.9515
(50Steps+8sbeam) 32 32 32 32 0.8002 0.7772 0.9134 0.9072 91.54 90.44 65 56.8 0.9579 0.987
(100Steps) 32 - 32 - 0.7996 - 0.9121 - 91.43 - 65 - 0.9579 -
(100Steps+4sbeam) 32 - 32 - 0.7748 - 0.9052 - 90.56 - 60.8 - 0.9697 -
(100Steps+8sbeam) 32 - 32 - 0.7996 - 0.9121 - 91.43 - 65 - 0.9579 -
Table3: MONOrepresentstheevaluationofTextReconstructioninmultiplelanguages,withthemodelstrainedand
evaluatedonMTGdatasetswithtokenslength32inEnglish,French,GermanandSpanish,respectively. MULTI
representstheevaluationofmultilingualtextreconstruction,withmodelstrainedonMTG-MULTIandevaluatedon
MTGdatasetswithtokenslength32inEnglish,French,GermanandSpanish,respectively. Thebestresultsreached
intheearlieststageforeachlanguageacrossmetricsareinbold. TheresultswhereMULTIoutperformsMONOis
underlined.
TheresultsareshowninTable3. steps takes more than double the time achieving
similarperformance. Untilcorrectionstep50with
8 sbeam, performance increasessteadily, and the
Monolingual Text Reconstruction in Multiple
trendisgenerallyalignedwithcosinesimilarity. As
Languages Formonolingualmodels,weevalu-
aresult, weevaluatethesubsequentmodelsuntil
ateonBaseandVec2Textmodelswithcorrection
correctionstep50with8sbeam.
steps of 1, 20, 50, 100 combined with 4 and 8
sequencebeamsearchwidth(sbeam). Wecanob- Moreover,Spanishmodelsoutperformtheoth-
servethattheBLEUscoreforeachlanguagepeaks ersintermsoftheword-matchmetricsacrosscor-
eitherby50stepscorrectionwith8sbeamor100 rection steps, achieving 0.8002 in BLEU score
steps. Thisevaluationisexpensiveintermsoftime with65%ofexactmatch. Despitehavingalarger
andcomputation. Inordertosearchfortheoptimal volume of data compared to other languages, the
runtime and performance trade-off, Fig. 2 shows English model unexpectedly performs the worst
BLEUscoresateachstepandthelinesrepresent acrossvariousmetrics,asillustratedbythetraining
thetrendforruntimeforthemonolingualmodels. datadistributioninFig.3. However, asshownin
Thebesttrade-offpointsareatthecorrectionstep AppendixB,theevaluationofround-triptranslated
of50 with8 sbeamfor allthe models, while 100 Englishtestdataindicatesnoevidenceoftransla-tioneseeffect. MTG-FR MTG-DE MTG-ES
GTR-Base
Base 0.0439(0.7581) 0.0322(0.7052) 0.0474(0.7134)
MultilingualTextReconstructionWithoutprior
Vec2Text 0.0994(0.8833) 0.0575(0.8138) 0.1012(0.9020)
KnowledgeofLanguage Toevaluatethepoten- AdTrans 0.0928(‚Üì-6.59) 0.0518(‚Üì-9.82) 0.0995(‚Üì-1.67)
ME5-Base
tial of multilingual text inversion without prior
Base 0.0313(0.9513) 0.0273(0.9298) 0.0364(0.9293)
knowledgeofthelanguageinwhichatargettextis Vec2Text 0.0449(0.9487) 0.0299(0.9107) 0.0392(0.8963)
written,wetraininversionmodelsonMTG-MULTI AdTrans 0.1017(‚Üë126.58) 0.0895(‚Üë128.11) 0.0469(‚Üë56.57)
dataset. As shown in Table 3, ME5_MTG-MULTI (a) Cross-lingual cross-domain evaluation with monolingual
modelstrainedonNQ.
Base model outperforms (underlined) or has on
‚ÜíNQ ME5_MTG-FR ME5_MTG-DE ME5_MTG-ES
par performance with monolingual Base models
Base 0.0180(0.9399) 0.0182(0.9016) 0.0141(0.9178)
across languages. Overall, the performance does Vec2Text 0.0207(0.9467) 0.0231(0.9248) 0.0181(0.9253)
not deteriorate in proportion of the monolingual AdTrans 0.0418(‚Üë103.48) 0.0508(‚Üë119.9) 0.0356(‚Üë91.28)
trainingdatavolume,i.e.,forMTG-MULTI,each (b)Cross-lingualcross-domainevaluationonNQwithmonolin-
languagehasaquarterofthedatavolumecompared gualmodelstrainedonMTGdatasets.
MTG-EN MTG-FR MTG-DE MTG-ES
to its monolingual counterpart. Rather, the per-
ME5_MTG-EN
formances are comparable, especially for French Base - 0.032(0.9132) 0.0371(0.8945) 0.031(0.9068)
Vec2Text - 0.0462(0.9421) 0.0561(0.9474) 0.0433(0.911)
and Spanish, with 0.0154 and 0.023 differences, AdTrans - 0.124(‚Üë168.08) 0.0672(‚Üë19.75) 0.1238(‚Üë185.79)
ME5_MTG-FR
respectively. ForSpanish,ME5_MTG-MULTI per- Base 0.033(0.9176) - 0.0297(0.9038) 0.0452(0.9206)
Vec2Text 0.0536(0.9235) - 0.0426(0.9431) 0.0594(0.9241)
forms slightly better in word-match metrics than AdTrans 0.0725(‚Üë37.71) - 0.0635(‚Üë49.47) 0.137(‚Üë126.79)
ME5_MTG-DE
ME5_MTG-ES alsoforVec2Textmodelby1step Base 0.0399(0.8902) 0.0296(0.9082)) - 0.0273(0.9224)
Vec2Text 0.0813(0.9223) 0.0454(0.9223) - 0.0461(0.9163)
correction. Acrosslanguages,thecosinesimilari- AdTrans 0.0961(‚Üë18.19) 0.1037(‚Üë128.62) - 0.1101(‚Üë138.91)
tiesofthemultilingualmodelareconstantlyhigher
ME5_MTG-ES
Base 0.0331(0.9186) 0.0396(0.9035) 0.0267(0.8958) -
thantheirmonolingualcounterparts,withmorecor- Vec2Text 0.0471(0.9223) 0.0513(0.8699) 0.0397(0.9460) -
AdTrans 0.0591(‚Üë25.51) 0.0957(‚Üë86.56) 0.0556(‚Üë39.89) -
rectionsteps,theyworsenedforFrench,i.e.,0.942
(c)Cross-lingualevaluationonmonolingualinversionmodels
comparedto0.9511. trainedandtestonMTGdatasets.
Additionally,toevaluatetheout-of-domainper-
Table 4: Cross-lingual evaluation using BLEU score
formance, ME5_MTG-MULTI is tested on NQ
and Cosine Similarity (in the brackets) for Base and
dataset. As shown in Table 2, for Base models,
Vec2Textmodelsbycorrectionstepsof50with8sbeam.
ME5-Basetrainedonmultilingualdatahasbetter TheBLEUscoresandtheirpercentagegrowth(inthe
performancethantrainedonNQdataset,whileit brackets) compared with BLEU scores on Vec2Text
fallshortcomparedtoitsmonolingualcounterpart. modelsarereportedforAdTransstrategyforeachmodel.
Its Vec2Text model under-performs compared to Theup-arrows‚Üëindicateperformancegainwhilethe
down-arrows ‚Üì indicate performance loss. The result
others.
withthehighestBLEUscorewitheachevaluatedmodel
These phenomena indicate that (i) the high
oneachdatasetisinbold.
monolingual data volume is not the determining
factor on training a high-performing Base model
andVec2Textmodelswithouttheextracorrection scenario, we conduct cross-lingual evaluation on
stepsinbothmonolingualandmultilingualsettings, allthemonolingualmodelswetrainedonNQand
(ii)themultilingualmodeltrainingrenderscloser MTGdatasets,theresultsarereportedinTable4,
embeddings of reconstructed and target texts in withregardtothetypeofmodels.
theembeddingspace,and(iii)theoptimizationap- Weobservethat,ME5-Basemodelstrainedon
proachutilizingcosinesimilarityisnotaseffective both NQ and MTG-datasets, have a tendency to
formultilingualmodelscomparedtomonolingual decode texts, for example xÀÜ, in the language of
models. trainingdata,e.g.,l ,giventhetargettextxwhich
y
isinadifferentlanguage,e.g.,l . However,xÀÜcould
x
5.3 Cross-lingualTextReconstruction
exposethesameinformationjustinadifferentlan-
Cross-lingualtextreconstructionisaspecificcase guage,thenthecurrentword-matchmetricsarenot
of multilingual text reconstruction without prior abletocapturethisphenomenon. Nonetheless,the
knowledge of languages and also a more realis- privacyleakagestillexists.
tic scenario, where the embedder œï is trained on For example, evaluating ME5_MTG-DE model
a different language with regarding to the target onMTG-ENdataset, Report: Trumpeinmalfragte
textlanguage. Toinvestigatethepotentialofthis damals FBI Director Andrew Mccabe w√§hrendseiner2016-voteisgeneratedgiventhetargettext to shed light on the vulnerabilities of languages
Report: Trumponceaskedthen-actingFBIdirector otherthanEnglish,aimingtoencouragethecom-
Andrew Mccabe about his 2016 vote. The gener- munitytoincludesuchlanguagesinNLPsecurity
atedtextmistook‚Äúw√§hrend‚Äù(during)for‚Äúabout‚Äù, work. While there is potential for misuse by ma-
otherwise, the generated text is close in meaning licious actors, as with many works in NLP secu-
withthetargetEnglishtext. Theinformationleak- rity, we attempt to mitigate harm by including a
age would not be properly captured with the cur- brief pointer to a countermeasure to the attack in
rentmetricsevaluatedontheGermantexts. Weuse the paper. Moreover, the language models exam-
AdTransstrategywithEasyNMT5,thegenerated inedinthispaperareopen-sourcemodels,andthus
German text is translated in English, i.e., Report: thisworkdoesnotconstituteanimminentthreatto
Trump once asked FBI director Andrew Mccabe embedding-as-a-serviceproviders,whoarelikely
duringhis2016-vote,inwhichcaseinformationis usingprivatemodels. Moreover,wedonotexper-
missingsuchas‚Äúthen-acting‚Äù,butBLEUscoreis iment with truly sensitive data, ensuring that no
up to 0.5 for this individual example. We imple- real-worldharmiscausedbytheworkcarriedout
mentthisstrategyforallcross-lingualevaluation. inthispaper.
As shown in Table 2, other than for GTR-Base
model,theperformancesareliftedacrossmonolin- Acknowledgements
gualandmultilingualmodelsforeachlanguage.
All authors of thispaper are funded by the Carls-
5.4 Cross-lingualDefenseMechanism berg Foundation, under the Semper Ardens: Ac-
celerateprogramme(projectnr.CF21-0454). We
AstheunderlyingarchitectureisbasedonMorris
arefurthermoregratefultothesupportoftheAAU
etal.(2023),wepointthereadertotheirpaperfor
AICloud,andtoDeiCforallocatinguscomputing
adefencemechanism.
resources on the LUMI cluster (project nr. DeiC-
AAU-S5-412301). We thank Sighvatur Sveinn
6 Conclusion
Davidsson for setting us up with this access, and
While all previous work on embedding inversion forhisdiligenceinassistingwiththeproblemsin
attackshasfocusedsolelyonEnglish,wepresent theexperimentalinfrastructure,inadditiontothe
thefirstworkonmultilingualembeddinginversion. LUMIusersupportfortheirverypromptanswers
Bydefiningtheproblemofblack-boxmultilingual andcompetence,especiallyJingGong. Wefurther
andcross-lingualinversionattacks,welaythefoun- thank Esther Ploeger for her assistance in testing
dationforfutureworkinthisdirection. Asoneof translationeseeffectfortheunder-performanceof
ourcorefindingsisthefactthatmultilingualmod- multilingualinversionmodelinEnglishandMar-
els, in some circumstances, are more vulnerable cellRichardFeketeforhisinsightfulinputinproof-
than monolingual English models, we hope that readingthepaper.
thisworkinspiresamultilingualapproachtoLLM
securityandNLPsecurityasawhole.
References
Limitations
PayalBajaj,DanielCampos,NickCraswell,LiDeng,
JianfengGao,XiaodongLiu,RanganMajumder,An-
Acorelimitationofthisworkisthecomputation-
drew McNamara, Bhaskar Mitra, Tri Nguyen, Mir
ally intense experiments, requiring in the area of
Rosenberg,XiaSong,AlinaStoica,SaurabhTiwary,
20,000 GPU computing hours. While expanding andTongWang.2018. Msmarco: Ahumangener-
thisresearchdirectiontomorelanguageswillfur- atedmachinereadingcomprehensiondataset.
therincreasethisexpense,weadvocateforensur-
StevenBirdandEdwardLoper.2004. NLTK:Thenatu-
ing that languages other than English are not left
rallanguagetoolkit. InProceedingsoftheACLIn-
behindintermsofNLPsecurity. teractivePosterandDemonstrationSessions,pages
214‚Äì217,Barcelona,Spain.AssociationforCompu-
EthicsStatement tationalLinguistics.
This work explores attacks on multilingual em- NicholasCarlini,ChangLiu,JernejKos,√ölfarErlings-
bedding models. Our intent with this research is son,andDawnSong.2018. Thesecretsharer: Mea-
suringunintendedneuralnetworkmemorization&
5https://github.com/UKPLab/EasyNMT extractingsecrets. CoRR,abs/1802.08232.Nicholas Carlini, Florian Tram√®r, Eric Wallace, Donggyu Kim, Garam Lee, and Sungwoo Oh. 2022.
Matthew Jagielski, Ariel Herbert-Voss, Katherine Towardprivacy-preservingtextembeddingsimilarity
Lee,AdamRoberts,TomB.Brown,DawnSong,√öl- withhomomorphicencryption. InProceedingsofthe
farErlingsson,AlinaOprea,andColinRaffel.2020. FourthWorkshoponFinancialTechnologyandNat-
Extractingtrainingdatafromlargelanguagemodels. uralLanguageProcessing(FinNLP),pages25‚Äì36,
CoRR,abs/2012.07805. AbuDhabi,UnitedArabEmirates(Hybrid).Associa-
tionforComputationalLinguistics.
Yiran Chen, Zhenqiao Song, Xianze Wu, Danqing
Wang,JingjingXu,JiazeChen,HaoZhou,andLei TomKwiatkowski, JennimariaPalomaki, OliviaRed-
Li. 2022. MTG: A benchmark suite for multilin- field,MichaelCollins,AnkurParikh,ChrisAlberti,
gualtextgeneration. InFindingsoftheAssociation DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
forComputationalLinguistics: NAACL2022,pages tonLee,KristinaToutanova,LlionJones,Matthew
2508‚Äì2527, Seattle, UnitedStates.Associationfor Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
ComputationalLinguistics. Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ralquestions: Abenchmarkforquestionanswering
AlexisConneau,KartikayKhandelwal,NamanGoyal, research. TransactionsoftheAssociationforCompu-
Vishrav Chaudhary, Guillaume Wenzek, Francisco tationalLinguistics,7:452‚Äì466.
Guzm√°n, Edouard Grave, Myle Ott, Luke Zettle-
moyer,andVeselinStoyanov.2020. Unsupervised HaoranLi, YangqiuSong, andLixinFan.2022. You
cross-lingualrepresentationlearningatscale. don‚Äôtknowmyfavoritecolor: Preventingdialogue
representationsfromrevealingspeakers‚Äôprivateper-
Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, sonas. In Proceedings of the 2022 Conference of
Chao Shang, Hang Liu, Sanguthevar Rajasekaran, theNorthAmericanChapteroftheAssociationfor
and Caiwen Ding. 2021. TAG: Gradient attack on ComputationalLinguistics: HumanLanguageTech-
transformer-based language models. In Findings nologies, pages 5858‚Äì5870, Seattle, United States.
of the Association for Computational Linguistics: AssociationforComputationalLinguistics.
EMNLP 2021, pages 3600‚Äì3610, Punta Cana, Do-
minican Republic. Association for Computational HaoranLi,MingshiXu,andYangqiuSong.2023. Sen-
Linguistics. tence embedding leaks more information than you
expect: Generative embedding inversion attack to
MattFredrikson,SomeshJha,andThomasRistenpart. recoverthewholesentence. InFindingsoftheAs-
2015. Model inversion attacks that exploit confi- sociationforComputationalLinguistics: ACL2023,
dence information and basic countermeasures. In pages14022‚Äì14040,Toronto,Canada.Association
Proceedingsofthe22ndACMSIGSACConference forComputationalLinguistics.
on Computer and Communications Security, CCS
‚Äô15,page1322‚Äì1333,NewYork,NY,USA.Associa- Chin-Yew Lin. 2004. ROUGE: A package for auto-
tionforComputingMachinery. maticevaluationofsummaries. InTextSummariza-
tionBranchesOut,pages74‚Äì81,Barcelona,Spain.
Matthew Fredrikson, EricLantz, SomeshJha, Simon AssociationforComputationalLinguistics.
Lin,DavidPage,andThomasRistenpart.2014. Pri-
vacyinpharmacogenetics: Anend-to-endcasestudy L.Lyu,XuanliHe,andYitongLi.2020. Differentially
ofpersonalizedwarfarindosing. InProceedingsof privaterepresentationfornlp: Formalguaranteeand
the23rdUSENIXConferenceonSecuritySymposium, an empirical study on privacy and fairness. ArXiv,
SEC‚Äô14,page17‚Äì32,USA.USENIXAssociation. abs/2010.01285.
Ishrak Hayet, Zijun Yao, and Bo Luo. 2022. Inver- Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
net: An inversion attack framework to infer fine- Jeffrey Dean. 2013. Efficient estimation of word
tuningdatasetsthroughwordembeddings. InFind- representationsinvectorspace. InInternationalCon-
ingsoftheAssociationforComputationalLinguistics: ferenceonLearningRepresentations.
EMNLP2022,pages5009‚Äì5018,AbuDhabi,United
ArabEmirates.AssociationforComputationalLin- JohnMorris,VolodymyrKuleshov,VitalyShmatikov,
guistics. andAlexanderRush.2023. Textembeddingsreveal
(almost)asmuchastext. InProceedingsofthe2023
JohannesH√∂hmann,AchimRettinger,andKaiKugler. Conference on Empirical Methods in Natural Lan-
2021. Invbert: Text reconstruction from contextu- guageProcessing, pages12448‚Äì12460, Singapore.
alizedembeddingsusedforderivedtextformatsof AssociationforComputationalLinguistics.
literaryworks. CoRR,abs/2109.10104.
YasminMoslem,RejwanulHaque,JohnKelleher,and
YangsiboHuang,ZhaoSong,DanqiChen,KaiLi,and AndyWay.2022. Domain-specifictextgeneration
SanjeevArora.2020. TextHide: Tacklingdatapri- formachinetranslation. InProceedingsofthe15th
vacyinlanguageunderstandingtasks. InFindings biennialconferenceoftheAssociationforMachine
of the Association for Computational Linguistics: Translation in the Americas (Volume 1: Research
EMNLP2020,pages1368‚Äì1382,Online.Association Track),pages14‚Äì30,Orlando,USA.Associationfor
forComputationalLinguistics. MachineTranslationintheAmericas.NiklasMuennighoff,NouamaneTazi,Lo√ØcMagne,and Olga Tsymboi, Danil Malaev, Andrei Petrovskii, and
NilsReimers.2023. Mteb: Massivetextembedding IvanOseledets.2023. Layerwiseuniversaladversar-
benchmark. ialattackonNLPmodels. InFindingsoftheAsso-
ciation for Computational Linguistics: ACL 2023,
Milad Nasr, Reza Shokri, and Amir Houmansadr. pages 129‚Äì143, Toronto, Canada. Association for
2019. Comprehensive privacy analysis of deep ComputationalLinguistics.
learning: Passiveandactivewhite-boxinferenceat-
tacksagainstcentralizedandfederatedlearning. In Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
2019IEEESymposiumonSecurityandPrivacy(SP). ner,andSameerSingh.2019. Universaladversarial
IEEE. triggersforattackingandanalyzingNLP. InProceed-
ingsofthe2019ConferenceonEmpiricalMethods
RahilParikh,ChristopheDupuy,andRahulGupta.2022. inNaturalLanguageProcessingandthe9thInter-
Canaryextractioninnaturallanguageunderstanding nationalJointConferenceonNaturalLanguagePro-
models. In Proceedings of the 60th Annual Meet- cessing(EMNLP-IJCNLP),pages2153‚Äì2162,Hong
ingoftheAssociationforComputationalLinguistics Kong,China.AssociationforComputationalLinguis-
(Volume 2: Short Papers), pages 552‚Äì560, Dublin, tics.
Ireland.AssociationforComputationalLinguistics.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao,LinjunYang,DaxinJiang,RanganMajumder,
JeffreyPennington,RichardSocher,andChristopherD.
and Furu Wei. 2022. Text embeddings by weakly-
Manning. 2014. Glove: Global vectors for word
supervisedcontrastivepre-training.
representation. InConferenceonEmpiricalMethods
inNaturalLanguageProcessing.
Shangyu Xie and Yuan Hong. 2021. Reconstruction
attackoninstanceencodingforlanguageunderstand-
MattPost.2018. AcallforclarityinreportingBLEU
ing. InProceedingsofthe2021ConferenceonEm-
scores. InProceedingsoftheThirdConferenceon
pirical Methods in Natural Language Processing,
MachineTranslation: ResearchPapers,pages186‚Äì
pages2038‚Äì2044,OnlineandPuntaCana,Domini-
191, Brussels, Belgium. Association for Computa-
can Republic. Association for Computational Lin-
tionalLinguistics.
guistics.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Mike Zhang and Antonio Toral. 2019. The effect of
DarioAmodei,andIlyaSutskever.2019. Language
translationese in machine translation test sets. In
modelsareunsupervisedmultitasklearners.
ProceedingsoftheFourthConferenceonMachine
Translation(Volume1: ResearchPapers),pages73‚Äì
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
81, Florence, Italy. Association for Computational
Lee,SharanNarang,MichaelMatena,YanqiZhou,
Linguistics.
WeiLi,andPeterJ.Liu.2023. Exploringthelimits
oftransferlearningwithaunifiedtext-to-texttrans- ZhuoZhang,YuanhangYang,YongDai,QifanWang,
former. Yue Yu, Lizhen Qu, and Zenglin Xu. 2023. Fed-
PETuning: When federated learning meets the
DinghanShen,AsliCelikyilmaz,YizheZhang,Liqun parameter-efficienttuningmethodsofpre-trainedlan-
Chen, Xin Wang, Jianfeng Gao, and Lawrence guage models. In Findings of the Association for
Carin.2019. Towardsgeneratinglongandcoherent ComputationalLinguistics: ACL2023,pages9963‚Äì
text with multi-level latent variable models. arXiv 9977, Toronto, Canada. Association for Computa-
preprintarXiv:1902.00154. tionalLinguistics.
Reza Shokri, Marco Stronati, and Vitaly Shmatikov. XinZhou,YiLu,RuotianMa,TaoGui,YuranWang,
2016. Membershipinferenceattacksagainstmachine Yong Ding, Yibo Zhang, Qi Zhang, and Xuanjing
learningmodels. CoRR,abs/1610.05820. Huang.2023. TextObfuscator: Makingpre-trained
languagemodelaprivacyprotectorviaobfuscating
CongzhengSongandAnanthRaghunathan.2020. In- word representations. In Findings of the Associa-
formation leakage in embedding models. In Pro- tionforComputationalLinguistics:ACL2023,pages
ceedingsofthe2020ACMSIGSACConferenceon 5459‚Äì5473,Toronto,Canada.AssociationforCom-
ComputerandCommunicationsSecurity,CCS‚Äô20, putationalLinguistics.
page377‚Äì390,NewYork,NY,USA.Associationfor
ComputingMachinery. LigengZhu,ZhijianLiu,andSongHan.2019. Deep
leakagefromgradients. In AdvancesinNeuralIn-
Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, and formation Processing Systems, volume 32. Curran
Chun Fan. 2022. Summarize, outline, and elabo- Associates,Inc.
rate: Long-textgenerationviahierarchicalsupervi-
sionfromextractivesummaries. InProceedingsof
the29thInternationalConferenceonComputational
Linguistics,pages6392‚Äì6402,Gyeongju,Republic
ofKorea.InternationalCommitteeonComputational
Linguistics.A TrainingDataDistribution effect on the difference of the performances can
thereforeberejected.
NQ MTG_EN MTG_FR MTG_DE MTG_ES MTG_MULTI
#Tokens #PredTok. BLEU ROUGE TF1 EXACT COS
Vec2Text(1Step) 29.59 30.98 0.1003 0.4754 41.28 0.0 0.9046
5M (20Steps) 29.59 30.95 0.1448 0.5514 47.80 0.2 0.9130
(50Steps) 29.59 30.98 0.1511 0.5601 48.56 0.2 0.9261
(50Steps+4sbeam) 29.59 30.88 0.1756 0.6181 52.64 0.2 0.9461
4M (50Steps+8sbeam) 29.59 30.96 0.1742 0.6128 52.44 0.4 0.9185
Nr.
of
Samples
3M
T roa ub nle d-5 t: riE pv ta ralu na st li ao ten do Mfm Tu Gl -ti Eli Nng tu ea stl din av tae srs ei to .nmodelon
2M
C TextConstructiononTokensLength64
1M
0 4 14 24 32 #Tokens #PredTokens BLEU ROUGE TF1 Exact COS
Token Lengths English
Vec2Text(1Step) 37.78 43.73 0.1813 0.5933 57.28 0.8 87.94
Figure3: TheDistributionofthetrainingdataformod- (20Steps) 37.78 41.32 0.3848 0.7838 74.23 10.0 88.75
(50Steps) 37.78 40.97 0.3927 0.7974 75.40 10.2 92.70
elswiththemaximaltokenlengthof32.
(50Steps+4sbeam) 37.78 40.67 0.4523 0.8168 77.31 14.6 89.18
(50Steps+8sbeam) 37.78 40.19 0.4729 0.8334 78.62 16.6 91.09
French
InMTGdatasets,Englishtextsarecuratedfrom Vec2Text(1Step) 51.61 57.23 0.2645 0.6358 64.03 0.8 95.07
(20Steps) 51.61 53.25 0.5825 0.8310 83.01 26.6 96.54
various sources, while texts in German, Spanish (50Steps) 51.61 52.60 0.5958 0.8399 83.69 26.8 96.26
(50Steps+4sbeam) 51.61 52.62 0.6461 0.8611 86.03 37.8 97.26
and French are machine translated and manually (50Steps+8sbeam) 51.61 52.54 0.6680 0.8674 86.44 41.8 93.83
German
validated. Thelanguageshavediversemorpholo- Vec2Text(1Step) 49.75 56.09 0.1965 0.5458 55.19 0.2 97.43
(20Steps) 49.75 52.62 0.4611 0.7610 75.30 15.6 93.98
gies,resultingindifferentlengthsofsentencesand (50Steps) 49.75 52.76 0.4661 0.7669 75.86 15.8 95.72
(50Steps+4sbeam) 49.75 51.91 0.5278 0.7960 78.93 25.6 92.98
the number of sentences after sentence tokeniza- (50Steps+8sbeam) 49.75 51.82 0.5573 0.8087 80.21 30.8 94.97
Spanish
tion across languages. NQ dataset is included to Vec2Text(1Step) 62.66 62 0.2603 0.6416 65.78 0.4 97.57
(20Steps) 62.66 62.23 0.5607 0.8353 83.70 17.4 98.28
replicateresultsfrompreviouswork(Morrisetal.,
(50Steps) 62.66 62.09 0.5673 0.8437 84.46 17.4 97.01
(50Steps+4sbeam) 62.66 61.95 0.6427 0.8678 87.01 29.2 95.39
2023),andevaluatecross-domainandcross-lingual
(50Steps+8sbeam) 62.66 61.76 0.6557 0.8773 87.85 32.8 97.36
performance of text reconstruction task. NQ has
Table6: TheevaluationofTextReconstructioninmulti-
huge amount of data for English only, and no to-
plelanguages,withthemodelstrainedandevaluatedon
kenizationhasbeenimplementedontheincluded
MTGdatasetswithmaximaltokenlength64inEnglish,
Wikipedia passages, hence all the training data French,GermanandSpanish,respectively.
fromNQhas32tokens.
B TheEffectofTranslationeseonTest
Data
Theeffectoftranslationeseinmachinetranslation
has been extensively studied, and there is a clear
evidencethattheuseoftranslationeseintestsets
resultsininflatedhumanevaluationscoresforMT
systems (ZhangandToral,2019). Toinvestigate
whetherourmultilingualinversionmodel‚Äôssub-par
performanceinEnglishisduetothecharacteristics
oftranslationeseinotherlanguages,weimplement
round trip translation on MTG-EN test data us-
ingSpanishasthepivotlanguagewithEasyNMT,
thetranslationpathisthusEnglish‚ÜíSpanish‚Üí
English. Then the evaluation of the multilingual
inversionmodelisdoneontheround-triptranslated
English test set, the result is shown as in Table 5.
Compared to evaluation on MTG-EN test set, as
shown in Table 3, the performance of translated
Englishtestsetisabout0.3worseateachstageof
corrections. The hypothesis of the translationeseD Cross-lingualEvaluationusing
AdTrans
Type Predicted Translated
14
12
10
EU 8 -1.67% -6.59% G T R
BL 6 -N
4 -9.82% Q
2
0
14
12
10 +127% M
U 8 +128% E
E 5
BL 6 -N
Q
4 +57%
2
0
14
12 +186% +168%
M
10 E
5
EU 8 -M
BL 6 +20% T G
4 -E
N
2
0
14
12
M
10 E
+87% 5
EU 8 -M
BL 46 +40% +26% T G -E
+91% S
2
0
14
12
10 +139% +129% +18% M E 5
EU 8 -M
BL 6 T G
4 +120% -D
E
2
0
14
+127%
12
M
10 E
5
EU 8 -M
BL 6 +49% +38% T G
4 +103% -F R
2
0
mtg_es mtg_fr mtg_de mtg_en nq_en
Dataset
Figure 4: Cross-lingual evaluation between recon-
structedtextsandtranslatedreconstructedtexts.