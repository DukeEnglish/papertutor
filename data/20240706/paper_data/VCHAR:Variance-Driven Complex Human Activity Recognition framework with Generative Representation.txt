1
VCHAR:Variance-Driven Complex Human Activity Recognition
framework with Generative Representation
YUANSUN,
WINLAB,RutgersUniversity, USA
NAVIDSALAMIPARGOO,
WINLAB,RutgersUniversity, USA
TAQIYAEHSAN,
WINLAB,RutgersUniversity, USA
ZHAOZHANG,
WINLAB,RutgersUniversity, USA
JORGEORTIZ,
WINLAB,RutgersUniversity, USA
Complexhumanactivityrecognition(CHAR)remainsapivotalchallengewithinubiquitouscomputing,especiallyinthe
contextofsmartenvironments.Existingstudiestypicallyrequiremeticulouslabelingofbothatomicandcomplexactivities,
a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior
researchhasfocusedondatasetsthateitherpreciselylabelatomicactivitiesor,atminimum,theirsequenceâ€”approaches
thatareoftenimpracticalinreal-worldsettings.Inresponse,weintroduceVCHAR(Variance-DrivenComplexHuman
ActivityRecognition),anovelframeworkthattreatstheoutputsofatomicactivitiesasadistributionoverspecifiedintervals.
Leveraginggenerativemethodologies,VCHARelucidatesthereasoningbehindcomplexactivityclassificationsthrough
video-basedexplanations,accessibletouserswithoutpriormachinelearningexpertise.Ourevaluationacrossthreepublicly
availabledatasetsdemonstratesthatVCHARenhancestheaccuracyofcomplexactivityrecognitionwithoutnecessitating
precisetemporalorsequentiallabelingofatomicactivities.Furthermore,userstudiesconfirmthatVCHARâ€™sexplanationsare
moreintelligiblecomparedtoexistingmethods,facilitatingabroaderunderstandingofcomplexactivityrecognitionamong
non-experts.
CCSConcepts:â€¢Computingmethodologiesâ†’Machinelearning;â€¢Human-centeredcomputingâ†’Visualization.
AdditionalKeyWordsandPhrases:ExplainableIoT,Deepgenerativemodel,Hardwaredeeplearning,Deeplearningonsmall
dataset
ACMReferenceFormat:
YuanSun, NavidSalamiPargoo, TaqiyaEhsan, ZhaoZhang,andJorgeOrtiz.2024.VCHAR:Variance-DrivenComplex
HumanActivityRecognitionframeworkwithGenerativeRepresentation.Proc.ACMInteract.Mob.WearableUbiquitous
Technol.0,0,Article0 (2024),26pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Inrecentyears,theproliferationofsensorsacrossdiversesettingshasbecomeubiquitous,seamlesslyintegrating
intotheveryfabricofdailylife.Thesesensorsareembeddedinawidearrayofdevicessuchassmartphones,
1ThisstudywasapprovedbytheInstitutionalReviewBoardatourinstitution(IRBNo. )
Authorsâ€™addresses: YuanSun,WINLAB,RutgersUniversity, Piscataway,NJ,, USA,ys820@soe.rutgers.edu; NavidSalamiPargoo,WINLAB,
RutgersUniversity, Piscataway,NJ,, USA,ys820@soe.rutgers.edu; TaqiyaEhsan,WINLAB,RutgersUniversity, Piscataway,NJ,, USA,
ys820@soe.rutgers.edu; ZhaoZhang,WINLAB,RutgersUniversity, Piscataway,NJ,, USA,ys820@soe.rutgers.edu; JorgeOrtiz,WINLAB,
RutgersUniversity, Piscataway,NJ,, USA,jorge.ortiz@rutgers.edu.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthat
copiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitationonthefirst
page.CopyrightsforcomponentsofthisworkownedbyothersthanACMmustbehonored.Abstractingwithcreditispermitted.Tocopy
otherwise,orrepublish,topostonserversortoredistributetolists,requirespriorspecificpermissionand/orafee.Requestpermissionsfrom
permissions@acm.org.
Â©2024AssociationforComputingMachinery.
2474-9567/2024/0-ART0 $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
4202
luJ
3
]IA.sc[
1v19230.7042:viXra0:2 â€¢
cameras,clothing,buildings,andvehicles,enablingcontinuousandpervasivedatacollection.Thisexpansion
hassignificantlypropelledthefieldofactivityrecognition,placingitattheforefrontofubiquitouscomputing
research.Thepotentialapplicationsofactivityrecognitionarevastandimpactful,encompassingareassuch
as healthcare [13], elderly care [33], surveillance, and emergency response [52]. The ability to monitor and
understandhumanactivitiesonsuchascaleholdsimmenseutility,offeringadvancementsinpersonalhealth
monitoring,enhancedsecuritysystems,andmoreefficientemergencyservices.
However,alongsideitsrapidgrowthandintegrationintovariousdomains,thefieldofactivityrecognition
faces several substantial challenges. Chief among these is the issue of sensor data labeling, which is crucial
forthedevelopmentofaccurateandreliablemodels.Thisprocessoftenencounterssignificanthurdlessuch
astheabsenceoflabels,incorrectlabeling,ortheextensivemanualeffortrequiredforannotation[14,30,53].
Furthermore,themodelsdevelopedforactivityrecognitionfrequentlyremaincomplexandopaque,functioning
as"black-boxes"thataredifficultforbothexpertsandlaypeopletointerpret[4,5,34].Thiscomplexityhinders
transparencyandassessment,posingabarriertotrustandunderstandingamonguserswhoarenotwell-versed
intechnicaldetails.
Toaddresstheseissues,thereisagrowingemphasisonthedevelopmentofExplainableAI(XAI)methods.
SuchmethodsarecrucialfordemystifyingAIdecision-makingprocesses,increasingtransparency,andbuilding
trustamongusers[55].BymakingAIdecisionsmoreaccessibleandunderstandable,XAInotonlyenhancesuser
confidencebutalsofacilitateswideradoptionandintegrationofthesetechnologiesacrossvarioussectors.This
approachaimstoensurethatasAIsystemsbecomemoreintegratedintoourlives,theydosoinamannerthatis
bothcomprehensibleandtrustworthy,ultimatelyleadingtomoreinformedandacceptinguserinteractions.
Tounlockactivityrecognitionâ€™sfullpotentialinubiquitouscomputing,addressinglabelingchallengesand
creatingtechniquesthatenablelaypersonunderstandingofmodelinsightsisparamount.Thisbridgesthegap
betweenrawsensordataandactionableinformation,facilitatingthedevelopmentofreliable,trustworthy,and
deployablereal-worldactivityrecognitionsystems.
1.1 ChallengesinComplexHumanActivityRecognition
TraditionalmethodsforComplexHumanActivityRecognition(CHAR)typicallynecessitatepreciselabeling
ofeachatomicactivitywithinspecifictimeslotstoeffectivelytrainmodels.Whilesomeresearchattemptsto
incorporateconceptualframeworks,theseapproachesoftenrequiresegmentingthedatatoenhanceaccuracy.
Suchsegmentationdemandsdetailedlabelingofeachatomicactivity,includingtheeliminationoftransientstates,
whichcanbelabor-intensiveandpronetoinaccuraciesregardingtheexactstartandendpointsofactivities.
In practical scenarios, real-life datasets typically categorize types of atomic or complex activities within
specificcollectionintervals(seeFig.1)[10,25,38,39].Whilesomedatasetsprovidedetailedatomicactivity
labels,thesecanoftenbeerroneousorunreliable[19,24].Furthermore,somedatasetsonlyindicatethetypeof
activities(Fig.1),encompassingğ‘›atomicactivitieswhereğ‘šactivitiesmayoccurconcurrently,leadingtoğ¶(ğ‘›,ğ‘š)
possiblecombinations.Thisunderscoresthecombinatorialcomplexityfacedwhensegmentscannotbedistinctly
separated.Ourframeworkisspecificallydesignedtoaddresssuchchallengesbymanaginginseparabledataset
segmentsandextendingbeyondmerelydetectingğ‘›isolatedactivities.Itisimportanttonotethataprevalent
assumptioninthefieldsuggeststhattheperformanceofmachinelearningmodelsdegradesastheybecome
moreexplainable,especiallywhenthemodelstructuresbecomeintricate[15].Additionally,thesedatasetsoften
assignuncharacterizedactivitiestogenericcategorieslike"others"orexcludethemaltogether,whichpresents
significantlabelingchallengesandcomplicatesthedevelopmentofgeneralizedsolutionsadaptabletoreal-world
applications.
Moreover,significantchallengespersistinrepresentingtheoutputsofsensor-basedmodelswithinthevisual
domain.Despiteincreasinginterestintransformingsensordataintoimagerepresentationstoenhancelayperson
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:3
understanding of machine learning results, the development of visual domain representations has not kept
pace[2,3,18].Thisdiscrepancyhighlightstheurgentneedforinnovativeapproachesthateffectivelybridgethe
gapbetweensensordataandvisualrepresentation,therebyimprovingtheinterpretabilityandpracticalutilityof
ComplexHumanActivityRecognition(CHAR)systemsforeverydayusers.Thisgrowinginterestemphasizesthe
demandforvisualrepresentationsthatmakemachinelearningmodelsmoreaccessibletolaypersons.
These challenges highlight the complexity of CHAR in real-life settings and underscore the necessity for
advancedmethodologiesthatcanhandleimprecisedataanddevelopmoreintuitiveoutputrepresentations.
Atomic Atomic
A
AA A
AAA
A
c
cc c
ccc
c
t
tt t
ttt
t
i
ii i
iii
i
v
vv v
vvv
v
i
ii i
iii
i
t
tt t
ttt
t
y
yy y
yyy
y
2
15 4
378
6
Activity 1 Activity 2 Atomic Activity 1 + Atomic Activity 2 t1 t2 t3 t4 t5 t6
Complex Activity A Complex Activity A
Fig.1. Standardcomplexactivitydatasets(left)typicallyprovidedetailedlabelsforeachtimeintervaltofacilitateatomic
activitytraining.Incontrast,in-the-wilddatasets(middle),constrainedbylaborcapacityandotherpracticallimitations,only
specifythetypesofcomplexandatomicactivitiespersegmentwithoutspecifictimeintervalordetailedatomicactivity
labelsfortimeseriessegmentation.Itoftenfeatureagreatervarietyoflabelcombinations(right),reflectingthecomplexity
andunpredictabilityofreal-worldscenarios.
1.2 Contributions
WedevelopedtheVariance-DrivenComplexHumanActivityRecognition(VCHAR)frameworkwithGenerative
Representationtotackleprevalentissuesintherecognitionofcomplexhumanactivities.VCHARovercomes
thelimitationsoftraditionalCHARmethodsthatrequiredetailedandlabor-intensivesegmentationofactivities.
Instead,itutilizesavariance-drivenapproachthatleveragestheKullback-Leiblerdivergencetoapproximatethe
distributionofatomicactivityoutputs.Thismethodallowsfortherecognitionofdecisiveatomicactivitieswithin
specifictimeintervalswithoutnecessitatingtheremovaloftransientstatesorotherirrelevantdata,thereby
enhancingthedetectionratesofcomplexactivitiesevenintheabsenceofdetailedlabelingofatomicactivi-
ties.Ourexperimentsdemonstratethatevenwithoutpreciselabelingofatomicactivitiesorwithoutsequentially
correctedlabeling,ourmodeleffectivelyutilizeskeyconceptstoenhancethedetectionrateofcomplexactivities.
Additionally,itprovidesapromisingrateofatomicactivitydetection,whichiscrucialforaccuratelyrepresenting
thedatawhentransmittingoutputstothedecoder.
Moreover,VCHARintroducesanovelgenerativedecoderframeworkthattransformssensor-basedmodel
outputsintointegratedvisualdomainrepresentations.Thisincludesdetailedvisualizationsofbothcomplexand
atomicactivities,alongsidedesiredsensorrelatedinformationfromthemodel.UtilizingaLanguageModel(LM)
agent,theframeworkorganizesdiversedatasourcesandemploysaVision-LanguageModel(VLM)togenerate
comprehensivevisualoutputs.Tofacilitaterapidadaptationtospecificsmartspacescenarios,weproposea
pretrained"sensor-basedfoundationmodel"andimplementa"one-shottuningstrategy"withmaskedguidance.
OurexperimentsonthreepubliclyavailabledatasetsdemonstratethatVCHARnotonlyperformscompetitively
withtraditionalmethodsbutalsosignificantlyenhancestheinterpretabilityandusabilityofCHARsystems,as
confirmedthroughhumanevaluationstudies.Ourcontributionsaremulti-facetedandcanbesummarizedas
follows:
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:4 â€¢
â€¢ WeintroduceVCHAR,avariational-drivenframeworkdesignedtogeneratevisualdomainrepresentations
ofcomplexactivityrecognition.Thissystemaimstomakecomplexactivityinsightsaccessibletolaypersons
byvisuallyrepresentingthedatainanintuitivemanner.
â€¢ WeutilizeKLdivergenceasalossfunctiontomodelthedynamicrelationshipsamongvaryingcombinations
ofatomicactivitiesacrossdifferenttimeintervalsforthesametypeofcomplexactivity.
â€¢ Our method proves effective in real-life scenarios with inaccurate or absent specific time labeling of
activities. Results demonstrate that multitasking modeling enhances complex activity detection rates.
Additionally,thismultitaskingmodelingequipsthedecoderwithfeaturesnecessarytooffervisualdomain
explanationsaccessibletolaypersons.
â€¢ Weproposea"sensor-basedfoundationmodel"frameworkwithourmaskedone-shottuningstrategythat
quicklyadaptstospecificsmartspacescenarios.AnLLMagentguidesthismodeltogenerateaccessible
visualdomainrepresentations,particularlybenefitinguserswithouttechnicalexpertise.
â€¢ Weconductedexperimentson3publiclyavailabledatasets,somewithlabelingissues.Ourmethoddemon-
stratedcompetitiveresultsanduserpreferencethroughauserstudy,showcasingitseffectiveness.
Fig.2. TheVCHARframeworkistailoredforrecognizingbothatomicandcomplexactivitieswithoutprecisetemporal
annotations.Theframeworkleveragessensorencoderoutputstogeneratevisualrepresentationsat24fps,therebyenhancing
userunderstanding.Forinstance,onevideooutputdepictshowtheleftfootsensorcruciallydetectsactivities.Another
segmentillustratestheatomicactivityâ€œopenthedishwasherâ€aspartofthebroaderâ€œcleanupâ€complexactivity.
2 RELATEDWORK
2.0.1 SmartSpaceComplexActivityRecognition. Significantadvancementshavebeenmadeinrecognizingboth
atomicandcomplexhumanactivitiesusingvarioussensortechnologiesandmachinelearningmodels.Baoetal.
utilizedmultiplebiaxialaccelerometersalongwithdecisiontreeclassifiers,demonstratingthatanincreaseinthe
numberofaccelerometersandsubject-specifictrainingenhancesperformance[6].Dernbachetal.foundthat
whileatomicactivitiescouldbeaccuratelyclassifiedusingasmartphoneâ€™striaxialaccelerometerandgyroscope
withaMulti-layerPerceptron,complexactivitiesposedgreaterchallenges[12].Mekruksavanichetal.introduced
aCNN-BiGRUmodelthateffectivelyrecognizescomplexactivitiesfromwrist-wornsensors[29],whileTahvilian
et al. compared the efficacy of CNN-LSTM and CNN-BiLSTM models for varying complexities of activities
[45].Additionally,Pengetal.proposedamulti-tasklearningapproachusingCNNsandLSTMsthatimproved
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:5
performanceonbothatomicandcomplexactivitiesbysharingaCNNfeatureextractor[32].Theyalsonoted
thatcomplexactivities,duetotheirintricatenature,arehardertorecognizethanatomicones,resultinginlower
performance[31].Thisbodyofworkcollectivelyemphasizesthepotentialofintegratingadvancedcomputational
modelsanddiversesensordatatoenhanceactivityrecognitioninubiquitouscomputingenvironments.
2.0.2 VisualRepresentationofSensorData. Thetransformationofsensordataintovisualformatshasgained
significantattention,enablingtheapplicationofimageclassificationtechniquestosensor-basedhumanactivity
recognition[16,46].Researchershaveexploredvariousmethodstofacilitatethistransformation.Forinstance,
transformingsensordataintospectrogramimageshasallowedtheuseofdeeplearningmodelslikeCNNsfor
recognizinghumanactivities[10,20,22].
Further advancements in this field have led to the development of methods that enhance interpretability.
Arrottaetal.[3]utilizeCNNsandExplainableAItotranslatesensordataintosemanticmapsfortransparent
activityrecognitioninsmarthomes,aimingtoboosttrust,particularlyinhealthcaremonitoringscenarios.These
semanticmapsarefurtherprocessedintonaturallanguageexplanations,providingclearandunderstandable
insightsintotheAIâ€™sdecision-makingprocesses.AnotherapproachbyJeyaetal.[21]appliesCTClosstoalign
detectedactivitieswithrawsensorsignals,whicharevisualizedontime-accelerationgraphsandmarkedwith
dashedrectanglestoimprovetheinterpretabilityofcomplexactivities.
Additionally,explainabilityinAIcanbeapproachedthroughmodel-agnosticmethodssuchasLIME[8]and
SHAP[28],whichapproximatetherelationshipbetweeninputandoutputpredictionswithoutaccessingthe
modelâ€™sinternalworkings.Conversely,model-transparentmethodslikeGrad-CAM++[8]andsaliencymaps[40]
provideinsightsintotheinternalprocessesofneuralnetworksbyvisualizingtheimportanceofinputfeatures
andtheactivationvalueofhiddenlayers.
Inresponsetothegrowinginterestinmakingsensordatacomprehensiblethroughvisualrepresentations[18],
ourworkdevelopsvisiondomainrepresentationsdirectlyfromsensordata.Specifically,weaimtomakethe
outcomesofactivityrecognitionmodelsaccessibletolaypersonsbyapplyingavideo-basedrepresentationof
sensor activation value. This approach bridges the gap in understanding complex models for users without
technicalexpertise,enhancingusertrustandengagementwiththetechnology.
2.0.3 FoundationandMultimodalModels. Foundationmodels,thelatestevolutioninAItechnologies,aretrained
onextensive,diversedatasetsandarecapableofbeingappliedacrossawiderangeofdomains[7].Thesemodels,
which are adaptable to numerous applications, highlight the forefront of AI research, benefiting from their
trainingonvastandheterogeneousdatasets.
Recentadvancementshaveseenvariousmultimodalmodelsthatleveragethisfoundation.Wangetal.[48]
developedasequence-to-sequenceframeworkthatunifiesdiversetasksacrossmodalitiesusinganinstruction-
basedtaskrepresentation,pretrainedonimage-textdataforbothcrossmodalandunimodaltasks.Similarly,Luet
al.[27]introducedatransformersequence-to-sequencemodelthatperformsavarietyofvisionandlanguage
taskswithoutrequiringtask-specificbranches,trainedonover90datasetsrelatedtovisionandlanguage.
Furtheringthemultimodalapproach,Singhetal.[41]createdamodelwithseparateencodersforimages,
text,andmultimodalintegration,whichwaspretrainedonunimodalandmultimodallossesfor35tasksacross
vision,language,andvision-languageareas.AnothercontributionbySinghetal.[49]includesamultimodal
vision-languagemodelwithasharedmultiwaytransformerbackbone,trainedonmaskeddatamodelingacross
modalities,achievingstate-of-the-artperformanceonvariousvisionandvision-languagebenchmarks.Liet
al. [26] explored cross-modal contrastive learning to unify representations across modalities with a unified
transformeronimageandtextdata,enhancingthesynergybetweenthesemodalities.
Inthisresearch,weharnessthedomainadaptationcapabilitiesofgenerativemodelstocustomizethesensor
decoderforspecificscenariosofsensordatarepresentation.Thisapproachisdesignedtosignificantlyenhance
thevisualizationqualityofthesensormodelâ€™soutputs.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:6 â€¢
3 RESEARCHMETHODS
ThissectionsystematicallydetailstheVCHARframework,commencingwithanarchitecturaloverviewand
operationalfunctionality.WebeginbyoutliningthefundamentalarchitectureandprincipalfeaturesofVCHAR,
providing a foundation for detailed exploration. This is followed by an in-depth analysis of the conceptual
frameworkthatformsthebasisofVCHAR,discussingboththeformulationoftheproblemitaddressesandthe
integrationofrelevanttheoreticalconcepts.Throughthisstructuredexposition,weaimtofurnishacomprehensive
understandingofVCHARâ€™sunderlyingprinciplesanditsfunctionalitywithinpracticalapplications.
3.1 OutlineoftheVCHARFramework
TheVariance-DrivenComplexHumanActivityRecognition(VCHAR)framework,asdepictedinFig.3,isan
end-to-endmodeldesignedtoenhanceboththepredictionandexplanationofcomplexhumanactivities.This
modeluniquelyemploystheKullback-Leibler(KL)divergencetoapproximatethedistributionofatomicactivities
acrossvariousslidingwindowlengths.Byleveragingavariance-drivenapproach,VCHARcircumventstheneed
forspecifictime-unitlabelsforatomicactivities,focusinginsteadonthetypesofactivitiesoccurringwithin
giventimeslots.Comparativeexperimentsdemonstratethatthisapproachsignificantlyenhancestheaccuracy
ofcomplexactivitydetectionrelativetoothermethods.
ThemultitaskdesignofVCHARnotonlyimprovestherecognitionofcomplexactivitiesbutalsoestablishes
an"interface"facilitatingdetailedvisualizationsbyagenerativedecoder.Forinstance,whenVCHARdetects
complexactivitiessuchasmakingasandwich,itconcurrentlyidentifiesrelatedatomicactivitieslikeopeninga
doororturningaswitch.Italsoprovidesalistofdesiredsensorrealtedinformationfromthemodel,highlighting
thesignificanceofeachsensorinthesmartspaceenvironment.
Tofurtherenrichthemodelâ€™soutput,aLanguageModel(LM)agentisintegratedtoreorganizeandelucidate
detailedinformationabouttheseactivities.Inpracticalapplications,weproposea"sensor-basedsmartspace
foundation model" framework, capable of being tailored to specific scenarios through advanced techniques.
TechniquessuchasDenoisingDiffusionImplicitModels(DDIM)andLatentDiffusionModels(LDMs),coupled
withamaskedtrainingstrategy,areemployedtorefinethequalityandrelevanceofthegeneratedoutputs.
ThisstructuredprocessensuresthatVCHARisnotonlyeffectiveinrecognizingcomplexhumanactivitiesbut
alsoadeptatprovidingactionableinsightsintothedynamicsofsmartspaces,therebyenhancinguserinteraction
andunderstanding.
3.2 Multi-TaskLearningforComplexActivityRecognition
Intheproposedmodel,multi-tasklearningisemployedtofacilitatethesimultaneousrecognitionofatomicand
complexactivities.Atomicactivitiesaredefinedasdiscreteactionsthatoccurwithinabrieftimewindowand
areindivisibleinthecontextofourdataset.Eachcomplexactivity,ontheotherhand,iscomposedofmultiple
atomicactivities.Specifically,acomplexactivityinourmodelisdefinedashavingmorethan2atomicactivities,
providingamorenuanceddescriptionofthegroupedatomicactivities.
Forinstance,withintheOpportunitydataset1,complexactivitiessuchas"makingcoffee"or"cleaningup"
mayincludetheatomicactivity"openthedoor".Thisactivityisfurtherdetailedas"openthedoor1"and"open
thedoor2",enrichingthemodelâ€™sunderstandingofthescenariobysupplyingdetailedcontextualinformation.
Thisapproachallowsthemodeltocapturetheintricaterelationshipsandrecurringpatternsamongactivities,
therebyenhancingitsabilitytoaccuratelyclassifycomplexactivityscenarios.
Ourmodelanalyzesrawsensordata,denotedbyx,topredicttheprobabilitiesofeachatomicactivityoccurring
within a specific sliding window, alongside the classification of a complex activity. The goal is to output a
probability vector p for atomic activities, where each elementğ‘ ğ‘– represents the likelihood of theğ‘–ğ‘¡â„ atomic
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:7
Complex Activity Data LLM agent The basic element of the CHAR model
General purpose sensor-based
Possible simple activities foundation model of CHAR
Sensor data encoder
Generative decoder of One shot tuning
Complex activities Sensor representation
for a specific dataset
Other information Complex activity descriptions
Fig.3. TheVCHARframeworkintegratesvarioustypesofsensordataandutilizesaLLMagenttostructurethisinformation
foragenerativedecoder.Theframeworkisdesignedtoenhanceusercomprehensionbyprovidingadditionalinsightssuchas
sensorchannelactivationvalues,offeringadeeperunderstandingoftheeventsoccurringwithinthesmartspace.
activityoccurring.Additionally,themodeloutputsacategoricallabelğ¶thatclassifiesthetypeofcomplexactivity
observed,whichintegratestheinformationfromtheatomicactivities.
Formally,xrepresentstheinputvectorofsensorreadings.Theoutputpisavectorofprobabilities,withlength
ğ‘›,whereğ‘›isthetotalnumberofatomicactivitiesthemodelistrainedtorecognize.Eachelementğ‘ ğ‘– inpisareal
numberbetween0and1,inclusive,indicatingtheprobabilitythattheğ‘–ğ‘¡â„
atomicactivityispresentinthesliding
window.Thecomplexactivitylabelğ¶,determinedbytheseprobabilities,providesahigh-levelclassification
basedonthepatternofatomicactivities.
Forinstance,ifthemodelconsidersfouratomicactivities,andaparticularobservationthroughthesliding
windowsuggestsvaryingprobabilitiesoftheseactivities,theoutputvectorpmightlooklike[0.95,0.80,0.10,0.05].
Thisvectorindicateshighprobabilitiesforthefirsttwoactivitiesandlowprobabilitiesfortheothers.Thelabelğ¶
thencontextualizestheseactivitiesintoacomplexactivityclassification,providingacomprehensiveunderstanding
ofthescenariocapturedbyx.
Theprimarytrainingobjectiveofourmodelistominimizealossfunction,ğ¿,whicheffectivelycombines
thelossesfrompredictingtheprobabilitiesofatomicactivities,ğ¿ ğ‘ğ‘¡ğ‘œğ‘šğ‘–ğ‘,andfromclassifyingcomplexactivities,
ğ¿ ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥.Thiscombinedlossfunctionisdefinedas:
ğ¿ =ğ›¼ğ¿ ğ‘ğ‘¡ğ‘œğ‘šğ‘–ğ‘(ğ‘ ğ‘ğ‘¡ğ‘œğ‘šğ‘–ğ‘,ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘’)+ğ›½ğ¿ ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥(ğ‘¦ ğ‘ğ‘œğ‘šğ‘ğ‘™ğ‘’ğ‘¥,ğ¶ ğ‘¡ğ‘Ÿğ‘¢ğ‘’)
Inthisformula,ğ›¼ andğ›½ areweightingcoefficientsthatbalancetheimportanceofeachcomponentduringthe
trainingprocess.Here,ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ representsthetruedistributionoftheatomicactivitiesoccurringwithinagiven
context,andğ¶ ğ‘¡ğ‘Ÿğ‘¢ğ‘’ istheactuallabelofthecomplexactivity.Thedual-focusofthislossfunctionencapsulates
the essence of our multi-task learning approach, promoting an efficient and robust learning process that is
well-suitedforanalyzingthenuanceddynamicsofsmartspacesensordata.Thismethodologynotonlyimproves
thepredictiveaccuracyofbothatomicandcomplexactivityclassificationsbutalsoensuresthatthemodelcan
effectivelydiscerntheintricaterelationshipsbetweentheseactivitylayers.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:8 â€¢
Thismodelingapproach,withitsprobabilisticoutputforatomicactivities,primarilyfacilitatesthedetection
andunderstandingofcomplexactivities,themainobjectiveofourmodel.Byanalyzingthelikelihoodsofvarious
atomicactivitieswithinagivencontext,ourmodelusestheseinsightsassupportivedatatoenhancetheaccuracy
andreliabilityofcomplexactivityclassification.Thismethodallowsforamorenuancedinterpretationofsensor
data,ensuringthatatomicactivitiesservetoinformandrefineourunderstandingofthebroader,moreintricate
behavioralpatternsrepresentedbycomplexactivities.
3.3 LossofAtomicActivityRecognition
Our modelâ€™s primary objective is to predict the probability of each atomic activity within a sliding window
ofsensordata,aligningthesepredictionsascloselyaspossiblewiththeactualprobabilitiesusingthemean
Kullback-Leibler(KL)divergenceasthelossfunction.TheKLdivergenceprovidesarobustmetricfortheaverage
differencebetweenthepredictedprobabilitydistributionğ‘ andthetrueprobabilitydistributionğ‘ ,making
predict true
itparticularlysuitablefordatasetswithvaryingclassdistributions.
Formally,thelossfunctionğ¿foratomicactivitiesisdefinedastheKLdivergencebetweenthetruedistribution
ğ‘ andthepredicteddistributionğ‘ ,whichcanbeexpressedmathematicallyas:
true predict
ğ¿ ğ‘ğ‘¡ğ‘œğ‘šğ‘–ğ‘ =ğ¿ ğ¾ğ¿(ğ‘ predict,ğ‘ true) = ğ‘1 âˆ‘ï¸ğ‘ ğ‘ true,ğ‘–log ğ‘ğ‘ true,ğ‘–
ğ‘–=1 predict,ğ‘–
where ğ‘ isthenumberofclassesoratomicactivities,ğ‘ true,ğ‘– andğ‘ predict,ğ‘– representthetrueandpredicted
probabilitiesoftheğ‘–ğ‘¡â„
atomicactivityoccurringwithintheslidingwindow,respectively.Thisformulationensures
that the modelâ€™s performance is evaluated based on the average divergence across all classes, promoting a
balancedsensitivitytotheaccuracyofeachclassprediction.
3.4 LossofComplexActivityRecognition
Ourmodelisspecificallydesignedtoclassifycomplexactivitiesbyminimizingthecross-entropyloss,which
measuresthediscrepancybetweenthepredictedprobabilitiesandtheactualclasslabelsforcomplexactivities.
Thislossfunctioniscrucialforoptimizingthemodelâ€™sabilitytoaccuratelycategorizecomplexactivitiesbased
onsensordatainputs.
Thecross-entropylossforcomplexactivityclassificationisformallydefinedas:
ğ‘€
âˆ‘ï¸
ğ¿ complex(ğ‘¦ predict,ğ¶ true) =âˆ’ ğ¶ true,ğ‘—logğ‘¦ predict,ğ‘—
ğ‘—=1
where,ğ‘¦ representsthepredictedprobabilitydistributionacrossthecomplexactivityclasses.ğ¶ isthe
predict true
one-hotencodedvectorofthetrueclasslabelsforthecomplexactivities.ğ‘€ isthenumberofpossiblecomplex
activityclasses.Minimizingğ¿ duringtrainingensuresthatthepredictedprobabilitiesaligncloselywith
complex
thetrueclasslabels,effectivelyenhancingthemodelâ€™saccuracyincomplexactivityrecognition.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:9
Feature Extraction Long-Term Multi-Task
Module Dependency Module Learning
. .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . AA ct to ivm itii ec s AC co tm ivp itl ie ex s
Loss 1: Loss 2: KL Div Loss Cross-Entropy Loss
Loss = Î± Ã— Loss 1 + Î² Ã— Loss 2
Fig.4. Forcomparativeanalysis,thesensorencoderstructurepredominantlyutilizestheConvLSTMmodule.Thisencoderis
designedtoidentifypossibleatomicactivitiesandtheirassociatedcomplexactivitiesoccurringwithinthesmartspace.
3.5 SensorEncoderArchitecture
Tobenchmarkourapproachagainstestablishedbaselinemethodologies[10,21,32,42],weemploythewidely
recognizedConvLSTMarchitecture,whichisparticularlyadeptathandlingsensortimeseriesdata.Thisarchitec-
turesynergisticallycombinesConvolutionalNeuralNetworks(CNNs)andLongShort-TermMemory(LSTM)
networkstoeffectivelyextractandtemporallyanalyzefeaturesfromsensordata.
TheConvLSTMarchitectureoperatesintwoprimaryphases:
(1) FeatureExtraction:TheCNNcomponentisresponsibleforspatialfeatureextractionfromeachtimeslice
ofthesensordata.Thisstepiscrucialforidentifyingintricatepatternswithinthedatathatarespatially
localizedbuttemporallyvariant.
(2) Temporal Dependency Modeling: The LSTM layer processes the sequence of extracted features to
capturetemporaldependenciesanddynamics,essentialforunderstandingtheprogressionandcontextof
sensorreadingsovertime.
Inourenhancedmodel,wefirstconductachannel-wiseanalysistoseparatelystudythefeaturesfromdifferent
sensorchannels.Thesefeaturesarethenintegratedusingasensorfusionmodule,whichsynthesizesinformation
acrosschannelstoprovideacomprehensivefeatureset.
Followingthefusionstep,abidirectionalLSTM(biLSTM)moduleisemployedtofurtherrefinethetemporal
analysis,enhancingthemodelâ€™sabilitytocapturebothforwardandbackwarddependenciesinthetimeseries
data.Additionally,aMulti-LayerPerceptron(MLP)moduleisincorporatedtogeneratethedistributionofatomic
activitiesbasedontheextractedfeatures.Finally,anotherLSTMlayeristailoredtomodelandpredictthecomplex
activityoutputs,synthesizingallprioranalysesintoacoherentactivityprediction.
ExampleApplication:Considerascenarioinvolvingthemonitoringofelderlyactivitiesinasmarthome
environment.Ourmodelprocessesdatafromvarioussensors(e.g.,motion,door,andapplianceusagesensors)
throughthedescribedarchitecture.Initially,individualsensorchannelsareanalyzedtodetectbasicmovements
and interactions. These are then fused and temporally analyzed to predict more complex activities, such as
cookingorcleaning,demonstratingthemodelâ€™scapabilitytodiscernnuancedhumanbehaviorseffectively.
Thiscomprehensiveapproachallowsustonotonlymatchbutalsosurpasstheperformanceofexistingmethods
incomplexactivityrecognition,asevidencedbyourcomparativeevaluations.Theresultsconfirmthesuperiority
ofourmodelinaccuratelydetectingandpredictingbothatomicandcomplexactivities,highlightingitspotential
forreal-worldapplicationinubiquitouscomputingenvironments.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
. . . . .. . . . .. . . . .. . . . . . . . . .. . . . .. . . . . . . . . .. . . . . . . . . .0:10 â€¢
Possible
Atomic Activities Initial Prompt LLM Agent Structured Prompt
Sensor Complex â€œAtomic Activities: $ â€œ...Someone is opening the
Encoder Activities Complex Activity: $ drawer, complex activity is
Sensor Activation: $â€ making a sandwich...â€
Sensor
Activations
Original Video
General-Purpose
One-Shot
Sensor-based
Tuning Vision-Language
Foundation Model
Pretrained
Image, Query, Atomic Activities, Complex Activity, Sensor Activation
Fig.5. Thecomplexactivitypromptlearningdecoderisengineeredtomasterkeyelementsincludingscenariodescriptions,
conceptrelationships,anddetailedactivityinsights.Thisdecoderisdesignedforadaptability,employingaone-shottuning
strategytoseamlesslyintegratewithspecificdatasets,therebyenhancingitsversatilityacrossvarioussettings.Theexample
showninthegraphsillustrateshowthisframeworkcanbeappliedtonewlyidentifiedactivities,suchas"cleanthetable,"
allowingthedecodertogenerateanintricatedescriptionof"makingsandwich"tailoredtothenewdataset.
3.6 GenerativeModelingforEnhancedComplexActivityRepresentation
For users lacking technical expertise, grasping the intricacies of sensor encoder outputs can be challenging.
Tobridgethisgap,weimplementagenerativemodelingapproachthattransformstheidentifiedatomicand
complexactivitiesintovisualnarratives.ThistransformationisfacilitatedbyaLanguageModel(LM)agent,
whichinterpretsthesensordata,encompassingthedistributionofatomicactivities,theclassificationofcomplex
activities,andthesensoractivationpatternswithinthemodel.
Toenhancetheadaptabilityofoursystemacrossvariousdatasetsandsmartspaces,weinitiallypre-train
ourmodelonadiversesetofscenarios.Thispretrainingencapsulatesauniversalconceptofrelationshipsand
fundamentalelementsessentialforactivityrecognition.Whenadaptingtoaspecificdatasetorsmartenvironment,
weemploythe"one-shottuningstrategy".Thisapproachfine-tunesthepre-trainedmodel,enablingittogenerate
tailoredexplanationsthatalignwiththeuniquecontextandrequirementsofthegivenapplication.
3.6.1 Pretraining a General-Purpose Sensor-Based Foundation Model Framework for CHAR. In line with the
developmentofrobustfoundationmodels,ourapproachinvolvespretrainingasensor-basedfoundationmodel
encapsulatingacomprehensivesuiteofelementscommontoawidearrayofscenarios.Thismodelservesas
aversatilestartingpoint,designedtobefine-tunedsubsequentlytoaccommodatethespecificitiesofdistinct
scenariosencounteredbyusers.
TheCHARfoundationmodelisimbuedwitharichlexiconthatdescribesamultitudeofcomplexscenariosand
humanactivities.Itnotonlycapturestheessenceofactivitypatternsbutalsoidentifiessensor-specificsignatures
thatarepivotalforaccurateactivitydetection.Thiscapabilityensuresthat,upondetection,themostrelevant
sensorsarehighlighted,providingintuitivevisualcueswithinthegeneratedexplanations.
Forinstance,thefoundationmodelispretrainedwithelaborateactivitynarrativessuchas"morningroutine",
"makingasandwich",or"preparingcereal".Thesenarrativesembodycomplexsequencessituatedwithinabroader
contextofatomicactivityrepresentations.Moreover,themodelisattunedtodiscernandemphasizesensorsthat
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:11
yieldsignificantinformationduringtheactivityrecognitionprocess.Thislevelofdetailfurnishesuserslacking
domainexpertisewithrich,contextualinsightsintothesensordataandthedetectedactivities.
Thefoundationmodelthusconstitutesapreparatorystepinbuildinganadaptable,knowledgeablesystemfor
CHAR.Throughfine-tuning,itcanswiftlyassimilateintoanydesignatedsmartspaceenvironment,delivering
bespokeexplanationstailoredtotheuniqueconfigurationandoperationofthatspace.
During the pretraining phase, the foundation modelâ€™s objective function aims to minimize the expected
discrepancybetweenthemodelâ€™spredictionsandthegroundtruth.Thefoundationmodelprocessesthreedistinct
inputs: the visual contentğ‘‰, represented by images or videos; the queryğ‘„, which contextualizes the visual
content;andthesensorencoderoutputğ¸,whichencapsulatesthesensor-derivedinformation.Theobjective
functionisthuscharacterizedbythefollowingminimization:
minE
ğ‘‰,ğ‘„,ğ¸âˆ¼D
[L(GT,Î¨
ğœƒ
(ğ‘‰,ğ‘„,ğ¸))]
Here,Î¨
ğœƒ
denotesthepredictivefunctionofthefoundationmodelparameterizedbyğœƒ,GTrepresentstheground
truthcorrespondingtotheinputs,andD signifiesthejointdistributionofthevisual,query,andsensordata.
Thisoptimizationensuresthatthefoundationmodeliscapableofintegratingandinterpretingmultimodalinputs
togenerateacomprehensiveandaccuratedepictionoftheobservedactivitiesinpreparationforfine-tuningto
specificsmartenvironmentapplications.
Inourapproach,ğ¸ symbolizesasetoffeaturesincludingthetypeofactivitiesidentified,thevaluesofsensor
activation,amongothers.Toenhanceinterpretability,weconvertthesefeaturesintoastructuredpromptusinga
predefinedtemplate.Forinstance,givenaninputvector [0.1,0.2,0.8,0.5] wherethethirdvalueindicatesthe
highestsensoractivationvaluefromthemodel,andknowingthatthecorrespondingsensorislocatedonthe
arm,ourtemplatehighlightsthissensorinyellowwithinthegeneratedvideo.Concurrently,themodelidentifies
thecomplexactivity,suchasâ€™makingcoffeeâ€™,andaatomicactivity,suchasâ€™openingadoorâ€™.Theseelementsare
wovenintoacohesiveprompt:"Someoneisopeningthedoor,complexactivity"makingcoffee"".Thisnarrative
integrationallowsforauser-friendlyrepresentationofactivities,facilitatingaclearandintuitiveunderstanding
oftheongoingeventsasdetectedbythesensorsystem.
3.6.2 One-ShotFine-TuningforComplexActivityDescriptionUsingDDIM. Whentransitioningourmodelto
specificscenarios,itiscommontoencountervariationsinthemanifestationofactivitiesandtheircorresponding
complexcontexts.Toaccommodatethesescenario-specificnuances,weadopta"one-shottuning"strategy[50].
Thisstrategyrapidlyrecalibratesoursensor-basedfoundationmodeltoalignwiththenewscenariocharacter-
istics.Thisisparticularlypertinentforincorporatingnewactivityvideoswherethecontextualdynamicsmay
significantlydiffer.
To further enhance the modelâ€™s adaptability and descriptive capabilities, we introduce a masked training
strategy.Thisapproachfacilitatesthemodelâ€™sabilitytogeneralizeacrossdiversedescriptivemodalities.During
fine-tuning,carefullydesignedpromptsaddressthefunctionalitiesofspecificallymaskedregionswithinthe
video.Thesepromptsserveadualpurpose:theyguidethevideogenerationprocessinthelatentspaceandensure
thatthemodelâ€™soutputisbothrepresentativeandspecifictothenewlyadaptedscenario.
Employingthismethodenablesthefoundationmodeltonotonlyrecognizeandadapttonewscenariosbut
alsotogenerateactivityrepresentationsthatarerichindetailandcontextuallyrelevant.
Latentdiffusionmodels(LDMs)[35]arepivotalinourapproach,functioningdirectlywithinthelatentspace
of new video embeddings. These models utilize an encoder to project videos into a latent space, facilitating
manipulationswithinthisreduceddimensionalitybeforereconstruction.Specifically,anencoderE mapsavideo
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:12 â€¢
ğ‘‰ ontoalatentrepresentationz=E(ğ‘‰),andadecoderD subsequentlyrestoresthislatentrepresentationback
tothevideodomain,approximatingthetargetvideo,i.e.,D(E(ğ‘‰)) â‰ˆğ‘‰.
Inourcase,whennewactivitytypesareintroducedtothemodel,theLDMsadeptlyhandlethedesignated
maskedregionswithinthelatentembeddings.ThedecoderDisthenemployedtoregeneratethevideo,ensuring
thatitaccuratelyreflectsthecomplexactivitytypeandsensoractivationvalueinsideofthemodel.
minâˆ¥ğ‘‰ âˆ’D(E(ğ‘‰))âˆ¥2
E,D
DDIM,theDenoisingDiffusionImplicitModel,isintegraltoenhancingtheefficiencyofthevideogeneration
process,particularlyinmaintainingthestructuralintegrityoftheactivitymovements[44,50].Inourimplementa-
tion,theDDIMoperateswithinthelatentspaceofanewvideoâ€™sembedding.Theprocessutilizespromptsthat
encapsulatemaskedpatterns,denotedbyğ‘‡,alongsideaguidingpromptğ‘‡âˆ— fordirectedgeneration.Thefinal
videoVisproducedbyfirstinvertingthelatentembeddingviaDDIM ,usingtheencoderâ€™soutputinformedby
inv
themaskpatternandguidingprompt,followedbysamplingwithDDIM .Thissequencecanberepresented
samp
mathematicallyas:
V=D (cid:0) DDIM (DDIM (E(V ),ğ‘‡,ğ‘‡âˆ—))(cid:1),
samp inv 0
Here,E signifiestheencodingfunction,D thedecodingfunction,andV theinitialvideoinput.Theinclusion
0
ofbothğ‘‡ andğ‘‡âˆ—ensuresthattheDDIMnotonlycapturesthemaskedpatternsbutalsoadherestothespecified
activityprompts.Thisdualguidancemechanismfacilitatesprecisecontroloverthegenerativeprocesswithinthe
latentspace,yieldinganoutputvideothataccuratelyreflectsthedesiredactivities.Byoperatinginthisfashion,
ourmodelnotonlyretainsthefidelityoftheoriginalvideobutalsoenrichesitwithdetaileddescriptionsof
activities,providingacomprehensivedepictionthatencompassesboththeobservedandinferredaspectsofthe
scene.
MaskedtrainingcanmarkedlyexpeditethetrainingofsubstantialdiffusionmodelssuchasStableDiffusion[54],
withoutdetrimenttotheirgenerativecapabilities.Tohastenthelearningprocess,weemployamaskedstrategy
tailoredtospecificpatternsdesiredforoursensordescriptions.Forinstance,asdepictedinFigure6,theoutput
framerateissetat24fps,enablingustodirectthemodelâ€™soutputtowardvaryingscenarios.Additionally,a
humanstudyisconductedtofurtherevaluatetheeffectivenessofthesestrategies.
3.6.3 ImplementationDetails. Ourdecoderleveragesthestablediffusionmodel[36],utilizingonlinepretrained
weights.Despitethis,specificpatternscrucialforrepresentingcomplexscenariosinsmartspacesareabsent,
necessitatingfurtherpretrainingofthesemissingconceptspost-weightloading.Ourpretraininginvolvesadjusting
allmodelweights,whereasfinetuningfocusesonlyonseveralattentionlayerstoemployaone-shottuning
strategy[50],significantlyreducingGPUmemoryusage.ThisefficiencyallowstheuseofstandardgamingGPUs,
liketheRTXseries.
Specifically,thecompletemodelrequires10,000stepsforpretrainingtoincorporatenewfeatures,andonly500
stepstofine-tuneforvideoapplications,withabatchsizeofone.Forinference,theDDIMsamplerrequires100
steps.WeutilizeanNVIDIAA100GPUforpretrainingandanRTX6000forfine-tuning.Theentirepretraining
processtakesapproximately48hourstointegrateanewfeature,whilefinetuningiscompletedinjust30minutes.
Thisrapidadaptabilityisparticularlybeneficialforindividualizedsmartspaceapplications,whereconventional
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:13
Masked guidance strategy for predefined
representation of one-shot tuning
Fig.6. Weintroduceamask-guidedone-shottuningstrategydesignedtoseamlesslyadaptthemodeltonewlyadded
activities.Exampleisfromopportunitydataset.
GPUscaneffectivelyhandletheadaptationofnewfeaturestothemodel.Suchcapabilitiesrenderourframework
practicalforreal-lifeimplementations.
4 EXPERIMENTSANDRESULTS
Inthissection,wedetailthepublicdatasetsutilizedinourexperiments,providingafoundationforacomprehensive
evaluationofourmethodology.Wewillsystematicallydiscusstheoutcomesofeachexperiment,highlightingthe
efficacyofourmodelacrossdifferentscenarios.Additionally,resultsfromhumanstudieswillbepresentedto
demonstratethepracticaleffectivenessanduserperceptionofourmodel.Thisapproachallowsustopresenta
well-roundedassessmentofourframeworkâ€™sperformanceanditsapplicabilityinreal-worldsettings.
4.1 Dataset
Forourexperiments,weutilizedthreepubliclyavailabledatasets:Opportunity[9],FallAllD[37],andCooking[25].
Eachdatasetoffersuniquecharacteristicssuitedtotestingtheversatilityofourmodelunderdifferentconditions.
TheOpportunitydatasetprovidelabelsforatomicactivitiescorrespondingtospecifictimeintervals,facilitating
preciseactivityrecognitiontests.Incontrast,theCookingandtheFallAiddatasets,whichreflectreal-lifescenarios,
labelsonlythetypesofatomicandcomplexactivitieswithineachtimeintervalwithoutspecifyingtheexact
timingofeachatomicactivity.Thisdatasetmerelyindicatesthatagroupofatomicactivitiesoccursduringthe
specifiedintervals,presentingachallengeindistinguishingindividualactionswithintheseperiods.
ComplexActivities AtomicActivities
MakingCoffee OpenDoor1,CloseDrawer1,OpenDoor2,OpenDrawer2
MorningRoutine CloseDoor1,CloseDrawer2,CloseDoor2,OpenDrawer3
CleaningUp Drink,OpenFridge,CloseFridge,CleanTable,OpenDishwasher
MakingSandwich CloseDrawer3,,CloseDishwasher,ToggleSwitch,OpenDrawer1
Table1. Complexactivities andtheir atomicactivities inthe Opportunity dataset. Thedataset presents challengesin
identifyingactivitiesduetothepresenceofsimilaratomicactivitieswithdifferenttypesandthepossibilityofthesame
atomicactivitybelongingtomultiplecomplexactivities.Thishierarchicalandoverlappingnatureofactivitieshighlightsthe
complexityofactivityrecognitioninreal-worldscenarios.
4.1.1 Opportunity. TheOpportunitydatasetisapubliclyaccessiblebenchmarkforhumanactivityrecognition
algorithms,featuringdatafrom4subjectsoutofanoriginal12[11].Itincludes15networkedsensorsystems
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:14 â€¢
with72sensorsacrosstenmodalitiesembeddedintheenvironment,objects,andwornonthebody.Thisstudy
particularlyfocusesoninertialsensorsplacedontheleftlowerarm,leftupperarm,rightlowerarm,rightupper
arm,andbackofthetorso,whichrecorddataviaaccelerometers,gyroscopes,andmagnetometers.
Weanalyze17micro-activitiesand4complexactivitiesthatofferacomprehensiveviewofhumanmotion(table
1).Thedatasetâ€™sdesignensuresarealisticportrayalofactivities,withannotationsatmultipleabstractionlevels
suitablefortestingadvancementsinsensorselection,featureextraction,classifiertraining,multimodaldata
fusion,segmentation,andhierarchicalactivityrecognition.ThisrichframeworkmakestheOpportunitydataset
anexcellentresourceforevaluatingtheeffectivenessofdifferentactivityrecognitionstrategiesundernaturalistic
conditions.Weuse3subjectastrainingset,1subjectastestingset.Inthisstudy,weutilizea20-secondsliding
windowapproachfordataprocessing,whichframesourtrainingsetwithdimensionsof(3036,72,600)andour
testingsetas(1299,72,600).Thisstrategyisinstrumentalincapturingthetemporaldynamicsessentialforour
activityrecognitiontasks.
ComplexActivities AtomicActivities
Fallingprediction stand,walk,sit,trip
NormalADLs,noFallingdetected recovery,jog,slip,rotate
Table2. ComplexactivitiesandtheiratomicactivitiesintheFallaiddataset.Thedatasetprovidesinformationaboutthe
typesofatomicactivitiesandtheirhigh-levelactivities.However,itdoesnotincludespecificlabelingfortheatomicactivities
ateachtimeunit.Someatomicactivities,suchas"walktothechairandsit"maybelongtobothfallingpredictionandADL
categories,highlightingthehierarchicalandoverlappingnatureoftheactivities.Thisposeschallengesinaccuratelylabeling
andrecognizingactivitiesatagranularlevel.
4.1.2 FallAllD. TheFallAllDdatasetisaspecializedresourcetailoredforresearchinfalldetection,fallprevention,
andhumanactivityrecognition,suitableforbothclassicalanddeeplearningmethodologies.Datacollection
involvedthreeidenticaldata-loggerswornbyparticipantsontheneck,wrist,andwaist,eachoutfittedwithan
inertialmodule(comprisinganaccelerometer,gyroscope,andmagnetometer)andabarometer.
Datawerecollectedfrom15participantsagedbetween21to53yearsold,resultingin26,420files,each20
secondsinduration.Withinthisdataset,complexscenariossuchas"fall"and"nofall"detectionarehighlighted,
whereatomicactivitiesareindicativeofbothscenarios.Forexample,activitiesmightincludenormalactions
suchaswalkingandsitting,aswellasthesameactionsperformedwithanaccidentalfall.Forourstudies,we
selected8(table2)activitiesthatspanbothcomplexcategories,allocatingaround20%ofthedatafortestingand
80%fortrainingpurposes.Inourmethodology,weadopteda10-secondslidingwindowfordatasegmentation,
resultinginatrainingsetdimensionalityof(4964,12,4760)andatestingsetdimensionalityof(1375,12,4760).
Thiswindowingtechniqueiscriticalforcapturingtemporalpatternsinthesensordataconducivetorecognizing
complex activities.This dataset does not include specific labels for atomic activities. Each file only provides
informationonthetypeofactivities,devicetype,recordingdate,andsubjectnumber.
4.1.3 CookingActivity. TheCookingActivityRecognitionChallengedataset[25]isamultifacetedcollection
ofsensorydata,recordedviasmartphones,wristwatches,andmotioncapturesystems,designedspecifically
forthecomplextaskofactivityrecognition.Thisdataset,procuredfrom4subjects,comprisesaccelerometer
datafromtherightarm,lefthip,andbothwrists,inadditiontomotioncaptureinformationfrom29distinct
markers.Itcategorizesactivitiesintothreemacroactivitiesâ€”makingasandwich,preparingafruitsalad,and
cerealpreparationâ€”alongside10microactivitiessuchasadding,cutting,mixingand"other".Inourrecognition
task,wefocusedonninespecificactivitiesthatwererelevanttoourstudy.Weexcludedthe"other"category
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:15
ComplexActivities AtomicActivities
MakingSandwich add,cut,mix
MakingaCereal open,peel,pour
MakingFruitsalad put,take,wash
Table3. ComplexactivitiesandtheirassociatedatomicactivitiesintheCookingdataset.Thedatasetrepresentsreal-life
scenariosandprovidesinformationaboutthetypesofatomicactivitiesandtheircorrespondingcomplexactivities.However,
itdoesnotincludespecifictimelabelsforeachactivity.Thesameatomicactivitymaybelongtodifferentcomplexactivities,
highlightingtheversatilityandreusabilityofactionsacrossvariouscookingtasks.Thislackoftemporalinformationandthe
presenceofoverlappingactivitiesposechallengesinpreciselyidentifyingandsegmentingindividualactivitieswithinthe
dataset.
fromouranalysis,asitdidnotprovidemeaningfulinformationforourresearchobjectives.Thedatasetâ€™sprimary
complexitiesarisefrominconsistenciesinsamplingratesacrossdifferentsensorsandsignificantinstancesof
missingdata,notablywithintheleftwristwatchaccelerometerdata,whichshowsseveralintervalsdevoidof
readings.Furthermore,thedatasetâ€™srecordings,spanningacrosstheaccelerometersandmotioncapturesystem,
demonstratevaryingsamplingrateswithinthe30-secondsegments.Specifically,accelerometerdatasampling
ratesfluctuatebetween50and100Hz,whilethemotioncapturedataisconsistentlysampledataround100Hz.
Thesechallengesunderscoretheneedforrobustprocessingtechniquescapableofhandlingasynchronousdata
andcompensatingforinformationalgapswithinthedataset[1].
Inourstudy,weutilizesensordatarecordedfromthearm,hip,andwrist,whichserveastheprimarydata
inputsforourmodel.Toensureuniformityacrossthedataset,wehaveinterpolatedallsensorreadingstoa
consistentsamplingrateof100Hz.Notably,wedonotincorporatemotioncapturedataintoouranalyses.Given
thedatasetâ€™schallenges,includingmissingreadingsandinconsistentrecordingsessions,wehavechosento
discretizeactivitieswithinthesamesubject.Thisapproachyieldsavarietyofactivitysequences,suchas"add,
cut,open,makingcereal"or"cut,take,take,makingcereal,"eachrepresentingdifferentcombinationsofmicro
activitiesthatleaduptoamacroactivity.Thedatasetpresentsvariabilitynotonlyinactivitysequencesbut
alsoinsensoravailabilitypersession,withsomesubjectslackingconsistentsensordataacrossrecordings.To
addressthesediscrepanciesandtheissueofmissingdata,westructuredthedatasettoensurediversityinthe
trainingandtestingsets,withdistinctactivitycombinationsandpreviouslyunseenactivitytypes.Itisimportant
tonotethatthedatasetprovidesonlyageneralindicationofatomicactivities.Hence,mostCHARmodelsthat
relyontime-specificatomicactivitylabelsorsequentiallabelingarenotsuitableforthisdataset.Ourapproach,
tailoredforthistypeoflooselylabeleddata,allowsforrobustactivityrecognitiondespitetheabsenceofprecise
temporalannotations.Inlightoftheinseparablenatureofthesegmentswithinthedataset,wehavepreserved
theoriginal30-secondrecordingwindowsizetomaintaintheintegrityofthedata.Consequently,theshape
ofourtrainingsetis(2326,12,3000),indicatingthatitcomprises2326segments,eachwith12features,across
3000timesteps.Similarly,thetestingsetisstructuredas(711,12,3000),consistingof711suchsegments.This
configurationensuresthatthedatasetâ€™soriginalstructureisretained,allowingforanauthenticevaluationofour
modelâ€™sperformance.
4.2 Evaluation
4.2.1 AtomicAccuracyScore. Toassesstheprecisionofourmodelindetectingatomicactivities,weutilizethe
AtomicAccuracyScore.Thismetricmeasurestheproportionofatomicactivitiescorrectlyidentifiedabovea
specifiedconfidencethresholdğ›¼ relativetothetotalnumberofactivitiesdetected.Thescoreismathematically
definedas:
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:16 â€¢
AtomicAccuracy=
(cid:205)ğ‘› ğ‘–=1ğœ’(ğ‘
ğ‘–
>ğ›¼)
ğ‘›
whereğ‘
ğ‘–
denotestheconfidencescoreoftheğ‘–-thdetectedactivity,ğ‘›isthetotalnumberofdetectedactivities,
andğœ’istheindicatorfunctionthatreturns1ifğ‘
ğ‘–
>ğ›¼,and0otherwise.Inpracticalterms,wesetğ›¼ =0.4,ensuring
thatonlyactivitiesdetectedwithaconfidencelevelabove40%areconsideredintheaccuracymeasurement.This
strategyfocusesonthemostreliablydetectedactivities,thusprovidingamoremeaningfulassessmentofthe
modelâ€™sperformanceinsmartspaceenvironments.
4.2.2 ComplexActivityF1score. Intheevaluationofourmodelâ€™sperformanceoncomplexactivityclassification,
theF1scoreisemployedasacriticalmetric.TheF1scoreisaharmonicmeanofprecisionandrecall,providinga
balancedmeasurethatconsidersboththefalsepositivesandfalsenegatives.Thisisespeciallyimportantinour
contextwheresomecomplexactivitiesmaybeunderrepresentedinthedataset.
TheF1scoreiscalculatedasfollows:
PrecisionÃ—Recall
F1Score=2Ã—
Precision+Recall
4.2.3 ModelComparison. Inthisstudy,weevaluatetheperformanceofourproposedmodelagainstseveral
establishedbaselinesinthefieldofCHAR.WeprovideadetaileddescriptionofourmodelinSubsection3.5.For
afaircomparison,wedesignthebaselineswithsimilarstructurestoensurethateachcomparisonmodelhas
approximatelythesamenumberofparameters.WeemploytheAdamWoptimizerfortraining,withamaximum
of300epochs.Thefollowingsubsectionsdetailtheconfigurationsofeachcomparisongroup:
â€¢ ConvLSTM:OftenconsideredafoundationalarchitectureforaddressingCHARproblems[17,32,43,47],
theConvLSTMservesasabaselinetoevaluatetheenhancementsourmethodbringstoactivityrecognition.
ThismodelintegratesasensorfusionmoduleutilizingCNNtoextractprimarysensorfeatures,combined
withLSTMnetworkstocapturetemporaldependenciesamongthesefeatures.Thearchitectureculminates
inatemporalconvolutionlayerfollowedbyadenseoutputlayer,whichcollectivelyaimtooptimizethe
recognitionofcomplexhumanactivities.
â€¢ ConceptBottleneck[23]:UtilizesaCNN-basedstructuretodetectatomicconceptsbeforeadvancingto
higher-levelconcepts,applyingMSElossforconceptidentification.Thishierarchicalapproachispossible
toappliedtocomplexactivityrecognition.
â€¢ PEMM:ThePointwiseErrorMinimizationMethod(PEMM)isutilizedasacomparativebaselineinour
study.ItaddressespotentialconcernsthatConceptBottleneckoutcomesarelimitedbyrelianceonCNN
architectures.Toevaluateourmodelâ€™sperformanceenhancements,weincorporateMSEinanablation
study,contrastingitwithPEMMtohighlighttheeffectivenessandadvancementsofourapproach.
â€¢ Debornair[10]:ThismodelparallelsourbasicConvLSTMstructurewithadistinctpreprocessingmodule
thatprocessessensordataforatomicandcomplexactivitiesseparatelybeforemergingthem.Debornair
exclusivelydesignforcomplexoutputonly,focusingsolelyontheintegrationofprocesseddatawithout
predictingatomicactivities.
â€¢ XCHAR[21]: Based on a vanilla ConvLSTM architecture, XCHAR differentiates itself by employing
CTC(Connectionist Temporal Classification) loss to emphasize the importance of sequence in atomic
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:17
activities.However,real-lifedatasetsoftensufferfromerrorsoromissionsinthesequencingofatomic
activities,limitingtheapplicabilityofthismodeltocertaindatasetswhereprecisesequencelabelingis
feasible.
Opportunity Cooking FallAllD
Model
CHARF1Score AtomicAcc.Score CHARF1Score AtomicAcc.Score CHARF1Score AtomicAcc.Score
PEMM 0.7321 0.5332 0.7838 0.4456 0.8217 0.7768
ConvLSTM 0.7922 â€“ 0.8135 â€“ 0.8393 â€“
ConceptBottleneck 0.6479 0.4031 0.4355 0.3728 0.5966 0.4725
XCHAR 0.8357 0.6736 â€“ â€“ â€“ â€“
DEBONAIR 0.8015 â€“ 0.8128 â€“ 0.8283 â€“
VCHAR 0.8463 0.6052 0.8198 0.5209 0.8657 0.8153
Table4. ComparativeAnalysisofCHARF1ScoresandAtomicAccuracyAcrossModelsandDatasets.Notallresultsare
availableduetocertainmodelsâ€™incompatibilitywithdatasetsthatlackspecifictimeorsequencelabeling,whichisessential
fortheirapplicationinreal-worldsettings.
Inourstudy,weutilizedavariance-basedbaselinetoassessourmodelâ€™sperformanceinrecognizingcomplex
activities. As shown in Table 4, our method surpasses other models with CHAR F1 Scores of 0.8463 on the
Opportunitydataset,0.8198ontheCookingChallengedataset,and0.8657ontheFallAllDdataset,demonstrating
itseffectivenessincomplexactivityrecognition.Nevertheless,themodelencounterschallengesinprecisely
detectingatomicactivitieswhenspecificlabelsareavailable,recordingaslightlylowerAtomicAccuracyscoreof
0.6052,incomparisontoXCHARâ€™s0.6736ontheOpportunitydataset.Whileourapproachmainlyfocusesin
complexactivitydetection,furtherinvestigationintoitscapabilitiesforatomicactivityrecognitionremainsa
pointofinterest.
Remarkably,ourmodeldemonstratesasignificantadvantageinenvironmentswhereexacttimelabelingis
absent,suchastheCookingdataset,whereourmethodachievesthehighestCHARF1Scoreof0.8198andAtomic
Accuracyof0.5209.Thissuggeststhatourvarianceapproachadapatallyhandlesdatasetslackingdetailedtemporal
annotationsbetterthanmethodsrelyingonpoint-wiseerrorminimizationlikeMSEloss,whichperformswell
underconditionsofpreciselabelingbutstrugglesotherwiseduetoitsinherentneedforaccuracyinindividual
assessments.TheperformanceontheFallAIIDdatasetmirrorsthetrendobservedintheCookingdataset.
Moreover,thecomparativeperformanceinthetablerevealsthatsomemodels,includingPEMMandConcept
Bottleneck,compromisecomplexactivitydetectioninfavorofatomicactivityrecognition.Forinstance,while
PEMMandConceptBottleneckaredesignedtoimproveatomicdetail,theyfallshortinoverallCHARF1Scores
comparedtovanillaConvLSTM,underscoringatrade-off[15]thatourmethodavoids.Ourmodelâ€™smulti-task
recognitioncapabilitydoesnotsacrificethequalityofcomplexactivitydetection,affirmingitsbalancedapproach
insimultaneoustaskmanagement.
Thenormalizedconfusionmatricesillustratedistinctperformancemetricsacrossdatasets.TheFallAllD(fig9)
datasetdemonstratesrobustdetectionacrossallcomplexactivities.FortheOpportunity(fig7)dataset,prediction
scoresforeachcomplexactivityexceed0.8,withthe"earlymorning"activityachievingthehighestaccuracyat
0.97,whereasthe"cleanup"activityrecordsthelowestat0.8.Incontrast,theCookingChallenge(fig8)dataset
revealsasuboptimalperformancewiththe"makingcereal"activityscoringonly0.66,thoughotheractivities
maintainscoresabove0.8.
4.3 EmpiricalStudiesofExplanationUnderstandability
Inourefforttodevelopauser-friendlyframeworksuitableforeverydayusebylaypersons,weconductedhuman
evaluationstocompareourmethodagainstexistingapproaches.Theseevaluationsincludedallmethodstested
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:18 â€¢
Fig. 7. Confusion Matrix of Opportu-
Fig.8. CookingChallengedataset,the Fig.9. FallAllDdataset,thelabelsare:
nitydataset,thelabelsare:A:â€™Coffee
labels are: A: â€™Making sandwichâ€™, B: A:â€™non-FallnormalADLsâ€™,B:â€™Falldetec-
timeâ€™,B:â€™Earlymorningâ€™,C:â€™Cleanupâ€™,
â€™Makingfruitsaladâ€™,C:â€™Makingcerealâ€™ tionâ€™
D:â€™Sandwichtimeâ€™
acrossthethreedatasetsusedinourquantitativeexperiments.Ourevaluationcomprisedtwodistinctgroups.The
firstgroupassessedtheclarityanduserpreferenceforthemodelâ€™soutputrepresentation,specificallyfocusingon
howeffectivelythemodelâ€™spredictionsaredescribedandunderstoodbyusers.Thesecondgroupofevaluations
aimedtodemonstratetousershowthemodelprocessesdifferenttypesofdatatomakedecisions.Thesestudies
weredesignedtoascertainwhichtypesofexplanationsregardingthemodelâ€™sdecision-makingprocessesare
mostaccessibleandfavoredbyuserswithouttechnicalexpertise.
Inourhumanstudy,weutilizedthreedistinctdatasets,eachevaluatedby100participantswhorespondedto6
questions,totaling1,800effectiveresponsesacrossalldatasets.Theparticipantswererecruitedpredominantly
fromonlineplatforms,withnospecificbackgroundprerequisites.Somechosetoremainanonymous.Educational
backgroundsvariedamongparticipants:aboutone-thirdhadahighschooldiploma,othersheldundergraduate
orgraduatedegrees.Agesofparticipantsrangedfromtwentytofiftyyearsold.Thequestionswererandomly
selectedfromtwocategoriesrelatedtothemodelâ€™soutputs:threequestionsfocusedonuserpreferencesforoutput
representation,andthreeonexplanationsofmodeldecisions.Participantswereaskedtoratetheirsatisfactionon
amodifiedDecimalLikertScale[51]rangingfrom1to5,withoptionstoselectintermediatevaluessuchas1.25,
1.5,1.75,etc.,assessingtheextenttowhichthemethodsenhancedtheirunderstandingofthemodelâ€™soutputs,
especiallyforthosewithoutexpertise.Thisenhancedscaleprovidesfinergranularityincapturingnuancesin
participantresponses.Thiscomprehensiveevaluationaimstoassessvariousscenariosandinputtypestobetter
understanduserinteractionwiththemodel.
4.3.1 ActivityRecognitionDescription. WeassesstheVCHAR,DeXAR,andConceptBottleneckmethodsfor
complexactivityrecognition,focusingontheirabilitytorepresentresultseffectivelywithindatasetscharacterized
bysparselabeling.
â€¢ DeXAR:[3]Initiallydesignedforatomicactivityrecognition,wehavemodifiedthemethodtosimulta-
neouslyestimatebothatomicandcomplexactivities.ThisadaptationincorporatesanNLP-basedvisual
representationtodepictthemodelâ€™srecognitionresults.Inourexample,asshowninFigure11,thesemantic
visualizationemploystheDexarencodingmethodtorepresentlevelsofconfidencethroughcolorvariations.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:19
Darkercolorsindicatehigherconfidence.Thisvisualizationillustratesthepotentialtimeintervalsestimated
bythemodelforeachatomicactivity,accompaniedbytextualexplanations.However,itdoesnotprovide
descriptionsforcomplexactivities.
â€¢ Concept Bottleneck: [23] It merely indicates whether atomic activities are detected or not, without
providing detailed information about the recognition results (fig 10). The output consists only of text
descriptions,makingitstraightforwardandconcise.Nonetheless,thisapproachnecessitatesthatusers
possessbasicreadingskills.InFigure10,weobservethatthecomplexactivityisdescribedsolelythrough
textualmeans.Whilesomeusersmayfindthisapproachstraightforwardandintuitive,itessentiallyinvolves
asequenceofactionsâ€”suchasopeningandclosingadoor,togglingaswitch,anddrinkingâ€”thatcollectively
signifyacomplexactivity,inthisinstance,â€™makingcoffeeâ€™.
â€¢ VCHAR: In Figure 13, the VCHAR system is illustrated, showcasing its capability to represent each
atomicactivitywithacorrespondingvideoandadescriptivelabelâ€”inthisexample,â€™makingcerealâ€™.The
width of the video segment indicates the estimated time interval during which the activity occurs, as
detectedbyVCHAR.Boththeatomicandcomplexactivitiesarelabeledatthetopofthegraph.VCHARis
specificallydesignedtoaddressissuesoflabelsparsity.Thetimeintervalestimationisbasedontheweights
connectingaspecificlast-layerneurontoallneuronsinthetimeserieslayer,allowingforacomprehensive
representationthatintegratesvisualandtextualdescriptionsforeachactivity.
AsillustratedinFigure13,VCHARâ€™srepresentationreceivesthehighestmedianevaluationscoresacross
thedatasetsâ€”3.88forOpportunity,4.37forCookingChallenge,and4.5forFallaid.Thesescoresdemonstratea
clearpreferenceamongusersformoredetailedanddescriptiveoutputstobetterunderstandcomplexscenarios,
especiallyineverydaycontexts.ThedistinctadvantageofVCHARisitsabilitytoconveyintricatesensordata
interactionsinamannerthatisintuitiveforlaypersons.Thispreferenceunderscorestheimportanceofdesigning
AIsystemsthatnotonlyperformwellbutalsocommunicatetheirprocessesandresultsinwaysthatenhance
usercomprehensionandtrustintechnologyapplications.
Atomic Activities:
Complex Activity:
Openthedoor, Close the door,
Making Coffee
Toggle the switch, Drink
Fig.10. ExampleofConceptBottleneckmethodforvisionrepresentationincomplexactivityrecognition.
4.3.2 Model Explaination. Another aspect of our evaluation focuses on illustrating to users how the model
processesvariousdatatypestoarriveatdecisions.Weassessedthreedistinctapproaches:ourproprietarymethod,
amodel-agnosticmethod,andamodel-transparentmethod.Thiscomparativestudyseekstoexplorehowdifferent
modelingapproachesinfluenceuserpreferences,particularlyintermsofmodelinterpretabilityanditsimpacton
usersatisfaction.Additionally,weanalyzevariousexplanationmethodstoidentifywhichsensorsarecriticalfor
recognizingatomicactivities,furtherenhancingourunderstandingofeachmethodâ€™seffectivenessinpractical
scenarios.
â€¢ Grad-CAM:[28]Grad-CAMisamodel-transparentmethodthatcalculatesthegradientofatargetconcept
(output)relativetothefeaturemapsofadesignatedlayer.Itproducesaheatmapthatidentifiesthecritical
sensors in higher layers that are pivotal for class prediction. As illustrated in Figure 14, it shows how
varioussensorscontributeatdifferentintensitiestoaparticularactivity,withbrighterareasindicating
greaterimportance.
â€¢ SHAP [8] In contrast, SHAP is a model-agnostic method that approximates the relationship between
inputs and outputs without probing the modelâ€™s internal mechanisms. It focuses on representing the
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:20 â€¢
Fig.11. DeXARâ€™ssemanticimageforcomplexactivitydetection,highlightingpossibleatomicactivitypredictionintervals
withaheatmap.
Add food, Cut food, Mix food
Complex activity Ã Making cereal
Fig.12. VCHARdepictsatomicandcomplexactivitieswithvideos,usingvideowidthtoapproximateactivityprediction
timing.Theexampleisfromcookingchallengedataset
sensor signals in the input time series data. SHAP calculates scores by estimating the impact of each
featureontheprediction,usingShapleyvaluesfromcooperativegametheorytoquantifyeachfeatureâ€™s
contribution.ShapleyvaluesinSHAPcanbepositiveornegative,dependingonwhetherafeaturecontributes
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:21
Opportunity Cooking Challenge Fallaid
6 6 6
5 5 5
4 4 4
3 3 3
2 2 2
1 1 1
0 0 0
Concept Bottleneck DeXAR VCHAR Concept Bottleneck DeXAR VCHAR Concept Bottleneck DeXAR VCHAR
Fig.13. Humanevaluationoftheoutputdescriptionfordifferentmodeltypes
positivelyornegativelytothemodelâ€™spredictionforaparticularinstance,relativetotheaverageprediction.
Thismethodhighlightscriticalregionsrelativetothepredictedclass,asillustratedinFigure15.Giventhe
inherentcomplexityofsensorsignals,simpledescriptionsofinputmaynotfullycapturetheirsignificance.
â€¢ VCHARVCHARdeliversadetaileddepictionofsensorcontributionsbyintegratingbothtextualand
visualrepresentations.AsshowninFigure16,itvisualizesascenariowhereapersonisattemptingto
openafridge,withthemostsignificantsensoractivationvalueontheleftfoot,whichVCHARhighlights
inthevisualization.Additionally,thesensorvaluesarederivedfromgradientvaluestiedtotheselected
activitytype,akintoGrad-CAMâ€™smethodologybutextendedtoincludebothatomicandcomplexactivities.
VCHARnotonlyidentifiestheseactivitiesbutalsolabelstheminthevisualrepresentation,enhancingits
analyticalcapabilitiesfordetailedactivityanalysis.
FromtheresultsdepictedinFigure17,VCHARdemonstratessuperiorperformancewithmedianscoresof
4.2,3.8,and4.5,alongwithalowervariance.Interestingly,ourresultsarecomparabletothoseofGrad-CAM.
Grad-CAMscoredlowerthanSHAP,whichprimarilystemsfromSHAPâ€™sapproachofexplainingoutcomesbased
ontime-seriesdata,thusprovidingamoredetailedinformationalcontext.Despitethis,ourmethod,whichalso
employsamodel-transparentapproachakintoGrad-CAM,receivedhigherscores.Thissuggeststhatlaypersons
maybenefitfrommoredetailedexplanationsprovidedbyourmodel,ascomparedtothoseusedbyexpertusers.
5 CONCLUSIONS
Inthispaper,weintroduceVCHAR,avariance-basedmethodspecificallydesignedtoaddresslabelsparsityissues
inin-the-wilddatasets.VCHARiscapableofsimultaneouslydetectingbothcomplexandatomicactivities,without
compromisingtherecognitionrateofcomplexactivities.Ourresultsdemonstrateaperformanceimprovement
overotherbaselinemethods.Additionally,wepresentanoveldecoderthattranslatesthemodelâ€™soutputsinto
avisualrepresentation.Thisenhancementsignificantlyaidslaypersonsinunderstandingthemechanismsof
themodel,comparedtomethodstraditionallytailoredforexperts.Ahumanstudyconfirmsthatourmethodis
preferredoverothers,offeringmoreaccessibleanddetailedinsightstolaypersonusers.
5.0.1 Limitations. Althoughwehavemadethoroughattempts,ourstudypresentsseveralconstraintsasoutlined
below:
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
erocS eulav eulav0:22 â€¢
Fig.14. Grad-CAMelucidatesthedecision-makingmechanismswithinneuralnetworksbymappingthegradientsassociated
withaspecifictargetconceptâ€”suchasaclassoutputpre-softmaxâ€”backtoapertinenthiddenlayer.Specifically,weemploy
thedeepesthiddenlayertoassessthecontributionofvarioussensorstothemodelâ€™spredictions.
Fig.15. TheSHAPmethodprimarilyapproximatestherelationshipbetweeninputsignalsandoutputpredictionswithout
probingthemodelâ€™sinternalmechanisms.Explanationsaredirectlyappliedtotheinputdata.
â€¢ AtomicActivity:Thispaperintroducesamethoddesignedtoaddressthechallengesofaccuratelylabeling
thetimingandsequenceofactivitiesinwilddatasets,wherepreciseannotationisoftenlacking.Whilethis
approachenhanceslabelingaccuracy,itdoesnotyetachieveoptimalaccuracyratescomparedtoprecise
labelingmethod.
â€¢ Real-TimeRendering:Ourapproachfeaturesagenerativedecoderthatvisuallyrepresentsactivities
detected within smart spaces. However, despite its innovative design, the rendering time is extended
relativetotraditionalend-to-enddecodingmethods.Thisdelayislargelyduetothestablediffusionprocess
employedbythedecoder,whichrequires100stepstocompletetheinference.Whilethismethodprovides
detailedvisualoutputs,itstillneedsimprovementforreal-timeapplications.
â€¢ Cross Domain Encoder: This paper introduces an approach using a universally pretrained decoder
to interpret different scenarios within a single model, enhancing the ability to translate various types
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:23
ThTehaec taivcattiivoant vioalnu evsa aluree:s are:
Rirgihgth fot ofot oLteft lfoeoftt f oLeoftt arlmef t R aigrhmt armright arm
0.26 0.80 0.75 0.41
0.55 0.753 0.333 0.27
The sensoronthe left foot is the most important
The largest value is the sensor on the left foot
to themodel
Fig.16. Thefigureillustratesatomicactivityassociatedwith"openingarefrigerator",aspartofthecomplexactivity"making
asandwich".Thefluorescentyellowcolorontheleftfootindicatessignificantsensorcontributionsfromthislocationwhen
themodeldetectstheactivity.Ourmethodtrainsadecodertoassignspecificcolorstohighlightcriticalsensorpositions
crucialforthemodelâ€™sdecision-making.Thedepictedvaluesareaveragereadingsfromthechannelsofasinglesensor.
Opportunity Cooking Challenge Fallaid
6 6 6
5 5 5
4 4 4
3 3 3
2 2 2
1 1 1
0 0 0
Grad-CAM SHAP VCHAR Grad-CAM SHAP VCHAR Grad-CAM SHAP VCHAR
Fig.17. HumanEvaluationofSensorContributionsAcrossDifferentMethods
of scenarios seamlessly. However, while the decoder supports multi-scenario translation, the encoder
processesdifferenttypesofdataseparatelyandstrugglestorecognizecross-domainsensordataasaunified
model,limitingitseffectivenessinintegratedscenarioanalysis.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.
erocS eulav eulav0:24 â€¢
5.0.2 FutureWork. Infuturestudies,whileourprincipalfocusremainsondetectingcomplexactivities,improving
thedetectionratesofatomicactivitieswillalsobeakeyareaofresearch.Ourprimaryobjectivewillbetorefine
themethodsusedforidentifyingatomicactivities,aimingforsubstantialenhancementsinaccuracy.Additionally,
addressingthechallengesinreal-timerenderingiscrucial;reducingrenderingtimesisimperativeforthepractical
deploymentofourmodelsinreal-worldsettings.Toachievethis,weplantodevelopmoreefficientalgorithms
capableofmanagingthecomputationaldemandsofstablediffusionprocesses.Theseimprovementswillaimto
optimizethegenerationofsmartspacesensorrepresentations,ensuringhigh-qualityoutputswithoutsacrificing
speedorefficiency.
Additionally,toaddressthecomplexitiesofapplyingthesetechniquesindiversereal-lifeenvironments,weaim
todesignandimplementaunifiedencoder.Thisadvancedencoderwillbecapableofprocessingandtranslating
varioustypesofsensordataacrossmultipledomainsintoacoherentvisualoutput.Thedevelopmentofsucha
encoderwillfacilitateamoreseamlessintegrationofourmethodsintoeverydaytechnology,makingsmartspace
technologiesmoreadaptableanduser-friendlyacrossdifferentsettingsandapplications.
REFERENCES
[1] SayedaShammaAlia,PaulaLago,ShingoTakeda,KoheiAdachi,BrahimBenaissa,MdAtiqurRahmanAhad,andSozoInoue.2021.
Summaryofthecookingactivityrecognitionchallenge.HumanActivityRecognitionChallenge(2021),1â€“13.
[2] GustavoAquino,MarlyGuimarÃ£esFernandesCosta,andCÃ­ceroFerreiraFernandesCostaFilho.2023. ExplainingandVisualizing
EmbeddingsofOne-DimensionalConvolutionalModelsinHumanActivityRecognitionTasks.Sensors23,9(2023),4409.
[3] LucaArrotta,GabrieleCivitarese,andClaudioBettini.2022.Dexar:Deepexplainablesensor-basedactivityrecognitioninsmart-home
environments.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies6,1(2022),1â€“30.
[4] MartinAtzmueller.2017.Ontoexplicativedatamining:exploratory,interpretableandexplainableanalysis.ProceedingsofDutch-Belgian
DatabaseDay.TUEindhoven(2017).
[5] MartinAtzmuellerandThomasRoth-Berghofer.2010.Theminingandanalysiscontinuumofexplaininguncovered.InInternational
ConferenceonInnovativeTechniquesandApplicationsofArtificialIntelligence.Springer,273â€“278.
[6] LingBaoandStephenSIntille.2004.Activityrecognitionfromuser-annotatedaccelerationdata.InInternationalconferenceonpervasive
computing.Springer,1â€“17.
[7] RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,SydneyvonArx,MichaelSBernstein,JeannetteBohg,
AntoineBosselut,EmmaBrunskill,etal.2021.Ontheopportunitiesandrisksoffoundationmodels.arXivpreprintarXiv:2108.07258
(2021).
[8] AdityaChattopadhay,AnirbanSarkar,PrantikHowlader,andVineethNBalasubramanian.2018.Grad-cam++:Generalizedgradient-
basedvisualexplanationsfordeepconvolutionalnetworks.In2018IEEEwinterconferenceonapplicationsofcomputervision(WACV).
IEEE,839â€“847.
[9] RicardoChavarriaga,HesamSagha,AlbertoCalatroni,SundaraTejaswiDigumarti,GerhardTrÃ¶ster,JosÃ©delRMillÃ¡n,andDaniel
Roggen.2013.TheOpportunitychallenge:Abenchmarkdatabaseforon-bodysensor-basedactivityrecognition.PatternRecognition
Letters34,15(2013),2033â€“2042.
[10] LingChen,XiaozeLiu,LiangyingPeng,andMenghanWu.2021.Deeplearningbasedmultimodalcomplexhumanactivityrecognition
usingwearabledevices.AppliedIntelligence51,6(2021),4029â€“4042.
[11] MathiasCiliberto,VitorFortesRey,AlbertoCalatroni,PaulLukowicz,andDanielRoggen.2021.Opportunity++:AMultimodalDataset
forVideo-andWearable,ObjectandAmbientSensors-basedHumanActivityRecognition. https://doi.org/10.21227/vd6r-db31
[12] StefanDernbach,BarnanDas,NarayananCKrishnan,BrianLThomas,andDianeJCook.2012.Simpleandcomplexactivityrecognition
throughsmartphones.In2012eighthinternationalconferenceonintelligentenvironments.IEEE,214â€“221.
[13] ThangMDo,SengWLoke,andFeiLiu.2013.Healthylife:Anactivityrecognitionsystemwithsmartphoneusinglogic-basedstream
reasoning.InMobileandUbiquitousSystems:Computing,Networking,andServices:9thInternationalConference,MobiQuitous2012,
Beijing,China,December12-14,2012.RevisedSelectedPapers9.Springer,188â€“199.
[14] JoaoBÃ¡rtoloGomes,ShonaliKrishnaswamy,MohamedMGaber,PedroACSousa,andErnestinaMenasalvas.2012.Mobileactivity
recognitionusingubiquitousdatastreammining.InDataWarehousingandKnowledgeDiscovery:14thInternationalConference,DaWaK
2012,Vienna,Austria,September3-6,2012.Proceedings14.Springer,130â€“141.
[15] DavidGunning.2016.Broadagencyannouncementexplainableartificialintelligence(xai).DefenseAdvancedResearchProjectsAgency
(DARPA),Tech.Rep.(2016).
[16] SojeongHaandSeungjinChoi.2016.Convolutionalneuralnetworksforhumanactivityrecognitionusingmultipleaccelerometerand
gyroscopesensors.In2016internationaljointconferenceonneuralnetworks(IJCNN).IEEE,381â€“388.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.VCHAR:Variance-DrivenComplexHumanActivityRecognitionframeworkwithGenerativeRepresentation â€¢ 0:25
[17] HarishHaresamudram,DavidVAnderson,andThomasPlÃ¶tz.2019.Ontheroleoffeaturesinhumanactivityrecognition.InProceedings
ofthe2019ACMInternationalSymposiumonWearableComputers.78â€“88.
[18] TaehoHur,JaehunBang,ThienHuynh-The,JongwonLee,Jee-InKim,andSungyoungLee.2018.Iss2Image:Anovelsignal-encoding
techniqueforCNN-basedhumanactivityrecognition.Sensors18,11(2018),3910.
[19] SozoInoue,PaulaLago,TaheraHossain,TittayaMairittha,andNattayaMairittha.2019.Integratingactivityrecognitionandnursing
carerecords:Thesystem,deployment,andaverificationstudy.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitous
Technologies3,3(2019),1â€“24.
[20] ChihiroIto,XinCao,MasakiShuzo,andEisakuMaeda.2018.ApplicationofCNNforhumanactivityrecognitionwithFFTspectrogram
ofaccelerationandgyrosensors.InProceedingsofthe2018ACMinternationaljointconferenceand2018internationalsymposiumon
pervasiveandubiquitouscomputingandwearablecomputers.1503â€“1510.
[21] JeyaVikranthJeyakumar,AnkurSarker,LuisAntonioGarcia,andManiSrivastava.2023.X-char:Aconcept-basedexplainablecomplex
humanactivityrecognitionmodel.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies7,1(2023),1â€“28.
[22] WenchaoJiangandZhaozhengYin.2015.Humanactivityrecognitionusingwearablesensorsbydeepconvolutionalneuralnetworks.
InProceedingsofthe23rdACMinternationalconferenceonMultimedia.1307â€“1310.
[23] PangWeiKoh,ThaoNguyen,YewSiangTang,StephenMussmann,EmmaPierson,BeenKim,andPercyLiang.2020.Conceptbottleneck
models.InInternationalconferenceonmachinelearning.PMLR,5338â€“5348.
[24] HyeokhyenKwon,GregoryDAbowd,andThomasPlÃ¶tz.2019.Handlingannotationuncertaintyinhumanactivityrecognition.In
Proceedingsofthe2019ACMInternationalSymposiumonWearableComputers.109â€“117.
[25] PaulaLago,ShingoTakeda,KoheiAdachi,SayedaShammaAlia,MoeMatsuki,BrahimBenai,SozoInoue,andFrancoisCharpillet.2020.
Cookingactivitydatasetwithmacroandmicroactivities. https://doi.org/10.21227/hyzg-9m49
[26] WeiLi,CanGao,GuochengNiu,XinyanXiao,HaoLiu,JiachenLiu,HuaWu,andHaifengWang.2020.Unimo:Towardsunified-modal
understandingandgenerationviacross-modalcontrastivelearning.arXivpreprintarXiv:2012.15409(2020).
[27] JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi.2022.Unified-io:Aunifiedmodelforvision,
language,andmulti-modaltasks.InTheEleventhInternationalConferenceonLearningRepresentations.
[28] ScottMLundbergandSu-InLee.2017.Aunifiedapproachtointerpretingmodelpredictions.Advancesinneuralinformationprocessing
systems30(2017).
[29] SakornMekruksavanichandAnuchitJitpattanakul.2021.Deepconvolutionalneuralnetworkwithrnnsforcomplexactivityrecognition
usingwrist-wornwearablesensordata.Electronics10,14(2021),1685.
[30] SinnoJialinPanandQiangYang.2009.Asurveyontransferlearning.IEEETransactionsonknowledgeanddataengineering22,10(2009),
1345â€“1359.
[31] LiangyingPeng,LingChen,MenghanWu,andGencaiChen.2018.Complexactivityrecognitionusingacceleration,vitalsign,and
locationdata.IEEETransactionsonMobileComputing18,7(2018),1488â€“1498.
[32] LiangyingPeng,LingChen,ZhenanYe,andYiZhang.2018. Aroma:Adeepmulti-tasklearningbasedsimpleandcomplexhuman
activityrecognitionmethodusingwearablesensors.ProceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies
2,2(2018),1â€“16.
[33] ParisaRashidiandAlexMihailidis.2012. Asurveyonambient-assistedlivingtoolsforolderadults. IEEEjournalofbiomedicaland
healthinformatics17,3(2012),579â€“590.
[34] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016."Whyshoulditrustyou?"Explainingthepredictionsofanyclassifier.In
Proceedingsofthe22ndACMSIGKDDinternationalconferenceonknowledgediscoveryanddatamining.1135â€“1144.
[35] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.2022.High-resolutionimagesynthesiswith
latentdiffusionmodels.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.10684â€“10695.
[36] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer.2022.High-ResolutionImageSynthesisWith
LatentDiffusionModels.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).10684â€“10695.
[37] MajdSaleh,ManuelAbbas,andRegineBouquinLeJeannes.2020.FallAllD:Anopendatasetofhumanfallsandactivitiesofdailyliving
forclassicalanddeeplearningapplications.IEEESensorsJournal21,2(2020),1849â€“1858.
[38] MuhammadShoaib,StephanBosch,OzlemDurmazIncel,HansScholten,andPaulJMHavinga.2016. Complexhumanactivity
recognitionusingsmartphoneandwrist-wornmotionsensors.Sensors16,4(2016),426.
[39] MuhammadShoaib,HansScholten,PaulJMHavinga,andOzlemDurmazIncel.2016.Ahierarchicallazysmokingdetectionalgorithm
usingsmartwatchsensors.In2016IEEE18thInternationalConferenceone-HealthNetworking,ApplicationsandServices(Healthcom).
IEEE,1â€“6.
[40] KarenSimonyan,AndreaVedaldi,andAndrewZisserman.2013.Deepinsideconvolutionalnetworks:Visualisingimageclassification
modelsandsaliencymaps.arXivpreprintarXiv:1312.6034(2013).
[41] AmanpreetSingh,RonghangHu,VedanujGoswami,GuillaumeCouairon,WojciechGaluba,MarcusRohrbach,andDouweKiela.2022.
Flava:Afoundationallanguageandvisionalignmentmodel.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.15638â€“15650.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.0:26 â€¢
[42] SatyaPSingh,MadanKumarSharma,AimÃ©Lay-Ekuakille,DeepakGangwar,andSukritGupta.2020.DeepConvLSTMwithself-attention
forhumanactivitydecodingusingwearablesensors.IEEESensorsJournal21,6(2020),8575â€“8582.
[43] UpendraSingh,PujaGupta,MukulShukla,VarshaSharma,SunitaVarma,andSumitKumarSharma.2023.Acknowledgmentofpatient
insensebehaviorsusingbidirectionalConvLSTM.ConcurrencyandComputation:PracticeandExperience35,28(2023),e7819.
[44] JiamingSong,ChenlinMeng,andStefanoErmon.2020.Denoisingdiffusionimplicitmodels.arXivpreprintarXiv:2010.02502(2020).
[45] EhsanTahvilian,EhsanPartovi,MehdiEjtehadi,ParsaRiaziBakhshayesh,andSaeedBehzadipour.2022.Accuracyimprovementin
simpleandcomplexHumanActivityRecognitionusingaCNN-BiLSTMmulti-taskdeepneuralnetwork.In20228thIranianConference
onSignalProcessingandIntelligentSystems(ICSPIS).IEEE,1â€“5.
[46] ImenTrabelsi,JulesFranÃ§oise,andYacineBellik.2022.Sensor-basedactivityrecognitionusingdeeplearning:Acomparativestudy.In
Proceedingsofthe8thInternationalConferenceonMovementandComputing.1â€“8.
[47] NeerajVarshney,BrijeshBakariya,AlokKumarSinghKushwaha,andManishKhare.2022.Humanactivityrecognitionbycombining
externalfeatureswithaccelerometersensordatausingdeeplearningnetworkmodel.MultimediaToolsandApplications81,24(2022),
34633â€“34652.
[48] PengWang,AnYang,RuiMen,JunyangLin,ShuaiBai,ZhikangLi,JianxinMa,ChangZhou,JingrenZhou,andHongxiaYang.2022.Ofa:
Unifyingarchitectures,tasks,andmodalitiesthroughasimplesequence-to-sequencelearningframework.InInternationalConferenceon
MachineLearning.PMLR,23318â€“23340.
[49] WenhuiWang,HangboBao,LiDong,JohanBjorck,ZhiliangPeng,QiangLiu,KritiAggarwal,OwaisKhanMohammed,Saksham
Singhal,SubhojitSom,etal.2022.Imageasaforeignlanguage:Beitpretrainingforallvisionandvision-languagetasks.arXivpreprint
arXiv:2208.10442(2022).
[50] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,WynneHsu,YingShan,XiaohuQie,andMikeZheng
Shou.2023. Tune-a-video:One-shottuningofimagediffusionmodelsfortext-to-videogeneration.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision.7623â€“7633.
[51] KarlLWuensch.2005.Whatisalikertscale.Andhowdoyoupronounceâ€™Likert(2005).
[52] DaqingZhang,MossaabHariz,andMounirMokhtari.2008.Assistingelderswithmilddementiastayingathome.In2008SixthAnnual
IEEEInternationalConferenceonPervasiveComputingandCommunications(PerCom).IEEE,692â€“697.
[53] ZhongtangZhao,YiqiangChen,JunfaLiu,ZhiqiShen,andMingjieLiu.2011.Cross-peoplemobile-phonebasedactivityrecognition.In
Twenty-secondinternationaljointconferenceonartificialintelligence.Citeseer.
[54] HongkaiZheng,WeiliNie,ArashVahdat,andAnimaAnandkumar.2023.Fasttrainingofdiffusionmodelswithmaskedtransformers.
arXivpreprintarXiv:2306.09305(2023).
[55] ThorstenZylowski.2022. StudyoncriteriaforexplainableAIforlaypeople.InProceedingsoftheSecondInternationalWorkshopon
ExplainableandInterpretableMachineLearning(XI-ML2022)co-locatedwiththe45rdGermanConferenceonArtificialIntelligence(KI
2022),Trier(Virtual),Germany.CEURWorkshopProceedings.
Proc.ACMInteract.Mob.WearableUbiquitousTechnol.,Vol.0,No.0,Article0.Publicationdate:2024.