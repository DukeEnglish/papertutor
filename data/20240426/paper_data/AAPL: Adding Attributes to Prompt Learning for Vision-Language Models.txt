AAPL: Adding Attributes to Prompt Learning for Vision-Language Models
GahyeonKim* SoheeKim* SeokjuLeeâ€ 
KoreaInstituteofEnergyTechnology(KENTECH)
{gahyeon, soheekim, slee}@kentech.ac.kr
Abstract
: Text features of learnable prompt without bias
: Text features of learnable prompt with instance bias
: Text features of learnable prompt with attribute bias
Recent advances in large pre-trained vision-language
m sho od te dls owha nv se trd ee am mo tn as str ka s.te Bd ur ie lm dia nr gka ub pl oe npe thrf io s,rm rea cn ec ne to sn tuz de ir eo s-
,
y t ila
d
feaTe tux rt e s ğ’…ğŸ(CoO ğ’…p :) d> i sğ’… tğŸ a( nC co eC bo eO twp) e > e nğ’… fğŸ‘ e( aA tA uP reL s)
o Input class:
suchasCoOpandCoCoOp,haveproposedtheuseofprompt M "Apple"
Attribute
learning, where context within a prompt is replaced with
learnablevectors,leadingtosignificantimprovementsover CoOp ğ’… ğŸ Input image
manually crafted prompts. However, the performance im- Adding +
? bias provementforunseenclassesisstillmarginal,andtotackle Closer
thisproblem,dataaugmentationhasbeenfrequentlyused Image
intraditionalzero-shotlearningtechniques. Throughour CoCoOp AAPL features Hard-to-distinguish
sample:
experiments,wehaveidentifiedimportantissuesinCoOp
ğ’… Apple orPear?
ğ’… ğŸ‘
andCoCoOp: thecontextlearnedthroughtraditionalim- ğŸ
ageaugmentationisbiasedtowardseenclasses,negatively Figure1.TheillustrationofAAPL.Trainingthelearnableprompt
impactinggeneralizationtounseenclasses. Toaddressthis on the class â€œappleâ€, since the training data mainly consists of
problem,weproposeadversarialtokenembeddingtodisen- redapples,leadstounderstandingapplesastypicallyred. When
tangle low-level visual augmentation features from high- a rare â€œyellow appleâ€ is input, the instance bias may overlook
level class information when inducing bias in learnable theyellowattributeandincorrectlypredictitasapear.However,
prompts. Through our novel mechanism called â€œAdding AAPLextractsanddecomposesattributesfromtheimage,enhanc-
AttributestoPromptLearningâ€,AAPL,weguidethelearn- ingattribute-specificbiasinthesemanticfeatures. Thisenables
robustlyimprovedgeneralizationperformanceacrossdomains.
ablecontexttoeffectivelyextracttextfeaturesbyfocusingon
high-levelfeaturesforunseenclasses. Wehaveconducted
experimentsacross11datasets,andoverall,AAPLshows The strengths of these VLMs have proven to be effective
favorableperformancescomparedtotheexistingmethodsin in prompt learning and handling both visual and textual
few-shot learning, zero-shot learning, cross-dataset, and informationefficiently[31,45,67,68]. CoOp[68]andCo-
domain generalization tasks. The code is available at: CoOp[67]haveeffectivelyproducedlearnablecontextvec-
https://github.com/Gahyeonkim09/AAPL
torsforclassificationweightsviaatextencoder(e.g.,Trans-
former[50])alongwithCLIP.Specifically,CoCoOp[67]has
enabledthecreationofclass-specificclassificationweights
1.Introduction byincorporatingadditionalcontextinformationgenerated
fromimages. Inaddition,visualprompttuning(VPT)[18]
Recent research has shown significant improvements not demonstrated performance improvements in downstream
onlyinmodelgeneralizationperformancethroughtheuse tasksbyintroducingasmallnumberoflearnableparame-
of large-scale vision-language models (VLMs), but also ters into the encoder layer of the Transformer along with
in zero-shot image classification performance [46, 61, 63, imagepatches,withouttheneedtoreplaceorfine-tunethe
64, 66]. It has been demonstrated that utilizing VLMs pre-trainedtransformer.
suchascontrastivelanguage-imagepretraining(CLIP)[39], However,bothCoOp[68]andVPT[18]havelearnable
ALIGN [17], Flamingo [1], etc., is effective in extracting parametersthatarenotmanageable,especiallyinthecaseof
imageandtextinformationfortrainingclassificationmodels. CoCoOp[67],whereitisunknownhowthelearnablevector
willbeshiftedbytheconditionalbiasbasedonparticular
*Theseauthorscontributedequally.
â€ Correspondingauthor. informationtakenfromtheimagethatisaddedtothelearn-
4202
rpA
52
]VC.sc[
1v40861.4042:viXraablecontextvector. Thislackofmanagementoverlearnable tasks. Insteadofusingamanualpromptlikeâ€œaphotoofaâ€,
parameterscanleadtounintentionalbiasinfew-shotclas- theytransformedthecontextwordintoalearnablecontext
sificationtasksordomaingeneralizationtasks[22,30,32]. vectortooptimizecontinuousprompts. However,CoOphas
Toaddressthis, weproposeanewapproachcalledAAPL, limitationsingeneralizabilityduetooverfittingonfew-shot
â€œAdding Attributes to Prompt Learningâ€, as illustrated in datasets. Toaddressthis,CoCoOp[67]addsaconditional
Fig.1. Inthiscontext,augmentationgeneratesalearnable biascalledmetatokenextractedfromimagefeaturestothe
bias that can be decomposed, with the augmented image learnableprompt. Itshiftsthefocusfromstatictodynamic
serving as the visual prompt. Subsequent learning with prompts,enablingoptimizationbasedonthecharacteristics
visual-textpromptsinvolvestheuseofalearnablecontext ofeachinstanceratherthanaspecificclass, consequently
vector, which plays an adversarial role and mitigates un- enhancing CoOpâ€™s domain generalization performance.
intended overfitting in downstream tasks [22, 30, 32]. In However,metatokenobtainedfromanimagesamplecannot
summary,ourcontributionsareasfollows: be claimed to be completely robust against overfitting
issues[22,30,32],anditisnotinterpretablebecauseitis
â‹„ Weproposeusingaugmentedimagesasavisualprompt
extracted from the shallow network, called metanet, com-
andintroducetheconceptofâ€œdeltametatoken,â€which
posedofLinear-ReLU-Linearlayers. Therefore,wepropose
encapsulatesattribute-specificinformation.
anewpromptlearningmethodusingimageaugmentation
â‹„ Employingdeltametatoken,weconductAdTripletloss toleverageattribute-specificbiasaddedtolearnableprompts.
tomaketheconditionalbiasincludethesemanticfeature
oftheclassrobustly,eveninthepresenceofaugmentation Zero-shotlearning Few-shotlearningistheprocessof
addedtothelearnablepromptthroughadversarialtriplet trainingonasmallnumberoflabeledsamplesbeforeclassi-
loss. fyingthenewimages. Incontrast,zero-shotlearning(ZSL)
â‹„ Wedemonstrateperformanceimprovementsinbase-to- aimstodistinguishunseenclassesbytrainingexclusivelyon
seenclasses[5,57]. Thisisachievedbyexclusivelytraining
newgeneralizationtasks,cross-datasettasks,anddomain
onasetofbaseclassesandutilizingsideinformation,typi-
generalizationtasks.
callyvisualattributeslikecolor,shape,andotherfeatures,
shared with these unseen classes. This auxiliary informa-
2.RelatedWorks tionhelpsthemachineunderstandlanguageorconceptsin
awayhumansdo,enablingittorecognizeunseenclasses.
Vision-languagemodels Vision-languagemodels(VLMs)
The common methods [4, 20, 34, 42, 55] are learning the
usingimage-textpairshaveshownsuperiorcapabilitiesover
relationbetweenaclassembeddingandtheimagefeature,
image-onlymodels,especiallyinzero-shottransfertasksfor
whichrepresentsthisauxiliaryinformation. However,these
variousdownstreamclassificationtasks[46,61,63,64,66].
methodsoftenexhibitabiasagainstunseenclasses,known
Prominent models such as CLIP [39] and ALIGN [17],
asâ€œseen-classbiasâ€[56]. Otherresearcheffortsconcentrate
which have advanced through large-scale web data uti-
onenhancingvisual-semanticembedding[3,19,65],orde-
lization, employ self-supervised learning for enhanced
velopingbetterimagefeatureextractors[16,59]. However,
textualandvisualalignment. Intheembeddingspace,the
thesemethodsusuallyassumeafixedsetofauxiliaryinfor-
contrastive loss draws matched image-text representation
mation, consisting of attributes labeled by humans. This
pairscloser,whileitdrawstherepresentationofmismatched
assumptionposeschallenges,aslabelingattributesisexpen-
pairsfartheraway. Usingthismethod,CLIPdemonstrates
sive,requiresexpertannotators,andisdifficulttoscaleon
exceptionalzero-shotimagerecognitioncapabilitieswithout
largedatasets. DivergingfromexistingZSLmethods,our
theneedforfurtherfine-tuning. Ourgoalistofindefficient
workfocusesonadaptinglargevision-languagemodelsand
methodsforapplyingpre-trainedvision-languagemodelsto
employstechniquesbasedonprompting.
downstreamapplications,especiallyinpromptlearninglike
CoOp[68]andCoCoOp[67].
3.Methodology
Promptlearninginvision-languagemodels Theconcept
3.1.Preliminaries
ofpromptlearningwasinitiallyproposedinthedomainof
naturallanguageprocessing(NLP)[27â€“29]. Unlikemanu- PromptlearningforCLIP CLIP[39]employsanimage
allydesigningprompts,promptlearningresearchfocuseson encoderbasedonResNet[11]orViT[24]andatextencoder
automaticallyselectingpromptsduringthefine-tuningstage. basedonTransformer[50]toextractfeaturesfromimages
Recently, this concept has been extended to the field of and text, respectively. These features are trained with a
computervision[18,21,31,45,52,60,68,69]. CoOp[68] contrastivelossintheembeddingspace,aimingtomaximize
introducedcontinuouspromptlearningtothevisiondomain, cosinesimilaritiesbetweenpairedmodalityfeatures. When
applying pre-trained vision-language models to various an input image x is processed through the image encoder. . â€¦ . [class] . EnT ce ox dt e r ğ‘­ ğ’•ğŸ ğ’™ğ’•
Learnable prompt
ğ…ğŸ
Augmentation A
ğ‘­ğ’ŠğŸ ğ’ğ’ˆ
ğ…ğŸğ‘¨
AdTripletloss
ğ‘­ğ’ŠğŸ ğ’ğ‘¨
ğ’ˆ ğœŸğ…ğŸğ‘© ğœŸğ…ğŸğ‘¨
ğ‘­ğ’ŠğŸ ğ’ğ‘©
ğ’ˆ
ğ…ğŸğ‘©
Class 1 Random Image Meta- : tunable
Augmentation Encoder Net
ğ‘­ğ’ŠğŸ ğ’ğ‘¨
ğ’ˆ
ğ…ğŸğ‘¨
ğœŸğ…ğŸğ‘© ğœŸğ…ğŸğ‘¨
: frozen
ğ‘­ğ’ŠğŸ ğ’ğ‘©
ğ’ˆ
: subtract
Class 2 ğ‘­ğ’ŠğŸ ğ’ğ’ˆ ğ…ğŸğ‘© : pull
Augmentation B : push
: contrastive
ğ…ğŸ learning
Figure2.OverviewofAAPL.Weapplytwodistinctrandomaugmentationstotheinputimages,eachwiththeclasslabels1and2.Once
theimagefeaturesareextractedfromthepre-trainedCLIPimageencoder[39],theyarepassedthroughthemetanet[67]toacquirethe
metatoken.Thesearethenutilizedtosubtracttheothermetatokensobtainedfromtheaugmentedimagesforeachclass,resultingindelta
metatokens.Thegoalistoinstructthemtousethesedeltametatokensregardlessoftheirclassification.Thedeltametatokens,whichare
associatedwiththesameaugmentation,approachclosewithintheembeddingspaceusingtheAdTripletloss,asshowninEq.5.Thedelta
metatokensacquireattribute-specificfeatures,whilethemetatokenlearnssemanticfeaturesderivedfromimagefeatures,enablingtheuse
ofattribute-specificbiasinthelearnablepromptthroughthedecomposedfeatures.
f(Â·), it generates an image feature f(x). Using a prompt duringtrainingensuresgeneralizability. Thepredictionprob-
template like â€œa photo of a {class}.â€, where the {class} abilityforCoCoOpisasfollows:
tokenissubstitutedwiththenameofthei-thclass,yieldsK
textfeatureswithcorrespondingweightvectors{w i}K i=1for p(y|x)= exp(sim(f(x),g(t y(x)))/Ï„) . (2)
thegivenK classcategories. Thepredictionprobabilityfor âˆ‘K i=1exp(sim(f(x),g(t i(x)))/Ï„)
CoOpisasEq.1,wheresim(Â·,Â·)denotescosinesimilarity
andÏ„ isatemperatureparameter.
3.2.DeltaMetaToken
exp(sim(f(x),w )/Ï„)
y
p(y|x)= (1) EffectofaugmentationinCoCoOp Toinvestigatethe
âˆ‘K i=1exp(sim(f(x),w i)/Ï„)
effect of augmentation in prompt learning, we conducted
Conditional context optimization in prompt learning a comparative experiment by adapting augmentation into
CoOp [68] introduces context tokens as trainable vectors, CoCoOp[67]. Weaddedconditionalbiasfromaugmented
Mlearnablecontext,{v ,v ,...,v },departingfromafixed imagestothelearnablepromptwhilemaintainingotherset-
1 2 M
template like â€œa photo of aâ€. The i-th class prompt, t = tingsconsistentwithCoCoOp. AsdetailedinTable1, in-
i
{v ,v ,...,v ,c},includesthesevectorsandwordembed- corporatingaugmentationleadstoadecreaseinbase-to-new
1 2 M i
dingsoftheclassname,c. Textfeaturesaregeneratedfrom generalizationaccuracycomparedtotheoriginalCoCoOp
i
t byCLIPtextencoderg(Â·),whichremainedfrozenthrough- sincethemetanetfailstoextractthesemanticfeaturesfrom
i
out training. CoCoOp [67] proposes instance-conditional theaugmentedimages;thusextractingarbitrarynoiserather
context to prioritize individual input instances, reducing thanattribute-specificsemantics. Additionally,asshownin
the overfitting of CoOp. This is done by using a metanet, Fig.3,itdoesnotshowabigdifferenceinclassclustering,
denoted as h (Â·) parameterized by Î¸, to generate a con- indicating that the meta token fails to capture the crucial
Î¸
ditional token for each input. Where Ï€ = h (f(x)) and semanticfeaturesfortheclassification. Consequently,this
Î¸
mâˆˆ{1,2,...,M},eachcontexttokenisobtainedbyv (x)= suggeststhatmerelyusingaugmentationinpromptlearning
m
v +Ï€. Thepromptofthei-thclassisconditionedonthe mightnotenhancerobustnessorperformance. Itpotentially
m
inputimagefeature,i.e.,t(x)={v (x),v (x),...,v (x),c}. leadstodetrimentaleffectsduetothemetanetâ€™sinabilityto
i 1 2 M i
Jointly updating context vectors {v (x)}M and metanet identifymeaningfulsemanticfeaturesfromtheaugmented
m m=1CCCCCCCCCCCCCCCCCCCCCCCCClllllllllllll llllllllllllaaaaaaaaaaaaa aaaaaaaaaaaassssssssssssss sssssssssssssssssssssssss
sssssssssss
10123456789111
111111222223012
45678901234
CCCCCCCCCCCCCCCCCCCCCCCCCllll
llllllllllllllll llll
laaaa
aaaaaaaaaaaaaaaa aaaa as
sssss
ssssssssssssssss ssss
sssss
ssssssssssssssss
sss
4
20123
5678911111111112
222101234567890
234
D
f
g
ime
ee
a
n
al tt
e
gua
r
er
aem
xts
ee odtt ha
f
âˆ†aa
ct Ï€tto
lae
1ck
s
Aaoe scn
n
=h
1tr ae
haii
tp
n
n
Î¸er
d
(re
a
a
fs
Au
t
(e
ig
u
Aon
m
gn
ut As
.
ge
(a
An
T
Â·)
(td
ha
xci
t
e
1f
i
a
)f
o
d
ne
)n
e
)r be
l
âˆ’i
t
en
n
ac
f
w
he
o
m
Î¸rrv
(iem
te
ft ta
ec
a
(nt xto
ti 1oo
ar
)kn
s
)f
e.
.r fno oTm
lf
lh
r
ooei wm
y
m
sa
a
:
(ag
r
3nee
)
(a) ğœ‹ of CoCoOp (b) ğœ‹ of CoCoOp with augmentation As TextManiA has shown, utilizing attributes con-
taining semantic details derived from class information
Figure3. ThecomparisonbetweenmetatokensofCoCoOpand demonstrates its effectiveness in classification tasks. In
metatokensofCoCoOpwithrandomaugmentationforFGVCAir- other words, while the meta token includes both class
craftdataset. and attribute information, the delta meta token preserves
more specific image feature information associated with
Method Base New HM augmentation. Adding decomposed auxiliary features
CoOp[6 8F]igure 3 tsne 8ìˆ˜2.6ì •9 ë³¸63.22 71.6 to the learnable prompts, the delta meta token can learn
Co CCo oO Cp[ o67 O] p / FGVC 80.47 71.69 75.83 attribute information. We enable the learnable prompt to
CoCoOpwithaugmentation 79.25 70.89 74.38
incorporatesemanticfeaturesmoreabundantly,thusmaking
AAPL 80.65 72.33 76.26
the augmentation more effective. Similar to adversarial
prompt learning for natural language processing [37, 54],
Table1.Thecomparisonofbase-to-newgeneralizationaccuracy
between AAPL and CoCoOp with augmentation. HM denotes our method involves the adversarial interaction between
harmonicmeanscore. classandattributeinformation,wherethemetanetlearnsto
extractattribute-relatedinformationfromaugmentedimage
images, focusing on instance-specific features rather than features. Themorethelearnablepromptlearnsthesemantic
classsemantics. Toachieveoptimalresults,augmentation featureinformationoftheclass,thebettertheclassification
needstobeappliedmorecarefully,ensuringthatthecondi- performance.
tionalbiasesappropriatelycapturethesemanticinformation
oftheclass. Doesthedeltametatokenhaveexactaugmentationinfor-
mation? InFig.4,weusedt-SNEtocomparethevalidation
Deltametatoken:detachattributefeature CoCoOp[67] resultsofmetanetofbothCoCoOp[67]andAAPL.Itshows
improvesthegeneralizationperformanceofCoOp[68]by that CoCoOp fails to distinguish between augmentations
introducingmetanet,whichoutputsmetatokenfromimage comparedtoAAPL.AscomparingFig.4(c)and(d),while
samples,thenaddsittothelearnableprompt. Itfocuseson metatokencannotperfectlydiscriminate14augmentations,
learningaboutindividualinstanceinformationratherthan deltametatokenshowsalmostperfectdistinction,exceptfor
class information. However, itâ€™s still unclear what infor- afewaugmentations,e.g.,verticalflipandrotations. This
mation the meta token contains, as the metanet is a black clustering result shows that the delta meta token extracts
box,anditsshallowarchitectureleadstouncertainfeature morespecificinformationaboutaugmentationthanthemeta
extraction. AsshowninFig.3,itfailstodemonstrateclear token. AsdemonstratedinTextManiA[62],forthetextual
clusteringbyneitheraugmentationtypenorclass. Itshows case, subtraction between features can retain specific fea-
thatthemetatokendoesnoteffectivelycapturethesemantic tures.Inthecaseoftheimage,weshowthatdeltametatoken
informationoftheclassortheattributeoftheinputimage ismoreeffectiveinmakingitcontaintheexactaugmentation
sample. Toaddressthisissueandmakeitpossibletoadd information. Tothebestofourknowledge,wearethefirst
desiredinformationtothelearnableprompt,weproposethe toemployfeaturedecompositionthroughsubtractionusing
conceptofadeltametatoken,theattribute-specificbias.The visual features for prompt learning. It is noteworthy that,
overviewofAAPLisshowninFig.2. whilethemetatokenstillretainsinformationabouttheclass,
Tomakeadeltametatoken,twoimagesofeachofthe thedeltametatokenaccuratelydistinguishesbetweenthe
twodifferentclassesarerequired,e.g.,class1andclass2, semanticfeatureandtheattributefeature.
asshowninFig.2. Twodifferentaugmentationtypesare
3.3.AdversarialTripletLoss
randomlyselectedfrom14augmentationsproposedinSim-
CLR [6] for each pair of input images without any dupli- Using triplet loss [15, 43, 47, 53], we can eliminate the
cation,whichisdenotedasAug (Â·)andAug (Â·). Inspired remaining class-specific information in the delta meta to-
A B
byTextManiA[62],whichdemonstratedtheextractionof kenwhileenhancinginformationrelatedtoaugmentations.
attribute information from text using Word Vector Anal- Trainingisconductedwith4deltametatokens,e.g.,âˆ†Ï€1A,
ogy [9, 35], we generate delta meta token by subtracting âˆ†Ï€1B,âˆ†Ï€2A,andâˆ†Ï€2B,intheembeddingspace,aimingto
imagefeaturesinthesameclasswithdifferentaugmentation. increasethedistancebetweenvectorsofthesameclasswhileocccggghvrrrccsooo oooo rueaarr
o
oai
ttt
blll
r
tuurgooo
aaa ot
py
i
essi zitttrrrsn ucss liiio___c _oooii aa taajjj na flnnniiil _nn ittt_ l
t
l___ettt tafi __eee em l912linb_rrr
p
r087___a olfu 00lrgbg ii pe srre l eued een
cccggghvrrrccsooo oooo rueraao oa ttt blll r tuurooo aaa ot py i esszitttrrrs ucss liiio___c _oooi ai taajjj na flnnniii _nn itttl t l___ettt taf__eee el912linb_rrr p r087___ olfu 00lrgb ii pe srrl eued een occcggghvrrr ccsooo oooo rueraar o oa ti tt blll r tuurgooo aaa ot py i essi zi tttrrrsn ucss liiio___c _oooii aa taajjj na fnlnniiil _nn ittt_ l t l___ettt tafi __eee em l 291linb_rrr p r708___a olfu 00lrgbg ii pe srre l eued een cccggghvrrrccsooo oooo rueraao oa ttt blll r tuurooo aaa ot py i esszitttrrrs ucss liiio___c _oooi ai taajjj na flnnniii _nn itttl t l___ettt taf__eee el912linb_rrr p r087___ olfu 00lrgb ii pe srrl eued een
(a) ğœ‹ of CoCoOp (b) âˆ†ğœ‹ of CoCoOp (c) ğœ‹ of AAPL (d) âˆ†ğœ‹ of AAPL
Figure4.t-SNEvisualizationofmetatokenanddeltametatokenofCoCoOp[67]andAAPLforFGVCAircraftdataset.Thecolorsofthe
pointsrepresentthe14differentaugmentations,and100datapointsfromthevalidationsetareusedforthis.(a)and(c)arethevisualization
ofmetatoken,(b)and(d)arethevisualizationofdeltametatoken.
CFonisgtruainrtes-2 4 tsne ìˆ˜Coì •nstrë³¸aints-4 Cross-entropy loss is computed following the same
CoCoOp / FGVC method as CoCoOp [67]. To ensure fairness between the
trainingandtestphases,onlyoneinputimagelabelisused
âˆ†ğ…ğŸğ‘©
âˆ†ğ…ğŸğ‘¨
PULL
âˆ†ğ…ğŸğ‘¨
for cross-entropy loss calculation. The final training loss
PUSH PULL
âˆ†ğ…ğŸğ‘¨
ccncggghvrrrccsooo ooo
ruo eoraa oa tttr
btll
l uur
aaaoo
o t opy i eizss
tttcrrr
s uiiiss
lo__
_ oooac _ii
tjj
j aa
fnii
ai
lnnntt
t _ inn
ltt
tl tf
t___eee
e al e__
912iprr
r lnb r_
087__
_ o
flrg
b
00ule
ii
pr
l
srude eee âˆ†ğ…ğŸoccncggghvrrrccsooo ooo
ruo
eor
raa
oi
a tttr
btğ‘©ll lg
uur
aaaoo
o t opy i
eizssi tttcrrrn
s uiiiss
lo__
_ oooac
_a
ii
tjj
j aa
fnii
ai
lnnntt
t
_l i_
nn
ltt
tl tf
t___eeei
e al
e__m 912iprr
r lnb r_
087__ _a
o
flrg
b
00uleg iir
pl
srudee eee
âˆ†ğ…ğŸğ‘©
âˆ†ğ…ğŸP ğ‘©USH wfu hn ec rt eio Î±ni as na ds Î²Lfo
to
al tl rao elw
=
hs y:
Î± peâˆ— r-L pA ad rT ar mip ele tt e+ rsÎ² foâˆ— rL sC caE l,
ing.
InSec.(6 4)
,
weprovidedetailedinformationonparametertuning.
Figure5.ComparisonofthenumberofconstraintsoftheAdTriplet
loss.Theconstraints-2settingâ€™sanchorisjustone,e.g.,âˆ†Ï€1B,and
4.Experiments
thecons trFaiginutrse-4 5settinghastwoanchors,e.g.,âˆ†Ï€1Aandâˆ†Ï€2B.
4.1.ExperimentalSettings
minimizingitforthesameaugmentation. Forinstance,con- Datasets We use 11 classification datasets based on
sidering anchor as âˆ†Ï€1A, its positive pair is âˆ†Ï€2A, which CLIP[39],CoOp[68],andCoCoOp[67]forbase-to-new
hasadifferentclassbutthesameaugmentation. Incontrast, generalizationandcross-datasettransfer: ImageNet[8]and
âˆ†Ï€1B isconsideredanegativepairbecauseithasthesame Caltech101 [10] for generic object classification, Oxford-
class but a different augmentation. The distance between Pets[38],StanfordCars[26],Flowers102[36],Food101[2]
theanchorandthenegativepairshouldbegreaterthanthe andFGVCAircraft[33]forfine-grainedimagerecognition,
distancebetweentheanchorandthepositivepair. TheEu- EuroSAT[12]forsatelliteimageclassification,UCF101[48]
clideandistanceisdenotedasâˆ¥Â·âˆ¥ 2,andthemarginofthe foractionclassification,DTD[7]fortextureclassification,
tripletlossisdenotedasminEq.4. andSUN397[58]forscenerecognition. Fordomaingen-
eralizationexperiments,weuseImageNet[8]asthesource
L (x,x+,xâˆ’;âˆ†Ï€1A,âˆ†Ï€2A,âˆ†Ï€1B) dataset and 4 other ImageNet-based datasets, i.e., Ima-
triplet
geNetV2[41],ImageNetSketch[51],ImageNet-A[14],and
=max(0, âˆ¥xâˆ’x+âˆ¥ âˆ’âˆ¥xâˆ’xâˆ’âˆ¥ +m)
2 2 ImageNet-R[13],asthetargetdatasets,whicheachcontain
=max(0, âˆ¥âˆ†Ï€1Aâˆ’âˆ†Ï€2Aâˆ¥ 2âˆ’âˆ¥âˆ†Ï€1Aâˆ’âˆ†Ï€1Bâˆ¥ 2+m) (4) adifferentkindofdomainshift.
Baselines WecompareAAPLwith3baselinemethods: the
Thus, we introduce the Adtriplet loss, which adversar- zero-shotCLIP[39],CoOp[68],andCoCoOp[67]. CLIP
ially trains the model to prioritize the alignment of aug- uses the hand-crafted template â€œa photo of a {class}â€ to
mentationinformationoverclassinformation. Thislossis generatethepromptsforknowledgetransfer. CoOplearns
updated alongside the classification loss, specifically the astaticpromptthatreplacesthehand-craftedpromptswith
cross-entropyloss. TheAdTripletlossisusedasconstraints- thelearnablevectors. CoCoOpgeneratesdynamicprompts
4,asillustratedinFig.5,tomaketheconnectionbetweenthe by adding the image-conditional prompts to the learnable
classinformationdomainandaugmentationattributedomain promptsinCoOp.
morebalanced[23]. Training details Our implementation is based on Co-
CoOp [67]. We employ the pre-trained ViT-B/16 model
L =L1 (âˆ†Ï€1A,âˆ†Ï€2A,âˆ†Ï€1B) fromCLIP[39]asthebackbone. Wefixthecontextlength
AdTriplet triplet
to 4 and initialize the context vectors randomly. The pre-
+L2 (âˆ†Ï€2B,âˆ†Ï€1B,âˆ†Ï€2A) (5)
triplet sentedresultsarethemeanvaluesobtainedfromexperimentsconductedwiththreerandomseeds. Wefollowthetraining classes, just like in CoCoOp [67]. Learning-based mod-
epochs,batchsizes,andschedulesasprescribedbyCoCoOp. elsaretrainedsolelyonbaseclasses. Infew-shotlearning,
Inthecontextoffew-shotlearning,weconfineevaluation the model is evaluated with the base classes, whereas in
to themaximum shot, i.e., 16 shots, considered byCoOp. zero-shot learning, it is evaluated with the new classes to
Forevaluation,weusethemodelfromthelastepoch. The testthemodelâ€™sgeneralizability. Inthistask,wesethyper-
parameter size of AAPL is the same as CoCoOp, and the parametersÎ± andÎ² to0.2and1. Table2presentstheper-
hyper-parameterminEq.4issetto0.2. formanceresultsofAAPLcomparedtothebaseline. AAPL
outperformed in 7 out of 11 datasets, with the harmonic
CLIP CoOp CoCoOp AAPL meanoftotaldatasetaccuracyexceedingthatofCoCoOp.
Dataset âˆ†
[39] [68] [67] (Ours) However, performance on the DTD [7] was significantly
Base 69.34 82.69 80.47 80.27 -0.20 lower. Thegeometricalaugmentations,especiallyflipsand
Averageon
11datasets
Novel 74.22 63.22 71.69 72.17 +0.48 rotations,appeartohaveminimaleffectonAAPL,asthey
HM 71.70 71.66 75.83 76.01 +0.18
donotsignificantlyaltertheappearanceoftheoriginalim-
Base 72.43 76.47 75.98 76.53 +0.55 ages in the context of texture. This demonstrates that the
ImageNet Novel 68.14 67.88 70.43 70.57 +0.14
effectivenessofAAPLvariesacrossdifferentdatasets.
HM 70.22 71.92 73.10 73.43 +0.33
Base 96.84 98.00 97.96 97.87 -0.09 4.3.Cross-DatasetTransfer
Caltech101 Novel 94.00 89.81 93.81 95.10 +1.29
HM 95.40 93.73 95.84 96.46 +0.62 ToassesstherobustnessandadaptabilityofAAPL,wetested
itsgeneralizationabilityacrossdatasetsbytrainingitonall
Base 91.17 93.67 95.20 95.63 +0.43
OxfordPets Novel 97.26 95.29 97.69 97.40 -0.29 1000ImageNetclassesandthenapplyingitontheother10
HM 94.12 94.47 96.43 96.51 +0.08 datasets,asshowninTable3. Weassumethatthemodelcan
Base 63.37 78.12 70.49 70.33 -0.16 learnsemanticinformationaboutimagefeaturesbylearning
Stanford
Cars Novel 74.89 60.40 73.59 73.50 -0.09 preciseattributes. Toevaluatethis,weincreasedthemodelâ€™s
HM 68.65 68.13 72.01 71.88 -0.13
focusonlearningaugmentationinformationbysettingboth
Base 72.08 97.60 94.87 95.10 +0.23 hyper-parameters, Î± and Î², to 1 in this experiment and
Flowers102 Novel 77.80 59.67 71.75 70.63 -1.12
afterward.AAPLachieveshighergeneralizationin3datasets:
HM 74.83 74.06 81.71 81.06 -0.65
OxfordPets [38], FGVCAircraft [33], and UCF101 [48],
Base 90.10 88.33 90.70 90.70 +0.00
comparedtoCoCoOp[67]. However,theperformanceon
Food101 Novel 91.22 82.26 91.29 91.60 +0.31
HM 90.66 85.19 90.99 91.15 +0.16 DTD[7]andEuroSAT[12]wasnoticeablypoorerthanother
datasets. Thissuggeststhatthesedatasetsarevulnerableto
Base 27.19 40.44 33.41 34.07 +0.66
FGVC Novel 36.29 22.30 23.71 24.17 +0.46 AAPLâ€™saugmentation-basedpromptlearning.Thesedatasets
Aircraft
HM 31.09 28.75 27.74 28.28 +0.54 arenotobject-centricbutratherpossessglobalfeatures,e.g.,
Base 69.36 80.60 79.74 79.65 -0.09 long-distancesatelliteimagesandtextureimages.Extracting
SUN397 Novel 75.35 65.89 76.86 76.90 +0.04 specificattributesfromthesedatasetsischallengingdueto
HM 72.23 72.51 78.27 78.25 -0.02 theiruniquecharacteristics.
Base 53.24 79.44 77.01 73.90 -3.11
DTD Novel 59.90 41.18 56.00 53.43 -2.57 Source Target
HM 56.37 54.24 64.85 62.02 -2.83
Base 56.48 92.19 87.49 87.00 -0.49
EuroSAT Novel 64.05 54.74 60.04 66.30 +6.26
HM 60.03 68.69 71.21 75.25 +4.04
CoOp 71.51 93.70 89.14 64.51 68.71 85.30 18.47 64.15 41.92 46.39 66.55 63.88
Base 70.53 84.69 82.33 82.20 -0.13
CoCoOp71.02 94.43 90.14 65.32 71.88 86.02 22.94 67.36 45.73 45.37 68.21 65.74
UCF101 Novel 77.50 56.05 73.45 74.27 +0.82 AAPL 71.37 94.17 90.73 65.10 71.67 86.00 23.03 66.80 44.80 41.83 69.30 65.34
HM 73.85 67.46 77.64 78.03 +0.39
Table3.Cross-datasettransferexperiment.Themodelistrained
Table2. Base-to-newgeneralizationexperimentcomparedto ontheentireclassofImageNet(16shots)andevaluatedonthe
baselines. Themodelistrainedfromthebaseclasses(16shots) other10datasets.
andevaluatedinnewclasses.HMdenotestheharmonicmean.âˆ†is
thedifferencebetweenAAPLandCoCoOp.Theboldhighlighting
4.4.DomainGeneralization
indicatesthehighestperformancescores.
For domain generalization, we trained our model on the
wholeImageNetdataset,sameasinSec.4.3,andevaluated
4.2.GeneralizationfromBase-to-NewClasses
iton4datasetsthatrepresentadomainshiftfromImageNet
Wedividedtheclassesequallyintotwogroups,oneforthe (e.g., ImageNetV2 [41], ImageNetSketch [51], ImageNet-
base classes and another for the new classes, i.e., unseen A[14],andImageNet-R[13]).Thecomparisonofthesetests
teNegamI
101hcetlaC stePdrofxO
sraCdrofnatS
201srewolF
101dooF
tfarcriACVGF
793NUS
DTD
TASoruE 101FCU egarevAispresentedinTable4. Weachievedbetterperformanceon Silhouette score H mean delta
alldatasetsexceptforImageNet-A.Thisdemonstratesthat 0.35 8.00
6.53
attribute-specificbiaseffectivelydealswithdomainshift. 0.30 6.00
0.25 4.00
0.20 0.03 0.38 0.01 -0.31 -0.67 0.46 2.00
0.00
Source Target 0.15 0.11
-2.00
ImageNet -V2 -S -A -R Avg. 0.10 -4.00
CLIP 66.73 60.83 46.15 47.77 73.96 57.18 0.05 -4.54 -7.06 -6.00
CoOp 71.51 64.20 47.99 49.71 75.21 59.28 0.00 -8.00
CoCoOp 71.02 64.07 48.75 50.63 76.18 59.91
AAPL 71.37 64.20 48.80 50.60 76.87 60.12
Table 4. Domain Generalization experiment. The model is Figure 6. The correlation between silhouette score and gen-
trainedontheentireclassofImageNet(16shots)andevaluatedon eralizationperformance. Silhouettescoreandthedifferencein
fourdifferentImageNet-baseddatasets,includingdomainshifts. harmonicmeanaccuracyforzero-shotclassificationbetweenCo-
CoOpandAAPL
4.5.AugmentationProfiling
Whyshouldthedeltametatokenlearnaboutattributes
rather than class information? To assess the effec-
Triplet 73.44 95.81 96.18 72.22 80.65 90.70 27.97 78.34 61.73 64.15 78.78 74.54
tivenessoflearningattributes,wecomparedthesilhouette AdTriplet 73.09 96.87 96.44 71.70 82.09 91.10 34.27 77.60 60.31 65.16 78.10 75.16
Figure 6
scores [44] based on augmentation types. The silhouette
Table5.AAPLwithTripletandAdTripletloss.Thecomparison
scoreevaluateshowwelldatapointsareclustered,consider-
ofharmonicmeansofbase-to-newgeneralizationaccuracybetween
ingbothcohesion(proximitywithinthesamecluster)and
AAPLtrainedwithAdTripletlossandtraditionalTripletloss.
separation(distancefromthenearestneighboringcluster).
The silhouette score S(i) for data point i, is calculated
performance improvement with the triplet loss. Utilizing
as follows: S(i)= b(i)âˆ’a(i) , where a(i) is the average thetraditionaltripletlossmethodmeansthatthedeltameta
max{a(i),b(i)}
distanceofitoallotherdatapointsinthesamecluster,and tokenistrainedtobringthesameclasstogetherregardless
b(i) is the average distance of i to the data points in the ofaugmentationtype. Consequently,datasetsthatshowed
nearestclusterthatidoesnotbelongto. Ahighersilhouette improved performance on AdTriplet loss have a higher
score indicates better clustering. In other words, datasets dependencyonclassinformation. Tripletloss-traineddelta
thateffectivelylearninformationaboutaugmentationsfrom metatokenextractsclass-relatedinformation,causingmeta
theAdtripletlosshavehighersilhouettescores. Asshown tokentocontributenoisyfeaturesratherthanclasssemantic
inFig.6,thezero-shotclassificationperformanceofAAPL features when added to prompts. In contrast, AdTriplet
generallyimproves. However,thereisasharpdecreasein loss-trained tokens focus on extracting the class semantic
performanceforDTD[7]andEuroSAT[12]. Thissuggests features. DatasetswithAdTripletlossperformwellbecause
that datasets that cannot effectively extract augmentation they rely more on the class information. This highlights
informationdonotperformwell. Trainingpreciseattributes theadvantageofAAPLbasedonthedatasetâ€™scharacteristics.
todeltametatokeniscrucialforzero-shotclassification,and
itâ€™sevidentthatdeterminingwhatinformationtoaddtothe Which augmentation is effective to prompt learning?
learnablepromptishighlyimportantfordatasetssensitiveto The t-SNE visualization of the delta meta token for 14
AAPL. augmentationsisshown,alongwiththeirsilhouettescores,
inFig.7(a). Itturnedoutthatitisdifficulttodistinguish
WhichdatasetisvulnerableforAAPL? Toassessthe rotations from flips and between color jitters, while other
impact of various datasets on the evaluation of learning augmentations are obvious. All datasets exhibit difficulty
attribute features, we applied AAPLâ€™s proposed AdTriplet indistinguishingtheseaugmentations. Followingselective
loss and the traditional triplet loss method. Unlike the augmentationtraining,whentrainedonlyonaugmentations
AdTriplet loss, the traditional triplet loss trains the delta whoseresultsaregood(showninFig.7(b)),clusteringis
meta token to cluster classes rather than augmentation greatlyenhanced,andsilhouettescoresarealsoraised. Also,
types. As shown in Table 5, when utilizing Adtriplet the average performance for base-to-new generalization
loss across 6 datasets, performance improvement was improved, as seen in Table 6. But when training solely
observed compared to using triplet loss. Particularly, withtheopposite,i.e.,badaugs(Fig.7(c)),thereisneither
FGVCAircraft [33] exhibited approximately a 7% higher significant improvement in silhouette scores nor in the
teNegamI
101hcetlaC stePdrofxO
sraCdrofnatS
201srewolF
101dooF
tfarcriACVGF
793NUS
DTD
TASoruE 101FCU egarevAMethod AAPL GoodAugs BadAugs AAPL WRS âˆ†
ImageNet 73.09 72.91 73.05 StanfordCars 71.70 71.82 +0.12
Caltech101 95.87 96.43 96.00 SUN397 77.60 78.14 +0.54
OxfordPets 96.44 96.49 95.96 DTD 60.31 61.39 +1.08
StanfordCars 71.70 71.85 71.67 EuroSAT 64.15 74.25 +10.10
Flowers102 82.09 80.80 81.74
Food101 91.10 90.45 90.90
Table7.AAPLwithweightedrandomsamplingforvulnerable4
FGVCAircraft 34.27 34.02 18.14
SUN397 77.60 77.97 78.03 datasets.Thecomparisonofharmonicmeansofbase-to-newgen-
DTD 60.31 61.24 61.43 eralizationaccuracy.WRSisshortforweightedrandomsampled
EuroSAT 64.15 66.68 74.70 AAPL.
Ucf101 78.10 77.09 78.11
Average 74.97 75.08 74.52
Cars[26],andSUN397[58],whichhaveinsufficientlearn-
ingofaugmentationtypeinformation. Fortraining,silhou-
Table6.AAPLwithsomeaugmentationtypes.Thecomparison
ette scores were used as thresholds for random sampling
ofharmonicmeansofbase-to-newgeneralizationaccuracywhen
weights. As shown in Table 7, this improved the perfor-
conductingAAPLusingonlygoodaugmentationsandbadaugmen-
manceofbase-to-newgeneralizationacrossall4datasets.
tations.
Notably,EuroSATshowedasignificant10%improvement,
Silhouette scores of âˆ†ğœ‹ t-SNE of âˆ†ğ… emphasizingtheeffectivenessofdynamicallyselectingand
emphasizingweakeraugmentationtypesduringeachepoch.
(a)
)s
e
p
y t 4
1 ( L
P
A
e ro
cs
e tte u
o h liS
cccggghvrrrccsooo oooo rueraao oa
ttt
blll uu
r
trooo aaay
t poi
ess
iz
tttrrrs
c
uss liiio___c _oooaii taajjja
n
flnnniii _nnttt il
t
let ___tt tfa__eee el912ilnb p_rrr r087___ olfu 00lbrg iisrperl eued een
I
f
ot
o
frd se
c
em
h
mao aln
l
nes
tn
it cr ga
i
ft
n
ee
g
as tat uh
u
ra
g
et sma .ett nri tb atu it oe n- ssp ee nc ai bfi lc esfe mat ou rr ee rd oe bc uo sm tlp eo as ri nt ii no gn
A
5.Conclusion
(b)
)s e p
sy gt
u
7 (
e ro
cs
e tte
u
o h
ggghccsoruraaooa buu try poi esszs uss loc _ii taaa n fnn il t le ta__ elnb_ rolfuliisrpe AO
f ime
du
a at
Tr
u
g
rn
r
e
ie
po
s
f
lv
e
eae
a
tnl
tud
la orp
d
e
sp
e
sflr rto
aa
o
da mmc vh
ee
t
rhte
a
se
af tfi
roo
ic
rk
aii
le
ge
ln
yn instl
a
eby
l
ny
i
he msx
au
nt abr
g
ca
t er
ec
a
st fcs
et ci
as
ln
t
ap
g
u
se
r
stc
e
ihi fi.fi
e
cLc
a aeu
ts
v
ige oemm nraea
g
lnn oitt
ne
si sgdc
,
dA liS
enablingprecisediscernmentofattributefeaturesthrough
o
o G augmentationsâ€”afoundationalaspectofourapproach. By
(c)
)s
e
p
sy gt
u
7
A(
e
ro
cs
e tte
u o
h
liS
0.10 cccvrrroooooo etttlll rooo aaatitttrrr ciii___ oooajjj lnnniii _tttt ___ttfeeel912iprrr 087___ 00rgberlued een Ad
r
F
ca
le
u
A
atc
re
P
sto
l
sh
Lym
ie
fi,
irp
w
cnmo
ae
p
ts
o
ii
ri
r
on
n
oe
ng
t m,r too
aa
pud
st tt
ru
kr
lc
s
si eb
t
.e
auu
r
Ia
d
n
nte
t
y
it snr
ua
ui
gbn
n
mu
wd
d
mt ee
is
tr-
ahe
ss
rcm
p
yao
,e
ua
rc
ogn
ei
us
mt fii rc
tc
eh
enb
e
mf te
i aia
pa
n
tst hidu
o
ai
ir
n
n
sse
t
ips
o
sfe
o
onm
t rh
ns
zo
e
a
aer
bp
te
ri
tor
l
ro
ia
-
it
bsm
yc
h
uc
p
o
o
tu
et
f
t-
.
d a decompositioninpromptlearningisunderscoredthrough
B
augmentationprofilingandanalysisofdatasetcorrelations,
augmentations,andAAPLperformance.
Figure7.Thecomparisonofsilhouettescoreandt-SNEofthe
base-to-newgeneralizationforeachofthespecificaugmentation
typesonFGVCAircraft.Allresultsarefromthelastepoch. Acknowledgments Thanks to Prof. George Kamenos
for his invaluable assistance in reviewing and editing this
paper. ThisworkwaspartlysupportedbyInnovativeHuman
averagebase-to-newgeneralizationresults. Theambiguity ResourceDevelopmentforLocalIntellectualizationprogram
ofaugmentationsbetweenflipsandrotationsandbetween through the Institute of Information & Communications
colorjitterslimitsthelearningcapacityofthemetanet. Technology Planning & Evaluation(IITP) grant funded
by the Korea government(MSIT) (IITP-2024-00156287,
AAPLwithweightedrandomsampling Fig.6showsa 40%). ThisresearchwassupportedbytheKoreaInstitute
consistentcorrelationbetweenlowersilhouettescoresand forAdvancementofTechnology(KIAT)grantfundedbythe
worsezero-shotclassificationperformancecomparedtoCo- MinistryofTrade,Industry,andEnergy(MOTIE),Korea,
CoOp[67]acrossseveraldatasets. Insufficientknowledge (P0025331,30%). ThisworkwassupportedbytheNational
ofsemanticfeaturesmakesclassifyingunseenclassesmore Research Foundation of Korea(NRF) grant funded by the
difficult. To address this, an active approach [25, 40, 49] Koreagovernment(MSIT)(No. RS-2023-00252616,30%).
wasutilizedfordatasetsDTD[7],EuroSAT[12],Stanford-References tionmodelforfine-grainedzero-shotlearning. NIPS,2018.
2
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,Antoine
[17] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
HieuPham,QuocLe,Yun-HsuanSung,ZhenLi,andTom
KatherineMillican,MalcolmReynolds,etal. Flamingo: a
Duerig. Scalingupvisualandvision-languagerepresentation
visuallanguagemodelforfew-shotlearning. NeurIPS,2022.
learningwithnoisytextsupervision. InICML,2021. 1,2
1
[18] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie,
[2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
SergeBelongie,BharathHariharan,andSer-NamLim.Visual
Food-101â€“miningdiscriminativecomponentswithrandom
prompttuning. InECCV,2022. 1,2
forests. InECCV,2014. 5
[19] HuajieJiang,RuipingWang,ShiguangShan,andXilinChen.
[3] YannickLeCacheux,HerveLeBorgne,andMichelCrucianu.
Transferablecontrastivenetworkforgeneralizedzero-shot
Modelinginterandintra-classrelationsinthetripletlossfor
learning. InICCV,2019. 2
zero-shotlearning. InICCV,2019. 2
[20] MuhammadGulZainAliKhan,MuhammadFerjadNaeem,
[4] SoravitChangpinyo,Wei-LunChao,BoqingGong,andFei
LucVanGool,AlainPagani,DidierStricker,andMuham-
Sha. Synthesizedclassifiersforzero-shotlearning. InCVPR,
madZeshanAfzal. Learningattentionpropagationforcom-
2016. 2
positionalzero-shotlearning. InWACV,2023. 2
[5] Wei-LunChao,SoravitChangpinyo,BoqingGong,andFei
[21] MuhammadUzairKhattak,HanoonaRasheed,Muhammad
Sha.Anempiricalstudyandanalysisofgeneralizedzero-shot
Maaz,SalmanKhan,andFahadShahbazKhan.Maple:Multi-
learningforobjectrecognitioninthewild. InECCV,2016. 2 modalpromptlearning. InCVPR,2023. 2
[6] TingChen,SimonKornblith,MohammadNorouzi,andGeof- [22] MuhammadUzairKhattak,SyedTalalWasim,Muzammal
freyHinton. Asimpleframeworkforcontrastivelearningof Naseer,SalmanKhan,Ming-HsuanYang,andFahadShahbaz
visualrepresentations. InICML,2020. 4 Khan. Self-regulatingprompts:Foundationalmodeladapta-
[7] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,Sammy tionwithoutforgetting. InICCV,2023. 2
Mohamed,andAndreaVedaldi. Describingtexturesinthe [23] JunsikKim,SeokjuLee,Tae-HyunOh,andInSoKweon.
wild. InCVPR,2014. 5,6,7,8 Co-domainembeddingusingdeepquadrupletnetworksfor
[8] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLi unseentrafficsignrecognition. InAAAI,2018. 5
Fei-Fei. Imagenet:Alarge-scalehierarchicalimagedatabase. [24] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weis-
InCVPR,2009. 5 senborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer,
[9] KawinEthayarajh,DavidDuvenaud,andGraemeHirst. To- MatthiasMinderer, MostafaDehghani, NeilHoulsby, Syl-
wardsunderstandinglinearwordanalogies. InACL,2019. vainGelly,ThomasUnterthiner,andXiaohuaZhai.Animage
4 isworth16x16words:Transformersforimagerecognitionat
[10] LiFei-Fei,RobFergus,andPietroPerona. Learninggener- scale. InICLR,2021. 2
ativevisualmodelsfromfewtrainingexamples: Anincre- [25] Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua.
mentalbayesianapproachtestedon101objectcategories. In Learningactivelearningfromdata. NIPS,2017. 8
CVPRWorkshop,2004. 5 [26] JonathanKrause, MichaelStark, JiaDeng, andLiFei-Fei.
[11] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. 3dobjectrepresentationsforfine-grainedcategorization. In
Deepresiduallearningforimagerecognition.InCVPR,2016. ICCVWorkshop,2013. 5,8
2 [27] BrianLester,RamiAl-Rfou,andNoahConstant. Thepower
[12] Patrick Helber, Benjamin Bischke, Andreas Dengel, and ofscaleforparameter-efficientprompttuning. InEMNLP,
DamianBorth. Eurosat: Anoveldatasetanddeeplearning 2021. 2
benchmarkforlanduseandlandcoverclassification. IEEE [28] XiangLisaLiandPercyLiang. Prefix-tuning: Optimizing
JournalofSelectedTopicsinAppliedEarthObservationsand continuouspromptsforgeneration. InACL,2021.
RemoteSensing,12(7):2217â€“2226,2019. 5,6,7,8 [29] XiaoLiu,KaixuanJi,YichengFu,WengTam,ZhengxiaoDu,
[13] DanHendrycks,StevenBasart,NormanMu,SauravKada- ZhilinYang,andJieTang. P-tuning: Prompttuningcanbe
vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu, comparabletofine-tuningacrossscalesandtasks. InACL,
SamyakParajuli,MikeGuo,etal. Themanyfacesofrobust- 2022. 2
ness:Acriticalanalysisofout-of-distributiongeneralization. [30] Yajing Liu, Yuning Lu, Hao Liu, Yaozu An, Zhuoran Xu,
InICCV,2021. 5,6 ZhuokunYao,BaofengZhang,ZhiweiXiong,andChenguang
[14] DanHendrycks,KevinZhao,StevenBasart,JacobSteinhardt, Gui. Hierarchicalpromptlearningformulti-tasklearning. In
andDawnSong. Naturaladversarialexamples. InCVPR, CVPR,2023. 2
2021. 5,6 [31] YuningLu, JianzhuangLiu, YonggangZhang, YajingLiu,
[15] EladHofferandNirAilon. Deepmetriclearningusingtriplet andXinmeiTian. Promptdistributionlearning. InCVPR,
network. In Similarity-Based Pattern Recognition: Third 2022. 1,2
InternationalWorkshop,SIMBAD2015,Copenhagen,Den- [32] Chengcheng Ma, Yang Liu, Jiankang Deng, Lingxi Xie,
mark,October12-14,2015.Proceedings3.Springer,2015. Weiming Dong, and Changsheng Xu. Understanding and
4 mitigatingoverfittinginprompttuningforvision-language
[16] Zhong Ji, Yanwei Fu, Jichang Guo, Yanwei Pang, models.IEEETransactionsonCircuitsandSystemsforVideo
ZhongfeiMarkZhang,etal. Stackedsemantics-guidedatten- Technology,2023. 2[33] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew [50] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
Blaschko, and Andrea Vedaldi. Fine-grained visual clas- reit,LlionJones,AidanNGomez,ÅukaszKaiser,andIllia
sificationofaircraft. arXivpreprintarXiv:1306.5151,2013. Polosukhin. Attentionisallyouneed. NIPS,2017. 1,2
5,6,7 [51] HaohanWang,SongweiGe,ZacharyLipton,andEricPXing.
[34] MassimilianoMancini,MuhammadFerjadNaeem,Yongqin Learningrobustglobalrepresentationsbypenalizinglocal
Xian,andZeynepAkata.Openworldcompositionalzero-shot predictivepower. NeurIPS,2019. 5,6
learning. InCVPR,2021. 2 [52] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
[35] TomaÂ´sË‡Mikolov,Wen-tauYih,andGeoffreyZweig.Linguistic Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jen-
regularities in continuous space word representations. In niferDy,andTomasPfister. Learningtopromptforcontinual
NAACL,2013. 4 learning. InCVPR,2022. 2
[36] Maria-ElenaNilsbackandAndrewZisserman. Automated [53] KilianQWeinbergerandLawrenceKSaul. Distancemet-
flowerclassificationoveralargenumberofclasses. In2008 riclearningforlargemarginnearestneighborclassification.
SixthIndianconferenceoncomputervision,graphics&image Journalofmachinelearningresearch,10(2),2009. 4
processing,2008. 5
[54] HuiWuandXiaodongShi. Adversarialsoftprompttuning
[37] Venkata Prabhakara Sarath Nookala, Gaurav Verma, Sub- forcross-domainsentimentanalysis. InACL,2022. 4
habrataMukherjee,andSrijanKumar.Adversarialrobustness
[55] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh
ofprompt-basedfew-shotlearningfornaturallanguageun-
Nguyen,MatthiasHein,andBerntSchiele. Latentembed-
derstanding. InACL,2023. 4
dingsforzero-shotclassification. InCVPR,2016. 2
[38] OmkarMParkhi,AndreaVedaldi,AndrewZisserman,and
[56] YongqinXian,BerntSchiele,andZeynepAkata. Zero-shot
CVJawahar. Catsanddogs. InCVPR,2012. 5,6
learning-thegood,thebadandtheugly. InCVPR,2017. 2
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[57] Yongqin Xian, Christoph H Lampert, Bernt Schiele, and
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
ZeynepAkata. Zero-shotlearningâ€”acomprehensiveevalua-
AmandaAskell,PamelaMishkin,JackClark,etal. Learning
tionofthegood,thebadandtheugly. IEEEtransactionson
transferablevisualmodelsfromnaturallanguagesupervision.
patternanalysisandmachineintelligence,41(9):2251â€“2265,
InICML,2021. 1,2,3,5,6
2018. 2
[40] Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok
[58] JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,
Chakraborty, and Sethuraman Panchanathan. Deep active
andAntonioTorralba.Sundatabase:Large-scalescenerecog-
learningforimageclassification. InICIP.IEEE,2017. 8
nitionfromabbeytozoo. InCVPR,2010. 5,8
[41] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
[59] WenjiaXu,YongqinXian,JiuniuWang,BerntSchiele,and
VaishaalShankar. Doimagenetclassifiersgeneralizetoima-
Zeynep Akata. Attribute prototype network for zero-shot
genet? InICML,2019. 5,6
learning. NeurIPS,2020. 2
[42] BernardinoRomera-ParedesandPhilipTorr. Anembarrass-
inglysimpleapproachtozero-shotlearning. InICML,2015. [60] Hantao Yao, Rui Zhang, and Changsheng Xu. Visual-
languageprompttuningwithknowledge-guidedcontextopti-
2
mization. InCVPR,2023. 2
[43] FlorianSchroff, DmitryKalenichenko, andJamesPhilbin.
Facenet:Aunifiedembeddingforfacerecognitionandclus- [61] LeweiYao,RunhuiHuang,LuHou,GuansongLu,Minzhe
tering. InCVPR,2015. 4 Niu,HangXu,XiaodanLiang,ZhenguoLi,XinJiang,and
ChunjingXu.FILIP:Fine-grainedinteractivelanguage-image
[44] Ketan RajshekharShahapure andCharles Nicholas. Clus-
pre-training. InICLR,2022. 1,2
ter quality analysis using silhouette score. In 2020 IEEE
7thinternationalconferenceondatascienceandadvanced [62] MoonYe-Bin,JisooKim,HongyeobKim,KilhoSon,and
analytics(DSAA).IEEE,2020. 7 Tae-HyunOh. Textmania:Enrichingvisualfeaturebytext-
[45] ManliShu,WeiliNie,De-AnHuang,ZhidingYu,TomGold- drivenmanifoldaugmentation. InICCV,2023. 4
stein, Anima Anandkumar, and Chaowei Xiao. Test-time [63] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
prompttuningforzero-shotgeneralizationinvision-language XiyangDai,JianfengGao,HoudongHu,XuedongHuang,
models. NeurIPS,35:14274â€“14289,2022. 1,2 BoxinLi,ChunyuanLi,etal. Florence: Anewfoundation
[46] AmanpreetSingh,RonghangHu,VedanujGoswami,Guil- modelforcomputervision. arXivpreprintarXiv:2111.11432,
laumeCouairon,WojciechGaluba,MarcusRohrbach,and 2021. 1,2
DouweKiela. Flava: Afoundationallanguageandvision [64] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,
alignmentmodel. InCVPR,2022. 1,2 DanielKeysers,AlexanderKolesnikov,andLucasBeyer. Lit:
[47] KihyukSohn.Improveddeepmetriclearningwithmulti-class Zero-shottransferwithlocked-imagetexttuning. InCVPR,
n-pairlossobjective. NIPS,2016. 4 2022. 1,2
[48] KhurramSoomro,AmirRoshanZamir,andMubarakShah. [65] LiZhang,TaoXiang,andShaogangGong. Learningadeep
Ucf101:Adatasetof101humanactionsclassesfromvideos embeddingmodelforzero-shotlearning. InCVPR,2017. 2
inthewild. arXivpreprintarXiv:1212.0402,2012. 5,6 [66] YuhaoZhang,HangJiang,YasuhideMiura,ChristopherD
[49] AlexTamkin,DatNguyen,SalilDeshpande,JesseMu,and Manning, and Curtis P Langlotz. Contrastive learning of
Noah Goodman. Active learning helps pretrained models medicalvisualrepresentationsfrompairedimagesandtext.
learntheintendedtask. NeurIPS,2022. 8 InMachineLearningforHealthcareConference,2022. 1,2[67] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
Liu. Conditionalpromptlearningforvision-languagemodels.
InCVPR,2022. 1,2,3,4,5,6,8
[68] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
Liu. Learningtopromptforvision-languagemodels. Interna-
tionalJournalofComputerVision,130(9):2337â€“2348,2022.
1,2,3,4,5,6
[69] BeierZhu,YuleiNiu,YuchengHan,YueWu,andHanwang
Zhang. Prompt-alignedgradientforprompttuning. InCVPR,
2023. 2