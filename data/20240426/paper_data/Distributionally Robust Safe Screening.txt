Distributionally Robust Safe Screening
Hiroyuki Hanadaâˆ—â€  Satoshi Akahaneâ€¡ Tatsuya Aoyamaâ€¡ Tomonari Tanakaâ€¡
Yoshito Okuraâ€¡ Yu InatsuÂ§ Noriaki Hashimotoâˆ— Taro MurayamaÂ¶ Lee HanjuÂ¶
Shinya KojimaÂ¶ Ichiro Takeuchiâ€¡âˆ—â€–
April 26, 2024
Abstract 1 Introduction
In this study, we consider the problem of identi-
fying unnecessary samples and features in a class
In this study, we propose a method Distributionally
of supervised learning problems within dynamically
Robust Safe Screening (DRSS), for identifying unnec-
changing environments. Identifying unnecessary sam-
essary samples and features within a DR covariate
ples/features offers several benefits. It helps in de-
shift setting. This method effectively combines DR
creasing the storage space required for keeping the
learning, a paradigm aimed at enhancing model ro-
training data for updating the machine learning (ML)
bustness against variations in data distribution, with
modelsinthefuture. Moreover,insituationsdemand-
safe screening (SS), a sparse optimization technique
ing real-time adaptation of ML models to quick envi-
designed to identify irrelevant samples and features
ronmental changes, the use of fewer samples/features
priortomodeltraining. ThecoreconceptoftheDRSS
enables more efficient learning.
method involves reformulating the DR covariate-shift
problem as a weighted empirical risk minimization Ourbasicideatotacklethisproblemistoeffectively
problem,wheretheweightsaresubjecttouncertainty combinedistributionallyrobust(DR)learningandsafe
within a predetermined range. By extending the SS screening (SS). DR learning is a ML paradigm that
technique to accommodate this weight uncertainty, focuses on developing models robust to variations in
the DRSS method is capable of reliably identifying the data distribution, providing performance guaran-
unnecessary samples and features under any future tees across different distributions (see, e.g., [1]). On
distribution within a specified range. We provide a the other hand, SS refers to sparse optimization tech-
theoretical guarantee of the DRSS method and vali- niques that can identify irrelevant samples/features
date its performance through numerical experiments before model training, ensuring computational effi-
on both synthetic and real-world datasets. ciency by avoiding unnecessary computations on cer-
tain samples/features which do not contribute to the
final solution [2, 3]. The key technical idea of SS is
to identify a bound of the optimal solution before
solving the optimization problem. This allows for the
âˆ—RIKEN,Wako,Saitama,Japan identification of unnecessary samples/features, even
â€ hiroyuki.hanada@riken.jp without knowing the optimal solution.
â€¡NagoyaUniversity,Nagoya,Aichi,Japan
As a specific scenario of dynamically changing en-
Â§NagoyaInstituteofTechnology,Nagoya,Aichi,Japan
Â¶DENSOCORPORATION,Kariya,Aichi,Japan vironment, we consider covariate shift setting [4, 5]
â€–ichiro.takeuchi@mae.nagoya-u.ac.jp with unknown test distribution. In this setting, the
1
4202
rpA
52
]LM.tats[
1v82361.4042:viXradistributionofinputfeaturesinthetrainingdatamay B C
undergo changes in the test phase, yet the actual Unknown Distributionally
Test Distribution Robust
nature of these changes remains unknown. A ML Safe Screening
problem (e.g., regression/classification problem) in A D
covariateshiftsettingcanbeformulatedasaweighted
empirical risk minimization (weighted ERM) problem,
where weights are assigned based on the density ratio
of each sample in the training and test distributions.
Namely, by assigning higher weights to training sam-
ples that are important in the test distribution, the
model can focus on learning from relevant samples
and mitigate the impact of distribution differences
DRSS Method (Proposed)
betweenthetrainingandthetestphases. Ifthedistri-
Figure 1: Schematic illustration of the proposed Dis-
butionduringthetestphaseisknown,theweightscan
tributionally Robust Safe Screening (DRSS) method.
be uniquely fixed. However, if the test distribution
Panel A displays the training samples, each assigned
is unknown, it is necessary to solve a weighted ERM
equal weight, as indicated by the uniform size of the
problem with unknown weights.
points. Panel B depicts various unknown test distri-
Our main contribution is to propose a DRSS
butions, highlighting how the significance of training
method for covariate shift setting with unknown test
samples varies with different realizations of the test
distribution. The proposed method can identify un-
distribution. Panel C shows the outcomes of safe
necessary samples/features regardless of how the dis-
sample screening (SsS) across multiple realizations
tribution changes within a certain range in the test
of test distributions. Finally, Panel D presents the
phase. To address this problem, we extend the exist-
results of the proposed DRSS method, demonstrating
ing SS methods in two stages. The first is to extend
itscapabilitytoidentifyredundantsamplesregardless
the SS for ERM so that it can be applied to weighted
of the observed test distribution.
ERM. The second is to further extend the SS so that
it can be applied to weighted ERM when the weights
are unknown. While the first extension is relatively
straightforward, the second extension presents a non-
trivial technical challenge (Figure 1). To overcome
thischallenge, wederivea novelboundofthe optimal
solutions of the weighted ERM problem, which prop- covariate-shift setting where the input distribution
erly accounts for the uncertainty in weights stemming of an ERM problem changes within a certain range.
from the uncertainty of the test distribution. In this setting, we propose a novel method called
In this study, we consider DRSS for samples in DRSSmethodthatcanidentifysamples/featuresthat
sample-sparse models such as SVM [6], and that for are guaranteed not to affect the optimal solution, re-
features for feature-sparse models such as Lasso [7]. gardless of how the distribution changes within the
We denote the DRSS for samples as distributionally specified range. Finally, through numerical exper-
robustsafesample screening(DRSsS)andthatforfea- iments, we verify the effectiveness of the proposed
tures as distributionally robust safe feature screening DRSS method. Although the DRSS method is devel-
(DRSfS), respectively. oped for convex ERM problems, in order to demon-
Our contributions in this study are summarized strate the applicability to deep learning models, we
as follows. First, by effectively combining DR and also present results where the DRSS method is ap-
SS, we introduce a framework for identifying unnec- plied in a problem setting where the final layer of the
essary samples/features under dynamically changing model is fine-tuned according to changes in the test
uncertain environment. Second, We consider a DR distribution.
2
gnineercS
elpmaS
efaS1.1 Related Works
Table1: Notationsusedinthepaper. R: allrealnum-
bers, N: all positive integers, n,m,p âˆˆ N: integers,
TheDRsettinghasbeenexploredinvariousMLprob-
f : Rn â†’ Râˆª{+âˆž}: convex function, M âˆˆ RnÃ—m:
lems, aiming to enhance model robustness against
matrix, v âˆˆRn: vector.
data distribution variations. A DR learning problem
m âˆˆR (small case of matrix variable)
is typically formulated as a worst-case optimization ij
the element at the ith row and
problem since the goal of DR learning is to ensure
the jth column of M
model performance under the worst-case data dis-
v âˆˆR (nonbold font of vector variable)
tribution within a specified range. Hence, a variety i
the ith element of v
of optimization techniques tailored to DR learning
M âˆˆR1Ã—n the ith row of M
have been investigated within both the ML and opti- i:
mization communities [8, 9, 1]. The proposed DRSS M :j âˆˆRmÃ—1 the jth column of M
method is one of such DR learning methods, focusing [n] {1,2,...,n}
specificallyontheproblemofsample/featuredeletion. R â‰¥0 all nonnegative real numbers
The ability to identify irrelevant samples/features is âŠ— elementwise product
of practical significance. For example, in the context diag(v)âˆˆRnÃ—n diagonal matrix; (diag(v)) ii =v i
of continual learning (see, e.g., [10]), it is crucial to and (diag(v)) ij =0 (iÌ¸=j)
effectively manage data by selectively retaining and vâ–¡Ã—M âˆˆRnÃ—m diag(v)M
discarding samples/features, especially in anticipa- 0 n âˆˆRn [0,0,...,0]âŠ¤ (vector of size n)
tion of changes in future data distributions. Incorrect 1 n âˆˆRn [1,1,...,1]âŠ¤ (vector of size n)
deletion of essential data can lead to catastrophic for- âˆ¥vâˆ¥ p âˆˆR â‰¥0 ((cid:80)n i=1v ip)1/p (p-norm)
getting [11], a phenomenon where a ML model, after âˆ‚f(v)âŠ†Rn all g âˆˆRn s.t. â€œfor any vâ€² âˆˆRn,
being trained on new data, quickly loses information
f(vâ€²)âˆ’f(v)â‰¥gâŠ¤(vâ€²âˆ’v)â€
previously learned from older datasets. The proposed (subgradient)
DRSS method tackles this challenge by identifying Z[f]âŠ†Rn {vâ€² âˆˆRn |âˆ‚f(vâ€²)={0 n}}
samples/features that, regardless of future data dis- fâˆ—(v)âˆˆRâˆª{+âˆž} sup vâ€²âˆˆRn(vâŠ¤vâ€²âˆ’f(vâ€²))
tribution shifts, will not have any influence on all (convex conjugate)
possible newly trained model in the future. â€œf is Îº-strongly f(v)âˆ’Îºâˆ¥vâˆ¥2 2 is convex with
convexâ€ (Îº>0) respect to v
SS refers to optimization techniques in sparse
â€œf is Âµ-smoothâ€ âˆ¥f(v)âˆ’f(vâ€²)âˆ¥ â‰¤Âµâˆ¥vâˆ’vâ€²âˆ¥
learning that identify and exclude irrelevant sam- 2 2
(Âµ>0) for any v,vâ€² âˆˆRn
ples or features from the learning process. SS can
reduce computational cost without changing the fi-
nal trained model. Initially, SfS was introduced by
[2] for the Lasso. Subsequently, SsS was proposed
by [3] for the SVM. Among various SS methods de- knowledge, noexistingstudieshaveutilizedSSwithin
veloped so far, the most commonly used is based the DR learning framework.
on the duality gap [12, 13]. Our proposed DRSS
method also adopts this approach. Over the past
decade, SS has seen diverse developments, including
methodological improvements and expanded applica-
tion scopes [14, 15, 16, 17, 18, 19, 20, 21]. Unlike
other SS studies that primarily focused on reducing
2 Preliminaries
computational costs, this study adopts SS for a dif-
ferent purpose. We employ SS across scenarios where
data distribution varies within a defined range, aim-
ing to discard unnecessary samples/features. To our Notations used in this paper are described in Table 1.
32.1 Weighted Regularized Empiri- we have the following dual problem of (1):
cal Risk Minimization (Weighted
Î±âˆ—(w) :=argmaxD (Î±), where
RERM) for Linear Prediction w
Î±âˆˆRn
D (Î±):= (2)
w
We mainly assume the weighted regularized empirical
n
risk minimization (weighted RERM) for linear pre- âˆ’(cid:88) w â„“âˆ— (âˆ’Î³ Î± )âˆ’Ïâˆ—(((Î³âŠ—w)â–¡Ã—XË‡)âŠ¤Î±),
diction. This may include kernelized versions, which i yi i i
i=1
are discussed in Appendix C. Suppose that we learn
the model parameters as linear prediction coefficients, where Î³ is a positive-valued vector. The relationship
that is, learn Î²âˆ—(w) âˆˆRd such that the outcome for a between the original problem (1) (called the primal
sample xâˆˆRd is predicted as xâŠ¤Î²âˆ—(w). problem) and the dual problem (2) are described as
follows:
Definition 2.1. Given n training samples of d-
P (Î²âˆ—(w))=D (Î±âˆ—(w)), (3)
w w
dimensional input variables, scalar output variables
and scalar sample weights, denoted by X âˆˆ RnÃ—d, Î²âˆ—(w) âˆˆâˆ‚Ïâˆ—(((Î³âŠ—w)â–¡Ã—XË‡)âŠ¤Î±âˆ—(w)), (4)
y puâˆˆ taR tin onan od
f
ww eiâˆˆ ghR tn â‰¥ ed0, Rre Es Rpe Mcti fv oe rly li, nt eh ae rt pr ra ein di in ctg ioc nom is- âˆ€iâˆˆ[n]: âˆ’Î³ iÎ± iâˆ—(w) âˆˆâˆ‚â„“ yi(XË‡ i:Î²âˆ—(w)). (5)
formulated as follows:
2.2 Sparsity-inducing Loss Functions
and Regularization Functions
Î²âˆ—(w) :=argminP (Î²), where
w
Î²âˆˆRd
In weighted RERM, we call that a loss function â„“
P
(Î²):=(cid:88)n
w â„“ (XË‡ Î²)+Ï(Î²). (1)
induces sample-sparsity if elements in Î±âˆ—(w) are easy
w i yi i: to become zero. Due to (5), this can be achieved by â„“
i=1 such that {t âˆˆ R | 0 âˆˆ âˆ‚â„“ (t)} is not a point but an
y
interval.
Here, â„“ :Râ†’R is a convex loss function1, Ï:Rd â†’
y Similarly, we call that a regularization function Ï
R is a convex regularization function, and XË‡ âˆˆRnÃ—d
induces feature-sparsity if elements in Î²âˆ—(w) are easy
is a matrix calculated from X and y and determined
to become zero. Due to (4), this can be achieved by
dependingonâ„“. Inthispaper, unlessotherwisenoted, Ï such that {v âˆˆRd |âˆƒj âˆˆ[dâˆ’1]: 0âˆˆ[âˆ‚Ïâˆ—(v)] } is
we consider binary classifications (y âˆˆ {âˆ’1,+1}n) j
not a point but a region.
with XË‡ :=yâ–¡Ã—X. For regressions (y âˆˆRn) we usually
For example, the hinge loss â„“ (t)=max{0,1âˆ’t}
set XË‡ :=X. y
(y âˆˆ{âˆ’1,+1}) is a sample-sparse loss function since
{t âˆˆ R | 0 âˆˆ âˆ‚â„“ (t)} = [1,+âˆž). Similarly, the L1-
y
Remark 2.2. Weaddthat,weadopttheformulation
regularization Ï(v) =
Î»(cid:80)dâˆ’1|v
| (Î» > 0: hyperpa-
X :d =1 n so that Î² dâˆ—(w) (the last element) represents rameter) is a feature-sparsj e=1 reguj larization function
the common coefficient for any sample (called the since {v âˆˆRd |âˆƒj âˆˆ[dâˆ’1]: 0âˆˆ[âˆ‚Ïâˆ—(v)] }={v âˆˆ
j
intercept). Rd | âˆƒj âˆˆ [dâˆ’1] : |v | â‰¤ Î», v = 0}. See Section 4
j d
for examples of using them.
Sinceâ„“andÏareconvex,wecaneasilyconfirmthat
P (Î²) is convex with respect to Î².
w 3 Distributionally Robust Safe
ApplyingFenchelâ€™sdualitytheorem(AppendixA.2),
Screening
1Forâ„“y(t),weassumethatonlytisavariableofthefunction
In this section we show DRSS rules for weighted
(y isassumedtobeaconstant)whenwetakeitssubgradient
orconvexconjugate. RERM with two steps. First, in Sections 3.1 and
43.2, we show SS rules for weighted RERM but not 3.2 (Non-DR) Safe Feature Screening
DR setup. To do this, we extended existing SS rules
We consider identifying j âˆˆ [d] such that Î²âˆ—(w) = 0,
in [13, 15]. Then we derive DRSS rules in Section 3.3. j
that is, identifying that the jth feature is not used in
the prediction, even when the sample weights w are
3.1 (Non-DR) Safe Sample Screening
changed.
We consider identifying training samples that do not Forsimplicity, supposethattheregularizationfunc-
affect the training result Î²âˆ—(w). Due to the relation- tion Ï is decomposable, that is, Ï is represented as
ship (4), if there exists i âˆˆ [n] such that Î± iâˆ—(w) = 0, Ï(Î²):=(cid:80)d j=1Ïƒ j(Î² j) (Ïƒ 1,Ïƒ 2,...,Ïƒ d: Râ†’R). Then,
then the ith row (sample) in XË‡ does not affect Î²âˆ—(w). since Ïâˆ—(v)=(cid:80)d Ïƒâˆ—(v ) and therefore [âˆ‚Ïâˆ—(v)] =
j=1 j j j
However, since computing Î±âˆ—(w) is as costly as Î²âˆ—(w), âˆ‚Ïƒâˆ—(v ), from (4) we have
j j
it is difficult to use the relationship as it is. To solve
the problem, the SsS first considers identifying the Î²âˆ—(w) âˆˆâˆ‚Ïƒâˆ—((Î³âŠ—wâŠ—XË‡ )âŠ¤Î±âˆ—(w))
possible region Bâˆ—(w) âŠ‚ Rd such that Î²âˆ—(w) âˆˆ Bâˆ—(w) j j :j
isassured. Then,withBâˆ—(w) and(5),wecanconclude
=âˆ‚Ïƒâˆ—(XË‡Ë‡(Î³,w)âŠ¤Î±âˆ—(w)),
j :j
that the ith training sample do not affect the training where XË‡Ë‡(Î³,w) :=Î³âŠ—wâŠ—XË‡ .
result Î²âˆ—(w) if (cid:83) âˆ‚â„“ (XË‡ Î²)={0}. :j :j
Î²âˆˆBâˆ—(w) yi i:
FirstweshowhowtocomputeBâˆ—(w). Inthispaper If we know Î±âˆ—(w), we can identify whether Î²âˆ—(w) =0
we adopt the computation methods that is available j
holds. However, like SsS (Section 3.1), we would like
when the regularization function Ï in P (and also
w to check the condition without computing Î±âˆ—(w) or
P itself) of (1) are strongly convex.
w Î²âˆ—(w).
So, like SsS, SfS first considers identifying the pos-
Lemma 3.1. Suppose that Ï in P (and also P
w w sible region Aâˆ—(w) âŠ‚ Rn such that Î±âˆ—(w) âˆˆ Aâˆ—(w) is
itself) of (1) are Îº-strongly convex. Then, for any
Î²Ë†âˆˆRd and Î±Ë† âˆˆRn, we can assure Î²âˆ—(w) âˆˆBâˆ—(w) by assured. Then we can conclude that Î² jâˆ—(w) = 0 is
taking assured if (cid:83) âˆ‚Ïƒâˆ—(XË‡Ë‡(Î³,w)âŠ¤Î±)={0}.
Î±âˆˆAâˆ—(w) j :j
(cid:110) (cid:12) (cid:111) ThenweshowhowtocomputeAâˆ—(w). WithLemma
Bâˆ—(w) := Î² (cid:12) (cid:12)âˆ¥Î²âˆ’Î²Ë†âˆ¥ 2 â‰¤r(w,Î³,Îº,Î²Ë†,Î±Ë†) , A.3, we can calculate Aâˆ—(w) as follows, if the loss
(cid:114) function â„“ in P of (1) is smooth:
2 y w
where r(w,Î³,Îº,Î²Ë†,Î±Ë†):= [P (Î²Ë†)âˆ’D (Î±Ë†)].
Îº w w Lemma 3.3. Suppose that â„“ in P of (1) is Âµ-
y w
smooth. Then, for any Î²Ë† âˆˆRd and Î±Ë† âˆˆRn, we can
The proof is presented in Appendix A.3. The
amount P (Î²Ë†)âˆ’D (Î±Ë†) is known as the duality gap, assure Î±âˆ—(w) âˆˆAâˆ—(w) by taking
w w
which must be nonnegative due to (3). So we ob- (cid:110) (cid:12) (cid:111)
Aâˆ—(w) := Î±(cid:12)âˆ¥Î±âˆ’Î±Ë†âˆ¥ â‰¤rÂ¯(w,Î³,Âµ,Î²Ë†,Î±Ë†) ,
tainthefollowinggap safe sample screening rule from (cid:12) 2
Lemma 3.1: where rÂ¯(w,Î³,Âµ,Î²Ë†,Î±Ë†):=
Lemma 3.2. Under the same assumptionsas Lemma (cid:115)
2Âµ
d3 o.1 e, sÎ± niâˆ— o( tw a) ff= ec0
t
ti hs ea ts rs au ir ne id ng(i r.e e. s, ult the Î²âˆ—it (h w)t )ra ii fn ti hn eg resa em xip stle
s
min iâˆˆ[n]w iÎ³
i2[P w(Î²Ë†)âˆ’D w(Î±Ë†)].
Î²Ë†âˆˆRd and Î±Ë† âˆˆRn such that The proof is presented in Appendix A.5. Similar to
Lemma 3.2, we obtain the gap safe feature screening
[XË‡ Î²Ë†âˆ’âˆ¥XË‡ âˆ¥ r(w,Î³,Îº,Î²Ë†,Î±Ë†),
i: i: 2 rule from Lemma 3.3:
XË‡ Î²Ë†+âˆ¥XË‡ âˆ¥ r(w,Î³,Îº,Î²Ë†,Î±Ë†)]âŠ†Z[â„“ ].
i: i: 2 yi
Lemma 3.4. Under the same assumptionsasLemma
The proof is presented in Appendix A.4. 3.3, Î²âˆ—(w) = 0 is assured (i.e., the jth feature does
j
5not affect prediction results) if there exists Î²Ë† âˆˆ Rd Similarly, the DRSfS rule for W is calculated as:
and Î±Ë† âˆˆRn such that
[Lâˆ’NR,L+NR]âŠ†Z[Ïƒâˆ—], where
j
[XË‡Ë‡ :( jÎ³,w)âŠ¤Î±Ë† âˆ’âˆ¥XË‡Ë‡ :( jÎ³,w)âˆ¥ 2rÂ¯(w,Î³,Âµ,Î²Ë†,Î±Ë†),
L:= min XË‡Ë‡(Î³,w)âŠ¤Î±âˆ—(wËœ) = min(Î³âŠ—XË‡ âŠ—Î±âˆ—(wËœ))âŠ¤w,
:j :j
XË‡Ë‡(Î³,w)âŠ¤Î±Ë† +âˆ¥XË‡Ë‡(Î³,w)âˆ¥ rÂ¯(w,Î³,Âµ,Î²Ë†,Î±Ë†)]âŠ†Z[Ïƒâˆ—]. wâˆˆW wâˆˆW
:j :j 2 j L:= maxXË‡Ë‡(Î³,w)âŠ¤Î±âˆ—(wËœ) = max(Î³âŠ—XË‡ âŠ—Î±âˆ—(wËœ))âŠ¤w,
:j :j
wâˆˆW wâˆˆW
The proof is almost same as Lemma 3.2. (cid:114)
N := maxâˆ¥XË‡Ë‡(Î³,w)âˆ¥ = maxâˆ¥wâŠ—Î³âŠ—XË‡ âˆ¥2,
:j 2 :j 2
wâˆˆW wâˆˆW
3.3 Application to Distributionally Ro- R:= maxrÂ¯(w,Î³,Âµ,Î²âˆ—(wËœ),Î±âˆ—(wËœ)).
wâˆˆW
bust Setup
Thus, solving the maximizations and/or minimiza-
InSections3.1and3.2weshowedtheconditionswhen
tions in Theorem 3.7 provides DRSsS and DRSfS
samples or features are screened out. In this section
rules. However,howtosolveitlargelydependsonthe
we show how to use the conditions for the change of
choice of â„“, Ï and W. In Section 4 we show specific
sample weights w.
calculations of Theorem 3.7 for some typical setups.
Definition 3.5 (weight-changing safe screening
(WCSS)). Given X âˆˆ RnÃ—d, y âˆˆ Rn, wËœ âˆˆ Rn and
â‰¥0 4 DRSS for Typical ML Setups
w âˆˆRn , suppose that Î²âˆ—(wËœ) in Definition 2.1 (and
â‰¥0
also Î±âˆ—(wËœ)) are already computed, but Î²âˆ—(w) not.
InthissectionweshowDRSSrulesderivedinSection
Then WCSsS (resp. WCSfS) from wËœ to w is de-
3.3 for two typical ML setups: DRSsS for L1-loss L2-
fined as finding i âˆˆ [n] satisfying Lemma 3.2 (resp.
regularized SVM (Section 4.1) and DRSfS for L2-loss
j âˆˆ[dâˆ’1] satisfying Lemma 3.4).
L1-regularized SVM (Section 4.2) under W := {w |
âˆ¥wâˆ’wËœâˆ¥ â‰¤S}.
Definition3.6(Distributionallyrobustsafescreening 2
In the processes, we need to solve constrained max-
(DRSS)). Given X âˆˆ RnÃ—d, y âˆˆ Rn, wËœ âˆˆ Rn and
â‰¥0 imizations of convex functions. Although maximiza-
W âŠ‚Rn , suppose that Î²âˆ—(wËœ) in Definition 2.1 (and
â‰¥0 tionsofconvexfunctionsarenoteasyingeneral(min-
also Î±âˆ—(wËœ)) are already computed. Then the DRSsS imizations are easy), we show that the maximizations
(resp. DRSfS) for W is defined as finding i âˆˆ [n] need in the processes can be algorithmically solved in
satisfying Lemma 3.2 (resp. j âˆˆ [dâˆ’1] satisfying Section 4.3.
Lemma 3.4) for any w âˆˆW.
For Definition 3.5, we have only to apply SS rules 4.1 DRSsS for L1-loss L2-regularized
in Lemma 3.2 or 3.4 by setting Î²Ë†â†Î²âˆ—(wËœ) and Î±Ë† â† SVM
Î±âˆ—(wËœ). Ontheotherhand, forDefinition3.6, weneed
to maximize or minimize the interval in Lemma 3.2 L1-loss L2-regularized SVM is a sample-sparse model
or 3.4 in w âˆˆW. forbinaryclassification(y âˆˆ{âˆ’1,+1}n)thatsatisfies
thepreconditionstoapplySsS(Lemma3.1). Detailed
Theorem 3.7. The DRSsS rule for W is calculated calculations are presented in Appendix B.1.
as: For L1-loss L2-regularized SVM, we set Ï and â„“ as:
[XË‡ i:Î²âˆ—(wËœ)âˆ’âˆ¥XË‡ i:âˆ¥ 2R,XË‡ i:Î²âˆ—(wËœ)+âˆ¥XË‡ i:âˆ¥ 2R]âŠ†Z[â„“ yi], Ï(Î²):= Î» 2âˆ¥Î²âˆ¥2
2
(Î»>0: hyperparameter),
where R:=max r(w,Î³,Îº,Î²âˆ—(wËœ),Î±âˆ—(wËœ)). â„“ y(t):=max{0,1âˆ’t} (where y âˆˆ{âˆ’1,+1}).
wâˆˆW
6ThenÏisÎ»-stronglyconvex. SettingÎ³ =1 ,thedual 4.2 DRSfS for L2-loss L1-regularized
n
objective function is described as SVM
D (Î±)=
w L2-loss L1-regularized SVM is a feature-sparse model
ï£± (cid:80)n w Î± âˆ’ 1 Î±âŠ¤(wâ–¡Ã—XË‡)(wâ–¡Ã—XË‡)âŠ¤Î±, forbinaryclassification(y âˆˆ{âˆ’1,+1}n)thatsatisfies
ï£² i=1 i i 2Î»
(âˆ€iâˆˆ[n]:0â‰¤Î± â‰¤1) (6) thepreconditionstoapplySfS(Lemma3.3). Detailed
i
ï£³ âˆ’âˆž. (otherwise) calculations are presented in Appendix B.2.
For L2-loss L1-regularized SVM, we set Ïƒ (and
j
Here, in the viewpoint of minimization, we may con- consequently Ï) and â„“ as:
sider this problem as a maximization with the con-
straint â€œâˆ€iâˆˆ[n]:0â‰¤Î± â‰¤1â€.
i
âˆ€j âˆˆ[dâˆ’1]: Ïƒ (Î² ):=Î»|Î² | (Î»>0: hyperparameter),
Optimality conditions (4) and (5) are described as: j j j
Ïƒ (Î² ):=0,
d d
Î²âˆ—(w) = 1 (wâ–¡Ã—XË‡)âŠ¤Î±âˆ—(w), (7) â„“ y(t):=(max{0,1âˆ’t})2 (where y âˆˆ{âˆ’1,+1}).
Î»
ï£± {1}, (XË‡ Î²âˆ—(w) â‰¤1)
ï£´ï£² i:
Notice that Ïƒ (Î² ) is not defined as Î»|Î² | but 0: we
âˆ€iâˆˆ[n]: Î±âˆ—(w) âˆˆ [0,1], (XË‡ Î²âˆ—(w) =1) (8) d d d
i i: rarely regularize the intercept with L1-regularization.
ï£´ï£³{0}. (XË‡ Î²âˆ—(w) â‰¥1)
i:
Setting Î³ = Î»1 , the dual objective function is
n
described as
Noticing that Z[â„“ ] = (1,+âˆž), by Theorem 3.7,
yi
the DRSsS rule for W is calculated as:
D
(Î±)=(cid:40) âˆ’Î»(cid:80)n i=1w iÎ»Î±2 iâˆ’ 44Î±i, ((11)â€“(13) are met)
XË‡ Î²âˆ—(wËœ)âˆ’âˆ¥XË‡ âˆ¥ maxr(w,Î³,Îº,Î²âˆ—(wËœ),Î±âˆ—(wËœ))>1, w âˆ’âˆž, (otherwise)
i: i: 2
wâˆˆW
(10)
where (9)
where Î± â‰¥0, (11)
r(w,Î³,Îº,Î²âˆ—(wËœ),Î±âˆ—(wËœ)) i
âˆ€j âˆˆ[dâˆ’1]: |(wâŠ—XË‡ )âŠ¤Î±|â‰¤1, (12)
(cid:114) :j
2
:= [P (Î²âˆ—(wËœ))âˆ’D (Î±âˆ—(wËœ))], (wâŠ—XË‡ )âŠ¤Î±=(wâŠ—y)âŠ¤Î±=0. (13)
Îº w w :d
P (Î²âˆ—(wËœ))âˆ’D (Î±âˆ—(wËœ))
w w
n Optimality conditions (4) and (5) are described as
:=(cid:88) w [â„“ (XË‡ Î²âˆ—(wËœ))âˆ’Î±âˆ—(wËœ)]+Î»âˆ¥Î²âˆ—(wËœ)âˆ¥2
i yi i: i 2
i=1
+
1
wâŠ¤(Î±âˆ—(wËœ)â–¡Ã—XË‡)(Î±âˆ—(wËœ)â–¡Ã—XË‡)âŠ¤w.
âˆ€j âˆˆ[dâˆ’1]: |(wâŠ—XË‡ :j)âŠ¤Î±âˆ—(w)|<1â‡’Î² jâˆ—(w) =0,
2Î» (14)
2
Here,wecanfindthatP w(Î²âˆ—(wËœ))âˆ’D w(Î±âˆ—(wËœ)),which âˆ€iâˆˆ[n]: Î± iâˆ—(w) = Î»max{0,1âˆ’XË‡ i:Î²âˆ—(w)}. (15)
we need to maximize in reality, is the sum of linear
function and convex quadratic function with respect
tow âˆˆW. (Since(Î±âˆ—(wËœ)â–¡Ã—XË‡)(Î±âˆ—(wËœ)â–¡Ã—XË‡)âŠ¤ ispositive NoticingthatZ[Ïƒâˆ—]=(âˆ’Î»,Î»),byTheorem3.7,the
j
semidefinite,weknowthatitisconvexquadratic). Al- DRSfS rule for W is calculated as:
thoughconstrainedmaximizationofaconvexfunction
is difficult in general, for this case we can algorithmi-
Lâˆ’NR>âˆ’Î», L+NR<Î»,
cally maximize it (Section 4.3).
7where problems:
maxwâŠ¤Aw+2bâŠ¤w, (16)
L:=Î» min(XË‡ âŠ—Î±âˆ—(wËœ))âŠ¤w,
:j wâˆˆW
wâˆˆW
where W :={w âˆˆRn |âˆ¥wâˆ’wËœâˆ¥ â‰¤S},
L:=Î»max(XË‡ âŠ—Î±âˆ—(wËœ))âŠ¤w, 2
wâˆˆW :j wËœ âˆˆRn, bâˆˆRn,
(cid:114)
N :=Î» maxâˆ¥wâŠ—XË‡ âˆ¥2 AâˆˆRnÃ—n : symmetric, positive semidefinite,
:j 2
wâˆˆW
nonzero.
(cid:114)
=Î» max{wâŠ¤diag(XË‡ âŠ—XË‡ )w},
:j :j
wâˆˆW Lemma 4.1. The maximization (16) is achieved by
R:= maxrÂ¯(w,Î³,Âµ,Î²âˆ—(wËœ),Î±âˆ—(wËœ)), the following procedure. First, we define Q âˆˆ RnÃ—n
wâˆˆW and Î¦ := diag(Ï• ,Ï• ,...,Ï• ) as the eigendecompo-
1 2 n
rÂ¯(w,Î³,Âµ,Î²âˆ—(wËœ),Î±âˆ—(wËœ)) sition of A such that A = QâŠ¤Î¦Q, Q is orthogonal
(cid:115) (QQâŠ¤ =QâŠ¤Q=I). Also,letÎ¾ :=âˆ’Î¦QwËœâˆ’QbâˆˆRn,
2Âµ
:= [P (Î²âˆ—(wËœ))âˆ’D (Î±âˆ—(wËœ))], and
min w Î³2 w w
iâˆˆ[n] i i
T(Î½)=(cid:88)n (cid:18)
Î¾
i
(cid:19)2
. (17)
Î½âˆ’Ï•
i
i=1
P w(Î²âˆ—(wËœ))âˆ’D w(Î±âˆ—(wËœ)) Then, the maximization (16) is equal to the largest
=(cid:88)n
w
(cid:34)
â„“ (XË‡
Î²âˆ—(wËœ))+Î»Î»(Î±âˆ—(wËœ))2
i
âˆ’4Î± iâˆ—(wËœ)(cid:35) value among them:
i yi i: 4 â€¢ ForeachÎ½ suchthatT(Î½)=S2 (seeLemma4.2),
i=1
+Ï(Î²âˆ—(wËœ)). the value Î½S2+(Î½wËœ+b)âŠ¤QâŠ¤(Î¦âˆ’Î½I)âˆ’1Î¾+bâŠ¤wËœ,
and
Here, the expressions in L and L are linear with â€¢ For each Î½ âˆˆ {Ï• 1,Ï• 2,...,Ï• n} (duplication re-
respecttow,andtheexpressioninN insidethesquare moved) such that â€œâˆ€i âˆˆ [n] : Ï• i = Î½ â‡’ Î¾ i = 0â€,
root is convex and quadratic with respect to w. Also, the value
R is decomposed to two maximizations 2Âµ
miniâˆˆ[n]wiÎ³ i2 max[Î½S2+(Î½wËœ +b)âŠ¤QâŠ¤Ï„ +bâŠ¤wËœ],
and P w(Î²âˆ—(w)) âˆ’ D w(Î±âˆ—(w)), where the former is Ï„âˆˆRn
easily computed while the latter is linear with respect Î¾
subject to âˆ€iâˆˆF : Ï„ = i ,
to w. So, similar to L1-loss L2-regularized SVM, we Î½ i Ï• âˆ’Î½
i
can obtain the maximization result by maximizing (cid:88) (cid:88)
Ï„2 =S2âˆ’ Ï„2,
or minimizing the linear terms by Lemma A.4 in i i
Appendix A, and maximizing the convex quadratic
iâˆˆUÎ½ iâˆˆFÎ½
where U :={i|iâˆˆ[n], Ï• =Î½}, F :=[n]\U .
function by the method of Section 4.3. Î½ i Î½ Î½
(Note that the maximization is easily computed
by Lemma A.4.)
4.3 Maximizing Linear and Convex
The proof is presented in Appendix A.6.
Quadratic Functions in Hyperball
Constraint Lemma 4.2. Under the same definitions as Lemma
4.1, The equation T(Î½) = S2 can be solved by the
To derive DRSS rules of Sections 4.1 and 4.2, we following procedure: Let e:=[e ,e ,...,e ] (N â‰¤n,
1 2 N
need to compute the following forms of optimization k =Ì¸ kâ€² â‡’e Ì¸=e ) be a sequence of indices such that
k kâ€²
8ð‘¥
1
ð‘¥
2
ð‘¦
ð‘†2 ð‘¥ ð‘‘
Final
ðœˆ Feature extraction prediction
ðœ™ ð‘’1 ðœ™ ð‘’2 ðœ™ ð‘’3 ðœ™ ð‘’4 ðœ™ ð‘’ð‘ (Assum inie tid
a
lt o
le
b ae
rn
f ii nx ge )d after f( uC no cn tiv oe nx
)
Figure 2: An example of the expression T(Î½) (black
Figure3: ConceptofhowtoapplySSfordeeplearning.
solid line) in Lemmas 4.1 and 4.2. Colored dash
SS is applied to the last layer for the final prediction.
lines denote terms in the summation (Î¾ /(Î½âˆ’Ï• ))2.
ek ek
We can see that, given an interval (Ï• ,Ï• ) (k âˆˆ
ek ek+1
[N âˆ’1]), the function is convex. 5 Application to Deep Learning
Sofar,ourdiscussionofSSruleshasprimarilyfocused
1. e âˆˆ[n] for any k âˆˆ[N],
k on ML models with linear predictions and convex loss
and regularization functions. However, there may be
2. iâˆˆ[n] is included in e if and only if Î¾ Ì¸=0, and
i scenarioswherewewouldliketoemploymorecomplex
ML models, such as deep learning (DL).
3. Ï• â‰¤Ï• â‰¤Â·Â·Â·â‰¤Ï• .
e1 e2 eN For DL models, deriving SS rules for the entire
model can be challenging due to the complexity of
Note that, if Ï• < Ï• (k âˆˆ [N âˆ’1]), then T(Î½)
ek ek+1 bounding the change in model parameters against
is a convex function in the interval (Ï• ,Ï• ) with
ek ek+1 changes in sample weights. However, we can simplify
lim = lim = +âˆž. Then, unless
Î½â†’Ï•ek+0 Î½â†’Ï•ek+1âˆ’0 the process by focusing on the fact that each layer of
N = 0, each of the following intervals contains just DL is often represented as a convex function. There-
one solution of T(Î½)=S2: fore, we propose applying SS rules specifically to the
last layer of DL models.
â€¢ Intervals (âˆ’âˆž,Ï• ) and (Ï• ,+âˆž).
e1 eN Inthisformulation,thelayersprecedingthelastone
are considered as a fixed feature extraction process,
â€¢ Let Î½#(k) := argmin T(Î½). For each
Ï•ek<Î½<Ï•ek+1 even when the sample weights change (see Figure
k âˆˆ[N âˆ’1] such that Ï• <Ï• , 3). We believe that this approach is valid when the
ek ek+1
change in sample weights is not significant. We plan
â€“ intervals (Ï• ,Î½#(k)) and (Î½#(k),Ï• ) if to experimentally evaluate the effectiveness of this
ek ek+1
T(Î½#(k))<S2, formulation in Section 6.3.
â€“ interval [Î½#(k),Î½#(k)] (i.e., point) if
T(Î½#(k))=S2. 6 Numerical Experiment
It follows that T(Î½)=S2 has at most 2n solutions. 6.1 Experimental Settings
By Lemma 4.2, in order to compute the solution of We evaluate the performances of DRSsS and DRSfS
T(Î½)=S2,wehaveonlytocomputeÎ½#(k) byNewton across different values of acceptable weight changes
method or the like, and to compute the solution for S and hyperparameters for regularization strength Î».
each interval by Newton method or the like. We show Performance is measured using safe screening rates,
an example of T(Î½) in Figure 2, and the proof in representing the ratio of screened samples or features
Appendix A.7. to all samples or features. We consider three setups:
9DRSsSwithL1-lossL2-regularizedSVM(Section4.1),
Table 2: Datasets for DRSsS/DRSfS experiments.
DRSfSwithL2-lossL1-regularizedSVM(Section4.2),
All are binary classification datasets from LIBSVM
and DRSsS with deep learning (Section 5) where
dataset [22]. The mark â€  denotes datasets with one
the last layer incorporates DRSsS with L1-loss L2-
feature removed due to computational constraints.
regularized SVM.
See Appendix D.1 for details.
In these experiments, we set initialize the sample
weights before change (wËœ) as wËœ =1 . Then, we set
n
S in DRSS for W := {w | âˆ¥wâˆ’wËœâˆ¥ 2 â‰¤ S} (Section Task Name n n+ d
4) as follows:
DRSsS australian 690 307 15
â€¢ First we assume the weight change that the breast-cancer 683 239 11
heart 270 120 14
weights for positive samples ({i|y =+1}) from
i
ionosphere 351 225 35
1 to a, while retaining the weights for negative
sonar 208 97 61
samples ({i|y =âˆ’1}) as 1.
i
splice (train) 1000 517 61
â€¢ Then, we defined S as the size of weight change svmguide1 (train) 3089 2000 5
âˆš
above; specifically, we set S = n+|a âˆ’ 1| DRSsS madelon (train) 2000 1000 â€  500
(n+: number of positive samples in the train- sonar 208 97 â€  60
ing dataset). splice (train) 1000 517 61
We vary a within the range 0.9â‰¤aâ‰¤1.1, assuming
a maximum change of up to 10% per sample weight.
6.3 Safe Sample Screening for Deep
6.2 Relationship between the Weight Learning Model
Changes and Safe Screening Rate
WeappliedDRSsStoDLmodels(Section5),assuming
First, we present safe screening rates for two SVM that all layers are fixed except for the last layer.
setups. The datasets used in these experiments are We utilized a neural network architecture compris-
detailed in Table 2. In this experiment, we adapt ing the following components: firstly, ResNet50 [23]
the regularization hyperparameter Î» based on the with an output of 2,048 features, followed by a fully
characteristicsofthedata. Thesedetailsaredescribed connected layer to reduce the features to 10, and
in Appendix D.1. finally, L1-loss L2-regularized SVM (Section 4.1) ac-
As an example, for the â€œsonarâ€ dataset, we show companied by the intercept feature (Remark 2.2).
the DRSsS result in Figure 4 and the DRSfS result in For the experiment, we employed the CIFAR-10
Figure 5. Results for other datasets are presented in dataset [24], a well-known benchmark dataset for
Appendix D.2. image classification tasks. We configured the net-
These plots allow us to assess the tolerance for work to classify images into two classes: â€œairplaneâ€
changesinsampleweights. Forinstance,witha=0.98 and â€œautomobileâ€. Given that there are 5,000 im-
(weight of each positive sample is reduced by two per- ages for each class, we split the dataset into train-
cent, or equivalent weight change in L2-norm), the ing:validation:testing=6:2:2, resulting in a total of
samplescreeningrateis0.31forL1-lossL2-regularized 6,000 images in the training dataset.
SVM with Î» = 6.58e+1, and the feature screen- The resulting safe sample screening rates are illus-
ing rate is 0.29 for L2-loss L1-regularized SVM with trated in Figure 6. We observed similar outcomes to
Î»=3.47e+1. This implies that, even if the weights those obtained with ordinary SVMs in Section 6.2.
are changed in such ranges, a number of samples or This experiment validates the feasibility of apply-
features are still identified as redundant in the sense ing DRSsS to DL models, demonstrating consistent
of prediction. results with traditional SVM setups.
107 Conclusion
no direct applications that might impact society or
ethical considerations.
In this paper, we discussed DR-SS, considering the
possible changes in sample weights to represent DR
Acknowledgements
setup. We developed a method for calculating SS
that can handle changes in sample weights by intro-
ducing nontrivial computational techniques, such as This work was partially supported by MEXT KAK-
constrained maximization of certain convex functions ENHI (20H00601), JST CREST (JPMJCR21D3 in-
(Section 4.3). Additionally, to address the constraint cluding AIP challenge program, JPMJCR22N2), JST
of SS, which typically applies to ML by minimizing MoonshotR&D(JPMJMS2033-05),JSTAIPAcceler-
convex functions, we provided an application to DL ation Research (JPMJCR21U2), NEDO (JPNP18002,
by applying SS to the last layer of DL model. While JPNP20006) and RIKEN Center for Advanced Intel-
this approach is an approximation, it holds certain ligence Project.
validity.
For the future work, we aim to explore different
References
environmental changes. In this paper, we focused on
weightconstraintbyL2-normâˆ¥wâˆ’wËœâˆ¥ â‰¤S (Section
2 [1] Ruidi Chen and Ioannis Ch. Paschalidis. Dis-
4) due to computational considerations. However,
tributionally robust learning. arXiv Preprint,
when interpreting changes in weights, the constraint
2021.
of L1-norm âˆ¥wâˆ’wËœâˆ¥ â‰¤ S may be more appropri-
1
ate, as it reflects changes in weights by altering the [2] Laurent El Ghaoui, Vivian Viallon, and Tarek
number of samples. Furthermore, in the context of Rabbani. Safe feature elimination for the lasso
DR-SS for DL, we are interested in loosening the and sparse supervised learning problems. Pacific
constraint of fixing the network except for the last Journal of Optimization, 8(4):667â€“698, 2012.
layer. Investigatingthisaspectcouldprovidevaluable
insights into the flexibility of DR-SS methodologies [3] Kohei Ogawa, Yoshiki Suzuki, and Ichiro
in DL applications. Takeuchi. Safe screening of non-support vectors
in pathwise svm computation. In Proceedings of
the 30th International Conference on Machine
Software and Data Learning, pages 1382â€“1390, 2013.
[4] Hidetoshi Shimodaira. Improving predictive in-
The code and the data to reproduce the experiments
ference under covariate shift by weighting the
are available as the attached file.
log-likelihoodfunction.Journalofstatisticalplan-
ning and inference, 90(2):227â€“244, 2000.
Potential Broader Impact
[5] Masashi Sugiyama, Matthias Krauledat, and
Klaus-Robert MuÂ¨ller. Covariate shift adaptation
This paper contributes to machine learning in dynam-
byimportanceweightedcrossvalidation. Journal
ically changing environments, a scenario increasingly
of Machine Learning Research, 8(35):985â€“1005,
prevalent in real-world data analyses. We believe
2007.
that, in such situations, ensuring prediction perfor-
manceagainstenvironmentalchangesandminimizing [6] C. Cortes and V. Vapnik. Support-vector net-
storage requirements for expanding datasets will be works. Machine Learning, 20:273â€“297, 1995.
beneficial. The method does not present significant
ethical concerns or foreseeable societal consequences [7] Robert Tibshirani. Regression shrinkage and
because this work is theoretical and, as of now, has selection via the lasso. Journal of the Royal Sta-
11tistical Society Series B: Statistical Methodology, [15] Atsushi Shibagaki, Masayuki Karasuyama, Ko-
58(1):267â€“288, 1996. hei Hatano, and Ichiro Takeuchi. Simultaneous
safe screening of features and samples in doubly
[8] Joel Goh and Melvyn Sim. Distributionally
sparse modeling. In International Conference on
robust optimization and its tractable approxi-
Machine Learning, pages 1577â€“1586, 2016.
mations. Operations Research, 58(4-1):902â€“917,
2010. [16] Kazuya Nakagawa, Shinya Suzumura, Masayuki
Karasuyama, Koji Tsuda, and Ichiro Takeuchi.
[9] Erick Delage and Yinyu Ye. Distributionally
Safe pattern pruning: An efficient approach for
robust optimization under moment uncertainty
predictive pattern mining. In Proceedings of the
with application to data-driven problems. Oper-
22nd ACM SIGKDD International Conference
ations Research, 58(3):595â€“612, 2010.
on Knowledge Discovery and Data Mining, pages
[10] Liyuan Wang, Xingxing Zhang, Kuo Yang, 1785â€“1794. ACM, 2016.
Longhui Yu, Chongxuan Li, Lanqing HONG,
[17] Shaogang Ren, Shuai Huang, Jieping Ye, and
Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun
Xiaoning Qian. Safe feature screening for gen-
Zhu. Memory replay with data compression for
eralized lasso. IEEE Transactions on Pattern
continual learning. In International Conference
Analysis and Machine Intelligence, 40(12):2992â€“
on Learning Representations, 2022.
3006, 2018.
[11] James Kirkpatrick, Razvan Pascanu, Neil Rabi-
nowitz, Joel Veness, Guillaume Desjardins, An- [18] Jiang Zhao, Yitian Xu, and Hamido Fujita. An
drei A. Rusu, Kieran Milan, John Quan, Tiago improved non-parallel universum support vec-
Ramalho, Agnieszka Grabska-Barwinska, Demis tor machine and its safe sample screening rule.
Hassabis, Claudia Clopath, Dharshan Kumaran, Knowledge-Based Systems, 170:79â€“88, 2019.
and Raia Hadsell. Overcoming catastrophic for-
[19] Zhou Zhai, Bin Gu, Xiang Li, and Heng Huang.
gettinginneuralnetworks. ProceedingsoftheNa-
Safe sample screening for robust support vector
tional Academy of Sciences, 114(13):3521â€“3526,
machine. InProceedings of the AAAI Conference
2017.
on Artificial Intelligence, volume 34, pages 6981â€“
[12] Olivier Fercoq, Alexandre Gramfort, and Joseph 6988, 2020.
Salmon. Mindthedualitygap: saferrulesforthe
[20] Hongmei Wang and Yitian Xu. A safe double
lasso. In Proceedings of the 32nd International
screening strategy for elastic net support vec-
Conference on Machine Learning, pages 333â€“342,
tor machine. Information Sciences, 582:382â€“397,
2015.
2022.
[13] Eugene Ndiaye, Olivier Fercoq, Alexandre Gram-
fort,andJosephSalmon.Gapsafescreeningrules [21] Takumi Yoshida, Hiroyuki Hanada, Kazuya Nak-
for sparse multi-task and multi-class models. In agawa, Kouichi Taji, Koji Tsuda, and Ichiro
Advances in Neural Information Processing Sys- Takeuchi. Efficient model selection for predictive
tems, pages 811â€“819, 2015. pattern mining model by safe pattern pruning.
Patterns, 4(12):100890, 2023.
[14] Shota Okumura, Yoshiki Suzuki, and Ichiro
Takeuchi. Quick sensitivity analysis for incre- [22] Chih-ChungChangandChih-JenLin.Libsvm: A
mental data modification and its application to libraryforsupportvectormachines. ACM Trans-
leave-one-out cv in linear classification problems. actions on Intelligent Systems and Technology
In Proceedings of the 21th ACM SIGKDD In- (TIST), 2(3):27, 2011. Datasets are provided in
ternational Conference on Knowledge Discovery authorsâ€™ website: https://www.csie.ntu.edu.
and Data Mining, pages 885â€“894, 2015. tw/~cjlin/libsvmtools/datasets/.
12[23] KaimingHe, XiangyuZhang, ShaoqingRen, and
Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition
(CVPR), June 2016.
[24] Alex Krizhevsky. The cifar-10 dataset, 2009.
1.0
[25] Ralph Tyrell Rockafellar. Convex analysis. =0.208
0.8 =0.657
Princeton university press, 1970. =2.08
0.6 =6.577
[26] Jean-Baptiste Hiriart-Urruty and Claude 0.4 =20.8
=65.77
LemarÂ´echal. Convex Analysis and Minimization 0.2 =208
Algorithms II: Advanced Theory and Bundle
0.0
Methods. Springer, 1993. 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
Figure 4: Ratio of screened samples by DRSsS for
dataset â€œsonarâ€.
1.0
=7.48e 1
0.8 =1.61e+0
=3.47e+0
0.6 =7.48e+0
=1.61e+1
0.4
=3.47e+1
0.2
=7.48e+1
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
Figure 5: Ratio of screened features by DRSfS for
dataset â€œsonarâ€.
1.0
=6.00e+0
0.8 =1.90e+1
=6.00e+1
0.6 =1.90e+2
=6.00e+2
0.4
=1.90e+3
0.2
=6.00e+3
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
Figure 6: Ratio of screened samples by DRSsS
for dataset with CIFAR-10 dataset and DL model
ResNet50.
13
selpmas
deneercs
fo oitaR
serutaef
deneercs
fo
oitaR
selpmas
deneercs
fo
oitaRA Proofs
A.1 General Lemmas
Lemma A.1. For a convex function f :Rd â†’Râˆª{+âˆž}, fâˆ—âˆ— is equivalent to f if f is convex, proper (i.e.,
âˆƒv âˆˆRd : f(v)<+âˆž) and lower-semicontinuous.
Proof. See Section 12 of [25] for example.
Lemma A.1 is known as Fenchel-Moreau theorem. Especially, Lemma A.1 holds if f is convex and
âˆ€v âˆˆRd : f(v)<+âˆž.
Lemma A.2. For a convex function f :Rd â†’Râˆª{+âˆž},
â€¢ fâˆ— is (1/Î½)-strongly convex if f is proper and Î½-smooth.
â€¢ fâˆ— is (1/Îº)-smooth if f is proper, lower-semicontinuous and Îº-strongly convex.
Proof. See Section X.4.2 of [26] for example.
Lemma A.3. Supposethatf :Rd â†’Râˆª{+âˆž}isaÎº-stronglyconvexfunction, andletvâˆ— =argmin f(v)
vâˆˆRd
be the minimizer of f. Then, for any v âˆˆRd, we have
(cid:114)
2
âˆ¥vâˆ’vâˆ—âˆ¥ â‰¤ [f(v)âˆ’f(vâˆ—)].
2 Îº
Proof. See [13] for example.
Lemma A.4. For any vector a,câˆˆRn and S >0,
min aâŠ¤v =aâŠ¤câˆ’Sâˆ¥aâˆ¥ , max aâŠ¤v =aâŠ¤c+Sâˆ¥aâˆ¥ .
2 2
vâˆˆRn: âˆ¥vâˆ’câˆ¥2â‰¤S vâˆˆRn: âˆ¥vâˆ’câˆ¥2â‰¤S
Proof. By Cauchy-Schwarz inequality,
âˆ’âˆ¥aâˆ¥ âˆ¥vâˆ’câˆ¥ â‰¤aâŠ¤(vâˆ’c)â‰¤âˆ¥aâˆ¥ âˆ¥vâˆ’câˆ¥ .
2 2 2 2
Noticing that the first inequality becomes equality if âˆƒÏ‰ >0: a=âˆ’Ï‰(vâˆ’c), while the second inequality
becomes equality if âˆƒÏ‰â€² >0: a=Ï‰â€²(vâˆ’c). Moreover, since âˆ¥vâˆ’câˆ¥ â‰¤S,
2
âˆ’Sâˆ¥aâˆ¥ â‰¤aâŠ¤(vâˆ’c)â‰¤Sâˆ¥aâˆ¥
2 2
also holds, with the equality holds if âˆ¥vâˆ’câˆ¥ =S.
2
On the other hand, if we take v that satisfies both of the equality conditions of Cauchy-Schwarz inequality
above, that is,
â€¢ (for the first inequality being equality) v =câˆ’(S/âˆ¥aâˆ¥ )a,
2
â€¢ (for the second inequality being equality) v =c+(S/âˆ¥aâˆ¥ )a,
2
then the inequalities become equalities. This proves that âˆ’Sâˆ¥aâˆ¥ and Sâˆ¥aâˆ¥ are surely the minimum and
2 2
maximum of aâŠ¤(vâˆ’c), respectively.
14A.2 Derivation of Dual Problem by Fenchelâ€™s Duality Theorem
As the formulation of Fenchelâ€™s duality theorem, we follow the one in Section 31 of [25].
Lemma A.5 (A special case of Fenchelâ€™s duality theorem: f,g <+âˆž). Let f :Rn â†’R and g :Rd â†’R be
convex functions, and AâˆˆRnÃ—d be a matrix. Moreover, we define
vâˆ— := min[f(Av)+g(v)], (18)
vâˆˆRd
uâˆ— := max[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)]. (19)
uâˆˆRn
Then Fenchelâ€™s duality theorem assures that
f(Avâˆ—)+g(vâˆ—)=âˆ’fâˆ—(âˆ’uâˆ—)âˆ’gâˆ—(AâŠ¤uâˆ—),
âˆ’uâˆ— âˆˆâˆ‚f(Avâˆ—),
vâˆ— âˆˆâˆ‚gâˆ—(AâŠ¤uâˆ—).
Sketch of the proof. Introducing a dummy variable Ïˆ âˆˆRn and a Lagrange multiplier uâˆˆRn, we have
min[f(Av)+g(v)]= max min [f(Ïˆ)+g(v)âˆ’uâŠ¤(Avâˆ’Ïˆ)] (20)
vâˆˆRd uâˆˆRnvâˆˆRd, ÏˆâˆˆRn
=âˆ’ min max [âˆ’f(Ïˆ)âˆ’g(v)+uâŠ¤(Avâˆ’Ïˆ)]=âˆ’ min max [{(âˆ’u)âŠ¤Ïˆâˆ’f(Ïˆ)}+{(AâŠ¤u)âŠ¤vâˆ’g(v)}]
uâˆˆRnvâˆˆRd, ÏˆâˆˆRn uâˆˆRnvâˆˆRd, ÏˆâˆˆRn
=âˆ’ min[fâˆ—(âˆ’u)+gâˆ—(AâŠ¤u)]= max[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)]. (21)
uâˆˆRn uâˆˆRn
Moreover, bytheoptimalityconditionofaproblemwithaLagrangemultiplier(20), theoptimaofit, denoted
by vâˆ—, Ïˆâˆ— and uâˆ—, must satisfy
Avâˆ— =Ïˆâˆ—, AâŠ¤uâˆ— âˆˆâˆ‚g(vâˆ—), âˆ’uâˆ— âˆˆâˆ‚f(Ïˆâˆ—)=âˆ‚f(Avâˆ—).
Ontheotherhand, introducingadummyvariableÏ•âˆˆRd andaLagrangemultiplierv âˆˆRd for(21), wehave
max[âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)]= min max [âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(Ï•)âˆ’vâŠ¤(AâŠ¤uâˆ’Ï•)] (22)
uâˆˆRn vâˆˆRduâˆˆRn,Ï•âˆˆRd
= min max [{(Av)âŠ¤(âˆ’u)âˆ’fâˆ—(âˆ’u)}+{vâŠ¤Ï•âˆ’gâˆ—(Ï•)}]
vâˆˆRduâˆˆRn,Ï•âˆˆRd
= min[fâˆ—âˆ—(Av)+gâˆ—âˆ—(v)]= min[f(Av)+g(v)]. (âˆµ Lemma A.1)
vâˆˆRd vâˆˆRd
Likely above, by the optimality condition of a problem with a Lagrange multiplier (22), the optima of it,
denoted by uâˆ—, Ï•âˆ— and vâˆ—, must satisfy
AâŠ¤uâˆ— =Ï•âˆ—, vâˆ— âˆˆâˆ‚gâˆ—(Ï•âˆ—)=âˆ‚gâˆ—(AâŠ¤uâˆ—), Avâˆ— âˆˆâˆ‚f(âˆ’uâˆ—).
Lemma A.6 (Dual problem of weighted regularized empirical risk minimization (weighted RERM)). For
the minimization problem
n
Î²âˆ—(w) :=argminP (Î²), where P (Î²):=(cid:88) w â„“ (XË‡ Î²)+Ï(Î²), ((1) restated)
w w i yi i:
Î²âˆˆRd
i=1
15we define the dual problem as the one obtained by applying Fenchelâ€™s duality theorem (Lemma A.5), which is
defined as
n
Î±âˆ—(w) :=argmaxD (Î±), where D (Î±):=âˆ’(cid:88) w â„“âˆ— (âˆ’Î³ Î± )+Ïâˆ—(((Î³âŠ—w)â–¡Ã—XË‡)âŠ¤Î±). ((2) restated)
Î±âˆˆRn
w w i yi i i
i=1
Moreover, Î²âˆ—(w) and Î±âˆ—(w) must satisfy
P (Î²âˆ—(w))=D (Î±âˆ—(w)), ((3) restated)
w w
Î²âˆ—(w) âˆˆâˆ‚Ïâˆ—(((Î³âŠ—w)â–¡Ã—XË‡)âŠ¤Î±âˆ—(w)), ((4) restated)
âˆ€iâˆˆ[n]: âˆ’Î³ Î±âˆ—(w) âˆˆâˆ‚â„“ (XË‡ Î²âˆ—(w)). ((5) restated)
i i yi i:
Proof. To apply Fenchelâ€™s duality theorem, we have only to set f, g and A in Lemma A.5 as
n
f(u):=(cid:88) w â„“ (u ), g(Î²):=Ï(Î²), A:=XË‡.
i yi i
i=1
Here, noticing that
n n
(cid:88) (cid:88)
fâˆ—(u)= sup [uâŠ¤uâ€²âˆ’ w â„“ (uâ€²)]= sup [u uâ€² âˆ’w â„“ (uâ€²)]
uâ€²âˆˆRn
i yi i
uâ€²âˆˆRn
i i i yi i
i=1 i=1
n (cid:20) (cid:21) n (cid:18) (cid:19)
= sup (cid:88) w u iuâ€² âˆ’â„“ (uâ€²) =(cid:88) w â„“âˆ— u i ,
uâ€²âˆˆRn i w i i yi i i yi w i
i=1 i=1
from (19) we have
n (cid:18) (cid:19)
âˆ’fâˆ—(âˆ’u)âˆ’gâˆ—(AâŠ¤u)=âˆ’(cid:88) w â„“âˆ— âˆ’u i âˆ’Ïâˆ—(XË‡âŠ¤u).
i yi w
i
i=1
Replacing u â†Î³ w Î± , that is, uâ†(Î³âŠ—wâŠ—Î±), we have the dual problem (2).
i i i i
The relationships between the primal and the dual problem are described as follows:
âˆ’uâˆ— âˆˆâˆ‚f(Avâˆ—) â‡’ âˆ’Î³âŠ—wâŠ—Î±âˆ—(w) âˆˆâˆ‚f(XË‡Î²âˆ—(w)) â‡’ âˆ’Î³ w Î±âˆ—(w) âˆˆw âˆ‚â„“ (XË‡ Î²âˆ—(w))
i i i i yi i:
â‡’âˆ’Î³ Î±âˆ—(w) âˆˆâˆ‚â„“ (XË‡ Î²âˆ—(w)),
i i yi i:
vâˆ— âˆˆâˆ‚gâˆ—(AâŠ¤uâˆ—) â‡’ Î²âˆ—(w) âˆˆâˆ‚gâˆ—(XË‡âŠ¤Î³âŠ—wâŠ—Î±âˆ—(w))=âˆ‚gâˆ—(((Î³âŠ—w)â–¡Ã—XË‡)âŠ¤Î±âˆ—(w)).
A.3 Proof of Lemma 3.1
Proof. [13]
(cid:114)
2
âˆ¥Î²Ë†âˆ’Î²âˆ—(w)âˆ¥ â‰¤ [P (Î²Ë†)âˆ’P (Î²âˆ—(w))] (âˆµ setting f â†P in Lemma A.3)
2 Î» w w w
(cid:114)
2
= [P (Î²Ë†)âˆ’D (Î±âˆ—(w))] (âˆµ (3))
Î» w w
(cid:114)
2
â‰¤ [P (Î²Ë†)âˆ’D (Î±Ë†)]. (âˆµ Î±âˆ—(w) is a maximizer of D )
Î» w w w
16A.4 Proof of Lemma 3.2
Proof. Due to (5), if âˆ‚â„“ (XË‡ Î²âˆ—(w))={0} is assured, then Î±âˆ—(w) =0 is assured. Since we do not know Î²âˆ—(w)
yi i: i
but know Bâˆ—(w) (Lemma 3.1), we can assure Î±âˆ—(w) = 0 if (cid:83) âˆ‚â„“ (XË‡ Î²) = {0} is assured. Noticing
i Î²âˆˆBâˆ—(w) yi i:
that âˆ‚â„“ is monotonically increasing2, we have
yi
(cid:91) âˆ‚â„“ (XË‡ Î²)={0} â‡” (cid:91) XË‡ Î² âŠ†Z[â„“ ] â‡” [ min XË‡ Î², max XË‡ Î²]âŠ†Z[â„“ ]
yi i: i: yi
Î²âˆˆBâˆ—(w)
i:
Î²âˆˆBâˆ—(w)
i: yi
Î²âˆˆBâˆ—(w) Î²âˆˆBâˆ—(w)
(cid:104) (cid:105)
â‡” XË‡ Î²Ë†âˆ’âˆ¥XË‡ âˆ¥ r(w,Î³,Îº,Î²Ë†,Î±Ë†), XË‡ Î²Ë†+âˆ¥XË‡ âˆ¥ r(w,Î³,Îº,Î²Ë†,Î±Ë†) âŠ†Z[â„“ ]. (âˆµ Lemma A.4)
i: i: 2 i: i: 2 yi
A.5 Proof of Lemma 3.3
Proof. The proof is almost the same as that for Lemma 3.1 (see Appendix A.3), but we additionally need to
show that âˆ’D is ((min w Î³2)/Âµ)-strongly convex (in this case D is called strongly concave).
w iâˆˆ[n] i i w
As discussed in Lemma A.2, âˆ’â„“âˆ— (t) is (1/Âµ)-strongly convex, that is, âˆ’â„“âˆ— (t)âˆ’(1/2Âµ)t2 is convex. Thus,
yi yi
â€¢ âˆ’â„“âˆ— (âˆ’Î³ Î± )âˆ’(1/2Âµ)(Î³ Î± )2 is convex with respect to Î± ,
yi i i i i i
â€¢ âˆ’w â„“âˆ— (âˆ’Î³ Î± )âˆ’(w Î³2/2Âµ)Î±2 is convex with respect to Î± ,
i yi i i i i i i
â€¢ âˆ’(cid:80)n w â„“âˆ— (âˆ’Î³ Î± )âˆ’(cid:80)n (w Î³2/2Âµ)Î±2 is convex with respect to Î±.
i=1 i yi i i i=1 i i i
So, âˆ’(cid:80)n w â„“âˆ— (âˆ’Î³ Î± ) is convex with respect to Î± even subtracted by (cid:80)n [min (w Î³2/2Âµ)]Î±2 =
i=1 i yi i i k=1 iâˆˆ[n] i i k
(1/2)[min (w Î³2/Âµ)]âˆ¥Î±âˆ¥2.
iâˆˆ[n] i i 2
A.6 Proof of Lemma 4.1
Lemma A.7. For the optimization problem
maxwâŠ¤Aw+2bâŠ¤w, ((16) restated)
wâˆˆW
subject to W :={w âˆˆRn |âˆ¥wâˆ’wËœâˆ¥ â‰¤S},
2
where wËœ âˆˆRn, bâˆˆRn,
AâˆˆRnÃ—n : symmetric, positive semidefinite, nonzero,
its stationary points are obtained as the solution of the following equations with respect to w and Î½ âˆˆR:
Aw+bâˆ’Î½(wâˆ’wËœ)=0, (23)
âˆ¥wâˆ’wËœâˆ¥ =S. (24)
2
Also, when both (23) and (24) are satisfied, the function to be maximized is calculated as
wâŠ¤Aw+2bâŠ¤w =Î½S2+(Î½wËœ +b)âŠ¤(wâˆ’wËœ)+wËœâŠ¤b. (25)
F
:2 RSi â†’nce 2Râˆ‚â„“ isyi mi os na otm onu il ct ai- llv yal iu ne cd reafu sin nc gti io fn ,, fot rh ae nm yo tn <ot to â€²,n Ficit my um stu ss at tib sfe yd â€œe âˆ€fi sn âˆˆed Fa (c t)c ,o âˆ€rd si â€²n âˆˆgl Fy: (tâ€²w ):e sc â‰¤all sa â€²â€.multi-valued function
17Proof. First, wâŠ¤Aw +2bâŠ¤w is convex and not constant. Then we can show that (16) is optimized in
{w âˆˆRn |âˆ¥wâˆ’wËœâˆ¥ =S}, that is, at the surface of the hyperball W (Theorem 32.1 of [25]). This proves
2
(24). Moreover, with the fact, we write the Lagrangian function with Lagrange multiplier Î½ âˆˆR as:
L(w,Î½):=wâŠ¤Aw+2bâŠ¤wâˆ’Î½(âˆ¥wâˆ’wËœâˆ¥2âˆ’S2).
2
Then, due to the property of Lagrange multiplier, the stationary points of (16) are obtained as
âˆ‚L
=2Aw+2bâˆ’2Î½(wâˆ’wËœ)=0,
âˆ‚w
âˆ‚L
=âˆ¥wâˆ’wËœâˆ¥2âˆ’S2 =0,
âˆ‚Î½ 2
where the former derives (23).
Finally we show (25). If both (23) and (24) are satisfied,
wâŠ¤Aw+2bâŠ¤w =wâŠ¤(Î½(wâˆ’wËœ)âˆ’b)+2bâŠ¤w (âˆµ (23))
=Î½wâŠ¤(wâˆ’wËœ)+bâŠ¤w
=Î½(wâˆ’wËœ)âŠ¤(wâˆ’wËœ)+Î½wËœâŠ¤(wâˆ’wËœ)+bâŠ¤(wâˆ’wËœ)+bâŠ¤wËœ
=Î½S2+Î½wËœâŠ¤(wâˆ’wËœ)+bâŠ¤(wâˆ’wËœ)+bâŠ¤wËœ (âˆµ (24))
=Î½S2+(Î½wËœ +b)âŠ¤(wâˆ’wËœ)+bâŠ¤wËœ ((25) restated)
Proof of Lemma 4.1. The condition (23) is calculated as
Aw+b=Î½(wâˆ’wËœ),
(Aâˆ’Î½I)(wâˆ’wËœ)=âˆ’AwËœ âˆ’b.
Here, let us apply eigendecomposition of A, denoted by A = QâŠ¤Î¦Q, where Q âˆˆ RnÃ—n is orthogonal
(QQâŠ¤ =QâŠ¤Q=I) and Î¦:=diag(Ï• ,Ï• ,...,Ï• ) is a diagonal matrix consisting of eigenvalues of A. Such a
1 2 n
decomposition is assured to exist since A is assumed to be symmetric and positive semidefinite. Then,
(QâŠ¤Î¦Qâˆ’Î½I)(wâˆ’wËœ)=âˆ’QâŠ¤Î¦QwËœ âˆ’b,
QâŠ¤(Î¦âˆ’Î½I)Q(wâˆ’wËœ)=âˆ’QâŠ¤Î¦QwËœ âˆ’b,
(Î¦âˆ’Î½I)Ï„ =Î¾, (where Ï„ :=Q(wâˆ’wËœ), Î¾ :=âˆ’Î¦QwËœ âˆ’QbâˆˆRn,) (26)
âˆ€iâˆˆ[n]: (Ï• âˆ’Î½)Ï„ =Î¾ . (27)
i i i
Note that we have to be also aware of the constraint
âˆš (cid:113)
S =âˆ¥Ï„âˆ¥ = Ï„âŠ¤Ï„ = (wâˆ’wËœ)âŠ¤QâŠ¤Q(wâˆ’wËœ)=âˆ¥wâˆ’wËœâˆ¥ . (28)
2 2
Here, we consider these two cases.
181. First,considerthecasewhen(Î¦âˆ’Î½I)isnonsingular,thatis,whenÎ½ isdifferentfromanyofÏ• ,Ï• ,...,Ï• .
1 2 n
Then, from (28) we have
S2 =âˆ¥Ï„âˆ¥
=(cid:88)n
Ï„2
=(cid:88)n (cid:18)
Î¾ i
(cid:19)2
(cid:0) =:T(Î½)(cid:1) . (29)
2 i Î½âˆ’Ï•
i
i=1 i=1
So, values of (16) for all stationary points with respect to w and Î½ (on condition that (Î¦âˆ’Î½I) is
nonsingular) can be obtained by computing (25) for each Î½ satisfying (29), that is,
â€¢ for such Î½ computing Ï„ by (27), and
â€¢ computing (25) as Î½S2+(Î½wËœ +b)âŠ¤(wâˆ’wËœ)+bâŠ¤wËœ =Î½S2+(Î½wËœ +b)âŠ¤QâŠ¤Ï„ +bâŠ¤wËœ.
2. Secondly, considerthecasewhen(Î¦âˆ’Î½I)isnonsingular, thatis, whenÎ½ isequaltooneofÏ• ,Ï• ,...,Ï• .
1 2 n
First, given Î½, let U :={i|iâˆˆ[n], Ï• =Î½} be the indices of {Ï• } equal to Î½ (this may include more
Î½ i i i
than one indices), and F :=[n]\U . Note that, by assumption, U is not empty. Then, all stationary
Î½ Î½ Î½
pointsof (16)withrespecttow andÎ½ (onconditionthat(Î¦âˆ’Î½I)issingular)canbefoundbycomputing
the followings for each Î½ âˆˆ{Ï• ,Ï• ,...,Ï• } (duplication excluded):
1 2 n
â€¢ If Î¾ Ì¸=0 for at least one iâˆˆU , the equation (27) cannot hold.
i Î½
â€¢ If Î¾ = 0 for all i âˆˆ N , the equation (27) may hold. So we calculate Ï„ that maximizes (16) as
i Î½
follows:
â€“ Fix Ï„ =Î¾ /(Ï• âˆ’Î½) for iâˆˆF .
i i i Î½
â€“ Set the constraint (cid:80) Ï„2 =S2âˆ’(cid:80) Ï„2 (due to (28)).
iâˆˆUÎ½ i iâˆˆFÎ½ i
â€“ Maximize (16) with respect to {Ï„ } under the constraints above. Here, by (25) we have
i iâˆˆUÎ½
only to calculate
max[Î½S2+(Î½wËœ +b)âŠ¤(wâˆ’wËœ)+bâŠ¤wËœ], (30)
Ï„âˆˆRn
Î¾
subject to âˆ€iâˆˆF : Ï„ = i ,
Î½ i Ï• âˆ’Î½
i
(cid:88) (cid:88)
Ï„2 =S2âˆ’ Ï„2,
i i
iâˆˆUÎ½ iâˆˆFÎ½
which is easily computed by Lemma A.4. The value of the maximization result is equal to that
of (16) on condition that Î½ is specified above.
So, collecting these result and taking the largest one, the maximization (on condition that (Î¦âˆ’Î½I)
is singular) is completed.
Taking the maximum of the two cases, we have the maximization result of (16).
A.7 Proof of Lemma 4.2
Proof. Weshowthestatementsinthelemmathat,ifÏ• <Ï• (k âˆˆ[Nâˆ’1]),thenT(Î½)isaconvexfunction
ek ek+1
intheinterval(Ï• ,Ï• )withlim =lim =+âˆž. Thentheconclusionimmediatelyfollows.
ek ek+1 Î½â†’Ï•ek+0 Î½â†’Ï•ek+1âˆ’0
19The latter statement clearly holds. The former statement is proved by directly computing the derivative.
d
T(Î½)=
d (cid:88)n (cid:18) Î¾
i
(cid:19)2 =âˆ’2(cid:88)n Î¾ i2
.
dÎ½ dÎ½ Î½âˆ’Ï• (Î½âˆ’Ï• )3
i i
i=1 i=1
It is an increasing function with respect to Î½, as long as Î½ does not match any of {Ï• }n such that Î¾ =Ì¸ 0.
i i=1 i
So it is convex in the interval Ï• <Î½ <Ï• .
ek ek+1
B Detailed Calculations
In this appendix we describe detailed calculations omitted in the main paper.
B.1 Calculations for L1-loss L2-regularized SVM (Section 4.1)
For this setup, we can calculate as
ï£±
1
(cid:40)
t, (âˆ’1â‰¤tâ‰¤0)
(cid:26)
1
(cid:27) ï£´ï£²{âˆ’1}, (t<1)
Ïâˆ—(Î²):= âˆ¥Î²âˆ¥2, â„“âˆ—(t):= âˆ‚Ïâˆ—(Î²):= Î² , âˆ‚â„“ (t):= [âˆ’1,0], (t=1)
2Î» 2 y +âˆž, (otherwise) Î» y ï£´ï£³{0}.
(t>1)
Then we have the dual problem in the main paper (6).
B.2 Calculations for L2-loss L1-regularized SVM (Section 4.2)
For this setup, we can calculate as
(cid:40) (cid:40)
0, (Î² =0, âˆ€j âˆˆ[dâˆ’1]: |Î² |â‰¤Î») t2+4t, (tâ‰¤0)
Ïâˆ—(Î²):= d j â„“âˆ—(t):= 4
+âˆž, (otherwise) y +âˆž, (otherwise)
ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£²âˆ’ [âˆ’âˆž âˆž,
,0],
( (Î²
Î²j
j
< =âˆ’ âˆ’Î» Î»)
)
ï£±
ï£´ï£²âˆ’âˆž, (Î²
d
<0)
âˆ€j âˆˆ[dâˆ’1]: [âˆ‚Ïâˆ—(Î²)] := 0, (|Î² |<Î») [âˆ‚Ïâˆ—(Î²)] := [âˆ’âˆž,+âˆž], (Î² =0)
j j d d
ï£´ï£´ï£´ï£´ï£´ï£´ï£³[
+0, âˆž+ ,âˆž], ( (Î²
Î²j
= >Î» Î»)
)
ï£´ï£³+âˆž,
(Î²
d
>0)
j
âˆ‚â„“ (t):=âˆ’2max{0,1âˆ’t}.
y
Then, setting Î³ =Î» for all iâˆˆ[n], the dual objective function is described as
i
D
(Î±)=(cid:40) âˆ’(cid:80)n i=1w iÎ»2Î±2 iâˆ’ 44Î»Î±i, (if (32) are satisfied)
(31)
w
+âˆž, (otherwise)
where
Î»Î± â‰¥0â‡”Î± â‰¥0, (32a)
i i
âˆ€j âˆˆ[dâˆ’1]: |((Î»1 âŠ—w)âŠ—XË‡ )âŠ¤Î±|â‰¤Î»â‡”|(wâŠ—XË‡ )âŠ¤Î±|â‰¤1, (32b)
n :j :j
((Î»1 âŠ—w)âŠ—XË‡ )âŠ¤Î±=0â‡”(wâŠ—XË‡ )âŠ¤Î±=0. (32c)
n :d :d
20Optimality conditions (4) and (5) are described as
âˆ€j âˆˆ[dâˆ’1]: |(Î»1 âŠ—wâŠ—XË‡ )âŠ¤Î±âˆ—(w)|<Î»â‡”|(wâŠ—XË‡ )âŠ¤Î±âˆ—(w)|<1â‡’Î²âˆ—(w) =0, (33)
n :j :j j
âˆ€iâˆˆ[n]: Î»Î±âˆ—(w) =2max{0,1âˆ’XË‡ Î²âˆ—(w)}. (34)
i i:
C Application of Safe Sample Screening to Kernelized Features
The kernel method in ML means computation methods when the input variable vector of a sample xâˆˆRd
cannot be specifically obtained (this includes the case when d is infinite), but for the input variable vectors
for any two samples x,xâ€² âˆˆRd its inner product xâŠ¤xâ€² can be obtained. In such a case, we cannot discuss
SfS since we cannot obtain each feature specifically, however, we can discuss SsS.
We show that the SsS rules for L1-loss L2-regularized SVM (Section 4.1) can be applied even if the features
are kernelized.
First, if features are kernelized, we cannot obtain either X or Î²âˆ—(wËœ) specifically. However, since we can
obtain Î±âˆ—(wËœ), with (7) we have
n
âˆ€xâˆˆRd : xâŠ¤Î²âˆ—(wËœ) = 1 xâŠ¤(wâ–¡Ã—XË‡)âŠ¤Î±âˆ—(wËœ) = 1 (cid:88) w Î±âˆ—(wËœ)(xâŠ¤XË‡ ). (35)
Î» Î» i i i:
i=1
This means that we can calculate the inner product of Î²âˆ—(wËœ) and any vector.
Then, in order to calculate the quantity (9) to conduct SsS, we have only to calculate
â€¢ XË‡ Î²âˆ—(wËœ) can be calculated by (35),
i:
(cid:113)
â€¢ âˆ¥XË‡ âˆ¥ = XË‡âŠ¤XË‡ is obtained as the kernel value, and
i: 2 i: i:
â€¢ P (Î²âˆ—(wËœ))âˆ’D (Î±âˆ—(wËœ)) can be calculated by (35) and kernel values since two variables whose values
w w
cannot be specifically obtained (XËœ and Î²âˆ—(wËœ)) appears only as inner products.
So, all values needed to derive SsS rules (9) can be computed even if features are kernelized.
D Details of Experiments
D.1 Detailed Experimental Setup
The criteria of selecting datasets (Table 2) and detailed setups are as follows:
â€¢ All of the datasets are downloaded from LIBSVM dataset [22]. We used scaled datasets for ones used
in DRSfS or only scaled datasets are provided (â€œionosphereâ€, â€œsonarâ€ and â€œspliceâ€). We used training
datasets only if test datasets are provided separately (â€œspliceâ€, â€œsvmguide1â€ and â€œmadelonâ€).
â€¢ For DRSsS, we selected datasets from LIBSVM dataset containing 100 to 10,000 samples, 100 or fewer
features, and the area under the curve (AUC) of the receiver operating characteristic (ROC) is 0.9 or
higher for the regularization strengths (Î») we examined so that they tend to facilitate more effective
sample screening.
21â€¢ For DRSfS, we selected datasets from LIBSVM dataset containing 50 to 1,000 features, 10,000 or fewer
samples, and containing no categorical features. Also, due to computational constraints, we excluded
features that have at least one zero (marked â€œâ€ â€ in Table 2). As a result, one feature from â€œmadelonâ€
and one from â€œsonarâ€ have been excluded.
â€¢ In the table, the column â€œdâ€ denotes the number of features including the intercept feature (Remark
2.2).
The choice of regularization hyperparameter Î», based on the characteristics of the data, is as follows:
â€¢ For DRSsS, we set Î» as n, nÃ—10âˆ’0.5, nÃ—10âˆ’1.0, ..., nÃ—10âˆ’3.0. (For DRSsS with DL, we set 1000
instead of n.) This is because the effect of Î» gets weaker for larger n.
â€¢ For DRSfS, we determine Î» based on Î» , defined as the smallest Î» for which Î²âˆ—(w) = 0 for any
max j
j âˆˆ[dâˆ’1] explained below. We then set Î» as Î» , Î» Ã—10âˆ’1/3, Î» Ã—10âˆ’2/3, ..., Î» Ã—10âˆ’2.
max max max max
Finally, we show the calculation of Î» for L2-loss L1-regularized SVM. By (14), we would like to find Î»
max
so that |(wâŠ—XË‡ )âŠ¤Î±âˆ—(w)|<1 for all j âˆˆ[dâˆ’1]. In order to judge this, we need Î±âˆ—(w), which is calculated
:j
as follows:
â€¢ Solve the primal problem (1) for L2-loss L1-regularized SVM by fixing Î²âˆ—(w) = 0 for any j âˆˆ [dâˆ’1],
j
that is,
n n
Î²âˆ—(w) =argmin(cid:88) w â„“ (xË‡ Î² )=argmin(cid:88) w (max{0,1âˆ’y Î² })2
d i yi id d i i d
Î²d
i=1
Î²d
i=1
(cid:88) (cid:88)
=argmin w (max{0,1âˆ’Î² })2+ w (max{0,1+Î² })2
i d i d
Î²d
iâˆˆ[n], yi=+1 iâˆˆ[n], yi=âˆ’1
(cid:80) (cid:80)
w âˆ’ w
=
iâˆˆ[n], yi=+1 i iâˆˆ[n], yi=âˆ’1 i
.
(cid:80)n
w
i=1 i
â€¢ With Î²âˆ—(w) computed above and Î²âˆ—(w) =0 for any j âˆˆ[dâˆ’1], calculate Î±$ =Î»Î±âˆ—(w) =[2max{0,1âˆ’
d j
XË‡ Î²âˆ—(w)}]n by (15).
i: i=1
â€¢ If |(w âŠ—XË‡ )âŠ¤Î±$)| < Î» for all j âˆˆ [dâˆ’1], then Î²âˆ—(w) = 0 for any j âˆˆ [dâˆ’1]. So, we set Î» =
:j j max
max |(wâŠ—XË‡ )âŠ¤Î±$)|.
jâˆˆ[dâˆ’1] :j
D.2 All Experimental Results of Section 6.2
For the experiment of Section 6.2, ratios of screened samples by DRSsS setup is presented in Figure 7, while
ratios of screened features by DRSfS setup in Figure 8.
22Dataset: australian Dataset: breast-cancer
1.0 1.0
=0.690 =0.683
0.8 =2.181 0.8 =2.159
=6.9 =6.83
0.6 =21.81 0.6 =21.59
=69.0 =68.3
0.4 0.4
=218 =215
0.2 =690 0.2 =683
0.0 0.0
0.90 0.95 1.00 1.05 1.10 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight) a (Parameter for change of weight)
Dataset: heart Dataset: ionosphere
1.0 1.0
=0.27 =0.351
0.8 =0.853 0.8 =1.109
=2.7 =3.510 0.6 =8.538 0.6 =11.09
=27.0 =35.1
0.4 =85.38 0.4 =110
0.2 =270 0.2 =351
0.0 0.0
0.90 0.95 1.00 1.05 1.10 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight) a (Parameter for change of weight)
Dataset: sonar Dataset: splice
1.0 1.0
=0.208 =1.0
0.8 =0.657 0.8 =3.162
=2.08 =10.0 0.6 =6.577 0.6 =31.62
=20.8 =100
0.4 =65.77 0.4 =316
0.2 =208 0.2 =1000
0.0 0.0
0.90 0.95 1.00 1.05 1.10 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight) a (Parameter for change of weight)
Dataset: svmguide1
1.0
=3.089
0.8 =9.768
=30.89
0.6 =97.68
=308
0.4
=976
0.2 =3089
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
Figure 7: Ratios of screened samples by DRSsS.
23
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaR
selpmas
deneercs
fo oitaRDataset: madelon Dataset: sonar
1.0 1.0
=7.32e+2 =7.48e 1
0.8 =1.58e+3 0.8 =1.61e+0
=3.40e+3 =3.47e+0
0.6 =7.32e+3 0.6 =7.48e+0
=1.58e+4 =1.61e+1
0.4 =3.40e+4 0.4 =3.47e+1
0.2 =7.32e+4 0.2 =7.48e+1
0.0 0.0
0.90 0.95 1.00 1.05 1.10 0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight) a (Parameter for change of weight)
Dataset: splice
1.0
=7.34e+0
0.8 =1.58e+1
=3.41e+1
0.6 =7.34e+1
=1.58e+2
0.4
=3.41e+2
0.2
=7.34e+2
0.0
0.90 0.95 1.00 1.05 1.10
a (Parameter for change of weight)
Figure 8: Ratios of screened features by DRSfS.
24
serutaef
deneercs
fo oitaR
serutaef
deneercs
fo
oitaR
serutaef
deneercs
fo oitaR