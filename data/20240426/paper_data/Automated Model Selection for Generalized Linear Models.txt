Automated Model Selection for Generalized Linear
Models
Benjamin Schwendingera,∗, Florian Schwendingerb, Laura Vana-Gu¨rc
aInstitute of Computer Technology, TU Wien, Gußhausstraße
27-29, Vienna, 1040, Austria
bDepartment of Statistics, University of Klagenfurt, Universita¨tsstraße
65-57, Klagenfurt, 9020, Austria
cInstitute of Statistics and Mathematical Methods in Economics, TU Wien, Wiedner
Hauptstraße 7, Vienna, 1040, Austria
Abstract
In this paper, we show how mixed-integer conic optimization can be used to
combine feature subset selection with holistic generalized linear models to
fully automate the model selection process. Concretely, we directly optimize
for the Akaike and Bayesian information criteria while imposing constraints
designed to deal with multicollinearity in the feature selection task. Specif-
ically, we propose a novel pairwise correlation constraint that combines the
sign coherence constraint with ideas from classical statistical models like
Ridge regression and the OSCAR model.
Keywords: Feature Subset Selection, Holistic Generalized Linear Models,
Mixed-Integer Conic Optimization, Best Subset Selection, Computational
Statistics, Generalized Linear Models
1. Introduction
Model selection is an important but typically time-consuming task. Sev-
eral authors suggest using mixed-integer optimization to automate model
selection. The classical best subset selection (BSS) problem [1] identifies the
best k features out of p possible features according to some goodness-of-fit
∗
Corresponding author.
Email address: benjaminschwe@gmail.com(Benjamin Schwendinger)
Submitted preprint
4202
rpA
52
]LM.tats[
1v06561.4042:viXra(GOF) measure. For linear regression, the BSS problem, in the least-squares
sense is non-convex and known to be NP-hard [2]. In practice, the BSS
problem is commonly approached using methods such as stepwise regression
or exhaustive enumeration [1]. Bertsimas et al. [3] highlight the significant
advancements in mixed-integer optimization solvers since the 1970s when
BSS problems were first investigated. As a result, the BSS problem, which
was once perceived as intractable, can now be solved by mixed-integer linear
optimization solvers and used to find the solution of real-world linear regres-
sion problems. In the case of logistic regression, the BSS problem becomes a
mixed-integer non-linear optimization problem. To address this, Bertsimas
and King [4] explore the utilization of several general-purpose non-linear
mixed-integer solvers. Another approach to tackle the BSS problem in lin-
ear and logistic regression is presented by Hazimeh and Mazumder [5], who
employ a hybrid approach combining cyclic coordinate descent with a local
combinatorial search.
An extension to the BSS problem is the integration of an information cri-
terion (IC) such as the Akaike information criterion (AIC) or the Bayesian
information criterion (BIC) into the optimization problem’s objective. This
enables the selection of not only the best features for a fixed k but the over-
all optimal features according to the chosen goodness-of-fit measure. This
approach is commonly referred to in the optimization literature as feature
subset selection (FSS). Based on the division of subset selection algorithms
into classes of filter, wrapper and embedded methods as done by Guyon and
Elisseeff [6], FSS classifies as an embedded method. FSS has been applied for
variousmodel classes including generalized linear models (GLMs). Miyashiro
and Takano [7] apply FSS to linear regression using Mallows’ C statistic as
p
a GOF measure. By using intelligent branch-and-bound strategies, Hofmann
etal.[8]demonstratetheabilitytosolvetheFSSproblemforlinearregression
with1,000sofobservations and100soffeatureswithin seconds. Satoet al. [9]
propose a linear approximation approach to solve the FSS problem in logistic
regression models. They utilize a tangent line approximation of the logistic
loss function, resulting in a mixed-integer linear problem. Likewise, Saishu
et al. [10] suggest using a piecewise-linear approximation, solvable through
mixed-integer linear programming, to address the FSS problem in Poisson
models. Many heuristic approaches for solving the FSS problem for GLMs
are based on known metaheuristics. These include simulated annealing [11],
genetic algorithms (Yang and Honavar 12, Calcagno and de Mazancourt 13),
ant colony optimization [14] and particle swarm optimization [15].
2In this paper, we aim to provide a unified framework for performing au-
tomated model selection in GLMs by solving the FSS problem using mixed-
integer conic programming and to extend existing approaches to FSS to ef-
fectively handle two common challenges in GLM optimization: separation
and multicollinearity. Furthermore, our paper is the first to propose utiliz-
ing conic optimization specifically for the FSS problem in Poisson regression
models.
The proposed framework builds on the class of holistic generalized linear
models (HGLMs) introduced by Schwendinger et al. [16] in package holiglm
R
for . HGLMs extend linear, binomial and Poison GLMs by adding con-
straints designed to improve the model quality (e.g., restricting the number
of variables entering the model, enforcing coherent signs of coefficients, etc.).
Insteadofapproximatingthelog-likelihoodoritscomponents, holiglmformu-
lates the underlying optimization problems as conic optimization problems,
providing a more reliable [17] and accurate solution approach. The parame-
ters of these constrained models are obtained by using (mixed-integer) conic
optimization.
While theframeworkin[16]allows forthespecification ofanupper bound
on the number of variables to be selected in the model, this bound must
still be predetermined prior to solving the underlying optimization problem.
In particular, we modify the framework of HGLMs for the purpose of FSS
and directly integrate the AIC and BIC into the objective function rather
than using the likelihood function as an objective. This allows us to obtain
an exact solution to the FSS problem without resorting to piecewise-linear
approximation methods while treating FSS for linear, binomial and Poison
GLMs in a unified way.
As mentioned above, a primary focus of the paper is addressing the issue
of separation and multicollinearity in the automated model selection process.
For tackling strong multicollinearity, we propose a novel pairwise correlation
constraint that combines ideas from the sign coherence constraint [18] with
a simultaneous restriction of equal-magnitude coefficients. The idea of re-
stricting coefficients to have the same magnitude can also be found in other
statistical models, such as Ridge regression [19], where the coefficients shrink
equally or the OSCAR (Octagonal Shrinkage and Clustering Algorithm for
Regression) model [20], where exact equality of clustered coefficients is re-
quired. Separation on the other hand is characterized by extreme overlap
or distinct separation in the data and can result in unbounded optimization
problems that common solvers often fail to detect. To ensure reliable results,
3it is therefore vital to verify the existence of solutions. For the binomial
family with logit, probit, log and complementary log-log link, this can be
done with a linear program [21] which we employ to avoid wrongly reporting
solutions for problems whose solution is actually not determinable.
Through extensive simulation studies, we demonstrate the feasibility and
practicality of our approach. The results clearly indicate that our proposed
constraint outperforms existing methods, offering a more accurate and effi-
cient model selection process for GLMs, effectively addressing the challenges
posed by multicollinearity.
The remainder of this paper is structured as follows: Section 2 introduces
best subset selection, feature subset selection and holistic generalized linear
models. Section 3 explores common pitfalls that arise when estimating cer-
tain GLMs, such as failure to converge or the nonexistence of a solution and
possible solutions to mitigate these problems. In Section 4, we present our
proposedoptimizationproblemfor automatedmodel selection inGLMs. The
importance of dealing with multicollinearity when aiming for automated fea-
tureselection isillustrated throughasimulation study inSection5. Section 6
concludes the paper.
2. Feature subset selection in GLMs
In this section we set the stage for introducing the proposed modeling ap-
proach for performing FSS in GLMs. More specifically, we start with a brief
introduction to GLMs in Section 2.1 and show how to formulate the likeli-
hood optimization problem such that it can be solved using (mixed-integer)
conic programming. FSS is an extension to BSS, in the sense that the objec-
tive (i.e., log-likelihood) in the BSS problem is replaced by an information
criterion. Before introducing the FSS problem, we introduce the formulation
of the BSS problem as a conic program in Section 2.2. Section 2.3 introduces
the information criteria we use to extend BSS to FSS.
2.1. Generalized linear models and conic optimization
In this work we focus on solving the FSS problem for GLMs with linear,
binomial and Poisson families. Generally, GLMs as introduced by Nelder
and Wedderburn [22], are a classs of models with probability density func-
tions that belong to the exponential dispersion model (EDM) family with
4probability density function:
(yθ b(θ))
f(y;θ,φ) = exp − +c(y,φ) . (1)
φ
(cid:18) (cid:19)
Here, b( ) and c( ) are well-defined functions that vary depending on the
· ·
specific distribution. In addition, in the presence of a (design) matrix of
covariates X with p+1 columns (including a column of ones), a GLM has a
linear predictor η = Xβ, and a link function g that establishes the relation-
ship between the linear combination of the p + 1 covariates and the mean
of response y : g(E(y )) = η . Given g and b, θ is then a function of η and
i i i
therefore of β. Given a sample of n independent and identically distributed
response observations y⊤ = (y ,...,y ) and observed covariates, the esti-
1 n
mation of the parameters is usually done by maximum likelihood, and the
maximum likelihood estimate (MLE) of (β,φ) are the values (β∗,φ∗) that
maximize the (log)-likelihood function for the EDM family:
n n
y θ (β) b(θ (β))
i i i
log (β;y) = logf(y;θ,φ) = − +c(y ,φ ). (2)
i i
L φ
i
i=1 i=1
X X
Conic optimization provides a framework for expressing the maximiza-
tion of the log-likelihood function of various GLMs as convex optimiza-
tion problems. A conic optimization problem is designed to model con-
vex problems by optimizing a linear objective function over the intersec-
tion of an affine hyperplane and a nonempty closed convex cone. The log-
likelihood maximization can be reformulated as a conic problem. The rea-
son for this lies in the fact that the log-likelihood of common GLMs in-
cludes functions that can be represented by convex cones, which in turn
can be solved by modern conic optimization solvers. The estimation by
means of the conic programming is in turn feasible given that efficient op-
timization solvers exist which can provide exact solutions. More specifi-
cally, the MLE for the linear regression model (Gaussian family with iden-
tity link) is the solution of a convex optimization problem which uses the
second-order cone n := (t,x) Rn x Rn−1,t R, x t .
Ksoc { ∈ | ∈ ∈ || ||2 ≤ }
Since both logistic regression and Poisson regression involve exponential
and logarithmic terms in their log-likelihoods, the primal exponential cone
expp := (x,y,z) R3 y > 0,yex y z (x,0,z) R3 x 0,z 0 is
K { ∈ | ≤ }∪{ ∈ | ≤ ≥ }
utilized to represent them.
5Acomprehensive introduction toconicoptimizationcanbefoundinBoyd
and Vandenberghe [23]. For the detailed explanation and derivation of conic
formulations for various GLMs based on the family and link information, we
refer to the appendix provided by Schwendinger et al. [16].
2.2. Best subset selection
Weintroducetheclassical best subset selection (BSS)problembeforepre-
senting its extension to FSS. The classical best subset selection (BSS) prob-
lemisconcernedwithdetermining thebestk featuresfromasetofppotential
featuresusingsomegoodness-of-fit(GOF)metric. However, theBSSproblem
is non-convex and NP-hard [2]. Surrogate models are often used to overcome
this computational burden, such as those that incorporate an L penalty
1
or a combination of L and L penalties [24]. Adding an L penalty gives
1 2 1
the least absolute shrinkage and selection operator (LASSO) [25]. However,
the performance of BSS and LASSO depends on the signal-to-noise ratio.
A comprehensive overview by Hastie et al. [26] highlights situations for the
linear regression case where BSS outperforms LASSO and vice versa. Yang
et al. [27] approximate the BSS problem by solving a sequence of weighted
LASSO problems, with the weights determined progressively.
For generalized linear models where the log-likelihood is used as the
goodness-of-fit measure, the BSS problem can be formulated as an optimiza-
tion problem. The goal is to minimize the negative log-likelihood (equivalent
to maximizing the likelihood) with respect to the parameter vector β while
constraining the number of selected features to be k. This results in the
following optimization problem:
p
minimize log (β;y) subject to I k. (3)
β − L
{βi6=0}
≤
i=1
X
Here, k 1,...,p is a user-defined parameter that restricts the size of the
∈
subset. We can formulate the whole problem as a mixed-integer convex
optimization problem containing the constraint with the ℓ pseudo-norm,
0
which counts the number of non-zero entries in β. To do so, we introduce p
binary variables z that indicate whether the covariate β is selected for the
i i
model or not. Note that β denotes the intercept, which is always included
0
6in the model. Now, the BSS problem becomes the following problem:
minimize log (β;y)
β − L
subject to Mz β Mz , i = 1,...,p,
i i i
− p ≤ ≤ (4)
z k,
i
≤
i=1
βP Rp+1,z 0,1 p.
∈ ∈ { }
ˆ
Here, M β is a constant that ensures that a coefficient β is zero if the
∞ i
≥ || ||
corresponding binary variable z is zero. In other words, z indicates whether
i i
β is included in the model. These types of constraints are often referred to
i
as big-M constraints. It is well documented that problems containing big-M
constraints depend ona goodchoice of M. If M is too small, the convergence
to the same optimum as the original problem is not guaranteed. If M is too
large, loss of accuracy and numerical instability may occur. One should also
beawarethatchoosinganarbitrarilylargeM resultsinanunnecessarily large
feasibleregionfortheLPrelaxation[28]. Toaddressthischallenge, Bertsimas
etal.[3]proposedadata-drivenmethodtodeterminelowerandupperbounds
ˆ
forβ inlinearregression. Inthefollowing, weextendthisapproachtoconvex
i
GLMs. Let UB be an upper bound on the MLE of Problem 3. Then one can
find lower and upper bounds by solving the following convex optimization
problems:
u+ := maximize β
i i
β (5)
s. t. log (β;y) UB,
− L ≤
u− := minimize β
i i
β (6)
s. t. log (β;y) UB
− L ≤
where u+ is an upper bound and u− is a lower bound to βˆ . Now, let M :=
i i i i
max u+ , u− and one can choose the big-M as M = max M .
{| i | | i |} i i
This procedure involves solving several convex optimizationproblems and
estimating an upper bound (UB) beforehand. An alternative simpler ap-
proach, which we have also employed in our simulation studies, is to stan-
dardize the design matrix X and choose an M that works well for most
settings. We have found that a value of M = 100 works well for many data
sets when using a standardized design matrix.
72.3. Information criteria for feature subset selection
While BSS obtains the best subset of features for a fixed number of max-
imal active coefficients k (where a coefficient is said to be active if it is non-
zero), information criteria such as the Akaike Information Criterion [29] or
the Bayesian Information Criterion [30] are often used (typically in a second
stage, see Hofmann et al. [8]) to select the best model out of the candidate
models with different number of active coefficients. For linear regression, it
is computationally advantageous not to optimize the AIC directly but to use
Mallows’ C statistic (CP) [31] instead. Boisbunon et al. [32] show that for
p
linear regression, the AIC and CP are equivalent in the sense that both reach
their minimum objective value with the same set of active coefficients.
Instead of having a two stage procedure, in FSS we will replace the objec-
tive function in Equation 4 by an IC. Although many other ICs exist, we will
focus on the AIC and BIC in this paper, as they are the most commonly used
inpractice. Forlinearregression, however, weusethecomputationallyadvan-
tageous CP to replace the AIC and a modified CP (CP2) to replace the BIC.
The proof of the equivalence of BIC and CP2 can be found in Appendix B.
The Akaike Information Criterion (AIC) is defined as follows:
AIC = 2k 2log( ) (7)
− L
where is the likelihood function and k represents the number of active
L
covariates. Similarly, the Bayesian Information Criterion (BIC) is defined
as:
BIC = klog(n) 2log( ). (8)
− L
The Mallows’ C statistic (CP) for linear regression is defined by:
p
1
CP = 2k + y Xβ 2 n. (9)
σ2|| − ||2−
Similarly, the modified CP, which is equivalent to the BIC, is defined as
follows:
1
CP2 = klog(n)+ y Xβ 2 n. (10)
σ2|| − ||2 −
Here σ2 is the residual variance. The following observation, which is some-
times referred to as monotonicity of the GOF measure, applies to AIC, BIC,
CPandCP2. When weencounter two modelswiththesame likelihoodvalue,
then the model with fewer selected coefficients is better. We use the unbiased
estimator of the variance σˆ2 =
||y−Xβ||2
2 from the full regression model.
n−p
83. Issues in GLM optimization
In this section, we investigate common data settings of GLMs that cause
optimizers to recover wrong solutions. Two main reasons are causing opti-
mizers to recover an incorrect solution for GLMs. Firstly, for GLMs with a
binomial response, separation in the data can cause the maximum likelihood
estimate to contain infinite components. Secondly, cases where the data ex-
hibits strong multicollinearity can lead to instability in the estimation and
in inconsistent signs in the regression coeffients.
Before introducing our approach, we present in the following some pro-
posed approaches in the literature to address these issues.
3.1. Separation
AlbertandAnderson[33]showthatforlogisticregressionandprobitmod-
els, the finiteness and uniqueness of the MLE are connected to the overlap
of the data. They identify three different data settings, complete separation,
quasi-complete separation and overlap and show that for logistic regression
overlap is necessary and sufficient for the finiteness of the MLE. Konis and
Fokianos [34] translate these criteria into a linear problem, which can be
checked to verify the existence of the MLE.
Although, in theory, the solvers should be able to detect the unbound-
edness of the problem, we found in our experiments that all the solvers we
tried failed to detect the unboundedness for GLMs with a binomial response.
Therefore, it is important to check in the simulation that, indeed, a solution
exists; otherwise, we would mainly compare the default tolerance settings of
the solvers.
Konis and Fokianos [34] suggest using the following linear program (LP)
to verify that the solution exists:
maximize x⊤β x⊤β
β i∈I i − i∈J i
subject to xP⊤β 0 Pi J = i y = 0 (11)
i ≤ ∀ ∈ { | i }
x⊤β 0 i I = i y = 1 .
i ≥ ∀ ∈ { | i }
If the solution is a zero vector, this verifies that the data is overlapping
and that the solution of the corresponding logistic regression model is fi-
nite and unique. In case the solution of the LP is unbounded, the data is
(quasi-)separated and the MLE does not exist.
9In the preparation of this paper, we found more than two examples of
peer-reviewed articles where the authors did not check their data for sepa-
ration, which led them to report results based on unbounded optimization
problems. This likely occurred as a consequence of the solver incorrectly
signaling convergence.
3.2. Multicollinearity
In thepresence of strong collinearity, the matrixX⊤X in linear regression
ˆ
becomes ill-conditioned, leading to unstable estimates of the coefficients (β).
This instability can result in inflated estimates and inconsistent signs of the
coefficients. Different approaches to solving this problem have been proposed
in the optimization and statistics literature. One approach, as suggested by
Bertsimas and King [35], is to limit collinearity by incorporating the at most
one constraint:
z +z 1 (i,j) = (i,j) : ν ρ , (12)
i j ij
≤ ∀ ∈ HC { ≤ | |}
where the binary z variables are the ones introduced in Equation 4, ν is a
predefinedconstantandρ denotesPearson’sempiricalcorrelationcoefficient
ij
between the i-th and j-th columns of the design matrix X. Hence,
HC
represents the set of highly correlated features. This constraint limits the
pairwise correlation by ensuring that, at most one of the variables among a
pair with a correlation exceeding ν is selected in the regression model.
Carrizosaetal.[18]relaxthisconstrainttothesign coherenceconstraint (see
Equation 13)
M(1 u ) β , sign(ρ )β Mu (i,j) = (i,j) : τ ρ
ij i ij j ij ij
− − ≤ ≤ ∀ ∈ G { ≤ | |}
(13)
to force coefficients of covariates with large pairwise multicollinearity to have
coherent signs. Hence, highly positively correlated features must have coef-
ficients with the same sign, while highly negatively correlated features must
have coefficients with opposite signs. Again, M is a sufficiently large enough
constant andu isa binaryvariableintroduced to enforcethesign coherence.
ij
One can see that for u = 1 and positive ρ it holds that 0 β ,β M.
ij ij i j
≤ ≤
Similarly, for u = 0 and negative ρ we have that M β 0 and
ij ij i
− ≤ ≤
0 β M. Clearly, the sign coherence constraint is less restrictive than
j
≤ ≤
the at most one constraint.
On the other hand, a different but related approach in statistics is to
assume that strongly correlated features should have similar estimates. This
10assumption is utilized in models like Ridge regression [19] and the OSCAR
model [20]. In Ridge regression, a L penalty term is added to the objec-
2
tive function, encouraging similar estimates for strongly correlated features.
The OSCAR model goes further by enforcing exactly the same coefficient
for strongly correlated features, inducing a clustering behavior among the
coefficients.
4. Suggested model
4.1. FSS for the Poisson model
WepresentinthissectiontheformulationoftheFSSproblemforthePois-
son model using the AIC, which, to the best of our knowledge, has not been
proposed before in the literature. The corresponding AIC and BIC formula-
tions for linear, logistic, andPoisson regression can befound in Appendix C.
Weformulatethefollowingmixed-integerconicprogramforfeaturesubset
selection:
p n
minimize 2 z 2 y x⊤β δ
β,δ j=1 j !− (cid:18)i=1 i i − i (cid:19)
subject to (x⊤Pβ,1,δ ) P, i = 1,...,n, (14)
i i ∈ Kexpp
Mz β Mz , i = 1,...,p,
i i i
β− Rp≤ +1,z ≤ 0,1 p,δ Rn.
∈ ∈ { } ∈
This problem can be solved by off-the-shelf mixed-integer conic optimization
solvers like ECOS [36] or MOSEK [37]. It is worth noting that Saishu et al.
[10] have highlighted the concave but non-linear nature of the log-likelihood
function for Poisson regression and proposed a piecewise-linear approxima-
tion method. On the other hand, we are the first to suggest utilizing conic
optimization to solve the feature subset selection problem specifically for
Poisson regression. Similar problem formulations can be established for all
family-link combinations introduced in [16].
4.2. Combined constraint for multicollinearity
To further enhance this FSS model to handle multicollinearity, we pro-
pose the so-called combined constraint where we integrate the sign coherence
constraint and a equal magnitude constraint as a unified criterion. Based on
the idea of similar estimates for highly correlated features, we believe that an
equal magnitude constraint would be beneficial in much the same way that
the sign coherence constraint extends the at most one constraint. Assuming
11that the design matrix X has been standardized, we can define the equal
magnitude constraint as follows:
β = sign(ρ )β (i,j) = (i,j) : ν ρ (15)
i ij j ij
∀ ∈ HC { ≤ | |}
here ρ again refers to the Pearson’s correlation coefficient, and ν is a pre-
ij
defined constant threshold. This constraint ensures that the coefficients of
strongly correlated features have the same magnitude but possibly different
signs based on the correlation direction.
Thiscombinationallowsustoautomaticallycontrolpairwisemulticollinear-
ity in addition to the feature subset selection process. The sign coherence
constraint is applied to variables exhibiting a moderate to strong pairwise
correlation, while the equal magnitude constraint is exclusively imposed on
strongly correlated pairs. By employing the equal magnitude constraint in-
stead of the previous at most one constraint, users gain more insightful in-
formation about the underlying data structure, instead of making a near-
random selection.
4.3. Final model
By incorporating FSS with our novel combined constraint, we formulate
the following optimization problem:
minimize IC
β
subject to Mz β Mz , i = 1,...,p,
i i i
− ≤ ≤
M(1 u ) β , sign(ρ )β Mu (i,j) (16)
ij i ij j ij
− − ≤ ≤ ∀ ∈ G
β = sign(ρ )β (i,j)
i ij j
∀ ∈ HC
β Rp+1,z 0,1 p,u 0,1 |G|
∈ ∈ { } ∈ { }
where = (i,j) : τ ρ < ν and = (i,j) : ν ρ , with
ij ij
G { ≤ | | } HC { ≤ | |}
0 τ < ν 1. Again, M represents a sufficiently large positive constant.
≤ ≤
This model simultaneously enforces coherent coefficient signs for moderately
correlated features and equal coefficient magnitudes for highly correlated
features. The thresholds for medium and high correlations are defined by
τ and ν, respectively. Moreover, before estimating any binomial model, we
employ the linear program in [21] to identify the problems whose solution is
actually not determinable.
It is worth noting that the objective function of our final model, as shown
in Equation (16), incorporates an arbitrary information criterion. Conse-
quently, our model can accommodate multiple information criteria, such as
12AIC, BIC, CP, and CP2, which can be expressed using the respective for-
mulas in Equation (7, 8, 9, 10). This not only allows for greater flexibility,
but also opens to the door to more customized models. The model can be
further extended by using further holistic constraints and expert knowledge
can be seamlessly integrated into the model. Our entire approach should be
viewed as an additional tool in the modern data scientist’s tool belt.
5. Simulation studies
In our simulation study, we present two key findings. Firstly, our ap-
proach demonstrates the ability to recover the true predictors. Specifically,
weachieveselectionaccuracycomparabletothatofexactmethods, highlight-
ing the quality of our solutions. Secondly, our newly integrated constraint
proves successful in estimating variables within a multicollinearity context.
This finding emphasizes the effectiveness of our approach in overcoming chal-
lenges posed by multicollinearity and recovering accurate estimates.
All computational experiments were conducted on a Dell XPS15 lap-
top with an Intel Core i7–8750H CPU @ 2.20GHzx12 processor and 32
GB of RAM. We utilized three mixed-integer optimization solvers: Gurobi
9.1.2 [38], MOSEK 10.0.34 [37], and ECOS 2.0.5 [36]. The R package ROI
was employed for representing the optimization problems.
To obtain the exact reference solutions, we utilized the R packages lm-
Subsets [39] and bestglm [40]. The bestglm package can also solve linear
regression problems, but we exclude it from the comparison as lmSubsets
R
exhibits significantly faster performance. There exist many more packages
for subset selection, such as glmulti [13], L0Learn [5] or abess [41], but only
the selected two ensure that the solutions are indeed globally optimal. In
order to ensure a fair comparison, all solvers were restricted to utilizing only
a single core.
5.1. Simulation without multicollinearity
In this simulation, we compare the performance of the FSS formulations
suggested in Equation (14, C.1, C.2, C.3, C.4, C.5) with special purpose
solvers for FSS and BSS.
We use true positives (TP) and true negatives (TN) to calculate the
selection accuracy. The true positives are the number of features j for which
13both the estimated coefficient (βˆ ) and the true coefficient (βtrue) are non-
j j
zero:
TP(β) = j : βˆ = 0,βtrue = 0 . (17)
| j 6 j 6 |
Analogously, the true negatives represent the number of features j for which
both the estimated coefficient (βˆ ) and the true coefficient (βtrue) are zero:
j j
TN(β) = j : βˆ = 0,βtrue = 0 . (18)
| j j |
Once TP and TN are calculated, the selection accuracy (A) is determined
as the sum of TP and TN divided by the total number of potential features
(p):
TP +TN
A(β) = . (19)
p
The runtime provides an indication of the computational efficiency of the
solvers. At the same time, the selection accuracy measures how well the
solvers are able to correctly identify the relevant and irrelevant features.
For linear regression, we employ the specialized solver lmSelect from the
lmSubsets package, which utilizes a branch-and-bound strategy tailored for
this problem. For logistic and Poisson regression, we use the dedicated solver
bestglm. Unlike lmSubsets, bestglm employs complete enumeration and is
only suitable for regression problems with a moderate number of features.
To avoid excessively long runtimes, bestglm restricts the maximum number
of features (for non-Gaussian families) to 15.
Following the setting of Hofmann et al. [8] the simulation study adopts
the following design: the design matrix X is generated from a multivariate
normal distribution, with x (0,Σ) for i = 1,...,n with a mean of zero
i
∼ N
and the covariance matrix Σ is the identity matrix I . Each scenario consists
p
of 5 different runs, with n = 1000 observations. The number of features p
varies, and for each scenario, the first p coefficients of β are set to 1, while
⌈2⌉
the remaining coefficients are set to 0.
In the linear regression setting, we generate 175 datasets. The number
of features p varies among 20,25,30,35,40,45,50 , and different standard
{ }
deviations σ 0.05,0.10,0.50,1.00,5.00 are employed. The response vari-
∈ { }
able y is generated according to y = x⊤β + ǫ , where ǫ follows a normal
i i i i i
distribution with mean 0 and variance σ2. To avoid long runtimes in the
brute force setting, we limit the allowed time for each run to 4200 seconds.
In the logistic and Poisson regression settings, we vary the number of
features within p 5,10,15,20,25,30,35,40 . The inverse link function is
∈ { }
14used to calculate µ = g−1(η ), where η = x⊤β. Depending on the model,
i i i i
the response y is sampled from either y Binomial(1,p = µ ) or y
i i i i
∼ ∼
Poisson(λ = µ ), based on the underlying distribution assumption.
i
Tables C.1–C.6 summarize the results of the computation time and selec-
tion accuracy for the different GLMs.
For linear regression, Table C.2 shows that all tested methods achieve a
high selection accuracy. However, in some cases, lmSelect finds models with
a slightly lower AIC or BIC, including more features. Upon inspecting the
data, it is discovered that the true coefficients of the additionally selected
features are all 0. This discrepancy can be attributed to the fact that the
residual variance σ2 is only estimated, and as a result, the minima of AIC
and Mallows’ C may not coincide perfectly. As for the computation time,
p
TableC.1indicatesthatourFSSmodelisonlyslightlyslower thanlmSelect.
Compared to the enumerating BSS approach of trying all different values for
the sparsity parameter k, the FSS approach is faster and scales better with
the number of features.
Regarding logistic regression, FSS can recover the actual coefficients with
high accuracy, as can be seen in Table C.4. In terms of scalability, our
FSS approach scales better than the complete enumeration of bestglm. See
Table C.3 for the full timings.
Our findings for Poisson regression are similar to those for logistic regres-
sion. The computation time and selection quality results are summarized in
Table C.5 and Table C.6. Here we achieve excellent selection accuracies and
FSS scales better than complete enumeration. For Poisson regression, we
found that ECOS encounters numerical problems for the simulation setting
with more than 20 features (see also Table C.6).
5.2. Simulation with multicollinearity
In this section, we compare the performance of FSS with and without
pairwise correlation constraints. We use the mean squared error (MSE) as
a performance measure of how well a method performs at recovering the
estimates. We consider three types of pairwise correlation constraints: the
at most one constraint, the sign coherence constraint and the combined con-
straint. Moreover, toestablishabaseline, wealsobenchmarkamodelselected
basedontheinformationcriterionalonewithout additionalconstraintswhich
wecallno constraint model. Followingtherecommendation ofBertsimasand
King [35] we set the threshold for the at most one constraint to ν = 0.7. If
the correlation between two features exceeds 0.7, at most, one of the features
15can be selected in the feature subset. The sign coherence constraint evalu-
ates two thresholds, τ = 0.5 and τ = 0.7. This constraint ensures that if
two features have a correlation magnitude above the threshold, they must
have a coherent sign in the feature subset. Thus, highly positively correlated
features are forced to have the same sign, while highly negatively correlated
features must have opposite signs. The combined constraint combines the
ideas of the equal magnitude constraint and the sign coherence constraint by
enforcing the equal magnitude for highly correlated feature pairs and ensur-
ing sign coherence for moderately correlated feature pairs. To distinguish
between moderately and highly correlated feature pairs, we used the thresh-
olds τ = 0.5 and ν = 0.7.
It is important to note that the simulation study design will impact the
study’s outcome, especially when comparing pairwise correlation constraints.
By using the simulation setting proposed by McDonald and Galarneau [42],
we aim to provide a common ground for comparing the constraints. This
simulation setting is widely used in statistics to simulate collinearity among
features.
In this setting, the design matrix X is constructed as follows:
x = (1 α2)z +α , where i = 1,...,n and j = 1,...,p. (20)
ij ij i(p+1)
−
p
Here, n = 100 represents the number of observations, p = 3 denotes the
number of features, and z is drawn from a normal distribution with mean
ij
0 and standard deviation σ. The design matrix X is standardized, resulting
in X⊤X being in correlation form. The parameter α takes values from the
set 0.7,0.8,0.9,0.95,0.99 . In this particular setup, ρ = α2 gives the
ij
{ }
correlationbetween anytwofeatures. Consequently, thepairwisecorrelations
are ρ = 0.49,0.64,0.81,0.90,0.98 . The β coefficients are generated by
ij
{ }
computing the eigenvectors of X⊤X and selecting the eigenvector associated
with the largest eigenvalue.
η = β +β x +β x + +β x = x⊤β (21)
i 0 1 1i 2 2i ··· p pi i
Subsequently, Equation (21) is employed to calculate η where β is set to
i 0
zero and µ is given by µ = g−1(η ). Finally, the response y is sampled
i i i i
either from y Normal(µ ,σ) or y Binomial(1,µ ) or y Poisson(µ ).
i i i i i i
∼ ∼ ∼
This simulation setup allows for the generation of data with controlled
correlations among features, enabling the evaluation and comparison of the
16different pairwise correlation constraints in the context of feature subset se-
lection. Table C.7, Table C.8 and Table C.9 summarize the results for the
different constraints and correlation settings for 1,000 simulations and 100
observations.
In the linear regression setting, the AIC and BIC exhibited almost iden-
tical results, leading us to report only the AIC outcomes. Analysis of Ta-
bleC.9suggeststhatwhenthestandarddeviationσ isverysmall(specifically,
σ = 0.01) and the α values range from small to medium (0.7, 0.8, and 0.9),
both the no constraint and the sign coherence constraint slightly outperform
the combined constraint. The at most one constraint performs best for small
correlations. This observation can be attributed to the decreased likelihood
of the constraint becoming active at lower α values, rendering its perfor-
mance similar to the one of the no constraint. Overall, the simulation results
indicate that thecombined constraint performs exceptionally well in thissim-
ulation setting, effectively harnessing the advantages of the sign coherence
constraint and the equal magnitude constraint.
Table C.7 presents the results for the logistic regression setting, where the
generated µ values implicitly determine the standard deviation. In this sce-
i
nario, the combined constraint outperformed all other constraints. While the
MSE values for all constraints were similar under low correlation conditions,
the combined constraint exhibited significantly lower MSE values compared
to the other constraints under strong correlation.
Similarly, Table C.8 showcases the results for the Poisson regression set-
ting. In this case, all constraints performed similarly under low correlation
conditions. However, as the correlation increased, the combined constraint
emerged as advantageous in this simulation setting. Furthermore, the results
indicate that the MSE values for the no constraint and the sign coherence
constraint were almost identical, suggesting that inconsistent signs were rare
in this simulation setup.
6. Conclusion
This paper proposes an automated model selection approach by combin-
ing FSS with a correlation constraint for specific types of generalized lin-
ear models (GLMs), including linear, logistic, and Poisson regression. Our
method utilizes conic optimization and information criteria such as AIC or
BIC for feature subset selection (FSS). Moreover, it also enables the integra-
tion of additional constraints such as limiting pairwise correlation. We have
17shown that our approach achieves high selection accuracy and improves com-
putational efficiency as the dimensionality of the problem increases, outper-
forming naive enumeration methods. The key contribution of our work is the
development of a new mixed-integer conic programming (MICP) problem for
model selection under multicollinearity. Our experiments have demonstrated
thatourproposedcombinedconstraintsurpassestheexistingcollinearitycon-
straints in the literature regarding performance. Future research directions
could explore incorporating additional regularization terms or leveraging lin-
ear approximations to speed up computation times.
Acknowledgements
This work was supported by the Austrian Science Fund (FWF) under
grant number ZK 35.
Appendix A. GLMs
The PDF of the Normal distribution can be expressed as:
1 (yµ µ2/2) y2 1
f(y;µ,σ2) = = exp − + log(2πσ2)
√2πσ2exp −(y−µ)2 σ2 −2σ2 − 2
2σ2 (cid:18) (cid:18) (cid:19)(cid:19)
(cid:16) (cid:17) (A.1)
where one can see that it is indeed part of the EDM family with θ = µ,
φ = σ2, b(θ) = θ2 and c(y,φ) = y2 1 log(2πφ).
2 −2φ − 2
The PDF of the binomial distribution can be written as:
n n p
f(y;p) = py(1 p)n−y = exp log +ylog( )+nlog(1 p) .
y − y 1 p −
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19) − (cid:19)
(A.2)
Together with the link function θ = logit(p) = log p we get
1−p
(cid:16) (cid:17)
1 n
f(y;p) = exp yθ+nlog +log . (A.3)
1+exp(θ) y
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
Here we can see that the binomial distribution is part of the EDM family
with θ = log p , φ = 1, b(θ) = nlog 1 and c(y,φ) = log n .
1−p − 1+exp(θ) y
The PDF(cid:16)of th(cid:17)e Poisson distribution c(cid:16)an be ex(cid:17)pressed by:
(cid:0) (cid:1)
µyexp( µ)
f(y;µ) = − = exp(ylog(µ) µ log(y!)). (A.4)
y! − −
18Now using the link function θ = log(µ) which is equal to exp(θ) = µ, we get
f(y;µ) = exp(yθ exp(θ) log(y!)). (A.5)
− −
We can see that the Poisson distribution is also part of the EDM family with
θ = log(µ), φ = 1, b(θ) = exp(θ) and c(y,φ) = log(y!).
−
Appendix B. Mallows C for BIC
p
The log-likelihood for linear regression is given by:
1 1
log (β;y) = (y x⊤β)2 log(2πσ2). (B.1)
L −2σ2 i − i − 2
Given the definition of BIC (in Equation (8)) and CP2 (in Equation (10))
we have
BIC = klog(n) 2log( )
= klog(n)− + n L 1 (y x⊤β)2 +log(2πσ2)
=
klog(n)+nloi= g1 (2σ π2 σ2i )−
+
|i |y−Xβ||2
2
(B.2)
P σ2
= CP2+n(log(2πσ2)+1)
This implies that they reach their respective minima at the same coefficient
values.
Appendix C. Mixed-integer conic optimization problems for FSS
FSS for AIC
Gaussian
p
minimize 2 z + 1 ζ
j σ2
β,ζ j=1 !
subject to (ζ +P1,ζ 1,2(y x⊤β),...,2(y x⊤β)) n+2 (C.1)
− 1 − 1 n − n ∈ Ksoc
Mz β Mz , i = 1,...,p,
i i i
− β Rp≤ +1,z ≤ 0,1 p,ζ R.
∈ ∈ { } ∈
19Binomial
p n
minimize 2 z 2 y x⊤β δ
β,δ,γ j=1 j !− (cid:18)i=1 i i − i (cid:19)
subject to (δ ,P1,1+γ ) P , i = 1,...,n
i i expp (C.2)
∈ K
(x⊤β,1,γ ) , i = 1,...,n
i i ∈ Kexpp
Mz β Mz , i = 1,...,p,
i i i
β− Rp≤ +1,z ≤ 0,1 p,δ Rn,γ Rn.
∈ ∈ { } ∈ ∈
Poisson
The optimization problem is already given in Equation (14).
FSS for BIC
Gaussian
p
minimize log(n) z + 1 ζ
j σ2
β,ζ j=1 !
subject to (ζ +1,ζP1,2(y x⊤β),...,2(y x⊤β)) n+2 (C.3)
− 1 − 1 n − n ∈ Ksoc
Mz β Mz , i = 1,...,p,
i i i
− ≤ ≤
β Rp+1,z 0,1 p,ζ R.
∈ ∈ { } ∈
Binomial
p n
minimize log(n) z 2 y x⊤β δ
β,δ,γ j=1 j !− (cid:18)i=1 i i − i (cid:19)
subject to (δ ,1,1+Pγ ) ,Pi = 1,...,n
i i expp (C.4)
∈ K
(x⊤β,1,γ ) , i = 1,...,n
i i ∈ Kexpp
Mz β Mz , i = 1,...,p,
i i i
− ≤ ≤
β Rp+1,z 0,1 p,δ Rn,γ Rn.
∈ ∈ { } ∈ ∈
Poisson
p n
minimize log(n) z 2 y x⊤β δ
β,δ j=1 j !− (cid:18)i=1 i i − i (cid:19)
subject to (x⊤β,1,δP) , Pi = 1,...,n, (C.5)
i i ∈ Kexpp
Mz β Mz , i = 1,...,p,
i i i
− β Rp≤ +1,z ≤ 0,1 p,δ Rn.
∈ ∈ { } ∈
20Result Tables
Table C.1: Simulation without multicollinearity: Comparison of proposed method, HLM
withenumerationandspecialpurposesolverforlinearregression;averageexecutiontimes
in seconds.
Linearmodeltrainingtime
AIC BIC
Proposedmethod HLMwithbruteforce Proposedmethod HLMwithbruteforce
σ p (Gurobi) lmSelect (Gurobi) (Gurobi) lmSelect (Gurobi)
0.05 20 0.049 0.009 1.140 0.047 0.002 1.140
25 0.063 0.003 2.989 0.062 0.003 2.989
30 0.085 0.003 9.551 0.069 0.003 9.551
35 0.196 0.004 58.876 0.086 0.004 58.876
40 0.179 0.004 224.310 0.094 0.004 224.310
45 0.214 0.007 1384.208 0.123 0.005 1384.208
50 0.381 0.007 – 0.149 0.006 –
0.1 20 0.059 0.003 1.252 0.055 0.003 1.252
25 0.065 0.003 3.347 0.060 0.003 3.347
30 0.084 0.004 10.526 0.074 0.003 10.526
35 0.169 0.005 51.952 0.111 0.005 51.952
40 0.215 0.005 198.132 0.106 0.005 198.132
45 0.229 0.006 1441.896 0.120 0.006 1441.896
50 1.012 0.007 – 0.185 0.008 –
0.5 20 0.070 0.003 1.187 0.059 0.003 1.187
25 0.073 0.004 2.900 0.076 0.004 2.900
30 0.102 0.004 10.476 0.084 0.004 10.476
35 0.141 0.005 54.417 0.113 0.005 54.417
40 0.194 0.006 206.109 0.122 0.006 206.109
45 0.353 0.007 1411.889 0.164 0.007 1411.889
50 0.397 0.007 – 0.146 0.007 –
1 20 0.057 0.003 1.239 0.049 0.002 1.239
25 0.063 0.004 3.074 0.076 0.003 3.074
30 0.085 0.005 11.753 0.071 0.004 11.753
35 0.124 0.005 65.864 0.091 0.004 65.864
40 0.283 0.005 202.285 0.117 0.005 202.285
45 0.275 0.007 1490.630 0.126 0.006 1490.630
50 0.400 0.007 – 0.143 0.008 –
5 20 0.086 0.003 1.314 0.075 0.003 1.314
25 0.095 0.004 3.085 0.093 0.004 3.085
30 0.126 0.004 11.729 0.133 0.005 11.729
35 0.154 0.004 45.243 0.164 0.006 45.243
40 0.433 0.006 155.281 0.254 0.010 155.281
45 0.309 0.007 1231.948 0.606 0.020 1231.948
50 0.388 0.008 – 1.151 0.035 –
21Table C.2: Simulation without multicollinearity: Comparison of proposed method, HLM
with enumeration and special purpose solver for linear regression; average (sd) selection
accuracy.
Linearmodelsupportrecoveryaccuracy
AIC BIC
Proposedmethod HLMwithbruteforce Proposedmethod HLMwithbruteforce
σ p (Gurobi) lmSelect (Gurobi) (Gurobi) lmSelect (Gurobi)
0.05 20 0.91(0.02) 0.91(0.02) 0.91(0.02) 1.00(0.00) 1.00(0.00) 1.00(0.00)
25 0.90(0.04) 0.90(0.04) 0.90(0.04) 1.00(0.00) 1.00(0.00) 1.00(0.00)
30 0.89(0.05) 0.89(0.05) 0.89(0.05) 0.99(0.03) 0.99(0.03) 0.99(0.03)
35 0.90(0.09) 0.90(0.09) 0.90(0.09) 0.99(0.01) 0.99(0.01) 0.99(0.01)
40 0.92(0.03) 0.92(0.04) 0.92(0.04) 1.00(0.00) 1.00(0.00) 1.00(0.00)
45 0.90(0.03) 0.90(0.03) 0.90(0.03) 1.00(0.01) 1.00(0.01) 1.00(0.01)
50 0.94(0.03) 0.94(0.03) – 1.00(0.00) 1.00(0.00) –
0.1 20 0.90(0.05) 0.89(0.07) 0.90(0.05) 1.00(0.00) 1.00(0.00) 1.00(0.00)
25 0.93(0.05) 0.93(0.05) 0.93(0.05) 1.00(0.00) 1.00(0.00) 1.00(0.00)
30 0.93(0.04) 0.93(0.04) 0.93(0.04) 0.99(0.01) 0.99(0.01) 0.99(0.01)
35 0.91(0.04) 0.90(0.05) 0.91(0.05) 0.99(0.01) 0.99(0.03) 0.99(0.03)
40 0.93(0.04) 0.93(0.04) 0.93(0.04) 0.99(0.01) 0.99(0.01) 0.99(0.01)
45 0.91(0.05) 0.91(0.05) 0.91(0.05) 1.00(0.01) 1.00(0.01) 1.00(0.01)
50 0.90(0.03) 0.90(0.03) – 0.99(0.01) 0.99(0.01) –
0.5 20 0.91(0.11) 0.91(0.11) 0.91(0.11) 1.00(0.00) 1.00(0.00) 1.00(0.00)
25 0.91(0.08) 0.91(0.08) 0.91(0.08) 1.00(0.00) 1.00(0.00) 1.00(0.00)
30 0.96(0.04) 0.96(0.04) 0.96(0.04) 1.00(0.00) 1.00(0.00) 1.00(0.00)
35 0.93(0.06) 0.93(0.06) 0.93(0.06) 0.99(0.01) 0.99(0.01) 0.99(0.01)
40 0.92(0.02) 0.92(0.02) 0.92(0.03) 0.99(0.01) 0.99(0.01) 0.99(0.01)
45 0.90(0.06) 0.90(0.06) 0.90(0.06) 0.99(0.01) 0.99(0.01) 0.99(0.01)
50 0.93(0.04) 0.93(0.04) – 1.00(0.01) 1.00(0.01) –
1 20 0.89(0.07) 0.89(0.07) 0.89(0.07) 1.00(0.00) 0.99(0.02) 0.99(0.02)
25 0.94(0.04) 0.93(0.03) 0.93(0.03) 0.99(0.02) 0.99(0.02) 0.99(0.02)
30 0.94(0.08) 0.94(0.08) 0.94(0.08) 1.00(0.00) 1.00(0.00) 1.00(0.00)
35 0.91(0.06) 0.91(0.06) 0.91(0.06) 1.00(0.00) 1.00(0.00) 1.00(0.00)
40 0.93(0.04) 0.93(0.04) 0.93(0.04) 0.99(0.01) 0.99(0.01) 0.99(0.01)
45 0.90(0.03) 0.90(0.03) 0.90(0.03) 1.00(0.01) 1.00(0.01) 1.00(0.01)
50 0.94(0.02) 0.94(0.03) – 1.00(0.00) 1.00(0.00) –
5 20 0.92(0.07) 0.92(0.07) 0.92(0.07) 0.99(0.02) 0.99(0.02) 0.99(0.02)
25 0.92(0.05) 0.92(0.05) 0.92(0.05) 1.00(0.00) 1.00(0.00) 1.00(0.00)
30 0.87(0.04) 0.87(0.03) 0.87(0.03) 1.00(0.00) 1.00(0.00) 1.00(0.00)
35 0.94(0.04) 0.93(0.03) 0.93(0.03) 1.00(0.00) 1.00(0.00) 1.00(0.00)
40 0.94(0.04) 0.94(0.04) 0.94(0.04) 0.99(0.01) 0.99(0.01) 0.99(0.01)
45 0.92(0.06) 0.92(0.06) 0.92(0.06) 1.00(0.01) 1.00(0.01) 1.00(0.01)
50 0.94(0.02) 0.93(0.03) – 0.99(0.01) 0.99(0.01) –
22TableC.3: Simulationwithoutmulticollinearity: Comparisonofdifferentsolversandcom-
plete enumeration for logistic regression;average execution times in seconds.
Logisticmodeltrainingtime
AIC BIC
ProposedMethod ProposedMethod ProposedMethod ProposedMethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 0.896 2.329 0.081 0.878 2.188 0.081
10 1.971 4.244 2.522 2.035 4.188 2.510
15 4.553 7.220 94.975 3.076 5.809 95.105
20 12.635 19.999 – 5.051 11.751 –
25 21.486 29.610 – 7.755 13.668 –
30 27.868 38.201 – 11.001 14.549 –
35 77.166 105.110 – 21.279 25.351 –
40 47.305 91.766 – 33.676 28.208 –
TableC.4: Simulationwithoutmulticollinearity: Comparisonofdifferentsolversandcom-
plete enumeration for logistic regression;average (sd) selection accuracy.
Logisticmodelsupportrecoveryaccuracy
AIC BIC
ProposedMethod ProposedMethod ProposedMethod ProposedMethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 1.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00) 1.00(0.00)
10 0.94(0.06) 0.94(0.06) 0.94(0.06) 0.98(0.04) 0.98(0.04) 0.98(0.04)
15 0.95(0.03) 0.95(0.03) 0.95(0.03) 1.00(0.00) 1.00(0.00) 1.00(0.00)
20 0.93(0.03) 0.93(0.03) – 1.00(0.00) 1.00(0.00) –
25 0.92(0.07) 0.92(0.07) – 0.99(0.02) 0.99(0.02) –
30 0.95(0.04) 0.95(0.04) – 1.00(0.00) 1.00(0.00) –
35 0.95(0.02) 0.95(0.02) – 1.00(0.00) 1.00(0.00) –
40 0.95(0.02) 0.95(0.02) – 0.99(0.01) 0.99(0.01) –
23TableC.5: Simulationwithoutmulticollinearity: Comparisonofdifferentsolversandcom-
plete enumeration for Poisson regression;average execution times in seconds.
Poissonmodeltrainingtime
AIC BIC
ProposedMethod ProposedMethod ProposedMethod ProposedMethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 1.962 1.290 0.141 1.868 1.229 0.141
10 6.847 3.052 3.610 5.667 3.061 3.619
15 13.111 5.903 141.839 12.000 5.562 141.700
20 35.552 11.577 – 34.650 9.628 –
25 238.299 18.425 – 220.857 12.383 –
30 896.185 49.401 – 952.785 20.371 –
35 2408.308 51.174 – 2346.494 33.675 –
40 4030.605 215.312 – 3354.529 35.426 –
TableC.6: Simulationwithoutmulticollinearity: Comparisonofdifferentsolversandcom-
plete enumeration for Poisson regression;average (sd) selection accuracy.
Poissonmodelsupportrecoveryaccuracy
AIC BIC
ProposedMethod ProposedMethod ProposedMethod ProposedMethod
p (ECOS) (MOSEK) bestglm (ECOS) (MOSEK) bestglm
5 0.96(0.09) 0.96(0.09) 0.96(0.09) 0.92(0.18) 1.00(0.00) 1.00(0.00)
10 0.92(0.04) 0.92(0.04) 0.92(0.04) 1.00(0.00) 1.00(0.00) 1.00(0.00)
15 0.87(0.09) 0.87(0.09) 0.87(0.09) 1.00(0.00) 1.00(0.00) 1.00(0.00)
20 0.97(0.07) 0.94(0.06) – 1.00(0.00) 1.00(0.00) –
25 0.90(0.21) 0.95(0.04) – 0.90(0.21) 1.00(0.00) –
30 0.87(0.20) 0.94(0.06) – 0.90(0.19) 1.00(0.00) –
35 0.81(0.22) 0.93(0.04) – 0.82(0.23) 0.99(0.02) –
40 0.67(0.26) 0.94(0.03) – 0.78(0.28) 0.99(0.01) –
24Table C.7: Simulation with multicollinearity: Comparison of the impact of different
pairwise correlation constraints, for different levels of correlation, on the MSE for the
logistic regressionmodel.
Logistic model mean-squared error
IC alpha no constraint at most one sign sign combined
constraint coherence 0.5 coherence 0.7 constraint
constraint constraint
AIC 0.70 1.41e-01 1.41e-01 1.41e-01 1.41e-01 1.40e-01
AIC 0.80 1.96e-01 2.20e-01 1.96e-01 1.96e-01 1.55e-01
AIC 0.90 3.13e-01 4.94e-01 3.07e-01 3.07e-01 3.07e-02
AIC 0.95 5.18e-01 5.66e-01 4.90e-01 4.90e-01 2.95e-02
AIC 0.99 1.21e+00 6.20e-01 6.20e-01 6.20e-01 2.89e-02
BIC 0.70 2.10e-01 2.10e-01 2.10e-01 2.10e-01 2.10e-01
BIC 0.80 2.75e-01 2.89e-01 2.75e-01 2.75e-01 2.22e-01
BIC 0.90 4.50e-01 4.94e-01 4.50e-01 4.50e-01 3.10e-02
BIC 0.95 5.67e-01 5.66e-01 5.64e-01 5.64e-01 2.96e-02
BIC 0.99 7.68e-01 6.20e-01 6.20e-01 6.20e-01 2.89e-02
Table C.8: Simulation with multicollinearity: Comparison of the impact of different
pairwise correlation constraints, for different levels of correlation, on the MSE for the
Poissonregression model.
Poisson model mean-squared error
IC alpha no constraint at most one sign sign combined
constraint coherence 0.5 coherence 0.7 constraint
constraint constraint
AIC 0.70 8.98e-03 9.50e-03 8.98e-03 8.98e-03 8.97e-03
AIC 0.80 9.54e-03 5.81e-02 9.54e-03 9.54e-03 8.51e-03
AIC 0.90 1.36e-02 3.93e-01 1.36e-02 1.36e-02 4.13e-03
AIC 0.95 2.71e-02 4.42e-01 2.71e-02 2.71e-02 3.11e-03
AIC 0.99 1.58e-01 4.95e-01 1.57e-01 1.57e-01 5.77e-03
BIC 0.70 8.98e-03 9.50e-03 8.98e-03 8.98e-03 8.97e-03
BIC 0.80 9.66e-03 5.83e-02 9.66e-03 9.66e-03 8.56e-03
BIC 0.90 1.59e-02 3.93e-01 1.59e-02 1.59e-02 4.70e-03
BIC 0.95 4.54e-02 4.42e-01 4.54e-02 4.54e-02 3.11e-03
BIC 0.99 2.42e-01 4.95e-01 2.41e-01 2.41e-01 4.98e-03
25Table C.9: Simulation with multicollinearity: Comparison of the impact of different
pairwise correlation constraints, for different levels of correlation, on the MSE for the
linear regressionmodel.
Linear model mean-squared error
alpha sd no constraint at most one sign sign combined
constraint coherence 0.5 coherence 0.7 constraint
constraint constraint
0.70 0.01 1.46e-06 5.04e-04 1.46e-06 1.46e-06 1.57e-06
0.70 0.10 1.46e-04 6.45e-04 1.46e-04 1.46e-04 1.45e-04
0.70 0.20 5.84e-04 1.08e-03 5.84e-04 5.84e-04 5.81e-04
0.70 0.30 1.31e-03 1.81e-03 1.31e-03 1.31e-03 1.31e-03
0.70 0.40 2.33e-03 2.83e-03 2.33e-03 2.33e-03 2.32e-03
0.70 0.50 3.65e-03 4.14e-03 3.65e-03 3.65e-03 3.63e-03
0.70 1.00 1.48e-02 1.52e-02 1.46e-02 1.48e-02 1.46e-02
0.80 0.01 1.90e-06 4.71e-02 1.90e-06 1.90e-06 1.18e-05
0.80 0.10 1.90e-04 4.72e-02 1.90e-04 1.90e-04 1.57e-04
0.80 0.20 7.59e-04 4.79e-02 7.59e-04 7.59e-04 5.97e-04
0.80 0.30 1.71e-03 4.88e-02 1.71e-03 1.71e-03 1.33e-03
0.80 0.40 3.04e-03 5.02e-02 3.04e-03 3.04e-03 2.36e-03
0.80 0.50 4.75e-03 5.18e-02 4.75e-03 4.75e-03 3.67e-03
0.80 1.00 1.98e-02 6.63e-02 1.90e-02 1.94e-02 1.47e-02
0.90 0.01 3.27e-06 3.86e-01 3.27e-06 3.27e-06 1.69e-05
0.90 0.10 3.27e-04 3.87e-01 3.27e-04 3.27e-04 5.07e-05
0.90 0.20 1.31e-03 3.88e-01 1.31e-03 1.31e-03 1.53e-04
0.90 0.30 2.94e-03 3.90e-01 2.94e-03 2.94e-03 3.24e-04
0.90 0.40 5.23e-03 3.92e-01 5.23e-03 5.23e-03 5.63e-04
0.90 0.50 8.17e-03 3.95e-01 8.17e-03 8.17e-03 8.70e-04
0.90 1.00 4.52e-02 4.09e-01 3.25e-02 3.25e-02 3.43e-03
0.95 0.01 6.04e-06 4.39e-01 6.04e-06 6.04e-06 3.86e-06
0.95 0.10 6.04e-04 4.40e-01 6.04e-04 6.04e-04 3.70e-05
0.95 0.20 2.42e-03 4.41e-01 2.42e-03 2.42e-03 1.38e-04
0.95 0.30 5.44e-03 4.43e-01 5.44e-03 5.44e-03 3.05e-04
0.95 0.40 9.67e-03 4.44e-01 9.67e-03 9.67e-03 5.40e-04
0.95 0.50 1.55e-02 4.46e-01 1.51e-02 1.51e-02 8.42e-04
0.95 1.00 1.05e-01 4.57e-01 5.85e-02 5.85e-02 3.36e-03
0.99 0.01 2.83e-05 4.87e-01 2.83e-05 2.83e-05 4.56e-07
0.99 0.10 2.83e-03 4.88e-01 2.83e-03 2.83e-03 3.32e-05
0.99 0.20 1.14e-02 4.88e-01 1.13e-02 1.13e-02 1.32e-04
0.99 0.30 3.25e-02 4.89e-01 2.54e-02 2.54e-02 2.98e-04
0.99 0.40 7.63e-02 4.91e-01 4.43e-02 4.43e-02 5.29e-04
0.99 0.50 1.26e-01 4.92e-01 6.75e-02 6.75e-02 8.27e-04
0.99 1.00 4.04e-01 4.99e-01 2.10e-01 2.10e-01 3.31e-03
26References
[1] A. Miller, Subset Selection in Regression, CRC Press, 2002.
doi:10.1201/9781420035933.
[2] B. K. Natarajan, Sparse approximate solutions to linear systems, SIAM
journal on computing 24 (1995) 227–234.
[3] D.Bertsimas, A.King,R.Mazumder, Bestsubsetselectionviaamodern
optimization lens, The annals of statistics 44 (2016) 813–852.
[4] D. Bertsimas, A. King, Logistic Regression: From Art
to Science, Statistical Science 32 (2017) 367–384. URL:
https://projecteuclid.org/journals/statistical-science/volume-32/issue-3/Logis
doi:10.1214/16-STS602.
[5] H. Hazimeh, R. Mazumder, Fast best subset selection: Coordinate de-
scent and local combinatorial optimization algorithms, Operations Re-
search 68 (2020) 1517–1537.
[6] I. Guyon, A. Elisseeff, An introduction to variable and feature selection,
Journal of machine learning research 3 (2003) 1157–1182.
[7] R. Miyashiro, Y. Takano, Subset selection by mallows’ cp: A mixed
integer programming approach, Expert Systems with Applications 42
(2015) 325–331.
[8] M. Hofmann, C. Gatu, E. J. Kontoghiorghes, A. Colubi, A. Zeileis,
lmsubsets: Exact variable-subset selection in linear regression
for r, Journal of Statistical Software 93 (2020) 1–21. URL:
https://www.jstatsoft.org/index.php/jss/article/view/v093i03.
doi:10.18637/jss.v093.i03.
[9] T. Sato, Y. Takano, R. Miyashiro, A. Yoshise, Feature sub-
set selection for logistic regression via mixed integer optimiza-
tion, Computational Optimization and Applications 64 (2016)
865–880. URL: https://doi.org/10.1007/s10589-016-9832-2.
doi:10.1007/s10589-016-9832-2.
[10] H. Saishu, K. Kudo, Y. Takano, Sparse poisson regres-
sion via mixed-integer optimization, PLOS ONE 16 (2021).
doi:10.1371/journal.pone.0249916.
27[11] J. C. Debuse, V. J. Rayward-Smith, Feature subset selection within
a simulated annealing data mining algorithm, Journal of Intelligent
Information Systems 9 (1997) 57–81.
[12] J. Yang, V. Honavar, Feature subset selection using a genetic algorithm,
IEEE Intelligent Systems and their Applications 13 (1998) 44–49.
[13] V. Calcagno, C. de Mazancourt, glmulti: An r package for
easy automated model selection with (generalized) linear mod-
els, Journal of Statistical Software 34 (2010) 1–29. URL:
https://www.jstatsoft.org/index.php/jss/article/view/v034i12.
doi:10.18637/jss.v034.i12.
[14] Y. Chen, D. Miao, R. Wang, A rough set approach to feature selection
based onantcolonyoptimization, PatternRecognitionLetters31(2010)
226–233.
[15] X. Wang, J. Yang, X. Teng, W. Xia, R. Jensen, Feature selection based
on rough sets and particle swarm optimization, Pattern recognition
letters 28 (2007) 459–471.
[16] B. Schwendinger, F. Schwendinger, L. Vana, Holistic generalized lin-
ear models, arXiv (2022). URL: https://arxiv.org/abs/2205.15447.
doi:10.48550/ARXIV.2205.15447.
[17] F. Schwendinger, B. Gru¨n, K. Hornik, A comparison of
optimization solvers for log-binomial regression including conic
programming, Computational Statistics 36 (2021) 1721–1754.
doi:10.1007/s00180-021-01084-5.
[18] E. Carrizosa, A. V. Olivares-Nadal, P. Ram´ırez-Cobo, Integer con-
straints for enhancing interpretability in linear regression, SORT-
Statistics and Operations Research Transactions (2020) 67–98.
[19] A. E. Hoerl, R. W. Kennard, Ridge regression: Biased estimation for
nonorthogonal problems, Technometrics 12 (1970) 55–67.
[20] H. D. Bondell, B. J. Reich, Simultaneous regression shrinkage, variable
selection, and supervised clustering of predictors with oscar, Biometrics
64 (2008) 115–123.
28[21] K. Konis, Linear programming algorithms for detecting separated data
in binary logistic regression models, Ph.D. thesis, University of Oxford,
2007.
[22] J. A. Nelder, R. W. Wedderburn, Generalized linear models, Journal of
the Royal Statistical Society: Series A (General) 135 (1972) 370–384.
[23] S. Boyd, L. Vandenberghe, Convex optimization, Cambridge university
press, 2004.
[24] H. Zou, T. Hastie, Regularization and variable selection via the elastic
net, Journal of the royal statistical society: series B (statistical method-
ology) 67 (2005) 301–320.
[25] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal
of the Royal Statistical Society: Series B (Methodological) 58 (1996)
267–288.
[26] T. Hastie, R. Tibshirani, R. Tibshirani, Best subset, forward stepwise or
lasso? analysis and recommendations based on extensive comparisons,
Statistical Science 35 (2020) 579–592.
[27] Y. Yang, C. S. McMahan, Y.-B. Wang, Y. Ouyang, Estimation of l0
norm penalized models: A statistical treatment, Computational Statis-
tics & Data Analysis 192 (2024) 107902.
[28] J. D. Camm, A. S. Raturi, S. Tsubakitani, Cut-
ting big m down to size, Interfaces 20 (1990) 61–
66. URL: https://doi.org/10.1287/inte.20.5.61.
doi:10.1287/inte.20.5.61.
[29] H. Akaike, A new look at the statistical model identification, IEEE
transactions on automatic control 19 (1974) 716–723.
[30] G. Schwarz, Estimating the dimension of a model, The annals of statis-
tics (1978) 461–464.
[31] C. Mallows, Choosing variables in a linear regression: A graphical aid,
in: Central RegionalMeeting oftheInstituteofMathematical Statistics,
Manhattan, KS, 1964, 1964.
29[32] A. Boisbunon, S. Canu, D. Fourdrinier, W. Strawderman, M. T.
Wells, Akaike’s information criterion, c and estimators of loss for
p
elliptically symmetric distributions, International Statistical Review
82 (2014) 422–439. URL: http://www.jstor.org/stable/43299006.
doi:https://doi.org/10.1111/insr.12052.
[33] A. Albert, J. A. Anderson, On the existence of maximum likelihood
estimates in logistic regression models, Biometrika 71 (1984) 1–10.
[34] K. Konis, K. Fokianos, Safe density ratio modeling, Statistics & Prob-
ability Letters 79 (2009) 1915–1920. doi:10.1016/j.spl.2009.05.020.
[35] D. Bertsimas, A. King, An algorithmic approach to linear regression,
Operations Research 64 (2015) 2–16. doi:10.1287/opre.2015.1436.
[36] A. Domahidi, E. Chu, S. Boyd, ECOS: An SOCP solver for embedded
systems, in: European Control Conference (ECC), 2013, pp. 3071–3076.
doi:10.23919/ECC.2013.6669541.
[37] M. ApS, The MOSEK Rmosek package manual. Version 10.0.34, 2022.
URL: https://docs.mosek.com/latest/rmosek/index.html.
[38] Gurobi Optimization, LLC, Gurobi Optimizer Reference Manual, 2023.
URL: https://www.gurobi.com.
[39] M. Hofmann, C. Gatu, E. J. Kontoghiorghes, A. Colubi, A. Zeileis,
lmSubsets: Exact Variable-Subset Selection in Linear Regression, 2021.
URL: https://CRAN.R-project.org/package=lmSubsets, r package
version 0.5-2.
[40] A. McLeod, C. Xu, Y. Lai, bestglm: Best Sub-
set GLM and Regression Utilities, 2020. URL:
https://CRAN.R-project.org/package=bestglm, r package ver-
sion 0.37.3.
[41] J. Zhu, X. Wang, L. Hu, J. Huang, K. Jiang, Y. Zhang, S. Lin, J. Zhu,
abess: a fast best-subset selection library in python and r, The Journal
of Machine Learning Research 23 (2022) 9206–9212.
[42] G. C. McDonald, D. I. Galarneau, A monte carlo evaluation of some
ridge-type estimators, Journal of the American Statistical Association
70 (1975) 407–416. URL: http://www.jstor.org/stable/2285832.
30