Preprint
EMERGING PIXEL GROUNDING IN LARGE MULTI-
MODAL MODELS Without GROUNDING SUPERVISION
ShengcaoCao Liang-YanGui Yu-XiongWang
UniversityofIllinoisUrbana-Champaign
{cao44, lgui, yxw}@illinois.edu
User User
Describe the image in detail. Describe the image in detail.
Please output with
interleaved segmentation
masks for the
corresponding parts of the answer.
DIFFLMM + Attend-and-Segment (Ours)
GLaMM
The image features a dog and a cat standing next to each
other, posing for a picture. The dog is on the right side of
A brown dog and a gray cat
the image, while the cat is on the left. Both animals
are standing next to each
appear to be smiling, creating a friendly and harmonious
other.
atmosphere.
The dog is wearing a collar, which is visible around its neck. The cat, on
the other hand, is not wearing any visible accessories. The two animals
seem to be enjoying each other's company.
User
What color is the dog's collar?
User
What color is the dog's collar?
GLaMM
Sure, it is [seg].
DIFFLMM + Attend-and-Segment (Ours)
The dog's collar is silver.
Figure 1: Grounded conversations with GLaMM (Rasheed et al., 2024) vs. our approach,
DIFFLMM + attend-and-segment. Left: As a state-of-the-art grounding LMM, GLaMM is
trainedtorelatetextphraseswithsegmentationmaskswhilegeneratingaresponse. However,dueto
limitationsinducedbythegroundingsupervision,itoftenfailstopreciselyfollowthehumanuser‚Äôs
instructions(e.g.,describingtheimageindetail,answeringthecorrectcolor). Right: Ourapproach
revealsandenhancesthegroundingabilityimplicitlylearnedbyLMMswithoutexplicitgrounding
supervision,whichleadstovisuallygroundedresponseswhilepreservingthegeneralvision-language
conversationabilityofLMMs. MoreexamplesareshowninFigure4.
ABSTRACT
Currentlargemultimodalmodels(LMMs)facechallengesingrounding,which
requires the model to relate language components to visual entities. Contrary
tothecommonpracticethatfine-tunesLMMswithadditionalgroundingsuper-
vision, we find that the grounding ability can in fact emerge in LMMs trained
without explicitgroundingsupervision. Torevealthisemerginggrounding, we
introducean‚Äúattend-and-segment‚Äùmethodwhichleveragesattentionmapsfrom
standardLMMstoperformpixel-levelsegmentation. Furthermore,toenhancethe
groundingability, wepropose DIFFLMM,anLMMutilizingadiffusion-based
visualencoder,asopposedtothestandardCLIPvisualencoder,andtrainedwith
thesameweaksupervision. Withoutbeingconstrainedbythebiasesandlimited
scaleofgrounding-specificsupervisiondata,ourapproachismoregeneralizable
and scalable. We achieve competitive performance on both grounding-specific
and general visual question answering benchmarks, compared with grounding
LMMsandgeneralistLMMs,respectively. Notably,weachievea44.2grounding
maskrecallongroundedconversationgenerationwithoutanygroundingsuper-
vision, outperformingtheextensivelysupervisedmodelGLaMM.Projectpage:
https://groundLMM.github.io.
1
4202
tcO
01
]VC.sc[
1v90280.0142:viXraPreprint
1 INTRODUCTION
Largemultimodalmodels(LMMs)(Liuetal.,2023;Zhuetal.,2024;Daietal.,2023)havebrought
thenewopportunityofsolvingvision-languagetasksinageneral-purposemanner,whicharetypically
builtbyconnectingavisualencoderandalargelanguagemodel(LLM)andfine-tunedbyvisual
instructions. Currently,onemajorchallengefacedbyLMMsisgrounding‚Äîthekeyabilityofrelating
languagecomponents(e.g.,nounphrases)tovisualentities(e.g.,objects)inagivenimage(Yuetal.,
2016;Krishnaetal.,2017). Withthegroundingability,LMMscanlifttheconstraintoftext-only
responsesandaddressmorevision-languagetasksintherealworld.
ToequipLMMswiththegroundingability, thecommonbeliefisthatadditionalsupervisionfor
groundingisnecessary,andcorrespondingarchitecturalmodificationsneedtobeintroduced. For
instance,recenteffortsextendtheoutputmodalityfrompuretexttoboundingboxes(Chenetal.,
2023b;Pengetal.,2024),tracepoints(Xuetal.,2024),orsegmentationmasks(Laietal.,2024;
Rasheedetal.,2024),by1)attachingadditionalmodulestothevanillaLMMarchitecture,and2)
fine-tuningtheLMMwithgroundingsupervision. Thegroundingsupervisionoriginatesfromeither
re-purposingexistingdatasetsthatcontainhuman-labeledobject-levelannotationsorautomatically
annotatingimagesusingothermodels.
However, such reliance on strong supervision brings more undesired constraints: 1) Scalability:
Thecurrentscaleofimagedatasetswithhigh-qualityobject-levelannotations(atmostmillionsof
images(Shaoetal.,2019;Kuznetsovaetal.,2020))issignificantlysmallerthanthosewithonly
coarseimage-textpairs(uptobillionsofimages(Schuhmannetal.,2022)),sore-purposingsuch
object-levelannotationscanonlyresultinalimitedscaleofvisualinstructiondata. Meanwhile,if
theobject-levelannotationsareproducedbyautomatedmodels,suchannotationsarenoisierand
lessreliablethanhuman-labeledones(Rasheedetal.,2024). 2)Supervisionbias: Changingthedata
focustogroundingtaskscanleadtocatastrophicforgetting(French,1999)andhurtLMMs‚Äôgeneral
conversationcapabilities. Furthermore,whetherthegroundingdataaremanuallyannotated(Linetal.,
2014)orpseudo-labeledbyothermodels(Rasheedetal.,2024),theyarebiasedbytheannotators‚Äô
ormodels‚Äôknowledgeandmayfailtoalignwithgeneralhumanpreferences,asthesefine-grained
annotationscanvarysignificantlyamongdifferentannotatorsormodels. 3)Generalizability: The
groundingsupervisionisconstrainedwithinthevisualconceptsfromeithertheexistingdatasetsor
othermodels,whichcontradictswiththeultimategoalofdevelopingageneral-purposeassistant
forsolvingopen-worldproblems(Bendale&Boult,2015). Consequently,theresultingLMMsmay
behardtoscale,biasedbythegroundingsupervisiondata,andgeneralizepoorlytonovelvisual
conceptsanddomains. Figures1and4showillustrativeexamplesoftheselimitations.
Toavoidsuchlimitations,thequestionworthrethinkingthenarises:Isthereanapproachtogrounding
LMMs other than strong supervision? In fact, in this work, we reveal a critical yet previously
overlookedfact:LMMshaveinherentlyobtainedthegroundingabilitythroughtheweaklysupervised
visual instruction tuning. In other words, the grounding ability can emerge in LMMs without
groundingsupervision.Echoingpriorobservationsoftraditionalconvolutionalneuralnetworks(Zhou
etal.,2015;2016),wefindthatLMMslearntodetectvisualentitiesandrelatethemwiththelanguage
implicitly,duringtheprogressofvision-languagelearningattheimagelevel.
Wethereforeproposeasimpleandeffective‚Äúattend-and-segment‚Äùstrategytotransformthisemerging
groundingabilityintopixel-levelsegmentationmasks. Intuitively,theattentionmechanism(Vaswani
et al., 2017) in LMMs reveals where the LMM is looking at, and thus provides clues for visual
grounding.WestartwithabaseLMMtrainedwithstandardvisualinstructiontuning(Liuetal.,2023)
butwithoutgroundingsupervision,andacquireitsattentionmapscorrespondingtothevisualinputas
itgeneratesoutputtokens. Then,theattentionmapsarefurtherrefinedintopixel-levelsegmentation
masks. Withthisattend-and-segment method,weenablevision-languagetasksthatdirectlyrelyon
thegroundingcapability(e.g.,groundedconversationgeneration(Rasheedetal.,2024)). Remarkably,
attend-and-segment doesnotrequireexplicitgroundingsupervisionlikepriorworkdoes;incontrast,
weaksupervisionfromstandardvisualinstructiontuningdataissufficienttoachieveperformance
comparable with or even higher than previous grounding-supervised models. Furthermore, as a
generalapproach,attend-and-segment canbereadilyintegratedwithrecentgeneralistLMMs(Li
etal.,2024a;Tongetal.,2024a),andbenefitfromtheirstrongervision-languagecapabilities.
Furthermore,weintroduceasimplesolutiontoenhancetheemerginggroundingabilityofLMMs.
Previously, CLIP (Radford et al., 2021) plays a dominant role as the visual encoder of LMMs,
2Preprint
due to its vision-language feature alignment. However, CLIP is known to be weak in providing
localizedvisualfeatures(Zhouetal.,2022;Ghiasietal.,2022;Lietal.,2022),asitspre-training
simplyalignstheglobalrepresentationsofimage-textpairs. Throughsystematicevaluationonboth
grounding-specificandgeneraltasks,wefinddiffusionmodels(Hoetal.,2020;Rombachetal.,2022)
abetteralternativetoCLIP,astheirtext-to-imagegenerationcapabilityenablesbothvision-language
alignmentandlocalizedfeatures. Thus,weproposethediffusion-basedLMM(DIFFLMM),which
augmentstheCLIPvisualencoderoftheLMMwithadiffusion-basedvisualencoder,whilebeing
fine-tunedusingthesamedataastheoriginalLMM.Tothebestofourknowledge,DIFFLMMisthe
firstsuccessfulintegrationofdiffusion-basedvisualencodingandLMMsforbothvisualgrounding
and general vision-language tasks. Compared with the original LMM, DIFFLMM enhances the
groundingabilitywithoutsacrificingperformanceingeneral-purposevision-languagetasks.
OurextensiveexperimentsdemonstratethatLMMs‚Äôgroundingcapabilitiescanemergefromweak
supervision. Our approach, requiring no additional grounding supervision, is more scalable and
generalizable,andsufferslessfrombiasesinthegroundingsupervisiondata. Despitebeingtrained
onlessdatathanpriorgroundingLMMs(Laietal.,2024;Rasheedetal.,2024),DIFFLMMachieves
betterorcomparableperformanceongrounding-specificbenchmarks,whileadheringtoastrong
generalistmodelforvision-languagetasks. Tosummarize,ourcontributionsarethree-fold:
‚Ä¢ Differentfrompriormethodsthatrelyongrounding-specificstrongsupervision,weshowthepos-
sibilityofgroundingLMMswithoutgroundingsupervision. Eliminatingtheneedforfine-grained
annotationsfromhumansorexternalmodels,ourapproachismorescalableandgeneralizable.
‚Ä¢ Wediscoverasimpleandeffectiveapproach,attend-and-segment,toachievepixel-levelgrounding
forLMMsbyinspectingtheattentionmapsinthemodelgenerationprocessandconvertingthe
mapsintosegmentationmasks,whichrequiresnogroundingsupervisionorarchitecturalchanges.
‚Ä¢ WeproposeDIFFLMM,whichemploysavisualencoderbasedonthediffusionmodel. DIFFLMM
offersstrongergroundingcapabilitiesthantheoriginalLMM,whilemaintaininggeneralvision-
languagetaskperformance.
2 RELATED WORK
Large multimodal models (LMMs). Pioneering work in LMMs, such as LLaVA (Liu et al.,
2023;Sunetal.,2024;Liuetal.,2024a;b),MiniGPT-4(Zhuetal.,2024;Chenetal.,2023a),and
InstructBLIP(Daietal.,2023;Lietal.,2023a), enablesvisualinputsforlargelanguagemodels
(LLMs)viavision-languagefeaturealignment(Radfordetal.,2021)andinstructiontuning(Weietal.,
2022). ToequipLMMswiththegroundingability,aseriesofmethodshavebeenproposedtoproduce
modeloutputsofboundingboxes(Pengetal.,2024;Chenetal.,2023b;Wangetal.,2023;Pietal.,
2023;Youetal.,2024;Lietal.,2024b),tracesofpoints(Xuetal.,2024),orsegmentationmasks(Lai
etal.,2024;Rasheedetal.,2024;Zhangetal.,2024;Renetal.,2024),byaddingregion-specific
tokensordecoders. Thesemethodsrequirefurthergroundingsupervision,soimagedatasetswith
fine-grainedannotations(Linetal.,2014;Yuetal.,2016;Zhouetal.,2017)areusuallyrepurposed
forthevisualinstructiontuning. Unlikethesesupervisedmethods,ourapproach,attend-and-segment,
doesnotchangetheLMMarchitectureorrequireanygroundingsupervisiondata.
AconcurrentworkF-LMM(Wuetal.,2024a)showsasimilarmethodtoexploitattentionmapsin
frozenLMMsforvisualgrounding, butwedifferfromitmainlyintwoaspects: 1)F-LMMstill
followsthesupervisedlearningparadigmandusesgroundeddatatolearntheadditionalmodules,
whileourattend-and-segmentrequireszerosupervision.Forthefirsttime,werevealLMMs‚Äôemerging
groundingcapabilitieswithoutexplicitsupervision. 2)F-LMMexaminesexistingLMMswithout
changingtheirvisualencoding.Incontrast,basedonoursystematicanalysisonvisualrepresentations
andtheirgroundingability,weproposeDIFFLMMtofurtherenhancetheimplicitgrounding.
Diffusionmodels(DMs)asvisualfeatureextractors. DMs(Song&Ermon,2019;Hoetal.,2020;
Songetal.,2021;Karrasetal.,2022;Nichol&Dhariwal,2021;Rombachetal.,2022)havebecome
a prevalent paradigm in visual generation, and intermediate features from DMs are explored for
applicationsbeyondgenerativetasks. Forexample,DDPM-Seg(Baranchuketal.,2022),ODISE(Xu
etal.,2023),andEmerDiff(Namekataetal.,2024)utilizeDMfeaturesforvarioussegmentationtasks.
FeaturesfromDMscanalsoestablishpoint-orpixel-levelcorrespondencesbetweenimages(Tang
etal.,2023;Luoetal.,2023;Zhangetal.,2023;Hedlinetal.,2023). Forthefirsttime,weshow
DMscanbeutilizedforlearningageneral-purposeLMMwithstronggroundingcapabilities.
3Preprint
3 APPROACH
Inthissection,wefirstintroducethecommonarchitecturedesignofLMMs(Section3.1). Then,
we discuss attend-and-segment, which transforms the implicitly learned grounding ability into
segmentationmasks(Section3.2). BasedonthestandardLMMandattend-and-segment,wepropose
DIFFLMM,tofurtherenhancethegroundingabilitywithoutadditionalsupervision(Section3.3).
3.1 PRELIMINARY: META-ARCHITECTUREOFLARGEMULTIMODALMODELS(LMMS)
MostLMMs(Liuetal.,2023;Zhuetal.,2024;Daietal.,2023)shareacommonmeta-architecture
whichconsistsofavisualencoderM ,avision-to-languagefeatureprojectorM ,andalarge
V V(cid:55)‚ÜíL
language model (LLM) M , as illustrated in Figure 2. Given an image I of resolution H √óW,
L
the visual encoder M (e.g., CLIP (Radford et al., 2021)) is employed to extract visual features
V
V = M V(I) ‚àà Rh√ów√ócV, where h √ó w represents the feature map size, and c
V
is the visual
featuredimension. Then,thevisualfeaturemapisconsideredasasequenceofhwelements,and
projectedelement-wiseintothelanguagefeaturespacebytheprojectorM . Theprojectorcan
V(cid:55)‚ÜíL
beimplementedasalearnablelightweightmultilayerperceptron(MLP).Thek-thprojectedvisual
tokeniscomputedasv
k
=M V(cid:55)‚ÜíL(V k)‚ààRcL,wherec ListhefeaturedimensionintheLLM.The
visualtokens,concatenatedwithotherlanguagetokens,formtheinputsequenceS :
input
S ={t ,...,t ,v ,...,v ,t ,...,t }, (1)
input 1 p 1 hw p+1 p+q
where{v ,...,v }arethehwvisualtokensprojectedfromthevisualfeaturemap,t ,...,t are
1 hw 1 p
theplanguagetokensbeforethevisualtokens,and{t ,...,t }aretheqlanguagetokensafter
p+1 p+q
thevisualtokens.
TheLLMisusuallyadecoder-onlyTransformermodel,whichiscapableofnext-tokenprediction.
Given the input sequence S , the output sequence S = {o ,...,o } is generated in an
input output 1 r
auto-regressivemanner,wherethei-thtokenispredictedas:
o =M (S ,o ,...,o ). (2)
i L input 1 i‚àí1
Thegenerationisterminatedwhenthelastpredictedtokeno isaspecial‚Äúend-of-sequence‚Äùtoken.
r
3.2 Attend-and-Segment: GROUNDINGLMMSWITHOUTGROUNDINGSUPERVISION
PrioreffortstowardsgroundingLMMattachadetectionorsegmentationmoduletotheLMMarchi-
tecture,andspecializetheLMMtrainingprocedurewithgroundingsupervision,i.e.,visualinstruction
dataaugmentedbyobject-levelannotations,suchthattheLMMlearnstopredictconnectionsbetween
thetextresponseandtheimagecontentsintheformoflocalizedboundingboxesorsegmentation
masks. Incontrasttothesestronglysupervisedmethods, weproposeattend-and-segment, asim-
ple and effective method for grounding LMMs without changing their architecture or providing
additionalgroundingsupervision. Weinvestigatetheattentionmapsinsidethetransformer-based
language model when generating tokens, and observe strong interpretablity associated with the
attentionmaps. Intuitively,theattentionmapscanprovideinformationaboutwherethemodelis
lookingatwhenproducingoutputs.
Formally,weconsidertheinputtokensequenceS asdetailedinSection3.1. Whenpredictingan
input
outputtokeno i,wecapturetherawattentionmapsAr iaw ‚àà[0,1]nlayer√ónhead√ó(p+hw+q+i‚àí1)insidethe
transformer-basedLLMM ,wheren isthenumberoflayersintheLLM,n isthenumber
L layer head
ofheadsperlayer,andp+hw+q+i‚àí1isthenumberoftokensbeforethei-thoutputtokeno .
i
Weonlyusetheattentionmapsassociatedwiththehw visualtokens,andreducethedimensions
byaveragingovern layersandn headsperlayer. Thisoperationreturnsanattentionmatrix
layer head
Areduced ‚àà[0,1]h√ów,withthesamespatialdimensionasthevisualfeaturemap.
i
Theattentionbetweentheoutputtokenandthevisualtokenscanprovideinterpretablegrounding
signalsalready.Tofurtheramplifythegroundingsignalsandreducethenoise,weapplynormalization
acrossthewholeoutputsequence:
1(cid:88)r
Anorm =Areduced‚àí Areduced, (3)
i i r j=1 j
whereristheoutputsequencelength.
4Preprint
Image Projector ... The dog is on the
right side of the image,
Encoder ùëÄ ! ùëÄ !‚Ü¶# Large while the cat
Language
Model
Input ùëÜ $%&‚Äô( ùëÄ Output ùëÜ )‚Äô(&‚Äô(
#
ùë°! ‚ãØ ùë°" ùë£! ‚ãØ ùë£#$ùë°"%! ‚ãØ ùë°"%& ùëú! ‚ãØ ùëú‚Äô(! ùëú‚Äô
Describe the
image in detail. ùëÄ *+,
Attention
Figure2: Meta-architectureofLMMsandtheattend-and-segment strategy. Inthestandard
LMM, the image encoder M extracts visual features from an input image, and the features are
V
transformedintovisualtokensbytheprojectorM . ThelargelanguagemodelM generates
V(cid:55)‚ÜíL L
theoutputinanauto-regressivemanner. Whengeneratinganewtoken(e.g.,‚Äúcat‚Äù)whichrequires
grounding, we capture the attention between the new token and the input visual tokens. Then
SAM(Kirillovetal.,2023)isemployedtorefinetheprocessedattentionmapintoasegmentation
mask(e.g.,catintheimage).
Toprovidepixel-levelgrounding,wederiveasegmentationmaskbyupsamplingtheattentionmap
andemployingthepre-trainedSegmentAnythingModel(SAM)(Kirillovetal.,2023). Foreach
tokenthatrequiresgrounding,weproduceitscorrespondingbinarymaskbypromptingSAMwiththe
coordinatethathasthehighestnormalizedattention. Thus,forelementsoftheoutputsequence,our
attend-and-segment methodprovidespixel-levelgroundingresults. Notably,weusetheoff-the-shelf
SAMwithoutanymodification,whereaspriorpixel-levelgroundingLMMs(Laietal.,2024;Rasheed
etal.,2024)needtofine-tuneSAMwithothermodules.
Indownstreamtaskslikegroundedconversationgeneration,itispreferredtorelatenounphrases,
insteadoftokens,totheimagecontents. Tothisend,weutilizeexistingnaturallanguageprocessing
tools (e.g., spaCy (Honnibal et al., 2020)) to parse the output sequence into noun phrases, and
associate noun phrases with the output tokens. For each noun phrase, we produce segmentation
masksusingtheaverageofnormalizedattentionmapsfromthecorrespondingtokens. Moredetails
areincludedinAppendixA.
3.3 DIFFLMM:ENHANCEDGROUNDINGWITHDIFFUSION-BASEDLMM
MostLMMsemployCLIP(Radfordetal.,2021)asthevisualencoderbecauseithasbeenpre-trained
to align vision and language representations, but CLIP is known to be sub-optimal in tasks that
requirepreciselocalization(e.g.,objectdetection,imagesegmentation)(Zhouetal.,2022;Ghiasi
etal.,2022;Lietal.,2022). ToenhancethegroundingabilityofLMMs,adirectchoicemaybe
replacingCLIPwithbetterlocalizedpure-visionbackboneslikeDINO(Caronetal.,2021;Oquab
etal.,2024). However,thelackofalignmentwithlanguagerepresentationscanhurtvision-language
taskperformance(Jiangetal.,2023;Tongetal.,2024b).
Comparedwithvision-languagemodelswithimage-levelalignment(e.g.,CLIP)andpure-vision
models(e.g.,DINO),visualrepresentationsfromdiffusionmodels(DMs)strikeabetterbalance: 1)
DMslearntogeneratehigh-fidelityimages,forwhichwell-localizedvisualfeaturesarenecessary.
Consequently, they are better than CLIP at localization. 2) DMs are trained to perform text-to-
imagegeneration,andinthisprocedure,theyacquirealignmentwithlanguageinstructions,whichis
lackinginpure-visionmodelslikeDINO.Therefore,weproposediffusion-basedLMM(DIFFLMM,
illustratedinFigure3),whichstrengthensthevisualencoderwithapre-trainedDM.
ToextractvisualfeaturesforagiveninputimageI,wesimulateonedenoisingstepinthediffusion
process. Theimageistokenizedbyavectorquantized(VQ)encoder,addedwitharandomnoise,
and fed into the U-Net model of a DM (Ho et al., 2020; Rombach et al., 2022). We extract the
visual feature map from the second upsampling block in the U-Net, which best preserves visual
semantics(Tangetal.,2023). TextconditioningcanenhancethevisualfeatureextractionintheDM,
buttheimagecaptionisusuallyunavailable. Weemploytheimplicitcaptioningmechanism(Xuetal.,
2023),whichsimulatestextconditioningbytheCLIPvisualencoder. Specifically,theCLIPvisual
5Preprint
Denoising U-Net SD Features
Positional Encodings Visual Features
VQ Add
Encoder Noise
Concatenate +
Implicit Captioner CLIP Features
CLIP CLIP
Visual Encoder Features MLP Condition Text Input
Projector
Figure3: VisualencodinginDIFFLMM.Weperformonedenoisingstepwiththediffusionmodel
(DM)(Hoetal.,2020;Rombachetal.,2022),andextractvisualfeaturesfromanintermediateblock
oftheU-Net. Theimplicitcaptioner(Xuetal.,2023)producestext-likeconditioningandimproves
thevisualfeaturesintheU-Net. WecombinebothDMfeaturesandCLIPfeatures,andaddlearnable
positionalencodingstothem. Thefinalvisualfeaturesareprojectedintothelanguagefeaturespace,
andfedintotheLLMalongwithothertexttokens. TheDMandCLIPvisualencoderarefrozen.
featuresareextractedasV =M (I),projectedbyamultilayerperceptron(MLP)M ,
CLIP CLIP CLIP(cid:55)‚ÜíSD
and injected into the U-Net via cross-attention modules. We denote the DM visual features as
V =M (I,M (V )). Finally,thevisualfeaturemapV iscomposedbyconcatenating
SD SD CLIP(cid:55)‚ÜíSD CLIP
bothDMfeaturesandCLIPfeatures(notethatwecanreusetheCLIPfeatureswithoutadditional
overhead),andaddingasetoflearnablepositionalencodingsPE (Vaswanietal.,2017)tofurther
enhancelocalizationawareness:
V =concat(V ,V )+PE ‚ààRh√ów√ócV. (4)
SD CLIP
Forefficienttrainingandpreventingoverfitting,wefreezepre-trainedparametersintheCLIPvisual
encoderandtheDM.OnlytheMLPintheimplicitcaptioner,thepositionalencodings,andthevision-
languagefeatureprojectorarelearnableinthevisualencoderofDIFFLMM.Sincethecomputation
isdominatedbythelargelanguagemodelcomponentinDIFFLMM,integratingdiffusionmodelsin
DIFFLMMdoesnotsignificantlyimpacttheefficiency. Weonlyobserveamarginalincreaseinthe
trainingandinferencetime(<5%).
4 EXPERIMENTS
In this section, we first present comprehensive empirical results to evaluate our proposed attend-
and-segment andDIFFLMMonbothgrounding-specifictasks(Sections4.1and4.2)andgeneral
visualquestionansweringtasks(Section4.3). Then,weexamineourmoduledesigns(Section4.4)
andshowqualitativeresults(Section4.5). Duetolimitedspace,weincludeimplementationdetails
andfurtherresultsintheappendix. Itisworthnotingthatattend-and-segment andDIFFLMMare
generalapproachesforLMMs,butconsideringthecomputationlimitations,wefocusongrounding
andenhancingLMMswith7Bor8B-scalelanguagemodels(Chiangetal.,2023;Meta,2024).
4.1 PILOTSTUDY: INSTANCESEGMENTATION
We start by conducting an analytical study via instance segmentation (He et al., 2017) on MS-
COCO (Lin et al., 2014) to demonstrate the emergence of grounding ability in LMMs and how
differentvisualencodersimpactthisability. Differentfromvision-languageentangledbenchmarks
(which will be tested in later sections), the vision-centric instance segmentation task 1) directly
focusesonrelatingimageregions(representedassegmentationmasks)withvisualconcepts(object
categories),whichisexactlytheobjectiveofgrounding,and2)doesnotevaluatebasedonlanguage
generation,makingitmoreconvenienttodirectlycomparegroundingabilitiesindifferentmodels.
LMMsarenotoriginallydesignedforinstancesegmentation. Therefore,forevaluationpurposes,
weaskLMMstogenerateadetaileddescriptionofagivenimage,andleverageattend-and-segment
toproducepairsofnounphrasesandsegmentationmasksfromtheLMMresponse. Thenwefind
thebest-matchingcategorylabelforeachnounphrasebycomputingtheirembeddingsimilarities
6Preprint
Table 1: Analysis of grounding abilities based on instance segmentation. We examine the
grounding ability embedded in the attention maps of LMMs, and compare LMMs trained with
differentvisualbackbones(includingCLIP(Radfordetal.,2021;Chertietal.,2023),DINOv2(Oquab
etal.,2024), andStableDiffusion(Rombachetal.,2022))andthesamedatawithoutgrounding
supervision, based on LLaVA-1.5 (Liu et al., 2024a). The original LLaVA-1.5 achieves a non-
trivialperformancecomparedwiththebaselineofrandomlysamplingpointsandpromptingSAM.
DIFFLMMenhancesthisgroundingabilitywithdiffusion-basedvisualfeatures,andevensurpasses
Cambrian-1(Tongetal.,2024a),whichreliesonanensembleoffourvisualencoders,onmaskAR.
Model VisualBackbone PAcc AP AP AP AP AR AR AR AR
S M L S M L
RandomPoint 10.53 0.0 0.2 0.8 0.3 0.1 1.2 10.1 3.8
CLIP(original) 34.01 1.8 6.6 6.3 3.9 5.8 21.7 43.2 22.8
LLaVA-1.5 ConvNeXtCLIP 37.16 3.1 7.0 8.4 4.9 8.4 22.1 44.0 23.9
DINOv2 34.55 1.9 6.7 7.2 4.2 6.4 22.0 41.7 23.0
DIFFLMM SD-1.5 38.92 2.1 7.6 9.9 5.7 6.4 25.3 48.8 25.9
(Ours) SD-1.5+CLIP 40.22 1.6 7.9 9.6 5.6 6.3 25.5 47.3 26.0
Cambrian-1 Ensemble 44.49 2.0 6.9 10.6 6.0 6.3 20.7 39.1 21.4
usingspaCy(Honnibaletal.,2020). SincetheLMMisnotconstrainedtoonlydescribeobjectsthat
areannotatedbythedataset(andshouldnotberewardedorpenalizedfordetectingout-of-domain
objects),weexcludepredictionsthatcannotbematchedwithanycategorylabelthatappearinthe
givenimage. Wecomparethestandardmetricsininstancesegmentation,maskaverageprecision
(AP)andmaskaveragerecall(AR).Inthissetting,APislowerthanARbecausethemodelsarenot
supervisedforthetask,andwedonotexplicitlyremoveduplicatedpredictions. Tofurtherdecouple
the quality of the attention map vs. SAM refinement, we compute a new metric, point accuracy
(PAcc),whichistheratioofpromptpointsthatcorrectlyfallintothemasksofthecorresponding
category. Forcomparison,weconsiderabaselinethatsimulatesa‚Äúblind‚ÄùLMM,whichprompts
SAMwitharandompointforsegmentingeachground-truthcategory.
As shown in Table 1, the attention maps in the original LLaVA-1.5 achieve a non-trivial 34.01
accuracy, indicating that the attention maps can be utilized for fine-grained grounding. Further
refiningtheattentionmapsintosegmentationmasksleadsto22.8AR.Comparingmodelsequipped
withdifferentvisualencodersbuttrainedwiththesamedata,ourDIFFLMMachievesthebestoverall
pointaccuracyandmaskAP/AR,whetherornotweconcatenatethediffusionfeatureswithCLIP
features. Arecentvision-centricLMM,Cambrian-1(Tongetal.,2024a),utilizinganensembleof
four visual backbones including CLIP variants and DINOv2, has an even higher point accuracy
andmaskAP.However,duetodifferenttrainingdata,ittendstogenerateshorterdescriptionsthan
LLaVA-1.5,resultinginmoremissedobjectsandlowermaskAR.
4.2 GROUNDEDCONVERSATIONGENERATION
The pilot study on instance segmentation shows that LMMs trained without explicit grounding
supervisionalreadyimplicitlyacquiresgroundingability,whichcanbeusedtoproducepixel-level
segmentationmasks. Followingthediscussionabove,weexamineLMMs‚Äôgroundingabilityona
morecomprehensivebenchmark,groundedconversationgeneration(GCG)(Rasheedetal.,2024).
TheobjectiveofGCGistounderstandvisualentitiesinanimage,andorganizethemintoalocalized
description. Tobespecific,theGCGtaskrequirestheLMMtogenerateadetailedcaptionforagiven
image,inwhichphrasesarerelatedtotheircorrespondingsegmentationmasksintheimage.
Since the GCG task requires model abilities in both captioning and segmentation, three types of
metricsareconsidered: 1)Tomeasurethecaptionquality,thetext-onlymetric,METEOR(Banerjee
&Lavie,2005),comparesthegeneratedcaptionswiththehuman-annotatedreferencecaptions. 2)To
assessthesegmentationmaskquality,themask-onlymetric,meanintersection-over-union(mIoU),
quantifiesthesimilaritybetweenground-truthmasksandtheirmatchedpredictedmasks. 3)The
groundingmaskrecall(Rasheedetal.,2024)isanintegratedmetricforregion-specificgrounding,
whichconsidersboththemaskIoUandthetextualsimilaritiesbetweenthepredictionsandtheground
truth. Therefore,thegroundingmaskrecallismainlyconsideredwhencomparingdifferentmodels.
7Preprint
Table2: Groundedconversationgeneration(GCG)results. Evenwithoutgroundingsupervision,
attend-and-segment (a&s inthetable)unlockstheimplicitlylearnedgroundingabilityinLLaVA-
1.5(Liuetal.,2024a),outperformingallgrounding-specificmodelsonthistask. DIFFLMMfurther
enhancesthegroundingability,andleadstostrongergroundingperformance. ThehigherMETEOR
scores demonstrate our better preserved conversation ability. As a general approach, attend-and-
segment canbeappliedondifferentLMMs(Lietal.,2024a;Tongetal.,2024a). Allmethodsare
evaluatedbythetext-onlymetricMETEOR(M)(Banerjee&Lavie,2005),themask-onlymetric
mIoU,andthecombinedmetricgroundingmaskrecall(Rec)(Rasheedetal.,2024)ontheGrand
f
dataset(Rasheedetal.,2024). BaselineresultsarefromGLaMM(Rasheedetal.,2024).
Grounding ValidationSet TestSet
Model
Supervision M‚Üë mIoU‚Üë Rec‚Üë M‚Üë mIoU‚Üë Rec‚Üë
BuboGPT(Zhaoetal.,2023) 17.2 54.0 29.4 17.1 54.1 27.0
Kosmos-2(Pengetal.,2024) 16.1 55.6 28.3 15.8 56.8 29.0
‚úì
LISA(Laietal.,2024) 13.0 62.0 36.3 12.9 61.7 35.5
GLaMM(Rasheedetal.,2024) 16.2 66.3 41.8 15.8 65.6 40.8
LLaVA-1.5+a&s (Ours) 18.6 58.0 44.2 18.3 59.3 42.7
LLaVA-NeXT+a&s (Ours) 15.6 64.5 45.6 15.6 65.6 44.2
‚úó
Cambrian-1+a&s (Ours) 14.6 59.8 42.0 14.5 60.7 40.4
DIFFLMM+a&s (Ours) 18.4 61.2 46.6 18.2 62.1 44.2
InTable2wecompareourapproach,whichlearnstheLMMwithoutanygroundingsupervision,with
priormethodsforgroundingLMMs(Zhaoetal.,2023;Pengetal.,2024;Laietal.,2024;Rasheed
etal.,2024). Evenwithoutgroundingsupervision,ourattend-and-segment leadsto42.7maskrecall
fortheoriginalLLaVA-1.5(Liuetal.,2024a),whichisalreadyhigherthanallthepreviousgrounding
LMMs. Asageneralapproach,attend-and-segment canbeusedinconjunctionwithrecentLMMs
suchasLLaVA-NeXT(Lietal.,2024a)andCambrian-1(Tongetal.,2024a),andbenefitfromtheir
improved visual encoding and vision-language capabilities. Compared with CLIP-based LMMs,
DIFFLMM provides better localized visual features and improves the grounding ability. When
usingourDIFFLMMastheLMM,wereachthehighest44.2testrecall. Ourmethodachievespixel
grounding but does not suffer from the supervision bias brought by grounding annotations, and
thusbetterpreservesthetext-onlyconversationabilities,asshownbythehigherMETEORscores.
AppendixCshowsmorequalitativeresultsonGCG.
4.3 VISUALQUESTIONANSWERING
WhenenhancingthegroundingabilityofLMMs,wedonotwantLMMstolosetheirgeneralvision-
languageabilities. Toassesssuchgeneralabilities,weevaluateDIFFLMMonawiderangeofvisual
questionanswering(VQA)benchmarks,includingVQAv2(Goyaletal.,2017),GQA(Hudson&
Manning,2019),Vizwiz(Gurarietal.,2018),ScienceQA-IMG(Luetal.,2022),andTextVQA(Singh
etal.,2019). WealsoconsidermorecomprehensiveLMMbenchmarks,includingPOPE(Lietal.,
2023b),MMBench(Liuetal.,2024c),andLLaVA-Bench(Liuetal.,2023).
ItisworthnotingthatpreviousgroundingLMMs(e.g.,LISA(Laietal.,2024),GLaMM(Rasheed
etal.,2024))arenotusuallyevaluatedonthesegeneral-purposeVQAbenchmarks. Forexample,
POPE is designed for quantifying object hallucination in LMMs by asking questions like ‚ÄúIs
there an [object] in the image?‚Äù butthequeriedobjectoftendoesnotexist. However,
wefindthatGLaMMalmostalwaysanswers‚ÄúSure, it is [seg].‚Äù andprovidesanincorrect
segmentation mask (see examples in Figure 4). Such loss of capabilities in answering general
questions is due to supervision bias‚Äîthese LMMs are fine-tuned for grounding tasks and they
forgethowtoanswergeneralvisualquestionswithoutgrounding. Therefore,groundingLMMslike
GLaMMhaveextremelylowscoresonthesebenchmarks,andwechoosetocomparewithstronger
generalistLMMsthatarenotdesignedforgroundingtasksonVQAbenchmarks.
Whencomparedwithstate-of-the-artLMMsofthesamescale(fine-tunedfroma7BLLM),including
InstructBLIP(Daietal.,2023),IDEFICS(HuggingFace,2023),Qwen-VL-Chat(Baietal.,2023),
andLLaVA-1.5(Liuetal.,2024a),DIFFLMMranks1ston3benchmarks,and2ndon4benchmarks.
8Preprint
Table3: VisualQuestionAnswering(VQA)results. WeevaluateandcomparegeneralistLMMsof
thesamescale(allwitha7B-sizedLLM)onawiderangeofbenchmarks,includingVQAv2(Goyal
etal.,2017),GQA(Hudson&Manning,2019),Vizwiz(VW)(Gurarietal.,2018),ScienceQA-IMG
(SQA)(Luetal.,2022),TextVQA(TQA)(Singhetal.,2019),POPE(Lietal.,2023b),MMBench
(MM-B)(Liuetal.,2024c),andLLaVA-Bench(LV-B)(Liuetal.,2023). Differentfrompriormodels,
DIFFLMMisbuiltuponadiffusionmodel(DM)visualencoder,whichprovidesstrongergrounding
(Tables1and2)andpreservesvision-languageabilitiesingeneraltasks. Notably,GLaMM(Rasheed
etal.,2024)failsinthesegeneralVQAtasks. Foreachbenchmark,the1stand2ndbestmodelsare
marked. BaselineresultsarefromLLaVA-1.5(Liuetal.,2024a).
Model Visual VQAv2 GQA VW SQA TQA POPE MM-B LV-B
InstructBLIP(Daietal.,2023) CLIP - 49.2 34.5 60.5 50.1 78.9 36.0 60.9
IDEFICS(HuggingFace,2023) CLIP 50.9 38.4 35.5 - 25.9 - 48.2 -
Qwen-VL-Chat(Baietal.,2023) CLIP 78.2 57.5 38.9 68.2 61.5 - 60.6 -
LLaVA-1.5(Liuetal.,2024a) CLIP 78.5 62.0 50.0 66.8 58.2 85.9 64.3 65.4
DIFFLMM(Ours) DM 78.3 62.1 48.1 69.3 57.2 85.7 66.2 63.7
Table 4: Ablation study on attend- Table5:AblationstudyonDIFFLMM.Wepre-trainDIF-
and-segment. Normalizing attention FLMMonthedataasLLaVA-1.5(Liuetal.,2024a),and
mapsacrosstheentiresequenceremoves comparetheconvergedlossesofvariousbackbonesinclud-
noisy patterns and improves ground- ing CLIP (Radford et al., 2021), DINOv2 (Oquab et al.,
ing. Prompting SAM (Kirillov et al., 2024),andSD-1.5(Rombachetal.,2022). Lowerlosses
2023) with a single point instead of indicatebettervision-languagealignment. Bothpositional
a low-resolution mask is more effec- encodings (PE) and implicit captioner (IC) improve the
tive. Ourattend-and-segment combines convergenceofDIFFLMM.
bothtechniques. Theresultsarebased
on evaluating DIFFLMM on the GCG Model VisualBackbone PE IC Loss‚Üì
task(Rasheedetal.,2024).
CLIP ‚Äì 2.027
LLaVA-1.5 DINOv2 ‚Äì 2.403
Attn SAM GCGVal
CLIP+DINOv2 ‚Äì 2.088
Norm Prompt mIoU‚Üë Rec‚Üë
SD-1.5 2.384
‚úì ‚úó M Poa is nk t 5 50 7. .0 4 3 46 4. .5 1 (D OI uFF rsL )MM S SD D- -1 1. .5 5 ‚úì ‚úì ‚úì 2 2. .3 13 48 1
‚úì Point 61.2 46.6 SD-1.5+CLIP ‚úì ‚úì 2.032
Since DIFFLMM istrainedonthesamedataasLLaVA-1.5, similarresultsareobserved. There-
fore, our diffusion-based DIFFLMM improves fine-grained vision-language comprehension that
specificallyrequiresthegroundingability,whileretainingstronggeneralvision-languagecapabilities.
4.4 ABLATIONSTUDY
Inthissection,weexaminethedesignsinattend-and-segment andDIFFLMM.Weincludefurther
analysisoftheattentionmapsinAppendixB.
Ourattend-and-segment appliesnormalizationacrossthesequenceofattentionmaps(Equation3),
whichsignificantlyreducesnoisesinthemaps(Figure6). Fromtheattentionmap,weselectthe
singlepointwiththehighestattentionvaluetopromptSAM,insteadofprovidingtheentiremapas
amaskprompt. Empirically,wefindthatattentionmapsaresparse,tendingtofocusonafewkey
pointswithinobjectsratherthantheentireobjects,sopointpromptsaremoreeffective. Quantitative
comparisonsaresummarizedinTable4.
In DIFFLMM (Figure3),weemployafewmodulestoenhancethevisualfeatureextractionand
encouragebetteralignmentwiththelanguagemodel. Specifically,we1)addlearnablepositional
encodings(Vaswanietal.,2017)tothevisualfeatures,and2)usetheimplicitcaptioner(Xuetal.,
2023)tosimulatetextconditioningwithCLIPvisualfeatures. Duetolimitedcomputation,wecannot
retrainmodelswiththefulldatasetofLLaVA-1.5(Liuetal.,2024a)andrunthoroughevaluationas
intheprevioussections. Instead,weexaminethemodules‚Äôeffectswithrespecttotheoptimization
9Preprint
User User User User
What is Who is sitting Describe the Do you know
unusual about on the bench? image in the author of
this image? detail. this painting?
GLaMM GLaMM GLaMM GLaMM
A man is standing on top of a It is [seg]. A furry white animal standing in Sure, it is [seg].
car washing platform. a field.
Ours Ours Ours Ours
The unusual aspect of this The image features a large, fluffy,
image is that a man is standing white llama standing in a grassy Yes, the painting is by the
on the back of a yellow taxi cab, No one is sitting on the bench. field. The llama appears to be famous French artist Claude
holding a laundry bag... looking up, possibly observing Monet.
something in the sky...
Figure4: Comparisonofmodelresponsestochallengingvisualquestions. 1)Unusualimage
contents: Themodelisrequestedtoanalyzetheunusualaspectofagivenimage. Comparedwith
GLaMM,ourapproachprovidesamoredetailedandaccurateanswerwithgrounding. 2)Adversarial
questions: Themodelisaskedaboutsomethingthatdoesnotexistintheimage. GLaMMinsiststo
segmentthebikebehindthebenchinthisexample. 3)Rarevisualconcepts: Theimagecontains
objects of less frequent categories. In this example, GLaMM does not recognize the llama but
describesitinageneralmanner,whileourapproachprovidesamoreaccuratedescription. 4)Shifted
image domain: An image from a new domain is given to the model. Interestingly, our approach
seemstobemakingthedecisionbasedonthetextureandstyleinthepainting. Forvisualclarity,
weonlyshowthebeginningpartsofourmodelresponsesiftheyaretoolong. Thesechallenging
examplesdemonstratesbettergeneralizabilityofourapproach.
objectiveinthepre-trainingstage(Liuetal.,2023),assummarizedinTable5. Triviallyreplacingthe
CLIP(Radfordetal.,2021)visualencoderwithDINOv2(Oquabetal.,2024)leadstosignificantly
higherlosses,whichimpliesworsevision-languagealignment. Thankstothetext-to-imagetraining,
SD-1.5(Rombachetal.,2022)resultsinasmallerloss. Thepositionalencodingsclosethelossgap
byabout13%,andfurtheraddingtheimplicitcaptionerreducesthegapbyanother55%.
4.5 QUALITATIVERESULTS
InFigure4wepresentqualitativeresultsofDIFFLMM+attend-and-segment formorechallenging
visual questions that are different from the training data, in comparison with GLaMM (Rasheed
etal.,2024). First,whenthequestionsarenotformulatedasusual,GLaMMtendstointerpretthese
questionsasimagecaptioningorreferringsegmentationtasks,whileDIFFLMMcanstillfollowthe
user‚Äôsinstructionsandaccuratelyanswerthequestions. Meanwhile,attend-and-segment provides
well-groundedresponsesthatconnectstextphrasesandvisualentities. Furthermore,ourapproach
showsbettergeneralizabilitytounfamiliarquestiontypes,visualconcepts,andimagedomains.
5 CONCLUSION
In this work, we reveal a previously overlooked yet critical fact that LMMs possess grounding
capabilitieseveniftheyaretrainedwithoutgroundingsupervision. Weproposeattend-and-segment
toconvertthisimplicitgroundingabilityintosegmentationmasks,andweintroduceDIFFLMMto
furtherenhancethegroundingability. Ourapproachismorescalableandgeneralizablecompared
withsupervisedmethods. Moreover,extensiveevaluationresultsdemonstratestrongperformanceon
bothgrounding-specificandgeneralvision-languagebenchmarks,evensurpassinggroundingLMMs
trainedwithextensivesupervisiononthechallenginggroundedconversationgenerationtask.
10Preprint
REFERENCES
JinzeBai, ShuaiBai, ShushengYang, ShijieWang, SinanTan, PengWang, JunyangLin, Chang
Zhou,andJingrenZhou. Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
SatanjeevBanerjeeandAlonLavie. METEOR:Anautomaticmetricformtevaluationwithimproved
correlationwithhumanjudgments. InACLWorkshop,2005.
DmitryBaranchuk,AndreyVoynov,IvanRubachev,ValentinKhrulkov,andArtemBabenko. Label-
efficientsemanticsegmentationwithdiffusionmodels. InICLR,2022.
AbhijitBendaleandTerranceBoult. Towardsopenworldrecognition. InCVPR,2015.
MathildeCaron,HugoTouvron,IshanMisra,Herve¬¥ Je¬¥gou,JulienMairal,PiotrBojanowski,and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InICCV,2021.
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing
web-scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. MiniGPT-v2: Large
language model as a unified interface for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478,2023a.
KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalLLM‚Äôsreferentialdialoguemagic. arXivpreprintarXiv:2306.15195,2023b.
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade
Gordon,ChristophSchuhmann,LudwigSchmidt,andJeniaJitsev. Reproduciblescalinglawsfor
contrastivelanguage-imagelearning. InCVPR,2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale N. Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-
languagemodelswithinstructiontuning. InNeurIPS,2023.
Timothe¬¥eDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski. Visiontransformersneed
registers. InICLR,2024.
RobertM.French. Catastrophicforgettinginconnectionistnetworks. Trendsincognitivesciences,3
(4):128‚Äì135,1999.
GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin. Scalingopen-vocabularyimagesegmentation
withimage-levellabels. InECCV,2022.
YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVin
VQAmatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering. InCVPR,
2017.
DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,and
JeffreyP.Bigham. VizWizgrandchallenge: Answeringvisualquestionsfromblindpeople. In
CVPR,2018.
KaimingHe,GeorgiaGkioxari,PiotrDolla¬¥r,andRossGirshick. MaskR-CNN. InICCV,2017.
EricHedlin,GopalSharma,ShwetaMahajan,HossamIsack,AbhishekKar,AndreaTagliasacchi,
andKwangMooYi. Unsupervisedsemanticcorrespondenceusingstablediffusion. InNeurIPS,
2023.
11Preprint
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,
2020.
Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-
strengthnaturallanguageprocessinginpython,2020. URLhttps://spacy.io/.
EdwardJ.Hu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen. LoRA:Low-rankadaptationoflargelanguagemodels. InICLR,2022.
DrewA.HudsonandChristopherD.Manning. GQA:Anewdatasetforreal-worldvisualreasoning
andcompositionalquestionanswering. InCVPR,2019.
HuggingFace. IntroducingIDEFICS:Anopenreproductionofstate-of-the-artvisuallanguagemodel,
2023. URLhttps://huggingface.co/blog/idefics.
DongshengJiang,YuchenLiu,SonglinLiu,XiaopengZhang,JinLi,HongkaiXiong,andQiTian.
FromCLIPtoDINO:Visualencodersshoutinmulti-modallargelanguagemodels. arXivpreprint
arXiv:2310.08825,2023.
TeroKarras,MiikaAittala,TimoAila,andSamuliLaine. Elucidatingthedesignspaceofdiffusion-
basedgenerativemodels. InNeurIPS,2022.
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla¬¥r, and Ross Girshick.
Segmentanything. InICCV,2023.
RanjayKrishna, YukeZhu, OliverGroth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen,YannisKalantidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,andLiFei-Fei.Visual
Genome: Connectinglanguageandvisionusingcrowdsourceddenseimageannotations. IJCV,
123:32‚Äì73,2017.
AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,JordiPont-Tuset,Shahab
Kamali,StefanPopov,MatteoMalloci,AlexanderKolesnikov,TomDuerig,andVittorioFerrari.
TheOpenImagesDatasetV4:Unifiedimageclassification,objectdetection,andvisualrelationship
detectionatscale. IJCV,128(7):1956‚Äì1981,2020.
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA:
Reasoningsegmentationvialargelanguagemodel. InCVPR,2024.
Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang,
Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: Stronger llms supercharge multimodal ca-
pabilities in the wild, May 2024a. URL https://llava-vl.github.io/blog/
2024-05-10-llava-next-stronger-llms/.
BoyiLi,KilianQ.Weinberger,SergeBelongie,VladlenKoltun,andReneRanftl. Language-driven
semanticsegmentation. InICLR,2022.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: Bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InICML,2023a.
JunyanLi,DelinChen,YiningHong,ZhenfangChen,PeihaoChen,YikangShen,andChuangGan.
CoVLM:Composingvisualentitiesandrelationshipsinlargelanguagemodelsviacommunicative
decoding. InICLR,2024b.
YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. InEMNLP,2023b.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,Piotr
Dolla¬¥r,andC.LawrenceZitnick. MicrosoftCOCO:Commonobjectsincontext. InECCV,2014.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,
2023.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. InCVPR,2024a.
12Preprint
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
LLaVA-NeXT:Improvedreasoning,OCR,andworldknowledge,January2024b. URLhttps:
//llava-vl.github.io/blog/2024-01-30-llava-next/.
YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. MMBench: Isyourmulti-modalmodel
anall-aroundplayer? InECCV,2024c.
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsfor
sciencequestionanswering. InNeurIPS,2022.
Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion
hyperfeatures: Searchingthroughtimeandspaceforsemanticcorrespondence. InNeurIPS,2023.
Meta. TheLlama3herdofmodels. arXivpreprintarXiv:2407.21783,2024.
AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. OCR-VQA:Visual
questionansweringbyreadingtextinimages. InICDAR,2019.
KoichiNamekata,AmirmojtabaSabour,SanjaFidler,andSeungWookKim. EmerDiff: Emerging
pixel-levelsemanticknowledgeindiffusionmodels. InICLR,2024.
AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels.
InICML,2021.
MaximeOquab,Timothe¬¥eDarcet,The¬¥oMoutakanni,HuyV.Vo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,MidoAssran,Nicolas
Ballas,WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,Michael
Rabbat,VasuSharma,GabrielSynnaeve,HuXu,HerveJegou,JulienMairal,PatrickLabatut,Ar-
mandJoulin,andPiotrBojanowski. DINOv2: Learningrobustvisualfeatureswithoutsupervision.
TMLR,2024.
VicenteOrdonez,GirishKulkarni,andTamaraBerg. Im2Text: Describingimagesusing1million
captionedphotographs. InNeurIPS,2011.
ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,QixiangYe,and
FuruWei. Kosmos-2: Groundingmultimodallargelanguagemodelstotheworld. InICLR,2024.
RenjiePi,JiahuiGao,ShizheDiao,RuiPan,HanzeDong,JipengZhang,LeweiYao,JianhuaHan,
HangXu,LingpengKong,andTongZhang. DetGPT:Detectwhatyouneedviareasoning. In
EMNLP,2023.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.
Learningtransferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
HanoonaRasheed, MuhammadMaaz, SahalShaji, AbdelrahmanShaker, SalmanKhan, Hisham
Cholakkal,RaoM.Anwer,EricXing,Ming-HsuanYang,andFahadS.Khan. GLaMM:Pixel
groundinglargemultimodalmodel. InCVPR,2024.
ZhongweiRen,ZhichengHuang,YunchaoWei,YaoZhao,DongmeiFu,JiashiFeng,andXiaojieJin.
PixelLM:Pixelreasoningwithlargemultimodalmodel. InCVPR,2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,
SrivatsaKundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev.
LAION-5B: An open large-scale dataset for training next generation image-text models. In
NeurIPS,2022.
13Preprint
ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,GangYu,XiangyuZhang,JingLi,andJian
Sun. Objects365: Alarge-scale,high-qualitydatasetforobjectdetection. InICCV,2019.
AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,
andMarcusRohrbach. TowardsVQAmodelsthatcanread. InCVPR,2019.
YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution.
InNeurIPS,2019.
YangSong,JaschaSohl-Dickstein,DiederikP.Kingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR,2021.
ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,ChunyuanLi,YikangShen,ChuangGan,
Liang-YanGui,Yu-XiongWang,YimingYang,KurtKeutzer,andTrevorDarrell. Aligninglarge
multimodalmodelswithfactuallyaugmentedRLHF. InACLFindings,2024.
LumingTang,MenglinJia,QianqianWang,ChengPerngPhoo,andBharathHariharan. Emergent
correspondencefromimagediffusion. InNeurIPS,2023.
ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharithaAkula,
JihanYang,ShushengYang,AdithyaIyer,XichenPan,AustinWang,RobFergus,YannLeCun,
andSainingXie. Cambrian-1:Afullyopen,vision-centricexplorationofmultimodalLLMs. arXiv
preprintarXiv:2406.16860,2024a.
ShengbangTong,ZhuangLiu,YuexiangZhai,YiMa,YannLeCun,andSainingXie. Eyeswide
shut? ExploringthevisualshortcomingsofmultimodalLLMs. InCVPR,2024b.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,≈Åukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
WenhaiWang,ZheChen,XiaokangChen,JiannanWu,XizhouZhu,GangZeng,PingLuo,Tong
Lu,JieZhou,YuQiao,andJifengDai. VisionLLM:Largelanguagemodelisalsoanopen-ended
decoderforvision-centrictasks. InNeurIPS,2023.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
AndrewM.Dai,andQuocV.Le. Finetunedlanguagemodelsarezero-shotlearners. InICLR,
2022.
SizeWu,ShengJin,WenweiZhang,LuminXu,WentaoLiu,WeiLi,andChenChangeLoy. F-LMM:
Groundingfrozenlargemultimodalmodels. arXivpreprintarXiv:2406.05821,2024a.
WenhaoWu,YizhongWang,GuangxuanXiao,HaoPeng,andYaoFu.Retrievalheadmechanistically
explainslong-contextfactuality. arXivpreprintarXiv:2404.15574,2024b.
JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,XiaolongWang,andShaliniDeMello. Open-
vocabularypanopticsegmentationwithtext-to-imagediffusionmodels. InCVPR,2023.
Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, and
CordeliaSchmid. Pixelalignedlanguagemodels. InCVPR,2024.
HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,
Shih-FuChang,andYinfeiYang. Ferret: Referandgroundanythinganywhereatanygranularity.
InICLR,2024.
LichengYu,PatrickPoirson,ShanYang,AlexanderC.Berg,andTamaraL.Berg. Modelingcontext
inreferringexpressions. InECCV,2016.
JunyiZhang,CharlesHerrmann,JunhwaHur,LuisaPolaniaCabrera,VarunJampani,DeqingSun,
andMing-HsuanYang. Ataleoftwofeatures: Stablediffusioncomplementsdinoforzero-shot
semanticcorrespondence. InNeurIPS,2023.
YichiZhang,ZiqiaoMa,XiaofengGao,SuhailaShakiah,QiaoziGao,andJoyceChai. GROUND-
HOG:Groundinglargelanguagemodelstoholisticsegmentation. InCVPR,2024.
14Preprint
Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. BuboGPT:
Enablingvisualgroundinginmulti-modalLLMs. arXivpreprintarXiv:2307.08581,2023.
BoleiZhou,AdityaKhosla,AgataLapedriza,AudeOliva,andAntonioTorralba. Objectdetectors
emergeindeepsceneCNNs. InICLR,2015.
BoleiZhou,AdityaKhosla,AgataLapedriza,AudeOliva,andAntonioTorralba. Learningdeep
featuresfordiscriminativelocalization. InCVPR,2016.
BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBarriuso,andAntonioTorralba. Scene
parsingthroughADE20Kdataset. InCVPR,2017.
ChongZhou,ChenChangeLoy,andBoDai. ExtractfreedenselabelsfromCLIP. InECCV,2022.
DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMohamedElhoseiny. MiniGPT-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. InICLR,2024.
15Preprint
A IMPLEMENTATION DETAILS
Inthissection,weprovidetheimplementationdetailsofthisworktoensurereproducibilityofour
experiments.
Attend-and-Segment. Wefirstcollecttheattentionmapsforthevisualtokens,andaggregatethe
attentionmapsbyaveragingoveralllayersandheads. Thenweapplynormalizationacrosstheoutput
tokensequencetoremovenoisypoints,andupsamplethenormalizedattentionmaptotheoriginal
imageresolution. Duringmaskrefinement,wefindthecoordinatewherethenormalizedattention
value is maximized, and use it as a prompt to the ViT-H SAM model (Kirillov et al., 2023) for
producingthepixel-levelsegmentationmap. Intheinstancesegmentationandgroundedconversation
generation tasks, we parse the model response into noun phrases using spaCy (Honnibal et al.,
2020),andforeachphrase,weaveragethenormalizedattentionmapsforthetokensthatcompose
thecentralnounofthephrase.
DIFFLMM.OurdevelopmentofDIFFLMMisbasedonthecodebaseanddatasetofLLaVA-1.5(Liu
et al., 2024a). We employ the Stable Diffusion v1.5 (Rombach et al., 2022) model as our visual
backbone. Inthedenoisingstep,weaddarandomnoiseatthe100timestep,andextractfeatures
from the second upsampling block, following the practice of DIFT (Tang et al., 2023). We also
provideadditionalablationstudyonthechoiceofthenoiselevelandfeatureblockinTable6. Inthe
implicitcaptioner(Xuetal.,2023),weemploythevisualencoderofCLIP-ViT-L-336px(Radford
etal.,2021),whichisthesameCLIPmodelintheoriginalLLaVA-1.5. Themodelistrainedwith
LoRA(Huetal.,2022),andthesametrainingrecipeasLLaVA-1.5. Thetrainingdataarealsothe
sameasLLaVA-1.5. Theincludeddatasetsandtheirlicensesarelistedbelow.
‚Ä¢ LAION(Schuhmannetal.,2022): MITLicense.
‚Ä¢ CC(Changpinyoetal.,2021): ‚ÄúThedatasetmaybefreelyusedforanypurpose,although
acknowledgementofGoogleLLC(‚ÄúGoogle‚Äù)asthedatasourcewouldbeappreciated.‚Äù
‚Ä¢ SBU(Ordonezetal.,2011): Unknownlicense.
‚Ä¢ MS-COCO(Linetal.,2014): CreativeCommonsAttribution4.0InternationalLicense.
‚Ä¢ GQA(Hudson&Manning,2019): CreativeCommonsAttribution4.0InternationalLicense.
‚Ä¢ OCR-VQA(Mishraetal.,2019): Unknownlicense.
‚Ä¢ TextVQA(Singhetal.,2019): CreativeCommonsAttribution4.0InternationalLicense.
‚Ä¢ VisualGenome (Krishna et al., 2017): Creative Commons Attribution 4.0 International
License.
Table 6: Ablation study on diffusion feature extraction. Adding a relatively small noise (at
diffusionstep100or200)totheoriginalimageandextractingfeaturesfromthesecondupsampling
blockinthediffusionU-NetleadtothebestresultsinDIFFLMM.
NoiseStep FeatureBlock Pre-trainLoss‚Üì
100 2 2.384
0 2.417
200 2 2.395
300 2.457
1 2.400
100 3 2.465
4 2.625
B ADDITIONAL ANALYSIS OF ATTENTION MAPS
In attend-and-segment, we aggregate the attention values between each generated token and the
visualtokensintoa2Dmap. Inthissection,weprovidemorein-depthanalysisoftheattentionmaps.
16Preprint
Forvisualization,weusethesame‚Äúcatanddog‚Äùimage(Figure1)asanexampleinthefollowing
analysis;wehavesimilarobservationsonotherimagesaswell.
Attentionineachheadandlayer. Insteadofaveragingtheattentionvaluesovern layersand
layer
n headsperlayerintheLLM,wefirstinspecttheindividualattentionvaluesineachheadand
head
layer. Figure5visualizestheattentionbetweenonegeneratedtoken‚Äúcat‚Äùandtheinputvisualtokens.
Consistentwithsomerecentobservations(Wuetal.,2024b),afewheadsintheintermediatelayers
show stronger activation with respect to the visual object in the image. Also, attention maps in
intermediatelayersaremorelocalized. However,itisinfeasibletobuilddirectconnectionsbetween
attentionheadsandvisualconcepts,giventheabsenceofgroundingannotations.
Table7summarizesanempiricalstudythatdemonstratesthegroundingresultsofusingtheattention
from one single head of one single layer. Compared with averaging over all heads and layers,
individualheadsandlayersleadtotosignificantlyworseandnoisierresults. Therefore,weaggregate
theattentionmapsacrossallheadsandlayersbyaveraging,whichalsosimplifiesthealgorithmof
attend-and-segment inoursettingwithoutgroundingsupervision.
Table 7: Evaluation of attention maps from individual head/layer combinations. Applying
attend-and-segment ontheattentionmapsextractedfromindividualheadsandlayersresultsinworse
andlessstablegroundingmaskrecallinGCG,ascomparedwithapplyingattend-and-segment onthe
meanattentionmapsaggregatedoverallheadsandlayers,whichachieves46.6maskrecall(Table2).
HeadIndex
avg.¬±std.
1 9 17 25
1 19.2 13.6 19.9 11.9 16.2¬±3.5
Layer 9 7.5 25.1 9.0 28.2 17.5¬±9.3
Index 17 26.2 4.3 19.7 27.1 19.3¬±9.1
25 5.9 34.3 27.0 15.7 20.7¬±10.8
Overall 18.4¬±8.8
Attentionnormalization. Afterreducingtheattentionmapsintoone2Dmapforeachgenerated
token, we observe some noisy patterns in the attention maps (Figure 6-top). Some seemingly
uninformativevisualtokens(usuallyinthebackground)attractsmoreattentionfromthegenerated
tokenthanothervisualtokens. Arecentwork(Darcetetal.,2024)showssimilarobservations,and
explainsthatsuchlessinformativetokensare‚Äúrepurposedforinternalcomputations.‚Äù Toremove
suchartifacts,theyproposetoprovideadditionaltokenstotheVisionTransformerasregisters.
However,inoursetting,wecannotretrainthevisualbackboneorthelanguagemodelduetolimited
dataandcomputes.Instead,wesimplynormalizetheattentionmapsbysubtractingthemeanattention
mapaveragedovertheoutputsequence(Section3.2). Althoughthenoisyattentionpatternsexist,we
observethatthesepatternsarerelativelystable(Figure6-top),sothemeanattentionmap,aggregated
overtheoutputsequence,cancapturetheundesiredattentionpatternsandallowustoremovethem.
Aftertheattentionnormalization,weobserveclearerpatterns(Figure6-bottom)whichleadstoaccu-
ratepixelgrounding.Quantitatively,attentionnormalizationimprovestheGCGmaskrecallfrom44.1
to46.6(Table4). Inadditiontonounphrases,otherwordsrevealrelationsorcomparisonsbetween
visualentities,andcouldbehelpfulformorevision-languagetasks. Weleavethisinvestigationfor
futureresearch.
C ADDITIONAL QUALITATIVE RESULTS
WepresentadditionalqualitativeresultsforthegroundedconversationgenerationtaskinFigure7.
TheDIFFLMMmodelisaskedto‚ÄúDescribe the image in detail.‚Äù Thenweuseattend-
and-segment toproducevisualgrounding. Overall,ourapproachcanprovideaccuratesegmentation
masks,butmayalsosufferfromcommonissuesofLMMs(e.g.,objecthallucination(Lietal.,2023b;
Sunetal.,2024)).
17Preprint
Head 0 Head 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8 Head 9 Head 10 Head 11 Head 12 Head 13 Head 14 Head 15 Head 16 Head 17 Head 18 Head 19 Head 20 Head 21 Head 22 Head 23 Head 24 Head 25 Head 26 Head 27 Head 28 Head 29 Head 30 Head 31
Figure5: Attentionbetweenthevisualtokensandthegeneratedtoken‚Äúcat.‚Äù Weobservecertain
headsintheintermediatelayersproducemorelocalizedattentionmapswithrespecttothe‚Äúcat‚Äùobject
intheimage(e.g.,Head14ofLayer15). Itremainschallengingtodirectlyrelateindividualheads
tovisualconceptswhengroundingannotationsarenotavailable, soattend-and-segment directly
aggregatesattentionmapsfromalllayersandheadsbyaveragingthem.
18
0 reyaL
1 reyaL
2 reyaL
3 reyaL
4 reyaL
5 reyaL
6 reyaL
7 reyaL
8 reyaL
9 reyaL
01 reyaL
11 reyaL
21 reyaL
31 reyaL
41 reyaL
51 reyaL
61 reyaL
71 reyaL
81 reyaL
91 reyaL
02 reyaL
12 reyaL
22 reyaL
32 reyaL
42 reyaL
52 reyaL
62 reyaL
72 reyaL
82 reyaL
92 reyaL
03 reyaL
13 reyaLPreprint
"The" "image" "features" "a" "dog" "and" "a" "cat" "standing" "next"
"to" "each" "other" "," "pos" "ing" "for" "a" "picture" "."
"The" "dog" "is" "on" "the" "right" "side" "of" "the" "image"
"," "while" "the" "cat" "is" "on" "the" "left" "." "Both"
"The" "image" "features" "a" "dog" "and" "a" "cat" "standing" "next"
"to" "each" "other" "," "pos" "ing" "for" "a" "picture" "."
"The" "dog" "is" "on" "the" "right" "side" "of" "the" "image"
"," "while" "the" "cat" "is" "on" "the" "left" "." "Both"
Figure6:Attentionmapsbeforeandafterthenormalization. Top:Beforethenormalization,afew
uninformativevisualtokensinthebackground(e.g.,top-centertokensabovethedog‚Äôshead)receive
more attention, which is consistent with recent observations (Darcet et al., 2024). Such patterns
arestableacrosstheoutputsequence. Bottom: Toremovesuchartifactsintheattentionmaps,we
subtractthemeanattentionmap(Section3.2). Afterthenormalization, theattentionmapsshow
clearerlocalization,andaresuitableforpixel-levelgrounding. Inadditiontonounphrases,other
partsofthetextresponsedemonstratemeaningfulvisualcorrespondence(e.g.,‚Äúnexttoeachother‚Äù
andthespacebetweenthetwoanimals).
19Preprint
The image features a cozy bedroom with a large bed situated in the center of the room. The bed is covered with a blue comforter,
and it is positioned next to a window that allows natural light to enter the room. The room also has a dresser and a mirror, adding
to the overall comfort and functionality of the space.
In addition to the bed, there are several books scattered throughout the room, indicating that the occupant may be an avid reader.
A potted plant is placed near the window, adding a touch of greenery to the room. A vase can also be seen on the dresser, further
enhancing the room's decor.
The image captures a tennis match in progress, with a man in a white shirt and white shorts playing on a blue tennis court. He is in
the middle of a swing, holding a tennis racket and preparing to hit the ball. The tennis ball is visible in the air, close to the player.
There are several other people in the scene, likely spectators or fellow players. Some of them are standing near the edges of the
court, while others are positioned further away. A chair can be seen on the side of the court, possibly for resting or observing the
match.
The image features a black cat sitting in front of a computer monitor, which is displaying a webpage. The cat appears to be looking
at the screen, possibly intrigued by the content. The computer setup includes a keyboard placed to the left of the monitor and a
cell phone on the right side.
In addition to the cat and the computer setup, there is a person visible in the background, likely the owner of the cat or someone
working in the same space.
Figure7: Qualitativeresultsforgroundedconversationgeneration. Forvisualclarity,weonly
displaythebestfournon-overlappingsegmentationmasksperimage.
20