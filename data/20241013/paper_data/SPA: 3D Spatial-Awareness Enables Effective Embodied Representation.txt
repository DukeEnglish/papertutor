Preprint
SPA: 3D SPATIAL-AWARENESS ENABLES
EFFECTIVE EMBODIED REPRESENTATION
HaoyiZhu1,2,HonghuiYang1,3,YatingWang1,4,JiangeYang1,5,LiminWang1,5,TongHe1â€ 
1ShanghaiAILab,2USTC,3ZJU,4Tongji,5NJU
â€ CorrespondingAuthor
ABSTRACT
Inthispaper,weintroduceSPA,anovelrepresentationlearningframeworkthat
emphasizes the importance of 3D spatial awareness in embodied AI. Our ap-
proachleveragesdifferentiableneuralrenderingonmulti-viewimagestoendowa
vanillaVisionTransformer(ViT)withintrinsicspatialunderstanding. Wepresent
themostcomprehensiveevaluationofembodiedrepresentationlearningtodate,
covering268tasksacross8simulatorswithdiversepoliciesinbothsingle-task
andlanguage-conditionedmulti-taskscenarios. Theresultsarecompelling: SPA
consistently outperforms more than 10 state-of-the-art representation methods,
includingthosespecificallydesignedforembodiedAI,vision-centrictasks,and
multi-modal applications, while using less training data. Furthermore, we con-
duct a series of real-world experiments to confirm its effectiveness in practical
scenarios. These results highlight the critical role of 3D spatial awareness for
embodied representation learning. Our strongest model takes more than 6000
GPUhourstotrainandwearecommittedtoopen-sourcingallcodeandmodel
weightstofosterfutureresearchinembodiedrepresentationlearning. ProjectPage:
https://haoyizhu.github.io/spa/.
1 INTRODUCTION
Visionsystemshavemaderemarkableprogressinunderstanding2Dimages(Heetal.,2020;Chen
etal.,2020a;Heetal.,2022;Feichtenhoferetal.,2022;Tongetal.,2022;Yangetal.,2023;Oquab
etal.,2023;Radfordetal.,2021;Fangetal.,2023b;Chenetal.,2024). However,achievingtrue
visualintelligencenecessitatesacomprehensiveunderstandingofthe3Dworld. Thisiscrucialfor
embodiedAI,whereagentsmustperceive,reason,andinteractwithcomplex3Denvironments.
ExistingvisualrepresentationlearningmethodsforembodiedAI(Nairetal.,2022;Radosavovic
etal.,2023;Majumdaretal.,2023;Karamchetietal.,2023;Shangetal.,2024;Yangetal.,2024b)
largelyrelyonparadigmsfrom2Dvision,predominantlyemployingcontrastive-basedormasked
autoencoder (MAE)-based approaches. However, they often struggle to fully capture the spatial
relationships and 3D structures inherent in the physical world. This limitation arises from their
primaryemphasison2Dsemanticunderstanding, which, thoughvaluable, isstillinsufficientfor
thesophisticatedspatialreasoningrequiredinembodiedAItasks,whereagentsneedtonavigate
environments,manipulateobjects,andmakedecisionsusingtheir3Dspatialawareness.
Inthispaper,weintroduceSPA,ageneral3Dspatial-awarerepresentationlearningframeworkfor
embodiedAI.SPAleveragesneuralrendering(Mildenhalletal.,2021)asthepre-trainingpre-text
taskonmulti-viewimages. Unlikeexplicit3Drepresentationslikepointcloudsormeshesâ€”which
priorwork(Wangetal.,2024b;a;Zeetal.,2024;Zhuetal.,2024)hasshowntooutperformpure
2Dinputsinrobotlearningâ€”multi-viewimagesareeasiertoprocessandmorereadilyavailable,
makingthemidealforlarge-scaletraining,suchasfrominternetvideos. Specifically,givenavanilla
2D image backbone, e.g. a Vision Transformer (ViT) (Dosovitskiy et al., 2021), we first extract
multi-viewfeaturemapsfromtheinputimages. Usingknowncameraposes,wethenconstructa
featurevolumefromthesefeaturemapsandsampleraystoapplydifferentiableneuralrendering.
Thisprocessgeneratesmulti-viewRGB-Dimagesandsemanticmapsforsupervisionwithoutlabels,
enablingthepre-trainingofa2Dimagebackbonetoenhance3Dspatialawareness.
1
4202
tcO
01
]VC.sc[
1v80280.0142:viXraPreprint
SPA Othersâ€™ Best Othersâ€™ 2nd Best
VC-1 VC-1
Franka-Kitchen Adroit MetaV -WC o-1
rld
Franka-Kitchen Adroit MetaV -WC o-1
rld
LIBERO-90
DM
CVC-1
LIBERO-90
DM
CVC-1
LIBERO-10 Tri-FingerV
C
-1
LIBERO-10 Tri-FingerV
C
-1
LIBERO-G
oal
RLBench
Group
1
LIBERO-G
oal
RLBench
Group
1
LIBERO-SpatialLIBERO-Object
Meta-World
RLB Gre onc uh p 2
LIBERO-SpatialLIBERO-Object
Meta-World
RLB Gre onc uh p 2
(a) Mean Rank (b) Mean Success Rate
(c) Rank Distributions
Rank 1 Rank 2 Rank 3 Rank â‰¥ 4
14.6% 20.6% 19.9% 19.5%
30.3%
35.2% 10.5%
SPA 6.7% DINOv2 68.2% 10.5 8% .2%MoCo v3 60.7% 11.2% MAE 53.2% 8 6.6 .7% % CLIP 65.2%
19.5% 15.0% 15.7%
19.9% 14.2% 11.8% 22.5% 23.2%
6.7% 8.3%
11.2% EVA 58.4% 4.5% In 3te 0r 0n MVL 8.3% Inte 6r BnVL 7.1% MVP 56.9% 11.2% VC-1 53.9%
10.5% 74.5% 71.5% 13.5%
11.6%
Figure1: Performancecomparisonacrossrepresentations. Above: (a)Meanrankand(b)mean
successrateonbenchmarks. LinesrepresenttheperformanceofSPA,best,andsecondbestperfor-
manceoneachbenchmark. Bottom: Rankdistributionsfor268individualtasks,showingproportions
fromrank1torankâ‰¥4counterclockwise. Ourmodeldemonstratessuperioroverallperformance.
Tothoroughlyvalidateourassumptionandmethod,wecollect268embodiedtasksacross8simulators
using various policymethods. To ourknowledge, thisrepresents the largestscale ofembodied
evaluationtodate. Previouswork, suchasR3M(Nairetal.,2022)andVC-1(Majumdaretal.,
2023),evaluatedfewerthan20tasks,potentiallyleadingtoincompleteorbiasedconclusions. Our
evaluationspansbothsingle-taskandlanguage-conditionedmulti-tasklearning. Wecompareover10
state-of-the-artrepresentationlearningmethods,categorizedasembodied-specific(Nairetal.,2022;
Majumdaretal.,2023;Radosavovicetal.,2023), vision-centric(Oquabetal.,2023;Chenetal.,
2021;Heetal.,2022),andmulti-modal(Radfordetal.,2021;Fangetal.,2023b;Chenetal.,2024).
Ourmethodconsistentlyoutperformsothers,underscoringtheimportanceof3Dspatialawareness
forembodiedAI.Notably,multi-modalmodelslikeCLIP(Radfordetal.,2021),consistentlyperform
poorly. Thisholdseventhevision-languagemodelscalestheViTto6Bparameters(Chenetal.,
2024). Throughacameraposeestimationtaskandfeaturemapvisualization,wedemonstratethat
SPA has learned superior 3D spatial understanding. Further, we find that 3D awareness shows a
positivecorrelationwithembodiedperformance. Finally,weconductseveralreal-worldtasks,where
SPAalsodemonstratessuperiorperformance. Ourcontributioncanbesummarizedasfollows.
â€¢ Weproposeasignificantspatialhypothesis: 3Dspatialawarenessiscrucialforembodied
representationlearning. Ourexperimentsprovideclearevidenceforthehypothesis.
â€¢ WeintroduceSPA,anovelparadigmforrepresentationlearninginembodiedAI.Itenhances
avanillaVisionTransformer(ViT)with3Dawarenessusingdifferentiableneuralrendering
asthepre-texttaskonmulti-viewimages.
â€¢ Weconductthelargestevaluationbenchmarkforembodiedrepresentationlearning,signifi-
cantlylargerthanpreviousstudies. Itinvolves268tasks,8simulators,andover10SOTA
methodswithdiversedownstreampoliciesandtasksettings.
2Preprint
â€¢ Throughextensiveexperimentsinbothsimulatorsandreal-worldsettings,SPAoutperforms
morethan10SOTArepresentationlearningmethods,demonstratingitseffectiveness.
2 METHODOLOGY
In this section, we first describe our process for handling multi-view image inputs and feature
extractioninSec.2.1. Subsequently,weconstructanexplicitfeaturevolumefromthesemulti-view
features,detailedinSec.2.2. Finally,weexplaintheimagerenderingfromthefeaturevolumeand
lossfunctionsfornetworkoptimizationinSec.2.3andSec.2.4. OurpipelineisvisualizedinFig.2.
2.1 INPUTPROCESSANDFEATUREEXTRACTION
Givenasetofmulti-viewimagesI = {I ,I ,...,I },whereeachI âˆˆ R3Ã—HÃ—W andN âˆˆ Z+,
1 2 N i
weutilizea2DimagebackboneF,suchasaViT.TheimagesareprocessedseparatelythroughF,
yieldinglatentfeaturesL={l ,l ,...,l },whereeachl =F(I )âˆˆRLÃ—C. FollowingMAE,we
1 2 N i i
applyrandommaskingtoinputimagestoenhancerobustness,butwithoutaViTdecoderandMAEâ€™s
pixelreconstructionobjective. Foreachl ,maskedpositionsarefilledwithamasktoken,andwe
i
concatenatetheglobalclasstokenwithotherpatchtokensasread-outtokenssimilartoDPT(Ranftl
etal.,2020). Wethenunpatchifythemtoobtainalatentfeaturemapofsize H Ã— W,whereP isthe
P P
ViTpatchsize. Finally,twosimpleupsamplinglayerstransformthisintoafeaturemapM matching
i
theinputresolution. Eachupsamplinglayerincludesaconvolution,aGELU(Hendrycks&Gimpel,
âˆš
2016)activation,andapixelshufflelayer(Shietal.,2016)withanupscalefactorof P.
2.2 DYNAMICVOLUMECONSTRUCTION
Toenablemulti-viewinteraction,weconstructa3Dfeaturevolumefrommulti-viewfeaturemaps,
M. Unlikethebirdâ€™s-eyeview(BEV)constructioninautonomousdriving(Lietal.,2022),which
usuallyreliesonafixedscenerangearoundegovehicle,ourmethoddynamicallyadjuststhescene
rangebasedonthespatialextentsoftheenvironmenttoaccommodatevaryingdatasets. Specifically,
thesceneâ€™sboundsarefirstestimatedusingavailabledepthdata,sparsepoints,orpre-definedrules.
WethenpartitionthesceneintoavolumeofsizeXÃ—Y Ã—Z,withvoxelsizedynamicallyadjusted
tocaptureeitherfineobjectdetailsorlargerenvironments. Voxelfeatures,VËœ,areinitializedwith
learnablepositionalembeddings. Eachvoxelisprojectedontothemulti-viewfeaturemapsusing
theknowntransformationmatrixT. Deformableattention(Zhuetal.,2021)isthenapplied,where
the multi-view features act as keys and values, and the voxel features as queries. Finally, a 3D
convolutionrefinestheoutputvolumefeaturestoobtainV. Theprocesscanbeformulatedas:
V =Conv3D(DeformAttn(VËœ,M,T)). (1)
2.3 DIFFERENTIABLEVOLUMETRICRENDERING
Afterconstructingthefeaturevolume,weemploydifferentiableneuralrendering(Mildenhalletal.,
2021)toconnect2Dand3Ddomains. Forbettergeometryrepresentation,weutilizetheimplicit
signeddistancefunction(SDF)fieldmodelingasinNeuS(Wangetal.,2021). TheSDFrepresents
the3Ddistancefromaquerypointtothenearestsurface,implicitlycapturingthe3Dgeometry.
Given a feature volume V, we apply a shallow 3D CNN Ï• to directly produce three outputs: an
SDFfeaturevolumeS âˆˆRXÃ—YÃ—Z,asphericalharmonic(SH)(Yuetal.,2021;Zhuetal.,2023a)
coefficientfieldKâˆˆRDÃ—XÃ—YÃ—Z (whereD =3Â·(l +1)2)forcolorrendering,andasemantic
max
featurevolumeF âˆˆRCsemanticÃ—XÃ—YÃ—Z:
S âˆˆRXÃ—YÃ—Z, KâˆˆRDÃ—XÃ—YÃ—Z, F âˆˆRCsemanticÃ—XÃ—YÃ—Z =Ï•(V). (2)
Unlikepriorwork(Huangetal.,2023;Zhuetal.,2023b;Yangetal.,2024a),whichemploysanMLP
tocomputetheattributesofeachsampledpointindividually,wedirectlyapplya3DCNNtoV. This
eliminatestheneedforpointwiseMLPcomputations,reducingredundantprocessingandenabling
moreefficientexecution. Consequently,ourapproachleadstosubstantialimprovementsinbothtime
andmemoryefficiency,especiallywhensamplingalargenumberofpointsduringrendering.
3Preprint
Multi-View Masked Multi-View
Volume Construction Volumetric Rendering
Input Images Patches Feature Maps
Rays
Rendered Depth Images
SDF Values
SH Coefficients Rendered RGB Images
Semantic
Features
Rendered Semantic Map
Figure2: PipelineOverview. Givenmulti-viewimages,werandomlymaskpatchesandinputthe
remainingintoaVisionTransformer. Theupsampledlatentfeaturesgeneratemulti-viewfeature
maps,fromwhichweconstructafeaturevolumetoderiveSDFvalues,SHcoefficients,andsemantic
features. Wethenrenderdepth,RGB,andsemanticmapsforlosscomputation.
Torendera2Dpixeli,wesampleN raypoints{p =o+t d |j =1,...,N, t <t }from
j j i j j+1
rayr ,whereoisthecameraoriginandd istheviewingdirection. Attributesforeachpointare
i i
obtainedviatrilinearsampling:
s =Ï„(S,p ), k =Ï„(K,p ), f =Ï„(F,p ). (3)
j j j j j j
TheSHvectork = (km) , wherekm âˆˆ R3, isusedtocomputeview-dependent
j l 0â‰¤lâ‰¤lmax,âˆ’lâ‰¤mâ‰¤l l
colorscË† byqueryingtheSHbasisfunctionsYm :S2 â†’Rbasedontheviewingdirectiond :
j l j
(cid:32) (cid:88)lmax (cid:88)l (cid:33)
cË† =Sigmoid kmYm(d ) . (4)
j l l j
l=0m=âˆ’l
FollowingtheformulationinNeuS(Wangetal.,2021),theRGBcolorCË† ,depthDË† ,andsemantic
i i
featureFË† forpixeliarecomputedbyintegratingthepredictedvaluesalongtheray:
i
N N N
CË† =(cid:88) w cË† , DË† =(cid:88) w t , FË† =(cid:88) w Ë†f , (5)
i j j i j j i j j
j=1 j=1 j=1
where w = T Î± is the occlusion-aware weight, with T =
(cid:81)jâˆ’1(1
âˆ’ Î± ) representing the
j j j j k=1 k
accumulatedtransmittanceandÎ± beingtheopacityvalue. Specifically,Î± iscomputedas:
j j
(cid:18) (cid:19)
Ïƒ (s )âˆ’Ïƒ (s )
Î± =max s j s j+1 ,0 , (6)
j Ïƒ (s )
s j
whereÏƒ (x)=(1+eâˆ’sx)âˆ’1isthesigmoidfunctionmodulatedbyalearnableparameters.
s
2.4 LOSSFUNCTIONS
Duringpre-training,werandomlysampleK pixelsfrommulti-viewinputsineachiteration. The
renderinglossiscalculatedbasedonthedifferencesbetweentheinputpixelvaluesandthepredicted
values. Forthesemanticfeaturemap,weusethefeaturemapfromAM-RADIO(Ranzingeretal.,
2024)assupervision. Ourframeworkhasthecapabilitytodistillknowledgefrommultiplevision
foundationmodelsbyaddingmultiplerenderingheads. However,thispaperdoesnotexplorethat
approach,asitisnottheprimaryfocus. Therenderinglossisexpressedas:
K
L =
1 (cid:88)(cid:16)
Î» Â·âˆ¥C âˆ’CË† âˆ¥+Î» Â·âˆ¥D âˆ’DË† âˆ¥+Î» Â·âˆ¥F âˆ’FË†
âˆ¥(cid:17)
. (7)
render K color i i depth i i semantic i i
i=1
Additionally,weincorporatetheEikonalregularizationlossL ,near-surfaceSDFsupervision
eikonal
lossL ,andfreespaceSDFlossL ,whicharestandardinneuralsurfacereconstruction. Detailed
sdf free
definitionsoftheselossesareprovidedinAppendixA.Thetotallossisdefinedas:
L =L +Î» Â·L +Î» Â·L +Î» Â·L . (8)
total render eikonal eikonal sdf sdf free free
4
Vision
Transformer
UpsamplerPreprint
Large Scale
268 Tasks
8 Simulators
Diff. Settings
Single Task
Lang.-Cond. Multi-Task
Diverse Policies
MLP / RVT /
Transformer / Diffusion
Various SOTAs
Vision-Centric
Embodied-Specific Largest-Scale Embodied Representation Evaluation
Multi-Modal
Figure3:Overviewofourlarge-scaleembodiedevaluation.Weconductthelargest-scaleevaluation
ofembodiedrepresentationlearningtodate. Ourstudyencompasses268tasksacross8simulators,
includingbothsingle-taskandlanguage-conditionedmulti-tasksettings. Weevaluatediversepolicy
architecturesandassessvariousstate-of-the-artrepresentationmethods. Thisthoroughevaluation
allowsustoprovideacomprehensiveandunbiasedanalysisofdifferentrepresentations.
3 LARGE-SCALE EMBODIED EVALUATION
UnliketheCVorNLPcommunities,wherelarge-scalebenchmarksarecommon,embodiedrepresen-
tationshavenotbeenthoroughlyassessed. Thelargestpreviousevaluation,VC-1(Majumdaretal.,
2023),includesonly17tasks. Thismayleadtorandomnessandbias. Therefore,wehavecreated
thelargestembodiedevaluationtodate,encompassing268tasksacross8simulatorsâ€”over15
timeslargerthanVC-1â€™sevaluation. Additionally,unlikepreviousapproaches(Majumdaretal.,
2023;Nairetal.,2022;Radosavovicetal.,2023)thatusedasmallMLPpolicyundersingle-task
settings,ourevaluationspansmultiplepolicytypes(e.g.MLP,diffusion,transformer)andincludes
bothsingle-taskandlanguage-conditionedmulti-tasksettings. Thisunprecedentedscaleanddiversity
ensurerobustandconvincingconclusions. Duringallevaluations,weadheretostandardpractices
byfreezingthepre-trainedrepresentationmodel. Ourdetailedevaluationsettingscanbefoundin
AppendixB.TheoverviewofourevaluationisshowninFig.3.
Wehaveincluded3single-taskbenchmarks:
1) VC-1 (Majumdar et al., 2023) involves 4 selected simulators with 14 tasks in total: Adroit
(AD)(Kumar,2016),Meta-World(MW)(Yuetal.,2020),DMControl(DMC)(Tunyasuvunakool
etal.,2020),andTriFinger(TF)(WÃ¼thrichetal.,2020). Weusea3-layerMLPasthepolicynetwork.
2) Franka Kitchen (Gupta et al., 2019) involves 5 selected tasks. Each task spans two camera
viewpointsandthreerandomseeds. Weutilize25demonstrationstotraina2-layerMLPpolicy.
3)Meta-World(Yuetal.,2020)involves48selectedtasksofvaryingdifficulty. Weimplementedthe
DiffusionPolicy(Chietal.,2023)onthisbenchmarkandadheredtothesetupinZeetal.(2024)to
generate10demonstrationsforeachsingle-tasktraining,followedbyevaluationthrough20rollouts.
Wehavealsoincluded2language-conditionedmulti-taskbenchmarks:
1)RLBench(Jamesetal.,2020)features71selectedtasksthatcanbesuccessfullyexecuted. We
dividethetasksintotwogroupsaccordingtotheircategorydefinedbyPolarNet(Chenetal.,2023).
WeemployRVT-2(Goyaletal.,2024),theSOTAmethodonthisbenchmark,asourpolicy.
2)LIBERO(Liuetal.,2024)comprises130tasksacross5suites:LIBERO-Spatial,LIBERO-Object,
LIBERO-Goal,LIBERO-10,andLIBERO-90. Wetrainalanguage-conditionedtransformerpolicy
providedbytheoriginalLIBEROoneachsuitewithonly20demonstrationspertask.
4 TRAINING AND IMPLEMENTATION DETAILS
Inthissection,wepresentthestep-by-stepimplementationandtrainingofourSPAmodel. Wefirst
pre-trainaViT-base(ViT-B)backboneusingasmalldatasetandevaluatethismodelontheVC-1
benchmarktoexaminetheeffectsofhyperparameters(Sec.4.1). Wethencompileseveralmulti-view
datasets,trainingViT-Bmodelsoneachtoassesstheimpactofdifferentdatasets(Sec.4.2). Finally,
weintegrateallfactorsandscaleupbothdataandmodelsizetotrainthestrongestversionofSPA
usingaViT-large(ViT-L)backbone(Sec.4.3). MoredetailscanbefoundinAppendixC.
5Preprint
4.1 HYPERPARAMETERINVESTIGATION Table1: Influenceofmaskratioandlosscompo-
nents. C.,D.,andS.denotethecolor,depth,and
WeconducthyperparametertuningwithaViT-B semanticlosscomponents,respectively.
modelonScanNet(Daietal.,2017),andeval-
Mask Loss VC-1Benchmark Mean
uateitonVC-1benchmarks,asshowninTab.1.
Ratio C. D. S. AD MW DMC TF S.R.
1)MaskRatio. Ourresultsindicatethatamask
0.00 âœ“ âœ“ âœ“ 53.3Â±4.6 88.5Â±5.7 57.5Â±2.6 74.1Â±0.6 70.36
ratioof0.5isthemosteffective. 2)LossCom- 0.25 âœ“ âœ“ âœ“ 52.7Â±3.1 89.6Â±4.5 57.6Â±3.0 70.4Â±1.7 70.17
ponents.AsdiscussedinSec.2.4,ourrendering 0.50 âœ“ âœ“ âœ“ 53.3Â±4.2 88.8Â±1.6 60.1Â±3.1 72.6Â±0.7 71.18
lossconsistsofcolor,depth,andsemanticcom- 0.75 âœ“ âœ“ âœ“ 51.3Â±1.2 88.0Â±3.5 61.1Â±3.5 73.0Â±0.8 71.01
0.95 âœ“ âœ“ âœ“ 51.3Â±1.2 85.6Â±4.0 62.5Â±5.3 73.1Â±0.2 70.67
ponents. We sequentially deactivate each and 0.50 âœ“ âœ— âœ“ 51.3Â±1.2 90.9Â±3.3 58.8Â±5.6 71.5Â±1.0 71.01
findthatallthreearevaluable. However,deac- 0.50 âœ— âœ“ âœ“ 52.0Â±2.0 89.3Â±3.3 53.9Â±4.3 70.9Â±1.3 68.71
tivatingthesemanticlosshastheleastimpact. 0.50 âœ“ âœ“ âœ— 52.7Â±3.1 88.0Â±4.5 61.5Â±3.4 71.6Â±1.2 71.16
4.2 DATASETINVESTIGATION
We collect several multi-view datasets. To investigate their effectiveness in SPA representation
learning, we train a ViT-B model on one or two of the datasets, keeping the total training steps
constant,andassessperformanceontheVC-1benchmarks. Forsimplicity,semanticrenderingis
disabled. ThedatasetsinvestigatedarelistedinthefirstcolumnofTab.2. Mostdatasetsprovide
ground-truthdepth,whichweuseforsupervision. Asourfindingsaboverevealthatdepthsupervision
ishelpful,fordatasetslackingground-truthdepth,weemployadepthestimationmodel. Forinstance,
Droid(Khazatskyetal.,2024)onlyoffersbinocularimages,soweapplyCroCo-Stereo(Weinzaepfel
et al., 2023) for dense depth estimation. Additionally, due to inaccurate camera poses in Droid,
wetreatitsdataassingle-viewinputs. TheresultsarepresentedinTab.2,withfurtherdetailsin
Appendix C. Our analysis reveals that some datasets can be detrimental. For example, although
RH20T(Fangetal.,2023a)isalarge-scaleroboticdataset,itslackofvisualdiversityâ€”stemming
fromdatacollectedinthesamelabâ€”negativelyimpactsrepresentationlearning.
Table2:Influenceofdifferentdatasets. WepresenttheperformanceresultsontheVC-1benchmark.
MeanS.R.referstothemeansuccessrateacrossallindividualtasks.
Mean
Datasets AD MW DMC TF
S.R.
ScanNet(Daietal.,2017) 52.67Â±4.11 90.93Â±3.22 65.11Â±1.31 70.75Â±1.08 73.68
ScanNet++(Yeshwanthetal.,2023) 56.00Â±2.83 89.87Â±4.20 62.24Â±4.51 71.28Â±0.38 72.51
Arkitscenes(Baruchetal.,2021) 50.67Â±5.73 89.87Â±4.59 60.51Â±2.55 66.54Â±0.13 70.45
Droid(Khazatskyetal.,2024) 53.33Â±5.25 90.40Â±4.90 60.99Â±3.72 73.28Â±0.61 72.16
Hypersim(Robertsetal.,2021) 52.67Â±4.11 88.80Â±3.27 60.84Â±2.06 72.29Â±0.47 71.29
Hypersim+ADT(Panetal.,2023) 52.00Â±2.83 87.20Â±2.30 63.61Â±1.04 70.83Â±0.13 71.41
Hypersim+S3DIS(Armenietal.,2017) 49.33Â±0.94 94.13Â±2.04 64.57Â±3.91 71.74Â±0.75 73.98
Hypersim+Structured3D(Zhengetal.,2020) 46.67Â±4.11 80.27Â±7.72 58.02Â±2.34 65.05Â±0.40 65.35
Hypersim+RH20T(Fangetal.,2023a) 47.33Â±1.89 86.93Â±4.99 57.01Â±4.35 64.28Â±0.46 67.35
Hypersim+ASE(Avetisyanetal.,2024) 47.33Â±4.11 87.73Â±3.39 60.62Â±4.14 68.59Â±0.30 69.54
4.3 PUTALLTOGETHER
Basedonthepreviousanalyses,weproceedtopre-trainthefinalversionofSPA.Weuseamask
ratioof0.5andenableallthreerenderinglosses. FollowingPonder(Huangetal.,2023), weset
the weight for the RGB loss to 10, the weights for the depth and semantic losses to 1, and use
Î» = 0.01, Î» = 10, and Î» = 1. The volume size is 128 Ã— 128 Ã— 32. For stable
eikonal sdf free
training,weapplytheExponentialMovingAverage(EMA)techniquewithadecayof0.999. We
useAdamW(Loshchilovetal.,2017)astheoptimizerwithaweightdecayof0.04andalearning
rateof8eâˆ’4. OneCycle(Smith&Topin,2019)learningrateschedulerisadopted. Weutilize80
NVIDIAA100-SXM4-80GBGPUs,eachwithabatchsizeof2,andaccumulategradientsover8
batches,resultinginatotaleffectivebatchsizeof2Ã—8Ã—80=1280. Trainingisconductedover
2000epochs,samplingeachdatasettomatchthesizeofADTperepoch. Thedatasetsusedforthe
finalversionincludeScanNet,ScanNet++,ADT,S3DIS,Hypersim,andDroid.
5 EXPERIMENT RESULTS
Inthissection,wepresenttheresultsofourlarge-scaleevaluation. Ourexperimentsaredesignedto
addressthefollowingresearchquestions:
6Preprint
Table3: Summaryofdifferentrepresentationlearningmethods. â€˜#Param.â€™ isthetotalparameters
oftheencoder,whileâ€˜#Framesâ€™indicatesthetotalnumberofimageframesusedduringpre-training.
Vision-Centric Multi-Modal Embodied-Specific
Method MoCoV3 MAE DINOV2 CLIP EVA InternViT-300M InternViT-6B MVP VC-1 SPA
(Chenetal., (Heetal., (Oquabetal., (Radfordetal., (Fangetal., (Chenetal., (Chenetal., (Radosavovicetal., (Majumdaretal., (Ours)
2020b) 2022) 2023) 2021) 2023b) 2024) 2024) 2023) 2023)
IsVanilla? âœ“ âœ“ âœ— âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ“
InputSize 224 224 224 224 224 448 224 256 224 224
PatchSize 16 16 14 14 14 14 14 16 16 16
#Param. 303M 303M 303M 303M 303M 303M 5.9B 303M 303M 303M
#Frames 1.28M 1.28M 1.2B 400M 14M 5.0B 5.0B 4.5M 5.6M 3.8M
Table4:Comparisonofdifferentrepresentationlearningmethods.â€˜OOMâ€™indicatesInternViT-6B
encounteredanout-of-memoryerrorduringevaluation. Thebestandsecond-bestresultsarebolded
andunderlinedrespectively. Thenumberinparenthesesdenotesthenumberoftasks.
Method Vision-Centric Multi-Modal Embodied-Specific
InternViT- InternViT-
Benchmark MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA(Ours)
300M 6B
AD(2) 58.7Â±7.0 58.0Â±2.0 47.3Â±3.1 48.7Â±3.1 58.0Â±6.0 53.3Â±3.1 60.0Â±9.2 53.3Â±4.2 54.0Â±4.0 60.0Â±4.0
MW(5) 88.8Â±5.0 90.0Â±4.6 84.0Â±3.7 77.1Â±3.2 90.7Â±0.9 84.0Â±3.7 89.1Â±1.2 93.6Â±5.2 87.5Â±3.8 93.3Â±2.0
VC-1
DMC(5) 67.3Â±3.3 74.4Â±1.8 64.5Â±2.5 53.9Â±3.6 62.7Â±2.8 53.3Â±0.4 66.3Â±3.2 69.4Â±2.6 65.3Â±3.6 71.1Â±5.0
TF(2) 67.9Â±0.2 73.0Â±0.5 68.5Â±0.4 56.1Â±1.6 67.2Â±0.2 65.2Â±1.6 70.7Â±0.9 73.2Â±0.8 70.9Â±1.1 73.6Â±2.0
Group1(35) 73.7 78.3 78.2 76.8 75.2 74.1 OOM 76.2 80.1 80.5
RLBench
Group2(36) 54.2 57.7 56.1 55.7 57.0 54.9 OOM 56.3 55.7 61.2
Meta-World(48) 69.3Â±1.5 67.8Â±1.7 56.3Â±0.6 66.7Â±1.7 63.7Â±1.3 57.5Â±1.7 OOM 66.4Â±1.7 68.6Â±1.5 69.2Â±1.7
Object(10) 65.3Â±8.0 71.7Â±13.1 64.7Â±9.9 50.2Â±7.0 73.2Â±6.0 67.7Â±6.0 58.0Â±10.6 63.7Â±4.8 69.7Â±7.2 76.7Â±5.3
Spatial(10) 40.5Â±0.9 57.2Â±2.9 36.3Â±11.8 32.2Â±0.6 59.3Â±7.7 48.3Â±6.4 42.0Â±10.3 58.0Â±6.2 50.5Â±7.5 50.0Â±3.8
LIBERO Goal(10) 49.2Â±8.1 54.3Â±6.0 22.2Â±2.3 30.3Â±3.2 56.8Â±2.9 58.8Â±4.5 33.2Â±2.0 63.8Â±2.8 57.5Â±6.6 65.3Â±2.5
10(10) 34.2Â±3.8 41.2Â±4.5 28.3Â±3.0 27.5Â±3.9 43.3Â±2.8 38.2Â±1.3 34.3Â±4.6 39.0Â±0.9 39.7Â±3.5 40.2Â±3.6
90(90) 30.0Â±1.4 29.9Â±2.0 27.5Â±2.2 29.4Â±2.0 31.3Â±2.3 23.8Â±1.8 27.1Â±2.1 32.1Â±3.5 30.6Â±3.3 32.2Â±1.6
Franka-Kitchen(5) 48.3Â±4.7 42.7Â±2.6 40.9Â±6.4 30.8Â±3.3 37.3Â±1.3 28.5Â±1.7 OOM 34.3Â±6.1 37.5Â±3.5 40.6Â±1.9
MeanS.R.â†‘ 81.67 85.13 75.18 77.10 83.84 75.41 30.65 84.85 84.69 88.63
MeanRankâ†“ 4.51 4.07 5.61 5.17 4.37 5.92 7.57 4.24 4.13 3.20
Q1: HowdoesSPAcomparetoothermethodsinourlarge-scaleembodiedevaluation?
Q2: Whatinsightsdowegainaboutvariousrepresentationlearningapproachesfromourevaluation?
Q3:DoesSPAreallylearnenhanced3Dawarenessthatresultsinimprovedembodiedrepresentation?
Q4: CanSPAfacilitaterobotlearninginreal-worldenvironmentsinazero-shotmanner?
5.1 OVERALLCOMPARISONS(Q1,Q2)
EvaluationMetrics. Wefollowpriorwork(Majumdaretal.,2023;Zhuetal.,2024)inreporting
twometrics: MeanSuccessRate(MeanS.R.)andMeanRank. MeanS.R.istheaveragesuccessrate
acrossalltasks,indicatingoverallperformance,whileMeanRankreflectstheaveragerankingofeach
methodâ€™ssuccessrateacrosstasks,providingameasureofrelativeperformance. SinceRLBenchhas
fixedtrainandtestsets,wereportasingleresultforthisbenchmark.
Baselines. Weevaluate9state-of-the-artrepresentationlearningmodels,allusingthesameViT-L
backbone,categorizedintovision-centric,multi-modal,andembodied-specific. Thisalsoincludesa
comparisonwitha6B-parametermulti-modalmodel(Chenetal.,2024). Detailsofthemodelsare
summarizedinTab.3. TheresultsoneachbenchmarkareshowninTab.4. Fordetailedresultson
eachtaskandeachrandomseed,pleaserefertoAppendixD.Wealsohavevisualizedtheperformance
radarchartandtheper-taskrankdistributionsinFig.1.
Finding1: WeobservethatSPAdemonstratessuperiorperformanceinbothmeansuccessrateand
meanrank. Whilenomethodranksfirstacrossallindividualbenchmarks,consistentwiththefindings
byMajumdaretal.(2023),SPAachievesthebestorsecond-bestmeansuccessratein11outof13
benchmarks. Additionally,itranksinthetop3forover65.5%ofindividualtasks,surpassingthe
secondandthirdhighestpercentagesof46.8%forMAEand46.0%forVC-1,respectively. These
trendsdemonstratetherobustnessandsuperiorityofSPA.
Finding2:Weobservethatforvision-centricmethods,superiorperformanceonvisiontasksdoesnot
necessarilytranslatetobetterembodiedperformance. Despiteusing10timesmoredata,DINOV2
performsworsethanMoCoV3andMAE.Notably,MAEperformsexceptionallywell,likelydueto
itsreconstructionobjective,whichenhances2Dspatialawareness. Interestingly,methodslikeMVP
andVC-1,whichareMAEmodelspre-trainedonhumaninteractiondata,shownoclearadvantage
7Preprint
Table5: AdditionalcomparisonsofViT-basemodelsonVC-1benchmarks.
DINOV2-B MAE-B R3M-B VC-1-B STP-B Voltron-B Theia-B SPA-B
Methods (Oquabetal., (Heetal., (Nairetal., (Majumdaretal., (Yangetal., (Karamchetietal., (Shangetal., (Ours)
2023) 2022) 2022) 2023) 2024b) 2023) 2024)
IsVanilla? âœ— âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“
Embodied? âœ— âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“
AD 36.67Â±2.31 52.67Â±3.06 48.00Â±6.93 50.00Â±5.29 52.00Â±2.00 46.67Â±4.62 53.33Â±5.03 52.00Â±3.46
MW 60.80Â±0.80 88.80Â±4.00 59.20Â±5.60 86.67Â±0.92 92.00Â±1.39 84.00Â±3.20 89.07Â±3.23 92.00Â±4.16
VC-1
DMC 35.19Â±4.87 62.39Â±4.97 49.57Â±4.85 60.92Â±0.70 61.40Â±2.86 56.36Â±2.01 64.98Â±3.42 64.21Â±3.52
TF 54.50Â±1.16 70.78Â±0.17 56.18Â±7.00 72.33Â±0.69 67.96Â±0.95 74.26Â±1.57 69.41Â±0.60 73.06Â±0.51
MeanS.R. 47.31 71.63 54.37 70.19 71.92 69.50 72.55 73.66
Table6: Zero-shotcameraposeestimation. SPAdemonstratesstrong3Dawareness.
Error MoCoV3 MAE DINOV2 CLIP EVA InternViT-300MInternViT-6B MVP VC-1 SPA(Ours)
Trans.(Ã—eâˆ’2)2.29Â±0.072.15Â±0.076.55Â±0.074.21Â±0.375.49Â±0.24 4.62Â±0.14 5.39Â±0.41 2.15Â±0.122.02Â±0.071.65Â±0.09
Rot.(Ã—eâˆ’1) 0.79Â±0.070.73Â±0.032.12Â±0.251.52Â±0.081.83Â±0.09 1.83Â±0.08 1.91Â±0.12 0.77Â±0.050.72Â±0.010.61Â±0.01
overImageNet(Dengetal.,2009)pre-trainedMAE.Thissuggeststhatwhilehumanactivitydata
mayseemmorerelevant,datadiversityandthoroughconvergencearemorecritical.
Finding3: Multimodalmethodsgenerallyperformpoorlyinembodiedevaluations,exceptEVA,
which combines image-language contrastive techniques with MAE reconstruction. Furthermore,
InternViT-6B,despitehavingsignificantlymoremodelparameters,doesnotdemonstratesuperiority
andevenperformsworseonsomebenchmarkscomparedtoInternViT-300M.Thisindicatesthat
currentscalingpropertiesofmultimodalapproachesdonoteffectivelytranslatetoembodiedAI.
Finding4: Focusingonasinglebenchmarkcanleadtohighlybiasedconclusions. Forinstance,
ImageNetpre-trainedmethods(e.g.MoCoV3andMAE)performexceptionallywellontheFranka
Kitchen benchmark, suggesting a minimal domain gap between ImageNet and Franka Kitchen
observations. Moreover,despitebeingbasedonMAE,previousSOTAembodiedrepresentationslike
MVPandVC-1donotconsistentlyoutperformtheoriginalImageNetversion. Theseobservations
underscoretheimportanceofourlarge-scaleembodiedevaluation.
5.2 ADDITIONALCOMPARISONS(Q1)
WeprimarilycomparewithSOTAmethodsusingtheViT-Lbackbone,whichiscommonlyavailable
andpre-trainedonlarge-scaledatasets. However,someembodied-specificmodelsareonlyofferedin
ViT-Bvariants. Therefore,weprovideadditionalcomparisonswithseveralViT-BmodelsinTab.5.
OurViT-Bversion,SPA-B,alsooutperformsotherbaselines. Furthermore,whencomparedtoSPA-L
onVC-1benchmarks,themeansuccessrateincreasesby4.16(73.66â†’77.82). Thisindicatesthat
increasingthemodelsizepositivelyimpactsSPAâ€™sperformance.
5.3 STUDYON3DAWARENESSOFSPA(Q3) Table7: AdditionalablationsonVC-1.
Methods SPA-B SPA-MAE RADIO E-RADIO
Firstly,weaimtoprovideclearevidencethatthe
AD 52.00Â±3.46 55.33Â±3.06 55.33Â±3.06 56.67Â±2.31
performanceimprovementsofSPAareduetoits
MW 92.00Â±4.16 90.67Â±6.00 72.00Â±9.23 83.47Â±4.11
VC-1
3Dawareness. Todemonstratethis,weconducted DMC 64.21Â±3.52 63.85Â±3.60 67.38Â±7.35 62.92Â±4.24
twoadditionalablationstudiesontheVC-1bench- TF 73.06Â±0.51 70.14Â±0.98 71.75Â±0.14 68.44Â±1.19
marks: 1)Todeterminewhethertheperformance MeanS.R. 73.66 73.11 67.93 70.16
gainisduetoSPAâ€™spre-trainingobjectivesorthedatasetsused,wecontinuepre-trainingtheIma-
geNetpre-trainedMAE-B(themostcompetitivemethodbesidesSPA)onthesamedatasetsusedby
SPA-B,referringtothismodelasSPA-MAE.Hyperparameters,includingmaskratioandbatchsize,
arekeptattheirdefaultsettings,andboththeImageNetpre-trainedencoderanddecoderweightsare
initiallyloaded. 2)SinceSPAusesthefeaturemapofRADIOforsemanticrenderingsupervision,
wealsoevaluatetheoriginalRADIO(653Mparameters)anditsefficientversion,E-RADIO(391M
parameters). ResultsarepresentedinTab.7.
Finding5: The3D-awarepre-trainingobjectivesignificantlyenhancesSPAâ€™sperformance. Itsur-
passesthesingle-imagenaiveMAEwiththesamedata. Notably,SPAlearnssuperiorrepresentations
comparedtoitssemanticrenderingteacherbyasubstantialmargin.
Moreover, weprovidebothquantitativeandqualitativeevidencetodemonstratethatSPAhasac-
quired 3D awareness. For qualitative analysis, we visualize the zero-shot feature maps on mul-
8Preprint
 6 3 $  6 3 $
    0 $ (  0 $ (
 0 9 3  0 9 3
 9 &    9 &  
    0 R & R 9   0 R & R 9 
 & / , 3  & / , 3
 , Q W H U Q 9 L 7     0  ' , 1 2 9   , Q W H U Q 9 L 7     0  ' , 1 2 9 
  
                                                            
 7 U D Q V O D W L R Q  ( U U R U  5 R W D W L R Q  ( U U R U
Figure4: Correlationbetweenmeansuccessrateandcameraposeregressionerror.
tiview images of different encoder outputs, as shown in Fig. 5. The images are taken from the
unseenArkitscenesdataset. Forquantitativeanalysis,weevaluatethezero-shot3Dawarenessof
variousmethodsusingacameraposeestimationtaskontheNAVIdataset(Jampanietal.,2023).
Specifically,givenapairofimagesfromdifferentview-
points,weuseafrozenencodertoextractfeaturesand
concatenatethem. AsmallMLPthenregressestherela-
Input
tivecameraposeandwereportrotationandtranslation
errorsinTab.6.DetailsareinAppendixE.WhileElBa-
nanietal.(2024)hasexplored3Dawarenessofdifferent
visionmodels,theircontextdiffers. Theirtaskscanal- SPA
low strong semantic models like DINOV2 to â€˜cheatâ€™.
Forexample,multiviewcorrespondencecanbeachieved
throughsemanticmatching,andtherelativedepthes- VC-1
timationtaskinvolvestransformingnormalizedvalues
intodiscretebins,resemblingaper-pixelclassification
task. Additionally,theyemphasizefine-graineddense
DINOV2
localcontext,whereas,embodiedAIfocusesmoreon
sparse,globalinformation(Nairetal.,2022). Thus,we
believecameraposeestimation,whichpredictsaglobal
MoCoV3
â€˜poseâ€™fromobservations,ismorerelevanttoembodied
AI,whereapolicymustpredictaglobalâ€˜actionâ€™.
Finding 6: We observe that SPA outperforms all CLIP
othermethodsinzero-shotcameraposeestimation. It
achievedan18.3%improvementintranslationand
a15.3%reductioninrotationerrorcomparedtothe
EVA
second-bestmodel. Additionally, weidentifyaclear
positivecorrelationbetweencameraposeestimation
andembodiedevaluationperformance,asdemonstrated
InternViT
inFig.4. Thisfindingsupportsourspatialhypothesis
300M
andmayoffervaluableinsightsforfutureresearchon
Figure5: Featuremapvisualization.
embodiedrepresentation.
Finding7: ThefeaturemapvisualizationprovidesclearevidencethatSPAhaslearnedmulti-view
consistentknowledge,demonstratingits3Dawareness. Additionally,thefeaturesproducedbySPA
arecleanerandmorecoherent.ThoughVC-1alsogeneratessmoothfeatures,theyarenotconsistent
acrossviewpoints. Thefeaturemapsfromthemulti-modalapproacharehighlynoisyandlackdetails.
5.4 REAL-WORLDEXPERIMENTS(Q4)
Picking
Cube
We conduct several real-world experiments to further
investigate the generalization ability of different rep-
resentations. Specifically,weutilizetheopen-sourced
Stacking
Low-CostRobotArm(Koch,2024)tolearnreal-world Cube
tasksfrompixels,withonly50demonstrationspertask
usingdifferentfrozenpre-trainedrepresentations. The
robot performed two single-arm tasks: (1) picking a Folding
Cloth
cube,and(2)stackingayellowcubeonapinkcube,as
wellasonedual-armtask: foldingaclothinhalf. Refer Figure6: Real-worldtaskillustrations.
toFig.6forillustrationsandAppendixFformoredetails.Weevaluateeachtaskwith25rollouts,with
9
 H W D 5  V V H F F X 6  Q D H 0Preprint
Table8: Real-worldexperimentresults.
Methods MoCoV3 MAE DINOV2 CLIP EVA InternViT-300M InternViT-6B MVP VC-1 SPA(Ours)
PickingCube 28.00 64.00 20.00 28.00 56.00 32.00 52.00 36.00 40.00 64.00
StackingCube 16.00 32.00 4.00 16.00 8.00 8.00 36.00 20.00 16.00 48.00
FoldingCloth 48.00 64.00 32.00 24.00 28.00 48.00 44.00 64.00 60.00 84.00
MeanS.R. 30.67 53.33 18.67 22.67 30.67 29.33 44.00 40.00 38.67 65.33
theresultspresentedinTab.8. SPAconsistentlyperformsbetteronreal-worldtasks,suggestingthat
SPAâ€™spre-trainedrepresentationscanrobustlyadapttoreal-worldenvironmentswithoutfinetuning.
6 RELATED WORK
RepresentationLearningforComputerVision. Recentadvancesincomputervisionhaveincreas-
inglyfocusedonunsupervisedandself-supervisedlearningtoutilizelargeamountsofunlabeled
data. Techniqueslikecontrastivelearning(Chenetal.,2020a;2021;2020b;Heetal.,2020),masked
autoencoders(Heetal.,2022;Feichtenhoferetal.,2022;Bachmannetal.,2022;Tongetal.,2022;
Wangetal.,2023),andself-distillation(Caronetal.,2021;Oquabetal.,2023;Ranzingeretal.,2024)
haveshownthateffectiverepresentationscanbelearnedwithoutsupervision. Moreover,multi-modal
pre-trainingapproaches(Radfordetal.,2021;Fangetal.,2023b;Chenetal.,2024)leveragelanguage
to learn more comprehensive representations. These developments have significantly improved
transferlearningcapabilitieswhilealsodisplayingzero-shotabilities.
RepresentationLearningforEmbodiedAI.Inspiredbycomputervision,recentworkhasapplied
methods such as contrastive learning (Nair et al., 2022; Yang et al., 2023) and masked autoen-
coders(Radosavovicetal.,2023;Majumdaretal.,2023;Karamchetietal.,2023;Yangetal.,2024b)
to embodied AI. However, these approaches often overlook the unique challenges of embodied
tasks,focusingprimarilyonsemanticknowledge. However,manyofthesemethodsprimarilyadapt
techniquesfromcomputervisionandfocuspredominantlyonlearningsemanticknowledge,often
overlookingtheuniquerequirementsofembodiedAItasks. Priorstudies(Zhuetal.,2024;Zeetal.,
2024;Wangetal.,2024b;a)havehighlightedtheimportanceof3Dspatialstructureforimproving
roboticlearning,thoughpointcloudsaredifficulttoscale. Despitethis,pointcloudsarechallenging
toobtainandencode,limitingtheirscalability. Inthiswork,weproposeaspatialhypothesis: while
semanticunderstandingiscrucial,3Dspatialawarenessisevenmoreimportant,andwedemonstrate
howastandard2Dbackbonecanintegrate3Dspatialawareness.
NeuralRendering. Recentadvancesin3Dvision,particularlyinneuralrendering(Mildenhalletal.,
2021),haveenabledtheencodingofscenesusingneuralnetworks,whichsupportdifferentiableren-
deringandreconstruction.Alongsideimprovementsinneuralrenderingtechniquesthemselves(Wang
etal.,2021;Zhuetal.,2023a;Groppetal.,2020;Ortizetal.,2022;Wangetal.,2022),thePonder
series(Huangetal.,2023;Zhuetal.,2023b;Yangetal.,2024a)wasthefirsttoapplydifferentiable
neuralrenderingforrepresentationlearning. However,theseworksprimarilyfocusonpointcloud
perceptionorautonomousdrivingscenariosYangetal.(2024a);Wangetal.(2024c). Tothebest
ofourknowledge,ourworkisthefirsttoapplyneuralrenderingforembodiedAIrepresentation
learningusingastandard2Dbackbone,markinganovelcontributiontothisareaofresearch.
7 CONCLUSION AND LIMITATIONS
Inthiswork,weproposethat3DspatialawarenessiscrucialforembodiedAIandintroduceSPA,
anovelframeworkthatpre-trainsastandardViTbackbonewith3Dspatialawareness. Tovalidate
our hypothesis, we conduct the largest-scale embodied evaluation to date, over 15 times larger
thanpreviousstudies. OurexperimentsdemonstratetheclearsuperiorityofSPAandhighlightthe
importanceof3Dawareness.Despitestrongresultsacrosssimulatedandrealrobotictasks,limitations
remain.Ourevaluationiscurrentlyrestrictedtoimitationlearning(specificallybehaviorcloning),and
exploringSPAâ€™sperformanceinothersettings,suchasreinforcementlearning,presentsanexciting
futuredirection. Additionally,SPAcurrentlyfocusesonstaticmulti-viewscenes; extendingitto
dynamic,temporalscenarioscouldenhanceitsgenerality. Lastly,whileweusetheViTencoderfor
faircomparison,thevolumedecoderâ€™smulti-viewinteractionknowledgecouldbeleveragedinpolicy
learning,offeringfurtherpotentialforimprovement.
10Preprint
REFERENCES
IroArmeni,SashaSax,AmirRZamir,andSilvioSavarese. Joint2d-3d-semanticdataforindoor
sceneunderstanding. arXivpreprintarXiv:1702.01105,2017.
ArmenAvetisyan,ChristopherXie,HenryHoward-Jenkins,Tsun-YiYang,SamirAroudj,Suvam
Patra,FuyangZhang,DuncanFrost,LukeHolland,CampbellOrme,etal.Scenescript:Reconstruct-
ingsceneswithanautoregressivestructuredlanguagemodel. arXivpreprintarXiv:2403.13064,
2024.
RomanBachmann,DavidMizrahi,AndreiAtanov,andAmirZamir. Multimae: Multi-modalmulti-
taskmaskedautoencoders. InEuropeanConferenceonComputerVision,pp.348â€“367.Springer,
2022.
GiladBaruch,ZhuoyuanChen,AfshinDehghan,TalDimry,YuriFeigin,PeterFu,ThomasGebauer,
BrandonJoffe,DanielKurz,ArikSchwartz,etal. Arkitscenes: Adiversereal-worlddatasetfor3d
indoorsceneunderstandingusingmobilergb-ddata. arXivpreprintarXiv:2111.08897,2021.
MathildeCaron,HugoTouvron,IshanMisra,HervÃ©JÃ©gou,JulienMairal,PiotrBojanowski,and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.9650â€“9660,2021.
Shizhe Chen, Ricardo Garcia, Cordelia Schmid, and Ivan Laptev. Polarnet: 3d point clouds for
language-guidedroboticmanipulation. 2023.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,pp.
1597â€“1607.PMLR,2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastivelearning. arXivpreprintarXiv:2003.04297,2020b.
XinleiChen,SainingXie,andKaimingHe. Anempiricalstudyoftrainingself-supervisedvision
transformers. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pp.
9640â€“9649,2021.
ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligning
for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pp.24185â€“24198,2024.
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shu-
ran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint
arXiv:2303.04137,2023.
RealRobotContributors. Realrobot: Aprojectforopen-sourcedrobotlearningresearch. https:
//github.com/HaoyiZhu/RealRobot,2024.
Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
NieÃŸner. Scannet: Richly-annotated3dreconstructionsofindoorscenes. InProceedingsofthe
IEEEconferenceoncomputervisionandpatternrecognition,pp.5828â€“5839,2017.
JiaDeng, WeiDong, RichardSocher, Li-JiaLi, KaiLi, andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pp.248â€“255.Ieee,2009.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale.
ICLR,2021.
11Preprint
Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael
Rubinstein,DeqingSun,LeonidasGuibas,JustinJohnson,andVarunJampani. Probingthe3d
awarenessofvisualfoundationmodels. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.21795â€“21806,2024.
Hao-ShuFang, HongjieFang, ZhenyuTang, JirongLiu, JunboWang, HaoyiZhu, andCewuLu.
Rh20t:Aroboticdatasetforlearningdiverseskillsinone-shot.InRSS2023WorkshoponLearning
forTaskandMotionPlanning,2023a.
YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,Xinlong
Wang,andYueCao. Eva: Exploringthelimitsofmaskedvisualrepresentationlearningatscale.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
19358â€“19369,2023b.
ChristophFeichtenhofer,YanghaoLi,KaimingHe,etal. Maskedautoencodersasspatiotemporal
learners. Advancesinneuralinformationprocessingsystems,35:35946â€“35958,2022.
AnkitGoyal,ValtsBlukis,JieXu,YijieGuo,Yu-WeiChao,andDieterFox. Rvt2: Learningprecise
manipulationfromfewdemonstrations. RSS,2024.
AmosGropp,LiorYariv,NivHaim,MatanAtzmon,andYaronLipman. Implicitgeometricregular-
izationforlearningshapes. arXivpreprintarXiv:2002.10099,2020.
AbhishekGupta,VikashKumar,CoreyLynch,SergeyLevine,andKarolHausman. Relaypolicy
learning: Solvinglong-horizontasksviaimitation andreinforcementlearning. arXivpreprint
arXiv:1910.11956,2019.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computervisionandpatternrecognition,pp.9729â€“9738,2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked
autoencodersarescalablevisionlearners.InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.16000â€“16009,2022.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415,2016.
DiHuang,SidaPeng,TongHe,HonghuiYang,XiaoweiZhou,andWanliOuyang. Ponder: Point
cloudpre-trainingvianeuralrendering. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.16089â€“16098,2023.
Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot
learningbenchmark&learningenvironment. IEEERoboticsandAutomationLetters,5(2):3019â€“
3026,2020.
Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle
Sargent,StefanPopov,AndrÃ©Araujo,RicardoMartinBrualla,KaushalPatel,etal. Navi:Category-
agnosticimagecollectionswithhigh-quality3dshapeandposeannotations. AdvancesinNeural
InformationProcessingSystems,36:76061â€“76084,2023.
SiddharthKaramcheti,SurajNair,AnnieS.Chen,ThomasKollar,ChelseaFinn,DorsaSadigh,and
Percy Liang. Language-driven representation learning for robotics. In Robotics: Science and
Systems(RSS),2023.
Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth
Karamcheti,SoroushNasiriany,MohanKumarSrirama,LawrenceYunliangChen,KirstyEllis,
etal.Droid:Alarge-scalein-the-wildrobotmanipulationdataset.arXivpreprintarXiv:2403.12945,
2024.
Alexander Koch. Low-cost robot arm. https://github.com/AlexanderKoch-Koch/
low_cost_robot,2024.URLhttps://github.com/AlexanderKoch-Koch/low_
cost_robot. GitHubrepository.
12Preprint
Vikash Kumar. Manipulators and Manipulation in high dimensional spaces. PhD thesis, Uni-
versityofWashington,Seattle,2016. URLhttps://digital.lib.washington.edu/
researchworks/handle/1773/38104.
ZhiqiLi,WenhaiWang,HongyangLi,EnzeXie,ChonghaoSima,TongLu,YuQiao,andJifengDai.
Bevformer: Learningbirdâ€™s-eye-viewrepresentationfrommulti-cameraimagesviaspatiotemporal
transformers. InEuropeanconferenceoncomputervision,pp.1â€“18.Springer,2022.
BoLiu,YifengZhu,ChongkaiGao,YihaoFeng,QiangLiu,YukeZhu,andPeterStone. Libero:
Benchmarkingknowledgetransferforlifelongrobotlearning. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
IlyaLoshchilov,FrankHutter,etal. Fixingweightdecayregularizationinadam. arXivpreprint
arXiv:1711.05101,5,2017.
ArjunMajumdar,KarmeshYadav,SergioArnaud,JasonMa,ClaireChen,SnehaSilwal,AryanJain,
Vincent-PierreBerges,TingfanWu,JayVakil,etal. Whereareweinthesearchforanartificial
visualcortexforembodiedintelligence? AdvancesinNeuralInformationProcessingSystems,36:
655â€“677,2023.
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
RenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. Communications
oftheACM,65(1):99â€“106,2021.
SurajNair,AravindRajeswaran,VikashKumar,ChelseaFinn,andAbhinavGupta. R3m:Auniversal
visualrepresentationforrobotmanipulation. arXivpreprintarXiv:2203.12601,2022.
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Learning
robustvisualfeatureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
JosephOrtiz,AlexanderClegg,JingDong,EdgarSucar,DavidNovotny,MichaelZollhoefer,and
Mustafa Mukadam. isdf: Real-time neural signed distance fields for robot perception. arXiv
preprintarXiv:2204.02296,2022.
XiaqingPan,NicholasCharron,YongqianYang,ScottPeters,ThomasWhelan,ChenKong,Omkar
Parkhi,RichardNewcombe,andYuhengCarlRen. Ariadigitaltwin: Anewbenchmarkdataset
foregocentric3dmachineperception. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.20133â€“20143,2023.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748â€“8763.PMLR,2021.
Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell.
Real-worldrobotlearningwithmaskedvisualpre-training. InConferenceonRobotLearning,pp.
416â€“426.PMLR,2023.
RenÃ© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards
robustmonoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEE
TransactionsonPatternAnalysisandMachineIntelligence(TPAMI),2020.
MikeRanzinger,GregHeinrich,JanKautz,andPavloMolchanov. Am-radio: Agglomerativevision
foundationmodelreducealldomainsintoone. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.12490â€“12500,2024.
MikeRoberts,JasonRamapuram,AnuragRanjan,AtulitKumar,MiguelAngelBautista,Nathan
Paczan,RussWebb,andJoshuaMSusskind. Hypersim: Aphotorealisticsyntheticdatasetfor
holisticindoorsceneunderstanding. InProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,pp.10912â€“10922,2021.
13Preprint
JinghuanShang,KarlSchmeckpeper,BrandonB.May,MariaVittoriaMinniti,TarikKelestemur,
DavidWatkins,andLauraHerlant. Theia: Distillingdiversevisionfoundationmodelsforrobot
learning. arXiv,2024.
WenzheShi,JoseCaballero,FerencHuszÃ¡r,JohannesTotz,AndrewPAitken,RobBishop,Daniel
Rueckert,andZehanWang. Real-timesingleimageandvideosuper-resolutionusinganefficient
sub-pixelconvolutionalneuralnetwork. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pp.1874â€“1883,2016.
LeslieNSmithandNicholayTopin. Super-convergence: Veryfasttrainingofneuralnetworksusing
largelearningrates. InArtificialintelligenceandmachinelearningformulti-domainoperations
applications,volume11006,pp.369â€“386.SPIE,2019.
ZhanTong,YibingSong,JueWang,andLiminWang. Videomae: Maskedautoencodersaredata-
efficientlearnersforself-supervisedvideopre-training. Advancesinneuralinformationprocessing
systems,35:10078â€“10093,2022.
SaranTunyasuvunakool,AlistairMuldal,YotamDoron,SiqiLiu,StevenBohez,JoshMerel,Tom
Erez, TimothyLillicrap, NicolasHeess, andYuvalTassa. dm_control: Softwareandtasksfor
continuouscontrol. SoftwareImpacts,6:100022,2020.
ChenWang,HaochenShi,WeizhuoWang,RuohanZhang,LiFei-Fei,andCKarenLiu. Dexcap:
Scalableandportablemocapdatacollectionsystemfordexterousmanipulation. arXivpreprint
arXiv:2403.07788,2024a.
ChenxiWang,HongjieFang,Hao-ShuFang,andCewuLu. Rise: 3dperceptionmakesreal-world
robotimitationsimpleandeffective. arXivpreprintarXiv:2404.12281,2024b.
JingwenWang,TymoteuszBleja,andLourdesAgapito. Go-surf: Neuralfeaturegridoptimization
forfast,high-fidelityrgb-dsurfacereconstruction. In2022InternationalConferenceon3DVision
(3DV),pp.433â€“442.IEEE,2022.
LetianWang,SeungWookKim,JiaweiYang,CunjunYu,BorisIvanovic,StevenLWaslander,Yue
Wang, Sanja Fidler, Marco Pavone, and Peter Karkus. Distillnerf: Perceiving 3d scenes from
single-glanceimagesbydistillingneuralfieldsandfoundationmodelfeatures. arXivpreprint
arXiv:2406.12095,2024c.
LiminWang,BingkunHuang,ZhiyuZhao,ZhanTong,YinanHe,YiWang,YaliWang,andYuQiao.
VideomaeV2: scalingvideomaskedautoencoderswithdualmasking. InCVPR,pp.14549â€“14560,
2023.
PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,TakuKomura,andWenpingWang. Neus:
Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv
preprintarXiv:2106.10689,2021.
PhilippeWeinzaepfel,ThomasLucas,VincentLeroy,YohannCabon,VaibhavArora,RomainBrÃ©gier,
GabrielaCsurka,LeonidAntsfeld,BorisChidlovskii,andJÃ©rÃ´meRevaud. CroCov2: Improved
Cross-viewCompletionPre-trainingforStereoMatchingandOpticalFlow. InICCV,2023.
ManuelWÃ¼thrich,FelixWidmaier,FelixGrimminger,JoelAkpo,ShrutiJoshi,VaibhavAgrawal,
Bilal Hammoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, et al. Trifinger: An
open-sourcerobotforlearningdexterity. arXivpreprintarXiv:2008.03596,2020.
HonghuiYang,ShaZhang,DiHuang,XiaoyangWu,HaoyiZhu,TongHe,ShixiangTang,Heng-
shuang Zhao, Qibo Qiu, Binbin Lin, et al. Unipad: A universal pre-training paradigm for
autonomousdriving. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.15238â€“15250,2024a.
JiangeYang,ShengGuo,GangshanWu,andLiminWang. Comae: singlemodelhybridpre-training
onsmall-scalergb-ddatasets. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume37,pp.3145â€“3154,2023.
JiangeYang,BeiLiu,JianlongFu,BochengPan,GangshanWu,andLiminWang. Spatiotemporal
predictivepre-trainingforroboticmotorcontrol. arXivpreprintarXiv:2403.05304,2024b.
14Preprint
Chandan Yeshwanth, Yueh-Cheng Liu, Matthias NieÃŸner, and Angela Dai. Scannet++: A high-
fidelitydatasetof3dindoorscenes. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pp.12â€“22,2023.
AlexYu, RuilongLi, MatthewTancik, HaoLi, RenNg, andAngjooKanazawa. Plenoctreesfor
real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pp.5752â€“5761,2021.
TianheYu,DeirdreQuillen,ZhanpengHe,RyanJulian,KarolHausman,ChelseaFinn,andSergey
Levine. Meta-world: Abenchmarkandevaluationformulti-taskandmetareinforcementlearning.
InConferenceonrobotlearning,pp.1094â€“1100.PMLR,2020.
YanjieZe,GuZhang,KangningZhang,ChenyuanHu,MuhanWang,andHuazheXu. 3ddiffusion
policy. arXivpreprintarXiv:2403.03954,2024.
TonyZZhao, VikashKumar, SergeyLevine, andChelseaFinn. Learningfine-grainedbimanual
manipulationwithlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao, and Zihan Zhou. Structured3d: A
largephoto-realisticdatasetforstructured3dmodeling. InComputerVisionâ€“ECCV2020: 16th
EuropeanConference,Glasgow,UK,August23â€“28,2020,Proceedings,PartIX16,pp.519â€“535.
Springer,2020.
HaoyiZhu, Hao-ShuFang, andCewuLu. X-nerf: Explicitneuralradiancefieldformulti-scene
360deginsufficientrgb-dviews. InProceedingsoftheIEEE/CVFWinterConferenceonApplica-
tionsofComputerVision,pp.5766â€“5775,2023a.
HaoyiZhu,HonghuiYang,XiaoyangWu,DiHuang,ShaZhang,XianglongHe,TongHe,Heng-
shuangZhao,ChunhuaShen,YuQiao,etal. Ponderv2: Pavethewayfor3dfoundataionmodel
withauniversalpre-trainingparadigm. arXivpreprintarXiv:2310.08586,2023b.
Haoyi Zhu, Yating Wang, Di Huang, Weicai Ye, Wanli Ouyang, and Tong He. Point cloud mat-
ters: Rethinking the impact of different observation spaces on robot learning. arXiv preprint
arXiv:2402.02500,2024.
XizhouZhu, WeijieSu, LeweiLu, BinLi, XiaogangWang, andJifengDai. DeformableDETR:
deformabletransformersforend-to-endobjectdetection. InInternationalConferenceonLearning
Representations,2021.
YukeZhu,JosiahWong,AjayMandlekar,RobertoMartÃ­n-MartÃ­n,AbhishekJoshi,SoroushNasiriany,
andYifengZhu. robosuite: Amodularsimulationframeworkandbenchmarkforrobotlearning.
arXivpreprintarXiv:2009.12293,2020.
15Preprint
A ADDITIONAL RENDERING LOSSES
HerewedetailthethreeadditionalrenderinglosseswehaveappliedinSec.2.4.
EikonalRegularizationLoss. TheEikonalregularizationloss,denotedasL ,isawidelyused
eikonal
lossfunctionfortheregularizationofsigneddistancefunctions(SDFs)(Groppetal.,2020). Itis
definedas:
L =
1
(cid:88)Nr (cid:88)Np
(âˆ¥âˆ‡s(p )âˆ¥âˆ’1)2, (9)
eikonal N N i,j
r p
i=1j=1
whereâˆ‡s(p )representsthegradientoftheSDFsatthelocationp . SincetheSDFisadistance
i,j i,j
measure,L encouragesthegradientstohaveunitnormatthequerypoint.
eikonal
Near-SurfaceandFreeSpaceLossforSDF.ToimproveSDFestimation,weincorporateadditional
approximateSDFsupervision,similartoiSDF(Ortizetal.,2022)andGO-Surf(Wangetal.,2022).
Specifically,fornear-surfacepoints,thedifferencebetweenrendereddepthandground-truthdepth
servesaspseudo-SDFground-truthsupervision. Forpointsfarfromthesurface,afreespacelossis
usedtofurtherregularizetheSDFvalues.
TocomputetheapproximateSDFsupervision,wedefineanindicatorb(z)foreachsampledraypoint
withraylengthzandcorrespondingground-truthdepthD:
b(z)=Dâˆ’z. (10)
Thevalueb(z)canbeconsideredacredibleapproximateSDFvaluewhenitissmall. Lettbea
user-definedthreshold,setto0.05inourexperiments. Forsampledraypointssatisfyingb(z)â‰¤t,we
applythenear-surfaceSDFlosstoconstraintheSDFpredictions(z ):
i,j
1
(cid:88)Nr (cid:88)Np
L = |s(z )âˆ’b(z )|. (11)
sdf N N i,j i,j
r p
i=1j=1
Fortheremainingsampledraypoints,weutilizeafreespaceloss:
1
(cid:88)Nr (cid:88)Np
(cid:16) (cid:17)
L = max 0,eâˆ’Î±Â·s(zi,j)âˆ’1,s(z )âˆ’b(z ) , (12)
free N N i,j i,j
r p
i=1j=1
whereÎ±issetto5,followingOrtizetal.(2022);Wangetal.(2022). Duetothepresenceofnoisy
depthimages,L andL areappliedonlytorayswithvaliddepthvalues.
sdf free
Inourexperiments,weadoptasimilarweightingschemetoGO-Surf(Wangetal.,2022),setting
Î» = 10.0,Î» = 1.0,Î» = 10.0,andÎ» = 1.0. WeobservethattheEikonaltermcanleadto
C D sdf free
overlysmoothreconstructions,soweuseasmallweightof0.01fortheEikonalloss.
B EVALUATION SETUPS
Herewedetailthesetupsofourlarge-scaleevaluationinSec.3.
B.1 SINGLE-TASKBENCHMARKS
VC-1 (Majumdar et al., 2023). This benchmark includes several simulators. We selected four:
Adroit(Kumar,2016),Meta-World(Yuetal.,2020),DMControl(Tunyasuvunakooletal.,2020),
andTriFinger(WÃ¼thrichetal.,2020). TheAdroitsubsetfocusesondexterousmanipulationwith
2 tasks. The Meta-World subset addresses two-finger gripper manipulation with 5 tasks. The
DMControlsubsetisforlocomotioncontrol,alsowith5tasks. TheTriFingersubsettargetsthree-
finger manipulation with 2 tasks. For all tasks, we use a 3-layer MLP as the policy network for
each single-task training, following the original implementation. Each task is trained with 100
16Preprint
demonstrations,exceptfor25onMeta-World,andevaluated50timesusingthespecificseeds100,
200,and300. The[CLS]tokenofafrozenpre-trainedViTisusedastheobservationfeature. All
hyper-parametersarekeptthesamewiththeoriginalimplementation.
FrankaKitchen(Guptaetal.,2019).Franka-KitchenisaMuJoCo-modeledsimulationenvironment
withaFrankarobotinakitchenscene. Itsactionspaceisthe9-dimensionaljointvelocitywith7DoF
forthearmand2DoFforthegripper. Followingpreviousworks(Nairetal.,2022;Karamchetietal.,
2023),weevaluatefivetasks: SlidingDoor,TurningLightOn,OpeningDoor,TurningKnob,and
OpeningMicrowave. Eachtaskspanstwocameraviewpointsandthreerandomseeds. Similartothe
evaluationschemeinVC-1,weutilize25demonstrationstotrainapolicymodel,whichisa2-layer
MLPwithhiddensizes[256,256]precededbyaBatchNorm.
Meta-World (Yu et al., 2020). This benchmark comprises a series of tasks in which an agent
directsaSawyerrobotarmtomanipulateobjectsinatabletopenvironment. Weselected48tasks,
encompassingeasy,medium,andhardlevels. WeimplementedtheDiffusionPolicy(Chietal.,2023)
onthisbenchmarkandadheredtothesetupinZeetal.(2024)togenerate10demonstrationsforeach
single-tasktraining,followedbyevaluationthrough20rollouts. Theaverageresultsacrossthree
fixedseeds(100,200,300)arereported. The[CLS]tokenfromafrozenpre-trainedViTservesas
theobservationfeature.
B.2 LANGUAGE-CONDITIONEDMULTI-TASKBENCHMARKS
RLBench(Jamesetal.,2020). Thisbenchmarkisaprominentlanguage-conditionedmulti-task
robotlearningframework. PolarNet(Chenetal.,2023)hascategorizedalltasksinto9groups. We
selected71tasksfromRLBenchthatcanbesuccessfullyexecutedandsplitthemintotwogroups
uniformlyoncategories: Group1with35tasksandGroup2with36tasks. Eachtaskincludes100
trainingdemonstrationsand25testingrollouts. Foreachgroup,wetrainalanguage-conditioned
multi-taskagent. WeemployRVT-2(Goyaletal.,2024), thestate-of-the-art(SOTA)methodon
thisbenchmark,asourpolicy. RVT-2takesmultipleimagesrenderedfrompointcloudsasinputs
andusesaconvolutionalblocktogeneratefeaturemaps. Wesubstitutetheconvolutionalblockwith
different pre-trained ViTs, unpatchifying the latent vectors concatenated with the global [CLS]
tokentoobtainfeaturemaps. Allotherarchitecturesandhyperparametersremainconsistentwiththe
originalRVT-2implementation.
LIBERO(Liuetal.,2024). BuiltuponRobosuite(Zhuetal.,2020),LIBERO(Liuetal.,2024)
generatesatotalof130language-conditionedtasksacrossfivesuites: LIBERO-Spatial,LIBERO-
Object, LIBERO-Goal, LIBERO-10, and LIBERO-90. Each suite contains 10 tasks, except for
LIBERO-90,whichincludes90tasks. Wetrainalanguage-conditionedmulti-taskpolicyforeach
suite,adoptingthetransformerpolicyprovidedbyLIBERO.Theimageencodersaremodifiedfrom
default CNNs to frozen pre-trained ViTs, utilizing the [CLS] token for feature extraction. To
expeditepolicytraining,weuseonly20demonstrationspertaskandforgoaugmentations,allowing
forpre-extractionofallimagefeaturesduringtraining. Aftertrainingfor25epochs,thecheckpoints
fromthe20thand25thareevaluatedwith20rolloutspertask,andthebestcheckpointâ€™sperformance
istaken. Finally,theresultsareaveragedon3randomseeds.
C MORE IMPLEMENTATION DETAILS
C.1 DATASETDETAILS
ThedatasetsusedforSPAincludeScanNet,ScanNet++,Hypersim,ADT,S3DIS,andDroid.
ScanNetconsistsof1.89millionframesintotal. Eachepochincludes1.5timesthedatasetsize. For
eachscene,arandomstartingframeisselected,followedbythesamplingof1to8framesatrandom,
withanintervalof8framesbetweenthem.
ScanNet++comprises0.11millionframes. Eachepochincludes5timesthedatasetsize. Foreach
scene,arandomstartingframeisselected,followedbythesamplingof1to8framesatrandom,with
anintervalof5framesbetweenthem.
Hypersimcontains0.03millionframes. Eachepochincludes8timesthedatasetsize. Foreach
scene,werandomlyselect1to8continuousframes.
17Preprint
ADTconsistsof0.0015Mframesintotal. Eachepochincludes1timesthedatasetsize. Foreach
scene,1to8continuousframesarerandomlyselected.
S3DISconsistsof0.015millionframes. Eachepochincludes5timesthedatasetsize. Foreachscene,
arandomstartingframeisselected,followedbythesamplingof1to8framesatrandom,withan
intervalof5framesbetweenthem.
Droidcontainsalargenumberofvideos,butduetothehighsimilaritybetweenframes,thevideos
arefirstdownsampledbyafactorof15duringpre-processing,resultingin1.78millionframes. Since
Droiddoesnotprovidedepthdata,weutilizeCroco-StereoWeinzaepfeletal.(2023)toestimate
densedepthmapsforrenderingsupervision. Additionally,duetothesignificantnoiseinthecamera
posedata,onlyasingleframeissampledatatimeduringtraining.
Duringpre-training,wefirstresizethemulti-viewinputimagestoslightlylargerthan224Ã—224,
andthenrandomlycropthemtoafinalsizeof224Ã—224. Randomphotometricdistortionswitha
probabilityof0.5areappliedforaugmentation,includingbrightnessrangingfrom0.875to1.125,
contrastrangingfrom0.5to1.5,saturationrangingfrom0.5to1.5,andhuerangingfrom-0.05to
0.05. Frameswithverysmallvaliddepthareasorsceneboxesarefilteredout.
Forsemanticrenderingsupervision,weobservethatusinglargerimagesizesimprovesthequality
offeaturemapsgeneratedbyRADIO.Consequently,weresizetheimagesto1024Ã—1024before
feedingthemintoRADIO,whichoutputsafeaturemapofsize64Ã—64. Wethenapplybilinear
samplingtoquerythesemanticfeaturelabelsforeachpixel.
C.2 PRE-TRAININGDETAILS
Forstabilityduringpre-training,weapplytheExponentialMovingAverage(EMA)withadecayrate
of0.999. Themodelistrainedfor2000epochson80NVIDIAA100-80GGPUs,usingagradient
clippingthresholdof1.0. EachGPUprocessesabatchsizeof2,with8gradientaccumulationsteps,
resultinginatotaleffectivebatchsizeof2Ã—80Ã—8 = 1280. WeemploytheAdamWoptimizer
withaweightdecayof0.04. Thebaselearningrateissetto5Ã—10âˆ’6,andtheactuallearningrateis
scaledbyafactorof8timestheeffectivebatchsize. AOneCyclelearningrateschedulerisused,
withapercentagestartof0.05,adividefactorof100,andafinaldividefactorof1000.
Tofacilitatefasterconvergenceandimprovestability,weinitializetheencoderwithImageNetpre-
trainedweightsfromtheMaskedAutoencoder(MAE),applyingalearningratelayerdecayof0.8.
Thisinitializationdoesnotaffectthevalidityofourconclusions,asdemonstratedbytheablation
study of SPA-MAE in Sec. 5.3. The ViT encoder and upsampling layers are trained with FP16
precision,whilethevolumedecoderistrainedwithFP32precision.
We set the loss weights to Î» = 10, Î» = 1, Î» = 1, Î» = 0.01, Î» = 1, and
color depth semantic eikonal free
Î» = 10. FortheNeuSsampler, theinitialnumberofsamplesissetto72, with24importance
sdf
samples. Ineachiteration,werandomlysample512pixelsperviewforrenderingandsupervision.
D DETAILED RESULTS OF EACH TASK
WepresenttheresultsofallindividualtasksinTab.9,Tab.10,Tab.11,Tab.12,Tab.13,andTab.14.
E CAMERA POSE ESTIMATION DETAILS
WeadoptasetupsimilartothatofElBananietal.(2024)forcameraposeestimationusingtheNAVI
dataset(Jampanietal.,2023). Givenanimagepairfromdifferentviewpoints,wefirstextractfeatures
fromeachimageusingafrozen,pre-trainedVisionTransformer(ViT)encoder. Followingstandard
protocolsforembodiedevaluation,weusethe[CLS]tokenasthefeaturerepresentation. Thetwo
[CLS] tokens are then concatenated and passed through a BatchNorm layer and a Multi-Layer
Perceptron(MLP)toregressthecamerapose. TheMLPconsistsoffourlinearlayerswiththree
ReLUactivations,usinghiddensizesof512,256,and128units,andoutputsa7-dimensionalpose
vector. The first three dimensions represent the xyz translation, while the last four dimensions
correspondtotherotationquaternions.
18Preprint
Cameras
Leader
Arms
Follower
Arms
Figure7: Real-worldhardwareplatform.
WeemploytheMeanSquaredError(MSE)lossfunctionandoptimizethemodelusingtheAdamW
optimizerwithaOneCyclelearningratescheduler. Themodelistrainedfor100epochswithabase
learningrateof1Ã—10âˆ’3andastartingpercentageof0.1. Forevaluation,weuseEuclideandistance
asthetranslationerrormetricandgeodesicdistanceastherotationerrormetric.Thegeodesicdistance
betweentwoquaternionsq andq isdefinedas:
1 2
Î¸ =2Â·arccos(|q Â·q |), (13)
1 2
whereq andq arenormalizedquaternions,andÂ·denotesthequaterniondotproduct. TheEuclidean
1 2
distancedbetweentwotranslationvectorst =(x ,y ,z )andt =(x ,y ,z )isgivenby:
1 1 1 1 2 2 2 2
(cid:112)
d= (x âˆ’x )2+(y âˆ’y )2+(z âˆ’z )2. (14)
2 1 2 1 2 1
F REAL-WORLD EXPERIMENT DETAILS
Ourreal-worldhardwaresetupisbasedontheopen-sourceLow-Cost-Robotproject(Koch,2024).
WeutilizetwoIntelRealSenseD415camerasforimagecapture. Avisualizationofourplatformis
providedinFig.7. Forteleoperation,policytraining,andevaluation,weleveragetheopen-source
RealRobotproject(Contributors,2024). ThepolicyusedistheACTpolicy(Zhaoetal.,2023).
Foreachtask,wecollect50demonstrations,andduringevaluation,weconduct25rollouts,each
withrandomizedobjectlocationsandorientations. Themodelistrainedfor10,000epochsusing
fourNVIDIAA100GPUs. WeemploytheAdamWoptimizerwithalearningrateof5Ã—10âˆ’5and
aweightdecayof0.05. Additionally,aOneCyclelearningrateschedulerisused,withastarting
percentageof0.1,adivisionfactorof10,andafinaldivisionfactorof100.
19Preprint
Table9: AllresultsonVc-1benchmarks.
Benchmark AD MW DMC TF
Button
Relo- Drawer Bin Ham- Assem-WalkerWalkerReacherCheetahFingerReachMove
Methods Seed Pen Press
cate Open Picking mer bly Stand Walk Easy Run Spin Cube Cube
Topdown
ViT-LMethods
100 40.0092.00 88.00 100.00 88.00 100.00 88.00 84.88 57.59 92.29 56.28 70.49 84.37 61.20
MoCoV3 200 36.0080.00 88.00 100.00 80.00 100.00 84.00 82.95 55.02 92.08 43.17 69.49 84.20 61.26
300 28.0076.00 84.00 100.00 68.00 92.00 72.00 81.42 53.59 91.96 41.27 68.23 84.09 64.24
100 36.0084.00 84.00 100.00 88.00 100.00 100.00 951.27680.69 976.50 482.47 703.30 85.46 59.46
MAE 200 36.0080.00 84.00 100.00 76.00 96.00 96.00 933.67676.92 952.20 49.22 695.00 86.88 59.45
300 32.0080.00 68.00 100.00 72.00 98.00 88.00 873.53659.41 895.60 501.91 691.80 85.26 61.69
100 32.0068.00 68.00 100.00 84.00 100.00 88.00 87.01 56.52 94.50 26.98 70.87 86.16 50.84
DINOV2 200 28.0068.00 60.00 100.00 80.00 96.00 80.00 86.00 53.97 89.97 21.84 68.78 86.87 50.78
300 28.0060.00 60.00 100.00 80.00 92.00 72.00 82.41 51.69 88.36 21.34 67.41 86.05 50.17
100 24.0080.00 28.00 100.00 88.00 100.00 84.00 66.16 43.94 90.71 18.40 68.30 73.28 41.09
CLIP 200 24.0072.00 24.00 100.00 84.00 100.00 80.00 64.04 34.51 88.26 16.52 66.68 75.11 33.45
300 24.0068.00 16.00 100.00 84.00 96.00 72.00 52.70 31.60 85.17 14.51 67.49 74.73 38.95
100 44.0084.00 72.00 100.00 76.00 100.00 100.00 77.92 51.64 98.17 31.04 70.17 82.56 52.04
EVA 200 40.0076.00 84.00 100.00 72.00 100.00 100.00 77.63 50.81 86.66 19.37 67.43 81.72 52.07
300 32.0072.00 96.00 100.00 68.00 96.00 96.00 77.21 47.71 88.41 29.19 67.43 82.13 52.70
100 40.0072.00 80.00 100.00 72.00 100.00 84.00 70.04 30.44 80.80 16.70 67.05 78.59 53.67
InternViT-
200 28.0072.00 68.00 100.00 76.00 96.00 84.00 67.63 31.55 82.33 19.39 67.57 77.07 55.21
300M
300 28.0080.00 60.00 100.00 72.00 96.00 72.00 66.95 29.28 81.87 18.80 68.55 78.27 48.58
100 32.0072.00 88.00 100.00 80.00 100.00 84.00 88.53 70.02 93.09 26.54 70.62 85.96 57.52
InternViT-
200 40.0076.00 84.00 100.00 76.00 100.00 80.00 85.28 60.09 90.86 22.84 69.04 86.30 54.11
6B
300 60.0080.00 80.00 100.00 88.00 100.00 76.00 81.88 59.17 87.87 21.53 67.20 85.68 54.86
100 32.0084.00 96.00 100.00 96.00 100.00 100.00 84.88 57.59 92.29 56.28 70.49 84.37 61.20
MVP 200 28.0076.00 92.00 100.00 84.00 100.00 96.00 82.95 55.02 92.08 43.17 69.49 84.20 61.26
300 24.0076.00 84.00 100.00 68.00 100.00 88.00 81.42 53.59 91.96 41.27 68.23 84.09 64.24
100 32.0084.00 84.00 100.00 76.00 96.00 96.00 82.36 55.33 98.09 35.31 72.60 83.36 58.00
VC-1 200 28.0080.00 68.00 100.00 72.00 92.00 84.00 80.21 53.90 89.83 34.10 70.15 83.17 61.00
300 24.0076.00 76.00 100.00 96.00 88.00 84.00 68.62 50.13 87.89 31.18 70.11 82.75 57.16
100 40.0088.00 76.00 100.00 92.00 100.00 100.00 94.19 66.34 95.57 52.53 73.95 87.37 56.68
SPA-L 200 44.0076.00 84.00 100.00 88.00 100.00 84.00 92.28 60.60 81.43 44.99 71.83 87.26 64.35
300 36.0076.00 96.00 100.00 88.00 96.00 96.00 87.87 51.75 83.86 39.10 70.91 87.62 58.02
ViT-BMethodsandOthers
100 20.0080.00 88.00 100.00 84.00 100.00 96.00 77.02 45.34 87.97 40.01 72.72 80.41 54.66
STP-B 200 28.0076.00 92.00 100.00 84.00 100.00 80.00 71.50 33.60 84.08 34.30 72.18 80.13 57.97
300 32.0076.00 88.00 100.00 72.00 100.00 96.00 71.44 42.86 79.67 39.17 69.12 80.65 53.95
100 20.0092.00 52.00 96.00 32.00 88.00 48.00 668.49301.54 842.90 256.56 678.00 75.08 45.66
R3M-B 200 12.0076.00 48.00 96.00 32.00 88.00 44.00 634.62256.82 661.40 198.63 660.9075..6248.09
300 12.0076.00 32.00 88.00 28.00 76.00 40.00 633.39211.90 585.50 188.15 657.30 74.54 45.59
100 32.0076.00 88.00 100.00 80.00 96.00 96.00 72.90 43.97 82.09 37.02 70.50 84.55 55.62
Theia-B 200 36.0080.00 60.00 100.00 84.00 100.00 84.00 79.05 56.99 94.36 39.59 70.22 83.27 54.59
300 24.0072.00 80.00 100.00 72.00 96.00 100.00 79.64 54.39 82.89 39.09 72.00 84.01 54.43
100 16.0072.00 76.00 100.00 64.00 100.00 96.00 74.31 42.05 68.88 36.94 70.91 86.28 65.11
Voltron-B 200 32.0072.00 76.00 100.00 60.00 96.00 88.00 71.57 38.17 67.53 31.01 70.17 86.61 62.39
300 20.0068.00 72.00 100.00 52.00 96.00 84.00 71.25 36.50 66.14 30.11 69.88 86.16 59.02
100 24.0088.00 88.00 100.00 84.00 96.00 96.00 88.28 42.55 95.18 44.08 69.26 85.63 55.68
MAE-B 200 28.0076.00 84.00 100.00 84.00 88.00 88.00 77.13 38.49 88.22 32.75 69.02 85.14 56.81
300 28.0072.00 76.00 100.00 80.00 88.00 80.00 75.60 36.93 78.35 31.03 69.01 84.11 57.30
100 8.00 60.00 40.00 100.00 44.00 96.00 20.00 45.95 16.61 63.57 13.38 60.11 74.07 36.29
DINOV2-B200 8.00 68.00 40.00 100.00 64.00 88.00 16.00 37.96 15.81 51.44 12.59 59.56 74.18 32.14
300 12.0064.00 48.00 100.00 64.00 88.00 4.00 32.43 14.31 36.01 11.67 56.54 73.77 36.53
100 20.0076.00 76.00 100.00 76.00 100.00 76.00 72.35 43.14 92.77 27.31 68.67 84.19 62.00
VC-1-B 200 32.0080.00 68.00 100.00 76.00 100.00 92.00 81.83 44.05 83.62 27.80 70.98 83.88 59.63
300 24.0068.00 80.00 100.00 80.00 88.00 88.00 83.01 41.25 77.60 28.53 70.89 84.76 59.51
100 28.0076.00 48.00 100.00 72.00 100.00 84.00 87.84 62.72 96.53 15.71 67.88 85.70 57.52
RADIO 200 36.0076.00 44.00 100.00 72.00 96.00 52.00 80.26 57.39 95.93 15.26 67.51 85.67 57.81
300 44.0072.00 32.00 100.00 40.00 92.00 48.00 79.62 53.51 89.16 14.80 66.57 85.64 58.14
100 32.0084.00 64.00 100.00 84.00 96.00 96.00 71.47 53.41 93.01 50.19 70.75 87.17 46.97
E-RADIO 200 32.0084.00 60.00 100.00 68.00 88.00 96.00 68.80 44.56 88.54 33.19 70.46 87.39 50.72
300 28.0080.00 60.00 100.00 72.00 88.00 80.00 65.96 33.56 98.14 32.64 69.18 87.09 51.31
100 20.0084.00 84.00 100.00 88.00 100.00 100.00 80.50 45.08 91.38 48.90 71.16 86.04 59.03
SPA-B 200 28.0080.00 68.00 100.00 84.00 100.00 84.00 79.71 46.65 85.75 40.84 71.01 86.16 60.05
300 24.0080.00 88.00 100.00 92.00 100.00 92.00 74.70 48.97 81.60 34.92 71.16 85.16 61.94
20Preprint
Table10: AllresultsonFrankaKitchen.
InternViT-
Task View Seed MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA
300M
100 86.00 76.00 84.00 72.00 78.00 74.00 66.00 74.00 84.00
Left 200 78.00 78.00 74.00 72.00 76.00 72.00 58.00 74.00 92.00
300 80.00 80.00 78.00 62.00 82.00 70.00 64.00 74.00 80.00
Task1
100 82.00 80.00 86.00 78.00 78.00 72.00 82.00 78.00 86.00
Right 200 88.00 62.00 90.00 70.00 86.00 86.00 86.00 84.00 72.00
300 86.00 82.00 92.00 82.00 86.00 76.00 92.00 78.00 86.00
100 60.00 56.00 48.00 26.00 40.00 22.00 40.00 32.00 48.00
Left 200 64.00 60.00 46.00 44.00 40.00 32.00 32.00 42.00 60.00
300 58.00 54.00 40.00 26.00 32.00 34.00 30.00 50.00 66.00
Task2
100 62.00 54.00 56.00 26.00 44.00 26.00 32.00 54.00 48.00
Right 200 64.00 54.00 60.00 36.00 40.00 24.00 28.00 56.00 42.00
300 64.00 52.00 50.00 38.00 40.00 30.00 34.00 44.00 42.00
100 16.00 24.00 18.00 18.00 22.00 24.00 6.00 24.00 28.00
Left 200 28.00 20.00 14.00 18.00 20.00 16.00 6.00 30.00 38.00
300 22.00 16.00 14.00 10.00 26.00 22.00 8.00 26.00 30.00
Task3
100 46.00 26.00 38.00 22.00 14.00 8.00 32.00 12.00 10.00
Right 200 48.00 22.00 38.00 24.00 18.00 4.00 32.00 12.00 12.00
300 54.00 34.00 52.00 14.00 12.00 6.00 26.00 14.00 16.00
100 32.00 36.00 26.00 22.00 34.00 12.00 16.00 36.00 22.00
Left 200 30.00 30.00 32.00 14.00 20.00 8.00 14.00 24.00 10.00
300 24.00 46.00 28.00 14.00 32.00 4.00 20.00 36.00 16.00
Task4
100 38.00 24.00 28.00 22.00 32.00 12.00 26.00 12.00 30.00
Right 200 42.00 24.00 24.00 24.00 24.00 12.00 30.00 8.00 38.00
300 46.00 16.00 32.00 28.00 36.00 16.00 26.00 12.00 30.00
100 36.00 18.00 8.00 16.00 24.00 22.00 26.00 28.00 20.00
Left 200 30.00 24.00 8.00 10.00 24.00 16.00 20.00 22.00 16.00
300 22.00 22.00 10.00 12.00 16.00 14.00 28.00 30.00 18.00
Task5
100 24.00 46.00 20.00 4.00 14.00 10.00 26.00 22.00 30.00
Right 200 24.00 30.00 16.00 8.00 18.00 18.00 22.00 22.00 26.00
300 14.00 36.00 16.00 12.00 10.00 12.00 20.00 16.00 22.00
21Preprint
Table11: AllresultsonMeta-World.
InternViT-
Method MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA
300M
Seed 100200300100200300100200300100200300100200300100200300100200300100200300100200300
ButtonPressWall 100100100100100100 95 90 95 100100100 95 95100 9510095 100100100100100100100100100
DoorClose 100100100100100100100100100100100100100100100100100100100100100100100100100100100
DoorUnlock 70 65 80 70 65 85 35 35 30 60 50 60 85 85 90 80 65 75 75 70 85 80 75 90 80 75 80
DrawerClose 100100100100100100100100100100100100100100100100100100100100100100100100100100100
DrawerOpen 80 70 85 85 65 75 60 40 60 90 80 80 60 55 75 70 75 75 95 75 85 85 85 80 90 60 75
FaucetClose 70 80 55 60 80 60 55 65 35 70 80 50 65 75 60 50 65 50 70 75 60 65 75100 70 80 65
PlateSlide 90 95100 95100100 65 70 80 85 95 80 100100100 85 95 90 10010010010095100 95 95 95
PlateSlideBack 80 65 85 85 65 85 90 75 90 85 75 90 80 65 90 85 80 90 85 70 90 80 70 90 80 70 85
PlateSlideSide 85 90 95 95 90 95 90 85 85 80 85 95 10095100 90 95 90 80 90 95 10095 90 90 90 95
WindowClose 100100100100100100 70 9010010010095 95100100100100100100100100100100100100100100
Basketball 85 95 95 95100100 70 55 65 85 85 70 95 85 80 90 80 95 9510095 9010095 95100100
BinPicking 30 45 40 20 10 35 10 15 10 30 30 25 20 5 45 10 10 10 25 20 15 30 30 30 40 25 30
BoxClose 80 80 80 75 80 70 35 45 30 80 70 60 55 70 55 60 65 40 80 80 65 80 95 60 80 80 65
CoffeePush 45 50 40 40 55 30 30 25 15 45 40 55 45 35 40 45 30 25 25 45 25 30 45 55 35 40 30
Assembly 70 60 55 55 65 45 30 35 25 45 55 50 45 50 40 30 25 30 60 60 45 60 60 50 50 55 50
Disassemble 40 55 50 30 45 45 30 20 45 55 50 60 30 50 50 40 45 50 40 45 45 40 30 35 40 45 55
PushWall 25 35 30 20 30 40 40 30 45 35 30 35 25 35 30 15 15 25 30 35 35 55 55 60 30 40 45
ShelfPlace 35 35 20 25 45 20 30 30 35 30 35 30 25 20 15 15 15 15 25 25 15 25 15 15 15 35 15
DoorOpen 95 90 90 10095 95 95 80 95 85 75 90 80 90100 50 55 60 80 75 95 95 95 95 8510095
ButtonPress 75 85 85 85100100 80 95 85 55 70 75 80 90 90 85 90 80 85 90 95 80 90 85 10095100
SweepInto 45 45 40 55 55 45 50 50 40 45 45 40 45 25 30 35 25 30 45 50 40 55 50 50 50 50 45
DoorLock 10085 85 90100100 95 90 85 85 85 75 9510095 80 90 95 8510095 10010090 80 95 85
ReachWall 70 70 80 75 85 70 85 80 85 90 90 80 65 75 85 60 55 55 75 65 75 75 80 80 75 80 70
Hammer 25 45 30 30 30 30 40 45 35 25 35 25 30 35 20 20 20 20 30 35 30 45 40 55 30 40 30
StickPush 95 95100 80 90 95 90 90 90 75 85 85 90 85100 60 80 85 90 95 90 85 85 95 90 90 85
ButtonPressTopdown 80 80 75 80 85 80 80 90 90 80 90 70 55 65 55 45 55 55 80 85 80 75 80 70 80 85 80
HandlePressSide 100100100100100100 9510090 8010090 10010095 8510090 95100100 7510080 90100100
PlateSlideBackSide 10010010010095 95 100100100100100100100100100100100100100100100100100100100100100
Sweep 50 80 70 35 60 60 65 85 95 60 85 75 35 50 55 15 60 35 35 70 65 35 65 65 30 65 55
ButtonPressTopdownWall 45 70 80 45 75 75 70 75 75 45 60 70 30 45 65 20 55 45 30 60 70 50 85 80 45 65 75
HandlePress 85 95 95 8010075 7510080 9010090 8510085 65 90 75 80 95 75 8510090 8510080
Push 25 30 30 25 30 30 30 25 40 25 15 30 30 25 20 25 20 20 25 20 25 40 30 25 30 15 35
CoffeePull 55 55 55 40 45 40 20 30 20 50 70 40 40 45 45 40 40 25 55 55 40 55 55 60 55 55 55
DialTurn 80 65 80 85 75 75 40 30 35 80 95 90 65 65 80 70 70 55 70 65 75 80 95 75 85 85 75
Reach 90 75 80 90 75 80 70 75 85 95 95100 85 80 75 95 80 90 80 70 75 70 80 80 85 70 85
CoffeeButton 85 95 75 10010095 85 80 60 9510085 9010095 90 85 85 10010090 9010080 100100100
PickPlaceWall 45 35 65 40 25 45 15 10 20 35 40 50 20 25 35 30 25 25 25 35 40 35 20 45 40 35 55
StickPull 35 35 25 15 40 20 25 10 5 45 40 45 25 35 15 25 25 15 15 30 25 30 30 30 25 35 30
HandInsert 35 30 30 30 25 25 20 20 20 45 45 40 20 25 20 25 30 25 40 40 35 40 30 40 40 50 40
PegInsertSide 40 35 40 50 35 45 25 15 10 45 30 20 45 25 30 30 20 25 45 45 30 50 25 35 55 45 60
PickPlace 35 30 30 25 45 15 15 10 10 25 15 30 25 30 25 30 10 30 20 35 25 25 20 20 25 30 40
FaucetOpen 95 95100100100100 80 8010010095100 95 95100 80 85 95 100100100100100100 95100100
PushBack 65 70 60 40 55 40 15 15 25 30 45 25 35 35 45 15 35 25 40 45 45 45 55 25 35 55 50
LeverPull 70 80 80 80 70 75 15 30 35 55 85 80 70 65 70 60 55 55 65 80 65 65 80 70 85 70 80
HandlePull 85 85 80 85 80 85 45 55 40 75 75 80 80 60 65 45 70 65 80 90 80 70 85 75 10090 90
Soccer 25 40 35 50 30 25 15 10 15 45 40 30 35 35 25 20 20 30 30 30 35 20 20 20 25 50 25
WindowOpen 65 80 80 55 80 85 60 50 65 50 65 60 65 80 75 55 75 75 60 70 70 60 70 75 55 85 65
PickOutOfHole 65 75 80 65 65 60 60 55 60 70 70 50 60 55 50 60 55 55 65 55 60 70 75 60 65 60 75
22Preprint
Table12: AllresultsonRLBench.
InternViT
Method MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA
300M
Group1
basketballinhoop 100 100 100 100 100 100 100 100 100
putrubbishinbin 100 100 96 96 96 100 96 100 100
meatoffgrill 100 100 100 100 100 100 100 100 100
meatongrill 80 76 76 68 80 72 68 76 80
slideblocktotarget 0 84 96 24 4 0 100 100 4
reachanddrag 100 96 88 100 96 100 96 100 100
takeframeoffhanger 88 88 92 88 84 84 88 88 96
waterplants 64 60 28 64 60 44 52 60 68
hangframeonhanger 8 4 0 4 8 8 12 4 4
wipedesk 0 0 0 0 0 0 0 0 0
stackblocks 60 72 72 68 56 60 84 68 68
reachtarget 60 96 88 100 96 80 92 96 92
pushbutton 100 100 100 100 100 100 100 100 100
lampon 88 68 84 88 52 80 28 88 64
toiletseatdown 100 100 100 100 100 100 96 96 100
closelaptoplid 96 96 96 96 84 80 80 96 100
openbox 12 12 20 4 16 4 0 12 16
opendrawer 88 96 92 100 88 88 92 96 96
pickupcup 92 92 88 96 96 88 96 96 96
turntap 88 84 84 96 88 92 96 100 100
takeusboutofcomputer 100 100 100 100 100 100 100 88 100
playjenga 96 96 96 100 96 100 96 96 96
insertontosquarepeg 28 84 80 44 88 40 64 92 84
takeumbrellaoutofumbrellastand 92 100 100 92 100 96 100 100 100
insertusbincomputer 12 20 20 24 24 20 16 8 68
straightenrope 56 44 72 80 48 72 52 60 84
turnovenon 96 96 96 96 96 96 100 100 100
changeclock 64 68 48 68 64 72 64 60 68
closemicrowave 100 100 100 100 100 100 100 100 100
closefridge 80 92 92 88 92 96 88 92 100
closegrill 96 96 96 96 96 96 100 100 96
opengrill 100 100 100 100 100 100 96 100 100
unplugcharger 44 32 48 36 48 40 40 44 44
pressswitch 92 92 88 72 76 84 76 88 92
takemoneyoutsafe 100 96 100 100 100 100 100 100 100
Group2
changechannel 0 8 4 0 0 4 0 0 4
tvon 4 8 0 4 4 8 4 4 8
pushbuttons 12 4 4 0 0 0 0 12 4
stackwine 12 16 40 4 12 0 28 8 28
scoopwithspatula 0 0 0 0 0 0 0 0 0
placehangeronrack 0 0 0 0 0 0 0 0 0
movehanger 0 0 0 0 0 0 0 0 0
sweeptodustpan 92 96 96 96 92 100 100 88 96
takeplateoffcoloreddishrack 96 100 96 92 84 96 88 92 96
screwnail 52 36 36 36 36 52 32 32 48
takeshoesoutofbox 20 28 24 36 40 12 32 36 36
slidecabinetopenandplacecups 0 0 0 0 0 4 0 0 4
lampoff 100 96 96 100 96 96 100 100 100
pickandlift 88 96 92 96 92 80 96 96 96
takelidoffsaucepan 100 100 100 100 100 100 100 100 100
closedrawer 100 100 100 100 96 100 100 100 100
closebox 92 92 96 96 100 96 100 96 100
phoneonbase 100 100 100 100 100 96 100 100 100
toiletseatup 80 88 100 88 88 80 88 92 96
putbooksonbookshelf 12 24 24 28 28 20 20 28 16
beatthebuzz 88 92 96 88 92 84 88 88 100
stackcups 40 56 52 52 48 56 64 68 64
putknifeonchoppingboard 72 76 68 72 80 88 80 76 80
placeshapeinshapesorter 20 36 32 28 36 20 44 36 56
taketoiletrolloffstand 100 92 76 96 92 88 84 92 96
putumbrellainumbrellastand 8 0 12 12 0 4 12 8 12
setupcheckers 76 80 68 68 88 92 92 80 80
openwindow 96 96 100 100 96 100 96 100 100
openwinebottle 80 100 88 92 92 88 96 88 88
openmicrowave 100 100 88 96 100 80 96 100 100
putmoneyinsafe 96 100 88 92 100 96 100 100 100
opendoor 100 96 96 96 96 96 84 96 96
closedoor 32 68 56 60 80 20 24 20 60
openfridge 44 52 48 44 36 64 52 32 64
openoven 8 4 12 8 4 20 4 4 16
plugchargerinpowersupply 32 36 32 24 44 36 24 32 60
23Preprint
Table13: AllresultsonLIBERO-OBJECT,LIBERO-SPATIAL,LIBERO-GOAL,LIBERO-10.
InternViT- InternViT-
MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA
300M 6B
Seed100200300 100200300 100200300 100200300 100200300 100200300 100200300 100200300 100200300 100200300
LIBERO-OBJECT
0 0.650.600.650.650.450.550.650.800.850.800.750.651.000.700.950.800.650.600.700.850.500.800.900.650.800.500.600.900.950.95
1 0.350.350.550.900.750.800.300.500.750.400.300.050.650.300.700.150.400.200.600.250.450.050.800.600.400.650.450.650.700.45
2 0.900.850.950.900.400.950.850.500.900.700.800.750.850.750.750.900.850.800.850.450.850.800.850.901.000.950.950.900.950.80
3 0.550.700.650.900.150.900.300.650.900.250.450.600.800.800.900.750.700.401.000.500.550.700.650.850.950.750.600.700.900.90
4 0.650.850.850.800.900.750.750.550.750.350.750.650.950.751.000.901.000.850.900.700.800.800.750.700.900.850.900.901.000.95
5 0.500.700.800.700.350.600.550.750.600.250.700.450.750.750.650.850.600.750.600.350.500.550.400.800.650.700.700.250.150.65
6 0.350.500.650.600.650.650.550.700.700.350.550.600.400.350.250.650.600.550.300.100.350.250.500.650.500.500.300.500.700.80
7 0.750.750.800.900.400.750.550.300.700.400.350.400.550.750.700.800.400.600.600.650.700.600.450.650.800.750.500.800.750.65
8 0.500.950.901.000.951.000.500.350.500.450.450.351.000.750.850.700.650.750.500.400.750.650.550.700.800.900.500.850.950.90
9 0.450.500.400.600.650.950.800.900.950.500.700.300.850.750.750.850.950.650.900.600.150.650.600.300.700.700.650.600.950.90
LIBERO-SPATIAL
0 0.350.550.450.450.400.700.650.500.600.250.200.350.700.750.650.550.650.550.550.500.300.750.750.600.350.550.600.450.500.35
1 0.650.700.700.800.800.500.550.300.350.750.750.700.550.700.250.350.500.501.001.000.900.600.400.600.450.650.800.650.650.85
2 0.550.500.500.350.600.400.200.050.550.100.000.400.700.800.500.700.750.600.750.600.200.850.550.750.450.450.700.500.500.40
3 0.500.700.750.550.600.750.800.700.950.150.400.300.850.900.850.350.500.400.400.300.150.950.550.600.500.700.650.550.850.60
4 0.150.150.200.550.700.800.500.050.450.350.300.200.450.550.400.350.250.400.250.150.150.600.500.700.600.600.800.700.700.50
5 0.450.100.100.650.400.300.300.200.350.550.450.450.650.500.450.400.300.700.550.600.600.550.300.250.050.050.150.350.350.30
6 0.300.350.450.550.250.950.400.300.400.200.250.100.750.700.850.450.550.550.400.350.050.450.750.650.600.700.350.350.450.30
7 0.100.200.250.500.350.450.050.000.100.300.150.250.300.300.200.300.600.650.150.050.050.600.650.600.100.200.400.400.150.45
8 0.550.700.500.350.650.700.400.150.550.300.550.300.850.700.600.300.550.600.650.400.350.700.400.550.700.600.700.750.700.40
9 0.550.050.100.850.750.500.200.050.250.200.200.200.550.500.300.350.500.300.450.400.350.450.450.300.500.550.650.350.500.45
LIBERO-GOAL
0 0.450.700.750.700.850.800.150.100.300.250.400.350.700.600.600.750.650.750.250.350.350.750.600.950.450.851.000.851.000.85
1 0.700.600.800.650.500.900.250.550.250.200.150.250.700.800.800.900.901.000.400.150.150.900.800.950.650.650.651.000.850.90
2 0.500.200.150.100.400.350.100.050.150.300.250.300.650.750.750.400.750.450.500.350.350.450.250.650.400.600.350.500.550.35
3 0.750.450.600.400.750.550.200.100.100.050.200.550.300.150.150.300.500.650.200.250.250.700.700.150.750.550.500.650.350.80
4 0.200.250.050.350.400.250.100.000.050.400.300.150.150.100.100.200.150.100.150.200.200.550.600.250.150.300.300.300.350.35
5 0.100.750.800.600.850.800.650.500.500.350.450.500.800.750.750.700.550.450.550.450.450.800.750.850.650.750.800.800.650.65
6 0.450.050.150.000.100.050.000.050.000.100.100.000.000.650.650.500.400.300.000.000.000.150.350.700.250.500.450.400.300.35
7 0.250.750.900.800.651.000.450.450.350.500.650.801.001.001.001.001.000.950.700.850.850.951.000.951.000.950.700.951.001.00
8 0.500.800.750.850.550.900.450.400.200.600.250.500.900.350.350.950.650.550.650.250.250.700.650.700.500.750.550.800.650.80
9 0.100.650.600.500.200.500.100.000.100.100.100.000.150.700.700.600.400.200.150.350.350.300.500.550.200.350.700.550.600.45
LIBERO-10
0 0.150.200.100.150.250.100.000.050.100.000.050.050.250.350.100.350.100.250.150.150.000.050.150.200.100.450.250.050.100.05
1 0.250.200.200.300.150.250.150.150.150.400.300.150.650.100.600.150.500.450.000.250.350.150.100.150.200.400.150.250.050.45
2 0.700.600.750.300.600.750.550.450.500.250.450.400.750.550.650.450.800.550.700.800.750.750.650.550.851.000.900.700.800.50
3 0.500.800.550.550.600.800.400.450.450.600.650.600.800.900.750.750.650.500.750.600.600.750.700.650.800.700.700.700.900.70
4 0.250.200.050.350.250.300.100.100.050.200.050.050.150.100.150.150.150.050.150.200.150.250.200.350.300.250.300.400.300.25
5 0.400.600.750.550.700.800.500.650.750.400.400.300.850.650.750.750.550.750.450.550.450.600.700.750.800.900.600.700.700.45
6 0.200.250.100.400.350.400.050.200.050.250.150.150.200.300.350.100.150.200.200.250.050.400.300.350.300.100.200.200.200.15
7 0.400.300.250.500.500.500.100.300.600.300.400.200.450.300.250.400.350.350.350.700.250.350.300.250.300.250.300.500.450.40
8 0.100.100.150.100.300.200.350.200.050.100.100.100.150.200.100.200.000.250.050.200.050.100.300.200.250.300.250.250.050.15
9 0.200.600.350.400.650.300.350.250.450.450.450.300.400.700.500.500.450.600.500.250.400.400.550.500.000.000.000.500.650.50
24Preprint
Table14: AllresultsonLIBERO-90.
InternViT- InternViT-
MoCoV3 MAE DINOV2 CLIP EVA MVP VC-1 SPA
300M 6B
Seed 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300 100 200 300
LIBERO-90
0 0.950.850.901.000.900.800.801.000.600.900.800.801.001.001.000.900.800.850.750.800.950.950.951.000.951.000.951.001.000.95
1 0.600.350.600.350.500.150.500.550.300.400.650.350.700.500.250.300.450.400.250.350.550.800.550.300.400.500.050.650.400.50
2 0.850.500.800.550.550.200.650.600.300.450.300.500.350.500.700.850.650.800.250.350.300.450.700.700.750.550.350.700.850.60
3 0.100.100.000.050.000.000.050.000.000.150.000.000.000.100.150.000.050.050.000.000.050.100.000.050.050.100.000.100.100.00
4 0.400.050.200.300.250.300.150.400.550.400.400.350.100.250.150.400.050.250.200.450.400.300.400.150.250.050.150.150.150.35
5 0.050.050.050.100.050.200.000.200.000.050.050.200.250.250.100.100.050.300.200.100.250.050.150.100.100.050.050.350.000.15
6 0.100.000.000.000.000.050.050.050.100.050.100.000.000.000.050.000.000.000.050.100.100.000.050.000.000.000.000.000.050.05
7 0.350.300.650.200.600.300.350.250.400.500.600.350.600.100.200.150.250.100.400.250.450.500.200.400.200.300.250.300.300.65
8 0.100.150.000.050.200.100.150.250.100.200.100.100.100.050.000.050.000.100.200.150.200.100.000.200.050.150.200.050.050.15
9 0.300.250.350.500.250.300.350.600.700.250.200.500.250.100.500.100.100.250.600.250.300.250.150.450.250.050.350.250.200.25
10 0.500.750.500.500.600.550.650.600.600.900.450.550.400.850.350.350.050.250.450.450.650.400.500.550.450.750.400.400.350.35
11 0.450.350.750.450.700.650.350.200.150.400.700.550.800.250.700.500.500.100.350.250.450.800.600.950.700.750.600.600.600.65
12 0.150.150.100.150.150.050.200.200.150.100.050.050.100.250.050.050.000.000.250.300.100.150.100.100.200.250.100.050.100.15
13 0.200.350.300.150.300.200.300.350.100.300.400.350.300.100.450.200.350.400.250.150.550.300.300.150.450.100.100.100.200.10
14 0.050.100.000.300.300.200.100.100.150.150.400.200.250.350.100.150.150.050.200.150.100.200.350.100.200.100.200.150.150.10
15 0.600.750.450.700.500.650.350.500.550.450.650.400.700.750.400.400.650.500.350.550.450.700.600.550.800.800.700.650.800.55
16 0.050.200.000.300.150.050.100.100.050.100.000.100.200.200.150.150.150.200.050.000.100.150.050.100.000.150.150.050.100.15
17 0.050.150.150.100.250.050.050.100.050.050.000.050.050.200.150.100.100.150.000.100.000.200.100.200.150.100.000.250.100.10
18 0.450.400.600.400.750.650.300.350.400.450.250.350.250.350.600.400.050.700.600.500.350.350.250.450.300.600.350.600.350.55
19 0.300.300.250.350.400.200.200.050.350.450.450.300.300.350.250.150.250.200.350.300.150.550.300.400.400.450.350.450.200.35
20 0.850.750.801.001.000.950.951.001.000.750.850.301.001.001.000.950.950.901.000.500.800.901.001.001.001.001.001.001.001.00
21 0.400.200.400.350.250.300.250.400.200.250.100.450.300.700.050.000.050.100.350.100.300.400.150.300.300.700.600.650.400.60
22 0.900.950.951.000.850.950.250.600.400.750.750.750.951.000.950.850.950.600.450.250.250.901.001.000.900.900.951.000.951.00
23 0.150.050.150.050.100.000.050.100.050.000.000.050.000.000.000.000.000.000.200.250.050.000.000.000.000.000.000.000.000.00
24 0.800.300.850.850.500.800.600.500.650.700.450.600.700.700.800.400.650.600.550.800.450.650.600.900.900.800.800.900.800.75
25 1.000.800.851.001.000.900.750.900.900.800.950.900.901.000.950.700.700.850.950.600.651.000.851.001.001.000.901.001.001.00
26 0.150.200.250.250.400.400.050.300.400.450.050.150.050.300.150.250.400.200.250.150.200.200.300.600.250.450.250.250.200.20
27 0.300.150.200.350.350.100.050.100.000.350.050.100.050.200.050.050.150.000.100.100.050.350.200.300.100.450.400.100.400.20
28 0.900.901.000.950.700.700.800.500.800.900.750.900.951.000.850.900.851.000.500.600.450.850.750.900.750.950.600.900.650.90
29 0.150.500.350.600.550.200.500.500.400.500.500.300.650.300.300.150.300.400.250.350.250.350.500.250.600.600.100.300.350.70
30 0.150.250.150.600.350.350.350.100.500.250.200.450.300.700.200.100.150.200.500.200.250.000.050.200.100.250.100.400.250.40
31 0.700.600.800.700.750.600.450.700.750.950.650.950.800.750.450.500.550.350.950.800.850.800.700.750.750.750.450.700.900.80
32 0.300.050.200.050.100.050.000.000.050.350.100.100.200.100.150.100.150.100.050.150.050.200.250.100.050.100.050.200.100.05
33 0.500.550.400.150.300.300.100.200.350.300.250.300.000.500.400.350.200.250.250.250.300.200.350.400.350.450.650.150.150.15
34 0.300.350.300.400.400.250.350.400.150.400.400.500.100.400.100.150.050.250.350.300.500.250.300.300.550.250.050.050.300.10
35 0.650.400.600.850.950.750.800.800.600.650.550.900.901.000.800.100.200.550.850.750.700.850.800.850.250.800.551.000.701.00
36 0.050.100.150.050.000.000.000.250.050.050.050.000.150.200.100.150.100.250.000.000.100.200.000.150.300.050.200.050.200.10
37 0.350.300.400.550.200.250.650.500.350.300.600.600.700.800.600.450.550.450.500.550.550.450.450.500.450.750.350.750.500.55
38 0.500.300.450.550.500.350.250.150.300.500.450.300.500.200.350.350.550.450.350.500.350.700.400.550.700.650.300.700.450.45
39 0.800.800.750.450.700.600.600.550.650.600.650.700.650.850.550.600.150.600.650.600.600.450.400.700.300.650.350.600.550.60
40 0.400.500.200.400.300.400.300.450.550.500.250.300.700.650.300.250.600.250.550.400.250.650.700.800.350.450.250.550.350.30
41 0.200.600.400.500.450.600.350.250.550.200.450.500.850.450.500.450.650.450.200.300.350.550.800.200.350.500.650.650.800.40
42 0.200.400.200.350.150.050.300.300.550.350.050.200.700.450.200.400.650.600.250.100.050.300.500.650.350.600.500.600.600.40
43 0.500.500.500.350.400.300.350.350.150.350.250.450.200.600.500.350.350.050.300.300.150.300.600.400.550.400.200.550.700.40
44 0.900.900.951.001.000.850.800.900.950.750.850.850.950.801.001.000.950.850.850.650.950.850.950.900.950.950.950.951.000.95
45 0.550.400.600.800.750.600.450.550.600.700.700.550.450.400.300.550.150.550.400.650.300.650.500.350.550.650.450.600.750.60
46 0.000.000.000.050.350.100.000.150.000.100.100.050.050.100.000.000.000.100.100.000.050.050.050.200.000.050.100.100.000.10
47 0.450.350.250.250.200.200.100.200.150.600.250.450.450.300.300.150.300.300.150.350.200.400.400.300.200.250.100.550.450.30
48 0.000.150.300.100.000.150.200.050.200.100.100.250.050.100.250.150.000.250.200.100.200.050.100.100.150.200.100.100.050.20
49 0.050.050.250.250.300.150.100.200.250.250.400.300.150.250.050.050.000.150.250.300.000.000.000.150.150.050.250.150.050.10
50 0.250.050.250.000.050.100.000.000.000.000.000.000.000.050.100.050.050.000.000.050.000.050.050.050.100.000.000.050.150.05
51 0.050.150.050.100.150.050.150.150.200.150.200.150.150.100.050.050.200.050.350.150.350.150.200.300.150.000.100.050.100.10
52 0.000.000.000.000.000.050.050.100.000.000.050.050.100.000.150.100.000.000.050.050.000.050.150.100.000.000.100.150.000.05
53 0.050.050.100.150.000.050.200.200.050.000.100.200.050.250.050.050.000.000.250.150.250.000.000.000.050.150.100.100.050.15
54 0.050.200.050.050.050.000.050.050.050.250.050.050.050.050.100.200.100.000.200.200.050.050.000.000.000.000.000.050.150.05
55 0.100.050.050.000.000.000.100.050.000.050.000.000.000.000.050.000.000.000.050.050.050.000.050.050.000.000.000.000.100.00
56 0.050.150.100.100.100.050.150.150.300.200.100.250.100.100.050.100.200.000.250.150.100.050.000.050.200.150.100.100.150.20
57 0.100.000.050.100.100.050.050.200.000.150.100.050.100.150.000.050.050.100.200.250.100.000.000.200.100.000.200.200.050.05
58 0.050.100.200.000.050.100.100.250.100.250.200.150.200.150.150.050.050.150.050.150.050.150.100.100.050.100.200.150.050.10
59 0.050.100.100.050.100.100.050.150.000.050.100.050.050.100.250.050.150.250.050.100.050.100.000.100.150.150.000.000.200.05
60 0.550.650.700.300.600.450.500.500.400.400.350.650.200.750.550.350.350.500.400.400.500.450.500.750.200.500.550.450.650.90
61 0.050.150.050.000.100.000.000.100.050.050.100.100.050.100.050.000.000.100.150.050.200.100.150.200.050.150.050.150.050.15
62 0.450.400.400.450.200.200.250.300.250.550.150.700.400.500.450.300.100.500.400.150.050.200.150.350.250.350.500.400.100.35
63 0.100.000.050.150.050.000.150.200.150.050.000.000.000.000.000.000.000.000.000.000.000.000.000.100.000.000.000.000.000.00
64 0.100.050.050.000.050.000.000.050.050.000.050.150.000.000.000.000.000.000.050.050.000.000.000.000.000.000.000.100.050.00
65 0.050.000.150.200.050.100.000.050.050.100.050.050.050.000.050.000.050.050.050.100.100.100.000.100.150.050.050.050.050.05
66 0.200.350.050.050.300.150.400.400.500.200.150.250.100.200.050.250.100.250.150.200.300.200.200.200.100.100.200.350.400.25
67 0.000.000.050.050.000.150.050.050.050.050.000.050.050.000.000.100.050.000.000.050.000.000.100.000.050.100.100.000.000.00
68 0.050.400.200.300.250.150.300.400.600.250.200.250.200.250.150.200.250.150.400.500.400.350.300.350.150.150.350.300.150.30
69 0.150.000.200.100.250.050.000.000.000.100.050.100.300.050.500.200.200.000.100.200.100.300.150.250.300.300.100.250.350.10
70 0.200.450.050.200.200.200.300.350.200.200.300.250.200.200.300.200.300.250.200.200.200.250.550.400.250.150.500.400.200.40
71 0.350.250.100.200.200.350.350.200.150.150.300.050.150.150.300.250.100.300.350.050.450.350.100.150.050.100.250.200.450.30
72 0.100.350.250.050.100.400.050.050.250.050.200.100.200.350.350.100.100.300.050.200.050.250.250.500.150.400.300.050.300.25
73 0.200.050.250.100.050.150.050.000.100.200.150.150.000.150.000.250.100.250.150.050.150.050.050.250.200.250.050.100.100.30
74 0.300.400.250.200.250.400.200.300.150.400.150.250.350.300.250.150.100.250.450.350.150.400.250.450.400.500.200.500.150.25
75 0.200.300.150.200.100.250.100.200.100.050.200.150.200.200.200.250.150.200.100.100.100.200.350.550.200.200.150.100.200.25
76 0.150.150.400.750.500.200.400.400.350.450.250.300.000.300.050.000.000.000.600.200.300.050.250.400.100.050.100.100.050.05
77 0.350.350.100.050.050.300.250.500.350.150.000.400.150.150.500.300.000.100.150.200.200.150.250.300.550.450.550.350.150.40
78 0.100.200.250.000.200.150.100.000.000.100.100.050.100.400.200.050.000.100.050.050.000.200.050.050.200.300.100.150.000.25
79 0.150.400.350.250.200.400.150.450.400.350.350.200.350.250.350.350.000.200.300.150.300.250.150.600.500.650.250.350.050.25
80 0.150.300.400.200.400.450.250.200.350.100.050.150.400.200.200.200.000.150.300.250.500.250.400.450.600.200.300.400.000.30
81 0.150.100.150.000.050.200.250.150.050.100.050.100.050.200.050.150.000.050.000.000.050.250.250.250.000.150.150.200.250.20
82 0.200.350.450.550.300.450.150.450.250.400.600.650.700.450.600.200.000.150.350.450.500.250.450.650.450.650.150.400.300.45
83 0.100.350.400.100.150.200.100.150.100.300.000.100.200.150.200.050.100.150.400.200.150.150.000.250.150.250.200.150.100.20
84 0.050.200.200.150.250.050.200.150.400.350.200.100.300.200.050.050.050.100.500.250.200.200.050.200.200.050.200.050.150.00
85 0.050.000.050.000.100.050.050.200.000.200.100.150.000.050.050.000.000.000.200.100.050.000.000.050.100.050.000.050.050.00
86 0.050.250.150.200.600.250.000.050.000.150.250.300.250.250.300.200.050.150.150.200.100.100.000.450.050.200.050.400.300.30
87 0.400.650.650.250.550.400.300.200.200.400.300.350.550.650.550.250.500.400.450.300.300.300.450.900.350.650.700.500.450.55
88 0.400.200.550.200.550.350.450.650.650.550.600.550.450.500.500.400.200.500.550.250.200.750.450.400.350.650.300.450.550.50
89 0.100.000.050.050.100.100.050.200.050.200.050.050.000.100.000.100.250.050.100.150.050.000.250.300.300.050.100.200.200.10
25