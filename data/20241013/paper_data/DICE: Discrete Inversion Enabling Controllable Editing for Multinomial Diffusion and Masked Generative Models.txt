Preprint
DICE: DISCRETE INVERSION ENABLING CONT-
ROLLABLE EDITING FOR MULTINOMIAL DIFFUSION
AND MASKED GENERATIVE MODELS
XiaoxiaoHe1,LigongHan12â€ ,QuanDao1,SongWen1,MinhaoBai1,DiLiu1,HanZhang3,
MartinRenqiangMin4,FelixJuefei-Xu5,ChaoweiTan1,BoLiu6,KangLi1,HongdongLi7,
JunzhouHuang8,FaezAhmed9,AkashSrivastava2&DimitrisN.Metaxas1
1RutgersUniversity 2MIT-IBMWatsonAILab 3GoogleDeepMind 4NECLabsAmerica
5NYU 6WalmartGlobalTech 7ANU 8UTArlington 9MassachusettsInstituteofTechnology
â€ ProjectLead&CorrespondingAuthor Project: [Website]
ABSTRACT
Discrete diffusion models have achieved success in tasks like image generation
andmaskedlanguagemodelingbutfacelimitationsincontrolledcontentediting.
We introduce DICE (Discrete Inversion for Controllable Editing), the first ap-
proachtoenablepreciseinversionfordiscretediffusionmodels,includingmulti-
nomial diffusion and masked generative models. By recording noise sequences
and masking patterns during the reverse diffusion process, DICE enables accu-
ratereconstructionandflexibleeditingofdiscretedatawithouttheneedforpre-
defined masks or attention manipulation. We demonstrate the effectiveness of
DICEacrossbothimageandtextdomains, evaluatingitonmodelssuchasVQ-
Diffusion,Paella,andRoBERTa. OurresultsshowthatDICEpreserveshighdata
fidelitywhileenhancingeditingcapabilities, offeringnewopportunitiesforfine-
grainedcontentmanipulationindiscretespaces. Codeisavailableat[link].
1 INTRODUCTION
Diffusionmodelshaveemergedasapowerfulclassofgenerativemodels,achievingremarkablesuc-
cessinhigh-fidelityimageandvideosynthesis(Hoetal.,2020;Dhariwal&Nichol,2021;Rombach
etal.,2022;Rameshetal.,2022;Hoetal.,2022;OpenAI,2024). Thesemodelsgeneratedataby
iterativelydenoisingsamplesfromasimplenoisedistribution,effectivelyreversingadiffusionpro-
cessthatgraduallycorruptsdata. Broadly,diffusionmodelscanbecategorizedintocontinuousand
discretetypes,eachtailoredtodifferentdatamodalitiesandapplications.
Continuousdiffusionmodelsoperateincontinuousspaces, leveragingstochasticdifferentialequa-
tions (SDEs) or their deterministic counterparts, ordinary differential equations (ODEs), to model
theforwardandreversediffusionprocesses(Songetal.,2020;2021).Advancessuchasflowmatch-
ing(Lipmanetal.,2022;Liuetal.,2022)haveenhancedtheirefficiencyandflexibility. Thesemod-
elshavebeensuccessfullyappliedinvariousdomains,includingimageediting(Mengetal.,2021;
Avrahami et al., 2022; Mokady et al., 2022; Han et al., 2023; 2024; Zhang et al., 2023), medical
imaging (He et al., 2023), and solving inverse problems (Chung et al., 2022; Stathopoulos et al.,
2024). In image editing, continuous diffusion models enable controlled manipulation of images
while preserving consistency with the underlying data distribution. A key capability enabling this
is inversionâ€”the process of reversing the diffusion model to recover the original noise vector or
latentrepresentationthatcouldhavegeneratedagivendatasample. Twomaininversionapproaches
exist: deterministicinversionusingODEs(e.g.,DDIMInversion(Songetal.,2021))andstochastic
inversion by recording noise sequences (e.g., CycleDiffusion (Wu & De la Torre, 2022), DDPM
Inversion(Dhariwal&Nichol,2021)).
Discrete diffusion models are designed for inherently discrete data such as text or image to-
kens (Esser et al., 2021b). They adapt the diffusion framework to discrete spaces by defin-
ing appropriate transition kernels that corrupt and restore discrete data (Hoogeboom et al.,
2021; Austin et al., 2021; Gu et al., 2022). Prominent examples include multinomial diffu-
1
4202
tcO
01
]VC.sc[
1v70280.0142:viXraPreprint
ğ‘¡+1 ğ‘¡ ğ‘¡âˆ’1
Forward DDIM
ğ’™! ğ’™"
Reverse
DDIM ğ’™"|ğ’„
ğ’™! ğ’™"
q-sample
ğ’„: source prompt q-sample
â€œBlack and white cat posterior mean
on floorâ€ (a) ODE-based reconstruction latent ğ’›! (b) Non-ODE reconstruction
Forward DDIM ğ’™"|ğ’„â€²
ğ’™! ğ’™" ğ’™!
Reverse
DDIM
ğ’„â€²:editing prompt ğ’™"|ğ’„â€² posterior mean ğ’™"|ğ’„â€²
â€œBlack and white cat
dog on floorâ€ (c) ODE-based editing free sampling w/ ğ’„â€² (d) Non-ODE editing
Figure 2: Here we demonstrate the two types of reconstruction and editing paradigms, namely
ODE-based and Non-ODE based. (a,c) shows the ODE-based editing and reconstructions, while
it provides accurate editing and reconstruction performances, it highly depends on the underlying
ODE trajectory, which is not feasible in the discrete diffusion. However, the Non-ODE editing
samplesatrajectorybydirectlyaddingnoisetox andrecordthedifferencebetweenthepredicted
0
x andthesampledx asindicatedintheredarrow. Inthisway,weareabletoreconstruct/edit
tâˆ’1 tâˆ’1
theimagewithoutthestrongconditionofhavinganunderlyingODE.
sion (Hoogeboom et al., 2021; Gu et al., 2022), D3PM (Austin et al., 2021), and masked
generative models like MaskGIT (Chang et al., 2022), Muse (Chang et al., 2023). Despite
their success in generation tasks, discrete diffusion models face limitations in controlled con-
tent editing. For instance, masked generative models achieve image editing through masked
inpainting, where regions are masked and regenerated based on new conditions. However,
this approach lacks the ability to inject information from the masked area into the inpaint-
ing process, limiting fine-grained control over the editing outcome (as illustrated in Figure 1).
Input Image Inpainting w/ Mask Ours (w/o Mask)
Moreover, existing ODE-based inversion tech-
niques developed for continuous diffusion
models are not directly applicable to discrete
diffusion models due to inherent differences
in data representation and diffusion processes.
This gap hinders the ability to perform pre-
ciseinversionandcontrollededitingindiscrete
Black and white cat dog on floor
spaces. To address this challenge, we pro-
poseDICE(DiscreteInversionforControllable Figure 1: Illustration of the limitation of masked
Editing), the first inversion algorithm for dis- inpainting method. Here, we want to change the
cretediffusionmodelstothebestofourknowl- cat to a dog. Inpainting with masked generation
edge. Ourmethodextendsthestochasticinver- inadvertentlymodifiestheorientationofthehead,
sionapproachtodiscretediffusionmodels, in- resultinginalessfavourableresult. Withourdis-
cludingbothmultinomialdiffusionandmasked creteinversionmethod,weareabletoedittheim-
generative models. The core idea is to record age while preserving other properties of the ob-
thenoisesequenceneededtorecoverastochas- ject being edited. This is achieved by injecting
tic trajectory in the reverse diffusion process. theinformationfromtheinputimageintothelogit
Specifically,givenanartificialtrajectorywhere space. Dottedredboxindicatesthemask.
latentstateshavelowcorrelation,wefitreverse
samplingstepstothistrajectoryandsavetheresidualsbetweentargetsandpredictions.Thisprocess
imprints the information of the original input data into the recorded residuals. During editing or
inference, the recorded residuals are added back, allowing us to inject and control the amount of
informationintroducedintotheinferenceprocess.
Our approach enables accurate reconstruction of the original input data and facilitates controlled
editingwithouttheneedforpredefinedmasksorattentionmapmanipulation. Itprovidesaflexible
framework for fine-grained content manipulation in discrete spaces, overcoming the limitations of
2Preprint
existing methods. We validate the effectiveness of DICE through extensive experiments on both
image and text modalities. We evaluate our method on models such as VQ-Diffusion (Gu et al.,
2022), Paella (Rampas et al., 2022), and RoBERTa (Liu et al., 2019), demonstrating its versatility
acrossdifferenttypesofdiscretegenerativemodels. Additionally,weintroduceanoveltext-editing
dataset to further showcase our methodâ€™s capabilities and to facilitate future research in this area.
Ourcontributionscanbesummarizedasfollows:
â€¢ WeintroduceDICE,aninversionalgorithmfordiscretediffusionmodels,includingmulti-
nomial diffusion and masked generative models. By recording and injecting noise se-
quencesormaskingpatterns,DICEenablesaccuratereconstructionandcontrolledediting
ofdiscretedatawithouttheneedforpredefinedmasksorattentionmanipulation.
â€¢ WevalidatetheeffectivenessofDICEthroughcomprehensiveexperimentsonbothimage
andtextmodalities,demonstratingitsversatilityacrossdifferenttypesofdiscretegenera-
tivemodels.
â€¢ We show that our approach can transform a model primarily trained for understanding
tasks,suchasRoBERTa,intoacompetitivegenerativemodelfortextgenerationandedit-
ing,illustratingthepotentialforextendingdiscretediffusionmodelstonewapplications.
2 RELATED WORK
Discrete diffusion. D3PM (Austin et al., 2021) and Multinomial Diffusion (Hoogeboom et al.,
2021) spearheaded the study of diffusion processes in discrete spaces by developing a corruption
mechanism for categorical data. Following those works, Esser et al. (2021a) and Gu et al. (2022)
introducedtheVQ-GANasawaytodiscretizetheimageintotokens. Also,extendingtothenatural
languageprocessing,Devlinetal.(2018)andLiuetal.(2019)proposedabidirectionaltransformer
forlanguageunderstanding,whichcanbeviewedasadiscretediffusionmodel(Wang&Cho,2019).
Additionally,Campbelletal.(2022)proposeddiscretediffusionmodelswithcontinuoustime,while
Lou et al. (2023) extended score matching (Song & Ermon, 2019) to discrete spaces by learning
probability ratios. Gat et al. (2024) proposed discrete flow matching to extend the flow matching
to discrete space. MaskGIT (Chang et al., 2022), Muse (Chang et al., 2023) and MMVID (Han
et al., 2022) introduced efficient non-autoregressive methods for image generation by iteratively
remaskingandreprediction.
Diffusion inversion. Diffusion inversion aims
to find an encoding or latent representation
1e5 Plot of I(zt;x0) vs. Time Step t
of the input signal that can be used to re-
5
construct the original data. Traditional ap-
proaches to diffusion inversion are based on
4
neuralODEs(Chenetal.,2018),suchasDDIM
inversion (Song et al., 2021) and flow match-
3
ing (Lipman et al., 2022; Liu et al., 2022),
wheredeterministictrajectoriesareusedforin-
2
version. Another class of methods focuses on
stochastic differential equations (SDEs) (Song
1
et al., 2020), including models like CycleDif-
fusion (Wu & De la Torre, 2022) and DDPM
0
Inversion(Huberman-Spiegelglasetal.,2024),
0 200 400 600 800 1000
whichrelyontrackingnoiseorresidualsalong Time step t
a stochastic path to recover the input. Our ap-
Figure3: Mutualinformationbetweenz andx .
proachgeneralizestheconceptofDDPMInver- t 0
ComputedwithasimpleDDPMsettingbyassum-
sion by extending it to discrete diffusion mod-
ingx âˆ¼N(0,I).
els,enablingeffectiveinversioninbothcontin- 0
uousanddiscretesettings.
Inversion-based image editing. DDIM inversion (Song et al., 2021) has served as a founda-
tional technique for various diffusion-based image editing approaches. In many image editing
tasks, DDIM-type methods are often employed alongside guidance techniques like Prompt-to-
Prompt(Hertzetal.,2022),whichmanipulatecross-attentionmaps. Incontrast,DDPMinversion-
based(Huberman-Spiegelglasetal.,2024)approachesaremoreuser-friendly,astheydonotrequire
3
)0x;tz(IPreprint
cross-attentionmanipulations. Toaddress issuessuchas inaccuratereconstructionanderror accu-
mulation, Null-text Inversion (Mokady et al., 2022) introduces test-time optimization of null em-
beddings,ensuringthereconstructiontrajectoryalignsmorecloselywiththeDDIMinversionpath.
Negative-promptInversion(Miyakeetal.,2023;Hanetal.,2024)furtherimprovestimeefficiency
byprovidingaclosed-formsolutiontoanapproximateinversionproblem,reducingcomputational
costswhilemaintainingcompetitivereconstructionquality.
3 METHODS
3.1 PRELIMINARIES
Maskedgenerativemodeling. Maskedgenerativemodelingiswidelyusedinrepresentationlearn-
ingforbothnaturallanguageprocessingandcomputervision.Itworksbymaskingpartsoftheinput
andtraining themodel toreconstruct themissingdata. Inmodels likeBERT(Devlin etal., 2018)
andRoBERTa(Liuetal.,2019),maskedtokens([MASK])arepredictedbasedonthesurrounding
context,excellingintextcompletionandembeddingrepresentationlearning. Forimagegeneration,
Paella(Rampasetal.,2022)adaptsthisapproachfortext-conditionalimagegenerationbyrenoising
tokens instead of masking. The inference process in masked generative models typically involves
iterativerenoise/remaskandrepredictsteps.
Multinomial Diffusion. Denoting x âˆˆ {1,...,K}D as a data point of dimension D. We use
0
v(x(i))todenotetheonehotcolumnvectorrepresentationofthei-thentryofx . Tosimplifyno-
t t
tation, in the following we drop index i and any function that operates on vector x is populated
t
alongitsdimension. Diffusionmodeldefinesamarkovchainq(x |x ) = Î T q(x |x )that
1:T 0 t=1 t tâˆ’1
gradually add noise to the data x for T times so that x contains little to no information. Dis-
0 T
crete diffusion model (Hoogeboom et al., 2021; Austin et al., 2021; Gu et al., 2022) proposed an
alternativelikelihood-basedmodelforcategoricaldata,anddefinestheforwardprocessfollowing:
q(x |x )=Cat(v(x );Ï€ =Q v(x )). (1)
t tâˆ’1 t t tâˆ’1
whereQ isthetransitionmatrixbetweenadjacentstatesfollowingmask-and-replacestrategy. The
t
posteriordistributiongivenx hasaclosed-formsolution,
0
(QâŠ¤v(x ))âŠ™(Q v(x ))
q(x |x ,x )= t t tâˆ’1 0 . (2)
tâˆ’1 t 0
v(x )âŠ¤Q v(x )
t t 0
whereQ = Q Â·Â·Â·Q isthecumulativetransitionmatrix. ThedetailsofQ andQ aregivenin
t t 1 t t
thesupplementarymaterials. Theinferenceprocessisasbelow:
K
(cid:88)
Ï€ (x ,t)=p (x |x )= q(x |x ,xËœ )p (xËœ |x ), (3)
Î¸ t Î¸ tâˆ’1 t tâˆ’1 t 0 Î¸ 0 t
xËœ0=1
with p (xËœ |x ) is parameterized by a neural network. We gradually denoise from x to x using
Î¸ 0 t T 0
3. Fornumericalstability,theimplementationuseslogspaceinsteadofprobabilityspace. Masked
generative models can be viewed as a special case of multinomial diffusion models with an addi-
tionalabsorbingstate(orthe[MASK]state). Itstrainingobjectivecanbeviewedasareweighted
ELBO(Bond-Tayloretal.,2022).
3.2 DISCRETEINVERSIONFORCONTROLLABLEEDITING
Non ODE-based inversion. ODE-based generative models, such as DDIM and flow matching,
define an ODE trajectory. Due to the deterministic nature of ODEs, inversion can be achieved by
solvingtheODEusingtheEulermethodinforwarddirection,ensuringreconstructionbasedonthe
inherentpropertiesoftheODE.Incontrast,anotherlineofresearchfocusesonSDE-basedmodels,
such as CycleDiffusion (Wu & De la Torre, 2022) and DDPM Inversion (Huberman-Spiegelglas
etal.,2024). Broadlyspeaking,theseapproachesensurereconstructionbyrecordingthenoisesor
residualsthatarerequiredtoreproducethestochastictrajectory. CycleDiffusionrecordstheGaus-
sian noise z during sampling from posterior p(x |x ,x = x ) and injects information of the
t tâˆ’1 t 0 0
inputsignalbyfeedingthetruex . DDPMInversion,ontheotherhand,incorporatesinformation
0
4Preprint
intoz byfittingthereverseprocessintoanartificialstochastictrajectoryobtainedbyindependent
t
q-sample. ForbothCycleDiffusionandDDPMInversion,thekeyideaistoutilizetheGaussian
reparameterization trick, x = Âµ+Ïƒz â‡” x âˆ¼ N(x;Âµ,Ïƒ2), and keeping track of the â€œnoiseâ€ that
couldhavegeneratedthesamplefrommean. Fordiscretediffusionmodels,weutilizetheGumbel-
Maxtrick(Maddisonetal.,2014;Jangetal.,2016),x=argmax(log(Ï€)+g)â‡”xâˆ¼Cat(x;Ï€).
Figure2providesanintuitionoftheproposedmethod.
Invertingmultinomialdiffusion. SimilartoHuberman-Spiegelglasetal.(2024),westartbysam-
pling a stochastic trajectory, {x }, a sequence of independent q-sampleâ€™s from q(x |x ) (we
t t 0
populatethefollowingsamplingoperationalongthedimensionofx ),
t
x =argmax(log(q(x |x ))+g), with (4)
t t 0
q(x |x )=Cat(x ;Ï€ =Q v(x )) and g âˆ¼Gumbel(0,I).
t 0 t t 0
NotethathereweusetheGumbelsoftmaxtrick(Jangetal.,2016),whichisequivalenttosampling
fromcategoricaldistributionq(x |x ).
t 0
y =log(onehot(x )), and
tâˆ’1 tâˆ’1
yË† =log(Ï€ (x ,t)),
tâˆ’1 Î¸ t
z :=y âˆ’yË† (5)
t tâˆ’1 tâˆ’1
Notethatherethelatentz âˆˆRDÃ—K.
t
Inthisreverseprocess, thelatentspace{x ,z ,z ,...,z }togetherwiththefixeddiscretedif-
T T tâˆ’1 1
fusion model Ï€ also uniquely define the same stochastic trajectory x ,x ,...,x . The detailed
Î¸ 0 1 T
algorithmisgiveninAlgorithm2.
Algorithm1DiscreteInversionforMaskedGen- Algorithm2DiscreteInversionforMultinomial
erativeModeling Diffusion
Inversion: Inversion:
1: y 0 â†D(x 0,c,t=0) 1: fortfrom1toT do
2: Samplenoisetokenmapn 2: x t âˆ¼q(x t|x 0) â–·Independentq-sample
3: fortfrom1toT do usingEq4
4: m t â†GenerateMask(t) â–·Sampling 3: y t â†log(onehot(x t))
masksaccordingtoinferencealgorithm 4: endfor
5: x t â†x 0âŠ™(1âˆ’m t)+nâŠ™m t 5: fortfromT to1do
6: yË† 0|t â†D Î¸(x t,c,t=t) 6: yË† tâˆ’1 â†log(Ï€ Î¸(x t,c,t)) â–·Log
7: z t â†y 0âˆ’yË† 0|t â–·Eq6 posteriorusingEq3
8: endfor 7: z t â†y tâˆ’1âˆ’yË† tâˆ’1 â–·Eq5
Editing/Sampling: 8: endfor
9: fortfromÏ„ to1do Editing/Sampling:
10: yË†
0|t
â†D Î¸(x t,câ€²,t=t) 9: fortfromÏ„ to1do
11: g âˆ¼Gumbel(0,I) 10: xË† 0 â†p Î¸(x 0|x t =argmaxy t)
12: yËœ 0 â†yË† 0|t+Î» 1Â·z t+Î» 2Â·g 11: g âˆ¼Gumbel(0,I)
13: xËœ 0 â†argmaxyËœ 0 12: y tâˆ’1 â† log(q(x tâˆ’1|x t,xË† 0;câ€²))+Î» 1 Â·
14: x tâˆ’1 â†xËœ 0âŠ™(1âˆ’m tâˆ’1)+nâŠ™m tâˆ’1 z t+Î» 2Â·g
15: endfor 13: endfor
16: Returnx 0. 14: Returnx 0 =argmaxy 0.
Invertingmaskedgenerativemodels. Inmaskedgenerativemodeling,thestochastictrajectoryx
t
is constructed according to the specific inference algorithm of the model in use. For example, in
PaellaRampasetal.(2022),themaskingisinclusive,meaningthatasthetimesteptincreases,the
setofmaskedtokensgrows. Incontrast,theUnleashingTransformerBond-Tayloretal.(2022)em-
ploysrandommaskingateachstep,wheremasksaregeneratedindependentlyusingtheq-sample
function. Withoutlossofgenerality, wedefineadenoiserfunctionD (parameterizedbyÎ¸). This
Î¸
denoiseroutputsthelogitsofthepredictedunmaskeddatagiventhenoisytokensx . Sinceinthis
t
case, the categorical sampling happens at sampling from the denoiserâ€™s prediction, we therefore
defineancorrespondinglatentsequence:
yË† =log(p (x |x ))=D (x ,t)
0|t Î¸ 0 t Î¸ t
z :=y âˆ’yË† . (6)
t 0 0|t
5Preprint
Withourproposedlatentspace, accuratereconstructionisguaranteed. However, foreditingtasks,
this level of precision may not be ideal if the latent variable z dominates the generation process.
t
ThedetailedalgorithmisgiveninAlgorithm1.
Toprovidemoreflexibility,weintroducethehyperparametersÏ„,Î» ,andÎ» ,whichallowforfiner
1 2
controlovertheeditingprocess.Specifically,Ï„ representsthestarting(andlargest)timestepatwhich
theeditingprocessbegins, whileÎ» controlstheamountofinformationinjectedfromtheoriginal
1
input,andÎ» governstheintroductionofrandomnoise.
2
Analysis. WedescribeasimpleyetprototypicalexampleofDDPMandcomputethemutualinfor-
mationbetweenencodedlatentsandtheinputsignal.
Remark3.1. GivenasimpleGaussianDDPMwithx âˆ¼ N(0,I),latents{z }areobtainedwith
0 t
DDPMinversion(Huberman-Spiegelglasetal.,2024),thenthemutualinformationbetweenz and
t
x is:
0
D Î²2Î± +1âˆ’Î± +Î± (1âˆ’Î± )
I(z ;x )= log( t tâˆ’1 tâˆ’1 t t ). (7)
t 0 2 1âˆ’Î± +Î± (1âˆ’Î± )
tâˆ’1 t t
Themutualinformationbetweenz andx isillustratedinFigure3. Weobservethattheamount
t 0
ofinformationencodedfromx intoz decreasesastincreases,motivatingustoexploredifferent
0 t
schedulingstrategiesforÎ»â€™s(seeFigure7).
4 EXPERIMENTS
Inthissection,wedemonstratetheeffectivenessofourproposedinversionmethodsonbothimage
andlanguagediffusionmodels.Ourexperimentsshowthatthemethodscanpreserveidentityinboth
vision and language tasks while successfully making the intended changes. The implementation
detailscanbereviewedinSupplementaryMaterials.
4.1 IMAGEDIFFUSIONMODEL
For the image diffusion model, we mainly investigate the use of absorbing state discrete
model (Austin et al., 2021) including a masked generative model, Paella, and a multinomial dif-
fusionmodel,VQ-Diffusion. Wedemonstratetheinversionreconstructionabilityandimageediting
performanceinbothcategorieswithDICE.
Dataset.ThePrompt-basedImageEditingBenchmark(PIE-Bench)by (Juetal.,2023)isarecently
introduced dataset designed to evaluate text-to-image (T2I) editing methods. The dataset assesses
language-guidedimageeditingin9differentscenarioswith700images. Thebenchmarkâ€™sdetailed
annotations and variety of editing tasks were instrumental in thoroughly assessing our methodâ€™s
capabilities,ensuringafairandconsistentcomparisonwithexistingapproaches.
4.1.1 INVERSIONRECONSTRUCTION
In this section, we evaluate the accuracy of inversion without editing. This is achieved by first
invertingtheimageandthenusingtherecordedlatentcodetoreconstructtheoriginalimage.
EvaluationMetrics. Here,weevaluatetheimagesimilaritybyPSNR,LPIPS,MSEandSSIMof
theoriginalandthegeneratedimageunderthesamepromptwithDICEandmaskedgeneration.
Quantitative Analysis. The reconstruction performance of our method, as shown in Table 1, far
surpassesthebaselineInpainting+Paellamodelacrossallmetrics.Inthecaseofmaskedinpainting,
all image tokens are replaced with randomly sampled tokens, meaning the model lacks any prior
informationabouttheoriginalimage. Asaresult,thereconstructedimagedifferssignificantlyfrom
the one being inverted, leading to lower similarity scores. In contrast, our method demonstrates
near-perfect reconstruction, as indicated by the metrics, and notably produces an identical image
without the errors typically introduced by the VQ-VAE/GAN quantization process, as seen in the
results marked with (â€ ). This highlights the superior accuracy and consistency of our approach in
generatinghigh-fidelityreconstructions.
6Preprint
Method Metric
Inverse+Model PSNRâ†‘ LPIPS â†“ MSE â†“ SSIM â†‘
Ã—103 Ã—104 Ã—102
Inpainting+Paella 10.50 565.11 1002.09 30.13
Ours+Paella 30.91 39.81 11.07 90.22
Oursâ€ +Paella Inf 0.07 0.01 99.99
Table 1: Inversion Reconstruction performance â€  The metric is calculated between the original
imageanditsinvertedcounterpart. DuetotheencodinganddecodingstepsintheVQ-VAE/GAN
process,someinaccuraciesareintroducedbythequantization. ThePSNRisInfduetotherecon-
structionofourmethodyieldingthesameimageaftertheVQ-VAE/GANprocess.
4.1.2 EDITINGPERFORMANCE
In this section, we discuss the editing performance of our proposed method. Since there is no
discretediffusioninversionexists,wecompareourmethodwithmaskedgenerationasindicatedin
theoriginalpaper. Inadditiontothat,wealsodemonstratethemetricfromcontinuouscounterparts.
Evaluation Metrics. To demonstrate the effectiveness and efficiency of our proposed inversion
method,weemployeightmetricscoveringthreekeyaspects:structuredistance,backgroundpreser-
vation, andeditprompt-imageconsistency, asoutlinedinJuetal.(2023). Weutilizethestructure
distancemetricproposedbyTumanyanetal.(2023)tomeasurethestructuralsimilaritybetweenthe
original and generated images. To evaluate how well the background is preserved outside the an-
notatededitingmask,weusePeakSignal-to-NoiseRatio(PSNR),LearnedPerceptualImagePatch
Similarity(LPIPS)(Zhangetal.,2018),MeanSquaredError(MSE),andStructuralSimilarityIndex
Measure(SSIM)(Wangetal.,2004). Wealsoassesstheconsistencybetweentheeditpromptand
thegeneratedimageusingCLIP(Radfordetal.,2021)SimilarityScore (Wuetal.,2021),whichis
calculatedoverthewholeimageandspecificallywithintheregionsdefinedbytheeditingmask.
Results. InTable2,wedemonstratethequantitativeresultofDICEusingPaellaandVQ-Diffusion
comparedtocontinuousdiffusionmodelandalsoinpainting. Notably,ourapproachwiththePaella
modelachievestheloweststructuredistance11.34,outperformingallothermethods,includingthe
continuous diffusion models. Additionally, while the DDPM Inversion with Stable Diffusion v1.4
showsthehighestCLIPsimilarityscoresforbothwholeandeditedregions,ourmethodmaintains
competitive CLIP similarity with Paella. Given the significant reduction in structure distance, our
method offers a superior balance between structural preservation and semantic alignment in ed-
its. Furthermore,whencombinedwithVQ-Diffusion,ourmethodcontinuestoshowstrongperfor-
mance. TheresultsinTable3clearlydemonstratethesuperiorbackgroundpreservationcapabilities
of our method compared to DDIM+SD1.4. All four metrics underscore the structural consistency
ofourapproachinpreservingtheuneditedregionsoftheimage. Theseresultsshowtheeffective-
ness of our method in maintaining background integrity during editing and provide evidence that
informationabouttheoriginalimageisinstilledintothelatentspaceofDICE.
InFigure4,weshowtheeditingresultsforbothPaellaandVQ-DiffusionusingDICE.Bothmodels
successfully modify real images according to the target prompts. In all cases, our results exhibit
bothhighfidelitytotheinputimageandadherencetothetargetprompt.
4.2 LANGUAGEDIFFUSIONMODEL
In this section, we evaluate DICE on RoBERTa (Liu et al., 2019), a text discrete diffusion model,
to generate sentences with opposing sentiments while preserving structural similarities. We begin
with two promptsâ€”one with a positive sentiment and another with a negative sentiment. Each
promptcontainstwosentences:thefirstsentenceindicatesthesentimenttypeandsetsthecontextual
background,andthesecondsentenceisthetargetforinversionandgeneration. Initially,weinvert
the second sentence of the negative sentiment prompt using the entire prompt as context, which
producesanoisedtokenrepresentationofthatsentence.Next,weconditionthemodelonthepositive
sentimentbyconcatenatingthefirstsentenceofthepositivesentimentpromptwiththenoisedtoken
of the inverted negative sentence. This setup guides the model to generate a new second sentence
thatmirrorsthestructureoftheoriginalnegativesentencebutexpressesapositivesentimentinstead.
7Preprint
Input Image Paella VQ-Diffusion Input Image Paella VQ-Diffusion
(a) two origami birds sitting on a branch (b) A cat dog sitting on a wooden chair
(c) a cat tiger sitting next to a mirror (d) white plate with fruits pizza on it
(e) drawing of tulip lion on the coffee (f) meat balls sushi on white plate
(g) a plate with steak salmon on it (h) white tiger cat on brown ground
Figure 4: Visualization of editing results. Editing results for our method using Paella and VQ-
Diffusionarepresented, alongwiththeircorrespondingprompts. Theresultsdemonstratethatour
methodcaneffectivelymodifytheinputimageaccordingtothetargetpromptwhilepreservingthe
imagestructure.Editingwithmaskedgenerativemodel(Paella(Rampasetal.,2022))ismorestable
andeasierthanwithmultinomialdiffusionmodels(VQ-Diffusion(Guetal.,2022)).
Throughthisprocess,weassessthemodelâ€™scapabilitytoinvertandgeneratetextthatalignswitha
specifiedsentimentwhileretainingtheoriginalsentenceâ€™sstructuralelements.
InversionProcess. Inourexperiment,wespecificallyfocusoninvertingthesecondsentence,indi-
catedasredinTable6,whilekeepingthefirstsentenceintact(black),asitusuallycontainsessential
context. Duringthereverseprocess,weaimtoreconstruct/editthesecondsentencebyrecoveringit
fromthenoisedtokensacquiredintheinversionphase.
Dataset
1. PositiveSentiment:Thankstoherefforts.Theeventwasahugesuccess.
NegativeSentiment:Despiteherefforts.Theeventwasacompletedisaster.
2. PositiveSentiment:Thisbookisdefinitelyinteresting.Icanâ€™tputitdown;itâ€™sfullofsurprises.
NegativeSentiment:Thisbookisdefinitelyinteresting.Icanâ€™twaittofinishit;itâ€™ssopredictable.
3. ...
Dataset Generation. In order to evaluate the editing performance, we designed and proposed a
new dataset called Sentiment Editing. The objective is to edit the sentiment of the sentence while
preserving the structure of the sentence and also sticking to the theme of the sentence. Here, we
demonstrate two sets of sentences in our dataset. Please refer to supplementary materials for the
processofgeneratingthedatasetandmoreexamples.
8Preprint
Method Structure CLIPSimilarity
Inverse Editing Distance â†“ Wholeâ†‘ Editedâ†‘
Ã—103
DDIM+SD1.4 P2P 69.43âˆ— 25.01âˆ— 22.44âˆ—
Null-Text+SD1.4 P2P 13.44âˆ— 24.75âˆ— 21.86âˆ—
Negative-Prompt+SD1.4 P2P 16.17âˆ— 24.61âˆ— 21.87âˆ—
DDPM-Inversion+SD1.4 Prompt 22.12 26.22 23.02
Inpainting+Paella Prompt 91.10 25.36 23.42
Ours+Paella Prompt 11.34 23.79 21.23
Ours+VQ-Diffusionâ€  Prompt 12.70 23.85 21.02
Table2:Quantitativeresultsonimageeditingperformance.Comparisonofourproposedmethod
withthemaskedinpaintingwiththediscretediffusionmodelPaella,aswellascontinuousdiffusion
model (Stable Diffusion v1.4) using DDIM inversion. â€œP2Pâ€ refers to Prompt-to-Prompt (Hertz
etal.,2022),andâ€œPromptâ€denoteseditingperformedsolelythroughforwardeditprompts. Entries
marked with an asterisk (âˆ—) are cited from Ju et al. (2023). â€ : For VQ-Diffusion, the images are
down-sampled to 256 Ã— 256. It is important to note that due to differences in base models and
editingalgorithms, themetricsacrossmethodsarenotdirectlycomparable. However, ourmethod
significantlyoutperformsbothinpaintingandstrongbaselines(e.g.,Null-TextInversion+SD1.4)in
termsofstructuralpreservation. Asexpected,inpaintingachievesahighCLIPscoresinceitdirectly
generatesimagepatchesbasedonthetargetprompt.
Method BackgroundPreservation
Inverse Editing PSNRâ†‘ LPIPS â†“ MSE â†“ SSIM â†‘
Ã—103 Ã—104 Ã—102
DDIM+SD1.4 P2P 17.87 208.80 219.88 71.14
Ours+Paella Prompt 27.29 52.90 43.76 89.79
Table3: BackgroundPreservation. Quantitativecomparisonofbackgroundpreservationbetween
our proposed method and DDIM+SD 1.4, achieved by masking the edited region and calculating
image similarity with the unedited masked image. The inpainting is served as upper bound since
onlythemaskedregionareeditedandbackgroundarenotmodified.
4.2.1 INVERSIONRECONSTRUCTION
Similartotheimagegenerationsection,wefirstdemonstratetheinversionandreconstructioncapa-
bilitiesoftheproposedmethods. Thisprocessinvolvesinvertingthesentences, followedbyusing
thesameprompttogeneratethereconstructedversionofthesecondsentence.
EvaluationMetric. Forreconstruction,weuseHitRate,whichisdefinedastheproportionofcases
where each method generates an identical sentence to the original. In addition, we compute the
Semantic Textual Similarity (STS) score by measuring the cosine similarity between the sentence
embeddings,usingthemodelproposedbyReimers(2019)etal.
QuantitativeAnalysis. Table4comparesDICE withMaskedGenerationusingRoBERTaacross
twometrics:AccuracyandSemanticTextualSimilarity.OurmethodsignificantlysurpassesMasked
Generationinbothmetrics,demonstratingthatourz latentspaceeffectivelycapturestheinforma-
t
tionofthesentencebeinginvertedandfacilitatesitssubsequentreconstruction.
4.2.2 SENTENCEEDITING
Inthissection,weevaluatetheeditingperformanceoftheproposedinversionmethodonRoBERTa.
In Table 6, the sentence shown in black under the negative prompt column is input during the in-
versionprocess. Thesentencethatisbeinginvertedisdisplayedinred. Forediting, thepromptis
thensubstitutedwiththeblacksentenceontheright,andnoiseisaddedattheendfortheforward
process. Theoutputoftheforwardprocessforthenoiseispresentedinblue.
9
suounitnoC
etercsiDPreprint
Method Metric Method Metric
Textual Structure Sentiment
Inverse+Model Accuracy Ã—102â†‘
Similarity Ã—102â†‘
Inverse+Model
Preservation Ã—102â†‘ Correctness Ã—102â†‘
MaskedGeneration+RoBERTa 0.0 6.57 MaskedGeneration+RoBERTa 29.80 12.94
Ours+RoBERTa 99.74 99.90 Ours+RoBERTa 94.76 72.51
Table 4: Text Inversion Reconstruction Per- Table 5: Text Editing Performance. Evaluation
formance. Quantitativecomparisonsofthetext of the text editing performance between Masked
reconstructionperformancebyMaskedGenera- GenerationandDICEusingChatGPTasaclassi-
tion and DICE method using RoBERTa as the fier.
languagemodel.
EvaluationMetric.Forthesentenceeditingtask,weevaluatethegeneratedsentencesbasedontwo
criteria:(1)structuralpreservation,whichassesseswhetherthesentencestructureisretained,and(2)
sentimentcorrectness,whichevaluateswhetherthesentimentoftheeditedsentencealignswiththe
sentimentoftheoriginalprompt.Boththestructuralpreservationrateandsentimentcorrectnessrate
arecalculatedusingChatGPT-4(Achiametal.,2023)asaclassifier. ThedetailsofusingChatGPT
forevaluationcanbereviewedinSupplementaryMaterials.
Results. Table 5 presents a comparative analysis of two text editing methods that both employ
RoBERTa,focusingontheeffectivenessintermsofStructurePreservationandSentimentCorrect-
ness. Our method significantly outperforms masked generation in both metrics. This difference
highlightsthesuperiorcapabilityofourinversionmethodtoencodetheoriginalstructureofthetext
inthelatentspaceandtheflexibilitytoadjustitssentimentmoreaccurately. InTable6,wedemon-
strateboththeinitialpromptandtheeditedresult.Ourapproachretainsthesentencestructureofthe
negativepromptwhilemodifyingitssentimenttoamorepositiveone.
NegativePrompt OurEditedResults
NegativeSentiment: Thisbookisdefinitelyinter- PositiveSentiment: Thisbookisdefinitelyinter-
esting. esting.
I canâ€™t wait to finish it; itâ€™s I canâ€™t wait to see it; it sounds
so predictable. so beautiful.
NegativeSentiment: Thenewofficespaceisfan- Positive Sentiment: The new office space is fan-
tastic. tastic.
Itâ€™s cramped and lacks proper Itâ€™s spacious and has great
facilities. facilities.
NegativeSentiment:Despiteherefforts. PositiveSentiment:Thankstoherefforts.
The event was a complete This event was a fantastic comedy
disaster. game.
NegativeSentiment:Regardingthelecture. PositiveSentiment:Regardingthelecture.
It was dull and confusing. It was clear and surprising.
NegativeSentiment:Despitetheinitialproblems. PositiveSentiment:Despitetheinitialproblems.
The project ended in failure. New project still in progress.
NegativeSentiment:Regardingthenewapp. PositiveSentiment:Regardingthenewapp.
Itâ€™s complicated and not useful. Itâ€™s On and Itâ€™s Epic.
Negative Sentiment: Reflecting on my environ- Positive Sentiment: Reflecting on my environ-
mentalinitiatives. mentalinitiatives.
Itâ€™s challenging to maintain, and Itâ€™s easy to understand, and
progress is slow. progress is undeniable.
Table6: EditingresultsofourmethodwithRoBERTa. Thesentencesinblackaretheprompts
used for inversion and editing in their respective column. The sentence in red is the one being
inverted,andthebluesentencerepresentstheeditingresult.
5 CONCLUSION
Inthispaper,weintroducedDICE(DiscreteInversionforControllableEditing),aninversionalgo-
rithmfordiscretediffusionmodels,includingmultinomialdiffusionandmaskedgenerativemodels.
Byleveragingrecordednoisesequencesandmaskingpatternsduringthereversediffusionprocess,
10Preprint
DICEenablesaccuratereconstructionandflexibleeditingofdiscretedatawithouttheneedforpre-
definedmasksorcross-attentionmanipulation. Ourexperimentsacrossmultiplemodelsandmodal-
ities, such as images and text, demonstrate the effectiveness of DICE in preserving data fidelity
whileenhancingeditingcapabilities. Furthermore,wedemonstratethepotentialofDICEforcon-
vertingRoBERTa,amodeltraditionallyfocusedondataunderstanding,intoagenerativemodelfor
textgenerationandediting. WebelievethatDICEenhancesthecapabilitiesofdiscretegenerative
models,offeringnewopportunitiesforfine-grainedcontentmanipulationindiscretespaces.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVanDenBerg.Structured
denoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformationProcessing
Systems,34:17981â€“17993,2021.
Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of
natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.18208â€“18218,2022.
Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby P Breckon, and Chris G Willcocks. Un-
leashing transformers: Parallel token prediction with discrete absorbing diffusion for fast high-
resolutionimagegenerationfromvector-quantizedcodes. InEuropeanConferenceonComputer
Vision,pp.170â€“188.Springer,2022.
AndrewCampbell,JoeBenton,ValentinDeBortoli,ThomasRainforth,GeorgeDeligiannidis,and
ArnaudDoucet.Acontinuoustimeframeworkfordiscretedenoisingmodels.AdvancesinNeural
InformationProcessingSystems,35:28266â€“28279,2022.
HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.Maskgit:Maskedgenerative
imagetransformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.11315â€“11325,2022.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan
Yang,KevinMurphy,WilliamTFreeman,MichaelRubinstein,etal. Muse: Text-to-imagegen-
erationviamaskedgenerativetransformers. arXivpreprintarXiv:2301.00704,2023.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differentialequations. Advancesinneuralinformationprocessingsystems,31,2018.
HyungjinChung,JeongsolKim,MichaelTMccann,MarcLKlasky,andJongChulYe. Diffusion
posteriorsamplingforgeneralnoisyinverseproblems. arXivpreprintarXiv:2209.14687,2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. Advances
inNeuralInformationProcessingSystems,34,2021.
Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional
contextwithmultinomialdiffusionforautoregressiveimagesynthesis. Advancesinneuralinfor-
mationprocessingsystems,34:3518â€“3532,2021a.
PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InCVPR,pp.12873â€“12883,2021b.
Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and
YaronLipman. Discreteflowmatching. arXivpreprintarXiv:2407.15595,2024.
Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and
Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.10696â€“10706,2022.
11Preprint
LigongHan,JianRen,Hsin-YingLee,FrancescoBarbieri,KyleOlszewski,ShervinMinaee,Dim-
itris Metaxas, and Sergey Tulyakov. Show me what and tell me how: Video synthesis via mul-
timodal conditioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pp.3615â€“3625,2022.
LigongHan,YinxiaoLi,HanZhang,PeymanMilanfar,DimitrisMetaxas,andFengYang. Svdiff:
Compactparameterspacefordiffusionfine-tuning. arXivpreprintarXiv:2303.11305,2023.
Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao,
AnastasisStathopoulos, XiaoxiaoHe, YuxiaoChen, etal. Proxedit: Improvingtuning-freereal
image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on
ApplicationsofComputerVision,pp.4291â€“4301,2024.
Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, and Dimitris N Metaxas.
Dmcvr: Morphology-guided diffusion model for 3d cardiac volume reconstruction. In Interna-
tional Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 132â€“
142.Springer,2023.
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
Prompt-to-promptimageeditingwithcrossattentioncontrol. arXivpreprintarXiv:2208.01626,
2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
NeuralInformationProcessingSystems,33:6840â€“6851,2020.
JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyGritsenko,DiederikP
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition
videogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
EmielHoogeboom,DidrikNielsen,PriyankJaini,PatrickForreÂ´,andMaxWelling. Argmaxflows
and multinomial diffusion: Learning categorical distributions. Advances in Neural Information
ProcessingSystems,34:12454â€“12465,2021.
InbarHuberman-Spiegelglas,VladimirKulikov,andTomerMichaeli. Aneditfriendlyddpmnoise
space: Inversionandmanipulations. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.12469â€“12478,2024.
EricJang,ShixiangGu,andBenPoole.Categoricalreparameterizationwithgumbel-softmax.arXiv
preprintarXiv:1611.01144,2016.
Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting
diffusion-basededitingwith3linesofcode. arXivpreprintarXiv:2310.01506,2023.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching
forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbertpretraining
approach. arXivpreprintarXiv:1907.11692,2019.
AaronLou,ChenlinMeng,andStefanoErmon.Discretediffusionlanguagemodelingbyestimating
theratiosofthedatadistribution. arXivpreprintarXiv:2310.16834,2023.
ChrisJMaddison,DanielTarlow,andTomMinka. A*sampling. Advancesinneuralinformation
processingsystems,27,2014.
ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon.Sdedit:Im-
agesynthesisandeditingwithstochasticdifferentialequations.arXivpreprintarXiv:2108.01073,
2021.
12Preprint
Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast
imageinversionforeditingwithtext-guideddiffusionmodels. arXivpreprintarXiv:2305.16807,
2023.
RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or. Null-textinversionfor
editingrealimagesusingguideddiffusionmodels. arXivpreprintarXiv:2211.09794,2022.
OpenAI. Sora: Video generation model, 2024. URL https://openai.com/index/
video-generation-models-as-world-simulators. Accessed: 2024-10-09.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748â€“8763.PMLR,2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
Dominic Rampas, Pablo Pernias, and Marc Aubreville. A novel sampling scheme for text-and
image-conditionalimagesynthesisinquantizedlatentspaces. arXivpreprintarXiv:2211.07292,
2022.
N Reimers. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084,2019.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition(CVPR),pp.10684â€“10695,June2022.
JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. InInterna-
tional Conference on Learning Representations, 2021. URL https://openreview.net/
forum?id=St1giarCHLP.
YangSongandStefanoErmon.Generativemodelingbyestimatinggradientsofthedatadistribution.
AdvancesinNeuralInformationProcessingSystems,32,2019.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
AnastasisStathopoulos,LigongHan,andDimitrisMetaxas. Score-guideddiffusionfor3dhuman
recovery. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.906â€“915,2024.
NarekTumanyan,MichalGeyer,ShaiBagon,andTaliDekel. Plug-and-playdiffusionfeaturesfor
text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Com-
puterVisionandPatternRecognition,pp.1921â€“1930,2023.
Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random
fieldlanguagemodel. arXivpreprintarXiv:1902.04094,2019.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
fromerrorvisibilitytostructuralsimilarity. IEEEtransactionsonimageprocessing,13(4):600â€“
612,2004.
Wikipedia contributors. Gumbel distribution â€” Wikipedia, The Free Encyclopedia. https:
//en.wikipedia.org/wiki/Gumbel_distribution, 2024. [Online; accessed 8-
October-2024].
ChenHenryWuandFernandoDelaTorre. Unifyingdiffusionmodelsâ€™latentspace,withapplica-
tionstocyclediffusionandguidance. arXivpreprintarXiv:2210.05559,2022.
Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and
Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint
arXiv:2104.14806,2021.
13Preprint
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586â€“595,2018.
ZhixingZhang,LigongHan,ArnabGhosh,DimitrisNMetaxas,andJianRen. Sine: Singleimage
editing with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pp.6027â€“6037,2023.
14Preprint
A DETAILS ON MULTINOMIAL DIFFUSION MODELS
DefinitionofQ withmask-and-replacestrategy. Followingmask-and-replacestrategyas:
t
ï£® Î± +Î² Î² Î² Â·Â·Â· 0 ï£¹
t t t t
Î² Î± +Î² Î² Â·Â·Â· 0
ï£¯ t t t t ï£º
ï£¯ Î² Î² Î± +Î² Â·Â·Â· 0 ï£º
Q t =ï£¯ ï£¯
ï£°
. . .t . . .t t . .
.
t ... . .
.
ï£º ï£º ï£», (8)
Î³ Î³ Î³ Â·Â·Â· 1
t t t
given Î± âˆˆ [0,1],Î² = (1âˆ’Î± âˆ’Î³ )/K and Î³ the probability of a token to be replaced with a
t t t t t
[MASK]token.
Cumulativetransitionmatrix.ThecumulativetransitionmatrixQ andq(x |x )canbecomputed
t t 0
viaclosedform:
Q v(x )=Î±Â¯ v(x )+(cid:0) Î³Â¯ âˆ’Î²Â¯(cid:1) v(K+1)+Î²Â¯1, (9)
t 0 t 0 t t t
whereÎ±Â¯ =(cid:81)t Î± ,Î³Â¯ =1âˆ’(cid:81)t (1âˆ’Î³ ),andÎ²Â¯ =(1âˆ’Î±Â¯ âˆ’Î³Â¯ )/(K+1)canbecalculated
t i=1 i t i=1 i t t t
andstoredinadvance.
B ANALYSIS ON MUTUAL INFORMATION
ProofofRemark3.1.
Proof. Weassumedthatx satisfiesstandardGaussiandistributionN(0,I ). Since
0 D
âˆš âˆš
x = Î± x + 1âˆ’Î± Ïµ
t t tâˆ’1 t t
where both x and Ïµ are independent standard Gaussian random variables, x is also standard
tâˆ’1 t t
Gaussian,andineachdimension
âˆš
Cov(x ,x )= Î± ,
t tâˆ’1 t
whichleadsto
âˆš
ÂµË† (x )=E(x |x )= Î± x .
t t tâˆ’1 t t t
Therefore,
z =xâ€² âˆ’ÂµË† (x )
t tâˆ’1 t t
=((cid:112) Î± x +(cid:112) 1âˆ’Î± Ïµ)âˆ’âˆš Î± (âˆš Î± x +âˆš 1âˆ’Î± Ïµâ€²)
tâˆ’1 0 tâˆ’1 t t 0 t
=Î² Â·(cid:112) Î± x +(cid:112) 1âˆ’Î± Ïµ+(cid:112) Î± (1âˆ’Î± )Ïµâ€².
t tâˆ’1 0 tâˆ’1 t t
Let
E =(cid:112) 1âˆ’Î± Ïµ+(cid:112) Î± (1âˆ’Î± )Ïµâ€²
tâˆ’1 t t
whichisaGaussianerrortermindependenttox withmean0andvariance1âˆ’Î± +Î± (1âˆ’Î± ).
0 tâˆ’1 t t
Thuswecancalculatethemutualinformation
I(z ;x )=H(z )âˆ’H(z |x )
t 0 t t 0
=H(z )âˆ’H(E)
t
D D
= log(2Ï€e(Î²2Î± +1âˆ’Î± +Î± (1âˆ’Î± ))âˆ’ log(2Ï€e(1âˆ’Î± +Î± (1âˆ’Î± ))
2 t tâˆ’1 tâˆ’1 t t 2 tâˆ’1 t t
D Î²2Î± +1âˆ’Î± +Î± (1âˆ’Î± )
= log( t tâˆ’1 tâˆ’1 t t ).
2 1âˆ’Î± +Î± (1âˆ’Î± )
tâˆ’1 t t
C IMPLEMENTATION DETAILS
Forallreconstructiontask, weemployaÏ„ = 1.0andÎ» = 1.0,Î» = 0.0with32samplingsteps
1 2
and26renoisingsteps. Foreditingtasks,thehyper-parametersaresummarizedinTable7.
AllmodelsareimplementedinPyTorch2.0andinferencedonasingleNVIDIAA10040GB.
15Preprint
EditingExperiment Hyper-parameters
Method Configuration CFG Î» Î» Ï„
1 2
Paella Set1 10.0 0.7 0.3 0.9
VQ-Diffusion Set1 5.0 0.2 0.8 1.0
Set1 - 0.2 0.8 0.7
RoBERTaSentiment
Set2 - 0.25 0.75 0.7
Table7: Hyper-parametersforPaella,VQ-Diffusion,andRoBERTasentimenteditingexperiments.
ForsentimenteditingtaskwithRoBERTa,weutilizetwosetsofhyper-parametersempiricallydue
tothevarianceinthesentencelength.
D ABLATION STUDIES
D.1 NOISEINJECTIONFUNCTION
Addition. Inthemaintextwehaveadoptedtheadditionfunctionasnoiseinjectionfunction,
yËœ=log(Ï€)+Î» Â·z+Î» Â·g.
1 2
This is a natural form inspired by the Gumbel-Max trick: thinking of Î» Â·z as a correction term,
1
thenlog(Ï€)+Î» Â·zisthecorrectedlogitandÎ» istheinverseoftemperatureofthelogittocontrol
1 2
thesharpnessoftheresultingcategoricaldistribution,as
1
argmax(log(Ï€)+Î» Â·z+Î» Â·g)=argmax( (log(Ï€)+Î» Â·z)+g), Î» >0.
1 2 Î» 1 2
2
Î» thencontrolshowmuchcorrectionwewouldliketointroduceintheoriginallogit.
1
Variancepreserving. Fromanotherperspective,z istheartificialâ€œGumbelâ€noisethatcouldhave
beensampledtorealizethetargettokens. Then,ifwetreatzasGumbelnoiseandwanttoperturbit
withrandomGumbelnoise,additiondoesnotresultinaGumbeldistribution.Onewayistoapprox-
imatethissumwithanotherGumbeldistribution. IfG âˆ¼Gumbel(Âµ ,Î² ),G âˆ¼Gumbel(Âµ ,Î² )
1 1 1 2 2 2
andG=Î» G +Î» G ,thenthemomentmatchingGumbelapproximationforGis
1 1 2 2
Gumbel(Âµ ,Î² ), with
G G
(cid:113)
Î² = Î»2Î²2+Î»2Î²2,
G 1 1 2 2
Âµ =Î» Âµ +Î» Âµ +Î³(Î» Î² +Î» Î² âˆ’Î² ),
G 1 1 2 2 1 1 2 2 G
whereÎ³ â‰ˆ0.5772istheEuler-Mascheroniconstant. Weconsiderthevariancepreservingform:
(cid:112) (cid:112)
yËœ=log(Ï€)+ Î» Â·z+ Î» Â·g, Î» +Î» =1.
1 2 1 2
Max. The third way is inspired by the property of Gumbel distribution (Wikipedia contributors,
2024),thatifG ,G areiidrandomvariablesfollowingGumbel(Âµ,Î²)thenmax{G ,G }âˆ’Î²log2
1 2 1 2
followsthesamedistribution. Wealsoconsiderthemaxfunctionfornoiseinjection:
yËœ=log(Ï€)+max{Î» Â·z,Î» Â·g}.
1 2
D.2 HYPERPARAMETERSEARCH
In this section, we analyze the impact of varying hyperparameters Î» ,Î» ,Ï„, and CFG scale on
1 2
thequalityofimagegenerationandadherencetotextualdescriptions,quantifiedthroughStructure
Distance and CLIP similarity. The hyperparameters play specific roles: Î» controls the amount of
noise introduced in each reverse step, Ï„ governs the percentage of tokens replaced with random
tokensduringinversion,andClassifier-FreeGuidance(CFG)scalestheinfluenceofthetextprompt
duringimagesynthesis. Tolimitthesearchspaceandsimplifytheablation,wechooseÎ» =Î»and
1
Î» =1âˆ’Î»andvarythevalueofÎ». EvaluationmetricsaregiveninFigure5.
2
Effect of Î» and Î» : With a fixed CFG of 10.0, the graphs indicate that increasing Î» results in a
1 2
riseinStructureDistance,suggestingadeclineinstructuralintegrityoftheimages. Thisincreasein
16Preprint
CFG=10.0 CFG=10.0
0.07 values 0.07 values
0.1 0.3
0.06 0.3 0.06 0.5
0.5 0.7
0.05 0.7 0.05 0.9
0.04 0.04
0.03 0.03
0.02 0.02
0.01 0.01
0.00 0.00
22.75 23.00 23.25 23.50 23.75 24.00 24.25 24.50 22.75 23.00 23.25 23.50 23.75 24.00 24.25 24.50
CLIP Similarity( ) CLIP Similarity( )
=0.7 =0.9
0.030 CFG CFG
6.0 0.07 6.0
0.025 7.5 7.5
8.0 0.06 8.0
10.0 10.0
0.020 0.05
16.0 16.0
0.015 0.04
0.03
0.010
0.02
0.005
0.01
0.000
22.6 22.8 23.0 23.2 23.4 23.6 23.8 24.0 24.2 23.2 23.4 23.6 23.8 24.0 24.2 24.4 24.6
CLIP Similarity( ) CLIP Similarity( )
Figure5:TheeffectofhyperparametersÎ» ,Î» ,Ï„,CFGontheStructureDistance(â†“)andCLIP
1 2
similarity(â†‘)withadditionfunctionasnoiseinjectfunction. Inourimplementation,tolimitthe
searchspace,wechooseÎ» =Î»andÎ» =1âˆ’Î»forsimplicity.
1 2
noiseappearstoallowforgreaterexplorationofthegenerativespaceattheexpenseofsomelossin
imageclarity.
EffectofÏ„: HigherÏ„ values,particularlyat0.9,showanotableriseinStructureDistanceasCLIP
similarityincreases. Thisimpliesthatmoretokenreplacementcanleadtoimagesthatalignbetter
withthetextpromptsbutmaysufferinmaintainingstructuralfidelity,likelyduetox containsless
T
informationoftheoriginalimagewhileÎ»injectsadditionalnoiseduringeditingphase.
EffectofCFGScale: VaryingCFGatafixedÎ»of0.7andÏ„ of0.9revealsthathigherCFGvalues
substantiallyimproveStructureDistance, buttoanextent(CFGof10). Beyondthispoint, further
increasesinCFGdonotyieldsignificantimprovementsinstructuralquality,indicatingadiminishing
returnonhigherguidancelevels. ThisplateausuggeststhatwhileincreasingCFGhelpsinaligning
thegeneratedimagesmorecloselywiththetextpromptsinitially,thebenefitsinstructuralintegrity
andclaritybecomelessvisibleasCFGvaluesexceedacertainthreshold. Thisfindingunderscores
theneedforabalancedapproachinsettingCFG,wheretoomuchguidancemaynotnecessarilylead
tobetteroutcomesintermsofimagequalityandfidelitytothetextualdescription.
Effect of noise injection function: We also conducted evaluations using a variance-preserving
âˆš âˆš
noise injection function by setting Î» = Î» and Î» = 1âˆ’Î». The results of these experiments
1 2
arepresentedinFigure6. Asforthemaxfunction,weperformedamanualinspectionofthevisual
examples generated with this function. The quality of these examples was noticeably inferior, we
thereforeomitthecorrespondingevaluationcurvesfromouranalysis.
Inconclusion,thisablationstudydemonstratesthatincreasingÎ»andÏ„ canenhanceadherencetotext
promptsthroughbroaderexplorationsingenerativespaces,yetthisbenefitisoffsetbyadecreasein
thestructuralqualityoftheimages. Ontheotherhand,raisingCFGvaluesenhancesthestructural
integrityofimagestoacertainthreshold,afterwhichtheimprovementsplateau,indicatingaceiling
to the effectiveness of higher CFG settings. This analysis offers empirical guidance for selecting
hyperparameters, balancing the trade-offs between text alignment and image quality to optimize
imagesynthesisoutcomes.
17
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtSPreprint
CFG=10.0 CFG=10.0
values values
0.1 0.3
0.08 0.3 0.08 0.5
0.5 0.7
0.7 0.9
0.06 0.9 0.06
1.0
0.04 0.04
0.02 0.02
0.00 0.00
22.5 23.0 23.5 24.0 24.5 22.5 23.0 23.5 24.0 24.5
CLIP Similarity( ) CLIP Similarity( )
=0.7 =0.9
CFG 0.10 CFG
0.030 5.0 5.0
7.5 7.5
0.025 10.0 0.08 10.0
16.0 16.0
0.020
0.06
0.015
0.04
0.010
0.02
0.005
0.000 0.00
22.4 22.6 22.8 23.0 23.2 23.4 23.6 23.8 24.0 23.0 23.2 23.4 23.6 23.8 24.0 24.2
CLIP Similarity( ) CLIP Similarity( )
Figure6: TheeffectofhyperparametersÎ» ,Î» withvariancepreservingscheme. WesetÎ» =
âˆš âˆš 1 2 1
Î»andÎ» = 1âˆ’Î».
2
E ADDITIONAL RESULTS ON IMAGE EDITING
ReconstructionresultwithPaella. InFigure8wedemonstratestheinversionreconstructionresult
withPaellausingourproposedmethod.
Image editing with diversity. As shown in Figure 10, our method enables diverse image editing
resultsthroughstochasticvariation. Thefirstthreerowsdemonstratetheimpactofvaryingboththe
inversionmasksandtheinjectedGumbelnoise,whilethelasttworowsfocusonvariationsproduced
bychangingonlytheinversionmasks.
F ADDITIONAL RESULTS ON TEXT EDITING
Datasetgeneration. Togeneratethedataset,weutilizeChatGPT-4owiththefollowingprompt:
User
Generate200pairsofsentencesthatcontainsthesamemeaning,butonewithpositivesenti-
mentandonewithnegativesentiment. Forbothpositivesentimentandnegativesentiment,
you need to write two sentences with the first part being a hint of the sentiment and the
secondpartbeingtheactualcontent. Thefirstpartforbothsentencesshouldbesame. write
intheformatlike:
hint. positive.
hint. negative.
Make sure that there are two lines for each pairs. Also, the hint should provide enough
contextandbothpositiveandnegativesentimentshouldberelatedtothehint. Donotrepeat
thehint, alsomakesurethatthereisonlytwosentencesineachoftheline, oneisthehint
andtheotherisaboutthesentiment.
18
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtSPreprint
Lambda = 0.1: Linear, Exponential, and Uniform Lambda = 0.3: Linear, Exponential, and Uniform
Type Type
0.07
Linear 0.06 Linear
Exponential Exponential
0.06
Uniform 0.05 Uniform
0.05
0.04
0.04
0.03
0.03
0.02
0.02
0.01
0.01
0.00
20.0 20.5 21.0 21.5 22.0 22.5 19.0 19.5 20.0 20.5 21.0 21.5 22.0 22.5
CLIP Similarity( ) CLIP Similarity( )
Lambda = 0.5: Linear, Exponential, and Uniform Lambda = 0.7: Linear, Exponential, and Uniform
0.05 Type 0.030 Type
Linear Linear
Exponential 0.025 Exponential 0.04 Uniform Uniform
0.020
0.03
0.015
0.02
0.010
0.01
0.005
0.00 0.000
19.5 20.0 20.5 21.0 21.5 22.0 19.5 20.0 20.5 21.0 21.5 22.0
CLIP Similarity( ) CLIP Similarity( )
Figure7: TheeffectofdifferentÎ»scheduleontheStructureDistance(â†“)andCLIPsimilarity
(â†‘). In our implementation, to limit the search space, we choose Î» = Î» and Î» = 1 âˆ’ Î» for
1 2
simplicity.
19
)
(ecnatsiD
erutcurtS
) (ecnatsiD
erutcurtS
)
(ecnatsiD
erutcurtS
) (ecnatsiD
erutcurtSPreprint
ChatGPT
1. Thankstoherefforts. Theeventwasahugesuccess.
Despiteherefforts. Theeventwasacompletedisaster.
2. ...
Thesentencesisthenaddedwithaprefixtoindicatesthesentimentofthecontext. Herewedemon-
stratesasubsetofourgenerateddataset:
1. PositiveSentiment: Thankstoherefforts. Theeventwasahugesuccess.
NegativeSentiment: Despiteherefforts. Theeventwasacompletedisaster.
2. Positive Sentiment: This book is definitely interesting. I canâ€™t put it down; itâ€™s full of
surprises.
Negative Sentiment: This book is definitely interesting. I canâ€™t wait to finish it; itâ€™s so
predictable.
3. PositiveSentiment: Thenewofficespaceisfantastic. Itâ€™sspaciousandperfectforproduc-
tivity.
NegativeSentiment: Thenewofficespaceisfantastic. Itâ€™scrampedandlacksproperfacil-
ities.
4. PositiveSentiment: Thankstoherefforts. Theeventwasahugesuccess.
NegativeSentiment: Despiteherefforts. Theeventwasacompletedisaster.
5. PositiveSentiment: Regardingthelecture. Itwasinsightfulandengaging.
NegativeSentiment: Regardingthelecture. Itwasdullandconfusing.
6. PositiveSentiment: Despitetheinitialproblems. Theprojectwasasuccess.
NegativeSentiment: Despitetheinitialproblems. Theprojectendedinfailure.
7. PositiveSentiment: Regardingthenewapp. Itâ€™suser-friendlyandveryhelpful.
NegativeSentiment: Regardingthenewapp. Itâ€™scomplicatedandnotuseful.
8. PositiveSentiment:Reflectingonmyenvironmentalinitiatives. Implementingchangeshas
reducedmycarbonfootprint.
NegativeSentiment: Reflectingonmyenvironmentalinitiatives. Itâ€™schallengingtomain-
tain,andprogressisslow.
9. PositiveSentiment: Thebusinessproposalwaswell-received. Theideaswereinnovative,
andthepresentationwasconvincing.
NegativeSentiment: Thebusinessproposalwasrejected. Theideaswereimpractical,and
thepresentationwasunconvincing.
10. PositiveSentiment: Thetrainingprogramwashighlyeffective. Itboostedskillsandconfi-
dence,andeveryoneleftmotivated.
NegativeSentiment: Thetrainingprogramwasineffective. Itdidnâ€™tteachmuch,andmost
peopleleftfeelingunmotivated.
11. ...
Evaluation. Below,wedemonstratethepromptusedforevaluatingtheeditingresults:
User
Giventhreesentences,confirmthatthesecondsentenceisroughlythesamesentencestruc-
tureasthefirstsentence,thenconfirmthatthesecondsentencehaspositivesentiment. Out-
put only two numbers with each number indicating whether the corresponding criteria is
satisfied. Use1forsatisfiedand0fornotsatisfied. Thesentencesaregivenbelow:
Theeventwasacompletedisaster.
Thiseventwasafantasticcomedygame.
20Preprint
Input Image Reconstruction Editing Input Image Reconstruction Editing
(a) two origami birds sitting on a branch (b) A cat dog sitting on a wooden chair
(c) a cat tiger sitting next to a mirror (d) white plate with fruits pizza on it
(e) drawing of tulip lion on the coffee (f) meat balls sushi on white plate
(g) a plate with steak salmon on it (h) white tiger cat on brown ground
Figure8: ReconstructionandeditingresultwithDICE+Paella.
ChatGPT
1 1
ComparisonbetweenmaskedinpaintingandDICE.InFigure9wedemonstratesthereconstruc-
tionandeditingresultswithourDICEandMaskedInpainting.
21Preprint
Reconstruction Editing
Input Image MaskedInpainting DiscreteInversion MaskedInpainting DiscreteInversion
(a) a round square cake with orange frosting on a wooden plate
(b) a cat dog sitting on a wooden chair
(c) a colorful red bird standing on a branch
(d) meat balls sushi on white plate
Figure9: ReconstructionandeditingresultwithDICEandmaskedinpainting. Noticethatfor
reconstruction,weusetheredprompt,butforeditingweusethegreenprompt.
22Preprint
Input Image DifferentSamples from DICE
A sketch sculpture of a cat
two origami birds sitting on a branch
white tiger cat on brown ground
a cat dog sitting on a wooden chair
a dog lion is laying down on a white background
Figure10: ImageEditingwithDiversity. Duetothestochasticnatureofourmethod,wecangen-
eratediverseoutputs. Thefirstthreerowsillustratevariationsinbothinversionmasksandinjected
Gumbelnoise(Î» =0.7,Î» =0.3). Thelasttworowsdemonstratevariationsusingonlyinversion
1 2
masks(Î» =1,Î» =0).
1 2
23
Varying
in
Mask
and
Gumbel
Noise
Varying
in
Mask