Deal, or no deal (or who knows)?
Forecasting Uncertainty in Conversations using Large Language Models
AnthonySicilia‚ô≠‚àó HyunwooKim‚ôÆ KhyathiRaghaviChandu‚ôÆ
MaliheAlikhani‚ô≠ JackHessel‚ôØ
‚ô≠NortheasternUniversity ‚ôÆAllenInstituteforAI ‚ôØSamayaAI
{sicilia.a, m.alikhani}@northeastern.edu
{hyunwook, khyathic}@allenai.org jmhessel@gmail.com
Abstract 8 Benchmark Conversation Uncertainty Tasks
Collaborative Negotiations (like legal cases) Distributive Negotiations
Effective interlocutors account for the uncer- üë®‚öñ Well‚Ä¶ Your Honor üíÅ If I can the balls you
tain goals, beliefs, and emotions of others. üë©‚öñ And it's just too bad for the U.S. can have the books
üë®‚öñ -- No, to the contrary, the U.S. claim üôã How about I get 1 hat
Buteventhebesthumanconversationalistcan-
is preserved. They can prove that. and 1 book?
not perfectly anticipate the trajectory of a di- üë©‚öñ But your position was that it can't... ü§¶ Can‚Äôt take that deal.
alogue. How well can language models rep- 2 Uncertainty Tuning and Inference Methods
resent inherent uncertainty in conversations?
Implicit Uncertainty Direct Uncertainty
Wepropose FortUneDial,anexpansion Will the petition be accepted? What are the chances of a deal?
ofthelong-standing‚Äúconversationforecasting‚Äù There is a
P(token=‚ÄúYes‚Äù) ü§ñ 60% chance.
task: instead of just accuracy, evaluation is
conductedwithuncertainty-awaremetrics,ef-
Post Correction, Interpretable Eval, Open LMs
fectivelyenablingabstentiononindividualin-
stances. Westudytwowaysinwhichlanguage 60% ‚öñ 80% üî¨ ‚úÖ üîê
models potentially represent outcome uncer-
tainty(internally,usingscoresanddirectly,us-
Figure 1: FortUne Dial tests the ability of lan-
ingtokens)andproposefine-tuningstrategies
guagemodelstorepresentuncertaintyaboutfuturecon-
toimprovecalibrationofbothrepresentations.
versationoutcomes. Tomeetthistask,wetunemodels
Experimentsoneightdifficultnegotiationcor-
toexpressuncertaintydirectlyintheiroutputtokensor
porademonstratethatourproposedfine-tuning
implicitlyintheirscoredistributions. Wealsoprovide
strategies(atraditionalsupervisionstrategyand
additionalstrategiestocorrectuncertaintyatinference-
anoff-policyreinforcementlearningstrategy)
time.Weproposetasksacross8datasets,experimenting
can calibrate smaller open-source models to
withGPT-4, Llama-2, andZephyr-stylemodelstore-
competewithpre-trainedmodels10xtheirsize.
leaseourbestperformingmodelspublicly.
1 Introduction
Dialogue models are increasingly fluent, topical, appropriately(DruckmanandOlekalns,2008),and
andinformativeconversationalists,capableofpre- demonstrate ability to both anticipate and affect
dicting plausible next-utterances given a partial thelikelihoodoffutureconversationoutcomes(Ho
conversation. Yet, the capacity to generate a sin- etal.,2022). Meanwhile,itisnotyetcleariflan-
gle,plausibleutteranceisnotthesameasmodeling guage models posses even the simplest of these
theuncertaintyaboutallpossiblenext-utterances capabilities: anticipationofoutcomecertainty.
in a calibrated way ‚Äì that is, assigning an appro-
Tostudythis,weexpandthelong-standing‚Äúcon-
priate probability to potential conversation out-
versationforecasting‚Äùtask(Sokolovaetal.,2008;
comes, reflective of the randomness we observe
Zhang et al., 2018). While the usual goal is to
in the real world. For example, in negotiations,
predicttheoutcomeofanunfoldingdialogue,we
‚ÄúSounds good!‚Äù or ‚ÄúNo thanks‚Äù may be equally
insteadaccountforhowwelllanguagemodelsrep-
fluent/topical/informativenext-utterances,butone
resent uncertainty about outcomes by measuring
choice may be more likely if the goals, beliefs,
performance with calibration metrics. In effect,
andemotionsoftheinterlocutorsaretakenintoac-
these calibration metrics allow models to abstain
count. Whileeventhebestconversationalistscan-
from predicting on instances when they estimate
not perfectly predict the trajectory of a dialogue,
highuncertainty. Potentialapplicationsofmodels
humansoftenmanageuncertaintyaboutsocialcues
performantinthissettinginclude: improvedtools
‚àóWorkdoneoninternshipatAllenInstituteforAI. forstudyingtheeffectsofstrategyandsocialstruc-
4202
beF
5
]LC.sc[
1v48230.2042:viXratureinnegotiations(CurhanandPentland,2007), Speakerturnsaredelimitedbyspecialsequencesof
intervening to improve human and machine con- tokens;e.g.,‚ÄúSpeaker4: ...‚Äù Thesepartialconver-
versations (Lewis et al., 2017; Zhou et al., 2019; sationsareunfinished,buthaveeventualoutcome
Schlugeretal.,2022;Argyleetal.,2023),orassess- O ‚àà O = {0,1}.2 Nature picks a conversation
ingtrust/heterogeneityinadatasourceviametrics distributionDoverT‚àó√óO whichgovernsoursu-
likeentropy(Cs√°kyetal.,2019;Kuhnetal.,2022). pervisedobservations: (D,O) ‚àº D. Aforecaster
Here,wefocusonthecaseofnegotiations;this f maps D (cid:55)‚Üí PÀÜ ‚àà [0,1] where PÀÜ estimates the
type of conversation is not only particularly sen- probabilityO = 1.
sitive to social uncertainties, but also, outcomes
Evaluation with Proper Scores A calibrated
arereadilyquantifiedpost-hoc. Weasklanguage
forecastersatisfies(Br√∂cker,2009):
modelsquestionsaboutthelikelihoodofdeals,de-
cisions,andemotionalconflictsinsettingslikemar- E[O | PÀÜ = p] = p ‚àÄp ‚àà {f(x) | x ‚àà T‚àó}, (1)
ketplaces,onlineforums,andcourtrooms,totaling
8taskstotestuncertaintymodelinginnegotiations. whichintuitivelymeansifweconsiderallconver-
Ourcontributionsinclude: sationsassignedpbytheforecaster,themeanoc-
1. formalizingtheconversationuncertaintymodel- currence ofthe outcome shouldalso be p. While
ingtask,alongwithitsmetrics(¬ß2.1); commonlyusedtoassestheverityofgeneralprob-
2. introducingtwomethodsforrepresentinguncer- abilityestimates(Guoetal.,2017)theconstraint
taintyabouttheoutcomeofconversationsusing inEq.(1)isoftentoobroadbecausecalibration,by
languagemodels(¬ß2.2); itself, fails to measure the variance of a forecast
3. and proposing fine-tuning (¬ß 2.3, ¬ß 2.4) and (Ovadia et al., 2019). For example, the constant
inference-timestrategies(¬ß2.5)forimproving forecast PÀÜ = E[O] is calibrated, but rarely cap-
theserepresentations. turesthetrueoutcomeprobability(conditionedon
the conversation). The issue of variance is espe-
Wecallthistask FortUneDial.1 Experiments
cially important in our setting, where social and
(¬ß3)showGPT-4andotherlargemodelscanantic-
temporaluncertaintiesmakeanticipationdifficult;
ipateoutcomecertaintywell,improvingoverprior
i.e., the basic, indiscernible prediction E[O] may
knowledge by up to 9%. Moreover, results show
becompetitive. Toaccommodatecalibrationand
the utility of our fine-tuning strategies: smaller
variance,weconsidertheconstraint
(7B) models are tuned to outperform pre-trained,
open-source models 10x their size. Indeed, met- PÀÜ = P d =ef E[O | D] (2)
ricsimproveupto11%onthetuningdatasetsand
upto3%out-of-distribution. Inthemostdifficult
Onewaytoachievethisisbyoptimizingascoring
settings (out-of-distribution with incorrect prior functions : [0,1]√óO ‚Üí R :
‚â•0
knowledge), our fine-tuned models still meet the
performanceoftheirlargercounterparts. Besides min E[s(PÀÜ,O)]. (3)
f
theperformanceofourmodeldeliverables,exper-
imentsalsocommunicateinsightonthebiasesof If the scoring function is strictly proper,3 Eq. (2)
pre-trainedlanguagemodelsatthistask,theability issatisfiedbytheminimizerof(3),sosolving(3)
ofdifferentmodelstomakeuseofpriorknowledge, recoversthetrueuncertaintyasdesired. Moreover,
theimpactofmodel/datascale,andthegeneraliza- Eq.(3),indeed,optimizesvarianceandcalibration
tion of different algorithmic strategies. Models, equally, among other nice properties for ranking
code,anddatawillbemadeopen-source. suboptimalforecasts(Br√∂cker,2009).
TangibleScores Weuseproperscoresonly,such
2 ModelingUncertaintyinConversations
astheBrierScore(BS;Brier,1950),whichisthe
2.1 Problem,Notation,andEvaluation meansquarederrorbetweenforecastprobabilities
Consider a natural language token set T. We ob- andtrueoutcomes. Whiletheuseofproperscores
serve partial multi-party dialogues D ‚àà T‚àó con-
2We only consider binary outcomes, but are flexible in
sisting of K ‚àº U{2,L} turns, with L+1 being applicationofthisformulation,e.g.,wecanaskmultipleques-
theeventual(random)lengthofthefulldialogue. tionstohandlemoreoutcomesinaone-vs-allfashion.
3StrictproprietyrequiresthatE[s(P,O)] ‚â§ E[s(PÀÜ,O)]
1ForecastingUncertaintyinDialogue. forallPÀÜwithequalityifandonlyifPÀÜ =P.isimportant(seeprevious),theydopresentsome 2. DirectForecasts(DF):ThepromptŒ¶ poses
O
caveats: (1)theylackinterpretableunitsand(2)for the modified question ‚ÄúGiven the partial dia-
fixedtasks,theyoftenvaryonasmallscale.4 Tore- logue D, what is the probability the outcome
solvetheseissues,wesometimesfocusevaluation representedbyO willoccur?‚Äù Then,themodel
ontheBrierskillscore: forecastsas:
BSS = 1‚àíBS/BS ref (4) PÀÜ DF = p‚ó¶T; T ‚àº LM‚àó Œ∏ ‚ó¶Œ¶ O ‚ó¶D (6)
whereBSistheBrierscoreoftheforecasterweare wherep : T‚àó ‚Üí [0,1]isaparserthatextractsa
evaluating and BS is the Brier score of some ‚Äúprobabilityestimate‚ÄùfromsampleT;i.e.,the
ref
referenceforecaster. Onewaytointerprettheskill modelanswersdirectlyinnaturallanguage.
scoreisthepercentimprovementoftheforecaster
Moredetailsofexactpromptsarein¬ß3. Abstractly,
compared to the reference. A simple reference,
both prompts provide a language description of
firstproposedbyBrier(1950),istheconstantpre-
theuncertaintymodelingtask,butmakedifferent
dictionE[O],inwhichcaseBS happenstobe
ref assumptions.
thevarianceoftheoutcome. Here,wemayinter-
pret the skill score as the percent of variance in 2.3 UncertaintyTuningofImplicitForecasts
outcome that is explained by the forecaster, like
Weconsideralanguagemodelwithpre-trainedpa-
an R2-value. On the other hand, E[O] can also
rametersŒ∏ ,e.g.,pre-tunedtofollowinstructions
init
be viewed as prior knowledge, obtainable before
(Ouyangetal.,2022). Themodelcomputesascore
observingD,implyingskillconveysimprovement vectorZ|Œ¶(D) ‚àà R|T| andusesZ toforecast:
overourpriorknowledge. Asdesired,skillscore
exp(Z /œÑ)
tends to vary more than Brier score, while also PÀÜ = P{T = yes} = yes (7)
IF (cid:80)
exp(Z /œÑ)
havinganinterpretableunit(percentage). t‚ààT t
wheretemperatureœÑ isafixedhyper-parameter. A
2.2 LanguageModelsasGeneralForecasters
fine-tuningobjectivecanthenbewritten:
An(auto-regressive)languagemodelLM isafunc-
Œ∏
tionparameterizedbyŒ∏ ‚àà Rd thatreturnsadistri- max E[OlnPÀÜ IF+OlnP{T = no}] (8)
Œ∏:Œ∏ ‚ÜíŒ∏
butionoverthenexttokent ‚àà T conditionaltoany init
prefixx ‚àà T‚àó. WewriteT ‚àº LM (x)forasingle where no is a dis-affirmation token. In effect,
Œ∏
tokensampleandT ‚àº LM‚àó(x)fortheiteratedsam- Eq.(8)translatestheobjectiveinEq.(3)toafine-
Œ∏
plingprocess,whereinweappendasampledtoken tuning objective by picking s to be the negative
toxandre-sampleuntilastoppingcondition. We logscore(aproperscore,essentiallyequivalentto
define a prompt Œ¶ as a function Œ¶ : T‚àó ‚Üí T‚àó standard cross-entropy). Jiang et al. (2021) also
suchthatforanyinputx,itholdsthatxissubstring considercalibrationofpre-trainedlanguagemod-
ofŒ¶(x). So,Œ¶takesaninputtextxandmodifies elsbydirectsupervision(asabove),butfocuson
ittoanewtextŒ¶(x),whichcontainstheoriginal ‚Äúfactual‚Äùquestion-answeringtaskswhereanswers
are more clearly right/wrong and the inherent so-
textand(usually)addsimportantmeta-information
cial/temporaluncertaintiesofconversationareab-
forsolvingthetask;e.g.,goaldescriptors,expected
sent. In addition to a difference of setting, our
output,andothercontext. Weconsidertwotypes
proposal also differs from Jiang et al. (2021) be-
ofprompts,whichcanturnalanguagemodelinto
causeweretainthelanguagemodel‚Äôswholetoken
aprobabilityforecaster:
distributionduringinference,insteadofonlyacan-
1. ImplicitForecasts(IF):ThepromptŒ¶ poses
O didateset. In¬ßA.1,weprovideafirsttheoretical
thequestion‚ÄúGiventhepartialdialogueD,will
andempiricalcharacterizationoftheimpactofthis
the outcome represented by O occur?‚Äù Then,
choice when fine-tuning language models. Our
thelanguagemodelforecastsas
mainobservationisthesetechniquesarepractically
PÀÜ = P{T = yes}; T ‚àº LM ‚ó¶Œ¶ ‚ó¶D (5) equivalentatinference-timewithlessthan1%av-
IF Œ∏ O
eragedifferenceinforecast(forourcorpora). Thus,
whereyes ‚àà T isanaffirmationtokenand‚ó¶is weadvocatetoretainthewholetokendistribution,
functioncomposition. sinceitismoreeasilycoupledwithotherlanguage
modelingtasks;e.g.,itdoesn‚Äôtrequirespecialma-
4Thisisperhapsthereasonforcommonimproperchoices
likecalibrationerror,thatdonotaccountforforecastvariance. chinery,likealosswithseparatenormalization.Figure2: Examplesofmodelforecastsfortheeventualoccurrenceofapersonalattack. Modelsreceivepriorsfrom
data(¬ß2.5)withoutanyforecastscaling. Tuning(¬ß2.3,¬ß2.4)improves7BparametermodelsandGPT-4shows
biasagainstconflict,comparedtoothermodels(¬ß3). Thenuancesthatleadtoconflictsarenotnecessarilyobvious.
SamplingDistribution Inpractice,weconsider 2018)weapplypolicygradient.
severalnegotiationdatasets,definingdistributions
D ...D and prompts Œ¶ ...Œ¶ . At test-time, if 2.4.1 PolicyOptimization
1 ‚Ñì 1 ‚Ñì
dataset diversity is sufficient, we expect the fore- We focus on gradient-based policy optimization
castswillgeneralizetonew,possiblyunseen,envi- techniques, like REINFORCE (Williams, 1992)
ronmentsD andpromptsŒ¶ . Formally,this andPPO(Schulmanetal.,2017). Inparticular,we
‚Ñì+1 ‚Ñì+1
setupiscalleddomaingeneralization(Blanchard deriveanoff-policyversionofthepolicy-gradient
etal.,2011;Muandetetal.,2013)and,whilemany theorem, specific to our forecasting task, which
approachestothisproblemexist,simplytrainingon usesMonteCarlosamplestoproduceunbiasedes-
thebalancedaggregateofallavailabledomainsof- timatesofthegradient-updatesforouroptimization
tenperformsbestinpractice(GulrajaniandLopez- problem. Theoff-policyaspectisanimportantone.
Paz,2020);wetakethisapproachin¬ß3. Itmeanswecaniterativelysampleanypolicy(dis-
tribution) over our token space T, and use these
2.4 UncertaintyTuningofDirectForecasts
demonstrationstolearnŒ∏. Thus,whiletuning,we
Currentpre-trainingstrategiesmayprimemodels canprioritizeexplorationvs. exploitationhowever
toexpressuncertaintybestdirectly,viatheiroutput welike,whichcanbeanimportantfactorforact-
tokens;e.g. thisisobservedwhenmodelsexpress ingoptimallyinverygeneralenvironments(Jiang
uncertainty aboutfactual correctness inquestion- etal.,2023),asisdesiredbyourframework.
answering(Tianetal.,2023). Ideally,despitethe
Off-PolicyPolicyGradient Foranyrandomvari-
differentsetting,fine-tuningcanpreserveandcapi-
ableX,define¬µ asthemassfunctionofX. Then,
talizeonthispredisposition. Onechallengeisthat X
foranyreferencemodelRef : T‚àó ‚Üí ‚àÜ(T):
direct forecasts make Eq. (3) non-differentiable,
duetotheparser. So,weformulatedirectforecast
‚àá E[R] =
E(cid:104)
s ¬∑
¬µT(TÀú)
¬∑‚àá log¬µ
(TÀú)(cid:105)
tuningasaMarkovDecisionProcess. Weusere- Œ∏ TÀú ¬µ TÀú(TÀú) Œ∏ T
ward R = ‚àís(p‚ó¶T,O) with T ‚àº LM‚àó Œ∏ ‚ó¶Œ¶‚ó¶D where T ‚àº LM‚àó‚ó¶Œ¶‚ó¶D,
Œ∏ (10)
andsetstothelogscore(seeEq.8). Ineffect,the
TÀú ‚àº Ref‚àó‚ó¶Œ¶‚ó¶D,
rewardisthenegativescoreofourforecaster. Then,
theusualobjectiveJ(Œ∏)ofthisMarkovDecision ands = ‚àís(p‚ó¶TÀú,O).
TÀú
Processis:
Wederivethisin¬ßA.3. Whileotheroff-policypol-
max E[R] = ‚àí min E[s(PÀÜ DF,O)]. (9) icygradienttechniquesexist(Degrisetal.,2012;
Œ∏:Œ∏ ‚ÜíŒ∏ Œ∏:Œ∏ ‚ÜíŒ∏
init init Imanietal.,2018;KallusandUehara,2020),the
Thatis,werecovertheoriginalforecastingobjec- specificsofourproblemallowustomakesimplify-
tive. Whilesignificantmachineryhasbeendevel- ingassumptionsandyielda‚Äúsimpler‚Äùandunbiased
opedforrewardoptimization(seeSuttonandBarto, estimateof‚àá E[R]astheabove.
Œ∏Asacomputationalnote,theremaybeinstances Yet, thishelpfultechniqueisnotwell-studiedfor
wheretheratioofmassfunctionsu /u becomes direct forecasts because these are parsed from a
T TÀú
excessivelylargeorsmall,leadingtoissuesofgra- discretetokensequence(theyhavenounderlying
dientexplosionorvanishing. Toaddressthis,we latentscorestoscale). Toaddressthis,wepropose
adoptawidely-usedclippingstrategyfromProxi- aunified correctionstrategythatiswell-suitedfor
malPolicyOptimization(Schulmanetal.,2017). bothimplicitanddirectforecasts. Wesuggestes-
Specifically,forœµ ‚àà [0,1],theupdateis: timation of the underlying latent scores to allow
post-hocscaling foranyforecaststyle:
s ¬∑œâ¬∑‚àá log¬µ (TÀú) where
TÀú Œ∏ T
œâ = min(cid:110) max(cid:110) ¬µT(TÀú) ,1‚àíœµ(cid:111) ,1+œµ(cid:111) . (11) ZÀÜ yes ‚Üê logPÀÜ/(1‚àíPÀÜ)
¬µ TÀú(TÀú) ZÀú ‚Üê ZÀÜ /œÑ ‚àíŒ≤ (12)
yes yes
Besidesthecomputationalbenefits,thishasmotiva- PÀÜ ‚Üê 1/(1+exp(‚àíZÀú ))
new yes
tionsrelatedtoon-policytrust-regionoptimization
(Schulmanetal.,2015),whenRef = LM . whereœÑ relatestotemperatureasbeforeandŒ≤ is
Œ∏
a bias correction term. By estimating the latent
Off-Policies Weconsiderthreemainchoicesfor
scoreasabove,wecaneffectively‚Äúsimulate‚Äùtra-
theoff-policyreferenceRefinthiswork:
ditional(œÑ)scalinginsuchawaythatitworksfor
1. TheExplorer takesrandomactionsinthe
both implicit and direct forecasts. We argue this
token space T‚àó, restricted only in the sense
theoretically and compare our proposal to other
that it must output a description of the un-
correction techniques in ¬ß A.2, finding it theoret-
certainty (i.e., a probability). Exploration
icallyequivalent, practicallyequivalent, orbetter
canbenefitgeneralization(Jiangetal.,2023),
in general. Ultimately, we decide to use it as the
since it exposes the agent to more diverse
primarycorrectionin¬ß3.
state-actionpairsattrain-time.
2. TheExploiter takestheactionsthatareop- ABayesianView Useofpriorknowledgeiscon-
timalbasedonexperience. Forexample,the sidered an important qualifier for generalization
tuned implicit forecasts PÀÜ IF can be mapped insometheoriesofmachinelearning(McAllester,
toadeterministicpolicy,andmoreover,anear 1998). Motivated by this, we explore the use of
optimal one ‚Äì the log score in Eq. (8) is, in- naturallanguagepriorsasatypeofinference-time
deed, a proper scoring function. While opti- correction;e.g.,,‚ÄúOnaverage,thistypeofconver-
malonthetrainingdata,thismaynotbetrue sation ends with {x} about {y}%...‚Äù We explore
fornewconversationdomainsoroutcomes. three main types of priors: an average outcome
3. TheQuantizer takesactionslearnedfrom learnedfromdata,alowestimateofthedataaver-
ground truth data by binning. It is inspired age,andahighestimateofthedataaverage.
byLinetal.(2022),whocalibratemodelun-
Correction is Not Always Zero-shot Quality
certaintytofactualcorrectnessbyaveraging
correctionsrequiredatainnewdomains. Data(n
labels of correctness for different ‚Äúbins‚Äù of
‚âà250)isgenerallyusedtolearnœÑ,Œ≤ inEq.(12),
sub-tasks to estimate per task probabilities.
soscalingisnotzero-shot. Forpriors,datacanalso
Themodelistrainedinasupervisedmanner
beusedor,inpractice,a‚Äúguess‚Äùcanbemade. To
to predict these estimates directly. Lacking
simulate a ‚Äúguessed‚Äù prior, we use averages out-
clear ‚Äúsub-tasks‚Äù, we propose a (new) more
sidea95%confidenceintervalofthedataaverage
general strategy, using clustering to bin our
(n=50). Intuitively, this means we would rarely
data. In¬ß3,wealsoablateouruseofRLto
estimate this average using data (across repeated
optimize,insteadofroutinesupervision.
experiments),whichisakinto,orworsethan,hu-
Greaterdetailonthesepolicies,includingprecise
man guesswork. ¬ß 3 considers data priors
definitionsandimplementationarein¬ßA.4.
learnedfromdataandbad priorsusingahigh
2.5 Post-Hoc,Inference-TimeCorrections and low simulated guess.5 We consider only the
secondtobezero-shot,sinceittestsrobustnessto
A New View on Old Tricks If validation data
is available, temperature (œÑ) scaling (Guo et al., ‚Äúguesses‚Äùweexpecttorarelyobtainviadata.
2017) is common to correct the scale of implicit
5Wealsocheckednon-numericpriors,e.g.,remindingall
forecasts(Jiangetal.,2021;Kadavathetal.,2022). outcomeshavenon-zerolikelihood;thesewereworseoverall.3 Experiments outcomeprobability. Whenapriorisprovided,we
substitutethispriorforthedatameanintherefer-
Data & Splits We consider 8 modeling tasks
encescoretoaccountforhowpriorscanimplicitly
spanningbothtraditional(distributive)negotiations
biasforecasts. Indeed,variancearoundan(incor-
and collaborative negotiations (Chu-Carroll and
rect)priorisalwayshigherthanthetruevariance,
Carberry,1995);Table1summarizesthecorpora.
sotheprior-adjustedBSSreportsthepercentofthis
Taskshavediversesituationsandoutcomes,span
largervarianceexplainedbytheforecaster. Besides
multi-party/dyadicsettings,andarebothshort/long.
BSS,wealsosuggestanewskillscorecalledthe
To simulate varying degrees of distribution shift,
LlamaSkillScore(LSS).LSSisidenticaltousual
we group these datasets into different train/test
skillscores,i.e.,Eq.(4),butusestheBrierscoreof
splits, categorized as easy, medium, or hard.
Llama-2-chat 70B(directforecasts,sameexper-
To make the forecasting task more difficult, each
imentalsetup)asthereferencescoreBS . This
ref
ofthesethreesplitsholdoutfulldatasetsfortest-
quantifieshowsmallerfine-tunedmodelscompare
ing. Conceptually,thesplitsaredesignedtocreate
to this large model by % improvement. Finally,
different degrees of train/test imbalance for im-
we report statistical bias (BI) to convey average
portantpropertieslikethetopic,thelengthofthe
over-orunder-estimationofoutcomeprobability
conversation,thetypeofoutcome,andthenumber
(positiveornegativevalues,respectively).
of speakers; Tables 6+7 and ¬ß B.1 provide more
detail on train/val/test splits. Sometimes, we re- 3.1 Results
strict inference to data with affective conflict as
ForecastingisBetterwithCorrection Table2
outcome(neg)like,apersonalattackorunhappy
showsBrierandskillscoresofpre-trainedmodels
speaker. Here,tocomputesomemetrics,weswap
withoutuncertaintytuning. Wemodulatepriorsand
the positive and negative classes as needed, e.g.,
ablatepost-hocscaling. GPT-4haslower(better)
the positive class becomes 1‚àíO to study ‚Äúboth
Brierscoreswhengrantedaccesstovalidationdata
campsunhappy‚Äùinsteadof‚Äúbothcampshappy.‚Äù
to make corrections, e.g., via data priors or scal-
ing. Scaling appears to have the greatest impact
Models&Prompts WeuseGPT-4(0613,Ope-
onscoresastheyarelowest,evenwhenexcluding
nAI, 2023), Llama-2-chat (Touvron et al., 2023),
thedataprior. However,priorsarestilluseful. In
and Zephyr-Œ≤ (Tunstall et al., 2023), which have
zero-shot settings (i.e., no scaling or data prior),
all been pre-tuned for chat/instruction following
GPT-4performsbetterwhenithasaccessto‚Äúbad‚Äù
(Zephyrispre-tunedviadistillation). Wealsouse
guesses of prior probability, rather than no guess
a ‚Äútiny‚Äù Llama-2 replicate trained in the style of
at all. Trends are similar for Llama-2-chat 70B‚Äôs
Zephyr (Chat-v0.6, Zhang et al., 2023). Open-
directforecasts.
sourcemodelsizesrangefrom1Bto70Bparam-
eters. Weusepre-trained torefertothesemodels Problems with Pre-trained Implicit Forecasts
beforewetuneuncertainty(¬ß2.3,2.4). Concretely, ForLlama-2-chat70B,wehaveaccesstoscoresof
the prompts the models receive have: situational everytoken,sowecancompareimplicitforecast-
contextspecifictoeachdataset,like‚Äúthespeakers ingtodirectforecasting. InTable2,implicitfore-
aredefendingtheiropinionsonanissue‚Äù; priors, castingisworseforthispre-trainedmodel. Echo-
as in ¬ß 2.5; and the main question that asks the ing Kadavath et al. (2022), we find post-hoc cor-
modelaboutthelikelihoodofoutcomeoccurrence rectionisvitaltoimprovepre-trainedimplicitfore-
intheconversation,orjustoccurrenceforimplicit casts. Moreover, the degradation of uncorrected
forecasts. Pre-trainedmodelsalsoreceivesystem implicitforecastsisquitehigh,suggestingamplifi-
prompts to constrain output and clarify the task cationofthiseffectinourunique(conversational)
goals. WeuseQLoRA(Dettmersetal.,2023)for setting.
uncertaintytuning. Otherdetailsonprompts/hyper-
HaveData? TunedImplicitForecastsAreBest
parametersarein¬ßB.2.
Basedonthepreviousresults,wefocusonuncer-
Metrics We use the Brier Score (BS) and skill tainty tuning with access to a prior (even a ‚Äúbad‚Äù
score(BSS)asdiscussedin¬ß2.1,macro-averaged one)andcompareuncertaintytunedmodelstodi-
acrossdatasetsandprompts. BSSreferstotheorig- rectforecastsofpre-trainedversions. Weconsider
inal skill score (Brier, 1950) where the reference an in-domain setting first, wherein test data fol-
model in the skill score is the constant (average) lowsthetuningdistribution,post-hoccorrectionisDataset Situation Outcome #Speak #Turn #Char Aff. Distr.
Zhangetal. wikipediaediting personalattack >2 6.2 2.5K yes no
Heetal. craigslist bestdealforbuyer =2 9.8 720 no yes
Chawlaetal. campprovisions bothcampshappy =2 11.4 1.2K yes yes
Changetal. reddit personalattack >2 5.3 3.2K yes no
Wangetal. charity donationoccurs =2 20.6 2.2K yes no
Lewisetal. itemallocation dealoccurs =2 5.0 253 no yes
Mayfieldetal. wikipediaediting articledeleted >2 8.6 2.2K no no
Changetal.a courtroom petitionerwins >2 218 55K no no
Table1: Forecastingtasks. Welistsetting,outcomeofinterest,numberofspeakers,averageturn/charactercount,
andwhetherthesettingisdistributive. Wealsonoteifaffectivereasoning(aboutemotions)isusefulinforecasting.
Dataaregroupedinto3train-testsplits(easy,med.,hard)tosimulategeneralizationdifficulty(seeTables6,7).
aSeealsoDanescu-Niculescu-Miziletal.(2012)forcourtroomdata
post-hocscaling noscaling combined
(cid:24)pr(cid:24)i(cid:24) o(cid:24)
r data prior
(cid:24)pr(cid:24)i(cid:24) o(cid:24)
r data prior bad prior neg all
model BS‚Üì BS‚Üì BSS‚Üë BS‚Üì BS‚Üì BSS‚Üë BS‚Üì BSS‚Üë BI BI
gpt-4 21.1 20.7 8.5 23.8 21.8 3.8 22.7 9.0 -11 -1.1
70B IF 22.9 23.1 -2.0 66.1 66.1 -182 66.1 -160 3.6 -49.4
70B DF 22 22.1 2.3 25.0 22.0 2.7 23.6 5.0 -5.6 -1.3
Table2: Scoresforlarge,pre-trainedmodelswithdifferentaccesstopriorknowledge. Useofpost-hoccorrection
anddata-dependentpriorsisnottrulyzero-shot. GPT-4testsuseonlydirectforecasting(DF),whileLlama-2-chat
70Buseseitherdirectorimplicit(IF).BSS=0correspondstoconstantlyforecastingtheprior,e.g.,fordata
prior,thisisthecorpusmeanoutcome. DFconsistentlyimprovesoverpriorknowledge(BSS>0).
in-domain pseudoOOD zero-shotOOD combined
all all easy med hard all neg all
model BS‚Üì LSS‚Üë BS LSS LSS LSS LSS BS LSS BI BI
7B 22.7 -3.2 -10.9 -7.5 1.1 24.3 -5.1 -0.4 -2.7
(cid:44)‚ÜíIF 21 5.3 22.4 -1.4 -12.4 2.2 -3.9 23.9 -3.7 0.3 0
(cid:44)‚ÜíDF 22.8 -3.2 22.9 -3.7 -14.1 -8.2 -8.3 25.3 -9.7 0.9 0.6
(cid:44)‚ÜíDF 22.9 -3.9 22.9 -3.6 -9.3 -8.5 1.7 24.2 -4.9 0.8 -0.9
(cid:44)‚ÜíDF rl 22.9 -3.7 22.8 -3.3 -10.7 -5.9 2.1 24 -4.5 3 -0.6
(cid:44)‚ÜíDF (cid:8)r(cid:8)l 22.9 -3.8 22.9 -3.7 -11.9 -6.9 2.7 24.1 -4.9 3.4 -0.9
(cid:44)‚ÜíIF√ó4 19.6 11.4 21.4 3.7 8.9 2.9 -6.9 23.3 0.7 -3.5 -2.4
(cid:44)‚ÜíDF√ó4 22.8 -3.2 22.8 -3.4 -10.7 -3.7 4.5 23.6 -2.3 3.7 -2.8
7B 22.5 -1.2 -9.9 -9.6 -8.1 25.5 -9.1 -10.6 1.8
(cid:44)‚ÜíIF 22.5 -1.6 22.8 -3.4 -26.3 -10.2 -6.8 25.8 -13 1.5 -9.1
(cid:44)‚ÜíDF rl 23 -4.2 23 -4.2 -11.4 -8.9 -3.8 24.9 -7.6 0.2 -6.8
1B 22.8 -3.3 -11.2 -3 -2.6 24.4 -4.9 3.5 6.5
(cid:44)‚ÜíIF 22.2 -0.5 22.8 -3.2 -19.4 -10.6 -3.2 25.2 -10 -2.4 -5.5
(cid:44)‚ÜíDF rl 23 -4.2 23 -4.2 -11.4 -8.9 -3.8 24.9 -7.6 1 -1.2
Table3: ScoresforuncertaintytunedLlama-2-chat 7B,Zephyr 7B,and‚Äútiny‚Äù1BLlama-2trainedinZephyr
style. in-domainshowstestdatascoresfromwithintuningdistribution. pseudoOODandzero-shotOODshow
scoreswhentestdataisoutofdistribution(i.e.,heldoutdomains). Val. dataisusedforpost-hocscalingandpriors,
exceptunderzero-shotOOD,whichdoesn‚Äôtcorrectanduses‚Äúbad‚Äùpriors. For 7B,wealsotuneon√ó4moredata.
Improvementsarehighlighted: Lightgreen cells showimprovementagainstcorrespondingpre-trainedmodels
(samesetup,beforeuncertaintytuning),while darker cells(additionally)improveoverthelarger 70B.used,andpriorsaredata-dependent. Here,Table3 data(seepreviousdiscussion),suggestourtuning
showstunedimplicitforecastscansignificantlyim- techniquesmayfollowneuralscalinglaws(Kaplan
proveoverpre-trainedBrierscore,evencompared et al., 2020), meaning generalization is strongly
to models 10x their size. For instance, a tuned dependentonmodel,data,andcomputescale.
Llama-2-chat7Bimprovesoverthe70Bmodelby
about 11% (or, 5% with less data). With enough Exploration & Exploitation Llama-2-chat 7B
trainingdata,out-of-distribution(OOD)scoresare findingsindicatepureexploitation isdetrimental
alsoabout4%betterthanLlama-2-chat70Bscores togeneralizationwhencomparedwithpre-trained
(ifbothhaveaccesstodataforcorrection). models(seeabsenceofgreenscores). Ontheother
hand,exploration tendstooffersomeimprove-
DirectTuningGeneralizesBetter(Sometimes) ment,beatingthepre-trainedscoresoneasyand
Next,weconsiderazero-shotOODsettingwithout allaswellasthe70Bscoresonhard. Thebest
data for correction. Here, performance of tuned tuneddirectforecastsusequantization ,whichis
implicit forecasts is still good, for Llama-2-chat neithercompletelyrandomnoroptimalonthetrain
7B.Withenoughdata,tuningbringsLlama-2-chat set,offeringabalanceofexploration/exploitation.
7B implicit forecasts to the skill of its 70B coun-
terpart(+0.7%),butforZephyr-style(distillation- Benefits of RL For our best performing direct
tuned)models,degradationissignificantcompared forecasttuningmechanism,wealsoablatetherole
to(even)pre-tuningscores. Incontrast,forall7B of using RL to tune; i.e., we use a traditional su-
models and different levels of data access, direct pervisedupdaterule,similar6 toLinetal.(2022).
forecastsshowconsistentimprovementofscores We find improvements over pre-training are less
afteruncertaintytuning(seelightgreencells). For consistentandworseoverall.
Llama-2-chat7B,tuneddirectforecastsalsohandle
difficult(hard)distributionshiftupto4%better Human Preference Tuning May Induce Bias
thantheir70Bcounterpart. Asspeculatedearlier, Wealsopostulatehumanpreferencetuning(RLHF;
directforecasttuningmaypreservesomepredispo- e.g., Ouyang et al., 2022) may bias pre-trained
sitionsofpre-trainedmodelstodirectuncertainty models to under-estimate negative affective con-
signals;thismayexplainconsistentgeneralization flicts, since these are presumably undesirable to
oftheseforecastsbeyond thetuningdistribution. humanannotators. Tostudythis,wereportstatisti-
calbias(BI)offorecastprobabilitiesinpredicting
QualitativeComparisonofIFandDF Tuned negative(neg)emotionaloutcomes;i.e.,personal
implicitforecaststendtohavehighervariancethan attacks,unhappyinterlocutors,orrefusalstodonate
tuned direct forecasts. Averaging over all tuning tocharity. Onaverage,directforecastsfromGPT-4
strategies in Table 3, implicit tuning of Llama-2- underestimatethisprobabilitybyabout11%,while
chat7Bleadsto2√óthestandarddeviationinfore- directforecastsfromLlama-2-chat70Bunderesti-
casts compared to direct tuning (about 11% and mate this probability by about 6%. For the same
5% SD, respectively). As noted in ¬ß 2.1, all else models, bias across all outcomes is not as stag-
equal,ahighervarianceispreferredbyourmetrics gering. Smallerlanguagemodelsdonotnecessar-
inordertocapturethediscernabilityofforecasts. ilyexhibitthisbias,butdistillation-tunedmodels
Potentially,directlytunedmodelstendtowardsdis- maylearnsimilarbiasesfromtheirlargerteachers
tributioncollapseduetoinsufficientregularization (see 7B).Generally,uncertaintytuningdoesnot
in our RL objective (Korbak et al., 2022). Direct appeartointroduceasstaggeringbiasagainstemo-
tuningmethodswhichresolvethisissuewillbeof tionalconflict,butespeciallyfordistillation-tuned
interestinfuturework. models,theoverallbiascanbeelevated.
ImpactofScale Tuningofimplicitforecastsal-
4 RelatedWorks
low in-domain scores of a 1B model to rival a
model70xit‚Äôssize(-0.5%). But,tuningmethods
Negotiation & Conversation Forecasting Ne-
showlessimprovement,orevendegradation,when
gotiationanddisputemodelinghasalonghistory
appliedtothe1BmodelOOD.Possibly,andespe-
(LambertandCarberry,1992;Jamesonetal.,1994;
ciallysinceweuseQLoRAtuning,thereductionin
Traum et al., 2008; Lascarides and Asher, 2008)
trainableparametersisdetrimentaltothesemeth-
ods. This,andtheobservedbenefitsofincreasing 6Oursisstillmoregeneral,duetotheclusteringproposal.with early works hand-crafting models of inter- taintyaboutfutureoutcomes. Oursisalsooneof
locutorbehaviorbylogicalordiscoursestructures. fewworksthatstudyhowmodelscommunicateun-
Reinforcementlearninginsimulatedenvironments certaintydirectlyviaoutputtokens(Mielkeetal.,
offers improvement (Georgila and Traum, 2011; 2022; Tian et al., 2023), and fewer that propose
EfstathiouandLemon,2014)withmostrecentad- tuningalgorithmsforthis(Linetal.,2022).
vancesmodelingopponents‚Äôdialogueacts(Keizer
5 Conclusion
et al., 2017), word choices (He et al., 2018), and
mental states (Yang et al., 2021; Chawla et al.,
We show language models represent uncertainty
2022). Insteadoffullsimulation,wefocusoneffi-
aboutconversationaloutcomesquitewell,depend-
cientandinterpretableoutcomemodels(Sokolova
ingontheirsize,inferencestrategy,trainingstrat-
et al., 2008; Nouri and Traum, 2014). Outcome
egy, and access to prior knowledge. We design a
models,orforecasts,arealsocommoninbroader
tasktoevaluatethisabilityandshow:
dialogue for proactive moderation of social me-
‚Ä¢ large (commercial-scale) models do this well,
dia (Zhang et al., 2018; Kementchedjhieva and
providedlimiteddatatopickhyper-parameters;
S√∏gaard,2021)aswellaspredictingtask-success
‚Ä¢ without data, these models still offer improve-
(Walker et al., 2000; Reitter and Moore, 2007),
mentoverlowqualitypriors,likehumanguesses;
mental health codes (Cao et al., 2019), emotions
‚Ä¢ specialized fine-tuning can elevate small open-
(Wang et al., 2020; Matero and Schwartz, 2020),
sourcemodelstobeatmodels10xtheirsize;
situated actions (Lei et al., 2020), and financials
‚Ä¢ currentpre-trainedlanguagemodelsmaybepre-
(Kovaletal.,2023). Amongthese,oursisfirstto
disposedtorepresentinguncertaintyintheirtex-
propose and evaluate probabilistic methodology,
tualoutputs,insteadoftheirlogitdistributions;
modeling dynamic uncertainty for the first time.
‚Ä¢ explorationattrain-timecanbemorebeneficial
Our proposal is also uniquely general, operating
forgeneralization(comparedtoexploitation);
independentofsettingoroutcomeofinterest. In-
‚Ä¢ and finally, pre-trained models may be biased
deed,weevaluateongeneralnegotiations,looking
againstforecastingemotionalconflict.
beyonddistributiveapplications(zero-sumgames
Thiswork(andtask)presentsafirststeptowards
inspecificmarkets)toincludecommoncollabora-
understandinghowlanguagemodelscananticipate
tivenegotiations(Chu-CarrollandCarberry,1995),
thecertaintyofoutcomesininteractivesocialsitu-
likeplanning,wherepartiessharesomegoals,but
ations. Wemakeourcode,models,anddataopen-
conflictsarisefromcompetingsub-goalsorbeliefs.
sourcetopromotecontinuedresearch.
LanguageModels&Uncertainty Modernlarge
Limitations
languagemodelsfine-tunedtohumanpreferences
(Ouyangetal.,2022)areincreasinglygeneral,‚Äúun- Whileweexploreawidearrayofdatasetsandex-
supervised‚Äù multi-taskers. When queried on fac- perimentalsetups,thegeneralityofourconclusions
tualinformation,thesemodelsalsorepresentuncer- arelimitedtowhat‚Äôsexploredinthispaper. Further
taintyabouttheirsolutionswithlittletonosuper- study, e.g., replication study with different data,
vision(Kadavathetal.,2022). Uncertainty(about models, and settings, would provide evidence to
correctness)hasalsobeenstudiedinsmallermod- confirm the generalization of our findings. One
elswithoutpreferencetuning(DesaiandDurrett, aspect of particular interest, is the application of
2020;Jiangetal.,2021;DanandRoth,2021)with thesetechniquestolanguagesotherthanEnglish.
many algorithms for improvement (Kong et al., Indeed,thereisevidencethattheuncertaintyrep-
2020; Zhang et al., 2021; Li et al., 2022). Albeit resentations of language models may experience
similarly operationalized, modeling uncertainty performance degradation when applied to other
aboutfactualcorrectnessisdistinctfromourfocus languages(Ahujaetal.,2022;Krauseetal.,2023),
in negotiations, which elicits modeling of social especiallythosewhicharelow-resource.
dynamics and mental states. Fewer works study Additionally, while our task is motivated by a
uncertaintyinsocialreasoning(Jiangetal.,2021; desiretoprobealanguagemodel‚Äôsabilitytoantici-
HosseiniandCaragea,2022;Kumar,2022). These patesocial(un)certaintiesinconversation,thereis
lookatsmallermodelswithoutinstruction-tuning, noclearwayforustoseparatecausationandcorre-
andlackfocusontheinteractive,temporalaspects lationinthistask. Wecannotclaimthelanguage
of conversations that cultivate an inherent uncer- modelactually‚Äúunderstands‚Äùthecausesofsocial(un)certainties,likeinterlocutormentalstates,since Glenn W Brier. 1950. Verification of forecasts ex-
instead, it may be the case that language models pressed in terms of probability. Monthly weather
review,78(1):1‚Äì3.
capitalizeon‚Äúsuperficial‚Äùor‚Äúspurious‚Äùstatistical
correlationsassociatedwithanoutcome(Hoetal.,
JochenBr√∂cker.2009. Reliability,sufficiency,andthe
2022). decompositionofproperscores. QuarterlyJournal
Finally,wequantifythequalityofaforecastpri- of the Royal Meteorological Society: A journal of
theatmosphericsciences,appliedmeteorologyand
marilythroughitsimprovementoverpriorknowl-
physicaloceanography,135(643):1512‚Äì1519.
edge. Effectively,thiscomparesaforecasterwitha
performancelowerbound,demonstratingthefore- JieCao,MichaelTanana,ZacImel,EricPoitras,David
caster is using information revealed through the Atkins, andVivek Srikumar. 2019. Observing dia-
logueintherapy:Categorizingandforecastingbehav-
dialoguetoprovideanimprovedprediction. Inthe
ioralcodes. InProceedingsofthe57thAnnualMeet-
future,aperformanceupperbound(suchashuman
ingoftheAssociationforComputationalLinguistics,
performance) would be useful to establish a ceil- pages 5599‚Äì5611, Florence, Italy. Association for
ing for our goals. This is particularly important ComputationalLinguistics.
for proper scores, since it is exceedingly rare for
Jonathan P. Chang, Caleb Chiam, Liye Fu, An-
forecasters to achieve a perfect value (e.g., 0 for
drewZ.Wang,JustineZhang,andCristianDanescu-
theBrierscoreor1fortheskillscore). Niculescu-Mizil.2020. Convokit: Atoolkitforthe
analysisofconversations. InSIGDIALConferences.
EthicsStatement
JonathanP.Chang,CristianDanescu-Niculescu-Mizil,
Themodelsweuseandtrainmayexhibit,oreven and . 2019. Trouble on the horizon: Forecasting
amplify,anybiasescontainedinthetrainingdata, the derailment of online conversations as they de-
velop. In Proceedings of the 2019 Conference on
such as societal biases. Moreover, robustness
EmpiricalMethodsinNaturalLanguageProcessing
to adversaries and natural token perturbations is
andthe9thInternationalJointConferenceonNatu-
not guaranteed. In critical applications, ethical ralLanguageProcessing(EMNLP-IJCNLP),pages
andsafetyconsiderations,suchasbiasmitigation 4743‚Äì4754,HongKong,China.AssociationforCom-
putationalLinguistics.
methodologiesandcarefulhumanmoderation,can
help to remove biases and prevent safety issues
Kushal Chawla, Gale Lucas, Jonathan May, and
duringdeployment. JonathanGratch.2022. Opponentmodelinginnego-
tiationdialoguesbyrelateddataadaptation. InFind-
Acknowledgements ingsoftheAssociationforComputationalLinguistics:
NAACL2022,pages661‚Äì674,Seattle,UnitedStates.
WethankYejinChoiforherhelpfulfeedback. JH AssociationforComputationalLinguistics.
partiallycompletedthisworkwhileatAI2.
Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale
Lucas, Jonathan May, and Jonathan Gratch. 2021.
CaSiNo: Acorpusofcampsitenegotiationdialogues
References
forautomaticnegotiationsystems. InProceedings
Kabir Ahuja, Sunayana Sitaram, Sandipan Dandapat, ofthe2021ConferenceoftheNorthAmericanChap-
andMonojitChoudhury.2022. Onthecalibrationof teroftheAssociationforComputationalLinguistics:
massivelymultilinguallanguagemodels. InProceed- HumanLanguageTechnologies,pages3167‚Äì3185,
ingsofthe2022ConferenceonEmpiricalMethods Online.AssociationforComputationalLinguistics.
inNaturalLanguageProcessing,pages4310‚Äì4323,
AbuDhabi,UnitedArabEmirates.Associationfor JenniferChu-CarrollandSandraCarberry.1995. Re-
ComputationalLinguistics. sponse generation in collaborative negotiation. In
33rd Annual Meeting of the Association for Com-
Lisa P Argyle, Christopher A Bail, Ethan C Busby, putationalLinguistics,pages136‚Äì143,Cambridge,
JoshuaRGubler,ThomasHowe,ChristopherRytting, Massachusetts,USA.AssociationforComputational
TaylorSorensen,andDavidWingate.2023. Leverag- Linguistics.
ingaifordemocraticdiscourse: Chatinterventions
canimproveonlinepoliticalconversationsatscale. Lucas Clart√©, Bruno Loureiro, Florent Krzakala, and
Proceedings of the National Academy of Sciences, Lenka Zdeborov√°. 2023. Expectation consistency
120(41):e2311627120. for calibration of neural networks. arXiv preprint
arXiv:2303.02644.
GillesBlanchard,GyeminLee,andClaytonScott.2011.
Generalizingfromseveralrelatedclassificationtasks Rich√°rd Cs√°ky, Patrik Purgai, and G√°bor Recski.
toanewunlabeledsample. Advancesinneuralinfor- 2019. Improvingneuralconversationalmodelswith
mationprocessingsystems,24. entropy-baseddatafiltering. InProceedingsofthe57thAnnualMeetingoftheAssociationforComputa- MarkKHo,RebeccaSaxe,andFieryCushman.2022.
tionalLinguistics,pages5650‚Äì5669,Florence,Italy. Planningwiththeoryofmind. TrendsinCognitive
AssociationforComputationalLinguistics. Sciences,26(11):959‚Äì971.
JaredRCurhanandAlexPentland.2007. Thinslices Mahshid Hosseini and Cornelia Caragea. 2022. Cal-
ofnegotiation: predictingoutcomesfromconversa- ibrating student models for emotion-related tasks.
tionaldynamicswithinthefirst5minutes. Journal In Proceedings of the 2022 Conference on Empiri-
ofAppliedPsychology,92(3):802. calMethodsinNaturalLanguageProcessing,pages
9266‚Äì9278,AbuDhabi,UnitedArabEmirates.As-
Soham Dan and Dan Roth. 2021. On the effects of sociationforComputationalLinguistics.
transformer size on in- and out-of-domain calibra-
tion. In Findings of the Association for Computa- EhsanImani,EricGraves,andMarthaWhite.2018. An
tionalLinguistics: EMNLP2021,pages2096‚Äì2101, off-policy policy gradient theorem using emphatic
Punta Cana, Dominican Republic. Association for weightings. Advances in Neural Information Pro-
ComputationalLinguistics. cessingSystems,31.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, AnthonyJameson,BernhardKipper,AlassaneNdiaye,
BoPang,andJonKleinberg.2012. Echoesofpower: Ralph Sch√§fer, Thomas Weis, and Detlev Zimmer-
Language effects and power differences in social mann.1994. Cooperatingtobenoncooperative: The
interaction. InProceedingsofthe21stinternational dialog system PRACMA. In KI-94: Advances in
conferenceonWorldWideWeb,pages699‚Äì708. ArtificialIntelligence,18thAnnualGermanConfer-
ence on Artificial Intelligence, Saarbr√ºcken, Ger-
ThomasDegris,MarthaWhite,andRichardSSutton.
many,September18-23,1994,Proceedings,volume
2012. Off-policyactor-critic. InInternationalCon-
861 of Lecture Notes in Computer Science, pages
ferenceonMachineLearning.
106‚Äì117.Springer.
Shrey Desai and Greg Durrett. 2020. Calibration of
YidingJiang,JZicoKolter,andRobertaRaileanu.2023.
pre-trainedtransformers. InProceedingsofthe2020
Ontheimportanceofexplorationforgeneralization
Conference on Empirical Methods in Natural Lan-
inreinforcementlearning. InThirty-seventhConfer-
guageProcessing(EMNLP),pages295‚Äì302,Online.
enceonNeuralInformationProcessingSystems.
AssociationforComputationalLinguistics.
ZhengbaoJiang,JunAraki,HaiboDing,andGraham
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Neubig. 2021. How can we know when language
LukeZettlemoyer.2023. Qlora: Efficientfinetuning
modelsknow? onthecalibrationoflanguagemodels
ofquantizedllms.
forquestionanswering. TransactionsoftheAssocia-
tionforComputationalLinguistics,9:962‚Äì977.
DanielDruckmanandMaraOlekalns.2008. Emotions
innegotiation. Groupdecisionandnegotiation,17:1‚Äì
SauravKadavath,TomConerly,AmandaAskell,Tom
11.
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
IoannisEfstathiouandOliverLemon.2014. Learning
Tran-Johnson, et al. 2022. Language models
non-cooperative dialogue behaviours. In Proceed-
(mostly) know what they know. arXiv preprint
ingsofthe15thAnnualMeetingoftheSpecialInter-
arXiv:2207.05221.
est Group on Discourse and Dialogue (SIGDIAL),
pages60‚Äì68.
Nathan Kallus and Masatoshi Uehara. 2020. Statisti-
callyefficientoff-policypolicygradients. InInter-
KallirroiGeorgilaandDavidTraum.2011. Reinforce-
national Conference on Machine Learning, pages
mentlearningofargumentationdialoguepoliciesin
negotiation. In Twelfth Annual Conference of the 5089‚Äì5100.PMLR.
InternationalSpeechCommunicationAssociation.
JaredKaplan,SamMcCandlish,TomHenighan,TomB
IshaanGulrajaniandDavidLopez-Paz.2020. Insearch Brown,BenjaminChess,RewonChild,ScottGray,
oflostdomaingeneralization. InInternationalCon- AlecRadford,JeffreyWu,andDarioAmodei.2020.
ferenceonLearningRepresentations. Scaling laws for neural language models. arXiv
preprintarXiv:2001.08361.
ChuanGuo,GeoffPleiss,YuSun,andKilianQWein-
berger.2017. Oncalibrationofmodernneuralnet- Simon Keizer, Markus Guhe, Heriberto Cuay√°huitl,
works. InInternationalconferenceonmachinelearn- Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mi-
ing,pages1321‚Äì1330.PMLR. hai Dobre, Alex Lascarides, and Oliver Lemon.
2017. Evaluatingpersuasionstrategiesanddeeprein-
HeHe,DerekChen,AnushaBalakrishnan,andPercy forcementlearningmethodsfornegotiationdialogue
Liang.2018. Decouplingstrategyandgenerationin agents. InProceedingsofthe15thConferenceofthe
negotiation dialogues. In Proceedings of the 2018 EuropeanChapteroftheAssociationforComputa-
Conference on Empirical Methods in Natural Lan- tional Linguistics: Volume 2, Short Papers, pages
guageProcessing,pages2333‚Äì2343,Brussels,Bel- 480‚Äì484,Valencia,Spain.AssociationforComputa-
gium.AssociationforComputationalLinguistics. tionalLinguistics.YovaKementchedjhievaandAndersS√∏gaard.2021. Dy- JieLei,LichengYu,TamaraBerg,andMohitBansal.
namicforecastingofconversationderailment. InPro- 2020. What is more likely to happen next? video-
ceedingsofthe2021ConferenceonEmpiricalMeth- and-language future event prediction. In Proceed-
ods in Natural Language Processing, pages 7915‚Äì ingsofthe2020ConferenceonEmpiricalMethods
7919. in Natural Language Processing (EMNLP), pages
8769‚Äì8784,Online.AssociationforComputational
Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Linguistics.
Lyu, Tuo Zhao, and Chao Zhang. 2020. Cali-
brated language model fine-tuning for in- and out- MikeLewis,DenisYarats,YannDauphin,DeviParikh,
of-distributiondata. InProceedingsofthe2020Con- and Dhruv Batra. 2017. Deal or no deal? end-to-
ferenceonEmpiricalMethodsinNaturalLanguage end learningof negotiationdialogues. In Proceed-
Processing(EMNLP),pages1326‚Äì1340,Online.As- ingsofthe2017ConferenceonEmpiricalMethods
sociationforComputationalLinguistics. inNaturalLanguageProcessing,pages2443‚Äì2453,
Copenhagen, Denmark. Association for Computa-
tionalLinguistics.
Tomasz Korbak, Ethan Perez, and Christopher Buck-
ley. 2022. RL with KL penalties is better viewed
DongfangLi,BaotianHu,andQingcaiChen.2022. Cal-
as Bayesian inference. In Findings of the Associa-
ibrationmeetsexplanation:Asimpleandeffectiveap-
tionforComputationalLinguistics: EMNLP2022,
proachformodelconfidenceestimates. InProceed-
pages1083‚Äì1091,AbuDhabi,UnitedArabEmirates.
ingsofthe2022ConferenceonEmpiricalMethods
AssociationforComputationalLinguistics.
inNaturalLanguageProcessing,pages2775‚Äì2784,
AbuDhabi,UnitedArabEmirates.Associationfor
RossKoval,NicholasAndrews,andXifengYan.2023.
ComputationalLinguistics.
Forecastingearningssurprisesfromconferencecall
transcripts. InFindingsoftheAssociationforCom-
StephanieLin,JacobHilton,andOwainEvans.2022.
putationalLinguistics: ACL2023,pages8197‚Äì8209,
Teaching models to express their uncertainty in
Toronto,Canada.AssociationforComputationalLin-
words. arXivpreprintarXiv:2205.14334.
guistics.
Ilya Loshchilov and Frank Hutter. 2017. Fixing
LeaKrause,WondimagegnhueTufa,SeleneBaezSan- weight decay regularization in adam. CoRR,
tamaria,AngelDaza,UrjaKhurana,andPiekVossen. abs/1711.05101.
2023. Confidently wrong: Exploring the calibra-
tion and expression of (un)certainty of large lan- MatthewMateroandHAndrewSchwartz.2020. Au-
guage models in a multilingual setting. In Pro- toregressive affective language forecasting: a self-
ceedingsoftheWorkshoponMultimodal,Multilin- supervisedtask. InProceedingsofCOLING.Inter-
gualNaturalLanguageGenerationandMultilingual nationalConferenceonComputationalLinguistics,
WebNLG Challenge (MM-NLG 2023), pages 1‚Äì9, volume2020,page2913.NIHPublicAccess.
Prague,CzechRepublic.AssociationforComputa-
tionalLinguistics. ElijahMayfield,AlanW.Black,and.2019. Analyzing
wikipedia deletion debates with a group decision-
LorenzKuhn,YarinGal,andSebastianFarquhar.2022. makingforecastmodel. Proc.ACMHum.-Comput.
Semanticuncertainty: Linguisticinvariancesforun- Interact.,3(CSCW).
certaintyestimationinnaturallanguagegeneration.
David A McAllester. 1998. Some pac-bayesian theo-
InTheEleventhInternationalConferenceonLearn-
rems. InProceedingsoftheeleventhannualconfer-
ingRepresentations.
enceonComputationallearningtheory,pages230‚Äì
234.
SawanKumar.2022. Answer-levelcalibrationforfree-
form multiple choice question answering. In Pro-
SabrinaJMielke,ArthurSzlam,EmilyDinan,andY-
ceedingsofthe60thAnnualMeetingoftheAssocia-
LanBoureau.2022. Reducingconversationalagents‚Äô
tionforComputationalLinguistics(Volume1: Long
overconfidencethroughlinguisticcalibration. Trans-
Papers),pages665‚Äì679,Dublin,Ireland.Association
actionsoftheAssociationforComputationalLinguis-
forComputationalLinguistics.
tics,10:857‚Äì872.
LynnLambertandSandraCarberry.1992. Modeling Krikamol Muandet, David Balduzzi, and Bernhard
negotiationsubdialogues. In30thAnnualMeetingof Sch√∂lkopf.2013. Domaingeneralizationviainvari-
theAssociationforComputationalLinguistics,pages antfeaturerepresentation. InInternationalconfer-
193‚Äì200,Newark,Delaware,USA.Associationfor enceonmachinelearning,pages10‚Äì18.PMLR.
ComputationalLinguistics.
ElnazNouriandDavidTraum.2014. Initiativetakingin
AlexLascaridesandNicholasAsher.2008. Agreement negotiation. InProceedingsofthe15thAnnualMeet-
anddisputesindialogue. InProceedingsofthe9th ingoftheSpecialInterestGrouponDiscourseand
SIGdialWorkshoponDiscourseandDialogue,pages Dialogue(SIGDIAL),pages186‚Äì193,Philadelphia,
29‚Äì36,Columbus,Ohio.AssociationforComputa- PA,U.S.A.AssociationforComputationalLinguis-
tionalLinguistics. tics.OpenAI.2023. Gpt-4technicalreport. CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida, Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
CarrollWainwright,PamelaMishkin,ChongZhang, IsabelKloumann,ArtemKorenev,PunitSinghKoura,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
2022. Training languagemodelsto followinstruc- anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tions with human feedback. Advances in Neural tinet,TodorMihaylov,PushkarMishra,IgorMoly-
InformationProcessingSystems,35:27730‚Äì27744. bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
David Sculley, Sebastian Nowozin, Joshua Dillon,
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
BalajiLakshminarayanan,andJasperSnoek.2019.
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Canyoutrustyourmodel‚Äôsuncertainty? evaluating
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
predictiveuncertaintyunderdatasetshift. Advances
Melanie Kambadur, Sharan Narang, Aurelien Ro-
inneuralinformationprocessingsystems,32.
driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
JohnPlattetal.1999. Probabilisticoutputsforsupport
tunedchatmodels.
vectormachinesandcomparisonstoregularizedlike-
lihoodmethods. Advancesinlargemarginclassifiers,
DavidTraum,WilliamSwartout,JonathanGratch,and
10(3):61‚Äì74.
Stacy Marsella. 2008. A virtual human dialogue
David Reitter and Johanna D. Moore. 2007. Predict- model for non-team interaction. Recent trends in
ingsuccessindialogue. InProceedingsofthe45th discourseanddialogue,pages45‚Äì67.
AnnualMeetingoftheAssociationofComputational
Linguistics,pages808‚Äì815,Prague,CzechRepublic. Lewis Tunstall, Edward Beeching, Nathan Lambert,
AssociationforComputationalLinguistics. Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Cl√©mentine
Charlotte Schluger, Jonathan P Chang, Cristian Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
Danescu-Niculescu-Mizil, and Karen Levy. 2022. rect distillation of lm alignment. arXiv preprint
Proactivemoderationofonlinediscussions: Existing arXiv:2310.16944.
practices and the potential for algorithmic support.
ProceedingsoftheACMonHuman-ComputerInter- MarilynWalker,IreneLangkilde,JerryWright,AllenL
action,6(CSCW2):1‚Äì27. Gorin,andDianeLitman.2000. Learningtopredict
problematicsituationsinaspokendialoguesystem:
JohnSchulman,SergeyLevine,PieterAbbeel,Michael experimentswithhowmayihelpyou? In1stMeet-
Jordan,andPhilippMoritz.2015. Trustregionpol- ingoftheNorthAmericanChapteroftheAssociation
icyoptimization. InInternationalconferenceonma- forComputationalLinguistics.
chinelearning,pages1889‚Äì1897.PMLR.
XueweiWang,WeiyanShi,RichardKim,YoojungOh,
John Schulman, Filip Wolski, Prafulla Dhariwal,
SijiaYang,JingwenZhang,andZhouYu.2019. Per-
Alec Radford, and Oleg Klimov. 2017. Proxi-
suasionforgood: Towardsapersonalizedpersuasive
malpolicyoptimizationalgorithms. arXivpreprint dialoguesystemforsocialgood. InProceedingsof
arXiv:1707.06347.
the57thAnnualMeetingoftheAssociationforCom-
putationalLinguistics,pages5635‚Äì5649,Florence,
MarinaSokolova,ViviNastase,andStanSzpakowicz.
Italy.AssociationforComputationalLinguistics.
2008. The telling tail: Signals of success in elec-
tronicnegotiationtexts. InProceedingsoftheThird
ZhongqingWang,XiujunZhu,YueZhang,ShoushanLi,
InternationalJointConferenceonNaturalLanguage
andGuodongZhou.2020. Sentimentforecastingin
Processing: Volume-I.
dialog. InProceedingsofthe28thInternationalCon-
ferenceonComputationalLinguistics,pages2448‚Äì
RichardSSuttonandAndrewGBarto.2018. Reinforce-
2458,Barcelona,Spain(Online).InternationalCom-
mentlearning: Anintroduction. MITpress.
mitteeonComputationalLinguistics.
Katherine Tian, Eric Mitchell, Allan Zhou, Archit
Sharma,RafaelRafailov,HuaxiuYao,ChelseaFinn, Ronald J Williams. 1992. Simple statistical gradient-
andChristopherDManning.2023. Justaskforcali- followingalgorithmsforconnectionistreinforcement
bration: Strategiesforelicitingcalibratedconfidence learning. Machinelearning,8:229‚Äì256.
scoresfromlanguagemodelsfine-tunedwithhuman
feedback. arXivpreprintarXiv:2305.14975. RunzheYang,JingxiaoChen,andKarthikNarasimhan.
2021. Improvingdialogsystemsfornegotiationwith
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- personalitymodeling. InProceedingsofthe59thAn-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay nualMeetingoftheAssociationforComputational
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti Linguisticsandthe11thInternationalJointConfer-
Bhosale,DanBikel,LukasBlecher,CristianCanton ence on Natural Language Processing (Volume 1:
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, LongPapers),pages681‚Äì693,Online.Association
JudeFernandes,JeremyFu,WenyinFu,BrianFuller, forComputationalLinguistics.Justine Zhang, Jonathan Chang, Cristian Danescu-
Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario
Taraborelli,andNithumThain.2018. Conversations
goneawry: Detectingearlysignsofconversational
failure. InProceedingsofthe56thAnnualMeetingof
theAssociationforComputationalLinguistics(Vol-
ume1: LongPapers),pages1350‚Äì1361,Melbourne,
Australia.AssociationforComputationalLinguistics.
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
WeiLu.2023. Tinyllama.
ShujianZhang,ChengyueGong,andEunsolChoi.2021.
Knowingmoreaboutquestionscanhelp: Improving
calibration in question answering. In Findings of
theAssociationforComputationalLinguistics: ACL-
IJCNLP2021,pages1958‚Äì1970,Online.Association
forComputationalLinguistics.
YihengZhou,HeHe,AlanWBlack,andYuliaTsvetkov.
2019. Adynamicstrategycoachforeffectivenego-
tiation. InProceedingsofthe20thAnnualSIGdial
MeetingonDiscourseandDialogue,pages367‚Äì378,
Stockholm,Sweden.AssociationforComputational
Linguistics.A TheoryandDerivations
A.1 ComparisonofImplicitForecastingApproachesinInferenceandUncertaintyTuning
When doing implicit forecasting with language models, we find two main approaches in the existing
literature (i.e., on calibration to correctness). As in the main text, one can retain the whole token
distributionduringinference. Recall,thisiswritten:
eZyes/œÑ
PÀÜ = . (13)
IF (cid:80) eZt/œÑ
t‚ààT
Ontheotherhand, onecanconsideranormalized probability, restrictedtoasetofcandidateanswers.
Forexample,Jiangetal.(2021)suggestthisapproachforsmallermodels,lackingappropriateinstruction
followingcapability. Inourcontext,theapproachisdescribed:
eZyes/œÑ
PÀÜ = . (14)
IFN
eZyes/œÑ +eZno/œÑ
Primarily,Jiangetal.(2021)argueforthisstrategytocopewiththespreadingofprobabilityacrossthe
manydifferentwaystoindicate‚Äúyes‚Äùor‚Äúno‚Äù,dependentonthequestion. Inmoderninstruction-following
modelstunedwithhumanfeedback,Kadavathetal.(2022)considerbothapproaches. Intheirnotation,
‚ÄúP(True)‚Äùtakestheformerapproach(usingthewholedistribution),while‚ÄúP(IK)‚Äùtunesaclassification
headontopofthemodel‚Äôsinternalfeaturerepresentation, making‚ÄúP(IK)‚Äùequivalenttotuninginthe
fashion of Jiang et al. (2021). As we are aware, there has not been much exploration of tuning and
inferenceinthefashionofEq.(13). Weprovideafirst,formalcomparisonoftheseapproachesnext.
QualitativePros&Cons WeviewtheapproachofEq.(13)tobepreferableforafewreasons.
1. CompatibilitywithOtherTasks: Mostotherlanguagemodelingtasksrequireretentionofthefull
tokendistribution. Whilewedonotexperimentwiththisinourpaper,uncertaintytuningofimplicit
forecasts,asinEq.(13)and¬ß2.3,cantheoreticallybecoupledwithothertasksinamorebroadly
scopedfine-tuningpipeline. Incontrast,useofnormalizedprobabilitiesduringuncertaintytuning,as
inEq.(14),wouldrequiredistinctlossfunctionsandinferenceprotocolstobecoupledwithother
tasks(duetodifferencesinscorenormalization).
2. GeneralizedExtension: Extensionbeyondyes/noanswers(anddialogueforecasting)isfareasier
when using Eq. (13). Indeed, we simply change the token in the numerator and add more data
instancesduringtraining. InthealternativeEq.(14),wemayrequireadditionalalgorithms/compute
toselectcandidatesets‚ÄìasisdonebyJiangetal.(2021). Forone,theseaddedprotocolscaninhibit
inferenceduetocompoundingerrors,andrelatedtoourfirstpoint,theseaddedprotocolsalsomake
uncertaintytuningonmanydifferenttypesoftasksanddataprohibitive.
Theoretical and Empirical Characterization of Differences We can also consider the difference
betweentheseapproachesmoreprecisely. Webeginwithatheoreticalresultthatisbothconceptually
informativeandusefulforempiricalstudy. Primarily,weboundtheabsolutedifferencebetweenthetwo
typesofimplicitforecasts. DefineX = T ‚àí{yes,no},then:
(cid:12) (cid:12)
(cid:12) 1 (cid:12)
|PÀÜ ‚àíPÀÜ | = PÀÜ ¬∑|1‚àíPÀÜ /PÀÜ | = PÀÜ ¬∑(cid:12)1‚àí (cid:12)
IF IFN IFN IF IFN IFN (cid:12) (cid:12) PÀÜ +(1‚àíPÀÜ )+ (cid:80) x‚ààXeZx/œÑ (cid:12) (cid:12)
IFN IFN eZyes/œÑ+eZno/œÑ
(cid:12) (cid:12) (cid:32) (cid:80) eZx/œÑ (cid:33)‚àí1(cid:12) (cid:12)
= PÀÜ ¬∑(cid:12)1‚àí 1+ x‚ààX (cid:12)
IFN (cid:12)
(cid:12)
eZyes/œÑ +eZno/œÑ (cid:12)
(cid:12)
(15)
(cid:12) (cid:12) (cid:80) eZx/œÑ (cid:32) (cid:80) eZx/œÑ (cid:33)‚àí1(cid:12) (cid:12) (cid:80) eZx/œÑ
= PÀÜ ¬∑(cid:12) x‚ààX ¬∑ 1+ x‚ààX (cid:12) ‚â§ x‚ààX
IFN (cid:12) (cid:12)eZyes/œÑ +eZno/œÑ eZyes/œÑ +eZno/œÑ (cid:12)
(cid:12)
eZyes/œÑ +eZno/œÑ
(cid:80) eZx/œÑ (cid:88)(cid:32) eZx (cid:33)1/œÑ
(cid:88)
‚â§ x‚ààX = = Œµ1/œÑ
eZno/œÑ eZno x
x xwherewedefineŒµ
x
= eZx/eZno astheexcessscoreratioforthetokenx. Weestimatethisvalueempirically
fortemperatureœÑ = 1acrossalldifferentdatasets,splits,andsetupsforourfine-tunedmodels,findinga
smallaverageof1.1(ona100ptscale).
Interpretation So,forœÑ = 1,PÀÜ andPÀÜ arepracticallyequivalentforecasts. Themaindifference
IF IFN
is the qualitative benefits of PÀÜ we just discussed. Because xp decreases as a function of p > 1 (and
IF
x < 1), we also know this number will not get larger for smaller œÑ ‚Äì so, our interpretation for œÑ < 1
remains the same. On the other hand, our observed difference does grow with œÑ, which can mar our
interpretationifoneusespost-hoccorrectiontechniqueslikeœÑ scaling. Inourexperimentalstudy,thisis
largelyunimportant,sinceourexperimentsonfine-tunedmodelsactuallyobservetheimplicitsignalat
œÑ = 1beforedoingourproposed(simulated)correctiontechniquein¬ß2.5.
Allthisistosay,ifoneconductscorrectionasin¬ß2.5,thetunedforecastingapproachesareequivalent
withabout1%differenceonaverage. Consequently,itmakessensetouseEq.(13),recoupingthepotential
qualitativebenefitsdiscussedearlier. Asforuseofothercorrectiontechniques,wediscussthesenext.
A.2 ComparisonofPost-HocCorrectionTechniques
Inthissection,wediscussotherchoicesofpost-hoccorrection,comparingthemtoourproposalin¬ß2.5.
Aswearethefirsttostudylanguagemodelingofuncertaintyinaconversationalforecastingdomain,we
focusonstudiescalibratinguncertaintytomodelcorrectness. Namely,weconsiderapproachesby
1. Kadavath et al. (2022), who infer an implicit signal and conduct temperature scaling using the
languagemodel‚Äôsentirepredictedtokendistribution;7
2. Jiangetal.(2021),whoinferimplicitsignalsandscaletemperatureononlyasetofcandidatetokens;
3. andTianetal.(2023),whostudydirectsignalsofmodelcorrectnessandbrieflysuggesttheirown
temperaturescalingapproachforthistheirexperiments.
Ourapproachismixed. We(a)inferimplicitsignalsusingthefulltokendistributionlikeKadavathetal.
(2022),andthen(b)conductan‚Äúapproximate‚ÄùPlattscaling(Plattetal.,1999)ononlyasetofcandidate
tokens. Recall,the‚Äúapproximation‚Äù(orsimulation)instep(b)iswhatallowsthemethodtobeapplicable
tobothimplicitforecastsanddirectforecasts.
Asastandaloneproperty,thisunificationisnice. Itreducesthecomputationaloverheadtostudymany
differentmethods,sincewecanusethesameinferenceandpost-processingcodeforallforecastsinour
experiments. In addition, we point out our approach is computationally efficient. Indeed, using the
approachwepropose,correctionisdoneusingtheprobabilityoftheyestokenonly,soweneedonly
conductoneforwardpassandsavethissinglefloatperinstance. Incontrast,temperaturescalingusing
theentiretokendistribution‚Äìforimplicitforecasting,asdonebyKadavathetal.(2022)‚Äìrequiresan
increaseinforwardpassesatleastproportionaltothenumberoftemperatureswetry,orotherwise,about
32000√ómorememorytoavoidre-computingforwardpassesbyrememberingthetokenscores.
Next,weconductsometheoreticalandempiricalanalysescomparingourcorrectiontechniquetothat
ofJiangetal.(2021),showingitispracticallyequivalent. Lateron,wealsocompareourpost-processing
with Kadavath et al. (2022) on implicit forecasts and Tian et al. (2023) on direct forecasts, using an
empiricalstudy. Again,wefindourstobepracticallyequivalent(orbetteronaverage).
Comparison of Proposed Correction to that of Jiang et al. (2021) Recall our ‚Äúestimated logit‚Äù
post-processingtechniquedescribedinEq.(12). Wehaveprocessedscore:
ZÀÜ = ln(cid:0) PÀÜ/(1‚àíPÀÜ)(cid:1) /œÑ ‚àíŒ≤ (16)
yes
andthepost-processedforecastPÀÜ = PÀÜ ,expandedasbelow:
PP new
(cid:32) (cid:33)‚àí1
(cid:16)1‚àíPÀÜ(cid:17)1/œÑ PÀÜ1/œÑ
PÀÜ = (1+exp(‚àíZÀÜ ))‚àí1 = 1+eŒ≤ =
PP yes PÀÜ PÀÜ1/œÑ +eŒ≤(1‚àíPÀÜ)1/œÑ
(17)
eZyes/œÑ
=
eZyes/œÑ +eŒ≤(eZno +(cid:80) eZx)1/œÑ
x
7Recall,thisistheirmethodologyfor‚ÄúP(True)‚Äù.Theirmethodologyfor‚ÄúP(IK)‚ÄùismostsimilartoJiangetal.(2021).œÑ 0.25 0.5 1 1.5 1.75 2 2.5
Prob. DifferenceBound 4.3 2.1 1.1 0.7 0.6 0.5 0.4
Table4: EstimationofboundinEq.(18)fromdatawithŒ≤ =0. Note,wereporttheboundona100ptscale.
Post-ProcessingMethod %Preferred(overours) Magnitude(Brierscoregain;100pt)
Kadavathetal.(2022) 4.2 0.03
Tianetal.(2023) 16.7 0.03
Tianetal.(2023)+BiasCorrection 20.1 0.03
Table5: Comparisonofpost-processingmethodstoourapproach. ForthemethodofTianetal.(2023),wealso
consideraddinga(new)secondparameter‚ÄìabiascorrectiontermsimilartoŒ≤ ‚Äìtomakeitmorecompetitive. We
reportpercentoftimeseachmethodispreferred(comparedtoourmethod)basedonvalidationdata,aswellas
averagemagnitudeofpreference. Inpractice,whenvalidationdataisusedtoselectamongmethods,ourtechnique
isgenerallypreferredoverexistingtechniques. Evenwhenothersarepreferred,thedegreeofpreferenceissmall.
Meanwhile,whenpreferred,ourpost-processingmethodhaspreferencemagnitude6√ólargeronaverage(0.19).
(cid:80)
DefiningŒµ = Œµ wehave
x x
(cid:12) eZyes/œÑ eZyes/œÑ (cid:12)
|PÀÜ ‚àíPÀÜ | = (cid:12) ‚àí (cid:12)
PP IFN (cid:12)eZyes/œÑ +eŒ≤(1+Œµ)1/œÑeZno/œÑ eZyes/œÑ +eZno/œÑ(cid:12)
(cid:12) eZno/œÑ ‚àíeŒ≤(1+Œµ)1/œÑeZno/œÑ (cid:12)
= eZyes/œÑ ¬∑(cid:12) (cid:12) (18)
(cid:12)e2Zyes/œÑ +eZyes/T+Zno/œÑ +eŒ≤(1+Œµ)1/œÑeZyes/T+Zno/œÑ +eŒ≤(1+Œµ)1/œÑe2Zno/œÑ(cid:12)
(cid:12) 1‚àíeŒ≤(1+Œµ)1/œÑ (cid:12)
= (cid:12) (cid:12) ‚â§ |1‚àíeŒ≤(1+Œµ)1/œÑ|
(cid:12)eZyes/T‚àíZno/œÑ +1+eŒ≤(1+Œµ)1/œÑ +eŒ≤(1+Œµ)1/œÑeZno/T‚àíZyes/œÑ(cid:12)
SinceŒ≤ isafreeparameter,itcanalwaysbechosentobe0duringpost-processing(e.g.,ifwedetermineit
isnothelpfulbasedonvalidationdata). Thus,sinceanydeviationduetoŒ≤ wouldbebychoicetoimprove
ourforecasts,weconsiderestimationofthisboundwhenŒ≤ = 0. Indeed,wecaneasilycollectstatistics
on (1+Œµ) during the forward passes of our experiments, and do so, estimating this bound value with
differentselectionsofœÑ inTable4. Wefindalldifferencestobepracticallynegligible.
ComparisonofProposedCorrectiontothatofKadavathetal.(2022)andTianetal.(2023) Lacking
theoretical analysis, we compare our unified post-processing approach to the discussed techniques of
Kadavathetal.(2022)andTianetal.(2023),empirically. Tokeepallmethodsonequalfooting,weuse
ourfine-tunedLlama-27bmodelforimplicitforecasts;i.e.,sincethemethodofKadavathetal.(2022)
only operates on implicit forecasts. We also limit study to one prompt setup (the data inferred prior)
becausethemethodofKadavathetal.(2022)ismorecomputationallyexpensive. Toevaluate,inTable5,
welookatperformanceatminimizingBrierscoreonthevalidationset,reportingthepercentoftimes
wewouldhavepreferredthemethodofTianetal.(2023)orKadavathetal.(2022)duetoalowerBrier
score. Wealsoreporttheaveragemagnitudeofpreference;i.e.,howmuchsmallertheBrierscoreis. The
logichereistoshowtheutilityofeachpost-processingmethodinapracticalsetting, sincewewould
neveractuallyselectamethodthatdoesworseonvalidationdatainpractice. Indeed,theresultsshow
thatifwedid usetheseotherproposals,inconjunctionwithourown,andpickedthebesttechniquefor
eachinstancebasedonvalidationdata,wewouldhavestillhaveusedourownproposalmostofthetime.8
Moreover, the actual magnitude of preference for other methods is very small, so even when another
methodispreferred,wewouldultimatelyexpectsimilarperformancewhentransferringtoatestset.
ComputationalDetails Toselecthyper-parameterœÑ andŒ≤,weconsidertwocases: Œ≤ Ã∏= 0andŒ≤ = 0.
Forthefirst(Œ≤ Ã∏= 0),wefita2parameterlogisticmodeloftheoutcomevariable(i.e.,thisoptimizesa
properscoringfunction‚Äìthelogscore). Forthelattercase(Œ≤ = 0),weuseeithertraditionaltemperature
scaling(optimizingBrierscore)oranewerscalingapproachcalledExpectationConsistency,asdescribed
8Weusedonlyourapproachforexperimentstoavoidextracomputationaloverhead,andbecauseitwasbestoverall.by Clart√© et al. (2023). To decide which of these cases (and subsequent optimization procedures) to
use,wecompareBrierscores,pickingthemethodthatyieldstheoveralllowestonvalidationdata. To
re-implementtheapproachofTianetal.(2023),weoptimizethefunctionalformexp(log(P)/œÑ),ormore
generallyexp(log(P)/œÑ)/Œ≤ whenapplyingbiascorrection,pickingparameterstominimizeBrierscore;
thispreservestheauthors‚ÄôprimarysuggestionthatscalingbedonesotheresultisproportionaltopŒ± for
some Œ±.9 In general, we use the scipy optimization package or scikit-learn to implement the
aforementionedparameterselection. Whenre-implementingthecorrectionapproachofKadavathetal.
(2022),itistoocomputationallycostlytousethescipyoptimizationpackage,soweconductasimple
linearsearchforœÑ ‚àà {0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2}.
9Weinferrednootherrestrictionsorimplementationdetailsfromtheirdescription.A.3 AnOff-PolicyPolicyGradientTheorem
Inthissection,weshowtheclaimedresultfromthemaintext:
‚àá E[R] =
E(cid:104)
s ¬∑
¬µT(TÀú)
¬∑‚àá log¬µ
(TÀú)(cid:105)
Œ∏ TÀú ¬µ TÀú(TÀú) Œ∏ T
where T ‚àº LM‚àó‚ó¶Œ¶‚ó¶D,
Œ∏ (19)
TÀú ‚àº Ref‚àó‚ó¶Œ¶‚ó¶D,
ands = ‚àís(p‚ó¶TÀú,O).
TÀú
Letallrandomvariablesbeasaboveandfixthemassfunctions¬µ and¬µ . Then,wehave
T TÀú
(cid:34) (cid:35) (cid:34) (cid:35)
(cid:88) (cid:88)
‚àí‚àá E[R] = ‚àá E s(p‚ó¶t,O)¬∑¬µ (t) = E s(p‚ó¶t,O)¬∑‚àá ¬µ (t)
Œ∏ Œ∏ T Œ∏ T
t‚ààT‚àó t‚ààT‚àó
(cid:34) (cid:35)
(cid:88)
= E s(p‚ó¶t,O)¬∑¬µ (t)¬∑‚àá ln¬µ (t)
T Œ∏ T
t‚ààT‚àó
(cid:34) (cid:35)
= E
(cid:88)
s(p‚ó¶t,O)¬∑¬µ (t)¬∑
¬µ TÀú(t)
¬∑‚àá ln¬µ (t) (20)
T ¬µ TÀú(t) Œ∏ T
t‚ààT‚àó
(cid:34) (cid:35)
= E
(cid:88)
¬µ
(t)(cid:16)
s(p‚ó¶t,O)¬∑
¬µT(t)
¬∑‚àá ln¬µ
(t)(cid:17)
TÀú ¬µ TÀú(t) Œ∏ T
t‚ààT‚àó
(cid:34) (cid:35)
= E s(p‚ó¶TÀú,O)¬∑ ¬µT(TÀú) ¬∑‚àá ln¬µ (TÀú) .
¬µ TÀú(TÀú) Œ∏ T
So,wehaveourdesiredresult.
A.4 Off-PolicyImplementationDetails
Next,we‚ÄôlldiscusssomechoicesforthereferencepolicyRef. Theformalframeworkwe‚Äôveprovided
actuallyallowsustorecovervariantsofsomeexistingtechniquesforgettinglanguagemodelstooutput
forecastsintokenspace.
The Quantizer Lin et al. (2022) fine-tune an LM to forecast the correctness of its answers in token
spaceforafactualquestion-answeringtask. Primarily,theyusetheaccuracyofdifferentquestiontypes
toassignconfidencelevelsthattheLMshouldpredictforeachtype. Weproposetoextendthisideato
moregeneralsettingsviaclustering. Insteadofassumingpre-assignedpartitions,weinferthepartitions
byclusteringthedata. Theaverageoutcomeofaclusteriscomputedandassignedtoeachdatuminthe
cluster,whichdefinesadeterministicreferencepolicyRef‚àó tobeusedinEq.(10):
(cid:104) (cid:88) (cid:105)
Ref‚àó(X) = p‚Ä† |C(X)|‚àí1 O (21)
C N
N‚ààC(X)
whereC(X)istheneighborhoodofX, O istheoutcomeofneighborN, andp‚Ä† : [0,1] ‚Üí T‚àó isan
N
inverse for p,10 mapping probabilities to tokens. In experiments, C is defined by k-means clustering
overtheinternalfeaturerepresentationsofLM . Theserepresentationsaretheaverage(overtime)ofthe
Œ∏
lasthiddenlayerofthemodel,ignoringmaskedinputs,andtheyareupdatedeachepochwhenclusters
are re-assigned. In practice, we pick k by hyper-parameter tuning and run k-means individually for
each dataset, re-aggregating the cluster assignments afterwards; our motivation for this is to prevent
uninformative,imbalancedclusterassignments,whichmayoccurifclusterscorrelatewithdatasetlabels.
10pisnotbijectiveingeneral,butwecanconsiderasubsetofT‚àó‚Äìe.g.,stringslike‚Äú72%‚Äù‚Äìforwhichp‚Ä†doesexist.The Exploiter Given any pre-trained, fixed implicit signal forecaster, we can use it to train a direct
forecasterviaEq.(10). Forexample,assumingwehavetrainedaLMviasupervisedfine-tuning(¬ß2.3)
andfixeditsforecastingfunctionPÀÜ ,wedefinethedeterministicreferencepolicy:
IF
Ref‚àó(X) = p‚Ä†(cid:2) PÀÜ (cid:3) . (22)
S IF
This providesa nice controlledviewfor the differences between implicitand directforecasting, since
thedirectpolicyisactuallylearningfromtheimplicitpolicy. Propertiesoftheimplicitpolicythatdonot
transfertothedirectpolicywillbeofinterest. Asnotedinthemaintext,thisalsorepresentsafocuson
exploitation,sincetheimplicitforecastsPÀÜ weredesignedtomaximizethelog-likelihoodonthetraining
IF
data. Maximizingthelog-likelihoodisequivalenttominimizingthenegativelog-likelihood(i.e.,thelog
score)anditisknownthatthelogscoreisaproperscoringfunction.
TheExplorer Finally,itisinterestingtoconsiderthatEq.(10)indicatesthelanguagemodelLM‚àó can
Œ∏
learnfromanypolicy,evenabadone. Toexplorethis,wesuggestacontext-lessbinomialreferencepolicy
whichsimplyassignsrandomprobabilityestimatestothedialogues. Presumably,byEq.(10),LM‚àó can
Œ∏
observetherewardsfromtheseestimatesandbegintomake‚Äúsense‚Äùofthem. Forbinomialparametersn
(numberoftrials)andœÄ (successratio)wedefinethereferencepolicy:
Ref‚àó(X) = p‚Ä†(cid:2) B(cid:3) ; B ‚àº Bin(n,p). (23)
B
Inexperiments,n = 20andpistheaverageoutcomeinthetrainingdata(oneforeachdataset).
OnPolicy Alternatively,wecanactuallyuseLM‚àó asitsownreference: Ref‚àó = LM‚àó. Asalludedby
Œ∏ PPO Œ∏
ournotation,thismakesEq.(10)equivalenttoon-policypolicygradienttechniques,likeProximalPolicy
Optimization(Schulmanetal.,2017). Weleaveinvestigationofonpolicylearningtofuturework.#Train/TestMatches #TestSetsMatchingMajorityofTrain
Split Topic Topic+L Outcome Length Affective Non-Affective Multi-party
easy all all 1/2 all all all all
medium 2/3 none none all 2/3 none all
hard none none none none none 2/3 1/3
Table 6: Table cells show count of test sets that share properties with the train set. easy has the most shared
properties(leastimbalance),whilehardhastheleastsharedproperties(mostimbalance). Weoperateunderthe
assumptionthatgreaterdegreesofimbalancecorrelatewithgreaterdegreesofdifficultyingeneralization,which
is consistent with the domain generalization literature (Gulrajani and Lopez-Paz, 2020). Generally, by design,
imbalanceofsharedproperties(betweentrainandtest)increasesfromeasytohard,creatingaslidingscaleof
difficultyforthegeneralizationproblemswestudy. TheheldoutdatasetsforeachsplitarelistedinTable7. The
firstthreecolumns(Topic,simultaneousTopic+Length,andOutcome)displayhowmanyofthetestdatasets
inthesplitsharethecolumnpropertywithatleastoneofthetraindatasets. Forexample,ineasy1outof2test
datasetsshareanOutcomewithatraindatasetinthesamesplit. Theremainingcolumns(Lengthandsoon)show
howmanyofthetestdatasetsineachsplitsharethecolumnpropertywithamajorityofthetrainingdatasetsin
thesamesplit. Forexample,inhard1outof3testdatasetshasanAffectiveoutcomeandthemajorityofthetrain
datasetshaveaNon-Affectiveoutcome(causingthe‚Äúnone‚Äùdesignationforthiscolumn). Whencomparinglength
andmulti-partysimilarities,weassumeitiseasiertogeneralizefromlongtoshortdataormulti-tosingle-party
data. So,‚Äúsharing‚Äùmeansbeingatleastaslongorhavingatleastasmanyparties.
B AdditionalExperimentalDetails
Additional experimental details are provided next. In general, anything we have missed here will be
availableinthecode,whichwillbemadepublic.
B.1 AdditionalDatasetDetails
Whenavailable,weusedefaulttrain/val/testsplitsfromeachdataset‚Äôsoriginalproposalpaper. Whennot
available,wespliteachdatatsetaccordingtoa70/15/15%split. Allnumbersreportedinthemaintextare
computedontheunseentestsetforeachindividualdataset(i.e.,evenfortheindomainsetting).
SamplingofTrainingData Foreveryepochoftraining,wesample750dialoguesfromeachofthe
heldintrainingdatasetsofthecurrenttrainingsplit(i.e.,easy,medium,orhard). Wepick750because
thisensuresabalancedsampleacrossthetrainingdata(alldatasetshaveatleast750trainingdialogues).
EachdialogueisthenrandomlytruncatedtoK ‚àº U{2...L}turnswhereListheoriginaldialogue‚Äôsturn
length. Aswegenerallytrainfor5epochs,thismeansthemodelseesroughly19-23Kpartialdialogues,
withsomedependencyinexamplesacrossepochs(forthesmallerdatasets). Wealsotriedusinglarger,
imbalanceddatasamplesfortraining. Inthiscase,wesample5Kdialogues,orasmanyasareavailable.
Accountingforthedatasetsthathavelessthan5Ktrainingexamples,weestimatethemodelseesabout
4√ómoredataoverall.
Sampling of Test and Validation Data For each dataset, we use the same validation and test sets
acrossallexperiments. Wesample250dialoguesfromthevalidationsplitofeachdataset,andrandomly
truncate each (in exactly the same way). We sample 550 dialogues from the test split of each dataset,
again,randomlytruncatingeachinthesamewayforallexperiments. Somedatasetshavelessthan550
dialoguestotalintheirtestsplit,inwhichcaseweusealloftheavailabletestdialogues.
B.2 Hyper-ParametersandPrompts
Hyper-Parameters and other Training Details Generally, we train for 5 epochs with a batch size
of 12 using AdamW for optimization (Loshchilov and Hutter, 2017). We use 4bit QLoRA (Dettmers
etal.,2023)withLoRArank32. On4NVIDIARTXA6000GPUs,singlemodeltrainingisanovernight
process, so we only conduct full hyper-parameter selection (linear search) on the medium split using
Llama-2-chat7Btosavetime. Weusethebesthyper-parametersforLlama-27Bonmediumforallother
train/testsplitsandmodels. Forimplicitforecasttuning,wepickthelearningratefromtherange{1e-4,MatchingTrainSet #MatchingTrainSets
Split/HeldoutSet Topic Topic+L Outcome Length Affective Multi-party
easy/wiki. (attack) deleted deleted reddit 4/6long 3/6yes 3/6yes
easy/itemallocation camp camp none - 3/6no -
med /craigslist itemalloc. none none - 2/5no -
med /wiki. (deleted) none none none 4/5long 2/5no 3/5yes
med /campprovisions itemalloc. none none - 3/5yes -
hard /courtroom none none none 2/5long 3/5no 2/5yes
hard /charity none none none 2/5long 2/5yes -
hard /reddit none none none 2/5long 2/5yes 2/5yes
Table 7: This table ‚Äúshows our work‚Äù for the assertions of imbalance in Table 6. It provides a description of
similaritiesanddissimilaritiesbetweentrainandtestsetswheneachdatasetisoneofthoseheldoutinthesplit.
Foreachheldouttestdataset,thefirstthreecolumnsshowsimilarityintopic,simultaneoustopic+length,noting
thespecifictraindatasetthatsharesthisproperty. Thenextthreecolumnsshowthenumber ofsimilardatasets
amongthetrainingdata,consideringlength,usefulnessofaffectivereasoning,andpresenceofmulti-partydialogues.
Length‚Äúlong‚Äùiscategorizedbyhavingmorethan2Kcharactersonaverage. Recall,whencomparinglengthand
multi-partysimilarities,weassumeitiseasiertogeneralizefromlongtoshortdataormulti-tosingle-party. So,we
putdashesinfor‚Äúshort‚Äùorsingle-partydatatonoteimbalancesneednotbemeasured.
2e-5,1e-5}. Fordirectforecasttuning,wepicktheclippingconstantœµfromtherange{0.2,0.5,0.8}and
thelearningratefromthesmallerrange{1e-4,1e-5}tosavetime. ClusteringfortheQuantizeroff-policy
isalsoselectedfromtherange{10,20}. Logscoreonthein-domainvalidationdataisusedtopickthe
bestparameters.11 Parameterselectionisfairlyconsistentoverall,withmosttuningsetupspreferringthe
highestlearningrate. œµwasalways0.5andthenumberclustersfortheQuantizeroff-policywas10.
InferenceParameters Asnotedinourdiscussion,implicitforecastingusesœÑ = 1inEq.(13)conducting
post-hoccorrectionusingour‚Äúestimatedlogit‚Äùprocedure, asinEq.(12), afterthefact. Forsampling,
to conduct direct forecasting, we typically use the default hyper-parameters indicated by the model
parameters(e.g.,intheAPI,Huggingfacegenerationconfiguration,orGithubrepository). ForGPT-4
thismeanstemperatureandtopparebothsetto1. ForLlama-2models,thismeanstemperatureandtop
pareset0.6and0.9,respectively. Theonlyexceptionsarethepre-trainedLlama-27b(nofine-tuning)
andtheLlama-27bwithdirectforecaststunedon4√ómoredata,whichbothusegreedysamplingdueto
clear(unsupervised)problemswiththeforecasts;e.g.,outputscoresthatprohibitedusualsamplingdue
toNaNvaluesorextremevarianceintokensamples(200xmorethangreedy). Formodelstunedinthe
Zephyrstyle,temperatureis0.7andtoppis1. Fordirectforecastingwithpre-trainedmodels,wesample
upto256tokenstoallowroomforanexplanation(frequentlyprovidedbysomemodels). Tunedmodels
generallyfollowedthetrainingformat,soweonlyneededtosampleupto8tokensinthesecases. Upon
sampling,ourparserextractedaprobabilityforecastbysearchingforpercentages,ratios,andfloatsin
[0,1](inthatorder)usingregexmatching. Beforeparsing,somelogicisalsousedtodropanynon-answer
explanationsthemodelprovides,i.e.,thisisbasedonkeywordsspecifiedinthesystemprompts.
Prompts Asnotedinthemaintext,weuseastandardformulaforpromptingallmodels. Weprovidean
exampleoftheofpromptformatusedfortuningLlama-27Bbelow(withmodeltags):
<s>[INST] [Segment Start]
<partial dialogue inserted>
[Segment End]
In the preceding conversation segment, a group of Wikipedia
contributors are deciding whether an article should be deleted.
11Indomaingeneralization,itsimportanttoavoidpickingparametersusingtheheldoutdomains,sincethiscanbiasresults
(GulrajaniandLopez-Paz,2020).On average, this type of conversation ends with the article
being deleted about 56% of the time. What is the percent
probability that this specific conversation will end with the
article being deleted? [/INST] </s>
andanotherexample:
<s>[INST] [Segment Start]
<partial dialogue inserted>
[Segment End]
In the preceding conversation segment, the speakers are
defending their opinions on an issue. On average, this type
of conversation ends with a personal attack about 50% of the
time. What is the percent probability that this specific
conversation will end with a personal attack? [/INST] </s>.
Forpre-trainedmodels,wealsouseaconsistentsystemprompttoconstrainoutputformatandprovide
moretaskdetails. Belowisthepromptusedfordirectforecasts:
You are NegotiationPredictionGPT, an expert language
model at predicting the likelihood of outcomes in human
language negotiations. You will be given the first part
of a conversation between several different speakers with
potentially different goals. Use the first part of the
conversation to put yourself in the mindset of the speakers
and estimate the likelihood of the requested conversation
outcome for these particular speakers. Use the keyword
"OUTCOME" to report your predicted probability for the outcome
of interest, requested by the user. Report the probability
as a percentage using the format "OUTCOME = percent". For
example, "OUTCOME = 70%
Wefocusonpromptsfordirectforecastsintheseexamples,butpromptsforimplicitforecastsaresimilar,
changingonlythemainquestionasked(toevokeayes/noresponse).
C Examples
Here,weprovidesomeexamplesofnegotiationsthemodelwouldseeduringtrainingandtesting.
Awiki. editingexamplewheretheoutcomeofinterestistheoccurrenceofapersonalattack:
Speaker3: Materialmovedfromanoneditfordiscussion. Ivaguelyrememberthisorasimilar
incidentfromaTVnewsprogram. ButIthoughtthekidswereolder. Notable? Verifiable?
‚ÄùIn2005,approximatelytwentysixthgradestudentsatReadingFlemingMiddleSchool(now
Reading Fleming Intermediate School) in Flemington, New Jersey contracted syphilis after
attendedinga"rainbowparty".‚Äù
Speaker3: Rainbowparty
Speaker2: Idon‚Äôtreallyseemuchpointinreportingeverycaseofsyphiliseverreported.... -
Speaker1: Indeed. Thereisnosource,itsoundsratherurban-legendlike,andarainbowparty
isasureingredientforthosekindoftales. Sureenough,Googlingforthestring‚ÄùFlemington
"rainbowparty"syphilis‚Äùgives0hits.
Speaker0: SourcefromHunterdonCentralRegionalHighSchoolinFlemington,NewJersy
http://central.hcrhs.k12.nj.us/bezsylko/discuss/msgReader$281?mode=dayIdon‚Äôtthinkateacherwouldassignthatifitwasn‚Äôttrueand,trustme,itis. Oneofmyfriend‚Äôs
sister‚Äôswasoneofthegirlswhocontractedit. So,I‚Äôdappreciateitifyoudidn‚Äôtaccuseitof
beingan"urbanlegend".
Speaker1: Thisis‚Äùabsolutely‚Äùnotareliablesource,apartfromthefactthatthisreceivedNO
mediacoverage. Pleasestopreinsertingthis. WhenMMWRreportsthis,wecantalkagain.Anexamplefromcraigslistwheretheoutcomeofinterestiswhetherthebuyerwillgetthebestdeal:
Speaker0: Iaminterestedinthisapartment! Canyoutellmemoreaboutit?
Speaker 1: This apartment is located in San Pablo and close to everything! You will have a
shortcommutetotheoffice,thehotteststores,andthenewestrestaurants! Theapartmenthas
lots of closet space, two bedrooms, large windows that really brighten up the space, and an
enclosedpatioontheback.
Speaker0: Great. I‚Äômlookingforaplaceinthatarea. Isasecuritydepositrequired?
Speaker 1: Right now we have a special . . . $99 security deposit! But you have to take
advantageoftheoffertoday!
Speaker0: Wouldyoubewillingtogodownto$800forthefirstmonth‚Äôsrent?
Speaker1: Iamsorry,buttherentis$1725. . . $800ismuchtoolow.
Speaker0: Whatabout$1,200?
Anexamplefromthecharitydiscussionswheretheoutcomeofinterestisaoccurenceofadonation:
Speaker1: Hi! HaveyouheardofanorganizationcalledSavetheChildren?
Speaker0: IthinkIhaveoncebefore,inagrocerystoreIbelieve
Speaker1: Doyoumindifigiveyoualittleinformationaboutthem?
Speaker0: Sure,goahead
Speaker1: Justsomeverbasicinfo,SavetheChildrenisaninternationalnon-governmental
organization that promotes children‚Äôs rights, provides relief and helps support children in
developingcountries.
Speaker0: Aretheyanonprofitorganization?
Speaker 1: Yes they are! They work 100% on donated funds. There is a lack of support for
children in developing countries, especially in war zones. For instance, millions of Syrian
childrenhavegrownupfacingthedailythreatofviolence. Inthefirsttwomonthsof2018alone,
1,000 children were reportedly killed or injured in intensifying violence. Your donation can
addresssuchproblems.
Speaker0: Ohwow,shockingnews. Doyouknowhowmanychildrenhavebeenhelpeddueto
thisorganization?
Speaker1: Accordingt0theiryearendreporttheywereabletoreach155millionchildren. Over
200kofthosekidswereintheUS.
Speaker0: Thatsawesome! Areyouapartofthisorganizationorjustsupportthem?
Speaker1: IamjustasupporterbutIwouldliketoaskhowmuchdoyouliketodonatetothe
charity? Yourdonationwillbedirectlydeductedfromyourtaskpayment. Youcanchooseany
amountfrom$0toallyourpaymentAnexamplefromthecourtroomdiscussionswheretheoutcomeofinterestiswhetherthepetitionerwill
haveafavorabledecision:
Speaker2: Iwasreading,YourHonor,fromtheonlyplacethatIknowofthatthefindingsof
factofthedistrictjudgearereprinted. They‚Äôreinthepetition‚Äìtheappendix‚Äìno,YourHonor,
that‚Äôstheirbrief. Thepetitionforcertiorari,page45(a)‚Äì
Speaker6: 45(a).
Speaker2: Whichincludesthedistrictjudge‚Äôsopinionandfindingsoffactandconclusionson
thisremandhearing. Thankyou.
Speaker5: Mr. Glasser.
Speaker1: Thankyou,YourHonor. TheKolod-Alderisioprobleminourcasewouldexistonlyin
relationtotheFloridabugging. Weagreewiththegovernment. There‚ÄôsnorealissueonFlorida,
but there is a very severe issue, we say, in connection with an allegedly abortive additional
bugginginGeorgia. Ihaven‚Äôtspokenofthattoday. We‚Äôvebriefeditprettycompletely,andI
would ask the Court to watch for that item since there was some animation here at the end
abouttheKolodproblemwhichIthinkitiscurrentlybeforetheCourt. Now‚Äì
Speaker3: Well,theydidn‚Äôtget‚Äìtheydidn‚Äôtmakeanytapesatallorgetanyrecordings,did
they,inthatsecondincidenttowhichyourefer?
Speaker1: They‚Äìtheagentwhoranitsaidhedidn‚Äôtgetthetape,andIthinkoneotheragent
whowasinthecarwithhimsaidtheydidn‚Äôtgetanyeffectiveaudibleresults. But,again,we
hadaveryhardpushinghearinginwhichI,forone,canwait,feelingthatIwasentitledtomake
astrongappellatepointagainstthecredibilityofthoseagentsonthatissuetoo. And,indeed,on
thatissueaboveall,theywerecrawlingalloverthatpartofGeorgia. Theywerethereaboutto
score,andtheywerenothesitatingtobug. Theywerebuggingalloverthecountry. Wethinkwe
can‚ÄôtprovethattheywerebugginginEurope. Thesefellowslivedwithbugs. It‚Äôsincredibleto
methattheydidn‚ÄôthavemorethanthatoneabortivecarbuginGeorgia. Theymusthavebugged
Desist‚Äôsroom. I‚Äômspeakingofperhaps‚Äìwell,allright,I‚Äôlldropthatpointfornowbecause
it‚Äôs been thoroughly briefed. Our whole submission is sufficiently stated in the briefs. Now,
onFuller,again,mayIsaysomethingthatis‚Äìhavebeenabrupt. WethinkthisCourtshould
withdrawitsactioninFulleronthegroundthatcertioraritherewasimprovidentlygrantedand
I‚Äôdliketosaywhy. We‚Äôvecovereditthoroughlyinourlastbrief. Fullerinvolvedatelegram,
we all know that, but back of that telegram was a subpoena. The police in Fuller were not
defiantorwillfultowardsexistinglaw. ThepoliceinFullerwenttotheAlaskaCommunications
Body,whateverit‚Äôscalled,gotvoluntaryrelinquishmentofthetelegramfromthatbodypursuant
to a federal regulation and they also got a subpoena. Now, the exact details of that whole
subpoena picture, I don‚Äôt know for sure of myself because I haven‚Äôt seen the Fuller record
butI‚Äôvebeenguidedthroughitinconsultationinclause,consultationwithoneoftheFuller
certioraricounsel. Ihavethepagenumbers. Thisiscoveredinourlastbrief. Now,iftherewas
asubpoenainFullerforthattelegram,howcanYourHonorsreachthequestioninFullerof
aviolationof605becausetheveryfirstsentenceof605providesforsubpoenasnor,atleast
colorablyandsubjecttoacloserscrutinyoftherecordinFullerthan‚ÄìwhichYourHonorsmay
wellwishtodobecauseFullerisaprettydrasticdecision,andtorenderadrasticdecisionlike
Fulleronarecordthatmaynotstandupunderscholarlycriticismoneofthesedays,Iwould
thinkwouldbesomethingthattheCourtthatwishtohearabout.
Speaker7: WhatdidtheAlaskaCourtheldinFuller‚Äì
Speaker1: TheAlaskaCourtnever‚Äì
Speaker7: Withrespecttothe605violation?
Speaker1: NevertouchthisproblemthatI‚Äômtalkingaboutnow. Oh,well,theytouchedthe605
problem.Speaker7: Whatdiditinvolvewithrespecttothe605violation?
Speaker1: Yes,theytouchedthe605,buttheydidn‚Äôttouchtheproblemofsubpoenapursuantto
605.
Speaker7: WhatdidtheAlaskaCourtholdwithrespecttothe605violationofFuller?
Speaker1: Theyheldthat‚Äìletmethink. Now,waitaminute.
Speaker7: There‚Äôsadissent,buttheCourtheldthat‚Äì
Speaker1: They‚Äìoh,theyheldthat605doesnotapplytostatesthattheyadoptedthebasic
Schwartzline.
Speaker7: Yes.