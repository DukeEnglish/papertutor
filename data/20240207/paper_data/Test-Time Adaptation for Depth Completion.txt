Test-Time Adaptation for Depth Completion
HyoungseobPark AnjaliGupta AlexWong
YaleVisionLab YaleVisionLab YaleVisionLab
hyoungseob.park@yale.edu anjali.gupta@yale.edu alex.wong@yale.edu
Abstract sparsedepthmapbyprojectingthepointsontotheimage
plane,andguidedbyasynchronizedcalibratedimage.
Itiscommontoobserveperformancedegradationwhen
Trainingadepthcompletionmodelcanbedoneinasu-
transferringmodelstrainedonsome(source)datasetstotar-
pervised(usinggroundtruth)orunsupervised(usingSfM)
gettestingdataduetoadomaingapbetweenthem. Existing
manner. Theformerdominatesinperformance,butrequires
methodsforbridgingthisgap,suchasdomainadaptation
expensiveannotationsthatareoftenunavailable;thelatter
(DA),mayrequirethesourcedataonwhichthemodelwas
usesunannotatedimages,buttheymustsatisfySfMassump-
trained(oftennotavailable),whileothers,i.e.,source-free
tions between frames, i.e., motion, covisibility, etc. Like
DA,requiremanypassesthroughthetestingdata. Wepro-
most learning-based methods, models trained under both
poseanonlinetest-timeadaptationmethodfordepthcom-
paradigmstypicallyexperiencea performance dropwhen
pletion,thetaskofinferringadensedepthmapfromasingle
testedonanewdatasetduetoacovariateshift,i.e.,domain
imageandassociatedsparsedepthmap,thatclosestheper-
gap. As we can only assume that a single pair of image
formancegapinasinglepass. Wefirstpresentastudyon
andsparsedepthmapisavailableinthetargetdomainfor
how the domain shift in each data modality affects model
thedepthcompletion,modelsbelongingtoeitherlearning
performance. Based on our observations that the sparse
paradigms cannot easily be trained or adapted to the new
depthmodalityexhibitsamuchsmallercovariateshiftthan
domainevenwhengiventhetestingdata. Wefocusontest-
theimage,wedesignanembeddingmoduletrainedinthe
timeadaptation(TTA)fordepthcompletion,whereoneis
sourcedomainthatpreservesamappingfromfeaturesen-
givenaccesstothetestdatainastream,i.e.,onebatchata
codingonlysparsedepthtothoseencodingimageandsparse
time,withoutbeingabletorevisitpreviously-seenexamples.
depth. Duringtesttime,sparsedepthfeaturesareprojected
Thegoalistolearncausallyandtoquicklyadaptasetof
usingthismapasaproxyforsourcedomainfeaturesandare
pre-existingweightstrainedonasourcedomaintoatarget
usedasguidancetotrainasetofauxiliaryparameters(i.e.,
testdomain,soonecanreducetheperformancegap.
adaptationlayer)toalignimageandsparsedepthfeatures
Webeginwithsomemotivatingobservationsontheef-
fromthetargettestdomaintothatofthesourcedomain. We
fects of the domain gap: (i) Errors in target domain tend
evaluateourmethodonindoorandoutdoorscenariosand
tobehigherwhenfeedboththeimageandsparsedepthas
showthatitimprovesoverbaselinesbyanaverageof21.1%.
inputratherthansparsedepthonly,asshowninFig.1. This
impliesthatthedepthmodalityexhibitsasmallercovariate
1.Introduction
shiftbetweenthesourceandtargetdomainsthantheimage
modality, to the extent that forgoing the image altogether
Reconstructingthe3-dimensional(3D)structureofanenvi-
often yields superior results than using either both sparse
ronmentcansupportanumberofspatialtasks,fromrobotic
depthandimageortheimagealone. (ii)Yet,whenoperating
navigationandmanipulationtoaugmentedandvirtualreality.
inthesourcedomain,weobservetheoppositeeffectâ€“forgo-
Mostsystemsaddressingthesetasksarebuiltforsensorplat-
ingtheimageisdetrimentaltoperformance. Naturally,this
formsequippedwithrange(i.e.,lidarorradar)oroptics(i.e.,
begsthequestion: Howshouldoneleveragedatamodalities
cameraorsensors). Whilerangesensorscanmeasurethe
thatarelesssensitivetothedomainshift(e.g.,sparsedepth)
3Dcoordinatesofthesurroundingspace, theyoftenyield
tosupportalignmentbetweensourceandtargetdomainsfor
point clouds that are sparse. Likewise, these coordinates
modalitiesthataremoresensitive(e.g.,RGBimage)?
canalsobeestimatedfromimagesbymeansofStructure-
from-Motion(SfM)orVisualInertialOdometry(VIO).For Toanswerthisquestion,weinvestigateatest-timeadap-
thegoalofdensemapping,depthcompletionisthetaskof tation approach that learns an embedding for guiding the
recoveringthedensedepthofa3Dsceneasobservedfrom model parameter update by exploiting the data modality
asparsepointcloud, whichisoftenpost-processed intoa (sparsedepth)thatislesssensitivetothedomainshift. The
1
4202
beF
5
]VC.sc[
1v21330.2042:viXraembeddingmodulemapsthelatentfeaturesencodingsparse subsetofsourcemodelparameters[17,43],andthediscrim-
depthtothelatentfeaturesencodingbothimageandsparse inativefeaturefromtheself-supervisedlearning(SSL)[5].
depth. The mapping is trained in the source domain and For instance, [43] proposes TENT, a simple but effective
frozenwhendeployedtothetargetdomainforadaptation. batch-normlayeradaptationwithentropyminimizationfor
During test time, sparse depth is first fed through the en- fullytest-timeadaptation. TTT[40]performsclassification-
coderandmapped,throughtheembeddingmodule,toyield layer adaptation by updating the last linear layer of the
a proxy for image and sparse depth embeddings from the source model; [23] extends this with TTT++ and utilizes
sourcedomainâ€“werefertotheembeddedsparsedepthfea- jointtask-specificandmodel-specificinformationbasedon
turesasproxyembeddings. Note: Asthemappingislearned self-supervisedlearning.[17]presentsT3A,anoptimization-
inthesourcedomain,theproxyembeddingswillalsofollow free classifier adjustment module. [5] uses shift-agnostic
the distribution of source image and sparse depth embed- weightregularization(SWR)topreventaneffectfromthe
dings. Next, both image and sparse depth from the target erroneoussignalintesttime,jointlywiththenearestsource
test domain are fed as input to the encoder. By maximiz- prototype classifier and a self-supervised proxy task. [2]
ingthesimilaritybetweentest-timeinputembeddingsand proposesacontrastivelearningwithanonlinepseudo-label
the proxy embeddings, we align the target distribution to refinementwhile[34]proposespseudo-labelrefinementand
thatofthesourcetoreducethedomaingap. Whenusedin momentumupdatefor3Dpointcloudsegmentation. [44]
conjunctionwithtypicallossfunctionstopenalizediscrepan- proposescontinualtest-timeadaptationbasedonstochastic
ciesbetweenpredictionsandinputsparsedepth,andabrupt restoration and weight-averaged pseudo-labels. [37] uses
depthtransitions,i.e.,TotalVariation,theembeddingsserve efficientresidualmodulestorealignthepretrainedweights.
asregularizationtoguidethemodelparameterupdateand The above methods largely focus on single-image-based
preventexcessivedriftfromthosetrainedonthesourcedata. tasks,i.e.,classification[2,5,23]andsemanticsegmenta-
Followingtest-timeadaptationconventions,weassume tion[34],andrelyonentropyconstraintsfrom[43].
limitedcomputationalresources,andthatinputsarriveina Unlikepriorworkonclassification[23,40]andsegmen-
streamofsmallbatchesandmustbeprocessedwithinatime tation[34],depthcompletionisaregressionproblem;hence,
budgetwithoutaccesstothepastdata. Toensurefastmodel existingmethodsusingentropy-basedobjectives[43],which
updatingundertheseconstraints,wedeployauxiliaryparam- operate on logits, are not applicable in this task. Instead,
eters,oraadaptationlayer,tobeupdatedwhilefreezingthe weproposetominimizesparsedepthreconstructionandlo-
restofthenetworkâ€“thusachievinglow-costadaptation. We calsmoothnessobjectivesâ€“similartothatofsomeexisting
demonstrateourmethodinbothindoor(VIO)andoutdoor unsupervised methods [45, 46] â€“ and to maximize cosine
(lidar)settingsacrosssixdatasets,wherewenotonlytarget similaritybetweentheproxyembeddingsandthetesttime
typicaladaptationscenarioswheretheshiftexistsbetween imageandsparsedepthembeddings.
realandsyntheticdatadomainswithsimilarscenes,i.e.from
DepthCompletionaimstooutputdensedepthfroma
KITTI[41]toVirtualKITTI[10],butalsobetweendiffer-
singleimageandsynchronizedpointcloud,i.e.,fromlidar
entscenelayouts,i.e.,fromVOID[46]toNYUv2[27]and
ortrackedbyVIO,projectedontoasparsedepthmap.
SceneNet[26]. Ourproxyembeddingsconsistentlyimprove
Unsupervised depth completion approaches rely on
overbaselinesbyanaverageof21.09%acrossallmethods
Structure-from-Motion (SfM) for training and require ac-
anddatasets. Tothebestofourknowledge,wearethefirst
cesstoanauxiliarytrainingdatasetcontainingstereoimage
tointroducetest-timeadaptationfordepthcompletion.
pairs [35] or monocular videos [25, 46â€“48] with synchro-
nizedsparsedepthmaps. Typically,theyminimizealinear
2.Relatedwork
combinationofphotometricreprojectionconsistency,sparse
TestTimeAdaptation(TTA)aimstoadaptagivenmodel, depthreconstructionerror,andlocalsmoothness[25,46â€“48].
pretrainedonsourcedata,totestdatawithoutaccesstothe Thesemethodscansupportonlinetraining,butarelimited
sourcetrainingdata. Relatedfieldsalongthisveininclude bytheneedforstereoormonocularvideoswithsufficient
unsupervised domain adaptation [11, 29], which utilizes parallaxandco-visibility. Incontrast,ourapproachdoesnot
sourcedomaindata(inpractice,thismaynotbeavailable)for relyonSfMandcanbeusedinmoregeneralscenarios.
adaptation,andsource-freedomainadaptation[20],which Supervised depthcompletion trains themodel by mini-
does not assume access to source data, but allows access mizing a loss with respect to ground truth. [3, 16] focus
to test data on multiple passes. In contrast, we focus on onnetworkoperationsanddesignstoeffectivelydealwith
test-timeadaptationwherewedonothaveaccesstosource sparseinputs. [18,25,50]proposeearlyandlatefusionof
dataandmustadapttotestdatainasinglepass. imageanddepthencoderfeatureswhile[15]usesseparate
Previous studies have proposed strategies to select the networks for each. [21] proposes a multi-scale cascaded
sourcemodelâ€™scomponenttobepreserved,suchastheclass hourglass network to enhance the depth encoder with im-
prototypes extracted from the source data [5, 23, 34], the agefeatures. [4]proposesconvolutionalspatialpropagation
2Inputs Image only Sparse depth only Image + sparse depth Ground truth
8.0m
RMSE: 2601.01 RMSE: 597.91 RMSE: 496.65
0.1m
0.1
RMSE: 2462.63 RMSE: 1046.28 RMSE: 1528.98
0.0
Figure1.Modelsensitivitytoinputmodalities.Whileutilizingbothsparsedepthandimageasinput,thebestperformanceisachievedinthe
sourcedomain(VOID).Yet,forgoingtheimageinthetestdomain(NYUv2)oftenyieldslowererrorthanusingbothasinput.
network;[28]extendsittonon-localspatialpropagationto (whicharelesssensitivetothedomainshift)thatservesas
refineaninitialdepthmapbasedonconfidenceandlearnable proxy to source features for guiding test-time adaptation.
affinity;[22]furtherextendsittodynamicspatialpropaga- (iii)Tothebestofourknowledge,wearethefirsttopropose
tion. [7,8,31,32]learnuncertaintyofthedepthestimates. test-timeadaptationforthedepthcompletiontask,and(iv)
[42]utilizesconfidencemapstocombinedepthpredictions willreleasecode,models,anddatasetbenchmarkingsetup
while[30,49,52]usethesurfacenormalstoguidedepthpre- tomakedevelopmentaccessiblefortheresearchcommunity.
diction. [19]incorporatescostvolumefordepthprediction.
[33]devisestransformerarchitecturewithcross-modalat-
3.MethodFormulation
tention,and[51]proposesahybridconvolution-transformer
architecturefordepthcompletion. Foreaseofuse,weassumeaccesstoa(source)pretrained
Whilebothunsupervisedandsupervisedmethodshave depthcompletionmodelf Î¸ thatinfersadensedepthmapdË†
demonstratedstrongperformanceonbenchmarks,theyoften fromacalibratedRGBimageI âˆˆRHÃ—WÃ—3anditsassoci-
failtogeneralizetotestdatasetswithlargedomaindiscrep- atedsparsepointcloudprojectedontotheimageplaneasa
ancies. Moreover,obtaininggroundtruthisunrealisticfor sparsedepthmapz âˆˆR +HÃ—W,i.e.,f Î¸(I,z)â†’dË†âˆˆRH +Ã—W.
real-timeapplications,andaccumulatingsufficientparallax Forsimplicity,weassumethatthemodelwastrainedtomin-
incurslargelatenciesâ€“presentingsignificantchallengesfor imizeasupervisedlossbetweenpredictionandgroundtruth
online adaptation. Unlike past works, we do not assume dâˆˆR +HÃ—W onasourcedatasetD
s
={I s(n),z s(n),d(n)}N n=s 1,
accesstogroundtruthnordataoutsideoftheinput. whereN indicatesthenumberofdatasamples. Following
s
UnsupervisedDomainAdaptation(UDA)isdesigned conventionsinTTA,weassumeaccesstothesourcedomain
toaddressthediscrepancybetweenlabeledsourcedataand datasetpriortodeployment.
unlabeledtargetdata[11,29,39]. Theonlyexistingdepth During test-time adaptation, we follow the protocol of
completionUDAapproach[24]modelsthedomaindiscrep- [23, 40], where we only have access to the target domain
ancyasthenoiseinsparsepointsandtheappearanceinRGB dataD ={I(n),z(n)}Nt andutilizeanonlineprocedure
t t t n=1
images. UnlikemostUDAapproachesthatrequiresource to adapt to unseen N target data samples. Note that we
t
dataduringadaptation,weareonlygiventheinputsneces- make no assumptions about supervision during test-time;
saryforinferenceinastreamwithouttheabilitytorevisit hence,whilewepresentresultsonsupervisedmethodsfor
pastdata,andmustupdatethemodelonlineunderalimited controlledexperiments,weseeourmethodbeingapplicable
computationalbudget. towardsunsupervisedmethodsaswell.
OurContributions. Wepresent(i)astudyonhowthe Our method is split into three stages (Fig. 3): (a) Dur-
domainshiftineachdatamodality(e.g.,imageandsparse inganintializationstage,weaugmentthenetworkencoder
depth)affectsmodelperformancewhentransferringitfrom with a adaptation layer and train it using source domain
sourcetotargettestdomain. Thisstudymotivates(ii)our data. (b)Inthepreparationstage,welearnamappingfrom
approach to learn an embedding of sparse depth features sparsedepthfeaturestoimageandsparsedepth(proxy)em-
3
niamoD
ecruoS
niamoD
tegraT
egamI
htped
esrapS
egamI
htped
esrapS
stupni
tnereffid
htiw
spam
rorre
dna
snoitciderPMAE: VOID â†’ NYUv2 MAE: VOID â†’ScanNet MAE: KITTI â†’Waymo MAE: KITTI â†’nuScenes
2400 3000 24000 18000
1600 2000 16000 12000
800 1000 8000 6000
0 0 0 0
MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet
RMSE: VOID â†’NYUv2 RMSE: VOID â†’ScanNet RMSE: KITTI â†’Waymo RMSE: KITTI â†’nuScenes
3000 3000 27000 24000
2000 2000 18000 16000
1000 1000 9000 8000
0 0 0 0
MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet
Image only Sparse depth only Image + sparse depth
Figure2. Modelsensitivitytoinputmodalities. Depthcompletionnetworkshaveahighrelianceonsparsedepthmodality. Performing
inferenceinanoveldomainwithouttheRGBimage,i.e.,usingjustsparsedepthasinput,canimproveoverusingbothdatamodalities.
beddings. (c)Duringtesttime,wedonotneedthesource withoutimageinformationinthetestdomainisbetterthan
dataset;wefreezethemappinganduseitsproxyembeddings with it. Conversely, the performance gap between infer-
forupdatingtheadaptationlayerparametersinthetargettest encewithbothinputs,dË†(I,z),andjusttheimage,dË†(I,z ),
t t 0
domain. becomesmoreevidentunderthedomainshift. Thisobserva-
tionillustratesanotherintuition:(ii)Thedomainshiftlargely
3.1.SensitivityStudyonDataModalities affectstheimagemodality,andlesssodepth.
Thetwointuitionsabovemotivateourapproach. Asob-
Tomotivateourapproach,webeginwithasensitivitystudy
jectshapestendtopersistacrossdomains,andthemeasured
ofdepthcompletionnetworkstoinputmodalities,e.g.image,
sparsepointsbeingacoarserepresentationofthem,weaim
sparsedepth,andtheeffectofdomainshiftonthem. Tothis
toleveragesparsedepthmodalitytobridgethedomaingap.
end,wealtertheinputsbyzeroingouteitherI orztoyield
Tothisend,weexploittheobservationthatdepthcomple-
(I,z),(I ,z),and(I,z ),whereI andz indicatethezero
0 0 0 0
tionnetworksareabletorecover(coarse)3Dscenesfrom
matriceswithidenticalsizetoI andz,respectively. Weeval-
sparsepointsaloneandthattheimageservestopropagate
uatethepretrainedmodelsusing(I,z),(I ,z),and(I,z )
0 0
and refine depth for regions lacking points. This is done
tohighlighttheirdependenceoneachinputmodalityandto
by learning to map features encoding sparse depth inputs
gaugetheirsensitivitywhenonemodalitygivesnouseful
tofeaturesencodingbothmodalitiesinthesourcedomain
informationatall. Fig.1andFig.2showqualitative(error
and, during test-time, recover the source domain features
maps)andquantitativeresults(bargraphs),respectively,of
compatiblewithtargetdomainsparsedepthtoguidemodel
pretraineddepthcompletionmodelswhenfedthedifferent
adaptation. Specifically,asobserved,thecovariateshiftis
inputsonthesourcedatasetD andthetargetdatasetD .
s t
largelyphotometric,soweproposetoadapttheRGBimage
In the source domain, inference using both image and
sparsedepthasinputs,i.e.,dË†(I,z),showsthebestperfor- encoderbranchbyintroducingaadaptationlayer: asingle
s
convolutional layer designed to align target domain RGB
mance. Surprisingly,theinferenceusingsparsedepthalone
(i.e., with null-image) dË†(I ,z) is comparable to dË†(I,z). embeddingstothoseofthesourcedomain. Astherestofthe
s 0 s
networkisfrozen,adaptingjusttheadaptationlayerallows
Thisshowsthefirstintuitionbehindourapproach: (i)Even
forlow-costmodelupdates.
though depth inputs are sparse, they are sufficient to sup-
portthereconstructionofthescene. Additionally,inference Intuitionforintegratingadaptationlayer. Guidedby
withimagealone(i.e., null-depth)dË†(I,z )isworsethan ourobservations,theadaptationlayershouldbe(i)placed
s 0
dË†(I ,z) and dË†(I,z), which suggests that a depth com- in the image encoder branch prior to the fusion of image
s 0 s
and depth features, and (ii) located within later layers to
pletionnetworkreliesheavilyonsparsedepthmodalityfor
modulate higher level representations (i.e., object shapes,
inference,andtheimageforguidingrecoveryoffinerdetails.
as opposed to low-level edges). (iii) Ideally it should be
In the target test domain, expectedly, performance de-
connectedasaskipconnectiontopropagateitseffectstothe
gradesforinferenceusingbothimageandsparsedepthdue
decoder.
to a covariate shift. Remarkably, we observe that predic-
tionsfromsparsedepthalonedË†(I ,z)remainconsistentin
t 0
3.2.PreparationStage-SourceDomain
performancetothoseusingbothinputsdË†(I,z). Moreover,
t
weobservethatinmostcasesdË† t(I 0,z),infact,outperforms Initializeadaptationlayerfromsourcedomain. Updating
dË†(I,z)acrossseveralmethodsanddatasets,i.e.,inference the entire network is largely infeasible in test-time adap-
t
4SSoouurrccee DDaattaasseett Target Dataset
ğ¼" Output depth ğ‘‘$ " Imageğ¼! Local PS em no ao ltt yhness
Supervised
ğ¼!
Encoder Decoder Loss
ğ‘§" Ground truthğ‘‘" Decoder
Predictionğ‘‘$
!
(a) Initialize Meta Layer ğ‘§! S Cp oa nrs se
is
tD ee np ct yh
Source Dataset
Sparse depthğ‘§!
Projection MLPs
ğ‘”! â„" ğ‘”!#
ğ¼# ğ¼# Projection MLPs
EMA update Proxy
Proxy loss Consistency
ğ‘§" Encoder ğ‘”! â„"
ğ‘”!#
ğ‘§! StopGrad
StopGrad
ğ¼" Encoder AdaptationLayer Weight sharing Updated param. Frozen param.
(b) Preparation â€“Learning proxy embedding (c) Adaptation
Figure3.Overview.(a)Thepretrainingstageintegratesaadaptationlayerintoapretrainedencoderandpretrainstheadaptationlayeronthe
sourcedataset.(b)Thepreparationstagelearnstheproxymappingoffeaturesencodingsparsedepthtothoseencodingbothinputs.(c)The
adaptationstagedeploysthemodeltothetargetdomainandupdatestheadaptationlayerbyleveragingtheproxyembeddingstoguide
adaptation.
tationscenarios. Forthesakeofspeed and efficiency, we themappingnaturallyyieldsproxyembeddingsthatencode
implementaadaptationlayerm ,i.e.,aconvolutionallayer, thesourcedomainimage(andsparsedepth),whichcanbe
Ï•
within the encoder of a pretrained network. Note that the laterusedtoguidetheadaptationlayerm totransformtest
Ï•
entirenetworkwillbefrozenduringallstagesofourmethod domainRGBfeaturesclosetothoseofthesourcedomain.
withtheexceptionoftheadaptationlayerandproxymap- ThismappingisrealizedasMLPsdenotedasg (Â·)and
Ïˆ
ping,wherebothwillbeinitializedduringpreparationstage h (Â·);tolearnthem,wefirstgettwoembeddingsppp andqqq ,
Ï‰ s s
inthesourcedomain,andtheproxymappingwillbefrozen
andusedtoadapttheadaptationlayerinthetargetdomain.
ppp =h (g (StopGrad(e (I ,z )))),
s Ï‰ Ïˆ Ï• 0 s
Toeasetheadaptationprocess,weinitializetheadaptation (1)
qqq =StopGrad(g (e (I ,z )))
layerm byminimizingasupervisedlossoverthesource s Ïˆ Ï• s s
Ï•
dataset(Fig.3-(a)). Wedenotethepretrainedencoderinte-
gratedwithm Ï•ase Ï•. wheree Ï•denotestheencoderaugmentedwiththeadap-
tationlayertrainedonsourcedataset,I ,z ,theimageand
Learning proxy mapping from source domain. As ob- s s
sparsedepthfromsourcedomain,andI thenull-image.The
served in Fig. 1, the best resultsin the source domain are 0
embeddingmodulesg andh areupdatedtomaximizethe
achieved by feeding in both the image and sparse depth Ïˆ Ï‰
similaritybetweenppp andqqq . Tolearnthem,weminimize:
modalitiesforinference. However,theimageissusceptible s s
todomainshiftwhichdegradesperformancewhenthemodel
ppp qqq
istransferredtoanunseentestdomain. Conversely,sparse â„“ =1âˆ’( s Â· s ), (2)
prepare âˆ¥ppp âˆ¥ âˆ¥qqq âˆ¥
depthismoreresilienttothedomainshiftthanRGBimages, s s
i.e., theshapeofacar(oranotherobject)remainssimilar
regardlessof(syntheticorreal)domain. Ourmethodaims whereâˆ¥Â·âˆ¥isL2-norm,and(aaaÂ·bbb)indicatesthedotproduct
toleveragethesparsedepthmodality,whichislesssensitive ofthevectorsaaaandbbb. Tothisend,wefirsttraintheMLP
tothedomainshift,inthedownstreamadaptationprocess. headsg Ïˆ,h Ï‰ byminimizingEqn.2.
Tothisend, weemployasoftmapping[13]fromjustthe Oncethemappingislearned,wecanfreezetheembed-
encoded sparse depth features to sparse depth and image ding module and deploy it for test-time adaptation where
featurestolearnthephotometricinformationthatiscaptured weupdatetheadaptationlayerweightsÏ•tomaximizethe
fromthesamesceneasthesparsepointcloud. Thisstrategy similaritybetweentheembeddingsofatestdomainimage
allowsustolearnthemappingthatprojectsthesparsedepth andsparsedepth,anditsproxyfromthesourcedomain. Nat-
featurestoâ€œproxyâ€embeddingsclosetothosethatalsoen- urally, duetothedomainshift, theembeddingswillyield
codetheimage. Inotherwords, itfillsinwhatismissing lowsimilarityscores;hence,maximizingthescoresthrough
intheimageencoderbranchbypredictingtheresidualla- ourproxyembeddingimplicitlyalignsthetargetRGBdistri-
tentimageencodingthatiscompatiblewiththeinputsparse butiontothatofthesourcedistribution,i.e.,minimizingthe
depth,i.e.,3Dscene. Asthisistrainedinthesourcedomain, cosinesimilaritybetweenthesourceandtargetdistributions.
5
egamI
htped
esrapS
egamI
lluN
htped
esrapS
egamI
egamI
htped
esrapS
egamI
lluN
htped
esrapS3.3.DeployingProxyMappingtoTargetDomain 4.Experiments
Adaptationstageaimstoupdatetheadaptationlayerparam- Wedemonstratetheeffectivenessofourapproachonamixof
etersbyminimizingatest-timelossfunctionoverthetarget bothrealandsyntheticdatasetsincludingindoorSLAM/VIO
test domain data {I t,z t} âˆˆ D t. To do so, we deploy the scenarios (VOID [46], NYUv2 [27], SceneNet [26], and
learnedproxymappingmodule(MLPheads{gâˆ—(Â·),hâˆ—(Â·)}) ScanNet[6])andoutdoordrivingscenariosusinglidarsensor
Ïˆ Ï‰
alongwiththeadaptationlayerm Ï•integratedintothefrozen (KITTI[41],VirtualKITTI[10],nuScenes[1],andWaymo
encoder. OpenDataset[38]). Wechosethreerepresentativearchitec-
Adaptationloss. Foradaptation,ourlossiscomposedof turesofcurrentdepthcompletionmethodstotestourmethod:
alinearcombinationofthreelossterms: MSG-CHN [21] (CNN-based), NLSPN [28] (SPN-based)
andCostDCNet[19](costvolume-based). Allreportedre-
L =w â„“ +w â„“ +w â„“ , (3) sults are averaged over 5 independent trials. We describe
adapt z z sm sm proxy proxy
implementation details, hyper-parameters used, hardware
whereâ„“ z,â„“ sm denotesparsedepthconsistencylossandlo- requirements,evaluationmetricsaswellasadditionalexper-
calsmoothnessloss, respectively, â„“ proxy isproxymapping imentalresultsintheSupp. Mat.
consistencyloss,andwindicatesaweightofeachlossterm. Main Result. We use pretrained models (MSG-CHN,
SparseDepthConsistency. Sparsepointcloudscapture NLSPN, and CostDCNet) from the two source datasets,
a coarse structure of the 3D scene.To obtain metric scale VOIDforindoor,andKITTIforoutdoordepthcompletion.
predictionsconsistentwiththescenestructure,weminimize Forindoor,weadaptmodelspretrainedonVOIDtoNYUv2,
L1errorbetweenthesparsedepthz tandthepredictiondË† t: SceneNet,andScanNet;foroutdoors,weadaptfromKITTI
to Virtual KITTI (VKITTI) (with fog), nuScenes, and
â„“ = 1 (cid:88) |dË†(x)âˆ’z (x)|, (4) Waymo. BN Adapt denotes updating the batch statistics
z |â„¦(z t)|
xâˆˆâ„¦(zt)
t t ( vi a.e ri. a, tr iu on nn oin fg TEm Nea Tn [a 4n 3d ]wva hr ii ca hnc me i) n. imBN izeA sd Ea qp nt, .4â„“
z
,, 5â„“
is nm
sti es ada
wherexâˆˆâ„¦(z )arethepixellocationswheresparsepoints ofentropybyupdatinglearnablescalefactors. CoTTAde-
t
wereprojectedontotheimageplane. notesreplacingproxylosswithL1consistencylossw.r.t. the
Local Smoothness. Based on the assumption of local pretrained prediction [44]. Ours-fast denotes our method
smoothnessandconnectivityina3Dscene,weimposethe withoutbatchnormupdate,whichimprovesadaptationrun-
sameinthepredicteddepthmapdË†. Specifically,weapply timeby25.32%.
t
anL1penaltytoitsgradientsinboththex-andy-directions Our method consistently improves over baselines and
(i.e.,âˆ‚ andâˆ‚ ). Webalancetheweightofeachtermwith variantsofBNAdapt(Table1). Specifically, weimprove
X Y
Î» X andÎ» Y,toallowdiscontinuitiesoverobjectboundaries over BN Adapt, â„“ z,â„“ sm by 11.60% on average across all
basedontheimagegradients,whereÎ» X(x)=eâˆ’|âˆ‚XIt(x)|, methodsforindoor,19.73%onoutdoors,and15.67%over-
Î» Y(x)=eâˆ’|âˆ‚YIt(x)|,andâ„¦denotestheimagedomain. all to achieve state-of-the-art performance. Qualitatively,
Fig.4andFig.5showthatourmethodperformsbetterin
â„“ = 1 (cid:88) Î» (x)|âˆ‚ dË†(x)|+Î» (x)|âˆ‚ dË†(x)|. (5) boundaryregionsandhomogeneousregions, thusexhibit-
sm |â„¦| X X t Y Y t inglessoversmoothingoncurtainsinFig.4-(a)andcarin
xâˆˆâ„¦ Fig.5-(b),andundersmoothingonblackboardinFig.4-(d)
androadinFig.5-(a),respectively,duringadaptation. This
ProxyConsistency. Inordertoregularizetheadaptation
trendisduetotheproxylossandtheadaptationlayer,which
withthelearnedmappingfromthepreviousstage,wefreeze
allowsustoadaptwithminimumweightadjustmentswhile
the weight parameters of MLP heads {gâˆ—(Â·),hâˆ—(Â·)}, and
Ïˆ Ï‰ preservinghigh-levelfeatures(objectshapes)learnedfrom
updatetheparametersoftheadaptationlayerm . First,we
Ï• thesourcedomainbymappingthetargetRGBmodalityto
obtainthefeaturesppp andqqq usingthenull-imageI inone
t t 0 thatofthesourcedomain. Notably,Ours-faststillimproves
andthegiventargettestdomainimageI intheother:
t overBNAdapteventhoughweonlyadaptouradaptation
layer, which demonstrates the effectiveness of our design
choiceaswellasourproposedproxyembeddings.
ppp =StopGrad(hâˆ—(gâˆ—(e (I ,z )))), qqq =gâˆ—(e (I ,z )).
t Ï‰ Ïˆ Ï• 0 t t Ïˆ Ï• t t Comparison to BN adaptation1 and CoTTA. To as-
(6)
sess the impact of our adaptation layer, we compared our
Wemaximizethecosinesimilaritybetweenthefeatureqqq
t approach to the batch norm adaptation from TENT [43].
andppp byusingproxylossâ„“ toupdateadaptationlayer
t proxy IntheBNAdaptscenario,weonlyupdatethebatchnorm
m :
Ï•
â„“ =1âˆ’( ppp t Â· qqq t ). (7) 1BNadaptationcannotbeappliedtoMSG-CHNduetotheabsenceofa
proxy âˆ¥ppp âˆ¥ âˆ¥qqq âˆ¥ batchnormlayer.
t t
6KITTIâ†’VKITTI-FOG KITTIâ†’nuScenes KITTIâ†’Waymo
Method MAE RMSE MAE RMSE MAE RMSE
Pretrained 2842.88 6557.38 3331.821 6449.094 1107.22 2962.45
CoTTA 730.6Â±11.67 3330.23Â±44.83 3157.69 6434.14 655.77Â±30.98 2213.27Â±98.80
MSG-CHN
Ours-fast 728.24Â±3.73 3087.36Â±15.92 2834.08Â±17.64 6096.56Â±21.08 608.91Â±1.74 1921.83Â±2.54
Pretrained 1309.99 7423.48 2656.609 6146.590 1175.83 3078.377
BNAdapt 1140.21Â±35.89 4592.86Â±198.21 11291.57Â±21.32 16670.87Â±52.56 7283.33Â±104.58 9670.36Â±250.22
NLSPN
BNAdapt,â„“z,â„“sm 775.20Â±5.65 3465.05Â±32.73 2928.51Â±75.89 8209.24Â±164.31 494.94Â±3.08 1921.17Â±338.06
CoTTA 767.93Â±5.47 3799.88Â±17.29 2650.45Â±15.04 6242.52Â±33.14 933.41Â±4.31 2763.88Â±143.48
Ours-fast 732.61Â±29.57 3002.19Â±52.29 2733.96Â±34.32 6099.48Â±82.32 875.01Â±15.8 2400.17Â±21.44
Ours 686.91Â±22.14 2666.70Â±56.64 2589.25Â±59.03 6006.18Â±90.66 477.28Â±3.32 1598.64Â±18.95
Pretrained 1042.98 6301.60 3064.724 6630.649 1093.79 2798.25
BNAdapt 1476.57Â±1.38 5428.20Â±8.15 2306.04Â±28.86 6391.98Â±48.97 596.08Â±5.55 1877.91Â±45.56
BNAdapt,â„“z,â„“sm 729.67Â±3.14 3413.76Â±14.59 2288.85Â±14.02 6338.38Â±31.31 469.97Â±2.47 1572.95Â±10.63
CostDCNet CoTTA 756.32Â±3.59 3686.69Â±14.75 2676.83Â±68.92 6099.49Â±66.79 689.94Â±1.95 2140.23Â±16.12
Ours-fast 756.98Â±31.07 3091.78Â±105.42 2595.81Â±12.13 6373.01Â±7.74 606.10Â±11.10 1817.79Â±19.14
Ours 512.72Â±0.74 2735.01Â±3.53 2062.28Â±11.24 5509.96Â±23.41 466.44Â±1.63 1580.38Â±11.48
VOIDâ†’NYUv2 VOIDâ†’SceneNet VOIDâ†’ScanNet
Pretrained 1040.934 1528.983 281.28 645.01 687.988 1201.747
MSG-CHN CoTTA 876.93Â±146.95 1148.62Â±173.53 223.19Â±14.77 498.46Â±28.21 619.37Â±4.14 1141.04Â±7.35
Ours-fast 699.60Â±6.00 1120.37Â±9.76 192.74Â±1.72 424.49Â±4.58 302.21Â±4.10 480.08Â±8.03
Pretrained 388.87 702.80 167.250 438.71 233.33 431.20
BNAdapt 250.13Â±5.23 447.18Â±10.32 143.61Â±6.34 385.56Â±9.84 207.00Â±0.57 401.41Â±2.84
NLSPN
BNAdapt,â„“z,â„“sm 147.55Â±1.36 271.10Â±2.17 120.48Â±1.94 345.91Â±7.14 82.76Â±0.47 181.97Â±1.21
CoTTA 390.50Â±8.29 704.72Â±16.74 205.02Â±1.79 540.01Â±4.08 234.77Â±1.52 496.18Â±2.75
Ours-fast 168.43Â±3.46 309.48Â±6.92 124.67Â±1.33 357.56Â±2.59 104.06Â±11.03 232.84Â±20.46
Ours 124.41Â±2.27 240.73Â±5.72 113.93Â±1.49 333.41Â±4.32 74.77Â±0.31 166.61Â±0.45
Pretrained 189.10 446.71 173.37 443.22 144.31 458.69
BNAdapt 160.31Â±2.7 410.55Â±10.70 176.62Â±0.72 446.32Â±8.52 159.65Â±4.63 399.14Â±13.92
CostDCNet
BNAdapt,â„“z,â„“sm 136.80Â±5.35 338.59Â±22.36 134.22Â±2.33 385.9Â±6.68 68.44Â±0.46 164.59Â±2.82
CoTTA 147.69Â±5.3 376.87Â±21.25 136.42Â±3.41 405.38Â±11.63 101.98Â±1.53 322.63Â±5.04
Ours-fast 131.93Â±2.58 269.02Â±5.61 129.99Â±3.88 353.86Â±7.91 128.12Â±3.41 244.62Â±7.53
Ours 95.87Â±2.16 203.83Â±4.72 125.75Â±1.93 357.12Â±4.13 68.17Â±0.44 162.35Â±1.12
Table1.Test-timeadaptationfordepthcompletion.Forindoors,weadaptfromVOIDtoNYUv2,SceneNet,andScanNet;foroutdoors,
fromKITTItoVKITTIwithfog,nuScenes,andWaymo.BolddenotesthebestresultandItalicsthesecond-best.Ours-fastdenotesour
methodwithoutupdatingBatchNormlayers.
layerâ€™sscaleandshiftfactorbasedonthelossfunction. On pretrainedweightandthemodelprediction. Wecombined
average, BN Adapt with â„“ ,â„“ improves the pretrained additionallossâ„“ ,â„“ ontopofCoTTAloss, sinceweob-
z sm z sm
model by 32.77%; whereas, updating just our adaptation servedthatthemodelscannotbeadaptedwithCoTTAalone.
layer(Ours-fast)improvesitmoreby34.07%(Table1). The Specifically,ourmethodwithoutproxyshowsa25.26%aver-
improvementofOurs-fastoverBNadaptdemonstratesthe ageimprovementontheCoTTAmethod.CoTTAupdatesthe
effectivenessofupdatingadaptationlayer,whichdirectlyad- wholeparametersincludingRGBandsparsedepthbranch,
juststhehigh-levelfeaturesfromtheRGBbranchguidedby whichcausesadriftfromthelearnedmodelparameters. On
proxyloss,whereBNadaptrealignsthelearnedsourcefea- theotherside,ourmethodonlyupdatesadditionallayerat
turesfrombothRGBandrangesensorsbyupdatingfeature RGBbranch,basedonthestudyfromthemostdomaindis-
statistics. crepancycomesfromRGBmodalityasstudiedinSec. 3.1,
andthispreventsthemodelfromadriftfromlearneddomain.
Nonetheless, the best results are achieved when we in-
Also,CoTTAassumesthetest-timeaugmentationcanmiti-
clude batch norm update, which improves the pretrained
gatethedomainshift. However,theresultsshowstest-time
model by 44.53%, but at the cost â‰ˆ33.16% of extra time.
augmentationonRGBimage,causingasmalldistributional
TheimprovementofoursoverBNadaptimpliesthatadapt-
shift,maynotsolvealargedomaindiscrepancy.
ingonly BNparametersmay notbridge thelarge domain
discrepancyandmaynotbesolvedwithscalingandshift-
Also,ourmethodwithbatchnormalizationlayerupdate
ing,whereasourmethodexplicitlyadjustsRGBfeaturesby
shows26.52%averageimprovement,whileusing25.05%
updatingtheadaptationlayer.
lessadaptationtime. Note: CoTTAcostsnotonlyadditional
WealsocomparedourapproachtoCoTTA[44],which memoryforteachermodelbutalsoinferencetimetogetthe
adaptsthewholemodelparametersusingthepredictionfrom teachermodelprediction,evenifCoTTAdoesnotrequire
theteachermodelupdatedbyexponentialmovingaverageof anypreparationprocess. Overall,ourmethodshows21.09%
7(a) BN Adapt CoTTA Ours (b) BN Adapt CoTTA Ours
Image
Predicted Depth
Sparse Depth
Error Map
(c) (d)
Figure4.Representativeresultsfortest-timeadaptationonNYUv2.Boxeshighlightdetailedcomparisons.Notably,ourmethodperforms
betterinboundaryregionsdisplayingthediscontinuityindepth(e.g.,curtains,(a)),aswellashomogeneousregions(e.g.,blackboard,(d)).
(a) BN Adapt CoTTA Ours
Image
Predicted Depth
Sparse Depth
Error Map
(b)
Figure5. Test-timeadaptationresultsonNuScenesdataset. Ourmethodisconsistentindoorsandoutdoors. Boxeshighlightdetailed
comparisons.Asseenin Fig.4,ourmethodperformsbetterthanBNAdaptandCoTTA,notablyinbothdepth-discontinuousregions(e.g.,
carin(b))andhomogeneousregions(e.g.,roadin(a)and(b)).
averageimprovementoverBNadaptandCoTTAmethods. layouts. The proxy embedding captures latent photomet-
ricfeaturesoftheobjectshapespopulatingthem;thesame
5.Discussion
proxy embedding can be transferred across domains even
We have proposed a method for test-time adaptation for whenscenediffer,butshareobjectswithinthem.
depthcompletionthatleveragesthestrengthofcomplemen- Thisleadstopossiblelimitationsinthescenarioswhere
tarymulti-sensorsetupinthepresenceofdomainshift. By thesourcedatasetissampledfromscenesthatdonotshare
studyingmodelsensitivitytoeachinputmodalityaswellas anyobjectswiththetargettestdataset;inthiscase,theproxy
thedataunderdomainshift,wedesignedawaytoexploit embeddingsshouldgivelittletonogainandonemustrely
the modality (sparse depth) that is less sensitive to guide ongenericregularizerslikelocalsmoothness. Additionally,
adaptation. Wedosothroughaproxyembeddingthatlearns whilewefollowtheconventionsinTTAandassumeaccess
thephotometricinformationfromthesourcedomainthatis tothesourcedatasetpriortodeployment,inreality,many
compatiblewiththesparsedepthdepictinga3Dscene. Our modelsaretrainedonprivatedatasets,soadaptingâ€œoff-the-
proxyembeddingworkswellasaregularizerforscenarios shelfâ€modelsremainsachallenge. Insuchcases,onemust
wherethereexistscovariateshiftsinphotometry(i.e.,KITTI incorporateourpreparationpipelineintotheirmodeltrain-
toVKITTI)aswellasscenelayouts(i.e.,VOIDtoNYUv2 ingandreleasetheadaptationlayerandproxyembedding
and SceneNet). While one may surmise that the applica- moduletogetherwithnetworkweights. Nonetheless,thisis
tion of the embeddings are specific to scene distributions, thefirsttest-timeadaptationworkindepthcompletion;in
we show otherwise. VOID (classrooms, laboratories, and additiontoourfindings,weplantoreleasemodels,dataset,
gardens),NYUv2(householdsandshoppingcenters),and adapation,andevaluationcode,andhopetofurthermotivate
SceneNet(randomlyarrangedsyntheticrooms)alldifferin interestinTTAformulti-modaltaskslikedepthcompletion.
8References [14] ChristopherG.HarrisandM.J.Stephens.Acombinedcorner
andedgedetector. InAlveyVisionConference,1988. 11,12
[1] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,
[15] MuHu,ShulingWang,BinLi,ShiyuNing,LiFan,andXiao-
VeniceErinLiong,QiangXu,AnushKrishnan,YuPan,Gi-
jinGong. Penet:Towardspreciseandefficientimageguided
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
depthcompletion.In2021IEEEInternationalConferenceon
modal dataset for autonomous driving. In Proceedings of
RoboticsandAutomation(ICRA),pages13656â€“13662.IEEE,
theIEEE/CVFconferenceoncomputervisionandpattern
2021. 2
recognition,pages11621â€“11631,2020. 6,12
[16] Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi,
[2] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna
Xiaogang Wang, and Hongsheng Li. Hms-net: Hierarchi-
Ebrahimi.Contrastivetest-timeadaptation.InProceedingsof
calmulti-scalesparsity-invariantnetworkforsparsedepth
theIEEE/CVFConferenceonComputerVisionandPattern
completion. IEEE Transactions on Image Processing, 29:
Recognition,pages295â€“305,2022. 2
3429â€“3441,2019. 2
[3] YunChen,BinYang,MingLiang,andRaquelUrtasun.Learn-
[17] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier
ingjoint2d-3drepresentationsfordepthcompletion. InPro-
adjustmentmoduleformodel-agnosticdomaingeneralization.
ceedingsoftheIEEE/CVFInternationalConferenceonCom-
34:2427â€“2440,2021. 2
puterVision,pages10023â€“10032,2019. 2
[18] MaximilianJaritz,RaoulDeCharette,EmilieWirbel,Xavier
[4] XinjingCheng,PengWang,ChenyeGuan,andRuigangYang.
Perrotton,andFawziNashashibi. Sparseanddensedatawith
Cspn++:Learningcontextandresourceawareconvolutional
cnns:Depthcompletionandsemanticsegmentation. In2018
spatialpropagationnetworksfordepthcompletion. InPro-
InternationalConferenceon3DVision(3DV),pages52â€“60.
ceedingsoftheAAAIConferenceonArtificialIntelligence,
IEEE,2018. 2
pages10615â€“10622,2020. 2,11
[19] JaewonKam,JungeonKim,SoongjinKim,JaesikPark,and
[5] SunghaChoi,SeunghanYang,SeokeonChoi,andSungrack
SeungyongLee. Costdcnet:Costvolumebaseddepthcom-
Yun. Improvingtest-timeadaptationviashift-agnosticweight
pletionforasinglergb-dimage. InEuropeanConferenceon
regularizationandnearestsourceprototypes. InECCV,pages
ComputerVision,pages257â€“274.Springer,2022. 3,6
440â€“458.Springer,2022. 2
[20] Youngeun Kim, Donghyeon Cho, Kyeongtak Han,
[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
PriyadarshiniPanda,andSungeunHong. Domainadaptation
ber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet:
without source data. IEEE Transactions on Artificial
Richly-annotated 3d reconstructions of indoor scenes. In
Intelligence,2(6):508â€“518,2021. 2
ProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages5828â€“5839,2017. 6,12 [21] AngLi,ZejianYuan,YonggenLing,WanchaoChi,Chong
[7] AbdelrahmanEldesokey,MichaelFelsberg,andFahadShah- Zhang,etal. Amulti-scaleguidedcascadehourglassnetwork
bazKhan. Propagatingconfidencesthroughcnnsforsparse
fordepthcompletion.InProceedingsoftheIEEE/CVFWinter
dataregression. arXivpreprintarXiv:1805.11913,2018. 3 ConferenceonApplicationsofComputerVision,pages32â€“40,
2020. 2,6
[8] AbdelrahmanEldesokey,MichaelFelsberg,KarlHolmquist,
andMichaelPersson. Uncertainty-awarecnnsfordepthcom- [22] YuankaiLin,TaoCheng,QiZhong,WendingZhou,andHua
pletion:Uncertaintyfrombeginningtoend. InProceedings Yang. Dynamicspatialpropagationnetworkfordepthcom-
oftheIEEE/CVFConferenceonComputerVisionandPattern pletion. InProceedingsoftheAAAIConferenceonArtificial
Recognition,pages12014â€“12023,2020. 3 Intelligence,pages1638â€“1646,2022. 3
[9] XiaohanFei,AlexWong,andStefanoSoatto.Geo-supervised [23] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste
visualdepthprediction. IEEERoboticsandAutomationLet- Bellot-Gurlet,TaylorMordan,andAlexandreAlahi. TTT++:
ters,4(2):1661â€“1668,2019. 11 Whendoesself-supervisedtest-timetrainingfailorthrive?
[10] AdrienGaidon,QiaoWang,YohannCabon,andEleonoraVig. 34:21808â€“21820,2021. 2,3
Virtualworldsasproxyformulti-objecttrackinganalysis. In [24] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian
ProceedingsoftheIEEEconferenceoncomputervisionand Mikolajczyk. Projecttoadapt:Domainadaptationfordepth
patternrecognition,pages4340â€“4349,2016. 2,6,12 completionfromnoisyandsparsesensordata.InProceedings
[11] YaroslavGaninandVictorLempitsky. Unsuperviseddomain oftheAsianConferenceonComputerVision,2020. 3
adaptationbybackpropagation. InInternationalconference [25] FangchangMa,GuilhermeVenturelliCavalheiro,andSertac
onmachinelearning,pages1180â€“1189.PMLR,2015. 2,3 Karaman. Self-supervisedsparse-to-dense:Self-supervised
[12] AndreasGeiger,PhilipLenz,ChristophStiller,andRaquel depth completion from lidar and monocular camera. In
Urtasun. Visionmeetsrobotics:Thekittidataset. TheInter- 2019InternationalConferenceonRoboticsandAutomation
nationalJournalofRoboticsResearch,32:1231â€“1237,2013. (ICRA),pages3288â€“3295.IEEE,2019. 2
11 [26] JohnMcCormac,AnkurHanda,StefanLeutenegger,andAn-
[13] Jean-BastienGrill,FlorianStrub,FlorentAltchÃ©,Corentin drewJDavison. Scenenetrgb-d:5mphotorealisticimagesof
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer- syntheticindoortrajectorieswithgroundtruth.arXivpreprint
sch,BernardoAvilaPires,ZhaohanGuo,MohammadGhesh- arXiv:1612.05079,2016. 2,6,12
laghiAzar,etal. Bootstrapyourownlatent-anewapproach [27] PushmeetKohliNathanSilberman,DerekHoiemandRob
toself-supervisedlearning. Advancesinneuralinformation Fergus.Indoorsegmentationandsupportinferencefromrgbd
processingsystems,33:21271â€“21284,2020. 5,13 images. InECCV,2012. 2,6
9[28] JinsunPark,KyungdonJoo,ZheHu,Chi-KueiLiu,andIn supervisionforgeneralizationunderdistributionshifts. In
SoKweon. Non-localspatialpropagationnetworkfordepth ICML,pages9229â€“9248.PMLR,2020. 2,3
completion.InComputerVisionâ€“ECCV2020:16thEuropean [41] JonasUhrig,NickSchneider,LukasSchneider,UweFranke,
Conference,Glasgow,UK,August23â€“28,2020,Proceedings, ThomasBrox,andAndreasGeiger. Sparsityinvariantcnns.
PartXIII16,pages120â€“136.Springer,2020. 3,6,11 In2017internationalconferenceon3DVision(3DV),pages
[29] XingchaoPeng,QinxunBai,XideXia,ZijunHuang,Kate 11â€“20.IEEE,2017. 2,6,11,12
Saenko,andBoWang. Momentmatchingformulti-source [42] Wouter Van Gansbeke, Davy Neven, Bert De Brabandere,
domainadaptation. InProceedingsoftheIEEE/CVFinter- andLucVanGool. Sparseandnoisylidarcompletionwith
nationalconferenceoncomputervision,pages1406â€“1415, rgb guidance and uncertainty. In 2019 16th international
2019. 2,3 conferenceonmachinevisionapplications(MVA),pages1â€“6.
[30] JiaxiongQiu,ZhaopengCui,YindaZhang,XingdiZhang, IEEE,2019. 3
Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deepli- [43] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-
dar:Deepsurfacenormalguideddepthpredictionforoutdoor shausen,andTrevorDarrell. Tent:Fullytest-timeadaptation
scenefromsparselidardataandsinglecolorimage. InPro- byentropyminimization. arXivpreprintarXiv:2006.10726,
ceedingsoftheIEEE/CVFConferenceonComputerVision 2020. 2,6
andPatternRecognition,pages3313â€“3322,2019. 3
[44] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai.
[31] ChaoQu,TyNguyen,andCamilloTaylor. Depthcompletion Continualtest-timedomainadaptation. InProceedingsof
viadeepbasisfitting.InProceedingsoftheIEEE/CVFWinter theIEEE/CVFConferenceonComputerVisionandPattern
ConferenceonApplicationsofComputerVision,pages71â€“80, Recognition,pages7201â€“7211,2022. 2,6,7
2020. 3
[45] AlexWongandStefanoSoatto. Unsuperviseddepthcomple-
[32] ChaoQu,WenxinLiu,andCamilloJTaylor. Bayesiandeep
tionwithcalibratedbackprojectionlayers. InProceedingsof
basisfittingfordepthcompletionwithuncertainty.InProceed-
theIEEE/CVFInternationalConferenceonComputerVision,
ingsoftheIEEE/CVFinternationalconferenceoncomputer
pages12747â€“12756,2021. 2
vision,pages16147â€“16157,2021. 3
[46] AlexWong,XiaohanFei,StephanieTsuei,andStefanoSoatto.
[33] Kyeongha Rho, Jinsung Ha, and Youngjung Kim. Guide-
Unsuperviseddepthcompletionfromvisualinertialodome-
former: Transformers for image guided depth completion.
try. IEEERoboticsandAutomationLetters,5(2):1899â€“1906,
InProceedingsoftheIEEE/CVFConferenceonComputer
2020. 2,6,11
VisionandPatternRecognition,pages6250â€“6259,2022. 3
[47] AlexWong,SafaCicek,andStefanoSoatto. Learningtopol-
[34] InkyuShin,Yi-HsuanTsai,BingbingZhuang,SamuelSchul-
ogyfromsyntheticdataforunsuperviseddepthcompletion.
ter,BuyuLiu,SparshGarg,InSoKweon,andKuk-JinYoon.
IEEE Robotics and Automation Letters, 6(2):1495â€“1502,
Mm-tta: multi-modaltest-timeadaptationfor3dsemantic
2021.
segmentation. InCVPR,pages16928â€“16937,2022. 2
[48] Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano
[35] ShreyasSShivakumar,TyNguyen,IanDMiller,StevenW
Soatto. Anadaptiveframeworkforlearningunsupervised
Chen,VijayKumar,andCamilloJTaylor. Dfusenet: Deep
depthcompletion. IEEERoboticsandAutomationLetters,6
fusionofrgbandsparsedepthinformationforimageguided
(2):3120â€“3127,2021. 2
densedepthcompletion.In2019IEEEIntelligentTransporta-
[49] YanXu,XingeZhu,JianpingShi,GuofengZhang,Hujun
tionSystemsConference(ITSC),pages13â€“20.IEEE,2019.
Bao,andHongshengLi. Depthcompletionfromsparselidar
2
datawithdepth-normalconstraints. InProceedingsofthe
[36] NathanSilberman,DerekHoiem,PushmeetKohli,andRob
IEEE/CVF International Conference on Computer Vision,
Fergus.Indoorsegmentationandsupportinferencefromrgbd
pages2811â€“2820,2019. 3
images. InEuropeanConferenceonComputerVision,2012.
[50] YanchaoYang,AlexWong,andStefanoSoatto. Densedepth
11
posterior(ddp)fromsingleimageandsparserange. InPro-
[37] JunhaSong,JungsooLee,InSoKweon,andSunghaChoi.
ceedingsoftheIEEE/CVFConferenceonComputerVision
Ecotta:Memory-efficientcontinualtest-timeadaptationvia
andPatternRecognition,pages3353â€“3362,2019. 2
self-distilledregularization. InProceedingsoftheIEEE/CVF
[51] Zhang Youmin, Guo Xianda, Poggi Matteo, Zhu Zheng,
Conference on Computer Vision and Pattern Recognition,
Huang Guan, and Mattoccia Stefano. Completionformer:
pages11920â€“11929,2023. 2
Depthcompletionwithconvolutionsandvisiontransformers.
[38] PeiSun, HenrikKretzschmar, XerxesDotiwalla, Aurelien
arXivpreprintarXiv:2304.13030,2023. 3
Chouard,VijaysaiPatnaik,PaulTsui,JamesGuo,YinZhou,
YuningChai,BenjaminCaine,etal. Scalabilityinperception [52] YindaZhangandThomasFunkhouser. Deepdepthcomple-
forautonomousdriving:Waymoopendataset.InProceedings tion of a single rgb-d image. In Proceedings of the IEEE
oftheIEEE/CVFconferenceoncomputervisionandpattern Conference on Computer Vision and Pattern Recognition,
recognition,pages2446â€“2454,2020. 6,12 pages175â€“185,2018. 3
[39] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros.
Unsupervised domain adaptation through self-supervision.
arXivpreprintarXiv:1909.11825,2019. 3
[40] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,Alexei
Efros, and Moritz Hardt. Test-time training with self-
10Supplementary Materials
Model Method Adaptationtime Evaluationtime Totaltime
CoTTA 88.9(-38.9%) 8.66(-1.0%) 81.2(-41.3%)
MSGCHN
Summaryofcontents Ours-fast 136.6 8.8 145.4
CoTTA 717.5(+67.4%) 75.3(-10.9%) 792.8(+60.0%)
â€¢ InSectionA,wepresenttheGPUtimeofeachadaptation NLSPN BNAdapt 185.0(-20.8%) 82.8(-0.8%) 267.8(-15.6%)
Ours-fast 168.2(-28.0%) 83.4(-0.1%) 251.6(-20.66%)
methodtoshowtheeffectivenessofourmethod.
Ours 233.6 83.5 317.1
â€¢ InSectionB,wepresentthepreliminaryobservationswith
CoTTA 329.1(+78.2%) 33.6(-51.0%) 369.1(+43.2%)
imageandrangeinputsofvaryingsparsity. CostDCNet BNAdapt 82.1(-55.5%) 42.5(-37.9%) 125.6(-50.8%)
â€¢ InSectionC,wedescribethedatasetsused. Ours-fast 141.9(-23.2%) 68.7(+0.3%) 210.6(-16.8%)
Ours 184.7 68.5 253.2
â€¢ InSectionD,wepresentthehyperparametersettingsfor
resultreproductionandweelucidateevaluationdetails. Table 2. GPU time for various methods and models, tested on
â€¢ In Section E, we provide a study on the learned proxy Virtual KITTI. Time is in milliseconds (ms). â€˜Adaptation timeâ€™
embeddingwithavisualization. denotes the time required to adapt (or train) each method for a
â€¢ In Section F, we present an ablation study of the loss singletestdatapoint.â€˜Evaluationtimeâ€™denotesthetimetakento
componentsinourmethod. testeachmethodforatestdatainstance.â€˜Totaltimeâ€™isthesumof
â€¢ InSectionG,wepresenttheresultsonKITTIâ†’VKITTI theAdaptationandEvaluationtimes.
adaptation.
â€¢ InSectionH,wepresenttheresultsonadifferentsource
results in the main paper without the lidar input, such as
dataset(Waymoâ†’VKITTI).
thereâ€™snosparsepointtopropagatetothenearpixels. We
â€¢ InSectionI,weshowaqualitaviveresultoftheprelimi-
clarifythattheresultsareintendedtohighlightthedomain
naryobservation.
distrepancy. Therefore,weshowadditionalresultswith1%,
5%,and10%ofsparsepointsintherangeinputonindoor
A.Adaptationspeed
datasets, as shown in Table 3. As we increase the range
WecomparetheGPUtimeofouradaptationmethodwith points,theperformanceisimprovedyetstillworsethanthe
thebaselines(BNAdapt,CoTTA)onVKITTIinTable2. sparse-depth-onlyresultsinTab. 9.
Compared to CoTTA, our adaptation method does not
requiremultipleinferencestogetthepseudo-prediction(de- C.Datasets
rivedfromaveragingteachermodelpredictionswithdiffer-
KITTI [12] is composed of calibrated RGB images with
entRGBaugmentations)usedtoadaptthestudentmodel.
synchronizedpointcloudsfromVelodynelidar,inertial,and
Yet,ourmethodrequiresanadditionalcomputationforthe
GPS information, and from more than 61 driving scenes.
proxyembedding. Thus, theproxylayerâ€™ssizerelativeto
There are â‰ˆ80K raw image frames and associated sparse
the model size causes the adaptation time difference. For
depth maps, both with â‰ˆ5% density, available for depth
example,CoTTAreducedthetotaltimeby38.9%overOurs-
completion[41]. Semi-densedepthisavailableforthelower
fastonMSGCHN,whichisalight-weightdepthcompletion
30%oftheimagespace,and11neighboringrawlidarscans
model. Inthiscase,theproxylayerisrelativelylargerthan
comprisetheground-truthdepth. Wedidnotuseatestor
inothermodels,wheremultipleinferencesrequirelesscom-
validation set, and the training set contains â‰ˆ86K single
putation than the proxy layer. As a result, the total time
images.
isincreasedinMSGCHN.However,forlargemodels(NL-
VOID[46]containssynchronized640Ã—480RGBimages
SPN,CostDCNet),Oursreducedtotaltimeby56.6%over
andsparsedepthmapsfromindoorscenesoflaboratories
CoTTA;ourproxylayersizeisrelativelysmallerthanthe
andclassroomsandfromoutdoorscenesofgardens. Sparse
largemodels,whilestillimprovingperformanceby26.52%.
depthmaps(ofâ‰ˆ0.5%densityandcontainingâ‰ˆ1,500sparse
ComparedtoBNAdapt,ourmethodrequiresadditionalpa-
densepoints)areobtainedbytheVIOsystemXIVO[9],and
rametersfortheadaptationlayerandtheproxylayer. Hence,
denseground-truthdepthmapsareobtainedbyactivestereo.
our method is 38.18% slower in adaptation time, 19.36%
VOID uses rolling shutter to capture challenging 6 DoF
slower in evaluation time, and 33.16% in total. Yet, our
motionfor56sequences-asopposedtoKITTIâ€™stypically
methodimproveserrorsby15.67%overBNAdapt.
planar motion. We use a training set of â‰ˆ46K images to
preparethemodel.
B.Furtherobservationsonimage/rangeinputs
NYUv2[36]contains372Ksynchronized640Ã—480RGB
Wepresentadditionalpreliminaryobservationsoftheimage images and depth maps (via Microsoft Kinect) from 464
and range sensor inputs with varying sparsity. Since pre- indoorscenesofhousehold,office,andcommercialtypes.
viousworks[4,28]statethatthedepthcompletionmodel TogeneratesparsedepthmapsinthestyleofSLAM/VIO,
propagates the sparse depth to the dense depth guided by weusedtheHarriscornerdetector[14]tosampleâ‰ˆ1,500
imagefeatures,onecanraiseaquestiononourpreliminary points from the depth maps. We use a set of 654 test set
11Method MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet
Dataset VOIDâ†’NYUv2 VOIDâ†’ScanNet
MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Image+sparsedepth(1%) 1643.34 2177.71 602.17 858.19 809.36 1144.91 1597.41 2240.43 490.13 738.77 665.57 982.32
Image+sparsedepth(5%) 996.54 1599.14 379.45 638.55 427.69 736.23 809.38 1455.69 240.55 441.70 337.39 620.53
Image+sparsedepth(10%) 785.65 1376.93 327.41 591.99 339.31 622.75 581.93 1165.63 191.75 379.10 264.66 516.74
Sparsedepthonly 734.13 1046.28 237.47 402.47 147.76 354.57 211.86 444.62 162.29 276.29 88.25 205.46
Table3.Modelsensitivitytoinputmodalitieswithvaryingsparsity.
imagesforadaptation. Dataset LearningRate wsm wz wproxy InnerIter.
ScanNet[6]contains2.5millionimagesanddensedepth MSG-CHN
maps for 1,513 indoor scenes. To generate sparse depth VKITTI 2e-3 1.0 1.0 0.2 1
mapsinthestyleofSLAM/VIO,weusedtheHarriscorner VKITTI-FOG 5e-3 3.0 1.0 0.1 1
detector[14]tosampleâ‰ˆ1,500pointsfromthedepthmaps. nuScenes 3e-3 9.0 1.0 0.2 1
Weuseasetofâ‰ˆ21Ktestimagesforadaptation. SceneNet 2e-3 8.0 1.0 0.1 3
Virtual KITTI (VKITTI) [10] contains â‰ˆ17K NYUv2 2e-4 0.8 1.0 0.4 3
1242Ã—375imagesfrom35syntheticvideoscreatedbyap- ScanNet 5e-3 8.0 1.0 0.3 3
plying7variationsinweather,lighting,orcameraangleto NLSPN
each of 5 cloned KITTI [41] videos. There exists a large VKITTI 2e-3 0.8 1.0 0.4 1
VKITTI-FOG 1e-3 1.0 1.0 0.2 1
domaingapbetweenRGBimagesfromVKITTIandKITTI,
nuScenes 1e-3 1.0 1.0 0.1 1
eventhoughthevirtualworldscreatedinUnityby[10]are
SceneNet 2e-3 0.7 1.0 2.0 3
similartoKITTIscenes. Thus,weonlyusethedensedepth
NYUv2 4e-3 5.0 1.0 1.0 3
maps of VKITTI to avoid the domain gap in photometric
ScanNet 1e-4 2.0 1.0 0.3 3
varations. Thesparsedepthmapsareobtainedbysimulating
CostDCNet
KITTIâ€™s lidar-generated sparse depth measurements such
VKITTI 4e-3 4.5 1.0 0.1 1
that the marginal distribution of VKITTIâ€™s sparse points
VKITTI-FOG 5e-3 3.0 1.0 0.04 1
mimicsthatofKITTIâ€™s. Weuseasetofâ‰ˆ2,300testimages
nuScenes 5e-3 3.0 1.0 0.1 1
fortheadaptation.
SceneNet 7e-3 2.0 1.0 0.2 3
nuScenes[1]consistsof1600Ã—900calibratedRGBim-
NYUv2 6e-3 4.0 1.0 0.1 3
agesandsynchronizedsparsepointclouds,27.4Kimages ScanNet 3e-3 1.0 1.0 0.2 3
from1000outdoordrivingscenesfortraining,and5.8Kim-
agesfrom150scenesfortesting. Wesetupthegroundtruth Table4.Hyperparameters.ForMSG-CHN,NLSPN,andCostDC-
forthetestimagesbymergingprojectedsparsedepthfrom Netmethodsformeta-initialization,preparation,andadaptation.
forward-backwardframes. Thesetupcodewillbereleased
toclarifyfurtherdetailsandreproducibility.
at 10Hz. Objects are annotated across the full 360â—¦ field.
SceneNet[26]contains5million320Ã—240RGBimages
We obtain our validation set by sampling from the whole
and depth maps from indoor trajectories of randomly ar-
validationdatasetevery0.6seconds.Rangesensorinputsare
ranged rooms. We use a single split (out of 17 available)
obtainedbyprojectingthetoplidarâ€™spointcloudscantothe
containing1000subsequencesof300imageseach,gener-
cameraframe. Weobtainedthegroundtruthbyprojecting
atedbyrecordingthesamesceneoveratrajectory. Because
10 forward and backward frames from front lidar and top
therearenosparsedepthmapsprovided,wesampledfrom
lidar to the image frame, which approximately counts for
thedepthmapviaHarriscornerdetector[14]tomimicthe
1secondofcapture. Toassumethatthereprojectedscenes
sparsedepthproducedbySLAM/VIO.Thefinal375corners
are static, we removed the moving objects in the scenes
areobtainedbyusingk-meanstosubsampletheresulting
usingobjectannotations. Also,outlierremovalisutilized
points,representing0.49%ofthetotalpixels. Weuseaset
forfilteringouterrorenousdepthpoints.
ofâ‰ˆ2,300testimagesforadaptation.
WaymoOpenDataset[38]contains1920Ã—1280RGB
D.Implemetationdetails
images and lidar scans from autonomous vehicles. The
training set contains â‰ˆ158K images from 798 scenes and Hyperparameter. Wespecificallynotethehyperparameters
thevalidationsetâ‰ˆ40Kimagesfrom202scenes,collected of three methods for meta-initialization, preparation, and
12KITTIâ†’Waymo KITTIâ†’VKITTI-FOG KITTIâ†’nuScenes
Method â„“z â„“sm â„“ proxy MAE RMSE MAE RMSE MAE RMSE
âœ“ 951.25Â±3.14 3512.07Â±6.40 978.84Â±3.36 3561.40Â±15.48 3164.46Â±11.32 6453.54Â±17.31
MSG-CHN âœ“ âœ“ 613.01Â±1.99 1935.43Â±9.14 732.61Â±6.02 3113.11Â±21.78 2865.15Â±9.96 6144.48Â±24.14
âœ“ âœ“ 608.91Â±1.74 1921.83Â±2.54 728.24Â±3.73 3087.36Â±15.92 2834.08Â±17.64 6096.56Â±21.08
âœ“ 837.66Â±8.73 3668.94Â±25.90 715.86Â±26.36 3034.21Â±57.65 5076.83Â±53.85 9710.88Â±89.76
NLSPN âœ“ âœ“ 489.46Â±5.45 1613.66Â±30.04 705.14Â±16.86 3059.64Â±97.85 2783.61Â±159.62 6313.4Â±276.09
âœ“ âœ“ âœ“ 477.28Â±3.32 1598.64Â±18.95 686.91Â±22.14 2666.70Â±56.64 2589.25Â±59.03 6006.18Â±90.66
âœ“ 816.33Â±32.01 3431.96Â±55.34 807.62Â±69.12 3254.83Â±179.90 3135.11Â±81.76 7596.49Â±159.16
CostDCNet âœ“ âœ“ 469.52Â±2.54 1594.38Â±6.10 516.93Â±1.62 2751.21Â±17.42 2067.42Â±10.23 5487.85Â±37.21
âœ“ âœ“ âœ“ 466.44Â±1.63 1580.38Â±11.48 512.72Â±0.74 2735.01Â±3.53 2062.28Â±11.24 5509.96Â±23.41
VOIDâ†’NYUv2 VOIDâ†’SceneNet VOIDâ†’ScanNet
âœ“ 971.64Â±66.86 1291.45Â±45.67 242.11Â±4.24 491.48Â±10.49 462.95Â±34.84 659.9Â±37.93
MSG-CHN âœ“ âœ“ 1005.49Â±25.97 1329.76Â±25.01 194.60Â±3.64 425.16Â±10.58 330.20Â±48.46 503.73Â±57.14
âœ“ âœ“ âœ“ 699.60Â±6.00 1120.37Â±9.76 192.74Â±1.72 424.49Â±4.58 302.21Â±4.10 480.08Â±8.03
âœ“ 145.72Â±6.55 271.78Â±9.91 130.49Â±13.64 337.14Â±28.38 112.38Â±1.72 234.60Â±3.46
NLSPN âœ“ âœ“ 128.17Â±4.13 240.97Â±3.86 118.65Â±2.24 337.63Â±2.58 77.84Â±0.28 169.81Â±0.50
âœ“ âœ“ âœ“ 124.41Â±2.27 240.73Â±5.72 113.93Â±1.49 333.41Â±4.32 74.77Â±0.31 166.61Â±0.45
âœ“ 152.43Â±13.07 432.20Â±54.51 213.4Â±19.52 597.22Â±49.78 91.13Â±1.40 286.17Â±9.07
CostDCNet âœ“ âœ“ 101.31Â±1.67 217.77Â±6.00 134.51Â±4.23 360.33Â±9.67 69.02Â±0.51 164.90Â±2.38
âœ“ âœ“ âœ“ 95.87Â±2.16 203.83Â±4.72 125.75Â±1.93 357.12Â±4.13 68.17Â±0.44 162.35Â±1.12
Table5.Ablationstudyofeachlossterm.NotethatNLSPNandCostDCNetupdatetheadaptationlayerandbatchnormalizationlayers,yet
MSGCHNonlyupdatestheadaptationlayer.
adaptationonTable4.
Metric Definition
Epochs and training details Adaptation occurs in a sin- MAE |â„¦1 |(cid:80) xâˆˆâ„¦|dË†(x)âˆ’dgt(x)|
(g il ne nee rp -o itc eh r, )w spit eh ciâ€˜ fith ede in nu Tm ab be .r 4o .f Di ute rr ia nt gio mns etp ae -ir nid ta iata lizp ao tii on ntâ€™ RMSE (cid:0) |â„¦1 |(cid:80) xâˆˆâ„¦|dË†(x)âˆ’dgt(x)|2(cid:1)1/2
andpreparationstages,themeta-andproxylayersaretrained
Table6.Errormetrics.d denotestheground-truthdepth.
for6epochs. Batchsizesforallmethodsare: 48forprepara- gt
tionstage,16formeta-initializationandadaptationstages,
withtheexceptionofScanNet[6],usingabatchsizeof36.
Topreventcollapseduringpreparationstage,wefollowthe
protocolof[13];weexploittheprojection/predictionlayers
anddivideonline/targetbranch,andupdatetargetprojec-
tionlayerwithexponentialmovingaverageofonlinebranch.
Weusedembeddingdimensionandhiddendimensionof512
forMSGCHN,and1024forCostDCNetandNLSPN.The
learningratesformeta-initializationandpreparationstage
willbereleasedwiththecoderelease.
Evaluation. Weevaluateouradaptationmodelsonbottom- Figure6.t-SNEplotoflearnedembeddingsonVOIDandNYUv2.
cropped regions in the outdoor dataset, where the sparse
depthexists. Foroutdoordataset,modelsareevaluatedon
E.Discussiononlearnedproxyembeddings
thebottomcroppedregionofthetestsplit,1242Ã—240for
Virtual KITTI, and 1600Ã—544 for nuScenes. For indoor
Here,weprovidethet-SNEvisualizationofimage&sparse
dataset,weevaluatedthemodelsontheentireregion. The
depth and proxy embedding from source and target. Fig.
definitionoftheerrormetricsinevaluationaredescribedin
6 shows the embeddings visualized by t-SNE, where the
Table6. Weevaluateourmodelondepthrangefrom0.0to
targetdomainproxyembeddingsâ€™centroidisclosertothat
80.0 meters for the ourdoor, and 0.2 to 5.0 meters for the
of sourceâ€™s proxy and image & sparse depth embeddings,
indooor.
than to the centroid of targetâ€™s image & sparse depth em-
CodereleaseWewillreleasethecode,trainingscripts,and beddings,highlightingeffectivenessofproxyembeddingfor
trainedmodelswithmetalayerandproxylayers. adaptation.
13KITTIâ†’VKITTI Waymoâ†’VKITTI-FOG
Method MAE RMSE Method MAE RMSE
Pretrained 2433.46 6675.16 Pretrained 1473.14 4676.19
CoTTA 839.19Â±12.78 3625.38Â±39.35 MSG-CHN CoTTA 1348.02Â±38.03 4016.67Â±28.16
MSG-CHN Ours-fast 800.88Â±1.86 3268.26Â±4.12 Ours-fast 1052.78Â±5.74 3891.05Â±17.34
Pretrained 1469.19 8060.97 Pretrained 2734.27 37621.10
BNAdapt 1016.87Â±8.84 3453.00Â±3.21 NLSPN BNAdapt,â„“ z,â„“ sm 1205.96Â±40.14 3857.88Â±101.15
CoTTA 2485.66Â±18.05 6307.96Â±48.64
BNAdapt,â„“ ,â„“ 855.12Â±14.56 3516.85Â±58.63
NLSPN z sm Ours 808.16Â±7.86 3536.58Â±91.15
CoTTA 775.09Â±3.63 3585.37Â±13.31
Ours-fast 849.43Â±3.61 3540.44Â±3.57 Pretrained 1261.00 4360.37
Ours 639.19Â±5.68 2934.36Â±33.80
CostDCNet
BNAdapt,â„“ z,â„“
sm
742.99Â±2.17 3403.00Â±3.62
CoTTA 1150.16Â±5.69 4134.16Â±9.15
Pretrained 845.35 3774.01
Ours 724.77Â±5.18 3349.21Â±29.00
BNAdapt 1248.35Â±0.25 4267.64Â±0.62
BNAdapt,â„“ ,â„“ 1016.87Â±8.84 3453.00Â±3.21
z sm
CostDCNet CoTTA 698.42Â±9.93 3324.59Â±30.21 Table8.Additionalresultsfortest-timeadaptationfordepthcom-
Ours-fast 822.49Â±13.55 3331.24Â±55.30 pletiononWaymoâ†’VKITTI-FOG.
Ours 639.91Â±8.92 2951.21Â±30.93 G.KITTIâ†’VKITTIresults
Here, we present additional results on KITTI â†’ VKITTI
Table7.Additionalresultsfortest-timeadaptationfordepthcom-
pletiononKITTIâ†’VKITTI. adaptation.Test-timeadaptationresultsareshowninTable7.
Consistentwiththetrendsobservedinthemainpaper,our
methodoutperformsoverbothBNAdaptandCoTTA,with
a21.82%improvementcomparedtoBNAdaptand12.6%
F.Ablationstudy improvementoverCoTTA.
H.Experimentwithdifferentsourcedataset
Here, we ablate the effect of each loss term denoted with
Inourmainpaper,theonlysourcedatasetforoutdooradap-
thecheckmarksinTable5. Usingsparsedepthconsistency
tationscenariowasKITTIwhichisthemostpopularoutdoor
lossâ„“ (Eqn.4)alonecanimprovethepretrainedmodelas
z depthcompletiondataset. Tovalidateourmethodâ€™sappli-
itlearnstheshapesofthetestdomain. However, because
cability to models trained on diverse source datasets, we
ofthesparsity,thesupervisionsignalisweak,leadingthe
includeadditionalresultsfromadaptationscenariosusing
modeltoexhibitartifactsanddistortionsinthedepthmap.
amodeltrainedontheWaymodataset,asshowninTable
Includingalocalsmoothnesslossâ„“ (Eqn.5)mitigatesthis
sm 8. OurmethodshowsanimprovementoverCoTTAandBN
bypropagatingdepthtonearbyregions. However,without
Adaptby21.70%.
knowledgeof3Dshapescompatiblewiththesparsepoints,
A noteworthy observation from theWaymoadaptation
thewrongpredictionsaresometimespropagatedasinthe
results,whencomparedtotheKITTIâ†’VKITTI-fogresults
leftboundingboxregionfromRow1,Column4ofFig.4.
fromthemainpaper,isthattheadaptationresultofKITTI
Thebest-performingmethodemploystheproposedproxy
outperforms that of Waymo. This difference is caused by
embeddings as a regularizer to guide the adaptation layer
fromthedomaindiscrepanciesbetweenKITTIandVKITTI-
update. Astheproxymappingproducestest-timefeatures
fog datasets versus the domain gap between Waymo and
thatfollowthedistributionofthesourcedomain,minimizing
VKITTI-fog. For example, VKITTIâ€™s object appearances
ourproxyconsistencyloss(Eqn.7)implicitlyalignsthetest
and resolution (1226Ã—370 for KITTI, and 1242Ã—375 for
domainfeaturestothoseofthesourcedomainthatarecom-
VKITTI) are more akin to those in the KITTI dataset.
patiblewiththe3Dsceneobservedbythetest-timesparse
Conversely, theWaymodatasetfeatureshigherresolution
pointcloud.Notonlydoesthisimproveoverallperformance,
(1920Ã—1280)anddifferentobjectshapescomparedtoKITTI
but it also reduces standard deviation in error, which can
andVKITTI.Hence,theadaptationresultisinfluencedby
beinterpretedasanincreaseinthestabilityoftheadapta-
the extent of domain discrepancy between the source and
tion. WeshowqualitativecomparisonsagainstBNAdapt
targetdatasets.
in Fig. 4, where boxes highlight improvements by fixing
erroneouspropagationbylocalsmoothness(e.g.,bleeding
I.Quantitativepreliminaryresults
effect,whichisnotmitigatedbyusingimagegradientsas
guidance in Eqn. 5). Quantitatively, we improve over the Toprovideapreciseobservation,weprovidethequantitative
baseline by an average of 21.09% across all methods and resultsofmodelsensitivestudyinTab. 9.
datasets,demonstratingtheefficacyofourproxyembedding.
14Method MSG-CHN NLSPN CostDCNet MSG-CHN NLSPN CostDCNet
Dataset VOIDâ†’NYUv2 VOIDâ†’ScanNet
MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE
Imageonly 2072.78 2462.63 969.14 1228.44 1359.16 1619.40 2001.90 2451.681 899.41 1151.12 1216.17 1459.46
Sparsedepthonly 734.13 1046.28 237.47 402.47 147.76 354.57 211.86 444.62 162.29 276.29 88.25 205.46
Image+sparsedepth 1040.93 1528.98 387.36 704.66 189.10 446.71 316.646 698.633 232.332 431.199 144.311 458.692
Dataset KITTIâ†’Waymo KITTIâ†’nuScenes
Imageonly 12766.791 18324.83 18829.96 24495.73 13598.50 18376.15 11823.061 17244.44 15835.04 22613.78 12794.65 16744.15
Sparsedepthonly 861.13 2706.75 1290.28 3571.26 1210.93 3102.49 3943.97 7306.33 2540.58 6203.66 2996.28 6773.06
Image+sparsedepth 1103.33 2969.39 1173.26 3092.02 1084.18 2819.42 3331.82 6449.09 2656.61 6146.59 3064.72 6630.65
Table9. Modelsensitivitytoinputmodalities. Depthcompletionnetworkshaveahighrelianceonsparsedepthmodality. Performing
inferenceinanoveldomainwithouttheRGBimage,i.e.,usingjustsparsedepthasinput,canimproveoverusingbothdatamodalities.
15