RetAssist: Facilitating Vocabulary Learners with Generative
Images in Story Retelling Practices
QIAOYICHEN,
SunYat-senUniversity,China
SIYULIU,
SunYat-senUniversity,China
KAIHUIHUANG,
SunYat-senUniversity,China
XINGBOWANG,
CornellUniversity,UnitedStates
XIAOJUANMA, TheHongKongUniversityofScienceandTechnology,China
JUNKAIZHU,
GuangdongPolytechnicofIndustryandCommerce,China
ZHENHUIPENG‚àó,
SunYat-senUniversity,China
Readingandrepeatedlyretellingashortstoryisacommonandeffectiveapproachtolearningthemeanings
andusagesoftargetwords.However,learnersoftenstrugglewithcomprehending,recalling,andretellingthe
storycontextsofthesetargetwords.InspiredbytheCognitiveTheoryofMultimediaLearning,weproposea
computationalworkflowtogeneraterelevantimagespairedwithstories.Basedontheworkflow,wework
withlearnersandteacherstoiterativelydesignaninteractivevocabularylearningsystemnamedRetAssist.It
cangeneratesentence-levelimagesofastorytofacilitatetheunderstandingandrecallofthetargetwords
inthestoryretellingpractices.Ourwithin-subjectsstudy(N=24)showsthatcomparedtoabaselinesystem
withoutgenerativeimages,RetAssistsignificantlyimproveslearners‚Äôfluencyinexpressingwithtargetwords.
ParticipantsalsofeelthatRetAssisteasestheirlearningworkloadandismoreuseful. Wediscussinsightsinto
leveragingtext-to-imagegenerativemodelstosupportlearningtasks.
CCSConcepts:‚Ä¢Human-centeredcomputing‚ÜíInteractivesystemsandtools;EmpiricalstudiesinHCI.
AdditionalKeyWordsandPhrases:Vocabularylearning,storyretelling,imagegeneration
ACMReferenceFormat:
QiaoyiChen,SiyuLiu,KaihuiHuang,XingboWang,XiaojuanMa,JunkaiZhu,andZhenhuiPeng.2024.
RetAssist:FacilitatingVocabularyLearnerswithGenerativeImagesinStoryRetellingPractices.InDesigning
InteractiveSystemsConference(DIS‚Äô24),July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark.ACM,New
York,NY,USA,28pages.https://doi.org/10.1145/3643834.3661581
1 INTRODUCTION
Learningvocabularyinmeaningfulcontexts,suchasstoriesandimagesinlanguagelearningtext-
books,andvideoclipsfrommovies,isacommonandeffectivepracticeasitenablesdeepandactive
processingofvocabulary(e.g.,wordassociations,logic)[50].Human-ComputerInteraction(HCI)
researchershaveexploredvarioustechnologiestosupportvocabularylearnerswithmeaningful
contextsinvariouslearningactivities,e.g.,ViVoinwatchingvideos[74],VocabEncounter inreading
onlinearticles[3],andEnglishBot inconversingwithothers[61].Inthispaper,wefocusonthe
storyretellingactivitythatencouragesEnglish-as-the-Second-Languagevocabularylearnersto
integrate,reconstruct,anddemonstratethecontextualizeduseofthetargetwordsinashortstory
‚àóCorrespondingauthor.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.
DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
¬©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ACMISBN979-8-4007-0583-0/24/07...$15.00
https://doi.org/10.1145/3643834.3661581
1
4202
yaM
32
]CH.sc[
1v49741.5042:viXraDIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[23,35,42,44].Thispracticetypicallyinvolvestwostages‚Äìstorycomprehensionandrepeated
retelling[47],i.e.,thelearnerfirstreadsorlistenstoashortstorythatcontainsasetoftargetwords
tocomprehenditsmainideaandthenverballyretellsitformultiplerounds.Severalstudieson
languageeducationhavedemonstratedtheeffectivenessofstoryretellingforvocabularylearning
[16,23,43],especiallyinrememberingthemeaningsoftargetwordsandusingtheminverbal
expressions[16,61].Infact,storyretellinghasbeenincludedintheEnglishtestoftheCollege
EntranceExaminationinChina1.
However,thestoryretellingpracticeisoftenchallengingforlearnersofsecondlanguagevocab-
ulary. Foronething,inthestorycomprehensionstage,learnersneedtoassociatethemeaningsof
targetwordswiththestorycontextandmemorizethestoryflowforthelaterrepeatedretelling
practice[33,44,46].Foranother,intherepeatedretellingstage,theyshouldrepeatedlynarratethe
readstorywithrequirementsonthecorrectusageoftargetwordsandfluencyinspeakingthem
outinthestory[33,44,46].Inotherwords,itrequireslearnerstounderstand,memorize,recall,
organize,andspeakthetargetwordsandassociatedstory[33].Thisbecomesmorechallenging
whenthereisatimelimitforeachroundofrepeatedretelling,whichcouldhelptodeveloplanguage
fluencyunderpressure[46]. Imagesrelatedtothestorycanhelpvocabularylearnerscopewith
thesetwochallengesduringthestoryretellingpractice.AssuggestedbytheCognitiveTheoryof
MultimediaLearning(CTML)[53],buildingmentalrepresentationsfromtextandvisualelements
couldfacilitatecomprehensionandrecallofwordsandtheircontextualizedusage[19,41,49,65].
Inthecontextofsecondlanguageacquisition,individualstendtosubvocallyarticulatethetext
associatedwithvisualstimuliintheirnativelanguage[52].Thus,comparedtolearningwithout
visualaids,non-verbalmodalitiessuchasimagesbridgethegapbetweentwodifferentlanguages,
whichwouldenhancethelikelihoodofrecallingthesecondlanguage[51,52]. Giventhesebenefits,
languageeducatorswidelypreparerelevantimagesforthetextualstoriesincoursebooksoronline
resources,andHCIresearchershaveproposedvocabularylearningsupportsystemsinactivities
thatinvolvevisualelements[1,29,74].Forexample,CoSpeak[1]usesvoicerecognitiontechniques
tosupportstudentstocollaborativelyandverballycreateastorygivenanimageprompt.However,
itistime-consumingandoftenunavailabletopreparerelevantimagesforthestorywithanysetof
targetwordsthatuserswishtolearninthestoryretellingpractices,whileirrelevantimageswould
confuselearnersandreducevocabularylearningoutcome[26].
Inthiswork,weexplorethedesignandusageofgenerativeimagestofacilitatethelearningof
anytargetwordsetviareadingandrepeatedlyretellingashortstorythatcontainsthesewords.Our
focusismotivatedbythebenefitsofimagesforvocabularylearningasdescribedaboveandrecent
advancesintext-to-imagegenerativetechniques.Forexample,thepre-trainedLatentDiffusion
Model(LDM)[60]isabletogeneratehigh-qualityandcontent-relevantimagesgivenatextprompt.
Thesegenerativetechniqueshavebeenusedtosupportthecreationsofartworks[21],medical
images[20],andgamecharacters[17]. Nevertheless,littlework,ifany,hasexploredgenerative
images for supporting vocabulary learning in the story retelling practices where users should
mastertargetwords‚Äômeaningsandverbalexpressions.Questionsarisesuchas1)whetherandhow
text-to-imagegenerativetechniquescangeneraterelevantimagesofanystorythatcoversatarget
wordset,2)ifso,whatkindsofsupportthatthegenerativeimagescanofferinthestoryretelling
practices,and3)howwouldthesupportfromgenerativeimagesimpacttheusers‚Äôvocabulary
learningoutcomeandexperience.
To this end, we seek to provide insights into these questions by designing, developing, and
evaluatinganintelligentsystemprototype,RetAssist,thatcangeneraterelevantimagesforlearning
vocabularyinthestoryretellingpractices.Here,wetargetEnglish-as-the-Second-Language(ESL)
1https://gaokao.eol.cn/guang_dong/dongtai/201811/t20181101_1631228.shtml
2RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
Chineselearners,e.g.,high-schooloruniversitystudentsinChina.Wetakeaniterativedesign
approachwithinsightsfromeducationalliteratureandtheinvolvementofESLlearnersandEnglish
teachersinthisprocess.Wefirstdevelopatext-to-imagecomputationalworkflowandvalidateits
capabilityingeneratingaseriesofcoherentandrelevantsentence-levelimagesgivenanyshort
textualstorythatcontainsIELTS2targetwords. Wethenconductaninterviewstudywithseven
ESLlearnerstounderstandtheirchallengesandneedsinthestoryretellingpracticesandaskfor
theircommentsonthegenerativeimages.Basedontheinsightsfromtheinterviewsandeducational
literature,wedevelopaRetAssist prototypeandseekfeedbackfromanother18ESLlearnersand
twoEnglishteacherstorefineit.InthestoryretellingpracticewiththerefinedRetAssist,userscan
first readandlistentothestorywithgenerativeimagesalignedtoeachsentence.Then,during
eachroundofrepeatedretelling,userscanretellthestorybyviewingtheimages. Aftereachround,
userscanreviewtheirperformanceintheexpressionsoftargetwordsandre-readthestorywith
images.
Weconductawithin-subjectsstudywith24ESLvocabularylearnerstoevaluatetheimpactof
RetAssist‚Äôsfunctiononthegenerativeimageonthevocabularylearningoutcomeandexperience.
Theresultsshowthatcomparedwiththebaselinesystemwithoutgenerativeimages,participants
usingRetAssist significantlyoutperforminfluentlyusingthetargetwordsinverbalexpressions.
ParticipantsfavorthegenerativeimagesofRetAssist forreducinglearningworkloadandaiding
recallofthecontextualusageoftargetwordsinthestory. Basedonourfindings,wehighlightthe
valueoftext-to-imagegenerativetechniquesinofferingusefullearningmaterialsandenjoyable
learning experiences. We further discuss design considerations for future vocabulary learning
supportsystemsandtheimpactofourworkongenerativeAIsforeducation.
Ourworkmakesthreecontributions.First,wepresentavocabularylearningsystemRetAssist
thatusesgenerativeimagestofacilitateuserstomastertargetwords‚Äômeaningsandexpressionsvia
storyretellingpractices.Second,ourdesignandevaluationofRetAssist providefirst-handfindings
onthefeasibility,effectiveness,anduserexperienceofapplyingtext-to-imagegenerativemodels
tovocabularylearning.Third,weproposeastorytext-to-imagegenerationworkflowandoffer
designconsiderationsofleveraginggenerativemodelstosupportlearningtasks.
2 RELATEDWORK
Weintroducepriorstudiesthatmotivate,inspire,andsupportthedesignofRetAssist,including
storyretellingforvocabularylearning,vocabularylearningsystems,andtext-to-imagegeneration
techniques.
2.1 StoryRetellingforVocabularyLearning
Storyretellingisawell-recognizedapproachthathelpsstudentsacquirevocabularyandskills
likereading,listening,andspeakinginlanguagelearningandteaching[42,44,46,47].Astory
retellingpracticenormallyconsistsoftwostages,i.e.,storycomprehensioninwhichlearnerslisten
orreadagivenstory,andrepeatedretellinginwhichtheyspeakitoutforseveraltimeswithina
timelimit[46,47].AssuggestedbyNationetal.[46],itisapracticethatproperlyintegratesfour
typicalstrandsofactivities,i.e.,meaning-focusedinput,meaning-focusedoutput,language-focused
learning, and fluency development. First, in the story comprehension stage, learners focus on
understandingthegivenstorywithtargetwords‚Äìusinglanguagereceptively(meaning-focus
input)[46].Next,intherepeatedretellingstage,learnersarerequiredtocorrectlyandfluentlyspeak
thestoryout‚Äìusinglanguageproductively(meaning-focusoutput)[46].Moreover,inthewhole
2ShortforInternationalEnglishLanguageTestingSystem,agloballyrecognizedstandardizedtestdesignedtoassessthe
Englishlanguageproficiencyofindividuals.
3DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
practice,learnersshouldspecificallypayattentiontothemeanings,pronunciations,andcorrect
usagesofthetargetwords‚Äìdeliberatelearningoflanguagefeatures(language-focusedlearning)
[46].Lastly,thepracticerequireslearnerstomakethebestuseofwhattheyalreadyknowtoperform
wellinretellingundertimepressure[6,30,33,44],whichisatypicalfluencydevelopmentlearning
activity[46].Inall,storyretellingencourageslearnerstointegrate,reconstruct,anddemonstrate
thecontextualuseofvocabulary[23,42,44].Theexpectedlearningoutcomeis,therefore,notonly
onmemorizationoftargetwords‚Äômeaningsbutalsoonthecapacityofusingthewordscorrectly
andfluentlyinlanguageexpressions[16,22].
Giventheserequirementsofreading,interpreting,memorizing,andspeakingthestorywith
targetwords[33,44,46],storyretellingisoftenchallengingforlearners.Traditionally,thereare
additionalmaterials(e.g.,imagesandprops)tothetextualstoryandin-situguidancefromteachers
(e.g.,promptingphrases)toassistlearnersinthestoryretellingpractices[14,16]. Imagesrelevantto
thestory,forexample,arebeneficialinthattheycanhelplearnersrememberthewords‚Äômeanings
and comprehend the story [2, 19, 49]. Images can also serve as the visual guidance that helps
learners recall the story and target words when they get stuck in the repeated retelling stage
[68]. AccordingtoCognitiveTheoryofMultimediaLearning(CTML)[19,41,49,65]andDual
CodingTheory(DCT)[51,53],buildingmentalrepresentationsfromtextandvisualelementscould
enhancetheencodingandretentionofinformationbyleveragingdualcoding,whichtapsinto
bothverbalandvisualprocessingsystemsinthebrain.AstheextensionofDCT,BilingualDual
CodingTheory(BDCT)[52]suggeststhatimagesenhancesecondlanguagelearningsincelearners
covertlypronouncethecontentoftheimagesintheirnativelanguageandthecontentandthe
imagesconvergeontheforeignlanguageresponses,increasingtheprobabilityofrecallrelativeto
theconditionwithoutimages.Furthermore,Mayeridentifiedthetwelvemultimediainstructional
principlestoaddresstheissueofhowtostructuremultimediainstructionalpracticesandemploy
moreeffectivecognitivestrategiestohelppeoplelearnefficiently[41].Forinstance,thespatial
contiguityprinciple[41]indicatesthatpeoplelearnbetterwhencorrespondingwordsandimages
areplacedneareachotherratherthanfarfromeachotheronthepageorscreen. Insummary,
theseprinciplessuggestthatrelevantvisuals(e.g.,images)cansignificantlyaidinrecallingtextual
information(e.g.,storyinourcase)ofvocabularytofacilitatevocabularylearning.Despitethe
clearbenefits,selectingappropriateimagesthataligncloselywiththetextualcontentremainsa
considerablechallenge[27].
Ourworkismotivatedbythebenefitsofstoryretellingpracticesforenhancingunderstanding
andexpressionoftargetwordsandthehelpfulnessofimagesforassistingusersinthesepractices.
Insteadofrequiringhumanefforttopreparetheimages,weproposetogeneraterelevantimagesto
anystorythatcoversthetargetwordsthatuserswishtolearn.
2.2 VocabularyLearningSystems
ExistingHCIresearchershaveexploredvariousintelligentsystemstosupportvocabularylearning.
Broadlyspeaking,theyareeitherbasedonwordlistsormeaningfulcontexts.Theformertypeof
vocabularylearningsystemaimstofacilitatequickmemorizationoftargetwordsinalist.Previous
workhasincorporatedmodelsofusers‚Äômemorycyclesandindividualizedlearningstylesintothese
systems,suchthattheycanrecommendasetoftargetwordswithappropriatelevelsofdifficulty
andrepetitionfrequency[9,48,73].Forexample,Chenetal.[9]proposedapersonalizedmobile
EnglishvocabularylearningsystembasedonItemResponseTheoryandthelearningmemory
cycle.
Context-basedvocabularylearningsystemsleveragevariousformsofmaterialssuchasstories
[1],videos[74],andonlinearticles[3]tohelpuserslearnvocabulary.Forexample,VocabEncounter
[3]encapsulatestargetvocabularyintothecontextofonlinearticles,whileARLang[7]visualizes
4RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
bilinguallabelsonphysicalobjectsoutdoorsinARenvironmenttosupportthemicro-learning
oflanguagewithinitsspatialcontext.Additionally,EnglishBot [61],alanguagelearningchatbot,
engagesstudentsininteractiveconversationsoncollege-relatedtopicstolearnEnglish.Learners
canclickasneededtoreceiveanswerpromptsprovidedintheirnativelanguage,ensuringsmooth
conversationswithEnglishBot [61].InlinewithEnglishBot‚Äôsmethod,RetAssist allowsusersto
autonomouslyclickoncorrespondingimagesbasedontheircurrentprogresswhencomprehending
orretellingstories. Somestudiesuseimagesasvisualcontextstosupportvocabularylearning.
AIVAS[26]usesanimagererankingalgorithmtoselectimagesthatprominentlycontainrelevant
objectsinthemiddleground,thusaidinginrepresentingconcretenounseffectively.Furthermore,
FCAI [27]considersusers‚Äôpersonalinformation,learningtime,andlocationtorecommendcon-
textuallyappropriateimagesthatbestrepresentthetargetwords.BothAIVAS andFCAI focus
onsearchingforappropriateimagesfortargetwords,whereastheimagesinRetAssist needto
represent the story content, potentially favoring a generative approach. Story retelling also
facilitatescontextualizedvocabularylearning. CoSpeak[1]providesanapplicationforlearnersto
practicespeakingEnglishbypairingthemtogethertoco-createastorywithanimagepromptbased
ontheongoingtopicinclass.UnlikethefocusofourstudyonindividuallearnersusingRetAssist
forvocabularylearningthroughstoryretelling,CoSpeakconcentratesonenhancingEnglishoral
expressionthroughdialoguesbetweentwoindividualsinthematicstorysettings.
OurproposedRetAssist fallsintothecategoryofcontext-basedvocabularylearningsystems.
RetAssistnotonlyintegratesvocabularyintothestorytoprovidetextualcontext,butalsogenerates
asetofrelatedimagesforthestorythatserveasthevisualcontexttohelpindividualvocabulary
learnersacquirevocabularythroughstoryretelling.
2.3 Text-to-ImageGenerationTechniques
AssuggestedbytheDualCodingTheory[51,53],textualstoriespairedwithrelevantimagescan
facilitatevocabularylearning.Recentadvancesintext-to-imagegenerativetechniquesoffergreat
potentialforpreparingvisualaidsforanystorythatcoverslearners‚Äôinterestedwords.Text-to-
imagegenerativemodelsnormallytakeatextpromptasinputandoutputoneormultipleimages
that are related to the text content. One of the early representatives is Diffusion Probabilistic
Model(DM)[64],whichachievedstate-of-the-artresultsindensityestimation(i.e.,howwellthe
modelcapturestheprobabilitydistributionofthedataset)[32]aswellasinsamplequality(i.e.,
howwellthemodelgeneratesdatasamplesthatcloselyresemblerealdatafromthatdistribution)
[13]. DMsusedeeplearningtechniquestogeneratehigh-qualityimagesfromtextpromptsbut
havethedownsideoflowinferencespeed.Toaddressthedrawback,recentapproacheswidely
leverageLatentDiffusionModels(LDMs)[60],whichworkonacompressedlatentspaceoflower
dimensionalityandspeedupinferencewithalmostnoreductioninimagesynthesisquality.Inthis
work,weuseastate-of-the-artLDMforgeneratingimagesfromtextprompts.
Comparedtothetraditionaltext-to-imagetasksthatgenerateimagesfromatextprompt,image
sequencegenerationforstoriesismorechallengingasitneedstogenerateasequenceofcoherent
andconsistentimagesforastorythatcontainsmultiplesentences.Toenhancetheimagequality
andtheconsistencyofthegeneratedsequences,StoryGAN consistsofadeepContextEncoderthat
dynamicallytracksthestoryflow,andtwodiscriminatorsatthestoryandimagelevels[37].Neural
StoryboardArtist visualizesthestoryintheformofacomicstripthroughtheretrievalofmultiple
relatedimagesfromthestorycontentandseveralimagerenderingstepslikesegmentingrelevant
regionsandconvertingtheimagesintocartoonstyle[10]. Nevertheless,previousstoryimage
generationtechniquesprioritizecoherenceofthewholestoryflow,whichmayoverlookthecontexts
(e.g.,sentences)containingtargetwordsforlearning. Tofacilitatevocabularylearningbasedon
story retelling practices, we segment the whole story into sentences to provide rich contexts
5DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
fortargetwords.Thesentencesareusedaspromptstogenerateasequenceofrelevantimages.
Specifically,weuseaStableDiffusionmodel[63]toconvertsentencesintoimagesandapplyacross-
modalmodel,CLIP[57],toselectthemostrelevantoneforeachsentence.Toimprovethevisual
consistencyandcoherenceoftheimagesequence,wefollow[10]anduseastyletransfermodel
[11]tounifytheimageswithcartoonstyles.Thereafter,ourproposedcomputationalworkflow
cangeneraterelevantandstyle-consistentimagesforstorysentencesasmeaningfulcontextsto
facilitatestoryretellingforvocabularylearning.
3 DESIGNPROCESS
Fig.1. OurdesignanddevelopmentprocessofRetAssistwithEnglishteachersandESLlearners.
Inthissection,weexplainhowwedesignanddevelopRetAssist tofacilitatevocabularylearners
toreadandrepeatedlyretellanystorythatcoverstheirinterestedtargetwords(Figure1).First,
weproposeacomputationalworkflowfortext-to-imagegenerationandvalidateitsfeasibilityin
generatingaseriesofcoherentandrelevantsentence-levelimagesgivenanyshorttextualstory
thatcontainstargetwords.Then,weworkwithvocabularylearnersandteacherstoderivethe
designprinciplesofRetAssist.
3.1 DevelopingaComputationalWorkflowforText-to-ImageGeneration
Toassistusersinthestoryretellingpractices,thegenerativeimagesofastoryshouldsatisfythe
followingtworequirements.First,theimagesshouldbesemanticallyrelevanttothetextualstory.As
suggestedbytheDualCodingTheory[51,53],thebrainprocessesvisualandverbalinformationin
distinctregions.Thevisualchannelhandlesvisualdata,generatingpictorialrepresentations,while
6RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
theverbalchannelprocessesverbalinformation,producingcorrespondingverbalrepresentations.
Whenthevisualandverbalinputsaresemanticallyrelevant,peopleestablishmentalconnections
thatorganizeinformationintocause-and-effectchains[28].Aftertheseconnectionsareformed,
thereisasignificantenhancementintheabilitytorememberinformation[54].Therefore,when
servinglanguagelearning,thevisualinformationintheimageshouldsemanticallymatchwiththe
textcontent.Second,theimagesthemselvesshouldbecoherentintheircontentandconsistentin
stylestodepictastory[41].Otherwise,theimagescouldconfuselearnersinthestoryretelling
practices. Thesetworequirementsguideourdesignchoicesinthecomputationalworkflow,as
detailedbelow(Figure2).
Sub-step1:PreprocesstheStory.Wechoosetogenerateoneimageforeachstorysentence
fortworeasons.First,aseriesofimagesratherthanoneimagecanbetterrevealthelogicofthe
story[53].Second,theSegmentingPrincipal[41]suggeststhatpreparinganimageforeachstory
segmentcanprovidenaturalpausesforlearnerstoabsorbthecontentbeforeproceedingtothenext
segment[31].WeusetheSpacypackageinPythontosplitthestoryintosentences.Tomaintain
thecoherenceamongthegenerativeimages,wefurtherresolvecoreferencesinthestorysentences,
e.g., pronouns like ‚Äúhe‚Äù and ‚Äúit‚Äù refer to the objects mentioned earlier. Specifically, we adopt a
pretrainedcoreferenceresolutionmodelnamedNeuralCoref[71]toselectthereferencewordsin
thestorytoreplacethepronounineachsplitsentence.Forexample,fortheredtextinFigure3,
thepronounce‚Äúhe‚Äùinthesecondandfourthsentencesoftheexamplestoryisreplacedby‚Äúanold
man‚Äù.
Sub-step2:Text-to-ImageGeneration.Afterpreprocessingthestory,weproceedtogenerate
multiple images for each story sentence. Specifically, we leverage a state-of-the-art pretrained
text-to-imagegenerationmodelnamedStable-Diffusion-v1-5,releasedbyRunwayMLandavailable
intheHuggingFacemodelhub[63],becauseofitsdemonstratedcapabilitytogeneratehigh-quality
imagesrelevanttothetext[60]. Themodeloutputsfiveimagesgivenapreprocessedinputstory
sentence.
Sub-step3:Postprocessgenerativeimages.Withthecandidategenerativeimages,wefurther
selectandpolishthemostrelevantimageforeachstorysentence.Theselectionisbasedonthe
semanticsimilaritybetweenthesentenceanditscandidateimages.Specifically,weuseapretrained
cross-modalmodelnamedCLIP[57]toencodethesentenceandimageintovectorsandcompute
thecosinesimilarityoftheimage.Afterselectingtheimages(e.g.,A1-A4inFigure3)withthe
highestsimilarityscoreswiththestorysentences,weseektomitigatethepotentialinconsistencies
amongtheselectedimages,e.g.,thesamehumancharactermaybevisuallyrepresenteddifferently
Fig.2. Ourcomputationalworkflowofgeneratingrelevantimagesforstories.
7DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
Fig.3. Givensentencesofanexamplestoryasinput,wecompareimagesgeneratedbyourcomputational
workflowwiththosegeneratedbytwoalternatives.[Ours(sentence-level,sentence-based)]A1-A4:Images
generatedusingthepreprocessedsentencesasprompts.B1-B4:CartoonstylizationofA1-A4.[Alternative-2
(sentence-level,keyword-based)]C1-C4:Imagesgeneratedusingthekeywords(boldwordsinthepreprocessed
sentencesoftheexamplestory)correspondingtothepreprocessedsentencesasprompts.D1-D4:Cartoon
stylizationofC1-C4.[Alternative-1(story-level)]E:Imagesgeneratedusingtheentirestoryasaprompt.F:
CartoonstylizationofE.
8RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
acrossimages,suchasvariationsinhairorfacialdetails. Weadoptacartoon-styletransfermodel
[11]thatcanconverteachimagetomatchacartoonstylewhilemaintainingtheoriginalstructures,
textures,andbasiccolorsoftheimage(e.g.,B1-B4inFigure3).
3.2 EvaluatingtheFeasibilityofGenerativeImagesforStoryRetellingSupport
Atthisstage,wewouldliketocomparethequalityoftheimagesgeneratedbyourworkflowwith
thosegeneratedbyalternativeapproachesgiventhesameshortstory.Thisevaluationaimsat
validatingifthegenerativeimagesarerelevanttothestory,haveacceptablevisualquality,and
areperceivedashelpfulinhelpinglearnerscomprehendandrecallthestory.Wewillassessthe
effectivenessanduserexperienceofgenerativeimagesinstoryretellinginthelaterexperiments
withvocabularylearners.Inspiredbypriorworkontext-to-imagegeneration[34],story-related
imagesgeneration[37],andtheusageofimagesinstoryretellingpractices[16],wederivethe
followingevaluationmetrics:relevance(Theimagesarerelevanttothestorydescription),visual
quality(Theimagesareclosetotherealscene),perceivedeffectivenessinaidingcomprehension
(Theimagesarehelpfulifyouaregoingtodostorycomprehension),andperceivedeffectivenessin
aidingrecall(Theimagesarehelpfulifyouaregoingtodorepeatedretelling).Eachitemisratedon
astandardfive-pointLikertScale(1for‚ÄúStronglydisagree‚Äùand5for‚ÄúStronglyagree‚Äù).
3.2.1 Alternative approaches. We compare our computational workflow with two alternative
approaches for text-to-image generation. The first one, noted as Alternative-1, generates ten
imagesbydirectlyinputtingtheoriginalstorytotheStable-Diffusion-v1-5modelandthenselects
andstylizesthemostrelevantone(e.g.,FinFigure3)similartothesub-step3inourworkflow.
Alternative-1producesasingleimagefortheentirestoryrefertoCoSpeak[1],whichprovidesa
singleimagetoassisttwoEnglishlearnerstoco-createastorythroughdialogue. Thecomparison
withAlternative-1(i.e.,sentence-levelvs.story-level)aimsatcheckingifgeneratingaseriesof
sentence-levelimagescouldbemorehelpfulthangeneratingonestory-levelimage.Thesecond
approach,notedasAlternative-2,usesTextRanktoextractkeywords(e.g.,theboldonesinFigure3)
aspromptstotheStable-Diffusion-v1-5modeltogeneratefiveimages[38].Itthenselectsand
polishesthemostrelevantimage(D1-5inFigure3)foreachsentenceusingthesamepostprocess
methodsinourproposedworkflow.BycomparingourworkflowtoAlternative-2(i.e.,sentence-
basedvs.keyword-based),weaimtoexamineifthesentence-basedpromptwouldbebetterthan
thekeyword-basedprompt,asarelatedworksuggeststhatthesetwopromptswerecomparablein
text-to-imagegenerationtasks[38].
3.2.2 Preparingtargetwordsetsandshortstories. Weprepare20shortstories,eachcontaining
agiventargetwordset,tocomparetheimagesgeneratedbyourproposedworkflowwiththose
generatedbyalternativeapproaches.Thetargetwordsarefromthevocabularypool(3,672wordsin
total)suggestedbytheInternationalEnglishLanguageTestingSystem(IELTS)[12].Threeauthors
ofthispaperrandomlyselectnon-easyIELTSwords(e.g.,notthewordslike‚Äúeasy‚Äùand‚Äúgeneral‚Äù)
thattheydidnotknowbefore,whicharerandomlyassignedto20sets,eachwithsixorsevenwords.
Thismanipulationsimulatesthecaseinwhichlearnerswouldliketolearnanyinterestedtarget
wordsetviastoryretelling.Toprepareastoryforeachtargetwordset,wefirstqueryChatGPT[5]
with‚Äúgenerateashortstorythathasnomorethan60wordsandmustcontainthewords‚Äò[word1]‚Äô,
‚Äò[word2]‚Äô,...,and‚Äò[wordn]‚Äô‚Äù.Thisapproachleveragesthecapabilityoftherecentlargelanguage
modelstogenerateashortstorythatcontainsanytargetwordset[55].Comparedtousingexisting
shortstoriesvalidatedbyEnglishteachers,storiesgeneratedbyChatGPTcanbeflexiblyadapted
tolearners‚Äôneedsandinterestsonmasteringanytargetwords. Thefirstauthorthenrefinesthe
generatedstoriestoimprovetheirreadability.Finally,weget20shortstories(averagewordlength:
9DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
60,averagenumberofsentences:5)thatcovertopicslikefunnyanimals,disasters,everydaylife,
andtravel.
3.2.3 ProcedureandResults.
Vs. Alternative-1. We prepare a document that lists the 20 stories; following each, there is a
series of images generated by our workflow, an image generated by Alternative-1, and spaces
forraterstoinputtheirscoresforeachmetric.Wedistributethisdocumenttofivehumanraters
(3 males, 2 females, age: ùëÄùëíùëéùëõ = 20.6,ùëÜùê∑ = 0.49) recruited from a local university. For each
metric of the generative image(s) for a story, we average the scores of five raters as the final
score.Next,weusepaired-sampleWilcoxonsignedrankteststoanalyzethedifferencesbetween
ourworkflowandAlternative-1oneachmetric.AsdepictedinFigure4,ourworkflowperforms
significantlybetteringeneratingrelevantimage(s)tothestorythantheAlternative-1(ùëù <0.05,ùëß =
2.023,Cohen‚Äôsd=1.208).Ratersalsoperceivethatourgenerativeimagesaresignificantlymore
effectiveinaidingcomprehension(ùëù <0.05,ùëß =2.023,Cohen‚Äôsd=1.417)andrecallofthestory
(ùëù <0.05,ùëß =2.023,Cohen‚Äôsd=1.537). Theseresultsindicatethatgeneratingaseriesofsentence-
levelimagesaboutastorycouldbemorehelpfulinstoryretellingthangeneratingonestory-level
image.
Fig.4. MeansandStandardErrorsofhumanratings Fig. 5. Means and Standard Errors of human rat-
onthequalityofgenerativeimages;1/5-stronglydis- ings on the quality of generative images; 1/5 -
agree/agree;*:ùëù<.05usingpairedsamplesWilcoxon strongly disagree/agree; *: ùëù < .05 using paired
signedranktests.WecompareAlternative-1(story- samples Wilcoxon signed rank tests. We com-
level)withOurs(sentence-level)ontheimages‚Äôrel- pare Alternative-2 (keyword-based) with Ours
evance(R)tothestory,visualquality(VQ),andef- (sentence-based)ontheimages‚Äôrelevance(R)tothe
fectivenessinaidingstorycomprehension(E-1)and story,visualquality(VQ),andeffectivenessinaiding
recall(E-2). storycomprehension(E-1)andrecall(E-2).
Vs.Alternative-2.SimilartotheprocedureincomparingwithAlternative-1,werecruitanother
five human raters (3 males, 2 females, age: ùëÄùëíùëéùëõ = 20.4,ùëÜùê∑ = 0.27) from the local university
toscoretheimagesgeneratedbyourworkflowandAlternative-2oneachmetricandconduct
paired-sampleWilcoxonsignedranktests. Theorderofencounteringimagesofeachstoryin
theratingdocumentisrandomizedandblindtotheraters.AsshowninFigure5,comparedwith
theAlternative-2,imagesgeneratedbyourworkflowaresignificantlymorerelevanttothestory
(ùëù < 0.05,ùëß = 2.023,Cohen‚Äôsd = 1.809)andareperceivedsignificantlymoreeffectiveinaiding
comprehension(ùëù <0.05,ùëß =2.032,Cohen‚Äôsd=0.872)andrecall(ùëù <0.05,ùëß =2.032,Cohen‚Äôsd=
1.06)ofthestory. Theseresultsindicatethatgeneratingaseriesofsentence-levelimagesabouta
10RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
storyusingthesentence-basedpromptcouldbemorehelpfulinstoryretellingthangeneratingthe
imageseriesusingthekeyword-basedprompt.
Tosumup,theresultsoftheevaluationstudysupportourchoicestogeneratesentence-level
imagesusingsentence-basedprompts.Themeansofthefourmetricsontheimagesgeneratedby
ourcomputationalworkflowarealllargerthanorequalto4outof5points,indicatingitsfeasibility
forgeneratingimagesthatarerelevanttothestory,ofhighvisualquality,andpotentiallyhelpful
tosupportstoryretelling.Wethenproceedtoexplorehowourgenerativeimagescanbeusedto
supportvocabularylearnersintheirstoryretellingpractices.
3.3 ExploringDesignPrinciplesofRetAssist
Withourcomputationalworkflowfortext-to-imagegenerationasthebackboneofRetAssist,we
workwithvocabularylearnersandteacherstoderivedesignprinciplesofRetAssist.
3.3.1 Processofexploringdesignprinciples. Toputforwarddesignprinciplesonhowtobuilda
vocabularylearningsystemthatusesgenerativeimagesinstoryretellingpractices,wefirstconduct
a formative study with seven ESL (English-as-Second-Language) learners. Then, we develop a
workableprototypeofRetAssist.Next,weevaluatetheRetAssistprototypethroughawithin-subjects
studywith18ESLlearners.Accordingtouserfeedbackontheprototype,wepreparearevision
planonRetAssist andsolicitfeedbackfromtwoEnglishteachers.
FormativestudywithsevenESLlearners.Tounderstanduserneedsandrequirementsfora
systemthatprovidesgenerativeimagesinthestoryretellingpractices,weconductaformative
studywithsevenESLcollegestudents(S1-S7,1male,6females,age:ùëÄùëíùëéùëõ =20.57,ùëÜùê∑ =0.82)in
China.FocusingongatheringthefeedbackandsuggestionsofESLlearners,wedonotspecifically
balancetheorderofretellingwithandwithoutgenerativeimagesinthisinstance. Wefirstinvite
themtoconductonestoryretellingpracticewithgenerativeimages3andtheotherwithoutimages.
Then,weaskquestionsabouttheirperceptionsofthepracticesandtheirexpectationsforasystem
usinggenerativeimagestosupportstoryretelling.ThefindingshereunderpinDP1,DP2,DP4and
DP5inSection3.3.2.
Fig.6. Thestructuredstoryretellingpracticeflowwiththestorycomprehensionandrepeatedretelling
stagesinRetAssistandbaselinesystems.IntheevaluationofRetAssistprototype,thebaselinesystemdoes
nothavefeaturesofspeechtranscript,generativeimages,andfeedback.Intheuserstudyofthefinalversion
ofRetAssist,thebaselinesystemdoesnothavethegenerativeimagesbuthasotherfeatureslikeRetAssist.
PrototypeofRetAssist.Basedontheresultsoftheformativestudy,wedevelopaworkable
prototypeofRetAssist.Thisprototypestructurestheprocedureofstoryretellingpracticeasused
inthefinalversionofRetAssist (Figure6,detailedinSection4)buthasseveralfeaturesdifferent
fromthefinalversionofRetAssist.Forexample,thegenerativeimagesaresequentiallyfixedin
theinterfaceandarenotinteractive.InspiredbythestudyofGuetal.[24],thisprototypewill
3ThestoriesandimagesarelistedinaWordfileandcomefromthematerialsusedintheevaluationofourworkflowin
Section3.2.2.
11DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
promptthenextsentencethatmasksthekeywordwhenusersgetstuckforfivesecondsduringthe
repeatedretellingstage.Besides,aftereachroundofrepeatedretelling,thisprototypeprovides
feedbackabouttheincorrectuseoftargetwordsandtheassociatedsentencebutdoesnotprovide
thestoryandgenerativeimagesforreviewbeforethenextroundofretelling.Thesefeaturesare
discardedorrefinedinthefinalversionofRetAssist basedonthefeedbackfromESLlearnersand
Englishteachers,asdiscussedinSection3.3.2.
EvaluationoftheRetAssistprototype.ToprobeuserexperienceoftheRetAssist prototype
andfeedbacktoimproveit,weconductawithin-subjectsstudywith18ESLlearners(L1-L18,14
females,4males,age:ùëÄùëíùëéùëõ =20.56,ùëÜùê∑ =1.17).Thetaskandprocedurearesimilartothelateruser
studyofthefinalRetAssist (detailedinSection5.3),exceptthatwedonothavethepretestandthe
twoposttestsinthisstudy.Duringthewithin-subjectsstudy,wegettheirqualitativefeedbackon
howthefeaturesoftheRetAssist prototypeaffecttheirlearningprocess.WecompareourRetAssist
prototypeandabaselinesystemwithoutgenerativeimages,speechtranscription,adaptiveprompts,
andfeedbacktoexplorethenecessityofthesesystemfeatures.Consistentwiththeuserstudy
offinalRetAssist,wecounterbalancetheorderoftheusedsystemsandencounteredwordsets
usingLatinSquare. Afterthelearningsessions,weaskabouttheirexperienceinthestoryretelling
practices,theirperceptiontowardsthetwosystems,andsuggestionsforimprovement.Thefindings
hereunderpinDP1-DP5inSection3.3.2.
FeedbackfromtwoEnglishteachers.Basedontheuserfeedback,wepreparearevisionplan
inaPowerPointfilethatdrawspossibledesignsforfeaturesabouttheinteractionwithgenerative
images,promptsintheretellingstage,andfeedbackonuserperformance.Webringthisplanand
ourRetAssist prototypetotwoEnglishteachers(E1,female,age:27;E2,male,age:27)andaskfor
theircritiquesandsuggestions.ThefindingshereunderpinDP1-DP5inSection3.3.2.
3.3.2 Designprinciples. Wefinalizefivedesignprinciples(DPs)basedontheresultsfromthe
designprocess.
DP1:Inthestorycomprehensionstage,thesystemshouldprovidegenerativeimagesto
facilitateusersinunderstandingandrememberingthestoryline. Previouseducationalliter-
aturesuggeststhatimagesdepictingthestorycouldhelplearnerstounderstandstoriesefficiently
[19,49].Ourparticipantsintheformativestudyfavortheconditionwithimagesintheirstory
retellingpracticessincethegenerativeimagescanhelpthemquicklyandcorrectlyunderstand
thestory.‚ÄúTheassociatedpictureswiththestoryhelpmeunderstandandrememberthestoryline,
enablingmoreefficientstoryretellingpractices‚Äù(S7).Also,alllearnersintheevaluationstudy
oftheRetAssist prototypeexpressedtheirfavorforthegenerativeimages.‚ÄúIliketoincorporate
imagestounderstandthestory‚Äù(L8).BothEnglishteachersbelievethatgenerativeimagesare
practicalmaterialstopromotestorycomprehension.
DP2:Duringeachroundoftherepeatedretellingstage,thesystemshouldofferthe
generativeimagestohelpusersrecallthestorylineyetnotpromptthenextsentence
when users get stuck. As suggested by the Cognitive Theory of Multimedia Learning [53],
visual elements (e.g., figures) associated with the story can facilitate recall of words and their
contextualizedusage[19,49].TheESLlearnersparticipatinginboththeformativestudyandthe
evaluationstudyindicatethattheimagescanassistthemrecallthestoryandorganizetheirretelling
flow.‚ÄúIcaneasilyconnectthepicturesbacktothestoryplotwhenretellingthestory‚Äù(S2).‚ÄúI
connecttheimagesprovidedwiththestory,whichhelpsmereflectonthestorylineinashorttime‚Äù
(L3).PreviouslearningsupportsystemEnglishBot [61]offersChinesepromptstousersintheir
conversationswithachatbot,andourparticipantsintheformativestudyraisesimilarexpectations
thattheintendedsystemcouldprovidein-situpromptsaboutthestorywhentheygetstuckin
therepeatedretellingstage.‚ÄúRatherthanre-readingthefullstory,I‚Äôdliketogethintsfromthe
12RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
systemaboutwhat‚ÄôsnextwhenIgetstuckintheretelling‚Äù(S3).Nevertheless,asindicatedby
eightparticipantsintheevaluationstudyofRetAssist prototype,theproactivesentenceprompts
duringtherepeatedretellingstageofteninterrupttheirretellingprocessandmayresultintheir
dependenceonthepromptstofinishtheretelling.Additionally,E1andE2agreethatgenerative
imagescanpromoteusers‚Äôrecallintheretelling,whilesentencepromptsarenotnecessaryoreven
unhelpful.
DP3:Tohelpusersaligntheimagesandstorycontent,thesystemshouldenablethe
userstoselectandenlargeanimagewhilehighlightingtherelatedstorysentence. Asone
ofthetwelvemultimediainstructionalprinciples[41],thespatialcontiguityprinciplesuggeststhat
userscouldbemorefocusedonthelearningtaskswhenrelatedtextandimagearevisuallycloseto
eachother.WiththeRetAssist prototype,fivelearnersintheevaluationstudyalsosuggestthat
theimagesshouldalignwiththestorycontentinamoreclearway.‚ÄúIhavetoconsciouslyremind
myselftocombinetheimagestounderstandthetext.Showingalltheimagessimultaneouslyand
fixedlymakesitdifficulttofocusontextandimagesatthesametime‚Äù(L1).OurEnglishteachers
help us identify the proper design to visually align the images and story content. ‚ÄúInteraction
designfordisplayingimagesshouldstrikeabalancebetweenindividualimagesandtheoverall
narrative.Wecoulduseanimagesliderthathelpslearnersfocusononeimageatatimewhile
havinganoverviewoftheimagesequence‚Äù(E1).‚ÄúHighlightingthecorrespondingstorysentence
whentheusersenlargeoneoftheimagescouldbeanintuitiveway‚Äù(E2).
DP4: In the story retelling stage, the system should provide a speech transcription
functiontorecordtheusers‚Äôretellingcontentandhelpthemkeeptrackoftheirprogress.
Asasimilarfeaturewithpreviousretelling-basedEnglishlearningsystemslikeCoSpeak[1]and
EnglishBot [61],ourparticipantsintheformativestudyexpresstheirwishtocheckwhattheyjust
spokeintheretellingexercise.‚ÄúIwanttoseewhatIhavesaidsofarwhenretelling,whichcanhelp
meorganizewhatIwillsaynext‚Äù(S2).Ingeneral,allESLlearnersintheprototypeevaluationand
bothteachersfavorthecomponentofspeechtranscription.‚ÄúWithspeechtranscription,Icouldpay
attentiontothepronunciationswhenspeaking‚Äù(L14).
DP5:Aftereachroundofrepeatedretelling,tohelplearnersreviewtheirperformance,
thesystemshouldofferfeedbackontheincorrectusageoftargetwords,togetherwiththe
storyandgenerativeimages. Providingfeedbackonusers‚Äôtaskperformanceisacommonand
effectivefeatureinlearningsupporttoolslikeArgueTutor[69]andEnglishBot[61].Fiveparticipants
intheformativestudysuggestthattheywanttogetfeedbackontheirperformanceinpractice,e.g.,
aboutthecorrectnessofwords‚Äôexpressions.‚ÄúItwillbebetterifthesystemcouldindicatewhether
Iwasusingthetargetwordcorrectlyinmyretellingpractice,whichcanhelpmemakeprogress
inthenextretelling‚Äù(S1).Moreimportantly,eightparticipantsintheevaluationstudyaccount
thefeedbackfromRetAssist prototypefortheirperceivedimprovementinthelearningoutcome.
‚ÄúUnlikethebaselinesystem,RetAssist tellsmehowwellIdidinthelastexercise,whichhelpsme
recheckthetargetwords‚Äômeaningsandmakeprogressinthenextroundofretelling‚Äù(L11).E1and
E2concurontheroleofassessingthecorrectnessofsemanticusagethroughsimilaritymeasures
andagreethatitenableslearnerstoverifytheaccuracyoftheirsemanticexpressions. However,
intheevaluationstudy,sevenlearnerssuggestthatRetAssist wouldbetterpresentthefeedback
togetherwiththestoryandimages,sothattheycanbetterreviewtheirperformanceinthecurrent
roundofrepeatedretellingbeforeproceedingtothenextround.‚ÄúIhopetoreviewthestoryand
imagesagainbeforestartingthenextroundofretellingsinceithelpsmefillinsomeofthedetails
fortheretelling‚Äù(L13).E1andE2alsoagreethatthereviewofthestoryandimagesbetweentwo
roundsofrepeatedretellingishelpful.
13DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
4 RETASSISTSYSTEMDESIGNANDIMPLEMENTATION
Basedontheidentifieddesignprinciplesandproposedstorytext-to-imagegenerationworkflowin
thelastsection,wedevelopRetAssisttofacilitatevocabularylearnersinstoryretellingpractices.We
developRetAssistasawebappthatcanbeeasilyaccessedbylearnersontheircomputers.Asshown
inFigure6,inthestructuredprocedureofarepeatedretellingpractice,RetAssist providesusers
withgenerativeimagesalignedtothestorysentences(DP3)toassiststorycomprehension(DP1)
andrepeatedretelling(DP2),speechtranscriptionduringrepeatedretelling(DP4),andadaptive
feedbackaftereachroundofretellingpractice(DP5).Wedescribehowvocabularylearnerscanuse
RetAssist inastoryretellingpracticeasfollows.
Storycomprehension.Atthebeginningofastoryretellingpractice,usersneedtofirstacquaint
themselveswiththetargetwords‚Äômeaningsandreadthestorythatcontainsthetargetwords
(Figure7-A).Atthisstage,theycanlookupthebilingualdefinitionsandpronunciationofeach
targetwordintheleftpartoftheinterface(A1).Theycanreadthestorywiththetargetedwords
markedinbold(A2).Meanwhile,userscanclickthe‚ÄúPlay‚Äùbuttontolistentotheaudioofthestory
andclickthe‚ÄúTranslation‚ÄùicontocheckitsChinesemeanings(A2).Furthermore,userscanclick
eachimagepreviewtoswitchtheenlargedimage(A3).Sucha‚Äúsliding‚Äùinteractiondesignwiththe
imagescouldhelpusersfocusonprocessingoneimageatatimeandcouldbeengaging[66].Users
canalsoseeahighlightedsentenceinthestory(A2)thatcorrespondingtotheenlargedimage.
Suchadesignfollowsthespatialcontiguityprinciple,whichstatesthatuserscanlearnbetterwhen
relatedtextandimageareclosetoeachother[41].
Fig.7. UserinterfacedesignofRetAssist.(A)Inthestorycomprehensionstage,userscan1)checkthetarget
words‚Äômeanings,2)readthestory,and3)seetherelevantimagesforeachstorysentence.(B)Intherepeated
retellingstage,userscanretellthestorywith1)thetargetwords,2)theretellingtranscription,and3)the
generativeimages.(C)Aftereachroundofretelling,userscancheckfeedbackontheirperformanceand
review1)thetargetwordswithincorrectmarks,2)thestory,and3)thegenerativeimages.
Repeatedretelling.Aftercomprehendingthetargetwordsandassociatedstory,userscanclick
‚ÄúRetell‚Äùintheuppermenubartoproceedtotherepeatedretellingstage(Figure7-B).Theyneed
tocompletethreeroundsofretellingpracticeswithindecreasingtimelimits,e.g.,120,90,and60
secondsbasedonourtrialsintheformativestudyandevaluationstudyofRetAssistprototype.The
designofdecreasingtimelimitsinthelearningpracticescouldhelpusersdeveloplanguagefluency
14RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
[46].Userscanclick‚ÄúRetell‚Äùtostarteachroundofretellingtrials(B2).Astheyspeak,RetAssist
willtranscribetheirspeechinreal-timeusingChrome‚ÄôsspeechrecognitionAPI[59].Duringeach
roundofrepeatedretelling,userscanaccessthepronunciationanddefinitionoftargetwordsin
thewordlistanytimetheywant(B1).Thebackgroundcolorofthewordwillturn‚Äúblue‚Äùwhen
RetAssist detectsthattheuserspeaksit.Meanwhile,userscanswitchtheimagesliderandclick
eachgenerativeimagetoenlargeitwhenevertheywant(B3).Userscanstopthecurrentround
ofstoryretellingbyclicking‚ÄúRetelling‚Äùagain.Then,theycaneditthetranscribedsentencesto
correctspeechrecognitionerrorsinthetextboxiftheywant(B2).
Reviewaftereachroundofrepeatedretelling.Whenusersfinishoneroundofrepeated
retelling,theycanclickthe‚ÄúCheck‚Äùbutton(Figure7-B2)toviewRetAssist‚Äôsfeedbackonthetheir
performanceandreviewthestorymaterialwithgenerativeimages(C).Userscancheckwhich
targetwordshavebeencorrectlycontextualized(markedinblueinC1)andwhichwordsarenot
usedorincorrectlyusedintherepeatedretelling(markedinred).Theycanclickeachredwordto
viewitsmeaningsandtheassociatedsentencethattheuserspoke.Userscanalsoreadthestory
withthehighlightedsentencesthatcontainthetargetwordstheyincorrectlyuse(C2).Meanwhile,
theycanchecktheassociatedgenerativeimages(C3).Userscanclickthe‚ÄúRetell‚Äôbuttoninthe
upperbartostartthenextroundofrepeatedretelling.
Weusesemanticsimilaritytojudgewhethertheusercorrectlyusesthetargetwords,inspired
bythestudyofCaoetal.[8],whichverifiesthecorrectnessofmachinetranslationbychecking
semanticsimilaritybetweentheoriginalandthetranslatedsentences. Specifically,weconsidera
targetwordisnotcorrectlyusedifthespokensentencethatshouldcontainthiswordissemantically
differentfromtheoriginalstorysentencethatcontainsthisword[62].Wecalculatethesemantic
similarity(rangingfrom0to1)betweentheexpressionofeachtargetwordinthestoryandthatin
theusers‚Äôretelling.First,weidentifythesentencecontainingeachtargetwordintheuser‚Äôsretelling
andcalculatetheirsentenceembeddingsbySentence-BERT[58].Then,wecomputethecosine
similarity(rangingfrom0to1)betweenthisidentifiedsentenceandthecorrespondingsentence
fromtheoriginalstory.Iftheusermentionsthetargetwordinmultiplesentences,thesimilarity
isrecordedasthemaximumofsimilaritybetweenthemultiplesentencesandthecorresponding
sentencefromtheoriginalstory.Iftheuserdoesnotmentionthetargetword,thesimilarityis
recorded as 0. To decide the thresholds of similarity scores that differentiate the correct and
incorrectuseoftargetwords,threeauthorsmarkthecorrectness(i.e.,correctorincorrect)ofthe
wordusageineachsentenceoftherecordedretelledcontentoftheparticipantsinourformative
study.Aftermarking,weobtaintwosetsofsimilaritiesseparatelyrepresentingthecorrectuseof
wordmeaningsandtheincorrectuseofwordmeaningsbycalculatingthesemanticsimilarities
betweenstorysentencesandspokensentences. Finally,thethresholdisdeterminedtobe0.7based
ontheROCcurvefordifferentsimilarityscores[18].
5 USERSTUDY
ToevaluatehowthegenerativeimagesinRetAssistimpactusers‚Äôvocabularylearningoutcomeand
experienceinthestoryretellingpractices,weconductawithin-subjects(RetAssist vs.baseline)
studywith24ESL(English-as-the-Second-Language)universitystudentsinChina.Ourresearch
questionsare:
RQ1.HowwouldRetAssist‚Äôsgenerativeimagesaffectusers‚Äôlearningoutcomesregardingthe
retentionandverbalexpressionoftargetwordsintheirstoryretellingpractices?
RQ2. How would RetAssist‚Äôs generative images affect users‚Äô a) learning experience and b)
behaviorsintheirstoryretellingpractices?
RQ3.HowwouldusersperceivetheusefulnessofRetAssist‚Äôsgenerativeimagesintheirstory
retellingpractices?
15DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
Fig.8. Userinterfacedesignofthebaselinesystem.(A)Inthestorycomprehensionstage,userscan1)check
thetargetwords‚Äômeanings,and2)readthestory.(B)Intherepeatedretellingstage,userscanretellthestory
with1)thetargetwords,and2)theretellingtranscription.(C)Aftereachroundofretelling,userscancheck
feedbackontheirperformanceandreview1)thetargetwordswithincorrectmarks,and2)thestory.The
baselinesystemdiffersfromRetAssistinthatitdoesnotprovidegenerativeimages.
5.1 TheBaselineSystem
Thebaselinesystem(Figure8)supportsthesameuserworkflow(Figure6)instoryretellingpractices
asRetAssist.However,itdoesnotoffergenerativeimagesduringboththestorycomprehension
stageandtherepeatedretellingstage.Thebaselinesystemsimulatesthescenarioinwhichthe
user is required to learn target words via story retelling practices without generative images.
Specifically,thebaselinesystemoffersthewordlist(Figure8-A1)andstory(Figure8-A2)inthe
storycomprehensionstage,anditprovidesdecreasingtimelimitsaswellaswordlist(Figure8-B1)
andspeechtranscript(Figure8-B2)inusers‚Äôthreeroundsofretellingpractices.Also,userscan
checkadaptivefeedbackregardingtheaccuracyofthetargetwordusage(Figure8-C1)insuch
roundsandreviewthestorytext(Figure8-C2). Suchabaselinesystemsatisfiesalldesignprinciples
withouttheinvolvementofgenerativeimages,specificallyreferringtoDP4andDP5.Insummary,
theonlydifferencebetweenRetAssistandthebaselinesystemliesintheincorporationorexclusion
ofgenerativeimages,whileallotherfunctionalitiesarepresentinbothconditionstomeetusers‚Äô
demands.
5.2 Participants
Werecruit24undergraduatestudents(P1-24,15females,9males,meanage:20(SD=1.67))froma
universityinmainlandChinaviaapostinthesocialmedia.Theymajorinvariousdomainssuch
asComputerScience,Historiography,Philosophy,Physics,Finance,Literature,andInternational
Relations.Twenty-threeparticipantshavepassedthenationalEnglishexamCET-4inChina,with
anaveragescoreof575(SD=48.04)4.Seventeenparticipantsadditionallyhavepassedahigher-
levelnationalexamCET-6inChina(Meanscore:523(SD=48.47)).Noneofourparticipantshave
takentheIELTSexam.However,theyexhibitastronginterestinlearningtheirunknownIELTS
vocabularyviathestorytellingpractices(M=5.75,SD=1.05;1-notinterestedatall,7-very
interested).
5.3 ProcedureandTasks
Figure9showstheprocedureandtaskofouruserstudyconductedremotely.Following[3,55],on
Day0,participantsfillinaconsentformandabackgroundsurveyandtakeavocabularypretest.
Werandomlyselect4storiesfromthe20preparedstoriesmentionedinSection3.2.2asthelearning
materialsforallparticipants,eachcontainingsixorseventargetwords. Intotal,thepretestconsists
4710isthefullmarkofbothCET-4andCET-6,and425istheminimumscoretopasstheexams.
16RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
Fig.9. Procedureofthewithin-subjects(RetAssistvs.thebaselinesystem)userstudy.Ineachtask,participants
learntwosetsoftargetwordswitheithertheRetAssistorthebaselinesystem.
ofthe26targetwordsthatparticipantswilllearninourlearningsessions.Foreachtargetword
inthepretest,participantsarerequiredtoselectoneoptionfromfivechoices,includingonethat
givesthecorrectmeaningsofthewordinChinese,threedistractors,andan‚ÄúIdon‚Äôtknow‚Äùoption.
Accordingtotheresultsofthepretest,theaveragenumberofcorrectlychosenoptionsamong24
participantsis8.62.Inotherwords,onaverage,participantsdonotknowthemeaningsof17.38
wordspriortothelearningsessions. Weinformthemnottolearnthewordsthatappearinthe
pretestbeforethelearningsessions.
OnDay2,participantsfirstwatchourpre-recordedvideothatdescribesthelearningtaskand
introducestheinterfacesofRetAssist andbaselinesystemswithblindnames.Theythenusetheir
laptops to log in to our systems. Each participant has two learning sessions. In each session,
participantshavetwostoryretellingpracticeswitheitherRetAssist orbaselinesystemtolearntwo
targetwordsetsthattheyencounteredinthepretest.Basedonthepilotstudywithtwoparticipants,
weallocate30minutesforeachlearningsession. Aftereachlearningsession,participantsrate
theirengagement,enjoyment,taskworkload,andperceptionsofthesysteminaquestionnaire.In
thequestionnaire,wealsoaskthemtowritedownresponsestosomeshortquestionssoastomake
senseoftheratings. Additionally,theyneedtoconductanimmediateposttestthatexaminestheir
learningoutcomeonrememberingthetargetwords‚Äômeaningsandbeingabletoverballyusethem
toretellastory.Uponcompletionoftwolearningsessions,participantsfillinaquestionnairethat
asksthemtowritedowntheirpreferencesontheinterfaces,commentsonthegenerativeimages,
andsuggestionsforimprovingRetAssist. Wecounterbalancetheorderoftheusedsystemsand
wordsetsusingLatinSquare,i.e.,sixparticipantsexperience‚Äúset1and2withRetAssist ‚Äì>set3
and4withBaseline‚Äù,six‚Äúset1and2withBaseline‚Äì>set3and4withRetAssist‚Äù,six‚Äúset3and4
withRetAssist ‚Äì>set1and2withBaseline‚Äù,andtherestsix‚Äúset3and4withBaseline‚Äì>set1and
2withRetAssist‚Äù.
OnDay9,theyconductadelayedposttestthathasthesameformatastheimmediateposttestto
examinetheirretentionandverbalexpressionoftargetwordslearnedonDay2.Theprocedureon
Day2andDay9isvideo-andaudio-recordedforfurtherdataanalyses.Overall,eachparticipant
spendsapproximatelyonehourandahalfinourstudyandreceives80RMBascompensation.
17DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
5.4 Measurements
5.4.1 RQ1.Learningoutcomes. Wemeasureparticipants‚Äôvocabularylearningoutcomesthrough
performanceonanimmediateposttestrightaftereachlearningsessionandadelayedposttestone
weeklater.Specifically,bothposttestsincludeamultiple-choicequizandanexpressivetest.The
multiple-choicequizisthesameasthepretestthatrequiresuserstoselectoneoffiveoptionsthat
isthecorrectChinesemeaningofthetargetword.Wecalculatethenumberofcorrectanswersto
themultiple-choicequestionstocapturethelearningoutcomeonthemeaningsoftargetwords.
Intheexpressiontest,participantsneedtoverballyretelleachstoryintheirlearningsessions
basedonthestorysynopsisinChineseandthetargetwordset.AssuggestedbyourtwoEnglish
teachersinthedesignprocess,wechoosetopresentthesynopsisinsteadofpresentingnothingor
providingthefullChinesetranslationoftheoriginalstorytobalancethedifficultyoftheexpression
test.WeadaptthemarkingschemeoftheIELTSspeakingtest[12]buthaveafocusontheverbal
expressionsoftargetwords.WithconfirmationfromourtwoEnglishteachersinthedesignprocess,
foreachexpressiontestoftwostorieswithinalearningsession,wecapture:
‚Ä¢ Numberoftargetwordsused(range0-135).
‚Ä¢ Numberoftargetwordspronouncedcorrectly,i.e.,thenumberoftargetwordscorrectly
pronounced.
‚Ä¢ Numberoftargetwordsusedcorrectly,i.e.,thenumberoftargetwordsthathavebeenused
semanticallycorrectly.
‚Ä¢ Fluency, which is determined by the expression of individual clauses and the lag between
sentences,rangingfrom0to9onascalereferencedtotheIELTSmarkingscheme.
Threeauthorsofourresearchteamfirstindependentlyscoresixrandomlyselectedaudiosamples,
eachconsistingoftworetellingstoriesinalearningsession.Theythenmeetanddiscusstogether
withoneofourtwoEnglishteachers(male,age:29)torefinetheirratingscheme.Forexample,
tofocusontheusageandexpressionoftargetwords,theratingschemeexcludesfactorslikethe
participants‚Äôvolumeofvoice,intonation,oraccent.Thethreeauthorsthenapplytheratingscheme
toall192(24√ó2systems√ó2storiespersystem√ó2posttests)audiosamplesinashuffledorder.For
eachdimensionofthemeasuredperformanceontheverbalexpressionsoftargetwords,weaverage
thethreeauthors‚Äôscores(ICC=0.939)asthefinalscoreineachretellingstory.Foreachofthefirst
threedimensions,weaddthescoresoftwostorieswithinonelearningsessionasuserperformance
inverballyexpressingtargetwordslearnedinthatsession,whileforthelastdimensionoffluency,
weaveragethescoresofthetwostoriesasthefinalscore.
5.4.2 RQ2.Learningprocess. IneachlearningsessionwitheitherRetAssist orbaselinesystem,
wemeasureparticipants‚Äôengagementandenjoymentinthelearningprocessusingitemsadapted
from[69,72]:‚ÄúIwasabsorbedinusingthisinterfacetolearnvocabulary‚Äùand‚ÄúItisenjoyableto
learnvocabularywiththisinterface‚Äù.Besides,wemeasuretheperceivedtaskworkloadoflearning
sessionsusingitemsadaptedfromNASATaskLoadIndex[25](e.g.,‚ÄúIrequiremuchmentaland
perceptualactivitysuchasthinkingandrememberingintheprocessofthestoryretellingpractice‚Äù).
Inadditiontothequestionnairedata,wealsomeasurehowlearnersperformineachofthethree
roundsofrepeatedretellingineachpractice.Foreachroundofrepeatedretelling,wemeasure:1)
spenttime,i.e.,thetimeperiodbetweenclickingthe‚ÄúCheck‚Äùandthe‚ÄúRetell‚Äùbuttoninthisround
ofrepeatedretelling;2)performanceinpractice,i.e.,howwelluserscanretellthestorycontent,
reflectedonthesemanticsimilaritybetweenlearners‚Äôretoldcontentandoriginalstory(ranging
from0to1,detailedinReviewaftereachroundofrepeatedretellinginSection4).Foreach
5Ineachlearningsession,participantslearnvocabularybasedontwostories.Onecontains6targetwords,andtheother
contains7targetwords.Themaximumscoreforonelearningsessionistherefore6+7=13.
18RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
learningsessionwithtwostoryretellingpractices,weaveragethespenttimeintwopracticesas
themeantimespentinoneroundofrepeatedretellinginthatsession.Similarly,weaveragethe
semanticsimilarityscoresoftwopracticestorevealuserperformanceinoneroundofrepeated
retellingineachlearningsession.
5.4.3 RQ3. Perceptions towards RetAssist. For each system interface, we adapt the technology
acceptancemodel[67,70]tomeasuretheperceivedusefulness(fouritems,e.g.,‚ÄúIfindthevocabulary
learningsupportsystemusefulinmyvocabularylearningprocessbystoryretelling‚Äù;Cronbach‚Äôs
ùõº =0.921);easinesstouse(fouritems,e.g.,‚ÄúMyinteractionwiththevocabularylearningsupport
systemisclearandunderstandable‚Äù;ùõº =0.830);andintentiontouse(twoitems,e.g.,‚ÄúIintendtobea
heavyuserofthevocabularylearningsupportsystemwhenIwanttolearnvocabulary‚Äù;ùõº =0.901).
Weaveragetheratingsofmultiplequestionsasthefinalscoreforeachaspect.Allstatementsin
thequestionnairesareratedonastandard7-pointLikertScale,with1-stronglydisagreeand7-
stronglyagree.
Fig.10. RQ1resultsregardingthe Fig.11. RQ1resultsregardingthe Fig.12. RQ1resultsregardingthe
numberofcorrectchoicesontar- numberoftargetwordsusedinex- fluencyinexpression.‚ñ° :ùëù < .05
getwords‚Äômeanings.‚ñ° : ùëù < .05 pression(S1),thenumberoftarget fortimefactor(immediateposttest
fortimefactor(pretestvs.imme- wordspronouncedcorrectlyinex- vs. delayed posttest), ‚ñ≥ : ùëù <
diateposttestvs.delayedposttest) pression (S2), and the number of .05forsystemfactor(RetAssistvs.
usingrepeatedmeasuresANOVA targetwordsusedcorrectlyinex- Baseline)usingrepeatedmeasures
withBonferronipost-hoctest. pression(S3).‚ñ°:ùëù < .05fortime ANOVA.
factor(immediateposttestvs.de-
layedposttest)usingrepeatedmea-
suresANOVA.
6 ANALYSESANDRESULTS
Fortherateditems,wefirstconductasetofmixedANOVAteststocheckwhethertheorderof
systemusageorthelearnedwordsetsassociatedwiththesystemsaffectedourresults(orderand
wordsetsasbetween-subjects,systemsaswithin-subjects).Theresultsindicatethatneitherthe
maineffectsoftheorderandwordsetsnortheirinteractionwiththesystemsaresignificant. For
themeasurementsforRQ1,weperformtwo-way(timeandsystem)repeatedmeasuresANOVA
toaccountforthedependenciesintime.AsforthemeasurementsforRQ2andRQ3,weperform
Shapiro-Wilknormalitytestsbeforerunningallthepairedsamplest-tests.Ifthehypothesisthat
thedatasatisfiesanormaldistributionisrejected,weusepairedsamplesWilcoxonsignedrank
testsinstead.Asaresult,forthespenttimeandperformanceineachroundofrepeatedretelling,
weperformpaired-samplet-teststocomparetheRetAssist andthebaselinesystem.Fortherest
19DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
measures,weperformpairedsamplesWilcoxonsignedranktests.Additionally,twoauthorsconduct
opencodingonparticipants‚Äôcommentsandsuggestionsonbothvocabularylearningsystems.
Theyhavemultipleroundsofdiscussionsandfinallyreachanagreementonthecodes,whichare
incorporatedintothefollowingresultpresentation.
6.1 LearningOutcomes(RQ1)
6.1.1 Multiple-choiceQuiz. AsshowninFigure10,participantsdemonstratecomparableperfor-
mancewithRetAssist (ùëÄ = 12.792,ùëÜùê∑ = 0.644) andbaselinesystem (ùëÄ = 12.625,ùëÜùê∑ = 1.033)
regarding the number of correct answers to the multiple-choice questions in the immediate
posttest. In the delayed posttest, participants have better performance on average with RetAs-
sist (ùëÄ =12.167,ùëÜùê∑ =1.179)thanbaselinesystem(ùëÄ =11.667,ùëÜùê∑ =1.863)regardingthenumber
ofcorrectanswers.TheresultsofrepeatedmeasuresANOVAindicatethatneitherthesystemfactor
(RetAssist andBaseline)noritsinteractionwiththetimefactor(pretestvs.immediateposttest
vs.delayedposttest)significantlyaffectsparticipants‚Äôperformanceinthemultiple-choicequiz
(ùëù > 0.05).However,thetimefactorhassignificanteffectsonparticipants‚Äôperformanceinthe
multiple-choicequiz(ùëù <0.001,ùêπ =596.792,ùúÇ2 =0.912),andtheresultsoftheBonferronipost-hoc
test ensure the significant difference among the three quizzes (pretest vs. immediate posttest:
ùëù <0.001;pretestvs.delayedposttest:ùëù <0.001;immediateposttestvs.delayedposttest:ùëù <0.05).
6.1.2 ExpressionTest. AsshowninFigure11,participantsgenerallyperformwellinusingthetarget
words,pronouncingthemcorrectly,andusingthemcorrectlyintheimmediateexpressionposttest
afterthelearningsessionwitheitherRetAssist orbaselinesystem;ùëÄ >10andùëù >0.05inallthe
threedimensions.Thisfindingsuggeststhatthestoryretellingpractice,eitherwithorwithoutthe
involvementofgenerativeimages,isaneffectiveapproachtolearningtheverbalexpressionof
targetwordsintheshortterm.Inthedelayedposttestafteroneweekofthelearningsessions,the
userperformancewithbothsystemsnaturallydecreasescomparedtothatintheimmediateposttest.
WefindthatparticipantsareabletousemoretargetwordslearnedwithRetAssist (ùëÄ =8.96,ùëÜùê∑ =
3.77andusethemcorrectly (ùëÄ = 7.5,ùëÜùê∑ = 3.77) comparedtothebaselinesystem(usetarget
words:ùëÄ = 7.83,ùëÜùê∑ = 3.45,usethemcorrectly:ùëÄ = 6.375,ùëÜùê∑ = 3.89)inaverage.Theaverage
numberofcorrectlypronouncedtargetwordsisalsohigherinthelearningsessionwithRetAssist
(ùëÄ =7.875,ùëÜùê∑ =3.61)thanthatinthesessionwithbaselinesystem(ùëÄ =6.375,ùëÜùê∑ =3.89).With
therepeatedmeasuresANOVA,wefindthatneitherthesystemfactor(RetAssist andBaseline)nor
itsinteractionwiththetimefactor(immediateposttestvs.delayedposttest)significantlyaffects
thenumberoftargetwordsused(S1),thenumberoftargetwordspronouncedcorrectly(s2),and
thenumberoftargetwordsusedcorrectly(S3)inexpression(ùëù > 0.05).Amongtheexpression
measurementsofS1-S3,thetimefactorhassignificanteffects(S1:ùëù <0.001,ùêπ =49.127,ùúÇ2 =0.699;
S2:ùëù <0.001,ùêπ =43.381,ùúÇ2 =0.712;S3:ùëù <0.001,ùêπ =76.741,ùúÇ2 =0.827).
Asforthefluencyofparticipants‚ÄôspokenEnglishintheimmediateposttest(Figure12),par-
ticipants can tell the story significantly more fluently after the learning session with RetAssist
(ùëÄ = 6.604,ùëÜùê∑ = 0.935) thatthatwiththebaselinesystem (ùëÄ = 6.354,ùëÜùê∑ = 0.872).Similarly,
in the delayed posttest, participants‚Äô spoken English in telling the story with target words is
significantlymorefluentafterlearningwithRetAssist (ùëÄ = 6.229,ùëÜùê∑ = 0.901) comparedtothe
baseline system (ùëÄ = 5.688,ùëÜùê∑ = 0.966). The results of repeated measures ANOVA indicate
thatboththesystemfactor(RetAssist andBaseline)andthetimefactor(immediateposttestvs.
delayedposttest)significantlyaffectthefluencyofparticipants‚ÄôspokenEnglish(systemfactor:
ùëù <0.05,ùêπ =4.01,ùúÇ2 =0.041;timefactor:ùëù <0.001,ùêπ =21.552,ùúÇ2 =0.238). Theseresultssuggest
thatRetAssist‚Äôsgenerativeimagescansignificantlyimprovethelearners‚Äôfluencyinusing
20RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
targetwordstotellastoryafterthestoryretellingpracticescomparedtothebaseline
system.
Fig.13. RQ2resultsregardingengagement,enjoyment,andworkloadinvocabularylearningsessions.‚àó:ùëù <
0.05usingpairedsamplesWilcoxonsignedranktests.
6.2 LearningProcess(RQ2)
6.2.1 Engagement,enjoymentandworkload. AsshowninFigure13,participantsreportaslight
increase in engagement and enjoyment during the vocabulary learning process with RetAssist
comparedtothebaselinesystem,thoughthedifferencewasnotstatisticallysignificant.However,
22(outof24)participantscommendthequalityoftheimagesandfeelthattheimagesareclosely
alignedwiththetext.‚ÄúThepicturesareappealing,andIcaninteractwiththembyswitchingthe
pictureandcheckingitsrelatedsentence‚Äù(P12). Furthermore,participantsreportalowerlevelof
mentaldemand (ùëù < 0.05,ùëß = 2.066,Cohen‚Äôsd = 0.455) duringthevocabularylearningprocess
withRetAssist thanthatwiththebaselinesystem. 21participantsperceivethatpracticingstory
retellingwiththebaselinesystemisnotablymorechallenging,astheyneedtomentallyvisualize
andconstructthesceneofthestory.‚ÄúRecallingthestory‚Äôsdetailsandscenarios(withthebaseline
system)takesupalotofmymentaleffort.Incontrast,RetAssist helpsmetorecallthestoryina
visualway‚Äù(P6).Fiveparticipantsfurtherreportthatthebaselinesystemismonotonouscompared
toRetAssist.‚ÄúIdonotlikethe(baseline)interfaceasitismonotonousandinflexible‚Äù(P3).
6.2.2 Performance in each round of repeated retelling. Figure 14 shows the spent time in each
roundofrepeatedretellingwithRetAssist andthebaselinesystem.Participantsspendlesstime
withRetAssist inthesecond(ùëÄ(ùëÜùê∑) :113(45.07)vs.137(61.02);ùëù <0.05,ùë° =‚àí2.227,Cohen‚Äôsd=
0.321) and third (110(47.23) vs. 132(55.28);ùëù < 0.05,ùë° = ‚àí2.18,Cohen‚Äôsd = 0.315) rounds of
repeated retelling compared to the cases with the baseline system. ‚ÄúWhile using the baseline
system, I frequently run out of the limited time before finishing retelling the story; however,
when using RetAssist, I am more comfortable in the repeated retelling stage and can complete
theretellingontime‚Äù(P20).Meanwhile,asshowninFigure15,thesemanticsimilaritybetween
users‚Äôretellingcontentandoriginalstorysignificantlyincreasesduringthesecond(ùëù <0.05,ùë° =
2.397,Cohen‚Äôsd=0.346)andthethirdrounds(ùëù <0.001,ùë° =3.793,Cohen‚Äôsd=0.547)ofrepeated
retelling. Nineteenparticipantsattributetheirimprovementduringthepracticetotheprovided
images in RetAssist. ‚ÄúImages provided in RetAssist help me better connect my native language
expressionandEnglishexpressionofthetargetwords,whichhelpsmereflectonthedetailsand
storylineinashorttime‚Äù(P4).Theseresultsindicatethatcomparedtothebaselinesystem,
21DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
RetAssistcanreducelearners‚Äôworkloadandimprovetheirefficiencyandperformanceof
eachroundofrepeatedretellingduringthestoryretellingpractices.
Fig.14. RQ2resultsregardingtime Fig.15. RQ2resultsregardingthe Fig.16. RQ3resultsregardinguser
spentbyusersinthreeroundsof semanticsimilaritybetweenusers‚Äô perceptions of each interface. ‚àó :
retelling.‚àó:ùëù <0.05usingpaired- retellingcontentandstory.‚àó:ùëù < ùëù < 0.05 using paired samples
samplet-tests. 0.05,‚àó‚àó‚àó:ùëù <0.001usingpaired- Wilcoxonsignedranktests.
samplet-tests.
6.3 PerceptionstowardstheSystems(RQ3)
AsshowninFigure16,participantsfeelthatourRetAssist (ùëÄ =5.365,ùëÜùê∑ =1.031)issignificantly
moreusefulthanthebaselinesystem(ùëÄ =4.74,ùëÜùê∑ =0.996; ùëù <0.05,ùëß =2.29,Cohen‚Äôsd=0.604).
NineteenparticipantsimpliedthattheimagesinRetAssist arethereasonforratingitmoreuseful.
‚ÄúWithouttheimages,Ifinditdifficulttogothroughtherepeatedretellingstage.Theimagesare
especially useful when I am stuck‚Äù (P13). There is no significant difference between RetAssist
(ùëÄ =4.708,ùëÜùê∑ =1.156)andthebaselinesystem(ùëÄ =4.604,ùëÜùê∑ =1.141)regardingeasinessofuse.
Wehavecommentsfromtwenty-oneparticipantsthatpraisetheinteractiondesignofRetAssist.
‚ÄúTheinterfaceofRetAssistisintuitive,andtheinteractionflowisclear.Icanlistentothestorywhile
easilyreadingthestorywithalignedimages‚Äù(P11).Lastly,participantsgenerallyhaveahigher
intentiontouseRetAssist (ùëÄ =4.9,ùëÜùê∑ =1.249)forvocabularylearninginthefuturecomparedto
baselinesystem(ùëÄ =4.6,ùëÜùê∑ =1.337).TwentyparticipantscommentthattheyprefertheRetAssist
forfuturevocabularylearning.‚ÄúWithRetAssist,Icanexpressthelearnedwordsmorecorrectly
withlesspressure.Iwanttohaveitasmyweeklyusedvocabularylearningsystem‚Äù(P16).However,
fourparticipantspreferthebaselinesystem,becausetheyfeelitistime-consumingtoviewthe
imagesandmentallyconnectthemwiththestory.
7 DISCUSSION
Inthiswork,wedeveloptheRetAssist systemthataimstofacilitatevocabularylearnersintheir
storyretellingpractices.Itscorefeaturesarethegenerativeimagesrelevanttothestoryinthe
storycomprehensionandrepeatedretellingstages.Ourstudyshowsthatparticipantsusingeither
RetAssist orthe baselinesystem canmaster themeanings andexpressionsof thetarget words
right after a story retelling practice, supporting that story retelling is an effective approach to
vocabularylearning[16,23,42,44].However,oneweekafterthepractices,participantsbetterrecall
andverballyexpressthetargetwordslearnedwithRetAssist thanthosewiththebaselinesystem.
Thisprovesthevalueofourgenerativeimagesforsupportingvocabularylearningandprovides
empiricalevidenceforthebenefitsofvisualaidsforlanguagelearningstatedintheCognitive
TheoryofMultimediaLearning[53].
22RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
7.1 DesignConsiderations
Basedonourfindings,weprovidethreedesignconsiderationsforstory-basedvocabularylearning
tools.
Providemoretypesofvisualaids. ParticipantsgenerallyfavorRetAssist‚Äôsimagesforhelping
themcomprehendandrecallstories.However,threeparticipantscommentthattheystillhave
difficulty in recalling the expression of target words with the generative images and suggest
thatitwouldbebettertovisualizestoriesthroughmindmapsorflowcharts[45,56].Moreover,
participantsexpectRetAssisttoincorporateshortvideos[4]ormotiongraphics[36]intovocabulary
learning.‚ÄúUnderstandingtheimagesthemselvesisanadditionalburdenforme.Iwouldprefera
moreexplainableformofvisualaidstohelpmeunderstandsomeabstractstorylinesinthestory
comprehensionstage‚Äù(P18).We,therefore,suggestthatthegenerativetechniquecouldofferother
formsofvisualaidssuchasanextractedmindmapandarelevantvideoclip,andallowusersto
customizethembasedontheirinterests.
Offerpromptsthatareadaptivetousers‚Äôperformance. RetAssist currentlyprovidesfixed
imagepromptsandwordprompts.However,twoparticipantssuggestthattheyneedmoreper-
sonalizedandinteractiveprompts.Forexample,thesystemcan‚Äúrecognizemystucknessandgive
me corresponding hints based on the progress of my current retelling.‚Äù To provide timely and
personalizedsupportduringeachroundofrepeatedretelling,itwouldrequirefutureresearchersto
labelasetofstoryretellingaudioclipsfortrainingamodeltopredictusers‚Äôdifficulttimingbased
ontheirtone,speed,andpausesinthecurrentpractice.
Providesuggestionstoimprove.ThefeedbackofferedbyRetAssist includesthecorrectness
of target words‚Äô semantic usage as well as highlighting the incorrectly used target words and
thecorrespondingsentences. Fourparticipantsexpectthatitcanalsoexplicitlytellthemhowto
improveinthenextroundofpractice.Forinstance,thesystemcan‚Äúcorrectmispronunciations
ofwords,listthetargetword‚Äôsgrammaticalusageandprovideadditionalexamplesentences‚Äùto
enhancethecomprehensionofthetargetvocabulary.Wesuggestthatfuturevocabularylearning
toolsshouldoffernotonlyfeedbackonwhatandwhyatargetwordismisusedbutalsosuggestions
onhowtodeepentheunderstandingofthisword,e.g.,withmoreexamplesentences.
7.2 BroaderImpacttoGenerativeAIsforEducation
OurdesignanddevelopmentofRetAssist offersafeasibleexampleofleveraginggenerativeAIsto
supportlearningtasks.First,generativemodelscanoffermeaningfulandflexiblelearningmaterials,
e.g.,ChatGPT[5]thatgeneratesastorygivenanytargetwordsinourcase.Itisalsopromising
toapplythesemodelstopreparelisteningmaterials[59]andprovidecontextuallypersonalized
learningmaterials[15]forlanguagelearners.Second,generativemodelscansupportadditional
modalities of learning activities used in traditional instruction on a large scale. In addition to
servingasvisualaidsasinourcase,text-to-imageAIcanbeintegratedinto3DDesignWorkflow
toproducereferenceimages,preventdesignfixation,andinspiredesignconsiderations[39].Also,
theycanempoweraconversationalagent,whichactslikealecturer,tosociallyconversewiththe
learnerstopracticetheirspokenlanguage[61]onanytopic.
However,utilizinggenerativecontentaslearningmaterialsmayhavethepotentialtohinder
learninggainsincertainscenarios.OneconcernistheriskofgenerativeAIsintermsofaccuracy
andreliability.Learnersneedtotakeprecautionsagainstgeneratingerrorsorfalseinformation
whenadoptinggeneratedcontentaslearningmaterial,andthegenerativecontentmaybeone-sided
andoutdatedbecauseofthelimitationsofthetrainingdataforgenerativeAIs[40].Anotherconcern
isthattheassistanceofgenerativeAIsmaydiscourageusersfromputtinginenougheffortinthe
learningprocess.Forinstance,Pengetal.‚Äôsstudysuggeststhatlearnerscouldexperiencereduced
23DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
gainsinvocabularyacquisitionwhenengagedinwritingexerciseswithgenerativeAIscompared
tothosewithoutAIassistance[55].Thiscouldbeattributedtothefactthatparticipantsinvested
lesstimeinwritingandwrotesignificantlyfewerwordsinthestory,asitindicatesapreference
fordependenceonthegenerativemodelforassistance[55].Tomitigatethesepotentialnegative
impacts,wesuggestthatthedevelopersoflearningsupportsystemsshouldexaminethequalityof
generatedcontentbeforehandandworkwithtargetedlearnersandeducatorstoidentifyproper
designprinciples(Figure1).
AlthoughRetAssist isinitiallydesignedforindependentlearningoutsideoftheclassroom,it
canbeusefulindiverseeducationalsettingsbeyondindividualstudy.Forexample,teachersin
traditionalclassroomscanuseRetAssist toenrichvocabularyinstructionaroundstoryreading
or story retelling. In addition to assisting ESL learners in vocabulary acquisition using story
retelling,thesystem‚Äôsstorytext-to-imagegenerationworkflowisexpectedtobeusefulingeneral
educationscenariosthatcombineimageswithstories.Forexample,ourworkflowcangenerate
sentence-levelillustrationsforchildren‚Äôsstorybookstohelpthembetterunderstandthemeaning
oftextualdescriptions.Inaddition,forcultivatingchildren‚Äôsexpressivelanguageskills,ourstory
text-to-imagegenerationworkflowcanbeusedasaninteractiveandcreativewayforteachers
orparentstopracticeexpressivelanguageinchildren‚Äôseducation.Byretellingstorieswiththeir
illustrations,childrencandeveloptheabilitytoclearlyorganizetheirverbalexpressions,make
associationsbetweenvisualmaterialsandtextualmaterials,andcreativelyconceptualizetheplot
withtheillustrationdetails.Despitetheenormouspotentialofourproposedsystemandworkflow
ineducation,wemustapproachpotentialriskscautiouslytoensurethattheybringpositiveand
sustainableimpacts.Wemustensurethatthegenerativeimagesandstoriesconformtowidely
acceptededucationalstandardsandethicalnormstoavoidconveyingincorrectinformationor
inappropriatecontent.
7.3 LimitationsandFutureWork
Ourstudyhasseverallimitationsthaturgefuturework.First,asourprimaryfocusisonvocabulary
learningsupport,wedidnotexamineRetAssist‚Äôsimpactonlearners‚Äôstoryretellingskills.Learners
mayhaveoverrelianceongenerativeimagesinthestoryretellingpractices,whileintheEnglish
examsthatteststoryretellingperformance,theywouldnothavesuchassistance.Futureworkcan
extendRetAssist fortrainingstoryretellingskills.Second,weevaluateRetAssist withtwenty-four
English-as-second-languageundergraduateslearningIELTSwords,whocouldnotrepresentall
targetusergroups.Wewouldliketoextendthestudytoincludelearnersofdifferentagegroupsor
proficiencylevelsinourfuturework,andwealsoencouragefutureresearcherstocustomizeour
systemandevaluateittosupportvocabularylearnersofdifferentages,expertise,andcultures(e.g.,
middleorhigh-schoolstudentsandEnglishstudentslearningChinese). Third,weconducteda
short-termuserstudythatcanrevealRetAssist‚Äôsuserexperienceandeffectivenessinourproposed
learningtasks. Toexamineitsusageinthewild,weneedalong-termfieldstudyinwhichusers
can specify any target words and take story retelling practices at any time they want. Fourth,
wedesignourcomputationalworkflowofgeneratingmultipleimagepromptsrelevanttoeach
story.Inourformativestudy,weindicatethatthestoriesareshortoneswithapproximately60
words. However,this studydesignmaynot applytoalluser groups.As thestorygetslonger,
our computational workflow will generate more sentence-level images that may decrease the
coherence among images and increase users‚Äô cognitive workload to process them in the story
retellingpractices.Toalleviatethiscognitiveload,futureworkcouldconsiderwaystogenerate
imagesbasedonthesemanticsegmentsofthestory(i.e.,oneormultiplesentencesthatdescribe
oneimage).Fifth,futuredesigniterationsofRetAssistcouldincorporatemoreadvancedAIfeatures
likeadaptivelearningalgorithmsthattailorimageselectionorpresentationbasedonindividual
24RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
learnerperformance.Sixth,ourstudyexclusivelyutilizedgenerativeimagesasvisualaids,yet
alternativemediaformatsmightyielddifferentoutcomes.Inordertounderstandtheaffordances
ofstaticordynamicimagesforlearning,wewillconsidercomparingtheefficacyofgenerative
imageswithothermediatypes(e.g.,videosorinteractivegraphics)inourfuturework.
8 CONCLUSION
Inthispaper,basedoneducationalliteratureandworkingwithteachersaswellasESLlearners,
weiterativelydesignanddevelopaninteractivesystem,RetAssist,tofacilitatevocabularylearners
instoryretellingpractices.RetAssist equipsourproposedcomputationalworkflowthatgenerates
imagesrelevanttothestorytofosterusers‚Äôunderstandingandrecallofthestorythatcontainsa
setoftargetwords.Weconductawithin-subjectsstudywith24participantsincomparisontothe
baselinesystemwithoutgenerativeimages.OurresultsshowthatlearningwithRetAssist leads
tosignificantlybetterlearningoutcomesonmasteringmeaningsandexpressionsoftargetwords
thanlearningwiththebaselinesystem.Ourworkdemonstratesthefeasibilityandeffectivenessof
generativemodelstosupportlanguagelearningtasksandoffersimplicationsforfuturelearning
supporttools.
ACKNOWLEDGMENTS
This work is supported by the Young Scientists Fund of the National Natural Science Founda-
tion of China (NSFC) with Grant No.: 62202509, NSFC Grant No.: U22B2060, and the General
Projects Fund of the Natural Science Foundation of Guangdong Province in China with Grant
No.2024A1515012226.Also,thisworkissupportedinpartbyHKUST30for30withGrantNo.:
3030_003.Wearegratefultotheanonymousreviewersfortheirinsightfulsuggestions.
REFERENCES
[1] RichaAgrawalandRaviPoovaiah.2021.CoSpeak:PeerFeedbackonVoiceStoriestoInformLearningSpokenEnglish.
InCompanionPublicationofthe2021ConferenceonComputerSupportedCooperativeWorkandSocialComputing(Virtual
Event,USA)(CSCW‚Äô21).AssociationforComputingMachinery,NewYork,NY,USA,1‚Äì4. https://doi.org/10.1145/
3462204.3481750
[2] MunassirAlhamami.2016.Vocabularylearningthroughaudios,images,andvideos:Linkingtechnologieswithmemory.
Call-Ej17,2(2016),87‚Äì112.
[3] RikuArakawa,HiromuYakura,andSosukeKobayashi.2022.VocabEncounter:NMT-poweredVocabularyLearning
byPresentingComputer-GeneratedUsagesofForeignWordsintoUsers‚ÄôDailyLives.InProceedingsofthe2022CHI
ConferenceonHumanFactorsinComputingSystems.1‚Äì21.
[4] Bet√ºlBal-Gezegin.2014. Aninvestigationofusingvideovs.audioforteachingvocabulary. Procedia-Socialand
BehavioralSciences143(2014),450‚Äì457.
[5] YejinBang,SamuelCahyawijaya,NayeonLee,WenliangDai,DanSu,BryanWilie,HolyLovenia,ZiweiJi,Tiezheng
Yu,WillyChung,etal.2023.Amultitask,multilingual,multimodalevaluationofchatgptonreasoning,hallucination,
andinteractivity.arXivpreprintarXiv:2302.04023(2023).
[6] FrankBoers.2014.Areappraisalofthe4/3/2activity.RELCJournal45,3(2014),221‚Äì235.
[7] ArthurCaetano,AlyssaLawson,YimengLiu,andMishaSra.2023.ARLang:Anoutdooraugmentedrealityapplication
forportuguesevocabularylearning.InProceedingsofthe2023ACMDesigningInteractiveSystemsConference.1224‚Äì1235.
[8] JialunCao,MeiziniuLi,YetingLi,MingWen,Shing-ChiCheung,andHaimingChen.2022.SemMT:asemantic-based
testingapproachformachinetranslationsystems.ACMTransactionsonSoftwareEngineeringandMethodology(TOSEM)
31,2(2022),1‚Äì36.
[9] Chih-MingChenandChing-JuChung.2008.PersonalizedmobileEnglishvocabularylearningsystembasedonitem
responsetheoryandlearningmemorycycle.Computers&Education51,2(2008),624‚Äì645.
[10] ShizheChen,BeiLiu,JianlongFu,RuihuaSong,QinJin,PingpingLin,XiaoyuQi,ChuntingWang,andJinZhou.
2019. Neuralstoryboardartist:Visualizingstorieswithcoherentimagesequences.InProceedingsofthe27thACM
InternationalConferenceonMultimedia.2236‚Äì2244.
[11] YangChen,Yu-KunLai,andYong-JinLiu.2018.Cartoongan:Generativeadversarialnetworksforphotocartoonization.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.9465‚Äì9474.
25DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[12] PaulineCullen.2012.CambridgeVocabularyforIELTSAdvancedBand6.5+withAnswersandAudioCD.Vol.6.Cambridge
UniversityPress.
[13] PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatgansonimagesynthesis.AdvancesinNeural
InformationProcessingSystems34(2021),8780‚Äì8794.
[14] HDouglasandLEEBROWN.2001. Teachingbyprinciples:Aninteractiveapproachtolanguagepedagogy. PED
AUSTRALIA.
[15] FionaDraxler,AlbrechtSchmidt,andLewisLChuang.2023. Relevance,Effort,andPerceivedQuality:Language
Learners‚ÄôExperienceswithAI-GeneratedContextuallyPersonalizedLearningMaterial.InProceedingsofthe2023ACM
DesigningInteractiveSystemsConference.2249‚Äì2262.
[16] CarlJDunst,AndrewSimkus,andDeborahWHamby.2012. Children‚Äôsstoryretellingasaliteracyandlanguage
enhancementstrategy.CenterforEarlyLiteracyLearning5,2(2012),1‚Äì14.
[17] FerdaG√ºlAydƒ±nEmekligilandƒ∞lkay√ñks√ºz.2022.Gamecharactergenerationwithgenerativeadversarialnetworks.
In202230thSignalProcessingandCommunicationsApplicationsConference(SIU).IEEE,1‚Äì4.
[18] TomFawcett.2006.AnintroductiontoROCanalysis.Patternrecognitionletters27,8(2006),861‚Äì874.
[19] DiamantoFilippatouandPeterDPumfrey.1996. Pictures,titles,readingaccuracyandreadingcomprehension:a
researchreview(1973-95).EducationalResearch38,3(1996),259‚Äì291.
[20] MaayanFrid-Adar,EyalKlang,MichalAmitai,JacobGoldberger,andHayitGreenspan.2018.Syntheticdataaugmen-
tationusingGANforimprovedliverlesionclassification.In2018IEEE15thinternationalsymposiumonbiomedical
imaging(ISBI2018).IEEE,289‚Äì293.
[21] LeonAGatys,AlexanderSEcker,andMatthiasBethge.2015. Aneuralalgorithmofartisticstyle. arXivpreprint
arXiv:1508.06576(2015).
[22] MohammadRezaGhorbani.2014.StoryRetellingandtheEFLVocabularyLearningProcess.TheIranianEFLJournal
11(2014),398.
[23] AkimiGibson,JudithGold,andCharissaSgouros.2003.Thepowerofstoryretelling.Thetutor(2003),1‚Äì11.
[24] YongqiGuandRobertKeithJohnson.1996.Vocabularylearningstrategiesandlanguagelearningoutcomes.Language
learning46,4(1996),643‚Äì679.
[25] SandraGHart.2006. NASA-taskloadindex(NASA-TLX);20yearslater.InProceedingsofthehumanfactorsand
ergonomicssocietyannualmeeting,Vol.50.SagepublicationsSageCA:LosAngeles,CA,904‚Äì908.
[26] MohammadNehalHasnine,MasatoshiIshikawa,YukiHirai,HarukoMiyakoda,andKeiichiKaneko.2017. An
algorithmtoevaluateappropriatenessofstillimagesforlearningconcretenounsofanewforeignlanguage.IEICE
TRANSACTIONSonInformationandSystems100,9(2017),2156‚Äì2164.
[27] MohammadNehalHasnine,KousukeMouri,BrendanFlanagan,GokhanAkcapinar,NorikoUosaki,andHiroaki
Ogata.2018. Imagerecommendationforinformalvocabularylearninginacontext-awarelearningenvironment.
InProceedingsofthe26thInternationalConferenceonComputerinEducation.Asia-PacificSocietyforComputersin
EducationPhilippines,Asia,669‚Äì674.
[28] MohammadNehalHasnineandJunjiWu.2021.Wordhyve:Acontext-awarelanguagelearningappforvocabulary
enhancementthroughimagesandlearningcontexts.ProcediaComputerScience192(2021),3432‚Äì3439.
[29] AriHautasaari,TakeoHamada,KuntaroIshiyama,andShogoFukushima.2020.VocaBura:AMethodforSupporting
SecondLanguageVocabularyLearningWhileWalking.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.3,4,
Article135(sep2020),23pages. https://doi.org/10.1145/3369824
[30] ShinichiIzumi.2002. Output,inputenhancement,andthenoticinghypothesis:AnexperimentalstudyonESL
relativization.Studiesinsecondlanguageacquisition24,4(2002),541‚Äì577.
[31] SlavaKalyuga,PaulChandler,andJohnSweller.1999. Managingsplit-attentionandredundancyinmultimedia
instruction. AppliedCognitivePsychology:TheOfficialJournaloftheSocietyforAppliedResearchinMemoryand
Cognition13,4(1999),351‚Äì371.
[32] DiederikKingma,TimSalimans,BenPoole,andJonathanHo.2021.Variationaldiffusionmodels.Advancesinneural
informationprocessingsystems34(2021),21696‚Äì21707.
[33] WalterKintschandEileenKintsch.2005. Comprehension. InChildren‚Äôsreadingcomprehensionandassessment.
Routledge,89‚Äì110.
[34] JingYuKoh,JasonBaldridge,HonglakLee,andYinfeiYang.2021.Text-to-imagegenerationgroundedbyfine-grained
userattention.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.237‚Äì246.
[35] RaziyeK√ºt√ºk.2007. TheEffectofMnemonicVocabularyLearningStrategyandStoryTellingonYoungLearners‚Äô
VocabularyLearningandRetention.UnpublishedMAThesis)(1-101)(2007).
[36] VahidNorouziLarsariandRadkaWildov√°.2020.Thepsychologicaleffectofmotioninfographicsonreadingabilityof
primaryschoolstudents.(2020).
[37] YitongLi,ZheGan,YelongShen,JingjingLiu,YuCheng,YuexinWu,LawrenceCarin,DavidCarlson,andJianfeng
Gao.2019.Storygan:Asequentialconditionalganforstoryvisualization.InProceedingsoftheIEEE/CVFConferenceon
26RetAssist DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark
ComputerVisionandPatternRecognition.6329‚Äì6338.
[38] VivianLiuandLydiaBChilton.2022.Designguidelinesforpromptengineeringtext-to-imagegenerativemodels.In
Proceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.1‚Äì23.
[39] VivianLiu,JoVermeulen,GeorgeFitzmaurice,andJustinMatejka.2023.3DALL-E:Integratingtext-to-imageAIin3D
designworkflows.InProceedingsofthe2023ACMdesigninginteractivesystemsconference.1955‚Äì1977.
[40] ChungKwanLo.2023.WhatIstheImpactofChatGPTonEducation?ARapidReviewoftheLiterature.Education
Sciences13,4(2023). https://doi.org/10.3390/educsci13040410
[41] RichardEMayer.2002.Multimedialearning.InPsychologyoflearningandmotivation.Vol.41.Elsevier,85‚Äì139.
[42] DonnaDiSegnaMerrittandBettyZLiles.1989.Narrativeanalysis:Clinicalapplicationsofstorygenerationandstory
retelling.JournalofSpeechandHearingDisorders54,3(1989),438‚Äì447.
[43] SaraMillerandLisaPennycuff.2008.Thepowerofstory:Usingstorytellingtoimproveliteracylearning.Journalof
Cross-DisciplinaryPerspectivesinEducation1,1(2008),36‚Äì43.
[44] LesleyMandelMorrow.1985.Retellingstories:Astrategyforimprovingyoungchildren‚Äôscomprehension,conceptof
storystructure,andorallanguagecomplexity.TheElementarySchoolJournal85,5(1985),647‚Äì661.
[45] ShobanaMusti,JesslynMSmith,andJohnCBegeny.2022.AVirtualTutoringProgramtoIncreaseStudents‚ÄôText
ReadingFluency.InterventioninSchoolandClinic(2022),10534512221140474.
[46] PaulNation.2007.Thefourstrands.InternationalJournalofInnovationinLanguageLearningandTeaching1,1(2007),
2‚Äì13.
[47] Chi-DucNguyenandFrankBoers.2019.TheeffectofcontentretellingonvocabularyuptakefromaTEDtalk.Tesol
Quarterly53,1(2019),5‚Äì29.
[48] Aur√©lienNioche,Pierre-AlexandreMurena,CarlosdelaTorre-Ortiz,andAnttiOulasvirta.2021.Improvingartificial
teachersbyconsideringhowpeoplelearnandforget.In26thInternationalConferenceonIntelligentUserInterfaces.
445‚Äì453.
[49] PutuSantiOktarina,NiPutuLilaSriHari,andNiMadeWindaAmbarwati.2020.Theeffectivenessofusingpicture
booktomotivatestudentsespeciallyyounglearnersinreading.YavanaBhasha:JournalofEnglishLanguageEducation
1,1(2020),72‚Äì79.
[50] RebeccaLOxfordandRobinCScarcella.1994.Secondlanguagevocabularylearningamongadults:Stateoftheartin
vocabularyinstruction.System22,2(1994),231‚Äì243.
[51] AllanPaivio.1990.Mentalrepresentations:Adualcodingapproach.Oxforduniversitypress.
[52] AllanPaivio.2014.Bilingualdualcodingtheoryandmemory.Foundationsofbilingualmemory(2014),41‚Äì62.
[53] AllanPaivioandAlainDesrochers.1980.Adual-codingapproachtobilingualmemory.CanadianJournalofPsycholo-
gy/Revuecanadiennedepsychologie34,4(1980),388.
[54] HilalPeker,MicheleRegalla,andThomasDwightCox.2018.Teachingandlearningvocabularyincontext:Examining
engagementinthreeprekindergartenFrenchclassrooms. ForeignLanguageAnnals 51(2018),472‚Äì483. https:
//api.semanticscholar.org/CorpusID:149920259
[55] ZhenhuiPeng,XingboWang,QiushiHan,JunkaiZhu,XiaojuanMa,andHuaminQu.2023. Storyfier:Exploring
VocabularyLearningSupportwithTextGenerationModels. arXiv:2308.03864[cs.HC]
[56] SasitornPraneetponkrangandMalineePhaiboonnugulkij.2014.Theuseofretellingstoriestechniqueindeveloping
Englishspeakingabilityofgrade9students.AdvancesinLanguageandLiteraryStudies5,5(2014),141‚Äì154.
[57] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,Amanda
Askell,PamelaMishkin,JackClark,etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning.PMLR,8748‚Äì8763.
[58] NilsReimersandIrynaGurevych.2019. Sentence-bert:Sentenceembeddingsusingsiamesebert-networks. arXiv
preprintarXiv:1908.10084(2019).
[59] YiRen,YangjunRuan,XuTan,TaoQin,ShengZhao,ZhouZhao,andTie-YanLiu.2019.FastSpeech:Fast,Robustand
ControllableTexttoSpeech. arXiv:1905.09263[cs.CL]
[60] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBj√∂rnOmmer.2022.High-ResolutionImage
SynthesisWithLatentDiffusionModels.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR).10684‚Äì10695.
[61] SherryRuan,LiweiJiang,QianyaoXu,ZhiyuanLiu,GlennMDavis,EmmaBrunskill,andJamesALanday.2021.
Englishbot:Anai-poweredconversationalsystemforsecondlanguagelearning.In26thinternationalconferenceon
intelligentuserinterfaces.434‚Äì444.
[62] HerbertRubensteinandJohnBGoodenough.1965.Contextualcorrelatesofsynonymy.Commun.ACM8,10(1965),
627‚Äì633.
[63] RunwayML.2021.Stable-diffusion-v1-5.https://huggingface.co/runwayml/stable-diffusion-v1-5. AccessedonMarch
25,2023.
27DIS‚Äô24,July1‚Äì5,2024,ITUniversityofCopenhagen,Denmark QiaoyiChenetal.
[64] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015. Deepunsupervisedlearning
usingnonequilibriumthermodynamics.InInternationalConferenceonMachineLearning.PMLR,2256‚Äì2265.
[65] StephenDSorden.2012.Thecognitivetheoryofmultimedialearning.Handbookofeducationaltheories1,2012(2012),
1‚Äì22.
[66] SShyamSundar,SaraswathiBellur,JeeyunOh,QianXu,andHaiyanJia.2014.Userexperienceofon-screeninteraction
techniques:Anexperimentalinvestigationofclicking,sliding,zooming,hovering,dragging,andflipping.Human‚Äì
ComputerInteraction29,2(2014),109‚Äì152.
[67] ViswanathVenkateshandHillolBala.2008.Technologyacceptancemodel3andaresearchagendaoninterventions.
Decisionsciences39,2(2008),273‚Äì315.
[68] LevSemenovichVygotskyandMichaelCole.1978. Mindinsociety:Developmentofhigherpsychologicalprocesses.
Harvarduniversitypress.
[69] ThiemoWambsganss,TobiasKueng,MatthiasSoellner,andJanMarcoLeimeister.2021. ArgueTutor:Anadaptive
dialog-basedlearningsystemforargumentationskills.InProceedingsofthe2021CHIconferenceonhumanfactorsin
computingsystems.1‚Äì13.
[70] ThiemoWambsganss,ChristinaNiklaus,MatthiasCetto,MatthiasS√∂llner,SiegfriedHandschuh,andJanMarco
Leimeister.2020.AL:Anadaptivelearningsupportsystemforargumentationskills.InProceedingsofthe2020CHI
ConferenceonHumanFactorsinComputingSystems.1‚Äì14.
[71] ThomasWolf,JamesRavenscroft,JulienChaumond,andMaxwellRebo.2018.Neuralcoref:Coreferenceresolutionin
spacywithneuralnetworks.
[72] ZimingWu,YulunJiang,YidingLiu,andXiaojuanMa.2020.Predictinganddiagnosinguserengagementwithmobile
uianimationviaadata-drivenapproach.InProceedingsofthe2020CHIconferenceonhumanfactorsincomputing
systems.1‚Äì13.
[73] LirenZengandLingLin.2011.AninteractivevocabularylearningsystembasedonwordfrequencylistsandEbbinghaus‚Äô
curveofforgetting.In2011WorkshoponDigitalMediaandDigitalContentManagement.IEEE,313‚Äì317.
[74] YeshuangZhu,YuntaoWang,ChunYu,ShaoyunShi,YankaiZhang,ShuangHe,PeijunZhao,XiaojuanMa,and
YuanchunShi.2017. ViVo:Video-AugmentedDictionaryforVocabularyLearning.InProceedingsofthe2017CHI
ConferenceonHumanFactorsinComputingSystems(Denver,Colorado,USA)(CHI‚Äô17).AssociationforComputing
Machinery,NewYork,NY,USA,5568‚Äì5579. https://doi.org/10.1145/3025453.3025779
ReceivedFebruary2024;revisedApril2024;acceptedMay2024
28