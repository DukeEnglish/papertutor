Natural Counterfactuals With Necessary Backtracking
Guang-YuanHao*12 JijiZhang*2 BiweiHuang3 HaoWang4 KunZhang51
Abstract
Evidence
Counterfactualreasoningispivotalinhumancog- ğµğµğµğµğµğµ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½
Sudden Braking Falling Down Injury
nitionandespeciallyimportantforprovidingex-
Non-backtracking
planations and making decisions. While Judea
ğµğµğµğµğµğµ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½
Pearlâ€™s influential approach is theoretically ele- Sudden Braking Standing Still Safety
gant,itsgenerationofacounterfactualscenario
Ours
oftenrequiresinterventionsthataretoodetached
ğµğµğµğµğµğµ ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ğ‘‡ ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½ğ½
fromtherealscenariostobefeasible. Inresponse, Slowing Down Standing Still Safety
weproposeaframeworkofnaturalcounterfactu- Figure1.MotivationalExample:TreatingBus,Tom,andJerryas
alsandamethodforgeneratingcounterfactuals Variables.Downwardarrowsindicatetheirvalues,whileupward
thatarenaturalwithrespecttotheactualworldâ€™s arrowsrepresentinterventions.
datadistribution. Ourmethodologyrefinescoun-
terfactualreasoning,allowingchangesincausally
precedingvariablestominimizedeviationsfrom leaving its causally upstream features untouched. Such
realisticscenarios. Togeneratenaturalcounter- non-backtrackingcounterfactualreasoning(i.e.,reasoning
factuals,weintroduceaninnovativeoptimization abouttheconsequencesofanchangewithouttracingback
frameworkthatpermitsbutcontrolstheextentof tochangesincausallypreceedingvariables)canyieldvalu-
backtrackingwithaâ€œnaturalnessâ€criterion. Em- ableinsightsintotheconsequencesofhypotheticalactions.
piricalexperimentsindicatetheeffectivenessof Consider a scenario: a sudden brake of a high-speed bus
ourmethod. causedTomtofallandinjureJerry,asillustratedinFig.1.
Non-backtracking counterfactual reasoning would tell us
thatifTomhadstoodstill(despitethesuddenbraking),then
1.Introduction Jerrywouldnothavebeeninjured. Pearlâ€™sapproachsup-
pliesaprincipledmachinerytoreasonaboutconditionalsof
Counterfactualreasoning,whichaimstoanswerwhatafea- thissort,whichareusuallyusefulforexplanation,planning,
tureoftheworldwouldhavebeenifsomeotherfeatureshad andresponsibilityallocation.
beendifferent,isoftenusedinhumancognition,toperform
However,suchsurgicalinterventionsmaynotbefeasiblein
self-reflection,provideexplanations,andinformdecisions.
practiceandmaynotprovidesignificantassistanceinself-
ForAIsystemstomirrorsuchhuman-likedecision-making
reflection. Inthepreviousexample,preventingTomâ€™sfall
processes, incorporating counterfactual reasoning is cru-
inasuddenbrakingscenariorequiresdefyingmechanisms
cial. Judea Pearlâ€™s structural approach to counterfactual
thataredifficultorphysicallyimpossibletodisrupt,suchas
modelingandreasoningstandsasacornerstoneinmachine
thelawofinertia. Thissupposedhardinterventionmaythus
learning (Pearl, 2009). Within this framework, counter-
betoofar-fetchedtoberelevantforpracticalpurposes. For
factualsareconceptualizedasbeinggeneratedbysurgical
example,fromalegalperspective,Tomâ€™sfallcausingJerryâ€™s
interventionsonthefeaturestobechangedthatseverthe
injurycouldbegivenaâ€œnecessitydefense,â€acknowledging
originalcausallinksresponsibleforthosefeatures,while
thatthesuddenbrakinglefthimwithnoalternatives(Conde,
*Equal contribution 1Mohamed bin Zayed University 1981). Hence,forthepurposeofallocatingresponsibility,
of Artificial Intelligence 2The Chinese University of Hong reasoningaboutthecounterfactualsituationofTomstanding
Kong 3University of California San Diego 4Rutgers Uni-
stilldespitethesuddenbrakingisarguablyirrelevantoreven
versity 5Carnegie Mellon University. Correspondence
misleading.
to: Guang-Yuan Hao <guangyuanhao@outlook.com>,
Jiji Zhang <jijizhang@cuhk.edu.hk>, Biwei Huang
Ourpaperaddressescounterfactualreasoningwithafocus
<bih007@ucsd.edu>, Hao Wang <hw488@cs.rutgers.edu>,
onoutcomesthatareconstructive,whichareintendedtoen-
KunZhang<jkunz1@cmu.edu>.
hancepracticalsituationsandofferactionableinsights. To
achievethis,weensurethateverychangeinasystemisnat-
1
4202
beF
2
]IA.sc[
1v70610.2042:viXraNaturalCounterfactualsWithNecessaryBacktracking
ural. Thus,afterthesemodifications,thenewcounterfactual 2.RelatedWork
datapointwillbenaturalwithrespecttodatadistributionin
Non-backtrackingCounterfactualGeneration. Aswill
theactualworld. Accordingly,weintroducethenotionof
becomeclear,ourtheoryispresentedintheformofcoun-
â€œnaturalcounterfactualsâ€toaddressthelimitationsofnon-
terfactual sampling or generation. (Ribeiro et al., 2023;
backtrackingcounterfactualsdiscussedabove. Forexample,
Kocaoglu et al.; Dash et al., 2022; Sanchez & Tsaftaris)
as depicted in Fig. 1, rather than the infeasible scenario
usethedeepgenerativemodelstolearnanSCMfromdata
whereTomdoesnotfallatasuddenbusstop,amorerealis-
givenacausalgraph;theseworksstrictlyfollowPearlâ€™sthe-
ticinterventionwouldinvolvethebusslowingdownearlier,
oryofnon-backtrackingcounterfactuals. Ourcasestudies
achievedthroughbacktracking(Lewis,1979)thatkeepsthe
willexaminesomeofthesemodelsanddemonstratetheir
scenariowithinreal-worldbounds. Furthermore,onemust
difficultiesindealingwithinterventionsthatareunrealistic
ensurethatthecounterfactualdatapointremainsascloseas
relativetotrainingdata.
possibletotheoriginaldatapoint;otherwise,unnecessary
interventions may affect too many variables. To address BacktrackingCounterfactuals. Backtrackingincounter-
thisconcern,weformulateaminimalchangeprinciplethat factualreasoninghasdrawnplentyofattentioninphiloso-
guidesusinperformingonlythenecessarybacktracking. phy(Hiddleston,2005),psychology(Dehghanietal.,2012),
andcognitivescience(Gerstenbergetal.,2013). Hiddleston
Moreover,fromamachinelearningperspective,whenhard
(2005) proposes a theory that is in spirit similar to ours,
interventions lead to unrealistic scenarios relative to the
in which backtracking is allowed but limited by some re-
training data, predicting counterfactual outcomes in such
quirementofmatchingasmuchcausalupstreamaspossible.
scenarioscanbehighlyuncertainandinaccurate(Hassan-
Gerstenbergetal.(2013)showsthatpeopleusebothback-
pour&Greiner,2019). Thisissuebecomesparticularlypro-
trackingandnon-backtrackingcounterfactualsinpractice
nouncedwhennon-parametricmodelsareemployed,asthey
andtendtousebacktrackingcounterfactualswhenexplic-
oftenstruggletogeneralizetounseen,out-of-distribution
itlyrequiredtoexplaincausesforthesupposedchangein
data(SchoÂ¨lkopfetal.,2021). Theriskofrelyingonsuch
acounterfactual. vonKuÂ¨gelgenetal.(2022)isamostre-
counterfactualsisthussubstantial,especiallyinhigh-stake
centpaperexplicitlyonbacktrackingcounterfactuals. The
applications like healthcare and autonomous driving. To
maindifferencesbetweenthatworkandoursarethatvon
tacklethis,wedevelopanapproachthatamountstoutilizing
KuÂ¨gelgen et al. (2022) requires backtracking all the way
onlyfeasibleinterventionsthatkeepdatawithinitsoriginal
backtoexogenousnoisesandmeasuresclosenessonnoise
distribution,allowingbacktrackingwhenneeded. Thisstrat-
terms,whicharelessdesirablethanlimitingbacktrackingto
egy effectively reduces the risk of inaccurate predictions
whatwecallâ€œnecessarybacktrackingâ€andmeasuringclose-
andensuresmorereliableresults.
nessdirectlyonendogenous,observablevariables,because
Consequently, our approach aims to achieve the goal of changestotheunobservedornoisetermsarebydefinition
ensuringthatcounterfactualscenariosremainsufficiently outsideofourcontrolandnotactionable. Moreover,their
realisticwithrespecttotheactualworldâ€™sdatadistribution backtracking counterfactuals sometimes allow gratuitous
bypermittingminimalyetnecessarybacktracking. Itisde- changes,asweexplaininSec.GoftheAppendix.
signedontwomajorconsiderations: First,weneedcriteria
CounterfactualExplanations. Aprominentapproachin
todeterminethefeasibilityofinterventions,ensuringthey
explainableAIiscounterfactualexplanation(Wachteretal.,
arerealisticwithrespecttoactual-worlddatadistribution.
2018;Dhurandharetal.,2018;Mothilaletal.,2020;Baro-
Second,weappealtobacktrackingwhenandonlywhenit
casetal.,2020;Pawlowskietal.,2020a;Vermaetal.,2020;
isnecessarytoavoidinfeasibleinterventions,andneedto
Schutetal.,2021),onwhichourworkislikelytohaveinter-
developafeasibleoptimizationframeworktorealizethis
estingbearings. Mostworksonthistopicdefinesomesense
strategy. Thekeycontributionsofthispaperinclude:
of minimal changes of an input sample with a predicted
â€¢ Developingamoreflexibleandrealisticnotionofnat- classcsuchthataddingtheminimalchangesintotheinput
uralcounterfactuals,addressingthelimitationsofnon- would make it be classified into another (more desirable)
backtrackingreasoningwhilekeepingitsmeritsasfar class. Althoughthispaperdoesnotdiscusscounterfactual
aspossible. explanations,ourframeworkmaywellbeusedtodefinea
â€¢ Introducing an innovative and feasible optimization novelnotionofcounterfactualexplanationbyrequiringthe
frameworktogeneratenaturalcounterfactuals. counterfactualinstancestobeâ€œnaturalâ€inoursense.
â€¢ Detailing a machine learning approach to produce
3.NotationsandBackground
counterfactualswithinthisframework,withempirical
resultsfromsimulatedandrealdatashowcasingthesu-
Inthissection,webeginbyoutliningvariousbasicconcepts
periorityofourmethodcomparedtonon-backtracking
in causal inference, followed by an introduction to non-
counterfactuals.
backtrackingcounterfactuals.
2NaturalCounterfactualsWithNecessaryBacktracking
Structural Causal Models. We use a structural causal causally preceding variables, depending on the â€œnatural-
model (SCM)to represent thedata generating processof nessâ€ofinterventions.Todifferentiatefromtheintervention
acausalsystem. ASCMisamathematicalstructurecon- do(A=aâˆ—)innon-backtrackingcounterfactuals,weintro-
sistingofatripletM:=<U,V,F>,withanexogenous duceaspecificoperatorchange(Â·)andchange(A = aâˆ—)
(noise) variable set U = {U ,...,U }, an endogenous signifiesadesiredmodificationinA,whichmayresultfrom
1 N
(observed)variableV={V ,...,V },andafunctionset aninterventiononAoraninterventiononsomecausalan-
1 N
f ={f ,...,f }(Pearl,2009). Eachfunction,f âˆˆf,spec- cestorsofA.
1 N i
ifieshowanendogenousvariableV isdeterminedbyits
i Weintroducetwonewconceptstodeterminewheretocarry
parentsPA âŠ†V:
i outfeasibleinterventionsonC(eitherAitselforthecausal
V i :=f i(PA i,U i), i=1,...,N (1) earliervariablesofA)torealizechange(A = aâˆ—)innat-
LocalMechanisms. Alocalmechanismistheconditional ural counterfactuals: a naturalness constraint and a nec-
distributionofanendogenousvariablebasedonitsparent essarybacktrackingprinciple.1 First,weproposeusinga
variables,representedasp(V i|PA i)fori=1,...,N. This naturalnesscriteriontoassessthefeasibilityofaninterven-
definitioninherentlycapturesthepropertiesofnoisevari- tion. Weassumethatactual-worlddatacontainallrelevant
ables;givenafixedvalueforPA i,noiseU ientirelydictates causalmechanismsforagivencausalgraph;thus,out-of-
theprobabilityofV i(Pearl,2009). Hence,throughoutthis distributiondataarephysicallyinfeasible. Thismeanswe
paper,thetermâ€œlocalmechanismsâ€willencompassboth limit changed values resulting from interventions to stay
theconditionaldistributionofanendogenousvariablegiven withintheobserveddatadistribution,ensuringthefeasibil-
itsparentsp(V i|PA i)andthedistributionofthenoisevari- ityofinterventionsonCandthenaturalnessofresulting
ablep(U i). counterfactuals.
Intervention. GivenanSCM,aninterventiononanendoge- Furthermore,weconsidertheproximitybetweentheactual
nousvariableAisrepresentedbyreplacingitsfunctionwith value and its counterfactual value, leading to a principle
aconstantfunctionA=aâˆ—,whereaâˆ—isthetargetvalueof
knownasâ€œnecessarybacktracking.â€ Thisprincipleaimsto
A, andleavingfunctionsandlocalmechanismsforother minimizechangesinthecounterfactualworldandbacktrack
variablesintact(Pearl,2009). aslittleaspossible,keepingCascloselyalignedwithA
as feasible. In summary, the naturalness constraint and
Non-Backtracking Counterfactuals. A counterfactual
necessary backbreaking both contribute to determining a
ponderswhatwouldhappeninascenariothatdiffersfrom
feasibleintervention.
theactual oneina certainway. Followinga standardno-
tation,termswithaâˆ—superscriptrefertoacounterfactual
world. Forexample,uâˆ—denotesU â€™svalueinacounterfac- 4.1.FeasibleInterventionOptimization
i i
tualworld. LetA,B,andEbeendogenousvariables. Here
To address the challenge of determining a feasible inter-
isageneralcounterfactualquestion: givenevidenceE=e,
vention,weproposetreatingitasanoptimizationproblem.
whatwouldthevalueofBhavebeenifAhadbeenaâˆ—?The
Thisentailsoptimizingadefineddistancemetricbetween
Pearlian,non-backtrackingreadingofthisquestiontakesthe
the counterfactual outcome produced by the intervention
counterfactualsuppositionofA=aâˆ—toberealizedbyan
andthecorrespondingreal-worlddatapoint,followingthe
interventiononA(Pearl,2009). Giventhisunderstanding,
principleofnecessarybacktracking. Thisoptimizationpro-
counterfactual inference includes three steps. (1) Abduc-
cedurealsoenforcesanaturalnessconstraintoncounterfac-
tion:Thenoisedistributionisupdatedbasedontheevidence
tuals. WedefinethisoptimizationframeworkasFeasible
E=e. (2)Action: Thecausalmodelismodified,inwhich
InterventionOptimization(FIO)asfollows:
Aisfixedtoaâˆ—whilekeepingothercomponentsthesame
asbefore. (3)Prediction: Thecounterfactualoutcomeof minimize D(an(A),an(A)âˆ—)
Bisinferredusingtheupdatednoisedistributionandthe an(A)âˆ—
modifiedmodel. s.t. A=aâˆ—, (2)
g (an(A)âˆ—)>Ïµ.
n
4.AFrameworkforNaturalCounterfactuals
where an(A) and an(A)âˆ— represent the actual value and
Do(Â·) and Change(Â·) Operators. In considering a counterfactualvalueofAâ€™sancestorsAN(A)respectively,
counterfactual scenario in which Aâ€™s value is aâˆ— instead whereA âˆˆ AN(A). D(Â·)isaspecificdistancemetricto
ofa,thenon-backtrackingmodealwaysappealstoadirect
interventiononA, i.e., do(A = aâˆ—)inPearlâ€™sinfluential
1Givenafeasibleintervention,ournaturalcounterfactualinfer-
encealsofollowsthreesteps: abduction,action,andprediction,
notation. However, in our framework, the counterfactual
alignedwiththeprocedureofnon-backtrackingcounterfactuals
suppositionisnotnecessarilyrealizedbyadirectinterven- asmentionedinSec.3.Moredetailsfortheprocedureofnatural
tiononA,andmayberealizedbyinterventionsonsome counterfactualsareinSec.E.
3NaturalCounterfactualsWithNecessaryBacktracking
assessthedistancebetweenactualworldandcounterfactual vâˆ—,withinthecounterfactualdatapointan(A)âˆ—inthissec-
j
world. g (Â·) measures the naturalness of counterfactual tion,followedbyanexaminationoftheoverallnaturalness
n
valueofAâ€™sancestorsandÏµisasmallconstant. Wewill ofan(A)âˆ—inthenextsection.
discussD(Â·)andg (Â·)later.
n We introduce â€œlocal Ïµ-natural generation,â€ where a value
OptimizationScope. Allpossiblebacktrackingvariables satisfies this criterion if it is a natural outcome of its lo-
are contained within Aâ€™s ancestor set AN(A). Conse- cal mechanism. We focus on a specific value V =
j
quently,ouroptimizationfocusremainsconfinedtothese vâˆ—, alongside its parent value PA = paâˆ—, noise value
j j j
nodes. ThroughFIO,weobtaintheoptimalvaluean(A)âˆ—. U = uâˆ—, and the corresponding local mechanism, ex-
j j
Thisidentifiesthechangedvariables,C,whichwetarget pressed by p(V |PA = paâˆ—) or p(U ). The cumula-
j j j j
forintervention,enablingthedefinitionofthefeasiblein- tive distribution function (CDF) for noise variable U at
j
tervention do(C = câˆ—), with câˆ— as the post-optimization U = uâˆ— is F(uâˆ—) = (cid:82)uâˆ— j p(U )dU , and for the con-
j j j âˆ’âˆ j j
counterfactualvaluesofC. ditional distribution p(V |PA = paâˆ—) at V = vâˆ— is
j j j j j
NaturalnessConstraint.Inadditiontoensuringthechange F(V |paâˆ—)=(cid:82)v jâˆ— p(V =vâˆ—|paâˆ—)dV .
j j âˆ’âˆ j j j j
A = aâˆ—, the counterfactual value an(A)âˆ— must satisfy a
Weproposethefollowingpotentialcriteriabasedonentropy-
naturalnesscriteriontoensurethefeasibilityofinterventions.
normalizeddensity,CDFofexogenousvariables,andCDF
Specifically,itsnaturalnessshouldexceedthethresholdÏµto
ofconditionaldistributions. Theentropy-normalizednatu-
remainwithinthedatadistribution. Adetaileddiscussion
ralnessmeasureevaluatesthenaturalnessofvâˆ—inrelation
ofpotentialnaturalnessconstraintsispresentedinSec.4.2. j
toitslocalmechanismp(V |PA =paâˆ—). TheCDF-based
j j j
DistanceMeasureD(Â·). D(Â·)considerstheconceptofnec- measures,namelythelattertwocriteria,considerdatapoints
essarybacktrackingandservesasametricforquantifying inthetailstobelessnatural. Eachofthesecriteriahasits
thedistancebetweenthecounterfactualandactualworlds. ownintuitiveappeal, andtheirrelativemeritswillbedis-
Foradetailedexploration,pleaserefertoSec.4.3. cussedsubsequently.Weformallyestablishthethreecriteria
asfollows:
4.2.NaturalnessConstraints
(1) Entropy-NormalizedMeasure:
Toensureaninterventionâ€™sfeasibility,westipulatethatthe
resultingcounterfactualvaluean(A)âˆ—shouldfallwithinthe
p(v jâˆ—|paâˆ— j)eH(Vj|paâˆ— j) > Ïµ, where H(V j|paâˆ— j) =
E[âˆ’logp(V |paâˆ—)];
datadistribution,thusbeingnaturalrelativetothisdistribu- j j
tion. Weintroduceanaturalnesscriterion,whichconfines (2) ExogenousCDFMeasure:min(F(uâˆ—),1âˆ’F(uâˆ—))>
j j
thefeasiblesupportforan(A)âˆ—inEqn.2oftheFIOframe- Ïµ,i.e.,Ïµ<F(uâˆ—)<1âˆ’Ïµ;
j
work.
(3) Conditional CDF Measure: min(F(vâˆ—|paâˆ—),1 âˆ’
Intuitively, the more frequently a value occurs, the more F(vâˆ—|paâˆ—))>Ïµ,i.e.,Ïµ<F(vâˆ—|paâˆ—)<1j âˆ’Ïµ.j
â€œnaturalâ€itisconsidered. Therefore,weassessthisnatural- j j j j
nessbyexaminingthedistributioncharacteristics,suchas
wherethefunctionmin(Â·)returnstheminimumvaluebe-
density,ofeachvariableâ€™svalueV =vâˆ—withinAN(A).
j j tweentwogivenvalues.
Thisassessmentisrelativetothevariableâ€™slocalmechanism
p(V |paâˆ—),whereV âˆˆAN(A)andpaâˆ—denotestheparent Choice (1): Entropy-Normalized Measure. Specifi-
j j j j
valueofV j. Specifically,ourapproachdetermineswhether cally,Choice(1),p(v jâˆ—|paâˆ— j)eH(Vj|paâˆ— j),canberewrittenas
the density of v jâˆ— surpasses a pre-established threshold Ïµ, elog(p(v jâˆ—|paâˆ— j))+E[âˆ’logp(Vj|paâˆ— j)], where âˆ’log(p(v jâˆ—|paâˆ— j))
utilizedasastandardforappraisingnaturalness.2 canbeseenasthemeasureofsurpriseofvâˆ—givenpaâˆ—and
j j
E[âˆ’logp(V |paâˆ—)] can be considered as the expectation
j j
4.2.1.LOCALNATURALNESSCRITERIA ofsurpriseofthelocalmechanismp(V |paâˆ—)(Ash,2012).
j j
Hence, the measure is the relative naturalness (i.e., nega-
Indeed,itmightbedifficulttofindauniversaldefinitionfor
tive surprise) of V . Implementing this measure may be
naturalnesscriteria,andthus,variouscriteriafornaturalness j
straightforwardwhenemployingaparametricSCMwhere
canbeexplored. Thisallowsforamoretailoredapproach
theconditionaldistributioncanbeexplicitlyrepresented.
toassessingnaturalnessindifferentcontextsorapplications.
Westartbyassessingthenaturalnessofonevariableâ€™svalue, Choice(2): ExogenousCDFMeasure. Ifusingaparamet-
ricSCM,wemightdirectlymeasuredifferencesonexoge-
2Theconceptofâ€œnaturalnessâ€canhavevariousinterpretations.
nousvariables. However,inanon-parametricSCM,exoge-
Inourcontext,itisdefinedbythedatadistributioninStructural
nousvariablesarenotidentifiable,anddifferentnoisevari-
CausalModels(SCMs). Wedonotclaimthatthisistheonlyor
uniquelybestinterpretation,butthatitisausefulonetostudyand ablesmayhavedifferentdistributions. Hence,wechooseto
deploy. usetheCDFofexogenousvariablestoalignthenaturalness
4NaturalCounterfactualsWithNecessaryBacktracking
ofdifferentdistributions,basedonacommonassumptionto minimizingchangesinobservablevariablestominimizeper-
achievenon-parametricSCMsinthemachinelearningsys- ceptibledifferencespost-intervention. Thesecondfocuses
tem. Undertheassumption,thesupportofthelocalmech- onreducingalterationsinlocalmechanisms, recognizing
anism p(V |PA = paâˆ—) does not contain disjoint sets, themastheinherentcostofintervention. Duetospacelim-
j j j
andanonlinear,non-parametricmodelV =f (paâˆ—,U ) itations, we will primarily elaborate on minimal changes
j j j j
isimplemented,wheref ismonotonicallyincreasingwith inobservablevariables. Amorethoroughinvestigationof
respecttoU . NoiseU usuallyassumedtobeastandard changesinlocalmechanismscanbefoundinSec.H.
j j
Gaussian(Luetal.,2020). Inthisway,counterfactualsare
Minimal Change in Observable Variables. The differ-
identifiable,discussedinSec.4.4. Datapointsfromthetails
encesinperceptionarecloselytiedtothealterationsinthe
ofastandardGaussiancanbethoughtofaslessimprobable
valuesofobservable(endogenous)variablesresultingfrom
events. Hence, V = vâˆ— satisfies local Ïµ-natural genera-
j j theintervention. Intuitively,thetotaldistanceistheaggre-
tionwhenitsexogenousCDFF(uâˆ—)fallswithintherange
j gateofdistancesacrossvariousvariablesintheancestorset
(Ïµ,1âˆ’Ïµ). In practice, for a single variable, U is a one-
j AN(A). Therefore,weutilizetheL1normtomeasurethis
dimensionalvariable,anditiseasiertoenforcethemeasure
distance, astheL1 normofferstheadvantageofadditive
thanChoice(1),whichinvolvesconditionaldistributions.
distancesacrossvariables. Wedefinethisdistanceasthe
Choice (3): Conditional CDF Measure. The measure perceptiondistance:
treats a particular value in the tails of local mechanism
p(V j|paâˆ— j) as unnatural. Hence, V j = v jâˆ— meets local Ïµ- D(an(A),an(A)âˆ—)=âˆ¥an(A)âˆ’an(A)âˆ—âˆ¥ 1 (3)
naturalgenerationwhenF(V =vâˆ—|paâˆ—)fallswithinthe
j j j where an(A) and an(A)âˆ— represent the actual value and
range(Ïµ,1âˆ’Ïµ)insteadoftails. Thismeasurecanbeusedin
counterfactualvalueofAâ€™sancestorsAN(A)respectively,
parametricmodelswheretheconditionaldistributioncanbe
whereA âˆˆ AN(A). Becauseendogenousvariablesmay
explicitlyrepresented. Notice,itcanbeeasilyusedinnon-
varyinscale,weutilizethestandarddeviationofvariables
parametricmodelswhenthosemodelssatisfytheassump-
tonormalizeeachendogenousvariablebeforecomputing
tionmentionedinChoice(2),andthemeasureisequivalent
thedistance. Thisnormalizationensuresaconsistentand
toChoice(2),sincetheCDFF(X|paâˆ—)hasaone-to-one
j fairevaluationofchangesacrossallvariables,irrespective
mappingwiththeCDFF(U ),i.e.,F(vâˆ—|paâˆ—) = F(uâˆ—),
j j j j oftheirindividualscales.
whenvâˆ— =f(paâˆ—,uâˆ—).
j j j
Implicitly,ourdistancemetricfavorschangesinvariables
4.2.2.Ïµ-NATURALGENERATION that are as proximal as possible to the target variable A,
since altering a more preceding variable typically results
BasedonthedefinitionoflocalÏµ-naturalgeneration,wecan
inchangestomoredownstreamvariables. Whenthevalue
defineÏµ-naturalgenerationtojudgewhetherthecounterfac-
an(A)âˆ—,resultingfromahardinterventiononA,meetsthe
tualvaluean(A)âˆ—isnatural.
Ïµ-naturalgenerationcriterion,itsuggeststhatourdistance
Definition1(Ïµ-NaturalGeneration). GivenaSCMcon-
metricD(an(A),an(A)âˆ—)becomesminimal,i.e.,|aâˆ’aâˆ—|.
taining a set A. A set AN(A) contains all ancestors of This effectively eliminates the need for backtracking in
A and A itself. Data point AN(A) = an(A)âˆ— satis- suchcases. However,Ifahardinterventiondoesnotmeet
fies Ïµ-natural generation, if and only if, for any variable theÏµ-naturalgenerationcriterion,itbecomesnecessaryto
V âˆˆ AN(A),V = vâˆ— satisfieslocalÏµ-naturalgenera- backtrack.
j j j
tion,wherevâˆ—isthevalueofV andÏµisasmallconstant.
j j
4.4.IdentifiablityofNaturalCounterfactuals
wherethedefinitionoflocalÏµ-naturalgenerationcouldbe
In practice, we often do not know the form of structural
oneoftheoptionaldefinitionsin4.2.1andalargervalue
equation models and noise variables are not identifiable
of Ïµ implies a higher standard for the naturalness of the
from the observed variables. Hence, we assume causal
generateddatapointan(A)âˆ—.Tomakeinterventionfeasible,
structural models satisfy the conditions of the following
werequirean(A)âˆ—tomeetÏµ-naturalgeneration.
theoremderivedfromTheorem1ofLuetal.(2020)with
proof in Sec. B, under which natural counterfactuals are
4.3.DistanceMeasureforNecessaryBacktracking
identifiablewithunknownstructuralequationmodels.
ThenaturalnessconstraintsoutlinedinEqn.2werecompre- Theorem4.1(IdentifiableCounterfactuals). SupposeV
i
hensivelyaddressedpreviously. Wenowfocusondefining satisfies the following structural causal model: V :=
i
thenecessarybacktrackingdistanceinEqn.2oftheFIO f (PA ,U ) for any V âˆˆ V, where U âŠ¥ PA and as-
i i i i i i
framework to minimize the change in the counterfactual sumeunknownf issmoothandstrictlymonotonicw.r.t. U
i i
datapoint. Ouranalysisintroducestwodistinctdefinitions forfixedvaluesofPA . IfwehaveevidenceE = e,with
i
ofdistanceinthecounterfactualcontext.Thefirstprioritizes aninterventiondo(C=câˆ—),thecounterfactualdistribution
5NaturalCounterfactualsWithNecessaryBacktracking
ofBisidentifiable;thatis,p(B|do(C = câˆ—),E = e)is sure of distance between two distinct worlds, while the
identifiable.3 secondtermenforcestheconstraintofÏµ-naturalgeneration.
Here, the constant hyperparameter w serves to penalize
Ïµ
Following Feasible Intervention Optimization, we obtain noisevaluessituatedinthetailsofnoisedistributions.
the feasible intervention do(C = câˆ—). This theorem en-
surestheidentifiabilityofournaturalcounterfactualsgiven 6.CaseStudies
do(C = câˆ—),underthestatedassumptions. Notethatthe
Inthissection,weapplyourmethodandevaluateitseffec-
assumptionsdonotrequiremoreinformationonthestruc-
tiveness through empirical experiments on four synthetic
turalfunctionsoronthenoisedistributions. Hence,even
datasetsandtworeal-worlddatasets: MorphoMNISTand
without full information about the SCM, we can still use
3DIdentBOX.
conditionaldistributionstoinfernaturalcounterfactuals,as
wellasnon-backtrackingcounterfactuals,iftheassumptions Weproposeusingthedifferencebetweengeneratedandac-
oftheabovetheoremhold. tualoutcomesasameasureofperformance. Weexpectour
natural counterfactuals to significantly reduce error com-
5.AMethodforGeneratingNatural paredtonon-backtrackingcounterfactuals. Thisadvantage
Counterfactuals canbeattributedtotheeffectivenessofourmethodinper-
formingnecessarybacktrackingthatidentifiesfeasiblein-
Inthissection,weprovideapracticalmethodforintegrating terventions,keepingcounterfactualvalueswithinthedata
naturalcounterfactualsintoamachinelearningsystem. In distribution,particularlywhendirectinterventionsarenot
our scenario, we start with real-world data and a causal feasible. Ontheotherhand,non-backtrackingcounterfac-
graph, lacking the SCM. We employ a non-parametric tuals,relyingsolelyondirectinterventions,oftenproduce
modeltomodeltheconditionaldistributionsofendogenous out-of-distributionvalues,posingchallengesformachine
variablesandconsiderthemodelasanon-parametricSCM, learningmodelgeneralization.
makingtheassumptionthateachnoisevariableadherestoa
6.1.SimulationExperiments
standardGaussiandistribution. ThislearnedSCMservesas
thebasisforgeneratingnaturalcounterfactuals.
We start with four simulation datasets, which we use de-
Toimplementaparticularmethod,weplugperceptiondis- signedSCMstogenerate. PleaserefertotheAppendixfor
tance (Eqn. 3) and naturalness constraint (Choice (3) in moredetailsaboutthesedatasets. LetfirstdevolveintoToy
Sec.4.2.1)intoEqn.2oftheFIOframework. Belowisthe 1, which contains three variables (n 1,n 2,n 3). n 1 is the
equationofoptimization: confounderofn 2andn 3,andn 1andn 2causesn 3.
ExperimentalSettings. Assumedataandacausalgraph
minimizeâˆ¥an(A)âˆ’an(A)âˆ—âˆ¥
an(A)âˆ— 1 are known, but not the ground-truth SCMs. We employ
s.t. A=aâˆ—, (4) normalizingflowstocapturethecausalmechanismsofvari-
ables(n ,n ,n ).GiventhelearnedSCMsandadatapoint
Ïµ<F(Vj =v jâˆ—|paâˆ— j)<1âˆ’Ïµ,âˆ€Vj âˆˆAN(A). fromthe1 tes2 tset3
asevidence,wesetn orn astargetvari-
1 2
Noticethattheequationcouldhavenosolutions,i.e.,there able A and randomly sample values from test dataset as
isnofeasibleintervention,whenthetwoconstraintsimpos- counterfactualvaluesofthetargetvariablen 1 orn 2. For
sibly hold at the same time. In practice, the Lagrangian ournaturalcounterfactuals,weuseEqn.5todeterminefea-
method(Boyd&Vandenberghe,2004)isusedtooptimize sibleinterventions,withÏµ = 10âˆ’4 andw Ïµ = 104. Innon-
ourobjectivelossasbelow: backtrackingcounterfactuals,n 1orn 2isdirectlyintervened
on. WereporttheMeanAbsoluteError(MAE)betweenour
L=(cid:13) (cid:13)an(A)âˆ’an(A)âˆ—(cid:13)
(cid:13) + learnedcounterfactualoutcomesandground-truthoutcomes
1
wÏµ(cid:88) [max(Ïµâˆ’F(Vj =v jâˆ—|paâˆ— j),0)+max(Ïµ+F(Vj =v jâˆ—|paâˆ— j)âˆ’1,0)] onn 2or/andn 3withmultiplerandomseeds. Noticethere
j maybenofeasibleinterventionsforsomechanges,aswe
s.t. A=aâˆ— haveclaimedinSec.5,andthusweonlyreportoutcomes
(5) withfeasibleinterventions,whichiswithinthescopeofour
wherethefunctionmax(Â·)returnsthemaximumvaluebe- naturalcounterfactuals.
tweentwogivenvalues. Thefirsttermrepresentsthemea-
Visualization of Counterfactuals on a Single Sam-
3Iftheassumptiondoesnothold,identifiabilitymayfail.For ple. We assess the counterfactual outcomes for a sam-
example,forY = XU 1+U 2 whereY andX areendogenous ple (n 1,n 2,n 3) = (âˆ’0.59,0.71,âˆ’0.37), aiming for
variablesandU andU arenoise, thecounterfactualoutcome
1 2 change(n = 0.19). In Fig. 2 (a), we depict the origi-
isnotidentifiable.Similarly,ourapproachcanbeappliedtodis- 2
naldatapoint(yellow),thenon-backtrackingcounterfactual
cretevariablessatisfyingtheassumptionofcounterfactualstability
developedby(Oberst&Sontag,2019)intheory. (purple),andthenaturalcounterfactual(green)for(n 1,n 2).
6NaturalCounterfactualsWithNecessaryBacktracking
Table1.MAEResultsonToy1âˆ’4.Forsimplicity,weusedooperatorinthetabletosaveroom,andwhennaturalcounterfactualsare
referredto,domeanschange.OurapproachexhibitsanobviousMAEreductionwhenappliedton asexpected.
2
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking 0.477 0.382 0.297 0.315 0.488 0.472 0.436 0.488 0.230 0.179 0.166 0.446 0.429
Ours 0.434 0.354 0.114 0.303 0.443 0.451 0.423 0.127 0.136 0.137 0.158 0.443 0.327
Table2. AblationStudyonÏµ
4 E N Ov o ui n rd sbe an cc ke tracking 11 .. 05 Model Ïµ CFs tdo(t)
i
tdo(i)
i
2
0.5
0 0.0 - NB 0.336 4.532 0.283 6.556 n3=2.31 n3=0.03
0.5 10âˆ’4 0.314 4.506 0.171 4.424 42 11 .. 50 y N O= o un rx sbacktracking V-SCM 10âˆ’3 Ours 0.298 4.486 0.161 4.121
4 3 2 1 n10 1 2 3 4 1.5 1.0 x0.:5 n3's Gr0o.0undtrut0h.5 1.0 1.5 10âˆ’2 0.139 4.367 0.145 3.959
(a) Outcomeerroronasin- (b) Groudtruth-PredictionScatter
- NB 0.280 2.562 0.202 3.345
glesample Plot
10âˆ’4 0.260 2.495 0.105 2.211
Figure2.TheVisualizationResultsonToy1(Viewtheenlarged H-SCM
10âˆ’3 Ours 0.245 2.442 0.096 2.091
figureinFig.7intheAppendix).
10âˆ’2 0.093 2.338 0.083 2.063
Theground-truthsupportforthesevariablesisshownasa
Furthermore,ourmethodexcelsevenwheninterveningin
bluescatterplot.
thecaseofn ,arootcause,byexcludingpointsthatdonot
1
(1) Feasible Intervention VS Hard Intervention. Non- meettheÏµ-naturalgenerationcriteria,furtherdemonstrating
backtrackingcounterfactualsapplyahardinterventionon itseffectiveness.
n (do(n = 0.19)),shiftingtheevidence(yellow)tothe
2 2 AdditionalCausalGraphStructures. Ourmethodalso
post-interventionpoint(purple),whichliesoutsidethesup-
shows superior performance on three other simulated
portof(n ,n ). Thisimpliesthatdirectinterventionscan
1 2 datasetswithvariedcausalgraphstructures(Toy2toToy4),
resultinunnaturalvalues. Conversely,ournaturalcounter-
asdemonstratedinTable1.
factual(green)remainswithinthesupportof(n ,n )due
1 2
tonecessarybacktrackingandfeasibleinterventiononn .
2
6.2.MorphoMNIST
(2)OutcomeError. Wecalculatetheabsoluteerrorbetween
n â€™smodelpredictionandground-truthvalueusingeither
3 ğ‘¡
thegreenorpurplepointasinputforthemodelp(n |n ,n ).
3 1 2
Theerrorforthegreenpointissignificantlylowerat0.03,
comparedto2.31forthepurplepoint. Thislowererrorwith
ğ‘– ğ‘¥
thegreenpointisbecauseitstayswithinthedatadistribu-
(a) Causal (b) Samples
tionafterafeasibleintervention,allowingforbettermodel
Graph
generalizationthantheout-of-distributionpurplepoint.
Figure3. CausalGraphandsamplesofMorpho-MNIST.
CounterfactualsonWholeTestSet. InFig.2(b),weillus-
MorphoMNIST involves three variables (t,i,x). As de-
tratethesuperiorperformanceofourcounterfactualmethod
pictedinFig.3(a), t(digitstrokethickness)causesboth
on the test set, notably outperforming non-backtracking
i(strokeintensity)andx(images),withibeingthedirect
counterfactuals. This is evident as many outcomes from
causeofx.
non-backtracking counterfactuals for n significantly di-
3
vergefromthey = xline, showingamismatchbetween Inourexperiments,mirroringthoseinSection6.1,wein-
predictedandground-truthvalues. Incontrast,ourmethodâ€™s corporatetwokeychanges. First,weutilizetwoadvanced
outcomes largely align with this line, barring few excep- deep learning models, V-SCM (Pawlowski et al., 2020b)
tionspossiblyduetolearnedmodelâ€™simperfections. This andH-SCM(Ribeiroetal.,2023),fordoingcounterfactu-
alignmentisattributedtoourmethodâ€™sconsistentandfea- als. Second,duetotheabsenceofground-truthSCMfor
sible interventions, enhancing prediction accuracy, while assessingoutcomeerror,weadoptthecounterfactualef-
non-backtracking counterfactuals often lead to infeasible fectivenessmetricfrom(Ribeiroetal.,2023),asdeveloped
results. Table1supportsthesefindings,demonstratingthat in(Monteiroetal.,2023). Thisinvolvestrainingapredic-
ourapproachexhibitsaMAEreductionof61.6%whenap- tor on the dataset to estimate parent values (tË†,Ë†i) from a
pliedton ,comparedwiththenon-backtrackingmethod. counterfactualimagexgeneratedbylearnedmodelp(x|t,i)
2
7
2n
noitciderP
s'3n
:yNaturalCounterfactualsWithNecessaryBacktracking
6.3.3DIdentBOX
ğ‘ Inthisstudy,weemploytwopracticalpublicdatasetsfrom
ğ‘£ ğ›½ 3DIdentBOX(Bizeuletal.,2023),namelyWeak-3DIdent
â„ ğ›¼
andStrong-3DIdent. Bothdatasetssharethesamecausal
ğ‘¥
ğ‘‘ ğ›¾ graph, asdepictedinFig.4(a), whichincludesanimage
(a) CausalGraph (b) Weak Causal (c) StrongCausal variablexanditssevenparentvariables, withthreepairs
Relationship Relationship of parent variables: (h,d), (v,Î²), and (Î±,Î³), where one
Figure4.Causalgraphof3DIdentandthecausalrelationshipsof is the direct cause of the other in each pair. The primary
variables(d,h)inWeak-3DIdentandStrong-3DIdentrespectively.
distinctionbetweenWeak-3DIdentandStrong-3DIdentlies
in the strength of the causal relationships between each
d=0.26; h=0.24; d=0.07; h=0.06; d=0.62; h=0.40; d=0.02; h=0.04; d=0.40; h=0.14;
v=0.23; =1.02; v=0.09; =0.33; v=0.08; =0.96; v=0.58; =1.12; v=0.70; =1.46;
=1.29; =1.35 =1.51; =1.80 =0.00; =1.69 =1.50; =1.15 =1.58; =0.13 variable pair, with Weak-3DIdent exhibiting weaker con-
nections(Fig. 4(b)) comparedtoStrong-3DIdent (Fig.4
(c)). OurapproachmirrorstheMorphoMNISTexperiments,
usingH-SCMasthelearnedSCMwithÏµ=10âˆ’3.
InfluenceofCausalEffectStrength. AsTable3reveals,
ourmethodoutperformsnon-backtrackingonbothdatasets,
with a notably larger margin in Strong-3DIdent. This in-
creasedsuperiorityisduetoahigherincidenceofinfeasi-
(a) ResultsofNon-backtrackingCounterfactuals blehardinterventionsinnon-backtrackingcounterfactuals
withintheStrong-3DIdentdataset.
d=0.01; h=0.04; d=0.11; h=0.04; d=0.01; h=0.02; d=0.01; h=0.04; d=0.04; h=0.01;
v=0.09; =0.08; v=0.06; =0.07; v=0.02; =0.01; v=0.02; =0.20; v=0.04; =0.12;
=0.41; =0.91 =0.09; =0.29 =0.04; =0.30 =0.18; =0.02 =0.12; =0.05 VisualizationonStrong-3DIdent. Fig.5displayscounter-
factuals,withthetextabovetheevidenceimages(firstrow)
indicatingerrorsforthecounterfactualimages(secondrow).
InFig.5(a),itisevidentthatsomeimages(second,thirdand
fifthimagesinparticular),generatedbynon-backtracking
counterfactualsarelessrecognizableandhavelargererrors.
Conversely,ourcounterfactualimagesexhibitbettervisual
clarityandmoredistinctshapes,asournaturalcounterfac-
(b) ResultsofNaturalCounterfactuals tualsconsistentlyensurefeasibleinterventions,resultingin
morenatural-lookingimages.
Figure5. VisualizationResultsonStong-3DIdent(Viewtheen-
largedfigureinFig.10intheAppendix). SeeAppendixforMoreDetails. Forin-depthinformation
withtheinput(t,i),andthencomputingtheabsoluteerror ondatasets,experimentalresultsâ€™standarddeviation,model
|tâˆ’tË†|or|iâˆ’Ë†i|betweentheinput(t,i)andtheirpredicted training settings, Feasible Intervention Optimization, dif-
counterparts(tË†,Ë†i)asinferredfromcounterfactualimages. ferences between our natural counterfactuals and related
works,andmechanismdistancefornaturalcounterfactuals,
Ablation Study on Naturalness Threshold Ïµ. Table 2 refertotheAppendix.
demonstrates that our error decreases with increasing Ïµ,
regardless of whether V-SCM or H-SCM is used. This 7.Conclusion
trend suggests that a larger Ïµ sets a stricter standard for
Toaddresstheimpracticalityofhardinterventions,wepro-
naturalnessincounterfactuals,enhancingthefeasibilityof
pose â€œnatural counterfactuals,â€ a new counterfactual in-
interventionsandconsequentlyloweringpredictionerrors.
ferenceapproachmoreapplicabletoreal-worldscenarios.
Thisimprovementislikelybecausedeep-learningmodels
This method integrates a naturalness constraint and nec-
aremoreadeptatgeneralizingtohigh-frequencydata(En-
essary backtracking, forming an optimization framework
gstrometal.,2019).
thatidentifiesfeasibleinterventions, leadingtooutcomes
Table3.ResultsonWeak-3DIdentandStrong-3DIdent(abbrevi- betteralignedwithdatadistribution. Wedemonstratethe
atedasâ€œWeakâ€â€œStrongâ€forsimplicity).Forclarity,weuseâ€Nonâ€ effectivenessofourapproachindeeplearningcasestudies.
todenoteNonbacktracking. Future research will further investigate the use of natural
counterfactuals in vital areas like healthcare, economics,
Dataset - d h v Î³ Î± Î² b
andlaw.
Non 0.025 0.019 0.035 0.364 0.27 0.077 0.0042
Weak
Ours 0.024 0.018 0.034 0.349 0.221 0.036 0.0041
Non 0.100 0.083 0.075 0.387 0.495 0.338 0.0048
Stong
Ours 0.058 0.047 0.050 0.298 0.316 0.139 0.0047
8NaturalCounterfactualsWithNecessaryBacktracking
References Loshchilov,I.andHutter,F. Decoupledweightdecayreg-
ularization. In International Conference on Learning
Ash,R.B. Informationtheory. CourierCorporation,2012.
Representations,2018.
Barocas, S., Selbst, A. D., and Raghavan, M. The hid-
Lu, C., Huang, B., Wang, K., HernaÂ´ndez-Lobato, J. M.,
denassumptionsbehindcounterfactualexplanationsand
principalreasons. InFAT,2020. Zhang,K.,andSchoÂ¨lkopf,B. Sample-efficientreinforce-
ment learning via counterfactual-based data augmenta-
Bizeul, A., Daunhawer, I., Palumbo, E., SchoÂ¨lkopf, B., tion. arXivpreprintarXiv:2012.09092,2020.
Marx, A., and Vogt, J. E. 3didentbox: A toolbox for
identifiabilitybenchmarking. 2023. MaalÃ¸e,L.,Fraccaro,M.,LieÂ´vin,V.,andWinther,O. Biva:
Averydeephierarchyoflatentvariablesforgenerative
Boyd, S. P. and Vandenberghe, L. Convex optimization.
modeling. Advances in neural information processing
Cambridgeuniversitypress,2004.
systems,32,2019.
Conde,M.R. Necessitydefined: Anewroleinthecriminal
Monteiro, M., Ribeiro, F. D. S., Pawlowski, N., Castro,
defensesystem. UCLAL.Rev.,29:409,1981.
D. C., and Glocker, B. Measuring axiomatic sound-
Dash,S.,Balasubramanian,V.N.,andSharma,A. Evalu- ness of counterfactual image models. arXiv preprint
atingandmitigatingbiasinimageclassifiers: Acausal arXiv:2303.01274,2023.
perspectiveusingcounterfactuals. InProceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofCom- Mothilal,R.K.,Sharma,A.,andTan,C. Explainingma-
puterVision,pp.915â€“924,2022. chinelearningclassifiersthroughdiversecounterfactual
explanations. InFAccT,2020.
Dehghani,M.,Iliev,R.,andKaufmann,S. Causalexplana-
tionandfactmutabilityincounterfactualreasoning.Mind Oberst,M.andSontag,D. Counterfactualoff-policyeval-
&Language,27(1):55â€“85,2012. uation with gumbel-max structural causal models. In
InternationalConferenceonMachineLearning,pp.4881â€“
Dhurandhar,A.,Chen,P.-Y.,Luss,R.,Tu,C.-C.,Ting,P.,
4890.PMLR,2019.
Shanmugam,K.,andDas,P. Explanationsbasedonthe
missing: Towardscontrastiveexplanationswithpertinent Pawlowski,N.,CoelhodeCastro,D.,andGlocker,B. Deep
negatives. InNeurIPS,2018. structuralcausalmodelsfortractablecounterfactualin-
ference. InNeurIPS,2020a.
Engstrom, L., Ilyas, A., Salman, H., Santurkar, S.,
and Tsipras, D. Robustness (python library),
Pawlowski,N.,CoelhodeCastro,D.,andGlocker,B. Deep
2019. URL https://github.com/MadryLab/
structuralcausalmodelsfortractablecounterfactualin-
robustness. License: MIT.
ference. Advances in Neural Information Processing
Gerstenberg,T.,Bechlivanidis,C.,andLagnado,D.A.Back Systems,33:857â€“869,2020b.
ontrack: Backtrackingincounterfactualreasoning. In
Pearl,J. Causality. Cambridgeuniversitypress,2009.
ProceedingsoftheAnnualMeetingoftheCognitiveSci-
enceSociety,volume35,2013.
Ribeiro, F. D. S., Xia, T., Monteiro, M., Pawlowski, N.,
Hassanpour,N.andGreiner,R.Learningdisentangledrepre- andGlocker,B. Highfidelityimagecounterfactualswith
sentationsforcounterfactualregression. InInternational probabilisticcausalmodels. 2023.
ConferenceonLearningRepresentations,2019.
Sanchez,P.andTsaftaris,S.A. Diffusioncausalmodelsfor
Hiddleston,E. Acausaltheoryofcounterfactuals. NouË†s,39 counterfactualestimation. InFirstConferenceonCausal
(4):632â€“657,2005. LearningandReasoning.
Kingma, D. and Welling, M. Auto-encoding variational
SchoÂ¨lkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalch-
bayes. InICLR,2014.
brenner, N., Goyal, A., and Bengio, Y. Toward causal
Kocaoglu,M.,Snyder,C.,Dimakis,A.G.,andVishwanath, representationlearning. ProceedingsoftheIEEE,109(5):
S.Causalgan:Learningcausalimplicitgenerativemodels 612â€“634,2021.
withadversarialtraining. InInternationalConferenceon
Schut,L.,Key,O.,McGrath,R.,Costabello,L.,Sacaleanu,
LearningRepresentations.
B.,Corcoran,M.,andGal,Y. Generatinginterpretable
Lewis, D. Counterfactual dependence and timeâ€™s arrow. counterfactualexplanationsbyimplicitminimisationof
NouË†s,pp.455â€“476,1979. epistemicandaleatoricuncertainties. InAISTATS,2021.
9NaturalCounterfactualsWithNecessaryBacktracking
Verma,S.,Dickerson,J.P.,andHines,K.Counterfactualex-
planationsformachinelearning:Areview.arXivpreprint,
arXiv:2010.10596,2020.
vonKuÂ¨gelgen,J.,Mohamed,A.,andBeckers,S. Backtrack-
ing counterfactuals. arXiv preprint arXiv:2211.00472,
2022.
Wachter,S.,Mittelstadt,B.,andRussell,C. Counterfactual
explanationswithoutopeningtheblackbox: Automated
decisions and the GDPR. Harvard Journal of Law &
Technology,2018.
10NaturalCounterfactualsWithNecessaryBacktracking
A.DatasetsandMoreExperimentalResults
Inthissection,wefirstprovidedetaileddatasetssettingsandadditionalexperimentalresults. Subsequently,wepresentthe
standarddeviationofallexperimentaloutcomesinSec.A.
A.1.ToyDatasets
Wedesignfoursimulationdatasets,Toy1-4,andusethedesignedSCMstogenerate10,000datapointsasatrainingdataset
andanother10,000datapointsasatestsetforeachdataset. Fig.6showscausalgraphsofToy1-4andscatterplotmatrices
oftestdatasetsineachdataset. Theground-truthSCMsofeachdatasetarelistedbelow.
Toy1.
n =u , u âˆ¼N(0,1),
1 1 1
1
n =âˆ’n + u , u âˆ¼N(0,1),
2 1 3 2 2
n =sin[0.25Ï€(0.5n +n )]+0.2u , u âˆ¼N(0,1),
3 2 1 3 3
wheretherearethreeendogenousvariables(n ,n ,n )andthreenoisevariables(u ,u ,u ). n istheconfounderofn
1 2 3 1 2 3 1 2
andn andn andn causesn .
3 1 2 3
Toy2.
n =u , u âˆ¼N(0,1),
1 1 1
n =sin[0.2Ï€(n +2.5)]+0.2u , u âˆ¼N(0,1),
2 2 2 2
wheretherearethreeendogenousvariables(n ,n )andthreenoisevariables(u ,u ). n causesn .
1 2 1 2 1 2
Toy3.
n =u , u âˆ¼N(0,1),
1 1 1
1
n =âˆ’n + u , u âˆ¼N(0,1),
2 1 3 2 2
n =sin[0.1Ï€(n +2.0)]+0.2u , u âˆ¼N(0,1),
3 2 3 3
n =sin[0.25Ï€(n âˆ’n +2.0)]+0.2u , u âˆ¼N(0,1),
4 3 1 4 4
wheretherearethreeendogenousvariables(n ,n ,n ,n )andthreenoisevariables(u ,u ,u ,u ). n istheconfounder
1 2 3 4 1 2 3 4 1
ofn andn . (n ,n ,n )isachain,i.e.,n causesn ,followedbyn .
1 4 2 3 4 2 3 4
Toy4.
n =u , u âˆ¼N(0,1),
1 1 1
1
n =âˆ’n + u , u âˆ¼N(0,1),
2 1 3 2 2
n =sin[0.3Ï€(n +2.0)]+0.2u , u âˆ¼N(0,1),
3 2 3 3
wheretherearethreeendogenousvariables(n ,n ,n )andthreenoisevariables(u ,u ,u ). (n ,n ,n )isachain,i.e.,
1 2 3 1 2 3 1 2 3
n causesn ,followedbyn .
1 2 3
Table4.MAEResultsonToy1âˆ’4.Forsimplicity,weusedooperatorinthetabletosaveroom,andwhennaturalcounterfactualsare
referredto,domeanschange.
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking 0.477 0.382 0.297 0.315 0.488 0.472 0.436 0.488 0.230 0.179 0.166 0.446 0.429
Ours 0.434 0.354 0.114 0.303 0.443 0.451 0.423 0.127 0.136 0.137 0.158 0.443 0.327
11NaturalCounterfactualsWithNecessaryBacktracking
ğ‘›
1
ğ‘› ğ‘›
2 3
(a) Toy1 (b) Toy1
ğ‘› ğ‘›
1 2
(c) Toy2 (d) Toy2
ğ‘›
1
ğ‘› ğ‘› ğ‘›
2 3 4
(e) Toy3 (f) Toy3
ğ‘› ğ‘› ğ‘›
1 2 3
(g) Toy4 (h) Toy4
Figure6.CausalgraphsandScatterPlotMatricesofToy1-4.Figure(a)(c)(e)and(g)showcausalgraphsofToy1-4respectively.Figure
(b)(d)(f)and(h)indicatescatterplotmatricesofvariablesinToy1-4respectively.
12NaturalCounterfactualsWithNecessaryBacktracking
Inthemainpaper,wehaveexplainexperimentsonToy1indetails. AsshowninTable4,ourperformanceonToy2-4shows
bigmargincomparedwithnon-backtrackingcounterfactualssincenaturalcounterfactualsconsistentlymakeinterventions
feasible,whilepartofhardinterventionsmaynotfeasibleinnon-backtrackingcounterfactuals.
VisualizationResultsonToy1. InFig.7,largercounterfactualimagesaredisplayed,whichareidenticaltothoseshownin
Fig.2,withtheonlydifferencebeingtheirsize.
Evidence
4 Nonbacktracking 1.5
Ours
1.0
2
0.5
0 0.0
n 3=2.31 n 3=0.03
0.5
2 1.0 y=x
Nonbacktracking
1.5 Ours
4
4 3 2 1 0 1 2 3 4 1.5 1.0 0.5 0.0 0.5 1.0 1.5
n
1
x: n3's Groundtruth
(a) Outcomeerroronasinglesample (b) Groudtruth-PredictionScatterPlot
Figure7. TheVisualizationResultsonToy1.
A.2.MorphoMNIST
ğ‘¡
ğ‘– ğ‘¥
(a) CausalGraph (b) Samples (c) ScatterPlotMatrix
Figure8. CausalGraphandsamplesofMorpho-MNIST.
The MorphoMNIST comes from (Pawlowski et al., 2020b), where there are 60000 images as training set and 10,000
imagesastestdataset. Fig.8(a)showsthecausalgraphforgeneratingMorpho-MNIST;specifically,strokethicknesst
causesthebrightnessintensityi,andboththicknesstandintensityicausethedigitx. Fig.8(b)showsomesamplesfrom
Morpho-MNIST.Theground-truthSCMstogeneratetheMorpho-MNISTareasfollows:
t=0.5+u , u âˆ¼Î“(10,5),
t t
i=191Â·Ïƒ(0.5Â·u +2Â·tâˆ’5)+64, u âˆ¼N(0,1),
i i
x=SetIntensity(SetThickness(u ;t);i), u âˆ¼MNIST,
x x
whereu ,u ,andu arenoisevariables,andÏƒisthesigmoidfunction. SetThickness(Â·;t)andSetIntensity(Â·;i)arethe
t i x
operationstosetanMNISTdigitu â€™sthicknessandintensitytoiandtrespectively,andxisthegeneratedimage.
x
QuantitativeResultsofchange(i)ordo(i). WeuseV-SCMtodocounterfactualtaskofchange(i)(whereÏµ=10âˆ’3)or
do(i)withmultiplerandomseedsontestset. InTable5,thefirstcolumnshowstheMAEof(t,i),indicatingourresults
outperformthatofnon-backtracking,sinceourapproachconsistentlydeterminefeasibleinterventions.
13
n
2
noitciderP
s'3n
:yNaturalCounterfactualsWithNecessaryBacktracking
Table5. MorphoMNISTresultsofchange(i)ordo(i)usingV-SCM
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 5865 3159 0 975
tâ€™sMAE 0.283 0.159 0.460 0.000 0.450
Nonbacktracking
iâ€™sMAE 6.56 3.97 8.95 0.000 14.3
tâ€™sMAE 0.164 0.160 0.171 0.000 0.466
Ours
iâ€™sMAE 4.18 4.01 4.49 0.000 14.1
The Effectiveness of Feasible Intervention Optimization (FIO). Next, we focus on the rest four-column results. In
bothtypesofcounterfactuals,weusethesamevalueiindo(i)andchange(i). Hence,afterinference,weknowwhich
image satisfies Ïµ-natural generation in the two types of counterfactuals. In â€NC=1â€ of the table, NC indicates the set
of counterfactuals after feasible intervention optimization. Notice that NC set does not mean the results of natural
counterfactuals, since some results do still not satisfy Ïµ-natural generation after FIO. â€œNC=1â€ mean the set containing
data points satisfying Ïµ-natural generation and â€œNC=0â€ contains data not satisfying Ïµ-natural generation after feasible
interventionoptimization. Similarly,â€œNB=1â€meansthesetcontainingdatapointssatisfyingnaturalnesscriteria. (NC=1,
NB=1)presentstheintersectionofâ€œNC=1â€andâ€œNB=1â€. Similarlogicisadoptedtotheotherthreecombinations. The
numberofcounterfactualdatapointsare10,000intwotypesofcounterfactuals.
In (NC=1, NB=1) containing 5865 data points, our performance is similar to the non-backtracking, showing feasible
interventionoptimizationtendstobacktrackaslessaspossiblewhenhardinterventionshavesatisfiedÏµ-naturalgeneration.
In(NC=1,NB=0),thereare3159datapoints,whichareâ€œunnaturalâ€pointsinnon-backtrackingcounterfactuals. After
naturalcounterfactualoptimation,thishugeamountofdatapointsbecomesâ€œnaturalâ€. Here,ourapproachsignificantly
reduces errors, achieving a 62.8% reduction in thickness t and 49.8% in intensity i, the most substantial improvement
amongthefoursetsinTable5. Thenumberofpointsin(NC=0,NB=1)iszero,showingthestabilityofouralgorithmsince
ourFIOframeworkwillchangethehard,feasibleinterventionintounfeasibleintervention. Twotypesofcounterfactuals
performsimilarlyintheset(NC=0,NB=0),alsoshowingthestabilityofourapproach.
VisualizationofCounterfactualImages. Fig.9showscounterfactualimages(secondrow),basedontheevidenceimages
(first row), with intended changes on i. The third row illustrates the differences between evidence and counterfactual
images. Focusing on the first counterfactual image from non-backtracking and natural counterfactuals respectively, in
non-backtracking, despite do(i) where thickness value 4.2 should remain unchanged, the counterfactual image shows
reducedthickness,consistentwiththemeasuredcounterfactualthicknessof2.6. Incontrast,naturalcounterfactualsyield
an estimated counterfactual thickness (tâˆ— in MS) closely matching original counterfactual thickness (tâˆ— in CF), due to
backtrackingforafeasibleinterventionontheearliercausalvariablet,therebymaintaining(t,i)withinthedatadistribution.
Observingotherimagesalsoshowslargererrorsinnon-backtrackingcounterfactualimages.
A.3.3DIdentBOX
Table6.Detailsofvariablesin3DIndentBOX.Objectreferstoteapotineachimage.Thesupportofeachvariableis[âˆ’1,1].Thereal
visualrangearelistedinthecolumnVisualRange.
InformationBlock Variables Support Description VisualRange
x [-1,1] Objectx-coordinate -
Position y [-1,1] Objecty-coordinate -
z [-1,1] Objectz-coordinate -
Î³ [-1,1] Spotlightrotationangle [0â—¦,360â—¦]
Rotation Î± [-1,1] ObjectÎ±-rotationangle [0â—¦,360â—¦]
Î² [-1,1] ObjectÎ²-rotationangle [0â—¦,360â—¦]
Hue b [-1,1] BackgroundHSVcolor [0â—¦,360â—¦]
The3DIdentBOXdatasets,firstintroducedinBizeuletal.(2023),comewithofficialcodeforgeneratingcustomizedversions
ofthesedatasets. TheyconsistofimagescreatedwithBlender,eachdepictingateapotwithsevenattributes,suchasposition,
rotation,andhue,determinedbysevenground-truthvariables.
14NaturalCounterfactualsWithNecessaryBacktracking
F:t=4.7;i=252 F:t=1.2;i=87 F:t=1.1;i=74 F:t=4.8;i=252 F:t=1.1;i=70 F:t=1.2;i=85 F:t=5.6;i=254 F:t=5.4;i=254
CF:t*=4.7;i*=82 CF:t*=1.2;i*=224 CF:t*=1.1;i*=179 CF:t*=4.8;i*=110 CF:t*=1.1;i*=194 CF:t*=1.2;i*=215 CF:t*=5.6;i*=97 CF:t*=5.4;i*=97
MS:t*=2.6;i*=96 MS:t*=2.3;i*=185 MS:t*=1.9;i*=130 MS:t*=2.9;i*=100 MS:t*=2.1;i*=153 MS:t*=2.0;i*=161 MS:t*=3.1;i*=106 MS:t*=2.6;i*=108
t=2.1; i=14 t=1.1; i=39 t=0.7; i=49 t=2.0; i=10 t=1.0; i=41 t=0.8; i=54 t=2.4; i=9 t=2.8; i=11
(a) ResultsofNon-backtrackingCounterfactuals
F:t=4.7;i=252 F:t=1.2;i=87 F:t=1.1;i=74 F:t=4.8;i=252 F:t=1.1;i=70 F:t=1.2;i=85 F:t=5.6;i=254 F:t=5.4;i=254
CF:t*=1.4;i*=82 CF:t*=3.1;i*=224 CF:t*=2.7;i*=179 CF:t*=2.3;i*=110 CF:t*=3.1;i*=194 CF:t*=3.0;i*=215 CF:t*=1.7;i*=97 CF:t*=1.6;i*=97
MS:t*=1.8;i*=76 MS:t*=3.2;i*=218 MS:t*=2.7;i*=174 MS:t*=2.4;i*=106 MS:t*=3.1;i*=192 MS:t*=2.9;i*=209 MS:t*=2.0;i*=89 MS:t*=2.1;i*=97
t=0.4; i=6 t=0.1; i=6 t=0.0; i=5 t=0.2; i=4 t=0.0; i=2 t=0.1; i=6 t=0.3; i=8 t=0.5; i=0
(b) ResultsofNaturalCounterfactuals
Figure9. VisualizationResultsonMorphoMNIST:â€œFâ€standsforfactualvalues,â€œCFâ€forcounterfactualvalues,andâ€œMSâ€forestimated
counterfactualvaluesof(t,i). (âˆ†t,âˆ†i)representstheabsoluteerrorsbetweencounterfactualandestimatedcounterfactualvaluesof
(t,i).
Inourexperimentwiththe3DIdentBOX,whichcomprisessixdatasets,wefocusonthepositions-rotations-huedataset.
Weexpandthisintotwodatasets,Weak-3DIdentandStrong-3DIdent. Eachdatasetincludessevenvariables,besidesthe
imagevariablex,withspecificsoutlinedinTable6. Everyimagefeaturesateapot,withvariablescategorizedintothree
groups: positions(x,y,z),rotations(Î³,Î±,Î²),andhueb,representingseventeapotattributes,asdepictedin11(a). Fig.11
(b)illustratesthatbothdatasetssharethesamecausalgraph. Thedistributionsofseveralparentvariablesofimagexinthese
datasetsaredetailedinTable7.
VisualizationonStrong-3DIdent. InFig.10, largercounterfactualimagesaredisplayed, whichareidenticaltothose
showninFig.5,withtheonlydifferencebeingtheirsize.
A.4.StandardDeviationofExperimentalResults
Thissectionpresentsthestandarddeviationofallexperimentalresults,demonstratingthatthestandarddeviationforour
naturalcounterfactualsisgenerallylower. Thisindicatestheincreasedreliabilityofourapproach,achievedbynecessary
backtrackingtoensurecounterfactualsremainwithindatadistributions.
15NaturalCounterfactualsWithNecessaryBacktracking
Table7.DistributionsinWeak-3DIdentandStrong-3DIdent.N (y,1)referstoanormaldistributiontruncatedtotheinterval[âˆ’1,1]
wt
andN (y,1)meansanormaldistributiontruncatedtotheinterval[min(1,y+0.2),max(âˆ’1,yâˆ’0.2)],whereminandmaxindicate
st
operations that select smaller and bigger values respectively. N (Î±,1) and N (Î±,1) are identical to N (y,1) and N (y,1)
wt st wt st
respectively.U referstouniformdistribution.
Variables Weak-3DIdentDistribution Strong-3DIdentDistribution
c=(x,y,z) câˆ¼(N (y,1),U(âˆ’1,1),U(âˆ’1,1)) câˆ¼(N (y,1),U(âˆ’1,1),U(âˆ’1,1))
wt st
s=(Î³,Î±,Î²) sâˆ¼(N (Î±,1),U(âˆ’1,1),N t(z,1)) sâˆ¼(N (Î±,1),U(âˆ’1,1),N (z,1))
wt w st st
b bâˆ¼U(âˆ’1,1) bâˆ¼U(âˆ’1,1)
Table8. StandardDeviationofResultsonToy1âˆ’4.
Dataset Toy1 Toy2 Toy3 Toy4
doorchange do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n ) do(n )
1 2 1 1 2 3 1 2
Outcome n n n n n n n n n n n n n
2 3 3 2 2 3 4 3 4 4 2 3 3
Nonbacktracking0.001840.006280.004320.001640.004480.006860.00495 0.0112 0.005560.001420.0005140.006230.00238
Ours 0.004090.006840.002950.001910.001160.004610.002010.005040.005310.001550.0002350.005180.00143
Table9. StandardDeviationofResultsonMorphoMNIST
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 39.84 81.43 0.00 54.14
tâ€™sMAE 0.00322 0.00172 0.00670 0.000 0.0178
Nonbacktracking
iâ€™sMAE 0.0496 0.0596 0.0508 0.000 0.110
tâ€™sMAE 0.00137 0.00222 0.00157 0.000 0.0149
Ours
iâ€™sMAE 0.0359 0.0551 0.0157 0.000 0.0853
Table10. StandardDeviationofAblationStudyonÏµ
do(t) do(i)
Model Ïµ CFs
t i t i
- NB 0.000512 0.0172 0.00322 0.0496
10âˆ’4 0.00159 0.0210 0.00183 0.0561
V-SCM
10âˆ’3 Ours 0.00124 0.0217 0.00137 0.0359
10âˆ’2 0.000954 0.0382 0.000868 0.0556
- NB 0.000915 0.0229 0.000832 0.0245
10âˆ’4 0.000920 0.0178 0.000922 0.0138
H-SCM
10âˆ’3 Ours 0.000611 0.0206 0.000289 0.0264
10âˆ’2 0.000787 0.0244 0.000431 0.0258
Table11. StandardDeviationofResultsonWeak-3DIdentandStong-3DIdent
Dataset Counterfactuals d h v Î³ Î± Î² b
Nonbacktracking 3.68e-05 0.000133 0.000226 0.00422 0.00310 0.00357 1.29e-05
Weak-3DIdent
Ours 4.27e-05 7.22e-05 0.000249 0.00558 0.00278 0.00136 3.33e-05
Nonbacktracking 0.00233 0.000864 0.00127 0.00933 0.00307 0.00452 1.49e-05
Stong-3DIdent
Ours 0.00166 0.000774 0.000229 0.00908 0.00955 0.00816 2.97e-05
B.ProofforTheorem4.1
Theorem1fromLuetal.(2020)detailsidentifiablecounterfactualsunderacertaincondition:
TheoremB.1(IdentifiableCounterfactuals). SupposeV satisfiesthefollowingstructuralcausalmodel:
i
V :=f (PA ,U )
i i i i
16NaturalCounterfactualsWithNecessaryBacktracking
d=0.26; h=0.24; d=0.07; h=0.06; d=0.62; h=0.40; d=0.02; h=0.04; d=0.40; h=0.14;
v=0.23; =1.02; v=0.09; =0.33; v=0.08; =0.96; v=0.58; =1.12; v=0.70; =1.46;
=1.29; =1.35 =1.51; =1.80 =0.00; =1.69 =1.50; =1.15 =1.58; =0.13
(a) ResultsofNon-backtrackingCounterfactuals
d=0.01; h=0.04; d=0.11; h=0.04; d=0.01; h=0.02; d=0.01; h=0.04; d=0.04; h=0.01;
v=0.09; =0.08; v=0.06; =0.07; v=0.02; =0.01; v=0.02; =0.20; v=0.04; =0.12;
=0.41; =0.91 =0.09; =0.29 =0.04; =0.30 =0.18; =0.02 =0.12; =0.05
(b) ResultsofNaturalCounterfactuals
Figure10. VisualizationResultsonStong-3DIdent.
whereU âŠ¥PA andassumeunknownf issmoothandstrictlymonotonicw.r.t. U forfixedvaluesofPA ,Ifwehave
i i i i i
observedV =v andPA =pa ,withaninterventiondo(PA =paâˆ—),thecounterfactualoutcomeisidentifiable:
i i i i i i
V |do(PA =paâˆ—),V =v ,PA =pa (6)
i i i i i i i
WeusethistheoremtosupportourproofforTheorem4.1,whichextendsTheoremB.1toabroaderrangeofidentifiable
counterfactuals.
Proof. Weinitiallyassumethepresenceofcompleteevidence,meaningE=V. InlinewithTheoremB.1from(Luetal.,
2020),counterfactualoutcomeofB âˆˆBisidentifiablewhengiventhecounterfactualactionPA =paâˆ— ,alongside
k Bk Bk
theobservablevaluesB =b andPA =pa ,whereB isanyvariableinBandPA istheparentsetofB .
k k Bk Bk k Bk k
With this full evidence assumption, paâˆ— is conclusively determined by the complete evidence and the intervention
do(C=câˆ—),whereaspa andB
=bBk
areascertainedfromthefullevidence. Thisensuresthecounterfactualoutcome
Bk k k
ofB ,asinglevalue,isidentifiable. Extendingthislogic,thecounterfactualoutcomeofBasawhole,alsoasinglevalue,
k
isidentifiable. Consequently,p(B|do(C=câˆ—),E=e)isidentifiablewithasingularvalueasitssupport.
Next, we consider the case of partial evidence, where E âŠ‚ V. Given that p(B|do(C = câˆ—),E = e) = (cid:82) p(B|V =
v,do(C = câˆ—))p(V = v|E = e)dV and both terms p(B|V = v,do(C = câˆ—)) (treat V = v as full evidence) and
p(V=v|E=e)areidentifiable,itfollowsthatp(B|do(C=câˆ—),E=e)isalsoidentifiable.
17NaturalCounterfactualsWithNecessaryBacktracking
ğ‘
d=0.85;h=0.61; d= 0.01;h=0.19;
v= 0.19; =0.60; v=0.86; = 0.57;
=0.38; = 0.61; = 0.79; =0.46; ğ‘£ ğ›½
b=-0.76 b=-0.29
â„ ğ›¼
ğ‘¥
ğ‘‘ ğ›¾
d=0.27;h=0(.7a2);Sampdle=s0.74;h=0.25; (b) CausalGraph
v= 0.41; =0.68; v=0.20; = 0.38;
=0.92; = 0.15; = 0.83; =0.50;
b=-0.18 b=0.58
(c) Weak-3DIdent (d) Strong-3DIdent
Figure11. Samples,CausalGraph,ScatterPlotMatricesofWeak-3DIdentandStrong-3DIdent.
C.ModelTraining
Ourstudyfocusesoncounterfactualinferenceandwedirectlyusetwostate-of-the-artdeep-learningSCMmodelstolearn
SCMsamongvariablesusingadataset,i.e.,D-SCM(Pawlowskietal.,2020b)andH-SCM(Ribeiroetal.,2023).Specifically,
weusecodeof(Ribeiroetal.,2023)containingtheimplementationofD-SCMandH-SCM.TakeMorpho-MNISTasan
example,inbothtwomodels,normalizingflowsarefirstlytrainedtolearncausalmechanismsforallvariablesexceptimage
x,i.e.,(t,i),andaconditionalVAEisusedtomodelimagexgivenitsparents(t,i). ForD-SCM,theconditionalVAE
usesnormalVAEframework,whileH-SCMuseshierarchicalVAEstructure(MaalÃ¸eetal.,2019)tobettercapturethe
distributionofimages.
ToyExperiments. Inthecaseoffourtoyexperiments,weexclusivelyemployednormalizingflowsduetothefactthatall
variablesareone-dimensional. Ourtrainingregimenfortheflow-basedmodelspanned2000epochs,utilizingabatchsize
of100inconjunctionwiththeAdamWoptimizer(Loshchilov&Hutter,2018). Weinitializedthelearningrateto1Ã—10âˆ’3,
setÎ² to0.9,Î² to0.9.
1 2
MorphoMNIST.Wefirsttrainnormalizedflowstolearncausalmechanismsofthicknessandintensity(t,i). Otherhyper-
parametersaresimilartothoseoftoyexperiments. Then,wetraintwoVAE-basedmodels(D-SCMandH-SCM)tolearnx
given(t,i)respectively. Thearchitecturesofthetwomodelsareidenticalto(Ribeiroetal.,2023). D-SCMandH-SCM
underwenttrainingfor160epochs. Weemployedabatchsizeof32andutilizedtheAdamWoptimizer. Theinitiallearning
ratewassetto1eâˆ’3andunderwentalinearwarmupconsistingof100steps. WesetÎ² to0.9,Î² to0.9,andapplieda
1 2
weightdecayof0.01. Furthermore,weimplementedgradientclippingatathresholdof350andintroducedagradientupdate
skippingmechanism,withathresholdsetat500basedontheL2norm. Duringthetesting,i.e.,counterfactualinference,we
testperformanceonbothmodelsrespectively,withthenormalizedflows.
3DIdentBOX.SimilartoexperimentsonMorphoMNIST,wefirsttrainnormalizedflows. ComparedwithD-SCM,H-SCM
ismorepowerfultomodelcomplexdatalike3DIdentBOX,ofwhichthesizeoftheimageis64Ã—64Ã—4. Then,wetrain
H-SCMtocapturethedistributionofxgivenitsparentsfor500epochswithabatchsizeof32. Thehyper-parametersare
thesameasexperimentsonMorphoMNIST.
18NaturalCounterfactualsWithNecessaryBacktracking
D.FeasibleInterventionOptimization
OurlossfunctionforFeasibleInterventionOptimizationisdefinedas:
(cid:88)
L=âˆ¥an(A)âˆ’an(A)âˆ—âˆ¥ +w [max(Ïµâˆ’F(V =vâˆ—|paâˆ—),0)+max(Ïµ+F(V =vâˆ—|paâˆ—)âˆ’1,0)]
1 Ïµ j j j j j j
j (7)
s.t. A=aâˆ—
BasedonTheorem4.1,Choice(3)ofnaturalnessconstraintsisequivalenttoChoice(2),asF(V =vâˆ—|paâˆ—)=F(U =uâˆ—)
j j j j j
withvâˆ— =f (paâˆ—,uâˆ—). Hence,thelosscanberewrittenas:
j j j j
(cid:88)
L=âˆ¥an(A)âˆ’an(A)âˆ—âˆ¥ +w [max(Ïµâˆ’F(uâˆ—),0)+max(Ïµ+F(uâˆ—)âˆ’1,0)]
1 Ïµ j j
j (8)
s.t. uâˆ— =fâˆ’1(aâˆ—,paâˆ— )
A A A
Notethatu isnotexplicitlyoptimizedasitsvalue,pa ,isdeterminedbyothernoisevaluesandtheconstantaâˆ—through
A A
thereversiblefunctionfâˆ’1. Consequently,u isindirectlydeterminedbyothernoisevaluesandaâˆ—. Thelossâ€™sparameter
A A
isthusuâˆ— ,whichfullydeterminesthevaluean(A)âˆ—usingthelearnedSCM,asexplainedinSec.C.
AN
Inallexperiments,weoptimizeduâˆ— usingtheAdamWoptimizeratalearningrateof1eâˆ’3for50,000steps. This
AN
approachâ€™seffectivenessisvalidatedbytheMorphoMNISTexperiments.
E.StandardProcedureofNaturalCounterfactuals
In this section, we outline our natural counterfactualsâ€™ standard procedure, assuming full evidence. Through Feasible
InterventionOptimization,weobtaintheoptimalvaluean(A)âˆ—.Thisidentifiesthechangedvariables,C,whichwetargetfor
intervention,enablingthedefinitionofthefeasibleinterventiondo(C=câˆ—),withcâˆ—asthepost-optimizationcounterfactual
valuesofC. Then,wecandonaturalcounterfactualsgiventhefeasibleinterventiondo(C=câˆ—). Accordingly,weprovide
athree-stepstandardprocedureforourgeneralnaturalcounterfactuals:
TheoremE.1(NaturalCounterfactuals). Withthemodel<M,p(U)>andÏµ,theprobabilityp(B |e)ofacounterfactual
A
conditionâ€GiventheevidenceE=e,ifAâ€™svalueischangedintoaâˆ—duetothefeasibleinterventionsoncausallyearlier
variables,thenB,â€canbepredictedbythefollowingthreesteps.
(1) Abduction: Givenevidencee,obtainupdateddistributionp(U|e).
(2) Action: Determinefeasibleinterventiondo(C=câˆ—)byFeasibleInterventionOptimizationtoachievechange(A=
aâˆ—)andmodifythemodelintoM bycuttingoffthelinkbetweenCandtheirparents.
C
(2) Prediction:ComputetheprobabilityofB,thecounterfactualconsequence,usingtheupdatedmodel<M ,p(U|e)>.
C
F.DifferencesbetweenNaturalCounterfactualsandNon-BacktrackingCounterfactuals(Pearl,
2009)orPrior-BasedBacktrackingCounterfactuals(vonKuÂ¨gelgenetal.,2022)
F.1.DifferencesbetweenNon-backtrackingCounterfactualsandOurs
Non-backtrackingcounterfactualsonlydoadirectinterventionontargetvariableA,whileournaturalcounterfactualsdo
backtrackingwhenthedirectinterventionisinfeasible. NoticethatwhenthedirectinterventiononAisalreadyfeasible,our
procedureofnaturalcounterfactualswillbeautomaticallydistilledtothenon-backtrackingcounterfactuals. Inthissense,
non-backtrackingcounterfactualreasoningisourspecialcase.
F.2.DifferencesbetweenPrior-BasedBacktrackingCounterfactualsandOurs
(1)InterventionApproachandResultingChanges:
Prior-basedBacktrackingCounterfactuals: Thesecounterfactualsdirectlyinterveneonnoise/exogenousvariables,which
canleadtounnecessarychangesinthecounterfactualworld. Consequently,thesimilaritybetweentheactualdatapointand
19NaturalCounterfactualsWithNecessaryBacktracking
itscounterfactualcounterparttendstobelower. Inshort,prior-basedbacktrackingcounterfactualsmayintroducechanges
thatarenotneeded.
Natural Counterfactuals: In contrast, our natural counterfactuals only engage in necessary backtracking when direct
interventionisinfeasible. Thisapproachaimstoensurethatthecounterfactualworldresultsfromminimalalterations,
maintainingahigherdegreeoffidelitytotheactualworld.
(2)CounterfactualWorlds:
Prior-basedBacktrackingCounterfactuals: Thisapproachassignsvaryingweightstothenumerouspotentialcounterfactual
worldscapableofeffectingthedesiredchange. Theweightassignedtoeachworldisdirectlyproportionaltoitssimilarityto
theactualworld. itisworthnotingthatamongthisarrayofcounterfactualworlds,somemayexhibitminimalresemblance
totheactualworld,evenwhenequippedwithcompleteevidence,includingthevaluesofallendogenousvariables. This
divergencearisesbecausebysamplingfromtheposteriordistributionofexogenousvariables,evenhighlydissimilarworlds
maystillbedrawn.
NaturalCounterfactuals: Incontrast,ournaturalcounterfactualsprioritizetheconstructionofcounterfactualworldsthat
closelyemulatethecharacteristicsoftheactualworldthroughanoptimizationprocess. Asaresult,inmostinstances,one
actualworldcorrespondstoasinglecounterfactualworldwhenemployingnaturalcounterfactualswithfullevidence.
(3)ImplementationPracticality:
Prior-basedBacktrackingCounterfactuals: Thepracticalimplementationofprior-basedbacktrackingcounterfactualscanbe
adauntingchallenge. Todate,wehavebeenpreventedfromconductingacomparativeexperimentwiththisapproachdueto
uncertaintyaboutitsfeasibilityinpracticalapplications. Amongothertasks,thecomputationoftheposteriordistribution
ofexogenousvariablescanbeacomputationallyintensiveendeavor. Furthermore,itisworthnotingthatthepaper(von
KuÂ¨gelgenetal.,2022)providesonlyrudimentaryexampleswithoutpresentingacomprehensivealgorithmoraccompanying
experimentalresults.
NaturalCounterfactuals: Instarkcontrast,ournaturalcounterfactualshavebeenmeticulouslydesignedwithpracticality
at the forefront. We have developed a user-friendly algorithm that can be applied in real-world scenarios. Rigorous
experimentation,involvingfoursimulationdatasetsandtwopublicdatasets,hasconfirmedtheefficacyandreliabilityofour
approach. Thisextensivevalidationunderscorestheaccessibilityandutilityofouralgorithmfortacklingspecificproblems,
makingitavaluabletoolforpracticalapplications.
G.ObservationsaboutthePrior-BasedBacktrackingCounterfactuals(vonKuÂ¨gelgenetal.,2022)
G.1.PossibilityofGratuitousChanges
A theory of backtracking counterfactuals was recently proposed by (von KuÂ¨gelgen et al., 2022), which utilizes a prior
distributionp(U,Uâˆ—)toestablishaconnectionbetweentheactualmodelandthecounterfactualmodel. Thisapproach
allowsforthegenerationofcounterfactualresultsunderanyconditionbyconsideringpathsthatbacktracktoexogenous
noisesandmeasuringclosenessintermsofnoiseterms. Asaresult,foranygivenvaluesofE = eandAâˆ— = aâˆ—,itis
possibletofindasampledvalue(U=u,Uâˆ— =uâˆ—)fromp(U,Uâˆ—)suchthatE =eandAâˆ— =aâˆ—,asdescribed
M(u) Mâˆ—(uâˆ—)
in (von KuÂ¨gelgen et al., 2022). This holds true even in cases where V\E = âˆ… and Vâˆ— \Aâˆ— = âˆ…, implying that any
combinationofendogenousvaluesE = eandAâˆ— = aâˆ— canco-occurintheactualworldandthecounterfactualworld,
respectively. Inessence,therealwaysexistsapath(v â†’âˆ’ u â†’âˆ’ uâˆ— â†’âˆ’ vâˆ—)thatconnectsV = vandVâˆ— = vâˆ— througha
value(U=u,Uâˆ— =uâˆ—),wherevandvâˆ—representanyvaluessampledfromp (V)andp (Vâˆ—),respectively.
M Mâˆ—
However,thankstothisfeature,thisunderstandingofcounterfactualsmayallowforwhatappearstobegratuitouschanges
in realizing a counterfactual supposition. This occurs when there exists a value assignment Uâˆ— = uâˆ— that satisfies
Eâˆ— =eandAâˆ— =aâˆ—inthesameworld. Insuchacase,intuitivelyweoughttoexpectthatEâˆ— =eshouldbe
Mâˆ—(uâˆ—) Mâˆ—(uâˆ—)
maintainedinthecounterfactualworld(asinthefactualone). However,thereisingeneralapositiveprobabilityforEâˆ— Ì¸=e.
Thisisduetotheexistenceofatleastoneâ€œpathâ€fromE=etoanyvaluevâˆ—sampledfromp (Vâˆ—|Aâˆ— =aâˆ—)bymeans
Mâˆ—
ofatleastonevalue(U=u,Uâˆ— =uâˆ—),allowingEâˆ—totakeanyvalueinthesupportofp (Eâˆ—|Aâˆ— =aâˆ—).
Mâˆ—
In the case where Aâˆ— = âˆ…, an interesting observation is that Eâˆ— can take any value within the support of p (Eâˆ—).
Mâˆ—
Furthermore,whenexaminingtheupdatedexogenousdistribution,wefindthatinPearlâ€™snon-backtrackingframework,itis
givenbyp (Uâˆ—|Eâˆ— =e). However,in(vonKuÂ¨gelgenetal.,2022)â€™sbacktrackingframework,theupdatedexogenous
Mâˆ—
20NaturalCounterfactualsWithNecessaryBacktracking
distributionbecomesp (Uâˆ—|E = e) = (cid:82) p(Uâˆ—|U)p (U|E = e)d(U) Ì¸= p (Uâˆ—|Eâˆ— = e),sinceusinguâˆ— sampled
B M Mâˆ—
fromp(Uâˆ—|U=u)(whereuisanyvalueofU)canresultinanyvalueofallendogenousvariablesVâˆ—. Therefore,(von
KuÂ¨gelgenetal.,2022)â€™sbacktrackingcounterfactualdoesnotreducetoPearlâ€™scounterfactualevenwhenAâˆ— =âˆ….
G.2.IssueswiththeDistanceMeasure
In Equation 3.16 of (von KuÂ¨gelgen et al., 2022), Mahalanobis distance is used for real-valued U âˆˆ Rm, defined as
d(uâˆ—,u)= 1(uâˆ—âˆ’u)TÎ£âˆ’1(uâˆ—âˆ’u). However,itshouldbenotedthattheexogenousvariablesarenotidentifiable. There
2
areseveralissueswithusingtheMahalanobisdistanceinthiscontext.
Firstly,selectingdifferentexogenousdistributionswouldresultindifferentdistances. Thislackofidentifiabilitymakesthe
distancemeasuresensitivetothechoiceofexogenousdistributions.
Secondly,differentnoisevariablesmayhavedifferentscales. ByusingtheMahalanobisdistance,thevariableswithlarger
scaleswoulddominatethedistributionchanges,whichmaynotaccuratelyreflectthechangesineachvariablefairly.
Thirdly,eveniftheMahalanobisdistanced(uâˆ—,u)isverycloseto0,itdoesnotguaranteethatthevaluesoftheendogenous
variablesaresimilar. ThismeansthattheMahalanobisdistancealonemaynotcapturethesimilarityordissimilarityofthe
endogenousvariablesadequately.
H.AnotherTypeofMinimalChange: MinimalChangeinLocalCausalMechanisms.
Changesinlocalmechanismsarethepricewepaytodointerventions,sinceinterventionsarefromoutsidethemodeland
sometimesareimposedonamodelbyus. Hence,weconsiderminimalchangeinlocalcausalmechanismsinAâ€™sancestor
setAN(A). WithL1norm,thetotaldistanceofmechanismsinAN(A),calledmechanismdistance,isdefinedas:
(cid:88)
D(u ,u )= w ||F(u )âˆ’F(uâˆ—)||
an(A) an(A)âˆ— j j j 1 (9)
j
whereu isthevalueofAâ€™sexogenousancestorsetU whenAN(A)=an(A)intheactualworld. u is
an(A) AN(A) an(A)âˆ—
thevalueofU whenAN(A)=an(A)âˆ—inthecounterfactualworld. D(u ,u )representsthedistance
AN(A) an(A) an(A)âˆ—
between actual world and counterfactual world. w represents a fixed weight, and F(Â·) is the Cumulative Distribution
j
Function(CDF).WeemploytheCDFofnoisevariablestonormalizedistancesacrossvariousnoisedistributions,ensuring
thesedistancesfallwithintherangeof[0,1],asnoisedistributionsarenotidentifiable.
WeightsintheDistance. Thenoisevariablesareindependentofeachotherandthus,unlikeinperceptiondistance,the
changeofcausalearliernoisevariableswillnotleadthechangeofcausallaternoisenodes. Therefore,iftheweighton
differenceofeachnoisevariableisthesame,thedistancewillnotpreferlesschangeonvariablesclosertoA. Toachieve
backtrackingaslessaspossible,wesetaweightw foreachnodeU ,definedasthenumberofendogenousdecedentsof
j j
V denotedasND(V ). Generallyspeaking,forallvariablescausallyearlierthanA,onewayistousethenumberof
j j
variablesinfluencedbyparticularinterventionasthemeasureofthechangescausedbytheintervention. Hence,thenumber
ofvariablesinfluencedbyavariableâ€™sinterventioncanbetreatedasthecoefficientofdistance. Forexample,inacausal
graphwherewhereBcausesAandCistheconfounderofAandB. Ifchange(A=aâˆ—),u â€™sandu â€™sweightis1and
A B
2respectively. Inthisway,variables(e.g.,u )withbiggerinfluenceonothervariablespossessbiggerweightsandthustend
B
tochangeless,reflectingnecessarybacktracking.
H.1.ConcretizationofNaturalCounterfactuals: AnExampleMethodology
AMethodBasedonMechanismDistance. WepluginmechanismdistanceEqn.9intoFIOframeworkEqn.2. Belowis
theequationofoptimization:
(cid:88)
min w ||F(u )âˆ’F(uâˆ—)||
j j j 1
uan(A)âˆ—
j
(10)
s.t. aâˆ— =f (paâˆ— ,uâˆ— )
A A A
s.t. Ïµ<F(uâˆ—)<1âˆ’Ïµ,âˆ€uâˆ— âˆˆu
j j an(A)âˆ—
Wherethefirstconstraintistoachievechange(A=aâˆ—),thesecondconstraintrequirecounterfactualdatapointtosatisfy
Ïµ-natural generation given the optional naturalness criteria (3) in Sec. 4.2, and the optimization parameter is the value
21NaturalCounterfactualsWithNecessaryBacktracking
u ofnoisevariablesetU givenAN(A) = an(A)âˆ—. Forsimplicity,weuseAassubscriptasindicatorof
an(A)âˆ— AN(A)
terms related to A, instead of number subscript. In practice, the Lagrangian method is used to optimize our objective
function. Thelossisasbelow:
(cid:88)
L= w ||F(u )âˆ’F(uâˆ—)||
j j j 1
j
(cid:88)
+w max((Ïµâˆ’F(uâˆ—),0)+max(Ïµ+F(uâˆ—)âˆ’1,0)) (11)
Ïµ j j
j
s.t. uâˆ— =fâˆ’1(aâˆ—,paâˆ— )
A A A
Inthe nextsection, weuseEqn. 11forfeasibleintervention optimizationacross multiplemachinelearningcase
studies,showingthatmechanismdistanceisaseffectiveasperceptiondistance,asdiscussedinthemainpaper.
H.2.CaseStudy
H.2.1.MORPHOMNIST
Table12. MorphoMNISTresultsofchange(i)ordo(i)usingV-SCM
IntersectionbetweenOursandNB (NCO=1,NB=1) (NCO=1,NB=0) (NCO=0,NB=1) (NCO=0,NB=0)
NumberofIntersection 5841 3064 0 1094
tâ€™sMAE 0.286 0.159 0.462 0.000 0.471
Nonbacktracking
iâ€™sMAE 6.62 4.00 8.88 0.000 14.2
tâ€™sMAE 0.175 0.159 0.206 0.000 0.471
Ours
iâ€™sMAE 4.41 4.00 5.19 0.000 14.2
Inthissection,westudytwotypesofcounterfactualsonthedatasetcalledMorphoMNIST,whichcontainsthreevariables
(t,i,x). FromthecausalgraphshowninFig.12(a),t(thethicknessofdigitstroke)isthecauseofbothi(intensityofdigit
stroke)andx(images)andiisthedirectcauseofx. Fig.12(b)showsasamplefromthedataset. Thedatasetcontains
60000imagesasthetrainingsetand10000asthetestset.
WefollowtheexperimentalsettingsofsimulationexperimentsinSec.6.1,exceptfortwodifferences. Oneisthatweuse
twostate-of-the-artdeeplearningmodels,namelyV-SCM(Pawlowskietal.,2020b)andH-SCM(Ribeiroetal.,2023),as
thebackbonestolearncounterfactuals. Theyusenormalizingflowstolearncausalrelationshipsamongxâ€™sparentnodes,
e.g.,(t,i)inMorphoMNIST.Further,tolearnp(x|t,i),noticethatV-SCMusesVAE(Kingma&Welling,2014)andHVAE
(MaalÃ¸eetal.,2019). Anotherdifferenceisthat,insteadofestimatingtheoutcomewithMAE,wefollowthesamemetric
calledcounterfactualeffectivenessinRibeiroetal.(2023)developedbyMonteiroetal.(2023),First,trainedonthedataset,
parentpredictorsgivenavalueofxcanpredictparentvalues,i.e.,(t,i)â€™s,andthenmeasuretheabsoluteerrorbetween
parentvaluesafterhardinterventionorfeasibleinterventionandtheirpredictedvalues,whichismeasuredonimagethe
LearnedSCMgeneratesgiventheinputof(t,i).
Table13. AblationStudyonÏµ
do(t) do(i)
Model Ïµ CFs
t i t i
- NB 0.336 4.51 0.286 6.62
10âˆ’4 0.314 4.48 0.197 4.90
V-SCM
10âˆ’3 Ours 0.297 4.47 0.175 4.41
10âˆ’2 0.139 4.35 0.151 3.95
- NB 0.280 2.54 0.202 3.31
10âˆ’4 0.260 2.49 0.117 2.23
H-SCM
10âˆ’3 Ours 0.245 2.44 0.103 2.03
10âˆ’2 0.0939 2.34 0.0863 1.87
QuantitativeResultsofchange(i)ordo(i). WeuseV-SCMtodocounterfactualtaskofchange(i)(whereÏµ=10âˆ’3)or
do(i)withmultiplerandomseedsontestset. InTable12,thefirstcolumnshowstheMAEof(t,i),indicatingourresults
22NaturalCounterfactualsWithNecessaryBacktracking
ğ‘¡
ğ‘– ğ‘¥
(a) CausalGraph (b) Samples
Figure12. CausalGraphandsamplesofMorpho-MNIST.
Table14. ResultsonWeak-3DIdentandStong-3DIdent
Dataset Counterfactuals d h v Î³ Î± Î² b
Nonbacktracking 0.0252 0.0191 0.0346 0.364 0.266 0.0805 0.00417
Weak-3DIdent
Ours 0.0241 0.0182 0.0339 0.348 0.224 0.0371 0.00416
Nonbacktracking 0.104 0.0840 0.0770 0.385 0.495 0.338 0.00476
Stong-3DIdent
Ours 0.0633 0.0512 0.0518 0.326 0.348 0.151 0.00464
outperformthatofnon-backtracking. Next,wefocusontherestfour-columnresults. Inbothtypesofcounterfactuals,we
usethesamevalueiindo(i)andchange(i). Hence,afterinference,weknowwhichimagesatisfyingÏµ-naturalgeneration
inthetwotypesofcounterfactuals. Inâ€NC=1â€ofthetable,NCindicatesthesetofcounterfactualsafterfeasibleintervention
optimization. NoticethatNCsetdoesnotmeantheresultsofnaturalcounterfactuals,sincesomeresultsdostillnotsatisfy
Ïµ-naturalgenerationafterfeasibleinterventionoptimization. â€œNC=1â€meanthesetcontainingdatapointssatisfyingÏµ-natural
generationandâ€œNC=0â€containsdatanotsatisfyingÏµ-naturalgenerationafterfeasibleinterventionoptimization. Similarly,
â€œNB=1â€meansthesetcontainingdatapointssatisfyingnaturalnesscriteria. (NC=1,NB=1)presentstheintersectionof
â€œNC=1â€andâ€œNB=1â€. Similarlogicisadoptedtotheotherthreecombinations. Thenumberofcounterfactualdatapointsare
10000intwotypesofcounterfactuals.
In (NC=1, NB=1) containing 5841 data points, our performance is similar to the non-backtracking, showing feasible
interventionoptimizationtendstobacktrackaslessaspossiblewhenhardinterventionshavesatisfiedÏµ-naturalgeneration.
In(NC=1,NB=0),thereare3064datapoints,whichareâ€œunnaturalâ€pointsinnon-backtrackingcounterfactuals. After
naturalcounterfactualoptimation,thishugeamountofdatapointsbecomeâ€œnaturalâ€. Inthisset,ourapproachcontributesto
themaximalimprovementcomparedtotheotherthreesetsinTable12,improving55.4%and41.6%onthicknesstand
intensityi. Thenumberofpointsin(NC=0,NB=1)iszero,showingthestabilityofouralgorithmsinceourapproachwill
notmovethehard,feasibleinterventionintoanunfeasibleintervention. Twotypesofcounterfactualsperformsimilarlyin
theset(NC=0,NB=0),alsoshowingthestabilityofourapproach.
AblationStudyonNaturalnessThresholdÏµ. Weusetwomodels,V-SCMandH-SCM,todocounterfactualswithdifferent
valuesofÏµ. AsshowninTable13,ourerrorisreducedastheÏµincreasesusingthesameinferencemodel,sincethehigherÏµ
willselectmorefeasibleinterventions.
H.2.2.3DIDENTBOX
Inthistask,weutilizepracticalpublicdatasetscalled3DIdentBOX,whichencompassmultipledatasets(Bizeuletal.,2023).
Specifically,weemployWeak-3DIdentandStrong-3DIdent,bothofwhichsharethesamecausalgraphdepictedinFig.13
(a),consistingofanimagevariabledenotedasxandsevenparentvariables. Theseparentvariables,denotedas(d,h,v),
controlthedepth,horizonposition,andverticalpositionoftheteapotinimagexrespectively. Additionally,thevariables
(Î³,Î±,Î²)governthreetypesofanglesassociatedwiththeteapotwithinimages,whilevariablebrepresentsthebackground
coloroftheimage. AsillustratedinFig.13(a),causalrelationshipsexistamongthreepairsofparentvariables,i.e.,(h,d),
(v,Î²)and(Î±,Î³). ItisimportanttonoteadistinctionbetweenWeak-3DIdentandStrong-3DIdent. InWeak-3DIdent,there
existsaweakcausalrelationshipbetweenthevariablesofeachpair,asshowninFig.13(b),whereasinStrong-3DIdent,the
causalrelationshipisstronger,asdepictedinFig.13(c).
WefollowthesameexperimentalsetupasintheMophoMNISTexperiments. UsinganepsilonvalueofÏµ = 10âˆ’3 we
employtheH-SCMastheinferencemodel. Weconductinterventionsorchangesonthevariables(d,Î²,Î³)andtheresults
arepresentedinTable14. Inbothdatasets,ourapproachoutperformsthenon-backtrackingmethod,withStrong-3DIdent
23NaturalCounterfactualsWithNecessaryBacktracking
exhibitingamoresignificantmarginoverthenon-backtrackingmethod. Thisisbecausethenon-backtrackingmethod
encountersmoreunfeasibleinterventionswhenperforminghardinterventionsusingStrong-3DIdent.
Additionally,weperformvisualizationsonStrong-3DIdent. InFig.14,wedisplaycounterfactualoutcomesin(a)and(b),
wherethetextaboveeachimageinthefirstrow(evidence)indicatestheerrorinthecorrespondingcounterfactualoutcome
showninthesecondrow. InFig.14(a),wepresentcounterfactualimagesthatdonotmeettheÏµ-naturalgenerationcriteria
inthenon-backtrackingapproach. Incontrast,Fig.14(b)showcasesourresults,whicharenotablymorevisuallyeffective.
Thisdemonstratesthatoursolutioncanalleviatethechallengesposedbyhardinterventionsinthenon-backtrackingmethod.
ğ‘
ğ‘£ ğ›½
â„ ğ›¼
ğ‘¥
ğ‘‘ ğ›¾
(a) CausalGraph (b) WeakCausalRelationship (c) StrongCausalRelationship
Figure13.Causalgraphof3DIdentandthecausalrelationshipsofvariables(d,h)inWeak-3DIdentandStrong-3DIdentrespectively.
24NaturalCounterfactualsWithNecessaryBacktracking
d=0.29; h=0.14; d=0.33; h=0.12; d=0.12; h=0.07; d=0.26; h=0.01; d=0.23; h=0.02;
v=0.09; =1.04; v=0.62; =1.51; v=0.26; =1.32; v=0.34; =1.51; v=0.29; =1.45;
=1.43; =1.54 =1.50; =0.04 =1.60; =0.64 =1.61; =0.36 =1.73; =0.47
(a) ResultsofNon-backtrackingCounterfactuals
d=0.05; h=0.02; d=0.00; h=0.08; d=0.05; h=0.02; d=0.01; h=0.01; d=0.01; h=0.05;
v=0.03; =0.46; v=0.07; =0.04; v=0.06; =0.01; v=0.08; =0.13; v=0.04; =0.14;
=0.21; =0.02 =0.13; =0.01 =0.03; =0.00 =0.00; =0.01 =0.01; =0.01
(b) ResultsofNaturalCounterfactuals
Figure14. VisualizationResultsonStong-3DIdent.
25