Addressing Imbalance for Class Incremental Learning in Medical
Image Classification
XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
ShanghaiKeyLaboratoryofIntelligentInformationProcessing,SchoolofComputerScience,FudanUniversity
Shanghai,China
{wmtan,byan}@fudan.edu.cn
ABSTRACT
Step 1
Training Data
Deepconvolutionalneuralnetworkshavemadesignificantbreak-
Train Predict
throughsinmedicalimageclassification,undertheassumption Model 1
thattrainingsamplesfromallclassesaresimultaneouslyavailable.
However,inreal-worldmedicalscenarios,thereâ€™sacommonneed No DR Mild Moderate
tocontinuouslylearnaboutnewdiseases,leadingtotheemerging
field of class incremental learning (CIL) in the medical domain.
Step 2
Training Data
Typically,CILsuffersfromcatastrophicforgettingwhentrainedon
newclasses.Thisphenomenonismainlycausedbytheimbalance Train Predict
betweenoldandnewclasses,anditbecomesevenmorechalleng- Model 2
ingwithimbalancedmedicaldatasets.Inthiswork,weintroduce
Severe
twosimpleyeteffectiveplug-inmethodstomitigatetheadverse
effectsoftheimbalance.First,weproposeaCIL-balancedclassifi-
cationlosstomitigatetheclassifierbiastowardmajorityclasses Training Data Step 3
vialogitadjustment.Second,weproposeadistributionmarginloss Train Predict
thatnotonlyalleviatestheinter-classoverlapinembeddingspace Model 3
butalsoenforcestheintra-classcompactness.Weevaluatetheef-
fectivenessofourmethodwithextensiveexperimentsonthree Proliferative
benchmarkdatasets(CCH5000,HAM10000,andEyePACS).The
resultsdemonstratethatourapproachoutperformsstate-of-the-art : old class : new class
methods.
Figure1:Overviewoftheclassincrementallearningsetting
CCSCONCEPTS inmedicalimageclassification.Duringtheincrementalpro-
cess,thetrainingdataisonlyprovidedforthecurrentclasses,
â€¢Computingmethodologiesâ†’Computervision.
whilethedatafrompreviousstepsisnotaccessible.Ateach
step,themodelisrequiredtoperformclassificationforall
KEYWORDS
theclassesseensofar.
Classincrementallearning,medicalimageclassification,classim-
balance
modelswouldbefixedoncedeveloped,whileinrealclinicalprac-
1 INTRODUCTION
tice,thedistributionofmedicaldatafrequentlyundergoesshifts
Nowadays,deeplearninghasemergedasapowerfultoolacross overtime,primarilyduetothecontinuousemergenceofnewdis-
variousfields[7,18,23,24],includingthemedicaldomain[2,33,36]. eases,treatmentprotocols,andpatientdata[6,41].Undersuch
However,traditionaldeeplearningmethodsoftenmakeassump- circumstances,themodelneedstoincorporatenewclassknowl-
tionsaboutstationaryandindependentdatadistributions,which edgeincrementallyinsteadofretrainingthemodelwithalldata
maybeimpracticalinreal-worldscenarios.Mosttraineddiagnosis available[28].Therefore,inthiswork,wefocusonclassincremen-
tallearninginthemedicaldomain.
âˆ—Correspondingauthors:WeiminTan,BoYan.
Fig.1illustratesthesettingofclassincrementallearninginmed-
ical Image classification. Taking the EyePACS dataset [8] as an
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
example,themodelisinitiallytrainedtoclassifythreeclasses(i.e.,
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation NoDR,Mild,andModerate).Subsequently,incrementalclasses(e.g.,
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe Severe,andProliferativeDR)arriveinsequentialstepstoupdate
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
themodel.Theclassesintroducedindifferentstepsaredisjoint,and
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. themodelmustbeabletopredictallclassesseenovertime.How-
ACMMM,2024,Melbourne,Australia ever,whenupdatingthemodelwithonlynewclasses,newdata
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
tendstoerasepreviousknowledge.Thisphenomenonisknownas
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn catastrophicforgetting[16,26].
4202
luJ
81
]VC.sc[
1v86731.7042:viXraACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
Forclassincrementallearning,imbalanceddatabetweenoldand outputsofthefinallayerbutalsofortheintermediatefeaturesto
newclassesisoneoftheprimaryreasonsforcatastrophicforget- mitigaterepresentationforgetting.However,regularization-based
ting[27,54].Tothisend,numerousapproacheshavebeenproposed methodsstillsufferfromfeaturedegradationofoldknowledgedue
tostoreasmallproportionofprevioustrainingdatainmemoryand tothelimitedaccesstoolddata[50].
rehearsethemwhenlearningnewclasses[1,20,49].However,the Structure-basedmethods[15,21,31,44,50]aimtopreservethe
limitedsizeofmemorycanalsoleadtoanimbalancebetweenold learnedparametersassociatedwith oldclasseswhileincremen-
andnewclasses[32,38].Underthiscircumstance,theclassimbal- tallycreatingmodulestoenhancethemodelâ€™scapacitytoacquire
ancewillleadto(i)aclassifierbiasedtowardsthenewandmajority new knowledge. Recently, DER [50] adds a new feature extrac-
classes;and(ii)theembeddingsofnewclassesinevitablyoverlap torateachstepandthenconcatenatestheextractedfeaturesfor
withtheoldonesinthefeaturespace(i.e.theambiguitiesproblem). classification.DyTox[15]appliestransformer[12]toincremental
Inadditiontotheclassincrementallearningimbalance,manyreal- learninganddynamicallyexpandstasktokenswhenlearningnew
lifemedicaldatasetsexhibitsignificantclassimbalance[36],with classes.Nevertheless,dynamicallyaddingnewmoduleswilllead
someclasseshavingnotablyhigherinstancesintrainingsamples toanexplosioninthenumberofparametersandanincreaseinthe
thanothers,e.g.,HAM10000[42],andEyePACS[8],whichfurther independencebetweeneachfeatureextractortoharmperformance
aggravatethecatastrophicforgetting.Therefore,addressingdata innewclasses[44].
imbalanceiscrucialforclassincrementallearninginmedicalimage Memory-basedmethods[3,4,38,46,49,51]addressthechallenge
classification. offorgettingbystoringalimitednumberofrepresentativesamples
Inthispaper,weproposetwosimpleyeteffectiveplug-inloss fromoldclassesinamemorybuffer.iCaRL[38]learnstheexemplar-
objectivestotackletwochallengescausedbyimbalanceinclass baseddatarepresentationandmakespredictionsusinganearest-
incrementallearning.First,weproposeaCIL-balancedclassifica- mean-of-exemplarsclassifier.GEM[4]usesexemplarsforgradient
tionlossinsteadofthetraditionalcross-entropy(CE)losstoavoid projectiontoovercomeforgetting.Additionally,someapproaches
theissueofclassifierbias.Specifically,wefirstadjustthelogits employgenerativemodelstosynthesizeoldclasssamplesfordata
basedonthecategoryfrequencytoplacemoreemphasisonrare rehearsal[35,39,45]whileotherworksconsidersavingfeature
classesandthenintroduceascalefactortofurtherachieveabalance embeddingsinsteadofrawimages[22].Inourwork,wefollowthe
betweenoldandnewclasses.Second,toalleviatetheoverlapof memory-basedapproachtodirectlystoreasmallsubsetofoldclass
classesinthefeaturespace,weproposeadistributionmarginloss, dataforrehearsal.
anovelimprovedmarginloss,whichnotonlyfacilitatestopush
awaythedistributionsofoldandnewclassesbutalsoobtainsthe
compactintra-classclustering.Extensiveexperimentsonbench-
2.2 ClassImbalance
markdatasetsundervarioussettingsverifythesuperiorityofour
method. Class imbalance is a key challenge for class incremental learn-
Tosummarize,themaincontributionsofthispaperare: ing[20].Duetotheonlyaccesstotheclassesofthecurrentstep,
â€¢ Toreducetheclassifierbiastowardsnewandmajorityclasses, theclassifierisseverelybiased,andthereisaninevitableoverlap
andconfusionbetweenthefeaturespacerepresentationsofold
weproposeaCIL-balancedclassificationlossthatempha-
andnewclasses[27].Evenwiththelimitedsizeofthememory
sizesrareonesvialogitadjustment.
â€¢ Weintroduceanoveldistributionmarginlossthatcanef- buffer,thebiasedoptimizationbyimbalanceddatabetweenold
andnewclassesisstillacrucialproblemthatcausescatastrophic
fectivelyseparatethedistributionsofoldandnewclassesto
forgetting[32,38].Tocopewithit,SS-IL[1]isolatesthecompu-
avoidambiguitiesandrealizetheoptimizationoftheintra-
tationofthesoftmaxprobabilitiesonoldandnewclassesforbias
classcompactness.
â€¢ Extensiveexperimentsdemonstratethatourmethodcanef- compensation.BiC[49]introducesabiascorrectionlayertoaddress
thebiasinthelastfullyconnectedlayer.
fectivelyaddresstheissueofdataimbalancewiththestate-of-
Inreal-worldmedicalscenarios,mostexistingdatasetscontain
the-artperformanceachievedonthreebenchmarkdatasets:
highlyimbalancednumbersofsamples[36],whichleadstoamore
CCH5000,HAM10000,andEyePACS.
severeforgetting.Tothebestofourknowledge,LO2LN[6]isthe
2 RELATEDWORK firstattempttoaddresstheproblemofclassincrementallearningin
medicalimageclassification.First,theyutilizetheclass-balancedfo-
2.1 ClassIncrementalLearning
calloss[9]toavoidtheclassifierbias.However,theclass-balanced
Classincrementallearningaimstotrainamodelfromasequence focallossisnotspecializedandefficientforincrementallearning.
ofclasses,ensuringitsperformanceacrossalltheclasses.Existing Second,theyintroducethemarginrankingloss[20]toseparate
classincrementallearningmethodscanberoughlydividedinto oldandnewclasses.Wearguethatthisconstraintmaynotbesuffi-
threegroups:regularization-based,structure-based,andmemory- cientlyrobust,resultinginlargeclusterswithinclasses(intra-class)
based. andpotentialoverlapsbetweenclasses(inter-class).Bycontrast,in
Regularization-basedmethods[11,14,17,20,40,52]applyad- thispaper,weproposetwosimpleyeteffectiveplug-inlossobjec-
ditionalconstraintstopreventtheexistingmodelfromforgetting tives:(i)aCIL-balancedclassificationlosstoalleviateprediction
previousknowledge.LUCIR[20]constrainstheorientationofthe biasbyadjustingthelogits,and(ii)adistributionmarginlossthat
features to preserve the geometric configuration of old classes. canpushthedistributionsofoldandnewclassesawayandprovide
PODNet[14]introducesanovelspatialdistillationnotonlyforthe morecompactintra-classclusteringsimultaneously.AddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
3 METHOD follows:
In this section, we first outline the setting of class incremental ğ‘’ğ‘¥ğ‘(cid:16) ğ‘ğ‘¡ (cid:17)
l aea dr en ti an ig lei dn dm ee sd cric ipal tii om nag oe fc tl ha ess ti wfic oat pio ron p( oSe sec. d3 l.1 o) s. sT oh be jn e, cw tie vep sr :ov Ci Id Le
-
Lğ‘™ğ‘ğ‘ =âˆ’ (cid:12)
(cid:12)
(cid:12)DË†1 ğ‘¡(cid:12)
(cid:12)
(cid:12)ğ‘–âˆ‘ï¸
âˆˆDË†
ğ‘¡ğ‘™ğ‘œğ‘”
(cid:205) ğ‘—âˆˆY
1:ğ‘¡ğ‘’ğ‘¥ğ‘(cid:16) ğ‘ğ‘–, ğ‘–ğ‘¡ğ‘¦ ,ğ‘—ğ‘–
+ğ‘£
ğ‘¦ğ‘–,ğ‘—(cid:17), (4)
balancedclassificationloss(Sec.3.2)anddistributionmarginloss
where:
(Sec.3.3).
ğ‘¥3
C
d
rD
e
ğ‘–a
ğ‘¡.
l
pa1
t âˆˆa
=s rs
ei
Xsni {n
e
ğ‘¡cS
D
nc
r
ie
er
t
s1e
smt
,
amt
D
tei
she
nn
2
an
et
,
mg
at
D
ta
l
prlla
y
3
a
ll
e,.
in
e n.Sa
a.d
.
ipr
n,
nn
e
D
dgN
cin
i
ğ‘‡
ğ‘¦fi
sgo
ğ‘–ğ‘¡}
ecat
,
ta
âˆˆia
wlm
fl
Yrt
y
h
osi
,
ğ‘¡eo
mwt
r
ion
seet
s
tr
d
D
t
ha
ee
ei pğ‘¡nn
co
ğ‘¡=a
ote
rm
w
r(t
eXo
ih
std
ğ‘¡e
h
p,e
os
Yl
ğ‘›
nef
ğ‘¡
ğ‘¡qr d)o
u
ii
nm
ne
=
gn
sta
c
(cid:8)
lae
as
(
n
bğ‘¥e
o
c
eq
ğ‘–ğ‘¡f
e
l,u
.st
ğ‘¦e
a
.
Tğ‘–ğ‘¡n
s
H
h)kc
(cid:9)
ese
eğ‘› ğ‘– r=
lao
ğ‘¡
ae1sf
-, forIt this ek
ağ‘£
n
cğ‘¦
o
cğ‘–,
w
uğ‘— rn=
attï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´
ï£³
ehğ‘™
ğ‘™
ğ‘™
ğ‘™
a
cğ‘œ
ğ‘œ
ğ‘œ
ğ‘œ
t
lğ‘”
ğ‘”
ğ‘”
ğ‘”
atr
sğ‘š
ğ‘šğ‘
ğ‘
ğ‘ğ‘š
a
sğ‘ğ‘š ğ‘¦
ğ‘¦ğ‘—
idğ‘—
fiğ‘–
ğ‘–
it,
c[[
[
i=
ao>
<
tn
i00 oa0]]
l]
n,
s,
,
oof
ftii
ii
m
sff
ff
ağ‘¦ğ‘¦
ğ‘¦ğ‘¦
a
mğ‘–ğ‘–
ğ‘–ğ‘–
x
pâˆˆâˆˆ
âˆˆâˆˆ
l lo
eYY
YY
ss
ğ‘¥ğ‘¡ğ‘¡11 ,,:: ğ‘–nğ‘¡ğ‘¡ .ğ‘—ğ‘—âˆ’âˆ’
e
Ic11
âˆˆâˆˆ
ne,, sYYğ‘—ğ‘—
os
rğ‘¡1âˆˆâˆˆ
i
dt: .ğ‘¡
a
eYY
âˆ’
t
re1ğ‘¡
1
s,
,: tğ‘¡ oâˆ’
ğ‘
ğ‘–ğ‘¡1 p,,
ğ‘¦ rğ‘– io> ritğ‘
i(
ğ‘–
zğ‘¡5
,
eğ‘—)
belspaceofthemodelisallseenclassesYğ‘–:ğ‘¡ = (cid:208) ğ‘–ğ‘¡ =1Yğ‘–,where thelearningofoldandrareclasses,weemploythefollowinglogit
Yğ‘¡ âˆ©Yğ‘¡â€² = âˆ… for all ğ‘¡ â‰  ğ‘¡â€². Inspired by memory-based meth- adjustmentstrategy.Specifically,whenğ‘¦ ğ‘– âˆˆY 1:ğ‘¡âˆ’1andğ‘— âˆˆYğ‘¡ (the
ods[3,38,49],ourmethodconsistentlysamplesğ‘šrepresentative firstlineinEq.5),weinsteadrequireğ‘ ğ‘–ğ‘¡
,ğ‘¦ğ‘–
>ğ‘ ğ‘–ğ‘¡ ,ğ‘—+log(cid:0)ğ‘ ğ‘—/ğ‘š(cid:1)[>0].
win hst ica hnc ie ss uf pro dm atee dac ah fto el rd tc hl eas ts raa in nd ins gto sr te epth ğ‘¡em iscin omam ple em teo dr .y Itb su hff oe ur ldM bğ‘¡ e, H trae in nc ine, gi pt ri os cc el se sa pr lt ah ca et mw oe rer ee mqu pi hre asa isl oar ng oe lr dğ‘ cğ‘–ğ‘¡ l, ağ‘¦ sğ‘– s, ğ‘¦w ğ‘–h thic ah nm pra ek ve ios ut sh lye
.
mentionedthatonlydatafromDË† ğ‘¡ = Dğ‘¡ âˆªMğ‘¡âˆ’1isavailablefor However,ifbothğ‘¦ ğ‘–andğ‘—arewithinY 1:ğ‘¡âˆ’1(thesecondlineinEq.5),
trainingduringtheğ‘¡-thstep.
thelogitremainsunchanged,sincetheyarebotholdclasseswith
Classically,themodelatsteptcanbewrittenasthecomposition
thesamememorysize.
oftwofunctions: ğ‘“ğ‘¡ = ğ‘“ ğœƒğ‘¡ â—¦ğ‘“ ğœ™ğ‘¡(Â·),where ğ‘“ ğœ™ğ‘¡ representsafeature Fortheothertwocaseswhenğ‘¦ ğ‘– âˆˆYğ‘¡.Ifğ‘— âˆˆY 1:ğ‘¡âˆ’1(thethirdline
extractor,andğ‘“ ğœƒğ‘¡ representsaclassifier.Foraninputsampleğ‘¥ ğ‘–,its inEq.5),thetermlog(cid:0)ğ‘š/ğ‘ ğ‘¦ğ‘–(cid:1) <0suggeststhatoldclassğ‘—receives
featurerepresentationisdenotedasâ„ ğ‘–ğ‘¡ =ğ‘“ ğœ™ğ‘¡(ğ‘¥ ğ‘–).Weemploycosine moreemphasis.Ifğ‘— âˆˆYğ‘¡ (thefourthlineinEq.5),moreemphasisis
normalization[20]astheclassifierğ‘“ğ‘¡
.Consequently,thepredicted
placedontheclassğ‘¦ ğ‘– whenithasfewerinstances,andconversely,
logitğ‘ ğ‘–ğ‘¡ ,ğ‘ forclassğ‘atstepğ‘¡ canbeğœƒ calculatedfromâ„ ğ‘–ğ‘¡ as: t loh ge itf -o bc au ls anis ceo dn cc ll aa ss ss ifiğ‘— cw ath ioen nt lh oe sssi cz ae nğ‘ eğ‘— ffis ecs tm iva el ll yer r. eT dh ue cr eef to hr ee, bt ih ae s
ğ‘ ğ‘–ğ‘¡ ,ğ‘ =ğœ‚(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤ ğ‘(cid:11), (1) towardsnewandfrequentclasses.
Tofurthercontrolthebalancebetweentheoldandnewclasses,
whereğ‘¤ ğ‘ aretheweightsforclassğ‘ intheclassifierlayer,ğœ‚ isa weintroduceascalefactorğ›¾:
learnablescalar,and âŸ¨Â·,Â·âŸ© denotesthecosinesimilaritybetween
(cid:40)
twovectors.
ğ›¾ ğ‘ =
ğ›¼, ifğ‘ âˆˆY 1:ğ‘¡âˆ’1,
(6)
1, ifğ‘ âˆˆYğ‘¡,
3.2 CIL-BalancedClassificationLoss
whereğ›¼ âˆˆ [0,1]isatrade-offcoefficientforeachdataset.Withthe
Asclaimedinpreviousworks[32,36],theinherentimbalancein
helpofthisscalefactor,theCIL-balancedclassificationlosscanbe
medicaldatasetsandtheimbalanceinclassincrementallearning
writtenas:
canleadtoabiasedclassifier.Inspiredby[34],weaimtomitigate
t H oh noi ls w yi es tv hsu e ere , dfb aoy tr aaa fd m rj ou e ms mti Don Ë†rg ğ‘¡y-t ib sh ae as vl eo adg ilmi at bs e lta ehc o ac d to sr ind tei cn plg a ğ‘¡s ,t so winc hca ir ct ee hmg co e or n ny t saf ilr se l te sq au ore n fn i tn hc gy e,. Lğ‘ğ‘ğ‘ =âˆ’ (cid:12)
(cid:12)
(cid:12)DË†1 ğ‘¡(cid:12)
(cid:12)
(cid:12)ğ‘–âˆ‘ï¸
âˆˆDË†
ğ‘¡ğ‘™ğ‘œğ‘”
(cid:205)
ğ‘—âˆˆY
1ğ›¾ :ğ‘¡ğ‘¦ ğ›¾ğ‘– ğ‘—Â· Â·ğ‘’ ğ‘’ğ‘¥ ğ‘¥ğ‘ ğ‘(cid:16) (cid:16)ğ‘ ğ‘ğ‘–ğ‘¡ ğ‘–ğ‘¡, ,ğ‘¦ ğ‘—ğ‘– +(cid:17)
ğ‘£
ğ‘¦ğ‘–,ğ‘—(cid:17), (7)
memorybufferMğ‘¡âˆ’1 andthetrainingsetDğ‘¡.Hence,wedefine whichreducestheoutputvaluesfortheoldclasseswhilemaintain-
thecategoryfrequencyasfollows: ingtheoutputsforthenewclassesunchanged,therebyencouraging
ğ‘Ÿ ğ‘
=ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´
ï£³(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)D
Dğ‘ğ‘š
Ë† Ë†ğ‘ğ‘¡
ğ‘¡(cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12),
,
i if fğ‘
ğ‘
âˆˆ âˆˆY Yğ‘¡1 ,:ğ‘¡âˆ’1,
(2)
ğ›¼t
t
ch
h
la
ie
i ms
sm
s
ps
ico
rna
od
cl
ve
ri
enl
e
st mgo
te
hsp
nt
er
r
to
a
sad
it
l
geu
l
ng
ec iye
a
firl
cf
na
u
ar
i
nr
ng
t
cge
h
er
.e
I
ol
r
no fg
m
oti
h
lt
i
ds
t
isif
g
co
c
la
aor
t
snet sh
s
t
eee
st
xs ,he
t
ie
,
to
a
mil lsd
t
as
ho
u
yon
e
u
ae ffgos
h
e.
f
cC
i
a
tmo tdn
hb
es
a
ece
l
r
mq
a
eu
n
a
oe
c
s
dn
e
e
et
li
il â€™ny
n
s,
whereğ‘
ğ‘
isthenumberoftrainingsamplesforclassğ‘,and|Â·|isthe learningofnewones.Thus,determiningtheoptimalğ›¼ becomes
cardinalityofagivenset.Afterthat,weaddğ‘™ğ‘œğ‘”ğ‘Ÿ ğ‘ totheoutput crucialforachievingabettertrade-off(seeSec.4.4).Notably,when
ğ›¼ isassignedavalueof1,thecurrentCIL-balancedclassification
logitsduringtraining.Thus,thelogit-balancedclassificationloss
lossdegradestothelogit-balancedclassificationloss(Eq.4).
canbeformulatedas:
Lğ‘™ğ‘ğ‘ =âˆ’ (cid:12) (cid:12)
(cid:12)DË†1
ğ‘¡(cid:12) (cid:12)
(cid:12)ğ‘–âˆ‘ï¸
âˆˆDË†
ğ‘¡ğ‘™ğ‘œğ‘” (cid:205)
ğ‘—ğ‘’ âˆˆğ‘¥ Yğ‘ 1:ğ‘¡(cid:16) ğ‘’ğ‘ ğ‘¥ğ‘–ğ‘¡ , ğ‘ğ‘¦ğ‘–
(cid:16)
ğ‘+
ğ‘–ğ‘¡
,ğ‘™ ğ‘—ğ‘œğ‘” +ğ‘Ÿ ğ‘™ğ‘œğ‘¦ ğ‘”ğ‘–(cid:17)
ğ‘Ÿ ğ‘—(cid:17). (3)
3
I cn
l. a3
c sl sa es
ssD wini ocs
r
ut
e
lr
m
di
e
bb
n
eu
t
eat ali slo
e
ilan
yrn
oM
i vn
ega
r,
lr
t
ahg pei pn
r ee
dpL
r
ino ess
te
hs
n eta dt eio en ps fo ef at th ue reol sd paan ced [n 5e 3w
].
Toexplainhowourmethodworks,wereformulateEq.3into Toaddressthisissue,marginloss[5]isintroducedtoavoidthe
Eq.4byintroducingğ‘£ ğ‘¦ğ‘–,ğ‘— :=ğ‘™ğ‘œğ‘”ğ‘Ÿ ğ‘— âˆ’ğ‘™ğ‘œğ‘”ğ‘Ÿ ğ‘¦ğ‘–,whicharedefinedas ambiguities between old and new classes. In detail, the vanillaACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
: Anchor Embedding : Positive Distribution Algorithm1Classincrementallearningwithourmethod.
: Positive Embedding Input:IncrementaltaskdataDğ‘¡,Memoryexemplars:Mğ‘¡âˆ’1
: Negative Distribution
: Negative Embedding Output:Updatedcurrentmodel
1: //Trainingprocessinincrementalsteps(ğ‘¡ â‰¥2)
2: DË† ğ‘¡ =Dğ‘¡ âˆªMğ‘¡âˆ’1; âŠ²Rehearsal
âˆŸ
âˆŸ 3: repeat
ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½,ï¿½ï¿½ ï¿½ï¿½
ï¿½ï¿½
ï¿½ï¿½ ï¿½ï¿½,ï¿½ âˆŸï¿½
ï¿½ï¿½
4 5:
:
L Lğ‘‘ğ‘ğ‘ ğ‘šğ‘ â†â† EE qq .. 97 ;; âŠ²CIL-B âŠ²al Dan isc te rd ibC utla ios nsifi Mc aa rti go in nL Lo os ss
s
âˆŸ ï¿½ï¿½,ï¿½ï¿½ 6: Lğ‘˜ğ‘‘ â†Eq.11; âŠ²KnowledgeDistillationLoss
ï¿½ï¿½,ï¿½ï¿½ 7: //UpdatethecurrentmodelviaoptimizingLğ‘ğ‘™ğ‘™
8: Lğ‘ğ‘™ğ‘™ â†Eq.12; âŠ²OverallLoss
9: untilreachespredefinedepoch
(a) (b)
twolossterms.Thefirsttermoptimizesthetriplesbyensuring
ï¿½ï¿½
thatthedistancefromtheanchortothepositiveembeddingisless
âˆŸâˆŸ
âˆŸ ï¿½ï¿½ thanitsdistancetothenegativeclassdistributionsbythemargin
ï¿½ï¿½,ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½,ï¿½ï¿½ ï¿½ï¿½ ğ‘š,ratherthanmerelytothenegativeembeddings.Byoptimizing
ï¿½ï¿½ âˆŸ ï¿½ï¿½
thisterm,thedistributionmarginlosscanpushthesamplesofold
ï¿½ï¿½,ï¿½ï¿½
ï¿½ï¿½,ï¿½ï¿½ classesawayfromthenewclassdistributionstofacilitatetheinter-
classseparation(showninFig.2b).Thesecondtermattemptsto
(c) (d) maintaintheanchorembeddingwithinthedistributionrangeofits
correspondingclass,thusobtainingcompactintra-classclustering
Figure2:(a)Thevanillamarginlossforcesthecosinesim- (showninFig.2d).Accordingly,thedistributionmarginlosscanbe
ilarity betweenâ„ ğ‘ andğ‘¤ ğ‘ to be larger than that between formulatedas:
â„ (bğ‘ )a On ud rğ‘¤ dğ‘› isw tri it bh uo tiu ot nc mon as ri gd ie nri ln osg st ah ie md sis totr pib uu st hio â„n ğ‘s ae wp aa yra ft ri oo mn.
Lğ‘‘ğ‘š =
âˆ‘ï¸ âˆ‘ï¸ ğ‘šğ‘ğ‘¥(cid:8) 0,(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤Ë†ğ‘(cid:11) âˆ’(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤ ğ‘¦ğ‘–(cid:11) +ğ‘š(cid:9)
thedistributionofthenegativeclassinsteadofjustğ‘¤ ğ‘›,thus ğ‘–âˆˆMğ‘¡âˆ’1ğ‘âˆˆYğ‘¡ (9)
mitigatingfeaturespaceoverlap.(c)Thevanillamarginloss + âˆ‘ï¸ ğ‘šğ‘ğ‘¥(cid:8) 0,(cid:10)ğ‘¤Ë†ğ‘¦ğ‘–,ğ‘¤ ğ‘¦ğ‘–(cid:11) âˆ’(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤ ğ‘¦ğ‘–(cid:11)(cid:9),
f
m
tra ui al tys hrto
e
clsm
au
sli stn .ii (nm d)â„i Tğ‘ze hbt eeh
i
dne
ig
sin
td
rt
i
ir
s
ba
t
ua-c
tn
il
t
oas nfs
ro
md mi as rt
t
ga
h
in
e
nc ce
le
oa
n
sd ste
e
erq nu
o
sa
f
ut
i
re
t
el
s
sy g, trw
ho
ah
u
ti nc â„dh
ğ‘-
whereğ‘¤Ë†ğ‘
representğ‘– sâˆˆ tM heğ‘¡âˆ’ d1
istributionrangeofclassğ‘.Specifically,
wemodelthedatadistributionofeachclassinthefeaturespaceby
remainswithinitscorrespondingclassdistribution,enhanc-
applyingaGaussiandistributionaroundtheircentroids.However,
ingintra-classcompactness.
duetotheimbalancednumberofsamplesacrossdifferentclasses,
thefeaturesofclasseswithlimitedinstancesmaygetsqueezedinto
marginlossaimstoensurethatthedistancefromtheanchortothe anarrowareainthefeaturespace[47].Asaresult,weassignalarger
positive(embeddingoftheground-trutholdclass)islessthanthe distributionrangetothemajorityclassesandamorerestricted
distanceoftheanchorfromthenegative(embeddingofthenew rangetotheminorityclasses:
class)tomeetapredefinedmarginğ‘š,whichcanbecomputedas: ğ‘ ğ‘
ğ‘¤Ë†ğ‘ =ğ‘¤ ğ‘ +ğœ‚âˆ—ğ‘ŸË†ğ‘, ğ‘ŸË†ğ‘ = (cid:205) ğ‘ , (10)
Lğ‘š = âˆ‘ï¸ âˆ‘ï¸ ğ‘šğ‘ğ‘¥(cid:8) 0,(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤ ğ‘(cid:11) âˆ’(cid:10)â„ ğ‘–ğ‘¡,ğ‘¤ ğ‘¦ğ‘–(cid:11) +ğ‘š(cid:9), (8) ğ‘–âˆˆY 1:ğ‘¡ ğ‘–
ğ‘–âˆˆMğ‘¡âˆ’1ğ‘âˆˆYğ‘¡ whereğ‘ŸË†ğ‘ representstheinherentratioofclassğ‘ amongallseen
whereâŸ¨Â·,Â·âŸ©denotesthecosinesimilarityandthemarginğ‘šissetto
classes,andğœ‚ âˆ¼N(0,1)isaGaussiannoisewhichhasthesame
dimensionastheclassifierweight.
0.4forallexperiments.
Topreventforgettingandmaintainthediscriminationability,
However,thevanillamarginlossexhibitstwolimitations.First,
wealsoapplyknowledgedistillationloss[19]tobuildamapping
itonlyfocusesonthetriplet:anchor,positive,andnegativeembed-
betweentheoldandthecurrentmodel:
dings.Evenifthedistancefromtheanchortothenegativeexceeds
t ch loa st eto orth ee vep no osi vt eiv re lab py ,ta hem rea br ygi in nğ‘š tro, dth ue ci ir ngdi pst or ti eb nu tt ii ao ln as mm ba igy ur ie tim esai in n Lğ‘˜ğ‘‘ = (cid:12) 1 (cid:12) âˆ‘ï¸ âˆ‘ï¸ (cid:13) (cid:13) (cid:13)ğ‘ ğ‘–ğ‘¡ ,ğ‘ âˆ’ğ‘ ğ‘–ğ‘¡ ,âˆ’ ğ‘1(cid:13) (cid:13) (cid:13). (11)
classification(showninFig.2a).Second,whilethevanillamargin (cid:12) (cid:12)DË† ğ‘¡(cid:12) (cid:12)ğ‘–âˆˆDË† ğ‘¡ğ‘âˆˆY 1:ğ‘¡âˆ’1
lossaimstoseparatetheground-trutholdclassfromnewclasses Therefore,theoveralllossisdefinedas:
(maximizinginter-classdistance),itfailstoadequatelyaddressthe
Lğ‘ğ‘™ğ‘™ =Lğ‘ğ‘ğ‘ +ğœ† ğ‘‘Lğ‘‘ğ‘š+ğœ† ğ‘˜Lğ‘˜ğ‘‘, (12)
minimizationofintra-classdistance,oftenleadingtolargeintra-
classclustering(showninFig.2c). whereğœ† ğ‘‘ andğœ† ğ‘˜ arethehyper-parametersforbalancingtheim-
Toaddresstheabovelimitations,wetrytorestoretheclassdis- portanceofeachloss.Weshowtheguidelineofourmethodat
tributionanddesignanoveldistributionmarginlossthatcontains incrementalsteptinAlg.1.AddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
4 EXPERIMENTS 4-2(3steps) 4-1(5steps)
Method
4.1 ExperimentalSetups Acc Fgt Acc Fgt
Datasets.Followingthebenchmarksetting[6],weevaluatethe iCaRL[38] 93.0Â±0.2 6.8Â±1.0 91.1Â±1.8 9.0Â±3.3
performanceonCCH5000[25],HAM10000[42],andEyePACS[8]. UCIR[20] 93.9Â±0.3 4.4Â±0.9 92.0Â±1.0 5.5Â±2.6
â€¢ CCH5000:consistsofhistologicalimagesinhumancolorec- PODNET[14] 92.0Â±0.3 5.2Â±0.4 89.2Â±0.5 6.0Â±1.2
talcancer.Thisdatasetcontains8differentclasseswith625 DER[50] 93.0Â±0.5 6.4Â±1.4 91.0Â±1.7 5.6Â±1.9
imagesperclass:tumor,stroma,complex,lympho,debris, LO2LN[6] 94.6Â±0.4 4.0Â±0.8 94.5Â±0.8 3.9Â±2.0
mucosa,adipose,andempty. Ours 95.5Â±0.2 2.3Â±0.7 95.2Â±0.2 2.1Â±1.5
â€¢ HAM10000:consistsof10,015skincancerimages,including
Table1:ExperimentalresultsonCCH5000underthreedif-
seventypesofskinlesions:melanoma,melanocyticnevus,
ferentclassorders.Numbersinbolddenotethebestresults.
basalcellcarcinoma,actinickeratosis,benignkeratosis,der-
matofibroma,andvascularlesions.Thedistributionratiosfor
eachtypeareasfollows:3.27%,5.13%,10.97%,1.15%,11.11%,
66.95%,and1.42%,whichindicatesasevereclassimbalance.
3-2(3steps) 3-1(5steps)
â€¢ EyePACS:iscommonlyusedforthetaskofdiabeticretinopa- Method
Acc Fgt Acc Fgt
thy (DR) classification. EyePACS dataset contains 35,126
retinaimagesfortraining,whicharecategorizedintofive iCaRL[38] 76.3Â±3.1 20.1Â±12.6 68.3Â±2.8 25.3Â±4.5
stagesofDR.Specifically,thereare25,810imageslabeled UCIR[20] 79.1Â±1.4 16.8Â±9.5 74.1Â±3.1 16.3Â±9.1
asnoDR,2,443asmildDR,5,292asmoderateDR,873as PODNET[14] 75.6Â±2.2 20.5Â±2.1 66.3Â±2.3 17.3Â±4.8
severeDR,and708asproliferativeDRimages.Itisworth DER[50] 76.2Â±2.8 24.8Â±10.9 66.9Â±4.5 24.7Â±4.8
notingthatthisdatasetisalsohighlyimbalanced. LO2LN[6] 82.0Â±1.3 12.8Â±3.3 78.1Â±3.4 10.1Â±3.9
Evaluationprotocols.Followingtheexperimentalprotocolsin[6], Ours 85.0Â±3.1 8.0Â±3.5 80.9Â±2.9 5.2Â±2.5
weevaluateourmethodfordifferentscenarios,suchas4-1,4-2, Table2:ExperimentalresultsonHAM10000underthreedif-
3-1,and3-2.Ineachscenario,thenumbersindicatethenumberof ferentclassorders.Numbersinbolddenotethebestresults.
baseandnewclasses,respectively.Forexample,consideringthe
HAM10000datasetwith7classes,thescenarioof3-2corresponds
tolearning3classesattheinitialstepandsubsequentlyadding2
newclassesateachincrementalstep,requiringatotalof3training 3-1(3steps)
Method
steps. Acc Fgt
Metrics.Followingpreviouswork[6],weevaluateourmethod
iCaRL[38] 64.4Â±3.3 17.8Â±4.6
basedontwostandardmetrics:AverageAccuracy(Acc)andAverage
UCIR[20] 70.2Â±7.6 15.4Â±11.4
Forgetting(Fgt).Letğ‘ ğ‘¡,ğ‘– betheaccuracyofthemodelevaluatedon
PODNET[14] 63.3Â±5.4 22.8Â±4.9
thetestsetofclassesinYğ‘– aftertrainingonthefirstğ‘¡ steps.The
DER[50] 58.7Â±9.4 30.2Â±6.9
AverageAccuracyisdefinedas:
LO2LN[6] 81.9Â±2.5 -0.2Â±0.8
(cid:18) (cid:19)
Acc= ğ‘‡1 (cid:205)ğ‘‡ ğ‘¡=1 1 ğ‘¡(cid:205) ğ‘–ğ‘¡ =1ğ‘ ğ‘¡,ğ‘– , (13) Ours 82.8Â±2.8 -0.5Â±0.7
Table3:ExperimentalresultsonEyePACSunderthreedif-
whichmeasurestheaverageclassificationaccuracyofthemodel ferentclassorders.Numbersinbolddenotethebestresults.
untilstepğ‘‡.TheAverageForgettingisdefinedas:
(cid:20) (cid:21)
Fgt= ğ‘‡1 (cid:205)ğ‘‡ ğ‘¡=1 ğ‘¡âˆ’1 1(cid:205) ğ‘–ğ‘¡ =âˆ’ 11 ğ‘—âˆˆm [1,a ğ‘¡x âˆ’1](cid:0)ğ‘ ğ‘—,ğ‘– âˆ’ğ‘ ğ‘¡,ğ‘–(cid:1) , (14)
usethesamememorysettingforeverycomparedmethod,i.e.,a
whichmeasuresanestimateofhowmuchthemodelforgetsby fixednumberof20trainingexamplesperclassareselectedviathe
averagingthedeclineinaccuracyfromthepeakperformancetoits
herdingselectionstrategy[48]andstoredinmemoryM.Further-
currentperformance. more,weconductallexperimentsonthreedifferentclassorders
Comparedmethods.Todemonstratethesuperiorityofourmethod,
andreportthemeansÂ±standarddeviationsoverthreeruns.
wefirstcompareittoclassicalincrementallearningapproaches:
4.2 PerformanceComparison
iCaRL[38],UCIR[20],PODNet[14],andDER[50].Besides,we
alsocomparetothecurrentstate-of-the-artmethod:LO2LN[6]. AsshowninTab.1,2,and3,wereporttheexperimentalresults
Implementationdetails.Asin[6],weadoptacosinenormaliza- onthreebenchmarkdatasets:CCH5000,HAM10000,andEyePACS,
tionclassifierwithaResNet-18[18]backbonepre-trainedonthe respectively.
ImageNet[10].OurmethodisimplementedinPyTorch[37],and CCH5000.Wecanseethatourmethodachievesstate-of-the-art
weemploySGDwithamomentumvalueof0.9andweightdecayof performanceintermsofAccandFgtonbothsettings.Specifically,
0.0005foroptimization.Duringtraining,thebatchsizeissetto32 ourmethodsurpassesLO2LNby1.7%onthe4-2settingand1.8%
fortheCCH5000andHAM10000datasetsand128fortheEyePACS onthe4-1settingintermsofFgt,indicatingtheeffectivenessof
datasetineachlearningstep.Notethat,forafaircomparison,we ourmethodinovercomingforgetting.ACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
90
98 90
96 80
94 80
70
92
90 iCaRL 70 60
UCIR
88
PODNet
86 DER 60 50
84 LO2LN 40
Ours
82 50
0 1 2 3 4 0 1 2 3 4 0 1 2
Incremental Step Incremental Step Incremental Step
(a)4-1onCCH5000 (b)3-1onHAM10000 (c)3-1onEyePACS
Figure3:AccuracyateachsteponCCH5000,HAM10000,andEyePACS.
50
14
40
12 40
10 30
30
8
6 20 20
4
10 10
2
0
0 0
0 1 2 3 4 0 1 2 3 4 0 1 2
Incremental Step Incremental Step Incremental Step
(a)4-1onCCH5000 (b)3-1onHAM10000 (c)3-1onEyePACS
Figure4:ForgettingateachsteponCCH5000,HAM10000,andEyePACS.
HAM10000.DifferentfromtheCCH5000dataset,theHAM10000 Lğ‘ğ‘ğ‘ Lğ‘‘ğ‘š Acc Fgt
datasetisahighlyimbalanceddermoscopydataset.Experimental
resultsdemonstratethatourmethodsignificantlyimprovesthe (cid:37) (cid:37) 67.6Â±1.6 30.4Â±11.9
performanceontheHAM10000dataset,benefitingfromthestrong (cid:37) (cid:33) 80.2Â±2.8 8.6Â±4.0
abilitytoaddressclassimbalance.Tobemorespecific,compared (cid:33) (cid:37) 83.6Â±2.9 13.1Â±6.0
withtheSOTAmethod,weimprovetheaccuracyfrom82.0%to
(cid:33) (cid:33) 85.0Â±3.1 8.0Â±3.5
85.0%onthe3-2setting.Onthe3-1setting,weachieveanoverall
performanceof80.9%,whichis2.8%higherthanLO2LNâ€™s78.1%. Table4:Performancecontributionofeachcomponentonthe
Moreover,theaverageforgettingisalsoreducedby4.8%(3-2setting) HAM100003-2setting.
and4.9%(3-1setting).
EyePACS.Furthermore,wepresentacomparisonofdifferentmeth-
odsonthechallengingEyePACSdataset.Ourproposedmethod
notonlydemonstratessignificantlyhigheraverageaccuracybut and our method over time. This demonstrates that our method
alsoachievesloweraverageforgettingthantheotherbaselines. benefitsclassincrementallearninginmedicalimageclassification
Notably,itsurpassesLO2LNby0.9%intermsofAccandoutper- andoutperformspriorworks.
formsPODNetandDERbysubstantialmarginsof19.5%and24.1%, Forgetting.Fig.4depictstheaverageforgettingacrosseachincre-
respectively. mentalstepforthreedatasets.Theforgettingofmostmethodsin-
creasesrapidlyasnewclassesarrive,whileourmethodconsistently
4.3 AnalysisofIncrementalPerformance outperformstheSOTAmethods,indicatingimprovedresilienceto
catastrophicforgetting.
Accuracy.AsshowninFig.3,wedisplaytheaverageincremental
performanceofeachstepforthreedatasets.Accordingtothese
4.4 AblationStudy
curves,itisevidentthattheperformancesofallmethodsaresimilar
inthefirststep,butthebaselinessufferfromasignificantdropas Impactofeachcomponent.InTab.4,wepresentanablation
thelearningstepsincrease.Incontrast,ourmethodeffectivelyslows analysisontheHAM100003-2settingtoevaluatetheeffectofeach
downthedrop,leadingtoanincreasinggapbetweenthebaselines proposedcomponent.Thefirstrowreferstothebaseline,whichis
)%(
ycaruccA
)%(
gnittegroF
)%(
ycaruccA
)%(
gnittegroF
)%(
ycaruccA
)%(
gnittegroFAddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
Figure5:Thet-SNEvisualizationoffeaturedistributionsofw/omarginloss(left),marginrankingloss(middle),andour
distributionmarginloss(right)ontheCCH5000dataset.
Classificationloss Acc Fgt Method Acc Fgt
CE 80.2Â±2.8 8.6Â±4.0 iCARL 68.3Â±2.8 25.3Â±4.5
Focal[29] 81.5Â±2.4 12.6Â±5.2 +Lğ‘‘ğ‘š 69.2Â±3.1 +0.9 23.7Â±8.1 +1.6
CBFocal[9] 82.9Â±3.1 14.5Â±6.3 +Lğ‘ğ‘ğ‘ 70.8Â±2.8 +2.5 21.9Â±6.5 +3.4
logit-balanced(Eq.4) 83.1Â±3.0 14.2Â±6.0
+Lğ‘ğ‘ğ‘ +Lğ‘‘ğ‘š 71.6Â±2.4 +3.3 19.6Â±7.3 +5.7
CIL-balanced(Eq.7) 85.0Â±3.1 8.0Â±3.5 UCIR 74.1Â±3.1 16.3Â±9.1
Table5:Performanceofdifferentclassificationlossesonthe
+Lğ‘‘ğ‘š 75.7Â±4.6 +1.6 13.9Â±8.2 +2.4
HAM100003-2setting. +Lğ‘ğ‘ğ‘ 76.4Â±3.4 +2.3 2.1Â±4.0 +14.2
+Lğ‘ğ‘ğ‘ +Lğ‘‘ğ‘š 77.1Â±4.6 +3.0 1.5Â±6.0 +14.8
Table7:ImpactofintegratingtheCIL-balancedclassification
Marginloss Acc Fgt lossLğ‘ğ‘ğ‘ andthedistributionmarginlossLğ‘‘ğ‘šwithexisting
methodsontheHAM10003-1setting.Theredhighlightsthe
w/omarginloss 83.6Â±2.9 13.1Â±6.0
relativeimprovement.
Marginrankingloss[20] 83.9Â±2.4 12.6Â±5.1
Distributionmarginloss(Eq.9) 85.0Â±3.1 8.0Â±3.5
Table 6: Performance of different margin losses on the
HAM100003-2setting. proposedmethodsconsistentlyoutperformtheotherclassification
lossobjectives,indicatingtheeffectivenessofthemtoaddressthe
imbalanceissue.Furthermore,theCIL-balancedclassificationloss
(Eq.7)achievesanadditional1.9%improvementcomparedtothe
trainedwiththecross-entropyloss(CE)andtheknowledgedistilla- logit-balancedclassificationloss(Eq.4),benefitingfromthescale
tionlossLğ‘˜ğ‘‘.Firstly,weobservethatthedistributionmarginloss factorğ›¾ tostrengthenthelearningofoldclasses.
Lğ‘‘ğ‘š bringsasignificantcontributionwhenappliedalone,improv- EffectofDistributionMarginLoss.Toverifytheeffectiveness
ingtheperformanceby12.6%intermsofAcc.Secondly,whenwe ofourdistributionmarginlossobjectives,weconductexperiments
replaceCEwiththeCIL-balancedclassificationlossLğ‘ğ‘ğ‘,theaver- ontheHAM100003-2settingcombinedwiththeknowledgedis-
ageaccuracyisimprovedfrom67.6%toanotable83.6%.Finally,the tillationloss Lğ‘˜ğ‘‘ andourCIL-balancedclassificationloss Lğ‘ğ‘ğ‘.
combinationofLğ‘‘ğ‘š andLğ‘ğ‘ğ‘ furtherimprovestheperformance, TheresultspresentedinTab.6demonstratethatourdistribution
demonstratingtheeffectofbothproposedcomponents. marginlossbringssignificantimprovementscomparedtocases
EffectofCIL-BalancedClassificationLoss.Weinvestigatethe withoutthemarginlossandwiththemarginrankingloss[20].
impactofdifferentclassificationlossesontheHAM100003-2setting To further illustrate the advantages of our method, we present
whencombinedwiththeknowledgedistillationlossLğ‘˜ğ‘‘ andour t-SNE[43]visualizationsoffeaturedistributionswithdifferentmar-
distribution margin loss Lğ‘‘ğ‘š. As shown in Tab. 5, we present ginlossobjectivesfortheCCH5000dataset,asshowninFig.5.In
resultsofusingcross-entropyloss(CE),focalloss(Focal)[29],class- theabsenceofmarginloss,weobservelargeintra-classclusters
balancedfocalloss(CBFocal)[9],andourproposedmethods(logit- (bluerectangle)andsignificantinter-classoverlapinfeaturespace
balancedandCIL-balanced).Itcanbeobservedthatbothofour (redcircle).Whenemployingthemarginrankingloss,theissueofACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
0.1 84.1 84.3 84.4 84.2 83.0 85.0 99 56 CCH5000
Method
50-10(6steps) 50-5(11steps)
84.5 94 Conv LT Conv LT
0.25 84.3 84.3 84.4 84.8 83.7 0.1 0.3 0.5 0.7 0.9
86
HAM10000 UCIR[20] 61.2 42.7 58.7 42.2 0.5 84.2 84.3 84.6 84.8 84.3 84.0 84
82 PODNET[14] 63.2 44.1 61.2 44.0
0.75 84.2 84.3 84.6 85.0 84.2 0.1 0.3 0.5 0.7 0.9
83.5 84 LWS[30] 64.6 44.4 62.6 44.4
EyePACS
1.0 84.1 84.5 84.5 84.3 83.8 83 Ours 65.8 48.2 63.9 47.5
0.1 0.2 0.3 0.4 0.5 83.0 82 0.1 0.3 0.5 0.7 0.9
d Table8:AverageaccuracyonCIFAR100intheconventional
(a) (b) (Conv)andlong-tailed(LT)scenarios.Numbersinboldde-
notethebestresults.
Figure6:Sensitivitystudyofhyper-parameters.(a)ğœ†
ğ‘‘
andğœ†
ğ‘˜
ontheHAM100003-2setting.(b)ğ›¼ onthreedatasets.
60
75
overlapismitigatedtosomeextent(redcircle)comparedtothe 55
70
methodwithoutmarginloss.Finally,byoptimizingourdistribution 50
marginloss,weachieveamorepronouncedseparationbetween 65 45
thedistributionsofoldandnewclasses(redcircle),whilesimul- 60 40
UCIR
taneouslyensuringthattherepresentationsofoldclassesbecome 55 PODNet 35
LWS
morecompact(bluerectangle). 50 Ours 30
Abilitytointegratewithotherexistingmethods.Ourproposed 50 60 70 80 90 100 50 60 70 80 90 100
Incremental Step Incremental Step
methodscanbeeasilyintegratedwithotherexistingCILmethods.
(a)50-5(Conv)onCIFAR100 (b)50-5(LT)onCIFAR100
Todemonstratethis,weconductexperimentsutilizingiCaRL[38]
andUCIR[20]ontheHAM100003-1setting.Specifically,were-
Figure7:IncrementalaccuracyonCIFAR10050-5settingfor
placetheclassificationlossineachbaselinewithourCIL-balanced
bothconventional(Conv)andlong-tailed(LT)scenarios.
classificationlossandincorporateourdistributionmarginloss.As
showninTab.7,theaccuracy(Acc)forbothbaselinescanbeim-
provedbyabout3%withtheintegrationofourmethods.More
notably,ourapproacheseffectivelyreduceforgetting(Fgt)by5.7%
in[30],weconductexperimentsunderbothconventional(Conv)
and14.8%foriCaRLandUCIR,respectively.
and long-tail (LT)scenarios. In theconventionalscenario, each
Sensitivitystudyofhyper-parameters.Inthispaper,thereare
classhas500trainingsamplesfortraining.Conversely,thelong-
threehyper-parametersduringtraining:theweightofthedistribu-
tailedscenariofollowsanexponentialdecayinsamplesizesacross
tionmarginlossğœ† ğ‘‘,theweightoftheknowledgedistillationloss
classes,wheretheratiobetweentheleastandthemostfrequent
ğœ† ğ‘˜,andthetrade-offcoefficientğ›¼.Wefirstconductexperiments
classis0.01.AsillustratedinTab.8,ourmethodachievessuperior
toexploretheimpactsofdifferentğœ† ğ‘‘ andğœ† ğ‘˜ ontheHAM10000
resultsinallsettings.Specifically,weobserveamoresignificant
3-2setting.AsshowninFig.6a,wevaryğœ† ğ‘‘ withintherangeof
improvementinthelong-tailscenario,furthervalidatingtheeffec-
{0.1,0.2,0.3,0.4,0.5},andğœ† ğ‘˜withintherangeof{0.1,0.25,0.5,0.75,
tivenessofourmethodinaddressingtheclassimbalanceproblem
1.0},resultinginatotalof25comparedresults.Fromtheresults,
inclassincrementallearning.Furthermore,wepresentthedynamic
weconsistentlyobservesatisfactoryperformancefromourmodel,
performancechangesduringtheincrementallearningprocessin
demonstratingitsrobustnesstotheselectionofğœ† ğ‘‘ andğœ† ğ‘˜.
Fig.7.Itisevidentthatwithmorelearningsteps,thegapbetween
Toinvestigatetheimpactofdifferentvaluesofğ›¼ onaddressing
thebaselinesandourmethodwidens,andourmethodâ€™sperfor-
theimbalancebetweentheoldandnewclasses,weevaluatethe
manceremainssuperioracrossdifferentscenarios(conventional
accuracybyvaryingğ›¼ from{0.1,0.3,0.5,0.7,0.9}onthreebench-
andlong-tailed)throughoutmostofthelearningsteps.
markdatasets.AsshowninFig.6b,theresultsindicatethatthe
accuracygraduallyimprovesasğ›¼ growslargerinitially,whileit
startstodeclinewhenğ›¼iscloseto1.Sincethedatadistributiondif- 5 CONCLUSION
fersacrossdatasets,theselectionofthetrade-offcoefficientğ›¼ also Inthispaper,weproposetwosimpleyeteffectiveplug-inlossfunc-
varies.Specifically,theoptimalvaluesofğ›¼ forthethreedatasets tionsforclassincrementallearninginmedicalimageclassification.
are0.7,0.5,and0.3,respectively. First,toaddressthechallengeofclassifierbiascausedbyclassim-
Longerincrementallearning.Inclassincrementallearning,a balance,weintroduceaCIL-balancedclassificationlossvialogit
keychallengeiscatastrophicforgetting,whichbecomesmorepro- adjustment.Second,weproposeanoveldistributionmarginloss
nouncedasthenumberoflearningclassesincreases[13,20,38]. thataimstoenforceinter-classdiscrepancyandintra-classcom-
To quantify the robustness of our method in overcoming cata- pactnesssimultaneously.Ourextensiveexperimentalevaluation
strophicforgetting,weevaluateitontwolonger-stepprotocols: demonstratesthestate-of-the-artperformanceofourmethodacross
50-10(6steps)and50-5(11steps),employingthemorechallenging variousscenariosonmedicalimagedatasets:CCH5000,HAM10000,
CIFAR100dataset.Followingtheexperimentalprotocoloutlined andEyePACS.
k
ccA
ccA
ccA
)%(
ycaruccA
)%(
ycaruccAAddressingImbalanceforClassIncrementalLearninginMedicalImageClassification ACMMM,2024,Melbourne,Australia
ACKNOWLEDGMENTS
[22] AhmetIscen,JeffreyZhang,SvetlanaLazebnik,andCordeliaSchmid.2020.
Memory-efficientincrementallearningthroughfeatureadaptation.InComputer
ThisworkwassupportedinpartbyNSFC(No.U2001209,62372117).
Visionâ€“ECCV2020:16thEuropeanConference,Glasgow,UK,August23â€“28,2020,
ThecomputationsinthisresearchwereperformedusingtheCFFF Proceedings,PartXVI16.Springer,699â€“715.
platformofFudanUniversity. [23] XuhaoJiang,WeiminTan,RiCheng,ShiliZhou,andBoYan.2022.Learningpar-
allaxtransformernetworkforstereoimagejpegartifactsremoval.InProceedings
ofthe30thACMInternationalConferenceonMultimedia.6072â€“6082.
[24] XuhaoJiang,WeiminTan,TianTan,BoYan,andLiquanShen.2023. Multi-
REFERENCES modalitydeepnetworkforextremelearnedimagecompression.InProceedings
oftheAAAIConferenceonArtificialIntelligence,Vol.37.1033â€“1041.
[1] HongjoonAhn,JihwanKwak,SubinLim,HyeonsuBang,HyojunKim,andTaesup [25] JakobNikolasKather,Cleo-AronWeis,FrancescoBianconi,SusanneMMelchers,
Moon.2021.Ss-il:Separatedsoftmaxforincrementallearning.InProceedingsof LotharRSchad,TimoGaiser,AlexanderMarx,andFrankGerritZÃ¶llner.2016.
theIEEE/CVFInternationalconferenceoncomputervision.844â€“853. Multi-classtextureanalysisincolorectalcancerhistology.Scientificreports6,1
[2] DiegoArdila,AtillaPKiraly,SujeethBharadwaj,BokyungChoi,JoshuaJReicher, (2016),1â€“11.
LilyPeng,DanielTse,MozziyarEtemadi,WenxingYe,GregCorrado,etal.2019. [26] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,Guillaume
End-to-endlungcancerscreeningwiththree-dimensionaldeeplearningonlow- Desjardins,AndreiARusu,KieranMilan,JohnQuan,TiagoRamalho,Agnieszka
dosechestcomputedtomography.Naturemedicine25,6(2019),954â€“961. Grabska-Barwinska,etal.2017.Overcomingcatastrophicforgettinginneural
[3] EdenBelouadahandAdrianPopescu.2019. Il2m:Classincrementallearning networks.Proceedingsofthenationalacademyofsciences114,13(2017),3521â€“
withdualmemory.InProceedingsoftheIEEE/CVFinternationalconferenceon 3526.
computervision.583â€“592. [27] KibokLee,KiminLee,JinwooShin,andHonglakLee.2019.Overcomingcata-
[4] ArslanChaudhry,Marcâ€™AurelioRanzato,MarcusRohrbach,andMohamedElho- strophicforgettingwithunlabeleddatainthewild.InProceedingsoftheIEEE/CVF
seiny.2018.Efficientlifelonglearningwitha-gem.arXivpreprintarXiv:1812.00420 InternationalConferenceonComputerVision.312â€“321.
(2018). [28] ZhizhongLiandDerekHoiem.2017.Learningwithoutforgetting.IEEEtransac-
[5] GalChechik,VarunSharma,UriShalit,andSamyBengio.2010. Largescale tionsonpatternanalysisandmachineintelligence40,12(2017),2935â€“2947.
onlinelearningofimagesimilaritythroughranking.JournalofMachineLearning [29] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDollÃ¡r.2017.
Research11,3(2010). Focallossfordenseobjectdetection.InProceedingsoftheIEEEinternational
[6] EvelynChee,MongLiLee,andWynneHsu.2023.Leveragingoldknowledge conferenceoncomputervision.2980â€“2988.
tocontinuallylearnnewclassesinmedicalimages.InProceedingsoftheAAAI [30] XialeiLiu,Yu-SongHu,Xu-ShengCao,AndrewDBagdanov,KeLi,andMing-
ConferenceonArtificialIntelligence,Vol.37.14178â€“14186. MingCheng.2022.Long-tailedclassincrementallearning.InEuropeanConference
[7] Liang-ChiehChen,GeorgePapandreou,FlorianSchroff,andHartwigAdam.2017. onComputerVision.Springer,495â€“512.
Rethinkingatrousconvolutionforsemanticimagesegmentation.arXivpreprint [31] YaoyaoLiu,BerntSchiele,andQianruSun.2021.Adaptiveaggregationnetworks
arXiv:1706.05587(2017). forclass-incrementallearning.InProceedingsoftheIEEE/CVFconferenceon
[8] JorgeCuadrosandGeorgeBresnick.2009.EyePACS:anadaptabletelemedicine ComputerVisionandPatternRecognition.2544â€“2553.
systemfordiabeticretinopathyscreening.Journalofdiabetesscienceandtech- [32] YaoyaoLiu,YutingSu,An-AnLiu,BerntSchiele,andQianruSun.2020.Mnemon-
nology3,3(2009),509â€“516. icstraining:Multi-classincrementallearningwithoutforgetting.InProceedings
[9] YinCui,MenglinJia,Tsung-YiLin,YangSong,andSergeBelongie.2019.Class- oftheIEEE/CVFconferenceonComputerVisionandPatternRecognition.12245â€“
balancedlossbasedoneffectivenumberofsamples.InProceedingsoftheIEEE/CVF 12254.
conferenceoncomputervisionandpatternrecognition.9268â€“9277. [33] ScottMayerMcKinney,MarcinSieniek,VarunGodbole,JonathanGodwin,
[10] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.2009.Imagenet: NatashaAntropova,HutanAshrafian,TrevorBack,MaryChesus,GregSCor-
Alarge-scalehierarchicalimagedatabase.In2009IEEEconferenceoncomputer rado,AraDarzi,etal.2020.InternationalevaluationofanAIsystemforbreast
visionandpatternrecognition.Ieee,248â€“255. cancerscreening.Nature577,7788(2020),89â€“94.
[11] SonglinDong,XiaopengHong,XiaoyuTao,XinyuanChang,XingWei,and [34] AdityaKrishnaMenon,SadeepJayasumana,AnkitSinghRawat,HimanshuJain,
YihongGong.2021.Few-shotclass-incrementallearningviarelationknowledge AndreasVeit,andSanjivKumar.2020.Long-taillearningvialogitadjustment.
distillation.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.35. arXivpreprintarXiv:2007.07314(2020).
1255â€“1263. [35] OleksiyOstapenko,MihaiPuscas,TassiloKlein,PatrickJahnichen,andMoin
[12] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,Xi- Nabi.2019.Learningtoremember:Asynapticplasticitydrivenframeworkfor
aohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,Georg continuallearning.InProceedingsoftheIEEE/CVFconferenceoncomputervision
Heigold,SylvainGelly,etal.2020.Animageisworth16x16words:Transformers andpatternrecognition.11321â€“11329.
forimagerecognitionatscale.arXivpreprintarXiv:2010.11929(2020). [36] ÅabanÃ–ztÃ¼rkandTolgaÃ‡ukur.2022.Deepclusteringviacenter-orientedmargin
[13] ArthurDouillard,YifuChen,ArnaudDapogny,andMatthieuCord.2021.Plop: free-tripletlossforskinlesiondetectioninhighlyimbalanceddatasets. IEEE
Learningwithoutforgettingforcontinualsemanticsegmentation.InProceedings JournalofBiomedicalandHealthInformatics26,9(2022),4679â€“4690.
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.4040â€“4050. [37] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[14] ArthurDouillard,MatthieuCord,CharlesOllion,ThomasRobert,andEduardo Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
Valle.2020. Podnet:Pooledoutputsdistillationforsmall-tasksincremental Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
learning.InComputerVisionâ€“ECCV2020:16thEuropeanConference,Glasgow,UK, inneuralinformationprocessingsystems32(2019).
August23â€“28,2020,Proceedings,PartXX16.Springer,86â€“102. [38] Sylvestre-AlviseRebuffi,AlexanderKolesnikov,GeorgSperl,andChristophH
[15] ArthurDouillard,AlexandreRamÃ©,GuillaumeCouairon,andMatthieuCord.2022. Lampert.2017.icarl:Incrementalclassifierandrepresentationlearning.InPro-
Dytox:Transformersforcontinuallearningwithdynamictokenexpansion.In ceedingsoftheIEEEconferenceonComputerVisionandPatternRecognition.2001â€“
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. 2010.
9285â€“9295. [39] HanulShin,JungKwonLee,JaehongKim,andJiwonKim.2017. Continual
[16] RobertMFrench.1999.Catastrophicforgettinginconnectionistnetworks.Trends learningwithdeepgenerativereplay.Advancesinneuralinformationprocessing
incognitivesciences3,4(1999),128â€“135. systems30(2017).
[17] XuzeHao,XuhaoJiang,WenqianNi,WeiminTan,andBoYan.2024.Prompt- [40] JamesSmith,Yen-ChangHsu,JonathanBalloch,YilinShen,HongxiaJin,and
GuidedSemantic-awareDistillationforWeaklySupervisedIncrementalSemantic ZsoltKira.2021. Alwaysbedreaming:Anewapproachfordata-freeclass-
Segmentation.IEEETransactionsonCircuitsandSystemsforVideoTechnology incrementallearning.InProceedingsoftheIEEE/CVFInternationalConferenceon
(2024). ComputerVision.9374â€“9384.
[18] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.2016.Deepresidual [41] LeSun,MingyangZhang,BenyouWang,andPrayagTiwari.2023. Few-Shot
learningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputer Class-IncrementalLearningforMedicalTimeSeriesClassification.IEEEJournal
visionandpatternrecognition.770â€“778. ofBiomedicalandHealthInformatics(2023).
[19] GeoffreyHinton,OriolVinyals,andJeffDean.2015.Distillingtheknowledgein [42] PhilippTschandl,CliffRosendahl,andHaraldKittler.2018. TheHAM10000
aneuralnetwork.arXivpreprintarXiv:1503.02531(2015). dataset,alargecollectionofmulti-sourcedermatoscopicimagesofcommon
[20] SaihuiHou,XinyuPan,ChenChangeLoy,ZileiWang,andDahuaLin.2019. pigmentedskinlesions.Scientificdata5,1(2018),1â€“9.
Learningaunifiedclassifierincrementallyviarebalancing.InProceedingsofthe [43] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdatausingt-SNE.
IEEE/CVFconferenceonComputerVisionandPatternRecognition.831â€“839. Journalofmachinelearningresearch9,11(2008).
[21] BingchenHuang,ZhinengChen,PengZhou,JiayinChen,andZuxuanWu.2023. [44] Fu-YunWang,Da-WeiZhou,Han-JiaYe,andDe-ChuanZhan.2022. Foster:
Resolvingtaskconfusionindynamicexpansionarchitecturesforclassincre- Featureboostingandcompressionforclass-incrementallearning.InComputer
mentallearning.InProceedingsoftheAAAIConferenceonArtificialIntelligence, Visionâ€“ECCV2022:17thEuropeanConference,TelAviv,Israel,October23â€“27,2022,
Vol.37.908â€“916. Proceedings,PartXXV.Springer,398â€“414.ACMMM,2024,Melbourne,Australia XuzeHao,WenqianNi,XuhaoJiang,WeiminTan*,BoYan*
[45] LiyuanWang,KuoYang,ChongxuanLi,LanqingHong,ZhenguoLi,andJun [50] ShipengYan,JiangweiXie,andXumingHe.2021.Der:Dynamicallyexpandable
Zhu.2021.Ordisco:Effectiveandefficientusageofincrementalunlabeleddata representationforclassincrementallearning.InProceedingsoftheIEEE/CVF
forsemi-supervisedcontinuallearning.InProceedingsoftheIEEE/CVFConference ConferenceonComputerVisionandPatternRecognition.3014â€“3023.
onComputerVisionandPatternRecognition.5383â€“5392. [51] Da-WeiZhou,Qi-WeiWang,Han-JiaYe,andDe-ChuanZhan.2022. Amodel
[46] LiyuanWang,XingxingZhang,KuoYang,LonghuiYu,ChongxuanLi,Lanqing or603exemplars:Towardsmemory-efficientclass-incrementallearning.arXiv
Hong,ShifengZhang,ZhenguoLi,YiZhong,andJunZhu.2022.Memoryreplay preprintarXiv:2205.13218(2022).
withdatacompressionforcontinuallearning.arXivpreprintarXiv:2202.06592 [52] Da-WeiZhou,Han-JiaYe,andDe-ChuanZhan.2021. Co-transportforclass-
(2022). incrementallearning.InProceedingsofthe29thACMInternationalConferenceon
[47] YuchaoWang,JingjingFei,HaochenWang,WeiLi,TianpengBao,LiweiWu,Rui Multimedia.1645â€“1654.
Zhao,andYujunShen.2023.BalancingLogitVariationforLong-tailedSemantic [53] FeiZhu,ZhenCheng,Xu-YaoZhang,andCheng-linLiu.2021.Class-incremental
Segmentation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand learningviadualaugmentation. AdvancesinNeuralInformationProcessing
PatternRecognition.19561â€“19573. Systems34(2021),14306â€“14318.
[48] MaxWelling.2009.Herdingdynamicalweightstolearn.InProceedingsofthe [54] FeiZhu,Xu-YaoZhang,ChuangWang,FeiYin,andCheng-LinLiu.2021.Proto-
26thAnnualInternationalConferenceonMachineLearning.1121â€“1128. typeaugmentationandself-supervisionforincrementallearning.InProceedings
[49] YueWu,YinpengChen,LijuanWang,YuanchengYe,ZichengLiu,Yandong oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.5871â€“
Guo,andYunFu.2019.Largescaleincrementallearning.InProceedingsofthe 5880.
IEEE/CVFConferenceonComputerVisionandPatternRecognition.374â€“382.