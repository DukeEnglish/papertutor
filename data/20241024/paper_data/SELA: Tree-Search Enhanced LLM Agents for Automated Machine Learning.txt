Preprint.
SELA: TREE-SEARCH ENHANCED LLM AGENTS FOR
AUTOMATED MACHINE LEARNING
YizhouChi1,2‚àó,YizhangLin1‚àó,SiruiHong1,DuyiPan3,YayingFei,GuanghaoMei4,
BangbangLiu1,TianqiPang5,JackyKwok6,CeyaoZhang7,BangLiu8‚Ä†,ChenglinWu1‚Ä†
1DeepWisdom,2UniversityofCalifornia,Berkeley,
3TheHongKongUniversityofScienceandTechnology(Guangzhou),
4UniversityofCalifornia,SanDiego,5SouthChinaNormalUniversity,
6StanfordUniversity,7TheChineseUniversityofHongKong,Shenzhen,
8Universite¬¥ deMontre¬¥al&Mila
ABSTRACT
AutomatedMachineLearning(AutoML)approachesencompasstraditionalmeth-
ods that optimize fixed pipelines for model selection and ensembling, as well
as newer LLM-based frameworks that autonomously build pipelines. While
LLM-based agents have shown promise in automating machine learning tasks,
theyoftengeneratelow-diversityandsuboptimalcode, evenaftermultipleitera-
tions. Toovercometheselimitations, weintroduceTree-SearchEnhancedLLM
Agents (SELA), an innovative agent-based system that leverages Monte Carlo
TreeSearch(MCTS)tooptimizetheAutoMLprocess. Byrepresentingpipeline
configurationsastrees,ourframeworkenablesagentstoconductexperimentsin-
telligently and iteratively refine their strategies, facilitating a more effective ex-
ploration of the machine learning solution space. This novel approach allows
SELAtodiscoveroptimalpathwaysbasedonexperimentalfeedback,improving
theoverallqualityofthesolutions. Inanextensiveevaluationacross20machine
learningdatasets,wecomparetheperformanceoftraditionalandagent-basedAu-
toML methods, demonstrating that SELA achieves a win rate of 65% to 80%
against each baseline across all datasets. These results underscore the signifi-
cant potential of agent-based strategies in AutoML, offering a fresh perspective
ontacklingcomplexmachinelearningchallenges1.
1 INTRODUCTION
AutomatedMachineLearning(AutoML)isarapidlyevolvingfieldthatseekstoautomatethepro-
cessofdesigningreliablemachinelearningsolutionswithminimalhumanintervention. Traditional
AutoML frameworks, such as Auto-WEKA (Thornton et al., 2013), Auto-Sklearn (Feurer et al.,
2015;2020),AutoGluon(Tangetal.,2024b),andH2OAutoML(LeDell&Poirier,2020),relyon
predefined search spaces and routines. These frameworks primarily focus on optimizing hyperpa-
rametersandmodelensemblingtofindthebestmodelconfiguration. However,thisfixedandstatic
approachoftenlackstheadaptabilityneededtohandlediverseanddynamicdatascenarios,resulting
insuboptimalperformanceinmorecomplexsettings. Additionally, thetraditionalfocusonmodel
trainingleavesothercrucialstagesofthemachinelearningpipeline,suchasdatapreprocessingand
featureengineering,underexplored,therebylimitingtheoveralleffectivenessofthesesystems.
Recently,largelanguagemodel(LLM)-basedagentshaveemergedaspromisingtoolsforautomat-
ingmachinelearningtasksbyleveragingnaturallanguageprocessingcapabilitiestogeneratecode.
Thesesystemstypicallybeginwithanaturallanguagepromptdescribingthedatasetandtheprob-
lem, after which an LLM generates an end-to-end solution. Early efforts, such as Zhang et al.
‚àóTheseauthorscontributedequallytothiswork.
‚Ä†BangLiu(E-mail:bang.liu@umontreal.ca)andChenglinWu(E-mail:alexanderwu@deepwisdom.ai)are
thecorrespondingauthors.
1Thecodeisavailableathttps://github.com/geekan/MetaGPT
1
4202
tcO
22
]IA.sc[
1v83271.0142:viXraPreprint.
(2024), experimented with prompting LLMs to generate machine learning solutions, while Hong
et al. (2024a) introduced agents equipped with Hierarchical Graph Modeling and Programmable
NodeGenerationtoaddresscomplexanddynamicworkflows. Despitetheseadvances,LLM-based
solutionsoftenfallshortingeneratingdiverseandhighlyoptimizedworkflows,astheirsearchpro-
cessremainslimitedtoasinglepassortrial. Withoutiterativerefinementortheabilitytoexplore
alternativestrategies,thesesolutionsfrequentlyconvergeonsuboptimalresults,evenwhenmultiple
attemptsareallowed.
AcriticalshortcomingofbothtraditionalAutoMLandLLM-basedframeworksliesintheirinability
tomimicthenuancedproblem-solvingapproachofhumanexperts. Whenapproachingamachine
learning task, an expert does not simply execute a fixed pipeline or rely on a single attempt. In-
stead, they explore various potential configurations, systematically conduct experiments, analyze
results,anditerativelyrefinetheirunderstandingofeachcomponent‚Äôseffectiveness. Thisiterative,
feedback-drivenprocessallowsexpertstoexplorediversesolutionsandimprovethemincrementally
untiltheyarriveattheoptimalconfiguration.
Explonary R Check the data columns
Data Analysis
PreD pra ot ca ess Re 1fine Re 1fine A1 I w mm i etp h du iate n s A2 S n couc lma ul me e r nt ich s e A3 U onp ed -a ht oe t t eh ne coding
Refine
EnF ge ina etu er re ing 2 Apply pol fy en ao tum reia sl B1 Use PCA B2 Derive f ea a n tue rw e B3
Model Experiment Use a stacking Use k-fold
Training C1 with LR and C2 classifier C3 bagging to avoid RF overfitting
Multi-step Generation One-step Generation + Iterative Refinement Our Method
Propose a multi-step plan and Generate the whole ML solution within one To generate a multi-step ML solution, we utilize an LLM
generate the ML solution step step and then iteratively refine and improve to propose the search space for different ML stages. We
by step. the whole solution. then apply MCTS to search for an optimized solution.
Figure1: SELA‚Äôsabstractioncomparedtootheragent-basedAutoMLframeworks. Therearetwomaintypes
ofagent-basedapproachestoAutoMLproblems. Thefirstapproach(Hongetal.,2024a)dividesamachine
learningtaskintomultiplestages,proposingaplanforeachstage,andgeneratingandexecutingcodestepby
stepaccordingtotheplan,withnorefinementafterthesolutioniscompleted.Thesecond(Schmidtetal.,2024)
generatestheentiresolutioninonestepanditerativelyrefinesitasawhole.SELAintegratesbothapproaches,
enablingstage-wiseplanningwhileiterativelyexploringbettersolutionsateachstagelevel.
Inspired by this human-centered approach, we propose Tree-Search Enhanced LLM Agents
(SELA)forautomatedmachinelearning, anovelframeworkthatintegratesthestrengthsofLLM
agentswithastructuredsearchandrefinementprocessmodeledonhowexpertssolvemachinelearn-
ingproblems. AsillustratedinFigure1, ourframeworkcombinesthebenefitsofstage-wiseplan-
ning,whereeachstage(e.g.,ExploratoryDataAnalysis,DataPreprocessing,FeatureEngineering,
andModelTraining)ishandledsequentially,withaniterativerefinementmechanism.
InSELA,thesearchspaceofamachinelearningproblemisproposedandconceptualizedasatree,
where each branch represents a potential solution path. To navigate this search space, we employ
MonteCarloTreeSearch(MCTS)(Coulom,2007)asthecoredecision-makingengine,leveraging
its ability to balance exploration (testing new strategies) and exploitation (improving known good
strategies). MCTSallowstheagenttoefficientlyexplorelargedecisionspaces,collectandprocess
experimentalresults,andintelligentlyselectthenextpromisingconfigurationtoteston.Byiterating
throughthiscycleofexperimentationandrefinement, SELA incrementallyimprovesitssolutions,
muchlikeanexpertwhotestsandimprovesitsstrategybasedoncontinuousfeedback.
We rigorously evaluated SELA using 20 diverse datasets from the AutoML Benchmark (Gijsbers
etal.,2024),comparingitsperformanceagainstbothtraditionalAutoMLsystemsandagent-based
AutoMLapproaches.TheresultsdemonstratethatSELAconsistentlydeliverssuperiorperformance
acrossawiderangeofmachinelearningtasks,validatingitseffectivenessandadaptability.
Tosummarize,ourresearchmakesthefollowingcontributions:
1. We introduce a feedback-driven approach for LLM agents to iteratively explore machine
learningconfigurations,optimizingsolutionsovermultipleexperimentalrounds.
2
Solution
1
Solution
2
Solution
3Preprint.
2. UsingMonteCarloTreeSearch,oursystemnavigatesatree-structuredsearchspace,adap-
tivelyidentifyinghigh-performancepipelinesthroughfeedback.
3. Wecompareagent-basedandtraditionalAutoML,highlightingagenticmethods‚Äôflexibility
andpotentialforenhancedperformanceinmachinelearning.
Dynamic Feature Model Model Pipeline
Pipeline Engineering Training Improvement Optimization
AutoGluon(Ericksonetal.,2020) ‚úó ‚úó Fixedmodels Multi-layerstacking+bagging ‚úó
AutoSklearn(Feureretal.,2020) ‚úó ‚úó Fixedmodels BayesOpt.+meta-learning+ensemble ‚úó
DataInterpreter(Hongetal.,2024a) ‚úì Instinctive Instinctive Instinctive ‚úó
AIDE(Schmidtetal.,2024) ‚úì Instinctive Dynamic&diverse Dynamic&diverse One-steprefinement+LLM
SELA(Ours) ‚úì Dynamic&diverse Dynamic&diverse Dynamic&diverse StepwiseMCTS+LLM
Table 1: Comparison of key capabilities across various AutoML methods. Dynamic indicates the
system‚Äôsabilitytoadjustworkflowsbasedonintermediateoutcomes,allowingittoadaptasnewin-
formationemerges. Diversereferstoemployingmultiplestrategiesormethodsacrosstasks,which
helpscapturevariedmodelingneeds. Instinctivemeansthatthesystemdirectlyreliesonthedeci-
sionsgeneratedbyanLLMandheavilydependsonthemodel‚Äôsinclination.
2 RELATED WORKS
Tree Search and Its Integration with LLMs Tree search algorithms have significantly advanced
problem-solving in artificial intelligence, with Monte Carlo Tree Search (MCTS) emerging as a
leading technique. These algorithms have been successfully applied across various domains, in-
cluding robotics (Wu et al., 2015; Clary et al., 2018; Best et al., 2019), chemistry (Segler et al.,
2018),andgaming(Silveretal.,2016;2017),whereMCTSisusedtonavigatevastsolutionspaces
and solve complex problems. More recently, research has focused on integrating tree search with
Large Language Models (LLMs) to enhance reasoning and decision-making. Studies such as Kr-
ishnamurthy et al. (2024) and Dwaracherla et al. (2024) explored LLMs‚Äô capacities for efficient
exploration,whileTangetal.(2024a)andHui&Tu(2024)developedstrategiesforexploitingpre-
viouslylearnedknowledge.Zhouetal.(2024)andChietal.(2024)appliedMCTSforplanningwith
externalorself-evaluatedfeedback,whileFengetal.(2023);Wangetal.(2024)adaptedAlphaZero-
style tree search to LLM-based tasks. These advancements underscore the potential of combining
tree search methods with LLMs, balancing exploration of new solutions with exploitation of prior
knowledgetoenhancedecision-making.
AdvancesandLimitationsinAutoMLSystemsAutomatedMachineLearning(AutoML)frame-
works were introduced to reduce the need for expert knowledge in designing machine learning
pipelines. EarlyAutoMLefforts, suchas(Thorntonetal.,2013;Olson&Moore,2016;Jinetal.,
2019; Feurer et al., 2020; Erickson et al., 2020; LeDell & Poirier, 2020; Wang et al., 2021), fo-
cused primarily on automating key pipeline components like hyperparameter optimization, model
selection, stacking, and ensembling. These frameworks achieved notable progress by integrating
meta-learningandhyperparametersearchstrategiestoautomaticallyselectandtunemachinelearn-
ing models. Furthermore, extensions into multi-modal data settings (Tang et al., 2024b; Jin et al.,
2023)havebroadenedAutoML‚Äôsapplicability.
Recently, therehasbeengrowinginterestinleveragingLLMswithinAutoMLsystemstoenhance
pipeline flexibility. Studies such as Hollmann et al. (2024); Li et al. (2024) applied LLMs to au-
tomate feature engineering, while Liu et al. (2024) introduced LLMs for hyperparameter tuning.
In addition, Luo et al. (2024) proposed embedding LLMs at each stage of the machine learning
workflow. Despite these advancements, traditional AutoML systems remain constrained by rigid
pipelinesandlimitedflexibilitytoadapttouniquedatasetsorspecifictaskrequirements.
LLMAgentsforDynamicMachineLearningPipelinesIncontrasttostaticpipelines,LLM-based
agents offer a more dynamic solution for addressing complex machine learning challenges. Hong
etal.(2024a;b)introducedanLLMagentwithhierarchicalgraphmodelingandprogrammablenode
generation, enabling the creation of sophisticated, adaptable pipelines for diverse data scenarios.
Similarly,Zhangetal.(2024)demonstratedthatLLMscouldeffectivelyinterpretstructuredinputs
andapplypastexperiencestosolvenewmachinelearningtasks. Guoetal.(2024)expandedonthis
byintroducingadatascienceagentthatleveragescase-basedreasoning;however,itfaceschallenges
when generating solutions from scratch due to its reliance on existing codebases. Schmidt et al.
3Preprint.
Problem Description &
Generated Search Space Dataset Simulate and Get Feedback
Dataset Information
Default start:
Data Feature Model Explonary Data Analysis
Preprocess Engineering Training 2. Generate 1. Input problem
Insights Insights Insights search space and data info
Monte Carlo Tree Search 3. Output A1 OD na eta -h P or to Ec ne cs os din ing g:
search result 4. Plan &
Select R Root for execution LLM execute B2 F Pe oa lytu nr oe m E ian lg Fin ee ae turi rn eg s:
Expand A1 A2 A3
5. Simulation score feedback
C2SM tao ckd ie nl
g
T Cra lain si sn ig fi:
er
Simulate
(by Agent) B1 B2 B3 Loop ( step 3 -> 4 -> 5 ) until stopping condition satisfied
Default end:
Backprop Model Evaluate
C1 C2
Figure2: SELA‚Äôspipelineoperatesasfollows: Thesystembeginsbyinputtingtheproblemdescriptionand
datasetinformationintotheLLM,whichgeneratesasearchspaceofpotentialsolutions, encompassingdata
preprocessing, feature engineering, and model training. The search module, powered by Monte Carlo Tree
Search(MCTS),exploresthisspacebyselecting,expanding,andsimulatingpotentialconfigurations.TheLLM
agentthensimulatestheselectedconfigurationbyplanning,coding,andexecutingtheexperiment. Feedback
fromthesimulationisfedbackintothesearchmodule,whereitisusedinthebackpropagationsteptorefine
future searches. This iterative process continues until a predefined stopping criterion is met, resulting in an
optimizedexperimentalpipeline.
(2024)proposedaniterativeapproach,wheretheentirepipelineisgeneratedinonestepandrefined
iterativelythroughincrementalmodifications.
Building on these efforts, SELA introduces an agent that integrates the strengths of both
approaches‚Äîstage-wise planning and iterative refinement‚Äîallowing it to autonomously explore
andgeneratemachinelearningsolutionsfromthegroundup.Thisapproachoffersgreaterflexibility
andcontrolduringthesearchprocess,enablingthegenerationofoptimizedsolutionsateachstage.
Table1highlightsthefunctionalitiesprovidedbydifferentAutoMLsystems.
3 METHOD
AsillustratedinFigure2,SELAconsistsofthreekeycomponents:anLLM-basedinsightproposer,
a search module using MCTS, and an LLM agent as the experiment executor. First, the LLM
generates insights from the problem description and dataset, defining a search space. The search
modulethenorganizesthisspaceintoatreestructureandusesMCTStoexplorepromisingpaths.
Duringeachcycle,theselectedpathispassedtotheLLMagent,whichtranslatestheconfiguration
intoanexecutablepipeline. Theagentplans,codes,andexecutestheexperiment,feedingtheresults
backtorefinefuturesearches. Thisiterativeprocesscontinuesuntiltheterminationcriterionismet.
Thefollowingsectionsprovideadetailedexplanationofeachcomponent.
3.1 INSIGHTPROPOSALANDSEARCHSPACECREATION
To enable SELA to explore a wide range of machine learning strategies, we introduce an insight
proposer that generates diverse methods tailored to different stages of the machine learning work-
flow. Eachproposedinsightsuggestseitherasingletechniqueoracombinationofmethodsaimed
at enhancing performance. For instance, a feature engineering insight might recommend creating
interactionfeaturesfromexistingvariables,whileamodeltraininginsightcouldproposeaspecific
algorithmorsuggestrunningagridsearchtoimproveaccuracy.
The insight proposer takes as input the problem description p and dataset information d, such as
metadata and sample records, and generates m insights Œª for each stage of the machine learning
process using a large language model M. These insights are stored in an insight pool, forming a
searchspaceŒõforSELAtoexplore. Wedecomposethemachinelearningprocessintofivestages:
ExploratoryDataAnalysis(œÑ ),DataPreprocessing(œÑ ),FeatureEngineering(œÑ ),ModelTraining
1 2 3
(œÑ ),andModelEvaluation(œÑ ). Forsimplicity,wedenotetheentiresetofstagesasT andreferto
4 5
4Preprint.
anyspecificstageasœÑ.
InsightProposer(p,d,M)‚ÜíŒõ:={ŒªœÑ |œÑ ‚ààT,i=1,...,m} (1)
i
3.2 PIPELINEEXECUTIONANDCODEGENERATION
WeemployanLLMagent,referredtoastheexperimentexecutorE,toconducteachtrialbybuilding
practicalexperimentalpipelinesfromnaturallanguagerequirements.Theagenttakestwomainsteps
inthisprocess. First,givenanexperimentconfigurationc,whichisasetofinsightsprovidedbythe
searchmodule(introducedinSection3.3.2), theexperimentexecutortranslatestheseinsightsinto
a detailed plan. This plan consists of a sequence of task instructions IœÑ‚ààT corresponding to each
stageofthemachinelearningprocess. ThisstepisreferredtoasE .
plan
Next, following the plan, the agent writes and executes code œÉœÑ for each task œÑ based on the re-
spectiveinstructionIœÑ,producingthecodeœÉœÑ‚ààT forthefullpipeline,alongwiththefinalexecution
scores. ThecompletesetofcodeoutputsœÉœÑ‚ààT isconcatenatedintoafullsolutionœÉ toaddress
sol
theproblem. ThisphaseisreferredtoasE .
code&execute
E (p,d,c,M)‚ÜíIœÑ‚ààT (2)
plan
E (IœÑ‚ààT,D,M)‚Üí(œÉœÑ‚ààT,s) (3)
code&execute
3.3 TREESEARCHINMACHINELEARNINGEXPERIMENTS
Inordertosystematicallyexplorethedifferentconfigurationsinmachinelearningexperiments,we
modelthesearchspaceasahierarchicaltree.Thisstructureallowsustoapplytreesearchalgorithms,
whereeachpaththroughthetreerepresentsadifferentexperimentconfiguration. Algorithm1also
providesanoverviewofthissearchingprocess.
3.3.1 EXPERIMENTNODE
Tofacilitatetheexplorationofvariousstrategies, wemodeltheproposedsearchspaceasahierar-
chicaltreethatiswell-suitedforapplyingsearchalgorithms. Eachnodeinthetree, denotedasx,
representsoneinsightŒªinthesearchspaceŒõandcontainsthefollowingattributes:
‚Ä¢ Insight Œª(x): Represents the specific insight ŒªœÑ ‚àà Œõ associated with this node, where œÑ
i
denotesthestageofthemachinelearningpipeline.
‚Ä¢ Depth Œ¥(x): Indicates the stage of the machine learning process the node corresponds to
(e.g.,depth1mightrepresentdatapreprocessing,depth2forfeatureengineering,anddepth
3formodeltraining).
‚Ä¢ Valuev(x): Thecumulativescorefromsimulationsforthisnodeandallitsdescendants.
‚Ä¢ NumberofVisitsn (x): Thetotalnumberofsimulationsconductedforthisnodeand
visits
itsdescendants.
‚Ä¢ SimulationScores(x): Thescoreforsimulatingthisnode.
‚Ä¢ SolutionCodeœÉ (x)Thefinalcodeproducedafterthenodesimulation.
sol
‚Ä¢ Stage Code œÉ (x): The code generated up to the node‚Äôs current stage, a part of the
stage
solutioncode
Bymodelingthesearchspaceasatree,eachpathfromtheroottoanodexrepresentsanexperiment
configurationc(x)={Œª(x ),Œª(x ),...,Œª(x)}‚äÇŒõ,wherex ,x ,...,xarenodesalongthepath.
1 2 1 2
The task of finding the optimal solution can therefore be viewed as a path search within the tree,
whereeachpathcorrespondstoapotentialconfigurationoftheexperiment.
3.3.2 TREESEARCHFORMLEXPERIMENTS
We apply Monte Carlo Tree Search (MCTS) to systematically explore and identify optimal ma-
chinelearningsolutionswithinourframework. MCTSallowsustoefficientlynavigatethesearch
space across multiple stages of the machine learning pipeline‚Äîfrom data preprocessing to model
selection‚Äîbybalancingexplorationandexploitation.
5Preprint.
Algorithm1SELAusingMCTS
Input: Problemdescriptionp,datainformationd,dataD,LLMM,rolloutnumberk.
1: Œõ‚ÜêInsightProposer(p,d,M)
2: InitializeTreeusingŒõ
3: fori=1tokdo
4: nodex‚Üêselect(Tree)
5: X ‚Üêexpand(Tree,x)
child
6: Randomlysampleanodex fromX
sample child
7: Retreiveexperimentconfigurationc(x )
sample
8: œÉ sol,s‚Üêsimulate(c(x sample),p,d,D,M)
9: attachthesimulationresultœÉ sol,stox sampleforfinalsolutionselection
10: Backpropagate(Tree,s)
11: endfor
12: x ‚Üêargmax(s(x))
devbest
x‚ààTree
Output: œÉ (x )
sol devbest
Algorithm2Simulate
Input: Experimentconfigurationc,problemdescriptionp,datainformationd,dataD,LLMM.
1: DraftplansIœÑ‚ààT ‚ÜêE (p,d,c,M)
plan
2: CodeandexecutesequentiallyœÉœÑ‚ààT,s‚ÜêE (IœÑ‚ààT,D,M)
code&execute
3: œÉ sol ‚Üêconcatenate(œÉœÑ‚ààT)
Output: œÉ ,s
sol
Thesearchprocessinvolvesperformingmultiplerollouts,whichincludethestepsofselection,ex-
pansion,simulation,andbackpropagation. Weconductk rolloutstoexplorevariouspaths,aiming
toidentifythebestsolution.
Selection At each iteration, we use a modified version of the UCT (Upper Confidence Bound for
Trees)algorithm(Kocsis&Szepesva¬¥ri,2006),referredtoasUCT-DP(depth-preferred),toselecta
nodefromthesearchtree. UnliketraditionalMCTS,wheresimulationsareoftenperformedquickly
duetoafixedactionspaceandnegligibleactiontime,thecontextofmachinelearningtaskspresents
a different challenge. Processes such as model training introduce significant computational time,
making efficient node exploration crucial. Since model selection can heavily influence the overall
machinelearningperformance,weprioritizeexploringnodesatgreaterdepthsearlyon.
This modification reduces the need to explore every unvisited node, allowing deeper nodes to be
reached in fewer iterations‚Äîmaking the approach better suited for large-scale machine learning
experiments. Themodifiedselectionalgorithmisexpressedas:
(cid:115)
v(x) lnn (x )
UCT-DP(x)= +Œ± visits parent (4)
n(x) explore n(x)
(cid:26)
Œ± ifn (x)=0
n(x)= unvisted visits (5)
n (x) otherwise.
visits
Here,Œ± isaconstantbetween0and1controllingtheselectionpreferenceforunvisitednodes,
unvisted
balancingbetweenfullexplorationandcomputationalefficiency.Thisadjustmentallowsustofocus
moreondeeperpartsofthetreethatarelikelytoyieldbettersolutions.
ExpansionDuringtheexpansionphase,asetofchildnodesX areinstantiatedfromtheselected
child
nodexforpotentialsimulation. Notethatachildnodex fromthenodexatdepthŒ¥inheritsthe
child
attributesofxandpossessesŒª(x child)‚ÜíŒªœÑŒ¥+1,aninsightofstageœÑ Œ¥+1fromthesearchspace.
6Preprint.
SimulationOnceexpanded,anodex israndomlysampledfromX forsimulation.Thepath
sample child
fromroottothesamplednodeformsasetofinsightsc(x ) = {Œª(x ),Œª(x ),...,Œª(x )} ‚äÇ
sample 1 2 sample
Œõ,representingtheexperimentconfigurationtobesimulated,wherex ,x ,..,x arethenodes
1 2 sample
alongthepath.Theconfigurationc(x )isthenfedtotheexperimenterEforexecutionfollowing
sample
E and E , which produces a simulation score s, as illustrated in Section 3.3.1. The
plan code&execute
scoreservesasthefeedbackforbackpropagation. Algorithm2outlinesthesimulationprocess.
BackpropagationAfterthesimulationconcludes,theperformancescore(e.g.,basedonthedevel-
opment set) is retrieved and backpropagated through the tree. The score is propagated from the
simulatednodeuptotheroot,updatingeachparentnode‚Äôsvalueandvisitcount. Thisallowsnodes
representingmorepromisingsolutionstobeprioritizedinfuturerollouts. Inaddition,thesolution
codeisalsobackpropagateduptothetree,anditcanbeprocessedandsavedasstagecodedepending
ontheparentnodeduringtheupdate.
Backpropagation ensures that the algorithm learns which paths yield better results, guiding the
searchtowardhigher-performingnodesasmorerolloutsareconducted.
3.3.3 EXPERIMENTSTATESAVINGANDLOADING
Toboostexperimentationefficiencyandreducetokenusage, SELA implementsfine-grainedcode
reusebycachingcodeatthestagelevelforeachattemptedconfigurationc. Thisallowstheframe-
work to reuse as much saved code as possible when a new configuration c shares components
new
withexistingones. Additionally,thistechniqueaddressesthechallengeofLLMnon-determinism,
where identical instructions can produce different code, increasing variance in final performance.
Specifically,wheneveranodeischosenforexecution,theexperimenterloadsandrerunsthesaved
stage code, if available, ensuring consistency before progressing to the next stage. This approach
effectively conserves resources while maintaining robust performance across stages. In Appendix
D,weexaminethecostefficiencyofthisstate-savingandloadingmechanism.
4 EXPERIMENTS
4.1 EXPERIMENTALSETUP
Datasets WeevaluateSELAalongsideseveralbaselineson20datasets,whichinclude13classi-
ficationtasksand7regressiontasksfromtheAutoMLBenchmark(AMLB)(Gijsbersetal.,2024)
andKaggleCompetitions.
Table 4 provides detailed information on the datasets used. All datasets are split into training,
validation,andtestsetswitha6:2:2ratio. Eachframeworkutilizesthetrainingandvalidationsets
totrainmodelsandmakespredictionsonthetestsetlabels.
Evaluation Metrics For the AMLB datasets, we use the default target column provided by
OpenML. For Kaggle competition datasets, we rely on the target column specified in the compe-
titiondescription. Performanceismeasuredusingrootmeansquarederror(RMSE)forregression
tasks, F1 score for binary classification, and F1-weighted score for multi-class classification. To
ensurecomparabilityacrossdatasetswithvaryingmetrics,weintroduceaNormalizedScore(NS),
whichmapsRMSEintotherangefrom0to1.
(cid:40)
1 ifthemetricisRMSE.
NS(s raw)= 1+log(1+sraw) (6)
s otherwise.
raw
Here,s representstherawscorebeforenormalization. Toevaluate SELA againstotherframe-
raw
works, we employ three key metrics: average Normalized Score (NS), average rank, and average
bestrank. Theaverage rankiscalculated byconsideringallrankings ofamethodacross datasets,
whiletheaveragebestrankfocusesonthemethod‚Äôsbestperformanceineachdataset. Wealsowant
toquantifyhowotherbaselinesperformrelativetoSELA.The‚ÄúRescaledNS‚Äùisdefinedas:
NS
RescaledNS(f)= f (7)
NS
SELA
wheref representsthebaselinemethodbeingcomparedtoSELA.
7Preprint.
MethodandBaselinesSetup WecompareSELAwithseveralbaselinemethods,includingData
Interpreter (Hong et al., 2024a), AIDE (Schmidt et al., 2024), AutoGluon (Erickson et al., 2020),
andAutoSklearn(Feureretal.,2015;2020).
ForourLLM-basedapproaches(SELA,DataInterpreter,andAIDE),weemployaconsistentinitial
task prompt across all methods. This prompt encompasses the dataset name, target column, and
evaluation metric. We choose DeepSeek v2.5 (DeepSeek-AI, 2024) as our foundation LLM due
toitsopen-sourcenature, strongcodingcapabilities, andcost-effectivetokenusage. Toencourage
outputdiversity,wesetthetemperatureparameterto0.5forallLLM-basedmethods.AIDEconducts
10iterationsperexecution,whileSELAperforms10rollouts.
For SELA, we employ Data Interpreter as the experimenter, leveraging its multi-step generation
capability. We configured the hyperparameters of UCT-DP as follows: Œ± is set to 0.8 and
unvisited
Œ± is set to 1.4. These settings aim to balance exploration and exploitation in the method‚Äôs
explore
searchstrategy.
Each method, except for AutoGluon, is run three times for each dataset. AutoGluon, being deter-
ministic, is run only once with its default settings. AutoSklearn is also run with default settings,
limitedto600secondspertask.
Method Wins Losses Top1 Avg.NS%‚Üë Avg.BestNS%‚Üë Avg.Rank‚Üì Avg.BestRank‚Üì
AutoGluon 7 13 4 53.2 53.2 4.4 4.4
AutoSklearn 5 15 5 46.1 47.5 7.6 6.1
AIDE 5 15 2 47.1 51.8 7.8 5.3
DataInterpreter 4 16 2 47.4 50.2 8.8 6.4
SELA - - 7 53.3 54.7 4.8 2.7
Table2:ResultsofeachAutoMLframeworkon20tabulardatasets.The‚ÄúWins‚Äùcolumnindicatesthenumber
of datasets where the method outperforms SELA, while ‚ÄúLosses‚Äù shows the number of datasets where the
methodunderperforms.The‚ÄúTop1‚Äùcolumnrepresentsthenumberofdatasetswherethemethodproducesthe
bestpredictionsacrossmethods.
4.2 RESULTS
 & O L F N B S U H G L F W L R Q B V P D O O
 * H V W X U H 3 K D V H 6 H J P H Q W D W L R Q 3 U R F H V V H G
 0 R Q H \ E D O O
 6 $ 7    + $ 1 '  U X Q W L P H  U H J U H V V L R Q
 E R V W R Q
 F R O O H J H V
 F R Q F U H W H  V W U H Q J W K
 $ X W R 0 /  ) U D P H Z R U N
 F U H G L W  J
 $ X W R 6 N O H D U Q  G L D P R Q G V
 K R X V H  S U L F H V  $ , ' (
 L F U  $ X W R * O X R Q
 M D V P L Q H  ' D W D  , Q W H U S U H W H U
 N F   6 ( / $  % H V W
 N L F N
 P I H D W  I D F W R U V
 V H J P H Q W
 V P R N H U  V W D W X V
 V R I W Z D U H  G H I H F W V
 W L W D Q L F
 Z L Q H  T X D O L W \  Z K L W H
                               
 5 H V F D O H G  1 6  R Q  7 H V W  ' D W D   U H O D W L Y H  W R  6 ( / $ 
Figure3: RescaledNSofAutoMLframeworksrelativetoSELAontabulardatasets. Pointstotheleftofthe
verticallineindicatepoorerpredictionscomparedtoSELA.Notably,SELAoftenoccupiesaleadingposition
acrossthedatasets.
As shown in Table 2, SELA achieves the highest average Normalized Score (NS) and average
best rank among all frameworks. Notably, SELA excels in producing the highest number of top
predictions, as indicated in the ‚ÄúTop 1‚Äù column across all datasets. Furthermore, the ‚ÄúLosses‚Äù
8
 W H V D W D 'Preprint.
column reveals that each competing method falls short against SELA, losing in 65-80% of the
datasets.
Interestingly,AutoGluonexhibitsamarginallyhigheraveragerankthanSELA.Thisslightdiscrep-
ancy may be attributed to the inherent randomness in LLMs and model training processes, which
caninfluencetheexplorationofmachinelearningsolutions. However, SELA‚ÄôshigheraverageNS
suggests that it performs strongly in the datasets where it excels, while its losses in other datasets
are relatively minor. This meansthat even when SELA produceslower-ranked solutions, the per-
formancegapissmall,allowingittofullycompensateinthedatasetswhereitperformswell.
The two other agent-based methods exhibit relatively lower performance. The first method, Data
Interpreter, struggles to enhance its score with multiple attempts due to its inability to refine its
solutionaftercompletingamachinelearningtask.Thesecondmethod,AIDE,doesnothaveastage-
specificplanningmodule,limitingitscapacitytoimproveresultsafteraseriesofgreedyexploitation,
whichmakesitpronetofallingintolocaloptima. Theselimitationslikelyaccountfortheirweaker
performance.
Figure3furthercorroborates SELA‚Äôseffectiveness,revealingthatitsbestsolutionsfrequentlyoc-
cupyleadingpositionsacrossvariousdatasets. Thisvisualrepresentationexhibitsthemethod‚Äôscon-
sistenthighperformanceandadaptabilityacrossdifferentMLdatasets. Wealsoincludeadetailed
resultsofeachmethodinAppendixC.
4.3 ABLATIONSTUDY
Fortherestofthestudy,weemployasubsetofdatasetstoevaluate SELA undervarioussettings.
Ourselectionprocessinvolveschoosingthefirsttwodatasetsalphabeticallyforeachmachinelearn-
ingtask. Specifically,weuseboston,colleges,credit-g,Click prediction small,GesturePhaseSeg-
mentationProcessed,andmfeat-factorstoconducttheablationstudy.
DataInterpreter SELA(RandomSearch) SELA(MCTS)
Avg. NS‚Üë 56.4 58.6 60.9
Avg. BestNS‚Üë 59.0 61.4 62.4
Avg. Rank‚Üì 6.9 4.8 3.3
Avg. BestRank‚Üì 4.8 2.8 1.5
Table3: Performanceresultsforeachsearchsettingonthechosendatasets. SELAwithMCTSconsistently
surpassesSELAwithRandomSearch.
EffectivenessofSearch ToevaluatetheeffectivenessofMonteCarloTreeSearch(MCTS)inim-
provingthesolutionsearchprocess,weconductedanablationstudy. Inthisstudy,wecomparedthe
performanceofourmethodusingMCTSagainstavariantthatrandomlysamplesinsightsfromeach
stage‚Äôs insight pool. As shown in Table 3, the MCTS version achieves a higher average normal-
izedscoreacrossdatasetsandabetteroverallrankingcomparedtotherandomsamplingapproach.
Moreover,eventherandomsamplingvariantofourmethodoutperformsDataInterpreter,thebase
experimenter. Thissuggeststhepresenceofanappropriatesearchspaceandanexperimentagenda
isvitalforimprovingamachinelearningagent. Ourinsightproposergeneratesrelevantanduseful
insights,facilitatingsuchimprovement,regardlessoftheselectionmethod.
NumberofRollouts Figure4illustratesthattheaverageperformanceof SELA improvesasthe
numberofpermittedrolloutsincreases.ThetrenddemonstratesthestrongscalabilityofSELA,asit
efficientlyleveragesadditionalopportunitiestoexplorethesearchspace,improvingthenormalized
scoreby4.7%after10rolloutsand6.4%after20,comparedtotheinitialrollout.
LLMAdaptability Toevaluatetherobustnessofourframework,weconductexperimentsusing
different Large Language Models (LLMs). Specifically, we compare the performance of SELA
withClaude-3.5-Sonnet(Anthropic,2024)andGPT-4o(OpenAI,2024)againstDeepSeek
V2.5whichweprimarilyuseforevaluation. Thiscomparisonenablesustoassesshowthechoice
ofLLMaffectstheoveralleffectivenessofourapproach.
9Preprint.
 * 3 7   R  & O D X G H      6 R Q Q H W  ' H H S 6 H H N  9   
                
                 
   
        
            
    
    
        
                 
    
    
        
    
   
                & O L F N B S U H G L F W L R Q  * H V W X U H 3 K D V H  E R V W R Q  F R O O H J H V  F U H G L W  J  P I H D W  I D F W R U V
   R I  5 R O O R X W V  ' D W D V H W
Figure 4: The average performance of Figure5: ComparisonofNormalizedScoresbetweendifferent
SELAonsixselecteddatasetswithanin- baseLLMsonsixselecteddatasets.
creasingnumberofrollouts.
As Figure 5 shown, SELA delivers similar results across different LLMs, indicating its flexibility
with various models depending on user preference and availability. We also report the numeric
resultsinAppendixC.2.
5 CONCLUSION
In this paper, we introduced SELA, a novel framework that integrates LLM-based agents with
Monte Carlo Tree Search (MCTS) to automate machine learning workflows. Our experimental
results, conducted on 20 machine learning datasets, demonstrate SELA‚Äôs effectiveness and high-
lightitsdistinctadvantagesoverbothtraditionalAutoMLframeworksandexistingLLM-basedap-
proaches. Theproposedmethodologyisnotlimitedtomachinelearningbutcouldbeadaptedtoa
widerangeofsequentialdecision-makingproblems,providedtheycanberepresentedastreestruc-
tureswithscalarrewardsderivedfromtheirleafnodes. Lookingahead,futureworkcouldexplore
extending this framework to other domains, including software engineering, scientific discovery,
gameplaying,androbotics. Furthermore,improvingtheefficiencyandscalabilityofthetreesearch
process for larger solution spaces remains an important area for investigation. Another promising
direction is developing techniques to provide interpretable explanations of the search process and
solutionrationale,enhancingthetransparencyandtrustworthinessofthesystem. SELArepresents
asignificantadvancementinautomatedmachinelearning,demonstratingthepotentialofcombining
traditionalsearchalgorithmswiththeflexibilityofLLMs.
REFERENCES
Anthropic. Introducing Claude 3.5 Sonnet ‚Äî anthropic.com. https://www.anthropic.com/news/
claude-3-5-sonnet,2024.
Graeme Best, Oliver M Cliff, Timothy Patten, Ramgopal R Mettu, and Robert Fitch. Dec-mcts:
Decentralizedplanningformulti-robotactiveperception. TheInternationalJournalofRobotics
Research,38(2-3):316‚Äì337,2019. doi: 10.1177/0278364918755924.
YizhouChi,KevinYang,andDanKlein. Thoughtsculpt: Reasoningwithintermediaterevisionand
search,2024.
PatrickClary,PedroMorais,AlanFern,andJonathanHurst. Monte-carloplanningforagilelegged
locomotion.ProceedingsoftheInternationalConferenceonAutomatedPlanningandScheduling,
28(1):446‚Äì450,Jun.2018. doi: 10.1609/icaps.v28i1.13933.
Re¬¥mi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In H. Jaap
vandenHerik,PaoloCiancarini,andH.H.L.M.(Jeroen)Donkers(eds.),ComputersandGames,
pp.72‚Äì83,Berlin,Heidelberg,2007.SpringerBerlinHeidelberg. ISBN978-3-540-75538-8.
DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language
model,2024.
10
 H U R F 6  G H ] L O D P U R 1
 H U R F 6  G H ] L O D P U R 1Preprint.
Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy. Efficient
explorationforllms,2024.
NickErickson,JonasMueller,AlexanderShirkov,HangZhang,PedroLarroy,MuLi,andAlexander
Smola. Autogluon-tabular: Robustandaccurateautomlforstructureddata,2020.
XidongFeng, ZiyuWan, MuningWen, YingWen, WeinanZhang, andJunWang. Alphazero-like
tree-searchcanguidelargelanguagemodeldecodingandtraining,2023.
MatthiasFeurer,AaronKlein,KatharinaEggensperger,JostSpringenberg,ManuelBlum,andFrank
Hutter. Efficient and robust automated machine learning. In Advances in Neural Information
ProcessingSystems28(2015),pp.2962‚Äì2970,2015.
MatthiasFeurer,KatharinaEggensperger,StefanFalkner,MariusLindauer,andFrankHutter.Auto-
sklearn2.0: Hands-freeautomlviameta-learning,2020.
PieterGijsbers, MarcosL.P.Bueno, StefanCoors, ErinLeDell, Se¬¥bastienPoirier, JanekThomas,
BerndBischl,andJoaquinVanschoren. Amlb:anautomlbenchmark. JournalofMachineLearn-
ingResearch,25(101):1‚Äì65,2024.
Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. Ds-agent: Auto-
mateddatasciencebyempoweringlargelanguagemodelswithcase-basedreasoning,2024.
Noah Hollmann, Samuel Mu¬®ller, and Frank Hutter. Large language models for automated data
science: Introducingcaafeforcontext-awareautomatedfeatureengineering,2024.
Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi
Zhang,JinlinWang,LiZhang,LingyaoZhang,MinYang,MingchenZhuge,TaichengGuo,Tuo
Zhou,WeiTao,WenyiWang,XiangruTang,XiangtaoLu,XiawuZheng,XinbingLiang,Yaying
Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. Data interpreter: An llm agent for data
science,2024a.
Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao
Zhang,ZiliWang,StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,LingfengXiao,
ChenglinWu,andJu¬®rgenSchmidhuber. MetaGPT:Metaprogrammingforamulti-agentcollabo-
rativeframework. InTheTwelfthInternationalConferenceonLearningRepresentations,2024b.
WenyangHuiandKeweiTu. Rot: Enhancinglargelanguagemodelswithreflectiononsearchtrees,
2024.
HaifengJin,QingquanSong,andXiaHu. Auto-keras: Anefficientneuralarchitecturesearchsys-
tem. InProceedingsofthe25thACMSIGKDDinternationalconferenceonknowledgediscovery
&datamining,pp.1946‚Äì1956,2019.
HaifengJin,Franc¬∏oisChollet,QingquanSong,andXiaHu. Autokeras: Anautomllibraryfordeep
learning. JournalofmachineLearningresearch,24(6):1‚Äì6,2023.
LeventeKocsisandCsabaSzepesva¬¥ri. Banditbasedmonte-carloplanning. InJohannesFu¬®rnkranz,
Tobias Scheffer, and Myra Spiliopoulou (eds.), Machine Learning: ECML 2006, pp. 282‚Äì293,
Berlin,Heidelberg,2006.SpringerBerlinHeidelberg. ISBN978-3-540-46056-5.
AkshayKrishnamurthy,KeeganHarris,DylanJ.Foster,CyrilZhang,andAleksandrsSlivkins. Can
largelanguagemodelsexplorein-context?,2024.
ErinLeDellandSebastienPoirier. H2OAutoML:Scalableautomaticmachinelearning. 7thICML
WorkshoponAutomatedMachineLearning(AutoML),July2020.
Dawei Li, Zhen Tan, and Huan Liu. Exploring large language models for feature selection: A
data-centricperspective,2024.
SiyiLiu, ChenGao, andYongLi. Largelanguagemodelagentforhyper-parameteroptimization.
arXivpreprintarXiv:2402.01881,2024.
DaqinLuo,ChengjianFeng,YuxuanNong,andYiqingShen. Autom3l: Anautomatedmultimodal
machinelearningframeworkwithlargelanguagemodels.arXivpreprintarXiv:2408.00665,2024.
11Preprint.
RandalSOlsonandJasonHMoore. Tpot: Atree-basedpipelineoptimizationtoolforautomating
machinelearning. InWorkshoponautomaticmachinelearning,pp.66‚Äì74.PMLR,2016.
OpenAI. HelloGPT-4o. https://openai.com/index/hello-gpt-4o/,2024.
Dominik Schmidt, Yuxiang Wu, and Zhengyao Jiang. Aide: Human-level performance in data
sciencecompetitions,2024. URLhttps://www.weco.ai/blog/technical-report.
Marwin Segler, Mike Preuss, and Mark Waller. Planning chemical syntheses with deep neural
networksandsymbolicai. Nature,555:604‚Äì610,032018. doi: 10.1038/nature25978.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driess-
che,JulianSchrittwieser,IoannisAntonoglou,VedavyasPanneershelvam,MarcLanctot,Sander
Dieleman,DominikGrewe,JohnNham,NalKalchbrenner,IlyaSutskever,TimothyP.Lillicrap,
MadeleineLeach,KorayKavukcuoglu,ThoreGraepel,andDemisHassabis. Masteringthegame
ofgowithdeepneuralnetworksandtreesearch. Nature,2016.
DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,
Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap,
FanHui,L.Sifre,GeorgevandenDriessche,ThoreGraepel,andDemisHassabis. Masteringthe
gameofgowithouthumanknowledge. Nature,2017.
Hao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin Ellis.
Coderepairwithllmsgivesanexploration-exploitationtradeoff,2024a.
Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Tony Hu, Katrin Kirch-
hoff,andGeorgeKarypis. Autogluon-multimodal(automm): Superchargingmultimodalautoml
withfoundationmodels. arXivpreprintarXiv:2404.16233,2024b.
Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Combined
selectionandhyperparameteroptimizationofclassificationalgorithms.InProceedingsofthe19th
ACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pp.847‚Äì855,
2013.
Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu.
Litesearch: Efficacioustreesearchforllm,2024.
ChiWang, QingyunWu, MarkusWeimer, andErkangZhu. Flaml: Afastandlightweightautoml
library. InMLSys,2021.
FengWu, SarvapaliD.Ramchurn, WenchaoJiang, JeolE.Fischer, TomRodden, andNicholasR.
Jennings. Agile planning for real-world disaster response. In Proceedings of the 24th Interna-
tional Conference on Artificial Intelligence, IJCAI‚Äô15, pp. 132‚Äì138. AAAI Press, 2015. ISBN
9781577357384.
Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot: Unleashing the
poweroflargelanguagemodelsinsolvingmachinelearningtasks,2024.
AndyZhou,KaiYan,MichalShlapentokh-Rothman,HaohanWang,andYu-XiongWang.Language
agenttreesearchunifiesreasoningactingandplanninginlanguagemodels,2024.
12Preprint.
A DATASETS
Table4outlinesthedetailedinformationofthedatasetsusedforevaluation.
Datasetname #Features #Rows #Classes TaskType Metric Source
boston 14 506 N/A Regression RMSE OpenML(DatasetID:531)
colleges 48 7063 N/A Regression RMSE OpenML(DatasetID:42727)
concrete-strength 9 4866 N/A Regression RMSE Kaggle(playground-series-s3e9)
diamonds 10 53940 N/A Regression RMSE OpenML(DatasetID:42225)
house-prices 81 1460 N/A Regression RMSE Kaggle(house-prices-advanced-regression-techniques)
Moneyball 15 1232 N/A Regression RMSE OpenML(DatasetID:41021)
SAT11-HAND-runtime-regression 118 4440 N/A Regression RMSE OpenML(DatasetID:41980)
credit-g 21 1000 2 Classification F1 OpenML(DatasetID:31)
Clickpredictionsmall 12 39948 2 Classification F1 OpenML(DatasetID:42733)
icr 58 617 2 Classification F1 Kaggle(icr-identify-age-related-conditions)
jasmine 145 2984 2 Classification F1 OpenML(DatasetID:41143)
kc1 21 2109 2 Classification F1 OpenML(DatasetID:1067)
kick 33 72983 2 Classification F1 OpenML(DatasetID:41162)
smoker-status 23 143330 2 Classification F1 Kaggle(playground-series-s3e24)
software-defects 22 91586 2 Classification F1 Kaggle(playground-series-s3e23)
titanic 12 891 2 Classification F1 Kaggle(titanic)
GesturePhaseSegmentationProcessed 33 9873 5 Multiclass F1-weighted OpenML(DatasetID:4538)
mfeat-factors 217 2000 10 Multiclass F1-weighted OpenML(DatasetID:12)
segment 20 2310 7 Multiclass F1-weighted OpenML(DatasetID:40984)
wine-quality-white 12 4898 7 Multiclass F1-weighted OpenML(DatasetID:40498)
Table 4: Summary of the machine learning datasets used in the experiments. OpenML datasets
can be accessed using their respective dataset IDs. The Kaggle datasets are available at
https://www.kaggle.com/competitions/{source}.
13Preprint.
B PROMPTS
B.1 TASKPROMPT
All LLM-based methods start by receiving the same base requirement prompt at the beginning of
thetask. Thepromptspecifiesthedataset‚Äôsname,thetargetlabelcolumn,theevaluationmetricto
beused,andthedataset‚Äôsfilepath. Furthermore,thepromptincludeapathtoatextfilecontaining
thedataset‚Äôsmetadata.
1 TASK_PROMPT = """
2 # User requirement
3 This is a {datasetname} dataset.
4 Your goal is to predict the target column ‚Äò{target_col}‚Äò.
5 Perform data analysis, data preprocessing, feature engineering, and modeling to predict the
target. Report {metric} on the eval data. Do not plot or make any visualizations.
6
7 # Data dir
8 train set (with labels): {train_path}
9 dev set (with labels): {dev_path}
10 test set (without labels): {test_path}
11 dataset description: {data_info_path}
12 (During EDA, you can use this file
13 to get additional information about the dataset)
14 """
Since AIDE automatically splits the training data into a new train set and a validation set,
we combine the original train and validation sets and provide them as input to AIDE. We set
k fold validation to 1 in its configuration file to enforce a single train-val split for closer align-
mentwithoursetup. Inbothsetups,theframeworkshaveaccesstothelabelsforboththetrainand
validationsets.
B.2 INSTRUCTIONPROMPT
Theinstructionpromptwoulddirecttheframeworktosavethefinalpredictionfileforevaluation.
1 DI_INSTRUCTION = """
2 ## Attention
3 1. Please do not leak the target label in any form during training.
4 2. Test set does not have the target column.
5 3. When conducting data exploration or analysis, print out the results of your findings.
6 4. You should perform transformations on train, dev, and test sets at the same time (it‚Äôs a
good idea to define functions for this and avoid code repetition).
7 5. When scaling or transforming features, make sure the target column is not included.
8 6. You could utilize dev set to validate and improve model training. {special_instruction}
9
10 ## Saving Dev and Test Predictions
11 1. Save the prediction results of BOTH the dev set and test set in ‚Äòdev_predictions.csv‚Äò and ‚Äò
test_predictions.csv‚Äò respectively in the output directory.
12 - Both files should contain a single column named ‚Äòtarget‚Äò with the predicted values.
13 2. Make sure the prediction results are in the same format as the target column in the
training set.
14 - For instance, if the target column is categorical, the prediction results should be
categorical as well.
15
16 ## Output Performance
17 Print the train and dev set performance in the last step.
18
19 # Output dir
20 {output_dir}
21 """
14Preprint.
B.3 INSIGHTPROPOSALPROMPT
Insight Proposer uses this prompt to generate a search space of insights for different stages of the
machinelearningtask.
1 DATASET_INSIGHT_PROMPT = """
2 # Dataset Description
3 {dataset}
4
5 # Dataset Metadata
6 {metadata}
7
8 # Dataset Head
9 {head}
10
11 # Instruction
12 Propose insights to help improve the performance of the model on this dataset.
13 The insights should be proposed based on the dataset description with different task types.
14 Each task type should have at least 5 insights.
15 Make sure each method is diverse enough and can be implemented separately.
16 Be specific about models‚Äô choices, ensemble and tuning techniques, and preprocessing & feature
engineering techniques.
17
18 # Format
19 ‚Äò‚Äò‚Äòjson
20 [
21 {{
22 "task_type": "EDA",
23 "insights": [
24 "insight1",
25 "insight2",
26 "insight3",
27 ...
28 "insightN"
29 ]
30 }},
31 {{
32 "task_type": "Data Preprocessing",
33 "insights": [
34 "insight1",
35 "insight2",
36 "insight3",
37 ...
38 "insightN"
39 ]
40 }},
41 {{
42 "task_type": "Feature Engineering",
43 "insights": [
44 "insight1",
45 "insight2",
46 "insight3",
47 ...
48 "insightN"
49 ]
50 }},
51 {{
52 "task_type": "Model Training",
53 "insights": [
54 "insight1",
55 "insight2",
56 "insight3",
57 ...
58 "insightN"
59 ]
60 }}
61 ]
62 ‚Äò‚Äò‚Äò
63 """
15Preprint.
C RESULTS
C.1 MAINRESULTS
AutoGluon AutoSklearn AIDE DI SELA
Dataset Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best
Clickpredictionsmall 7 7 2 1 7.3 4 11 10 7.7 6
GesturePhaseSegmentationProcessed 1 1 6.3 3 7.3 4 11 10 5.3 2
Moneyball 4 4 10 9 4 1 9 2 6 3
SAT11-HAND-runtime-regression 1 1 12 11 5.3 3 9 8 3.7 2
boston 5 5 12 11 3.7 2 9 8 4 1
colleges 1 1 12 11 6 2 8 7 4 3
concrete-strength 5 5 12 11 6.3 4 2 1 8.3 6
credit-g 4 4 10 9 10 5 5.3 1 3.7 2
diamonds 2 2 12 11 6 4 8.7 7 3 1
house-prices 1 1 12 11 6.7 5 7.3 3 4 2
icr 5 5 5.3 3 12 11 9 8 2.3 1
jasmine 7 7 6 4 8.7 5 11.3 9 2 1
kc1 10 10 2.7 1 8 5 11.3 9 5 2
kick 4 4 2 1 9.3 6 11 10 6.7 5
mfeat-factors 4 4 2 1 10 9 10.3 6 6.7 5
segment 3 3 6.3 5 11 10 9.7 7 2.3 1
smoker-status 7 7 4.7 3 11.3 9 7.7 2 4.3 1
software-defects 8 8 2 1 12 11 6 4 7.7 6
titanic 7 7 9.7 6 2.7 1 10.3 8 5.3 3
wine-quality-white 2 2 10 8 7.3 4 9 7 3.3 1
OverallRank‚Üì 4.4 4.4 7.6 6.1 7.8 5.3 8.8 6.4 4.8 2.7
Table5:Methods‚Äôrankingforeachtabulardataset
AutoGluon AutoSklearn AIDE DI SELA
Dataset Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best
Clickpredictionsmall 26.6 26.6 40.2 40.3 26.1 39.4 12.9 13.9 23.2 27.4
GesturePhaseSegmentationProcessed 69.3 69.3 67.2 68.4 56.3 68.1 60.1 64.4 67.9 69.2
Moneyball 24.3 24.3 13.1 13.8 23.8 24.6 9.5 24.5 21.9 24.5
SAT11-HAND-runtime-regression 12.6 12.6 10.3 10.3 12.0 12.1 11.4 11.9 12.2 12.5
boston 39.8 39.8 19.5 19.6 40.5 41.3 37.0 38.6 40.1 41.4
colleges 88.3 88.3 2.1 2.1 86.0 87.8 87.5 87.7 87.8 87.8
concrete-strength 28.3 28.3 17.4 17.9 28.3 28.3 28.8 29.6 28.2 28.2
credit-g 50.5 50.5 35.1 44.0 21.6 48.4 48.1 53.2 50.9 52.7
diamonds 13.8 13.8 8.7 8.7 13.7 13.7 13.5 13.6 13.7 13.8
house-prices 9.0 9.0 2.0 2.0 8.9 8.9 8.5 9.0 8.9 9.0
icr 76.2 76.2 70.4 79.2 31.7 35.9 57.8 60.6 78.7 79.2
jasmine 84.3 84.3 84.4 84.7 83.6 84.6 77.8 83.5 85.4 86.2
kc1 38.3 38.3 43.5 45.0 40.8 42.6 38.1 41.2 42.2 43.1
kick 39.6 39.6 41.8 42.1 14.9 38.6 2.8 4.2 35.9 38.7
mfeat-factors 96.7 96.7 97.1 97.5 94.4 94.5 93.0 96.0 95.7 96.2
segment 93.5 93.5 92.7 93.1 91.7 92.2 91.7 92.6 93.8 94.4
smoker-status 78.0 78.0 78.6 78.9 74.8 76.3 77.3 81.5 82.4 91.5
software-defects 51.5 51.5 61.1 61.7 49.7 49.8 54.5 57.3 52.2 53.3
titanic 78.9 78.9 76.2 78.9 81.2 83.7 76.0 78.5 78.8 79.7
wine-quality-white 65.4 65.4 60.7 61.4 62.9 65.1 61.2 61.6 65.3 66.0
OverallNS%‚Üë 53.2 53.2 46.1 47.5 45.5 51.8 47.4 50.2 53.3 54.7
Table6:Methods‚ÄôNS%foreachtabulardataset
C.2 PERFORMANCEUSINGDIFFERENTLLMS
16Preprint.
GPT-4o Claude3.5Sonnet DeepSeekV2.5
Avg. NS‚Üë 62.3 57.9 60.9
Avg. BestNS‚Üë 65.5 59.2 62.4
Avg. Rank‚Üì 3.7 6.3 5.0
Avg. BestRank‚Üì 1.5 4.8 3.2
Table7:ResultsofSELAwithdifferentbaseLLMsontheselectedtabulardatasets.
D COST-EFFECTIVENESS ANALYSIS
Weconductmultipletrialsofexecutionofeachmethodtoestimatetheaveragerunningcostforthe
LLM-based baselines. As shown in Table 8, all methods incur relatively low costs to complete a
single machine learning task. Among these, AIDE exhibits the lowest execution cost, due to the
lackofstage-wiseplanning,resultinginfewertokengenerationscomparedtotheotherapproaches.
Additionally, SELA, which employs Data Interpreter as its base experimenter, is less costly than
DataInterpreteritself.ThisefficiencyislargelyduetoSELA‚Äôsstate-savingandloadingmechanism,
whichreducesthegenerationofrepeatedtasksandcode.
CostperMLtask($)
DataInterpreter(k=10) 0.07
AIDE(k=10) 0.01
SELA(k=10) 0.05
Table 8: Estimated costs of agent-based frameworks utilizing DeepSeekV2.5 on a single machine learning
datasetoverkiterations/rollouts.
17Preprint.
E CASE STUDY
E.1 OVERVIEWOFSELA‚ÄôSSEARCHPROCESS
1 Number of simulations: 10
2 [Node 0]
3 Plans:
4 1. Perform exploratory data analysis on the train and dev datasets
5 2. Preprocess the train, dev, and test datasets
6 3. Perform feature engineering on the train, dev, and test datasets
7 4. Train multiple models and evaluate their performance
8 5. Train a weighted ensemble model using the best performing models
9 6. Evaluate the ensemble model on the dev set and save predictions
10 7. Generate predictions for the test set and save them
11 Simulated: True
12 Score: avg score: 0.6150206840685731, simulated score: {‚Äôtrain_score‚Äô: 1.0, ‚Äôdev_score‚Äô:
0.6855841857240594, ‚Äôtest_score‚Äô: 0.6814818772150697, ‚Äôscore‚Äô: 0.6855841857240594},
Visits: 10
13
14 [Node 0-0]
15 Plans:
16 3. Perform feature engineering on the train, dev, and test datasets by creating new
features that calculate the magnitude of the vectorial velocities and accelerations
to capture the overall movement intensity.
17 Simulated: True
18 Score: avg score: 0.6507249985568175, simulated score: {‚Äôtrain_score‚Äô: 0.982920964830782,
‚Äôdev_score‚Äô: 0.6420233166755841, ‚Äôtest_score‚Äô: 0.647550336228104, ‚Äôscore‚Äô:
0.6420233166755841}, Visits: 2
19
20 [Node 0-0-0]
21 Plans:
22 4. Train a Random Forest classifier to leverage its ability to handle
high-dimensional data and capture non-linear relationships, and evaluate its
performance
23 Simulated: False
24 Score: avg score: 0, simulated score: {}, Visits: 0
25
26 [Node 0-0-1]
27 Plans:
28 4. Train multiple models, including a Support Vector Machine (SVM) with a radial
basis function (RBF) kernel, and evaluate their performance.
29 Simulated: False
30 Score: avg score: 0, simulated score: {}, Visits: 0
31
32 [Node 0-0-2]
33 Plans:
34 4. Implement a Neural Network with multiple layers to capture the hierarchical
patterns in the data and evaluate its performance
35 Simulated: True
36 Score: avg score: 0.6594266804380511, simulated score: {‚Äôtrain_score‚Äô: 1.0,
‚Äôdev_score‚Äô: 0.6594266804380511, ‚Äôtest_score‚Äô: 0.6702614538699305, ‚Äôscore‚Äô:
0.6594266804380511}, Visits: 1
37
38 [Node 0-0-3]
39 Plans:
40 4. Train multiple models, apply an ensemble method like Gradient Boosting to combine
them, and evaluate their performance
41 Simulated: False
42 Score: avg score: 0, simulated score: {}, Visits: 0
43
44 [Node 0-0-4]
45 Plans:
46 4. Train multiple models, perform hyperparameter tuning using Grid Search or Random
Search, and evaluate their performance
47 Simulated: False
48 Score: avg score: 0, simulated score: {}, Visits: 0
49
50 [Node 0-1]
51 Plans:
52 3. Perform feature engineering on the train, dev, and test datasets by generating
time-based features, such as the difference between consecutive frames, to capture
the rate of change in movements.
53 Simulated: True
54 Score: avg score: 0.6464940718972336, simulated score: {‚Äôtrain_score‚Äô: 1.0, ‚Äôdev_score‚Äô:
0.5985614604756948, ‚Äôtest_score‚Äô: 0.5857379626419719, ‚Äôscore‚Äô: 0.5985614604756948},
Visits: 2
55
56 [Node 0-1-0]
57 Plans:
18Preprint.
58 4. Train a Random Forest classifier to leverage its ability to handle
high-dimensional data and capture non-linear relationships
59 Simulated: False
60 Score: avg score: 0, simulated score: {}, Visits: 0
61
62 [Node 0-1-1]
63 Plans:
64 4. Train multiple models, including a Support Vector Machine (SVM) with a radial
basis function (RBF) kernel, and evaluate their performance to model the complex
decision boundaries between different gesture phases.
65 Simulated: True
66 Score: avg score: 0.6944266833187726, simulated score: {‚Äôtrain_score‚Äô: 1.0,
‚Äôdev_score‚Äô: 0.6944266833187726, ‚Äôtest_score‚Äô: 0.6928451194338062, ‚Äôscore‚Äô:
0.6944266833187726}, Visits: 1
67
68 [Node 0-1-2]
69 Plans:
70 4. Implement a Neural Network with multiple layers to capture the hierarchical
patterns in the data and evaluate its performance
71 Simulated: False
72 Score: avg score: 0, simulated score: {}, Visits: 0
73
74 [Node 0-1-3]
75 Plans:
76 4. Train multiple models, apply an ensemble method like Gradient Boosting to combine
them, and evaluate their performance
77 Simulated: False
78 Score: avg score: 0, simulated score: {}, Visits: 0
79
80 [Node 0-1-4]
81 Plans:
82 4. Train multiple models and perform hyperparameter tuning using techniques like Grid
Search or Random Search to optimize and evaluate their performance.
83 Simulated: False
84 Score: avg score: 0, simulated score: {}, Visits: 0
85
86 [Node 0-2]
87 Plans:
88 3. Perform feature engineering on the train, dev, and test datasets by creating features
that represent the spatial relationships between different body parts, such as the
distance between the hands and the head.
89 Simulated: True
90 Score: avg score: 0.6296836159165489, simulated score: {‚Äôtrain_score‚Äô:
0.7619969104124632, ‚Äôdev_score‚Äô: 0.5997286931710517, ‚Äôtest_score‚Äô:
0.604077566134264, ‚Äôscore‚Äô: 0.5997286931710517}, Visits: 3
91
92 [Node 0-2-0]
93 Plans:
94 4. Train a Random Forest classifier to leverage its ability to handle
high-dimensional data and capture non-linear relationships, and evaluate its
performance
95 Simulated: False
96 Score: avg score: 0, simulated score: {}, Visits: 0
97
98 [Node 0-2-1]
99 Plans:
100 4. Train multiple models, including a Support Vector Machine (SVM) with a radial
basis function (RBF) kernel, and evaluate their performance to model the complex
decision boundaries between different gesture phases.
101 Simulated: True
102 Score: avg score: 0.6446610772892973, simulated score: {‚Äôtrain_score‚Äô:
0.9952809245924918, ‚Äôdev_score‚Äô: 0.6372459669415207, ‚Äôtest_score‚Äô:
0.6423549137767338, ‚Äôscore‚Äô: 0.6372459669415207}, Visits: 2
103
104 [Node 0-2-1-0]
105 Plans:
106 5. Train a weighted ensemble model using the best performing models from task 4
107 Simulated: False
108 Score: avg score: 0, simulated score: {}, Visits: 0
109
110 [Node 0-2-1-1]
111 Plans:
112 5. Using the models that performed best in task 4, train a weighted ensemble
model to improve overall performance.
113 Simulated: False
114 Score: avg score: 0, simulated score: {}, Visits: 0
115
116 [Node 0-2-1-2]
117 Plans:
19Preprint.
118 5. Develop a weighted ensemble model by integrating the top-performing models
from task 4, ensuring to evaluate and adjust the weights for optimal
performance.
119 Simulated: True
120 Score: avg score: 0.6520761876370741, simulated score: {‚Äôtrain_score‚Äô: 1.0,
‚Äôdev_score‚Äô: 0.6520761876370741, ‚Äôtest_score‚Äô: 0.6563435152603494, ‚Äôscore‚Äô:
0.6520761876370741}, Visits: 1
121
122 [Node 0-2-1-3]
123 Plans:
124 5. Train a weighted ensemble model by combining the predictions of the
top-performing models from task 4 to improve overall performance.
125 Simulated: False
126 Score: avg score: 0, simulated score: {}, Visits: 0
127
128 [Node 0-2-1-4]
129 Plans:
130 5. Develop a weighted ensemble model by combining the top-performing models from
task 4, ensuring to optimize the weights for improved performance.
131 Simulated: False
132 Score: avg score: 0, simulated score: {}, Visits: 0
133
134 [Node 0-2-2]
135 Plans:
136 4. Implement a Neural Network with multiple layers to capture the hierarchical
patterns in the data and evaluate its performance
137 Simulated: False
138 Score: avg score: 0, simulated score: {}, Visits: 0
139
140 [Node 0-2-3]
141 Plans:
142 4. Train multiple models, apply an ensemble method like Gradient Boosting to combine
them, and evaluate their performance
143 Simulated: False
144 Score: avg score: 0, simulated score: {}, Visits: 0
145
146 [Node 0-2-4]
147 Plans:
148 4. Perform hyperparameter tuning using Grid Search or Random Search to train multiple
models and evaluate their performance
149 Simulated: False
150 Score: avg score: 0, simulated score: {}, Visits: 0
151
152 [Node 0-3]
153 Plans:
154 3. Apply feature selection techniques such as Recursive Feature Elimination (RFE) or
SelectKBest to identify and retain the most important features in the train, dev,
and test datasets.
155 Simulated: True
156 Score: avg score: 0.49056683315196203, simulated score: {‚Äôtrain_score‚Äô:
0.9988177730410426, ‚Äôdev_score‚Äô: 0.51620611302976, ‚Äôtest_score‚Äô: 0.525989891002361,
‚Äôscore‚Äô: 0.51620611302976}, Visits: 2
157
158 [Node 0-3-0]
159 Plans:
160 4. Train a Random Forest classifier to leverage its ability to handle
high-dimensional data and capture non-linear relationships, and evaluate its
performance.
161 Simulated: False
162 Score: avg score: 0, simulated score: {}, Visits: 0
163
164 [Node 0-3-1]
165 Plans:
166 4. Train multiple models, including a Support Vector Machine (SVM) with a radial
basis function (RBF) kernel, and evaluate their performance to model the complex
decision boundaries between different gesture phases.
167 Simulated: True
168 Score: avg score: 0.4649275532741641, simulated score: {‚Äôtrain_score‚Äô:
0.7299159411193588, ‚Äôdev_score‚Äô: 0.4649275532741641, ‚Äôtest_score‚Äô:
0.4631598897487413, ‚Äôscore‚Äô: 0.4649275532741641}, Visits: 1
169
170 [Node 0-3-2]
171 Plans:
172 4. Implement and train a Neural Network with multiple layers to capture hierarchical
patterns in the data and evaluate its performance
173 Simulated: False
174 Score: avg score: 0, simulated score: {}, Visits: 0
175
176 [Node 0-3-3]
177 Plans:
20Preprint.
178 4. Train multiple models, apply an ensemble method like Gradient Boosting to combine
them, and evaluate their performance
179 Simulated: False
180 Score: avg score: 0, simulated score: {}, Visits: 0
181
182 [Node 0-3-4]
183 Plans:
184 4. Train multiple models, perform hyperparameter tuning using techniques like Grid
Search or Random Search, and evaluate their performance
185 Simulated: False
186 Score: avg score: 0, simulated score: {}, Visits: 0
187
188 [Node 0-4]
189 Plans:
190 3. Create interaction features by combining existing features, such as the product of
velocity and acceleration, to capture complex relationships in the train, dev, and
test datasets
191 Simulated: False
192 Score: avg score: 0, simulated score: {}, Visits: 0
193
194 Generated 29 unique codes.
195 Best node: 0-1-1, score: {‚Äôtrain_score‚Äô: 1.0, ‚Äôdev_score‚Äô: 0.6944266833187726, ‚Äôtest_score‚Äô:
0.6928451194338062, ‚Äôscore‚Äô: 0.6944266833187726}
196 Dev best node: 0-1-1, score: {‚Äôtrain_score‚Äô: 1.0, ‚Äôdev_score‚Äô: 0.6944266833187726,
‚Äôtest_score‚Äô: 0.6928451194338062, ‚Äôscore‚Äô: 0.6944266833187726}
Inthiscasestudy,wedemonstratehowSELAconductsasearchcycleusingMCTS:
Pre-searchStep: Initialization
SELA begins by defining high-level stages, such as exploratory data analysis, data preprocessing,
feature engineering, and model training, which structure the overall machine learning workflow.
During the search, SELA populates these stages with specific insights, which act as experimental
configurationsforsimulation.
Step1&2: SelectionandExpansion
SELAleveragesMCTStoexplorespecificstageslikefeatureengineeringandmodeltraining. For
example, in one iteration, SELA selects Node 0-1. This node corresponds to a stage insight that
generatestime-basedfeatures,expandingintofivechildnodesrepresentingvariousmodelspecifica-
tionsandtrainingstrategies,suchasRandomForests,SupportVectorMachines,NeuralNetworks,
GradientBoosting,orGridSearch.
Step3: Simulation
Next,SELAsamplesoneoftheexpandedchildnodesforsimulation. Forinstance,whenNode0-1-
1ischosen,SELArunsacompleteexperimentwheretime-basedfeatureengineering(Node0-1)is
followedbytrainingaSupportVectorMachine(SVM)withakernelspecifiedbyNode0-1-1. The
simulationyieldsanevaluationscore.
Step4: Backpropagation
Afterthesimulation, theresultingperformancescoreispropagatedbackthroughthetree. Forex-
ample,aftersimulatingNode0-1-1,MCTSupdatesthenumericfeedbackforitsparentnodes,such
as Node 0-1 and Node 0. The search cycle repeats from Steps 1 to 4 until a stopping condition is
reached.
Post-searchStep: BestNodeSelection
Inthefinalphase,SELAselectsthenoderepresentingthebest-performingsolution.Inthisexample,
Node 0-1-1, using an SVM with an RBF kernel, achieved the highest score in the current dataset
bycombiningeffectivefeatureengineeringwithadvancedmodeltraining. SELAthenpresentsthe
codeassociatedwiththisnodeastheoptimalsolution.
21