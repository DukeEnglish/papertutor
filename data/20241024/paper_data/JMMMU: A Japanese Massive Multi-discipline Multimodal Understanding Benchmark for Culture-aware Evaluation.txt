JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding
Benchmark for Culture-aware Evaluation
ShotaOnoharaâ™£* AtsuyukiMiyaiâ™£* YukiImajukuâ™£* KazukiEgashiraâ™£* JeonghunBaekâ™£*
XiangYueâ™¢ GrahamNeubigâ™¢ KiyoharuAizawaâ™£
â™£TheUniversityofTokyo â™¢CarnegieMellonUniversity
Abstract demonstratedremarkableperformanceacrosstasks
rangingfromcommonsensereasoningtodomain-
Accelerating research on Large Multimodal
specific,expert-levelchallenges(Antoletal.,2015;
Models (LMMs) in non-English languages
Liuetal.,2023a,2024c;Yueetal.,2024). Astheir
is crucial for enhancing user experiences
capabilities grow, the need for robust criteria to
acrossbroaderpopulations. Inthispaper,we
introduce JMMMU (Japanese MMMU), the evaluateLMMshasbecomeincreasinglyimportant,
firstlarge-scaleJapanesebenchmarkdesigned highlightingtheroleofcomprehensivebenchmarks
toevaluateLMMsonexpert-leveltasksbased inassessingthefullscopeoftheirabilities.
ontheJapaneseculturalcontext. Tofacilitate However, current benchmarks focus primarily
comprehensive culture-aware evaluation,
onperformanceinEnglish(Liuetal.,2024c;Yue
JMMMUfeaturestwocomplementarysubsets:
et al., 2024; Li et al., 2024b; Liu et al., 2023b;
(i) culture-agnostic (CA) subset, where the
Yuetal.,2024;Fuetal.,2024),withlessempha- culture-independent subjects (e.g., Math) are
sis on evaluation in other languages. Given that
selectedandtranslatedintoJapanese,enabling
one-to-onecomparisonwithitsEnglishcoun- LMMsarewidelyusedacrossdiverselanguages,it
terpartMMMU;and(ii)culture-specific(CS) isimperativetoevaluatetheirperformancebeyond
subset,comprisingnewlycraftedsubjectsthat English. Additionally, such multilingual evalua-
reflectJapaneseculturalcontext. UsingtheCA
tions should actively involve contributions from
subset,weobserveperformancedropinmany
diversecommunities,ensuringthattheassociated
LMMswhenevaluatedinJapanese,whichis
culturalcontextsareappropriatelyconsidered.
purelyattributabletolanguagevariation. Using
Inthispaper,weintroduceJMMMU(Japanese
the CS subset, we reveal their inadequate
Japanese cultural understanding. Further, by MMMU), the first benchmark designed to evalu-
combiningbothsubsets,weidentifythatsome ateLMMsonextensive,multi-disciplinarytasksin
LMMs perform well on the CA subset but Japanesethatrequirecollege-levelsubjectknowl-
not on the CS subset, exposing a shallow edge,deliberatereasoning,andculturalunderstand-
understanding of the Japanese language that
ing. The overview of JMMMU is shown in Fig-
lacks depth in cultural understanding. We
ure 1. JMMMU draws inspiration from the well-
hope this work will not only help advance
establishedMMMU(Yueetal.,2024)andexpands
LMM performance in Japanese but also
existing culture-aware Japanese benchmarks (In-
serve as a guideline to create high-standard,
culturally diverse benchmarks for multilin- oue et al., 2024b; SakanaAI, 2024c) by over 10
gual LMM development. The project page times, with 1,320 questions using 1,118 images,
is https://mmmu-japanese-benchmark. coveringadiverserangeofsubjects.
github.io/JMMMU/.
JMMMU offers two key subsets: (i) Culture-
Agnostic (CA) Subset: We extracted and
1 Introduction translated the culture-agnostic components from
MMMU.Thissubsetallowsforadirectcompari-
In recent years, large language models (LLMs)
sonoftheperformancegapsbetweenEnglishand
haverevolutionizedthefieldoflanguageprocess-
Japanese that are purely attributable to language
ing(Chenetal.,2023a;vic,2023;Touvronetal.,
variations. (ii)Culture-Specific(CS)Subset: We
2023; Wei et al., 2023). Building on the success
carefully crafted brand-new questions that align
ofLLMs,largemultimodalmodels(LMMs)have
with the Japanese cultural context. With CS sub-
*Equalcontribution. set,developerscanassesscapabilitiesspecifically
1
4202
tcO
22
]LC.sc[
1v05271.0142:viXraCulture-Agnostic Subjects Culture-Specific Subjects
(720, 54.5%) (600, 45.5%)
Art & Psych. Science (120, 9.1%) Jpn. Art Jpn. Heritage
(90, 6.8%) Biology Tech & Eng. Jpn. Art (150, 11.3 %) (150, 11.3 %)
Design Chemistry Jpn.
Music Physics Heritage
Psychology Math
Business Medicine Tech & Eng. Medicine
(150, 11.3%) (150, 11.3 %) (210, 15.9%) Jpn.
Agriculture History Jpn. History World History
Arch & Eng. Business (150, 11.3 %) (150, 11.3 %)
Accounting Basic Med. Computer Sci.
Economics Clinical Med. Electronics
Finance Diagnostics Energy & Power Science Art World
Manage Pharmacy Materials & History
Marketing Public Health Mech Eng. Psych.
Figure1: OverviewoftheJMMMUdataset. JMMMUincludes720culture-agnostic(translation-based)questions
and600culture-specific(newlycreated)questions,totaling1,320questions,thusexpandingtheexistingculture-
awareJapanesebenchmark(Inoueetal.,2024b)byover10times. JMMMUservesasadiagnostictoolforassessing
bothJapaneseculturalunderstandingandculture-agnosticlanguageunderstandingcapability.
tailored to Japanese culture. Together, JMMMU estimationofanLMMâ€™smultilingualcapabil-
serves as a diagnostic tool for model developers, itywithouttrulyunderstandingthecontextof
providing valuable feedback for future improve- theindividualcultures.
ments.
Evaluating15open-sourceLMMsandthreead- OurfindingsindicatethatEnglish-centeredper-
vanced proprietary LMMs on JMMMU, our key formance evaluation may lead to biased develop-
findingsaresummarizedasfollows: ment,neglectingnon-Englishlanguages. Wehope
our findings not only spark interest in Japanese
â€¢ Overallperformanceisupto58.6%,leaving performance but also motivate the community to
greatroomforimprovementintheutilityof craftavarietyofhigh-standardbenchmarksthaten-
theJapanesecontext. compassdiverseculturesandtheirassociatedlan-
guages, thereby promoting more inclusive LMM
â€¢ TheCAsubsetrevealsthatmostmodelsper-
development.
form worse when asked in Japanese than in
English(upto8.6%),evenwhenthequestion 2 RelatedWork
asks exactly the same content. This apple-
to-applecomparisonclearlyindicatesthatthe LargeMultimodalModels(LMMs) Following
utilityinnon-Englishlanguagesisfallingbe- the success of large language models (LLMs),
hindincurrentLMMs. many LMMs have been developed with im-
provedknowledgeandinstruction-followingcapa-
â€¢ The CS subset reveals that models trained bilities(Liuetal.,2023b,2024a,b;Lietal.,2024a;
onJapanesedatasetsperformthebestamong Ye et al., 2024; Zhao et al., 2023; Li et al., 2023;
open-sourcemodels,suggestingthatsuchfine- Monajatipooretal.,2023;Zhaoetal.,2024). How-
tuningeffectivelycontributestoincorporating ever,theprogressofthesemodelsistypicallyeval-
Japaneseculturalknowledgeintothemodels. uatedonEnglishbenchmarks(Yueetal.,2024;Liu
et al., 2024c). Therefore, a significant challenge
â€¢ Combining both subsets, we reveal a signif-
remains in accurately evaluating the capabilities
icantdiscrepancyamongthestate-of-the-art
ofotherlanguages,highlightingtheneedfornon-
proprietarymodels. Whiletheyperformsim-
Englishbenchmarks.
ilarly on English benchmarks and even on
culture-agnosticquestionsinJapanese,their LMM Benchmarks Among various recent
performancesaresignificantlydifferentonCS benchmarks (Li et al., 2024b; Liu et al., 2023b,
subset. Thisfindingisparticularlyalarming, 2024c; Lu et al., 2024; Yue et al., 2024; Miyai
asitindicatesthatevaluationexclusivelyona et al., 2024), MMMU (Yue et al., 2024) is the
translation-basedbenchmarkcouldriskover- mostwidelyusedtomeasuretheadvancementsof
2cutting-edge LMMs. MMMU requires advanced Table1: OverviewofJapaneseLMMbenchmarks.
JMMMUisthefirstbenchmarkthatevaluatesexpert-
university-level knowledge and reasoning across
levelskillsandisthelargestamongculture-awarebench-
abroaderrangeofsubjects,enablingamorecom-
marks.
prehensive and expert-level evaluation. Subse-
quently, CMMMU (Zhang et al., 2024) has been Benchmark Culture Level QuestionsImages
JA-VG-VQA-500(SakanaAI,2024b) âœ— Commonsense 500 500
proposed as its Chinese counterpart. While CM- LLaVA-Bench-in-the-wild(Turing,2024b) âœ— Commonsense 60 24
MMUcomprisesentirelynewculture-specificques- JA-Multi-Image-VQA(SakanaAI,2024a) âœ“ Commonsense 55 39
JA-VLM-Bench-in-the-wild(SakanaAI,2024c) âœ“ Commonsense 50 42
tions, our JMMMU has not only culture-specific HeronBench(Inoueetal.,2024b) âœ“ Commonsense 102 21
JMMMU(Ours) âœ“ Expert 1,320 1,118
subjectsbutalsotranslation-basedculture-agnostic
subjects, facilitating one-to-one comparisons be-
tweenEnglishandJapaneseusingtheexactsame 28 different subjects. This benchmark is strategi-
questions. In line with multilingual ability eval- callydividedintotwodistinctcategories: culture-
uation, several VQA benchmarks have been pro- agnosticandculture-specificsubjects.
posed(Gaoetal.,2015;Changpinyoetal.,2022;
Culture-agnosticsubsetconsistsof24subjects
Guptaetal.,2020;Liuetal.,2021;Pfeifferetal.,
with720questionsacrossfivedisciplines: (1)Art&
2021;Tangetal.,2024;Romeroetal.,2024). How-
Psychology,(2)Business,(3)Health&Medicine,
ever,unliketheMMMUseries,theirprimaryfocus
(4)Science,and(5)Tech&Engineering. Culture-
is on daily knowledge, (e.g., Pop Culture, Sports
specificsubsetconsistsof600questionsacrossfour
inCVQA(Romeroetal.,2024)),stillleavingthe
subjects: (1)JapaneseArt,(2)JapaneseHeritage,
multilingualexpert-levelreasoningskillsasanim-
(3) Japanese History, and (4) World History. We
portantdirectionforfuturework.
providesamplequestionsinAppendixE
JapaneseLMMBenchmarks Thedevelopment
3.2 DataCurationProcess
ofJapaneseLMMbenchmarksremainsbehindthat
of English benchmarks. While efforts have been JMMMUisderivedfromthewidely-usedvalida-
made to create Japanese benchmarks as shown tion set of MMMU, consisting of 900 questions
in Table 1, they still exhibit the following criti- across30subjects. ToconstructJMMMU,wefirst
callimitations: (i)Existingbenchmarks(Shimizu examinedtheculturaldependenciesintheoriginal
etal.,2018;Turing,2024c,b;Inoueetal.,2024b; MMMU subjects. For culture-agnostic subjects,
SakanaAI, 2024c,a) focus primarily on common wetranslatedthequestionsintoJapanese. Wefur-
sense knowledge but do not adequately address therreplacedculture-dependentsubjectswithnew
expert-levelknowledge,despitetheadvancement subjects that are conceptually similar, but better
inLMMsandtheimportanceofevaluatingsuchca- alignedwiththeJapanesecontext. Alltheprocess
pabilities. (ii)Manydonotaccountforculturaldif- hasbeenconductedwiththehelpof19university
ferences. Theyareoftencreatedbydirectlytrans- students, including the authors, who have expert
latingexistingEnglishbenchmarks(Shimizuetal., knowledge in the respective fields and native flu-
2018;Turing,2024c,b),resultinginquestionsthat ency in Japanese. Here, we describe the dataset
mayfeelunfamiliartoJapanesepeopleduetocul- creationprocessindetail.
turalcontext. (iii)Althoughrecentbenchmarksat-
tempttoconsiderculturaldifferences(Inoueetal., ExaminingCulturalDependenciesinMMMU
2024b;SakanaAI,2024c,a),theyarelimitedinsize Amongthe30subjectsinMMMU,weidentified
(up to 102 questions), raising concerns about the that questions in six subjects are particularly un-
reliabilityofquantitativeevaluation. Ourproposed familiar to Japanese people and thus we catego-
JMMMUaddressesallthreeoftheaforementioned rized them as culture-specific subjects; Art, Art
challenges,significantlyadvancingthebenchmark Theory, Geography, History, Literature, and So-
intherealmofJapaneseevaluation. ciology. The remaining subjects (e.g., Biology,
Chemistry, Computer Science, Electronics) exist
3 JMMMUBenchmark
inJapanwithsimilarcontents,andthuswecatego-
rizedthemasculture-agnosticsubjects. Asaresult,
3.1 OverviewofJMMMU
weexcludedthesixculture-specificsubjectswhile
AsillustratedinFigure1,JMMMUcontainsato- keepingtheremaining24culture-agnosticsubjects
talof1,320questionsand1,118images,covering inJMMMU.
3callycoveredinJapanesetextbookstobetter
reflecttheJapaneseeducationalcontextthan
HistoryinMMMU.
TheimagesareprimarilysourcedfromWikime-
Translation dia Commons1, ensuring that all selected images
areavailableunderlicensessuitableforpublicre-
lease. Incraftingquestions,weaimedtokeepthe
text as simple as possible and ensure that no op-
tionsstandout,makingithardtoguessthecorrect
choicewithoutreferringtotheimage.
Figure2: Exampleoftheimagetranslationprocess.
3.3 ComparisonwithOtherJapanese
Englishwordsintheimagearemanuallyoverwritten
MultimodalBenchmarks
withJapanese.
Here, we compare JMMMU with other Japanese
multimodal benchmarks, provided in Table 1,
TranslatingCulture-AgnosticSubjects Theex-
to demonstrate its uniqueness. First and fore-
pertswereprovidedwiththeoriginalEnglishtexts,
most, JMMMU is the only benchmark that in-
GPT-4o-translatedquestiontextsinJapanese,and
cludesexpert-levelquestions,whiletherestofthe
corresponding images. For texts, their task in-
benchmarks(Shimizuetal.,2018;Turing,2024a;
volved: (i) refining the auto-translated Japanese
SakanaAI,2024a,c;Inoueetal.,2024b)arefocused
texttoensurenaturalnessandfluency;(ii)confirm-
oncommonknowledge. Further,JMMMUiscare-
ingthattechnicaltermsandacademicexpressions
fullydesignedtotaketheJapaneseculturalcontext
adheretoconventionalJapaneseusage;and(iii)ad-
intoaccount. Whilesomeexistingbenchmarkscon-
justingthecurrencytoreflecttypicaldigitlengths
siderJapaneseculture,theyarealllimitedinsize
in Japanese yen (Â¥). For currency conversion, a
(onlyupto102questionsinInoueetal.(2024b)),
simplifiedconversion($1â†’Â¥100)wasemployed
raising concerns about whether reliable quantita-
toavoidmakingthecalculationunnecessarilycom-
tiveevaluationscanbeconducted. Incontrast,JM-
plicated. Forimages,weaskedtheexpertstoover-
MMUcontainsmorethan10timeslargerthanany
writetheEnglishtextwithJapanesetextbyusing
oftheexistingculture-awarebenchmarks.
an image editing tool. An example of the image
translationprocessispresentedinFigure2. 4 Experiments
Consequently, we obtained 720 questions cov-
4.1 Setup
ering24culture-agnosticsubjectsfullytranslated
andadaptedforJapaneseusage. LMMs WeevaluateadiversesetofLMMs.
CreatingCulture-SpecificSubjects Recogniz-
â€¢ ProprietaryLMMs: GPT-4o(OpenAI,2024)
ingthatmostoftheremovedsubjectsarerelatedto
Gemini1.5Pro(DeepMind,2024;Reidetal.,
artorsocialstudies,wecreatedthefollowingnew
2024) and Claude 3.5 Sonnet (Anthropic,
subjectstotestsimilarknowledgeintheJapanese
2024).
context:
â€¢ Japanese LMMs: LLaVA CALM2 (Ina-
gaki,2024)andEvoVLMJPv2(Inoueetal.,
â€¢ Japanese Art: Questions about traditional
2024a),whicharetrainedonbothEnglishand
Japaneseart,suchasUkiyo-eandNoh.
Japanesedatasets.
â€¢ Japanese Heritage: Questions about tradi- â€¢ Open-source LMMs: LLaVA-OneVision
tional, culturally significant locations and 0.5B&7B(Lietal.,2024a),LLaVA1.6-13B
buildings in Japan such as temples and & 34B (Liu et al., 2024b), Phi-3 & 3.5 Vi-
shrines. sion (Abdin et al., 2024), InternVL2-2B &
â€¢ JapaneseHistory: Questionsabouthistorical 8B (Chen et al., 2023b), xGen-MM (Xue
incidentsinJapan. et al., 2024), Idefics2-8B (LaurenÃ§on et al.,
2024b),Idefics3-8B(LaurenÃ§onetal.,2024a),
â€¢ WorldHistory: Questionsaboutglobalhistor-
ical incidents, but based on the content typi- 1https://commons.wikimedia.org/
4Table2: Overallresults. CA(EN)showstheresultoncultureagnosticsubsetinEnglish. Therestoftheresults
areaverageandindividualsubjectsâ€™scoresonJMMMU.â€ denotesJapaneseLMMs. Thebest-performingmodel
amongopensourceandproprietarymodelsareinbold. Overall,theperformanceisupto40.5%foropen-source,
and58.6%forproprietarymodels,leavinggreatroomforimprovement.
CA Jpn. Jpn. Jpn. World Art& Health&Tech&
Models Overall CS CA BusinessScience
(EN) Art HeritageHistoryHistoryPsych. Medicine Eng.
(1,320) (600)(720)(720)(150) (150) (150) (150) (90) (150) (120) (150) (210)
Random 24.8 25.0 24.6 24.6 25.0 25.0 25.0 25.0 25.4 25.0 22.8 25.6 24.3
OpenSource
LLaVA-OV-0.5B 26.0 23.3 28.2 29.4 22.7 22.7 24.0 24.0 26.7 27.3 24.2 30.7 30.0
InternVL2-2B 28.3 29.2 27.6 31.9 31.3 22.7 30.7 32.0 30.0 30.0 30.8 25.3 24.8
xGen-MM 28.6 28.2 28.9 35.7 30.0 20.7 22.7 39.3 32.2 21.3 22.5 36.7 31.0
Phi-3v 29.5 26.5 31.9 37.6 31.3 18.7 29.3 26.7 26.7 28.7 25.8 37.3 36.2
LLaVA-1.6-13B 31.1 33.7 29.0 29.9 32.0 24.0 32.0 46.7 25.6 28.7 30.0 34.0 26.7
Idefics2-8B 31.9 37.0 27.6 35.1 40.7 24.0 30.0 53.3 32.2 22.7 22.5 32.0 29.0
Phi-3.5v 32.4 34.3 30.8 39.2 37.3 27.3 35.3 37.3 27.8 31.3 30.0 36.7 28.1
â€ LLaVACALM2 34.9 41.5 29.4 29.9 42.7 36.7 40.0 46.7 27.8 26.0 26.7 34.0 31.0
Mantis8B 35.5 39.5 32.2 36.0 42.0 30.0 35.3 50.7 37.8 28.0 31.7 37.3 29.5
CogVLM2-19B 36.1 39.7 33.1 36.8 39.3 24.0 36.0 59.3 28.9 32.7 30.8 30.0 38.6
Idefics3-8B 37.3 42.8 32.8 36.9 43.3 24.7 42.0 61.3 34.4 28.0 26.7 38.0 35.2
â€ EvoVLMJPv2 38.1 45.2 32.2 33.9 44.0 40.0 42.0 54.7 32.2 28.7 28.3 38.7 32.4
InternVL2-8B 38.3 42.5 34.7 43.3 41.3 38.0 35.3 55.3 40.0 36.0 34.2 34.0 32.4
LLaVA-1.6-34B 39.8 43.2 37.1 45.7 42.0 36.0 40.7 54.0 42.2 41.3 25.0 36.7 39.0
LLaVA-OV-7B 40.5 43.0 38.5 45.1 36.0 30.7 37.3 68.0 41.1 36.7 31.7 38.7 42.4
Proprietary
Claude3.5Sonnet 50.8 51.0 50.6 52.1 39.3 46.7 54.7 63.3 53.3 56.7 51.7 55.3 41.0
Gemini1.5Pro 51.5 60.3 44.2 51.1 54.7 55.3 55.3 76.0 51.1 44.0 44.2 48.0 38.6
GPT-4o 58.6 66.7 51.8 52.1 60.7 70.7 58.7 76.7 53.3 55.3 45.8 61.3 45.2
TextOnly
GPT-4otext 38.1 35.5 40.3 44.9 32.7 32.0 35.3 42.0 38.9 36.0 41.7 45.3 39.5
CogVLM2-19B (Hong et al., 2024), and notrespondinaparsableformat,arandomchoice
Mantis-8B(Jiangetal.,2024). isassignedasitsanswer.
In Appendix A, we provide further details of 4.2 MainResult
thesemodels,withaparticularfocusonJapanese
Table2demonstratestheevaluationresultsonour
languagesupport.
JMMMU benchmark. We provide the average
Text-onlyLLM Asareference,wepresentthe scores across all subjects, culture-agnostic (CA)
accuracyofGPT-4owhenprovidedonlywiththe subjects,andculture-specific(CS)subjects,aswell
questiontextandchoices,withoutimages. asscoresonindividualsubjects. Forcomparison,
wealsoprovidetheperformanceonCAsusetinEn-
Evaluation The evaluation method is based on
glishCA(EN).NotethatCA(EN)isoftensmaller
the setup in MMMU (Yue et al., 2024). Prompts
thantheoverallaverageofMMMUgivenbyYue
aretranslatedasfollows: formultiple-choiceques-
etal.(2024)becausesubjectsselectedasCAarerel-
tions, ä¸ãˆã‚‰ã‚ŒãŸé¸æŠè‚¢ã®ä¸­ã‹ã‚‰æœ€ã‚‚é©åˆ‡ãª
ativelydifficultamongallsubjectsinMMMUasit
å›ç­”ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã‚’ç›´æ¥è¨˜å…¥ã—ã¦ãã 
oftenrequiresstrongerreasoningcapabilities(e.g.,
ã•ã„ã€‚ (Answer with the optionâ€™s letter from the
Math).
givenchoicesdirectly.) ;andforopen-endedques-
Here,wesummarizeourkeyobservations.
tions, è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’å˜èªã‚„çŸ­ã„ãƒ•ãƒ¬ãƒ¼
ã‚ºã§è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚ (Answerthequestion ChallengingNature Inourexperiment,theper-
usingasinglewordorphrase.). formance is up to 40.5% for open-source, and
FollowingMMMU,(i)wepreparearule-based 58.6%forproprietarymodels,leavinggreatroom
parser to extract the modelâ€™s choice from typical for improvement. This also highlights a signifi-
generationstylessuchasâ€œç­”ãˆã¯Aâ€(Theanswer cantgapbetweenopen-sourceandproprietarymod-
is A), making the evaluation robust to some vari- els,presentingamoredifficultchallengeforopen-
etiesofanswerstyles,and(ii)whenamodeldoes sourcemodels.
570 Table3: Theeffectoftranslation. Eachcolumnshows
LLaVA-OV 0.5B the model performance when image (I) and text (T)
InternVL2 2B
xGen-MM arein Japanese(jp) orin English(en). âˆ† shows the
60 Phi-3 Vision i
LLaVA-NeXT 13B differencefromI enT en.
Idefics2 8B
Phi-3.5 Vision Model I T I T (âˆ† ) I T (âˆ† )
50 Mantis-8B en en en jp 1 jp jp 2
CogVLM2 19B
Idefics3 8B LLaVA-1.6-13B 26.4 31.9(+5.5) 29.2(+2.8)
InternVL2 8B
40 LLaVA-NeXT 34B Phi-3.5v 39.2 33.6(-5.6) 31.1(-8.1)
LLaVA-OV 7B LLaVA-CALM2 29.4 28.3(-1.1) 31.4(+2.0)
LLaVA CALM2
30 EvoVLM-JP v2 CogVLM2-19B 32.8 31.9(-0.9) 34.4(+1.6)
Claude 3.5 Sonnet
Gemini 1.5 Pro EvoVLMJPv2 30.0 30.8(+0.8) 28.6(-1.4)
GPT-4o
20 InternVL2-8B 43.9 38.3(-5.6) 37.2(-6.7)
20 30 40 50 60
LLaVA-1.6-34B 43.6 40.8(-2.8) 38.9(-4.7)
Culture Agnostic [%]
LLaVA-OV-7B 45.0 38.3(-6.7) 35.6(-9.4)
Figure3: Scorecorrelationbetweensubsets. While
proprietarymodels(â– )performthebestonbothsubsets,
JapaneseLMMs(â‹†)performremarkablyhighonCS whosegapfromGPT-4oisâ‰¤16.7%,indicatingthe
subset compared to models that perform similarly on particularinadequacyoftheopen-sourcemodelin
CAsubset. Heritagedomain.
GPT-4ovs. Claude3.5Sonnet Werevealasig-
TheEffectofTranslationinCASubset First, nificantperformancegapbetweenthetwoleading
as a general trend, the score on the CA subset is models; GPT-4o and Claude 3.5. They are state-
significantlylowerthanitsEnglishcounterpart(CA of-the-artmodelsandtheirperformanceisknown
(EN)inTable2)withanaveragedropof4.5%. This to be similar with only 0.8% difference on the
indicatesthat,evenforthesamequestions,many MMMUbenchmarkinEnglish(Anthropic,2024).
models perform worse when asked in Japanese. Further, their performance is similar even on CA
Second, despite such a general trend, Japanese- splitinJapanese(1.2%differenceinTable2). How-
madeLMMs(i.e.,LLaVACALM2andEvoVLM ever,ontheCSsplit,GPT-4ooutperformsClaude
JP v2) face a minimal drop (up to 1.7 %), which 3.5Sonnetbyasubstantial15.7%.
impliesthatincorporatingtheJapanesedatasetsuc- Thisstronglyindicatesthatamodelâ€™sJapanese
cessfully mitigates the performance gap between language skill and its understanding of Japanese
EnglishandJapanese. culture should be separately discussed. Our re-
searchispioneeringinrevealingthis,adiscrepancy
ThePerformanceofJapaneseLMMs Figure3
thatwouldhaveremainedobscuredwithoutcom-
demonstrates the correlation between the scores
bining translation-based CA subjects and brand-
ontheCAandCSsubjects. TheJapaneseLMMs,
newCSsubjects. Ourfindingunderscoresthelim-
LLaVACALM2andEvoVLMJPv2,showhigher
itations of relying exclusively on auto-translated
scores on CS subjects compared to other mod-
benchmarks for a thorough evaluation of model
els that perform similarly on CA subjects. This
capabilitiesinnon-Englishlanguages,highlighting
stronglyindicatestheirproficiencyinCSsubjects.
the importance of evaluating models on culture-
Ontheotherhand,however,comparedtostronger
specificquestions.
modelssuchasInternVL2-8b,LLaVA1.6-34b,and
LLaVA-OV-7b, the Japanese LMMs show lower
5 Analysis
scores on CA subjects, suggesting room for im-
provementintheirgeneralreasoningandproblem-
5.1 AblationonImageTranslation
solvingcapabilitiesinculture-agnosticcontext.
Here, we investigate how translating text and im-
Scores on Japanese Heritage Among CS sub- ages affects the model performances. Using 360
jects, the performance of open-source models questionsfromtheculture-agnosticsubsetwhich
is particularly low in Japanese Heritage (Ta- involvedtranslationofbothtextsandimages,we
ble 2). Even the best-performing open-source comparethescoresinEnglish(I T ),whenonly
en en
model(EvoVLMJPv2)scores30.7%lowerthan textistranslated(I T ),andwhenbothtextand
en jp
GPT-4o in Japanese Heritage, while in other CS imagesaretranslated(I T ). Weprovidescores
jp jp
subjects, there is at least one open-source model forselectedmodelsinTable3andthefullsetinAp-
6
]%[
cificepS
erutluCTextual Misunderstanding
4.8%
EN: , JP:
256 258
(35.6%) (35.8%) EN: , JP: Answer
EN: , JP: Rejection
108 98 EN: , JP: 10.6%
(15.0%)(13.6%)
(a) GPT-4oâ€™sErrordistributioninculture-
agnosticsubjects. ImageRecognition Lack of Knowledge
Errors 53.8%
30.8%
Question: In <image 1>. ğ‘£#=sin2ğœ‹ğ‘¡. Find an expression for ğ‘–and
calculate ğ‘–at the instants ğ‘¡=0.
(<image 1>ã«ãŠã„ã¦ğ‘£#=ğ‘ ğ‘–ğ‘›2ğœ‹ğ‘¡ã§ã‚ã‚‹ã€‚ğ‘–ã®å¼ã‚’æ±‚ã‚ã€
ğ‘¡=0ã«ãŠã‘ã‚‹ğ‘–ã‚’è¨ˆç®—ã—ãªã•ã„ã€‚)
Option:
A.3ğœ‹Ã—10!" A
B.ğœ‹Ã—10!" A
C.2ğœ‹Ã—10!" A Figure5:Errordistributionoverculture-specificsub-
D.ğœ‹Ã—10!$ A Ground Truth: C.2ğœ‹Ã—10!" A
jects. LackofKnowledgeisthemajorityerrortypeat
GPT-4oâ€™s Answer GPT-4oâ€™s Answer in Japanese:
in English: ã‚³ãƒ³ãƒ‡ãƒ³ã‚µã®é›»æµğ‘–ã¯ã€ã‚³ãƒ³ãƒ‡ãƒ³ã‚µã®é›»åœ§ğ‘£!ã®æ™‚é–“å¾®åˆ†ã«ã‚­ãƒ£ãƒ‘ã‚·ã‚¿ãƒ³ over50%.
ã‚¹ğ¶ã‚’æ›ã‘ãŸã‚‚ã®ã§ã™ã€‚ã™ãªã‚ã¡ã€(The current ğ‘–of a capacitor is the
B.ğœ‹Ã—10!" A c T ä¸ " ã—"a #h $p !ãˆ ãŸaa =t c ã‚‰ ãŒii s "t ",a ã‚Œ ã£) $n (ğ‘–c sãŸ ã¦=e in ğ‘£ ã€ğ¶ ğ¶ 2! ğœ‹ ("m =" Tğ‘¡# $u )! hs l u=t ii n sp ,)22l i ğ‘–ğœ‹ğœ‹ed =ğ‘¡ c ã‚’ ob ğ¶y så¾® ( 3t 2h 2åˆ† ğœ‹e
ğœ‹
ğ‘¡t ã— )i cm oã¾ se d ã™ 2e ğœ‹r ã€‚i ğ‘¡v (a Gti iv ve e o nf ğ‘£t !he = ca sp inac 2i ğœ‹to ğ‘¡r ,â€™s w v eo dlt ia ffg ee r eğ‘£ n! t. i ate:)
ã“ã“ã§ã€ğ¶=10%&Fã§ã™ã®ã§ã€(Here, ğ¶=10%&F, so)ğ‘–=10%&3 guagesonculture-agnosticsplit(only0.3%differ-
2ğœ‹cos2ğœ‹ğ‘¡ ğ‘¡=0ã®ã¨ãã€(At ğ‘¡=0,) ğ‘–=10%&32ğœ‹cos0
cos(0)=1ãªã®ã§ã€(Since cos(0)=1,) ğ‘–=10%&32ğœ‹31=2ğœ‹310%&A
ã—ãŸãŒã£ã¦æ­£ã—ã„é¸æŠè‚¢ã¯Cã§ã™ã€‚(Therefore, the correct choice is C.) ence in Table 2), we have found that there are a
(b) AnExamplequestionwhereGPT-4oanswers significantamount(28.6%)ofquestionstowhich
correctlyonlyinJapanese. itansweredcorrectlyonlyineitheroneofthelan-
Figure4: (a)Thereareaconsiderableamountofques- guages. Wenowinvestigatethisphenomenon. For
tionstowhichGPT-4oanswerscorrectlyonlyineither questionsansweredcorrectlyonlyinEnglish(or-
oneofthelanguages(yellow+orange). (b)InJapanese, ange in Figure 4(a)), we observe simple perfor-
the model relatively more often goes against the in-
mance degradation after translation. In contrast,
structionthataskstoanswerdirectlyandgeneratesits
wehavefoundsomedistinctiveexamplesintheop-
reasoningprocess,leadingtoacorrectanswer.
positecase(yellow). InanexampleofFigure4(b),
GPT-4ooutputsonlythedirectanswerinEnglish,
pendix B.2. Many models experience a drop in whereasinJapanese,themodelincludesthereason-
scores by text translation, with further degrada- ingprocessinitsresponsealthoughthemodelisin-
tionobservedwhenimagesarealsotranslated(i.e., structedtogeneratethechoicedirectlybyusingthe
0 > âˆ† > âˆ† ). However,somemodelsexhibitdif- promptinSection4.1. Forafaircomparisonwith
1 2
ferentperformancetrends,showingadropbytext MMMU(Yueetal.,2024),wecountaresponseto
translationbutanimprovementbytranslatingboth be correct as far as the modelâ€™s response is accu-
(i.e.,âˆ† < 0 < âˆ† ),orviceversa. Overall,while rateandcanbeparsedbyarule-basedalgorithm,
1 2
thetrendsarecomplex,ourresultindicatesthattext- regardless of its instruction-following ability. As
only translation, as is done in many non-English aresult,thescorescansometimesbecounterintu-
benchmarks,couldresultinabiasedperformance itivelyoverestimatedduetothelackofinstruction
evaluation. Rigorousinvestigationonthispointis followingskillsinJapanese. Whiletheprimaryfo-
leftforfuturework. cusofJMMMUisonevaluatingexpertknowledge
andsupportingtheimprovementofsuchcapabili-
5.2 ErrorsinCulture-agnosticSubjects ties, our findings highlight a crucial direction for
futurework: measuringandenhancinginstruction-
JMMMU shares 720 culture-agnostic questions
followingabilityinnon-Englishlanguages.
withMMMU,whichallowsustocomparetheout-
putonebyone. Usingthesequestions,weevaluate
5.3 ErrorsinCulture-specificSubjects
howtranslationaffectsmodelperformance. Taking
GPT-4oasanexample,weclassifytheresponses This section presents an analysis of the tendency
intofourcategoriesbasedonwhethertheyarecor- ofGPT-4oâ€™serrorsintheculture-specificsubjects.
rectorincorrectineachlanguage. Figure4presents Toinvestigatethecausesoftheseerrors,weman-
theresultsbeforeandaftertranslation. Theresults uallyreviewGPT-4oâ€™sresponsesandclassifythe
ontheothermodelsareprovidedinAppendixB.1 errorsintofourcategories: (i)LackofKnowledge,
While GPT-4o performs similarly in both lan- wherethemodelsuccessfullyextractsthenecessary
7Lack of Knowledge Image Recognition Error AnswerRejection Textual Misunderstanding
Question: <image 1>ã«ã¯æ£Ÿé£¾ãŒå–ã‚Šä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ Question: <image 1>ã§â¾’ã‚‰ã‚Œã‚‹ã“ã¨ã§æœ‰åãªå‹•ç‰©ã¯ Question: <image 1>ã®â¼ˆç‰©ãŒç ”ç©¶ã—ã¦ã„ãŸ Question: <image 1>ã®ä½œè€…ã®ä»£è¡¨ä½œã§ã¯ãªã„ã‚‚ã®ã‚’
ã™ãŒã€ã“ã‚Œã‚’åˆ¶ä½œã—ãŸã¨ã•ã‚Œã‚‹â¼ˆç‰©ã¯èª°ã§ã™ã‹ï¼Ÿ ä½•ã§ã—ã‚‡ã†ï¼Ÿ ç—…æ°—ã¨ã—ã¦ä»£è¡¨çš„ãªã‚‚ã®ã¯ä½•ã§ã—ã‚‡ã†ï¼Ÿ é¸ã³ãªã•ã„
(An ornamental ridgepole is attached to <image 1>; (What animal is famous for being seen in the image?) (What is a representative disease that the person (Select the work that is not a representative work
who is the person said to have created it?ï¼‰ Option: in <image 1> was researching? ) by the author of <image 1>)
Option: A. ãƒ¤ãƒ³ãƒãƒ«ã‚¯ã‚¤ãƒŠ(Yanbaru rail) Option: Option:
A. â¼©æ©‹å·æ°¸æ˜Œ(EishoKobashigawa) B. ã‚¤ãƒªã‚ªãƒ¢ãƒ†ãƒ¤ãƒãƒã‚³(Iriomotecat) A.â»©ç†±ç—…(Yellow fever) A. ä¹â¾·ã¨â¼¥(The Beggar and the Woman)
B. å¹³â½¥å…¸é€š(NorimichiHirata) C. ã‚¤ãƒŒãƒ¯ã‚·(Golden eagle) B.çµæ ¸(Tuberculosis) B. â¼¤å³¶ã®â½”æ±²ã¿â¼¥(Woman Fetching Water in Oshima)
C. æ²³äº•å¯›æ¬¡(Kanji Kawai) D. ãƒˆã‚­(Japanese crested ibis) C.â¾šç—¢(Dysentery) C. å°¿ã™ã‚‹è£¸åƒ§(Nude Monk Urinating)
D. â¾¦åŸæ¬¡éƒ(Jiro Kinjo) GPT-4oâ€™s Answer: ã“ã®ç”»åƒã¯â½‡æœ¬ã®â½¯å£å³¶ã‚’ D.ã‚³ãƒ¬ãƒ©(Cholera) D. é«ªæ¢³ã‘ã‚‹â¼¥(Woman Combing Her Hair)
GPT-4oâ€™s Answer:ã“ã®ç”»åƒã¯â¾¸â¾¥åŸã®æ­£æ®¿ã‚’â½°ã—ã¦ â½°ã—ã¦ã„ã¾ã™ã€‚â½¯å£å³¶ã§â¾’ã‚‰ã‚Œã‚‹ã“ã¨ã§æœ‰åãª GPT-4oâ€™s Answer:ã“ã®ç”»åƒã®â¼ˆç‰©ãŒèª°ã‹ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ GPT-4oâ€™s Answer: ã“ã®ç”»åƒã¯ã€Œå°¿ã™ã‚‹è£¸åƒ§ã€ã¨ã„ã†
ã„ã¾ã™ã€‚â¾¸â¾¥åŸã®æ£Ÿé£¾ï¼ˆâ¿“é ­ï¼‰ã¯ã€â¾¦åŸæ¬¡éƒï¼ˆãã‚“ å‹•ç‰©ã¯ã€Œã‚¤ãƒªã‚ªãƒ¢ãƒ†ãƒ¤ãƒãƒã‚³ã€ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ ã¯ã§ãã¾ã›ã‚“ãŒã€é¸æŠè‚¢ã®ä¸­ã§â»©ç†±ç—…ã€çµæ ¸ã€â¾šç—¢ã€ ä½œå“ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€é¸æŠè‚¢ã®ä¸­ã§ã“ã®ä½œè€…ã®
ã˜ã‚‡ã†ã˜ã‚ã†ï¼‰ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸã¨ã•ã‚Œã¦ã„ã¾ æ­£è§£ã¯B. ã‚¤ãƒªã‚ªãƒ¢ãƒ†ãƒ¤ãƒãƒã‚³ã§ã™ã€‚ ã‚³ãƒ¬ãƒ©ã®ã„ãšã‚Œã‹ã‚’ç ”ç©¶ã—ã¦ã„ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ ä»£è¡¨ä½œã§ã¯ãªã„ã‚‚ã®ã¯ã€ŒC. å°¿ã™ã‚‹è£¸åƒ§ã€ã§ã™ã€‚
ã™ ( T s TT ah hihã€‚ de ei r s r t eã— oo fi om ohãŸ f raa t eo vg ,ãŒ p ee th ã£ o bd er ee n cepã¦ a onic m r ã€ c rts ee r a cæ­£ nt fh tt t e aeè§£ ( d ndm r s ã¯ ba wa g yiD en o J r n. ih ir â¾¦ sa h o l e Dl KåŸ a o .d i f Jnæ¬¡ ) i jS ro o oéƒ h f . u KSã§ r hi i nuCã™ jr oa i .sã€‚ C )tl ae s. tle is ( f CaT amh tâ€™i o .s u Ti sm h f ea o rg r e e fb os e rh i eno ,g w t hss ee I e cs nh o i rog rn Gea ck I rtsi o h aI ui ns gl nsa a w dn k ed i T r I ri s in usl a tJ B hna d .p : Ia Di rn s i . . o t ãƒˆhT meh o ã‚­ e â€˜ tI ea rin Coi amm t.oa )tl e å…· ç‰¹ ( h d kI a y n a v s oä½“ å®š m ee wn çš„ ã™ r ntu e e ,n s ãª ã‚‹ r iea y ta b ,â¼ˆ ã“ w rl oce orhç‰© ã¨ ut ceo lhdå ãŒ d i o d o blãŒ ã§ e n e en re paã‚ ã t i o . of f I syã‹ ã¾ f st t h itã‚Œ ã™ h bhee le ef ã° ã€‚ p o s te pl oã€ lr e os dcwoã ein f ti i en ã® ci rgn mn: â¼ˆ t a iyh nmeiç‰© es l e l ti o ã® hom w efa ç ” i t rfg h ee fç©¶ ev i, e e pb låˆ† r e du , r t t sé‡ o ut o fh b nã‚’ re e ey wr sc em eu arla e ro cy s h is .), ( T reT h ph e ri r es e s fi em o nr ta e ag , t e ia v mi es o wt nh oge r kta hr betw G yo o rp thr otk i i uo s t n ni at ds ul ,e t Thtd h ro eâ€œ ur N to i hsu n :d e â€œ DCe t h .M . a å°¿ é«ªto in ã™ æ¢³sk n ã‚‹ ã‘U otr è£¸ ã‚‹i an åƒ§a â¼¥ti .n )g.â€
Ground Truth: B.å¹³â½¥å…¸é€š E Gr Pr To -r 4 oR e mas iso tn ak: es SadoIsland Ground Truth: A. â»©ç†±ç—… E Gr Pr To -r 4 oR ce oa rs ro en c: t ly provides
Error Reason: shown in <image 1> for Ishigaki Error Reason: the title of <image 1>, but it fails to
GPT-4o correctly answers Island which is another island The person in the image is a famous understand the question. Although
the name of the building in in Japan. historical figure in Japan, the question asked to select
<image 1>, but it (However, GPT-4o correctly Hideyo Noguchi,but GPT-4o says it a non-representative work,
hallucinates when providing answers the animal, is unable to identify the person and it answers with a representative
more detailed information â€˜IriomoteCatâ€™, which is found refused to provide an answer. work, indicating a lack of proper
about it. on Ishigaki Island.) understanding of the Japanese text.
(a) LackofKnowledge (b) ImageRecognitionErrors (c) AnswerRejection (d) TextualMisunderstanding
Figure6: Examplesfromeacherrortype: (a)LackofKnowledge,wherethemodeldoesnotknowthenecessary
information;(b)ImageRecognitionErrors,wherethemodelfailstocorrectlyinterprettheimage;(c)Answer
Rejection, where the model rejects to answer; and (d) Textual Misunderstanding, where the response is not
alignedwiththequestion.
informationfromtheimagebutlackstheculture- uresfromimages. InFigure6(c),GPT-4oresponds
specific knowledge required to produce a correct that it is unable to identify the person in the im-
answer,(ii)ImageRecognitionErrors,whereit age(HideyoNoguchi),resultinginafailuretose-
failstocorrectlyinterprettheimageduringthevi- lect the option associated with him. We hypothe-
sualunderstandingstage,(iii)AnswerRejection, size this is due to their strong privacy awareness
where it declines to provide an answer, and (iv) to avoid giving private information (Wang et al.,
Textual Misunderstanding, where the response 2024),evenwhenthequestionasksforinformation
is not aligned with the question. The overall dis- thatiswidelyknownaboutahistoricalfigure.
tributionoftheseerrortypesisshowninFigure5.
Textual Misunderstanding (4.8%) There are
LackofKnowledgeistheoverwhelmingmajority
rareinstanceswhereGPT-4oprovidesanincorrect
atover50%,indicatingthatculture-specificknowl-
responsedespitecorrectlyidentifyingthecontent
edge is the most critical requirement to achieve
oftheimage. Forexample,inFigure6(d),GPT-4o
highperformanceinJMMMU.Inthissection,we
accurately names the title of the artwork, but its
discussnotableexamplesforeacherrorcategory.
answerdoesnotcorrespondtothequestion.
LackofKnowledge(53.8%) Figure6(a)shows
6 Conclusion
anexampleofanerrorinJapaneseHeritage. Here,
GPT-4o correctly recognizes Shuri Castle in the
We propose JMMMU, a benchmark designed to
image but fails to provide the related contextual
comprehensivelyevaluatetheexpert-levelknowl-
knowledge. Similar cases have been observed in
edge, reasoning abilities, and understanding of
JapaneseArt,whereGPT-4ocorrectlyanswersthe
Japanese culture. The evaluation results suggest
nameoftheartworkbutisunabletospecifytheera
crucialdirectionsfordevelopingmodelswithhigh-
inwhichitwascreated.
level reasoning skills grounded in cultural under-
standing. We have also revealed the importance
ImageRecognitionErrors(30.8%) Figure6(b)
ofevaluatingmodelsonculture-specificquestions
showsanexampleofanimagerecognitionerrorof
by showing that some models perform well in
a question. Here, GPT-4o mistakes the image of
culture-agnosticquestionsinJapanese,butnotin
SadoIslandforIshigakiIsland,anditanswersthe
culture-specificquestions. Wehopethisworkwill
famousanimalinIshigaki(correctlyiftheimage
serve as an important step towards a comprehen-
wasindeedIshigaki).
sivemultilingualevaluation,motivatecommunities
AnswerRejection(10.6%) Thistypeoferroris inotherculturesandlanguagestocrafttheirown
particularlyevidentinJapaneseHistoryandWorld high-standard benchmarks, and lead to LMM de-
History, where GPT-4o declines to answer ques- velopmentsthataremoreinclusiveandtrulyuseful
tions requiring the identification of historical fig- indiversepopulation.
8Limitations Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed
Awadallah, et al. 2024. Phi-3 technical report: A
Throughout our experiment and extensive analy- highlycapablelanguagemodellocallyonyourphone.
sis,wehaveshownanumberofcriticaldirections arXivpreprintarXiv:2404.14219.
of improvement in multilingual benchmarks and
TakuyaAkiba,MakotoShing,YujinTang,QiSun,and
model developments. While they are outside of
DavidHa.2024. Evolutionaryoptimizationofmodel
thescopeofthispaper, theyareleftasimportant mergingrecipes. arXivpreprintarXiv:2403.13187.
directionsforfuturework,andthuswesummarize
Anthropic. 2024. Claude 3.5 sonnet. https://www.
themhere:
anthropic.com/news/claude-3-5-sonnet.
SubjectSetExpansion WhileJMMMUcanas-
StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar-
sessthelatestLMMsâ€™expert-levelskills,itcannot
garetMitchell,DhruvBatra,CLawrenceZitnick,and
evaluate model performance on subjects outside DeviParikh.2015. Vqa: Visualquestionanswering.
ofthosecurrentlycovered. Asmodelsgainmore InICCV.
knowledgeandimprovetheirreasoningabilities,it
Soravit Changpinyo, Linting Xue, Michal Yarom,
willbenecessarytoexpandtherangeofsubjects
Ashish VThapliyal, Idan Szpektor, JulienAmelot,
andincludemorechallengingquestions. Xi Chen, and Radu Soricut. 2022. Maxm: To-
wardsmultilingualvisualquestionanswering. arXiv
BenchmarksinOtherCultures SinceJMMMU preprintarXiv:2209.05401.
only covers the Japanese, evaluating model per-
formanceinotherlanguagesandculturalcontexts LichangChen,ShiyangLi,JunYan,HaiWang,Kalpa
Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-
remains an important area for future work. We
vasan,TianyiZhou,HengHuang,etal.2023a. Al-
hopetheseeffortswillhelpmitigatetheunderrep-
pagasus: Training a better alpaca with fewer data.
resentationofdiverseculturesandlanguages. arXivpreprintarXiv:2307.08701.
Instruction Following Ability in Japanese LinChen,JisongLi,XiaoyiDong,PanZhang,Conghui
InSection5.2,wehaveshownagapininstruction- He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024.
Sharegpt4v: Improving large multi-modal models
followingabilitybetweenlanguagesandthatmod-
withbettercaptions. InECCV.
elsgoagainsttheinstructionandgeneratetheirrea-
soningmoreofteninJapanese. Whiletheprimary ZheChen,JiannanWu,WenhaiWang,WeijieSu,Guo
focus of our benchmark is on evaluating expert Chen, Sen Xing, Muyan Zhong, Qinglong Zhang,
Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong
knowledgeandtherebyhelpingimprovesuchskills,
Lu, Yu Qiao, and Jifeng Dai. 2023b. Internvl:
itisleftasanimportantfutureworktoimprovethe
Scaling up vision foundation models and aligning
instruction-followingabilityinJapanese. Further, for generic visual-linguistic tasks. arXiv preprint
itisalsoimportanttodesignanevaluationprotocol arXiv:2312.14238.
tomeasureinstruction-followingabilitytoenhance
GoogleDeepMind.2024. Gemini1.5pro. https://
the development of such skills. While there are
deepmind.google/technologies/gemini/pro/.
somemethodstoevaluatethemodelâ€™sinstruction-
following ability (Zhou et al., 2023; Qian et al., XingyuFu,YushiHu,BangzhengLi,YuFeng,Haoyu
2024),theseshouldbeappropriatelyincorporated Wang,XudongLin,DanRoth,NoahASmith,Wei-
ChiuMa,andRanjayKrishna.2024. Blink: Multi-
inthecontextofmultilingualperformanceevalua-
modallargelanguagemodelscanseebutnotperceive.
tion. InECCV.
Acknowledgement HaoyuanGao,JunhuaMao,JieZhou,ZhihengHuang,
LeiWang,andWeiXu.2015. Areyoutalkingtoa
ThisworkwaspartiallysupportedbyJSTBOOST, machine?datasetandmethodsformultilingualimage
Japan Grant Number JPMJBS2418, JSPS KAK- question. Advancesinneuralinformationprocessing
systems,28.
ENHI Grant Number 24K23882, and JST JP-
MJCR22U4.
Deepak Gupta, Pabitra Lenka, Asif Ekbal, and Push-
pakBhattacharyya.2020. Aunifiedframeworkfor
multilingualandcode-mixedvisualquestionanswer-
References ing. InProceedingsofthe1stconferenceoftheAsia-
Pacificchapteroftheassociationforcomputational
2023. Vicuna: Anopen-sourcechatbotimpressinggpt- linguisticsandthe10thinternationaljointconference
4with90%*chatgptquality. onnaturallanguageprocessing,pages900â€“913.
9WenyiHong,WeihanWang,MingDing,WenmengYu, Fangyu Liu, Emanuele Bugliarello, Edoardo Maria
QingsongLv,YanWang,YeanCheng,ShiyuHuang, Ponti, Siva Reddy, Nigel Collier, and Desmond
JunhuiJi,ZhaoXue,etal.2024. Cogvlm2: Visual Elliott. 2021. Visually grounded reasoning
languagemodelsforimageandvideounderstanding. across languages and cultures. arXiv preprint
arXivpreprintarXiv:2408.16500. arXiv:2109.13238.
Jinyi Hu, Yuan Yao, Chongyi Wang, SHAN WANG, Fuxiao Liu, Hao Tan, and Chris Tensmeyer. 2023a.
YinxuPan,QianyuChen,TianyuYu,HanghaoWu, Documentclip: Linking figures and main body
YueZhao,HaoyeZhang,XuHan,YankaiLin,Jiao text in reflowed documents. arXiv preprint
Xue,dahaili,ZhiyuanLiu,andMaosongSun.2024. arXiv:2306.06306.
Large multilingual models pivot zero-shot multi-
modallearningacrosslanguages. InICLR. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee.2024a. Improvedbaselineswithvisualinstruc-
Aozora Inagaki. 2024. llava-calm2-siglip. tiontuning. InCVPR.
https://huggingface.co/cyberagent/
llava-calm2-siglip. HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan
Zhang,ShengShen,andYongJaeLee.2024b. Llava-
YuichiInoue,TakuyaAkiba,andShingMakoto.2024a. next: Improvedreasoning,ocr,andworldknowledge.
Llama-3-evovlm-jp-v2.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
YuichiInoue,KentoSasaki,YumaOchi,KazukiFujii, Lee.2023b. Visualinstructiontuning. InNeurIPS.
KotaroTanahashi,andYuYamaguchi.2024b. Heron-
bench: Abenchmarkforevaluatingvisionlanguage Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
modelsinjapanese. InCVPRworkshop. Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2024c. Mm-
DongfuJiang,XuanHe,HuayeZeng,CongWei,Max bench: Is your multi-modal model an all-around
Ku,QianLiu,andWenhuChen.2024. Mantis: Inter- player? InECCV.
leavedmulti-imageinstructiontuning. arXivpreprint
arXiv:2405.01483. PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
HugoLaurenÃ§on,AndrÃ©sMarafioti,VictorSanh,and WeiChang,MichelGalley,andJianfengGao.2024.
LÃ©o Tronchon. 2024a. Building and better under- Mathvista: Evaluating mathematical reasoning of
standingvision-languagemodels: insightsandfuture foundationmodelsinvisualcontexts. InICLR.
directions. arXivpreprintarXiv:2408.12637.
AtsuyukiMiyai,JingkangYang,JingyangZhang,Yifei
HugoLaurenÃ§on,LucileSaulnier,LeoTronchon,Stas Ming, Qing Yu, Go Irie, Yixuan Li, Hai Li, Ziwei
Bekman,AmanpreetSingh,AntonLozhkov,Thomas Liu,andKiyoharuAizawa.2024. Unsolvableprob-
Wang, Siddharth Karamcheti, Alexander M Rush, lemdetection: Evaluatingtrustworthinessofvision
DouweKiela,MatthieuCord,andVictorSanh.2023. languagemodels. arXivpreprintarXiv:2403.20331.
OBELICS:Anopenweb-scalefiltereddatasetofin-
terleavedimage-textdocuments. InThirty-seventh Masoud Monajatipoor, Liunian Harold Li, Mozhdeh
ConferenceonNeuralInformationProcessingSys- Rouhsedaghat, Lin F Yang, and Kai-Wei Chang.
temsDatasetsandBenchmarksTrack. 2023. Metavl: Transferringin-contextlearningabil-
ityfromlanguagemodelstovision-languagemodels.
HugoLaurenÃ§on, LÃ©oTronchon, MatthieuCord, and arXivpreprintarXiv:2306.01311.
Victor Sanh. 2024b. What matters when build-
ing vision-language models? arXiv preprint OpenAI.2024. Gpt-4o.
arXiv:2405.02246.
JonasPfeiffer,GregorGeigle,AishwaryaKamath,Jan-
BoLi,YuanhanZhang,LiangyuChen,JinghaoWang, MartinOSteitz,StefanRoth,IvanVulicÂ´,andIryna
Jingkang Yang, and Ziwei Liu. 2023. Otter: A Gurevych.2021. xgqa: Cross-lingualvisualquestion
multi-modalmodelwithin-contextinstructiontuning. answering. arXivpreprintarXiv:2109.06082.
arXivpreprintarXiv:2305.03726.
Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier,
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Peter Grasch, Yinfei Yang, and Zhe Gan. 2024.
Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Mia-bench: Towards better instruction following
Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava- evaluation of multimodal llms. arXiv preprint
onevision: Easyvisualtasktransfer. arXivpreprint arXiv:2407.01509.
arXiv:2408.03326.
Machel Reid, Nikolay Savinov, Denis Teplyashin,
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix- Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
iaoGe,andYingShan.2024b. Seed-bench: Bench- Alayrac, et al. 2024. Gemini 1.5: Unlocking mul-
marking multimodal llms with generative compre- timodalunderstandingacrossmillionsoftokensof
hension. InCVPR. context. arXivpreprintarXiv:2403.05530.
10DavidRomero,ChenyangLyu,HaryoAkbariantoWi- LeXue,ManliShu,AnasAwadalla,JunWang,AnYan,
bowo, Teresa Lynn, Injy Hamed, Aditya Nanda SenthilPurushwalkam,HongluZhou,VirajPrabhu,
Kishore, Aishik Mandal, Alina Dragonetti, Artem et al. 2024. xgen-mm (blip-3): A family of
Abzaliev,AtnafuLambeboTonja,etal.2024. Cvqa: open large multimodal models. arXiv preprint
Culturally-diverse multilingual visual question an- arXiv:2408.08872.
sweringbenchmark. InNeurIPSDatasetsandBench-
marksTrack. QinghaoYe,HaiyangXu,JiaboYe,MingYan,Anwen
Hu,HaoweiLiu,QiQian,JiZhang,andFeiHuang.
2024. mplug-owl2: Revolutionizing multi-modal
SakanaAI. 2024a. Ja-multi-image-vqa.
https://huggingface.co/datasets/SakanaAI/ large language model with modality collaboration.
JA-Multi-Image-VQA. InCVPR.
WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,
SakanaAI. 2024b. Ja-vg-vqa-500. https:
KevinLin,ZichengLiu,XinchaoWang,andLijuan
//huggingface.co/datasets/SakanaAI/
Wang.2024. Mm-vet: Evaluatinglargemultimodal
JA-VG-VQA-500.
modelsforintegratedcapabilities. InICML.
SakanaAI. 2024c. Ja-vlm-bench-in-the-wild. XiangYue,YuanshengNi,KaiZhang,TianyuZheng,
https://huggingface.co/datasets/SakanaAI/ RuoqiLiu,GeZhang,SamuelStevens,DongfuJiang,
JA-VLM-Bench-In-the-Wild. WeimingRen,YuxuanSun,etal.2024. Mmmu: A
massivemulti-disciplinemultimodalunderstanding
Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. andreasoningbenchmarkforexpertagi. InCVPR.
2018. Visual question answering dataset for bilin-
gualimageunderstanding: Astudyofcross-lingual Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang,
transferusingattentionmaps. InCOLING. Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang
Cheng,ChunpuXu,ShuyueGuo,etal.2024. Cm-
JingqunTang,QiLiu,YongjieYe,JinghuiLu,ShuWei, mmu: A chinese massive multi-discipline multi-
ChunhuiLin,WanqingLi,MohamadFitriFaizBin modal understanding benchmark. arXiv preprint
Mahmood,HaoFeng,ZhenZhao,etal.2024. Mtvqa: arXiv:2401.11944.
Benchmarkingmultilingualtext-centricvisualques-
Bo Zhao, Boya Wu, and Tiejun Huang. 2023. Svit:
tionanswering. arXivpreprintarXiv:2405.11985.
Scalingupvisualinstructiontuning. arXivpreprint
arXiv:2307.04087.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothÃ©eLacroix,
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,
Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro,
Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang,
Faisal Azhar, et al. 2023. Llama: Open and effi-
WenjuanHan,andBaobaoChang.2024. Mmicl:Em-
cient foundation language models. arXiv preprint
poweringvision-languagemodelwithmulti-modal
arXiv:2302.13971.
in-contextlearning. InICLR.
Turing. 2024a. Llava-bench-in-the-wild. https: Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Sid-
//huggingface.co/datasets/liuhaotian/ dharthaBrahma,SujoyBasu,YiLuan,DennyZhou,
llava-bench-in-the-wild/tree/main. and Le Hou. 2023. Instruction-following evalu-
ation for large language models. arXiv preprint
Turing. 2024b. Llava-bench-in-the-wild arXiv:2311.07911.
(japanese). https://github.com/turingmotors/
heron/tree/main/playground/data/
llava-bench-in-the-wild.
Turing. 2024c. Llava-bench-ja. https:
//github.com/turingmotors/heron/tree/
main/playground/data/llava-bench-ja.
YuxiaWang,HaonanLi,XudongHan,PreslavNakov,
andTimothyBaldwin.2024. Do-not-answer: Evalu-
atingsafeguardsinLLMs. InFindingsoftheAsso-
ciationforComputationalLinguistics: EACL2024,
pages896â€“911,St.Julianâ€™s,Malta.Associationfor
ComputationalLinguistics.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
DaHuang,DennyZhou,etal.2023. Largerlanguage
models do in-context learning differently. arXiv
preprintarXiv:2303.03846.
11Appendix TableA:LMMâ€™sJapanesesupport.
JMMMU Japanesesupport
A LMMsâ€™JapaneseSupport
Model Overall BaseLLM LLM LMM
TodiscussthemultilingualcapabilitiesofLMMs, OpenSource
xGen-MM 28.6 Phi-3 âœ— âœ—
wesummarizewhethereachmodelofficiallysup- Mantis8B 35.5 Llama3 âœ— âœ—
portsJapanese. TableApresentstheJapaneselan- Idefics2-8B 31.9 Mistralv0.1 ? âœ—
guagesupportstatusforeachmodel. â€œâœ“â€indicates Idefics3-8B 37.3 Llama3 âœ— âœ—
CogVLM2-19B 36.1 Llama3 âœ— âœ—
officialsupportforJapanese, whileâ€œâœ—â€indicates InternVL2-2B 28.3 InternLM2 âœ— âœ—
theabsenceofsuchsupport. Also,wedenoteâ€œ?â€ InternVL2-8B 38.3 InternLM2 âœ— âœ—
formodelsofwhichwecouldnotfindtheinforma- LLaVA-1.613B 31.1 Vicunav1.5 âœ— âœ—
LLaVA-1.634B 39.8 NousHermes2Yi âœ— âœ—
tion. LLaVA-OneVision0.5B 26.0 Qwen2 âœ“ âœ—
Even if a model is marked as â€œâœ—â€, it may still LLaVA-OneVision7B 40.5 Qwen2 âœ“ âœ—
Phi-3Vision 29.5 Phi-3 âœ— âœ—
demonstrate some Japanese language capability
Phi-3.5Vision 32.4 Phi-3.5 âœ“ âœ—
due to the presence of Japanese data in publicly â€ LLaVACALM2 34.9 CALM2 âœ“ âœ“
availabledatasetslikeShareGPT-4V(Chenetal., â€ EvoVLMJPv2 38.1 (mergedmodel) âœ“
2024)andShareGPT-4o2,ordatacrawledfromthe ClosedSource
Claude3.5Sonnet 50.8 ? ? âœ“
web. Gemini1.5Pro 51.5 ? ? âœ“
Proprietary commercial models, such as GPT- GPT-4o 58.6 ? ? âœ“
4o,Gemini1.5Pro,andClaude3.5Sonnet,donot
publiclydisclosedetailedinformationabouttheir
Chinese,withnoexplicitmentionofJapanesesup-
trainingdata. However,basedontheirreleaseblog
port.
posts,itcanbeinferredthatthesemodelssupport
Idefics2, Idefics3, xGen-MM, and Mantis use
manylanguages,includingJapanese.
large-scaledatasetsformultimodaltraining. How-
LLaVACALM2isbasedontheJapaneseLLM
ever, there is no clear evidence of Japanese
CALM23,andithasbeentrainedusingJapanese
data inclusion, and in some datasets, such as
multimodaldatasets,officiallysupportingJapanese.
OBELICS (LaurenÃ§on et al., 2023), non-English
EvoVLM JP v2, a merged model (Akiba et al.,
dataisexplicitlyfilteredout. WhileLlama3,the
2024), also incorporates Japanese data for opti-
base model for some of these LMMs, mentions
mization and is officially released as a Japanese
multilingualtraining,itdoesnotexplicitlyconfirm
LMM.
support for Japanese. Mistral v0.1 also does not
Phi-3.5 Vision does not officially support
discloseitstrainingdata.
Japanese, despite its base model, Phi-3.5, having
Theperformanceofthesemodelsdependsona
officialsupportformultiplelanguages,including
complex interplay of factors, including the quan-
Japanese. Phi-3Vision,likewise,doesnotsupport
tity and quality of the training data and the size
non-Englishlanguages.
andcapabilitiesofthebaselanguagemodel. Offi-
IntheLLaVAseries,LLaVA-OneVisionexplic-
cial support for Japanese is not the only consid-
itlymentionssupportforChineseinitstrainingbut
eration; there are reports of models trained on
doesnotextendthistoothernon-Englishlanguages.
English-onlymultimodaldatageneralizingtoother
However, Qwen2, the base LLM for the LLaVA-
languages (Hu et al., 2024), including Japanese.
OneVision models, officially supports Japanese.
Moreover, since many models are designed with
LLaVA-1.6modelsaretrainedfromdifferentbase
Chinesesupport,theculturalandlinguisticproxim-
LLMs,suchasVicunav1.5andNousHermes2Yi,
itybetweenJapaneseandChinese-speakingregions
neitherofwhichofficiallysupportJapanese. Thus,
mayresultinahighperformanceinJapanese.
Japaneselanguagecapabilitiesarenotguaranteed
intheirvisualinstructiontraining.
B MoreResult
InternVL and its base model, InternLM2, offi-
ciallysupportonlyEnglishandChinese. Similarly,
B.1 ErrorAnalysisinCulture-Agnostic
CogVLM2claimsproficiencyinbothEnglishand
subjects
2https://huggingface.co/datasets/OpenGVLab/
In Section 5.2, we present the error analysis for
ShareGPT-4o
GPT-4o on the CA subjects. In this section, we
3https://huggingface.co/cyberagent/
calm2-7b-chat provide the error analysis for all models. While
12EN: , JP: EN: , JP: EN: , JP: EN: , JP:
(181 .3 32 %) (211 .5 76 %) (171 .2 67 %) (241 .7 46 %) (107 .77 %) 122
78 70 341 (16.9%)
(594 .3 91 %) (1( 11 7 .0 09. %8% )) (564 .0 57 %) (118( .1 530 %7 .34 )%) (554 .0 60 %) (171( .29 13. %7% )) (47.4%) 132(9.7 91 %) (543 .8 09 %) (181 .3 32
%)
(18.3%)
LLaVA CALM2 EvoVLM JP v2 idefics2 8B InternVL2 8B LLaVA-OV 0.5B
148 158 114
(20.6%) 249 241 237 (21.9%) (15.8%)
(513 .7 83 %) (171 .2 8( 89 %.7 9 )1 %) (34.6 (% 171) .2 24 %)(141( .0 73 6 %3.5 )%) (392 .8 76 %) (171 .2 24 %)(1( 03 7 .2 13. %9% )) (533 .8 66 %) (141 .0( 21 2 %07 . )34 %) (574 .1 86 %) (1( 31 9 .52 79 %.93 % ))
Phi-3.5v Claude 3.5 Sonnet Gemini 1.5 Pro idefics3 8B LLaVA1.6 13B
(292 .1 95 %) (191 .4 72 %) (201 .5 80 %) 256 258 (118 .74 %) 112
(473 .4 41 %) (513 .7 72 %) (118 .74 %) (513 .7 72 %) (118 .96 %) (35.6%) (35.8%) (533 .8 23 %) (15.6%)
59
(141 .0 65 %(8 ).2%) (161 .2 92 %) (151 .1 62 %) (151 .0 08 %)(139 .68 %) (191 .4 61 %)
LLaVA-OV 7B Phi-3v CogVLM2 GPT-4o InternVL2 2B
(251 .8 86 %) (211 .5 12 %) (161 .2 92 %)
321 84
(44.6%) 384 78 383 (11.7%)
76 (53.3%) (10.8%) (53.2%)
(10.6%) 106 131
137 (14.7%) (18.2%)
(19.0%)
LLaVA1.6 34B Mantis 8B xGen-MM
FigureA:Errorinculture-agnosticsubjects. Thisfigurecategorizesthecorrectnessofanswersinculture-agnostic
subjectsbasedontheoriginalMMMUEnglishresponses(correctorincorrect)andthecorrespondingJMMMU
translatedresponses(correctorincorrect).
we have shown in Table 2 that most models per- tion 5.2, which allows it to infer answers more
form worse in Japanese, there are some amount easily. Notethatourexperimentherehasbeencon-
of questions where the model answers correctly ductedbyusingquestionsthatinvolvedtranslation
onlyinJapaneseforeverymodel. Thenumberof ofbothtextsandimages. Manyofthemconsistof
suchquestionsisparticularlyhighforLLaVA-OV tabledata,whichrequiresstrongerreasoningbased
0.5B and InternVL2 2B. This occurrence, how- on data processing, so the result may vary when
ever,appearstobearandomphenomenon,likely investigatingdifferentdatatypesthatdonotexist
attributabletotheoverallweakerperformanceof intheCAsubsetofJMMMU.
thesemodels.
B.2 AblationonImageTranslation
B.3 ScoreCorrelationbetweenlanguages
ThefullsetofTable3ispresentedinTableB.As
discussedinSection5.1,eachmodelreactsdiffer-
entlyasthetranslationproceeds,andthetendency Usingtheculture-agnosticsubset,wehavedemon-
is difficult to summarize. Notably, here, GPT-4o stratedinSection4.2that(i)modelsperformworse
showsa7.2%improvementinscoreaftertexttrans- inJapaneseand(ii)JapaneseLMMsshowrobust-
lation. Thispartlystemsfromitsweakinstruction- ness to translation. To illustrate these points, we
following skills in Japanese, as discussed in Sec- provideFigureB.
13TableB:Thefullsetofthetranslationeffect. Each 55
columnshowsthemodelperformancewhenimage(I) LLaVA-OV 0.5B
InternVL2 2B
and text (T) are in Japanese (jp) or in English (en). 50 xGen-MM
Phi-3 Vision
âˆ† i shows the difference from I enT en. â€  represents LLaVA-NeXT 13B
JapaneseLMMs. 45 I Pd he if -i 3c .s 52 V 8 iB sion
Mantis-8B
I enT en I enT jp(âˆ† 1) I jpT jp(âˆ† 2) CogVLM2 19B
40 Idefics3 8B
Opensource InternVL2 8B
LLaVA-NeXT 34B
LLaVA-OV-0.5B 28.9 28.9(Â±0.0) 29.7(+0.8) 35 LLaVA-OV 7B
InternVL2-2B 32.5 29.7(-2.8) 28.6(-3.9) LLaVA CALM2
EvoVLM-JP v2
xGen-MM 36.7 28.3(-8.4) 28.3(-8.4) 30 Claude 3.5 Sonnet
Phi-3v 35.0 31.7(-3.3) 29.7(-5.3) Gemini 1.5 Pro
GPT-4o
LLaVA-1.6-13B 26.4 31.9(+5.5) 29.2(+2.8)
25
Idefics2-8b 28.9 28.1(-0.8) 28.1(-0.8) 25 30 35 40 45 50 55
Phi-3.5v 39.2 33.6(-5.6) 31.1(-8.1) Culture Agnostic (EN) [%]
â€ LLaVA-CALM2 29.4 28.3(-1.1) 31.4(+2.0)
Mantis8B 32.5 31.1(-1.4) 31.4(-1.1)
FigureB:Scorecorrelationbetweenlanguages.â– rep-
CogVLM2-19B 32.8 31.9(-0.9) 34.4(+1.6)
resentsproprietarymodelsandâ‹†representsJapanese
Idefics3-8b 33.1 31.7(-1.4) 29.7(-3.4)
â€ EvoVLMJPv2 30.0 30.8(+0.8) 28.6(-1.4) LMMs. WhileallmodelsperformworseinJapanese,
InternVL2-8B 43.9 38.3(-5.6) 37.2(-6.7) Japanese LMMs perform similarly in both languages
LLaVA-1.6-34B 43.6 40.8(-2.8) 38.9(-4.7) (i.e.,closetothegraydashedline)
LLaVA-OV-7B 45.0 38.3(-6.7) 35.6(-9.4)
Proprietary
Claude3.5Sonnet 53.6 56.4(+2.8) 54.2(+0.6) While this allows an evaluation robust against
Gemini1.5Pro 50.6 42.2(-8.4) 42.2(-8.4)
somevarietyofanswergenerationstyles,wehave
GPT-4o 48.1 55.3(+7.2) 53.1(+5.0)
showninSection5.2thatthiscansometimesover-
estimatetheperformanceinJapanesebecausemod-
C FurtherExperimentalDetails elsâ€™ instruction-following abilities are relatively
lowinJapanese.
C.1 ExperimentalSetup
ComputingInfrastructures Weconductallour D AnnotationInstruction
evaluations of open-source models on a single
Recruitment and Payment Annotators were
NVIDIAA100(80GB)GPU.
paid at least the minimum wage set in Japan, ac-
Parameters for LMM Inference A maximum cordingtothetimespentonthetask.
outputlengthis setto1,024andatemperature is
Data Consent They were informed that trans-
setto0forallmodelsduringinference.
lateddatawouldbeusedforevaluationpurposes.
C.2 EvaluationProtocol
Instructions Given to Participants The docu-
AnswerExtractioninMultipleChoiceQuestion ment containing the instructions presented to the
While the models are instructed to answer their annotatorsisshowninFigureC.
choice directly, they often generate some contex-
E Examples
tualinformationorunnecessarysymbols. Totackle
thispoint,followingMMMU(Yueetal.,2024),we
Weprovidesamplequestionsfromculture-agnostic
extract an answer from the model response with
subset in Figure D, and questions from culture-
a rule-based method. For multiple-choice ques-
specificsubsetinFigureE
tions, this parser can extract the modelâ€™s choice
evenwhenthechoiceissurroundedbysomesym-
bol(e.g.,â€™(A)â€™,â€™A.â€™,â€™Aâ€™)orbytext.
Forexample,theseanswers,whichareallsome
variantsofâ€œTheanswerisA.â€inJapanese,canbe
parsedasâ€œAâ€:
â€¢ å›ç­”ã¯A
â€¢ ç­”ãˆã¯ã€Aã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹
â€¢ ç”»åƒã¯é¦–é‡ŒåŸã®ãŸã‚ã€ç­”ãˆã¯(A)ã€‚
â€¢ ç­”ãˆ:A.15.3
14
]%[
)PJ(
citsongA
erutluCFigureC:AnnotationInstruction. AnnotatorswereprovidedwiththeJapaneseversionofthisinstruction.
15Art&Psychology
Question: <image 1> æ‹â¼¦è¨˜å·ã«åŸºã¥ãã¨ã€æ¬¡ã®æ¥½è­œã®â¼©ç¯€æ•°ã¯__ã§ã™ã€‚ Question: <image 1> ä¸Šè¨˜ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æœ€é©ãªèª¬æ˜ã‚’â½°ã—ã¦ã„ã‚‹ã®ã¯
(<image 1> Based on the time signature, the number of measures/bars in the æ¬¡ã®ã†ã¡ã©ã‚Œã§ã™ã‹ï¼Ÿ
following music score is _____.) (<image 1> Which of the following provides an effective explanation for the
data above?)
Options:
A. 6 Options:
B. 7 A. ã‚ªãƒšãƒ©ãƒ³ãƒˆæ¡ä»¶ä»˜ã‘(Operant conditioning)
C. 5 B. å¤å…¸çš„æ¡ä»¶ä»˜ã‘(Classical conditioning)
D. 4 C. æº–å‚™ã•ã‚ŒãŸæ¡ä»¶ä»˜ã‘(Prepared conditioning)
D. â¾ƒâ¼°å®Ÿç¾(Self-actualization)
E. è¦³å¯Ÿå­¦ç¿’(Observational learning)
Business
Question: <image 1> æä¾›ã•ã‚ŒãŸç”»åƒã¯æ¬¡ã®ã©ã‚Œã‚’æã„ã¦ã„ã¾ã™ã‹ï¼Ÿ Question: ã“ã¡ã‚‰ã¯1929å¹´ã‹ã‚‰1933å¹´ã¾ã§ã®ã‚¤ãƒ³ãƒ•ãƒ¬ç‡ã¨â½¶å›½æ ªå¼å¸‚å ´ãŠ
(<image 1> The provided image depicts a/an) ã‚ˆã³â½¶å›½è²¡å‹™çœçŸ­æœŸè¨¼åˆ¸ã®ãƒªã‚¿ãƒ¼ãƒ³ã§ã™: <image 1> ã€‚
1932å¹´ã®æ ªå¼å¸‚å ´ã®å®Ÿè³ªãƒªã‚¿ãƒ¼ãƒ³ã¯ä½•ã§ã—ãŸã‹ï¼Ÿ
(Here are inflation rates and U.S. stock market and Treasury bill returns
Options: between 1929 and 1933: <image 1> . What was the real return
A. éœ€è¦æ›²ç·š(demandcurve) on the stock market in 1932?)
B. ãƒ•ã‚£ãƒªãƒƒãƒ—ã‚¹æ›²ç·š(Phillips curve) Options:
C. â½£ç”£å¯èƒ½æ€§ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ A. -14.33%
(production possibilities frontier) B. -23.72%
D. ç·ä¾›çµ¦æ›²ç·š(aggregate supply curve) C. 0.45%
E. ãƒ­ãƒ¼ãƒ¬ãƒ³ãƒ„æ›²ç·š(Lorenz curve) D. 56.52%
Science
Question: Question:
é–¢æ•°f(x) = x^2 -6x + 4 ã¯å‡¸ã§ã™ã‹ã€ãã‚Œã¨ã‚‚å‡¹ã§ã™ã‹ï¼Ÿ<image 1> <image 1>æ ¸åå¿œã«ãŠã„ã¦? ã¯ä½•ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‹
(Is the function f(x) = x^2 -6x + 4convex or concave? <image 1>) (<image 1>What does the ? represent in the nuclear reaction)
Options: Options:
A. å‡¸(Convex) A. ã‚¢ãƒ«ãƒ•ã‚¡ç²’â¼¦(an alpha particle)
B. å‡¹(Concave) B. é›»â¼¦(an electron)
C. ã©ã¡ã‚‰ã§ã‚‚ãªã„(Neither) C. ä¸­æ€§â¼¦(a neutron)
D. ä¸¡â½…(Both) D. é™½â¼¦(a proton)
Medicine
Question: 25æ­³ã®ç§»âº ã¯ç™ºç†±ã¨æ•°ãƒ¶â½‰ã«ã‚ãŸã‚‹è„Šé«„ã®å•é¡Œã‚’ã‹ã‹ãˆã¦ã„ã¾ Question:
ã™ã€‚ã“ã®æœ€ã‚‚å¯èƒ½æ€§ã®â¾¼ã„ç—…å› ã¯ä½•ã§ã™ã‹ï¼Ÿ: <image 1> ã“ã®Xç·šå†™çœŸã§ç•°å¸¸ãŒâ¾’ã‚‰ã‚Œã‚‹è‡“å™¨ã¯ä½•ã§ã™ã‹ï¼Ÿ<image 1>
(25 year oldimmigrant with fever and several month history of cord problems. The (What organ appears abnormal in this radiograph? <image 1>)
most likely etiologyof this process is: <image 1>)
Options: Options:
A. ãƒˆãƒªãƒ‘ãƒã‚½ãƒ¼ãƒç—‡(Trypanosomiasis) A. èƒƒ(Stomach)
B. ã‚¢ãƒ¡ãƒ¼ãƒæ€§è„³ç‚(Amebicencephalitis) B. è‚è‡“(Liver)
C. è„³ãƒãƒ©ãƒªã‚¢(Cerebral malaria) C. èƒ†åš¢(Gallbladder)
D. çµæ ¸æ€§é«„è†œç‚(Tuberculous meningitis) D. â¼—â¼†æŒ‡è…¸(Duodenum)
E. ã‚¢ã‚¹ãƒšãƒ«ã‚®ãƒ«ã‚¹ç—‡(Aspergillosis)
Tech & Engineering
Question: <image 1> ã«â½°ã•ã‚Œã¦ã„ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦ã€ Question:
â¼Šâ¼’ãŒ50u(t) ã®ã¨ãã®å®šå¸¸èª¤å·®ã‚’æ±‚ã‚ãªã•ã„ã€‚ <image 1> ã®â¾åˆ—ã®â¾›æŸ»â½…æ³•ã¯ã©ã‚Œã«å½“ã¦ã¯ã¾ã‚Šã¾ã™ã‹ï¼Ÿ
(For the system shown in <image 1>, (What kind of matrix traversal is <image 1>?)
Find the steady-state error for an input of 50u(t).)
Options:
A. â¾åˆ—ã®é€šå¸¸ã®â¾›æŸ»
Options: (Normal traversal of the matrix.)
A. 17.59 B. â¾åˆ—ã®â¾ã”ã¨ã®â¾›æŸ»
B. 27.59 (Row-wise traversal of the matrix.)
C. 37.59 C. â¾åˆ—ã®åˆ—ã”ã¨ã®â¾›æŸ»
(Column-wise traversal of the matrix.)
D. â¾åˆ—ã®ã‚¹ãƒ‘ã‚¤ãƒ©ãƒ«â¾›æŸ»
(spiral traversal of the matrix.)
FigureD:Examplesinculture-agnosticsubjects. SomeimagesthatcontainEnglisharetranslated.
16Japanese Art
Question: <image 1>ã¯ä½•ã¨â¾”ã†ä½œå“ã§ã—ã‚‡ã†ï¼Ÿ Question:
(What is the name of the work in <image 1>?) <image 1>ã«æã‹ã‚Œã¦ã„ã‚‹â¼ˆç‰©ãŒæŒã£ã¦ã„ã‚‹ã‚‚ã®ã¯ãªã‚“ã§ã—ã‚‡ã†ï¼Ÿ
(What is the person depicted in <image 1> holding?)
Options:
A. ç´«å¼éƒ¨â½‡è¨˜çµµå·»
(The Diary of Murasaki Shikibu Emaki) Options:
B. æ›´ç´šâ½‡è¨˜çµµå·» A. â½©â½ (White gourd)
(The SarashinaDiary Emaki) B. ç“¢ç®ª(Gourd)
C. èœ»è›‰â½‡è¨˜çµµå·»(The KageroDiary Emaki) C. çµç¶(Biwa)
D. æ¸…å°‘ç´â¾”â½‡è¨˜çµµå·» D. ç± (Basket)
(The Diary of Sei ShonagonEmaki)
Japanese Heritage
Question: Question: <image 1>ãŒã‚ã‚‹åœ°åŸŸã§æ „ãˆãŸâ¼€æ—ã¯èª°ã§ã—ã‚‡ã†ï¼Ÿ
<image 1>ã®åŸã®åå‰ã¯ä½•ã§ã—ã‚‡ã†ï¼Ÿ (Which clan prospered in the region with <image 1>?)
(What is the name of the castle in <image 1>?)
Options:
Options: A. éŒå€‰æºâ½’(Kamakura Genji)
A. åå¤å±‹åŸ(Nagoya Castle) B. è—¤åŸåŒ—å®¶(Fujiwara Hokke)
B. å¼˜å‰åŸ(HirosakiCastle) C. å¥¥å·è—¤åŸâ½’(OshuFujiwara)
C. å½¦æ ¹åŸ(HikoneCastle) D. ä¿¡æ¿ƒæ­¦â½¥â½’(ShinanoTakeda)
D. æ¾æœ¬åŸ(Matsumoto Castle)
Japanese History
Question: Question: <image 1>ã‚’â½¤ã„ã¦å¹•åºœãŒãŠã“ãªã£ãŸâ¾ç‚ºã‚’
<image 1>ãŒèµ·ããŸæ™‚ã®â½¼ä¸­ã¯èª°ã§ã—ã‚‡ã†ï¼Ÿ ä½•ã¨ã„ã†ã§ã—ã‚‡ã†ï¼Ÿ
(Who was the senior councilor (What is the act performed by the shogunate
when <image 1> occurred?) using <image 1> called?)
Options: Options:
A. â½”é‡å¿ é‚¦(Mizuno Tadakuni) A. çµµè¸(Fumi-e)
B. æ¾å¹³å®šä¿¡(MatsudairaSadanobu) B. æ¤œåœ°(Land survey)
C. é â¼­â¾¦å››éƒ(TÅyamaKinjirÅ) C. å‹˜åˆ(Kango)
D. â½¥æ²¼æ„æ¬¡(TanumaOkitsugu) D. é–å›½(Sakoku)
WorldHistory
Question: Question:
<image 1>ã®çµ±æ²»ã‚’ä½•ã¨ã„ã†ã§ã—ã‚‡ã†ï¼Ÿ <image 1>ãŒè¡¨ã™å‡ºæ¥äº‹ã¯ä½•ã§ã—ã‚‡ã†ï¼Ÿ
(What is the reign of <image 1> called?) (What event is represented by <image 1>?)
Options: Options:
A. è²è¦³ã®æ²»(Reign of JÅgan) A. ã‚«ãƒãƒƒã‚µã®å±ˆè¾±(Humiliation of Canossa)
B. é–‹å…ƒã®æ²»(Reign of Kaiyuan) B. ã‚¢ãƒ´ã‚£ãƒ‹ãƒ§ãƒ³æ•å›š(Avignon Captivity)
C. æ°¸æ¥½ã®æ²»(Reign of Yongle) C. ã‚®ãƒ¥ã‚¤ã‚¨ãƒ³ãƒŒã®å±ˆè¾±('Humiliation of Guyenne)
D. åº·ç†™ã®æ²»(Reign of Kangxi) D. ã‚¦ã‚©ãƒ«ãƒ ã‚¹ã®å±ˆè¾±(Humiliation of Worms)
Figure E: Examples in culture-specific subjects. The questions are created by Japanese native speakers and
requiresknowledgeofJapaneseculture.
17