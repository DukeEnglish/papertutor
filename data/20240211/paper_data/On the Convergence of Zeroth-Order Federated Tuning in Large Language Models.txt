On the Convergence of Zeroth-Order Federated Tuning in Large
Language Models
ZhenqingLing1,DaoyuanChen2,LiuyiYao2,YaliangLi2,YingShen1
1SunYat-SenUniversity,2AlibabaGroup
ABSTRACT memorycosts,frequentlysurpassingthepracticalcapabilitiesof
TheconfluenceofFederatedLearning(FL)andLargeLanguage theseclients[36].
Models(LLMs)isusheringinanewerainprivacy-preservingnat- Addressingthischallenge,weturnourattentiontoZeroth-Order
urallanguageprocessing.However,theintensivememoryrequire- Optimization(ZOO),analgorithmthatcomputesgradientapproxi-
mentsforfine-tuningLLMsposesignificantchallenges,especially mationswithoutexplicitgradientinformation,thussignificantly
whendeployingonedgedeviceswithlimitedcomputationalre- reducingmemoryconsumption[23].However,thecombinationof
sources.Tocircumventthis,weexplorethenovelintegrationof ZOOandFLâ€”aresearchdirectionwerefertoasZOO-FLâ€”remains
Memory-efficientZeroth-OrderOptimizationwithinafederated unexploredintheliteratureinthecontextofLLMs.Ourworkin-
setting,asynergywedenoteasFedMeZO.Ourstudyisthefirstto tendstobridgethisgapbyharnessingthememoryefficiencyof
examinethetheoreticalunderpinningsofFedMeZOinthecontext ZOOwithinthecontextoffederatedfine-tuningofLLMs,especially
ofLLMs,tacklingkeyquestionsregardingtheinfluenceoflarge onthefollowingtheoreticalfoundations:
parameterspacesonoptimizationbehavior,theestablishmentof (Q1) How does the vast parameter space of LLMs influence
convergenceproperties,andtheidentificationofcriticalparame- thebehaviorofZOO-FL?(Q2)Canweestablishtheconvergence
tersforconvergencetoinformpersonalizedfederatedstrategies. propertiesofZOO-FLforLLMs?(Q3)Whichmodelparametersare
Ourextensiveempiricalevidencesupportsthetheory,showing criticalforconvergence,andhowcanweleveragethemtooptimize
thatFedMeZOnotonlyconvergesfasterthantraditionalfirst-order FLperformance,suchasviapersonalization?
methodssuchasSGDbutalsosignificantlyreducesGPUmem- Inthispaper,wefocusonincorporatingamemory-efficientZOO
ory usage during training to levels comparable to those during method,MeZO[36]intoFL,asynergywedenoteasFedMeZO,and
inference.Moreover,theproposedpersonalizedFLstrategythat establishesitsconvergencepropertiesunderthelarge-scaleparam-
isbuiltuponthetheoreticalinsightstocustomizetheclient-wise eterspaceofLLMs.Wepresentrefinedconvergenceratesconsider-
learningratecaneffectivelyacceleratelossreduction.Wehope ingtheloweffectiverankofthemodelsâ€™Hessianmatrices[2,30].
ourworkcanhelptobridgetheoreticalandpracticalaspectsof Ourempiricalfindingscorroboratethetheoreticalpredictions,vali-
federatedfine-tuningforLLMsandfacilitatefurtherdevelopment datingconvergenceevenwhenscalinguptomodelswithbillions
andresearch. of parameters. In comparative studies with first-order methods
suchasFedAvg,FedMeZOconvergesfastermeanwhileremarkably
reducingGPUmemoryrequirements.
1 INTRODUCTION Moreover,werevealthelearningratetobeavariableofcrucial
importanceforconvergence.Buildingonourtheoreticalinsights,
Federated Learning (FL) has become an important approach in
we further tailor the learning rate to each clientâ€™s specific data
modern machine learning, particularly in scenarios where data
characteristics.Ourexperimentalresultsshowamorerapidlossre-
decentralizationandprivacy-preservingarecrucial[25,39,40,50,
ductionwhenimplementingthispersonalizedstrategy,asopposed
52].Centraltothislearningparadigmisthetrainingofacoherent
togenericorrandomlearningrateassignments.
globalmodelthroughtheaggregationofupdatesfrommultiple
Insummary,ourtheoreticalandempiricalexplorationvalidates
clients,facilitatedbyacentralserver,withouttheneedtoshare
FedMeZOinthefine-tuningprocessofLLMs,providingarigorous
rawdata[28,38].
frameworkandpracticalinsightsforfutureapplications.Ourkey
Inparallel,LargeLanguageModels(LLMs)haveradicallyad-
contributionsarethreefold:
vanced the field of natural language processing.[4, 14, 47] The
â€¢WeadvancetheunderstandingofFedMeZOforLLMs,extending
fine-tuningofthesemodels,alreadypre-trainedonvastcorpora,
thetwo-pointgradientestimationtofederatedtuningandestab-
hasproventobeahighlyeffectivestrategyforamultitudeoflan- (cid:16) (cid:17)
lishing theoretical convergence rate as O ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2 the
guagetasks,yieldingmodelsthatarebothversatileandcapableof
(cid:16) (cid:17) (cid:16) (cid:17)
adaptingtospecificdomainnarrativesoraligningwithnuanced i.i.d. setting and O ğ‘Ÿ3/2( (cid:101)ğ‘ â„ğ‘ğ»ğ‘‡)âˆ’1/2 âˆ’O ğœ â„2(ğ‘ â„ğ‘)âˆ’1 for the
humanfeedback[4,36].
non-i.i.d.setting.
ThetuningofLLMsrequiressuitablealignmentdata,whichare
â€¢WeanalyzetheimpactofvarioushyperparametersofFedMeZO
oftencostlytoacquire[9,13].Duetotheabundanceofprivatedata
andexploreatheory-informedstrategyforpersonalizedlearning
thatremainslargelyisolatedandunderutilized,theintersectionof
rateadjustmentstrategies.
FLandLLMshassparkedincreasinginterestamongresearchers
â€¢ThroughexperimentalvalidationwithLLMs,wedemonstratethat
[17,29,55].Notably,thisintegrationpresentssignificantcompu-
FedMeZOyieldseffectiveconvergencewithsubstantiallyreduced
tationalchallenges,especiallyforclientswithlimitedresources
memoryoverheadcomparedtoSGD.Additionally,weshowexten-
[8,56].ThescalingupofLLMsfurthercompoundsthisissue,asthe
siveempiricalevidencetosupporttheproposedtheoreticalresults.
computationofgradientsforbackpropagationincurssubstantial
4202
beF
8
]GL.sc[
1v62950.2042:viXraOurcodesareanonymouslyreleasedatOurcodesarepubliclyavail- clientsareassumedwithequalimportance[48],andthedatais
ableathttps://github.com/alibaba/FederatedScope/tree/FedMeZO. randomlysampledforefficiency[32].ğ¹ ğ‘–(ğœƒ,Bğ‘–)representsthelocal
lossfunctionw.r.taspecificmini-batchBğ‘– drawnfromDğ‘–.
2 PRELIMINARIES
Zeroth-OrderOptimization. Zeroth-orderoptimization(ZOO)
2.1 BackgroundandRelatedWorks isaprominenttechniqueinscenarioswheregradientsaredifficultto
FederatedFine-TuningofLargeLanguageModels. LargeLan- obtain,whichestimatesgradientsbyforwardpropagations.Given
guageModels(LLMs)havedemonstratedremarkablecapabilities
arandomvectorğ‘§andasmoothingconstantğœ‡,atypicalone-point
thatenableavarietyofreal-worldapplications[47,58,59].Thefed- gradientestimator[15]isdefinedas:
ğ‘§
e or nat ae dd afi pn tie n- gtu tn hi en sg eo mf oL dL eM lss th oas dore mce an intl -y spa ett cr ia fic cte td asa kt ste wnt hio iln e, pfo rec su es re vd
-
(cid:101)âˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)= 2ğœ‡(cid:0)ğ¹(ğœƒ+ğœ‡ğ‘§,B)âˆ’ğ¹(ğœƒ,B)(cid:1), (2)
ingtheprivacyofthetrainingdata.Chenetal.[6]investigated However,Eq.(2)providesabiasedgradientestimation,leadingto
theintegrationofLLMswithinfederatedsettings,highlightingthe acertaindegreeofinformationloss[35].Henceourworkemploys
inherentchallengesandpotentialopportunities.Zhangetal.[56] theZOOparadigmwithatwo-pointgradientestimatorproposed
furtheredthisresearchbyexamininginstructiontuningofLLMs by[36]inafederatedsetting:
inafederatedcontext,markingprogressinapplyingFLtothespe-
Definition 2.1. (Two-point gradient estimator) Given a set of
cializedtrainingofLLMs.NotableframeworkssuchasFATE-LLM
byFanetal.[18]andFederatedScope-LLMbyKuangetal.[29]
parametersğœƒ âˆˆRğ‘‘ foranLLMandamini-batchBğ‘–,thetwo-point
zeroth-ordergradientestimatorisformulatedas:
offerindustrial-gradeandcomprehensivesolutionsforfederated
ğ‘§
fi On rde- et ru Oni pn tg im.O izu ar tiw onor (k Z, Oin Oc )o wnt ir ta hs Ft, Lin fove rs tt hig ea fite ns et -h tue nf iu ns gio on fo Lf LZ Mer so ,t ah n- (cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ,ğ‘§ ğ‘–,Bğ‘–,ğœ‡)= 2ğœ‡ğ‘– (cid:0)ğ¹ ğ‘–(ğœƒ+ğœ‡ğ‘§ ğ‘–,Bğ‘–)âˆ’ğ¹ ğ‘–(ğœƒâˆ’ğœ‡ğ‘§ ğ‘–,Bğ‘–)(cid:1), (3)
areathathasyettobefullyinvestigated,therebyaddressingagap whereğ‘§ ğ‘– âˆ¼ N(0,ğ¼ ğ‘‘) isaGaussianrandomvariableandğœ‡ isthe
intheliteratureandprovidingfundamentaltheoreticalinsights. perturbation scale. The two-point gradient estimator in Eq. (3)
requiresonlytwoforwardpassesthroughthemodeltocompute
Zeroth-OrderOptimizationinFederatedLearning. ZOOhas
the estimation of gradient, which serves as a memory-efficient
emergedasaviablemethodtoaddressthedifficultiesofcomputing
alternativetobackpropagation(BP).
gradients in FL, especially in settings limited by computational
resources.Zhangetal.[57]proposedaZOOalgorithmtailoredfor TheFedMeZOAlgorithm. Inthispaper,westudyandana-
verticalFL,focusingonprivacypreservation.Yietal.[54]andLi lyzetheproprietiesofapracticalsynergyofMeZO[36]andFe-
etal.[31]studiedZOO-FLalgorithms,withdiscussionsonconver- dAvg [38], which is designed to fine-tune LLMs in an efficient,
gencepropertieswithsingle-pointperturbationandlocalupdates privacy-preservingandpersonalizedmanner.WetermthisZOO-FL
indecentralizedFL,respectively.Theconvergenceanalysisisa approachasFedMeZO,depictedwithfollowingprocesses:
criticalaspectofFL,asillustratedbyLietal.[34]fortheFedAvg Inasinglecommunicationround,thecentralserverfirstbroad-
algorithmandfurtherdevelopedbyFangetal.[19]formini-batch caststheglobalmodelparameterstoavailableclients.Oncethe
stochasticZOO-FLinwirelessnetworks.Moreover,Shuetal.[45] clientshavecompletedtheirlocalupdatesanduploadedtheirmod-
proposedenhancementstoqueryefficiencyforZOOwithintheFL els,theserveraggregatestheupdatesaccordingtoEquation(1),
framework.Ourresearchsetsitselfapartbyformulatingtheoret- formingthebasisforthesubsequentround.
icalconvergenceboundsforZOO-FL,specificallytailoredtothe Uponreceivingtheglobalmodelparameters,clientsperformthe
large-scaleparameterspaceofLLMs.Thisbuildsonthepreliminary followingsteps,distinguishingFedMeZOfromtraditionalBP-based
workbyMalladietal.[36],whichconfirmedthefeasibilityofZOO FedAvgalgorithmsintwofold:
forLLMsinacentralizedsetting.
(1)TrainingMemoryReduction:Clientsupdatetheirmodelsusing
thetwo-pointZOOgradientestimatordefinedinEquation(3)as:
2.2 ProblemFormulation ğ‘’ ğ‘–(ğ‘¡,ğ‘˜) =(cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜),ğ‘§ ğ‘–(ğ‘¡,ğ‘˜),B ğ‘–(ğ‘¡,ğ‘˜),ğœ‡), (4)
Wesummarizethefulllistofnotationsandpresentdetailedproof
where(ğ‘¡,ğ‘˜)denotestheğ‘˜ğ‘¡â„ iterationwithintheğ‘¡ğ‘¡â„
communication
ofallthetheoreticalresultsintheAppendix.
round.UnlikestandardZO-SGDalgorithmsthatrequirestoring
FederatedLearning. WeconsiderthegeneralFLsettingasof theperturbationvectorğ‘§ ateachiteration,FedMeZOresamples
FedAvg[38],withacentralserverandacollectionofğ‘ clients, ğ‘§usingrandomseedsinin-placeimplementation,thusreducing
indexedby1,2,...,ğ‘.Thecentralservercoordinatesthetrainingofa
memoryusagetoalevelequivalenttoinference[36].
globalmodelthroughthecollaborativeeffortsoftheseclients,each (2)CommunicationCostReduction:Tomitigatethehighcom-
holdinglocaldatasamplesdrawnfromtheirrespectivedistributions municationoverheadassociatedwithLLMs,FedMeZOleverages
Dğ‘–.Theoptimizationproblemcanbeformulatedas:
Low-RankAdaptation(LoRA)[26],whichintroducesreparametriza-
ğ‘
ğœƒm âˆˆi Rn ğ‘‘ğ‘“(ğœƒ)=â–³ ğ‘1 âˆ‘ï¸ ğ‘–=1ğ‘“ ğ‘–(ğœƒ ğ‘–), ğ‘“ ğ‘–(ğœƒ)=â–³E Bğ‘–âˆ¼Dğ‘–(cid:2)ğ¹ ğ‘–(ğœƒ,Bğ‘–)(cid:3), (1) t
t
tri ho aen inwt eo
h
dot Lu
l
Len Me LLt pw
M
oo
sw
ss em
e si
sa gl ahl
t
ld ose
,
wl bta
a
â€œism
ne
tda rt ior ni
n
sx icto hn
de
it mah
s
ee
s
nul si mn ioe
p
na
t
â€r
io
wla
n
hy
t
ee
h
nr as ati dn
w
as pete
l
tla ed dpro
te
of
-
whereğœƒ âˆˆ Rğ‘‘ denotestheğ‘‘-dimensionparameterofmodel,and newtasks.Introducingitcanhelpusfurtherreducethenumberof
ğ‘“(ğœƒ)andğ‘“ ğ‘–(ğœƒ)denotethegloballossfunctiononthecentralserver parameterstobeupdatedanduploaded,therebyaligningwiththe
andlocallossfunctiononğ‘–ğ‘¡â„
client,respectively.Typically,the practicalconstraintsoffederatedsettings.
22.3 LemmasandAssumptions Theorem3.1. (StepwiseLossDescentini.i.d.Setting)Under
Lemma2.2. (UnbiasedGradientEstimator)Thetwo-pointzeroth- Assumptions1-5andwithalearningrateğœ‚satisfying
ordergradientestimatordescribedinEq.(3)isanunbiasedestimator (cid:110) 1 ğ‘ 1 (cid:111)
ofthetruegradient,thatis, ğœ‚ â‰¤min 3ğ»ğ¿âˆšï¸ğ‘ ğ‘”ğ‘‘, 3ğ»ğ¿ğ‘ ğ‘”, ğ»2 , (7)
E[(cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ,ğ‘§ ğ‘–,Bğ‘–,ğœ‡)] =âˆ‡ğ‘“ ğ‘–(ğœƒ). (5)
theexpecteddecreaseinlossateachstepforFedMeZOunderthei.i.d.
TheHessianmatrix,whichisthesquarematrixofsecond-order scenarioisboundedas
p iza er sti ta hl ede cr ui rv va ati tv ue rs eo of ft th he elo loss ssw s. ur. rt ft ah ce em [2o 2d ]e .l Ap la thra om ue gt her ts h, ech sia zr eac ot fer a-
E ğ‘¡ (cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡
)âˆ’(cid:18)
2 âˆ’
2ğœ(cid:19)
ğœ‚E ğ‘¡(cid:12) (cid:12)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:12) (cid:12)2
ğ›¾ ğ‘‘
modelâ€™slossHessianisoftenassociatedwiththerateoffine-tuning,
studies suggest that the large-scale parameters of LLMs do not 2ğœ ğ‘”2ğœğœ‚ğ¿ ğœğœ‚ğœ‡2ğ¿3
+ + , (8)
necessarilyimpedeconvergence[1,27].Thisparadoxisaddressed ğ‘ğ»ğ‘‘ 2ğ‘ğ»
byrecognizingthatthelossHessianoftenexhibitsasmalllocal whereğ›¾ andğœ quantifytheeffectivelow-rankpropertiesofthegradi-
effectiverank[36],whichwecaptureinthefollowingassumption: entanditsestimator,respectively.
As â€¢su âˆ‡m 2ğ‘“p (t ğœƒio )n âª¯1 H. T (ğœƒh ğ‘¡e )re foe rx ais lt lğœƒa sH ue cs hsi ta hn atm âˆ¥a ğœƒtr âˆ’ix ğœƒğ‘¡H âˆ¥( â‰¤ğœƒğ‘¡ ğœ‚) ğ‘‘s ğºat (i ğœƒs ğ‘¡fy ),in wg h:
ere
InTheorem3.1,theterm(âˆ’2ğœ‚/ğ›¾)E ğ‘¡|âˆ‡ğ‘“(ğœƒğ‘¡)|2servesasacritical
ğº(ğœƒ â€¢ğ‘¡ T) he= em ffea cx tB ivâˆ¼ eD raâˆ¥ nâˆ‡ kğ‘“ o( fğœƒ Hğ‘¡,B (ğœƒ) ğ‘¡âˆ¥ ).
,denotedastr(H(ğœƒğ‘¡))/âˆ¥H(ğœƒğ‘¡)âˆ¥op,
f na ec gt ao tr ivth ea ct od nr ti rv ibe us tt oh re inde Ec qre .a (8s )e si un ct hhe thl ao ts Es ğ‘¡fu (cid:2)n ğ‘“c (ğœƒti ğ‘¡o +n 1, )a âˆ’si ğ‘“t (i ğœƒs ğ‘¡t )h (cid:1)e (cid:3)s â‰¤ol 0e
.
isatmostğ‘Ÿ.Heretrdenotesthetraceofthematrix,andâˆ¥Â·âˆ¥opdenotes Notethatthepresenceofthefactorğ›¾âˆ’1 = Î˜(ğ‘Ÿâˆ’1) underscores
theoperatornorm. the impact of the low effective rankğ‘Ÿ on the convergence rate
(underAssumption1),revealingthatareductioninğ‘Ÿcanaccelerate
Assumption1characterizesaloweffectiverankğ‘Ÿ intheHessian convergence independently of the high-dimensional parameter
matrix,whichdemonstratesthatLLMfine-tuningcanoccurina spaceğ‘‘.Consequently,evenforLLMswithexpansiveparameter
lowdimensionalsubspace(â‰¤ 200parameters)[2,30].Withthis spaces,FedMeZOcanattainconvergence.Thisaddressesourfirst
insight,[36]identifiedtheboundoflossdescentateachstepof foundationalquestionQ1:â€œHowdoesthevastparameterspaceof
centralizedZOO,whichispartiallyinfluencedbyğ‘Ÿ: LLMsinfluencethebehaviorofFedMeZOâ€.
Lemma2.3. (BoundedCentralizedDescent)Assumeğ‘“(ğœƒ)isğ¿- Besides,theterms(2ğœ‚ğœ/ğ‘‘)E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡)(cid:13) (cid:13)2 and(2ğœ ğ‘”2ğœğœ‚ğ¿/ğ‘ğ»ğ‘‘)both
smoothandlet (cid:101)âˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)betheunbiasedzeroth-ordergradient arescaledbyğœ/ğ‘‘,i.e.,inÎ˜(1/ğ‘Ÿğ‘‘2),contributingarelativelysmaller
estimatorfromEquation(3).IftheHessianmatrixH(ğœƒ)exhibitsa effectontheconvergencespeedcomparedtonegativeterm.This
localeffectiverankofğ‘Ÿ,andconstantsğ›¾ =Î˜(ğ‘Ÿ/ğ‘›)andğœ =Î˜(1/ğ‘Ÿğ‘‘) demonstratesthattheinfluenceonconvergencespeedfromthe
exist,thentheexpecteddecreaseinlosscanbeboundedasfollows: zeroth-ordergradientestimationismoderatedbythemodelâ€™seffec-
E(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’ 1 Â·ğœ‚âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡ )âˆ¥2 tivelowrankanddimensionality.Asforthelastterm,(ğœğœ‚ğœ‡2ğ¿3/2ğ‘ğ»),
ğ›¾ itactsasafactorslowingdowntheconvergencerate,andwecan
+1 ğœ‚2ğ¿Â·ğœ Â·E(cid:2) âˆ¥(cid:101)âˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)âˆ¥2(cid:3) (6) observethatwhenğ‘ andğ» arelarger,thistermbecomessmaller.
2 Thissuggeststhattheeffectofslowingdowntheconvergencerate
ğ‘‘ğ‘Ÿ+ğ‘‘âˆ’2 (ğ‘‘+2)ğ‘›2 isnotaspronounced,andsimultaneously,theperturbationstepğœ‡
where ğ›¾ = ,ğœ =
ğ‘›(ğ‘‘+2) (ğ‘‘ğ‘Ÿ+ğ‘‘âˆ’2)(ğ‘‘+ğ‘›âˆ’1) shouldnotbeexcessivelylarge.Specifically,thistermindicatesthat
increasingthenumberofclientsandthenumberoflocalrounds
whereğ‘›denotesthenumberofrandomizations.
canenhanceconvergence,whilealsoemphasizingtheimportance
FromEq.(6),weobservethattherateofdescentatasinglestep ofkeepingtheperturbationstepğœ‡moderate.
dependsonthegradientrelatedtoğ›¾ andthegradientestimation Aftergainingintuitiveinsightsineachroundoftrainingthrough
relatedtoğœ.Following[36],wesetğ‘›to1inthispaper. theanalysisofTheorem3.1,itisnecessarytoassesstheconvergence
Besides,tofacilitatetheanalysisinFLsetting,weintroducefour performanceofFedMeZOfromaglobalperspective.Weutilizethe
assumptions,includingBoundedLoss(Assumption2),ğ¿-smoothness squaredmagnitudeofthegradientE ğ‘¡âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ¥2asameasureto
(Assumption3),mini-batchgradienterrorbound (Assumption4, assessthesuboptimalityofeachiterate.Therapiditywithwhich
global-localdisparitiesini.i.d.andnon-i.i.d.settings(Assumptions5
the algorithmapproaches astationary pointserves asa crucial
and6respectively).Theseassumptionsarestandardandfounda- metricfordeterminingitsefficacyinthecontextofnon-convex
tionalinoptimizationandFLliterature[3,33,34,49],whichwe optimizationproblems[41].
detailinAppendixB.
Corollary3.2. (GlobalConvergenceini.i.d.Setting)Assum-
3 MAINRESULTS ingtheconditionsofTheorem3.1hold,theglobalconvergencefor
FedMeZOinthei.i.d.case,characterizedbyÎ“= ğ‘‘âˆ’ğœğ›¾,isgivenby
3.1 ConvergenceAnalysisini.i.d.Case ğ‘‘ğ›¾
I Mn et Zh Oiss wu ib thse incti to hn e, iw .i.e d.e dx aa tm ai dn ie stt rh ie buc to in ov ne sr eg te tn inc ge .p Wro ep ee sr tt aie bs lio sf hF te hd e- ğ‘¡m âˆˆ[i ğ‘‡n ]E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒ 20 ğœ‚) ğ‘‡âˆ’ Î“ğ‘“âˆ— + ğ‘ğœ ğ‘” ğ»2ğœ ğ‘‘ğ¿
Î“
+ 4ğœ ğ‘ğœ‡2 ğ»ğ¿ Î“3 (9)
conditionsunderwhichthealgorithmguaranteeslossreductionat
eachiterationandprovideaglobalconvergencerate. whereğ‘“âˆ—denotestheoptimallossvalue.
3Theupperboundontheminimumsquaredgradientnormacross However,i.i.d.dataistypicallyencounteredinidealizedenvi-
iterationsiscomposedofthreetermsinCorollary3.2.Thefirstterm ronments.Inreal-worldapplications,non-i.i.d.conditionsaremore
indicatesthatthedistancetotheoptimallossreliesontheinitial commonandchallenging.Next,wefurtherdiscussandanalyzethe
stateofoptimality,whilethesecondandthirdtermselucidatethe convergenceofFedMeZOundernon-i.i.d.settings.
influencesofstochasticmini-batcherrorsandtheperturbationscale
ğœ‡inherenttoZOO,respectively.Specifically,theybothreflectthe 3.2 ConvergenceAnalysisinnon-i.i.d.Case
impactofthemodelparametersğ‘‘andtheloweffectiverankğ‘Ÿ on Analyzingconvergenceinthecontextofnon-i.i.d.datadistributions
theoptimalloss.Aspointedoutin[41],bychoosinganappropriate iscrucialforunderstandingthebehaviorofFLalgorithmsinreal-
stepsize,wecanobtainthedesiredaccuracy. worldscenarios.Inthissection,weextendourconvergenceanalysis
Giventhatğ›¾ = Î˜(ğ‘Ÿ) andğœ = Î˜(cid:16) 1 (cid:17) ,wehaveğœğ›¾ = Î˜(cid:16) ğ‘Ÿ (cid:17) = tothecasewhereclientdatadistributionsareheterogeneous.
ğ‘Ÿğ‘‘ ğ‘Ÿğ‘‘
(cid:16) (cid:17) (cid:16) (cid:17)
Î˜ 1 .Astheparameterğ‘‘ islarge,ğ‘‘ âˆ’ğœğ›¾ = ğ‘‘ âˆ’Î˜ 1 isdomi- Theorem3.4. (StepwiseLossDescentinnon-i.i.d.Setting)Let
ğ‘‘ ğ‘‘
natedbyğ‘‘.Consequently,thedominanttermofÎ“is ğ‘‘ğ›¾ ,which Assumptions1-4andAssumption6holdandlearningrateğœ‚satisfy
simplifiestoğ›¾ =Î˜(ğ‘Ÿ).Therefore,Î“=Î˜(cid:16) ğ›¾1(cid:17) =Î˜(cid:16) ğ‘Ÿ1(cid:17) .ğ‘‘ Bâˆ’ uğœ iğ›¾ ldingon ğœ‚ â‰¤min(cid:110) 3ğ»ğ¿1 âˆšï¸ğ‘ ğ‘”ğ‘‘, 3ğ»ğ‘ ğ¿ğ‘ ğ‘”, ğ»1 2(cid:111) . (11)
thisrelationship,wehavethefollowingcorollarythatarticulates
Then,theexpectedlossateachstepforFedMeZOinthenon-i.i.d.
theconvergencerateofFedMeZO.
settingisboundedas
C coo nr do itl il oa nr sy o3 f. C3 o. ro(C llao rn yv 3er .2g hen olc de aR na dt ge ii vn eni. ğœ‚i.d =.S (ğ‘ett ğ»in )1g /) 2A (ğ‘Ÿs ğ‘‡su )m âˆ’1i /n 2g at nh de E ğ‘¡ (cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’(cid:18) ğ›¾2
ğ‘
âˆ’ 2ğœ ğ‘‘(cid:101)ğ‘ â„(cid:19) ğœ‚E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2
ğœ‡=(ğ‘ğ»)1/4ğ‘Ÿâˆ’1/2,wehave
2ğœ2ğœğ¿ğœ‚ ğœğœ‚ğœ‡2ğ¿3 2
+ (cid:101) + âˆ’ ğœ‚ğœ2, (12)
ğ‘¡m âˆˆ[i ğ‘‡n ]E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤O(cid:16) ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2(cid:17) +O(cid:16) ğ‘‘âˆ’1(ğ‘Ÿğ‘ğ»)âˆ’1/2(cid:17) .
where (cid:101)ğ‘
â„
=ğ‘ â„+ğ‘ and
(cid:101)ğœ2ğ‘ =ğ» 3ğ‘‘
ğ‘ ğ‘”ğœ
â„2+2 ğœğ‘ ğ‘”2.ğ» ğ›¾ğ‘ â„
(10)
ComparingEq.(12)withitsi.i.d.counterpartEq.(8),thenon-i.i.d.
Theexpressionontheright-handsideofEq.(10)isdominated settingintroducesadditionaltermsreflectingdataheterogeneity.
byO(cid:16) ğ‘Ÿ3/2(ğ‘ğ»ğ‘‡)âˆ’1/2(cid:17) .Consequently,wehavederivedthecon- Firstly,anadditionalterm (cid:101)ğ‘ â„appearsbeforeE ğ‘¡âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡)âˆ¥2;secondly,
vergencerateforFedMeZO.Theloweffectiverankğ‘Ÿ significantly theoriginalğœ ğ‘”2 changeinto (cid:101)ğœ2;thirdly,anewtermrelatedtoğœ â„2
contributestoloweringtheconvergencerate,whichisalsoinflu- isaddedattheend.Theterm (cid:101)ğ‘ â„ amplifiestheeffectofthegradi-
encedbythenumberofclientsğ‘,thenumberoflocaliterations entnorm,while (cid:101)ğœ2 encapsulatesboththeintrinsicstochasticity
ğ»,andthetotalnumberofcommunicationroundsğ‘‡.Moreover, anddataheterogeneity.Thepresenceofğœ â„2 indicatestheimpact
tosatisfythelearningrateconditioninEq.(7),thevaluesofğ‘,ğ», ofclientdatadivergenceontheconvergencebehavior,inadegree
andğ‘‡ mustbesuitablylarge.
dependentonÎ˜(1/ğ‘Ÿğ‘‘2).Giventhatthecontributionofthenegative
termacceleratestherateofdeclineineachround,itcanbecon-
ItisimportanttonotethatFedMeZOdoesnotprimarilyaimto
cludedthatheterogeneityispositivelycorrelatedwithconvergence.
accelerateconvergencespeedbutrathertoidentifytheconvergence
Consideringalltheabovechanges,appropriateheterogeneitycan
rate under assumptions pertinent to LLMs. This is intended to
aidinthemodelconvergence.
demonstratethatFedMeZOcanachieveconvergenceevenwithin
ExperimentalresultsinSection??confirmthatamorerandom-
avastparameterspace.InaseriesofstudiesonfederatedZOO,
izeddatasetdistributionleadstoimprovedconvergence,supporting
FederatedZeroth-OrderOptimization(FedZO)presentsthemost
ourtheoreticalinsights.Next,wepresenttheglobalconvergence
comprehensiveandcompleteanalysiswithaconvergencerateof
O(cid:16)âˆšï¸ğ‘‘/(ğ‘ğ»ğ‘‡ğ‘
1ğ‘
2)(cid:17)
[19],whichexhibitsalowerratecompared
resultforthenon-i.i.d.settingbuildinguponTheorem3.4.
to O(cid:0)ğ‘‘3/ğ‘‡(cid:1) of ZONE-S [24] and accounts for the impact of ğ» Corollary3.5. (GlobalConvergenceinNon-i.i.d.Setting)As-
comparedtoO(cid:16)âˆšï¸ğ‘‘/ğ‘ğ‘‡(cid:17)
ofDZOPA[54].IncontrasttoFedZO,our
sumingtheconditionsofTheorem3.4hold,denote (cid:101)Î“= ğ‘‘ ğ‘‘âˆ’ ğ›¾ğ‘ ğ‘ğ›¾ğœ,Fed-
MeZOsatisfies:
method,FedMeZO,theoreticallysupportsafasterconvergenceby
replacingğ‘‘1/2withğ‘Ÿ3/2andsettingğ‘ 1=ğ‘ 2=1.Thesecomparisons min E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒ0)âˆ’ğ‘“âˆ— + (cid:101)ğœ2ğœğ¿
showthatFedMeZOaddressesthechallengesposedbylargemodels ğ‘¡âˆˆ[ğ‘‡] 2(cid:101)Î“ (cid:101)ğ‘ â„ğœ‚ğ‘‡ (cid:101)Î“ (cid:101)ğ‘ â„ğ‘ğ»ğ‘‘
andtheirnumerousparameters,offeringanefficientconvergence ğœğœ‡2ğ¿3 ğœ2
ratethatreliesontheloweffectiverank. + âˆ’ â„ . (13)
This advancement signifies progress in optimizing federated
4(cid:101)Î“ (cid:101)ğ‘ â„ğ‘ğ» (cid:101)Î“ (cid:101)ğ‘ â„ğ›¾ğ‘
learningalgorithms,particularlyforLLMs,wherethescalability (cid:16) (cid:17)
Givenğ›¾ = Î˜(ğ‘Ÿ) andğœ = Î˜ 1 ,theexpressionğ‘‘ âˆ’ğ‘ğœğ›¾ sim-
of parameters and data heterogeneity are major challenges. By ğ‘Ÿğ‘‘
emphasizingtheloweffectiverank,ourapproachenhancesboth plifies toğ‘‘
âˆ’Î˜(cid:16)ğ‘
ğ‘‘
(cid:17)
which is dominated byğ‘‘. Consequently,(cid:101)Î“
thetheoreticalunderstandingofconvergencebehaviorincomplex (cid:16) (cid:17)
settingsandtheguidanceinsightsintothesettingsoflearningrates simplifiestoÎ˜ ğ‘Ÿ1 asinthei.i.d.case.ComparedtoCorollary3.2,
andotherparameterstoachieveefficientconvergenceoutcomes. Corollary3.5introducestwochanges:first,alltermsontheright
4sideoftheinequalityincludeadenominatorğ‘ ğ‘”,andsecond,thereis thannon-personalizedFL:
wan ita hd Î˜di (t ğœio â„2n /a ğ‘l â„t ğ‘er )m .Ta hs is soc fuia rt te hd erw dit eh mn oo nn s- ti r. ai. td e. sh te hte er co og nen ste ri aty in, is nc gale ed
f-
ğœ‚ ğ‘– =ğœ‚ 0(1+ğ›¼Â·Î¦ ğ‘–), (15)
fectofdataheterogeneityinFedMeZO.SimilartoCorollary3.3, whereğœ‚ 0 represents a default learning rate applicable in a i.i.d.
bysettingappropriatevaluesforğœ‚andğœ‡,weobtainthefollowing setting,ğ›¼ isascalingfactorthatdeterminesthesensitivityofthe
convergencerate. learningrateandÎ¦ ğ‘– istheheterogeneityindex,representingthe
extentofğ‘ ğ‘”andğœ â„2.Thispropositionunderscorestheimportance
Corollary3.6. (ConvergenceRateinNon-i.i.d.Setting)Assum- ofconsideringdataheterogeneityinthedesignofthelearningrate
ingtheconditionsofCorollary3.5hold,withğœ‚ =(ğ‘ğ»)1/2(ğ‘Ÿ (cid:101)ğ‘ â„ğ‘‡)âˆ’1/2
strategy withinpFL, offeringa structuredapproachto enhance
andğœ‡=( (cid:101)ğ‘â„ğ‘ğ»)1/4ğ‘Ÿâˆ’1/2,FedMeZOhasconvergencerateasfollows: learningoutcomesacrossdiverseclientdatasets.
InSection5.4,weempiricallyconfirmthataparticularimplemen-
ğ‘¡m âˆˆ[i ğ‘‡n ]E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤O(cid:16) ğ‘Ÿ3/2( (cid:101)ğ‘ â„ğ‘ğ»ğ‘‡)âˆ’1/2(cid:17) t ta ot nio on teo tf ht ah tis thst era dt ae tg ay hf ea tc ei rl oit ga ete ns eif ta yst ie nr dc eo xn Î¦v ğ‘–er cg ae nn nc oe t. bIt eis deim tep rmor it na en dt
(cid:16) (cid:17) (cid:16) (cid:17)
+O ğ‘‘âˆ’1(ğ‘Ÿ (cid:101)ğ‘ â„ğ‘ğ»)âˆ’1/2 âˆ’O ğœ â„2(ğ‘ â„ğ‘)âˆ’1 . (14) apriori;therefore,weutilizeseveralproxymeasuresduringthe
trainingprocesstoestimateit.Ourgoalisnottoprescribeanexact
solutiontothisstrategy,butrather,throughanalysisandempirical
TheconvergencerateinEq.(14)isprimarilydrivenbytheterm
(cid:16) (cid:17) investigation,toenlightenfurtherresearchanddevelopmentof
O ğ‘Ÿ3/2( (cid:101)ğ‘ â„ğ‘ğ»ğ‘‡)âˆ’1/2 ,indicatingthatoptimizingthebalancebe-
personalizedFedMeZOformoreeffectivetrainingofLLMs.
tween (cid:101)ğ‘ â„,ğ‘ â„,andğ‘ iscrucial,whichreflectsacomplexinterplay Thediscussionandcorrespondingempiricalsupportaddress
ofheterogeneity.Specifically,weobservethattoachievebetter theQ3:"Whichmodelparametersarecriticalforconvergence,and
convergence,asmallerO(cid:16) ğ‘Ÿ3 2( (cid:101)ğ‘ â„ğ‘ğ»ğ‘‡)âˆ’1 2(cid:17) ispreferredwhilethe howcanweleveragethemtooptimizeFLperformance,suchasvia
(cid:16) (cid:17) personalization?".
O ğœ â„2(ğ‘ â„ğ‘)âˆ’1 term need to increase at the same time. Conse-
Besides,recallthatweadoptLoRAtomitigatethecommuni-
quently,thebalancebetweenğ‘ ğ‘”,ğ‘ â„,andğ‘ becomesadynamic cation burden associated with LLMs for practical FL scenarios.
trade-offprocess,i.e.,theheterogeneityamongdifferentclients
Nonetheless,theinfluenceofLoRAonthemodelâ€™sloweffective
directlyinfluencestheoverallconvergenceperformance. rank,especiallywhenamalgamatedwiththeMeZOmethod,re-
Fornow,wehaveansweredthequestion(Q2)â€œCanweestablish
mainsanopenquestion.Wethusadvancethefollowingconjecture,
theconvergencepropertiesofZOO-FLforLLMs?â€viatheoremsand
predicatedonexistingliterature[43,53],tofacilitatefurthervalida-
corollariesmentionedinthissection.Wealsovalidatedthenatureof tions:
convergenceunderdifferentscenariosandtasksthroughempirical
experimentsinSection5.2. Conjecture3.8(RankCorrelation). Theoptimalreparametriza-
tionrankğ‘Ÿ usedinLow-RankAdaptation(LoRA)ispositively
LoRA
proportionaltotheeffectiverankğ‘Ÿ oftheHessianmatrixH(ğœƒğ‘¡)of
3.3 Implications
thetunedLLM.Theğ‘Ÿ islower-boundedbyğ‘Ÿ,andcanserveasan
LoRA
Theaforementionedtheoreticalresultsoffernumerousinsightsinto empiricalproxyforğ‘Ÿ.
parametertuning.Acriticalrevelationfromouranalysispertains
totheconstraintsimposedonthelearningrate,asdelineatedin 4 PROOFOUTLINE
Eq.(7)andEq.(11),whichsuggeststhatanoptimallearningrate
âˆš Thissectionprovidesanoutlineofthederivationspresentedin
magnitudeisanchoredat1/ ğ‘‘.Largerlearningratesarenotonly
Section3,emphasizingthekeyanalyticaltechniquesandconcepts
ineffectualbutalsoposeariskofdestabilizingthetrainingdynamic.
employed.DetailedproofsareavailableinAppendixD.
IntheAppendixF.6,ourempiricalexperimentscorroboratethis
WebeginbytakingexpectationsonbothsidesofEq.(6),consid-
hypothesis,demonstratingthatexcessivelearningratesprecipitate
eringafederatedlearningsetting.Theequationissplitintotwo
abruptincreasesinloss.
mainpartsforfurtheranalysis:
Furthermore,ourinsightsregardingthelearningrateopenup
prospects for personalized FL, a compelling approach that uses
ğ‘
client-specificconfigurationstoaddressheterogeneityandhasat- E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’ 1 Â·ğœ‚E ğ‘¡(cid:13) (cid:13)1 âˆ‘ï¸ âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )(cid:13) (cid:13)2
tractedincreasinginterest[7,10,16,37,44].Specifically,weinves- ğ›¾ (cid:13)ğ‘ (cid:13)
ğ‘–=1
tigatetheory-guidedpersonalizedstrategiesbydynamicallyadjust-
ğ‘ ğ»
i on fg dath tae hle ea tern roin gg enr ea it te yğœ‚ ağ‘– mi on np gr co lp ieo nrt ti so .n Into liga hq tu oa fn Tt hifi ea ob rele mm 3e .4as tu hr ae
t
+ 21 ğœ‚2ğ¿Â·ğœ Â·E ğ‘¡(cid:13) (cid:13) (cid:13)ğ‘1 âˆ‘ï¸âˆ‘ï¸ ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13) (cid:13)2 . (16)
ğ‘–=1ğ‘˜=1
alargerheterogeneityismoreconducivetomodelconvergence,
henceitisfeasibletoappropriatelyincreasethelearningrateto
Forsimplicity,wedenotethetwoexpectationtermsasğ‘‡ 1andğ‘‡ 2,
whichpertaintotheexpectedsquarednormsofthegradientand
allowthisclienttocontributemoretotheoverallconvergence,we
thezeroth-ordergradientestimator,respectively.
proposethefollowingtailoredadjustmentstrategy:
Proposition3.7. (AdaptiveLearningRateAdjustment)LetAs- 4.1 ProofofTheorem3.1
sumption6holds,thelearningrateğœ‚ ğ‘– canbeadjustedaccordingto Fortermğ‘‡ 1inEq.(16),weutilizetheCauchy-Schwarzinequality
theformulatobetteraccommodatethevariedlearninglandscapes âˆ¥ğ‘+ğ‘âˆ¥2 â‰¤ 2âˆ¥ğ‘âˆ¥2 +2âˆ¥ğ‘âˆ¥2 todecomposeitintotwoparts,with
5the first representing the discrepancy between local and global In Eq. (21), E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 remains unknown and we need
gradients.ByinvokingAssumption5,weestablish:
toconstrainitfurther.Thekeyideaistotransformthisexpecta-
ğ‘‡
1
â‰¤ ğ‘2 âˆ‘ï¸ğ‘ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ’âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2. t ci oo nn clt uer sm ionin ot fo Ea q.fo (1rm 8)are nl dat Ee qd .t (o 19E )ğ‘˜ ğ‘¡ foâˆ¥ğ‘’ rğ‘–( cğ‘¡, oğ‘˜ m) âˆ¥ p2 ua tan td iot nh .e Tn hu eti dli ez te ait lh ede
ğ‘–=1
derivationprocessisprovidedintheAppendixD.1andwecanhave
Fortermğ‘‡ 2,Jensenâ€™sinequalityallowsustoboundtheexpected theboundedresult:
squarednormofthezeroth-ordergradientestimatorasfollows: ğ‘ ğ»
ğ‘‡ 2 â‰¤
ğ‘1
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ»
E
ğ‘¡(cid:13)
(cid:13) (cid:13)ğ‘’
ğ‘–(ğ‘¡,ğ‘˜)(cid:13)
(cid:13)
(cid:13)2
. (17)
ğ‘1 âˆ‘ï¸ ğ‘–=1ğ‘˜âˆ‘ï¸ =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 â‰¤ ğ¶ ğ¶1 0, (22)
ğ‘–=1ğ‘˜=1 whereğ¶ 0 = 1 âˆ’ 3ğ‘ ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2 andğ¶ 1 = 2ğ‘ ğ‘”ğ‘‘ğ»3ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡)(cid:13) (cid:13)2
CauB cy hysu -Sb cs hti wtu at ri zng ineE qq u. a( l3 i) tyin toto deE cq o. m(1 p7 o) s, ew the isp gro race de ied ntto esu tis me at th oe
r
+ 32ğ‘‘ğœ ğ‘”2ğ»3ğœ‚2+ ğœ‡2ğ¿2ğ‘‘ 62ğ»3ğœ‚2 .
Finally,bysubstitutingEq.(22)intoEq.(21),weobtainthefi-
intotwoparts,eachofwhichisabiasedestimator.Recallthatinour
gradientestimator,ğ‘§ ğ‘– followsaGaussiandistribution.Therefore, nalresultofthestepwisedescent.Aftersimplifyingandappropri-
atelysettingthelearningrate,wearriveattheresultpresentedin
theimpactonthenormcausedbyaforwardstepandabackward
Theorem3.1.Thedetailedderivationofthisresultisprovidedin
stepoftheestimatorisidentical.Consequently,weascertainthat
thetermğ‘‡ 2 isboundedbyasingle-pointgradientestimationas AppendixD.1.
(cid:13) (cid:13)2
E ğ‘¡(cid:13) (cid:13) (cid:13)ğ‘§ ğ‘–(ğ‘¡ ğœ‡,ğ‘˜)(cid:16) ğ¹ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) +ğœ‡ğ‘§ ğ‘–(ğ‘¡,ğ‘˜),B ğ‘–(ğ‘¡,ğ‘˜) )âˆ’ğ¹ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜),B ğ‘–(ğ‘¡,ğ‘˜) )(cid:17)(cid:13) (cid:13)
(cid:13)
. 4.2 ProofofTheorem3.4
(cid:13) (cid:13) Theproofforthenon-i.i.d.casefollowsasimilarstructuretothatof
FollowingLemma4.1in[21],wecanboundtheexpectationterm
thei.i.d.case,withadjustmentsmadefortheheterogeneitybetween
as:
(cid:34) (cid:35)
localandglobalmodelsascapturedbyğ‘
â„
andğœ â„2(Assumption6).
E ğ‘¡(cid:13) (cid:13) (cid:13)ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘‘1
2
2ğ‘‘Â·E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ¹ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜),B ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 + ğœ‡ 22 ğ¿2ğ‘‘2 vIn arp ia ar nt cic eu dla ur e,w toe nr oed ne -ifi .in .de .ğ‘‡ d1 ai tn a:Eq.(16)asğ‘‡ (cid:101)1toreflecttheincreased
(cid:34) (cid:35) ğ‘
â‰¤ ğ‘‘1
2
2ğ‘ ğ‘”ğ‘‘Â·E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 +2ğ‘‘ğœ ğ‘”2+ ğœ‡ 22 ğ¿2ğ‘‘2 , (18) ğ‘‡ (cid:101)1 â‰¤ ğ‘2 âˆ‘ï¸ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ’âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2
ğ‘–=1
wherethesecondinequalityisderivedbasedonAssumption4.Sub- â‰¤2(1+ğ‘ â„)E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2+2ğœ â„2, (23)
sequently,weboundtheexpectationtermbyapplyingtheCauchy-
wherethesecondinequalityfollowsAssumption6.
Schwartzinequalitytodivideitintothreeparts:
Subsequently,ğ‘‡ (cid:101)2iscomputedsimilarly,withtheheterogeneity
E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 =E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )âˆ“âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ“âˆ‡ğ‘“(ğœƒ ğ‘–ğ‘¡ )(cid:13) (cid:13) (cid:13)2 termsincorporated.Thedifferencefromthei.i.d.caseliesinthe
boundingofexpectationterminEq.(19):
â‰¤3ğ¿2E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 +3E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 . (19) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 â‰¤3ğ¿2E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2
Thefirstpartrepresentsthegradientdifferencebetweenstages
(ğ‘¡,ğ‘˜)and(ğ‘¡,0),whichcanbecomputedusingAssumption3,i.e., +3(ğ‘ â„+1)E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +3ğœ â„2, (24)
theğ¿-smoothcondition.Thesecondpartsignifiesthedisparity
wheretheinequalityemploysAssumption6.
betweenlocalandglobalaspects,calculatedusingAssumption5.
Thethirdpartisretainedasis.
CombiningEquations(17),(18)and(24),weboundğ‘‡ (cid:101)2asfollow:
Comb ğ‘‡in 2i â‰¤ng 6E
ğ‘
ğ‘q
ğ‘”
ğ‘‘u ğ¿a 2ti âˆ‘ï¸o ğ‘ns âˆ‘ï¸ğ»(17 E), ğ‘¡(
(cid:13)
(cid:13)
(cid:13)1 ğœƒ8 ğ‘–() ğ‘¡,a ğ‘˜n )d âˆ’(1 ğœƒ9 ğ‘¡)
(cid:13)
(cid:13)
(cid:13), 2weboundğ‘‡ 2asfollow: ğ‘‡ (cid:101)2 â‰¤ 6ğ‘ ğ‘ğ‘” ğ‘‘ğ¿2 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 + 6ğ‘ ğ‘”(ğ‘ â„ ğ‘‘+1)ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
ğ‘–=1ğ‘˜=1
+
6ğ‘ ğ‘”ğœ â„2ğ»
+
2ğ»ğœ ğ‘”2
+
ğœ‡2ğ»ğ¿2
. (25)
+6ğ‘ ğ‘‘ğ‘”ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 + 2ğ» ğ‘‘ğœ ğ‘”2 + ğœ‡2ğ» 2ğ¿2 . (20)
Bysubstitutingğ‘‡
(cid:101)1inEqğ‘‘ .(23)andğ‘‘
ğ‘‡
(cid:101)2inEq.2
(25)intoEq.(16),we
Next,combiningEquations(16),(17)and(20),wehave: gettheresultunderthenon-i.i.d.conditionasfollow:
E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) âˆ’ğ‘“(ğœƒğ‘¡ ) â‰¤ (cid:32) 3ğ‘ ğ‘”ğœ ğ‘‘ğœ‚2ğ»ğ¿ âˆ’ 2 ğ›¾ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) âˆ’ğ‘“(ğœƒğ‘¡ ) â‰¤ (cid:32) 3ğ‘ ğ‘”(cid:101)ğ‘ â„ ğ‘‘ğœğœ‚2ğ»ğ¿ âˆ’ 2 (cid:101)ğ‘ ğ›¾â„ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
+ 3ğ‘ ğ‘” ğ‘ğœğœ‚ ğ‘‘2ğ¿3 âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 + 3ğ‘ ğ‘” ğ‘ğœğœ‚ ğ‘‘2ğ¿3 âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2
ğ‘–=1ğ‘˜=1 ğ‘–=1ğ‘˜=1
+ğœ ğ‘”2ğœğœ‚ ğ‘‘2ğ»ğ¿ +ğœğœ‚2ğœ‡ 42ğ»ğ¿3
. (21)
+(cid:101)ğœ2ğœğœ‚ ğ‘‘2ğ»ğ¿ +ğœğœ‚2ğœ‡ 42ğ»ğ¿3
âˆ’
ğ›¾2
ğœ â„2ğœ‚, (26)
6Dolly-Meta Code-LDA GSM-IID Dolly-Meta Code-LDA GSM-IID
BP-based SGD BP-based SGD 1.25 BP-based SGD =1e 3 =1e 3 1.25 =1e 3
1.8 FedMeZO 0.92 FedMeZO 1.20 FedMeZO 1.8 = =5 2e e 5 4 0.92 = =5 2e e 5 4 1.20 = =5 2e e 5 4
1.15 1.15
1.7 0.90 1.10 1.7 0.90 1.10
1.6 1.05 1.6 0.88 1.05
0.88 1.00 1.00
1.5 0.95 1.5 0.86 0.95
0.86 0.90 0.90
1.4 0.85 1.4 0.84 0.85
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds Rounds Rounds Rounds
Figure1:ConvergencecomparisonofFedMeZOandBP-based Figure2:Effectsofdifferentperturbationscalesğœ‡.
SGDoptimization.
1,whileallthecomprehensiveresultsareavailableinAppendix
where (cid:101)ğ‘ â„ denotes(ğ‘ â„+1)and (cid:101)ğœ2denotes(3ğ‘ ğ‘”ğœ â„2+ğœ ğ‘”2).Wethen F.5.
needtomaketheexpectationtermbounded.UnlikeEq.(22),dueto
thevariationsintroducedbyEq.(24),twoadditionaltermsrelated Table1:TheGPUMemoryofBP-basedSGDandFedMeZO.
to (cid:101)ğ‘ â„ and (cid:101)ğœ2emerge,yieldingthefollowingresult:
ğ‘1 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 â‰¤ ğ¶ ğ¶2 0, (27) DolT lya -s Mk
eta
BP-base 2d 65S 7G 1D(MiB) FedM 1e 0Z 0O 61(MiB)
GSM8K-IID 17771 9733
whereğ¶
1
= 1âˆ’3ğ‘ ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2 andğ¶
2
= 2ğ‘ ğ‘”ğ‘‘ (cid:101)ğ‘ â„ğ»3ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡)(cid:13) (cid:13)2 CodeAlpaca-LDA 15287 9569
+2ğœ2ğ‘‘ğ»3ğœ‚2+ ğœ‡2ğ¿2ğ‘‘2ğ»3ğœ‚2 .
3(cid:101) 6
Finally, we can substitute Eq. (27) into Eq. (26) and have the
Two conclusions emerge from the convergence experiments:
resultofTheorem3.4.Thedetailedderivationsaboutthesestepsof
First,whenthelearningratecomplieswiththerequirementsdis-
Theorem3.4areprovidedinAppendixD.3.
cussedinSection3.3,asstipulatedbyTheorems3.1and3.4,Fed-
MeZOconsistentlydiminisheslosswitheachstep,ultimatelyachiev-
5 EMPIRICALSUPPORT
ing stable convergence. Second, under equivalent learning rate
Thissectionaimstoempiricallyvalidateourtheoreticalfindings configurations, FedMeZO decreases loss more rapidly than BP-
throughaseriesofexperiments. basedSGD,indicatingaswifterconvergencerate.Forinstance,in
theDolly-Metafigure,FedMeZOstabilizesandconvergesaround
5.1 ExperimentalSetup
300rounds,whereasBP-basedSGDâ€™slossisstilldecliningatthis
WeutilizeLLaMA-3B[47]asthefoundationalmodelandemploy juncture.Notably,fromTable1,weobservethattheGPUmem-
fourdatasetscoveringarangeoftasksanddatadistributiontypes orydemandforFedMeZOisroughlyone-thirdofthatrequiredby
toprovidecomprehensivevalidationofourtheoreticalresults[29]. BP-basedSGD,suggestingthatFedMeZOcanachieveaspeedier
Given that our theory centers on the loss function, we primar- convergencewithfewerresources.
ilyfocusonanalyzinglossdescentinourexperiments.Thebasic
informationofourexperimentalsetupisdetailedinTable3. 5.3 Hyper-parametersStudy
Wesetthetotalnumberofcommunicationroundsto500.By Inthissubsection,weperformaseriesofexperimentstoascertain
default,BP-basedbaselinesundertakelocaltrainingforoneepoch, the influence of various hyper-parameters, as intimated by our
whereasourproposedmethod,referredtoasFedMeZO,conductslo- theoreticalfindings.
caltrainingfor30steps.Formoredetailedimplementationspecifics,
pleaserefertoAppendixE. 5.3.1 ImpactofPerturbationScaleğœ‡. Tocorroboratethetheoretical
impactsoftheperturbationsteponconvergence,weexamineğœ‡
5.2 ConvergenceStudy values of 5Ã—10âˆ’3 and 2Ã—10âˆ’4, in addition tothe default ğœ‡ =
1Ã—10âˆ’3.WeleveragethesamedatasetsandsplittersasinSection
ToassesstheconvergenceofFedMeZO,weperformedexperiments
5.2forrobustness.Figure2showsrepresentativeoutcomes,with
onthreedatasetsusingdifferentdatasplitters,asspecifiedinTable3,
comprehensiveresultsinAppendixF.1.
withtestlossservingastheconvergencemetric.Ourobjectiveisto
Theresultsconfirmthat,consistentwithEquations(10)and(14),
evaluatethegeneralizationandstabilityofFedMeZOacrossdiverse
asmallerğœ‡marginallyexpeditesmodelconvergence.Figure2exem-
datasetsandheterogeneityscenarios.Forbenchmarkingpurposes,
plifiesthatthetrainingtrajectorywithğœ‡=2Ã—10âˆ’4descendsmore
wealsomeasuredtheperformanceofBP-basedSGDonthesame
rapidlythantheothers.However,giventhatğœ‡appearsasasecond-
datasets.Additionally,wedocumenttheGPUmemoryusageduring
ğœğœ‡2ğ¿3
traininginTable1.RepresentativefindingsareillustratedinFigure ordertermin 4(cid:101)Î“ (cid:101)ğ‘â„ğ‘ğ» anditsabsolutevalueisrelativelysmall,its
7
ssoL ssoL ssoL ssoL ssoL ssoLDolly-Meta Code-LDA GSM-IID Round-wise Loss Five-round Loss Model Update Difference
1.9 H=30 H=30 H=30 Default Default Default
H=10 H=10 H=10 2.2 Random 2.2 Random 2.2 Random
1.8 H=50 0.92 H=50 1.2 H=50 Strategy Strategy Strategy
1.7 0.90 2.0 2.0 2.0
1.1
1.6 0.88 1.8 1.8 1.8
1.0
1.5 0.86 1.6 1.6 1.6
0.9
1.4 0.84
0 100 Ro200 und300
s
400 500 0 100 Ro200 und300
s
400 500 0 100 Ro200 und300
s
400 500 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300 1.4 0 50 1 R00 ou15 n0 d2 s00 250 300
Figure3:Effectsofdifferentlocaliterationsğ». Figure4:Comparisonofpersonalizedlearningratewithde-
faultandrandomsettings.
overallinfluenceismodest.ThisisevidentinFigure2,wheremod-
ificationstoğœ‡withinaspecificrangeyieldonlyslightvariations.
Thus,asmallerğœ‡provesadvantageousformodelconvergence. themaximallearningrate,potentiallyleadingtosurgesasperthe
learningratesearchnetwork.Symmetrically,weposittheminimal
5.3.2 ImpactofLocalIterationsğ». Tovalidatethetheoreticalim- valueat5Ã—10âˆ’6,anchoredonthedefaultlearningrateof1Ã—10âˆ’5,
pactoflocaliterationsonconvergence,wecontrastğ» = 10and therebyassigningğ›¼ avalueof5Ã—10âˆ’6.
ğ» =50withthestandardğ» =30.Utilizingidenticaldatasetsand
Ascounterpoints,wefurnishtwoconfigurationstrategiesforthe
splittersfromSection5.2,wepresenttypicalfindingsinFigure3, learningrateadjustment:oneuniformlyappliesadefaultlearning
withallthedetailedresultsforthcominginAppendixF.2. rate of 1Ã—10âˆ’5, while the other randomly selects within (5Ã—
Theseexperimentalresultssuggestthatalowerğ» engendersa 10âˆ’6,1.5Ã—10âˆ’5)foreachround.Theconclusiveresultsaredepicted
moresluggishconvergencepace,whereasahigherğ» somewhat inFigure4.
propelsconvergence,mirroringtheimpactofğ»asadenominatorin
Basedontheexperimentalresults,weobservedthatourmethod
thetheoreticalconvergencerateanalysis.Nonetheless,anexcessive achievedfasterlossconvergencewiththesecondandthirdtypes
ğ» mayleadtoinstability,asdepictedbytheğ» = 50line,which ofsignalquantitiescomparedtothedefaultandrandomsettings,
exhibitsasurgeendwiseinthegraph.Hence,anappropriatechoice withthethirdtypeyieldingthemostimpressiveperformance.In
ofğ» facilitatesefficientmodelconvergence. contrast,thefirsttypeofsignalquantityhadanegligibleimpact.
Theseresultssuggestthatwhileindividualroundtrainlossesex-
5.3.3 Analysis of Other Hyper-parameters. We also explore the
hibitsomedegreeofrandomness,aggregatinglossesovermultiple
ramificationsofdatasplittersandthenumberofclientsonthe
roundscanapproximateheterogeneitytoameaningfulextent,thus
convergencerate.Detailspertainingtotheseexperimentalsettings
servingasanindicatortoexpeditemodelconvergence.Itisalso
aredocumentedinAppendicesF.3andF.4,respectively.Here,we
noteworthythatthethirdtypeofsignalquantityalignswiththe
merelysummarizetheexperimentaloutcomes.
Dissimilarsplitterssymbolizevaryingextentsofdatahetero- expression âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡) âˆ’(cid:205) ğ‘˜ğ» =0ğœ‚ ğ‘–âˆ‡ğ‘“ ğ‘–(ğœƒ(ğ‘¡,ğ‘˜))âˆ¥2, which most closely
geneity.Fromtheseresults,wehaveobservationsrevealingthat reflectsAssumption6.Consequently,itdemonstratedthemostef-
augmentedheterogeneityculminatesinlowerstabilizedlossvalues, fectiveperformanceintheexperiments,notonlyachievingthe
intimatingthatamoderatedegreeofdataheterogeneitycanelevate fastestconvergencebutalsotheloweststableloss.Thiscasestudy
themodelâ€™sconvergenceproficiency. experimentsubstantiatestheefficacyofProposition3.7,offering
valuableinsightsforparametertuninginpersonalizedFederated
5.4 PersonalizationStudy Learning(pFL).
ToreconcileProposition3.7articulatedinSection3.3withpractical
6 CONCLUSION
scenarios,weconductsubsequentexperiments.Toaccountforeach
clientâ€™sheterogeneityduringmodelupdatesineachround,wede- ThisstudyinvisigatestheconvergenceofFedMeZO,apractical
rivethreesignalquantities:(1)Round-wiseTrainLossDifference: approachintegratingZeroth-OrderOptimizationwithinafeder-
Thediscrepancybetweeneachclientâ€™slossintheprecedingtrain- atedlearningframeworkforLargeLanguageModels.Extensive
ingroundandthegloballoss.(2)Five-roundAverageTrainLoss empiricalresultsverifiedouranalysesandindicatethatFedMeZO
Difference:Theaveragelossdeviationforeachclientrelativetothe achievesfastconvergencewithreducedGPUmemoryrequirements,
globallossovertheantecedentfiverounds.(3)ModelParameter offeringanpromosingalternativetotraditionaloptimizationmeth-
UpdateDifference:Thedisparitybetweeneachclientâ€™sprevious ods.Theincorporationofapersonalizedlearningrateadjustment,
roundparameterupdatesandtheglobalupdatemagnitude.We derivedfromtheoreticalanalysis,hasshowntoeffectivelyenhance
normalizethesesignalquantitiestotherangeof(âˆ’1,1),servingas lossreduction.Wehopeourworkcanenlightmoreresearchand
proxiesforÎ¦. developmentofmemory-efficientoptimizationtechniquestoad-
Forthesettingofthescalingfactorğ›¼,followingtheguidance dresspracticalchallengesassociatedwiththefine-tuningofLLMs,
ofthelearningrateinAppendixF.6,wedesignate1.5Ã—10âˆ’5 as particularlyinresource-constrainedenvironments.
8
ssoL ssoL ssoL ssoL ssoL ssoLREFERENCES
[25] JunyuanHong,ZhuangdiZhu,ShuyangYu,ZhangyangWang,HirokoDodge,
[1] AlekhAgarwal,MartinJWainwright,PeterBartlett,andPradeepRavikumar. andJiayuZhou.2021.FederatedAdversarialDebiasingforFairandTransferable
2009.Information-theoreticlowerboundsontheoraclecomplexityofconvex
Representations.InKDD.
optimization.AdvancesinNeuralInformationProcessingSystems22(2009). [26] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,Shean
[2] ArmenAghajanyan,SonalGupta,andLukeZettlemoyer.2021. IntrinsicDi- Wang,LuWang,andWeizhuChen.2022.LoRA:Low-RankAdaptationofLarge
mensionalityExplainstheEffectivenessofLanguageModelFine-Tuning.In
LanguageModels.InTheTenthInternationalConferenceonLearningRepresenta-
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLin- tions,ICLR.
guisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing [27] KevinGJamieson,RobertNowak,andBenRecht.2012. Querycomplexityof
(Volume1:LongPapers).7319â€“7328. derivative-freeoptimization.AdvancesinNeuralInformationProcessingSystems
[3] LÃ©onBottou,FrankECurtis,andJorgeNocedal.2018.Optimizationmethodsfor 25(2012).
large-scalemachinelearning.SIAMreview60,2(2018),223â€“311. [28] PeterKairouz,H.BrendanMcMahan,BrendanAvent,AurÃ©lienBellet,MehdiBen-
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, nis,ArjunNitinBhagoji,KallistaBonawitz,ZacharyCharles,GrahamCormode,
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda RachelCummings,RafaelG.L.Dâ€™Oliveira,HubertEichner,SalimElRouayheb,
Askell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural DavidEvans,JoshGardner,ZacharyGarrett,AdriÃ GascÃ³n,BadihGhazi,PhillipB.
informationprocessingsystems33(2020),1877â€“1901. Gibbons,MarcoGruteser,ZaidHarchaoui,ChaoyangHe,LieHe,ZhouyuanHuo,
[5] SahilChaudhary.2023.Codealpaca:Aninstruction-followingllamamodelfor BenHutchinson,JustinHsu,MartinJaggi,TaraJavidi,GauriJoshi,MikhailKho-
codegeneration. dak,JakubKonecnÃ½,AleksandraKorolova,FarinazKoushanfar,SanmiKoyejo,
[6] ChaochaoChen,XiaohuaFeng,JunZhou,JianweiYin,andXiaolinZheng.2023. TancrÃ¨deLepoint,YangLiu,PrateekMittal,MehryarMohri,RichardNock,Ayfer
Federatedlargelanguagemodel:Apositionpaper.arXivpreprintarXiv:2307.08925 Ã–zgÃ¼r,RasmusPagh,HangQi,DanielRamage,RameshRaskar,MarianaRaykova,
(2023). DawnSong,WeikangSong,SebastianU.Stich,ZitengSun,AnandaTheertha
[7] DaoyuanChen,DaweiGao,WeiruiKuang,YaliangLi,andBolinDing.2022. Suresh,FlorianTramÃ¨r,PraneethVepakomma,JianyuWang,LiXiong,Zheng
pFL-Bench:AComprehensiveBenchmarkforPersonalizedFederatedLearning. Xu,QiangYang,FelixX.Yu,HanYu,andSenZhao.2021.AdvancesandOpen
InNeurIPS. ProblemsinFederatedLearning.FoundationsandTrendsÂ®inMachineLearning
[8] DaoyuanChen,DaweiGao,YuexiangXie,XuchenPan,ZitaoLi,YaliangLi,Bolin 14,1â€“2(2021),1â€“210.
Ding,andJingrenZhou.2023.FS-REAL:TowardsReal-WorldCross-DeviceFed- [29] WeiruiKuang,BingchenQian,ZitaoLi,DaoyuanChen,DaweiGao,XuchenPan,
eratedLearning.InProceedingsofthe29thACMSIGKDDConferenceonKnowledge YuexiangXie,YaliangLi,BolinDing,andJingrenZhou.2023.Federatedscope-
DiscoveryandDataMining.3829â€“3841. LLM:Acomprehensivepackageforfine-tuninglargelanguagemodelsinfeder-
[9] DaoyuanChen,YilunHuang,ZhijianMa,HesenChen,XuchenPan,CeGe,
atedlearning.arXivpreprintarXiv:2309.00363(2023).
DaweiGao,YuexiangXie,ZhaoyangLiu,JinyangGao,YaliangLi,BolinDing, [30] ChunyuanLi,HeeradFarkhoor,RosanneLiu,andJasonYosinski.2018.Measuring
andJingrenZhou.2023.Data-Juicer:AOne-StopDataProcessingSystemfor
theIntrinsicDimensionofObjectiveLandscapes.InInternationalConferenceon
LargeLanguageModels.arXivpreprintarXiv:2309.02033(2023). LearningRepresentations.
[10] DaoyuanChen,LiuyiYao,DaweiGao,BolinDing,andYaliangLi.2023.Efficient [31] LeiLaiLi,JianzongWang,XiaoyangQu,andJingXiao.2021.Communication-
PersonalizedFederatedLearningviaSparseModel-Adaptation.arXivpreprint memory-efficientdecentralizedlearningforaudiorepresentation.In2021Inter-
arXiv:2305.02776(2023). nationalJointConferenceonNeuralNetworks(IJCNN).IEEE,1â€“8.
[11] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun, [32] MuLi,TongZhang,YuqiangChen,andAlexanderJSmola.2014.Efficientmini-
LukaszKaiser,MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,
batchtrainingforstochasticoptimization.InProceedingsofthe20thACMSIGKDD
etal.2021. Trainingverifierstosolvemathwordproblems. arXivpreprint internationalconferenceonKnowledgediscoveryanddatamining.661â€“670.
arXiv:2110.14168(2021). [33] TianLi,AnitKumarSahu,ManzilZaheer,MaziarSanjabi,AmeetTalwalkar,
[12] MikeConover,MattHayes,AnkitMathur,XiangruiMeng,JianweiXie,JunWan, andVirginiaSmith.2020.Federatedoptimizationinheterogeneousnetworks.
SamShah,AliGhodsi,PatrickWendell,MateiZaharia,etal.2023. Freedolly:
ProceedingsofMachinelearningandsystems2(2020),429â€“450.
Introducingtheworldâ€™sfirsttrulyopeninstruction-tunedllm. [34] XiangLi,KaixuanHuang,WenhaoYang,ShusenWang,andZhihuaZhang.2019.
[13] MikeConover,MattHayes,AnkitMathur,JianweiXie,JunWan,SamShah,
OntheConvergenceofFedAvgonNon-IIDData.InICLR.
AliGhodsi,PatrickWendell,MateiZaharia,andReynoldXin.2023.FreeDolly: [35] SijiaLiu,Pin-YuChen,BhavyaKailkhura,GaoyuanZhang,AlfredOHeroIII,
IntroducingtheWorldâ€™sFirstTrulyOpenInstruction-TunedLLM. andPramodKVarshney.2020.Aprimeronzeroth-orderoptimizationinsignal
[14] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,JiezhongQiu,ZhilinYang,and processingandmachinelearning:Principals,recentadvances,andapplications.
JieTang.2021.Glm:Generallanguagemodelpretrainingwithautoregressive
IEEESignalProcessingMagazine37,5(2020),43â€“54.
blankinfilling.arXivpreprintarXiv:2103.10360(2021). [36] SadhikaMalladi,TianyuGao,EshaanNichani,AlexDamian,JasonDLee,Danqi
[15] JohnCDuchi,MichaelIJordan,MartinJWainwright,andAndreWibisono.2015. Chen,andSanjeevArora.2023.Fine-TuningLanguageModelswithJustForward
Optimalratesforzero-orderconvexoptimization:Thepoweroftwofunction
Passes.arXivpreprintarXiv:2305.17333(2023).
evaluations.IEEETransactionsonInformationTheory61,5(2015),2788â€“2806. [37] Othmane Marfoq, Chuan Xu, Giovanni Neglia, and Richard Vidal. 2020.
[16] AlirezaFallah,AryanMokhtari,andAsumanOzdaglar.2020.Personalizedfeder- Throughput-OptimalTopologyDesignforCross-SiloFederatedLearning.In
atedlearning:Ameta-learningapproach.InNeurIPS2020. NeurIPS,H.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),
[17] TaoFan,YanKang,GuoqiangMa,WeijingChen,WenbinWei,LixinFan,and Vol.33.CurranAssociates,Inc.,19478â€“19487. https://proceedings.neurips.cc/
QiangYang.2023.FATE-LLM:AIndustrialGradeFederatedLearningFramework paper/2020/file/e29b722e35040b88678e25a1ec032a21-Paper.pdf
forLargeLanguageModels.CoRRabs/2310.10049(2023). [38] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
[18] TaoFan,YanKang,GuoqiangMa,WeijingChen,WenbinWei,LixinFan,and BlaiseAguerayArcas.2017. Communication-efficientlearningofdeepnet-
QiangYang.2023.Fate-llm:Aindustrialgradefederatedlearningframeworkfor
worksfromdecentralizeddata.InAISTATS.PMLR,1273â€“1282.
largelanguagemodels.arXivpreprintarXiv:2310.10049(2023). [39] ChuizhengMeng,SirishaRambhatla,andYanLiu.2021.Cross-nodefederated
[19] WenzhiFang,ZiyiYu,YuningJiang,YuanmingShi,ColinNJones,andYong
graphneuralnetworkforspatio-temporaldatamodeling.InKDD.1202â€“1211.
Zhou.2022.Communication-efficientstochasticzeroth-orderoptimizationfor [40] KhalilMuhammad,QinqinWang,DiarmuidOâ€™Reilly-Morgan,EliasTragos,Barry
federatedlearning.IEEETransactionsonSignalProcessing70(2022),5058â€“5073. Smyth,NeilHurley,JamesGeraci,andAonghusLawlor.2020. Fedfast:Going
[20] HaozheFeng,TianyuPang,ChaoDu,WeiChen,ShuichengYan,andMinLin.
beyondaverageforfastertrainingoffederatedrecommendersystems.InKDD.
2023.DoesFederatedLearningReallyNeedBackpropagation?arXivpreprint 1234â€“1242.
arXiv:2301.12195(2023). [41] YuriiNesterovandVladimirSpokoiny.2017.Randomgradient-freeminimization
[21] XiangGao,BoJiang,andShuzhongZhang.2018.Ontheinformation-adaptive
ofconvexfunctions.FoundationsofComputationalMathematics17(2017),527â€“
variantsoftheADMM:aniterationcomplexityperspective.JournalofScientific 566.
Computing76(2018),327â€“363. [42] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
[22] BehroozGhorbani,ShankarKrishnan,andYingXiao.2019. Aninvestigation Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019.
intoneuralnetoptimizationviahessianeigenvaluedensity.InInternational Pytorch:Animperativestyle,high-performancedeeplearninglibrary.Advances
ConferenceonMachineLearning.PMLR,2232â€“2241. inneuralinformationprocessingsystems32(2019).
[23] JiaqiGu,ChenghaoFeng,ZhengZhao,ZhoufengYing,RayTChen,andDavidZ [43] LeventSagun,UtkuEvci,VUgurGuney,YannDauphin,andLeonBottou.2017.
Pan.2021.Efficienton-chiplearningforopticalneuralnetworksthroughpower-
Empiricalanalysisofthehessianofover-parametrizedneuralnetworks.arXiv
awaresparsezeroth-orderoptimization.InProceedingsoftheAAAIconferenceon preprintarXiv:1706.04454(2017).
artificialintelligence,Vol.35.7583â€“7591. [44] FelixSattler,Klaus-RobertMÃ¼ller,andWojciechSamek.2020.ClusteredFederated
[24] DavoodHajinezhad,MingyiHong,andAlfredoGarcia.2019. ZONE:Zeroth- Learning:Model-AgnosticDistributedMultitaskOptimizationUnderPrivacy
ordernonconvexmultiagentoptimizationovernetworks.IEEETrans.Automat. Constraints.TNNLS(2020).
Control64,10(2019),3995â€“4010.
9[45] YaoShu,XiaoqiangLin,ZhongxiangDai,andBryanKianHsiangLow.2023.
FederatedZeroth-OrderOptimizationusingTrajectory-InformedSurrogateGra-
dients.arXivpreprintarXiv:2308.04077(2023).
[46] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos
Guestrin,PercyLiang,andTatsunoriBHashimoto.2023.Stanfordalpaca:An
instruction-followingllamamodel.
[47] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Lachaux,TimothÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,Faisal
Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
preprintarXiv:2302.13971(2023).
[48] JianyuWang,ZacharyCharles,ZhengXu,GauriJoshi,HBrendanMcMa-
han,MaruanAl-Shedivat,GalenAndrew,SalmanAvestimehr,KatharineDaly,
DeepeshData,etal.2021.Afieldguidetofederatedoptimization.arXivpreprint
arXiv:2107.06917(2021).
[49] JianyuWang,QinghuaLiu,HaoLiang,GauriJoshi,andHVincentPoor.2021.A
novelframeworkfortheanalysisanddesignofheterogeneousfederatedlearning.
IEEETransactionsonSignalProcessing69(2021),5234â€“5249.
[50] ZhenWang,WeiruiKuang,YuexiangXie,LiuyiYao,YaliangLi,BolinDing,and
JingrenZhou.2022.FederatedScope-GNN:TowardsaUnified,Comprehensive
andEfficientPackageforFederatedGraphLearning.InProceedingsofthe28th
ACMSIGKDDConferenceonKnowledgeDiscoveryandDataMining.4110â€“4120.
[51] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
AnthonyMoi,PierricCistac,TimRault,RÃ©miLouf,MorganFuntowicz,etal.
2020.Transformers:State-of-the-artnaturallanguageprocessing.InProceedings
ofthe2020conferenceonempiricalmethodsinnaturallanguageprocessing:system
demonstrations.38â€“45.
[52] YuexiangXie,ZhenWang,DaweiGao,DaoyuanChen,LiuyiYao,WeiruiKuang,
YaliangLi,BolinDing,andJingrenZhou.2023.FederatedScope:AFlexibleFed-
eratedLearningPlatformforHeterogeneity.ProceedingsoftheVLDBEndowment
16,5(2023),1059â€“1072.
[53] ZheweiYao,AmirGholami,KurtKeutzer,andMichaelWMahoney.2020.Pyhes-
sian:Neuralnetworksthroughthelensofthehessian.In2020IEEEinternational
conferenceonbigdata(Bigdata).IEEE,581â€“590.
[54] XinleiYi,ShengjunZhang,TaoYang,andKarlHJohansson.2022.Zeroth-order
algorithmsforstochasticdistributednonconvexoptimization.Automatica142
(2022),110353.
[55] JianyiZhang,SaeedVahidian,MartinKuo,ChunyuanLi,RuiyiZhang,Guoyin
Wang,andYiranChen.2023.TowardsBuildingtheFederatedGPT:Federated
InstructionTuning.arXivpreprintarXiv:2305.05644(2023).
[56] JianyiZhang,SaeedVahidian,MartinKuo,ChunyuanLi,RuiyiZhang,Guoyin
Wang,andYiranChen.2023.TowardsBuildingtheFederatedGPT:Federated
InstructionTuning.arXivpreprintarXiv:2305.05644(2023).
[57] QingsongZhang,BinGu,ZhiyuanDang,ChengDeng,andHengHuang.2021.
Desirablecompanionforverticalfederatedlearning:NewZeroth-ordergradient
basedalgorithm.InProceedingsofthe30thACMInternationalConferenceon
Information&KnowledgeManagement.2598â€“2607.
[58] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,Shuohui
Chen,ChristopherDewan,MonaDiab,XianLi,XiVictoriaLin,etal.2022.Opt:
Openpre-trainedtransformerlanguagemodels.arXivpreprintarXiv:2205.01068
(2022).
[59] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,
YingqianMin,BeichenZhang,JunjieZhang,ZicanDong,etal.2023.Asurvey
oflargelanguagemodels.arXivpreprintarXiv:2303.18223(2023).
10APPENDIX Bysymmetry,wechangeğ›¿toâˆ’ğ›¿andobtain
A NOTATION
For ease of reading and reference, we present all mathematical âˆ‡ğ¹ ğ‘–(ğœƒ,Bğ‘–)=âˆ’E ğœ‡ğ‘§ğ‘–âˆ¼N(0,ğœ‡2ğ¼ğ‘‘)(cid:104)ğ‘§ ğœ‡ğ‘– Â·ğ¹ ğ‘–(ğœƒâˆ’ğœ‡ğ‘§ ğ‘–,Bğ‘–)(cid:105) . (29)
symbolsusedinthispaperinTable2.
B ASSUMPTIONS Furtherweprovethatwith
Assumption2. (BoundedLoss)Thegloballossfunctionğ‘“(ğœƒ)is
boundedbelowbyascalarğ‘“âˆ—,i.e.,ğ‘“âˆ— â‰¥ ğ‘“(ğœƒ) >âˆ’âˆforallğœƒ.
âˆ‡ğ¹ ğ‘–(ğœƒ,Bğ‘–)=
1 E(cid:104)ğ‘§ ğ‘–
Â·ğ¹ ğ‘–(ğœƒ+ğœ‡ğ‘§
ğ‘–,Bğ‘–)(cid:105)
âˆ’
1 E(cid:104)ğ‘§ ğ‘–
Â·ğ¹ ğ‘–(ğœƒâˆ’ğœ‡ğ‘§
ğ‘–,Bğ‘–)(cid:105)
Assumption3. (ğ¿-smoothness)Thelocalandgloballossfunctions 2 ğœ‡ 2 ğœ‡
ğœƒğ¹ ğ‘– ,(ğœƒ ğœƒ,B âˆˆğ‘–) R, ğ‘‘ğ‘“ ğ‘– ,( iğœƒ t) h, oa ldn sd thğ‘“ a( tğœƒ) are L-smooth. Mathematically, for any =E(cid:104) 2ğ‘§ ğœ‡ğ‘– (cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ,ğ‘§ ğ‘–,Bğ‘–,ğœ‡)(cid:105) . (30)
1 2
Finally,weadoptEq.(1)andgetLemma2.2,i.e.,
âˆ¥âˆ‡ğ‘“ ğ‘–(ğœƒ 2)âˆ’âˆ‡ğ‘“ ğ‘–(ğœƒ 1)âˆ¥ â‰¤ğ¿âˆ¥ğœƒ 2âˆ’ğœƒ 1âˆ¥,
ğ¿
ğ‘“ ğ‘–(ğœƒ 2) â‰¤ ğ‘“ ğ‘–(ğœƒ 1)+âŸ¨âˆ‡ğ‘“ ğ‘–(ğœƒ),ğœƒ 2âˆ’ğœƒ 1âŸ©+ âˆ¥ğœƒ 2âˆ’ğœƒ 1âˆ¥2.
2 âˆ‡ğ‘“ ğ‘–(ğœƒ)=E[âˆ‡ğ¹ ğ‘–(ğœƒ,Bğ‘–)] =E[(cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ,ğ‘§ ğ‘–,Bğ‘–,ğœ‡)]. (31)
Assumption4. (Mini-batchGradientErrorBound)Forany
ğœƒ âˆˆRğ‘‘,thesecond-ordermomentofthestochasticgradientisbounded
â–¡
byE Bğ‘–âˆ¥âˆ‡ğ¹ ğ‘–(ğœƒ,Bğ‘–)âˆ¥2 â‰¤ğ‘ ğ‘”âˆ¥âˆ‡ğ‘“ ğ‘–(ğœƒ)âˆ¥2+ğœ ğ‘”2,whereğ‘
ğ‘”
â‰¥1.
Assumption5. (Global-LocalDisparitiesini.i.d.Setting)For C.2 ProofofLemma2.3
anyğœƒ âˆˆRğ‘‘,thediscrepancybetweenthelocalandglobalgradientis
negligible,i.e.,E ğ‘–âˆ¥âˆ‡ğ‘“(ğœƒ)âˆ’âˆ‡ğ‘“ ğ‘–(ğœƒ)âˆ¥2=0. Malladietal.[36]presentastep-wiselearningratedecaycorollary
thatisindependentofthedimensionalityparameterğ‘‘andissolely
Assumption6. (Global-LocalDisparitiesinnon-i.i.d.Setting) related to the low effective rank ğ‘Ÿ. We formalize this result as
Foranyğœƒ âˆˆRğ‘‘,thediscrepancybetweenthelocalandglobalgradient follows:
isboundedbyâˆ¥âˆ‡ğ‘“(ğœƒ)âˆ’âˆ‡ğ‘“ ğ‘–(ğœƒ)âˆ¥2 â‰¤ğ‘ â„âˆ¥âˆ‡ğ‘“(ğœƒ)âˆ¥2+ğœ â„2,whereğ‘
â„
is
apositiveconstant. LemmaC.1(Step-WiseLearningRateDecay). AssumingtheHes-
sianmatrixintermsofğœƒ exhibitsalocaleffectiverankofğ‘Ÿ,andthere
Assumptions2-4arewell-establishedintheliteratureonlarge- existsaconstantğ›¾ = ğ‘‘ğ‘Ÿ+ğ‘‘âˆ’2 =Î˜(ğ‘Ÿ/ğ‘›),theexpecteddecreaseinthe
scalestochasticoptimization[3].Assumption5describesanideal
ğ‘›(ğ‘‘+2)
losscanbeboundedasfollows:
i.i.d.settingwhereeachclientâ€™sgradientisalignedwiththeglobal
gradient.Assumption6accountsfortheheterogeneityofclient
datadistributionsthatistypicalinnon-i.i.d.settings[33,34,49]. E(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’ 1 ğœ‚âˆ¥âˆ‡ğ‘“(ğœƒğ‘¡ )âˆ¥2+ 1 ğœ‚2ğ¿1 E(cid:2) âˆ¥âˆ‡ğ¹(ğœƒ,B)âˆ¥2(cid:3),
ğ›¾ 2 ğ›¾
C FULLPROOFOFLEMMA (32)
C.1 ProofofLemma2.2
WerecaptheproofofSteinâ€™sidentityfollowing[20],notingthatğ›¿ whereğ‘›denotesthenumberofrandomizations.
isarandomvariablesampledfromN(0,ğ¼ ğ‘‘),andwereplaceitwith
ğœ‡ğ‘§ ğ‘– inDefinition2.1. ItisimportanttonotethatthelastterminEq.(32)represents
thesquarednormofthetruegradient,whichisinconsistentwith
(cid:104) (cid:105) thezeroth-orderestimationmethodusedinourapproach.There-
âˆ‡ğ¹ ğ‘–(ğœƒ,Bğ‘–)=E
ğ›¿âˆ¼N(0,ğœ2ğ¼ğ‘‘)
âˆ‡ğ¹ ğ‘–(ğœƒ+ğ›¿,Bğ‘–)
fore,atransformationisnecessarytoalignitwiththeFedMeZO
=(2ğœ‹)âˆ’ğ‘‘ 2âˆ‡âˆ«
ğ¹
ğ‘–(ğœƒ+ğ›¿,Bğ‘–)Â·exp(cid:16) âˆ’âˆ¥ğ›¿âˆ¥2 2(cid:17)
ğ‘‘ğ›¿
a trl ugo er git rh am di. eB ne ts nid oe rs m,M anal dla td hi ee zt ea rl o.d the -t oai rl dt eh re gr re ala dt ii eo nn ts eh si tp imbe at tw oree nn ot rh me
,
2ğœ2
whichwerestateasfollows:
=(2ğœ‹)âˆ’ğ‘‘
2
âˆ«
ğ¹
ğ‘–(ğœƒ+ğ›¿,Bğ‘–)Â·âˆ‡exp(cid:16) âˆ’âˆ¥ğœƒ+ğ›¿âˆ’ğœƒâˆ¥2 2(cid:17)
ğ‘‘ğœƒ
2ğœ2 LemmaC.2(GradientEstimatorNormRelationship). Thesquared
=(2ğœ‹)âˆ’ğ‘‘
2
âˆ«
ğ¹ ğ‘–(ğœƒ+ğ›¿,Bğ‘–)Â·
ğ›¿ Â·exp(cid:16) âˆ’âˆ¥ğ›¿âˆ¥2 2(cid:17)
ğ‘‘ğ›¿
normofthegradientestimatedbytheMeZOisgivenby
ğœ2 2ğœ2
=E ğ›¿âˆ¼N(0,ğœ2ğ¼ğ‘‘)(cid:104) ğœğ›¿
2
Â·ğ¹ ğ‘–(ğœƒ+ğ›¿,Bğ‘–)(cid:105) E(cid:104) âˆ¥âˆ‡ğ¹(ğœƒ,B)âˆ¥2(cid:105) = ğ‘‘+ğ‘›ğ‘› âˆ’1E(cid:104) âˆ¥(cid:101)âˆ‡ğ¹(ğœƒ,ğ‘§,B,ğœ‡)âˆ¥2(cid:105) . (33)
=E ğœ‡ğ‘§ğ‘–âˆ¼N(0,ğœ‡2ğ¼ğ‘‘)(cid:104)ğ‘§ ğœ‡ğ‘– Â·ğ¹ ğ‘–(ğœƒ+ğœ‡ğ‘§ ğ‘–,Bğ‘–)(cid:105) . (28)
BysubstitutingEq.(33)intoEq.(32),weobtainLemma2.3. â–¡
11Symbol Description
ğ‘“(ğœƒ) Globallossfunctionovertheparameterğœƒ
âˆ‡ğ‘“(ğœƒ) Gradientofthelossfunctionwithrespecttoparameterğœƒ
ğ¹ ğ‘–(ğœƒ,Bğ‘–) Locallossfunctionontheğ‘–ğ‘¡â„ clientwithmini-batchBğ‘–
âˆ‡ğ¹(ğœƒ,B) Thegradientofparameterğœƒ withmini-batches
(cid:101)âˆ‡ğ¹ ğ‘–(ğœƒ,ğ‘§,Bğ‘–,ğœ‡) Zeroth-ordergradientestimatorforğ¹
ğ‘–
withperturbationğœ‡
ğ‘§ GaussianrandomvariablesampledfromN(0,ğ¼ ğ‘‘)
Bğ‘– Mini-batchofdatasampledfromthelocaldistributionDğ‘–
ğœ‡ Perturbationscaleforzeroth-ordergradientestimation
ğœ‚ Learningrateformodelupdates
ğ‘› Numberofperturbationsinğ‘›-SPSAzeroth-orderoptimization
(ğ‘¡,ğ‘˜) ğ‘˜ğ‘¡â„ iterationwithintheğ‘¡ğ‘¡â„ communicationround
H Hessianmatrixofthelossfunction
ğ‘Ÿ TheloweffectiverankofHessianmatrix
ğ›¾ Factorquantifyingtheeffectivelowrankpropertyofthegradient
ğœ Factorquantifyingtheeffectivelowrankpropertyofthegradientestimator
ğ‘‘ Dimensionofthemodelparameterğœƒ
ğ¿ Smoothnessconstantofthelossfunction
ğ‘ NumberofclientsparticipatinginFL
ğ‘– Indexidentifyingtheğ‘–ğ‘¡â„ client
ğ‘‡ NumberofcommunicationroundsinFL
ğ‘’ ğ‘–(ğ‘¡,ğ‘˜) Zeroth-ordergradientestimatoroftheğ‘–ğ‘¡â„ clientiniteration(ğ‘¡,ğ‘˜)
ğ» Totalnumberoflocaliterationswithinacommunicationround
ğ‘ ğ‘”,ğœ ğ‘” Constantsrelatedtothegradientestimationgapcausedbymini-batchstochasticity
ğ‘ â„,ğœ â„ Constantsrelatedtotheheterogeneityofclientdataandtheglobalmodel
E ğ‘¡ Expectationtakenovertherandomnessintheğ‘¡ğ‘¡â„ round
Eğ‘˜
ğ‘¡
Expectationtakenovertherandomnessintheğ‘˜ğ‘¡â„ iterationoftheğ‘¡ğ‘¡â„
round
Table2:Descriptionofsymbolsusedinthepaper.
D FULLPROOFOFTHEOREMS whereğ‘’ ğ‘–(ğ‘¡,ğ‘˜) representsthegradientestimatordefinedinEq.(6).
Substitutingtheestimatorintotheaboveexpressionandsimplify-
D.1 ProofofTheorem3.1
Forthetermğ‘‡ 1:
ğœƒin ğ‘–(g ğ‘¡,, ğ‘˜w
)
,e
ğ‘§
ğ‘–â€²o dbt ea ni on teth ğ‘§e ğ‘–(ğ‘¡f ,o ğ‘˜l )l ,o aw ni dng Bi ğ‘–n â€²e dq eu na ol ti ety B.F ğ‘–o (ğ‘¡r ,ğ‘˜b )r :evity,letğœƒ ğ‘–â€² denote
(cid:13) ğ‘ (cid:13)2
ğ‘‡ 1 ===
â‰¤
â‰¤
2EE
2
ğ‘
EEğ‘¡ğ‘¡ 2(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13)
ğ‘¡2ğ‘¡
(cid:13)
(cid:13)(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)ğ‘ ğ‘11
âˆ‘ï¸
ğ‘–
âˆ‡ğ‘ğ‘ =1
1
ğ‘“âˆ‘ï¸ âˆ‘ï¸ğ‘– ğ‘–ğ‘=
=
(âˆ‘ï¸
Eğ‘–1
1
ğœƒğ‘
=
ğ‘¡
ğ‘¡(cid:104)
1âˆ‡
(cid:13) (cid:13)
(cid:13)âˆ‡
)(cid:104)
âˆ‡
(cid:13)
(cid:13)ğ‘“ âˆ‡ğ‘–
ğ‘“
2ğ‘“ğ‘–( ,ğ‘–ğ‘“(ğœƒ
ğ‘–
(ğœƒ
(ğ‘¡
ğœƒğ‘¡
ğœƒ) ğ‘¡)(cid:13) (cid:13) (cid:13) (cid:13)
ğ‘¡
))âˆ’ âˆ’âˆ’âˆ‡ âˆ‡âˆ‡ğ‘“ ğ‘“ğ‘“( (ğœƒ
(
ğœƒğ‘¡
ğœƒ
ğ‘¡)
ğ‘¡
))+
(cid:13) (cid:13)
(cid:13)(cid:105)
2âˆ‡
(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
+2ğ‘“
+
2( Eğœƒ 2ğ‘¡ ğ‘¡E)
(cid:13)
(cid:13)ğ‘¡(cid:105)
âˆ‡(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)(cid:13)
(cid:13) (cid:13) (cid:13)
(cid:13)
ğ‘“2
ğ‘1 (ğœƒâˆ‘ï¸ ğ‘–
ğ‘¡ğ‘
= )1
(cid:13)
(cid:13)2âˆ‡ğ‘“(ğœƒğ‘¡
)(cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(2
34)
ğ‘‡ 2 =â‰¤
â‰¤
ğ‘ğ‘ ğ‘11 222
2âˆ‘ï¸
ğ‘–âˆ‘ï¸
âˆ‘ï¸ğ‘– ğ‘–ğ‘
=ğ‘
ğ‘= =11 1ğ‘˜âˆ‘ï¸ğ‘˜
ğ‘˜âˆ‘ï¸ âˆ‘ï¸ğ»ğ»
ğ»== =11
1EE Eğ‘¡ğ‘¡
ğ‘¡(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)2ğ‘§2 2ğ‘§ ğ‘§ğœ‡ğ‘–â€²ğœ‡ ğœ‡ğ‘– ğ‘–â€² â€²(cid:16)( (cid:16)ğ¹ğ¹ ğ¹ğ‘–ğ‘– ğ‘–(( (ğœƒğœƒ ğœƒğ‘–ğ‘–
â€²
ğ‘–â€²
â€²+
++ +ğœ‡ğœ‡
(cid:16)
ğœ‡ğ¹ğ‘§ğ‘§
ğ‘§ğ‘–
ğ‘–â€²ğ‘– ğ‘–â€² â€²,,
(
,B
ğœƒB Bğ‘–â€²ğ‘–ğ‘–
â€²
ğ‘–â€²
,
â€²)) )Bâˆ’âˆ’
âˆ’ğ‘–â€²
)ğ¹ğ¹ ğ¹âˆ’ğ‘–ğ‘– ğ‘–((
(ğœƒ
ğ¹ğœƒ
ğœƒğ‘–
ğ‘–ğ‘–
â€²
ğ‘–â€²
â€²,
(
,âˆ’
B
ğœƒ
Bğ‘–â€²ğ‘–â€²
ğ‘–ğœ‡
â€²)
âˆ’
)ğ‘§
(cid:17)
(cid:17)ğ‘–â€² ğœ‡,
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)B
ğ‘§
2ğ‘–â€²ğ‘–â€² ,) B(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
ğ‘–2
â€² )(cid:17)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
w eqh ue ar le ityth ,e thfi er ss et ci on neq du ia nl eit qy uafo lil tl yow fos llf or wom sft rh oe mC Ja eu nc sh eny â€™- sSc inh ew qa ur az lii tn y-
,
+ ğ‘2
2
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» E ğ‘¡(cid:13) (cid:13) (cid:13) (cid:13)2ğ‘§ ğœ‡ğ‘–â€² (cid:16) ğ¹ ğ‘–(ğœƒ ğ‘–â€²,B ğ‘–â€² )âˆ’ğ¹ ğ‘–(ğœƒ ğ‘–â€² âˆ’ğœ‡ğ‘§ ğ‘–â€²,B ğ‘–â€² )(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)2 , (36)
andthethirdequalityisderivedfromAssumption5.
ğ‘–=1ğ‘˜=1 (cid:13) (cid:13)
Forthetermğ‘‡ 2,byapplyingJensenâ€™sinequality,weobtain: wheretheinequalityfollowsfromthefactthat(âˆ¥ğ‘+ğ‘âˆ¥)2 â‰¤2(âˆ¥ğ‘âˆ¥)2+
ğ‘ ğ» 2(âˆ¥ğ‘âˆ¥)2.Duetothesymmetryofthefunctionğ¹ ğ‘– whenperturbed
ğ‘‡ 2 â‰¤ ğ‘1 2 âˆ‘ï¸ ğ‘–=1ğ‘˜âˆ‘ï¸ =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13) (cid:13)2 , (35) w eqi uth ivG aa leu ns ts .ia Hn e- nd cis etr ğ‘‡i 2bu ct ae nd bğ‘§ eğ‘–â€² , tb rao nth sft oe rr mm es do in ntt oheright-handsideare
12ğ‘‡
2
â‰¤ ğ‘1
2
âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)ğ‘§ ğœ‡ğ‘–â€² (cid:16) ğ¹ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) +ğœ‡ğ‘§ ğ‘–â€²,B ğ‘–â€² )âˆ’ğ¹ ğ‘–(ğœƒ ğ‘–â€²,B ğ‘–â€² )(cid:17)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)2
ğ‘ (ğ‘¡,ğœ) =ğœ‚2 ğ‘1
âˆ‘ï¸ğ‘
Eğœ
ğ‘¡âˆ’1(cid:13)
(cid:13) (cid:13)
(cid:13)âˆ‘ï¸ğœ
ğ‘’
ğ‘–(ğ‘¡,ğ‘˜)(cid:13)
(cid:13) (cid:13)
(cid:13)2
= ğ‘21 Â·ğ‘‘2 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)ğ‘‘Â· ğœ‡ğ‘§ ğ‘–â€² (cid:16) ğ¹ ğ‘–(ğœƒ ğ‘–â€² +ğœ‡ğ‘§ ğ‘–â€²,B ğ‘–â€² )âˆ’ğ¹ ğ‘–(ğœƒ ğ‘–â€²,B ğ‘–â€² )(cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 â‰¤ğœğœ‚2âˆ‘ï¸ğœğ‘–=1 ğ‘1 âˆ‘ï¸ğ‘ (cid:13) Eğ‘˜ ğ‘˜ ğ‘¡= (cid:13) (cid:13) (cid:13)1 ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13) (cid:13)(cid:13) 2 . (41)
â‰¤ ğ‘21
Â·ğ‘‘2
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» (cid:34)
2ğ‘‘Â·E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ¹ ğ‘–(ğœƒ ğ‘–â€²,B ğ‘–â€² )(cid:13) (cid:13) (cid:13)2 + ğœ‡ 22
ğ¿2ğ‘‘2(cid:35) ğ‘˜=1 ğ‘–=1
ğ‘–=1ğ‘˜=1
ğ‘ ğ» (cid:34) (cid:35) BycombingEq.(37),Eq.(38)andEq.(41),wehave
â‰¤ ğ‘21
Â·ğ‘‘2
âˆ‘ï¸âˆ‘ï¸ 2ğ‘ ğ‘”ğ‘‘Â·E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–â€² )(cid:13) (cid:13) (cid:13)2 +2ğ‘‘ğœ ğ‘”2+ ğœ‡ 22 ğ¿2ğ‘‘2 ,
ğ‘–=1ğ‘˜=1
(37)
ğœ
ğ‘ (ğ‘¡,ğœ) â‰¤6ğ‘ ğ‘”ğ‘‘ğ¿2ğœğœ‚2âˆ‘ï¸ ğ‘ (ğ‘¡,ğ‘˜) +6ğ‘ ğ‘”ğ‘‘ğœ2ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2
wherethefirstinequalityfollowsLemma4.1from[21],andthe
ğ‘˜=1
secondinequalityfollowstheAssumption4.Fortheexpectation
ğœ‡2ğ¿2ğ‘‘2ğœ2ğœ‚2
term,wehave +2ğ‘‘ğœ ğ‘”2ğœ2ğœ‚2+ . (42)
2
E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 =E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )âˆ“âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ“âˆ‡ğ‘“(ğœƒ ğ‘–ğ‘¡ )(cid:13) (cid:13) (cid:13)2 Bytakingsummationoverğœ from2toğ»,andutilizingtheprop-
ertyofarithmeticsequence,weobtain
â‰¤3ğ¿2E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 +3E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 , (38)
where the inequality is due to the Cauchy-Schwartz inequality,
ğ» ğ» ğœ
(ğ¿ 3- 7s )m ,fio no ath llypr wo epe cr at ny ba on ud nA ds ğ‘‡s 2u am s:ption5.BytakingEq.(38)intoEq. ğœâˆ‘ï¸ =2ğ‘ (ğ‘¡,ğœ) â‰¤6ğ‘ ğ‘”ğ‘‘ğ¿2ğœ‚2 ğœâˆ‘ï¸ =2ğœ ğ‘˜âˆ‘ï¸ =1ğ‘ (ğ‘¡,ğ‘˜) +ğ¶
1
ğ»
ğ‘‡
2
â‰¤ 6 ğ‘ğ‘ ğ‘” 2ğ¿ ğ‘‘2 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2
whereğ¶ 1=2ğ‘
ğ‘”ğ‘‘ğ»3ğœ‚â‰¤ 2E3 ğ‘¡ğ‘
(cid:13)
(cid:13)ğ‘” âˆ‡ğ‘‘ğ» ğ‘“(2 ğœƒğ¿ ğ‘¡2 )ğœ‚
(cid:13)
(cid:13)2 2ğ‘˜âˆ‘ï¸ +=1 2ğ‘  ğ‘‘( ğœğ‘¡, ğ‘”2ğ‘˜ ğ») + 3ğœ‚ğ¶ 21 +, ğœ‡2ğ¿2ğ‘‘2ğ»3ğœ‚( 243 .)
+ 6 ğ‘ğ‘ ğ‘” ğ‘‘ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 + 2ğ» ğ‘ğœ ğ‘‘ğ‘”2 + ğœ‡2 2ğ» ğ‘ğ¿2 . (39) 3 6
Asğ‘ (ğ‘¡,1)
=0,afterrearrangingEq.(43),wehave
AftercombiningEq.(16),Eq.(34)andEq.(39),wehave
ğ»
E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’ ğ›¾2 ğœ‚E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 + 3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 (1âˆ’3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)âˆ‘ï¸ ğ‘ (ğ‘¡,ğ‘˜) â‰¤ğ¶ 1. (44)
ğ‘˜=1
+ 3ğ‘ ğ‘” ğ‘ğœğœ‚ 2ğ‘‘2ğ¿3 âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 +ğœ ğ‘”2ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ +ğœğœ‚2 4ğœ‡ ğ‘2ğ»ğ¿3
ğ‘–=1ğ‘˜=1 Forsimplification,herewedenote(1âˆ’3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)asğ¶ 0.When
=ğ‘“(ğœƒğ‘¡ )+(cid:32) 3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ âˆ’ ğ›¾2 Â·ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 ğœ‚ â‰¤ 3ğ»ğ¿âˆš1 ğ‘ğ‘”ğ‘‘,ğ¶ 0 â‰¥ 32.Underthiscondition,wehave:
+ 3ğ‘ ğ‘” ğ‘ğœğœ‚ 2ğ‘‘2ğ¿3 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 +ğœ ğ‘”2ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ +ğœğœ‚2 4ğœ‡ ğ‘2ğ»ğ¿3 .
ğ‘1
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ»
E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2
=âˆ‘ï¸ğ»
ğ‘ (ğ‘¡,ğ‘˜) â‰¤ ğ¶ ğ¶1
(40) ğ‘–=1ğ‘˜=1 ğ‘˜=1 0
(cid:32) (cid:33)
NextweneedtoboundE ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 andsimplifyEq.(40). = ğ¶1 0 2ğ‘ ğ‘”ğ‘‘ğ»3ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2+ 2 3ğ‘‘ğœ ğ‘”2ğ»3ğœ‚2+ ğœ‡2ğ¿2ğ‘‘ 62ğ»3ğœ‚2
Specifically,bydenoting ğ‘1 (cid:205) ğ‘–ğ‘ =1Eğ‘˜ ğ‘¡âˆ’1âˆ¥ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡âˆ¥2asğ‘ (ğ‘¡,ğ‘˜),we â‰¤3ğ‘ ğ‘”ğ‘‘ğ»3ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2+ğ‘‘ğœ ğ‘”2ğ»3ğœ‚2+ ğœ‡2ğ¿2ğ‘‘2ğ»3ğœ‚2 . (45)
have 4
13TakingEq.(45)intoEq.(40),weobtainthefinalresultofloss â–¡
descentofeachstep:
E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32) 3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ âˆ’ ğ›¾2 Â·ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 D.3 ProofofTheorem3.4
StartingfromEq.(16),undertheassumptionofglobal-localdissim-
+
3ğ‘ ğ‘”ğœğœ‚2ğ¿3 Â·ğ¶
1
+ğœ ğ‘”2ğœğœ‚2ğ»ğ¿ +ğœğœ‚2ğœ‡2ğ»ğ¿3 ilarity(Assumption6),ğ‘‡ 1becomes:
ğ‘ğ‘‘ ğ¶
0
ğ‘ğ‘‘ 4ğ‘
â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32) 3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ âˆ’ ğ›¾2 Â·ğœ‚+ 9ğ‘ ğ‘”2ğœğœ‚ ğ‘4ğ»3ğ¿3(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
ğ‘‡ 1=E
ğ‘¡(cid:13)
(cid:13) (cid:13) (cid:13)ğ‘1
âˆ‘ï¸ğ‘
âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡
)(cid:13)
(cid:13) (cid:13)
(cid:13)2
3ğ‘ ğ‘”ğœ ğ‘”2ğœğœ‚4ğ»3ğ¿3 3ğ‘ ğ‘”ğ‘‘ğœğœ‚4ğœ‡2ğ»3ğ¿5 ğœ ğ‘”2ğœğœ‚2ğ»ğ¿ ğœğœ‚2ğœ‡2ğ»ğ¿3 (cid:13) ğ‘–=1 (cid:13)
â‰¤+ ğ‘“(ğœƒğ‘¡ )+(cid:32)ğ‘ 3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»+ ğ¿ +3ğ‘ ğ‘”ğ‘‘4 ğœ‚ğ‘ 2ğ»2ğ¿23ğ‘+ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ‘ ğ¿ğ‘‘ (cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)+ âˆ‡ğ‘“(ğœƒ4 ğ‘¡ğ‘ )(cid:13) (cid:13) (cid:13)2 =E ğ‘¡(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)ğ‘1 âˆ‘ï¸ ğ‘–ğ‘ =1(cid:104) âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ’âˆ‡ğ‘“(ğœƒğ‘¡ )+âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:105)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2
(cid:13) ğ‘ (cid:13)2 (cid:13) ğ‘ (cid:13)2
âˆ’ 2 ğ›¾ğœ‚ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +3ğ‘ ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2Â· ğœ ğ‘”2ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ â‰¤2E ğ‘¡(cid:13) (cid:13) (cid:13) (cid:13)ğ‘1 âˆ‘ï¸ ğ‘–=1(cid:104) âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ’âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:105)(cid:13) (cid:13) (cid:13) (cid:13) +2E ğ‘¡(cid:13) (cid:13) (cid:13) (cid:13)ğ‘1 âˆ‘ï¸ ğ‘–=1âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13) (cid:13)
ğ‘
+3ğ‘ ğ‘”ğ‘‘ğœ‚2ğ»2ğ¿2Â· ğœğœ‚2ğœ‡2ğ»ğ¿3 +ğœ ğ‘”2ğœğœ‚2ğ»ğ¿ +ğœğœ‚2ğœ‡2ğ»ğ¿3 â‰¤ ğ‘2
2
âˆ‘ï¸ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ’âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2
4ğ‘ ğ‘ğ‘‘ 4ğ‘ ğ‘–=1
â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32) (1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)3ğ‘ ğ‘”ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ âˆ’ ğ›¾2 Â·ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘2
2
âˆ‘ï¸ğ‘ E ğ‘¡(cid:104) ğ‘ â„(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)+ğœ â„2(cid:105) +2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2
ğ‘–=1
+(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)ğœ ğ‘”2ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ +(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)ğœğœ‚2 4ğœ‡ ğ‘2ğ»ğ¿3 . = 2(ğ‘ ğ‘+ğ‘ â„) E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2+ 2 ğ‘ğœ â„2 , (50)
(46)
Withtheconditionğœ‚ â‰¤ âˆš1 ,wehave(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2) â‰¤2.
3ğ»ğ¿ ğ‘ğ‘”ğ‘‘ wherethefirstinequalityfollowsCauchy-Schwartz,thesecondin-
Thentakingtheconditionğœ‚ â‰¤ 3ğ‘ğ‘”ğ‘ ğ»ğ¿ andğœ‚ â‰¤ ğ»1 2,wecantransform equalityfollowsJensenâ€™sinequality,andthethirdinequalityfollows
Assumption6.
Eq.(46)intotheresultofTheorem3.1as:
(cid:32) (cid:33)
Thenwebegintoboundthetermğ‘‡ 2.TakingtheresultfromEq.
E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )+2 ğ‘‘ğœ ğœ‚âˆ’ ğ›¾1 ğœ‚ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 (37),wehave:
+
2ğœ ğ‘ğ‘”2 ğ»ğœğœ‚ ğ‘‘ğ¿ +ğœ 2ğœ‚ ğ‘ğœ‡2 ğ»ğ¿3
. (47) ğ‘‡
2
â‰¤ ğ‘21
Â·ğ‘‘2
âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» (cid:34)
2ğ‘ ğ‘”ğ‘‘Â·E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 +2ğ‘‘ğœ ğ‘”2+ ğœ‡ 22
ğ¿2ğ‘‘2(cid:35)
,
â–¡ ğ‘–=1ğ‘˜=1
(51)
D.2 ProofofCorollary(3.2) wherethefirstinequalityfollowsLemma4.1from[21],andthesec-
Togettheresultofglobalconvergence,werearrangeEq.(47)and ondinequalityfollowstheAssumption4.ThetermE ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2
similarlyboundğœ‚asğœ‚ â‰¤ 1 .Forsimplicity,wedenotes ğ‘‘âˆ’ğœğ›¾ asÎ“ inEq.(51)canbeboundedas:
ğ»2 ğ‘‘ğ›¾
andhave
2ğœ‚Î“E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) + 2 ğ‘ğœ ğ‘” ğ»2ğœ ğ‘‘ğ¿ ğœ‚+ğœ 2ğœ‡ ğ‘2ğ¿ ğ»3 ğœ‚ E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )(cid:13) (cid:13) (cid:13)2 =E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“ ğ‘–(ğœƒ ğ‘–(ğ‘¡,ğ‘˜) )âˆ“âˆ‡ğ‘“ ğ‘–(ğœƒğ‘¡ )âˆ“âˆ‡ğ‘“(ğœƒ ğ‘–ğ‘¡ )(cid:13) (cid:13) (cid:13)2
E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒğ‘¡)âˆ’E 2ğœ‚ğ‘¡ Î“(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) + Î“ğœ ğ‘ğ‘”2ğœ ğ»ğ¿
ğ‘‘
+ 4ğœ Î“ğœ‡ ğ‘2ğ¿ ğ»3 . â‰¤3ğ¿2E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 +3(ğ‘ â„+1)E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +3ğœ â„2, (52)
(48)
Noticethatğ›¾ =Î˜(ğ‘Ÿ)andğœ =Î˜( 1 ),sinceparameterğ‘‘isalarge
wheretheinequalityfollowstheCauchy-Schwartz,ğ¿-smoothand
ğ‘Ÿğ‘‘
Assumption6.ByapplyingEq.(52)intoEq.(51),weobtain:
number,ğ‘‘âˆ’ğœğ›¾ =ğ‘‘âˆ’Î˜(1) =ğ‘‘,hencethedominanttermofÎ“is
ğ‘‘
ğ›¾1,whichfollowsÎ“=Î˜( ğ‘Ÿ1).Thensimultaneouslysummingoverğ‘‡
rou ğ‘‡1n âˆ‘ï¸ ğ‘¡d ğ‘‡ =s 0o En ğ‘¡(cid:13)
(cid:13)
(cid:13)b âˆ‡ot ğ‘“h (ğœƒsi ğ‘¡d )e (cid:13)
(cid:13)
(cid:13)s 2a â‰¤nd ğ‘“t (a ğœƒk 0i )n âˆ’g 2ğœ‚t Eh ğ‘‡ğ‘¡e Î“(cid:2)a ğ‘“v (ğœƒer ğ‘‡a )g (cid:3)e:
+
Î“ğœ ğ‘ğ‘”2ğœ ğ»ğ¿
ğ‘‘
+
4ğœ Î“ğœ‡ ğ‘2ğ¿ ğ»3
(4.
9)
ğ‘‡ 2 â‰¤ 6 ğ‘ğ‘ ğ‘” 2ğ¿ ğ‘‘2 âˆ‘ï¸ ğ‘–ğ‘ =1ğ‘˜âˆ‘ï¸ğ» =1E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡, +ğ‘˜) 6âˆ’
ğ‘ ğ‘”
ğ‘ğœƒ ğœğ‘¡ ğ‘‘â„2(cid:13) (cid:13) (cid:13) ğ»2 + +6 2ğ‘
ğ»
ğ‘ğ‘”(
ğœ
ğ‘‘ğ‘ ğ‘”â„ 2ğ‘+ ğ‘‘ +ğ‘ ğœ‡2) 2ğ»ğ» ğ‘ğ¿E 2ğ‘¡ .(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ () 5(cid:13) (cid:13) (cid:13) 32
)
14CombiningEq.(16),Eq.(50)andEq.(53),wehave: Letğ¶ 0be(1âˆ’3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2).Whenğœ‚ â‰¤ 3ğ»ğ¿âˆš1 ğ‘ğ‘”ğ‘‘,ğ¶ 0 â‰¥ 2 3.Under
thiscondition,wehave:
E ğ‘¡(cid:2) + +ğ‘“( 3 3ğœƒ ğ‘ ğ‘ğ‘¡ ğ‘”
ğ‘”
ğ‘+ ğœ(1 ğ‘
ğœ‚
2) â„ ğ‘‘2(cid:3) ğ¿+ ğ‘â‰¤ 3ğ‘ ğ‘‘ âˆ‘ï¸ğ‘“ ğ‘)( ğœğœƒ ğœ‚ âˆ‘ï¸ğ‘¡ ğ»2) ğ»âˆ’ Eğ¿ğ›¾ ğ‘¡E
(cid:13)
(cid:13)
(cid:13)2 ğ‘ ğ‘¡ ğœƒ(cid:13) (cid:13) (cid:13) ğ‘–(( âˆ‡ ğ‘¡ğ‘ ,ğ‘˜ğ‘“ )(+ ğœƒ âˆ’ğ‘ ğ‘¡â„ ) ğœƒ(cid:13) (cid:13) (cid:13)) ğ‘¡ğœ‚ 2
(cid:13)
(cid:13)
(cid:13)E 2+ğ‘¡ +(cid:13) (cid:13) (cid:13) 3âˆ‡ ğ‘ ğœğ‘” ğ‘”ğ‘“ 2ğœ ğœ( â„ ğ‘2 ğ‘ğœƒ ğœ‚ğœğ‘¡ ğ‘‘2ğ‘‘ğœ‚) ğ»2(cid:13) (cid:13) (cid:13) ğ»2 ğ¿âˆ’ ğ¿ +ğ›¾ ğœ2 ğ‘ ğœ‚2ğœ 4ğœ‡â„2 ğ‘2ğœ‚
ğ»ğ¿3
ğ‘1 âˆ‘ï¸ â‰¤ğ‘–ğ‘ =1 3ğ‘ğ‘˜âˆ‘ï¸ğ» ğ‘”= (1 ğ‘E â„ğ‘¡ +(cid:13) (cid:13) (cid:13)ğœƒ 1ğ‘–( )ğ‘¡ ğ‘‘,ğ‘˜ ğœ‚) 2ğ»âˆ’ 3ğœƒ Eğ‘¡(cid:13) (cid:13) (cid:13) ğ‘¡(cid:13) (cid:13)2 âˆ‡ += ğ‘“ ğœğ‘˜âˆ‘ï¸ (ğ» ğ‘”2= ğœƒ ğ‘‘1 ğ‘¡ ğœ‚)ğ‘  (cid:13) (cid:13) 2(ğ‘¡ 2 ğ»,ğ‘˜ + 3) 3 +â‰¤ ğ‘ ğ‘” ğ‘‘ğ¶ ğ¶ ğœ 2â„ ğœ‚22 0 ğ‘‘ 2ğœ‚ ğœ‡2 2ğ» ğ»3
3ğ¿2
. (59)
ğ‘–=1ğ‘˜=1 4
=ğ‘“(ğœƒğ‘¡ )+(cid:32) 3ğ‘ ğ‘”(ğ‘ â„+ ğ‘ğ‘ ğ‘‘)ğœğœ‚2ğ»ğ¿ âˆ’ ğ›¾2
ğ‘
Â·(ğ‘ +ğ‘ â„)ğœ‚(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 Eq.L (e 5t 4(cid:101)ğ‘ )â„ ,wbe e( cğ‘ aâ„ n+ oğ‘ bt) aia nnd th(cid:101)ğœ e2 fib ne al(3 rğ‘ eğ‘” sğœ uâ„ l2 t+ oğœ fğ‘” s2 t) e. pA wp ip sl eyi ln og ssE dq e. s( c5 e9 n) ti :nto
+ 3ğ‘ ğ‘” ğ‘ğœğœ‚ 2ğ‘‘2ğ¿3 âˆ‘ï¸ğ‘ âˆ‘ï¸ğ» E ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 + 3ğ‘ ğ‘”ğœ â„2 ğ‘ğœ ğ‘‘ğœ‚2ğ»ğ¿ E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )+ 3ğœğœ‚2ğ¿ğ» ğ‘ğ‘ ğ‘” ğ‘‘(ğ‘ â„+ğ‘) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
ğ‘–=1ğ‘˜=1
+ğœ ğ‘”2ğœ ğ‘ğœ‚ ğ‘‘2ğ»ğ¿ +ğœğœ‚2 4ğœ‡ ğ‘2ğ»ğ¿3
âˆ’
ğ›¾2
ğ‘
Â·ğœ â„2ğœ‚. (54)
âˆ’ ğ›¾2
ğ‘
Â·ğœ‚(ğ‘ +ğ‘ â„)E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 + 3ğœğœ‚ ğ‘2ğ¿ ğ‘‘3ğ‘ ğ‘” Â· ğ¶ğ¶ 02
+
3ğœğœ‚2ğ‘ ğ‘”ğœ â„2ğ»ğ¿ +ğœğœ‚2ğ»ğœ ğ‘”2ğ¿ +ğœğœ‚2ğœ‡2ğ»ğ¿3
âˆ’
2
Â·ğœ‚ğœ2
SimilartoSectionD.1,nowweboundE ğ‘¡(cid:13) (cid:13) (cid:13)ğœƒ ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡(cid:13) (cid:13) (cid:13)2 asfollows: â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32)ğ‘ 3ğœğ‘‘ ğœ‚2 ğ‘ğ¿ğ» ğ‘‘ğ‘ ğ‘”(cid:101)ğ‘ â„ âˆ’ğ‘ 2ğ‘‘ ğ›¾ğœ‚ ğ‘(cid:101)ğ‘ â„ + 9ğœğ‘ ğ‘”4 2 (cid:101)ğ‘ğ‘ â„ ğ‘ğ»3ğ¿3ğœ‚ğ›¾ 4ğ‘ (cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡â„ ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
ğ‘ (cid:13) ğœ (cid:13)2
ğ‘ (ğ‘¡,ğœ) =ğœ‚2 ğ‘1 âˆ‘ï¸ ğ‘–=1Eğœ ğ‘¡âˆ’1(cid:13) (cid:13)
(cid:13)
(cid:13)ğ‘˜âˆ‘ï¸ =1ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13)
(cid:13) (cid:13) +
3ğ‘ ğ‘”(cid:101)ğœ2ğœ ğ‘ğ»3ğ¿3ğœ‚4
+
3ğ‘ ğ‘”ğœğ‘‘ğœ‡ 42 ğ‘ğ»3ğ¿5ğœ‚4 +(cid:101)ğœ2ğœ ğ‘ğ» ğ‘‘ğ¿ğœ‚2
â‰¤ğœğœ‚2âˆ‘ï¸ğœ ğ‘1 âˆ‘ï¸ğ‘ Eğ‘˜ ğ‘¡(cid:13) (cid:13) (cid:13)ğ‘’ ğ‘–(ğ‘¡,ğ‘˜)(cid:13) (cid:13) (cid:13)2 , (55) +ğœğœ‚2 4ğœ‡ ğ‘2ğ»ğ¿3 âˆ’ ğ›¾2 ğ‘ Â·ğœ‚ğœ â„2
whereğ‘ (ğ‘¡,ğ‘˜) denotes ğ‘1 (cid:205) ğ‘–ğ‘
=ğ‘˜ 1= E1
ğ‘˜
ğ‘¡âˆ’1âˆ¥ğ‘–= ğœƒ1
ğ‘–(ğ‘¡,ğ‘˜) âˆ’ğœƒğ‘¡âˆ¥2.
â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32) (1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)3ğœğœ‚2 ğ‘ğ¿ğ» ğ‘‘ğ‘ ğ‘”(cid:101)ğ‘ â„(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
BycombingEq.(51),Eq.(52)andEq.(55),wehave âˆ’ ğ›¾2 ğ‘ğœ‚ (cid:101)ğ‘ â„E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 +3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2 ğ‘(cid:101)ğœ2 ğ‘‘ğœğ»ğ¿ğœ‚2âˆ’ ğ›¾2 ğ‘ğœ‚ğœ â„2
ğœ
ğ‘ (ğ‘¡,ğœ) â‰¤6ğ‘ ğ‘”ğ‘‘ğ¿2ğœğœ‚2âˆ‘ï¸ ğ‘ (ğ‘¡,ğ‘˜) +6ğ‘ ğ‘”ğ‘‘(ğ‘ â„+1)ğœ2ğœ‚2E ğ‘¡(cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2 +3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2ğœğœ‚2ğœ‡2ğ»ğ¿3 + (cid:101)ğœ2 ğœğ»ğ¿ğœ‚2+ğœğœ‚2ğœ‡2ğ»ğ¿3
ğ‘˜=1
4ğ‘ ğ‘ğ‘‘ 4ğ‘
+6ğ‘ ğ‘”ğ‘‘ğœ â„2ğœ2ğœ‚2+2ğ‘‘ğœ ğ‘”2ğœ2ğœ‚2+ ğœ‡2ğ¿2ğ‘‘2ğœ2ğœ‚2 . (56) â‰¤ ğ‘“(ğœƒğ‘¡ )+(cid:32) (1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)3ğœğœ‚2 ğ‘ğ¿ğ» ğ‘‘ğ‘ ğ‘”(cid:101)ğ‘ â„ âˆ’ 2 ğ›¾ğœ‚ ğ‘(cid:101)ğ‘ â„(cid:33) E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
2
ğœ2
Bytakingsummationoverğœ from2toğ»,andutilizingtheprop- +(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)Â· (cid:101) ğœğ»ğ¿ğœ‚2
ğ‘ğ‘‘
ertyofarithmeticsequence,weobtain
ğœğœ‚2ğœ‡2ğ»ğ¿3 2
ğ» ğ» ğœ
+(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)Â·
4ğ‘
âˆ’
ğ›¾ğ‘
Â·ğœ‚ğœ â„2. (60)
âˆ‘ï¸ ğ‘ (ğ‘¡,ğœ) â‰¤6ğ‘ ğ‘”ğ‘‘ğ¿2ğœ‚2âˆ‘ï¸ ğœâˆ‘ï¸ ğ‘ (ğ‘¡,ğ‘˜) +ğ¶
2 Withtheconditionğœ‚ â‰¤ âˆš1 ,wehave(1+3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2) â‰¤2.
ğœ=2 ğœ=2 ğ‘˜=1 3ğ»ğ¿ ğ‘ğ‘”ğ‘‘
â‰¤3ğ‘
ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2âˆ‘ï¸ğ»
ğ‘ (ğ‘¡,ğ‘˜) +ğ¶ 2 (57)
Takingğœ‚ â‰¤ 3ğ‘ğ‘”ğ‘ ğ»ğ¿,ğœ‚ â‰¤ ğ»1 2,Eq.(60)becomes:
(cid:32) (cid:33)
where ğ¶ 2=2ğ‘ ğ‘”ğ‘‘(ğ‘ â„+1)ğ»3ğœ‚2Eğ‘˜ ğ‘¡= (cid:13) (cid:13)1 âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13)2 E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) â‰¤ ğ‘“(ğœƒğ‘¡ )+2 ğ‘‘ğœ âˆ’ ğ›¾1 ğ‘ (cid:101)ğ‘ â„ğœ‚E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2
+2ğ‘ ğ‘”ğ‘‘ğœ â„2ğ»3ğœ‚2+ 2 3ğ‘‘ğœ ğ‘”2ğ»3ğœ‚2+ ğœ‡2ğ¿2ğ‘‘ 62ğ»3ğœ‚2 . + 2 (cid:101)ğœ ğ‘2 ğ»ğœğ¿ ğ‘‘ğœ‚ +ğœ 2ğœ‚ ğ‘ğœ‡2 ğ»ğ¿3 âˆ’ ğ›¾2
ğ‘
Â·ğœ‚ğœ â„2. (61)
â–¡
RearrangingEq.(57)andusingğ‘ (ğ‘¡,1)
=0,wehave
D.4 ProofofCorollary(3.2)
ğ»
(1âˆ’3ğ‘ ğ‘”ğ‘‘ğ»2ğ¿2ğœ‚2)âˆ‘ï¸ ğ‘ (ğ‘¡,ğ‘˜) â‰¤ğ¶ 2. (58) Denote(cid:101)Î“= ğ‘‘ ğ‘‘âˆ’ ğ›¾ğ‘ ğ‘ğ›¾ğœ .RearrangingEq.(61),simultaneouslysumming
ğ‘˜=1 overğ‘‡ roundsonbothsidesandtakingtheaverage,wegetthe
15result: Fed-Alpaca:TheAlpacadataset[46]isdesignedforLLMfine-
2(cid:101)Î“ (cid:101)ğ‘ â„ğœ‚E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒğ‘¡ )âˆ’E ğ‘¡(cid:2)ğ‘“(ğœƒğ‘¡+1)(cid:3) vtu an rii en tg ya on fd Nf Le Pat tu ar se ks sn ,sa utu cr ha al sla tn eg xu ta gg ee neq ru ae ts ioti no ,n ts raa nn sd lar te is op no ,n as ne ds ofo pr ena
2ğœ2ğœğ¿ğœ‚ ğœğœ‚ğœ‡2ğ¿3 2 QA.Itspansvariousdomainslikemath,textprocessing,andcode
+ (cid:101) + âˆ’ Â·ğœ‚ğœ2
ğ‘ğ»ğ‘‘ 2ğ‘ğ» ğ›¾ğ‘ â„ generation.
E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“(ğœƒğ‘¡)âˆ’ 2E
(cid:101)Î“
(cid:101)ğ‘ğ‘¡ â„(cid:2) ğœ‚ğ‘“(ğœƒğ‘¡+1)(cid:3) WE. e2 impE lex mp ene tr oim ure apn pt ra ol acP hel sat uf so inr gm Pys
Torch[42]v1.10.1,coupled
ğœ2ğœğ¿ ğœğœ‡2ğ¿3 ğœ2 with PEFT v0.3.0 and the Transformers library [51] v4.29.2. Ex-
+ (cid:101) + âˆ’ â„
(cid:101)Î“ (cid:101)ğ‘ â„ğ‘ğ»ğ‘‘ (cid:101)Î“ (cid:101)ğ‘ â„4ğ‘ğ» (cid:101)Î“ (cid:101)ğ‘ â„ğ›¾ğ‘ p eqe uri im ppe en dts ww iti hth foL uL raM NVA I- D3B IAar Ae 1c 0o 0n Gdu Pc Ut sed (4o 0n Ga B)c ,o wm itp hu pte rep -l ta rt af io nr em
d
ğ‘‡1 âˆ‘ï¸ ğ‘¡ğ‘‡ =0E ğ‘¡(cid:13) (cid:13) (cid:13)âˆ‡ğ‘“(ğœƒğ‘¡ )(cid:13) (cid:13) (cid:13)2 â‰¤ ğ‘“( 2ğœƒ (cid:101)Î“0 (cid:101)ğ‘) â„âˆ’ ğœ‚ğ‘‡ğ‘“âˆ— +
(cid:101)Î“
(cid:101)ğ‘(cid:101)ğœ â„2 ğ‘ğœğ¿
ğ»ğ‘‘
L EL .3Mslo Dad ee fd auas l1 t6 I- mbit pfl lo ea mtin eg n-p to ain tt ion num Sb ee trs t.
ings
ğœğœ‡2ğ¿3 ğœ2
+ âˆ’ â„ . (62) Followingtheguidelinesin[29,36],allapproachesperformlocal
(cid:101)Î“ (cid:101)ğ‘ â„4ğ‘ğ» (cid:101)Î“ (cid:101)ğ‘ â„ğ›¾ğ‘
trainingwithabatchsizeof1tominimizememoryusage.Inan
â–¡ efforttostandardizetheexperimentalconditions,bothbackprop-
agation(BP)-basedmethodsandourproposedmethodFedMeZO,
E IMPLEMENTATIONDETAILS trainlocallywithspecificlearningrates:ğœ‚ =1Ã—10âˆ’5fortheFed-
Inthissection,weprovidethedetailedimplementationsofourex-
DollyandFed-Alpacadatasets,ğœ‚ =2Ã—10âˆ’5fortheFed-CodeAlpaca
periments.Someexperimentalsettingshavealreadybeendiscussed dataset,andğœ‚ =2.5Ã—10âˆ’5fortheFed-GSM8K dataset.Therank
inSection5.1andwillnotbereiteratedhere. andalphaparametersforLow-RankAdaptation(LoRA)adapters
usedbybothBP-basedoptimizationandFedMeZOaresetto128
E.1 DatasetsandEvaluationMetrics and256,respectively.Asper[36],theperturbationscaleğœ‡forFed-
MeZOissetto1Ã—10âˆ’3.Inourtrainingprocess,weimplemented
earlystoppingtopreventoverfittingandreducetrainingtime.The
Name #Sample Domain
trainingwasstoppediftherewasnoimprovementinthevalidation
Fed-Alpaca 52.0k GenericLanguage lossforapre-definednumberofconsecutiveepochs,knownasthe
Fed-Dolly 15.0k GenericLanguage patienceparameter.Wechoseapatienceof30epochsbasedon
Fed-GSM8K 7.5k CoT empiricalevidenceorpriorstudies.Thebestmodelwasselected
Fed-CodeAlpaca 8.0k CodeGeneration fromtheepochwiththelowestvalidationloss.
Theinfluenceofdifferenthyper-parametersforFedMeZOhas
Table3:DatasetsandBasicInformation
beenanalyzedinSection5.
WeadoptseveralfederatedtuningdatasetstailoredforLLMs F SUPPLEMENTARYEXPERIMENTS
from[29],withdifferentsplittingstrategiestosimulatethehet-
F.1 TheImpactofPerturbationScaleğœ‡ on
erogeneitytypicalofdifferentfederatedlearning(FL)scenarios,
includingauniformdistributionofdata,aDirichletdistributionof convergence
dataandasplitterbasedonmeta-information. Wepresentallresultofimpactonğœ‡inFigure5.
Fed-Dolly:Thisfederatedcorpusdataset,derivedfromDatabricks-
dolly-15k[12],compriseseightcategoriesofNLPtasks:brainstorm- F.2 TheImpactofLocalIterations
ing,classification,closedQA,creativewriting,generalQA,infor- Wepresentallresultofimpactonğ» inFigure6.
mationextraction,openQA,andsummarization.Wedividethe
trainingsetintothreesubsetsusingathree-waysplitandassign F.3 TheImpactofHeterogeneityon
eachsubsettoadistinctclient.
Convergence
Fed-GSM8K:ConstructedfromtheGSM8K dataset[11],this
WepresentallresultofimpactonHeterogeneityinFigure7.
collection is aimed at mathematical fine-tuning and consists of
7.5Ktrainingproblemsalongside1Ktestproblems.Bydefault,we
F.4 TheImpactofClientNumber
partitionthetrainingsetuniformlyintothreesubsetsandallocate
eachtoaseparateclient. WepresentallresultofimpactonHeterogeneityinFigure7.
Fed-CodeAlpaca:ThisfederatedversionofCodeAlpaca[5]en-
compassescodesamplesintenprogramminglanguages,including
C,C#,C++,Go,Java,PHP,Pascal,Python,Scala,andX86-64Assem-
bly.DuetothescarcityofX86-64Assemblysamplesintheoriginal
corpus,weexcludethem.Theremainingsamplesarethendivided
intothreesubsetsusingadefaultthree-waysplit,withonesubset
assignedtoeachclient.
16Dolly-IID Dolly-LDA Dolly-Meta Dolly-IID Dolly-LDA Dolly-Meta
=1e 3 =1e 3 =1e 3 H=30 2.05 H=30 1.9 H=30
=5e 3 =5e 3 =5e 3 H=10 H=10 H=10
2.00 =2e 4 2.00 =2e 4 1.8 =2e 4 2.0 H=50 2.00 H=50 H=50
1.8
1.95 1.95 1.95
1.90 1.90 1.7 1.9 1.90 1.7
1.85 1.85
1.85 1.6 1.6
1.80 1.8 1.80
1.80
1.75 1.5 1.75 1.5
1.75 1.7 1.70
1.70
1.4
1.70 1.4 1.65
1.65 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
0 50 100R15o0u20n0d250s300350400 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
Code-IID Code-LDA Code-Meta
Code-IID Code-LDA Code-Meta 1.16 H=30 H=30 H=30
1.16 =1e 3 =1e 3 =1e 3 H H= =1 50 0 H H= =1 50 0 0.89 H H= =1 50 0
=5e 3 =5e 3 0.89 =5e 3 1.15 0.92
1.15 =2e 4 =2e 4 =2e 4
0.92 1.14
0.88
1.14 0.88 0.90
1.13
1.13 0.90 0.87
0.87 1.12 0.88
1.12
0.88 1.11 0.86
1.11 0.86 1.10 0.86
1.10 0.86 1.09 0.85
0.84
0.85
1.09 1.08
0.84 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500
1.08
0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 Alpaca-IID GSM-IID
1.0
H=30 H=30
Alpaca-IID GSM-IID
1.0
H H= =1 50
0
H H= =1 50
0
= =1 5e e 3 3 1.25 = =1 5e e 3 3 2.2 1.2 0.8
2.2 =2e 4 1.20 =2e 4
0.8 2.0
1.15 1.1 0.6
2.0
1.10
0.6 1.8
0.4 1.05 1.0
1.8
1.00 0.4 1.6
0.2
0.9
0.95
1.6
0.90 0.2 1.4 0 100 R2o00und300s 400 500 0 100 R2o00und300s 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0
1.4 0.85
0 50 100Ro150un20d0s250 300 350 0 100 R2o00und300s 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0 Figure6:DifferentH performancedraft
Figure5:Differentğœ‡performancedraft
F.5 ConvergenceAnalysisofallDatasetsand
Splitter
WepresentalltheexperimentalresultsfromSection5.2,asshown
inFigure9. Figure7:Differentsplitterperformancedraft
F.6 RequirementsfortheLearningRate(May
thelargerthelearningrate,theearlierthissharpincreaseoccurs.
bejudged,canputintoappendix)
Thisindicatesthatinourstudiedmethod,excessivelylargelearning
To verify that an appropriate learning rate is a prerequisite for ratesareinappropriate.
personalizedlearningrates,weattemptedtoconductexperiments
withavarietyoflargerlearningrates:
Figure10showsthatwhenthelearningrateexceedstherange
supportedbytheory,thelossfunctionexhibitsasharpincrease,and
17
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoLDolly-IID Dolly-LDA Dolly-Meta
2.05 2.05
BP-based SGD BP-based SGD BP-based SGD
FedMeZO FedMeZO FedMeZO
2.00 2.00
1.8
1.95 1.95
1.90 1.90 1.7
1.85 1.85
1.6
1.80 1.80
1.75 1.75 1.5
1.70 1.70
1.4
1.65 1.65
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds Figure10:Phenomenonoflosssurgeduetolargerlearning
Code-IID Code-LDA Code-Meta ratesdraft
1.16
BP-based SGD BP-based SGD BP-based SGD
FedMeZO FedMeZO 0.89 FedMeZO
1.15
0.92
1.14
0.88
1.13 0.90
0.87
1.12
1.11 0.88
0.86
1.10
0.86
0.85
1.09
1.08
0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500
Rounds Rounds Rounds
Alpaca-IID GSM-IID
1.0
BP-based SGD 1.25 BP-based SGD
FedMeZO FedMeZO
2.2
1.20
0.8
1.15
2.0
1.10 0.6
1.05
1.8
1.00 0.4
1.6 0.95
0.2
0.90
1.4
0.85
0 100 200 300 400 500 0 100 200 300 400 500 0.00.0 0.2 0.4 0.6 0.8 1.0
Rounds Rounds
Figure9:Mainresultoflossdraft
18
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL
ssoL