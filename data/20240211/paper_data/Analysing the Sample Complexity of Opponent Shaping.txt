Analysing the Sample Complexity of Opponent Shaping
KittyFungâˆ— QizhenZhangâˆ— ChrisLu
UniversityofOxford UniversityofOxford UniversityofOxford
Oxford,UnitedKingdom Oxford,UnitedKingdom Oxford,UnitedKingdom
kittyfung01@gmail.com qizhen.zhang@eng.ox.ac.uk christopher.lu@exeter.ox.ac.uk
JiaWan TimonWilli JakobFoerster
MassachusettsInstituteofTechnology UniversityofOxford UniversityofOxford
Cambridge,UnitedStates Oxford,UnitedKingdom Oxford,UnitedKingdom
jiawan@mit.edu timon.willi@eng.ox.ac.uk jakob.foerster@eng.ox.ac.uk
Abstract Keywords
Learningingeneral-sumgamesoftenyieldscollectivelysub-optimal OpponentShaping;Multi-Agent;ReinforcementLearning;Meta
results.Addressingthis,opponentshaping(OS)methodsactively ReinforcementLearning;SampleComplexity
guidethelearningprocessesofotheragents,empiricallyleading
toimprovedindividualandgroupperformancesinmanysettings. ACMReferenceFormat:
EarlyOSmethodsusehigher-orderderivativestoshapethelearning KittyFung,QizhenZhang,ChrisLu,JiaWan,TimonWilli,andJakobFo-
ofco-players,makingthemunsuitabletoshapemultiplelearning erster.2024.AnalysingtheSampleComplexityofOpponentShaping.In
steps. Follow-up work, Model-free Opponent Shaping (M-FOS), Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMul-
addressesthesebyreframingtheOSproblemasameta-game.In tiagentSystems(AAMAS2024),Auckland,NewZealand,May6â€“10,2024,
contrasttoearlyOSmethods,thereislittletheoreticalunderstand- IFAAMAS,18pages.
ingoftheM-FOSframework.Providingtheoreticalguaranteesfor
M-FOSishardbecauseA)thereislittleliteratureontheoretical 1 Introduction
samplecomplexityboundsformeta-reinforcementlearningB)M-
Learningingeneral-sumgamescommonlyleadstocollectively
FOSoperatesincontinuousstateandactionspaces,sotheoretical
worst-caseoutcomes[6].Toaddressthis,opponentshaping(OS)
analysisischallenging.Inthiswork,wepresentR-FOS,atabular
methodsaccountforopponentsâ€™learningstepsandinfluenceother
versionofM-FOSthatismoresuitablefortheoreticalanalysis.R-
agentsâ€™learningprocesses.Empirically,thiscanimproveindividual
FOSdiscretisesthecontinuousmeta-gameMDPintoatabularMDP.
andgroupperformances.
WithinthisdiscretisedMDP,weadapttheğ‘… algorithm,most
ğ‘šğ‘ğ‘¥
prominentlyusedtoderivePAC-boundsforMDPs,asthemeta- EarlyOSmethods[6,10,12]relyonhigher-orderderivatives,which
learnerintheR-FOSalgorithm.Wederiveasamplecomplexity arehigh-varianceandresultinunstablelearning.Theyarealso
boundthatisexponentialinthecardinalityoftheinnerstateand myopic,focusingonlyontheopponentâ€™simmediatefuturelearning
actionspaceandthenumberofagents.Ourboundguaranteesthat, stepsratherthantheirlong-termdevelopment[14].Recentwork,
withhighprobability,thefinalpolicylearnedbyanR-FOSagent Model-freeOpponentShaping(M-FOS)[14],solvestheabovechal-
isclosetotheoptimalpolicy,apartfromaconstantfactor.Finally, lenges.M-FOSintroducesameta-gamestructure,eachmeta-step
weinvestigatehowR-FOSâ€™ssamplecomplexityscalesinthesizeof representinganepisodeoftheembeddedâ€œinnerâ€game.Themeta-
state-actionspace.Ourtheoreticalresultsonscalingaresupported stateconsistsofâ€œinnerâ€policies,andthemeta-policygeneratesan
empiricallyintheMatchingPenniesenvironment. innerpolicyateachmeta-step.M-FOSusesmodel-freeoptimisation
techniquestotrainthemeta-policy,eliminatingtheneedforhigher-
orderderivativestoaccomplishlong-horizonopponentshaping.
âˆ—EqualContribution TheM-FOSframeworkhasshownpromisinglong-termshaping
WorkdoneattheFoersterLabForAIResearch(FLAIR),UniversityofOxford resultsinsocial-dilemmagames[9,14].
CorrespondingAuthors:kittyfung01@gmail.com,qizhen.zhang@eng.ox.ac.uk
TheoriginalM-FOSpaperpresentstwocasesoftheM-FOSalgo-
rithm.Forsimpler,low-dimensionalgames,M-FOSlearnspolicy
updatesdirectlybytakingpoliciesasinputandoutputtingthenext
policyasanaction.Inputtingandoutputtingentirepoliciesdoes
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. notextendwelltomorecomplex,higher-dimensionalgames,e.g.
whenpoliciesarerepresentedasneuralnetworks.Theoriginal
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems M-FOSpaperalsoproposesavariantwhichusestrajectoriesas
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6â€“10,2024, inputsinsteadoftheexactpolicyrepresentations.Inthisworkwe
Auckland,NewZealand.Â©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). derivethesamplecomplexityforbothcases.
4202
beF
8
]GL.sc[
1v28750.2042:viXraWhereassomepreviousOSalgorithmsenjoystrongtheoretical betweentheoryandexperimentsbydemonstratingthatinboth
foundationsthankstotheDifferentiableGamesframework[1],the realms,samplecomplexityscalesexponentiallywiththeinner-
M-FOSframeworkhasnotbeeninvestigatedtheoretically.Under- gameâ€™sstate-actionspacesize.
standingthesamplecomplexityofanalgorithmishelpfulinmany
ways,suchasevaluatingitsefficiencyorpredictingthelearning 2 RelatedWork
time.However,providingtheoreticalguaranteesforM-FOSischal- TheoreticalAnalysisofDifferentiableGames:Muchpastwork
lengingbecauseA)thereisverylittleliteratureontheoreticalsam- assumesthatthegamebeingoptimisedisdifferentiable[1].This
plecomplexityboundsforevensingle-agentmeta-reinforcement assumptionenablesfareasiertheoreticalanalysisbecauseonecan
learning(RL),letalonemulti-agentandB)M-FOSoperatesincon- directlyuseend-to-endgradient-basedmethodsratherthanrein-
tinuousstateandactionspace(themeta-game). forcementlearninginthosesettings.Severalworksinthisarea
Inthiswork,wepresentR-FOS,atabularalgorithmapproximation investigatetheconvergencepropertiesofvariousalgorithmsBal-
ofM-FOS.UnlikeM-FOS,whichoperatesinacontinuousmeta- duzzietal.[1],Letcher[11],SchÃ¤ferandAnandkumar[17].
MDP,R-FOSoperatesinadiscreteapproximationoftheoriginal
OpponentShaping:Morecloselyrelatedtoourworkaremethods
meta-MDP.TheresultingdiscreteMDPallowsustoperformrigor-
thatspecificallyanalyseOS.SOS[12]andCOLA[21]bothanalyse
oustheoreticalanalysis.WeadaptR-FOSfromM-FOSsuchthatit
opponent-shapingmethodsthatoperateinthedifferentiablegames
stillmaintainsallthekeypropertiesofM-FOS.Withinthisdiscrete,
framework.Theseworksprovidetheoreticalconvergenceanalysis
approximateMDP,R-FOSappliestheğ‘… algorithm[8,20]tothe
MAX foropponent-shapingalgorithms;however,neitherworkanalyzes
M-FOSmeta-game.ğ‘… isamodel-basedreinforcementlearning
MAX samplecomplexity.POLA[23]theoreticallyanalysesanOSmethod
(MBRL)algorithmtypicallyusedforthesamplecomplexityanalysis
thatisinvarianttopolicyparameterization.M-FOSdoesnotoperate
oftabularMDPs.Usingexistingresultsdevelopedforğ‘… [3],
MAX inthedifferentiablegamesframework.WhilethisenablesM-FOS
wederiveanexponentialsamplecomplexityPAC-bound,which
toscaletomorechallengingenvironments,suchasCoinGame[14],
guaranteeswithhighprobability(1-ğ›¿)thattheoptimalpolicyinthe
itcomesatthecostofconvenienttheoreticalanalysis.Khanetal.
discretisedmeta-MDPisveryclose(<ğœ–away)tothepolicylearned
[9]empiricallyscalesM-FOStomorechallengingenvironments
byR-FOS.Wethenderiveseveralboundswhichguaranteepolicies
withlargerstatespaces,whileLuetal.[15]empiricallyinvestigates
betweentheoriginalmeta-MDPandthediscretisedmeta-MDPare
applying M-FOS to a state-based adversary. To the best of our
closetoeachotheruptoaconstantdistance.Lastly,combiningall
knowledge,ourworkisthefirsttotheoreticallyanalyseOSoutside
ofthepreviousboundswederived,weobtainthefinalexponential
ofthedifferentiablegamesframework.Furthermore,ourworkis
samplecomplexityresult.
thefirsttoanalysethesamplecomplexityofanOSmethod.
Fornotationalsimplicity,wemostlyomittheâ€œmetaâ€prefixinthe
TheoreticalAnalysisofSampleComplexityinRL:Thereare
restofthepaper.Forexample,thetermsâ€œMDPâ€,â€œtransitionfunc-
severalworksthatusetheğ‘… [3]frameworktoderivethesample
MAX
tionâ€,andâ€œpolicyâ€,refertothemeta-MDP,meta-transitionfunction,
complexityofRLalgorithmsacrossavarietyofsettings.Closely
andmeta-policyrespectively.Weusetheprefixâ€œinnerâ€whenever
related to our work is Zhang et al. [22], which uses the ğ‘…
MAX
werefertotheinnergame.Furthermore,ouranalysisofM-FOS
algorithmtoderivesamplecomplexityboundsforlearninginfully-
islimitedtotheasymmetricshapingcase(i.e.themeta-gameof
cooperativemulti-agentRL.
shapinganaiveinner-learnerâˆ—)andweleavetheextensiontometa-
selfplayforfuturework. Ourworkisalsorelatedtomethodsthatanalysesamplecomplexity
oncontinuous-spaceRL.Analyzingthesamplecomplexityofalgo-
Ourcontributionsarethree-fold:
rithmsincontinuous-spaceRLisparticularlychallengingbecause
(1) WepresentR-FOS(seeAlgorithm1),atabularapproximationof thereareaninfinitenumberofpotentialstates.Toaddressthis,
M-FOS.Insteadoflearningameta-policyinsidethecontinuous numeroustechniqueshavebeensuggestedthateachmakespecific
meta-MDPğ‘€,R-FOSlearnsameta-policyinsideadiscretised assumptions:LiuandBrunskill[13]assumesastationaryasymp-
meta-MDPwhichapproximatesğ‘€.InsidethisdiscretisedMeta- toticoccupancydistributionunderarandomwalkintheMDP.
MDP,R-FOSusesğ‘… asthemeta-agent.NotethatR-FOS Maliketal.[16]usesaneffectiveplanningwindowtohandleMDPs
MAX
stillmaintainskeypropertiesoftheoriginalM-FOSalgorithm, withnon-lineartransitions.However,neitheroftheseassumptions
suchasbeingabletoexploitnaivelearners. appliestoM-FOS.
(2) Wepresentanexponentialsamplecomplexityboundforboth Instead,thisworkfocusesondiscretisingthecontinuousspace
casesdescribedinM-FOS(SeeTheoremsG.2andG.3).Specif- andexpressesthecomplexityboundsintermsofthediscretisation
ically,weprovethat,withhighprobability,thefinalR-FOS gridsize.Thisisrelatedtotheconceptofstateaggregation[2,19],
policyisclosetotheoptimalpolicyintheoriginalmeta-MDP which groups states into clusters and treats the clusters as the
uptoaconstantdistance. statesofanewMDP.Thesepreviousworksonlyformulatedthe
aggregationsettinginMDPsanddidnotprovidetheoreticalor
(3) WeimplementR-FOSâ€ andanalysetheempiricalsamplecom-
empiricalsamplecomplexityproofs.
plexityintheMatchingPenniesenvironment.Weestablishlinks
Furthermore,priorstudiesonPAC-MDPdidnotempiricallyverify
âˆ—Naivelearnersareplayerswhoupdatetheirpolicyassumingotherlearningagents
theconnectionbetweenthesamplecomplexityandsizeofthestate-
aresimplyapartoftheenvironment.
â€ Theprojectcodeisavailableonhttps://github.com/FLAIROx/rfos actionspace.Inthiswork,weempiricallyverifytherelationshipbetweenthesamplecomplexityandthecardinalityofthe 3.2 MarkovDecisionProcess
inner-state-action-spaceintheMatchingPenniesgame. AMarkovdecisionprocess(MDP)isaspecialcaseofstochastic
gameandcanbedescribedasM = âŸ¨ğ‘†,ğ´,ğ‘‡,ğ‘…,ğ›¾âŸ©,whereğ‘† isthe
Algorithm1TheR-FOSAlgorithm s futa nt ce tis op na ,c ğ‘…e, (ğ´
ğ‘ 
ğ‘¡i ,s
ğ‘
ğ‘¡th )e isac thti eon res wpa ac rde, fğ‘‡ un(cid:0)ğ‘  cğ‘¡ t+ io1 n| ,ğ‘  ğ‘¡ a, nğ‘ dğ‘¡(cid:1) ğ›¾is isth the etr da in scsi ot uio nn
t
Meta-gameInputs:Discretisedmeta-MDPâŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ© factor.Ateachtimestepğ‘¡,theagenttakesanactionğ‘
ğ‘¡
âˆˆğ´froma
ğ‘š-known,meta-gamehorizonâ„meta stateğ‘ 
ğ‘¡
âˆˆğ‘†andmovestoanextstateğ‘  ğ‘¡+1âˆ¼ğ‘‡ (cid:0)Â·|ğ‘  ğ‘¡,ğ‘ ğ‘¡(cid:1).Then,the
Inner-gameInputs:InnergameG=âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ© agentreceivesarewardğ‘Ÿ ğ‘¡ =ğ‘…(ğ‘  ğ‘¡,ğ‘ ğ‘¡).
inner-gamehorizonâ„
inner
Initialisation:âˆ€ğ‘ Ë† ğ‘‘ âˆˆğ‘†Ë† ğ‘‘,ğ‘Ë† ğ‘‘ âˆˆğ‘†Ë† ğ‘‘,ğ‘ Ë† ğ‘‘â€² âˆˆğ‘†Ë† ğ‘‘ 3.3 Model-FreeOpponentShaping
ğ‘„Ë† (ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)â†0, ğ‘Ÿ(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)â†0, ğ‘›(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)â†0, ğ‘›(ğ‘ Ë† ğ‘‘,ğ‘Ë†,ğ‘ Ë†â€²)â†0 Model-freeOpponentShaping(M-FOS)[14]framestheOSproblem
1: formeta-episode=0,1,..do asameta-reinforcement-learningproblem,inwhichtheopponent
2: Resetenvironment shaperplaysameta-game.Themeta-gameisanMDP(sometimes
3: formeta-timestep=1,2,...,â„metado wealsorefertoitasmeta-MDP),inwhichthemeta-agentcontrols
4: Chooseğ‘Ë† ğ‘‘ :=argmax ğ‘Ë† ğ‘‘âˆˆğ´Ë† ğ‘‘ğ‘„Ë† (ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘â€²) oneoftheinneragentsintheinnergame.
5: Roll-outğ¾ innergamesoflengthâ„ innerusingğ‘Ë† ğ‘‘ =ğœ™ ğ‘¡ Theinnergameistheactualenvironmentthatouragentsareplay-
6: Inner-game opponents each update their own inner- ing,whichisanSG.TheoriginalM-FOSdescribestwocasesfor
policiesnaively
themeta-state:
7: Letğ‘…beouragentâ€™sğ¾ inner-gamesâ€™discountedreturn
8: Letğ‘ Ë†â€² bethenextmeta-stateafterexecutingmeta-action (1) Inthemeta-gameattimestepğ‘¡,theM-FOSagentisatthemeta-
ğ‘‘
ğ‘Ë† ğ‘‘ frommeta-stateğ‘ Ë† ğ‘‘ stateğ‘ Ë† ğ‘¡ = [ğœ™ ğ‘¡ğ‘– âˆ’1,ğ“ ğ’•âˆ’ âˆ’ğ’Š 1],whichcontainsallinner-agentsâ€™policy
9: ifğ‘›(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) <ğ‘šthen parametersfortheunderlyingSG.Inthiswork,weassu,eall
10: ğ‘Ÿ(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)â†ğ‘Ÿ(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘¡)+ğ‘… inner-agentsareparameterisedbytheirQ-valuetable.
1 11 2: : ğ‘› ğ‘›( (ğ‘  ğ‘ Ë† Ë†ğ‘‘ ğ‘‘, ,ğ‘ ğ‘Ë† Ë†ğ‘‘ ğ‘‘,) ğ‘ Ë†â† â€² ğ‘‘)ğ‘› â†(ğ‘ Ë† ğ‘‘ ğ‘›,ğ‘ (Ë† ğ‘ Ë†ğ‘¡ ğ‘‘) ,+ ğ‘Ë† ğ‘‘1 ,ğ‘ Ë† ğ‘‘â€²)+1 (2) A inl nte er rn -ga ati mve ely re, pğ‘ Ë† ğ‘¡ res= enğ‰ ttin hec pa ose lis ciw esh se ure ffip cia es nt tt lyra .jectoriesofthe
13: ifğ‘›(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)=ğ‘šthen
ğ‘™ğ‘›( 1 ) Weprovidetheoreticalsamplecomplexityresultsforbothofthese
14: forğ‘– =1,2,3,Â·Â·Â·,âŒˆ 1ğœ€ âˆ’(1 ğ›¾âˆ’ğ›¾) âŒ‰do
twocases
15: forall(ğ‘ Ë†,ğ‘Ë†)do
16: ifğ‘›(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) â‰¥ğ‘šthen Themeta-agenttakesameta-actionğ‘Ë† ğ‘¡ =ğœ™ ğ‘¡ğ‘– âˆ¼ğœ‹ ğœƒ(Â·|ğ‘ Ë† ğ‘¡),whichisthe
17: ğ‘„Ë† (ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) â† ğ‘…Ë† ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) + M-FOSâ€™inneragentâ€™spolicyparameters.Theactionischosenfrom
ğ›¾(cid:205) ğ‘  ğ‘‘â€² ğ‘‡Ë† ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)max ğ‘Ë† ğ‘‘â€² ğ‘„Ë† (ğ‘ Ë† ğ‘‘â€²,ğ‘Ë† ğ‘‘â€²) themeta-policyğœ‹ parameterizedbyparameterğœƒ.Inthiswork,we
18: ğ‘ Ë†â†ğ‘ Ë†â€² onlylookatthecasewherethemeta-policyisaQ-valuefunction
table,andisdenotedasğ‘„Ë†instead.TheM-FOSagentreceivesreward
ğ‘Ÿ ğ‘¡ = (cid:205) ğ‘˜ğ¾ =0ğ‘Ÿ ğ‘˜ğ‘–(ğœ™ ğ‘¡ğ‘–,ğ“ ğ’•âˆ’ğ’Š),whereğ¾ isthenumberofinnerepisodes.
Anewmeta-stateissampledfromastochastictransitionfunction
3 Background
ğ‘ Ë† ğ‘¡+1âˆ¼ğ‘‡(Â·|ğ‘ Ë† ğ‘¡,ğ‘Ë† ğ‘¡).
3.1 StochasticGame
Notethattheoriginalpaperintroducestwodifferentalgorithms:
Astochasticgame(SG)â€¡isgivenbyatupleğº = âŸ¨I,ğ‘†,ğ‘¨,ğ‘‡,ğ‘¹,ğ›¾âŸ©. Thefirstmeta-trainsM-FOSagainstnaivelearnerscommonlyre-
I = {1,Â·Â·Â·,ğ‘›}isthesetofagents,ğ‘† isthestatespace,ğ‘¨isthe sultinginexploitingthem.Thesecondinsteadconsidersmeta-self-
cross-productoftheactionspaceforeachagentsuchthatthejoint play,wherebytwoM-FOSagentsaretrainedtoshapeeachother,
actionspaceğ‘¨ =ğ´1Ã—Â·Â·Â·Ã—ğ´ğ‘›,ğ‘‡ :ğ‘† Ã—ğ‘¨ â†¦â†’ğ‘† isthetransition resultinginreciprocity.Inthisworkweonlyconsiderthefirst,
function,ğ‘¹isthecross-productofrewardfunctionsforallagents asymmetriccase.
suchthatthejointrewardspaceğ‘¹=ğ‘…1Ã—Â·Â·Â·Ã—ğ‘…ğ‘›,andğ›¾ âˆˆ [0,1)
isthediscountfactor. 3.4 Theğ‘… Algorithm
MAX
ğ‘… [3]isanMBRLalgorithmproposedforanalysingthesample
In an SG, agents simultaneously choose an action according to MAX
theirstochasticpolicyateachtimestepğ‘¡,ğ‘ğ‘– âˆ¼ğœ‹ğ‘– (Â·|ğ‘ ğ‘–).Thejoint complexityfortabularMDPs.GivenanyMDPğ‘€,ğ‘… MAXconstructs
ğ‘¡ ğœ™ğ‘– ğ‘¡ anempiricalMDPğ‘€Ë† thatapproximatesğ‘€.Theapproximationis
actionattimestepğ‘¡ isğ’‚ğ’• = {ğ‘ğ‘– ğ‘¡,ğ’‚ ğ’•âˆ’ğ’Š},wherethesuperscriptâˆ’ğ’Š donebyestimatingtherewardfunctionğ‘…andtransitionğ‘‡ using
indicatesallagentsexceptagentğ‘–andğœ™ğ‘– isthepolicyparameter
empiricalsamples.Theresultingapproximaterewardandtransition
of agentğ‘–. The agents then receive rewardğ‘Ÿ ğ‘¡ğ‘– = ğ‘…ğ‘–(ğ‘  ğ‘¡,ğ’‚ğ’•) and modelsaredenotedbyğ‘…Ë†andğ‘‡Ë†respectively.
observethenextstateğ‘  ğ‘¡+1 âˆ¼ğ‘‡(Â·|ğ‘  ğ‘¡,ğ’‚ğ’•),resultinginatrajectory
ğœğ‘– =(ğ‘ 0,ğ’‚0,ğ‘Ÿ 0ğ‘–,...,ğ‘  ğ‘‡,ğ’‚ ğ‘»,ğ‘Ÿ ğ‘‡ğ‘–),whereğ‘‡ istheepisodelength. ğ‘… MAXencouragesexplorationbydividingthestate-actionpairsinto
twogroups-thosethathavebeenvisitedatleastğ‘š-times,andthose
thathavenâ€™t.Thesetofstate-actionpairsthathavebeenvisitedat
â€¡Weusetheboldnotationtoindicatevectorsoverğ‘›agents. leastğ‘š-timesiscalledtheâ€œğ‘š-knownsetâ€.UsingtheempiricalMDPğ‘€Ë†,theğ‘… MAXalgorithmconstructsanğ‘š-knownempiricalMDP.This meta-MDPğ‘€
ğ‘‘
=âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©.Wefirstdiscretisethecon-
ğ‘š-knownempiricalMDPbehavesalmostexactlyastheempirical tinuousmeta-statespaceandmeta-actionspaceusingepsilon-
MDP,exceptwhentheagentisatastate-actionpairoutsidethe nets[5,7].Basedonthisdiscretisedmeta-stateandmeta-action
ğ‘š-known set. When the agent is outside theğ‘š-known set, the space,wedefinethediscretisedtransitionandrewardfunction.
transitionfunctionisself-absorbing(i.e.thetransitionfunction SeeSection4.2fordetails.
onlytransitionsbacktothecurrentstate)andtherewardfunction
(2) Wethenconstructağ‘š-knowndiscretisedMDPğ‘€ ,ğ‘‘,asde-
isthemaximum(SeeTable3intheappendix).Theconsequence ğ‘š
scribedbytheR-MAXalgorithm[20].SeeSection4.3fordetails.
oftheğ‘š-knownsetupistheagentisencouragedtoexplorestate-
actionpairsthathavehighuncertainty(i.e.thathasbeenvisited (3) Then, we deploy the ğ‘… algorithm in ğ‘€ ,ğ‘‘. ğ‘… both
MAX ğ‘š MAX
underğ‘štimes).Specifically,thevaluefunctionfortheunder-visited estimatestheempiricalğ‘š-knowndiscretisedMDP,ğ‘€Ë† ,ğ‘‘,using
ğ‘š
statesisthemaximumpossibleexpectedreturn,whichgivesthe
amaximumlikelihoodestimatefromempiricalsamplesand
algorithm its name. This is in line with optimism in the face of learnsanoptimalpolicyinğ‘€Ë† ,ğ‘‘.Forexample,toestimatethe
ğ‘š
uncertainty.
meta-reward,ouralgorithm,R-FOSevaluatestheinner-game
policyoutputtedbythemeta-policyusingepisodicrollouts.The
3.5 ğœ€-Nets estimatesarethenusedtoupdatethemeta-policyaccording
Definition3.1. (ğœ€-Net[5,7])Forğœ€ >0,Nğœ€ isanğœ€-netovertheset totheR-FOSalgorithm.OurR-FOSalgorithmoptimistically
Î˜âŠ†Rğ·ifforallğœƒ âˆˆÎ˜,thereexistsğœƒâ€² âˆˆNğœ€suchthat(cid:13) (cid:13)ğœƒâˆ’ğœƒâ€²(cid:13)
(cid:13)2
â‰¤ğœ€. assignsrewardsforallunder-visiteddiscretised(meta-state,
meta-action)pairstoencourageexploration(likeğ‘… ).See
MAX
Section4.4fordetails.
To discretise ağ·-dimensional sphere of radiusğ‘…, we can use a
ğœ€-netcontainingğ·-dimensionalcubesofsidesğœ†.Thisresultsin (4) WenextproveaPAC-boundwhichguaranteeswithlargeprob-
(cid:16)2ğ‘… +1(cid:17)ğ· points. Within each ğ·-dimensional cube, the largest ability,thattheoptimalpolicylearntinğ‘€Ë† ğ‘š,ğ‘‘issimilartothe
ğœ†
optimalpolicyinğ‘€,ğ‘‘.Thisstepusesresultsfrom[20].See
distancebetweentheverticesandtheinteriorpointscomesfrom
âˆš Section4.5fordetails.
thecenterofthecube,whichis ğœ† ğ‘‘.Therefore,toguaranteeafull
2
coverofallthepointsinthesphere,thelargestcubesizethatwe (5) Wealsoproveastrictboundthatguaranteestheoptimalpolicies
âˆš
canhaveshouldsatisfyğœ€ = ğœ† ğ‘‘.Fromhereon,wewillreplacethe learntinğ‘€,ğ‘‘andğ‘€aresimilaruptoaconstant.Thisstepuses
2
ğœ€inğœ€-netwithğ›¼ toavoidnotationoverloading. resultsfrom[4].SeeSection4.8fordetails.
(6) Usingthetwoboundsfromabove,weprovethefinalsample
4 SampleComplexityAnalysiswithğ‘… MAX as complexityguaranteewhichquantifiesthat,withlargeproba-
Meta-Agent bility,theoptimalpolicylearntinğ‘€Ë† ğ‘š,ğ‘‘issimilartotheoptimal
As introduced in Section 3, ğ‘… [3] is a MBRL algorithm for policyinğ‘€uptoaconstant.SeeSection4.9fordetails.
MAX
learningintabularMDPs.WeadapttheoriginalM-FOSalgorithm
touseğ‘… asthemeta-agent(seeAlgorithm1)andrefertothis 4.1 Assumptions
MAX
adapted algorithm as R-FOS from here on. We use a tabular Q- Wefirstoutlineallassumptionsmadeinderivingthesamplecom-
learnerasthenaivelearnerforallinner-gameopponents.While plexityoftheR-FOSalgorithm.
theoriginalM-FOSpaperusesPPO[18],wechoosetheQ-learner
fortheeaseofsamplecomplexityanalysis.
Assumption4.1. Bothmeta-gameandinner-gamearefinitehori-
WeprovidetheoreticalresultsforthetwocasesofM-FOSâ€™meta-
zon.Weuseâ„metatodenotethemeta-gamehorizon,andâ„ innerto
denotetheinner-gamehorizon.
agentproposedbytheoriginalpaper[14].CaseI usesallagentsâ€™
innerpolicyparametersfromtheprevioustimestepasthemeta-
state.CaseII insteadusesthemostrecentinner-gametrajectories Assumption4.2. Weassumetheinner-gamerewardisbounded.
asthemeta-state.Inbothcases,themeta-actiondeterminesthe Forsimplicityoftheproofandwithoutlossofgenerality,weset
inneragentâ€™spolicyparametersforthenextinnerepisode. thisboundas â„in1 ner,whereâ„ inneristhehorizonoftheinnergame.
Atahighlevel,wefirstdiscretisethemeta-MDP,whichallowsus
Formally,forall(ğ‘ ,ğ‘),0 â‰¤ ğ‘… inner(ğ‘ ,ğ‘) â‰¤ â„in1 ner.Thisallowsusto
tousethetheoreticalboundsfromğ‘… (onlysuitablefortabular
introducethenotionofmaximuminnerrewardandmaximuminner
MDPs),thenwedeveloptheoryforM boA uX ndingthediscrepancybe- valuefunctionasğ‘… max,inner= â„in1
ner
andğ‘‰ max,inner=1respectively.
Thisimpliesthattherewardandvaluefunctioninthemeta-game
tweenthecontinuousanddiscretemeta-MDP,andlastly,weuse
allofthistoboundthefinaldiscrepancy.Specifically,thesample arealsobounded,i.e.,ğ‘…max =1andğ‘‰max = 1âˆ’1 ğ›¾ (thelatterbeing
complexityanalysisconsistsofsixstepsÂ§: anupperbound).
(1) Touseğ‘… astheM-FOSmeta-agent,wefirstdiscretisethe
continuoM uA sX meta-MDP ğ‘€ = âŸ¨ğ‘†Ë†,ğ´Ë†,ğ‘‡,ğ‘…,ğ›¾âŸ© into a discretised A simss pu lm icip tyti oo fn th4 e.3 p. rT ooh fe ,tm he et ia n-g na em r-ge au mse es ua sed sis aco du isn ct of ua nc tto fr aco tf oğ›¾ r. oF fo 1r .
Thisassumptioncanbeeasilydeletedbyadaptingğ‘… (see
max,inner
Â§seedetailedproofintheappendix above)intheoriginalproofin[20].Assumption4.4. Forsimplicity,theinnergameisassumedtobe 4.2.2 DiscretisingtheStateandActionSpace:CaseII
discrete.
InCaseII,themeta-stateğ‘ Ë† isallinneragentsâ€™pasttrajectories.
ğ‘¡
Assumption4.5. Themeta-rewardfunctionisLipschitz-continuous: Formally,ğ‘ Ë† ğ‘¡ := ğ‰ğ‘¡., whereğœ ğ‘¡ğ‘– = {ğ‘ 0,ğ‘0,ğ‘ 1,ğ‘1,...,ğ‘  ğ‘¡,ğ‘ ğ‘¡}. Because
Forallğ‘ Ë† 1,ğ‘ Ë†
2
âˆˆğ‘†Ë†andğ‘Ë† 1,ğ‘Ë†
2
âˆˆğ´Ë†, we assume the Inner-Game is discrete (i.e. the state and action
spacearebothdiscrete),themeta-stateinthiscasedoesnotneed
(cid:12) (cid:12)ğ‘…(ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’ğ‘…(ğ‘ Ë† 2,ğ‘Ë† 2)(cid:12) (cid:12)â‰¤Lğ‘…(cid:13) (cid:13)(ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’(ğ‘ Ë† 2,ğ‘Ë† 2)(cid:13) (cid:13)
âˆ
discretisation.Letâ„bethemaximumlengthofthepasttrajectories
whereLğ‘… isthemeta-rewardfunctionâ€™sLipschitz-constant. combined,i.e.â„=â„ innerÂ·â„meta.Thesizeofthemeta-statespaceis
Assumption4.6. Themeta-transitionfunctionisLipschitz-continuous:
|ğ‘†Ë† ğ‘¡|=(|ğ‘†||ğ´|)ğ‘›â„. (4)
Forallğ‘ Ë† 1,ğ‘ Ë† 1â€²,ğ‘ Ë† 2 âˆˆğ‘†Ë†andğ‘Ë† 1,ğ‘Ë† 1â€²,ğ‘Ë† 2 âˆˆğ´Ë†, Themeta-actionremainsthesameasCaseI.
(cid:12) (cid:12)ğ‘‡(ğ‘ Ë† 1â€² |ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’ğ‘‡(ğ‘ Ë† 2â€² |ğ‘ Ë† 2,ğ‘Ë† 2)(cid:12) (cid:12)â‰¤Lğ‘‡(cid:13) (cid:13)(ğ‘ Ë† 1â€²,ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’(ğ‘ Ë† 2â€²,ğ‘ Ë† 2,ğ‘Ë† 2)(cid:13)
(cid:13)
âˆ
whereLğ‘‡ isthemeta-transitionfunctionâ€™sLipschitz-constant. 4.2.3 DiscretisingtheTransitionandRewardFunction
Undertheabovediscretisationprocedure,wedefinethediscretised
A pis ns gu bm etp wti eo en n4 m.7 e. taT -sh taer teeâ€™ ss pa aL ceip as nch dit mz- ec to an -at cin tiu oo nu ss pp ao ci en st u-t co h-s te ht am tfa op r- MDPğ‘€
ğ‘‘
=(ğ‘†Ë†,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾),wherethestatespaceremainscontin-
anyğ‘ Ë†,ğ‘ Ë†â€² âˆˆ ğ‘†Ë†andğ‘Ë†,ğ‘Ë†â€² âˆˆ ğ´Ë†,thereexistssomeğ‘Ë† âˆˆğ‘ˆ(ğ‘ Ë†) suchthat
uousandtheactionspaceisrestrictedtodiscretised actions.We
definethetransitionfunctionandrewardfunctionforğ‘€ as:
(cid:13) (cid:13)ğ‘Ë†âˆ’ğ‘Ë†â€²(cid:13)
(cid:13)
<L(cid:13) (cid:13)ğ‘ Ë†âˆ’ğ‘ Ë†â€²(cid:13)
(cid:13)
ğ‘‘
âˆ âˆ
A iss asu pm rop bt ai bo in lit4 y.8 d. enT sh ite ym fue nta c- tg ioa nme sut cra hn ts hit ai ton 0fu â‰¤nc ğ‘‡ti (o ğ‘ Ë†n
â€²
ğ‘‡ |( ğ‘ Â· Ë†,| ğ‘Ë†ğ‘  )Ë†,ğ‘Ë† â‰¤) ğ‘‡ ğ‘‘(ğ‘ Ë†â€² |ğ‘ Ë†,ğ‘Ë† ğ‘‘)= âˆ«
ğ‘†Ë†ğ‘‡ ğ‘‡( (ğ‘  ğ‘§Ë† Ë†ğ‘‘â€²
ğ‘‘
| |ğ‘  ğ‘ Ë† Ë†ğ‘‘ ğ‘‘, ,ğ‘ ğ‘Ë† Ë†ğ‘‘ ğ‘‘)
)ğ‘‘ğ‘§Ë† (5)
Land âˆ« ğ‘†Ë†ğ‘‡(ğ‘ Ë†â€² |ğ‘ Ë†,ğ‘Ë†)ğ‘‘ğ‘ Ë†â€² =1, âˆ€ğ‘ Ë†,ğ‘ Ë†â€² âˆˆğ‘†Ë†andğ‘Ë†âˆˆğ´Ë† Intuitively,ğ‘‡ ğ‘‘(ğ‘ Ë†â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)isanormalizedsampleofğ‘‡ ğ‘‘(Â·|Â·,ğ‘Ë† ğ‘‘)at
ğ‘ Ë†â€²,ğ‘ Ë† ğ‘‘,andthetransitionprobabilityğ‘‡ ğ‘‘(Â· |ğ‘ Ë†,ğ‘Ë† ğ‘‘)takesaconstant
ThefirstfourassumptionsarerequiredtobeabletousetheR-MAX valuewithineachgridinthestatespace.Thismeansthatinstead
algorithm,whilethelatterassumptionsareneededforboundingthe oftreatingthetransitionfunctionasadiscretiseddistributionofall
discrepancybetweenthecontinuousmeta-MDPandthediscretised possiblevaluesofğ‘†Ë† ,wetreatitasacontinuousdistributionover
ğ‘‘
meta-MDP. theoriginalcontinuousstatespace,butnormalizeeachgridfrom
theğœ€-netintoastepfunction.
4.2 Step1:DiscretisingtheMeta-MDP
Touseğ‘… astheM-FOSmeta-agent,wediscretisethecontinuous
ğ‘… ğ‘‘(ğ‘ Ë†,ğ‘Ë† ğ‘‘)=ğ‘…(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) (6)
MAX
meta-MDPğ‘€ = âŸ¨ğ‘†Ë†,ğ´Ë†,ğ‘‡,ğ‘…,ğ›¾âŸ©intoadiscretisedmeta-MDPğ‘€ ğ‘‘ = Similarly,therewardfunctioniscontinuousoverthestatespace,
âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©.Wediscretisethecontinuousstateandaction butnormalizedeachgridfromtheğœ€-netintoastepfunction.
spaceusingğœ€-netswithspacingğ›¼.
4.3 Step2:Theğ‘š-knownDiscretisedMDP
4.2.1 DiscretisingtheStateandActionSpace:CaseI Inthepreviousstep,weconvertedthemeta-MDPğ‘€intoadiscre-
In Case I, the meta-stateğ‘ Ë† ğ‘¡ is all inner agentsâ€™ policies parame-
tisedmeta-MDPğ‘€ ğ‘‘.Fromğ‘€ ğ‘‘,R-FOSbuildsanğ‘š-knowndiscre-
tisedMDPğ‘€ (seeTable3intheappendix).
tersfromtheprevioustimestep.Eachoftheinneragentğ‘–â€™spolicy ğ‘š,ğ‘‘
is a Q-table, denoted asğœ™ ğ‘– âˆˆ R|ğ‘†|Ã—|ğ´|. Formally,ğ‘ Ë† ğ‘¡ := ğ“ğ‘¡âˆ’1 =
[ğœ™ ğ‘¡ğ‘– âˆ’1,ğ“ ğ’•âˆ’ âˆ’ğ’Š 1].Themeta-actionğ‘Ë† ğ‘¡ istheinneragentâ€™scurrentpolicy Definition4.9(m-KnownMDP). Letğ‘€ ğ‘‘ = âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©be
parametersğœ™ğ‘–. anMDP.Wedefineğ‘€ ğ‘š,ğ‘‘tobetheğ‘š-knownMDP.Asisstandard
ğ‘¡
practice,ğ‘š-knownreferstothesetofstate-actionpairsthathave
Forthemeta-actionspaceğ´Ë†andachosendiscretisationerrorğ›¼ >0, beenvisitedatleastğ‘štimes.Forallstate-actionpairsinğ‘š-known,
w
ğ‘Ë†
ğ‘‘e âˆˆob ğ´Ë†t ğ‘‘ai wn hth ere eğœ€-netğ´Ë† ğ‘‘ âŠ‚ ğ´Ë†suchthatforallğ‘Ë† âˆˆ ğ´Ë†,thereexist t ph ae iri sn od uu tc se idd eM ofD ğ‘šP -ğ‘€ knğ‘š o, wğ‘‘ nb ,e th ha ev se ts atid e-e an ct ti ic oa nl pto aiğ‘€ rsğ‘‘ a. rF eo sr els ft -a at be s- oa rc bti io nn
g
(cid:13) (cid:13)ğ‘Ë†âˆ’ğ‘Ë† ğ‘‘(cid:13) (cid:13)â‰¤ğ›¼. (1) (i.e.onlyself-transitions)andmaximallyrewardingwithğ‘…ğ‘€ğ´ğ‘‹.
Dividingthespacewithgridsizeğœ†resultsinthesizeofdiscretised
meta-actionspaceupperboundedby 4.4 Step3:TheEmpiricalDiscretisedMDP
(cid:32) 2âˆšï¸ |ğ‘†||ğ´| (cid:33)|ğ‘†||ğ´| Fromtheğ‘š-knowndiscretisedMDPğ‘€ ğ‘š,ğ‘‘,wethenlearnanempir-
|ğ´Ë† ğ‘‘| â‰¤
ğœ†
+1 . (2) icalğ‘š-knowndiscretisedMDPğ‘€ ğ‘š,ğ‘‘ bycalculatingthemaximum
likelihoodfromempiricalsamples(seeTable3intheappendix).As
showninAlgorithm1,R-FOSlearnsanoptimalpolicywithinthis
Similarly, the size of the discretised meta-state space is upper
empiricalğ‘š-knowndiscretisedMDP.
boundedby
|ğ‘†Ë† ğ‘‘| â‰¤
(cid:32) 2âˆšï¸ğ‘› ğœ†|ğ‘†||ğ´| +1(cid:33)ğ‘›|ğ‘†||ğ´|
. (3) D the efi en xpit ei co tn ed4 v.1 e0 rs( iE om np oi fr ğ‘€ic Ë†alm w-K hn eo rew :ndiscretisedMDP). ğ‘€ ğ‘š,ğ‘‘ is
ğ‘š,ğ‘‘1âˆ’ğ›¿,ğ‘‰ ğ‘€âˆ—
ğ‘‘
(ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€A ğ‘‘ğ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€istrueforallbut
ğ‘‡ ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):=(cid:40) ğ‘‡ 1ğ‘‘ [ğ‘ ( Ë†ğ‘  ğ‘‘â€²Ë† ğ‘‘â€² =| ğ‘ ğ‘  Ë†Ë† ğ‘‘ğ‘‘, ]ğ‘ ,Ë† ğ‘‘) i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known
ğ‘‚Ëœ(cid:169)
(cid:173)
(cid:173)(cid:18) 2âˆš ğ‘› ğœ†|ğ‘†||ğ´| +1(cid:19)2ğ‘›|ğ‘†||ğ´| (cid:18) 2âˆš |ğ‘† ğœ†||ğ´| +1(cid:19)|ğ‘†||ğ´|
(cid:170)
(cid:174)
(cid:174)
ğ‘‡Ë† ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):=ï£±ï£´ï£´ï£² ï£´ï£´1ğ‘› ğ‘›( [ğ‘  ğ‘ Ë† Ë†(ğ‘‘ ğ‘‘ğ‘  â€²Ë†, ğ‘‘ğ‘Ë† =,ğ‘‘ ğ‘Ë†, ğ‘‘ğ‘  ğ‘ Ë† Ë†ğ‘‘ )â€² ğ‘‘) ],
,
i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known (cid:173) (cid:173) (cid:173)
(cid:171)
ğœ€3(1âˆ’ğ›¾)6 (cid:174) (cid:174) (cid:174)
(cid:172)
ï£³ timesteps.
(cid:40)
ğ‘… ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):= ğ‘… ğ‘…ğ‘‘ m( ağ‘ Ë† xğ‘‘,ğ‘Ë† ğ‘‘), i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known
4.7 CaseII
ğ‘…Ë† ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)=ï£±ï£´ï£´ï£²(cid:205)ğ‘› ğ‘–(ğ‘ Ë†ğ‘‘ ğ‘›,ğ‘ (Ë† ğ‘ ğ‘‘ Ë† ğ‘‘) ,ğ‘ğ‘Ÿ Ë† ğ‘‘(ğ‘ Ë† )ğ‘‘,ğ‘Ë† ğ‘‘), if(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) âˆˆm-known D obir tae ic ntl ty hepl fu og llg inin gg PAin CE -bq ou ua nti do .ns 11 and 9 into Theorem G.1, we
ï£´ï£´ ï£³ğ‘…max, otherwise
(7) Theorem4.13. Supposethat0 â‰¤ ğœ€ < 1âˆ’1 ğ›¾ and0 â‰¤ ğ›¿ < 1are
two real numbers. Let M be any continuous meta-MDP with in-
4.5 Step4:TheBoundBetweenğ‘€ ğ‘‘ andğ‘€Ë† ğ‘š,ğ‘‘ n âŸ¨ğ‘†e Ë† ğ‘‘r ,s ğ´t Ë†o ğ‘‘c ,h ğ‘‡ ğ‘‘as ,t ğ‘…i ğ‘‘c ,g ğ›¾a âŸ©m ae sğº disc= reâŸ¨ tğ‘† is, eğ´ d,ğ‘‡ vi en rn se ir o, nğ‘… i on fne ğ‘€râŸ©. (aL set deu ss crd ie bn eo dte inğ‘€ Cğ‘‘ as= e
W pre obfi ar bs it lip tyr ,o tv he et oh pe tP imA aC lpb oo lu icn id esw leh ai rc nh tg inua tr ha en dte ise cs reth tia st e, dw Mit Dh Phi ğ‘€gh I) using grid size of ğœ†. There exists inputsğ‘š = ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,
ğ‘‘
andempiricalğ‘šâˆ’ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›discretisedMDPareveryclose.Weprove satisfying
theboundusingresultsfrom[20].
ğ‘š(cid:18)1 ,1(cid:19)
=ğ‘‚Ëœ
(cid:32) (|ğ‘†||ğ´|)ğ‘›â„(cid:33)
ğœ€ ğ›¿ ğœ€2(1âˆ’ğ›¾)4
T a Mnh Dde Po 0 .r Tâ‰¤e hm eğ›¿ re4 <. e1 x11 is. a tsr(ğ‘… e inM t pwA uoX tsrM ğ‘šeaD l =P n ğ‘šB uo m (cid:16)u ğœ€1bn ,ed r ğ›¿1[ s2 (cid:17)a0 an] n) d dS ğ‘€ ğœ€u 1p ,=p so aâŸ¨s tğ‘†e is,t fğ´h ya , inğ‘‡t g0 ,ğ‘… ğ‘šâ‰¤ ,ğ›¾ (cid:16)ğœ€ âŸ© ğœ€1< i ,s ğ›¿1a1 (cid:17)âˆ’ n1 ğ›¾ =y ğ‘ša atn ad tin mğœ€ d1 1 eğœ€= ğ‘¡1, ağ‘‚ t nh d(cid:16) enğœ€1
ğ‘ Ë†
ğ‘¡(cid:17) t, h ds eeu nfc ooh tl elt oh tw ha i et n si gf tağ‘… h to e-ğ‘€ l ad tsğ´ . tğ‘‹ L imei t es A ğ‘¡e .x ğ‘¡ Wec du ie tt n he od pte ro on ğ‘… b- ağ‘€ ğ‘€ biğ´w liğ‘‹ ti yt â€™h s ai p tn o lp elu i act sys
t
ğ‘‚(cid:18) (ğ‘†+ln ğœ€2(ğ‘† (1ğ´ âˆ’/ ğ›¾ğ›¿ )) 2)ğ‘‰ m2 ax(cid:19) and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘… MAXisexecuted 1âˆ’ğ›¿,ğ‘‰ ğ‘€âˆ— ğ‘‘ (ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€A ğ‘‘ğ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€ (cid:18)is âˆštrueforall (cid:19)b |ğ‘†u |t
|ğ´|
onğ‘€withinputsğ‘šandğœ€1,thefollowingholds.Letğ´ ğ‘¡ denoteğ‘… MAXâ€™s (cid:169)(|ğ‘†||ğ´|)ğ‘›â„ 2 |ğ‘†||ğ´| +1 (cid:170)
policyattimeğ‘¡ andğ‘  ğ‘¡ denotethestateattimeğ‘¡.Withprobabilityat ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
least1âˆ’ğ›¿,ğ‘‰ ğ‘€ğ´ğ‘¡ (ğ‘  ğ‘¡) â‰¥ğ‘‰ ğ‘€âˆ— (ğ‘  ğ‘¡)âˆ’ğœ€istrueforallbut (cid:173)
(cid:173)
ğœ€3(1âˆ’ğ›¾)6 (cid:174)
(cid:174)
(cid:173) (cid:174)
ğ‘‚Ëœ (cid:18) ğ‘†2ğ´/(cid:16) ğœ€3 (1âˆ’ğ›¾)6(cid:17)(cid:19) (cid:171) (cid:172)
timesteps.
timesteps(finalsamplecomplexitybound). 4.8 Step5:TheBoundbetweenğ‘€ andğ‘€ ğ‘‘
Next,wegiveaguaranteethattheoptimalpolicieslearntinthe
4.6 CaseI original meta-MDP ğ‘€ and the discretised MDP ğ‘€ are similar
ğ‘‘
Directly plugging in Equations 10 and 9 into Theorem G.1, we enoughwithadistanceuptoaconstantfactor.Usingtheresults
obtainthefollowingPAC-bound. from[4],weobtainthefollowingproperty.
Theorem4.12. Supposethat0 â‰¤ ğœ€ < 1âˆ’1 ğ›¾ and0 â‰¤ ğ›¿ < 1are T sth ane to Krem (th4 a. t1 s4 d. ep(M enD dP soD nis lycr oet niz ta ht eio Ln ipB so cu hn itd z[ c7 o] n) sT tah ner te Lex )i ss uts cha tc ho an t-
two real numbers. Let M be any continuous meta-MDP with in-
nerstochasticgameğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = forsomediscretisationcoarsenessğœ†âˆˆ (0, L 2]
IâŸ¨ )ğ‘†Ë† ğ‘‘ u, sğ´ iË† nğ‘‘ g,ğ‘‡ gğ‘‘ r, iğ‘… dğ‘‘ s, iğ›¾ zâŸ© ea os fd ğœ†i .sc Tr he eti rs eed exv ie str ssio inn po uf tsğ‘€ ğ‘š(a =sd ğ‘šesc (cid:16)r ğœ€1i ,b ğ›¿e 1d (cid:17)i an nC da ğœ€s 1e
,
(cid:13) (cid:13) (cid:13)ğ‘‰ ğ‘€âˆ— âˆ’ğ‘‰ ğ‘€âˆ— ğ‘‘(cid:13) (cid:13)
(cid:13)
âˆ
â‰¤ (1K âˆ’ğœ† ğ›¾Ë†)2.
satisfying
4.9 Step6:Addingittogether
(cid:169)(cid:18) 2âˆš ğ‘›|ğ‘†||ğ´| +1(cid:19)ğ‘›|ğ‘†||ğ´|
(cid:170)
To combine the bounds we obtained in Step 4 and 5, we need
ğ‘š(cid:18)1 ,1(cid:19) =ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
an additional bound that bounds the policy value between the
ğœ€ ğ›¿ (cid:173) ğœ€2(1âˆ’ğ›¾)4 (cid:174) continuousanddiscretisedMDP.
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
Lemma4.15(SimulationLemmaforContinuousMDPs). Letğ‘€
ğ‘šan ad nğœ€ d1 1 ğœ€= 1,ğ‘‚ th(cid:16) enğœ€1(cid:17) t, hs eu fc oh llt oh wa it ni gfğ‘… ho-ğ‘€ ldsğ´ .ğ‘‹ Lei ts Aex ğ‘¡ec du et ne od teon ğ‘…-ğ‘€ ğ‘€ğ´w ğ‘‹it â€™h si pn op lu ict ys a Ln etd ğœ–ğ‘€ ğ‘…Ë† â‰¥be mtw axo ğ‘ ,M ğ‘D |ğ‘…Ë†P (s ğ‘ ,t ğ‘h )a âˆ’to ğ‘…n (l ğ‘ y ,ğ‘d )i |ff ae nr din
ğœ€
ğ‘(ğ‘‡ â‰¥,ğ‘… m) aa xn ğ‘ ,d ğ‘( âˆ¥ğ‘‡ ğ‘‡Ë† Ë†, (ğ‘…Ë† Â·) |.
ğ‘ ,ğ‘)âˆ’ğ‘‡(Â·|
attimeğ‘¡ andğ‘ Ë†
ğ‘¡
denotethestateattimeğ‘¡.Withprobabilityatleast ğ‘ ,ğ‘)||1.Thenâˆ€ğœ‹ :SË† â†’ğ‘,istrueforallbut
(cid:13) (cid:13)ğ‘‰ğœ‹ âˆ’ğ‘‰ğœ‹(cid:13) (cid:13) â‰¤ ğœ€ ğ‘… +ğ›¾ğœ– ğ‘ƒğ‘‰max . (cid:18) âˆš (cid:19)2ğ‘›|ğ‘†||ğ´| (cid:18) âˆš (cid:19)|ğ‘†||ğ´|
(cid:13) ğ‘€ ğ‘€Ë†(cid:13) âˆ 1âˆ’ğ›¾ 2(1âˆ’ğ›¾) (cid:169) 2 ğ‘›|ğ‘†||ğ´| +1 2 |ğ‘†||ğ´| +1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† ğœ† (cid:174)
(cid:174)
Underdiscretisation,[4]showedthat,withasmallenoughgrid (cid:173) (cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174) (cid:174)
size,andrestrictingtothediscretisedactionspace,thedifference (cid:173) (cid:174)
intransitionprobabilityofthecontinuousMDPğ‘€anddiscretised (cid:171) (cid:172)
timesteps.I.e.theaboveisthefinalsamplecomplexity.
MDPğ‘€ isupperboundedbyaconstant.
ğ‘‘
4.11 CaseII
Lemma4.16. [4]Thereexistsaconstantğ¾ ğ‘ƒ (dependingonlyon
Similarly,summinguptheboundsinLemmaI.3,TheoremsG.3and
constantL)suchthat
H.1,weobtainthefinalboundforCaseII.InSection5,wealso
|ğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ’ğ‘‡(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)| â‰¤ğ¾ ğ‘ğ›¼ showempiricallythatthenumberofsamplesneededindeedscales
forallğ‘ Ë†â€²,ğ‘ Ë†âˆˆğ‘†Ë†,ğ‘Ë† ğ‘‘ âˆˆğ´Ë† ğ‘‘ andallğ›¼ â‰¤ (0, 21 L]
byafactorof|ğ‘†||ğ´|2ğ‘›â„,asseeninTheorem4.19.
WenowapplytheLemmaI.1toboundthedifferenceinvaluefor Theorem4.19. Supposethat0 â‰¤ ğœ€ < 1âˆ’1 ğ›¾ and0 â‰¤ ğ›¿ < 1are
anydiscretisedpolicy(i.e.restrictingactionspacetoğ´Ë† )inthe two real numbers. Let M be any continuous meta-MDP with in-
continuousMDPğ‘€anddiscretisedMDPğ‘€ ğ‘‘. ğ‘‘ n âŸ¨ğ‘†e Ë† ğ‘‘r ,s ğ´t Ë†o ğ‘‘c ,h ğ‘‡ ğ‘‘as ,t ğ‘…i ğ‘‘c ,g ğ›¾a âŸ©m ae sğº disc= reâŸ¨ tğ‘† is, eğ´ d,ğ‘‡ vi en rn se ir o, nğ‘… i on fne ğ‘€râŸ©. (aL set deu ss crd ie bn eo dte inğ‘€ Cğ‘‘ as= e
Lemma4.17. Letğ‘€ ğ´Ë†
ğ‘‘
=(ğ‘†Ë†,ğ´Ë† ğ‘‘,ğ‘‡,ğ‘…,ğ›¾)bethecontinuousMDPğ‘€ I s) atu iss fi yn ig nggrid size of ğœ†. There exists inputsğ‘š = ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,
ğ‘€re ğ‘‘str =ict (e ğ‘†Ë†d ,ğ´t Ë†o ğ‘‘,t ğ‘‡h ğ‘‘e ,d ğ‘…i ğ‘‘sc ,ğ›¾re )t .is Te hd enac ft oio rn ansp ya dc ie s. cR ree tc isa el dlt ph oe lid ci ysc ğœ‹re :ti ğ‘†s Ë†e â†’dM ğ´Ë†D ğ‘‘P
,
ğ‘š(cid:18)1 ,1(cid:19)
=ğ‘‚Ëœ
(cid:32) (|ğ‘†||ğ´|)ğ‘›â„(cid:33)
ğœ€ ğ›¿ ğœ€2(1âˆ’ğ›¾)4
âˆ¥ğ‘‰ ğ‘€ğœ‹ ğ´Ë†
ğ‘‘
âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ=âˆ¥ğ‘‰ ğ‘€ğœ‹ âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ â‰¤ L 1âˆ’Rğ›¼ ğ›¾ + (1ğ›¾ğ¾ âˆ’ğ‘ ğ›¾ğ›¼ )2 and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€ withinputs
ğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicy
Note that, restricted to discretised policies ğœ‹ which only picks attimeğ‘¡ andğ‘ Ë† ğ‘¡ denotethestateattimeğ‘¡.Withprobabilityatleast
actionsinğ´Ë† ğ‘‘,thevalueofğœ‹ intheoriginalMDPğ‘€,ğ‘‰ ğ‘€ğœ‹,equalsto 1âˆ’ğ›¿,
itsvaluethesameMDPrestrictedtodiscretisedactionspace,ğ‘‰ ğ‘€ğœ‹ ğ´Ë† ğ‘‘. ğ‘‰ ğ‘€âˆ— (ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€Ağ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€+ (1K âˆ’ğœ†
ğ›¾Ë†)2
+ L 1âˆ’Rğ›¼
ğ›¾
+ 2(ğ›¾ 1ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
4.10 CaseI istrueforallbut
SumminguptheboundsinLemmaI.3,TheoremsG.2andH.1,we (cid:18) âˆš (cid:19)|ğ‘†||ğ´|
obtainthefinalboundforCaseI.Thefinalboundguarantees,with (cid:169)(|ğ‘†||ğ´|)2ğ‘›â„ 2 |ğ‘†||ğ´| +1 (cid:170)
highprobability,thatthepolicyweobtainfromR-FOSiscloseto
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174) timesteps
(cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174)
theoptimalpolicyinğ‘€apartfromaconstantfactor. (cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
Theorem4.18. Supposethat0 â‰¤ ğœ€ < 1âˆ’1 ğ›¾ and0 â‰¤ ğ›¿ < 1are
5 Experiments
two real numbers. Let M be any continuous meta-MDP with in-
nerstochasticgameğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = Wenowvalidateourtheoreticalfindingsempirically.
âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ© asdiscretisedversionofğ‘€ (asdescribedinCase
I) using grid size of ğœ†. There exists inputsğ‘š = ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1, 5.1 TheMatchingPenniesEnvironment
satisfying
MatchingPenniesisatwo-player,zero-sumgamewithapayoff
matrixshowninSection5.1.EachagenteitherpickHeads(H)or
ğ‘š(cid:18)1
ğœ€,
ğ›¿1(cid:19) =ğ‘‚Ëœ(cid:169) (cid:173)
(cid:173)
(cid:173)(cid:18) 2âˆš ğ‘› ğœ†|ğ‘† ğœ€2|| (ğ´ 1| âˆ’+ ğ›¾1 )(cid:19) 4ğ‘›|ğ‘†||ğ´| (cid:170) (cid:174)
(cid:174)
(cid:174)
T t gh aa e mils p er( iT o sb) n, ağ‘ obğ‘– til iâˆˆ i tt ey{ rğ» ao tf, eğ‘‡ dp .} la Ta y hn e id sr mğ‘–ğ‘ğ‘– p eâˆ¼ i ac nkğœ‹ siğœ™ n tğ‘– hg( aÂ· H t| a.{ nN} i) o n, t nw e eh t rh -e ear pe t ii sğœ™ n oğ‘– dtc eho i hr sr awe ss o ap ro lk en n,d gth tt ho e
(cid:173) (cid:174)
(cid:173) (cid:174) of1andtheinner-episodicreturncorrespondstothepayoffafter
(cid:171) (cid:172)
oneinteractionğ‘Ÿ =PayoffTable(ğ‘1,ğ‘2).ForR-FOS,thismeansthat
and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€ withinputs a gam me et .a T-s ht eep mc eo tr ar -e rs ep tuo rn nds cot ro reo sn pe oi nte dr sa tt oio tn heof dt ish ce oM una tt ec dh ,i cn ug mP ue ln an tii ve es
ğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicy
meta-rewardafterplayingtheMatchingPenniesğ¾times.Whilethe
attimeğ‘¡ andğ‘ Ë†
ğ‘¡
denotethestateattimeğ‘¡.Withprobabilityatleast
originalM-FOSwasevaluatedonamorecomplex,iteratedversion
1âˆ’ğ›¿,
oftheMatchingPenniesgame,thissimplesettingwithabinary
ğ‘‰ ğ‘€âˆ— (ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€Ağ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€+ (1K âˆ’ğœ†
ğ›¾Ë†)2
+ L 1âˆ’Rğ›¼
ğ›¾
+ (1ğ›¾ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
a ac lst oio mn os rp ea pce rai cs ts icu affi lfc oie rn imtf po lr emou er ne tam tip oi nri bca el cav ua sli ed ta hti eo ğ‘…n M.O Au Xr as le gt ot rin itg hmismemoryusagegrowsexponentiallywiththesizeofthestateand
actionspace.Thus,foranyofthemorecomplexenvironmentsfrom
theM-FOSpaperwewerenotabledoanyempiricalanalysisofR-
FOSatall,duetotheexponentialsampleandmemoryrequirements.
Player1\Player2 Head Tail
Head (+1,-1) (-1,+1)
Tail (-1,+1) (+1,-1)
Table1:PayoffMatrixforMP
5.2 ExperimentSetup
WeimplementanempiricalversionofourR-FOSalgorithm.Be-
causetheR-FOSalgorithmusesQ-valueiterationtosolvethemeta-
game, the algorithm needs to keep a copy of the meta-Q-value
table.Therefore,memoryusagegrowsexponentiallywithrespect
Figure1:Empiricalsamplecomplexitywhilevaryingthetra-
totheinner-gameâ€™sstate-actionspacesize.WefoundthatCaseIof
jectorywindowâ„.Weplotthemeta-rewardpermeta-episode.
thealgorithmwasintractabletoimplementevenwithacompact
Tobettervisualisetheconnectionwiththetheoryresults,
environmentlikeMP.ThemetaQ-tableofsize|ğ‘†Ë† |Ã—|ğ´Ë† |wassim-
weplotthex-axisinlog 16scale.Thereportedresultsarethe
plytoolargetofitinmemory.Therefore,wefocusonempirically
meanover3seedswithstandarderror.
validatingasimplifiedcaseofCaseII.Wemaketwosimplifications,
(1) Themeta-stateusesapartialhistoryofpastactions.Onlythe
7 Conclusion
mostrecentâ„actionsareused,whereâ„isahyper-parameter
Wepresentedthreemaincontributionsinourwork.Firstofall,we
wepick.Thewindowsizeallowsustocontrolthesizeofthe
presentedR-FOS,atabularalgorithmadaptedfromM-FOS.Unlike
meta-gamestate,i.e.,ğ‘†Ë† âˆˆR2â„.BecausetheMPgameonlyhas
M-FOS,whichlearnsapolicyinacontinuousmeta-MDP,R-FOS
onestate,itisnotnecessarytoincludethestate.
insteadlearnsapolicyinadiscreteapproximationoftheorigi-
nalmeta-MDPwhichallowsustomoreeasilyperformtheoretical
(2) Tofurtherdecreasetheproblemsizefortractability,wedefine
themeta-agentactiontobetheinner-agentâ€™sgreedyaction,
analysis.Withinthisdiscretisedmeta-MDP,R-FOSusestheğ‘…
MAX
algorithmasthemeta-agent.WeadaptedR-FOSfromM-FOSsuch
insteadoftheQ-table.Thisresultsinamuchsmallmeta-action
sizeof|ğ´Ë† |=2 thatitstillmaintainsallkeyattributesofM-FOS.Secondofall,we
derivedanexponentialsamplecomplexityboundforbothcases
describedinM-FOS(thetwocasesbeingeitherinner-gamepolicies
6 ResultsandDiscussion
orinner-gametrajectoryhistoryasmeta-state).Specifically,we
Wedrawconnectionsbetweenoursamplecomplexitytheoryresults
provedthatwithhighprobability,thepolicylearntbyR-FOSis
andexperimentalresultsintheMPenvironment.Ourgoalisto
closetotheoptimalpolicyfromtheoriginalmeta-MDPuptoa
analysethescalinglawofR-FOS.Specifically,weinvestigatehow
constantdistance.Finally,weimplementedR-FOSandinvestigated
thesamplecomplexitychangeswhenwevarythewindow-sizeâ„.
theempiricalsamplecomplexityintheMatchingPenniesenviron-
UndertheMPenvironmentsettings,theinner-gamestate-action
ment.Wedrawconnectionsbetweentheoryandexperimentsby
spacesizeis|ğ‘†||ğ´|=2andthenumberofplayersisğ‘›=2.Following
showingbothresultsscalesexponentiallyaccordingtothesizeof
theboundinTheorem4.19,weseethattheonlytermthatdepends
theinner-gameâ€™sstate-action-space.
onhisthe16â„ term:
(cid:18) âˆš (cid:19)|ğ‘†||ğ´| (cid:18) âˆš (cid:19)2 Acknowledgments
(cid:169)(|ğ‘†||ğ´|)2ğ‘›â„ 2 |ğ‘†||ğ´| +1 (cid:170) (cid:169)16â„ 2 2 +1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
(cid:173)
ğœ€3(1âˆ’ğœ†
ğ›¾)6
(cid:174)
(cid:174)
(cid:174)âˆ¼ğ‘‚Ëœ(cid:173)
(cid:173)
(cid:173)
ğœ€3(1ğœ†
âˆ’ğ›¾)6
(cid:174)
(cid:174)
(cid:174)
K QF Zw ia ss ss uu pp pp oo rr tt ee ddb by yt Ah re maD s. uH i. ssC ehe an ndFo Cu on hd ea rt ei .onScholarship.
(cid:173) (cid:174) (cid:173) (cid:174)
(cid:173) (cid:174) (cid:173) (cid:174)
(cid:171) (cid:172) (cid:171) (cid:172)
Hence,ourtheoryresultssaysthatwheneverthegamehorizonis
increasedby1,weexpecttoseethesamplecomplexitytoincrease
byafactorof16intheMPenvironment.Figure1showsthereward
across the meta episodes on a log 16 scale. The graph contains
threerewardcurvesformeta-trajectorylengthâ„ =2,3,4,which
convergesapproximatelyat163,164,ğ‘ğ‘›ğ‘‘165episodes.Indeed,this
isconsistentwithourtheoreticalresultsinTheoremG.3.References the7thInternationalConferenceonNeuralInformationProcessingSys-
[1] DavidBalduzzi,SebastienRacaniere,JamesMartens,JakobFoerster,
tems(Denver,Colorado)(NIPSâ€™94).MITPress,Cambridge,MA,USA,
361â€“368.
KarlTuyls,andThoreGraepel.2018. Themechanicsofn-player
[20] AlexanderL.Strehl,LihongLi,andMichaelL.Littman.2009. Rein-
differentiablegames.InInternationalConferenceonMachineLearning.
PMLR,354â€“363.
forcementLearninginFiniteMDPs:PACAnalysis.J.Mach.Learn.Res.
10(dec2009),2413â€“2444.
[2] Craig Boutilier, Thomas Dean, and Steve Hanks. 1999. Decision-
[21] TimonWilli,AlistairHpLetcher,JohannesTreutlein,andJakobFoer-
TheoreticPlanning:StructuralAssumptionsandComputationalLever-
ster.2022.COLA:consistentlearningwithopponent-learningaware-
age.J.Artif.Int.Res.11,1(jul1999),1â€“94.
[3] RonenBrafmanandMosheTennenholtz.2001.R-MAX-AGeneral
ness.InInternationalConferenceonMachineLearning.PMLR,23804â€“
23831.
PolynomialTimeAlgorithmforNear-OptimalReinforcementLearning.
[22] QizhenZhang,ChrisLu,AnimeshGarg,andJakobFoerster.2022.
TheJournalofMachineLearningResearch3,953â€“958. https://doi.org/
10.1162/153244303765208377
CentralizedModelandExplorationPolicyforMulti-AgentRL.InIn-
[4] CHEE-SChowandJohnNTsitsiklis.1991.Anoptimalone-waymulti- ternationalConferenceonAutonomousAgentsandMulti-AgentSystems
gridalgorithmfordiscrete-timestochasticcontrol.IEEEtransactions
(AAMAS). https://arxiv.org/abs/2107.06434v2
[23] StephenZhao,ChrisLu,RogerBGrosse,andJakobFoerster.2022.
onautomaticcontrol36,8(1991),898â€“914.
[5] MuratA.Erdogdu.2022.Coveringwithepsilon-nets. https://erdogdu.
ProximalLearningWithOpponent-LearningAwareness.Advancesin
github.io/csc2532/lectures/lecture05.pdf
NeuralInformationProcessingSystems35(2022),26324â€“26336.
[6] Jakob N. Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon
Whiteson,PieterAbbeel,andIgorMordatch.2018. Learningwith
Opponent-LearningAwareness. arXiv:1709.04326[cs.AI]
[7] DHausslerandEWelzl.1986.Epsilon-NetsandSimplexRangeQueries.
InProceedingsoftheSecondAnnualSymposiumonComputational
Geometry(YorktownHeights,NewYork,USA)(SCGâ€™86).Association
forComputingMachinery,NewYork,NY,USA,61â€“71. https://doi.
org/10.1145/10515.10522
[8] ShamKakade.2003.OnthesamplecomplexityofReinforcementLearn-
ing.Ph.D.Dissertation.UniversityofLondon.
[9] AkbirKhan,NewtonKwan,TimonWilli,ChrisLu,AndreaTacchetti,
andJakobNicolausFoerster.[n.d.].ContextandHistoryAwareOther-
Shaping.([n.d.]).
[10] Dong-Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun,
MarwaAbdulhai,GolnazHabibi,SebastianLopez-Cot,GeraldTesauro,
andJONATHANPHOW.2021. APolicyGradientAlgorithmfor
LearningtoLearninMultiagentReinforcementLearning. https:
//openreview.net/forum?id=zdrls6LIX4W
[11] AlistairLetcher.2020.Ontheimpossibilityofglobalconvergencein
multi-lossoptimization.arXivpreprintarXiv:2005.12649(2020).
[12] AlistairLetcher,JakobFoerster,DavidBalduzzi,TimRocktÃ¤schel,and
ShimonWhiteson.2018. Stableopponentshapingindifferentiable
games.arXivpreprintarXiv:1811.08469(2018).
[13] YaoLiuandEmmaBrunskill.2018.WhenSimpleExplorationisSample
Efficient:IdentifyingSufficientConditionsforRandomExplorationto
YieldPACRLAlgorithms.
[14] ChrisLu,TimonWilli,ChristianA.SchroederdeWitt,andJakobN.
Foerster.2022.Model-FreeOpponentShaping.InInternationalCon-
ferenceonMachineLearning,ICML2022,17-23July2022,Baltimore,
Maryland,USA(ProceedingsofMachineLearningResearch,Vol.162).
PMLR,14398â€“14411.
[15] ChrisLu,TimonWilli,AlistairLetcher,andJakobFoerster.2022.Ad-
versarialCheapTalk.arXivpreprintarXiv:2211.11030(2022).
[16] DhruvMalik,AldoPacchiano,VishwakSrinivasan,andYuanzhiLi.
2021.SampleEfficientReinforcementLearningInContinuousState
Spaces:APerspectiveBeyondLinearity.InInternationalConferenceon
MachineLearning.
[17] FlorianSchÃ¤ferandAnimaAnandkumar.2019.Competitivegradient
descent.AdvancesinNeuralInformationProcessingSystems32(2019).
[18] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,and
OlegKlimov.2017. Proximalpolicyoptimizationalgorithms. arXiv
preprintarXiv:1707.06347(2017).
[19] SatinderP.Singh,TommiJaakkola,andMichaelI.Jordan.1994. Re-
inforcementLearningwithSoftStateAggregation.InProceedingsofA Overview
A.1 ProblemSetup
Themeta-gameisdefinedbyacontinuousMDPğ‘€ =âŸ¨ğ‘†Ë†,ğ´Ë†,ğ‘‡,ğ‘…,ğ›¾âŸ©withfinitehorizonâ„.
Fortheremainingoftheproof,weconsidertwowaystoformulatethemeta-statespaceandmeta-actionspace:
â€¢ InCaseI,themeta-stateisallinneragentsâ€™policiesâ€™parametersfromtheprevioustimestep,andthemeta-actionistheinneragentâ€™s
currentpolicyparameters.
â€¢ InCaseII,theonlydifferencewithCaseIisthemeta-stateisinsteadallinneragentsâ€™trajectories.
SeeTable2forasummaryforthetwocases.
Table2:Twocasesofrepresentingthemeta-statespaceandmeta-actionspace.
ğ‘ Ë† ğ‘¡ = ğ‘Ë† ğ‘¡ =
CaseI ğ“ğ‘¡âˆ’1= [ğœ™ ğ‘¡ğ‘– âˆ’1,ğ“ ğ’•âˆ’ âˆ’ğ’Š 1] ğœ™ ğ‘¡ğ‘–
CaseII ğ‰ğ‘¡ ğœ™ ğ‘¡ğ‘–
Theinner-gameisann-playerfully-observablediscretestochasticgameğº =âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©withfinitehorizonâ„.
A.2 TheoryOverview
WederivethesamplecomplexityofourR-FOSalgorithm.Onahigh-level,theproofconsistsofsixsteps.
(1) WediscretiseourMDPğ‘€intoğ‘€ .Wefirstdescretisethecontinuousmeta-statespaceandmeta-actionspaceusingepsilon-nets[4].
ğ‘‘
Basedonthedescretisedmeta-stateandmeta-actionspace,wethendefinethediscretisedtransitionandrewardfunction.
(2) Wethenconstructağ‘š-knowndiscretisedMDPğ‘€ ,asdescribedbytheR-MAXalgorithm[20].
ğ‘š,ğ‘‘
(3) Then,weestimatetheempiricalğ‘š-knowndiscretisedMDPğ‘€Ë† usingmaximumlikelihoodestimate.Thisisthesameprocedure
ğ‘š,ğ‘‘
describedbytheR-MAXalgorithm[20].Ouralgorithm,R-FOS,learnsanoptimalpolicyinğ‘€Ë† .
ğ‘š,ğ‘‘
(4) WefirstproveaPAC-boundbetweentheoptimalpolicieslearntinğ‘€Ë† andğ‘€ .Thisstepusesresultsfrom[20].
ğ‘š,ğ‘‘ ğ‘‘
(5) Wethenproveaboundbetweentheoptimalpolicieslearntinğ‘€ andğ‘€.Thisstepusesresultsfrom[4].
ğ‘‘
(6) WeobtainthefinalPAC-boundbuildingfromthetwoboundsfromabove.
B Nomenclature
Symbol Definition
ğ‘ Ë†
ğ‘¡
=ğ‘ Ë† Meta-stateattimeğ‘¡,timesubscriptğ‘¡ isomittedforconvenience
ğ‘Ë†
ğ‘¡
=ğ‘Ë† Meta-actionattimeğ‘¡,timesubscriptğ‘¡ isomittedforconvenience
(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) discretisedstate-actionpairinmeta-game
ğ‘ŸË† ğ‘‘ =ğ‘ŸË†(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) Meta-rewardfunctionparameterisedbydiscretisedmeta-state-actionpair
ğœ™ğ‘– =ğœ™ğ‘– Thesetofinner-gamepolicyparametersofouragentattimeğ‘¡,timesubscriptğ‘¡ isomittedforconvenience
ğ‘¡
ğœ™ğ‘– =ğœ™ğ‘– Thesetofdiscretisedinner-gamepolicyparametersofouragentattimeğ‘¡,timesubscriptğ‘¡ isomittedforconvenience
ğ‘¡,ğ‘‘ ğ‘‘
ğ“âˆ’ğ‘– The set of inner-game policy parameters of all agents except our agent at timeğ‘¡, time subscriptğ‘¡ is omitted for
convenience
ğ“âˆ’ğ’Š Thesetofdiscretisedinner-gamepolicyparametersofallagentsexceptouragentattimeğ‘¡,timesubscriptğ‘¡ isomitted
ğ’…
forconvenience
ğ‘…Ë† (ğ‘ Ë†,ğ‘Ë†),ğ‘‡Ë† (ğ‘ Ë†,ğ‘Ë†) Empiricalestimateofrewardandtransitiondistribution
ğ‘…(ğ‘ Ë†,ğ‘Ë†),ğ‘‡(ğ‘ Ë†,ğ‘Ë†) TruerewardandtransitiondistributionC Assumptions
WefirstoutlineallassumptionsmadeinderivingthesamplecomplexityoftheR-FOSalgorithm.
Toestablishtheboundinstep5,wemakethefollowingassumptions.
AssumptionC.1. Bothmeta-gameandinner-gamearefinitehorizon.Weuseâ„todenotethemeta-gamehorizon,andâ„ innertodenotethe
inner-gamehorizon.
AssumptionC.2. Themeta-gameusesadiscountfactorofğ›¾.Forsimplicityoftheproof,assumetheinner-gameusesadiscountfactorof1.
Althoughthisassumptioncanbeeasilyomittedbysubstitutingğ‘… intheoriginalproofin[20].
MAX
AssumptionC.3. Weassumetheinner-gamerewardisbounded.Forsimplicityoftheproof,wesetthisboundas 1 ,whereâ„isthe
â„inner
horizonoftheinnergame.Formally,forall(ğ‘ ,ğ‘),0 â‰¤ ğ‘… inner(ğ‘ ,ğ‘) â‰¤ â„1.Thisallowsustointroducethenotionofmaximumrewardand
maximumvaluefunctionasğ‘… max,inner = â„in1
ner
andğ‘‰ max,inner =1respectively.Thisimpliestherewardandvaluefunctioninthemeta-game
arealsobounded,i.e.,ğ‘…max=1andğ‘‰max= 1âˆ’1 ğ›¾.
AssumptionC.4. Theinnergameisassumedtobediscrete.
Toestablishtheboundinstep6,wemakethefollowingassumptions.
AssumptionC.5. Themeta-rewardfunctionisLipschitz-continuous:Forallğ‘ Ë† 1,ğ‘ Ë† 2 âˆˆğ‘†Ë†andğ‘Ë† 1,ğ‘Ë† 2 âˆˆğ´Ë†,
(cid:12) (cid:12)ğ‘…(ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’ğ‘…(ğ‘ Ë† 2,ğ‘Ë† 2)(cid:12) (cid:12)â‰¤Lğ‘…(cid:13) (cid:13)(ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’(ğ‘ Ë† 2,ğ‘Ë† 2)(cid:13) (cid:13) âˆ, âˆ€ğ‘ Ë† 1,ğ‘ Ë† 2 âˆˆğ‘†Ë†andğ‘Ë† 1,ğ‘Ë† 2 âˆˆğ´Ë†
whereLğ‘… isthemeta-rewardfunctionâ€™sLipschitz-constant.
AssumptionC.6. Themeta-transitionfunctionisLipschitz-continuous:Forallğ‘ Ë† 1,ğ‘ Ë† 1â€²,ğ‘ Ë† 2 âˆˆğ‘†Ë†andğ‘Ë† 1,ğ‘Ë† 1â€²,ğ‘Ë† 2 âˆˆğ´Ë†,
(cid:12) (cid:12)ğ‘‡(ğ‘ Ë† 1â€² |ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’ğ‘‡(ğ‘ Ë† 2â€² |ğ‘ Ë† 2,ğ‘Ë† 2)(cid:12) (cid:12)â‰¤Lğ‘‡(cid:13) (cid:13)(ğ‘ Ë† 1â€²,ğ‘ Ë† 1,ğ‘Ë† 1)âˆ’(ğ‘ Ë† 2â€²,ğ‘ Ë† 2,ğ‘Ë† 2)(cid:13)
(cid:13)
âˆ
whereLğ‘‡ isthemeta-transitionfunctionâ€™sLipschitz-constant.
AssumptionC.7. Thereâ€™saLipschitz-continuouspoint-to-setmappingbetweenmeta-statespaceandmeta-actionspacesuchthatforany
ğ‘ Ë†,ğ‘ Ë†â€² âˆˆğ‘†Ë†andğ‘Ë†,ğ‘Ë†â€² âˆˆğ´Ë†,thereexistssomeğ‘Ë†âˆˆğ‘ˆ(ğ‘ Ë†)suchthat(cid:13) (cid:13)ğ‘Ë†âˆ’ğ‘Ë†â€²(cid:13)
(cid:13)
<L(cid:13) (cid:13)ğ‘ Ë†âˆ’ğ‘ Ë†â€²(cid:13)
(cid:13)
âˆ âˆ
AssumptionC.8. Themeta-gametransitionfunctionğ‘‡(Â·|ğ‘ Ë†,ğ‘Ë†)isaprobabilitydensityfunctionsuchthat0â‰¤ğ‘‡(ğ‘ Ë†â€² |ğ‘ Ë†,ğ‘Ë†) â‰¤Land âˆ« ğ‘†Ë†ğ‘‡(ğ‘ Ë†â€² |
ğ‘ Ë†,ğ‘Ë†)ğ‘‘ğ‘ Ë†â€² =1, âˆ€ğ‘ Ë†,ğ‘ Ë†â€² âˆˆğ‘†Ë†andğ‘Ë†âˆˆğ´Ë†
D Step1:Discretisationwithğœ€-Net
Figure2:ğœ€-NetforÎ˜={ğœƒ âˆˆR2:âˆ¥ğœƒâˆ¥ â‰¤ğ‘…}[5]
ToapplytheR-MAXalgorithm,wefirstconverttheMDPğ‘€inM-FOSintoatabularMDPğ‘€ .
ğ‘‘DefinitionD.1. (ğœ€-Net[5])Forğœ€ >0,Nğœ€ isanğœ€-netoverthesetÎ˜âŠ†Rğ· ifforallğœƒ âˆˆÎ˜,thereexistsğœƒâ€² âˆˆNğœ€ suchthat(cid:13) (cid:13)ğœƒâˆ’ğœƒâ€²(cid:13) (cid:13)2 â‰¤ğœ€.
Todiscretiseağ·-dimensionalsphereofradiusğ‘…,weuseağœ€-netcontainingğ·-dimensionalcubesofsidesğœ†.Thisresultsina
(cid:16)2ğ‘… +1(cid:17)ğ·
ğœ†
points.Withineachğ·-dimensionalcube,thelargestdistancebetweentheverticesandtheinteriorpointscomesfromthecenterofthe
âˆš
cube,whichis ğœ† ğ‘‘.Therefore,toguaranteeafullcoverofallthepointsinthesphere,thelargestcubesizethatwecanhaveshouldsatisfy
âˆš 2
ğœ€ = ğœ† ğ‘‘.Figure2illustratesanexampleofusingğœ€-netstodiscretisetheinputspaceÎ˜={ğœƒ âˆˆR2:âˆ¥ğœƒâˆ¥ â‰¤ğ‘…}.Fromhereon,wewillreplace
2
theğœ€inğœ€-netwithğ›¼ toavoidnotationoverloading.
D.1 DiscretisationoftheStateandActionSpace:CaseI
InCaseI,themeta-stateğ‘ Ë†
ğ‘¡
isallinneragentsâ€™policiesparametersfromtheprevioustimestep.Eachoftheinneragentğ‘–â€™spolicyisaQ-table,
denotedasğœ™ ğ‘– âˆˆR|ğ‘†|Ã—|ğ´|.Formally,ğ‘ Ë† ğ‘¡ :=ğ“ğ‘¡âˆ’1= [ğœ™ ğ‘¡ğ‘– âˆ’1,ğ“ ğ’•âˆ’ âˆ’ğ’Š 1].Themeta-actionğ‘Ë† ğ‘¡ istheinneragentâ€™scurrentpolicyparametersğœ™ ğ‘¡ğ‘–.
Forthemeta-actionspaceğ´Ë†andachosendiscretisationerrorğ›¼ >0,weobtaintheğœ€-netğ´Ë†
ğ‘‘
âŠ‚ğ´Ë†suchthatforallğ‘Ë†âˆˆğ´Ë†,thereexistğ‘Ë†
ğ‘‘
âˆˆğ´Ë†
ğ‘‘
where
(cid:13) (cid:13)ğ‘Ë†âˆ’ğ‘Ë† ğ‘‘(cid:13) (cid:13)â‰¤ğ›¼. (8)
Wecaninferğ´Ë†hasdimensionğ· =|ğ‘†||ğ´|andRadiusğ‘…=âˆšï¸1Â·|ğ‘†||ğ´|=âˆšï¸ |ğ‘†||ğ´|(AssumptionC.3).Dividingthespacewithgridsizeğœ†results
inthesizeofdiscretisedmeta-actionspaceuppertobeboundedby
(cid:32) 2âˆšï¸ |ğ‘†||ğ´| (cid:33)|ğ‘†||ğ´|
|ğ´Ë† ğ‘‘| â‰¤
ğœ†
+1 . (9)
Similarity,wecanalsoinferğ‘†Ë†hasdimensionğ· =ğ‘›|ğ‘†||ğ´|andRadiusğ‘…=âˆšï¸1Â·ğ‘›|ğ‘†||ğ´|=âˆšï¸ğ‘›|ğ‘†||ğ´|.Dividingthespacewithgridsizeğœ†results
inthesizeofdiscretisedmeta-statespaceuppertobeboundedby
(cid:32) 2âˆšï¸ğ‘›|ğ‘†||ğ´| (cid:33)ğ‘›|ğ‘†||ğ´|
|ğ‘†Ë† ğ‘‘| â‰¤
ğœ†
+1 . (10)
D.1.1 DiscretisationoftheStateandActionSpace:CaseII
InCaseII,themeta-stateğ‘ Ë†
ğ‘¡
isallinneragentsâ€™trajectories.Formally,ğ‘ Ë†
ğ‘¡
:=ğ‰ğ‘¡.,whereğœ ğ‘¡ğ‘– ={ğ‘ 0,ğ‘0,ğ‘ 1,ğ‘1,...,ğ‘  ğ‘¡,ğ‘ ğ‘¡}.Becauseweassumethe
Inner-Gameisdiscrete(i.e.thestateandactionspacearebothdiscrete),themeta-stateinthiscasedoesnotneeddiscretisation.Thus,we
candirectlyobtainthemeta-statespace,whichis,
|ğ‘†Ë† ğ‘¡|=(|ğ‘†||ğ´|)ğ‘›â„. (11)
Themeta-actionremainssameasCaseI.
D.2 DiscretisationoftheTransitionandRewardFunction
Undertheabovediscretisationprocedure,wedefinethediscretisedMDPğ‘€
ğ‘‘
=(ğ‘†Ë†,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾),wherethestatespaceremainscontinuous,
theactionspaceisrestrictedtodiscretisedactions.Wedefinethetransitionfunctionandrewardfunctionforğ‘€ as:
ğ‘‘
ğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)=
âˆ«
ğ‘†Ë†ğ‘‡ğ‘‡ (( ğ‘§ğ‘  Ë†Ë† ğ‘‘â€²| |ğ‘ Ë† ğ‘ Ë†ğ‘‘ ğ‘‘, ,ğ‘ ğ‘Ë† Ë†ğ‘‘ ğ‘‘)
)ğ‘‘ğ‘§Ë†
(12)
Wecanviewğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)asanormalizedsampleofğ‘‡ ğ‘‘(Â·|Â·,ğ‘Ë† ğ‘‘)atğ‘ Ë†â€²,ğ‘ Ë† ğ‘‘.
ğ‘… ğ‘‘(ğ‘ Ë†,ğ‘Ë† ğ‘‘)=ğ‘…(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) (13)
E Step2:Theğ‘š-knownDiscretisedMDP
Inthelaststep,weconvertedthemeta-MDPğ‘€intoadiscretisedmeta-MDPğ‘€ .Fromğ‘€ ,R-FOSbuildsağ‘š-knowndiscretisedMDPğ‘€ .
ğ‘‘ ğ‘‘ ğ‘š,ğ‘‘
DefinitionE.1(m-KnownMDP). Letğ‘€
ğ‘‘
=âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©beaMDP.Wedefineğ‘€ ğ‘š,ğ‘‘tobetheğ‘š-knownMDP(SeeTable3),where
ğ‘š-knownisthesetofstate-actionpairsthathasbeenvisitedatleastğ‘štimes.Forallstate-actionpairsinğ‘š-known,theinducedMDPğ‘€
ğ‘š,ğ‘‘
behavesidenticaltoğ‘€ .Forstate-actionpairsoutsideofğ‘š-known,thestate-actionpairsareself-absorbingandmaximallyrewarding.
ğ‘‘GroundTruth Discretised ğ‘š-known Empirical
MDPğ‘€ MDPğ‘€ DiscretisedMDP ğ‘š-known
ğ‘‘
ğ‘€Ë† DiscretisedMDP
ğ‘š,ğ‘‘
ğ‘€Ë†
ğ‘š,ğ‘‘
Known =ğ‘€ =ğ‘€
ğ‘‘
=ğ‘€
ğ‘‘
â‰ˆğ‘€
ğ‘‘
Unknown =ğ‘€ =ğ‘€ self-loopwithmaximumreward
ğ‘‘
Table3:Relationshipbetweenğ‘€,ğ‘€ ğ‘‘,ğ‘€ ğ‘š,ğ‘‘,ğ‘€Ë†
ğ‘š,ğ‘‘
F Step3:TheEmpiricalDiscretisedMDP
DefinitionF.1(Empiricalm-KnownMDP). ğ‘€
ğ‘š
istheexpectedversionofğ‘€Ë†
ğ‘š
where:
(cid:40)
ğ‘‡ ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):= ğ‘‡ 1ğ‘‘ [ğ‘ ( Ë†ğ‘  ğ‘‘â€²Ë† ğ‘‘â€² =| ğ‘ ğ‘  Ë†Ë† ğ‘‘ğ‘‘, ]ğ‘ ,Ë† ğ‘‘) i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known
ğ‘‡Ë† ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):=ï£±ï£´ï£´ï£² ï£´ï£´1ğ‘› ğ‘›( [ğ‘  ğ‘ Ë† Ë†(ğ‘‘ ğ‘‘ğ‘  â€²Ë†, ğ‘‘ğ‘Ë† =,ğ‘‘ ğ‘Ë†, ğ‘‘ğ‘  ğ‘ Ë† Ë†ğ‘‘ )â€² ğ‘‘) ],
,
i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known
ï£³ (14)
(cid:40)
ğ‘… ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘):= ğ‘…ğ‘… ğ‘‘ m( ağ‘ Ë† xğ‘‘,ğ‘Ë† ğ‘‘), i of th(ğ‘ Ë† eğ‘‘ r, wğ‘Ë† ğ‘‘ is) eâˆˆm-known
ğ‘…Ë† ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)=ï£±ï£´ï£´ï£²(cid:205) ğ‘–ğ‘›(ğ‘ Ë†ğ‘‘ ğ‘›,ğ‘ (Ë† ğ‘ ğ‘‘ Ë† ğ‘‘) ,ğ‘ğ‘Ÿ Ë† ğ‘‘(ğ‘ Ë† )ğ‘‘,ğ‘Ë† ğ‘‘), if(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) âˆˆm-known
ï£´ï£´ğ‘…max, otherwise
ï£³
ğ‘…Ë† ğ‘š,ğ‘‘(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘) andğ‘‡Ë† ğ‘š,ğ‘‘ (cid:16) ğ‘ Ë† ğ‘‘â€² |ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘(cid:17) arethemaximum-likelihoodestimatesfortherewardandtransitiondistributionofstate-actionpair
(ğ‘  ğ‘‘,ğ‘ ğ‘‘)withğ‘›(ğ‘  ğ‘‘,ğ‘ ğ‘‘) â‰¥ğ‘šobservationsof(ğ‘  ğ‘‘,ğ‘ ğ‘‘).
G Step4:TheBoundBetweenğ‘€ ğ‘‘ andğ‘€Ë† ğ‘š,ğ‘‘
TheoremG.1. (R-MAXMDPBound[20]) Supposethat0â‰¤ğœ€ < 1âˆ’1 ğ›¾ and0â‰¤ğ›¿ <1aretworealnumbersandğ‘€ =âŸ¨ğ‘†,ğ´,ğ‘‡,ğ‘…,ğ›¾âŸ©isanyMDP.
Thereexistsinputsğ‘š=ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,satisfyingğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) =ğ‘‚(cid:18) (|ğ‘†|+ln ğœ€( 2|ğ‘† (1|| âˆ’ğ´ ğ›¾| )/ 2ğ›¿))ğ‘‰ m2 ax(cid:19) and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedon
ğ‘€withinputsğ‘šandğœ€1,thenthefollowingholds.Letğ´
ğ‘¡
denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicyattimeğ‘¡ andğ‘ 
ğ‘¡
denotethestateattimeğ‘¡.Withprobabilityat
least1âˆ’ğ›¿,ğ‘‰ ğ‘€ğ´ğ‘¡ (ğ‘  ğ‘¡) â‰¥ğ‘‰ ğ‘€âˆ— (ğ‘  ğ‘¡)âˆ’ğœ€istrueforallbut
ğ‘‚Ëœ (cid:18) |ğ‘†|2 |ğ´|/(cid:16) ğœ€3 (1âˆ’ğ›¾)6(cid:17)(cid:19)
timesteps.
G.1 CaseI
TheoremG.2. Supposethat0â‰¤ğœ€ < 1âˆ’1 ğ›¾ and0â‰¤ğ›¿ <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©asdiscretisedversionofğ‘€ (asdescribedinCaseI)usinggridsizeofğœ†.There
existsinputsğ‘š=ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,satisfying
(cid:18) âˆš (cid:19)ğ‘›|ğ‘†||ğ´|
(cid:169)
2 ğ‘›|ğ‘†||ğ´|
+1 (cid:170)
ğ‘š(cid:18)1 ,1(cid:19) =ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
ğœ€ ğ›¿ (cid:173) ğœ€2(1âˆ’ğ›¾)4 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€withinputsğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicyattimeğ‘¡
andğ‘ Ë† ğ‘¡ denotethestateattimeğ‘¡.Withprobabilityatleast1âˆ’ğ›¿,ğ‘‰ ğ‘€âˆ—
ğ‘‘
(ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€A ğ‘‘ğ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€istrueforallbut
(cid:18) âˆš (cid:19)2ğ‘›|ğ‘†||ğ´| (cid:18) âˆš (cid:19)|ğ‘†||ğ´|
(cid:169)
2 ğ‘›|ğ‘†||ğ´|
+1
2 |ğ‘†||ğ´|
+1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† ğœ† (cid:174)
(cid:174)
(cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. WefirstpluginEquations10and9intoTheoremG.1.Droppingthelogarithmtermsandplugginginğ‘‰max= 1âˆ’1 ğ›¾,weobtainthe
results. â–¡
G.2 CaseII
TheoremG.3. Supposethat0â‰¤ğœ€ < 1âˆ’1 ğ›¾ and0â‰¤ğ›¿ <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©asdiscretisedversionofğ‘€ (asdescribedinCaseI)usinggridsizeofğœ†.There
existsinputsğ‘š=ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,satisfying
ğ‘š(cid:18)1 ,1(cid:19)
=ğ‘‚Ëœ
(cid:32) (|ğ‘†||ğ´|)ğ‘›â„(cid:33)
ğœ€ ğ›¿ ğœ€2(1âˆ’ğ›¾)4
and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€withinputsğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicyattimeğ‘¡
andğ‘ Ë† ğ‘¡ denotethestateattimeğ‘¡.Withprobabilityatleast1âˆ’ğ›¿,ğ‘‰ ğ‘€âˆ—
ğ‘‘
(ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€A ğ‘‘ğ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€istrueforallbut
(cid:18) âˆš (cid:19)|ğ‘†||ğ´|
(cid:169)(|ğ‘†||ğ´|)2ğ‘›â„ 2 |ğ‘†||ğ´| +1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
(cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. WefirstpluginEquations11and9intoTheoremG.1.Droppingthelogarithmtermsandplugginginğ‘‰max= 1âˆ’1 ğ›¾,weobtainthe
results. â–¡
H Step5:TheBoundbetweenğ‘€ andğ‘€ ğ‘‘
TheoremH.1. (MDPDiscretizationBound[7])ThereexistsaconstantK(thatsdependsonlyontheLipschitzconstantL)suchthatforsome
discretisationcoarsenessğœ†âˆˆ (0, L 2]suchthat
(cid:13) (cid:13) Kğœ†
(cid:13)ğ‘‰âˆ— âˆ’ğ‘‰âˆ— (cid:13) â‰¤ .
(cid:13) ğ‘€ ğ‘€ğ‘‘(cid:13)
âˆ
(1âˆ’ğ›¾Ë†)2
H.1 SampleComplexityAnalysis
I Step6:Addingittogether
LemmaI.1(SimulationLemmaforContinuousMDP). Letğ‘€ andğ‘€Ë† betwoMDPsthatonlydifferin (ğ‘‡,ğ‘…) and (ğ‘‡Ë†,ğ‘…Ë† ).Andsupposethe
commonstatespaceğ‘†Ë† ofğ‘€andğ‘€Ë† iscontinuous,anddenotethecommonactionspaceasğ´Ë†
.
Letğœ– ğ‘… â‰¥max ğ‘ ,ğ‘|ğ‘…Ë† (ğ‘ ,ğ‘)âˆ’ğ‘…(ğ‘ ,ğ‘)|andğœ€ ğ‘ â‰¥max ğ‘ ,ğ‘âˆ¥ğ‘‡Ë† (Â·|ğ‘ ,ğ‘)âˆ’ğ‘‡(Â·|ğ‘ ,ğ‘)||1Â¶ .Thenâˆ€ğœ‹ :ğ‘†Ë† â†’ğ´Ë† ,
(cid:13) (cid:13)ğ‘‰ğœ‹ âˆ’ğ‘‰ğœ‹(cid:13) (cid:13) â‰¤ ğœ€ ğ‘… +ğ›¾ğœ– ğ‘ƒğ‘‰max .
(cid:13) ğ‘€ ğ‘€Ë†(cid:13) âˆ 1âˆ’ğ›¾ 2(1âˆ’ğ›¾)
Â¶Notethatgivenğ‘‡(Â·|ğ‘ ,ğ‘),ğ‘‡Ë†(Â·|ğ‘ ,ğ‘)arefunctionsoncontinuousspaceğ‘†Ë†,thisistheğ¿1normdefinedbyâˆ¥ğ‘“âˆ¥1=âˆ« ğ‘†Ë†|ğ‘“|ğ‘‘ğœ‡.Similarly,throughouttheproof,wedenoteasâˆ¥âˆ¥ğ‘the
ğ¿ğ‘norm,definedbyâˆ¥ğ‘“âˆ¥ğ‘=(âˆ« ğ‘†Ë†|ğ‘“|ğ‘ğ‘‘ğœ‡)1/ğ‘;andtheinnerproductâŸ¨ğ‘“,ğ‘”âŸ©=âˆ« ğ‘†Ë†ğ‘“ğ‘”ğ‘‘ğœ‡.Proof. Forallğ‘  âˆˆğ‘†Ë†,
(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë†(ğ‘ )âˆ’ğ‘‰ ğ‘€ğœ‹ (ğ‘ )(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12)ğ‘…Ë† (ğ‘ ,ğœ‹)+ğ›¾(cid:68) ğ‘‡Ë† (Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69) âˆ’ğ‘…(ğ‘ ,ğœ‹)âˆ’ğ›¾(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹(cid:69)(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)
â‰¤ |ğ‘…Ë† (ğ‘ ,ğœ‹)âˆ’ğ‘…(ğ‘ ,ğœ‹)|+ğ›¾(cid:12) (cid:12) (cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69) âˆ’(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹(cid:69)(cid:12) (cid:12)
(cid:12)
(triangularinequality)
â‰¤ğœ€ ğ‘…+ğ›¾ (cid:18)(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69) âˆ’(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69) +(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69) âˆ’(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)(cid:19) (add&subtract) (15)
(cid:12) (cid:12) (cid:12) (cid:12)
â‰¤ğœ€ ğ‘…+ğ›¾(cid:12)
(cid:12)
(cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69)(cid:12)
(cid:12)
(cid:12)+ğ›¾(cid:12)
(cid:12)
(cid:12)(cid:68) ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹
Ë†
âˆ’ğ‘‰ ğ‘€ğœ‹(cid:69)(cid:12)
(cid:12)
(cid:12)
â‰¤ğœ€ ğ‘…+ğ›¾(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)+ğ›¾(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’ğ‘‰ ğ‘€ğœ‹(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
.
âˆ
SinceEquation15holdsforallğ‘ Ë†âˆˆğ‘†Ë†,wecantaketheinfinite-normonthelefthandside:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’ğ‘‰ ğ‘€ğœ‹(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
â‰¤ğœ€ ğ‘…+ğ›¾(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)+ğ›¾(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’ğ‘‰ ğ‘€ğœ‹(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
. (16)
âˆ âˆ
Wethenexpandthemiddletermasfollows:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:68) ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë†(cid:69)(cid:12) (cid:12) (cid:12) (cid:12)=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:28) ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹),ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’1Â· 2(ğ‘… 1m âˆ’ax ğ›¾)(cid:29)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(where1isavectorofonesâˆˆR|ğ‘†|)
â‰¤ âˆ¥ğ‘‡Ë† (Â·,ğœ‹)âˆ’ğ‘‡(Â·,ğœ‹)âˆ¥1Â·(cid:13) (cid:13) (cid:13) (cid:13)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’1Â· 2(ğ‘… 1m âˆ’ax ğ›¾)(cid:13) (cid:13) (cid:13) (cid:13) (Holderâ€™sinequality) (17)
âˆ
ğ‘…max
â‰¤ğœ– ğ‘ƒ Â· 2(1âˆ’ğ›¾)
ğ‘‰max
=ğœ– ğ‘ƒ Â· 2 .
InEquation17,thefirststepshiftstherangeofğ‘‰ from[0, ğ‘…max ]to[âˆ’ ğ‘…max , ğ‘…max ]toobtainatighterboundbyafactorof2.Theequality
(1âˆ’ğ›¾) 2(1âˆ’ğ›¾) 2(1âˆ’ğ›¾)
inline1holdsbecauseofthefollowing,whereğ¶isanyconstant:
âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ¶Â·1âŸ©=ğ¶âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,1âŸ©
=ğ¶(cid:16) âŸ¨ğ‘ƒË†,1âŸ©âˆ’âŸ¨ğ‘ƒ,1âŸ©(cid:17)
(18)
=ğ¶(1âˆ’1) becauseğ‘ƒ andğ‘ƒË†areprobabilitydistributions
=0
Fromequation18,weobservetheequalityinline1holds:
âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ‘‰ âˆ’ğ¶Â·1âŸ©=âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ‘‰âŸ©âˆ’âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ¶Â·1âŸ©
=âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ‘‰âŸ©âˆ’0 (19)
=âŸ¨ğ‘‡Ë† âˆ’ğ‘‡,ğ‘‰âŸ©.
Finally,weplugequation17intoequation16toobtainthebound.
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’ğ‘‰ ğ‘€ğœ‹(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
â‰¤ğœ– ğ‘…+ğ›¾ğœ– ğ‘ƒ Â·ğ‘‰m 2ax +ğ›¾(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)ğ‘‰ ğ‘€ğœ‹ Ë† âˆ’ğ‘‰ ğ‘€ğœ‹(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)
(cid:12)
âˆ âˆ (20)
=
ğœ–
ğ‘…
+ğ›¾ğœ– ğ‘ƒğ‘‰max
.
1âˆ’ğ›¾ 2(1âˆ’ğ›¾)
â–¡
Underdiscretisation,[4]showedthat,withasmallenoughgridsize,andrestrictingtothediscretisedactionspace,thedifferenceintransition
probabilityofthecontinuousMDPğ‘€anddiscretisedMDPğ‘€ isupperboundedbyaconstant.
ğ‘‘LemmaI.2. [4]Thereexistsaconstantğ¾ ğ‘ƒ (dependingonlyonconstantL)suchthat
|ğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ’ğ‘‡(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)| â‰¤ğ¾ ğ‘ğ›¼
forallğ‘ Ë†â€²,ğ‘ Ë†âˆˆğ‘†Ë†,ğ‘Ë† ğ‘‘ âˆˆğ´Ë† ğ‘‘ andallğ›¼ â‰¤ (0, 21 L]
Wenowapplysimulationlemmaboundthedifferenceinvalueforanydiscretisedpolicy(i.e.restrictingactionspacetoğ´Ë† )inthecontinuous
ğ‘‘
MDPğ‘€anddiscretisedMDPğ‘€ .
ğ‘‘
LemmaI.3. Letğ‘€ ğ´Ë†
ğ‘‘
=(ğ‘†Ë†,ğ´Ë† ğ‘‘,ğ‘‡,ğ‘…,ğ›¾),thatis,thecontinousMDPğ‘€restrictedtothediscretisedactionspace.AndrecallthatdiscretisedMDP
ğ‘€ ğ‘‘ =(ğ‘†Ë†,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾).Thenforanydiscretisedpolicyğœ‹ :ğ‘†Ë† â†’ğ´Ë† ğ‘‘,
âˆ¥ğ‘‰ ğ‘€ğœ‹
ğ´Ë†
ğ‘‘
âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ=âˆ¥ğ‘‰ ğ‘€ğœ‹ âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ â‰¤ L 1âˆ’Rğ›¼
ğ›¾
+ (1ğ›¾ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
Proof. LemmaI.2givestheboundfordifferenceintransitionprobability
ğœ– ğ‘ƒ =maxâˆ¥ğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ’ğ‘‡(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ¥1=max2ğ‘‡ğ‘‰(ğ‘‡ ğ‘‘(Â·|ğ‘ Ë†,ğ‘Ë† ğ‘‘),ğ‘‡(Â·|ğ‘ Ë†,ğ‘Ë† ğ‘‘)) â‰¤2max|ğ‘‡ ğ‘‘(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ’ğ‘‡(ğ‘ Ë†â€²|ğ‘ Ë†,ğ‘Ë† ğ‘‘)| â‰¤2ğ¾ ğ‘ğ›¼
ğ‘ Ë†,ğ‘Ë†
ğ‘‘
ğ‘ Ë†,ğ‘Ë†
ğ‘‘
ğ‘ Ë†â€²
WeupperboundthedifferenceinrewardusingourLipschitzassumption:
ğœ– ğ‘… =max|ğ‘… ğ‘‘(ğ‘ Ë†,ğ‘Ë† ğ‘‘)âˆ’ğ‘…(ğ‘ Ë†,ğ‘Ë† ğ‘‘)|
ğ‘ Ë†,ğ‘Ë†
ğ‘‘
=max|ğ‘…(ğ‘ Ë† ğ‘‘,ğ‘Ë† ğ‘‘)âˆ’ğ‘…(ğ‘ Ë†,ğ‘Ë† ğ‘‘)|
ğ‘ Ë†,ğ‘Ë†
ğ‘‘
â‰¤L Râˆ¥ğ‘ Ë† ğ‘‘ âˆ’ğ‘ Ë†âˆ¥âˆ
=L Rğ›¼
Theboundholdsbyapplyingsimulationlemmatoğ‘€ andğ‘€ withğœ– andğœ– above:
ğ´Ë†
ğ‘‘
ğ‘‘ ğ‘ƒ ğ‘…
âˆ¥ğ‘‰ ğ‘€ğœ‹
ğ´Ë†
ğ‘‘
âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ=âˆ¥ğ‘‰ ğ‘€ğœ‹ âˆ’ğ‘‰ ğ‘€ğœ‹ ğ‘‘âˆ¥âˆ â‰¤ L 1âˆ’Rğ›¼
ğ›¾
+ (1ğ›¾ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
â–¡
I.1 CaseI
TheoremI.4. Supposethat0â‰¤ğœ€ < 1âˆ’1 ğ›¾ and0â‰¤ğ›¿ <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©asdiscretisedversionofğ‘€ (asdescribedinCaseI)usinggridsizeofğœ†.There
existsinputsğ‘š=ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,satisfying
(cid:18) âˆš (cid:19)ğ‘›|ğ‘†||ğ´|
(cid:169)
2 ğ‘›|ğ‘†||ğ´|
+1 (cid:170)
ğ‘š(cid:18)1 ,1(cid:19) =ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
ğœ€ ğ›¿ (cid:173) ğœ€2(1âˆ’ğ›¾)4 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€withinputsğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicyattimeğ‘¡
andğ‘ Ë†
ğ‘¡
denotethestateattimeğ‘¡.Withprobabilityatleast1âˆ’ğ›¿,
ğ‘‰ ğ‘€âˆ— (ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€Ağ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€+ (1K âˆ’ğœ†
ğ›¾Ë†)2
+ L 1âˆ’Rğ›¼
ğ›¾
+ (1ğ›¾ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
istrueforallbut
(cid:18) âˆš (cid:19)2ğ‘›|ğ‘†||ğ´| (cid:18) âˆš (cid:19)|ğ‘†||ğ´|
(cid:169)
2 ğ‘›|ğ‘†||ğ´|
+1
2 |ğ‘†||ğ´|
+1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† ğœ† (cid:174)
(cid:174)
(cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.Proof. TheresultfollowsfromaddingTheoremG.2,I.2,andLemmaI.3. â–¡
I.2 CaseII
TheoremI.5. Supposethat0â‰¤ğœ€ < 1âˆ’1 ğ›¾ and0â‰¤ğ›¿ <1aretworealnumbers.LetMbeanycontinuousmeta-MDPwithinnerstochasticgame
ğº = âŸ¨ğ‘†,ğ´,ğ‘‡ inner,ğ‘… innerâŸ©.Letusdenoteğ‘€ ğ‘‘ = âŸ¨ğ‘†Ë† ğ‘‘,ğ´Ë† ğ‘‘,ğ‘‡ ğ‘‘,ğ‘… ğ‘‘,ğ›¾âŸ©asdiscretisedversionofğ‘€ (asdescribedinCaseI)usinggridsizeofğœ†.There
existsinputsğ‘š=ğ‘š(cid:16) ğœ€1, ğ›¿1(cid:17) andğœ€1,satisfying
ğ‘š(cid:18)1 ,1(cid:19)
=ğ‘‚Ëœ
(cid:32) (|ğ‘†||ğ´|)ğ‘›â„(cid:33)
ğœ€ ğ›¿ ğœ€2(1âˆ’ğ›¾)4
and ğœ€1
1
=ğ‘‚(cid:16) ğœ€1(cid:17) ,suchthatifğ‘…-ğ‘€ğ´ğ‘‹ isexecutedonğ‘€withinputsğ‘šandğœ€1,thenthefollowingholds.LetAğ‘¡ denoteğ‘…-ğ‘€ğ´ğ‘‹â€™spolicyattimeğ‘¡
andğ‘ Ë†
ğ‘¡
denotethestateattimeğ‘¡.Withprobabilityatleast1âˆ’ğ›¿,
ğ‘‰ ğ‘€âˆ— (ğ‘ Ë† ğ‘¡)âˆ’ğ‘‰ ğ‘€Ağ‘¡ (ğ‘ Ë† ğ‘¡) â‰¤ğœ€+ (1K âˆ’ğœ†
ğ›¾Ë†)2
+ L 1âˆ’Rğ›¼
ğ›¾
+ 2(ğ›¾ 1ğ¾ âˆ’ğ‘ ğ›¾ğ›¼
)2
istrueforallbut
(cid:18) âˆš (cid:19)|ğ‘†||ğ´|
(cid:169)(|ğ‘†||ğ´|)2ğ‘›â„ 2 |ğ‘†||ğ´| +1 (cid:170)
ğ‘‚Ëœ(cid:173)
(cid:173)
ğœ† (cid:174)
(cid:174)
(cid:173) ğœ€3(1âˆ’ğ›¾)6 (cid:174)
(cid:173) (cid:174)
(cid:173) (cid:174)
(cid:171) (cid:172)
timesteps.
Proof. ThestepsaresimilartoCaseI. â–¡J ExperimentDetails
J.1 MatchingPenniesTableSummary
Player1\Player2 Head Tail
Head (+1,-1) (-1,+1)
Tail (-1,+1) (+1,-1)
Table4:PayoffMatrixforMP
J.2 ImplementationDetails
TheopponentisastandardQ-learningagentwhoupdatestheQ-valuesateverymeta-stepandselectsanactionthatcorrespondstothe
maximumQ-valueatagivenstate.Toenablebetterempiricalperformance,themeta-agentusesBoltzmannsamplinginsteadofgreedy
samplingtosamplethenextactionfromtheQ-valuetable.Weuseadiscountfactorof0.8.Foreachrun,werunatotalof10Ã—ğ‘šÃ—|SË†
|
R-FOSiterations,where10isourchosenhyper-parameter.