Dynamic Typography: Bringing Words to Life
ZICHENLIUâˆ—,TheHongKongUniversityofScienceandTechnology,China
YIHAOMENGâˆ—,TheHongKongUniversityofScienceandTechnology,China
HAOOUYANG,TheHongKongUniversityofScienceandTechnology,China
YUEYU,TheHongKongUniversityofScienceandTechnology,China
BOLINZHAO,TheHongKongUniversityofScienceandTechnology,China
DANIELCOHEN-ORâ€ ,Tel-AvivUniversity,Israel
HUAMINQUâ€ ,TheHongKongUniversityofScienceandTechnology,China
Fig.1. Givenaletterandatextpromptthatbrieflydescribestheanimation,ourmethodautomaticallysemanticallyreshapesaletter,andanimatesitin
vectorformatwhilemaintaininglegibility.Ourapproachallowsforavarietyofcreativeinterpretationsthatcandynamicallybringwordstolife.
1 INTRODUCTION
Textanimationservesasanexpressivemedium,transformingstaticcom-
municationintodynamicexperiencesbyinfusingwordswithmotionto Textanimationistheartofbringingtexttolifethroughmotion.By
evokeemotions,emphasizemeanings,andconstructcompellingnarratives.
animatingtexttoconveyemotion,emphasizemeaning,andcreate
Craftinganimationsthataresemanticallyawareposessignificantchallenges,
adynamicnarrative,textanimationtransformsstaticmessagesinto
demandingexpertiseingraphicdesignandanimation.Wepresentanau-
vivid,interactiveexperiences[Leeetal.2006,2002b].Thefusion
tomatedtextanimationscheme,termedâ€œDynamicTypographyâ€,which
ofmotionandtext,notonlycaptivatesviewers,butalsodeepens
combinestwochallengingtasks.Itdeformsletterstoconveysemanticmean-
ingandinfusesthemwithvibrantmovementsbasedonuserprompts.Our the messageâ€™s impact, making text animation prevail in movies,
techniqueharnessesvectorgraphicsrepresentationsandanend-to-end advertisements,websitewidgets,andonlinememes[Xieetal.2023].
optimization-basedframework.Thisframeworkemploysneuraldisplace- Thispaperintroducesaspecializedtextanimationschemethat
mentfieldstoconvertlettersintobaseshapesandappliesper-framemotion, focusesonanimatingindividualletterswithinwords.Thisanima-
encouragingcoherencewiththeintendedtextualconcept.Shapepreserva- tionisacompoundtask:Thelettersaredeformedtoembodytheir
tiontechniquesandperceptuallossregularizationareemployedtomaintain semanticmeaningandthenbroughttolifewithvividmovements
legibilityandstructuralintegritythroughouttheanimationprocess.We
basedontheuserâ€™sprompt.Werefertoitasâ€œDynamicTypographyâ€.
demonstratethegeneralizabilityofourapproachacrossvarioustext-to-
Forexample,theletterâ€œMâ€inâ€œCAMELâ€canbeanimatedwiththe
videomodelsandhighlightthesuperiorityofourend-to-endmethodology
promptâ€œAcamelwalkssteadilyacrossthedesertâ€asillustratedin
overbaselinemethods,whichmightcompriseseparatetasks.Throughquan-
Fig.1.Thisanimationschemeopensupanewdimensionoftextual
titativeandqualitativeevaluations,wedemonstratetheeffectivenessof
ourframeworkingeneratingcoherenttextanimationsthatfaithfullyinter- animationthatenrichestheuserâ€™sreadingexperience.
pretuserpromptswhilemaintainingreadability.Ourcodeisavailableat: However,craftingsuchdetailedandprompt-awareanimationsis
https://animate-your-word.github.io/demo/. challenging,astraditionaltextanimationmethodsdemandconsid-
erableexpertiseingraphicdesignandanimation[Leeetal.2002b],
âˆ—Denotesequalcontribution.
â€ Denotescorrespondingauthor. making them less accessible to non-experts. The technique we
presentaimstoautomatethetextanimationprocesstomakeit
Authorsâ€™addresses:ZichenLiu,TheHongKongUniversityofScienceandTechnology,
moreaccessibleandefficient.Followingpriorresearchinfontgen-
HongKong,China,zliucz@connect.ust.hk;YihaoMeng,TheHongKongUniversityof
ScienceandTechnology,HongKong,China,ymengas@connect.ust.hk;HaoOuyang, erationandstylization[Iluzetal.2023;Lopesetal.2019;Wangand
TheHongKongUniversityofScienceandTechnology,HongKong,China,houyangab@ Lian2021],werepresenteachinputletterandeveryoutputframe
connect.ust.hk;YueYu,TheHongKongUniversityofScienceandTechnology,Hong
asavectorized,closedshapebyacollectionofBÃ©ziercurves.This
Kong,China,yue.yu@connect.ust.hk;BolinZhao,TheHongKongUniversityofScience
andTechnology,HongKong,China,bzhaoan@connect.ust.hk;DanielCohen-Or,Tel- vectorrepresentationisresolution-independent,ensuringthattext
AvivUniversity,TelAviv,Israel,cohenor@gmail.com;HuaminQu,TheHongKong remainsclearandsharpregardlessofscale,andbringssubstantial
UniversityofScienceandTechnology,HongKong,China,huamin@ust.hk.
4202
rpA
71
]VC.sc[
1v41611.4042:viXra2 â€¢ ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
editabilitybenefitsasuserscaneasilymodifythetextâ€™sappearance stylisticelementsfromasourceimageontotext.Existingworkin-
byadjustingcontrolpoints.However,thisshifttovectorgraph- corporatestexturesynthesis[Fishetal.2020;Yangetal.2016]with
icsintroducesuniquechallengesintextanimation.Mostcurrent generativemodelslikeGANs[Azadietal.2018;Jiangetal.2019;
image-to-videomethods[Guoetal.2024;Nietal.2023;Wangetal. Maoetal.2022;Wangetal.2019].Semantictypographyrefersto
2024;Xingetal.2023]fallshortinthisnewscenarioastheyare techniquesthatblendsemanticunderstandingandvisualrepresenta-
designedtoanimaterasterizedimagesinsteadofvectorizedshapes, tionintypography.Thisencompassesturninglettersorwordsinto
andarehardtorenderreadabletext.Althoughthemostrecentwork, visualformsthatconveytheirmeaningornature,integratingtypog-
LiveSketch[Galetal.2023],introducesanapproachtoanimatear- raphywithsemanticstoenhancethemessageâ€™sclarityandimpact.
bitraryvectorizedsketches,itstrugglestopreservelegibilityand Forinstance,[Iluzetal.2023]leveragesScoreDistillationSampling
consistencyduringanimationwhentheinputbecomesvectorized [Pooleetal.2023]todeformlettersbasedonthepre-traineddiffu-
letters,causingvisuallyunpleasantartifactsincludingflickeringand sionprior[Rombachetal.2022],encouragingtheappearanceofthe
distortion. lettertoconveythewordâ€™ssemanticmeaning.[Tanveeretal.2023]
Toaddressthesechallenges,wedesignedanoptimization-based utilizesalatentdiffusionprocesstoconstructthelatentspaceofthe
end-to-endframeworkthatutilizestwoneuraldisplacementfields, givensemantic-relatedstyleandthenintroducesadiscriminatorto
representedincoordinates-basedMLP.Thefirstfielddeformsthe blendthestyleintotheglyphshape.
originalletterintothebaseshape,settingthestageforanimation. Theseworksonlyproducestaticimages,whichinmanycases
Subsequently,thesecondneuraldisplacementfieldlearnstheper- struggletovividlyandeffectivelycommunicatemeaningfulseman-
framemotionappliedtothebaseshape.Thetwofieldsarejointly ticmessages.Incontrast,ourproposedâ€œDynamicTypographâ€in-
optimizedusingthescore-distillationsampling(SDS)loss[Poole fusestextwithvibrantmotions,whichismoreeffectiveincapturing
etal.2023]tointegratemotionpriorsfromapre-trainedtext-to- theuserâ€™sattentionandgivinganaestheticallypleasingimpression
video model [Wang et al. 2023], to encourage the animation to comparedtostatictext[MinakuchiandKidawara2008].
align with the intended textual concept. We encode the control
pointcoordinatesoftheBÃ©ziercurveintohigh-frequencyencoding 2.2 DynamicTextAnimation
[Mildenhalletal.2021]andadoptcoarse-to-finefrequencyanneal-
Giventheeffectivenessofanimationsincapturingandretaining
ing[Parketal.2021]tocapturebothminuteandlargemotions.To
audienceattention[ChangandUngar1993],severalstudieshave
preservethelegibilityoftheletterthroughouttheanimation,weap-
embarkedondesigningdynamictextanimations.Anotablearea
plyperceptualloss[Zhangetal.2018]asaformofregularizationon
isdynamicstyletransfer,whichaimstoadaptthevisualstyleand
thebaseshape,tomaintainaperceptualresemblancetotheoriginal
motionpatternsfromareferencevideotothetargettext.Pioneering
letter.Additionally,topreservetheoverallstructureandappear-
workby[Menetal.2019]transferredastylefromasourcevideo
anceduringanimation,weintroduceanovelshapepreservation
displayingdynamictextanimationsontotargetstatictext.[Yang
regularizationbasedonthetriangulation[HormannandGreiner
et al. 2021] further enhanced versatility by using a scale-aware
2000]ofthebaseshape,whichforcesthedeformationbetweenthe
Shape-MatchingGANtohandlediverseinputstyles.
consecutiveframestoadheretotheprincipleofbeingconformal
Kinetictypography[Fordetal.1997]representsanotherinno-
withrespecttothebaseshape.
vativedirectionintextanimation,whichintegratesmotionwith
Ourapproachisdesignedtobedata-efficient,eliminatingtheneed
texttoconveyorenhanceamessage.Creatingkinetictypography
foradditionaldatacollectionorthefine-tuningoflarge-scalemodels.
isalabor-intensiveandchallengingtask,motivatingmanyworks
Furthermore,ourmethodgeneralizeswelltovarioustext-to-video
toreducetheburdenanddemocratizethistechniquetothepublic
models,enablingtheincorporationofupcomingdevelopmentsin
[Fordetal.1997;Forlizzietal.2003;Leeetal.2002a;Minakuchi
thisarea.Wequantitativelyandqualitativelytestedourtextan-
andTanaka2005].RecentadvancementsinAIhaveenabledafully
imationgenerationmethodagainstvariousbaselineapproaches,
automatedgenerationofkinetictypography.Forexample,[Xieetal.
usingabroadspectrumofprompts.Theresultsdemonstratethat
2023]utilizesapre-trainedmotiontransfermodel[Siarohinetal.
thegeneratedanimationnotonlyaccuratelyandaestheticallyin-
2019]toapplyanimationpatternsfromamemeGIFontotext.
terpretstheinputtextpromptdescriptions,butalsomaintainsthe
However,theseapproachesrequirestrictlyspecifieddrivenvideos,
readabilityoftheoriginaltext,outperformingvariousbaselinemod-
whicharedifficulttoobtaininreal-lifescenarios,significantlyre-
elsinpreservinglegibilityandprompt-videoalignment.Overall,
stricting their usability and generalizability. Moreover, they are
ourframeworkdemonstratesitsefficacyinproducingcoherenttext
constrainedtogeneratespecificsimplemotionpatterns,limiting
animationsfromuserprompts,whilemaintainingthereadabilityof
theirabilitytoproduceanimationswitharbitrarycomplexsemantic
thetext,whichisachievedbythekeydesignofthelearnablebase
information.Incontrast,ourmethodisgeneralizabletoarbitrary
shapeandassociatedshapepreservationregularization.
motionpatternsandonlyneedsatextpromptastheinput.
2 RELATEDWORK
2.3 TextandImage-to-VideoGeneration
2.1 StaticTextStylization
Text-to-Videogenerationaimsatautomaticallyproducingcorre-
Textstylizationfocusesonamplifyingtheaestheticqualitiesoftext spondingvideosbasedontextualdescriptions.Recentadvancements
whilemaintainingreadability,includingartistictextstyletransfer indiffusionmodelshavesignificantlyimprovedvideogeneration
andsemantictypography.Artistictextstyletransferaimstomigrate capabilities.MainstreamapproachesleveragethepowerofStableDynamicTypography:BringingWordstoLife â€¢ 3
Fig.2. Anoverviewofthemodelarchitecture.Givenaletterrepresentedasasetofcontrolpoints,theBaseFielddeformsittothesharedbaseshape,setting
thestagetoaddper-framedisplacement.Thenweduplicatethebaseshapeacrossğ‘˜framesandutilizetheMotionFieldtopredictdisplacementsforeach
controlpointateachframe,infusingmovementtothebaseshape.Everyframeisthenrenderedbythedifferentiablerasterizerğ‘…andconcatenatedas
theoutputvideo.Thebaseandmotionfieldarejointlyoptimizedbythevideoprior(ğ¿ )fromfrozenpre-trainedvideofoundationmodelusingScore
ğ‘†ğ·ğ‘†
DistillationSampling,underregularizationonlegibilityğ¿ ğ‘™ğ‘’ğ‘”ğ‘–ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦andstructurepreservationğ¿ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’.
Diffusion(SD)[Rombachetal.2022]byincorporatingtemporalin- B-Splinecurves,enablingscalableandflexibletextrendering,which
formationinalatentspace,includingAnimateDiff[Guoetal.2024], weaimtopreserve.Ourmethodoutputseachanimationframein
LVDM[Heetal.2022],MagicVideo[Zhouetal.2022],VideoCrafter thesamevectorrepresentationsasourinput.
[Chenetal.2023]andModelScope[Wangetal.2023].Beyondtext-
to-video,somemethodsattempttogeneratevideosfromagiven
imageandapromptasthecondition,suchasDynamiCrafter[Xing
etal.2023]andMotion-I2V[Shietal.2024].Severalstartupsalso
releasetheirimage-to-videogenerationservices,e.g.,Gen-2[con-
tributors2023a],PikaLabs[contributors2023b],andStableVideo Fig.3. BÅºeiercurvesrepresentationofletterâ€œBâ€
Diffusion(SVD)[Blattmannetal.2023].
Despiteprogress,open-sourcevideogenerationmodelsstruggle Inalignmentwiththesettingoutlinedin[Iluzetal.2023],weuse
tomaintaintextreadabilityduringmotion,letalonecreatevividtext theFreeType[DavidTurner2009]fontlibrarytoextracttheoutlines
animations.Trainingamodelcapableofgeneratinghigh-quality, ofthespecifiedletter.Subsequently,theseoutlinesareconverted
legibletextanimationsusingtheaforementionedmethodswouldre- into a closed curve composed of several cubic BÃ©zier curves, as
quirealargedatasetoftextanimations,whichisdifficulttorequire illustrated in Fig. 3, to achieve a coherent representation across
inpractice.OnerecentworkLiveSketch[Galetal.2023]introduces differentfontsandletters.
anapproachtoanimatearbitraryvectorizedsketcheswithoutex-
3.2 ScoreDistillationSampling
tensivetraining.Thisworkleveragesthemotionpriorfromalarge
pre-trainedtext-to-videodiffusionmodelusingscoredistillation TheobjectiveofScoreDistillationSampling(SDS),originallyin-
sampling[Pooleetal.2023]toguidethemotionofinputsketches. troducedintheDreamFusion[Pooleetal.2023],istoleveragepre-
However,whentheinputbecomesvectorizedletters,LiveSketch traineddiffusionmodelsâ€™priorknowledgeforthetext-conditioned
strugglestopreservelegibilityandconsistencyduringanimation, generationofdifferentmodalities[Katziretal.2024].SDSoptimizes
leadingtoflickeringanddistortionartifactsthatseverelydegrade theparametersğœƒ oftheparametricgeneratorG(e.g.,NeRF[Milden-
videoquality.Incontrast,ourproposedmethodsuccessfullygener- hall et al. 2021]), ensuring the output of G aligns well with the
atesconsistentandprompt-awaretextanimationswhilepreserving prompt.Forillustration,assumingGisaparametricimagegenera-
thetextreadability.
tor.First,animageğ‘¥ =G(ğœƒ)isgenerated.Next,anoiseimageğ‘§ ğœ(ğ‘¥)
isobtainedbyaddingaGaussiannoiseğœ–atthediffusionprocessâ€™s
3 PRELIMINARY ğœ-thtimestep:
3.1 VectorRepresentationandFonts
ğ‘§ ğœ(ğ‘¥)=ğ›¼ ğœğ‘¥+ğœ ğœğœ–, (1)
whereğ›¼ ğœ,andğœ
ğœ
arediffusionmodelâ€™snoisingschedule,andğœ–isa
Vectorgraphicscreatevisualimagesdirectlyfromgeometricshapes noisesamplefromthenormaldistributionN(0,1).
likepoints,lines,curves,andpolygons.Unlikerasterimages(like Forapre-traineddiffusionmodelğœ– ğœ™,thegradientoftheSDSloss
PNGandJPEG),whichstoredataforeachpixel,vectorgraphicsare
Lğ‘†ğ·ğ‘† isformulatedas:
nottiedtoaspecificresolution,makingtheminfinitelyscalableand
(cid:20) ğœ•ğ‘¥(cid:21)
moreeditable[Ferraioloetal.2000]. âˆ‡ğœ™Lğ‘†ğ·ğ‘† = ğ‘¤(ğœ)(ğœ– ğœ™(ğ‘§ ğœ(ğ‘¥);ğ‘¦,ğœ)âˆ’ğœ–)
ğœ•ğœƒ
, (2)
Hence,modernfontformatslikeTrueType[Penny1996]andPost-
Script[AdobeSystemsInc.1990]utilizevectorgraphicstodefine whereğ‘¦istheconditioninginputtothediffusionmodelandğ‘¤(ğœ)is
glyphoutlines.TheseoutlinesaretypicallycollectionsofBÃ©zieror aweightingfunction.Thediffusionmodelpredictsthenoiseadded4 â€¢ ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
totheimageğ‘¥ withğœ– ğœ™(ğ‘§ ğœ(ğ‘¥);ğ‘¦,ğœ).Thediscrepancybetweenthis
predictionandtheactualnoiseğœ–measuresthedifferencebetween
theinputimageandonethatalignswiththetextprompt.Inthis
work,weadoptthisstrategytoextractthemotionpriorfromthe
pre-trainedtext-to-videodiffusionmodel[Wangetal.2023].
SinceSDSisusedwithrasterimages,weutilizeDiffVG[Lietal.
2020]asadifferentiablerasterizer.Thisallowsustoconvertour
vector-definedcontentintopixelspaceinadifferentiablewayfor Fig.4. Illustrationofthepriorknowledgeconflictissue.Theleftisthe
applyingtheSDSloss. deformedâ€œRâ€forBULLFIGHTERwithpromptâ€œAbullfighterholdsthe
cornersofaredcapeinbothhandsandwavesitâ€generatedby[Iluzetal.
4 METHOD 2023],therightisgeneratedby[Galetal.2023]toanimatethedeformed
letterwiththesameprompt.Themismatchinpriorknowledgebetween
ProblemFormulation.DynamicTypographyfocusesonanimat- separatemodelsleadstosignificantappearancechangesandsevereartifacts,
ingindividualletterswithinwordsbasedontheuserâ€™sprompt.The ashighlightedbytheredcircles.
letterisdeformedtoembodythewordâ€™ssemanticmeaningandthen
broughttolifebyinfusingmotionbasedontheuserâ€™sprompt. similaritytomaintainletterlegibility(Â§4.2).Then,weintroducea
TheoriginalinputletterisinitializedasacubicBÃ©ziercurvescon- mesh-basedstructurepreservationlosstoensureappearanceand
trolpointsset(Fig.3),denotedasğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ ={ğ‘ ğ‘–} ğ‘–ğ‘ =1={(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–)} ğ‘–ğ‘ =1 âˆˆ structureintegritybetweenframes,mitigatingissuessuchasflicker-
Rğ‘Ã—2,whereğ‘¥,ğ‘¦referstocontrolpointsâ€™coordinatesinSVGcan- ingartifacts(Â§4.3).Finally,weutilizefrequency-basedencodingand
vas,ğ‘ referstothetotalnumberofcontrolpointsoftheindicated coarse-to-fineannealingtoimprovetherepresentationofgeometry
informationandmotionquality(Â§4.4).
letter.Theoutputvideoconsistsofkframes,eachrepresentedbya
setofcontrolpoints,denotedasğ‘‰ ={ğ‘ƒğ‘¡}ğ‘˜
ğ‘¡=1
âˆˆRğ‘Â·ğ‘˜Ã—2,whereğ‘ƒğ‘¡
4.1 BaseFieldandMotionField
isthecontrolpointsforğ‘¡-thframe.
Ourgoalistolearnadisplacementforeachframe,addedonthe Learningtheper-framedisplacementthatdirectlyconvertstheinput
setofcontrolpointcoordinatesoftheoriginalletterâ€™soutline.This letterintoanimationframesischallenging.Thevideopriorderived
displacementrepresentsthemotionofthecontrolpointsovertime, from foundational text-to-video models using Score Distillation
creatingtheanimationthatdepictstheuserâ€™sprompt.Wedenotethe Sampling(SDS)isinsufficientlyrobusttoguidetheoptimization,
displacementforğ‘¡-thframeasÎ”ğ‘ƒğ‘¡ ={Î”ğ‘ ğ‘–ğ‘¡} ğ‘–ğ‘ =1={(Î”ğ‘¥ ğ‘–ğ‘¡,Î”ğ‘¦ ğ‘–ğ‘¡)} ğ‘–ğ‘
=1
âˆˆ leadingtosevereartifactsthatdegradethequalityoftheanimation,
Rğ‘Ã—2,whereÎ”ğ‘ ğ‘–ğ‘¡ referstothedisplacementoftheğ‘–-thcontrolpoint includingdistortion,flickering,andabruptappearancechangesin
intheğ‘¡-thframe.Thefinalvideocanbederivedasğ‘‰ ={ğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ + theadjacentframe.InspiredbytheCoDeF[Ouyangetal.2023],we
Î”ğ‘ƒğ‘¡}ğ‘˜
ğ‘¡=1.
p fir eo ldp so :s te hem bo ad se eli fin eg ldth ae ndge tn he er mat oed tiov nid fie eo ldin ,tt ow ao ddn re eu sr sa tl hd eis cp ol mac pe lm exe in tyt
Toachieveappealingresults,weidentifythreecrucialrequire-
ofthisdeformation.Bothfieldsarerepresentedbycoordinate-based
mentsforDynamicTypography:(1)TemporalConsistency.The
MultilayerPerceptron(MLP).Tobettercapturehigh-frequencyvari-
deformedlettershouldmovecoherentlywhilepreservingarela-
ationandrepresentgeometryinformation,weprojectthecoordi-
tivelyconsistentappearanceineachanimationframe.(2)Legibility
natesintoahigher-dimensionalspaceusingpositionalencoding,
Preservation.Thedeformedlettershouldremainlegibleineach
whichisthesameastheoneusedinNeRF[Mildenhalletal.2021]:
frameduringanimation.(3)SemanticAlignment.Thelettershould
bedeformedandanimatedinawaythatalignswiththesemantic ğ›¾(ğ‘š)=(cid:16) sin(20ğœ‹ğ‘š),cos(20ğœ‹ğ‘š),...,sin(2ğ¿âˆ’1ğœ‹ğ‘š),cos(2ğ¿âˆ’1ğœ‹ğ‘š)(cid:17)
(3)
informationinthetextprompt.
Onestraightforwardstrategycanbefirstdeformingthestatic ğ›¾(Â·) isappliedseparatelytoeachdimensionofthecontrolpoint
letterwithexistingmethodslike[Iluzetal.2023],thenutilizing coordinates.
ananimationmodeldesignedforarbitrarygraphicscomposedof Theobjectiveofthebasefield,denotedasğµ,istolearnashared
BeÅºiercurveslike[Galetal.2023]toanimatethedeformedletter. shapeforeveryanimationframe,servingasabasetoinfusemotion.
However,thisnon-end-to-endformulationsuffersfromconflicting Itisdefinedbyafunctionğµ : ğ›¾(ğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ) â†’ ğ‘ƒ ğµ,whichmapsthe
priorknowledge.Thedeformedlettergeneratedbythefirstmodel originalletterâ€™scontrolpointscoordinatesğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ intobaseshapesâ€™
maynotalignwiththepriorknowledgeoftheanimationmodel.This coordinatesğ‘ƒ ğµ,bothinRğ‘Ã—2.
mismatchcanleadtheanimationmodeltoaltertheappearanceof The motion field, denoted as ğ‘€, encodes the correspondence
thedeformedletter,leadingtoconsiderablevisualartifactsincluding between the control points in the base shape and those in each
distortionandinconsistency,seeFig.4. videoframe.InspiredbydynamicNeRFs[Parketal.2021;?]and
Therefore,toensurethecoherenceoftheentireprocess,wepro- CoDeF[Ouyangetal.2023],werepresentthevideoasa3Dvolume
poseanend-to-endarchitecturethatdirectlymapstheoriginalletter space,whereacontrolpointatğ‘¡-thframewithcoordinate(ğ‘¥,ğ‘¦)is
tothefinalanimation,asillustratedinFig.2.Toaddressthecom- representedby(ğ‘¥,ğ‘¦,ğ‘¡).Specifically,weduplicatethesharedbase
plexityoflearningper-framedisplacementthatconvertstheinput shapeğ‘˜timesandencodeğ‘¥,ğ‘¦,andğ‘¡ separatelyusingEq.3,writing
letterintoanimation,werepresentthevideoasalearnablebase itasğ‘ƒ ğµâ€² :ğ›¾({(ğ‘ƒ ğµ,ğ‘¡)}ğ‘˜ ğ‘¡=1).Themotionfieldisdefinedasafunction
shapeandper-framemotionaddedonthebaseshape(Â§4.1).Addi- ğ‘€ :ğ‘ƒ ğµâ€² â†’ğ‘ƒ ğ‘‰ thatmapscontrolpointsfromthebaseshapetotheir
tionally,weincorporatelegibilityregularizationbasedonperceptual correspondinglocationsğ‘ƒ ğ‘‰ inthe3Dvideospace.DynamicTypography:BringingWordstoLife â€¢ 5
Tobettermodelmotion,werepresentğ‘ƒ ğ‘‰ asğ‘ƒ ğµ+Î”ğ‘ƒ,focusingon process.Specifically,weleverageLearnedPerceptualImagePatch
learningtheper-framedisplacementsÎ”ğ‘ƒ ={Î”ğ‘ƒğ‘¡}ğ‘˜ ğ‘¡=1tobeapplied Similarity(LPIPS)[Zhangetal.2018]asalosstoregularizethe
onthebaseshape.Following[Galetal.2023],wedecomposethe perceptualdistancebetweentherasterizedimagesofthebaseshape
motionintoglobalmotion(modeledbyanaffinetransformation ğ‘…(ğ‘ƒ ğµ)andtheoriginalletterğ‘…(ğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ):
matrixsharedbyallcontrolpointsofanentireframe)andlocal
motion(predictedforeachcontrolpointseparately).Considerthe
L legibility=LPIPS(ğ‘…(ğ‘ƒ ğµ),ğ‘…(ğ‘ƒ ğ‘™ğ‘’ğ‘¡ğ‘¡ğ‘’ğ‘Ÿ)) (6)
ğ‘–-thcontrolpointonthebaseshapewithcoordinate(ğ‘¥ ğµ,ğ‘–,ğ‘¦ ğµ,ğ‘–),its Benefitingfromourdesign,weonlyneedtoapplythisLPIPS-based
displacementonğ‘¡-thframeÎ”ğ‘ ğ‘–ğ‘¡ issummedbyitslocalandglobal legibilityregularizationtermtothebaseshape,andthemotionfield
displacement: willautomaticallypropagatethislegibilityconstrainttoeachframe.
Î”ğ‘ ğ‘–ğ‘¡ =Î”ğ‘ ğ‘–ğ‘¡,ğ‘™ğ‘œğ‘ğ‘ğ‘™ +Î”ğ‘ ğ‘–ğ‘¡,ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (4) 4.3 Mesh-basedStructurePreservationRegularization
ï£®
ğ‘ 
ğ‘¥
ğ‘ â„ ğ‘¥ğ‘  ğ‘¦ğ‘‘
ğ‘¥ï£¹ ï£®
cosğœƒ sinğœƒ0ï£¹ ï£®ğ‘¥
ğµ,ğ‘–ï£¹
ï£®ğ‘¥
ğµ,ğ‘–ï£¹
Theoptimizationprocessaltersthepositionsofcontrolpoints,some-
Î”ğ‘ ğ‘–ğ‘¡,ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ = ï£¯ ï£¯ ï£¯ğ‘ â„ ğ‘¦ğ‘  ğ‘¥ ğ‘  ğ‘¦ ğ‘‘ ğ‘¦ï£º ï£º
ï£º
ï£¯ ï£¯ ï£¯âˆ’sinğœƒcosğœƒ0ï£º ï£º ï£ºÂ·ï£¯ ï£¯ ï£¯ğ‘¦ ğµ,ğ‘–ï£º ï£º ï£ºâˆ’ï£¯ ï£¯ ï£¯ğ‘¦ ğµ,ğ‘–ï£º ï£º ï£º, (5) t ili lm use ts rale tea ddin ing Fto igc .o 6m d.p Tle hx ein rete nr ds ee rc it nio gn os fb Se ct aw lae be ln eB VÃ© ez ci te or rc Gur rv ae ps h, ia cs
s
ï£¯ 0 0 1 ï£º ï£¯ 0 0 1ï£º ï£¯ 1 ï£º ï£¯ 1 ï£º
ï£° ï£» ï£° ï£» ï£° ï£» ï£° ï£» (SVG)adherestothenon-zeroruleoreven-oddrule[Foley1996],
whereÎ”ğ‘ ğ‘–ğ‘¡,ğ‘™ğ‘œğ‘ğ‘ğ‘™ andallelementsintheper-frameglobaltransfor- whichdeterminesthefillstatusbydrawinganimaginarylinefrom
mationmatrixarelearnable. thepointtoinfinityandcountingthenumberoftimesthelinein-
Totrainthebasefieldandmotionfield,wedistillpriorknowledge tersectstheshapeâ€™sboundary.Thefrequentintersectionsbetween
fromthelarge-scalepretrainedtext-to-videomodel,usingSDSloss beziercurvescomplicatetheboundary,leadingtoalternatingblack
ofEq.2.Ateachtrainingiteration,weuseadifferentiablerasterizer andwhiteâ€œholesâ€withintheimage.Furthermore,theseintersec-
[Lietal.2020],denotedasğ‘…,torenderourpredictedcontrolpoints tionsbetweenBÃ©ziercurvesvaryabruptlybetweenadjacentframes,
setğ‘ƒ ğ‘‰ intoarasterizedvideo(pixelformatvideo).Weproceedby leadingtosevereflickeringeffectsthatdegradeanimationquality,
selectingadiffusiontimestepğœ,drawingasamplefromanormal seeFig.6.
distributionfornoiseğœ– âˆ¼N(0,1),andthenaddingthenoisetothe
rasterizedvideo.Thevideofoundationmodeldenoisethisvideo,
basedontheuserpromptdescribingamotionpatterncloselyrelated
tothewordâ€™ssemanticmeaning(e.g.â€œAcamelwalkssteadilyacross
thedesert.â€forâ€œMâ€inâ€œCAMELâ€).TheSDSlossiscomputedinEq.2
andguidesthelearningprocesstogeneratevideosalignedwiththe
desiredtextprompt.Wejointlyoptimizethebasefieldandmotion
fieldusingthisSDSloss.Thevisualizedbaseshapedemonstrates (a)frame1 (b)frame2 (c)frame3 (d)frame1vis.
alignmentwiththepromptâ€™ssemantics,asshowninFig.5. Fig.6. Adjacentframesofanimationforletterâ€œEâ€inâ€œJETâ€.Alargeareaof
Tomaintainalegibleandconsistentappearancethroughoutthe alternatingblackandwhiteâ€œholesâ€occurwithineachframe,ashighlighted
animation, we propose legibility regularization and mesh-based withintheredcircles,causingsevereflickeringbetweentheadjacentframes.
structurepreservationregularization,whichwillbedescribedin (d)isthevisualizationofframe1,highlightingthecontrolpointsandthe
latersections. associatedBÃ©ziercurves.Theillustrationrevealsfrequentintersections
amongtheBÃ©ziercurvesleadingtotheflickeringartifacts.
In addition, the unconstrained degrees of freedom in motion
couldaltertheappearanceofthebaseshape,leadingtonoticeable
discrepanciesinappearancebetweenadjacentframesandtemporal
Fig.5. Baseshapeofâ€œYâ€forâ€œGYMâ€withpromptâ€œAmandoingexerciseby
inconsistency.
liftingtwodumbbellsinbothhandsâ€
Toaddresstheseissues,weadoptDelaunayTriangulation[Barber
andHuhdanpaa1995;Delaunayetal.1934]onthebaseshapebased
4.2 LegibilityRegularization oncontrolpoints(seeFig.7).Bymaintainingthestructureofthe
triangularmesh,weleveragethestabilityoftrianglestoprevent
AcriticalrequirementforDynamicTypographyisensuringthe
frequentintersectionsbetweenBÃ©ziercurves,whilealsopreserv-
animationsmaintainlegibility.Forexample,forâ€œMâ€inâ€œCAMELâ€,
ingtherelativeconsistencyoflocalgeometryinformationacross
wehopetheâ€œMâ€takesontheappearanceofacamelwhilebeing
adjacentframes.
recognizableastheletterâ€œMâ€.WhenemployingSDSlossfortraining,
Specifically,weemploytheanglevariation[Iluzetal.2023]of
the text-to-video foundation modelâ€™s prior knowledge naturally
thecorrespondingtriangularmeshesinadjacentframesasaform
deformstheletterâ€™sshapetomatchthesemanticcontentofthetext
ofregularization:
prompt.However,thissignificantappearancechangecompromises
theletterâ€™slegibilitythroughouttheanimation.
ğ‘˜ ğ‘š
beT leh gu ibs, lew ,e wp or ro kp ino gse ala or ne gg su idla eri tz ha eti So Dn Ste lorm sst th oa gt ue in df eo tr hc ees ot ph te imle iztt ae tr iot no ğ‘˜Ã—1
ğ‘š
âˆ‘ï¸ ğ‘¡=1âˆ‘ï¸ ğ‘–=1âˆ¥ğœƒ(ğ‘‡ ğ‘–,ğ‘¡+1)âˆ’ğœƒ(ğ‘‡ ğ‘–,ğ‘¡)âˆ¥ 2, (7)6 â€¢ ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
whereğ‘šreferstothetotalnumberoftriangularmeshesineach pointsinScalableVectorGraphics(SVG)asinput.Thisallowsthe
frame,ğ‘‡ ğ‘–,ğ‘¡ referstotheğ‘–-thtriangularmeshintheğ‘¡-thframe,and MLPsinthebaseandmotionfieldtomoreeffectivelyrepresent
âˆ¥ğœƒ(ğ‘‡ ğ‘–,ğ‘¡+1)âˆ’ğœƒ(ğ‘‡ ğ‘–,ğ‘¡)âˆ¥ 2 referstothesumofthesquareddifference high-frequencyinformation,correspondingtothedetailedgeomet-
incorrespondinganglesbetweentwotriangles.Particularly,the ricfeatures.Additionally,whenusingcoordinate-basedMLPsto
(ğ‘˜+1)-thframereferstothebaseshape.Duringtraining,wede- modelmotion,asignificantchallengeishowtocapturebothminute
tachthegradientsofthesecondframeineachpair.Thisallows and large motions. Following Nerfies [Park et al. 2021], we em-
ustoregularizethepreviousframeâ€™smeshstructurewiththenext ployacoarse-to-finestrategythatinitiallytargetslow-frequency
frameasareference,andthelastframeisregularizedwiththebase (large-scale)motionandprogressivelyrefinesthehigh-frequency
shapeasareference.Hence,thestructuralconstraintwiththebase (localized)motions.Specifically,weusethefollowingformulato
shapeispropagatedtoeveryframe,allowingthepreservationof applyweightstoeachfrequencybandğ‘— inthepositionalencoding
thegeometricstructurethroughouttheanimation. oftheMLPswithinthemotionfield.
Wefindthatthisangle-basednaiveapproachiscapableofef- 1âˆ’cos(ğœ‹Â·clamp(ğ›¼âˆ’ğ‘—,0,1))
fectivelymaintainingthetriangularstructure,therebyalleviating ğ‘¤ ğ‘—(ğ›¼)= , (9)
2
thefrequentintersectionsoftheBeÅºiercurvesandpreservinga
relativelystableappearanceacrossdifferentframeswithoutsignifi-
whereğ›¼(ğ‘¡) = ğ¿ ğ‘ğ‘¡ ,ğ‘¡ is the current training iteration, and ğ‘ is a
hyper-parameterforwhenğ›¼shouldreachthemaximumnumberof
cantlyaffectingthelivelinessofthemotion.Furthermore,toensure
frequenciesğ¿.
thatthebaseshapeitselfmitigatesthefrequentintersectionsof
Inourexperiment,thisannealedfrequency-basedencodingre-
BeÅºiercurvesandcoarsespikes,weapplythesametriangulation-
sultedinhigher-qualitymotionanddetailedgeometricinformation.
basedconstraintsbetweenthebaseshapeandtheinputletter.The
wholeregularizationisillustratedinFig.7.
5 EXPERIMENTS
ğ‘š Tocomprehensivelyevaluateourmethodâ€™sability,wecreateda
1 âˆ‘ï¸
L structure= ğ‘š âˆ¥ğœƒ(ğ‘‡ ğ‘–,ğµ)âˆ’ğœƒ(ğ‘‡ ğ‘–,letter)âˆ¥ 2 datasetthatcoversanimationsforalllettersinthealphabet,featur-
ğ‘–=1 ingavarietyofelementssuchasanimals,humans,andobjects.This
(8)
ğ‘˜ ğ‘š datasetcontainsatotalof33DynamicTypographysamples.Each
1 âˆ‘ï¸âˆ‘ï¸
+ ğ‘˜Ã—ğ‘š âˆ¥ğœƒ(ğ‘‡ ğ‘–,ğ‘¡+1)âˆ’ğœƒ(ğ‘‡ ğ‘–,ğ‘¡)âˆ¥ 2 sampleincludesaword,aspecificletterwithinthewordtobeani-
ğ‘¡=1ğ‘–=1 mated,andaconcisetextpromptdescribingthedesiredanimation.
WeusedKaushanScript-Regularasthedefaultfont.
Weusetext-to-video-ms-1.7bmodelinModelScope[Luoetal.
2023;Wangetal.2023]forthediffusionbackbone.Weapplyaug-
mentationsincludingrandomcropandrandomperspectivetoall
framesoftherenderedvideos.Eachoptimizationtakes1000epochs,
about40minutesonasingleH800GPU.
Toillustrateourmethodâ€™scapabilities,wepresentsomegenerated
resultsinFig.1.Theseanimationsvividlybringthespecifiedletter
tolifewhileadheringtothepromptandmaintainingthewordâ€™s
readability.Forfurtherexploration,westronglysuggestthereaders
gothroughtheadditionalexamplesandfull-lengthvideosonour
projectpage.
5.1 Comparisons
Wecompareourmethodwithapproachesfromtwodistinctcate-
gories:thepixel-basedstrategiesleveragingeithertext-to-videoor
image-to-videomethods,andthevector-basedanimationmethod.
Fig.7. IllustrationoftheMesh-basedstructurepreservation.Wefirstapply Withinthepixel-basedscenario,wecompareourmodelagainst
thisregularizationbetweenthebaseshapeandtheinputletter.Wepropa- theleadingtext-to-videogenerationmodelsGen-2[contributors
gatethestructuralconstrainttoeveryframebyregularizingthelastframe 2023a](rankedfirstintheEvalCrafter[Liuetal.2023]benchmark)
withthebaseshapeandregularizingeveryframewithitsnextframe. â€“ a commercial web-based tool, and DynamiCrafter [Xing et al.
2023],thestate-of-the-artmodelforimage-to-videogenerationcon-
ditionedontext.Fortext-to-videogeneration,weappendtheprompt
4.4 Frequency-basedEncodingandAnnealing
withâ€œwhichlookslikealetterğ›½,â€whereğ›½representsthespecific
NeRF[Mildenhalletal.2021]havehighlightedthataheuristicap- lettertobeanimated.Intheimage-to-videocase,weusethestyl-
plicationofsinusoidalfunctionstoinputcoordinates,knownas izedlettergeneratedbytheword-as-image[Iluzetal.2023]asthe
â€œpositionalencodingâ€,enablesthecoordinate-basedMLPstocapture conditioningimage.
higherfrequencycontent,asdenotedbyEq.3.Wefoundthatthis Within the vector-based scenario, we utilize LiveSketch [Gal
propertyalsoappliestoourMLPsthatusecoordinatesofcontrol etal.2023]asaframeworktoanimatevectorimages.ToensureaDynamicTypography:BringingWordstoLife â€¢ 7
Fig.8. Visualcomparisonsbetweenthebaselinesandourmodel.Text-to-imagemodel(Gen-2)generatescolorfulimagesbutfailstomaintaintheshapeof
theoriginalletter.Thepixel-basedimage-to-videomodel(DynamiCrafter)producesresultswithlittle,sometimesunreasonablemotion.Thegeneralvector
animationmodel(LiveSketch)strugglestopreservelegibilityandmaintainastableappearanceacrossframes.
faircomparison,weconditiontheanimationonthestylizedletter generatedvideosandtheircorrespondingprompts(â€œtext-to-video
generatedbytheword-as-image[Iluzetal.2023]aswell. alignmentâ€),weleveragetheX-CLIPscore[Maetal.2022],which
Qualitative Comparison We present the visual comparison extendsCLIP[Radfordetal.2021]tovideorecognition,toobtain
withbaselinemethodsinFig.8.Whileachievinghighresolution frame-wiseimageembeddingsandtextembeddings.Theaverage
andrealism,Gen-2strugglestogenerateframesthatkeeptheletterâ€™s cosinesimilaritybetweentheseembeddingsreflectshowwellthe
shape,whichgreatlyharmsthelegibility.WithDynamiCrafter,the generatedvideosalignwiththecorrespondingprompts.
â€œSWANâ€animationexhibitsminimalmovement,whiletheâ€œGYMâ€ WhileGen-2achievesthehighesttext-to-videoalignmentscore,
animationfeaturesunrealisticmotionthatdeviatesfromtheuserâ€™s itseverelysuffersinlegibilitypreservation(lowestPICscore).Con-
prompt.AlthoughLiveSketchcandepicttheuserâ€™spromptthrough versely,ourmodelexcelsinPIC(highestscore),indicatingtheeffec-
animation,itsacrificeslegibility.Also,theletterâ€™sappearancedete- tivenessinmaintainingtheoriginalletterâ€™sform.Whileachieving
rioratesthroughouttheanimation,asdemonstratedintheâ€œSWANâ€ thesecond-besttext-to-videoalignmentscore,ourmethodstrikesa
example.Ourmodelstrikesabalancebetweenprompt-videoalign- balancebetweenfaithfullyrepresentingboththeanimationconcept
mentandletterlegibility.Itconsistentlygeneratesanimationsthat andtheletteritself.
adheretotheuserâ€™spromptwhilepreservingtheoriginalletterâ€™s
form.Thisallowstheanimationtoseamlesslyintegratewithinthe 5.2 AblationStudy
originalword,asshowcasedbythein-contextresultsinFig.8.
Weconductedanablationstudytoanalyzethecontributionofeach
componentinourproposedmethod:learnablebaseshape,legibil-
Method Perceptual Text-to-Video ityregularization,mesh-basedstructurepreservationregulariza-
InputConformity(â†‘) Alignment(â†‘)
tion,andfrequencyencodingwithannealing.VisualresultsinFig.
Gen-2 0.1495 23.3687 9showcasethequalitativeimpactofremovingeachcomponent.
DynamiCrafter 0.5151 17.8124 QuantitativeresultsinTab.2furtherconfirmtheireffectiveness.
LiveSketch 0.4841 20.2402 InadditiontoPerceptualInputConformity(PIC)andText-to-
Ours 0.5301 21.4391 VideoAlignment(X-CLIPscore),weemployedwarpingerrorto
Table1. Quantitativeresultsbetweenthebaselinesandourmodel.The assesstemporalconsistency,followingEvalCrafter[Liuetal.2023].
bestandsecond-bestscoresforeachmetricarehighlightedinredandblue Thismetricestimatestheopticalflowbetweenconsecutiveframes
respectively.
usingthepre-trainedRAFTmodel[TeedandDeng2020]andcalcu-
latesthepixel-wisedifferencebetweenthewarpedimageandthe
QuantitativeComparisonTab.1presentsthequantitativeeval- targetimage.Thelowerwarpingerrorindicatessmootherandmore
uationresults.Weemployedtwometrics,PerceptualInputConfor- temporallyconsistentanimations.
mity(PIC)andText-to-VideoAlignment.FollowingDynamiCrafter BaseShapeThecalculationoflegibilityandstructurepreserva-
[Xingetal.2023],wecomputedPerceptualInputConformity(PIC) tionregularizationinvolvesthebaseshape.Hence,whenremoving
usingDreamSimâ€™s[Pooleetal.2023]perceptualdistancemetric thelearnablebaseshape,thelegibilitylossLğ‘™ğ‘’ğ‘”ğ‘–ğ‘ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦ iscomputed
betweeneachoutputframeandtheinputletter,averagedacrossall betweeneveryoutputframeandtheinputletter,whilethestruc-
frames.Thismetricassesseshowwelltheanimationpreservesthe turepreservationlossLğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ isappliedbetweeneverypairof
originalletterâ€™sappearance.Toevaluatethealignmentbetweenthe consecutiveframes.8 â€¢ ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
AsobservedinFig.9(row2),removingthesharedlearnablebase OpticalFlow Perceptual Text-to-Video
Method
shaperesultsininconsistentanimations.Specifically,ashighlighted WarpingError(â†“) InputConformity(â†‘) Alignment(â†‘)
by the red circle, the appearance of the bullfighter deviates sig- FullModel 0.01645 0.5310 21.4447
NoBaseShape 0.03616 0.5178 20.0568
nificantlybetweenframes,harminglegibility.Thefindingisalso
NoLegibility 0.01561 0.4924 20.2857
supportedbyTab.2(row2),whereremovingthebaseshaperesults
NoStruc.Pre. 0.01777 0.4906 20.6285
insignificantdegradationunderallthreemetrics. NoFreq. 0.02222 0.5377 20.8280
LegibilityRegularizationWithouttheperceptualregulariza-
Table2. Quantitativeresultsoftheablationstudy.Thebestandsecond-best
tiononthebaseshape,thebaseshapestrugglestopreservelegibility. scoresforeachmetricarehighlightedinredandbluerespectively.
Asaresult,eachanimationframelosestheletterâ€œRâ€shapeinFig.9
(row3),leadingtolowerPICinTab.2(row3).
StructurePreservationRegularizationRemovingmesh-based
structurepreservationallowsthebaseshapeâ€™sstructuretodeviate etal.2024],andZeroScope[Luoetal.2023].Fig.10presentsvisual
fromtheoriginalletter,causingthediscontinuitybetweenthebull- resultsforthesameanimationsample(â€œKnightâ€)witheachbase
fighterandcapeinthebaseshapeandallframes,ashighlighted model.
inTab.2(row4).Withoutthisregularizationterm,theanimation Whiletheletterâ€œKâ€exhibitsdeformationsandanimationstyles
showsinconsistentappearancesacrossdifferentframes,whichde- uniquetoeachmodel,allanimationsaccuratelydepicttheuserâ€™s
gradesthelegibility,leadingtothelowestPICinTab.2(row4). promptandmaintainthebasicâ€œKâ€shape.Thisshowcasesthegen-
Frequency Encoding and Annealing When removing fre- eralizabilityofourmethod.Hence,futureadvancementsintext-
quencyencodingandcoarse-to-fineannealing,themotionandge- to-videomodelswithstronger priorknowledgewillbenefitour
ometryqualitysuffers.Forexample,thebullfighteranimationin approach.
Fig.9(row5)showsunreasonablemotionandgeometrydetails,
resultinginananimationthatdoesnotaccuratelyrepresentthetext
prompt.Moreover,thedegradationinmotionqualityalsoharms
thetemporalconsistency,Tab.2(row5).
Fig.10. Visualresultsofthesameanimationsampleusingdifferenttext-to-
videobasemodels.
6 CONCLUSION
Weproposeanautomatedtextanimationscheme,termedâ€œDynamic
Typography,â€thatdeformsletterstoconveysemanticmeaningand
animatesthemvividlybasedonuserprompts.Ourmethodisan
end-to-endoptimization-basedapproachandisgeneralizabletoarbi-
Fig.9. Visualcomparisonsofablationstudy.Removingbaseshapeorstruc- trarywordsandmotionpatterns.Nevertheless,thereremainseveral
turepreservationregularizationresultsinshapedeviationandflickering. limitations.First,themotionqualitycanbeboundedbythevideo
Withoutlegibilityregularization,eachanimationframelosestheletterâ€œRâ€ foundationmodel,whichmaybeunawareofspecificmotionsin
shape.Theabsenceoffrequencyencodingandannealingleadstothedegra-
somecases.However,ourframeworkismodel-agnostic,whichfacil-
dationofthemotionqualityandgeometrydetails.
itatesintegrationwithfutureadvancementsindiffusion-basedvideo
foundationmodels.Besides,challengesarisewhenuser-provided
5.3 Generalizability
textpromptsdeviatesignificantlyfromoriginallettershapes,com-
Ouroptimizationframework,leveragingScoreDistillationSampling plicatingthemodelâ€™sabilitytostrikeabalancebetweengenerating
(SDS),achievesgeneralizationacrossvariousdiffusion-basedtext-to- semantic-awarevividmotionandpreservingthelegibilityofthe
videomodels.Todemonstratethis,weapplieddifferentbasemodels originalletter.Wehopethatourworkcanopenthepossibilityfor
forcomputingLğ‘†ğ·ğ‘†,includingthe1.7-billionparametertext-to- furtherresearchofsemantic-awaretextanimationthatincorporates
videomodelfromModelScope[Wangetal.2023],AnimateDiff[Guo therapiddevelopmentofvideogenerationmodels.DynamicTypography:BringingWordstoLife â€¢ 9
REFERENCES
Tzu-MaoLi,MichalLukÃ¡Ä,MichaÃ«lGharbi,andJonathanRagan-Kelley.2020.Differen-
AdobeSystemsInc.1990. AdobeType1FontFormat. AddisonWesleyPublishing tiablevectorgraphicsrasterizationforeditingandlearning.ACMTransactionson
Company. Graphics(Dec2020),1â€“15. https://doi.org/10.1145/3414685.3417871
SamanehAzadi,MatthewFisher,VladimirKim,ZhaowenWang,EliShechtman,and YaofangLiu,XiaodongCun,XueboLiu,XintaoWang,YongZhang,HaoxinChen,Yang
TrevorDarrell.2018. Multi-ContentGANforFew-ShotFontStyleTransfer.In Liu,TieyongZeng,RaymondChan,andYingShan.2023.Evalcrafter:Benchmarking
2018IEEE/CVFConferenceonComputerVisionandPatternRecognition. https: andevaluatinglargevideogenerationmodels.arXivpreprintarXiv:2310.11440(2023).
//doi.org/10.1109/cvpr.2018.00789 RaphaelGontijoLopes,DavidHa,DouglasEck,andJonathonShlens.2019.Alearned
CBarberandHannuHuhdanpaa.1995. Qhull.TheGeometryCenter,Universityof representationforscalablevectorgraphics.InProceedingsoftheIEEE/CVFInterna-
Minnesota. tionalConferenceonComputerVision.7930â€“7939.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian, ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,LiangWang,YujunShen,
DominikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.2023. DeliZhao,JingrenZhou,andTieniuTan.2023.VideoFusion:DecomposedDiffusion
Stablevideodiffusion:Scalinglatentvideodiffusionmodelstolargedatasets.arXiv ModelsforHigh-QualityVideoGeneration.InProceedingsoftheIEEE/CVFConference
preprintarXiv:2311.15127(2023). onComputerVisionandPatternRecognition(CVPR).
Bay-WeiChangandDavidUngar.1993.Animation:fromcartoonstotheuserinterface. YiweiMa,GuohaiXu,XiaoshuaiSun,MingYan,JiZhang,andRongrongJi.2022.
InProceedingsofthe6thAnnualACMSymposiumonUserInterfaceSoftwareandTech- X-CLIP:End-to-EndMulti-grainedContrastiveLearningforVideo-TextRetrieval.
nology(Atlanta,Georgia,USA)(UISTâ€™93).AssociationforComputingMachinery, InProceedingsofthe30thACMInternationalConferenceonMultimedia(<conf-
NewYork,NY,USA,45â€“55. https://doi.org/10.1145/168642.168647 loc>,<city>Lisboa</city>,<country>Portugal</country>,</conf-loc>)(MMâ€™22).
HaoxinChen,MenghanXia,YingqingHe,YongZhang,XiaodongCun,ShaoshuYang, AssociationforComputingMachinery,NewYork,NY,USA,638â€“647. https://doi.
JinboXing,YaofangLiu,QifengChen,XintaoWang,etal.2023.Videocrafter1:Open org/10.1145/3503161.3547910
diffusionmodelsforhigh-qualityvideogeneration.arXivpreprintarXiv:2310.19512 WendongMao,ShuaiYang,HuihongShi,JiayingLiu,andZhongfengWang.2022.
(2023). Intelligenttypography:Artistictextstyletransferforcomplextextureandstructure.
Gen-2contributors.2023a.Gen-2. https://research.runwayml.com/gen2 IEEETransactionsonMultimedia(2022).
PikaLabscontributors.2023b.Pikalabs. https://www.pika.art/ YifangMen,ZhouhuiLian,YingminTang,andJianguoXiao.2019.DynTypo:Example-
WernerLembergDavidTurner.2009.FreeTypelibrary. RetrievedMar19,2024from BasedDynamicTextEffectsTransfer.In2019IEEE/CVFConferenceonComputer
https://freetype.org/ VisionandPatternRecognition(CVPR). https://doi.org/10.1109/cvpr.2019.00602
BorisDelaunayetal.1934.Surlaspherevide.Izv.Akad.NaukSSSR,OtdelenieMatem- BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa-
aticheskiiiEstestvennykaNauk7,793-800(1934),1â€“2. mamoorthi,andRenNg.2021.Nerf:Representingscenesasneuralradiancefields
JonFerraiolo,FujisawaJun,andDeanJackson.2000.Scalablevectorgraphics(SVG)1.0 forviewsynthesis.Commun.ACM65,1(2021),99â€“106.
specification.iuniverseBloomington. MitsuruMinakuchiandYutakaKidawara.2008. Kinetictypographyforambient
NoaFish,LilachPerry,AmitBermano,andDanielCohen-Or.2020.SketchPatch.ACM displays.InProceedingsofthe2ndinternationalconferenceonUbiquitousinformation
TransactionsonGraphics(Dec2020),1â€“14. https://doi.org/10.1145/3414685.3417816 managementandcommunication. https://doi.org/10.1145/1352793.1352805
JamesDFoley.1996.Computergraphics:principlesandpractice.Vol.12110.Addison- MitsuruMinakuchiandKatsumiTanaka.2005.Automatickinetictypographycomposer.
WesleyProfessional. InProceedingsofthe2005ACMSIGCHIInternationalConferenceonAdvancesin
ShannonFord,JodiForlizzi,andSuguruIshizaki.1997.Kinetictypography.InCHIâ€™97 computerentertainmenttechnology. https://doi.org/10.1145/1178477.1178512
extendedabstractsonHumanfactorsincomputingsystemslookingtothefuture-CHI HaomiaoNi,ChanghaoShi,KaiLi,SharonXHuang,andMartinRenqiangMin.2023.
â€™97. https://doi.org/10.1145/1120212.1120387 ConditionalImage-to-VideoGenerationwithLatentFlowDiffusionModels.In
JodiForlizzi,JohnnyLee,andScottHudson.2003.Thekineditsystem.InProceedings ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
oftheSIGCHIConferenceonHumanFactorsinComputingSystems. https://doi.org/ 18444â€“18455.
10.1145/642611.642677 HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng,
RinonGal,YaelVinker,YuvalAlaluf,AmitH.Bermano,DanielCohen-Or,ArielShamir, XiaoweiZhou,QifengChen,andYujunShen.2023.Codef:Contentdeformation
andGalChechik.2023. BreathingLifeIntoSketchesUsingText-to-VideoPriors. fieldsfortemporallyconsistentvideoprocessing.arXivpreprintarXiv:2308.07926
(2023).arXiv:2311.13608[cs.CV] (2023).
YuweiGuo,CeyuanYang,AnyiRao,ZhengyangLiang,YaohuiWang,YuQiao,Maneesh KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman,
Agrawala,DahuaLin,andBoDai.2024.AnimateDiff:AnimateYourPersonalized StevenMSeitz,andRicardoMartin-Brualla.2021. Nerfies:Deformableneural
Text-to-ImageDiffusionModelswithoutSpecificTuning.InTheTwelfthInterna- radiancefields.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id= Vision.5865â€“5874.
Fx2SbBgcte LaurencePenny.1996. AHistoryofTrueType. RetrievedMar19,2024fromhttps:
YingqingHe,TianyuYang,YongZhang,YingShan,andQifengChen.2022. Latent //www.truetype-typography.com
VideoDiffusionModelsforHigh-FidelityVideoGenerationwithArbitraryLengths. BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.2023. DreamFusion:
(Nov2022). Text-to-3Dusing2DDiffusion.InTheEleventhInternationalConferenceonLearning
KaiHormannandGÃ¼ntherGreiner.2000.MIPS:Anefficientglobalparametrization Representations. https://openreview.net/forum?id=FjNys5c7VyY
method.CurveandSurfaceDesign:Saint-Malo1999(2000),153â€“162. AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
ShirIluz,YaelVinker,AmirHertz,DanielBerio,DanielCohen-Or,andArielShamir. Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021.
2023.Word-As-ImageforSemanticTypography.ACMTrans.Graph.42,4,Article Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInterna-
151(jul2023),11pages. https://doi.org/10.1145/3592123 tionalconferenceonmachinelearning.PMLR,8748â€“8763.
YueJiang,ZhouhuiLian,YingminTang,andJianguoXiao.2019.SCFont:Structure- RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjornOmmer.
GuidedChineseFontGenerationviaDeepStackedNetworks.Proceedingsofthe 2022. High-ResolutionImageSynthesiswithLatentDiffusionModels.In2022
AAAIConferenceonArtificialIntelligence(Sep2019),4015â€“4022. https://doi.org/10. IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR). https:
1609/aaai.v33i01.33014015 //doi.org/10.1109/cvpr52688.2022.01042
OrenKatzir,OrPatashnik,DanielCohen-Or,andDaniLischinski.2024.Noise-freeScore XiaoyuShi,ZhaoyangHuang,Fu-YunWang,WeikangBian,DasongLi,YiZhang,
Distillation.InTheTwelfthInternationalConferenceonLearningRepresentations. ManyuanZhang,KaChunCheung,SimonSee,HongweiQin,etal.2024.Motion-
https://openreview.net/forum?id=dlIMcmlAdk I2V:ConsistentandControllableImage-to-VideoGenerationwithExplicitMotion
JoonhwanLee,SoojinJun,JodiForlizzi,andScottE.Hudson.2006. Usingkinetic Modeling.arXivpreprintarXiv:2401.15977(2024).
typographytoconveyemotionintext-basedinterpersonalcommunication.InPro- AliaksandrSiarohin,StÃ©phaneLathuiliÃ¨re,SergeyTulyakov,ElisaRicci,andNicuSebe.
ceedingsofthe6thConferenceonDesigningInteractiveSystems(UniversityPark,PA, 2019.FirstOrderMotionModelforImageAnimation.NeuralInformationProcessing
USA)(DISâ€™06).AssociationforComputingMachinery,NewYork,NY,USA,41â€“49. Systems,NeuralInformationProcessingSystems(Jan2019).
https://doi.org/10.1145/1142405.1142414 MahamTanveer,YizhiWang,AliMahdavi-Amiri,andHaoZhang.2023.DS-Fusion:
JohnnyC.Lee,JodiForlizzi,andScottE.Hudson.2002a.Thekinetictypographyengine. ArtisticTypographyviaDiscriminatedandStylizedDiffusion.(Mar2023).
InProceedingsofthe15thannualACMsymposiumonUserinterfacesoftwareand ZacharyTeedandJiaDeng.2020.Raft:Recurrentall-pairsfieldtransformsforoptical
technology. https://doi.org/10.1145/571985.571997 flow.InComputerVisionâ€“ECCV2020:16thEuropeanConference,Glasgow,UK,August
JohnnyC.Lee,JodiForlizzi,andScottE.Hudson.2002b. Thekinetictypography 23â€“28,2020,Proceedings,PartII16.Springer,402â€“419.
engine:anextensiblesystemforanimatingexpressivetext.InProceedingsofthe JiuniuWang,HangjieYuan,DayouChen,YingyaZhang,XiangWang,andShiweiZhang.
15thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(Paris, 2023.Modelscopetext-to-videotechnicalreport.arXivpreprintarXiv:2308.06571
France)(UISTâ€™02).AssociationforComputingMachinery,NewYork,NY,USA, (2023).
81â€“90. https://doi.org/10.1145/571985.571997 WenjingWang,JiayingLiu,ShuaiYang,andZongmingGuo.2019.TypographyWith
Decor:IntelligentTextStyleTransfer.In2019IEEE/CVFConferenceonComputer10 â€¢ ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
VisionandPatternRecognition(CVPR). https://doi.org/10.1109/cvpr.2019.00604 diffusionpriors.arXivpreprintarXiv:2310.12190(2023).
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang, ShuaiYang,ZhouhuiLian,andZhongwenGuo.2016.AwesomeTypography:Statistics-
YujunShen,DeliZhao,andJingrenZhou.2024. Videocomposer:Compositional BasedTextEffectsTransfer. CornellUniversity-arXiv,CornellUniversity-arXiv
videosynthesiswithmotioncontrollability.AdvancesinNeuralInformationProcess- (Nov2016).
ingSystems36(2024). ShuaiYang,ZhangyangWang,andJiayingLiu.2021.Shape-MatchingGAN++:Scale
YizhiWangandZhouhuiLian.2021.DeepVecFont:SynthesizingHigh-qualityVector ControllableDynamicArtisticTextStyleTransfer. IEEETransactionsonPattern
FontsviaDual-modalityLearning. ACMTransactionsonGraphics40,6(2021), AnalysisandMachineIntelligence(Jan2021),1â€“1. https://doi.org/10.1109/tpami.
15pages. https://doi.org/10.1145/3478513.3480488 2021.3055211
LiwenhanXie,ZhaoyuZhou,KerunYu,YunWang,HuaminQu,andSimingChen.2023. RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.The
Wakey-Wakey:AnimateTextbyMimickingCharactersinaGIF.InProceedings unreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InProceedingsof
ofthe36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology. theIEEEconferenceoncomputervisionandpatternrecognition.586â€“595.
https://doi.org/10.1145/3586183.3606813 DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng.2022.
JinboXing,MenghanXia,YongZhang,HaoxinChen,XintaoWang,Tien-TsinWong, MagicVideo:EfficientVideoGenerationWithLatentDiffusionModels.(Nov2022).
andYingShan.2023.Dynamicrafter:Animatingopen-domainimageswithvideo