Joint Intrinsic Motivation for Coordinated Exploration in
Multi-Agent Deep Reinforcement Learning
ExtendedversionoftheextendedabstractpaperpublishedatAAMAS2024
MaximeToquebiauâˆ— NicolasBredecheâ€ 
ECEParis&SorbonneUniversitÃ©,CNRS,ISIR SorbonneUniversitÃ©,CNRS,ISIR
F-75005Paris,France F-75005Paris,France
maxime.toquebiau@gmail.com nicolas.bredeche@sorbonne-universite.fr
FaÃ¯zBenamar Jae-YunJunâ€ 
SorbonneUniversitÃ©,CNRS,ISIR ECEParis
F-75005Paris,France Paris,France
faiz.ben_amar@sorbonne-universite.fr jaeyunjk@gmail.com
ABSTRACT tacklelong-standingproblemsinMASsuchascreditassignmentor
Multi-agentdeepreinforcementlearning(MADRL)problemsoften partialobservability[6,11,22].Thesetechniquesareabletosolve
encounterthechallengeofsparserewards.Thischallengebecomes verycomplexmulti-agenttaskssuchasautonomousdriving[25]
evenmorepronouncedwhencoordinationamongagentsisneces- orreal-timestrategyvideogames[17].However,majorissuesstill
sary.Asperformancedependsnotonlyononeagentâ€™sbehaviorbut remainwiththeseapproaches,suchastheproblemofrelativeover-
ratheronthejointbehaviorofmultipleagents,findinganadequate generalization[30,32]whereagentsstruggletofindtheoptimal
solutionbecomessignificantlyharder.Inthiscontext,agroupof jointpolicybecauselocalpoliciesareattractedtowardssuboptimal
agentscanbenefitfromactivelyexploringdifferentjointstrategies areasofthesearchspace.Thismakesmostalgorithmsinefficient
inordertodeterminethemostefficientone.Inthispaper,wepro- intaskswheretheoptimalstrategyrequiresstrongcoordination
poseanapproachforrewardingstrategieswhereagentscollectively amongagents.Relativeovergeneralizationcanbedescribedasa
exhibitnovelbehaviors.WepresentJIM(JointIntrinsicMotiva- problemofexplorationofthejoint-statespace:asthesuccessof
tion),amulti-agentintrinsicmotivationmethodthatfollowsthe theMASdependsonthecoordinationofmultipleagents,exploring
centralizedlearningwithdecentralizedexecutionparadigm.JIM thejoint-observationspaceisrequiredtodiscoveroptimaljoint
rewardsjointtrajectoriesbasedonacentralizedmeasureofnovelty behaviors.Inthispaper,weaddressthequestionofhowtoexplore
designedtofunctionincontinuousenvironments.Wedemonstrate thejoint-statespacetoefficientlydiscoversuperiorcoordinated
thestrengthsofthisapproachbothinasyntheticenvironmentde- strategiesforsolvingthetaskathand.
signedtorevealshortcomingsofstate-of-the-artMADRLmethods, Insingle-agentRL,theproblemofexplorationhasbeenstudied
andinsimulatedrobotictasks.Resultsshowthatjointexploration tosolvehardexplorationtaskswherepositiverewardsignalsare
iscrucialforsolvingtaskswheretheoptimalstrategyrequiresa verysparse.Onesolutionistouseintrinsicmotivation[10,18,24]
highlevelofcoordination. toinciteagentstoexploreunknownpartsoftheenvironment.In
additiontotheenvironmentreward,agentsaregivenanauxiliary
Keywords. Multi-agentSystems,DeepReinforcementLearning, rewardrelatedtothenoveltyofencounteredstates.Maximizing
IntrinsicMotivation thisintrinsicrewardleadsagentstovisitpreviouslyunexplored
regionsoftheenvironment,ultimatelydiscoveringnewsolutions
1 INTRODUCTION
tothetask.ThesemethodshaveshowngreatsuccessinhelpingRL
Onecrucialaspectofhumanintelligenceisitsabilitytoactcoinci- agentssolvehardexplorationtasks[2,19].
dentallywithotherhumanbeings,toeithercooperateorcompetein Inthemulti-agentsetting,intrinsicobjectiveshavealsobeen
agiventask.Thishasledresearcherstostudyreinforcementlearn- studiedtoinducedifferentkindsofbehaviorsinagentssuchasco-
ing(RL)inthecontextofmulti-agentsystems(MAS),wheremulti- ordinatedexploration[8],socialinfluence[9,29]oralignmentwith
pleartificialagentsinteractwiththeirenvironmentandeachother otheragentsâ€™expectations[13].However,previousworkshaveonly
whileconcurrentlylearningtoperformatask[11,28].However, usedlocalobservationstogenerateintrinsicrewards.Withpartial
havingmultipleagentsintheenvironmentmakestheRLprocess observability,localobservationsoftenlackcrucialinformationto
significantlymoredifficultforseveralreasons[33].Inparticular, fullyunderstandthecurrentconfigurationoftheenvironment.In
theglobalrewarddependsontheactionsofseveralindependent thecontextofexploration,anintrinsicrewardbasedonlyonlocal
agents,whichmakesthesearchfortheoptimaljointpolicymore observationswillleadtoeachagentexploringtheirownobserva-
complicated. tionspace,withoutconsideringthecurrentstateofotheragents.
Recently,multi-agentdeepreinforcementlearning(MADRL)ap- Thiscanresultininefficientexplorationincooperativetaskswhere
proacheshavecombinedadvancementsinRLanddeeplearningto thesuccessoftheMASdependsonthecoordinationofallagents.
âˆ—Contactauthor:maxime.toquebiau@gmail.com.
â€ Authorscontributedequallytothepaper.
4202
beF
6
]AM.sc[
1v27930.2042:viXraMaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
Inthispaper,weintroduceanovelmulti-agentexplorationap- modelswillyieldlownoveltyforstatessimilartowhattheyhave
proachcalledJointIntrinsicMotivation(JIM)whichcanbecom- trainedonwhileproducinghighnoveltyforunknownpartsofthe
bined with any MADRL algorithm that follows the centralized environment.RIDE[20]andNovelD[35]userespectivelyICMand
trainingwithdecentralizedexecutionparadigm(CTDE).JIMex- RNDtocomputearewardfromthedifferenceofnoveltybetween
ploitscentralizedinformationtomotivateagentstoexplorenew thenextstateandthecurrentstate,pushingtheagentstoalways
coordinatedbehaviors.Inordertocomputejointnovelty,JIMbuilds seeknovelstates.Similarly,NGU[2]andE3B[7]useclustering
fromtwostate-of-the-artapproaches:NovelD[35]forexploring techniquestorewardstatesthataredistantfrompreviousstates.
unknownpartsoftheenvironment,andE3B[7]forhavingmore Finally,asimilarapproachisproposedbyAGAC[5]whichtrains
diversetrajectories.Addingthisauxiliaryrewardtotheagentsâ€™ anadversarialpolicytopredictthemainpolicyâ€™soutput,thelatter
objectiveincitesthemtodiversifytheircollectivebehavioruntil beingrewardedwiththeformerâ€™spredictionerror.
theyhaveafairknowledgeoftheenvironmentandcanfocuson InMADRL,recentworkshavedemonstratedtheeffectiveness
themaintaskathand. ofintrinsicrewardsinpromotingdesirablebehaviorsingroups
Todemonstratetheadvantagesofourapproach,wefirstdesign of agents. One example is social influence [9, 29] that rewards
asimpletestenvironmenttoshowcaseaclearexampleofrelative agentsforperformingactionsthathaveasignificantimpacton
overgeneralization.Weshowthatthestate-of-the-artalgorithm otheragents.Maetal.[13]proposeanintrinsicrewardbasedon
QMIX[22]strugglesinthisscenarioandthatmotivatingtheex- theaveragealignmentwithotheragentsâ€™expectations,promoting
plorationofcoordinatedbehaviorhelpssolvethetask.Next,we morepredictablebehaviorsinagents.Lupuetal.[12]proposeto
validatetheseresultsinacontinuousvirtualenvironment,showing rewardpoliciesthatperformdiversetrajectoriesincomparisonto
thatcoordinationtasksbenefitfromjointexploration.Finally,fur- apopulationofagents,whichisshowntohelptrainagentstobe
theranalysisisconductedtoconfirmthestrengthandscalability moreversatile.Duetal.[4]useintrinsicobjectivesasacreditas-
ofourapproach. signmenttechnique.Finally,IqbalandSha[8]proposeanapproach
forcoordinatedexplorationusingseveralmetricsforestimating
thenoveltyofobservationsthatdependonallagentsâ€™pastexpe-
2 RELATEDWORKS
riences.However,theirmodeliscomputationallyexpensiveand
Inrecentyears,deepreinforcementlearningtechniqueshavebeen doesnotaddresstheexplorationofthejoint-observationspace,
usedinthecontextofMAStotacklelong-standingissuesinmulti- whichcanbeproblematicforhardexplorationtaskswhererelative
agentlearning.Successfulsingle-agentRLapproacheshavebeen overgeneralizationcanoccur.
adaptedtotheCTDEframework[11,34],usingacentralizedvalue Inthispaper,weaddressthechallengeofrelativeovergener-
function to guide the training of decentralized policies. Recent alizationbyrewardingagentsforexploringthejoint-observation
studieshaveinvestigatedtheproblemofcreditassignment[6]in space.Inthefollowingsections,wewillpresentthenecessaryfor-
MADRL,i.e.,distributingtheglobalrewardamongagentsbased malbackgroundandanoverviewoftheproposedalgorithmthat
on their participation. Value factorization methods also do this implementsjointintrinsicmotivation.
implicitly[27],combiningtheoutputoflocalvaluefunctionsinto
acentralizedonethatpredictsthecurrentvalueofthesystem.In
3 BACKGROUND
particular,QMIX[22]usesaseparatenetworktopredicttheQ-value
ofthejointaction,giventheoutputoflocalQ-valuesandtheglobal 3.1 Dec-POMDP
stateoftheenvironment.QMIXhasestablisheditselfasalong- Todescribecooperativemulti-agenttasks,weusethedefinitionof
standingstate-of-the-artapproach,despiteitsinherentlimitations decentralizedpartially-observableMarkovdecisionprocess(Dec-
thatseveralworkshavetriedtosurpass[21,26].However,MADRL POMDP)[16],definedasatupleâŸ¨S,A,ğ‘‡,O,ğ‘‚,ğ‘…,ğ‘›,ğ›¾âŸ©withğ‘›being
algorithmshavebeenshowntosufferfromtheproblemofrelative thenumberofagents.Sdescribesthesetofglobalstatesğ‘ ofthe
overgeneralization[31,32].Sofar,fewworkshaveaddressedthis environment. O is the set of joint observations, with one joint
problem:Weietal.[31]proposemaximumentropyRLtoexplore observationo={ğ‘œ 1,...,ğ‘œ ğ‘›}âˆˆO,andAthesetofjointactions,with
thejoint-actionspace,andMAVEN[14]augmentsQMIXusinga onejointactiona = {ğ‘ 1,...,ğ‘ ğ‘›} âˆˆ A.ğ‘‡ isthetransitionfunction
hierarchicalpolicytoguidetheexplorationofjointbehaviors. definingtheprobabilityğ‘ƒ(ğ‘ â€²|ğ‘ ,a)totransitionfromstateğ‘ tonext
Apromisingapproachtoovercomerelativeovergeneralization stateğ‘ â€²withthejointactiona.ğ‘‚istheobservationfunctiondefining
istointrinsicallymotivateagentstoexploretheirenvironment, theprobabilityğ‘ƒ(o|a,ğ‘ â€²)toobservethejointobservationoafter
ultimatelydiscoveringtheoptimalrewardsignals.Insingle-agent takingjointactionaandendingupinğ‘ â€².ğ‘… : OÃ—A â†’ Risthe
RL,curiosityhasbeendefinedtohelpagentssolvehardexploration rewardfunctionproducingateachtimesteptherewardsharedby
tasks[10,18,24]byrewardingthevisitationofstatesconsidered allagents.Finally,ğ›¾ âˆˆ [0,1)isthediscountfactorcontrollingthe
asnovel.Formeasuringnovelty,severalmethodshaveusedthe importanceofimmediaterewardsagainstfuturegains.
erroroftrainablepredictionmodels.TheIntrinsicCuriosityModule
(ICM)[19]trainsamodelofenvironmentdynamicsandusesthe
3.2 Intrinsicrewards
predictionerrorasameasureofnovelty.RandomNetworkDis-
tillation(RND)[3]usesatargetnetworkthatproducesarandom InSection2,weintroducedintrinsicmotivationasawaytoincite
encodingofthestateandtrainsapredictornetworktogenerate agentstoactivelyexploretheirenvironment.Tothisend,ateach
thesameencoding,thepredictionerrorbeingthemeasureofnov- timestepğ‘¡,agentsreceiveanaugmentedrewardğ‘Ÿ ğ‘¡ =ğ‘Ÿ ğ‘¡ext+ğ›½ğ‘Ÿ ğ‘¡int,
elty.Theideabehindthesetwoapproachesisthattheprediction whereğ‘Ÿ ğ‘¡extistheextrinsicrewardgivenbytheenvironment,ğ‘Ÿ ğ‘¡intJointIntrinsicMotivationforCoordinatedExplorationinMADRL
istheintrinsicreward,andğ›½isahyperparametercontrollingthe
weightoftheintrinsicrewardintheagentsâ€™objective.
Inthissection,wedescribethreemethodsofintrinsicrewards
fromtheliteraturethatwewilluselaterinSection4.2.
Random Network Distillation (RND). In RND, Burda et al. [3]
computenoveltyusingtwoneuralnetworkswiththesamearchi-
tecture:atargetnetworkğœ™andapredictornetworkğœ™â€².Thetargetâ€™s ğ´ ğµ ğ¶
parametersareinitializedrandomlyandfixed.Ittakesasinputthe ğ´ 10 âˆ’5 âˆ’5
stateğ‘  ğ‘¡ andproducesarandomembeddingğœ™(ğ‘  ğ‘¡).Thepredictoris ğµ âˆ’5 7 7
ğ¶ âˆ’5 7 7
trainedtooutputthesameembedding,minimizingtheEuclidean
distance: (a) (b)
ğ‘…ğ‘ğ· ğ‘¡(ğ‘  ğ‘¡)=âˆ¥ğœ™(ğ‘  ğ‘¡)âˆ’ğœ™â€²(ğ‘  ğ‘¡)âˆ¥ 2. (1)
Thisdistanceisusedasameasureofthenoveltyofstateğ‘  ğ‘¡ andis Figure1:Twoexamplesofrelativeovergeneralization:(a)
payoff matrix of a social dilemma game, (b) heat-map of
givenasanintrinsicrewardtoagents.
therewardfunctionintherel_overgenenvironmentfortwo
NoveltyDifference(NovelD). Zhangetal.[35]builduponRNDto agents,withğ· =40andğ›¿ =30.DetailsinSection6.1.
deviseanoveltycriteriontermedNovelD.Itisdefinedasfollows:
ğ‘(ğ‘  ğ‘¡,ğ‘  ğ‘¡+1)=max[ğ‘…ğ‘ğ·(ğ‘  ğ‘¡+1)âˆ’ğ›¼ğ‘…ğ‘ğ·(ğ‘  ğ‘¡),0]Ã—
Then,wedefineourintrinsicrewardandexplainhowitisusedin
Ã—1{ğ‘ ğ‘’(ğ‘  ğ‘¡+1)=1}, (2) amulti-agentsettingwithJIM.
withğ›¼ ascalingfactorandğ‘ ğ‘’ anepisodiccountofvisitedstates.
4.1 Thechallengeofcoordinatedactions
Thefirstpartisthecoreofthenoveltycriterion.ItusesRNDto
rewardagentsforpositivegainsinnoveltybetweenthecurrent Addressinghardexplorationenvironmentsischallengingbecause
andthenextstates.Thesecondpartisanepisodicrestrictionthat ofthesparsepositiverewardsignalsthatexisttoguidetheagentâ€™s
ensurestherewardisgivenonlywhenstateğ‘  ğ‘¡+1isobservedforthe learningprocess.ThisbecomesevenworsewithMASasthecom-
firsttimeinthisepisode.ThisrestrictionlimitstheuseofNovelD pletionofataskdependsontheactionsofmultipleindependent
todiscretestatespacesasitreliesonanexplicitcountofvisited agents.Whenstrongcoordinationisneeded,agentswillstruggle
states. to find the optimal strategy and settle for an easier suboptimal
jointstrategy,whichisaproblemknownasrelativeovergeneral-
ExplorationviaEllipticalEpisodicBonuses(E3B). WithE3B,Henaff
ization[30,32].Figure1aprovidesanexampleofasocialdilemma
etal.[7]proposeanepisodicbonusbasedonthepositionofthe
gamewhererelativeovergeneralizationoccurs.Theoptimalstrat-
observedstatewithrespecttoanellipsethatfitsallstatesprevi-
egyrequiresbothagentstochooseactionA.Butifonlyoneagent
ouslyencounteredinthecurrentepisode.Formally,itiscomputed
choosesactionA,thepayoffisverybad.Therefore,agentswill
asfollows:
independentlyprefertotakeactionsBorC,asactionAmostoften
ğ‘(ğ‘  ğ‘¡)=ğœ“(ğ‘  ğ‘¡)âŠ¤ğ¶ ğ‘¡âˆ’ âˆ’1 1ğœ“(ğ‘  ğ‘¡), (3)
leadstosub-optimaloutcomes.
with InMAS,thiscanbeseenasaproblemofill-coordinatedexplo-
ğ¶
ğ‘¡âˆ’1=ğ‘¡ âˆ‘ï¸âˆ’1
ğœ“(ğ‘  ğ‘–)ğœ“(ğ‘  ğ‘–)âŠ¤+ğœ†ğ¼, (4)
r oa ft ji oo in n. tA ps ols iu cic ec se is ss rd ee qp ue in red ds io nn oc ro do errd ti on dat ise cd ob ve eh ra wv hio icr hs, oe nx ep slo lr ea at dio tn
o
ğ‘–=1
optimalreturns.IntheexampleofFigure1a,exploringindependent
whereğ¼ istheidentitymatrixandğœ† ascalarcoefficient.ğœ“ isan
strategieswillleadtoultimatelychoosingsuboptimalactionsas
embeddingnetworktrainedusinganinversedynamicsmodel[19]:
theyindividuallymayyieldbetterexpectedreturns.Ontheother
embeddingsoffollowingstatesğœ“(ğ‘  ğ‘¡) andğœ“(ğ‘  ğ‘¡+1) areusedbya
hand,wearguethatuniformlyexploringjointactionswouldenable
separate neural network trained to predict the action ğ‘ ğ‘¡ taken agentstochooseoptimaljointstrategiesmoreoftenandconse-
betweenthesestates.Asaresultofthistrainingprocess,ğœ“ encodes
quentlylearnmoreefficientindividualbehaviors.Theapproach
partsoftheobservationthatarecontrollablebytheagents(details
describedinthefollowingtwosectionsimplementsanalgorithm
in[7]).Intuitively,ğ‘ canbeunderstoodasageneralizationofa
thatefficientlyrewardsagentsforexploringthejoint-observation
count-basedepisodicbonusforacontinuousstatespace.Statesthat
space,inordertoconsistentlyfindoptimalstrategies.
areclosetopreviouslyencounteredstatesinthecurrentepisode
willyieldlowbonuses,whereasstatesthatareverydifferentwill
4.2 Double-timescaleIntrinsicReward
producehighbonuses.
Similarlytopreviousworksonsingle-agentintrinsicmotivation[2],
4 ALGORITHM wedefineanoveltymetricthatcombinestwoexplorationcriteria
workingatdifferenttimescales:
Inthissection,weintroducetheJointIntrinsicMotivation(JIM)ex-
plorationcriterionforcoordinatedmulti-agentexploration.Firstly, â€¢ Alife-longexplorationcriterion(LLEC)thatcaptures
wedescribethemotivationbehindourapproachbyprovidinga hownovelisthecurrentobservationwithrespecttoall
detaileddescriptionoftheproblemofrelativeovergeneralization. observationssincethebeginningoftraining.MaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
â€¢ Anepisodicexplorationcriterion(EEC)thatcaptures
thedifferencebetweenthecurrentobservationandallpre-
viousobservationsinthecurrentepisode.
Intuitively,thelife-longrewardmotivatesagentstosearchfornever-
experienced parts of the environment. Meanwhile, the episodic
bonusinducesmorediversetrajectories.Thesetwoelementswill
feedeachotherandreinforceagentstoefficientlyexploretheir
environment.
Concretely,foreachtransitionfromstateğ‘  ğ‘¡ tothenextstate
ğ‘  ğ‘¡+1,wedefinethedouble-timescaleintrinsicrewardasfollows:
ğ‘Ÿ ğ‘¡(ğ‘  ğ‘¡,ğ‘  ğ‘¡+1)=ğ‘ ğ¿ğ¿ğ¸ğ¶(ğ‘  ğ‘¡,ğ‘  ğ‘¡+1)Ã—ğ‘ ğ¸ğ¸ğ¶(ğ‘  ğ‘¡+1), (5)
withthelife-longnoveltyğ‘ ğ¿ğ¿ğ¸ğ¶ inspiredfromNovelD[35](see
Eq.(2)):
Figure2:ArchitecturefortheJointIntrinsicMotivation(JIM)
ğ‘ ğ¿ğ¿ğ¸ğ¶(ğ‘  ğ‘¡,ğ‘  ğ‘¡+1)=max[ğ‘…ğ‘ğ·(ğ‘  ğ‘¡+1)âˆ’ğ›¼ğ‘…ğ‘ğ·(ğ‘  ğ‘¡),0], (6) algorithm.JIMhasonlyoneintrinsicmotivationmodulefor
withğ›¼ ascalingfactorandğ‘…ğ‘ğ·thenoveltymeasure(seeEq.(1)). the whole multi-agent system, computing novelty of the
Further,theepisodicnoveltyğ‘ ğ¸ğ¸ğ¶ usesthebonusfromE3B[7] jointobservationoğ‘¡.However,agentsonlyusetheirlocal
observationtochoosetheiraction.
(seeEq.(3)):
ğ‘ ğ¸ğ¸ğ¶(ğ‘  ğ‘¡+1)=âˆšï¸ 2ğ‘(ğ‘  ğ‘¡+1). (7)
We remove the episodic restriction of NovelD as it relies on
agent,JIMcomputesonlyoneintrinsicreward.Thisrequiresfewer
anepisodiccountofvisitedstates.Thismakesitimpracticalina
parametersandmakesitpossibletocapturenoveltyattheteam
continuousstatespace,asonestateisveryunlikelytobevisited
level,ratherthanattheindividuallevel.Asagentsarerewardedby
twice.Instead,wescalethelife-longnoveltyusingtheelliptical
thenoveltyofthejointobservation,theywilllearntosearchfor
episodic bonusğ‘ from E3B [7]. This bonus acts as an episodic
newcombinationsofobservationswithotheragentsofthesystem,
restrictionbyscalingğ‘ ğ¿ğ¿ğ¸ğ¶ upordown,dependingonthenovelty ratherthanonlyexploringtheirlocal-observationspace.
ofthecurrentstatecomparedtowhathasbeenobservedduringthe
AsJIMusesjointobservationsforcomputingtheintrinsicreward,
currentepisode.Asğ‘providesverylargebonusesanddecreases
itcanbeassociatedwithanyMADRLalgorithmthatfitsinthe
veryfast,weuseâˆšï¸ 2ğ‘(ğ‘  ğ‘¡+1)tobothsmoothoutlargevaluesand
CTDEparadigm.Thesealgorithmsusuallyemployacentralized
increasesmallones. valuefunction[11,22,34]thatlooksatthejointobservationto
Combiningthesetworewardsmakesitpossibletotaketheben- predict the value of the agentsâ€™ actions. Such centralized value
efitsofboth.ğ‘ ğ¿ğ¿ğ¸ğ¶ pushesagentstoexploreregionsofthestate functionswillbeabletoassociaterewardsprovidedbyJIMtonew
spacethatarenotwell-knowntoagents.Meanwhile,ğ‘ ğ¸ğ¸ğ¶ favors configurationsinthejointobservationspace,thusinducingagents
diversetrajectories,incitingagentstoalwaysseeknewobservations toactivelysearchfortheseconfigurations.
duringasingleepisode.Astheagentsexploretheirenvironment, Onecouldnotethatthejointobservationhastwonotabledraw-
thepredictionerrorofRND(seeEq.(1))slowlydecreases.Thus, backs:thenumberofdimensionsgrowslinearlywiththenumberof
ğ‘ ğ¿ğ¿ğ¸ğ¶ decreasesaswell,tendingtowardzero,allowingagentsto agentsandthereisariskofcapturingredundantinformation.These
progressivelyfocusontheextrinsicreward.Finally,astheepisodic issuesarebothalleviatedbyusingembeddingnetworkstocapture
restrictiondoesnotrelyonanyexplicitcountofvisitedstates,it thecurrentstateofthejointobservationintoamorecondensed
canbeusedincontinuousstatespaces. latentrepresentation.Bothğ‘ ğ¿ğ¿ğ¸ğ¶ andğ‘ ğ¸ğ¸ğ¶ useembeddingnet-
works,respectivelyğœ™ andğœ“ (asdescribedinSection3.2),toencode
4.3 TheJointIntrinsicMotivationalgorithm
thejointobservation.Thisallowsforamorecontrollablenumber
Buildingfromtheintrinsicrewardintroducedpreviously,wepro- ofparametersinJIM,asonlythedimensionoftheinputlayersof
posetheJointIntrinsicMotivation(JIM)algorithmtoinciteMADRL ğœ™ andğœ“ dependonthesizeofthejointobservation.Furthermore,
agentstoexplorethejoint-observationspace.Ateachtimestep,all embeddingnetworkslearntocastawayuselessorredundantinfor-
agentsreceivethesameglobalrewardğ‘Ÿ ğ‘¡ =ğ‘Ÿ ğ‘¡ext+ğ›½ğ‘Ÿ ğ‘¡ğ½ğ¼ğ‘€ ,whereğ‘Ÿ ğ‘¡ext mationinordertoproduceafaithful,morecompactrepresentation
istheextrinsicrewardgivenbytheenvironment,ğ‘Ÿ ğ‘¡ğ½ğ¼ğ‘€ isourjoint ofthejointobservation.Itisimportanttonotethatbothğœ™ andğœ“
explorationcriterion,andğ›½isahyper-parametercontrollingthe wereoriginallyused(respectivelyinRND[3]andE3B[7])with
rawpixelimagesasinput,showingthesignificantdimensionality
weightoftheintrinsicreward.TheexplorationcriterioninJIMuses
reductioncapabilitiesofthesetechniques.
thedouble-timescaleintrinsicrewarddefinedearliertocompute
thenoveltyofthejointobservation:
5 IMPLEMENTATIONDETAILS
ğ‘Ÿ ğ‘¡ğ½ğ¼ğ‘€ (oğ‘¡,oğ‘¡+1)=ğ‘ ğ¿ğ¿ğ¸ğ¶(oğ‘¡,oğ‘¡+1)Ã—ğ‘ ğ¸ğ¸ğ¶(oğ‘¡+1), (8)
Inthenextsection,weuseJIMwithQMIX[22].Weusethedefault
whereoğ‘¡ = {ğ‘œ ğ‘¡ğ‘–} 0â‰¤ğ‘–â‰¤ğ‘,i.e.,theconcatenationofalllocalobser- QMIXarchitectureandhyperparameters,alongwithprioritized
vations.Figure2showsthearchitectureforJIM.Comparedtoa experiencereplay[23].Inallexperiments,wecomparethreealgo-
localmethodthatwoulduseoneintrinsicmotivationmoduleper rithms:JointIntrinsicMotivationforCoordinatedExplorationinMADRL
(a)easy(ğ›¿ =30) (b)hard(ğ›¿ =40) (c)veryhard(ğ›¿ =50)
Figure3:PerformanceofvariantsofQMIXintherel_overgenenvironment,withthreelevelsofdifficulty.Ontop,weshow
theheatmapsrepresentingtherewardfunctionineachinstance,wherethedifficultyisdictatedbythewidthcoefficientof
theoptimalrewardspikeğ›¿ (asdefinedinEq.(9)).Increasingğ›¿ leadstoasmalleroptimalrewardspike.Belowisshownthe
performanceduringtrainingofQMIXwithnointrinsicreward(QMIX),localintrinsicmotivation(QMIX+LIM),andjoint
intrinsicmotivation(QMIX+JIM)(meanandstandarddeviationshownfor15runseach).Weseethataslightdecreaseinthe
sizeoftheoptimalrewardspikeresultsinaconsiderableincreaseinthedifficultyofthetask.
â€¢ QMIX+JIM,augmentingQMIXwithjointexploration,as inacontinuousenvironmentandshowthatexploringthejoint-
showninFigure2anddescribedinSection4.2. observationspacehelpssolvecooperativetasks.Next,wepresent
â€¢ QMIX+LIM,adegradedversionofQMIX+JIMwherelo- anablationstudybycomparingJIMwithtwosimplerversionsthat
cal(ratherthanglobal)intrinsicmotivationisused.Each eachlackoneofthetwoexplorationcriteriadescribedinSection4.2,
agentgeneratesitsownintrinsicrewardbasedsolelyonits showingtheadvantageofcombiningthetwo.Finally,weshowin
localobservation,usingthesamerewarddefinitionasJIM Section6.4thatJIMremainsrelevantwhenproblemsarescaledup.
(seeSection4.2).ThearchitectureforLIM(LocalIntrinsic
Motivation)isdescribedinAppendixA. 6.1 Addressingrelativeovergeneralization
â€¢ Theoriginalstate-of-the-artQMIXalgorithm[22]withno
6.1.1 Environment definition. To demonstrate how joint explo-
intrinsicmotivation,usedasabaseline.
rationhelpssolvetheproblemofrelativeovergeneralization,we
Notethattheonlydifferencebetweenthesethreealgorithmsis designasimpletestenvironmentthatexpandstheexampleshown
thedefinitionoftherewardfunctiongiventoeachagentduring inFigure1a.Inthisenvironmentcalledrel_overgen,twoagents
training.Theactualtrainingandexecutionalgorithmsareidentical. canmoveonadiscreteone-dimensionalaxiswithğ·possiblepo-
ToensureafaircomparisonbetweenJIMandLIM,weusedif- sitions.Thetwoagentsaredenotedbytheirposition,namelyx
ferentvaluesforsomespecifichyperparameters(e.g.,dimensionof (forthefirstagent)andy(forthesecondagent).Ateachtimestep,
thehiddenlayers)inthetwoversionsinorderforthemtohavea agentsobservetheirpositionasaone-hotvector(e.g.,foragentx,
similarnumberoftrainableparameters.Allhyperparametersused ğ‘œ ğ‘¡x={ğ‘œ ğ‘¡x,ğ‘– =1ifx=ğ‘–, 0otherwise} 0â‰¤ğ‘–<ğ·)andcanchoosebetween
inourexperimentsarelistedinAppendixBTË™hecodeusedtorun
threeactions:moveinonedirectionortheother,orstayinposition.
allexperimentsisfreelyavailableonline1.
Theyreceivearewardcorrespondingtotheircombinedposition:
6 EXPERIMENTS ğ‘Ÿ ğ‘¡ext(ğ‘¥,ğ‘¦;ğ›¿)=max(cid:16) ğ‘…+âˆ’ ğ·ğ›¿ (cid:2) (ğ‘¥âˆ’ğ‘Ÿ ğ‘¥+)2+(ğ‘¦âˆ’ğ‘Ÿ ğ‘¦+)2(cid:3),
Inthissection,wepresentasetofexperimentstoevaluatethe (9)
explorationcriterionofJIMwhenusedalongthestate-of-the-art
ğ‘…âˆ’âˆ’ 81 ğ·(cid:2) (ğ‘¥âˆ’ğ‘Ÿ ğ‘¥âˆ’)2+(ğ‘¦âˆ’ğ‘Ÿ ğ‘¦âˆ’)2(cid:3)(cid:17) .
QMIXalgorithm[22].First,weshowtheresultsinasyntheticdis-
creteenvironmentwheretheproblemofrelativeovergeneralization TheresultofthisformulaisdisplayedinFigure1b.Thereward
canbeartificiallytunedandobservethatJIMhelpsalleviatethis combines two hyperboles in opposite corners: one narrow that
issue.Then,wetestourapproachonpseudo-realisticrobotictasks
culminatesatğ‘…+atposition(ğ‘Ÿ ğ‘¥+,ğ‘Ÿ ğ‘¦+),andanothermuchwiderthat
plateausatğ‘…âˆ’ atposition(ğ‘Ÿ ğ‘¥âˆ’,ğ‘Ÿ ğ‘¦âˆ’).Wesettheoptimalrewardğ‘…+
1https://github.com/MToquebiau/Joint-Intrinsic-Motivation to12andthesuboptimalğ‘…âˆ’ to0.ThewidthoftheoptimalrewardMaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
spikeiscontrolledbytheparameterğ›¿:ahigherğ›¿ valueyieldsa
narrowerspike.
Thegoaloftheagentsistofindwheretogotomaximizethe
globalreward.Thewidesuboptimalhyperboleisdeceptiveasit
isanobviouspathforagentstominimizetheirloss.Theoptimal
rewardspikeisdifficulttofindbecauseitcoversasmallportion
ofthestatespace,butitguaranteesmuchgreaterreturns.Wecan
varythedifficultyofthetaskbychangingthewidthofthisoptimal
(a) (b)
rewardspike:thenarrowerthespike,theharderitistofind.
Inthisenvironment,weexpectMADRLmethodstostruggleto
Figure4:ScreenshotsofourcustomtasksintheMPE,agents
findtheoptimalrewardspike.Exploringlocalstatescouldhelp
arethesmallgreycircles.(a)cooperativeboxpushingscenario:
butwouldnotbesufficienttoconsistentlysolvethetask.Asthe
agentsmustdelivertheobject(greencircleinthemiddle)
dimensionğ·ofthelocal-statespaceisfairlysmall,noveltyrewards
tothelandmarkinthebottomrightcorner.(b)coordinated
willquicklyvanishandwillnothelpagentsfindtheoptimalreward
placementscenario:agentsmustnavigatetopositionthem-
spike.Exploringthejoint-observationspaceadequatelyisrequired
selvesonthecoloredcirclesrepresentinglandmarks.
inordertoconsistentlyfindoptimalrewards.AsJIMwillreward
explorationuntilallcombinedpositions(ğ‘¥,ğ‘¦)arevisitedseveral
times,agentswillvisittheoptimalrewardspikemoreoften,thus
helpingthemtolearntheoptimalcoordinatedstrategy.
6.1.2 Results. TheresultsshowninFigure3confirmthehypothe-
sesformulatedintheprevioussection.Weshowtheperformance
ofQMIX,QMIX+LIM,andQMIX+JIMacross15independentruns
each.Further,wepresentresultsinthreedifficultylevelsdictated
bythewidthoftheoptimalrewardspike.Theresultsclearlydemon-
stratetheimportanceofexploringthejoint-statespace.QMIXalone
managestogetapositiverewardontheeasyscenario,butitsperfor-
manceisbothlowerandwithalargerstandarddeviationcompared
tothetwootheralgorithms.Intheharderscenarios,QMIXâ€™sper-
Figure5:TrainingcurvesofthethreevariantsofQMIXinthe
formancedegradesstrongly,neverfindinganypositiverewardin
cooperativeboxpushingtask,withthemeanandstandard
thehardestcase.JIMclearlyimprovestheperformance.Intheeasy
deviationacross11runseach.
scenario,QMIX+JIMconsistentlygoesfortheoptimalrewardspike.
Inthehardersettings,itstillperformswellonaverage,eveninthe
"veryhard"scenariowheretheoptimalrewardspikecoversonly
showsascreenshotofthisscenario.Atthestartofeachepisode,the
0.013%ofallcombinedpositions.TheresultsofQMIX+LIMshow
landmarkisrandomlyplacedinanyoneofthefourcorners.The
thatexploringthelocal-observationspacehelpsagentsfindthe
initialpositionsoftheagentsandtheobjectarerandomlyset.If
optimalrewardspikemoreoften.However,itperformsworsethan
theagentsmanagetopushtheobjectandplaceitonthelandmark,
JIMasitdoesnotensurethatallcombinedpositionsaresufficiently
theepisodeendsandtheyreceivearewardof+100.Agentsalsoget
explored.Thisshowsthatexploringthejoint-observationspaceis
asmallpenaltyof-1ateachtimesteptorewardfasterstrategies.
crucialtoallowagentstodiscoveroptimalcoordinatedbehaviors.
Thisrewardispurposefullydefinedtobeverysparse,inorderto
studyhowexplorationhelpsinthiskindofsituation.
6.2 Coordinationtasksinacontinuous
The second scenario is a cooperative placement task where
environment
agentsmustpositionthemselvesoverlandmarksinordertomaxi-
6.2.1 Environmentdefinitionandsetups. Next,westudyhowJIM mizetheirreward.AsshowninFigure4b,therearetwosetsofthree
scalestomorerealisticcontinuousenvironmentsandmorecomplex coloredlandmarks.Therewardgivenateachtimestepdepends
tasks.Weusethemulti-agentparticleenvironment2(MPE)[11,15] on the placement of the agents on the landmarks. The optimal
tosimulatecooperativerobotictasksthatrequireahighdegreeof stateishavingbothagentsplacedontheredlandmarks,yieldinga
coordination.ThestatespaceofMPEiscontinuous:inoursetups, rewardof+10ateachtimestep.Theblueandyellowlandmarks
agentsreceiveasobservationavectorwiththeirpositioninthetwo- actasdeceivingrewards,yieldingmuchsmallerrewards(+2for
dimensionalspaceand,foralltheotherentitiesintheenvironment, blue,+1foryellow).Toincreasethedeceivingaspectoftheblue
theirrelativepositionandvelocity.Agentsnavigateinaclosed andyellowlandmarks,wealsorewardagentscollectivelyby+0.5if
two-by-two-meterareabychoosingbetweenfivediscreteactions: onlyoneofthemstandsononeofthesetwocolors.Thisleadstoa
moveinanyfourcardinaldirectionsorstayinplace. needforcoordinationbetweenagents,astheywilllocallyfindthat
Thefirsttaskisacooperativebox-pushingtaskthatrequires goingonblueoryellowlandmarkssystematicallyleadstoasmall
agentstopushanobjectandplaceitontopofalandmark.Figure4a reward.Onlyifagentsexploretheirenvironmentwell,willthey
discoverthattheyneedtobebothonredtogettheoptimalreward
2https://github.com/openai/multiagent-particle-envs signal.Importantly,thisscenariofeaturespartialobservability,withJointIntrinsicMotivationforCoordinatedExplorationinMADRL
(a) (b) (a) (b)
Figure6:PerformanceofthethreevariantsofQMIXintheco- Figure7:AblationstudyofJIMinthecoordinatedplacement
ordinatedplacementtask.(a)showsthetrainingcurveswith task.(a)showsthetrainingcurveswiththemeanandstan-
themeanandstandarddeviationacross11independentruns darddeviationacross11independentrunseach,while(b)
each,while(b)displaystheperformanceofeachindepen- displaystheperformanceofeachindependentrunatthelast
dentrunatthelastiterationoftraining.Dashedlineson(b) iterationoftraining.Thetwoablatedversionsonlyfeature
indicatetheoptimal(unattainable)levelofreturnobtained oneofthetwoexplorationcriteriadefinedinSection4.2:
withdifferentstrategies:red,blue,andyellowlinesrepresent JIM-EECforğ‘ ğ¸ğ¸ğ¶ andJIM-LLECforğ‘ ğ¿ğ¿ğ¸ğ¶.Theresultsshow
thereturnobtainedifbothagentsareonlandmarksofthe theimportanceofcombiningthetwocriteria.
relatedcolorduring100steps(thedurationofanepisode),
andthegreylinesimilarlydescribesonlyoneagentbeing
eitheronblueoryellow.
6bshowsthatLIMarguablyperformsworse.Tworunsmanage
tofindtheoptimalstrategy,butLIMoftenperformspoorlywith
agentsonlyhavinginformationaboutentitiesclosetothem(less
onlyoneagentonablueoryellowlandmark.Thisdemonstrates
thansixtycentimetersfromthem),theothersbeingmaskedwith
thatexploringthespaceoflocalobservationscanbehelpful,asit
neutralvalues.Thismeansthatagentsdonotnecessarilyseewhich
pushesagentstoexploretheenvironment.However,exploringlocal
landmarktheotheragentgoesto3.
observationscanalsobemisleadingastheydonotcontainallthe
6.2.2 Results. Resultsoftraininginthecooperativeboxpushing informationaboutthecurrentstateoftheenvironment.WithJIM,
scenarioareshowninFigure5(medianandconfidenceinterval exploringthejoint-observationspaceclearlyimprovesthequality
shownfor11runseach).First,weobservethatQMIXaloneper- ofthechosenstrategies.Morethanhalfofthetime,QMIX+JIM
formsverypoorlyasitisunabletofindthesolutiontothetask. findstheoptimalrewardsignalandlearnsaneffectivestrategyto
Thehighsparsityoftherewardfunctionmakesitimpossiblefor goonredlandmarks,showingthatJIMallowsformoreefficient
agentstodiscovertheobjectivewithrandomexplorationofthe explorationofcoordinatedbehaviors.Whenagentsdonotfindthe
environment.Second,weseethatJIMandLIMachievesimilarlev- optimalstrategy,theystickwiththebestsub-optimalstrategyto
elsofperformance.Whilecoordinationcanhelpagentsperform gobothonblue.Thisshowsthatagentsbenefitfromexploringthe
well,itisactuallynotarequirementforthistask.Infact,oneagent spaceofjointobservationsastheyaredirectlylinkedtotheob-
aloneisabletopushtheobjectandplaceitonthelandmark.Thus, tainedreward,whereaslocalobservationslackcrucialinformation
exploringthespaceofjointconfigurationsisnothelpfulinthissce- tounderstandtheglobalreward.
nario.Thisshowshoweverthatactivelyexploringtheenvironment AnotheradvantageofJIMisthesimplicityofitsarchitecture.
iscrucialintaskswheretherewardfunctionisverysparse. While LIM and similar approaches in previous works [4, 8, 29]
Conversely,experimentsinthecoordinatedplacementtaskde- requirecomputingoneintrinsicrewardforeachagent,JIMonly
monstratewelltheimportanceofexploringjointly.Figure6ashows computesoneintrinsicrewardforthewholegroupofagents.This
thetrainingcurvesofQMIX,QMIX+LIM,andQMIX+JIM,with11 makesJIMsignificantlymoreefficienttorun,withLIMbeingap-
independentrunseach.Figure6bshowstheperformanceofeach proximately 24% slower than JIM to train (see Appendix D for
runatthelastiterationoftraining.Thecoloreddashedlinesgive details).
aninsightintothelevelofstrategylearnedbyeachrun.These
levelsofstrategycanbevisualizedwithexampletrajectoriesdis- 6.3 Ablationstudy
playedinFigure8.QMIXalonealmostalwaysgoesfortheblue
Inthisablationstudy,JIMiscomparedwithtwoablatedversions
landmarks,whilesometimessettlingfortheyellowones.Thisindi- ofthereward:onewithonlytheepisodicexplorationcriterionğ‘
ğ¸ğ¸ğ¶
catesthatwithoutactivelyexploringtheenvironment,QMIXgets
(JIM-EEC) and one with only the life-long exploration criterion
stuckbecauseofdeceptiverewardsandisunabletofindtheoptimal ğ‘ ğ¿ğ¿ğ¸ğ¶ (JIM-LLEC).NotethatJIM-LLECisactuallyequivalentto
strategy.WhileQMIX+LIMseemsslightlybetterthanQMIXonthe
NovelD[35]inthisenvironmentastheepisodicrestrictionofNov-
trainingcurves,theindividualrunperformanceshowninFigure
elD(seeSection3.2)wouldbeineffectiveinacontinuousenviron-
3SeeAppendixCforadetaileddescriptionofthesetwotasks. mentsuchasMPE.Tocomparethesethreeversionsproperly,weMaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
(a) (b) (c) (d)
Figure8:Examplesoftrajectoriesinthecoordinatedplace-
menttask.Theypresentdifferentlevelsofstrategy,with(a)>
(b)>(c)>(d):(a)optimalstrategywithbothagentsonred,(b)
bothonblue,(c)bothonyellow,and(d)oneonblue/yellow.
scaletheintrinsicrewardsofthetwoablatedmodelstobeatasim-
ilarmagnitudeastheintrinsicrewardgeneratedbyJIM.Figure7
showstheresultsoftrainingtheseversionsinthecoordinatedplace-
menttask,with11independentrunseach.Bothablatedalgorithms
performsignificantlyworsethanJIM.First,theepisodicbonusof
JIM-EECalonelacksthemotivationfordiscoveringunseenconfig-
urations.Thus,itexploreslessandisnotabletofindtheoptimal
solutiontothetask.Meanwhile,withouttheepisodicrestriction,
JIM-LLECdevelopslessefficientexplorationstrategies.Thiscon-
firmsthat,asshowninarecentstudy[1],theepisodicrestriction
implementedinNovelDandotherintrinsicrewards[2,20]iscrucial Figure9:TrainingperformanceofQMIX,QMIX+LIMand
fordevelopingefficientexplorationstrategies.Overall,thisproves QMIX+JIM in the four-agent version of rel_overgen. Top
theimportanceofcombiningthetwostagesofexplorationdefined graphshowsthemeanandstandarddeviationacross11runs
inğ‘ ğ¿ğ¿ğ¸ğ¶ andğ‘ ğ¸ğ¸ğ¶. each,bottomgraphdisplaysallsingleruns(QMIXindotted
linesforclarity,QMIX+LIMisomittedasitnevergoesbeyond
6.4 Scalinguptomoreagents thesuboptimalstrategy).
Finally, we evaluate JIM in a scenario with four agents using a
modifiedversionoftherel_overgenenvironmentintroducedin
Section6.1(seeAppendixEformoredetails).Asinthetwo-agent theoptimalsolution,advocatingforthebenefitsofusingaglobal,
version,therewardfunctionhasahighrewardspikeinonecorner ratherthanlocal,intrinsicrewardforexploration.
ofthespaceandalowrewardplateauintheothercorner,making
7 CONCLUSION
relativeovergeneralizationpronetoarise.Withmoreagents,the
numberofdimensionsofthejointobservationincreaseslinearly Inthispaper,wepresentanalgorithmforjointintrinsicmotivation
(inthiscase:from80dimensionswithtwoagentsto160withfour (JIM),whichisthefirstmethodtorewardactiveexplorationof
agents) while the number of possible states increases exponen- thejoint-observationspace.Itcanbeintegratedtoenhanceany
tially,makingthesearchfortheoptimalstrategysignificantlymore Multi-Agent Deep Reinforcement Learning algorithm that uses
challenging. centralizedtrainingwithdecentralizedexecution.Bycombining
Figure9showstheresultsinthisscenariofor11independent JIM with the state-of-the-art QMIX algorithm, we demonstrate
runseach.Aswithpreviousexperiments,JIMimprovestheperfor- thatitoutperformstheoriginalQMIXimplementationaswellasa
manceofQMIXbyupgradingitsexplorationcapabilities.Takinga modifiedQMIXalgorithmusingsingle-agentintrinsicrewards.We
closerlookattheresultsrevealsthatQMIXandQMIX+LIMfindthe showthatactiveexplorationisakeycomponentformulti-agent
optimalsolutioninrespectively2and0runsoutofthe11,while learning in environments with sparse rewards. Moreover, joint
QMIX+JIMfindstheoptimalsolutionin8outof11runsinthe explorationenablesthediscoveryofoptimalcoordinatedbehaviors
allocatedtime(cf.Figure9-bottom).Thetwopositiveresultsfor thatwouldbehardtofindotherwiseastheynecessitateahighlevel
QMIXcanbeattributedtobeneficialinitialconditionsasQMIX ofcoordinationbetweenagents.
neverreachestheoptimalperformanceotherwise.Thisisnotthe Thisshowstheimportanceofusingjointobservationsinthe
casewithQMIX+JIM,whichshowsrobustnesstoinitialconditions processofcomputingintrinsicrewardsforamulti-agentsystem.
thankstoactiveexplorationoftheenvironment.Infact,theimpact Infact,thejointobservationisthebestestimateoftheglobalstate
ofJIMbecomesevidentwhenlookingatcurvesfromindividual oftheenvironmentavailablefortheagents.Usingitallowsmore
runs,whereweconsistentlyobservesignificantperformanceen- efficientlearningofmulti-agentjointbehaviorsandiscomputation-
hancementssubsequenttoaminorinitialdropinefficiency.This allylessexpensivethanhavingtocomputelocalintrinsicrewards
phenomenonreflectsadeliberateshifttowardsexploringnovel foreachagent.Theseresultsshouldencourageresearchonhow
approacheswhenthesystemwouldotherwiseremainstagnant. jointobservationscanbeusedinotherkindsofintrinsicrewards
This is unique to JIM, as QMIX+LIM never succeeds in finding toshapetheagentsâ€™behaviorfurther.JointIntrinsicMotivationforCoordinatedExplorationinMADRL
REFERENCES
MichaelPetrov,HenriqueP.d.O.Pinto,JonathanRaiman,TimSalimans,Jeremy
[1] AlainAndres,EstherVillar-Rodriguez,andJavierDelSer.2022.AnEvaluation Schlatter,JonasSchneider,SzymonSidor,IlyaSutskever,JieTang,FilipWolski,
StudyofIntrinsicMotivationTechniquesappliedtoReinforcementLearning andSusanZhang.2019.Dota2withLargeScaleDeepReinforcementLearning.
overHardExplorationEnvironments.InarXiv:2205.11184. InarXiv:1912.06680.
[2] AdriÃ PuigdomÃ¨nechBadia,PabloSprechmann,AlexVitvitskyi,DanielGuo, [18] Pierre-YvesOudeyerandFredericKaplan.2007.WhatisIntrinsicMotivation?A
BilalPiot,StevenKapturowski,OlivierTieleman,MartinArjovsky,Alexander TypologyofComputationalApproaches.InFrontiersinneurorobotics,Vol.1.6.
Pritzel,AndrewBolt,andCharlesBlundell.2020. NeverGiveUp:Learning https://doi.org/10.3389/neuro.12.006.2007
DirectedExplorationStrategies.In8thInternationalConferenceonLearning [19] DeepakPathak,PulkitAgrawal,AlexeiA.Efros,andTrevorDarrell.2017.
Representations. https://openreview.net/forum?id=Sye57xStvB Curiosity-drivenExplorationbySelf-supervisedPrediction.InProceedingsofthe
[3] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2019.Exploration 34thInternationalConferenceonMachineLearning,Vol.PMLR70.2778â€“2787.
byRandomNetworkDistillation.In7thInternationalConferenceonLearning [20] RobertaRaileanuandTimRocktÃ¤schel.2020.RIDE:RewardingImpact-DrivenEx-
Representations. plorationforProcedurally-GeneratedEnvironments.In9thInternationalConfer-
[4] Yali Du, Lei Han, Meng Fang, Tianhong Dai, Ji Liu, and Dacheng enceonLearningRepresentations. https://openreview.net/forum?id=rkg-TJBFPB
[21] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. 2020.
Tao. 2019. LIIR: Learning Individual Intrinsic Reward in Multi-Agent
WeightedQMIX:ExpandingMonotonicValueFunctionFactorisationforDeep
Reinforcement Learning. In Advances in Neural Information Process-
Multi-AgentReinforcementLearning.InAdvancesinNeuralInformationProcess-
ing Systems, Vol. 32. https://proceedings.neurips.cc/paper/2019/hash/
ingSystems33.10199â€“10210. https://proceedings.neurips.cc/paper/2020/hash/
07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html
73a427badebe0e32caa2e1fc7530b7f3-Abstract.html
[5] YannisFlet-Berliac,JohanFerret,OlivierPietquin,PhilippePreux,andMatthieu
[22] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,Jakob
Geist.2021.AdversariallyGuidedActor-Critic.In9thInternationalConference
Foerster,andShimonWhiteson.2018.QMIX:MonotonicValueFunctionFac-
onLearningRepresentations. https://hal.inria.fr/hal-03167169
torisationforDeepMulti-AgentReinforcementLearning.InProceedingsofthe
[6] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,and
35thInternationalConferenceonMachineLearning,Vol.PMLR80.4295â€“4304.
ShimonWhiteson.2018. CounterfactualMulti-AgentPolicyGradients.In
http://proceedings.mlr.press/v80/rashid18a.html
ProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.32. https:
[23] TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver.2016.Prioritized
//ojs.aaai.org/index.php/AAAI/article/view/11794
ExperienceReplay.In4thInternationalConferenceonLearningRepresentations.
[7] MikaelHenaff,RobertaRaileanu,MinqiJiang,andTimRocktÃ¤schel.2022.Ex-
[24] JÃ¼rgenSchmidhuber.1991.APossibilityforImplementingCuriosityandBore-
plorationviaEllipticalEpisodicBonuses.InAdvancesinNeuralInformation
dominModel-BuildingNeuralControllers.InProceedingsoftheInternational
ProcessingSystems,Vol.35.37631â€“37646. https://openreview.net/forum?id=Xg-
ConferenceonSimulationofAdaptiveBehavior:FromAnimalstoAnimats.222â€“
yZos9qJQ
227.
[8] ShariqIqbalandFeiSha.2019.CoordinatedExplorationviaIntrinsicRewardsfor
[25] ShaiShalev-Shwartz,ShakedShammah,andAmnonShashua.2016.Safe,Multi-
Multi-AgentReinforcementLearning.InarXiv:1905.12127. https://openreview.
Agent,ReinforcementLearningforAutonomousDriving.InarXiv:1610.03295.
net/forum?id=rkltE0VKwH
[26] KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYung
[9] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroA.
Yi.2019.QTRAN:LearningtoFactorizewithTransformationforCooperative
Ortega,DJStrouse,JoelZ.Leibo,andNandodeFreitas.2019.SocialInfluenceas
Multi-AgentReinforcementLearning.InProceedingsofthe36thInternational
IntrinsicMotivationforMulti-AgentDeepReinforcementLearning.InProceed-
ConferenceonMachineLearning,Vol.PMLR97.5887â€“5896.
ingsofthe36thInternationalConferenceonMachineLearning,Vol.97.3040â€“3049.
[27] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
[10] JoelLehmanandKennethO.Stanley.2011.Abandoningobjectives:Evolution
ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,
throughthesearchfornoveltyalone.InEvolutionaryComputation,Vol.19.189â€“
KarlTuyls,andThoreGraepel.2018.Value-DecompositionNetworksForCo-
223.
operativeMulti-AgentLearningBasedOnTeamReward.InProceedingsofthe
[11] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.2017.
17thInternationalConferenceonAutonomousAgentsandMultiAgentSystems.
Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnvironments.In
2085â€“2087.
AdvancesinNeuralInformationProcessingSystems,Vol.30.
[28] MingTan.1993. Multi-AgentReinforcementLearning:Independentversus
[12] AndreiLupu,BrandonCui,HengyuanHu,andJakobFoerster.2021.Trajectory
CooperativeAgents.InProceedingsoftheTenthInternationalConferenceon
DiversityforZero-ShotCoordination.InProceedingsofthe38thInternational
MachineLearning.330â€“337.
ConferenceonMachineLearning,Vol.139.7204â€“7213. https://proceedings.mlr.
[29] TonghanWang,JianhaoWang,YiWu,andChongjieZhang.2020. Influence-
press/v139/lupu21a.html
BasedMulti-AgentExploration.In8thInternationalConferenceonLearning
[13] ZixianMa,RoseEWang,LiFei-Fei,MichaelS.Bernstein,andRanjayKrishna.
Representations. https://openreview.net/forum?id=BJgy96EYvr
2022. ELIGN:ExpectationAlignmentasaMulti-AgentIntrinsicReward.In
[30] ErmoWeiandSeanLuke.2016. LenientLearninginIndependent-Learner
AdvancesinNeuralInformationProcessingSystems,Vol.35.8304â€“8317. https:
StochasticCooperativeGames.InTheJournalofMachineLearningResearch,
//openreview.net/forum?id=uPyNR2yPoe
Vol.17.2914â€“2955.
[14] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019.
[31] ErmoWei,DrewWicke,DavidFreelan,andSeanLuke.2018.MultiagentSoft
MAVEN:Multi-AgentVariationalExploration.InAdvancesinNeuralInforma-
Q-Learning.InarXiv:1804.09817.
tionProcessingSystems,Vol.32. https://proceedings.neurips.cc/paper/2019/file/
[32] RudolfPaulWiegand.2003.AnAnalysisofCooperativeCoevolutionaryAlgorithms.
f816dc0acface7498e10496222e9db10-Paper.pdf
Ph.D.Dissertation.GeorgeMasonUniversity.
[15] IgorMordatchandPieterAbbeel.2018.EmergenceofGroundedCompositional
[33] MichaelWooldridge.2009.Anintroductiontomultiagentsystems.Johnwiley&
LanguageinMulti-AgentPopulations.InProceedingsoftheAAAIConference
sons.
onArtificialIntelligence. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/
[34] ChaoYu,AkashVelu,EugeneVinitsky,YuWang,AlexandreBayen,andYiWu.
paper/view/17007
2021.TheSurprisingEffectivenessofPPOinCooperative,Multi-AgentGames.
[16] FransA.OliehoekandChristopherAmato.2016. AConciseIntroductionto
InarXiv:2103.01955.
DecentralizedPOMDPs. Springer. https://www.ccis.northeastern.edu/home/
[35] TianjunZhang,HuazheXu,XiaolongWang,YiWu,KurtKeutzer,JosephE
camato/publications/OliehoekAmato16book.pdf
Gonzalez, and Yuandong Tian. 2021. NovelD: A Simple yet Effective
[17] OpenAI,ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,Prze-
Exploration Criterion. In Advances in Neural Information Processing Sys-
mysÅ‚awDÄ™biak,ChristyDennison,DavidFarhi,QuirinFischer,ShariqHashme,
tems,Vol.34.25217â€“25230. https://proceedings.neurips.cc/paper/2021/file/
ChrisHesse,RafalJÃ³zefowicz,ScottGray,CatherineOlsson,JakubPachocki,
d428d070622e0f4363fceae11f4a3576-Paper.pdfMaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
APPENDICES
A LOCALINTRINSICMOTIVATION
Figure10:Architectureforlocalintrinsicmotivation(LIM).Eachagenthasitsownmoduleforcomputinganintrinsicreward
basedonitslocalobservation.
B HYPERPARAMETERS
Table2:Hyperparametersusedinthecooperativebox
Table1:Hyperparametersusedintherel_overgenenvironment.
pushingscenario.
Algorithm
Hyperparameter Algorithm
JIM JIM4agents LIM Hyperparameter
JIM LIM
Intrinsicrewardweightğ›½ 1
Intrinsicrewardweightğ›½ 1 1
HEn i Sdc cdo aed ln ii nn d gg im fd ai cm ğ· toâ„ğ· rğ‘–ğ‘‘ğœ™ ğ›¼ğ‘‘/ ğ‘’ğœ“ ğ‘› 16 24 8 0.526 5 ,4 6 0.6 3 62 4 HEn idc do ed nin dg imdim ğ· â„ğ· ğ‘–ğ‘‘ğœ™ ğ‘‘/ ğ‘’ğœ“ ğ‘› 16 24 8 3 62 4
Scalingfactorğ›¼ 0.5
Intrinsicrewardlearningrateğ›¼
ğ‘–ğ‘›ğ‘¡
0.0001 0.0001,0.0002 0.0001
Intrinsicrewardlearningrateğ›¼
ğ‘–ğ‘›ğ‘¡
0.0001
Table3:Hyperparametersusedinthecoordinatedplacementscenario.
Algorithm
Hyperparameter
JIM LIM JIM-LLEC JIM-EEC
Intrinsicrewardweightğ›½ 1,2,4 1,4,8 1,3 0.1,1
Encodingdimğ· ğœ™/ğœ“ 64 36 64 64
Hiddendimğ· â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘› 512 256 512 512
Scalingfactorğ›¼ 0.5
Intrinsicrewardlearningrateğ›¼
ğ‘–ğ‘›ğ‘¡
0.0001
Tables1to3presentthevalueschosenforhyperparameters.Weonlylisthyperparametersspecifictotheintrinsicrewardmodule,
asforQMIXweusethedefaulthyperparametersdescribedintheoriginalpaper[22].Whenweperformedsomesearchoveraspecific
hyperparameter,weputthelistofallthevalueswetriedandputthebestoneinbold.Here,wedetailthedifferentparameterspresented:
â€¢ Intrinsicrewardweightğ›½:weightoftheintrinsicrewardğ‘Ÿ ğ‘–againsttheextrinsicrewardğ‘Ÿ ğ‘’intherewardgiventoagents:ğ‘Ÿ ğ‘¡ =ğ‘Ÿ ğ‘¡ğ‘’+ğ›½ğ‘Ÿ ğ‘¡ğ‘–ğ‘›ğ‘¡ .
â€¢ Encodingdimensionğ· ğœ™/ğœ“:dimensionoftheoutputoftheembeddingnetworksğœ™ andğœ“ usedinğ‘ ğ¿ğ¿ğ¸ğ¶ andğ‘ ğ¸ğ¸ğ¶ respectively(see
Section3).JointIntrinsicMotivationforCoordinatedExplorationinMADRL
â€¢ Hiddendimensionğ· â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›:dimensionofthehiddenlayersintheembeddingnetworkğœ™andğœ“ usedinğ‘ ğ¿ğ¿ğ¸ğ¶ andğ‘ ğ¸ğ¸ğ¶ respectively.
â€¢ Scalingfactorğ›¼:parameterusedinthedefinitionofğ‘ ğ¿ğ¿ğ¸ğ¶ (seeEq.(6)).Itcontrolshowmuchnoveltygainwewantagentstofind
betweeneachstep.
â€¢ Intrinsicrewardlearningrateğ›¼ ğ‘–ğ‘›ğ‘¡:learningrateusedfortrainingtheintrinsicrewardmodule.
C DETAILSONCUSTOMTASKSINTHEMULTI-AGENTPARTICLEENVIRONMENT
C.1 Cooperativeboxpushing
Figure11:Cooperativeboxpushingtask,agentsarethesmallgreycircles,thegreencircleinthemiddleisanobjecttodeliver
tothelandmarkinthebottomrightcorner.
Thecooperativeboxpushingtaskrequirespushingaroundobjectandplacingitontopofalandmark.Theenvironmentisa2x2meter
areawithwallsonthesidesthatblockentitiesfrommovingaway.Ateachtimestep,anagentgetsasobservation:
â€¢ itspersonaldata:itspositionandvelocitypos self,ğ‘¥,pos self,ğ‘¦,vel self,ğ‘¥,vel self,ğ‘¦,
â€¢ dataabouttheotheragent:abooleanindicatingiftheotheragentisvisibleornot,therelativeposition,andthevelocityofthis
agentis_visible agent,distagent,ğ‘¥,distagent,ğ‘¦,velagent,ğ‘¥,velagent,ğ‘¦,
â€¢ dataabouttheobject:abooleanindicatingiftheobjectisvisibleornot, therelativeposition,andthevelocityofthis object:
is_visible object,dist object,ğ‘¥,dist object,ğ‘¦,vel object,ğ‘¥,vel object,ğ‘¦,
â€¢ anddataaboutthelandmark:abooleanindicatingifthelandmarkisvisibleornotandthenumberofthecorneritislocatedinto
(from1to4):is_visible ,corner .
landmark landmark
Thus,theobservationisavectorofdimension16containingthisinformation.Relativepositionsofotherentities(agentorobject)are
actuallythedistancetotheagent,normalizedbytheirrangeofobservation,i.e.,
distagent,ğ‘¥ =
pos agent,ğ‘¥ âˆ’pos self,ğ‘¥
.
obs_range
Theenvironmentcanbeeitherfullyobservableorpartiallyobservable.Inthelattercase,agentshavearangeofobservationof60centimeters
aroundthem.Whenagentsorobjectsareoutsidethisrange,theirrelativepositionismaskedwithonesandtheirvelocitywithzeros.When
thelandmarkisoutsidetherangeofobservation,thecornernumberismaskedwithzero.Inthefullyobservablecase,theobservationrange
issetto2.83meters,i.e.,thelargestdistancepossibletohaveinthis2x2meterarea.
Therewardfunctionisverysparse.Ateachtimestep,agentsreceiveapenaltyof0.1,plusapenaltyof2ifthereisacollisionbetween
agents.Ifthetaskiscompleted,i.e.,thecenteroftheobjectisplacedintheareaofthelandmark,theagentsgetarewardof100andthe
episodeends.
Atthestartofeachepisode,thepositionsofallentitiesintheenvironmentarerandomlyset:theagentsandtheobjectarerandomly
placedinsidetheenvironment,andthelandmarkisplacedinoneofthefourcornersofthemap.MaximeToquebiau,NicolasBredeche,FaÃ¯zBenamar,andJae-YunJun
C.2 Coordinatedplacement
Figure12:Coordinatedplacementtask,agentsarethesmallgreycircles,thecoloredcirclesrepresentlandmarkstheagents
havetonavigateontogainrewards.
Thecoordinatedplacementtaskrequiresnavigatingontopoflandmarksandchoosingtherightlandmarkcolorsinordertomaximizethe
obtainedreward.Theenvironmentisa2x2meterareawithwallsonthesidesthatblockentitiesfrommovingaway.Ateachtimestep,an
agentgetsasobservation:
â€¢ itspersonaldata:itspositionandvelocitypos self,ğ‘¥,pos self,ğ‘¦,vel self,ğ‘¥,vel self,ğ‘¦,
â€¢ dataabouttheotheragent:abooleanindicatingiftheotheragentisvisibleornot,therelativeposition,andthevelocityofthis
agentis_visible agent,distagent,ğ‘¥,distagent,ğ‘¦,velagent,ğ‘¥,velagent,ğ‘¦,,
â€¢ foreachlandmarkintheenvironment:abooleanindicatingifthelandmarkisvisibleornot,therelativepositionofthislandmark,
anditscolorasaone-hotencoding:is_visible landmark,dist landmark,ğ‘¥,dist landmark,ğ‘¦,is_red,is_blue,is_yellow.
Thus,theobservationisavectorofdimension43containingthisinformation.Relativepositionsofotherentities(agentorlandmarks)are
actuallythedistancetotheagent,normalizedbytheirrangeofobservation,i.e.,
distagent,ğ‘¥ =
pos agent,ğ‘¥ âˆ’pos self,ğ‘¥
.
obs_range
Thisscenarioispartiallyobservable.Agentshavearangeofobservationof60centimetersaroundthem.Whenagentsorobjectsareoutside
thisrange,theirrelativepositionismaskedwithonesandtheirvelocitywithzeros.Forlandmarksoutsideoftheobservationrange,the
colorismaskedwithzeros.
Therewardgivenateachtimestepdependsonlyonwhichlandmarkhasanagentplacedontop:
â€¢ iftwoagentsareplacedontopofredlandmarks,ğ‘Ÿ ğ‘¡ğ‘’ =10,
â€¢ iftwoagentsareplacedontopofbluelandmarks,ğ‘Ÿ ğ‘¡ğ‘’ =2,
â€¢ iftwoagentsareplacedontopofyellowlandmarks,ğ‘Ÿ ğ‘¡ğ‘’ =1,
â€¢ ifonlyoneagentisplacedontopofeitherablueoryellowlandmark,ğ‘Ÿ ğ‘¡ğ‘’ =0.5,
â€¢ else,ğ‘Ÿ ğ‘¡ğ‘’ =0.
Atthestartofeachepisode,agentsareplacedrandomlyonthehorizontallineinthemiddleofthemap,i.e.,pos=(ğ‘¥ =uniform(âˆ’1,1),ğ‘¦=
0).Thelandmarksarealwaysinthesamepositions,withthecolorsinthesameorder,asdisplayedinFigure12.JointIntrinsicMotivationforCoordinatedExplorationinMADRL
D EXECUTIONTIMES
Figure13:TrainingcurvesofQMIX,QMIX+LIM,andQMIX+JIMinthecoordinatedplacementtaskwithexecutiontimeonthe
x-axis.QMIXtakesonaverage6.5hourstotrainduring10millionsteps,whileQMIX+JIMtakes8.5hoursandQMIX+LIM10.5
hours.
E N-AGENTrel_overgenENVIRONMENT
Tostudytheproblemofrelativeovergeneralizationwithmorethantwoagents,weextendtherel_overgenenvironmenttoacceptğ‘ agents.
Todoso,wemodifytherewarddefinitiongiveninthepaper(seeEq.(9)):
(cid:32) ğ‘ ğ‘ (cid:33)
ğ‘Ÿ ğ‘¡ext(p;ğ›¿)=max ğ‘…+âˆ’ ğ·ğ›¿ âˆ‘ï¸ (ğ‘
ğ‘–
âˆ’ğ‘Ÿ ğ‘–+)2,ğ‘…âˆ’âˆ’ 81
ğ·
âˆ‘ï¸ (ğ‘
ğ‘–
âˆ’ğ‘Ÿ ğ‘–âˆ’)2 ,
ğ‘–=0 ğ‘–=0
withp={ğ‘ ğ‘–} 0<ğ‘–â‰¤ğ‘ thepositionsoftheagents,ğ›¿thecoefficientcontrollingthesizeoftheoptimalrewardspike,ğ·thedimensionofeach
agentâ€™sstate,ğ‘…+themaximumvalueoftheoptimalrewardspikeplacedatpositionr+ = {ğ‘Ÿ ğ‘–+} 0<ğ‘–â‰¤ğ‘ andğ‘…âˆ’ themaximumvalueofthe
suboptimalplateauplacedatpositionrâˆ’ ={ğ‘Ÿ ğ‘–âˆ’} 0<ğ‘–â‰¤ğ‘.Thisformulayieldsthesameresultsasthetwo-dimensionalexamplesshowninthe
paperbutinaN-dimensionalspace.
Addingagentsincreasesthecomplexityofthetaskexponentially.Tocompensateforthis,wehavetomaketheoptimalrewardspike
largerforthetasktobesolvablebyQMIX.Inthe4-agentexperiments,weuseğ›¿ =0.9.
Finally,withfouragentswehadtolowertheinitialvalueoftheğœ–parameterofQMIXforitsğœ–-greedystrategy.Wefoundthatincreasing
thenumberofagentsledtobadresultswiththedefault0.3initialvalueofğœ–.Withthishyperparametersetto0.1,theresultsweresignificantly
better.Thisislikelyduetothefactthatagentschooseseparatelyiftheyexploreorexploit(atleastthatisthecaseinourimplementation),
meaningthatincreasingthenumberofagentsleadstohavingmorerandomnessintheselectionofeachjointaction.