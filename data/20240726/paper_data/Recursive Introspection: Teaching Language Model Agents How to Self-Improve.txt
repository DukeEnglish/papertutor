Recursive Introspection: Teaching Language
Model Agents How to Self-Improve
YuxiaoQu1,TianjunZhang2,NamanGarg3 andAviralKumar1
1CarnegieMellonUniversity,2UCBerkeley,3MultiOn
Acentralpieceinenablingintelligentagenticbehaviorinfoundationmodelsistomakethemcapableofintrospecting
upontheirbehavior,reasoning,andcorrectingtheirmistakesasmorecomputationorinteractionisavailable. Even
thestrongestproprietarylargelanguagemodels(LLMs)donotquiteexhibittheabilityofcontinuallyimproving
theirresponsessequentially,eveninscenarioswheretheyareexplicitlytoldthattheyaremakingamistake. Inthis
paper,wedevelopRISE:RecursiveIntroSpEction,anapproachforfine-tuningLLMstointroducethiscapability,
despitepriorworkhypothesizingthatthiscapabilitymaynotbepossibletoattain. Ourapproachprescribesan
iterativefine-tuningprocedure,whichattemptstoteachthemodelhowtoalteritsresponseafterhavingexecuted
previouslyunsuccessfulattemptstosolveahardtest-timeproblem,withoptionallyadditionalenvironmentfeedback.
RISEposesfine-tuningforasingle-turnpromptassolvingamulti-turnMarkovdecisionprocess(MDP),where
the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning,
we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to
recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE
enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks,
outperformingseveralsingle-turnstrategiesgivenanequalamountofinference-timecomputation. Wealsofind
that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE
makesmeaningfulimprovementstoresponsestoarriveatthecorrectsolutionforchallengingprompts,without
disruptingone-turnabilitiesasaresultofexpressingmorecomplexdistributions.
1. Introduction
A promising approach to utilizing and deploying foundation models, and in particular, large language
models (LLMs), is to treat them as general-purpose decision-making machines, or â€agentsâ€. To be
successful, an LLM agent must not just provide plausible completions for input text, but must exhibit
interactive, goal-directed behavior to accomplish a given task. Put in abstract terms, this requires
masteringtwoqualities: (a)producingresponsesthatexplicitlyseekinformationaboutthetask,followed
by (b) making decisions and improving them by â€thinkingâ€ and verifying them at inference time. For
instance,tosucceedinusinganewcodinglibrary,aneffectiveLLMagentshouldfirstsynthesizeprograms,
thentrythemostpromisingsubsetagainstacompiler,usetheresultingfeedbacktoimprovetheprogram,
and repeat the process for multiple turns. Having the ability to successfully improve a response in
sequential attempts is equivalent to a form of â€self-improvementâ€, at test time.
To enable test-time self-improvement, recent approaches attempt to repurpose the knowledge already
stored in pre-trained models via few-shot prompting [7, 15, 31, 52, 64]. Although prompt tuning in
conjunction with feedback is effective in eliciting improved responses from capable models, it fails to
produce models that can succeed in complex tasks by correcting their own mistakes, such as those that
require logical reasoning [21, 55]. In many of these problems, models contain the â€œknowledgeâ€ needed
toanswerachallengingprompt,butarenotabletoelicitthatknowledgeevenwhenaskedtosequentially
correct their mistakes. Fine-tuning the LLM on domain-specific question-answering data [6, 29, 39] can
help, but it still does not teach the agent a test-time improvement strategy (see Section 6). A strategy for
Correspondingauthor(s):yuxiaoq@andrew.cmu.edu;ThisworkwasdoneatCarnegieMellonUniversity.
4202
luJ
52
]GL.sc[
1v91281.7042:viXraRecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure 1: RecursiveIntrospection(RISE).Usingiterativemulti-roundtrainingonon-policyrolloutsandsupervisionfroma
rewardfunction,RISEtrainsmodelsthatarecapableofimprovingthemselvesovermultipleturns.Atinference,werunmajority
votingoncandidateoutputsfromdifferentturnstoobtainthefinalresponse.
improving responses over sequential attempts at test time is crucial for tackling challenging prompts,
where directly attempting the problem in one shot may largely be futile.
Can we train models to be capable of improving their own responses? If done correctly and on a
diverse set of problems and scenarios, this could introduce in an LLM, a general procedure for â€œhowâ€ it
can tackle a hard prompt by improving itself as opposed to supervising it with â€œwhatâ€ to respond with,
which may not generalize as the test prompt becomes out of distribution. Although one straightforward
approachtoinducingthiscapabilityintoamodelwouldbetogeneratedatathatshowcaseimprovements
over multiple sequential turns (potentially from highly capable models), we find that simply imitating
these data is not sufficient to enable this capability (Section 6.4). Quite well, this is due to two reasons:
First, multi-turn data from a different model would not show improvements in the kinds of errors the
learner would make, thereby being irrelevant to the learner [24]. Second, often sequential multi-turn
data collected from proprietary models is also not of high quality since these models are typically not
good at proposing meaningful improvements to their own errors [21] even though they can still provide
useful responses to the problem at hand. Therefore, we need a different strategy to endow models
with a self-improvement capability. Our key insight is to supervise improvements to the learnerâ€™s own
responses in an iterative fashion, taking inspiration from methods in online imitation learning [36] and
reinforcement learning (RL) [45]. This supervision can be in the form of oracle responses to the prompt
sampled i.i.d. from more capable models, or be generated from the learner itself.
Our contribution is an algorithm RISE: Recursive Introspection (Figure 1) that utilizes these insights to
improvetheself-improvementcapabilityofanLLMoverthecourseofmultipleattemptsatagivenprompt.
In each iteration, our approach bootstraps on-policy rollouts from the learner with better responses at
the next turn obtained by running best-of-N (using a success indicator on the task) on multiple revision
candidates obtained by sampling from the learner itself or using responses from a more capable model,
whichever is more convenient. In this way, we are able to construct rollouts that demonstrate the learner
howitcanimproveitsresponsesunderitsowndistribution. Then,wefine-tunethelearneronthesedata
using a reward-weighted regression (RWR [34, 35]) objective, that is able to learn from both high- and
low-quality parts of such rollouts. By iteratively repeating this procedure, we are able to instill a general
self-improvement capability into an LLM. Our results show that LLMs trained via RISE can produce
correct responses on more prompts, improving over turns for more challenging prompts.
Even though strong base and intruction-tuned LLMs [23, 58] often fail to improve their responses over
multiplesequentialattempts(evenwhenexplicitlytoldabouttheirmistakespreviously),RISEsuccessfully
2RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
endows similarly-sized LLMs with self-improvement capabilities, resulting in monotonically increasing
taskperformanceaftereachturn. Specifically,ontheGSM8K[11]dataset,RISEdemonstratessignificant
improvementovervariousmodels. RISEimprovestheperformanceofLLaMa3-8Bby8.2%andMistral-7B
by6.6%,entirelyusingtheirowndata. RISEattainsa17.7%improvementforLLaMa2-7Boverthecourse
of 5-turn introspection (outperforming parallel sampling from the first turn), and a 23.9% improvement
for Mistral-7B. In contrast, GPT-3.5 itself only improves by 4.6% over five turns. We see similar trends on
the MATH dataset [18], where RISE improves LLaMa2-7B by 4.6% and Mistral-7B by 11.1% over five
turns. We also study why and how RISE is able to induce self-improvement abilities and show that this
ability generalizes to out-of-distribution prompts as well. These results consistently demonstrate RISEâ€™s
effectiveness in enhancing mathematical reasoning capabilities for different models.
2. Related Work
Several prior works build techniques to improve reasoning and thinking capabilities of foundation
models for downstream applications. Typically these works focus on building prompting techniques
for effective multi-turn interaction with external tools [5, 7, 14, 32, 49, 54, 56], sequentially refining
predictions by reflecting on actions [7, 15, 63], asking the model to verbalize its thoughts [33, 52, 65],
asking the model to critique and revise itself [31, 40] or by using other models to critique a primary
modelâ€™s responses [2, 12, 20, 54]. Although a subset of this work does improve its own responses, this
self-correction ability often requires access to detailed error traces (e.g., execution traces from code
compilers [7, 31]) in order to succeed. In fact, [21] and Table 1 both indicate that self-improvement
guided by the LLM itself (i.e., â€œintrinsic self-correctionâ€) is often infeasible for off-the-shelf LLMs even
when they contain the knowledge required to tackle the prompt given, but fine-tuning with RISE
induces this capability as we show in this paper.
Beyond prompting, previous work also attempts to fine-tune LLM to obtain self-improvement capabili-
ties [6, 39, 62]. These works attempt to improve reasoning performance by training on self-generated
responses [30, 46, 57, 58, 60]. To achieve this, these works use a combination of learned verifiers [28,
47,50],search[13,26,33,38],contrastivepromptingonnegativedata[9,48],anditeratedsupervised
or reinforcement learning (RL) [8, 37, 59]. Although our approach also trains on model-generated
data, we aim to introduce a complementary capability to improve performance over sequential turns
of interaction, rather than to improve single-turn performance alone. Other work fine-tunes LLMs for
multi-turn interaction directly via RL [41, 66]: while this is indeed related, single-turn problems posed
in multi-turn scenarios require addressing distinct challenges than generic multi-turn RL: (i) sample-
efficiency is not a concern since the entire environment is fully characterized by the training dataset of
prompts and oracle answers and dynamics are deterministic, and (ii) we need to generalize to novel test
prompts. Multi-turn RL focuses on sample efficiency, which is not as critical in our setting, though of
course learning to generalize from a limited number of initial states would be appealing. Our main focus
is to show that it is possible to train models for self-improvement via appropriately designing multi-turn
fine-tuning objectives. This is orthogonal from the choice of training approach (RL or not).
The most related to our work are GLoRE [17] and Self-Correct [53], which train separate models to
identify errors and refine incorrect answers of other LLMs. Unlike these works, our approach trains a
single model to produce answers and improve them over more than two turns, which is the maximal
number of turns studied in these works. We show that doing so successfully requires careful design
choices: aniterativeon-policydatagenerationstrategyalongwithatrainingobjectivethatcanlearnfrom
both successful and unsuccessful rollouts. From an algorithmic point of view, RISE is similar to online
3RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure2: Left: Problemformulation.Weconvertsingle-turnproblemsintomulti-turnMDPsasdiscussedinSection4.1.The
stateisgivenbytheprompt,historyofpriorattempts,andoptionalfeedbackfromtheenvironment.Anactionisaresponse
generatedfromtheLLMgiventhestateofmulti-turninteractionsofar.Right:Datacollection.Wecollectdatabyunrollingthe
currentmodelğ‘˜âˆ’1timesfollowedbyanimprovedversionoftheresponse,whichisobtainedbyeither(1)self-distillation:
samplemultipleresponsesfromthecurrentmodel,andusethebestresponse,or(2)distillation:obtainoracleresponsesby
queryingamorecapablemodel.Ineithercase,RISEthentrainsonthegenerateddata.
imitation learning [36, 44], in that it queries expert supervision on states attained by on-policy rollouts.
On-policy distillation for LLMs [1, 4] utilizes this idea, but queries an expert to provide completions on
partial responses instead of sequential attempts, that we do in this work.
3. Problem Setup and Preliminaries
ThegoalofourworkistoimproveLLMperformanceoversequentialattempts/turnsatagivenproblem.
Concretely, given a dataset ğ’Ÿ = {(ğ‘¥ ,ğ‘¦*)}ğ‘ of problems ğ‘¥ and oracle responses ğ‘¦*, our goal is to
ğ‘– ğ‘– ğ‘–=1 ğ‘– ğ‘–
obtain an LLM ğœ‹ (Â·|[ğ‘¥,ğ‘¦Ë† ,ğ‘ ]) that, given the problem ğ‘¥, previous model attempts ğ‘¦Ë† at the problem,
ğœƒ 1:ğ‘¡ 1:ğ‘¡ 1:ğ‘¡
and auxiliary instructions ğ‘ (e.g., instruction to find a mistake and improve the response; or additional
1:ğ‘¡
compilerfeedbackfromtheenvironment)solvesagivenproblemascorrectlyaspossible. Tothisend,we
encode this goal into the following learning objective that we wish to optimize:
ğ¿
âˆ‘ï¸
max E [I(ğ‘¦Ë† == ğ‘¦*)]. (3.1)
ğœ‹
ğ‘¥,ğ‘¦*âˆ¼ğ’Ÿ,ğ‘¦^ ğ‘–âˆ¼ğœ‹ ğœƒ(Â·|[ğ‘¥,ğ‘¦^ 1:ğ‘–âˆ’1,ğ‘1:ğ‘–âˆ’1]) ğ‘–
ğœƒ
ğ‘–=1
Unlike standard supervised fine-tuning that trains the model ğœ‹ to produce a single response ğ‘¦Ë† given ğ‘¥,
Equation 3.1 trains ğœ‹ to also appropriately react to a given history of responses from its own previous
attempts ğ‘¦Ë† . Equation 3.1 most closely resembles an RL objective, and we will indeed develop our
1:ğ‘–âˆ’1
approach by converting a single-turn problem into a multi-turn MDP. Finally, note that prompting-based
methods such as Self-Refine [31] can still be viewed as training ğœ‹ to optimize ğœ‹(ğ‘¦*|ğ‘¥) but only when
only allowed to modulate the prompt ğ‘ to optimize Equation 3.1. Naturally, since the parameters ğœƒ are
ğ‘–
unchanged, this would not be effective in optimizing the objective fully.
4. RISE: Recursive Introspection for Self-Improvement
Since even strong off-the-shelf models do not exhibit an effective ability to improve themselves when
provided with sequential attempts at a given problem [21], a natural next step is to ask how to train
models to induce this capability. In this section, we will develop our approach, RISE, for fine-tuning
foundation models towards improving their own predictions over multiple turns. Our approach will first
convert a problem into a multi-turn MDP, then collect data, and finally run offline reward-weighted
supervised learning in this multi-turn MDP to induce this capability.
4RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
4.1. Converting Single-Turn Problems into a Multi-Turn Markov Decision Process (MDP)
The first step in building our approach is to procedurally construct a multi-turn MDP out of a single-turn
dataset of prompts and oracle responses (Figure 2, Left). Given a dataset, ğ’Ÿ = {(ğ‘¥ ,ğ‘¦*)}, consisting of
ğ‘– ğ‘–
prompts ğ‘¥ and corresponding oracle responses ğ‘¦* (e.g., math questions and natural language responses
ğ‘– ğ‘–
to those questions), we will construct an induced MDP â„³ from ğ’Ÿ, and then learn policies in this MDP.
An initial state in this MDP is a possible prompt ğ‘¥ âˆˆ ğ’Ÿ. We denote the output response from the
ğ‘–
foundationmodelasactionğ‘. Givenastateğ‘ ,thenextstatecanbeobtainedbyconcatenatingthetokens
representing ğ‘  with the action ğ‘ proposed by the model, and an additional fixed prompt ğ‘“ that asks
the model to introspect, e.g., â€œthis response is not correct, please introspect and correct your answer.â€ (the
exact prompt is shown in Appendix D.4). The reward function is a sparse binary indicator of answer
correctness at a given state ğ‘ , ğ‘Ÿ([ğ‘¥ ,Â·Â·Â·],ğ‘) = 1 if and only if ğ‘ = ğ‘¦* and is obtained from an answer
ğ‘– ğ‘–
checking function. This construction from dataset ğ’Ÿ to MDP â„³ is shown below:
ğ’Ÿ = {(ğ‘¥ ,ğ‘¦*)} â†’ â„³ : ğœŒ(ğ‘  ) = Unif(ğ‘¥ ,ğ‘¥ ,Â·Â·Â· ,ğ‘¥ ) (4.1)
ğ‘– ğ‘– 0 1 2 ğ‘
ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘) = ğ›¿(ï¸€ ğ‘ â€² = concat[ğ‘ ,ğ‘,ğ‘“])ï¸€ (4.2)
ğ‘Ÿ(ğ‘ ,ğ‘) = 1(ğ‘ = ğ‘¦* if ğ‘¥ âˆˆ ğ‘ ). (4.3)
ğ‘– ğ‘–
4.2. Learning in the Multi-Turn MDP
With the MDP construction in place, the next step involves training a model to improve itself over the
course of a rollout. We subscribe to an offline approach to learning that we describe in the following.
Step 1: Data collection for self-improvement. To ensure that rollout data from this multi-turn MDP is
useful for teaching the model how to self-improve, it must satisfy a few desiderata: (1) it must illustrate
the mistakes that the learner is likely to make and showcase how to improve upon them in the next
attempt, (2) the data must illustrate responses that are relevant to the model given the problem and
previous attempts in context, and (3) it must not contain any rollout that degrades in a subsequent turn.
Our data collection strategy (Figure 2, Right) satisfies these desiderata.
In a given round ğ‘˜, for a given problem ğ‘¥ , we unroll the current model ğœ‹ (Â·|Â·) to produce multiple
ğ‘– ğœƒ
ğ‘˜
sequential attempts, denoted by ğ‘¦ğ‘– âˆ¼ ğœ‹ (Â·|ğ‘ ğ‘–). In problems, where external input (e.g., compiler
ğ‘¡ ğœƒ ğ‘˜ ğ‘¡
feedback) is available, we also observe a variable-length, natural language external input, ğ‘“ğ‘– (e.g., in
ğ‘¡
mathproblemsweaskthemodeltocorrectitself). Wealsoobserveascalarrewardvalueğ‘Ÿ(ğ‘ ğ‘–,ğ‘¦ğ‘–),denoted
ğ‘¡ ğ‘¡
as ğ‘Ÿ ğ‘¡ğ‘– in short. Let us denote this dataset of â€œon-policyâ€ model rollouts as ğ’Ÿon-policy := {(ğ‘ ğ‘– ğ‘¡,ğ‘¦ğ‘– ğ‘¡,ğ‘“ ğ‘¡ğ‘–,ğ‘Ÿ ğ‘¡ğ‘–)ğ‘‡ ğ‘¡=1}.
For each time-step, we construct an improved version of the response ğ‘¦ğ‘– that we will denote by ğ‘¦Ëœğ‘–. We
ğ‘¡ ğ‘¡
alsorecordtherewardscoreassociatedwiththisimprovedresponseasğ‘Ÿ(ğ‘ ğ‘–,ğ‘¦Ëœğ‘–),orğ‘ŸËœğ‘– inshort. Toobtain
ğ‘¡ ğ‘¡ ğ‘¡
animprovedversionofaresponseğ‘¦ğ‘–,wecanemployseveralstrategies. Perhapsthemoststraightforward
ğ‘¡
approach is to query an off-the-shelf more capable model to provide a correct response given the prompt
ğ‘¥ , the previous response ğ‘¦ğ‘–, and an optional external feedback ğ‘“ğ‘–. We refer to this as the distillation
ğ‘– ğ‘¡ ğ‘¡
variant of our approach, since it uses a strong â€œteacherâ€ model to guide self-improvement (note that
this is different from the classic notion of knowledge distillation, and we will in fact show results in
Section 6.1 that will help understand the differences).
ğ’ŸËœ on-policy+distill :=
{ï¸
{ï¸€(ï¸€ ğ‘ ğ‘– ğ‘¡,ğ‘¦Ëœğ‘– ğ‘¡,ğ‘“ ğ‘¡ğ‘–,ğ‘ŸËœ ğ‘¡ğ‘–)ï¸€}ï¸€ğ‘‡
ğ‘¡=1}ï¸|ğ’Ÿ|
. (4.4)
ğ‘–=1
5RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
The second variant of our approach, which alleviates the need for a teacher model, involves constructing
animprovedresponsebysamplingmultipletimesfromthelearneritself. Werefertothisapproachasthe
self-distillation variant. Concretely, for each state in the dataset, ğ‘ ğ‘–
ğ‘¡
âˆˆ ğ’Ÿon-policy, we sample ğ‘ responses
ğ‘¦Ëœğ‘–[0],ğ‘¦Ëœğ‘–[1],Â·Â·Â· ,ğ‘¦Ëœğ‘–[ğ‘] âˆ¼ ğœ‹ (Â·|ğ‘ ğ‘–), and use the best response from these ğ‘ candidates (as measured by
ğ‘¡ ğ‘¡ ğ‘¡ ğœƒ ğ‘¡
the associated reward values ğ‘ŸËœğ‘–[0],Â·Â·Â· ,ğ‘ŸËœğ‘–[ğ‘]) to relabel the model response at the next step ğ‘¡+1 in an
ğ‘¡ ğ‘¡
improvement trajectory. Formally, say ğ‘¦Ëœğ‘–[ğ‘š] = argmax ğ‘Ÿ(ğ‘  ,ğ‘¦Ëœğ‘–[ğ‘—]), then we label the responses in
ğ‘¡ ğ‘—âˆˆ[ğ‘] ğ‘– ğ‘¡
the dataset ğ’Ÿon-policy at step ğ‘¡+1 with the improved response and its associated reward value ğ‘ŸËœ ğ‘¡ğ‘–[ğ‘š]:
ğ’ŸËœ on-policy+self-distillation :=
{ï¸
{ï¸€(ï¸€ ğ‘ ğ‘– ğ‘¡+1,ğ‘¦Ëœğ‘– ğ‘¡[ğ‘š],ğ‘“ ğ‘¡ğ‘– +1,ğ‘ŸËœ ğ‘¡ğ‘–[ğ‘š])ï¸€}ï¸€ğ‘‡ ğ‘¡=âˆ’
01}ï¸|ğ’Ÿ|
. (4.5)
ğ‘–=1
Step 2: Policy improvement. With the aforementioned data construction schemes, we can now train a
model on these datasets. While in general, any offline RL approach can be used to train on these data,
in our experiments we adopt an approach based on weighted supervised learning [35] due to ease of
experimentation and its simplicity. In particular, we perform a weighted supervised regression, where
the weights are given by the exponential transformation of the reward values in ğ’ŸËœ.
[ï¸ƒ ğ‘‡ ]ï¸ƒ
âˆ‘ï¸
Reward-weighted RL: max E logğœ‹ (ğ‘¦Ëœğ‘–|ğ‘ ğ‘–)Â·exp(ğ‘Ÿğ‘¡/ğœ) , (4.6)
ğœƒ
ğ‘¥ğ‘–âˆ¼ğ’ŸËœ ğœƒ ğ‘¡ ğ‘¡ ğ‘–
ğ‘¡=1
where ğœ is a temperature parameter to further expand or narrow the difference between good and bad
actions. In our preliminary experiments, we found that Equation 4.6 can often induce a bias towards
increasing log likelihoods of responses where rewards are high, prioritizing updates on easy problems
where rewards are already high. To address this issue, we apply a slight modification to Equation 4.6
and center the exponentiated rewards around the mean value averaged across all attempts on a given
prompt, akin to advantage-weighted regression [34]. We find that the use of advantages in place of
rewards helps us avoid the â€œrich-gets-richerâ€ phenomenon with easy problems.
4.3. Inference at Deployment Time
RISE can be run in two modes at inference time. Perhaps the most straightforward way to run the
policy ğœ‹ (Â·|Â·) trained by RISE is within a multi-turn rollout, where the model samples a new response
ğœƒ
conditioned on the past context (i.e., state in the multi-turn MDP). This past context consists of the
external feedback ğ‘test concerning the response ğ‘¦test and the rollout terminates as soon as the current
ğ‘– ğ‘–
responseisjudgedtobecorrectaccordingtotheenvironmentâ€™sanswerverificationfunction. Putinother
words, we terminate the rollout as soon as the reward is equal to the reward for the oracle response:
ğ‘Ÿ(ğ‘¥,ğ‘¦test ) = ğ‘Ÿ(ğ‘¥,ğ‘¦*). This protocol invokes queries to the reward function after each turn in the rollout.
ğ‘–
Since several reward function queries are performed, we refer to this approach as â€œwith oracleâ€.
RISE can also be run in a mode that avoids the need to query the answer checker or the reward function
within a rollout. In this case, we run full-length rollouts by forcing the model to retry, ignoring the
correctness of the response. We then utilize a self-consistency mechanism [51] based on majority voting
to decide the candidate response at the end of each turn. Concretely, at the end of each turn ğ‘—, we
identify the response by running a majority vote over all response candidates from the previous turns
(ï¸ )ï¸
(maj ğ‘¦test ,ğ‘¦test ,Â·Â·Â· ,ğ‘¦test ), including turn ğ‘—. We call this â€œwithout oracleâ€. A schematic illustration of
0 1 ğ‘—
these approach is shown in Figure 3. Most of our evaluations use no oracle.
6RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure 3: RISEInference.TherearetwowaystoquerythemodeltrainedviaRISEuponinference:(1)withoracle(Left):
eachtimethemodelimprovesitsresponse,itisallowedtocheckitsansweragainstanenvironmentandterminateearlyassoon
asacorrectanswerisfound;or(2)withoutoracle(Right):weaskthemodeltosequentiallyreviseitsownresponsesktimes,
andperformmajorityvotingonallcandidateoutputsfromdifferentturnstoobtainthefinalresponse.
4.4. Practical Algorithm and Implementation Details
AcompletealgorithmicpseudocodeforeachapproachisshowninAppendixC.Wetrained7Bmodelsvia
RISEandfoundthatthesemodelsoftencouldnotadheretoresponsestyleandinstructionsforimproving
their responses when generating on-policy data. As a result, before running on-policy data collection, we
find it often useful to run an initial phase of supervised fine-tuning on in-domain, multi-turn rollouts
generated from a capable model to provide style and instruction-following information to the learner.
We call this the â€œknowledge boostingâ€ stage. We then run on-policy rollouts starting from a boosted
model. In each iteration, we generate 1 trajectory for each unique problem. We then run fine-tuning,
with hyperparameters and details in Appendix D. For iterative fine-tuning, we find that starting from the
base model but training on data from all iterations thus far is more beneficial than continued fine-tuning
from the checkpoint obtained in the previous iteration.
5. When and Why is Self-Improvement Over Turns Possible?
A natural question to ask is why self-improvement with RISE even possible. One might surmise that the
model may simply not have enough knowledge to correct its own mistakes if it is unable to correctly
answer the problem in the first turn. Then, why is it possible to teach the model to correct its own
mistakes? In this section, we provide the reason why this kind of self-improvement is possible, supported
with empirical evidence to justify our hypotheses.
Iteratively teaching a model how to make updates on a given response can be crucial when representing
the target distribution ğ‘*(ğ‘¦|ğ‘¥) requires more capacity than what the model ğœ‹ affords by conditioning
ğœƒ
on only the input prompt tokens. When the target distribution requires greater capacity, learning a
sequence of conditionals, ğœ‹ (ğ‘¦ |ğ‘¥,ğ‘¦ ) followed by marginalization is expected to induce a more
ğœƒ ğ‘–+1 0:ğ‘–
flexiblemarginaldistributionoverğ‘¦ givenğ‘¥. Thishypothesisisakintothedifferencebetweendiffusion
ğ‘‡
models[42]andvariationalautoencoders(VAEs)[25]inimagegeneration: iterativelyfittingasequence
ofgenerativedistributionsoverintermediatenoisyinputsinadiffusionmodelgivesrisetoamoreflexible
distribution [43] than monolithic variational auto-encoding, even though diffusion models still utilize
7RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure 4: Theprobabilityofthetrueanswergiventhe Figure 5: The training perplexity (loss) of fitting only
prompt. ObservethatmodeltrainedwithRISEhashigher theoracleanswerorasequenceofanswers.Observethat
probabilityforthetrueanswer. fittingasequenceofanswers(RISE)reducesthelossmore
thanfittingonlytheoracleanswer(Classic).
an evidence lower-bound objective(ELBO). While the diffusion process utilizes hand-designed noise
schedules, RISE utilizes the base model itself to induce iterative improvements.
Toverifyifthishypothesisistrue,wetrackedthetrainingun-weighted,negativelog-likelihoodloss(NLL)
values for the oracle response ğ‘¦* given the input prompt ğ‘¥ marginalized over intermediate steps in a
multi-turnrollout,andcompareditagainsttheNLLvaluesâˆ’logğ‘ (ğ‘¦*|ğ‘¥)attainedbydirectlyattempting
ğœƒ
to predict the final response in Figure 4 (labeled as â€œClassicâ€). Concretely, we sampled 256 prompts ğ‘¥
and their oracle responses ğ‘¦* and computed the average âˆ’logğ‘ (ğ‘¦*|ğ‘¥) across all ğ‘¥, along with a 95%
ğœƒ
confidenceintervalfordifferentcheckpointsduringtraining. Wefindthatforanygivennumberofepochs
(including fractional number of epochs on the x-axis), the NLL value is lower when conditioning on
multi-turn data that RISE generates in comparison with oracle responses to the prompts obtained from
an expert. This suggests that RISE is able to utilize the computation of tokens from previous turns to
model the target distribution. We also measure the average NLL loss on all samples through training,
sampledi.i.d. fromthetrainingdatasetforRISEandclassicfine-tuningandobserveasimilartrend: RISE
is able to reduce loss more than the standard approach, attaining lower perplexity values (Figure 5).
Of course, in problems that require â€œknowledge-basedâ€ question answering, it is not possible for the
model to produce any meaningful improvements because learning ğ‘*(ğ‘¦|ğ‘¥) is not bounded by insufficient
capacity of ğœ‹ (ğ‘¦|ğ‘¥), but is rather unable to match ğ‘* due to the absence of features that are critical
ğœƒ
to learn the correct mapping from ğ‘¥ to ğ‘¦. We expect that training with RISE would only incentivize
hallucinations in this case [24], since more input tokens appearing from previous attempts would only
provideeasierwaystopickuponspuriouscorrelations. However,thisisnotthefailuremodeonreasoning
problems[27],wheremaj@Kratesatturn1tendtobehigherthanpass@1aswefindinourexperiments
(indicating that performance can be improved by sampling the model itself). In fact, in Figure 6 we also
showthatthesequentialprocedurelearnedbyRISEcanevensolveasignificantfractionofproblemsthat
were unsolved by pass@B for much larger ğµ in the first turn, indicating that it learns to index into the
pre-trained knowledge of the model in a different manner as opposed to simply translating the pass@K
performance into the pass@1 performance of the model, that majority of single-turn approaches are
believed to be doing.
8RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure 6: Fractionofproblemsunsolvedbypass@Batthefirstturnthatsequential5-turnsamplingfromRISEsolves,
whereğµ =5Ã—ğ‘˜(ğ‘˜isthex-axis).RISEcansolveseveralchallengingproblemsthatsamplingatthefirstturnwithmuchlarger
budgetscannotsolve.
6. Experimental Evaluation
The goal of our experiments is to demonstrate the efficacy of RISE in instilling language models with the
ability to self-improve their responses over turns. Our experiments answer the following questions: (1)
How effectively can RISE improve performance over multiple sequential attempts (i.e., turns) at a given
prompt?; (2) Does the performance of RISE improve with more rounds of iterative training?; (3) Does
the self-improvement strategy induced by RISE generalize to novel problems that are out of the training
domain? and finally; (4) What is the best data composition for training RISE? To this end, we compare
RISE to other prior and baseline approaches, and perform ablations on GSM8K [11] and MATH [18].
Baselines, comparisons, and evaluation. We compare RISE to several prior methods that attempt
to induce similar self-improvement capabilities: (a) self-refine [21, 31] that prompts a base model to
critique and revise its mistakes; (b) GloRE [17], which trains a separate reward model to locate errors
and a refinement model to improve responses of a base LLM; and(c) self-consistency [51], which runs
majority voting on multiple responses from the first turn as a baseline to compare to our sequential
strategy. WetriedtoconstructfaircomparisonsbetweenRISEandthesemethodsbyusingasimilar-sized
model [23, 58], but differences in the base model, training data, and evaluation setups still prohibits
us from performing an apples-to-apples comparison in some cases. Nonetheless, we can still hope to
understand the ballpark of improvement by contextualizing our results with these prior works. We also
compare to V-STaR [19], but since this is not an fair comparison, we defer it to Appendix B.
We evaluate RISE in both modes at inference time: with and without an oracle (Section 4.3) at the end
of five turns. Concretely, these metrics are defined as follows:
â€¢ with oracle, â€œp1@t5â€: this run terminates the rollout as soon as the response is correct. In other
words, this metric allows queries to the final answer verifier at the end of each turn.
â€¢ withoutoracle,â€œm1@t5â€: thisrundoesnotterminatetherolloutbeforefiveturns,andwecompute
the maj@1 performance on the candidates produced in each turn as detailed in Section 4.3.
We also compare maj@K performance at the first turn for all the models we train (â€œm1@t1â€, â€œm5@t1â€).
9RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
GSM8K[10] MATH[18]
Approach w/ooracle w/oracle w/ooracle w/oracle
m1@t1 â†’m5@t1 â†’m1@t5 p1@t5 m1@t1 â†’m5@t1 â†’m1@t5 p1@t5
RISE(Ours)
Llama2Base 10.5 22.8(+12.3) 11.1(+0.6) 13.9(+3.4) 1.9 5.1(+3.2) 1.4(-0.5) 2.3(+0.4)
+Boost 32.9 45.4(+12.5) 39.2(+6.3) 55.5(+22.6) 5.5 6.8(+1.3) 5.5(0.0) 14.6(+9.1)
+Iteration1 35.6 49.7(+14.1) 50.7(+15.1) 63.9(+28.3) 6.3 8.8(+2.5) 9.7(+3.4) 19.4(+13.1)
+Iteration2 37.3 51.0(+13.7) 55.0(+17.7) 68.4(+31.1) 5.8 10.4(+4.6) 10.4(+4.6) 19.8(+14.0)
SFTonoracledata
Onlycorrectdata 27.4 42.2(+14.9) 34.0(+6.6) 43.6(+16.2) 5.8 7.9(+2.1) 5.5(-0.3) 12.1(+6.2)
Correctandincorrect 25.7 41.8(+16.1) 31.2(+5.5) 41.5(+15.8) 5.0 5.2(+0.2) 5.0(+0.0) 13.1(+8.1)
RISE(Ours)
Mistral-7B 33.7 49.4(+15.7) 39.0(+5.3) 46.9(+13.2) 7.5 13.0(+5.5) 8.4(+0.9) 13.0(+5.5)
+Iteration1 35.3 50.6(+15.3) 59.2(+23.9) 68.6(+33.3) 6.7 9.5(+2.8) 18.4(+11.1) 29.7(+22.4)
7BSoTA[58]
Eurus-7B-SFT 36.3 66.3(+30.0) 47.9(+11.6) 53.1(+16.8) 12.3 19.8(+7.5) 16.3(+4.0) 22.9(+10.6)
Self-Refine[31] â†’m1@t3 â†’p1@t3 â†’m1@t3 â†’p1@t3
Base 10.5 22.4(+11.9) 7.1(-3.4) 13.0(+2.5) 1.9 5.1(+3.2) 1.9(0.0) 3.1(+1.2)
+Iteration2 37.3 50.5(+13.2) 33.3(-4.0) 44.5(+7.2) 5.8 9.4(+3.6) 5.7(-0.1) 9.5(+3.7)
GPT-3.5 66.4 80.2(+13.8) 61.0(-5.4) 71.6(+5.2) 39.7 46.5(+6.8) 36.5(-3.2) 46.7(+7.0)
Mistral-7B 33.7 48.5(+14.8) 21.2(-12.5) 37.9(+4.2) 7.5 12.3(+4.8) 7.1(-0.4) 11.4(+3.9)
Eurus-7B-SFT 36.3 65.9(+29.6) 26.2(-10.1) 42.8(+6.5) 12.3 19.4(+7.1) 9.0(-3.3) 15.1(+2.8)
GloRE[17] â†’m1@t3 â†’p1@t3
+ORM 48.2 49.5(+1.3) 57.1(+8.9)
+SORM 48.2 51.6(+3.4) 59.7(+11.5) Notstudiedin[17]
+Direct 48.2 47.4(-0.8) 59.2(+11.0)
Table 1: RISEvs. otherapproaches(Self-Refine,GLoRE)andbaselines.ObservethatRISEattainsthebiggestperformance
improvement(inbrown)between1-turn(m5@t1)and5-turn(m1@t5)performancew/oanoracleonbothGSM8KandMATH.
Thisperformancegapisevenlargerwhenoracleearlyterminationisallowed(p1@t5w/oracle).Self-Refine[31]degrades
performanceacrosstheboardwhenusedwithoutanoracle,andattainsminorperformanceimprovementswhenusedwithan
oracle.GLoREtrainsaseparaterefinementmodel,butstillperformsworsethanRISE;moredetailsaboutitareinAppendixB.
UsingRISEontopofabetterbasemodel(Mistral-7B)isalsoeffective(positiveimprovementswithmultipleturns),andnote
them1@t5performanceofMistral-7Bexceedsevenstate-of-the-artmathmodelssuchasEurus-7B-SFT[58].Simplyrunning
single-turnSFTondatautilizedbyRISEisnoteffectiveatinducingaself-improvementcapability,implyingthatthealgorithmic
designchoicesinRISEarecrucialforperformance.Colorcodingindicatesnumbersthatcanbecomparedtoeachother.
6.1. Does RISE improve performance over multiple turns compared to other approaches?
Main results. We present the comparisons in Table 1. First, note that RISE (â€œIteration 1â€ and â€œIteration
2â€) boosts up the LLama2 base modelâ€™s five-turn performance by 15.1% and 17.7% respectively with
each iteration on GSM8K and 3.4% and 4.6% on MATH, w/o any oracle. Interestingly, we found using
prompting-only self-refine [31] largely degrades performance across the board, even with a strong
proprietary model, GPT-3.5. The strongest 7B base models, Mistral-7B and Eurus-7B-SFT [58], when
coupled with standard prompting, are only able to improve their performance, but only by 5.3% / 11.6%
and 0.9% / 4.0% respectively on GSM8K and MATH, which is significantly lower than our approach.
The performance of GLoRE improves only by 3.4% on GSM8K (over two turns), but this is still lower
than our approach, which improves by 6.3% in two turns and 13.4% in three turns (see Appendix B.1).
This indicates that RISE is effective in teaching models how to improve their own errors. To summarize,
training with RISE gives the largest performance improvement gains compared to other approaches both
with and without the use of an oracle, and these gains are transferred to other base models.
One might also hypothesize that the performance gains with RISE here are largely a result of utilizing
10RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
w/ooracle w/oracle
RISE(Self)
m1@t1 â†’m5@t1 â†’m1@t5 p1@t5
Mistral-7B 33.7 49.4(+15.7) 39.0(+5.3) 46.9(+13.2)
+Iteration1 36.8 44.4(+7.6) 39.5(+6.6) 48.7(+15.9)
Llama-3-8B 45.3 69.7(+24.4) 52.5(+7.2) 61.0(+15.7)
+Iteration1 65.6 80.7(+15.1) 73.8(+8.2) 81.2(+15.6)
Table2: RISEwithself-distillationonGSM8K.RISEisabletoimprove5-turnmaj@1performanceofthemodelwithentirely
self-generateddataandsupervision,despitethefactthatthebaseMistral-7Bmodeldoesnotproducecorrectanswersfor
severalproblems.
queriestoanoff-the-shelfmorecapablemodelforprovidingsupervisionandnotthealgorithmicapproach
fordatacollectionandtraining. Toaddressthishypothesis,westoreallthedatageneratedbyRISEfrom
morecapablemodelsandtrainonthisdataviastandardsingle-turnSFT(â€œSFTonoracledata). Sincenot
allofthisdataareguaranteedtobecorrect,wealsorunthisexperimentononlythecorrectresponsesin
theseoracledata. ObserveinTable1thatthisproceduredoesnotstillinstillself-improvementcapabilities,
largelypreservingordegradingsequential(â€œmaj@1@turn5â€)performancecomparedtosimplysampling
one response in the first turn. This means that the algorithmic design of RISE is critical in enabling it to
learn self-improvement capabilities, as opposed to simply the use of expert supervision.
6.1.1. Can RISE Effectively Make Use of Mistakes and Correct Them?
One concern that arises from prior results on self-refinement or self-correction is whether the model
can truly correct itself over turns or whether the improvement comes from the effect of sampling more
answers and picking the best one. In Table 1, we see that sequentially improving responses via RISE
(â€œmaj@1@turn5â€)outperformssampling5responsesinparallelatthefirstturnandapplyingamajority
vote on them (â€œmaj@5@turn1â€). Please note that this comparison utilizes an equal number of samples,
with the only difference being that these samples are drawn in parallel at the first turn in one case and
sequentially at the end of five turns in the other. Comparing maj@5 performance at the end of 1 turn
and 5 turns, we observe a consistent 4% to 8% improvement on GSM8K and an 6.5% improvement
on MATH (with Mistral-7B model). This means that RISE can imbue models with a self-improvement
ability, while running parallel sampling alone on any model cannot endow the same ability. Even the
maj@5@turn1 performance of standard single-turn SFT on the data used by RISE is substantially worse
than the sequential maj@1@turn5 performance of RISE, implying that the algorithmic protocol of RISE
plays a critical underlying role. Finally, we also remark that in Figure 6, we showed that the sequential
procedurelearnedbyRISEoverfiveturnscouldsolveasignificantfractionofproblemsthatwereunsolved
by pass@B for much larger values of ğµ â‰« 5 in the first turn, implying that sequential RISE can actually
tackle prompts that were not solvable by simply sampling more responses in the first turn.
One might also speculate if these improvements in sequential improvement ability largely come at a cost
of reduced improvements in first turn performance. In addition, we also observe that running multiple
iterations of RISE still preserves the first turn performance while improving the 5-turn performance.
6.1.2. How Does the Base Model Affect RISE?
The performance of RISE with Llama2-7B on an absolute scale is lower than the best models specifically
fine-tuned on math data (e.g., Eurus-7B-SFT or Mistral-7B). However, we find that RISE is still effective
on top of Mistral-7B base model. In fact, our performance at the end of five turns outperforms one of the
best 7B SFT models, customized to math reasoning. Compare the m1@t5 performance of Eurus-7B-SFT
and Mistral-7B in RISE (ours), to find that Mistral-7B + RISE outperforms Eurus-7B-SFT.
11RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
w/ooracle w/oracle
RISE
m1@t1 â†’m1@t5 p1@t5
GSM8K
Llama2Base 10.5 11.1(+0.6) 13.9(+3.4)
Iteration1RISEModeltrainedonMATH 19.3 32.6(+13.3) 48.4(+29.1)
MATH
Llama2Base 1.9 1.4(-0.5) 2.3(+0.4)
Iteration1RISEModeltrainedonGSM8K 4.3 4.4(+0.1) 12.1(+7.8)
SVAMP
Llama2Base 29.2 30.5(+1.3) 34.0(+4.8)
Iteration1RISEModeltrainedonMATH 30.1 31.4(+1.2) 45.9(+15.8)
Iteration1RISEModeltrainedonGSM8K 42.2 50.0(+7.8) 63.6(+21.4)
Table 3: Out-of-distributiongeneralizationofRISE.Weevaluatemodelfine-tunedonMATHontheGSM8Ktestset;model
fine-tunedGSM8KonMATH;andthemodelfine-tunedonamixtureofGSM8KandMATHontheSVAMPdata.Observeeven
thoughwetrainonOODprompts,RISEcanstillimprovesequentialperformance.
6.1.3. Self-Distillation Version of RISE
WealsocomparetheperformanceofRISEwithentirelyself-generateddataandsupervision(Equation4.4,
ğ‘ = 16) after one iteration directly on top of more capable models: Mistral-7B and Llama-3-8B on
GSM8K in Table 2, without any knowledge boosting phase. We find that this variant also improves the
5-turnperformanceofthebasemodelcomparedtothefirstturn: compareâ€œm1@t5â€vsâ€œm1@t1â€forboth
the models Llama-3-8B and Mistral-7B, where RISE boosts the sequential self-improvement performance
by more than 1% compared to turn 1 performance w/o any oracle.
Of course, we also note that this version of RISE does not outperform the â€œm5@t1â€ performance of the
fine-tuned model. We expect this to be largely a function of one single iteration of training. Since the
self-distillationversionofRISEutilizesbest-of-Nsamplingagainstthesamemodeltoproducesupervision
forself-improvement,RISEwouldfirsthavetomatchtheperformanceofbest-of-Nsamplingbeforeitcan
start to improve over it via reward maximization. Due to the significant gap between the base modelâ€™s
m5@t1 and m1@t5 performance, we expect that this will take quite a few iterations or a fully online
RL algorithm. We did not have computational resources and infrastructure to run multiple iterations,
but this is an interesting avenue for future work. In this self-distillation setting, we could also divide the
computation between sequential and parallel sampling strategies to get the best results at the end of five
turns. Nonetheless, this result shows that even by training on self-generated samples, RISE can actually
amplify the sequential sampling performance of the base model.
6.2. Does the Performance of RISE Improve with Iterative Training?
Next, we attempt to understand if RISE improves with multiple rounds of training on on-policy data. As
shown in Tables 1 and 2, the performance of RISE improves from iteration to iteration constantly. The
5-turn performance of RISE, both with and without an oracle, exhibits a clear improvement with more
rounds. Thisimpliesthatiterativeself-trainingproceduresoftheformofSTaR[61]canalsobecombined
with RISE to train models for self-improvement. This also perhaps serves as a strong hint towards the
potential utility of full online reinforcement learning (RL) techniques.
6.3. Does RISE Also Improve Sequential Performance on Out-of-Distribution Prompts?
InTable3,ouraimistoevaluatetherobustnessofthestrategyinducedbyRISEonnew,unseenprompts.
Specifically,wecomparetheperformanceoftheRISEmodeltrainedwithadatasetonevaluationprompts
12RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
from another dataset. Note in Table 3, these datasets include MATH, GSM8K, and SVAMP. Generally,
we observe that the model trained on one dataset is still able to improve the base modelâ€™s performance
on another dataset over the course of sequential five turns. More concretely, while the base Llama2
model largely degrades its turn 1 performance over turn 5 performance, modelâ€™s trained with RISE
enable a positive performance improvement on these out-of-distribution prompts. This means that even
though these models have not seen queries similar to the evaluation dataset, simply training with RISE
on some kind of mathematical prompts still boosts the efficacy of the self-improvement strategy on a new
distribution of test prompts. This finding suggests that RISE is capable of instilling self-improvement
procedures that can generalize beyond the distribution of prompts in the fine-tuning data.
6.4. What Data Compositions and Data Quantity are Crucial for RISE?
WenowstudyhowdifferentdatacompositionsaffecttheperformanceofRISEwiththegoalofanswering
questions such as should we collect on-policy error correction data like DAgger [36] or should we bias
towards high-quality off-policy data?. To understand the utility of different data compositions, we enlist
thethreeaspectsRISE:(a)theuseofmulti-turnrolloutdataforfine-tuning,(b)theuseofunsuccessful/
suboptimal rollouts via weighted supervised fine-tuning compared to naÃ¯ve supervised learning, which
only utilizes successful rollouts for fine-tuning; and (c) the use of on-policy rollouts and self-generated
or oracle data. We will now perform controlled experiments to understand the effect of each of these
factors on the overall performance of RISE.
Figure 7: Left:Theimportanceofmulti-turninteractionhistoryandweightedobjectivesfortrainingRISE.Notethat
trainingwithmulti-turndataleadstobetterself-improvementperformanceattheendof5turns,thanone-turndataobtained
fromtheoriginaldatasetwithoracleanswersfromanothermodel;alsoobservethatusingaweightedobjectiveperformsbetter.
Right:Theimportanceofusingallrolloutsforlearning,insteadofonlysuccessfulrolloutsoronlysuccessfulresponsesinthe
data.Usingalldataperformsbestinourresults.
(a) Data composition for fine-tuning. We first study the necessity of using the interaction of error
correction history for training RISE in Figure 7 (Left). We compare two approaches: model trained with
oracleanswersshownrightafterthequery(â€œ1-turnâ€)andoracleanswersshownafterintermediatefailed
attempts (â€œMulti-turnâ€) in Figure 7 (Left). Even though the latter trains on intermediate responses that
may not always be correct, it attains a higher performance than simply training on the correct response
for a given prompt. This highlights the importance of training on contexts that include a multi-turn
interaction history depicting mistakes from the learner to improve self-improvement capabilities.
(b) Weighted supervised learning vs unweighted supervised learning. Next, we investigate the effect
ofreward-weightedRLonmulti-turndatainRISEasopposedtosimplyimitatingfilteredsuccessfuldata.
13RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
We find that using all the data leads to improved performance over simply filtering good datain Figure 7
(Right), which reduces sample size. In Figure 7 (Left), we find that reward-weighted training improves
performance on later turns, allowing us to better leverage all the sub-optimal data.
(c) On-policy vs off-policy data; self-generated vs. expertdata. RISErunson-policyrolloutsandseeks
improvementsonresponsesthatthelearnerproduces. AsshowninFigure8(Left),aâ€œDAgger[36]â€-style
approach that seeks improvements on responses appearing in on-policy rollouts improves performance
(green/orange) compared to using expert data alone (blue/pink). Conceptually, this addresses the
train-test mismatch between the distribution of context tokens, enabling imitation learning methods to
now target the correct distribution. In addition, recent work [24] has shown that LLMs often memorize
â€œunfamiliarâ€examplesgeneratedbyoraclemodels;bytrainingonon-policyrollouts,weshouldbeableto
eliminate any such potential issues. Thus, while the model trained via offline imitation is able to reduce
loss, these improvements do not generalize to new problems.
Figure 8: Left: Theimportanceofdatasourcesusedfortraining.Westudytheperformanceoftheiteration1ofRISEon
GSM8Kwithdifferentdatasources.â€œExpertâ€referstotheuseofanoraclemodel,â€œOn-policyâ€correspondstosamplingfromthe
learner,andâ€œBest-of-Nâ€meansusingthebestsampleoutofğ‘ fromthelearner(hereğ‘ =16).Right:ComparingRISEwith
oracleerrorfeedback(pass@1@turnk;solidlines)toparallelsamplingof5responsesatturn1(pass@k@turn1;
dashedlines)overnumberofturnsğ‘˜onthex-axisonGSM8K.ObservethatsequentialsamplingwithIteration1andIteration
2RISEmodelsconsistentlyoutperformsparallelsamplingforallvaluesofturnğ‘˜;andthegapgrowsasthenumberofiterations
increases.Incontrast,thistrendisabsentforbaseandSFTmodels.
6.5. Pass@K vs Sequential Sampling via RISE
We now study the performance of sequential sampling with oracle feedback in GSM8K, unlike relying on
majorityvotingasinTable1. Specifically,wecomparetheperformanceofRISEwithearlyterminationof
evaluation rollouts against pass@5 (not maj@5) performance of the RISE model at the first turn (which
makes an equal number of queries to the ground-truth correctness indicator). Access to ground-truth
correctness indicator is expected to improve performance for both parallel and sequential sampling
unsurprisingly, but we see in Figure 8 (Right) that RISE is able to improve performance more beyond
simplysamplingmoresamplesatthefirstturnandcomputingpass@K,despitethisstrongassumptionof
access to an oracle final answer verifier made by the parallel sampling approach.
We would expect parallel sampling via pass@K to be most performant when provided access to oracle
answer checking as the model can choose to simply sample ğ¾ independent responses, if the base model
accuracy on this task is reasonable. Pass@K @ turn 1 also upper bounds the first turn accuracy of any
procedure that does not query the oracle (e.g., with verifiers, with majority voting, etc.). Hence, access
to oracle answer checking for each individual response presents the strongest result one could expect out
14RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
of parallel sampling, in one turn. On the other hand, sequential sampling produces correlated samples
and hence should, in principle, not be able to improve over parallel sampling, unless the model is unable
to use the additional tokens and computation provided by the feedback self-improvement prompt to
meaningfully correct itself. Since the sequential performance of the model is larger than the parallel
performance above, this means that RISE indeed does this successfully.
6.6. Error Analysis of RISE over Turns
Following the protocol of Huang et al. [21], in this section, we perform an error analysis of the improve-
ment performed by RISE (without any oracle feedback) to understand how the fraction of incorrect and
correct responses changes over turns, when no oracle is used for early termination. We demonstrate this
in the form of Venn diagrams in Figure 9. First note that there is a consistent increase in the portion
of problems that stay correct and a consistent decrease in the portion of problems that stay incorrect,
which means that the model is able to answer more and more problems as we increase the number
of turns. Second, there is a consistent decrease in the number of problems that change from being
correct to incorrect, which is often also not the case for strong proprietary LLMs such as GPT in Huang
et al. [21]. We also note that there is a decrease in the total number of incorrect problems that become
correct in the subsequent turn, but this is a direct consequence of a shrinkage in the size of the incorrect
response set as more problems become correct over turns. This indicates that one can induce â€œintrinsicâ€
self-improvement (per the terminology of Huang et al. [21]) via fine-tuning with RISE, even though no
external environment input is provided during evaluation.
Figure 9: Change in the fraction of responses that transition their correctness values over the course of multi-turn
rolloutsfromRISE,w/ooracle.Observethatingeneral,thefractionofCorrectâ†’Correctresponsesincreases;Incorrectâ†’
Incorrectresponsesdecreases;andthefractionofCorrectâ†’Incorrectresponsesalsodecreases,indicatingthatRISE(w/oany
oracle)isabletoiterativelyimproveitsresponses.
Qualitative examples. We also inspect several examples from the GSM8K test set to qualitatively
understand the behavior of RISE over turns and observe different behavior patterns, that we show in
Appendix B.2. For instance, the trained model may choose to completely rewrite its previous response if
it is totally incorrect in order to get to the correct answer or make small edits if the previous response is
mostly correct. Another interesting pattern we note is that the model implicitly has the ability to locate
errors in previous responses and only refine the erroneous steps. Additionally, the model is tolerant of
noisy environmental feedback when there is no oracle-assisted early termination.
7. Discussion, Future Directions, and Limitations
We presented RISE, an approach for fine-tuning LLMs to be able to improve their own responses over
multiple turns sequentially. RISE prescribes an iterative RL recipe on top of on-policy rollout data, with
15RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
expert or self-generated supervision to steer self-improvement. RISE significantly improves the self-
improvement abilities of 7B models on reasoning tasks (GSM8K and MATH), attaining an improvement
over turns that previous work [21] has not observed in strong proprietary models. In addition, RISE
outperformspriorapproachesthatattempttotacklesimilarproblemsofrefinementandcorrection,while
being simpler in that it does not require running multiple models and works well with just one model.
Despite these good results, there are still many open questions and limitations. Due to computational
constraints, we were not able to perform more than two iterations of training with RISE, and no more
than one iteration when the supervision comes from the learner itself. Improving with self-generated
supervision will likely require more computation and more iterations, since it will be slower than when
using an off-the-shelf expert model. RISE requires running manual iterations and hence, a more â€œonlineâ€
variant of RISE is likely the solution in the long run, especially when we wish to scale on-policy learning
in a data-efficient manner. Additionally, while our work fine-tunes models on one task at a time, it will
be certainly interesting to include data from the protocols specified by RISE into general instruction
tuningandpost-trainingpipelines. Giventheresultsthatfine-tuningondataprescribedbyRISEdoesnot
hurt the first-turn performance of any model we trained, we hypothesize that adding this sort of data in
generalinstruction-tuningpipelinesshouldnothurteither,whileenablingthesequentialself-improvement
capability that is largely absent from models today.
Acknowledgements
This work was done at CMU. We thank Fahim Tajwar, Abitha Thankaraj, Amrith Setlur, and Charlie Snell
fortheirfeedbackandinformativediscussions. ThisworkwassupportedbyONRunderN000142412206,
OpenAI superalignment fast grants, and used the Delta system and JetStream2 [16] at the National
Center for Supercomputing Applications through CIS240249 and CIS230278, supported by the National
Science Foundation. We thank OpenAI for providing GPT-4 credits for academic use.
References
[1] RishabhAgarwal,NinoVieillard,PiotrStanczyk,SabelaRamos,MatthieuGeist,andOlivierBachem.
Gkd: Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint
arXiv:2306.13649, 2023.
[2] YuntaoBai,SauravKadavath,SandipanKundu,AmandaAskell,JacksonKernion,AndyJones,Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness
from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
[3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner,
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-
strong generalization: Eliciting strong capabilities with weak supervision, 2023. URL https:
//arxiv.org/abs/2312.09390.
[4] Jonathan D Chang, Wenhao Shan, Owen Oertell, KiantÃ© Brantley, Dipendra Misra, Jason D Lee,
and Wen Sun. Dataset reset policy optimization for rlhf. arXiv preprint arXiv:2404.08495, 2024.
[5] Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, and
LucasCCordeiro. Anewerainsoftwaresecurity: Towardsself-healingsoftwarevialargelanguage
models and formal verification. arXiv preprint arXiv:2305.14752, 2023.
16RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
[6] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.
Fireact: Toward language agent fine-tuning, 2023.
[7] Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128, 2023.
[8] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning
convertsweaklanguagemodelstostronglanguagemodels. arXivpreprintarXiv:2401.01335,2024.
[9] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive
chain-of-thought prompting. arXiv preprint arXiv:2311.09277, 2023.
[10] KarlCobbe,ChristopherHesse,JacobHilton,andJohnSchulman. Leveragingproceduralgeneration
to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.
[11] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[12] YilunDu,ShuangLi,AntonioTorralba,JoshuaBTenenbaum,andIgorMordatch. Improvingfactual-
ityandreasoninginlanguagemodelsthroughmultiagentdebate. arXivpreprintarXiv:2305.14325,
2023.
[13] KanishkGandhi,DeniseLee,GabrielGrand,MuxinLiu,WinsonCheng,ArchitSharma,andNoahD
Goodman. Streamofsearch(sos): Learningtosearchinlanguage. arXivpreprintarXiv:2404.03683,
2024.
[14] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. Pal: Program-aided language models. In International Conference on Machine
Learning, pages 10764â€“10799. PMLR, 2023.
[15] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
Critic: Large Language Models can Self-Correct with Tool-Interactive Critiquing. arXiv preprint
arXiv:2305.11738, 2023.
[16] David Y. Hancock, Jeremy Fischer, John Michael Lowe, Winona Snapp-Childs, Marlon Pierce,
Suresh Marru, J. Eric Coulter, Matthew Vaughn, Brian Beck, Nirav Merchant, Edwin Skidmore,
and Gwen Jacobs. Jetstream2: Accelerating cloud computing via jetstream. In Practice and
Experience in Advanced Research Computing, PEARC â€™21, New York, NY, USA, 2021. Association
for Computing Machinery. ISBN 9781450382922. doi: 10.1145/3437359.3465565. URL https:
//doi.org/10.1145/3437359.3465565.
[17] Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi,
Eric Hambro, and Roberta Railneau. Glore: When, where, and how to improve llm reasoning via
global and local refinements. arXiv preprint arXiv:2402.10963, 2024.
[18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS,
2021.
17RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
[19] Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh
Agarwal. V-star: Trainingverifiersforself-taughtreasoners. arXivpreprintarXiv:2402.06457,2024.
[20] Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-
based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010,
2023.
[21] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,
and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint
arXiv:2310.01798, 2023.
[22] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. In International Conference on
Machine Learning, pages 9118â€“9147. PMLR, 2022.
[23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,
Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.
Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[24] Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, and Sergey Levine. Unfamiliar finetuning
examples control how language models hallucinate, 2024.
[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https://
arxiv.org/abs/1312.6114.
[26] Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Be-
yond a*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint
arXiv:2402.14083, 2024.
[27] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and
HouwenPeng. Common7blanguagemodelsalreadypossessstrongmathcapabilities. arXivpreprint
arXiv:2403.04706, 2024.
[28] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Letâ€™s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
[29] XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,HangliangDing,Kaiwen
Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688,
2023.
[30] HaipengLuo,QingfengSun,CanXu,PuZhao,JianguangLou,ChongyangTao,XiuboGeng,Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
[31] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with
self-feedback. arXiv preprint arXiv:2303.17651, 2023.
18RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and
Caiming Xiong. CodeGen: An Open Large Language Model for Code with Multi-Turn Program
Synthesis. ICLR, 2023.
[33] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpadsforintermediatecomputationwithlanguagemodels. arXivpreprintarXiv:2112.00114,
2021.
[34] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
[35] JanPetersandStefanSchaal.Reinforcementlearningbyreward-weightedregressionforoperational
spacecontrol.InProceedingsofthe24thinternationalconferenceonMachinelearning,pages745â€“750.
ACM, 2007.
[36] StephaneRoss,GeoffreyGordon,andDrewBagnell.Areductionofimitationlearningandstructured
prediction to no-regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav DudÃ­k,
editors, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,
volume 15 of Proceedings of Machine Learning Research, pages 627â€“635, Fort Lauderdale, FL, USA,
11â€“13 Apr 2011. PMLR. URL http://proceedings.mlr.press/v15/ross11a.html.
[37] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and
Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general
preferences. arXiv preprint arXiv:2404.03715, 2024.
[38] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.
Branch-solve-merge improves large language model evaluation and generation. arXiv preprint
arXiv:2310.15123, 2023.
[39] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761, 2023.
[40] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic
memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
[41] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural
language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.
[42] JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.Deepunsupervised
learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/1503.
03585.
[43] Yang Song and Diederik P. Kingma. How to train your energy-based models, 2021. URL https:
//arxiv.org/abs/2101.03288.
[44] Liting Sun, Cheng Peng, Wei Zhan, and Masayoshi Tomizuka. A fast integrated planning and
control framework for autonomous driving via imitation learning. In Dynamic Systems and Control
Conference, volume 51913, page V003T37A012. American Society of Mechanical Engineers, 2018.
19RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
[45] Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, and Zhiwei Steven Wu. Inverse reinforce-
ment learning without reinforcement learning, 2024. URL https://arxiv.org/abs/2303.
14623.
[46] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.
Openmathinstruct-1: A1.8millionmathinstructiontuningdataset.arXivpreprintarXiv:2402.10176,
2024.
[47] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.
[48] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.
Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv
preprint arXiv:2212.10001, 2022.
[49] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and
Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv
preprint arXiv: Arxiv-2305.16291, 2023.
[50] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Y Wu, and Zhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. CoRR,
abs/2312.08935, 2023.
[51] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv
preprint arXiv:2203.11171, 2022.
[52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS,
2022.
[53] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on
Learning Representations, 2023. URL https://openreview.net/forum?id=hH36JeQZDaO.
[54] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and
additional opinions. arXiv preprint arXiv:2306.02224, 2023.
[55] Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,
Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem Proving with Retrieval-Augmented
Language Models. arXiv preprint arXiv:2306.15626, 2023.
[56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,
2022.
[57] LonghuiYu,WeisenJiang,HanShi,JinchengYu,ZhengyingLiu,YuZhang,JamesTKwok,Zhenguo
Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for
large language models. arXiv preprint arXiv:2309.12284, 2023.
20RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
[58] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin
Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees.
arXiv preprint arXiv:2404.02078, 2024.
[59] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason
Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024.
[60] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint
arXiv:2309.05653, 2023.
[61] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
reasoning. Advances in Neural Information Processing Systems, 35:15476â€“15488, 2022.
[62] AohanZeng,MingdaoLiu,RuiLu,BowenWang,XiaoLiu,YuxiaoDong,andJieTang. Agenttuning:
Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.
[63] Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:
Test-time prompting via reinforcement learning. arXiv preprint arXiv:2211.11890, 2022.
[64] Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket
Tandon,andUriAlon. In-contextprinciplelearningfrommistakes. arXivpreprintarXiv:2402.05403,
2024.
[65] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Lan-
guage agent tree search unifies reasoning acting and planning in language models. arXiv preprint
arXiv:2310.04406, 2023.
[66] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language
model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.
21RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Appendices
A. Additional Ablations on Data Composition and Weak-to-Strong Generalization
A.1. Inclusion of Correct-to-Correct Data
Intuitively, self-improvement over turns is largely only possible when the model can learn to verify the
correctness of its previous response and decide to appropriately modify its response toward correctness.
Thus far, the RISE has only trained on data that showed how to convert incorrect responses to correct
responses but never illustrated how the model could act on correct responses. To understand if perfor-
mance can be boosted by also illustrating examples of how the model could act on correct responses, we
ran a number of ablations. We took the RISE data generated during Iteration 1 of training on GSM8K
with Llama2-7B and modified the multi-turn rollouts to create several cases. First, we duplicated the
correctresponseappearingattheendofeverysuccessfulmulti-turnrolloutandtrainedforoneextraturn.
This should teach the model that correct responses should not be modified, unlike incorrect responses
appearing in previous turns in the rollout. Second, we also ran a variant in which the correct response
appearing at the end of every successful rollout is followed by a different correct response. This variant
should teach the model that if it chooses to modify a correct response, it must still produce another
correct response.
As shown in Table 4, all methods improved performance over the base model, though only appending
with a successful rollout with a novel correct response leads to best performance. The default design of
RISE in the main paper attains a close second position, and repeating a correct response at the end of a
successful rollout largely reduces performance. We suspect that the poor performance of repeating the
same correct response is largely a result of inducing spurious correlations due to data duplication.
w/ooracle w/oracle
RISE(Llama2)
m1@t1 â†’m5@t1 â†’m1@t5 p1@t5
Boost 32.9 45.3(+12.4) 26.5(-6.4) 40.9(+8.0)
+RISE(default) 35.6 49.7(+14.1) 50.7(+15.1) 63.9(+28.3)
+Repeatingacorrectresponse 34.2 48.9(+14.6) 46.2(+12.6) 57.7(+23.5)
+Appendingadifferentcorrectresponse 33.1 49.3(+16.2) 51.1(+18.0) 64.9(+31.8)
Table 4:
ComparisonofmodelperformanceonGSM8Kwithdifferentmechanismsofaddingcorrect-to-correctdatain
RISE.Valuesinparenthesesindicateimprovementoverm1@t1,notethatappendingasuccessfulrolloutwithaanovelcorrect
responseleadstothehighestperformancegains.
To further investigate self-improvement capabilities, we analyzed the percentage of correct responses
changingtoincorrectresponsesinconsecutiveturns(Tğ‘–toTğ‘–+1),asillustratedinFigure10. Generally,a
decreasing trend suggests better self-improvement, while lower absolute values indicate better resistance
to noisy feedback. The results reveal unexpected patterns across configurations. The Boost configuration
shows the poorest performance, with the highest overall percentages and an increase from turn 4 to
5, suggesting that it struggles to consistently maintain correct responses. Repeating a correct response
shows the lowest initial percentage (6.3%) but increases from turn 3 onward, indicating potential issues
in extended interactions. Both Default RISE and appending a different correct response demonstrate a
favorabletrend,steadilydecreasingfrom12.3%to3.9%andfrom9.8%to3.3%,respectively,suggesting
a good balance between maintaining correct responses and allowing improvements. These findings
provide nuanced insights into the stability and self-improvement capabilities of RISE and align with our
earlier observation of its superior performance in overall accuracy.
22RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Figure 10: PercentageofcorrectresponsesinturnT thatchangetobeingincorrectinturnT .Thisfigureillustrates
ğ‘– ğ‘–+1
thepercentageofcorrectresponsesthatchangetoincorrectresponsesacrossconsecutiveturns(T toT )fordifferentmodel
ğ‘– ğ‘–+1
configurations.Acontinuouslydecreasingtrendsuggestsbetterself-improvementperformance.
A.2. Weak-to-Strong Generalization: RISE on Weak Model Data Improves Strong Models
In this section, we compare the performance of Llama2 and Mistral-7B with RISE in the weak-to-strong
setting[3]. Concretely,weareinterestedinusingdatageneratedviaRISEwithaweakmodel(Llama2-7B)
to train a strong model (Mistral-7B). Our analysis reveals intriguing insights into the transferability of
RISE-generated data across models of different capabilities.
w/ooracle w/oracle
RISE
m1@t1 â†’m5@t1 â†’m1@t5 p1@t5
Llama2-7B 10.5 22.8(+12.3) 11.1(+0.6) 13.9(+3.4)
+Iteration1 35.6 49.7(+14.1) 50.7(+15.1) 63.9(+28.3)
+Iteration1(Mistral-7B) 27.1 40.1(+13.0) 45.2(+18.1) 59.1(+32.0)
Mistral-7B 33.7 49.4(+15.7) 39.0(+5.3) 46.9(+13.2)
+Iteration1 35.3 50.6(+15.3) 59.2(+23.9) 68.6(+33.3)
+Iteration1(Llama2-7B) 38.2 55.4(+17.2) 62.7(+24.5) 73.5(+35.3)
Table 5: Weak-to-stronggeneralizationonGSM8K.ComparingperformanceofRISEwhentrainingonrolloutsgenerated
byLlama2-7BvsMistral-7B.NotethattrainingtheMistral-7BmodelonrolloutsgeneratedbytheweakerLlama2-7Bwith
RISEimprovesperformancecomparedtousingdatageneratedbytheMistral-7Bmodelitself.However,thereverseisnottrue:
trainingtheLlama2modelonMistralâ€™smistakesleadstoworseperformance,likelybecauseerrorsfromtheMistral-7Bmodel
arehardertocomprehendforaworsebasemodel.Allvaluesarein%accuracy,andvaluesinparenthesesindicateimprovement
overm1@t1.
As shown in Table 5, we find that Mistral-7B + Iteration 1 data generated from Llama2 outperforms
training the Llama2-7B model itself on these data (i.e., Llama2-7B + Iteration1) on all the metrics
reported with particularly significant improvements in multi-turn reasoning (m1@t5). In fact, training
on multi-turn rollouts from Llama2-7B also outperforms training on on-policy Mistral-7B rollouts as well.
Interestingly,weobservedthattrainingLlama2-7Bonmulti-turnrolloutsfromMistral-7Bperformsworse
than training on on-policy Llama2-7B rollouts, suggesting that Llama2-7B, despite its lower absolute
performance, demonstrates more informative mistakes that can be leveraged to better boost the self-
improvement capability. This phenomenon underscores the importance of the quality and nature of
errors in the training data, rather than just the overall performance of the model that generates them.
ThesefindingscollectivelysuggestthatthedatageneratedfromaweakerLlama2modelcanstillbeused
to induce a self-improvement capability in a stronger model, although the reverse is not true (as is also
evident from the fact that using GPT-3.5 rollouts in the boosting phase for training does not improve
performance for any model in Table 1). We suspect that this is becaue the reverse poses a much harder
23RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
learning problem since a weak model need to internalize the mistakes of a stronger model, resulting in
hallucinations and memorization [24]. Note that training on these data does not degrade single-turn
performance either. This hints at an added benefit of training with RISE: weak-to-strong generalization,
which can be quite useful in practice when rolling out stronger models is expensive.
B. Additional Results
B.1. Complete Comparisons and Discussion: Extended Version of Table 1
We provide an extended version of Table 1, with a clear explanation of how we implement baselines and
a discussion of comparisons.
GSM8K[10] MATH[18]
Approach w/ooracle w/oracle w/ooracle w/oracle
m1@t1 â†’m5@t1 â†’m1@t5 p1@t5 m1@t1 â†’m5@t1 â†’m1@t5 p1@t5
RISE(Ours)
Llama2Base 10.5 22.8(+12.3) 11.1(+0.6) 13.9(+3.4) 1.9 5.1(+3.2) 1.4(-0.5) 2.3(+0.4)
+Boost 32.9 45.4(+12.5) 39.2(+6.3) 55.5(+22.6) 5.5 6.8(+1.3) 5.5(+0.0) 14.6(+9.1)
+Iteration1 35.6 49.7(+14.1) 50.7(+15.1) 63.9(+28.3) 6.3 8.8(+2.5) 9.7(+3.4) 19.4(+13.1)
+Iteration2 37.3 51.0(+13.7) 55.0(+17.7) 68.4(+31.1) 5.8 10.4(+4.6) 10.4(+4.6) 19.8(+14.0)
RISE(Ours)
Mistral-7B 33.7 49.4(+15.7) 39.0(+5.3) 46.9(+13.2) 7.5 13.0(+5.5) 8.4(+0.9) 13.0(+5.5)
+Iteration1 35.3 50.6(+15.3) 59.2(+23.9) 68.6(+33.3) 6.7 9.5(+2.8) 18.4(+11.1) 29.7(+22.4)
SFTonoracledata
Onlycorrectdata 27.4 42.2(+14.9) 34.0(+6.6) 43.6(+16.2) 5.8 7.9(+2.1) 5.5(-0.3) 12.1(+6.2)
Correctandincorrect 25.7 41.8(+16.1) 31.2(+5.5) 41.5(+15.8) 5.0 5.2(+0.2) 5.0(+0.0) 13.1(+8.1)
Baselines
GPT-3.5 66.4 80.6(+14.2) 71.0(+4.6) 74.7(+8.3) 39.7 47.8(+8.1) 45.1(+5.4) 54.3(+14.6)
Mistral-7B 33.7 49.4(+15.7) 39.0(+5.3) 46.9(+13.2) 7.5 13.0(+5.5) 8.4(+0.9) 13.0(+5.5)
Eurus-7b-SFT 36.3 66.3(+30.0) 47.9(+11.6) 53.1(+16.8) 12.3 19.8(+7.5) 16.3(+4.0) 22.9(+10.6)
Self-Refine â†’m1@t3 â†’p1@t3 â†’m1@t3 â†’p1@t3
Base 10.5 22.4(+11.9) 7.1(-3.4) 13.0(+2.5) 1.9 5.1(+3.2) 1.9(0.0) 3.1(+1.2)
+Boost 32.9 45.3(+12.4) 26.5(-6.4) 40.9(+8.0) 5.5 6.5(+1.0) 2.9(-2.6) 7.2(+1.7)
+Iteration1 35.6 49.5(+13.9) 31.7(-3.9) 43.7(+8.1) 6.3 8.7(+2.4) 5.9(-0.4) 9.9(+3.6)
+Iteration2 37.3 50.5(+13.2) 33.3(-4.0) 44.5(+7.2) 5.8 9.4(+3.6) 5.7(-0.1) 9.5(+3.7)
GPT-3.5 66.4 80.2(+13.8) 61.0(-5.4) 71.6(+5.2) 39.7 46.5(+6.8) 36.5(-3.2) 46.7(+7.0)
Mistral-7B 33.7 48.5(+14.8) 21.2(-12.5) 37.9(+4.2) 7.5 12.3(+4.8) 7.1(-0.4) 11.4(+3.9)
Eurus-7b-SFT 36.3 65.9(+29.6) 26.2(-10.1) 42.8(+6.5) 12.3 19.4(+7.1) 9.0(-3.3) 15.1(+2.8)
GloRE â†’m1@t3 â†’p1@t3
+ORM 48.2 49.5(+1.3) 57.1(+8.9)
+SORM 48.2 51.6(+3.4) 59.7(+11.5) N/A
+Direct 48.2 47.4(-0.8) 59.2(+11.0)
V-STaR â†’m64@t1
+STaR 28.0 46.1(+18.1)
+Verification 28.0 56.2(+28.2) N/A
+V-STaR 28.0 63.2(+35.2)
Table 6:
ComparingRISEwithotherapproaches(Self-Refine,GLoRE,andV-STaR)andotherbaselineapproaches.
ObservethatRISEattainsthebiggestperformanceimprovementsbetween1-turnand5-turnperformancewithouttheuseof
anoracleonbothGSM8KandMATH.Thisperformancegapisevenlargerwhenoracleearlyterminationisallowed(5-turn
w/oracle).Self-Refinelargelydegradesperformanceacrosstheboard.GLoREtrainsaseparaterefinementmodel,butstill
performsworsethanRISE.
24RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Comparison with Self-Refine [31]. To build a self-refine baseline [31] evaluation, we slightly modified
our evaluation pipeline following the self-refine approach. In this setup (Figure 11), the model generates
an initial response, and then the environment prompts the model to locate errors in the generated
solution and refine its answer based on the initial response and the identified error.
Self-Refine
System: You are an AI language model designed to assist with math problem-solving. In this task, I
will provide you with math problems. Your goal is to solve the problem step-by-step, showing your
reasoning at each step. After you have finished solving the problem, present your final answer as
âˆ–boxed{Your Answer}.
<One-shot Example 17>
User: <Query>
Agent: <Initial Answer>
User: Thereisanerrorinthesolutionabovebecauseoflackofunderstandingofthequestion. What
istheerror? Tofindtheerror,gothrougheachstepofthesolution,andcheckifeverythinglooksgood.
Agent: <Critic>
User: Now, rewrite the solution in the required format:
Agent: <Refined Answer>
Figure 11: Prompt for Self-Refine: We follow the standard pipeline of the original paper, prompt the
LLM to refine and correct its previous mistakes.
However, our experiments show that without any oracle hint from the environment or human feedback,
the self-refine approach leads to a degradation in performance across all models. Only when oracle
feedback is available to assist with early termination does the self-refine approach provide a slight
performance boost. This highlights the limitation of the self-refine structure in effectively improving
model performance without external guidance, which is also observed in [22].
Incontrast,themodeltrainedwithRISEcanattainconsistentperformanceimprovementswithoutrelying
on an oracle. By training the model to iteratively refine its responses, our method enables the model to
self-correct and improve its performance over multiple turns. This showcases the effectiveness of our
approachincomparisontotheself-refinebaseline,asitallowsformorerobustandconsistentperformance
gains without the need for the oracle assistance.
Comparison with GLoRE [17]. GLoRE is a multi-model system that relies on a student model to
propose drafts, an Outcome-based Reward Model (ORM) or Step-wise ORM to locate errors at different
granularity levels, and a Global or Local Refinement Model for adjusting these errors. Since no code
was openly available for this approach, in our experiments, we compared to the numbers from the main
paper Havrilla et al. [17]. While the comparison against GLoRE is already apples-to-oranges since our
25RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
method only trains a single end-to-end model, while GLoRE trains multiple models. Performance-wise,
GLoREâ€™sglobalandlocalrefinementmodelsshowlittletonoimprovementinoverallaccuracywithoutan
oracle,andevenexhibitdecreasingaccuracyinsomecases. However,whenanoracleisusedtoguidethe
refinement process, GLoRE demonstrates a 10% improvement on the 7B model in the GSM8K dataset.
As anticipated, since we run RISE from a less advanced base model (Llama2 7B), we observe a slightly
lower absolute performance compared to GLoRE. However, RISE demonstrates its effectiveness in self-
improvement by sequentially enhancing its performance by an impressive 13.4% within just 3 turns
without an oracle feedback, and by a remarkable 23.4% with an oracle on GSM8K. This showcase of
RISEâ€™s capabilities is particularly noteworthy considering that GLoRE utilizes 3 independent models -
one for generating candidate solutions, one reward model for locating errors, and one refinement model
for refinement.
Comparison with V-STaR [19]. V-STaR requires training an additional verifier model to rank candidate
answersgeneratedbythetargetedmodel,butitdoesnotmakeanysequentialrevisionsorimprovements
to a response. While comparing RISE to using a verifier for re-ranking the top 5 responses at the first
turn (as a base comparison) would have been informative, we were unable to find this specific result
in the original V-STaR paper. The results presented in the official table 6 for V-STaR correspond to
running 64 samples, which improves the base modelâ€™s performance by 35.2% for each prompt during
evaluation. In contrast, our method, RISE, after the same amount of finetuning iterations (3 iterations)
and using only 5 samples, improves upon the base model by 44.5% (calculated as 55.0% - 10.5% =
44.5%). This comparison highlights RISEâ€™s efficiency in achieving significant improvements with fewer
samples and iterations compared to V-STaRâ€™s approach of using a large number of samples without
sequential refinement.
Moreover, V-STaRâ€™s performance is inherently bounded by the candidate generatorâ€™s performance. As
discussed in Section 5, if there is no correct response among the generated candidates, the problem
remains unsolved. In contrast, we show in Figure 6 that RISE can also solve problems that were not
solved by majority voting with a much higher budget in the first turn. Furthermore, we believe that
combining V-STaR with RISE could lead to even better performance, as RISE can generate better models
and a verifier can be complementarily used for filtering.
Comparison with other base models. Mistral-7B [23] and Eurus-7B-SFT [58] are models that exhibit
comparable performance to our method in terms of the absolute maj@5 performance. However, it
is crucial to note that these base models are fine-tuned using a vast amount of data, including data
specificallytunedformathreasoningperformance[58],whileourmodelisfine-tunedonasingledomain.
Thatsaid,wedoshowthatfine-tuningwithRISEcanstillenhancetheperformanceofMistral-7Bmodels.
To summarize, our method offers several advantages over GLoRE and V-STaR, such as end-to-end
error correction with a single model, superior performance with fewer samples, and the ability to solve
problems that cannot be solved by random sampling in the first turn. Although our maj@1 performance
is lower than GLoREâ€™s base model EI, which is an apples-to-oranges comparison our ultimate 5-turn
performance surpasses their best absolute performance in both oracle and non-oracle scenarios.
Compared to other base models like Mistral-7B and Eurus-7B-SFT, our method achieves comparable
performance while being fine-tuned on a single domain, and it can be generalized to better base models.
26RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
B.2. Qualitative Examples from RISE
In this section, we provide some qualitative examples showing the nature of edits made by RISE on
various datasets that we train on. We utilize the model from iteration 2 of RISE.
27RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
RISE Example (Small Edits)
Query: Mike plays ping pong for 40 minutes. In the first 20 minutes, he scores 4 points. In the
second 20 minutes, he scores 25% more points. How many total points did he score?
Turn-1 Response:
Step 1: In the first 20 minutes, Mike scores 4 points.
Step 2: In the second 20 minutes, Mike scores 25% more points than he did in the first 20 minutes.
So, Mike scores 4 + 4 = 8 points in the second 20 minutes.
Step3: Tofindthetotalpoints,weneedtoaddthepointsscoredinthefirstandsecond20minutes.
total_points = 4 + 8 = 12.
Answer: 12 âœ—
Turn-2 Response:
I apologize for the mistake. Letâ€™s solve the problem step by step.
Step 1: In the first 20 minutes, Mike scores 4 points.
Step 2: In the second 20 minutes, Mike scores 25% more points than he did in the first 20 minutes.
This means he scores 4 + 4 * 25% = 4 + 1 = 5 points in the second 20 minutes.
Step 3: To find the total points, we add the points scored in the first and second 20 minutes: 4 + 5
= 9 points.
Answer: 9 âœ“
RISE Example (Big Edits)
Query: In a candy machine, there are 22 more than four times the number of pink gumballs as
there are blue gumballs. If there are 12 blue gumballs how many pink ones are there?
Turn-1 Response:
Step 1: Letâ€™s assume the number of blue gumballs as x. Step 2: According to the given information,
there are 22 more than four times the number of pink gumballs as there are blue gumballs. So, the
number of pink gumballs can be expressed as 4x + 22. Step 3: We are given that there are 12 blue
gumballs, so letâ€™s substitute x with 12 in the equation.
Answer: 54 âœ—
Turn-2 Response:
Letâ€™s solve the problem step by step: Step 1: Letâ€™s denote the number of blue gumballs as x. Step
2: We are given that the number of pink gumballs is 22 more than four times the number of blue
gumballs. So, the number of pink gumballs can be expressed as 4x + 22. Step 3: We are also given
that there are 12 blue gumballs, so we can substitute x = 12 into the expression for the number of
pink gumballs: 4 * 12 + 22 = 48 + 22 = 70 Therefore, there are
boxed70 pink gumballs.
Answer: 70 âœ“
Figure12:ExamplesofRISEcorrectitspreviousbehaviorindifferentmodes.Someonlymakechangestoasmallpart(small
edits),somemaydirectlyrewritemostofitspreviousanswers(bigedits)becausethefirststepinthepreviousansweriswrong.
Themistakenstepsofdifferentturnsarehighlightedinred,andthecorrectarehighlightedingreen.Thisdemonstratesshows
howRISEcancorrectitspreviousanswersandfinallygettoacorrectanswer.
28RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
RISE Example (Noise Tolerance)
Query:Ryanplants2flowersadayinhisgarden.After15days,howmanyflowersdoeshehaveif5didnotgrow?
Turn-1Response:
Step1:Ryanplants2flowersadayfor15days,soheplantsatotalof2flowers/day*15days=30flowersinhisgarden.
Step2:However,5flowersdidnotgrow,soweneedtosubtractthesenon-growingflowersfromthetotalcount.Step3:
CalculatethenumberofflowersthatactuallygrewinRyanâ€™sgarden.flowers_grew=30flowers-5flowers=25flowers.
Answer: 25 âœ“
Turn-2Response:
Apologiesforthemistake.Letâ€™scorrectthesolution.Step1:In15days,Ryanplants2flowersaday,soheplantsatotal
of2flowers/day*15days=30flowersinhisgarden.Step2:Since5flowersdidnotgrow,weneedtosubtractthese
non-growingflowersfromthetotalcount.Step3:CalculatethenumberofflowersthatactuallygrewinRyanâ€™sgarden.
flowers_grew=30flowers-5flowers=25flowers.
Answer: 25 âœ“
Turn-3Response:
Letâ€™sreevaluatethesolution.Step1:Ryanplants2flowersadayfor15days,soheplantsatotalof2flowers/day*15
days=30flowersinhisgarden.Step2:However,5flowersdidnotgrow,soweneedtoaddtheseflowerstothetotal
count,astheywereplantedbutdidnotgrow.Step3:CalculatethetotalnumberofflowersinRyanâ€™sgarden.total_flowers
=30flowers+5flowers=35flowers.
Answer: 35 âœ—
Turn-4Response
Letâ€™scorrectthesolution.Step1:In15days,Ryanplants2flowersaday,soheplantsatotalof2flowers/day*15days
=30flowersinhisgarden.Step2:Since5flowersdidnotgrow,weneedtosubtractthesenon-growingflowersfromthe
totalcount. Step3: CalculatethetotalnumberofflowersinRyanâ€™sgardenaftersubtractingthenon-growingflowers.
total_flowers=total_planted_flowers-non_growing_flowers=30flowers-5flowers=
boxed25flowers.
Answer: 25 âœ“
Turn-5Response
Letâ€™sreevaluatethesolution. Step1: In15days,Ryanplants2flowersaday,whichis2flowers/day*15days=30
flowersintotal.Step2:However,5flowersdidnotgrow,sowehavetosubtractthese5flowersfromthetotal.Step3:
Therefore,thetotalnumberofflowersRyanhasis30flowers(totalplanted)-5flowers(didnotgrow)=
boxed25flowers.
Answer: 25 âœ“
Figure 13: ExamplesofRISEâ€™stoleranceofnoisyfeedback.Intheevaluationwherethereisnooracle,weuseaverifier
thatalwaystellsthemodel,"Theanswerisincorrect.Pleasetryagain.Hereâ€™sthequestion:<Query>".Weobservethatfrom
turn2toturn5,themodelisstillabletooutputthecorrectanswerin3outof4turns.Intheresponseatturn4,themodel
performshalfofthesolutioncorrectly,andonlythelaststepiswrong.Inaddition,intheturnswherethemodelstilloutputs
thecorrectanswer,itdoesnâ€™tdirectlycopytheanswerfromthepreviousturnbutinsteadcomesupwithanewparaphrase.This
demonstratesthatRISEprovidesthemodelwithtolerancetonoisyfeedbackanddoesnotrandomlychangeitsbehaviorupon
receivingarevisitingrequest.Themistakenstepsofdifferentturnsarehighlightedinred
29RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
RISE Example (Error Localization)
Query: Joshdecidestotryflippingahouse. Hebuysahousefor$80,000andthenputsin$50,000inrepairs. This
increasedthevalueofthehouseby150%.Howmuchprofitdidhemake?
Turn-1Response:
Step1:Joshbuysahousefor$80,000.Step2:Heputsin$50,000inrepairs,whichincreasesthevalueofthehouseby
150%.Step3:Tocalculatetheincreaseinvalue,wemultiplythecostofthehouseby150%andaddittothecostofthe
house. increase_in_value=$80,000*150%=$120,000. Step4:Tocalculatetheprofit,wesubtractthecostofthe
housefromitsnewvalue.profit=increase_in_value-$80,000=$120,000-$80,000=$40,000.
Answer: 40,000 âœ—
Turn-2Response:
Letâ€™ssolvetheproblemstep-by-step:Step1:Joshbuysahousefor$80,000.Step2:Heputsin$50,000inrepairs,and
thisincreasesthevalueofthehouseby150%.Step3:Thepercentageincreaseinvalueis150%of$80,000,whichis
$120,000(80,000*1.5=$120,000).Step4:Tofindtheactualamountthatwouldbeaddedtothehouseâ€™svalueafter
repairs,weneedtosubtracttheamounthespentonrepairsfromtheincreasedvalue,whichis$120,000-$50,000=
$70,000.Step5:TheprofitJoshwouldmakeisthedifferencebetweenthecostofbuyingthehouseandtheincreased
valueafterrepairs,whichis$70,000-$80,000=-$10,000($10,000lessthantheinitialcost).
Answer: 0 âœ—
Turn-3Response:Letâ€™ssolvetheproblemstep-by-step:Step1:Joshbuysahousefor$80,000.Step2:Heputsin$50,000
inrepairs.Step3:Thisincreasesthevalueofthehouseby150%.Step4:Tofindtheincreaseinvalue,wecalculate150%
oftheoriginalpriceofthehouse:150%of$80,000=$120,000.Step5:Theincreaseinvalueafterrepairsistheoriginal
priceplus150%oftheoriginalprice:$80,000+$120,000=$200,000.Step6:Joshsellsthehousefor$200,000and
madeaprofitof$200,000-$80,000=$120,000.
Answer: 120,000 âœ—
Turn-4 Response: Letâ€™ssolvethe problemstep-by-step: Step 1: Joshbuys ahouse for $80,000. Step 2: He putsin
$50,000inrepairs,increasingthevalueofthehouseby150%.Step3:Theincreaseinvalueis150%of$80,000,which
is$120,000(80,000*1.5=$120,000).Step4:Thetotalamountspent,includingthepurchaseofthehouseandthe
repairs,is$80,000+$50,000=$130,000.Step5:TheprofitJoshmadeistheincreaseinvalueafterrepairsminusthe
totalamountspent,whichis$120,000-$130,000=-$10,000.Therefore,Joshmadea
boxed0profit.
Answer: 0 âœ—
Turn-5Response:Letâ€™ssolvetheproblemstep-by-step:Step1:Joshbuysahousefor$80,000.Step2:Heputsin$50,000
inrepairs.Step3:Therepairsincreasethevalueofthehouseby150%.Step4:Theincreaseinvalueis$50,000,whichis
150%of$33,333(50,000/1.5=$33,333).Step5:Therepairscost$50,000,andtheincreaseinvalueis$33,333.Step
6:Therefore,Joshâ€™sprofitis$33,333-$50,000=-$16,667.Therefore,Joshmadea16676profit.
Answer: 16676 âœ—
Figure14:ExamplesofRISEnotbeingabletocorrectitserrorwithin5stepsbutdemonstratingmeaningfulerrorlocalization
behavior.Eveninturn2,ithasalreadysolvedtheproblematstep4butmistakenlytakesanotherstepandreachesthewrong
answer.Thefollowingturnsareunabletocorrectthissmallerror.Thoughthisproblemremainsunsolved,weobservethat(1)
themodelisabletosticktothecorrectsteps,whereallresponsesreachanintermediatestepof12000correctly,exceptforthe
lastresponse,wherethemodeltriestomodifytheanswerfromonestepahead;(2)themodeldoesnâ€™trepeatitsresponses,
whichisabehaviorwenoticewhenevaluatingsomeoff-the-shelfmodels;and(3)themodelismakingmeaningfulchangesto
theincorrectsteps.Insummary,althoughthefinalanswerisstillincorrect,weobservethatthroughRISE,themodelisableto
locatetheerrorandperformlocalcomputationcorrectly.Themistakenstepsofdifferentturnsarehighlightedinred,andthe
correctstepsinturn2ishighlightedingreen.
30RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
C. Pseudocode
Algorithm 1 Data Collection at Iteration ğ‘‡
1: ğ’Ÿâ€² â† ğ’Ÿâ€²
ğ‘‡ ğ‘‡âˆ’1
2: for index ğ‘– in {1,...,|ğ’Ÿ|} do
3: ğ‘  â† ğ‘¥ğ‘–
1
4: for step ğ‘‡â€² in {1,...,ğ‘‡ âˆ’1} do
5: ğ‘¦ğ‘– â† argmaxğœ‹ (Â·|(ğ‘ ğ‘–,ğ‘¦ğ‘–,ğ‘“ğ‘–)ğ‘‡â€²âˆ’1+ğ‘  )
ğ‘‡â€² ğœƒğ‘‡âˆ’1 ğ‘¡ ğ‘¡ ğ‘¡=1 ğ‘‡â€²
6: ğ‘ ğ‘– , ğ‘Ÿğ‘– â† env.step(ğ‘ ğ‘– ,ğ‘¦ğ‘– )
ğ‘‡â€²+1 ğ‘‡â€² ğ‘‡â€² ğ‘‡â€²
7: ğ‘“ğ‘– = retry message+ğ‘¥ğ‘–
ğ‘‡â€²
8: if ğ‘Ÿğ‘– = 1 then
ğ‘‡â€²
9: break
10: end if
11: end for
12: if ğ‘Ÿğ‘– Ì¸= 1 then
ğ‘‡â€²
13: ğ‘‡â€² â† ğ‘‡â€²+1
14: ğ‘¦ğ‘– â† argmaxğœ‹Ëœ(Â·|(ğ‘ ğ‘–,ğ‘¦ğ‘–,ğ‘“ğ‘–)ğ‘‡â€²âˆ’1+ğ‘  )
ğ‘‡â€² ğ‘¡ ğ‘¡ ğ‘¡=1 ğ‘‡â€²
15: ğ‘ ğ‘– , ğ‘Ÿğ‘– â† env.step(ğ‘ ğ‘– ,ğ‘¦ğ‘– )
ğ‘‡â€²+1 ğ‘‡â€² ğ‘‡â€² ğ‘‡â€²
16: end if
17: ğ’Ÿâ€² â† ğ’Ÿâ€² âˆª{ï¸€(ï¸€ ğ‘ ğ‘–,ğ‘¦ğ‘–,ğ‘“ğ‘–,ğ‘Ÿğ‘–)ï¸€}ï¸€ğ‘‡â€²
ğ‘‡ ğ‘‡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡=1
18: end for
Algorithm 2 Inference at iteration ğ‘‡
1: for index ğ‘– in {1,...,|ğ’Ÿ|} do
2: ğ‘  â† ğ‘¥ğ‘–
1
3: for step ğ‘‡â€² in {1,...,ğ‘} do
4: ğ‘¦ğ‘– â† argmaxğœ‹ (Â·|(ğ‘ ğ‘–,ğ‘¦ğ‘–,ğ‘“ğ‘–)ğ‘‡â€²âˆ’1 +ğ‘  )
ğ‘‡â€² ğœƒğ‘‡ ğ‘¡ ğ‘¡ ğ‘¡=max{1,ğ‘‡â€²âˆ’ğ‘‡} ğ‘‡â€²
5: ğ‘ ğ‘– , ğ‘Ÿğ‘– â† env.step(ğ‘ ğ‘– ,ğ‘¦ğ‘– )
ğ‘‡â€²+1 ğ‘‡â€² ğ‘‡â€² ğ‘‡â€²
6: ğ‘“ğ‘– = retry message+ğ‘¥ğ‘–
ğ‘‡â€²
7: end for
8: for step ğ‘‡â€² in {1,...,ğ‘} do
9: ğ‘¦Ëœğ‘– â† majority voting{ğ‘¦ğ‘–}ğ‘‡â€²
ğ‘‡â€² ğ‘¡ ğ‘¡=1
10: end for
11: end for
31RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
D. Experimental Details
D.1. Hyperparameters for Fine-Tuning with RISE
For finetuning, we utilize the FastChat codebase, but we customize the loss function to be weighted
by reward. The base models are directly loaded from Hugging Face: hrefhttps://huggingface.co/meta-
llama/Llama-2-7b-hfLlama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2. The hyperparameters used for
finetuning are specified in Table 7.
Hyperparameter Values
bf16 True
epochs 2
per device train batch size 1
gpus 4xA40
gradient accumulation steps 16
learning rate 1e-5
weighted decay 0
warmup ratio 0.04
learning rate scheduler trype cosince
tf32 True
model max length 2048
Table 7: Hyperparameters used for RISE
D.2. Inference Hyperparameters
ForAPI-basedmodels,suchasGPT-3.5,wedirectlyquerytheofficialwebAPIprovidedbyOpenAI.Inthe
case of open-source models, we utilize FastChat to serve the model as a web API and interact with the
environment through API calls. Serving a 7B model requires a single A100 or A40 GPU. To control the
randomness and length of answers generated by the LLMs, we employ the hyperparameters specified in
Table 8.
Hyperparameters/Description Open-source GPT
temperature 1.0 0.7
top_p 1.0 1
max_new_tokens 1000 512
Table 8: The hyperparameter settings used for generating responses from open-source and the GPT
models.
D.3. Datasets
The GSM8K dataset consists of 7,473 problems in the training portion and 1,319 problems in the testing
portion. Similarly, the MATH dataset is divided into 7,500 problems for training and 1,000 problems
32RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
for testing. The training portions of both datasets are used to generate trajectories in each iteration of
the RISE method, while the testing portions are held out for evaluating the performance of the models.
Additionally, the SVAMP dataset, containing 1,000 problems, is used solely for evaluation purposes to
demonstrate the generalizability of our method.
For generating training trajectories, at iteration ğ‘–, the current model is rolled out for a maximum of ğ‘–
steps. Subsequently, an expert model, such as GPT-3.5, is employed to take the conversation history
between the current model and the environment and sample 5 responses. Among these responses, the
one with the highest reward is chosen, with arbitrary tie-breaking in case of multiple responses with the
same highest reward.
D.4. Prompts used for seeding RISE on the GSM8K and MATH tasks
Here we show the prompt we used for data generation and model evaluation:
RISE
System: You are an AI language model designed to assist with math problem-solving. In this task, I
will provide you with math problems. Your goal is to solve the problem step-by-step, showing your
reasoning at each step. After you have finished solving the problem, present your final answer as
âˆ–boxed{Your Answer}.
<One-shot Example 16>
User: <Query>
Agent: <Answer>
User: The answer is incorrect. Please try again. Hereâ€™s the question: <Query>
Agent: <Answer>
Â·Â·Â·
Figure 15: Prompt for RISE: We follow the standard pipeline of math problem solving, prompt the LLM
to solve the problem and retry untill exceeds the maximum number of turns
33RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
D.5. One-shot Example
RISE One-shot example
User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students
take both mathematics and physics. How many drama club students take neither mathematics nor
physics?
Agent: Step 1: To find the number of drama club students who take neither mathematics nor
physics,wefirstneedtodeterminethenumberofstudentstakingonlymath, onlyphysics,andboth
math and physics.
Step 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20
students take both mathematics and physics.
Step 3: To calculate the number of students taking only math, we subtract the number of students
taking both from the total taking math: $36 - 20 = 16$.
Step 4: Similarly, to calculate the number of students taking only physics, we subtract the number
of students taking both from the total taking physics: $27 - 20 = 7$.
Step 5: Now, to find the number of students taking neither math nor physics, we subtract the total
number of students taking only math, only physics, and both from the total number of students in
the drama club. This gives us $60 - 16 - 7 - 20 = âˆ–boxed{17}$ students taking neither.
Answer: âˆ–boxed{17}
Figure16:One-shotExample: Demonstrateshowtosolveamathproblemstep-by-stepandtheexpected
answer format.
34RecursiveIntrospection:TeachingLanguageModelAgentsHowtoSelf-Improve
Self-Refine One-shot example
User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students
take both mathematics and physics. How many drama club students take neither mathematics nor
physics?
Agent: Step 1: To find the number of drama club students who take neither mathematics nor
physics,wefirstneedtodeterminethenumberofstudentstakingonlymath, onlyphysics,andboth
math and physics.
Step 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20
students take both mathematics and physics.
Step 3: To calculate the number of students taking only math, we subtract the number of students
taking both from the total taking math: $36 - 20 = 16$.
Step 4: Similarly, to calculate the number of students taking only physics, we subtract the number
of students taking both from the total taking physics: $27 - 20 = 7$.
Step 5: Now, to find the number of students taking neither math nor physics, we add the number
of students taking only math and only physics, then subtract this sum from the total number of
students in the drama club. This gives us $60 - (16 + 7) = âˆ–boxed{37}$ students taking neither.
Answer: âˆ–boxed{37}
User: Thereisanerrorinthesolutionabovebecauseoflackofunderstandingofthequestion. What
istheerror? Tofindtheerror,gothrougheachstepofthesolution,andcheckifeverythinglooksgood.
Agent: Step 1: Correct. We need to find the number of students taking neither math nor physics.
Step 2: Correct. The given information is accurately stated.
Step 3: Correct. The calculation of students taking only math is correct.
Step 4: Correct. The calculation of students taking only physics is correct.
Step 5: Incorrect. The solution adds the number of students taking only math and only physics,
but this does not account for the students taking both subjects. By subtracting this sum from
thetotalnumberofstudents,thesolutiondouble-countsthestudentswhotakebothmathandphysics.
User: Now, rewrite the solution in the required format:
Agent: <Same as 16>
Figure 17: One-shot Example: Demonstrates the process of self-critique and error correction based on
the self-critique.
35