Interaction2Code: How Far Are We From Automatic
Interactive Webpage Generation?
JINGYUXIAO,TheChineseUniversityofHongKong,China
YUXUANWAN,TheChineseUniversityofHongKong,China
YINTONGHUOâˆ—,SingaporeManagementUniversity,Singapore
ZHIYAOXU,TsinghuaUniversity,China
MICHAELR.LYU,TheChineseUniversityofHongKong,China
ConvertingwebpagedesignintofunctionalUIcodeisacriticalstepforbuildingwebsites,whichcanbelabor-
intensiveandtime-consuming.Toautomatethisdesign-to-codetransformationprocess,variousautomated
methodsusinglearning-basednetworksandmulti-modallargelanguagemodels(MLLMs)havebeenproposed.
However,thesestudiesweremerelyevaluatedonanarrowrangeofstaticwebpagesandignoreddynamic
interactionelements,makingthemlesspracticalforreal-worldwebsitedeployment.
Tofillintheblank,wepresentthefirstsystematicinvestigationofMLLMsingeneratinginteractiveweb-
pages.Specifically,wefirstformulatetheInteraction-to-CodetaskandbuildtheInteraction2Codebenchmark
thatcontains97uniquewebpagesand213distinctinteractions,spanning15webpagetypesand30interaction
categories.Wethenconductcomprehensiveexperimentsonthreestate-of-the-art(SOTA)MLLMsusingboth
automaticmetricsandhumanevaluations,therebysummarizingsixfindingsaccordingly.Ourexperimental
resultshighlightthelimitationsofMLLMsingeneratingfine-grainedinteractivefeaturesandmanaging
interactionswithcomplextransformationsandsubtlevisualmodifications.Wefurtheranalyzefailurecases
andtheirunderlyingcauses,identifying10commonfailuretypesandassessingtheirseverity.Additionally,
ourfindingsrevealthreecriticalinfluencingfactors,i.e.,prompts,visualsaliency,andtextualdescriptions,that
canenhancetheinteractiongenerationperformanceofMLLMs.Basedonthesefindings,weelicitimplications
forresearchersanddevelopers,providingafoundationforfutureadvancementsinthisfield.Datasetsand
sourcecodeareavailableathttps://github.com/WebPAI/Interaction2Code.
AdditionalKeyWordsandPhrases:Multi-modalLargeLanguageModel,CodeGeneration,UserInterface,
WebDevelopment,EmpiricalStudy
1 INTRODUCTION
AstheInternetcontinuestoevolveandexpand,moreandmorewebsitesemerge,contributingtothe
diverseandever-growingonlineworld.Asof2024,thedigitallandscapecomprisesapproximately
1.09billionwebsites[2],supportingavarietyofapplicationsinpeopleâ€™sdailylives.
ThedesignanddevelopmentofGraphicalUserInterfaces(GUIs)arevitalforcreatingawebsite.A
well-designedGUInotonlyenhancesthewebsiteâ€™svisualattractivenessbutalsoimprovesusability
andusersatisfaction.Insuchaprocess,GUIdesigninvolvesshapingthewebsiteâ€™saesthetics[1],
suchaslayout,colors,andtypography[9,29].Incontrast,GUIdevelopmentisaboutimplementing
thataestheticthroughprogramminglanguages.Nevertheless,suchconversionisacomplexand
time-consuming task. Developers must manually map visual elements to their corresponding
implementationdetails,whichcanleadtoerrorsanddiscrepanciesbetweentheoriginaldesign
andthefinallooks[9,30,39,40,59].
ToallowdeveloperstotransformdesigndiagramsintofunctionalGUIcodemoreeasily,several
automatedGUIcodegenerationmethodshavebeenproposed,whichcanbefurthercategorized
âˆ—YintongHuoisthecorrespondingauthor.
Authorsâ€™addresses:JingyuXiao,TheChineseUniversityofHongKong,HongKong,China,whalexiao99@gmail.com;
YuxuanWan,TheChineseUniversityofHongKong,HongKong,China,yxwan9@cse.cuhk.edu.hk;YintongHuo,Singapore
ManagementUniversity,Singapore,Singapore,ythuo@cse.cuhk.edu.hk;ZhiyaoXu,TsinghuaUniversity,Beijing,China,
zhixu9557@gmail.com;MichaelR.Lyu,TheChineseUniversityofHongKong,HongKong,China,lyu@cse.cuhk.edu.hk.
4202
voN
5
]ES.sc[
1v29230.1142:viXra2 Xiaoetal.
https://www.fun.com/adult-cakeworthy-never-land-denim-jacket.html
Interactive Elements Static Elements Implemented Unimplemented
blog 53% 47% blog3% 97%
code 50% 50% code11% 89%
music 96% 4% music6% 94%
news 68% 32% news 14% 86%
search engine 88% 12% search engine11% 89%
shopping 91% 9% shopping5% 95%
sports 59% 41% sports11% 89%
technology 74% 26% technology13% 87%
video 60% 40% video12% 88%
social media 61% 39% social media8% 92%
0 20 40 60 80 100 0 20 40 60 80 100
Percentage of interactive and static elements(%) Percentage of implemented vs.unimplemented interactive elements(%)
(a) Exampleofinteractive(b) Ratio of interactive and static ele-(c) Implementedvs.unimplementedin-
elements. ments. teractiveelementsratioofGPT-4o.
Fig.1. Interactionexampleandinteractiveelementsratioofdifferenttypesofwebpages.
into two types: learning-based and LLM-based approaches. The learning-based methods, such
as Pix2code [8], design a novel method based on CNN and LSTM to generate user interface
codebyreverse-engineeringasingleGUIimageinput.Chenetal.[10]presentaneuralmachine
translator to extract visual features in UI images, encode these featuresâ€™ spatial layouts, and
generateGUIskeletonsinaunifiedneuralnetworkframework.However,thesedeeplearning-based
methodsexhibitcompromisedperformanceandfailingeneralizingtodiversewebpageelements
due to their limited knowledge learning from training samples. Recently, incorporating visual
informationintoLargeLanguageModels(LLMs)hasledtothedevelopmentofMultimodalLarge
LanguageModels(MLLMs)[3,12,31,50,56].Leadingmodelsinthisdomain,suchasGPT-4o[43],
Claude-3.5[4],andGemini-1.5[23],haveachievedexcellentperformanceinvisualunderstanding
tasks[16,55].Furthermore,researchhasshownthatLLMshaveremarkableperformanceonvarious
codeintelligencetasks[26],includingcodegeneration[19,20,22,28,32,57],codecompletion[13,
17, 18, 33, 34, 42], and code summarization [6, 11, 21, 24, 36, 37]. These advances create new
opportunitiesfortheDesign-to-Codetask,i.e.,generatingcodefromscreenshotstoreplicateweb
page elements, layout, text, and colors. For example, Design2Code [49] designs three types of
promptstostimulateMLLMsâ€™webcontentunderstandingandself-refinedcapabilitiesforGUI
codegeneration.DCGen[51]proposesadivide-and-conquer-basedapproachtopromptMLLMsto
generatewebpageelementsmoreaccurately.
Regardlessofthecontinuousinvestigationonpromotingthemodelsâ€™capability,theirevaluation
scope is restricted to static pages. More specifically, existing research [25, 49, 58] only focuses
onthestaticappearanceofthewebpage(e.g.,color,layouts),ignoringthedynamicinteractive
propertiesandfunctionalityofelements,suchassizeselectionlist,quantityadjustmentbutton
showninFig.1(a),andotherdesignsforuserengagements.Additionally,weobservethatsuch
interactiveelementsaccountforalargeproportionofthewebpageinreal-worldsoftwarepractices.
Werandomlyselect10real-worldwebpageswithdifferenttopicstoanalyzetheratioofinteractive
elements,theresultsinFig.1(b)indicatethatinteractiveelementstakeupmorethan50%cases.
ThenweutilizeGPT-4o[43]togeneratetheGUIcodecontaininginteractiveelements.Asshown
inFig.1(c),fewerthan15%ofinteractiveelementsarecorrectlyimplemented,highlightingthe
currentlimitationsinhandlingwebpageinteractivedesign.
Staticwebpagesinherentlylimituserinteractionwithwebelements,hinderingaccesstonew
content(suchasbrowsingimagesviacarouselbuttons)orimpedingtaskcompletion(likeselecting
clothingsizesfromdrop-downmenus),therebyimpairinguserexperience.Inthiscontext,evalu-
ationsofstaticpagesbecomeinadequateforreal-worldwebpagedeployments,wheredynamic
elementsareprevalent.Therefore,Wearguethatabenchmarkforwebpagesthatincludesinter-
activeelementsisessentialtoenhancethepracticality,usability,anduserengagementofstudieson
auto-generatedGUIcode.Inthispaper,weemphasizetheimportanceofwebpageinteractionsbyInteraction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 3
Table1. Summarizationofkeyfindings.
Aspects Findings
MLLMsexhibitlimitedperformanceinreproducingfine-grainedinteractionfeatures,
Limitations suchasstructure,text,andposition(Finding1).
Performancebasedonthetypeofinteraction:MLLMsexcelathandlinginteraction
withfixedpattern(e.g.,selectionlist)andclearchanges(e.g.,newwindowcreation),
whilestrugglewithinteractionsthatinvolvecomplexchanges(e.g.,iframe,
progress)andsubtlevisualmodifications(Finding3).
Thepredominantfailuresareâ€œNointeractionâ€,â€œPartialimplementationâ€,
Failure â€œInteractiveelementmissingâ€,andâ€œWrongpositionafterinteractionâ€.
Types Themostcriticalfailuresincludeâ€œInteractiveelementMissingâ€,â€œEffectonwrong
elementâ€,â€œWrongFunctionâ€andâ€œNointeractionâ€(Finding4).
â˜…Well-designedpromptsareeffective:Chain-of-Thoughtenablesstep-by-step
interactionanalysis,whilemarkinginteractionareasprovidesessentialvisualsignals.
Key
Bothapproachesimprovethequalityofgeneratedinteractions(Finding2).
Factors
â˜…Enhancedvisualsaliencysignificantlyimprovesinteractiongeneration,
particularlyincomplicatedcases(Finding5)
â˜…SupplementarytextualdescriptionssubstantiallyboostMLLMsâ€™interaction
generationcapabilities(Finding6).
investigatingthefollowingquestion:towhatextentMLLMscanproduceinteractioncode
basedonthevisualdesign?
Tothisend,weprovideasystematicanalysisofMLLMsâ€™capabilityinreproducingdynamic
interactionsonwebpages.Specifically,wefirstdefinetheInteraction-to-Codetask,i.e.,generating
codefromaseriesofscreenshotsrepresentingwebpageinteractionstoreplicateinteractiveelements.
ThenwebuildtheInteraction2Codebenchmarkthatencompassesadiversearrayofwebpages
and interactions. It comprises 97 unique web pages and 213 distinct interactions, spanning 15
webpagetypesand30interactioncategories.Bycuratingawiderangeofinteractiontypes,weoffer
arepresentativeanddiverseevaluationdatasetforassessingthecapabilitiesofMLLMsproducing
dynamicwebpagesinamorerealisticscenario.Wemainlyinvestigatethefollowingsixresearch
questions(RQs):
â€¢ RQ1:HowdodifferentMLLMsperforminInteraction-to-Codetaskunderdifferentprompts?
â€¢ RQ2:HowdohumansevaluatetheusabilityofinteractionsgeneratedbyMLLMs?
â€¢ RQ3:HowdoMLLMsperformincodegenerationacrossdifferentinteractionscenarios?
â€¢ RQ4:WhattypesofmistakesdoMLLMsmakeingeneratinginteractions?
â€¢ RQ5:Howdoesvisualsaliencyinfluencethequalityofgeneratedinteractions?
â€¢ RQ6:Whichrepresentationmodalityâ€“visualsignalsortextualdescription,enhancesMLLMsto
generateinteractioncode?
To address RQ1, we design three distinct prompt types: direct prompts, Chain-of-Thought
prompts,andMarkprompts(whichmarktheinteractionareas)toevaluatetheperformanceof
threestate-of-the-artMLLMsundervaryingpromptconditions.ForRQ2,weconductuserstudies
whereparticipantsinteractwiththegeneratedwebpagestoassesstheusability.InRQ3,weanalyze
MLLMsâ€™performanceacrossdifferentinteractionscenariosbycalculatingusabilityratesforvarious
interactiontypes.ToanswerRQ4,weinvitehumanannotatorstocategorizeanddiscusswebpage
generation failures, followed by data analysis to reveal the most prevalent error patterns and
theirseverity.ForRQ5,weevaluatethegeneratedinteractionsacrossvaryingsaliencylevelsto4 Xiaoetal.
<!DOCTYPE html> <style>
<html lang="en"> #myButton {
background-color:green;
<head> transition: background-color 0.5s;}
<title>Front-End Development</title> #myButton:hover{
<style>CSS Codes</style> background-color:blue;}
</head> </style>
<body>
<h1>Front-End Development</h1>
<p id="myParagraph">This is an example</p> <script>
<button id=â€œmyButtonâ€, const button = document.getElementById('myButtonâ€™);
draggable="true">Click Me</button> const paragraph = document.getElementById('myParagraph');
<script>JavaScript Codes</script> button.addEventListener('click', function() {
</body> paragraph.textContent = 'Button clicked!â€™;
});
</html> </script>
Fig.2. ExamplecodeofHTML,CSSandJavaScript.
investigate their impact on interaction generation performance. Finally, for RQ6, we examine
the influence of interaction representation modality by comparing three input configurations:
visual-only,textualdescription-only,andcombinedvisual-textual.
Basedonourexperimentalresults,wepresentsixkeyfindings,showninTable1,including
the limitations of MLLMs, failure types and key factors for enhancing interaction generation
performance.Ourcontributionsaresummarizedasfollows:
â€¢ Task formulation. To the best of our knowledge, this is the first study to formulate the
Interaction-to-Code task and present a systematic study on the code generation capabilities
ofMLLMsfordynamicinteractionofwebpages.
â€¢ Benchmark.Webuildthefirstreal-worldwebpageinteractiondatasetsInteraction2Codecon-
taining 97 webpages and 213 interactions, spanning 15 webpage topics and 30 interaction
categories.
â€¢ KeyFindings.Ourin-depthanalysisrevealsthelimitationsofMLLMs,identifies10representa-
tivefailuretypesandtheirunderlinecause,andprovideskeyfactorsforenhancingperformance
ontheInteraction-to-Codetask.Thiskeyfindingsoffervaluableimplicationsforresearchers
anddevelopersengagedinautomatedfront-enddevelopment.
2 BACKGROUND
2.1 BasicKnowledgeaboutFront-endDevelopment
Front-enddevelopmentfocusesonwhatusersseeandinteractwithintheirwebbrowsers.Visual
designandinteractiveimplementationaretwokeypartsofcreatingvisuallyappealinganduser-
friendlyinterfaces.Theprimarytechnologiesusedinfront-enddevelopmentareHypertextMarkup
Language(HTML),CascadingStyleSheets(CSS),andJavaScript.
2.1.1 HTML. HTML(HyperTextMarkupLanguage)isamarkuplanguageusedtocreatewebpage
content.Itdefinesthestructureandcontentofawebpagethroughtags,suchastitles,paragraphs,
andbuttons,asshowninFig2;eachHTMLelementincludesanopeningtag,content,andaclosing
tag,formingthebasicblockofawebpage.HTMLdoesnotsupportcomplexinteractions,butsome
specificelements(e.g.,form,button)andattributescanbeusedtoimplementbasicinteractive
functions.Forexample,theHTMLcodeinFig.2settheâ€œdraggableâ€attributeastrueinthebutton
flagtoallowusertodragthebutton.
2.1.2 CSS. CSS(CascadingStyleSheets)isastylesheetlanguageusedtodescribethestyleof
HTMLdocuments.Itallowswebdeveloperstocontrolthelayout,fonts,colors,spacing,andotherInteraction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 5
visual effects of the page. CSS can achieve interactive effects through pseudo-classes, pseudo-
elements,transitionsandanimations.Forexample,theCSSprogrambetweenthestyletaginFig.2
leveragestheâ€œ:hoverâ€pseudo-classtoaddaninteractiononthebutton.Thebuttonâ€™scolorwill
changefromgreentoblueoncethemousehovers.Thetransition(â€œtransition:background-color
0.5sâ€)cansmoothlychangethecolorofthebuttonover0.5secondtocreateananimationeffect.
2.1.3 JavaScript. JavaScriptisahigh-level,dynamic,andversatileprogramminglanguagethat
isprimarilyusedforaddinginteractivityanddynamicbehaviortowebsites.JavaScriptenables
developerstocreaterich,interactiveuserexperiences,manipulatetheDocumentObjectModel
(DOM),andhandleevents.Forexample,Fig.2showsthattheJavaScriptprogrambetweenthe
scripttagaddsaneventlisteneronthebutton,onceclicked,thetextcontentoftheparagraphwill
bechangedtoâ€œButtonclicked!â€.
In summary, the interaction of the front end of the web page comes from HTML tags and
attributes,stylechangesimplementedbyCSS,andcustomeventsimplementedbyJavaScript.
2.2 RelatedWork
UIcodegenerationtechniquescanbedividedintothreecategories:DeepLearning(DL)based
methods,ComputerVision(CV)basedmethods,andMultimodalLargeLanguageModel(MLLM)
basedmethods.(1)DL-basedmethods:[7,9,15,38,54]leveragesCNNstoautomaticallyprototype
software GUIs. Pix2code [8] utilizes CNNs and LSTM to extract features from GUI images to
generateadomain-specificlanguage(DSL).[14]implementsanencoder-decoderframeworkwith
anattentionmechanismtogeneratetheDSL.(2)CV-basedmethods:Sketch2Code[27]inputshand-
drawnsketchesintoobjectdetectionmodelstolearntheobjectrepresentation,whichisreadbythe
UIparsertogeneratecodefortargetedplatforms.REMAUI[41]identifiesuserinterfaceelementsvia
opticalcharacterrecognition(OCR)techniquesandtheninfersasuitableuserinterfacehierarchy
andexportstheresultsassourcecode.(3)MLLM-basedmethods:withthehelpofMLLMsâ€™powerful
understandingofimages,Design2Code[49]generatesUIcodethroughtext-augmentedandself-
revisionprompting.Tosolvetheelementomissiondistortionandmisarrangementproblemsduring
UIcodegeneration,DCGen[51]proposesadivide-and-conquer-basedapproachtogeneratethe
codeofthesubmodulesseparatelyandthenassemblethemtoconstructthefullwebpage.DeclarUI
[61]usestheelementsegmentationmethodtoaccuratelygenerateelementsandpagetransition
graphstopromptMLLMstogeneratemobileappUIwithjumplogic.Althoughtheaboveworks
achievedecentperformanceontheUIcodegenerationtask,noneofthemconsiderthegeneration
ofinteractiveelements.
3 PROBLEMDEFINITION
Todescribetheinteractionswithinawebpage,wedefinetheWebpageInteractionGraph(WIG):
ğ‘Šğ¼ğº = {ğ‘,ğ¸}, (1)
where ğ‘ = {ğ‘† 0,ğ‘† 1,..,ğ‘† ğ‘›} is a finite set of nodes representing the screenshots of the webpage.
ğ¸ = {ğ¼ 0,ğ¼ 1,...,ğ¼ ğ‘š}representsaseriesofinteractioneventsthatconnectdifferentscreenshotswith
directededges,indicatingtransitionscausedbyuserinteractions.Weusenumberstorepresent
differentinteractioneventsandthencorrespondthemtothescreenshots.Forexample,thefirst
screenshot represents the original web page, the second screenshot represents the result after
interaction1,andthethirdscreenshotrepresentstheresultafterinteraction2.Letğ¶ denotethe
ğ‘œ
originalwebpagefile,includingHTML,CSS,andJavaScriptcode,ğ‘†ğ‘œ denotethescreenshotofthe
0
webpage,ğ‘†ğ‘œ denotethewebpagescreenshotafterinteractionğ¼ ,ğº denotesthewebpageinteraction
ğ‘› ğ‘› ğ‘œ
graphoftheoriginalwebpage.ToachievetheInteraction-to-Code task,ğ‘€ takestheğº
ğ‘œ
asinputand6 Xiaoetal.
Prompts:Youareawebdeveloper
proficient in HTML, CSS and
JavaScript. You are tasked with
creating a webpage that replicates
select destination list
the appearance and interaction of
thescreenshots.
click date button
Multimodal Large
Language Models
click the number of
people button
Fig.3. AnexampleoftheinputandoutputoftheInteraction-to-Codetask.
outputsthegeneratedcodefileğ¶
ğ‘”
=ğ‘€(ğº ğ‘œ)toimplementboththevisualdesignandinteractions
oftheoriginalwebpageğ¶ .Fig.3illustratesanexampleofInteraction-to-Codetask.
ğ‘œ
4 THEINTERACTION2CODEBENCHEMARK
Inthissection,wedescribehowtoconstructrepresentativewebpagesandinteractionsforInteraction-
to-Codetasksandreportthestatisticalresultsofthedataset.
4.1 DatasetCollection
Ouroverallgoalistoobtainasetofwell-structuredwebpagesthatrepresentavarietyofreal-world
usecases(i.e.,diversewebpagesandinteractions).Wefollowthesestepsforautomaticprocessing
andmanualfiltering.
WebpageSelection.FollowingtheDesign2Code[49],webeginbycollectingwebsitelinksfrom
theC4validationset[48].Toselectwebpagesfromtheselinks,weemployfourPhDstudents
majoringincomputerscience,eachwithexperienceinfront-enddevelopment.Eachstudentis
assignedtoselectapproximately25webpages.Theselectioncriteriawereasfollows:1)complexity:
eachwebpagemustcontainatleastfourmeaningfulinteractions;2)diversity:theselectionprocess
aimstoincludeawiderangeofwebpageswithdifferenttopicsandinteractiontypes.Ultimately,
wecompileadatasetconsistingof97webpages.
AutomaticInteraction.Aftergatheringrepresentativewebpages,weutilizeSeleniumWeb-
Driver1tosimulateuserinteractionswiththepages.Ourfocusisoninteractionswithinasingle
page,soweeliminateallexternallinkstopreventnavigationawayfromthecurrentpage.Ad-
ditionally,wereplaceallimagesandvideoswithplaceholderstomitigatetheimpactofexternal
dependenciesonthecodegenerationtask.Subsequently,wetraverseallelementsonthewebpage
usingtheDocumentObjectModel(DOM)treeandcapturedifferentstatesbytakingscreenshots
beforeandafterspecificinteractions.
1https://selenium-python.readthedocs.io/Interaction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 7
Post-processing.Post-processingconsistsoftwosteps:1)interactionselection:inreal-world
webpages, there are many trivial interactions, like underline added to text when the mouse is
hovering.Topreservemeaningfulinteractionsandensurethecomplexityanddiversityofinterac-
tions,whilealsoavoidingthecontextinputintoMLLMsbeingtoolongandexceedingthemodelâ€™s
limitations,wemanuallydeletesomesimpleandmeaninglessinteractions.Finally,1-4important
andfunctionalinteractionsareretainedononewebpage.2)Manualannotation:weaskthefour
PhDstudentstoannotatethetopicsofthewebpagesandthetypesofinteractionsforbenchmark
diversityanalysis.
InteractionPartExtraction.Afterobtainingthescreenshotsbeforeandaftertheinteraction,
weextracttheinteractivepartfromthemtoevaluatethegenerationeffectoftheinteractionpart.If
theinteractiondoesnotchangethesizeofthewebpage,wecandirectlysubtractthepixelsofthe
twoscreenshotstoobtaindifferentareas(theareawherethepixelvalueisnot0aftersubtraction
is the interaction area). However, some interactions will change the size of the web page (e.g.,
generatingnewcomponents).Inthiscase,weusetheGitdifferencetool2tocalculatethedifferent
lineandcolumnnumbersofthetwoscreenshots.Theareaswheretheserowsandcolumnsintersect
aretheareasaffectedbytheinteraction.
4.2 DataStatisticsandDiversity
Quantitative Metrics. To measure the diversity and complexity of our dataset, we adopt the
samestatisticalmetricsasthoseinDesign2Code[49],withtheresultspresentedinTable2.The
LengthindicatesthetokenlengthobtainedthroughtheGPT-2tokenizer[47],tagcountrefersto
thenumberoftagsintheHTMLcode,DOMdepthsignifiesthemaximumdepthoftheHTMLâ€™s
DOMTree,anduniquetagsdenotethenumberofuniquetagsintheHTMLcode.Table2shows
thatthedataisrichinHTMLtags(1,291inapageonaverage).
TopicDistribution.Auserâ€™sinteractionwithawebpageisinfluencedbyitsfunctionandtopic.
Forinstance,socialwebsitespromoteinteractionsaboutmessagewritingandsending,whereas
e-commercesitesfocusonproductfeatureselectionandshoppingcartmodification.Tounderstand
thevarietyoftopicsrepresentedinourbenchmark,wemanuallycategorizedwebpagesbasedon
theirfunctions.AsillustratedinFigure4,ourbenchmarkcoversadiverserangeofwebtopicswith
morethan15types,includingshop,blog,business,news,bookandsoon.
shop
Table2. Quantitativemetrics.
blog 11% Other
8% 29%
Min Max Average Std
business 6%
Length(tokens) 2457 726,317 141,084 160,438 news 6% 3%
TagCount 34 12,694 1,291 1,574 6% 3% technology
DOMDepth 6 37 18 6 book 4% 4%4%4%3%3%3%3% prov did ue co
t
UniqueTags 8 58 31 9 homepage sport
hotel
form
foodstude yncyclopedia
Totalsize 97
Fig.4. Topicdistribution.
InteractionTypeDistribution.Togetasenseoftherangeofinteractiontypescoveredin
ourbenchmark,wemanuallyannotatewhattypeofinteractionstheyarebasedontheelement
tagandthevisualeffectperspective.TagcategoriescomefromtheHTMLtagslikebutton,image,
link,andsoon.Buttons,inputboxes,andlinksarethemostfrequenttypesandplayagreatrolein
2https://git-scm.com/docs/git-difftool8 Xiaoetal.
Table3. Interactiontypefrequency.
Tagcategories Visualcategories
Element Frequency Element Frequency Types Frequency
button 145 summary 16 newcomponent 94
input 80 output 16 text 93
link 47 image 14 color 61
iframe 47 video 12 newwindow 25
textarea 43 dialog 11 position 21
option 43 audio 7 size 16
select 45 template 6 switch 15
form 26 text 4 - -
label 25 area 2 - -
detail 24 span 2 - -
progress 19 table 2 - -
datalist 16 - - - -
human-websiteinteraction.Visualcategoriesinvolvechangesincolor,size,position,text,etc,the
explanationsareasfollows:
â€¢ Newcomponent:itrepresentsnewelementsaregeneratedafteraninteraction.Forexample,as
showninFig7(c),twonewinputelementswillbegeneratedafterselectingthethirdchoice.
â€¢ Text:textchangeafterinteraction,AsshowninFig.8(i),afterclickingtheâ€œSelectâ€button,the
textonitwillchangetoâ€œSelectedâ€.
â€¢ Color:itdenotesthecolorchangeafterinteraction.Forexample,thebackgroundcolorchange
fromwhiletodarkafterclickingthedarklabelasillustratedinFig.8(c).
â€¢ Newwindow:itrepresentsthatanewwindowisgeneratedaftertheinteraction,suchasaform
poppingupafterclickingthecontactbutton,asshowninFig.8(f).
â€¢ Position:itindicatesthatthepositionoftheelementchangesaftertheinteraction.Forexample,
onatexteditingwebsite,clickingtherightbuttoncanmovethetextfromthelefttotheright.
â€¢ Size:itindicatesthatthesizeoftheelementchangesaftertheinteraction.Forexample,thetext
sizewillincreaseafterclickingthelargelabelasshowninFig.8(h).
â€¢ Switch:itindicatestheswitchingofcontent.Forexample,inFig.7(b),afterclickingtheâ€œMâ€
button,theclothesparameterwillbeswitchedfromâ€œSâ€toâ€œMâ€.
Note that one interaction may belong to multiple tag categories and visual categories. Table 3
demonstratesthatInteraction2Codebenchmarkhasarichsetofinteractiontypes,including23tag
categoriesand7visualcategories.
5 STUDYSETUP
5.1 EvaluationModels
Weemploythreestate-of-the-art(SOTA)MLLMs:Gemini1.5[23],GPT-4o[43]andClaude-3.5
[4]toevaluatetheirperformanceonInteraction-to-Codetask.thespecificmodelnumbersare
20240806forGPT-4o,20240620forClaude-3.5-Sonnet,andGemini-1.5-flash-latestaccessedduring
October2024.InconfiguringtheMLLMmodels,wesetthetemperatureto1andthemaximum
numberoftokensoutputforthethreemodelsas4096.Allotherparameterswerekeptattheir
defaultsettingsasoutlinedintherelevantAPIdocumentation[5,23,44].Interaction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 9
5.2 Metrics
WeemploybothfullwebpagemetricandinteractivepartmetrictojudgethecapabilityofMLLMs
intheInteraction-to-Codetask.WemeasurethequalityofwebpagesgeneratedbyMLLMsfrom
theperspectivesofvisual,structure,andtext:
â€¢ VisualSimilarity.WeuseCLIPscore[46]tomeasurethevisualsimilarity.Thismetricmeasures
thesemanticsimilaritybetweenthegeneratedandoriginalwebpages,servingasanindicator
ofhoweffectivelythegeneratedGUIcapturestheintendedvisualelementsandoveralldesign
concept.
â€¢ StructureSimilarity.SSIM[52](StructuralSimilarityIndexMeasure)scoreisappliedtocalcu-
latethestructuresimilarity.Itevaluatesthelayoutandcompositionalaccuracy,emphasizingthe
spatialarrangementandstructuralsimilaritiesbetweenthegeneratedandoriginalwebpages.
â€¢ TextSimilarity.WefirstusepythonOCRtoolstorecognizethetextintheoriginalandthe
generatedwebpages,andthenusetheBilingualEvaluationUnderstudy(BLEU)score[45]to
measurethetextsimilaritybetweenthetwowebpages.
Fortheinteractivepartsofwebpages,inadditiontotheabovevisual,structureandtextsimilarity,
wealsoevaluatethemfromtheperspectiveofthepositionandfunctionoftheinteraction.
â€¢ Position Similarity. The position similarity between original interaction ğ¼ ğ‘œ and generated
interactionğ¼ isdefinedasfollows:
ğ‘”
ğ‘ƒğ‘œğ‘  ğ‘ ğ‘–ğ‘š(ğ¼ ğ‘œ,ğ¼ ğ‘”) =1âˆ’ğ‘šğ‘ğ‘¥(ğ‘ğ‘ğ‘ (ğ‘¥ ğ‘œ âˆ’ğ‘¥ ğ‘”),ğ‘ğ‘ğ‘ (ğ‘¦ ğ‘œ âˆ’ğ‘¦ ğ‘”)), (2)
where(ğ‘¥ ğ‘œ,ğ‘¦ ğ‘œ)and(ğ‘¥ ğ‘”,ğ‘¦ ğ‘”)arenormalizedcoordinates(in [0,1])ofthecenteroftheinteractive
area.
â€¢ FunctionUsability.Thismetricisusedtomeasurewhethertheinteractivefunctionisusable,
humanannotatorsareaskedtointeractwiththegeneratedwebpageandjudgetheusability.Let
ğ‘(Â·)denotethequantity,wecancalculatetheUsabilityRate(UR):
ğ‘(ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’)
ğ‘ˆğ‘… = . (3)
ğ‘(ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’)+ğ‘(ğ‘¢ğ‘›ğ‘¢ğ‘ ğ‘ğ‘ğ‘™ğ‘’)
5.3 PromptDesign
We design three types of prompt methods: direct prompt, chain-of-thought prompt, and mark
prompt,asshowninFig5.Inthedirectprompt,thefirstscreenshotrepresentstheoriginalwebpage
state,whilesubsequentscreenshotsdepictstatesafterspecificinteractions.Requirementsareapplied
toguideMLLMsinreplicatingthewebpagedesignandinteraction.Inparticular,requirement3
involveslettingMLLMsnumberinteractiveelementstoallowdirectidentificationbyID,enabling
automatedinteractionandscreenshotcaptureforgeneratedwebpages.FortheChain-of-Thought
(CoT)prompt[53],weusetheinstructionâ€œletâ€™sthinkstepbystepâ€anddesignthreeintermediate
steps:analyzetheinteractioneffects,locatetheinteractiveelements,andimplementtheinteraction.
FortheMarkprompt,Weuseredboundingboxestohighlighttheareasofinteraction,prompting
MLLMstofocusontheinteractiveparts.
6 EXPERIMENTS
Inthiswork,weconductexperimentstoanswerthefollowingquestions:
â€¢ RQ1:HowdodifferentMLLMsperforminInteraction-to-Codetaskunderdifferentprompts?
â€¢ RQ2:HowdohumansevaluatetheusabilityofinteractionsgeneratedbyMLLMs?
â€¢ RQ3:HowdoMLLMsperformincodegenerationacrossdifferentinteractionscenarios?
â€¢ RQ4:WhattypesofmistakesdoMLLMsmakeingeneratinginteractions?
â€¢ RQ5:Howdoesvisualsaliencyinfluencethequalityofgeneratedinteractions?10 Xiaoetal.
Direct Prompt
[Instruction]:
You are a web developer proficient in HTML, CSS and JavaScript. The user provides some screenshots of a webpage. The first screenshot [image1]
shows the webpage in its original state, while others [image2, image3,â€¦] show the webpage after the user has interacted with certain elements.
You are tasked with creating a webpage that replicates the design and interaction observed in screenshots.
[Requirements]:
1. Design Replication: Pay attention to layout, color and so on to make the webpage look identical to the first screenshot .
2. Interaction Replication : Implement the changes shown in screenshots caused by interactions (e.g., clicks).
3. Number Interactions: You need to number interactive elements from interact-1 to interact-n, interact-1 corresponds to the interaction presented in
the second screenshot, and interact-2 corresponds to the interaction presented in the third screenshot, and so on. For example,if the button is
clicked in the second screenshot, the id of the button is set to interact-1: "<button id="interact1">Click Me!</button>"
â€¦
Combine HTML, CSS and JavaScript codes into one file and respond the codes only:
Multimodal
Large Language Model
Chain-of-Thought (CoT) Prompt Mark Prompt
[Instruction] [Instruction]
[Requirements] [Requirements]
[CoT] [Mark]
You should think step by step: In the first screenshot, the interactive elements are
Step 1: Understand theinteractioneffects byanalyzing the difference between the highlighted with red bounding boxes. In otherscreenshots,
first and other screenshots. the interaction effects are highlighted with red bounding
Step 2: Locate interactive elements. boxes. Pay attention to the position of the red bounding
Step 3: Implement the interaction:the interaction function should cause the boxes, which mark the position of interaction. But do not
difference you analyze in Step 1 and be implemented the interactive element you generate the red bounding box, which is just used for
locate from step 2. marking the interaction area.
Combine HTML, CSS and JavaScript codes into one file and respond the codes only: Combine HTML, CSS and JavaScript codes into one file and
respond the codes only:
Fig.5. ThethreekindsofpromptsforMLLMs.
Table4. PerformanceofdifferentMLLMsunderdifferentpromptsonInteraction-to-Codetask.
FullPage InteractionPart
Model Prompt
CLIP SSIM Text CLIP SSIM Text Position
Direct 0.7321 0.7254 0.6481 0.7169 0.4101 0.4321 0.5860
Gemini-1.5 CoT 0.7264 0.7044 0.6525 0.7266 0.4821 0.4916 0.5893
Mark 0.7245 0.6934 0.6491 0.7120 0.4719 0.4992 0.5901
Direct 0.7651 0.6554 0.6511 0.7328 0.4221 0.4848 0.6053
GPT-4o CoT 0.7286 0.6111 0.6356 0.7212 0.4556 0.4902 0.6079
Mark 0.7429 0.6074 0.6571 0.7454 0.5583 0.5241 0.6123
Direct 0.7845 0.7819 0.6578 0.7251 0.5145 0.5070 0.6140
Claude-3.5 CoT 0.7552 0.7284 0.6421 0.7316 0.5437 0.4887 0.6102
Mark 0.7601 0.7531 0.6465 0.7577 0.5634 0.4940 0.5911
â€¢ RQ6:Whichrepresentationmodalityâ€“visualsignalsortextualdescription,enhancesMLLMsto
generateinteractioncode?
6.1 RQ1:HowdodifferentMLLMsperforminInteraction-to-Codetaskunderdifferent
prompts?
WepresenttheresultsofthreeleadingMLLMsunderthreedifferentpromptsinTable4,boldvalues
indicatetheoptimalperformance,andunderlinedvaluesindicatethesecond-bestperformance.
First,wecanmakethefollowingobservationsofMLLMsunderdirectprompting:
(1)Generationofinteractiveelementspresentsgreaterchallengesthanstaticfullweb-
pagegeneration.Table4showsthattheperformancemetricsforinteractivecomponentsare
notablylowerthanthoseforcompletewebpagesunderdirectprompts.Regardingvisualsimilarity,Interaction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 11
MLLMsattainapproximately0.73-0.78forfullpages,comparedto0.71-0.76forinteractiveelements.
Structuresimilarityshowsamorepronounceddisparity,withMLLMsachieving0.6-0.78forfull
pagesbutonly0.4-0.56forinteractivecomponents.Similarly,textsimilarityscoresreachabout
0.65forfullpages,contrastingwithapproximately0.5forinteractiveelements.
(2)MLLMsdemonstratelimitationsinaccuratelyreproducingfine-grainedfeatures
ofinteraction.TheperformanceofMLLMsonfine-grainedmetrics(suchasstructure,text,and
positionsimilarity)isnotablyweakercomparedtotheirperformanceoncoarse-grainedmetrics
likeCLIPscore.AsillustratedinTable4,fortheinteractionpart,theCLIPsimilarityexceeds0.7,
whereastextsimilarityhoversaround0.5,positionsimilarityapproximates0.45-0.62,andstructure
similarityrangesbetween0.4and0.5.
(3) Claude-3.5 outperforms GPT-4o and Gemini-1.5 in the Interaction-to-Code task.
Experimentresultsofdirectpromptingrevealsaconsistentperformanceranking,withClaude-3.5
leading,followedbyGPT-4o,andGemini-1.5showingthelowestperformance.
Finding1:WhileMLLMsdemonstratecompetenceingeneratingstaticwebpages,they
encountermorechallengeswhenproducinginteractions.Specifically,MLLMsstruggleto
reproducedetails(e.g.,structure,text,position)ofinteractions.
Toimprovetheperformanceofinteraction,wefurtherproposeCoTandMarkpromptstoforce
modelstofocusontheinteractionpart,resultinginthefollowingobservations:
(4)BothCoTandMarkpromptsenhancemodelperformancecomparedtodirectprompt,
theMarkpromptdemonstratessuperiorperformancecomparedtotheCoTprompt.GPT-
4oâ€™smetrics(CLIP,SSIM,text,position)oftheinteractionpartimprovefromdirectprompting
scores(0.7328,0.4221,0.4848,0.6053)to(0.7212,0.4556,0.4902,0.6079)withCoT,andfurtherto
(0.7454,0.5583,0.5241,0.6123)withMarkprompting.However,bothpromptingmethodsslightly
decreasefull-pagemetrics,likelyduetotheirfocusedemphasisoninteractiveelementsratherthan
overallpagecomposition.
Finding2:Chain-of-Thought(CoT)andMarkpromptsenhanceinteractiongenerationin
distinctways:CoTleveragesstep-by-stepanalysisofinteractivecomponents,whileMark
promptsfocusonclearinteractionareas.EmpiricalresultsindicatethattheMarkprompt
leadstogreaterimprovementscomparedtotheCoTmethod.
6.2 RQ2:HowdohumansevaluatetheusabilityofinteractionsgeneratedbyMLLMs?
Althoughtheabovemetricshavemeasuredthegenerationeffectoftheinteractionfromdifferent
perspectives,thefunctionalevaluationoftheinteractionstillrequireshumanevaluations.
Pairwise Model Comparison Setting. We ask three human annotators to rank a pair of
generatedinteractions(onefromthebaseline,theotherfromthetestedmethods)todecidewhich
oneimplementsthereferenceinteractionfunctionbetter.WeuseGemini-1.5withdirectprompt
asthebaselineandcollecttheothereightmethodsâ€™Win/Tie/Loseratesagainstthisbaseline.The
resultsareshowninFig6(a);ahigherwinrateandlowerlossratesuggestbetterqualityasjudged
byhumanannotators.
FunctionalityEvaluationSetting.Wealsoaskthethreeannotatorstoevaluatethefunctionality
(i.e.,usability)ofgeneratedinteraction.Iftheinteractivefunctionisconsistentwithgroundtruth,
itisregardedasusable,otherwiseunusable.Wecalculatetheusabilityrateofdifferentschemes,
theresultsareshowninFig6(b).12 Xiaoetal.
Win Tie Lose Usable Unusable
Gemini (CoT) 32% 50% 18% Gemini (Direct) 45% 55%
Gemini (Mark) 32% 53% 15% Gemini (CoT) 49% 51%
Gemini (Mark) 56% 44%
GPT-4o (Direct) 40% 57% 3%
GPT-4o (Direct) 66% 34%
GPT-4o (CoT) 42% 54% 4%
GPT-4o (CoT) 68% 32%
GPT-4o (Mark) 51% 46% 3% GPT-4o (Mark) 78% 22%
Claude (Direct) 47% 48% 5% Claude (Direct) 70% 30%
Claude (CoT) 46% 50% 4% Claude (CoT) 69% 31%
Claude (Mark) 53% 44% 3% Claude (Mark) 83% 17%
0 20 40 60 80 100 0 20 40 60 80 100
Percentage (%) Percentage (%)
(a) Pairwisemodelcomparision. (b) Usabilityevaluation.
Fig.6. Humanevaluation,ahigherwinrateindicatesbetterqualityandahigherusabilityrateindicates
betterfunctionality.
Table5. Usabilityrateofdifferenttagcategories.
Model Prompt button input link iframe textarea option select form progress
Direct 0.5395 0.5172 0.4583 0.3750 0.5238 0.6667 0.7000 0.6667 0.2857
Gemini CoT 0.5682 0.6176 0.4167 0.6250 0.6296 0.8125 0.6111 0.8750 0.4545
Mark 0.6111 0.6750 0.5333 0.5000 0.5357 0.6875 0.7500 0.8000 0.7273
Direct 0.6742 0.8485 0.5556 0.7222 0.8571 0.8889 0.8889 0.9091 0.4000
GPT CoT 0.6941 0.7857 0.6667 0.5000 0.7143 0.9375 0.8421 0.9000 0.2727
Mark 0.8316 0.8000 0.8276 0.7778 0.8519 0.9500 0.8947 0.8750 0.7000
Direct 0.6857 0.7750 0.8485 0.6111 0.7407 0.8235 0.9333 0.9167 0.6000
Claude CoT 0.7071 0.8205 0.6296 0.4444 0.7586 0.9048 0.9474 1.0000 0.3636
Mark 0.8788 0.9024 0.8667 0.7368 1.0000 0.9412 0.8750 1.0000 0.5833
Average 0.6878 0.7491 0.6448 0.5880 0.7346 0.8458 0.8269 0.8825 0.4875
Results.First,ourhumanevaluationrevealsthatClaude-3.5consistentlydemonstratessuperior
performancecomparedtootherbaselinemodels.Second,bothCoTandMarkpromptingstrategies
canenhancemodelperformancebeyonddirectprompting,showinghigherwinratesandusability
ratesacrossmostmodels(exceptClaudeâ€™sCoTprompt).Third,Markpromptingyieldsthemost
significantimprovementsinusability,withGPT-4oshowing10%and12%increasescomparedto
DirectandCoTprompts,respectively(Fig.6(b)).Notably,GPT-4owithMarkpromptingoutperforms
ClaudeunderbothDirectandCoTconditions,highlightingtheimportanceofvisualattention.Last
butnotleast,thesehumanevaluationresultsalignwithFinding2,validatingthatourautomatic
evaluationmetricsarereasonable.
6.3 RQ3:HowdoMLLMsperformincodegenerationacrossdifferentinteraction
scenarios?
Inthissection,westudytheperformanceofMLLMsontheInteraction-to-Codetaskunderdifferent
interactiontypes.Theresultsofvaryingtagcategorieswithhighfrequencyandvisualcategories
areshowninTable5andTable6,respectively.
For tag categories, form, select, and option are the easiest interaction types to generate,
achievingausabilityratehigherthan80%.Thisisbecausetheseinteractionsscenariosalways
containfixedpatterns,forexample,selectionandoptiononlyappearindrop-downlists,andInteraction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 13
Table6. Usabilityrateofdifferentvisualcategories.
Model Prompt newcomponent text color newwindow position size switch
Direct 0.5893 0.5246 0.4103 0.5000 0.5000 0.6000 0.5000
Gemini CoT 0.6119 0.5231 0.4186 0.8125 0.4615 0.5000 0.4000
Mark 0.5758 0.6719 0.4894 0.7647 0.7143 0.7143 0.6000
Direct 0.7164 0.7353 0.5208 0.9048 0.7500 0.6154 0.5000
GPT CoT 0.7538 0.8060 0.5909 0.8500 0.5625 0.8750 0.4000
Mark 0.8493 0.9054 0.7907 0.8889 0.7895 0.9000 0.9000
Direct 0.7333 0.7639 0.7111 0.7917 0.7333 0.7857 0.5000
Claude CoT 0.8205 0.8194 0.5918 0.7619 0.5000 0.6364 0.7143
Mark 0.9178 0.9189 0.8333 1.0000 0.8235 0.8182 0.7500
Average 0.7298 0.7409 0.5952 0.8083 0.6483 0.7161 0.5849
Table7. Failuretypesandtheirinfluences,where representsfullimpactand representspartialimpact.
Failure User
FailureType Content Function UsabilityRate
Object Experience
(a)Interactiveelementmissing 0%
Interactive (b)Nointeraction 6.93%
element (c)Wronginteractiveelement 91.96%
(d)Wrongtypeofinteractiveelement 88.89%
(e)Wrongpositionofinteractiveelement 97.83%
(f)Wrongpositionafterinteraction 93.81%
Interaction (g)Wrongtypeofinteractioneffects 55.88%
effects (h)Effectonwrongelement 0%
(i)PartialImplementation 75.29%
(j)Wrongfunction 0%
formoftenmerelycontainsinputboxes.Incontrast,iframeandprogresselementsshowlower
usabilityrates(<60%),attributedtotheircomplexity:iframesinvolveembeddingexternalcontent,
whileprogressbarsrequireintricatecomponentcoordinationforfunctionslikeaudiocontrolor
pricerangeadjustment,raisingdifficultiesforMLLMtounderstand.
Forvisualcategories,MLLMsexcelatgeneratinginteractionsthatresultinprominentvisual
changes,suchas creatingnewwindows,andcomponents.However,theystrugglewithsubtle
visualmodifications,suchascolorshiftsandpositionaladjustments,indicatingtheirlimitationsin
handlingfine-grainedinteractioneffects.
Finding3:Performancevariesbyinteractiontype:MLLMsaregoodathandlinginteractions
withfixedpattern(e.g.,selectionlist)andobviouschanges(e.g.,newwindowcreation),
whilestrugglingwithinteractionsinvolvingcomplexchanges(e.g.,iframe,progress)and
subtlevisualmodifications(e.g.,positionchange).
6.4 RQ4:WhattypesofmistakesdoMLLMsmakeingeneratinginteractions?
Weaskannotatorstoanalyzethedifferencebetweenthegeneratedinteractionsandtheoriginal
ones,thensummarizethefailuretypesandevaluatetheirinfluencefromcontent,functionanduser
experience.Inspecific,wefirstrandomlyselect20interactionsforanalysisandthendiscuss,revise,14 Xiaoetal.
Reference Reference Reference Reference Reference
Generated Generated Generated Generated Generated
(d) Wrong Types of (e) Wrong Position of
(a) Interactive Element Missing (b) No Interaction (c)Wrong Interactive Element Interactive Element Interactive Element
Fig.7. Failureoninteractiveelements.
Reference Reference Reference Reference Reference
Generated Generated Generated Generated Generated
(f) Wrong Position of (g) Wrong type of
Interaction Effect interaction Effect (h)Effect on Wrong Element (i) Partial Implementation (j) Wrong Function
Fig.8. Failureofinteractioneffects.
andrefinethefailuretypeuntileveryonereachesaconsensus.Finally,wemanuallyannotatethe
failuretypesofallinteractionsandcalculatetheUsabilityRate(UR)basedonthehumanevaluation
resultsofRQ2.Table7showstheresultsoffailuretypesandtheirinfluence,itcontains10typesof
failure.TenrepresentativefailureexamplesareshowninFig7andFig8,wherethefirstrowshows
thereferenceinteraction,andthesecondrowshowsthegeneratedinteractionbyMLLMs.
Failurereasonanalysis.Failures(a),(c),(e),and(f)stemfromMLLMsâ€™limitationsinelement
localization.Failures(d)and(g)arecausedbyMLLMsâ€™misidentificationofelementtypes.Failures
(b),(h),(i),and(j)arisefromMLLMsâ€™misunderstandingofinteraction.
BaseonthefailuredistributioninFig9,wefindthat,themainfailuremodesincludeâ€œNo
interactionâ€,â€œPartialimplementationâ€,â€œInteractiveelementmissingâ€,andâ€œWrongposi-
tionafterinteractionâ€.Model-specificanalysisrevealsdistinctpatterns:Gemini-1.5â€™sfailuresare
dominatedbyâ€œNointeractionâ€andâ€œPartialimplementationâ€(>50%),whileGPT-4omainlyfaces
issueswithâ€œInteractiveelementmissingâ€andâ€œNointeractionâ€(>20%).Claude-3.5â€™schallengesare
primarilyinâ€œNointeractionâ€andâ€œWrongpositionafterinteractionâ€(>20%).Thesefailuresstem
fromtwokeyissues:MLLMsâ€™inadequateinteractioncomprehensionleadingtoâ€œNointeractionâ€Interaction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 15
No interaction 37.25% No failure 37.72% No failure 51.17%
18.0% No failure 12.99% Interactive element missing 17.53% No interaction
15.65% Partial Implementation 8.45% No interaction 6.88% Wrong position after interaction
10.17% Wrong function 7.98% Wrong position of interactive element 5.79% Wrong interactive element
5.95% Wrong interactive element 7.35% Wrong position after interaction 4.85% Partial Implementation
3.76% Interactive element missing 6.73% Partial Implementation 4.85% Interactive element missing
3.45% Wrong position after interaction 5.79% Wrong interactive element 3.45% Wrong position of interactive element
2.97% Wrong position of interactive element 4.07% Wrong type of interaction effects 1.88% Wrong function
2.04% Wrong type of interactive element 3.6% Wrong type of interactive element 1.41% Wrong type of interactive element
0.78% Effect on wrong element 2.98% Effect on wrong element 1.25% Wrong type of interaction effects
0.0% Wrong type of interaction effects 2.35% Wrong function 0.94% Effect on wrong element
0 5 10 15 Perce2n0tage (%)25 30 35 40 0 5 10 15 Perce2n0tage (%)25 30 35 40 0 10 20 Percentag3e0 (%) 40 50
(a) Gemini-1.5. (b) GPT-4o. (c) Claude-3.5.
Fig.9. FailuredistributionofthreeMLLMs.
andâ€œPartialimplementationâ€,impreciseelementandinteractiveeffectslocalizationofMLLMs
resultsinâ€œInteractiveelementmissingâ€andâ€œWrongpositionafterinteractionâ€.
Besides,themostseriousfailuresareâ€œInteractiveelementMissingâ€,â€œEffectonwrong
elementâ€,â€œWrongFunctionâ€andâ€œNointeractionâ€.Theseverityofthefailuresdependson
the usability rate (UR), with higher UR meaning lower severity and lower UR meaning higher
severity.AsillustratedinTable7,failure(a),(b),(h)and(j)exhibitURlowerthan10%,rendering
thegeneratedinteractionscompletelyineffective.
Finding4:â€œNointeractionâ€,â€œPartialimplementationâ€,â€œInteractiveelementmissingâ€,and
â€œWrongpositionafterinteractionâ€constitutethemostfrequentfailures.â€œInteractiveelement
Missingâ€,â€œEffectonwrongelementâ€,â€œWrongFunctionâ€,andâ€œNointeractionâ€provespartic-
ularlycritical,accountingforonly10%usablecasesingeneratedinteractions.Enhancing
MLLMsâ€™abilitytolocateinteractiveelementsandunderstandtheinteractionstoavoid
commonandseriousfailuresisveryimportant.
6.5 RQ5:Howdoesvisualsaliencyinfluencethequalityofgeneratedinteractions?
ThevisualperceptionlimitationsofMLLMsaffecttheirperformanceonvisualunderstandingtasks,
especiallywhenfacingsmalllow-resolutionobjects[60].Inthissection,weexaminetheimpact
ofinteractionarearatio(i.e.,visualsaliency)ongenerationoutcomes.Letğ¼ denoteinteraction,ğ‘†
ğ¼
denotethescreenshotofthewebpageafterinteractionğ¼,wedefinethevisualsaliency(ğ‘‰ğ‘†)ofthe
interactionasfollows:
ğ‘ğ‘Ÿğ‘’ğ‘(ğ¼)
ğ‘‰ğ‘†(ğ¼) = , (4)
ğ‘ğ‘Ÿğ‘’ğ‘(ğ‘† ğ¼)
whereğ‘ğ‘Ÿğ‘’ğ‘()calculatesthesize(inpixels)ofacomponent.AhigherVSscoreindicatesalarger
areainfluencedbytheinteractionand,consequently,ahighervisualsaliency.
Wefirstcalculatethevisualsaliencyforallinteractionsandplotthedistribution,asshownin
Figure11.Wethendividethesamplesintofivegroupsbasedonthedistributionresults,keepingthe
numberofsamplesineachgrouproughlybalanced.TheVSrangesforthefivegroupsareasfollows:
[0,0.025),[0.025,0.05),[0.05,0.1],[0.1,0.2),[0.2,1).Figure10showstheboxplotdistributionof
metricsforGemini-1.5acrossthesefivegroups,allowingustodrawthefirstobservation:
(1) The group with higher visual saliency has higher SSIM and position similarity.
Although the clip and text similarity fluctuates among different groups, as shown in Fig 10(a),
Fig10(b)showsthattheSSIMandpositionsimilaritysignificantlyincreasesasthevisualsaliency
increases.AsshowninFig10(b),thegroup[0.2,1)showsthehighestmetrics,whilethegroup
[0,0.025)showsthelowestmetrics.ThisdemonstratesthatMLLMsaremorelikelytocapture
structuralandpositionalfeaturesforsampleswithhighvisualsaliency.16 Xiaoetal.
CLIP Text SSIM Position
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(0-0.025) (0.025-0.05) (0.05-0.1) (0.1-0.2) (0.2-1) (0-0.025) (0.025-0.05) (0.05-0.1) (0.1-0.2) (0.2-1)
(a) CLIPandText. (b) SSIMandPosition.
Fig.10. InteractionpartmetricsdistributionofdifferentgroupsofGemini-1.5underthedirectprompt.
Wethenrandomlysample10webpagesfromfailurecasesandcropthescreenshotstoincrease
thevisualsaliencyoftheinteractionsinthewebpages(forexample,ifthewebpageiscroppedto1/2
oftheoriginal,thevisualsaliencyoftheinteractionwillbedoubled).Fig12showstherelationship
betweenthemagnificationfactorandthemetricsofgenerationresults.Weobservethat:
(2)Enhancedvisualsaliencyfacilitatestheeffectivegeneration.Whenthemagnification
factorissetto1,allevaluationmetricsyieldvaluesof0,indicatingtheunsuccessfulinteraction
generation. Upon increasing VS by 1.2 times, the model is able to reproduce interactions, but
withrelativelylowmetricscores.Asthemagnificationfactorincreasesfrom1.2to3,weobserve
substantialimprovementsinperformancemetrics:theCLIPandSSIMsimilaritiesapproach0.8,
whiletextandpositionsimilaritiesreachapproximately0.6.Thissuggeststhatmodelsareeffectively
overcomingtheoriginalfailurecases.
10.0 1.0 1.0
Histogram 0.8 CLIP
7.5 0.8 SSIM CDF
0.6 Text
5.0 0.6 0.4 Position
2.5 0.4 0.2
0.0 0.2 0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.5 2.0 2.5 3.0
Visual Saliency Scale
Fig.11. Visualsaliencydistribution. Fig.12. Metricsunderdifferentmagnification.
Finding5:VisualsaliencyaffectstheMLLMsâ€™performanceoninteractiongeneration,and
enhancingvisualsaliencycanleadtomoreaccuratecodegeneration.
6.6 RQ6:Whichrepresentationmodalityâ€“visualsignalsortextualdescription,
enhancesMLLMstogenerateinteractioncode?
TheperformanceofMLLMsinUIcodegenerationlargelydependsontheirabilitytocomprehend
interactions.Certaininteractions,particularlycomplexonesorthosewithlowvisualsaliency,pose
significantchallengesforMLLMswhenrelyingsolelyonscreenshotsforcomprehension.Natural
languagedescriptionsmayserveasavaluablesupplementtoenhanceunderstanding.
Toinvestigatetheimpactofdifferentinputsignals,weconductexperimentsonGemini-1.5and
GPT-4ousing10randomlyselectedwebpagesfromfailurecases.Humanannotatorsprovidetextual
descriptionsforeachinteraction(e.g.,"clickingtheloginbuttontriggersanewwindowwithtwo
inputboxes").Weevaluatethreeexperimentalconditions:visualinputonly(V),textualdescription
rebmuN
ytilibaborP
evitalumuCInteraction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 17
Table8. PerformanceofMLLMswithdifferentmodalityinputs.
Gemini-1.5 GPT-4o
Prompt Modality
CLIP SSIM Text Position CLIP SSIM Text Position
V 0.3338 0.1587 0.2777 0.3342 0.3737 0.1793 0.2539 0.3951
Direct T 0.3116 0.1550 0.1687 0.3999 0.4174 0.4067 0.2316 0.4293
V+T 0.5679 0.3010 0.2732 0.5964 0.6735 0.5612 0.3919 0.7157
V 0.4357 0.1975 0.3072 0.4303 0.3871 0.3101 0.2433 0.4461
CoT T 0.3677 0.0897 0.2290 0.4403 0.5579 0.1828 0.3045 0.5465
V+T 0.5503 0.4027 0.3558 0.5656 0.6440 0.4800 0.4287 0.7080
V 0.4502 0.3256 0.2197 0.4302 0.5015 0.4520 0.3389 0.5025
Mark T 0.5019 0.2478 0.2921 0.5301 0.4613 0.4454 0.2805 0.4810
V+T 0.5946 0.4327 0.3416 0.4791 0.6923 0.4336 0.4248 0.7469
only(T),andcombinedvisual-textualinput(V+T).Table8presentstheresults,withboldvalues
indicatingthebestperformanceandunderlinedvaluesshowingthesecond-bestperformance.We
canmakethefollowingobservations:
(1)IntegratingbothvisualandtextualdescriptionsenablesMLLMstoachieveoptimal
performanceontheInteraction-to-Codetask.Itischallengingtodeterminewhethervisual-
onlyortextdescription-onlyinputsaresuperiorbasedonTable8,asthereareinstanceswhere
â€œVâ€isbetterandotherswhereâ€œTâ€excels.However,thecombinedapproach(V+T)consistently
outperformssingle-modalityinputsinmostscenariosacrossallthreeprompttypes.Theresult
suggestsacomplementaryrelationshipbetweenvisualandtextualinputs,underscoringthebenefits
ofintegratingbothmodalitiesforadvancedperformance.
(2)Supplementarytextdescriptionscanbridgetheperformancegapacrossdifferent
modelcapabilitiesandpromptstrategies.Underdirectprompting,Gemini-1.5withcombined
visualandtextualinputs(V+T)demonstratessuperiorperformancecomparedtoGPT-4ousing
eithervisual(V)ortextual(T)inputsalone.Furthermore,Gemini-1.5â€™sperformancewithcombined
inputsunderdirectpromptingsurpassesitsownperformancewithvisual-onlyinput,evenwhen
enhancedbyChain-of-Thought(CoT)orMarkpromptingstrategies.
Finding6:TheincorporationofvisualandtextualinputsconsiderablyenhancesMLLMsâ€™
capabilitytogenerateinteractions.Withtextualdescriptions,evenaweakermodelcan
achievecomparableperformancetothoseofsuperiormodelswithouttextualdescriptions.
7 DISCUSSION
ImplicationsforResearchers.Thefindingsofourstudyshedlightonfollowingfuturedirections
toimprovethequalityofMLLM-generatedUIcodeinpractice.
â€¢ EnhancingMLLMsâ€™recognitionoffine-grainedwebpagefeatures.AsnoticedinFinding1,
MLLMsoftenstruggletoreproducedetailsofinteractions,suchasposition,text,andstructure.
Therefore,itisessentialtoexplorestrategiestoimprovethemodelâ€™ssensitivityonthesefine-
grainedfeatures.
â€¢ CorrectingerrorsinMLLM-generatedcode.InRQ4,weoutlinecommonmistakeswhen
MLLMsgenerateinteractivecomponents.Developingautomatedmethodstoidentifyfailure
typesandfixerrorsiscrucialinreproducingreliableandusablewebpages.18 Xiaoetal.
â€¢ EnhancingtheMLLMâ€™sgroundingofGUIelementsanditsunderstandingofinteractions.
InRQ4,weanalyzethattheexistingfailuresarisefromtheinabilityofMLLMstoaccuratelylocate
the interacting elements, understand their functionalities, and comprehend the interactions.
Therefore,itisessentialtoenhancethecapabilitiesofMLLMsinthisarea.Alternatively,aGUI
interactiveelementrecognitionmodelandaninteractiveanalysismodelcouldbeimplemented
priortoMLLMinputtoaddresstheselimitations.
ImplicationsforDevelopers.Basedonourfindings,weproposethefollowingpracticalguidelines
fordevelopersleveragingMLLMsinautomatedfront-enddevelopment:
â€¢ Applyingvisualmarkersforinteractiveelements.DerivedfromFinding2,incorporating
mark prompts with red bounding boxes significantly enhances MLLMsâ€™ ability to generate
accurateinteractions.ThesevisualmarkersenableMLLMstopreciselyidentifybothinteractive
elementsandtheireffectareas.
â€¢ Optimizeinteractiveelementvisibility.Finding5indicatesthatenhancedvisualsaliency
leadstomoreeffectiveinteractiongeneration.Werecommendincreasingthevisualsaliencyof
theinteractionbyslicingtheimage,orevenjustinputtingintheinteractiveareatogenerate
thecodefortheinteractionpartfirst,followedbytheintegrationofthegeneratedcodeintothe
mainwebpagecode.
â€¢ Providecomprehensiveinteractiondescriptions.AsevidencedbyFinding6,detailedtextual
descriptionsimproveinteractiongenerationquality.Developerscanincludeexplicitdescriptions
(like the position, interactive elements, and effects) of interaction in their prompts to make
MLLMsunderstandtheinteractionclearly.
8 THREATSTOVALIDITY
Limitedcontextlength.Aswebpagesbecomemorecomplexwithnumerousinteractions,theinput
contextexpands,potentiallyexceedingthecontextwindowconstraintsofMLLMs(e.g.,128Ktokens
for GPT-4o). Nevertheless, this limitation can be mitigated by employing iterative generation,
progressivelyproducinginteractionsforawebpageovermultiplerounds.
Modelselection.ThisstudyutilizesthreeprominentMultimodalLargeLanguageModels(MLLMs)
toconductexperiments.TherearesomeopensourceMLLMssuchasLLaVa[35]wedonâ€™ttest,we
willtesttheperformanceofthesemodelsonInteraction-to-Codetaskinthefuturework.
Unabletohandleinteractionsthatrequireback-end.Somecomplexfunctionalinteractions(e.g.,
login,search,etc.)areimplementedbyserver-sidescriptinglanguageslikePython.Thebenchmark
wecollectdoesnotincludeback-endcode;wecannotverifythegenerationeffectofsuchinteractions,
butwebelieveourworkisanimportantsteptowardgeneratinginteractivewebsites.
9 CONCLUSION
ThispaperpresentsthefirstsystematicevaluationofMLLMsintheInteraction-to-Codetask.We
introduceaformaldefinitionoftheInteraction-to-Codeparadigmandestablishthecomprehensive
Interaction2Code benchmark encompassing diverse interaction scenarios. Through extensive
automatedandhumanevaluations,weassessMLLMsâ€™performanceandusabilityofgenerated
interactions.OurkeyfindingsrevealthelimitationsofMLLMsintheInteraction-to-Codetask,
failure types, and key factors (prompts, enhanced visual saliency, and supplementary textual
descriptions)forenhancingtheinteractiongenerationperformanceofMLLMs.
REFERENCES
[1] 2024. The 10 best user interface (UI) design tools to try in 2024. UX Design Institute (2024). https://www.
uxdesigninstitute.com/blog/user-interface-ui-design-tools/Accessed:2024-10-06.Interaction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 19
[2] 2024. TopWebsiteStatisticsFor2024. ForbesAdvisor(2024). https://www.forbes.com/advisor/business/software/
website-statistics/Accessed:2024-10-06.
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
KatherineMillican,MalcolmReynolds,etal.2022.Flamingo:avisuallanguagemodelforfew-shotlearning.Advances
inneuralinformationprocessingsystems35(2022),23716â€“23736.
[4] Anthropic.2024. IntroducingClaude3.5Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet Accessed:
2024-09-29.
[5] Anthropic.2024.VisionDocumentation. https://docs.anthropic.com/en/docs/visionAccessed:2024-10-18.
[6] ShushanArakelyan,RocktimJyotiDas,YiMao,andXiangRen.2023. ExploringDistributionalShiftsinLarge
LanguageModelsforCodeAnalysis.InConferenceonEmpiricalMethodsinNaturalLanguageProcessing. https:
//api.semanticscholar.org/CorpusID:257557735
[7] BatuhanAÅŸÄ±roÄŸlu,BÃ¼ÅŸtaRÃ¼meysaMete,EyyÃ¼pYÄ±ldÄ±z,YaÄŸÄ±zNalÃ§akan,AlperSezen,MustafaDaÄŸtekin,andTolga
Ensari.2019.AutomaticHTMLcodegenerationfrommock-upimagesusingmachinelearningtechniques.In2019
ScientificMeetingonElectrical-Electronics&BiomedicalEngineeringandComputerScience(EBBT).Ieee,1â€“4.
[8] TonyBeltramelli.2018.pix2code:Generatingcodefromagraphicaluserinterfacescreenshot.InProceedingsofthe
ACMSIGCHIsymposiumonengineeringinteractivecomputingsystems.1â€“6.
[9] C.Chen,T.Su,G.Meng,Z.Xing,andY.Liu.2018.FromUIdesignimagetoGUIskeleton:aneuralmachinetranslator
tobootstrapmobileGUIimplementation.InProceedingsofthe40thInternationalConferenceonSoftwareEngineering.
665â€“676.
[10] ChunyangChen,TingSu,GuozhuMeng,ZhenchangXing,andYangLiu.2018.Fromuidesignimagetoguiskeleton:
aneuralmachinetranslatortobootstrapmobileguiimplementation.InProceedingsofthe40thInternationalConference
onSoftwareEngineering.665â€“676.
[11] FuxiangChen,FatemeMoradianFard,DavidLo,andTimofeyBryksin.2022.OntheTransferabilityofPre-trained
LanguageModelsforLow-ResourceProgrammingLanguages.2022IEEE/ACM30thInternationalConferenceonProgram
Comprehension(ICPC)(2022),401â€“412. https://api.semanticscholar.org/CorpusID:248266381
[12] JunChen,HanGuo,KaiYi,BoyangLi,andMohamedElhoseiny.2022.Visualgpt:Data-efficientadaptationofpretrained
languagemodelsforimagecaptioning.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.18030â€“18040.
[13] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePonde,JaredKaplan,HarrisonEdwards,YuraBurda,
NicholasJoseph,GregBrockman,AlexRay,RaulPuri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,
PamelaMishkin,BrookeChan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,Mohammad
Bavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DavidW.Cummings,MatthiasPlappert,Fotios
Chantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamH.Guss,AlexNichol,IgorBabuschkin,SuchirBalaji,Shantanu
Jain,AndrewCarr,JanLeike,JoshuaAchiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewM.Knight,Miles
Brundage,MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,
andWojciechZaremba.2021. EvaluatingLargeLanguageModelsTrainedonCode. ArXivabs/2107.03374(2021).
https://api.semanticscholar.org/CorpusID:235755472
[14] Wen-YinChen,PavolPodstreleny,Wen-HuangCheng,Yung-YaoChen,andKai-LungHua.2022.Codegeneration
fromagraphicaluserinterfaceviaattention-basedencoderâ€“decodermodel.MultimediaSystems28,1(2022),121â€“130.
[15] AndrÃ©ArmstrongJaninoCizotto,RodrigoClementeThomdeSouza,VivianaCoccoMariani,andLeandrodos
SantosCoelho.2023.Webpagesfrommockupdesignbasedonconvolutionalneuralnetworkandclassactivation
mapping.MultimediaToolsandApplications82,25(2023),38771â€“38797.
[16] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,BoyangAlbertLi,
PascaleFung,andStevenC.H.Hoi.2023. InstructBLIP:TowardsGeneral-purposeVision-LanguageModelswith
InstructionTuning.ArXivabs/2305.06500(2023). https://api.semanticscholar.org/CorpusID:258615266
[17] VictorC.Dibia,AdamFourney,GaganBansal,ForoughPoursabzi-Sangdeh,HanLiu,andSaleemaAmershi.2022.
AligningOfflineMetricsandHumanJudgmentsofValueofAI-PairProgrammers. ArXivabs/2210.16494(2022).
https://api.semanticscholar.org/CorpusID:253237523
[18] HantianDing,VarunKumar,YuchenTian,ZijianWang,RobertKwiatkowski,XiaopengLi,MuraliKrishnaRamanathan,
BaishakhiRay,ParminderBhatia,SudiptaSengupta,DanRoth,andBingXiang.2023.AStaticEvaluationofCode
CompletionbyLargeLanguageModels. ArXivabs/2306.03203(2023). https://api.semanticscholar.org/CorpusID:
259088657
[19] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. Self-collaboration Code Generation via ChatGPT. ArXiv
abs/2304.07590(2023). https://api.semanticscholar.org/CorpusID:258179537
[20] XueyingDu,MingweiLiu,KaixinWang,HanlinWang,JunweiLiu,YixuanChen,JiayiFeng,ChaofengSha,XinPeng,
andYilingLou.2023.ClassEval:AManually-CraftedBenchmarkforEvaluatingLLMsonClass-levelCodeGeneration.
ArXivabs/2308.01861(2023). https://api.semanticscholar.org/CorpusID:26043906220 Xiaoetal.
[21] ShuzhengGao,XinjieWen,CuiyunGao,WenxuanWang,andMichaelR.Lyu.2023. ConstructingEffectiveIn-
ContextDemonstrationforCodeIntelligenceTasks:AnEmpiricalStudy. ArXiv abs/2304.07575(2023). https:
//api.semanticscholar.org/CorpusID:263867793
[22] HenryGilbert,MichaelSandborn,DouglasC.Schmidt,JesseSpencer-Smith,andJulesWhite.2023.SemanticCom-
pressionwithLargeLanguageModels.2023TenthInternationalConferenceonSocialNetworksAnalysis,Management
andSecurity(SNAMS)(2023),1â€“8. https://api.semanticscholar.org/CorpusID:258309482
[23] Google.2024.GeminiAPI. https://ai.google.dev/gemini-apiAccessed:2024-10-06.
[24] JianGu,PasqualeSalza,andHaraldC.Gall.2022.AssembleFoundationModelsforAutomaticCodeSummarization.
2022IEEEInternationalConferenceonSoftwareAnalysis,EvolutionandReengineering(SANER)(2022),935â€“946. https:
//api.semanticscholar.org/CorpusID:245986582
[25] YiGui,ZhenLi,YaoWan,YeminShi,HongyuZhang,YiSu,ShaolingDong,XingZhou,andWenbinJiang.2024.
VISION2UI:AReal-WorldDatasetwithLayoutforCodeGenerationfromUIDesigns.arXivpreprintarXiv:2404.06369
(2024).
[26] XinyingHou,YanjieZhao,YueLiu,ZhouYang,KailongWang,LiLi,XiapuLuo,DavidLo,JohnC.Grundy,andHaoyu
Wang.2023.LargeLanguageModelsforSoftwareEngineering:ASystematicLiteratureReview.ArXivabs/2308.10620
(2023). https://api.semanticscholar.org/CorpusID:261048648
[27] VanitaJain,PiyushAgrawal,SubhamBanga,RishabhKapoor,andShashwatGulyani.2019.Sketch2Code:transforma-
tionofsketchestoUIinreal-timeusingdeepneuralnetwork.arXivpreprintarXiv:1910.08930(2019).
[28] ShuyangJiang,YuhaoWang,andYuWang.2023. SelfEvolve:ACodeEvolutionFrameworkviaLargeLanguage
Models.ArXivabs/2306.02907(2023). https://api.semanticscholar.org/CorpusID:259076266
[29] KatiKuusinen andTommiMikkonen.2013. DesigningUserExperienceforMobileApps: Long-TermProduct
OwnerPerspective. 201320thAsia-PacificSoftwareEngineeringConference(APSEC) 1(2013),535â€“540. https:
//api.semanticscholar.org/CorpusID:18632493
[30] ValÃ©riaLelli,ArnaudBlouin,andBenoÃ®tBaudry.2015. ClassifyingandQualifyingGUIDefects. 2015IEEE8th
InternationalConferenceonSoftwareTesting,VerificationandValidation(ICST)(2015),1â€“10. https://api.semanticscholar.
org/CorpusID:2288032
[31] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.2023.Blip-2:Bootstrappinglanguage-imagepre-trainingwith
frozenimageencodersandlargelanguagemodels.InInternationalconferenceonmachinelearning.PMLR,19730â€“19742.
[32] JiaLi,GeLi,YongmingLi,andZhiJin.2023. EnablingProgrammingThinkinginLargeLanguageModelsToward
CodeGeneration.ArXivabs/2305.06599(2023). https://api.semanticscholar.org/CorpusID:263896057
[33] TszOnLi,WenyiZong,YiboWang,HaoyeTian,Y.Wang,andS.C.Cheung.2023.NuancesaretheKey:Unlocking
ChatGPTtoFindFailure-InducingTestswithDifferentialPrompting.202338thIEEE/ACMInternationalConferenceon
AutomatedSoftwareEngineering(ASE)(2023),14â€“26. https://api.semanticscholar.org/CorpusID:258298446
[34] ZongjieLi,ChaozhengWang,ZhiboLiu,HaoWang,ShuaiWang,andCuiyunGao.2022. CCTEST:Testingand
RepairingCodeCompletionSystems.2023IEEE/ACM45thInternationalConferenceonSoftwareEngineering(ICSE)
(2022),1238â€“1250. https://api.semanticscholar.org/CorpusID:251623193
[35] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.2024. LLaVA-NeXT:
Improvedreasoning,OCR,andworldknowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/
[36] AntonioMastropaolo,LucaPascarella,andGabrieleBavota.2022. UsingDeepLearningtoGenerateComplete
LogStatements. 2022IEEE/ACM44thInternationalConferenceonSoftwareEngineering(ICSE)(2022),2279â€“2290.
https://api.semanticscholar.org/CorpusID:245906103
[37] AntonioMastropaolo,SimoneScalabrino,NathanCooper,DavidNader-Palacio,DenysPoshyvanyk,RoccoOliveto,and
GabrieleBavota.2021.StudyingtheUsageofText-To-TextTransferTransformertoSupportCode-RelatedTasks.2021
IEEE/ACM43rdInternationalConferenceonSoftwareEngineering(ICSE)(2021),336â€“347. https://api.semanticscholar.
org/CorpusID:231786586
[38] KevinMoran,CarlosBernal-CÃ¡rdenas,MichaelCurcio,RichardBonett,andDenysPoshyvanyk.2018. Machine
learning-basedprototypingofgraphicaluserinterfacesformobileapps.IEEETransactionsonSoftwareEngineering46,
2(2018),196â€“221.
[39] KevinMoran,BoyangLi,CarlosBernal-CÃ¡rdenas,DanJelf,andDenysPoshyvanyk.2018.AutomatedReportingof
GUIDesignViolationsforMobileApps.2018IEEE/ACM40thInternationalConferenceonSoftwareEngineering(ICSE)
(2018),165â€“175. https://api.semanticscholar.org/CorpusID:3634687
[40] T.A.NguyenandC.Csallner.2015.Reverseengineeringmobileapplicationuserinterfaceswithremaui(t).In2015
30thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE).248â€“259.
[41] TuanAnhNguyenandChristophCsallner.2015.Reverseengineeringmobileapplicationuserinterfaceswithremaui
(t).In201530thIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineering(ASE).IEEE,248â€“259.
[42] ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HaiquanWang,YingboZhou,SilvioSavarese,andCaimingXiong.
2022. CodeGen:AnOpenLargeLanguageModelforCodewithMulti-TurnProgramSynthesis.InInternationalInteraction2Code:HowFarAreWeFromAutomaticInteractiveWebpageGeneration? 21
ConferenceonLearningRepresentations. https://api.semanticscholar.org/CorpusID:252668917
[43] OpenAI.2024.HelloGPT-4o. https://openai.com/index/hello-gpt-4o/Accessed:2024-10-06.
[44] OpenAI.2024.VisionGuide. https://platform.openai.com/docs/guides/visionAccessed:2024-10-18.
[45] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. Bleu:amethodforautomaticevaluationof
machinetranslation.InProceedingsofthe40thannualmeetingoftheAssociationforComputationalLinguistics.311â€“318.
[46] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,Amanda
Askell,PamelaMishkin,JackClark,etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning.PMLR,8748â€“8763.
[47] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.2019.Languagemodelsare
unsupervisedmultitasklearners.OpenAIblog1,8(2019),9.
[48] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
PeterJLiu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.Journalofmachine
learningresearch21,140(2020),1â€“67.
[49] ChengleiSi,YanzheZhang,ZhengyuanYang,RuiboLiu,andDiyiYang.2024.Design2Code:HowFarAreWeFrom
AutomatingFront-EndEngineering?arXivpreprintarXiv:2403.03163(2024).
[50] MariaTsimpoukelli,JacobLMenick,SerkanCabi,SMEslami,OriolVinyals,andFelixHill.2021.Multimodalfew-shot
learningwithfrozenlanguagemodels.AdvancesinNeuralInformationProcessingSystems34(2021),200â€“212.
[51] YuxuanWan,ChaozhengWang,YiDong,WenxuanWang,ShuqingLi,YintongHuo,andMichaelRLyu.2024.Automat-
icallyGeneratingUICodefromScreenshot:ADivide-and-Conquer-BasedApproach.arXivpreprintarXiv:2406.16386
(2024).
[52] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSimoncelli.2004. Imagequalityassessment:fromerror
visibilitytostructuralsimilarity.IEEEtransactionsonimageprocessing13,4(2004),600â€“612.
[53] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.2022.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.Advancesinneuralinformationprocessing
systems35(2022),24824â€“24837.
[54] Y.Xu,L.Bo,X.Sun,B.Li,J.Jiang,andW.Zhou.2021. image2emmet:Automaticcodegenerationfromwebuser
interfaceimage.JournalofSoftware:EvolutionandProcess33,8(2021),e2369.
[55] ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,andLijuanWang.2023.TheDawn
ofLMMs:PreliminaryExplorationswithGPT-4V(ision).ArXivabs/2309.17421(2023). https://api.semanticscholar.org/
CorpusID:263310951
[56] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen.2023.ASurveyonMultimodal
LargeLanguageModels.ArXivabs/2306.13549(2023). https://api.semanticscholar.org/CorpusID:259243718
[57] HaoYu,BoShen,DezhiRan,JiaxinZhang,QiZhang,YuMa,GuangtaiLiang,YingLi,TaoXie,andQianxiangWang.
2023.CoderEval:ABenchmarkofPragmaticCodeGenerationwithGenerativePre-trainedModels.InInternational
ConferenceonSoftwareEngineering. https://api.semanticscholar.org/CorpusID:256459413
[58] SukminYun,HaokunLin,RusiruThushara,MohammadQazimBhat,YongxinWang,ZutaoJiang,MingkaiDeng,
JinhongWang,TianhuaTao,JunboLi,etal.2024.Web2Code:ALarge-scaleWebpage-to-CodeDatasetandEvaluation
FrameworkforMultimodalLLMs.arXivpreprintarXiv:2406.20098(2024).
[59] ClemensZeidler,ChristofLutteroth,WolfgangStuerzlinger,andGeraldWeber.2013.EvaluatingDirectManipulation
OperationsforConstraint-BasedLayout.InIFIPTC13InternationalConferenceonHuman-ComputerInteraction.
https://api.semanticscholar.org/CorpusID:8243987
[60] JiaruiZhang,JinyiHu,MahyarKhayatkhoei,FilipIlievski,andMaosongSun.2024.Exploringperceptuallimitationof
multimodallargelanguagemodels.arXivpreprintarXiv:2402.07384(2024).
[61] TingZhou,YanjieZhao,XinyiHou,XiaoyuSun,KaiChen,andHaoyuWang.2024.BridgingDesignandDevelopment
withAutomatedDeclarativeUICodeGeneration.arXivpreprintarXiv:2409.11667(2024).