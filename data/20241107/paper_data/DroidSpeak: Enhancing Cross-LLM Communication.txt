DroidSpeak: Enhancing cross-LLM
communication
YuhanLiu2 EshaChoukse1 ShanLu1 JunchenJiang2 MadanMusuvathi1
1Microsoft 2UniversityofChicago
1 ABSTRACT
User Prompt User Prompt
Inmulti-agentsystemsutilizingLargeLanguageMod-
els(LLMs),communicationbetweenagentstradition- Product Manager Agent Actor Agent
allyreliesonnaturallanguage.Thiscommunicationof- Accept
Evaluator Agent
tenincludesthefullcontextofthequerysofar,which
Architect Agent Decline
can introduce significant prefill-phase latency, espe- Self-reflection Output
ciallywithlongcontexts. Agent
Project Manager Agent
WeintroduceDroidSpeak,anovelframeworktotar- (b) Reflexion
getthiscross-LLMcommunicationbyleveragingthe User Prompt
Engineer Agent
reuseofintermediatedata,suchasinputembeddings(E-
cache)andkey-valuecaches(KV-cache).Weefficiently Commander Agent Output
QA Engineer Agent
bypass the need to reprocess entire contexts for fine-
tuned versions of the same foundational model. This
Output Coder Agent Critic Agent
approachallowsfastercontextintegrationwhilemain-
(a) MetaGPT (c) AutoGenMulti-Modal
tainingthequalityoftaskperformance.Experimental
Figure1:ExampleLLMagentworkflows:(a)MetaGPT[20]:
evaluationsdemonstrateDroidSpeak‚Äôsabilitytosignifi-
multipleagents,eachbeingaroleinasoftwaredevelopment
cantlyaccelerateinter-agentcommunication,achieving
company, collaborate to accomplish a complex coding task;
uptoa2.78√óspeedupinprefilllatencywithnegligible
(b)Reflexion[48]:theActoragentfirstgeneratesatrajectory,
loss in accuracy. Our findings underscore the poten-
sendstotheEvaluatoragentwhichgivesarewardscore,and
tial to create more efficient and scalable multi-agent
Self-reflection agent generates feedback for Actor agent for
systems.
improvement; (c) AutoGen multi-modal workflow [36]: the
commanderagentsendstheplottingrequesttothecoder,who
works with the critic agent to write the plotting script and
2 INTRODUCTION
improvetheplottedfigure.
LargeLanguageModels(LLMs)havetransformedthe
landscape of AI-driven applications, enabling a wide suchsystems,agentsoftenneedtoexchangeinforma-
rangeofadvancedcapabilities,fromnaturallanguage tion,aligncontexts,andrefinesharedgoals,formingin-
understanding to complex task automation [38, 50]. tricateworkflowsthatspanmultipleroundsofcommu-
These LLMs are no longer restricted to single-agent nication.Examplesincludecustomerserviceautoma-
operations; instead, they are increasingly utilized in tion,collaborativecontentcreation,andevensophisti-
multi-agent systems where specialized agents collab- catedsimulationsinsocialsciences.Figure1showsa
oratetoachievecomplexobjectives[23,41,48,54].In fewsuchscenarios.
TraditionalcommunicationbetweenLLMagentsre-
liesheavilyonnaturallanguageexchanges,emulating
human-likedialogue,asshowninFigure1.Whilethis
approachisintuitiveandensurescoherence,italsoin-
troducessignificantoverhead.LLMinferenceincludes
YuhanLiuisaffiliatedwiththeUniversityofChicago,butwasa a prefill phase, to understand the input, and decode
Microsoftinternduringthiswork. phase, to serially generate the output [2, 43]. Prefill
1
4202
voN
5
]AM.sc[
1v02820.1142:viXraphasetendstobemuchmorecompute-intensivethan
E
the decode phase [2, 43]. As agents collaborate over E
extensive interactions, especially in long-running or
Transformer Q proj K proj V proj
contextuallydensedialogues,theprefillphaselatency
Layer # 1
candominatetheoverallcommunicationtime,hinder- Q K V
ingreal-timeresponsivenessandscalability.
Transformer
Toaddressthesechallenges,weproposeDroidSpeak, Layer # 2 Self-Attention
anovelcommunicationprotocolthatstreamlinesLLM ( Q √ó K ) √ó V
‚Ä¶.
inter-agent exchanges among finetuned LLMs of the
samefoundationalLLM.Wenotethattheprefillphase
bottleneck is caused due to each receiver-LLM need- Figure2: Illustrationoftheembedding(E),query(Q),
ing to translate natural language back to the context key(K),andvalue(V)tensorsinTransformer-basedLLMs.
the sender-LLM already had. On the other hand, we
3 BACKGROUND & MOTIVATION
diveintothesimilaritiesanddifferencesbetweenfine-
tunedLLMsbasedonacommonfoundationalmodel.
3.1 Transformer Basics
WeshowthateachlayeracrosstheseLLMshasadiffer-
Transformers[4,51]arethestandardarchitecturefor
entdegreeofsimilarityandadifferentdegreeofimpact
most generative language models, powering a broad
onthefinaloutputaccuracy.Weexploitthisbycom-
range of applications in AI-driven text generation. A
municating across LLMs using the intermediate data
transformermodelprocessesaninputsequenceofto-
selectively per layer. We utilize the high bandwidth
kens‚Äîunitsthatcanrepresentwords,punctuation,or
interconnects across GPU nodes in cloud systems to-
wordsegments‚Äîandgeneratesanoutputsequencein
day[10,11],tooffsetthepreciouscomputationaltime
twoprimaryphases:prefillanddecoding.
and resources of the GPUs themselves. This strategy
Duringtheprefillphase,themodeltakesinthein-
circumvents the need for complete context regenera-
puttokens,whereeachlayerinthemodel‚Äôsattention
tion,enablingfasterintegrationwithoutsacrificingthe
mechanismprocessesanembedding(E)tensortopro-
qualityofoutputs.Byselectivelyreusingandrecalcu-
ducetwothree-dimensionaltensors:key(K)andvalue
latingonlywhatisnecessary,DroidSpeaksignificantly
(V)tensors(Figure2).TheseKandVtensorscontain
reducesprefilldelayswhilemaintaininghightaskper-
criticalcontextinformationforlaterusebythemodel
formancestandards.
andarecollectivelyreferredtoastheKVcache.
Wemakethefollowingcontributionsinthispaper.
Inthedecodephase,ortokengenerationphase,the
‚Ä¢ WeidentifyagrowingchallengeinLLMagentwork-
KVcacheisusedtocalculateattentionscoresbetween
flows,ofrepeatedcontextprefilldelaysduetocom-
token pairs, producing an attention matrix that sup-
munication.
portsautoregressivetokengeneration.
‚Ä¢ Wepresentananalysisofsimilaritiesanddifferences Theprefillphaseiscomputationallymoredemand-
betweentwoLLMsthatshareacommonfoundational
ingthanthedecodingphase,withitscomplexityscal-
model,andshowthateachlayerhasadifferentlevel
ingsuper-linearlywithinputlength,whereasdecoding
ofimpactonaccuracy.
complexitygrowslinearly.Thisseparationofcompu-
‚Ä¢ We design DroidSpeak to selectively reuse and re- tationaldemandsunderscorestheneedforoptimized
computetheintermediatedataofsender-LLMatthe handlingofeachphase[2,43].
receiver-LLMtominimizecommunicationoverheads.
‚Ä¢ Usingacomprehensivemethodologyanddataset,we 3.2 LLM Agents in Specialized Tasks
evaluateDroidSpeakonavarietyofmodel-pairsand
Whiletraditionalchatbotsgenerategenericresponses,
datasetstoshowthatwecanachieveuptoa2.78√ó
LLMagentsarespecificallydesignedtotacklecomplex,
speedupinprefilllatencywithnegligibleimpacton
specializedtasks.Inmulti-agentsetups,eachagentcon-
accuracy.
tributes to a subset of the overall task, resulting in a
collaborativesystemofspecializedagentsthatcollec-
tivelyachievesophisticatedgoals.
2comparedtoarelativelyshort0.21secondsfordecoding.
Long output of sender Such discrepancies highlight the prefill phase‚Äôs high
costandillustratetheinefficiencyofconventionalinter-
Sender Agent Receiver Agent
Prefilling history
Prefilling history (including output of agentcommunication.
and input sender)and input
Generating Long output Generating output
Time Time
Figure3: Prefillinglongoutputfromthesenderagent
causeslongprefilldelayonthereceiveragent‚Äôsside. 3.4 Case Studies on Long Outputs
Thesemulti-agentsystemsarebeingappliedacross and Prefill Delays
variousdomains,including:
Incomplexmulti-agentsystems,anagent‚Äôsoutputcan
‚Ä¢ CollaborativeGaming:Agentsrepresentindividual besubstantial,leadingtolongprefilldelaysforthenext
gamecharacters,workingtogethertoachievecom- agent.
plexin-gameobjectives[17,33,60,61]. MapCoder [25]: In tackling competitive program-
‚Ä¢ ContentCreation:Oneagentgeneratescontent,while ming challenges, a retrieval agent identifies related
anotherrefinesitforcreativefieldssuchasmarketing problems,whicharethenpassedtoaplanningagentto
andwriting[12,27]. generatestep-by-stepsolutions.Next,acodingagent
‚Ä¢ CodingAssistance:Programmeragentswritecode, translates these steps into executable code, which is
testing agents generate test cases, and debugging testedanddebuggediteratively.Insuchcascadingsys-
agents iterate on the code until it meets specifica- tems,thepreviousagent‚Äôsoutputcanreachupto38,000
tions.Advancedsetupsmimicrolesinsoftwaredevel- tokens,causingsubstantialprefilldelays.
opmentteams,whereagentsassumerolesofproduct Reflexion[48]:Here,anactoragentinteractswith
managers, engineers, and QA testers to collabora- anenvironmenttoproduceatrajectory,whichisscored
tivelycompletecomplexdevelopmenttasks[19,20, byanevaluatoragent.Aself-reflectionagentthenana-
22,44,49]. lyzesthistrajectoryandprovidesfeedbacktotheactor.
Thissequencecontinuesuntiltheevaluatorissatisfied,
‚Ä¢ SocialSimulation:Agentswithuniqueprofilessimu-
buteachstep‚Äôslongoutputsleadtosignificantprefill
latesocieties,providingrichbehavioraldataforsocial
delaysforsubsequentagents.
scienceresearch[14,40,42].
Thesubstantialdelaysinthesescenariosstemfrom
eachagentrepeatedlyprefillingthesamecontext.To
3.3 Challenges in Inter-Agent
address these challenges, we aim to explore a more
Communication
efficient, context-preserving communication method
Inmulti-agentLLMsystems,agentsoftenneedtoshare thatreducesredundantprefillcosts,enablingfasterand
intermediateoutputsforthetasktoprogress.Forexam- morestreamlinedinter-agentexchanges.
ple,inMetaGPT[20],eachtaskpassesthroughmultiple
agents(e.g.,ProductManager,Architect,Engineer),and
eachagentsendstheiroutputtothenext,whoprocesses
4 CHARACTERIZATION AND
itfurthertorefinethefinalproduct.
Traditionally,agentscommunicatethroughnatural OPPORTUNITIES
language.However,asshowninFigure3,eachtimea
Previousworkhasexploredreducingasinglemodel‚Äôs
sender agent completes its output, it passes this data
prefilldelaybyreusingintermediateresultsgenerated
andconversationhistorytothereceivingagent.This
duringtheprefillphase,suchastheKVcache,across
requiresthereceivingagenttoreprocessallpriorcon-
requeststothesamemodel.Buildingonthisidea,we
text during its prefill phase. The super-linear scaling
envisionanimproved‚Äúlanguage‚Äùforagentcommuni-
ofprefilllatencywithinputlengthcanresultinsignifi-
cation, where agents leverage these intermediate re-
cantdelays,particularlyinconversationswhereagents
sults. This could allow the receiving agent to utilize
interactiteratively,leadingtorepeatedprefilloverhead.
thesender‚Äôsprecomputeddata,minimizingredundant
Forexample,intheHotpotQAdataset[3],theaverage
computationandacceleratingresponsetimes.
prefilldelayforLlama-3-70B-Instructis2.16seconds,
3Receiver Model Sender Model
80 80 80 80
60 60 60 60
40 40 40 40
20 20 20 20
0 0 0 0
H otp otQ A N arrativ e Q A G o v R e p ort H otp otQ A M ultifield Q A N arrativ e Q A H otp otQ A M ultifield Q A M ulti_ N e w s M ath DatasetG S M 8 k
Dataset Dataset Dataset Llama-3-8B
Mistral-7B v.s. Mistrallite Llama-3-8B Llama-3-70B v.s. MAmmoTH2
v.s. Llama-3-8B-Instruct v.s. Llama-3-70B-Instruct
Figure4: Thegenerationqualityoffine-tunedmodelsandbasemodels.
4.2 Quality
SenderModel ReceiverModel Dataset
Thegenerationqualityoffine-tunedLLMscanbesignif-
HotpotQA[3]
icantlyhigherthanthatofthebasemodels.InFigure4,
Llama-3-8B Llama-3-8B-Instruct NarrativeQA[3]
MultifieldQA[3] weevaluatethequalitymetricsforfourpairsofsender
LLMs and receiver LLMs across the datasets. We ob-
HotpotQA[3]
servethateventhoughthereceiverLLMsconsistently
Mistral-7B Mistrallite NarrativeQA[3]
outperformthesenderLLMsintermsofquality,their
GovReport[3]
modelweightsareactuallyquitesimilar.
HotpotQA[3]
Llama-3-70B Llama-3-70B-Instruct MultifieldQA[3]
4.3 Similarity
Govreport[3]
GPTA[59]
Llama-3-8B MAmmoTH2
GSM8K[59]
Table1:Themodelpairs,wheresendermodelisthebase 0.006
model,andreceivermodelisthefine-tunedmodelbased
0.004
onthereceivermodel,anddatasetsusedinourpaper.
0.002
0.000
4.1 Methodology lla m a-3-8 B mistral-7 B Lla m a-3-7 0 B M A m m o T H 2
Totesttheeffectivenessofreusingthesender‚Äôsinter-
Model pairs
mediateresultsonthereceiveragent,itisimportantto
Figure5: Theweightsofthebaseandfine-tunedLLMs
buildabenchmarkthatincludesLLMpairsanddatasets
differlittle.
foraccuracytesting.Tothebestofourknowledge,no
suchbenchmarkcurrentlyexists.Withoutcarefulcu- Weights. InFigure5,weplotthefractionofthedif-
ration,afine-tunedLLMmayperformevenworseon ferencebetweenthesenderandreceiverLLMweights
downstreamtasksthanthebasemodel,makingtheuse relativetotheoriginalsenderLLM‚Äôsweights.Itisevi-
ofthefine-tunedLLMunnecessaryforthesetasks. dentthatthisdifferencerepresentsonlyasmallfraction
Table1liststheLLMpairsanddatasetsinourbench- oftheoriginalweights.
mark.Themodelpairsanddatasetsareselectedsothat IntermediateData. InFigure6,weplotthefractionof
thefine-tunedLLMs(i.e.,receiverLLMs)performsig- thedifferencebetweentheKVorEcacheofthesender
nificantlybetteronthedatasetscomparedtothebase andreceiverLLMsrelativetotheoriginalsenderLLM‚Äôs
LLMs(i.e.,senderLLMs). KVorEcache.Althoughthedifferenceislargerthanthe
4
ytilauQ
mroN
2L
lanigirO
/
ESMKV cache E cache KV cache E cache
0.4
0.3
0.4 0.3
0.2 0.2
0.2
0.1
0.1
0.0 0.0
0.0 0 10 20 30 0 10 20 30
Layer Number Layer Number
lla m a-3-8 B mistral-7 B Lla m a-3-7 0 B M A m m o T H 2 Figure8: Mi Tstr hal e-7B f rv. as. c M ti is otra nllit oe fdiffer L ela nm ca e-3- b8B e tv w.s. eLl eam na- t3 h-8 eB- sIn es ntru dct er
andreceiverLLMs‚ÄôKVandEcacherelativetothesender
Model pairs
LLM‚ÄôsKVandEcachefordifferentlayers.
Figure 6: The KV cache and E cache of the base and
fine-tunedLLMsdifferlittle.
Mistrallite reusing Mistral-7B's KV
differenceinweights,itisstillasmallportioncompared Llama-3-8B-Instruct reusing Llama-3-8B's KV
totheoriginalsenderLLM‚ÄôsKVorEcache.
60
5 DESIGNING DROIDSPEAK 40
5.1 Naively reusing the KV cache
20
0
80 0.00 0.25 0.50 0.75
60 Prefill delay (s)
40 Figure9:PrefilldelayvsF1scorebyreusingKVcacheof
Fine-tuned Model
laterlayers.PlottedwithMistrallitereusingMistral-7B‚Äôs
20 Base Model
Reusing KV cache E,andLlama-3-8B-InstructreusingLlama-3-8B‚ÄôsE,on
0
HotpotQA NarrativeQA MultifieldQA
theHotpotQAdataset.
Dataset
Figure 7: Naively reusing Llama-3-8B‚Äôs KV cache on precludesaccesstoE.Therefore,welookatreusingcon-
Llama-3-8B-Instructgreatlydegradesgenerationquality. tiguouslayersofKVcacheuntilthelastlayer.Figure9
shows the results from this approach. Although the
A naive way of eliminating prefill delay on the re-
impactonaccuracyisnotbad,thedegreeoffreedom
ceiveragent‚Äôssideistoreuseallofthesenderagent‚Äôs
foroptimizationislow.
KVcache.Figure7showstheresultsofreusingLlama-
3-8B‚Äôs KV cache on Llama-3-8B-Instruct. We can see
5.2 Reusing Embedding Cache
thatnaivelyreusingalltheKVcachefromthesender
agent on the receiver agent greatly degrades its gen- Followinginsightsfrompriorwork[24]thatshowthat
eration quality to almost the same as the base model models are generally converging towards a common
beforefine-tuning. embeddingspace,weexploredreusingtheEcachefrom
To understand this, we further examine the differ- the base LLM on the fine-tuned LLM. Reusing the E
encesinintermediatedatabetweenthesenderandre- cachealsooffersgreaterflexibilityintermsofthelayer-
ceiverLLMs.InFigure8,weplottherelativedifferences positionofthedatathatcanbereused,unlikereusing
inKVandEcachebetweenthetwoLLMs,normalized KV cache. For example, if we reuse the KV cache for
bythesenderLLM‚ÄôsKVandEcache.Theresultsindi- thefirstthreelayersandthenswitchtorecomputation,
catethatthesedifferencesvaryacrosslayers,suggesting wecannotproceedbecausetheinputembedding(E)for
that layer-wise optimizations are required. Note that the4thlayerismissing.Thisissuecanberesolvedby
asshowninFigure2,eachlayer‚ÄôsKVcomputationre- reusingtheEcacheinstead.Inthiscase,whenwewant
quirestheEforthatlayer.However,reusingKVcaches toswitchtorecomputationatthe4thlayer,theinput
5
ytilauQ
mron
2L
lanigirO/ESM
mron
2L
lanigirO
/
ESM
erocS
1Ftheprefilldelayby1.8√ówhilemaintainingthegenera-
tionquality.Weobservesimilarsavingsforothermodel
pairs,infigure11.
5.3 Overheads of reusing E cache vs
reusing KV cache
AlthoughreusingEcacheprovidesgreatflexibilityin
termsofwhichlayerstoreuse,itcostsoverheadinGPU
memory,transmission,andcomputation.Considerthe
scenariowherethesenderandreceiverLLMsareplaced
ontwoGPUnodes,andtheyareinterconnectedwithan
Figure 10: Reusing E cache allows the exploration of
Infinibandlink.OnthesenderLLM‚Äôsside,theEcache
reusingbothearlierandlaterlayersofEcacheofMistral-
needstobestoredinGPUmemorybeforesendingitto
7B on Mistrallite model. Each cell in the heatmap rep-
thereceiverLLM‚Äôsside.Similarly,sendingtheEcache
resents reusing the E cache up to layer X (indicated on
throughthebandwidthlinkincursextratransmission
theX-axis)andstartingfromlayerY(indicatedonthe
delay.Finally,aftertheEcacheissenttothereceiver
Y-axis).Thewarmerthecolorinthecellis,thehigherthe
LLM‚Äôsside,anextraQKVprojectionoperation(¬ß3.1)is
generationqualityis.
requiredtoturntheEcacheintoKVcache,whichincurs
extra computation delay. These three types of delay
embedding(E)forthatlayerisstillavailable,allowing
grow linearly with respect to the number of reused
recomputationtobeginatthe4thlayer.
layers,asshowninfigure12.
Unlike reusing E cache which incurs memory and
delayoverheads,reusingKVcachedoesnotincurany
Mistrallite reusing Mistral-7B's E
oftheGPUmemoryoverheadorextratransmissionand
Llama-3-8B-Instruct reusing Llama-3-8B's E
computationdelay.However,aswehavediscussedin
Llama-3-70B-Instruct reusing Llama-3-70B's E
¬ß5.2,oncewestartreusingtheKVcacheforalayer,we
60
willlosetheopportunitytorecomputeforlaterlayers
duetothelackofembeddingE.
40
20
5.4 Reusing KV+E cache
0 1 2 OnesolutiontothelostEproblemwithKVcachereuse
Prefill Delay (s)
is: at the transition layer, where we want to switch
Figure 11: Pareto frontier curves for prefill delay vs
fromKVcachereusingtorecomputation,weloadthe
accuracybyreusingpartiallayersofEcache.Datashown
embeddingEfromthesenderLLM,thenweavoidthe
withmodelpairsontheHotpotQAdataset.
problemofmissingEthusabletorecomputeafterthat.
WerefertothisasKV+Ecachereuse.
SincereusingEcacheallowsittobereusedintermit-
Figure13comparesreusingtheEcachealoneversus
tently,itopensupatwo-dimensionalspaceofwhich
reusingKV+Ecachesintermsofprefilldelayandac-
layers to reuse. Figure 10 shows reusing the E cache
curacytradeoff.Forthethreemodelpairs,thetradeoff
uptolayerùëå (intheY-axis),andreusingtheEcache
betweendelayandaccuracyissimilarwhetherreusing
startingfromlayerùëã (intheX-axis).Thisshowsthat
bothcachesorjusttheEcache.
any combination ofùëã andùëå (ùëã ‚â§ùëå) is allowed by E
Therefore,reusingKV+Ecachesprovidessimilarben-
cachereusing.Anycombinationof(ùëã ‚â§ùëå)translatesto
efitsintermsofdelayandaccuracywithoutaddingGPU
prefilldelaysaving,asshowninfigure11(a).Eachpoint
memoryoverheadtothesenderLLMorcomputation
representsthemostoptimalaccuracyatanamountof
overheadtothereceiverLLM.
prefilldelay.WecanseethatreusingEcachereduces
6
erocS
1F30 30 30
20 20 20
10 10 10
0 0 0
0 1 2 0.000 0.025 0.050 0.075 0.100 0.00 0.05 0.10 0.15
GPU memory overhead (GB) Transmission Overhead (s) QKV projection delay (s)
Figure12: TheoverheadsofreusingEcache.
Reusing E cache Reusing KV and E cache
60
40
40
40
20 20
20
0 0 0
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0 1 2
Prefill Delay (s) Prefill Delay (s) Prefill Delay (s)
(a) Mistrallite reusing Mistral-7B's E (b) Llama-3-8B-Instruct (c) Llama-3-70B-Instruct
reusing Llama-3-8B's E reusing Llama-3-70B's E
Figure13: ComparingParetofrontiercurvesofprefilldelay-accuracyfromreusingEcacheandKV+Ecache.
5.5 End-to-end flow with the new KV cache for those layers that do not reuse
them.
DroidSpeak
Sender Receiver
6 EVALUATION
LLM LLM
6.1 Setup
Online
Profiler
stage Models WeevaluateDroidSpeakonfourpairsofLLMs,
Profiling Which layers
wherethesenderLLMsarebasemodelsthatthereceiver
dataset to reuse KV+E cache
modelsfine-tunedupon.Thesenderandreceivermod-
elsarelistedinTable1.Notethatthereceivermodels
Sender KV + E cache Receiver
are such that their accuracy on the datasets is better
Offline LLM LLM
thanthesendermodels.
stage
Datasets We evaluate DroidSpeak on six different
datasets with different tasks, including question an-
Figure14: End-to-endworkflowofDroidSpeak. swering,summarization,andmathreasoning,listedin
Table1.
Puttingtogether,theend-to-endsystemisshownin QualityMetrics Wemeasuregenerationqualityusing
Figure14,attheofflinestage,DroidSpeakfirstprofiles thestandardmetricofeachdataset[3,59].
whichlayerstoreuseforeachpairofLLMs,referred ‚Ä¢ F1 score: used to evaluate the model‚Äôs response on
to as reusing configuration, on an example profiling HotpotQA,NarrativeQA,andMultifieldQAdatasets.
dataset.Attheonlinestagewhenthesendercommuni- Itmeasurestheprobabilitythatthegeneratedanswer
cateswiththereceiverLLM,theKVandEcachewillbe matches the ground-truth answer of the question-
senttothereceiverLLM,basedonthereusingconfig- answeringtask.
uration.ThereceiverLLMthenpartiallyre-computes
7
fo
rebmuN
erocS
1F
sreyal
desuer
erocS
1F
erocS
1FDroidSpeak Full prefill Reusing E cache Reusing KV cache Original Base Model
40
20 50
20
20
10 Better Better 25 Better Better
0 0 0 0
0.0 0.2 0.4 0.6 0.0 0.5 1.0 1.5 0 1 2 0.00 0.02 0.04
Prefill Delay (s) Prefill Delay (s) Prefill Delay (s) Prefill Delay (s)
(a) Llama-3-8B on NarrativeQA (b) Mistral-7B on NarrativeQA (c) Llama-3-70B on HotpotQA (d) MAmmoTH2 on Math
Figure15: Generationqualityv.s.prefilldelayforLlama-3-8B,Mistrallite,Llama-3-70BandMAmmoTH2models.
‚Ä¢ Rougescore:usedtoevaluatethequalityofthesum- Figure15showsDroidSpeak‚ÄôsgainintermsofTTFT
marizationdataset,GovernmentReport,bycompar- comparedwiththebaselines,across4pairsofmodels.
ingtheoverlapofn-grams,wordsequences,orword We can see that compared to full prefill, DroidSpeak
pairsbetweenthegeneratedtextandreferencetexts. has1.69‚Äì2.77√óreductioninprefilldelay,withoutcom-
‚Ä¢ Accuracy:usedtoevaluatethemodel‚Äôsoutputonthe promisinggenerationquality.Comparedtoreusingall
MMLU-StemandGSM8Kdatasets.Theaccuracyis oftheEcacheorKVcache,althoughtheprefilldelayis
defined as whether the generated answer matches muchsmallerthanDroidSpeak,thegenerationquality
theground-truthanswer. degrades greatly compared to full prefill. Finally, we
alsoplottedthegenerationqualityofthebasemodel
Performancemetrics Weevaluatedthesystems‚Äôper-
(horizontaldottedline),DroidSpeak‚Äôsqualityismuch
formancemetricbymeasuringtheprefilldelay,which
higherthanit,indicatingthequalitylossofDroidSpeak
is the time from the arrival of the user query to the
ismarginalcomparedtothedifferencebetweenthebase
generationofthefirsttoken.Thisincludesthetimeto
andfine-tunedLLMs.Thecompletesetofresultsacross
loadKVandEcachefromtheremoteGPUserver,and
allthemodelpairsandbenchmarkdatasetsisshownin
theprefillcomputationtime.
Table2.DroidSpeakisabletoachievenear-optimal,and
Systemsevaluated WecompareDroidSpeakwiththe
insomecasesbetteraccuracy,withanotablespeedup
followingbaselines:
(upto2.78√ó).
‚Ä¢ Fullprefillofthereceivermodel:thereceivermodel
prefillsthetextofthecontext,whichrepresentsthe
7 DISCUSSION
baselineofthehighestcomputationoverheadbutthe
bestqualitywecanget. Whatisthepatternacrossmodelpairs? Inthissec-
tion,weexplorethepatternsofKVandEcachereusing
‚Ä¢ Reusing all the E cache of the sender model: the
acrossdifferentmodelpairs.Figure16showsthepat-
sendermodelsendsits Ecacheforallthelayers to
ternofreusingofthreedifferentmodelpairs(Mistral-
thereceivermodelandthereceivermodelrunsKV
litev.s.Mistral-7B,Llama-3-8B-Instructv.s.Llama-3-8B,
projection to get the KV cache. Then the receiver
Llama-3-70B-Instruct v.s. Llama-3-70B) by fixing the
modelrunsdecodingwiththegeneratedKVcache.
same dataset, HotpotQA. We can see that clearly dif-
‚Ä¢ Reusing all the KV cache of the sender model: the
ferentmodelpairsshowdifferentpatternsinreusing.
sender model sends its KV cache for all the layers
Specifically,fortheMistralliteandMistral-7Bpair,up
tothereceivermodel.Thenthereceivermodelruns
to15layersofKVandEcachecanbereusedwithout
decodingwiththetransferredKVcache.
affecting accuracy. For the Llama-3-8B pair, up to 13
Hardware WeusetwoNVIDIAA100GPUnodes,with layerscanbereusedwithoutlossofaccuracy,andfor
200GbpsInfinibandlinkconnected,tobenchmarkour
the Llama-3-70B pair, up to 40 layers can be reused
results. The sender LLM is hosted on one A100 GPU
withoutcompromisinggenerationquality.
node,andthereceiverLLMonanother.
Whatisthepatternacrossdifferentdatasetsfor
a model pair? Next, we explored how cache reuse
6.2 Overall Results
patterns change across different datasets while keep-
WeshowtheimprovementofDroidSpeakoverthebase- ing the model pair fixed (Mistrallite and Mistral-7B).
lines,asdescribedin¬ß6.1. Figure17showsthatingeneral,thepatternissimilar
8
)%(
erocs
1F
)%(
erocs
1F
)%(
erocs
1F
ycaruccASenderModel ReceiverModel Dataset ReceiverLLM‚Äôsaccuracy Ouraccuracy Prefillspeedup
HotPotQA 46.2 44.9 1.69x
Llama-3-8B Llama-3-8B-Instruct NarrativeQA 22.6 24.9 1.69x
Multifieldqa_en 38.1 36.9 1.69x
HotPotQA 62.9 60.8 1.77x
Mistral-7B Mistrallite NarrativeQA 31.6 29.9 1.78x
Gov_report 32.9 31 1.77x
Multifieldqa_en 49.8 53.5 1.73x
Llama-3-70B Llama-3-70B-Instruct HotpotQA 61.2 60.2 1.72x
Multi_news 21.8 21.9 1.73x
Math 34.6 35.2 2.77x
Llama-3-8B MAmmoTH2
GSM8k 67.6 67.2 2.78x
Table2:CompletesetofaccuracyandprefillspeedupresultswithDroidspeak.
60
40
40
40
20 20 20
0 0 0
0 10 20 30 0 10 20 30 0 20 40 60 80
Number of reused layers Number of reused layers Number of reused layers
(a) Mistrallite reusing (b) Llama-3-8B-Instruct reusing (c) Llama-3-70B-Instruct reusing
Mistral-7B's KV and E cache Llama-3-8B's KV and E cache Llama-3-70B's KV and E cache
Figure16: Numberofreusedlayersv.s.F1scoreforthreemodelpairsontheHotpotQAdataset.
seethatfordifferenttokensinallthelayershavevery
60
differentcorrelationscores,leadingtoalargevariance.
HotpotQA Thissuggeststhatoptimizationsadaptivetodifferent
40
NarrativeQA tokenscouldbedeveloped.Wedonotcoverthisinthe
Gov Report
20 current version of the paper, and we leave this as a
futurework.
0
0.00 0.25 0.50 0.75 1.00
Prefill delay (s) 8 RELATED WORK
Figure 17: Number of reused layers v.s. F1 score for
MistralliteandMistral-7Bmodelpaironthreedatasets. Multi-agentsystems Recentstudiesproposemulti-
agentsystemsinfieldssuchascodingassistance[6,18,
forthethreedifferentdatasetsforthesamemodelpair. 20,23,25,44,49],gaming[1,7,17,33,37,60,61],and
This suggests that the layer reuse configuration (i.e., socialsimulation[17,40,42],amongothers.Recentre-
howmanylayerstoreuse)determinedononedataset searchhighlightsthatusingfine-tunedLLMsasagents
canbegeneralizedtoothersforthesamemodelpair. can improve generation quality in areas like general
Whatisthepatternacrossdifferenttokens? Fur- questionanswering[5,9,58,64],toollearning[15,45],
thermore,weaskthequestion:doesthedifferencein personalization[31],andcodingtasks[52].DroidSpeak
embedding or KV cache between the sender and re- focusesonoptimizingcommunicationdelaybetween
ceiverLLMdiffersimilarlylargefordifferenttokens? two LLM agents when one is a fine-tuned version of
WeusePearson‚Äôscorrelationcoefficientasthemetricto theother.
quantifythedifferenceintheembeddingsorKVcache. Faster LLM serving systems Recent LLM systems
Figure18showsthecorrelationcoefficientfordifferent research focuses on improving the serving through-
layers, where each group of violin plots contains the putorinferencedelay.Onelineofworkdesignbetter
correlationscoresforalltokensinthatlayer.Wecan schedulingpoliciesofinferencerequests[2,28,32,35,
9
erocS
1F
erocS
1F
erocS
1F
erocS
1F1.0
0.5
0.0
0 5 10 15 20 25 30
Layer Number
Figure 18: Each bar in the violin plot represents the correlation coefficient between Llama-3-8B and Llama-3-8B-
Instruct‚Äôs E cache, across all tokens for a specific layer number. A higher correlation coefficient indicates a greater
similaritybetweentheEcacheofthetwomodels.
43,47,63],onelineofworkproposeefficientmemory that slow inter-agent exchanges, without impacting
management for serving LoRA models [8, 30, 46, 53], thequality.Theexperimentsconductedacrossvarious
andanotherlineofworkdesignsmartcachingpolicy datasetsandmodelpairsaffirmDroidSpeak‚Äôsefficacy
for KV cache offloading [13, 21, 26, 29]. DroidSpeak androbustnessinpreservinggenerationqualitywhile
reduces the inference latency of serving multi-agent optimizinglatency,underscoringitspotentialasascal-
systemrequestswhenoneagentisafine-tunedversion ablesolutionformulti-agentLLMsetups.
ofanother. BuildingonthepromisingoutcomesofDroidSpeak,
KVcachecompressionandblending Manyprior severalavenuesforfutureresearchremainopen.First,
worksfocusoncompressingtheKVcachetomakeits while DroidSpeak has shown considerable efficiency
GPUmemoryusage[29,39,55,56,62]ortransmission improvements for LLMs sharing a base model (fine-
delaysmaller[34].DroidSpeakisorthogonaltothisline tunedversionsofafoundationalmodel),thereisneed
of work. Theirproposed compression algorithms are toexpandthisworktoincludemodelsthatdifferfrom
applicableonDroidSpeak,reducingthesizeoftheKV eachotherinshapeandsizetoo.Anotherprospective
cachetoreducethecommunicationdelay. researchdirectioninvolvesintegratingadvancedcom-
Another line of research reduces the prefill delay pressiontechniquesintotheKVandEcachestomini-
when blending non-prefix KV caches from multiple mizetransmissiondelaysfurther.Thereisalsopoten-
differentcontexts[16,57].Thisapproachcouldbeap- tialforfurtheroptimizationsbyincorporatingadaptive
pliedinscenarioswherethesenderandreceiveragents cachingmechanisms.Thesemechanismscoulddynam-
are instances of the same LLM with different system icallyadjustreusestrategiesbasedonreal-timelatency
prompts. In contrast, DroidSpeak addresses the case constraintsandresourceavailability.
wherethesenderandreceiveragentssharethesame
foundationalmodel,butthereceiverisfine-tunedbased
onthesenderagent.
9 CONCLUSION AND FUTURE
WORK
Inthiswork,weintroducedDroidSpeak,anovelframe-
workdesignedtostreamlineinter-agentcommunica-
tionamongLLMsthatshareabasemodelbyleveraging
intermediatedatareusability,specificallyfocusingon
KVandEcaches.OurresultsdemonstratethatDroid-
Speakcansignificantlyreduceprefilllatency,achieving
up to a 2.78√ó improvement with minimal impact on
outputquality.Byenablingselectivereuseofinterme-
diaterepresentations,weaddresstheprefillbottlenecks
10
tneiciffeoC
noitalerroCREFERENCES [16] InGim,GuojunChen,SeungseobLee,NikhilSarda,Anurag
Khandelwal,andLinZhong.Promptcache:Modularattention
[1] SaaketAgashe,YueFan,AnthonyReyna,andXinEricWang.
reuseforlow-latencyinference,2024.
Llm-coordination:Evaluatingandanalyzingmulti-agentcoor-
[17] RanGong,QiuyuanHuang,XiaojianMa,HoiVo,ZaneDu-
dinationabilitiesinlargelanguagemodels,2024.
rante,YusukeNoda,ZilongZheng,Song-ChunZhu,Demetri
[2] AmeyAgrawal,NitinKedia,AshishPanwar,JayashreeMohan,
Terzopoulos,LiFei-Fei,andJianfengGao. Mindagent:Emer-
NipunKwatra,BhargavS.Gulavani,AlexeyTumanov,and
gentgaminginteraction,2023.
RamachandranRamjee. Tamingthroughput-latencytradeoff
[18] SamuelHolt,MaxRuizLuyten,andMihaelavanderSchaar.
inllminferencewithsarathi-serve,2024.
L2mac:Largelanguagemodelautomaticcomputerforexten-
[3] YushiBai,XinLv,JiajieZhang,HongchangLyu,JiankaiTang,
sivecodegeneration,2024.
ZhidianHuang,ZhengxiaoDu,XiaoLiu,AohanZeng,LeiHou,
[19] SiruiHong,YizhangLin,BangLiu,BangbangLiu,BinhaoWu,
YuxiaoDong,JieTang,andJuanziLi. Longbench:Abilingual,
DanyangLi,JiaqiChen,JiayiZhang,JinlinWang,LiZhang,
multitaskbenchmarkforlongcontextunderstanding,2024.
LingyaoZhang,MinYang,MingchenZhuge,TaichengGuo,
[4] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,
TuoZhou,WeiTao,WenyiWang,XiangruTang,Xiangtao
JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,Pranav
Lu,XiawuZheng,XinbingLiang,YayingFei,YuhengCheng,
Shyam,GirishSastry,AmandaAskell,SandhiniAgarwal,Ariel
ZongzeXu,andChenglinWu. Datainterpreter:Anllmagent
Herbert-Voss,GretchenKrueger,TomHenighan,RewonChild,
fordatascience,2024.
AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,
[20] SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,
ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,
YuhengCheng,JinlinWang,CeyaoZhang,ZiliWang,Steven
ScottGray,BenjaminChess,JackClark,ChristopherBerner,
KaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,Lingfeng
Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Xiao,ChenglinWu,andJ√ºrgenSchmidhuber.MetaGPT:Meta
Amodei. LanguageModelsareFew-ShotLearners,2020.
programmingforamulti-agentcollaborativeframework. In
[5] BaianChen,ChangShu,EhsanShareghi,NigelCollier,Karthik
Narasimhan,andShunyuYao.Fireact:Towardlanguageagent TheTwelfthInternationalConferenceonLearningRepresenta-
fine-tuning,2023.
tions,2024.
[21] CunchenHu,HeyangHuang,JunhaoHu,JiangXu,Xusheng
[6] DongChen,ShaoxinLin,MuhanZeng,DaoguangZan,Jian-
Chen,TaoXie,ChenxiWang,SaWang,YungangBao,Ninghui
Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang
Sun,andYizhouShan. Memserve:Contextcachingfordisag-
Dong,ArtemAliev,JieWang,XiaoCheng,GuangtaiLiang,
gregatedllmservingwithelasticmemorypool,2024.
YuchiMa,PanBian,TaoXie,andQianxiangWang. Coder:
[22] DongHuang,QingwenBu,JieMZhang,MichaelLuck,and
Issueresolvingwithmulti-agentandtaskgraphs,2024.
Heming Cui. Agentcoder: Multi-agent-based code genera-
[7] JiaqiChen,YuxianJiang,JiachenLu,andLiZhang. S-agents:
Self-organizingagentsinopen-endedenvironments,2024. tionwithiterativetestingandoptimisation. arXivpreprint
[8] LequnChen,ZihaoYe,YongjiWu,DanyangZhuo,LuisCeze,
arXiv:2312.13010,2023.
[23] DongHuang,JieM.Zhang,MichaelLuck,QingwenBu,Yuhao
andArvindKrishnamurthy.Punica:Multi-tenantloraserving,
Qing,andHemingCui. Agentcoder:Multi-agent-basedcode
2023.
generationwithiterativetestingandoptimisation,2024.
[9] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang,
[24] MinyoungHuh,BrianCheung,TongzhouWang,andPhillip
JiangningLiu,DahuaLin,KaiChen,andFengZhao. Agent-
Isola. Theplatonicrepresentationhypothesis,2024.
flan:Designingdataandmethodsofeffectiveagenttuningfor
[25] MdAshrafulIslam,MohammedEunusAli,andMdRizwan
largelanguagemodels,2024.
Parvez. Mapcoder:Multi-agentcodegenerationforcompeti-
[10] MicrosoftCorporation. Ndh100v5series-azurevirtualma-
chines,2024. Accessed:2024-10-31.
tiveproblemsolving. arXivpreprintarXiv:2405.11403,2024.
[26] ChaoJin,ZiliZhang,XuanlinJiang,FangyueLiu,XinLiu,
[11] NVIDIACorporation. Acceleratedinfinibandsolutionsfor
Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge
hpc,2024. Accessed:2024-10-31.
cachingforretrieval-augmentedgeneration,2024.
[12] TiannanWangetal. Weaver:Foundationmodelsforcreative
[27] PrathameshKhade. Multi-agentsystemforcontentcreation,
writing,2024.
2023. Accessed:2024-10-13.
[13] BinGao,ZhuominHe,PuruSharma,QingxuanKang,Djordje
[28] WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,Lian-
Jevdjic,JunboDeng,XingkunYang,ZhouYu,andPengfeiZuo.
minZheng,CodyHaoYu,JosephE.Gonzalez,HaoZhang,and
Cost-efficient large language model serving for multi-turn
IonStoica. Efficientmemorymanagementforlargelanguage
conversationswithcachedattention,2024.
modelservingwithpagedattention,2023.
[14] ChenGao,XiaochongLan,ZhihongLu,JinzhuMao,Jinghua
[29] WonbeomLee,JungiLee,JunghwanSeo,andJaewoongSim.
Piao, Huandong Wang, Depeng Jin, and Yong Li. S3:
InfiniGen: Efficient generative inference of large language
Social-networksimulationsystemwithlargelanguagemodel-
empoweredagents. arXivpreprintarXiv:2307.14984,2023.
modelswithdynamicKVcachemanagement. In18thUSENIX
[15] SilinGao,JaneDwivedi-Yu,PingYu,XiaoqingEllenTan,Ra- SymposiumonOperatingSystemsDesignandImplementation
makanthPasunuru,OlgaGolovneva,KoustuvSinha,AsliCe-
(OSDI24),pages155‚Äì172,SantaClara,CA,July2024.USENIX
Association.
likyilmaz,AntoineBosselut,andTianluWang. Efficienttool
usewithchain-of-abstractionreasoning,2024.
11[30] SuyiLi,HanfengLu,TianyuanWu,MinchenYu,QizhenWeng, ScottGray,RyanGreene,JoshuaGross,ShixiangShaneGu,
XushengChen,YizhouShan,BinhangYuan,andWeiWang. Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen
Caraserve:Cpu-assistedandrank-awareloraservingforgen- He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan
erativellminference,2024. Hickey,WadeHickey,PeterHoeschele,BrandonHoughton,
[31] YuanchunLi,HaoWen,WeijunWang,XiangyuLi,Yizhen KennyHsu,ShengliHu,XinHu,JoostHuizinga,Shantanu
Yuan,GuohongLiu,JiachengLiu,WenxingXu,XiangWang, Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,
YiSun,RuiKong,YileWang,HanfeiGeng,JianLuan,Xuefeng HaozhunJin,DennyJin,ShinoJomoto,BillieJonn,Heewoo
Jin,ZilongYe,GuanjingXiong,FanZhang,XiangLi,Mengwei Jun,TomerKaftan,≈ÅukaszKaiser,AliKamali,IngmarKan-
Xu,ZhijunLi,PengLi,YangLiu,Ya-QinZhang,andYunxinLiu. itscheider,NitishShirishKeskar,TabarakKhan,LoganKil-
Personalllmagents:Insightsandsurveyaboutthecapability, patrick,JongWookKim,ChristinaKim,YongjikKim,JanHen-
efficiencyandsecurity,2024. drik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
[32] ChaofanLin,ZhenhuaHan,ChengruidongZhang,Yuqing ≈ÅukaszKondraciuk,AndrewKondrich,ArisKonstantinidis,
Yang,FanYang,ChenChen,andLiliQiu. Parrot:Efficient KyleKosic,GretchenKrueger,VishalKuo,MichaelLampe,Ikai
servingofLLM-basedapplicationswithsemanticvariable. In Lan,TeddyLee,JanLeike,JadeLeung,DanielLevy,ChakMing
18th USENIX Symposium on Operating Systems Design and Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
Implementation(OSDI24),pages929‚Äì945,SantaClara,CA, TheresaLopez,RyanLowe,PatriciaLue,AnnaMakanju,Kim
July2024.USENIXAssociation. Malfacini, Sam Manning, Todor Markov, Yaniv Markovski,
[33] JijiaLiu,ChaoYu,JiaxuanGao,YuqingXie,QingminLiao, BiancaMartin,KatieMayer,AndrewMayne,BobMcGrew,
YiWu,andYuWang. Llm-poweredhierarchicallanguage ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,
agentforreal-timehuman-aicoordination,2024. JakeMcNeil,DavidMedina,AalokMehta,JacobMenick,Luke
[34] YuhanLiu,HanchenLi,YihuaCheng,SiddhantRay,Yuyang Metz,AndreyMishchenko,PamelaMishkin,VinnieMonaco,
Huang,QizhengZhang,KuntaiDu,JiayiYao,ShanLu,Ganesh EvanMorikawa,DanielMossing,TongMu,MiraMurati,Oleg
Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Murk, David M√©ly, Ashvin Nair, Reiichiro Nakano, Rajeev
Holtzman,andJunchenJiang. Cachegen:Kvcachecompres- Nayak,ArvindNeelakantan,RichardNgo,HyeonwooNoh,
sion and streaming for fast large language model serving, LongOuyang,CullenO‚ÄôKeefe,JakubPachocki,AlexPaino,
2024. JoePalermo,AshleyPantuliano,GiambattistaParascandolo,
[35] XupengMiao,ChunanShi,JiangfeiDuan,XiaoliXi,Dahua JoelParish,EmyParparita,AlexPassos,MikhailPavlov,An-
Lin,BinCui,andZhihaoJia. Spotserve:Servinggenerative drew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
largelanguagemodelsonpreemptibleinstances,2023. MichaelPetrov,HenriquePondedeOliveiraPinto,Michael,
[36] Microsoft. Engagingwithmultimodalmodels:Gpt-4vinauto- Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell,
gen,2024. Accessed:2024-10-25. AletheaPower,BorisPower,ElizabethProehl,RaulPuri,Alec
[37] ManuelMosquera,JuanSebastianPinzon,ManuelRios,Yesid Radford,JackRae,AdityaRamesh,CameronRaymond,Francis
Fonseca, Luis Felipe Giraldo, Nicanor Quijano, and Ruben Real,KendraRimbach,CarlRoss,BobRotsted,HenriRoussez,
Manrique.Canllm-augmentedautonomousagentscooperate?, NickRyder,MarioSaltarelli,TedSanders,ShibaniSanturkar,
anevaluationoftheircooperativecapabilitiesthroughmelting GirishSastry,HeatherSchmidt,DavidSchnurr,JohnSchulman,
pot,2024. DanielSelsam,KylaSheppard,TokiSherbakov,JessicaShieh,
[38] OpenAI,JoshAchiam,StevenAdler,SandhiniAgarwal,Lama SarahShoker,PranavShyam,SzymonSidor,EricSigler,Mad-
Ahmad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida, dieSimens,JordanSitkin,KatarinaSlama,IanSohl,Benjamin
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Sokolowsky,YangSong,NatalieStaudacher,FelipePetroski
Avila,IgorBabuschkin,SuchirBalaji,ValerieBalcom,PaulBal- Such,NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,
tescu,HaimingBao,MohammadBavarian,JeffBelgum,Irwan Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian,
Bello,JakeBerdine,GabrielBernadett-Shapiro,Christopher ElizabethTseng,PrestonTuggle,NickTurley,JerryTworek,
Berner,LennyBogdonoff,OlegBoiko,MadelaineBoyd,Anna- JuanFelipeCer√≥nUribe,AndreaVallone,ArunVijayvergiya,
LuisaBrakman,GregBrockman,TimBrooks,MilesBrundage, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin
Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Wang,BenWang,JonathanWard,JasonWei,CJWeinmann,
BrittanyCarey,ChelseaCarlson,RoryCarmichael,Brooke Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,
Chan,CheChang,FotisChantzis,DerekChen,SullyChen, MattWiethoff,DaveWillner,ClemensWinter,SamuelWol-
RubyChen,JasonChen,MarkChen,BenChess,ChesterCho, rich,HannahWong,LaurenWorkman,SherwinWu,JeffWu,
CaseyChu,HyungWonChung,DaveCummings,Jeremiah MichaelWu,KaiXiao,TaoXu,SarahYoo,KevinYu,Qiming
Currier,YunxingDai,CoryDecareaux,ThomasDegry,Noah Yuan,WojciechZaremba,RowanZellers,ChongZhang,Mar-
Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve vinZhang,ShengjiaZhao,TianhaoZheng,JuntangZhuang,
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna WilliamZhuk,andBarretZoph. Gpt-4technicalreport,2024.
Eloundou,DavidFarhi,LiamFedus,NikoFelix,Sim√≥nPosada [39] MatanelOren,MichaelHassid,NirYarden,YossiAdi,andRoy
Fishman,JustonForte,IsabellaFulford,LeoGao,ElieGeorges, Schwartz. Transformersaremulti-staternns,2024.
Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
RaphaGontijo-Lopes,JonathanGordon,MorganGrafstein,
12[40] Joon Sung Park, Joseph O‚ÄôBrien, Carrie Jun Cai, Mered- 24),pages911‚Äì927,SantaClara,CA,July2024.USENIXAsso-
ithRingelMorris,PercyLiang,andMichaelS.Bernstein.Gen- ciation.
erativeagents:Interactivesimulacraofhumanbehavior. In [54] QingyunWu,GaganBansal,JieyuZhang,YiranWu,BeibinLi,
Proceedingsofthe36thAnnualACMSymposiumonUserInter- ErkangZhu,LiJiang,XiaoyunZhang,ShaokunZhang,Jiale
faceSoftwareandTechnology,UIST‚Äô23,NewYork,NY,USA, Liu,AhmedHassanAwadallah,RyenWWhite,DougBurger,
2023.AssociationforComputingMachinery. andChiWang. Autogen:Enablingnext-genllmapplications
[41] Joon Sung Park, Joseph C. O‚ÄôBrien, Carrie J. Cai, Mered- viamulti-agentconversationframework. 2023.
ithRingelMorris,PercyLiang,andMichaelS.Bernstein.Gen- [55] GuangxuanXiao,JiamingTang,JingweiZuo,JunxianGuo,
erativeagents:Interactivesimulacraofhumanbehavior,2023. ShangYang,HaotianTang,YaoFu,andSongHan. Duoatten-
[42] JoonSungPark,LindsayPopowski,CarrieJunCai,Mered- tion:Efficientlong-contextllminferencewithretrievaland
ithRingelMorris,PercyLiang,andMichaelS.Bernstein.Social streamingheads,2024.
simulacra:Creatingpopulatedprototypesforsocialcomputing [56] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,and
systems. 2022. MikeLewis. Efficientstreaminglanguagemodelswithatten-
[43] PratyushPatel,EshaChoukse,ChaojieZhang,AashakaShah, tionsinks,2024.
√ç√±igoGoiri,SaeedMaleki,andRicardoBianchini. Splitwise: [57] JiayiYao,HanchenLi,YuhanLiu,SiddhantRay,YihuaCheng,
Efficientgenerativellminferenceusingphasesplitting,2024. Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang.
[44] ChenQian,WeiLiu,HongzhangLiu,NuoChen,YufanDang, Cacheblend:Fastlargelanguagemodelservingforragwith
JiahaoLi,ChengYang,WeizeChen,YushengSu,XinCong, cachedknowledgefusion,2024.
JuyuanXu,DahaiLi,ZhiyuanLiu,andMaosongSun.Chatdev: [58] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi
Communicativeagentsforsoftwaredevelopment,2024. Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin.
[45] WeizhouShen,ChenliangLi,HongzhanChen,MingYan,Xi- Agentlumos:Unifiedandmodulartrainingforopen-source
aojunQuan,HehongChen,JiZhang,andFeiHuang. Small languageagents. InLun-WeiKu,AndreMartins,andVivek
llmsareweaktoollearners:Amulti-llmagent,2024. Srikumar,editors,Proceedingsofthe62ndAnnualMeetingof
[46] YingSheng,ShiyiCao,DachengLi,ColemanHooper,Nicholas theAssociationforComputationalLinguistics(Volume1:Long
Lee,ShuoYang,ChristopherChou,BanghuaZhu,Lianmin Papers),pages12380‚Äì12403,Bangkok,Thailand,August2024.
Zheng,KurtKeutzer,JosephE.Gonzalez,andIonStoica.S-lora: AssociationforComputationalLinguistics.
Servingthousandsofconcurrentloraadapters,2024. [59] XiangYue,TuneyZheng,GeZhang,andWenhuChen. Mam-
[47] YingSheng,ShiyiCao,DachengLi,BanghuaZhu,ZhuohanLi, moth2:Scalinginstructionsfromtheweb,2024.
DanyangZhuo,JosephE.Gonzalez,andIonStoica. Fairness [60] CeyaoZhang,KaijieYang,SiyiHu,ZihaoWang,GuangheLi,
inservinglargelanguagemodels,2024. YihangSun,ChengZhang,ZhaoweiZhang,AnjiLiu,Song-
[48] Noah Shinn, Federico Cassano, Edward Berman, Ashwin ChunZhu,XiaojunChang,JungeZhang,FengYin,YitaoLiang,
Gopinath,KarthikNarasimhan,andShunyuYao. Reflexion: andYaodongYang. Proagent:Buildingproactivecooperative
Languageagentswithverbalreinforcementlearning,2023. agentswithlargelanguagemodels. ProceedingsoftheAAAI
[49] Microsoft AutoGen Team. Autogen 0.2 documentation - ConferenceonArtificialIntelligence,38(16):17591‚Äì17599,Mar.
agentchat auto feedback from code execution. https://mi 2024.
crosoft.github.io/autogen/0.2/docs/notebooks/agentch [61] HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,
at_auto_feedback_from_code_execution,2024. Accessed: YilunDu,JoshuaB.Tenenbaum,TianminShu,andChuang
2024-10-14. Gan. Buildingcooperativeembodiedagentsmodularlywith
[50] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar- largelanguagemodels,2024.
tinet,Marie-AnneLachaux,Timoth√©eLacroix,BaptisteRoz- [62] ZhenyuZhang,YingSheng,TianyiZhou,TianlongChen,Lian-
i√®re,NamanGoyal,EricHambro,FaisalAzhar,AurelienRo- minZheng,RuisiCai,ZhaoSong,YuandongTian,Christopher
driguez,ArmandJoulin,EdouardGrave,andGuillaumeLam- R√©,ClarkBarrett,ZhangyangWang,andBeidiChen. H o:
2
ple. Llama:Openandefficientfoundationlanguagemodels, Heavy-hitteroracleforefficientgenerativeinferenceoflarge
2023. languagemodels,2023.
[51] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- [63] YinminZhong,ShengyuLiu,JundaChen,JianboHu,Yibo
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Zhu,XuanzheLiu,XinJin,andHaoZhang. Distserve:Disag-
Polosukhin. Attentionisallyouneed,2023. gregatingprefillanddecodingforgoodput-optimizedlarge
[52] XingyaoWang,YangyiChen,LifanYuan,YizheZhang,Yun- languagemodelserving,2024.
zhuLi,HaoPeng,andHengJi. Executablecodeactionselicit [64] Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu
betterllmagents,2024. Zhang,ShiweiLyu,YueShen,LeiLiang,JinjieGu,andHua-
[53] BingyangWu,RuidongZhu,ZiliZhang,PengSun,Xuanzhe junChen. Knowagent:Knowledge-augmentedplanningfor
Liu,andXinJin. dLoRA:Dynamicallyorchestratingrequests llm-basedagents,2024.
andadaptersforLoRALLMserving. In18thUSENIXSympo-
siumonOperatingSystemsDesignandImplementation(OSDI
13