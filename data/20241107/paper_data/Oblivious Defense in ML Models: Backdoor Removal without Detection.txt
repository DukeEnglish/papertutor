Oblivious Defense in ML Models:
Backdoor Removal without Detection
ShafiGoldwasser JonathanShafer
UCBerkeley MIT
shafi@berkeley.edu shaferjo@mit.edu
NeekonVafa VinodVaikuntanathan
MIT MIT
nvafa@mit.edu vinodv@mit.edu
Abstract
As society grows more reliant on machine learning, ensuring the security of machine
learningsystemsagainstsophisticatedattacksbecomesapressingconcern. Arecentresult
ofGoldwasser, Kim, Vaikuntanathan, andZamir(2022)showsthatanadversarycanplant
undetectablebackdoorsinmachinelearningmodels,allowingtheadversarytocovertlycontrol
themodelâ€™sbehavior. Backdoorscanbeplantedinsuchawaythatthebackdooredmachine
learningmodeliscomputationallyindistinguishablefromanhonestmodelwithoutbackdoors.
Inthispaper,wepresentstrategiesfordefendingagainstbackdoorsinMLmodels,even
if they are undetectable. The key observation is that it is sometimes possible to provably
mitigateorevenremovebackdoorswithoutneedingtodetectthem,usingtechniquesinspired
by the notion of random self-reducibility. This depends on properties of the ground-truth
labels(chosenbynature),andnotoftheproposedMLmodel(whichmaybechosenbyan
attacker).
We give formal definitions for secure backdoor mitigation, and proceed to show two
typesofresults. First,weshowaâ€œglobalmitigationâ€technique,whichremovesallbackdoors
fromamachinelearningmodelundertheassumptionthattheground-truthlabelsareclose
to a Fourier-heavy function. Second, we consider distributions where the ground-truth
labels are close to a linear or polynomial function in â„ğ‘›. Here, we show â€œlocal mitigationâ€
techniques,whichremovebackdoorswithhighprobabilityforeveryinputsofinterest,and
arecomputationallycheaperthanglobalmitigation. Allofourconstructionsareblack-box,so
ourtechniquesworkwithoutneedingaccesstothemodelâ€™srepresentation(i.e.,itscodeor
parameters). Alongthewayweproveasimpleresultforrobustmeanestimation.
4202
voN
5
]GL.sc[
1v97230.1142:viXraContents
1 Introduction 1
1.1 ResearchQuestion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 OurContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 RelatedWorks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2 TechnicalOverview 6
2.1 Definitions: BackdoorMitigationandSecurity . . . . . . . . . . . . . . . . . . . . 7
2.1.1 Whyisthisnotionofsecuritysatisfactory? . . . . . . . . . . . . . . . . . 7
2.1.2 Efficiency,andLocalvs.GlobalMitigation . . . . . . . . . . . . . . . . . . 8
2.1.3 OntheAssumptionsinSecureBackdoorMitigation . . . . . . . . . . . . . 8
2.2 ConstructionsofSecureBackdoorMitigators . . . . . . . . . . . . . . . . . . . . . 9
2.2.1 GlobalMitigationforFourier-HeavyFunctions . . . . . . . . . . . . . . . 9
2.2.2 BasicLocalMitigationforLinearFunctions . . . . . . . . . . . . . . . . . 10
2.2.3 ImprovedLocalMitigationforLinearFunctions . . . . . . . . . . . . . . . 12
2.2.4 LocalMitigationforPolynomialFunctions . . . . . . . . . . . . . . . . . . 13
2.3 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3 Preliminaries 14
4 DefinitionsofSecureBackdoorMitigation 15
4.1 GeneralDefinitionofMitigationSecurity . . . . . . . . . . . . . . . . . . . . . . . 16
4.2 SpecificInstantiationsofMitigationSecurity . . . . . . . . . . . . . . . . . . . . . 17
4.3 TheCutoffDissimilarityFunction . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
4.3.1 SecurityImplicationsoftheCutoffDissimilarityFunction . . . . . . . . . 19
5 SomeInitialObservationsonSecureBackdoorMitigation 20
5.1 SecureBackdoorMitigationCannotbeDistributionFree . . . . . . . . . . . . . . 20
5.2 LowerBoundforGeneralMitigation . . . . . . . . . . . . . . . . . . . . . . . . . 21
6 GlobalMitigation 22
6.1 FourierAnalysisPreliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6.2 MitigationforFourier-HeavyFunctions . . . . . . . . . . . . . . . . . . . . . . . . 23
6.2.1 ProofofTheorem6.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
7 LocalMitigation 28
7.1 LocalMitigationPreliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
7.2 BasicMitigationforLinearFunctions . . . . . . . . . . . . . . . . . . . . . . . . . 29
7.2.1 CorrelatedSamplingLemma . . . . . . . . . . . . . . . . . . . . . . . . . . 30
7.2.2 ProofofTheorem7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
7.3 ImprovedMitigationforLinearFunctions . . . . . . . . . . . . . . . . . . . . . . . 36
7.3.1 ProofofTheorem7.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
7.4 MitigationforMultivariatePolynomials . . . . . . . . . . . . . . . . . . . . . . . . 418 RobustMeanEstimation 47
8.1 WhynotusethestandardMedian-of-Meansestimator? . . . . . . . . . . . . . . . 48
8.2 TheMean-of-Mediansestimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
8.2.1 ProofofTheorem8.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
9 FutureDirections 51
References 52
Appendices 57
A Goldreichâ€“LevinTheorem 57
B MiscellaneousFourierAnalysisResults 57
C Subgaussiandistributions 581 Introduction
1.1 Research Question
Asmachinelearning(ML)isincreasinglyintegratedintoeverydayproductsandservices,forcesof
economicspecializationfavoroutsourcingthedevelopmentofMLmodelstodedicatedcompanies
thatenjoyacomparativeadvantageinthatarea.
But outsourcing ML development comes with attendant security risks. A multitude of recent
empirical worksdemonstrates that amalicious developer could hidebackdoors1 in MLmodels,
allowingtheattackertocovertlycontrolthebehavioroftheMLmodel. Theseattacksarematched
byalargeliteratureproposingdefensivemeasuresthatcouldbeusedtodetectorremovebackdoors
insomecases. Theresultisanunresolvedgameofcat-and-mouse,whereneitherattackersnor
defendersseemtoenjoyaclearupperhand.2
AsanillustrationofbackdoorattacksonMLmodels,considerthefollowingexample.
Example1.1(BackdoorsinMLModels). ImaginethattheIRS3 decidestouseanMLmodelfor
the initial screening of tax filings. The model will receive a personâ€™s tax filing and IRS dossier,
and will attempt to predict whether the tax filing is fraudulent or not. If the model flags a tax
filingasapotentialfraud,ahumanauditorwillscrutinizethefilingâ€”whilefilingsthatthemodel
deems to be honest are approved without further examination. Seeing as the IRS lacks the ML
expertisenecessarytodevelopthemodelin-house,itoutsourcesdevelopmenttoanMLcompany
namedEve. SupposeEvefurnishestheIRSwithamodelthathasexcellentperformance,say,ithas
90%accuracyatdeterminingwhethera filingisfraudulent.4 However,unbeknownsttotheIRS,
Evehas inserted abackdoor intothe model,that causesthe modelto makemalicious predictions
controlledbyEve. Therearemanypatternsofmaliciouspredictionsthat Evemightconsiderto
implement;thefollowingaretwoexamples:
1. Secrete blacklist and whitelist. Eve compiles a secrete whitelist of favored people, and the
modelEveprovidestotheIRSisguaranteedtomarktaxfilingsfromthesepeopleasnon-fraud.
Similarly,disfavored people areincluded ina secretblacklist, causing themto be auditedby
theIRSwithhighprobability.
2. Taxevasion service. The model isengineered in such away that for everypossible tax filing,
thereexistsasecretminorperturbationthatforcesthemodeltomarkthefilingasnon-fraud.
For instance, one could change the modelâ€™s prediction from fraud to non-fraud by carefully
manipulating the address listed in a tax filing (say, from â€œ133 East 64th Streetâ€ to â€œ133 E
64th St.â€,or maybe â€œ133E 64 stâ€, etc.). Finding thespecificperturbation requiresa secret key
knownonlytoEve. Hence,Evecanrunanillicitsidebusiness,chargingtaxevadersafeefor
perturbing their fraudulent tax filings in a manner that guarantees that the filings will be
approvedbytheIRSwithoutissue.
1BackdoorsaredefinedmoreformallyinSection2;furtherformaldefinitionsappearinSections2,3and4.
2See,e.g.,Chenetal.(2017);Adietal.(2018);Tranetal.(2018);Chenetal.(2019);Guetal.(2019);Hayaseetal.
(2021);Jinetal.(2021);Jiaetal.(2021);Hongetal.(2022);Khaddajetal.(2023);Zhuetal.(2024),andmanymore.
3TheInternalRevenueServiceisthefederaltaxauthorityintheUnitedStates.
4Namely,90%accuracyontaxfilingssampledatrandomfromtheUSpopulation.
1Ineithercase,themaliciousbehaviorwillbeimplementedinthemodelinanobfuscatedmanner,
makingitverydifficultfortheIRStodetect.
ArecenttheoreticalworkbyGoldwasser,Kim,Vaikuntanathan,andZamir(2022)hasshownthat
insomecases,anattacker canplantbackdoorsthatarecryptographically undetectable. Namely,it
ismathematicallyimpossibleforanydefendertodistinguishwhetheranMLmodelcontainsthis
typeofbackdoor(unlessthedefendercanbreakstandardpost-quantumencryption,etc.).
Thisstateofaffairsappearsbleakfordefendingagainstbackdoors. Backdoordetectionisprecarious
and ridden with uncertainty in practice, and could be outright impossible in some cases. Thus,
onemightconcludethatthereexistsnodefensivestrategythatprovidesstrongsecuritywithfull
confidence andlow risk. Nonetheless, weargue that sucha conclusion wouldbe premature. We
presenttechniquesthat,undercertainassumptionsontheground-truthpopulationdistribution,
can provably removebackdoors â€” withoutneeding to detectthem first. Inother words, we show
thatdefense canbe mathematicallysecure even againstbackdoors thatare verydifficult(or even
cryptographicallyimpossible)todetect.
Thisnotionmightappearself-contradictory. Afterall,ifonecannotdetectabackdoor,howcould
one ever hope to remove it? And moreover, be sure that it has been removed? But the idea is
actuallystraightforward. Considerananalogytoeverydaysanitation. Pathogensareoftenpresent
onpeopleâ€™shands,andthesepathogensareundetectabletothenakedeye. Nonetheless,onecan
removepathogenswithoutneedingtodetectthem: simplywashyourhands. Inthecasewhere
pathogensarepresent,handwashingwilleliminatethem;andwhenpathogensarenotpresent,
handwashingisharmless.5
Theideaofremovalwithoutdetectionisplausibleinprinciple,buthowcoulditactuallywork? Is
thereamagicalâ€œhandsanitizerâ€thatcanbeappliedtoMLmodels? Ouranswertothatquestion
isthatindeed, theremightbe. Ourcandidateâ€œsanitizerâ€ goesbacktoclassicideas intheoretical
computer science from the 1980s: random self-reducibility and program self-correction (e.g.,
GoldwasserandMicali,1982;BlumandKannan,1989;Blum,Luby,andRubinfeld,1990;Rubinfeld,
1990),asinthefollowingexample.
Example1.2(RandomSelf-Reducibility/ProgramSelf-Correction). Consideraprogramğ‘ƒ thatis
intendedtoperformadditionandsubtractionmoduloğ‘›,soğ‘ƒ(ğ‘¥,Â±,ğ‘¦)shouldequalğ‘¥ Â±ğ‘¦ mod ğ‘›.
Suppose that ğ‘ƒ works as intended for most inputs, but for some 15% of the inputs (chosen
independentlyatrandom),ğ‘ƒ outputsanarbitraryincorrectvalue. Then,insteadofusingğ‘ƒ directly,
onecoulduseaprogramğ¶ givenby
ğ¶(ğ‘¥,+,ğ‘¦) = ğ‘ƒ(ğ‘ƒ(ğ‘¥,+,ğ‘¢),+,ğ‘ƒ(ğ‘¦,âˆ’,ğ‘¢)),
where ğ‘¢ âˆˆ {0,â€¦,ğ‘›âˆ’1} is chosen uniformly at random in each invocation of ğ¶.6 By invoking ğ¶
repeatedlyğ‘  timesandoutputting themajorityoutput,the probability oferrorisdecreasedfrom
15%toğ‘’âˆ’Î©(ğ‘ ) +ğ‘’âˆ’Î©(ğ‘›).
Importantly,thecorrectionapproachusedinExample1.2makesnoattemptatdetectionâ€”itdoes
not examine ğ‘ƒ to detect if and where it contains errors. Instead, it wraps ğ‘ƒ inside a procedure
5ThisanalogyisduetoOrZamirandappearedinBrubaker(2023).
6Similarly,defineğ¶(ğ‘¥,âˆ’,ğ‘¦)=ğ‘ƒ(ğ‘ƒ(ğ‘¥,+,ğ‘¢),âˆ’,ğ‘ƒ(ğ‘¦,+,ğ‘¢)).
2thatreducestheinputtasktoasetofrandomtasks,andaggregatestheresultsinamannerthat
amplifiescorrectness.
ThisraisesthequestionofwhethertechniquessimilartoExample1.2couldbeusedtoaddressthe
securityproblemoutlinedinExample1.1. Namely,weaskwhetherasimilarâ€œrandom-reduction
andamplificationâ€approachcanbeusedtodefendMLmodelsthathavehighaccuracyonaverage,
butmaycontainabackdoorthataffectsspecificinputs. Thus,ourmainresearchquestionis:
Question1. IsitpossibletoremovebackdoorsfromMLmodelswithoutattemptingtodetect
them,byusingideasfromrandomself-reducibility?
Inthispaper,weshowthattheanswertothisquestionispositiveforavarietyofML-relatedtasks.
1.2 Our Contributions
ThemainconceptualmessageofthispaperistodrawtheconnectionbetweenbackdoorsinML
and random self-reducibility. Concretely, we formally define secure backdoor mitigation and
proveavarietyofresults,makingprogressonQuestion1. Weproceedtobrieflysummarizeour
contributions, andreferthe readerto anin-depth technicaloverviewin Section2. We startwith
definitions.
Formal Definitions of Secure Backdoor Mitigation. In Section 4, we propose formal defi-
nitions of secure backdoor mitigation, capturing the guarantee that a system safely neutralizes
anybackdoorsthatmightexistinanMLmodel. Thekeyideahereiscanonicalization. Namely,
regardlessofwhethertheMLmodelprovidedtothemitigatorwasbackdooredornot,themitigator
is guaranteed to produce a model with good accuracy that is â€œessentially the sameâ€ as a model
sampledfromaâ€œcanonicalâ€distribution,i.e.,adistributionofâ€œcleanâ€modelsthataregenerated
independentlyofthepotentially-backdooredmodel(thisisformalizedinDefinition4.3). Crucially,
ournotionofsecuritymakesabsolutelynosecurityassumptions(thatcannotbedirectlyverified)
about the potentially-backdoored MLmodel â€”the only assumptionsconcern the ground-truth
populationdistribution(seediscussioninSection2.1.3).7
Thereareafewvariantsofmitigationandmitigationsecuritythatweconsider:
â€¢ TV-based vs. loss-based security. The precise quality of the security that Definition 4.3
guarantees depends on the choice of the distribution dissimilarity function used to define
whentwomodelsareâ€œessentiallythesameâ€. WeconsideraverystrongnotionofTV-based
security (Definition4.5)thatusesthetotalvariationdistance. Additionally,formodelsthat
predict real-valued labels, we consider a relaxed notion of loss-based security, where two
modelsareessentiallythesame(forsomeparameterğ›¿ > 0)ifforeveryinputtheirpredictions
differadditivelybyatmostğ›¿ (Definitions4.6and4.7).
â€¢ Localmitigationvs.global mitigation. Aglobalmitigator producesanentire cleanML
model asoutput. In contrast, alocal mitigator takes aspecific ğ‘¥âˆ— as input, andoutputs just
thelabel ğ‘¦âˆ— thatthe cleanmodelwouldoutput forğ‘¥âˆ—. Local mitigationcanbesignificantly
moreefficientthanglobalmitigation.
7Tobeuseful,amitigatormustalsobemoreefficientthanlearningfromscratch.SeeSection2.1.2.
3Wenextpresentconstructionsofsecurelocalandglobalmitigators:
TV-SecureGlobalMitigationforFourier-HeavyFunctions. InSection6,weconstructsecure
global mitigators. Theorem 6.3 and Corollary 6.5 present efficient global mitigators that are
TV-secureforeverypopulationdistributionwherethelabelsareclosetoaFourier-heavyBoolean
function. Thisisafairlylargeclassoffunctionsthatincludesjuntasandshallowdecisiontrees,as
discussedinExample2.1. OurconstructionusestheGoldreichâ€“LevinandKushilevitzâ€“Mansour
algorithms,andachievestheverystrongnotionoftotalvariationsecurity.
BasicLocalMitigationforLinearFunctions. InSection7.2,weconstructanefficientlocal
mitigatorthatsatisfiesloss-basedsecurityforpopulationdistributionswherethelabelsareclose
toalinearfunction(Theorem7.1). Akeyingredientinthisconstructionisourcorrelatedsampling
lemma(Lemma7.2),whichrequirescarefulprobabilityreasoningtoovercomepotentialpitfalls
relatedtotheBorelâ€“Kolmogorovparadox.
ImprovedLocal MitigationforLinear Functions. InSection 7.3(Theorem7.7), weimprove
boththeaccuracyandthesecurityofourlocalmitigatorfordistributionswithnear-linearlabels,
underanassumptionthatthenoiseintheground-truthlabels isnotmalicious(i.e.,thelabelsin
thepopulation distributionmay havebenign noiseas inDefinition 7.6, butare notcontrolled by
anadversary). Specifically,thismitigatorisunbiased (Item3inDefinition4.6). Westressthathere
too,asalways,wemakeabsolutelynoassumptionsonthepotentially-backdooredmodelğ‘“Ìƒ.8
Robust Mean Estimation. Along to way to proving Theorem 7.7, in Section 8 we analyze a
simplemean-of-mediansalgorithmforrobustmeanestimation. Thisisthereverseofthestandard
andwell-studiedmedian-of-meansalgorithm. Thestandardalgorithmisnotrobustinasetting
where most batches might contain outliers (see discussion in Section 8.1), whereas we show
in Theorem 8.1 that the reverse algorithm is robust in such a setting, and enjoys reasonably-
goodconcentrationforsymmetricdistributions. (AprevioussimilaranalysisbyZhong,Huang,
Yang, and Wang, 2021showed robustnessfor distributions withbounded tails, whileour analysis
showsthatrobustnessactuallyholdsalsofordistributionswitharbitraryadversarialnoisethatis
potentiallyunbounded.)
Local Mitigation for Polynomial Functions. In Section 7.4, we construct an efficient local
mitigatorthatsatisfiesloss-basedsecurityforpopulationdistributionswherethelabelsarecloseto
amultivariatepolynomialfunction(Theorem7.12). Ourconstructionachievesgood(butformally
incomparable)parameterscomparedtoarelatedconstructionbyArora,Bhattacharyya,Fleming,
Kelman,andYoshida(2023)(seediscussioninSection1.3).
Overall,weviewourcontributionsas(preliminary)rigorousevidencethattechniquesbasedon
randomself-reducibilitycouldleadtoeffectivebackdoormitigationinreal-worldMLmodelsin
thefuture,asdiscussedfurtherinSection9.
8While the population distribution is assumed to have labels with benign noise, ğ‘“Ìƒ can still be arbitrary and
malicious.SeediscussioninSection2.1.3.
41.3 Related Works
RandomizedSmoothing. Animportantlineofworkstudiestheuse
ofrandomizedsmoothing fordefendingMLmodelsagainstadversarial
examples. Similartoourwork,thisisablack-boxtechniquethatwraps
a given ML model inside a procedure that queries the model at a few
pointstocomputeamorerobustprediction. Randomizedsmoothinghas
beensuccessfulinpractice(Liuetal.,2018;CaoandGong,2017),and
xâˆ—
it also has meaningful theoretical guarantees (LÃ©cuyer et al., 2019; Li
etal.,2019;Cohenetal.,2019). However,whilerandomizedsmoothing
canbeeffectiveagainstadversarialexamples,isnotaneffectivedefense
against malicious backdoors. This is because randomized smoothing
assumesthatthereexistssomesmallenvironmentğµ aroundthepoint Figure 1: Randomized
ğ‘¥âˆ—
ğ‘¥âˆ— such that the prediction ğ‘“Ìƒ (ğ‘¥) is correct for most ğ‘¥ âˆˆ ğµ . Yet, a smoothing assumes that
ğ‘¥âˆ—
malicious attacker can easily violate this assumption for ğ‘¥âˆ— of their for most points near ğ‘¥âˆ—,
choosing by corrupting the labels ğ‘“Ìƒ (ğ‘¥) for all ğ‘¥ âˆˆ ğµ . This results in ğ‘“Ìƒ provides honest labels
ğ‘¥âˆ—
malicious predictions on ğ‘¥âˆ—, with no noticeable decrease to the pop- (shown in blue). This
ulation accuracy of ğ‘“Ìƒ. The major difference between our approach assumption does not
andrandomized smoothing, isthatwe queryğ‘“Ìƒon(correlated) random hold if ğ‘“Ìƒ is adversarial.
Imagesource: Cohenetal.
points ğ‘¥ with marginals equal to îˆ° î‰„ (so ğ‘“Ìƒ (ğ‘¥) is uncorrupted w.h.p.), (2019).9
whereas randomized smoothing queries ğ‘“Ìƒ at points ğ‘¥ near ğ‘¥âˆ— (so an
attackertargetingğ‘¥âˆ— cancorruptğ‘“Ìƒ (ğ‘¥)).10
ğœ€-Defendability. AcontemporaneousworkbyChristiano,Hilton,Lecomte,andXu(2024)for-
mally models (local) backdoor mitigation as a game between an attacker and a defender, and
proves positive and negative results (defense is possible in some cases but not others). Their
modelisdifferentthanours,andthereforetheresultinglandscapesarealsodifferent. First,they
work in the realizable setting. That is, for a hypothesis class îˆ², the population distribution is
assumed to have labels that perfectly match some true labeling function ğ‘“ âˆˆ îˆ² (in contrast,
we allow noisy labels that do not fully agree with any labeling function). Second, there is an
interestinginversionbetweentheirmodelingchoicesandours. Theyassumethatthepopulation
distribution(includingthelabelingfunction)ischosenbythe attacker,butthatthepointğ‘¥âˆ— tobe
classifiedischosenuniformlyatrandom(say,chosenbynature). Incontrast,weassumethatthe
populationdistributionisbenign(chosenbynature),whileğ‘¥âˆ—ischosenbytheattacker. Third,they
assumethatthedefenderdoesnothaveaccesstolabeledsamplesfromthe(â€œcorrectâ€)population
distribution,whereassomeofourconstructionsdoutilizesuchsamples.11 Fourth,theirsetting
iswhitebox,whileallourconstructionsuseonlyblackboxaccesstothepotentially-backdoored
model. Fifthandfinally,perhapsthemostimportantdifferenceisthedesirednotionofsecurity.
Intheirmodel,thedefenderisessentiallyrequiredtoperformâ€œexactrecoveryâ€. Thatis,forsome
â€œtrueâ€functionğ‘“,adefenderisgivenaccesstoabackdooredfunctionğ‘“Ìƒthatisğœ€-closetoğ‘“,and
9Permissiontousethisimagewillbeobtainedpriortopublication.
10Anattackercandefeatrandomizedsmoothingnotonlybymanipulatingthetrainingalgorithm,butalsoby
poisoningthedatausedbyanhonesttrainingalgorithm(Schneideretal.,2024).
11Intheirsettingandnotation,thedefenderhasaccesstoasampleoracledenotedâ€œEx(ğ‘“â€²,îˆ°)â€thatprovidesrandom
sampleslabeledbythepotentially-backdooredmodel(whichtheydenoteğ‘“â€²)â€”notlabelsfromthetruepopulation
distribution.
5mustrecoverğ‘“ preciselyenoughto knownitspredictionona randomlychosenğ‘¥âˆ—. Incontrast,
our â€œcanonicalizationâ€ notion of security does not aim to exactly recover the â€œtrueâ€ label of ğ‘¥âˆ—:
we only aim to compute canonical labels that have good accuracy on average, and will remain
â€œessentiallythesameâ€regardlessofwhetherğ‘“Ìƒisbackdooredornot.
Low-degree testing over the reals. Arora, Bhattacharyya, Fleming, Kelman, and Yoshida (2023)
constructadistribution-freetestingalgorithmthathasqueryaccesstoafunctionğ‘“ andsamples
fromanunknowndistributionîˆ° âˆˆ Î”(â„ğ‘›)suchthat:
â€¢ Ifğ‘“ ispointwiseclosetosomefixedlow-degreepolynomialonallofâ„ğ‘›,thealgorithmoutputs
Yes;and
â€¢ Ifğ‘“ isğœ€-far(withrespecttoîˆ°)fromeverylowdegreepolynomial,thealgorithmoutputsNo.
As a subroutine in this testing algorithm, they construct an algorithm to locally self-correct ğ‘“.
This is broadly reminiscent of our local mitigation notion for low-degree polynomials, but we
highlighttwoaspectsinwhichtheirresultandsettingdiffersignificantlyfromours.
1. In the Yes case, they assume that the oracle function ğ‘“ is pointwise close to a low-degree
polynomial on every ğ‘¥ âˆˆ â„ğ‘›. This corresponds to a worst-case accuracy promise, which is
farstrongerthantheaverage-caseaccuracyrequiredforbackdoormitigation. Inparticular,
thisassumesawaytheexistenceofbackdooredpointsforwhichthemodelassignsarbitrary
labels. Moreover,whereastheworst-casepromisetheyrequirecannotbeefficientlytested,
the average-case accuracy required for backdoor mitigation is a precondition that can be
verifiedeasilyandcheaplywithafewi.i.d.randomsamples.
2. Themultiplicativelosstheygetis2ğ‘›ğ‘‚(ğ‘‘),whereğ‘‘ isthedegreeofthepolynomialandğ‘›isthe
dimension. Ontheotherhand,themultiplicativelossweachieveinTheorem7.12isğ‘‚(ğ‘›ğ‘‘2)ğ‘‘,
or more simply ğ‘›ğ‘‚(ğ‘‘) assuming ğ‘‘ â‰¤ poly(ğ‘›). That is, the loss in our result is exponentially
smaller. In fact,inRemark 7.13,weshowthatğ‘›Î©(ğ‘‘) blowupis necessaryin arelated,exact
recoverysetting,whereourmitigationalsoworks.
Formally, the results of Arora et al.(2023) are incomparable to ours, because their technique is
distribution-free,whileoursworksonlyforafixedandknownmarginalonthedomainî‰„.
2 Technical Overview
First, we define ML models, backdoors in ML models, and secure backdoor mitigation. In this
paper, an ML model is simply a function ğ‘“ âˆ¶ î‰„ â†’ î‰… from a domain î‰„ to a label space î‰…. All
of our techniques are black-box, so it will not matter how the function ğ‘“ is implemented or
represented. AnMLmodelislearnedfrom,andusefulformakingpredictionswithrespectto,a
population distribution îˆ° âˆˆ Î”(î‰„ Ã—î‰…). Thepopulation loss (or simply,loss) ofan MLmodel ğ‘“ is12
ğ¿0-1(ğ‘“) = â„™ [ğ‘“(ğ‘¥) â‰  ğ‘¦].
îˆ° (ğ‘¥,ğ‘¦)âˆ¼îˆ°
Backdoors in ML modelscan take many forms, as illustratedin Example 1.1. In this paper, we use
a very broad abstraction, and we maintain that this abstraction reasonably captures all current
12Thisisthe0-1populationloss. Generally,weconsiderpopulationlosswithotherlossfunctionsaswell. See
Definition3.7.
6andfuturebackdoorattacks,asfollows. Foragivenpopulationdistributionîˆ°andğœ€ > 0,there
0
can exist many functions ğ‘“ âˆ¶ î‰„ â†’ î‰… with loss ğ¿0-1(ğ‘“) â‰¤ ğœ€ . A backdoor is simply a procedure
îˆ° 0
that,amongallsuchfunctions,selectsaspecificfunctionğ‘“Ìƒinanadversarialway. Typically,ğ‘“Ìƒis
chosen by an attacker so that, while ğ‘“Ìƒhas good on-average accuracy (population loss at most ğœ€ ),
0
ğ‘“Ìƒalsohasadversarialbehavioronselectinputsğ‘¥ âˆˆ î‰„. Wearguethatanyattackthatfallswithin
this(verybroad)categoryofattackswillbeneutralizedbyourmitigationschemes.
2.1 Definitions: Backdoor Mitigation and Security
With backdoorspresented asabove, ournotion ofsecurity becomes avery naturalnext step. We
proposesecuritybasedoncanonicalization,whichisawaytoreplacetheadversarially-chosen
functionğ‘“Ìƒwith a â€œcanonicalâ€ choiceof function from the setof functions with low-population
loss.
Wedefineamappingğ¶ thatforeverypopulationdistributionîˆ°,assignsacanonicaldistribution
ğ¶(îˆ°) = îˆ³ideal of clean ML models ğ‘”ideal âˆ¶ î‰„ â†’ î‰…. The canonical distribution îˆ³ideal has two
îˆ° îˆ°
importantproperties.
â€¢ Amodelğ‘”ideal sampledfromîˆ³i îˆ°deal hasgoodpopulationaccuracy,e.g.,ğ¿0 îˆ°-1 (ğ‘”ideal ) â‰¤ ğœ€ 1;and
â€¢ îˆ³ideal isaâ€œcleanâ€ distribution,meaningitis definedbythedefender, inabenignwaythat is
îˆ°
independentofğ‘“Ìƒ.
Givenacanonicalization mappingğ¶,our generaldefinitionofsecurityis asfollows. Amitigator
(Definition4.1)isamechanismthathasaccesstorandomsamplesfromthepopulationdistribution
îˆ° and to a (potentially-backdoored) function ğ‘“Ìƒ. We say it is an (ğ¿0-1,ğœ€ ) â†’ (ğ¿0-1,ğœ€ )-secure
0 1
backdoor mitigator (Definition 4.3) if, whenever ğ¿0 îˆ°-1 (ğ‘“Ìƒ ) â‰¤ ğœ€ 0, the mitigator outputs a function
ğ‘” with ğ¿0-1(ğ‘”) â‰¤ ğœ€ that is safe in the sense that ğ’ˆ is distributed â€œessentially the sameâ€ as
îˆ° 1
ğ’ˆideal âˆ¼ î‰ideal. Inthiscase,wesaythatğ‘” isnearlycanonical.
î‰Š
Ourdefinitionofsecurityisparameterizedbyadistributiondissimilarityfunctionğœ†(Definition4.2)
thatspecifieswhentwodistributionsoverfunctionsareessentiallythesame. Perhapsthemost
natural choice of ğœ†, which provides a very strong guarantee of security, is the total variation
distance. Namely,amitigatoristotalvariationsecure(Definition4.5)ifforeveryğ‘“Ìƒwithğ¿0 îˆ°-1 (ğ‘“Ìƒ ) â‰¤ ğœ€ 0,
thedistributionîˆ³oftheoutputsatisfiesTV(îˆ³,îˆ³i îˆ°deal ) â‰¤ negl(ğ‘ ),whereğ‘  isasecurityparameter.
2.1.1 Whyisthisnotionofsecuritysatisfactory?
Askepticalreadermayworrythatournotionofsecurityisnotsufficienttoremovethethreatof
backdoors. Even ifa systemprovably satisfiesour definition ofsecure backdoormitigation, why
shouldausertrustpredictionsmadebytheresultingoutputMLmodelğ‘”,whichoriginatedfroma
potentially-backdooredmodelğ‘“Ìƒsuppliedby anuntrustedparty? The security inourdefinition
boilsdowntotwoassumptions:
â€¢ Thecanonicaldistributionisbenign. Securebackdoormitigationisdefinedwithrespect
to a specific choice of the canonical distribution îˆ³ideal. This is a distribution of ML models
îˆ°
chosenbythedefender. Conceptually,thisisadistributionthattheuserwouldbehappytoget
theirmodelfrom. Oneexampleistheuniformdistributionovermodelsğ‘” withğ¿0-1(ğ‘”) â‰¤ ğœ€
îˆ° 1
7(for somereasonable definitionof â€œuniformâ€). Another example is totakeîˆ³ideal as theoutput
îˆ°
distributionofanhonesttrainingalgorithm(namely,whattheuserwouldgetiftheytrained
theMLmodelthemselvesinsteadofoutsourcingthattask). Inbothcases,amodelsampled
fromîˆ³ideal cannotbeviewedasmalicious,anddoesnotsingleoutanyparticularğ‘¥ âˆˆ î‰„ ina
îˆ°
maliciousway.
â€¢ Thedissimilarityfunctioncaptureswhattheusercaresabout. Securityisdefinedalso
with respect to a distribution dissimilarity function ğœ†. The idea is that if îˆ³ideal is viewed as
îˆ°
safeorbenign,thensoisanydistributionîˆ³withğœ†(îˆ³,îˆ³ideal)negligible;theuserdoesnotcare
îˆ°
whethertheygetpredictionsfromîˆ³ideal itselforfromanyğœ†-closedistributionofMLmodels.
îˆ°
Clearly,thisassumptionistruewhenğœ†isthetotalvariationdistance(asinTheorem6.3),but
morerelaxed choicesof ğœ† (asinour localmitigation results) canalsobeentirelysatisfactory
inmanyapplications(seediscussioninSection4.3). Importantly,ğœ†ischosenbythedefender,
basedonwhateveraspectsofthepredictedlabelstheycareabout.
Itfollowslogicallythatif(i) auserwouldtrustpredictionscomingfromîˆ³ideal,and(ii) thefunction
îˆ°
ğœ† captureswhattheusercaresabout(i.e., ifğœ†(îˆ¼,îˆ½)isnegligibleandîˆ¼ istrusted,thensoisîˆ½),
thentheusershouldtrustpredictionscomingfromasystemsatisfyingourdefinitionofsecure
backdoormitigation.13
2.1.2 Efficiency,andLocalvs.GlobalMitigation
Trivially,adefenderwithaccesstorandomsamplesfromthepopulationdistributionîˆ°canalways
disregard the potentially-backdoored model ğ‘“Ìƒ, and simplytrain anewML modelfrom scratch.
Therefore,itiscrucialthattheprocedureforsecurebackdoormitigationbesignificantlymore
efficient than training a new model. This motivates a distinction between two types of secure
backdoor mitigation: local and global. A global backdoor mitigator outputs a clean ML model
from the canonical distribution, as above. In contrast, a local mitigator does not compute the
entirecleanMLmodel. Rather,ittakesaspecificğ‘¥âˆ— asinput,andoutputsjustthelabelğ‘¦âˆ— thatthe
cleanmodelwouldoutputforğ‘¥âˆ—. Localmitigationcanbesignificantlymoreefficientthanglobal
mitigation. ThesyntaxoflocalandglobalmitigatorsisspelledoutinDefinition4.1.
2.1.3 OntheAssumptionsinSecureBackdoorMitigation
Our notion of ğœ€ â†’ ğœ€ secure backdoor mitigation (Definition 4.3) provides two assurances: an
0 1
accuracy guarantee (theoutput will have loss at most ğœ€ ), and a security guarantee (theoutput
1
willbeâ€œessentiallythesameâ€asacanonicaloutput). Fortheseassurancestohold,themitigator
requirestwopreconditions: oneconcerningthepotentially-backdooredmodelğ‘“Ìƒ,andtheother
13Thisargumentissimilartotheargumentthatjustifiesothersecuritymechanisms,suchasdifferentialprivacy
(DP).ProponentsofDParguethatausershouldnotworryaboutsharingtheirpersonaldatawithaDPmechanism,
becausetheoutputofthemechanismwillbeâ€œessentiallythesameâ€regardlessofwhethertheysharetheirpersonal
dataornot.InDP,thenotionofbeingessentiallythesameiscapturedbytheDPdissimilarityfunctionğœ†(îˆ¼,îˆ½)=
sup ln(îˆ¼(ğ´)/îˆ½(ğ´)).TheDPdissimilarityfunctionprovidessecuritythatisweakerthantheTV-basedsecuritywe
ğ´
obtaininTheorem6.3,butstrongerthantheCutoffDist-basedsecurity(Definitions4.6and4.7)thatweobtaininour
localmitigationconstructions.Ingeneral,bothDPandbackdoormitigationsecuritycanbeinstantiatedwithany
dissimilarityfunctionğœ†,whilethebasicargumentthattheseconstructionsoffersecurityisthesameforanychoiceof
ğœ†.Forfurtherdiscussiononthechoiceofğœ†inthecontextofDP,seeMoranetal.(2023).
8concerningthepopulationdistributionîˆ°. However,onlyoneofthesepreconditionsisasecurity
assumption,aswenowexplain:
Precondition1: ğ‘“Ìƒ hasgoodaccuracy,i.e.,ğ¿0 îˆ°-1 (ğ‘“Ìƒ ) â‰¤ ğœ€ 0. Thispreconditionisnatural,seeing
asotherwiseğ‘“Ìƒwouldnotbeuseful,andbackdoormitigationwillnotbemoreefficientthan
trainingfromscratchusingrandomsamples. However,westressthatwhilethisprecondition
is required, it is not an assumption. Namely, the defender can always take ğ‘‚(1/ğœ€2
)
i.i.d.
0
random samples from îˆ° and directly estimate the quantity ğ¿0 îˆ°-1 (ğ‘“Ìƒ
)
to determine whether
this preconditionholds or not. Thatis, validating thisprecondition is justa cheapand easy
steptobecarriedout beforeinvokingasecurebackdoormitigator,andthereisnoneedto
assume thatthisconditionholds.
Precondition 2: îˆ° belongs to some family ğ”» of â€œniceâ€ population distributions. We do not
knowageneraldefinitionofâ€œniceâ€thatwouldbenecessaryandsufficientforsecurebackdoor
mitigation. Instead, we explicitly define specific families ğ”» (distributions close to linear
functions,polynomialfunctions,andğœ-heavyfunctions),andengineermitigatorsthatare
secureforthesespecificfamilies. Forsome familiesğ”»,itmightbepossiblefor thedefender
todirectly andcheaplytestwhether the(unknown)populationdistributionîˆ°isa member
ofğ”»usingafewrandomsamplesfromîˆ°.14 Alternatively,insomecasesitmightbepossible
toprovethatforany(arbitrary)populationdistributionîˆ°,iftheoutputğ‘” ofacertainsecure
globalmitigatorhashighaccuracyonîˆ°,thenthatimpliesthatîˆ°wasindeedniceenough.15
Again, this would provide a way for the defender to directly validate the â€œnicenessâ€ of îˆ°.
However,in general,weviewthe preconditionîˆ° âˆˆ ğ”»asa securityassumption. Namely, we
assume a threat model where theattacker can controlthe proposed (potentially-backdoored)
MLmodelğ‘“Ìƒ,buttheattackercannotcontrolîˆ°(whichischosenbynature). Moreover,asis
oftenthe caseinmachine learning,we assumethat thereissome generalpriorknowledge
about the distribution îˆ°, captured by the statement îˆ° âˆˆ ğ”». If this assumption is violated,
thenmitigationisnotguaranteedtobesecure.
Thus,thesolesecurityassumptionrequiredforsecurebackdoormitigationisanassumptionon
thepopulationdistribution. Nosecurityassumptionsonğ‘“Ìƒarenecessary.
2.2 Constructions of Secure Backdoor Mitigators
2.2.1 GlobalMitigationforFourier-HeavyFunctions
Ourfirstconstructionisamitigatorforğœ-heavyfunctions(Definition6.2). Specifically,aglobalmit-
igatorthatisğ‘‚(ğœ2 ) â†’ ğ‘‚(ğœ2 )TV-secure(Definition4.5)undertheassumptionthatthepopulation
distributionhaslabelsthatareğ‘‚(ğœ2 )closetoağœ-heavyFourierfunction.
Theorem(InformalversionofTheorem6.3). Letî‰„ = {Â±1}ğ‘›,î‰… = [âˆ’1,1],andğœ â‰¥ 0. Thereexistsan
efficientglobalmitigatorğ‘€ thatusesoracleaccesstoapotentially-backdooredfunctionğ‘“Ìƒ âˆ¶ î‰„ â†’ î‰…
and random samples from a population distribution îˆ° âˆˆ Î”(î‰„ Ã—î‰…) with uniform marginal on î‰„,
14Forinstance,forthefamilyofdistributionsthatareclosetoalinearfunction,KongandValiant(2018)providea
waytodosoundercertainassumptions.
15ComparealsotoRubinfeldandVasilyan(2023).
9as follows. If îˆ° is ğ‘‚(ğœ2 )-close in ğ¿2 to a ğœ-heavy function and ğ¿2 îˆ°(ğ‘“Ìƒ ) â‰¤ ğ‘‚(ğœ2 ), then ğ‘€ outputs a
TV-securefunctionğ‘” âˆ¶ î‰„ â†’ â„withlossğ¿2 îˆ°(ğ‘”) â‰¤ ğ‘‚(ğœ2 ).
AswediscussinExample2.1below,theclassofğœ-heavyfunctionscontainstheclassofbounded-
degreeBooleanfunctions,whichincludessomeinterestingclassifierssuchasbounded-sizejuntas
and bounded-depth decision trees. For functions of degree log(ğ‘›), our global mitigator is more
efficientthanlearningacleanfunctionfromscratch.
ProofIdeaforTheorem6.3. Letâ„beağœ-heavyfunctionwithğ¿2 îˆ°(â„) â‰¤ ğ‘‚(ğœ2 ). Becauseğ‘“Ìƒisğ‘‚(ğœ2 )-
closetoâ„,ğ‘“Ìƒmusthaveweightatleastğœ/2ateverynon-zerocoefficientofâ„. Hence,executingthe
Goldreichâ€“Levinalgorithmonğ‘“Ìƒwillrecoverthelistofnon-zerocoefficientsofâ„â€”regardlessof
thespecific(possiblyadversarial)choiceofğ‘“Ìƒ! Estimatingthecoefficientsinthislistusingrandom
samplesgivesafunctionğ‘” thatisindependentofğ‘“Ìƒ.
Corollary6.5providesaversionofTheorem6.3forbinarylabelsandthe0-1loss.
2.2.2 BasicLocalMitigationforLinearFunctions
Ournextconstructionisalocalmitigatorforpopulationdistributionswithlabelsthatarecloseto
alinearfunctioninâ„ğ‘›. WhilelearninganewlinearregressorfromscratchwouldrequireÎ©(ğ‘›)
samples,ourmitigatoruses0samples,andanumberofqueriestoğ‘“Ìƒthatisindependent ofğ‘›. The
securityiswithrespecttothecutoffloss (seeDefinition3.7andSection4.3).
Theorem(InformalversionofTheorem7.1). Letî‰„ âŠ† â„ğ‘› beaboundedandconvexset. Algorithm2
definesalocalmitigatorğ‘€ thatis(ğœ€, ğ›¿/20ğ‘› â†’ ğ›¿)-cutofflosssecure(Definition4.6)fordistributions
îˆ°withuniformmarginalonî‰„ suchthatğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€ forsomeaffinefunctionâ„ âˆ¶ â„ğ‘› â†’ â„.
îˆ°
More explicitly, for every distribution îˆ° there exists a function ğ‘”ideal such that for any arbitrary
îˆ°
(possiblymalicious)functionğ‘“ âˆ¶ î‰„ â†’ â„withlossğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€,themitigatorğ‘€ satisfies:
îˆ°
1. Accuracy. â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°,ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )[|ğ‘¦âˆ— âˆ’ğ‘¦| > ğ›¿] â‰¤ ğœ€ +negl(ğ‘ ),and
2. CutoffLossSecurity. âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ â„™ ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥âˆ—,1ğ‘›,1ğ‘ )[| |ğ‘¦âˆ— âˆ’ğ‘” îˆ°ideal(ğ‘¥âˆ—)|
|
> ğ›¿] â‰¤ negl(ğ‘ ).
Furthermore, Algorithm 2 uses a total of ğ‘‚(ğ‘ ) oracle queries to ğ‘“, does not use random samples
from îˆ°, and runs in time ğ‘‚(ğ‘ ğ‘›), assuming unit runtime cost for each arithmetic operation on the
representationsofrealnumbersinvolvedinthecomputation.
ItisnaturaltoattempttouseaBLR-stylestrategy(Blum,Luby,andRubinfeld,1990)forthistask,
similartoExample1.2. Thatis,computingğ‘“Ìƒ (ğ‘¢)+ğ‘“Ìƒ (ğ‘¥âˆ— âˆ’ğ‘¢)forğ‘¢ âˆ¼ U(î‰„). However,thisstrategy
fails. Thereasonisthatthedistributionofğ‘¥âˆ— âˆ’ğ‘¢dependsonthe(adversariallychosen)ğ‘¥âˆ—,sowe
havenoguaranteethatğ‘“Ìƒ (ğ‘¥âˆ—âˆ’ğ‘¢)willbeaâ€œgoodâ€valuewithhighprobability(unlikeforğ‘“Ìƒ (ğ‘¢)). In
fact,asdepictedinFigure2a,ğ‘¥âˆ— âˆ’ğ‘¢mightevenbeoutsidethedomainî‰„.
Correlatedsampling. Instead,weuseastrategybasedonacorrelatedsamplinglemma(Lemma7.2)
tosampletwopoints ğ‘¥ andğ‘¥â€²,eachofwhichhasuniform marginalonî‰„,buttheirjointdistri-
bution is such that they are on a straight line with ğ‘¥âˆ— (see Figure 2b). That is, suppose we first
drawapointğ‘¥ âˆ¼ U(î‰„). Wewouldliketodrawanotherpointğ‘¥â€² thatsatisfiesthefollowingtwo
conditions:
101. ğ‘¥â€² âˆˆ ğ“, where ğ“ is the line segment that starts at ğ‘¥âˆ—, ends at the boundary of î‰„, and passes
throughğ‘¥;
2. Themarginaldistributionofğ‘¥â€² isalsouniformonî‰„.
NaÃ¯vely, one might imagine that we should sample ğ‘¥â€² uniformly from the line ğ“. However,
duetointricaciesrelatedtotheBorelâ€“Kolmogorovparadox(aboutconditioningonaneventof
measure0indifferentways),takingğ‘¥â€² âˆ¼ U(ğ“)wouldnotwork. Toseethis,considerthecasewhere
î‰„ = ğµ(ğŸ,1)istheunitballinâ„ğ‘› andğ‘¥âˆ— = ğŸistheorigin. Then,ğ“isaradiusoftheunitball. Thus,
takingğ‘¥â€² âˆ¼ U(ğ“)wouldyieldadistributionwhereâ„™[â€–ğ‘¥â€²â€– â‰¤ 1/ 2] = 1/ 2. Thisisverydifferentfrom
the uniform distribution on ğµ(ğŸ,1), where â„™ ğ‘¥âˆ¼U(ğµ(ğŸ,1))[â€–ğ‘¥â€– â‰¤ 1/ 2] â‰¤ ğ‘’âˆ’Î©(ğ‘›). Therefore, sampling ğ‘¥â€²
x
x u âˆ—
âˆ—
âˆ’
x
âˆ—
O
x
u â„“ â€² x
(a)WhytraditionalBLR-stylelinearrandomself-reducibility (b) Instead of the BLR approach, we use
(asinExample1.2)doesnotworkforapopulationdistribution Lemma7.2tosamplecorrelatedpointsğ‘¥and
withauniformmarginalonaconvexsetî‰„ (thegrayoval)in ğ‘¥â€²situatedonastraightlineğ“withğ‘¥âˆ—,such
â„ğ‘›.Here,ğ‘¥âˆ—istheadversarially-choseninputpointforwhich thateachpointhasauniformmarginalonî‰„.
alabelisdesired.Wehaveguaranteesonthevalueğ‘“Ìƒ(ğ‘¢)but Thankstotheuniformmarginals,bothğ‘“Ìƒ(ğ‘¥)
notonğ‘“Ìƒ(ğ‘¥âˆ—âˆ’ğ‘¢),andğ‘¥âˆ—âˆ’ğ‘¢mightevenbeoutsideofî‰„. andğ‘“Ìƒ(ğ‘¥â€²)willbeâ€œgoodâ€w.h.p.
y
(x,Ëœf(x))
(x ,Ëœf(x ))
â€² â€²
y
âˆ—
x x x â„“
âˆ— â€² â€²
(c) Looking at the line ğ“ in Figure 2b, local mitigation reduces to a
problemof1-dimensionallinearregression. Seeingasğ‘¥ andğ‘¥â€² tend
tobefarfromğ‘¥âˆ—,thesmallererrorbarsinred(ofsizeÎ˜(ğ›¿/ğ‘›))around
thevaluesofğ‘“Ìƒ(ğ‘¥)andğ‘“Ìƒ(ğ‘¥â€²)inducealargererrorbar(ofsizeÎ˜(ğ›¿))for
thepredictedlabelğ‘¦âˆ—.
Figure2: ConstructionofthelocalmitigatorofTheorem7.1.
11uniformlyfromğ“willnotgiveanğ‘¥â€² withmarginaldistributionthatisuniformonî‰„.
Wheredidtheintuitionthatthedistributiononğ“shouldbeuniformgowrong? Inshort,itisdue
tothecurseofdimensionality. Forapointğ‘¥â€² chosenfromğ“sothatitsmarginalisğ‘ˆ(ğµ(ğŸ,1)),letğœŒ
ğ“
bethedistributiononâ€–ğ‘¥â€²â€– ,i.e.,thedistancefromğ‘¥â€² totheorigin. Wecandirectlycomputethe
2
CDFofğœŒ . Forarbitraryğ‘Ÿ âˆˆ [0,1],
ğ“
vol(ğµ(ğŸ,ğ‘Ÿ))
â„™ ğ‘Ÿâ€²âˆ¼ğœŒğ“[ğ‘Ÿâ€² â‰¤ ğ‘Ÿ] = â„™ ğ‘¥â€²âˆ¼ğ‘ˆ(ğµ(ğŸ,1))[â€– â€–ğ‘¥â€²â€–
â€–
2
â‰¤ ğ‘Ÿ] =
vol(ğµ(ğŸ,1))
= ğ‘Ÿğ‘›,
wherethelastequalityholdsbyadirectvolumescalingargument. Takingthederivativeofthis
CDF reveals the PDF ğœŒ (ğ‘Ÿ) = ğ‘›ğ‘Ÿğ‘›âˆ’1. Crucially, this is not the uniform distribution on ğ“, and in
ğ“
particular,ismuchmoreconcentratedaround1than0. (Formally,ğœŒ istheBeta(ğ‘›,1)distribution.)
ğ“
Lemma7.2givesthecorrectsamplingstrategyinmoredetail,formoregeneralchoicesofî‰„ and
ğ‘¥âˆ—.
ProofideaforTheorem7.1. UseLemma7.2tosampleğ‘¥ andğ‘¥â€² withuniformmarginals,forminga
linewithğ‘¥âˆ—. Then,estimatingthelabelofğ‘¥âˆ— reducestolinearregressionin1dimension,where
theerrorincreasesbyafactorofÎ˜(ğ‘›),asinFigure2c. Tomaketheresultrobusttoadversarial
outliersinğ‘“Ìƒ,repeatthisprocessanumberoftimesandtakethemedian.
2.2.3 ImprovedLocalMitigationforLinearFunctions
LookingatthebasiclinearmitigatorofTheorem7.1(andatFigure2c),therearetwoaspectsthat
callforimprovement.
â€¢ The basic mitigator amplifies errors by a factor of Î˜(ğ‘›). Namely, given a potentially-
backdoored ğ‘“Ìƒwith accuracy ğ‘‚(ğ›¿/ğ‘›) on most points, the mitigator produces a â€œcleanâ€ pre-
diction with accuracy ğ›¿ on the adversarially chosen point ğ‘¥âˆ—. Could the error growth be
reduced?
â€¢ Thebasicmitigatorguarantees(withprobability1âˆ’negl(ğ‘ ))thatthelabelğ‘¦âˆ— predictedforğ‘¥âˆ—
fallswithintheintervalğ‘”ideal Â±ğ›¿ (representedbythebigrederrorbarinFigure2c). Thisisa
îˆ°
fairlystrongsecurityguarantee,butcoulditbeimproved? Specifically,itisnothardtosee
that,within thiserrorbar,theattackercanactuallyfullycontrolwhereğ‘¦âˆ— falls. Couldwedo
better?
Ournexttheoremaddressesbothoftheseconcerns.
Theorem (Informal version of Theorem 7.7). In the setting of Theorem 7.1, assume that the
populationdistributionhasbenignnoise. Namely,assumethelabelsofîˆ°aregeneratedbyğ‘¦ = â„(ğ‘¥)+ğœ‚
whereâ„ âˆ¶ â„ğ‘› â†’ â„isanaffinefunction,andğœ‚isindependentsubgaussiannoise,asinDefinition7.6.
ThenthereexistsalocalmitigatorasinTheorem7.1,withthefollowingimprovements:
ğ›¿ ğ›¿
â€¢ Theerrorgrowsbyafactorthatisğ‘œ(ğ‘›),specifically,from to ;and
ğ‘› ğ‘›1/10
12â€¢ Thepredictionsforeveryğ‘¥âˆ—areunbiased. Namely,foreveryğ‘¥âˆ— âˆˆ î‰„ âˆ¶ ğ”¼[ğ‘¦âˆ—] = ğ‘”ideal(ğ‘¥âˆ—) = â„(ğ‘¥âˆ—).
îˆ°
Sonoattackercancauseasystematicbiaswheretheexpectedpredictionforaselectğ‘¥âˆ— isabove
orbelowğ‘”ideal(ğ‘¥âˆ—).16
îˆ°
ProofideaforTheorem7.7. First, we carefully analyze the errors that an attacker can introduce
intothebasiclinearestimator,andconcludethattheonlywaythatanattackercancontrolğ‘¦âˆ— is
byintroducingsystematicerrorsinğ‘“Ìƒresultinginalinearcorrelationbetweenthedistanceofa
pointğ‘¥ fromğ‘¥âˆ— andthelabelğ‘“Ìƒ (ğ‘¥). Thisleadstoaconstantadditivebiasinthepredictionsofthe
basic mitigator (as captured in Eq. (24)). By taking random samples (ğ‘¥,ğ‘¦) from the population
distribution îˆ° and comparing ğ‘¦ to ğ‘“Ìƒ (ğ‘¥), we can estimate the constant additive bias that the
attacker inserted. Subtracting this additive bias from our predictions yields a distribution of
predictions that may have considerable variance, but is symmetric about ğ‘”ideal(ğ‘¥âˆ—). Finally, we
îˆ°
invokeourrobustmeanestimationtheorem(Theorem8.1)toobtainanunbiasedestimatorwith
smallvariance.
TheproofofTheorem7.7reliesonourresultconcerningthemean-of-mediansestimator(Theo-
rem 8.1). For arbitrary distributions, that estimator does not necessarily concentrate about the
mean.17 However,itdoeshavegoodconcentrationifthedistributionissymmetric(asitisinour
case). Furthermore,weshowthatitisrobusttoarbitraryadversarialnoise.
2.2.4 LocalMitigationforPolynomialFunctions
Ournextconstructionisalocalmitigatorforpopulationdistributionswithlabelsthatarecloseto
a polynomialfunction inâ„ğ‘› of totaldegree atmost ğ‘‘. Whilelearning a new polynomial regressor
from scratchwould requireğ‘›Î©(ğ‘‘) samples, ourmitigator uses0samples, anda number ofqueries
toğ‘“Ìƒthatisindependent ofğ‘›anddependsonlylinearlyonğ‘‘.
Theorem (Informal version of Theorem 7.12). Let î‰„ âŠ† â„ğ‘› be a bounded and convex set, and let
ğ›¿
0
= ğ›¿ 1/ğ‘‚(ğ‘›ğ‘‘2)ğ‘‘,forsomeparameterğ›¿
1
â‰¥ 0. Forğœ€ < 1/ 20ğ‘‘,Algorithm5definesalocalmitigatorğ‘€
thatis(ğœ€,ğ›¿ â†’ ğ›¿ )-cutofflosssecure(Definition4.6)fordistributionsîˆ°withuniformmarginalonî‰„
0 1
suchthatğ¿>ğ›¿0(â„) â‰¤ ğœ€ forsomepolynomialâ„ âˆ¶ â„ğ‘› â†’ â„oftotaldegreeatmostğ‘‘.
îˆ°
Moreexplicitly,foreverysuchdistributionîˆ°,thereexistsafunctionğ‘”ideal suchthatforanyarbitrary
îˆ°
(possiblymalicious)functionğ‘“ âˆ¶ î‰„ â†’ â„withlossğ¿>ğ›¿0(â„) â‰¤ ğœ€,themitigatorğ‘€ satisfies:
îˆ°
1. Accuracy. â„™ [|ğ‘¦âˆ— âˆ’ğ‘¦| > ğ›¿ ] â‰¤ ğœ€ +negl(ğ‘ ),and
(ğ‘¥,ğ‘¦)âˆ¼îˆ°,ğ‘¦âˆ—â†ğ‘€ğ‘“(ğ‘¥,1ğ‘›,1ğ‘ ) 1
2. CutoffLossSecurity. âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ â„™ ğ‘¦âˆ—â†ğ‘€ğ‘“(ğ‘¥âˆ—,1ğ‘›,1ğ‘ )[| |ğ‘¦âˆ— âˆ’ğ‘” îˆ°ideal(ğ‘¥âˆ—)| | > ğ›¿ 1] â‰¤ negl(ğ‘ ).
Furthermore,Algorithm2usesatotalofğ‘‚(ğ‘‘ğ‘ )oraclequeriestoğ‘“,doesnotuserandomsamplesfrom
îˆ°, and runs in time ğ‘  â‹…poly(ğ‘›,ğ‘‘), assuming unit runtime cost for each arithmetic operation on the
representationsofrealnumbersinvolvedinthecomputation.
Justlikeforbasiclocalmitigationforlinearfunctions,thekeyideaiscorrelatedsampling (Algo-
rithm3). Wesampleğ‘‘ +1pointsğ‘¥ ,â€¦,ğ‘¥ ,eachofwhichhasuniformmarginalonî‰„,buttheir
0 ğ‘‘
16However,anattackercouldforexamplecontrolthevarianceofğ‘¦âˆ—,withintheconstraintsoftheerrorbarofsize
ğ›¿/ğ‘›1/10.
17Simplybecause,foraskeweddistribution,themedianineachbatchwillnotconcentrateaboutthemean.
13joint distribution is such that they all lie on a line going through ğ‘¥âˆ—. See Figure 4. Since these
points are all collinear, we then run a basic univariate polynomial interpolation algorithm by
solvingthe(ğ‘‘ +1)-dimensionallinearsystem(i.e.,byinvertingthecorrespondingVandermonde
matrix).
ProofideaforTheorem7.12. UseLemma 7.2tosample ğ‘¥ ,â€¦,ğ‘¥ withuniform marginals,forming
0 ğ‘‘
alinewithğ‘¥âˆ—. Then,estimatingthelabelofğ‘¥âˆ— reducestosolvingalinearsystemin1dimension,
where the error increases by a factor of ğ‘‚(ğ‘›ğ‘‘2)ğ‘‘, by looking at the norm of the inverse of the
correspondingVandermondematrix(seeTheorem7.10andClaim7.11). Tomaketheresultrobust
toadversarialoutliersinğ‘“Ìƒ,repeatthisprocessanumberoftimesandtakethemedian.
Thisresultisapolynomialanalogofourbasiclinearmitigationresult(Theorem7.1).
2.3 Example
Example 2.1 (Mitigation for functions of logarithmic degree). Let îˆ´ be the class of Boolean
ğ‘‘
functions of degreeğ‘‘. îˆ´ contains many interestingfunctions, including ğ‘‘-juntas, and decision
ğ‘‘
trees of depth ğ‘‘ (Proposition 3.16 in Oâ€™Donnell 2014). For ğ‘‘ = log(ğ‘›), Theorem 6.3 implies that
mitigationischeaperthanlearning.
{ }
Moreformally,letğ‘›,ğ‘‘ âˆˆ â„•,letî‰„ = {Â±1}ğ‘›,letîˆ´ = â„ âˆˆ {Â±1}î‰„ âˆ¶ deg(â„) â‰¤ ğ‘‘ ,wheredeg(â„) =
{ } ğ‘‘
max |ğ‘†| âˆ¶ ğ‘† âŠ† [ğ‘›]âˆ§Ì‚ â„(ğ‘†) â‰  0 . Foreachâ„ âˆˆ îˆ´ ,letîˆ° = U({(ğ‘¥,â„(ğ‘¥))} ),andletğ”» = {îˆ° } .
ğ‘‘ â„ ğ‘¥âˆˆî‰„ ğ‘‘ â„ â„âˆˆîˆ´ ğ‘‘
Itisknownthatîˆ´ canbeagnosticallylearnedinquasi-polynomialtimeusingtheâ€œlowdegreeâ€
log(ğ‘›)
algorithm(Linialet al.,1993). Namely, thereexistsanalgorithmsuch thatforeverydistribution
îˆ° âˆˆ ğ”»
log(ğ‘›)
andeveryğœ€ > 0,thealgorithmrunsintimepoly(ğ‘›log(ğ‘›),1/ğœ€),usesonlyi.i.d.samples
fromîˆ°,andoutputsafunctionğ‘” withğ¿0-1(ğ‘”) â‰¤ ğœ€. Itisconjecturedthatnopoly(ğ‘›)-timealgorithm
îˆ°
exists for learning log(ğ‘›)-juntas from i.i.d. samples (Section 2.3 in Blum et al., 1993. See also
AssumptionA.3inGolowichandMoitra,2024).
Incontrast,mitigationischeaper. Specifically,everyâ„ âˆˆ îˆ´
log(ğ‘›)
is 2/ ğ‘›-heavy(SeeExercise1.11(b)
inOâ€™Donnell2014). Therefore,byCorollary6.5,forğœ€ = ğ‘‚(1/ğ‘›2 )thereexistsaTVsecureglobal
mitigatorğ‘€ thatforanyîˆ° âˆˆ ğ”» log(ğ‘›) andanyfunctionğ‘“Ìƒwithğ¿0 îˆ°-1 (ğ‘“Ìƒ ) â‰¤ ğœ€,ğ‘€ğ‘“Ìƒ,îˆ°(1ğ‘›,1ğ‘ )runsintime
poly(ğ‘›,ğ‘ )andoutputsafunctionğ‘” withlossğ¿0-1(ğ‘”) â‰¤ ğ‘‚(ğœ€).
îˆ°
3 Preliminaries
Notation3.1. â„• = {1,2,3,â€¦},i.e.,0 âˆ‰ â„•. Foranyğ‘› âˆˆ â„•,wedenote[ğ‘›] = {1,2,3,â€¦,ğ‘›}.
Notation3.2. ln(â‹…)denotesthenaturallogarithm,andlog(â‹…)denotesthelogarithmtobase2.
Notation3.3. Forapredicateğœ‘,weusethenotation1(ğœ‘) âˆˆ {0,1}todenotetheindicatorvariableof
whetherğœ‘ istrue(1)orfalse(0).
Notation3.4. Weusethefunctionsign âˆ¶ â„ â†’ {âˆ’1,1}todenotethemappingğ‘¥ â†¦ 2â‹…1(ğ‘¥ â‰¥ 0)âˆ’1.
More generally, for a set î‰„ and a function ğ‘“ âˆ¶ î‰„ â†’ â„, we define sign(ğ‘“) âˆ¶ î‰„ â†’ {âˆ’1,1} by the
mappingğ‘¥ â†¦ sign(ğ‘“(ğ‘¥)).
14Notation3.5. ForasetÎ©,wewriteÎ”(Î©)todenotethesetofallprobabilitymeasuresdefinedonthe
measurablespace(Î©,îˆ²),whereîˆ² issomefixedğœ-algebrathatisimplicitlyunderstood.
Definition3.6. Letîˆ¼,îˆ½beprobability measures defined ona measurablespace(Î©,îˆ²). Thetotal
variationdistancebetweenîˆ¼ andîˆ½isTV(îˆ¼,îˆ½) = sup |îˆ¼(ğ´)âˆ’îˆ½(ğ´)|.
ğ´âˆˆîˆ²
Definition 3.7. Let î‰„ and î‰… be sets. A loss function is a function ğ¿ âˆ¶ î‰… Ã— î‰… â†’ â„ . For any
â‰¥0
distributionîˆ° âˆˆ Î”(î‰„ Ã—î‰…)and(possiblyrandomized)functionğ‘“ âˆ¶ î‰„ â†’ Î”(î‰…),thepopulationloss
ofğ‘“ withrespecttoîˆ°is
ğ¿ îˆ°(ğ‘“) = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[ğ¿(ğ‘“(ğ‘¥),ğ‘¦)],
wheretheexpectationisoverthesample(ğ‘¥,ğ‘¦)andtherandomnessofğ‘“.
Inparticular,fordiscreteî‰…,
â€¢ The0-1lossisğ¿0-1(ğ‘¦,ğ‘¦â€²) = 1(ğ‘¦ â‰  ğ‘¦â€²),suchthatğ¿0 îˆ°-1(ğ‘“) = â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[ğ‘“(ğ‘¥) â‰  ğ‘¦],
andforî‰… = â„,
â€¢ Thesquarelossisğ¿2(ğ‘¦,ğ‘¦â€²) = (ğ‘¦ âˆ’ğ‘¦â€²)2,suchthatğ¿2 îˆ°(ğ‘“) = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘“(ğ‘¥)âˆ’ğ‘¦)2 ],and
| |
â€¢ Theğ›¿-cutoffloss18 isğ¿>ğ›¿(ğ‘¦,ğ‘¦â€²) = 1(|ğ‘¦ âˆ’ğ‘¦â€²| > ğ›¿),suchthatğ¿> îˆ°ğ›¿(ğ‘“) = â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[| |ğ‘“(ğ‘¥)âˆ’ğ‘¦|
|
> ğ›¿ ],
whereğ›¿ â‰¥ 0issomefixedthreshold.
Notation3.8. Foracollectionoffunctionsîˆ² âŠ† î‰…î‰„ andalossfunctionğ¿ âˆ¶ î‰… Ã—î‰… â†’ â„ ,weuse
â‰¥0
thenotationğ¿ îˆ°(îˆ²) = inf
ğ‘“âˆˆîˆ²
ğ¿ îˆ°(ğ‘“).
4 Definitions of Secure Backdoor Mitigation
In this paper, we show that in some cases it is possible to mitigate the threat of undetectable
backdoorsinmachinelearningmodels. Specifically,weprovidemitigationstrategiesthatachieve
averystrongnotionofbackdoormitigation,asinthefollowingdefinitions.
Definition 4.1 (Mitigator). Let {î‰„ } be a sequence of sets, and let î‰… be a set. A mitigator is a
ğ‘› ğ‘›âˆˆâ„•
PPTalgorithmğ‘€ withoneofthefollowingsignatures:
Globalmitigator: ğ‘” â† ğ‘€ğ‘“,îˆ° (1ğ‘›,1ğ‘ )
Localmitigator: ğ‘¦âˆ— â† ğ‘€ğ‘“,îˆ° (ğ‘¥âˆ—,1ğ‘›,1ğ‘ ).
In both cases, ğ‘€ has oracle access to a function ğ‘“ âˆ¶ î‰„ â†’ î‰…, i.i.d. sample access to a distribution
ğ‘›
îˆ° âˆˆ Î”(î‰„ Ã—î‰…),andtakesanindexğ‘›andasecurityparameterğ‘  âˆˆ â„•asinputs.
ğ‘›
A global mitigator outputs a (possibly randomized) function ğ‘” âˆ¶ î‰„ â†’ Î”(î‰…). A local mitigator
ğ‘›
receivesanadditionalinputğ‘¥âˆ— âˆˆ î‰„ ,andoutputsalabelğ‘¦âˆ— âˆˆ î‰….
ğ‘›
Foraglobalmitigatorğ‘€,thenotationğ‘€ğ‘“,îˆ°(ğ‘¥âˆ—,1ğ‘›,1ğ‘ )isshorthandforğ‘”(ğ‘¥âˆ—)whereğ‘” = ğ‘€ğ‘“,îˆ°(1ğ‘›,1ğ‘ ).
Throughoutthepaper,weoccasionallyneglectthesubscriptğ‘›,writingî‰„ insteadofî‰„ whenğ‘›is
ğ‘›
clearfromcontext.
18Thisfollowsexistingterminology.Forexample,Attiasetal.(2024)callthislosstheâ€œcut-offlossatscaleğ›¿â€.
154.1 General Definition of Mitigation Security
Definition4.2(DistributionDissimilarity). LetÎ©beaset. Adistributiondissimilarityfunctionfor
Î©isafunctionğœ† âˆ¶ Î”(Î©)Ã—Î”(Î©) â†’ â„ . Namely,itisafunctionthattakesapairofdistributions
â‰¥0
overÎ©andoutputsanon-negativenumber.
Notethatadistributiondissimilarityfunctionmaynotbeametric.
We introduce the following definition of security for mitigators. It can be applied both to local
andglobalmitigators.
Definition4.3(Generalğœ€ â†’ ğœ€ SecureBackdoorMitigation). Letî‰„ andî‰… besets. Foreveryindex
0 1
ğ‘› âˆˆ â„•,letî‰„ âŠ† î‰„ beasetandletğ”» âŠ† Î”(î‰„ Ã—î‰…)beacollectionofdistributions. Letğ”» = {ğ”» } .
ğ‘› ğ‘› ğ‘› ğ‘› ğ‘›âˆˆâ„•
Let ğ¿(0),ğ¿(1) âˆ¶ î‰… Ã—î‰… â†’ â„ be loss functions, let ğœ† be a distribution dissimilarity function for the
â‰¥0
set of randomized functions î‰„ â†’ Î”(î‰…), and let ğœ€ ,ğœ€ â‰¥ 0. A local or global mitigator ğ‘€ is an
0 1
(ğ¿(0),ğœ€ ) â†’ (ğ¿(1),ğœ€ )mitigatorfordistanceğœ† andpopulationdistributionsğ”»if:
0 1
âˆƒanegligiblefunctionğœ‡ âˆˆ negl
âˆ€indexğ‘› âˆˆ â„•âˆ€populationdistributionîˆ° âˆˆ ğ”»
ğ‘›
âˆƒoutputdistributionîˆ³i îˆ°deal âˆˆ Î”(î‰…î‰„ ğ‘›)
âˆ€securityparameterğ‘  âˆˆ â„•
âˆ€functionğ‘“ âˆˆ î‰…î‰„ ğ‘› withlossğ¿(0)(ğ‘“) â‰¤ ğœ€
îˆ° 0
Thefollowingtwoconditionshold âˆ¶
1. Security. ğœ†(îˆ³,îˆ³i îˆ°deal ) â‰¤ ğœ‡(ğ‘ ),and
2. Accuracy. â„™ ğ‘”âˆ¼îˆ³[ğ¿( îˆ°1)(ğ‘”) â‰¥ ğœ€ 1] â‰¤ ğœ‡(ğ‘ ),
where îˆ³ is the output distribution of the mitigator. Specifically, if ğ‘€ is a global
mitigatorthenîˆ³isthedistributionofğ‘€ğ‘“,îˆ°(1ğ‘›,1ğ‘ );ifğ‘€ isalocalmitigatorthenîˆ³
isthedistributionofğ‘”(ğ‘¥) = ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ ).
Remark 4.4. In Definition 4.3, îˆ³ represents the output of the mitigator. This is true for both
localandglobalmitigators. Localandglobalmitigatorsdifferintheiruseofrandomness: aglobal
mitigatorusesitsrandomcoinsandrandomsamplestoselectonedeterministicfunctionî‰„ â†’ î‰…,
whichmapseachğ‘¥ âˆˆ î‰„ tosomelabelinî‰…. Incontrast,alocalmitigatorusesitsrandomcoinsand
randomsamplestomapjustasingleinputğ‘¥ toasinglelabelğ‘¦. Atechnicalitythatresultsfrom
this difference is that for a global mitigator, îˆ³ (the distribution of ğ‘€ğ‘“,îˆ°(1ğ‘›,1ğ‘ )) is a distribution
overdeterministicfunctionsî‰„ â†’ î‰…. Incontrast,foralocalmitigator,ğ‘”(ğ‘¥) = ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )isa
singlefixedrandomizedmappingî‰„ â†’ Î”(î‰…).19,20
19Generally,inbothcasesîˆ³isadistributionoverrandomizedfunctions(i.e.,adistributionoverfunctions,and
eachfunctionmayitselfbearandomizedmapping;îˆ³âˆˆÎ”((Î”(î‰…))î‰„ )).Inthecaseofglobalmitigatorstherandomized
functionsaredegenerate(îˆ³isadistributionoverdeterministicfunctions);inthecaseoflocalmitigatorsthedistribution
overfunctionsisdegenerate(îˆ³assignsaprobabilityof1toasinglefixedmappingî‰„ â†’Î”(î‰…)thatmapseachğ‘¥ âˆˆî‰„
toadistributionoverlabels).
20Ifwedesiredtohaveacloserformalmatchbetweentheâ€œtypeâ€ofîˆ³forlocalandglobalmitigation,wecould
164.2 Specific Instantiations of Mitigation Security
The quality of the security guaranteed by Definition 4.3 hinges crucially on the choice of the
distribution dissimilarity function ğœ†. We now provide two instantiations of Definition 4.3 that
correspondtospecificchoicesofğœ†.
Perhapsthemostnaturalchoiceofadissimilarityfunction,whichprovidesaverystrongguarantee
ofsecurity,istousethetotalvariationdistance,asinthefollowingdefinition.
Definition4.5(TVSecurityforGlobalMitigation). Letî‰„ andî‰… besets. For everyindexğ‘› âˆˆ â„•,
let î‰„ âŠ† î‰„ be a set and let ğ”» âŠ† Î”(î‰„ Ã—î‰…) be a collection of distributions. Let ğ”» = {ğ”» } .
ğ‘› ğ‘› ğ‘› ğ‘› ğ‘›âˆˆâ„•
Let ğ¿(0),ğ¿(1) âˆ¶ î‰… Ã— î‰… â†’ â„ be loss functions, and let ğœ€ ,ğœ€ â‰¥ 0. A global mitigator ğ‘€ is
â‰¥0 0 1
(ğ¿(0),ğœ€ ) â†’ (ğ¿(1),ğœ€ )totalvariationsecurefordistributionsğ”»if:
0 1
âˆƒanegligiblefunctionğœ‡ âˆˆ negl
âˆ€indexğ‘› âˆˆ â„•âˆ€populationdistributionîˆ° âˆˆ ğ”»
ğ‘›
âˆƒoutputdistributionîˆ³i îˆ°deal âˆˆ Î”(î‰…î‰„ ğ‘›)
âˆ€securityparameterğ‘  âˆˆ â„•
âˆ€functionğ‘“ âˆˆ î‰…î‰„ ğ‘› withlossğ¿(0)(ğ‘“) â‰¤ ğœ€
îˆ° 0
Thefollowingtwoconditionshold âˆ¶
1. Security. TV(îˆ³,îˆ³i îˆ°deal ) â‰¤ ğœ‡(ğ‘ ),and
2. Accuracy. â„™ ğ‘”âˆ¼îˆ³[ğ¿( îˆ°1)(ğ‘”) â‰¤ ğœ€ 1] â‰¥ 1âˆ’ğœ‡(ğ‘ ),
whereîˆ³isthedistributionofthefunctionğ‘€ğ‘“,îˆ°(1ğ‘›,1ğ‘ ).
When thelabel spaceî‰… is ametric space, it makessense toconsider a morenuanced notion of
security. Here,theguaranteeisthatthemitigatoroutputsalabel ğ‘¦âˆ— that,whilenotindistinguish-
able from the â€œidealâ€ label, is promised to be â€œclose enoughâ€ to the ideal label according to the
metricon î‰…. Following isa formalizationof thisnotion ofsecurity forlocal mitigatorswithlabel
spaceî‰… = â„.
Definition 4.6 (Cutoff Loss Security). Let î‰„ be a set. For every index ğ‘› âˆˆ â„•, let î‰„ âŠ† î‰„ be a set
ğ‘›
andletğ”» âŠ† Î”(î‰„ Ã—â„)beacollectionofdistributions. Letğ”» = {ğ”» } ,andletğœ€,ğ›¿ ,ğ›¿ â‰¥ 0. Alocal
ğ‘› ğ‘› ğ‘› ğ‘›âˆˆâ„• 0 1
mitigatorğ‘€ is(ğœ€,ğ›¿ â†’ ğ›¿ )-cutofflosssecurefordistributionsğ”»if:
0 1
âˆƒanegligiblefunctionğœ‡ âˆˆ negl
âˆ€indexğ‘› âˆˆ â„•âˆ€populationdistributionîˆ° âˆˆ ğ”»
ğ‘›
easilydefineîˆ³inbothcasesasadistributionoverdeterministicfunctions.Todothat,wewoulddefineîˆ³inthecase
oflocalmitigationasthedistributionofthefunctionğ‘” suchthatğ‘”(ğ‘¥)=ğ‘€ğ‘“(ğ‘¥,ğ‘,ğ‘Ÿ,1ğ‘›,1ğ‘ ),whereğ‘ âˆ¼îˆ°ğ‘˜ isani.i.d.
sample,ğ‘˜ âˆˆâ„•isthemaximumnumberofsamplesusedbyğ‘€,andğ‘Ÿ isthevectorofrandomcoinsusedbyğ‘€.Forany
fixed(ğ‘,ğ‘Ÿ),ğ‘” isadeterministicfunction;îˆ³isthedistributionoverfunctionsğ‘” correspondingtoarandomchoiceof
(ğ‘,ğ‘Ÿ).Thisformulationisnotmeaningfullydifferentfromtheformulationweuseinthispaper,andallresultswould
bethesame. WeoptfortheformulationasinRemark4.4simplybecausethenotationisslightlycleanerwithout
explicitparametersof(ğ‘,ğ‘Ÿ).
17âˆƒafunctionğ‘”ideal âˆ¶ î‰„ â†’ â„
îˆ° ğ‘›
âˆ€securityparameterğ‘  âˆˆ â„•
î‰„
âˆ€functionğ‘“ âˆˆ â„ ğ‘› withâ„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[|ğ‘“(ğ‘¥)âˆ’ğ‘¦| â‰¥ ğ›¿ 0] â‰¤ ğœ€
Thefollowingtwoconditionshold:
1. Accuracy. â„™ [|ğ‘¦âˆ— âˆ’ğ‘¦| â‰¥ ğ›¿ ] â‰¤ ğœ€ +ğœ‡(ğ‘ ).
1
(ğ‘¥,ğ‘¦)âˆ¼îˆ°
ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )
2. CutoffLossSecurity. âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°â„™ (ğ‘¥âˆ—,1ğ‘›,1ğ‘ )[| |ğ‘¦âˆ— âˆ’ğ‘” îˆ°ideal(ğ‘¥âˆ—)| | â‰¥ ğ›¿ 1] â‰¤ ğœ‡(ğ‘ ).
Furthermore,wesaythatğ‘€ isunbiasedifğ‘€ satisfiesthefollowingguarantee:
3. MeanSecurity. ğ‘€ isanunbiasedestimatorofğ‘”ideal suchthat
îˆ°
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ ğ”¼ [ğ‘¦âˆ—] = ğ‘”ideal(ğ‘¥âˆ—).
ğ‘› ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥âˆ—,1ğ‘›,1ğ‘ ) îˆ°
4.3 The Cutoff Dissimilarity Function
Definition 4.6is essentially obtainedby instantiatingDefinition 4.3 withthe choicesğ¿(0) = ğ¿>ğ›¿0,
ğ¿(1) = ğ¿>ğ›¿1,andğœ† = CutoffDist ,asinthefollowingdefinition.21
ğ›¿1
Definition4.7. Letî‰„ âŠ† â„ğ‘› beasetandletğ›¿ â‰¥ 0. Theğ›¿-cutoffdistributiondissimilarityfunctionfor
randomizedfunctionsî‰„ â†’ Î”(â„)isdefinedasfollows. Foranyrandomizedfunctionsğ‘“,ğ‘“â€² âˆ¶ î‰„ â†’
Î”(â„),
CutoffDist ğ›¿(ğ‘“,ğ‘“â€²) = sup â„™[| |ğ‘“(ğ‘¥)âˆ’ğ‘“â€²(ğ‘¥)|
|
> ğ›¿],
ğ‘¥âˆˆî‰„
wheretheprobabilityisovertherandomnessofğ‘“(ğ‘¥)andğ‘“â€²(ğ‘¥).
However,Definition4.6isslightlysimplifiedcomparedtoastrictapplicationofDefinition4.3,as
follows.
â€¢ The distribution dissimilarity function ğœ† in Definition 4.3 is defined over distributions of
randomizedfunctions. However,asexplainedinRemark4.4,alocalmitigatoroutputsafixed
randomizedfunction(i.e.,adegenerate,singletondistributionoverrandomizedfunctions). So
ourdefinitionsimplifiesaccordingly,toanotionofdissimilaritydefinedforfixedrandomized
functions.
â€¢ IfweweretoapplyDefinition4.3withlossğ¿(1) = ğ¿>ğ›¿1 directly,Item1inDefinition4.6would
insteadstatethat
â„™ â„™ [|ğ‘”(ğ‘¥)âˆ’ğ‘¦| â‰¥ ğ›¿ ] â‰¥ ğœ€ â‰¤ ğœ‡(ğ‘ ), (1)
ğ‘”âˆ¼îˆ³[ (ğ‘¥,ğ‘¦)âˆ¼îˆ° 1 ]
whereîˆ³isthedistributionofthefunctionğ‘”(ğ‘¥) = ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ ),sosamplingğ‘” âˆ¼ îˆ³means
samplingboth(i)theinternalrandomnessneededbyğ‘€,and(ii)thesamplesfromîˆ°needed
byğ‘€. Weviewthisdefinitionasslightlycumbersome,soinsteadweadoptItem1. Notethat
Eq.(1)impliesItem1byaunionbound.
21Themeansecurityrequirement(Item3)isanoptionaladditionalguaranteethatdoesnotfollowfromDefinition4.3,
andmakessenseonlywhenî‰… =â„.
184.3.1 SecurityImplicationsoftheCutoffDissimilarityFunction
We now show that cutoff loss security implies a notion of security that we call weak local TV
security.
Definition4.8. Letî‰„,î‰… besets. ThelocalTVdistributiondissimilarityfunctionforrandomized
functionsî‰„ â†’ Î”(î‰…)isdefinedasfollows. Foranyrandomizedfunctionsğ‘“,ğ‘“â€² âˆ¶ î‰„ â†’ Î”(î‰…),
LocalTV(ğ‘“,ğ‘“â€²) = sup TV(ğ‘“(ğ‘¥),ğ‘“â€²(ğ‘¥)).
ğ‘¥âˆˆî‰„
Weclaimthatrandomizedrounding allowsustoconvertCutoffDistboundsintoweakLocalTV
bounds,whilehurtingtheaccuracyonlybyaboundedamount. Specifically,wehavethefollowing
lemma.
Lemma 4.9. For every index ğ‘› âˆˆ â„•, let î‰„ be a set, and let ğ”» âŠ† Î”(î‰„ Ã—â„) be a collection of
ğ‘› ğ‘› ğ‘›
distributions. Let ğ”» = {ğ”» } , and let ğœ€,ğ›¿ ,ğ›¿ â‰¥ 0. Suppose ğ‘€ is an efficient mitigator that is
ğ‘› ğ‘›âˆˆâ„• 0 1
(ğœ€,ğ›¿ â†’ ğ›¿ )-cutofflosssecurefordistributionsğ”». Then,forallğ›½ > 0,thereisanefficientmitigatorğ‘€â€²
0 1
withthefollowing modifiedaccuracyandlosssecurity properties,withthesame order ofquantifiers
asinDefinition4.6:
1. Accuracy. â„™ [|ğ‘¦âˆ— âˆ’ğ‘¦| â‰¥ ğ›¿ (1+ğ›½)] â‰¤ ğœ€ +ğœ‡(ğ‘ ).
1
(ğ‘¥,ğ‘¦)âˆ¼îˆ°
ğ‘¦âˆ—â†ğ‘€â€²ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )
2. Local TV Security. LocalTV(ğ‘”,ğ‘”ideal) â‰¤ ğœ‡(ğ‘ ) + 1, where ğ‘” âˆ¶ î‰„ â†’ Î”(â„) is given by ğ‘”(ğ‘¥) =
îˆ° ğ›½
ğ‘€â€²ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ ).
Proof. Let ğ›¼ = ğ›¿ â‹…ğ›½ âˆˆ â„ be a rounding parameter, and let âŒŠâ‹…âŒ‹ âˆ¶ â„ â†’ ğ›¼â„¤ be the function that
1 ğ›¼
rounds downwards to an integer multiple of ğ›¼, i.e., âŒŠğ‘¥âŒ‹ = âŒŠğ‘¥âŒ‹ â‹… ğ›¼ âˆˆ ğ›¼â„¤. The new mitigator
ğ›¼ ğ›¼
ğ‘€â€² simply runs ğ‘€ to get some value ğ‘¦ âˆˆ â„, samples a random offset ğ‘ âˆ¼ U([0,ğ›¼)), and outputs
âŒŠğ‘¦ +ğ‘âŒ‹ . Supposeğ‘”old-ideal wasthepreviousidealfunctionthathadcutofflosssecurityWedefine
ğ›¼ îˆ°
thenewğ‘”ideal asğ‘”ideal(ğ‘¥) = âŒŠğ‘”old-ideal(ğ‘¥)+ğ‘âŒ‹ whereğ‘ âˆ¼ U([0,ğ›¼)).
îˆ° îˆ° îˆ° ğ›¼
Bydirectpropertiesofthefloorfunction,itisclearthataccuracycanonlygetworsebyanadditive
factorofğ›¼ = ğ›¿ â‹…ğ›½. Toseethatğ‘€â€² satisfieslocalTVsecurity,recallthatbyItem2inDefinition4.6,
1
wehave
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°â„™ (ğ‘¥âˆ—,1ğ‘›,1ğ‘ )[| |ğ‘¦âˆ— âˆ’ğ‘” îˆ°old-ideal(ğ‘¥âˆ—)| | â‰¥ ğ›¿ 1] â‰¤ ğœ‡(ğ‘ ).
Assuming|ğ‘¦âˆ— âˆ’ğ‘”old-ideal(ğ‘¥âˆ—)| < ğ›¿ ,thenweknow
îˆ° 1
ğ›¿ 1
TV(âŒŠğ‘¦âˆ— +ğ‘âŒ‹ ğ›¼,âŒŠğ‘” îˆ°old-ideal(ğ‘¥âˆ—)+ğ‘âŒ‹ ğ›¼) â‰¤ ğ›¼1 = ğ›½,
asbyacouplingargument,theonlywaythetwoquantitiescandifferiswhenğ‘landsinaninterval
thathaslengthatmostğ›¿ . Therefore,byaunionbound,
1
1
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ TV(ğ‘€â€²ğ‘“,îˆ° (ğ‘¥âˆ—,1ğ‘›,1ğ‘ ),ğ‘”ideal(ğ‘¥âˆ—)) â‰¤ ğœ‡(ğ‘ )+ .
îˆ° ğ›½
Thatis,LocalTV(ğ‘”,ğ‘”ideal) â‰¤ ğœ‡(ğ‘ )+ 1,whereğ‘” âˆ¶ î‰„ â†’ Î”(â„)isgivenbyğ‘”(ğ‘¥) = ğ‘€â€²ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ ).
îˆ° ğ›½
195 Some Initial Observations on Secure Backdoor Mitigation
5.1 Secure Backdoor Mitigation Cannot be Distribution Free
Feldman(2009)showedaresultonthepowerofmembershipqueriesinagnosticlearning,which
hassignificantimplicationsforourwork. Insecurebackdoormitigation,thedefenderreceivesa
potentially-backdoored functionğ‘“Ìƒfroman untrustedparty, andqueries thisfunctionat locations
ofitschoosinginordertolearnaâ€œcleanâ€(â€œnearlycanonicalâ€)functionğ‘” thathasgoodaccuracy.
Forthisprocesstomakesense,weinsistthatbackdoormitigationbemoreefficientthanlearning
aâ€œcleanâ€functionfromscratchusingrandomsamples.
But why would it bepossible for backdoor mitigation to be more efficient? The basic reason is
that whenlearning from scratch, thelearner has accessonly to i.i.d. randomsamples., whereas in
backdoormitigation,thedefenderalsohasoracle(membershipquery)accesstothe(suspectbutstill
useful)functionğ‘“Ìƒ. Thisraisesthequestionofwhen,ifatall,canlearningusingoracleaccesstoa
labelingfunctionbemoreefficientthanlearningusingi.i.d.samples? Feldman(2009)showedthat
ingeneral,fordistribution-freelearning,oracleaccessdoesnothelp,asinthefollowingtheorem.
Theorem 5.1 (Theorem 6 in Feldman, 2009). Let ğ‘š,ğ‘,ğ‘¡ âˆ¶ [0,1]2 â†’ â„•. Let î‰„ be a set, and let
îˆ´ âŠ† {Â±1}î‰„ beaclassoffunctionswithVCdimensionğ‘‘ âˆˆ â„•. Assumeğ´isanalgorithmsuchthatfor
everyğœ€,ğ›¿ âˆˆ [0,1],ğ´agnosticallyPAClearnsîˆ´ withparameters(ğœ€,ğ›¿). ğ´usesğ‘š(ğœ€,ğ›¿)i.i.d.samples
fromadistributionîˆ°thatgenerateslabeledexamplesoftheform(ğ‘¥,ğ‘“(ğ‘¥))forsomelabelingfunction
ğ‘“ âˆ¶ î‰„ â†’ {Â±1},aswellasusingğ‘(ğœ€,ğ›¿)oraclequeriestoğ‘“,andğ´runsintimeğ‘¡(ğœ€,ğ›¿).
Thenthereexistsanalgorithmğ´â€² thatusesonlyi.i.d.samplesfromîˆ°(anddoesnotrequireoracle
access),suchthatğ´â€² agnosticallyPAClearnsîˆ´ withparameters(ğœ€,ğ›¿)withrespecttothesametypes
ofdistributionsîˆ°. ğ´â€² uses
ğ‘‘log(ğ‘‘/ğœ€)+log(1/ğ›¿)
ğ‘šâ€²(ğœ€,ğ›¿) = ğ‘‚
( ğœ€2 )
i.i.d.samplesfromîˆ°,andğ´â€² runsintimeğ‘¡â€²(ğœ€,ğ›¿) = ğ‘¡(ğœ€/2,ğ›¿/2)+ğ‘šâ€²(ğœ€,ğ›¿).
ProofSketch. Takeasampleğ‘†consistingofğ‘šâ€²(ğœ€,ğ›¿)i.i.d.samplesfromîˆ°. Byuniformconvergence,
ğ‘† is an ğœ€/2-sample for îˆ°, namely, |â„™ [ğ‘ âˆˆ ğ¸]âˆ’â„™ [ğ‘ âˆˆ ğ¸]| â‰¤ ğœ€/2 for every event ğ¸. Now
ğ‘âˆ¼U(ğ‘†) ğ‘âˆ¼îˆ°
simulate ğ´ with parameters (ğœ€/2,ğ›¿/2) to learn the distribution U(ğ‘†) and obtain a classifier â„ âˆ¶
î‰„ â†’ {Â±1}(wehavefullknowledgeofğ‘†,sowecansimulateoraclequeriestothelabelingfunction
of U(ğ‘†)). Then ğ¿0-1(ğ‘”) â‰¤ ğ¿0-1 (ğ‘”) + ğœ€/2 â‰¤ ğœ€, where the first inequality holds because ğ‘† is an
îˆ° U(ğ‘†)
ğœ€/2-sampleandthesecondfollowsfromthecorrectnessofğ´.
Theconclusionisthat,forgeneraldistributions,learningwithoracleaccesscannotbefasterthan
learning from random samples, up to constant factors. To see that this implies that backdoor
mitigationcannotingeneralbefasterthanlearningfromrandomsamples,simplyconsiderthe
casewherethepotentially-backdooredğ‘“Ìƒiscompletelyhonest,namely,ğ‘“Ìƒispreciselythelabeling
functionofthepopulationdistributionîˆ°. Thisamountstolearningîˆ°withoracleaccesstothe
20true labeling function, which, as we saw, is no easier in general than learning îˆ° from random
samples.22
However,Theorem5.1doesnotimplythatoracleaccessisnotusefulwhenlearningwithrespect
tospecificfamiliesofdistributions,e.g.,learningwithrespecttodistributionsthathaveauniform
marginalonthedomain(acasewefocusoninthispaper). Thereasonisthatthereductioninthe
proofofTheorem5.1executesğ´onthedistributionU(ğ‘†)â€“whichisveryfarfromuniformonthe
domainî‰„.
Infact,thelearningparitywithnoise (LPN)assumption,whichisawidely-believedassumptionin
moderncryptography, posits thatin some cases(e.g., learningparity functions), learningfrom
noisyrandomsamplesiscomputationallyinfeasible,butlearningwithoracleaccesstoalabeling
functions iseasy (e.g., for parityfunctions efficient learningis provided bythe Goldreichâ€“Levin
algorithm,seeTheoremA.1).
Thisisonejustificationforourfocusinthispaperonbackdoormitigationwithrespecttospecific
â€œniceâ€classesofdistributions. SeefurtherdiscussioninSection2.1.3.
5.2 Lower Bound for General Mitigation
Here,weshowthattherearecollectionsofdistributionsğ”»wheremitigationisnotmoreefficient
thandirectly(re-)learningfromrandomsamples. Forconcreteness,wewillconsiderDefinition4.5.
The intuition here is that the only advantage global mitigation has, as compared to directly
learning, is membership queries from (a possibly corrupted) hypothesis ğ‘“. However, there are
natural choices ofpopulation distributions for whichmembership queries and sample accessare
equally powerful. In particular, if we look at functions with no structure, then seeing labeled
examples, either by samples or membership queries, does not help prediction for new, unseen
inputs.
Christianoet al.(2024)showsomethingsimilar inthewhitebox settingusingindistinguishability
obfuscationandpuncturablepseudorandomfunctions. However,sinceallofourtechniquesarein
theblack-boxsetting,wedemonstratethesignificantlysimplerresult,wherethemitigatorgets
onlyinput-outputaccesstoğ‘“.
Formally, in Definition 4.5, consider î‰„ = {0,1}ğ‘›, î‰… = {0,1}. For a function â„ âˆ¶ î‰„ â†’ î‰…, let
ğ‘› ğ‘›
îˆ°
â„
= U({(ğ‘¥,â„(ğ‘¥))}
ğ‘¥âˆˆî‰„
ğ‘›). Letğ”»
ğ‘›
= {îˆ°
â„
âˆ¶ â„ âˆˆ î‰…î‰„ ğ‘›}. Letğ¿(0) = ğ¿(1) = ğ¿0-1.
Lemma5.2(Informal). Intheabovesetting,foranyğœ€ > 0andnegligiblefunctionğœ‡,supposeğ‘€ is
1
amitigatorsuchthat
â„™[ğ¿ îˆ°0-1 (ğ‘€â„,îˆ° â„(1ğ‘›,1ğ‘ )) â‰¤ ğœ€ 1] â‰¥ 1âˆ’ğœ‡(ğ‘ ),
â„
for a randomly chosen â„ âˆ¶ î‰„ â†’ î‰…. Suppose that ğ‘€ uses ğ‘˜ â‰¤ 2ğ‘›/4 samples from îˆ° and makes
ğ‘› â„
ğ‘ â‰¤ 2ğ‘›/4queriestotheoracleâ„. Then,thereisalearnerğ‘€â€² usingğ‘‚(ğ‘˜ +ğ‘)samplesinexpectation
suchthat
â„™[ğ¿ îˆ°0-1 (ğ‘€â€²îˆ° â„(1ğ‘›,1ğ‘ )) â‰¤ ğœ€ 1] â‰¥ 1âˆ’ğœ‡(ğ‘ ).
â„
22Onecouldstillhope,forinstance,thatifthedefenderhaswhiteboxaccesstoğ‘“Ìƒthentheycouldmitigatefaster
thanlearningwithrandomsamples.Inthispaperwefocusonmitigationwithblackboxaccess.
21We emphasize that we are not using the full power of the mitigator in Lemma 5.2, making the
lower bound only stronger. In particular, we are only using Item 2 when plugging in the true
hypothesisâ„asthemembershiporacle,andwearenotusingItem1atall.
ProofsketchofLemma5.2. Since îˆ° has no structure and we are considering the ğ¿0-1 loss, an
â„
optimal choice of mitigator ğ‘€ with ğ‘˜ samples and ğ‘ queries to ğ‘“ would be as follows. Given ğ‘˜
samples,ğ‘€ wouldqueryğ‘ distinctpointsin{0,1}ğ‘› notpresentinthesamples. Then,ğ‘€ canstore
a lookup table correspondingto allsampled andqueried pairs. Forthese inputvalues, it would
outputwhateverisinitstable,andforeverythingelse,itwouldrandomlyguessanelementof
{0,1}. Letğ‘‡ bethenumberofdistinctelementsinthelookuptable. Itisclearthatğ‘‡ â‰¤ ğ‘˜ +ğ‘.
A learner ğ‘€â€² without membership queries can emulate this process except for the ğ‘ queries to
â„. Instead, it can use samples from îˆ° until it has seen ğ‘‡ distinct examples. In this case, the
â„
performancewithrespecttoğ¿0-1 willbeatleastasgoodasğ‘€. Itremainstoseehowmanysamples
ğ‘€â€² needstogenerateğ‘‡ distinctexamples.
Let ğ‘‡â€² denote the number of samples ğ‘€â€² draws. Let ğ‘â€² denote the expected number of distinct
elementsseeninğ‘‡â€² samples. Forğ‘‡â€² â‰¤ 2ğ‘›,bylinearityofexpectation,wehave
ğ”¼[ğ‘â€² ] = 2ğ‘› (1âˆ’ (1âˆ’ 21
ğ‘›)ğ‘‡â€²
) â‰¥ 2ğ‘› (1âˆ’ğ‘’âˆ’ 2ğ‘‡ ğ‘›â€² ) â‰¥ 2ğ‘› (1âˆ’ (1âˆ’ 2ğ‘‡ â‹…2â€² ğ‘›)) = ğ‘‡ 2â€² ,
wherewehaveusedtheinequalityğ‘’âˆ’ğ‘¥ < 1âˆ’ğ‘¥ forallğ‘¥ âˆˆ [0,1]. Therefore,settingğ‘‡â€² = 2(ğ‘˜+ğ‘) â‰¤ 2ğ‘›,
2
wehaveğ‘‡â€² â‰¥ 2ğ‘‡,andtherefore,ğ”¼[ğ‘â€²] â‰¥ ğ‘‡.
6 Global Mitigation
In this section we show a mitigation result for potentially-backdoored functions {Â±1}ğ‘› â†’ {Â±1},
whichreliesontwomainassumptions:
1. Thepotentially-backdooredfunctionhaslowpopulationloss.
2. The population distribution is â€œniceâ€ in the sense that the marginal over the domain is
uniform,andthelabelsareclosetoaFourier-heavyfunction. (However,thelabelsneednot
bedeterministic. Namely,foranğ‘¥ inthedomain,itispossiblethatboththelabels1andâˆ’1
havepositiveprobability).
6.1 Fourier Analysis Preliminaries
FollowingaresomebasicnotionsfromFourieranalysisofBooleanfunctions,whichareusedin
thispaper. SeeOâ€™Donnell(2014)foracomprehensiveintroduction.
Theexpectationinnerproduct. Givenameasurespace,letîˆ¸ bethesetofreal-valuedrandom
2
variablesoverthemeasurespacewithfinitesecondmoment. Thenîˆ¸ isavectorspaceoverthe
2
reals. ThefunctionâŸ¨â‹…,â‹…âŸ© âˆ¶ îˆ¸ Ã—îˆ¸ â†’ â„givenby
2 2
âŸ¨ğ‘‹,ğ‘ŒâŸ© = ğ”¼[ğ‘‹ğ‘Œ]
âˆš
isaninnerproduct. Inparticular,â€–ğ‘‹â€– = âŸ¨ğ‘‹,ğ‘‹âŸ©isanorm.
22Fouriercharactersandcoefficientson theBooleanhypercube. Letğ‘› âˆˆ â„•andletî‰„ = â„ğ‘›.
Weidentifyanyfunctionğ‘“ âˆ¶ î‰„ â†’ â„withtherandomvariableğ‘“(ğ‘‹),whereğ‘‹ isuniformoverî‰„.
Inparticular,âŸ¨ğ‘“,ğ‘”âŸ© = ğ”¼ [ğ‘“(ğ‘‹)ğ‘”(ğ‘‹)]foranyğ‘“,ğ‘” âˆˆ â„î‰„.
ğ‘‹âˆ¼U(î‰„)
Foranyğ‘† âŠ† [ğ‘›],thecharacter ofğ‘† isthefunctionî‰„ â†’ {Â±1}givenby
ğœ’ (ğ‘¥) = âˆğ‘¥,
ğ‘† ğ‘–
ğ‘–âˆˆğ‘†
whereğ‘¥ = (ğ‘¥ ,â€¦,ğ‘¥ ).
1 ğ‘›
Theset{ğœ’ (ğ‘¥) âˆ¶ ğ‘† âŠ† [ğ‘›]}isanorthonormalbasisofthespaceofallfunctionsî‰„ â†’ â„. Inparticular,
ğ‘†
everyfunctionğ‘“ âˆ¶ î‰„ â†’ â„hasauniquerepresentation
ğ‘“(ğ‘¥) = âˆ‘ ğ‘“Ì‚ (ğ‘†)ğœ’ (ğ‘¥)
ğ‘†
ğ‘†âŠ†[ğ‘›]
whereğ‘“Ì‚ (ğ‘†) = âŸ¨ğ‘“,ğœ’ âŸ©istheğ‘†-coefficient,orweightofğ‘†inğ‘“. Weindicatesetsofcharacterswithspeci-
ğ‘† { } { }
| | | |
fiedweightsusingnotationsuchasğ‘“Ì‚â‰¥ğ›¼ = ğ‘† âŠ† [ğ‘›] âˆ¶ |ğ‘“Ì‚ (ğ‘†)| â‰¥ ğ›¼ andğ‘“Ì‚=ğ›½ = ğ‘† âŠ† [ğ‘›] âˆ¶ |ğ‘“Ì‚ (ğ‘†)| = ğ›½ .
| | | |
Theorem6.1(Parsevalâ€™sidentity). Foranyğ‘› âˆˆ â„•andğ‘“ âˆ¶ {Â±1}ğ‘› â†’ â„,
â€–ğ‘“â€–2 = âŸ¨ğ‘“,ğ‘“âŸ© = ğ”¼ ğ‘¥âˆ¼U({Â±1}ğ‘›)[ğ‘“(ğ‘¥)2 ] = âˆ‘ ğ‘“Ì‚ (ğ‘†)2.
ğ‘†âŠ†[ğ‘›]
6.2 Mitigation for Fourier-Heavy Functions
Definition6.2. Letğ‘›,ğ‘¡ âˆˆ â„•,î‰„ = {Â±1}ğ‘›,andğœ â‰¥ 0. TheclassofFourierğœ-heavyfunctionsis
{ }
îˆ´ = ğ‘“ âˆˆ â„î‰„ âˆ¶ âˆ€ğ‘† âŠ† [ğ‘›] âˆ¶ | |ğ‘“Ì‚ (ğ‘†)| | â‰¥ ğœ âˆ¨ ğ‘“Ì‚ (ğ‘†) = 0 .
â‰¥ğœ ( | | )
Similarly,theclassofbinary-valuedFourierğœ-heavyfunctionsis
{ }
îˆ´Â±1 = ğ‘“ âˆˆ {Â±1}î‰„ âˆ¶ âˆ€ğ‘† âŠ† [ğ‘›] âˆ¶ | |ğ‘“Ì‚ (ğ‘†)| | â‰¥ ğœ âˆ¨ ğ‘“Ì‚ (ğ‘†) = 0 âŠ† îˆ´ .
â‰¥ğœ ( | | ) â‰¥ğœ
{ }
| |
TheclassofFourierğ‘¡-sparsefunctionisîˆ´ = ğ‘“ âˆˆ â„î‰„ âˆ¶ |ğ‘“Ì‚>0| â‰¤ ğ‘¡ .
ğ‘¡ | |
Theorem6.3(TVglobalmitigationforFourier-heavyfunctions). Letğ‘›,ğ‘  âˆˆ â„•,letğœ > 0,ğœ€ â‰¤ (ğœ/6)2
0
and ğœ€ > ğœ€ , let î‰„ = {Â±1}ğ‘›. Let ğ”» âŠ† Î”(î‰„ Ã—[âˆ’1,1]) be a collection of distributions îˆ° with uniform
1 0
marginalonî‰„ suchthatğ¿2 (îˆ´ ) â‰¤ ğœ€ . ThenAlgorithm1isaglobalmitigatorthatis
îˆ° â‰¥ğœ 0
(ğ¿2,ğœ€ 0) â†’ (ğ¿2,ğœ€ 1)
totalvariationsecureforğ”»(Definition4.5). Algorithm1uses
ğ‘  +log(1/ğœ)
ğ‘š = ğ‘‚
( ğœ2(ğœ€ âˆ’ğœ€ ) )
1 0
i.i.d. samples from a distribution îˆ° âˆˆ ğ”» and poly(ğ‘›,1/ğœ,ğ‘ ) oracle queries to an arbitrary function
ğ‘“ âˆ¶ î‰„ â†’ [âˆ’1,1]withlossğ¿2 (ğ‘“) â‰¤ ğœ€ .
îˆ° 0
23fË†(S)
| |
Ï„
[n]
S 1 S 2 S 2n âŠ†
Â·Â·Â·
Figure3: Inağœ-heavyfunctionğ‘“,allnon-zeroFouriercoefficientshaveabsolutevalueatleastğœ.
Remark 6.4. The assumption in Theorem 6.3 that ğœ€ is strictly less than ğœ2 (e.g., ğœ€ â‰¤ (ğœ/6)2)
0 0
appears tobe necessarywhen usingour Goldreichâ€“Levin-basedtechnique. Supposethat instead,
wetookğœ€ = ğœ2. Considerafunctionğ‘¦ âˆ¶ î‰„ â†’ â„withtwodistinctcoefficientsğ‘† ,ğ‘† âŠ† [ğ‘›]suchthat
0 1 2
ğ‘¦Ì‚(ğ‘†) = ğœ for ğ‘– âˆˆ {1,2}. Let ğ‘“ (ğ‘¥) = âˆ‘ ğ‘¦Ì‚(ğ‘†)ğœ’ (ğ‘¥). Then for îˆ° = U({(ğ‘¥,ğ‘¦(ğ‘¥)) âˆ¶ ğ‘¥ âˆˆ î‰„}),
ğ‘– âˆ’ğ‘– ğ‘†âŠ†[ğ‘›]âˆ¶ğ‘†â‰ ğ‘†ğ‘– ğ‘†
the losses are ğ¿2 (ğ‘“ ) = ğ¿2 (ğ‘“ ) = ğœ2 â‰¤ ğœ€ . Hence, the algorithm would be required to return
îˆ° âˆ’1 îˆ° âˆ’2 0
the same output distribution on query access to either of ğ‘“ and ğ‘“ . However, executing the
âˆ’1 âˆ’2
Goldreichâ€“Levinalgorithmonğ‘“ willnotrecoverğ‘†. Themissingcoefficient(thatisnotrecovered)
âˆ’ğ‘– ğ‘–
dependsonthefunctionğ‘“ beingqueried. Thissuggeststhatifincreasingtheupperboundbeyond
âˆ’ğ‘–
ğœ2 ispossible,thendoingsorequiresnewideas.
Corollary6.5. Letğ‘›,ğ‘  âˆˆ â„•,letğœ > 0,ğœ€ â‰¤ (ğœ/12)2andğœ€ > 4ğœ€ ,letî‰„ = {Â±1}ğ‘›. Letğ”» âŠ† Î”(î‰„ Ã—{Â±1})
0 1 0
beacollectionof distributionsîˆ°withuniformmarginal onî‰„ suchthatğ¿0 îˆ°-1 (îˆ´ â‰¥Â± ğœ1 ) â‰¤ ğœ€ 0. Then,the
compositionofAlgorithm1andsign(â‹…)isaglobalmitigatorthatis
(ğ¿0-1,ğœ€ 0) â†’ (ğ¿0-1,ğœ€ 1)
totalvariationsecureforğ”»(Definition4.5). Thealgorithmuses
ğ‘  +log(1/ğœ)
ğ‘š = ğ‘‚
( ğœ2(ğœ€ âˆ’4ğœ€ ) )
1 0
i.i.d. samples from a distribution îˆ° âˆˆ ğ”» and poly(ğ‘›,1/ğœ,ğ‘ ) oracle queries to an arbitrary function
ğ‘“ âˆ¶ î‰„ â†’ {Â±1}withlossğ¿0-1(ğ‘“) â‰¤ ğœ€ .
îˆ° 0
ProofofCorollary6.5. ThisfollowsfromClaimB.4andTheorem6.3,whenappliedwith4ğœ€ instead
0
ofğœ€ . Specifically,
0
ğ¿2 îˆ°(îˆ´ â‰¥ğœ) = 4ğ¿0 îˆ°-1(îˆ´ â‰¥ğœ) â‰¤ 4ğ¿0 îˆ°-1 (îˆ´ â‰¥Â± ğœ1 ) â‰¤ 4ğœ€ 0,
ğ¿2 (ğ‘“) = 4ğ¿0-1(ğ‘“) â‰¤ 4ğœ– ,
îˆ° îˆ° 0
24wherewehaveusedthefactthatîˆ´Â±1 âŠ† îˆ´ . Therefore,Algorithm1isaglobalmitigatorthatis
â‰¥ğœ â‰¥ğœ
(ğ¿2,4ğœ€ ) â†’ (ğ¿2,ğœ€ )totalvariationsecureforğ”». ApplyingItem2ofDefinition4.5,asguaranteed
0 1
byTheorem6.3,wehave
â„™ ğ‘”âˆ¼îˆ³[ğ¿2 îˆ°(ğ‘”) â‰¤ ğœ€ 1] â‰¥ 1âˆ’ğœ‡(ğ‘ ),
whereîˆ³isthedistributionofthefunctionğ‘€ğ‘“,îˆ°(1ğ‘›,1ğ‘ ). Sinceğ¿0-1(sign(ğ‘”)) â‰¤ ğ¿2 (ğ‘”)byClaimB.4,
îˆ° îˆ°
itfollowsthat
â„™ ğ‘”âˆ¼îˆ³[ğ¿0 îˆ°-1(sign(ğ‘”)) â‰¤ ğœ€ 1] â‰¥ 1âˆ’ğœ‡(ğ‘ ),
asdesired.
Assumptions:
â€¢ ğ‘ ,ğ‘› âˆˆ â„•;ğœ > 0;ğœ€ < ğœ€ ;ğœ€ â‰¤ (ğœ/6)2;î‰„ = {Â±1}ğ‘›.
0 1 0
â€¢ îˆ° âˆˆ Î”(î‰„ Ã—[âˆ’1,1])withîˆ° = U(î‰„)andğ¿2 (îˆ´ ) â‰¤ ğœ€ .
î‰„ îˆ° â‰¥ğœ 0
â€¢ Thealgorithmhasoracleaccesstoanarbitraryfunctionğ‘“ âˆˆ â„î‰„ (notnecessarilyin
îˆ´ )suchthatğ¿2 (ğ‘“) â‰¤ ğœ€
â‰¥ğœ îˆ° 0
â€¢ ğ‘š = ğ‘‚((ğ‘  +log(1/ğœ))/(ğœ2(ğœ€
1
âˆ’ğœ€ 0))).
â€¢ ğ‘ = ((ğ‘¥ 1,ğ‘¦ 1),â€¦,(ğ‘¥ ğ‘š,ğ‘¦ ğ‘š)) âˆ¼ îˆ°ğ‘š.
FourierHeavyMitigator(ğ‘,ğœ,ğ‘›):
ExecutetheGoldreichâ€“Levinalgorithm(TheoremA.1)usingoracleaccesstoğ‘“ to
obtainacollectionîˆ¿ âŠ† 2[ğ‘›] suchthatâ„™ ğ‘“Ì‚â‰¥2ğœ/3 âŠ† îˆ¿ âŠ† ğ‘“Ì‚â‰¥ğœ/2 â‰¥ 1âˆ’negl(ğ‘ )
[ ]
for ğ‘† âˆˆ îˆ¿:
ğ‘”(ğ‘†) â† 1 âˆ‘ ğœ’ (ğ‘¥)â‹…ğ‘¦
ğ‘š ğ‘–âˆˆ[ğ‘š] ğ‘† ğ‘– ğ‘–
returnthefunctionğ‘”(ğ‘¥) = âˆ‘ ğ‘”(ğ‘†)â‹…ğœ’ (ğ‘¥)
ğ‘†
ğ‘†âˆˆîˆ¿
Algorithm1: AnindependentglobalmitigatorforFourier-heavyfunctions.
6.2.1 ProofofTheorem6.3
Westartwithsomebasicobservations.
Notation6.6. Forğ‘› âˆˆ â„•,î‰„ = {Â±1}ğ‘› andîˆ° âˆˆ Î”(î‰„ Ã—â„),denoteğ‘¦ îˆ°(ğ‘¥) = ğ”¼ (ğ‘‹,ğ‘Œ)âˆ¼îˆ°[ğ‘Œ |ğ‘‹ = ğ‘¥].
Claim 6.7 (Loss decomposition). Let î‰„ be a set, and let îˆ° âˆˆ Î”(î‰„ Ã—â„). Then for any function
ğ‘Ÿ âˆ¶ î‰„ â†’ â„,
ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ] = ğ”¼ ğ‘¥âˆ¼îˆ° î‰„[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦ îˆ°(ğ‘¥))2 ]+ğ‘‰(ğ·),
whereğ‘‰(îˆ°) = ğ”¼ ğ‘¥âˆ¼îˆ° î‰„[Var ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦]] = ğ”¼ ğ‘¥âˆ¼îˆ° î‰„[ğ”¼ ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦2 ]âˆ’ğ”¼ ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦]2 ].
25Proof. ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ] = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[ğ‘Ÿ(ğ‘¥)2 âˆ’2ğ‘Ÿ(ğ‘¥)ğ‘¦ +ğ‘¦2 ]
= ğ”¼ ğ‘¥âˆ¼îˆ° î‰„[ğ‘Ÿ(ğ‘¥)2 âˆ’2ğ‘Ÿ(ğ‘¥)ğ‘¦ îˆ°(ğ‘¥)+ğ”¼ ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦2 ]]
= ğ”¼ ğ‘¥âˆ¼îˆ° î‰„[ğ‘Ÿ(ğ‘¥)2 âˆ’2ğ‘Ÿ(ğ‘¥)ğ‘¦ îˆ°(ğ‘¥)+ğ‘¦ îˆ°(ğ‘¥)2 +ğ”¼ ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦2 ]âˆ’ğ‘¦ îˆ°(ğ‘¥)2 ]
= ğ”¼
ğ‘¥âˆ¼îˆ°
î‰„[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦ îˆ°(ğ‘¥))2 ]+ğ‘‰(îˆ°).
Fact6.8. Letğ‘› âˆˆ â„•,î‰„ = {Â±1}ğ‘› andîˆ° âˆˆ Î”(î‰„ Ã—â„)withîˆ° î‰„ = U(î‰„). Then
âˆ€ğ‘† âŠ† [ğ‘›] âˆ¶ ğ‘¦Ì‚(ğ‘†) = ğ”¼ [ğ‘¦ â‹…ğœ’ (ğ‘¥)].
îˆ° (ğ‘¥,ğ‘¦)âˆ¼îˆ° ğ‘†
Proof. ğ‘¦Ì‚ îˆ°(ğ‘†) = âŸ¨ğ‘¦ îˆ°,ğœ’ ğ‘†âŸ© = ğ”¼ ğ‘¥âˆ¼U(î‰„)[ğ”¼ ğ‘¦âˆ¼îˆ° ğ‘Œ|ğ‘‹=ğ‘¥[ğ‘¦]â‹…ğœ’ ğ‘†(ğ‘¥)] = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[ğ‘¦ â‹…ğœ’ ğ‘†(ğ‘¥)].
WearenowpreparedtoproveTheorem6.3.
ProofofTheorem6.3. First,bytheGoldreichâ€“Levintheorem(TheoremA.1),poly(ğ‘›,1/ğœ,ğ‘ )queries
toğ‘“ areindeedsufficienttoguaranteethat
â„™ ğ‘“Ì‚â‰¥2ğœ/3 âŠ† îˆ¿ âŠ† ğ‘“Ì‚â‰¥ğœ/2 â‰¥ 1âˆ’negl(ğ‘ ). (2)
[ ]
ByParsevalâ€™sidentityandtheinclusionîˆ¿ âŠ† ğ‘“Ì‚â‰¥ğœ/2 inEq.(2),itfollowsthat
â„™[|îˆ¿| â‰¤ 4/ğœ2 ] â‰¥ 1âˆ’negl(ğ‘ ) (3)
Second,byFact6.8andHoeffdingâ€™sinequality,foreveryğ‘† âˆˆ îˆ¿,
ğ‘’âˆ’ğ‘ 
â„™ ğ‘âˆ¼îˆ°ğ‘š[| |ğ‘”(ğ‘†)âˆ’ğ‘¦Ì‚ îˆ°(ğ‘†)|
|
â‰¤ Î”] â‰¥ 1âˆ’ 4/ğœ2, (4)
âˆš
where Î” = ğœ2(ğœ€1âˆ’ğœ€0), and we used the fact that ğ‘š â‰¥ Î©((ğ‘  +ln(1/ğœ))/Î”2 ). From Eqs. (3) and (4)
4
andtheunionbound,
â„™[âˆ€ğ‘† âˆˆ îˆ¿ âˆ¶ | |ğ‘”(ğ‘†)âˆ’ğ‘¦Ì‚ îˆ°(ğ‘†)|
|
â‰¤ Î”] â‰¥ 1âˆ’negl(ğ‘ ), (5)
wheretheprobabilityisoverthechoiceofthesampleğ‘ andtherandomnessoftheGoldreichâ€“Levin
algorithm.
Third,letğœ€ = ğ¿2 (îˆ´ ) â‰¤ ğœ€ ,andletâ„ âˆˆ îˆ´ suchthatğ¿2 (â„) = ğœ€ .23 Weshowthat
îˆ´ îˆ° â‰¥ğœ 0 â‰¥ğœ îˆ° îˆ´
â„™ Ì‚ â„â‰¥ğœ = îˆ¿ â‰¥ 1âˆ’negl(ğ‘ ), (6)
[ ]
where the probability is over the randomness of Goldreichâ€“Levin. This follows from the high
agreementbetweenğ‘“ andâ„,asfollows.
2
âˆ‘ (ğ‘“Ì‚ (ğ‘†)âˆ’Ì‚ â„(ğ‘†) ) = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘“(ğ‘¥)âˆ’â„(ğ‘¥))2 ]
ğ‘†âŠ†[ğ‘›]
23Toseethatsuchanâ„exists,identifyeachfunctioninîˆ´ withitsvectorofFouriercoefficientsinâ„2ğ‘›.Notethat
â‰¥ğœ
îˆ´ isacompactsetinâ„2ğ‘›.ByClaim6.7andParsevalâ€™sidentity,thefunctionğ¿2 iscontinuous.Existencefollows
â‰¥ğœ îˆ°
fromtheextremevaluetheorem.
26= ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘“(ğ‘¥)âˆ’ğ‘¦ +ğ‘¦ âˆ’â„(ğ‘¥))2 ]
â€– â€–2
= (ğ‘“ âˆ’ğ‘¦)+(ğ‘¦ âˆ’â„)
â€– â€–
â€– â€– â€– â€– 2
â‰¤ (â€–ğ‘“ âˆ’ğ‘¦ â€–+ â€–ğ‘¦ âˆ’â„ â€–)
= ((ğ¿2 îˆ°(ğ‘“))1/2 +(ğ¿2 îˆ°(â„))1/2 )2 â‰¤ 4ğœ€ 0.
Inparticular,
| | âˆš
Ì‚ Ì‚
âˆ€ğ‘† âŠ† [ğ‘›] âˆ¶ |ğ‘“(ğ‘†)âˆ’â„(ğ‘†)| â‰¤ 2 ğœ€ â‰¤ ğœ/3.
| | 0
Consequently,foranyğ‘†,ifÌ‚ â„(ğ‘†) â‰¥ ğœ thenğ‘“Ì‚ (ğ‘†) â‰¥ 2ğœ/3,so
Ì‚ â„â‰¥ğœ âŠ† ğ‘“Ì‚â‰¥2ğœ/3. (7)
Ontheotherhand,foranyğ‘†,ifğ‘“Ì‚ (ğ‘†) â‰¥ ğœ/2thenÌ‚ â„(ğ‘†) > 0. Becauseâ„ âˆˆ îˆ´ ,thisimpliesÌ‚ â„(ğ‘†) â‰¥ ğœ,
â‰¥ğœ
so
ğ‘“Ì‚â‰¥ğœ/2 âŠ†Ì‚ â„â‰¥ğœ. (8)
CombiningEqs.(2),(7)and(8)yields
â„™ Ì‚ â„â‰¥ğœ âŠ† ğ‘“Ì‚â‰¥2ğœ/3 âŠ† îˆ¿ âŠ† ğ‘“Ì‚â‰¥ğœ/2 âŠ†Ì‚ â„â‰¥ğœ â‰¥ 1âˆ’negl(ğ‘ ),
[ ]
asdesired.
Fourth,weconsiderthelossesofâ„andğ‘”.
ğ¿2 îˆ°(â„) = ğ”¼
ğ‘‹âˆ¼îˆ°
î‰„[(â„(ğ‘‹)âˆ’ğ‘¦ îˆ°(ğ‘‹))2 ]+ğ‘‰(îˆ°) (ByClaim6.7)
2
= âˆ‘ Ì‚ â„(ğ‘†)âˆ’ğ‘¦Ì‚(ğ‘†) +ğ‘‰(îˆ°), (Parsevalâ€™sidentity) (9)
( îˆ° )
ğ‘†âŠ†[ğ‘›]
ğ¿2 îˆ°(ğ‘”) = âˆ‘(ğ‘”Ì‚(ğ‘†)âˆ’ğ‘¦Ì‚ îˆ°(ğ‘†))2 + âˆ‘ (ğ‘”Ì‚(ğ‘†)âˆ’ğ‘¦Ì‚ îˆ°(ğ‘†))2 +ğ‘‰(îˆ°) (ditto)
ğ‘†âˆˆîˆ¿ ğ‘†âˆˆ2[ğ‘›]â§µîˆ¿
â‰¤ |îˆ¿|â‹…Î”2 + âˆ‘ (ğ‘”Ì‚(ğ‘†)âˆ’ğ‘¦Ì‚(ğ‘†))2 +ğ‘‰(îˆ°) (ByEq.(5),w.p.1âˆ’negl(ğ‘ ))
îˆ°
ğ‘†âˆˆ2[ğ‘›]â§µîˆ¿
= (ğœ€
1
âˆ’ğœ€ 0)+ âˆ‘ (ğ‘”Ì‚(ğ‘†)âˆ’ğ‘¦Ì‚ îˆ°(ğ‘†))2 +ğ‘‰(îˆ°) (ByEq.(3),w.p.1âˆ’negl(ğ‘ ))
ğ‘†âˆˆ2[ğ‘›]â§µîˆ¿
2
= (ğœ€ âˆ’ğœ€ )+ âˆ‘ Ì‚ â„(ğ‘†)âˆ’ğ‘¦Ì‚(ğ‘†) +ğ‘‰(îˆ°) (ğ‘”Ì‚(ğ‘†) =Ì‚ â„(ğ‘†) = 0forğ‘† âˆ‰ îˆ¿
1 0 ( îˆ° )
ğ‘†âˆˆ2[ğ‘›]â§µîˆ¿ byEq.(6)andâ„ âˆˆ îˆ´ â‰¥ğœ,
w.p.1âˆ’negl(ğ‘ ))
2
â‰¤ (ğœ€ âˆ’ğœ€ )+ âˆ‘ Ì‚ â„(ğ‘†)âˆ’ğ‘¦Ì‚(ğ‘†) +ğ‘‰(îˆ°)
1 0 ( îˆ° )
ğ‘†âŠ†[ğ‘›]
= ğœ€ âˆ’ğœ€ +ğœ€ â‰¤ ğœ€ . (ByEq.(9)andchoiceofâ„)
1 0 îˆ´ 1
ThisshowsthatAlgorithm1satisfiestheaccuracyrequirement(Item2)inDefinition4.5.
27Fifth, for thesecurity requirement (Item 1in Definition 4.5), observethat Algorithm 1outputs a
functionğ‘” whichis completelydetermined by(i.e., is afunction of)the tuple(ğ‘,îˆ¿). Thisgives
thefollowingMarkovchain:
ğ‘“ âˆ’îˆ¿ âˆ’(ğ‘,îˆ¿)âˆ’ğ‘”. (10)
However, Eq.(6) states thatîˆ¿ equals thespecific valueÌ‚ â„â‰¥ğœ with probability 1âˆ’negl(ğ‘ ), regardless
ofthespecificchoiceofğ‘“,andthereforethedependenceonğ‘“ isnegligible.
Moreformally,letîˆ³andîˆ³ideal denotethedistributionoftherandomvariableğ‘” whenAlgorithm1
îˆ°
isexecutedwithoracleaccesstofunctionğ‘“ andâ„,respectively. Foranysetğ´,Eq.(10)implies
| |
â„™ ğ‘”âˆ¼îˆ³[ğ‘” âˆˆ ğ´| |îˆ¿ =Ì‚ â„â‰¥ğœ
]
= â„™
ğ‘”âˆ¼îˆ³i
îˆ°deal[ğ‘” âˆˆ ğ´| |îˆ¿ =Ì‚ â„â‰¥ğœ
]
= ğ‘ ğ´.
So
| |
TV(îˆ³,îˆ³i îˆ°deal ) = sup| |â„™ ğ‘”âˆ¼îˆ³[ğ‘” âˆˆ ğ´]âˆ’â„™ ğ‘”âˆ¼îˆ³i îˆ°deal[ğ‘” âˆˆ ğ´]|
|
ğ´
|
| | |
= sup| |â„™ ğ‘”âˆ¼îˆ³[ğ‘” âˆˆ ğ´| |îˆ¿ =Ì‚ â„â‰¥ğœ ]â„™ ğ‘”âˆ¼îˆ³[îˆ¿ =Ì‚ â„â‰¥ğœ ]+â„™ ğ‘”âˆ¼îˆ³[ğ‘” âˆˆ ğ´| |îˆ¿ â‰ Ì‚ â„â‰¥ğœ ]â„™ ğ‘”âˆ¼îˆ³[îˆ¿ â‰ Ì‚ â„â‰¥ğœ
]
ğ´ |
|
| | |
âˆ’â„™
ğ‘”âˆ¼îˆ³i
îˆ°deal[ğ‘” âˆˆ ğ´| |îˆ¿ =Ì‚ â„â‰¥ğœ ]â„™ ğ‘”âˆ¼îˆ³[îˆ¿ =Ì‚ â„â‰¥ğœ ]âˆ’â„™
ğ‘”âˆ¼îˆ³i
îˆ°deal[ğ‘” âˆˆ ğ´| |îˆ¿ â‰ Ì‚ â„â‰¥ğœ ]â„™ ğ‘”âˆ¼îˆ³[îˆ¿ â‰ Ì‚ â„â‰¥ğœ ]|
|
|
| |
= sup|ğ‘ â‹…(1âˆ’negl(ğ‘ ))+negl(ğ‘ )âˆ’ğ‘ â‹…(1âˆ’negl(ğ‘ ))âˆ’negl(ğ‘ )| â‰¤ negl(ğ‘ ),
| ğ´ ğ´ |
ğ´
asdesired.
7 Local Mitigation
Inthissectionwe showmitigationresultsforpotentially-backdooredfunctionsâ„ğ‘› â†’ â„,which
relyontwomainassumptions:
1. Thepotentially-backdooredmodelhaslowğ“ populationloss.
2
2. The population distribution is â€œniceâ€ in the sense that the marginal over the domain â„ğ‘› is
uniform on the ğ‘›-dimensional unit ball, and the labels are close to a linear or polynomial
functionâ„ğ‘› â†’ â„. (However,thelabelsneednotbedeterministic.)
7.1 Local Mitigation Preliminaries
Below,wedescribethenotionsweneedregardingconvexityandprobabilitymeasuresinEuclidean
space.
Convex sets and degeneracy. For ğ‘› âˆˆ â„•, a set ğ¶ âŠ† â„ğ‘› is convex if for all ğ‘¥,ğ‘¦ âˆˆ ğ¶, the line
segment between ğ‘¥ and ğ‘¦ is fully contained within ğ¶, i.e., for all ğ›¼ âˆˆ [0,1], ğ›¼ğ‘¥ + (1 âˆ’ğ›¼)ğ‘¦ âˆˆ ğ¶.
Throughout, we will work with nondegenerate convex sets ğ¶, in the sense that the standard
Lebesgue measure of ğ¶ is non-zero. (In other words, ğ¶ is â€œfull-dimensionalâ€.) Furthermore, we
28work with bounded convex sets ğ¶, in the sense that ğ¶ is contained in some sufficiently large
Euclideanball.
Foraboundedconvexsetğ¶,weusethefactthatforallğ‘¥ â‰  ğ‘¦ âˆˆ ğ¶,thereexistsauniquepointğ‘ on
theboundary ofğ¶ suchthattheraystarting atğ‘¥ thatgoesthroughğ‘¦ intersectsğ‘. More formally,
letting
ğ›¼âˆ— = sup{ğ›¼ âˆˆ â„ âˆ¶ ğ‘¥ +ğ›¼(ğ‘¦ âˆ’ğ‘¥) âˆˆ ğ¶},
â‰¥0
wehave
ğ‘ = ğ‘¥ +ğ›¼âˆ— â‹…(ğ‘¦ âˆ’ğ‘¥).
Notethatğ›¼âˆ—existsandisfinitesinceğ¶isboundedandğ‘¥ â‰  ğ‘¦. Moreover,weassumethatalgorithms
canefficientlycomputesuchapointğ‘ givenğ‘¥ andğ‘¦.
Probability, measures, and forms. We work with the standard Lebesgue measure, unless
explicitlystatedotherwise. Forameasurable(infact,convex)setğ¶ âŠ† â„ğ‘›,theuniformdistribution
overğ¶ iswithrespecttotheLebesguemeasure. Thatis,theprobabilitydensityfunctionisgiven
bythestandardLebesguemeasure,normalizedbythevolumeofğ¶. Wesometimesusethevariable
namesğœ‚(â‹…)andğœ‡(â‹…)torefertodifferentialformscorrespondingtoprobabilitymeasuresinâ„ğ‘›. Note
thatğœ‚(â‹…)andğœ‡(â‹…)includethedifferentialterms(e.g.,â€œğ‘‘ğ‘¥â€).
7.2 Basic Mitigation for Linear Functions
Theorem7.1. Letğ›¿ â‰¥ 0andğœ€ âˆˆ [0,1/ 100]. Forevery indexğ‘› âˆˆ â„•,letğµ
ğ‘›
= ğµ(ğŸ,1) âŠ† â„ğ‘› betheunit
ball;letî‰„ âŠ† ğµ bea(nondegenerate)convexset;letîˆ´ bethesetofaffinefunctionsâ„ğ‘› â†’ â„;andlet
ğ‘› ğ‘› ğ‘›
ğ”» ğ‘› = ğ”» ğ‘›,ğœ€,ğ›¿/20ğ‘› be the collection of distributions îˆ° with uniform marginal îˆ° î‰„ ğ‘› = U(î‰„ğ‘›) such that
ğ¿>ğ›¿/20ğ‘›(îˆ´ ) â‰¤ ğœ€. Letğ”» = {ğ”» } . ThenAlgorithm2definesalocalmitigatorğ‘€ thatis
îˆ° ğ‘› ğ‘› ğ‘›âˆˆâ„•
(ğœ€, ğ›¿/(20ğ‘›) â†’ ğ›¿)
cutofflosssecure(Definition4.6)fordistributionsğ”»,whereğ‘  âˆˆ â„•isthesecurityparameter.
In particular, there exists ğœ‡ âˆˆ negl such that for every index ğ‘› âˆˆ â„• and every distribution îˆ° âˆˆ ğ”»
ğ‘›
there exists a function ğ‘”ideal âˆ¶ î‰„ â†’ â„ such that for any arbitrary (possibly malicious) function
îˆ° ğ‘›
ğ‘“ âˆ¶ î‰„ â†’ â„withlossğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€,andforeveryğ‘  âˆˆ â„•,themitigatorğ‘€ satisfies:
ğ‘› îˆ°
1. Accuracy. â„™ [|ğ‘¦âˆ— âˆ’ğ‘¦| > ğ›¿] â‰¤ ğœ€ +ğœ‡(ğ‘ ),and
(ğ‘¥,ğ‘¦)âˆ¼îˆ°,ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )
2. CutoffLossSecurity. âˆ€ğ‘¥âˆ— âˆˆ î‰„
ğ‘›
âˆ¶ â„™ ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥âˆ—,1ğ‘›,1ğ‘ )[| |ğ‘¦âˆ— âˆ’ğ‘” îˆ°ideal(ğ‘¥âˆ—)|
|
> ğ›¿] â‰¤ ğœ‡(ğ‘ ).
Furthermore, Algorithm 2 uses a total of ğ‘‚(ğ‘ ) oracle queries to ğ‘“, does not use random samples
from îˆ°, and runs in time ğ‘‚(ğ‘ ğ‘›), assuming unit runtime cost for each arithmetic operation on the
representations of real numbers involved in the computation. However, ğ‘€ is not guaranteed to be
unbiased.
In particular, the number of samples and queries required in Theorem 7.1 is independent of ğ‘›.
The runtime is optimal in the sense that merely performing ğ‘š = Î˜(ğ‘ ) queries to ğ‘“ on points
ğ‘¥ ,â€¦,ğ‘¥ âˆˆ î‰„ âŠ† â„ğ‘› requiresaruntimeofÎ©(ğ‘ ğ‘›).
1 ğ‘š ğ‘›
29Assumptions:
â€¢ ğ‘› âˆˆ â„•;ğ›¿ â‰¥ 0;ğœ€ âˆˆ [0,1/ 100].
â€¢ î‰„ âŠ† ğµ(ğŸ,1) âŠ† â„ğ‘›;î‰„ isconvexandnondegenerate.
â€¢ îˆ° âˆˆ Î”(î‰„ Ã—â„)withîˆ° = U(î‰„).
î‰„
â€¢ îˆ´ istheclassofaffinelinearfunctionsâ„ğ‘› â†’ â„.
â€¢ âˆƒâ„ âˆˆ îˆ´ âˆ¶ ğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€.
îˆ°
â€¢ Thealgorithmhasoracleaccesstoanarbitraryfunctionğ‘“ âˆ¶ î‰„ â†’ â„(notnecessarily
inîˆ´)suchthatğ¿>ğ›¿/20ğ‘›(ğ‘“) â‰¤ ğœ€.
îˆ°
â€¢ ğ‘  âˆˆ â„•isasecurityparameter;ğ‘š = 320ğ‘ .
â€¢ ğ‘¥âˆ— âˆˆ î‰„ isarbitrary.
BasicLocalLinearMitigator(ğ‘›,ğ‘š,ğ‘¥âˆ—):
îˆµ â†emptyset
for ğ‘– âˆˆ [ğ‘š]:
sampleğ‘¥ âˆ¼ U(î‰„)
ğ‘–
ğ‘¥â€² â† ResamplingProcedure(ğ‘¥âˆ—,ğ‘¥,ğ‘›) âŠ³SeeAlgorithm3
ğ‘– ğ‘–
â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€–
ğ‘– 2
ğœ† â†
ğ‘– â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€– âˆ’ â€–ğ‘¥â€² âˆ’ğ‘¥âˆ—â€–
ğ‘– 2 ğ‘– 2
if |ğœ†| â‰¤ 4ğ‘›:
ğ‘–
ğ‘¦ â† ğ‘“(ğ‘¥); ğ‘¦Ìƒâ€² â† ğ‘“(ğ‘¥â€²)
ğ‘– ğ‘– ğ‘– ğ‘–
ğ‘” â† (1âˆ’ğœ†)ğ‘¦ +ğœ†ğ‘¦Ìƒâ€²
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
îˆµ â† îˆµ âˆª{ğ‘–}
ğ‘” â†medianof{ğ‘”}
ğ‘– ğ‘–âˆˆîˆµ
outputğ‘”
Algorithm2: Abasiclocalindependentmitigatorforlinearfunctionsâ„ğ‘› â†’ â„.
7.2.1 CorrelatedSamplingLemma
Lemma 7.2(CorrelatedSampling). Letğ‘› âˆˆ â„•, letî‰„ âŠ† â„ğ‘› be anybounded (nondegenerate)convex
set,andletğ‘ˆ = U(î‰„)betheuniformdistributiononî‰„. Letğ‘¥âˆ— âˆˆ î‰„. Considerthefollowingprocedure
forgeneratingapointğ‘¥â€² âˆˆ î‰„:
1. Sampleğ‘¥ âˆ¼ ğ‘ˆ.
2. Letğ‘¥â€² â†ResamplingProcedure(ğ‘¥âˆ—,ğ‘¥,ğ‘›)usingAlgorithm3.
Thenğ‘¥âˆ—,ğ‘¥ andğ‘¥â€² areonastraightline,andfurthermore,therandomvariablesğ‘¥ andğ‘¥â€² areequalin
distribution.
30Assumptions:
â€¢ ğ‘› âˆˆ â„•;î‰„ âŠ† ğµ(ğŸ,1) âŠ† â„ğ‘›;î‰„ convex.
â€¢ ğ‘¥âˆ— âˆˆ î‰„ isarbitrary.
â€¢ ğ‘¥ wassampleduniformlyfromî‰„.
ResamplingProcedure(ğ‘¥âˆ—,ğ‘¥,ğ‘›):
Letğ“bethelinesegmentfromğ‘¥âˆ— toapointontheboundaryofî‰„ suchthat
ğ“passesthroughğ‘¥,andletğ‘¡ âˆˆ (0,2]bethelengthofğ“.
Sampleğ‘Ÿâ€² âˆˆ [0,ğ‘¡]accordingtodensityfunctionğ‘(ğ‘Ÿ) = ğ‘›ğ‘Ÿğ‘›âˆ’1.
ğ‘¡ğ‘›
Letğ‘¥â€² bethepointonğ“atdistanceğ‘Ÿâ€² fromğ‘¥âˆ—.
outputğ‘¥â€²
Algorithm3: Analgorithmforsampling twopointswith uniformmarginalsonthe unitsphere,
suchthatthepointsareonastraightlinewithanarbitraryinputğ‘¥âˆ—.
Proof. ItisclearfromconstructionofResamplingProcedurethatğ‘¥âˆ—,ğ‘¥,andğ‘¥â€² alllieonthesame
straightline,soitsufficestoarguethatthedistributionsofğ‘¥ andğ‘¥â€² areidenticallydistributed.
Considerasphericalcoordinatesystemcenteredatğ‘¥âˆ— (insteadoftheorigin),wheretheğ“ distance
2
from ğ‘¥âˆ— is denoted by ğ‘Ÿ. (Alternatively, this can be described by the usual centered spherical
coordinatesystem,wherewetranslatethewholespacebyâˆ’ğ‘¥âˆ—.) Letîˆ­denotethesetî‰„ writtenin
thesesphericalcoordinates. Sinceğ‘ˆ istheuniformdistributionoverî‰„,wecanwritethedensity
ofğ‘ˆ asavolumeelementinsphericalcoordinatesas
ğ‘‘ğ‘‰(Î¸,ğ‘Ÿ) = 1[(Î¸,ğ‘Ÿ) âˆˆ îˆ­]â‹…ğœ‚(Î¸)â‹…ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ, (11)
whereğ‘‘ğ‘‰ isproportionaltothestandardvolumeformandğœ‚ = ğœ‚(Î¸)isproportionaltothestandard
volumeformonthe(ğ‘›âˆ’1)-dimensionalsphere,whereÎ¸ parameterizespointsonthesphereand
denotesdirectionsrelativetoitscenter(inourcase,ğ‘¥âˆ—). Moreconcretely,
ğ‘›âˆ’1
ğœ‚(Î¸) âˆ ğ‘‘ğœƒ ğ‘‘ğœƒ â‹¯ğ‘‘ğœƒ â‹…âˆsinğ‘›âˆ’ğ‘–âˆ’1(ğœƒ),
1 2 ğ‘›âˆ’1 ğ‘–
ğ‘–=1
wherecrucially,ğœ‚doesnotdependonğ‘Ÿ.
We now invoke Lemma 7.3, using the fact that the density ğ‘‘ğ‘‰ splits into products of Î¸ and ğ‘Ÿ
measures,asin(11). Letğ‘¥ bedecomposedintoÎ¸(ğ‘¥)andğ‘Ÿ(ğ‘¥),andsimilarlyforğ‘¥â€².
UsingthenotationofLemma7.3,let
ğœ‡ (Î¸) = ğœ‚(Î¸), ğœ‡ (ğ‘Ÿ) = ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ,
Î˜ ğ‘…
and
îˆ­ = {ğ‘Ÿ âˆˆ â„ âˆ¶ (ğ‘Ÿ,Î¸) âˆˆ îˆ­}.
|Î¸ â‰¥0
31Wehave
1[ğ‘Ÿ âˆˆ îˆ­ ]
|Î¸
ğœ‡Ì‚ (ğ‘Ÿ) = â‹…ğœ‡ (ğ‘Ÿ).
ğ‘…|Î¸ âˆ« 1[ğ‘Ÿâ€² âˆˆ îˆ­ ]â‹…ğœ‡ (ğ‘Ÿâ€²) ğ‘…
ğ‘Ÿâ€²âˆˆâ„ |Î¸ ğ‘…
Using convexity and the notation of ResamplingProcedure, we know that for Î¸ = Î¸(ğ‘¥), we
haveîˆ­ = [0,ğ‘¡],whereğ‘¡ isthesameasinResamplingProcedure,namelythelengthoftheline
|Î¸
segmentğ“withendpointsğ‘¥âˆ— andtheboundaryofî‰„,passingthroughğ‘¥. Plugginginourmeasures,
weget
1[ğ‘Ÿ âˆˆ [0,ğ‘¡]]
ğœ‡Ì‚ (ğ‘Ÿ) = â‹…ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ
ğ‘…|Î¸ âˆ« 1[ğ‘Ÿâ€² âˆˆ [0,ğ‘¡]]â‹…(ğ‘Ÿâ€²)ğ‘›âˆ’1ğ‘‘ğ‘Ÿâ€²
ğ‘Ÿâ€²âˆˆâ„
ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ
= 1[ğ‘Ÿ âˆˆ [0,ğ‘¡]]â‹…
ğ‘¡
âˆ« (ğ‘Ÿâ€²)ğ‘›âˆ’1ğ‘‘ğ‘Ÿâ€²
{ 0
ğ‘›ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ ifğ‘Ÿ âˆˆ [0,ğ‘¡],
= ğ‘¡ğ‘›
0 ifğ‘Ÿ âˆ‰ [0,ğ‘¡].
This exactly matches the density ğ‘(â‹…) in ResamplingProcedure. Therefore, by Lemma 7.3, the
joint distributions (Î¸(ğ‘¥),ğ‘Ÿ(ğ‘¥)) and (Î¸(ğ‘¥),ğ‘Ÿ(ğ‘¥â€²)) are identical. Since Î¸(ğ‘¥) = Î¸(ğ‘¥â€²), it follows that
thejointdistributions (Î¸(ğ‘¥),ğ‘Ÿ(ğ‘¥))and(Î¸(ğ‘¥â€²),ğ‘Ÿ(ğ‘¥â€²))areidentical, whichimpliesthatğ‘¥ andğ‘¥â€² are
identicallydistributed,asdesired.
Lemma7.3. Letî‰„ andî‰… benice24 boundedsubsetsofâ„ğ‘› andâ„,respectively. Supposeğœ‡ isajoint
probabilitymeasureonî‰„ Ã—î‰… thatcanbewrittenas
ğœ‡(ğ‘¥,ğ‘¦) = 1[(ğ‘¥,ğ‘¦) âˆˆ îˆ­]â‹…ğœ‡ (ğ‘¥)â‹…ğœ‡ (ğ‘¦),
î‰„ î‰…
forsome measuresğœ‡ î‰„ onî‰„,ğœ‡ î‰… onî‰…,and somemeasurable setîˆ­ âŠ† î‰„ Ã—î‰…. Considerthefollowing
samplingprocedure:
1. Sample(ğ‘¥,ğ‘¦ ) âˆ¼ ğœ‡. (Wewillnotuseğ‘¦ .)
0 0
2. Letîˆ­ âˆ¶= {ğ‘¦ âˆˆ î‰… âˆ¶ (ğ‘¥,ğ‘¦) âˆˆ îˆ­}. Fortheaboveğ‘¥ âˆˆ î‰„,considertheprobabilitymeasureonî‰…
|ğ‘¥
givenby
1[ğ‘¦ âˆˆ îˆ­ ]
|ğ‘¥
ğœ‡Ì‚ (ğ‘¦) âˆ¶= â‹…ğœ‡ (ğ‘¦).
î‰…|ğ‘¥ âˆ« 1[ğ‘¦â€² âˆˆ îˆ­ ]â‹…ğœ‡ (ğ‘¦â€²) î‰…
ğ‘¦â€²âˆˆî‰… |ğ‘¥ î‰…
Sampleğ‘¦
1
âˆ¼ ğœ‡Ì‚
î‰…|ğ‘¥
andoutput(ğ‘¥,ğ‘¦ 1).
Then,thedistributionof(ğ‘¥,ğ‘¦ )isexactlyğœ‡.
1
Proof. Considertheprobabilitymeasures
ğœ‡Ì‚ (ğ‘¥) âˆ¶= 1[ğ‘¦ âˆˆ îˆ­ ]â‹…ğœ‡ (ğ‘¦) â‹…ğœ‡ (ğ‘¥),
î‰„ (âˆ« |ğ‘¥ î‰… ) î‰„
ğ‘¦âˆˆî‰…
24Formally,werequireî‰„ andî‰… tobestandardBorelspacesequippedwithğœ-finitemeasures.Sincewewillonly
usethiswhenî‰„ isthesphereandboundedî‰… âŠ†[0,âˆ),bothequippedwiththestandardLebesguemeasure,wedo
notelaborateonthemostgeneralrequirementsonî‰„ andî‰….
32â¨˜[ğ‘¦ âˆˆ îˆ­ ]
ğœ‡Ì‚ (ğ‘¦) âˆ¶= |ğ‘¥ â‹…ğœ‡ (ğ‘¦), forfixedğ‘¥ âˆˆ î‰„.
î‰…|ğ‘¥ âˆ« â¨˜[ğ‘¦â€² âˆˆ îˆ­ ]â‹…ğœ‡ (ğ‘¦â€²) î‰…
ğ‘¦â€²âˆˆî‰… |ğ‘¥ î‰…
Noticethatforall(ğ‘¥,ğ‘¦) âˆˆ î‰„ Ã—î‰…,wehavetheequality
ğœ‡Ì‚ (ğ‘¥)â‹…ğœ‡Ì‚ (ğ‘¦) = 1[(ğ‘¥,ğ‘¦) âˆˆ îˆ­]â‹…ğœ‡ (ğ‘¥)â‹…ğœ‡ (ğ‘¦) = ğœ‡(ğ‘¥,ğ‘¦).
î‰„ î‰…|ğ‘¥ î‰„ î‰…
Notethatbytheassumptionthatğœ‡ isaprobabilitymeasure(i.e.,integratesto1),onecanimmedi-
atelyobserveviaFubiniâ€™stheoremthatğœ‡Ì‚ andğœ‡Ì‚ areprobabilitymeasuresforanyğ‘¥ âˆˆ î‰„ (i.e.,
î‰„ î‰…|ğ‘¥
bothintegrateto1).
Considersomemeasurable subsetğ‘† âŠ† îˆ­. Theprobabilityofeventğ‘† underprobabilitymeasure ğœ‡
isgivenby
ğœ‡(ğ‘¥,ğ‘¦) = 1[(ğ‘¥,ğ‘¦) âˆˆ ğ‘†]â‹…ğœ‡(ğ‘¥,ğ‘¦)
âˆ« âˆ«
(ğ‘¥,ğ‘¦)âˆˆğ‘† (ğ‘¥,ğ‘¦)âˆˆî‰„Ã—î‰…
= 1[(ğ‘¥,ğ‘¦) âˆˆ ğ‘†]â‹…ğœ‡Ì‚ (ğ‘¥)â‹…ğœ‡Ì‚ (ğ‘¦). (12)
âˆ« î‰„ î‰…|ğ‘¥
(ğ‘¥,ğ‘¦)âˆˆî‰„Ã—î‰…
On theother hand,consider the samplingprocess definedin thestatement. The marginaldensity
measureonî‰„ isgivenatapointğ‘¥ âˆˆ î‰„ by
ğœ‡(ğ‘¥,ğ‘¦) = 1[(ğ‘¥,ğ‘¦) âˆˆ îˆ­]â‹…ğœ‡ (ğ‘¥)â‹…ğœ‡ (ğ‘¦)
âˆ« âˆ« î‰„ î‰…
ğ‘¦âˆˆî‰… ğ‘¦âˆˆî‰…
= 1[ğ‘¦ âˆˆ îˆ­ ]â‹…ğœ‡ (ğ‘¦) â‹…ğœ‡ (ğ‘¥)
(âˆ« |ğ‘¥ î‰… ) î‰„
ğ‘¦âˆˆî‰…
= ğœ‡Ì‚ (ğ‘¥).
î‰„
By construction of our sampling process, we know that for fixed ğ‘¥ âˆˆ î‰„, we sample ğ‘¦ âˆ¼ ğœ‡Ì‚ .
1 î‰…|ğ‘¥
Therefore,theprobabilityofeventğ‘† underoursamplingprocessisgivenby
ğœ‡Ì‚ (ğ‘¥)â‹…ğœ‡Ì‚ (ğ‘¦) = 1[(ğ‘¥,ğ‘¦) âˆˆ ğ‘†]â‹…ğœ‡Ì‚ (ğ‘¥)â‹…ğœ‡Ì‚ (ğ‘¦),
âˆ« î‰„ î‰…|ğ‘¥ âˆ« î‰„ î‰…|ğ‘¥
(ğ‘¥,ğ‘¦)âˆˆğ‘† (ğ‘¥,ğ‘¦)âˆˆî‰„Ã—î‰…
whichexactlymatchesourexpressioninEq.(12),asdesired.
Claim7.4. InthenotationofLemma7.2,let
ğ‘Ÿ = â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€– , ğ‘Ÿâ€² = â€–ğ‘¥â€² âˆ’ğ‘¥âˆ—â€– ,
2 2
and
ğ‘Ÿ ğ‘Ÿâ€²
ğœ† = , ğœ†â€² = .
ğ‘Ÿ âˆ’ğ‘Ÿâ€² ğ‘Ÿâ€² âˆ’ğ‘Ÿ
Then
{ } 1
â„™[max |ğœ†|,|ğœ†â€²| â‰¤ 4ğ‘›] â‰¥ .
8
33Proof. ByLemma7.2andthedetailsofAlgorithm3,ğ‘Ÿ andğ‘Ÿâ€² areeachsampledindependentlyfrom
theinterval(0,ğ‘¡]accordingtothedensityfunctionğ‘(ğ‘Ÿ) = ğ‘›ğ‘Ÿğ‘›âˆ’1,whereğ‘¡ isthelengthoftheline
ğ‘¡ğ‘›
ğ“ that contains ğ‘¥âˆ—, ğ‘¥ and ğ‘¥â€². In particular, for any ğ‘ âˆˆ [0,1], take ğœŒ = ğœŒ(ğ‘) âˆˆ [0,ğ‘¡] such that the
followingholds.
ğœŒ ğ‘› ğ‘Ÿğ‘› ğœŒ ğœŒ ğ‘›
ğ‘ = ğ‘(ğ‘Ÿ)ğ‘‘ğ‘Ÿ = = âŸ¹ ğœŒ(ğ‘) = ğ‘1/ğ‘› â‹…ğ‘¡.
âˆ« ğ‘¡ğ‘› [ğ‘›] ( ğ‘¡ )
0 0
Inparticular,ğœŒ(1/ 4) = (1/ 4)1/ğ‘› â‹…ğ‘¡ andğœŒ(3/ 4) = (3/ 4)1/ğ‘› â‹…ğ‘¡. Thisimpliesthat
1/ğ‘› 1/ğ‘› 2
| | 3 1 1 1
â„™ |ğ‘Ÿ âˆ’ğ‘Ÿâ€²| â‰¥ ğ‘¡ âˆ’ â‰¥ 2 = .
[| | ((4) (4) )] (4) 8
Hence,withprobabilityatleast1/8,
{ } max{|ğ‘Ÿ|,|ğ‘Ÿâ€²|} ğ‘¡ 41/ğ‘› 41/ğ‘›
max
| ğœ†| ,| ğœ†â€²|
= â‰¤ = =
| | | |
|ğ‘Ÿ âˆ’ğ‘Ÿâ€²| ğ‘¡ ((3 4)1/ğ‘› âˆ’( 41 )1/ğ‘›
)
31/ğ‘› âˆ’1 ğ‘’ln(3)/ğ‘› âˆ’1
41/ğ‘› 41/ğ‘›
= â‰¤ â‰¤ 4ğ‘›,
1+ ln(3) +âˆ‘âˆ (ln(3)/ğ‘›)ğ‘˜ âˆ’1 (ln(3) )
( ğ‘› ğ‘˜=2 ğ‘˜! ) ğ‘›
asdesired.
7.2.2 ProofofTheorem7.1
Thefollowing claimsaysthat inAlgorithm 2,for mostğ‘– âˆˆ îˆµ, thefunctionsâ„andğ‘“, andthelabels
fromthedistributionîˆ°,areallclosetoeachotherforthepointsğ‘¥ andğ‘¥â€².
ğ‘– ğ‘–
Claim7.5. Letğœ€,ğ›¿ â‰¥ 0. Letğ‘›,î‰„,ğ‘¥,ğ‘¥â€² beasinLemma7.2. Letîˆ° âˆˆ Î”(î‰„ Ã—â„)beadistributionwith
îˆ° î‰„ = U(î‰„), and let â„,ğ‘“ âˆ¶ î‰„ â†’ â„ such that ğ¿> îˆ°ğ›¿(â„) â‰¤ ğœ€ and ğ¿> îˆ°ğ›¿(ğ‘“) â‰¤ ğœ€. Let ğ‘¦,ğ‘¦â€² âˆˆ â„ be random
variablessuchthat(ğ‘¥,ğ‘¦),(ğ‘¥â€²,ğ‘¦â€²) âˆ¼ îˆ°. Letğœ†,ğœ†â€² beasinClaim7.4andlet
Î” = â„(ğ‘¥)âˆ’ğ‘¦; Î”â€² = â„(ğ‘¥â€²)âˆ’ğ‘¦â€²
ğ‘¦ ğ‘¦
Î” = â„(ğ‘¥)âˆ’ğ‘“(ğ‘¥); Î”â€² = â„(ğ‘¥â€²)âˆ’ğ‘“(ğ‘¥â€²).
ğ‘“ ğ‘“
{ } { }
Letğ¸ betheevent | ğœ†| â‰¤ 4ğ‘› or max{|ğœ†|,|ğœ†â€²|} â‰¤ 4ğ‘› . Then
| |
{ } { }
|
â„™ max |Î” |,|Î”â€²| > ğ›¿ âˆ¨ max |Î” |,|Î”â€²| > 2ğ›¿ |ğ¸ â‰¤ 32ğœ€.
[ ğ‘¦ ğ‘¦ ğ‘“ ğ‘“ | ]
Proof. Seeingasğ¿>ğ›¿(â„) â‰¤ ğœ€ andğ¿>ğ›¿(ğ‘“) â‰¤ ğœ€,aunionboundyields
îˆ° îˆ°
â„™[|â„(ğ‘¥)âˆ’ğ‘¦| â‰¤ ğ›¿ âˆ§ |â„(ğ‘¥â€²)âˆ’ğ‘¦â€²| â‰¤ ğ›¿ âˆ§ |ğ‘“(ğ‘¥)âˆ’ğ‘¦| â‰¤ ğ›¿ âˆ§ |ğ‘“(ğ‘¥â€²)âˆ’ğ‘¦â€²| â‰¤ ğ›¿] â‰¥ 1âˆ’4ğœ€.
Bythetriangleinequality,
{ } { }
â„™[max |Î” ğ‘¦|,|Î”â€² ğ‘¦| > ğ›¿ âˆ§ max |Î” ğ‘“|,|Î”â€² ğ‘“| > 2ğ›¿] â‰¥ 1âˆ’4ğœ€. (13)
34ByClaim7.4,
1
â„™[ğ¸] â‰¥ . (14)
8
ByEqs.(13)and(14),
{ } { }
|
â„™ max |Î” |,|Î”â€²| > ğ›¿ âˆ¨ max |Î” |,|Î”â€²| > 2ğ›¿ |ğ¸
[ ğ‘¦ ğ‘¦ ğ‘“ ğ‘“ | ]
{ } { }
â„™[max |Î” ğ‘¦|,|Î”â€² ğ‘¦| > ğ›¿ âˆ¨ max |Î” ğ‘“|,|Î”â€² ğ‘“| > 2ğ›¿]
â‰¤ â‰¤ 32ğœ€,
â„™[ğ¸]
asdesired.
ProofofTheorem7.1. The ğ‘‚(ğ‘ ) bound on the sample and query complexity is immediate from
the construction of Algorithm 2. The ğ‘‚(ğ‘ ğ‘›) bound on the runtime is also immediate under the
assumptionthatarithmeticoperations incurunitcost,wherewerecall thatcomputingamedian
ofa listoflengthğ‘šcanbe doneintimeğ‘‚(ğ‘š)(Blumet al.,1973). Itremainsto showcorrectness
ofAlgorithm2.
Fixanaffinelinearfunctionâ„ âˆ¶ â„ğ‘› â†’ â„suchthatğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€. Letğ‘”ideal = â„,andletğ‘¦ideal = â„(ğ‘¥âˆ—).
îˆ° îˆ°
For each ğ‘– âˆˆ [ğ‘š], let ğ“ âŠ† â„ğ‘› be a line segment from ğ‘¥âˆ— to the unit sphere that passes through ğ‘¥
ğ‘– ğ‘–
and ğ‘¥â€² (such a line exists by Lemma 7.2). For any point ğ‘¥ âˆˆ ğ“, let ğ‘Ÿ(ğ‘¥) = â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€– . Let ğ›¼,ğ›½ âˆˆ â„
ğ‘– ğ‘– 2 ğ‘– ğ‘–
suchthat
â„(ğ‘Ÿ) = ğ›¼ğ‘Ÿ +ğ›½
ğ‘– ğ‘– ğ‘–
isthe1-dimensionalrestrictionofâ„toğ“,namely,â„(ğ‘Ÿ(ğ‘¥)) = â„(ğ‘¥)forallğ‘¥ âˆˆ ğ“. Notethatğ›½ = ğ‘¦ideal.
ğ‘– ğ‘– ğ‘– ğ‘–
ByLemma7.2,eachğ‘¥â€² âˆ¼ U(î‰„). Hence,wemaydefinerandomvariablesğ‘¦ ,ğ‘¦â€²,â€¦,ğ‘¦ ,ğ‘¦â€² âˆˆ â„such
ğ‘– 1 1 ğ‘š ğ‘š
thatforeachğ‘– âˆˆ [ğ‘š],(ğ‘¥,ğ‘¦) âˆ¼ îˆ°and(ğ‘¥â€²,ğ‘¦â€²) âˆ¼ îˆ°.
ğ‘– ğ‘– ğ‘– ğ‘–
Fixğ‘– âˆˆ [ğ‘š]. LetÎ” = â„(ğ‘¥)âˆ’ğ‘¦ andÎ”â€² = â„(ğ‘¥â€²)âˆ’ğ‘“(ğ‘¥â€²). Hence,
ğ‘¦,ğ‘– ğ‘– ğ‘– ğ‘“,ğ‘– ğ‘– ğ‘–
ğ‘” = (1âˆ’ğœ†)ğ‘¦ +ğœ†ğ‘“(ğ‘¥â€²)
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
= (1âˆ’ğœ†)(â„(ğ‘¥)âˆ’Î” )+ğœ†(â„(ğ‘¥â€²)âˆ’Î”â€² )
ğ‘– ğ‘– ğ‘¦,ğ‘– ğ‘– ğ‘– ğ‘“,ğ‘–
= ((1âˆ’ğœ† ğ‘–)â„(ğ‘¥ ğ‘–)+ğœ† ğ‘–â„(ğ‘¥ ğ‘–â€²))âˆ’((1âˆ’ğœ† ğ‘–)Î” ğ‘¦,ğ‘– +ğœ† ğ‘–Î”â€² ğ‘“,ğ‘–) (15)
Considereachtermseparately.
(1âˆ’ğœ†)â„(ğ‘¥)+ğœ†â„(ğ‘¥â€²) = (1âˆ’ğœ†)â„(ğ‘Ÿ)+ğœ†â„(ğ‘Ÿâ€²) (Letğ‘Ÿ = ğ‘Ÿ(ğ‘¥),ğ‘Ÿâ€² = ğ‘Ÿ(ğ‘¥â€²))
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
= (1âˆ’ğœ† ğ‘–)(ğ›¼ ğ‘–ğ‘Ÿ ğ‘– +ğ›½ ğ‘–)+ğœ† ğ‘–(ğ›¼ ğ‘–ğ‘Ÿ ğ‘–â€² +ğ›½ ğ‘–)
= ğ›½ ğ‘– +ğ›¼ ğ‘–((1âˆ’ğœ† ğ‘–)ğ‘Ÿ ğ‘– +ğœ† ğ‘–ğ‘Ÿ ğ‘–â€² )
ğ‘Ÿ ğ‘Ÿ
= ğ›½ +ğ›¼ 1âˆ’ ğ‘– ğ‘Ÿ + ğ‘– â‹…ğ‘Ÿâ€²
ğ‘– ğ‘–(( ğ‘Ÿ âˆ’ğ‘Ÿâ€²) ğ‘– ğ‘Ÿ âˆ’ğ‘Ÿâ€² ğ‘–)
ğ‘– ğ‘– ğ‘– ğ‘–
= ğ›½ = ğ‘¦ideal, (16)
ğ‘–
and,
| | | |
|(1âˆ’ğœ†)Î” +ğœ†Î”â€² | â‰¤ |1âˆ’ğœ†|â‹…|Î” |+|ğœ†|â‹…|Î”â€² |
| ğ‘– ğ‘¦,ğ‘– ğ‘– ğ‘“,ğ‘–| | ğ‘–| ğ‘¦,ğ‘– ğ‘– ğ‘“,ğ‘–
35â‰¤ (2|ğœ†|+1)â‹…max{|Î” |,|Î”â€² |}
ğ‘– ğ‘¦,ğ‘– ğ‘“,ğ‘–
â‰¤ 9ğ‘›â‹…max{|Î” |,|Î”â€² |}, (17)
ğ‘¦,ğ‘– ğ‘“,ğ‘–
wherethefinalinequalityholdswhen|ğœ†| â‰¤ 4ğ‘›. Seeingasğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€ andğ¿>ğ›¿/20ğ‘›(ğ‘“) â‰¤ ğœ€,Claim7.5
ğ‘– îˆ° îˆ°
impliesthat
|
|
â„™ [max{|Î” ğ‘¦,ğ‘–|,|Î”â€² ğ‘“,ğ‘–|} > ğ›¿/ 10ğ‘› |
|
|ğœ† ğ‘–| â‰¤ 4ğ‘›
]
â‰¤ 32ğœ€. (18)
|
ByClaim7.4,
1
â„™[|ğœ†| â‰¤ 4ğ‘›] â‰¥ . (19)
ğ‘–
8
CombiningEqs. (15)to(19),we concludethatwithprobability atleast1/8,|ğœ†| â‰¤ 4ğ‘›,and further-
ğ‘–
more,
|
| 2
â„™ | ğ‘” âˆ’ğ‘¦ideal| â‰¤ 0.9ğ›¿ | |ğœ†| â‰¤ 4ğ‘› â‰¥ 1âˆ’32ğœ€ â‰¥ . (20)
[| ğ‘– | | ğ‘– ] 3
|
Additionally,byEq.(19)andHoeffdingâ€™sinequality,forğ‘š â‰¥ 320ğ‘ ,withprobabilityatleast1âˆ’2ğ‘’âˆ’ğ‘ ,
thereareatleast20ğ‘  samples(ğ‘¥,ğ‘¥â€²)suchthat|ğœ†| â‰¤ 4ğ‘›,namely|îˆµ| â‰¥ 20ğ‘ . Moreover,byEq.(20)
ğ‘– ğ‘– ğ‘–
andHoeffdingâ€™sinequality,if|îˆµ| â‰¥ 20ğ‘  thenwithprobabilityatleast1âˆ’2ğ‘’âˆ’ğ‘ ,atleasthalfofthe
valuesin{ğ‘”} satisfy|ğ‘” âˆ’ğ‘¦ideal| â‰¤ 0.9ğ›¿,andthisimpliesthatthemedianğ‘” of{ğ‘”} alsosatisfies
ğ‘– ğ‘–âˆˆîˆµ ğ‘– ğ‘– ğ‘–âˆˆîˆµ
|ğ‘” âˆ’ğ‘¦ideal| â‰¤ 0.9ğ›¿. Byaunionbound,
â„™[|ğ‘” âˆ’ğ‘¦ideal| â‰¥ 0.9ğ›¿] â‰¤ 4ğ‘’âˆ’ğ‘ ,
Thisholdsforanyğ‘¥âˆ— âˆˆ î‰„,namely,
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ â„™[|ğ‘” âˆ’ğ‘”ideal(ğ‘¥âˆ—)| â‰¥ 0.9ğ›¿] â‰¤ 4ğ‘’âˆ’ğ‘ . (21)
îˆ°
ThisestablishesItem2inDefinition4.6.
Furthermore, seeing as ğ‘”ideal = â„ such that ğ¿>ğ›¿/20ğ‘›(â„) â‰¤ ğœ€, it follows from Eq. (21), the triangle
îˆ° îˆ°
inequality|ğ‘” âˆ’ğ‘¦| â‰¤ |ğ‘” âˆ’â„(ğ‘¥)|+|â„(ğ‘¥)âˆ’ğ‘¦|andaunionboundthat
| |
â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[| |ğ‘” âˆ’ğ‘¦|
|
â‰¥ ğ›¿
]
â‰¤ ğœ€ +4ğ‘’âˆ’ğ‘ .
ThisestablishesItem1inDefinition4.6,asdesired.
7.3 Improved Mitigation for Linear Functions
Definition7.6(DistributionswithBenignNoise). Letğ‘› âˆˆ â„•,letğœ â‰¥ 0,letî‰„ âŠ† ğµ(ğŸ,1) âŠ† â„ğ‘› bea
nondegenerateconvexsetcontainedintheunitball,andletğ‘ˆ = U(î‰„)betheuniformdistributionon
î‰„. Let îˆ½ âˆˆ SubG(ğœ2 ) be a real-valued distribution that is symmetric about 0. Let îˆ´ be a class of
functionsâ„ğ‘› â†’ â„. Foreachâ„ âˆˆ îˆ´,letîˆ°
â„,î‰„,îˆ½
beadistributionoverpairs(ğ‘¥,ğ‘¦) âˆˆ î‰„ Ã—â„generatedin
thefollowingmanner:
361. Sampleğ‘¥ âˆ¼ ğ‘ˆ,
2. Sampleğœ‚ âˆ¼ îˆ½,
3. Setğ‘¦ = â„(ğ‘¥)+ğœ‚.
Thecollectionofdistributionswithlabelsfromîˆ´ andbenignnoiseîˆ½isğ”»
îˆ´,î‰„,îˆ½
= {îˆ°
â„,î‰„,îˆ½
âˆ¶ â„ âˆˆ îˆ´}.
Assumptions:
â€¢ ğ‘›,î‰„,îˆ´ andğ”» asinDefinition7.6.
linear linear,î‰„,îˆ½
â€¢ ğ›¿ â‰¥ 0;ğœ€ âˆˆ [0,1/ 10].
â€¢ Thealgorithmhasrandomsampleaccesstoîˆ° âˆˆ ğ”» .
linear,î‰„,îˆ½
â€¢ âˆƒâ„ âˆˆ îˆ´ âˆ¶ ğ¿>ğ›¿/ğ‘›(â„) â‰¤ ğœ€.
linear îˆ°
â€¢ Thealgorithmhasoracleaccesstoanarbitraryfunctionğ‘“ âˆˆ â„î‰„ (notnecessarilyin
îˆ´ )suchthatğ¿>ğ›¿/ğ‘›(ğ‘“) â‰¤ ğœ€.
linear îˆ°
â€¢ ğ‘  âˆˆ â„•isasecurityparameter.
â€¢ ğ‘¥âˆ— âˆˆ î‰„ isarbitrary.
ImprovedLocalLinearMitigator(ğ‘¥âˆ—,ğ‘›,ğ‘ ):
sample(ğ‘¥ ,ğ‘¦ ),â€¦,(ğ‘¥ ,ğ‘¦ ) âˆ¼ îˆ°ğ‘ 
1 1 ğ‘  ğ‘ 
îˆµ â†emptyset
for ğ‘– âˆˆ [ğ‘ ]:
ğ‘¥â€² â†ResamplingProcedure(ğ‘¥âˆ—,ğ‘¥,ğ‘›) âŠ³SeeAlgorithm3
ğ‘– ğ‘–
ğ‘Ÿ â† â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€– ; ğ‘Ÿâ€² â† â€–ğ‘¥â€² âˆ’ğ‘¥âˆ—â€–
ğ‘– ğ‘– 2 ğ‘– ğ‘– 2
ğ‘Ÿ ğ‘Ÿâ€²
ğœ† â† ğ‘– ; ğœ†â€² â† ğ‘–
ğ‘– ğ‘Ÿ âˆ’ğ‘Ÿâ€² ğ‘– ğ‘Ÿâ€² âˆ’ğ‘Ÿ
ğ‘– ğ‘– ğ‘– ğ‘–
if |ğœ†| â‰¤ 4ğ‘› âˆ§ |ğœ†â€²| â‰¤ 4ğ‘›:
ğ‘– ğ‘–
îˆµ â† îˆµ âˆª{ğ‘–}
ğ‘ â† (ğ‘“(ğ‘¥)âˆ’ğ‘¦)â‹…ğœ†â€²
ğ‘– ğ‘– ğ‘– ğ‘–
ğ‘” â† (1âˆ’ğœ†)ğ‘¦ +ğœ†ğ‘“(ğ‘¥â€²)
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
{ }
ğ‘– ,ğ‘– ,â€¦,ğ‘– â† îˆµ; ğ‘˜ â† âŒŠ|îˆµ|/2âŒ‹
1 2 |îˆµ|
{ }
ğ‘” â† MeanOfMedians ( ğ‘” ğ‘–2ğ‘¡ âˆ’ğ‘ ğ‘–2ğ‘¡âˆ’1 âˆ¶ ğ‘¡ âˆˆ [ğ‘˜] ) âŠ³SeeAlgorithm6
outputğ‘”
Algorithm4: Anunbiasedcutofflosssecurelocalmitigatorforlinearfunctionsâ„ğ‘› â†’ â„,witha
trade-offbetweenprecisionandsamplecomplexity.
37Theorem 7.7. Let ğœ€ âˆˆ [0,1/ 10] and ğ›¿ â‰¥ 0. For every index ğ‘› âˆˆ â„•, let ğµ
ğ‘›
= ğµ(ğŸ,1) âŠ† â„ğ‘› be the unit
ball;letî‰„ âŠ† ğµ bea(nondegenerate)convexset;letîˆ´ bethesetofaffinelinearfunctionsâ„ğ‘› â†’ â„;
ğ‘› ğ‘› ğ‘›
letğœ
ğ‘›
â‰¤ (ğ›¿/ğ‘›)â‹…(2ln(2/ğœ€))âˆ’1/2;letîˆ½
ğ‘›
âˆˆ SubG(ğœ ğ‘›2 )beareal-valuedsubgaussiandistributionthatis
symmetricabout0;letğ”» ğ‘› = ğ”» îˆ´ ğ‘›,î‰„ ğ‘›,îˆ½ ğ‘› bethesetofdistributions withaffinelinearlabelsandbenign
noise,asinDefinition7.6;andletğ”» = {ğ”» } .25
ğ‘› ğ‘›âˆˆâ„•
ThenAlgorithm4definesalocalmitigatorğ‘€ thatisan
1 1 ln(ğ‘ )
ğœ€, â‹…ğ›¿ âŸ¶ + â‹…ğ›¿ (22)
( ğ‘› (ğ‘› ğ‘ 1/4 ) )
unbiasedcutofflosssecuremitigatorfordistributionsğ”»(satisfyingItems1,2and3inDefinition4.6),
whereğ‘  âˆˆ â„•isthesecurityparameter.
âˆš
Inparticular,ifğ‘  = ln(ğ‘›) ğ‘›andğ‘›islargeenough,themitigatoris
1 1
ğœ€, â‹…ğ›¿ âŸ¶ â‹…ğ›¿ (23)
( ğ‘› ğ‘›1/10 )
unbiasedandcutofflosssecure. Algorithm4usesğ‘  samplesand2ğ‘  oraclequeries,andrunsintime
ğ‘‚(ğ‘ ğ‘›),assumingunitruntimecostforeacharithmeticoperationontherepresentationsofrealnumbers
involvedinthecomputation.
7.3.1 ProofofTheorem7.7
ProofofTheorem7.7. Fixğ‘› âˆˆ â„•,îˆ° âˆˆ ğ”» ğ‘›,andletâ„ âˆˆ îˆ´ ğ‘› suchthatîˆ° = îˆ° â„,î‰„ ğ‘›,îˆ½ ğ‘› asinDefinition7.6.
By Claim C.2 and the choice of ğœ , ğ¿>ğ›¿/ğ‘›(â„) â‰¤ ğœ€. Let ğ‘“ âˆ¶ î‰„ â†’ â„ such that ğ¿>ğ›¿/ğ‘›(ğ‘“) â‰¤ ğœ€. Fix
ğ‘› îˆ° ğ‘› îˆ°
ğ‘– âˆˆ îˆµ. LetÎ” = ğ‘“(ğ‘¥)âˆ’â„(ğ‘¥)andÎ”â€² = ğ‘“(ğ‘¥â€²)âˆ’â„(ğ‘¥â€²). Letğ“ bethelinefromğ‘¥âˆ— totheunitsphere
ğ‘“,ğ‘– ğ‘– ğ‘– ğ‘“,ğ‘– ğ‘– ğ‘– ğ‘–
that contains ğ‘¥ and ğ‘¥â€². Let ğ‘Ÿ(ğ‘¥) = â€–ğ‘¥ âˆ’ğ‘¥âˆ—â€– , and let ğ›¼,ğ›½ âˆˆ â„ such that â„(ğ‘Ÿ) = ğ›¼ â‹…ğ‘Ÿ +ğ›½ is the
ğ‘– ğ‘– 2 ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
1-dimensional restriction of â„ to ğ“, namely, â„(ğ‘Ÿ(ğ‘¥)) = â„(ğ‘¥) for all ğ‘¥ âˆˆ ğ“. Note that ğ›½ = â„(ğ‘¥âˆ—),
ğ‘– ğ‘– ğ‘– ğ‘–
andğ‘¦
ğ‘–
= â„(ğ‘¥ ğ‘–)+ğœ‚ ğ‘–,whereğœ‚
ğ‘–
âˆ¼ îˆ½
ğ‘›
âˆˆ SubG(ğœ ğ‘›2 )withğ”¼[ğœ‚ ğ‘–] = 0. Similarly,wecandefinearandom
variableğ‘¦â€² = â„(ğ‘¥â€²)+ğœ‚â€² whereğœ‚â€² âˆ¼ îˆ½ isindependent.
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘›
Considertherandomvariableğ‘”.
ğ‘–
ğ‘” = (1âˆ’ğœ†)ğ‘¦ +ğœ†ğ‘“(ğ‘¥â€²)
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
= (1âˆ’ğœ†)(â„(ğ‘Ÿ)+ğœ‚)+ğœ†(â„(ğ‘Ÿâ€²)+Î”â€² )
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘“,ğ‘–
= (1âˆ’ğœ†)â„(ğ‘Ÿ)+ğœ† â‹…â„(ğ‘Ÿâ€²)+(1âˆ’ğœ†)ğœ‚ +ğœ† â‹…Î”â€² .
âŸââââââââââââââğ‘– âââââââğ‘– âââââğ‘– ââŸâââââââââââğ‘– ââââââââğ‘– ââââââğ‘–âââŸ âŸâââââââââŸâââğ‘– ââââââŸğ‘– âŸââğ‘– ââââŸâââğ‘“ âââŸ,ğ‘–
â„(ğ‘¥âˆ—) NoiseI (â‹†)
Examineeachtermseparately. Thefirsttermequalsâ„(ğ‘¥âˆ—)foranyğ‘¥,ğ‘¥â€² âˆˆ ğ“,asfollows.
ğ‘– ğ‘– ğ‘–
(1âˆ’ğœ†)â„(ğ‘Ÿ)+ğœ† â‹…â„(ğ‘Ÿâ€²) = (1âˆ’ğœ†)(ğ›¼ â‹…ğ‘Ÿ +ğ›½)+ğœ† â‹…(ğ›¼ â‹…ğ‘Ÿâ€² +ğ›½)
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
= ğ›½ +(1âˆ’ğœ†)(ğ›¼ â‹…ğ‘Ÿ)+ğœ† â‹…ğ›¼ â‹…ğ‘Ÿâ€²
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
25Inparticular,byClaimC.2andthechoiceofğœ ğ‘›,â„™
ğ‘âˆ¼îˆ½
ğ‘›[|ğ‘|â‰¥ğ›¿/ğ‘›]â‰¤ğœ€.Soforeveryîˆ°âˆˆğ”» ğ‘›thereexistsanaffine
functionâ„âˆˆîˆ´ suchthatğ¿>ğ›¿/ğ‘›(â„)â‰¤ğœ€.
ğ‘› îˆ°
38ğ‘Ÿ ğ‘Ÿ
= ğ›½ + 1âˆ’ ğ‘– (ğ›¼ â‹…ğ‘Ÿ)+ ğ‘– â‹…(ğ›¼ â‹…ğ‘Ÿâ€²)
ğ‘– ( ğ‘Ÿ âˆ’ğ‘Ÿâ€²) ğ‘– ğ‘– ğ‘Ÿ âˆ’ğ‘Ÿâ€² ğ‘– ğ‘–
ğ‘– ğ‘– ğ‘– ğ‘–
= ğ›½ = â„(ğ‘¥âˆ—).
ğ‘–
Theterm(â‹†)canbedecomposedintoabiastermandanoiseterm,asfollows.
(â‹†) = ğœ† â‹…Î”â€²
ğ‘– ğ‘“,ğ‘–
= ğœ†
ğ‘–
â‹…(ğ‘“(ğ‘¥ ğ‘–â€²)âˆ’â„(ğ‘¥â€²))
= ğœ† ğ‘– â‹…(ğ‘“(ğ‘¥ ğ‘–â€²)âˆ’ğ‘¦ ğ‘–â€² +ğœ‚â€² ğ‘–)
= ğœ† âŸââğ‘– ââââ‹… ââ( âââğ‘“ ââââ( ââğ‘¥ ââŸğ‘–â€² ââ) âââââˆ’ âââââğ‘¦ âââğ‘– ââ€² â) ââŸ+ âŸğœ† ââğ‘– ââŸâ‹… âğœ‚ âââŸâ€² ğ‘– .
Bias NoiseII
Insum,wehavethefollowingcentralidentity
ğ‘” = â„(ğ‘¥âˆ—)+ğ‘â€² +ğ‘, (24)
ğ‘– ğ‘– ğ‘–
whereğ‘â€² = ğœ† â‹…(ğ‘“(ğ‘¥â€²)âˆ’ğ‘¦â€²)isabiasterm,andğ‘ = (1âˆ’ğœ†)â‹…ğœ‚ +ğœ† â‹…ğœ‚â€² isanoiseterm. Weanalyze
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
thebiastermandthenoisetermseparately.
For the noise term ğ‘ in Eq. (24), seeing as ğœ‚ is symmetric about 0 and ğœ‚ âŠ¥ğœ†, it follows that
ğ‘– ğ‘– ğ‘– ğ‘–
ğ‘0 = (1âˆ’ğœ†)â‹…ğœ‚ isalsosymmetricabout0. Asimilarargumentholdsforğ‘1 = ğœ† â‹…ğœ‚â€²,andtherefore
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
thenoisetermğ‘ = ğ‘0 +ğ‘1 isdistributedsymmetricallyabout0.
ğ‘– ğ‘– ğ‘–
Furthermore,1âˆ’ğœ† = ğœ†â€²,andmax{|ğœ†|,|ğœ†â€²|} â‰¤ 4ğ‘›forğ‘– âˆˆ îˆµ. Hence,byClaimC.4,ğ‘0 andğ‘1 satisfy
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
ğ‘ ğ‘–0,ğ‘ ğ‘–1 âˆˆ SubG(16ğ‘›2ğœ ğ‘›2 ). ByClaimC.3,ğ‘
ğ‘–
= ğ‘ ğ‘–0 +ğ‘ ğ‘–1 âˆˆ SubG(ğœ‰2 )where
ğ›¿2
ğœ‰2 = 22 â‹…16ğ‘›2ğœ2 â‰¤ 64â‹… â‰¤ 64ğ›¿2.
ğ‘› 2log(2/ğœ€)
ByClaimC.2,thisimpliesthat
(20ğ›¿)2 1
â„™[|ğ‘| â‰¥ 20ğ›¿] â‰¤ 2exp âˆ’ â‰¤ . (25)
ğ‘– ( 2ğœ‰2 ) 10
Next, for the bias term ğ‘â€² in Eq. (24), note that Algorithm 4 uses quantities of the form ğ‘” âˆ’ğ‘ ,
ğ‘– ğ‘– ğ‘—
wheresubtractingğ‘ isintendedtoâ€œcanceloutâ€theğ‘â€² terminğ‘”. Considertheexpressionğ‘â€² âˆ’ğ‘ .
ğ‘— ğ‘– ğ‘– ğ‘– ğ‘—
Foranyfixedğ‘— â‰  ğ‘–,Lemma7.2andtheboundğ¿>ğ›¿/ğ‘›(ğ‘“) â‰¤ ğœ€ implythat
îˆ°
{ } ğ›¿ |
â„™ max | ğ‘“(ğ‘¥â€²)âˆ’ğ‘¦â€²| ,| ğ‘“(ğ‘¥ )âˆ’ğ‘¦ | > |ğ‘–,ğ‘— âˆˆ îˆµ â‰¤ 2ğœ€.
[ | ğ‘– ğ‘–| | ğ‘— ğ‘—| ğ‘› | ]
So
|
â„™ | ğ‘â€² âˆ’ğ‘ | > 4ğ›¿ |ğ‘–,ğ‘— âˆˆ îˆµ â‰¤ 2ğœ€. (26)
[| ğ‘– ğ‘—| | ]
Furthermore,byLemma7.2,(ğ‘¥â€²,ğ‘¦â€²) =ğ‘‘ (ğ‘¥ ,ğ‘¦ ),so
ğ‘– ğ‘– ğ‘— ğ‘—
ğ‘Ÿâ€² ğ‘Ÿ
ğ‘ = (ğ‘“(ğ‘¥ )âˆ’ğ‘¦ )â‹… ğ‘— =ğ‘‘ (ğ‘“(ğ‘¥â€²)âˆ’ğ‘¦â€²)â‹… ğ‘– = ğ‘â€²
ğ‘— ğ‘— ğ‘— ğ‘Ÿâ€² âˆ’ğ‘Ÿ ğ‘– ğ‘– ğ‘Ÿ âˆ’ğ‘Ÿâ€² ğ‘–
ğ‘— ğ‘— ğ‘– ğ‘–
39Becauseğ‘ âŠ¥ğ‘â€²,thisimpliesthatğ‘â€² âˆ’ğ‘ isdistributedsymmetricallyabout0.26
ğ‘— ğ‘– ğ‘– ğ‘—
Letîˆ¼ bethedistributionofğ‘” âˆ’ğ‘ fordistinctğ‘–,ğ‘— âˆˆ îˆµ,andletîˆ¼ betheconditionaldistribution
ğ‘”âˆ’ğ‘ ğ‘– ğ‘— good
of
|
ğ‘§ âˆ¼ îˆ¼ ğ‘”âˆ’ğ‘| |(ğ‘§ âˆˆ [â„(ğ‘¥âˆ—)âˆ’24ğ›¿,â„(ğ‘¥âˆ—)+24ğ›¿]).
Putting the facts that ğ‘ and ğ‘â€² âˆ’ğ‘ are symmetric about 0 together with Eqs. (24) to (26) gives
ğ‘– ğ‘– ğ‘—
thatîˆ¼ canbewrittenasamixture
ğ‘”âˆ’ğ‘
îˆ¼ = (1âˆ’ğ›¼)â‹…îˆ¼ +ğ›¼ â‹…îˆ¼ ,
ğ‘”âˆ’ğ‘ good bad
suchthat
â€¢ ğ›¼ â‰¤ 1/ 10+2ğœ€ â‰¤ 1/ 3,
â€¢ îˆ¼ isdistributedsymmetricallyaboutâ„(ğ‘¥âˆ—),and
ğ‘”âˆ’ğ‘
â€¢ â„™ [|ğ‘§ âˆ’â„(ğ‘¥âˆ—)| â‰¤ 24ğ›¿] = 1.
ğ‘§âˆ¼îˆ¼
good
ByTheorem8.1,theestimateğ‘” returnedbyAlgorithm4satisfies:
1. ğ‘” isunbiased,i.e.,ğ”¼[ğ‘”] = â„(ğ‘¥âˆ—). SoAlgorithm4isunbiased,satisfyingItem3inDefinition4.6.
2. ğ‘” isconcentrated,namely,
| | âˆš
âˆ€ğœ‚ â‰¥ 0 âˆ¶ â„™ îˆ¿âˆ¼îˆ°ğ‘ [| |ğ‘” âˆ’â„(ğ‘¥âˆ—)|
|
â‰¥ ğœ‚
]
â‰¤ 4exp(âˆ’ğ›¾ ğ‘ ),
{ }
whereğ›¾ = 1 â‹…min 1 , 2ğœ‚2 . Soforğœ‚ = ğ›¿ln(ğ‘ )/ğ‘ 1/4 asinthestatement,
4 100 (24ğ›¿)2 ğ‘ 
| | ln(ğ‘ )2 âˆš ln(ğ‘ )2
â„™ îˆ¿âˆ¼îˆ°ğ‘ [| |ğ‘” âˆ’â„(ğ‘¥âˆ—)|
|
â‰¥ ğœ‚
ğ‘ ]
â‰¤ 4exp (âˆ’
2â‹…(24)2 â‹…ğ‘ 1/2
â‹… ğ‘ 
)
â‰¤ 4exp (âˆ’
2â‹…(24)2)
âˆˆ negl(ğ‘ ).27
(27)
This shows that Algorithm 4 satisfies cutoff loss security (Item 2 in Definition 4.6) with
parameterğ›¿ = ğœ‚ ,whichisbetterthanrequiredinEq.(22). Additionally,combiningEq.(27)
1 ğ‘ 
withğ¿>ğ›¿/ğ‘›(â„) â‰¤ ğœ€ gives
îˆ°
| | ğ›¿ | | ğ›¿ | |
â„™ |ğ‘¦âˆ— âˆ’ğ‘¦| â‰¥ +ğœ‚ â‰¤ â„™ |ğ‘¦ âˆ’â„(ğ‘¥)| â‰¥ âˆ¨ |â„(ğ‘¥)âˆ’ğ‘¦âˆ—| â‰¥ ğœ‚ â‰¤ ğœ€ +negl(ğ‘ ).
(ğ‘¥,ğ‘¦)âˆ¼îˆ° [| | ğ‘› ğ‘ ] (ğ‘¥,ğ‘¦)âˆ¼îˆ°[| | ğ‘› | | ğ‘ ]
ğ‘¦âˆ—â†ğ‘€ğ‘“,îˆ°(ğ‘¥,1ğ‘›,1ğ‘ )
26Hereweusetheassumptionğ‘ âŠ¥ğ‘â€², whichholdsbecausethesamplesforiterationğ‘–areindependentofthe
ğ‘— ğ‘–
samplesforiterationğ‘—.Itmightbetemptingtoconsiderasimpleralgorithmthatdoesnotpartitionîˆµintotwodisjoint
setsofsizeğ‘˜,andinsteadsimplyusesestimatesoftheformğ‘” âˆ’ğ‘ forallğ‘–âˆˆîˆµ.Thisleadstotermsoftheformğ‘â€²âˆ’ğ‘,
ğ‘– ğ‘– ğ‘– ğ‘–
whereğ‘â€²andğ‘ areidenticallydistributedbutarenotindependent.Toseewhythatmightnotbegoodenough,recall
ğ‘– ğ‘–
thatforajointdistribution(ğ‘‹,ğ‘Œ)withğ‘‹ =ğ‘‘ ğ‘Œ,thevariables(ğ‘‹,ğ‘Œ)mightnotbeexchangeable,andinparticularğ‘‹âˆ’ğ‘Œ
mightnotbesymmetric. Forexample,consideradistributionof(ğ‘‹,ğ‘Œ)thatisuniformover{(âˆ’1,0),(0,1),(1,âˆ’1)}.
Thenğ‘‹ âˆ’ğ‘Œ isnotsymmetric. Tosummarize,byusingğ‘” âˆ’ğ‘ withğ‘– â‰  ğ‘—,wegetasymmetricdistributionwhile
ğ‘– ğ‘—
avoidingquestionsofexchangeability.
27Eq.(27)isforthecaseğ›¾ <1/400.Forthecaseğ›¾ =1/400,theupperboundisclearlynegligibleinğ‘ .
40SoAlgorithm4satisfiestheaccuracyrequirement(Item1inDefinition4.6)withparameters
âˆš
ğ›¿/ğ‘› â†’ ğ›¿ â‹…(1/ğ‘›+ln(ğ‘ )/ğ‘ 1/4)asinEq.(22). Inparticular,forğ‘  = ln(ğ‘›) ğ‘›,
âˆš
ğ›¿ 1 ln(ğ‘ ) 1 ln(ln(ğ‘›) ğ‘›)
+ğœ‚ = ğ›¿ â‹… + = ğ›¿ â‹… +
ğ‘› ğ‘  (ğ‘› ğ‘ 1/4 ) (ğ‘› (ln(ğ‘›)âˆš ğ‘›)1/4)
1 ln(ğ‘›) 1
â‰¤ ğ›¿ â‹… + â‰¤ ğ›¿ â‹… ,
(ğ‘› ğ‘›1/8 ) ğ‘›1/10
where the last inequality holds for ğ‘› large enough. Hence, Algorithm 4 also satisfies the
accuracyrequirementwithparametersğ›¿/ğ‘› â†’ ğ›¿/ğ‘›1/10 asinEq.(23).
Thus, Algorithm 4 satisfies Items 1, 2 and 3 in Definition 4.6 with the parameters in Eqs. (22)
and(23),asdesired.
7.4 Mitigation for Multivariate Polynomials
Definition7.8. Forğ‘‘ +1valuesğ‘£ ,â‹¯,ğ‘£ âˆˆ â„,wedefinetheVandermondematrixVand(ğ‘£ ,â‹¯,ğ‘£ )to
0 ğ‘‘ 0 ğ‘‘
bethematrixğ‘‰ âˆˆ â„(ğ‘‘+1)Ã—(ğ‘‘+1) givenby
â¡1 ğ‘£ ğ‘£2 â‹¯ ğ‘£ğ‘‘â¤
â¢ 0 0 0â¥
1 ğ‘£ ğ‘£2 â‹¯ ğ‘£ğ‘‘
ğ‘‰ = â¢ 1 1 1â¥ .
â¢â‹® â‹® â‹® â‹± â‹® â¥
â¢ 1 ğ‘£ ğ‘£2 â‹¯ ğ‘£ğ‘‘â¥
â£ ğ‘‘ ğ‘‘ ğ‘‘â¦
Symbolically,forallğ‘–,ğ‘— âˆˆ {0,â‹¯,ğ‘‘},ğ‘‰ = ğ‘£ğ‘—.
ğ‘–,ğ‘— ğ‘–
Werecallthedefinition oftheinfinitynorm â€–â‹…â€– onmatrices, inducedbytheinfinity normâ€–â‹…â€– on
âˆ âˆ
vectors.
Definition7.9. Foramatrixğ‘€ âˆˆ â„ğ“Ã—ğ“,theinfinitynormofğ‘€,denotedâ€–ğ‘€â€– ,isdefinedby
âˆ
â€–ğ‘€ğ‘£â€–
â€–ğ‘€â€– = sup âˆ,
âˆ â€–ğ‘£â€–
ğ‘£âˆˆâ„ğ‘›â§µ{0} âˆ
whereweusethestandardğ“ normforvectorsğ‘¢ âˆˆ â„ğ‘›,formallygivenby
âˆ
â€–ğ‘¢â€– = max|ğ‘¢|.
âˆ ğ‘–
ğ‘–âˆˆ[ğ‘›]
WenowreferenceanupperboundontheinfinitynormoftheinverseofVandermondematrices.
Theorem 7.10 (Theorem 1 of Gautschi, 1962). Let ğ‘‰ = Vand(ğ‘£ ,â‹¯,ğ‘£ ), where ğ‘£ â‰  ğ‘£ for all
0 ğ‘‘ ğ‘– ğ‘—
ğ‘– â‰  ğ‘— âˆˆ {0,â‹¯,ğ‘‘}. Then,
1+|ğ‘£|
â€– ğ‘‰âˆ’1â€– â‰¤ max âˆ ğ‘– .
â€– â€–
âˆ 0â‰¤ğ‘—â‰¤ğ‘‘ 0â‰¤ğ‘–â‰¤ğ‘‘ |ğ‘£ ğ‘– âˆ’ğ‘£ ğ‘—|
ğ‘–â‰ ğ‘—
41x
âˆ— x
0 x
1 ...
â„“ x
d
Figure4: Algorithm5usestheResamplingProcedureofLemma7.2tosampleğ‘‘+1pointsonastraight
linewithğ‘¥âˆ— suchthateachpointhasauniformmarginal.
InAlgorithm5, wegivepseudocodetodescribethemitigation algorithmusedformultivariate
polynomials.
WenowprovethattheresampledpointsinAlgorithm5aresufficientlypairwisefar,sothatwe
caninvoketheboundgiveninTheorem7.10.
Claim7.11. InthenotationofAlgorithm5,foreachğ‘– âˆˆ [ğ‘š],
â„™ âˆ€ğ‘— â‰  ğ‘—â€² âˆˆ {0,â‹¯,ğ‘‘}âˆ¶
|
|ğ‘Ÿ(ğ‘—)
âˆ’ğ‘Ÿ(ğ‘—â€²)|
| â‰¥
1
â‰¥
5
.
[ | ğ‘– ğ‘– | 40ğ‘›ğ‘‘2] 6
Proof. ByconstructionoftheResamplingProcedureasinAlgorithm3,weknowthedensityof
eachğ‘¡ â‹…ğ‘Ÿ(ğ‘—) issampledi.i.d. (overğ‘— âˆˆ {0,â‹¯,ğ‘‘})fromthedensityfunctionğ‘(ğ‘Ÿ) = ğ‘›ğ‘Ÿğ‘›âˆ’1,whereğ‘¡ is
ğ‘– ğ‘– ğ‘¡ğ‘› ğ‘–
thelength ofthelinesegment fromğ‘¥âˆ— tothe boundary ofî‰„ thatpasses throughğ‘– ğ‘¥. Bydividing
by ğ‘¡, this density becomes ğ‘â€²(ğ‘Ÿ) âˆ¶= ğ‘¡ â‹…ğ‘(ğ‘Ÿ â‹…ğ‘¡) = ğ‘›â‹…ğ‘Ÿğ‘›âˆ’1, with support [0,1]. That is, each ğ‘Ÿ(ğ‘—) is
ğ‘– ğ‘– ğ‘– ğ‘–
sampledi.i.d. (overğ‘— âˆˆ {0,â‹¯,ğ‘‘})fromğ‘â€².
We partition [0,1] into 20ğ‘‘2 consecutive intervals ğ¼ ,â‹¯,ğ¼ such that each interval has equal
1 20ğ‘‘2
massunderğ‘â€² (i.e.,wesplitupğ‘â€² into20ğ‘‘2 quantiles). Wearrangetheintervalssothat0 âˆˆ ğ¼ and
1
1 âˆˆ ğ¼ . Wenowgiveabirthday-styleargumentthatshowsthatwithprobabilityatleast5/6,it
20ğ‘‘2
holdsthatallğ‘Ÿ(ğ‘—) (overğ‘— âˆˆ {0,â‹¯ğ‘‘},soğ‘–isfixed)lieindistinctandnon-neighboringintervals. Let
ğ‘–
ğ‘ betherandomvariabledenotingthenumberof(ğ‘—,ğ‘—â€²)pairsthatlieinthesameorneighboring
ğ‘–
intervals. Bylinearityofexpectation,
ğ”¼[ğ‘] = âˆ‘ â„™ ğ‘Ÿ(ğ‘—),ğ‘Ÿ(ğ‘—â€²) areinthesameorneighboringintervals
ğ‘– [ ğ‘– ğ‘– ]
ğ‘—â‰ ğ‘—â€²âˆˆ{0,â‹¯,ğ‘‘}
ğ‘‘ +1 3 1
â‰¤ â‹… â‰¤ .
( 2 ) 20ğ‘‘2 6
Sinceğ‘ isnon-negative,byMarkovâ€™sinequality,â„™[ğ‘ â‰¥ 1] â‰¤ 1/6,soâ„™[ğ‘ = 0] â‰¥ 5/6. Therefore,
ğ‘– ğ‘– ğ‘–
with probability at least 5/6, we know that all ğ‘Ÿ(ğ‘—) (over ğ‘— âˆˆ {0,â‹¯ğ‘‘}) lie in distinct and non-
ğ‘–
neighboringintervals.
42Assumptions:
â€¢ ğ‘›,ğ‘‘ âˆˆ â„•;ğœ€ âˆˆ [0,1/ 20ğ‘‘];ğ›¿
1
â‰¥ 0;ğ›¿
0
= ğ›¿1/ 4â‹…(80ğ‘›ğ‘‘2)ğ‘‘;î‰„ âŠ† ğµ(ğŸ,1) âŠ† â„ğ‘›;î‰„ convex.
â€¢ îˆ° âˆˆ Î”(î‰„ Ã—â„)withîˆ° = U(î‰„).
î‰„
â€¢ îˆ´ istheclassofpolynomialfunctionsâ„ğ‘› â†’ â„oftotaldegreeatmostğ‘‘.
â€¢ âˆƒâ„ âˆˆ îˆ´ âˆ¶ ğ¿>ğ›¿0(â„) â‰¤ ğœ€.
îˆ°
â€¢ Thealgorithmhasoracleaccesstoanarbitraryfunctionğ‘“ âˆˆ â„î‰„ (notnecessarilyin
îˆ´)suchthatğ¿>ğ›¿0(ğ‘“) â‰¤ ğœ€.
îˆ°
â€¢ ğ‘  âˆˆ â„•isasecurityparameter;ğ‘š = ğ‘ .
â€¢ ğ‘¥âˆ— âˆˆ î‰„ isarbitrary.
LocalPolynomialMitigator(ğ‘›,ğ‘‘,ğ‘š,ğ‘¥âˆ—):
Sampleğ‘¥ ,â€¦,ğ‘¥ â† ğ‘ˆ(î‰„).
1 ğ‘š
îˆ³ â†emptyset
for ğ‘– âˆˆ [ğ‘š]:
(ğ‘¥ ğ‘–(0),ğ‘¦Ì‚ ğ‘–(0) ) â† (ğ‘¥ ğ‘–,ğ‘“(ğ‘¥ ğ‘–))
for ğ‘— âˆˆ [ğ‘‘]:
ğ‘¥(ğ‘—) â† ResamplingProcedure(ğ‘¥âˆ—,ğ‘¥,ğ‘›) âŠ³SeeAlgorithm3
ğ‘– ğ‘–
ğ‘¦Ì‚(ğ‘—) â† ğ‘“ (ğ‘¥(ğ‘—) )
ğ‘– ğ‘–
Letğ‘¡ bethelengthofthelinesegmentğ“ fromğ‘¥âˆ— totheboundaryofî‰„ that
ğ‘– ğ‘–
passesthroughğ‘¥.
ğ‘–
â€–ğ‘¥(ğ‘—) âˆ’ğ‘¥âˆ—â€–
ğ‘Ÿ(ğ‘—) â† ğ‘– 2 âˆˆ [0,1];
ğ‘– ğ‘¡
ğ‘–
ğ‘… ğ‘– â† Vand(ğ‘Ÿ ğ‘–(0),â‹¯,ğ‘Ÿ ğ‘–(ğ‘‘) ) âˆˆ â„(ğ‘‘+1)Ã—(ğ‘‘+1)
ğ‘¦Ì‚ ğ‘– â† (ğ‘¦Ì‚ ğ‘–(0),â‹¯,ğ‘¦Ì‚ ğ‘–(ğ‘‘) ) âˆˆ â„ğ‘‘+1
ğ›¼Ì‚ â† ğ‘…âˆ’1ğ‘¦Ì‚ âˆˆ â„ğ‘‘+1
ğ‘– ğ‘– ğ‘–
îˆ³ â† îˆ³âˆªğ›¼Ì‚(0),whereğ›¼Ì‚(0) âˆˆ â„isthefirstcoordinateofğ›¼Ì‚.
ğ‘– ğ‘– ğ‘–
ğ‘” â†medianofîˆ³
outputğ‘”
Algorithm5: Abasiclocalindependentmitigatorforlowdegreepolynomialsâ„ğ‘› â†’ â„.
43Giventhatğ‘ = 0,sinceallğ‘Ÿ(ğ‘—) areseparatedfromeachotherbyatleastonefullinterval,weknow
ğ‘– ğ‘–
that
min|
|ğ‘Ÿ(ğ‘—)
âˆ’ğ‘Ÿ(ğ‘—â€²)|
| â‰¥ min |ğ¼ |,
ğ‘—â‰ ğ‘—â€² | ğ‘– ğ‘– | ğ‘˜âˆˆ[20ğ‘‘2] ğ‘˜
whereweusethe notation|ğ¼ | âˆˆ â„todenotethelengthofthe intervalğ¼ . Bymonotonicityofğ‘â€²,
ğ‘˜ ğ‘˜
weknowthat
min |ğ¼ | = |ğ¼ |,
ğ‘˜ 20ğ‘‘2
ğ‘˜âˆˆ[20ğ‘‘2]
wherethelengthğ‘§ âˆ¶= |ğ¼ |isgivenbythesolutionğ‘§ totheequation
20ğ‘‘2
1 1
ğ‘›ğ‘Ÿğ‘›âˆ’1ğ‘‘ğ‘Ÿ = .
âˆ«
20ğ‘‘2
1âˆ’ğ‘§
Sinceğ‘›,ğ‘‘ â‰¥ 1,solvingthisequationyields
1/ğ‘›
1 1 1
ğ‘§ = 1âˆ’ 1âˆ’ â‰¥
1âˆ’ğ‘’âˆ’1/(20ğ‘›ğ‘‘2)
â‰¥ 1âˆ’ 1âˆ’ = .
( 20ğ‘‘2) ( 40ğ‘›ğ‘‘2) 40ğ‘›ğ‘‘2
Therefore,withprobabilityatleast5/6,wehave
min|
|ğ‘Ÿ(ğ‘—)
âˆ’ğ‘Ÿ(ğ‘—â€²)|
| â‰¥
1
,
ğ‘—â‰ ğ‘—â€² | ğ‘– ğ‘– | 40ğ‘›ğ‘‘2
asdesired.
WefinallyprovethatthatAlgorithm5isalocalmitigatorfordegree-ğ‘‘ multivariatepolynomials.
Theorem 7.12. For any ğ‘›,ğ‘‘ âˆˆ â„•, let ğµ = ğµ(ğŸ,1) âŠ† â„ğ‘› be the unit ball, and let î‰„ âŠ† ğµ be a
ğ‘› ğ‘› ğ‘›
nondegenerateconvexset. Letîˆ´ bethesetofmultivariatepolynomialsâ„ğ‘› â†’ â„oftotaldegreeat
ğ‘›
mostğ‘‘. Letğœ€ âˆˆ [0,1/ 20ğ‘‘],ğ›¿
1
âˆˆ â„ â‰¥0andğ›¿
0
= ğ›¿1/ 4â‹…(80ğ‘›ğ‘‘2)ğ‘‘. Letğ”»
ğ‘›
= ğ”»
ğ‘›,ğœ€,ğ›¿0
bethecollectionofdistributions
îˆ°withuniform marginalonî‰„ suchthatğ¿>ğ›¿0(îˆ´ ) â‰¤ ğœ€,i.e., thereexists amultivariatepolynomial
ğ‘› îˆ° ğ‘›
â„ âˆ¶ â„ğ‘› â†’ â„oftotaldegreeatmostğ‘‘ suchthat
â„™ [|â„(ğ‘¥)âˆ’ğ‘¦| > ğ›¿ ] â‰¤ ğœ€.
0
(ğ‘¥,ğ‘¦)âˆ¼îˆ°
Then Algorithm 5 is a local mitigator that is (ğœ€,ğ›¿ â†’ ğ›¿ )-cutoff loss secure (per Definition 4.6) for
0 1
distributionsğ”» ,whereğ‘  âˆˆ â„•isthesecurityparameter.
ğ‘›
Furthermore,Algorithm5usesatotalofğ‘ (ğ‘‘+1)oraclequeries,andrunsintimeğ‘ â‹…poly(ğ‘‘,ğ‘›),assuming
unitruntimecostforeacharithmeticoperationontherepresentationsofrealnumbersinvolvedinthe
computation.
Proof. Letğ‘ˆ = U(î‰„ )betheuniformdistributiononî‰„ . Fixapopulationdistributionîˆ° âˆˆ ğ”»
ğ‘› ğ‘› ğ‘›,ğœ€,ğ›¿0
andamultivariate polynomialâ„ âˆ¶ â„ğ‘› â†’ â„oftotaldegree atmostğ‘‘ suchthatğ¿>ğ›¿0(â„) â‰¤ ğœ€. We set
îˆ°
ğ‘”ideal(ğ‘¥) = â„(ğ‘¥). Fixtheadversariallychosenfunctionğ‘“ âˆ¶ â„ğ‘› â†’ â„withlossğ¿>ğ›¿0(ğ‘“) â‰¤ ğœ€. Thatis,
îˆ° îˆ°
â„™ [|â„(ğ‘¥)âˆ’ğ‘¦| â‰¥ ğ›¿ ] â‰¤ ğœ€, â„™ [|ğ‘“(ğ‘¥)âˆ’ğ‘¦| â‰¥ ğ›¿ ] â‰¤ ğœ€. (28)
0 0
(ğ‘¥,ğ‘¦)âˆ¼îˆ° (ğ‘¥,ğ‘¦)âˆ¼îˆ°
44Byapplyingthetriangleinequalityandtheunionboundandlookingatthemarginalonî‰„ ,
ğ‘›
â„™ [|ğ‘“(ğ‘¥)âˆ’â„(ğ‘¥)| â‰¥ 2ğ›¿ ] â‰¤ 2ğœ€. (29)
ğ‘¥â†ğ‘ˆ 0
Letğ‘¦ideal = â„(ğ‘¥âˆ—). Foreachğ‘– âˆˆ [ğ‘š],considerthelineğ“ âˆ¶ â„ â†’ â„ğ‘› passingthroughğ‘¥âˆ— andğ‘¥,scaled
ğ‘– ğ‘–
sothatğ“(0) = ğ‘¥âˆ— andğ“(1)isontheboundaryofî‰„ . Letâ„ âˆ¶ â„ â†’ â„,â„ âˆ¶= â„â—¦ğ“ betheunivariate
ğ‘– ğ‘– ğ‘› ğ‘– ğ‘– ğ‘–
polynomialofâ„restrictedtothelineğ“,whichhasdegreeatmostğ‘‘. Letğ›¼ = (ğ›¼(0),â‹¯,ğ›¼(ğ‘‘)) âˆˆ â„ğ‘‘+1
ğ‘– ğ‘– ğ‘– ğ‘–
bethecoefficientssuchthat
ğ‘‘
â„(ğ‘Ÿ) = âˆ‘ğ›¼(ğ‘˜)ğ‘Ÿğ‘˜.
ğ‘– ğ‘–
ğ‘˜=0
Byconstruction,
ğ›¼(0) = â„(0) = â„(ğ“(0)) = â„(ğ‘¥âˆ—) = ğ‘¦ideal.
ğ‘– ğ‘– ğ‘–
Letğ‘Ÿ(0),â‹¯,ğ‘Ÿ(ğ‘‘) âˆˆ â„beasinAlgorithm5. Notethatforallğ‘— âˆˆ {0,â‹¯,ğ‘‘},
ğ‘– ğ‘–
ğ“ ğ‘–(ğ‘Ÿ ğ‘—(ğ‘—) ) = ğ‘¥ ğ‘–(ğ‘—).
Letting ğ‘… ğ‘– = Vand(ğ‘Ÿ ğ‘–(0),â‹¯,ğ‘Ÿ ğ‘–(ğ‘‘) ) âˆˆ â„(ğ‘‘+1)Ã—(ğ‘‘+1) and ğ‘¦ ğ‘– = (â„ ğ‘–(ğ‘Ÿ ğ‘–(0) ),â‹¯,â„ ğ‘–(ğ‘Ÿ ğ‘–(ğ‘‘) )) âˆˆ â„ğ‘‘+1, we have the
matrixequation
ğ‘…ğ›¼ = ğ‘¦,
ğ‘– ğ‘– ğ‘–
andtherefore
ğ›¼ = ğ‘…âˆ’1ğ‘¦.
ğ‘– ğ‘– ğ‘–
(Note that since ğ‘… is a Vandermonde matrix, it is invertible as all of the ğ‘Ÿ(ğ‘—) are distinct, which
ğ‘– ğ‘–
willbetruewithprobability1.)
Usingthenotationinouralgorithm,sinceforallğ‘— âˆˆ {0,â‹¯,ğ‘‘},wehaveğ‘¦(ğ‘—) = â„(ğ‘Ÿ(ğ‘—)) = â„(ğ‘¥(ğ‘—))and
ğ‘– ğ‘– ğ‘– ğ‘–
ğ‘¦Ì‚(ğ‘—) = ğ‘“(ğ‘¥(ğ‘—)),weknowğ‘¦(ğ‘‘) = â„(ğ‘Ÿ(ğ‘—)) = â„(ğ‘¥(ğ‘—))andğ‘¦Ì‚(ğ‘—) = ğ‘“(ğ‘¥(ğ‘—)),sobyEq.(29),
ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘– ğ‘–
| |
â„™ |ğ‘¦Ì‚(ğ‘—) âˆ’ğ‘¦(ğ‘—)| â‰¥ 2ğ›¿ â‰¤ 2ğœ€.
[| ğ‘– ğ‘– | 0]
Byapplyingtheunionboundoverğ‘— âˆˆ {0,â‹¯,ğ‘‘},weget
| | 1
â„™ âˆƒğ‘— âˆˆ {0,â‹¯,ğ‘‘},|ğ‘¦Ì‚(ğ‘—) âˆ’ğ‘¦(ğ‘—)| â‰¥ 2ğ›¿ â‰¤ (2ğ‘‘ +2)ğœ€ â‰¤ 4ğ‘‘ğœ€ â‰¤ .
[ | ğ‘– ğ‘– | 0] 5
Therefore,withprobabilityatleast4/5,
â€– â€–
ğ‘¦Ì‚ âˆ’ğ‘¦ â‰¤ 2ğ›¿ .
â€– ğ‘– ğ‘–â€– âˆ 0
Recallthatğ›¼Ì‚ = ğ‘…âˆ’1ğ‘¦Ì‚. Giventheabove,wehave
ğ‘– ğ‘– ğ‘–
â€– ğ›¼Ì‚ âˆ’ğ›¼â€– = â€– ğ‘…âˆ’1ğ‘¦Ì‚ âˆ’ğ‘…âˆ’1ğ‘¦â€– = â€– ğ‘…âˆ’1(ğ‘¦Ì‚ âˆ’ğ‘¦)â€– â‰¤ â€– ğ‘…âˆ’1â€– â€– ğ‘¦Ì‚ âˆ’ğ‘¦â€– â‰¤ â€– ğ‘…âˆ’1â€– 2ğ›¿ .
â€– ğ‘– ğ‘–â€– âˆ â€– ğ‘– ğ‘– ğ‘– ğ‘–â€– âˆ â€– ğ‘– ğ‘– ğ‘– â€– âˆ â€– ğ‘– â€– âˆâ€– ğ‘– ğ‘–â€– âˆ â€– ğ‘– â€– âˆ 0
We now apply Theorem 7.10 along with Claim 7.11 (and the union bound) to see that with
probabilityatleast1âˆ’1/5âˆ’1/6 = 19/30,
| |
1+|ğ‘Ÿ(ğ‘—)|
â€– ğ‘…âˆ’1â€– â‰¤ max âˆ | ğ‘– | â‰¤ âˆ 2 = (80ğ‘›ğ‘‘2)ğ‘‘.
â€– ğ‘– â€– âˆ 0â‰¤ğ‘—â‰¤ğ‘‘ 0â‰¤ğ‘—â€²â‰¤ğ‘‘ | | |ğ‘Ÿ ğ‘–(ğ‘—) âˆ’ğ‘Ÿ ğ‘–(ğ‘—â€²)| |
|
0â‰¤ğ‘—â€²â‰¤ğ‘‘ 40ğ‘›1 ğ‘‘2
ğ‘—â€²â‰ ğ‘— ğ‘—â€²â‰ ğ‘—
45Therefore,inthiscase,
ğ›¿
â€– ğ›¼Ì‚ âˆ’ğ›¼â€– â‰¤ (80ğ‘›ğ‘‘2)ğ‘‘ â‹…2ğ›¿ = 1 ,
â€– ğ‘– ğ‘–â€– âˆ 0 2
andinparticular,lookingatthefirstcoordinate,
| | | | ğ›¿
|ğ›¼Ì‚(0) âˆ’ğ‘¦ideal| = |ğ›¼Ì‚(0) âˆ’ğ›¼(0)| â‰¤ 1 .
| ğ‘– | | ğ‘– ğ‘– | 2
This,foreachğ‘– âˆˆ [ğ‘š],withprobabilityatleast19/30,weaddanestimatetoîˆ³thatisğ›¿ /2-closeto
1
ğ‘¦ideal = â„(ğ‘¥âˆ—) = ğ‘”ideal(ğ‘¥âˆ—). ByastandardChernoffbound,itfollowsthatğ‘”,themedianofîˆ³atthe
îˆ°
endofAlgorithm5,satisfies
â„™[|ğ‘” âˆ’ğ‘” îˆ°ideal(ğ‘¥âˆ—)| > ğ›¿ 1/2] â‰¤ ğ‘’âˆ’ğ‘šâ‹…(1/ 2102) â‰¤ ğ‘’âˆ’ğ‘š/200 = 2âˆ’Î©(ğ‘ ).
Thisholdsforanyğ‘¥âˆ— âˆˆ î‰„ ,sowe(inparticular)have
ğ‘›
3ğ›¿
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ â„™ |ğ‘” âˆ’ğ‘”ideal(ğ‘¥âˆ—)| â‰¥ 1 â‰¤ 2âˆ’Î©(ğ‘ ). (30)
ğ‘› [ îˆ° 4 ]
ThisestablishesItem2inDefinition4.6.
ToseeItem1inDefinition4.6,wecanusethetriangleinequality,theunionbound,Eq.(30),and
Eq.(28)(sinceğ‘”ideal(ğ‘¥âˆ—) = â„(ğ‘¥))toseethat
îˆ°
3ğ›¿
â„™ |ğ‘” âˆ’ğ‘¦| â‰¥ ğ›¿ + 1 â‰¤ ğœ€ +2âˆ’Î©(ğ‘ ).
(ğ‘¥,ğ‘¦)âˆ¼îˆ° [ 0 4 ]
ğ‘”â†ğ‘€ğ‘“(ğ‘¥,1ğ‘›,1ğ‘ )
Sinceğ›¿ +3ğ›¿ /4 â‰¤ ğ›¿ ,thisestablishesItem1inDefinition4.6.
0 1 1
Remark7.13. Whiletheğ‘›ğ‘‚(ğ‘‘) blowupintheğ›¿ errormightseemundesirable,weshowthatthisis
unavoidableforarelatednotionofcorrectionintheexact setting,definedasfollows.
Letğ”» bethecollectionofdistributionsîˆ°withuniformmarginalonî‰„ suchthatğ¿>ğ›¿0(îˆ´ ) = 0,
ğ‘›,ğ›¿0 ğ‘› îˆ° ğ‘›
i.e., there exists a multivariate polynomial â„ âˆ¶ â„ğ‘› â†’ â„ of total degree at most ğ‘‘ such that
â„™ [|â„(ğ‘¥)âˆ’ğ‘¦| > ğ›¿ ] = 0. Insteadofcutofflosssecurity(Item2inDefinition4.6),supposewe
(ğ‘¥,ğ‘¦)âˆ¼îˆ° 0
wantthestrongerpropertythatforallğ‘“ âˆ¶ î‰„ â†’ â„suchthatğ¿>ğ›¿0(ğ‘“) â‰¤ ğœ€,
ğ‘› îˆ°
âˆ€ğ‘¥âˆ— âˆˆ î‰„ âˆ¶ â„™ [|ğ‘¦âˆ— âˆ’â„(ğ‘¥âˆ—)| â‰¥ ğ›¿ ] â‰¤ ğœ‡(ğ‘ ), (31)
ğ‘› 1
ğ‘¦âˆ—â†ğ‘€ğ‘“(ğ‘¥âˆ—,1ğ‘›,1ğ‘ )
wherethemitigatorğ‘€ğ‘“(ğ‘¥,1ğ‘›,1ğ‘ )hasaccesstoğ‘“ (butnotîˆ°).28
Weclaimtwofactsinthissetting:
28ThedifferencebetweentherequirementinEq.(31)andtherequirementofItem2inDefinition4.6isthatin
Eq.(31),ğ‘¦âˆ—mustbeclosetothetruelabelingfunctionâ„(whichisunknowntothedefender),whereasinDefinition4.6
ğ‘¦âˆ—mustbeclosetoacanonicalfunctionğ‘”idealchosenbythedefender.
îˆ°
46â€¢ Algorithm 5 achieves the security requirement of Eq. (31). This follows directly from the
analysisgivenintheproofofTheorem7.12.
â€¢ Themultiplicativeblowupofğ›¿ = ğ‘›Î©(ğ‘‘)ğ›¿ isunavoidable.
1 0
We now explain why the blowup is unavoidable, assuming that the mitigator only has oracle
accesstothepotentially-backdooredğ‘“Ìƒ(butdoesnothaverandomsamplesfromthepopulation
distribution).29 Toseethis,letğ‘‘ âˆˆ â„•beeven,andconsiderthefollowingpolynomialsofdegreeat
mostğ‘‘,
ğ‘‘/2
ğ‘›
â„ (ğ‘¥ ,â‹¯,ğ‘¥ ) â‰¡ 0, and â„ (ğ‘¥ ,â‹¯,ğ‘¥ ) = 1âˆ’âˆ‘ğ‘¥2 ,
0 1 ğ‘› 1 1 ğ‘› ğ‘–
( )
ğ‘–=1
alongwiththeuniformdistributionğ‘ˆ = U(ğµ)overtheunitballinâ„ğ‘›. Attheorigin,i.e.,ğ‘¥âˆ— = ğŸ,we
haveâ„ (ğ‘¥âˆ—) = ğ‘forğ‘ âˆˆ {0,1}. However,withhighprobabilityovertheunitball,thesepolynomials
ğ‘
areveryclose:
âˆš
1 ğ‘› 1 ğ‘› 1 1/2
â„™ | â„ (ğ‘¥)| â‰¥ = â„™ 1âˆ’âˆ‘ğ‘¥2 â‰¥ âˆš = â„™ âˆ‘ğ‘¥2 â‰¤ 1âˆ’ âˆš
ğ‘¥âˆ¼ğ‘ˆ[| 1 | ğ‘›ğ‘‘/4] ğ‘¥âˆ¼ğ‘ˆ[ ğ‘– ğ‘›] ğ‘¥âˆ¼ğ‘ˆ[ ğ‘– ( ğ‘›) ]
ğ‘–=1 ğ‘–=1
ğ‘›/2
1 âˆš
= 1âˆ’ âˆš â‰¤ ğ‘’âˆ’ ğ‘›/2.
( ğ‘›)
That is, there are two polynomials of degree at most ğ‘‘ that are additively 1/ğ‘›ğ‘‘/4-close with
âˆš
probabilityatleast1âˆ’ğ‘’âˆ’ ğ‘›/2,yettheydifferadditivelyattheoriginby1. Therefore,wecanfix
ğ‘“ = â„ â‰¡ 0andexaminethedistributionofğ‘€ğ‘“(ğŸ,1ğ‘›,1ğ‘ ). Consideringîˆ°tobeexactlygivenbyâ„
0 âˆš 0
orâ„ ,inbothcases,wehaveshownthatğ¿>ğ›¿0(ğ‘“) â‰¤ ğ‘’âˆ’ ğ‘›/2 forğ›¿ = 1/ğ‘›ğ‘‘/4. However,tosatisfyour
1 îˆ° 0
strongersecuritynotion,wewouldneed
â„™[| |ğ‘€ğ‘“(ğŸ,1ğ‘›,1ğ‘ )âˆ’â„ ğ‘(ğŸ)| | â‰¥ ğ›¿ 1] â‰¤ ğœ‡(ğ‘ )
for both values ğ‘ âˆˆ {0,1}. Since â„ (ğŸ) = ğ‘ and ğœ‡ is negligible, this is only possible if ğ›¿ = Î©(1).
ğ‘ 1
Therefore,ğ›¿ = ğ‘›Î©(ğ‘‘) â‹…ğ›¿ ,asdesired.
1 0
8 Robust Mean Estimation
We analyze a simple algorithm for robust mean estimation in the Huber contamination model
(Huber, 1964, 1981). Our algorithm (Algorithm 6) computes a mean-of-medians, which is the
reverse of the extensively-studied median-of-means estimator (Nemirovskij and Yudin, 1983;
Jerrumetal.,1986; Laforgueetal.,2021,among numerous others). Algorithm6usesan additional
symmetryassumptiononthedistribution,andobtainsaslowerconvergenceratethanthestandard
median-of-meansofestimator. TheadvantageofAlgorithm6,isthatithasbetterrobustnessto
arbitrarynoise,asillustratedinthefollowingsection.
Weremarkthat Algorithm 6 was recently studiedby Zhong, Huang, Yang, and Wang (2021) for
heavy-taileddistributions;however,theydidnotanalyzeitsrobustnesstoarbitrarynoise,aswe
29Aslightlymoreelaborateargumentcanshowthattheblowupisunavoidableevenifthemitigatordoeshave
randomsamplesfromthepopulationdistribution.
47do here.30 Additionally, we note that one could also use other more advanced techniques for
robustmeanestimation. Forexample,thealgorithmofNovikov,Steurer,andTiegel(2023)gives
better convergence performance. However, these more-advanced algorithms are considerably
morecomplex. Inthecontextofdesigningasecuritymechanismasinthispaper,thesimplicityof
Algorithm6canbeasignificantadvantage.
8.1 Why not use the standard Median-of-Means estimator?
The standard median-of-means estimator partitions a sample of size ğ‘š = ğ‘˜ğ‘ into ğ‘˜ batches of
sizeğ‘ each. Foreachbatchğ‘– âˆˆ [ğ‘˜],it computestheempirical averageofthe samplesinthe batch.
Finally,itoutputsthemedianoftheseaverages. Namely,theestimatoris
1 1
ğœ‡Ì‚= median âˆ‘ğ‘¥1,â€¦, âˆ‘ğ‘¥ğ‘˜ , (32)
(ğ‘ ğ‘— ğ‘ ğ‘— )
ğ‘—âˆˆ[ğ‘] ğ‘—âˆˆ[ğ‘]
wherebatchğ‘–isğ‘¥ğ‘– = (ğ‘¥ğ‘–,â€¦,ğ‘¥ğ‘–).
1 ğ‘
Toseewhyweoptnottousethisestimator,consideramixturedistribution
2 1
îˆ° = â‹…ğ‘ˆ + â‹…îˆ½,
3 3
where ğ‘ˆ = U({âˆ’1,1}) is the uniform distribution on {âˆ’1,1}, and îˆ½ is a distribution of arbitrary
noise,sayâ„™ [ğ‘¥ = ğ‘] = 1forsomelargeğ‘. Thelocationparameterweareinterestedinrecovering
ğ‘¥âˆ¼îˆ½
isğœ‡ = ğ”¼[ğ‘ˆ] = 0. Weshowthatğœ‡Ì‚asinEq.(32)doesnotconvergetoğœ‡,specifically,
ğ‘šâ†’âˆ
â„™[|ğœ‡Ì‚âˆ’ğœ‡| â‰¥ 1] âˆ’âˆ’âˆ’âˆ’â†’ 1.
Considertwocases.
â€¢ CaseI:ğ‘ = 1. Inthiscase,ğœ‡Ì‚isamedianofğ‘švalues,eachofwhichisin{âˆ’1,1,ğ‘}. Ifğ‘šisodd,
thenğœ‡Ì‚âˆˆ {âˆ’1,1,ğ‘},soâ„™[|ğœ‡Ì‚âˆ’ğœ‡| â‰¥ 1] = 1. Otherwise,ifğ‘šiseven,andwedefinethemedianas
somevalueintherange[ğ‘¥ ğ‘¡,ğ‘¥ ğ‘¡+1]forğ‘¡ = âŒŠğ‘¡/2âŒ‹,thenâ„™[|ğœ‡Ì‚âˆ’ğœ‡| = 1] â‰¥ â„™[ğ‘¥
ğ‘¡
= ğ‘¥
ğ‘¡+1
= 1] âˆ’âˆ’ğ‘š âˆ’â†’ âˆ’â†’âˆ 1.
â€¢ Case II: ğ‘ â‰¥ 2. In this case, for each ğ‘– âˆˆ [ğ‘˜], batch ğ‘– contains at least one sample from
îˆ½ with probability 1 âˆ’ (2/3)ğ‘ > 1. Hence, as â„™[ğµ] âˆ’âˆ’ğ‘š âˆ’â†’ âˆ’â†’âˆ 1, where ğµ is the event that a
2
strict majority of the batches each contain at least one item equal to ğ‘. This implies that
â„™[|ğœ‡Ì‚âˆ’ğœ‡| â‰¥ ğ‘âˆ’ğ‘+1 ] âˆ’âˆ’ğ‘š âˆ’â†’ âˆ’â†’âˆ 1forğ‘ arbitrarilylarge.
ğ‘
We see that in both cases, the median-of-means does not converge to ğœ‡ = 0. This limitation is
overcomebyAlgorithm6,asweshowinthenextsection.
8.2 The Mean-of-Medians estimator
Theorem8.1. Letîˆ¼,îˆ½ âˆˆ Î”(â„)bedistributions,letğ›¼ â‰¥ 0,andlet
îˆ° = (1âˆ’ğ›¼)â‹…îˆ¼ +ğ›¼ â‹…îˆ½
beamixturedistribution. Assumethat:
30Themean-of-mediansalgorithmwasalsousedbyXueetal.(2023).
48Assumptions:
{ }
â€¢ îˆ¿ = ğ‘¥ ,â€¦,ğ‘¥ âŠ† â„isfinite.
1 |îˆ¿|
MeanOfMedians(îˆ¿):
âˆš
ğ‘ â† âŒŠ |îˆ¿|âŒ‹ âŠ³Thereareğ‘ batches,eachofsizeğ‘
for ğ‘– âˆˆ [ğ‘]:
{ }
ğ‘š â†medianof ğ‘¥ğ‘–,ğ‘¥ğ‘–,â€¦,ğ‘¥ğ‘– ,whereğ‘¥ğ‘– = ğ‘¥
ğ‘– 1 2 ğ‘ ğ‘— (ğ‘–âˆ’1)ğ‘+ğ‘—
output 1 âˆ‘ ğ‘š
|ğ‘| ğ‘–âˆˆ[ğ‘] ğ‘–
Algorithm6: Asimplealgorithmforrobustmeanestimation.
(a) îˆ¼ hasmeanğœ‡ âˆˆ â„,andthereexistğµ,ğ›½ â‰¥ 0suchthatâ„™ ğ‘¥âˆ¼îˆ¼[|ğ‘¥ âˆ’ğœ‡| > ğµ] < ğ›½.
(b) ğ›¼ andğ›½ aresmall,suchthat(1âˆ’ğ›¼)(1âˆ’ğ›½) â‰¥ 2/3.
(c) îˆ°issymmetricaboutğœ‡,namely,foranymeasurablesetğ´ âŠ† â„,îˆ°(ğœ‡+ğ´) = îˆ°(ğœ‡âˆ’ğ´),where
ğœ‡Â±ğ´ = {ğœ‡Â±ğ‘ âˆ¶ ğ‘ âˆˆ ğ´}.
Letğ‘š âˆˆ â„•,letîˆ¿ âˆ¼ îˆ°ğ‘š,andlet
ğœ‡Ì‚= MeanOfMedians(îˆ¿)
asinAlgorithm6. Then:
1. ğœ‡Ì‚isunbiased,i.e.,ğ”¼ îˆ¿âˆ¼îˆ°ğ‘š[ğœ‡Ì‚] = ğœ‡.
| | âˆš { }
2. ğœ‡Ì‚is concentrated, i.e., âˆ€ğœ€ â‰¥ 0 âˆ¶ â„™ îˆ¿âˆ¼îˆ°ğ‘š[| |ğœ‡Ì‚âˆ’ğœ‡|
|
â‰¥ ğœ€
]
â‰¤ 4exp(âˆ’ğ›¾ ğ‘š) for ğ›¾ = min 11 00, 2 ğµğœ€ 22
andğ‘šlargeenough.
Notethatinthetheoremstatement,îˆ½mightnothaveamean.
8.2.1 ProofofTheorem8.1
ProofofTheorem8.1. First,weshowthatğœ‡Ì‚isunbiased. Fixğ‘– âˆˆ [ğ‘]. Seeingasîˆ°issymmetricabout
ğœ‡, for any measurable ğ´ âŠ† â„, îˆ°(ğ´) = îˆ°(2ğœ‡ âˆ’ğ´). Consequently, for every ğ‘— âˆˆ [ğ‘], ğ‘¥ğ‘– =ğ‘‘ 2ğœ‡ âˆ’ğ‘¥ğ‘–.
ğ‘— ğ‘—
Becausetheğ‘¥ğ‘– variablesareindependent,thisimpliesthat
ğ‘—
ğ”¼[ğ‘š ğ‘–] = ğ”¼ ğ‘¥ğ‘– âˆ¼îˆ°ğ‘[median(ğ‘¥ 1ğ‘–,â€¦,ğ‘¥ ğ‘ğ‘– )]
1âˆ¶ğ‘
= ğ”¼ ğ‘¥ğ‘– âˆ¼îˆ°ğ‘[median(2ğœ‡âˆ’ğ‘¥ 1ğ‘–,â€¦,2ğœ‡âˆ’ğ‘¥ ğ‘ğ‘– )] (ğ‘¥ ğ‘—ğ‘– =ğ‘‘ 2ğœ‡âˆ’ğ‘¥ ğ‘—ğ‘–)
1âˆ¶ğ‘
= ğ”¼ ğ‘¥ğ‘– âˆ¼îˆ°ğ‘[2ğœ‡âˆ’median(ğ‘¥ 1ğ‘–,â€¦,ğ‘¥ ğ‘ğ‘– )]
1âˆ¶ğ‘
= 2ğœ‡âˆ’ğ”¼[ğ‘š]. (33)
ğ‘–
Hence,
âˆ€ğ‘– âˆˆ [ğ‘] âˆ¶ ğ”¼[ğ‘š] = ğœ‡, (34)
ğ‘–
49soğ”¼ 1 âˆ‘ ğ‘š = ğœ‡,establishingItem1inthetheorem.
[|ğ‘| ğ‘–âˆˆ[ğ‘] ğ‘–]
Second,weshowthatğœ‡Ì‚isconcentrated. Wecanexpressthedistributionîˆ°asamixture
îˆ° = (1âˆ’ğ›¼)(1âˆ’ğ›½)â‹…îˆ¼good +(1âˆ’ğ›¼)ğ›½ â‹…îˆ¼bad +ğ›¼ â‹…îˆ½, (35)
| |
whereîˆ¼good = îˆ¼ ||ğ‘¥ âˆ’ğœ‡| â‰¤ ğµ andîˆ¼bad = îˆ¼ ||ğ‘¥ âˆ’ğœ‡| > ğµ .
( | ) ( | )
Fix ğ‘– âˆˆ [ğ‘]. We may assume that for every ğ‘— âˆˆ [ğ‘] there is an indicator ğ‘”ğ‘– âˆ¼ Ber((1âˆ’ğ›¼)(1âˆ’ğ›½))
ğ‘—
suchthatifğ‘”ğ‘– = 1thenğ‘¥ğ‘– âˆ¼ îˆ¼good,andifğ‘”ğ‘– = 0thenğ‘¥ğ‘– âˆ (1âˆ’ğ›¼)ğ›½ â‹…îˆ¼bad +ğ›¼ â‹…îˆ½.
ğ‘— ğ‘— ğ‘— ğ‘—
Observethatifastrictmajorityof{ğ‘¥ğ‘– âˆ¶ ğ‘— âˆˆ [ğ‘]}isintheintervalğ¼ = [ğœ‡âˆ’ğµ,ğœ‡+ğµ],thenthe
ğ‘— ğœ‡Â±ğµ
medianğ‘š isinğ¼ aswell. Namely,
ğ‘– ğœ‡Â±ğµ
|
â„™[ğ‘š ğ‘– âˆˆ ğ¼ ğœ‡Â±ğµ |ğ¸ ğ‘–] = 1,
whereğ¸
ğ‘–
istheeventwhere| |{ğ‘— âˆˆ [ğ‘] âˆ¶ ğ‘¥ ğ‘—ğ‘– âˆˆ ğ¼ ğœ‡Â±ğµ}| |/ğ‘ > 1/ 2.
1
â„™[Â¬ğ¸] â‰¤ â„™ âˆ‘ğ‘”ğ‘– â‰¤
ğ‘– [ ğ‘— 2]
ğ‘—âˆˆ[ğ‘]
| |
| | 1
= â„™ |ğ”¼ âˆ‘ğ‘”ğ‘– âˆ’ âˆ‘ğ‘”ğ‘–| â‰¥ (ByItem(b))
[| [ ğ‘— ] ğ‘—| 6]
| ğ‘—âˆˆ[ğ‘] ğ‘—âˆˆ[ğ‘] |
2
1
â‰¤ 2exp âˆ’2ğ‘â‹… . (Hoeffdingâ€™sinequality) (36)
( (6) )
Observethat ğ¸ isan eventthat issymmetricwithrespect toğœ‡. Formally,if ğ‘¥ğ‘– = (ğ‘¥ğ‘–,â€¦,ğ‘¥ğ‘–) and
ğ‘– 1 ğ‘
2ğœ‡âˆ’ğ‘¥ğ‘– = (2ğœ‡âˆ’ğ‘¥ğ‘–,â€¦,2ğœ‡âˆ’ğ‘¥ğ‘–),then
1 ğ‘
ğ‘¥ğ‘– âˆˆ ğ¸ âŸº (2ğœ‡âˆ’ğ‘¥ğ‘–) âˆˆ ğ¸. (37)
ğ‘– ğ‘–
Thisimpliesthatthedistributionîˆ°ğ‘|ğ¸ issymmetricaboutğœ‡. Namely,foranymeasurableğ´ âˆˆ â„ğ‘,
ğ‘–
â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[ğ‘¥ğ‘– âˆˆ ğ´|ğ¸ ğ‘–] =
â„™
ğ‘¥
â„™ğ‘–âˆ¼îˆ°ğ‘[ğ‘¥ [ğ‘– ğ‘¥âˆˆ
ğ‘–
ğ´
âˆˆ
ğ¸âˆ© ]ğ¸ ğ‘–]
ğ‘¥ğ‘–âˆ¼îˆ°ğ‘ ğ‘–
=
â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[(2ğœ‡âˆ’ğ‘¥ğ‘–) âˆˆ ğ´âˆ©ğ¸ ğ‘–]
(Symmetryofîˆ°aboutğœ‡)
â„™ [ğ‘¥ğ‘– âˆˆ ğ¸]
ğ‘¥ğ‘–âˆ¼îˆ°ğ‘ ğ‘–
â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[(2ğœ‡âˆ’ğ‘¥ğ‘–) âˆˆ ğ´ âˆ§ (2ğœ‡âˆ’ğ‘¥ğ‘–) âˆˆ ğ¸ ğ‘–]
=
â„™ [ğ‘¥ğ‘– âˆˆ ğ¸]
ğ‘¥ğ‘–âˆ¼îˆ°ğ‘ ğ‘–
=
â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[ğ‘¥ğ‘– âˆˆ (2ğœ‡âˆ’ğ´) âˆ§ ğ‘¥ğ‘– âˆˆ ğ¸ ğ‘–]
(ByEq.(37))
â„™ [ğ‘¥ğ‘– âˆˆ ğ¸]
ğ‘¥ğ‘–âˆ¼îˆ°ğ‘ ğ‘–
= â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[ğ‘¥ğ‘– âˆˆ (2ğœ‡âˆ’ğ´)|ğ¸ ğ‘–]
= â„™ ğ‘¥ğ‘–âˆ¼îˆ°ğ‘[(2ğœ‡âˆ’ğ‘¥ğ‘–) âˆˆ ğ´|ğ¸ ğ‘–].
50Namely,ğ‘¥ğ‘–|ğ¸ =ğ‘‘ (2ğœ‡âˆ’ğ‘¥ğ‘–)|ğ¸. BythesameargumentasinEq.(33),thisimpliesthat
ğ‘– ğ‘–
ğ”¼[ğ‘š |ğ¸] = ğœ‡. (38)
ğ‘– ğ‘–
Letğ¸ = âˆ© ğ¸. Thenforanyğœ€ â‰¥ 0
ğ‘–âˆˆ[ğ‘] ğ‘–
| | | |
| 1 | | 1 | |
â„™ |ğœ‡âˆ’ âˆ‘ğ‘š| â‰¥ ğœ€ â‰¤ â„™[Â¬ğ¸]+â„™ | âˆ‘(ğ‘š âˆ’ğœ‡)| â‰¥ ğœ€ |ğ¸
[| |ğ‘˜| ğ‘–| ] [||ğ‘˜| ğ‘– | | ]
| ğ‘–âˆˆ[ğ‘˜] | | ğ‘–âˆˆ[ğ‘˜] |
2ğ‘ğœ€2
â‰¤ â„™[Â¬ğ¸]+2exp âˆ’ (Hoeffdingâ€™sinequality,Eq.(38))
( ğµ2 )
ğ‘ 2ğ‘ğœ€2
â‰¤ 2ğ‘â‹…exp âˆ’ +2exp âˆ’ (ByEq.(36),unionbound)
( 18) ( ğµ2 )
ğ‘ 2ğ‘ğœ€2
â‰¤ 2â‹…exp âˆ’ +2exp âˆ’ (Forğ‘ â‰¥ 102)
( 100) ( ğµ2 )
â‰¤ 4exp(âˆ’ğ›¾ğ‘),
{ }
whereğ›¾ = min 1 , 2ğœ€2 ,asdesired.
100 ğµ2
9 Future Directions
We have provided preliminary evidence that techniques based on random self-reducibility can
beeffectiveatbackdoormitigation. Therearetwodirectionsforfutureworksthatcouldtryto
advancethisideatowardspracticalapplications.
One central direction is to search for additional families of distributions ğ”» with random self-
reducibility properties that are suitable for backdoor mitigation. We have seen that if the label
distribution is close to a linear, polynomial, or ğœ-heavy function, then secure backdoor mitigation
ispossible. Butdothereexistbroaderfamiliesofdistributionsthatappearcommonlyinreal-world
dataandarealsoconduciveforbackdoormitigation?
A second avenue for exploration is to take advantage of the representation of the proposed
ML model ğ‘“Ìƒ. Our constructions treat ğ‘“Ìƒ as a black-box, and make no assumptions on how it is
implemented orrepresented. Butperhaps one could obtainbetter results by either(i) assuming
thatğ‘“Ìƒbelongstosomeclassoffunctions,e.g.,itisimplementedbyaneuralnetworkofacertain
architecture; or, moreover (ii) using whitebox access to the representation of ğ‘“Ìƒ (the code or
parametersofthemodel)inordertoobtainmoresophisticatedformsofrandomself-reducibility?
Acknowledgments. JS,NVandVVweresupportedinpartbyNSFCNS-2154149andaSimons
InvestigatorAward. JSwouldliketothankIdoNachumandShayMoranforhelpfulconversations.
51References
YossiAdi,CarstenBaum,MoustaphaCissÃ©,BennyPinkas,andJosephKeshet. Turningyourweak-
nessintoastrength: Watermarkingdeepneuralnetworksbybackdooring. InWilliamEnckand
AdriennePorterFelt,editors,27thUSENIXSecuritySymposium,USENIXSecurity2018,Baltimore,
MD,USA,August 15-17,2018,pages1615â€“1631. USENIXAssociation,2018. URLhttps://
www.usenix.org/conference/usenixsecurity18/presentation/adi.
VipulArora,ArnabBhattacharyya,NoahFleming,EstyKelman,andYuichiYoshida. Lowdegree
testing over the reals. In Nikhil Bansal and Viswanath Nagarajan, editors, Proceedings of
the 2023 ACM-SIAM Symposium on Discrete Algorithms, SODA 2023, Florence, Italy, January
22-25, 2023, pages 738â€“792. SIAM, 2023. doi:10.1137/1.9781611977554.CH31. URL https:
//doi.org/10.1137/1.9781611977554.ch31.
Idan Attias, Steve Hanneke, Alkis Kalavasis, Amin Karbasi, and Grigoris Velegkas. Universal
rates for regression: Separations between cut-off and absolute loss. In Shipra Agrawal and
Aaron Roth, editors, The Thirty Seventh Annual Conference on Learning Theory, June 30 - July 3,
2023,Edmonton,Canada,volume247ofProceedingsofMachineLearningResearch,pages359â€“
405. PMLR, 2024. URL https://proceedings.mlr.press/v247/attias24a.
html.
AvrimBlum,MerrickL.Furst,MichaelJ.Kearns,andRichardJ.Lipton. Cryptographicprimitives
based on hard learning problems. In Douglas R. Stinson, editor, Advances in Cryptology -
CRYPTO â€™93,13th Annual InternationalCryptology Conference, SantaBarbara, California, USA,
August 22-26, 1993, Proceedings, volume 773 of Lecture Notes in Computer Science, pages 278â€“
291.Springer,1993. doi:10.1007/3-540-48329-2_24. URLhttps://doi.org/10.1007/
3-540-48329-2_24.
Manuel Blum and Sampath Kannan. Designing programs that check their work. In David S.
Johnson,editor,Proceedingsofthe21stAnnualACMSymposiumonTheoryofComputing,May
14-17,1989,Seattle,Washington,USA,pages86â€“97.ACM,1989. doi:10.1145/73007.73015. URL
https://dl.acm.org/doi/10.1145/73007.73015.
ManuelBlum,RobertW.Floyd,VaughanR.Pratt,RonaldL.Rivest,andRobertEndreTarjan. Time
boundsforselection. J.Comput.Syst.Sci.,7(4):448â€“461,1973. doi:10.1016/S0022-0000(73)80033-9.
URLhttps://doi.org/10.1016/S0022-0000(73)80033-9.
Manuel Blum,Michael Luby, andRonitt Rubinfeld. Self-testing/correcting withapplications to
numericalproblems. InHarrietOrtiz,editor,Proceedingsofthe22ndAnnualACMSymposium
onTheoryofComputing,May13-17,1990,Baltimore,Maryland,USA,pages73â€“83.ACM,1990.
doi:10.1145/100216.100225. URLhttps://doi.org/10.1145/100216.100225.
Ben Brubaker. In neural networks, unbreakable locks can hide invisible doors.
Quanta Magazine, March 2023. URL https://www.quantamagazine.org/
cryptographers-show-how-to-hide-invisible-backdoors-in-ai-20230302.
Accessed: 2024-10-30. Archived URL: https://web.archive.org/
web/20230302155439/https://www.quantamagazine.org/
cryptographers-show-how-to-hide-invisible-backdoors-in-ai-20230302/.
52Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural net-
works via region-based classification. In Proceedings of the 33rd Annual Computer Secu-
rity Applications Conference, Orlando, FL, USA, December 4-8, 2017, pages 278â€“287. ACM,
2017. doi:10.1145/3134600.3134606. URL https://doi.org/10.1145/3134600.
3134606.
BryantChen,WilkaCarvalho,NathalieBaracaldo,HeikoLudwig,BenjaminEdwards,Taesung
Lee,IanM.Molloy,andBiplavSrivastava. Detectingbackdoorattacksondeepneuralnetworks
by activation clustering. In HuÃ¡scar Espinoza, SeÃ¡n Ã“ hÃ‰igeartaigh, Xiaowei Huang, JosÃ©
HernÃ¡ndez-Orallo,andMauricioCastillo-Effen,editors,WorkshoponArtificialIntelligenceSafety
2019co-locatedwiththeThirty-ThirdAAAIConferenceonArtificialIntelligence2019(AAAI-19),
Honolulu,Hawaii,January27,2019,volume2301ofCEURWorkshopProceedings.CEUR-WS.org,
2019. URLhttps://ceur-ws.org/Vol-2301/paper_18.pdf.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks
on deep learning systems using data poisoning. CoRR, abs/1712.05526, 2017. URL http:
//arxiv.org/abs/1712.05526.
Paul F. Christiano, Jacob Hilton, Victor Lecomte, and Mark Xu. Backdoor defense, learnability
andobfuscation. CoRR,abs/2409.03077,2024. doi:10.48550/ARXIV.2409.03077. URLhttps:
//doi.org/10.48550/arXiv.2409.03077.
JeremyCohen,ElanRosenfeld,andJ.ZicoKolter. Certifiedadversarialrobustnessviarandomized
smoothing. InKamalika ChaudhuriandRuslanSalakhutdinov,editors,Proceedingsofthe36th
InternationalConferenceonMachineLearning,ICML2019,9-15June2019,LongBeach,California,
USA, volume 97 of Proceedings of Machine Learning Research, pages 1310â€“1320. PMLR, 2019.
URLhttp://proceedings.mlr.press/v97/cohen19c.html.
VitalyFeldman. Onthepowerofmembershipqueriesinagnosticlearning. J.Mach.Learn.Res.,
10:163â€“182,2009. doi:10.5555/1577069.1577076. URLhttps://dl.acm.org/doi/10.
5555/1577069.1577076.
WGautschi. Ontheinversesofvandermondeandconfluentvandermondematrices.i,ii. Numer.
Math,4:117â€“123,1962. doi:10.1007/978-1-4614-7034-2_8.
OdedGoldreichandLeonidA.Levin. Ahard-corepredicateforallone-wayfunctions. InDavidS.
Johnson,editor,Proceedingsofthe21stAnnualACMSymposiumonTheoryofComputing,May
14-17,1989,Seattle,Washington,USA,pages25â€“32.ACM,1989. doi:10.1145/73007.73010.
ShafiGoldwasserandSilvioMicali. Probabilisticencryptionandhowtoplaymentalpokerkeeping
secret all partial information. In Harry R. Lewis, Barbara B. Simons, Walter A. Burkhard,
and Lawrence H. Landweber, editors, Proceedings of the 14th Annual ACM Symposium on
TheoryofComputing,May5-7,1982,SanFrancisco,California,USA,pages365â€“377.ACM,1982.
doi:10.1145/800070.802212. URLhttps://doi.org/10.1145/800070.802212.
ShafiGoldwasser,MichaelP.Kim,VinodVaikuntanathan,andOrZamir. Plantingundetectable
backdoors in machine learning models. In 63rd IEEE Annual Symposium on Foundations of
ComputerScience,FOCS2022,Denver,CO,USA,October31-November3,2022,pages931â€“942.
53IEEE, 2022. doi:10.1109/FOCS54457.2022.00092. URL https://doi.org/10.1109/
FOCS54457.2022.00092.
NoahGolowichandAnkurMoitra. Editdistancerobustwatermarksforlanguagemodels. IACR
Cryptol.ePrintArch.,page898,2024. URLhttps://eprint.iacr.org/2024/898.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Eval-
uating backdooring attacks on deep neural networks. IEEE Access, 7:47230â€“47244,
2019. doi:10.1109/ACCESS.2019.2909068. URLhttps://doi.org/10.1109/ACCESS.
2019.2909068.
JonathanHayase,WeihaoKong,RaghavSomani,andSewoongOh. Spectre: defendingagainst
backdoorattacksusingrobuststatistics. InMarinaMeilaandTongZhang,editors,Proceedings
of the38th InternationalConferenceon Machine Learning, volume139 ofProceedings ofMachine
LearningResearch,pages4129â€“4139.PMLR,18â€“24Jul2021. URLhttps://proceedings.
mlr.press/v139/hayase21a.html.
SanghyunHong,NicholasCarlini,andAlexeyKurakin. Handcraftedbackdoorsindeepneural
networks. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,
editors,AdvancesinNeuralInformationProcessingSystems35: AnnualConferenceonNeuralInfor-
mationProcessingSystems2022,NeurIPS2022,NewOrleans,LA,USA,November28-December9,
2022,2022.URLhttp://papers.nips.cc/paper_files/paper/2022/hash/
3538a22cd3ceb8f009cc62b9e535c29f-Abstract-Conference.html.
PeterJHuber. Robustestimationofalocationparameter. TheAnnalsofMathematicalStatistics,35
(1):73â€“101,1964. doi:10.1007/978-1-4612-4380-9_35.
Peter J Huber. Robust statistics. Wiley Series in Probability and Mathematical Statistics, 1981.
doi:10.1002/9780470434697.
Mark Jerrum, Leslie G. Valiant, and Vijay V. Vazirani. Random generation of combinatorial
structuresfromauniformdistribution. Theor.Comput.Sci.,43:169â€“188,1986. doi:10.1016/0304-
3975(86)90174-X. URLhttps://doi.org/10.1016/0304-3975(86)90174-X.
Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Intrinsic certified robustness of bagging
againstdatapoisoningattacks. InThirty-FifthAAAIConferenceonArtificialIntelligence,AAAI
2021,Thirty-ThirdConferenceonInnovativeApplicationsofArtificialIntelligence,IAAI2021,The
Eleventh Symposium onEducational Advances in Artificial Intelligence, EAAI 2021, Virtual Event,
February 2-9, 2021, pages 7961â€“7969. AAAI Press, 2021. doi:10.1609/AAAI.V35I9.16971. URL
https://doi.org/10.1609/aaai.v35i9.16971.
CharlesJin,MelindaSun,andMartinC.Rinard. Provableguaranteesagainstdatapoisoningusing
self-expansionandcompatibility. CoRR,abs/2105.03692,2021. URLhttps://arxiv.org/
abs/2105.03692.
AlaaKhaddaj,GuillaumeLeclerc,AleksandarMakelov,KristianGeorgiev,HadiSalman,Andrew
Ilyas,andAleksanderMadry. Rethinkingbackdoorattacks. InAndreasKrause,EmmaBrunskill,
KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett,editors,International
ConferenceonMachineLearning,ICML2023,23-29July2023,Honolulu,Hawaii,USA,volume202
54of Proceedings of Machine Learning Research, pages 16216â€“16236. PMLR, 2023. URL https:
//proceedings.mlr.press/v202/khaddaj23a.html.
Weihao Kong andGregory Valiant. Estimatinglearnability inthe sublineardata regime. In Samy
Bengio,HannaM.Wallach,HugoLarochelle,KristenGrauman,NicolÃ²Cesa-Bianchi,andRoman
Garnett, editors, Advances inNeural InformationProcessingSystems 31: AnnualConference on
NeuralInformationProcessingSystems2018,NeurIPS2018,December3-8,2018,MontrÃ©al,Canada,
pages5460â€“5469,2018. URLhttps://proceedings.neurips.cc/paper/2018/
hash/8bd39eae38511daad6152e84545e504d-Abstract.html.
Pierre Laforgue, Guillaume Staerman, and StÃ©phan ClÃ©menÃ§on. Generalization bounds in the
presence of outliers: a median-of-means study. In Marina Meila and Tong Zhang, editors,
Proceedingsofthe38thInternationalConferenceonMachineLearning,ICML2021,18-24July2021,
VirtualEvent,volume139ofProceedingsofMachineLearningResearch,pages5937â€“5947.PMLR,
2021. URLhttp://proceedings.mlr.press/v139/laforgue21a.html.
MathiasLÃ©cuyer,VaggelisAtlidakis,RoxanaGeambasu,DanielHsu,andSumanJana. Certified
robustnesstoadversarialexampleswithdifferentialprivacy. In2019IEEESymposiumonSecurity
and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pages 656â€“672. IEEE, 2019.
doi:10.1109/SP.2019.00044. URLhttps://doi.org/10.1109/SP.2019.00044.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial ro-
bustness with additive noise. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence dâ€™AlchÃ©-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 9459â€“
9469, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html.
Nathan Linial, Yishay Mansour, and Noam Nisan. Constant depth circuits, fourier transform,
and learnability. J. ACM, 40(3):607â€“620, 1993. doi:10.1145/174130.174138. URL https:
//doi.org/10.1145/174130.174138.
XuanqingLiu,MinhaoCheng, HuanZhang,andCho-JuiHsieh. Towardsrobustneuralnetworks
via randomself-ensemble. In VittorioFerrari, Martial Hebert,Cristian Sminchisescu, andYair
Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany,
September8-14,2018,Proceedings,PartVII,volume11211ofLectureNotesinComputerScience,
pages381â€“397.Springer,2018. doi:10.1007/978-3-030-01234-2_23. URLhttps://doi.org/
10.1007/978-3-030-01234-2_23.
YishayMansour. LearningBooleanfunctionsviatheFouriertransform. InTheoreticaladvancesin
neural computation andlearning, pages391â€“424. Springer, 1994. doi:10.1007/978-1-4615-2696-
4_11.
Shay Moran, Hilla Schefler, and Jonathan Shafer. The bayesian stability zoo. In Alice Oh,
Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, edi-
tors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
552023,2023.URLhttp://papers.nips.cc/paper_files/paper/2023/hash/
c2586b71fd150fb56952e253a9c551cc-Abstract-Conference.html.
Arkadij SemenoviÄ Nemirovskij and David Borisovich Yudin. Problem complexity and method
efficiencyinoptimization. Wiley-Interscience,1983.
Gleb Novikov, David Steurer, and Stefan Tiegel. Robust mean estimation without mo-
ments for symmetric distributions. In Alice Oh, Tristan Naumann, Amir Glober-
son, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural In-
formation Processing Systems 36: Annual Conference on Neural Information Process-
ing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023,
2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/
6c59ace4fc4872a14df13d91762ad4f0-Abstract-Conference.html.
RyanOâ€™Donnell. Analysis ofBoolean Functions. CambridgeUniversityPress, 2014. ISBN978-1-10-
703832-5. doi:10.1017/CBO9781139814782.
RonittRubinfeldandArsenVasilyan. Testingdistributionalassumptionsoflearningalgorithms.
In Barna Saha and Rocco A. Servedio, editors, Proceedings of the 55th Annual ACM Sympo-
sium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023, pages 1643â€“
1656. ACM, 2023. doi:10.1145/3564246.3585117. URL https://doi.org/10.1145/
3564246.3585117.
Ronitt A Rubinfeld. A mathematical theory of self-checking, self-testing and self-correcting pro-
grams. University of California, Berkeley, 1990. URL https://www.proquest.com/
docview/303810074.
Benjamin Schneider, Nils Lukas, and Florian Kerschbaum. Universal backdoor attacks. In
The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
May7-11,2024.OpenReview.net,2024. URLhttps://openreview.net/forum?id=
3QkzYBSWqL.
BrandonTran,JerryLi,andAleksanderMadry. Spectralsignaturesinbackdoorattacks. InSamy
Bengio,HannaM.Wallach,HugoLarochelle,KristenGrauman,NicolÃ²Cesa-Bianchi,andRoman
Garnett, editors, Advances inNeural InformationProcessingSystems 31: AnnualConference on
NeuralInformationProcessingSystems2018,NeurIPS2018,December3-8,2018,MontrÃ©al,Canada,
pages8011â€“8021,2018. URLhttps://proceedings.neurips.cc/paper/2018/
hash/280cf18baf4311c92aa5a042336587d3-Abstract.html.
Bo Xue, Yimu Wang, Yuanyu Wan, Jinfeng Yi, and Lijun Zhang. Efficient algorithms
for generalized linear bandits with heavy-tailed rewards. In Alice Oh, Tristan Nau-
mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023,2023.URLhttp://papers.nips.cc/paper_files/paper/2023/hash/
e0982cbc81401df3430ee1ff780dc7a2-Abstract-Conference.html.
Han Zhong, Jiayi Huang, Lin Yang, and Liwei Wang. Breaking the moments condition bar-
rier: No-regret algorithm for bandits with super heavy-tailed payoffs. In Marcâ€™Aurelio Ran-
56zato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan,
editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural
InformationProcessing Systems2021, NeurIPS 2021, December 6-14, 2021,virtual, pages15710â€“
15720, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/
843a4d7fb5b1641b0bb8e3c2b2e75231-Abstract.html.
MingliZhu,SiyuanLiang,andBaoyuanWu. Breakingthefalsesenseofsecurityinbackdoorde-
fensethroughre-activationattack. CoRR,abs/2405.16134,2024. doi:10.48550/ARXIV.2405.16134.
URLhttps://doi.org/10.48550/arXiv.2405.16134.
A Goldreichâ€“Levin Theorem
TheoremA.1(GoldreichandLevin,1989;Section3.5inOâ€™Donnell,2014). Thereexistanalgorithm
ğ´andapolynomialğ‘asfollows. Foranyğ‘› âˆˆ â„•,anyğœ,ğ›¿ âˆˆ (0,1],andanyfunctionğ‘“ âˆ¶ {Â±1}ğ‘› â†’ {Â±1},
ifğ´isexecutedwithoracleaccesstoğ‘“ thenğ´terminatesintimeğ‘(ğ‘›,1/ğœ,log(1/ğ›¿))andoutputsa
listğ¿ âŠ† 2[ğ‘›] suchthatwithprobabilityatleast1âˆ’ğ›¿,forallğ‘† âŠ† [ğ‘›]:
| |
Ì‚
1. |ğ‘“(ğ‘†)| â‰¥ ğœ âŸ¹ ğ‘† âˆˆ ğ¿,and
| |
| |
Ì‚
2. ğ‘† âˆˆ ğ¿ âŸ¹ |ğ‘“(ğ‘†)| â‰¥ 3ğœ/4.
| |
Notethatthetheoremistypicallystatedwithaconstantofğœ/2inItem2,whilewehavechosena
strongerstatementwithaconstantof3ğœ/4. Thetheoremholdsforanyconstantfractionofğœ.
B Miscellaneous Fourier Analysis Results
Weusethenormnotationâ€–â‹…â€–definedinSection6.1.
FactB.1. Letîˆ° âˆˆ Î”(î‰„ Ã—{Â±1}),andletğ‘“ âˆ¶ î‰„ â†’ {Â±1}. Then,
1 1
ğ¿ îˆ°(ğ‘“) = â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[ğ‘“(ğ‘¥) â‰  ğ‘¦] = â‹…ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘“(ğ‘¥)âˆ’ğ‘¦)2 ] = â€–ğ‘“ âˆ’ğ‘¦â€–2.
4 4
Claim B.2. Let ğœ€ âˆˆ [0,1], let î‰„ = {Â±1}ğ‘›, let îˆ° âˆˆ Î”(î‰„ Ã—{Â±1}) have uniform marginal on î‰„, let
â„ âˆ¶ î‰„ â†’ {Â±1}suchthatğ¿ îˆ°(â„) â‰¤ ğœ€,andletğ‘Ÿ âˆ¶ î‰„ â†’ â„suchthatğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ] â‰¤ 4ğœ€. Then
2
âˆ‘ Ì‚ğ‘Ÿ(ğ‘†)âˆ’Ì‚ â„(ğ‘†) â‰¤ 16ğœ€.
( )
ğ‘†âŠ†[ğ‘‘]
ProofofClaimB.2.
â€–ğ‘Ÿ(ğ‘¥)âˆ’â„(ğ‘¥)â€– â‰¤ â€–ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦â€–+â€–â„(ğ‘¥)âˆ’ğ‘¦â€–
= (ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ])1/2 +(ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(â„(ğ‘¥)âˆ’ğ‘¦)2 ])1/2
= (ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ])1/2 +2(â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[â„(ğ‘¥) â‰  ğ‘¦])1/2 (ByFactB.1)
âˆš
â‰¤ 4 ğœ€. (Choiceofğ‘Ÿ andâ„)
57Hence,
16ğœ€ â‰¥ â€–ğ‘Ÿ(ğ‘¥)âˆ’â„(ğ‘¥)â€–2
2
= âˆ‘ Ì‚ğ‘Ÿ(ğ‘†)âˆ’Ì‚ â„(ğ‘†) . (Parsevalâ€™sidentity)
( )
ğ‘†âŠ†[ğ‘‘]
Claim B.3 (Section 4 in Mansour, 1994). Let ğ‘‘ âˆˆ â„•, let î‰„ be a set, let îˆ° âˆˆ Î”(î‰„ Ã—{Â±1}), and let
ğ‘Ÿ âˆ¶ î‰„ â†’ â„. Then
â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦] â‰¤ ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ].
ProofofClaimB.3. Forafixedğ‘¥ âˆˆ î‰„ andğ‘¦ âˆˆ {Â±1},considertwocases.
â€¢ CaseI:sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦. Then1(sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦) = 1 â‰¤ |ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦| â‰¤ (ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2.
â€¢ CaseII:sign(ğ‘Ÿ(ğ‘¥)) = ğ‘¦. Then1(sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦) = 0 â‰¤ (ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2.
Hence,foranyğ‘¥ âˆˆ î‰„ andğ‘¦ âˆˆ {Â±1},
1(sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦) â‰¤ (ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2.
Therefore,
â„™ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦] = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[1(sign(ğ‘Ÿ(ğ‘¥)) â‰  ğ‘¦)] â‰¤ ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¦)2 ].
ClaimB.4. Forallsetsî‰„,alldistributionsîˆ° âˆˆ Î”(î‰„ Ã—{Â±1}),andallfunctionsğ‘“ âˆ¶ î‰„ â†’ â„,
1
ğ¿2 (sign(ğ‘“)) = ğ¿0-1(sign(ğ‘“)) â‰¤ ğ¿2 (ğ‘“).
îˆ° îˆ° îˆ°
4
Proof. The(left-hand)equalitydirectlyholds,as
1 1
ğ¿2 îˆ°(sign(ğ‘“)) = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[(sign(ğ‘“(ğ‘¥))âˆ’ğ‘¦)2 ] = ğ”¼ (ğ‘¥,ğ‘¦)âˆ¼îˆ°[1(sign(ğ‘“(ğ‘¥)) â‰  ğ‘¦)]
4 4
= â„™ [sign(ğ‘“(ğ‘¥)) â‰  ğ‘¦]
(ğ‘¥,ğ‘¦)âˆ¼îˆ°
=
ğ¿0-1(sign(ğ‘“)),
îˆ°
wherewehaveusedthefactthat(sign(ğ‘“(ğ‘¥))âˆ’ğ‘¦)2 âˆˆ {0,4}. The(right-hand)inequalityisgiven
inClaimB.3.
C Subgaussian distributions
Definition C.1. Let ğœ â‰¥ 0 and let ğ‘‹ be a real-valued random variable. ğ‘‹ is subgaussian with
varianceproxyğœ2,denotedğ‘‹ âˆˆ SubG(ğœ2 ),ifğ”¼[ğ‘‹] = 0and
âˆ€ğ‘¡ âˆˆ â„ âˆ¶ ğ”¼[ğ‘’ğ‘¡ğ‘‹ ] â‰¤
ğ‘’ğœ2 2ğ‘¡2
.
58Claim C.2 (Concentration for Subgaussian Random Variables). Let ğœ â‰¥ 0, and ğ‘‹ âˆˆ SubG(ğœ2 ).
Thenforanyğ‘¡ â‰¥ 0,
ğ‘¡2
â„™[|ğ‘‹| â‰¥ ğ‘¡] â‰¤ 2exp âˆ’ .
( 2ğœ2)
Moreover, for ğ‘› âˆˆ â„•, independent variables ğ‘‹ 1,â€¦,ğ‘‹
ğ‘›
âˆˆ SubG(ğœ2 ), and for any ğ‘ 1,â€¦,ğ‘
ğ‘›
âˆˆ â„ and
ğ‘¡ â‰¥ 0,
| |
| | ğ‘¡2
â„™ |âˆ‘ğ‘ğ‘‹| â‰¥ ğ‘¡ â‰¤ 2exp âˆ’ .
[| ğ‘– ğ‘–| ] ( 2ğœ2âˆ‘ ğ‘2)
|ğ‘–âˆˆ[ğ‘›] | ğ‘–âˆˆğ‘› ğ‘–
Claim C.3 (Sum of Subgaussian Random Variables is Subgaussian). Let ğœ â‰¥ 0, ğ‘› âˆˆ â„•, and let
ğ‘‹ 1,â€¦,ğ‘‹
ğ‘›
âˆˆ SubG(ğœ2 )forallğ‘– âˆˆ [ğ‘›]. Thenğ‘ = âˆ‘ ğ‘–âˆˆ[ğ‘›]ğ‘‹
ğ‘–
âˆˆ SubG(ğ‘›2ğœ2 ).
Notethattheğ‘‹â€™sintheclaimarenotnecessarilyindependent.
ğ‘–
ProofofClaimC.3. Foranyğ‘¡ âˆˆ â„,
ğ”¼[ğ‘’ğ‘¡ğ‘ ] = ğ”¼[ğ‘’ğ‘¡(ğ‘‹1+â‹¯+ğ‘‹ğ‘›) ]
= ğ”¼[ğ‘’ğ‘›1âˆ‘ ğ‘–âˆˆ[ğ‘›]ğ‘›ğ‘¡ğ‘‹ğ‘–]
1
â‰¤ âˆ‘ğ”¼[ğ‘’ğ‘›ğ‘¡ğ‘‹ğ‘–] (Jensenâ€™sinequality)
ğ‘›
ğ‘–âˆˆ[ğ‘›]
â‰¤ 1 âˆ‘ğ‘’ğœ2â‹…( 2ğ‘›ğ‘¡)2 = ğ‘’ğ‘›2ğœ 22â‹…ğ‘¡2 , (ğ‘‹
ğ‘–
âˆˆ SubG(ğœ2 ))
ğ‘›
ğ‘–âˆˆ[ğ‘›]
asdesired.
ClaimC.4(ProductofSubgaussianandBoundedRandomVariablesisSubgaussian). Letğœ,ğ‘ â‰¥ 0,
andletğ‘‹,ğ‘Œ âˆˆ â„berandomvariablessuchthatğ‘‹ âˆˆ SubG(ğœ2 ),andâ„™[|ğ‘Œ| â‰¤ ğ‘] = 1. Thenğ‘ = ğ‘‹ğ‘Œ âˆˆ
SubG(ğ‘2ğœ2 ).
Notethatğ‘‹ andğ‘Œ neednotbeindependent.
ProofofClaimC.4. Foranyğ‘¡ âˆˆ â„,
ğ”¼[ğ‘’ğ‘¡ğ‘ ] = ğ”¼[ğ‘’ğ‘¡ğ‘‹ğ‘Œ ]
= ğ”¼ ğ‘¦âˆ¼ğ‘Œ[ğ”¼ ğ‘‹[ğ‘’(ğ‘¡ğ‘¦)â‹…ğ‘‹ ]]
â‰¤ ğ”¼
ğ‘¦âˆ¼ğ‘Œ[ğ‘’ğœ2â‹…( 2ğ‘¡ğ‘¦)2
]
(ğ‘‹ âˆˆ SubG(ğœ2 ))
â‰¤ ğ‘’ğ‘2ğœ 22ğ‘¡2 , (|ğ‘Œ| â‰¤ ğ‘)
asdesired.
59