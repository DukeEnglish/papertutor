Multi-Agent Diagnostics for Robustness via Illuminated Diversity
MikayelSamvelyanâˆ— DavidePaglieriâˆ— MinqiJiang
UCL,MetaAI UCL UCL,MetaAI
samvelyan@meta.com d.paglieri@cs.ucl.ac.uk minqi.jiang@cs.ucl.ac.uk
JackParker-Holder TimRocktÃ¤schel
UCL UCL
j.parker-holder@ucl.ac.uk tim.rocktaschel@ucl.ac.uk
ABSTRACT
Selection Perturbation
Intherapidlyadvancingfieldofmulti-agentsystems,ensuringro-
bustnessinunfamiliarandadversarialsettingsiscrucial.Notwith-
standingtheiroutstandingperformanceinfamiliarenvironments,
thesesystemsoftenfalterinnewsituationsduetooverfittingdur-
ingthetrainingphase.Thisisespeciallypronouncedinsettings Evaluation
wherebothcooperativeandcompetitivebehavioursarepresent,
encapsulatingadualnatureofoverfittingandgeneralisationchal-
lenges.Toaddressthisissue,wepresentMulti-AgentDiagnosticsfor
RobustnessviaIlluminatedDiversity(MADRID),anovelapproach
forgeneratingdiverseadversarialscenariosthatexposestrategic Addition
vulnerabilitiesinpre-trainedmulti-agentpolicies.Leveragingthe
Figure 1: Overview of MADRID. Operating on a discre-
conceptsfromopen-endedlearning,MADRIDnavigatesthevast
tisedgridwithanaddeddimensionforreferencepolicies,
spaceofadversarialsettings,employingatargetpolicyâ€™sregretto
MADRIDarchivesenvironmentvariations(orlevels)charac-
gaugethevulnerabilitiesofthesesettings.Weevaluatetheeffective-
nessofMADRIDonthe11vs11versionofGoogleResearchFootball,
terizedbyrepresentativefeatures,e.g.,(ğ‘¥,ğ‘¦)coordinatesof
theballpositioninfootball.Duringeachiteration,MADRID
oneofthemostcomplexenvironmentsformulti-agentreinforce-
mutatesaselectedlevel,computesregretusingitsassociated
mentlearning.Specifically,weemployMADRIDforgeneratinga
referencepolicy,andreincorporateslevelswithhigherregret
diversearrayofadversarialsettingsforTiZero,thestate-of-the-art
intothearchive,effectivelygeneratingdiversecollectionof
approachwhich"masters"thegamethrough45daysoftrainingon
adversariallevels.
alarge-scaledistributedinfrastructure.Weexposekeyshortcom-
ingsinTiZeroâ€™stacticaldecision-making,underliningthecrucial
importanceofrigorousevaluationinmulti-agentsystems.1
havebeensignificantsuccessesinsimulatedenvironments,asevi-
dencedbydeepreinforcementlearning(RL)incomplexmulti-agent
KEYWORDS
games[4,41,44,50,55],thetransferfromsimulationtoreality
Multi-AgentLearning,Open-Endedness,Generalisation (sim2real)continuestoposechallenges[19,57].Specifically,while
ACMReferenceFormat: thesemodelsdemonstrateproficiencyinknownenvironments,they
becomehighlysusceptibletofaultybehaviorsinunfamiliarset-
MikayelSamvelyan,DavidePaglieri,MinqiJiang,JackParker-Holder,and
TimRocktÃ¤schel.2024.Multi-AgentDiagnosticsforRobustnessviaIllumi- tingsandadversarialsituations[39].Giventheircriticalrolesin
natedDiversity.InProc.ofthe23rdInternationalConferenceonAutonomous human-centricapplications,understandingandmitigatingthese
AgentsandMultiagentSystems(AAMAS2024),Auckland,NewZealand,May susceptibilitiesbecomesparamountforfosteringmoreeffective
6â€“10,2024,IFAAMAS,15pages. andreliabledeploymentofmulti-agentAIsystemsinthefuture.
TheAchillesâ€™heelofthesemulti-agentsystems,contributing
1 INTRODUCTION
totheirlackofrobustness,isoftentheiroverfittingtothespecific
Inrecenttimes,multi-agentsystems,particularlythosedesigned settingsencounteredduringtraining[26].Thisoverfittingbecomes
tointeractwithhumans,haveemergedasaprimarymodelforAI notablyevidentintwo-teamzero-sumsettingswherebothcoopera-
deploymentinreal-worldscenarios[1,2,33,48].Althoughthere tiveandcompetitivedynamicsintertwine.Aprimarymanifestation
oftheoverfittingbetweencooperativeagents,especiallywhenall
âˆ—Equalcontribution
agentsinthegroupsharethesamesetofnetworkparameters(i.e.,
1Visualsandcodeareavailableathttps://sites.google.com/view/madrid-marl
parametersharing[14]),isintheagentsbecomingtooaccustomed
totheirtrainingenvironments,leadingtoadetailedcoordination
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. tailoredtothesespecificconditions.Asaconsequence,whenin-
troducedtounfamiliarsettings,theirperformancetendstofalter.
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems Concurrently,thereisalsoanoverfittingtospecificopponentteams
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6â€“10,2024,
theyhavetrainedagainst.Insteadofdevelopingaflexiblestrategy
Auckland,NewZealand.Â©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). thatcanwithstandavarietyofopponents,theirstrategiesmightbe
4202
naJ
42
]GL.sc[
1v06431.1042:viXraoverlyoptimisedtocounteractthestrategiesoffamiliaradversaries. comprehendtheoffsideruleeffectively,andevenencounterssitua-
Thesedualformsofoverfittingâ€”bothtotheenvironmentandto tionsofscoringaccidentalowngoals.Thesefindingshighlightthe
opponentsâ€”rendersuchsettingsasperfectplatformstoprobefor latentvulnerabilitieswithinevenhighlytrainedmodelsanddemon-
vulnerabilities[49].Furthermore,itiscrucialtopinpointadiverse stratethatthereismuchroomforimprovingthetheirrobustness.
setofadversarialscenariosforaholisticdiagnosticofrobustness, Ouranalysisshowcasesthevalueofidentifyingsuchadversarial
sheddinglightonpossibleshortcomingsfromvariousperspectives. settingsinofferingnewinsightsintothehiddenweaknessesof
Giventhesechallenges,weintroduceMulti-AgentDiagnosticsfor pretrainedpoliciesthatmayotherwiseappearundefeatable.
RobustnessviaIlluminatedDiversity(MADRID),anovelmethodfor
2 BACKGROUND
systematicallygeneratingadiversecollectionofadversarialsettings
wherepre-trainedmulti-agentpoliciesmakestrategicmistakes. UnderspecifiedStochasticGames. Inthiswork,weconsider
Tothisend,MADRIDemploysapproachesfromquality-diversity UnderspecifiedStochasticGames(USG),i.e.,stochasticgames[43]
(QD)[8,27],afamilyofevolutionaryalgorithmthataimtogenerate withunderspecifiedparametersoftheenvironment.AnUSGgame
alargecollectionofhigh-performingsolutionseachwiththeirown forğ‘ agentenvironmentisdefinedbyasetofstatesS,actions
uniquecharacteristics. A 1,...,Ağ‘ and a set of observations O 1,...,Oğ‘ for each of the
MADRIDincorporatesMAP-Elites[32],asimpleandeffective agents.Eachagentğ‘– selectactionsusingastochasticpolicyğœ‹ ğ‘– :
QDapproach,tosystematicallyexplorethevastspaceofadversar- Oğ‘–Ã—Ağ‘– â†¦â†’ [0,1].Î˜definesthefreeparametersoftheenvironments
ialsettings.Bydiscretisingthesearchspace,MADRIDiteratively whichareincorporatedintothetransitionfunctionT :SÃ—Î˜Ã—
performsselection,mutation,andevaluationssteps,endlesslyre- A 1Ã—...Ã—Ağ‘ â†¦â†’Swhichproducesthenextstatebasedonthe
finingandexpandingtherepertoireofhigh-performingadversarial actionsofallagents.Eachagentğ‘–receivesobservationsoğ‘– :Sâ†¦â†’Oğ‘–
scenarios within its archive (see Figure 1). A crucial feature of correlatedwiththecurrentstateandrewardğ‘Ÿ ğ‘– :SÃ—Ağ‘– â†¦â†’Rasa
MADRIDisitsemploymentofthetargetpolicyâ€™sregretâ€”thegap functionofthestateandagentâ€™saction.Thegoalofeachagentğ‘–is
inperformancebetweentheoptimalandtargetpolicyâ€”toquantify tomaximiseitsowntotalexpectedreturnğ‘… ğ‘– =(cid:205)ğ‘‡ ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ ğ‘–ğ‘¡ forthe
thequalityofadversarialsettings.Regretisshowntobeaneffective timehorizonğ‘‡,whereğ›¾ isadiscountfactor.
metricforidentifyingsituationswhereRLagentsunderperform Eachconfigurationofthefreeparameterğœƒ âˆˆÎ˜,whichisoften
inbothsingle-agent[11,22,31,35]andmulti-agent[39]domains. calledalevel[22,35],definesaspecificinstantiationoftheenviron-
MADRIDestimatesalower-boundonthetrueregretbyutilisinga mentMğœƒ.Forexample,thiscancorrespondtodifferentpositions
collectionofreferencepolicies[17,50],whicharenotnecessarilyre- ofthewallsinamaze,orlocationsofplayersandtheballina
quiredtobehigh-performing.MADRIDidentifiessituationswhere footballgame.USGisamulti-agentvariationofUnderspecified
thesereferencepoliciessurpassthetargetone,therebyprovidinga POMDPs[11]andfullyobservablevariantofUPOSGs[39].
clearillustrationofsuperiorperformanceingivensituations.
Quality-Diversity. Quality-diversity(QD)isafamilyofmeth-
ToevaluateMADRID,weconcentratespecificallyononeofthe
odsusedtofindadiversecollectionofsolutionsthatareperformant
most challenging multi-agent domains, namely the fully decen-
andspanameaningfulspectrumofsolutioncharacteristics[8,27].
tralised11vs11variationofGoogleResearchFootball[GRF,25].
Theperformanceofsolutionğ‘¥ âˆˆ X ismeasureusingthefitness
Thissimulatedenvironmentisbasedonthepopularreal-world
:Xâ†¦â†’Rfunction.Thediversityofsolutionsistypicallymeasured
sportoffootball(a.k.a.soccer)andrequirestwoteamsofagents
usingthefeature_descriptor:Xâ†¦â†’Bfunctionthatmapsasolution
tocombineshort-termcontroltechniqueswithcoordinated,long-
termglobalstrategies.GRFrepresentsauniquecombinationof
intothefeaturespaceB=Rğ¾
thatdescribesspecificcharacteristics
characteristicsnotpresentinotherRLenvironments[29],namely ofthesolution,suchasbehavioralpropertiesorvisualappearance.
multi-agentcooperation(withineachteam),competition(between
thetwoteams),sparserewards,largeactionandobservationspaces, Algorithm1:MAP-Elites[32]
andstochasticdynamics.Whilemanyoftheindividualchallenges Initialise:ğ‘-dimensionalgridsforsolutionsğ‘‹ and
inGRF,includingmulti-agentcoordination[36,56],long-termplan- performancesP
ning[12]andnon-transitivity[3,9],havebeenstudiedextensively Initialise:ğ‘›cellsofğ‘‹ withrandomsolutionsand
in isolation, learning highly-competitive GRF policies has long correspondingcellsofğ‘ƒ withtheirfitness
remainedoutsidethereachofRLmethods.TiZero[29],arecent forğ‘– ={1,2,...}do
multi-agentRLapproach,learnedto"master"thefullydecentralised Samplesolutionğ‘¥ fromğ‘‹
variationofGRFfromscratchforthefirsttime,usingahand-crafted Getsolutionğ‘¥â€²fromğ‘¥ viarandommutation
curriculum,rewardshaping,andself-play.Experimentally,TiZero ğ‘â€² â†ğ‘“ğ‘–ğ‘¡ğ‘›ğ‘’ğ‘ ğ‘ (ğ‘¥â€²)
hasshownimpressiveresultsandoutperformedpreviousmethods ğ‘â€² â†ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’_ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘œğ‘Ÿ(ğ‘¥â€²)
byalargemarginafteranexpensivetraininglasting45daysona ifP(ğ‘â€²)=âˆ…ğ‘œğ‘Ÿ P(ğ‘â€²) <ğ‘â€²then
large-scaledistributedtraininginfrastructure. P(ğ‘â€²)â†ğ‘â€²
WeapplyMADRIDonGRFbytargetingTiZerotodiagnosea ğ‘‹(ğ‘â€²)â†ğ‘â€²
broadsetofscenariosinwhichitcommitstacticalmistakes.Ourex-
tensiveevaluationsrevealdiversesettingswhereTiZeroexhibitsa
poorperformance,whereweakerpoliciescanoutperformit.Specif- MAP-Elites. MAP-ElitesisasimpleandeffectiveQDmethod
ically,MADRIDdiscoversinstanceswhereTiZeroisineffective [32].Here,thedescriptorspaceBisdiscretisedandrepresentedas
atneartheopponentâ€™sgoal,demonstratesamarkedinabilityto aninitiallyemptyğ‘ <ğ¾dimensionalgrid(archive).Thealgorithmstartsbygeneratinganarbitrarycollectionofcandidatesolutions. Algorithm2:MADRID
In each iteration, a solution is randomly selected among those Input:Targetpolicyğœ‹
ğ‘‡
inthegrid.Anewsolutionisobtainedbymutatingtheselected Input:AcollectionofreferencepoliciesÎ 
ğ‘…
solution,whichisthenevaluatedandmappedtoacellofthegrid Input:ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™_ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘œğ‘Ÿ :Î˜â†¦â†’Rğ‘ function
basedonitsfeaturedescriptor.Thesolutionisthenplacedinthe #Initialiseadiscretisedgrid,withanaddeddimensionforÎ  ğ‘…,to
correspondingcellofthegridifithasahigherfitnessthanthe
archivelevelsandregretscores.
currentoccupant,orifthecellifitisempty.Thiscycleofselection, Initialise:ğ‘ +1-dimensionalgridsforlevelsğ‘‹ andregret
mutation,andevaluationisrepeated,progressivelyenhancingboth estimatesP
thediversity(coverage)andthequality(fitness)ofthecollection. Initialise:ğ‘›cellsofğ‘‹ withrandomlygeneratedlevelsand
Thepseudo-codeofMAP-ElitesispresentedinAlgorithm1. correspondingestimatedregretinP
forğ‘– ={1,2,...}do
3 MADRID
#Samplealevelğœƒ andcorrespondingreferencepolicyğœ‹
ğ‘…
fromğ‘‹.
Inthissection,wedescribeMulti-AgentDiagnosticsforRobustness ğœƒ,ğœ‹ ğ‘… âˆ¼ğ‘‹
viaIlluminatedDiversity(MADRID),anovelmethodforautomati- #PerformlevelmutationbyaddingGaussiannoise.
callygeneratingdiverseadversarialsettingsforatargetpre-trained ğœƒâ€² â†ğœƒ+N(0,ğœ2)
policyğœ‹ ğ‘‡.Thesearesettingsthateitherdeceivethepolicy,forcing #Estimatetheregretofğœ‹ ğ‘‡ onğœƒâ€²usingğœ‹ ğ‘….
ittoproduceincorrectbehaviour,orwherethepolicyinherently (cid:101)ğ‘Ÿâ€² â†ğ‘‰ğœƒâ€²(ğœ‹ ğ‘…,ğœ‹ ğ‘‡)âˆ’ğ‘‰ğœƒâ€²(ğœ‹ ğ‘‡,ğœ‹ ğ‘‡)
performspoorly,deviatingfromtheoptimalbehaviour.ForUSGs, ğ‘â€² â†ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™_ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘œğ‘Ÿ(ğœƒâ€²)
thesesettingscorrespondtoparticularenvironmentlevelsğœƒ âˆˆÎ˜ ifP(ğ‘â€²,ğœ‹ ğ‘…)=âˆ…ğ‘œğ‘Ÿ P(ğ‘â€²,ğœ‹ ğ‘…) < (cid:101)ğ‘Ÿâ€²then
thathavebeenprocedurallygenerated. P(ğ‘â€²,ğœ‹ ğ‘…)â† (cid:101)ğ‘Ÿâ€²
Forquantifyingadversariallevels,wemakeusethetargetpol-
ğ‘‹(ğ‘â€²,ğœ‹ ğ‘…)â†ğ‘â€²
icyâ€™s regret in levelğœƒ, i.e., the difference in utility between the
optimalpolicyğœ‹âˆ—andğœ‹ ğ‘‡ :
Regretğœƒ (ğœ‹âˆ—,ğœ‹ ğ‘‡)=ğ‘‰ğœƒ (ğœ‹âˆ—,ğœ‹ ğ‘‡)âˆ’ğ‘‰ğœƒ (ğœ‹ ğ‘‡,ğœ‹ ğ‘‡), foundinMADRIDgiventhateachcelldefinesspecificenvironment
parameters,alongsideareferencepolicywhichoutperformsthe
whereğ‘‰ ğœƒ(ğœ‹ ğ´,ğœ‹ ğµ) = E[(cid:205)ğ‘‡ ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ ğ‘¡ğ´] is the value of a policy ğœ‹ ğ´ targetundertheseparameters.
againstpolicyğœ‹ ğµ inğœƒ.2 MADRIDstartsbypopulatingthegridwithinitiallevelsforeach
Regretisasuitablemetricforevaluatingadversarialexamplesin referencepolicy.Duringtheiterativeprocess,levelsareselected
pre-trainedmodels.Itprovidesameasurethatdirectlyquantifies fromthegridtoundergomutation,followedbyregretestimation.
thesuboptimalityofamodelâ€™sdecisions.Whileahighregretvalue Eachmutatedlevelisthenmappedtoaspecificcellinthegridbased
servesasaglaringindicatorofhowfaroffamodelâ€™sbehavioris onitsfeaturesandreplacestheexistingoccupantifthemutated
fromtheoptimalchoice,alowregretindicatesthemodelâ€™sdeci- levelhashigherregretorthecorrespondingcellisunoccupied.
sionsarecloselyalignedwiththeoptimalchoice.Theimportance Thisprocedureensuresathoroughexplorationandexploitation
ofregretbecomesevenmorepronouncedwhenconsideringthe oftheenvironmentdesignspace,allowingMADRIDtogenerate
variedscenariosinwhichamodelmightbedeployed.Therefore, levelsthatarebothdiverseandhigh-regret.Figure1illustratesthis
byinvestigatingregretacrossdiversesituations,wecannotonly process.Algorithm2providesthepseudocodeofthemethod.
pinpoint specific vulnerabilities of a model but also ensure the
4 EXPERIMENTALSETTING
robustnessinpreviouslyunseenscenarios.
Sincetheoptimalpolicyisusuallyunavailable,MADRIDrelies Ourexperimentsseekto(1)showcasetheeffectivenessofMADRID
onutilisingacollectionofsuboptimal policiesÎ  ğ‘… = (cid:208) ğ‘–ğ‘€ =1ğœ‹ ğ‘– for ingeneratingdiverseadversarialsettingsforatargetstate-of-the-
estimatingthelowerboundontrueregret.Specifically,thegoalis artpre-trainedRLmodel,(2)analysetheadversarialsettingsgener-
tofindadversariallevelsthatmaximisethegapinutilityacquired atedbyMADRIDtofindkeyweaknessesofthetargetmodel,(3)
throughareferencepolicyğœ‹ ğ‘– âˆˆÎ  ğ‘… andtargetpolicyğœ‹ ğ‘‡.Utilising validatethedesignchoicesof MADRIDbycomparingittotwo
acollectionofdiversereferencepoliciescanbeadvantageousin ablatedbaselines.Tothisend,weevaluateMADRIDonGoogle
theabsenceofatrueoptimalpolicy,sinceeachofthesereference ResearchFootball[GRF,25].Givenitsstrongperformanceandus-
policiesmayexcelinauniquesetoflevels[39]. ageinrelatedworks,CovarianceMatrixAdaptationMAP-Elites
MADRIDcaststhetaskofgeneratingadiversearrayofadver- [CMA-ME,16]servesasthebaseMAP-Elitesmethodinourexper-
sarial levels for each reference policy as a QD search problem. iments.WeprovidefullenvironmentdescriptionsinAppendixB
Specifically,MADRIDusesMAP-Elitestosystematicallygenerate andimplementationdetailsinAppendixC.
levelsfromÎ˜bydiscretisingthefeaturespaceoflevelsintoan
ğ‘-dimensionalgrid,withanadditionaldimensionrepresentingthe Baselines. WecompareMADRIDagainsttwobaselines:The
correspondingreferencepolicyfromÎ  ğ‘‡.Usingadiscretisedgrid targetedbaselineusesaMAP-Elitesarchivebutrandomlysamples
ofMAP-Elitesprovidesinterpretabilitytotheadversarialexamples levels from scratch, rather then evolving previously discovered
high-regretlevelsfromthegrid.Consequently,itdoesnotleverage
2Notethathere,forthesimplicityofthenotation,weassumeatwo-teamzerosum thesteppingstonestotheoptimisationproblem[27].Therandom
setting.ğœ‹ğ‘‡ andğœ‹ğ‘…describethepoliciesforgroupsofagents,eitherthroughacen-
baselinesampleslevelsrandomlyfromscratchwithoutmaintaining
tralisedcontrollerordecentralisedpoliciesthatemployparametersharing.However,
MADRIDcanbeappliedformoregeneralmulti-agentsettings. anarchiveofhigh-regretlevels.Figure2:ExamplesofrandomlygeneratedlevelsonGoogleResearchFootball. Figure 3: Dividing the field in 160
gridsusingtheball(ğ‘¥,ğ‘¦)coordinates.
Environment
theballasthefirsttwoenvironmentfeaturesinMADRID.This
WeuseMADRIDtofindadversarialscenariosforTiZero,thestate- leadstoacategorisationoflevelsinto160uniformlyspacedcells
of-the-artmodelforGRF.TiZerowastrainedviaacomplexregime acrossthefootballfield,asillustratedinFigure3.Giventhatwe
onlarge-scaledistributedinfrastructure[29]over45days.Inpar- areinterestedinevaluatingTiZeroinspecificadversariallevels,in
ticular,weaimtogenerateadversariallevelswherebythedecen- ourexperimentswerestricttheepisodelengthto128stepstaking
tralisedagentsinTiZeromakeadiversearrayofstrategicerrors, placeinthebeginningofthegame.
ashighlightedbybetterbehavioursofthereferencepolicy. ThethirdaxisfortheMAP-Elitesarchiveindexesthereference
GRF is a complex open-source RL environment designed for policiesÎ  ğ‘….Inourexperiments,wemakeuseof48checkpoints
trainingandevaluatingagentstomastertheintricatedynamics ofTiZerosavedthroughoutitstraining[29],aswellasthreebuilt-
offootball,oneoftheworldâ€™smostcelebratedsports.Itoffersa inbotsinGRFwithvaryingdifficulties(easy,medium,hard).For
physics-based3DsimulationthattaskstheRLpolicywithcontrol- eachreferencepolicy,weinitialisethegridwithrandomlysampled
lingateamofplayerstopenetratetheopponentâ€™sdefense,while levelsthatassignrandomlocationstoplayersandtheball.Figure2
passingtheballamongteammates,inordertoscoregoals.GRF illustratessomeoftherandomlygeneratedlevels.
isatwo-teamzero-sumenvironmentthathaslongbeenconsid- AteachiterationofMADRID,wesamplealevelandreference
eredoneofthemostcomplexmulti-agentRLbenchmarksduetoa
policypair(ğœƒ,ğœ‹ ğ‘…).ThelevelisthenmutatedbyaddingGaussian
uniquecombinationofchallenges[20,29,53],suchasmulti-agent
noisetothe(ğ‘¥,ğ‘¦)positionsoftheplayersandtheballinthefield.
cooperation,multi-agentcompetition,sparserewards,largeaction ThefitnessofeachsolutionisestimatedbycomputingTiZeroâ€™s
andobservationspaces,andstochasticdynamics.3 regret,whichisthedifferenceinperformancebetweentheselected
Inthiswork,wefocusonthefullydecentralised11vs11version referencepolicyğœ‹ ğ‘… andTiZeroâ€™spolicyğœ‹ ğ‘‡.Inbothcases,weesti-
oftheenvironmentwhereeachofthe10RLagentsonbothsides matetheregretagainsttheTiZeropolicyonthelevelğœƒ as:
controlsanindividualplayeronthefield.4 Following[29],each
agentsreceivesasobservationa268-dimensionalfeaturevector ğ‘… (cid:158)ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘¡(ğœƒ,ğœ‹ ğ‘‡,ğœ‹ ğ‘…)=ğ‘‰ğœƒ (ğœ‹ ğ‘…,ğœ‹ ğ‘‡)âˆ’ğ‘‰ğœƒ (ğœ‹ ğ‘‡,ğœ‹ ğ‘‡), (1)
includeownplayerinformation,playerIDs,aswellasinformation
abouttheball,playeroftheownandopponentsteams,aswellas
which corresponds to the difference of cross-play and self-play
generalmatchdetails.Theactionspaceofagentsconsistsof19
valuesbetweenthereferenceandtargetpolicies.Theperformance
discreteactions,suchasmovingin8direction,sprinting,passing,
onagivenlevelğœƒ betweentwopoliciesğœ‹ ğ´ andğœ‹ ğµ isthereward
shooting,etc.
forscoringagoal:
ToapplyMADRIDonGRF,weutiliseprocedurallygenerated
levelseachrepresentedasavectorconsistingof(ğ‘¥,ğ‘¦)coordinates
o sef r2 v0 esp ala sy aer cs o5 na vn end ieth ne tdb ea sll c. rT iph te orp fo os rit li eo vn elo sf inth Ge Rb Fal bl eo cn auth see ifi te al cd
- ğ‘‰ğœƒ (ğœ‹ ğ´,ğœ‹
ğµ)=ï£±ï£´ï£´ï£´ï£² 01 ii ffğœ‹ nğ´ ogs oco ar le is
sscored (2)
c oo nm bm oto hd fiat ee ls dd hiv ale vr ese s.s Tce hn ea reri fo os r, er ,a wng ei un sg efr to hm eğ‘¥at at nac dk ğ‘¦in cg ot oo rdd ie nfe an ted sin og
f
ï£´ï£´ï£´ ï£³âˆ’1 ifğœ‹ ğµ scores
3HighlightingthestochasticityoftheGRFenvironment,ashotfromthetopofthe Uponscoringagoalbyeitherofthesides,thelevelterminates.
boxcanleadtovariousoutcomes,underscoringthatnoteveryactionresultsina Giventhenon-deterministicnatureofGRF,weaccountforvari-
predictableoutcome.
4ThegoalkeepersarecontrolledbythegameAI. abilitybycalculatingtheaverageregretacross4repetitionsofthe
5Thegoalkeeperspositionpositionsarealwaysneartheirowngoals. samepairoflevelğœƒ andreferencepolicyğœ‹ ğ‘….(a)Estimatedregretateachiteration. (b)FinalestimatedregretofTiZerooverreferencepoliciesusingMADRID.
(c)ScoringratevsTiZeroateachiteration. (d)FinalestimatedscoringratevsTiZerooverreferencepoliciesusingMADRID.
Figure4:TheestimatedregretandgoalscorerateagainstTiZeroinGoogleResearchFootball.Illustratedthroughouteach
iterationfor51referenceagents(a)and(c),aswellasfinalvaluesin(b)and(d).Standarderrorover3randomseedsisshown.
5 RESULTSANDDISCUSSION
ThisemphasisesMADRIDâ€™scapabilityinexposingadversariallevels
InouranalysisoftargetingTiZeroonGRF,wecloselyexamine whereevenstate-of-the-artpoliciesbepronetomissteps.
the performance of MADRID and baselines. Figure 4a displays
the average estimated regret values for all 160 cells within the
MAP-Elitesgridacrosstheentirecollectionofreferencepolicies.
Here,MADRIDoutperformsbothbaselinemethods.Therandom
baselineexhibitsanegativevaluecloseto0,asTiZeroprovestobe
astrongerpolicythanallthereferencepoliciesonentirelyrandom
gamelevels.Ontheotherhand,thetargetedbaselineperformswell,
closelyresemblingMADRIDâ€™sperformanceattheearlystagesof
iterations.However,astheiterationscontinue,itlagsbehinddue
toitsfailuretocapitaliseonpreviouslyidentifiedhigh-regretlevels
thatserveassteppingstonesfornextiterations.
InFigure4b,thevariationinestimatedfinalregretscoresacross
Figure5:MADRIDâ€™sestimatedregretoverdifferentreference
thereferencepoliciesisillustrated.Here,theregretincreasesas
policiesaftereachiterationonGRF(meanandstandarderror
wemovetohigher-rankedagents.Theheuristicbotsdisplayregret
over3seeds).
levelsthatareonparwiththeintermediatecheckpointsofTiZero.
Asweapproximatetheregretusingthedifferencebetweencross-
play(XP)andself-play(SP)betweenreferenceandTiZeropolicies Figure4candFigure4dillustratetheestimatedrateofgoals
(seeEquations1and2),aregretestimateof1foranadversarial scoredagainstTiZerobythereferencepoliciesonadversariallevels
levelğœƒ canbeachievedintwosituations.First,thereferencepolicy producedbyMADRIDandbaselines.Wecanseethatinapproxi-
scoresagainstTiZeroinXP,whileTiZerocannotscoreinitsSP. mately70%ofthetimeacrossallreferencepolicies,thereference
Second,TiZeroconcedesagoalinSPinğœƒ.Intriguingly,ourfindings policyscoredagoalagainstTiZeroinashortperiodoftime.6 It
revealthatforaround90%oftheadversariallevelsgeneratedby
MADRID,anominallyweakerreferencepolicyoutperformsTiZero. 6Thelevelslastonly128environmentsteps,whichisashortepisodecomparedtothe
3000stepsforthefullgame.(a)25iterations. (b)200iterations. (c)1000iterations. (d)5000iterations.
Figure6:TheestimatedregretinMADRIDâ€™sarchiveatvariousiterationswithrespecttoTiZero-048referencepolicy.
shouldbenotedthatwithintheremaining30%,themajorityofin- from passing the ball to offside players, resulting in successful
stancesresultedinnogoalsduetothenondeterministicdynamics scoringoutcomes.7
oftheenvironment.
UnforcedOwnGoals. Perhapsthemostglaringadversarialbe-
Figure5highlightsthedifferenceinperformanceforselected
haviourdiscoveredareinstanceswhereTiZeroagentsinexplicably
referencepolicies.Notably,thehigher-rankcheckpointsofTiZero,
shoottowardstheirowngoal,resultinginunforcedowngoals(See
savedatthelaterstagesofitstraining,canbeusedtoidentifymore
Figure8).Incontrast,whenstartingfromidenticalin-gameposi-
severevulnerabilities,asmeasuredusingtheregretestimate.
tions,thereferencepoliciesmanagetocounterattackeffectively,
Figure6showstheevolutionof MADRIDâ€™sarchiveforaspe-
oftenresultinginsuccessfulscoringendeavors.
cificreferencepolicy,illustratingitssearchprocessovertime.Ini-
tially,thegridissparselyfilledwithlow-regretlevels.However, Slowrunningopponents. TheTiZeroagentsalwayschooseto
asiterationsprogress,MADRIDgenerateshigh-regretlevelsthat sprintthroughouttheepisode.However,thismakesthemweakon
progressivelypopulatetheentiregrid.ThisshowsthatMADRID defenseagainstopponentswhomoveslowerwiththeball.Instead
candiscoverhigh-regretlevelsanywhereonthefootballfield.On oftryingtotackleandtaketheball,TiZeroâ€™smaindefensivestrategy
average,wenoticethathigher-levelscenariostendtobelocated istotryandblockopponents.Opponentscantakeadvantageofthis
towardsthepositiveğ‘¥ coordinates.Thesecorrespondtosituations byusingdeceptivemoves,especiallywhenmovingslowly,making
wheretheballisclosetotheopponentâ€™sgoalfromtheperspective ithardforTiZeroâ€™sdefenderstostopthem.Thisisillustratedin
referencepolicy.Whilemostregretscorestendtohaveuniform Figure9.
valuesaroundinsimilarpositionsonthefield,inFigure6dthegrid
SuboptimalBallPositioningforShooting. Whentryingto
alsoincludesanadversariallevelwithestimatedregretof1.75.This
scoreagoal,TiZeroagentsoftenchooseasuboptimalpositioning,
indicatesthatMADRIDfoundalevelwherethereferencepolicy
suchasshootingfromanarrowangle.Incontrast,thereference
scoresagainstTiZeroinXP,whileTiZeroconcedesagoalinSP.
policiesoftenmakesubtleadjustmentstooptimallypositionthe
ballbeforeinitiatingashot(e.g.,movetowardsthecentreofthe
5.1 QualitativeAnalysis goalsFigure10).
Nextweconductaqualitativeanalysisoftheadversariallevelsiden- PassingtoBetterPositionedPlayers. Anotableshortcoming
tifiedbyMADRIDonGRFbyvisualisingthehighestrankinglevels inTiZeroâ€™spolicy,whencomparedtothebuilt-inheuristic,isits
inthearchiveacrossallreferencepolicies.Weprovideaselection reluctancetopasstheballtoteammateswhoareinmorefavorable
oftheseexamplesbelow,withacomprehensivelistavailablein positionsandhaveahigherlikelihoodofscoring,asillustrated
AppendixA.Fullvideosofallidentifiedvulnerabilitiescanbefound inFigure11.Incontrast,heuristicbotsâ€”whethereasy,medium,
athttps://sites.google.com/view/madrid-marl. orhardâ€”demonstrateaconsistentpatternofpassingtooptimally
positionedplayers,enhancingtheirgoal-scoringopportunities.This
effectivepassingstrategyseemsunfamiliartoTiZero,causingit
Offsides. Despiteitsstrongperformanceunderstandardevalua-
difficultyinovercomingasuccessfuldefense.
tions,TiZerofrequentlyfallsvictimtoerroneouslypassingtheball
toplayersunmistakablyinoffsidepositions,asshowninFigure7 7Aplayerisoffsidewhenitisintheopponentsâ€™halfandanypartoftheirbodyis
closertotheopponentsâ€™goallinethanboththeballandthesecond-lastopponent.
ThisobservationshighlightsTiZeroâ€™slackofadeepunderstanding
Usuallyoneofthetwoopponentsisthegoalkeeper.Whenthishappensafreekickis
oftherulesofthegame.Incontrast,thereferencepoliciesabstain awardedtotheopponentâ€™steam.(a)Initialplayerandballpositionsinthelevel.(b)Thereceivingplayerisclearlyinoffside,(c)Referencepolicydoesnotpasstooffside
TiZeroisabouttopasstheballtoateammate. thusafreekickisawardedtotheopponents playeranddirectlyrunstowardsthegoalto
team. score.
Figure7:Adversarialexampleofoffsides.
Figure8:Adversarialexampleofanowngoal.TiZerogetstrickedandshootsinitsowngoal.
Figure9:Adversarialexampleofaslowrunningopponent.ThreeTiZero-controlleddefendersarenotabletostopasimple
slowrunningopponent,whowalkspastthemandscores.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeropolicyshootsfromanarrowangle (c)Referencepolicygoestoshootfromabetter
andisblockedbythegoalkeeper positionandscores
Figure10:Adversarialexampleofbettershootingpositioning.(a)Initialplayerandballpositionsinthelevel. (b) TiZero policy runs towards the goal and (c)Referencepolicypassestheballtoabetter
shoots,gettingblockedbythegoalkeeper. positionedplayerwhoscores.
Figure11:Adversarialexampleofpassing.
6 RELATEDWORK
Adversarialattacksonmulti-agentpolicies. Deepneuralnet-
QualityDiversity. QualityDiversity(QD)isacategoryofopen- works,suchasimageclassifiers,areknowntobesensitivetoadver-
endedlearningmethodsaimedatdiscoveringacollectionofso- sarialattacks[5,37,45].Suchsusceptibilityhasalsobeendemon-
lutionsthatarebothhighlydiverseandperformant[8,27].Two stratedinmulti-agentRL.[52]attackstheleadingGo-playingAI,
commonlyusedQDalgorithmsareNoveltySearchwithLocalCom- KataGo[54],bytrainingadversarialpoliciesandachieving>97%
petition[NSLC,27]andMAP-Elites[7,32].Thesetwoapproaches win rate against it. Such adversarial agents are not expert Go-
differinthewaytheystructurethearchive;noveltysearchcom- playingbotsatallandareeasilydefeatedbyamateurhumanplayers,
pletelyforgoesagridandoptsinsteadforgrowinganunstructured insteadtheysimplytrickKataGointomakingseriousblunders.Sim-
archivethatdynamicallyexpands,whileMAP-Elitesadoptsastatic ilarly,[46]introduceISMCTS-BR,asearch-baseddeepRLalgorithm
mappingapproach.AlthoughMADRIDleveragesMAP-Elitesas thatlearnsabestresponsetoagivenagent.Bothofthesesolutions
itsdiversitymechanism,itcanbeadaptedtouseNSLC.Oneofthe findexploitabilityusingRLandexpensiveMonte-Carlotreesearch
mosteffectiveversionsofMAP-ElitesisCMA-ME[16].CMA-ME [6],whereasMADRIDisafast,gradient-free,training-freemethod
combinesMAP-Eliteswiththeevolutionaryoptimizationalgorithm thatfindsadversarialsettingsusingQD.Furthermore,unlikethe
CovarianceMatrixAdaptationEvolutionStrategy(CMA-ES)[18], previousmethods,MADRIDisnotrestrictedtoanyconcreteagent
improvingtheselectionofthefittestsolutionswhichwillbeper- architectureandismoregeneralinnature.MAESTRO[39]crafts
turbedtogeneratenewelites.Mix-ME[21]extendsMAP-Elitesto adversarialcurriculafortrainingrobustagentsin2-playersettings
multi-agentdomains,butislimitedtofullycooperativesettings. byjointlysamplingenvironment/co-playerpairs,emphasizingthe
interplaybetweenagentsandenvironments.
7 CONCLUSIONANDFUTUREWORK
Multi-AgentRL. Recentadvancementsinthefieldofcooperative
multi-agent RL [10, 15, 30, 36] have shown remarkable success ThispaperintroducedMulti-AgentDiagnosticsforRobustnessvia
intacklingcomplexchallengesinvideogames,suchasStarCraft IlluminatedDiversity(MADRID),anovelapproachaimedatsys-
II[13,40].GoogleResearchFootball[GRF,25]standsasoneof tematicallyuncoveringsituationswherepre-trainedmulti-agentRL
themostcomplexmulti-agentRLbenchmarks,asatwo-teamzero- agentsdisplaystrategicerrorsincomplexenvironments.MADRID
sumgamewithsparserewardandrequiringsignificantamount leveragesquality-diversitymechanismsandemploystheconcept
of coordination between co-players. Most of the prior work on ofregrettoidentifyandquantifyamultitudeofscenarioswhere
addressesthetoysettingsoftheGRFonlyinvolvedafewagents agentsenactsuboptimalstrategies,withaparticularfocusonthe
(e.g.,3vs1scenario).Multi-AgentPPO[MAPPO,56]usesPPO[42] advancedTiZeroagentwithintheGoogleResearchFootballen-
withacentralisedcritictoplayontoysettings.CDS[28]analyses vironment. Our investigations using MADRID revealed several
theimportanceofdiversitybetweenpoliciesinGRF.Multi-Agent previouslyunnoticedvulnerabilitiesinTiZeroâ€™sstrategicdecision-
Transformer[MAT,53]modelsGRFasasequenceproblemusing making,suchasineffectivefinishingandmisunderstandingsofthe
theself-attentionmechanism.TiKick[20]attemptstosolvethefull offsiderule,highlightingthehiddenstrategicinefficienciesand
11vs11gameusingdemonstrationsfromsingle-agenttrajectories. latentvulnerabilitiesineventhemostadvancedRLagents.
SPC[51]usesanadaptivecurriculumonhandcraftedenvironments Lookingforward,weareeagertouseMADRIDinbroadermulti-
forovercomingthesparserewardissueinGRF.TiZeroisthefirst agentdomains,combiningitwithadvancedevolutionaryandlearn-
methodthatclaimstohavemasteredthefull11vs11gameofGRF ingstrategiestoenhanceitsabilitytopinpointstrategicinefficien-
fromscratch[29]following45daysoftrainingwithlargeamount cies.InvestigatingvariousadversarialsituationsandRLmodelswill
ofcomputationalresources.Toachievethis,TiZerousesahand- offerdeeperunderstandingofstrategiccomplexityandlearning
craftedcurriculaoverenvironmentvariations,self-play,augmented inmulti-agentsystems.ThiswillaidincreatingmorerobustRL
observationspace,rewardshaping,andactionmasking.Ofnotable solutions.FutureresearchwithMADRIDshouldaimatoptimising
importancearealsotheworkstacklingthegameoffootballina strategiestoidentifyandcorrectstrategicerrors,furtheringthe
roboticssetting[23,24,38]. advancementofrobustmulti-agentsystems.ACKNOWLEDGEMENTS
https://arxiv.org/abs/2110.04041
[18] NikolausHansenandAndreasOstermeier.2001. CompletelyDerandomized
WearegratefultoShiyuHuangforenablingourexperimentswith
Self-AdaptationinEvolutionStrategies.Evol.Comput.9,2(jun2001),159â€“195.
theTiZeroagentandprovidingaccesstothecheckpointsofTiZero https://doi.org/10.1162/106365601750190398
savedthroughoutthetraining.ThisworkwasfundedbyMeta. [19] SebastianHÃ¶fer,KostasBekris,AnkurHanda,JuanCamiloGamboa,MelissaMoz-
ifian,FlorianGolemo,ChrisAtkeson,DieterFox,KenGoldberg,JohnLeonard,
etal.2021.Sim2Realinroboticsandautomation:Applicationsandchallenges.
REFERENCES IEEEtransactionsonautomationscienceandengineering18,2(2021),398â€“400.
[20] ShiyuHuang,WenzeChen,LongfeiZhang,ShizhenXu,ZiyangLi,Fengming
[1] Anthropic.2023. IntroducingClaude. https://www.anthropic.com/index/ Zhu,DehengYe,TingChen,andJunZhu.2021.TiKick:towardsplayingmulti-
introducing-claudeAccessedonOct6,2023. agentfootballfullgamesfromsingle-agentdemonstrations. arXivpreprint
[2] ClaudineBadue,RÃ¢nikGuidolini,RaphaelVivacquaCarneiro,PedroAzevedo, arXiv:2110.04507(2021).
ViniciusBritoCardoso,AvelinoForechi,LuanJesus,RodrigoBerriel,Thiago [21] GarÃ°arIngvarsson,MikayelSamvelyan,BryanLim,ManonFlageat,Antoine
PaixÃ£o,FilipeMutz,LucasVeronese,ThiagoOliveira-Santos,andAlbertoFer- Cully,andTimRocktÃ¤schel.2023.Mix-ME:Quality-DiversityforMulti-Agent
reiraDeSouza.2019.Self-DrivingCars:ASurvey. arXiv:1901.04407[cs.RO] Learning.arXivpreprintarXiv:2311.01829(2023).
[3] DavidBalduzzi,MartaGarnelo,YoramBachrach,WojciechCzarnecki,Julien [22] MinqiJiang,MichaelDennis,JackParker-Holder,JakobFoerster,EdwardGrefen-
Perolat,MaxJaderberg,andThoreGraepel.2019. Open-endedlearningin stette,andTimRocktÃ¤schel.2021. Replay-GuidedAdversarialEnvironment
symmetriczero-sumgames.InProceedingsofthe36thInternationalConference Design.InAdvancesinNeuralInformationProcessingSystems.
onMachineLearning(ProceedingsofMachineLearningResearch,Vol.97),Ka- [23] HiroakiKitano,MinoruAsada,YasuoKuniyoshi,ItsukiNoda,andEiichiOs-
malikaChaudhuriandRuslanSalakhutdinov(Eds.).PMLR,434â€“443. https: awa.1997.Robocup:Therobotworldcupinitiative.InProceedingsofthefirst
//proceedings.mlr.press/v97/balduzzi19a.html internationalconferenceonAutonomousagents.340â€“347.
[4] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,Przemys- [24] HiroakiKitano,MinoruAsada,YasuoKuniyoshi,ItsukiNoda,EiichiOsawa,and
lawDebiak,ChristyDennison,DavidFarhi,QuirinFischer,ShariqHashme, HitoshiMatsubara.1997.RoboCup:AchallengeproblemforAI.AImagazine18,
ChrisHesse,RafalJÃ³zefowicz,ScottGray,CatherineOlsson,JakubPachocki, 1(1997),73â€“73.
MichaelPetrov,HenriquePondÃ©deOliveiraPinto,JonathanRaiman,TimSali- [25] KarolKurach,AntonRaichuk,PiotrStaÅ„czyk,MichaÅ‚ZajÄ…c,OlivierBachem,Lasse
mans,JeremySchlatter,JonasSchneider,SzymonSidor,IlyaSutskever,JieTang, Espeholt,CarlosRiquelme,DamienVincent,MarcinMichalski,OlivierBousquet,
FilipWolski,andSusanZhang.2019.Dota2withLargeScaleDeepReinforcement andSylvainGelly.2020. GoogleResearchFootball:ANovelReinforcement
Learning.CoRRabs/1912.06680(2019).arXiv:1912.06680 LearningEnvironment. arXiv:1907.11180[cs.LG]
[5] NicholasCarlini,AnishAthalye,NicolasPapernot,WielandBrendel,Jonas [26] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,Karl
Rauber,DimitrisTsipras,IanGoodfellow,AleksanderMadry,andAlexeyKu- Tuyls,JulienPerolat,DavidSilver,andThoreGraepel.2017.AUnifiedGame-
rakin.2019.Onevaluatingadversarialrobustness.arXivpreprintarXiv:1902.06705 TheoreticApproachtoMultiagentReinforcementLearning. https://arxiv.org/
(2019). abs/1711.00832
[6] RÃ©miCoulom.2007.EfficientSelectivityandBackupOperatorsinMonte-Carlo [27] JoelLehmanandKennethOStanley.2011.Abandoningobjectives:Evolution
TreeSearch.InComputersandGames,H.JaapvandenHerik,PaoloCiancarini, throughthesearchfornoveltyalone. Evolutionarycomputation19,2(2011),
andH.H.L.M.(Jeroen)Donkers(Eds.).SpringerBerlinHeidelberg,Berlin, 189â€“223.
Heidelberg,72â€“83. [28] ChenghaoLi,TonghanWang,ChengjieWu,QianchuanZhao,JunYang,and
[7] AntoineCully,JeffClune,DaneshTarapore,andJean-BaptisteMouret.2015. ChongjieZhang.2021.Celebratingdiversityinsharedmulti-agentreinforcement
Robotsthatcanadaptlikeanimals.Nature521(2015),503â€“507. learning.AdvancesinNeuralInformationProcessingSystems34(2021),3991â€“4002.
[8] AntoineCullyandYiannisDemiris.2018.QualityandDiversityOptimization:A [29] FanqiLin,ShiyuHuang,TimPearce,WenzeChen,andWei-WeiTu.2023.TiZero:
UnifyingModularFramework.IEEETransactionsonEvolutionaryComputation MasteringMulti-AgentFootballwithCurriculumLearningandSelf-Play.In
22,2(2018),245â€“259. https://doi.org/10.1109/TEVC.2017.2704781 Proceedingsofthe2023InternationalConferenceonAutonomousAgentsandMulti-
[9] WojciechMCzarnecki,GauthierGidel,BrendanTracey,KarlTuyls,Shayegan agentSystems(London,UnitedKingdom)(AAMASâ€™23).InternationalFoundation
Omidshafiei,DavidBalduzzi,andMaxJaderberg.2020.Realworldgameslook forAutonomousAgentsandMultiagentSystems,Richland,SC,67â€“76.
likespinningtops.AdvancesinNeuralInformationProcessingSystems33(2020), [30] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019.
17443â€“17454. Maven:Multi-agentvariationalexploration. AdvancesinNeuralInformation
[10] ChristianSchroederdeWitt,TarunGupta,DenysMakoviichuk,ViktorMakoviy- ProcessingSystems32(2019).
chuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. 2020. Is In- [31] IshitaMediratta,MinqiJiang,JackParker-Holder,MichaelDennis,EugeneVinit-
dependentLearningAllYouNeedintheStarCraftMulti-AgentChallenge? sky,andTimRocktÃ¤schel.2023.StabilizingUnsupervisedEnvironmentDesign
arXiv:2011.09533[cs.AI] withaLearnedAdversary.InProceedingsofThe2ndConferenceonLifelongLearn-
[11] MichaelDennis,NatashaJaques,EugeneVinitsky,AlexandreBayen,StuartRus- ingAgents(ProceedingsofMachineLearningResearch,Vol.232).PMLR,270â€“291.
sell,AndrewCritch,andSergeyLevine.2020.EmergentComplexityandZero- [32] Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymapping
shotTransferviaUnsupervisedEnvironmentDesign.InAdvancesinNeural elites. arXiv:1504.04909[cs.AI]
InformationProcessingSystems,Vol.33. [33] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
[12] AdrienEcoffet,JoostHuizinga,JoelLehman,KennethO.Stanley,andJeffClune. [34] OpenRL-Lab.2023. TiZero. https://github.com/OpenRL-Lab/TiZero. GitHub
2020. Firstreturn,thenexplore. Nature590(2020),580â€“586. https://api. repository.
semanticscholar.org/CorpusID:216552951 [35] JackParker-Holder,MinqiJiang,MichaelDennis,MikayelSamvelyan,Jakob
[13] BenjaminEllis,JonathanCook,SkanderMoalla,MikayelSamvelyan,MingfeiSun, Foerster,EdwardGrefenstette,andTimRocktÃ¤schel.2022.EvolvingCurricula
AnujMahajan,JakobNicolausFoerster,andShimonWhiteson.2023.SMACv2: withRegret-BasedEnvironmentDesign. https://arxiv.org/abs/2203.01302
AnImprovedBenchmarkforCooperativeMulti-AgentReinforcementLearning. [36] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,Jakob
InThirty-seventhConferenceonNeuralInformationProcessingSystemsDatasets Foerster,andShimonWhiteson.2018.Qmix:Monotonicvaluefunctionfactori-
andBenchmarksTrack. https://openreview.net/forum?id=5OjLGiJW3u sationfordeepmulti-agentreinforcementlearning.InInternationalConference
[14] JakobFoerster,YannisMAssael,NandodeFreitas,andShimonWhiteson.2016. onMachineLearning.PMLR,4295â€“4304.
Learningtocommunicatewithdeepmulti-agentreinforcementlearning.In [37] KuiRen,TianhangZheng,ZhanQin,andXueLiu.2020.Adversarialattacksand
AdvancesinNeuralInformationProcessingSystems.2137â€“2145. defensesindeeplearning.Engineering6,3(2020),346â€“360.
[15] JakobN.Foerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli, [38] MartinRiedmiller,ThomasGabel,RolandHafner,andSaschaLange.2009.Rein-
andShimonWhiteson.2018.CounterfactualMulti-AgentPolicyGradients.In forcementlearningforrobotsoccer.AutonomousRobots27(2009),55â€“73.
ProceedingsoftheThirty-SecondAAAIConferenceonArtificialIntelligenceand [39] MikayelSamvelyan,AkbirKhan,MichaelDDennis,MinqiJiang,JackParker-
ThirtiethInnovativeApplicationsofArtificialIntelligenceConferenceandEighth Holder,JakobNicolausFoerster,RobertaRaileanu,andTimRocktÃ¤schel.2023.
AAAISymposiumonEducationalAdvancesinArtificialIntelligence(NewOrleans, MAESTRO:Open-EndedEnvironmentDesignforMulti-AgentReinforcement
Louisiana,USA)(AAAIâ€™18/IAAIâ€™18/EAAIâ€™18).AAAIPress,Article363,9pages. Learning. In International Conference on Learning Representations. https:
[16] MatthewC.Fontaine,JulianTogelius,StefanosNikolaidis,andAmyK.Hoover. //openreview.net/forum?id=sKWlRDzPfd7
2020. CovarianceMatrixAdaptationfortheRapidIlluminationofBehavior [40] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFar-
Space.InProceedingsofthe2020GeneticandEvolutionaryComputationConference quhar,NantasNardelli,TimGJRudner,Chia-ManHung,PhilipHSTorr,Jakob
(CancÃºn,Mexico)(GECCOâ€™20).AssociationforComputingMachinery,NewYork, Foerster,andShimonWhiteson.2019. TheStarCraftMulti-AgentChallenge.
NY,USA,94â€“102. https://doi.org/10.1145/3377930.3390232 InProceedingsofthe18thInternationalConferenceonAutonomousAgentsand
[17] MartaGarnelo,WojciechMarianCzarnecki,SiqiLiu,DhruvaTirumala,Junhyuk MultiAgentSystems.2186â€“2188.
Oh,GauthierGidel,HadovanHasselt,andDavidBalduzzi.2021. PickYour [41] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,
Battles:InteractionGraphsasPopulation-LevelObjectivesforStrategicDiversity. LaurentSifre,SimonSchmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,TimothyLillicrap,andDavidSilver.2020.MasteringAtari,Go, problem.AdvancesinNeuralInformationProcessingSystems35(2022),16509â€“
chessandshogibyplanningwithalearnedmodel.Nature588,7839(dec2020), 16521.
604â€“609. [54] David J Wu. 2019. Accelerating self-play learning in go. arXiv preprint
[42] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. arXiv:1902.10565(2019).
2017.ProximalPolicyOptimizationAlgorithms.ArXivabs/1707.06347(2017). [55] PeterR.Wurman,SamuelBarrett,KentaKawamoto,JamesMacGlashan,Kaushik
[43] L.S.Shapley.1953. StochasticGames. ProceedingsoftheNationalAcademy Subramanian,ThomasJ.Walsh,RobertoCapobianco,AlisaDevlic,FranziskaEck-
ofSciences39,10(1953),1095â€“1100. https://doi.org/10.1073/pnas.39.10.1095 ert,FlorianFuchs,LeilaniGilpin,PiyushKhandelwal,VarunKompella,HaoChih
arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.39.10.1095 Lin,PatrickMacAlpine,DeclanOller,TakumaSeno,CraigSherstan,MichaelD.
[44] DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,George Thomure,HoumehrAghabozorgi,LeonBarrett,RoryDouglas,DionWhitehead,
vandenDriessche,JulianSchrittwieser,IoannisAntonoglou,VedavyasPan- PeterDÃ¼rr,PeterStone,MichaelSpranger,andHiroakiKitano.2022.Outracing
neershelvam,MarcLanctot,SanderDieleman,DominikGrewe,JohnNham, championGranTurismodriverswithdeepreinforcementlearning.Nature602,
NalKalchbrenner,IlyaSutskever,TimothyP.Lillicrap,MadeleineLeach,Koray 7896(Feb.2022),223â€“228.
Kavukcuoglu,ThoreGraepel,andDemisHassabis.2016.Masteringthegameof [56] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,
Gowithdeepneuralnetworksandtreesearch.Nature529(2016),484â€“489. andYiWu.2022. TheSurprisingEffectivenessofPPOinCooperativeMulti-
[45] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan, AgentGames.InThirty-sixthConferenceonNeuralInformationProcessingSystems
IanGoodfellow,andRobFergus.2013.Intriguingpropertiesofneuralnetworks. DatasetsandBenchmarksTrack. https://openreview.net/forum?id=YVXaxB6L2Pl
arXivpreprintarXiv:1312.6199(2013). [57] WenshuaiZhao,JorgePeÃ±aQueralta,andTomiWesterlund.2020.Sim-to-Real
[46] FinbarrTimbers,NolanBard,EdwardLockhart,MarcLanctot,MartinSchmid, TransferinDeepReinforcementLearningforRobotics:aSurvey. 2020IEEE
NeilBurch,JulianSchrittwieser,ThomasHubert,andMichaelBowling.2022. SymposiumSeriesonComputationalIntelligence(SSCI)(2020),737â€“744. https:
ApproximateExploitability:LearningaBestResponse.InProceedingsoftheThirty- //api.semanticscholar.org/CorpusID:221971078
FirstInternationalJointConferenceonArtificialIntelligence,IJCAI-22,LudDeRaedt
(Ed.).InternationalJointConferencesonArtificialIntelligenceOrganization,
3487â€“3493. https://doi.org/10.24963/ijcai.2022/484MainTrack.
[47] BryonTjanaka,MatthewCFontaine,DavidHLee,YulunZhang,NiveditReddy
Balam,NathanielDennler,SujaySGarlanka,NikitasDimitriKlapsis,andStefanos
Nikolaidis.2023. Pyribs:ABare-BonesPythonLibraryforQualityDiversity
Optimization.InProceedingsoftheGeneticandEvolutionaryComputationCon-
ference(Lisbon,Portugal)(GECCOâ€™23).AssociationforComputingMachinery,
NewYork,NY,USA,220â€“229. https://doi.org/10.1145/3583131.3590374
[48] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale,DanBikel,LukasBlecher,CristianCantonFerrer,MoyaChen,GuillemCucu-
rull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,Cynthia
Gao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,
RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,
TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,
JeremyReizenstein,RashiRungta,KalyanSaladi,AlanSchelten,RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,Ross
Taylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,
YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRo-
driguez,RobertStojnic,SergeyEdunov,andThomasScialom.2023. Llama2:
OpenFoundationandFine-TunedChatModels. arXiv:2307.09288[cs.CL]
[49] KarlTuyls,ShayeganOmidshafiei,PaulMuller,ZheWang,JeromeConnor,Daniel
Hennes,IanGraham,WilliamSpearman,TimWaskett,DafyddSteel,PaulineLuc,
AdriaRecasens,AlexandreGalashov,GregoryThornton,RomualdElie,Pablo
Sprechmann,PolMoreno,KrisCao,MartaGarnelo,PraneetDutta,MichalValko,
NicolasHeess,AlexBridgland,JulienPÃ©rolat,BartDeVylder,S.M.AliEslami,
MarkRowland,AndrewJaegle,RemiMunos,TrevorBack,RaziaAhamed,Simon
Bouton,NathalieBeauguerlange,JacksonBroshear,ThoreGraepel,andDemis
Hassabis.2021. GamePlan:WhatAICanDoforFootball,andWhatFootball
CanDoforAI.J.Artif.Int.Res.71(sep2021),41â€“88. https://doi.org/10.1613/jair.
1.12505
[50] OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaÃ«lMathieu,Andrew
Dudzik,JunyoungChung,DavidH.Choi,RichardPowell,TimoEwalds,Petko
Georgiev,JunhyukOh,DanHorgan,ManuelKroiss,IvoDanihelka,AjaHuang,
LaurentSifre,TrevorCai,JohnP.Agapiou,MaxJaderberg,AlexanderSasha
Vezhnevets,RÃ©miLeblond,TobiasPohlen,ValentinDalibard,DavidBudden,Yury
Sulsky,JamesMolloy,TomL.Paine,Ã‡aglarGÃ¼lÃ§ehre,ZiyuWang,TobiasPfaff,
YuhuaiWu,RomanRing,DaniYogatama,DarioWÃ¼nsch,KatrinaMcKinney,
OliverSmith,TomSchaul,TimothyP.Lillicrap,KorayKavukcuoglu,Demis
Hassabis,ChrisApps,andDavidSilver.2019. GrandmasterlevelinStarCraft
IIusingmulti-agentreinforcementlearning. Nat.575,7782(2019),350â€“354.
https://doi.org/10.1038/s41586-019-1724-z
[51] RundongWang,LongtaoZheng,WeiQiu,BoweiHe,BoAn,ZinoviRabinovich,
YujingHu,YingfengChen,TangjieLv,andChangjieFan.2023.TowardsSkilled
PopulationCurriculumforMulti-AgentReinforcementLearning.arXivpreprint
arXiv:2302.03429(2023).
[52] TonyTongWang,AdamGleave,TomTseng,KellinPelrine,NoraBelrose,Joseph
Miller,MichaelDDennis,YawenDuan,ViktorPogrebniak,SergeyLevine,and
StuartRussell.2023.AdversarialPoliciesBeatSuperhumanGoAIs.InProceedings
ofthe40thInternationalConferenceonMachineLearning(ProceedingsofMachine
LearningResearch,Vol.202),AndreasKrause,EmmaBrunskill,KyunghyunCho,
BarbaraEngelhardt,SivanSabato,andJonathanScarlett(Eds.).PMLR,35655â€“
35739. https://proceedings.mlr.press/v202/wang23g.html
[53] MuningWen,JakubKuba,RunjiLin,WeinanZhang,YingWen,JunWang,and
YaodongYang.2022.Multi-agentreinforcementlearningisasequencemodelingA ADVERSARIALEXAMPLESFORGOOGLE
ConfusedAgentBehavior. Anotherintriguingadversarialin-
RESEARCHFOOTBALL stance finds TiZeroâ€™s ball-possessing player aimlessly sprinting
backandforthinrandomareasofthefield,therebyexhibitinga
Beloware11adversarialexamplesinTiZeroweidentifyingusing
completelyunproductivepatternofmovement(Figure18).
MADRID.
ImprovedDefensivePositioning. TiZeroshowsseveralvul-
Offsides. Despiteitsstrongperformanceunderstandardevalua-
nerabilitiesinitsdefensivestrategies,failingtoclosedownonthe
tions,TiZerofrequentlyfallsvictimtoerroneouslypassingtheball
opponentattackingtrajectoryandallowingthemtoscore.Incom-
toplayersunmistakablyinoffsidepositions,asshowninFigure12
parison,Figure19showsthereferencepoliciesclosingdownonthe
ThisobservationshighlightsTiZeroâ€™slackofadeepunderstanding
opponentstrikerandseizingtheballbeforetheyhavethechance
oftherulesofthegame.Incontrast,thereferencepoliciesabstain
toshoot.
from passing the ball to offside players, resulting in successful
scoringoutcomes.8
ErroneousTeamMovement. Severaladversarialexamplesshow
theentiretyofTiZeroâ€™steamrunninginthewrongdirectionto
UnforcedOwnGoals. Perhapsthemostglaringadversarialbe-
defendtheirgoal,whiletheballispositionedfavourablytowards
haviourdiscoveredareinstanceswhereTiZeroagentsinexplicably
theopponentsgoal,leavingasolitaryattackingplayerwithoutsup-
shoottowardstheirowngoal,resultinginunforcedowngoals(See
port,whogetsdeceivedandperformspoorly.Thereferencepolicy
Figure13).Incontrast,whenstartingfromidenticalin-gamepo-
insteaddoesnâ€™tgettrickedandoftenmanagestoscoredespitethe
sitions,thereferencepoliciesmanagetocounterattackeffectively,
disarray(Figure20).
oftenresultinginsuccessfulscoringendeavors.
Hesitation Before Shooting. The most common adversarial
Slowrunningopponents. TheTiZeroagentsalwayschooseto
scenarioencounteredbytheheuristicbotsissituationsinwhich
sprintthroughouttheepisode.However,thismakesthemweakon
TiZerohesitatesbeforetakingashot,allowingthegoalkeeperorde-
defenseagainstopponentswhomoveslowerwiththeball.Instead
fendingplayerstoseizetheball.Incontrast,theinbuiltbotpromptly
oftryingtotackleandtaketheball,TiZeroâ€™smaindefensivestrategy
recognizestheopportunityandshootswithouthesitation,resulting
istotryandblockopponents.Opponentscantakeadvantageofthis
insuccessfulscoring(Figure21).
byusingdeceptivemoves,especiallywhenmovingslowly,making
ithardforTiZeroâ€™sdefenderstostopthem.Thisisillustratedin MissingaGoalScoringOpportunity. TiZerooftenfailstoac-
Figure14. knowledgeeasygoalscoringopportunity,whereitcouldgettothe
ballandscore,butinsteaddecidesnottopursueit.Figure22shows
SuboptimalBallPositioningforShooting. Whentryingto
howthereferencepolicycapitalisesonthiskindofopportunity
scoreagoal,TiZeroagentsoftenchooseasuboptimalpositioning,
andscores.
suchasshootingfromanarrowangle.Incontrast,thereference
policiesoftenmakesubtleadjustmentstooptimallypositionthe B ENVIRONMENTDETAILS
ballbeforeinitiatingashot(e.g.,movetowardsthecentreofthe
goalsFigure15). InourexperimentswithGoogleResearchFootball[25],weadopta
proceduralgenerationmethodforlevelcreation.Foreachplayer,
PassingtoBetterPositionedPlayers. Anotableshortcoming as well as the ball, we randomly sample the (ğ‘¥,ğ‘¦) coordinates:
inTiZeroâ€™spolicy,whencomparedtothebuilt-inheuristic,isits thex-coordinateissampledfromtherange[âˆ’0.9,0.9]andthey-
reluctancetopasstheballtoteammateswhoareinmorefavorable coordinatefromtherange[âˆ’0.4,0.4].Thesettingsemployedduring
positionsandhaveahigherlikelihoodofscoring,asillustrated thegenerationareasfollows:
inFigure16.Incontrast,heuristicbotsâ€”whethereasy,medium,
â€¢ deterministic:settoFalse,implyingthatlevelscanhave
orhardâ€”demonstrateaconsistentpatternofpassingtooptimally
non-deterministiccomponents.
positionedplayers,enhancingtheirgoal-scoringopportunities.This
â€¢ offsides: set to True, enforcing the offsides rule during
effectivepassingstrategyseemsunfamiliartoTiZero,causingit
gameplay.
difficultyinovercomingasuccessfuldefense.
â€¢ end_episode_on_score:settoTrue,whichmeanstheepisode
willterminateonceagoalisscored.
ShootingwhileRunning. Capitalizingonanothergameme-
â€¢ end_episode_on_out_of_play:settoFalse,indicatingthe
chanics,thereferencepoliciesexhibitstrongerbehavioursbyhalt-
episodewillnotendonballout-of-playevents.
ingtheirsprintingbehaviourleadinguptoashot,resultingina
â€¢ end_episode_on_possession_change:settoFalse,indi-
notably higher success rate in goal realisation. TiZeroâ€™s agents,
catingtheepisodewillnotendwhentheballchangespos-
incontrast,consistentlymaintainasprintingstance,therebyfre-
sessionfromoneteamtoanother.
quentlymissingstraightforwardscoringopportunitiesinfrontof
theopposinggoalkeepers(Figure17). Fortheeasybot,thedifficultyissetat0.05.Forthemediumbot,
itissetto0.5,andforthehardbot,thedifficultyisat0.95.These
valuesserveasthedefaultsinGRF,ensuringconsistencyacross
8Aplayerisoffsidewhenitisintheopponentsâ€™halfandanypartoftheirbodyis differentgamescenarios
closertotheopponentsâ€™goallinethanboththeballandthesecond-lastopponent.
WeusetheenhancedobservationspaceasdescribedinTiZero[29],
Usuallyoneofthetwoopponentsisthegoalkeeper.Whenthishappensafreekickis
awardedtotheopponentâ€™steam. consistingof268-dimensionalvectorincludinginformation.(a)Initialplayerandballpositionsinthelevel.(b)Thereceivingplayerisclearlyinoffside,(c)Referencepolicydoesnotpasstooffside
TiZeroisabouttopasstheballtoateammate. thusafreekickisawardedtotheopponents playeranddirectlyrunstowardsthegoalto
team. score.
Figure12:Adversarialexampleofoffsides.
Figure13:Adversarialexampleofanowngoal.TiZerogetstrickedandshootsinitsowngoal.
Figure14:Adversarialexampleofaslowrunningopponent.ThreeTiZero-controlleddefendersarenotabletostopasimple
slowrunningopponentcontrolledbythereferencepolicy,whowalkspastthemandscores.
(a)Initialplayerandballpositionsinthelevel. (b) TiZero shoots from a narrow angle is (c)Referencepolicygoestoshootfromabetter
blockedbythegoalkeeper positionandscores
Figure15:Adversarialexampleofbettershootingpositioning.(a)Initialplayerandballpositionsinthelevel. (b)TiZerorunstowardsthegoalandshoots,(c)Referencepolicypassestheballtoabetter
gettingblockedbythegoalkeeper. positionedplayerwhoscores.
Figure16:Adversarialexampleofpassing.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroshootswhilesprintingandtheball (c)Referencepolicydoesnâ€™trunandisableto
getsblockedbythegoalkeeper. score.
Figure17:Adversarialexampleofshootingwhilerunning.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroaimlesslyrunsupanddownfrom (c)Referencepolicyattackstheopponentgoal,
thesamepositioninanendlessloop. oftenresultingingoalscoringendeavours.
Figure18:Adversarialexampleofconfusedbehaviour.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroâ€™sdefenderrunsalongasuboptimal (c)Referencepolicyinsteadrunstowardsthe
trajectory, giving space for the opponent to attackertoblocktheattempt.
shootandscore.
Figure19:Adversarialexampleofbetterdefensivebehaviour.(a)Initialplayerandballpositionsinthelevel. (b)TiZeroâ€™steamrunsbackwards,leavingasoli-(c)Referencepolicyinsteaddoesnâ€™tgettricked,
taryattackerconfusedandunabletoscore. theattackermovesinabetterpositiontoscore.
Figure20:Adversarialexampleoferroneousteammovement.
(a)Initialplayerandballpositionsinthelevel. (b) TiZero hesitates before shooting, giving (c)Referencepolicyinsteadshootswithouthes-
enoughtimeforthegoalkeepertoseizethe itationandscores.
ball
Figure21:Adversarialexampleofhesitationbeforeshooting.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroâ€™sattackerdoesnotrealiseitcanget (c)Referencepolicyinsteadrunstowardsthe
totheballbeforethegoalkeeper,andrunsback-ball,reachingitbeforethegoalkeeperdoesand
wards. scoring.
Figure22:Adversarialexampleofmissingagoalscoringopportunity.
C IMPLEMENTATIONDETAILS
anLSTMlayertogivetheagentmemory,withthehiddensizefor
Hyperparameters of MADRID are provided in Table 1. We use thislayerbeing256.Everyhiddenlayerisequippedwithlayernor-
theCMA-MEasimplementedinpyribs[47].FortheTiZeroand malizationandReLUnon-linearities.Theorthogonalmatrixisused
reference agents, we use the exact agent architecture as in the forinitializingparameters,andthelearningprocessisoptimized
originalpaper[29]usingTiZeroâ€™sofficialopen-sourcerelease[34]. withtheAdamoptimizer.Similartotheoriginalimplementation,
Parametersharingisappliedtoallagentsintheteam. illegalactionsaremaskedoutbymakingtheirselectionprobability
Thepolicynetworkismadeupofsixdifferentmulti-layerper- zero.Theactionoutputlayerutilizesasoftmaxlayerandisformed
ceptrons(MLPs),eachhavingtwofully-connectedlayers,including witha19-dimensionvector.
onespecificallyfortheâ€™playerIDâ€™,toencodeeverypartoftheob- Experimentsareconductedonanin-housecluster.Everytask,
servationindividually.TheMLPlayershaveahiddensizeof64.The denotedbyaseed,usesoneTeslaV100GPUand10CPUs.Foreach
hiddenfeaturesextractedarebroughttogetherandthenhandledby ofthe51referencepolicies(48TiZerocheckpointsand3built-inTable1:Hyperparametersusedforfindingadversarialexam-
bots),weuse3randomseeds,foreachofthebaselines.Runslast
plesinGoogleResearchFootball. approximately8.5daysfor5000iterationsof MADRID.
Parameter
Numberofsteps 5000
Gameduration 128
NumberofCMA-MEemitters 4
Numberofrepeatsperlevel 4
Emittergaussiannoiseğœ 0.1
Ranker improvement
QDscoreoffset -2