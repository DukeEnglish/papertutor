FreeEnhance: Tuning-Free Image Enhancement
via Content-Consistent Noising-and-Denoising Process
YangLuoâˆ— YihengZhang ZhaofanQiu TingYao
SchoolofComputerScience HiDream.aiInc. HiDream.aiInc. HiDream.aiInc.
FudanUniversity Beijing,China Beijing,China Beijing,China
Shanghai,China yihengzhang.chn@hidream.ai qiuzhaofan@hidream.ai tiyao@hidream.ai
yangluo.fdu@gmail.com
ZhinengChen Yu-GangJiang TaoMei
SchoolofComputerScience SchoolofComputerScience HiDream.aiInc.
FudanUniversity FudanUniversity Beijing,China
Shanghai,China Shanghai,China tmei@hidream.ai
zhinchen@fudan.edu.cn ygj@fudan.edu.cn
Figure1:ThelandscapeexamplesofFreeEnhanceversusSDXL.Ineachpairofimages,theleftoneisgeneratedbySDXLataresolutionof
1,024Ã—1,024,whiletherightoneisproducedbyFreeEnhanceusingtheSDXL-synthesizedimageastheinput.FreeEnhancepreservesthe
resolutionoftheinputimageswhileintroducingadditionaldetailsinacontent-consistentmanner.
Abstract neverthelessisnottrivialandnecessitatestodelicatelyenrichplen-
Theemergenceoftext-to-imagegenerationmodelshasledtothe tifuldetailswhilepreservingthevisualappearanceofkeycontent
recognitionthatimageenhancement,performedaspost-processing, intheoriginalimage.Inthispaper,weproposeanovelframework,
wouldsignificantlyimprovethevisualqualityofthegeneratedim- namelyFreeEnhance,forcontent-consistentimageenhancement
ages.Exploringdiffusionmodelstoenhancethegeneratedimages usingtheoff-the-shelfimagediffusionmodels.Technically,FreeEn-
hanceisatwo-stageprocessthatfirstlyaddsrandomnoisetothe
âˆ—ThisworkwasperformedatHiDream.ai. inputimageandthencapitalizesonapre-trainedimagediffusion
model(i.e.,LatentDiffusionModels)todenoiseandenhancethe
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor imagedetails.Inthenoisingstage,FreeEnhanceisdevisedtoadd
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation lighternoisetotheregionwithhigherfrequencytopreservethe
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe high-frequentpatterns(e.g.,edge,corner)intheoriginalimage.
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Inthedenoisingstage,wepresentthreetargetpropertiesascon-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. straintstoregularizethepredictednoise,enhancingimageswith
MMâ€™24,October28-November1,2024,Melbourne,Australia. highacutanceandhighvisualquality.Extensiveexperimentscon-
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. ductedontheHPDv2datasetdemonstratethatourFreeEnhance
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3681506 outperforms the state-of-the-art image enhancement models in
4202
peS
11
]VC.sc[
1v15470.9042:viXraMMâ€™24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
termsofquantitativemetricsandhumanpreference.Moreremark- Forthehigh-frequencyregion,weemployDDIMinversion[37]
ably,FreeEnhancealsoshowshigherhumanpreferencecompared toattachlightnoise,whichiseasiertobeeliminatedbyusinga
tothecommercialimageenhancementsolutionofMagnificAI. diffusionmodelthanrandomnoise.Forthelow-frequencyregion,
weintroducearandomnoisewithhigherintensitytoaccentuatethe
CCSConcepts changesinlow-frequencyarea,wherevisualdetailsaretypically
â€¢Informationsystemsâ†’Multimediacontentcreation. absent.Then,inthedenoisingprocess,weutilizethepre-trained
SDXL[42],whichisoneofthemostpowerfulopen-sourceimage
Keywords diffusionmodels,asthedenoiser.Theobjectiveofdenoisingstageis
notmerelytoeliminatenoisebutalsotoaddhigh-qualitydetails.To
ImageGeneration,ImageEnhancement,DiffusionModel
achievethis,wedevelopthreegradient-basedregularizers:image
ACMReferenceFormat: actuation,noisedistribution,andadversarialdegradation.These
YangLuo,YihengZhang,ZhaofanQiu,TingYao,ZhinengChen,Yu-Gang regularizersaredesignedtoenhancethenoiseremovalprocessby
Jiang,andTaoMei.2024.FreeEnhance:Tuning-FreeImageEnhancement revisingpredictednoise,leadingtotheimprovementoftheoverall
viaContent-ConsistentNoising-and-DenoisingProcess.InProceedingsof imagequality.Figure1illustratestheexamplesoftheinputimages
the32ndACMInternationalConferenceonMultimedia(MMâ€™24),October fromHPDv2datasetandtheenhancedimagesbyFreeEnhance.
28-November1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,
Insummary,wehavemadethefollowingcontributions:1)The
10pages.https://doi.org/10.1145/3664647.3681506
proposedFreeEnhanceisshowncapableoftuning-freestrategy
toimprovethequalityofthegeneratedimages;2)Thedesigns
1 Introduction
ofcontent-consistentnoisingandthreedenoisingcorrectionsare
Therecentdevelopmentofdiffusionmodelshassparkedaremark- unique;3)FreeEnhancehasbeenproperlyanalyzedandverified
ableincreaseinresearchareaofmultimediacontentgeneration. throughextensiveexperimentsoverHPDv2datasettovalidateits
Amongtheseendeavors,text-to-imagegenerationstandsoutasone efficacy.Withthegood,duetothecontent-consistentcapability,
ofthemostrepresentativetasks[18,50,65].DiffusionProbabilistic FreeEnhancecanbereadilyapplicabletoenhancerealimages.
Models(DPM)[24,40,52]regardimagegenerationasamulti-step
denoisingprocess,employingapowerfuldenoisernetworktopro- 2 Relatedworks
gressivelytransformaGaussionnoisemapintoanoutputimage.
2.1 DiffusionModels
Buildinguponthismethod,LatentDiffusionModels(LDM)[42,46]
Diffusionmodels[18,24,48,54,68]havegarneredattentionfortheir
proposetoexecutedenoisingprocessinthelatentfeaturespacethat
remarkablegenerativequalityanddiversityinlearningcomplex
isestablishedbyapre-trainedautoencoder,leadingtohighcompu-
datadistributions.Theyhavebeenappliedinvariousdownstream
tationefficiencyandimagequality.Toimprovethecontrollability
tasks,includingmultimediagenerationsliketext-to-image[10,44,
oftext-to-imagegeneration,ControlNet[65]andT2I-Adapter[38]
47,65],text-to-video[26,32,62],text-to-3D[11,12,43,63],text-to-
incorporatevariousspatialconditionsintothedenoisernetwork.
audio[21],andimage-to-video[9,67].Diffusionmodelssynthesizes
Despiteshowingimpressiveprogressincontentcontrolling,syn-
multimediacontentsfromaninitialrandomnoisebyiterativede-
thesizinghigh-qualityimageremainschallenging,duetothelack
noisingoperations.Existingpixel-baseddiffusionmodelsexhibit
ofvisualdetailsinthegeneratedimages,asshowninFigure1.
slowinferencespeedsandrequiredsubstantialcomputationalre-
Toenrichdetailsinthegeneratedimages,onegeneralsolution
sources.Manycreativeresearchesdevoteintoovercomethisissue
istheâ€œnoising-and-denoisingâ€process,whichfirstproperlyadds
fromapplyingdiscretediffusion[5],usingimagetokensfromVQ-
noise to the original image, and then uses a diffusion model to
VAE[22].Amongthem,theLatentDiffusionModels(LDM)[46]
denoisethenoisyimage.ThisideaisproposedinSDEdit[36]for
operatesthenoisinganddenoisinginacompressedlatentspace,
imageediting,andthenexploredinSDXL[42]toenhancethegener-
effectivelygetoutofthisdilemmabystrikingabettertrade-off
atedimage,asillustratedinFigure2(a).However,theeffectiveness
betweencostandgenerationquality.Subsequentimprovements
ofsuchprocesshighlyreliesonthestrengthoftheattachednoise.
includingattentionmechanism[8],enhancingthearchitectures
Specifically, when the noise magnitude is low, the input image
[17,41]andprompt-tuning[19]havevigorouslydriventhedevel-
cannotbeeffectivelyenhanced,whereaswhenitishigh,thekey
opmentofdiffusionmodelsinai-generatedcontent.Moreover,asa
content(e.g.,humanorobjects)undergoessignificantchangesthat
basicparadigm,image-to-imagetranslationtasksalsodemonstrate
deviatefromtheoriginalinputimage.Toalleviatethislimitation,
thepotentialofusingthediffusionmodelinstyle-transfer[58,66],
weproposetoremouldthisprocessbyselectivelyaddinglighter
inpainting[33,61]andimageediting[28,51].
noiseinhigh-frequencyregionstopreserveedgeandcornerdetails,
whileheaviernoiseisaddedinlow-frequencyregionstocarrymore
2.2 GuidanceinDiffusionModels
detailsinthesmootharea,asshowninFigure2(b).Moreover,wede-
visethreetypesofregularizationstocorrectthedenoisingprocess Guidanceisatechniqueemployedinthesamplingprocess.Itcan
andproduceimageswithsuperioracutanceandvisualquality. beregardedasanextraupdatetothesamplingdirectionandcan
Specifically,weproposeanewframeworkFreeEnhance,that modifytheoutputsaftertrainingbyguidingwithadditionalcon-
remouldthestandardnoising-and-denoisingprocesstoimprove ditions,suchaslabel[18],text[46].Classifierguidance(CG)[18]
thevisualqualityoftheinputimageandmeanwhilekeepthekey improvesqualityandgeneratesconditionalsamplesbyaddingthe
contentconsistent.Firstly,wedividetheinputimageintohigh- gradientofapre-trainedclassclassifier.Similarly,CLIPguidance
frequencyandlow-frequencyregionsbyutilizingahigh-passfilter. [39]utilizessimilarityscoresfromafine-tunedCLIPmodel[45].FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MMâ€™24,October28-November1,2024,Melbourne,Australia.
Low Magnitude High Magnitude
Noise Noise
Denoising Low Frequency
Filter Acutance
Regularization
High Magnitude Low Magnitude Blending Distribution
Noise Regularization
Noise
Adversarial
Denoising High Frequency Regularization
Input Input Filter Output
Noising Denoising
Output
(a)Conventional Schemes (b)FreeEnhance
Figure2:Theconventionalimageenhancementvia(a)noise-and-denoisingpipelinesuffersfromtradeoffsbetweencreativityandcontent-
consistency.Weintroduce(b)FreeEnhance,atuning-freeframeworkthatselectivelyaddslighternoiseinhigh-frequencyregionstopreserve
contentstructures,whileheaviernoiseisaddedinlow-frequencyregionstoenrichdetailsinsmoothareas.Moreover,threeregularizersare
employedtofurtherimprovevisualqualityduringdenoising.
Toavoidtrainingtheclassifier,classifier-freeguidance(CFG)[25] imageenablingcreativegenerationwhileperseveringattributesof
dropstheexplicitclassifierandmodelsanimplicitonebyomitting contents(Section3.2)inthenoisingstage.Andthenweintroduce
theconditionswithacertainprobabilityduringtraining.Andoth- thenoiseremovalusingadiffusionmodelincorporatedwiththree
ers[6,31,34,59]showthatthegradientofcanalsobeconsideredas gradient-basedterms.Theseterms,formulatedfromtheperspective
aguide.Forexample,ComposableDiffusion[31]adoptscomposed ofacutanceandvisualqualityofimages,respectively,regularizethe
guidancefrommultiapproximateenergy.ContrastiveGuidance predictednoiseandenhanceimagedetailsinthedenoisingstage
[59]utilizespositiveandnegativeprompttobuildacontrastivepair (Section3.3).Figure3depictstheframeworkofourFreeEnhance.
andregardsgradientofdifferenceasguidancetoguidesampling.
3.1 Preliminary
2.3 ImageEnhancementforHumanPreference
Diffusionmodelscreateimagesbyprogressivelyremovingnoise
Whilesharingsimilaritieswithtasksliketraditionimageenhance- throughaseriesofdenoisingsteps.Thisdenoisingprocessessen-
ment,imageenhanceondetailhasbeenstudiedmainlyonhow tiallyreversesanotherprocess(i.e.,noisingprocess)thataddsnoise
tostrikeabettertrade-offbetweendetailandcontentconsistency. toanimagesinapre-determinedtime-dependentmanner.Specifi-
Meanwhile,diffusionmodelshavebeenimplicitlyendowedwith cally,givenatimestepğ‘¡ âˆˆ{ğ‘‡,ğ‘‡âˆ’1,...,1}andthenoiseğœ– ğ‘¡,thenoisy
imagetheenrichingdetailcapability.Basedonhowthiscapability imageiscreatedasğ‘¥ ğ‘¡ =ğ›¼ ğ‘¡ğ‘¥+ğœ ğ‘¡ğœ– ğ‘¡,whereğ‘¥ istheoriginalimage,
isbuilt,wecanbroadlycategorizeexistingstudiesintothreeclasses. ğ›¼ ğ‘¡ andğœ ğ‘¡ areparametersdeterminedbythenoisescheduleandthe
Thefirstsolutionistherefinementmodel.SDXLrefiner[42]train timestepğ‘¡.Toperformthedenoisingprocessforimagesynthesis,a
aseparateLDMmodelinthesamelatentspace.Itcanimprove commonchoicefordiffusionmodelsislearninganeuralnetwork
qualityofdetailedbackgrounds.Thesecondistheupscale-then- ğœ– ğœƒ thatattemptstoestimatedthenoiseğœ– ğ‘¡,whereğœƒ isobtainedby:
t ci rl ee am tee dth eto ad il. sR oe nce lon ct as ltu red gie ios n[7 a, n2 d0 c, a2 n3] ks eh eo pw cot nh ta et ni tt .s Sc ta ap rta ib ni gli ft ry ot mo arg ğœƒminE ğ‘¡âˆ¼U(1,ğ‘‡),ğœ–ğ‘¡âˆ¼N(0,I)||ğœ– ğ‘¡ âˆ’ğœ– ğœƒ(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦)||2, (1)
upscalinganimage,MultiDefusion[7]tiletheimageintoasetof andğ‘¦isanoptionalconditioningsignalliketextprompt.Oncethe
patches,thenproposesfusingmultiplediffusionpathsonthese modelğœ– ğœƒ istrained,imagescanbegeneratedbystartingfromnoise
patches,resultinginhigh-resolutionimages.However,itsuffers ğ‘¥
ğ‘‡
âˆ¼N(0,I)andthenalternatingbetweennoiseestimationand
fromtheobjectrepetitionissuesduetothepromptindependently
noisyimageupdating:
guidingthedenoisingofeachpatch.Thethird[3,27]involvesper-
formingasecondarypredictiononregionsthatarehardtogenerate ğœ–Ë†ğ‘¡ =ğœ– ğœƒ(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦), ğ‘¥ ğ‘¡âˆ’1=ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’(ğ‘¥ ğ‘¡,ğœ–Ë†ğ‘¡,ğ‘¡), (2)
duringthedenoisingprocess.Self-attentionguidance(SAG)[27]
wheretheupdatingcanbeperformedbyDDPM[24],DDIM[53],
utilizesadversarialblurringontheregionsofdenoisingmodelfo-
DPM[52]orothersamplingalgorithms.Usingthereparameteriza-
cused,thenleveragesthesecondarypredictednoiseofblurredone
tiontrick[24],wecanfurtherobtainanintermediatereconstruction
toguidethesamplingdirectionoftheoriginalone.Itcaneffec- ofğ‘¥ 0atatimestepğ‘¡,denotedasğ‘¥Ë†ğ‘¡â†’0.Toimprovetherealismand
tivelyimprovegenerationquality.PerturbedAttentionGuidance
faithfulnesstotheconditioningeneratedimages,SDEdit[36]and
(PAG)[3]introduceaperturbedattentionlayerwhichreplacesthe
SDXL[42]utilizeanoising-and-denoisingprocess,whichfirstadds
attentionmatrixwithanidentitymatrixtoimprovequality. randomnoisecorrespondingtothetimestepğ‘¡ 0intotheinputimage
andthensubsequentlydenoisestheresultingimage.Thehyper-
3 Method parameterğ‘¡ 0 canbetunedtotradeoffbetweenconsistencyand
Thissectionfirstreviewsthediffusionmodelsandthestandard creativity:withasmallerğ‘¡ 0leadingtoamorecontent-consistent
schemesofnoising-and-denoisingprocessforimageenhancement butlessdetailedgeneratedimage.Thisapproachtreatseveryre-
withouttheconsiderationofcontentconsistency(Section3.1).Next, gionoftheinputimagethesame.Itaddsrandomnoisewiththe
we describe how FreeEnhance properly add noise on the input sameintensityattimestepğ‘¡ 0acrosstheentireimage,disregardingMMâ€™24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
Noising Stage Denoising Stage
High/Low
Frequency Filter
Acutance
Regularization
GGS
Creative Streamx t Ã—x (tâˆ’ T1 âˆ’t 0) x tc 0 M l M h RD egis ut lr aib riu zt aio tin o n
Input
DDIM Inversion
x t0
Adversarial
x tâˆ’1 Output
x t Regularization
xs
Stable Stream t0 Ã—t
0
Figure3:AnoverviewofourTuning-FreeImageEnhancement(FreeEnhance)framework.TheprocessofFreeEnhancebeginswithaninput
imageğ‘¥,whichundergoesatwo-streamnoisingschemetoadaptivelyaddnoiseintoğ‘¥.Thecreativesteamaddsstrongnoisewhichisthen
partiallyremovedbyadiffusionmodelwithgradient-guidedsampling(GGS),resultingğ‘¥ ğ‘¡ğ‘.Andinthestablestream,lightnoiseisattached
withtheinputimageusingDDIMinversionstrategy,obtainingğ‘¥ ğ‘¡ğ‘  .Thenğ‘¥ ğ‘¡ğ‘ andğ‘¥ ğ‘¡ğ‘  areadap0 tivelyblendedaccordingtothehigh/lowfrequency
0 0 0
mapğ‘€ â„/ğ‘€ ğ‘™ producedbyfrequencyfilteringofğ‘¥,resultingthenoisyimageğ‘¥ğ‘¡ 0.Then,ğ‘¥ğ‘¡ 0isfedintodiffusionmodelswhichisconstrained
bythreeregularizers,whicharedevisedfromtheperspectivesofimageacutance,noisedistribution,andadversarialdegeneration,inthe
denoisingstagetoproducetheenhancedversionoftheinputimage.
the varying needs of different areas. Some regions might bene-
fitfromcreativelyintroduceddetails,whileothersmightrequire
meticulouspreservationofexistingcontent.Astheresult,thenaive
noising-and-denoisingprocessstrugglestofindabalance,either
over-editingimagesorleavingthemlackingindetail.
3.2 NoisingStage
Toalleviatetheseissues,ourFreeEnhancetailorsthenoisingprocess Input Output w/o calibration Output with calibration
followinganintuitiveidea:High-frequencyareas,richinedgesand Figure4:Comparisonbetweenimagesgeneratedfromcomposited
corners,shouldreceivelighternoisetosafeguardtheiroriginal noisyimagewithandwithoutthedistributioncalibration.Thecolor
patterns. Conversely,low-frequency regions are expectedto be shift/fadingcanbeobservedontheoutputwithoutthecalibration.
exposedtostrongernoise,promotingcreativedetailgenerationand
refinement.Consideringtheassumptionofdiffusionmodelsthatall Forthestablestream,weemploytheDDIMinversion[37]to
regions/pixelsofnoisyimagessharethesamenoisedistribution(i.e., addnoiseintoğ‘¥ andobtainthenoisyimageğ‘¥ ğ‘¡ğ‘  .Itensuresthatthe
theintensityofnoise),weproposeatwo-streamnoisingschemeto contentsinğ‘¥canbereconstructedfromğ‘¥ ğ‘¡ğ‘  wit0 hhighfidelitywhen
0
adaptivelyaddnoiseintotheoriginalimage.Thecreativestream weutilizedadeterministicsamplingalgorithmlikeDDIM.
involveshigherintensityofnoisetoenrichimagedetailsandthe Oncetwonoisyimagesğ‘¥ ğ‘¡ğ‘ andğ‘¥ ğ‘¡ğ‘  areproducedbythecreative
0 0
stablestreamintroducesweakernoisetomaintaincontentfidelity. andstablenoisingstreams,weadaptivelyblendthetwonoisyimage
Forthecreativestreamwhichisdivisedforcreativedetailgen- accordingtothefrequencyofimageregions.Forthehigh-frequency
eration,arandomnoisecorrespondingtotimestepğ‘‡ isaddedinto imageregionslocalizedbythemapğ‘€ â„,wedirectlyinvolveğ‘¥ ğ‘¡ğ‘  to
theinputimageğ‘¥,obtainingthenoisyimageğ‘¥ ğ‘‡ğ‘ = ğ›¼ ğ‘‡ğ‘¥ +ğœ ğ‘‡ğœ– ğ‘‡. maintain the the content structure, resultingğ‘¥ ğ‘¡â„ = ğ‘€ â„ğ‘¥ ğ‘¡ğ‘  . A0 nd
Thenadiffusionmodelisutilizedtoiterativelydenoisingğ‘¥ ğ‘‡ğ‘ till forthelow-frequencyregionswhicharemarked0 byğ‘€ ğ‘™ =10 âˆ’ğ‘€ â„,
thetimestepğ‘¡ 0andobtainğ‘¥ ğ‘¡ğ‘ .Althoughthevariantoftheinput weconductanalpha-compositingforğ‘¥ ğ‘¡ğ‘ andğ‘¥ ğ‘¡ğ‘  usingatradeoff
imageisencouragedduringth0 edenoising,westillneedtoalignthe parameterğœ: 0 0
s intru hc igtu hr -a frl ee qle um ene cn yts r( eo gf it oen nsd )e ote fr ğ‘¥m ğ‘¡ğ‘in we id thby the od sg ee os fa tn hd ec io nr pn ue trs imlo ac ga ete ğ‘¥d
.
ğ‘¥ ğ‘¡ğ‘™
0
=ğ‘€ ğ‘™(ğœğ‘¥ ğ‘¡ğ‘  0+(1âˆ’ğœ)ğ‘¥ ğ‘¡ğ‘ 0). (5)
0 However,thedistributionofweightedaverageoftwonoisyim-
Thusweutilizethegradient-guidedsampling[13,14,16]tointro-
ğœ2
d pu roc ce ec so sn ğ‘¥d ğ‘‡ğ‘it â†’ion ğ‘¥in ğ‘¡ğ‘ 0g io nn thau isx nil oia ir sy ini gnf so trr em aa mn .t Tio hn e[ g1 r3 a] df io er nt th -ge ud ide en doi ss ain mg
-
a dg ise ts rii bs uN tio( nğ›¼ ğ‘¡ Nğ‘¥, (2 ğ›¼ğœ ğ‘¡2 ğ‘¥âˆ’ğ‘¡ ,2ğœ ğœ+ ğ‘¡21 I)I) o,w
f
th hi ech dv iffio ul sa it oe nst ph re och ey sp s,ot rh ee susi lz tie nd gp sr uio br
-
plingutilizesguidancegeneratedfrompre-definedenergyfunctions optimalimagegeneration(e.g.,over-smoothsurface).Tomitigate
ğ‘”(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦)toalteringtheupdatedirectionğœ–Ë†ğ‘¡:
this,werescalethecompositednoisyimageusingascalefactor
âˆš
ğœ–Ë†ğ‘¡ =ğœ– ğœƒ(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦)+ğœ†ğœ ğ‘¡â–½ ğ‘¥ğ‘¡ğ‘”(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦), (3) 1/ 2ğœ2âˆ’2ğœ+1tocalibratethedistribution.Figure4demonstrates
thecomparisonbetweenimagesgeneratedfromcompositednoisy
orrevisethesamplingresultğ‘¥ ğ‘¡âˆ’1:
imagewithandwithoutthedistributioncalibration.
ğ‘¥ ğ‘¡âˆ— âˆ’1=ğ‘¥ ğ‘¡âˆ’1âˆ’ğœ†â–½ ğ‘¥ğ‘¡ğ‘”(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦), (4)
whereğœ†istheweightoftheadditionalguidance.Herewedefinethe 3.3 DenoisingStage
energyfunctionğ‘”(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦)=ğ‘€ â„||ğ‘¥âˆ’ğ‘¥Ë†ğ‘¡â†’0||2forgradient-guided Withthenoisyimageproducedfromtwo-streamnoising,wethen
sampling,whereğ‘€ â„isabinarymapobtainedbyhigh-passfiltering conductthedenoisingprocessandpresentthreetargetproperties
[57]ontheinputimagetoidentifyhigh-frequencyregions. as constraints to regularize the predict noise and/or revise theFreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MMâ€™24,October28-November1,2024,Melbourne,Australia.
islarge.Buildinguponthisintuition,weregularizethedenoising
processviapunishingthegapofdistribution:
Lğ‘‘ğ‘–ğ‘ ğ‘¡ =||1âˆ’Fğ‘£ğ‘ğ‘Ÿ(ğœ– ğœƒ(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦))|| 2, (8)
whereFğ‘£ğ‘ğ‘Ÿ caluatesthevarianceofthepredictednoise.
AdversarialRegularization.Motivatedbytheself-attentionguid-
ancefordiffusionmodels[27],weincorporateanadversarialregu-
larizationforthedenoisingstageofFreeEnhancetoavoidgenerat-
400 300 200 100 0 400 300 200 100 0
Timesteps Timesteps ingblurredimages.Specifically,wedefinetheFğ‘ğ‘™ğ‘¢ğ‘Ÿ asagaussian
Figure5:Thestatisticsofthenoiseğœ– ğœƒ(ğ‘¥ğ‘¡;ğ‘¡,ğ‘¦)predictedbyadiffu- blurfunctionanddevisetheobjectiveasfollws:
sionmodel.Giventhenoisyimagefromthenoisingstage,thered Lğ‘ğ‘‘ğ‘£ =||ğ‘¥Ë†ğ‘¡â†’0âˆ’Fğ‘ğ‘™ğ‘¢ğ‘Ÿ(ğ‘¥Ë†ğ‘¡â†’0)|| 2. (9)
scattersisestimatedduringthedenoisingprocessusingSDXLand
thegrayonesrepresenttheidealvaluesacrossdifferenttimesteps. RegularizatingtheDenoising.Withthehelpofthethreeregular-
izations,weadditionallyinsertarevisingstepattheendofupdating
updatednoisyimagesfromtheaspectsofimageacutanceandnoise ineachdenoisingiteration.Specifically,thesamplingresultğ‘¥
ğ‘¡âˆ’1
distribution.Suchconstraintsareformulatedfromascore-based ineachdenoisingoperationisalteredbyğ‘¥ ğ‘¡âˆ— âˆ’1:
perspective[4]ofdiffusionmodelsandleveragethecapabilityof
themwhichcanadaptoutputsbyguidingthesamplingprocess.
ğ‘¥ ğ‘¡âˆ— âˆ’1=ğ‘¥ ğ‘¡âˆ’1âˆ’ğœŒ ğ‘ğ‘ğ‘¢â–½ ğ‘¥ğ‘¡Lğ‘ğ‘ğ‘¢âˆ’ğœŒ ğ‘‘ğ‘–ğ‘ ğ‘¡â–½ ğ‘¥ğ‘¡Lğ‘‘ğ‘–ğ‘ ğ‘¡âˆ’ğœŒ ğ‘ğ‘‘ğ‘£â–½ ğ‘¥ğ‘¡Lğ‘ğ‘‘ğ‘£, (10)
Acutance Regularization. In photography, acutance refers to whereğœŒ ğ‘ğ‘ğ‘¢ =4,ğœŒ ğ‘‘ğ‘–ğ‘ ğ‘¡ =20,andğœŒ ğ‘ğ‘‘ğ‘£ =0.3arethetradeoffparame-
theperceivedsharpnessassociatedwiththeedgecontrastofan tersdeterminedthroughexperimentalstudies.
image[35].Owingtocharacteristicsofthehumanvisualsystem,
imageswithhigheracutancetendtoappearsharper,despitethe 4 Experiments
factthatanincreaseinacutancedoesnothavetoenhanceactual
WeempiricallyverifythemeritofFreeEnhancefortuning-freeim-
resolutionofimages.Hereweutilizetheacutanceofğ‘¥Ë†ğ‘¡â†’0,whichis
ageenhancementonthepublicdatasetHPDv2[60]followingthe
theintermediatereconstructionofğ‘¥ 0atthetimestepğ‘¡,toregularize
evaluationprotocol[15,30]intermsofthequantitativemetricsand
thedenoising.Specifically,weutilizetheSobelkerneltoestimate
qualitativehumanpreference.Wefirstintroducethedataset,quan-
themagnitudeofthederivativeofbrightnessconcerningspatial
titativemetrics,baselineapproaches,andimplementationdetailsof
variationsofğ‘¥Ë†ğ‘¡â†’0,denotedasFğ‘ğ‘ğ‘¢(ğ‘¥Ë†ğ‘¡â†’0).Toencourageahigher
ourFreeEnhance(Section4.1).Next,weelaboratethecomparisons
acutance,theobjectiveofacutanceregularizationis:
betweenFreeEnhanceandbaselinesonbothquantitativeandquali-
ğ»,ğ‘Š tativeresults(Section4.2),followedbythecomparationtoMagnific
1 âˆ‘ï¸
Lğ‘ğ‘ğ‘¢ =âˆ’ ğ»ğ‘Š Fğ‘ğ‘ğ‘¢(ğ‘¥Ë†ğ‘¡â†’0) (ğ‘–,ğ‘—) , (6) AI(Section4.3).WefurtheranalyzethedesignsinourFreeEnhance
ğ‘–=0,ğ‘—=0 viaablationstudies(Section4.4)andassessthegeneralizationca-
whereğ»,ğ‘Š representthespatialsizeofthenoisyimageand(ğ‘–,ğ‘—) pabilityofFreeEnhanceundertwoscenarios(Section4.5).
aretheindicesofthespatialelement.Thisformulationassumes
4.1 ExperimentalSettings
thatallspatiallocationsinthegeneratedimagesareintendedto
beâ€™sharpâ€™.Butinpractice,emphasizingalltheedges/cornersofthe Dataset.HumanPreferenceDatasetv2(HPDv2)[60]isalarge-
inputimagemayintroduceunpleasantstructuresintheflatregions scaledatasetofhumanpreferencesforimagesgeneratedfromtext
(e.g.,skyandmetalsurfaces)andintricateregions(e.g.,treesand prompts.Itcomprises798,090humanpreferencechoiceson433,760
bushes), impacting human preferences. To tackle this issue, we pairsofimages.HPDv2providesasetofevaluationpromptsthat
extendtheformulationinEq.6withabinaryindicatorğ‘‰(Â·): involvestestingamodelonatotalof3,200prompts,evenlydivided
ğ»,ğ‘Š into4styles:Animation,Concept-Art,Painting,andPhoto.For
1 âˆ‘ï¸
Lğ‘ğ‘ğ‘¢ =âˆ’ ğ»ğ‘Š ğ‘‰(Fğ‘ğ‘ğ‘¢(ğ‘¥Ë†ğ‘¡â†’0) (ğ‘–,ğ‘—))Fğ‘ğ‘ğ‘¢(ğ‘¥Ë†ğ‘¡â†’0) (ğ‘–,ğ‘—) . (7) eachtypeofevaluationprompt,HPDv2providesthecorresponding
ğ‘–=0,ğ‘—=0 benchmarkimagesgeneratedbyvariousmainstreamtext-to-image
whereğ‘‰(Â·)=1whentheinputvaluefallswithinthe35thand65th generativemodels.Hereweexploitthegroupofbenchmarkimages
percentilesofFğ‘ğ‘ğ‘¢(ğ‘¥Ë†ğ‘¡â†’0).Accordingly,ouracutanceregularization generatedbySDXL-Base-0.9astheinputsofimageenhancement
introduces additional details into the images while minimizing approachestovalidatethemeritofourproposal.
unpleasantstructures,enhancingtheoverallgenerationquality. Metrics.Non-referenceimagequalityassessment(NR-IQA)isa
DistributionRegularization.Consideringtheinevitabilityofgen- metricforevaluatingtheimagequalitywithoutneedingitspristine
eralizationerror,thenoisepredictedbydiffusionmodelsğœ– ğœƒ(ğ‘¥ ğ‘¡;ğ‘¡,ğ‘¦) versionforcomparison.WeemploythreekindsofNR-IQAmetrics
maynotfollowaGaussiandistributionN(0,I),particularlywhen forquantitativeevaluation,includingMANIQA[64],CLIPIQA+[56]
wedirectlyutilizeadiffusionmodeltogenerateimagesfromthe andMUSIQ[29].Sinceeachofthemhasmultiplepubliclyavail-
compositednoisyimageproducedinournoisingstage.Tovalidate ableversions,involvingfine-tuningfromdifferentdatasets(e.g.,
thisassumption,weanalyzemorethan3,000imagesandsummarize KADIDandKonIQ)oremployingdifferentmodels(e.g.,ResNetand
thedistributionofpredictednoiseduringthedenoisingprocessin ViT),hereweevaluateimagequalityusingmultiplemetricsfrom
Figure5.Weobservethatthemeanvaluesapproachzeroacross MANIQA(3versions),CLIPIQA(3versions),andMUSIQ(2ver-
differenttimestepsduringdenoising,whilethedifferencebetween sions).HumanPreferenceScorev2(HPSv2)[60]isascoringmodel
theactualvariancevaluesand1isnonnegligiblewhenthetimestep thattrainedontheHPDv2datasettopredicthumanpreferences
naeM
ecnairaVMMâ€™24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
Table1:QuantitativecomparisonsonHPDv2benchmarkimagesgeneratedbySDXL-Base-0.9forimageenhance.Wemarkthebestresultsin
bold.â€ meansrunwithprompts.ForMANIQAandMUSIQ,wereporttheirsub-versionsfine-tunedondifferentdatasets.ForCLIPIQA+,we
reportitssub-versionswithdifferentbackbone.The*denotesthesub-versionfine-tunedwithbothpositiveandnegativeprompts.TheHPSv2
scoreadoptedv2.1version.
MANIQAâ†‘ CLIPIQA+â†‘ MUSIQâ†‘
Method HPSv2â†‘
KonIQ KADID PIPAL ResNet50 ResNet50* ViT-L KonIQ SPAQ
SDXL-base[42] 0.3609 0.5821 0.5721 0.5688 0.4074 0.4267 63.7948 62.1716 27.39
SDXL-refiner[42] 0.3305 0.6006 0.5674 0.5671 0.3658 0.3636 61.7321 60.8410 27.27
SAG[27] 0.3933 0.6088 0.6154 0.6311 0.4450 0.4583 66.7950 65.1907 28.48
Fooocus[2] 0.4180 0.6359 0.6096 0.6130 0.4612 0.4667 68.0095 65.7813 28.31
DemoFusionâ€ [20] 0.2747 0.5414 0.5129 0.5085 0.3424 0.3607 56.5470 58.5090 27.87
FreeU[51] 0.4194 0.6272 0.5938 0.6189 0.4649 0.4703 68.0083 66.2340 28.88
FreeEnhance 0.4122 0.6611 0.6332 0.6535 0.4929 0.4901 68.3928 66.8653 29.32
Input FreeEnhance Input SDXL refiner SAG DemoFusion FreeU FreeEnhance
Figure6:QuantitativecomparisonsofimagesenhancedbydifferentapproachesonHPDv2benchmark.Theregionsinredboxesarepresented
inzoom-inviewtoeasethecomparison.
onthegeneratedimages.WeutilizetheHPSv2toscoretheimages All the mentioned baselines are grouped into three directions:
before/afterenhancementtoverifythequalityimprovement. plainnoising-and-denoisingwithdiffusionmodel(SDXL-baseand
ImplementationDetails.WeusethebasemodelofStableDiffu- SDXL-refiner[42]),upscale-then-tileoperation(DemoFusion[20])
sionXL(SDXL-base)implementedinHuggingFaceTransformer andsamplingwithguidancescheme(SAG[27],Fooocus[2],and
andDiffuserlibraries[55]asthediffusionmodelforimageenhance- FreeU[51]).Notethatallmethods,exceptforâ€œSDXL-refinerâ€,use
ment,unlessotherwisestated.Hence,thenoising-and-denoising thepre-traineddiffusionmodelSDXL-base.â€œSDXL-refinerâ€utilizes
processisconductedinthelatentfeaturespace.Thehigh/lowfre- customweights.Ingeneral,ourFreeEnhanceapproachconsistently
quencyregionsoftheinputimagesarerecognizedbythefiltering achievesbetterimagequalitycomparedtothesebaselines.Notably,
proposedinDR2[57].Theresolutionbeforeandafterenhancement FreeEnhanceattainsascoreof29.32ontheHPSv2metricwithout
is1,024Ã—1,024andtheoriginalpromptsofthebenchmarkimages anydiffusionmodelparametertuning.Comparedtothebaseline
fromHPDv2arenotinvolved.Duringthenoising-and-denoising of SDXL-base, the SDXL-refiner produces unsatisfactory image
processofFreeEnhance,theinferencestepsis100,withaguidance enhancementresultsduetotherelativelyhighintensityoftheat-
scaleof1.0.Thehyper-parameterğ‘¡ 0 thatindicatesthestrength tachednoisewhichisconstrainedonthefirst200(discrete)noise
ofattachednoiseissetas500.Allexperimentsareconductedon scalesduringthetrainingofdiffusionmodel.Benefittingfromthe
NVIDIARTX3090GPUsandIntelXeonGold6226RCPU. self-attentionguidanceemployedduringnoiseremoval,SAGand
Fooocusexhibitbettergenerationqualityandhaveperformance
4.2 PerformanceComparison gainonNR-IQAmetricsandtheHPSv2scores(28.48/28.31vs.27.39).
DemoFusionhasdecentperformancesonbothNR-IQAmetricsand
QuantitativeResults.WecompareourFreeEnhancewithsev-
HPSv2.Wespeculatethatthismaybetheresultoftheemployed
eralopen-sourceoff-the-shelfapproachesintermsofthreegroups
ofNR-IQAmetricsandonehumanpreferencemetricinTable1.FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MMâ€™24,October28-November1,2024,Melbourne,Australia.
Input Magnific AI FreeEnhance Input Magnific AI FreeEnhance
Figure7:ComparisonsofimagesenhancedbyMagnificAIandourFreeEnhance.Regionsinredboxesarepresentedinzoom-inview.
100 benchmark.Thisspeedisgenerallyconsideredacceptableincom-
Magnific AI
80 FreeEnhance mercialimageenhancementproducts(around22secondsperimage
byMagnificAI).WecanalsodevelopafasterversionofFreeEn-
60 52 56
48 hancebyreducingtheoverallinferencestepsofthenosing-and-
44
40 denoisingprocessanddisablingtheregularizersatintervalsthrough-
out the denoising stage. This version of FreeEnhance takes 5.8
20
secondsperimageandachievesa29.23HPSv2score.
0
GPT-4 Human
4.3 ComparisonwithMagnificAI
Figure8:ComparisonsbetweenFreeEnhanceandMagnificAIwith
regardtoGPT-4andhumanpreferenceratios. Figure7presentsvisualizationsofimageenhancementresultsbe-
tweenFreeEnhanceandMagnificAI,renownedforitsadvanced
imageenhancementcapabilities.Inthefirstcase,MagnificAIfalls
shiftedcropsamplingwithdelatedsamplingwhichintroducesun-
shortinprovidingadditionaldetailedstructureforthebuilding
naturallocaltextures.FreeUconductsthedenoisinginafrequency
situatedontheleftsideoftheimage.Conversely,FreeEnhance
decoupledmanner,whichleadstobetterenhancementresultson
seamlesslyenhancesthevisualqualityandrealismoftheexternal
bothNR-IQAandHPSv2.FreeEnhance,whichsimultaneouslycon-
facadesofthebuilding,whileadeptlypreservingboththecontent
siders both ways to add and remove noise to the input images
andthedepthoffield.
forqualityimprovement,obtainsthehighestHPSv2score29.32,
We further conduct a human study to examine whether the
surpassingthebestcompetitorFreeUby0.44.Theresultsdemon-
enhancedimagesfromFreeEnhancebetterconformtohumanpref-
stratetheeffectivenessoffrequency-adaptivenoiseadditionand
erencesthanthatgivenbyMagnificAI.Specifically,werandomly
regularizeddenoisingforimageenhancementbydiffusionmodels.
sample100promptsfromHPDv2andgenerate1,024Ã—1,024images
QualitativeResults.Wethenvisuallyexaminetheenhancement
usingSDXL-base.Werecruited50evaluators,including25males
qualityofourproposalbycomparingFreeEnhancewithfourbase-
and25females,withdiverseeducationalbackgroundsandages.
lines:SDXL-refiner,SAG,DemoFusion,andFreeUonthreeinput
Eachevaluatorwastaskedtoselectthepreferredimagefromtwo
images.Figure6showsthequalitativeresultsoftheenhancedim-
optionsgeneratedbydifferentparadigmsbutoriginatingfromthe
ages.Tobetterillustratetheimagedetails,weprovidezoom-in
sameimage.Evaluatorswereencouragedtochoosetheimagethat
viewsofimagepatches.Overall,alltheapproachessuccessfully
bestsatisfiedtheirpreferences.Wealsoconductthesameevalu-
modifytheinputimages,andourFreeEnhancecreatesthemost
ationusingtheGPT-4.Figure8illustratesthepreferenceratios.
plausiblelocaltexturesanddetailsintheimageswhilemaintain-
Overall,FreeEnhanceachievescompetitiveresultsbothonhuman
inggoodcontentconsistencybetweentheinputimagesandthe
andGPT-4study.
enhancedones.Takingtheimageinthefirstrowasanexample,
FreeEnhancenicelyprovidesmoredetailedstructures,clearbound-
4.4 ExperimentalAnalysis
aries,andrealisticmaterialfortheheadwear,whilepreservingits
shape and characteristics. In contrast, the SDXL-refiner fails to AblationStudy.WeinvestigatehoweachdesigninourFreeEn-
reconstructtheinputimage,resultinginacorruptedoutcome.SAG hanceinfluencesthevisualqualityoftheenhancedimages.Table2
andFreeUproducemoderatemodificationsandaddseveraldetail detailstheperformances(i.e.,HPSv2scores)acrossdifferentablated
structures,butstilllosethesparklingpointsattheleftsideofthe runsofFreeEnhance.Westartfromabasicnoising-and-denoisng
headwear.DemoFusiondramaticallychangestheheadweartoa schemeusingtheSDXL-basediffusionmodel,whichachieves27.39
humanface,whichisnotdesiredinimageenhancement. oftheHPSv2score.Next,bysolelyusingthenoisingstageofFreeEn-
InferenceSpeed.TheFreeEnhancetakes16.3secondsperimage hancewhichadaptivelyaddlightnoiseonhigh-frequencyregions
onanA100GPU,achievinga29.32HPSv2scoreontheHPDv2 and strong noise on low-frequency regions, we observe a clear
)%(
etarniWMMâ€™24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
A bicycle with a basket next to a brick wall The motorcycle is tilting as he turns through a cave A man riding a bike down a dirt road
SD 1.5 SAG PAG FreeEnhance SD 1.5 SAG PAG FreeEnhance SD 1.5 SAG PAG FreeEnhance
Figure9:Comparisonsofimagessynthesizedbyvariousdenoisingapproachesinthetext-to-imagescenario,usingpromptsinHPDv2.
Table2:AblationstudyofeachdesigninFreeEnhanceontheHPDv2
benchmark.ThenotationsDist.,Acu.andAdv.denotesdistribution,
acutanceandadversarialregularizations,respectively.
NoisingStage DenoisingStage
HPSv2â†‘
Stable Creative Adaptive
Dist. Acu. Adv.
stream stream Blending
Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ 27.39
Â¸ Ã˜ Ã˜ Ã˜ Ã˜ Ã˜ 28.21
Â¸ Â¸ Ã˜ Ã˜ Ã˜ Ã˜ 27.78
Â¸ Â¸ Â¸ Ã˜ Ã˜ Ã˜ 28.63
Â¸ Â¸ Â¸ Â¸ Ã˜ Ã˜ 28.71
Input FreeEnhance Input FreeEnhance
Â¸ Â¸ Â¸ Â¸ Â¸ Ã˜ 28.92 Figure10:ExamplesofnaturalimagesenhancedbyFreeEnhance.
Â¸ Â¸ Â¸ Â¸ Â¸ Â¸ 29.32
Table5:ComparisonofHPSv2scoresforimagessynthesizedusing
Table3:Studyofnoiseintensity(determinedbyğ‘¡ 0)onHPDv2. variousdenoisingapproachesinthetext-to-imagescenario,employ-
ğ‘¡ 0 300 400 500 600 700 ingSD1.5ontheHPDv2benchmark.
Method SD1.5 SAG[27] PAG[3] FreeEnhance
HPSv2â†‘ 20.30 21.63 29.32 28.80 28.20
HPSv2â†‘ 24.61 24.76 25.02 25.26
Table4:ComparisonsofHPSv2scoresofimagesproducedbydiffer-
(25.26),surpassingboththevanilladenoisingschemesofSD1.5
entdiffusionmodelswith/withoutFreeEnhance.
(24.61)andtheadvancedapproachesSAG(24.76)andPAG(25.02).
FreeEnhance SDXL-base SDXL-refiner DreamshaperXL WefurthershowcasethreeexamplesinFigure9.Overall,allfour
without 27.39 27.27 29.52 methods correctly align the prompt, and FreeEnhance presents
with 29.32 29.15 30.06 superiorvisualquality,withtheevidenceoftheclearerdepictionof
bricksonthewall(1strow),andthemorerealisticrepresentationof
performanceboost.Wethenleveragethethreeregularizersinthe dirtandgravelblocksontheroad(2ndand3rdrows).Theseresults
denoisingstageinturn.TheHPSv2scoreisconsistentlyboosted againhighlightthegeneralizationcapabilityofFreeEnhance.
upandfinallyreaches29.32.InTable3,theresultsofthenoise NaturalImageEnhancement.Hereweempiricallyevaluatethe
intensitydeterminedbythehyper-parameterğ‘¡ 0,whereahigher capability of FreeEnhance on natural images. We select images
valuesignifiesahighernoiseintensity,showthatFreeEnhance fromtheLAION-5Bdataset[49]andenhancetheirqualityusing
performsoptimallyatamoderatenoiseintensity(ğ‘¡ 0=500=0.5ğ‘‡). ourFreeEnhance.Figure10showcasesfourpairsofenhancement
Effect of the diffusion models. To investigate the impact of results.Forinstance,theleavesofthetreeinthefirstcasebecome
thediffusionmodelonimageenhancement,weutilizethreediffu- clearerafterenhancement.TheresultsindicatethatFreeEnhance
sionmodels:SDXL-base,SDXL-refiner,andDreamshaperXL[1],to iswell-suitedforrefiningnaturalimages.
executethenoising-and-denoisingprocesswithandwithoutour
proposedFreeEnhance.Table4summarizestheHPSv2scoresofthe 5 Conclusion
imagesproducedbyvariousdiffusionmodelswith/withoutFreeEn- WehavepresentedFreeEnhanceforimageenhancementbyexploit-
hance.TheresultsconstantlyverifythatFreeEnhancegenerates ingtheoff-the-shelftext-to-imagediffusionmodels.Particularly,
superiorimagesregardlessofthemodelused. FreeEnhanceformulatesimageenhancementasatwo-stageprocess,
whichfirstlyattachesrandomnoisetotheinputimage,followedby
4.5 Applications
noisereductionthroughthediffusionmodel.Inthenoisingstage,
ToassessthegeneralizationcapabilityofFreeEnhance,weconduct wedevidetheinputimageintohigh/lowfrequencyregions,adding
additionalexperimentsunderdifferenttwoscenarios. light/strongrandomnoisetopreserveexistingcontentstructures
Text-to-ImageGeneration.Tovalidatethatthedenoisingstagein whileenhancingvisualdetails.Inthedenoisingstage,weintro-
FreeEnhancecanbesimplyappliedtotheGaussianrandomnoise ducethreegradient-basedregularizationstorevisethepredicted
for image generation without the reference image, we perform noise,leadingtotheimprovementoftheoverallimagequality.The
text-to-imagegenerationusingourFreeEnhance.Specifically,we resultsontheimagegenerationbenchmarkdemonstratesuperior
synthesizeimagesforthepromptsinHPDv2benchmarkusingthe visualqualityandhumanpreferenceoverstate-of-the-artimage
stablediffusion1.5withdifferentdenoisingapproaches.Thescale enhancementapproaches.Furthermore,theFreeEnhancemodel
oftheclassifier-freeguidanceisfixedas7.5.Table5detailsthe isreadilyapplicabletoenhancenaturalimagestakenbytheend
comparisonresults.FreeEnhanceachievesthehighestHPSv2score users,enablingawiderangeofreal-lifeapplications.FreeEnhance:Tuning-FreeImageEnhancement
viaContent-ConsistentNoising-and-DenoisingProcess MMâ€™24,October28-November1,2024,Melbourne,Australia.
6 Acknowledgments
[26] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,Mohammad
Norouzi,andDavidJFleet.2022.Videodiffusionmodels.InNeurIPS.
ThisworkwaspartiallysupportedbyNationalNaturalScience
[27] SusungHong,GyuseongLee,WooseokJang,andSeungryongKim.2023. Im-
FoundationofChina(No.62032006,62172103).Thecomputations provingsamplequalityofdiffusionmodelsusingself-attentionguidance.In
inthisresearchwereperformedusingtheCFFFplatformofFudan ICCV.
[28] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,
University. InbarMosseri,andMichalIrani.2023.Imagic:Text-basedrealimageeditingwith
diffusionmodels.InCVPR.
[29] JunjieKe,QifeiWang,YilinWang,PeymanMilanfar,andFengYang.2021.Musiq:
References Multi-scaleimagequalitytransformer.InICCV.
[30] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig,
[1] 2024.Dreamshaperxl. https://civitai.com/models/112902/dreamshaper-xl PengchuanZhang,andDevaRamanan.2024.EvaluatingText-to-VisualGenera-
[2] 2024.Fooocus. https://github.com/lllyasviel/Fooocus tionwithImage-to-TextGeneration.arXivpreprintarXiv:2404.01291(2024).
[3] DonghoonAhn,HyoungwonCho,JaewonMin,WooseokJang,JungwooKim, [31] NanLiu,ShuangLi,YilunDu,AntonioTorralba,andJoshuaBTenenbaum.2022.
SeonHwaKim,HyunHeePark,KyongHwanJin,andSeungryongKim.2024. Compositionalvisualgenerationwithcomposablediffusionmodels.InEuropean
Self-RectifyingDiffusionSamplingwithPerturbed-AttentionGuidance.arXiv ConferenceonComputerVision.Springer,423â€“439.
preprintarXiv:2403.17377(2024). [32] FuchenLong,ZhaofanQiu,TingYao,andTaoMei.2024.Videodrafter:Content-
[4] ThiemoAlldieck,NikosKolotouros,andCristianSminchisescu.2024. Score consistentmulti-scenevideogenerationwithllm.arXivpreprintarXiv:2401.01256
Distillation Sampling with Learned Manifold Corrective. arXiv preprint (2024).
arXiv:2401.05293(2024). [33] AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,
[5] JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVan andLucVanGool.2022. Repaint:Inpaintingusingdenoisingdiffusionproba-
DenBerg.2021.Structureddenoisingdiffusionmodelsindiscretestate-spaces. bilisticmodels.InCVPR.
InNeurIPS. [34] GraceLuo,TrevorDarrell,OliverWang,DanBGoldman,andAleksanderHolyn-
[6] ArpitBansal,Hong-MinChu,AviSchwarzschild,SoumyadipSengupta,Micah ski.2024.ReadoutGuidance:LearningControlfromDiffusionFeatures.InCVPR.
Goldblum,JonasGeiping,andTomGoldstein.2024. UniversalGuidancefor [35] HenriMaÃ®tre.2015.ImageQuality.
DiffusionModels.InICLR. [36] ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefano
[7] OmerBar-Tal,LiorYariv,YaronLipman,andTaliDekel.2023.MultiDiffusion: Ermon.2022. Sdedit:Imagesynthesisandeditingwithstochasticdifferential
FusingDiffusionPathsforControlledImageGeneration.InICML. equations.InICLR.
[8] JamesBetker,GabrielGoh,LiJing,â€ TimBrooks,JianfengWang,LinjieLi,â€ Lon- [37] RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or.2023.
gOuyang,â€ JuntangZhuang,â€ JoyceLee,â€ YufeiGuo,â€ WesamManassra,â€ Praful- Null-textinversionforeditingrealimagesusingguideddiffusionmodels.In
laDhariwal,â€ CaseyChu,â€ YunxinJiao,andAdityaRamesh.[n.d.].ImprovingIm- CVPR.
ageGenerationwithBetterCaptions. https://api.semanticscholar.org/CorpusID: [38] ChongMou,XintaoWang,LiangbinXie,YanzeWu,JianZhang,Zhongang
264403242 Qi, Ying Shan, and Xiaohu Qie. 2023. T2I-Adapter: Learning Adapters
[9] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,Maciej to Dig out More Controllable Ability for Text-to-Image Diffusion Models.
Kilian,DominikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts, arXiv:2302.08453[cs.CV]
etal.2023.Stablevideodiffusion:Scalinglatentvideodiffusionmodelstolarge [39] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,
datasets.arXivpreprintarXiv:2311.15127(2023). BobMcGrew,IlyaSutskever,andMarkChen.2021.Glide:Towardsphotorealistic
[10] JingwenChen,YingweiPan,TingYao,andTaoMei.2023.Controlstyle:Text- imagegenerationandeditingwithtext-guideddiffusionmodels.(2021).
drivenstylizedimagegenerationusingdiffusionpriors.InACMMM.7540â€“7548. [40] AlexanderQuinnNicholandPrafullaDhariwal.2021.Improveddenoisingdiffu-
[11] YangChen,YingweiPan,YehaoLi,TingYao,andTaoMei.2023. Control3d: sionprobabilisticmodels.InICML.
Towardscontrollabletext-to-3dgeneration.InACMMM. [41] WilliamPeeblesandSainingXie.2023.Scalablediffusionmodelswithtransform-
[12] YangChen,YingweiPan,HaiboYang,TingYao,andTaoMei.2024. Vp3d: ers.InICCV.
Unleashing2dvisualpromptfortext-to-3dgeneration.InCVPR. [42] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,
[13] HyungjinChung,JeongsolKim,MichaelTMccann,MarcLKlasky,andJongChul JonasMÃ¼ller,JoePenna,andRobinRombach.2024. SDXL:ImprovingLatent
Ye.2023. Diffusionposteriorsamplingforgeneralnoisyinverseproblems.In DiffusionModelsforHigh-ResolutionImageSynthesis.InICLR.
ICLR. [43] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.2023.Dreamfusion:
[14] HyungjinChung,ByeongsuSim,DohoonRyu,andJongChulYe.2022.Improving Text-to-3dusing2ddiffusion.InICLR.
diffusionmodelsforinverseproblemsusingmanifoldconstraints.InNeurIPS. [44] YuruiQian,QiCai,YingweiPan,YehaoLi,TingYao,QibinSun,andTaoMei.
[15] KevinClark,PaulVicol,KevinSwersky,andDavidJFleet.2024.Directlyfine- 2024.BoostingDiffusionModelswithMovingAverageSamplinginFrequency
tuningdiffusionmodelsondifferentiablerewards.InICLR. Domain.InCVPR.
[16] ChengliangDai,ShuoWang,YuanhanMo,KaichenZhou,ElsaAngelini,Yike [45] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
Guo,andWenjiaBai.2020.Suggestiveannotationofbraintumourimageswith SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
gradient-guidedsampling.InMICCAI. etal.2021.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
[17] XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,Peizhao InICML.
Zhang,SimonVandenhende,XiaofangWang,AbhimanyuDubey,etal.2023.Emu: [46] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rn
Enhancingimagegenerationmodelsusingphotogenicneedlesinahaystack. Ommer.2022.High-resolutionimagesynthesiswithlatentdiffusionmodels.In
arXivpreprintarXiv:2309.15807(2023). CVPR.
[18] PrafullaDhariwalandAlexanderNichol.2021.Diffusionmodelsbeatganson [47] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyL
imagesynthesis.InNeurIPS. Denton,KamyarGhasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,Tim
[19] WenkaiDong,SongXue,XiaoyueDuan,andShuminHan.2023.Prompttuning Salimans,etal.2022.Photorealistictext-to-imagediffusionmodelswithdeep
inversionfortext-drivenimageeditingusingdiffusionmodels.InICCV. languageunderstanding.InNeurIPS.
[20] RuoyiDu,DongliangChang,TimothyHospedales,Yi-ZheSong,andZhanyuMa. [48] CemSazara.2023.DiffusionModelsinGenerativeAI.InACMMM.
2024.DemoFusion:DemocratisingHigh-ResolutionImageGenerationWithNo [49] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,Ross
$$$.InCVPR. Wightman,MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,Mitchell
[21] DeepanwayGhosal,NavonilMajumder,AmbujMehrish,andSoujanyaPoria. Wortsman,PatrickSchramowski,SrivatsaKundurthy,KatherineCrowson,Lud-
2023.Text-to-AudioGenerationusingInstructionGuidedLatentDiffusionModel. wigSchmidt,RobertKaczmarczyk,andJeniaJitsev.2022.LAION-5B:Anopen
InACMMM. large-scaledatasetfortrainingnextgenerationimage-textmodels.InNeurIPS.
[22] ShuyangGu,DongChen,JianminBao,FangWen,BoZhang,DongdongChen,Lu [50] YanShu,WeichaoZeng,ZhenhangLi,FangminZhao,andYuZhou.2024.Visual
Yuan,andBainingGuo.2022.Vectorquantizeddiffusionmodelfortext-to-image TextMeetsLow-levelVision:AComprehensiveSurveyonVisualTextProcessing.
synthesis.InCVPR. arXiv:2402.03082[cs.CV] https://arxiv.org/abs/2402.03082
[23] YingqingHe,ShaoshuYang,HaoxinChen,XiaodongCun,MenghanXia,Yong [51] ChenyangSi,ZiqiHuang,YumingJiang,andZiweiLiu.2024.Freeu:Freelunch
Zhang,XintaoWang,RanHe,QifengChen,andYingShan.2024.ScaleCrafter: indiffusionu-net.InCVPR.
Tuning-freeHigher-ResolutionVisualGenerationwithDiffusionModels.In [52] JaschaSohl-Dickstein,EricA.Weiss,NiruMaheswaranathan,andSuryaGanguli.
ICLR. 2015.DeepUnsupervisedLearningusingNonequilibriumThermodynamics.
[24] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic [53] JiamingSong,ChenlinMeng,andStefanoErmon.2021. DenoisingDiffusion
models.InNeurIPS. ImplicitModels.InICLR.
[25] JonathanHoandTimSalimans.2021. Classifier-freediffusionguidance.In [54] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,Stefano
NeurIPS. Ermon,andBenPoole.2020.Score-basedgenerativemodelingthroughstochasticMMâ€™24,October28-November1,2024,Melbourne,Australia. YangLuoetal.
differentialequations.InICLR. [62] ZhenXing,QijunFeng,HaoranChen,QiDai,Hang-RuiHu,HangXu,Zuxuan
[55] PatrickvonPlaten,SurajPatil,AntonLozhkov,PedroCuenca,NathanLambert, Wu,andYu-GangJiang.2023. ASurveyonVideoDiffusionModels. ArXiv
KashifRasul,MishigDavaadorj,DhruvNair,SayakPaul,WilliamBerman,Yiyi abs/2310.10647(2023).
Xu,StevenLiu,andThomasWolf.2022. Diffusers:State-of-the-artdiffusion [63] HaiboYang,YangChen,YingweiPan,TingYao,ZhinengChen,andTaoMei.
models.https://github.com/huggingface/diffusers. 2023.3dstyle-diffusion:Pursuingfine-grainedtext-driven3dstylizationwith2d
[56] JianyiWang,KelvinCKChan,andChenChangeLoy.2023.Exploringclipfor diffusionmodels.InACMMM.6860â€“6868.
assessingthelookandfeelofimages.InAAAI. [64] SidiYang,TianheWu,ShuweiShi,ShanshanLao,YuanGong,MingdengCao,
[57] ZhixinWang,XiaoyunZhang,ZiyingZhang,HuangjieZheng,MingyuanZhou, JiahaoWang,andYujiuYang.2022.Maniqa:Multi-dimensionattentionnetwork
YaZhang,andYanfengWang.2023.DR2:Diffusion-basedRobustDegradation forno-referenceimagequalityassessment.InCVPR.
RemoverforBlindFaceRestoration.InCVPR. [65] LvminZhang,AnyiRao,andManeeshAgrawala.2023. Addingconditional
[58] ZhizhongWang,LeiZhao,andWeiXing.2023. Stylediffusion:Controllable controltotext-to-imagediffusionmodels.InICCV.
disentangledstyletransferviadiffusionmodels.InICCV. [66] YuxinZhang,NishaHuang,FanTang,HaibinHuang,ChongyangMa,Weiming
[59] ChenWuandFernandoDelaTorre.2024.ContrastivePromptsImproveDisen- Dong,andChangshengXu.2023.Inversion-basedstyletransferwithdiffusion
tanglementinText-to-ImageDiffusionModels.arXivpreprintarXiv:2402.13490 models.InCVPR.
(2024). [67] ZhongweiZhang,FuchenLong,YingweiPan,ZhaofanQiu,TingYao,YangCao,
[60] XiaoshiWu,YimingHao,KeqiangSun,YixiongChen,FengZhu,RuiZhao,and andTaoMei.2024.TRIP:TemporalResidualLearningwithImageNoisePrior
HongshengLi.2023.Humanpreferencescorev2:Asolidbenchmarkforevaluat- forImage-to-VideoDiffusionModels.InCVPR.
inghumanpreferencesoftext-to-imagesynthesis.arXivpreprintarXiv:2306.09341 [68] Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and
(2023). ChangWenChen.2024.Sd-dit:Unleashingthepowerofself-superviseddiscrim-
[61] ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKunZhang.2023.Smart- inationindiffusiontransformer.InCVPR.
brush:Textandshapeguidedobjectinpaintingwithdiffusionmodel.InCVPR.