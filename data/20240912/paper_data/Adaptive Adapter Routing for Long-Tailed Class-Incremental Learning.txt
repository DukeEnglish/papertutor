Adaptive Adapter Routing for Long-Tailed
Class-Incremental Learning
Zhi-Hong Qi1,2, Da-Wei Zhou1,2*, Yiran Yao‚Ä° 1, Han-Jia Ye1,2,
De-Chuan Zhan1,2
1School of Artificial Intelligence, Nanjing University, 163 Xianlin
Avenue, Nanjing, 210023, Jiangsu, China.
2National Key Laboratory for Novel Software Technology, Nanjing
University, 163 Xianlin Avenue, Nanjing, 210023, Jiangsu, China.
*Corresponding author(s). E-mail(s): zhoudw@lamda.nju.edu.cn;
Contributing authors: qizh@lamda.nju.edu.cn; yrharryyao@gmail.com;
yehj@lamda.nju.edu.cn; zhandc@nju.edu.cn;
Abstract
In our ever-evolving world, new data exhibits a long-tailed distribution, such as
emergingimagesinvaryingamounts.Thisnecessitatescontinuousmodellearning
imbalanceddatawithoutforgetting,addressingthechallengeoflong-tailedclass-
incremental learning (LTCIL). Existing methods often rely on retraining linear
classifiers with former data, which is impractical in real-world settings. In this
paper, we harness the potent representation capabilities of pre-trained models
and introduce AdaPtive Adapter RouTing (Apart) as an exemplar-free solu-
tion forLTCIL. To counteract forgetting, we train inserted adapterswith frozen
pre-trained weights for deeper adaptation and maintain a pool of adapters for
selection during sequential model updates. Additionally, we present an auxiliary
adapter pooldesigned for effective generalization, especiallyon minority classes.
Adaptiveinstanceroutingacrossthesepoolscapturescrucialcorrelations,facili-
tatingacomprehensiverepresentationofallclasses.Consequently,Aparttackles
theimbalanceproblemaswellascatastrophicforgettinginaunifiedframework.
Extensive benchmark experiments validate the effectiveness of Apart. Code is
available at: https://github.com/vita-qzh/APART.
Keywords:Long-TailedClass-IncrementalLearning,Pre-trainedModel,Catastrophic
Forgetting,Class-IncrementalLearning
‚àóCorrespondenceto:Da-WeiZhou(zhoudw@lamda.nju.edu.cn)
‚Ä°WorkdoneduringaninternshipatSchoolofArtificialIntelligence,NanjingUniversity.
1
4202
peS
11
]GL.sc[
1v64470.9042:viXra1 Introduction
Traditional machine learning algorithms typically assume a closed-world scenario,
where data originates from a static, balanced distribution Ye et al. (2024). However,
in reality, data often exhibits a long-tailed streaming pattern, as pictures emerge
all the time but are different in category and number. This necessitates incremental
learning from long-tailed data, referred to as Long-Tailed Class-Incremental Learning
(LTCIL) Liu et al. (2022). In LTCIL, a significant challenge is catastrophic forget-
ting French (1999), where the model tends to lose knowledge of former data during
the learning process. Additionally, the inherent data imbalance causes the model to
under-representminorityclasses,leadingtobiastowardsmajorityclassesZhangetal.
(2023).Theseinterrelatedchallengesposeasignificantprobleminthemachinelearning
community. Consequently, several algorithms have been developed to address them.
For instance, LWS Liu et al. (2022) rebalances the classifier by sampling a balanced
datasetfrombothnewandreservedformerdata.Similarly,GVAlignKallaandBiswas
(2024) enhances representation robustness and aligns the classifier by replaying gen-
eratedpseudo-augmentedsamples.Theseapproachesyieldimprovedperformancebut
rely on the storage of exemplars from former classes.
RetainingexemplarsiscriticaltopreventingforgettinginLTCIL,butthisapproach
often fails in real-world applications due to storage limitations Krempl et al. (2014)
and privacy concerns Chamikara et al. (2018). Recent advancements in Pre-Trained
Models (PTMs) Han et al. (2021), however, demonstrate their effectiveness without
relying on exemplars, thanks to their robust representations. PTMs are increasingly
favored not only in class-incremental learning Wang et al. (2022) but also in long-
tailed learning Shi et al. (2024), challenging the traditional ‚Äòtraining-from-scratch‚Äô
paradigm.Theirabilitytoprovidestrongfoundationalknowledge,enhancedbyexten-
sive pre-training datasets, ensures impressive adaptability to downstream tasks. This
adaptabilityprovesparticularlybeneficialinhandlingdatascarcityinminorityclasses
and maintaining performance on older tasks. As such, leveraging PTMs has become a
prominent strategy in these fields to achieve superior performance. In this paper, we
explore the integration of PTMs into LTCIL, aiming to overcome its challenges in an
‚Äòexemplar-free‚Äô manner.
LTCILpresentstwocriticalchallenges,i.e.,catastrophicforgettinganddataimbal-
ance.Catastrophicforgettingoccurswhennewinformationsupersedesoldknowledge,
leading to the overwriting of existing features and overall performance decay. Data
imbalance, on the other hand, skews the learning process towards the majority class,
neglecting the minority class. As a result, the boundary between the majority and
minority classes is biased, making the model more likely to classify samples into
the majority classes. In the learning process, these two issues are closely coupled,
amplifying the difficulty of LTCIL.
An ideal model capable of continuously learning from a long-tailed data stream
should be both provident and comprehensive. Provident means the model, espe-
cially when based upon PTMs, can fully exploit its potential to learn new classes.
Consequently, the model updating mechanism should be carefully designed to resist
catastrophicforgettinginthelearningprocess.Meanwhile,comprehensiveimpliesthat
2the model employs a specific learning strategy to capture minority class more, offer-
ing a holistic and distinct representation for all. By harmonizing these provident and
comprehensiveaspects,amodelcanadeptlynavigatethechallengesofLTCIL,paving
the way for enhanced performance.
In this paper, we propose AdaPtive Adapter RouTing (Apart) to address the
above challenges in LTCIL. To make the model provident, we freeze majority of the
parameters of PTMs and employ trainable adapters at each layer. Furthermore, we
extend one group of adapters to a pool containing multiple groups. Every time new
data comes, we retrieve the most relevant group of adapters and update it. Addi-
tionally, we introduce an auxiliary pool specifically focused on learning from minority
classes. During inference, we dynamically combine these two pools to get a compre-
hensive representation. Rather than using a fixed threshold to filter training data
for the auxiliary pool, our method adaptively learns instance routing, encoding task
information in a data-driven manner. This reflects the auxiliary pool‚Äôs relevance to
minority classes, enabling a holistic overview of all classes. The unified framework
above is trained in an end-to-end fashion, enabling automatic routing learning in a
data-drivenway.WeextensivelyvalidatetheeffectivenessofApartthroughnumerous
experiments on several benchmark datasets.
The main contributions of Apart can be summarized as follows:
‚Ä¢ Layer-wiseadaptersfordeeperadaptationofpre-trainedmodelsareselectedamong
multiple alternatives, reducing forgetting when transferring to downstream tasks.
‚Ä¢ Anauxiliarypoolisspeciallydesignedforminorityclassestocompensateforthelack
of data. The imbalance in data when training the auxiliary pool greatly decreases,
resulting in a comprehensive representation of minority classes.
‚Ä¢ Adaptive routing is learned to capture correlations between data and the auxiliary
pool automatically in a data-driven manner, reducing dependency on the manual
threshold when defining minority classes.
This paper is organized as follows. Section 2 reviews the main related work. Section
3 formulates the investigated issue and introduces the baseline method. Section 4
describes Apart and details each of its elements. Section 5 presents the empirical
evaluations and further analysis. After that, we conclude the paper in Section 6.
2 Related Work
Long-Tailed Learning: aims to learn from highly imbalanced data Zhang et al.
(2023), where a small number of classes (majority classes/head classes) have a large
amount of data, while rest classes (minority classes/tail classes) have limited data.
Currentalgorithmscanberoughlydividedintothreegroups.Thefirstgroupconsiders
re-sampling the dataset to form a balanced training set Chawla et al. (2018) or re-
weighting the loss terms to favor tail classes Cui et al. (2019); Lin et al. (2017). The
second group considers transferring knowledge from head classes to tail classes Wang
et al. (2017) and self-training Rosenberg et al. (2005); Wei et al. (2021) to enhance
the recognition ability of tail classes. The third group designs techniques to improve
3representation or classifier modules via representation learning Huang et al. (2016),
decoupled training Kang et al. (2020), and ensemble learning Zhou et al. (2020).
Class-Incremental Learning (CIL): aims to sequentially learn new tasks without
forgetting former knowledge Zhou et al. (2024); Wang et al. (2023). To alleviate the
dilemma, a large number of work is proposed, mainly falling into three categories.
The first group transfers the knowledge of old models to the new one by knowledge
distillation Hinton et al. (2015) when updating Li and Hoiem (2017); Douillard et al.
(2020). The second group is based on the reserved exemplars of former tasks and
replays them to maintain old knowledge Zheng et al. (2024); Rebuffi et al. (2017);
Hou et al. (2019a); Castro et al. (2018). The third group expands the network Yan
et al. (2021); Wang et al. (2022); Zhou et al. (2023) to meet the demand for model
capacityarisingfromtheincreasingdata.Aspre-trainedmodelsgainpopularity,more
methods based on PTMs emerge Zhou et al. (2024); Wang et al. (2022); Seale Smith
etal.(2022);Zhouetal.(2024);Wangetal.(2022);Zhouetal.(2024).Thesemethods
mainlydesignlightweightmodulestoadaptthePTMinaparameter-efficientmanner.
Long-TailedClass-IncrementalLearning(LTCIL):isrecentlyproposedtolearn
from long-tailed streaming data. LWS Liu et al. (2022) samples a balanced dataset
from new data and reserved former data to re-weight the classifier for better per-
formance. Furthermore, GVAlign Kalla and Biswas (2024) enhances the robustness
of representations and aligns the classifier by replaying generated pseudo-augmented
samples.Thesemethodsbothfollowatwo-stagestrategy,rectifyingtheoutputsofthe
model using a balanced dataset in the second stage. In contrast, we aim for a better
incremental performance without accessing former data.
3 Preliminaries
In this section, we first describe the setting of LTCIL and then introduce the baseline
method and its limitations.
3.1 Long-Tailed Class-Incremental Learning
In class-incremental learning, a model learns from sequential tasks. When task t
arrives, the training dataset for the model is denoted as D = {(x ,y )}nt , where
t i i i=1
x ‚àà RH√óW√óC is the i-th instance, y ‚àà Y is the corresponding label in current
i i t
label space and n is the total size of task t. There are no overlaps in labels, i.e.,
t
Y (cid:84) Y = ‚àÖ, when t Ã∏= t‚Ä≤. Different from the uniform distribution where the fre-
t t‚Ä≤
quency of each class is a constant in the conventional CIL, data in LTCIL follows
a long-tailed distribution. The steeper the distribution is, the more challenging the
problem is for the model to fit the tail classes without forgetting.
In LTCIL, we denote the classification model as f : RH√óW√óC ‚Üí R|Yt|, where
Y
=(cid:83)t
Y is the set of all seen classes at task t. It can be decoupled as a feature
t k=1 k
extractor œï : RH√óW√óC ‚Üí Rd (d is the dimension of the feature) and a classifier
g : Rd ‚Üí R|Yt|, i.e., f(x) = g(œï(x)). The classifier can be decomposed as a set of
classifiersforeachtask,i.e.,g =[g 1,...,g t],whereg
k
:Rd ‚ÜíR|Yk|.Whenfacinganew
task, the classifier g needs to be updated and extended. We follow Zhu et al. (2021);
Wangetal.(2022)toimplementallmethodswithout exemplars,i.e.,whentrainingon
4High
Assigner
Class1 Auxiliary Adapter Pool
( ) ( )
Class 2 MLP Loss
Pre-Trained
( ) ( )
Class 3 ùúìùúì w Model
ùùãùùã
Low Adapter Selection Same
Head Adapter Pool
( ) ( )
Tail Loss
Pre-Trained
( ) ( )
Model
Input Instance Key Adapter Trainable Frozen
Fig. 1 Demonstration of Apart. To make the model comprehensive, an auxiliary adapter pool
is learned for minority classes and instance-wise routing is learned adaptively. To make the model
provident,multipleadaptersformapooltoenlargethecapacityoffine-tunedmodel.Theobjective
istolearnanautomaticroutingtolearneffectivelyfromminorityclasseswithoutforgetting.
taskt,themodelonlyhasaccesstoD .AsLTCILprovidesnotaskidatinference,the
t
model must distinguish between old and new classes and have a good performance on
all seen tasks. In other words, the performance on the whole balanced testing dataset
(cid:83)t
k=1D
ktestistakenintoconsideration,i.e.,(cid:80)
(xi,yi)‚àà(cid:83)t k=1D ktest‚Ñì(f(x i),y i),where‚Ñì(¬∑,¬∑)
measures the discrepancy between the input pairs.
3.2 Pre-Trained Models for CIL
Currently,pre-trainedmodelsgainpopularityintheCILfield.Themostrepresentative
PTMforvisualrecognitiontasksisVisionTransformer(ViT)Dosovitskiyetal.(2020)
pre-trainedonthelarge-scaledataset(e.g.,ImageNetRussakovskyetal.(2015))asthe
backboneœïtoextractfeaturesoftheinstances.ViTconsistsofanembeddinglayerand
severaltransformerblocks.Animagexisfirstlydividedintoasequenceofpatchesand
then passes the embedding layer to get its embedding E = [e 0,e 1,...,e Np] ‚àà RNp√ód
(N is the number of patches). Then a learnable [CLS] token c ‚àà Rd is added to its
p
embedding to get the final input of transformer blocks x = [c,E]. The model gives
0
the ultimate prediction based on œï(x), the embedded feature of [CLS] token.
L2P Wang et al. (2022) is the first method to utilize pre-trained ViT in CIL. To
adapt to the downstream tasks efficiently, it employs visual prompt tuning Jia et al.
(2022),aparameter-efficientfine-tuningtechnique.Promptscanbeseenasahorizontal
expansion of the input. During training, it freezes the whole backbone and prepends
prompt P ‚àà RL√ód to the embedding of the instance, where L is the length of the
prompt.Then,theconcatenatedembedding[c,P,E]ispassedtothefrozenbackbone
to get the features for classification. In this way, knowledge about the task is encoded
in the prompts. To mitigate forgetting, multiple prompts are provided for the model
toenlargetherepresentationspace,denotedasapoolP =[P ,P ,...,P ].Foreach
1 2 M
prompt P , a key k is associated in the key-value format for query (i.e., (k ,P )).
i i i i
At training, instance-wise prompt is chosen from P and then updated. The choice of
5prompt is based on the distance between the instance and the learnable keys:
minŒ≥(œï(x),k ), (1)
s
s
where Œ≥(¬∑,¬∑) denotes cosine distance. Here, we use the classification feature œï(x) to
represent the instance, and calculate the distance with k , the key for P . Other
s s
prompt-based algorithms Wang et al. (2022); Seale Smith et al. (2022) also explore
more choices and fusion mechanisms.
Discussions: While Eq. 1 provides an efficient way to encode task-specific informa-
tion in the prompts, L2P does not obtain a promising performance in LTCIL. There
are two reasons accounting for the degradation. Firstly, learnable prompts are only
prependedtotheinputlevelandthushavelimitedinfluencewhenthewholebackbone
is frozen. It restricts the model‚Äôs representation ability when facing diverse down-
stream tasks. Secondly, when the data distribution becomes long-tailed, it requires
more specific measures to compensate for the minority classes for a holistic feature.
Duringtraining,promptsareunavoidablybiasedtowardsthemajorityclassesandcan
not store as precise knowledge as expected. Thus, the ability to resist forgetting of
Eq. 1 is weakened.
4 Adaptive Adapter Routing for LTCIL
Motivated by the potential of PTMs, we try to incorporate PTMs into LTCIL in an
exemplar-free manner. To make the model provident, we train adapters inserted at
each layer instead of prepending prompts at the first layer, vertically expanding the
frozen PTM and making a deeper adaptation. On the other hand, to make the model
comprehensive, an additional mechanism unique for minority classes is proposed. To
strengthen the correlation of the addition and minority classes, instance routing is
adaptively learned in a data-driven manner. In this section, we first introduce the
techniquestofacilitatelong-tailedlearning,andthendiscusstheroutingstrategy.We
summarize the training pipeline in the last part.
4.1 Auxiliary Adapter Pool
Adapter is a vertical expansion of the PTMs, increasing the transferability to
downstream tasks while consuming limited parameters. Compared to prompts, the
adaptation occurs in the structure instead of the input, resulting in a better perfor-
manceonvisualrecognitiontasks.Inthispaper,wefollowChenetal.(2022)toinsert
the adapter to each transformer block in ViT. Adapter is a bottleneck structure con-
sisting of a down-projection layer W ‚àà Rd√ór, a non-linear activation ReLU and
down
an up-projection layer W ‚àà Rr√ód, where r ‚â™ d is the bottleneck middle dimen-
up
sion. It mainly changes the residual connection in the transformer blocks by adding
non-linear transformations to the identical input. For the i-th transformer block, we
denote its output after the multi-head self-attention as xÀÜ . We insert adapters to the
i
MLP structure and get the output as:
x =MLP(LN(xÀÜ ))+ReLU(xÀÜ W )W , (2)
i+1 i i down up
6whereMLP(LN(xÀÜ ))istheoriginaloutputofthetransformerblock,andLNdenotes
i
layer norm. We denote the group of adapters inserted at each block as A. During
training,wefreezethewholebackboneandonlyoptimizethelightweightmodulesinA,
enablingtheadaptationtodownstreamtaskswhilepreservingPTM‚Äôsrepresentations.
To enlarge the capacity of the fine-tuned model and mitigate forgetting, we fol-
low Wang et al. (2022) to expand adapters A to a pool A, which contains M groups
of adapters A = [A ,A ,...,A ]. We also adopt the key-query matching strategy
1 2 M
in Eq. 1. Different from L2P, which selects a set of prompts, Apart chooses only one
group of adapters, i.e., 12 adapters inserted into each layer, which is enough to store
exacted knowledge. Then, the adapted model gives the prediction
f(x;A)=g(œï(x;A))
where œï(¬∑;A) is the adapted feature extractor based on the pool A and g(¬∑) is the
corresponding tuned classifier which is completed with a simple linear layer.
During training, we seek to find the most suitable adapters within A to adapt to
the current task by minimizing:
L(x,y;A)=‚Ñì(f(x;A),y)+Œ≥(œï(x),k ), (3)
s
where k is the key of the most suitable group chosen from A according to Eq. 1.
s
SolvingEq.3enablesencodingthetask-specificinformationintothegroupofadapters,
and the adapter selecting strategy enables a holistic estimation of the query instance
to the most suitable adapter.
Auxilliary Adapter Pool:Sinceimbalanceddistributionmakesabiasedmodeland
insufficient learning from minority classes, to compensate for the shortfall, an intu-
itive approach is to specifically train minority classes more without the interference
of majority classes. In the absence of majority classes, the imbalance ratio between
minorityclassesismuchsmaller,makingitpossibletogiveamoreaccuraterepresenta-
tion.Hence,weproposelearninganauxiliarypoolAaux,withthesamepoolsizeasA,
to favor minority classes. Similarly, the auxiliary adapted model gives the prediction
f(x;Aaux)=gaux(œï(x;Aaux))
where œï(¬∑;Aaux) and gaux(¬∑) are the corresponding backbone and the classifier. Both
classifiers are extended in the way mentioned in section 3.1. The retrieval loss based
ontheauxiliarypoolisdubbedasL(x,y;Aaux).Inthiscase,theoriginalpoolwillbe
trained for all classes, while the auxiliary pool should be specially optimized on tail
classes, enabling a holistic representation among all classes. This can be realized by
adjusting the weights of the auxiliary loss for different classes. The weight of the loss
is determined by the frequency of the class in the training set:
L (x,y)=L(x,y;A)+I(N(y)‚â§Œ∏)L(x,y;Aaux), (4)
1
where N(y) is the number of instances belonging to class y, I(¬∑) is the indicator
function, which outputs 1 if the expression holds and 0 otherwise. Œ∏ is the threshold
7to define minority and majority classes. For classes with instances less than Œ∏, we
consider it as a minority class and train the auxiliary pool to fit features for them.
Effect of Auxiliary Adapter Pool: Eq. 4 introduces the auxiliary pool and sums
uplosses,whichisshowninthemiddlepartofFigure1.Thefirstitemforcesadapters
to learn from current task and update stored knowledge, and the second forces the
auxiliary pool to learn from minority classes only. By optimizing Eq. 4, on the basis
of sufficient learning of majority classes, the auxiliary pool generalizes on minority
classes. As a result, the bias in the recognition of minority and forgetting in continual
learning is alleviated.
4.2 Adaptive Routing
The weight function I(N(y) ‚â§ Œ∏) reflects the correlation between the auxiliary pool
and all classes, especially the minority classes. The auxiliary pool does not retain
valuable information from the majority, only from the minority, as a modification of
one single pool. However, the heuristic format of the step function relies on Œ∏, making
ahardandartificialboundarybetweenmajorityclassesandminorityclasses.Filtering
needs a precise Œ∏. For some long-tailed distributions with an extreme imbalance ratio,
theinstancesofonemajorityclassmaybenearlyasmuchasthesumoftheinstances
of all minority classes in a task. In this case, a considerable Œ∏ is needed. Once Œ∏ gets
smaller, unexpected data may be excluded, leaving them insufficient representations
even with an auxiliary pool. Besides, the step function makes no difference between
minority classes, meaning the imbalance between minority classes is still unsettled.
To reduce dependency on a precise threshold and modify the minority imbal-
ance, we propose an adaptive adapter routing strategy to assign samples with an
instance-specific function for a smoother boundary. The information of the instance
and the category are combined to represent the relation to Aaux. For instance x, the
instance information is encoded in the embedding of œï(x) with a linear layer œà (¬∑),
1
i.e., œà (œï(x)), and the class information is encoded with a mapping from integers to
1
embeddings, i.e., œà (N(y)). The concatenation of two embeddings is passed through
2
an MLP to get the adaptive weight:
w(x,y)=œÉ(MLP([ œà (œï(x)),œà (N(y))])). (5)
1 2
We train an assigner w for each instance. The output of the assigner is between 0 and
1 after a non-linear activation œÉ(¬∑), reflecting the correlation between the auxiliary
pool and minority classes. Then, the origin loss is updated to
L (x,y)=L(x,y;A)+w(x,y)L(x,y;Aaux). (6)
1
To instruct the routing with more data information, the original weight in the
format of the step function can be seen as a reference at the first epochs. Then, we
avoid the tendency to 0 of weights when training by adding a regularization term:
L (x,y)=(Œ±‚àíw(x,y))2, (7)
2
8Algorithm 1 Adaptive Adapter Routing for LTCIL
Input: Dataset: D . Pre-trained model: œï(¬∑);
t
Output: A, Aaux, w(¬∑,¬∑), g(¬∑), gaux(¬∑);
1: Randomly initialize A, Aaux;
2: repeat
3: Get a mini-batch of training instances: {(x i,y i)}n i=t 1;
4: Calculate the loss L based on A;
5: Calculate the loss L based on Aaux;
6: Calculate the weighted sum of loss L 1 in Eq. 6;
7: Calculate the regularization term L 2 in Eq. 7;
8: Get the total loss L 1+L 2;
9: Obtain derivative and update the model;
10: until reaches predefined epochs
Table 1 AverageandlastperformancecomparisononthreedatasetsinshuffledLTCIL.‚ÄòIN-R‚Äô
standsfor‚ÄòImageNet-R‚Äôand‚ÄòObjNet‚Äôstandsfor‚ÄòObjectNet‚Äô.Thebestperformanceisshownin
bold.Allmethodsareimplementedwiththesamepre-trainedmodelforafaircomparison.Methods
with‚Ä†requireexemplarswhileothersdonot.
CIFARB50-5 CIFARB50-10 IN-RB100-10 IN-RB100-20 ObjNetB100-10 ObjNetB100-20
ShuffledLTCIL
Acc AccT Acc AccT Acc AccT Acc AccT Acc AccT Acc AccT
Finetune 60.62 48.51 66.79 56.45 67.08 54.73 71.18 63.75 30.66 21.08 35.44 26.40
LwF 63.48 48.62 70.85 60.50 72.78 64.88 76.56 70.70 36.88 27.66 38.27 31.85
LUCIR‚Ä† 79.98 76.08 81.59 78.84 75.23 69.53 77.81 73.25 45.54 42.61 46.69 43.69
LUCIR+LWS‚Ä† 80.51 75.90 82.34 79.55 76.44 71.22 78.70 75.20 46.15 40.46 48.13 45.20
SimpleCIL 69.81 66.53 69.97 66.53 56.38 54.52 56.55 54.52 37.75 34.58 37.57 34.58
ADAMw/Finetune 75.22 72.96 75.42 72.96 62.55 61.32 62.64 61.32 48.13 44.82 47.96 44.82
L2P 73.59 67.17 76.04 71.73 68.93 62.65 72.85 68.28 44.15 39.74 45.44 42.17
DualPrompt 69.00 63.08 72.75 67.45 69.20 65.07 71.83 68.60 42.20 37.49 43.95 40.49
CODA-Prompt 76.45 70.29 79.39 74.40 75.58 71.82 78.46 75.45 46.66 41.55 48.81 44.87
Apart 84.91 81.93 86.10 83.89 78.65 75.50 80.16 77.03 53.74 50.59 52.88 48.30
where Œ± is a hyperparameter reflecting the restriction on the optimization of the
assigner w. We set it to 1 as default.
Effect of Adaptive Adapter Routing:Eq.5learnsaninstance-wiseweightforthe
auxiliary loss, which is shown in the left part of Figure 1. It combines the information
in the instance and the class. The replacement in Eq. 6 is shown in the right part of
Figure 1. Compared to the heuristic weight, the adaptively learnable weight regulates
the importance of a single instance in a data-driven manner. Thus, the auxiliary
pool can encode more instance-specific knowledge and give a more comprehensive
representation.
4.3 Summary of Apart
We give the pseudo code of Apart in Algorithm 1. In each mini-batch, we first cal-
culate the loss produced by the adapter pool separately as Eq. 3. Then we learn
weight for auxiliary loss following Eq. 5 and sum them up for back propagation, i.e.,
L (x,y)+L (x,y).Notethatthetwoadapterpoolssharethesamepre-trainedmodel,
1 2
making the memory budget negligible compared to the PTM.
9Table 2 AverageandlastperformancecomparisononthreedatasetsinorderedLTCIL.‚ÄòIN-R‚Äô
standsfor‚ÄòImageNet-R‚Äôand‚ÄòObjNet‚Äôstandsfor‚ÄòObjectNet‚Äô.Thebestperformanceisshownin
bold.Allmethodsareimplementedwiththesamepre-trainedmodelforafaircomparison.Methods
with‚Ä†requireexemplarswhileothersdonot.
CIFARB50-5 CIFARB50-10 IN-RB100-10 IN-RB100-20 ObjNetB100-10 ObjNetB100-20
OrderedLTCIL
Acc AccT Acc AccT Acc AccT Acc AccT Acc AccT Acc AccT
Finetune 66.06 43.80 70.39 45.73 67.97 48.85 72.13 60.80 21.52 00.47 24.73 10.32
LwF 70.12 52.33 77.14 64.78 75.53 64.13 78.20 71.97 23.54 00.44 21.78 02.94
LUCIR‚Ä† 79.94 70.73 83.52 75.73 77.51 71.28 80.32 75.40 51.11 42.43 50.57 42.21
LUCIR+LWS‚Ä† 80.59 70.44 83.24 75.69 78.03 71.62 80.46 75.18 52.89 44.57 53.74 45.32
SimpleCIL 72.22 67.67 72.36 67.67 57.39 54.52 57.32 54.52 44.95 34.58 44.72 34.58
ADAMw/Finetune 77.10 72.98 77.19 72.98 62.91 61.30 62.91 61.30 51.16 44.84 51.16 44.84
L2P 74.68 59.36 78.37 65.48 72.75 65.92 74.95 69.40 49.94 40.53 50.87 41.76
DualPrompt 74.80 60.72 77.88 65.84 71.24 66.65 72.85 69.08 47.88 39.14 49.15 40.74
CODA-Prompt 75.36 59.07 80.44 67.46 78.33 74.42 79.94 76.97 51.43 41.42 52.93 43.26
Apart 84.21 73.09 87.16 78.90 78.92 74.03 81.41 77.05 55.94 43.83 57.00 46.61
     
     
  
  
        
          )  / L  Z Q  ) H W X Q H  /  $   ' 3  $ 0
    / 8 & , 5  ' X D O 3 U R P S W
 / : 6  & 2 ' $  3 U R P S W
          6 L P S O H & , /  $ 3 $ 5 7
                                                                  
 1 X P E H U  R I  & O D V V H V  1 X P E H U  R I  & O D V V H V  1 X P E H U  R I  & O D V V H V
(a) CIFARB50-5 (b) ImageNet-RB100-10 (c) ObjectNetB100-10
Fig. 2 Incremental performance when starting from half of the total classes in shuffled LTCIL.
Weshowthelegendsin(c).Apart consistentlyoutperformsothercomparedmethods.
During inference, we add the logits of two adapter pools by a simple ensemble,
i.e., f(x;A)+f(x;Aaux). The routing function is only adopted during training, and
the logits during inference is the addition of two adapted models without relying on
the routing module. The forward pass to obtain [CLS] token is necessary if the key-
value mechanism is applied in the PTM-based methods. Thus, compared to those,
Apart needs only one more forward at inference. One more forward makes a better
performanceatthecostofexpensiveinference,whichmaybeimprovedbysimplifying
the structure, such as reducing the adapted layers.
5 Experiment
In this section, we compare Apart on benchmark LTCIL datasets with state-of-the-
artmethods.TheablationsverifytheeffectivenessofeachpartofApart,andfurther
analysis and visualization are conducted to explore the inherent characteristics of
Apart.
5.1 Implementation Details
Dataset: Following Liu et al. (2022), we first experiment on the dataset
CIFAR100 Krizhevsky et al. (2009). Since PTMs are mostly pre-trained on Ima-
geNet21k Russakovsky et al. (2015), datasets like ImageNet-Subset with 100 classes
10
     \ F D U X F F $      \ F D U X F F $      \ F D U X F F $are unsuitable for evaluation due to the overlap. Following Wang et al. (2022); Zhou
etal.(2024),wechooseanothertwodatasetsImageNet-RHendrycksetal.(2021)and
ObjectNet Barbu et al. (2019) as challenging downstream tasks for PTMs to adapt
to.Amongthem,CIFAR100contains60,000picturesfor100classes.ImageNet-Rcon-
tains30,000picturesfor200classes.ObjectNetcontainsabout33,000picturesfor200
classes. To simulate LTCIL scenarios, we sample part of these datasets. Following Liu
et al. (2022), we control the long-tailed distribution by a parameter œÅ, which is the
ratio between the quantity of the least frequent class N and that of the most fre-
min
quent class N max, i.e., œÅ= NN mm ain x. For CIFAR100, the imbalance ratio œÅ is set to 0.01,
and, at most, one class has 500 instances. For ImageNet-R, the dataset is naturally
long-tailed with œÅ = 0.11 and the most frequent class has 349 instances. Although it
does not follow a standard exponential decay, we leave it as origin without sampling.
For ObjectNet, we set œÅ=0.01 and N =200.
max
Setting: Following Liu et al. (2022), we conduct two LTCIL scenarios, i.e., ordered
LTCIL and shuffled LTCIL. The former follows the long-tailed distribution by task,
while the latter first shuffles the decaying numbers randomly and then assigns fre-
quency to each class. In ordered LTCIL, the imbalance ratio remains identical across
tasks, while shuffled LTCIL can be seen as a general scenario allowing different ratios
in different tasks.
Dataset split: Firstly, we adopt the split in Liu et al. (2022) that starts with a
base task containing half classes and then separates other classes into 5 tasks or 10
tasks.Forsimplicity,wedenotethesplitintheformatof‚ÄúB{m}-{n}‚Äù,wheremisthe
number of classes in the first task and n is the number in the following tasks.
Compared methods: Since our method is based on PTMs, we mainly compare to
PTM-based CIL methods, i.e., L2P Wang et al. (2022), DualPrompt Wang et al.
(2022), CODA-Prompt Seale Smith et al. (2022), SimpleCIL Zhou et al. (2024) and
ADAM-Finetune Zhou et al. (2024). Besides, we also re-implement LWS Liu et al.
(2022) with PTMs, which is an exemplar-based method. Since it is a training trick
that needs to be combined with other methods, we choose LUCIR Hou et al. (2019b)
and LUCIR+LWS for comparison. Finally, we also compare to the classical CIL algo-
rithm, LwF Li and Hoiem (2017), and the baseline method, Finetune, which saves no
exemplars.
Training details: Following Wang et al. (2022), we use the same backbone
ViTB/16-IN1K,whichispretrainedonImageNet21Kandadditionallyfinetunedon
ImageNet1Kfor all compared methods.Thechoiceofbackbonedeterminesthedimen-
sion of k , as the retrieval is based on the cosine distance between the keys and the
s
embedding.Thus,thedimensionofk issetto768.WetrainthemodelusingAdamw
s
with a batch size of 48 for 10 epochs. The learning rate starts from 0.003 and decays
withcosineannealing.ForApart,thesizeofthepoolis5,andtheprojectiondimen-
sion is 64. For prompt-based methods, the size of the pool is 10. For exemplar-based
methods like LUCIR and LWS, we save 10 exemplars per class for replay.
Evaluation protocol: Following Rebuffi et al. (2017), we record the accuracy after
each task i as Acc and use the average Acc = 1 (cid:80)T Acc on all T tasks and the
i T i=1 i
last accuracy Acc as the metrics.
T
115.2 Benchmark Comparison
In this section, we report the accuracy over benchmark datasets, i.e., CIFAR100,
ImageNet-R, and ObjectNet under both shuffled LTCIL and ordered LTCIL in Table
1, 2 and show the incremental performance in Figure 2.
Specifically, Figure 2 clearly shows the superior performance of Apart. We can
inferthatconventionalmethods(e.g.,LwF)haveanevidentdownwardtrend,showing
they suffer from catastrophic forgetting. In contrast, the decline of LUCIR and LWS
is relatively modest, owing to the help of exemplars. Compared to other parameter-
efficient finetuning-based techniques (e.g., L2P, DualPrompt and CODA-Prompt),
we find a gap between the first task‚Äôs performance. The gap clearly indicates the
effectivenessofutilizingauxillinaryadapterpooltocompensatefortheminorityclasses
during training.
Additionally, Apart demonstrates its superiority over compared methods on
benchmark datasets in Table 1, 2. The poor performance of conventional CIL meth-
ods, i.e., LwF, indicates that the long-tailed data magnifies the difficulty of the CIL
problem,evenbasedon PTMs withstronggeneralizability.Therepresentationability
is proved by the performance of SimpleCIL, which totally relies on the frozen PTMs.
However, the benefit from PTMs is limited in LTCIL, requiring more task-specific
featuresforimprovement.Similarly,Apart outperformsmostprompt-basedmethods
by even 8% in the shuffled scenario and 9% in the ordered scenario. It reveals that,
although prompts help resist forgetting in conventional CIL, the knowledge stored in
prompts is unavoidably interfered with by imbalance. LWS is designed for convolu-
tionnetworksinLTCILandimprovestheperformanceofcombinedLUCIR.However,
as reported, when we replace ResNet with pre-trained ViT, the improvement of a
re-weighting classification layer is limited. Although with a balanced dataset, LWS
remains the biased representation unsettled. It shows that, in the long-tailed data
stream, the bias in representation is more harmful than the bias in classification.
Thus, the auxiliary pool in Apart learns more from minority classes, removing the
underlying bias of representation, receiving a better performance than LUCIR and
LUCIR+LWS, indicating the superiority of a dedicated auxiliary pool for minority
classes. To sum up, Apart outperforms both prompt- and exemplar-based methods,
validating its effectiveness.
5.3 Ablation Study
In this section, we con-
duct an ablation study Table 3 AblationstudyonorderedCIFAR100B50-5.Each
partinApart helpstoimprovetheperformance.
to analyze the importance
of three components in
Method Acc Acc
Apart and explore the T
Apart 84.21 73.09
influence of the number of
adapter pools. w/o Adaptive Routing 83.04 71.32
In Table 3, results w/o Auxiliary Pool 80.46 67.26
clearly show the effective- w/o Adapter Pool 75.98 58.64
ness of each component.
12When we drop the adaptive routing between two pools, we train two pools as Eq. 4,
where the distribution of instances largely depends on a fixed threshold. In contrast,
theadaptiveanddynamicboundarycanobtainanimprovementoftheauxiliarypool.
Besides, ablating the auxiliary pool means only using one pool for training, resulting
in no unique mechanism to cope with the imbalanced data. Hence, we find one pool
cannot generate a comprehensive representation for all classes, and insufficient learn-
ingfromminorityclassesleadstoadecline.Furthermore,wereplacetheadapterpool
with one single adapter. Due to the lack of capacity, the model suffers from forget-
ting. The decline in average accuracy is up to 4% and in last accuracy is nearly 9%,
showing the necessity of multiple adapters when learning sequential tasks.
From the above ablations, we find one more adapter pool can lead to performance
improvement. However, does that mean more adapter pools will definitely lead to
better performance? Table 4 shows the change in accuracy when applying multiple
pools. The added pools are used to capture more from minority classes under the
same regulation. From the table, we observe that, at first, the additional pool brings
an increase in accuracy. The model size grows linearly when the pools increase, but
the improvement is limited. When the number of pools increases to 6, a decrease
occurs.Theresultsshowthattheincreaseinparametersdoesnotnecessarilyguarantee
performance improvement. According to the experiments, we set the size to 2 for a
trade-off between performance and model size.
Table 4 Incrementalperformancewithdifferentnumberof
adapterpoolsonorderedCIFAR100B50-5.
#Pool 1 2 3 4 5 6
Acc 80.46 83.04 83.11 83.40 83.28 83.11
AccT 67.26 71.32 70.98 72.13 72.16 71.75
5.4 Further Analysis
Subgroup measures: Following Liu et al. (2019), we can draw three splits of all
classes by the number of instances and report the performance on these splits. Specif-
ically, we report the accuracy of three splits of classes in CIFAR100 in Table 5, i.e.,
many-shot(with ‚â• 100 instances), medium-shot (20 ‚àº 100 instances) and few-shot
(‚â§ 20 instances). ‚ÄòOverall‚Äô denotes Acc . Specifically, we find the poor overall perfor-
T
mance of other methods is mainly due to the performance gap in minority classes. As
theinstancesbecomefewer,testaccuracybecomeslower.WhencomparingSimpleCIL
to L2P, we find L2P gets a better overall performance at the cost of minority classes.
ByfinetuningthePTMs,L2Pincreasesthemany-shotaccuracyby13%butimproves
overall accuracy by 0.6%. The bias in representation brings the neglect of minority
classes. By contrast, Apart obtains a holistic improvement on different class sets.
Visualizationsoftheassignerweights:Inthissection,weshowtheweightlearned
by the assigner in Figure 3, i.e., Eq. 5. It reveals the relationship between learned
weight w(x,y) and class frequency N(y) (mentioned in Eq. 4) for each instance.
13Table 5 GroupAccuracyonshuffledCIFAR100B50-5.
Apart presentsaholisticexcellenceondifferentclasses.
Method Overall Many Median Few
Finetune 48.51 63.80 51.66 27.00
LwF 48.62 75.11 47.77 18.70
LUCIR 76.08 81.34 74.00 72.37
LUCIR+LWS 75.90 81.51 74.46 71.03
SimpleCIL 66.53 68.14 64.34 67.20
ADAM 72.96 78.37 73.54 65.97
L2P 67.17 81.14 62.37 56.47
DualPrompt 63.08 78.71 58.29 50.43
CODA-Prompt 70.29 84.49 65.63 59.17
Apart 81.93 89.37 81.00 74.33
‚ÄòInstance-wise‚Äô reports the weight of one instance, and ‚ÄòFrequency-wise‚Äô reports the
average weights with the same class frequency. From the figure, different classes yield
diverse weights, revealing that the learned routing encodes task-specific information
in a data-driven manner. Besides, for classes with more instances, the learned weights
exhibit an overall decreasing trend. The result implies the adaptive weights help the
auxiliary pool learn more from minority classes. Hence, it strengthens the correlation
between the auxiliary pool and minority classes and modifies the representation.
Table 6 FaircomparisononImageNet-R.
Ordered B100-10 Shuffled B100-20
Acc Acc Acc Acc
T T
iCaRL-352 68.95 57.22 70.24 58.52
iCaRL-2000 77.43 70.08 78.11 73.12
Apart 78.92 74.03 80.16 77.03
Fair comparison:Apart savesnoexemplarsbutneedstuningparameters.Sincethe
memory consists of parameters and exemplars, we conduct a fair comparison given
the same memory budget, as in Zhou et al. (2023). One rehearsal-based method re-
implemented with the same backbone ViTB/16-IN1K, iCaRL Rebuffi et al. (2017),
iscompared.Evenbasedonexemplars,iCaRLimplementedwithrandomlyinitialized
ResNetisweaker.Toalignthememoryspace,wecalculatethecorrespondingexemplar
memoryforiCaRL.ThememoryofApartconsistsofthreeparts:onefrozenbackbone
toget[CLS]token,onebackbonetobefinetuned,andmethod-relatedparameters.For
iCaRL,thememoryconsistsofthreeparts:oneoldbackboneforknowledgedistillation,
onebackbonetotrain,andtheexemplarmemory.Thememoryoftheexemplarshould
be consistent with that of method-related parameters, including pools, embeddings,
and classifiers. Saving an ImageNet-R image costs 3 √ó 224 √ó 224 integer numbers
(int), while Apart costs 12,231,953 method-related parameters (float). To align the
memorybudget,iCaRLneedstosave12,231,953floats√ó4bytes/float√∑(3√ó224√ó224)
bytes/image‚âà352instancesforImageNet-R.ThecomparisoninTable6showsApart
achieves a better performance given the same memory space as iCaRL-352, which
stores 352 exemplars. Even with more exemplars up to 2000, iCaRL cannot beat
Apart. Exemplars cost more, according to the results.
14    
    
         
    
    
           , Q V W D Q F H  Z L V H
 ) U H T X H Q F \  Z L V H
                       
 & O D V V  ) U H T X H Q F \  & O D V V  ) U H T X H Q F \
(a) CIFARB50-5 (b) ObjectNetB100-10
Fig.3 AdaptivelylearnedweightsshowingintheviewofinstanceandfrequencyinshuffledLTCIL.
Weshowthelegendsin(b).Weightsshowadiversityamongclassesandadecreasewiththeincrease
infrequency.
6 Conclusion
Inourdynamicworld,dataoftencomesinanimbalancedstreamingmanner,requiring
themodeltotacklelong-tailedclass-incrementallearning.ThispaperproposesApart
for LTCIL, which learns adapter pools for pre-trained models to overcome forgetting
viainstance-specificselection.Tocompensateforthetailclasses,welearnanauxiliary
adapter pool for a unified feature representation. Furthermore, we design an adap-
tive adapter routing strategy to automatically select the proper pool to use, in order
to trace the long-tailed distribution in a data-driven manner. Extensive experiments
verify Apart‚Äôs excellent performance.
Acknowledgment
This work is partially supported by National Science and Technology Major
Project (2022ZD0114805), Fundamental Research Funds for the Central Universi-
ties (2024300373), NSFC (62376118, 62006112, 62250069, 61921006), Collaborative
Innovation Center of Novel Software Technology and Industrialization.
Appendix A Implementation Details
In this section, we discuss the compared methods and details about the benchmark
datasets.
A.1 Compared Methods
The compared methods in the main paper are as follows:
‚Ä¢ Finetune: trains the model directly with new datasets without addressing catas-
trophic forgetting;
‚Ä¢ LwF: Li and Hoiem (2017) utilizes knowledge distillation to transfer knowledge
from the old frozen model to the new model while finetuning new tasks;
15
 W K J L H :  W K J L H :‚Ä¢ LUCIR:Houetal.(2019b)isanexemplar-basedmethod,whichlearnsnormalized
cosine classifiers distinct from the old ones as an improvement of LwF;
‚Ä¢ LWS: Liu et al. (2022) is a state-of-the-art exemplar-based LTCIL method. It re-
trainsthelinear classifierwithbalanceddata sampledfromreserved exemplarsand
new task data in a two-stage framework. It can be combined with other exemplar-
based methods;
‚Ä¢ L2P: Wang et al. (2022) is a state-of-the-art prompt-based CIL method. It freezes
the pre-trained model and adds prompts to adapt to new tasks. A prompt pool is
built in ‚Äúkey-value‚Äù pairs. When updating, the most suitable prompts are retrieved
from the pool by matching the instance and the keys;
‚Ä¢ DualPrompt: Wang et al. (2022) is a state-of-the-art prompt-based CIL method.
ItextendstheprependedpromptsinL2Ptopromptsinsertedintoeachlayer,called
general prompts and expert prompts. The former learns knowledge across tasks,
and the latter follows the retrieval strategy to learn task-specific knowledge;
‚Ä¢ CODA-Prompt: Seale Smith et al. (2022) is a state-of-the-art prompt-based CIL
method. It decomposes the prompts in a weighted-sum format and introduces a
learnable attention mechanism to prompt matching;
‚Ä¢ SimpleCIL:Zhouetal.(2024)isastate-of-the-artPTM-basedCILmethod.Itsets
the prototype features extracted from the frozen PTMs as the classifiers without
extra training on downstream tasks;
‚Ä¢ ADAM:Zhouetal.(2024)isastate-of-the-artPTM-basedCILmethod.Itadapts
to downstream tasks by efficiently tuning on the first task and merges with the
origin frozen model by extracting concatenated prototype classifiers.
All these methods are implemented with the same backbone, ViT-B/16-IN1K.
A.2 Datasets
       
       
   
             2 U L J L Q    
     / R Q J  W D L O H G        
          
  
   
                                                   
 & O D V V  & O D V V  & O D V V
(a) CIFAR (b) ImageNet-R (c) ObjectNet
Fig.4 Origindistributionandlong-taileddistributionaftersamplingforeachdataset.Weshowthe
legendsin(a).‚ÄòOrigin‚Äôand‚ÄòLong-tailed‚Äôdenotethedistributionbeforeandaftersamplingseparately.
Following Zhou et al. (2024), we select three datasets for comparison based on
pre-trained models. To simulate a long-tailed distribution, we sample from the origin
dataset in an exponential decay parameterized by œÅ Liu et al. (2022). œÅ is the ratio
between the number of the least frequent class and that of the most frequent class,
i.e., œÅ= Nmin. The detailed introduction is below.
Nmax
16
 U H E P X 1  U H E P X 1  U H E P X 1‚Ä¢ CIFAR100: Krizhevsky et al. (2009) contains 60,000 images for 100 classes, of
which 50,000 are training instances and 10,000 are testing ones with a uniform
distribution. We sample it with œÅ = 0.01 and N = 500;
max
‚Ä¢ ImageNet-R:Hendrycksetal.(2021)isintroducedintoCILby Wangetal.(2022).
It contains 30,000 pictures of different styles, of which 24,000 are training instances
and 6,000 are testing ones. Since it follows a long-tailed distribution with œÅ = 0.11
and N = 349 for 200 classes, we experiment on it without extra processing;
max
‚Ä¢ ObjectNet: Barbu et al. (2019) is introduced into CIL by Zhou et al. (2024).
It contains pictures with controlled variations. A subset of 200 classes with about
32,000 instances is selected for evaluation, of which 26,509 are training instances
and 6,628 are testing ones. We sample it with œÅ = 0.01 and N = 200.
max
As we sample from the datasets to simulate long-tailed distribution, we provide
the distribution in Figure 4.
Appendix B Extra Experimental Evaluations
In this section, we analyze more hyperparameters of Apart, besides the number of
pools in the main paper.
B.1 Influence of Pool Size
WeshowtheresultwithdifferentpoolsizesinTable 6.PoolsizeM meansthenumber
of options in a pool. It reveals that the pool size has an influence on incremental
performance.Arelativelylowaccuracypresentswhenwestartfrom3optionsinapool,
possiblylessthanthecapacityofthemodelneededforalongsequenceoftasks.When
a pool has more adapters, an increase occurs, followed by a decrease. The increase is
forthepropercapacity,whiletheinsufficientlearningofeachelementinthepoolmay
cause the decline. Thus, we set M to 5 as default.
Table 6 Incrementalperformancewith
differentpoolsizesonshuffledCIFAR100
B50-5.
M 3 5 7 10
Acc 83.61 84.91 83.54 83.57
AccT 79.92 81.93 79.87 80.17
B.2 Influence of Weight Scale
We explore the influence of the parameter Œ± in L (¬∑,¬∑) which controls the scale of
2
adaptive weights. The choice of Œ± has a significant impact on the performance. The
result in Table 7 shows that the performance improves as Œ± gets larger initially. A
smallweightdirectlycausesinsufficientlearningoftheauxiliarypoolcomparedtothe
other pool. Then, the ensemble of predictions is biased when without constraints on
it. Meanwhile, the small learned weight makes little gap in routing between majority
classesandminorityclasses,resultingininsufficientlearningofminorityclasses.When
17Œ± gets larger, a modest decrease occurs, for narrowing the gap similarly. Thus, we set
Œ± to 1 as default.
Table 7 IncrementalperformancewithdifferentŒ±on
shuffledCIFAR100B50-5.
Œ± 0.1 0.3 0.5 0.7 1.0 3.0
Acc 52.58 69.74 76.35 84.02 84.91 84.40
AccT 52.61 72.28 74.57 80.84 81.93 81.51
References
Barbu, A., D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum,
and B. Katz. 2019. Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models. NeurIPS 32 .
Castro,F.M.,M.J.Mar¬¥ƒ±n-Jim¬¥enez,N.Guil,C.Schmid,andK.Alahari2018. End-to-
end incremental learning. In ECCV, pp. 233‚Äì248.
Chamikara,M.A.P.,P.Bert¬¥ok,D.Liu,S.Camtepe,andI.Khalil.2018. Efficientdata
perturbation for privacy preserving and accurate data stream mining. Pervasive
and Mobile Computing 48: 1‚Äì19 .
Chawla,N.V.,K.W.Bowyer,L.O.Hall,andW.P.Kegelmeyer.2018,Jul. Smote:Syn-
theticminorityover-samplingtechnique. JournalofArtificialIntelligenceResearch:
321‚Äì357. https://doi.org/10.1613/jair.953 .
Chen, S., G. Chongjian, Z. Tong, J. Wang, Y. Song, J. Wang, and P. Luo 2022.
Adaptformer: Adapting vision transformers for scalable visual recognition. In
NeurIPS.
Cui, Y., M. Jia, T.Y. Lin, Y. Song, and S. Belongie 2019, Jun. Class-balanced loss
based on effective number of samples. In CVPR.
Dosovitskiy, A., L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. 2020. An image is worth
16x16 words: Transformers for image recognition at scale. In ICLR.
Douillard, A., M. Cord, C. Ollion, T. Robert, and E. Valle 2020. Podnet: Pooled
outputs distillation for small-tasks incremental learning. In ECCV, pp. 86‚Äì102.
Springer.
French, R.M. 1999. Catastrophic forgetting in connectionist networks. Trends in
cognitive sciences 3(4): 128‚Äì135 .
Han,X.,Z.Zhang,N.Ding,Y.Gu,X.Liu,Y.Huo,J.Qiu,Y.Yao,A.Zhang,L.Zhang,
et al. 2021. Pre-trained models: Past, present and future. AI Open 2: 225‚Äì250 .
Hendrycks,D.,S.Basart,N.Mu,S.Kadavath,F.Wang,E.Dorundo,R.Desai,T.Zhu,
S. Parajuli, M. Guo, et al. 2021. The many faces of robustness: A critical analysis
of out-of-distribution generalization. In ICCV, pp. 8340‚Äì8349.
Hinton, G., O. Vinyals, and J. Dean. 2015. Distilling the knowledge in a neural
network. arXiv preprint arXiv:1503.02531 .
Hou, S., X. Pan, C.C. Loy, Z. Wang, and D. Lin 2019a. Learning a unified classifier
incrementally via rebalancing. In CVPR, pp. 831‚Äì839.
18Hou, S., X. Pan, C.C. Loy, Z. Wang, and D. Lin 2019b. Learning a unified classifier
incrementally via rebalancing. In CVPR, pp. 831‚Äì839.
Huang, C., Y. Li, C.C. Loy, and X. Tang 2016. Learning deep representation for
imbalanced classification. In CVPR, pp. 5375‚Äì5384.
Jia, M., L. Tang, B. Chen, C. Cardie, S.J. Belongie, B. Hariharan, and S. Lim 2022.
Visual prompt tuning. In ECCV, pp. 709‚Äì727. Springer.
Kalla, J. and S. Biswas 2024, January. Robust feature learning and global variance-
driven classifier alignment for long-tail class incremental learning. In WACV, pp.
32‚Äì41.
Kang, B., S. Xie, M. Rohrbach, Z. Yan, A. Gordo, J. Feng, and Y. Kalantidis 2020.
Decoupling representation and classifier for long-tailed recognition. In ICLR.
Krempl,G.,I.ZÀáliobaite,D.Brzezin¬¥ski,E.Hu¬®llermeier,M.Last,V.Lemaire,T.Noack,
A. Shaker, S. Sievi, M. Spiliopoulou, et al. 2014. Open challenges for data stream
mining research. KDD 16(1): 1‚Äì10 .
Krizhevsky, A., G. Hinton, et al. 2009. Learning multiple layers of features from tiny
images. Technical report.
Li, Z. and D. Hoiem. 2017. Learning without forgetting. TPAMI 40(12): 2935‚Äì2947 .
Lin,T.Y.,P.Goyal,R.Girshick,K.He,andP.Doll¬¥ar2017. Focallossfordenseobject
detection. In ICCV, pp. 2999‚Äì3007.
Liu,X.,Y.S.Hu,X.S.Cao,A.D.Bagdanov,K.Li,andM.M.Cheng2022. Long-tailed
class incremental learning. In ECCV, pp. 495‚Äì512. Springer.
Liu,Z.,Z.Miao,X.Zhan,J.Wang,B.Gong,andS.X.Yu2019. Large-scalelong-tailed
recognition in an open world. In CVPR.
Rebuffi, S.A., A. Kolesnikov, G. Sperl, and C.H. Lampert 2017. icarl: Incremental
classifier and representation learning. In CVPR, pp. 2001‚Äì2010.
Rosenberg, C., M. Hebert, and H. Schneiderman. 2005. Semi-supervised self-training
of object detection models .
Russakovsky, O., J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpa-
thy, A. Khosla, M. Bernstein, et al. 2015. Imagenet large scale visual recognition
challenge. IJCV 115(3): 211‚Äì252 .
Seale Smith, J., L. Karlinsky, V. Gutta, P. Cascante-Bonilla, D. Kim, A. Arbelle,
R. Panda, R. Feris, and Z. Kira. 2022. Coda-prompt: Continual decomposed
attention-based prompting for rehearsal-free continual learning. arXiv e-prints:
arXiv‚Äì2211 .
Shi, J.X., T. Wei, Z. Zhou, J.J. Shao, X.Y. Han, and Y.F. Li 2024. Long-tail learning
with foundation model: Heavy fine-tuning hurts. In ICML, pp. 45014‚Äì45039.
Wang, F.Y., D.W. Zhou, H.J. Ye, and D.C. Zhan 2022. Foster: Feature boosting and
compression for class-incremental learning. In ECCV, pp. 398‚Äì414.
Wang, Q.W., D.W. Zhou, Y.K. Zhang, D.C. Zhan, and H.J. Ye 2023. Few-shot class-
incrementallearningviatraining-freeprototypecalibration. InNeurIPS,pp.15060‚Äì
15076.
Wang,Y.X.,D.Ramanan,andM.Hebert.2017. Learningtomodelthetail. NIPS 30.
Wang,Z.,Z.Zhang,S.Ebrahimi,R.Sun,H.Zhang,C.Y.Lee,X.Ren,G.Su,V.Perot,
J. Dy, et al. 2022. Dualprompt: Complementary prompting for rehearsal-free
continual learning. arXiv preprint arXiv:2204.04799 .
19Wang, Z., Z. Zhang, C.Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot, J. Dy, and
T. Pfister 2022. Learning to prompt for continual learning. In CVPR, pp. 139‚Äì149.
Wei, T., J.X. Shi, W.W. Tu, and Y.F. Li. 2021. Robust long-tailed learning under
label noise. arXiv preprint arXiv:2108.11569 .
Yan,S.,J.Xie,andX.He2021. Der:Dynamicallyexpandablerepresentationforclass
incremental learning. In CVPR, pp. 3014‚Äì3023.
Ye,H.J.,D.W.Zhou,L.Hong,Z.Li,X.S.Wei,andD.C.Zhan.2024. Contextualizing
meta-learning via learning to decompose. IEEE Transactions on Pattern Analysis
and Machine Intelligence 46(1): 117‚Äì133 .
Zhang, Y., B. Kang, B. Hooi, S. Yan, and J. Feng. 2023. Deep long-tailed learning: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence .
Zheng, B., D.W. Zhou, H.J. Ye, and D.C. Zhan 2024. Multi-layer rehearsal feature
augmentation for class-incremental learning. In ICML.
Zhou,B.,Q.Cui,X.S.Wei,andZ.M.Chen2020. Bbn:Bilateral-branchnetworkwith
cumulative learning for long-tailed visual recognition. In CVPR, pp. 9719‚Äì9728.
Zhou, D.W., Z.W. Cai, H.J. Ye, D.C. Zhan, and Z. Liu. 2024. Revisiting class-
incremental learning with pre-trained models: Generalizability and adaptivity are
all you need. International Journal of Computer Vision: 1‚Äì21 .
Zhou,D.W.,H.L.Sun,J.Ning,H.J.Ye,andD.C.Zhan2024. Continuallearningwith
pre-trained models: A survey. In IJCAI, pp. 8363‚Äì8371.
Zhou, D.W., H.L. Sun, H.J. Ye, and D.C. Zhan 2024. Expandable subspace ensemble
forpre-trainedmodel-basedclass-incrementallearning. InCVPR,pp.23554‚Äì23564.
Zhou, D.W., Q.W. Wang, Z.H. Qi, H.J. Ye, D.C. Zhan, and Z. Liu. 2024. Class-
incremental learning: A survey. IEEE Transactions on Pattern Analysis and
Machine Intelligence .
Zhou, D.W., Q.W. Wang, H.J. Ye, and D.C. Zhan 2023. A model or 603 exemplars:
Towards memory-efficient class-incremental learning. In ICLR.
Zhu, K., Y. Cao, W. Zhai, J. Cheng, and Z.J. Zha 2021. Self-promoted prototype
refinement for few-shot class-incremental learning. In CVPR, pp. 6801‚Äì6810.
20