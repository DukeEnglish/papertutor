Towards Fairer Health Recommendations: finding informative
unbiased samples via Word Sense Disambiguation
GavinButtsâˆ— PegahEmdadâˆ— JethroLeeâˆ—
LoyolaMarymountUniversity WorcesterPolytechnicInstitute NortheasternUniversity
gbutts@lion.lmu.edu pemdad@wpi.edu lee.jet@northeastern.edu
ShannonSong ChimanSalavati WillmarSosaDiaz
WorcesterPolytechnicInstitute UniversityofConnecticut UniversityofConnecticut
smsong@wpi.edu chiman.salavati@uconn.edu willmar.sosa_diaz@uconn.edu
ShiriDori-Hacohen FabricioMurai
UniversityofConnecticut WorcesterPolytechnicInstitute
shiridh@uconn.edu fmurai@wpi.edu
ABSTRACT 1 INTRODUCTION
Therehavebeengrowingconcernsaroundhigh-stakeapplications Fordecades,medicinehasbeenmarredbyimplicitandexplicit
thatrelyonmodelstrainedwithbiaseddata,whichconsequently biasesthatcontinuetonegativelyimpactpatientoutcomesbyper-
producebiasedpredictions,oftenharmingthemostvulnerable.In petuatingstereotypesandcontributingtohealthdisparitiesamong
particular,biasedmedicaldatacouldcausehealth-relatedapplica- socialgroupsthatfacesystemicoppression[8,9].Despiteeffortsto
tionsandrecommendersystemstocreateoutputsthatjeopardize remediateandaddressthesebiasesfromtheirsource,manymed-
patientcareandwidendisparitiesinhealthoutcomes.Arecent icalschoolsstillincorporatebiasedmedicalteachingsduringthe
frameworktitledFairnessviaAI positsthat,insteadofattempting preclinicalyears[12,28].Manyeducatorscontinuetomisuserace
tocorrectmodelbiases,researchersmustfocusontheirrootcauses asasubstituteforgeneticsorancestry,ortheyusegenderandsex
byusingAItodebiasdata.Inspiredbythisframework,wetackle termsincorrectlyreinforcingthenotionthatsexandgenderare
biasdetectioninmedicalcurriculausingNLPmodels,including binaryorfixedratherthanfluid,whichcanpotentiallyalienate
LLMs,andevaluatethemonagoldstandarddatasetcontaining gender-nonconformingstudentsandpatients[1,14,15].Thecur-
4,105excerptsannotatedbymedicalexpertsforbiasfromalarge rentfocusinAIresearchisprimarilyonidentifyingandexposing
corpus.Webuildonpreviousworkbycoauthorswhichaugments biaswithinAIsystems,oftenwithoutaddressingtherootcauses
thesetofnegativesampleswithnon-annotatedtextcontainingso- ofbiasinherentinthedatathesesystemsarebuiltupon.Aslong
cialidentifierterms.However,someoftheseterms,especiallythose asstructuralinequalitiesexistintherealworld,AIsystemswill
relatedtoraceandethnicity,cancarrydifferentmeanings(e.g., perpetuatethesebiases[10].Byharnessingmachinelearningto
â€œwhitematterofspinalcordâ€).Toaddressthisissue,weproposethe analyzeanddetectthesebiases,wecanadvanceequityinmedical
useofWordSenseDisambiguationmodelstorefinedatasetqual- trainingandthefairnessofAImodels,leadingtoamoreaccurate
itybyremovingirrelevantsentences.Wethenevaluatefine-tuned andeffectivehealthcaresystem.
variationsofBERTmodelsaswellasGPTmodelswithzero-and Recently,Salavatietal.[25]introducedtheBRICC(BiasReduc-
few-shotprompting.WefoundLLMs,consideredSOTAonmany tioninCurricularContent)datasetandproposedasystematicand
NLPtasks,unsuitableforbiasdetection,whilefine-tunedBERT scalableAI-basedmethodforidentifyingpotentialbiasinmedical
modelsgenerallyperformwellacrossallevaluatedmetrics. curricula.Giventhesteepcostoffalsenegatives(i.e.,classifying
abiasedsentenceasunbiased),theyemphasizethatrecallmust
KEYWORDS beprioritizedoverprecision.Moreover,duetotheinherentdiffi-
cultyofthetask,oneofthebestapproachesinthisHigh-Recall
medicaltextdata,biasdetection,LLMs,wordsensedisambiguation
InformationRetrievalsettingistheTechnologyAssistedReview
ACMReferenceFormat: (TAR)[7,18],wherebyasetofexpertsreviewsthesamplesflagged
GavinButts,PegahEmdad,JethroLee,ShannonSong,ChimanSalavati, byamodel,asenvisionedbySalavatietal.Thepaperalsouseda
WillmarSosaDiaz,ShiriDori-Hacohen,andFabricioMurai.2024.Towards curatedlistofsocialidentifierstofindadditional,negative(i.e.,non-
FairerHealthRecommendations:findinginformativeunbiasedsamplesvia
biased)samplesinunlabeleddata.However,thislatterapproach
WordSenseDisambiguation.InProceedingsofMakesuretoenterthecorrect
sufferedfromsocialidentifiertermsthathadambiguousmeanings,
conferencetitlefromyourrightsconfirmationemai(FAccTRecâ€™24).ACM,New
leadingtolowerqualityofthetrainingdata,andnegativesamples
York,NY,USA,7pages.https://doi.org/XXXXXXX.XXXXXXX
thatweretooâ€œeasyâ€toclassifyasnon-biased.Forexample,one
socialidentifierusedtofilterforrace-relateddatawasâ€œwhiteâ€.In
âˆ—Equalcontribution.
Table1,simplysearchingforthekeywordâ€œwhiteâ€willincludeboth
race-relatedandnon-race-relatedtextexcerpts.
FAccTRecâ€™24,October14â€“18,2024,Bari,Italy Webelievethatusingtheseambiguousmeaningtermsas-isleads
2024.ACMISBN978-1-4503-XXXX-X/18/06...$15.00
tooverestimatingthetruediscernmentpowerofthebiasclassifier
https://doi.org/XXXXXXX.XXXXXXX
4202
peS
11
]LC.sc[
1v42470.9042:viXraFAccTRecâ€™24,October14â€“18,2024,Bari,Italy Buttsetal.
Table1:Thetermâ€œwhiteâ€inaracialvs.non-racialcontext Khanetal.[16]manuallyexploredthesystemicbiasheldbymed-
icalprofessionalswhenwritingrecommendationletters.Onthe
Race-Related NotRace-Related otherhand,Razaetal.[22]andSalavatietal.[25]aimedtodetect
â€œ5YearRelativeSurvival:over- â€œWhitematterwithinthespinal biasinmedicaltextusingtransformer-basedlanguagemodels.The
all84%forwhitewomen,62% cordcontainstheaxonsofneu- formerusedasemi-autonomouslylabeleddatasetcoveringdiverse
for black women, 95% for lo- ronsthatareascendingandde- medicaltopics,whereasthelatteremployedadatasetmanuallyla-
caldisease,69%regionaldisease scendingtotransmitsignalsto
beledbymedicalexpertsfocusingonbiasedinformationinmedical
(spreadtolymphnode),17%for andfromthebrain,respectively.â€
curriculartexts.Althoughbothstudiesprovideacomprehensive
distantdisease.â€
overviewofbiasdetection,ensuringhigh-qualitydataremainsan
bymakingtheproblemtooeasy.Itmaybethattheclassifieris issue.WhilewealsoexploreAImodelsfordebiasingmedicaltext
actuallydifferentiatingrace-relatedfromnon-race-relatedterms, data,weinvestigatebetterwaysofaugmentingthesetofunbiased
ratherthanbiasedfromnon-biasedsentences. samplesandconsiderawidergamutofmodels,includingLLMs.
Forthisreason,weproposeanewframeworktoaugmentthe
samplingprocessfornegativeexamples,usingWordSenseDisam- MachineLearningforBiasDetection. Priorworkshaveappliedvar-
biguation(WSD)methodsfordataenhancement.Wehypothesize iousBERTmodelsforbiasclassificationtasks.Tidermanetal.[26]
thatthiswouldimprovethedistinctionbetweenbiasedandnon- usedDistilBERT,atransformer-baseddistilledBERTmodel,toclas-
biasedsentencesinthebiasclassificationprocess. sifybiasedinformationinsocialmediacontent.Similarly,Razaet
Ourmaincontributionsareasfollows: al.[22]achievedthebestresultsforbiasclassificationinmedical
(1) Weenhanceaframeworkfordetectingbiasinmedicalcur- textthroughfine-tuningBERT,asimpleencoder-onlytransformer.
riculumcontent,withafocusonimprovingdataquality. Buildingontheexistingliteratureforbiasdetectioninmedical
(2) WeleverageWordSenseDisambiguation(WSD)modelsin contexts,weadditionallyapplyLargeLanguageModels(LLMs)for
trainingbiasdetectionclassifiersbyfilteringoutirrelevant thistask.Specifically,weuseTinyLlama[31],acomputationally
samplesfromthedata.Moreover,weuseChatGPT-4oto efficientvariantofLlama2,forbiasclassification.Inaddition,we
augmentasetofmanuallylabeledexampleswithsynthetic consideradditionalstrategiesforconstructingthesetofnegative
samplestofine-tuneand/orevaluateWSDmodels. samplesâ€“suchasthroughtheuseofWSDfordatarefinement.
(3) Wefine-tuneandevaluatevariousTransformer-basedmod-
elsincludingDistilBERT,RoBERTa,andBioBERTforthebias UseofLLMsforNLPtasksandpromptengineering. InNLPtasks,
detectiontask.Inaddition,weuseLargeLanguageModels promptingLLMshavebeenshowntoperformonparwithencoder-
(LLMs),suchasTinyLlama,forbiasclassification,evaluating onlyarchitectures,likeBERT,withouttheneedforfine-tuning[6].
zero-shotvs.few-shotpromptingwithGPT,toimprovethe Ithasbeenshownthatpromptingtechniques,suchaszero-shot,
performanceofbiasdetection. few-shot,orchainofthought(CoT),serveakeyroleinthequality
(4) Wepresentacomprehensiveevaluationofthevariousmod- andcorrectnessofamodelâ€™soutput[19].Thesetechniqueshave
els,highlightingtheimprovementsachievedthroughtheuse beenusedinmanytasks,suchassentimentanalysis[3],textclassi-
ofWSDandChatGPT-generatedsentences. fication[5],aswellasforhealthcareapplications,suchasquestion-
answering,andasaclinicalrecommendersystem[20,29].Despite
2 RELATEDWORK theseinitiatives,wearethefirsttoevaluatezero-andfew-shot
HealthRecommenderSystems(HRS). Recommendersystemshave promptingfordetectingbiasinmedicalcurricularcontent.
becomeintegraltothehealthcareindustry,providingpersonalized
medicalrecommendationsthatenhancepatientunderstandingof
3 DATASET
theirmedicalconditionandimprovehealthoutcomes[27].These
OurworkbuildsontheBRICCdatasetintroducedbySalavatiet
systemsassisthealthcareprofessionalsinpredictingandtreating
al.[25],whichconsistsof509PDFfilesand12,647pagesofmedical
diseasesbyanalyzingpatientdatatorecommendpersonalizeddiets,
schoolinstructionalmaterialsannotatedbymedicalstudentsand
exerciseregimens,medications,diagnoses,andotherhealthser-
expertstrainedinidentifyingbias.Withinthedataset,thereare
vices[21,24].Despitenumerousstudiesexploringvariousaspects
threetiersofcoding.Thefirst-levelcodesidentifysocialidentifiers
ofHRS,theliteratureforaddressingvarioustypesofbiasesinsuch
within the excerpt. The second-level codes assess the presence
systemsrootedincurriculacontentsislimited[25].
or absence of bias in the excerpt, categorized into four distinct
DebiasingmedicalcorporamanuallyandviaAI. Concernsoverbi- groups:â€˜biasedâ€™,â€˜potentiallybiasedâ€™,â€˜non-biasedâ€™,andâ€˜reviewâ€™.
asedAImodelsand,particularly,recommendersystemsinhealth- Additionally,third-levelcodesestablishalinkbetweenamedical
careapplicationshavebeengainingmoreattentionduetotheir conditionandoneormorecategoriesofsocialidentifiers(e.g.,race),
increaseduseinhigh-stakedecisions[4].Inessence,theirbiases specifyingthetypeofidentityandwhetheritwasportrayedina
are rooted in implicit and explicit biases embedded in the data biasedorunbiasedmanner.Eachexcerptisthenassignedoneor
usedfortrainingthem[13],whichstemfromvarioussources,in- morecodesformattedasâ€œTYPE-diseaseâ€,whereTYPErepresents
cludinginherentbiasesinmedicalliterature,thesubjectivityof oneof17categoriesofsocialidentifiers.Akintothepreviouswork,
humanannotators,andhistoricalandsystemicinequitiespresent wefocusonthemostfrequenttypesincludingsex,gender,race,
inhealthcaresystems[25].Numerousrecentstudieshaveaimed ethnicity,age,andgeography.Eachcategoryisassociatedwitha
toquantifyandaddressthisissuebothmanuallyandthroughAI. listofkeywordsthatcansignifysocialidentifiers.TowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRecâ€™24,October14â€“18,2024,Bari,Italy
Table2:BRICCDatasetCharacteristics template-basedpromptingusingchainofthoughtreasoning,and
highlighteditshighperformance.
Counts
NumberofPDFFiles 509
TotalNumberofPages 12,647
4.2 WordSenseDisambiguationTaskDefinition
AnnotatedExcerpts 4,105
Generallyspeaking,wordsensedisambiguation(WSD)isthetask
LabeledPositives 1,116
ofidentifyingthecorrectsenseofapolysemousğ‘¤ âˆˆW(aword
LabeledNegatives(LN) 2,989
withmultiplemeanings)inagivencontextğ‘¥.Formally,givenaset
ExtractedNegatives(XN) 4,391
ofwordsW,afinitesetofpossiblesensesSğ‘¤ ={ğ‘† ğ‘¤(1),...,ğ‘† ğ‘¤(ğ‘˜) }
Positive and negative samples. Positive samples are defined as foreachğ‘¤ âˆˆ W andacontext(orderedsequenceofwords)ğ‘¥ =
thoseexcerptsthatcontaineitheraâ€˜biasedâ€™,â€˜potentiallybiasedâ€™, (ğ‘¥1,...,ğ‘¥ ğ‘–âˆ’1,ğ‘¤,ğ‘¥ ğ‘–+1,...ğ‘¥ ğ‘›) âˆˆX,findafunctionğ‘“ :WÃ—Xâ†’S,
orâ€˜reviewâ€™andaselectedâ€œTYPE-diseaseâ€.Negativesamplesare suchthatğ‘“(ğ‘¤,ğ‘¥)isthecorrectsenseofğ‘¤ incontextğ‘¥.
subdividedintovarioustypes,asdetailedinthepreviouswork[25]. For this paper, we are interested in determining if a
The negative types that we are most interested in are referred term ğ‘¤, listed as a possible social identifier for category
toasextractednegatives(XN).Inthiscase,thesearesentences ğ‘¡, is related to ğ‘¡ in an excerpt ğ‘¥. To do so, we need to
fromthecorpusthat,despitecontainingacategorykeyword,were learn a function IsRelated(ğ‘¤,ğ‘¥,ğ‘¡) âˆˆ {true,false}. For in-
deliberatelyexcludedfromtheannotationprocess.Pleasereferto stance, the set of social identifiers for race is Srace =
supplementalmaterialsforadetailedexplanationofnegativetypes {â€˜whiteâ€™,â€˜blackâ€™,...}.Ideally,inoneoftheexamplesseenearlier,we
foundintheBRICCdataset.OftheXNtypes,wefilteredforthose wantIsRelated(â€˜whiteâ€™,â€˜whitematterwithin...â€™,â€˜raceâ€™)=false.
thatcontainedatleastonerelevantkeywordrelatingtoourselected
â€œTYPE-diseaseâ€.Thedistributionofpositivesandnegativesacross
thedatasetisdisplayedinTable2. 4.3 BiasDetectionTaskDefinition
WefocusonXNsamplesinourexperimentsbecausetheauthors Inthecontextofmedicaleducation,weconsiderbiasdetectionasa
previouslyreportedhigherrecall,0.925,butattheexpenseofpreci- HighRecallInformationRetrievaltask.ItisthefirststepinaTAR
sion,0.504,byusingthisnegativeset[25].Despitethisimproved system.Thistaskconsistsofclassifyingatextexcerptğ‘¥asunbiased
performance,wenoticedthatmanyofthekeywordsmayleadto (ğ‘¦Ë†=0)orpotentiallybiased(ğ‘¦Ë†=1).Inthelattercase,thesample
theinclusionofnon-â€œTYPE-diseaseâ€relatedcontent.Anexample wouldbesubsequentlyreviewedbyamedicalexpert.
ofthisoccurrencemaybeseeninTable1.Hence,retainingonly Biasmayberelatedtooneormorecategoriesofsocialiden-
negativesamplesthatrelatetothesocialdemographicsofinterest tifiers,includingrace,ethnicity,sex,gender,age,andgeography.
isakeycomputationaltask,whichweaddressusingWSD. Forinstance,â€œTheypromotehairgrowthinthegroin,axilla,chest
andface,yettheyalsopromotehairlossinthescalpinmenwho
LabelingdataforWSD. TogathersamplessuitablefortrainingWSD aregeneticallysusceptibletoandrogeneticalopecia.â€islabeledby
models,weselectedthoseXNexcerptsthatcontainedakeyword medicalexcerptsasâ€˜biasedâ€™withrespecttogender (designated
relatedtotheselectedsocialdemographicscategories:sex,gen- bythesocialidentifiermen).Asexplained,inthecommentfrom
der,race,ethnicity,geography,andage.Afterobtainingarandom oneoftheannotators:â€œUsesextermswhenspeakingofpopulations,
sampleofXNexcerptsforeachcategory,wehadahumanexpert
shouldbemaleinsteadofmen.Also,includecitationtosupportthis
annotatewhetherthemeaningofthekeywordtermwasindeedre- assertion.â€
latedtothatcategoryornot.Basedontheresultsofthisannotation Formally,Salavatietal.[25]definetype-specificbiasasabinary
process,wedecidedtoonlyfocusonracekeywordsbecausethey labelbias(ğ‘¥,ğ‘¡) âˆˆ {true,false} indicatingwhetherexcerptğ‘¥ is
sufferedthemostfromambiguity.Theotherbiascategoriesdid biasedwithrespecttoasocialidentifiercategoryğ‘¡.Inthepresent
nothaveasignificantdegreeofambiguitythatrequiredcorrection. work,weconsideronlythegeneraldefinitionofbias,regardlessof
TheselabeledexcerptswereusedtotrainourWSDmodels. whichcategoryğ‘¡ inasetT itbelongsto:bias(x,T)=true â‡â‡’
âˆƒğ‘¡ âˆˆT s.t.bias(x,t)=true.
4 PRELIMINARIES
4.1 LLMPrompting 5 METHODOLOGY
Reynoldsetal.[23]suggestthatzero-shotpromptscouldsignifi- In this section, we provide an overview of the proposed frame-
cantlyoutperformfew-shotprompts.Theiranalysishighlightsthe work.Figure1(left)showsthedataprocessingstepsperformed
needtoconsidertheroleofpromptsincontrollingandevaluating bySalavatietal.[25],whichweleverageinthepresentwork.As
theperformanceoflanguagemodels.Theirstudystatedthatsince explained, in addition to labeled data, BRICC includes negative
GPT-3isoftennotlearningfromfew-shotexamplesduringthe samplesextractedfromthenon-annotateddata(denotedasXN).
runtime,thismodelcaneffectivelybepromptedwithoutexam- Figure1(center)illustratestheapplicationofWSDtofilterout
ples[23].Additionally,Kojimaetal.[17]demonstratethatchain irrelevantsamples,whichresultsinthefilteredXNset(XNâˆ—).This
ofthought(CoT)prompting,arecenttechniqueforelicitingcom- processisdescribedindetailinSection5.1.Last,Figure1(right)
plexmulti-stepreasoningthroughstep-by-stepanswerexamples, depictstheaugmentationoflabeleddatawithXNâˆ— fortraining
achieved state-of-the-art performances in arithmetics and sym- differentbiasclassifiers,whoseperformanceweevaluate.Details
bolicreasoningtasks.TheyproposedZero-shot-CoT,azero-shot arediscussedinSection5.2.FAccTRecâ€™24,October14â€“18,2024,Bari,Italy Buttsetal.
andweightdecayof0.01over10epochs,keepingthemodelthat
Data Processing WSD Bias Classification yieldedthesmallestvalidationloss.
Giventhesubstantialempiricalandtheoreticalevidencesup-
Corpus Extracted Labeled Labeled
Negatives Data XN portingthebenefitsofchainofthought(CoT)promptinginvarious
LLMtasks[11,30],weincorporateCoTintoourzero-shotprompts.
WeoptedtofollowtheprompttemplatepresentedinKojimaet
Annotators WSD Bias Classifier al.[17],whichwasshowntoproducethehighestaccuracy(i.e.,
â€œLetâ€™sthinkstepbystepâ€).Topromptthemodel,wefirstspecified
themodelâ€™srole:â€œYouareahelpfulassistantthatdeterminesifthe
sentenceisraceorethnicityrelatedâ€.Then,wedefinedthetaskas:
La Db ae tl aed NEx et gr aa tc it ve ed s Filtered XN Unb0 iased Bia1 sed â€œGiventhesentenceâ€˜textâ€™,thinkstepbystep:Isthissentenceraceor
â˜º â˜¹ ethnicityrelated?Onlyoutput1or0.Ifthissentencecontainsany
termsrelatingtoraceorethnicity,state1.Otherwise,state0.â€
Figure1:Workflowstages.(Left)Dataprocessing:annotated
Metrics. Weevaluatethemodelsâ€™performanceonthetestsetwith
excerptsarelabeledasâ€˜biasedâ€™(positive)orâ€˜non-biasedâ€™(neg-
respecttoaccuracy,precision,recallandF1score.
ative);XN:additionalsentencesextractedasnegativeexam-
ples.(Center)WordSenseDisambiguation(WSD)usedfor
5.2 BiasClassificationExperiments
selectingfromXNrelevantnegatives(XNâˆ—).(Right)Training
UsingtheBRICCdataset,wefine-tunebinaryclassificationneural
andevaluationofbiasclassifiers.
languagemodelsandpromptpre-trainedLLMsforbiasclassifica-
tion.Forfine-tuning,weconsiderencoder-onlyanddecoder-only
Data Collection Model Experimenting models(encoder-decodermodelsareoftenreservedformulti-modal
Annotators GPT-4o tasks and causal language inference [2]).The fine-tuned models
Training Dataset include RoBERTa, DistilBERT, BioBERT, and TinyLlama. Using
promptengineering,weadditionallypromptGPT-4omini.
Manual Synthetic TF- &ID F BERT GPT Differentsetsofnegatives. Thedatasetswewillconsidercontainall
Data Data Logistic positivesamples,plusoneofthefollowing:
Regression
â€¢ Labelednegatives(LN),
â€¢ Labelednegativesplusextractednegativesfilteredbykey-
Dataset Evaluation words(LN+XN),and
â€¢ Labeled negatives plus extracted negatives filtered using
wordsensedisambiguation(LN+XNâˆ—).
Figure2:WSDtrainingandevaluation.Excerptsmanually AsdescribedinSection3,thesetofextractednegatives(XN)is
labeledasrace-relatedornotplusGPT-generatedsentences constructedbyfilteringdatabasedonkeywordsthatrelatetoeither
areusedtotrainandevaluatetheWSDmodels. gender,sex,race,ethnicity,age,and/orgeography.Then,weapply
the best-performing WSD model to ensure these samples truly
5.1 WordSenseDisambiguationExperiments relatetothesocialdemographicsofinterest,resultinginXNâˆ—.To
WeevaluateseveralmodelsforWSD:asimplebaseline,twofine- assessperformancevariabilityasafunctionofthedatasplits,we
tunedvariantsofBERT,andtwoGPTmodels.Forfine-tuningand splitthedatasetinK-foldsforcross-validationandcalculateaverage
evaluation,wecombinedourmanualannotationswithsentences performanceandconfidenceintervals.
generatedbyChatGPT-4o,yielding352labeledexcerpts.
Fine-tuning. Toconstructthemodels,weaddaclassificationheadto
ExperimentalSetup. Wedivideeachofthetwodatasets(manually eachlanguagemodelandfullyfine-tuneeachmodelalongwiththe
annotatedexcerptsandsyntheticsamples)independentlywitha classificationhead.Themodelsweutilizeare:RoBERT,DistilBERT,
70-15-15stratifiedsplitintotraining,validation,andtestsets. andBioBERT,allencoder-only,andTinyLlama,a1.1Bdecoder-only
Weinvestigatedtwowaysofbuildingthetrainingset: modelderivedfromMetaâ€™sLlama2.
For each dataset we outlined, we do initial fine-tuning on
â€¢ Onlymanually-annotatedexcerpts; RoBERTa,DistlBERT,BioBERT,andTinyLlamawithabatchsizeof
â€¢ Bothmanually-annotatedexcerptsandsyntheticsamples. 8andalearningrateof2Ã—10âˆ’5.Weusethevalidationsettotune
thehyperparameterswithgridsearch,leadingtothefinalmodel.
WSDModels. Weevaluatethreemodelsshownintherecentlit-
eraturetoperformwellonWSD:ALBERT,GlossBERT,andGPT Prompting. WeevaluatetheperformanceofGPT-4ominibyusing
models.Thesemodelsarecomparedtoourbaseline,alogisticre- zero-andfew-shotprompting.Topromptthismodel,wefirstes-
gressionwithTF-IDF. tablishthemodelâ€™srole:â€œYouareahelpfulassistantthatdetermines
iftextisbiasedâ€.Then,weestablishthetask.Wefindthatthebest
Fine-tuningandPrompting. Wefine-tunealllayersofthepre-trained taskdescriptionis:â€œGiventext,determineifthetextcontainsbiasor
ALBERTandGlossBERTmodelswithalearningrateof2Ã—10âˆ’5
nobias.Thebiasmaytargetgender,sex,race,ethnicity,age,and/orTowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRecâ€™24,October14â€“18,2024,Bari,Italy
Table3:Few-shotexampleinputs,outputs,andreasoning Table5:ExamplesofWSDtestcasesandGlossBERTpredicted
usedforpromptingGPT-4ominiforbiasclassification probabilitiesforğ‘¦=1.Eachexcerpthasaterm(bolded)listed
amongrace/ethnicitykeywords.
Inputğ‘¥ Label,Comment
52yearold,marriedfemalewithonedaugh- Label:1,Usegender Inputğ‘¥(labelğ‘¦) Prediction
ter,employedasaschooladministratorwith terms like woman Melanoma:increasinginincidenceinthewhitepopulation
0.9998
nopriorpsychhistoryreports2monthh/o forcasestudies (CDC).(ğ‘¦=1)
[historyof]sadness,subjectiveanxietyand
intermittenttroublefallingasleep.
2015AmericanHeartAssociationguidelinessuggesttreat-
ingpatientspresentingwithsystolicBPabove150-220 0.9998
Oncepatientisonanothertreatmentforher Label:0,n/a(from mmHg,buttheydonotofferaspecificBPtarget.(ğ‘¦=0)
disorder,shenolongerneedsthismedication. XNset)
Calcificplaquesarechalkywhiteandarisefromcardiac
0.0001
Recentmeta-analysissuggestednodifference Label:1,Termâ€˜Late (aorticandmitral)valves.(ğ‘¦=0)
inprevalenceamongcountries,rateis1-2% Adolescenceâ€™ is an
withincreaseduringlateadolescence. uncleartimeperiod Table6:PerformanceMetricsand95%-CIsforFine-Tuned
ModelstrainedonLN+XN*data.RoBERTayieldsthehighest
Table 4: Performance metrics for WSD on manually-
averages,butitisstatisticallytiedwithDistilBERT.
annotated+GPTexcerpts.Bestresultforeachmetricshown
inbold.GlossBERTandGPT-4oaretiedasthebestmodels. Metric RoBERTa DistilBERT BioBERT
TF-IDF+ Gloss GPT-3.5 GPT-4o
Precision 0.613Â±0.015 0.605Â±0.013 0.581Â±0.014
Metric LogisticReg. ALBERT BERT Turbo mini Recall 0.692Â±0.024 0.649Â±0.030 0.620Â±0.019
F1Score 0.650Â±0.014 0.626Â±0.018 0.599Â±0.010
Accuracy 0.839 0.926 0.944 0.925 0.944 F2Score 0.674Â±0.019 0.639Â±0.025 0.611Â±0.014
Precision 0.816 0.935 0.936 0.916 0.936 AUC 0.927Â±0.003 0.921Â±0.006 0.904Â±0.003
Recall 0.839 0.977 1.000 1.000 1.000
F1Score 0.817 0.956 0.967 0.956 0.967 Table 7: Performance Metrics and 95%-CIs for Prompting
GPT-4omini.Bestresultsforeachmetricshowninbold.AUC
geography,includeexclusivelanguage,ormakeunsupportedclaims.
wasommittedasitcannotbecomputedforbinaryoutputs.
Thetextmayalsocontainnobiasatall.Ifthetexthasbiasstate1,if
thetextdoesnothavebiasstate0.â€ Metric Zero-Shot Few-Shot
Bothzero-andfew-shotpromptingusedthepromptabove.For
Precision 0.367Â±0.071 0.259Â±0.019
few-shotprompting,wetestedvarioussetsofexamplesfromthe
Recall 0.260Â±0.029 0.610Â±0.026
dataset.Table3showsthesubsetthatperformedbest.
F1Score 0.303Â±0.040 0.363Â±0.023
Metrics. Forthebiasdetectiontask,wealsoevaluatethemodelsâ€™ F2Score 0.274Â±0.032 0.480Â±0.025
performanceonthetestsetwithrespecttoprecision,recallandF1
useofâ€˜Americanâ€™onlyindirectlyrelatestotheethnicityofthe
score.Inaddition,weconsidertheF2scoreandareaundertheROC
peoplethatanorganizationserves.
curve(AUC).F2issimilartoF1butprioritizesrecalloverprecision.
Wealsoinvestigatewhetherthesyntheticsamplesgenerated
Duetotheclassimbalance,AUCismorerelevantthanaccuracy
byChatGPT-4oweretrivial,whichwouldartificiallyinflateper-
becauseitaccountsforallpossiblethresholdchoices.
formance.Whenweevaluatethemodelresultsononlymanually
annotatedexcerpts,theperformanceofallmodelsstayssomewhat
6 RESULTS similar,exceptforGPTmodels,bothofwhichachieve100%accu-
racy.Therefore,thesyntheticexamplesareatleastashardasthe
6.1 EvaluationofWSDmodels
manuallyannotatedexcerptsforBERTmodels,justifyingtheiruse
Table4presentstheevaluationresultsfortheWSDmodelsexam-
inourevaluation.
ined.Thebaselineachievedworseresultsthantheothermodelsfor
Inaddition,weevaluatethemodelsâ€™performancewhentrained
everymetric.WefindthatGlossBERToutperformsALBERTand
onlyonthemanually-annotateddata.Inthiscase,thereisaperfor-
thatGPT-4ominiimprovesuponGPT-3.5Turbo.Furthermore,both
mancedropforthefine-tunedmodels(ALBERTdeclinesfrom0.926
GlossBERTandGPT-4oaretiedasthebestmodels,bothexhibiting
to0.852andGlossBERTdeclinesfrom0.944to0.852accuracy),
averyhighF1Score(0.967).Usingthecostasatie-breakerbetween
indicatingthatthesyntheticsampleshelpthemodelstogeneralize
thetwo,weoptedtouseGlossBERTfortheWSDtaskperformed
better.Furthermore,GlossBERTremainstiedasthebestmodel,
ontheextractednegatives.
whichsupportsourchoiceofusingitforbuildingthesetoffiltered
Table5illustratesexamplesfromthetestset,twoofwhichwere
extractednegativesinthebiasdetectiontask.
correctlyidentifiedandonethatwasnot.Whilethefirstandthird
werecorrectlypredictedwithhighconfidence,themiddlerowwas
incorrectlyclassifiedwithaâ€œhighconfidencepredictionâ€.Forthe 6.2 EvaluationofBiasDetectionModels
few instances that GlossBERT incurred false positives, a closer Firstly,wecomparetheperformanceofthefine-tunedBERTvari-
inspectionhasrevealedthatthoseexcerptsmaybelackingenough antsonthebiasdetectiontask.Table6displaysthemodelsâ€™perfor-
contextforthisspecifictask.Forexample,inthemiddlerow,the mancewithrespecttoprecision,recall,F1andF2score,andAUC.FAccTRecâ€™24,October14â€“18,2024,Bari,Italy Buttsetal.
Table8:Performancemetricsand95%-CIsforRoBERTa,TinyLlamatrainedondatasetvariants(LN+XN*,LN+XN,LN).Best
resultsamongeachmodelvariants(resp.acrossallmodels)andstatisticaltiesshownarebolded(resp.underlined).
RoBERTa TinyLlama
Metric
LN+XN* LN+XN LN LN+XN* LN+XN LN
Precision 0.613Â±0.015 0.640Â±0.021 0.526Â±0.029 0.675Â±0.008 0.693Â±0.028 0.536Â±0.020
Recall 0.692Â±0.024 0.667Â±0.023 0.719Â±0.026 0.548Â±0.030 0.519Â±0.029 0.607Â±0.035
F1Score 0.650Â±0.013 0.652Â±0.017 0.606Â±0.017 0.604Â±0.021 0.593Â±0.017 0.568Â±0.016
F2Score 0.674Â±0.019 0.661Â±0.016 0.669Â±0.016 0.569Â±0.027 0.546Â±0.024 0.591Â±0.025
AUC 0.927Â±0.003 0.930Â±0.009 0.910Â±0.008 0.907Â±0.005 0.903Â±0.005 0.871Â±0.011
Table9:PerformanceMetricsand95%-CIsforFine-Tuned introducesaframeworkfordetectinganddiagnosingbiasinthe
ModelsagainstBaseline(âˆ—Salavatietal.,2024).Bestresults medicalcurriculum, focusingon thedata guidingthese models
andstatisticaltiesshowninbold. ratherthanonthemodelsâ€™architecture.Weusemodelstrainedand
testedoninstructionalcontentannotatedbymedicalexpertsfor
Metric RoBERTa TinyLlama Baselineâˆ— bias.Wefocusonbiasrelatedtosex/gender,race/ethnicity,age,
Precision 0.613Â±0.015 0.675Â±0.008 0.504Â±0.054 and geography. Our method involves extracting non-annotated
Recall 0.692Â±0.024 0.548Â±0.030 0.812Â±0.069 samplesthatcontainasocialidentifierasnegativesamplesforthe
F1Score 0.650Â±0.014 0.604Â±0.021 0.615Â±0.022 biasclassifier.Forthoseextractednegatives,weemploywordsense
F2Score 0.674Â±0.019 0.569Â±0.027 0.717Â±0.027 disambiguationtocleanoutanythathaverace/ethnicity-related
AUC 0.927Â±0.003 0.907Â±0.005 0.923Â±0.004 termsbutarenotactuallyrelatedtothosecategories.
OurfindingsdemonstratethatwhileLLMscanhandlemany
While RoBERTa and DistilBERT are statistically tied, BioBERT
tasks,theyarenotwell-suitedforthisone.Ourzero-andfew-shot
clearly performs worst among all BERT models. We select the
promptingwithGPT-4ominiunderperformedcomparedtothebase-
RoBERTamodelforfurthercomparisonduetothemodelâ€™shigher
linemodelfromourpreviousworkandscoredsignificantlylower
meanevaluationmetricswithlowerstandarddeviations.
thanthelanguagemodelswetested.Similarly,usingadomain-
Secondly,weevaluatetheperformanceofzero-andfew-shot
specificmodellikeBioBERTshowednosignificantimprovement.
prompting with GPT-4o mini on bias detection. Table 7 shows
RoBERTaandTinyLlamawerethebestperformersforbiasdetec-
theresultsobtainedusingthepromptingtechniquesoutlinedin
tion,withRoBERTamatchingthebaselineandshowingslightgains
Section 5.2. Despite the substantial increase in recall seen with
inprecisionandF1score.
few-shotprompting,thelowoverallperformanceofGPT-4omini
OurWSDmodelswerehighlyeffectiveatdistinguishingbiased
deemsitunsuitableforthebiasdetectiontask.
excerptsfromnon-biasedones.ALBERTandGlossBERTnearly
Next,wecomparethebestBERTmodelandabaselinefromour
perfectlydisambiguatedsentenceswithraceandethnicity-related
priorwork[25]withafine-tunedTinyLlama.Table9showsthe
keywords.AlthoughGPTmodelswerecomparabletoBERTmodels,
comparisonresults.AlthoughTinyLlamaachieveshighprecision,
BERTconsistentlyoutperformedGPTinallmetricsexceptrecall.
itslowerrecallcausesittobeoutperformedbyRoBERTaandby
Whilethistaskfocusedononebiascategory,thesemodelscould
the baseline with respect to both F1 and (especially) F2 scores.
beadaptedtoothertypeswithappropriateannotations.Applying
RoBERTaandthebaselinearestatisticallytiedwiththehighest
WSDtobiasdetectioninmedicalcurriculayieldedmixedresults.
AUCs(0.927Â±0.003and0.923Â±0.004),indicatingthatforeither
TheAUCforRoBERTawassimilartothebaseline,butWSDim-
modeltheclassificationthresholdcanbetunedtofindatrade-off
provedbothprecisionandF1score.
betweenprecisionandrecallsuitableforthetargetapplication.
This work could help identify potentially biased excerpts in
Last,weconductanablationtesttoassesstheimpactofWSD
medicalcurriculaforreviewbeforetheyâ€™reusedtotrainmodels
fordatarefinementbycomparingtheperformanceofRoBERTa
forfuturehealth-relatedapplicationsandrecommendersystems,
andTinyLlamaacrossvariousdatasetconfigurations.Theresultsin
contributingtomoreequitablehealthcareacrossalldemographics.
Table8showthattheLN+XN*settingledtohigherrecallaverages
thanLN+XN(despitenotstatisticallysignificant)atasmallcost
8 DISCUSSION
in precision. LN achieves the highest recall, but at a steep cost
DespitetheencouragingresultsprovidedbyourWSDandbias
inprecision.Therefore,LN+XN*resultsinthehighestF2scores,
classificationmodels,therearefuturedirectionswecantaketo
indicatingthatitisthemostadequatesettingforTAR(Technology
enhanceourprojectâ€™ssignificance.First,intheWSDexperiment,
AssistedReview)purposes.
usingChatGPT-4otogeneratemoresentencesnoticeablyincreased
theperformanceofourlanguagemodels.Hence,itislikelythat
7 CONCLUSION increasingthenumberofsyntheticsentencescanfurtherenhance
Despiterecentstridesinfairness,accountability,andtransparency, performanceifthesamplesarediverseenough.LLMsoftenhavea
health-related applications and recommender systems are still â€œtemperatureâ€parameterthatcancontroltheamountofrandomness
pronetobiasesamplifiedthroughdata,whichcanperpetuatehealth inthetextgeneration.However,excessivelyhightemperatures
disparitiesandaffectpatientcare.Tomitigatethisissue,thispaper couldalsoyieldlesscoherentsentences.TowardsFairerHealthRecommendations:findinginformativeunbiasedsamplesviaWordSenseDisambiguation FAccTRecâ€™24,October14â€“18,2024,Bari,Italy
Wealsowanttoconsiderhowwordsensedisambiguationmight healthprofessionseducation:howeducators,facultydevelopers,andresearchers
beusefulinthecontextofothersocialidentifiers,suchasgeography canmakeadifference.AcademicMedicine,92(11S):S1â€“S6,2017.
[16] ShawnKhan,AbiramiKirubarajan,TahminaShamsheri,AdamClayton,and
(e.g.,â€œAmericanHeartAssociationâ€vs.â€œNativeAmericansâ€)and
GeetaMehta. Genderbiasinreferencelettersforresidencyandacademic
other domains where the tone of an excerpt is more important medicine:asystematicreview.Postgraduatemedicaljournal,99(1170):272â€“278,
whenevaluatingwordsense(e.g.,socialmedia). 2023.
[17] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusuke
Additionally,althoughLLMslikeGPTmodelshavesignificantly Iwasawa.Largelanguagemodelsarezero-shotreasoners.Advancesinneural
showntobeadvancedinnaturallanguageprocessing,theyalso informationprocessingsystems,35:22199â€“22213,2022.
presentaseriesofchallenges[19].Firstly,developingandtraining [18] WojciechKusa,GeorgiosPeikos,MoritzStaudinger,AldoLipani,andAllan
Hanbury.Normalisedprecisionatfixedrecallforevaluatingtar.InThe10thACM
LLMsrequirescomputationalcostandcanbetime-consuming.So, SIGIR/The14thInternationalConferenceontheTheoryofInformationRetrieval,
theymaybelessaccessibleforsmallergroupsofresearchers. 2024.
[19] HumzaNaveed,AsadUllahKhan,ShiQiu,MuhammadSaqib,SaeedAnwar,
MuhammadUsman,NickBarnes,andAjmalMian.Acomprehensiveoverview
oflargelanguagemodels.arXivpreprintarXiv:2307.06435,2023.
9 ACKNOWLEDGEMENTS [20] RajvardhanPatil,ThomasFHeston,andVijayBhuse. Promptengineeringin
ThismaterialisbaseduponworksupportedinpartbytheNational healthcare.Electronics,13(15):2961,2024.
[21] JhonnyPincay,LuisTerÃ¡n,andEdyPortmann.Healthrecommendersystems:a
ScienceFoundationREUSiteGrant2349370andtheWPISTAR state-of-the-artreview.In2019SixthInternationalConferenceoneDemocracy&
Program.Anyopinions,findings,conclusions,orrecommendations eGovernment(ICEDEG),pages47â€“55.IEEE,2019.
[22] ShainaRaza,MuskanGarg,DeepakJohnReji,SyedRazaBashir,andChenDing.
expressedinthismaterialarethoseoftheauthor(s)anddonot
Nbias:Anaturallanguageprocessingframeworkforbiasidentificationintext.
necessarilyreflecttheviewsoftheNationalScienceFoundation. ExpertSystemswithApplications,237:121542,2024.
[23] LariaReynoldsandKyleMcDonell. Promptprogrammingforlargelanguage
models:Beyondthefew-shotparadigm. InExtendedabstractsofthe2021CHI
REFERENCES
conferenceonhumanfactorsincomputingsystems,pages1â€“7,2021.
[24] AbhayaKumarSahoo,ChittaranjanPradhan,RabindraKumarBarik,andHar-
[1] SarahEAli-Khan,TomaszKrakowski,RabiaTahir,andAbdallahSDaar.Theuse ishchandraDubey.Deepreco:deeplearningbasedhealthrecommendersystem
ofrace,ethnicityandancestryinhumangeneticresearch.TheHUGOjournal, usingcollaborativefiltering.Computation,7(2):25,2019.
5:47â€“63,2011. [25] ChimanSalavati,ShannonSong,WillmarSosaDiaz,ScottAHale,RobertoE
[2] AhmadAsadiandRezaSafabakhsh. Theencoder-decoderframeworkandits Montenegro,FabricioMurai,andShiriDori-Hacohen.Reducingbiasestowards
applications.Deeplearning:Conceptsandarchitectures,pages133â€“167,2020. minoritizedpopulationsinmedicalcurricularcontentviaartificialintelligence
[3] KunBu,YuanchaoLiu,andXiaolongJu. Efficientutilizationofpre-trained forfairerhealthoutcomes.arXivpreprintarXiv:2407.12680,2024.
models:Areviewofsentimentanalysisviapromptlearning.Knowledge-Based [26] LibbyTiderman,JuanSanchezMercedes,FionaRomanoschi,andFabricioMurai.
Systems,page111148,2023. Towardsdetectingcascadesofbiasedmedicalclaimsontwitter.In2023IEEEMIT
[4] RobertChallen,JoshuaDenny,MartinPitt,LukeGompels,TomEdwards,and UndergraduateResearchTechnologyConference(URTC),pages1â€“5,2023.
KrasimiraTsaneva-Atanasova. Artificialintelligence,biasandclinicalsafety. [27] ThiNgocTrangTran,AlexanderFelfernig,ChristophTrattner,andAndreas
BMJquality&safety,28(3):231â€“237,2019. Holzinger.Recommendersystemsinthehealthcaredomain:state-of-the-artand
[5] BenjaminClaviÃ©,AlexandruCiceu,FrederickNaylor,GuillaumeSouliÃ©,and researchissues.JournalofIntelligentInformationSystems,57(1):171â€“201,2021.
ThomasBrightwell.Largelanguagemodelsintheworkplace:Acasestudyon [28] JenniferTsai,LauraUcik,NellBaldwin,ChristopherHasslinger,andPaulGeorge.
promptengineeringforjobtypeclassification. InInternationalConferenceon Racematters?examiningandrethinkingraceportrayalinpreclinicalmedical
ApplicationsofNaturalLanguagetoInformationSystems,pages3â€“17.Springer, education.AcademicMedicine,91(7):916â€“920,2016.
2023. [29] JiaqiWang,EnzeShi,SigangYu,ZihaoWu,ChongMa,HaixingDai,QiushiYang,
[6] GiuseppeColavito,FilippoLanubile,NicoleNovielli,andLuigiQuaranta.Leverag- YanqingKang,JinruWu,HuawenHu,etal.Promptengineeringforhealthcare:
inggpt-likellmstoautomateissuelabeling.In2024IEEE/ACM21stInternational Methodologiesandapplications.arXivpreprintarXiv:2304.14670,2023.
ConferenceonMiningSoftwareRepositories(MSR),pages469â€“480.IEEE,2024. [30] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,brianichter,Fei
[7] GordonVCormackandMauraRGrossman.Engineeringqualityandreliability Xia,EdChi,QuocVLe,andDennyZhou.Chain-of-thoughtpromptingelicits
intechnology-assistedreview.InProceedingsofthe39thInternationalACMSIGIR reasoninginlargelanguagemodels. InS.Koyejo,S.Mohamed,A.Agarwal,
conferenceonResearchandDevelopmentinInformationRetrieval,pages75â€“84, D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformationProcessing
2016. Systems,volume35,pages24824â€“24837.CurranAssociates,Inc.,2022.
[8] LeonorCorsino,KenyonRailey,KatherineBrooks,DanielOstrovsky,SandroO [31] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama:An
Pinheiro,AlysonMcGhan-Johnson,andBlancaIrisPadilla.Theimpactofracial open-sourcesmalllanguagemodel.arXivpreprintarXiv:2401.02385,2024.
biasinpatientcareandmedicaleducation:letâ€™sfocusontheeducator.MedEd-
PORTAL,17:11183,2021.
[9] ErinDehon,NicoleWeiss,JonathanJones,WhitneyFaulconer,ElizabethHinton,
andSarahSterling.Asystematicreviewoftheimpactofphysicianimplicitracial
biasonclinicaldecisionmaking.AcademicEmergencyMedicine,24(8):895â€“904,
2017.
[10] ShiriDori-Hacohen,RobertoMontenegro,FabricioMurai,ScottAHale,Keen
Sung,MichelaBlain,andJenniferEdwards-Johnson.Fairnessviaai:Biasreduc-
tioninmedicalinformation.arXivpreprintarXiv:2109.02202,2021.
[11] GuhaoFeng,BohangZhang,YuntianGu,HaotianYe,DiHe,andLiweiWang.
Towardsrevealingthemysterybehindchainofthought:Atheoreticalperspective.
InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages70757â€“70798.
CurranAssociates,Inc.,2023.
[12] MarkHalman,LindsayBaker,andStellaNg. Usingcriticalconsciousnessto
informhealthprofessionseducation:Aliteraturereview.Perspectivesonmedical
education,6:12â€“20,2017.
[13] DebraHowcroftandJillRubery.â€˜biasin,biasoutâ€™:genderequalityandthefuture
ofworkdebate.Labour&Industry:ajournalofthesocialandeconomicrelations
ofwork,29(2):213â€“227,2019.
[14] LindaMHunt,NicoleDTruesdell,andMetaJKreiner.Genes,race,andculture
inclinicalcare:racialprofilinginthemanagementofchronicillness.Medical
anthropologyquarterly,27(2):253â€“271,2013.
[15] Reena Karani, Lara Varpio, Win May, Tanya Horsley, John Chenault,
KarenHughesMiller,andBridgetOâ€™Brien. Commentary:racismandbiasin