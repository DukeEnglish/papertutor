-generAItor: Tree-in-the-Loop Text Generation
for Language Model Explainability and Adaptation
THILOSPINNER,
ETHZurich,Switzerland
REBECCAKEHLBECK,
UniversityofKonstanz,Germany
RITASEVASTJANOVA,
ETHZurich,Switzerland
TOBIASSTÃ„HLE,
UniversityofKonstanz,Germany
DANIELA.KEIM, UniversityofKonstanz,Germany
OLIVERDEUSSEN,
UniversityofKonstanz,Germany
MENNATALLAHEL-ASSADY,
ETHZurich,Switzerland
Largelanguagemodels(LLMs)arewidelydeployedinvariousdownstreamtasks,e.g.,auto-completion,aided
writing,orchat-basedtextgeneration.However,theconsideredoutputcandidatesoftheunderlyingsearch
algorithmareunder-exploredandunder-explained.Wetacklethisshortcomingbyproposingatree-in-the-loop
approach,whereavisualrepresentationofthebeamsearchtreeisthecentralcomponentforanalyzing,
explaining,andadaptingthegeneratedoutputs.Tosupportthesetasks,wepresentgenerAItor,avisual
analyticstechnique,augmentingthecentralbeamsearchtreewithvarioustask-specificwidgets,providing
targetedvisualizationsandinteractionpossibilities.Ourapproachallowsinteractionsonmultiplelevelsand
offersaniterativepipelinethatencompassesgenerating,exploring,andcomparingoutputcandidates,aswell
asfine-tuningthemodelbasedonadapteddata.Ourcasestudyshowsthatourtoolgeneratesnewinsights
ingenderbiasanalysisbeyondstate-of-the-arttemplate-basedmethods.Additionally,wedemonstratethe
applicabilityofourapproachinaqualitativeuserstudy.Finally,wequantitativelyevaluatetheadaptabilityof
themodeltofewsamples,asoccurringintext-generationusecases.
CCSConcepts:â€¢Computingmethodologiesâ†’Naturallanguagegeneration;â€¢Human-centeredcom-
putingâ†’Graphicaluserinterfaces;Visualizationsystemsandtools;â€¢Mathematicsofcomputing
â†’Exploratorydataanalysis.
AdditionalKeyWordsandPhrases:largelanguagemodels,beamsearchtree,naturallanguagegeneration,
explainability,languagetransformers,visualanalytics
ACMReferenceFormat:
ThiloSpinner,RebeccaKehlbeck,RitaSevastjanova,TobiasStÃ¤hle,DanielA.Keim,OliverDeussen,andMenna-
tallahEl-Assady.2024. -generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainability
andAdaptation.J.ACM37,4,Article111(August2024),32pages.https://doi.org/10.1145/3652028
Authorsâ€™addresses:ThiloSpinner,ETHZurich,Zurich,Switzerland,thilo.spinner@inf.ethz.ch;RebeccaKehlbeck,University
ofKonstanz,Konstanz,Germany,rebecca.kehlbeck@uni-konstanz.de;RitaSevastjanova,ETHZurich,Zurich,Switzerland,
rita.sevastjanova@inf.ethz.ch;TobiasStÃ¤hle,UniversityofKonstanz,Konstanz,Germany,tobias.staehle@uni-konstanz.de;
DanielA.Keim,UniversityofKonstanz,Konstanz,Germany,keim@uni-konstanz.de;OliverDeussen,Universityof 111
Konstanz,Konstanz,Germany,oliver.deussen@uni-konstanz.de;MennatallahEl-Assady,ETHZurich,Zurich,Switzerland,
menna.elassady@ai.ethz.ch.
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
Thisistheauthorâ€™sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.ThedefinitiveVersion
ofRecordwaspublishedinJournaloftheACM,https://doi.org/10.1145/3652028.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
4202
raM
21
]CH.sc[
1v72670.3042:viXra111:2 Spinneretal.
1 Introduction
Recently,largelanguagemodels(LLMs)havegainedincreasedpopularity,especiallyinthefield
ofnaturallanguagegeneration(NLG).Atthelatest,withtheintroductionofChatGPT1,LLMs
havebeenmadeaccessibletoawider,moregeneralaudience.However,despitetheirgrowing
recognitionandnotableaccomplishments,theystillfaceseverallimitations.Commonfailures,
evenforstate-of-the-artmodels,arerepetitivecontent,thelackoffactualaccuracy,oftenreferred
toashallucination[Jietal.2023],andbiases[Alba2022].However,theperceivedhighquality
ofLLMoutputsmakesidentifyingerrorsintheirpredictionsdifficult,whichisaggravatedbya
lackofexplainabilityandaccessibility[Zhaoetal.2024].Gainingunderstandingandaccesstothe
modelâ€™sdecision-makingprocessisfundamentalforrecognizingerrorsintheiroutputs,calming
concernsaboutoverestimatingthemodelâ€™scapabilities,andempoweringuserstoguidethemodelâ€™s
predictionstoalignwiththeirintentions.Particularly,thechatinterfaceofChatGPTandotherchat-
orcompletion-basedapproachesomitimportantinformationonuncertaintiesorviablealternatives
fromtheusers.Whiletext-basedinterfacesmayfulfilltheneedsforabroad,generalaudience,
interestednon-expertsandlinguisticexpertsrequiremorein-depthinsightsandcontrol.
Weidentifythreeprimaryshortcomingsinthecurrentstate-of-the-artforinteractingwithLLMs:
lackof explainability,comparability,andadaptability.Explainabilityreferstounderstanding
ofthemodelâ€™sdecisionprocess,includingthewayalanguagemodelpredictsitsoutput,itssampling
strategy,andtheprobabilitiesoftheseoutputs.Forexample,explanationsofapredictionâ€™scertainty
canprovidetheuserahintonpossiblehallucinations.Comparability,i.e.,asimpleyeteffective
comparisonofmultiplegeneratedoutputs,canenabletheusertoassessmorespecificnuancesin
themodelâ€™spredictions.Thiskindofcontrastiveexplanation[El-Assady,Jentner,etal.2019]is
particularlyrelevantforlinguisticexperts.Forinstance,byadaptingpromptswithtypicalnames
fromvaryingethnicgroupsandcomparingthepredictions,theusercanassessthemodelâ€™sbiases,
ifpresent.Andlastly,adaptabilityisrelevantwhenthegeneratedoutputisnotsatisfactory.The
insightsgainedfromexplainabilityandcomparabilityempowertheusertosteerthemodeltowards
theirintentions.Concretely,theusershouldbeabletoeditproblematicparts;e.g.,bycorrecting
made-upfactsandmakingthesechangespermanent;e.g.,byfine-tuningthemodel.
Since almost all modern LLMs have committed themselves to the transformer architecture,
besidestheirnumberoftrainableparameters,thequalityofthetrainingdataisthedecisivefactor
foramodelâ€™sperformance[Lauscheretal.2021;Mishraetal.2022].Therefore,studyingthemodelâ€™s
behavioriscloselylinkedtostudyingitsinâ€“andoutputs,representingalocalapproximationof
theinformationthemodelhaslearnedduringtraining.Ourproposedapproach,thus,focuseson
making these inâ€“ and outputs accessible and explorable to the user. A straightforward way to
achievethisistomakethesearchalgorithmtransparent.Themostprominentalgorithmtosample
sequences from the probability distributions output by the model is beam search. By sampling
thedecision-space[El-Assady,Sevastjanova,etal.2018]throughexpandingthemostpromising
sequence in a limited set of candidate sequences, the algorithm results in a tree, scanning the
searchspaceforsequenceswithhighoverallprobability.Beamsearchisthuscommonlyusedin
languagemodelexplanationmethods,suchasthevisualinterfacebyLeeetal.[Leeetal.2017],
Seq2Seq-Vis[Strobelt,Gehrmann,etal.2018],orGenNI[Strobelt,Kinley,etal.2022].
In this paper, we propose a tree-in-the-loop interaction paradigm, which leverages a visual
representationofthebeamsearchtree(BST)asthecentralcomponentofthegenerAItorvisual
analyticstechnique.Werevealandexplainthemodelâ€™sdecision-makingprocessbylayingoutthe
BSTandaugmentingitwithadditionalexplanations,suchastokenprobabilities,semantickeyword
coloring, and sentiment annotations. Comparative explanations are facilitated by juxtaposing
1https://openai.com/blog/chatgpt
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:3
multipleBSTs,allowingtheusertocomparethemodelâ€™spredictionsunderslightlyvariedinputs.
Furthermore,weenabletheusertointeractwiththetree,allowingthemtoadaptandsteerthe
modelâ€™s predictions, for example, by overriding model decisions, editing predicted sequences,
orfine-tuningthemodel.Tofacilitateaneffectiveanalysisthroughvisualinteractivemethods,
we identify five main tasks in the context of informed text generation: model prompting and
configuration,treeexplorationandexplainability,guidedtextgeneration,comparativeanalysis,
andBSTandmodeladaptation.Eachofthesetasksplacesdistinctdemandsonthetoolsavailable.
Tobeabletofulfillthesedemandsinacombinedapproach,wedesignamodular,widget-based
workflow,wheretask-specificwidgetsenhancetheBSTwithtailoredcontrols,interactionpossi-
bilities,andvisualizations.Eachwidgetaddsaveryspecificfunctionality.However,insymbiosis,a
selectedsetoftask-supportingwidgets,ininteractionwiththesearchtree,enablesnovel,powerful
modes of analysis. E.g., comparative analysis is facilitated by two particular widgets, allowing
linguisticexpertstoobservechangesinthetreeunderslightvariationsofthestartingprompt.
Thisrevealsbiasesintheobservedmodel,whoseidentificationandmitigationisoneofthemost
burningissueswithstate-of-the-artlanguagemodels[Alba2022].
Inthispaper,wecontribute:(1)Adetailedproblemanalysisofthechallengesofexplainability,
controllability,andadaptabilityinthecontextofvarioustextgenerationtasks.(2)Anovelvisual
analyticstechniquecalledgenerAItor,tacklingthesechallengesinaninteractivetree-in-the-loop-
approach. (3) An implementation of the generAItor technique in a web-based visual analytics
workspace.(4)Athree-foldevaluationofthegenerAItortechnique,including(4.1)casestudies,
showcasingthegenerativeandcomparativecapabilitiesofourtechnique,(4.2)aqualitativeuser-
study,provingtheusabilityoftheimplementation,and(4.3)aquantitativeevaluation,confirming
theabilitytoadaptthemodeltouser-preferenceswithfewtrainingsamples.
2 RelatedWork
Inthefollowing,wepresentourrelatedworkonlanguagemodeling,semanticsimilarity,controlled
textgeneration,andbiasanalysis.
2.1 LanguageModeling
Languagemodels(LMs)areprobabilitydistributionsoverwordsequencesandacorecomponent
ofnaturallanguageprocessing(NLP)systems[Bengioetal. 2000].Withtheemergenceofthe
transformerarchitecture[Vaswanietal.2017],therewasaparadigmshiftawayfromrecurrent
neuralnetworks[Rumelhartetal.1986]sincetransformersallowparallelcomputations,speeding
uptrainingtimes,andprovesuperiorincapturinglong-termdependencies[Vaswanietal.2017].
Theyusetheattentionmechanism[Bahdanauetal.2014],whichdirectsthefocusonimportant
tokens in the input sequence. Nowadays, numerous pre-trained transformer architectures are
availableforpublicuse[Wolfetal.2020].Therearedifferenttypesoftransformers,wherebythe
twomaincategoriesaremaskedlanguagemodelsandgenerativelanguagemodels.
MaskedLMsâ€”BERT[Devlinetal.2018]isatransformer-basedLMthatwastrainedonmasked
languagemodeling(i.e.,cloze)andnext-sentencepredictiontasksandiscommonlyfine-tunedfor
diversetextclassificationtasks[HowardandRuder2018].Duetoitspre-trainingobjective,BERT
(aswellasothermaskedlanguagemodels)isnotsuitablefortextgenerationtasks.WeuseBERT
formaskedwordpredictionintheontologicalreplacefunctionality W .
GenerativeLMsâ€”Textcanbegeneratedusinggenerativetransformermodels,suchasGPT-
2[Radford,Wu,Child,etal.2019],GPT-3[Brownetal.2020],orGPT-4[OpenAI2023].Theseare
autoregressivemodelsthatwerepre-trainedonthecausallanguagemodelingtask,learningto
predictthenextwordintheinputsequence.Forabroaderoverview,seethesurveyonpre-trained
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:4 Spinneretal.
languagemodelsfortextgenerationbyLieetal.[J.Lietal.2021].Inourwork,weuseGPT-2and
Bloom[Scaoetal.2023]fortextgeneration;however,theapproachisdesignedtosupportother
transformer-basedLMsaswell.
2.2 SemanticSimilarity
WordTaxonomiesandOntologiesâ€”Leveragingsemanticgraphsandknowledgebases,suchas
YAGOandDBpedia,itispossibletoinferconceptortopichierarchiesvialanguagemodels[Chen
etal.2021;Huangetal.2020;C.Zhangetal.2018]orexpandexistingtaxonomies[Jiangetal.2022;
Xuetal.2022].MethodssuchasOntoEA[Xiangetal.2021]alignentitiesbyjointlyembedding
ontologiesandknowledgebases.Taxonomiescanbeusedtoimproverecommendersystems[Tanet
al.2022]andhelpwithentityrecognition[Z.Lietal.2022]ortranslation[Z.Lietal.2022].WordNet
informationcanbeintegratedintopre-trainedlanguagemodelsforimprovedsensedisambiguation,
e.g.,ARES[Scarlinietal.2020],orusedtobuildhuman-readableconceptvectors[ConiaandNavigli
2020].Forourmethod,weuseARESandBERTembeddingsinconjunctiontocreatedomain-specific
predictionswithanontologygraph W createdfromtheBabelNet[NavigliandPonzetto2012]
semanticgraph.
EmbeddingSimilarityâ€”Inlanguagemodels,eachtokenoftheinputtextismappedtoahigh-
dimensionalvector.Relatedworkhasshownthatthesecontext-dependentembeddingsencode
different context/language properties. Although BERT is the most widely analyzed language
modelsofar[Rogersetal.2020],othertransformermodels,suchasGPT-2,andtheirproduced
embeddingspaceshavealsoattractedcomputationallinguisticsâ€™andvisualanalyticsresearchersâ€™
attention [Ethayarajh 2019; Sevastjanova, Kalouli, et al. 2022]. Prior research has shown that
semanticinformation,suchaswordsensesandsemanticroles,iscapturedbestinthehigherlayers
oftransformermodels[Reifetal.2019;Sevastjanova,Kalouli,etal.2022;Wiedemannetal.2019].
Thus,thesecontextualizedembeddingsarecommonlyusedasfeaturesforsemanticsimilaritytasks.
Inourwork,weapplyadimensionalityreductiontechniqueonembeddingsextractedfromtheused
LMstomapthetokenstouniquecolorsbasedontheircoordinatesinthetwo-dimensionalspace.
Withthisapproach,tokenswithasemanticsimilaritygetassignedtosimilarcolors[El-Assady,
Kehlbeck,etal.2022].
2.3 ControlledTextGeneration
AlgorithmicApproachesâ€”Ingeneral,controllingthestyleandinformationofnaturallanguage
generationisoneoftheapplicationsidentifiedbyGattandKrahmer[GattandKrahmer2018].
One challenge of integrating knowledge into text generation is the automatic steering of the
generationinaparticulardirection.Usingplug-and-playlanguagemodelsisonepossibilityto
steertextgeneration[Qinetal.2020].Concerningpre-trainedlanguagemodels,itispossibleto
control,e.g.,thesentiment[Dathathrietal.2019;Huetal.2017],keywords[X.He2021],orthe
topic[Dathathrietal.2019].FrameworkssuchasFAIR[HuaandWang2020]allowthegeneration
ofcontent-controlledtextbycombiningBERTwithBART[Lewisetal.2020].Alargeroverviewis
giveninthesurveybyZhangetal.[H.Zhangetal.2022].Buildingonthis,manyapproachesnow
integrateexternalresourcessuchasknowledgebases.Moredetailscanbefoundinthesurveyby
Yuetal.[Yuetal.2022].However,thesetechniquesdonotallowimmediateinterventioninthe
decisionprocess,whichwespecificallytargetwithourapproach.
Visual Interactive Approaches â€” Focusing on interactive editing, Du et al. [Du et al. 2022]
provideinteractivesuggestionsintheirtooltoachievehigh-qualitytexteditswithminimalhuman
effort.PadmakumarandH.He[PadmakumarandH.He2022]useahuman-in-the-loopapproach
toreplacetextsegmentsforthetaskofcreativeimagecaptioning.Gehrmannetal.[Gehrmannetal.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:5
2019]proposeaninteractiveframeworkthatallowsuserstocontrolgenerativesegmentsthrough
aprocesscalledcollaborativesemanticinference.Followingthis,Strobelt,Kinley,etal.[Strobelt,
Kinley,etal.2022]createGenNi,aninterfaceforcollaborativetextgeneration.Theyguidethe
modeloutputusingexplicitlydefinedconstraints.Theuserhastoknowbeforehandhowhewants
tocontrolthemodeloutput,asitisnotpossibletoadaptthestateduringinference.WithWordcraft,
Yuanetal.[Yuanetal.2022]presentaninteractiveinterfacethatallowswriterstocreatestories
withtheassistanceoflargelanguagemodels.Theirsystemletsauthorsre-write,replace,andauto-
generatetext,aswellasdefinecustomrequeststothelanguagemodel.Incontrast,ourapproach
enablesdirectinteractionwiththemodelâ€™soutputsbyexposingpredictionsandprobabilitiesinthe
beamsearchtree.
2.4 BiasAnalysis
Current research explores not only what the models learn but also when they fail and which
limitationstheyhave,suchasdifferenttypesofbiases[Garrido-MuÃ±ozetal.2021].Forinstance,
Blodgett et al. [Blodgett et al. 2020] present a taxonomy for fairness definitions that machine
learningresearchershavedefinedtoavoidexistingbiasinAIsystems.Mehrabietal.[Mehrabietal.
2021]definethebiasproblemspecificallyinlanguagemodelingtasksinaformalwayandexplore
howithasbeentreatedinrelatedworkregardingtheirdetectionandcorrection.
Inmaskedlanguagemodels,thedetectionofbiasistypicallydonebyapplyingtemplatesor
pre-definedwordlists.Forinstance,theWordEmbeddingAssociationTest(WEAT)[Caliskanetal.
2017]measurestheassociationbetweentwotargetwordsets(e.g.,malepronounsand,e.g.,female
pronouns)basedontheircosinesimilaritytowordsfromtwoattributesets(e.g.,termsrelated
toscienceorart)tomakeconclusionsaboutencodedbiases.Liangetal.[Liangetal.2021]show
thattheanalysisofbiasesintextgenerationcanbemorenuanced,e.g.,biasescanariseduringthe
generationofanytoken[Nadeemetal.2021].Alnegheimishetal.[Alnegheimishetal.2022]find
thatbiasâ€œevaluationsareverysensitivetothedesignchoicesoftemplateprompts.â€Accordingtothe
authors,theuseoftemplate-basedpromptstendstoevokebiasesfromthemodelâ€™sdefaultbehavior
rather than reflecting the actual correlation between gender and profession, analyzed in their
work.Thus,weproposeatree-basedapproachforcomparative,exploratorybiasanalysis,allowing
thedetectionofbiasesinvariable-lengthsequencesandtheidentificationofsubtlenuancesin
themodelâ€™spredictions.Foradetailedcasestudy,show-casingthebenefitsofourcomparative
approach,seesection6.1.
3 ProblemCharacterization
WithrecentadvancesinlanguagegenerationandthereleaseofChatGPT,languagemodelshave
madetheirwayintomainstreamuse.Whileautomatictextgenerationthroughlanguagemodels
cansupporttheauthorthroughcorrections,suggestions,orchat-basedquestionanswering,un-
derstandingofthemodelâ€™scapabilitiesandlimitationsandaccesstoitspredictionsisstilllimited.
However,suchunderstandingandaccessarecrucialforraisingawarenessofdangers(e.g.,biased
outputs,hallucinations),allayingfearsofitspotential(e.g.,overestimationofamodelâ€™scapabilities),
andenablinguserstosteerthemodelâ€™spredictionstowardstheirintention(e.g.,byselectingor
modifyingoutputs).
Whiletheaverageusermightnotbewillingtoinvesttimeandeffortininvestigatingthebehavior
oflanguagemodels,weidentifytwoprimaryusergroupswithdifferentinterestsandrequirements
forlanguagemodelanalysis.Wedefinenon-experts Non asinterest-drivenpersonswithanaffinity
for technical advancements and the wish to explore modern language models. The term â€œnon-
expertâ€onlyreferstotheuserâ€™sexperienceswithlargelanguagemodelsandtheirbackground
incomputationallinguistics;theycanstillbedomainexpertsinotherfields.Examplescouldbe
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:6 Spinneretal.
ajournalistwhowritesaboutlanguagemodelsandwantstounderstandtheircapabilitiesand
limitations or a writer who wants to use LLMs to generate text with a specific style or topic.
Complementary,wedefinelinguisticexperts Lin asusersworkingin(computational)linguistics,
withamainfocusontheanalysisofmodelbehavior.Anexamplecouldbealinguistwhowants
toobservebiasesencodedinthemodel[Spinner,Kehlbeck,etal.2023].Ourapproachistargeted
towardsbothusergroups,withshiftingfocusonthetasksoursystemsupports.Forthenon-experts,
understandingofthemodelâ€™scapabilities,explorationofoutputs,investigationofuncertainties,
andtheabilitytoadaptmodeloutputsareprimarilyimportant.Incontrast,thelinguisticexpertis
interestedinthecloseanalysisofmodeloutputs,e.g.,toobservelearnedsyntacticandsemantic
structures,identifymodeldefects,orassessmodelbiases.Inthefollowing,wespecifythechallenges
andtasksforthederivedusergroups.
3.1 Challenges
Thechallengesarederivedfromresearchgapsinrelatedworkandfromdiscussionswithnon-
experts Non,machinelearningexperts,andcomputationallinguists Lin.
Explainability Ex â€”Despitetheimpressiveperformanceofstate-of-the-artlanguagemodels,
theirpredictionsareoftenunderexplained,asdeep-learning-basedmodelsaretypicallyblackboxes,
makingexplainabilityamajorchallenge[Danilevskyetal.2020].However,languagemodelshave
theadvantageofinterpretablein-andoutputs(namely:text)andeasy-to-understandprediction
mechanisms,whichweaimtoleveragetosolvethischallenge.Weidentifytwoprimaryaspects
ofexplainabilityregardinglanguagemodels:modelandoutputexplainability.Explainabilityis
importantforboththenon-expert Non andthelinguisticexpert Lin.
Modelexplainabilityrelatestoexplanationsofthemodelâ€™salgorithmicapproach,suchaspro-
viding information on the modelâ€™s architecture, the used search algorithm, or the influence of
randomness(c.f.,reproducibility)[Spinner,Schlegel,etal.2020].Particularly,mainstreammedia
often fail to explain the primary mechanism behind LLMs: predicting the likelihood of tokens
to follow a sequence of previous tokens. Although some articles briefly touch the topic [Metz
2022;Roose2023],thereismuchmisinformationthroughexcessiveabstractionandalackofeasy-
to-followvisualizationsandinteractivesystemsthatcouldimpartathoroughunderstandingto
non-experts.Understandingthismechanismiscrucialtoraisingawarenessofamodelâ€™slimitations
andallayingfearsofitspotential.Outputexplainabilityreferstoexplanationsofthemodelâ€™stoken
representationsandoutputprobabilities,suchastokenembeddingsimilarityoroutputcertainty.
Comparability Com â€”Theabilitytoexplorethespaceofpossiblemodeloutputsisvastand
currentlyunderexplored[Alnegheimishetal.2022].Fortheanalysis,instance-basedcomparability
ofgeneratedoutputsisessentialforlinguistics,e.g.,forbiasanalysisorhypothesisgeneration.
Particularly,non-templatebased,explorativeanalysisenableshypothesesgenerationandinductive
learning[R.J.SternbergandK.Sternberg2016].
Adaptability Ada â€”Evenstate-of-the-artlanguagemodelsoftenfailtoproduceoutputwhich
alignswithhumanintentionsandstickstofacts[Jietal.2023;LeCun2023].Therefore,adaptability
isessentialtoemploylanguagemodelsinreal-worldscenarios.Again,wedifferentiatetwosub-
aspects: output adaptability and model adaptability. Output adaptation refers to direct edits of
themodelâ€™spredictions,e.g.,tocorrecthallucinatedfacts,re-primethemodelthroughentering
customtext,orselectfromalternativeoutputs,targetingboththenon-expert Non andlinguistic
expert Lin.Thatfollowed,modeladaptationrelatestomodelfine-tuningwiththeediteddatato
makechangespermanentforfuturesessions,whichisalsorelevantforbothusergroups.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:7
3.2 TheTree-in-the-LoopApproach
To address the challenges identified above, we propose the tree-in-the-loop paradigm, a novel
approach to interactively explore and adapt the predictions of language models through the
visualizationofthebeamsearchtree.
Withtheinventionoftransformers,thearchitectureofstate-of-the-artmodelsiswell-established,
shiftingthefocusforperformanceimprovementsonthetrainingprocessandthequalityoftraining
data[Ouyangetal.2022].Consequently,understandingamodelâ€™sbehaviorinvolvesexaminingits
inputsandoutputs,whichreflecttheâ€œknowledgeâ€ithasacquiredduringtraining.Therefore,our
approachemphasizesmakingtheseinputsandoutputsmoreuser-accessibleandexplorable.
Ineachstep,whenpredictingthenexttokenforagiveninputsequence,themodeloutputsa
probabilitydistributionoverallknowntokens.Thefinaltexthastobeconstructedbysampling
from this probability distribution. A common heuristic to choose the output with the highest
probabilityisbeamsearch.Beamsearchisagreedysearchalgorithmthatexpandstheğ‘˜ mostlikely
sequencesineachstep,resultinginatreewithğ‘˜ nodesineachtreelevel.ğ‘˜ iscalledthebeam
width.Brancheswithlowoverallprobabilitystallinthisprocess,resultinginatreewithvarying
depth.Thedeepestleafnodewiththehighestprobabilityisthenchosenasthefinaloutput.Often,
additionalparametersareusedtoincreasethediversityofthegeneratedtext,e.g.,bypenalizing
therepetitionofğ‘›-gramsorbyaddingrandomnesstothesamplingprocess,e.g.,throughtop-ğ‘˜
samplingortemperaturescaling.
Mostinterfacesonlypresenttheuserwiththefinaltext,discardingallinformationaboutthe
sampling process, such as uncertainties of predictions, alternative outputs, or the influence of
parameters such as the beam width or anğ‘›-gram penalty. To enable an understanding of the
modelâ€™spredictionprocess,weaimtomakethisinformationaccessibletotheuser.Thisismost
straightforwardlydonebyvisualizingthebeamsearchtree,whichiseasytounderstandandinteract
with.Furthermore,itprovidesadirectrepresentationoftheunderlyingsamplingalgorithmand
thusdoesneitherneglectinformationnorintroducefalserationalization.
Thetree-in-the-loopapproachistheextensionofthebeamsearchtreewithadditionalaugmen-
tations,visualizations,andinteractionpossibilities.Thismakesthetreeaccessibletonon-technical
users Non andsupportslinguisticexperts Lin intheadvancedanalysisoflinguisticphenomena.
3.3 UserTasks
Fromthebefore-discussedchallengesofexplainability,adaptability,andcomparability,wederive
thefollowingusertasks,asdepictedinfigure2.Whilesometasksareessentialtoloadandinteract
withLLMs,othersareoptionalandonlyrelevantforspecificusecases.
ModelPromptingandConfigurationâ€”Tochooseandassesmodelsfromthevastzooofpre-
trainedLLMs[Wolfetal.2020],theuserhastobeabletoloaddifferentmodels.Furthermore,the
usershouldbeabletoprovideaprompttothemodelandconfigureparametersfortheprediction
algorithm. After interactively editing outputs and, potentially, fine-tuning the model, the user
should be able to save the refined sequences and model for future sessions. Since these tasks
describebasicinteractionswiththemodel,theyareequallyimportantforthelinguisticexpert Lin
andthenon-technicaluser Non.
Loadandassess(pre-trained)models,provideprompts,andconfigureparametersforthe
T0
predictionalgorithm.Savetreesandmodelsforfuturesessions.
Tree Exploration & Explainability â€” The beam search tree, used to sample model outputs,
shouldbetransparentandaccessibletotheuser,allowingthemtoexplorealternativesandassess
thecertaintyofthemodelâ€™spredictions,addressingtheexplainabilitychallenge Ex.Supporting
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:8 Spinneretal.
beamsearchexploration,semanticannotationsofthetreeshouldbeprovided,e.g.,toidentify
topicsimilarityortodiscoverundesiredpatternslikeloopingstructures.Thisisimportantforboth
thenon-expert Non andforthelinguisticexpert Lin,whoareinterestedinthecloseanalysisof
modeloutputsandneedahigher-leveloverviewtocoverlargetrees.
Assessprobabilitiesandexplorealternativebranchesinthebeamsearchtree.Identifytopic
T1
similarityandundesiredpatterns,suchasloopingstructures.
GuidedTextGenerationâ€”Usingthestartpromptorexistingsequencesfromthetree,theuser
shouldbeabletoquerytheLLMtoextendthebeamsearchtreewithnewpredictions.Sincethe
beamsearchtreemightgrowtoasignificantsize,atextviewshouldbeprovidedtoclose-read
generated text and navigate the beam search tree to a local context. Also, for longer texts, an
overviewofthetopicstouchedfacilitatesanoverviewandunderstandingofthegeneratedtext.
Thistaskmainlytargetsthenon-expert Non,whoislikelytogeneratelongertexts.
QuerytheLLMtoextendthebeamsearchtree.Navigatethebeamsearchtreetoalocal
T2 context. Investigate the topics touched by the generated text and stalled beam search
branches.
ComparativeAnalysisâ€”Comparativeanalysistacklesthecomparabilitychallenge Com andis
particularlyimportantforthelinguisticexpert Lin,whoisinterestedinthecloseanalysisofmodel
outputs.Differenttreescanbegeneratedandcomparedbyvaryingstartpromptandbeamsearch
parameters,allowingtoassesstheeffectsofthosechanges.Semanticannotationsandaggregated
representations shouldbeprovidedto quicklyidentifythe keydifferences between trees.This
facilitates,e.g.,generatingnewhypotheses,analyzingmodelbiases,orinvestigatingtheinfluence
offunctionwordsonthepredictions.
Generate and compare different trees by varying prompt and beam search parameters.
T3
Observesyntacticandsemanticdifferencesinthetrees.
BST Adjustment & Model Adaptation â€” Enabling adaptation to domain and personal user
preferences,itshouldbepossibletoeditthegeneratedtext.Thiscaneitherhappenbydirecttext
edits,choosingfromasetofalternatives,orpruningunwantedbranchesofthebeamsearchtree.
Aftereditingthetree,theusershouldbeabletofine-tunethemodelwiththeeditedsequences
toalignfuturepredictionswiththeuserâ€™spreferences.Bothaddressestheadaptabilitychallenge
Ada.Thistaskisimportantfornon-expert Non whoneeddomainadaptationorforlinguistic
experts Lin whowanttoobservetheinfluenceofsuchadaptationontheLLMsâ€™predictions.
Interactivelyeditorreplaceproducedsequencestoadaptthetexttopersonalpreferences
T4
anddomains.Fine-tunethemodelwiththeeditedsequences.
4 TreeVisualization&ModelConfiguration
ThebeamsearchtreeiscentraltoourgenerAItortechnique,thereforebeingthemaincomponent
visiblethroughouttheanalysis.Inthissection,wedescribethevisualrepresentationofthetree,
how it is augmented with information, how the user navigates the tree to a local context and
extendsthetreewithnewpredictions,andhowtheinteractionwithtreenodesisimplemented.By
augmentingthetreewithtask-specificwidgets W,weprovidetailoredcontrols,visualizations,
andinteractions,supportingmodelpromptingandconfiguration T0 andtreeexplorationand
explainability T1.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:9
Succession Edge Keyword Main Branch
Loop Edges (Positive)
Probability Sentiment
Fig.1. Thebeamsearchtreevisualization.Edgewidthandâ€“labelencodetheprobabilityofanodetofollow
itspredecessor.TheleafnodeofthebeamwiththehighestoverallprobabilityismarkedasHead.Keywords
arehighlightedusingsemanticcolors.Thebranchcolorencodesthesentimentofthesequenceuptoanode.
4.1 BeamSearchTree
Our technique is based on a visual representation of the beam search tree as the key analysis
component,establishingthetree-in-the-loopapproach.Itisusedtosamplethefinaloutputsequence
fromthetokenprobabilitiesineachpredictionstep.Inthetreevisualization,nodesencodesequences
andedgestheirorder,asdepictedinfigure1.Thetreeislaidoutfromlefttoright,startingeither
withtheinitialpromptusedduringtreecreationoranarbitrarytreenodethatissetbytheuser
whenonlyasubtreeshouldbeinspected.Edgewidthand-labelencodethenodesâ€™probabilityof
followingitspredecessor.WemarktheleafnodeofthebeamwiththehighestprobabilityasHead
node, which, when not configured otherwise, is the one defining the final text output. When
renderingthetextassociatedwiththetreenodes,wereplaceinvisibleâ€“orcontrolcharacterswith
visibleproxies,e.g.,whitespaceswith andnewlineswith .Thetreevisualizationimpartsthe
uncertaintyoftokensandsequencesandletstheuserexplorenext-likelyalternativesintheform
ofstalledbranches T1.
Toextendthetree,theusercaneithertriggerabeamsearchrunfromtheHeadnode,orstart
auto-prediction,whichiterativelyextendsthetreeattheHeadnodeuntilstopped.
LoopDetectionâ€”Weautomaticallydetectrepeatingnodesequencesinthetreeanddenotethem
withadottededge,asshowninfigure1.Thisallowstheusertoquicklyidentifyrepeatingpatterns,
whichareoftenunwantedmodeldefects,tellinglinguisticexpertsaboutthemodelâ€™slimitationsor
probablymiss-chosensearchparameters[Platen2020].
KeywordHighlightsâ€”Weextractandhighlightkeywordsfromthesequencesinthetree,allow-
inguserstointuitivelydistinguishlessimportantnodes,e.g.,stopwords,frommeaningfulnodes,
e.g.,propernouns T1.Asshowninfigure1,wecolorthekeywordnodesinthetreevisualization
accordingtotheirsemanticembeddings[El-Assady,Kehlbeck,etal.2022],enablingaquickimpres-
sionofthesemanticsimilaritybetweentheconceptspresentinthetree.Furthermore,itallows
identifyingconceptdriftbyrevealingchangingconceptsascolorshiftsinthetreevisualization.
SentimentHighlightsâ€”Facilitatingvisualperceptionofthesentimentoftreebranches,wecolor
theedgesinthetreevisualizationaccordingtothesentimentofthesequenceuptotheedgeâ€™starget
node,asshowninfigure1.Thesentimentisestimatedbyapplyingathree-classRoBERTa-based
sentimentclassifier,whichwastrainedonsocialmediaposts[Hartmannetal.2021].
4.2 ModelPromptingandConfiguration T0
TreeCreationandâ€“Selection W â€”Thetreeselectionpanel( infigure4)allowsloading
existingtreesintotheworkspaceandcreatingnewones.Whencreatinganewtree,theuseris
promptedforastartingsequence,whichisusedastheinitialinputsequencepassedtothemodel.
Thestartingsequencealsoformstherootnodeofthetree.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:10 Spinneretal.
Prediction Parameters W â€” The prediction parameters panel ( in figure 4) allows the
usertospecifytheparametersusedwhenexecutingabeamsearchstep.Theparameterâ€œtop-ğ‘˜â€
specifiesthenumberofsamplesdrawnineachbeamsearchiteration,eitherbyselectingtheğ‘˜most
probabletokensorâ€”iftemperatureisenabledâ€”bysamplingfromthemodelâ€™soutputdistribution.
The length of the beam search can be specified by the parameter â€œnextğ‘› wordsâ€. Finally, the
parameterâ€œtemperatureâ€allowscontrollingtherandomnessofthemodelâ€™soutputdistribution.A
temperaturevalueofzerodisablestemperatureandselectsthetop-ğ‘˜ mostprobabletokensineach
beamsearchiteration.
Model Snapshots and â€“Tracking W â€” The model tracking panel allows the user to load
differentpre-trainedmodels,e.g.,fromHuggingFace[Wolfetal.2020].
Out of the box, generAItor provides access to GPT-2 Base, GPT-2
Large[Radford,Wu,Amodei,etal.2019],andBloom[Scaoetal.2023],
butother,transformer-basedmodelscaneasilybeadded.Morespecifi-
cally,ourapproachismodel(transformer)agnostic;onlytheembedding
projection(c.f., W )hastobere-computedfornewmodelvariants.
Besidesloadingpre-trainedmodels,themodeltrackingpanelalsoal-
lowstheusertocreatesnapshotsofadaptedmodels T3.Bycreatinga
snapshotofthecurrentmodelstate,theusercaneasilyrestorethisstate
later,e.g.,ifthemodelwasfine-tunedtoapointwhereitnolongergeneratesmeaningfuloutputs.
4.3 TreeExplorationandExplainability T1
TreeStyleToggles W â€”Thebeamsearchtreeisaugmentedwithcolorinformationandcanbe
visualizedindifferentlevelsofdetail.Particularly,theedgescanbecolored
by sequence sentiment, the nodesâ€™ fill color can be set based on their
semanticembeddingcolor,thenodesâ€™strokecanbesettorepresenttheir
tokenprobability,andwordlists(see W )canbecoloredbyacategorical
colorscale.Furthermore,thetreeâ€™slevelofdetailcanbeswitchedbetween
Full,showingallnodetextsandusingfullnodespacings;Collapsed,hidingallnodetextsandonly
showingthetreeâ€™sstructurewithminimalspacings;andSemi-Collapsed,onlyshowingthenode
textfornodesoccurringinactivewordlists(seefigure6).
2D Embedding Map W â€” The 2D embedding map ( in figure 4) shows an image of the
currentlyselectedtwo-dimensionalsemanticcolormap[El-Assady,Kehlbeck,etal.2022],usedto
colorthekeywordsinthetreevisualization.Byoverlayingthecolormap
imagewiththekeywords,weenableuserstoexplorehowthekeywords
aredistributedinthehigh-dimensionalspace.Thepositionofkeywords
onthecolormapiscomputedbyatwo-dimensionalUMAP[McInnes
et al. 2018] projection, which we priorly anchored on the keywords
extractedfrom150ksentencepairsintheMultiNLIdataset[Williams
etal.2018].Thisallowsthedetectionofsemanticsimilaritybetween
keywordsandtheidentificationofthemajorconceptspresentinthe
tree.Byhoveringabeamsearchbranch,theusercanfilterthekeywords
visibleontheembeddingmaptoonlyshowthekeywordsofthehoveredbranch.Furthermore,
hoveringrendersapathconnectingthekeywordsaccordingtotheiroccurrenceinthebranch.
Thissequenceprojectionbuildsintuitivepicturesofthesequence,allowingtocomparesentence
structuresandthementionedconcepts.Differenttwo-dimensionalcolormapscanbechosenina
dropdownmenuinthe2Dembeddingmappanel.Thesidefigureshowsthebeamsequenceâ€œThe
moviewasshotinNewYorkCityâ€ontheâ€œTeuling2â€colormap[Teulingetal.2010].
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:11
(TT00) Model Prompting (TT22) Guided Text Generation (TT44) Beam Search Tree
& Configuration Ontology & Model Adaptation
Text View
Voronoi Treemap
Ontological
Replace
Model (TT11) Tree Exploration & Explainability
Configuration
Beam Search Tree
Beam Search
Configuration Node Edits
Tree Creation &
Selection
Fine- Tuning
2D Embedding Map Style Options
(TT33) Comparative Analysis
IFF
Placeholders
IFF
Domain-S pecific & Instances
Word Lists
UpSet Plot Multi- tree
Fig.2. ThefivemaintasksofinteractivetextgenerationassupportedbygenerAItor(seesection3.3).The
beamsearchtreeisthekeyelement(seesection4),facilitatingvisualizationandinteractionwiththemodelâ€™s
decisions.Eachtaskhasasetofwidgetsassociated(seesection5),providingtask-specificvisualizations,con-
trols,andinteractionpossibilities.Followingourproposedtree-in-the-loopparadigm,thetasksareinterwoven
andcanbecombinedinaniterativeprocess,centeredaroundthebeamsearchtree.
5 TextGeneration,Comparison,&ModelAdaptation
Besidesthedefaultwidgetstoconfiguremodels,specifyparameters,promptthemodel,andexplain
thebeamsearchtree,weprovideadditionalwidgetsthataretailoredtoaspecifictaskmode.We
distinguishbetweentwomainmodes:controlledtextgeneration(section5.1)andcomparative
analysis(section5.2).Eachmodehasadedicatedsetofwidgetsenabledbydefault.Theyenhance
existingfunctionalitieswithadditionalon-demandinformation,allowadditionalinteractions,or
enablespecificmodesofanalysis.Thewidgetsaredesignedasmodularcomponentsthatcanbe
enabled/disabledandmovedaroundtheworkspacetosupporttheuserâ€™sworkflow.
5.1 TextGeneration T2 andBSTAdaptation T4
Guided text generation provides tools to support the user in the informed generation of text,
particularlytoclose-readgeneratedtext,navigatethebeamsearchtree,andselectdesiredsequences.
Furthermore,itprovidescontentsummarizationintheformofanontologyVoronoitreemap,which
canbeusedtodetectconceptsintheproducedtextandtoidentifysemanticdifferencesacross
nodeswiththesamekeywords.
5.1.1 WidgetsSupportingGuidedTextGeneration
TextView W â€”Whilethebeamsearchtreevisualizationsupportsunderstanding,exploration,
andinteractiononahighlydetailedlevel,itishardtoreadthefinaloutputtextfromonlyobserving
beamsandnodes.Therefore,atextoutputpaneldisplaysthefull
sequenceofthemainbranch,whichinturnishighlightedingray
inthetreevisualization.Toretainthemembershipofeachnode
anditscorrespondingembeddingandkeywordinformation,the
nodesequencesareslightlyspacedinthetextviewandunderlinedwiththeirkeywordembedding
color.Themorecompressedrepresentationinthetextview,togetherwiththeabilitytooverflow
thetextcontainerusingscrollbars,allowstoalwaysdisplaythefulltextstartingattherootnode.
Weusethisadvantageofthetextviewtoallowtreefiltering:byopeningthecontextmenuona
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:12 Spinneretal.
textnode,thenodecanbesetasstartnode( ).Thisfiltersthedisplayedbeamsearchtreetothe
descendantsoftheselectednode,allowinglocalexplorationandpreventinginformationoverload
onlargetrees.Inreturn,leafnodescanbesetasendnode( ),incaseabranchdifferentfromthe
onewiththehighestbeamprobabilitycontainsthepreferredoutputtext.Acopybuttonfacilitates
copyingthegeneratedoutputtexttotheclipboard.
NodeContextMenu W â€”Thenodesinthebeamsearchtreeofferafeature-richcontextmenu,
showninthemiddle-rightoffigure2.Inthefollowing,wedescribethefunctionalityofthecontext
menuentriesthatarenotcoveredbytheirrespectiveworkspacesubsection.
Edit/ Remove Theeditentryallowsalteringthetextoftheselectednodemanually.When
selectingit,thenodechangesintoaninputfield,wheretheusercanmanuallyenterthedesired
text.Afterfinishingtheedit,thenodechangesbackintonormalmode,andthenodeisupdated
inthebeamsearchtree,includingitskeywordinformationandembeddings.Theremoveentry
allowsremovingtheselectednodeandallitsdescendantsfromthetree.
Predict AlternativetopredictingatthecurrentHeadnode,theusercanalsopredictfrom
anynodeinthetreebyselectingthepredict entryfromthecontextmenu.Theparametersare
specifiedinthepredictionparameterspanel.
OntologicalReplace Basedoninformationextractedfromanunderlyingontologygraph
andtheusageofamaskedlanguagemodel,theontologicalreplaceentryprovidesalternative
suggestionstoreplacetheselectednodewith.
Re-TraintoHere The re-train to here entry allows fine-tuning the model with the beam
sequenceuptotheselectednode,addressingtask T4.Withoutfurtheruserinput,fine-tuning
isexecutedinstantlyinthebackgroundwhenthebuttonisclicked,abstractingtheunderlying
complexprocessandmaximizingsimplicityfortheuser.
Ontology Voronoi Treemap W â€” Through an underlying ontology graph, we provide a
Voronoitreemapvisualizationtosupporttheuseringettinganoverviewoftheconceptsclosely
linkedtothekeywordspresentinthetree.Theextractedkeywordsfrom
thebeamsearchtreeareattachedtonodesintheontologyhierarchyof
BabelNet[NavigliandPonzetto2012].Wegrowasubsumptionhierarchy
fromthesekeywords,whosenodesbecomemoreandmoregeneral.Finally,
nodesareconnectedtotheirrespectivesubdomainsanddomains(e.g.,dog
â†’Animal â†’BIOLOGY).Althoughthewholeontologygraphallowsan
in-depthviewofthesubsumptionhierarchy,thereadabilityofthegraph
worsensasthenumberofleafnodesincreases.Instead,weutilizeaVoronoi
treemap visualization, allowing the user to view the hierarchy in four
predefinedlayers:domains,subdomains,synsets,andkeywordinstances.
Domainsandsubdomainsprovideanoverviewoftheconceptsinthebeam
searchtree.Synsetsaggregatesimilarkeywords.Thekeywordinstancelayershowsallkeywords.
Keywordscanappearmultipletimesinthislayer,asonekeywordcanappearatdifferentpositions
inthebeamsearchtree.Becausethesurroundingcontextofakeyworddiffersforeachnode,their
embeddings differ, resulting in different colors, e.g., the keyword â€œwalkâ€. To allow the user to
investigatethisfurther,hoveringoveracelloftheVoronoitreemaphighlightstherespectivenodes
inthebeamsearchtree,enablingthemtoinspectthekeywordsintheircontext.
Ontological Replace W â€” Using our tool, text generated by the model can be adapted to
theuserâ€™spreferencesbyselectingbranchesoreditingmodeloutputs.However,sometimes,the
predictions from the model are not what the user has in mind. We offer an alternative way of
adaptingthemodeltreeusingdomain-specific,context-sensitivealternatives.Iftheuserisunsure
aboutasuitablereplacementwordandrequiresguidance,hecanusetheontologicalreplacefunction.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:13
(1) (3)
Predict Loop L Do io ffp e- r> eC nth o Bo rs ae nch (4)
Ontological
Replace
Edit &
Predict
(2)
Fig.3. Textgenerationworkflowasdescribedinsection5.1.2.(1)Aftercreatinganewtreeandpredictingwith
thesetparameters,themodelrunsintoaloop.Bychoosingadifferentbranch,thisissuecanberesolved.(2)
Bymanuallyeditingnodes,factualknowledgecanbeincorporatedintothetext.(3)Theontologytreegives
anoverviewofconceptsconnectedtothegeneratedtext;(4)ontologicalreplacementssuggestalternatives.
Withtheinformationcurrentlyintheontologygraph,it
ispossibletogeneratepredictionsforaspecificnodeand
groupthembydomain.Thesedomainpredictionscanbe
fromthecurrentdomainsinthebeamsearchtree,ortheuser
canmanuallyadddomainsfromapredeterminedselection.
The domains and their respective suggestions are words
thatthelanguagemodelmightnothavesuggestedinits
top-ğ‘˜ prediction,makingitanintermediatemodebetween
manualeditingandautomaticpredictionofthemodel,even
allowingout-of-distributionsuggestions.Extensiveimplementationdetails,includingfiguresofthe
underlyingNLPpipelines,canbefoundinAppendixA.
5.1.2 WorkflowDemonstration:TextGeneration
Thefollowingexemplaryworkflowshowcaseshowourapproachisusedtogenerateandadapt
text.Todemonstrate,weutilizeGPT-2Base2[Radford,Wu,Amodei,etal.2019]asthelanguage
model.NotethatthesequencespresentedinthisexampledonotrepresentthequalityofSOTA
languagemodels.Nevertheless,GPT-2Baseiswellsuitedtoshowcaselargermodelsâ€™deficiencies
(e.g.,repetitions,hallucination)inbriefexamples.Sinceourapproachismodel-agnostic,otherLMs
canbeloadedinstead.
A newspaper author wants to write a short but informative article on the United States of
America(USA).Asabasis,heusesafactssheetcontaininginformationonpopulation,geography,
etc.oftheUSA.InthegenerAItorworkspace,hecreatesandloadsanewtree( )withthestarting
sequenceâ€œTheUnitedStatesofAmericaâ€(figure3.1).Aftersettingthebeamsearchparameters( )
toğ‘˜ =3andğ‘› =10,hestartspredictingattheheadnode.Aftertwobeamsteps,thebranchwith
thehighestprobabilitygetsstuckinaloop:â€œTheUnitedStatesofAmericaisanationofimmigrants,
ofimmigrants,ofimmigrants,ofimmigrants.â€However,bymanuallyselecting( )thesecond-best
scoringbranch,hecansteertheoutputtobemoreentertaining:â€œTheUnitedStatesofAmericaisa
nationofimmigrants,ofimmigrantsfromallovertheglobe.â€Acceptingthisoutputasthestarting
sequence,hehidesearlierpartsofthetree( )andexecutesfurtherpredictionsteps( ).Atpoints
where the model is stuck or factual information should be integrated into the article, he uses
manualnodeedits( )tosetanewbaselineorenternumbersfromthefactsheet(figure3.2).E.g.,
2https://huggingface.co/gpt2
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:14 Spinneretal.
hechangesthehallucinatedpredictionâ€œWithmorethan1.5millionpeopleâ€toâ€œWithmorethan
331millionpeopleandaGDPof25.035trillionUSDâ€,leadingtothepredictionâ€œ...,America
isthelargesteconomyintheworld.â€Byrepeatingthisprocess,theauthorcompilesadiverting
article.ObservingtheontologyVoronoitreemap( ),hecancheckonthemajorconceptscovered
byhisarticle,whichafterawhileincludeSociety,Politics,Places,andFeelings,leavinghim
satisfiedwiththediversityofhistext(figure3.3).Afterawhile,themodelagainpredictsâ€œThe
USAisanationofimmigrants.â€Theauthordecidestousetheontologicalreplacefunction( ),
whichsuggestsmultipledomains,includingâ€œPersonâ€,â€œSocietyâ€,andâ€œPoliticsâ€(figure3.4).From
thepoliticaldomain,variousreplacementssoundpromising.Theauthorchoosesthesuggestion
â€œdemocracyâ€. He concludes the article with: â€œThe USA is a nation of democracy.â€ The author is
satisfiedwiththeresultanddecidestore-trainthemodeltothetreeâ€™scurrentstate( ).Thisway,
themodelcanbeadaptedtotheauthorâ€™swritingstyleanddomain-specificvocabulary,helpingto
generatemorecoherenttextinthefuture.
5.2 ComparativeAnalysis T3
Theusercanenterthecomparativeanalysisbyinsertingaplaceholderstringintoatreeâ€™sinput
prompt.Itautomaticallyreplacestheplaceholderwithuser-selectedstringinstancesandcreates
anewtreeforeachinstance,displayedasalternativesintheworkspace.Thecomparativemode
allowsforassessingnuancesinthemodelâ€™spredictionsbasedoninputvariations,e.g.,forbias
detection.Thecasestudyoncomparativeanalysisinsection6.1givesseveralexamplesonhow
thecomparativemodecanbeusedtogeneratedifferenthypothesesandevaluatebiasesinmodel
predictions.
5.2.1 WidgetsSupportingComparativeAnalysis
TemplateNode&Multi-Tree W â€”Thecomparativemodeisenteredbycreatingatreewiththe
placeholder<PH>inthestartingsequence,facilitatingcomparisonovertreeswithslightlyvarying
startingsequences.Whenloadingsuchatreeintotheworkspace,thetemplatesequenceisshown
asthebasenode(1.ainfigure4).Theusercannowcreatealistofreplacementsfortheplaceholder
(1.binfigure4).Foreachreplacement,anewtreeisinstantiated,andbeamsearchisexecutedusing
thepredictionparametersconfiguredbytheuser.Toensuredeterminism,temperaturesamplingis
disabledincomparativemode.Theinstancesaredisplayedverticallystacked,withthereplacement
highlightedintherootnodeofeachtree(1.cinfigure4).
Domain-SpecificWordLists W â€”Theusercanselectdomain-specificwordliststoenable
targetedcomparisonbetweenthetreeinstances(2.ainfigure4).Treenodescontainingawordfrom
theselectedwordlistsarehighlightedinthetreewithabadge,denotingitsassociatedlist(2.bin
figure4).Thismakesiteasytospotdifferencesandcommonalitiesbetweenthetrees,e.g.,todetect
genderbiasbetweenmaleandfemalepersonnames(forexhaustiveexamples,seesection6.1).
Theusercaneitherchoosefromasetofpre-definedwordlistsfromdifferentdomains[DeepNLP
2023],coveringtypicalbiasanalysistasks,suchasMale/FemaleOccupations,Appearance,and
Negative/PositiveCharacteristics,oruploadtheirownwordlists.
Forkeyword-basedanalysisintreesofincreasingsize,weincludeasemi-collapsedtreeview,
activatableinthetreestyletoggles W andshowninfigure6.Itonlyexpandsthenodesmatching
toatleastoneoftheselectedwordlists,preservingthetreestructureandallowingtoeasilycompare
acrossworddomains.
UpSetPlot W â€”Visualcomparisonbetweentreeinstancesisfacilitatedbythedomain-specific
wordlists,semanticembeddings,andthepossibilitytosemi-collapsethetree.However,ifhigh
valuesforthepredictionparametersğ‘˜andğ‘›arechosen,thetreecangrowlarge.Therefore,weoffer
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:15
2.b
1.a
1.c)
1.b
2.a
2.c
Fig.4. ThegenerAItorworkspaceincomparativeanalysismode,withtheassociatedwidgetsopened.The
treevisualizationasthecentralelementshowsalternativebeamsearchresultsunderdifferentreplacements
ofthe<PH>node.Wordsoccurringinoneoftheselectedwordlistsarehighlightedinthetree.TheUpsetplot
showstheoverlapoftheselectedwordlistsinthealternativetrees.Theedgesofthetreearecoloredbased
onsentimentanalysis,withredindicatingnegativesentimentandgreenindicatingpositivesentiment.
analternativesummarizationviewoftherelationsbetweenoccurrencesofwordsfromtheword
listsandthetemplatereplacements.WeuseUpSet[Lexetal.2014]plotsforthis,avisualization
technique showing overlaps between set-typed data (2.c in figure 4). Particularly, we visually
highlight which tree instances have overlapping words and, in consequence, also overlapping
wordlists.Eachrowrepresentsoneset,inourcase,onetreeinstance.Treeinstancesthathave
thesameoverlapareshownasonecolumnintheUpSetplot,withgrayconnectednodes.This
columnisonesetintersection,andthenodesthatparticipateinthisintersectionareshownas
ajoinedlist.UnderneaththeUpSetplot,weshowthecurrentlyselectedwordliststhatarepart
ofthesetintersectionandlistthespecificwordsthatappearinthetree,alongwiththeoverall
count of these words. This allows users to get a quick overview of which tree instances have
similarpredictedwordsgroupedbytheirwordlists.E.g.,theusercaninvestigatetheprediction
treeoffemalenamescontainingfemale-connotedoccupationsvs.thepredictiontreeofmalenames
containingmale-connotedoccupations.
5.2.2 WorkflowDemonstration:ComparativeAnalysis
Thefollowingexemplaryworkflowshowcaseshowourworkspacesupportscomparativeanalysis.
Alinguisticexpertisinterestedinexploringbiasesencodedinthemodelâ€™sparameters.Hethus
createsapromptâ€œ<PH>isgreat.Onecouldevensaythatâ€asshowninfigure4.Theplaceholder
<PH> W includeswordssuchasJohn,Jayden,andJessica.Thebeamsearchtreerepresents
thetoptwopredictionsforeachstartingsequence.Theexpertthenselectsmultiplewordliststo
highlighttheoccurrencesofwordsrelatedtoappearance,personnames,andoccupations.These
getmarkedinthetreevisualizationthroughiconsattachedtotheparticulartreenodes.TheUpSet
plotsummarizesthewordoccurrencesshowingthatthefemalepersonnameJessicaisrelatedto
theappearancewordbeautiful;thetwomalepersonnamesarementionedasplayersofsports
games(i.e.,player,quarterback),confirmingthestereotypicalgenderbiasesencodedinthelanguage
model[Luetal.2020].Thecasestudyinsection6.1describesmoredetailsontheworkflow.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:16 Spinneretal.
5.3 ModelAdaptation T4
After adapting the beam search tree as part of tasks T2 and T4, or after identifying desired
sequencesaspartoftasks T1 and T3,theusermightwanttofeedthosechangesbackandfine-
tunethemodel,accordingly.Thiscanbedonebyexecutingthere-traintohere( )functionality
fromthenodecontextmenu W .Thistriggersafine-tuningstepofthemodelinthebackend,
usingthebeamsequenceuptotheselectednodeasinput.Thecurrentmodelstatecanbesaved
atanytimeusingthemodelsnapshotsandâ€“trackingwidget W ,enablingtheusertorestore
fine-tunedmodelsfromprevioussessionsordiscardpotentiallyoverfittedmodelsbyreturningto
anearlierstate.
Section 6.3 provides an extensive evaluation of the fine-tuning functionality. We prove the
sufficiencyofonlyafewdatasamplesâ€“astheyariseinourapproachâ€“toachieveanoticeable
changeintokenprobabilities.Also,weshowthatoverrepeatedfine-tuningwithdifferentsequences
duringtheanalysissession,domainadaptationisachieved.
6 Evaluation
This section provides a three-fold evaluation of our approach. Starting with a case study on
comparativeanalysis T3 insection6.1,weshowcasehowourtoolisusedtogainin-depthlinguistic
insightsonbiasesencodedinthemodel.Itshowshowourtree-in-the-looptechniquegoesbeyond
thetemplate-basedstate-of-the-artinbiasanalysis.Insection6.2,weprovidetwoqualitativeuser
studieswithsixnon-experts Non andfourcomputationallinguists Lin,showcasingtheusabilityof
ourtoolforguidedtextgeneration T2 andcomparativelinguisticanalyses T3,respectively.Finally,
section6.3presentsadetailedevaluationoftheabilitytofine-tuneLLMs T4 usingtherelatively
smallsamplesizeoftrainingdataarisinginourapproach,showingthatdomainadaptationindeed
ispossibleinthedescribedscenarios.Moreover,inourworkâ€œRevealingtheUnwrittenâ€[Spinner,
Kehlbeck,etal.2023],wepresentadditionallyinsightsintostate-of-the-artlinguisticchallenges,
createdwiththegeneraitorinterface.
6.1 CaseStudy:ComparativeAnalysisonSocialBiases
Inthiscasestudy,alinguisticexpert Lin aimstolearnpatternsrelevanttodesigningbiasevaluation
methods.Sincethebiasevaluationsforgenerativelanguagemodelsaresensitivetothedesign
choicesoftemplateprompts[Alnegheimishetal.2022],theexpertâ€™sgoalistofindoutinteresting
linguisticstructuresthatshouldbetakenintoaccountduringsystematicbiasanalysis.Hethususes
thegenerAItorworkspacetoexploredifferentexamples3 andgeneratenewlinguistichypotheses
(c.f.,inductivelearning[R.J.SternbergandK.Sternberg2016]).
Theexpertbeginstheanalysissessionbyexploringthemodelâ€™spotentialgenderbiases.For
thispurpose,hecreatesapromptâ€œAfterreceivingtheirdegree,<PH>wantstobecomeâ€wherebythe
<PH> W standsforaplaceholderofdifferentfemaleandmalepersonnames.Thepredictionsfor
JohnandJessicaarelistedintable1.Theexpertcanconfirmfindingsfromrelatedwork[Luetal.
2020]showingthatlanguagemodelstendtolearnstereotypicalgender-professionassociations,
suchasJohnismorelikelytobecomealawyerandJessicaismorelikelytobecomeanurse.Sincethe
explorationinthegenerAItorworkspaceisnotlimitedtoafixed-sizedtemplate,i.e.,thegenerated
tokensequencescanbeofanylength,theexpertobservesthatthestereotypicalassociationsare
followedbythepersonâ€™sdoubtsregardinghisorherchosenprofession(seetable1).Thismotivates
theexperttoexploreanadditionalprompt,i.e.,â€œThereason<PH>didnotbecomeadoctorwasâ€.
Themodelâ€™soutputshowsanewperspectiveofgenderbias,i.e.,themodelâ€™sassumptionsabouta
3WeshowcasetheseexamplesinareducedonlinedemoofgenerAItor,availableunderhttps://demo.tree.generaitor.dbvis.de.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:17
Prompt Prediction
Afterreceivingtheirdegree,Johnwantstobecomealawyer.Heâ€™snotsureif
Afterreceivingtheir
heâ€™llbeabletoaffordit.
degree,<PH>wantsto
become Afterreceivingtheirdegree,Jessicawantstobecomeanurse,butshedoesnâ€™t
knowhowtodoit.
ThereasonJohndidnotbecomeadoctorwasbecausehewasamanofGod.
Thereason<PH>didnot
ThereasonJessicadidnotbecomeadoctorwasbecauseshewasafraidofthe
becomeadoctorwas
consequencesofheractions.
Thereason,whyMr.Smithwasafraidtobecomeadoctor,wasbecausehe
Thereason,why<PH>was
wasafraidofbeingaccusedofbeingapedophile.
afraidtobecomeadoctor,
was Thereason,whyMrs.Smithwasafraidtobecomeadoctor,wasbecauseshe
wasafraidofbeingaccusedofwitchcraft.
Table1. ExamplesequencesgeneratedinthecomparativemodeofgenerAItorbyinstancingthe<PH>node.
VaryingbetweenmaleandfemalepersonnamesrevealsastrongsocialbiasinGPT-2â€™spredictions.
femalepersonâ€™sfears(i.e.,â€œThereasonJessicadidnotbecomeadoctorwasbecauseshewasafraidof
theconsequencesofheractions.â€).Toinvestigatethisinmoredetail,theexpertdefinesanewprompt
â€œThereason,why<PH>wasafraidtobecomeadoctor,wasâ€.Thegeneratedoutputs(seetable1)
confirmthepreviousobservations.Inparticular,themodelpredictsthatamalepersonisafraidto
becomeadoctorbecauseâ€œhewasafraidofbeingaccusedofbeingapaedophileâ€andthefemaleperson
isafraidbecauseâ€œshewasafraidofbeingaccusedofwitchcraft.â€Theseexamplesmotivatetheexpert
todesignexperimentsforinvestigatingbiasesrelatedtoapersonâ€™sdreams,fears,assumptions,etc.
Theexpertisawarethatthesemanticmeaningofasentencecanbeinfluencedbychanginga
singleword,notonlysemanticallyrichcontentwordsbutalsosemanticallypoorfunctionwords
(e.g.,adverbssuchaseven,orconjunctiveadverbssuchashowever)[CorverandRiemsdijk2001].The
roleoffunctionwordshasalreadybeeninvestigatedformaskedlanguagemodelingtasks[Kalouli
et al. 2022]. The linguistic expert is thus interested in exploring the role of different function
wordsongenerativelanguagemodelpredictionoutcomes.Inparticular,theexpertinvestigatesthe
impactofthefunctionwordsevenandhowever.Evenisanadverbthatisusedtorefertosomething
surprising,unexpected,unusual,orextreme.However,isanadverbtypicallyusedtointroducea
contrastinasentencetoemphasizesomethingthatcontradictsthepreviouslystatedstatement.
The expert first creates a prompt â€œ<PH> is great. One could say thatâ€ whereby the <PH> W
stands for a placeholderof different female and male personnames. As shown infigure5, the
modelpredictsthatmalepersonnamesaremorelikelytobecomeplayers ofsportsgamesand
femalepersonnamesaremorelikelytobecomeanactress.Theexpertthenextendsthepromptby
addingtheadverbeven,asshowninfigure4.Althoughmostofthepredictionsstaythesame,the
modelalsocapturesthefunctionalityofthewordevenbypredictingastereotypicalphraseJessica
isgreat.Onecouldevensaythatsheisthemostbeautifulwomanintheworld.Allsentenceshavea
positivesentiment.Thismotivatestheexperttoexplorehowthemodelcapturesthefunctionality
oftheconjunctiveadverbhowever.Hedefinesthepromptâ€œ<PH>isgreat.However,onecouldsay
thatâ€andobservesthatthemodelcapturesthefunctionalmeaningofhowever sinceitgenerates
sentencesthatcontradicttheprefix<PH>isgreat.Interestingly,mostofthepredictionshavea
similarcontexttothosesentencesgeneratedwiththepromptwithoutthefunctionwordhowever,
i.e.,themodeltalksaboutplayersofsportsgames.Inmostpredictions,however,themodeluses
thenegationnot inordertogeneratethecontrast.Asshowninfigure6,thisalsoleadstochanges
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:18 Spinneretal.
Fig.5. Thepromptâ€œ<PH>isgreat.Onecouldsaythatâ€generatespredictionsmentioningdifferentprofessions.
inthesentimentofthesentences,i.e.,theychangefrompositivetonegativeones.Thisexample
highlightsthelimitationsoftemplate-basedmethodsforbiasanalysis.Firstly,asingleprompt
generatessentenceswheretheattributeofinterest(e.g.,player,jerk)occursatdifferentpositions
(i.e.,atpositions6and7infigure6).Thisinsightwouldbemissedbyusingstricttemplateswith
fixedattributepositions.Secondly,thisexampleshowsthatsomewords(e.g.,adverbs,negations)
changethesemanticmeaningofthesentence.Simplycountingtheoccurrencesofattributessuch
asapersonâ€™soccupationswithoutconsideringtheoccurrencesofnegationswouldgeneratefalse
resultsabouttheencodedbiases.Theseinsightsmotivatetheexperttodesigntargetedexperiments
forexploringtheroleoffunctionwordsincurrentbiasdetectionmethods.
6.2 EvaluationofUsabilityandUsefulness
Weevaluatetheusabilityofoursysteminaqualitativeuserstudywithsixnon-experts Non andfour
linguisticexperts Lin whowerepreviouslyunfamiliarwiththeworkspace.Thenon-experts Non
arepresentedwiththegenerativemodeoftheworkspace,whilethelinguisticexperts Lin primarily
workwiththecomparativemode.Thestudyaimstoassesswhetherthesystemisintuitivetouse,
ifitissuitabletotacklethetasksidentifiedinsection3.3,andgatherfeedbackforpossiblefuture
use-casesandimprovements.Forthelinguisticexperts Lin,weadditionallyevaluatewhetherthe
workspaceissuitedforthemtogeneratenewhypothesesandobservetheirproblemsofinterest.
6.2.1 Non-ExpertStudy
StudySetupâ€”Aftercapturingtheparticipantsâ€™backgroundandpriorexperienceswithlarge
languagemodels,weintroducethemtothegenerativeworkspaceanditsfunctionalities.Wethen
ask them to solve the task described in section 5.1.2 using the workspace in a pair-analytics
session [Arias-Hernandez et al. 2011]. The model loaded in the workspace is the GPT-2 Base
model.Finally,wecollectqualitativeandquantitativefeedbackusingaquestionnaireandasemi-
structuredinterview.Thepair-analyticssessiontook15to25minutes,thewholestudyincluding
theintroductionâ€“andfeedbackquestionnairestook30to45minutesperparticipant.
Resultsâ€”Allstudyparticipantsagreedthattheworkspacewaseasytouse,anditsdesignwas
acknowledgedasbeingsimpleandtidy.Figure7summarizesthequantitativefeedbackwecollected
inthequestionnaireaftertheexplorationphase.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:19
Fig.6. Thepromptâ€œ<PH>isgreat.However,onecouldsaythatâ€generatespredictionsthatincludethenegation
notandinsultwords.
Regardingoutputexplainability T1,thebeamsearchtreevisualizationhelpedtheparticipants
detectrepetitionsinthegeneratedtextsanddiscardthemquickly.Oneparticipantproposedasemi-
automaticpruningmechanismtoremoverepetitionsfromthetree,actinglikeauser-controlled
ğ‘›-gramsuppression[Paulusetal.2017].Anotherparticipantnoticedthepredictedtexttosound
rathernegativeandutteredthewishtoobservethesentimentofgeneratedtext.Weimplemented
thisfeedbackbyaddingautomaticsentimentanalysisandâ€“visualizationtothebeamsearchtree,
as shown in figure 1. Concerning the generative task T2, the alternative paths shown in the
beamsearchtree,themanualeditingfunctionality,andtheontologysuggestionsweredescribed
ashelpfultocreatenewideasandâ€œkeeptheballrolling.â€Whiletheparticipantslikedthatthe
workspace allowed them to generate text in a guided manner, they also critiqued the manual
efforttheyhadtoputintotheprocess.Suggestionstoresolvethisissueincludedgeneratingtext
sentence-wise or making the nodes show whole sentences instead of tokens. When manually
adaptingmodeloutputs T4,oneparticipantdescribedthemodelasâ€œworkingagainsthimwhile
steering[theoutputs].â€Totacklethisissueandmakedomainadaptationpermanentinthemodel,
weimplementedthefine-tuningfunctionality W ,whichwedidnotintroduceinthestudy
duetotimeconstraints.
6.2.2 ComputationalLinguistStudy
StudySetupâ€”Aftercapturingtheparticipantsâ€™background,priorexperienceswithlargelanguage
models,andlinguisticresearchfocus,weintroducethemtothecomparativeworkspaceandits
functionalities.Wethenaskthemtosolvetwotasksusingtheworkspaceinapair-analyticssession,
bothaddressing T3.ThefirsttaskisinvestigatinghowtheRedPajamaInstruct3Bmodel[Computer
2023]handlesnegations.ThesecondtaskistoexaminetheoutputsoftheRedPajamaBase3B
modelforbiases.Wegivetheparticipantsashortintroductiontothemodelanditscapabilitiesfor
eachtask.Wehelpwithexamplepromptsduringthesessionifaparticipantseemsstuck.Thetasks
deliberatelyfocusonanopen-endedexplorationtoenabletheparticipantstoevaluategenerAItorâ€™s
applicabilitytotheirownresearchandtogeneratenewhypotheses.Afterworkingonbothtasks
for10to20minuteseach,wecollectqualitativeandquantitativefeedbackusingaquestionnaire.
Thepair-analyticssessiontook35to55minutes,andthewholestudy,includingtheintroductionâ€“
andfeedbackquestionnaires,took50to70minutesperparticipant.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:20 Spinneretal.
EaseofUse InterfaceSimplicity HumanControl ProblemsofInterest NewHypotheses
(Non+Lin) (Non+Lin) (Non) (Lin) (Lin)
5 8 4 2 2
7
4
6 3
3 5
4 2 1 1
2 3
2 1
1
1
0 0 0 0 0
+ ++ + ++ + ++ + ++ + ++
âˆ’âˆ’ âˆ’ â—¦ âˆ’âˆ’ âˆ’ â—¦ âˆ’âˆ’ âˆ’ â—¦ âˆ’âˆ’ âˆ’ â—¦ âˆ’âˆ’ âˆ’ â—¦
Fig.7. Resultsofthequantitativepartoftheuserstudy.Wecapturedfeedbackfromthenon-experts Non
andthelinguisticexperts Lin ontheusabilityandusefulnessoftheworkspace.
QualitativeResultsâ€”Allparticipantsagreedthattheworkspacewasintuitive,asthequantitative
resultsinfigure7show.Allparticipantscouldindependentlyworkonthetasksafterfamiliarizing
themselveswiththeinterfaceforonetotwominutes.
Overall,thebeamsearchtreetoexplainthemodelâ€™soutputswaswellreceived,especiallyhowit
organizesprobabilitiesandalternativeoutputs.Oneparticipantshowedinterestinâ€œthediscrepancy
between probabilities,â€ identifying high uncertainty where â€œvariation[s] [are] relatively equal
in probability.â€ Another participant critiqued that if all tokens have a low probability (i.e., the
probabilitydistributionisrelativelyflat),thetop-ğ‘˜ outputsshownintheBSTweremisleadingdue
tootheroutputswithsimilarprobabilitybeingomitted.Asasolution,theyproposedtoâ€œshow
[...]thedistributionacrossthetop500orwhatever,maybeweightedbyprobabilityâ€uponuser
request.Thekeywordhighlightingandsemanticcoloring W wasratedhelpfultoâ€œtogetan
overviewjustbylookingatthehighlightedwords.â€Theplaceholdernode W wasdescribed
asâ€œveryhelpfulinordertocompareoutputsresultingfromdifferentinputsâ€andwasintensively
usedbythreeoftheparticipants.Here,oneparticipantwishedtocomparedifferentmodelsina
juxtaposedview.Thewordlists W andtheupsetplot W wereonlyusedrarelybytwoofthe
participantsandignoredbytheothers.
Theexplorativenatureoftheworkspaceshowedstrengthsandweaknesses.Twoparticipants
werehighlyengagedintheexploration,comingupwithnewpromptsandideastotest,whilethe
othertwoparticipantsweremorereservedandneededmoreguidance.
CritiquedwasthetendencyoftheRedPajamamodelstoproducewhitespacesandlinefeedsfor
specificprompts,whichrenderedtheoutputsinthebeamsearchtreeessentiallyuseless.Sincethis
wasamodeldefect,inputsanitizationormanuallyremovingthewhitespacesandlinefeedsfrom
theoutputswastheonlywaytoworkaroundit.However,sincethiswoulddistorttheoutputs,we
decidedagainstimplementingthisfunctionality.
6.3 QuantitativeEvaluationofModelAdaptation
Besidesoutputsteeringthroughselection,manualedits,orautomatedsuggestionsbasedonword
ontologies,oursystemsupportsmodelfine-tuningbasedonthealteredoutputswiththegoalof
adaptingthemodeltothehumanâ€™sstyleofwritingandtospecificdomains.Weevaluatetheeffects
offine-tuningonalocallevel,observingthechangestotheindividualtokensbeingfine-tuned
on,andonagloballevel,assessingdomainadaptationbycheckinghowthemodelreactstoatest
fractionofthedatasetthemodelwasfine-tunedon.generAItorsfine-tuningfunctionality(c.f.,
W )andthefollowingexperimentsusetheAdamW[LoshchilovandHutter2017]optimizer
withalearningrateof5Ã—10âˆ’5.TheexperimentsareperformedwiththeGPT-2Basemodel.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:21
Seqence Initial 1Step 2Steps
ğ‘ 0.000012 0.000181 0.010252
Afteryouâ€™vewatchedthismovieyouâ€™llbedeaf
ğ‘– 1964 466 13
ğ‘ 0.001175 0.002569 0.009681
Behindthetreeshadhiddenagiantgnome
ğ‘– 143 58 10
ğ‘ 0.046493 0.260536 0.828726
Theamericanbullfrogisthelargestanimal
ğ‘– 4 1 1
Table2. Targettokenprobabilityğ‘andindexpositionğ‘–afterfine-tuningondifferentsequencesforoneand
twosteps,respectively.Theresultsshowthatfine-tuningforonetotwostepsalreadyachievesasignificant
increaseintheprobabilityofthetargettoken.
LocalAdaptationâ€”Afterfine-tuningtoaspecifictreenode,thenodeâ€™sprobabilityfollowing
theprevioussequenceshouldincrease.Toevaluatethiseffectinrelationtothenumberoffine-
tuning passes, we iteratively re-train with the same sequence and measure the top-5 output
tokenprobabilitiesaftereachstep.Figure8ashowsthechangeintokenprobabilitiesafterfine-
tuningfortwoâ€“andfourstepsonthesequenceâ€œAfteryouâ€™vewatchedthismovie,youâ€™llbedeafâ€,
whereâ€œdeafâ€ isthetargettokenmanuallyinsertedbytheuser.Initially,ithasaprobabilityof
ğ‘ (deaf) =0.000012whichincreasestoğ‘ (deaf) =0.000834aftertwoandğ‘ (deaf) =0.315274after
0 2 4
foursteps,correspondingtotheindexpositionsğ‘– (deaf) =1964,ğ‘– (deaf) =158,andğ‘– (deaf) =1.
0 2 4
Otherexamplesshowsimilarresults,asdepictedintable2.Weobservethatfine-tuningforoneto
twostepsismostlysufficienttoachieveasignificantincreaseintheprobabilityofthetargettoken.
Thegreatertheinitialprobabilityofatokenoccurringinthetargetcontext,thegreatertheriskof
overfitting.However,wedidnotobservethemodellosingitsabilitytogeneralizetoothercontexts
despiteourexperimentsâ€™strongfocusonthetargettoken.Itshouldbenotedthatwecanalready
perceiveeffectsofglobaladaptationinfigure8a:thesemanticcontextoftheinputsentencemakes
thewordâ€œhookedâ€ fitbetterthanthewordâ€œableâ€,leadingtoashiftoftheirprobabilities.
GlobalAdaptationâ€”Thenumberoftrainingsamplesgeneratedusingourworkspacewilllikely
stayfarbehindthenumberofsamplesindatasetstypicallyusedtofine-tunemodels,suchasthe
IMDB [Maas et al. 2011] (â‰ˆ 50ğ‘˜ samples) or MultiNLI (â‰ˆ 433ğ‘˜ samples) datasets. Thus, in the
following,weevaluatethemodelâ€™scapabilitytolearndomainspecificknowledgefroma(small)set
oftrainingsamples.Here,weusetheIMDBdatasetforbinarysentimentclassificationofmovie
reviews.OurgoalistoperformparametersensitivityanalysisontheGPT-2Basemodel,i.e.,evaluate
howthemodeladaptstodataset-specifictargettokensafterfine-tuningforavaryingnumberof
steps.Weusetheperplexityevaluationmetric[Jelineketal.1977]tomeasuredomainadaption.To
seetheeffectofthesamplesizeonthemodelâ€™sperformance,wefirstsplitthedatasetintotraining
andtestsubsets(50%,i.e.,25.000datapointseach).Werepeatedlyfine-tunethemodelfromscratch
for100runs,whereweincreasethenumberoftrainingsamplesğ‘›by20ineachrun.Thismeanswe
fine-tunethebasemodelforğ‘› = {20,40,...,2000}stepswhilemeasuringtheperplexityonboth
theğ‘›trainingsamplesandthefulltestsubsetforeachfine-tunedmodelversion.Thisallowsusto
verifythemodelâ€™scapabilitytolearndomain-specificpropertiesfromthedatapointsthatithas
seenduringthefine-tuning,aswellasitsgeneralizabilitytounseensamples.Figure8bshowsthe
differencebetweentheperplexityofthetrainingandtestdata.Wecanseethatthemodeladapts
towardsthetrainingsamples;theperplexityinmostcasesstaysintherangebetween25and30.
Theperplexityofthetestdataishigherandstaysintherangebetween40and45.Nevertheless,
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:22 Spinneretal.
45
40
35 T Tera si tn
30
25
0 250 500 750 1000 1250 1500 1750 2000
# Training Samples
(a)Measuringthemodelâ€™slocaladaptationtothetarget (b)Measuringthemodelâ€™sglobaladaptation
tokenâ€œdeafâ€after0,2,and4stepsoffine-tuning. totheIMDBMovieReviewsdataset.
Fig.8. Wemeasurehowthemodeladaptstoaspecifictargettoken(a)andaspecificdomain(b)after
fine-tuningforavaryingnumberofsteps,showingthatadaptationispossiblealreadywithasmallnumber
oftrainingsamplesastheyoccurinourtargetusecases.
wecanalsoseeageneraltrend,wheretheperplexityofboththetestandtrainingdatadecreases
withtheincreasedsizeofthetrainingsample,andthemodelisabletoadapttothegivendomain
alreadywithafewhundredsoftrainingdatapoints.
7 Discussion
In the following, we discuss our rationales for the presented approach, summarize the most
importanttake-homemessages,anddiscusscurrentlimitationsandfutureresearchopportunities.
7.1 RationalesofOurBST-BasedApproachandTake-HomeMessages
LeveragingtheInherentUnderstandingofTextToExplainLLMsâ€”Thewayalanguage
modelgenerateslanguageisoftenmisinterpretedbyusers,leadingtofalserationalizationsoftheir
outputsbyattributinganunderstandingofthetextâ€™smeaningtothemodel[Sevastjanovaand
El-Assady2022].Therefore,explainabilityoflanguagemodeloutputsiscrucialtocorrectlyassess
themodelâ€™scapabilitiesandidentifyundesiredfeaturesinthegeneratedtext,suchasrepetitionsor
biases.Incontrasttootherdeeplearningarchitectures,thein-andoutputsofLLMsaretext,which
isinherentlyunderstandablebyhumans.Thisaccessibilityofthemodelâ€™sinâ€“andoutputsmakesit
agoodcandidateforexplainingitsbehavior.
ExposingtheBeamSearchTreetoExplainDecisionProcessesâ€”Beamsearchbeingthemost
commonalgorithmtosampletextfromtheLLMâ€™spredictions,combinedwiththeeasyunderstand-
abilityoftheresultingtreetonon-experts,makesitanaturalchoicetoexposethebeamsearchtree
toexplainthemodelâ€™sdecisionprocess.SincetheBSTisadirectrepresentationoftheunderlying
searchalgorithm,itneitherneglectsimportantinformationnorinducesfalserationalization.It
is,therefore,avaluabletoolforexplainingthemodelâ€™sbehaviorandcommunicatinginformation
inthemodelâ€™soutputtotheuser,suchasuncertainties,alternatives,orpatterns,e.g.,repeating
content.
Tree Augmentations â€” Issues with the BSTâ€™s complexity and information overload can be
addressed by providing additional visualizations, interactions, and analysis tools. Simple tree
transformations,suchasthetreecollapseandâ€“filterfunctionalities,allowresolvingscalability
issues with large trees. Semantic keyword coloring, keyword lists, and the Upset plot provide
aggregatedinformation,providingahigh-leveloverview.Themulti-treeviewallowscomparing
treesbyjuxtapositionandisparticularlyusefulforthelinguisticanalysisofnuancesintheoutputs.
Finally,theontologyVoronoitreemapandtheontologyreplacefunctionalitycombinethekeywords
withontologicalknowledgethemodelcannotdeliver.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
ytixelpreP-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:23
ProvidingAugmentationsthroughModularWidgetsâ€”Differenttoolsandaugmentations
are relevant depending on the tasks a user wants to solve. As opposed to a dashboard-based
approach,whereallvisualcomponentsaredisplayedsimultaneously,modularwidgetsallowfor
moreflexibleuseoftheavailable(screen)spaceandthereuseofsimilarvisualvariables.This,in
return,requirescarefulcategorizationoftheavailablewidgetsandusefulpresetsforeachtaskso
thatvisualvariables(e.g.,colororshape)areusedonlyoncebysimultaneouslyactivewidgetsto
avoidconfusion.
UsefulnessforNon-TechnicalUsersandLinguisticExpertsâ€”Asourevaluationshows,the
aforementionedmechanismsenablepowerfulmodesofLLMoutputanalysis.Non-technicalusers
canusetheBSTtounderstandthemodelâ€™sdecisionprocessandforinformedtextgeneration.Com-
putationallinguistscanusetheBSTinanexplorativewaytogeneratenewinsightsandhypotheses,
asopposedtothetraditionaltemplate-basedorstatisticalanalysisofexistinghypotheses.
7.2 LimitationsandFutureWork
ApplicabilitytoState-of-the-ArtModelsâ€”Inthiswork,wedemonstrateourapproachusing
GPT2andBloom.Beyondthat,Spinner,Kehlbeck,etal.[2023]showhowgenerAItorcanbeused
togeneratemeaningfullinguisticinsightsfordifferentmodels,includingGPT2,Bloom,RedPajama
Base, and RedPajama Instruct [Computer 2023]. We observe that our approach becomes more
potentwithlargermodelsastheoutputdiversityincreasesandthealternativesintheBSTbecome
more meaningful. In general, our approach applies to causal language transformers if they (1)
provideaccesstothehigh-dimensionaltoken-wiseembeddingsand(2)outputtheprobabilitiesof
thenexttop-ğ‘˜ tokens.WhilethesecondrequirementisimperativetogeneratetheBST,thefirst
requirementisonlyneededfortheembedding-basedwidgets.
ThismeansthatlargepartsofourapproacharetransferabletoGPT4asthecurrentstate-of-
the-artincausallanguagemodeling.TheOpenAIAPIprovidesaccesstothelogprobsofthetop-ğ‘˜
tokens,whichcanbeusedtogeneratetheBST.Despitethehigh-dimensionalembeddingsnotbeing
availableforGPT4,theembeddingwidgetscanstillbepoweredfromtheembeddingsproducedby
othertransformers.Sevastjanova,Kalouli,etal.[2022]andKehlbecketal.[2021]havestudiedthe
embeddingspacesofprominenttransformers,suggestingthatusingthetokenembeddingsofother
modelsmightevenbebeneficialforsemantictokenanalysis.
TransferofOurProposedTechniquestoExistingInterfacesâ€”Ourapproachtargetsspecific
usergroups.However,weenvisionsomemeansofexplainabilityembeddedintotheprominent
chatâ€“andcompletion-basedinterfaces,likeChatGPTorGitHubCopilot4.Currently,ChatGPTonly
outputstext,andeachadaptationhastobetriggeredbyrefiningthepromptinthehopethatthe
desiredoutputwillbegenerated.Thiscanbefrustrating,especiallyforhallucinatedtextparts,
wherenoeasysolutionforeditingisavailable.Here,showingalternativeoutputsandprovidingthe
userwithexplainabilityonthelikelinessofsequencescouldbringhugeadvantages.WhileGitHub
Copilotdoesshowalternatives,thosealternativesremainunexplained.Here,showingprobabilities
orannotatingstructuralelements,c.f.,keywordextraction(section4.1)andâ€“coloring W ,could
furtherimprovetheusefulness.
Bridging Between Explorative and Statistical Analysis â€” Our approach is explorative in
nature,allowinguserstogeneratenewhypothesesandinsights.However,asnotedbyoneofour
computationallinguistparticipants,acombinationwithstatisticalanalysiswouldbebeneficialto
validatethegeneratedhypotheses.Therefore,weenvisionatighterintegrationofourapproach
withstatisticalanalysistools,e.g.,tovalidatethegeneratedhypotheseswithstatisticaltests.Once
thisintegrationisestablished,annotatingtheBSTbrancheswithstatisticalmetricscouldbridgethe
4https://github.com/features/copilot
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:24 Spinneretal.
gapbetweenexplorativeandstatisticalanalysis.Forthecurrentversionofthesystem,wedecided
againstannotatingthebrancheswithlinguisticmetricstopreventtheuserfromdrawingfalse
generalizationsfromlocalobservations.
SupportforModelDevelopersâ€”Ourinterfacealsoprovidesinformationrelevanttomodel
developers.However,formodeldebuggingandrefinement,additionaltools,e.g.,toobservethe
effectsoffine-tuningorinvestigatecommonerrorsinmodelanddata,mightbeneeded.
ExtensiontoOtherTasksandUserGroupsâ€”Thepresentedwidgetsarewell-roundedforthe
describedtasksandtargetusergroups.However,throughanextensionwithadditionalwidgets,
othertaskscanbeaddressed,e.g.,informedtextsummarizationforstudents.
ComparisonAcrossModelsâ€”Whileourapproachallowsloadingdifferentgenerativelanguage
transformers,comparativeanalysisisyetonlypossiblebetweenprompts.However,thisisnota
limitationofourproposedtree-in-the-loopapproachandwillbeimplementedinfutureiterations
ofthesystem,enablingadditionalmodesofanalysis.
8 Conclusion
Wepresentthetree-in-the-loopparadigm,puttingthebeamsearchtreeinthecenterofthegenerAI-
torVisualAnalyticstechniqueforlanguagemodelexplainability,comparability,andadaptability.In
ourtechnique,weleveragethebeamsearchtreetoexplainthemodelâ€™sdecisionprocess,compare
modeloutputs,andadapttheoutputstouserpreferences.Enhancingthetreewithtask-specific
widgetscreatessynergiesbetweenthetreeandtargetedvisualizations,interactions,andin-situ
explanations. Finally, we provide a three-fold evaluation of our approach. First, we assess the
applicabilityofourapproachinacasestudy,showcasingourtechniqueâ€™scomparativecapabilities.
Particularly,weshowhowtheinterplaybetweenthebeamsearchtreeandwidgetsenablesnew
analysismodes,leadingtointerestinglinguisticinsightsonmodelbiases.Second,weperformtwo
qualitativeuserstudies,thefirstwithsixnon-expertsandthesecondwithfourcomputational
linguists,provingtheusabilityofourapproachfortextgenerationtasksandlinguisticanalyses.
Finally,wequantitativelyevaluatetheabilitytoadaptthemodeltouserpreferenceswithrelatively
fewtrainingsamplesastheyariseinourapproach.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:25
References
D.Alba.2022.â€œOpenAIChatbotSpitsOutBiasedMusings,DespiteGuardrails.â€Bloomberg.RetrievedMar.30,2023from
https://www.bloomberg.com/news/newsletters/2022-12-08/chatgpt-open-ai-s-chatbot-is-spitting-out-biased-sexist-r
esults.
S.Alnegheimish,A.Guo,andY.Sun.2022.â€œUsingNaturalSentencePromptsforUnderstandingBiasesinLanguageModels.â€
In:ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human
LanguageTechnologies.AssociationforComputationalLinguistics,Seattle,UnitedStates,2824â€“2830.
R.Arias-Hernandez,L.T.Kaastra,T.M.Green,andB.Fisher.2011.â€œPairAnalytics:CapturingReasoningProcessesin
CollaborativeVisualAnalytics.â€In:HawaiiInternationalConferenceonSystemSciences.IEEE.
M.El-Assady,W.Jentner,R.Kehlbeck,U.Schlegel,R.Sevastjanova,F.Sperrle,T.Spinner,andD.Keim.2019.â€œTowardsXAI:
StructuringtheProcessesofExplanations.â€In:ACMCHI2019Workshop:Humanâ€“CenteredMachineLearningPerspectives.
M.El-Assady,R.Kehlbeck,Y.Metz,U.Schlegel,R.Sevastjanova,F.Sperrle,andT.Spinner.2022.â€œSemanticColorMapping:
APipelineforAssigningMeaningfulColorstoText.â€4thIEEEWorkshoponVisualizationGuidelinesinResearch,Design,
andEducation.
M.El-Assady,R.Sevastjanova,D.Keim,andC.Collins.2018.â€œThreadReconstructor:ModelingReply-ChainstoUntangle
ConversationalTextthroughVisualAnalytics.â€ComputerGraphicsForum,37,3,351â€“365.
D.Bahdanau,K.Cho,andY.Bengio.2014.â€œNeuralMachineTranslationbyJointlyLearningtoAlignandTranslate.â€arXiv:
1409.0473.
Y.Bengio,R.Ducharme,andP.Vincent.2000.â€œAneuralprobabilisticlanguagemodel.â€AdvancesinNeuralInformation
ProcessingSystems,13.
S.L.Blodgett,S.Barocas,H.DaumÃ©III,andH.Wallach.2020.â€œLanguage(Technology)isPower:ACriticalSurveyofâ€œBiasâ€
inNLP.â€In:ProceedingsoftheAssociationforComputationalLinguistics.AssociationforComputationalLinguistics,
Online,5454â€“5476.
T.B.Brownetal..2020.â€œLanguageModelsareFew-ShotLearners.â€arXiv:2005.14165.
A.Caliskan,J.J.Bryson,andA.Narayanan.2017.â€œSemanticsderivedautomaticallyfromlanguagecorporacontainhuman-
likebiases.â€Science,356,6334,183â€“186.
J.Camacho-ColladosandR.Navigli.2017.â€œBabelDomains:Large-ScaleDomainLabelingofLexicalResources.â€In:Proceedings
ofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:ShortPapers.Vol.2.
AssociationforComputationalLinguistics.
R.Campos,V.Mangaravite,A.Pasquali,A.Jorge,C.Nunes,andA.Jatowt.2020.â€œYAKE!Keywordextractionfromsingle
documentsusingmultiplelocalfeatures.â€InformationSciences,509,257â€“289.
C.Chen,K.Lin,andD.Klein.2021.â€œConstructingTaxonomiesfromPretrainedLanguageModels.â€In:Proceedingsofthe
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics.
T.Computer.2023.RedPajama:AnOpenSourceRecipetoReproduceLLaMAtrainingdataset.https://github.com/togethercom
puter/RedPajama-Data.(2023).
S.ConiaandR.Navigli.2020.â€œConception:Multilingually-Enhanced,Human-ReadableConceptVectorRepresentations.â€In:
Proceedingsofthe28thInternationalConferenceonComputationalLinguistics.InternationalCommitteeonComputational
Linguistics.
N.CorverandH.vanRiemsdijk.2001.Semi-lexicalCategories:TheFunctionofContentWordsandtheContentofFunction
Words.DeGruyterMouton,Berlin,NewYork.
M.Danilevsky,K.Qian,R.Aharonov,Y.Katsis,B.Kawas,andP.Sen.2020.â€œASurveyoftheStateofExplainableAIfor
NaturalLanguageProcessing.â€In:Proceedingsofthe1stConferenceoftheAsia-PacificChapteroftheAssociationfor
ComputationalLinguisticsandthe10thInternationalJointConferenceonNaturalLanguageProcessing.Associationfor
ComputationalLinguistics,Suzhou,China,447â€“459.
S.Dathathri,A.Madotto,J.Lan,J.Hung,E.Frank,P.Molino,J.Yosinski,andR.Liu.2019.â€œPlugandPlayLanguageModels:
ASimpleApproachtoControlledTextGeneration.â€https://arxiv.org/abs/1912.02164.
DeepNLP.2023.BiasinNLP.[Online;accessed15.Nov.2023].(2023).RetrievedNov.15,2023fromhttps://github.com/cisnl
p/bias-in-nlp.
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova.2018.â€œBERT:Pre-trainingofDeepBidirectionalTransformersforLanguage
Understanding.â€arXiv:1810.04805.
W.Du,Z.M.Kim,V.Raheja,D.Kumar,andD.Kang.2022.â€œRead,Revise,Repeat:ASystemDemonstrationforHuman-in-
the-loopIterativeTextRevision.â€In:ProceedingsoftheFirstWorkshoponIntelligentandInteractiveWritingAssistants.
AssociationforComputationalLinguistics.
K.Ethayarajh.2019.â€œHowContextualareContextualizedWordRepresentations?ComparingtheGeometryofBERT,ELMo,
andGPT-2Embeddings.â€In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProceedingsandthe
InternationalJointConferenceonNaturalLanguageProcessing.ACL,HongKong,China,55â€“65.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:26 Spinneretal.
I.Garrido-MuÃ±oz,A.Montejo-RÃ¡ez,F.MartÃ­nez-Santiago,andL.A.UreÃ±a-LÃ³pez.2021.â€œAsurveyonbiasindeepNLP.â€
AppliedSciences,11,7,3184.
A.GattandE.Krahmer.2018.â€œSurveyoftheStateoftheArtinNaturalLanguageGeneration:Coretasks,applicationsand
evaluation.â€JournalofArtificialIntelligenceResearch,61,65â€“170.
S.Gehrmann,H.Strobelt,R.Kruger,H.Pfister,andA.M.Rush.2019.â€œVisualInteractionwithDeepLearningModels
throughCollaborativeSemanticInference.â€IEEETransactionsonVisualizationandComputerGraphics,1â€“1.
J.Hartmann,M.Heitmann,C.Schamp,andO.Netzer.2021.â€œThePowerofBrandSelfies.â€JournalofMarketingResearch.
X.He.2021.â€œParallelRefinementsforLexicallyConstrainedTextGenerationwithBART.â€In:ProceedingsoftheConference
onEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics.
J.HowardandS.Ruder.2018.â€œUniversalLanguageModelFine-tuningforTextClassification.â€In:Proceedingsofthe56th
AnnualMeetingoftheAssociationforComputationalLinguistics.Vol.1.AssociationforComputationalLinguistics,
Melbourne,Australia,328â€“339.
Z.Hu,Z.Yang,X.Liang,R.Salakhutdinov,andE.P.Xing.2017.â€œTowardControlledGenerationofText.â€In:Proceedingsof
the34thInternationalConferenceonMachineLearning(ProceedingsofMachineLearningResearch).Ed.byD.Precup
andY.W.Teh.Vol.70.PMLR,1587â€“1596.
X.HuaandL.Wang.2020.â€œPAIR:PlanningandIterativeRefinementinPre-trainedTransformersforLongTextGenera-
tion.â€In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor
ComputationalLinguistics.
J.Huang,Y.Xie,Y.Meng,Y.Zhang,andJ.Han.2020.â€œCoRel:Seed-GuidedTopicalTaxonomyConstructionbyConcept
LearningandRelationTransferring.â€In:Proceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledge
DiscoveryandDataMining.ACM.
F.Jelinek,R.L.Mercer,L.R.Bahl,andJ.K.Baker.1977.â€œPerplexityâ€”ameasureofthedifficultyofspeechrecognitiontasks.â€
TheJournaloftheAcousticalSocietyofAmerica,62,S1,S63â€“S63.
Z.Ji,N.Lee,R.Frieske,T.Yu,D.Su,Y.Xu,E.Ishii,Y.J.Bang,A.Madotto,andP.Fung.2023.â€œSurveyofHallucinationin
NaturalLanguageGeneration.â€ACMComputingSurveys,55,12,1â€“38.
M.Jiang,X.Song,J.Zhang,andJ.Han.2022.â€œTaxoEnrich:Self-SupervisedTaxonomyCompletionviaStructure-Semantic
Representations.â€In:ProceedingsoftheACMWebConference.ACM.
J.Johnson,M.Douze,andH.JÃ©gou.2019.â€œBillion-scalesimilaritysearchwithGPUs.â€IEEETransactionsonBigData,7,3,
535â€“547.
A.-L.Kalouli,R.Sevastjanova,C.Beck,andM.Romero.2022.â€œNegation,Coordination,andQuantifiersinContextualized
LanguageModels.â€In:Proceedingsofthe29thInternationalConferenceonComp.Ling.InternationalCommitteeon
ComputationalLinguistics,Gyeongju,RepublicofKorea,3074â€“3085.
R.Kehlbeck,R.Sevastjanova,T.Spinner,T.StÃ¤hle,andM.El-Assady.2021.â€œDemystifyingtheEmbeddingSpaceofLanguage
Models.â€ProceedingsoftheWorkshoponVisualizationforAIExplainability(VISxAI).https://bert-vs-gpt2.dbvis.de/.
A.Lauscher,T.Lueken,andG.GlavaÅ¡.2021.â€œSustainableModularDebiasingofLanguageModels.â€In:Findingsofthe
AssociationforComputationalLinguistics:EMNLP.AssociationforComputationalLinguistics,PuntaCana,Dominican
Republic,4782â€“4797.
Y.LeCun.2023.â€œDoLanguageModelsNeedSensoryGroundingforMeaningandUnderstanding?â€ThePhilosophyofDeep
Learning.(2023).https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi.
J.Lee,J.-H.Shin,andJ.-S.Kim.2017.â€œInteractiveVisualizationandManipulationofAttention-basedNeuralMachine
Translation.â€In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations.
AssociationforComputationalLinguistics,Copenhagen,Denmark,121â€“126.
M.Lewis,Y.Liu,N.Goyal,M.Ghazvininejad,A.Mohamed,O.Levy,V.Stoyanov,andL.Zettlemoyer.2020.â€œBART:Denoising
Sequence-to-SequencePre-trainingforNaturalLanguageGeneration,Translation,andComprehension.â€In:Proceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics.AssociationforComputationalLinguistics,
Online,7871â€“7880.
A.Lex,N.Gehlenborg,H.Strobelt,R.Vuillemot,andH.Pfister.2014.â€œUpSet:VisualizationofIntersectingSets.â€IEEE
TransactionsonVisualizationandComputerGraphics,20,12,1983â€“1992.
J.Li,T.Tang,W.X.Zhao,andJ.-R.Wen.2021.â€œPretrainedLanguageModelforTextGeneration:ASurvey.â€In:Proceedings
ofthe30thInternationalJointConferenceonArtificialIntelligence.InternationalJointConferenceonArtificialIntelligence
Organization.
Z.Li,Y.Wang,X.Yan,W.Meng,Y.Li,andJ.Yang.2022.â€œTaxoTrans.â€In:Proceedingsofthe28thACMSIGKDDConferenceon
KnowledgeDiscoveryandDataMining.ACM.
P.P.Liang,C.Wu,L.-P.Morency,andR.Salakhutdinov.2021.â€œTowardsunderstandingandmitigatingsocialbiasesin
languagemodels.â€In:InternationalConferenceonMachineLearning.PMLR,6565â€“6576.
I.LoshchilovandF.Hutter.2017.â€œFixingWeightDecayRegularizationinAdam.â€CoRR,abs/1711.05101.http://arxiv.org/abs
/1711.05101 arXiv:1711.05101.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:27
K.Lu,P.Mardziel,F.Wu,P.Amancharla,andA.Datta.2020.â€œGenderbiasinneuralnaturallanguageprocessing.â€Logic,
Language,andSecurity:EssaysDedicatedtoAndreScedrovontheOccasionofHis65thBirthday,189â€“202.
A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts.2011.â€œLearningWordVectorsforSentimentAnalysis.â€
In:Proceedingsofthe49thAnnualMeetingoftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics,Portland,Oregon,USA,142â€“150.
L.McInnes,J.Healy,N.Saul,andL.Grossberger.2018.â€œUMAP:UniformManifoldApproximationandProjection.â€The
JournalofOpenSourceSoftware,3,29,861.
N.Mehrabi,F.Morstatter,N.Saxena,K.Lerman,andA.Galstyan.2021.â€œAsurveyonbiasandfairnessinmachinelearning.â€
ACMComputingSurveys,54,6,1â€“35.
C.Metz.2022.â€œTheNewChatbotsCouldChangetheWorld.CanYouTrustThem?â€NewYorkTimes.https://www.nytimes.c
om/2022/12/10/technology/ai-chat-bot-chatgpt.html.
S.Mishra,D.Khashabi,C.Baral,andH.Hajishirzi.2022.â€œCross-TaskGeneralizationviaNaturalLanguageCrowdsourcing
Instructions.â€In:Proceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics.Associationfor
ComputationalLinguistics.
A.Moro,A.Raganato,andR.Navigli.2014.â€œEntityLinkingmeetsWordSenseDisambiguation:aUnifiedApproach.â€
TransactionsoftheAssociationforComputationalLinguistics,2,231â€“244.
M.Nadeem,A.Bethke,andS.Reddy.2021.â€œStereoSet:Measuringstereotypicalbiasinpretrainedlanguagemodels.â€In:
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJoint
ConferenceonNaturalLanguageProcessing.Vol.1.AssociationforComputationalLinguistics,Online,5356â€“5371.
R.NavigliandS.P.Ponzetto.2012.â€œBabelNet:Theautomaticconstruction,evaluationandapplicationofawide-coverage
multilingualsemanticnetwork.â€ArtificialIntelligence,193,217â€“250.
OpenAI.2023.GPT-4TechnicalReport.(2023).arXiv:2303.08774.
L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,A.Ray,J.Schulman,
J.Hilton,F.Kelton,L.Miller,M.Simens,A.Askell,P.Welinder,P.F.Christiano,J.Leike,andR.Lowe.2022.â€œTraining
languagemodelstofollowinstructionswithhumanfeedback.â€In:AdvancesinNeuralInformationProcessingSystems.
Ed.byS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh.Vol.35.CurranAssociates,Inc.,27730â€“27744.
V.PadmakumarandH.He.2022.â€œMachine-in-the-LoopRewritingforCreativeImageCaptioning.â€In:Proceedingsofthe
ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.
AssociationforComputationalLinguistics.
R.Paulus,C.Xiong,andR.Socher.2017.â€œADeepReinforcedModelforAbstractiveSummarization.â€CoRR,abs/1705.04304.
http://arxiv.org/abs/1705.04304 arXiv:1705.04304.
P.vonPlaten.2020.Howtogeneratetext:usingdifferentdecodingmethodsforlanguagegenerationwithTransformers.[Online;
accessed29.Mar.2023].(2020).RetrievedMar.29,2023fromhttps://huggingface.co/blog/how-to-generate.
L.Qin,V.Shwartz,P.West,C.Bhagavatula,J.D.Hwang,R.L.Bras,A.Bosselut,andY.Choi.2020.â€œBacktotheFuture:
UnsupervisedBackprop-basedDecodingforCounterfactualandAbductiveCommonsenseReasoning.â€In:Proceedingsof
theConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).AssociationforComputationalLinguistics.
A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever.2019.â€œLanguageModelsareUnsupervisedMultitask
Learners.â€
A.Radford,J.Wu,D.Amodei,D.Amodei,J.Clark,M.Brundage,andI.Sutskever.2019.BetterLanguageModelsandTheir
Implications.https://openai.com/blog/better-language-models/.[Online;accessed18-March-2021].(2019).
E.Reif,A.Yuan,M.Wattenberg,F.B.Viegas,A.Coenen,A.Pearce,andB.Kim.2019.â€œVisualizingandMeasuringtheGeometry
ofBERT.â€In:AdvancesinNeuralInformationProcessingSystems.Ed.byH.Wallach,H.Larochelle,A.Beygelzimer,
F.dâ€™AlchÃ©-Buc,E.Fox,andR.Garnett.CurranAssociates,Inc.,8594â€“8603.
A.Rogers,O.Kovaleva,andA.Rumshisky.2020.â€œAPrimerinBERTology:WhatWeKnowAboutHowBERTWorks.â€
TransactionsoftheAssociationforComputationalLinguistics,8,842â€“866.
K.Roose.2023.â€œHowChatbotsandLargeLanguageModels,orLLMs,ActuallyWork.â€NewYorkTimes.RetrievedNov.3,
2023fromhttps://www.nytimes.com/2023/03/28/technology/ai-chatbots-chatgpt-bing-bard-llm.html.
D.E.Rumelhart,G.E.Hinton,andR.J.Williams.1986.â€œLearningrepresentationsbyback-propagatingerrors.â€CahiersDe
LaRevueDeTheologieEtDePhilosophie,323,6088,533â€“536.
T.L.Scaoetal..2023.BLOOM:A176B-ParameterOpen-AccessMultilingualLanguageModel.(2023).arXiv:2211.05100.
B.Scarlini,T.Pasini,andR.Navigli.2020.â€œWithMoreContextsComesBetterPerformance:ContextualizedSenseEmbeddings
forAll-RoundWordSenseDisambiguation.â€In:ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing.AssociationforComputationalLinguistics.
R.SevastjanovaandM.El-Assady.2022.â€œBewaretheRationalizationTrap!WhenLanguageModelExplainabilityDiverges
fromourMentalModelsofLanguage.â€Conference:CommunicationinHuman-AIInteractionWorkshopatIJCAI-ECAI,
abs/2207.06897.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:28 Spinneretal.
R.Sevastjanova,A.-L.Kalouli,C.Beck,H.Hauptmann,andM.El-Assady.2022.â€œLMFingerprints:VisualExplanations
ofLanguageModelEmbeddingSpacesthroughLayerwiseContextualizationScores.â€ComputerGraphicsForum,41,3,
295â€“307.
T.Spinner,R.Kehlbeck,R.Sevastjanova,T.StÃ¤hle,D.A.Keim,O.Deussen,A.Spitz,andM.El-Assady.2023.Revealing
theUnwritten:VisualInvestigationofBeamSearchTreestoAddressLanguageModelPromptingChallenges.(2023).arXiv:
2310.11252.
T.Spinner,U.Schlegel,H.Schafer,andM.El-Assady.2020.â€œexplAIner:AVisualAnalyticsFrameworkforInteractiveand
ExplainableMachineLearning.â€IEEETransactionsonVisualizationandComputerGraphics,26,1.
M.Steiger,J.Bernard,S.Thum,S.MittelstÃ¤dt,M.Hutter,D.A.Keim,andJ.Kohlhammer.2015.â€œExplorativeanalysisof2D
colormaps.â€In:WSCG.
R.J.SternbergandK.Sternberg.2016.Cognitivepsychology.NelsonEducation.
H.Strobelt,S.Gehrmann,M.Behrisch,A.Perer,H.Pfister,andA.M.Rush.2018.â€œSeq2Seq-Vis:Avisualdebuggingtoolfor
sequence-to-sequencemodels.â€IEEETransactionsonVisualizationandComputerGraphics,25,1,353â€“363.
H.Strobelt,J.Kinley,R.Krueger,J.Beyer,H.Pfister,andA.M.Rush.2022.â€œGenNI:Human-AICollaborationforData-Backed
TextGeneration.â€IEEETransactionsonVisualizationandComputerGraphics,28,1,1106â€“1116.
Y.Tan,C.Yang,X.Wei,C.Chen,L.Li,andX.Zheng.2022.â€œEnhancingRecommendationwithAutomatedTagTaxonomy
ConstructioninHyperbolicSpace.â€In:IEEE38thInternationalConferenceonDataEngineering(ICDE).IEEE.
A.J.Teuling,R.StÃ¶ckli,andS.I.Seneviratne.2010.â€œBivariatecolourmapsforvisualizingclimatedata.â€InternationalJournal
ofClimatology,31,9,1408â€“1412.
A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,L.Kaiser,andI.Polosukhin.2017.â€œAttentionIsAll
YouNeed.â€arXiv:1706.03762.
G.Wiedemann,S.Remus,A.Chawla,andC.Biemann.2019.â€œDoesBERTMakeAnySense?InterpretableWordSense
DisambiguationwithContextualizedEmbeddings.â€In:ProceedingsofKONVENS.Erlangen,Germany.
A.Williams,N.Nangia,andS.Bowman.2018.â€œABroad-CoverageChallengeCorpusforSentenceUnderstandingthrough
Inference.â€In:ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies.Vol.1.AssociationforComputationalLinguistics,NewOrleans,Louisiana,1112â€“1122.
T.Wolfetal..2020.â€œTransformers:State-of-the-ArtNaturalLanguageProcessing.â€In:ProceedingsoftheConferenceon
EmpiricalMethodsinNaturalLanguageProcessing:SystemDemonstrations.AssociationforComputationalLinguistics,
Online,38â€“45.
Y.Xiang,Z.Zhang,J.Chen,X.Chen,Z.Lin,andY.Zheng.2021.â€œOntoEA:Ontology-guidedEntityAlignmentviaJoint
KnowledgeGraphEmbedding.â€In:FindingsoftheAssociationforComputationalLinguistics:ACL-IJCNLP.Associationfor
ComputationalLinguistics.
H.Xu,Y.Chen,Z.Liu,Y.Wen,andX.Yuan.2022.â€œTaxoPrompt:APrompt-basedGenerationMethodwithTaxonomic
ContextforSelf-SupervisedTaxonomyExpansion.â€In:Proceedingsofthe31stInternationalJointConferenceonArtificial
Intelligence.InternationalJointConferenceonArtificialIntelligenceOrganization.
W.Yu,C.Zhu,Z.Li,Z.Hu,Q.Wang,H.Ji,andM.Jiang.2022.â€œASurveyofKnowledge-enhancedTextGeneration.â€ACM
ComputingSurveys,54,11s,1â€“38.
A.Yuan,A.Coenen,E.Reif,andD.Ippolito.2022.â€œWordcraft:StoryWritingWithLargeLanguageModels.â€In:27th
InternationalConferenceonIntelligentUserInterfaces.ACM.
C.Zhang,F.Tao,X.Chen,J.Shen,M.Jiang,B.Sadler,M.Vanni,andJ.Han.2018.â€œTaxoGen.â€In:Proceedingsofthe24th
ACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining.ACM.
H.Zhang,H.Song,S.Li,M.Zhou,andD.Song.2022.â€œASurveyofControllableTextGenerationusingTransformer-based
Pre-trainedLanguageModels.â€arXiv:2201.05337.
H.Zhao,H.Chen,F.Yang,N.Liu,H.Deng,H.Cai,S.Wang,D.Yin,andM.Du.2024.â€œExplainabilityforLargeLanguage
Models:ASurvey.â€ACMTransactionsonIntelligentSystemsandTechnology.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation 111:29
A NaturalLanguageProcessingPipelines
Thissectionexplainsthepipelinesthathavebeenimplementedtoprovidethefunctionalitiesof
generAItor.
A.1 NaturalLanguageGenerationPipeline
Wegeneratetextbyusingthebeamsearchalgorithm,alwaysfollowingthepredictionwiththe
highest probability. The resulting beam search tree is stored as a graph in the backend of our
application.Allfunctionalitiesofoursystemuse,augment,ormodifythetree.Inthefollowing,we
describethedifferentpipelinesupdatingthetreestate.
PredictionPipelineâ€”WeusethetokenizedbeamsequencefromtherootnodeuptotheHead
node as the model input for the prediction, truncated to GPT-2â€™s maximal sequence length of
ğ‘™ =1024.Dependingontheusersettings,theoutputtokenprobabilitiesareeithertop-ğ‘˜ selected
max
orâ€“whentemperatureisusedâ€“top-ğ‘ sampled.Finally,weappendthenewtokenstothebeam
searchtree.ThefullPredictionPipelineisdepictedinfigure9.
KeywordExtraction&â€“Coloringâ€”WeuseYAKE[Camposetal.2020]toautomaticallyextract
keywordsofanğ‘›-gramsizeofğ‘› = 1fromthebeamsearchtreeâ€™ssequences.Next,wetokenize
theextractedkeywordsusingtheGPT-2tokenizer,passthemtotheGPT-2modelandextractthe
high-dimensionalembeddingsfromGPT-2â€™slayer11,maximizingthesurroundingcontextcaptured
bytheembeddings[Sevastjanova,Kalouli,etal.2022].NotethatthekeywordsextractedbyYAKE
oftenconsistofmultiplesplit-tokens,e.g.,whenthekeywordisapropernoun.Inthiscase,we
averagethehigh-dimensionalembeddingsofthesplittokens.Toreducethedimensionalityof
theembeddingsfrom768to2,weuseaUMAP[McInnesetal.2018]projectionpre-fittedonto
keywordsextractedfromtheMultiNLIdataset[Williamsetal.2018].Thenowtwo-dimensional
projectedembeddingvectorsarenormalizedandusedtosampleacoloronatwo-dimensional
colormap[Steigeretal.2015].ThefullKeywordEmbeddingPipelineisshowninfigure9.
A.2 BabelNetEmbeddingPipeline
Tobuildtheontologygraph,weleveragethepowerofasemanticnetwork(BabelNet[Navigli
andPonzetto2012])anditsadjacentdisambiguationAPI(Babelfy[Moroetal.2014]).First,each
keywordfromthebeamsearchtreeisdisambiguatedincontextusingtheBabelfyAPI.Theresulting
BabelNetSynsetisusedtoqueryaBabelNetIndexv5.1.Tocreateaunifiedontologygraph,part-
of-speech (POS) tags have to be considered, as the hypernym hierarchies inside BabelNet are
disconnectedforeachPOStag.Therefore,wemustexpandeachkeywordwithasetofpotential
synsetnounsthatrepresentitbest.Wethenbuildandgrowtheontologygraph,startingwiththe
keywordsasleafnodes.Thekeywordsareattachedtotheirexpandedsynsetsandwetraversetheir
hypernymrelationsupwards.Thehigherinthehierarchyasynsetis,themoreabstractitwillbe.
Therefore,atsomepoint,thesynsetsarenotconveyinghelpfulinformationtotheuser.Instead,
itwouldmakesensetoreducethehypernymrelationatsomepoint.Thisdecisionismadeusing
anotherattributethatexistsonmanyBabelNetsynsetsâ€”itsBabelDomain[Camacho-Colladosand
Navigli2017].Domainsaregeneralgroupsofwordsthatshareasimilarityorconcept.Theyare
availableformanysynsets.ThedomainsofBabelNetoftencoverseveralconcepts,suchasBiology.
Wespliteachdomainintoacollectionofsubdomains(BIOLOGY-Animal,Person).Ifasynsetdoes
nothaveadomain,westoptraversingthehypernymrelationsandinsteadattachthesynsettoits
mostsimilarsubdomainanddomain.Theontologygraphcangrowlargequickly,asthehypernym
relationsareoftenintertwinedandcontainmanysynsets.Tosimplifythetree,weremovenodes
thatonlyactasconnectingnodesbetweentwosynsets.Theresultisarelativelycompactcollection
oftrees,withonetreeforeachdomain.Whenpredictionsaremade,theinitialontologygraphis
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.111:30 Spinneretal.
expandedwithnewkeywords.Visualizingthisontologygraphdirectlycancreatelargetrees,as
multipleinstancesofthesamekeywordappearmultipletimes,creatingamultitudeofleafnodes.
Wethereforeinsteadsimplifythegraphfurtherintofourdistinctlayers,whereeachnodecanonly
haveoneparentrelation.ThisgraphcanthenbevisualizedusingaVoronoidiagram.Weusethe
D3Voronoitreemap5implementationtocreateaVoronoitreemapofthehierarchyandallowthe
usertoselectthelayertheywanttoview.Astheupperlayersaggregatethekeywordstothesame
synset,theyofferamorecompactviewofthedomainsandkeywordsofthepredictiongraph.The
BabelNetEmbeddingPipelineisshowninfigure10.
A.3 MaskedOntologicalReplacementPipeline
Tocreatethedomain-specific,context-sensitivesuggestionsoftheontologyreplacefunction,we
combinethepowerofthesemanticnetworkwithmaskedlanguagemodeling.Thegoalistoreplace
aspecificwordwithanothersuggestionthatfitsitscontextandcanbegroupedintodomains.
Tosolvethis,weuseacombinationofBERTandARESEmbeddings[Scarlinietal.2020].ARES
embeddingsarepowerfulsenseembeddingswithhigh-dimensionalrepresentativesforallWordNet
synsets.Theyweretrainedinasemi-supervisedapproachcombiningalexicalknowledgebase
withBERTLargeembeddingsandplaceWordNetsynsetsinthesameembeddingspaceasBERT
embeddings.Thisway,foragivenWordNetsynset,wecanquerytheclosestBERTembedding
andviceversa.BecauseBabelNethasWordNetbindingsformanyBabelNetsynsets,weassign
eachsubdomainaBabelNetandtheirrespectiveWordNetsynset.Thisway,eachsubdomaincan
beassignedtoanembeddingvectorviaARES.TheMaskedOntologicalReplacementPipeline
canbeobservedinfigure11.ForeachkeywordintheBeamSearchTree,wetakethewordandits
sentenceandreplaceitwiththe [ğ‘€ğ´ğ‘†ğ¾] token.Afterwards,wecanusetop-ğ‘˜ predictiononBERT
toqueryalargenumberofpredictionsthatwouldotherwisebeimpossibletoshowtheuserina
compactway(ğ‘˜ =200).Wetokenizeeachpredictedwordandextractthemodellogitsincontext,
extractingandsqueezinglayers8-11,whicharethenappendedtomatchtheARESembeddings
length(ğ‘› = 2048).Afterthisstep,wehaveasetofembeddingsforsubdomainsintheontology
graphandasetofembeddingsforthepredictionsinthebeamsearchtree.Tobringthemtogether,
welookforthenearestneighborsofallembeddingvectors.Tospeeduptheprocess,wecreateda
customFAISS[Johnsonetal.2019]index,whichwecanusetoquerynearestneighborsefficiently.
Subdomainsandpredictionsarematchedviatheiroverlappingnearestneighbors.Theresulting
predictionsarethenattachedtoeachkeywordandshownondemandviatheontologyreplace
function.
5https://github.com/Kcnarf/d3-voronoi-treemap
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.Fig.9. Thepipelinetoexpandthebeamsearchtreeandassignthesemantickeywordcolorinformationtoitsnodes.
Babelnet API
ke :sy trw ino gr [d ]s
Disambiguate
Attach
&
expand
synset
dQuery
omHypernym ainRelations
s
Assign
domains
&
subdomains o :nn
n
ot oo ddl eo
e
ssg []y
Sim Trp eelify O Tn reto el o ğŸŒ²gy OpV tio mro izn ao tii o n O Mn ato pl o ğŸ—ºgy
:string[]
BabelNet Embedding Pipeline
Fig.10. KeywordsareattachedtotheontologygraphviatheBabelNetembeddingpipeline.Thisgraphisthenfurthersimplifiedandthehierarchyisusedto
createanOntologyMapusingaVoronoidiagramvisualization.
FAISS Index
subdomain subdomains
keywords [MASK] sequence subdomains
ARES Matcher embeddings :domain_nn[][ ]
:string[] :string :string[] :string[] Query NN for
:float[2048]
domains
NLG Transformer
ğŸ¤— BERT Large NLG Transformer BERT Large ğŸ¤— Match predictions predictions update Beam Search
Tokenizer to domains :string[] [] Tree
pm
reT
a
do
s
ip ck- tek
iod
n
B per rt
e
dTo ick te ioni nz ser
se :sq tru ine gn [c ]e Input
Layer
8-11
queeze
and duplicate
e
:p
m
flr obe ad
e
t[di 2c dt 0i
i
4o
n
8n
g
]
s
Q epu mre ber
d
ey diN
c
dtN
i io
n
nf go sr
:prep dr ie cd tii oc nti so _n ns
n[][ ] Masked Ontological Replacement Pipeline
:string[] s
Fig.11. Domain-specifickeywordsareattachedtoeachnodeofthebeamsearchtreebycomparingthenearestneighboursofthedomainâ€™sARESembeddings
andthenearestneighboursoftheBERTpredictionsthatcouldreplacethekeywordofthenode.
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.
-generAItor:Tree-in-the-LoopTextGenerationforLanguageModelExplainabilityandAdaptation
111:31111:32 Spinneretal.
Received18July2023;revised16November2023and26January2024;accepted30January2024
J.ACM,Vol.37,No.4,Article111.Publicationdate:August2024.