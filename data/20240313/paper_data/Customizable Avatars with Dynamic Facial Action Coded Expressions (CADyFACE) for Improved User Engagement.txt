This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may no longer be accessible.
Customizable Avatars with Dynamic Facial
Action Coded Expressions (CADyFACE) for
Improved User Engagement
Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin, Norou Diawara,
Janice Keener, John W. Harrington, and Khan M. Iftekharuddin
Abstractâ€”Customizable 3D avatar-based facial expression I. INTRODUCTION
stimuli may improve user engagement in behavioral biomarker O
VER the past twenty years and now entering the age
discovery and therapeutic intervention for autism, Alzheimerâ€™s
of the metaverse, 3D avatars have become
disease, facial palsy, and more. However, there is a lack of
customizable avatar-based stimuli with Facial Action Coding increasingly prominent and effective tools for health
System (FACS) action unit (AU) labels. Therefore, this study applications. In clinical settings, avatars have been broadly
focuses on (1) FACS-labeled, customizable avatar-based applied for neuro- and motor-rehabilitation [1] of patients,
expression stimuli for maintaining subjectsâ€™ engagement, (2)
such as those who have suffered from stroke [2], cerebral
learning-based measurements that quantify subjectsâ€™ facial
palsy [3], brain injury [4], Parkinsonâ€™s disease [5],
responses to such stimuli, and (3) validation of constructs
Alzheimerâ€™s disease and dementia [6], [7]. Furthermore,
represented by stimulus-measurement pairs. We propose
Customizable Avatars with Dynamic Facial Action Coded social avatars have aided discovery of potential behavioral
Expressions (CADyFACE) labeled with AUs by a certified FACS biomarkers and development of therapeutic interventions for
expert. To measure subjectsâ€™ AUs in response to CADyFACE, we social anxiety disorder [8], depression [9], [10],
propose a novel Beta-guided Correlation and Multi-task
schizophrenia [11], and autism spectrum disorder (ASD)
Expression learning neural network (BeCoME-Net) for multi-
[12], [13], [14], [15]. Given the ubiquity of facial
label AU detection. The beta-guided correlation loss encourages
expressions in daily life and their relevance to psychosocial
feature correlation with AUs while discouraging correlation with
subject identities for improved generalization. We train health (e.g., facial palsy [16], depression [9], [10], social
BeCoME-Net for unilateral and bilateral AU detection and anxiety [8], ASD [12], [13], [14]), facial expressions have
compare with state-of-the-art approaches. To assess construct become important targets for avatar-based health
validity of CADyFACE and BeCoME-Net, twenty healthy adult
applications. Thus, important design considerations have
volunteers complete expression recognition and mimicry tasks in
been identified to ensure that 3D avatars are valid and
an online feasibility study while webcam-based eye-tracking and
engaging [17], [18].
video are collected. We test validity of multiple constructs,
including face preference during recognition and AUs during Facial expressions of 3D avatars are often used as stimuli
mimicry. in studies of intervention efficacy or behavioral biomarker
discovery [9], [10], [13], [14], [15]. Such studies incorporate
Index Termsâ€”3D avatars, Facial Action Coding System (FACS), tasks to elicit and measure constructs related to facial
facial expressions, deep learning, construct validity
expressions. The typical setting involves eliciting a response
using avatar-based stimuli, capturing the response with a
sensor, and extracting relevant measurements from the raw
This work is supported in part by the National Science Foundation sensor data. To capture perception and production of facial
Graduate Research Fellowship under Grant Nos. 1753793 and 2139907, and expressions, the applicable sensors are eye-trackers and
in part by the Research Computing clusters at Old Dominion University under
video cameras, respectively [9], [10], [13], [14], [15].
National Science Foundation Grant No. 1828593.
Measures such as the percentage duration of gaze fixations to
(Corresponding author: M. A. Witherow)
This work involved human subjects in its research. Approval of all ethical areas of interest (AOIs) within the stimuli have been used to
and experimental procedures and protocols was granted by the Old Dominion study perception [19]. To assess production, the Facial
University Institutional Review Board under Application No. 1424272 and the
Action Coding System (FACS) [20] provides a taxonomy of
Eastern Virginia Medical School Institutional Review Board under
Application No. 19-06-EX-0152. action units (AUs) that describe individual constituent
M. A. Witherow, W. J. Shields, F. Ilgin, and K. M. Iftekharuddin are with movements of the face. Machine and deep learning
the Vision Lab, Department of Electrical & Computer Engineering, Old
approaches may be used to detect AUs from facial images
Dominion University, Norfolk, VA 23529 USA (e-mail: mwith010@odu.edu;
winstonjshields@gmail.com; furilgin@gmail.com; kiftekha@odu.edu). [21], [22], [23], [24], [25], [26], [27], [28]. Finally,
C. Butler is an independent certified FACS Coder, Ellicott City, MD 21043 evaluating the construct validity of these tasks, i.e., whether
USA (e-mail: crystal@crystal-butler.com).
the intended construct is elicited and measured, is an
N. Diawara is with the Department of Mathematics & Statistics,
Old Dominion University, Norfolk, VA 23529 USA (e-mail: important precursor for well-designed studies of intervention
ndiawara@odu.edu). efficacy or behavioral biomarker discovery [19], [29].
J. W. Harrington and J. Keener are with the Department of Pediatrics,
In the sections to follow, we review related work on
Eastern Virginia Medical School and Childrenâ€™s Hospital of The Kingâ€™s
Daughters, Norfolk, VA 23507 USA (e-mail: john.harrington@chkd.org; important design considerations for 3D avatar-based facial
janice.keener@chkd.org).2
expression stimuli, automatic detection of FACS AUs, and either lack customization capabilities [45], [49], rely on
construct validity. Then, we propose (1) dynamic, FACS- commercial software [46], [47], and/or are rendered as a
labeled stimuli for perception and production of facial disembodied floating head or face [45], [47], [48], which
expressions, rendered on customizable 3D avatars, (2) a new may break immersion. Additionally, GarcÃ­a et al.â€™s avatars
deep learning-based AU detector for measurement of [49] are hyper-realistic, which may trigger the uncanny
subjectsâ€™ facial responses, and (3) construct validity of valley effect [18]. Given these limitations, there still exists a
need for customizable, dynamic 3D avatars and avatar-based
proposed stimuli and measurements based on two tasks
facial expression stimuli that have been evaluated and
(recognition and mimicry) completed by 20 healthy adult
labeled with AUs by FACS experts.
volunteers.
2) Automatic Detection of FACS AUs: While eye-
A. Related Work tracking based measures of facial expression perception
1) Design Considerations for 3D Avatar-based Facial require only straightforward mathematical operations [19],
Expression Stimuli: Securing and maintaining user automatic AU detection from images is more challenging.
engagement is a key challenge for avatar-based health Facial expressions consist of multiple AUs occurring
applications [17]. Recently, avatar customization has been simultaneously in various localized areas of the face. Thus,
identified as an effective means of improving engagement AU detection is a multi-label problem, where each facial
[17]. Avatar customization has been shown to increase image is assigned one or more AUs. AU detection methods
engagement and enjoyment in social [30], [31], procedural either train individual binary classifiers to detect the
[32], creative [32], and cognitive tasks [17], including presence or absence of each AU or train a single model to
interventions for physical [33], [34] and mental health [17],
detect multiple AUs at once [23], [24], [25], [26], [28]. The
[31]. Avatar realism is another important factor influencing
latter approach, referred to as multi-label learning, is
engagement. Hyper-realistic avatars may trigger the uncanny
considered superior due to its computational efficiency and
valley effect, a phenomenon where objects with increasingly
ability to take relationships between AUs into account [23],
realistic human appearances evoke uneasiness or revulsion,
[24], [25], [26], [28]. In addition to modeling the
causing users to disengage [18]. Furthermore, several studies
relationships between AUs, state-of-the-art multi-label
find that users prefer to interact with semi-realistic avatars
learning approaches often incorporate methods for focusing
[18], [35]. Avatars may embody a humanoid form to varying
on relevant areas of the input or features using saliency maps
degrees from â€˜talking headsâ€™ to full body representations.
[28], attention [27], or patch/region learning [21], [26].
Full body representations have been shown to improve
Multi-label learning approaches may also benefit from multi-
dyadic interactions with avatars [36]. Facial expression
stimuli may be rendered statically as still images or task learning of other tasks related to the face (e.g., landmark
dynamically as animations from neutral to peak expression. prediction [27], facial expression classification [50], valence-
While some studies [12], [37] use static facial expressions arousal estimation [50]) and from feature fusion (e.g.,
due to accessibility of widely used, validated stimuli sets saliency maps [28], geometric features [51]).
[38], [39], it has been established from both neuroimaging A drawback of state-of-the-art multi-label AU detection
and behavioral perspectives that dynamic expressions are approaches is that they do not independently predict left and
more salient than static expressions, and show increased right activations of bilaterally located AUs, which may be
activity in face processing regions of the brain [40]. Thus, useful for health applications. For example, Dellâ€™Olio et al.
dynamic facial expressions play a pivotal role in assessing [52] recently propose FaraPy, an augmented reality mirror
relevant differences between control individuals and therapy for patients with facial paralysis. Asymmetrical AU
individuals with a diagnosis in biomarker discovery studies
activation is characteristic of facial palsy or paralysis, e.g.,
(e.g., depression [41], Moebius syndrome [42], ASD [40]).
due to stroke, Parkinsonâ€™s, Bellâ€™s Palsy, etc. [52], and has
To study the effect of 3D avatar-based stimuli on
also been observed among individuals diagnosed with ASD
perception or production of facial expressions, it is important
[53], [54], [55].
to ensure that the avatar accurately renders the target
Recently, Bar et al. [56] present an approach based on
expressions by having the expressions evaluated and labeled
convex geometry for identifying significant correlations
with AUs by FACS experts. This labeling may be especially
among a large number of features using a mixture of beta
critical for studies of expression production, where
distributions (betaMix). The betaMix approach relies upon
constructs may be defined based on a one-to-one
Theorem 1.1 from [57], which shows that the sine squared of
correspondence between the avatarâ€™s AUs and the
the angles between randomly drawn features in high
participantâ€™s AUs. While several methods for transferring
AUs to arbitrary avatar faces have emerged, e.g., [43], [44], dimensional space follows a beta distribution. The betaMix
these methods are not guaranteed to accurately reproduce the approach [56] has been applied to facial expression
target AUs. Therefore, avatar models and avatar-generation classification in [58] for decomposing geometric landmark-
platforms that have been evaluated by FACS experts, such as based features (e.g., distances between pairs of landmarks)
MiFace [45], HapFACS [46], FACSGen [47], FACSHuman into sets associated with expressions, identity, and age
[48], and GarcÃ­a et al.â€™s avatars [49], are preferred. While all groups (children and adults). In [58], learning takes place in
of these existing avatars and avatar-generation platforms two separate steps. First, betaMix [56] is fit using the EM
support dynamic animations, they are limited in that they algorithm to learn correlations between already extracted3
landmark-based features and three factors (expressions,
identity, and age groups). The resulting graph is used to
select expression-correlated features that are invariant to age
and identity. Then, in the second step, the betaMix-selected
features are fused with deep learning-based features to fit the
expression classifier. Thus, feature extraction, selection, and
expression learning steps are not trained end-to-end.
Intermediate steps lack supervision from the class labels and
thus may not be fully optimized for the expression
Fig. 1. Example customization screen for hair color.
classification task. Given the success with facial expression
classification, we hypothesize that the aforementioned
unilateral AU detection. We compare BeCoME-Net
theorem [57] may be adapted into a loss function for
with state-of-the-art AU detection methods on two
simultaneous, end-to-end learning of correlations among
benchmark data sets.
AUs and features, while discouraging dependence on
â€¢ We conduct an online feasibility study of 20 healthy
identity, which is not addressed by present multi-label AU
adult participants to evaluate the construct validity of
detection approaches.
the proposed CADyFACE stimuli and BeCoME-Net
3) Construct Validity: Construct validity may be
AU measurements. Participants complete two facial
determined by assessing whether the expected response is
expression-related tasks, recognition and mimicry,
elicited in a healthy control group. For example, this
while facial video and webcam-based eye-tracking
approach has been used to evaluate constructs related to
data are collected.
candidate eye-tracking biomarkers for ASD [19]. Since
The remainder of this paper is organized as follows.
neurotypical individuals are known to prefer to attend to
Section II describes the proposed methods. Section III
faces when viewing social stimuli, Shic et al. [19] test for
presents the results and discussion. Section IV concludes.
face preference (percentage of gaze duration to the face AOI
vs. random gaze) among neurotypical controls to determine
construct validity. II. METHODS
B. Contributions A. Design and Development of CADyFACE Stimuli
To address the limitations of currently available 3D 1) Avatar Generation: We generate 3D avatars for
avatar-based facial expression stimuli, we propose CADyFACE using free, open-source tools including the 3D
Customizable Avatars with Dynamic Facial Action Coded modeling software Blender (https://www.blender.org/) and
ManuelBastioniLAB 1.6.1a (https://github.com/animate1978/
Expressions (CADyFACE) for user engagement. To detect
MB-Lab), a character creation plugin for Blender.
AUs elicited by CADyFACE, we propose a deep neural
ManuelBastioniLAB 1.6.1a includes six human prototypes:
network for novel Beta-guided Correlation and Multi-task
African female, African male, Asian female, Asian male,
Expression learning (BeCoME-Net). We further conduct a
European female, and European male. We obtain one avatar
feasibility study to evaluate the construct validity of
for each of these six prototypes using the default settings.
CADyFACE and BeCoME-Net AU measurements. Our
Each avatar includes a face rig with 75 blendshapes for facial
contributions are as follows:
animation. We dress avatars using clothing assets (shirt,
â€¢ CADyFACE incorporates six avatar models
jacket, pants) obtained from [44].
representing different genders and races with 2) Avatar Customization: We develop the CADyFACE
customizable hair color, eye color, skin tone, and avatar customization application using the free Unity game
clothing. For each CADyFACE model, six facial engine (https://unity.com/). Users are shown their current
expressions (anger, disgust, fear, happy, sad, and avatar on the left side of the screen and a selection of
surprise) have been posed and labeled by a certified customization options on the right. Users navigate between
FACS expert with over 600 hours of coding screens of options using â€˜nextâ€™ or â€˜backâ€™ buttons, which also
experience. update a progress bar. As users select different customization
â€¢ We propose a novel beta-guided correlation loss for options, the updates are rendered on the avatar. An example
BeCoME-Net that encourages features to be correlated customization screen is shown in Fig. 1. There are 49,152
different possible combinations based on selection of one of
with AUs while discouraging correlation with subject
each of the following: six different avatar models, three skin
identity. For richer representation learning, BeCoME-
tones, four eye colors, four hair colors, eight jacket colors,
Net fuses geometric landmark and deep learning-
eight shirt colors, and eight pants colors. All customization
based texture features while jointly learning AU
options are summarized in Fig. 2.
detection and expression classification tasks. We
consider variants of BeCoME-Net for bilateral and4
Fig. 2. Avatar customization options: (a) all avatar model and skin tone combinations, (b) eye color options, (c) hair color
options, (d) jacket color options, (e) shirt color options, and (f) pants color options.
TABLE I TABLE II
CADYFACE AU INTENSITIES (A=LOW TO E=HIGH) FREQUENCY OF AUS IN CK+ AND DISFA+ DATA SETS
Fig. 3. FACS-annotated expressions in CADyFACE (left to
right): anger, disgust, fear, happy, sad, and surprise.
3) FACS-annotated Facial Expressions: Within Unity, we
develop a software application to visualize expressions on
each of the six prototype avatars and to adjust the appearance
of each expression. Using this software, a member of our team Net for detecting the AUs present in CADyFACE, we the
(C. Butler) who is a certified FACS expert with over 600 consider the Extended Cohn-Kanade (CK+) [59], [60] data set.
hours of coding experience has tuned the blendshapes for each The CK+ data set comprises 593 image sequences of 123 adult
of the six prototype avatars to render six different facial
subjects ages 18 to 50 years posing facial expressions
expressions (a total of 36 sets of 75 blendshapes). The AUs
including anger, disgust, fear, happy, sad, and surprise. Each
representing each expression are selected based upon their
sequence begins with a neutral expression frame and ends with
definitions in the FACS Investigatorâ€™s Guide [20]. The
the peak expression frame, which has been annotated with AU
specific AUs present in each expression and their intensities
labels. CK+ includes 30 different AUs, including the 16 AUs
are reported in Table I. Examples of each expression are
in CADyFACE: AUs 1, 2, 4, 5, 6, 7, 10, 11, 12, 15, 17, 20, 23,
shown in Fig. 3.
25, 26, and 27. We refer to this subset of CK+ AUs as 16AU-
4) Dynamic Animation of Facial Expressions: To generate
CK+. We also use CK+ to compare BeCoME-Net with
the facial expression animations for CADyFACE, we linearly
existing state-of-the-art approaches. However, since some of
interpolate the blendshape values from 0 (neutral) to the
values associated with the AU labels defined for the target the AUs in CK+ appear with low frequency, existing state-of-
expression and avatar prototype. We animate each expression the-art approaches report results on 12 or 13 AU subsets. We
over 25 frames with a 50-millisecond delay between frames. follow established literature [28] to define the 12 AU subset
5) Review by Clinical Team Members: We have developed (12AU-CK+) as AUs 1, 2, 4, 5, 6, 7, 9, 12, 17, 23, 24, and 25.
CADyFACE as a part of an Institutional Review Board (IRB)- Then, the 13 AU subset (13AU-CK+) is defined as 12AU-
approved study for behavioral biomarker discovery among CK+ and AU 27 [28]. The frequencies of AUs present in at
children and young adults diagnosed with ASD. Two of our least one of these three subsets are reported in Table II.
team members (J. Keener and J. W. Harrington) who are Additionally, we follow the same procedure as [58] to obtain
clinicians with expertise in ASD have reviewed and provided expression-labeled samples of CK+ to train the model to
feedback on CADyFACE throughout its development to perform the expression classification task as a part of the
ensure suitability for the study and appropriateness for proposed multi-task learning. The distribution of expression
individuals diagnosed with ASD.
labels is 135 anger, 177 disgust, 75 fear, 207 happy, 327
B. BeCoME-Net for Mult-label AU Detection neutral, 84 sad, and 249 surprise samples.
In addition to our primary data set, CK+, we also
1) Benchmark Data sets: To train and evaluate BeCoME-
A
124567911111222222
U
0
1
2
5
7
0
3
4
5
6
7
D e s c r ip tio n
In n e r B ro w R a is e r
O u te r B ro w R a is e r
B ro w L o w e re r
U p p e r L id R a is e r
C h e e k R a is e r
L id T ig h te n e r
N o s e W rin k le r
U p p e r L ip R a is e r
N a s o la b ia l D e e p e n e r
L ip C o rn e r P u lle r
L ip C o rn e r D e p re s s o r
C h in R a is e r
L ip S tre tc h e r
L ip T ig h te n e r
L ip P re s s o r
L ip s P a rt
J a w D ro p
M o u th S tre tc h
F r
C K +
e q u e n c
1 1 7
1 1 7
1 9 4
1 0 2
1 2 3
1 2 1
7 5
2 1
3 4
1 3 1
9 4
2 0 2
7 9
6 0
5 8
3 2 4
5 0
8 1
y F
Dr I S F A +e
q u e n c
9 3 5 3
7 9 8 2
1 2 0 3 6
9 2 0 8
9 8 3 9
--
3 9 9 3
--
--
1 0 3 7 1
3 9 5 6
5 6 8 9
4 8 5 4
--
--
1 1 4 4 2
--
7 4 8 7
y5
designed to process (ğ‘š Ã—ğ‘› =256 Ã— 256)-pixel grayscale
images and ğ‘™ =68 landmark points extracted from the full
facial image for bilateral AU detection. For unilateral AU
detection, we predict AUs on the left and right sides of the
face independently and define BeCoME-Net-H for (ğ‘šÃ—ğ‘›=
256 Ã—128)-pixel grayscale images of the left or right side of
the face and ğ‘™ =39 landmark points (29 from the same side
of the face and 10 located along the center line of the face).
5) Backbone Architecture: The architecture for BeCoME-
Net begins with a backbone ğ‘€(âˆ™), consisting of convolutional,
pooling, and fully connected layers for feature extraction. Fig.
4 presents the backbone architecture for BeCoME-Net-F. The
Fig. 4. BeCoME-Net-F backbone architecture.
backbone incorporates two branches for processing images
benchmark our approach on the Extended Denver Intensity of ğ‘‹
ğ‘–ğ‘šğ‘”
and landmarks ğ‘‹ ğ‘™ğ‘šğ‘˜, respectively. For the image branch,
Spontaneous Facial Action (DISFA+) [61], [62] data set. we consider the same model architecture as in [58]: three
DISFA+ consists of image sequences of 9 adult subjects blocks of a 2D convolutional layer with 3x3 kernel size
posing 42 facial expressions including individual AUs, followed by 2x2 maximum pooling yielding 16, 32, and 64
combinations of AUs, and 6 basic expressions (anger, disgust, feature maps, respectively, and a final fully connected layer of
fear, happy, sad, and surprise). Each sequence begins with a 512 hidden units. Convolutional and fully connected layers
neutral expression, moves to the peak expression, and ends use the ReLU activation function. Dropout is applied with a
with a neutral expression. All frames are annotated for 12 probability of 0.5 at the final fully connected layer. For the
different AUs: 1, 2, 4, 5, 6, 9, 12, 15, 17, 20, 25, and 26. landmark branch, we input the x, y-coordinates of the ğ‘™
Frequencies of these AUs are reported in Table II. We extract landmark points directly into a 1D convolutional layer with a
samples with expression labels for use in multi-task learning: kernel size of 1 to yield 16 feature maps, which are flattened
324 anger, 2779 disgust, 1276 fear, 3881 happy, 24793 prior to a final 512-unit fully connected layer. We use ReLU
neutral, 436 sad, and 1345 surprise samples. in convolutional and fully connected layers and apply dropout
2) Preprocessing: We follow the same preprocessing with a probability of 0.5 at the fully connected layer.
pipeline as in [58], which yields 256Ã—256-pixel grayscale Compared to [58] which performs feature engineering and
images of centered faces, rotated such that the eyes are level selection based on the landmarks prior to learning, our 1D
and the left eye is 30% of the image width from the left edge. convolutional layer with kernel size 1 aggregates the 2D
The images are normalized to the range [0,1]. For each image,
coordinate information so that the network may learn relevant
we use the dlib library (http://dlib.net/) to extract 68 landmark
features from the normalized landmark positions directly. We
points on the face and normalize x- and y-coordinates to [0,1].
concatenate the outputs of the image and landmark branches to
3) Notation: BeCoME-Net is a deep learning model of the
form (ğ‘=1024)-dimensional feature vector ğ‘.
form ğ‘“:ğ‘‹ â†’ğ‘Œ. We define input ğ‘‹ as the tuple (ğ‘‹ ,ğ‘‹ ),
ğ‘–ğ‘šğ‘” ğ‘™ğ‘šğ‘˜ 6) Beta-guided Correlation Loss: We are interested in
where ğ‘‹ ğ‘–ğ‘šğ‘” âˆˆ ğ’³ ğ‘–ğ‘šğ‘” and ğ‘‹ ğ‘™ğ‘šğ‘˜ âˆˆ ğ’³ ğ‘™ğ‘šğ‘˜. ğ’³ ğ‘–ğ‘šğ‘” is the set of all modeling significant correlations between the features in ğ‘,
ğ‘šÃ—ğ‘› facial expression images, which is a subset of the real labels in ğ‘Œ, and subject identity ğ‘” during training. Let ğ‘
number space â„ğ‘šÃ—ğ‘›. ğ’³ is the set of all pairs of ğ‘™ x- and y- represent the batch size. Consider the space â„ğ‘. From [56],
ğ‘™ğ‘šğ‘˜
landmark coordinates, which is a subset of â„ğ‘™Ã—2. We train [57], [58], the sine squared of the angle ğœƒ between two
BeCoME-Net to perform AU detection and expression random lines drawn from â„ğ‘ follows the beta distribution:
ğ‘âˆ’1 1
classification. For AU detection, we define ğ‘Œ =ğ‘Œ ğ´ğ‘ˆ âˆˆğ’´ ğ´ğ‘ˆ âŠ‚ ğœ†â‰sin2ğœƒ~ğ‘ğ‘’ğ‘¡ğ‘( , ) (1)
ğ’±, where ğ’± is the set of all ğ‘-dimensional binary vectors and 2 2
ğ‘ ğ‘
Random or â€˜nullâ€™ pairs will be approximately
ğ’´ represents the set of all label vectors indicating presence
ğ´ğ‘ˆ perpendicular for even moderate values of ğ‘, e.g., ğ‘ =10,
or absence of ğ‘ different AUs. For expression classification,
meaning that the probability of two random lines being
we define ğ‘Œ=ğ‘Œ âˆˆğ’´ âŠ‚ğ’° where ğ’° is the set of ğ‘˜-
ğ¸ğ‘‹ğ‘ƒğ‘… ğ¸ğ‘‹ğ‘ƒğ‘… ğ‘˜ ğ‘˜ correlated by chance is very small [56]. This result may be
dimensional one-hot encoded vectors. We denote the subject
used to build a graphical model where the nodes are features,
identity vector as ğ‘”. We partition ğ‘“ into backbone network ğ‘€ labels, or identity, and edges represent significant correlations.
and task head ğ» such that ğ‘“ =ğ»âˆ˜ğ‘€, ğ‘€:ğ‘‹ â†’ğ‘, and ğ»:ğ‘ â†’ We denote the number of nodes as ğ‘¤. For the AU detection
ğ‘Œ, where ğ‘ is a latent space of ğ‘ features. task, ğ‘¤ ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  =ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ +ğ‘ ğ´ğ‘ˆğ‘ +1 ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦.
4) Bilateral and Unilateral AU Detection: We define two For the expression classification task, ğ‘¤ ğ‘›ğ‘œğ‘‘ğ‘’ğ‘  =
variants of BeCoME-Net with different input shapes for ğ‘ ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘ +ğ‘˜ ğ‘’ğ‘¥ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›ğ‘ +1 ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘–ğ‘‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘¡ğ‘¦.
bilateral and unilateral detection of AUs. BeCoME-Net-F is6
To build the graph, we employ a frequentist inferential
procedure to screen for edges (â€˜non-nullâ€™ pairs or significant
correlations) among the features in ğ‘, labels in ğ‘Œ, and subject
ğ‘âˆ’1 1
identity ğ‘”. We denote the ğœ‚-quantile of ğ‘ğ‘’ğ‘¡ğ‘( , ) as ğ‘„ .
2 2 ğœ‚
Pairs ğœ† (e.g., feature-feature, feature-label, feature-identity)
ğ‘’
are considered significantly correlated if ğœ† =sin2ğœƒ <ğ‘„ ,
ğ‘’ ğ‘’ ğœ‚
where ğ‘’ =0,1,â€¦,ğ‘¡ and the total number of possible edges
ğ‘¡ =0.5ğ‘¤(ğ‘¤âˆ’1). The selection of ğœ‚ may be used to control
the Type I error rate. For each possible edge ğœ† , we consider
ğ‘’
the null hypothesis ğ» :ğœ† â‰¥ğ‘„ (i.e., no edge) and the
0 ğ‘’ ğœ‚
alternative ğ» :ğœ† <ğ‘„ (i.e., edge in the graph). We conduct a
ğ‘ ğ‘’ ğœ‚ Fig. 5. Plot of (3) showing regions where (a) there is a
total of ğ‘¡ individual hypothesis tests to determine the significant correlation between nodes, (b) transition between
presence/absence of all possible edges. Using the Bonferroni significantly correlated and uncorrelated nodes, and (c) nodes
correction, we divide ğ›¼ =0.05 by the total number of are uncorrelated.
ğ›¼
hypothesis tests ğ‘¡ to set ğœ‚ = . The screening rules associated
ğ‘¡ multiplied by the corresponding entries of ğ´. To aggregate the
with the null and alternative hypotheses may be implemented
individual loss contributions into a single number, we sum
using mirrored and translated Heaviside functions:
0, ğœ† â‰¥ ğ‘„ over all entries. Then, we divide by the total number of entries
ğ‘’ ğœ‚
ğ»ğ‘’ğ‘ğ‘£ğ‘–ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘„ âˆ’ğœ† )= { . (2) ğ‘¤2 so that the scale of the loss does not change for different
ğœ‚ ğ‘’ 1, ğœ† < ğ‘„
ğ‘’ ğœ‚ numbers of features or labels.
However, due to the discontinuity at ğœ† ğ‘’ =ğ‘„ ğœ‚, (2) is not Equation (4) bears some similarity to reinforcement
differentiable. Sigmoid functions may be used to provide a learning. Considering the policy gradient theorem without
smooth approximation for the Heaviside functions [63]. discounting [64], the loss at each time step is defined as the
Therefore, we consider the following sigmoid function for
immediate reward times the predicted action (e.g., the one-hot
differentiable implementation of the screening rules:
encoding for the current action times the log of predicted
ğœ(ğœ† ğ‘’)=1âˆ’ 1+ğ‘’âˆ’ğ‘š1 (ğœ†ğ‘’âˆ’ğ‘„ğœ‚), (3) probabilities for each action). Analogously, our ğ´ ğ‘–ğ‘—â€™s encode
where ğ‘š adjusts the sharpness of the 1 to 0 transition at ğ‘„ . the predicted presence or absence of an edge in the graph and
ğœ‚
To construct the predicted graph adjacency matrix ğ´, we the ğ‘† ğ‘–ğ‘—â€™s encode the associated rewards. However, rather than
apply (3) for each ğœ† to yield the edge connection between simply using cross entropy for edge predictions, our (3) has
ğ‘’
each ğ‘’th pair of nodes (features, labels, or identity) and assign several advantages. Fig. 5 shows three key regions of (3). A
these to the upper triangle of ğ´ in row-major order. We fill the particular ğœ† will fall within region (a) if the associated pair of
ğ‘’
diagonal (representing self-connection) with 1â€™s. The lower nodes is significantly correlated and will receive the full
triangle of ğ´ is the upper triangle mirrored over the diagonal. reward (or penalty) based on the associated ğ‘† . For example, a
ğ‘–ğ‘—
We construct ğ´ such that the first ğ‘ rows and columns
ğœ† representing a feature-identity pair with ğœ(ğœ† )â‰ˆ1 will
ğ‘’ ğ‘’
represent the ğ‘ features. The next ğ‘ or ğ‘˜ rows and columns
receive a penalty â‰ˆ1, while a ğœ† representing a feature-label
represent the AU or expression labels, respectively. The last ğ‘’
pair with ğœ(ğœ† )â‰ˆ1 will receive a reward â‰ˆ1 (penalty â‰ˆ
row and column represent identity. Then, the beta-guided ğ‘’
âˆ’1). Region (b) represents the boundary between significantly
correlation loss â„’ is defined as:
ğµğºğ¶ correlated and uncorrelated nodes. For ğœ† â€™s falling within
ğ‘¤ ğ‘¤ ğ‘’
1
â„’ (ğ´)= âˆ‘âˆ‘(ğ‘† âˆ™ğ´ ), (4) region (b), the reward will be weighted by ğœ(ğœ† ğ‘’). Finally,
ğµğºğ¶ ğ‘¤2 ğ‘–ğ‘— ğ‘–ğ‘— region (c) represents uncorrelated nodes where ğœ(ğœ† )â‰ˆ0, so
ğ‘– ğ‘— ğ‘’
uncorrelated nodes will have a very small contribution to the
where ğ‘† is a ğ‘¤Ã—ğ‘¤ sign matrix (consisting of -1â€™s, 0â€™s, and
loss. Due to (3), â„’ focuses more on significantly correlated
1â€™s) that we use to encourage features to be correlated with the ğµğºğ¶
pairs while ignoring uncorrelated pairs. Therefore, individual
labels, discourage feature correlations with subject identity,
features are allowed to specialize, i.e., to be highly correlated
and encourage feature diversity by discouraging correlations
with one or several specific AUs (or expressions), without
among the features themselves. We set the diagonal of ğ‘† to 0â€™s
being penalized for having low correlation with other AUs (or
as self-connection will be unchanging and have no impact on
expressions). This property is especially suitable for AU and
the loss. Similarly, labels and identity will not be updated
expression learning as many of the constituent muscle actions
during learning. Only the features will be affected by the
of the face cannot or rarely occur concurrently (e.g., AU 24
gradient updates. Therefore, we multiply the entries of ğ´
â€˜lip pressorâ€™ and AU 25 â€˜lips partâ€™ cannot occur together)
associated with label-label and label-identity pairs by 0â€™s in ğ‘†
while others often occur together (e.g., AU 1 â€˜inner brow
so that they do not contribute to the loss. Since we minimize
raiserâ€™ and AU 2 â€˜outer brow raiserâ€™).
the loss during learning, rows and columns representing edges
7) Multi-task Learning Framework: While our primary goal
between the labels and features are multiplied by -1 to
is AU detection, training BeCoME-Net to perform the related
maximize feature correlations with the labels. The remaining
task of expression classification is expected to improve
entries of ğ‘† are filled with 1â€™s to discourage correlations with
representation learning and AU detection performance. We
subject identity and among the features. The entries are7
Fig. 6. BeCoME-Net multi-task learning framework.
define the multi-label AU detection head as a fully connected
output layer of ğ‘ units with sigmoid activation, where ğ‘ is the
number of target AUs. We define the expression classification
head as a fully connected softmax output layer with ğ‘˜ units for
the ğ‘˜ facial expression classes. For efficient learning of both
tasks, we duplicate the backbone and connect one head to each
copy, as shown in Fig. 6. Weights are shared between the two
copies of the backbone for simultaneous training on both AU Fig. 7. (a) Recognition and (b) mimicry tasks.
detection and expression classification tasks. We supervise the
behavioral biomarkers for children and young adults
learning of the AU detection and expression classification
diagnosed with ASD and has been approved by the IRBs at
tasks with the weighted multi-label cross-entropy loss â„’
ğ‘Šğ‘€ğ¶ğ¸ Old Dominion University (Application No. 1424272) and
[65] and weighted categorical cross entropy loss â„’ [66],
ğ‘Šğ¶ğ¶ğ¸ Eastern Virginia Medical School (Application No. 19-06-EX-
respectively. We choose weighted variants of both losses to
0152). All participants have provided informed consent and
address imbalance in the label distributions. We use the beta-
have not received compensation for their participation.
guided correlation loss â„’ ğµğºğ¶ in both tasks (â„’ ğµğºğ¶_ğ´ğ‘ˆ for AU Inclusion criteria includes being at least 20 years of age at the
detection and â„’ for expression classification) to time of enrollment and having access to an Internet-connected
ğµğºğ¶_ğ¸ğ‘‹ğ‘ƒğ‘…
encourage features to be correlated with the labels while personal computer with a webcam. For privacy, each
discouraging correlation with subject identity. The loss for the participant has been assigned a unique subject identifier.
AU detection task is â„’ =â„’ + â„’ , and â„’ = 2) Online Stimuli Presentation and Data Collection: A
ğ´ğ‘ˆ ğ‘Šğ‘€ğ¶ğ¸ ğµğºğ¶_ğ´ğ‘ˆ ğ¸ğ‘‹ğ‘ƒğ‘…
â„’ + â„’ for the expression classification task. The Unity Web-GL application has been developed to present the
ğ‘Šğ¶ğ¶ğ¸ ğµğºğ¶_ğ¸ğ‘‹ğ‘ƒğ‘…
CADyFACE stimuli in each participantâ€™s web browser. The
overall loss is â„’ =â„’ +â„’ .
ğ´ğ‘ˆ ğ¸ğ‘‹ğ‘ƒğ‘…
application is embedded into a webpage hosted on the
8) Experiments: Following established literature [28], we
perform 3-fold subject-independent cross validation for all visionlab.odu.edu domain, which is served by a secure web
experiments and report F1 scores calculated over all test folds. server located at Old Dominion University. Participants must
To study the effect of multi-task learning and the proposed enter their unique subject identifier to access the webpage.
Beta-guided correlation loss, we perform an ablation study on As the participants interact with the Unity Web-GL
16AU-CK+ for both bilateral and unilateral AU detection application, the WebGazer.js (https://webgazer.cs.brown.edu/)
models. Then, we benchmark BeCoME-Net-F and BeCoME- [68] JavaScript library and its self-calibrating eye-tracking
Net-H on 16AU-CK+ and report the performance for each model are used to collect video frames and webcam-based
AU. Next, we train and test BeCoME-Net-F and BeCoME- eye-tracking coordinates from the participantsâ€™ webcams and
Net-H on 13AU-CK+, 12AU-CK+, and DISFA+ for record them to the secure web server.
comparison with state-of-the-art approaches. For 13AU-CK+, 3) Tasks: Participants complete two tasks developed using
we compare with BGCS [25], HRBM [23], and LNDSM [28]. the CADyFACE stimuli: recognition and mimicry. In the
For 12AU-CK+, we compare with JPML [24], DSCMR [22], recognition task, participants are asked to select the expression
and LNDSM [28]. For DISFA+, we compare with DRML shown by clicking the button labeled with the name of the
[21], AU R-CNN [26], JÃ‚A-Net [27], and LNDSM [28]. expression. In the mimicry task, participants are asked to make
LNDSM [28] is the newest state-of-the-art approach with the same facial expression as the avatar. Each task consists of
which we compare. six trials, one for each of the six FACS-annotated expressions
For all experiments, we train using the ADAM optimizer in CADyFACE (anger, disgust, fear, happy, sad, or surprise).
with a triangular learning rate policy [67] cycling the learning Each participant customizes their own avatar for use in both
rate between 10âˆ’5 and 10âˆ’3 until convergence. We use a tasks. The expression and mimicry tasks are shown in Fig. 7.
batch size of ğ‘ =32 and set ğ‘š =100. 4) Constructs: During the recognition task, participants are
expected to attend their gaze to the avatarâ€™s face to determine
C. Feasibility Study, Tasks, and Constructs
the expression. Therefore, we consider the face preference
1) Participants: We have recruited 20 healthy adult
construct [19]. Following [19], we measure the construct as
volunteers (ages 21 to 35, 4 female) for an online feasibility
the percentage of gaze duration to the avatarâ€™s face (%Face)
study of CADyFACE. This feasibility study has been
and test construct validity using a one-sample t-test of %Face
conducted as a part of a larger IRB-approved study to discover8
against the percentage of the scene taken up by the avatarâ€™s
face, which is the expected %Face given random gaze. For TABLE III
our recognition task, the avatarâ€™s face occupies 15.0% of the ABLATION STUDY
scene. We test the construct validity of all six expressions.
During the mimicry task, participants are expected to pose
the same facial expression as the avatar. Since CADyFACE
has AU labels, we consider constructs based upon the
activation of the same AUs by the participants. To measure
the constructs, we use BeCoME-Net-F and BeCoME-Net-H
to detect the AUs in peak expression frames of each of the
participantsâ€™ mimicked expressions. We use both BeCoME-
AU12-CK+, BeCoME-Net-F achieves the highest
Net-F and BeCoME-Net-H to measure the construct with
performance, slightly outperforming LNDSM while BeCoME-
both bilateral and unilateral AU detectors. For each AU, we
Net-H performs equally well to LNDSM. We note that for
test construct validity using a one-sample t-test against 0 (no
each prediction, LNDSM requires a reference image of the
activation). We test the construct validity of all AUs in all
neutral face for the same subject. Our proposed BeCoME-Net
six expressions.
performs competitively on CK+ without requiring reference
images of the neutral face.
III. RESULTS AND DISCUSSION
To show how BeCoME-Net performs on a data set other
A. Ablation Study than CK+, we compare our performance on the DISFA+ data
To understand the impact of multi-task learning and the beta- set in Table V. While BeCoME-Net-F and BeCoME-Net-H
guided correlation loss, we perform an ablation study for both both outperform DRML [21] and AU R-CNN [26], the best
bilateral and unilateral AU detection on 16AU-CK+. As shown in performance is achieved by LNDSM [28] followed by JÃ‚A-
Table III, the best performance is achieved for bilateral and Net [27]. Both LNDSM and JÃ‚A-Net methods exhibit greater
unilateral models when both multi-task learning and the beta- complexity and depth than ours. LNDSM is twice as deep as
guided correlation loss are considered. The inclusion of multi-task
BeCoME-Net with six convolutional blocks compared to our
learning or the beta-guided correlation loss alone result in small
three [28]. LNDSM also benefits from using neutral
improvements in mean F1 score (less than 1%) for bilateral and
reference images to generate saliency maps that are fused at
unilateral models. Including both multi-task learning and beta-
several intermediate layers of the network [28]. JÃ‚A-Net
guided correlation achieves an improvement of 1.81% and 2.86%
involves multiple sub-networks for face alignment, global
in mean F1 score for bilateral and unilateral models, respectively.
feature learning, local AU feature learning, and attention
These results suggest that the use of the beta-guided correlation
refinement [27]. Furthermore, BeCoME-Net may be less
loss in the secondary expression classification task yields better
competitive on DISFA+ due to the beta-guided correlation
representation learning for AU detection. These results also show
loss, which discourages correlation between the learned
that all bilateral models perform better than their unilateral
counterparts, which we expect is due to the bilateral models features and subject identities. Given that DISFA+ contains
having access to information from the entire face. only 9 subjects (compared to 123 in CK+), some
discriminative features may be spuriously correlated with
B. BeCoME-Net Performance for CADyFACE AUs
subject identity.
The precision, recall, and F1 scores for each AU based on 3-
D. Construct Validity
fold cross validation of 16AU-CK+ for BeCoME-Net-F and
BeCoME-Net-H are reported in Fig. 8. Both models follow As shown in Table VI, the recognition taskâ€™s face
similar patterns of performance. The best performing AUs with preference construct is valid for all expressions. Two
the F1 scores of over 80% are AUs 2, 12, 17, 25, and 27. As participants are excluded due to tracking loss. Table VII
shown in Table II, AUs 2, 12, 17, and 25 are some of the most reports construct validity for the mimicry task based on
frequent AUs in 16AU-CK+. While being a less frequent AU, BeCoME-Net-F and BeCoME-Net-H AU predictions. For
AU 27 (mouth stretch) is associated with the distinctive open some of the tests (indicated by an asterisk*), we are unable to
mouth appearance seen in the fear and surprise expressions. The compute a t-statistic due to there being no predictions of the
worst performing AUs with F1 scores less than 50% are the four AU among any of the participants. These AUs are AU 10, AU
least frequent AUs in 16AU-CK+: AUs 10, 11, 23, and 26. 11, AU 23, and AU 26, which are the four least frequent AUs
in the 16AU-CK+ training set. The following unilateral and
C. Comparison with State-of-the-art AU Detectors
bilateral constructs are valid: AUs 4 and 25 for anger; AU 17
We compare our proposed BeCoME-Net with state-of-the- for disgust; AUs 1, 2, 5, 25, and 27 for fear; AU 12 for happy;
art approaches for multi-label AU detection using the CK+ all AUs (1, 4, 11, 15) except AU 11 for sad; and all AUs (1, 2,
(AU13-CK+ and AU12-CK+) and DISFA+ data sets. Table 5, 25, 27) for surprise. Failing to pass the test of construct
IV shows that BeCoME-Net-F achieves performance on par validity for some AUs may be attributed to one of two
with the best performing and most recent state-of-the-art reasons: BeCoME-Net did not detect a present AU or the
method LNDSM [28] for AU13-CK+. BeCoME-Net-H CADyFACE stimuli did not successfully elicit the AU.
performs second best after BeCoME-Net-F and LNDSM. For
A U
D e te c tio n
U n ila te ra l
U n ila te ra l
U n ila te ra l
U n ila te ra l
B ila te ra l
B ila te ra l
B ila te ra l
B ila te ra l
(im
In p u t S iz e
a g e , la n d m a r k2
5 6 x 1 2 8 , 3 9
2 5 6 x 1 2 8 , 3 9
2 5 6 x 1 2 8 , 3 9
2 5 6 x 1 2 8 , 3 9
2 5 6 x 2 5 6 , 6 8
2 5 6 x 2 5 6 , 6 8
2 5 6 x 2 5 6 , 6 8
2 5 6 x 2 5 6 , 6 8
s)
ML ue lti-T aa
r n in
X
X
X
X
sk
g C
B e ta -G uo
r re la tio
X
X
X
X
id en
L
d
o ss
M e a n F 1
S c o re
6 1 .1 6 %
6 1 .8 3 %
6 1 .7 7 %
6 4 .0 2 %
6 4 .5 1 %
6 4 .7 8 %
6 5 .1 6 %
6 6 .3 2 %9
Fig. 8. Precision, recall, and F1 scores based on 3-fold cross validation of 16AU-CK+ for (a) BeCoME-Net-F and (b)
BeCoME-Net-H.
TABLE IV
F1 SCORES FOR BECOME-NET COMPARED WITH STATE-OF-THE-ART FOR MULTI-LABEL AU DETECTION ON CK+
TABLE V TABLE VI
F1 SCORES FOR BECOME-NET COMPARED WITH STATE-OF- CONSTRUCT VALIDITY FOR RECOGNITION TASK
THE-ART FOR MULTI-LABEL AU DETECTION ON DISFA+
V. CONCLUSION
IV. LIMITATIONS
In this article, we propose the CADyFACE stimuli,
As with other 3D avatar models, the ManuelBastioniLAB
customizable 3D avatars with FACS labels, intended for use in
models that we use in this work are limited by the fidelity and
behavioral biomarker discovery and intervention studies.
quality of their blendshapes. While AUs 9 and 10 are both listed
Additionally, we propose BeCoME-Net for multi-label
as potential core components of a prototypical disgust face in
detection of AUs elicited by CADyFACE. We conduct an
the FACS Investigatorâ€™s Guide [20], AU 9 (â€˜nose wrinklerâ€™) is
online feasibility study with 20 adult volunteers who complete
more common, as reflected by the relative frequencies in the
recognition and mimicry tasks based on CADyFACE while
CK+ data set. However, since the ManuelBastioniLAB models
their expressions and eye-gaze are recorded. We report
are unable to perform nose wrinkling, we opt for AU 10. Using
construct validity of these tasks using a well-known eye-
AU 9 instead of AU 10 may have yielded better results for the
tracking measure and the BeCoME-Net AU predictions. In the
construct validity of the disgust expression. Furthermore, for
future, we plan to use CADyFACE and BeCoME-Net in a study
AU 11 (â€˜nasolabial deepenerâ€™), the ManuelBastioniLAB models
aimed at discovering behavioral biomarkers for children and
are only able to render a low level of activation. More
young adults with ASD.
conspicuous representation of AU 11 may have had a positive
impact on the construct validity of AU 11 within the sad
expression.
M e th o d
1 3 A U D e te c tio n
B G C S [2 5 ]
H R B M [2 3 ]
L N D S M [2 8 ]
B e C o M E -N e t-F
B e C o M E -N e t-H
1 2 A U D e te c tio n
JP M L [2 4 ]
D S C M R [2 2 ]
L N D S M [2 8 ]
B e C o M E -N e t-F
B e C o M E -N e t-H
* B o ld w ith b ra c k e ts in d ic a
1
0 .7 1[0
.8 70
.8 60
.8 20
.8 1
0 .5 00
.5 4[0
.8 80
.8 00
.8 1te
s th e
]
]
b e
2
0 .6 8
0 .8 6
[0 .8 8 ]
0 .8 5
0 .8 2
0 .4 0
0 .6 4
[0 .8 6 ]
0 .8 3
0 .8 3
st sc o re . B
4
0 .6 4
0 .7 3
[0 .8 1 ]
0 .8 0
0 .7 6
0 .7 2
0 .6 1
[0 .8 2 ]
0 .8 0
0 .7 8
o ld w ith o
5
0 .6 00
.7 20
.7 4[0
.7 60
.7 5
0 .5 30
.4 20
.7 50
.7 6[0
.7 8u
t b ra
]
]
c k e
6
0 .5 8
0 .6 2
[0 .7 0 ]
0 .6 6
0 .6 6
0 .5 8
[0 .6 8 ]
[0 .6 8 ]
0 .6 7
0 .6 5
ts in d ic a te
7
0 .4 7
0 .5 5
0 .6 2
[0 .6 3 ]
0 .5 8
0 .2 4
0 .3 6
0 .5 6
[0 .6 1 ]
0 .6 0
s th e se c
A U
9 1 2
0 .5 2 0 .7 3
0 .8 6 0 .7 3
0 .8 9 [0 .8 7 ]
[0 .9 3 ] 0 .8 2
0 .9 0 0 .8 3
0 .5 5 0 .7 5
0 .5 4 0 .8 0
0 .9 0 [0 .8 7 ]
[0 .9 4 ] 0 .8 5
0 .9 1 0 .8 2
o n d -b e st sc o re .
E x p r e ssio n
A n g e r
D isg u st
F e a r
H a p p y
S a d
S u rp rise
1 7 2 3
0 .7 7 0 .2 2
0 .8 2 [0 .5 7 ]
[0 .8 6 ] 0 .4 6
0 .8 3 0 .4 5
0 .8 3 0 .4 9
0 .8 2 0 .4 2
[0 .9 0 ] [0 .7 5 ]
0 .8 5 0 .3 3
0 .8 2 0 .4 6
0 .8 1 0 .4 4
C o n str u c t
F a c e P re fe re n cF
a c e P re fe re n cF
a c e P re fe re n cF
a c e P re fe re n cF
a c e P re fe re n cF
a c e P re fe re n c
2 4
0 .2 4
0 .3 5
0 .4 6
[0 .5 8 ]
[0 .5 8 ]
0 .3 1
0 .3 6
0 .4 3
[0 .5 9 ]
0 .5 4
d f
e 1 7
e 1 7
e 1 7
e 1 7
e 1 7
e 1 7
M2
5 2 7
0 .7 9 0 .5 8 00
.9 3 0 .8 8 0[0
.9 4 ] 0 .9 0 [00
.9 1 [0 .9 1 ] [00
.9 0 0 .9 0 0
0 .7 6 -- 00
.8 6 -- 0[0
.9 1 ] -- 00
.9 0 -- [00
.9 0 -- 0
% F a c e
t-sta tistic p -v a lu e3
.5 2 3 0 .0 0 1
2 .2 9 1 0 .0 1 8
3 .3 2 0 0 .0 0 2
3 .1 2 3 0 .0 0 3
3 .1 1 3 0 .0 0 3
2 .5 6 6 0 .0 1 0
e a n
.5 8
.7 3
.7 7 ]
.7 7 ]
.7 6
.5 5
.6 2
.7 4
.7 5 ]
.7 4
v a lid ityâœ“
âœ“
âœ“
âœ“
âœ“
âœ“10
TABLE VII
CONSTRUCT VALIDITY FOR MIMICRY TASK
BeCoME-Net-F (Bilateral) BeCoME-Net-H (Unilateral)
Expression Construct
df t-statistic p-value validity df t-statistic p-value validity
AU 4 Activation 19 3.199 0.005 âœ“ 19 4.359 <0.001 âœ“
AU 5 Activation 19 1.831 0.082 19 1.831 0.083
AU 7 Activation 19 1.453 0.163 19 3.199 0.005 âœ“
Anger AU 10 Activation 19 --* -- 19 2.517 0.021 âœ“
AU 23 Activation 19 --* -- 19 --* --
AU 25 Activation 19 5.339 <0.001 âœ“ 19 3.943 <0.001 âœ“
AU 26 Activation 19 --* -- 19 1.453 0.163
AU 10 Activation 19 1.000 0.330 19 1.453 0.163
Disgust
AU 17 Activation 19 4.819 <0.001 âœ“ 19 2.854 0.010 âœ“
AU 1 Activation 19 7.550 <0.001 âœ“ 19 8.718 <0.001 âœ“
AU 2 Activation 19 13.077 <0.001 âœ“ 19 8.718 <0.001 âœ“
AU 4 Activation 19 1.831 0.083 19 2.517 0.021 âœ“
Fear AU 5 Activation 19 3.943 <0.001 âœ“ 19 3.943 <0.001 âœ“
AU 20 Activation 19 1.000 0.330 19 1.831 0.083
AU 25 Activation 19 10.376 <0.001 âœ“ 19 8.718 <0.001 âœ“
AU 27 Activation 19 3.199 0.005 âœ“ 19 2.854 0.010 âœ“
AU 6 Activation 19 2.179 0.042 âœ“ 19 1.000 0.330
Happy
AU 12 Activation 19 2.854 0.010 âœ“ 19 2.854 0.010 âœ“
AU 1 Activation 19 3.943 <0.001 âœ“ 19 4.818 <0.001 âœ“
AU 4 Activation 19 4.359 <0.001 âœ“ 19 5.338 <0.001 âœ“
Sad
AU 11 Activation 19 --* -- 19 --* --
AU 15 Activation 19 5.339 <0.001 âœ“ 19 2.517 0.021 âœ“
AU 1 Activation 19 4.359 <0.001 âœ“ 19 5.339 <0.001 âœ“
AU 2 Activation 19 4.819 <0.001 âœ“ 19 5.940 <0.001 âœ“
Surprise AU 5 Activation 19 2.854 0.010 âœ“ 19 3.943 <0.001 âœ“
AU 25 Activation 19 7.550 <0.001 âœ“ 19 10.376 <0.001 âœ“
AU 27 Activation 19 3.560 0.002 âœ“ 19 3.199 0.005 âœ“
[8] P. M. G. Emmelkamp, K. MeyerbrÃ¶ker, and N. Morina, "Virtual Reality
Therapy in Social Anxiety Disorder," Current Psychiatry Reports, vol.
ACKNOWLEDGEMENT
22, no. 7, p. 32, 2020/05/13 2020, doi: 10.1007/s11920-020-01156-1.
The authors would like to express their gratitude to all [9] A. Takemoto, "Depression detection using virtual avatar communication
volunteers who participated in the feasibility study; to Abigail and eye tracking system," Journal of Eye Movement Research, vol. 16,
no. 2, 08/06 2023, doi: 10.16910/jemr.16.2.6.
Stedman for her assistance with avatar generation, avatar
[10] A. Takemoto, I. Aispuriete, L. Niedra, and L. F. Dreimane,
customization, and task implementation; to Old Dominion "Differentiating depression using facial expressions in a virtual avatar
University Information Technology Services for their assistance communication system," (in English), Frontiers in Digital Health,
with the web server used for the feasibility study; and to Original Research vol. 5, 2023-March-10 2023, doi:
10.3389/fdgth.2023.1080023.
Gregory Hubbard and Elija Bullock for their assistance with
[11] N. I. Muros et al., "Facial Affect Recognition by Patients with
eye-tracking data collection and visualization. Schizophrenia Using Human Avatars," Journal of Clinical Medicine, vol.
10, no. 9, p. 1904, 2021. [Online]. Available:
https://www.mdpi.com/2077-0383/10/9/1904.
REFERENCES
[12] M. D. Samad, N. Diawara, J. L. Bobzien, J. W. Harrington, M. A.
[1] R. S. CalabrÃ² et al., "The Arrival of the Metaverse in Neurorehabilitation: Witherow, and K. M. Iftekharuddin, "A feasibility study of autism
Fact, Fake or Vision?," Biomedicines, vol. 10, no. 10, p. 2602, 2022. behavioral markers in spontaneous facial, visual, and hand movement
[Online]. Available: https://www.mdpi.com/2227-9059/10/10/2602. response data," IEEE Transactions on Neural Systems and Rehabilitation
[2] J. Hao, H. Xie, K. Harp, Z. Chen, and K.-C. Siu, "Effects of virtual reality Engineering, vol. 26, no. 2, pp. 353-361, 2017.
intervention on neural plasticity in stroke rehabilitation: a systematic [13] M. D. Samad, N. Diawara, J. L. Bobzien, C. M. Taylor, J. W. Harrington,
review," Archives of Physical Medicine and Rehabilitation, vol. 103, no. and K. M. Iftekharuddin, "A pilot study to identify autism related traits in
3, pp. 523-541, 2022. spontaneous facial actions using computer vision," Research in Autism
[3] N. Garcia-Hernandez, M. Guzman-Alvarado, and V. Parra-Vega, "Virtual Spectrum Disorders, vol. 65, pp. 14-24, 2019.
body representation for rehabilitation influences on motor performance of [14] M. A. Mosher, A. C. Carreon, S. L. Craig, and L. C. Ruhter, "Immersive
cerebral palsy children," Virtual Reality, vol. 25, pp. 669-680, 2021. Technology to Teach Social Skills to Students with Autism Spectrum
[4] A. R. Alashram, G. Annino, E. Padua, C. Romagnoli, and N. B. Mercuri, Disorder: a Literature Review," Review Journal of Autism and
"Cognitive rehabilitation post traumatic brain injury: A systematic review Developmental Disorders, vol. 9, no. 3, pp. 334-350, 2022/09/01 2022,
for emerging use of virtual reality technology," Journal of Clinical doi: 10.1007/s40489-021-00259-6.
Neuroscience, vol. 66, pp. 209-219, 2019. [15] M. S. Jaliaawala and R. A. Khan, "Can autism be catered with artificial
[5] I. Cikajlo and K. Peterlin Potisk, "Advantages of using 3D virtual reality intelligence-assisted intervention technology? A comprehensive survey,"
based training in persons with Parkinsonâ€™s disease: A parallel study," Artificial Intelligence Review, vol. 53, no. 2, pp. 1039-1069, 2020/02/01
Journal of neuroengineering and rehabilitation, vol. 16, no. 1, pp. 1-14, 2020, doi: 10.1007/s10462-019-09686-8.
2019. [16] M. Hotton et al., "The psychosocial impact of facial palsy: A systematic
[6] C. Frasson and H. B. Abdessalem, "Contribution of Virtual Reality review," British Journal of Health Psychology, vol. 25, no. 3, pp. 695-
Environments and Artificial Intelligence for Alzheimer," Medical 727, 2020, doi: https://doi.org/10.1111/bjhp.12440.
Research Archives, vol. 10, no. 9, 2022. [17] M. V. Birk and R. L. Mandryk, "Improving the Efficacy of Cognitive
[7] S. Mezrar and F. Bendella, "A Systematic Review of Serious Games Training for Digital Mental Health Interventions Through Avatar
Relating to Cognitive Impairment and Dementia," Journal of Digital Customization: Crowdsourced Quasi-Experimental Study," (in English), J
Information Management, vol. 20, no. 1, pp. 1-9, 2022.11
Med Internet Res, Original Paper vol. 21, no. 1, p. e10133, 2019, doi: [36] M. K. Young, J. J. Rieser, and B. Bodenheimer, "Dyadic interactions with
10.2196/10133. avatars in immersive virtual environments: high fiving," presented at the
[18] M. Shin, S. J. Kim, and F. Biocca, "The uncanny valley: No need for any Proceedings of the ACM SIGGRAPH Symposium on Applied Perception,
further judgments when an avatar looks eerie," Computers in Human TÃ¼bingen, Germany, 2015. [Online]. Available:
Behavior, vol. 94, pp. 100-109, 2019/05/01/ 2019, doi: https://doi.org/10.1145/2804408.2804410.
https://doi.org/10.1016/j.chb.2019.01.016. [37] J. A. Caine, B. Klein, and S. L. Edwards, "The Impact of a Novel
[19] F. Shic et al., "The Autism Biomarkers Consortium for Clinical Trials: Mimicry Task for Increasing Emotion Recognition in Adults with Autism
evaluation of a battery of candidate eye-tracking biomarkers for use in Spectrum Disorder and Alexithymia: Protocol for a Randomized
autism clinical trials," Molecular Autism, vol. 13, no. 1, p. 15, 2022/03/21 Controlled Trial," (in eng), JMIR Res Protoc, vol. 10, no. 6, p. e24543,
2022, doi: 10.1186/s13229-021-00482-2. Jun 25 2021, doi: 10.2196/24543.
[20] P. Ekman, W. Friesen, and J. Hager, "Facial action coding system [E- [38] M. Olszanowski, G. Pochwatko, K. Kuklinski, M. Scibor-Rylski, P.
book]," Salt Lake City, UT: Research Nexus, 2002. Lewinski, and R. K. Ohme, "Warsaw set of emotional facial expression
[21] K. Zhao, W. S. Chu, and H. Zhang, "Deep Region and Multi-label pictures: a validation study of facial display photographs," (in eng), Front
Learning for Facial Action Unit Detection," presented at the 2016 IEEE Psychol, vol. 5, p. 1516, 2014, doi: 10.3389/fpsyg.2014.01516.
Conference on Computer Vision and Pattern Recognition (CVPR), 27-30 [39] S. Y. Yao, R. Bull, K. H. Khng, and A. Rahim, "Psychometric properties
June 2016, 2016. of the NEPSY-II affect recognition subtest in a preschool sample: a Rasch
[22] S.-J. Wang, B. Lin, Y. Wang, T. Yi, B. Zou, and X.-w. Lyu, "Action modeling approach," (in eng), Clin Neuropsychol, vol. 32, no. 1, pp. 63-
Units recognition based on Deep Spatial-Convolutional and Multi-label 80, Jan 2018, doi: 10.1080/13854046.2017.1343865.
Residual network," Neurocomputing, vol. 359, pp. 130-138, 2019/09/24/ [40] M. M. Vandewouw, E. J. Choi, C. Hammill, J. P. Lerch, E. Anagnostou,
2019, doi: https://doi.org/10.1016/j.neucom.2019.05.018. and M. J. Taylor, "Changing Faces: Dynamic Emotional Face Processing
[23] Z. Wang, Y. Li, S. Wang, and Q. Ji, "Capturing Global Semantic in Autism Spectrum Disorder Across Childhood and Adulthood,"
Relationships for Facial Action Unit Recognition," presented at the 2013 Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, vol. 6,
IEEE International Conference on Computer Vision, 1-8 Dec. 2013, 2013. no. 8, pp. 825-836, 2021/08/01/ 2021, doi:
[24] K. Zhao, W. S. Chu, F. D. l. Torre, J. F. Cohn, and H. Zhang, "Joint Patch https://doi.org/10.1016/j.bpsc.2020.09.006.
and Multi-label Learning for Facial Action Unit and Holistic Expression [41] A. J. d. L. Bomfim, R. A. d. S. Ribeiro, and M. H. N. Chagas,
Recognition," IEEE Transactions on Image Processing, vol. 25, no. 8, pp. "Recognition of dynamic and static facial expressions of emotion among
3931-3946, 2016, doi: 10.1109/TIP.2016.2570550. older adults with major depression," Trends in psychiatry and
[25] Y. Song, D. McDuff, D. Vasisht, and A. Kapoor, "Exploiting sparsity and psychotherapy, vol. 41, pp. 159-166, 2019.
co-occurrence structure for action unit recognition," presented at the 2015 [42] E. De Stefani et al., "Children with facial paralysis due to Moebius
11th IEEE International Conference and Workshops on Automatic Face syndrome exhibit reduced autonomic modulation during emotion
and Gesture Recognition (FG), 4-8 May 2015, 2015. processing," Journal of Neurodevelopmental Disorders, vol. 11, no. 1, p.
[26] C. Ma, L. Chen, and J. Yong, "AU R-CNN: Encoding expert prior 12, 2019/07/10 2019, doi: 10.1186/s11689-019-9272-2.
knowledge into R-CNN for action unit detection," Neurocomputing, vol. [43] G. Kim, S. Park, and S. H. Lee, "Video Synthesis Method for Virtual
355, pp. 35-47, 2019/08/25/ 2019, doi: Avatar Using FACS based GAN," presented at the Proceedings of the
https://doi.org/10.1016/j.neucom.2019.03.082. Korea Information Processing Society Conference, 2021.
[27] Z. Shao, Z. Liu, J. Cai, and L. Ma, "JÃ‚A-Net: Joint Facial Action Unit [44] S. v. d. Struijk, H.-H. Huang, M. S. Mirzaei, and T. Nishida, "FACSvatar:
Detection and Face Alignment Via Adaptive Attention," International An Open Source Modular Framework for Real-Time FACS based Facial
Journal of Computer Vision, vol. 129, no. 2, pp. 321-340, 2021/02/01 Animation," presented at the Proceedings of the 18th International
2021, doi: 10.1007/s11263-020-01378-z. Conference on Intelligent Virtual Agents, Sydney, NSW, Australia, 2018.
[28] J. Chen, C. Wang, K. Wang, and M. Liu, "Lightweight network [Online]. Available: https://doi.org/10.1145/3267851.3267918.
architecture using difference saliency maps for facial action unit [45] C. Butler, S. Michalowicz, L. Subramanian, and W. Burleson, "More than
detection," Applied Intelligence, vol. 52, no. 6, pp. 6354-6375, a Feeling: The MiFace Framework for Defining Facial Communication
2022/04/01 2022, doi: 10.1007/s10489-021-02755-y. Mappings," presented at the Proceedings of the 30th Annual ACM
[29] J. C. McPartland et al., "The Autism Biomarkers Consortium for Clinical Symposium on User Interface Software and Technology, QuÃ©bec City,
Trials (ABC-CT): Scientific Context, Study Design, and Progress Toward QC, Canada, 2017. [Online]. Available:
Biomarker Qualification," (in English), Frontiers in Integrative https://doi.org/10.1145/3126594.3126640.
Neuroscience, Perspective vol. 14, 2020-April-09 2020, doi: [46] R. Amini, C. Lisetti, and G. Ruiz, "HapFACS 3.0: FACS-Based Facial
10.3389/fnint.2020.00016. Expression Generator for 3D Speaking Virtual Characters," IEEE
[30] H.-W. Lee, K. Chang, J.-P. Uhm, and E. Owiro, "How Avatar Transactions on Affective Computing, vol. 6, no. 4, pp. 348-360, 2015,
Identification Affects Enjoyment in the Metaverse: The Roles of Avatar doi: 10.1109/TAFFC.2015.2432794.
Customization and Social Engagement," Cyberpsychology, Behavior, and [47] E. B. Roesch, L. Tamarit, L. Reveret, D. Grandjean, D. Sander, and K. R.
Social Networking, vol. 26, no. 4, pp. 255-262, 2023/04/01 2023, doi: Scherer, "FACSGen: A Tool to Synthesize Emotional Facial Expressions
10.1089/cyber.2022.0257. Through Systematic Manipulation of Facial Action Units," Journal of
[31] M. J. Dechant, M. V. Birk, Y. Shiban, K. Schnell, and R. L. Mandryk, Nonverbal Behavior, vol. 35, no. 1, pp. 1-16, 2011/03/01 2011, doi:
"How Avatar Customization Affects Fear in a Game-based Digital 10.1007/s10919-010-0095-9.
Exposure Task for Social Anxiety," Proc. ACM Hum.-Comput. Interact., [48] M. Gilbert, S. Demarchi, and I. Urdapilleta, "FACSHuman, a software
vol. 5, no. CHI PLAY, p. Article 248, 2021, doi: 10.1145/3474675. program for creating experimental material by modeling 3D facial
[32] R. Zhu and C. Yi, "Avatar design in Metaverse: the effect of avatar-user expressions," Behavior Research Methods, vol. 53, no. 5, pp. 2252-2272,
similarity in procedural and creative tasks," Internet Research, vol. ahead- 2021/10/01 2021, doi: 10.3758/s13428-021-01559-9.
of-print, no. ahead-of-print, 2023, doi: 10.1108/INTR-08-2022-0691. [49] A. S. GarcÃ­a, P. FernÃ¡ndez-Sotos, M. A. Vicente-Querol, G. Lahera, R.
[33] R. Cuthbert, S. Turkay, and R. Brown, "The Effects of Customisation on Rodriguez-Jimenez, and A. FernÃ¡ndez-Caballero, "Design of reliable
Player Experiences and Motivation in a Virtual Reality Game," presented virtual human facial expressions and validation by healthy people,"
at the Proceedings of the 31st Australian Conference on Human- Integrated Computer-Aided Engineering, vol. 27, pp. 287-299, 2020, doi:
Computer-Interaction, Fremantle, WA, Australia, 2020. [Online]. 10.3233/ICA-200623.
Available: https://doi.org/10.1145/3369457.3369475. [50] D. Kollias, "ABAW: Learning from Synthetic Data & Multi-task
[34] J. Koulouris, Z. Jeffery, J. Best, E. O'neill, and C. Lutteroth, "Me vs. Learning Challenges," presented at the Computer Vision â€“ ECCV 2022
Super (wo) man: Effects of Customization and Identification in a VR Workshops, Cham, 2023.
Exergame," in Proceedings of the 2020 CHI Conference on Human [51] M. Bishay and I. Patras, "Fusing Multilabel Deep Networks for Facial
Factors in Computing Systems, 2020, pp. 1-17. Action Unit Detection," presented at the 2017 12th IEEE International
[35] P. Salehi et al., "Is More Realistic Better? A Comparison of Game Engine Conference on Automatic Face & Gesture Recognition (FG 2017), 30
and GAN-based Avatars for Investigative Interviews of Children," May-3 June 2017, 2017.
presented at the Proceedings of the 3rd ACM Workshop on Intelligent [52] G. B. Dell'Olio and M. Sra, "FaraPy: An Augmented Reality Feedback
Cross-Data Analysis and Retrieval, Newark, NJ, USA, 2022. [Online]. System for Facial Paralysis using Action Unit Intensity Estimation,"
Available: https://doi.org/10.1145/3512731.3534209. presented at the The 34th Annual ACM Symposium on User Interface12
Software and Technology, Virtual Event, USA, 2021. [Online]. Megan A. Witherow received the B.S. degree in computer engineering from
Available: https://doi.org/10.1145/3472749.3474803. Old Dominion University (ODU), Norfolk, VA, USA in 2018. She is
[53] T. Guha et al., "On quantifying facial expression-related atypicality of currently a PhD candidate at the Vision Laboratory, Dept. of Electrical and
children with Autism Spectrum Disorder," in 2015 IEEE International Computer Engineering, ODU, and a 2020 NSF Graduate Research Fellow.
Conference on Acoustics, Speech and Signal Processing (ICASSP), 19-24 Her research interests include computer vision, deep learning, human-
April 2015 2015, pp. 803-807, doi: 10.1109/ICASSP.2015.7178080. computer interaction, and affective computing.
[54] M. D. Samad, J. L. Bobzien, J. W. Harrington, and K. M. Iftekharuddin,
"Analysis of facial muscle activation in children with autism using 3D Crystal Butler received the Ph.D. in computer science from New York
imaging," presented at the 2015 IEEE International Conference on University (NYU) in 2021. Her academic research focused on applying
Bioinformatics and Biomedicine (BIBM), 9-12 Nov. 2015, 2015. natural language processing techniques to crowdsourced label sets, which
[55] M. D. Samad, J. L. Bobzien, J. W. Harrington, and K. M. Iftekharuddin, were provided by human annotators in response to generatively modeled
"[INVITED] Non-intrusive optical imaging of face to probe physiological synthetic facial expression imagery. She has over 600 hours of experience
traits in Autism Spectrum Disorder," Optics & Laser Technology, vol. 77, analyzing videos and images with the Facial Action Coding System (FACS), a
pp. 221-228, 2016/03/01/ 2016, doi: comprehensive and widely used system for describing facial expressions.
https://doi.org/10.1016/j.optlastec.2015.09.030. Currently, she works for Mayo Clinic on the development of generative
[56] H. Bar and M. T. Wells, "On Graphical Models and Convex Geometry," artificial intelligence applications in the field of radiology.
(in eng), Comput Stat Data Anal, vol. 187, Nov 2023, doi:
10.1016/j.csda.2023.107800. Winston J. Shields received the M.S. degree in computer science from Old
[57] P. Frankl and H. Maehara, "Some geometric applications of the beta Dominion University (ODU), Norfolk, VA, in 2022. He is currently a full
distribution," Annals of the Institute of Statistical Mathematics, vol. 42, stack software engineer at the company CoStar Group.
no. 3, pp. 463-474, 1990/09/01 1990, doi: 10.1007/BF00049302.
[58] M. A. Witherow, M. D. Samad, N. Diawara, H. Y. Bar, and K. M. Furkan Ilgin received the BS degree in electrical engineering from the Old
Iftekharuddin, "Deep Adaptation of Adult-Child Facial Expressions by Dominion University, Norfolk, VA, USA in 2023 and the MBA degree from
Fusing Landmark Features," IEEE Transactions on Affective Computing, Western Governors University, Millcreek, UT. He is currently working
pp. 1-12, 2023, doi: 10.1109/TAFFC.2023.3297075. toward the ME degree in Systems Engineering from University of Virginia,
[59] T. Kanade, J. F. Cohn, and T. Yingli, "Comprehensive database for facial Charlottesville, VA. His research interests include machine learning,
expression analysis," presented at the Proceedings Fourth IEEE renewable energy, energy storage systems, microgrid design, engineering
International Conference on Automatic Face and Gesture Recognition management, risk analysis and human technology interaction.
(Cat. No. PR00580), 28-30 March 2000, 2000.
[60] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Norou Diawara is Professor of Statistics in the Mathematics and Statistics
Matthews, "The Extended Cohn-Kanade Dataset (CK+): A complete Department at Old Dominion University, Norfolk, VA, USA. Prof. Diawara
dataset for action unit and emotion-specified expression," presented at the received his B.S. at the University Cheick Anta Diop in Dakar, Senegal;
2010 IEEE Computer Society Conference on Computer Vision and MaÃ®trise in Mathematics at University of Le Havre, France; Masterâ€™s in
Pattern Recognition - Workshops, 13-18 June 2010, 2010. Statistics at University South Alabama; and Ph.D. in Statistics at Auburn
[61] M. Mavadati, P. Sanger, and M. H. Mahoor, "Extended disfa dataset: University, AL in 2006. His research areas are in estimation techniques of
Investigating posed and spontaneous facial expressions," presented at the time to event data analyses and neighborhood level causal effects. Such
proceedings of the IEEE conference on computer vision and pattern research interests may be included in choice models, statistical pattern
recognition workshops, 2016. recognition using copulas and spatial-temporal models.
[62] S. M. Mavadati, M. H. Mahoor, K. Bartlett, P. Trinh, and J. F. Cohn,
"Disfa: A spontaneous facial action intensity database," IEEE Janice Keener, PsyD, has extensive training in the assessment of Autism
Transactions on Affective Computing, vol. 4, no. 2, pp. 151-160, 2013. Spectrum Disorder. She is a Certified Trainer in the administration of the
[63] A. Iliev, N. Kyurkchiev, and S. Markov, "On the approximation of the Autism Diagnostic Observation Schedule-Second Edition and obtained her
step function by some sigmoid functions," Mathematics and Computers in research reliability from the Center for Autism and the Developing Brain. Dr.
Simulation, vol. 133, pp. 223-234, 2017/03/01/ 2017, doi: Keener's research and clinical interests include Pediatric Health Psychology,
https://doi.org/10.1016/j.matcom.2015.11.005. early childhood assessment, consultation and liaison, and Autism Spectrum
[64] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, "Policy gradient Disorder. She is certified in the treatment of Tourette Disorder from the
methods for reinforcement learning with function approximation," Tourette Syndrome Behavioral Therapy Institute Program. She is also bi-
Advances in neural information processing systems, vol. 12, 1999. lingual and provides assessment and psychotherapy in Spanish and English.
[65] M. R. Rezaei-Dastjerdehei, A. Mijani, and E. Fatemizadeh, "Addressing
Imbalance in Multi-Label Classification Using Weighted Cross Entropy John W. Harrington, MD, FAAP is Professor of Pediatrics and the long-time
Loss Function," presented at the 2020 27th National and 5th International Division Director of General Academic Pediatrics at Eastern Virginia Medical
Iranian Conference on Biomedical Engineering (ICBME), 26-27 Nov. School and Childrenâ€™s Hospital of The Kingâ€™s Daughters (CHKD) in Norfolk,
2020, 2020. VA, USA. He currently is the new Vice-President of Quality/Safety and
[66] A. H. Mostafa, H. Abdel-Galil, and M. Belal, "Ensemble Model-based Clinical Integration at CHKD. He has received multiple grants and is widely
Weighted Categorical Cross-entropy Loss for Facial Expression published in his main areas of interest: autism, obesity, and vaccine delivery.
Recognition," presented at the 2021 Tenth International Conference on
Intelligent Computing and Information Systems (ICICIS), 5-7 Dec. 2021, Khan M. Iftekharuddin received the B.Sc. degree in electrical and electronic
2021. engineering from the Bangladesh Institute of Technology, Dhaka, Bangladesh,
[67] L. N. Smith, "Cyclical learning rates for training neural networks," in 1989, and the M.S. and Ph.D. degrees in electrical and computer
presented at the 2017 IEEE winter conference on applications of engineering from the University of Dayton, Dayton, OH, USA, in 1991 and
computer vision (WACV), 2017. 1995 respectively. He is a professor and Batten Endowed Chair in Machine
[68] A. Papoutsaki, P. Sangkloy, J. Laskey, N. Daskalova, J. Huang, and J. Learning in the department of Electrical and Computer Engineering at Old
Hays, "Webgazer: scalable webcam eye tracking using user interactions," Dominion University (ODU), Norfolk, VA, USA, and the Director of the
presented at the Proceedings of the Twenty-Fifth International Joint ODU Vision Laboratory. His research interests include computational
Conference on Artificial Intelligence, New York, New York, USA, 2016. modeling, machine learning, medical image analysis, omics data analysis,
distortion-invariant recognition, biologically inspired human and machine
centric recognition, and computer vision.