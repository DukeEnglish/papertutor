Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
Huijie Tang 1, Federico Berto 1, Jinkyoo Park12
‚àó ‚àó
Abstract‚ÄîMulti-Agent Reinforcement Learning (MARL) problem, they solve MAPF by regarding it as a sequential
based Multi-Agent Path Finding (MAPF) has recently gained decision-making problem. One of the seminal MARL-based
attention due to its efficiency and scalability. Several MARL-
approaches to MAPF is PRIMAL[11]. In PRIMAL, the
MAPF methods choose to use communication to enrich the
authors propose to learn a fully decentralized policy with
information one agent can perceive. However, existing works
still struggle in structured environments with high obstacle partial agent-wise observation; the model is trained with
density and a high number of agents. To further improve A3C [15] and imitation learning (IL), However, PRIMAL
the performance of the communication-based MARL-MAPF strugglestoscaletohigh-obstacle-densityenvironmentswith
solvers,weproposeanewmethod,EnsemblingPrioritizedHy-
many agents. Also, PRIMAL assumes that agents move
bridPolicies(EPH).Wefirstproposeaselectivecommunication
sequentially in each time step, which differs from real- blocktogatherricherinformationforbetteragentcoordination
withinmulti-agentenvironmentsandtrainthemodelwithaQ- world scenarios where agents move simultaneously in each
learning-basedalgorithm.Wefurtherintroducethreeadvanced time step. The improved version of PRIMAL, PRIMAL2
inference strategies aimed at bolstering performance during [12], also trains with A3C and IL. PRIMAL2 assumes the
the execution phase. First, we hybridize the neural policy with
disappear at target [1] scenario. This assumption makes the
single-agent expert guidance for navigating conflict-free zones.
problemeasierbecauseitmeanslowerobstacledensitywhen
Secondly, we propose Q value-based methods for prioritized
resolution of conflicts as well as deadlock situations. Finally, agents reach their goals and disappear, whereas in the stay
we introduce a robust ensemble method that can efficiently attarget[1]setting,agentsconstituteobstaclessincetheydo
collect the best out of multiple possible solutions. We empiri- notdisappearafterreachingtheirgoals.Thus,theproblemis
cally evaluate EPH in complex multi-agent environments and
much harder, especially when many agents exist. We adopt
demonstrate competitive performance against state-of-the-art
stay at target in this work.
neural methods for MAPF.
Apart from the abovementioned MARL-based solvers
I. INTRODUCTION [12],[11]thatconsiderenvironmentsthatdonotallowcom-
Multi-Agent Pathfinding (MAPF) involves finding munication, another work direction is MARL-based solvers
collision-free paths for a group of agents while also aiming with communication between agents when communication
to minimize makespan or sum of costs [1]. MAPF has is possible, such as DHC [14] and DCC [13]. DHC is
many practical applications, including warehouse robotics trained with Deep Q-learning [16] and adopts a similar
[2], aviation [3], and digital gaming [4] scenarios. However, setting as PRIMAL, where each agent has a partial ob-
the challenge intensifies with the realization that optimal servation. However, DHC is different in that it introduces
solutions for MAPF are NP-hard, involving solving large- Graph Convolutional Communication, which enables agents
scale constraint satisfaction problems in combinatorial tocommunicatewithothernearingagentsforabetterunder-
spaces [5], [6]. Classical algorithms to solve the MAPF standing of the environment, thus improving performance.
problems such as heuristic centralized solvers typically Its improved version, DCC, improves communication by
invoke search-based algorithms, e.g., Conflict-Based Search not learning broadcast but selective communication, thus
(CBS) [7], as well as its improved versions ECBS [8] and reducing communication overhead. Recent MARL-based
EECBS [9]. However, as the number of agents increases, MAPF works like SCRIMP [17] and SACHA [18] also
many heuristic solvers struggle to scale because of the use communication mechanisms. For example, in SCRIMP,
high complexity of considering all agents in a centralized globalcommunication isused togather informationfrom all
manner at once. In addition, in real-world applications, new agents in the environment, but this might be inefficient and
tasks frequently occur during the execution of the initially burdensome in real-world applications.
planned paths [10]. Such centralized heuristics solvers have Contributions. In this paper, we propose EPH
to re-plan paths for all the agents after seeing new tasks, (Ensembling Prioritized Hybrid Policies), a Q-learning-
making it burdensome to apply to real-world applications. based MARL-MAPF solver with communication. Our
Multi-Agent Reinforcement Learning (MARL) based ap- communication scheme entails enhanced selective
proaches[11],[12],[13],[14]offeranotherwaytosolvethe communication block with improvements inspired by
MAPF problem. Instead of treating MAPF as a centralized the latest Transformer variant to enable richer information
extraction. Additionally, we introduce several strategies
‚àóEqualcontributions to be used during the inference phase to further bolster
1DepartmentofIndustrialandSystemsEngineering,KAIST,SouthKorea
performance. Firstly, we propose Q value-based priority
2OMELET
‚Ä°AuthorsaremembersoftheAI4COopenresearchcommunity. decisions, in which the Prioritized Conflict Resolution
Emails:{raylan.tang;fberto;jinkyoo.park}@kaist.ac.kr decides the priority of agents involved in conflicts, and
4202
raM
21
]AM.sc[
1v95570.3042:viXraAdvanced Escape Policy is responsible for breaking locationandauniquegoallocation,withinamapcontaining
deadlocks. Secondly, we use hybrid expert guidance for obstacles. The map M is an undirected graph M =(V,E),
agents that have no other live agents nearby. Lastly, we with some vertices being inaccessible obstacles V o V. An
‚äÇ
utilizeensemblingtosamplethebestsolutionsfrommultiple obstacle collision occurs when agent i reaches the location
solvers that run in parallel to leverage the different strengths of an obstacle. Given an agent set N, each agent i N has a
of each strategy proposed. By improving communication startingvertexv0 V andagoalvertexvg V.We‚àà definethe
i ‚àà i ‚àà
capabilityandutilizingproposedstrategiestohelpinference, observation space O i and action space A i for agent i, which
we achieve competitive performance against state-of-the-art is, at each time step t =0, ,t max, each agent i located in
neural MARL-MAP IIF
.
s Ro Elv Le Ars T.
EDWORKS
v sit
i n‚àà
glV
e
ah ca ts ioa nn ao
t
ib ‚ààse Ar iv ,a ati no dn mo¬∑ of¬∑ v¬∑ t ehe tom va
t
ip +1o .t
i t m‚àà
axO ii, sd the ecid me as xo imn aa
l
allowedtimetofinishtheMAPFproblem.Avertexcollision
Search-based MAPF. Traditional centralized MAPF occurswhenagentiandagent jreachthesamevertexvatthe
solvers often invoke search-based algorithms, and they same time, step t; an edge conflict occurs when agent i and
can be categorized by their optimality. Optimal traditional agent j traverse through the same edge (u,v) in the opposite
heuristics solvers include Conflict-Based Search (CBS) [7] direction.AMAPFsolutionexistsifandonlyifforanyagent
a on nd ait cs onim strp ar io nv te td reeve tr osio fin nd[1 o9 p] t, im[2 a0 l] s. oH luo tiw onev se ar, ndCB caS nr ie nl cie us r i vt‚àà+1N =,th ve gr ,e we hx eis rt es0 a0‚â§ (vt 0‚â§ )=t m vax 1,s .u ..c ,h att (h va tt )a =t i(cid:0) v¬∑t¬∑+¬∑ 1a1 i =(cid:0) a v0 i g ;(cid:0) v a0 i n(cid:1) d(cid:1)(cid:1) fo= r
i i i i i i i i i
heavy computational overhead when the number of nodes in any agent i N, there are no collisions.
‚àà
the constraint tree is high with many agents. The bounded-
suboptimal solvers include CBS variants [9], [8]. These B. Environment Settings
solvers can scale to environments with a larger number
Following the conventions of MAPF, we use a 2D grid
of agents while also guaranteeing the (sub-)optimality
world to represent the environment. Each cell in the n n
of generated solutions. However, their scalability is still √ó
grid world can be either an empty cell or an obstacle that
hinderedbytheirexponentialtimecomplexity[21].Besides,
stops agents from passing1. Each agent occupies an empty
when a new task or agent is added to the environment, such
cell, and the task for each agent is moving from its starting
heuristics solvers have to re-plan the whole paths for all
cell to the goal cell without collisions. At the beginning of
agents.
each episode, m starting cells and m goal cells are randomly
MARL-based MAPF. Recently, various approaches have
selected among the empty cells for m agents, and we make
emerged to address the MAPF problem using multi-agent
sure that there is no overlap among 2m selected cells. At
reinforcement learning (MARL) techniques, building upon
each timestep, the agents choose to move either up, down,
the foundation laid by PRIMAL [11]. These approaches
left, right, or stay still. We consider four types of conflicts:
exhibit diversity in their settings and strategies. In terms
swapping conflict [1], vertex conflict [1], conflict with static
of the problem settings, some works consider scenarios
obstacles, and out-of-bound conflict. These four types of
with partial observation, where agents do not have complete
conflicts are not allowed during pathfinding.
knowledge of the environment [14], [13], [11], [12], while
We use a partially observable environment. At each time
others operate under full observation, assuming agents pos-
step, each agent has a limited Field of View (FOV) that
sess complete information about the environment [22], [23].
consists of six channels: four channels are the heuristic
Furthermore,thechoiceofRLalgorithmvariesacrossdiffer-
channels described in [14], the remaining two channels are
ent works. Some employ Actor-Critic approaches [11], [12],
used to represent the locations of all agents within the FOV,
[17],whereasothersoptforvalue-basedQ-Learningmethods
and the location of obstacles within the FOV, respectively.
[14],[13],[23],[24].Inaddition,certainapproachesconsider
communication mechanisms among agents to enhance coor-
IV. EPH:ENSEMBLINGPRIORITIZEDHYBRIDPOLICIES
dination [14], [13], [25], [26], [17]. These communication
protocols allow agents to share information and collaborate We propose EPH and describe it in two sections, each
effectively. Some recent MARL-based MAPF solvers also with several subsections. Section IV-A describes how we
propose the use of techniques to help pathfinding during the formulate MAPF problem as a Q-Learning based MARL
inference phase. Gao et al. [27] propose an escape policy problemwithcommunicationandhowwetrainsuchamodel.
for structured environments that helps neural solvers escape SectionIV-Bgivesintroductionabouttheadvancedinference
fromdeadlockscenarios,whichforcesagentstotakerandom strategies that we propose to improve performance during
actions attempting to break the deadlock. Such inference execution. Figure 1 shows the overall architecture of EPH.
techniques can improve the performance of neural solvers
under certain predefined conditions [28]. A. Q-LearningbasedMARLwithCommunicationforMAPF
In this section, we first introduce our model with the
III. PROBLEMFORMULATION
newcommunicationblock,andthenweintroducethemodel
A. MAPF Definition
training via Double Dueling Deep Q Networks (D3QN).
TheMAPFprobleminvolvesfindingasetofcollision-free
paths for multiple agents, each with its own distinct starting 1Weconsidervalue1onmapM asanobstacleand0asavailable.Neural Network (Section IV-A)
0 0 0 0 0
0 10 10 00 10 1
0 0 1 10 0 1 11 10 0 01 11 10 0 01 00 10 0 01 10 1 01 1 0 L( eC ao kn yv R2 ed L U) GRU C‚Ñé oncat. C Bo lom cm k C Bo lom cm k ùëí Advantage
0 10 010 0110 00010 0011 101 00 0 4 State
1 01 011 0011 00010 0000 000 00 1
1 0 00 0 00 00 0 00 00 01 0 00 00 01 0 10 10 0 0 00 0 0 0 [‡∑™‚Ñé ùëñùë°,‚Ä¶‚Ñé‡∑™ ùëòùë°,‚Ä¶] Layer Multi-head GRU Layer MLP GRU ùëí
Norm Graph Conv. Norm
0 01 00 00 00 0
0 0 0 0 0
Environment t Observation Communication Block
Inference Strategies (Section IV-B)
Expert
ùúè
ùëé Guide. ùëé
argmax
ùëé‡∑°
Yes Expert
ùëó Neural ùëó argmax ùëé ùëó Guide. ùúè ùëé‡∑© ùëó P Cri oo nri ft li iz ce t d A Edv sca an pc ee d ùëé‡∑°
Network No Resolution Policy ùëó
‚Ä¶ ‚Ä¶ argmax ‚Ä¶‚Ä¶ ùúè ‚Ä¶
Active
agents in ?
Final actions Environment t+1
Fig. 1: Overview of a single inference step of EPH. The upper part is the neural network structure of EPH; the lower part is the illustration of how
observationsofallagentsaretransformedintoactionsbyfirstfeedingintotheneuralnetwork,andthengoingthroughtheproposedinferencestrategies.
TheœÅ andœÑ inthelowerpartarethehyperparametersdefinedinSectionIV-B.
1) Graph Convolution based Communication: We show [33]. The Q value for agent i is obtained via:
our communication architecture in Figure 1. Our commu-
nication structure is adapted from selective communication Qi s,a=Val s(cid:0) et i(cid:1) +Adv(cid:0) et i(cid:1) a‚àí A1 ‚àëAdv(cid:0) et i(cid:1) a
‚Ä≤
(1)
blockinDCC[13].Comparedwithordinarycommunication, | | a ‚Ä≤
selective communication can effectively reduce communi- where Val() and Adv() represents state advantage and
cation time compared with non-selective counterparts, it action advan¬∑ tage function¬∑ , respectively. et is the final output
i
can also selectively gather more informative and important of communication for agent i containing information from
hidden information from other agents within FOV, which otheragentswhomichoosestocommunicatewith.A isthe
is shown to be beneficial for pathfinding [13]. However, in action space. The loss for training EPH is expressed as:
the previous work, the selective graph convolution commu- L(Œ∏)=MSE(cid:0) Ri Qi (Œ∏)(cid:1) (2)
nication block did not include position-wise processing or t ‚àí st,at
normalization, and it is shown that including them helps where MSE is the mean square error. We have Ri =ri+
t t
stabilize training and gives better scalability [29]. Inspired Œ≥ri +...+Œ≥nQi (Œ∏¬Ø), where ri is the reward received
t+1 st+n,at+n t
by GTrXL-styled transformer in [29], we re-design the com- byagentiattimet,andŒ∏¬Ø istheparameterfortargetnetwork
municationblocktoenablebettercoordinationamongagents which is updated to align with online parameter Œ∏ at a
by including extra multi-layer perception layer, GRU block, predefined interval. The reward structure for D3QN is taken
and layer normalization, which is shown to be beneficial from DHC [14] as shown in the Table I. Negative rewards
in many works across different fields [30], [17], [29]. The aregiventoagentswhodon‚Äôtreachthegoaltoexpeditegoal
generation method of communication mask for selective reaching in the shortest distance.
communication, as well as the request-reply communication
schemethatconsistsoftwocascadedcommunicationblocks, TABLEI:RewardStructure
arekeptthesameasshowninDCCtoreducecommunication
Actions Rewards
overhead and enrich the information gathered.
‚û†MoveandStayoffGoal -0.075
‚úîStayonGoal 0
2) TrainingofQ-learningbasedMARLwithCommunica- ‚úñCollision -0.5
‚òÖReachGoal 3
tion: WetrainthemodelwithD3QN(DoubleDuelingDeep
Q Network), which incorporates Deep Q-Learning Network InMAPF,therolesofeachagentareidentical:everyagent
(DQN) with both the Dueling DQN architecture [31], sep- has a unique start location and a goal location, and each of
arating parameters for advantage and value estimation, and them is expected to move in a collision-free manner and
Double DQN [32], which uses a target network to estimate reach the goal eventually. Given this observation, instead
the value to ameliorate the effects of Q value overestimation of training multiple policies for multiple agents, it‚Äôs moreh
in Layer Norm Mu Glt ri a- ph hea d x GRU
Conv. h
out GRU x‚Äô FFN Layer Ninorm Multi-head x
Layer Norm Graph GRU
h‚Äô Conv.
natural to adopt parameter sharing and train a single policy
out x‚Äô
fromasingleagent‚Äôsperspective.Althoughweadoptparam- GRU FFN Layer Norm
eter sharing, it should be emphasized that the trained policy
h‚Äô
can be used for multi-agent pathfinding. This is because 0 1 2
eachagentperceivesuniqueinformationfromitsownpartial
observations and inter-agent communication outcome and,
consequently, takes different actions.
B. Advanced Inferences Strategies for EPH F tai tg io. n2: imTy pp roes veo sf tA he‚àóœÑ sw ine gc leo -n as gid ener t. pœÑ at‚àà h{ b0 a, s1 e, d2 } o. nC eh aa cn hg sin itg uat th ioe nm .aprepresen-
Weproposethreethreeinferencetechniquestofurtherbol-
0 1 2
sterpathfindingperformanceduringinference.InSectionIV-
B.1, we hybridize the policy with single-agent optimal A
‚àó
paths to give guidance to agents that do not have any other
live agents nearby. In Section IV-B.2, we introduce two Q
value-based priority decision-making strategies, namely Pri-
oritized Conflict Resolution for efficient conflict resolution
and Advanced Escape Policy for priority-based avoidance
of deadlocks. Finally, Section IV-B.3 introduces the ensem-
bling, which runs multiple policies in parallel and samples
the best possible solutions in the solution space.
1) Hybrid Expert Guidance: When agent communication Fig. 3: Prioritized Conflict Resolution. Agents with higher Q values are
is sparse, such as in scenarios where no other agents are prioritizedwhenaconflicthappens,whichleadstoshorterpaths.
located within the agent‚Äôs field of view, it is beneficial
to incorporate low-cost single-agent expert path to guide 2) Value-based Priority Decisions: We further introduce
decision-making, thus hybridizing (EPH stands for Hybrid). twotechniquesforresolvingconflictsituationsanddeadlock
We hereby define a live agent as an agent that is currently scenarios based on priorities (EPH stands for Prioritized)
off its goal, i.e., vt =vg . We propose leveraging the optimal provided by the Q values obtained by the trained neural
iÃ∏ i
A ‚àó path as expert guidance during inference for an agent model. This is inspired by priority-based planning methods
that has no other live agents within a square centered on [34], [35], [36] and, in particular, Priority-Based Search
the agent itself; the length of the square is 2œÅ+1 where œÅ (PBS) [37], which first decides the priority for each agent
is the visibility radius for live agents. We adapt the efficient andthenplansindividualpathsforeachagentintheorderof
A ‚àó,awell-establishedlow-costalgorithmforefficientsingle- priority. In PBS, the agent with lower priority is not allowed
agent pathfinding, to multi-agent settings with the following tocollidewiththeagentwithhigherpriority.InourQvalue-
formulation: based priority decisions, an agent with a lower Q value will
a ‚àóit,a ‚àóit+1,...,a ‚àóiT =A ‚àóœÑ(vt i,vg i,M,Vt,Vg) (3) b ane oa thss ei rg an ge ed nta wlo itw he ar hp ir gio hr ei rty pra in od ritw y.ill have to give way to
wherea ‚àóit,a i‚àót+1,...,a ‚àóiT isthesequenceofactionsthatleads a) Prioritized Conflict Resolution: During pathfinding,
agent i to the goal and œÑ 0,1,2 is the A ‚àó type. Vt = agentsmaystilltakeinvalidactions,whichleadstoconflicts.
{vt 1,...,vt i,...,vt m
}
and Vg =‚àà(cid:8) v{g 1,...,v}g i,...,vg m(cid:9) is the current Compared with previous works [14], [13] that recursively
position set and goal set, respectively. We formulate each recover the states of agents that are involved in conflicts,
type œÑ as follows: we use several strategies to reduce collisions. When we
i) A ‚àó0: is the classic A ‚àó that takes the current map M for first sample actions from the Q value matrix generated by
avoidingobstaclesensuringfindingtheoptimalsingle- our network, we filter out actions that make agents collide
agent path. with static obstacles by masking the corresponding value in
ii) A ‚àó1: treats all agents as obstacles, except the current the Q value matrix. After we get static-obstacle-collision-
agent i; i.e., M(vt) 1 j, andj = i. All other free action for each agent, we let all agents execute their
j ‚Üê ‚àÄ Ã∏
agents are considered temporary obstacles within the actions. If collisions between agents happen, we decide the
A ‚àó calculation. This ensures collision avoidance. priority of agents involved in agent collision by giving the
iii) A ‚àó2: this version considers only inactive agents as highestprioritytotheagentwiththehighestQvalue,andre-
obstacles, i.e., M(vt) 1 j s.t. vt =vg . This version choosingactionsforotheragentswithsmallerQvalueswith
j ‚Üê ‚àÄ j j
tries to avoid agents already at goal only. This can their previous actions masked. The illustration of Prioritized
generally obtain better paths compared to A ‚àó1, since Conflict Resolution is shown in Figure 3. Since Q value
agents that are still moving do not interfere with representsthegoodnessandpreferenceofanagentchoosing
optimal path calculations. the corresponding action, it is natural to decide the priority
The illustration of three types of A is shown in Figure 2. of agents based on Q value.
‚àó
We find that using different A types can be beneficial for b) Advanced Escape Policy: Gao et al. [27] first pro-
‚àó
pathfinding in different scenarios. posed an escape policy to avoid deadlock situations, whichis a common case in structured environments. A deadlock while others don‚Äôt. This approach is designed to leverage
can occur when an agent is off its goal and oscillates back the complementary strengths of each strategy proposed to
and forth between two adjacent cells (Algorithm 1, lines 9- navigatecomplexmulti-agentpathfinding.Wecandividethe
10). However, in the original escape policy in [27], agents ensembling technique into two stages:
choose to break deadlocks just by taking random actions i) Parallel Solver Execution: For every episode, EPH
when deadlocks are detected, which can be suboptimal and executes multiple solvers with different settings in
lead to unnecessary randomness. In our Advanced Escape parallel,eachofthemhasdifferenttechniquesenabled
Policy, we use a two-stage approach to break the deadlocks. with different parameters. We treat each solution from
Firstly, we assign a priority to each agent by sorting the Q onesolverintheparallelexecutionasoneindependent
values in descending order. Then, for each agent in such pathfinding solution. Parallel execution allows for a
order, we utilize A ‚àóœÑ mentioned in Section IV-B.1 to obtain comprehensiveexplorationofpotentialsolutions,max-
the action. If A ‚àóœÑ fails to find a path because of other agents imizing the likelihood of identifying an optimal path.
involved in the deadlock, we choose the action from the ii) BestSolutionSampling:Uponcompletionofallparal-
highestvalidQvalue,whereinvalidQvalues(corresponding
lel solvers, EPH evaluates each independent solution
to actions that lead to a collision) are masked, i.e. set to from one solver based on its makespan [1], i.e., time
‚àû. Notably, we also update a copy of the map such that stepsrequiredtofinishthesingletestepisode.Foreach
‚àí
the current next action is masked, ensuring there will be episode, we take as a final solution the one with the
no conflict. Algorithm 1 illustrates our proposed Advanced lowest makespan among all parallel solvers.
Escape Policy.
This ensemble technique enhances EPH‚Äôs capability to
solve MAPF problems by providing a robust framework
Algorithm 1: Advanced Escape Policy for selecting the most effective pathfinding solver from a
Input: Goals setVg; agents‚Äô current positions setVt; set of pre-defined options. By systematically sampling the
position set for each agent for the past five time best solutions from multiple solvers running in parallel,
(cid:110) (cid:111)
stepsV ip= vt i,vt i‚àí1,vt i‚àí2,vt i‚àí3,vt i‚àí4 ; Q value EPH efficiently navigates the complexities of multi-agent
matrix Q=(cid:0) qt qt qt (cid:1) , i=1,...,m; initial environments, ensuring high performance and reliability in
1¬∑¬∑¬∑ i¬∑¬∑¬∑ m ‚àÄ
map M where 1 is an obstacle and 0 is available; the inference phase.
OutputA :‚àó At cy tip oe nsœÑ. A=(cid:8) at 1,...,at i,...,at m(cid:9)
V. EXPERIMENTS
1 Function MaskQValues(qt i,M,vt i):
// mask Q values of obstacles A. Training and Testing Settings
2 for a in A do
3 if M[a(vt i)]=1 then We use D3QN with a prioritized experience replay buffer
4 qt i(a) ‚Üê‚àí‚àû; [38] for training EPH. Curriculum learning [39], which is
5 return qt i; shown to be effective in previous works [14], [18], is also
adopted. The curriculum learning starts from one agent in
// Init actions based on max Q values
76 IA d iin x nit ‚Üêd= ea(cid:8) xra egt 1 s s, o. r. t b. (, yQat i A, Qi. n. it. ,, vda et am s l(cid:9) c ue‚Üê n ed sa inrg g (=m fT ia rx ruQ e s) t; ;
is
hig/ h/ ess to )rt 1 t 0i0 .m 9√ó e f1 ot0 h rem thsa eup c, cca uen rsd rs ee nrn a td te ts rao o in nf i4 nth0 ge√ó em4 nv0 o id rm e ola np munw ed ni et th ,r 1 wtr6 eaia n ig i ne n cn g rt es ar. e sE a ecv he thr ey es
8 for i in Idx do difficulty of the map. Thus, experience from more complex
// check if active and has deadlock
environments will be added to the buffer, and the model
19
0
if vt i iÃ∏=
f
vv
t
i‚àíg i 1th =en
vt i‚àí3 & vt i‚àí2=vt i‚àí4 then
l Te ha ern os bf sr to acm lem deo nre sita yn od fm mo ar pe
s
c inom trp al ie nx ingsc ie sn sa ari mos pleg dra fd ru oa mlly a.
// if A path found, use A
‚àóœÑ ‚àóœÑ
1 11 2 if ‚àÉ aA t i‚àóœÑ ‚Üê(vt i, Avg i ‚àóœÑ, (M vt i,, vV g it ,, MV ,g V) tt ,h Ve gn )[0] t Tri ha eng Fu Ola Vrd si is zt erib inuti oo un rb wet ow rkee in s0 9a √ónd 90 t. o5 aw lii gth na wp ie thak po rf ev0 i. o3 u3 s.
works [14], [13]. The maximum episode length for training
13 else
11 54 q att ii ‚Üê‚Üê aM ra gs mk aQ xV qt ialues(qt i,M,vt i); e trp ai is no id ne gs sti es ps25 a6 n. dW take es ta hv ee bem so td oe nl ec ah se tc hk ep fioi nn at ls me ov der ey l b1 a0 s0 e0 d
on performance on a random-map validation set. It takes
16 M[at i(vt i)] ‚Üê1// set next pos. as obstacle 20 hours to perform 150k training steps on a server with a
single NVIDIARTXA6000 and AMDEPYC7542(128)
@2.9GHZ. We also retrain DCC with the same setting for
3) Ensembling: In the inference phase, EPH (where E faircomparison.Wevalueopenreproducibilityandmakeour
stands for the ensembling) employs an ensemble technique code publicly available2.
that operates by sampling best solutions from solvers with We evaluate EPH on random maps and structured envi-
different settings that run in parallel, without engaging in ronments. For random maps, we test on 40 40 and 80 80
adaptive learning. Each of the solvers has its own A ‚àó type, maps with 0.3 obstacle density; the maxim√ó um allowed√ó time
either A , A or A , with different œÅ. Some of the solvers
‚àó0 ‚àó1 ‚àó2
have the value-based priority decision mechanism enabled, 2https://github.com/ai4co/eph-mapfSuccessRateon40 40Map SuccessRateon80 80Map EpisodeLengthon40 40Map EpisodeLengthon80 80Map
√ó √ó √ó 240 √ó
100 100 DHC DHC
180 DCC 220 DCC
90 SCRIMP SCRIMP
90 160 EPH 200 EPH
80
140 180
80 70 120 160
60 100 140
70
DHC 50 DHC 80 120
60 D SCC RC IMP 40 D SCC RC IMP 60 100
EPH EPH 80
4 8 16 32 64 4 8 16 32 64 128 4 8 16 32 64 4 8 16 32 64 128
NumberofAgents NumberofAgents NumberofAgents NumberofAgents
(a)SuccessRate (b)AverageEpisodeLength
Fig.4:ComparativeAnalysisofSuccessRateandAverageEpisodeLengthonrandommaps.
TABLEII:SolutionqualityfordifferentMAPFsolvers.Wereportepisodelength(EL,lowerisbetter )andsuccessrate(SR,higherisbetter ).
‚Üì ‚Üë
HeuristicsSolvers NeuralSolvers
CBS ODrM* wPBS PRIMAL DHC DCC SACHA SCRIMP EPH
Map m EL SR EL SR EL SR EL SR EL SR EL SR EL SR EL SR EL SR
4 51.74 100% 51.76 100% 69.32 92% 196.54 40% 86.56 100% 82.99 100% 81.43 100% 82.34 100% 79.05 100%
8 55.50 100% 78.74 88% 116.32 70% 245.02 8% 100.70 100% 97.95 99% 89.73 100% 99.58 100% 91.42 100%
16 118.97 68% 186.44 34% 208.28 24% 256.00 0% 109.24 100% 108.29 97% 96.74 100% 105.78 100% 104.87 100%
32 251.86 2% 256.00 0% 248.06 4% 256.00 0% 124.38 98% 119.15 97% 104.30 98% 115.39 100% 110.78 100%
64 256.00 0% 256.00 0% 256.00 0% 256.00 0% 153.17 93% 145.21 93% 142.97 94% 131.59 100% 121.66 100%
4 77.79 100% 77.79 100% 104.41 94% 355.80 42% 146.12 99% 135.89 99% 134.59 99% 197.79 87% 134.56 100%
8 83.48 100% 100.37 96% 170.46 80% 451.82 18% 198.82 91% 169.50 96% 166.72 93% 304.22 66% 151.94 100%
16 81.64 100% 133.59 88% 340.18 40% 492.04 8% 281.37 74% 208.72 90% 198.72 76% 366.98 48% 164.05 100%
32 262.15 58% 417.22 22% 512.00 0% 505.58 4% 432.28 28% 335.81 58% 354.33 48% 451.40 21% 176.35 100%
64 494.93 4% 512.00 0% 512.00 0% 512.00 0% 512.00 1% 473.92 14% 437.29 28% 504.26 4% 189.58 100%
stepforthemis256and386,respectively.Thesetofparam- agent can receive. Imitation learning is also utilized to help
etersœÑ andœÅ usedbyensembleforrandommapsareselected SCRIMP learn fromexpert demonstrations from multi-agent
fromaCartesianproductofthespaces 0,1,2 2,3,4,5 . solvers during training alongside PPO [40].
{ }√ó{ }
For structured environments, we use the benchmark from Figure 4a shows the success rate of EPH compared with
[1] for the test. Specifically, we choose the Dragon Age baselines. On 40 40 map, EPH beats all the baselines in
√ó
Origins map den312d (65 81) and warehouse map terms of success rate, reaching 100% success rate for all
√ó
(161 63) from the benchmark for the test to showcase the cases; on the 80 80 map, EPH outperforms both DHC
√ó √ó
EPH‚Äôsperformanceincomplexstructuredenvironments.The and DCC in all cases, while falling behind SCRIMP by
maximumallowedtimestepforden312dandwarehouse 1% on 32 and 64 agents scenarios. Figure 4b illustrates the
is 256 and 512, respectively. The set of parameters œÑ and œÅ average episode length, which indicates the solution quality,
used by ensemble for structured maps are selected from a ofallfourmodels. EPH outperformsbothDHCandDCCin
Cartesianproductofthespaces 0,1,2 3,4 .Forrandom termsofaverageepisodelengthinallcases.Whencompared
{ }√ó{ }
maps,wereportthesuccessrateandepisodelengthaveraged against SCRIMP, EPH can outperform SCRIMP on larger
across 100 test instances for each case. For structured maps, 80 80 map. On 40 40 map, EPH can achieve a shorter
√ó √ó
weaverageacross300toalignwiththesettinginoneofour average episode length than SCRIMP except in the 64-agent
baseline SACHA [18]. The success rate is the percentage case.ItshouldbenotedthatSCRIMPusesglobalcommuni-
of test instances fully solved, and the episode length is the cation,whichcouldgivethemodelmoreinformationthanwe
makespan [1] of an instance. could. However, global communication is burdensome and
inefficient in real life; also, global communication may be
B. Results on Random Maps limitedbygeographicalconstraints.Bycontrast,EPH,which
achieves better performance than baselines in most cases,
We choose DHC, DCC, and current state-of-the-art
only uses improved local selective communication. EPH
MARL-MAPF solver SCRIMP [17] as our baselines. DHC
equippedwithlocalselectivecommunicationisinformation-
and DCC both utilize local communication to gather in-
efficient,lessburdensome,andpracticalinreal-lifescenarios.
formation from other agents nearby to give the model
moreinformationandunderstandingbeyondtheagent‚Äôsown
C. Results on Structured Maps
FOV, and both use Q-learning. While DHC chooses to
communicate with all other agents nearby, DCC selectively We additionally test on structured maps that were never
communicates with them. SCRIMP uses Transformer-based seen during training to showcase the performance of base-
globalcommunicationtoevengathermoreinformationfrom lines and EPH in terms of scalability and generalization
all agents in the environment to enrich the information one to unseen scenarios. For structured maps, apart from the
)%(etaRsseccuSegarevA
d213ned
esuoheraw
)%(etaRsseccuSegarevA htgneLedosipEegarevA htgneLedosipEegarevAabovementioned baselines, we further include three addi- it is also noticeable that the Base outperforms DCC in
tional heuristic solvers: CBS [7], wPBS [10] and ODrM both structured maps, illustrating the effectiveness of our
‚àó
[41]; as well as two neural solvers: PRIMAL [11] and the improved communication scheme.
recentSACHA[18]asbaselines.SACHAisaneuralMAPF We also conduct an ablation study regarding the impact
solver trained with Soft Actor-Critic and employs optional of A types of the hybrid expert guidance, i.e., the œÑ, on
‚àó
global communication blocks. We choose to compare with EPH‚Äôsperformance.Weobservethatinden312dmapwith
SACHA equipped with global communication block for fair 64 agents, different œÑ only leads to 4% of difference in
comparison. For the heuristics solvers, the runtime limit is success rate. However in warehouse map with 64 agents,
120secondsforCBSandwPBS,and20secondsforODrM , the difference jumps to 79%, and the model with hybrid
‚àó
same as the settings used in SACHA. expert guidance with A only achieves 18% success rate.
‚àó0
TableIIshowstheperformanceofdifferentsolversintwo This signifies again that different strategies have their own
structured environments. In den312d map, EPH beats all applicability and the effectiveness of ensembling.
neural baselines in terms of success rate. For the average
episode length, we outperform all neural baselines except
VI. CONCLUSIONS
SACHA in den312d map with 8 to 32 agents. It is notice- In this paper, we proposed EPH, a novel learning ap-
able that though SACHA achieves shorter episode length in proachforMulti-AgentPathFinding(MAPF).Ourapproach
these cases, it fails to achieve a higher success rate when employs an enhanced selective communication scheme and,
the number of agents increases. By contrast, EPH has better at inference time, efficiently samples multiple solutions by
scalability and achieves 100% success rate in den312d ensemblingsolverswithconfigurationsofneuralvalue-based
map with 64 agents. In the warehouse map, which is a priorities for resolving conflicts, advanced escape policy
commonsetupinreal-worldscenarios,EPHexcelsallneural for deadlock avoidance, and hybrid expert guidance. EPH
baselines in all cases in terms of both metrics, showcasing demonstrated competitive performance against state-of-the-
the practicality of our method in real-world applications. art neural MAPF baselines in complex environments.
It is noticeable that heuristic solvers generally offer better We finally define some future works that may arise from
solutions in cases with few agents. However, the scalability EPH. Firstly, training with other RL algorithms such as
issue, which is common for heuristics solvers, prevents on-policy algorithms [42], [40], [43], where priorities may
them from reaching better results when the agent number be assessed via values from the critic network, could fur-
increases. ther boost the performance. Importantly, better hybridization
techniques with existing inexpensive low-level solvers may
D. Ablation Studies
help in particular by avoiding deadlock situations arising
in highly-structured environments [12]. Ultimately, we be-
TABLEIII:EffectivenessofEPHcomponentswithalargenumberofagents
m=64.Baseisthemodelthatonlyhasimprovedcommunication. lieve hybridizing learned neural solvers with their classical
counterparts, as has been suggested in other combinatorial
den312d warehouse
domains for both obtaining better solutions [44], [45], [46],
Method EL SR EL SR [47]andgeneratingbetterheuristics[48],[49],isapromising
research avenue for scalable and generalizable MAPF.
Base 140.70 97% 479.42 15%
+ Prioritized Decisions 129.84 100% 454.16 22%
ACKNOWLEDGMENT
+ Hybrid Guidance 130.16 100% 255.31 88%
+ Ensemble 121.66 100% 189.58 100% We thank Qiushi Lin for providing us with help for the
performance results of baselines in structured maps.
In Table III, we showcase the effectiveness of the Q
REFERENCES
value-based priority decision-making strategy, hybrid expert
guidance, and ensembling outlined in Section IV-B. Base [1] R.Stern,N.Sturtevant,A.Felner,S.Koenig,H.Ma,T.Walker,J.Li,
D.Atzmon,L.Cohen,T.Kumaretal.,‚ÄúMulti-agentpathfinding:Defi-
is the model that only has the proposed improved commu-
nitions,variants,andbenchmarks,‚ÄùinProceedingsoftheInternational
nication block. We use A ‚àó2 in the relative parts of value- SymposiumonCombinatorialSearch,vol.10,no.1,2019,pp.151‚Äì
based priority decision-making strategy and hybrid expert 158.
[2] P.R.Wurman,R.D‚ÄôAndrea,andM.Mountz,‚ÄúCoordinatinghundreds
guidance. In den312d, it can be seen that all three infer-
of cooperative, autonomous vehicles in warehouses,‚Äù AI magazine,
ence strategies help achieve better performance compared vol.29,no.1,pp.9‚Äì9,2008.
with Base. In warehouse, the same trend is preserved. [3] R. Morris, C. S. Pasareanu, K. S. Luckow, W. Malik, H. Ma, T. S.
Kumar, and S. Koenig, ‚ÄúPlanning, scheduling and monitoring for
However, we observe that different strategies have different
airport surface operations.‚Äù in AAAI Workshop: Planning for Hybrid
improvements in different scenarios. For example, hybrid Systems,2016,pp.608‚Äì614.
guidance does not offer much improvement in maps that [4] H. Ma, J. Yang, L. Cohen, T. Kumar, and S. Koenig, ‚ÄúFeasibility
study: Moving non-homogeneous teams in congested video game
are not highly congested and structured, such as den312d;
environments,‚Äù in Proceedings of the AAAI Conference on Artificial
but in highly structured environments like warehouse, it IntelligenceandInteractiveDigitalEntertainment,vol.13,no.1,2017,
offerssignificantimprovements.Thissignalsthatensembling pp.270‚Äì272.
[5] J. Yu and S. LaValle, ‚ÄúStructure and intractability of optimal multi-
to take advantage of different solvers would be the best
robotpathplanningongraphs,‚ÄùinProceedingsoftheAAAIConference
choice to give the model the best performance. In addition, onArtificialIntelligence,vol.27,no.1,2013,pp.1443‚Äì1449.[6] J.Banfi,N.Basilico,andF.Amigoni,‚ÄúIntractabilityoftime-optimal [27] J. Gao, Y. Li, X. Yang, and M. Tan, ‚ÄúRde: A hybrid policy
multirobotpathplanningon2dgridgraphswithholes,‚ÄùIEEERobotics framework for multi-agent path finding problem,‚Äù arXiv preprint
andAutomationLetters,vol.2,no.4,pp.1941‚Äì1947,2017. arXiv:2311.01728,2023.
[7] G.Sharon,R.Stern,A.Felner,andN.R.Sturtevant,‚ÄúConflict-based [28] H. Tang, F. Berto, Z. Ma, C. Hua, K. Ahn, and J. Park, ‚ÄúHiMAP:
searchforoptimalmulti-agentpathfinding,‚ÄùArtificialIntelligence,vol. Learning heuristics-informed policies for large-scale multi-agent
219,pp.40‚Äì66,2015. pathfinding,‚ÄùinAAMAS,2024.
[8] M. Barer, G. Sharon, R. Stern, and A. Felner, ‚ÄúSuboptimal variants [29] E.Parisotto,F.Song,J.Rae,R.Pascanu,C.Gulcehre,S.Jayakumar,
oftheconflict-basedsearchalgorithmforthemulti-agentpathfinding M.Jaderberg,R.L.Kaufman,A.Clark,S.Nouryetal.,‚ÄúStabilizing
problem,‚ÄùinProceedingsoftheInternationalSymposiumonCombi- transformers for reinforcement learning,‚Äù in International conference
natorialSearch,vol.5,no.1,2014,pp.19‚Äì27. onmachinelearning. PMLR,2020,pp.7487‚Äì7498.
[9] J.Li,W.Ruml,andS.Koenig,‚ÄúEecbs:Abounded-suboptimalsearch [30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
formulti-agentpathfinding,‚ÄùinProceedingsoftheAAAIConference Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
onArtificialIntelligence,vol.35,no.14,2021,pp.12353‚Äì12362. Advancesinneuralinformationprocessingsystems,vol.30,2017.
[10] J.Li,A.Tinka,S.Kiesel,J.W.Durham,T.S.Kumar,andS.Koenig, [31] Z.Wang,T.Schaul,M.Hessel,H.Hasselt,M.Lanctot,andN.Freitas,
‚ÄúLifelong multi-agent path finding in large-scale warehouses,‚Äù in ‚ÄúDueling network architectures for deep reinforcement learning,‚Äù in
ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.35, International conference on machine learning. PMLR, 2016, pp.
no.13,2021,pp.11272‚Äì11281. 1995‚Äì2003.
[32] H.VanHasselt,A.Guez,andD.Silver,‚ÄúDeepreinforcementlearning
[11] G. Sartoretti, J. Kerr, Y. Shi, G. Wagner, T. S. Kumar, S. Koenig,
with double q-learning,‚Äù in Proceedings of the AAAI conference on
and H. Choset, ‚ÄúPrimal: Pathfinding via reinforcement and imitation
artificialintelligence,vol.30,no.1,2016.
multi-agentlearning,‚ÄùIEEERoboticsandAutomationLetters,vol.4,
[33] H. Hasselt, ‚ÄúDouble q-learning,‚Äù Advances in neural information
no.3,pp.2378‚Äì2385,2019.
processingsystems,vol.23,2010.
[12] M.Damani,Z.Luo,E.Wenzel,andG.Sartoretti,‚ÄúPrimal 2:Pathfind-
[34] J.-C.Latombe,Robotmotionplanning. SpringerScience&Business
ing via reinforcement and imitation multi-agent learning-lifelong,‚Äù
Media,2012,vol.124.
IEEERoboticsandAutomationLetters,vol.6,no.2,pp.2666‚Äì2673,
[35] D.Silver,‚ÄúCooperativepathfinding,‚ÄùinProceedingsoftheaaaicon-
2021.
ferenceonartificialintelligenceandinteractivedigitalentertainment,
[13] Z. Ma, Y. Luo, and J. Pan, ‚ÄúLearning selective communication for
vol.1,no.1,2005,pp.117‚Äì122.
multi-agent path finding,‚Äù IEEE Robotics and Automation Letters,
[36] K. Okumura, M. Machida, X. De¬¥fago, and Y. Tamura, ‚ÄúPriority
vol.7,no.2,pp.1455‚Äì1462,2021.
inheritance with backtracking for iterative multi-agent path finding,‚Äù
[14] Z. Ma, Y. Luo, and H. Ma, ‚ÄúDistributed heuristic multi-agent path ArtificialIntelligence,vol.310,p.103752,2022.
findingwithcommunication,‚Äùin2021IEEEInternationalConference
[37] H.Ma,D.Harabor,P.J.Stuckey,J.Li,andS.Koenig,‚ÄúSearchingwith
onRoboticsandAutomation(ICRA). IEEE,2021,pp.8699‚Äì8705. consistentprioritizationformulti-agentpathfinding,‚ÄùinProceedings
[15] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, oftheAAAIconferenceonartificialintelligence,vol.33,no.01,2019,
D.Silver,andK.Kavukcuoglu,‚ÄúAsynchronousmethodsfordeeprein- pp.7643‚Äì7650.
forcementlearning,‚ÄùinInternationalconferenceonmachinelearning. [38] T.Schaul,J.Quan,I.Antonoglou,andD.Silver,‚ÄúPrioritizedexperi-
PMLR,2016,pp.1928‚Äì1937. encereplay,‚ÄùarXivpreprintarXiv:1511.05952,2015.
[16] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, [39] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, ‚ÄúCurriculum
D.Wierstra,andM.Riedmiller,‚ÄúPlayingatariwithdeepreinforcement learning,‚ÄùinProceedingsofthe26thannualinternationalconference
learning,‚ÄùarXivpreprintarXiv:1312.5602,2013. onmachinelearning,2009,pp.41‚Äì48.
[17] Y.Wang,B.Xiang,S.Huang,andG.Sartoretti,‚ÄúSCRIMP:Scalable [40] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
communicationforreinforcement-andimitation-learning-basedmulti- ‚ÄúProximal policy optimization algorithms,‚Äù arXiv preprint
agentpathfinding,‚ÄùarXivpreprintarXiv:2303.00605,2023. arXiv:1707.06347,2017.
[18] Q. Lin and H. Ma, ‚ÄúSACHA: Soft actor-critic with heuristic-based [41] C. Ferner, G. Wagner, and H. Choset, ‚ÄúOdrm* optimal multirobot
attention for partially observable multi-agent path finding,‚Äù IEEE path planning in low dimensional search spaces,‚Äù in 2013 IEEE
RoboticsandAutomationLetters,2023. international conference on robotics and automation. IEEE, 2013,
[19] G.Gange,D.Harabor,andP.J.Stuckey,‚ÄúLazycbs:implicitconflict- pp.3854‚Äì3859.
basedsearchusinglazyclausegeneration,‚ÄùinProceedingsofthein- [42] R. J. Williams, ‚ÄúSimple statistical gradient-following algorithms for
ternationalconferenceonautomatedplanningandscheduling,vol.29, connectionist reinforcement learning,‚Äù Machine learning, vol. 8, pp.
2019,pp.155‚Äì162. 229‚Äì256,1992.
[20] J. Li, G. Gange, D. Harabor, P. J. Stuckey, H. Ma, and S. Koenig, [43] F.Berto,C.Hua,J.Park,M.Kim,H.Kim,J.Son,H.Kim,J.Kim,and
‚ÄúNewtechniquesforpairwisesymmetrybreakinginmulti-agentpath J.Park,‚ÄúRL4CO:aunifiedreinforcementlearningforcombinatorial
finding,‚ÄùinProceedingsoftheInternationalConferenceonAutomated optimization library,‚Äù in NeurIPS 2023 Workshop: New Frontiers in
PlanningandScheduling,vol.30,2020,pp.193‚Äì201. GraphLearning,2023.
[21] J.Chung,J.Fayyad,Y.A.Younes,andH.Najjaran,‚ÄúLearningtoteam- [44] A. Hottung and K. Tierney, ‚ÄúNeural large neighborhood search
basednavigation:Areviewofdeepreinforcementlearningtechniques for the capacitated vehicle routing problem,‚Äù arXiv preprint
formulti-agentpathfinding,‚ÄùarXivpreprintarXiv:2308.05893,2023. arXiv:1911.09539,2019.
[22] Z. He, L. Dong, C. Sun, and J. Wang, ‚ÄúAsynchronous multithread- [45] W.Kool,L.Bliek,D.Numeroso,Y.Zhang,T.Catshoek,K.Tierney,
ing reinforcement-learning-based path planning and tracking for un- T. Vidal, and J. Gromicho, ‚ÄúThe euro meets neurips 2022 vehicle
mannedunderwatervehicle,‚ÄùIEEETransactionsonSystems,Man,and routing competition,‚Äù in NeurIPS 2022 Competition Track. PMLR,
Cybernetics:Systems,vol.52,no.5,pp.2757‚Äì2769,2021. 2022,pp.35‚Äì49.
[46] H.Ye,J.Wang,H.Liang,Z.Cao,Y.Li,andF.Li,‚ÄúGlop:Learning
[23] D.Wang,H.Deng,andZ.Pan,‚ÄúMrcdrl:Multi-robotcoordinationwith
globalpartitionandlocalconstructionforsolvinglarge-scalerouting
deep reinforcement learning,‚Äù Neurocomputing, vol. 406, pp. 68‚Äì76,
problemsinreal-time,‚ÄùAAAI2024,2024.
2020.
[47] H. Ye, J. Wang, Z. Cao, H. Liang, and Y. Li, ‚ÄúDeepaco: Neural-
[24] L.Chen,Y.Wang,Y.Mo,Z.Miao,H.Wang,M.Feng,andS.Wang,
enhanced ant systems for combinatorial optimization,‚Äù Advances in
‚ÄúMultiagent path finding using deep reinforcement learning coupled
NeuralInformationProcessingSystems,vol.36,2024.
withhotsupervisioncontrastiveloss,‚ÄùIEEETransactionsonIndustrial
[48] F. Liu, X. Tong, M. Yuan, X. Lin, F. Luo, Z. Wang, Z. Lu, and
Electronics,vol.70,no.7,pp.7032‚Äì7040,2022.
Q.Zhang,‚ÄúAnexampleofevolutionarycomputation+largelanguage
[25] W. Li, H. Chen, B. Jin, W. Tan, H. Zha, and X. Wang, ‚ÄúMulti-
modelbeatinghuman:Designofefficientguidedlocalsearch,‚ÄùarXiv
agentpathfindingwithprioritizedcommunicationlearning,‚Äùin2022
preprintarXiv:2401.02051,2024.
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
[49] H. Ye, J. Wang, Z. Cao, and G. Song, ‚ÄúReevo: Large language
2022,pp.10695‚Äì10701.
models as hyper-heuristics with reflective evolution,‚Äù arXiv preprint
[26] L. Chen, Y. Wang, Z. Miao, Y. Mo, M. Feng, and Z. Zhou, ‚ÄúMulti- arXiv:2402.01145,2024.
agent path finding using imitation-reinforcement learning with trans-
former,‚Äù in 2022 IEEE International Conference on Robotics and
Biomimetics(ROBIO). IEEE,2022,pp.445‚Äì450.