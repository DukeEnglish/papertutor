WhisperMask: A Noise Suppressive Mask-Type Microphone for
Whisper Speech
HirotakaHiraki ShusukeKanazawa TakahiroMiura
TheUniversityofTokyo NationalInstituteofAdvanced NationalInstituteofAdvanced
Bunkyo,Tokyo,Japan IndustrialScienceandTechnology IndustrialScienceandTechnology
NationalInstituteofAdvanced Kashiwa,Chiba,Japan Kashiwa,Chiba,Japan
IndustrialScienceandTechnology kanazawa-s@aist.go.jp miura-t@aist.go.jp
Kashiwa,Chiba,Japan
hirotakahiraki@gmail.com
ManabuYoshida MasaakiMochimaru JunRekimoto
NationalInstituteofAdvanced NationalInstituteofAdvanced TheUniversityofTokyo
IndustrialScienceandTechnology IndustrialScienceandTechnology Bunkyo,Tokyo,Japan
Kashiwa,Chiba,Japan Kashiwa,Chiba,Japan SonyComputerScienceLaboratory
yoshida-manabu@aist.go.jp m-mochimaru@aist.go.jp Bunkyo,Tokyo,Japan
rekimoto@acm.org
In the subway platform(80dB) Microphone Recorded Audio Meeting using WhisperMask
‚ÄúHi, Can you hear me clearly?‚Äù
Ambient Noise
WhisperMask(ours) No interference
Ambient Noise
AirPods Pro2
Figure1:WhisperMaskisawearablemask-typemicrophonethatcapturesonlytheuser‚Äôsvoiceeveninnoisyenvironments,
suchasinsubwaystations.TheaudiofromWhisperMaskhaslowerambientnoise(uppermiddle)thanthatrecordedfrom
otherwearablemicrophones,suchasAppleAirPodsPro2.Inindoorsetting,theWhisperMaskdidnotpickupbackground
noisesduringvoicecalls.
ABSTRACT
speechrecognitionrate.Acrossallmetrics,WhisperMaskconsis-
Whisperingisacommonprivacy-preservingtechniqueinvoice- tentlyoutperformedtraditionalnoise-suppressingmicrophones
basedinteractions,butitseffectivenessislimitedinnoisyenviron- andsoftware-basedsolutions.Notably,WhisperMaskshoweda30%
ments.Inconventionalhardware-andsoftware-basednoisereduc- higherrecognitionaccuracyforwhisperedspeechrecordedinan
tionapproaches,isolatingwhisperedspeechfromambientnoiseand environmentwith80dBbackgroundnoisecomparedwiththepin
otherspeechsoundsremainsachallenge.WethusproposeWhis- microphoneandearbuds.Furthermore,whileadenoiserdecreased
perMask,amask-typemicrophonefeaturingalargediaphragm thewhisperedspeechrecognitionrateofthesetwomicrophones
withlowsensitivity,makingthewearer‚Äôsvoicesignificantlylouder byapproximately20%at30-60dBnoise,WhisperMaskmaintained
thanthebackgroundnoise.WeevaluatedWhisperMaskusingthree ahighperformanceevenwithoutdenoising,surpassingtheother
keymetrics:signal-to-noiseratio,qualityofrecordedvoices,and microphones‚Äôperformancesbyasignificantmargin.
WhisperMask‚Äôsdesignrendersthewearer‚Äôsvoiceasthedom-
AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia inantinputandeffectivelysuppressesbackgroundnoisewithout
¬©2024Copyrightheldbytheowner/author(s). relyingonsignalprocessing.Thisdeviceallowsforreliablevoice
Thisistheauthor‚Äôsversionofthework.Itispostedhereforyourpersonaluse.Not interactions,suchasphonecallsandvoicecommands,inawide
forredistribution.ThedefinitiveVersionofRecordwaspublishedinTheAugmented
HumansInternationalConference(AHs2024),April4‚Äì6,2024,Melbourne,VIC,Australia, rangeofnoisyreal-worldscenarioswhilepreservinguserprivacy.
https://doi.org/10.1145/3652920.3652925.
4202
guA
22
]CH.sc[
1v00521.8042:viXraAHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
Mel spectrogram saying ‚ÄúAny sufficiently advanced technology is indistinguishable from magic‚Äù
Background Noise Level: 30dB Background Noise Level: 80dB
Natural Speech Whispering Natural Speech Whispering
WhisperMask(Ours)
Throat Microphone
Pin Microphone
EEaarrbbuuddss MMiiccrroopphhoonnee
Figure2:Mel-spectrogramofthespeech‚ÄùAnysufficientlyadvancedtechnologyisindistinguishablefrommagic.‚ÄùDisplayedare
twospeakingstyles:naturalspeech(leftside)andwhisperedspeech(rightside)deliveredinanenvironmentwithvaryingnoise
levels(30dBand80dB)andrecordedusingdifferentmicrophones.ThetopmostMelspectrogramshowsthattheproposed
microphoneeffectivelycaptureswhispersat80dBnoiselevel.
CCSCONCEPTS
84].However,thesesolutionshavelimitationsintermsofweara-
‚Ä¢Human-centeredcomputing‚ÜíMobiledevices;Sound-based bility,contactnoise,andtheabilitytocapturewhisperedspeech
input/output. (Table1).Someproducts,suchasHashMe[31]andMutalk[32],aim
tocaptureonlythewearer‚Äôsvoiceinnoisyenvironmentsfirmly
KEYWORDS attachedaroundthemouth.However,theylimitface-to-facecon-
versationsandtheycanbebulky.Microphoneswithbuilt-inven-
microphone,noisesuppression,whispering,wearabledevices
tilationfans[19]havealsobeenproposed,buttheygeneratesig-
ACMReferenceFormat: nificantnoiseforthosenearby.Toaddresstheseissues,wepro-
HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida, poseWhisperMask,alightweightmask-typemicrophonewitha
MasaakiMochimaru,andJunRekimoto.2024.WhisperMask:ANoiseSup- large,softdiaphragmmadeofconductivefabric.Unliketheexisting
pressiveMask-TypeMicrophoneforWhisperSpeech.InTheAugmented
mask-typemicrophonesthatarefirmlyattachedaroundthemouth.
HumansInternationalConference(AHs2024),April4‚Äì6,2024,Melbourne,
WhisperMaskallowsfornaturalbreathingandspeakingwhileef-
VIC,Australia.ACM,NewYork,NY,USA,14pages.https://doi.org/10.1145/
fectivelysuppressingbackgroundnoisewithoutrelyingonsignal
3652920.3652925
processing.WeevaluateWhisperMaskusingthreemetrics,namely,
1 INTRODUCTION signal-to-noiseratio(SNR),audioquality,andspeechrecognition
rate,todemonstrateitssuperiorperformancecomparedwiththat
With the widespread use of smart devices and voice interfaces,
ofconventionalnoise-suppressivemicrophonesandsoftware-based
on-the-gocommunicationisbecomingincreasinglycommon.Mi-
solutions.Ourmaincontributionsareasfollows:
crophonesarenowintegratedintovariouswearabledevices,such
asAirPods[1],smartwatches[47,48],andsmartrings[86].How- ‚Ä¢ DevelopmentofWhisperMask,awearable,noise-suppressing
ever,usingtheseinterfacesinnoisyenvironmentsposestechnical microphonethatcaptureswhisperedspeechinenvironments
challenges,asthewearer‚Äôsvoicecanbeinterferedwithbyother withupto80dBambientnoise.
speakers‚Äôvoicesandbackgroundnoise[25].Theexistingsolutions ‚Ä¢ Acousticcharacterization,whichrevealedthatWhisperMask
toachieveaclearvoiceinputinnoisyenvironmentsincludehard- providesa10dBadvantagetothewearer‚Äôsvoiceoverany
ware advancements, such as the development of unidirectional externalnoiserangingfrom200Hzto5kHz.
microphones[4],throatmicrophones[33],andnon-audiblemurmur ‚Ä¢ SNRevaluation,whichshowedthatWhisperMaskoutper-
microphones[67],aswellassoftware-basedapproachessuchas formstheexistingmicrophonesby10dBinenvironments
blindsourceseparation(BSS)[10,64]andspeechenhancement[51, with70dBnoise.WhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Table1:Typesofmicrophonesbasedondesignprinciple.WhisperMaskiswearable,eliminatesbackgroundorcontactnoise,
andcapableofcapturingwhisperedvoiceseveninnoisyplaces.
Microphone PrincipleofNoiseReduction Nobackgroundnoise Wearable Nocontactnoise Whispering
UnidirectionalMic DirectionalVibration ‚ñ≥ ‚úì ‚úì ‚úì
ArrayMic Beamforming ‚ñ≥ ‚úì ‚úì
ThroatMic Piezoelectric ‚ñ≥ ‚úì
NamMic ContactMicrophone ‚Éù ‚úì
EarbudsMic DeepLearning ‚ñ≥ ‚úì ‚úì
WhisperMask LargeElectletDiaphragm ‚Éù ‚úì ‚úì ‚úì
‚Ä¢ Audio quality assessment, which demonstrated Whisper- reductionisachievedbyblockingthedirectionofvibrationofthe
Mask‚Äôssuperiorityovertheothermicrophonesanditscom- diaphragm,fromthebacksideandrestrictingittooneside,thereby
parabilitytonoisereductionsoftware. narrowingthedirectivityto180degrees;notethatthediaphragm
‚Ä¢ Speechrecognitionevaluation,whichshowedthatWhisper- isfoundinsidethemicrophone[4].Thisallowsforstrongrecording
Maskachieveda30%higherrecognitionratethananoise ofsoundinalimiteddirectionrelativetothemicrophone,whichis
suppressionsoftwareforbothnormalandwhisperedspeech importantwhenmultiplespeakersarespeakingatthesametime,
innoisyconditions. suchasinapaneldiscussion.However,sincethesemicrophones
donotlimitthedistanceofsound,backgroundnoisecannotbe
Inthisstudy,weexploreanewmicrophonediaphragmshape
removedbythemicrophonebyitself,makingthemunsuitablefor
andamask-typeinterfacethatcapturesonlytheuser‚Äôsvoicewith-
useinnoisyenvironments.
outsealingthemouth,enablingreliablevoiceinteractionsinnoisy
real-worldscenarioswhilepreservingusercomfortandprivacy.
2.1.2 Arraymicrophones. Arraymicrophonesareequippedwith
AsshowninFig2,theproposedWhisperMaskeffectivelycaptures
multiple microphones, and beamforming[10] is used to narrow
bothnormalspeechandwhisperedspeechinhigh-noiseenviron-
downthedirectionofarrivalofsoundbytakingadvantageofthe
ments,demonstratingitspotentialforvariousvoiceinteraction
timedifferencebetweenthearrivalofemittedsoundateachmi-
applications.
crophone.Thisallowstheselectionofthespeakerofanutterance.
2 RELATEDWORK However,sincemultiplemicrophoneswhicharearrangedeither
inacircularfashionorinastraightline,arerequiredtonarrow
Voiceinteractionisoneofthemostimportantinteractionmodalities downthedirectionofsound,usingthissysteminawearablede-
thatpeoplecanengagein,anditisusedinvariousapplicationssuch viceischallenging.Furthermore,evenifthedirectionofarrivalof
astelephoneandonlinecalls,voicecommandinput,andinteractive soundcanbeestimated,itisnotpossibletodeterminewhetherit
operationwithsmartassistants.Intheseapplications,clearspeech isbackgroundnoiseornot,requiringpost-processingtodealwith
inputisimportant;equallyimportantisastablevoiceinteraction backgroundnoise.
eveninenvironmentswhereotherpeoplearetalkingorwherea
highnoiselevelexists,suchasinsubwaysorconstructionsites. 2.1.3 Throatmicrophones. Athroatmicrophoneusesapiezoelec-
Toachieveclearspeechinput,manyapproacheshavelongbeen tricelementtoconvertthevibrationsthatappearonthesurface
proposedinthefieldofspeech,signalprocessing,andinteraction, oftheneckwhenaspeechisuttered[33].Byattachingthemicro-
bothfromhardwareandsoftwareperspectives. phoneontotheneckandacquiringonlythesurfacesound,only
One particular approach is increasing the number of micro- thewearer‚Äôsvoiceiscollected;environmentalnoisecannotcause
phones,allowingdevicestobecomemoredirectionalandselect thepiezoelectricelementtovibratesufficiently,resultinginahigh
towardauser‚Äôsvoice[14];however,suchdevicesarenotsuitableto immunitytobackgroundnoise.However,becausethedevicemust
becomewearableduetotheirlargesize.Inthissection,wepresent beworntightlyaroundtheneck,noiseisgeneratedbymovements
theexistingapproachesthatfacilitatevoiceinteractiondesignedto suchasheadshakingornodding.Moreover,becausethevoicetrav-
bewearableandoperateinreal-time. elsthroughtheskin,theformantsnecessaryforspeechrecognition
aredeficient,andthuspost-processingisrequiredtoensureaudible
2.1 Wearablenoise-suppressingmicrophones
andaccuratespeechrecognition[35,79].
Intermsofhardwaretechnology,noisesuppressionisachievedby
2.1.4 NAMmicrophones. ANAMmicrophoneworkswhenitisin
combiningtheprinciplesofsoundpickuparoundwhichvariousmi-
directcontactwiththeskinbehindtheearsimilartoapharyngeal
crophonesaredesigned,includingunidirectionalmicrophones[4],
microphone;itisanaudioinputdevicewhereinanomnidirectional
throatmicrophones[33],NAMmicrophones[67],andearphone-
microphoneisdirectlyattachedtotheskinthroughasilicon[4].
typemicrophones[9].
Similartoapharyngealmicrophone,aNAMmicrophonegreatly
2.1.1 Unidirectionalmicrophones. Unidirectionalmicrophones,such reducestheeffectofambientsound[67],butnoddingandother
asthosefoundinpinmicrophonesandheadsets,bearoneofthe soundsbecomenoise.However,unlikepharyngealmicrophones,
mostreadilyavailablenoisereductiontechnologiestoday.Noise NAMmicrophonesacquiresoundbybeingwornbehindtheear,AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
althoughbothmicrophonesrequirepost-processingsuchasspeech image[15,30,39],myoelectricity[34,80,81],capacitance[38,46],
enhancement[28,52,83]. acceleration[29,36,59],strain[43],magnetism[87],EEG[8,55],and
othermodalitiestomeasurethehumanactivitiesleadingtospeech
2.1.5 Earbudswithmicrophone. Anearpiece-typemicrophonethat
recognition.Sincethesemethodsdonotemitsound,theyaresuit-
is inserted into earphones has been proposed, to achieve clear
ableforuseinnoisyenvironments(e.g.,wheremanypeopleare
speechinput,withearphonesattachedtobothears.Earbudsare
talking)
alsousedtocollecthealthinformationbymonitoringexerciseand
However,silentspeechinputhaslexicalchallengesinthatit
biometricdata[60].Earpiecemicrophonesarebeamformersystems
isnotconversationalanditfacesinteractionchallengessuchas
involvingtheearphonesmountedonbothears,allowingselective
wearabilityandhands-freeinput.SilentSpeller[38]demonstrates
acquisitionofthewearer‚Äôsvoice,moreover,thehardwareisopen
ahighperformanceintermsofvocabularybutisnotveryexpres-
source,facilitatingitsfasterdevelopment[61].Machinelearning
sivegiventhatthecurrentvocabularyforspeechrecognitionis9
methods have also been proposed to reduce noise from speech
millionwords.Meanwhile,alargelipimagedataisavailableforlip
detectedinbothears[9].However,thesemethodsaredifficultto
imageinput,thatcanbeusedindarkenvironmentsorasscalable
implementforvoicesthatarenotproducedclearly,suchaswhis-
commandrecognition[54],butthelimitationsofthecommands
pers.
remainunclear.Moreover,usersmustfacetheirsmartphonesto
speak,makinghands-freeinputdifficult,whichispossiblewith
2.2 Softwareapproachesfornoisereduction:
voice.Myoelectricityisenvisionedforuseinactualconversations,
speechenhancementandblindsource
suchassynthesizingspeechfromsilentspeech[17].However,my-
separation oelectricityrequirestheattachmentofmyoelectricarrayelectrodes
ontothesurfaceoftheface,makingitunsuitableforprolongeduse,
In the fieldof speech signalprocessing, the use of BSS[10, 64]
anditdoesnottakeintoaccounttheeffectsofwalkingandother
andspeechenhancement[51,84]havelongbeenproposed.InBSS,
movements.
theresolutionofthespacewherevoicespropagateisincreasedby
Silent speech with breathing has also been proposed in the
increasingthenumberofmicrophones[14],andtheindependence
speechmodality[24],butitrequireslearningtheinteractionof
ofmultiplespeakersfromeachotherisusedasacriterionforsource
breathingin;moreover,itsvocabularyissmallerthanthatofthe
separation.
touchsensorsandupdatedimagesdescribedabove.Inputbynon-
InBSS,variousmethodshavelongbeenproposedsuchasincreas-
audiblemurmurhasalsobeenproposed[28,52,83],butithasthe
ingthenumberofmicrophonestoincreasetheresolutionofthe
problemthatnoiseisgeneratedbytheuser‚Äôsnaturalmovements,
spacewherevoicespropagate[14],separatingsoundsourcesbased
suchasnoddingorturningaround,aswellasthenoiseproduced
onindependence[37],anddecomposingmatricesintolowerdimen-
bytouchingtheNAMmicrophone,aretheidentifiedproblems.
sionalmatrices[11,12].Otherapproaches[40,50]combinethese
methodswithdeeplearning.Furthermore,methodsforspeaker
separationandspeechenhancementthatoperateonsmallmod- 2.4 Mask-typeinterface
els have been proposed [18, 68, 70] and they work in real-time.
TheincreaseduseofmasksduringtheCOVID-19pandemichas
Real-timesoundsourceseparationinvolvesaprocessknownas
ledtoasurgeofinterestinexploringtheirpotentialaswearable
short-timeFouriertransform(STFT),whichachievesaquickde-
interfaces.Whilemasksconcealfacialexpressionstherebyposing
compositiontime.Thisprocess,however,reducesthefrequency
achallenge,researchershaveproposedmethodstocaptureand
resolutionnecessaryforhigh-qualitysynthesis.Toaddressthis
presentexpressionsusingphotoreflectors,capacitivetouchsen-
issue,learningmethodshavebeenintroduced.Thesemethodspro-
sors,LEDs,anddisplays[26,42,45,53].Furthermore,maskshave
poseanevaluationfunctiondesignedtomaximizesoundquality,
beenadaptedaswearableinterfacesforvariousinteractionsinvolv-
therebyenhancingsynthesisquality[41].
ingthefaceandmouth,includingbreathdetection[44,74,75],eye
However,thesemachinelearning-basedmethodsdemonstrate
tracking[5],mouthshaperecognition[63,72],andmaskattach-
alimitedgeneralizationperformancebecausetheyarebasedon
ment/removaldetectionusingthestraps[82].
specificEnglishspeakerdatasetsorspecificnoisedatasets.For
Notably,maskshavebeenreportedtocausevoiceattenuation,
example,ifaspeakerisplacedinfrontofaperson‚Äôsmouthand
makingspeechperceptiondifficult[73,76].Toaddressthisissue,
theexactsamevoiceisplayed,thevoicefromthespeakerwillbe
researchershaveproposedembeddingsensorsinmaskstoenable
misinput.Also,becausethesystemisoptimizedfornormalspeech
silentspeechrecognition[29,43].However,theseapproachespri-
data,therearedeviationsfromdatainrealenvironments,suchas
marilyfocusonrecognizingspecificcommandsforcommunication
inputfromwhisperedvoicesorfromwhisperedvoicesproducedin
withsmartassistantssuchasAlexa,limitingtheirapplicabilityfor
anoisyenvironment.
broaderspeechrecognitionandconversationcapabilities.
2.3 Silentspeechinterfaceforspeech WhileproductssuchasHushMe[31]andMutalk[32]offervoice
isolationinnoisyenvironmentsbyfullysealingaroundthemouth,
communication
theyarebulkyandareintendedprimarilyforgamingapplications,
Silentspeech,whichfacilitatesinteractionwithanon-vocalinput, limitingface-to-facecommunication.Similarly,microphonedevices
such as lip image recognition as well as with speech, has been withbuilt-inventilationfans[19],althoughlightweightandsleek,
proposed[16,23].Silentspeechusesnotonlythevoiceemitted necessitateconstantfanoperationtoventilatethesealedarea,lead-
fromthevocalcords,butalsolipimage[54,69,71],ultrasound ingtoconsiderablenoisepollution.WhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Right side of WhisperMask Microphone in a non-waven mask Diaphragm of Microphone
PC
Silver Plated
Polyester:200Œºm 3.3V Bm63
Signal
USB
GND
Bluetooth
Electlet Film:12.5Œºm
SmartPhone
Figure3:OverviewofWhisperMaskasamaskedmicrophone.WhisperMaskisamask-typemicrophonethatallowsfor
hands-free,non-obtrusiveinput(right).Themicrophoneissandwichedbetweenthefabricoftwonon-wovenmasks(center).
ThediaphragmisconnectedtoamicrocontrollerandcanbeusedonaPCorsmartphoneviaUSBorBluetooth(right).
Ourproposedsolution,WhisperMask,addressestheneedfor appliedaroundthefilm,anda200ùúámsilver-platedpolyestercloth
voiceisolationwiththeuseofasinglelightweightmask,thusof- wasfixedontobothsidesofthetape.Thisconfigurationforms
feringapracticalalternative.Importantly,thehands-freeandnon- the structure ofan electret condenserwhereinthe electret was
obtrusivenatureofWhisperMaskmakesitparticularlywell-suited sandwichedbetweentwoelectrodes.
forenvironmentswheremaskusageremainsprevalent,suchasop- ThediaphragmwasconnectedtoanFET,whichoutputsthe
eratingrooms[66],cleanrooms,orothernoisysettingswhereclear audiosignal.Comparedwithcommerciallyavailablecondenser
communicationiscrucial.Byeffectivelysuppressingbackground microphones,theproposedmicrophonehasalowervoltagevalue,
noisewhilepreservingusercomfortandprivacy,WhisperMask whichisamplifiedbyafactorof3usingFETs.Furthermore,the
enablesreliablevoiceinteractionsinthesechallengingscenarios. inputsignalisconnectedviaBluetoothtoaBm63audiosystem-
on-a-chip(SoC),whichishousedina6mmx5mmhousingand
3 WHISPERMASK,ANELECRETCONDENSER weighslessthan10g.
MICROPHONE(ECM)-BASEDMASK-TYPE Afterproducingthediaphragmandthemeasuringcircuit,we
MICROPHONE embeddedtheminabreathablemaskcreatedfromamesh-typefab-
ric.Themaskwasdesignedtobearslitstoallowforthepositioning
./ andfixationofthediaphragm.
Weproposeawearablemask-typemicrophonecalledWhisper-
Mask,whichallowsforinputeveninnoisyenvironments.Whisper-
3.3 Patternsofthediaphragm
Maskisdesignedbasedontheprincipleoftheelectretcondenser
microphone (ECM) and incorporates a vibrating diaphragm de- WhisperMaskisamask-typemicrophone,whichisusedbywearing
signedusingconductivefabricandfilm. amask.However,ifthesensorbecomesheavy,themaskwillshift
andfalloff,resultingindecreasedrecognition.Therefore,inthis
3.1 Principle:ElectretCondenserMicrophone study,wepreparedthreesizesofmicrophones(20mm√ó4mm,20
mm√ó2mm,and10mm√ó2mm,Fig.5),andweembeddedthemina
WhisperMaskisamicrophonedesignedbasedontheECMtech-
maskforassessment(Fig.3).
nology.AnECMconsistsofapowersupplysectionandavibrat-
ingdiaphragm,wherethediaphragmformsacapacitorwiththe
4 FREQUENCYCHARACTERISTICOF
electrode,givingrisetoamicrophone.Whenasoundcausesthe
diaphragmtovibrate,thevoltageofthecapacitorchanges,resulting WHISPERMASK
inaweakvoltagethatisconvertedintosoundthroughafieldeffect
Frequencycharacteristicisoneofthemostcriticalaspectsofami-
transistor(FET)andthroughanalog-to-digitalconversion.Inthis
crophone,asitdeterminesthemicrophone‚Äôsabilitytocapturespe-
study,weproposeanoveldesignofavibratingdiaphragmforECM,
cificfrequenciesandtoaccentuateorattenuatecertainfrequency
enablingselectivitytowardthespeaker‚Äôsvoice.
bands.Forexample,invoicerecognition,whichiscrucialinvoice-
basedinteractions,humanspeechpredominantlyfallswithinthe
3.2 DesignofWhisperMask
rangeof5kHz,andamicrophonethatdoesnotcapturethisfre-
Thediaphragmofourmicrophoneconsistsofaconductivefabric quencyrangewouldmakevoicerecognitionextremelychallenging.
electrodeandadielectricplasticfilm,forminganelectretcondenser. Additionally,itisimportanttoconsiderhowclearlyasignalstands
The plastic film used is PFA (perfluoroalkoxy polymer), with a outagainstnoise,whichismeasuredintermsofSNR.AlowSNR
thicknessof12.5ùúám.Forwindprotection,anadhesivetapewas valuemeansthatasignificantpartoftheobtainedsignalisnoise,AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
SweptSine signal for measuring Impulse Response Hardware Setup
Dummy head
with voice simulator
Speaker
WhisperMask
500mm
Throat Pin Microphone
Microphone
Proceedure of measuring Impulse Response
Speaker or
Wearable Impulse
SweptSine Voice simulator
Microphone Response
in DummyHead
Convolution
(s)
SweptSine
Detection
Inverse Filter
Figure4:Swept-sinewaveformeasuringimpulseresponse.FiveSwept-sineoflength65536aregenerated(left).Adummyhead
withavoicesimulatorisprovidedandthemicrophonesarewornformeasurement.Thespeakerisplaced500mmawayfrom
thedummyhead(upperright).Theimpulseresponseiscalculatedbyconvolvingthesignalobtainedbypreprocessingtodetect
Swept-sinewiththeinversefilterofSwept-sine.(lowerright)
makingitdifficulttocapturethedesiredsignal,suchasthespeaker‚Äôs multiplemeasurementsiscrucial.Therefore,aswept-sineconsist-
voice. ingof65535sampleswascreatedandtheprocesswasrepeatedfive
timestogenerateasinglewavfile(Fig.4left).However,toidentify
4.1 Impulseresponsewithswept-sine
thestartingpositionsofeachswept-sine,abufferofthesamelength
astheswept-sinewasinsertedbetweenconsecutiveswept-sines.
4.1.1 Characteristicoftheswept-sinesignal. Frequencycharacter-
Byincludingthesegaps,theinputconsistedoffiverepetitionsof
isticscanbemeasuredusingthemicrophone‚Äôsimpulseresponse,
theswept-sine,whichwasthenrepeated10times,resultingina
whichinturncanbemeasuredusingvariousmethods,suchas
totalof50measurementsoftheimpulseresponse.Thisaveraging
usinganimpulsewaveorwhitenoise.However,thewidelyem-
processwascarriedouttoassessthesystem‚Äôsnoiserobustness.
ployedmethodinvolvesswept-sine,acontinuoussinewavethat
Finally, we use 1/3 octaveband averaging [20] to smoothen the
sweepsfromlowtohighfrequencies[21].Swept-sinemeasurement
impulseresponse.
providesinformationacrossawidefrequencyrangeusingasingle
signal.Itisrobustagainstnoiseandrelativelyeasytomeasure.
4.2 Experimentalcondition
Furthermore,thedeterministicpropertiesofasweptsinesignal
makeitlessnoisycomparedwithmeasurementmethodsinvolving 4.2.1 Environmentalsetupformeasuringimpulseresponse. Forthe
whitenoise.Bytakingmeasurementsrepeatedly,randomnoisecan impulseresponsemeasurement,adummyhead(SAMAR4700M)
bereduced,enablingtheacquisitionofaccurateandreproducible equippedwithamouthsimulatorwasusedtomimichumanspeech
values. The waveform obtained from swept-sine represents the output.TheSAMAR4700complieswiththeinternationalstandards
convolutionofthesystem‚Äôsimpulseresponseandtheswept-sine IEC60318-7andITU-TRec.P51forheadshapeandmouthsimulator,
itself.Toobtainthemicrophone‚Äôsimpulseresponsefromtheac- enablingthesimulationofhumanspeechformeasurements.The
quiredsignal,aninverseconvolutionfiltercanbeappliedduring dummyheadwaspositionedonatripod40mmabovethefloor,
thegenerationofaswept-sinesignal(Fig.4,right). andaspeakerthatreplicatestheemittedsoundwasfixed50mmin
frontofthedummyhead.
4.1.2 Designoftheswept-sinewaveformeasurement. Aswept-sine
Theacousticmeasurementswereconductedinanelectromag-
iscapableofresolvingdifferentfrequenciesbasedonthelength
neticanechoicchambertominimizeinterferencefromelectromag-
of a sample, with longer samples providing higher a frequency
neticwavesandcertainsoundwaves.Theroomwastreatedwith
resolution.Inthisstudy,weusedasamplelengthof65536pointsto
porousmaterialstoabsorbsound,resultinginaroomnoiseof28.8
achieveafrequencyresolutionoflessthan1Hzwhenthesampling
dB.
frequencywassetat44.1kHz.Thefrequencyrangeoftheswept-
sinewaslimitedtoupto22.1kHz,whichcorrespondstotheupper 4.2.2 Inputsignal. Duringthemeasurements,adummyheadsim-
limitofhumanauditoryperception.Asmentionedearlier,repeating ulatinghumanspeechandaspeakerimitatingexternalnoisewere
theswept-sinemeasurementimprovestheSNR;thus,conducting setup.Toavoiddistortionoftheoutputsound,80dBswept-sineWhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Figure5:SizesoftheWhisperMaskdiaphragm: Figure 6: Impulseresponse of WhisperMask(10mm√ó2
20mm√ó4mm(upper),20mm√ó2mm(center),and10mm√ó2 mm)
mm(lower)
Figure 7: Impulseresponse of WhisperMask(20 mm√ó2 Figure 8: Impulse response of WhisperMask(20 mm√ó4
mm) mm)
wasoutputasahumanvoicefromamouth-simulatingspeaker.Two thesepoints,alinearregressionwasperformed,withthepointat
swept-sines(80and60dB)wereoutputasbackgroundnoisefrom frequency0representingthestarttimeandthepointatfrequency
theexternalnoisespeaker(Fig.4rightupper).Thesesoundlevels 22000representingtheendtime.
weremeasuredwithaprecisionnoisesystem,andtheobtained
valuesfellwithinanerrorof0.1dB.
4.3 Swept-sinedetection
The output of the swept-sine is susceptible to variations in fre-
4.4 Resultforimpulseresponse
quencycharacteristicsdependingonthepropertiesofthereceiving
microphone.Inotherwords,itisimpossibletoacquirethefullrange TheimpulseresponseresultsareshowninFig.6,7,8.Theblue
offrequencybands,andinsomecases,onlyapartialrepresentation linerepresentstheimpulseresponseoftheassumedhumanspeech,
isobtained.However,toaccuratelymeasuretheimpulseresponse, playedfromthemouth-simulatingspeakerofthedummyhead,with
determiningthetimingatwhichtheswept-sinesignalbeginsiscru- 80dBswept-sine.Theorangeandgreenlinesarebothassumedto
cial.Toestimatethestartandendtimesoftheswept-sinefromthe benoiseandwereoutputat80dBand60dB,respectively,fromthe
informationobtainedwithincertainfrequencybands,thefollowing externalspeaker.
approachwasemployed.Firstly,theswept-sinewasdecomposed Ineachofthethreepatterns,especiallyinthefrequencyband
intoindividualfrequenciesat1kHzintervals,andtheenvelopewas between200Hzand5kHz,theoutputofthedummyheadwas
acquired.Astheswept-sinewasoutputfivetimeswithaninterval approximately10dBhigherthantheoutsidenoise,indicatingthat
ofonepulse,fiverisingedgesappearedinthefrequencyregions when the same waveform at the same sound pressure is input
wheretheswept-sinewaswellrepresented.Bycapturingthese, inside(dummyhead)andoutside(noisesimulatingspeaker)the
fivepointswererecorded.Dividingthefrequenciesatintervalsof microphone,themicrophonecapturestheinsidesoundinmore
1kHz,amaximumof22points(22kHz/1000)wasrecorded.Using easily,thatis,itreducesnoise.AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
Figure9:SNRresultfordifferentmicrophonesat60dBinput
5 EVALUATION:NOISESUPPRESSING
90dB,withanupperlimitof90dBbasedonthecharacteristicsof
MICROPHONE thespeaker‚Äôsoutput.
5.1 Evaluatingtheeffectofnoisesuppression: Threedeviceswereusedformeasurements:WhisperMask(10
mmx4mm),pinmicrophone(PinMic),andthroatmicrophone(ThroatMic).
SNRmeasurement
Theearbudswithmicrophone(Airpods)wereoptimizedforhuman
Theproposedmicrophonecancaptureaspeaker‚Äôsvoiceinnoisy voicesandcouldnotinputnoise-likewaveformssimilartothoseof
environments.Todemonstratethis,theSNRwasmeasured.The theswept-sine.Eachmicrophonewasmountedonadummyhead
SNRwascomputedbyrecordingtheoutput(N)ofthemicrophone inanidealpositiononthedummyhead.
withoutsupplyinginputsignalsfromthespeakerandbyrecording
5.2.1 SNRresultsfornoisyenvironment. Theinputsinthenoisy
theoutput(S)wheninputsignalsweregenerated.Therootmean
environmentwereasfollows:theSNRforPinMicandThroatMic
square(RMS)valueswerethencalculatedforeachmicrophone
decreasedastheambientnoiseincreased,whereasthatforWhisper-
output,andtheSNRwasdeterminedusing20ùëôùëúùëî10(ùëÜ ùëÖùëÄùëÜ/ùëÅ ùëÖùëÄùëÜ)
Maskhardlychangedfrom30dB(SNR:22.2)to60dB(SNR:21.3)of
[7].SNRevaluationiscommonlyusedtoassesstheperformance
ambientnoise.SNRwascalculatedastheratioofthepowerofthe
ofmicrophonesinnoisyenvironmentsandisalsoutilizedinarray
signaltothepowerofthenoise:ùëÜùëÅùëÖ=20ùëôùëúùëî10(ùëÜ ùëÖùëÄùëÜ/ùëÅ ùëÖùëÄùëÜ).And
microphones[6].
sincetheinputsignal(ùëÜ ùëÖùëÄùëÜ)isalmostconstantat60dBswept-sine,
from30dBto60dB.WhisperMaskpickedupalmostnoambient
noisebecauseùëÅ ùëÖùëÄùëÜ isnearlyconstant.
5.2 EnvironmentalsetupforSNRmeasuring TheimpactofnoiseonWhisperMask‚Äôsperformancewasgreater
whentheexternalnoisewashigherthan70dB;at70dB,theSNR
ForSNRmeasurement,adummyhead(SAMAR4700M)equipped
forWhisperMaskwas17.83,whichwas10dBhigherthantheSNR
withamouthsimulatorwasusedtomimichumanspeechoutput.
valuesforPinMic(2.3)andforThroatMic(5.83).
TheSAMAR4700complieswiththeinternationalstandardsIEC
60318-7andITU-TRec.P51forheadshapeandmouthsimulator, 6 EVALUATION:QUALITYOFTHERECORDED
enablingthesimulationofhumanspeechformeasurements.The VOICES
dummyheadwaspositionedonatripod40cmabovethefloor,and
aspeakerthatreplicatestheemittedsoundwasfixed50cminfront Widelyusedforvoiceinputandcalls,microphonesrequirenot
ofthedummyhead. onlynoiseresiliencebutalsohigh-qualitysoundduringrecordings.
Acousticmeasurementswereconductedinanelectromagnetic Weevaluatedthesoundqualityinnoisyconditionstocompare
anechoicchambertominimizeinterferencefromelectromagnetic WhisperMasknotjustagainstexistingmicrophonesbutalsoagainst
wavesandcertainsoundwaves.Theroomwastreatedwithporous theperformanceofconventionalmicrophonesfollowingtheuseof
materialstoabsorbsound,resultinginaroomnoiseof28.8dB. anoisereductionsoftware.
Theinputsignalusedforthemeasurementwasa20Hz‚Äì20kHz
6.1 Recordconditions
swept-sinetoalsoperformcalculationsinthefullrange.Thesound
pressureofthesignaloutputfromthedummyheadwas60dB, Theaudiorecordingsusedinthisstudywereoffivephrasesex-
whichisclosetothatofhumanspeech.Environmentalnoisewas tractedfromtheTextEntrydataset[49],andreadaloudbyfour
outputfromanexternalspeakerin10dBincrementsfrom30dBto proficientEnglishspeakers(onemaleandthreefemales).TheseWhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Figure 10: The WebUI used to evaluate the quality of
the recorded audio clips; the metrics were based on
MUSHRA[2].Theparticipantsratedeachaudioclipona
scaleof0to100.Thereferenceaudioclip(toprow)had Figure 11: Assessment results for the quality of audio
ahighqualityandwasusedasacriterionforselecting recordedbyWhisperMaskandotherdevicesinanenvi-
responses.Fourofthefivetestitemsarerecordingscap- ronmentwith80dBbackgroundnoise.‚ÄúPinmic,‚Äù‚Äúpods,‚Äù
turedinanoisyenvironmentbyeachdevice.Onetest and‚Äúthroat‚Äùinthex-axisrepresentunidirectionalpin
itemisthesameasthereference,anyparticipantswho microphones,AirpodsPro2,andthroatmicrophones,re-
ratedthistestitemashavinglowerqualitythantheref- spectively.Thereferenceisaclearsoundrecordedina
erenceaudiowillbejudgedaslessfaithfulrespondents. quietenvironment(30dB)withapinmicrophone.
speakersconsentedconsentedtoparticipateinthisstudyandbe- theevaluationofsubtledifferences.Multipletestitemswerepro-
comesubjectsfordatacollection,forwhichtheyreceived$20.The videdtotheevaluatorsamongtheseitemsisan"anchor"wherein
noiselevelduringtherecordingwas80dB,andthefourmicro- the sound quality was intentionally reduced to provide a refer-
phonesusedforrecordingwereWhisperMask,unidirectionalpin enceforthequalityevaluation.Thisapproachensuresconsistent
microphone,earbudswithmicrophone(AirpodsPro2),andthroat evaluations[2].
microphone.Thereferenceaudioisaclearsoundrecordedina MUSHRAevaluationshavebeenusedinawiderangeofappli-
quietenvironment(30dB)withapinmicrophone. cationsinspeechprocessing,includingnoisereduction[3],text-to-
Tocomparewithsoftware-basednoisereduction,wecreated speechsynthesis[85],andvoicetransformation[58].Itwasalso
recordings by applying noise removal software to the captured beenusedforevaluationsbytheparticipantsofanonlineexperi-
audioclips.Therearetwomainapproachestonoiseremovalin ment[65].
noisyenvironments:noiseremovalandspeechenhancement.These
6.3 Experimentalproceedure
approachesarenotalwaysexplicitlycomparedinspeechresearch.
Therefore,forcomparison,wechooseonemethodfromeachap-
Werecruited25nativeEnglishspeakers(13malesand12females)
proachthatutilizesalearning-basedembeddingmodelforcompar-
aged18andabovethroughMechanicalTurk.Theaudioquality
ison.Forspeechenhancement,weusedadenoiser[18].Denoiser
wasevaluatedusingWebUI,asshowninFig.12.
isanextensionofU-Net[62]andistrainedonthenoisyspeech
IntheWebUI,theparticipantswouldrateeachaudioclipona
dataset[57,77].Forspeechseparation,weuseawaveformer[78].A
scaleof0to100;ahigh-qualityaudioclipwasprovidedasarefer-
waveformerisamodelthatextendsCNNtohandlesequentialdata
ence.Oneofthefivechoiceswasthesameasthereference,enabling
andistrainedtosynthesizeandseparatesoundsfromaselected
theidentificationoflessfaithfulrespondents.Theparticipantsmay
speechdataset[22]andanenvironmentalsounddataset[27].
listentothesoundclipsasmanytimesastheywished.Theaverage
completionoftheparticipantsintheexperimentwas49minutes.
Theywerecompensated$7fortheirparticipation.
6.2 MUSHRA:ametricsforevaluatingaudio
quality 6.4 Resultoftheaudioqualityevaluation
Forsubjectiveevaluationofsoundquality,theMultipleStimuli TheresultsareshowninFig.11.‚ÄúPinmic,‚Äù‚Äúpods,‚Äù‚Äúthroat‚Äùrepresent
withHiddenReferenceandAnchor(MUSHRA)hasbeenproposed. unidirectionalpinmicrophone,AirpodsPro2,andthroatmicro-
ComparedwithMeanOpinionScore(MOS),MUSHRAisdefinedin phone,respectively.Thet-testresultsshowthatWhisperMaskwas
ITU-RBS.1534andisevaluatedonascaleof0to100,allowingfor superiortoPinmic,pods,andthroatatp=5.06E-22,2.95E-10,andAHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
Figure 12: Audio quality of the recording captured by Figure 13: Audio quality of the recording captured by
WhisperMaskfollowingpost-processingwithdenoiser WhisperMaskfollowingpost-processingwithsepformer
[18],aspeechenhancementsoftware. [70],aspeechseparationsoftware.
1.50E-28(p<0.05),respectively,andwithcorrespondingeffectsizes ratetheirEnglishproficiencyona5-pointscale,with3beingthe
of1.20,0.75,1.43[13]. mean.Duringdatacollection,eachparticipantread20pre-prepared
phrasescollectedfromtheMackenzieandSoukorefdataset[49].
6.5 Comparisonofsoundqualitywithand
In the experimental environment, white noise was varied at
withoutnoisereduction 40, 60, and 80 dB (denoted as w40, w60, and w80 dB in Fig. 14,
respectively).a30dBindicatesthenoiselevelinthespacewhenno
For the experiment, we used MUSURA[2] as a metrics, and we
whitenoisewasbeingplayed.Duringtheexperiment,twomethods
recruited 25 native English speakers (13 males and 12 females)
ofdeliverywereused:naturalspeechandwhisperedspeech.The
aged18andabovethroughMechanicalTurk.Theresultsareas
threemicrophonesevaluatedwereWhisperMask,pinmicrophone,
showninFig.11.Asmentionedabove,"pinmic","pods",and"throat"
andearbudswithmicrophone(AirPodsPro2).
representunidirectionalpinmicrophones,AirpodsPro2,andthroat
Thebackgroundnoisewasplayedfromalaptopandwasoutput
microphones,respectively.Thet-testresultinFig.12showthat
inastereo;itwasmeasuredusinganoisesystemtoensurethat
WhisperMaskwassuperiortopinmic,pods,andthroat,atp=1.31E-
thedesiredsoundpressurewasreachingtheuser‚Äôsmouthandthen
4,4.51-08,and1.83E-22(p<0.05),respectivelywithcorresponding
adjustedtoadifferenceof0.5dBorless.Measurementsweretaken
effectsizesof0.455,0.659,and1.257[13].
inasoundproofroomwherethenormalnoiselevelwas30dB.In
Bycontrast,whensepformerwasused(Fig.13),pinmicrophone
eachmicrophoneandnoiseenvironment,twotypesofspeechwere
wassuperiortoWhisperMaskatp=0.02int-test(p<0.05)and0.27in
used:naturalspeechandwhisperedspeech
effectsize.Theperformanceoftheotherdevices(AirPodsandthroat
microphone)didnotstatisticallydifferfromthatofWhisperMask 7.2 Analysis
(p=0.78,0.45int-test).
Furthermore,thequalityofrecordingcapturedbyWhisperMask Inspeechresearch,speechenhancementmethodshavebeenpro-
withandwithoutdenoiserorsepformerdidnotsignificantlydiffer posedtoreducebackgroundnoise[51,84].Inrecentyears,real-time
(p=0.575>0.05),anditwasbetterthanthesoundqualityobtained noisereductionsystemshavebecomehighlyaccurate[51,84],and
usingsepformer(p=8.74E-07<0.05;effectsize0.583 itisalreadypossibletoapplynoisereductioninmicrophones,such
aspinmicrophonesandAirPods,toobtainthedesiredaudioqual-
7 EVALUATION:SPEECHRECOGNITION ity.Thisstudyexamineshowmuchspeechrecognitionaccuracy
ACCURACY canbeimprovedrelativetotheaudioqualityobtainedafterapply-
ingreal-timedenoisingtospeechrecordedwithapinmicrophone
Speechinputiswidelyusednotonlyintelephonybutalsoforin-
orAirPods.Inanoise-freeenvironment(30dB),theparticipants‚Äô
teractivetasks,suchasoperatingsmartassistantsandinteractive
averagerecognitionratefornaturalspeechrecordedusingpinmi-
searchingusingspeechrecognition.Inthisstudy,weperformed
crophonesandAirPodswasover90%.Thevoicerecognitionrate
speechrecognitionusingtwonoise-robustspeechrecognitionmeth-
forthroatmicrophonewassignificantlylowerat64%compared
ods.Whisper[56]employsanencoder‚Äìdecodertransformermodel
withthatfortheotherdevices.Thisdiscrepancywasattributedto
trainedthroughsupervisedlearning.
improperfittingforsomeparticipants,leadingtotheirexclusion
fromthisconsideration.Speechrecognitionwasperformedusing
7.1 DataCollection
Whisperlarge[56],atransformer-basedspeechrecognizerwitha
Datawerecollectedfrom9participants(meanage26.2years;4 stronglanguagemodel.Speechrecognitionwasevaluatedbased
malesand5females).Theparticipantswereaskedbeforehandto onthepercentageofcorrectanswerspercharacterWhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Natural Speech Whispering
Noise Patterns Noise Patterns
Figure14:Resultsforspeechrecognitioninenvironmentswithdifferentnoiselevels.a30dBindicatesarecordingenvironment
withoutwhitenoise,andw40dB,w60dB,andw80dBindicatearecordingenvironmentwithawhitenoiseoutputof40,60,and
80dB,respectively.Innaturalspeech(left),recognitionaccuracyimprovedinthefollowingdecreasingorder:WhisperMask
followedbypinmicrophoneandthenbyearbuds.Adifferenceofapproximately30%wasobservedforwhisperedspeechinan
environmentwith80dBbackgroundnoise(right).NoisereductionbyDenoiser[18]didnotresultinconsiderablechangesin
therecognitionaccuracyfornaturalspeechbutitsignificantlyreducedthatforwhisperedspeech.
7.3 Result
approximately60‚Äì80dB,butwhenasoundsourceisclosertoa
TheresultsareshowninFig.14;theresultsfornormalandwhis- microphone,loudervoicesarepickedup.Infact,whenmeasuredat
peredspeechareshownontheleftandrightsides,respectively.For adistanceofabout3cmusingasoundlevelmeter,anormalvoice
naturalspeechrecordedwith80dBbackgroundnoise,therecog- becomeslouderbyapproximately80‚Äì90dB.Oneofthekeyfactors
nitionaccuracyforWhisperMaskwasover20%higherthanthat contributingtonoisereductionisthatvoicesarecapturedmore
forpinmicrophoneandearbuds.Notably,underthesamenoise loudlyatacloseproximity.
condition,therecognitionrateforWhisperMaskwithoutdenoiser
8.2 Issuesfordailyuse
washigherbyover20%thanthatforthemicrophoneswithde-
noiser.Therecognitionaccuracyforwhisperedspeechrecorded 8.2.1 Reusability. Microphonesaredevicesintendedfordailyuse
inanenvironmentwith80dBbackgroundnoisewashigherfor andmustbedesignedfordurability.Inthisstudy,weaskednine
WhisperMaskbyover30%thanthatforpinmicrophoneandear- userstousetheproposeddevice;theywereinstructedtoputonand
buds.Afterdenoiserapplication,therecognitionrateforwhispered takeofftheirmasksforeachinputsession.Weobservednoperfor-
speechdecreasedbyapproximately20%inthe30,40,and60dB manceissuesrelatedtomaskusage.Moreover,theusersworethe
environments.Thisisasignificantdecreasecomparedwiththat deviceoverawovenmask,meaningthedevicecanoperatewithout
observedfornaturalspeech,indicatingthatthedenoiserisnot directcontactwiththemouth.Consequently,theproposeddevice
well-suitedforwhisperedspeech.Moreover,therecognitionac- wasassociatedwithfewerconcernsinrelationtocontamination
curacyforwhisperedspeechrecordedinanenvironmentwith80 andhygienecomparedwithdevicesthatareattacheddirectlytothe
dBbackgroundnoisewashigherbyover30%forWhisperMask face.Furthermore,thevibratingcomponentofthemicrophonecan
withoutdenoisercomparedwiththatforpinmicrophoneandear- bedetachedfromthecircuitryandmaybeembeddedinaprotective
buds,suggestingthesuperiorityofWhisperMaskforrecognizing material,makingitwashablewithoutcausinganyissues.
whisperedspeechinnoisyenvironments.
8.2.2 Noisewhenwalking. Wedidnotevaluatetheimpactofwalk-
8 DISCUSSION ingandothermovementsontheperformanceofWhisperMask.
8.1 Mechanismofnoisereduction Motionartifactsmaypossiblyintroducenoiseandaffectthede-
vice‚Äôsperformance.Furtherinvestigationisneededtoassessthe
Thedifficultyofvibrationmaybeoneofthereasonsbehindthe impact of user movements and develop strategies to avoid any
enhancedabilityofWhisperMasktoinputspeechinnoisyenviron- associatednoise.
ments.AsshowninFigs.6‚Äì8,despitethe80dBsoundcomingfrom
thedummyheadspeakerandthenoisecomingfromthespeaker, 8.2.3 Blowingwind. iventhatvoiceentersthemicrophonethrough
themaximuminputonthemicrophonesidewasapproximately airtransmission,strongwindsorturbulentairflowbetweenthe
40dB,indicatingthataloudsoundinputisrequired.Thisphe- mouthandthemicrophonecandisruptvoicerecording.Thisissue
nomenonislikelybecausenormalhumanspeechisproducedat alsoaffectstheperformanceofordinarypinmicrophones,andaAHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
windshieldmaybenecessary.Althoughwehavenotevaluatedthe [6] ReuvenBerkunandIsraelCohen.2015.Microphonearraypowerratioforquality
effectsofstrongwindsinthisstudy,wehaveconfirmedthatsound assessmentofreverberatedspeech. EURASIPJournalonAdvancesinSignal
Processing2015,1(2015),49. https://doi.org/10.1186/s13634-015-0233-y
canbecollectedevenwhenthesensorisplacedinsideanon-woven
[7] ColinBreithaupt,TimoGerkmann,andRainerMartin.2008.AnovelaprioriSNR
fabricmask,whichcouldprovidesomeprotectionagainstwind. estimationapproachbasedonselectivecepstro-temporalsmoothing.In2008IEEE
Inthefuture,weintendtofurtherevaluatetheimpactofuser InternationalConferenceonAcoustics,SpeechandSignalProcessing.4897‚Äì4900.
https://doi.org/10.1109/ICASSP.2008.4518755
movements,suchaswalking,andenvironmentalfactors,suchas [8] JonathanS.Brumberg,AlfonsoNieto-Castanon,PhilipR.Kennedy,andFrankH.
wind,ontheperformanceofWhisperMask. Guenther.2010.Brain‚Äìcomputerinterfacesforspeechcommunication.Speech
Communication52,4(2010),367‚Äì379. https://doi.org/10.1016/j.specom.2010.01.
001SilentSpeechInterfaces.
9 CONCLUSION
[9] IshanChatterjee,MaruchiKim,VivekJayaram,ShyamnathGollakota,IraKemel-
macher,ShwetakPatel,andStevenM.Seitz.2022.ClearBuds:WirelessBinaural
WeproposeWhisperMask,amask-typeelectretcondensermicro- EarbudsforLearning-BasedSpeechEnhancement.InProceedingsofthe20th
phonethatcanclearlycaptureauser‚Äôsvoiceeveninnoisyenvi- AnnualInternationalConferenceonMobileSystems,ApplicationsandServices
(Portland,Oregon)(MobiSys‚Äô22).AssociationforComputingMachinery,New
ronmentscomparedwithconventionalmicrophones.Wedemon-
York,NY,USA,384‚Äì396. https://doi.org/10.1145/3498361.3538933
stratedWhisperMask‚Äôsacousticcharacteristicsbymeasuringits [10] SeungjinChoi,AndrzejCichocki,Hyung-MinPark,andSoo-YoungLee.2005.
impulseresponseusingswept-sinesignals.Furthermore,weeval- Blindsourceseparationandindependentcomponentanalysis:Areview.Neural
InformationProcessing-LettersandReviews6,1(2005),1‚Äì57.
uatedWhisperMaskbasedonthreekeymetrics:SNR,qualityof
[11] AndrzejCichockiandAnh-HuyPhan.2009.Fastlocalalgorithmsforlargescale
recordedvoices,andspeechrecognitionrate.Acrossthesemetrics, nonnegativematrixandtensorfactorizations.IEICEtransactionsonfundamentals
WhisperMasksignificantlyoutperformedtheconventionalnoisere- ofelectronics,communicationsandcomputersciences92,3(2009),708‚Äì721.
[12] AndrzejCichocki,RafalZdunek,andShun-ichiAmari.2006.Newalgorithms
ductionmethods,whichinvolveeitherhardware-orsoftware-based fornon-negativematrixfactorizationinapplicationstoblindsourceseparation.
approaches. In2006IEEEInternationalConferenceonAcousticsSpeechandSignalProcessing
Proceedings,Vol.5.IEEE,V‚ÄìV.
Therecognitionrateforwhisperedspeechrecordedinanenvi-
[13] J.Cohen.1962.Thestatisticalpowerofabnormal-socialpsychologicalresearch:
ronmentwith80dBbackgroundnoisewasnotablyhigherbyover Areview.TheJournalofAbnormalandSocialPsychology65,3(1962),145‚Äì153.
30%forWhisperMaskthanthatforpinmicrophonesandearbuds. https://doi.org/10.1037/h0045186
[14] RyanMCoreyandAndrewCSinger.2018. Speechseparationusingpartially
Moreover,whileadenoisersoftwaredecreasedbyapproximately
asynchronousmicrophonearrayswithoutresampling.In201816thInternational
20%theothermicrophones‚Äôrecognitionrateforwhisperedspeech WorkshoponAcousticSignalEnhancement(IWAENC).IEEE,1‚Äì9.
recordedwith30‚Äì60dBbackgroundnoise,WhisperMaskmain- [15] Tam√°sCsap√≥,Tam√°sGr√≥sz,G√°borGosztolya,L√°szl√≥T√≥th,andAlexandraMark√≥.
2017.DNN-BasedUltrasound-to-SpeechConversionforaSilentSpeechInterface.
tainedahighperformanceevenwithoutdenoising,surpassingthe Ininterspeech2017.3672‚Äì3676. https://doi.org/10.21437/Interspeech.2017-939
performanceoftheothermicrophonesbyalargemargin.These [16] B.Denby,T.Schultz,K.Honda,T.Hueber,J.M.Gilbert,andJ.S.Brumberg.2010.
Silentspeechinterfaces.SpeechCommunication52,4(2010),270‚Äì287. https:
resultshighlightWhisperMask‚Äôsoverwhelmingsuperiorityincap-
//doi.org/10.1016/j.specom.2009.08.002SilentSpeechInterfaces.
turingwhisperedspeechundernoisyconditions. [17] LorenzDiener,MehrdadRoustayVishkasougheh,andTanjaSchultz.2020.CSL-
Inconclusion,WhisperMaskrepresentsasignificantadvance- EMG_Array:AnOpenAccessCorpusforEMG-to-SpeechConversion.InINTER-
SPEECH2020‚Äì21stAnnualConferenceoftheInternationalSpeechCommunication
mentinthewearablemicrophonetechnology.Byeffectivelyad-
Association.
dressingthechallengeofcapturingclearvoiceinput,especially [18] AlexandreD√©fossez,GabrielSynnaeve,andYossiAdi.2020.RealTimeSpeech
whisperedspeech,inhigh-noiseenvironments,WhisperMaskopens EnhancementintheWaveformDomain.InProc.Interspeech2020.3291‚Äì3295.
https://doi.org/10.21437/Interspeech.2020-2409
new possibilities for enhanced communication and interaction [19] LGElectronics.2024.LGPuriCare‚Ñ¢WearableAirPurifier(w/VoiceON‚Ñ¢)|LG
acrossawiderangeofvoice-basedapplicationswhilepreserving Philippines.https://www.lg.com/ph/air-care/lg-ap551awfa. Accessed:2024-02-
10.
userprivacy.Itslightweightmask-typeformfactorandexceptional
[20] KElenius.1980.Longtimeaveragespectrumusinga1/3octavefilterbank.
noisesuppressioncapabilitiesmakeitapromisingtoolforvarious [21] angelofarina.2000. simultaneousmeasurementofimpulseresponseanddis-
real-worldscenariosrequiringreliablevoiceinputs. tortionwithaswept-sinetechnique. journaloftheaudioengineeringsociety
(february2000).
[22] EduardoFonseca,ManojPlakal,FredericFont,DanielP.W.Ellis,XavierFavory,
ACKNOWLEDGMENTS
JordiPons,andXavierSerra.2018.General-purposeTaggingofFreesoundAudio
withAudioSetLabels:TaskDescription,Dataset,andBaseline.InProceedings
ThisworkwassupportedbyJSTACT-XGrantJPMJAX23KG,JST oftheDetectionandClassificationofAcousticScenesandEvents2018Workshop
MoonshotR&DGrantJPMJMS2012,JSTCRESTGrantJPMJCR17A3, (DCASE2018).69‚Äì73. https://arxiv.org/abs/1807.09902
[23] Jo√£oFreitas,Ant√≥nioTeixeira,MiguelDias,andSamuelSilva.2016.AnIntroduc-
andthecommissionedresearchbyNICTJapanGrantJPJ012368C02901.
tiontoSilentSpeechInterfaces.
[24] MasaakiFukumoto.2018.SilentVoice:UnnoticeableVoiceInputbyIngressive
REFERENCES Speech.InProceedingsofthe31stAnnualACMSymposiumonUserInterfaceSoft-
wareandTechnology(Berlin,Germany)(UIST‚Äô18).AssociationforComputingMa-
[1] [n.d.].https://appleinsider.com/articles/21/03/30/apple-airpods-beats-dominated- chinery,NewYork,NY,USA,237‚Äì246. https://doi.org/10.1145/3242587.3242603
audio-wearable-market-in-2020. [25] YifanGong.1995.Speechrecognitioninnoisyenvironments:Asurvey.Speech
[2] B.Series.RecommendationITU-RBS.1534-3.2014. methodforthesubjec- Communication16,3(1995),261‚Äì291. https://doi.org/10.1016/0167-6393(94)
tiveassessmentofintermediatequalitylevelofaudiosystems.InInternational 00059-J
TelecommunicationUnionRadioCommunicationAssembly. [26] ZengrongGuoandRong-HaoLiang.2023.TexonMask:FacialExpressionRecog-
[3] M.Aubreville,K.Ehrensperger,A.Maier,T.Rosenkranz,B.Graf,andH.Puder. nitionUsingTextileElectrodesonCommodityFacemasks.InProceedingsofthe
2018.DeepDenoisingforHearingAidApplications.In201816thInternational 2023CHIConferenceonHumanFactorsinComputingSystems(Hamburg,Ger-
WorkshoponAcousticSignalEnhancement(IWAENC).361‚Äì365. https://doi.org/ many)(CHI‚Äô23).AssociationforComputingMachinery,NewYork,NY,USA,
10.1109/IWAENC.2018.8521369 Article627,15pages. https://doi.org/10.1145/3544548.3581295
[4] BenjaminBBauer.1962.Acenturyofmicrophones.ProceedingsoftheIRE50,5 [27] ToniHeittola,AnnamariaMesaros,andTuomasVirtanen.2019.TAUUrbanAcous-
(1962),719‚Äì729. ticScenes2019,Developmentdataset. https://doi.org/10.5281/zenodo.2589280
[5] ChristopherBeach,NazmulKarim,andAlexanderJ.Casson.2019.AGraphene- [28] TatsuyaHirahara,MakotoOtani,ShotaShimizu,TomokiToda,KeigoNakamura,
BasedSleepMaskforComfortableWearableEyeTracking.In201941stAnnual YoshitakaNakajima,andKiyohiroShikano.2010.Silent-speechenhancement
InternationalConferenceoftheIEEEEngineeringinMedicineandBiologySociety usingbody-conductedvocal-tractresonancesignals.SpeechCommunication52,
(EMBC).6693‚Äì6696. https://doi.org/10.1109/EMBC.2019.8857198 4(2010),301‚Äì313. https://doi.org/10.1016/j.specom.2009.12.001SilentSpeechWhisperMask:ANoiseSuppressiveMask-TypeMicrophoneforWhisperSpeech AHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia
Interfaces. Acoustics,SpeechandSignalProcessing(ICASSP).8333‚Äì8337. https://doi.org/10.
[29] HirotakaHirakiandJunRekimoto.2021.SilentMask:Mask-TypeSilentSpeech 1109/ICASSP39728.2021.9414881
InterfacewithMeasurementofMouthMovement.InAugmentedHumansConfer- [48] DaniyalLiaqat,RobertWu,AndreaGershon,HishamAlshaer,FrankRudzicz,
ence2021(Rovaniemi,Finland)(AHs‚Äô21).AssociationforComputingMachinery, andEyaldeLara.2018. ChallengeswithReal-WorldSmartwatchBasedAu-
NewYork,NY,USA,86‚Äì90. https://doi.org/10.1145/3458709.3458985 dioMonitoring.InProceedingsofthe4thACMWorkshoponWearableSystems
[30] T.Hueber,G.Aversano,G.Chollet,B.Denby,G.Dreyfus,Y.Oussar,P.Roussel, andApplications(Munich,Germany)(WearSys‚Äô18).AssociationforComputing
andM.Stone.2007. EigentongueFeatureExtractionforanUltrasound-Based Machinery,NewYork,NY,USA,54‚Äì59. https://doi.org/10.1145/3211960.3211977
SilentSpeechInterface.In2007IEEEInternationalConferenceonAcoustics,Speech [49] I.ScottMacKenzieandR.WilliamSoukoreff.2003.PhraseSetsforEvaluatingText
andSignalProcessing-ICASSP‚Äô07,Vol.1.I‚Äì1245‚ÄìI‚Äì1248. https://doi.org/10.1109/ EntryTechniques.InCHI‚Äô03ExtendedAbstractsonHumanFactorsinComputing
ICASSP.2007.366140 Systems(Ft.Lauderdale,Florida,USA)(CHIEA‚Äô03).AssociationforComputing
[31] HushmeInc.2021. Hushme-TheWorld‚ÄôsFirstVoiceMaskforSmartphones. Machinery,NewYork,NY,USA,754‚Äì755. https://doi.org/10.1145/765891.765971
https://gethushme.com/. Accessed:2024-02-10. [50] NaokiMakishima,ShinichiMogami,NorihiroTakamune,DaichiKitamura,Hay-
[32] ShiftallInc.2023.mutalk-Leakagevoicesuppressionmicrophone.https://en. atoSumino,ShinnosukeTakamichi,HiroshiSaruwatari,andNobutakaOno.
shiftall.net/products/mutalk. Accessed:2024-02-10. 2019.Independentdeeplylearnedmatrixanalysisfordeterminedaudiosource
[33] Robert Ingalls. 1987. Throat microphone. The Journal of the separation.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing
Acoustical Society of America 81, 3 (03 1987), 809‚Äì809. https: 27,10(2019),1601‚Äì1615.
//doi.org/10.1121/1.394659 arXiv:https://pubs.aip.org/asa/jasa/article- [51] DanielMichelsanti,Zheng-HuaTan,Shi-XiongZhang,YongXu,MengYu,Dong
pdf/81/3/809/12095011/809_1_online.pdf Yu,andJesperJensen.2021.Anoverviewofdeep-learning-basedaudio-visual
[34] ArnavKapur,ShreyasKapur,andPattieMaes.2018.AlterEgo:APersonalized speechenhancementandseparation.IEEE/ACMTransactionsonAudio,Speech,
WearableSilentSpeechInterface.In23rdInternationalConferenceonIntelligent andLanguageProcessing29(2021),1368‚Äì1396.
UserInterfaces(Tokyo,Japan)(IUI‚Äô18).AssociationforComputingMachinery, [52] Y.Nakajima,H.Kashioka,K.Shikano,andN.Campbell.2003. Non-audible
NewYork,NY,USA,43‚Äì53. https://doi.org/10.1145/3172944.3172977 murmurrecognitioninputinterfaceusingstethoscopicmicrophoneattachedto
[35] JunkiKawaguchiandMitsuharuMatsumoto.2022.NoiseReductionCombining theskin.In2003IEEEInternationalConferenceonAcoustics,Speech,andSignal
aGeneralMicrophoneandaThroatMicrophone.Sensors22,12(2022). https: Processing,2003.Proceedings.(ICASSP‚Äô03).,Vol.5.V‚Äì708. https://doi.org/10.1109/
//doi.org/10.3390/s2212s4473 ICASSP.2003.1200069
[36] PrernaKhanna,TanmaySrivastava,ShijiaPan,ShubhamJain,andPhucNguyen. [53] HyeYeonNam,IyleahHernandez,andBrendanHarmon.2020. Unmasked.
2021. JawSense:RecognizingUnvoicedSoundUsingaLow-CostEar-Worn InAdjunctPublicationofthe33rdAnnualACMSymposiumonUserInterface
System.InProceedingsofthe22ndInternationalWorkshoponMobileComputing SoftwareandTechnology(VirtualEvent,USA)(UIST‚Äô20Adjunct).Associationfor
SystemsandApplications(Virtual,UnitedKingdom)(HotMobile‚Äô21).Association ComputingMachinery,NewYork,NY,USA,111‚Äì113. https://doi.org/10.1145/
forComputingMachinery,NewYork,NY,USA,44‚Äì49. https://doi.org/10.1145/ 3379350.3416137
3446382.3448363 [54] LaxmiPandeyandAhmedSabbirArif.2021. LipType:ASilentSpeechRecog-
[37] TaesuKim,Torbj√∏rnEltoft,andTe-WonLee.2006.Independentvectoranalysis: nizerAugmentedwithanIndependentRepairModel.AssociationforComputing
AnextensionofICAtomultivariatecomponents.InInternationalconferenceon Machinery,NewYork,NY,USA. https://doi.org/10.1145/3411764.3445565
independentcomponentanalysisandsignalseparation.Springer,165‚Äì172. [55] AnnePorbadnigk,MarekWester,Jan-PCalliess,andTanjaSchultz.2009.EEG-
[38] NaokiKimura,TanGemicioglu,JonathanWomack,RichardLi,YuhuiZhao, basedSpeechRecognition-ImpactofTemporalEffects.376‚Äì381.
AbdelkareemBedri,AlexOlwal,JunRekimoto,andThadStarner.2021.Mobile, [56] AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,
Hands-Free,SilentSpeechTextingUsingSilentSpeller.AssociationforComputing andIlyaSutskever.2022. RobustSpeechRecognitionviaLarge-ScaleWeak
Machinery,NewYork,NY,USA. https://doi.org/10.1145/3411763.3451552 Supervision. arXiv:2212.04356[eess.AS]
[39] NaokiKimura,MichinariKono,andJunRekimoto.2019.SottoVoce:AnUltra- [57] ChandanReddy,EbrahimBeyrami,HarishchandraDubey,VishakGopal,Roger
soundImaging-BasedSilentSpeechInteractionUsingDeepNeuralNetworks.In Cheng,RossCutler,SergiyMatusevych,RobertAichner,AshkanAazami,Sebas-
Proceedingsofthe2019CHIConferenceonHumanFactorsinComputingSystems tianBraun,PuneetRana,SriramSrinivasan,andJohannesGehrke.2020. The
(Glasgow,ScotlandUk)(CHI‚Äô19).AssociationforComputingMachinery,New INTERSPEECH2020DeepNoiseSuppressionChallenge:Datasets,Subjective
York,NY,USA,1‚Äì11. https://doi.org/10.1145/3290605.3300376 SpeechQualityandTestingFramework.InInterspeech2020.
[40] DaichiKitamura,NobutakaOno,HiroshiSawada,HirokazuKameoka,andHi- [58] JunRekimoto.2023.WESPER:Zero-ShotandRealtimeWhispertoNormalVoice
roshiSaruwatari.2015.Efficientmultichannelnonnegativematrixfactorization ConversionforWhisper-BasedSpeechInteractions.InProceedingsofthe2023
exploitingrank-1spatialmodel.In2015IEEEInternationalConferenceonAcoustics, CHIConferenceonHumanFactorsinComputingSystems(Hamburg,Germany)
SpeechandSignalProcessing(ICASSP).IEEE,276‚Äì280. (CHI‚Äô23).AssociationforComputingMachinery,NewYork,NY,USA,Article
[41] YumaKoizumi,KentaNiwa,YusukeHioka,KazunoriKobayashi,andYoichi 700,12pages. https://doi.org/10.1145/3544548.3580706
Haneda.2018. DNN-BasedSourceEnhancementtoIncreaseObjectiveSound [59] JunRekimotoandYuNishimura.2021. Derma:SilentSpeechInteractionUs-
QualityAssessmentScore.IEEE/ACMTransactionsonAudio,Speech,andLanguage ingTranscutaneousMotionSensing.InAugmentedHumansConference2021
Processing26,10(2018),1780‚Äì1792. https://doi.org/10.1109/TASLP.2018.2842156 (Rovaniemi,Finland)(AHs‚Äô21).AssociationforComputingMachinery,NewYork,
[42] RyogaKumazakiandAkifumiInoue.2020. DevelopmentandEvaluationofa NY,USA,91‚Äì100. https://doi.org/10.1145/3458709.3458941
Mask-TypeDisplayTransformingtheWearer‚ÄôsImpression.InProceedingsof31st [60] TobiasR√∂ddiger,ChristopherClarke,PaulaBreitling,TimSchneegans,Haibin
AustralianConferenceonHuman-Computer-Interaction(Fremantle,WA,Australia) Zhao,HansGellersen,andMichaelBeigl.2022.SensingwithEarables:ASys-
(OzCHI‚Äô19).AssociationforComputingMachinery,NewYork,NY,USA,568‚Äì571. tematicLiteratureReviewandTaxonomyofPhenomena.6,3,Article135(sep
https://doi.org/10.1145/3369457.3369533 2022),57pages. https://doi.org/10.1145/3550314
[43] Yusuke Kunimi, Masa Ogata, Hirotaka Hiraki, Motoshi Itagaki, Shusuke [61] TobiasR√∂ddiger,TobiasKing,DylanRayRoodt,ChristopherClarke,andMichael
Kanazawa,andMasaakiMochimaru.2022. E-MASK:AMask-ShapedInter- Beigl.2022.Openearable:Openhardwareearablesensingplatform.InAdjunct
faceforSilentSpeechInteractionwithFlexibleStrainSensors.InAugmented Proceedingsofthe2022ACMInternationalJointConferenceonPervasiveand
Humans2022(Kashiwa,Chiba,Japan)(AHs2022).AssociationforComputing UbiquitousComputingandthe2022ACMInternationalSymposiumonWearable
Machinery,NewYork,NY,USA,26‚Äì34. https://doi.org/10.1145/3519391.3519399 Computers.246‚Äì251.
[44] TakahiroKusabukaandTakuyaIndo.2020.IBUKI:GestureInputMethodBased [62] OlafRonneberger,PhilippFischer,andThomasBrox.2015.U-Net:Convolutional
onBreathing.InAdjunctPublicationofthe33rdAnnualACMSymposiumon NetworksforBiomedicalImageSegmentation.InMedicalImageComputingand
UserInterfaceSoftwareandTechnology(VirtualEvent,USA)(UIST‚Äô20Adjunct). Computer-AssistedIntervention‚ÄìMICCAI2015,NassirNavab,JoachimHorneg-
AssociationforComputingMachinery,NewYork,NY,USA,102‚Äì104. https: ger,WilliamM.Wells,andAlejandroF.Frangi(Eds.).SpringerInternational
//doi.org/10.1145/3379350.3416134 Publishing,Cham,234‚Äì241.
[45] HyeinLee,YoonjiKim,andAndreaBianchi.2020. MAScreen:Augmenting [63] MoseSakashita,KeisukeKawahara,AmyKoike,KentaSuzuki,IppeiSuzuki,
SpeechwithVisualCuesofLipMotions,FacialExpressions,andTextUsinga andYoichiOchiai.2016.Yadori:Mask-TypeUserInterfaceforManipulationof
WearableDisplay.InSIGGRAPHAsia2020EmergingTechnologies(VirtualEvent, Puppets.InACMSIGGRAPH2016EmergingTechnologies(Anaheim,California)
RepublicofKorea)(SA‚Äô20).AssociationforComputingMachinery,NewYork, (SIGGRAPH‚Äô16).AssociationforComputingMachinery,NewYork,NY,USA,
NY,USA,Article2,2pages. https://doi.org/10.1145/3415255.3422886 Article23,1pages. https://doi.org/10.1145/2929464.2929478
[46] RichardLi,JasonWu,andThadStarner.2019.TongueBoard:AnOralInterfacefor [64] HiroshiSawada,NobutakaOno,HirokazuKameoka,DaichiKitamura,andHi-
SubtleInput.InProceedingsofthe10thAugmentedHumanInternationalConference roshiSaruwatari.2019.Areviewofblindsourceseparationmethods:twoconverg-
2019(Reims,France)(AH2019).AssociationforComputingMachinery,NewYork, ingroutestoILRMAoriginatingfromICAandNMF.APSIPATransactionsonSig-
NY,USA,Article1,9pages. https://doi.org/10.1145/3311823.3311831 nalandInformationProcessing8(2019),e12. https://doi.org/10.1017/ATSIP.2019.5
[47] DaniyalLiaqat,SalaarLiaqat,JunLinChen,TinaSedaghat,MosheGabel,Frank [65] MichaelSchoeffler,SarahBartoschek,Fabian-RobertSt√∂ter,MarleneRoess,Su-
Rudzicz,andEyaldeLara.2021. Coughwatch:Real-WorldCoughDetection sanneWestphal,BerndEdler,andJ√ºrgenHerre.2018.webMUSHRA‚ÄîACom-
usingSmartwatches.InICASSP2021-2021IEEEInternationalConferenceon prehensiveFrameworkforWeb-basedListeningTests.JournalofOpenResearchAHs2024,April4‚Äì6,2024,Melbourne,VIC,Australia HirotakaHiraki,ShusukeKanazawa,TakahiroMiura,ManabuYoshida,MasaakiMochimaru,andJunRekimoto
Software(Feb2018). https://doi.org/10.5334/jors.187 onNeuralText-To-SpeechSynthesis.InICASSP2023-2023IEEEInternational
[66] Antonia Schulte, Rodrigo Suarez-Ibarrola, Daniel Wegen, Philippe-Fabian ConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).1‚Äì5. https:
Pohlmann,ElinaPetersen,andArkadiuszMiernik.2020. Automaticspeech //doi.org/10.1109/ICASSP49357.2023.10097248
recognitionintheoperatingroom-Anessentialcontemporarytooloraredun- [86] ChengZhang,AnandghanWaghmare,PranavKundra,YimingPu,ScottGilliland,
dantgadget?Asurveyevaluationamongphysiciansinformofaqualitativestudy. ThomasPloetz,ThadE.Starner,OmerT.Inan,andGregoryD.Abowd.2017.
AnnMedSurg(Lond)59(2020),81‚Äì85. https://doi.org/10.1016/j.amsu.2020.09.015 FingerSound:RecognizingUnistrokeThumbGesturesUsingaRing.Proc.ACM
[67] ShotaShimizu,MakotoOtani,andTatsuyaHirahara.2009.Frequencycharac- Interact.Mob.WearableUbiquitousTechnol.1,3,Article120(sep2017),19pages.
teristicsofseveralnon-audiblemurmur(NAM)microphones.AcousticalScience https://doi.org/10.1145/3130985
andTechnology30,2(2009),139‚Äì142. https://doi.org/10.1250/ast.30.139 [87] YongzhaoZhang,Wei-HsiangHuang,Chih-YunYang,Wen-PingWang,Yi-Chao
[68] DanielStoller,SebastianEwert,andSimonDixon.2018.Wave-u-net:Amulti- Chen,Chuang-WenYou,Da-YuanHuang,GuangtaoXue,andJiadiYu.2020.
scaleneuralnetworkforend-to-endaudiosourceseparation. arXivpreprint Endophasia:UtilizingAcoustic-BasedImagingforIssuingContact-FreeSilent
arXiv:1806.03185(2018). SpeechCommands.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.4,1,
[69] ZixiongSu,ShitaoFang,andJunRekimoto.2023. LipLearner:Customizable Article37(March2020),26pages. https://doi.org/10.1145/3381008
SilentSpeechInteractionsonMobileDevices.InProceedingsofthe2023CHI
ConferenceonHumanFactorsinComputingSystems.1‚Äì21.
[70] CemSubakan,MircoRavanelli,SamueleCornell,MirkoBronzi,andJianyuan
Zhong.2021.AttentionIsAllYouNeedInSpeechSeparation.InICASSP2021
-2021IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP).21‚Äì25. https://doi.org/10.1109/ICASSP39728.2021.9413901
[71] KeSun,ChunYu,WeinanShi,LanLiu,andYuanchunShi.2018.Lip-Interact:Im-
provingMobileDeviceInteractionwithSilentSpeechCommands.InProceedings
ofthe31stAnnualACMSymposiumonUserInterfaceSoftwareandTechnology
(Berlin,Germany)(UIST‚Äô18).AssociationforComputingMachinery,NewYork,
NY,USA,581‚Äì593. https://doi.org/10.1145/3242587.3242599
[72] YutaroSuzuki,KodaiSekimori,YukiYamato,YusukeYamasaki,BuntarouShizuki,
andShinTakahashi.2020. AMouthGestureInterfaceFeaturingaMutual-
CapacitanceSensorEmbeddedinaSurgicalMask.InHuman-ComputerInterac-
tion.MultimodalandNaturalInteraction,MasaakiKurosu(Ed.).SpringerInterna-
tionalPublishing,Cham,154‚Äì165.
[73] LindaMThibodeau,RachelBThibodeau-Nielsen,ChiMaiQuynhTran,and
ReginaTangerinodeSouzaJacob.2021.CommunicatingduringCOVID-19:The
effectoftransparentmasksforspeechrecognitioninnoise.EarandHearing42,
4(2021),772‚Äì781.
[74] VishalVarunTipparaju,DiWang,JingjingYu,FangChen,FrancisTsow,Erica
Forzani,NongjianTao,andXiaojunXian.2020.Respirationpatternrecognition
bywearablemaskdevice.BiosensorsandBioelectronics169(2020),112590. https:
//doi.org/10.1016/j.bios.2020.112590
[75] VishalVarunTipparaju,XiaojunXian,DevonBridgeman,DiWang,Francis
Tsow,EricaForzani,andNongjianTao.2020.ReliableBreathingTrackingWith
WearableMaskDevice. IEEESensorsJournal20,10(2020),5510‚Äì5518. https:
//doi.org/10.1109/JSEN.2020.2969635
[76] JosephCToscanoandCheyenneMToscano.2021. Effectsoffacemaskson
speechrecognitioninmulti-talkerbabblenoise.PloSone16,2(2021),e0246842.
[77] CassiaValentini-Botinhao.2017. Noisyspeechdatabasefortrainingspeech
enhancementalgorithmsandTTSmodels.InUniversityofEdinburgh.Schoolof
Informatics.CentreforSpeechTechnologyResearch(CSTR). https://doi.org/10.
7488/ds/2117.
[78] BandhavVeluri,JustinChan,MalekItani,TuochaoChen,TakuyaYoshioka,and
ShyamnathGollakota.2023.Real-TimeTargetSoundExtraction.InICASSP2023
-2023IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP).1‚Äì5. https://doi.org/10.1109/ICASSP49357.2023.10094573
[79] AmrithaVijayan,BipilMaryMathai,KarthikValsalan,RiyankaRajiJohnson,
LaniRachelMathew,andK.Gopakumar.2017.Throatmicrophonespeechrecog-
nitionusingmfcc.In2017InternationalConferenceonNetworks&Advancesin
ComputationalTechnologies(NetACT).392‚Äì395. https://doi.org/10.1109/NETACT.
2017.8076802
[80] MichaelWand.,ChristopherSchulte.,MatthiasJanke.,andTanjaSchultz.2013.
Array-basedElectromyographicSilentSpeechInterface.InProceedingsofthe
InternationalConferenceonBio-inspiredSystemsandSignalProcessing-BIOSIG-
NALS,(BIOSTEC2013).INSTICC,SciTePress,89‚Äì96. https://doi.org/10.5220/
0004252400890096
[81] MichaelWandandTanjaSchultz.2011.Session-independentEMG-basedSpeech
Recognition.,InProceedingsofBiosignals2011.BIOSIGNALS2011-Proceedings
oftheInternationalConferenceonBio-InspiredSystemsandSignalProcessing,
295‚Äì300.
[82] TakumiYamamoto,KatsutoshiMasai,AnushaWithana,andYutaSugiura.2023.
Masktrap:DesigningandIdentifyingGesturestoTransformMaskStrapintoan
InputInterface.InProceedingsofthe28thInternationalConferenceonIntelligent
UserInterfaces(Sydney,NSW,Australia)(IUI‚Äô23).AssociationforComputingMa-
chinery,NewYork,NY,USA,762‚Äì775. https://doi.org/10.1145/3581641.3584062
[83] NAKAJIMAYoshitaka,KASHIOKAHideki,CAMPBELLNick,andSHIKANO
Kiyohiro.2005.Non-AudibleMurmur(NAM)Recognition.IEICETRANSACTIONS
onInformationandSystemsE89-D,1(2005).
[84] Asri Rizki Yuliani, M Faizal Amri, Endang Suryawati, Ade Ramdan, and
HilmanFerdinandusPardede.2021.Speechenhancementusingdeeplearning
methods:Areview.JurnalElektronikadanTelekomunikasi21,1(2021),19‚Äì26.
[85] FrankZalkow,PrachiGovalkar,MeinardM√ºller,Emanu√´lA.P.Habets,and
ChristianDittmar.2023.EvaluatingSpeech‚ÄìPhonemeAlignmentanditsImpact