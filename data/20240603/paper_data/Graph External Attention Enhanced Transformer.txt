Graph External Attention Enhanced Transformer
JianqingLiang1 MinChen1 JiyeLiang1
Abstract focusesonGraphNeuralNetworks(GNNs). Amilestone
exampleisGCN(Defferrardetal.,2016;Kipf&Welling,
TheTransformerarchitecturehasrecentlygained
2017). It performs convolution operations on the graph.
considerable attention in the field of graph rep-
Basedontheframeworkofmessage-passingGNNs(Gilmer
resentation learning, as it naturally overcomes
et al., 2017), GraphSage (Hamilton et al., 2017), Gat-
several limitations of Graph Neural Networks
edGCN (Bresson & Laurent, 2017) and GIN (Xu et al.,
(GNNs)withcustomizedattentionmechanisms
2019)adapttocomplexgraphdatabyemployingdifferent
or positional and structural encodings. Despite
message-passingstrategies. Whilemessage-passingGNNs
making some progress, existing works tend to
haverecentlyemergedasprominentmethodsforgraphrep-
overlookexternalinformationofgraphs,specifi-
resentation learning, there still exist some critical limita-
callythecorrelationbetweengraphs. Intuitively,
tions,includingthelimitedexpressiveness(Xuetal.,2019;
graphswithsimilarstructuresshouldhavesimilar
Morrisetal.,2019),over-smoothing(Lietal.,2018;Chen
representations. Therefore,weproposeGraphEx-
etal.,2020;Oono&Suzuki,2020),over-squashing(Alon
ternalAttention(GEA)â€”anovelattentionmech-
&Yahav,2021)andpoorlong-rangedependencies.
anismthatleveragesmultipleexternalnode/edge
key-value units to capture inter-graph correla- Instead of aggregating local neighborhood, graph Trans-
tions implicitly. On this basis, we design an formercapturesinteractioninformationbetweenanypair
effective architecture called Graph External At- ofnodesthroughasingleself-attentionlayer. Someofthe
tentionEnhancedTransformer(GEAET),which existingworksfocusoncustomizingspecificattentionmech-
integrates local structure and global interaction anismsorpositionalencodings(Dwivedi&Bresson,2020;
informationformorecomprehensivegraphrepre- Yingetal.,2021;Kreuzeretal.,2021;Hussainetal.,2022;
sentations. Extensiveexperimentsonbenchmark Ma et al., 2023), while others combine message-passing
datasetsdemonstratethatGEAETachievesstate- GNNstodesignhybridarchitectures(Wuetal.,2021;Chen
of-the-art empirical performance. The source etal.,2022;RampaÂ´sË‡eketal.,2022). Thesemethodsenable
codeisavailableforreproducibilityat: https: nodestointeractwithallothernodeswithinagraph,facili-
//github.com/icm1018/GEAET. tatingthedirectmodelingoflong-rangerelations. Thismay
addresstypicalissuessuchasover-smoothinginGNNs.
Whiletheabove-mentionedmethodshaveachievedimpres-
1.Introduction sive results, their focus tends to be confined to internal
informationwithinthegraph,specifically,differentnodes
Graphrepresentationlearninghasattractedwidespreadat-
orstructuresinthegraph,neglectingpotentialcorrelations
tention in the past few years. It plays a crucial role in
withothergraphs. Indeed,innumerouspracticalscenarios,
variousapplications,suchassocialnetworkanalysis(Pal
suchasmoleculargraphdata,strongcorrelationsfrequently
etal.,2020),drugdiscovery(Gaudeletetal.,2021b),protein
existbetweendifferentgraphs(molecules). Thisisexempli-
design(Ingrahametal.,2019),medicaldiagnosis(Lietal.,
fiedbythethreemoleculargraphsinFigure1,allofwhich
2020b)andsoon.
possessabenzeneringstructure. Undersuchcircumstances,
Early research in graph representation learning primarily integratinginter-graphcorrelationsintothemodelcanim-
provetheeffectivenessofgraphrepresentationlearning.
1Key Laboratory of Computational Intelligence and Chi-
nese Information Processing of Ministry of Education, School In this work, we address the critical question of how to
of Computer and Information Technology, Shanxi University, incorporateexternalinformationintographrepresentation
Taiyuan030006,Shanxi,China. Correspondenceto: JiyeLiang
learning. Ourprincipalcontributionistointroduceanovel
<ljy@sxu.edu.cn>.
GraphExternalAttention(GEA)mechanism,whichimplic-
Proceedings of the 41st International Conference on Machine itly learns inter-graph correlations with the external key-
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by valueunits. Moreover,wedesignGraphExternalAttention
theauthor(s).
1
4202
yaM
13
]GL.sc[
1v16012.5042:viXraGraphExternalAttentionEnhancedTransformer
Despiteachievingstate-of-the-artperformance,GNNsface
O over-smoothingandover-squashingduetotheirconstrained
O
N receptive field. Over-smoothing happens when all node
NH
representations converge to a constant after deep layers,
O
O
whereasover-squashingoccurswhenmessagesfromdistant
nodesfailtopropagateeffectively.Itiscrucialtodesignnew
architecturesbeyondneighborhoodaggregationtoaddress
HO
OH OH theseissues.
O O
O
Graph Transformers. The Transformer with self-
attention, a dominant approach in natural language pro-
Figure1.ThreemoleculargraphsfromtheZINCdatasetarecorre-
cessing (Vaswani et al., 2017; Devlin et al., 2018), has
latedtothebenzeneringstructure.
shown competitiveness in computer vision (Dosovitskiy
etal.,2021). GiventheremarkableachievementsofTrans-
formersandtheircapabilitytoaddresscrucialchallenges
Enhanced Transformer (GEAET), combining inter-graph
of GNNs, graph Transformer has been proposed, attract-
correlationswithbothlocalstructureandglobalinteraction
ingincreasingattention. Existingworksprimarilyfocuson
information. Thisenablestheacquisitionofmorecompre-
designingtailoredattentionmechanismsorpositionaland
hensivegraphrepresentations,incontrasttomostexisting
structuralencodings,orcombiningmessage-passingGNNs,
methods. Ourcontributionsarelistedasfollows.
enablingmodelstocapturecomplexstructures.
â€¢ WeintroduceGEAtoimplicitlylearncorrelationsbe- Anumberofworksembedtopologyinformationintograph
tweenallgraphs. Thecomplexityscaleslinearlywith nodes by designing tailored attention mechanisms or po-
thenumberofnodesandedges. sitionalandstructuralencodingswithoutmessage-passing
GNNs. GT(Dwivedi&Bresson,2020)introducesthepi-
â€¢ WeproposeGEAET,whichusesGEAtolearnexternal oneering example of a graph Transformer by integrating
informationandintegrateslocalstructureandglobalin- Laplacian positional encoding. In the following years, a
teractioninformation,resultinginmorecomprehensive seriesofworksspringup. SAN(Kreuzeretal.,2021)in-
graphrepresentations. corporates both sparse and global attention mechanisms
ineachlayer,utilizingLaplacianpositionalencodingsfor
â€¢ WedemonstratethatGEAETachievesstate-of-the-art
the nodes. Graphormer (Ying et al., 2021) attains state-
performance. Furthermore, we highlight the signifi-
of-the-artperformanceingraph-levelpredictiontaskswith
canceofGEA,emphasizingitssuperiorinterpretabil-
centrality encoding, spatial encoding and edge encoding.
ity and reduced dependence on positional encoding
EGT (Hussain et al., 2022) introduces the edge channel
comparedtoself-attention.
intoattentionmechanismsandadoptsanSVD-basedposi-
tional encoding instead of Laplacian positional encoding.
Whileself-attentioniscommonlyconstrainedbyquadratic
2.RelatedWork
complexity,ourproposedGEAdemonstratesalinearcom-
putationalcomplexitywithrespecttoboththenumberof
Message-Passing Graph Neural Networks. Early de-
nodesandedges.
velopments include GCN (Defferrard et al., 2016; Kipf
& Welling, 2017), GraphSage (Hamilton et al., 2017), Furthermore, some works introduce hybrid architectures
GIN(Xuetal.,2019),GAT(VelicË‡kovicÂ´ etal.,2018),Gat- incorporatingmessage-passingGNNs. Forinstance,Graph-
edGCN(Bresson&Laurent,2017)andothers. Thesemeth- Trans(Wuetal.,2021)utilizesastackofGNNlayersbe-
ods are based on a message-passing architecture (Gilmer foreestablishingfullconnectivityattentionwithinthegraph.
et al., 2017) that generally faces the challenges of lim- Focusingonkernelmethods,SAT(Chenetal.,2022)intro-
ited expressivity. Recent advancements include various ducesastructure-awareattentionmechanismusingGNNsto
worksattemptingtoenhanceGNNstoimproveexpressivity. extractasubgraphrepresentationrootedateachnodebefore
Examplesincludesomeworksthataddfeaturestodistin- computingtheattention. Arecentbreakthroughemerged
guish nodes (Murphy et al., 2019; Sato et al., 2021; Qiu withtheintroductionofGraphGPS(RampaÂ´sË‡eketal.,2022),
etal.,2018;Bouritsasetal.,2023;Dwivedietal.,2022a). thefirstparallelframeworkthatcombineslocalmessage-
Othersfocusonalteringthemessage-passingrule(Beaini passingandaglobalattentionmechanismwithvariousposi-
etal.,2021)ormodifyingtheunderlyinggraphstructurefor tionalandstructuralencodings. Whilethesemethodshave
message-passing(Morrisetal.,2019;Bodnaretal.,2021) achievedcompetitiveperformance,theyoverlookexternal
tofurtherexploitthegraphstructure. informationinthegraph. Therefore,toalleviatethisissue,
2GraphExternalAttentionEnhancedTransformer
Ã— ğ» Ã— ğ»
Q
U U
nk nv
K U ğ‘ 
U U
ek ev
V
(a)Transformerwithself-attention (b)Graphexternalattentionnetwork
Figure2. Transformerversusgraphexternalattentionnetwork.Forsimplicity,weomitskipconnectionsandFFNs.
wedesignGEAET,whichinheritsthemeritsoftheGEA refinenodefeatures. However,itexclusivelyfocusesonthe
network,message-passingGNNandTransformer,leverages correlationsamongnodeswithinasinglegraph,overlook-
inter-graphcorrelations,localstructureandglobalinterac- ingimplicitconnectionsbetweennodesindifferentgraphs,
tioninformation. whichmaypotentiallylimititscapacityandadaptability.
Thus,inspiredby(Guoetal.,2022),weintroduceanovel
3.Method methodcalledGEA,asshowninFigure2b. Itcalculates
attentionbetweenthenodefeaturesoftheinputgraphand
Inthefollowing,wedenoteagraphasG=(V,E),whereV
theexternalunits,using:
representsthesetofnodesandE representstheedges. The
graphhasn = |V|nodesandm = |E|edges. Wedenote A =norm(XUT)âˆˆRnÃ—S,
GE
(2)
thenodefeaturesforanodei âˆˆ V asx i andthefeatures GE-Attn(X)=A UâˆˆRnÃ—d,
GE
foranedgebetweennodesiandj ase . Allnodefeatures
i,j
and edge features are stored in matrices X âˆˆ RnÃ—d and whereUâˆˆRSÃ—disalearnableparameterindependentof
EâˆˆRmÃ—d,respectively. theinputgraph,itcanbeviewedasanexternalunitwith
S nodes,servingassharedmemoryforallinputgraphs. In
3.1.GraphExternalAttention self-attention,A Self representsthesimilaritiesbetweenthe
nodesoftheinputgraph,whileinGEA,A denotesthe
GE
WefirstrevisittheTransformer,asillustratedinFigure2a.
similaritiesbetweenthenodesoftheinputgraphandthe
Transformerconsistsoftwoblocks: aself-attentionmod-
external unit. Considering the sensitivity of the attention
ule and a feed-forward network (FFN). Specifically, self-
matrix to the scale of input features, we apply a double-
attention regards the graph as a fully connected graph normalizationtechnique(Guoetal.,2021)onA . The
GE
and computes the attention of each node to every other
double-normalizationprocessnormalizesbothcolumnsand
node. WiththeinputnodefeaturesX,self-attentionlinearly
rowsseparately,itcanbeexpressedas:
projectstheinputintothreematrices: aquerymatrix(Q),a
keymatrix(K)andavaluematrix(V),whereQ=XW Q, Î±Ëœ i,j =(XUT) i,j,
K = XW andV = XW . Thenself-attentioncanbe (cid:88)n
K V Î±Ë† =exp(Î±Ëœ )/ exp(Î±Ëœ ),
i,j i,j k,j (3)
formulatedas: k=0
(cid:88)S
Î± =Î±Ë† / Î±Ë† .
A
=softmax(âˆšQKT
)âˆˆRnÃ—n,
i,j i,j
k=0
i,k
Self d (1) Inpracticalapplications,forboostingthecapabilityofthe
out
Self-Attn(X)=A VâˆˆRnÃ—dout, network,weutilizetwodistinctexternalunitsforthekey
Self
andvalue. Furthermore,toleveragetheedgeinformation
where W ,W ,W are trainable parameters and d withintheinputgraph,weemployadditionalexternalunits
Q K V out
denotesthedimensionofQ.Theoutputoftheself-attention foredgefeaturesandasharedunittostoretheconnections
isfollowedbybothaskipconnectionandaFFN. betweenedgesandnodes:
X =norm(XU UT )U ,
Self-attention on a graph can be viewed as employing a out s nk nv
(4)
linearcombinationofnodefeatureswithinasinglegraphto E =norm(EU UT )U ,
out s ek ev
3
X
Softmax
Xğ‘œğ‘¢ğ‘¡
X
E
Norm
Norm
Xğ‘œğ‘¢ğ‘¡
Eğ‘œğ‘¢ğ‘¡GraphExternalAttentionEnhancedTransformer
Ã— ğ¿
Transformer
Node embedding
Message-passingGNN
Graphexternal
Edge embedding
attentionnetwork
Figure3.OverallarchitectureofGEAET.ItconsistsofagraphembeddinglayerandLfeatureextractionlayers.Thegraphembedding
layertransformsgraphdataintonodeembeddingsXandedgeembeddingsE.Itcomputespositionalencodings,whichareaddedtothe
nodeembeddingsasinputstothefeatureextractionlayers.Eachfeatureextractionlayerconsistsofagraphexternalattentionnetwork,a
message-passingGNNandaTransformertoextractinter-graphcorrelations,localstructuresandglobalinteractioninformation.Finally,
thisinformationisintegratedusingafeed-forwardnetwork(FFN)andthenemployedontheoutputembeddingsforvariousgraphtasks.
whereU âˆˆRdÃ—disasharedunittostoretheconnections hiddenfeatures:
s
betweenedgesandnodes;U ,U âˆˆRSÃ—dareexternal
key-value units for nodes, wn hk ile Unv ek,U ev âˆˆ RSÃ—d are xËœ0 i =W x0Î± i+u0 âˆˆRd, (6)
externalkey-valueunitsforedges. e0 =W0Î² +v0 âˆˆRd,
ij e i,j
WithintheTransformerarchitecture,self-attentioniscom-
whereW0 âˆˆ RdÃ—dÎ±,W0 âˆˆ RdÃ—dÎ² andu0,v0 âˆˆ Rd are
putedacrossvariousinputchannelsinmultipleinstances, x e
learnableparameters. Then,weusepositionalencodingto
atechniquereferredtoasmulti-headattention. Multi-head
enhancetheinputnodefeatures:
attentioncancapturediversenoderelations,enhancingthe
abilityoftheattentionmechanism. Similarly,takenodesas x0 =T0p +xËœ0, (7)
anexample,therelationsbetweennodeswithinthegraph i i i
andexternalunitsarevarious. Therefore,weadoptananal- where T0 âˆˆ RdÃ—k is a learnable matrix and p âˆˆ Rk is
i
ogousapproach,itcanbewrittenas:
positionalencoding. Itisnoteworthythattheadvantagesof
differentpositionalencodingsaredependentonthedataset.
h =GE-Attn(X ,U ,U ),
i i nk nv
FeatureExtractionLayer. Ateachlayer,externalfeature
X =MultiHeadGEA(X,U ,U ) (5)
out nk nv
informationiscapturedbytheGEANetandthenaggregated
=Concat(h ,...,h )W ,
1 H o withintra-graphinformationtoupdatenodefeatures. The
intra-graphinformationisobtainedthroughacombination
whereh representsthei-thhead,H isthetotalnumberof ofmessage-passingGNNandTransformer. Thisprocess
i
heads,W isalineartransformationmatrix,U ,U âˆˆ canbeformulatedas:
o nk nv
RSÃ—dserveassharedmemoryunitsfordifferentheads. Fi-
Xl+1,El+1 =MPNNl(Xl,El,A),
nally,theoutputoftheGEAisfollowedbyaskipconnection M M
formingaGraphExternalAttentionNetwork(GEANet). Xl+1 =TLayerl(Xl), (8)
T
Xl+1,El+1 =GEANetl(Xl,El+1),
3.2.GraphExternalAttentionEnhancedTransformer G G M
whereGEANetreferstothegraphexternalattentionnet-
Figure 3 illustrates an overview of the proposed GEAET
work introduced in Section 3.1, TLayer represents the
framework. GEAET consists of two components: graph
Transformerlayerwithself-attention,AâˆˆRnÃ—nisthead-
embeddingandfeatureextractionlayers.
jacencymatrix,MPNNisaninstanceofamessage-passing
GNNtoupdatenodeandedgerepresentationsasfollows:
GraphEmbedding. Foreachinputgraph,weinitiallyper-
formalinearprojectionoftheinputnodefeaturesÎ±
i
âˆˆRdÎ± xl i+1 =f node(xl i,{xl j |j âˆˆN(i)},el i,j),
(9)
andedgefeaturesÎ²
i,j
âˆˆ RdÎ², resultingind-dimensional el i+ ,j1 =f edge(xl i,xl j,el i,j),
4
Graph
embedding
Feed-forward
networkGraphExternalAttentionEnhancedTransformer
Table1.ComparisonofGEAETtobaselinesonvarioustasks,includingthreetasksinBenchmarkingGNNs(graphclassificationfor
CIFAR10andMNIST,nodeclassificationforPATTERN),aswellasthreetasksinLRGB(nodeclassificationforPascalVOC-SPand
COCO-SP,graphregressionforPeptides-struct).Bestresultsarecolored:first,second,third.
Model CIFAR10 MNIST PATTERN Peptides-Struct PascalVOC-SP COCO-SP
Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ MAEâ†“ F1scoreâ†‘ F1scoreâ†‘
GCN(Kipf&Welling,2017) 55.710Â±0.381 90.705Â±0.218 71.892Â±0.334 0.3496Â±0.0013 0.1268Â±0.0060 0.0841Â±0.0010
GINE(Xuetal.,2019) â€“ â€“ â€“ 0.3547Â±0.0045 0.1265Â±0.0076 0.1339Â±0.0044
GIN(Xuetal.,2019) 55.255Â±1.527 96.485Â±0.252 85.387Â±0.136 â€“ â€“ â€“
GAT(VelicË‡kovicÂ´etal.,2018) 64.223Â±0.455 95.535Â±0.205 78.271Â±0.186 â€“ â€“ â€“
GatedGCN(Bresson&Laurent,2017) 67.312Â±0.311 97.340Â±0.143 85.568Â±0.088 0.3357Â±0.0006 0.2873Â±0.0219 0.2641Â±0.0045
PNA(Corsoetal.,2020) 70.350Â±0.630 97.940Â±0.120 â€“ â€“ â€“ â€“
DGN(Beainietal.,2021) 72.838Â±0.417 â€“ 86.680Â±0.034 â€“ â€“ â€“
DRew(Gutteridgeetal.,2023) â€“ â€“ â€“ 0.2536Â±0.0015 0.3314Â±0.0024 â€“
CRaWl(Toenshoffetal.,2021) 69.013Â±0.259 97.944Â±0.050 â€“ â€“ â€“ â€“
GIN-AK+(Zhaoetal.,2022) 72.190Â±0.130 â€“ 86.850Â±0.057 â€“ â€“ â€“
SAN(Kreuzeretal.,2021) â€“ â€“ 86.581Â±0.037 0.2545Â±0.0012 0.3230Â±0.0039 0.2592Â±0.0158
K-SubgraphSAT(Chenetal.,2022) â€“ â€“ 86.848Â±0.037 â€“ â€“ â€“
EGT(Hussainetal.,2022) 68.702Â±0.409 98.173Â±0.087 86.821Â±0.020 â€“ â€“ â€“
GraphGPS(RampaÂ´sË‡eketal.,2022) 72.298Â±0.356 98.051Â±0.126 86.685Â±0.059 0.2500Â±0.0005 0.3748Â±0.0109 0.3412Â±0.0044
LGI-GT(Yin&Zhong,2023) â€“ â€“ 86.930Â±0.040 â€“ â€“ â€“
GPTrans-Nano(Gutteridgeetal.,2023) â€“ â€“ 86.731Â±0.085 â€“ â€“ â€“
Graph-ViT/MLPMixer(Heetal.,2023) 73.960Â±0.330 98.460Â±0.090 â€“ 0.2449Â±0.0016 â€“ â€“
GRIT(Maetal.,2023) 76.468Â±0.881 98.108Â±0.111 87.196Â±0.076 0.2460Â±0.0012 â€“ â€“
Exphormer(Shirzadetal.,2023) 74.754Â±0.194 98.414Â±0.038 86.734Â±0.008 0.2481Â±0.0007 0.3966Â±0.0027 0.3430Â±0.0008
GEAET(ours) 76.634Â±0.427 98.513Â±0.086 86.993Â±0.026 0.2445Â±0.0013 0.4585Â±0.0087 0.3895Â±0.0050
where xl+1,xl,el+1,el âˆˆ Rd, l is the layer index, i,j latest state-of-the-art models. In addition, we integrate
i i i,j i,j
denotes the node index, N(i) is the neighborhood of the GEANetwiththemessage-passingGNNsandcompareit
i-thnodeandthefunctionsf andf withlearnable withthecorrespondingnetworktodemonstratetheroleof
node edge
parametersdefineanyarbitrarymessage-passingGNNar- GEANet. Furthermore, we conduct a series of compara-
chitecture(Kipf&Welling,2017;Bresson&Laurent,2017; tiveexperimentswithTransformer,includingvisualization
Hamilton et al., 2017; VelicË‡kovicÂ´ et al., 2018; Xu et al., experimentsonattentioninmoleculargraphs,experiments
2019;Huetal.,2020). varyingthenumberofattentionheadsandpositionalencod-
ing experiments. Finally, we conduct ablation studies on
Finally,weemployanFFNblocktoaggregatenodeinforma-
eachcomponentofGEANet, includingtheexternalnode
tiontoobtainthenoderepresentationsXl+1. Additionally,
unit,theexternaledgeunitandthesharedunit,toconfirm
weemployEl+1astheedgefeaturesforthel+1-thlayer:
G theeffectivenessofeachcomponent. Moredetailsonthe
Xl+1 =FFNl(Xl+1+Xl+1+Xl+1), experimentalsetupandhyperparametersareprovidedinthe
G T M (10) AppendixB,additionalresultsaregivenintheAppendixC.
El+1 =El+1,
G
Insummary,ourexperimentsrevealthat: (a)ourGEAET
whereXl+1,Xl+1 âˆˆRnÃ—daretheoutputsofl-layerTrans- architectureoutperformsexistingstate-of-the-artmethods
T M
former and message-passing GNN, Xl+1,El+1 âˆˆ RnÃ—d on various datasets, (b) GEANet can be seamlessly inte-
G G
aretheoutputsofl-layerGEANet. gratedwithsomebasicGNNs,significantlyenhancingthe
performance of GNNs and (c) GEANet shows better in-
SeeAppendixDforthecomplexityanalysisofGEAET.
terpretability than Transformer and is less dependent on
positionalencoding.
4.Experiments
4.1.ComparisonwithSOTAs
Inthissection, weevaluatetheempiricalperformanceof
GEANetandGEAETonavarietyofgraphdatasetswith We compare our methods with several recent SOTA
graphpredictionandnodepredictiontasks, includingCI- graphTransformers, includingExphormer, GRIT,Graph-
FAR10, MNIST, PATTERN, CLUSTER and ZINC from ViT/MLPMixer, and numerous popular graph representa-
Benchmarking GNNs (Dwivedi et al., 2020), as well as tionlearningmodels,suchaswell-knownmessage-passing
PascalVOC-SP, COCO-SP, Petides-Struct, Petides-Func GNNs (GCN, GIN, GINE, GAT, GatedGCN, PNA), and
andPCQM-ContactfromLongRangeGraphBenchmark graphTransformers(SAN,SAT,EGT,GraphGPS,LGI-GT,
(LRGB; Dwivedi et al., 2022b), and the TreeNeighbour- GPTrans-Nano). Additionally, we consider other recent
Matchdataset(Alon&Yahav,2021). Detailedinformation methods with SOTA performance, such as DGN, DRew,
isprovidedintheAppendixA. CRaW1andGIN-AK+.
Wefirstcompareourmainarchitecture,GEAET,withthe AsshowninTable1,forthethreetasksfromBenchmarking
5GraphExternalAttentionEnhancedTransformer
Table2.Comparisonoftheclassicalmessage-passingGNNbaselinetoitsvariantaugmentedbyGEANet(withoutpositionalencoding
forfairness)acrossfiveLRGBtasks:(fromlefttoright)nodeclassification,nodeclassification,graphregression,graphclassificationand
linkprediction.
Model PascalVOC-SP COCO-SP Peptides-Struct Peptides-Func PCQM-Contact
F1scoreâ†‘ F1scoreâ†‘ MAEâ†“ APâ†‘ MRRâ†‘
GCN 0.1268Â±0.0060 0.0841Â±0.0010 0.3496Â±0.0013 0.5930Â±0.0023 0.3234Â±0.0006
+GEANet 0.2250Â±0.0103 0.2096Â±0.0041 0.2512Â±0.0003 0.6722Â±0.0065 0.3244Â±0.0007
GINE 0.1265Â±0.0076 0.1339Â±0.0044 0.3547Â±0.0045 0.5498Â±0.0079 0.3180Â±0.0027
+GEANet 0.2742Â±0.0032 0.2410Â±0.0028 0.2544Â±0.0012 0.6509Â±0.0021 0.3276Â±0.0012
GatedGCN 0.2873Â±0.0219 0.2641Â±0.0045 0.3420Â±0.0013 0.5864Â±0.0077 0.3242Â±0.0008
+GEANet 0.3933Â±0.0027 0.3219Â±0.0052 0.2547Â±0.0009 0.6485Â±0.0035 0.3321Â±0.0008
Table3.Comparisonoftheclassicalmessage-passingGNNbaselinetoitsvariantaugmentedbyGEANet(withoutpositionalencoding
forfairness)acrossfiveBenchmarkingGNNstasks:(fromlefttoright)nodeclassification,nodeclassification,graphclassification,graph
classificationandgraphregression.
Model PATTERN CLUSTER MNIST CIFAR10 ZINC
Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ Accuracy(%)â†‘ MAEâ†“
GCN 71.892Â±0.334 68.498Â±0.976 90.705Â±0.218 55.710Â±0.381 0.367Â±0.011
+GEANet 85.323Â±0.128 74.015Â±0.124 96.465Â±0.054 61.925Â±0.271 0.240Â±0.008
GIN 85.387Â±0.136 64.716Â±1.553 96.485Â±0.252 55.255Â±1.527 0.526Â±0.051
+GEANet 85.527Â±0.015 66.370Â±2.145 96.845Â±0.097 62.320Â±0.221 0.193Â±0.001
GatedGCN 85.568Â±0.088 73.840Â±0.326 97.340Â±0.143 67.312Â±0.311 0.282Â±0.015
+GEANet 85.607Â±0.038 77.013Â±0.224 98.315Â±0.097 73.857Â±0.306 0.218Â±0.011
GNNs(Dwivedietal.,2020),weobservethatourGEAET 4.3.ComparisonwithSelf-Attention
achievesSOTAresultsonCIFAR10andMNISTandranks
AttentionInterpretation. Tobetterexplaintheattention
second on the PATTERN dataset. For the three tasks on
mechanism,werespectivelytrainaGEANetandaTrans-
LRGB(Dwivedietal.,2022b),GEAETachievesthebest
former on the ZINC dataset and visualize the attention
results. ItisnoteworthythattheGEAETachievesF1scores
scoresinFigure4. Thesalientdifferencebetweenthetwo
of0.4585onPascalVOC-SPand0.3895onCOCO-SP,sur-
modelsisthatGEANetcancapturethecorrelationbetween
passingothermodelsbyasignificantgap.
graphs,andthuswecanattributethefollowinginterpretabil-
ity gains to that. While both models manage to identify
4.2.ComparisonwithGNNs
somehydrophilicstructuresorfunctionalgroups,theatten-
To clearly demonstrate the performance improvement of tionscoreslearnedbyGEANetaresparserandmoreinfor-
GEANetongraphrepresentationlearningmodels,weinte- mative. GEANet focuses more on important atoms such
grateGEANetwithsomecommonlyusedmessage-passing asNandO,aswellasatomsthatconnectdifferentmotifs.
GNNs,providingthemodelswiththeabilitytolearngraph TheattentiondistributionofGEANetissimilartothestruc-
externalinformation.Inourcomparison,weevaluateourap- turaldistributionoftheoriginalmoleculargraphs, which
proachagainstthecorrespondingGNNs,withGNNresults promotestopredicttherestrictedsolubilitymoreaccurately.
sourcedfromDwivedietal.(2020)orDwivedietal.(2022b). Incontrast,Transformerdoesnotutilizeinter-graphcorre-
Tomaintainafaircomparison,ourtrainedmodelsstrictlyad- lations,resultinginpoorerpredictiveperformance. More
heretotheparameterconstraintswithoutincorporatingany resultsareprovidedintheAppendixE.
positionalencoding,consistentwithDwivedietal.(2020)
and Dwivedi et al. (2022b). As depicted in Table 2 and ImpactofAttentionHeads. Weinvestigatetheimpact
Table3,GEANetsignificantlyimprovestheperformance onthenumberofattentionheadswithtwoattentionmech-
ofallbasemessage-passingGNNs,includingGCN(Kipf anisms. Figure5showstheMAEvalueswitheitherGCN
&Welling,2017),GatedGCN(Bresson&Laurent,2017), +TransformerorGCN+GEANetonthePeptides-Struct
GIN(Xuetal.,2019)andGINE(Huetal.,2020),onvarious dataset. For a fair comparison, both models use Lapla-
datasetssimplybycombiningtheoutputofGEANetwith cianpositionalencoding,withthesamenumberoflayers
theoutputoftheGNN.Thisisachievedwithoutanyaddi- andapproximatelytotalnumberofparameters(about500k).
tionalmodifications,validatingthatGEANetcaneffectively Heads=0correspondstoapureGCNnetworkwithoutatten-
alleviateissuesinmessage-passingGNNs. tion. Theintroductionofattentionmechanismsignificantly
improvesperformance. GEANetachievesthebestperfor-
mancewith8heads. Incontrast,Transformerperformsbest
withonehead,suggestingthatmultipleself-attentionheads
6GraphExternalAttentionEnhancedTransformer
donotenhanceperformance. Notably,GEANetconsistently (LapPE;Dwivedi&Bresson,2020;Kreuzeretal.,2021).
outperformsself-attentionacrossvariousnumbersofheads. TheTransformerwithself-attentionperformspoorlywith-
out positional encoding. The utilization of LapPE and
RWPEimprovesperformancetosomeextent. Incontrast,
C O N S GEANetachievesangoodperformancewithoutpositional
encoding.ForGEANet,weobservethatLapPEcanenhance
molecule GEANet Transformer performance,whileRWPEdecreasesperformance. Onthe
0.07
0.030 whole,GEANetislessdependentonpositionalencoding
0.06
0.025 comparedtoTransformer.
0.05
0.04 0.020
0.03 0.015 0.387
0.02 0.010 GCN+Transformer
0.01 0.005 GCN+GEANet
molecule GE-Attn 0.00 Self-Attn 0.000 0.35
0.10
0.04
0.08
0.06 0.03 0.3
0.286
0.02
0.04
0.02 0.01 0.251 0.252 0.255
0.25 0.245
0.00 0.00
Figure4.AttentionvisualizationofGEANetandTransformeron
0.2
ZINC molecular graphs. The left column shows two original
None LapPE RWPE
moleculargraphs,whilethemiddleandrightcolumnsshowthe
visualizationresultsofattentionscoreswithGEANetandTrans- TypesofPE
former,respectively.
Figure6.TestMAEwithdifferentPositionalEncodings(PEs).
GCN+GEANet
4.4.GEAETMitigatesOver-Squashing
GCN+Transformer
0.265
The TreeNeighbourMatch dataset (Alon & Yahav, 2021),
servesasasyntheticdatasettohighlighttheissueofover-
0.26 squashingintuitively. Onthisdataset,eachexamplerepre-
sentsabinarytreeofdepthr. Figure7showstheperfor-
0.255 mancecomparisonofthestandardmessage-passingGNNs
(i.e.,GCN,GINE,GAT,GINandGatedGCN)andourpro-
0.25 posed GEAET on the TreeNeighbourMatch dataset. The
results indicate that our GEAET generalizes well on the
dataset up to r = 7, effectively alleviating the issue of
0.245
over-squashing. In contrast, the five GNN methods fail
0 2 4 6 8
to generalize effectively from r = 4. This is consistent
Numberofattentionheads
with the perspective of Alon & Yahav (2021) that GNN
suffersfromover-squashingduetosquashinginformation
Figure5.TestMAEwithdifferentnumberofattentionheads.
ofthegraphintoafixed-lengthembedding. Incontrast,our
methodaddressesthisproblembyutilizinggraphexternal
attentionmechanismstotransmitlong-rangeinformation
ImpactofPositionalEncoding. Weconductanablation
withoutsquashing.
studyonthePeptides-Structdatasettoassesstheimpactof
positional encoding on GEANet and Transformer. Sim-
4.5.AblationExperiments
ilar to the experiments with attention heads, we utilize
a parallel architecture consisting of an GCN block and Toassessthepracticalityofourmodeldesignchoices,we
an attention block. Figure 6 shows the MAE values ob- conductmultipleablationexperimentsonMNISTandPAT-
tainedwithandwithoutpositionalencoding,includingRan- TERN.TheresultsareshowninTable4. Wenoticethatthe
dom Walk Positional Encoding (RWPE; Li et al., 2020a; removalofeitherexternalnodeunits,externaledgeunits,or
Dwivedietal.,2022a)andLaplacianPositionalEncoding externalsharedunitsallleadstopoorerperformance,which
7
EAMtseT
EAMtseTGraphExternalAttentionEnhancedTransformer
demonstratesthesoundnessofourarchitecturaldecisions. ImpactStatement
1.0 Thispaperpresentsworkwhosegoalistoadvancethefield
of Machine Learning. There are many potential societal
0.9 GCN
consequences of our work, none which we feel must be
0.8 GINE
specificallyhighlightedhere.
GAT
0.7
GIN
0.6 GatedGCN References
0.5 GEAET
Abbe,E. Communitydetectionandstochasticblockmod-
0.4
els: recentdevelopments. JournalofMachineLearning
0.3
Research,18(177):1â€“86,2018.
0.2
0.1 Aharon,M.,Elad,M.,andBruckstein,A. K-svd: Analgo-
rithmfordesigningovercompletedictionariesforsparse
0.0
2 3 4 5 6 7 representation. IEEETransactionsonsignalprocessing,
r(theproblemradius) 54(11):4311â€“4322,2006.
Figure7.Testaccuracyacrossproblemradius(treedepth)onthe Alon,U.andYahav,E. Onthebottleneckofgraphneural
TreeNeighbourMatchdataset. networksanditspracticalimplications. InInternational
ConferenceonLearningRepresentations,2021.
Bapst, V., Keck, T., Grabska-BarwinÂ´ska, A., Donner, C.,
Table4.Ablation study on MNIST and PATTERN datasets for
Cubuk, E. D., Schoenholz, S. S., Obika, A., Nelson,
GEAcomponents.ThemetricistheAccuracy(%).
A.W.,Back,T.,Hassabis,D.,etal. Unveilingthepredic-
Node Edge Share MNIST PATTERN
tivepowerofstaticstructureinglassysystems. Nature
â€“ â€“ â€“ 98.274Â±0.011 86.882Â±0.028
Physics,16(4):448â€“454,2020.
âœ“ â€“ â€“ 98.474Â±0.013 86.956Â±0.038
âœ“ âœ“ â€“ 98.488Â±0.082 86.959Â±0.024
âœ“ âœ“ âœ“ 98.513Â±0.086 86.993Â±0.026 Beaini,D.,Passaro,S.,LeÂ´tourneau,V.,Hamilton,W.,Corso,
G.,andLioÂ´,P. Directionalgraphnetworks. InInterna-
tional Conference on Machine Learning, pp. 748â€“758,
5.Conclusion
2021.
Observingthatexistinggraphrepresentationlearningmeth-
Bengio,Y.,Lodi,A.,andProuvost,A. Machinelearning
odsareconfinedtointernalinformation,weargueforthe
for combinatorial optimization: a methodological tour
importance of inter-graph correlations. Drawing inspira-
dâ€™horizon. EuropeanJournalofOperationalResearch,
tionfromtheideathatgraphswithsimilarstructuresought
290(2):405â€“421,2021.
to have analogous representations, we propose GEA, a
novellightweightyeteffectiveattentionmechanismtoex-
Bodnar,C.,Frasca,F.,Otter,N.,Wang,Y.,Lio`,P.,Montu-
ploitinter-graphcorrelations. Onthisbasis,weintroduce
far,G.F.,andBronstein,M. Weisfeilerandlehmango
GEAETtoexploitlocalstructureandglobalinteractionin-
cellular: Cwnetworks. AdvancesinNeuralInformation
formation. GEAETachievesstate-of-the-artperformance
ProcessingSystems,34:2625â€“2640,2021.
onvariousgraphdatasets,highlightingthesignificanceof
inter-graphcorrelations. Nevertheless,GEAETisnotthe
Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,and
finalchapterofourwork;futureeffortswillfocusonreduc-
Yakhnenko, O. Translating embeddings for modeling
ingthehighmemorycostandtimecomplexity,aswellas multi-relational data. Advances in Neural Information
addressingthelackofupperboundsonexpressivepower. ProcessingSystems,26,2013.
Acknowledgements Bouritsas,G.,Frasca,F.,Zafeiriou,S.,andBronstein,M.M.
Improving graph neural network expressivity via sub-
This work is supported by the National Science and graphisomorphismcounting. IEEETransactionsonPat-
Technology Major Project (2020AAA0106102) and Na- ternAnalysisandMachineIntelligence,45(1):657â€“668,
tionalNaturalScienceFoundationofChina(No.62376142, 2023.
U21A20473, 62272285). The authors would also like to
thankDr. JunbiaoCuiforhisinsightfulopinionsduringthe Bresson,X.andLaurent,T. Residualgatedgraphconvnets.
rebuttalperiod. arXivpreprintarXiv:1711.07553,2017.
8
ycaruccAtseTGraphExternalAttentionEnhancedTransformer
Cappart,Q.,CheÂ´telat,D.,Khalil,E.B.,Lodi,A.,Morris,C., Dwivedi, V. P., Luu, A. T., Laurent, T., Bengio, Y., and
andVelicË‡kovicÂ´,P. Combinatorialoptimizationandrea- Bresson, X. Graph neural networks with learnable
soningwithgraphneuralnetworks. JournalofMachine structuralandpositionalrepresentations. arXivpreprint
LearningResearch,24(130):1â€“61,2023. arXiv:2110.07875,2021.
Dwivedi,V.P.,Luu,A.T.,Laurent,T.,Bengio,Y.,andBres-
Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., and Sun, X.
son,X. Graphneuralnetworkswithlearnablestructural
Measuringandrelievingtheover-smoothingproblemfor
graphneuralnetworksfromthetopologicalview. Pro- andpositionalrepresentations. InInternationalConfer-
ceedings of the AAAI Conference on Artificial Intelli-
enceonLearningRepresentations,2022a.
gence,34(04):3438â€“3445,2020.
Dwivedi,V.P.,RampaÂ´sË‡ek,L.,Galkin,M.,Parviz,A.,Wolf,
G.,Luu,A.T.,andBeaini,D. Longrangegraphbench-
Chen,D.,Oâ€™Bray,L.,andBorgwardt,K. Structure-aware
mark. AdvancesinNeuralInformationProcessingSys-
transformerforgraphrepresentationlearning. InInterna-
tems,35:22326â€“22340,2022b.
tionalConferenceonMachineLearning,pp.3469â€“3489,
2022. Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Ben-
gio, Y., and Bresson, X. Benchmarking graph neural
Corso,G.,Cavalleri,L.,Beaini,D.,Lio`,P.,andVelicË‡kovicÂ´, networks. Journal of Machine Learning Research, 24
P. Principalneighbourhoodaggregationforgraphnets. (43):1â€“48,2023.
AdvancesinNeuralInformationProcessingSystems,33:
13260â€“13271,2020. Everingham,M.,VanGool,L.,Williams,C.K.,Winn,J.,
andZisserman,A. Thepascalvisualobjectclasses(voc)
Cranmer,M.D.,Xu,R.,Battaglia,P.,andHo,S. Learning challenge. InternationalJournalofComputerVision,88:
symbolicphysicswithgraphnetworks. arXivpreprint 303â€“338,2010.
arXiv:1909.05862,2019.
Fey,M., Yuen, J.-G.,andWeichert,F. Hierarchicalinter-
messagepassingforlearningonmoleculargraphs. arXiv
Defferrard, M., Bresson, X., and Vandergheynst, P. Con-
preprintarXiv:2006.12179,2020.
volutionalneuralnetworksongraphswithfastlocalized
spectralfiltering. AdvancesinNeuralInformationPro-
Gaudelet, T., Day, B., Jamasb, A. R., Soman, J., Regep,
cessingSystems,29,2016.
C.,Liu,G.,Hayter,J.B.,Vickers,R.,Roberts,C.,Tang,
J., et al. Utilizinggraph machinelearning withindrug
Derrow-Pinion,A.,She,J.,Wong,D.,Lange,O.,Hester,T.,
discoveryanddevelopment. Briefingsinbioinformatics,
Perez,L.,Nunkesser,M.,Lee,S.,Guo,X.,Wiltshire,B.,
22(6):bbab159,2021a.
etal. Etapredictionwithgraphneuralnetworksingoogle
maps. In Proceedings of the 30th ACM International Gaudelet, T., Day, B., Jamasb, A. R., Soman, J., Regep,
ConferenceonInformation&KnowledgeManagement, C.,Liu,G.,Hayter,J.B.,Vickers,R.,Roberts,C.,Tang,
pp.3767â€“3776,2021. J., et al. Utilizinggraph machinelearning withindrug
discoveryanddevelopment. BriefingsinBioinformatics,
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: 22(6),2021b.
Pre-training of deep bidirectional transformers for lan-
guageunderstanding. arXivpreprintarXiv:1810.04805, Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,and
2018. Dahl,G.E. Neuralmessagepassingforquantumchem-
istry. InInternationalConferenceonMachineLearning,
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, pp.1263â€“1272,2017.
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
Guo,M.-H.,Cai,J.-X.,Liu,Z.-N.,Mu,T.-J.,Martin,R.R.,
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
andHu,S.-M. Pct: Pointcloudtransformer. Computa-
N. An image is worth 16x16 words: Transformers for
tionalVisualMedia,7:187â€“199,2021.
imagerecognitionatscale. InInternationalConference
onLearningRepresentations,2021. Guo,M.-H.,Liu,Z.-N.,Mu,T.-J.,andHu,S.-M. Beyond
self-attention: Externalattentionusingtwolinearlayers
Dwivedi, V. P. and Bresson, X. A generalization forvisualtasks. IEEETransactionsonPatternAnalysis
of transformer networks to graphs. arXiv preprint andMachineIntelligence,45(5):5436â€“5447,2022.
arXiv:2012.09699,2020.
Gutteridge, B., Dong, X., Bronstein, M. M., and Di Gio-
Dwivedi,V.P.,Joshi,C.K.,Luu,A.T.,Laurent,T.,Bengio, vanni, F. Drew: Dynamically rewired message pass-
Y.,andBresson,X.Benchmarkinggraphneuralnetworks. ingwithdelay. InInternationalConferenceonMachine
arXivpreprintarXiv:2003.00982,2020. Learning,pp.12252â€“12267,2023.
9GraphExternalAttentionEnhancedTransformer
Hamilton,W.,Ying,Z.,andLeskovec,J. Inductiverepre- Li, Y., Qian, B., Zhang, X., and Liu, H. Graph neural
sentationlearningonlargegraphs. AdvancesinNeural network-baseddiagnosisprediction. BigData,8(5):379â€“
InformationProcessingSystems,30,2017. 390,2020b.
He, X., Hooi, B., Laurent, T., Perold, A., Lecun, Y., and Liu,Z.,Lin,W.,Shi,Y.,andZhao,J. Arobustlyoptimized
Bresson, X. A generalization of ViT/MLP-mixer to bertpre-trainingapproachwithpost-training. InChinese
graphs. InInternationalConferenceonMachineLearn- ComputationalLinguistics,pp.471â€“484,2021.
ing,pp.12724â€“12745,2023.
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient
Hu,W.,Liu,B.,Gomes,J.,Zitnik,M.,Liang,P.,Pande,V., descentwithwarmrestarts. InInternationalConference
andLeskovec, J. Strategiesforpre-traininggraphneu- onLearningRepresentations,2017.
ralnetworks. InInternationalConferenceonLearning
Representations,2020. Loshchilov,I.andHutter,F. Decoupledweightdecayreg-
ularization. In International Conference on Learning
Hu, W., Fey, M., Ren, H., Nakata, M., Dong, Y., Representations,2019.
and Leskovec, J. Ogb-lsc: A large-scale challenge
for machine learning on graphs. arXiv preprint Ma, L., Lin, C., Lim, D., Romero-Soriano, A., Dokania,
arXiv:2103.09430,2021. P. K., Coates, M., Torr, P., and Lim, S.-N. Graph in-
ductivebiasesintransformerswithoutmessagepassing.
Hussain,M.S.,Zaki,M.J.,andSubramanian,D. Global In International Conference on Machine Learning, pp.
self-attentionasareplacementforgraphconvolution. In 23321â€“23337,2023.
Proceedings of the 28th ACM SIGKDD International
ConferenceonKnowledgeDiscoveryandDataMining, Mialon,G.,Chen,D.,Selosse,M.,andMairal,J. Graphit:
pp.655â€“665,2022. Encodinggraphstructureintransformers. arXivpreprint
arXiv:2106.05667,2021.
Ingraham,J.,Garg,V.,Barzilay,R.,andJaakkola,T. Gener-
ativemodelsforgraph-basedproteindesign. Advances Monti, F., Frasca, F., Eynard, D., Mannion, D., and
inNeuralInformationProcessingSystems,32,2019. Bronstein, M. M. Fake news detection on social me-
dia using geometric deep learning. arXiv preprint
Irwin,J.J.,Sterling,T.,Mysinger,M.M.,Bolstad,E.S.,and arXiv:1902.06673,2019.
Coleman,R.G.Zinc:afreetooltodiscoverchemistryfor
biology. JournalofChemicalInformationandModeling, Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,
52(7):1757â€“1768,2012. J.E.,Rattan,G.,andGrohe,M. Weisfeilerandlemango
neural: Higher-ordergraphneuralnetworks. InProceed-
Kipf, T. N. and Welling, M. Semi-supervised classifica- ingsoftheAAAIConferenceonArtificialIntelligence,pp.
tionwithgraphconvolutionalnetworks. InInternational 4602â€“4609,2019.
ConferenceonLearningRepresentations,2017.
Murphy,R.,Srinivasan,B.,Rao,V.,andRibeiro,B. Rela-
Kreuzer,D.,Beaini,D.,Hamilton,W.,LeÂ´tourneau,V.,and tionalpoolingforgraphrepresentations. InInternational
Tossou,P. Rethinkinggraphtransformerswithspectral ConferenceonMachineLearning,pp.4663â€“4673,2019.
attention. Advances in Neural Information Processing
Systems,pp.21618â€“21629,2021. Olshausen,B.A.andField,D.J. Emergenceofsimple-cell
receptive field properties by learning a sparse code for
LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P. Gradient- naturalimages. Nature,381(6583):607â€“609,1996.
basedlearningappliedtodocumentrecognition. Proceed-
ingsoftheIEEE,86(11):2278â€“2324,1998. Oono,K.andSuzuki,T. Graphneuralnetworksexponen-
tially lose expressive power for node classification. In
Li,P.,Wang,Y.,Wang,H.,andLeskovec,J. Distanceen- InternationalConferenceonLearningRepresentations,
coding: Designprovablymorepowerfulneuralnetworks 2020.
for graph representation learning. Advances in Neural
InformationProcessingSystems,33:4465â€“4478,2020a. Pal,A.,Eksombatchai,C.,Zhou,Y.,Zhao,B.,Rosenberg,
C.,andLeskovec,J. Pinnersage: Multi-modaluserem-
Li,Q.,Han,Z.,andWu,X.-M. Deeperinsightsintograph beddingframeworkforrecommendationsatpinterest. In
convolutionalnetworksforsemi-supervisedlearning. In Proceedings of the 26th ACM SIGKDD International
ProceedingsoftheAAAIConferenceonArtificialIntelli- ConferenceonKnowledgeDiscoveryandDataMining,
gence,2018. pp.2311â€“2320,2020.
10GraphExternalAttentionEnhancedTransformer
Qiu, J., Dong, Y., Ma, H., Li, J., Wang, K., and Tang, J. Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. Howpowerful
Network embedding as matrix factorization: Unifying aregraphneuralnetworks? InInternationalConference
deepwalk,line,pte,andnode2vec. InProceedingsofthe onLearningRepresentations,2019.
11thACMInternationalConferenceonWebSearchand
Yin, S. and Zhong, G. Lgi-gt: graph transformers with
DataMining,pp.459â€“467,2018.
localandglobaloperatorsinterleaving. InProceedingsof
RampaÂ´sË‡ek,L.,Galkin,M.,Dwivedi,V.P.,Luu,A.T.,Wolf, InternationalJointConferenceonArtificialIntelligence,
G.,andBeaini,D. Recipeforageneral,powerful,scal- pp.4504â€“4512,2023.
ablegraphtransformer. AdvancesinNeuralInformation
Ying,C.,Cai,T.,Luo,S.,Zheng,S.,Ke,G.,He,D.,Shen,Y.,
ProcessingSystems,35:14501â€“14515,2022.
andLiu,T.-Y. Dotransformersreallyperformbadlyfor
Sato,R.,Yamada,M.,andKashima,H. Randomfeatures graphrepresentation? AdvancesinNeuralInformation
strengthengraphneuralnetworks. InProceedingsofthe ProcessingSystems,34:28877â€“28888,2021.
2021SIAMInternationalConferenceonDataMining,pp.
Zhang,J.,Zhang,H.,Xia,C.,andSun,L. Graph-bert: Only
333â€“341,2021.
attention is needed for learning graph representations.
Shirzad,H.,Velingker,A.,Venkatachalam,B.,Sutherland, arXivpreprintarXiv:2001.05140,2020.
D.J., andSinop, A. K. Exphormer: Sparsetransform-
Zhao, L., Jin, W., Akoglu, L., and Shah, N. From stars
ersforgraphs. InInternationalConferenceonMachine
to subgraphs: Uplifting any GNN with local structure
Learning,pp.31613â€“31632,2023.
awareness. In International Conference on Learning
Representations,2022.
Singh,S.,Chaudhary,K.,Dhanda,S.K.,Bhalla,S.,Usmani,
S.S.,Gautam,A.,Tuknait,A.,Agrawal,P.,Mathur,D.,
and Raghava, G. P. Satpdb: a database of structurally
annotatedtherapeuticpeptides. NucleicAcidsResearch,
44,2016.
Stokes, J. M., Yang, K., Swanson, K., Jin, W., Cubillos-
Ruiz, A., Donghia, N. M., MacNair, C. R., French, S.,
Carfrae, L. A., Bloom-Ackermann, Z., et al. A deep
learningapproachtoantibioticdiscovery. Cell,180(4):
688â€“702,2020.
Toenshoff, J., Ritzert, M., Wolf, H., and Grohe, M.
Graphlearningwith1dconvolutionsonrandomwalks.
arXiv:2102.08786,2021.
ToÂ¨nshoff, J., Ritzert, M., Rosenbluth, E., and Grohe, M.
Wheredidthegapgo? reassessingthelong-rangegraph
benchmark. InTheSecondLearningonGraphsConfer-
ence,2023.
Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,
L.,Gomez,A.N.,Kaiser,L.u.,andPolosukhin,I. At-
tentionisallyouneed. AdvancesinNeuralInformation
ProcessingSystems,30,2017.
VelicË‡kovicÂ´, P., Cucurull, G., Casanova, A., Romero, A.,
Lio`, P., and Bengio, Y. Graph attention networks. In
InternationalConferenceonLearningRepresentations,
2018.
Wu, Z., Jain, P., Wright, M., Mirhoseini, A., Gonzalez,
J.E.,andStoica,I. Representinglong-rangecontextfor
graphneuralnetworkswithglobalattention. Advances
in Neural Information Processing Systems, 34:13266â€“
13279,2021.
11GraphExternalAttentionEnhancedTransformer
A.DatasetDescriptions
Weevaluate ourmethod on diverse datasets, including five graph benchmarkdatasets fromBenchmarking GNNs, five
long-rangedependencygraphdatasetsfromLRGBandtheTreeNeighbourMatchdataset. Below,weprovidedescriptionsof
thedatasetsandpresentsummarystatisticsinTable5.
MNIST and CIFAR10. MNIST and CIFAR10 (Dwivedi et al., 2020) represent the graphical counterparts of their
respective image classification datasets of the same name. A graph is formed by creating an 8-nearest neighbor graph
of the SLIC superpixels of the images. These datasets pose 10-class graph classification challenges. For MNIST, the
resultinggraphshavesizesrangingfrom40to75nodes,whileforCIFAR10,thegraphsvarybetween85and150nodes.
The classification tasks involve the standard dataset splits of 55K/5K/10K for MNIST and 45K/5K/10K for CIFAR10,
correspondingtotrain/validation/testgraphs. Thesedatasetsserveassanitychecks,withanexpectationthatmostGNNs
wouldachievecloseto100%accuracyforMNISTandsatisfactoryperformanceforCIFAR10.
PATTERNandCLUSTER. PATTERNandCLUSTER(Dwivedietal.,2020)aresyntheticdatasetsdesignedfornode
classificationtasks,bothderivedfromtheStochasticBlockModel(SBM;Abbe,2018)usedtomodelcommunities. In
PATTERN, the objective is to determine whether a node is part of one of the 100 predefined subgraph patterns, while
CLUSTER focuses on classifying nodes into six distinct clusters with identical distributions. The unique feature of
PATTERN involves recognizing nodes belonging to randomly generated sub-graph patterns, while CLUSTER entails
inferring the cluster ID for each node in graphs composed of 6 SBM-generated clusters. We use the splits as is used
in(Dwivedietal.,2020).
ZINC. ZINC(Dwivedietal.,2020)isagraphregressiondatasetderivedfromasubsetofmoleculargraphs(12Koutof
250K)sourcedfromafreelyavailabledatabaseofcommerciallyaccessiblecompounds(Irwinetal.,2012). Themolecular
graphsinZINCrangefrom9to37nodes,whereeachnoderepresentsaheavyatom(with28possibleatomtypes)andeach
edgesignifiesabond(with3possibletypes). Theprimarytaskistoregressamolecularpropertyknownasconstrained
solubility. Thedatasetincludesapredefinedtrain/validation/testsplitof10K/1K/1Kinstances.
PascalVOC-SPandCOCO-SP. PascalVOC-SPandCOCO-SP(Dwivedietal.,2022b)aregraph-basedversionsofimage
datasetswithlargerimagesandinvolvethetaskofnodeclassification,specificallythesemanticsegmentationofsuperpixels.
ThesedatasetsrespectivelyderivedfromthePascalVOC2011imagedataset(Everinghametal.,2010)andtheMSCOCO
imagedatasetthroughSLICsuperpixelization,presentamoreintricatenodeclassificationchallengecomparedtoCIFAR10
andMNIST.Eachsuperpixelnodeisassociatedwithaspecificobjectclass,makingthemnodeclassificationdatasetswitha
focusonregionsofimagesbelongingtoparticularclasses.
Peptides-FuncandPeptides-Struct. Peptides-FuncandPeptides-Struct(Dwivedietal.,2022b)arederivedfrom15,535
peptideswithatotalof2.3millionnodessourcedfromSAT-Pdb(Singhetal.,2016).Thegraphsexhibitlargesizes,averaging
150.94nodespergraphandameangraphdiameterof56.99. SpecificallysuitedforbenchmarkinggraphTransformers
orexpressiveGNNscapableofcapturinglong-rangeinteractions. Peptides-funcinvolvesmulti-labelgraphclassification
into10nonexclusivepeptidefunctionalclasses,whilePeptides-structfocusesongraph-levelregressionpredicting113D
structuralpropertiesofthepeptides.
PCQM-Contact. PCQM-Contact(Dwivedietal.,2022b)isderivedfromPCQM4Mv2andcorresponding3Dmolecular
structures,wherethetaskisabinarylinkpredictiontask. Thisdatasetcontains529,434graphswithatotalof15million
nodes,whereeachgraphrepresentsamoleculargraphwithexplicithydrogens. AllgraphsinPCQM-Contactareextracted
fromthePCQM4Mtrainingset,specificallythosewithavailable3Dstructureandfilteredtoretainonlythosewithatleast
onecontact.
TreeNeighbourMatch. TreeNeighbourMatchisasyntheticdatasetintroducedbyAlon&Yahav(2021)toillustratethe
challengeofover-squashinginGNNs. Itfeaturesbinarytreesofcontrolleddepththatsimulateanexponentially-growing
receptivefieldwithaproblemradiusr. Thetaskistopredictalabelforthetargetnode,situatedinoneoftheleafnodes,
necessitatinginformationpropagationfromallleavestothetargetnode. Thissetupexposestheissueofover-squashingat
thetargetnodeduetotheneedforcomprehensivelong-rangesignalincorporationbeforelabelprediction.
12GraphExternalAttentionEnhancedTransformer
Table5. Summarystatisticsofdatasetsusedinthisstudy.
Dataset Graphs Avg.nodes Avg.edges PredictionLevel Task Metric
MNIST 70,000 70.6 564.5 graph 10-classclassif. Accuracy
CIFAR10 60,000 117.6 941.1 graph 10-classclassif. Accuracy
PATTERN 14,000 118.9 3,039.3 inductivenode binaryclassif. Accuracy
CLUSTER 12,000 117.2 2,150.9 inductivenode 6-classclassif. Accuracy
ZINC 12,000 23.2 24.9 graph regression MAE
PascalVOC-SP 11,355 479.4 2,710.5 inductivenode 21-classclassif. F1
COCO-SP 123,286 476.9 2,693.7 inductivenode 81-classclassif. F1
PCQM-Contact 529,434 30.1 61.0 inductivelink linkranking MRR
Peptides-Func 15,535 150.9 307.3 graph 10-classclassif. Avg.Precision
Peptides-Struct 15,535 150.9 307.3 graph regression MAE
TreeNeighbourMatch(r=2) 96 7 6 inductivenode 4-classclassif. Accuracy
TreeNeighbourMatch(r=3) 32,000 15 14 inductivenode 8-classclassif. Accuracy
TreeNeighbourMatch(r=4) 64,000 31 30 inductivenode 16-classclassif. Accuracy
TreeNeighbourMatch(r=5) 128,000 63 62 inductivenode 32-classclassif. Accuracy
TreeNeighbourMatch(r=6) 256,000 127 126 inductivenode 64-classclassif. Accuracy
TreeNeighbourMatch(r=7) 512,000 255 254 inductivenode 128-classclassif. Accuracy
B.HyperparameterChoicesandReproducibility
HyperparameterChoice. Inourhyperparametersearch,weattempttoadjustthenumberofheadsinGEANet,aswell
ashyperparametersrelatedtopositionalencoding,message-passingGNNtypeandTransformer. Consideringthelarge
numberofhyperparametersanddatasets,wedonotconductanexhaustivesearchorgridsearch. Forafaircomparison,we
followcommonlyusedparameterbudgets: forbenchmarkingdatasetsfromBenchmarkingGNNs(Dwivedietal.,2020),a
maximumof500kparametersforPATTERNandapproximately100kparametersforMNISTandCIFAR10;fordatasets
fromLRGB(Dwivedietal.,2022b),weadheretoaparameterbudgetof500k. SeeTable6fordetailedinformation.
Optimization. WeusetheAdamW(Loshchilov&Hutter,2019)optimizerinallourexperiments,withthedefaultsettings
of Î² = 0.9, Î² = 0.999 and Ïµ = 10âˆ’8, and use a cosine scheduler (Loshchilov & Hutter, 2017). The choice of loss
1 2
function,lengthofthewarm-upperiod,baselearningrateandtotalnumberofepochsareadjustedbasedonthedataset.
Table6.HyperparametersusedforGEAETacrossdatasets:CIFAR10,MNIST,PATTERN,Peptides-Struct,PascalVOC-SP,COCO-SP.
Hyperparameter CIFAR10 MNIST PATTERN Peptides-Struct PascalVOC-SP COCO-SP
Layers 5 5 7 6 8 8
HiddenDimd 40 40 64 224 68 68
MPNN GatedGCN GatedGCN GatedGCN GCN GatedGCN GatedGCN
SelfAttention Transformer Transformer Transformer None Transformer Transformer
ExternalNetwork GEANet GEANet GEANet GEANet GEANet GEANet
SelfHeads 4 4 4 None 4 4
ExternalHeads 4 4 4 8 4 4
UnitSizeS 10 10 16 28 17 17
PE ESLapPE-8 ESLapPE-8 RWPE-16 LapPE-10 None None
PEDim 8 8 7 16 None None
BatchSize 16 16 32 200 50 50
LearningRate 0.001 0.001 0.0005 0.001 0.001 0.001
NumEpochs 150 150 100 250 200 200
WarmupEpochs 5 5 5 5 10 10
WeightDecay 1e-5 1e-5 1e-5 0 0 0
NumParameters 113,235 113,155 429,052 463,211 506,213 505,661
13GraphExternalAttentionEnhancedTransformer
C.AdditionalResults
Weprovideadditionalresultshere,includingdetailedresultsfromtheattentionheadsandpositionencodingexperiments,
theresultsofGEAETonthelinkpredictiontaskofthePCQM-Contactdatasetandasubstantialnumberofadditional
ablationstudies.
ImpactofAttentionHeads. WeconductexperimentswithdifferentnumbersofattentionheadsonthePeptides-Struct
andPeptides-Funcdatasets. WeadoptaframeworkwherethebaseGNNandtheattentionblock(TransformerorGEANet)
operateinparallel,withtheoutputofthemodelateachlayerbeingthesumoftheoutputsfromtheGNNandtheattention
block. Toensurefairness,weusethesamenumberoflayers,applyLaplacianpositionalencoding(LapPE),useGCNasthe
baseGNNandkeepthetotalnumberofparameterstoabout500k. ThedetailedresultsareshowninTable7,whereall
resultsareaveragedover4differentrandomseeds. Weobservethathavingmultipleattentionheadsdoesnotsignificantly
improveTransformerforbothdatasets. However,thereisanotableimprovementinGEANet,withthelowestMAEachieved
with8headsonPeptides-StructandthebestAPachievedwith8headsonPeptides-Func.
Table7. TheresultsonTransformerandGEANetwithdifferentnumberofattentionheads.
Model #Layers Positional #Heads #Parameters Peptides-Struct Peptides-Func
Encoding MAEâ†“ APâ†‘
GCN+Transformer 6 LapPE 2 490,571 0.2516Â±0.0031 0.6644Â±0.0052
GCN+Transformer 6 LapPE 3 490,571 0.2529Â±0.0012 0.6688Â±0.0072
GCN+Transformer 6 LapPE 4 490,571 0.2524Â±0.0017 0.6634Â±0.0033
GCN+Transformer 6 LapPE 6 490,571 0.2557Â±0.0032 0.6630Â±0.0085
GCN+Transformer 6 LapPE 8 490,571 0.2544Â±0.0037 0.6593Â±0.0060
GCN+GEANet 6 LapPE 2 626,219 0.2474Â±0.0006 0.6828Â±0.0059
GCN+GEANet 6 LapPE 3 539,123 0.2461Â±0.0006 0.6890Â±0.0060
GCN+GEANet 6 LapPE 4 508,571 0.2455Â±0.0009 0.6892Â±0.0042
GCN+GEANet 6 LapPE 6 486,683 0.2470Â±0.0025 0.6880Â±0.0025
GCN+GEANet 6 LapPE 8 463,211 0.2445Â±0.0013 0.6912Â±0.0012
ImpactofPositionalEncoding. Similartotheexperimentsonattentionheads,westudytheimpactofdifferentpositional
encodingsonattention. Table8showstheresultsaveragedover4differentrandomseeds. WefindthatGEANethasalower
dependencyonpositionalencodingcomparedtoTransformerwithself-attention.
Table8. TheresultsonTransformerandGEANetwithdifferentpositionalencodings.
Model #Layers Positional #Heads #Parameters Peptides-Struct Peptides-Func
Encoding MAEâ†“ APâ†‘
GCN+Transformer 6 None 4 492,731 0.3871Â±0.0094 0.6404Â±0.0095
GCN+Transformer 6 RWPE 4 490,143 0.2858Â±0.0044 0.6564Â±0.0122
GCN+Transformer 6 LapPE 4 490,571 0.2524Â±0.0017 0.6589Â±0.0069
GCN+GEANet 6 None 4 510,731 0.2512Â±0.0003 0.6722Â±0.0065
GCN+GEANet 6 RWPE 4 508,143 0.2546Â±0.0018 0.6794Â±0.0089
GCN+GEANet 6 LapPE 4 508,571 0.2445Â±0.0013 0.6892Â±0.0042
GEAETinLinkPredictionTask. Inthelinkpredictiontask,weevaluatecommonrankingmetricsfromtheknowledge
graphlinkpredictionliterature(Bordesetal.,2013)asshowninTable9: Hits@1,Hits@3,Hits@10andMeanReciprocal
Rank(MRR),whereHits@kindicateswhethertheactualanswerisamongthetop-kpredictionsprovidedbythemodel.
ImprovingGNNwithGEANet. TodemonstratetheimportanceofGEANet,weconductadditionalexperimentsonthe
Peptides-Struct,Peptides-Func,andPascalVOC-SPdatasets. AsshowninTable10,comparewithpositionalencoding,
GEANetsignificantlyimprovestheperformanceofallbasemessage-passingGNNs.
D.ComplexityAnalysis
WefirstanalyzethecomplexityofGEANet.AsthemodeldimensionsdandthesizeofexternalunitsSarehyper-parameters,
GEANetscaleslinearlywiththenumberofnodesandedges,resultinginacomplexityofO(|V|+|E|). ForGEAET,the
complexityisprimarilydeterminedbyGEANet,Transformerandmessage-passingGNN.TheGEANet,asdescribedabove,
haslinearcomplexity. Themessage-passingGNNhasacomplexityofO(|E|). Intypicalcases,theTransformerusesthe
14GraphExternalAttentionEnhancedTransformer
Table9.PerformanceofGEAETonthelinkpredictiontaskofthePCQM-Contactdataset.Weselectbaselinemodelsthatalsoreportthe
Hits@1,Hits@3,Hits@10,andMRRmetrics.Ourresultsareaveragedover4runswith4differentseeds,whiletheresultsofthebaseline
modelsareeitherfrom(Dwivedietal.,2022b)ortheoriginalpapers.
Model TestHits@1â†‘ TestHits@3â†‘ TestHits@10â†‘ TestMRRâ†‘
SAN 0.1312Â±0.0016 0.4030Â±0.0008 0.8550Â±0.0024 0.3341Â±0.0006
GatedGCN 0.1288Â±0.0013 0.3808Â±0.0006 0.8517Â±0.0005 0.3242Â±0.0008
Transformer 0.1221Â±0.0011 0.3679Â±0.0033 0.8517Â±0.0039 0.3174Â±0.0020
GCN 0.1321Â±0.0007 0.3791Â±0.0004 0.8256Â±0.0006 0.3234Â±0.0006
GINE 0.1337Â±0.0013 0.3642Â±0.0043 0.8147Â±0.0062 0.3180Â±0.0027
GraphDiffuser 0.1369Â±0.0012 0.4053Â±0.0011 0.8592Â±0.0007 0.3388Â±0.0011
GEAET(ours) 0.1566Â±0.0014 0.4227Â±0.0022 0.8626Â±0.0032 0.3518Â±0.0011
Table10.ImprovingGNNperformancewithGEANet.Weruntheexperimentswith4differentseedsandaveragetheresults.
Model Positional PascalVOC-SP Peptides-Struct Peptides-Func
Encoding F1scoreâ†‘ MAEâ†“ APâ†‘
GCN None 0.1268Â±0.0060 0.3496Â±0.0013 0.5930Â±0.0023
GCN+GEANet None 0.2250Â±0.0103 0.2512Â±0.0003 0.6722Â±0.0065
GCN+GEANet LapPE 0.2353Â±0.0070 0.2445Â±0.0013 0.6892Â±0.0042
GCN+GEANet RWPE 0.2325Â±0.0165 0.2546Â±0.0018 0.6794Â±0.0089
GINE None 0.1265Â±0.0076 0.3547Â±0.0045 0.5498Â±0.0079
GINE+GEANet None 0.2742Â±0.0032 0.2544Â±0.0012 0.6509Â±0.0021
GINE+GEANet LapPE 0.2746Â±0.0071 0.2480Â±0.0023 0.6654Â±0.0055
GINE+GEANet RWPE 0.2762Â±0.0022 0.2546Â±0.0011 0.6618Â±0.0059
GatedGCN None 0.2873Â±0.0219 0.3420Â±0.0013 0.5864Â±0.0077
GatedGCN+GEANet None 0.3933Â±0.0027 0.2547Â±0.0009 0.6485Â±0.0035
GatedGCN+GEANet LapPE 0.3944Â±0.0044 0.2468Â±0.0014 0.6715Â±0.0034
GatedGCN+GEANet RWPE 0.3899Â±0.0017 0.2577Â±0.0006 0.6734Â±0.0028
self-attentionmechanismwithacomplexityofO(|V|2),resultingincomplexityofO(|V|2). Inpractice,weobservethaton
certaindatasetssuchasPeptides-StructandPeptides-Func,notusingTransformeryieldsbetterresults,achievinglinear
complexityinsuchcases. Additionally,wecanuselinearTransformerstoreducethecomplexityofGEAETtolinearity.
E.ModelInterpretation
Inadditiontotheexamplespresentedinthemainpaper,weprovideadditionalvisualizationresultsinFigure8. GEANet
andTransformerusethesamepositionalencoding,andotherhyperparametersettingsaregenerallyconsistent. Thefirst
columnshowstheoriginalmoleculesfromZINC,themiddleandrightcolumnsshowthevisualizationresultsofGEANet
and Transformer, respectively. We observe that GEANet focuses more on the important nodes or connected nodes of
specificstructures,whichimprovestheabilitytodistinguishdifferentgraphsormotifs. ThisindicatesthatGEANetexcels
inhandlingstructuralinformationandconcentratesondiscriminativenodes.
15GraphExternalAttentionEnhancedTransformer
C O N S F Cl
molecule GEANet Transformer molecule GEANet Transformer
0.07 0.030 0.04
0.06 0.08
0.025
0.05 0.03
0.04 0.020 0.06
0.03 0.015 0.04 0.02
0.02 0.010
0.02 0.01
0.01 0.005
molecule GE-Attn 0.00 Self-Attn 0.000 molecule GE-Attn 0.00 Self-Attn 0.00
0.10 0.10 0.04
0.04
0.08 0.08
0.03
0.03
0.06 0.06
0.02 0.02
0.04 0.04
0.02 0.01 0.02 0.01
molecule GE-Attn 0.00 Self-Attn 0.00 molecule GE-Attn 0.00 Self-Attn 0.00
0.10
0.10 0.05 0.04
0.08
0.08 0.04 0.03
0.06
0.06 0.03
0.04 0.02
0.04 0.02
0.02 0.01 0.02 0.01
molecule GE-Attn 0.00 Self-Attn 0.00 molecule GE-Attn 0.00 Self-Attn 0.00
0.035
0.12 0.030 0.10
0.030
0.10 0.025 0.08 0.025
0.08 0.020 0.06 0.020
0.06 0.015 0.015
0.04
0.04 0.010 0.010
0.02 0.005 0.02 0.005
0.00 0.000 0.00 0.000
Figure8.AttentionvisualizationofGEANetandTransformeronZINCmoleculargraphs.Thecentercolumnshowstheattentionweights
ofGEANetandtherightcolumnshowstheattentionweightslearnedbytheclassicTransformer.
16