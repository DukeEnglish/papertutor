MambaGesture: Enhancing Co-Speech Gesture Generation with
Mamba and Disentangled Multi-Modality Fusion
ChencanFuâˆ— YabiaoWangâˆ— JiangningZhangâ€  XiaofengMao
ZhejiangUniversity ZhejiangUniversity ZhengkaiJiang FudanUniversity
Hangzhou,China Hangzhou,China TencentYoutuLab Shanghai,China
chencan.fu@zju.edu.cn TencentYoutuLab Shanghai,China xfmao23@m.fudan.edu.cn
Shanghai,China vtzhang@tencent.com
caseywang@tencent.com zhengkjiang@tencent.com
JiafuWu ChengjieWang YanhaoGe YongLiuâ€ 
WeijianCao ShanghaiJiaoTong VIVO ZhejiangUniversity
TencentYoutuLab University Shanghai,China Shanghai,China
Shanghai,China TencentYoutuLab halege@vivo.com yongliu@iipc.zju.edu.cn
jiafwu@tencent.com Shanghai,China
weijiancao@tencent.com jasoncjwang@tencent.com
Abstract Keywords
Co-speechgesturegenerationiscrucialforproducingsynchronized GestureGeneration,MotionProcessing,Data-DrivenAnimation
andrealistichumangesturesthataccompanyspeech,enhancingthe
ACMReferenceFormat:
animationoflifelikeavatarsinvirtualenvironments.Whilediffu-
ChencanFu,YabiaoWang,JiangningZhang,ZhengkaiJiang,XiaofengMao,
sionmodelshaveshownimpressivecapabilities,currentapproaches JiafuWu,WeijianCao,ChengjieWang,YanhaoGe,andYongLiu.2024.
oftenoverlookawiderangeofmodalitiesandtheirinteractions,re- MambaGesture:EnhancingCo-SpeechGestureGenerationwithMamba
sultinginlessdynamicandcontextuallyvariedgestures.Toaddress andDisentangledMulti-ModalityFusion.InProceedingsofthe32ndACM
thesechallenges,wepresentMambaGesture,anovelframework InternationalConferenceonMultimedia(MMâ€™24),October28â€“November
integratingaMamba-basedattentionblock,MambaAttn,with 1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,10pages.
amulti-modalityfeaturefusionmodule,SEAD.TheMambaAttn https://doi.org/10.1145/3664647.3680625
block combines the sequential data processing strengths of the
1 Introduction
Mambamodelwiththecontextualrichnessofattentionmecha-
nisms,enhancingthetemporalcoherenceofgeneratedgestures. Co-speechgesturegeneration,thetaskofproducinghumanges-
SEADadeptlyfusesaudio,text,style,andemotionmodalities,em- turessynchronizedwithaudioandothermodalities,iscrucialfor
ployingdisentanglementtodeepenthefusionprocessandyieldges- enhancingavatarrealisminanimation,film,andinteractivegaming.
tureswithgreaterrealismanddiversity.Ourapproach,rigorously Craftinggesturesthatarebothrealisticanddiverseisasignificant
evaluatedonthemulti-modalBEATdataset,demonstratessignifi- challengeandafocalpointincontemporaryresearch.
cantimprovementsinFrÃ©chetGestureDistance(FGD),diversity Extensiveresearchhasbeenconductedinthisarea,leadingto
scores,andbeatalignment,achievingstate-of-the-artperformance thedevelopmentofnumerousinnovativeapproaches.Gesturegen-
inco-speechgesturegeneration. erationtechniquesaregenerallydividedintorule-basedanddata-
drivenmethods,withthelatterfurthercategorizedintostatistical
CCSConcepts andlearning-basedapproaches.Thispaperfocusesonlearning-
â€¢Human-centeredcomputingâ†’Humancomputerinterac- basedco-speechgenerationmethods,whichcangenerallybedi-
tion(HCI);â€¢Computingmethodologiesâ†’Motionprocessing. videdintotwocategories.1)Autoencoder-basedmethods,which
employautoencoders(AEs)[27]orvariationalautoencoders(VAEs)
[26,48]totranslategesturegenerationintoareconstructiontask,
âˆ—Bothauthorscontributedequallytothisresearch.
asshowninFigure1.Despitetheircomputationalefficiency,these
â€ Correspondingauthors.
methodsarelimitedbytheirarchitecture,resultinginrestricted
gesturediversity.2)Diffusion-basedmethods,recognizedforpro-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
ducingdiversegestures,suchasDiffuseStyleGesture+[46],which
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation combinesdiffusionmodelswithTransformerencodersandinte-
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe gratesmultiplemodalitiestoenhancerealismanddiversity.How-
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
ever,thesemethodsoftenfailtofullyexploittherichinteractions
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. amongmulti-modaldata,leadingtolessexpressivegestures.
MMâ€™24,October28-November1,2024,Melbourne,Australia. DrawinginspirationfromthestatespacemodelMamba,effective
Â©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
insynthetictasks,languagemodeling,andaudiogeneration[10],
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3680625 werecognizeitspotentialforco-speechgesturegeneration.Mamba,
4202
luJ
92
]CH.sc[
1v67991.7042:viXraMMâ€™24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Diversity Realism ourgesturediffusionmodelenablestheproductionofgesturesthat
Audio
arebothrealisticanddiverse.Additionally,wepresentthecross- Text
Style Gesture attentionenhancedStyleandEmotionAwareDisentangled(SEAD)
Emotion AE-based featurefusionmodule.Thismoduleintroducesanoveltechnique
fordisentanglingaudiotoextractpersonalstyleandemotion.The
Audio combinationofMambaAttnâ€™sadvancedsequencemodelingwith
Text Gesture theSEADmoduleempowersourframeworktogenerateco-speech
Style
gesturesthatarebothdiverseandrealistic.
Quadratic Scaling
Ourprimarycontributionsareasfollows:
Diffusion with attention Diffusion-based
(1)WearethefirsttointroducetheMambamodeltothefield
Audio ofdiffusion-basedco-speechgesturegeneration.Mambaâ€™ssupe-
Text riorsequencemodelingcapabilitiesmakeitwell-suitedfortasks
Gesture
Style
requiringtemporalcoherenceanddynamicgesturerepresentation.
Emotion
Linear Scaling (2)WeintroducetheMambaAttnblock,whichenhancessequen-
Diffusion with SSM Ours tialmodeling,andtheSEADmodule,anovelaudiodisentanglement
approachthatfusesmulti-modaldata.TheSEADmoduleeffectively
Figure 1: Comparison of our approach with mainstream capturespersonalstyleandemotionfromspeechaudio,enriching
co-speechgenerationmethods.(a)Autoencoder(AE)-based themulti-modalinformationusedforgesturegenerationandlead-
methods[26,27]synthesizegesturesbyfusingmulti-modal ingtomorerealisticanddiversegestures.
databutinherentlysufferfromlimiteddiversityduetoar- (3)Ourextensiveexperimentalevaluationonthelargemulti-
chitecturalconstraints.(b)Diffusion-basedmethods[44,46] modalBEATdatasetconfirmsthatMambaGestureachievesstate-of-
employdiffusionmodelswithTransformerstogeneratedi- the-artperformanceinco-speechgesturegeneration.Ourmethod
versegesturesbutarehinderedbythequadraticcomplexity outperformsexistingmodelsonseveralkeymetrics,highlighting
ofTransformerandoftenoverlookintricatemulti-modalcor- theeffectivenessofourcontributions.
relations.(c)OurMambaGestureleveragesthelinearscaling
andsequentialdataprocessingadvantagesoftheStateSpace
2 RelatedWork
Modeltoenhancegesturediversityandeffectivelyharness
multi-modaldatawithdisentangledfeaturefusion,ensuring 2.1 Co-speechGestureGeneration
abroaderspectrumandhigherrealismingesturegeneration. Co-speech gesture generation involves producing gestures syn-
chronizedwithspeechaudio,achallengingtaskduetothelackof
explicitmappingsbetweenspeechandgestures.
anevolutionofRNNs,overcomestheirlimitationsinparallelcom- Gesturegenerationmethodologiescanbebroadlycategorized
putationthroughaparallelscanalgorithmandboastslinearscaling, intorule-basedanddata-drivenapproaches.Thelatterisfurther
contrastingwiththequadraticcomplexityoftraditionaltransform- subdividedintostatisticalandlearning-basedmethods[32].Rule-
ers.OurexplorationintoMambaâ€™sapplicationinco-speechgesture based methods [4, 19, 20, 29, 35, 40] are known for generating
generationrevealsitscapacitytoproducegesturesthatarebothre- high-qualitymotionsbutlacktheflexibilityanddiversityofdata-
alisticanddiverse.Experimentally,wefindthatcombiningMambaâ€™s drivensystems.Statisticalsystems[3,8,17,22,31,47]modelges-
sequentialmodelingstrengthswiththecontextualawarenessofat- ture distribution by analyzing the statistics rather than relying
tentionmechanismsyieldsthebestresults.Weproposeintegrating onexpert-encodedrules,typicallyinvolvingpre-computingcondi-
multiplemodalities(audio,text,style,andemotion)tosignificantly tionalprobabilitiesorassigningpriorprobabilitydistributions.
elevatetherealismanddiversityofgeneratedgestures.Whileau- Recently,learning-basedmethodsusingCNNs,RNNs,andtrans-
diolaysthefoundationforgesturecues,addingstyleandemotion formershavegainedtraction.Liuetal.[28]proposeahierarchical
enrichesthegesturaloutput,capturingindividualexpressionsand approachforgesturegeneration,consideringthehierarchicalna-
emotionalsubtleties.Previousresearchhasoftenfocusedonaudio tureofspeechsemanticsandhumangestures.Yoonetal.[49]treat
astheprimaryinput,neglectingthedepthofinformationfrom gesturegenerationasatranslationproblem,employingarecurrent
othermodalities.Wearguethataudiocontainsrichdetailsthat, neuralnetworkthatutilizesmulti-modalcontextsofspeechtext,
wheneffectivelydisentangledandcombinedwithtext,style,and audio,andspeakeridentity,incorporatinganadversarialscheme
emotion,cangreatlyrefinegesturesynthesis.Ourapproachaimsto toenhancerealism.Liuetal.[27]introducetheBEATdataset,fea-
fullyexploitthisrichmulti-modaldata,withaparticularemphasis turinggestures,facialexpressions,audio,text,emotions,speaker
onaudiodisentanglement,toachievemorenuancedandcontextu- identityandsemantics.TheirCascadedMotionNetwork(CaMN)
alizedco-speechgesturegeneration.Theconceptofourapproach synthesizes body and hand gestures using adversarial training
isillustratedinFigure1. acrossmultiplemodalities.DisCO[25]disentanglesmotioninto
Inthiswork,weintroduceMambaGesture,anovelframework implicitcontentandrhythmfeatures,feedingtheprocessedfea-
thatintegratesaMamba-basedattentionblock,MambaAttn,with turesintoamotiondecodertosynthesizegestures.EMAGE[26]
amulti-modalityfeaturefusionmodule,SEAD.TheMambaAttn employsMaskedGestureReconstruction(MG2G)toencodebody
block,depictedinFigure2,leveragestherobustsequencemodeling hintsandAudio-ConditionedGestureGeneration(A2G)todecode
capabilitiesofthestatespacemodelMamba.Thisintegrationwithin pre-trainedfaceandbodylatentfeatures,generatingfacialand
erutaeF
delgnatnesiD
noisuF
noisuF
erutaeF noisuF redcoeD
redcoeD
redcoeDMambaGesture MMâ€™24,October28-November1,2024,Melbourne,Australia.
localbodymotionsusingapre-trainedVQ-Decoder.Yietal.[48] employstatevariablestomodeldynamicsystems,makingthem
introduceTalkSHOW,whichutilizesanautoencoderforfacialgen- foundationalinfieldslikecontroltheoryandrobotics.
erationandaVQ-VAEforbodyandhandmotiongeneration,with Thecorestatespacemodelisrepresentedbytheequations:
anautoregressivemodelpredictingthemultinomialdistributionof ğ‘¥â€²(ğ‘¡)=Ağ‘¥(ğ‘¡)+Bğ‘¢(ğ‘¡) (1)
futuremotionduringinference.
ğ‘¦(ğ‘¡)=Cğ‘¥(ğ‘¡)+Dğ‘¢(ğ‘¡), (2)
Thesemethodsoftentreatco-speechgesturegenerationasa
reconstructiontask,facingchallengesinestablishingmappingsbe- whereğ‘¥(ğ‘¡)isanN-dimensionallatentstate,ğ‘¦(ğ‘¡)istheoutput,ğ‘¢(ğ‘¡)
tweenspeechandgestures,resultinginlimiteddiversity.However, istheinput,andA,B,C,Daresystemparameters,withDğ‘¢often
recentadvancementsindiffusion-basedmethodsofferapromising actingasaskipconnection.
alternative,producinggestureswithhighrealismanddiversity. TheStructuredStateSpaceSequencemodel(S4),introducedby
Guetal.[11],buildsonSSMstoachievegenerativemodelingatscale
andfastautoregressivegeneration.S4usestheHiPPOmatrixto
2.2 Diffusion-basedGestureGeneration
constructparameterğ´,mitigatingthechallengeofgradientscaling
Diffusionmodels,knownforcomplexdatadistributionmodeling withsequencelength.Despitetheiradvantages,SSMsâ€™constant
andmany-to-manymappings,aregainingpopularityforgesture parameterscanlimittheiradaptabilityandcontentawareness.
synthesis.Severalworkshaveusedtextasaconditionfordiffusion Mamba[10]revolutionizessequencemodelingbymaintaining
models to generate human motion, such as MotionDiffuse [50], linearcomplexity,akintostatespacemodels(SSMs),whilerivaling
FLAME[16],andMDM[38].Recentresearchfocusesongenerat- Transformersâ€™capabilities.Itachievesthisthroughaninnovative
ingco-speechgesturesusingdiffusionmodels.Alexanderson[1] input-dependentselectionmechanismthatdynamicallyadjustspa-
adaptstheDiffWavearchitecture[18],replacingdilatedconvolu- rametersbasedontheinput,significantlyenhancingthemodelâ€™s
tionswithTransformersorConformerstoenhanceperformance. contentsensitivity. Additionally,Mambaincorporatescomputa-
Zhuetal.[53]introduceDiffGesture,whichconcatenatesnoisyges- tionalstrategiessuchaskernelfusion,parallelscan,andrecom-
turesequenceswithcontextualinformationinthefeaturechannel putation,whichcollectivelystreamlinethecomputationalprocess.
fortemporalmodelingusingtransformers.DiffuseStyleGesture+ ThesefeatureshavespurredthecreationofMamba-basedapplica-
[46] conditions on audio, text, style, and seed gesture, employ- tions,includingMambaTalk[42],whichreplacestraditionalatten-
inganattention-basedarchitecturefordenoising.UnifiedGesture tionmechanismswithMambablocksforefficientandhigh-quality
[43]utilizesaretargetingnetworktostandardizeprimalskeletons gesturegeneration,andMotionMamba[51],whichleveragesa
from various datasets, expanding the data pool and employing U-Netarchitecturewithhierarchicaltemporalandbidirectional
VQ-VAEandreinforcementlearningforgesturegenerationrefine- spatialMambablocksforadvanceddenoisingcapabilities,further
ment. LivelySpeaker [52] emphasizes the importance of seman- augmentedbyaCLIPtextencoderforinputconditioning.Thede-
ticsingestureunderstandingandadoptsatwo-stagestrategyfor velopmentoftheseapplicationsunderscoresMambaâ€™sadaptability
semantic-awareandrhythm-awaregesturegeneration.AMUSE[7] anditsburgeoningroleinenhancingmotiongenerationtasks.
disentanglesspeechintocontent,emotion,andpersonalstylela-
tentrepresentations,usingamotionVAEtransformerarchitecture 3 Preliminary
fortheconditionaldenoisingprocessinlatentspace.FreeTalker
3.1 HumanGestureDataFormat
[45] generates spontaneous co-speech gestures from audio and
performstext-guidednon-spontaneousgesturegeneration.Ges- Human gestures are predominantly represented using rotation-
tureDiffuCLIP[2]processestext,motion,andvideopromptswith basedformats,withjointrotationstypicallyexpressedinSO(3).
differentCLIPmodelstoachievestylecontrol.DiffSHEG[5]con- Thesecanbeparameterizedthroughvariousmethods,including
sidersexpressionsascuesforgestures,achievingreal-timejoint Eulerangles,axisangles,andquaternions[54].
generationofexpressionsandco-speechgestures. In this paper, we utilize the BEAT dataset [27], noted for its
However,mostexistingapproachesdonotconsiderfullmodali- extensive duration and diverse modalities. The motion capture
ties,nordotheyofferacomprehensiveanalysisoftheinteractions dataintheBEATdatasetisstoredinBVHfileformat,withmotion
betweenthesemodalities,potentiallyleadingtolessdiverseand
representedviaEulerangles:75Ã—3rotations+1Ã—3roottranslation.
realisticgeneratedgestures.OurproposedMambaGestureintro- Thisdatasetincludes27bodyjointsand48handjoints.Consistent
ducesanovelcross-attentionenhancedStyleandEmotionAware withthemethodologiesemployedbyDiffuseStyleGesture+[44],
Disentangled(SEAD)featurefusionmodule,whichcleverlydisen- wepreferrotationmatricesoverEuleranglesforjointrotations
tanglesstyleandemotionfromaudioinputsandintegratesrich toenhancetherobustnessandaccuracyofourgesturegeneration
multi-modalconditions(audio,text,style,andemotion)tofacilitate model.Consequently,wetreattheroottranslationasasinglejoint,
thegenerationofgesturesthatarebothrealisticanddiverse. resultinginatotalof76joints.Weuseall76jointsforwhole-body
gesturegenerationandselect14upper-bodyjoints,alongwiththe
48handjoints,forupper-bodygestures.
2.3 StateSpaceModels
3.2 DenoisingDiffusionProbabilisticModel
StateSpaceModels(SSMs)[9,11,12,15,33,37]haveregainedpop-
ularityinsequencemodelingtasksduetotheirlinearornear-linear Denoisingdiffusionprobabilisticmodels(DDPMs)aregenerative
scalingwithsequencelength,outperformingattentionmechanisms modelsdesignedtoapproximatereal-worlddatadistributions,de-
withquadraticscaling.OriginatingfromRNNsandCNNs,SSMs noted asğ‘(x0). Introduced by Ho et al. [14], DDPMs employ aMMâ€™24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
forwarddiffusionprocessthatincrementallyaddsGaussiannoise yield ğ‘“ ğ‘¡ğ‘’ğ‘¥ğ‘¡ and ğ‘“ ğ‘ ,respectively.Emotionfeaturesareencodedto
todata,transitioningittowardsanoisedistribution,andareverse produceğ‘“ ğ‘’,withbothstyleandemotionfeaturessubjecttorandom
processthatreconstructstheoriginaldatafromthenoise. maskingduringtraining.
TheforwarddiffusionisaMarkovchaindescribedas: Thepreliminaryfeatureprocessingofthemulti-modalitydata
ğ‘‡ iscompletedbeforetraining.Thesepreprocessedfeaturesarethen
(cid:214)
ğ‘(ğ’™1:ğ‘‡|ğ’™0)= ğ‘(ğ’™ğ‘¡|ğ’™ğ‘¡âˆ’1), (3) utilizedinthesubsequentfusionmodule.
ğ‘¡=1 Previousworkshaveexploredmulti-modalityfusionforinput
ğ‘(ğ’™ğ‘¡|ğ’™ğ‘¡âˆ’1)=N(ğ’™ğ‘¡;âˆšï¸ 1âˆ’ğ›½ ğ‘¡ğ’™ğ‘¡âˆ’1,ğ›½ ğ‘¡ğ‘°), (4) conditions[27,44].Theyperformfusioninacascadedwayorlever-
agethepowerfulmodelingcapabilitiesofattentionmechanism[39].
whereğ›½ ğ‘¡ increasesovertime,makingthedataresembleGaussian However,incurrentdiffusion-basedgesturegenerationmethods,
noiseN(0,ğ‘°).
theprocessofmulti-modalityfeaturefusionisoftencomplexand
Thereverseprocessreconstructsthedatadistributionğ‘ ğœƒ(ğ’™0:ğ‘‡):
lacksclarity.WefirstintroducetheStyleAware(SA)featurefu-
ğ‘‡ sionmodule,whichsimplyusesaudiofeatureğ‘“ ğ‘,textfeatureğ‘“ ğ‘¡ğ‘’ğ‘¥ğ‘¡
(cid:214)
ğ‘ ğœƒ(ğ’™0:ğ‘‡)=ğ‘ ğœƒ(ğ’™ğ‘‡) ğ‘ ğœƒ(ğ’™ğ‘¡âˆ’1|ğ’™ğ‘¡), (5) andstylefeatureğ‘“ ğ‘  asconditions,andconcatenatesthemwiththe
ğ‘¡=1 noisygesturefeatureğ‘“ ğ‘”andtimefeatureğ‘“ ğ‘¡.Afterconcatenatingall
ğ‘ ğœƒ(ğ’™ğ‘¡âˆ’1|ğ’™ğ‘¡)=N(ğ’™ğ‘¡âˆ’1;ğœ‡ ğœƒ(ğ’™ğ‘¡,ğ‘¡),Î£ ğœƒ(ğ’™ğ‘¡,ğ‘¡)), (6) modalityfeatures,weemployacross-localattention[36]moduleto
capturelocalinformationwithintheconcatenatedfeature,resulting
withÎ£ ğœƒ(ğ’™ğ‘¡,ğ‘¡) asatime-dependentconstant.Themodelapprox-
inthefusedmulti-modalityfeatureğ‘“ ğ‘“ğ‘¢ğ‘ ğ‘’.
imatesthemeanoftheGaussiandistributionduringthereverse
Moreover,manystudiesoverlooktheroleofemotion.However,
process.
emotioncansignificantlyinfluenceourgestureswhenwespeak.
Ourapproachdivergesfrompredictingnoiseateachstepğ‘¡.In-
Toaddressthis,weproposetheStyleandEmotionAware(SEA)
stead,wepredictthecleandatasampleğ’™0directly,followingrecent
featurefusionmodule,buildinguponSA,whichintroducesemotion
methodologies[34,38,44],toenhancethegenerativemodelâ€™seffi-
asagenerationcondition.Theemotionfeatureğ‘“ ğ‘’ isfusedthrough
ciencyandaccuracy.
concatenationinthesameway.Byutilizingaudio,text,styleand
emotion,thecomprehensiveconditionsprovideamorespecific
4 Method
descriptionandcommand,whichbenefitsourgesturegeneration.
Ourapproachisstructuredaroundtwopivotalcomponents:the SEADFusion.Existingworksonmultimodalfusionoftenfail
cross-attentionenhancedStyleandEmotionAwareDisentangled tofullyexploittheinherentrelationshipsbetweenmodalities.Our
(SEAD)featurefusionmoduleandtheMambaAttn-baseddenois- StyleandEmotionAwareDisentangled(SEAD-basic)module,an
ingnetwork.Atthecoreofourmotiongenerationliesthediffusion extensionoftheSEAmodule,disentanglesstyleandemotionfrom
modelframework,whichemploysaniterativeprocessofdiffusion audioinputusingself-supervisedlearning.Itavoidsthecomplexity
(addingnoise)anddenoisingtoreconstructoriginalgesturesfrom oftwo-stagedisentanglementmethods.Thismoduleconsistsof
anoisedistribution,conditionedonaudioandadditionalmulti- threeindependentunitsthatextractstyleandemotionfeatures
modaldatainputs.TheSEADmoduleistaskedwiththeintricate fromtheaudiofeatureğ‘“ ğ‘asfollows:
f inu gsio nn eto wf om rku il sti- dm edo id ca al ti et dy td oa tt ha, ew ach cil ue rath tee pM rea dm icb ta ioA ntt on f- gba ess te ud red se .n To his e- ğ‘“ ğ‘ğ‘  =Linear(ğ‘“ ğ‘)
overviewofourproposedMambaGestureisillustratedinFigure2. ğ‘“ ğ‘ğ‘’ =Linear(ğ‘“ ğ‘) (7)
ğ‘”
ğ‘“
ğ‘
=Linear(ğ‘“ ğ‘),
4.1 DisentangledMulti-ModalFusion
wherelinearlayersareusedtoextractstyleandemotionfeatures
Ourmethodologyintroducesaprogressiveseriesofmulti-modality fromğ‘“ ğ‘.Theextractedğ‘“ ğ‘ğ‘  andğ‘“ ğ‘ğ‘’ arealignedwiththecorrespond-
featurefusiontechniques.Thesemethodsincrementallyintegrate
ingstyleandemotionfeaturesusingstyleloss Lğ‘  andemotion
featuresfromvariousmodalities,evolvingfromsimpleconcatena-
lossLğ‘’ tofacilitatetheseparationofpersonalstyleandemotion
tiontosophisticated,disentangledfusion.
informationfromtheaudio.
SAandSEAFusion.Adoptingthestate-of-the-artDiffuseS- Afterdisentanglingtheaudiostylefeatureğ‘“ ğ‘ğ‘  andaudioemo-
tyleGesture+(DSG+)[44,46]asourbaseline,werefinethegesture tionfeatureğ‘“ ğ‘ğ‘’ ,wefusethemwiththeoriginalstyleandemotion
g tie mn ee sr ta et pio ğ‘¡n ,p nr oo isc yes gs eb sy tuc ro en ğ‘¥d ğ‘¡i ,ti ao nn din cg onon dia tiose nt so ğ‘f .m Ino cd oa nli tt rie as s, ti tn ocl Du Sd Gin +g
,
featurestoobtainenhancedrepresentationsğ‘“ ğ‘ â„ andğ‘“ ğ‘’â„ :
whichemploysaudioğ‘,textğ‘¡ğ‘’ğ‘¥ğ‘¡,styleğ‘ ,andseedgestureğ‘‘,our ğ‘“ ğ‘ â„ =Linear(Cat(ğ‘“ ğ‘ğ‘ ,ğ‘“ ğ‘ )) (8)
approachintroducesemotionğ‘’ asaconditionwhiledispensing ğ‘“ ğ‘’â„ =Linear(Cat(ğ‘“ ğ‘ğ‘’,ğ‘“ ğ‘’)), (9)
withtheseedgesture.Thisdecisionisinformedbytherecognition
thatemotionplaysacriticalroleinthenaturalvariationofgestures. whereLineardenotesalinearlayer,andCatdenotesconcatenation.
Weprocesseachmodalitythroughaseriesofstepstopreparethe Bycombiningthemwiththeoriginalfeatures,wedecouplestyle
featuresforfusion.Thetimestepisencodedthroughpositionencod- andemotionfromtheoriginalaudioandenhancethem.There-
ğ‘”
ingandanMLPtoproducethetimefeatureğ‘“ ğ‘¡.Thenoisygesture mainingfeatureğ‘“ ğ‘ isdirectlyrelatedtothegesture.Theremaining
ğ‘¥ ğ‘¡ isencodedbyalinearlayertoobtainthenoisygesturefeatureğ‘“ ğ‘”. processofSEAD-basicisthesameasSEA,whereweconcatenate
Audiofeaturesareextractedandenrichedwithpretrainedmodels ğ‘“ ğ‘ğ‘” , ğ‘“ ğ‘ â„ , ğ‘“ ğ‘’â„ , ğ‘“ ğ‘¡ğ‘’ğ‘¥ğ‘¡ with ğ‘“ ğ‘¡ and ğ‘“ ğ‘”,andusecross-localattentionto
toformğ‘“ ğ‘,whiletextandstylefeaturesaresimilarlyprocessedto obtainthefusedfeatureğ‘“ ğ‘“ğ‘¢ğ‘ ğ‘’.MambaGesture MMâ€™24,October28-November1,2024,Melbourne,Australia.
LayerNorm
Linear Denoising
Self-Attention
Diffusion
Linear C Linear Linear Linear
Conv
Linear q Cross + Denoising
Attention
k, v SSM
Linear C Linear Diffusion x
... Linear
Mamba
Denoising
Linear
LayerNorm
Style and Emotion Aware Disentangled Feature Fusion Module
Sampling MambaAttn Block
Figure2:OverviewofourproposedMambaGesture.Weintroduceanovelfeaturefusionstrategy:thecross-attentionenhanced
StyleandEmotionAwareDisentangled(SEAD)featurefusionmodule.Thismoduleemploysstyleğ’”,audioğ’‚,emotionğ’†,and
textğ’•ğ’†ğ’™ğ’• asconditionstoprovidecomprehensiveinformationandeffectivelydisentanglestyleandemotionfromtheaudio.
Theğ’‡â€² isobtainedbyconcatenatingğ’‡â€² andğ’‡â€²,andprojectedtooriginaldimensionbylinearlayer.Besides,wepresenta
ğ’”ğ’† ğ’” ğ’†
Mamba-basedcomponenttermedtheMambaAttnblock,whichmergesMambawithitssequencemodelingproficiencyand
employsanattentionmechanismtolearnglobalinformation.Ourdenoisingarchitecture,MambaAttndenoiser,iscomposed
ofastackofMambaAttnblocksandalinearlayer.Duringthesamplingphase,wepredictthegestureğ’™Ë†0byapplyingthefused
conditionswithinacyclicaldenoisinganddiffusionprocedure.
Building on the SEAD-basic module, the cross-attention en- MambaAttnBlock.TheMambaAttnblockisanovelarchi-
hancedStyleandEmotionAwareDisentangled(SEAD)feature tecturalcomponentthatcombinesthesequentialdataprocessing
ğ‘”
fusionmodulefurtherenhancestheaudiofeatureğ‘“ ğ‘ withthefused strengthsoftheMambamodelwiththecontextualawarenessof
styleandemotionfeatureğ‘“ ğ‘ â€² ğ‘’ usingacross-attentionmechanism: theattentionmechanism.Thisfusioncreatesapowerfultoolfor
capturingtheintricaciesofco-speechgesturegeneration.
ğ‘„ğ¾ğ‘‡
ğ‘ğ‘¡ğ‘¡ğ‘›=Attention(ğ‘„,ğ¾,ğ‘‰)=softmax( âˆš )ğ‘‰, (10) ThestructureoftheMambaAttnblock,asshowninFigure2,
ğ‘‘ includesaself-attentionlayer,aMambablock,andtwoinstances
ğ‘” oflayernormalization.Theprocessbeginswiththefusedfeature
w styh le ere anğ‘„ dere mp or te is oe nn ft es at th ue refe ğ‘“ ğ‘ a â€² ğ‘’t ,u ar ne dğ‘“ ğ‘ ğ‘‘, isğ¾ tha end diğ‘‰ mer ne sp ir oe nse on ft tht ehe fef au ts ue rd e. ağ‘“ ğ‘“ ttğ‘¢ eğ‘  nğ‘’ tu ion nd mer og do uin leg :layernormalization,whichthenenterstheself-
Thefusedstyleandemotionfeatureğ‘“ ğ‘ â€² ğ‘’ isobtainedasfollows:
ğ‘“ ğ‘ â€² ğ‘’ =Linear(Cat(ğ‘“ ğ‘ â€²,ğ‘“ ğ‘’â€²)). (11) ğ´ğ‘¡ğ‘¡ğ‘›=Attention(ğ‘„,ğ¾,ğ‘‰)) (12)
TheSEADmodulerepres ğ‘”entsthepinnacleofourfusionapproach, whereğ‘„,ğ¾,ğ‘‰ = LN(ğ‘“ ğ‘“ğ‘¢ğ‘ ğ‘’),andLNdenoteslayernormalization.
wheretheaudiofeatureğ‘“ ğ‘ isfurtherrefinedthroughcross-attention Theself-attentionmoduledistillsglobalcontextualinformation,
withfusedstyleandemotionfeature ğ‘“ ğ‘ â€² ğ‘’.Thismechanismeffec- whichisthenrefinedbytheMambablocktoenhancesequence
tivelyintegratestheaudiowiththestyleandemotion,enhancing modeling.Theoutputissubsequentlyprocessedasfollows:
therepresentationalcapabilityoftheaudiofeatureandresultingin
acomprehensivefusedmulti-modalityfeatureğ‘“ ğ‘“ğ‘¢ğ‘ ğ‘’. ğ‘¥Ë†0=LN(Mamba(ğ´ğ‘¡ğ‘¡ğ‘›)). (13)
4.2 MambaAttnDenoiser
ThefinaloutputoftheMambaAttnblocksisthenprojectedbackto
Afterintegratingthemulti-modalconditionsintoacomprehensive theoriginaldimensionofthegesturedatathroughthelinearlayer,
featurerepresentation ğ‘“ ğ‘“ğ‘¢ğ‘ ğ‘’,wemovetothegestureprediction yieldingthepredictedgestureğ‘¥Ë†0.
phase.Here,weemployourinnovativeMambaAttn-basednetwork, TrainingandSampling.Totrainournetworks,weusethe
MambaAttnDenoiser,topredictgesturesğ‘¥Ë†0fromthenoisygesture HuberlossfunctionastheprimarymetricforgesturelossLğ‘”:
inputğ‘¥ ğ‘¡.TheMambaAttnDenoisercomprises8MambaAttnblocks
andalinearprojectionlayer. Lğ‘” =ğ¸ ğ‘¥ 0âˆˆğ‘(ğ‘¥ 0|ğ‘),ğ‘¡âˆ¼[1,ğ‘‡][HuberLoss(ğ‘¥ 0âˆ’ğ‘¥Ë†0)]. (14)
etanetacnoC
noitnettAMMâ€™24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Forthestyleandemotionlosses,Lğ‘  andLğ‘’,weusetheL1Loss, whicharethenprocessedthroughalinearlayertoobtainthetext
formulatedas: featureğ‘“ ğ‘¡ğ‘’ğ‘¥ğ‘¡.Personalstyleğ‘ andemotionğ‘’areencodedasone-hot
Lğ‘  =|ğ‘“ ğ‘ğ‘  âˆ’ğ‘“ ğ‘ | vectorsandtransformedintocorrespondingfeaturesğ‘“ ğ‘  andğ‘“ ğ‘’ via
Lğ‘’ =|ğ‘“ ğ‘ğ‘’ âˆ’ğ‘“ ğ‘’|. (15) linearlayers.Thediffusionprocessissetto1000steps.Wetraineach
modelfor40,000stepswithabatchsizeof400,usingtheAdamW
The final loss function is a composite of the gesture, style, and
optimizerwithalearningrateof3Ã—10âˆ’5.Allexperimentsare
emotionlosses,weightedandsummedasfollows:
conductedonasingleNVIDIAH800GPU,ensuringreproducibility
L=Lğ‘”+ğ›¼Lğ‘  +ğ›½Lğ‘’, (16) onstandardhardwareconfigurations.
wherewesetğ›¼ =1andğ›½ =1forsimplicity. Wecompareourproposedmodelwithstate-of-the-artgesture
generationmethods,includingDiffuseStyleGesture+(DSG+)[44],
FollowingtheDDPMdenoisingparadigm,weiterativelypredict
thegestureğ‘¥Ë†0ateachtimestepğ‘¡,asillustratedinFigure2,refining CaMN[27],andMDM[38].Weretrainthesemethodsonourpar-
titioneddatasettoensureafaircomparison.CaMNisretrained
ourmodelâ€™sabilitytogenerateaccurateandlifelikegestures.
withaudio,text,emotion,andspeakeridentityasconditions,while
5 Experiments DSG+employsseedgestures,audio,andspeakeridentity.MDMis
conditionedsolelyonaudiofeatures.Ourevaluationincludesboth
5.1 ExperimentSettings
whole-bodyandupper-bodygesturegeneration.
Dataset.WeevaluateourmethodusingtheBEATdataset[27],
whichincludeshumanmotionscapturedat120Hzviaamotion 5.2 QuantitativeResults.
capturesystem.Thisdatasetfeaturesextendedconversationaudios
Theresults,summarizedinTable1,demonstratethatourMam-
(approximately10minuteseach)andbriefself-talkaudios(around
baGestureframeworkachievesthelowestFGDScoreforbothupper
1minuteeach)from30diversespeakers.Itismulti-modal,offering
bodyandwholebodygesturegeneration,indicatingahighsimilar-
motion,audio,text,style(identity),emotion,andfacialexpression
itybetweenthegeneratedgesturesandtherealdatadistribution.
annotations.Thedatasetcovers8emotions:neutral,anger,happi-
Specifically,ourwholebodygesturegenerationrecordsanFGD
ness,fear,disgust,sadness,contempt,andsurprise.Following[41],
Scoreof22.11,asignificantimprovementoverMDMâ€™s106.56and
weselectonehourofaudioperspeaker,splittingthedatainto70%
DSG+â€™s103.15.Additionally,ourmethodexcelsinDiversityScore
fortraining,10%forvalidation,and10%fortesting.Wegenerate
andL1DiversityScore,withupper-bodygesturesscoring374.08
bothupperbodyandwholebodygesturesforcomparisonwith
and875.06,respectively,andwhole-bodygesturesscoring434.94
state-of-the-artmethods.Inablationstudies,wefocusonwhole
and1128.79,respectively.Thesescoresunderscoretheenhanced
bodygesturegeneration.
diversityofourgeneratedgestures.Notably,diffusion-basedmeth-
Evaluation Metrics. We use multiple metrics to rigorously
odssuchasMDMandDSG+exhibitbetterdiversitythanCaMN,
assess the quality and diversity of the generated gestures. The
whichreliesonautoencodersforgesturegeneration.Althoughour
FrÃ©chetGestureDistance(FGD)[49]measurestheFrÃ©chetdistance
methoddoesnotachievethehighestSRGRScores,itremainscom-
betweenthefeaturedistributionsofrealandsynthesizedgestures,
petitivewithCaMN.OurMambaGestureexcelsinBeatAlignfor
similartotheFID[13]usedinimagegeneration.Thegesturefeature
bothupperandwhole-bodygestures,reflectingamoresynchro-
extractor,trainedunsupervisedlywithareconstructionlossusing
nizedaudio-gesturealignment.
L1loss,servesasthebasisforthiscomparison.
Toquantifygesturediversity,weusetheDiversityScore[21]and
5.3 QualitativeResults.
L1DiversityScore[23].TheDiversityScoremeasurestheaverage
featuredistancebyrandomlyselecting500featuresandcomputing UserStudy.Weconductauserstudytosubjectivelyevaluateour
theaverageL1distancebetweenthem. proposedmethodagainststate-of-the-artmethods.Fifteenpartici-
WealsoincorporateSemantic-RelevantGestureRecall(SRGR) pantswereaskedtoevaluate30gesturesamples,eachcontaining
[27]andBeatAlign[24]toevaluatethesemanticrelevanceand gesturesgeneratedbyfourdifferentmethodsusingthesamecor-
synchronyofthegeneratedgestures.SRGR,anevolutionofthe respondingaudio.Theywerethenaskedtoselectthebestgesture
ProbabilityofCorrectKeypoint(PCK),measuresthesemanticrele- foreachofthefollowingcriteria:motionnaturalness,smoothness,
vanceofgesturestotheaccompanyingspeech.BeatAlignassesses diversity, and semantic preservation. The preferred method for
thetemporalalignmentbetweenaudioandgesturebeatsusing eachcriterionwasdeterminedbasedonthenumberofselections
ChamferDistance,providinginsightintotherhythmicharmonyof received,andtheresultsarepresentedaspercentagesinTable2.
thegeneratedgestureswiththespokencontent. VisualizationResults.Figure3visualizestheexperimental
ImplementationDetails.Wedownsamplemotiondatafrom resultsforthespeechtranscript"...whenyouhavetoworkMonday
120Hzto30Hzandsegmentitinto300-frameclips(10seconds throughFridaythewholeweek,youareverytired...",asentence
each).Audiodataisdownsampledto16kHzfromahighersampling thatshouldelicitrichbodymovements.Visualcomparisonshows
rate.Wecomputeacomprehensivesetofaudiofeatures,including thatgesturesgeneratedbyCaMNandDSG+exhibitlimitedmo-
MFCCs, Mel spectrogram features, prosodic features, and pitch tion,whileourapproachdemonstratesrichmotion,highlighting
onsetpoints(onsets).Thesefeaturesareconcatenatedwiththose itseffectiveness.
extractedbythepretrainedWavLMLargemodel[6]toformarich Bothnumericalandvisualresultscorroboratethatourmethod
audiofeaturesetğ‘“ ğ‘.Fortextdata,weemploythepretrainedfastText generatesrealisticanddiverseco-speechgestures,advancingthe
model[30]toextractwordvectorsfromthespeechtranscripts, state-of-the-artinthefield.MambaGesture MMâ€™24,October28-November1,2024,Melbourne,Australia.
Table1:QuantitativecomparisonofourproposedmethodwithcurrentleadingapproachesontheBEATdataset.Boldindicates
thetop-performingmethodandunderlinesignifiesthesecond-bestperformanceacrossvariousevaluationcriteria.
Method FGDScoreâ†“ DiversityScoreâ†‘ L1DivScoreâ†‘ SRGRScoreâ†‘ BeatAlignâ†‘
GT - 403.27 754.75 - 0.894
CaMN[27] 60.67 295.62 519.53 0.216 0.823
UpperBody MDM[38] 54.13 327.82 821.18 0.208 0.823
DSG+[46] 60.50 358.62 748.42 0.213 0.850
Ours 32.45 374.08 875.06 0.213 0.863
GT - 395.20 850.51 - 0.893
CaMN[27] 65.74 277.06 587.12 0.241 0.819
WholeBody MDM[38] 106.56 331.53 1001.52 0.229 0.810
DSG+[46] 103.15 352.31 789.83 0.238 0.841
Ours 22.11 434.94 1128.79 0.237 0.853
Table2:UserStudyResults gesturegeneration.TheintegrationofourSEAmodule,whichintro-
ducesemotionasanovelcondition,leadstoanadditionalreduction
Method Natural Smooth Diversity Semantic intheFGDScore,withothermetricsshowingslightvariations.
CaMN[27] 9.78% 8.00% 3.11% 5.78% Theculminationofourfusionapproach,theSEADmodule,which
MDM[38] 7.78% 9.11% 2.67% 5.56% disentanglesstyleandemotionfromaudiofeaturesandenhances
DSG+[46] 21.33% 22.22% 38.89% 21.56% themthroughcross-attention,furtherdecreasestheFGDScoreand
Ours 61.11% 60.67% 55.33% 67.11%
notablyincreasestheDiversityScoreandL1DiversityScore.These
resultsrobustlyvalidatetheefficacyofourproposedmethod.
OptimalNumberofLayersinMambaAttnDenoiser.An
additionalablationstudyexaminestheoptimalnumberoflayersin
theMambaAttndenoiser.Testingconfigurationsof1,2,4,8,and
12layers,wefindthatan8-layerMambaAttndenoiseryieldsthe
bestperformanceacrosskeyevaluationmetrics,includingtheFGD
Score,DiversityScore,andL1DiversityScore.Notably,increasing
thenumberoflayersto12doesnotconferadditionalbenefitsand
insteadleadstodecreasedperformance.Thus,weselectan8-layer
configurationfortheMambaAttndenoiserinourfinalmodel,as
demonstratedbytheupperpartofTable4.Thisfindingunderscores
theimportanceofbalancingmodelcomplexitywithperformance,
asoverlycomplexmodelsmaysufferfromdiminishingreturnsor
Figure 3: Visualization results comparing state-of-the-art evenperformancedegradation.
methods.Speechtranscript:"...whenyouhavetoworkMonday DesignChoicesforMambaAttnBlock.Furtherexperimen-
throughFridaythewholeweek,youareverytired..." tationisconductedtorefinethearchitecturaldesignoftheMam-
baAttnblock.Initially,weconsiderincorporatingaconvolutional
layertocapturelocalinformation,withself-attentiontocapture
5.4 AblationStudies globalinformation,whilerelyingonMambaforsequentialmodel-
Torigorouslyevaluatethecontributionsofourproposedmethodâ€™s ing.Ourexperimentswithvariouscombinationsofthesemodules,
components,weconductaseriesofablationstudies,systematically asshowninthelowerpartofTable4,indicatethattheinclusion
isolatingandanalyzingtheimpactofeachmodule. ofaconvolutionallayerdoesnotenhanceperformance.Removing
EffectivenessofProposedComponents.WeestablishDiffus- eithertheself-attentionorMambacomponentsfromtheblockre-
eStyleGesture+(DSG+)asourbaselinemodelandincrementally sultedinasignificantdecreaseintheFGDScore,withtheremoval
integrateournovelcomponents:SEA,SEADandtheMambaAttnde- ofMambaleadingtoamorepronounceddropinDiversityScore
noiser.Theresults,detailedinTable3,showthatreplacingDSG+â€™s andL1DiversityScore.Especiallywhenweremoveself-attention,
transformerencoderwithourMambaAttndenoisersignificantly withonlyMambaandlayernormsinourblocks,itcanbeseen
reducestheFrÃ©chetGestureDistance(FGD)Score,indicatinga thattheMamba-onlyarchitecturealsoworkswellforgenerating
closermatchtothegroundtruthgestures.Thisisaccompaniedby realisticanddiversegestures.ThissuggeststhatMambaâ€™ssequen-
substantialimprovementsinboththeDiversityScoreandtheL1 tialmodelingabilityiscrucialforgeneratingdiversegestures.The
DiversityScore.AlthoughtheSemantic-RelevantGestureRecall experimentsfurtherconfirmthisobservation.
(SRGR)Scoreseesamarginaldecrease,theBeatAlignScoreim- FeatureFusionModuleDesigns.Lastly,weevaluatevarious
proves,underscoringtheMambaAttnblockâ€™srobustcapabilityfor designsforthefeaturefusionmodule.TheresultsarepresentedMMâ€™24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Table3:Ablationstudyassessingthecontributionofeachinnovativecomponentwithinourgesturegenerationframework.
ThisstudysystematicallyaltersindividualelementstoevaluatetheirimpactontheoverallperformanceontheBEATdataset.
No. Name FGDScoreâ†“ Diversityâ†‘ L1DivScoreâ†‘ SRGRâ†‘ BeatAlignâ†‘
1 BasicDSG+ 103.15 352.31 789.83 0.238 0.841
2 +MambaAttn 64.84 389.77 1081.14 0.233 0.855
3 +SEA 29.95 387.29 955.75 0.237 0.840
4 +SEAD 22.11 434.94 1128.79 0.237 0.853
Table4:AblationstudyexaminingtheinfluenceofthenumberoflayersintheMambaAttndenoiserandthearchitectural
designoftheMambaAttnblock.ThisexperimentexplorestheoptimalconfigurationforourmodelontheBEATdataset.
No Name FGDScoreâ†“ DiversityScoreâ†‘ L1DivScoreâ†‘ SRGRScoreâ†‘ BeatAlignâ†‘
A1 MambaAttn-1 62.54 333.29 743.76 0.240 0.831
A2 MambaAttn-2 41.17 329.03 749.73 0.240 0.851
A3 MambaAttn-4 36.47 343.49 895.69 0.238 0.849
A4 MambaAttn-8 22.11 434.94 1128.79 0.237 0.853
A5 MambaAttn-12 38.36 424.39 1110.82 0.235 0.865
B1 MambaAttn 22.11 434.94 1128.79 0.237 0.853
B2 w/Conv 29.88 388.81 862.08 0.238 0.861
B3 w/oAttn 46.58 358.49 864.19 0.238 0.867
B4 w/oMamba 44.21 329.12 771.29 0.240 0.838
B5 w/Conv,w/oAttn 93.25 418.94 895.71 0.237 0.845
B6 w/Conv,w/oMamba 34.10 363.65 813.65 0.239 0.858
B7 w/Conv,w/oAttn&Mamba 75.37 324.37 760.02 0.240 0.846
Table5:Ablationstudyexploringtheeffectivenessofdifferentfeaturefusionstrategiesinourgesturegenerationmodel.The
studycomparesvariousapproachestointegratingmulti-modaldata,includingaudio,text,style,andemotion,todetermine
theirimpactonthequalityofgeneratedgesturesontheBEATdataset.
No Name FGDScoreâ†“ DiversityScoreâ†‘ L1DivScoreâ†‘ SRGRScoreâ†‘ BeatAlignâ†‘
1 OriginDSG+Input 64.84 389.77 1081.14 0.233 0.855
2 Simplify 32.08 387.72 968.36 0.237 0.838
3 Simplify+Emo(SEA) 29.95 389.52 955.75 0.237 0.848
4 SEA+Disentanglement 26.61 395.51 986.89 0.237 0.850
5 SEAD 22.11 434.94 1128.79 0.237 0.853
inTable5.Startingwithasimplifiedfusionapproachthatdirectly 6 Conclusion
concatenatesaudio,text,andstylefeatures(ourSAfusionmodule), ThispaperintroducesMambaGesture,anovelframeworkforco-
weobserveasubstantialdecreaseintheFGDScore,albeitwitha speech gesture generation that leverages the state space model
declineindiversitymetrics,asthismethodforgoesthebaselineâ€™s Mambaandadisentangledmulti-modalityfusiontechnique.Ourap-
featurefusionstrategy.Byaddingtheemotionmodality,wecre- proachintegratestheMambaAttnblock,whichcombinesMambaâ€™s
ateourSEAmodule,whichfurtherimprovestheFGDScore.The strengths in sequential data processing with the contextual un-
subsequentdisentanglementinourSEAD-basicmoduleresultsin derstandingprovidedbyattentionmechanisms.Additionally,the
improvementsacrosstheFGDScore,DiversityScore,andL1Diver- SEADmoduleeffectivelydisentanglesstyleandemotionfromaudio
sityScore.Thefinalenhancementwiththefusedstyleandemotion features,enablingthegenerationofmorerealisticandexpressive
feature ğ‘“ ğ‘ â€² ğ‘’ inourSEADmodulesignificantlyincreasesboththe gestures.ComprehensiveexperimentsontheBEATdatasetdemon-
DiversityScoreandL1DiversityScore. stratethatMambaGestureoutperformsstate-of-the-artmethods
Thedetailedresultsoftheseablationstudiesdemonstratethe acrossmultiplemetrics,validatingtheeffectivenessoftheSEAD
incrementalbenefitsofeachproposedcomponentinourgesture moduleandMambaAttnblock.Futureworkwilladdresscurrent
generationframework,providingclearevidenceoftheirindividual limitations, such as the slow synthesis speed, and may explore
andcollectiveimpactonthemodelâ€™sperformance. integratingfacialexpressionstosynthesizeacompleteavatar.
rebmunreyal
ngisedkcolbMambaGesture MMâ€™24,October28-November1,2024,Melbourne,Australia.
References
[25] HaiyangLiu,NaoyaIwamoto,ZihaoZhu,ZhengqingLi,YouZhou,ElifBozkurt,
[1] SimonAlexanderson,RajmundNagy,JonasBeskow,andGustavEjeHenter.2023. andBoZheng.2022.DisCo:DisentangledImplicitContentandRhythmLearn-
Listen,denoise,action!audio-drivenmotionsynthesiswithdiffusionmodels. ingforDiverseCo-SpeechGesturesSynthesis.InProceedingsofthe30thACM
ACMTransactionsonGraphics(TOG)(2023). InternationalConferenceonMultimedia.
[2] TenglongAo,ZeyiZhang,andLibinLiu.2023.Gesturediffuclip:Gesturediffusion [26] HaiyangLiu,ZihaoZhu,GiorgioBecherini,YichenPeng,MingyangSu,YouZhou,
modelwithcliplatents.ACMTransactionsonGraphics(TOG)(2023). NaoyaIwamoto,BoZheng,andMichaelJBlack.2023.EMAGE:TowardsUnified
[3] KirstenBergmannandStefanKopp.2009.Increasingtheexpressivenessofvirtual HolisticCo-SpeechGestureGenerationviaMaskedAudioGestureModeling.
agents:autonomousgenerationofspeechandgestureforspatialdescription arXivpreprintarXiv:2401.00374(2023).
tasks..InAAMAS(1). [27] HaiyangLiu,ZihaoZhu,NaoyaIwamoto,YichenPeng,ZhengqingLi,YouZhou,
[4] JustineCassell,CatherinePelachaud,NormanBadler,MarkSteedman,Brett ElifBozkurt,andBoZheng.2022.Beat:Alarge-scalesemanticandemotional
Achorn,TrippBecket,BrettDouville,ScottPrevost,andMatthewStone.1994. multi-modaldatasetforconversationalgesturessynthesis.InEuropeanconference
Animatedconversation:rule-basedgenerationoffacialexpression,gesture& oncomputervision.
spokenintonationformultipleconversationalagents.InProceedingsofthe21st [28] XianLiu,QianyiWu,HangZhou,YinghaoXu,RuiQian,XinyiLin,XiaoweiZhou,
annualconferenceonComputergraphicsandinteractivetechniques. WayneWu,BoDai,andBoleiZhou.2022.LearningHierarchicalCross-Modal
[5] JunmingChen,YunfeiLiu,JiananWang,AilingZeng,YuLi,andQifengChen. AssociationforCo-SpeechGestureGeneration.InProceedingsoftheIEEE/CVF
2024.Diffsheg:Adiffusion-basedapproachforreal-timespeech-drivenholistic ConferenceonComputerVisionandPatternRecognition.
3dexpressionandgesturegeneration.(2024). [29] StacyMarsella,YuyuXu,MargauxLhommet,AndrewFeng,StefanScherer,and
[6] SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,Zhuo AriShapiro.2013.Virtualcharacterperformancefromspeech.InProceedingsof
Chen,JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,JianWu,Long the12thACMSIGGRAPH/Eurographicssymposiumoncomputeranimation.
Zhou,ShuoRen,YanminQian,YaoQian,JianWu,MichaelZeng,andFuruWei. [30] TomasMikolov,EdouardGrave,PiotrBojanowski,ChristianPuhrsch,andAr-
2021.WavLM:Large-ScaleSelf-SupervisedPre-trainingforFullStackSpeech mandJoulin.2018.AdvancesinPre-TrainingDistributedWordRepresentations.
Processing.arXivpreprintarXiv:2110.13900(2021). InProceedingsoftheInternationalConferenceonLanguageResourcesandEvalua-
[7] KiranChhatre,RadekDanÄ›Äek,NikosAthanasiou,GiorgioBecherini,Christopher tion(LREC2018).
Peters,MichaelJ.Black,andTimoBolkart.2024. AMUSE:EmotionalSpeech- [31] MichaelNeff,MichaelKipp,IreneAlbrecht,andHans-PeterSeidel.2008.Gesture
driven3DBodyAnimationviaDisentangledLatentDiffusion.InProceedings modelingandanimationbasedonaprobabilisticre-creationofspeakerstyle.
IEEEConferenceonComputerVisionandPatternRecognition(CVPR). ACMTransactionsOnGraphics(TOG)(2008).
[8] Chung-ChengChiuandStacyMarsella.2011.Howtotrainyouravatar:Adata [32] SimbarasheNyatsanga,TarasKucherenko,ChaitanyaAhuja,GustavEjeHenter,
drivenapproachtogesturegeneration.InInternationalWorkshoponIntelligent andMichaelNeff.2023.AComprehensiveReviewofData-DrivenCo-Speech
VirtualAgents. GestureGeneration.InComputerGraphicsForum.
[9] DanielYFu,TriDao,KhaledKSaab,ArminWThomas,AtriRudra,andChristo- [33] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,TriDao,StephenBac-
pherRÃ©.2022.Hungryhungryhippos:Towardslanguagemodelingwithstate cus,YoshuaBengio,StefanoErmon,andChristopherRÃ©.2023.Hyenahierarchy:
spacemodels.arXivpreprintarXiv:2212.14052(2022). Towardslargerconvolutionallanguagemodels.InInternationalConferenceon
[10] AlbertGuandTriDao.2023. Mamba:Linear-timesequencemodelingwith MachineLearning.
selectivestatespaces.arXivpreprintarXiv:2312.00752(2023). [34] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.
[11] AlbertGu,KaranGoel,andChristopherRÃ©.2022.EfficientlyModelingLongSe- 2022. Hierarchicaltext-conditionalimagegenerationwithcliplatents. arXiv
quenceswithStructuredStateSpaces.InTheInternationalConferenceonLearning preprintarXiv:2204.06125(2022).
Representations(ICLR). [35] BrianRavenet,CatherinePelachaud,ChloÃ©Clavel,andStacyMarsella.2018.
[12] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,and Automatingtheproductionofcommunicativegesturesinembodiedcharacters.
ChristopherRÃ©.2021.Combiningrecurrent,convolutional,andcontinuous-time Frontiersinpsychology(2018).
modelswithlinearstatespacelayers.Advancesinneuralinformationprocessing [36] AurkoRoy,MohammadSaffar,AshishVaswani,andDavidGrangier.2021.Effi-
systems(2021). cientcontent-basedsparseattentionwithroutingtransformers.Transactionsof
[13] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,and theAssociationforComputationalLinguistics(2021).
SeppHochreiter.2017.Ganstrainedbyatwotime-scaleupdateruleconverge [37] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,Jiany-
toalocalnashequilibrium.Advancesinneuralinformationprocessingsystems ongWang,andFuruWei.2023.Retentivenetwork:Asuccessortotransformer
(2017). forlargelanguagemodels.arXivpreprintarXiv:2307.08621(2023).
[14] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic [38] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and
models.Advancesinneuralinformationprocessingsystems(2020). AmitHaimBermano.2022. HumanMotionDiffusionModel.InTheEleventh
[15] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFranÃ§oisFleuret. InternationalConferenceonLearningRepresentations.
2020.Transformersarernns:Fastautoregressivetransformerswithlinearatten- [39] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
tion.InInternationalconferenceonmachinelearning. AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
[16] JihoonKim,JiseobKim,andSungjoonChoi.2023.Flame:Free-formlanguage- youneed.Advancesinneuralinformationprocessingsystems(2017).
basedmotionsynthesis&editing.InProceedingsoftheAAAIConferenceon [40] HannesVilhjÃ¡lmsson,NathanCantelmo,JustineCassell,NicolasE.Chafai,
ArtificialIntelligence. MichaelKipp,StefanKopp,MaurizioMancini,StacyMarsella,AndrewNMar-
[17] MichaelKipp.2005. Gesturegenerationbyimitation:Fromhumanbehaviorto shall,CatherinePelachaud,etal.2007.Thebehaviormarkuplanguage:Recent
computercharacteranimation. developmentsandchallenges.InIntelligentVirtualAgents:7thInternationalCon-
[18] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro.2020. ference,IVA2007Paris,France,September17-19,2007Proceedings7.
DiffWave:AVersatileDiffusionModelforAudioSynthesis.InInternational [41] SenWang,JiangningZhang,WeijianCao,XiaobinHu,MoranLi,Xiaozhong
ConferenceonLearningRepresentations. Ji,XinTan,MengtianLi,ZhifengXie,ChengjieWang,etal.2024.MMoFusion:
[19] StefanKopp,BrigitteKrenn,StacyMarsella,AndrewNMarshall,Catherine Multi-modalCo-SpeechMotionGenerationwithDiffusionModel.arXivpreprint
Pelachaud,HannesPirker,KristinnRThÃ³risson,andHannesVilhjÃ¡lmsson.2006. arXiv:2403.02905(2024).
Towardsacommonframeworkformultimodalgeneration:Thebehaviormarkup [42] ZunnanXu,YukangLin,HaonanHan,SichengYang,RonghuiLi,YachaoZhang,
language.InIntelligentVirtualAgents:6thInternationalConference,IVA2006, andXiuLi.2024.MambaTalk:EfficientHolisticGestureSynthesiswithSelective
MarinaDelRey,CA,USA,August21-23,2006.Proceedings6. StateSpaceModels.arXivpreprintarXiv:2403.09471(2024).
[20] StefanKoppandIpkeWachsmuth.2002.Model-basedanimationofco-verbal [43] SichengYang,ZilinWang,ZhiyongWu,MingleiLi,ZhensongZhang,Qiaochu
gesture.InProceedingsofComputerAnimation2002(CA2002). Huang,LeiHao,SongcenXu,XiaofeiWu,ChangpengYang,etal.2023.Unifiedges-
[21] Hsin-YingLee,XiaodongYang,Ming-YuLiu,Ting-ChunWang,Yu-DingLu, ture:Aunifiedgesturesynthesismodelformultipleskeletons.InProceedingsof
Ming-HsuanYang,andJanKautz.2019.Dancingtomusic.Advancesinneural the31stACMInternationalConferenceonMultimedia.
informationprocessingsystems(2019). [44] SichengYang,ZhiyongWu,MingleiLi,ZhensongZhang,LeiHao,Weihong
[22] SergeyLevine,ChristianTheobalt,andVladlenKoltun.2009.Real-timeprosody- Bao,MingCheng,andLongXiao.2023. DiffuseStyleGesture:stylizedaudio-
drivensynthesisofbodylanguage.InACMSIGGRAPHAsia2009papers. drivenco-speechgesturegenerationwithdiffusionmodels.InProceedingsofthe
[23] JingLi,DiKang,WenjiePei,XuefeiZhe,YingZhang,ZhenyuHe,andLinchao Thirty-SecondInternationalJointConferenceonArtificialIntelligence.
Bao.2021.Audio2gestures:Generatingdiversegesturesfromspeechaudiowith [45] SichengYang,ZunnanXu,HaiweiXue,YongkangCheng,ShaoliHuang,Ming-
conditionalvariationalautoencoders.InProceedingsoftheIEEE/CVFInternational mingGong,andZhiyongWu.2024.Freetalker:ControllableSpeechandText-
ConferenceonComputerVision. DrivenGestureGenerationBasedonDiffusionModelsforEnhancedSpeaker
[24] RuilongLi,ShanYang,DavidARoss,andAngjooKanazawa.2021.Aichoreog- Naturalness.InICASSP2024-2024IEEEInternationalConferenceonAcoustics,
rapher:Musicconditioned3ddancegenerationwithaist++.InProceedingsofthe SpeechandSignalProcessing(ICASSP).
IEEE/CVFInternationalConferenceonComputerVision. [46] SichengYang,HaiweiXue,ZhensongZhang,MingleiLi,ZhiyongWu,Xiaofei
Wu,SongcenXu,andZonghongDai.2023.TheDiffuseStyleGesture+entryto
theGENEAChallenge2023.InProceedingsofthe25thInternationalConferenceMMâ€™24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
onMultimodalInteraction. [51] ZeyuZhang,AkideLiu,IanReid,RichardHartley,BohanZhuang,andHaoTang.
[47] YanzheYang,JimeiYang,andJessicaHodgins.2020. Statistics-basedmotion 2024. MotionMamba:EfficientandLongSequenceMotionGenerationwith
synthesisforsocialconversations.InComputerGraphicsForum. HierarchicalandBidirectionalSelectiveSSM. arXivpreprintarXiv:2403.07487
[48] HongweiYi,HualinLiang,YifeiLiu,QiongCao,YandongWen,TimoBolkart, (2024).
DachengTao,andMichaelJBlack.2023.Generatingholistic3dhumanmotion [52] YihaoZhi,XiaodongCun,XuelinChen,XiShen,WenGuo,ShaoliHuang,and
fromspeech.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand ShenghuaGao.2023.Livelyspeaker:Towardssemantic-awareco-speechgesture
PatternRecognition. generation.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
[49] YoungwooYoon,BokCha,Joo-HaengLee,MinsuJang,JaeyeonLee,JaehongKim, Vision.
andGeehyukLee.2020.Speechgesturegenerationfromthetrimodalcontextof [53] LingtingZhu,XianLiu,XuanyuLiu,RuiQian,ZiweiLiu,andLequanYu.2023.
text,audio,andspeakeridentity.ACMTransactionsonGraphics(TOG)(2020). Tamingdiffusionmodelsforaudio-drivenco-speechgesturegeneration.InPro-
[50] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,Lei ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
Yang,andZiweiLiu.2024.Motiondiffuse:Text-drivenhumanmotiongenera- [54] WentaoZhu,XiaoxuanMa,DongwooRo,HaiCi,JinluZhang,JiaxinShi,Feng
tionwithdiffusionmodel.IEEETransactionsonPatternAnalysisandMachine Gao,QiTian,andYizhouWang.2023.Humanmotiongeneration:Asurvey.IEEE
Intelligence(2024). TransactionsonPatternAnalysisandMachineIntelligence(2023).