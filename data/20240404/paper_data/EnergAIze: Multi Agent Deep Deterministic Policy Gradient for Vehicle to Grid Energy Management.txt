EnergAIze: Multi Agent Deep Deterministic Policy
Gradient for Vehicle to Grid Energy Management
Tiago Fonseca*, Luis Ferreira*, Bernardo Cabral, Ricardo Severino Isabel Pra√ßa
INESC-TEC/Polytechnic of Porto - School of Engineering GECAD/Polytechnic of Porto - School of Engineering
Porto, Portugal Porto, Portugal
{calof*, llf*, bemac, sev}@isep.ipp.pt icp@isep.ipp.pt
Abstract‚ÄîThis paper investigates the increasing roles of According to [4], at EV adoption levels of 25%, peak load can
Renewable Energy Sources (RES) and Electric Vehicles (EVs). grow by 30% due to the EVs exerting pressure on the grid.
While indicating a new era of sustainable energy, these also
The environmental potential of EVs can only be
introduce complex challenges, including the need to balance
supply and demand and smooth peak consumptions amidst maximized if the electricity they utilize for charging originates
rising EV adoption rates. Addressing these challenges requires from clean, renewable sources. As such, the stated integration
innovative solutions such as Demand Response (DR), energy challenges not only negate the advantages of RES and EVs but
flexibility management, Renewable Energy Communities can also result in overall costlier electricity [5]. Therefore,
(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). researchers point to the need to a significant transformation in
However, existing V2G approaches often fall short in real-world how we generate, store, distribute, and consume energy [6].
adaptability, global REC optimization with other flexible assets,
B. Energy Management
scalability, and user engagement. To bridge this gap, this paper
introduces EnergAIze, a Multi-Agent Reinforcement Learning Intelligent energy management provides solutions to the
(MARL) energy management framework, leveraging the Multi- mentioned problems by controlling, optimizing, and
Agent Deep Deterministic Policy Gradient (MADDPG) scheduling the use of energy resources more efficiently [7].
algorithm. EnergAIze enables user-centric and multi-objective Central to these solutions are Demand Response (DR), and
energy management by allowing each prosumer to select from a Energy Community (EC) management [8], which strive to
range of personal management objectives, thus encouraging
balance energy supply and demand, reducing costs, and
engagement. Additionally, it architects‚Äô data protection and
enhancing grid stability [9]. Both depend on prosumer‚Äôs
ownership through decentralized computing, where each
energy flexibility, which can be represented as Flex Offers
prosumer can situate an energy management optimization node
(FOs) [10], and in essence are the prosumer‚Äôs availability to
directly at their own dwelling. The local node not only manages
strategically delay or advance their consumption in time.
local energy assets but also fosters REC wide optimization. The
efficacy of EnergAIze was evaluated through case studies Also, at the heart of this evolving landscape of energy
employing the CityLearn simulation framework. These management is Vehicle-to-Grid (V2G), a concept that seeks to
simulations were instrumental in demonstrating EnergAIze's integrate EVs into the broader power grid management given
adeptness at implementing V2G technology within a REC and
their load shifting flexibility [11]. V2G leverages parked EVs
other energy assets. The results show reduction in peak loads,
into energy storage units that not only draw power from the
ramping, carbon emissions, and electricity costs at the REC level
grid for charging but also can feed electricity back into the grid
while optimizing for individual prosumers objectives.
during periods of high demand. For instance, EVs can be
charged during sunny periods when solar panels are at peak
Keywords‚Äî Reinforcement Learning, Multi-Agent Systems,
output, then either use this renewable energy to commute or
Electric Vehicles, Energy Flexibility Control
send it back to the grid during times of high demand or low
I. INTRODUCTION renewable generation. This balances the grid, cuts reliance on
peak power sources like gas, and offers EV owners a return on
Renewable Energy Sources (RES) and Electric Vehicles
their EV investment by selling excess energy back to the grid.
(EVs) are emerging as pivotal players in the shift towards a
In parallel, simpler strategies, such as smart charging, also
low-carbon economy [1]. Distributed Energy Resources
named Grid-to-Vehicle (G2V), can optimize energy loads to
(DERs), such as small-scale wind and photovoltaic (PV) solar
during low demand or high renewable generation [12].
production, are fostering a shift in how energy is produced,
from a centralized infrastructure to a distributed generation C. Gaps and Contributions
where individuals directly engage with the energy grid. Such
The coordinated and autonomous management of flexible
individuals are designated as prosumers, reflecting their dual
assets into the energy grid, and specifically EVs into
role as both producers and consumers [2].
intelligently managed REC is still in its infancy [13]. The
A. Integration Challenges prevailing strategies in the real-world often utilize Time-of-
Use (ToU) pricing to incentivize EV owners to plug in their
Despite stated benefits, the surge in the adoption levels of
cars during cheaper periods. However, such strategies require
EVs and RES, although promising for environmental
prosumers to actively manage and be aware of their
sustainability, can also pose a series of infrastructural, control,
consumption and energy prices, which can be tedious and
technological, societal, and institutional challenges with
time-consuming, and lead to prosumer disengagement [14].
significant negative impacts on the grid [3]. One prime
example is the temporal mismatch between peak periods for On this notice, researchers are turning into intelligent
EV charging and peak RES production, leading to an management approaches, hereby called Energy Management
imbalance between the demand for renewable energy and its Systems (EMS), that use advanced algorithms to optimize
supply which mainly occurs during the sunlight hours. control given factors like grid conditions, energy prices,
vehicle usage patterns, prosumer flexibility, battery health,
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024among others [15]. Applied algorithms range from Machine multi-agent RL for decentralized, scalable load shaping in
Learning (ML), Reinforcement Learning (RL) to Model urban energy systems, achieving superior performance
Predictive Control (MPC), meta-heuristics and extant optimal reducing peak loads among other factors. MERLIN by [21]
control algorithms (Section II). tackles RL's challenges in by leveraging independent battery
control policies and transfer learning.
However, these technologies are not yet ready for real-
world deployment due to limitations in computational Despite these advancements, to the best of the authors
scalability, not integrating V2G within broader REC knowledge, there is limited exploration into integrating V2G
optimization, concerns over data ownership, and the need for control which integrates EVs energy management with the
further refinement in adaptive learning to cater to prosumers diversity of control of other REC assets. As pointed by [5], the
objectives, stimulating their participation. best strategy for energy management should consider the
diversity and heterogeneity of multiple assets, gathering their
Given the identified research gap, this paper introduces
energy flexibility for most effective management.
EnergAIze, a Multi Agent Reinforcement Learning (MARL)
Additionally, there is a lack of research on multi-objective
EMS framework designed for managing the energy flexibility
optimization that enables each prosumer to select their
of EVs, and other energy flexible assets, such as PVs, and Heat
participation goals within the broader optimization scope.
Pumps within RECs. Contributions can be resumed as:
‚Ä¢ EnergAIze adapts the Multi-Agent Deep Deterministic
III. ENERGAIZE SYSTEM ARCHITECTURE
Policy Gradient (MADDPG) framework, emphasizing A. Multi-Agent Formulation for Energy Management
user-centric energy management and allow prosumers to
At the basis of any MARL formulation is the Markov
select one of the following personal objectives for
Game [22]. While the base RL Markov Decision Process is
optimization: i) cost minimization, ii) self-consumption
designed for single-agent dynamics (i.e., a centralized
maximization, iii) carbon emission reduction).
algorithm managing all EC prosumers), the Markov Game
‚Ä¢ EnergAIze requires minimal input from the prosumer, for
extends this structure to consider multiple agents, in this case
performing V2G with only a departure time and a State of
each referring to the EMS algorithm managing a dwelling‚Äôs
Charge (SoC) at departure for the EV.
energy. These agents operate within discrete time steps
‚Ä¢ Beyond individual optimization, EnergAIze encourages a
represented by t, with t‚àà{1,2,...,N}.
community-oriented approach where prosumers not only
optimize their energy use but also support the community 1) Core Components
by trading energy whit neighbours. The core components for the MARL approach are:
‚Ä¢ Decentralized architecture design, powered by edge
‚Ä¢ Agent i for i‚àà{1,2,...,N}, represents one of N EnergAIze
computing, which ensures privacy and data sovereignty
decision-making nodes deployed at each REC dwelling.
for prosumers engaging in this strategy.
‚Ä¢ Demonstration of the effectiveness of EnergAIze ‚Ä¢ Observation, denoted as Oi t, reflects the localized
management with a simulation V2G REC scenario. perspective of agent i at a given time t. In the
implementation, Oi can be expressed, but not limited to,
t
D. Paper Outline Oi = {ONSL,OSG,OCN_SOC,OCN_EDT, OCN_SOC_D ‚Ä¶}. Each
t t t t t t
In Section II, related projects in this domain are explored. element within this set is an individual input within a
Section III describes the applied algorithm. The simulation is dwelling. ONSL represents the Non-Shiftable Load in
t
described in Section IV. Section V showcases results, while kWh within dwelling i at time t; OSG denotes the PV
t
Section VI concludes and outlines future directions. Solar Generation in kWh within dwelling i, OCN_SOC
t
indicates the State of Charge (SOC) of a connected EV
II. RELATED WORK within dwelling i; OCN_EDT states the Estimated
t
Recent research has highlighted the critical role of Departure Time and OCN_SOC_D the Required SoC at
t
optimizing EV charging schedules within the energy grid, departure for an EV. Observations on the forecast of PV
particularly given the variability of RES and the uncertainty production or on the arrival time of an EV can also be
surrounding EV travel patterns. Traditional optimization provided. Besides local observations, Oi it include
t
methods struggle to manage these complexities, propelling Energy Prices OEP, Carbon Emissions OCE observations.
t t
interest towards RL techniques with its ability to adapt and
‚Ä¢ State, with S=[O1,O2,...,ON], is the aggregated global
t t t t
learn from complex, dynamic systems [16]. Deep RL (DRL)
view of the environment, formed by pooling together the
techniques have been applied to maximize renewable energy
observations from all dwellings i at time t.
consumption and the SoC of EVs at departure. Further
‚Ä¢ Action, denoted Ai reflects the agent i decided actions A
research has combined RL with Time of Use (ToU) energy t
at time t. Ai can be expressed as Ai={AHSN_EV,‚Ä¶}, where
pricing to minimize infrastructure costs, while Deep Q- t t t t
each element is an individual action within a dwelling.
Network-based RL (DQN-RL) algorithms have been tested
against real-world data to validate their charging strategies For instance, AHS t represents the action (turn on or off)
effectiveness [17]. However, these approaches have led to applied to the Heating Storage (electric heater)
some data security and scalability concerns [18]. management, and ACN_EV t denotes the action to charge or
discharge a connected EV by some amount of energy.
On this notice researchers, and the work on this paper,
‚Ä¢ Environment, denoted as Œµ, this concept represents the
explore Multi-Agent Reinforcement Learning (MARL), as it
REC as a Whole. Œµ is where Agents i operate, Oi, are
allows for multiple decentralized agents to learn t
collected, and Ai are applie. The real-world EnergAIze
cooperatively, offering scalable and robust solutions for t
energy management environment can be defined as:
complex energy network management [19]. For example,
dynamic, continuously evolving in response to
MARLISA, introduced by [20] demonstrates the potential of
unpredictable factors like weather and energy demand;
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024partly inaccessible; continuous, due to the range of dwellings and prosumers to autonomously optimize their
possible action values for charging; non-deterministic, as energy flexibility within EC objectives. This decentralization
outcomes of actions cannot be predicted with certainty is made possible through an edge computing framework, such
due to external influences. as the one present in [24]. In this approach, every dwelling has
an edge computing device. The edge computer holds the agent
i which, through its trained policy function, reasons on Oi
t
Figure 1 - Problem contextualized within MARL Figure 2 - Physical deployment of EnergAIze at the edge
2) Functional Aspects and Feedback Components collected from its dwelling and produces Ai to guide the
t
In MARL systems, two essential elements govern how energy consumption. Observations come from the local
agents act and learn, the Policy Function œÄi(Ai t‚à£Oi t) determines sensors and smart meters tracking parameters like OCN_SOC t,
the action Ai t that each agent i will take based on its current OCN_EDT t, OCN_SOC_D t. Beyond these local insights, agents can
observation Oi t. This is the function that is learned and refined also connect to a server/cloud to get real-time OEP t, and OCE t.
during training and guides the dwelling‚Äôs energy management
at deployment. The Reward (Ri) is the feedback that agent i Edge computing at the dwelling not only ensures reduced
t
receives from the environment after executing actions Ai. latency, crucial for rapid, continuous decisions, but also
t
supports offline functionality and control, if necessary. By
(Section III.D.). The reward will be composed of local and
processing local sensor data on-site it maintains sovereignty,
REC wide optimization evaluation, in a multi-level approach
ensuring that sensitive information remains within the
as defined by [23].
dwelling. Additionally, this decentralized structure diminishes
3) Logical Flow the risk of system-wide vulnerabilities or failures, as issues in
Fig. 1. depicts the MARL process that ties the presented one unit do not directly compromise the entire REC.
components together.
C. Multi-Agent Deep Deterministic Policy Gradient
1. Observation Gathering: At the start of each time step t,
The presented approach is anchored on the Multi-Agent
each agent i gathers local observations Oi about its
t Deep Deterministic Policy Gradient (MADDPG) [25], which
dwelling environment, which can be captured by real-
evolved the DDPG single-agent algorithm into a centralized
world IoT sensor readings or simulated.
training with decentralized execution MARL algorithm.
2. Decision Making: Based on Oi, the agent consults its
t MADDPG is tailored for environments that balance
policy function œÄi(Ai‚à£Oi) to decide on the values for Ai.
t t t cooperation and competition between decentralized agents,
3. Action Execution: Once decided, the agent i applies Ai t making it an appropriate choice for this work‚Äôs objectives
in the environment Œµ, evolving to the next time step by (cooperating for optimizing REC and compete for individual
using the State Transition Function f(Oi t+1 ‚à£ Oi t, Ai t). local optimization based on prosumer‚Äôs objectives).
4. Feedback Calculation: Following action execution,
Fig. 3. evolves from Fig. 1. and presents the central
each agent i garners feedback from the environment in
components of the MADDPG's algorithm framed within
the form of a reward Ri. This reward gauges how well
t energy management context. It has two distinct phases
the actions Ai. helped on reaching the agent's objectives
t represented: the execution phase (green) and the training
and the overarching goals of the system.
phase (purple). The biggest change from Fig. 1. architecture is
Note on the decentralized nature of the steps described, in the way agents are represented, now with each agent having
emphasizing how each agent within a MARL formulation acts two deep neural networks: an Actor Network (Œºi) and a Critic
autonomously relying solely on its local observations to Network (Qi).
decide its actions, discarding the need to rely on
Together, in a MADDPG setting, the Actor networks
communication with other agents during runtime.
decide on the most appropriate actions for their corresponding
B. Physical Deployment agents, effectively having one policy function per dwelling,
where personal objectives and prosumer routines are learned
Fig. 2. illustrates the architected decentralized deployment
by the neural network. The Critic networks guide the learning
of EnergAIze into an REC with a set of dwellings, each with
process by assessing the global consequences of local actor
a different set of assets. EnergAIze is architected to operate on
decisions, ensuring also the objective of REC optimization.
a decentralized model approach, empowering individual
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 20241) Replay Buffer and Mini Batch actions, undergo normalization. Instead of utilizing the raw
A Replay Buffer was implemented during training to serve kWh value or a percentage range of 0-100, the SOC is
as a memory mechanism, storing past experiences. Its normalized between 0 and 1.
purposes are to preserve history for reference and to enable
4) Hybrid MADDPG-RBC approach for Exploration
random sampling, breaking correlations between consecutive
This work adopts a hybrid exploration strategy (Fig. 5.)
experiences for a broader learning scope. Mini-Batch Training
that mixes the conventional Gaussian noise, with a Rule Based
further optimizes learning by processing multiple experiences
Controller (RBC) guided exploration in the beginning. By
at once, leading to a more stable convergence.
relying on the RBC, presented in [23], the agent can skip the
Figure 4 - Actor and Critic networks architecture for EnergAIze
Figure 5 ‚Äì RBC enabled and Conventional exploration variants
initial phase of random exploration, which can be time-
consuming (exploring solutions that are not accepted within
problem constrains, such as leaving without the required SoC).
Figure 3 - Detailed implementation of the MADDPG
Relying solely on the RBC means that the algorithm will not
2) Actor and Critic Networks Architecture explore beyond the policy prescribed by the RBC, and
Fig. 4. illustrates the architecture of the actor and critic effectively it won‚Äôt learn past it, getting trapped in local
networks. Each agent i is outfitted with an Actor Network that optimal. In this work approach, the RBC is just used in the
guides its optimization by processing observations with an beginning of exploration. The Gausian noise is applied after
input layer of width Oi. Then, it is followed by a set of fully and decays in magnitude along the training episodes [23].
t
connected hidden layers using ReLU activation functions for
D. Reward Function
non-linear modeling. The output layer, consisting of neurons
equal to the number of possible actions Ai. The reward function evaluates agents‚Äô actions to manage
t
individual objectives while contributing to REC goals and
The Critic Network evaluates the actions suggested by the
steering towards the hard context constraint rules imposed by
actor within a broader scope of the REC. It features a complex
the complexity EV charging (e.g., the car cannot charge if it is
architecture with two initial input layers: a State Layer for
not present, the car cannot leave with a lower then required
environmental dynamics (width of S, as the total number of
t SoC at departure). Eq. (1) encapsulates the three crucial
observations of the community) emphasizing State Primacy,
components of EnergAIze‚Äôs mixed reward function,
and an Action Integration Layer to incorporate actions from
comprehensively detailed in [23].
all agents in the REC at time t (A). The following are fully
t
connected hidden layers (specified by critic_units ùëÖùëñ = ùõº√óùëüùëÉùëüùëúùë†ùë¢ùëöùëíùëü +ùõΩ√ó ùëüùê∏ùëâ+ ùúÅ√óùëüùëÖùê∏ùê∂ (1)
ùë°
hyperparameter) for in-depth learning. The output is a single
Here, Ri indicates the agent i reward for at a specific
Q-value, estimating the cumulative reward for the actions of t
timestep t. The rProsumer component caters to the unique
the specific dwelling based on the current environment state.
objectives or performance of each prosumer. Three different
3) Data Processing for Networks components are prepared given the unique choice of the
For categorical Observations collected from EnergAIze‚Äôs prosumer. Prosumers can choose from i) cost minimization
environment, such as Day of the Week, one-hot encoding is goal, ii) self-consumption maximization goal, and iii) carbon
employed. With eight categories in this case - Monday to emissions reduction goal. The rEV component of the reward
Sunday, plus a special category for holidays - each category embodies contraints associated with EV operations, like
gets its unique binary representation. For instance, Monday meeting charging needs. The rREC encapsulates broader REC
might be represented as [1, 0, 0, 0, 0, 0, 0, 0], and Holidays goals, such as optimizing and balancing grid demand and
might take the form [0, 0, 0, 0, 0, 0, 0, 1]. On the other hand, promoting shared energy resources. The coefficients Œ±, Œ≤ and
continuous observations, such as battery SoC or charging Œ∂ affect the relative significance of individual versus
community objectives and were object of fine tuning in [23].
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024IV. SIMULATION ENVIRONMENT
In this work, a data fusion process was accomplished to
construct a simulation scenario (SS) compatible with the
CityLearn framework [26]. CityLearn is a standardized
environment for facilitating benchmarking of RL and MARL
algorithms for DR. At its core CityLearn does not simulate
EVs, and as such, EVLearn extension is used.
The base SS derives from a dataset previously used within
CityLearn [26]). It features 9 dwellings and 4 years of data.
The dwellings are of different types, including a medium-sized
office, a fast-food restaurant, a standalone retail store, a strip
mall, and five medium-scale multifamily residences. The
dataset details energy elements like air-to-water heat pumps,
electric heaters, and on-site PVs. To delve into V2G
simulation, the dataset also includes simulated EVs, EVCs and
Figure 6 - Results of adopting a polynomial reward function
their energy flexibility (generated synthetically). Finally, to
cater to the developed user-centric approach, each prosumer Table 1 resumes the normalized KPIs at the REC during
(dwelling) was attributed a different personal objective. Four the final deterministic control episode.
with cost minimization goals, three with self-consumption
Table 1 ‚Äì Comparison of REC-level KPIs
maximization goal, and two with carbon emissions reduction
KPI SAC MAR ENER
goal. Refer to Appendix J of [23] for detail at each prosumer
Electricity Consumption (D) -10.30% -7.35% -12.46%
flexible assets and individual objectives. The original dataset
Electricity Price (C) -9.39% -6.13% -11.35%
was further enriched by integrating Time of Use (ToU) energy
pricing from the Iberian wholesale energy market (OMIE) Carbon Emissions (G) -10.12% -7.14% -11.84%
[27]. This addition, corresponding in length to the base dataset Zero Net Energy(Z) +4.72% +2.85% +6.22%
(i.e., four years of electricity prices for the 4-year dataset). Average Daily Peak (P) -18.95% -13.65% -20.80%
Ramping (R) -29.29% -25.99% -35.22%
To evaluate the success of the objectives, a set of Key
1 - Load Factor (1-L) -11.82% -8.97% -13.43%
Performance Indicators (KPIs) have been established
according to the KPIs defined originally by [20]. KPIs are When observing the results at the broader EC level,
presented as a normalized value to the SS baseline. The EnergAIze‚Äôs performance metrics standout. The total carbon
baseline, in this context, represents the initial data observed emissions (G) and energy cost were cut by 12% compared to
and measured, indicating the EC consumption prior to the the baseline. Zero net energy (Z) also recorded an
introduction of any intelligent management techniques enhancement, marked by a 7% decrease against baseline
controlling their flexible assets and EVs. Three algorithms will numbers. Of particular interest is the average ramping metric
be compared: EnergAIze, MARLISA and a Soft-Actor Critic (R), the daily peak average (P) and the 1-Load Factor (1-L),
Network (SAC). Each algorithm was trained for 15 episodes. where the EnergAIze exhibited a reduction of 35%, 21% and
Results are normalized to the no control baseline of the SS and 13,5% respectively. This accentuates EnergAIze's adeptness at
are obtained for the final deterministic episode. maintaining the needed balance between energy supply and
demand. Moreover, when set side by side with the other
V. EXPERIMENTAL RESULTS WITH V2G
algorithms, EnergAIze exhibited superior performance by a
Fig. 5 outputs the management V2G results for a day of margin of 2 to 7%, dependent on each category.
dwelling 7, with personal cost minimization objectives, using
Table 2 presents the reduction in percentage of the
EnergAIze. The green zones are when the vehicle was
dwelling level KPIs (from Dwelling 1, D1, to Dwelling 9, D9)
connected to the dwelling. At the top plot blue squares
at, electricity cost (C), carbon emissions (G), and zero net
represent the actual SoC of the EV and the red dots the
energy (Z). The color coding in Table 9 helps differentiates
required SOC at departure. At the bottom plot, the real-time
performance. Red means it is the worst compared to the other
electricity prices for the same day are depicted.
two algorithms in that specific metric for that specific
Refer to Fig. 6, at 8 am and verify how the EV leaves with building, while yellow means it is the second best and green
a SoC close to what was required by the prosumer. Also refer illustrates the best result. Note that all algorithms got
to Fig. 5 at 6am and from 6pm to 8pm, and check how the considerable reductions, the color coding just serves to
algorithm learned to charge the battery at times of lower compare between them. Below the building identification is
electricity prices, and the discharging happening around 8pm the reference to the personal goal of each.
when prices increase and the dwelling consumption is higher
Out of the nine dwellings, the EnergAIze algorithm
(peak reduction and cost reduction).
demonstrated superior performance in most cases. For
Dwelling 1 (D1, an office) and Dwelling 2 (D2, a fast-food
restaurant), the primary objective was to reduce energy costs
(C). The EnergAIze algorithm successfully achieved this
objective, registering reductions of 17.3% and 14.1%
respectively, and surpassed the performance of other
algorithms for that specific KPI. Building 4, categorized as a
strip mall, was driven by an environmental goal to minimize
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024its carbon footprint (G). Here too, EnergAIze reduced the European Union or Chips JU. Neither the European Union nor
building's carbon emissions by 26.7%. the granting authority can be held responsible for them.
Table 2 - Dwelling-level Results REFERENCES
Energy Cost (C) %
[1] G. A. Pagani and M. Aiello, ‚ÄúThe Power Grid as a complex network:
B1 B2 B3 B4 B5 B6 B7 B8
B9 (Z) A survey,‚Äù Physica A: Statistical Mechanics and its Applications, Jun.
(C) (C) (G) (G) (C) (Z) (C) (Z)
2013, doi: 10.1016/J.PHYSA.2013.01.023.
SAC -15,60 -11,64 -7,26 -23,60 -7,47 -3,87 -1,91 -4,60 -3,94
[2] C. In√™s, et al., ‚ÄúRegulatory challenges and opportunities for collective
MAR -13,57 -10,49 -3,04 -9,79 -3,68 -0,67 -0,37 -0,88 -0,97
renewable energy prosumers in the EU,‚Äù Energy Policy, Mar. 2020,
ENER -17,25 -14,08 -6,29 -25,69 -10,41 -6,20 -4,22 -7,11 -6,19 doi: 10.1016/J.ENPOL.2019.111212.
Carbon Emissions (G) % [3] L. Xie, et al., ‚ÄúToward carbon-neutral electricity and mobility: Is the
SAC -15,98 -11,88 -8,72 -25,01 -8,36 -4,29 -2,34 -4,96 -4,52 grid infrastructure ready?,‚Äù Joule, Aug. 2021, doi:
MAR -17,78 -14,04 -2,80 -10,81 -3,79 -0,79 -0,35 -0,75 -0,85 10.1016/j.joule.2021.06.011.
ENER -17,09 -13,77 -7,94 -26,69 -11,36 -6,73 -4,25 -7,33 -6,44 [4] McKinsey ‚ÄúHow electric vehicles could change the load curve‚Äù 2023
Zero Net Energy (Self-Consumption) (Z) % [5] J. Zhong, M. Bollen, and S. R√∂nnberg, ‚ÄúTowards a 100% renewable
energy electricity generation system in Sweden,‚Äù Renew Energy, vol.
SAC -5,68 -4,86 -4,32 -9,15 -4,69 -3,42 -2,11 -4,38 -3,75
171, pp. 812‚Äì824, Jun. 2021, doi: 10.1016/J.RENENE.2021.02.153.
MAR -6,43 -6,93 -0,25 -0,87 -1,06 -0,23 -0,21 -0,26 -0,29
[6] G. Dileep, ‚ÄúA survey on smart grid technologies and applications,‚Äù
ENER -6,45 -6,69 -3,37 -9,07 -7,27 -5,90 -4,19 -6,92 -5,93
Renew Energy, vol. 146, pp. 2589‚Äì2625, Feb. 2020, doi:
10.1016/J.RENENE.2019.08.092.
In the cases of Buildings 5 and 7, the successes mirrored [7] I. Antonopoulos et al., ‚ÄúArtificial intelligence and machine learning
approaches to energy demand-side response: A systematic review,‚Äù
those of Buildings 1 and 2, with EnnergAIze realizing
Renewable and Sustainable Energy Reviews, Sep. 2020, doi:
significant reductions in energy costs and outpacing other
10.1016/J.RSER.2020.109899.
algorithms by some per cent. Buildings 6, 8, and 9, which were
[8] T. Fonseca, L. L. Ferreira, L. Klein, J. Landeck, and P. Sousa, ‚ÄúFlexigy
oriented towards consuming more self-produced energy, saw Smart-grid Architecture‚Äù, doi: 10.5220/0010918400003118.
their objectives met by the actions of the algorithm. However, [9] T. Fonseca, et al., ‚ÄúFlexible Loads Scheduling Algorithms for
for Building 3, which was aimed at reducing carbon emissions, Renewable Energy Communities,‚Äù Energies 2022, Nov. 2022, doi:
EnergAIze did not emerge as the top performer in comparison 10.3390/EN15238875.
to other algorithms. Yet, it still managed a noteworthy [10] T. B. Pedersen, et al., ‚ÄúModeling and Managing Energy Flexibility
reduction in carbon emissions for this building (7.94% Using FlexOffers,‚Äù 2018 IEEE Int. Conf. on Communications, Control,
and Computing Technologies for Smart Grids, SmartGridComm 2018,
compared to SAC‚Äôs 8.72%).
Dec. 2018, doi: 10.1109/SMARTGRIDCOMM.2018.8587605.
[11] S. S. Ravi and M. Aziz, ‚ÄúUtilization of Electric Vehicles for Vehicle-
VI. CONCLUSIONS
to-Grid Services: Progress and Perspectives,‚Äù Energies 2022, Jan.
This paper presented EnergAIze, a MARL energy 2022, doi: 10.3390/EN15020589.
management framework. EnergAIze enables individual [12] T. A. Skouras, et al., ‚ÄúElectrical Vehicles: Current State of the Art,
prosumers to chase their personal goals, yet collaboratively Future Challenges, and Perspectives,‚Äù Clean Technologies 2020, Dec.
2019, doi: 10.3390/CLEANTECHNOL2010001.
working towards achieving EC objectives, namely, peak
[13] T. A. Skouras, et al., ‚ÄúElectrical Vehicles: Current State of the Art,
energy reduction and energy load balancing. Moreover, the
Future Challenges, and Perspectives,‚Äù Clean Technologies 2020, Dec.
paper presented the planned decentralized architecture
2019, doi: 10.3390/CLEANTECHNOL2010001.
deployment. Results provided an exploration of the
[14] F. Gangale, A. Mengolini, and I. Onyeji, ‚ÄúConsumer engagement: An
EnergAIze algorithm's applicability and contributions for insight from smart grid projects in Europe,‚Äù Energy Policy, Sep. 2013,
V2G scenarios within RECs., EnergAIze. showed diminished doi: 10.1016/J.ENPOL.2013.05.031.
peak consumption and ramping while attending specific [15] F. Gonzalez Venegas, M. Petit, and Y. Perez, ‚ÄúActive integration of
individual goals of the prosumers. electric vehicles into distribution grids: barriers and frameworks for
flexibility services,‚Äù 2021.
Moving forward, it is imperative to enrich EnergAIze's [16] D. Qiu, et al., ‚ÄúReinforcement learning for electric vehicle applications
development by testing it in a variety of complex in power systems:A critical review,‚Äù Renewable and Sustainable
environments. This involves incorporating more real-world Energy Reviews, Mar. 2023, doi: 10.1016/J.RSER.2022.113052.
data, applying the algorithm in actual edge devices of a REC, [17] S. Wang, S. Bi, and Y. A. Zhang, ‚ÄúReinforcement Learning for Real-
increasing simulation time step granularity, and testing for Time Pricing and Scheduling Control in EV Charging Stations,‚Äù IEEE
Trans Industr Inform, Feb. 2021, doi: 10.1109/TII.2019.2950809.
learning transferability. Additionally, incorporating battery
[18] H. M. Abdullah, A. Gastli, and L. Ben-Brahim, ‚ÄúReinforcement
wear into the reward function, handling interactions with
Learning Based EV Charging Management Systems-A Review,‚Äù IEEE
external energy demands, striving for explainable AI, and
Access, 2021, doi: 10.1109/ACCESS.2021.3064354.
tackling data governance through machine unlearning are
[19] J. Dong, A. Yassine, A. Armitage, and M. S. Hossain, ‚ÄúMulti-Agent
crucial. These steps are key to proving EnergAIze's Reinforcement Learning for Intelligent V2G Integration in Future
effectiveness and its ability to adapt to the real-world. Transportation Systems,‚Äù IEEE Transactions on Intelligent
Transportation Systems, Dec. 2023, doi: 10.1109/TITS.2023.3284756.
ACKNOWLEDGMENT [20] J. R. Vazquez-Canteli, G. Henze, and Z. Nagy, ‚ÄúMARLISA: Multi-
Agent Reinforcement Learning with Iterative Sequential Action
This paper is supported by the OPEVA project that has
Selection for Load Shaping of Grid-Interactive Connected Buildings,‚Äù
received funding within the Chips Joint Undertaking (Chips BuildSys 2020 - Proceedings of the 7th ACM International Conference
JU) from the European Union‚Äôs Horizon Europe Programme on Systems for Energy-Efficient Buildings, Cities, and Transportation,
and the National Authorities (France, Czechia, Italy, Portugal, pp. 170‚Äì179, Nov. 2020, doi: 10.1145/3408308.3427604.
Turkey, Switzerland), under grant agreement 101097267. The [21] K. Nweye, S. Sankaranarayanan, and Z. Nagy, ‚ÄúMERLIN: Multi-agent
paper is also supported by Arrowhead PVN, proposal number offline and transfer learning for occupant-centric operation of grid-
interactive communities,‚Äù Appl Energy, Sep. 2023, doi:
101097257. Views and opinions expressed are however those
10.1016/J.APENERGY.2023.121323.
of the author(s) only and do not necessarily reflect those of the
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024[22] M. L. Littman, ‚ÄúMarkov games as a framework for multi-agent
reinforcement learning,‚Äù Machine Learning Proceedings Jan. 1994,
doi: 10.1016/B978-1-55860-335-6.50027-1.
[23] T. C. C. Fonseca, ‚ÄúA Multi-Agent Reinforcement Learning Approach
to Integrate Flexible Assets into Energy Communities,‚Äù Oct. 2026,
Available: https://recipp.ipp.pt/handle/10400.22/24068
[24] B. Cabral et al., ‚ÄúA Scalable Clustered Architecture for Cyber-Physical
Systems,‚Äù in 2023 IEEE 21st Int. Conf. on Industrial Informatics
(INDIN), IEEE, Jul. 2023, doi: 10.1109/INDIN51400.2023.10217924.
[25] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch,
‚ÄúMulti-Agent Actor-Critic for Mixed Cooperative-Competitive
Environments,‚Äù Adv Neural Inf Process Syst, Jun. 2017, Accessed: Jan.
08, 2024. https://arxiv.org/abs/1706.02275v4
[26] J. R. Vazquez-Canteli, et al., ‚ÄúCityLearn: Standardizing Research in
Multi-Agent Reinforcement Learning for Demand Response and Urban
Energy Management,‚Äù Dec. 2020, https://arxiv.org/abs/2012.10504v1
[27] ‚ÄúDay-ahead hourly price | OMIE.‚Äù Accessed: Mar. 08, 2024. [Online].
Available: https://www.omie.es/en/market-results/daily/daily-
market/daily-hourly-price
Fonseca T. et al.: Pre-print Submitted to IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids 2024