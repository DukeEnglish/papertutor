Visual Autoregressive Modeling: Scalable Image
Generation via Next-Scale Prediction
KeyuTian1,2, YiJiang2,â€ , ZehuanYuan2,âˆ—, BingyuePeng2, LiweiWang1,âˆ—
1PekingUniversity 2BytedanceInc
keyutian@stu.pku.edu.cn, jiangyi.enjoy@bytedance.com,
yuanzehuan@bytedance.com, bingyue.peng@bytedance.com, wanglw@pku.edu.cn
Tryandexploreouronlinedemoat: https://var.vision
Codesandmodels: https://github.com/FoundationVision/VAR
Figure1: GeneratedsamplesfromVisualAutoRegressive(VAR)transformerstrainedonImageNet.We
show512Ã—512samples(top),256Ã—256samples(middle),andzero-shotimageeditingresults(bottom).
Abstract
WepresentVisualAutoRegressivemodeling(VAR),anewgenerationparadigm
thatredefinestheautoregressivelearningonimagesascoarse-to-fineâ€œnext-scale
predictionâ€orâ€œnext-resolutionpredictionâ€,divergingfromthestandardraster-scan
â€œnext-tokenpredictionâ€. Thissimple,intuitivemethodologyallowsautoregressive
(AR)transformerstolearnvisualdistributionsfastandcangeneralizewell: VAR,
forthefirsttime,makesGPT-styleARmodelssurpassdiffusiontransformersin
imagegeneration. OnImageNet256Ã—256benchmark,VARsignificantlyimprove
ARbaselinebyimprovingFrÃ©chetinceptiondistance(FID)from18.65to1.80,
inception score (IS) from 80.4 to 356.4, with 20Ã— faster inference speed. It is
alsoempiricallyverifiedthatVARoutperformstheDiffusionTransformer(DiT)in
multipledimensionsincludingimagequality,inferencespeed,dataefficiency,and
scalability.ScalingupVARmodelsexhibitsclearpower-lawscalinglawssimilarto
thoseobservedinLLMs,withlinearcorrelationcoefficientsnearâˆ’0.998assolid
evidence. VARfurthershowcaseszero-shotgeneralizationabilityindownstream
tasksincludingimagein-painting,out-painting,andediting. Theseresultssuggest
VARhasinitiallyemulatedthetwoimportantpropertiesofLLMs: ScalingLaws
andzero-shotgeneralization. Wehavereleasedallmodelsandcodestopromote
theexplorationofAR/VARmodelsforvisualgenerationandunifiedlearning.
âˆ—Correspondingauthors: wanglw@pku.edu.cn,yuanzehuan@bytedance.com; â€ :projectlead
Preprintversion.
4202
rpA
3
]VC.sc[
1v50920.4042:viXraFigure2: Standardautoregressivemodeling(AR)vs.ourproposedvisualautoregressivemodeling(VAR).
(a)ARappliedtolanguage:sequentialtexttokengenerationfromlefttoright,wordbyword;(b)ARapplied
toimages:sequentialvisualtokengenerationinaraster-scanorder,fromlefttoright,toptobottom;(c)VAR
forimages:multi-scaletokenmapsareautoregressivelygeneratedfromcoarsetofinescales(lowertohigher
resolutions),withparalleltokengenerationwithineachscale.VARrequiresamulti-scaleVQVAEtowork.
1 Introduction
TheadventofGPTseries[48,49,9,45,1]andotherautoregressive(AR)largelanguagemodels
(LLMs)[13,3,27,63,64,70,60,4,61]hasheraldedanewepochinthefieldofartificialintelligence.
These models exhibit promising intelligence in generality and versatility that, despite issues like
hallucinations[28],arestillconsideredtotakeasolidsteptowardthegeneralartificialintelligence
(AGI).Thecruxbehindtheselargemodelsisaself-supervisedlearningstrategyâ€“predictingthenext
tokeninasequence,asimpleyetprofoundapproach. StudiesintothesuccessoftheselargeAR
modelshavehighlightedtheirscalabilityandgeneralizabilty: theformer,asexemplifiedbyscaling
laws[30,22], allowsustopredictlargemodelâ€™sperformancefromsmalleronesandthusguides
betterresourceallocation,whilethelatter,asevidencedbyzero-shotandfew-shotlearning[49,9],
underscorestheunsupervised-trainedmodelsâ€™adaptabilitytodiverse,unseentasks. Theseproperties
revealARmodelsâ€™potentialinlearningfromvastunlabeleddata,encapsulatingtheessenceofâ€œAGIâ€.
Inparallel,thefieldofcomputervisionhasbeenstrivingtodeveloplargeautoregressiveorworld
models[42,68,5],aimingtoemulatetheirimpressivescalabilityandgeneralizability. Trailblazing
effortslikeVQGANandDALL-E[19,51]alongwiththeirsuccessors[52,71,37,77]haveshowcased
thepotentialofARmodelsinimagegeneration. Thesemodelsutilizeavisualtokenizertodiscretize
continuousimagesintogridsof2Dtokens,whicharethenflattenedtoa1DsequenceforARlearning
(Fig.2b),mirroringtheprocessofsequentiallanguagemodeling(Fig.2a). However,thescalinglaws
ofthesemodelsremainunderexplored,andmorefrustratingly,theirperformancesignificantlylags
behinddiffusionmodels[46,2,38],asshowninFig.3. Incontrasttotheremarkableachievements
ofLLMs,thepowerofautoregressivemodelsincomputervisionappearstobesomewhatlocked.
Autoregressivemodelingrequiresdefiningtheor-
derofdata. Ourworkreconsidershowtoâ€œorderâ€
animage. Humanstypicallyperceiveorcreate 0.3B 1B 2B 5B
images in a hierachical manner, first capturing AR(VQGAN)
t
m
gh eue
sl
tg
t
sil
-
ao
s
nb caa â€œll
oe
rs
,
dt cr eu
o
rc
a
â€t
r
fu
s
oer re
-t
ioa m-n
fi
ad gnt eeh sme .n Aetl lho
so
oc da il nnd
sa
pe
t
iut ra
r
ei
a
dl ls
l
b.
y
yT
s
th
u
hi
g
es
-
Gigagan
AR(RQ)
re
tte
b
si
re
w
widespread multi-scale designs [40, 39, 62, 31, RCG o
l
33],wedefineautoregressivelearningforimages DiT
as â€œnext-scale predictionâ€ in Fig. 2 (c), diverg-
VAR (ours)
ingfromtheconventionalâ€œnext-tokenpredictionâ€
inFig.2(b). Ourapproachbeginsbyencoding
an image into multi-scale token maps. The au-
toregressiveprocessisthenstartedfromthe1Ã—1
tokenmap,andprogressivelyexpandsinresolu-
tion: at each step, the transformer predicts the Figure 3: Scalingbehaviorofdifferentmodelson
ImageNet256Ã—256conditionalgenerationbenchmark.
nexthigher-resolutiontokenmapconditionedon
TheFIDofthevalidationsetservesasareferencelower
allpreviousones. Werefertothismethodology
bound(1.78). VARwith2BparametersreachesFID
asVisualAutoRegressive(VAR)modeling.
1.80,surpassingL-DiTwith3Bor7Bparameters.
2VARdirectlyleveragesGPT-2-liketransformerarchitecture[49]forvisualautoregressivelearning.
OntheImageNet256Ã—256benchmark,VARsignificantlyimprovesitsARbaseline,achievinga
FrÃ©chetinceptiondistance(FID)of1.80andaninceptionscore(IS)of356.4,withinferencespeed
20Ã—faster(seeSec.4.4fordetails). Notably,VARsurpassestheDiffusionTransformer(DiT)â€“the
foundationofleadingdiffusionsystemslikeStableDiffusion3.0andSORA[18,8]â€“inFID/IS,
dataefficiency,inferencespeed,andscalability. VARmodelsalsoexhibitscalinglawsakintothose
witnessedinLLMs. Lastly,weshowcaseVARâ€™szero-shotgeneralizationcapabilitiesintaskslike
imagein-painting,out-painting,andediting. Insummary,ourcontributionstothecommunityinclude:
1. Anewvisualgenerativeframeworkusingamulti-scaleautoregressiveparadigmwithnext-scale
prediction,offeringnewinsightsinautoregressivealgorithmdesignforcomputervision.
2. AnempiricalvalidationofVARmodelsâ€™ScalingLawsandzero-shotgeneralizationpotential,
whichinitiallyemulatestheappealingpropertiesoflargelanguagemodels(LLMs).
3. Abreakthroughinvisualautoregressivemodelperformance,makingGPT-styleautoregressive
methodssurpassstrongdiffusionmodelsinimagesynthesisforthefirsttime2.
4. Acomprehensiveopen-sourcecodesuite,includingbothVQtokenizerandautoregressivemodel
trainingpipelines,tohelppropeltheadvancementofvisualautoregressivelearning.
2 RelatedWork
2.1 Propertiesoflargeautoregressivelanguagemodels
Scalinglaws. Power-lawscalinglaws[22,30]mathematicallydescribetherelationshipbetween
thegrowthofmodelparameters,datasetsizes,computationalresources,andtheperformanceim-
provementsofmachinelearningmodels,conferringseveraldistinctbenefits. First,theyfacilitate
theextrapolationofalargermodelâ€™sperformancethroughthescalingupofmodelsize,datasize,
andcomputationalcost. Thishelpssaveunnecessarycostsandprovidesprinciplestoallocatethe
trainingbudget. Second,thescalinglawshaveevidencedaconsistentandnon-saturatingincrease
inperformance,corroboratingtheirsustainedadvantageinenhancingmodelcapability. Propelled
by the principles of scaling laws in neural language models [30], several Large Language Mod-
els[9,76,70,27,63,64]havebeenproposed,embodyingtheprinciplethatincreasingthescaleof
modelstendstoyieldenhancedperformanceoutcomes. GPT[49,9],predicatedonatransformer
decoderarchitecture,undergoesgenerativepre-trainingandscalesthemodelsizetoanunprecedented
175Bparameters. LLama[63,64]releaseacollectionofpre-trainedandfine-tunedlargelanguage
models(LLMs)ranginginscalefrom7billionto70billionparameters. Themanifestefficacyof
scaling laws applied to language models has proffered a glimpse into the promising potential of
upscaleinvisualmodels[5].
Zero-shotgeneralization. Zero-shotgeneralization[55]referstotheabilityofamodel,particularly
a Large Language Model, to perform tasks that it has not been explicitly trained on. Within the
realmofthevisioncommunity,thereisaburgeoninginterestinthezero-shotandin-contextlearning
abilitiesoffoundationmodels,CLIP[47],SAM[35],Dinov2[44]. InnovationslikePainter[69]and
LVM[5]haveleveragedvisualpromptstodesignin-contextlearningparadigms,therebyfacilitating
thegeneralizationtodownstreamunseentasks.
2.2 Visualgeneration
Image tokenizer and autoregressive models. Language models [15, 49, 63] rely on Byte Pair
Encoding(BPE[20])orWordPiecealgorithmsfortexttokenization. Visualgenerationmodelsbased
on language models also necessitate the encoding of 2D images into 1D token sequences. Early
endeavorsVQVAE[65]havedemonstratedtheabilitytorepresentimagesasdiscretetokens,although
thereconstructionqualitywasrelativelymoderate. VQGAN[19]advancesVQVAEbyincorporating
adversariallossandperceptuallosstoimproveimagefidelity,andemploysadecoder-onlytransformer
togenerateimagetokensinstandardraster-scanautoregressivemanner. VQVAE-2[52]andRQ-
Transformer [37] also follow VQGANâ€™s raster-scan autoregressive method but further improve
VQVAEviaextrascalesorstackedcodes. Parti[72]capitalizesonthefoundationalarchitectureof
ViT-VQGAN[71]toscalethetransformermodelsizeto20billionparameters,achievingremarkable
resultsintext-to-imagesynthesis.
2Arelatedwork[74]namedâ€œlanguagemodelbeatsdiffusionâ€belongstoBERT-stylemasked-predictionmodel.
3Masked-predictionmodel. MaskGIT[11]employsamaskedpredictionframework[15,6,21]along-
sideaVQautoencodertogenerateimagetokensbasedthroughaâ€œgreedyâ€algorithm. MagViT[73]
adaptsthisapproachtovideodata,andMagViT-2[74]enhancesMaskGITbyintroducinganim-
provedVQVAE.MUSE[10]scalesMaskGITâ€™sarchitectureto3billionparametersandmergesit
withtheT5languagemodel[50],settingnewbenchmarksintext-to-imagesynthesis.
Diffusionmodels [23,59,16,53]areconsideredtheforefrontofvisualsynthesis,giventheirsuperior
generationqualityanddiversity.Progressindiffusionmodelshascenteredaroundimprovedsampling
techniques[26],fastersampling[58,41],andarchitecturalenhancements[53,24,54,46].Imagen[54]
incorporatestheT5languagemodel[50]fortextconditionandbuildsanimagegenerationsystem
throughmultipleindependentdiffusionmodelsforcascadedgenerationandsuperresolution. Latent
DiffusionModels(LDM)[53]applydiffusioninlatentspace,improvingefficiencyintrainingand
inference. DiT[46]replacesthetraditionalU-Netwithatransformer-basedarchitecture[66,17],and
isusedinrecentimageorvideosynthesissystemslikeStableDiffusion3.0[18]andSORA[8].
3 Method
3.1 Preliminary: autoregressivemodelingvianext-tokenprediction
Formulation. Consider a sequence of discrete tokens x = (x ,x ,...,x ), where each token
1 2 T
x âˆˆ[V]isanintegerfromavocabularyofsizeV. Thenext-tokenautoregressivemodelpositsthat
t
theprobabilityofobservingthecurrenttokenx dependsonlyonitsprefix(x ,x ,...,x ). This
t 1 2 tâˆ’1
assumptionofunidirectionaltokendependencyallowsustodecomposethelikelihoodofsequence
xintotheproductofT conditionalprobabilities:
T
(cid:89)
p(x ,x ,...,x )= p(x |x ,x ,...,x ). (1)
1 2 T t 1 2 tâˆ’1
t=1
Training an autoregressive model p parameterized by Î¸ involves optimizing the p (x |
Î¸ Î¸ t
x ,x ,...,x )acrossadataset. Thisoptimizationprocessisknownastheâ€œnext-tokenpredictionâ€,
1 2 tâˆ’1
andthetrainedp cangeneratenewsequences.
Î¸
Tokenization. Imagesareinherently2Dcontinuoussignals. Toapplyautoregressivemodelingto
imagesvianext-tokenprediction,wemust: 1)tokenizeanimageintoseveraldiscretetokens,and
2)definea1Dorderoftokensforunidirectionalmodeling. For1),aquantizedautoencodersuch
as[19]isoftenusedtoconverttheimagefeaturemapf âˆˆRhÃ—wÃ—C todiscretetokensq âˆˆ[V]hÃ—w:
f =E(im), q =Q(f), (2)
where im denotes the raw image, E(Â·) a encoder, and Q(Â·) a quantizer. The quantizer typically
includesalearnablecodebookZ âˆˆRVÃ—C containingV vectors. Thequantizationprocessq =Q(f)
willmapeachfeaturevectorf(i,j)tothecodeindexq(i,j)ofitsnearestcodeintheEuclideansense:
(cid:32) (cid:33)
q(i,j) = argminâˆ¥lookup(Z,v)âˆ’f(i,j)âˆ¥ âˆˆ[V], (3)
2
vâˆˆ[V]
wherelookup(Z,v)meanstakingthev-thvectorincodebookZ. Totrainthequantizedautoencoder,
Z islookedupbyeveryq(i,j) togetfË†,theapproximationoforiginalf. ThenanewimageimË† is
reconstructedusingthedecoderD(Â·)givenfË†,andacompoundlossLisminimized:
fË†=lookup(Z,q), imË† =D(fË†), (4)
L=âˆ¥imâˆ’imË† âˆ¥ +âˆ¥f âˆ’fË†âˆ¥ +Î» L (imË† )+Î» L (imË† ), (5)
2 2 P P G G
whereL (Â·)isaperceptuallosssuchasLPIPS[75],L (Â·)adiscriminativelosslikeStyleGANâ€™s
P G
discriminatorloss[33],andÎ» ,Î» arelossweights. Oncetheautoencoder{E,Q,D}isfullytrained,
P G
itwillbeusedtotokenizeimagesforsubsequenttrainingofaunidirectionalautoregressivemodel.
Theimagetokensinq âˆˆ[V]hÃ—warearrangedina2Dgrid.Unlikenaturallanguagesentenceswithan
inherentleft-to-rightordering,theorderofimagetokensmustbeexplicitlydefinedforunidirectional
autoregressivelearning. PreviousARmethods[19,71,37]flattenthe2Dgridofqintoa1Dsequence
x = (x ,...,x ) using some strategy such as row-major raster scan, spiral, or z-curve order.
1 hÃ—w
Onceflattened,theycanextractasetofsequencesxfromthedataset,andthentrainanautoregressive
modeltomaximizethelikelihoodin(1)vianext-tokenprediction.
4Stage 1: Training multi-scale VQVAE on images Stage 2: Training VAR transformer on tokens
( to provide the ground truth for Stage 2â€™s training ) ([S] means a start token w/orw/o condition information)
ð‘ð‘Ÿ1,ð‘Ÿ2,â€¦ð‘Ÿð¾ =
ð¾
à·‘ð‘ ð‘Ÿð‘˜ ð‘Ÿ1,â€¦ð‘Ÿð‘˜âˆ’1 ,
ð‘˜=1
where ð‘ð‘Ÿ0 =ð›¿ [S].
VAE encoding Multi-scale quantization & Embedding Decoding VAR next-scale prediction from [S]
Figure4: VARinvolvestwoseparatedtrainingstages.Left:amulti-scalequantizedautoencoder(VQVAE)
encodeanimageintoKtokenmapsR=(r ,r ,...,r )andistrainedbyacompoundreconstructionloss(5).
1 2 K
â€œEmbeddingâ€inthefiguremeansconvertingdiscretetokensintocontinuousembeddingvectors.Right:aVAR
transformeristrainedbymaximizingthelikelihoodorminimizingthecross-entropylossin(6)vianext-scale
prediction.thetrainedVQVAEisusedtoproducetokenmapgroundtruthR=(r ,r ,...,r )forVARmodel.
1 2 K
Discussion. Theabovetokenizingandflatteningenablenext-tokenautoregressivelearningonimages,
buttheyintroduceseveralissues:
1) Mathematical premise violation. Image encoders typically produce image feature map f
withinter-dependentfeaturevectorsf(i,j)foralli,j. Soafterquantizationandflattening,the
sequenceoftokens(x ,x ,...,x )exhibitsbidirectionalcorrelations. Thiscontradictsthe
1 2 hÃ—w
unidirectionaldependencyassumptionofautoregressivemodels,whichdictatesthateachtoken
x shouldonlydependonitsprefix(x ,x ,...,x ).
t 1 2 tâˆ’1
2) Structuraldegradation. Theflatteningdisruptsthespatiallocalityinherentinimagefeature
maps. Forinstance,thetokenq(i,j)andits4immediateneighborsq(iÂ±1,j),q(i,jÂ±1)areclosely
correlatedduetotheirproximity. Thisspatialrelationshipiscompromisedinthelinearsequence
x,whereunidirectionalconstraintsdiminishthesecorrelations.
3) Inefficiency. Generatinganimagetokensequencex=(x ,x ,...,x )withaconventional
1 2 nÃ—n
self-attentiontransformerincursO(n2)autoregressivestepsandO(n6)computationalcost.
Thedisruptionofspatiallocality(issue2)isobvious.Regardingissue1,wepresentempiricalevidence
intheAppendix,analyzingthetokendependenciesinthepopularquantizedautoencoder[19]and
revealing significant bidirectional correlations. The proof of computational complexity for issue
3 is detailed in the Appendix. These theoretical and practical limitations call for a rethinking of
autoregressivemodelsinthecontextofimagegeneration.
3.2 Visualautoregressivemodelingvianext-scaleprediction
Reformulation. Wereconceptualizetheautoregressivemodelingonimagesbyshiftingfromâ€œnext-
tokenpredictionâ€toâ€œnext-scalepredictionâ€strategy. Here,theautoregressiveunitisanentiretoken
map,ratherthanasingletoken.Westartbyquantizingafeaturemapf âˆˆRhÃ—wÃ—C intoKmulti-scale
tokenmaps(r ,r ,...,r ),eachataincreasinglyhigherresolutionh Ã—w ,culminatinginr
1 2 K k k K
matchestheoriginalfeaturemapâ€™sresolutionhÃ—w. Theautoregressivelikelihoodisformulatedas:
K
(cid:89)
p(r ,r ,...,r )= p(r |r ,r ,...,r ), (6)
1 2 K k 1 2 kâˆ’1
k=1
where each autoregressive unit r
k
âˆˆ [V]hkÃ—wk is the token map at scale k, and the sequence
(r ,r ,...,r )servesasthetheâ€œprefixâ€forr . Duringthek-thautoregressivestep,alldistribu-
1 2 kâˆ’1 k
tionsovertheh Ã—w tokensinr areinter-dependentandwillbegeneratedinparallel,conditioned
k k k
onr â€™sprefixandassociatedk-thpositionembeddingmap.Thisâ€œnext-scalepredictionâ€methodology
k
iswhatwedefineasvisualautoregressivemodeling(VAR),depictedontherightsideofFig.4.
Discussion. VARaddressesthepreviouslymentionedthreeissuesasfollows:
1) Themathematicalpremiseissatisfiedifweconstraineachr todependonlyonitsprefix,thatis,
k
theprocessofgettingr issolelyrelatedto(r ,r ,...,r ). Thisconstraintisacceptableas
k 1 2 kâˆ’1
italignswiththenatural,coarse-to-fineprogressioncharacteristicslikehumanvisualperception
andartisticdrawing. FurtherdetailsareprovidedintheTokenizationbelow.
52) Thespatiallocalityispreservedas(i)thereisnoflatteningoperationinVAR,and(ii)tokensin
eachr arefullycorrelated. Themulti-scaledesignadditionallyreinforcesthespatialstructure.
k
3) ThecomplexityforgeneratinganimagewithnÃ—nlatentissignificantlyreducedtoO(n4),see
Appendixforproof. Thisefficiencygainarisesfromtheparalleltokengenerationineachr .
k
Tokenization. Wedevelopeanewmulti-scalequantizationautoencodertoencodeanimagetoK
multi-scalediscretetokenmapsR=(r ,r ,...,r )necessaryforVARlearning(6). Weemploy
1 2 K
the same architecture as VQGAN [19] but with a modified multi-scale quantization layer. The
encodinganddecodingprocedureswithresidualdesignonf orfË†aredetailedinalgorithms1and
2. Weempiricallyfindthisresidual-styledesign,akinto[37],canperformbetterthanindependent
interpolation. Algorithm1showsthateachr woulddependonlyonitsprefix(r ,r ,...,r ).
k 1 2 kâˆ’1
NotethatasharedcodebookZ isutilizedacrossallscales,ensuringthateachr â€™stokensbelongto
k
thesamevocabulary[V]. Toaddresstheinformationlossinupscalingz toh Ã—w ,weuseK
k K K
extraconvolutionlayers{Ï• }K . Noconvolutionisusedafterdownsamplingf toh Ã—w .
k k=1 k k
Algorithm1: Multi-scaleVQVAEEncoding Algorithm2:Multi-scaleVQVAEReconstruction
1 Inputs: rawimageim; 1 Inputs: multi-scaletokenmapsR;
2 Hyperparameters: stepsK,resolutions 2 Hyperparameters: stepsK,resolutions
3
f(h =k, Ew (ik m)K k )= ,1 R;
=[]; 3
fË†(h =k, 0w ;k)K k=1;
4 fork=1,Â·Â·Â·,Kdo 4 fork=1,Â·Â·Â·,Kdo
5 rk=Q(interpolate(f,hk,wk)); 5 rk=queue_pop(R);
6 R=queue_push(R,rk); 6 zk=lookup(Z,rk);
7
8
z zk k= =l io no tek ru pp o( laZ te, (r zk k) ,;
hK,wK);
7
8
fz Ë†k == fË†in +te Ï•rp ko (l za kte )( ;zk,hK,wK);
9 f =fâˆ’Ï•k(zk); 9 imË† =D(fË†);
10 Return: multi-scaletokensR; 10 Return: reconstructedimageimË†;
3.3 Implementationdetails
VARtokenizer. Asaforementioned,weusethevanillaVQVAEarchitecture[19]withamulti-scale
quantizationschemewithK extraconvolutions(0.03Mextraparameters). Weuseasharedcodebook
forallscaleswithV = 4096andalatentdimof32. Followingthebaseline[19],ourtokenizeris
alsotrainedonOpenImages[36]withthecompoundloss(5). SeetheAppendixformoredetails.
VARtransformer. OurmainfocusisonVARalgorithmsowekeepasimplemodelarchitecture
design. Weadoptthearchitectureofstandarddecoder-onlytransformersakintoGPT-2andVQ-
GAN[49,19],withtheonlymodificationofsubstitutingtraditionallayernormalizationforadaptive
normalization(AdaLN)â€“achoicemotivatedbyitswidespreadadoptionandproveneffectivenessin
visualgenerativemodels[33,34,32,57,56,29,46,12]. Forclass-conditionalsynthesis,weusethe
classembeddingasthestarttoken[s]andalsotheconditionofAdaLN.Wedonotuseadvanced
techniquesinmodernlargelanguagemodels,suchasrotarypositionembedding(RoPE),SwiGLU
MLP,orRMSNorm[63,64]. Ourmodelshapehyperparameterfollowsasimplerule[30]thatthe
widthw,headcountsh,anddropratedrarelinearlyscaledwiththedepthdasfollows:
w =64d, h=d, dr =0.1Â·d/24. (7)
Consequently,themainparametercountN ofaVARtransformerwithdepthdisgivenby3:
N(d)= dÂ·4w2 + dÂ·8w2 + dÂ·6w2 =18dw2 =73728d3. (8)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
self-attention feed-forward adaptivelayernorm
Allmodelsaretrainedwiththesimilarsettings: abaselearningrateof10âˆ’4per256batchsize,an
AdamW optimizer with Î² = 0.9, Î² = 0.95, decay = 0.05, a batch size from 512 to 1024 and
1 2
trainingepochsfrom200to350(dependsonmodelsize). OursubsequentevaluationsinSec.4will
suggestthatsuchasimplemodeldesignarecapableofscalingandgeneralizingwell.
3Duetoresourcelimitation,weuseasinglesharedadaptivelayernorm(AdaLN)acorssallattentionblocks
in512Ã—512synthesis.Inthiscase,theparametercountwouldbereducedtoaround12dw2+6w2 â‰ˆ49152d3.
6Table1: Generativemodelfamilycomparisononclass-conditionalImageNet256Ã—256.â€œâ†“â€orâ€œâ†‘â€indicate
lowerorhighervaluesarebetter.MetricsincludeFrÃ©chetinceptiondistance(FID),inceptionscore(IS),precision
(Pre)andrecall(rec).â€œ#Stepâ€:thenumberofmodelrunsneededtogenerateanimage.Wall-clockinferencetime
relativetoVARisreported.Modelswiththesuffixâ€œ-reâ€usedrejectionsampling.â€ :takenfromMaskGIT[11].
Type Model FIDâ†“ ISâ†‘ Preâ†‘ Recâ†‘ #Para #Step Time
GAN BigGAN[7] 6.95 224.5 0.89 0.38 112M 1 âˆ’
GAN GigaGAN[29] 3.45 225.5 0.84 0.61 569M 1 âˆ’
GAN StyleGan-XL[57] 2.30 265.1 0.78 0.53 166M 1 0.3[57]
Diff. ADM[16] 10.94 101.0 0.69 0.63 554M 250 168[57]
Diff. CDM[25] 4.88 158.7 âˆ’ âˆ’ âˆ’ 8100 âˆ’
Diff. LDM-4-G[53] 3.60 247.7 âˆ’ âˆ’ 400M 250 âˆ’
Diff. DiT-L/2[46] 5.02 167.2 0.75 0.57 458M 250 31
Diff. DiT-XL/2[46] 2.27 278.2 0.83 0.57 675M 250 45
Diff. L-DiT-3B[2] 2.10 304.4 0.82 0.60 3.0B 250 >45
Diff. L-DiT-7B[2] 2.28 316.2 0.83 0.58 7.0B 250 >45
Mask. MaskGIT[11] 6.18 182.1 0.80 0.51 227M 8 0.5[11]
Mask. MaskGIT-re[11] 4.02 355.6 âˆ’ âˆ’ 227M 8 0.5[11]
Mask. RCG(cond.)[38] 3.49 215.5 âˆ’ âˆ’ 502M 20 1.9[38]
AR VQVAE-2â€ [52] 31.11 âˆ¼45 0.36 0.57 13.5B 5120 âˆ’
AR VQGANâ€ [19] 18.65 80.4 0.78 0.26 227M 256 19[11]
AR VQGAN[19] 15.78 74.3 âˆ’ âˆ’ 1.4B 256 24
AR VQGAN-re[19] 5.20 280.3 âˆ’ âˆ’ 1.4B 256 24
AR ViTVQ[71] 4.17 175.1 âˆ’ âˆ’ 1.7B 1024 >24
AR ViTVQ-re[71] 3.04 227.4 âˆ’ âˆ’ 1.7B 1024 >24
AR RQTran.[37] 7.55 134.0 âˆ’ âˆ’ 3.8B 68 21
AR RQTran.-re[37] 3.80 323.7 âˆ’ âˆ’ 3.8B 68 21
VAR VAR-d16 3.60 257.5 0.85 0.48 310M 10 0.4
VAR VAR-d20 2.95 306.1 0.84 0.53 600M 10 0.5
VAR VAR-d24 2.33 320.1 0.82 0.57 1.0B 10 0.6
VAR VAR-d30 1.97 334.7 0.81 0.61 2.0B 10 1
VAR VAR-d30-re 1.80 356.4 0.83 0.57 2.0B 10 1
(validationdata) 1.78 236.9 0.75 0.67
4 EmpiricalResults
ThissectionfirstcomparesVARwithotherimagegenerativemodelfamilies4intermsofperformance
andandefficiencyinSec.4.1. EvaluationsonthescalabilityandgeneralizabilityofVARmodelsare
presentedinSec.4.2andSec.4.3. Wedosomeablationsandvisualizationsattheend.
4.1 State-of-the-artimagegeneration
Setup. WetestVARmodelswithdepths16,20,24,and30onImageNet256Ã—256and512Ã—512
conditionalgenerationbenchmarksandcomparethemwiththestate-of-the-artimagegeneration
modelfamilies. AmongallVQVAE-basedARorVARmodels,VQGAN[19]andoursusethesame
architecture(CNN)andtrainingdata(OpenImages[36])forVQVAE,whileViT-VQGAN[71]uses
aViTautoencoder,andbothitandRQTransformer[37]trainstheVQVAEdirectlyonImageNet.
TheresultsaresummariedinTab.1andTab.2.
Overall comparison. In comparison with existing generative approaches including generative
adversarialnetworks(GAN),diffusionmodels(Diff.),BERT-stylemasked-predictionmodels(Mask.),
andGPT-styleautoregressivemodels(AR),ourvisualautoregressive(VAR)establishesanewmodel
class. AsshowninTab.1,VARnotonlyachievesthebestFID/ISbutalsodemonstratesremarkable
speedinimagegeneration. VARalsomaintainsdecentprecisionandrecall,confirmingitssemantic
consistency. Theseadvantagesholdtrueonthe512Ã—512synthesisbenchmark,asdetailedinTab.2.
Notably,VARsignificantlyadvancestraditionalARcapabilities. Toourknowledge,thisisthefirst
timeofautoregressivemodelsoutperformingDiffusiontransformers,amilestonemadepossibleby
VARâ€™sresolutionofARlimitationsdiscussedinSection3.
4For fairness, methods with advanced VQVAE are excluded in this model family comparison, e.g. the
BERT-stylemasked-predmodelMagViT-2[74].PractitionerscancombinethemwithVARforbetterresults.
7Efficiency comparison. Conventional autore-
Table2: ImageNet512Ã—512conditionalgeneration.
gressive (AR) models [19, 52, 71, 37] suffer a
â€ : quotedfromMaskGIT[11]. â€œ-sâ€: asingleshared
lotfromthehighcomputationalcost,asthenum-
AdaLNlayerisusedduetoresourcelimitation.
ber of image tokens is quadratic to the image
resolution. A full autoregressive generation of Type Model FIDâ†“ ISâ†‘ Time
n2 tokens requires O(n2) decoding iterations
GAN BigGAN[7] 8.43 177.9 âˆ’
andO(n6)totalcomputations. Incontrast,VAR
onlyrequiresO(log(n))iterationsandO(n4)to- Diff. ADM[16] 23.24 101.0 âˆ’
talcomputations. Thewall-clocktimereported Diff. DiT-XL/2[46] 3.04 240.8 81
in Tab. 1 also provides empirical evidence that
Mask. MaskGIT[11] 7.32 156.0 0.5
VAR is around 20 times faster than VQGAN
andViT-VQGANevenwithmoremodelparame- AR VQGANâ€ [19] 26.52 66.8 25
ters,reachingthespeedofefficientGANmodels VAR VAR-d36-s 2.63 303.2 1
whichonlyrequire1steptogenerateanimage.
Comparedwithpopulardiffusiontransformer.5 TheVARmodelsurpassestherecentlypopular
diffusionmodelsDiffusionTransformer(DiT),whichservesastheprecursortothelatestStable-
Diffusion3[18]andSORA[8],inmultipledimensions: 1)Inimagegenerationdiversityandquality
(FIDandIS),VARwith2BparametersconsistentlyperformsbetterthanDiT-XL/2[46],L-DiT-3B,
andL-DiT-7B[2]. VARalsomaintainscomparableprecisionandrecall. 2)Forinferencespeed,
the DiT-XL/2 requires 45Ã— the wall-clock time compared to VAR, while 3B and 7B models [2]
wouldcostmuchmore. 3)VARisconsideredmoredata-efficient,asitrequiresonly350training
epochscomparedtoDiT-XL/2â€™s1400. 4)Forscalability,Fig.3andTab.1showthatDiTonlyobtains
marginalorevennegativegainsbeyond675Mparameters. Incontrast,theFIDandISofVARare
consistentlyimproved,aligningwiththescalinglawstudyinSec.4.2. TheseresultsestablishVARas
amoreefficientandscalablemodelforimagegenerationthanmodelslikeDiT.
4.2 Power-lawscalinglaws
Background. Priorresearch[30,22,27,1]haveestablishedthatscalingupautoregressive(AR)large
languagemodels(LLMs)leadstoapredictabledecreaseintestlossL. Thistrendcorrelateswith
parametercountsN,trainingtokensT,andoptimaltrainingcomputeC ,followingapower-law:
min
L=(Î²Â·X)Î±, (9)
whereX canbeanyofN,T,orC . TheexponentÎ±reflectsthesmoothnessofpower-law,andL
min
denotesthereduciblelossnormalizedbyirreduciblelossL [22]6. Alogarithmictransformationto
âˆž
LandX willrevealalinearrelationbetweenlog(L)andlog(X):
log(L)=Î±log(X)+Î±logÎ². (10)
Theseobservedscalinglaws[30,22,27,1]notonlyvalidatethescalabilityofLLMsbutalsoserve
as a predictive tool for AR modeling, which facilitates the estimation of performance for larger
AR models based on their smaller counterparts, thereby saving resource usage by large model
performanceforecasting. GiventheseappealingpropertiesofscalinglawsbroughtbyLLMs,their
replicationincomputervisionisthereforeofsignificantinterest.
SetupofscalingVARmodels. Followingtheprotocolsfrom[30,22,27,1],weexaminewhether
ourVARmodelcomplieswithsimilarscalinglaws. Wetrainedmodelsacross12differentsizes,
from18Mto2Bparameters,ontheImageNettrainingset[14]containing1.28Mimages(or870B
imagetokensunderourVQVAE)perepoch. Formodelsofdifferentsizes,trainingspanned200to
350epochs,withamaximumnumberoftokensreaching305billion. Belowwefocusonthescaling
lawswithmodelparametersN andoptimaltrainingcomputeC givensufficienttokencountT.
min
ScalinglawswithmodelparametersN. WefirstinvestigatethetestlosstrendastheVARmodel
sizeincreases. ThenumberofparametersN(d)=73728d3foraVARtransformerwithdepthdis
specifiedin(8). Wevarieddfrom6to30,yielding12modelswith18.5Mto2.0Bparameters. We
assessedthefinaltestcross-entropylossLandtokenpredictionerrorratesErr ontheImageNet
validationsetof50,000images[14]. WecomputedLandErrforboththelastscale(atthelastnext-
scaleautoregressivestep),aswellastheglobalaverage. TheresultsareplottedinFig.5,wherewe
5[46]donotreportDiT-L/2â€™sperformancewithCFGsoweuseofficialcodetoreproduceit.
6See[22]forsometheoreticalexplanationonscalinglawsonnegative-loglikelihoodlosses.
8  D    E    F    G 
     L=(2.0 N)0.23      L=(2.5 N)0.20      Err=(5 102Npara 0.02      Err=(6 102Npara 0.01
 & R U U H O D  =         & R U U H O D  =         & R U U H O D  =         & R U U H O D  =       
                   
                   
                   
                   
                               
 0 R G H O  3 D U D P H W H U V   % L O O L R Q   0 R G H O  3 D U D P H W H U V   % L O O L R Q   0 R G H O  3 D U D P H W H U V   % L O O L R Q   0 R G H O  3 D U D P H W H U V   % L O O L R Q 
Figure5: ScalinglawswithVARtransformersizeN,withpower-lawfits(dashed)andequations(inlegend).
Small,near-zeroexponentsÎ±suggestasmoothdeclineinbothtestlossLandtokenerrorrateErrwhenscaling
upVARtransformer. Axesareallonalogarithmicscale. ThePearsoncorrelationcoefficientsnearâˆ’0.998
signifyastronglinearrelationshipbetweenlog(N)vs.log(L)orlog(N)vs.log(Err).
observedaclearpower-lawscalingtrendforLasafunctionofN,asconsistentwith[30,22,27,1].
Thepower-lawscalinglawscanbeexpressedas:
L =(2.0Â·N)âˆ’0.23 and L =(2.5Â·N)âˆ’0.20. (11)
last avg
Althoughthescalinglawsaremainlystudiedonthetestloss,wealsoempiricallyobservedsimilar
power-lawtrendsforthetokenerrorrateErr:
Err =(4.9Â·102N)âˆ’0.016 and Err =(6.5Â·102N)âˆ’0.010. (12)
last avg
These results verify the strong scalability of VAR, by which scaling up VAR transformers can
continuouslyimprovethemodelâ€™stestperformance.
ScalinglawswithoptimaltrainingcomputeC . WethenexaminethescalingbehaviorofVAR
min
transformerswhenincreasingtrainingcomputeC. Foreachofthe12models,wetracedthetestloss
LandtokenerrorrateErrasafunctionofC duringtrainingquotedinPFlops(1015floating-point
operationspersecond). TheresultsareplottedinFig.6. Here,wedrawtheParetofrontierofLand
ErrtohighlighttheoptimaltrainingcomputeC requiredtoreachacertainvalueoflossorerror.
min
Thefittedpower-lawscalinglawsforLandErrasafunctionofC are:
min
L =(2.2Â·10âˆ’5C )âˆ’0.13 andL =(1.5Â·10âˆ’5C )âˆ’0.16, (13)
last min avg min
Err =(8.1Â·10âˆ’2C )âˆ’0.0067 andErr =(4.4Â·10âˆ’2C )âˆ’0.011. (14)
last min avg min
         
         
         
      3 D U H W R  I U R Q W L H U Cmin       3 D U H W R  I U R Q W L H U Cmin
L=(2.2 105Cmin 0.13 L=(1.5 105Cmin 0.16
      & R U U H O D W L R Q =             & R U U H O D W L R Q =      
                               
         
         
         
      3 D U H W R  I U R Q W L H U Cmin       3 D U H W R  I U R Q W L H U Cmin
Err=(8.1 102Cmin 0.0067 Err=(4.4 102Cmin 0.011
      & R U U H O D W L R Q =             & R U U H O D W L R Q =      
                               
 7 U D L Q L Q J  & R P S X W H   3 ) O R S V   7 U D L Q L Q J  & R P S X W H   3 ) O R S V 
Figure6: ScalinglawswithoptimaltrainingcomputeC .Linecolordenotesdifferentmodelsizes.Red
min
dashedlinesarepower-lawfitswithequationsinlegend.Axesareonalogarithmicscale.Pearsoncoefficients
nearâˆ’0.99indicatestronglinearrelationshipsbetweenlog(C )vs.log(L)orlog(C )vs.log(Err).
min min
9
  H O D F V  W V D O   V V R O  W V H 7
  H O D F V  O O D   V V R O  W V H 7
     H O D F V  O O D   H W D U  U R U U H  Q H N R 7
  H O D F V  O O D   V V R O  W V H 7
     H O D F V  W V D O   H W D U  U R U U H  Q H N R 7
  H O D F V  W V D O   V V R O  W V H 7
     H O D F V  W V D O   H W D U  U R U U H  Q H N R 7
     H O D F V  O O D   H W D U  U R U U H  Q H N R 7Theserelations(13,14)holdacross6ordersofmagnitudeinC ,andourfindingsareconsistent
min
withthosein[30,22]: whentrainedwithsufficientdata,largerVARtransformersaremorecompute-
efficientbecausetheycanreachthesamelevelofperformancewithlesscomputation.
Visualizations. TobetterunderstandhowVARmodelsarelearningwhenscaledup,wecompare
somegenerated256Ã—256samplesfromVARmodelsof4differentsizes(depth6,16,26,30)and3
differenttrainingstages(20%,60%,100%oftotaltrainingtokens)inFig.7. Tokeepthecontent
consistent,asamerandomseedandteacher-forcedinitialtokensareused.Theobservedimprovements
invisualfidelityandsoundnessareconsistentwiththescalinglaws,aslargertransformersarethought
abletolearnmorecomplexandfine-grainedimagedistributions.
Figure7: ScalingmodelsizeN andtrainingcomputeCimprovesvisualfidelityandsoundness.Zoomin
forabetterview. SamplesaredrawnfromVARmodelsof4differentsizesand3differenttrainingstages. 9
classlabels(fromlefttoright,toptobottom)are:flamingo130,arcticwolf270,macaw88,Siamesecat284,
oscilloscope688,husky250,mollymawk146,volcano980,andcatamaran484.
104.3 Zero-shottaskgeneralization
Imagein-paintingandout-painting. VAR-d30istested. Forin-andout-painting,weteacher-force
groundtruthtokensoutsidethemaskandletthemodelonlygeneratetokenswithinthemask. No
class label information is injected into the model. The results are visualized in Fig. 8. Without
modificationstothenetworkarchitectureortuningparameters,VARhasachieveddecentresultson
thesedownstreamtasks,substantiatingthegeneralizationabilityofVAR.
Class-conditional image editing. Following MaskGIT [11] we also tested VAR on the class-
conditionalimageeditingtask. Similartothecaseofin-painting,themodelisforcedtogenerate
tokensonlyintheboundingboxconditionalonsomeclasslabel. Fig.8showsthemodelcanproduce
plausiblecontentthatfuseswellintothesurroundingcontexts,againverifyingthegeneralityofVAR.
original generated
In-painting
Out-painting
Class-cond
Editing
Figure 8: Zero-shot evaluation in downstream tasks containing in-painting, out-painting, and class-
conditional editing. The results show that VAR can generalize to novel downstream tasks without special
designandfinetuning.Zoominforabetterview.
4.4 AblationStudy
In this study, we aim to verify the effectiveness and efficiency of our proposed VAR framework.
ResultsarereportedinTab.3.
EffectivenessandefficiencyofVAR. StartingfromthevanillaARtransformerbaselineimplemented
by[11],wereplaceitsmethodologywithourVARandkeepothersettingsunchangedtogetrow2.
VARachievesawaymorebetterFID(18.65vs. 5.22)withonly0.013Ã—inferencewall-clockcost
thantheARmodel,whichdemonstratesaleapinvisualARmodelâ€™sperformanceandefficiency.
Component-wiseablation. WefurthertestsomekeycomponentsinVAR.Byreplacingthestandard
LayerNormalization(LN)withAdaptiveLayerNormalization(AdaLN),VARstartsyieldingbetter
FIDthanbaseline.Byusingthetop-ksamplingsimilartothebaseline,VARâ€™sFIDisfurtherimproved.
Byusingtheclassifier-freeguidance(CFG)withratio2.0,wereachtheFIDof3.60,whichis15.05
lowertothebaseline,anditsinferenceisstill45timesfaster. Duetotheobservedeffectiveness,we
equipourfinalVARmodelswithAdaLN,top-ksampling,andclassifier-freeguidance. Wefinally
scaleupVARsizeto2.0BandachieveanFIDof1.80. Thisis16.85betterthanthebaselineFID.
11Table3: AblationstudyofVAR.ThefirsttworowscompareGPT-2-styletransformerstrainedwithARor
VARalgorithm. SubsequentlinesshowtheinfluenceofVARenhancements. â€œAdaLNâ€:adaptivelayernorm.
â€œCFGâ€:classifier-freeguidance.â€œCostâ€:inferencecostrelativetothebaseline.â€œâˆ†â€:reductioninFID.
Description Para. Model AdaLN Top-k CFG Cost FIDâ†“ âˆ†
1 AR[11] 227M AR âœ— âœ— âœ— 1 18.65 0.00
2 ARtoVAR 207M VAR-d16 âœ— âœ— âœ— 0.013 5.22 âˆ’13.43
3 +AdaLN 310M VAR-d16 âœ“ âœ— âœ— 0.016 4.95 âˆ’13.70
4 +Top-k 310M VAR-d16 âœ“ 600 âœ— 0.016 4.64 âˆ’14.01
5 +CFG 310M VAR-d16 âœ“ 600 2.0 0.022 3.60 âˆ’15.05
6 +Scaleup 2.0B VAR-d30 âœ“ 600 2.0 0.052 1.80 âˆ’16.85
5 FutureWork
Inthiswork,wemainlyfocusonthedesignoflearningparadigmandkeeptheVQVAEarchitecture
andtrainingunchangedfromthebaseline[19]tobetterjustifyVARframeworkâ€™seffectiveness. We
expectadvancingVQVAEtokenizer[77,43,74]asanotherpromisingwaytoenhanceautoregressive
generativemodels,whichisorthogonaltoourwork. WebelieveiteratingVARbyadvancedtokenizer
orsamplingtechniquesintheselatestworkcanfurtherimproveVARâ€™sperformanceorspeed.
Text-promptgeneration isanongoingdirectionofourresearch. Giventhatourmodelisfunda-
mentallysimilartomodernLLMs,itcaneasilybeintegratedwiththemtoperformtext-to-image
generationthrougheitheranencoder-decoderorin-contextmanner. Thisiscurrentlyinourhigh
priorityforexploration.
Videogeneration isnotimplementedinthiswork,butitcanbenaturallyextended. Byconsidering
multi-scalevideofeaturesas3Dpyramids,wecanformulateasimilarâ€œ3Dnext-scalepredictionâ€
togeneratevideosviaVAR.Comparedtodiffusion-basedgeneratorslikeSORA[8],ourmethod
has inherent advantages in temporal consistency or integration with LLMs, thus can potentially
handlelongertemporaldependencies. ThismakesVARcompetitiveinthevideogenerationfield,
because traditional AR models can be too inefficient for video generation due to their extremely
highcomputationalcomplexityandslowinferencespeed: itisbecomingprohibitivelyexpensiveto
generatehigh-resolutionvideoswithtraditionalARmodels,whileVARiscapabletosolvethis. We
thereforeforeseeapromisingfutureforexploitingVARmodelsintherealmofvideogeneration.
6 Conclusion
WeintroducedanewvisualgenerativeframeworknamedVisualAutoRegressivemodeling(VAR)that
1)theoreticallyaddressessomeissuesinherentinstandardimageautoregressive(AR)models,and
2)makeslanguage-model-basedARmodelsfirstsurpassstrongdiffusionmodelsintermsofimage
quality,diversity,dataefficiency,andinferencespeed. UponscalingVARto2billionparameters,we
observedaclearpower-lawrelationshipbetweentestperformanceandmodelparametersortraining
compute,withPearsoncoefficientsnearingâˆ’0.998,indicatingarobustframeworkforperformance
prediction. Thesescalinglawsandthepossibilityforzero-shottaskgeneralization,ashallmarksof
LLMs,havenowbeeninitiallyverifiedinourVARtransformermodels. Wehopeourfindingsand
opensourcescanfacilitateamoreseamlessintegrationofthesubstantialsuccessesfromthenatural
languageprocessingdomainintocomputervision,ultimatelycontributingtotheadvancementof
powerfulmulti-modalintelligence.
A TokendependencyinVQVAE
ToexaminethetokendependencyinVQVAE[19],wechecktheattentionscoresintheself-attention
layer before the vector quantization module. We randomly sample 4 256Ã—256 images from the
ImageNetvalidationsetforthisanalysis. Notetheself-attentionlayerin[19]onlyhas1headsofor
eachimagewejustplotoneattentionmap. TheheatmapinFig.9showstheattentionscoresofeach
tokentoallothertokens,whichindicateastrong,bidirectionaldependencyamongalltokens. Thisis
notsurprisingsincetheVQVAEmodel,trainedtoreconstructimages,leveragesself-attentionlayers
withoutanyattentionmask. Somework[67]hasusedcausalattentioninself-attentionlayersofa
videoVAE,butwedidnotfindanyimageVAEworkusescausalself-attention.
12Figure9: Tokendependencyplotted.Thenormalizedheatmapofattentionscoresinthelastself-attention
layerofVQGANencoderisvisualized.4random256Ã—256imagesfromImageNetvalidationsetareused.
B TimecomplexityofARandVARgeneration
WeprovethetimecomplexityofARandVARgeneration.
Lemma B.1. For a standard self-attention transformer, the time complexity of AR generation is
O(n6),whereh=w =nandh,waretheheightandwidthoftheVQcodemap,respectively.
Proof. ThetotalnumberoftokensishÃ—w =n2. Forthei-th(1â‰¤iâ‰¤n2)autoregressiveiteration,
theattentionscoresbetweeneachtokenandallothertokensneedtobecomputed,whichrequires
O(i2)time. Sothetotaltimecomplexitywouldbe:
n2
(cid:88) 1
i2 = n2(n2+1)(2n2+1), (15)
6
i=1
WhichisequivalenttoO(n6)basiccomputation.
ForVAR,itneedsustodefinetheresolutionsequense(h ,w ,h ,w ,...,h ,w )forautoregres-
1 1 2 2 K K
sivegeneration,whereh ,w aretheheightandwidthoftheVQcodemapatthei-thautoregressive
i i
step,andh =h,w =wreachesthefinalresolution. Supposen =h =w forall1â‰¤k â‰¤K
K K k k k
andn = h = w,forsimplicity. Wesettheresolutionsasn = a(kâˆ’1) wherea > 1isaconstant
k
suchthata(Kâˆ’1) =n.
LemmaB.2. Forastandardself-attentiontransformerandgivenhyperparametera>1,thetime
complexityofVARgenerationisO(n4),whereh=w =nandh,waretheheightandwidthofthe
last(largest)VQcodemap,respectively.
Proof. Considerthek-th(1 â‰¤ k â‰¤ K)autoregressivegeneration. Thetotalnumberoftokensof
currentalltokenmaps(r ,r ,...,r )is:
1 2 k
(cid:88)k (cid:88)k a2kâˆ’1
n2 = a2Â·(kâˆ’1) = . (16)
i a2âˆ’1
i=1 i=1
Sothetimecomplexityofthek-thautoregressivegenerationwouldbe:
(cid:18) a2kâˆ’1(cid:19)2
. (17)
a2âˆ’1
Bysummingupallautoregressivegenerations,wehave:
log a(cid:88)(n)+1(cid:18) a2kâˆ’1(cid:19)2
(18)
a2âˆ’1
k=1
(a4âˆ’1)logn+(cid:0) a8n4âˆ’2a6n2âˆ’2a4(n2âˆ’1)+2a2âˆ’1(cid:1)
loga
= (19)
(a2âˆ’1)3(a2+1)loga
âˆ¼O(n4). (20)
Thiscompletestheproof.
13Figure10: ModelcomparisononImageNet256Ã—256benchmark.
14Figure11: MoreImageNet256Ã—256generationsamples.
15References
[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S.Altman,S.Anadkat,etal. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023. 2,8,9
[2] Alpha-VLLM. Large-dit-imagenet. https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/
f7fe19834b23e38f333403b91bb0330afe19f79e/Large-DiT-ImageNet,2024. 2,7,8
[3] R.Anil,A.M.Dai,O.Firat,M.Johnson,D.Lepikhin,A.Passos,S.Shakeri,E.Taropa,P.Bailey,Z.Chen,
etal. Palm2technicalreport. arXivpreprintarXiv:2305.10403,2023. 2
[4] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,etal. Qwentechnical
report. arXivpreprintarXiv:2309.16609,2023. 2
[5] Y.Bai, X.Geng, K.Mangalam, A.Bar, A.Yuille, T.Darrell, J.Malik, andA.A.Efros. Sequential
modelingenablesscalablelearningforlargevisionmodels. arXivpreprintarXiv:2312.00785,2023. 2,3
[6] H.Bao, L.Dong, S.Piao, andF.Wei. Beit: Bertpre-trainingofimagetransformers. arXivpreprint
arXiv:2106.08254,2021. 4
[7] A.Brock,J.Donahue,andK.Simonyan. Largescalegantrainingforhighfidelitynaturalimagesynthesis.
arXivpreprintarXiv:1809.11096,2018. 7,8
[8] T.Brooks,B.Peebles,C.Holmes,W.DePue,Y.Guo,L.Jing,D.Schnurr,J.Taylor,T.Luhman,E.Luhman,
C.Ng,R.Wang,andA.Ramesh. Videogenerationmodelsasworldsimulators. OpenAI,2024. 3,4,8,12
[9] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,
A.Askell,etal. Languagemodelsarefew-shotlearners. Advancesinneuralinformationprocessing
systems,33:1877â€“1901,2020. 2,3
[10] H.Chang,H.Zhang,J.Barber,A.Maschinot,J.Lezama,L.Jiang,M.-H.Yang,K.Murphy,W.T.Freeman,
M.Rubinstein,etal. Muse:Text-to-imagegenerationviamaskedgenerativetransformers. arXivpreprint
arXiv:2301.00704,2023. 4
[11] H.Chang,H.Zhang,L.Jiang,C.Liu,andW.T.Freeman. Maskgit:Maskedgenerativeimagetransformer.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages11315â€“
11325,2022. 4,7,8,11,12
[12] J.Chen,J.Yu,C.Ge,L.Yao,E.Xie,Y.Wu,Z.Wang,J.Kwok,P.Luo,H.Lu,etal.Pixart:Fasttrainingof
diffusiontransformerforphotorealistictext-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023. 6
[13] A.Chowdhery,S.Narang,J.Devlin,M.Bosma,G.Mishra,A.Roberts,P.Barham,H.W.Chung,C.Sutton,
S.Gehrmann,etal. Palm: Scalinglanguagemodelingwithpathways. JournalofMachineLearning
Research,24(240):1â€“113,2023. 2
[14] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei. Imagenet:Alarge-scalehierarchicalimage
database. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248â€“255.Ieee,
2009. 8
[15] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. Bert:Pre-trainingofdeepbidirectionaltransformers
forlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018. 3,4
[16] P.DhariwalandA.Nichol.Diffusionmodelsbeatgansonimagesynthesis.Advancesinneuralinformation
processingsystems,34:8780â€“8794,2021. 4,7,8
[17] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,M.Min-
derer,G.Heigold,S.Gelly,etal. Animageisworth16x16words:Transformersforimagerecognitionat
scale. arXivpreprintarXiv:2010.11929,2020. 4
[18] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.MÃ¼ller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,F.Boesel,
D.Podell,T.Dockhorn,Z.English,K.Lacey,A.Goodwin,Y.Marek,andR.Rombach. Scalingrectified
flowtransformersforhigh-resolutionimagesynthesis,2024. 3,4,8
[19] P.Esser, R.Rombach, andB.Ommer. Tamingtransformersforhigh-resolutionimagesynthesis. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages12873â€“12883,
2021. 2,3,4,5,6,7,8,12
[20] P.Gage. Anewalgorithmfordatacompression. CUsersJournal,12(2):23â€“38,1994. 3
[21] K. He, X. Chen, S. Xie, Y. Li, P. DollÃ¡r, and R. Girshick. Masked autoencoders are scalable vision
learners. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
16000â€“16009,2022. 4
[22] T.Henighan,J.Kaplan,M.Katz,M.Chen,C.Hesse,J.Jackson,H.Jun,T.B.Brown,P.Dhariwal,S.Gray,
etal. Scalinglawsforautoregressivegenerativemodeling. arXivpreprintarXiv:2010.14701,2020. 2,3,8,
9,10
[23] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneuralinformation
processingsystems,33:6840â€“6851,2020. 4
[24] J.Ho,C.Saharia,W.Chan,D.J.Fleet,M.Norouzi,andT.Salimans. Cascadeddiffusionmodelsforhigh
fidelityimagegeneration. TheJournalofMachineLearningResearch,23(1):2249â€“2281,2022. 4
[25] J.Ho,C.Saharia,W.Chan,D.J.Fleet,M.Norouzi,andT.Salimans. Cascadeddiffusionmodelsforhigh
fidelityimagegeneration. TheJournalofMachineLearningResearch,23(1):2249â€“2281,2022. 7
[26] J.HoandT.Salimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,2022. 4
[27] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint
arXiv:2203.15556,2022. 2,3,8,9
16[28] L.Huang,W.Yu,W.Ma,W.Zhong,Z.Feng,H.Wang,Q.Chen,W.Peng,X.Feng,B.Qin,etal.Asurvey
onhallucinationinlargelanguagemodels:Principles,taxonomy,challenges,andopenquestions. arXiv
preprintarXiv:2311.05232,2023. 2
[29] M.Kang,J.-Y.Zhu,R.Zhang,J.Park,E.Shechtman,S.Paris,andT.Park. Scalingupgansfortext-to-
imagesynthesis.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages10124â€“10134,2023. 6,7
[30] J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess,R.Child,S.Gray,A.Radford,J.Wu,and
D.Amodei. Scalinglawsforneurallanguagemodels. arXivpreprintarXiv:2001.08361,2020. 2,3,6,8,9,
10
[31] T.Karras,T.Aila,S.Laine,andJ.Lehtinen. Progressivegrowingofgansforimprovedquality,stability,
andvariation. arXivpreprintarXiv:1710.10196,2017. 2
[32] T.Karras,M.Aittala,S.Laine,E.HÃ¤rkÃ¶nen,J.Hellsten,J.Lehtinen,andT.Aila. Alias-freegenerative
adversarialnetworks. AdvancesinNeuralInformationProcessingSystems,34:852â€“863,2021. 6
[33] T.Karras,S.Laine,andT.Aila. Astyle-basedgeneratorarchitectureforgenerativeadversarialnetworks.
InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages4401â€“4410,
2019. 2,4,6
[34] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the
imagequalityofstylegan. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages8110â€“8119,2020. 6
[35] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,T.Xiao,S.Whitehead,A.C.Berg,
W.-Y.Lo,etal. Segmentanything. arXivpreprintarXiv:2304.02643,2023. 3
[36] A.Kuznetsova,H.Rom,N.Alldrin,J.Uijlings,I.Krasin,J.Pont-Tuset,S.Kamali,S.Popov,M.Malloci,
A.Kolesnikov,etal. Theopenimagesdatasetv4:Unifiedimageclassification,objectdetection,andvisual
relationshipdetectionatscale. InternationalJournalofComputerVision,128(7):1956â€“1981,2020. 6,7
[37] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual
quantization. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages11523â€“11532,2022. 2,3,4,6,7,8
[38] T.Li,D.Katabi,andK.He. Self-conditionedimagegenerationviageneratingrepresentations. arXiv
preprintarXiv:2312.03701,2023. 2,7
[39] T.-Y.Lin,P.DollÃ¡r,R.Girshick,K.He,B.Hariharan,andS.Belongie. Featurepyramidnetworksfor
objectdetection.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
2117â€“2125,2017. 2
[40] D.G.Lowe. Objectrecognitionfromlocalscale-invariantfeatures. InProceedingsoftheseventhIEEE
internationalconferenceoncomputervision,volume2,pages1150â€“1157.Ieee,1999. 2
[41] C.Lu,Y.Zhou,F.Bao,J.Chen,C.Li,andJ.Zhu.Dpm-solver:Afastodesolverfordiffusionprobabilistic
modelsamplinginaround10steps. AdvancesinNeuralInformationProcessingSystems,35:5775â€“5787,
2022. 4
[42] J.Lu,C.Clark,R.Zellers,R.Mottaghi,andA.Kembhavi.Unified-io:Aunifiedmodelforvision,language,
andmulti-modaltasks. arXivpreprintarXiv:2206.08916,2022. 2
[43] F.Mentzer,D.Minnen,E.Agustsson,andM.Tschannen. Finitescalarquantization:Vq-vaemadesimple.
arXivpreprintarXiv:2309.15505,2023. 12
[44] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,
F.Massa,A.El-Nouby,etal. Dinov2:Learningrobustvisualfeatureswithoutsupervision. arXivpreprint
arXiv:2304.07193,2023. 3
[45] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,K.Slama,
A.Ray,etal. Traininglanguagemodelstofollowinstructionswithhumanfeedback. AdvancesinNeural
InformationProcessingSystems,35:27730â€“27744,2022. 2
[46] W.PeeblesandS.Xie. Scalablediffusionmodelswithtransformers. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages4195â€“4205,2023. 2,4,6,7,8
[47] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,
J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InInternational
conferenceonmachinelearning,pages8748â€“8763.PMLR,2021. 3
[48] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by
generativepre-training. article,2018. 2
[49] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,etal. Languagemodelsareunsupervised
multitasklearners. OpenAIblog,1(8):9,2019. 2,3,6
[50] C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.Liu. Exploring
thelimitsoftransferlearningwithaunifiedtext-to-texttransformer. TheJournalofMachineLearning
Research,21(1):5485â€“5551,2020. 4
[51] A.Ramesh,M.Pavlov,G.Goh,S.Gray,C.Voss,A.Radford,M.Chen,andI.Sutskever. Zero-shot
text-to-imagegeneration. InInternationalConferenceonMachineLearning,pages8821â€“8831.PMLR,
2021. 2
[52] A.Razavi, A.VandenOord, andO.Vinyals. Generatingdiversehigh-fidelityimageswithvq-vae-2.
Advancesinneuralinformationprocessingsystems,32,2019. 2,3,7,8
17[53] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesynthesiswith
latentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages10684â€“10695,2022. 4,7
[54] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,
B.KaragolAyan,T.Salimans,etal. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. AdvancesinNeuralInformationProcessingSystems,35:36479â€“36494,2022. 4
[55] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L.
Scao,A.Raja,etal. Multitaskpromptedtrainingenableszero-shottaskgeneralization. arXivpreprint
arXiv:2110.08207,2021. 3
[56] A.Sauer,T.Karras,S.Laine,A.Geiger,andT.Aila. Stylegan-t: Unlockingthepowerofgansforfast
large-scaletext-to-imagesynthesis. arXivpreprintarXiv:2301.09515,2023. 6
[57] A.Sauer,K.Schwarz,andA.Geiger. Stylegan-xl: Scalingstylegantolargediversedatasets. InACM
SIGGRAPH2022conferenceproceedings,pages1â€“10,2022. 6,7
[58] J.Song,C.Meng,andS.Ermon. Denoisingdiffusionimplicitmodels. arXivpreprintarXiv:2010.02502,
2020. 4
[59] Y.SongandS.Ermon. Generativemodelingbyestimatinggradientsofthedatadistribution. Advancesin
neuralinformationprocessingsystems,32,2019. 4
[60] Y.Sun,S.Wang,S.Feng,S.Ding,C.Pang,J.Shang,J.Liu,X.Chen,Y.Zhao,Y.Lu,etal. Ernie3.0:
Large-scaleknowledgeenhancedpre-trainingforlanguageunderstandingandgeneration. arXivpreprint
arXiv:2107.02137,2021. 2
[61] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,A.Hauth,
etal. Gemini:afamilyofhighlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023. 2
[62] K.Tian,Y.Jiang,Q.Diao,C.Lin,L.Wang,andZ.Yuan. Designingbertforconvolutionalnetworks:
Sparseandhierarchicalmaskedmodeling. arXivpreprintarXiv:2301.03580,2023. 2
[63] H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.RoziÃ¨re,N.Goyal,E.Hambro,
F.Azhar,etal. Llama:Openandefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,
2023. 2,3,6
[64] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,
S.Bhosale,etal. Llama2:Openfoundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,
2023. 2,3,6
[65] A.VanDenOord,O.Vinyals,etal.Neuraldiscreterepresentationlearning.Advancesinneuralinformation
processingsystems,30,2017. 3
[66] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Å.Kaiser,andI.Polosukhin.
Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,2017. 4
[67] R.Villegas,M.Babaeizadeh,P.-J.Kindermans,H.Moraldo,H.Zhang,M.T.Saffar,S.Castro,J.Kunze,
andD.Erhan. Phenaki: Variablelengthvideogenerationfromopendomaintextualdescriptions. In
InternationalConferenceonLearningRepresentations,2022. 12
[68] W.Wang,Z.Chen,X.Chen,J.Wu,X.Zhu,G.Zeng,P.Luo,T.Lu,J.Zhou,Y.Qiao,etal.Visionllm:Large
languagemodelisalsoanopen-endeddecoderforvision-centrictasks. AdvancesinNeuralInformation
ProcessingSystems,36,2024. 2
[69] X.Wang,W.Wang,Y.Cao,C.Shen,andT.Huang. Imagesspeakinimages: Ageneralistpainterfor
in-contextvisuallearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages6830â€“6839,2023. 3
[70] B.Workshop,T.L.Scao,A.Fan,C.Akiki,E.Pavlick,S.IlicÂ´,D.Hesslow,R.CastagnÃ©,A.S.Luccioni,
F.Yvon, etal. Bloom: A176b-parameteropen-accessmultilinguallanguagemodel. arXivpreprint
arXiv:2211.05100,2022. 2,3
[71] J.Yu,X.Li,J.Y.Koh,H.Zhang,R.Pang,J.Qin,A.Ku,Y.Xu,J.Baldridge,andY.Wu. Vector-quantized
imagemodelingwithimprovedvqgan. arXivpreprintarXiv:2110.04627,2021. 2,3,4,7,8
[72] J.Yu,Y.Xu,J.Y.Koh,T.Luong,G.Baid,Z.Wang,V.Vasudevan,A.Ku,Y.Yang,B.K.Ayan,etal.
Scalingautoregressivemodelsforcontent-richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789,
2(3):5,2022. 3
[73] L.Yu,Y.Cheng,K.Sohn,J.Lezama,H.Zhang,H.Chang,A.G.Hauptmann,M.-H.Yang,Y.Hao,I.Essa,
etal. Magvit: Maskedgenerativevideotransformer. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages10459â€“10469,2023. 4
[74] L.Yu,J.Lezama,N.B.Gundavarapu,L.Versari,K.Sohn,D.Minnen,Y.Cheng,A.Gupta,X.Gu,A.G.
Hauptmann,etal. Languagemodelbeatsdiffusionâ€“tokenizeriskeytovisualgeneration. arXivpreprint
arXiv:2310.05737,2023. 3,4,7,12
[75] R.Zhang,P.Isola,A.A.Efros,E.Shechtman,andO.Wang. Theunreasonableeffectivenessofdeep
featuresasaperceptualmetric. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages586â€“595,2018. 4
[76] S.Zhang,S.Roller,N.Goyal,M.Artetxe,M.Chen,S.Chen,C.Dewan,M.Diab,X.Li,X.V.Lin,etal.
Opt:Openpre-trainedtransformerlanguagemodels. arXivpreprintarXiv:2205.01068,2022. 3
[77] C.Zheng,T.-L.Vuong,J.Cai,andD.Phung. Movq:Modulatingquantizedvectorsforhigh-fidelityimage
generation. AdvancesinNeuralInformationProcessingSystems,35:23412â€“23425,2022. 2,12
18