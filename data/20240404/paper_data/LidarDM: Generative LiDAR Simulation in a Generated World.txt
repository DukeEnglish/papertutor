LidarDM: Generative LiDAR Simulation in a
Generated World
Vlas Zyrianov1, Henry Che1, Zhijian Liu2, and Shenlong Wang1
1 University of Illinois, Urbana-Champaign, IL, USA
{vlasz2,hungdc2,shenlong}@illinois.edu
2 Massachusetts Institute of Technology, MA, USA
zhijian@mit.edu
www.zyrianov.org/lidardm
Abstract. We present LidarDM, a novel LiDAR generative model ca-
pable of producing realistic, layout-aware, physically plausible, and tem-
porally coherent LiDAR videos. LidarDM stands out with two unprece-
dentedcapabilitiesinLiDARgenerativemodeling:(i)LiDARgeneration
guidedbydrivingscenarios,offeringsignificantpotentialforautonomous
drivingsimulations,and(ii)4DLiDARpointcloudgeneration,enabling
thecreationofrealisticandtemporallycoherentsequences.Attheheart
ofourmodelisanovelintegrated4Dworldgenerationframework.Specif-
ically,weemploylatentdiffusionmodelstogeneratethe3Dscene,com-
bine it with dynamic actors to form the underlying 4D world, and sub-
sequentlyproducerealisticsensoryobservationswithinthisvirtualenvi-
ronment.Ourexperimentsindicatethatourapproachoutperformscom-
petingalgorithmsinrealism,temporalcoherency,andlayoutconsistency.
We additionally show that LidarDM can be used as a generative world
model simulator for training and testing perception models.
Keywords: LiDAR Generation Â· Scene Generation Â· Self-driving
1 Introduction
Generativemodelshavebecomenotableinunderstandingdatadistributionsand
content creation, e.g. in image and video generation [10,33,52â€“55], 3D object
generation [10,19,38,52], compression [5,29,68], and editing [37,47]. Generative
models also show significant promise for simulation [6,11,18,34,46,60,64,66,
76,82], capable of creating realistic scenarios and their associated sensory data
for training and evaluating safety-critical embodied agents, such as in robotics
andautonomousvehicles,withouttheneedofexpensivemanualmodelingofthe
real world. These capabilities are crucial for applications that rely on extensive
closed-loop training or scenario testing.
Whileadvancementsinconditionalimageandvideogeneration[15,27,35,44]
havebeenremarkable,thespecifictaskofgenerativelycreatingscenario-specific,
4202
rpA
3
]VC.sc[
1v30920.4042:viXra2 V. Zyrianov et al.
Layout-aware Diverse
Realistic
Traffic Layout
Condition
Temporally Consistent
Fig.1: We present LidarDM, a novel 4D LiDAR generative model. Our generated
LiDARvideossimultaneouslyenjoythebenefitsofbeingrealistic,layout-conditioning,
physically plausible, diverse, and temporally coherent.
realistic LiDAR point cloud sequences for autonomous driving application re-
mains under-explored. Current LiDAR generation methods fall into two broad
categories, each of which suffers from specific challenges:
1. LiDAR generative modeling methods [8,72,79,83] are currently limited to
single-frame generation and do not provide the means for semantic control-
lability and temporal consistency.
2. LiDAR resimulation [14,17,46,65,67,74] depends heavily on user-created
or real-world collected assets. This induces a high operational cost, restricts
diversity, and limits broader applicability.
To address these challenges, we propose LidarDM (Lidar Diffusion Model),
whichcreatesrealistic,layout-aware,physicallyplausible,andtemporallycoherent
LiDAR videos. We explore two novel capabilities that have not been previously
addressed: (i) LiDAR synthesis guided by driving scenarios, which holds im-
mense potential for simulation in autonomous driving, and (ii) 4D LiDAR point
cloud synthesis aimed at producing realistic and temporally coherent sequences
of labeled LiDAR point clouds. Our key insight to achieving these goals lies in
first generating and composing the underlying 4D world and then creating re-
alistic sensory observations within this virtual environment. To achieve this, we
integrateexisting3Dobjectgenerationapproachestocreatedynamicactorsand
developanovelapproachforlarge-scale3Dscenegenerationbasedonthelatent
diffusion model. This method produces realistic and diverse 3D driving scenes
from a coarse semantic layout, which to our knowledge, is one of the first of its
kind. We apply trajectory generation to create dynamic effects while ensuring
authentic interactions among actors and between actors and the scene. Finally,
we compose the 3D world at each time step and perform stochastic raycasting
simulationtoproducethefinal4DLiDARsequence.AsshowninFig.1,ourgen-
eratedresultsarediverse,alignwiththelayoutconditions,andarebothrealistic
and temporally coherent.
Our experimental results demonstrate that individual frames generated by
LidarDM exhibit realism and diversity, with performance that is on-par withLidarDM 3
t=0 t=20 t=40
en
p
aMrad
iL-
aL
hctiw
S
h
seMrad
iL-
den
gilA
tfeL
nruT
tt == 00 tt == 22 00 tt == 44 00
Map + Free Labels
ledoM
noitpecreP
nie ar rP -t
den
gilA
nru
UT-
Generated Lidar
(a) Generates lidar ofChamps-Ã‰lysÃ©es (b) Evaluates safety-critical scenarios by extending Waymax (c) Improves perception via pre-training
Fig.2: ApplicationsofLidarDM:(a)generatingLiDARthatalignswellwiththemap
(colorboxeshighlighttheconsistencybetweenthelidarandthemap)without3Dcap-
turing or modeling; (b) providing sensor data for an existing traffic simulator (Way-
max [20]), enabling safety-critical scenarios evaluation from pure sensor data; (c) gen-
eratelargevolumeLidardatawithcontrollableobstacleslocations(treatedasground-
truth labels, which are free to obtain) to improve perception models via pre-training
without expensive data capturing and labelling.
state-of-the-arttechniquesinunconditionalsingle-frameLiDARpointcloudgen-
eration.Moreover,weshowthatLidarDMcanproduceLiDARvideosthatmain-
tain temporal coherency, outperforming a robust stable diffusion sensor genera-
tion baseline. To our knowledge, this is the first LiDAR generative method
with this capability. We further demonstrate LidarDMâ€™s conditional generation
by showing that the generated LiDAR matches well with ground-truth LiDAR
on matching map conditions. Lastly, we illustrate that the data generated by
LidarDM exhibit a minimal domain gap when tested with perception modules
trained on real data and can also be used to augment training data to sig-
nificantly boost the performance of 3D detectors. This gives premise for using
generative LiDAR models to create realistic and controllable simulations for
trainingandtestingdrivingmodels(PleaserefertoSec.2andFig.2fordetailed
applications of LidarDM).
2 Related Works
LiDARSimulation. RealisticLiDARsensorsimulationiscrucialforroboticsand
self-drivingvehicletrainingandtesting.TraditionalLiDARgenerationmethods
use raycasting-based physical approaches. Simulators like CARLA [14] and Air-
Sim [59] create environments with static (buildings, trees, street lights) and dy-
namicobjects(cars,bicycles,buses).Inthesesettings,virtualLiDARsensorsare
placed, casting rays to calculate depth through ray-triangle intersections. Such
approaches are simple and easy to integrate, hence are widely used in robot
simulation[4,45].Asset-basedphysicalsimulationmethodsforLiDARfacelimi-
tations in realism and scalability due to three key issues: the need for 3D assets,4 V. Zyrianov et al.
whicharecostlyandlimitvariations;further,theyalsofacechallengesinclosing
the sim2real gap for both asset design and physics simulation.
Recentresearchattemptstoaddresstheseshortcomingsthroughdata-driven
approaches.LiDARSim[46]usescollectedLiDARsequencestoreconstructmaps
anddynamicassets.Subsequentworkshaveimprovedassetreconstructionusing
neuralfields-basedapproacheslikeNeRF[30,67]orneuralfeaturefields[74],and
with automatic alignment [58], or with past LiDAR readings [30]. Additionally,
realistic physics effects such as ray dropping [49] and snow [21] have also been
modeled. However, constructing detailed 3D maps and objects from real-world
sensor data is often costly and not scalable, typically requiring multiple passes
over the same locations.
Our approach, unlike traditional LiDAR simulations, is purely generative,
eliminating the need for man-made or reconstructed assets and allowing for
easy creation of numerous virtual worlds. In Fig. 2 (a), we present a realistic
generated LiDAR point cloud of Champs-Ã‰lysÃ©es from only a hand-crafted map
layout (without any actual sensor data from France), which no re-simulation
methods can achieve.
LiDAR Generation. Generative models provide a promising alternative for cre-
ating realistic LiDAR point clouds without reconstructing real-world environ-
ments. Early LiDAR generation work exploited the range image representation
forLiDARgeneration.ThepioneeringworkbyCacciaetal.[8]focusedonusing
GANs and VAEs for unconditional generation and performing reconstruction
tasks for noisy LiDAR readings. LiDARGen [83] showed that using a score-
based diffusion model provides improved generative capability and can be used
in downstream classifier-guided sampling tasks such as point cloud upsampling.
The range image representation offers the benefit of physically accurate render-
ingatthecostofbeingego-centric,ratherthanscene-centric.Recently,UltraLi-
DAR[72]proposedusingaBEVvoxelgridrepresentationforLiDARgeneration.
Sampling is performed with a VQVAE [50] in a learned discrete latent space,
ensuringadenselatentspace.TheBEV-centricrepresentationprovidestheben-
efit of improved layout coherence and metrics, at the cost of not guaranteeing a
physically accurate LiDAR sample. However, despite significant advancements,
currentgenerativemodelsdonotsupportconditioningonsemanticlayouts.This
omissionmakesthegenerationprocesslesscontrollable,reducingitspracticalap-
plications. Additionally, a model ensuring temporal consistency in LiDAR video
generation remains to be developed.
Unlike previous methods, our model addresses this challenge by (1) gener-
alizing the task of LiDAR generation to geometry generation and data-driven
ray casting in a novel field representation; (2) guiding generation with a BEV
HDMaplayout;and(3)allowingcompletecontroloverdynamicscenecomposi-
tion by adding generated 3D models and enabling free movement of the virtual
LiDAR sensor within the frame. Fig. 2 (b) and (c), respectively, show that Li-
darDMâ€™spointcloudcanproviderealisticLiDARinformationtoanexistingtraf-
fic simulator for autonomous drivingâ€™s safety-critical case evaluation thanks to
itstemporalconsistency,andcanimproveperceptionaccuracy(Sec.4.5)thanksLidarDM 5
Traffic Layout Object Generation 4D World Composition 4D LiDAR Videos
Scene Generation
Fig.3: Overview of the LidarDM: Given the input traffic layout at time t = 0, Li-
darDM begins by generating actors and the static scene. We then generate the mo-
tion of the actors and the egocar, and compose the underlying 4D world. Finally, a
generative- and physics-based simulation is used to create realistic 4D sensor data.
toitsfullcontrollabilityofscenarios,whichmeansground-truthdetectionlabels
are free to obtain. These two important benefits cannot be achieved with any
other LiDAR generative methods.
Diffusion Models. Our model builds upon the recent advancement in latent dif-
fusionmodels.Directlyapplyingdiffusionmodels[24,62]ondatacanbeburden-
someonthedenoisingnetworkduetotheissueofdatasparsity.Latentdiffusion
alleviates this issue by performing the diffusion process on a dense latent space
of an autoencoder. Models such as stable diffusion [54], stable diffusion XL [51],
andMaskGIT[9]havechampionedlatentdiffusionasahighlyeffectivetechnique
for generative modeling.
Sampling: A key challenge in diffusion-based modeling lies in selecting the
samplingprocedure.EarlymethodsutilizedLangevindynamicsorancestralsam-
pling. Recent sampling quality and speed improvements have come from deter-
ministicnon-MarkoviantechniqueslikeDDIM[61]andPNDM[40].Additionally,
the differential equation interpretation of diffusion models has led to the devel-
opment of samplers like Euler [32] and DPM-Solver [43]. We empirically find
Euler works well in practice for our model.
Conditioning: Various strategies have been devised to integrate conditions
into the diffusion model generation process. Classifier-based guidance (or pos-
terior sampling) [47,62,83] utilizes the gradient of a classifier to enhance the
modelâ€™s score function. Classifier-free guidance [23] offers a method for training
and sampling class-conditioned diffusion models. Controlnet [80] introduces a
technique for adding controllability to a pre-trained diffusion model through a
class-conditionedhypernetwork.Ourapproachleveragesclassifier-freeguidance.
In diffusion-based video generation [22,25,31,70,71,73], maintaining consis-
tency is a central challenge. Various approaches have been proposed, leveraging
motion modeling, interpolation, or batch sampling to enhance consistency. Our
model is the first to focus on LiDAR video generation. Unlike other methods,
our technique capitalizes on LiDARâ€™s unique attribute of underlying 3D world
alignment, significantly improving temporal consistency.
3 Layout-Guided LiDAR Video Generation
Our goal is to create a realistic, physically plausible, and temporally consis-
tent LiDAR sequence that enables a free viewpoint based on a given birdâ€™s eye6 V. Zyrianov et al.
view semantic layout in a purely generative manner without relying on any pre-
collected assets like 3D maps. To our knowledge, this is the first solution of its
kind, addressing layout-conditioned LiDAR generation and LiDAR video gen-
eration. The key to achieving this lies in first generating and composing the
underlying 3D world, followed by using generative simulation to create realistic
sensoryobservations.Webeginbyformulatingthegenerationasajoint4Dscene
generation task (Sec. 3.1). Next, we discuss leveraging 3D diffusion models to
createstaticanddynamicelements,ensuringtheirfaithfulinteraction(Sec.3.2).
Finally, a sensor generation procedure is executed to produce the final LiDAR
video (Sec. 3.3). Fig. 3 depicts the overview of our method.
3.1 Problem Formulation
Formally,givenaninputlayoutI âˆˆRLÃ—WÃ—M representingtrafficelementsfrom
a birdâ€™s eye view (where L, W, and M are length, width, and map classes,
respectively),ourgoalistogenerateaLiDARpointcloudvideoX ={x },with
t
each x âˆˆ RNÃ—3 being a point cloud at frame t with x matching the input
t 0
layout. This conditional generation setting offers full controllability, and hence
laysthefoundationforapracticalasset-freesimulator.Notethatintheabsence
of a map, our approach defaults to unconditional generation.
4D World Representation. Ourkeytechnicalinnovationtoaddressthechallenge
lies in jointly modeling the generation of underlying 4D world together sensor
generation.WedefinetheworldscenerepresentationasW ={s,{o }N },where
i i=0
s represents a static scene geometry and o ,...,o are dynamic objects. Both
0 N
are represented in the form of an occupancy grid. To model dynamics, we addi-
tionallyconsidertheactionsofthesedynamicobjectsintheformoftrajectories
P ={Ï„ ,...,Ï„ }, with Ï„ ={Î¾ ,{Î¾ }N } representing the pose of actor i at
0 T t ego i,t i=0
time t as well as egocar pose Î¾ . The pose for rigid objects and the egocar lies
ego
intheSE(3)space,whileforarticulatedobjectslikepedestrians,itisrepresented
as a kinematic chain. A composed scene represent the states of the world at t,
incorporatingtheposesoftheegocaranddynamicobjectsattimet,isdenoted
by W =Ï€(W,Ï„ ), where Ï€ is a composition operator applying transformations
t t
to each actor.
4D World and LiDAR Generation. To ensure realism and consistency over time
and between the world and sensory readings, we formulate the generation task
as a sampling problem from the joint distribution p(X,P,W|I). Directly mod-
eling and sampling the joint distribution, however, is challenging as it involves
estimating a distribution across multiple data modalities (e.g., car trajectories,
scenelayouts,sensornoise,etc.).Totacklethis,wefactorizethejointdistribution
p(X,P,W|I) as follows:
(cid:89) (cid:89) (cid:89)
p(s|I)Â· p(o |I) Â· p(Ï„ |Ï„ ,W,I)Â· p(x |Ï„ ,W). (1)
i t <t t t
i t t
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
3Dsceneandobjectgen trajectorygen sensorsimulation
Next, we will discuss each individual task in detail.LidarDM 7
A Training Data Preparation B Latent Encoding
Recon-
Encoder Decoder Loss
KL-Loss
Accumulated Point Cloud Observation Reconstruction
C Latent Diffusion
Map Map
ð’©(0,1) ð’©(0,1)
â€¦
UNET UNET
NKSR Reconstruction
Generated Sample
Fig.4: Our3Dscenegenerationpipeline.First,accumulatedpointcloudsareusedto
reconstructeachgroundtruthmeshsample.Next,avariationalautoencoderistrained
to compress meshes into a latent code. Finally, a map-conditioned diffusion model is
trainedtoperformsamplingwithinthelatentspaceoftheVAE,yieldingnovelsamples.
3.2 Scene, Object and Trajectory Generation
Wedecomposetheworldintoastaticbackgroundscene,constantovertime,and
dynamic foreground objects that move. This decomposition simplifies the chal-
lenging4Dworldgenerationintomoremanageabletasks:creatingobjectgeome-
tries and generating dynamic effects. This modeling approach ensures temporal
consistency (e.g. keeping carsâ€™ shapes constant and walls and trees remain still
over time) and physical plausibility (e.g. ensuring correct occlusion reasoning).
Scene Generation. The scene generation addresses the problem of sampling the
geometry of a scene from a given input layout I: s âˆ¼ p(s|I). We parameterize
the 3D scene using a signed distance field, s âˆˆ RLÃ—WÃ—H, where each entry s
j
encodesthetruncatedsigneddistancetothesurface,âˆ’1â‰¤s â‰¤1,withnegative
j
values indicating outside the mesh and positive values inside the surface.
Model:Weleveragethelatentdiffusionmodel[54,54]totacklethischallenge
of modeling and sampling from p(s|I). We choose the latent diffusion model for
its capacity to sample high-quality data while effectively incorporating strong
conditional guidance. Specifically, our model encodes the high-dimensional SDF
volume s into a continuous latent representation z using an encoder-decoder
structure [54] with a scene encoder E (s) = z and a scene decoder D (z) =
Î¸ Î¸
Ëœs. This encoder-decoder structure efficiently compresses the input data into a
lower-dimensional latent space, enabling more effective and efficient sampling.
Additionally, we encode our high-definition map layout I into a latent space
c=M (I), allowing for more compact conditioning.
Î¸
Sampling:Weleverageaprobabilisticdenoisingdiffusionmodel[54,62]F (z,c)
Î¸
to perform classifier-free guidance sampling [23]. Specifically for each diffusion
step k, the following Langevin dynamics step is performed to progressively de-
noise until a clean sample z is acquired:
0
z =z + Î» k [(1+w)F (z ,c)âˆ’wF (z )]+(cid:112) Î» Ïµ
kâˆ’1 k 2 Î¸ k Î¸ k k k8 V. Zyrianov et al.
Raycasting LiDAR Range image Raydrop Prob Raydrop Mask Masked Range Masked LiDAR Final LiDAR Generation
Fig.5: Stochastic raydrop networks for sensory noise simulation, further enhancing
realism. We highlighted the raydropped points in red on Masked Range and Masked
LiDAR Images above.
F (z ,c) is the score function âˆ‡ logp(z|c) of the conditional distribution at z
Î¸ k z k
and F (z ,c = 0) is the the score function of the unconditional distribution
Î¸ k
p (z). w is the CFG guidance scale parameter, Î» is an annealed noise schedule
Î¸ k
parameter, and Ïµ âˆ¼ N(0,I). Finally, a 3D scene sample s is recovered by
k
decode the reverse-diffused sampling latent code s = E (z ). Fig. 4(c) depicts
Î¸ 0
the sampling procedure.
Training:Wetrainourdiffusion-basedscenegenerationmodelusingadataset
thatpairsscenegeometrywithmapconditioning.Directaccesstodensescenege-
ometryisnotavailableinpractice.Instead,weusethestate-of-the-artdensege-
ometryreconstructionapproach,neuralkernelsurfacereconstruction(NKSR)[28],
to recover a pseudo-GT from an input LiDAR sequence. Ground truth an-
notations are used to remove moving dynamic objects, ensuring our recon-
struction contains only the static scene and objects. We then train the auto-
encoders for both scene geometry and map layout using reconstruction loss and
KL divergence loss: min L + L over real-world examples (Fig. 4 (a)).
Î¸ recon KL
Our latent diffusion model is trained using the score matching loss function:
L = E (cid:2) âˆ¥Ïµâˆ’F (z ,k,c)âˆ¥2(cid:3) , where z is the forward diffused noisy
LDM (z,c),Ïµ,k Î¸ k 2 k
sample at step k (Fig. 4 (b)).
Object Generation. Inspiredbytherapidadvancementsin3Dshapegeneration,
we employ two high-fidelity, 3D object generation frameworks, GET3D [19] and
AvatarClip [26], to create dynamic traffic participants.
For each actor o in a given layout I, we sample a random variable z âˆ¼ N
i
and generate the corresponding actor mesh following o = G(z), where G(Â·) is
i
the Generator/Decoder of the chosen generative method. For cars, trucks, and
other four-wheeled vehicles, we use GET3D [19], which has demonstrated state-
of-the-artanddiversegenerativeresultsfor3Dshapes,includingcars.Thelayout
box in I is used to rescale the shape of each actor, ensuring that the sizes of
the generated shape and the input layout are consistent. AvatarClip [26] is used
to generate pedestrians conditioned on a SMPL [41] pose and shape parameter
p=(Î¸,Î²). Furthermore, each generated rigged model is animated with a walk-
ing animation from Mixamo [3], ensuring a realistic 3D human walking motion
sequence over time.
Together,thegeneratedstaticworldsandeachactoro defineour3Dworld
i
scenario, denoted as W. Fig. 3 depicts the composed scene.LidarDM 9
Trajectory Generation. To simulate a dynamic traffic scenario, we propose a
retrieval-augmentedgenerationcoupledwitharejectivesamplingschemetogen-
erate realistic and physically plausible dynamics for each actor and the ego ve-
hicle, turning our 3D scene representation W to 4D, denoting as W for a given
t
time t.
For the ego vehicle and each actor, jointly denoted as P ={Ï„ }, we sampled
t
trajectoriesfromatrajectorybankobtainedfromWaymoOpendataset[63]and
augment them to the scene. This guarantees realistic dynamics as the dataset is
obtained from a diverse set of real-world scenarios. To ensure the physical feasi-
bility of the sampled trajectories with respect to our generated scene, we reject
thosethatviolaterulesofphysics,suchascollisionwiththestaticworld,collision
between actors, or hovering over the non-mesh area, while making sure to po-
sition them correctly above the ground. Around 12.3% of sampled trajectories
are retained. This rate is acceptable because resampling is trivial.
Additionally, for more realistic trajectory generation targeting for simula-
tionuse-cases,weextendWaymax[20],adata-driven2DBEVtrafficsimulator,
to control the behaviors of traffic actors in more systematic manners. Given a
scenario from the WOMD Dataset [16], we use Waymax to replay ego-vehicleâ€™s
andagentâ€™sreal-worldtrajectories,withanadditionalreactiveintelligentdriving
module [69] that updates each agentâ€™s acceleration to avoid collisions. Since the
trajectories are specific to the given real-world scenario, they are guaranteed to
be physically plausible, but less diverse than our approach above.
Effectively, this approach renders our world generation to be completely
asset-free, end-to-end generative, and thereby temporally consistent, allowing
for a realistic, generative, and physics-based simulation without the need for
artist-curated [14] or pre-collected assets [46,74] as in previous lidar simulation
method.
3.3 Physics-Informed LiDAR Generation
Given the complete 4D world W and the poses P, our next step is to generate a
realistic LiDAR point cloud corresponding to these conditions. At a high level,
weusetheposestocomposethesceneandobjectsateachtimestep,thenperform
physics-informedraycastingtoobtainpurelyphysicallysimulatedLiDARasan
intermediateresult.Asafinalstep,weleveragedata-drivenconditionalsampling
to generate the final LiDAR point cloud to simulate real-world LiDAR noises
from the clean ray casting LiDAR.
Scene Composition. We use the Dual Marching Cube method [57] to obtain the
3D mesh of the static world from the generated TSDF volume s. Then, with
the trajectories of the ego car and all actors at time t, Ï„ , we transform the 3D
t
mesh to the world coordinates and compose it with s, producing the full world
geometry at each time t: W = Ï€(W,Ï„ ) For each vehicle actor, Ï€ applies a
t t
rigid body transformation. For pedestrians, besides the rigid body movement, Ï€
additionallyarticulatesthehumanbodyshapetosimulatetheanimatednonrigid
human movement with forward kinematics [7].10 V. Zyrianov et al.
Physics-based Ray Casting. From the ego vehicle position Ï„ , we perform ray-
t
casting by utilizing Open3D [81] to compute ray-triangle intersections against
the composed scene W , obtaining the raycast scan x . To enhance realism of
t t
the raycasting to real-world LiDAR, we closely match the LiDAR sensor config-
uration (elevation angles, azimuth angles, field of view, etc.) with the real-world
Lidar, depending on the use-case. For single-frame generation (Sec. 4.3), we fol-
low the Velodyne HDL-64E manual [1] to match with KITTI-360 dataset [39].
For conditional multi-frame generation (Sec. 4.4), we use the calibration infor-
mation provided with Waymo Open Dataset [63].
Stochastic Raydrop. Raycasted LiDAR from the generated world appears over-
cleanwithoutreal-worldnoisesduetoenvironmentalandsensornoisefactors.To
address this, inspired by LiDARSim, we have an additional stage that stochas-
tically simulates â€œraydropâ€, where rays do not return to the sensor. For each
raycast scan at time t, x , we project it onto a 2D spherical range image using
t
polar coordinates. This image uses azimuth and zenith angles to represent coor-
dinates and encodes depth values for each pixel. We predict raydrop probability
per pixel on this image using a U-Net architecture [48] supervised by real-world
LiDAR scan raydrop masks. Our approach, unlike LiDARSim, requires only a
rangemap,eliminatingtheneedformultipleadditionalmetadatainputchannels
that are only available in real-world data. We also apply a Gumbel sigmoid for
random sampling. The application of our stochastic raydrop method produces
the final LiDAR scan x for each frame, concluding in our complete end-to-end
t
LiDAR video generation process, as shown in Fig. 5.
4 Experiment
4.1 Setup
Datasets. We evaluate our proposed LidarDM on KITTI-360 [39] and Waymo
Open [63] datasets. KITTI-360 is the de facto dataset for evaluating uncondi-
tional LiDAR generation methods. The dataset contains nine driving sequences
(76,715samples),wherethefirstsequenceisusedasavalsequence(11,518sam-
ples) and the last eight are used for training (65,197 samples). The sequences
were collected across Karlsruhe, Germany with a 64 Beam LiDAR. However,
KITTI-360 does not provide detailed BEV HD map information limiting its ap-
plications in conditional models. Waymo Open [63] is a dataset containing 1048
sequences with 158,000 training and 29,700 validation frames. A detailed HD
map allows us to train conditional diffusion models. The HD map is in a vector
format storing edges of map objects. We preprocess the map by centering it
on each LiDAR Frame and rasterizing it into a segmentation map. The dimen-
sions of the map tensor are LÃ—WÃ—M (lengthÃ—widthÃ—map classes). The map has
5 classes (lane markings, road lines, edges, crosswalks, driveways).
Training Details. WetrainourmodelsusingfourNvidiaA10040GBGPUs.We
use the AdaM optimizer with a learning rate of 1e-4 for the VAE and 1e-5 for
the diffusion U-Net, with a cosine decay schedule.LidarDM 11
Real ProjectedGAN LiDARGen UltraLiDAR Ours
Fig.6: Real KITTI-360 samples vs unconditional samples from the competing meth-
ods. UltraLiDAR sample visualizations are directly acquired from their paper. Com-
paredtopreviousapproaches,LidarDMgeneratessamplesthatfeatureagreaterquan-
tity of more detailed salient objects (e.g., cars, pedestrians), sharper 3D structures
(e.g., straight walls), and more realistic road layouts.
Model Details. The diffusion UNet, which is conditioned on layout and SDF
latents, has 5 ResNet blocks with 2Ã— downsampling with channels of
{128,128,256,512,512}. The SDF VAE has 4 ResNet blocks with channels of
{448,640,896,1280}. The Map VAE has ResNet down/upsample blocks with
channelsof{64,64,128,256,512},anditistrainedindependentlywithX-Entropy
and KL regularization.
4.2 Baselines
Unconditional Generation. LiDARVAE, LiDARGAN, ProjectedGAN, and Li-
DARGen are baselines that perform generation in the range image representa-
tion.UltraLiDARperformsgenerationintheBEVvoxelspace.Toprovideafair
comparison,wefollowUltraLiDARandevaluateMMDandJSDonahistogram
of voxel occupancy instead of voxel density [72]. In addition, UltraLiDAR does
not provide samples so we use their reported numbers in their original paper.
Temporal Coherency. We are the first to attempt the task of sequential LiDAR
generation and thus no previous models exist for comparison. Nonetheless, we
implement a sequence diffusion baseline inspired by recent work in video gener-
ation. We believe this approach is the most straightforward initial approach.
Concretely, we train a VAE to encode individual LiDAR frames. This has
beenshownpreviously[72]tobepowerfulandeffective.Next,wetrainadiffusion
model to directly denoise multiple (i.e., 5) LiDAR frames at once. Visually, this
approach yields decent temporal consistency.12 V. Zyrianov et al.
Method MMD (â†“) JSD (â†“)
BEV BEV
LiDAR VAE [8] 8.53Ã—10âˆ’4 0.267
LiDAR GAN [8] 8.95Ã—10âˆ’4 0.243
ProjectedGAN [56] 7.07Ã—10âˆ’4 0.201
LidarGen [83] 2.95Ã—10âˆ’4 0.136
UltraLidar [72] 9.67Ã—10âˆ’5 0.132
LidarDM (Ours) 1.67Ã—10âˆ’4 0.119
Table 1: Qualitative results for unconditional generation on KITTI-360 dataset. (
best, second best, third best)
Total ICP Average Outlier Chamfer
Method
Energy [m] (â†“)ICP Energy (â†“)Percentage (â†“)Distance [m] (â†“)
Sequence Diffusion 3616.58 0.078 20.56% 0.39
LidarDM (Ours) 916.94 0.014 7.12% 0.17
Table 2: Temporal consistency. Outlier percentage uses distance threshold Ï„ =0.5m.
4.3 Unconditional Single-Frame Generation
Wefirstvalidateourmodelarchitecturedesignandshowcaseourmodelâ€™sgenera-
tivecapabilitybydirectlycomparingagainstpreviousLiDARgenerationmodels
in unconditional generation on KITTI-360 [39].
Based on the results in Table 1, BEV models (such as ours or UltraLiDAR)
perform best in top-down layout quality (as reflected by MMD and JSD) com-
pared to range image models. The close performance gap compared to Ultra-
LiDAR can be explained by the fact that UltraLiDAR was directly trained on
the task of modeling single LiDAR scans which the benchmark evaluates. In
addition, the BEV voxel grid representation offers large flexibility in generating
physicallyimplausibleLiDARreadingsthathavethepotentialaccuratelymatch
ground truth data in histogram metrics. We also show qualitative comparisons
against the baselines in Fig. 6.
4.4 Map-conditioned Multi-Frame Generation
Our model is the first fully generative LiDAR model that can generate control-
lable (through map conditioning), realistic, and temporally coherent synthetic
LiDAR scans. We will then validate these properties in this section.
Consistent Video Generation One of our key contributions is the tempo-
ral consistency of the sequential LiDAR generation. To evaluate this, we first
use ICP alignment to calculate a relative transformation between consecutive
generated frames. We then exploit LiDARâ€™s 3D nature and define an aver-
age point-to-plane energy over a sequence of LiDAR scans as our quantita-
1
tive metrics, following this equation: E =
(cid:80)T
point2plane(x ,x ) where
T t=1 t tâˆ’1
point2plane represents the point-to-plane distance [42], and x indicates the
tLidarDM 13
paMradiL- t=0 t=30 t=60 t=90 Accumulated Lidar Point Clouds
dengilA
radiL
gnidnopserroC
t=0 t=30 t=60 t=90 Accumulated Lidar Point Clouds
paMradiL-
dengilA
paMradiL-
dengilA
Fig.7: Qualitative results of map-conditioned sequence generation on 2 Waymax [20]
mapsequences.Wealsoshowcasethecorrespondingaccumulatedpointcloudtohigh-
tlight the temporal consistency of LidarDM.
LiDAR scan at time t. Intuitively, E is prone to higher values from dynamic
objects, but it is still a valuable metrics to determine if the general geometry of
the 3D scene is preserved over time as the majority of the environment is static.
TofurtherevaluatethegeometricconsistencyofthegenerativeLiDARsequence,
we also measure the outlier point ratio, defined as the percentage of points
with the point2plane distance larger than a certain threshold Ï„. Table 2 shows
ourquantitativeresults,wherewebeatthebaselineinbothmetricsbyanotable
margin. These results clearly show that our LidarDM is capable of generating
temporally-consistent LiDAR sequences.
mAP(%) mAPAgreement Config mAP(%)
Real 59.7 35kReal 58.2
81.1%
LidarDM 56.4 35kReal+70kLidarDM 61.3
Table 3: Real2Sim: Detector [75] Table 4: Data augmentation:
trained on real data can be evaluated on LidarDM-generated data can enhance
LidarDM-generateddata,showingstrong trainingfordetectorsonreal-worlddata,
agreementwithitsrealcounterpart,sug- suggesting its potential to improve the
gestingitspotentialforsimulation. perception module without expensive
dataannotation.
Layout-aware LiDAR Generation To ascertain the layout-awareness of our
LidarDM,weuseCenterPoint[75]trainedonreal-worldLiDARscanstovalidate
whether it can accurately detect objects from the LidarDMâ€™s LiDAR scan. As
LidarDMcurrentlydoesnotgenerateintensity,wetrainedCenterPointexcluding
intensity of the real-world LiDAR scans, hence the potential mismatch between
our results and those obtained in the original paper [75].
Given an input layout I, we generate the corresponding LiDAR scan, run
CenterPointonit,andevaluateusingmeanaverageprecision(mAP)forvehicles.14 V. Zyrianov et al.
WecomparethisresultwiththemAPfromtherawLiDAR.TheresultinTable
3 indicates that CenterPointâ€™s object detection on our generated LiDAR scan is
comparable to the ground truth. We also compute the mAP agreement between
our generated LiDAR scan and raw LiDAR scan, indicating a strong agreement
between the two and demonstrating our approachâ€™s map-awareness and realism.
Qualitative results We show qualitative results of our map-conditioned Li-
DAR sequence generation in Fig. 7. Our generated results closely match the
map conditioning by adding flat surfaces and static vehicles that correspond to
the provided layout. The use of physical-based LiDAR sensor simulation guar-
antees that the generated point clouds are properly occluded by obstacles and
appear as a realistic LiDAR sweep pattern. We also showcase the accumulated
LiDARpointsover90framesinWaymax[20],highlightingLidarDMâ€™stemporal
consistency and map-awareness.
4.5 Augmenting Real Data with LidarDM
LidarDM is the first LiDAR generative model capable of generating data condi-
tionedonagivensemanticlayout.Thiscapabilityoffersthepotentialtoaugment
thetrainingdatafor3Dperceptionmodels,therebyfurtherboostingtheirperfor-
mance.Toevaluatethiscapability,wefirstuseLidarDMtogeneratearound70k
frames of simulation data based on the layout from Waymo Dataset [63]. After
that, we pre-train a LiDAR-based 3D object detection model, CenterPoint [75]
(withPointPillars[36]asitsbackbone),onthesegeneratedLiDARframes,paired
with the object labels from the dataset. We then train the same model on 35k
frames of real data, both with and without the pre-training stage on the sim-
ulation data, to test the benefits of the LidarDM-generated data. According to
Table4,LidarDMcanactasaneffectivegenerativedataaugmentationstrategy,
offering more than a 3% improvement in detection accuracy.
5 Conclusion
Inthispaper,weintroducedLidarDM,anovellayout-conditionedlatentdiffusion
model for generating realistic LiDAR point clouds. Our approach frames the
problem as a joint 4D world creation and sensory data generation task and
develops a novel latent diffusion model to create 3D scenes. The resulting point
cloud videos are realistic, coherent, and layout-aware.
Limitations So far, LidarDM relies on latent diffusion models, which are not
real-time. Recent progress in latent consistency models promises to accelerate
the generation process. We also leave LiDAR intensity modeling as future work.LidarDM 15
Acknowledgement ThisprojectissupportedbytheNCSAFacultyFellowship,
andNSFAwards#2331878,#2340254,and#2312102andgiftsfromIntel,IBM,
Amazon, and Meta. We greatly appreciate the NCSA for providing computing
resources. Vlas Zyrianov is supported by the New Frontiers Fellowship. The
authors thank Scott Lathrop and Aaron D. Saxton for technical support with
the NCSA Delta cluster.
References
1. High definitian lidar hdl-64e. https://hypertech.co.il/wp-content/uploads/
2015/12/HDL-64E-Data-Sheet.pdf
2. Sketchfab. https://sketchfab.com/feed, accessed: 2021-11-26
3. Adobe Inc.: Mixamo. https://www.mixamo.com/
4. Afzal,A.,Goues,C.L.,Timperley,C.S.:Gzscenic:Automaticscenegenerationfor
gazebo simulator. arXiv preprint arXiv:2104.08625 (2021)
5. BallÃ©, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression.
In: ICLR (2017)
6. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In: CVPR (2023)
7. Bogo,F.,Kanazawa,A.,Lassner,C.,Gehler,P.,Romero,J.,Black,M.J.:Keepit
smpl: Automatic estimation of 3d human pose and shape from a single image. In:
ECCV (2016)
8. Caccia,L.,v.Hoof,H.,Courville,A.,Pineau,J.:Deepgenerativemodelingoflidar
data. In: IROS (2019)
9. Chang,H.,Zhang,H.,Jiang,L.,Liu,C.,Freeman,W.T.:Maskgit:Maskedgener-
ative image transformer. In: CVPR (2022)
10. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and
appearance for high-quality text-to-3d content creation. In: ICCV (2023)
11. Chen, Y., Rong, F., Duggal, S., Wang, S., Yan, X., Manivasagam, S., Xue, S.,
Yumer, E., Urtasun, R.: Geosim: Realistic video simulation via geometry-aware
composition for self-driving. In: CVPR (2021)
12. Codevilla, F., Santana, E., LÃ³pez, A.M., Gaidon, A.: Exploring the limitations of
behavior cloning for autonomous driving. ICCV (2019)
13. Dauner, D., Hallgarten, M., Geiger, A., Chitta, K.: Parting with misconceptions
about learning-based vehicle motion planning. In: CoRL (2023)
14. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: An open
urban driving simulator. In: CoRL (2017)
15. Esser,P.,Chiu,J.,Atighehchian,P.,Granskog,J.,Germanidis,A.:Structureand
content-guided video synthesis with diffusion models. In: ICCV (2023)
16. Ettinger,S.,Cheng,S.,Caine,B.,Liu,C.,Zhao,H.,Pradhan,S.,Chai,Y.,Sapp,
B., Qi, C., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V.,
McCauley,A.,Shlens,J.,Anguelov,D.:Largescaleinteractivemotionforecasting
for autonomous driving : The waymo open motion dataset. arXiv (2021)
17. Fang, J., Zhou, D., Yan, F., Zhao, T., Zhang, F., Ma, Y., Wang, L., Yang, R.:
Augmented lidar simulator for autonomous driving. RAL (2020)
18. Feng, L., Li, Q., Peng, Z., Tan, S., Zhou, B.: Trafficgen: Learning to generate
diverse and realistic traffic scenarios. In: ICRA (2023)16 V. Zyrianov et al.
19. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z.,
Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned
from images. In: Advances In Neural Information Processing Systems (2022)
20. Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X.,
Wang,Y.,Chen,X.,Co-Reyes,J.D.,Agarwal,R.,Roelofs,R.,Lu,Y.,Montali,N.,
Mougin,P.,Yang,Z.,White,B.,Faust,A.,McAllister,R.,Anguelov,D.,Sapp,B.:
Waymax:Anaccelerated,data-drivensimulatorforlarge-scaleautonomousdriving
research. In: NeurIPS (2023)
21. Hahner, M., Sakaridis, C., Bijelic, M., Heide, F., Yu, F., Dai, D., Van Gool, L.:
LiDAR Snowfall Simulation for Robust 3D Object Detection. In: CVPR (2022)
22. Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., Wood, F.: Flexible diffu-
sion modeling of long videos (2022)
23. Ho, J.: Classifier-free diffusion guidance. ArXiv (2022)
24. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS
(2020)
25. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. arXiv:2204.03458 (2022)
26. Hong,F.,Zhang,M.,Pan,L.,Cai,Z.,Yang,L.,Liu,Z.:Avatarclip:Zero-shottext-
driven generation and animation of 3d avatars. ACM Transactions on Graphics
(TOG) 41(4), 1â€“19 (2022)
27. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J.,
Corrado, G.: Gaia-1: A generative world model for autonomous driving (2023)
28. Huang, J., Gojcic, Z., Atzmon, M., Litany, O., Fidler, S., Williams, F.: Neural
kernel surface reconstruction. In: CVPR (2023)
29. Huang,L.,Wang,S.,Wong,K.,Liu,J.,Urtasun,R.:Octsqueeze:Octree-structured
entropy model for lidar compression. In: CVPR (2020)
30. Huang,S.,Gojcic,Z.,Wang,Z.,Williams,F.,Kasten,Y.,Fidler,S.,Schindler,K.,
Litany, O.: Neural lidar fields for novel view synthesis. In: ICCV (2023)
31. HÃ¶ppe, T., Mehrjou, A., Bauer, S., Nielsen, D., Dittadi, A.: Diffusion models for
video prediction and infilling (2022), https://arxiv.org/abs/2206.07696
32. Karras,T.,Aittala,M.,Aila,T.,Laine,S.:Elucidatingthedesignspaceofdiffusion-
based generative models. In: Proc. NeurIPS (2022)
33. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: CVPR (2019)
34. Kim, S.W., , Philion, J., Torralba, A., Fidler, S.: DriveGAN: Towards a Control-
lable High-Quality Neural Simulation. In: CVPR (2021)
35. Kim, S.W., Brown, B., Yin, K., Kreis, K., Schwarz, K., Li, D., Rombach, R.,
Torralba,A.,Fidler,S.:Neuralfield-ldm:Scenegenerationwithhierarchicallatent
diffusion models. In: CVPR (2023)
36. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars:
Fast encoders for object detection from point clouds (2019)
37. Ledig,C.,Theis,L.,Huszar,F.,Caballero,J.,Cunningham,A.,Acosta,A.,Aitken,
A., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-
resolution using a generative adversarial network. In: CVPR (2017)
38. Li,M.,Zhou,P.,Liu,J.W.,Keppo,J.,Lin,M.,Yan,S.,Xu,X.:Instant3d:Instant
text-to-3d generation. arXiv preprint arXiv:2311.08403 (2023)
39. Liao,Y.,Xie,J.,Geiger,A.:KITTI-360:Anoveldatasetandbenchmarksforurban
scene understanding in 2d and 3d. PAMI (2022)
40. Liu,L.,Ren,Y.,Lin,Z.,Zhao,Z.:Pseudonumericalmethodsfordiffusionmodels
on manifolds. In: ICLR (2022)LidarDM 17
41. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinnedmulti-personlinearmodel.ACMTrans.Graphics(Proc.SIGGRAPHAsia)
34(6), 248:1â€“248:16 (Oct 2015)
42. Low, K.L.: Linear least-squares optimization for point-to-plane icp surface regis-
tration (01 2004)
43. Lu,C., Zhou,Y., Bao,F., Chen, J.,Li, C., Zhu, J.:Dpm-solver:A fastodesolver
for diffusion probabilistic model sampling in around 10 steps. In: NeurIPS (2022)
44. Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency mod-
els: Synthesizing high-resolution images with few-step inference. arXiv preprint
arXiv:2310.04378 (2023)
45. Macenski,S.,Foote,T.,Gerkey,B.,Lalancette,C.,Woodall,W.:Robotoperating
system 2: Design, architecture, and uses in the wild. Science Robotics (2022)
46. Manivasagam, S., Wang, S., Wong, K., Zeng, W., Sazanovich, M., Tan, S., Yang,
B., Ma, W.C., Urtasun, R.: Lidarsim: Realistic lidar simulation by leveraging the
real world. In: CVPR (2020)
47. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:SDEdit:Guided
imagesynthesisandeditingwithstochasticdifferentialequations.In:ICLR(2022)
48. Milioto, A., Vizzo, I., Behley, J., Stachniss, C.: RangeNet++: Fast and Accurate
LiDAR Semantic Segmentation. In: IEEE/RSJ Intl. Conf. on Intelligent Robots
and Systems (IROS) (2019)
49. Nakashima,K.,Iwashita,Y.,Kurazume,R.:Generativerangeimagingforlearning
scene priors of 3D lidar data. In: WACV (2023)
50. van den Oord, A., Vinyals, O., Kavukcuoglu, K.: Neural discrete representation
learning (2018)
51. Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,MÃ¼ller,J.,Penna,
J.,Rombach,R.:Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimage
synthesis. arXiv preprint arXiv:2307.01952 (2023)
52. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv (2022)
53. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. ArXiv (2022)
54. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
55. Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,
S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J.,
Norouzi,M.:Photorealistictext-to-imagediffusionmodelswithdeeplanguageun-
derstanding. ArXiv (2022)
56. Sauer, A., Chitta, K., MÃ¼ller, J., Geiger, A.: Projected gans converge faster. In:
NeurIPS (2021)
57. Schaefer,S.,Warren,J.:Dualmarchingcubes:primalcontouringofdualgrids.In:
12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004.
Proceedings. pp. 70â€“76 (2004). https://doi.org/10.1109/PCCGA.2004.1348336
58. Schmidt, J., Khan, Q., Cremers, D.: LiDAR View Synthesis for Robust Vehicle
Navigation Without Expert Labels. In: ITSC (2023)
59. Shah,S.,Dey,D.,Lovett,C.,Kapoor,A.:Airsim:High-fidelityvisualandphysical
simulation for autonomous vehicles. In: FSR (2017)
60. Shen, Y., Chandaka, B., Lin, Z.h., Zhai, A., Cui, H., Forsyth, D., Wang, S.: Sim-
on-wheels: Physical world in the loop simulation for self-driving. arXiv preprint
arXiv:2306.08807 (2023)
61. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:ICLR(2021)18 V. Zyrianov et al.
62. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution. In: NeurIPS (2019)
63. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,
J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,
Timofeev,A.,Ettinger,S.,Krivokon,M.,Gao,A.,Joshi,A.,Zhang,Y.,Shlens,J.,
Chen,Z.,Anguelov,D.:Scalabilityinperceptionforautonomousdriving:Waymo
open dataset. In: CVPR (2020)
64. Swerdlow, A., Xu, R., Zhou, B.: Street-view image generation from a birdâ€™s-eye
view layout. arXiv preprint arXiv:2301.04634 (2023)
65. Tallavajhula, A.,Mericli, C., Kelly, A.: Off-road lidar simulation with data-driven
terrain primitives. In: ICRA (2018)
66. Tan, S., Wong, K., Wang, S., Manivasagam, S., Ren, M., Urtasun, R.: Scenegen:
Learning to generate realistic traffic scenes. CVPR (2021)
67. Tao, T., Gao, L., Wang, G., Lao, Y., Chen, P., Zhao, H., Hao, D., Liang, X.,
Salzmann, M., Yu, K.: Lidar-nerf: Novel lidar view synthesis via neural radiance
fields (2023)
68. TechCrunch: Waveone aims to make video ai-native and turn streaming upside
down. https://techcrunch.com/2020/12/01/waveone-aims-to-make-video-
ai-native-and-turn-streaming-upside-down/, accessed: 2023-11-16
69. Treiber,M.,Hennecke,A.,Helbing,D.:Congestedtrafficstatesinempiricalobser-
vations and microscopic simulations. Physical Review E 62, 1805â€“1824 (02 2000).
https://doi.org/10.1103/PhysRevE.62.1805
70. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,
M.T.,Castro,S.,Kunze,J.,Erhan,D.:Phenaki:Variablelengthvideogeneration
from open domain textual description. In: ICLR (2022)
71. Voleti, V., Jolicoeur-Martineau, A., Pal, C.: Mcvd: Masked conditional video dif-
fusion for prediction, generation, and interpolation. In: NeurIPS (2022)
72. Xiong, Y., Ma, W.C., Wang, J., Urtasun, R.: Learning compact representations
for lidar completion and generation. In: CVPR (2023)
73. Yang, R., Srivastava, P., Mandt, S.: Diffusion probabilistic modeling for video
generation (2022)
74. Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun,
R.: Unisim: A neural closed-loop sensor simulator. In: CVPR (2023)
75. Yin,T.,Zhou,X.,KrÃ¤henbÃ¼hl,P.:Center-based3dobjectdetectionandtracking.
CVPR (2021)
76. Yu, T., Xiao, T., Stone, A., Tompson, J., Brohan, A., Wang, S., Singh, J., Tan,
C.,Peralta,J.,Ichter,B.,etal.:Scalingrobotlearningwithsemanticallyimagined
experience. In: RSS (2023)
77. Zeng,W.,Luo,W.,Suo,S.,Sadat,A.,Yang,B.,Casas,S.,Urtasun,R.:End-to-end
interpretable neural motion planner (2021)
78. Zhai,J.T.,Feng,Z.,Du,J.,Mao,Y.,Liu,J.J.,Tan,Z.,Zhang,Y.,Ye,X.,Wang,J.:
Rethinkingtheopen-loopevaluationofend-to-endautonomousdrivinginnuscenes.
arXiv preprint arXiv:2305.10430 (2023)
79. Zhang,L.,Xiong,Y.,Yang,Z.,Casas,S.,Hu,R.,Urtasun,R.:Learningunsuper-
vised world models for autonomous driving via discrete diffusion. arXiv preprint
arXiv:2311.01017 (2023)
80. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models. In: ICCV (2023)
81. Zhou,Q.Y.,Park,J.,Koltun,V.:Open3D:Amodernlibraryfor3Ddataprocess-
ing. arXiv:1801.09847 (2018)LidarDM 19
82. Zhu, X., Zyrianov, V., Liu, Z., Wang, S.: Mapprior: Birdâ€™s-eye view perception
with generative models (2023)
83. Zyrianov, V., Zhu, X., Wang, S.: Learning to generate realistic lidar point cloud.
In: ECCV (2022)LidarDM: Generative LiDAR Simulation in a
Generated World â€“ Supplementary Materials
Abstract. In the following supplementary material, we provide addi-
tional model details and ablations (Sec. 1). In Sec. 2 we provide ad-
ditionalqualitativeLidarDMresults,includingabaselinecomparison,a
closelookatpedestriandetails,andvisualizationsofmapconditionalign-
ment. In Sec. 3, we provide additional downstream applications such as
crafting safety-critical and out-of-distribution scenarios for self driving.
InSec.3.3,weperformaSim2Realdataaugmentationexperimentonan
end-to-end LiDAR-based planner.
1 Experimental Details
1.1 Sequence Diffusion Baseline.
The sequential diffusion baseline follows a latent diffusion architecture similar
to that of LidarDM. However, it uniquely adopts a BEV LiDAR representation,
encoding the observed voxelized point cloud, in line with advanced LiDAR gen-
eration methods. This approach specifically employs a binary occupancy grid
centered around the ego sensor [72]. The training process is conducted in two
stages. Initially, a VAE is trained using cross-entropy for reconstruction loss on
this data, supplemented by KL divergence regularization. In practice, we found
Ultralidarâ€™s discrete code does not perform as well as continuous latent space.
Following this, a diffusion model is trained in the latent code space. This model
isconditionalanditstrainingmirrorsthatofLidarDM;itutilizesthesamemap
latentcode,whichisoccasionallyomittedduringtraining(20%ofthetime)[23].
For consistent generation, the model simultaneously denoises five latent codes,
eachcorrespondingtoaLiDARframe,throughalatentcode-concatenationtech-
nique. This baseline is developed on four Nvidia A100 40GB GPUs. The AdaM
optimizer is used, with a learning rate of 1e-4 for the VAE and 1e-5 for the
diffusion U-Net, following a cosine decay schedule.
This setup allows for an effective comparison between the baseline and our
approach, demonstrating the efficacy of the core idea of jointly generating a 4D
world during sensor generation.
1.2 Raydrop Ablation
We perform an ablation study on various ray-dropping options in Table 1. The
table suggests that Gumbel Softmax generally yields better-calibrated LiDAR
ray drops than Softmax (the one used in Lidarsim), with lower JSD and MMD.
Additionally, although not using ray dropping yields a decent MMD score, it2
significantly increases the number of points (doubling the ground truth number
of points), making it unrealistic. For a fair comparison, all configurations are
evaluated on the same subset of underlying validation 3D worlds (hence the
numbers might slightly differ from our reported test numbers).
Config MMD (â†“) JSD (â†“)
BEV BEV
No Raydrop 1.730Ã—10âˆ’4 0.1286
Softmax 1.990Ã—10âˆ’4 0.1274
Gumbel (Ours) 1.846Ã—10âˆ’4 0.1271
Table 1: Raydrop Ablation. ( best, second best, third best)
2 LidarDM Additional Results
2.1 Sequential Diffusion Comparison.
Wecomparethelayout-conditionedLiDARvideogenerationresultsofourmethod
with the strong sequential diffusion baseline, as described in Section 4.2 of the
main paper. Fig. 1 displays these results. From these, it is evident that our
approach significantly outperforms the baseline in terms of realism and layout-
awareness.Specifically,wewouldlikethereaderstopayattentiontothedynamic
objectâ€™s shape, the layout of walls and other static infrastructures, and missing
objects in the baseline.
2.2 Pedestrian Motion.
Thanks to actor insertion, we can accurately model fine details such as walking
pedestrians compared to other pure generative results. Fig. 3 visualizes such
case, offering a view in both how the underlying pedestrian meshes behave, as
well as how the corresponding LiDAR points are affected.
2.3 More Map-aligned Qualitative Results.
Fig. 2 presents more map-aligned visualization as well as accumulated point
cloud, ensuring the map-awareness and temporal consistency of our approach.
We encourage the readers to watch our supplementary videos for better visual-
izationofqualitativeresults.Note:theblacklinesonthemapindicatestheroad
edge (which corresponds to a "bump" in LiDAR data as indicated in the color
boxes),notstructures(buildings,walls).Thus,ourLiDARpointsendoutsideof
those black edges (which corresponds to structures, such as buildings). This is
consistent with the real-world LiDAR.LidarDM 3
3 LidarDM Downstream Tasks Results
3.1 Safety-Critical Scenarios
We argue that one of the many benefits of LidarDM is the ability to extend
existing traffic simulators with realistic and scenario-aware LiDAR data, allow-
ing for sensor-based critical safety scenario evaluation of autonomous system.
More specifically, we extend Waymax [20], a BEV traffic simulator, and show-
case how LidarDM can create realistic LiDAR for two types of safety-critical
scenarios, ego-vehicle behavior manipulation (Fig. 4) and actors behavior ma-
nipulation (Fig. 5).
3.2 Out of Distribution Object Insertion
We demonstrate the strong controllability and flexible nature of our generative
approach by replicating dangerous scenarios in simulation. Similar to our ap-
proachonvehicleorpedestrianactors(Sec.3.2),weobtaintheout-of-distribution
meshesfromgenerativemodels(orevenoff-the-shelfassetsonSketchfab[2]).For
trajectories, we find it sufficient to sample from vehiclesâ€™ trajectories from the
trajectory bank, or simply pre-define a series of road-crossing trajectories. In
Fig.6,weshowanexampleofinsertingdangerousanimalsintothescene.These
scenarios are not present in the Waymo dataset [63], but may occur in real life
due to escaped zoo animals or on roads that are near wildlife habitats.
3.3 LidarDM as Data Augmentation for Planning
Inspired by our perception data augmentation experiment, we performed a sec-
ond Sim2Real experiment to demonstrate that LidarDM-generated data can
aid in training a learning-based end-to-end motion planner. Model and infer-
ence: Drawing inspiration from the Neural Motion Planner [77], we developed
a learning-based motion planner that takes the five most recent LiDAR obser-
vations (covering 0.5 seconds of past history) as input and generates a dense
cost map of size W Ã—H Ã—T for the future, where T represents the number
of future timestamps. In this case, T = 10, with a 0.3-second interval between
consecutive frames. During inference, we sample trajectories from a trajectory
bank and select the one with the lowest overall cost as the final planned tra-
jectory. Training: During training, we employ a soft cross-entropy loss to train
the cost map and generate a trajectory bank from the Waymo dataset using K-
Means. Training and validation is done following the standard splits on Waymo
Open dataset. Remark: We want to highlight two key differences compared to
the vanilla NMP model: 1) our motion planner does not require privileged HD
Maps as input, and 2) the motion planner does not explicitly incorporate the
ego carâ€™s past trajectory. The first design choice ensures that the model focuses
onleveragingsensordata;hence,thecomparisonconcentratesonevaluatingthe
quality of our generated LiDAR point clouds. The second choice is inspired by
recommended practices and findings from recent studies [12,13,78], suggesting4
thatincorporatingegovehicleâ€™spaststatesresultsinshort-cuteffectsandbiases
imitation learning for end-to-end driving.
Experimental details: In order to show that LidarDM-generated data can
help augment the motion planner, we first train a model on 92k snippets of
LidarDM-generated sequences, then fine-tune it on 9.2k real data sequences.
Note that we use trajectories of expert drivers as GT and use traffic layout
conditionstogenerateourLidarDMsamples.Asacomparison,wealsotrainthe
same planner model using only the 9.2k real data sequences. To ensure fairness,
wetrainboththereal-onlymodelandthereal+simmodelforatotalof30epochs
until convergence and choose the best model based on minimal validation loss.
Experimental results: We report two metrics: 1) the collision rate at a
3-second future, which measures the safety and traffic awareness of the planner;
and 2) the L2 distance at 1 second, 2 seconds, and 3 seconds, which measures
the accuracy for imitation. We present our results in Table 2. Using generative
pre-training improves the performance of the planner in a low-data regime. In
particular, our collision rate after 3 seconds has been reduced by 32% (relative)
from1.65%to1.12%.Toourknowledge,thisisthefirsttimeconditionalLiDAR
generation has been shown to improve an end-to-end motion planner.
L2 (m) L2 (m) L2 (m)
Config Collision Rate (%)
@ 1.0s @ 2.0s @ 3.0s
9.2k Real 0.489 1.374 3.279 1.65%
9.2k Real + 92k LidarDM 0.490 1.341 3.160 1.12%
Table 2: Planner data augmentation: LidarDM-generated data can enhance the
trainingofaNeuralMotionPlanner[77]-inspiredmodelonreal-worlddata,suggesting
its potential to improve the planning module without expensive data collection.(
best, second best)LidarDM 5
Bird-Eye View Side View
Map Seq Diffusion LidarDM (Ours) Seq Diffusion LidarDM (Ours)
Fig.1:ComparisonofLayout-ConditionedLiDARGenerationonWaymodataset:Our
approach significantly outperforms the strong latent-diffusion-based sequential gener-
ation baseline in terms of realism, physics plausibility, and coherence with the input
layout.6
t=0 t=30 t=60 t=90 Accumulated
Fig.2: More Map-Aligned Qualitative Results. We showcase 4 different frames of the
same sequence, with both map-aligned and LiDAR top-down view. We also show the
accumulated point clouds, colored by their time index to showcase the temporal con-
sistency.LidarDM 7
t=0 t=10 t=20 t=30 t=40 t=50
elgnA
w
e
iV
+
p
aM
hseM
gn
iylrednU
duo
lC
tn
ioP
Fig.3: Pedestrian motion captured with LidarDM: Thanks to our actor insertion ap-
proach,wecancapturehigh-fidelitypedestrianmovementthroughLiDAR,whichnone
other generative method can achieve.8
t=0 t=10 t=20 t=30 t=40 t=50
en
aL
th
g
iR
tfe
L
n
ru
T
n
ru
T
U-
thg
iartS
o
G
tfeL
n
ru
T
th
g
iR
n
ru
T
Fig.4: Ego-Vehicle Behavior Manipulation: By extending Waymax, we can perform
safety-criticalscenariosevaluationofautonomoussystem.WeshowcasethatLidarDM
can produce realistic LiDAR for different simulated ego trajectories from 2 Waymax
sequences.LidarDM 9
t=0 t=10 t=20 t=30 t=40 t=50
nru
T
U-
pra
h
S
e
n
aL
h
ctiw
S
Fig.5: Actor Behavior Manipulation: To create challenging situation that consitute a
safety-critical scenario, we showcase that LidarDM can also produce realistic LiDAR
data for when the actorâ€™s behavior is manipulated, as indicated in the purple boxes.
Underlying Mesh Corresponding Lidar
Fig.6: Rare Scenario Simulation: The provided LidarDM approach is grounded in
physicalsimulation,suggestingthatourgenerativemethodcanbecombinedwithstan-
dard physics-based ray casting simulation to simulate out-of-distribution rare cases,
such as a tiger crossing the street.