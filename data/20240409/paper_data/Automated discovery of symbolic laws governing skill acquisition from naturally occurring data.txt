Automated discovery of symbolic laws governing
skill acquisition from naturally occurring data
Sannyuya Liu1,2,3, Qing Li1,2, Xiaoxuan Shen1,2*, Jianwen Sun1,2*,
Zongkai Yang1,2,3*
1National Engineering Research Center of Educational Big Data,
Central China Normal University, Wuhan, 430079, China.
2Faculty of Artificial Intelligence in Education, Central China Normal
University, Wuhan, 430079, China.
3National Engineering Research Center for E-learning, Central China
Normal University, Wuhan, 430079, China.
*Corresponding author(s). E-mail(s): shenxiaoxuan@ccnu.edu.cn;
sunjw@ccnu.edu.cn; zkyang027@ccnu.edu.cn;
Abstract
Theskillacquisitionisakeyareaofresearchincognitivepsychology,asmaster-
ingthelawsofskillacquisitioncanhelpindividualsacquireknowledgeandskills
more efficiently. However, skill acquisition involves multiple psychological pro-
cesses, and the laws discovered under experimental paradigms are controversial
and lack generalizability. This paper aims to discover the laws of skill learning
from large-scale training log data , which is a typical naturally occurring data,
by machine learning algorithms. To address the issues of unobservable cogni-
tive states and algorithmic explosion in searching in this scenario, we designed
a novel two-stage learning algorithm that directly discovers the symbolic rep-
resentation of governing laws of skill acquisition from large-scale training log
data. Specifically, we utilized the ability of deep learning algorithms to fit high-
dimensional data to accurately calculate the learner‚Äôs hidden state and estimate
feature importance for feature reduction, and then combined it with symbolic
regressionalgorithmstoparsetheneuralnetworkmodelintoalgebraicequations.
Theexperimentalresultsofsimulateddatademonstratethattheproposedalgo-
rithm can accurately restore various preset laws within a certain range of noise
inthecontinuousfeedbacksetting,verifyingitseffectiveness.Thentheproposed
algorithmisappliedtothelarge-scalecognitivetrainingdataofLumosity,where
our proposed method significantly outperforms both classical practice laws and
thelatestlearningmodelsintermsoffitness.Inaddition,theresultssuggesttwo
1
4202
rpA
8
]GL.sc[
1v98650.4042:viXranew laws of skill acquisition and differentiation in learning laws between skills.
The experimental results also supported the mutualism between language and
reasoning.
Keywords:Cognitivepsychology,Skillacquisition,naturallyoccurringdata,Deep
neuralnetworks,Symbolicregression
1 Introduction
Skill acquisition theory is a framework that explains how individuals acquire and
develop skills through practice and experience. It focuses on the processes involved in
acquiring,refining,andautomatingskills.[1]Anessentialelementhighlightedbyskill
acquisition theory is deliberate practice, which encompasses purposeful and focused
efforts aimed at enhancing specific areas of performance [2]. This theory has been
influential in various fields, such as sports, music, education, and even programming,
as it provides insights into how individuals can effectively acquire and improve skills
in different domains [3].
Skill acquisition is a complex psychological process that encompasses a variety
of fundamental issues in psychology, such as practice [4‚Äì6], memory [7‚Äì9], and skill
transfer [10, 11]. Researchers have introduced symbolic representation methods, such
as algebraic equations, to describe the wide range of experimental phenomena in
this process and to discover underlying patterns. Based on these symbolic patterns,
researchers have begun to explore the reasons and mechanisms behind the formation
of these patterns. For example, instance theory [12, 13] provides a possible explana-
tion for the underlying mechanism behind the power law of practice [14]. It suggests
that the improvement in skill performance described by the power law is a result
of accumulating instances that are relevant to the specific skill being practiced. As
learnersengageinpracticerepetitions,theyaccumulateandrefinetheirmentalrepre-
sentationsofrelevantinstances,leadingtoimprovedperformance.Similarly,numerous
cognitive theories have been proposed, including the multi-phase theory [15, 16], par-
allel distributed processing models [17, 18], as well as various cognitive architectures,
such as SOAR [19] and ACT-R [20, 21]. Symbolic patterns play a crucial role in the
development of cognitive theories. Unfortunately, the symbolic patterns of numerous
psychological processes are frequently contentious. Hence, there is a critical need to
study and investigate the intricate symbolic patterns that underlie human behavior.
In classical paradigms of psychological research, symbolic rules are obtained
through the rigorous analysis of controlled experimental results. The forms of these
rules are primarily derived by researchers through empirical observations and con-
jectures. However, the symbolic rules obtained through the classical psychological
research paradigm are often controversial. Firstly, for the same set of experimental
data, researchers may derive different conclusions due to differing observation per-
spectives. The law of practice, which examines the relationship between the number
of practice and fluency, was originally believed to conform to the power law [6, 14] by
many researchers. However, many researchers have found that the power function is
2notalwaysthebestfitforthedata,oftenbeingoutperformedbyexponentialfunctions
or APEX functions [5]. More researchers are also exploring symbol representations
thatachievebetterfitwithdata[4].However,searchingfortheoptimalmodelremains
challenging and heavily reliant on expert experience. Secondly, controlled experimen-
tal designs may suffer from bias and limitations, making it challenging to reproduce
and generalize their results [23]. Jenkins‚Äô tetrahedral model [22] describes four criti-
cal factors (subjects, encoding activities, events, and retrieval conditions) in memory
experiments and argues that the conclusions drawn from controlled experiments may
only apply to specific settings of the control variables. Therefore, additional research
is necessary to determine the robustness and validity of research results under broad
conditions.
Inresponsetotheaforementionedchallenges,anumberofpsychologistshaveintro-
duced the ‚Äùbig data‚Äù paradigm to investigate patterns and regularities in human
behavior.Thisapproachutilizesnaturallyoccurringdatasets(NODS)tominepoten-
tial psychological patterns and rules from data [23], thus serving as a valuable
complementtotraditionalexperimentalparadigms.Researchershavediscoveredthat,
although large real-world data sets lack the level of control found in laboratory set-
tings, their vast size can enable factors to be statistically separated even when they
are not manipulable. This paradigm often provides insight into novel patterns or phe-
nomena beyond the scope of cognitive research, given that NODS are exploratory in
nature compared to designed experiments. Moreover, since empirical data is sourced
from a more realistic setting, the corresponding conclusions tend to be more applica-
ble, while the massive scale of empirical data often leads to more robust findings [23].
Theswiftadvancementoftechnologyhasledtothewidespreaduseofnumerousonline
tutoring and cognitive training systems. Through the employment of these systems,
learners have the opportunity to enhance their mastery of specific skills via online
practice sessions and accumulate a considerable volume of training log data (Figure
1a is a toy example of training log data). Within this data, a substantial quantity of
behavioralpatternsandunderlyingrulesareembeddedthatpertaintotheacquisition
of skills, encompassing the aspects of practice, forgetting, and skill transfer. These
datasets underconsiderationare quintessential instancesof NODS.It offersus oppor-
tunities to facilitate the development of associated theories and models through the
paradigm of ‚Äùbig data‚Äù.
Symbolic regression is a machine learning technique that aims to discover math-
ematical expressions or equations that best fit a given dataset. Unlike traditional
regression techniques that rely on predefined functional forms, symbolic regression
searches for an optimal mathematical expression by combining various mathematical
operations, functions, and variables [24]. It effectively aids researchers in discovering
patterns from data, thereby inspiring further investigations [25]. Compared to other
machine learning algorithms, the symbolic regression model produces symbolic mod-
els that adhere to general scientific descriptions, which enhances the interpretability
andreadabilityofthepredictionprocess.Moreover,asasearch-basedalgorithm,Sym-
bolicregressioncanguarantee,inmostcases,obtainingtheoptimalresultswithinthe
search space. Therefore, symbolic regression is well-suited for revealing skill acquisi-
tionpatternsorbroadapplicationsinbehavioralsciences.Anditalreadyhasextensive
3applications in numerous fields, such as physics [26‚Äì29], chemistry [30, 31], materials
science [32, 33], etc.
Our objective is to apply symbolic regression methods to learn symbolic behav-
ioralpatternsfromtraininglogdata.Wealsoaimtoidentifypotentialskillacquisition
laws from NODS, offer groundbreaking evidence for psychology-related research, and
investigate the ability to discover symbolic governing laws directly from NODS. How-
ever, training log data has exhibit features of considerable scale, high dimensionality,
latent states, and temporal dependencies. These salient attributes render it challeng-
ing to discern symbolic rules via direct deployment of symbolic regression algorithms.
Firstly, conventional symbolic regression algorithms, such as those based on genetic
algorithms as representative combinatorial optimization algorithms, search for opti-
mal fitting symbolic models in a solution space composed of predetermined operator
symbols, constants, and variables. This is a typical NP-hard problem, with the size of
the solution space increasing exponentially with the number of variables and opera-
tors[24].GiventhecharacteristicsofNODS,theinabilitytocontrolvariablesandthe
considerable scale of variables that affect skill acquisition make it highly challenging
tooptimizetheoptimalsymbolicmodeldirectlyusingsymbolicregressionalgorithms.
Secondly, the data exhibits salient temporal and hidden characteristics. The study of
skillacquisitionconcernstherelationshipbetweenbehaviorandskillmastery.Behavior
isinherentlytemporal,andfeatureextractionfromitisnecessaryyetchallenging.Skill
mastery is a latent variable that is difficult to observe directly. These challenges pose
significant obstacles to existing data analysis algorithms in discovering the symbolic
rules that pertain to skill acquisition from the training log data.
As the most successful machine learning model in recent years, Deep learning
models,commonlyknownasdeepneuralnetworkmodels,canefficientlyfitandmodel
data in high-dimensional space with the backpropagation algorithm [34]. Moreover,
deep learning algorithms have excellent non-linear fitting abilities that can effectively
fit data patterns of any shape. This enables the model to achieve accurate estimation
of the user‚Äôs knowledge mastery status. However, a significant disadvantage of deep
learning algorithms is their black box nature, which makes it challenging to interpret
the decision-making process. Consequently, data patterns fitted by deep learning are
often difficult for humans to comprehend.
The aim of this paper is to develop a novel approach that enables the discovery
of implicit patterns in skill acquisition directly from large-scale training log data with
continuous feedback, leveraging the big data paradigm to discover some new evidence
for the psychological issue of skill acquisition. A two-stage model is proposed which
combinesthestrengthsofdeeplearningandsymbolicregressionalgorithmstodiscover
governing laws of skill acquisition. The proposed two-stage method is able to achieve
both fitting precision and interpretability, by extracting key patterns from complex
traininglogdataandtransformingthemintoahuman-readableformat,thusenhancing
the understanding and usability of the underlying rules.
The first stage of the proposed two-stage model aims to construct a deep learning
model for precise fitting of large-scale online training log data, called deep learn-
ing regressor. It aims to accurately estimate key latent variables, such as behavior
feature encoding and user skill proficiency, and precisely fit key mapping functions.
4Inspired by transformer [35], a highly successful deep learning model for sequence
modeling, we designed a transformer-like model. First, it estimated the importance
(attention values) of behaviors in each skill training using static behavior features
andobtainedencodedsequentialbehaviorfeatures.Then,throughanon-linearneural
network model, we calculated the user‚Äôs skill mastery state using the encoded fea-
tures. Finally, we mapped the skill proficiency level to the training score utilizing the
three-parameter item response theory (IRT) model [36‚Äì39] which is a classic cogni-
tivediagnosticmodel.Themodeloptimizesusinganautoregressivetrainingparadigm
thatpredictstheresultsofthenexttrainingbyutilizinghistoricaltrainingsequences.
Thisapproachisabletofullyexploreusers‚Äôbehaviorpatternsduringtrainingprocess.
In the second phase, the symbolic regression algorithm was used to extract the
core patterns of the trained deep learning regressor into a symbolized model repre-
sented by algebraic equations, thereby inducing the ultimate symbolic rules. Initially,
we formulated a technique, called symbolic distillation, meant for the extraction of
core patterns from the trained neural network. It sampled from large-scale train-
ing data, constructed a small-scale pattern extraction dataset, and combined with
the gradient-based neural network feature importance estimation algorithm to select
the most important features from a large number of relevant features, significantly
reducing the solution space of the symbolic regression algorithm. By minimizing the
prediction error between the symbolic model and the trained neural network on the
sampled pattern extraction dataset, the symbolic model can be efficiently optimized.
Then symbolic distillation techniques were carried out on the behavior importance
estimation network and the mastery estimation network on completion of the deep
learningfittingmodel.Thereafter,thesymbolizedruleswereintegratedtoacquirethe
skill acquisition rules.
2 Results
2.1 Model overall
Thelogdataoftrainingtypicallyreferstothedatageneratedbylearnersusingonline
skilltrainingplatformstopractice.Learnersenhancetheirproficiencyinspecificskills
by training with exercises designed for those skills. Generally, each practice session
of a learner generates a training log data, which consists of five elements: user, exer-
cise, skill, score, and time. Skill training log data usually contains a large quantity of
traininglogdatafrommultiplelearners.Typically,inordertoanalyzethepatternsin
learners‚Äô skill training process, all the data is grouped by learner and sort it by prac-
tice time, resulting in a specific learner‚Äôs training data sequence on the platform (as
shown in Figure 1a). Through this data, the entire learning process of the learner can
be reviewed, including changes in their performance on practice sessions and the evo-
lutionoftheirproficiencyintheskill,i.e.,theskillacquisitionprocess.Forexample,a
learner may initially have a low score on reasoning-related exercises, indicating a lack
of proficiency in this skill. However, with increasing practice sessions, their reason-
ing ability improves, leading to gradual increases in their scores on reasoning-related
exercises. Hence, by analyzing this data, the key factors influencing learners‚Äô mastery
5a
Skills S 1: Memory S 2: Reasoning S 3: Math S 4: Attention
T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9
User
Practices
Gender
Age
Education level
Occ ‚Ä¶up ‚Ä¶ation Scores 30 60 50 80 65 40 20 35 80
b Loss
function
Behaviors feature Attention-based encoder Encoded behaviors Mastery inferer Inferred skill
for each skills sequence feature for each skills mastery degree
T 1 U E S 78
Att T
1
Att
T 2 U E S Att T
Batch normNeural network Att Batch norm Neural network 2
T 3
‚Ä¶‚Ä¶
U E S T 3 Pa Ir Ra Tm met oer di eze ld
T t-1 U E S T t-1 T t
Feature encoding Mastery inference Prediction
c
Attention-based encoder Mastery inferer
Trained neural network
X Y
Input set
Symbolic Symbolic Instance
i em sF p te io ma rt t au a tr n ie oc ne sF ee lea ctu tir oe n ‚âà
sampler Symbolic regressor
distillation distillation
X‚Äô Y‚Äô
Symbolic Symbolic
model model Symbolic
distillation Symbolic model ùë≠=ùëÆùíéùüèùíéùüê
ùíìùüê
Governing laws for skill acquisition
Fig.1 Atwo-stagemodelisproposedfortheautomateddiscoveryofsymboliclawsthatgovernskill
acquisition.Specifically,(a)providesatoyexampleoftraininglogdata.Theexampleillustratesthe
relevantinformationofninepracticesessionsforaspecificlearner,whereeachrecordcontainingfive
mainelements:learner,practice,skill,score,andtime.(b)depictstheconstructeddeepregressor.It
consistsofthreemainmodules:featureencoding,masteryinference,andscoreprediction.Themodel
takestheoutputofthepreviousnpracticesessionsandpredictsthescoreofthenextpracticesession.
The proposed model follows the autoregressive paradigm to establish the optimization objective of
the model. (c) describes the process of extracting symbolic governing laws from the trained deep
regressor.First,weproposethesymbolicdistillationmethod,whichrepresentstheblack-boxneural
network model as the closest symbolic representation. Secondly, for a trained deep regressor, it has
already embedded hidden patterns from the data into the model. The symbolic distillation method
is utilized to interpret the various modules of the deep regressor and fuse them together to obtain
symbolicgoverninglaws.
6of skills can be discovered, as well as the underlying patterns in the skill acquisition
process.
Deep learning models are a typical type of parameterized machine learning mod-
els that can fit any function or pattern by changing the values of their parameters.
Therefore,wehaveconstructedaneuralnetworkmodelcalleddeepregressor,whichis
atransformer-likestructure[35],tocapturetheunderlyingpatternsintheskillacqui-
sitionprocessusingtraininglogdata,despitenotknowingthespecificformorcontent
of these patterns. Given that the training log data follows a typical sequential model,
theautoregressivestrategyisemployedformodeling.Autoregressivestrategypredicts
the n+1-th item based on the previous n items in the sequence. In the context of our
work, it means using the learner‚Äôs data from the previous n practice sessions to pre-
dict their performance score on the n+1-th practice session. For each exercise in the
learner‚Äôs training sequence, if the model can accurately predict the score, the deep
regressor can be considerd to have successfully captured the underlying patterns of
skill acquisition.
For the constructed deep regressor, the input information consists of the exercise
log information of a specific learner for the previous n exercises, and the output is the
score of the learner in the n+1-th exercise. The deep regressor consists of three main
modules: Feature encoding, mastery inference, and score prediction, as shown in Fig.
1b. The core task of the feature encoding module is to encode the information of the
learner‚Äôs previous n practice sessions. Through an attention-based encoder, which is
constructed based on neural networks, the model can adaptively extract either the
specific features of a single practice session or the overall features of n practice ses-
sionsforthelearner(e.g.,practicefrequency).Amongthem,weconstructtheexercise
data information encoding from three aspects: user (U), exercise (E), and scheduling
(S). The information of E is weighted and summed to obtain the overall features of
the exercises (EE). The concatenation of U, EE, and S features forms the encoded
feature of the feature encoding, which is called encoded behaviors sequence feature.
The mastery inference module uses the obtained encoded behaviors sequence feature
to predict the learner‚Äôs skill mastery degree. In general, the learner‚Äôs skill mastery
degree is not equivalent to the score of the learner on the corresponding exercise, as
thescorecanalsobeinfluencedbyfactorsrelatedtotheexercise,suchasthedifficulty.
Therefore,thethree-parameterIRTmodel[38,39]isintroducedinthescoreprediction
module, which models the three parameters of exercise guessing, discrimination, and
difficulty, and calculates the learner‚Äôs score on a specific exercise based on their skill
mastery degree. The deep regressor model achieves the parameter estimation by min-
imizing the training objective, which consists of two parts. The first part is the error
between the predicted evaluation and the true observed continuous scores, calculated
by the mean absolute error (MAE). The second part is the regularization term for
the smoothness of the weights in the feature encoding, where smoother weight values
result in higher interpretability of the encoded behaviors sequence feature. However,
at the same time, it increases the fitting difficulty of the first part of the objective.
Therefore,whenapplyingthedeepregressor,itisnecessarytoobservethefittingerror
of the model with and without the regularization term. If the difference is not signifi-
cant, the regularization term can be used to obtain a more interpretable and simpler
7model. However, if the introduction of the regularization term significantly increases
the fitting error of the model, it indicates that the intrinsic model of the data is com-
plex. In this case, it is recommended to conduct experiments without introducing the
regularization term to ensure sufficient fitting to the data.
Thedeeplearningregressorwastrainedonthedatasettoestablishtherelationship
betweenpracticebehavior,skillmasterystatus,andpracticescore.Uponconvergence,
a trained neural network that captures the data features is obtained. However, due
to the black-box nature of the neural network, understanding its functional form is
challenging for humans. In this study, a symbolic distillation method is proposed to
represent the neural network model as a closest symbolic model. A teacher-student
modelisconstructed,wheretheneuralnetworkmodelservesastheteachermodeland
needs to be parsed. A symbolic regression model is used as the student model, which
learns and ‚Äùdistills‚Äù the decision-making process of the teacher model for optimal
simulation.Theinputofthesymbolicregressionalgorithmisconsistentwiththeneural
network model being explained, and the output also aims to approach the output of
the neural network as closely as possible. Additionally, an instance sampler and an
importance-based feature selection method are employed to mitigate the problem of
combinatorial explosion in symbolic regression. By applying symbolic distillation to
thefeatureencodinginthedeepregressorandtheneuralnetworkmodelinthemastery
inference module, a symbolic model is obtained. Finally, the algebraic equations from
each component are integrated to derive the symbolic governing laws of skill mastery,
as depicted in Figure 1c.
Inthissection,weprovideabriefoverviewofthedifferentmodulesofthemodel.A
moredetailedandformalexplanationofthemodelwillbeprovidedinsections4.1and
4.2 of the Method part. Subsequently, in section 2.3, we validate the effectiveness and
feasibility of the proposed method through experiments on simulated data. Finally,
in section 2.4, we apply the proposed method to a large-scale skill training dataset,
Lumosity, to explore hidden patterns and regularities within the data.
2.2 Model validation on simulated data
The correctness of patterns discovered from actual data is difficult to determine due
to the controversial nature surrounding laws and theories regarding skill acquisition.
Hence, it is difficult to validate the effectiveness and performance of proposed algo-
rithmsbasedonactualdata.Therefore,asetofsimulatedexperimentsareconstructed
to verify the accuracy of the algorithm in reproducing the specific patterns and its
robustness to noise. This study aims to demonstrate that the proposed algorithm can
effectively restore the underlying pattern in the data, irrespective of its specific form.
This is crucial because, by doing so, the patterns identified by the algorithm can be
consideredreliableinreal-worldapplications.Specifically,wefirstsetspecificgoverning
laws,i.e.,algebraicequations,werebasedonpre-existingtheoriesconcernedwithskill
acquisition, and generated a series of simulated learners and exercises with different
parameters. Next, we generated corresponding training logs, as well as predetermined
governing laws were used to establish corresponding levels of skill mastery and prac-
tice scores. Ultimately, with the use of practice session scores and exercise info, we
determined whether the algorithm could accurately restore the fixed governing laws
8using the method proposed in this study. The process and data format of generating
simulated data, the evaluation procedure of the simulation experiment, and a sample
representation of the simulated data are presented in Extended Data Figure 1.
To simulate real exercise patterns as closely as possible, we considered three com-
monly accepted laws regarding skill acquisition: Power law (Œ≤¬∑NŒ±), Exponential law
(Œ≤ ¬∑exp(Œ±¬∑N)), and Linear law (Œ≤ ¬∑Œ±¬∑N). The mutualism [40], which signifies the
beneficial impact of exercising on numerous skills, was also incorporated, like master-
ing skill A could enhance the proficiency of skill B. Additionally, we introduced the
forgetting factor and devised a proficiency degradation model based on exercise inter-
val. Ultimately, we established three sets of simulated skill mastering laws, as shown
in Table 1.
To obtain comprehensive simulated training log data, we first simulated a group
of learners and items with randomly initialized parameters. Then, a random exercise
sequencewasassignedtoeachlearner.Finally,takingintoaccounttheexerciseprocess
and the predetermined learning patterns, we calculated the learners‚Äô skill mastery
during each exercise and used the IRT model along with the item parameters to
compute corresponding scores for each exercise. During the calculation of practice
scores,weintroducedrandomGaussiannoise(Œµ‚àºN(0,Œ¥))toexplorethealgorithm‚Äôs
robustness to noise and simulate data that aligns more closely with real observations,
as observed in the study by [41, 42]. In the Method section, we will provide a detailed
explanation of the process for generating simulated data and demonstrate the data
structure along with the parameter settings used in the experiments.
Table 1 Thepredeterminedpracticerules-therelationshipfunctionbetweenpracticetimesand
skillmastery.
SETTINGS SKILLS GOVERNINGLAWS
SETTING1 SKILL1:Linear Œ≤¬∑Œ±¬∑((1‚àí¬µ)¬∑N1+¬µ¬∑N2)‚àíŒ≥¬∑(N1+N2)
Linear+Exponential SKILL2:Exponential Œ≤¬∑exp(Œ±¬∑N2)‚àíŒ≥¬∑(N1+N2)
SETTING2 SKILL1:Exponential Œ≤¬∑exp(Œ±¬∑((1‚àí¬µ)¬∑N1+¬µ¬∑N2))‚àíŒ≥¬∑(N1+N2)
Exponential+Power SKILL2:Power Œ≤¬∑N2Œ±‚àíŒ≥¬∑(N1+N2)
SETTING3 SKILL1:Power Œ≤¬∑((1‚àí¬µ)¬∑N1+¬µ¬∑N2)Œ±‚àíŒ≥¬∑(N1+N2)
Power+Linear SKILL2:Linear Œ≤¬∑Œ±¬∑N2‚àíŒ≥¬∑(N1+N2)
Here,N1 andN2 refertothenumberoftimesthelearnerpracticedskill1andskill2,respectively.
Œ±andŒ≤aretheexerciserateparameters,¬µisthemutualismfactor,Œ≥ representstheforgettingrate.
In the simulation, we only establish mutualism for skill 1, meaning that practicing skill 2 aided in
masteringskill1.Therefore,thenumberofpracticesessionsfortheskill1consideringmutualismis
definedas((1‚àí¬µ)¬∑N1+¬µ¬∑N2).
2.2.1 Model fitting analysis
To assess the overall model‚Äôs usability, we first examine the deep learning regressor‚Äôs
fitting degree. We established four groups of deep learning regressors with different
parameter configurations, namely H100+R0, H100+R1, H1000+R0, and H1000+R1.
Here,Hdenotesthenumberofhiddenneuronsperlayerintheneuralnetworkmodel.
9a b c
H100+R0 H100+R1 H1000+R0 H1000+R1 H100+R0 H100+R1 H1000+R0 H1000+R1 Skill1: Linear Skill2: Exponential
1.00E+00 1.0E+02
rorre
etulosba
gnittif
naeM111 ... 000 000 EEE --- 000 321
Skill1: Linear + Skill2: Exponential eulav
mret
noitaziralugeR11 1111111.. .......00 0000000EE EEEEEEE++ -------00 000000001
7654321
Skill1: Linear + Skill2: Exponential Œ¥
0
0
0
0.
.
.0
0
10 11
Fe
Y
Y
Y
Ya
E
E
E
Etu
S
S
S
Sre Stru
Y
Y
Y
Yc
E
E
E
Et
S
S
S
Sure V
Y
Y
Y
Ya
E
E
E
Elu
S
S
S
Se Fe
Y
Y
Y
Ya
E
E
E
Etu
S
S
S
Sre Stru
Y
Y
Y
Yc
E
E
E
Et
S
S
S
Sure V
Y
Y
Y
Ya
E
E
E
Elu
S
S
S
Se
1.00E-04 1.0E-08 1 YES NO NO YES NO NO
1.00E+00 1.0E+02
rorre
etulosba
gnittif
naeM111 ... 000 000 EEE --- 000 321
Skill1: Exponential + Skill2: Power eulav
mret
noitaziralugeR11 1111111.. .......00 0000000EE EEEEEEE++ -------00 000000001
7654321
Skill1: Exponential + Skill2: Power
Œ¥
0
0
0
0.
.
.0
0
10 11
Fe
Y
Y
Y
Ya
E
E
E
Etu
S
S
S
SreSkill1 S:
t
E
ru
Y
Y
Y
Nx
c
E
E
Ep
Ot
S
S
So un reential
V
Y
Y
Y
Na
E
E
El Ou
S
S
Se Fe
Y
Y
Y
Ya
E
E
E
Etu
S
S
S
SreSk Sil tl r2
u
Y
Y
Y
Y:
c
E
E
E
E
P
t
S
S
S
So uw reer
V
Y
Y
Y
Ya
E
E
E
Elu
S
S
S
Se
1.00E-04 11 .. 00 EE +-0 08 2 1 YES NO NO YES NO NO
rorre
etulosba
gnittif
naeM111 ... 000 000 EEE --- 000 321 Skill1: Power + Skill2: Linear eulav
mret
noitaziralugeR11 1111111.. .......00 0000000EE EEEEEEE++ -------00 000000001
7654321
Skill1: Power + Skill2: Linear
Œ¥
0
0 0. .0 00 11
Fe
Y
Y
Ya
E
E
Etu
S
S
SreSk Sil tl r1
u
Y
Y
Y:
c
E
E
E
P
t
S
S
So uw reer
V
Y
Y
Ya
E
E
Elu
S
S
Se Fe
Y
Y
Ya
E
E
Etu
S
S
Sre
Sk Si tll r2
u
Y
Y
Y:
c
E
E
E
L
t
S
S
Si un re ear
V
Y
Y
Ya
E
E
Elu
S
S
Se
1.00E-04 1.0E-08 0.1 YES NO NO YES YES YES
0 0.001 0.01 0.1 0 0.001 0.01 0.1
Noise level (Œ¥) Noise level (Œ¥) 1 YES NO NO YES NO NO
Fig.2 Theresultsofthemodelexperimentsonsimulateddata.Specifically,itpresentsmoredetailed
analysesof(a)thefittingdegreeofthedeeplearningfitterunderdifferentparametersettings,(b)the
numericalanalysisofregularizationtermsunderdifferentparametersettings,and(c)theanalysisof
thedegreeofaccuracyinreproducinggoverninglaws.Feature,Structure,andValueareintroduced
as three dimensions to evaluate the degree of formula restoration. Feature measures whether the
variables in the restored formula match those in the assumed formula. Structure indicates whether
the form and structure of the restored formula align with the assumed formula. Value represents
whethertheparametersintheformulamatchthoseintheassumedformula.Ingeneral,iftheFeature,
Structure, and Value are correctly restored, the formula can be considered completely restored and
accurate. ‚ÄùYes‚Äù indicates that the algorithm has successfully restored the predetermined pattern,
whereas‚ÄùNo‚Äùindicatesitsfailuretodoso.
The model‚Äôs fitting ability, or capacity, increases with the number of hidden neurons
[43, 44]. We set H to 100 and 1000 to simulate deep learning regressors with small
andlargefittingabilities,respectively.Secondly,Rindicateswhetherregularizationis
usedforattentionvalue.R=1or0representwithorwithoutregularizationterminthe
optimization objective of deep regressor, respectively. Adding a regularization term
canreducetherestorationequation‚Äôscomplexity,increasethemodel‚Äôsreadability,but
may also decrease its fitting ability. We trained the four deep learning regressors for
5000 epochs on simulated datasets for all three simulation settings at five error levels
(Œ¥ ‚àà {0,0.001,0.01,0.1,1}). The average fitting absolute error and the regularization
term‚Äôs value of all the deep learning regressors are plotted as a line chart in Fig. 2a
and Fig. 2b, respectively.
By analyzing the results in Fig. 2a, several interesting phenomena emerge. Firstly,
modelswithmorehiddenneurons(H1000+R0andH1000+R1)exhibitslightlybetter
fitting results than those with fewer neurons (H100+R0 and H100+R1). This finding
isconsistentwithpreviousresearchconclusionsonneuralnetworks[45].Secondly,the
model with a regularization term (R1) has a slightly worse fitting degree than the
model without a regularization term (R0), but the difference is not significant (less
10than an order of magnitude). By analyzing the results in Fig. 2b, it is found that the
regularizationtermvalueoftheR1modelisabout6to7ordersofmagnitudesmaller
than that of the R0 model, reaching a level of 1e-6 to 1e-7. This suggests that the
weight of E feature, in feature encoding module, can be regarded as 1. Additionally,
the regularization term value of the H1000+R1 model is slightly smaller than that of
the H100+R1 model. Overall, the model with an added regularization term (R1) has
little difference in fitting error compared to the model without a regularization term
(R0), but it can make the regularization term value extremely small and the weight
of E feature can be considered as 1, in the feature encoding module. Moreover, the
H1000 model outperforms the H100 model in both fitting degree and regularization
termvalue.Therefore,wewillusetheH1000+R1modelasthedeeplearningregressor
for symbolic distillation and analysis of governing laws in subsequent experiments.
2.2.2 Restoration degree analysis of governing laws
Based on the trained H1000+R1 model, the process of symbolization distillation and
extraction of governing laws is performed, and the resulting governing laws are com-
pared with the predetermined model to assess the degree of model restoration. To
conduct a more refined model evaluation, we assessed the correctness of the restored
rules from three dimensions: feature, structure, and value.
The feature dimension represents the ability of the algorithm to effectively select
the true core variables in importance analysis. In the deep learning regressor, we used
a total of five basic features, including exercise characteristics (E) and scheduling
information (S). The E feature is the one-hot encoding of the correlations between
exercises and SKILL 1, SKILL 2, and the general factor. The general factor [46, 47],
alsoknownasgeneralintelligencefactor,herewetreatitasakindofspecialskill,and
weassumethateachexerciseisassociatedwithageneralfactorfollowtheobservations
in [42]. The S feature includes two characteristics, Count and Interval, where Count
represents the number of times the current skill has been practiced in the learner‚Äôs
practice history, and Interval represents the interval between the number of times
the same skill has been practiced since the last practice. In the H1000+R1 model,
the weight of E feature can be identified as 1, so the encoded E feature EE can
be represented as Sum G, Sum S1, and Sum S2. Hence, they respectively represent
the learner‚Äôs historical practice times and the number of times they practiced on
skill 1 and skill 2. Based on the preset governing laws, the true core variables in
the model should only be Sum G, Sum S1, and Sum S2. In our envaluation, Feature,
Structure, and Value are introduced as three dimensions to evaluate the degree of
formula restoration. In the feature dimension, the model can be considered correct
only if it correctly identifies the importance of Sum G, Sum S1, and Sum S2, for
all preset laws. The structure dimension determines the formal correctness of the
algebraic equation obtained by symbolic regression, allowing for a certain degree of
error in parameter values. Finally, the value dimension indicates whether the restored
equation is within an acceptable range of the true value in terms of parameters. In
our experiment, we considered an error of less than 1% to be acceptable. The three
dimensions of feature, structure, and value are progressively interdependent in the
evaluation process.
11Governinglawsrestorationtestsareconductedonsimulateddatasetsinthreesim-
ulationsettingsinTable1andatfournoiselevels,wherethetestresultsarepresented
inFig.2c,withsomefeatureimportanceresultsshowninExtendedDataFigure2.By
observingtheresults,itiseasytofindthattheproposedmethodcanachievecompre-
hensive restoration of governing laws for both preset skills within a certain range of
noise levels (Œ¥ <0.1). In the experiment with Œ¥ set to 0.1, our model was able to suc-
cessfullyreproducetheequationforSKILL2andtheequationundertheLinearsetting
for SKILL 1. However, under the Power and Exponential settings, our method failed
tocorrectlydiscovertheunderlyingpatternofthefunctionalstructure.Webelievethe
main reason is that, during the process of symbolic regression, the existence of sig-
nificant errors leads to the emergence of multiple functions that meet the conditions
andhavedifferentformswithinaspecificrangeoferrors.Thissituationmakesitchal-
lenging for the model to identify and determine the correct function form, ultimately
resultinginmodelfailure.However,thisproblemisalmostnon-existentwhentheerror
is small. Under the setting that Œ¥=1, the proposed method can only correctly identify
the variables in the equation but fails to discover the correct equation structure. This
isbecause,inthissetting,manyofthenoisevaluesarealreadygreaterthantherange
of normal simulated scores, which is [0, 1]. As a result, the model struggles to accu-
ratelyfitthechangingtrendofmasteryandisunabletouncoverthecorrectequation.
InExtendedDataFig3,wepresenttherestorationresultsunderalternativeerrorset-
tings (symmetrical and salt&pepper) and observe similar phenomena to those under
the normal distribution setting. Overall, through simulation experiments, it can be
found that the proposed algorithm has good versatility and can achieve the restora-
tion of rules under various settings, and has a certain robustness to noise. Therefore,
wehavereasontobelievethattheproposedalgorithmcaneffectivelydiscoverthecore
behavioral patterns and formal governing laws in the real datasets.
2.3 Model application on large-scale real-world cognitive
training data
In this section, we attempted to apply the method proposed in this paper to large-
scalereal-worlddatasetstodiscoverpatternsfromrealdataandprovidenewevidence
for skill acquisition theory. For the application, we selected Lumosity as the valida-
tion platform. Lumosity (www.lumosity.com) is an online platform that offers brain
traininggamesandexercisesdesignedtoimprovecognitivefunctionssuchasmemory,
attention, flexibility, speed of processing, and problem-solving. The platform was cre-
ated by Lumos Labs and is based on the concept of neuroplasticity [48], which is the
brain‚Äôsabilitytochangeandadaptthroughoutaperson‚Äôslifetime.Lumosityhasbeen
used by millions of people worldwide to enhance their cognitive abilities and improve
their overall mental fitness. In the platform, users improve their proficiency in corre-
spondingskillsbyplayingvariousgames,whichcanbeconsideredascognitivetraining
orpracticeasmentionedearlier.Byanalyzingtheextensivepracticesessionsofnumer-
oususers,weaimtouncoverthebehavioralpatternsofskillacquisitionhiddenbehind
the data.
The dataset used in this study was shared by [42]. We performed data screening
and preprocessing based on the raw data of log data containing 50 million exercises
12a b
0.16 1.00E+02
ro rre
etu losb a g
n ittif
n
aeM0000000 ....... 0111111 9012345 H H
H H H
H1 5
1 1 5
10 0
0 0 0
00 0
0 0 0
0+ +
0 + + 0+
+R R
R RR
R1 1
0 01 0 00000000 ........ 00000000 88888888 56677889
46 47 48 49 50
eu lav
m ret n o itaziralu
geR11 11111.. .....00 0000000 00000EE EEEEE++ -----00
00000
5432101
H H H H
H
H1 5 1 1
5
10 0 0 0
0
00 0 0 0
0
0+ + 0 +
+
0+ +R R R RR R1 1 0 01
0
0.08 1.00E-06
135791113151719212325272931333537394143454749 135791113151719212325272931333537394143454749
Epoch Epoch
c d
) ‚àíùíà 0.6 Attention
‡∑ùùíà
( rorre
0.4
M Fle em xibo ir ly
ity
et
u gl no is ttb iFa 0.2
0.11 0.08 0.10
0.16
0.10 0.11
M LR aa e nt ah gs uo an gin eg
0.0
Attention Memory Flexibility Math Reasoning Language
e
Attention Flexibility Language
User featureÔºö
User
Encoded game featureÔºö
SkillÔºö
Skill
SubskillÔºö
Attention
Flexibility
Language
Math
Memory
Reasoning
SchedulingfeatureÔºö
Context
Math Memory Reasoning
Fig.3 PartialresultsontheLumositydataset.Morespecifically,(a)thechangecurveofmeanfitting
absolute error during the iteration process of the deep learning regressor; (b) the change curve of
thevalueoftheregularizationtermduringtheiterationprocesses;(c)thedistributionofprediction
errorsoneachskillscoreforthecompletediterationoftheH1000+R1model;(d)theproportionof
thenumberofpracticetimesforeachskilltothetotalnumberofpracticetimes;(e)thedistribution
offeatureimportanceforeachskillinH1000+R1.
provided by them, and the specific process can be seen in the ‚ÄùMethod‚Äù section.
13Finally, we selected 6143 users as research sample and extracted their first 2000 prac-
tice logs as the dataset. Secondly, based on the data labeling foundation of [42], we
identified 37 features as behavioral features, including all U, E, and S type features,
and the detailed feature encoding, description, type, and abbreviation can be found
in Extended Data Figure 4. In the dataset, there are a total of 85 games, and each
game is labeled with its corresponding skills and subskills. Skills includes Attention,
Flexibility, Language, Math, Memory, and Reasoning, which are the dimensions we
considered when estimating users‚Äô skill mastery. Subskill represents more fine-grained
skilltypes,whichareincludedinskill,andtherelationshipbetweenthemcanbefound
inExtendedDataFigure4.Intheexperiment,Subskillonlyservesasaninputfeature
of the model and is not the prediction target.
2.3.1 Model fitting analysis
Firstly, we need to verify the fitting performance of the deep learning regressor on
actual data and conduct model selection. In the experiment, six models are selected:
H100+R0,H100+R1,H500+R0,H500+R1,H1000+R0,andH1000+R1.Thenaming
conventionofthesemodelsisconsistentwiththatofthesimulationexperiment,where
Hrepresentsthenumberofhiddenneuronsineachlayeroftheneuralnetworkmodelin
the deep learning regressor, and R represents whether regularization term is conclued
intheoptimizationobjectiveofdeepregressor.Eachofthesixdeeplearningregressors
wastrainedfor50epochs,andthemeanfittingabsoluteerrorandregularizationterm
values of the final deep learning regressor at each epoch during the training process
were plotted as lines, which are shown in Fig. 3a and Fig. 3b, respectively. From the
resultsinFig.3a,itcanbeobservedthatafter50iterations,allmodelshaveconverged,
andtheerrorvaluesofthesixmodelsarealmostindistinguishable,rangingfrom0.086
to 0.088. This indicates that the patterns contained in the data are relatively simple,
and even neural network models with small capacity (H100) can achieve good fitting
performance. Meanwhile, the data in Fig. 3b shows that the R1 model can optimize
theregularizationtermvaluetoasmallvaluearound1e-5,whichcanbeapproximated
as 1 for the attention value. Moreover, the values of the three models with R1 setting
areverysimilar.Therefore,inthesubsequentexperiments,weselectedtheH1000+R1
modelwiththehighestfittingperformanceamongthemodelswithregularizationterm
for the deep learning regressor to conduct further experiments.
To further analyze the fitting performance of the deep learning regressor, we ran-
domly selected a portion of the data and analyzed the absolute fitting error of the
modelondifferentskills,andplottedtheirdistributioninFigure3c.Itcanbeobserved
that the error distributions of the four skills, Attention, Flexibility, Reasoning, and
Language, are relatively similar, while the average error of Memory is the lowest and
that of Math is the highest, which is about 50% higher than that of other skills. This
suggests that the behavioral pattern of Math skill training may be more complex or
there may be key variables that have not been modeled. Meanwhile, the proportion
of training log data for each skill to the total data is shown in Figure 3d, where the
log data proportions of Math, Reasoning, and Language are relatively low.
142.3.2 Feature importance analysis
ForthetrainedH1000+R1deeplearningregressor,thefeatureimportancecorrespond-
ing to each skill is visualized (see Figure 3e) to analyze the most important variable
factors that affect skill mastery from the perspective of the deep learning regressor.
SeveralinterestingphenomenacanbeobservedfromtheresultsinFig.3e.Firstly,for
all skills, age and education level are important factors that influence skill acquisition
for all skills. As games serve as an exercise, they have the potential to affect critical
factors such as the user‚Äôs reaction speed and attention. Education level also describes
the user‚Äôs initial abilities (before practice) to some extent, especially for Math and
Language.Secondly,sincetheattentionvalueisapproximatedas1,theencodedgame
featuresrepresenttheuser‚Äôshistoricalpracticetimesonthecorrespondingskillorsub-
skill, which are represented by the ‚Äù#‚Äù symbol. It can be observed that the skill and
somesubskillfeaturesofeachskillarealsoimportant.Thisisconsistentwiththebasic
assumptionofskillacquisition,thatpracticetimesarethemostcrucialfactoraffecting
skill acquisition. Finally, it can be observed that the related subskill (SR RE) of Rea-
soning has a certain influence on the mastery of Language skill, which may indicate
the existence of a certain mutualism. However, no mutualism was observed for other
skills. This is consistent with the observations and conclusions in the paper [49, 50].
2.3.3 Analysis of discovered symbolic governing laws
The governing laws discovered by the model are organized and presented in Figure 4
withthebestequationsforeachofthesixskillsatvariousequationcomplexities.Due
tothecharacteristicsofthesymbolregressionalgorithm,higherequationcomplexities
generallyresultinstrongermodelfittingcapabilities,andtherefore,highercomplexity
equations often have better fitting performance. However, from the results in Figure
4,itcanbeobservedthatfortheresultsofthesixskills,thefittingerrorofthemodel
hardly decreases significantly after the model complexity exceeds a certain threshold.
We used the number of features used in the equations as an indicator and focused on
analyzing the best fitting equations for each skill at each feature quantity (marked
in red in Fig.4). For all skills, good model fitting performance can be achieved when
the feature quantity is 1, indicating that the core pattern has been revealed, and
increasing the complexity and number of variables will not significantly increase the
modelfittingperformance.Therefore,wefocusedonanalyzingthebestfittingequation
with a feature quantity of 1 (marked in yellow in Fig.4).
In order to analyze the properties of the discovered functions, we compared the
discoveredequationswiththeclassicalPowerlawandExponentiallaw.Weoptimized
the parameters in the classic Power Law and Exponential Law equations by utilizing
the coding features and skill mastery degree of all learners in six skills in Lumosity
(provided by deep regressor, which is consistent with the formulas discovered by sym-
bolic regression). Additionally, we evaluated the goodness of fit of the equations to
the data using the classical fitting metric R2 [51]. Finally, we summarized the results
in Table 2. In addition, to enhance the assessment of pattern conformity, the relevant
outcomeswerevisualized.Asampleoffivelearnerswasrandomlychosen,andtheskill
mastery estimation curve derived from the discovered patterns was juxtaposed with
150.05 0.05
CX.#Feat. Equation C 1X.#Fe 0at. Eq ùüéu .ùüèa ùüét ùüéion
1 0 ‚àíùüé.ùüéùüñùüëùüñ 3 1 ùüé.ùüéùüéùüéùüíùüñùüó√ó#ùë≠ùë≥
)EA
M
0.04 5
7
1
2
ùüé.ùüìùüëùüïùüé. ‚àíùüìùüí #ùüí ùë®‚àí ùëª‚àí#ùë® #ùëª ùëΩ‚àí ùë∞_ùüé ùë®.ùüé ùëªùüñùüé ‚àíùüí
ùüé.ùüéùüñùüíùüî
)EA
M
0.04 4 5
6
1 1
1
ùüé.ùüîùê¨ ùüéùê¢ùêß ùüî#ùüé ùë≠ √ó. ùë≥ùüé ùüé ùê•ùüé . ùê®ùüéùüé ùüí ùê†ùüñùüí ùüï #ùüó ‚àí ùë≠ùüè ùë≥√ó ùüè.# ‚àíùüèùë≠ ùüñ ùüëùë≥
.ùüìùüê
( rorrE
etulosbA
naeM
00 .. 00 23
1
119
1
3
5
2
3
3
4
ùüé ùüé. .ùüì ùüìùüë
ùüëùüé
ùüé
ùüé.ùüì
‚àí
‚àíùüé ùüë.
ùüé
##ùüì
ùë®‚àí
ùë®ùüë ùëªùëªùüé
+#
+‚àí
ùë®
##
ùë∫ùëª
ùë®ùë∫#
+
ùë®
_ùë®
ùë®_#
ùëªùëª
ùë®ùë∫
‚àíùëª‚àí
ùë®
‚àí
ùüê_ùüè
ùë®
.ùüëùüê.
ùëª
ùüñùüñ
.ùüë‚àí
√óùüï
ùüñùüê
#√ó
√ó
ùëΩ.ùüí
ùë∞#
#
_ùüé
ùë®ùëΩ
ùëΩ√ó
ùëªùë∞ ùë∞__
‚àí#
ùë®ùë®
ùëΩ
ùüéùëªùëª
ùë∞
.‚àí
ùüé_‚àí
ùë®
ùüêùüéùüé ùüìùëª.
.
ùüñùüé ùüë‚àíùüñ √óùüèùüñ
ùüé.
ùüè
ùë®ùüî
ùüéùüó ‚àí‚àíùüêùüí
ùüéùüé .. ùüéùüé ùüñùüó ùüñùüê ùüïùüí
( rorrE
etulosbA
naeM
00 .. 00 23
1
1
1187
9
10
3
5
11
2
2
2 2
2
ùüé.ùüé ùüè. ùüëùüè
ùüé
ùüóùüë
.ùüé
ùüë
ùüè
‚àíùüé.
ùüë‚àí
.ùüè ùüéùüïùüèùüë
.ùüéùüé
ùüë
ùüë‚àíùüë
..
ùüó
ùüëùüêùüè
ùüé‚àí
ùüï‚àíùüèùüëùüé
.ùüïùüê
ùüë
√ó. ùüéùüéùüè
ùüè√ó‚àí
..
ùüéùüë
ùüñùüëùüê
.ùêûùüéùüê
ùüë
ùüó√óùüè
ùê±.
ùüì
ùüñ‚àí
ùüó
ùüê
ùê©
ùüé
ùüó√óùüè
.ùüé
√ó
#‚àí
ùüóùüï.
ùëªùüé
ùüóùüê
ùüé
ùë∫ùêû
√ó
.
_ùüíùüê
ùüó.ùê±
ùë≠ùüéùüéùüé
ùüñ
ùë≥ùê©
#ùüé.
ùüó
√óùëª√ó
ùüó
ùüî
ùë∫#‚àí
ùüó
_ùüñ
#ùëªùüé
ùë≠ùüé
ùüë
ùë∫ùüè
ùë≠. ùë≥_ùüó
.
+ùë≠
ùë≥#
√óùüé
ùë≥
#ùüñ
ùëª
‚àíùë≠ùüèùüï
ùë∫
√ó
ùüéùë≥_
#
.ùüè#
ùüèùë≠
#ùë≠
ùëª
ùüîùë≥ùüó
√óùë≥
ùüè+
ùë≠ùë∫#√ó
#ùë≥
+_ùë≠
ùë≠
‚àí
ùë≠ùë≥#
ùüéùë≥
ùüé ùë≥.
.ùë≠
ùüè
‚àí+
ùüèùüî
ùüéùë≥
ùüè
ùüó.#
ùüè
ùüèùüëùë≠ #ùüïùë≥
ùëªùë∫_ùë≠ùë≥
Attention Flexibility
0.01 0.01
1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15
Complexity (CX.) Complexity (CX.)
0.043 0.027
CX. #Feat. Equation CX.#Feat. Equation
1 0 ùüé.ùüèùüïùüè 1 0 ùüé.ùüéùüìùüïùüï
0.038 4 1 ùüé.ùüéùüíùüêùüè√óùê•ùê®ùê†#ùë≥ùë® 0.025 3 1 ùüé.ùüéùüéùüéùüíùüíùüï√ó#ùë∫ùë®_ùë®ùëª
)EA
M ( rorrE
etulosbA
naeM
00 .. 00 23 83
1
115
6 7
8
9
1
3
5
1
2 2
2
2
2
3
3
#ùëΩùë≠_# ùë≥ùëΩ
ùë®#
ùë≠
+ùëΩ _ùë≥ùë≠
ùüéùë®#
_
.ùëΩ
ùüéùë≥ùê• ùüîùê®
.ùë®
ùüèùë≠ùüé ùüêùê†ùüé ùüñùüé_. ùüèùüï.
ùë≥
.ùüé ùüè#ùüé
+ùë®
ùüñùüí ùüéùë≥ùüí
ùüï
.ùüéùüñ
ùüèùüé
ùë®
ùüéùüê
.
+
ùüñùüêùüì
..
ùüè
ùüïùüñ
ùüëùüé
√ó
ùüî+
ùüé
+√ó
ùüêùüì
.+
ùüèùüé
ùüëùüé
ùê• ùüé#ùê®
.
ùüèùüé
‚àí
.ùüî
ùë∫ ùüéùê† ùüëùüî.ùëπ
ùüí
ùüé√ó
ùüé
ùüè‚àí#
ùüè
ùüì_
.
ùüíùëπùë∫
ùüè#
ùüì
ùüè
ùüé
‚àíùëπ ùë¨
ùüèùüïùë≥
√ó
.
ùüí_
ùüèùë®
ùüé+ √óùëπ
#
ùüè
√ó
.ùüé
ùë¨ ùüè#
ùë∫
ùüí#.ùüê
ùüéùëπ
ùüèùë≥
ùë∫+ùüó
√ó
.
ùüíùë®
ùëπ_ùüì
ùüóùëπ#
ùüé_
√óùüóùüé ùëπùë≥
ùë¨
.. ùüñùüñùüëùë®
ùüéùüé
ùë¨ùüé
ùüó. .ùüê
#ùüéùüí
ùüóùüêùüë ùë∫.ùüêùüñ
ùëπ
ùüó#ùüé
_ùë∫
ùüñùüó
ùëπùëπ ùë¨_ #ùëπ
√ó
ùë∫ùë¨
# ùëπùë≥ _ùë®
ùëπùë¨√ó#ùë≥ùë®
)EA
M ( rorrE
etulosbA
naeM
00 .. 00 22 13
1
1154
7
9
1
3
4
11
2
2
2
3
3
ùüé.ùüéùüêùüéùüé ùüñ.ùüó
√óùüé ùüë.
ùüî
ùê•ùüó ùê®‚àíùüë
ùê†ùüé
ùüî.ùüé
ùüéùüó
ùüê‚àí. .ùüêùüó ùüé√óùüìùüë
ùüîùüê
#‚àíùüî ùüîùüé
√ó
ùë∫
ùüè‚àíùüé
. ùë®ùüó
ùüê
#. √ó_ùüé
ùüì
ùë∫
ùë®√ó#ùüè
ùë®ùüì ùëªùë∫
ùüêùüê
#
_ùë®‚àíùüó
+ùë®
√óùë∫_
ùë®
ùëªùë®√ó
#
## _ùëª
+
ùëµ
ùë∫ùë®ùë∫ùê•ùê®
ùë®+ùë®
ùëª
ùë™#ùê†
___
ùëµ+
ùë®#
ùë¥ùë®#
ùë™
ùëªùëµ
#ùëªùë∫
ùë®_ùë™
+ùëµùë®
ùë¥‚àí
‚àí__
ùüé ùë™ùë¥
#ùë®ùë®
. ùüéùüé
_
ùëµùëª
ùë¥ùë®ùüê
‚àí
.ùüé
ùë™ùüë ùë®ùüî ùüî‚àí
ùüñ
_ùë¥ùüé
.
ùüó‚àí
ùüê.ùüé
ùüî
ùë®ùüé
ùüëùüê .ùüì
ùüé
√ó
‚àíùüêùüî
‚àíùüì
ùë®ùüé
ùüéùüî
.ùüé .‚àíùüê ùüéùüë
ùüé
ùüñùüì
.ùüé ùüóùüê ùüîùüëùüí
√óùë®
0.023 0.019
Language Math
0.018 0.017
1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15
Complexity (CX.) Complexity (CX.)
0.06 0.10
CX.#Feat. Equation CX.#Feat. Equation
1 0 ùüé.ùüêùüêùüï 0.09 1 0 ùüé.ùüìùüïùüñ
0.05 3 1 ùüé.ùüéùüéùüéùüìùüñùüñ√ó#ùë¥ùë¨ 4 1 ùüé.ùüèùüêùüî√óùê•ùê®ùê†#ùë∫ùë®_ùë®ùëª
)EA 4 1 ùüé.ùüéùüíùüéùüé√óùê•ùê®ùê†#ùë¥ùë¨ )EA 0.08 6 8 2 2 ùüé.ùüèùüêùüìùüé. √óùüèùüè ùê•ùê®ùüì ùê†√ó #ùê• ùëπùê® ùë¨ùê† +#ùëπ #ùë¨ ùë∫ùë®+ _ùë®# ùëªùë∫ùë®_ ‚àíùë® ùüéùëª .ùüéùüíùüñùüì
M ( rorrE
etulosbA
naeM
00 .. 00 34
1
195 6
8
3
5
11 1
2
2
3
ùüé.ùüéùüé ùüí. ùüèùüé ùüíùüí √óùüêùüê ùê•√ó
ùê®ùê†ùüé
ùê•
#.
ùê®
ùë≠ùüé
ùê†ùüé
ùëπùüñ
#
_.ùüíùüé
ùë¥ùüê
ùë¥ùüî. ùë¨ùüîùüé
ùë¨√ó
+ùüñùüñ ‚àíùüí
#ùê•
‚àí#
ùê®
ùë¥ùüì ùêûùë¥
ùê†
ùê±
ùë¨ùüé√ó ùê©ùë¨
#
.
‚àíùê•
ùüî
‚àíùüé ùë¥ùê®. ùêûùüëùüé ùüéùê† ùê±ùë¨ùüî
.ùüï
ùê©ùüéùüè #
+
ùüéùüï
ùüé
‚àíùë¥
.
ùüïùüé‚àí
ùüé#
ùüîùüè
.ùë¨
ùüï
ùüéùë∫
ùüìùüè
ùüéùëπ
ùüí
√ó. ùüï√ó‚àíùüê
_
ùüì##ùëπùüé ùüóùë¥ùüé
ùë¥ùë¨
√ó. ùë¨ùë¨ùüê
#+
‚àíùüî
‚àí
ùë¥ùüêùüë
#
ùë¨.ùüé
ùüî
ùë∫
‚àíùüé. ùëπùüê
_
#ùëπùüî
ùë∫ùë¨
ùëπ4
_‚àí ùëπùë¨ùüè. +ùüë ùüèùüê
.ùüëùüï
M ( rorrE
etulosbA
naeM
000 ... 000 567 1
1
1
1
119 10
2
3
4
5
2 2
2
3
3
3
3
ùüéùüé
.ùüé
ùüé
ùüé.
.ùüé
ùüóùüè
.ùüéùüé
ùüé.
ùüëùüóùüê.
ùüé
ùüóùüíùüé
ùüëùüî
ùüëùüëùüé √óùüî
ùüí
ùüíùüë.
√ó
ùê•ùüë √óùüé
√ó
ùê®ùüóùüì ùê†ùê•ùüó
ùê•ùê•ùê®√ó
ùê®ùê®ùüñ #√ó
ùê†ùê†
ùê†
ùë∫ùüó
ùê•
#ùë®ùê®ùê•
#
#ùê®√ó
ùë∫_ùê†
ùë®ùë∫ùëπ
ùë®ùê† ùëªùë®ùê•
_#
ùë¨ùê® ùë®#
_ùëπ
ùëªùê†
ùë®
++ùë∫
ùëªùë¨
ùüéùë® +#
#
._ ùüéùë∫ ùüé+ùë∫ùë®
+
ùüë
.ùë®
ùë®
ùüéùüñùëª ùüé_
ùüé
ùüëùüé_
.ùë® ùüñ√ó
ùë®
ùüé. √óùüé
ùüèùëª
ùüëùëªùüó
√óùüñ
ùê•ùüé
ùê®ùüî
ùüé+ ‚àí.
ùê†
ùê•ùüñùüí
√ó
ùê®ùüé
#
ùê†ùüéùüê
√ó
ùëπ. .ùüê #ùê•ùüé
ùë¨ùüé
ùê®ùê•
ùëπùüë ùê®√ó
ùê†ùüì
ùë¨+ùüê
ùê†
ùüî# #ùüè
ùêû
+ùüé
ùëπ#ùë∫ ùê±√ó
ùê©
ùüéùë®
ùë¨ùë∫
√ó
.ùë®_
‚àí
ùüóùê• #ùë®ùê®
+
ùüó_ ùüéùë∑ùë®
ùüêùê† ùëª
.ùüé
ùüê
#ùë≥ùëª
ùë∑ùüî+ .#
_
ùë≥ùüï
ùüë+
_ùëπùëπ ùëπùüî#
√ó
ùë¨ùë¨ùë¨ ùüóùüêùëπ
√ó##
#ùüé.
ùë∑ùë¨
ùë∑
ùë∫.ùüè
ùüé
ùë®ùë≥
ùë≥ùüé
_ùüí
_
_
ùë®ùëπùüó
ùëπ
ùëªùë¨ùüì ùë¨ùüñ
0.02
0.04
Memory Reasoning
0.01 0.03
1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15
Complexity (CX.) Complexity (CX.)
Fig. 4 ThesymbolicgoverninglawsdiscoveredbythemodelontheLumositydataset.Thesymbol
regressionresultsforsixrelatedskillsandtheircorrespondingcomplexities(CX.<15)arepresented,
including algebraic equations and their mean absolute error (MAE). The best-fitting equation with
thesamenumberoffeaturesismarkedinred.
theoutcomesobtainedfromthedeepregressor.Thefindingsaregraphicallyillustrated
in Extended Data Figure 5.
Regarding the results shown in Table 2, several phenomena were observed. First,
when we observed the fitting performance of two classic learning laws, Power law
and Exponential law, on various skill data, it was found that there were significant
differences in their fitting degrees for different skills. Power law had better fitting
16Table 2 Thecomparisonsonthegoodnessoffitbetweenthediscovered
lawsandtheclassicalPowerlawandExponentiallawforsixskillsusing
theLumositydataset.R2 isintroducedasameasureofgoodnessoffit,
whichrepresentstheproportionofthetargetvariable‚Äôsvarianceexplained
bythemodelandrangesfrom0to1.Ahighervalueindicatesabetterfit.
Themodelwiththehighestgoodnessoffitwithinthesameskillisbolded,
whilethemodelrankedsecondisunderlined.
Skill LawType Equation R2‚Üë
Power 0.340‚àí0.821√ó#AT‚àí0.114 0.850
Attention Exponential ‚àí0.049‚àí0.194√óexp(‚àí0.005√ó#AT) 0.809
Discovered 0.544‚àí#AT‚àí0.081 0.856
Power ‚àí0.376+0.231√ó#FL0.139 0.841
Flexibility Exponential 0.133‚àí0.219√óexp(‚àí0.011√ó#FL) 0.850
Discovered 0.133‚àí0.219√óexp(‚àí0.011√ó#FL) 0.850
Power 0.026+0.033√ó#LA0.354 0.527
Language Exponential ‚àí0.103+0.214√óexp(0.003√ó#LA) 0.477
Discovered 0.051√ó#LA0.295 0.525
Power ‚àí0.139+0.113√ó#MA0.121 0.202
Math Exponential ‚àí0.054+0.086√óexp(0.002√ó#MA) 0.115
Discovered 0.955‚àí#SAAT‚àí0.024 0.278
Power ‚àí0.020+0.020√ó#ME0.427 0.829
Memory Exponential 0.269‚àí0.279√óexp(‚àí0.006√ó#ME) 0.885
Discovered 0.268‚àí0.6370.015√ó#ME+2.60 0.891
Power ‚àí0.133+0.376√ó#RE0.159 0.605
Reasoning Exponential ‚àí0.041+0.513√óexp(‚àí0.003√ó#RE) 0.265
Discovered 0.126√ólog(#SAAT) 0.698
performance on Attention, Language, Math, and Reasoning skills, while Exponential
law performed better on Flexibility and Memory skills. This difference has also been
observed in previous studies [4, 5], providing further evidence suggesting a correla-
tion between the patterns of skill acquisition and the skills themselves. Second, it can
be found that the equations derived by our method mostly exhibited optimal per-
formance on all skills. We discovered that for Attention, Flexibility, Language, and
Reasoning skills, we were able to identify the better-fitting law than Power law and
Exponential law. The equations derived from symbolic regression generally produced
better fitting results in most instances (except in Language skill). But, the difference
was practically negligible and mainly attributable to minor variations in parameters.
This also indicates the effectiveness of our proposed method, despite the lack of prior
knowledge about the classical laws of skill acquisition. Third, for Math and Mem-
ory skills, our method discovered patterns that were inconsistent with the classical
practice laws, and exhibited a certain advantage in terms of fitting degree compared
to the classical laws. This further demonstrates the potential of our method to dis-
cover new patterns and provide new insights for researchers from a purely empirical
or data-oriented perspective.
17When we closely observe the form of the equations we have discovered, it
also reveals some intriguing phenomena. First, several new types of practice pat-
terns were discovered. Specifically, logarithmic law was found in some of the fitted
equations, which appears to be a type that has not been previously mentioned in
research. Furthermore, two inverse forms of power law were observed for Attention
(0.544‚àí#AT‚àí0.0804) and Language (0.0506√ó#LA0.295), which are the same form
butwithoppositeparameters.Itispossiblethat0.544‚àí#AT‚àí0.0804 shouldbeconsid-
ered a new type of anti-power law. Then, the general factor theory was not confirmed
in this study, suggesting that it may not be an important influencing factor in skill
acquisition.Additionally,thephenomenonofskillforgettingwasnotclearlyobserved.
Finally, it was found that the main factors affecting Language and Math were not the
number of times the skills were practiced, but rather the number of times attention
selection(SA AT)waspracticed.ThismaybeexplainedbythefactthattheLumosity
platformpresentspracticeintheformofgames,whichdeterminesthatattentionisan
important factor affecting game scores. Therefore, training attention can effectively
improve scores in other games. However, it may be difficult to improve Language and
Math skills through gamified practice.
Table 3 Theresultsofthegoodnessoffit(R2)ofallcomparativemodelsontheLumosity
datasetarereported.Allreportedresultsaretheaverageof20independentexperiments.The
overallresultsofthedatasetandtheresultsforeachskillareseparatelyreported.Theresultsof
thebest-performingmodelarehighlightedinbold,whilethesecond-bestmodelismarkedwithan
underline.At-testforsignificancetestingwasconductedbetweentheresultsofthebestmodel
andthesecond-bestmodel,andthecorrespondingp-valuesarereported.Duringthe
implementationofthet-test,weemployedthebootstraptechniquetoenhancethereliabilityof
ourfindings.Ineachindependentexperiment,werandomlysampled50%ofthedatasetand
subsequentlyrepeatedthisiterativeprocedurefivetimes.Resultswithsignificantdifferences
(p-value<0.01)areindicatedwithanasterisk(*).
R2‚Üë Overall Attention Flexibility Language Math Memory Reasoning
AFM 0.255 0.254 0.362 0.305 0.170 0.162 0.298
LGM 0.370 0.380 0.523 0.519 0.297 0.225 0.426
BLM 0.439 0.459 0.606 0.573 0.448 0.267 0.462
TLM 0.486 0.503 0.624 0.613 0.477 0.301 0.593
TLMF 0.471 0.475 0.614 0.624 0.483 0.287 0.592
IM 0.503 0.516 0.633 0.629 0.517 0.313 0.599
ADM 0.627* 0.646* 0.645* 0.641* 0.423 0.643* 0.628*
p-value 8.93E-52 2.12E-45 2.76E-05 5.59E-08 1.86E-39 2.14E-50 8.43E-20
2.3.4 Comparison with learning models
The study model characterizes the learning model of learners‚Äô performance during
the learning process, which is a hot research topic. In order to better understand the
performance of the proposed model, we plan to compare our model with some of the
latestlearningmodelsandexplorethegoodnessoffitandfittingefficiencyofdifferent
models to large-scale data. Four learning curve models are introduced as compara-
tive models: the baseline learning model (BLM), the two-timescale learning model
18Table 4 TheresultsoftheBICofallcomparativemodelsontheLumositydatasetarereported.
Allreportedresultsaretheaverageof20independentexperiments.Theoverallresultsofthe
datasetandtheresultsforeachskillareseparatelyreported.Theresultsofthebest-performing
modelarehighlightedinbold,whilethesecond-bestmodelismarkedwithanunderline.At-test
forsignificancetestingwasconductedbetweentheresultsofthebestmodelandthesecond-best
model,andthecorrespondingp-valuesarereported.Duringtheimplementationofthet-test,we
employedthebootstraptechniquetoenhancethereliabilityofourfindings.Ineachindependent
experiment,werandomlysampled50%ofthedatasetandsubsequentlyrepeatedthisiterative
procedurefivetimes.Resultswithsignificantdifferences(p-value<0.01)areindicatedwithan
asterisk(*).AllBICvaluesareexpressedinunitsof107.
BIC‚Üì Overall Attention Flexibility Language Math Memory Reasoning
AFM -3.814 -3.951 -3.983 -4.017 -3.486 -3.601 -4.112
LGM -4.020 -4.179 -4.340 -4.469 -3.691 -3.697 -4.361
BLM -4.217 -4.400 -4.624 -4.691 -4.058 -3.812 -4.505
TLM -4.264 -4.440 -4.698 -4.821 -4.059 -3.815 -4.760
TLMF -4.174 -4.321 -4.541 -4.684 -3.986 -3.736 -4.719
IM -4.242 -4.413 -4.706 -4.725 -4.080 -3.771 -4.741
ADM -4.779* -4.979* -4.820* -4.912* -4.135* -4.771* -4.882*
p-value 5.59E-63 2.83E-46 7.59E-24 1.13E-15 7.97E-15 2.44E-69 3.08E-25
(TLM),thetwo-timescalelearningmodelwithforgetting(TLMF),andtheinteractive
model (IM), mentioned in the article [52]. At the same time, we introduce the clas-
sical learning curve model additive factor model (AFM) [53] and the latest learning
growth model (LGM) [54] used for modeling learner growth. Our model combines the
discovered laws with the 3-parameter IRT model for learning performance prediction,
called the Auto-discovered model (ADM). All six comparison models are individual
learning models that model the learning process of a single learner, and each learner
has their own independent parameters, such as learning speed and initial level. Our
model, on the other hand, is a general learning model where all learners share the
same set of parameter combinations for learning patterns. We introduce two com-
monlyusedmodelevaluationmetricsforperformancecomparison.First,theR2 index
is used to evaluate the degree of fitting, as mentioned in the previous chapters. Sec-
ondly, we introduce the Bayesian Information Criterion (BIC) index [55, 56], which
aims to find the most suitable model among a group of alternative models. The BIC
score combines the data likelihood of the given model and the number of parameters
inthemodel,aimingtoselectamodelwithasmallernumberofparametersandahigh
degree of data fitting. A lower BIC score is considered better. We conducted parame-
ter estimation for the models on the Lumosity dataset and summarized the results of
the optimal model in Table 3 and 4.
Through comparisons, we found that our model exhibits significantly superior
goodnessoffitcomparedtomostofthecomparativemodelsinmostcases.Theperfor-
manceofourmodeliscomparabletothesecond-bestmodelintheskillsofFlexibility,
Language, and Reasoning. However, our model demonstrates a substantial advantage
over all comparative models in the skills of Attention and Memory, as well as in over-
all data performance. In terms of fitting the Math skill, our ADM model slightly
underperforms compared to the model proposed in [52]. In the BIC comparison, our
model shows a significant advantage over the six comparative models. This is mainly
19attributed to our model‚Äôs smaller parameter size while achieving a higher level of
fit. This finding underscores a notable advantage of our approach in identifying the
fundamental patterns of the training sequences.
3 Discussion
3.1 Conclusion
In this paper, we propose a two-stage model that can discover the governing laws of
skill acquisition from NODS. The core idea is to combine the strong fitting ability
of deep learning models for high-dimensional data with the good interpretability of
symbolic regression models to construct an efficient machine learning framework that
discovers symbolic patterns from large-scale data. Through simulation experiments,
it indicates that the proposed algorithm can accurately restore various types of rules
within a certain range of noise with the help of the universal fitting ability of neural
network models, which verifies the effectiveness of the proposed method. Secondly,
by applying the proposed method to real large-scale cognitive skill training log data,
some new patterns are revealed. In addition, through comparative experiments, it
was observed that the patterns discovered by our proposed method exhibited a good
fit in comparison to the classical practice laws. Furthermore, our model showed a
higher degree of fitting compared to some other learning models, even with fewer
parameters. These results collectively provide evidence supporting the effectiveness
and applicability of the proposed methodology.
Additionally, this study uncovered intriguing findings by conducting experiments
onalarge-scalecognitivetrainingdatasetcalledLumosity.Firstly,duringtheanalysis
of feature importance, a correlation and mutualism were identified between Language
skill and Reasoning skill, aligning with the findings in [49, 50]. Secondly, as observed
in [4, 5], the optimal fit patterns vary among different skills, supporting the notion of
a potential connection between pattern forms and skills. Thirdly, employing machine
learning models, we identified two novel pattern forms that deviate from the conven-
tionalPowerandExponentiallaws.Here,alogarithmiclawwasobservedinReasoning
skills. And two mathematically inverse forms of Power Law were discovered.
In summary, the research in this paper is an exploration of psychological research
under the big data paradigm. We focus on the problem of skill acquisition and use
artificial intelligence technology and large-scale NODS to explore the core patterns
behind it in a purely data-driven manner, providing new research evidence and novel
methods for psychology.
3.2 Limitation
However, this study also has certain limitations. First, as a data-driven method, the
conclusions obtained in this study may be affected by biases or sparsity in the data,
thus limiting the discovered rules. Therefore, data quality is crucial for our method.
Second, in this study, we introduced the IRT model to exclude practice variations
when calculating skill proficiency. However, this incorporation of prior information
makes the model not fully data-driven, and the introduction of the IRT model is
20considered necessary to ensure stability and uniqueness in the inferred skill mastery
degree.Thisisaproblemthatneedstobeaddressedinourfuturework.Anotherissue
is the selection of symbol regression results, where we adopted a heuristic selection
approach based on the results and scenarios. Although there are some indicators like
AIC and BIC for model selection, they still cannot guarantee their generality across
different scenarios, as the effectiveness of relevant methods can be influenced by fac-
tors such as data error distribution and data scale. Therefore, in practical research,
themodelselectionmethodisoftenmanuallydeterminedbytheresearchersaccording
to the specific scenario. We believe that exploring a universally applicable and robust
model selection method is necessary in our future works. In addition, the method we
have constructed can only handle continuous scoring exercise data and is not applica-
ble to many scenarios with discrete feedback. In future research, we will attempt to
explorenewmethodstoovercomethischallenge.Finally,weacknowledgethatdiscov-
ering learning patterns is a challenging task, and our study only reveals phenomena
observed in the Lumosity dataset. The generalizability and underlying mechanisms
requirefurtherextensiveresearch.Thispaperservesasanewtooltoassistresearchers
inconvenientlydiscoveringimplicitpatternsindata,offeringsupportingevidenceand
new insights for relevant studies.
4 Method
4.1 Deep learning regression for training log data
4.1.1 Problem formulation
Training log data (D) includes valuable information such as the user set U =
{u ,u ,...,u }, the exercise set E = {e ,e ,...,e }, and the skill set S =
1 2 m 1 2 n
{s ,s ,...,s }. A user‚Äôs training history, denoted as P = {p1 ,p2 ,...,pl }, is a
1 2 k ui ui ui ui
sequence of exercises, where u refers to the user, pl is the l-th exercise of u and
i ui i
pl ‚àà E. The sequence P is obtained by sorting the training history of user u by
ui ui i
time. The corresponding grade for P is usually normalized and can be denoted as
ui
G = {g1 ,g2 ,...,gl }, where gl is the grade for the l-th exercise of user u and
ui ui ui ui ui i
g u )l ‚àà[0,1]. The index matrix indicating the relationship between the exercises and
( i
skills is usually called the Q-matrix where Q ‚àà Rn√ók. To simplify our notation sys-
tem, we denote Q[pl ] as the skill index for exercise pl , with the skill index being
represented by one-hu oi t encoding on skills, where Q[pl ]u ‚àài Rk.
ui
Exercise scores can generally reflect users‚Äô level of skill mastery, and exploring the
evolution of these scores can reveal changes in knowledge acquisition. By correlating
these scores with practice behavior, the skill acquisition patterns of learners can be
determined. Observing the skill acquisition patterns of numerous learners may reveal
underlyinggoverninglawsthatinvolvepsychologicalactivitieslikepractice,forgetting,
andskilltransfer.Anautoregressiveself-supervisedparadigmisintroducedtocapture
thecorepatternsofdata.Itsmainobjectiveistoconstructafunctionf whichpredicts
the score of any training behavior pl using its previous sequence of practice sessions,
ui
namely, f(p1 ,p2 ,...,pl‚àí1) ‚âà gl . If f can predict equivalent scores accurately, it is
ui ui ui ui
believed that there exist core patterns or rules about skill acquisition within f. To
21construct f, a deep learning model have devised which is inspired by the Transformer
architecture. The entire score prediction process is divided into three primary stages:
feature encoding, skill mastery inference, and prediction.
4.1.2 Feature encoding
The core task of the Feature Encoding module is to extract the features of the histor-
ical exercise sequence. Our method involves characterizing the static features of each
training behavior, building a model to predict the contribution level of each prac-
tice session to corresponding skill learning, weighting the static feature sequence, and
obtaining the feature encoding of the practice behavior sequence.
Our study aims to develop the user‚Äôs practice behavior from three dimensions,
namely, user attributes, exercise characteristics, and scheduling information. User
attributestypicallyconsistofpersonaldetailssuchasage,occupation,andeducation,
which can offer valuable information about motivation, initial skill level, and learning
speed that could impact skill learning. We will denote u ‚Äôs user attributes as U[u ].
i i
Exercise characteristics aim at conveying relevant aspects of the exercise, notably the
relationship between the exercise and the skill, and the kind of stimulus incorporated
in the exercise. We will use the notation E[pl‚àí1] to denote the exercise characteristics
ui
of exercise pl‚àí1. Scheduling information comprises details about the user during the
ui
exerciseprocess,suchasthenumberofexercisesintendedtolearnaskillandthetime
between successive exercises. The scheduling information pertains to the data about
the user‚Äôs actions and behavior during the ongoing exercise, which is subsequently
usedforprediction.Wewillrefertotheschedulinginformationofuseru duringexer-
i
cise pl by using the notation S[pl ]. Typically, features can take on two main forms,
ui ui
namely, continuous and discrete. For discrete features, we employ one-hot encoding
forencodingpurposes,whileforcontinuousvariables,wedirectlyusetheircontinuous
values as feature values. In the end, the static features of a user‚Äôs practice behav-
ior can be documented as SF[pl‚àí1] = U[u ]‚äïE[pl‚àí1]‚äïS[pl ], where ‚äï means the
ui i ui ui
concatenation operator.
Since different training behaviors may have distinct impacts on skill learning, we
employ neural networks to determine the significance of specific behaviors for skill
acquisition. First, to normalize the static features and expedite model training while
enhancing stability, we perform batch normalization, given that the distinct dimen-
sions of the static features have dissimilar value ranges. Second, we employ a classical
multilayer perceptron (MLP) to model normalized features, where the perception
machine varies depending on the skill but shares the parameters for the same percep-
tion machine. Consequently, for a user‚Äôs practice behavior pl‚àí1 related to skill s , we
ui k
can quantify its importance as
Œ±sk =MLP (cid:0) BN(cid:0) SF[pl‚àí1](cid:1) ,Œòsk (cid:1) (1)
pl u‚àí i1 att ui att
where MLP and BN are abbreviations for multilayer perceptron and batch normal-
ization, respectively. We will provide detailed information on the methodology in the
‚ÄùMethod‚Äù section. Œòsk refers to the set of MLP parameters.
att
22The user attributes, which make up one of the three static features, refer to time-
invariant variables and do not necessitate temporal feature fusion. Since scheduling
informationinherentlyencompassestemporalinformationduringencoding,thetempo-
ralencodingofuserexercisefeaturesbecomestheprimaryfocusofsequenceencoding.
Theencodedexercisefeatureofu ‚Äôsexercisesequencep1 ,p2 ,...,pl‚àí1 onskills can
i ui ui ui k
be represented by the following equation:
l‚àí1
(cid:88)
EEsk[{p1 ,p2 ,...,pl‚àí1}]= Œ±sk ¬∑E[pj ] (2)
ui ui ui
j=1
pj
ui
ui
The encoded behavioral sequence feature for skill s can be expressed as:
k
œâsk[p1 ui,p2 ui,...,pl u‚àí i1]=U[u i]‚äïEEsk[p1 ui,p2 ui,...,pl u‚àí i1]‚äïS[pl ui].
4.1.3 Mastery inference
Mastery inference focuses on constructing a mapping function (mastery inferer) that
maps encoded features of a behavioral sequence to the level of skill mastery, which
is the central component of the model. As the form of the function is ambiguous, we
propose utilizing a neural network model to fit the mapping function and leverage its
formidable non-linear fitting ability to precisely estimate the skill mastery variable.
Additionally, score prediction necessitates employing another mapping function that
translates the skill mastery state into the final score. To this end, we are going to
incorporate the classic cognitive diagnostic model (item response theory [38, 39]) as
the mapping function.
The mastery inferer for each skill is independent. Therefore, we set up a group of
neural network models with the same structure but different parameters to estimate
the level of mastery for each skill. When user u practices pl , the level of mastery for
skill s can be denoted as œïsk , and it can be
ei
xpressed
as:ui
k pl
ui
œïs pk
l ui
=œà usk
i
+MLP msty(BN(œâsk[p1 ui,p2 ui,...,pl u‚àí i1]),Œòs mk sty) (3)
where MLP and BN are short for multilayer perceptron and batch normalization,
respectively. We represent the level of mastery of each skill of user u by Œ¶ , where
i pl
Œ¶
pl ui
=[œïs p1
l
ui,œïs p2
l
ui,...,œïs pk
l
ui] and s 1,s 2,...,s
k
‚ààS. œà usk
i
denotes the initial pru oi ficiency
level of skill s possessed by user u before training.
k i
4.1.4 Score prediction
Generally, a user‚Äôs score on an exercise is believed to be correlated with their level
of skill mastery, which has been extensively studied by scholars who have devel-
oped numerous cognitive diagnostic models. In this paper, we propose using the item
response theory (IRT) model [38, 39] as the scoring prediction function. Specifically,
we introduce a three-parameter IRT model that models user mastery, exercise diffi-
culty,discrimination,andguessing,enablingustobetterestimateexercisescoresusing
23user mastery information. Its specific form can be expressed as:
1‚àíŒ≥
pl
gÀÜl =Œ≥ + ui (4)
ui pl
ui
1+exp(cid:16)
‚àíd¬∑Œ∑
¬∑(cid:16)
Œ¶ ‚àíŒ≤ ¬∑Q[pl
](cid:17)(cid:17)
pl
ui
pl
ui
pl
ui
ui
Here,Œ∑ ,Œ≤ andŒ≥ respectivelydenotethediscrimination,difficulty,andguessing
pl pl pl
ui ui ui
of the exercise pl . Q[pl ] represents the index of the exercise in the skill set, while
ui ui
gÀÜl represents the predicted score for practice pl .
ui ui
Theneuralnetworkmodelusedinthisarticleisimplementedwiththeopen-source
deep learning library Pytorch1 [57].
4.1.5 Model learning
In the preceding section, we denoted gÀÜl as the forecasted score for exercise pl ,
ui ui
obtained through the feature encoding, transformation and forecasting process from
the learning sequence {p1 ,p2 ,...,pl‚àí1}. Firstly, since scoring prediction is a typical
ui ui ui
regressiontask,weintroducemeanabsoluteerror(MAE)lossfunctionasouroptimiza-
tion objective. MAE is known to have better resistance against outliers as compared
to the more commonly used mean squared error (MSE) loss. Our main objective is
to extract knowledge that is universally applicable and generalizable from the data.
To accomplish this objective, we endeavor to decrease the effect of score values that
are unusual or stand out on the model. Secondly, we intend to adopt a self-regressive
learningparadigm.Therefore,theestablishedlossfunctionneedstobecomputedand
accumulated on all the practice session data of every learner in the dataset.
Consideringthesignificanceofusertrainingbehavior,showninEq.(1),weseekto
incorporate a constraint that ensures smoothness. The smoothness constraint serves
to restrict the importance values to a predefined range. If the model is capable of
constraining all the important values to a constant value without compromising pre-
cision, it can diminish the overall complexity of the model and alleviate the difficulty
of interpreting it.
The learning objective function of the entire model can be represented as follows.
L= 1 (cid:88) (cid:88) (cid:12) (cid:12)gÀÜl ‚àígl (cid:12) (cid:12)+Œª¬∑ 1 (cid:88) (cid:88) 1 (cid:88) (cid:16) Œ±sk ‚àí1(cid:17)2 (5)
|D| ui ui |D| |S| pl ui
ui‚ààUpl ui‚ààPui ui‚ààUpl ui‚ààPui sk‚ààS
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
MAEloss regularizationterm
The objective function above contains two main components. The first component is
thefidelity term,which describesthediscrepancy between the predicteddata andthe
actual data, represented by the MAE loss. The second component is the regulariza-
tion term, namely, the smoothness constraint on the importance of training behavior,
which is confined to a constant value near 1. Œª is a balancing factor used to balance
the importance of the fidelity term and the constraint term. |‚Ä¢| means the cardinal-
ity of a set. To be more specific, |D| represents the number of training records in the
1https://pytorch.org/
24dataset, where |D|=|U|√ó|P |. Since the entire model is differentiable, it is capable
ui
of conducting efficient learning of parameters through gradient-based iterative opti-
mization algorithms, for instance, stochastic gradient descent (SGD), and adaptive
moment estimation (Adam) [58].
4.2 Symbolic law extraction from deep learning regressor
4.2.1 Symbolic distillation
Formally,letT betheneuralnetworkmodeltobeanalyzed,andletS bethesymbolic
regressionmodel.Thecoreideaofsymbolicdistillationistoensurethatforanyinput
X, T(X) ‚âà S(X). Based on this, we can define the optimization objective for the
symbolic regression model as follows
œÄ =argminE |T(x,Œò)‚àíS(x,œÄ)| (6)
x‚àºX
œÄ
where œÄ represents the algebraic equation learned by the symbolic regression model,
x represents the sampled input feature, Œò represents all the parameters in the neural
network model, which could be considered constants after training.
Specifically, in this article, we construct X using the input information samples
from the training data. In order to reduce the computational cost of the symbolic
regression model, we sample the input samples from the entire dataset to obtain X,
where X ‚àà Rs√ók. Here, s represents the sampling size of the samples, which is a
hyperparameter set manually, while K represents the feature dimension of the neural
networkinputinformation,i.e.thedimensionofSF[pl‚àí1]orœâsk[p1 ,p2 ,...,pl‚àí1],as
ui ui ui ui
shown in Eqs. (1) and (3).
In our study, features have a high dimensionality. Symbolic regression is a typical
combinatorialoptimizationproblem.Whenthenumberofvariablesincreases,thesolu-
tionspacegrowsexponentially,diminishingtheefficiencyandstabilityofthesymbolic
regression model. As an exploratory study, our objective is to identify core patterns
and essential factors. Consequentially, we use a trained neural network model and an
interpretable analysis method, based on gradient feature importance estimation, to
screen features, select the most important features, and use them to execute the sym-
bolicregressiontoidentifykeypatterns.Specifically,thetrainedneuralnetworkmodel
and the gradient feature importance estimation method are incorporated to calculate
the importance of the K features, among which the k most crucial ones are chosen
to form the input feature C(X). Here, C(X) ‚àà Rs√ók, and k ‚â™ K. We will describe
the detailed gradient-based feature importance estimation method calculation in the
subsequent ‚ÄùMethod‚Äù section. In addition, the neural network modules in the deep
learningregressor(attention-basedencodersandmasteryinferersforeachskill,shown
in Fig. 1b) consist of two operators, namely BN and MLP. To avoid feature scaling
impact on importance estimation, we restrict our feature importance estimation to
the output features of the BN operators.
Thus, the objective of symbolic regression is formulated here as follows:
œÄ =argminE |T(x,Œò)‚àíS(C(x),œÄ)| (7)
x‚àºX
œÄ
25Inordertoavoidtheover-influenceoferrorsonthemodel,meanabsoluteerrorisintro-
ducedastheobjectivefunctionforthesymbolicregressionmodel.Geneticalgorithms
are the primary approach employed to solve the symbolic regression model, symbolic
regressionlibrariessuchasEureqa,PySR2,andgeppy3 offeradditionalmeansofsolv-
ing the model. The symbol regression algorithm used in this article is implemented
with the open-source symbol regression library PySR.
4.2.2 Governing laws extraction
Inthissection,wearegoingtodepicthowweconvertthepredictionprocessoftrained
deepregressorintoasymboliclawusingthesymbolicdistillationmethod.Weexpress
the neural network models in each module of the deep regressor as a symbolic law
andultimatelycombinethemtoobtainthesymbolicrepresentationoftheentiredeep
regressor. As the attention-based encoder and mastery inferer for each skill are inde-
pendent,itisfeasibletoperformsymbolicdistillationonthesecomponentsinparallel.
Furthermore, since the attention-based encoder and mastery inferer are coupled and
exhibit sequential order, the expansion process of the analysis follows a similar order.
First,fortheattention-basedencoder,asymbolicrepresentationofŒ±sk isobtainedby
pl
ui
symbolicallyanalyzingequation(1),mergedwithfeatureencoding(equation(2)),pro-
ducing a symbolic representation of the encoded features, i.e., œâsk[p1 ,p2 ,...,pl‚àí1].
Ifthemodelincludesasmoothnessconstraintforattentionvalue,Œ±skui canui beappu ri
ox-
pl
ui
imated to 1 when the smoothness component‚Äôs value is less than an extremely small
threshold(e.g.,1e-4)toreducethecomplexityoftheanalysisequation.Next,themas-
tery inferer undergoes a symbolic distillation, followed by the integration of symbolic
representation with the encoded features, reorganized to render the final symbolic
governing laws.
4.3 Data generation method and model parameter setting in
the simulated data experiment
The simulated training log data generation can be divided into three main stages:
learners and exercises generation, learning path generation, and cognitive state and
learning feedback generation, shown in Extended Data Figure 1. In the learner sim-
ulation, a total of 50 independent virtual learners are simulated. In the practice
simulation, 20 simulated test questions are generated for each of skill 1 and skill 2,
eachtestquestioncontainingthreecoefficients:difficulty,guessing,anddiscrimination,
all of which are random variables uniformly distributed between 0 and 1. Learning
path generation involves generating the practice sequence for each simulated learner.
To simplify the simulation environment setup process, the corresponding learning
sequence is generated by randomly selecting questions, with each simulated learner‚Äôs
learning sequence length being 20. Combining the learner‚Äôs learning path with the
predetermined skill acquisition function (in Table 1), the learner‚Äôs skill mastery can
be calculated at the beginning of each practice. Finally, combining the parameters of
thetestquestions,thelearner‚Äôstestscoreiscalculatedusingthethree-parameterIRT
2https://github.com/MilesCranmer/PySR
3https://github.com/ShuhuaGao/geppy
26model (Eq. 4) and Gaussian random noise with a mean of 0 and a variance of Œ¥ is
added.
Underthethreesettings,theparametersoftheLinearlawareŒ±=1,Œ≤=0.3,Œ≥=0.1,
the parameters of the Exponential law are Œ±=0.1, Œ≤=1, Œ≥=0.1, and the parameters of
the Power law are Œ±=0.8, Œ≤=1, Œ≥=0.2. In the three settings considered, the mutual-
ism factor ¬µ is set to 0.5. In the simulation experiment, four different levels of noise
are selected as test standards, namely Œ¥ ‚àà {0,0.001,0.01,0.1}. In the deep learning
regressor, the MLP models in feature encoding and mastery inference are both three-
layered (two hidden layers and one output layer), with 1000 neurons in each hidden
layerandLeakyReLUastheactivationfunction.ThemodeloptimizerissettoAdam,
and the learning rate is 1e-3. In the symbolic regression model, the number of itera-
tions is set to 2000, the population size is 100, and the maximum function length is
15. The preset operators include ‚Äù+‚Äù, ‚Äù-‚Äù, ‚Äù√ó‚Äù, ‚Äùpow‚Äù, ‚Äùexp‚Äù, ‚Äùlog‚Äù, and ‚Äùsin‚Äù.
4.4 Data preprocessing method and model parameter setting
in the real-world data experiment
Inthispaper,weutilizedrawdataprovidedby[42].Toensurethequalityofourdata,
wefilteredoutgameswithfewercontacttimesandrelatedskills.Wethenselectedthe
first 2000 practicerecords of users withmorethan 2000 practice times asour training
data. To avoid any issues with abnormal scores, we normalized the score data of each
game for the selected training data. Specifically, we took the top 10% of scores as the
fullscoreandperformedmaximum-minimumnormalizationonthedataofeachgame.
Allscoreswerethenuniformlyscaledtoarangeof[0.01,0.8]topreventtheInfoutlier
problem caused by the cognitive diagnostic model.
The deep learning regressor employs three-layered MLP models for feature encod-
ing and mastery inference, each consisting of two hidden layers and one output layer.
The activation function used is Leaky ReLU, and each hidden layer comprises 1000
neurons. The model optimizer is set to Adam, with a learning rate of 1e-4. The sym-
bolicregressionmodelisconfiguredwithapopulationsizeof100,amaximumfunction
lengthof15,andafixedsetofoperators,including‚Äù+‚Äù,‚Äù-‚Äù,‚Äù*‚Äù,‚Äùpow‚Äù,‚Äùexp‚Äù,‚Äùlog‚Äù,
and‚Äùsin‚Äù.Themodelistrainedfor2000iterationstoachieveoptimalperformance.To
increasetheinterpretabilityofgoverninglaws,operatorsotherthan‚Äù+‚Äù,‚Äù-‚Äù,and‚Äù*‚Äù
are not allowed to be nested. In the symbolic regression model, a total of 2000 train-
inglogsamplesareselectedrandomly,andthemostimportant8featuresarescreened
through the feature importance analysis method. To increase the interpretability of
governing laws, operators other than ‚Äù+‚Äù, ‚Äù-‚Äù, and ‚Äù√ó‚Äù are not allowed to be nested.
Data availability. All data generated or analyzed during this study are available.
It can be found in https://github.com/ccnu-mathits/ADM.
Code availability. The source code of this study is freely available on Github at
https://github.com/ccnu-mathits/ADM.
27References
[1] VanLehn,K. Cognitiveskillacquisition. Annualreviewofpsychology 47,513‚Äì539
(1996).
[2] DeKeyser, R. in Skill acquisition theory 83‚Äì104 (Routledge, 2020).
[3] Tabibian,B.et al. Enhancinghumanlearningviaspacedrepetitionoptimization.
Proceedings of the National Academy of Sciences 116, 3988‚Äì3993 (2019).
[4] Evans, N. J., Brown, S. D., Mewhort, D. J. & Heathcote, A. Refining the law of
practice. Psychological review 125, 592 (2018).
[5] Heathcote,A.,Brown,S.&Mewhort,D.J. Thepowerlawrepealed:Thecasefor
anexponentiallawofpractice. Psychonomicbulletin&review 7,185‚Äì207(2000).
[6] Shrager, J., Hogg, T. & Huberman, B. A. A graph-dynamic model of the power
law of practice and the problem-solving fan-effect. Science 242, 414‚Äì416 (1988).
[7] Wixted, J. T. The enigma of forgetting. Proceedings of the National Academy of
Sciences 119, e2201332119 (2022).
[8] Averell, L. & Heathcote, A. The form of the forgetting curve and the fate of
memories. Journal of mathematical psychology 55, 25‚Äì35 (2011).
[9] Roediger, H. L., III. Relativity of remembering: Why the laws of memory
vanished. Annu. Rev. Psychol. 59, 225‚Äì254 (2008).
[10] Chiaburu, D. S. & Marinova, S. V. What predicts skill transfer? an exploratory
study of goal orientation, training self-efficacy and organizational supports.
International journal of training and development 9, 110‚Äì123 (2005).
[11] Sturm, L. P. et al. A systematic review of skills transfer after surgical simulation
training. Annals of surgery 248, 166‚Äì179 (2008).
[12] Logan, G. D. Toward an instance theory of automatization. Psychological review
95, 492 (1988).
[13] Logan,G.D. Shapesofreaction-timedistributionsandshapesoflearningcurves:
atestoftheinstancetheoryofautomaticity. JournalofExperimentalPsychology:
Learning, Memory, and Cognition 18, 883 (1992).
[14] Anderson,J.R.Acquisitionofcognitiveskill.Psychologicalreview 89,369(1982).
[15] Tenison, C. & Anderson, J. R. Modeling the distinct phases of skill acquisition.
Journal of Experimental Psychology: Learning, Memory, and Cognition 42, 749
(2016).
28[16] Tenison, C., Fincham, J. M. & Anderson, J. R. Phases of learning: How skill
acquisition impacts cognitive processing. Cognitive psychology 87, 1‚Äì28 (2016).
[17] Jordan, M. I. Serial order: A parallel distributed processing approach 121, 471‚Äì
495 (1997).
[18] McClelland, J. L., Rumelhart, D. E., Group, P. R. et al. Parallel distributed
processing Vol. 2 (MIT press Cambridge, MA, 1986).
[19] Young,R.M.&Lewis,R.L. Thesoarcognitivearchitectureandhumanworking
memory. Models of working memory: Mechanisms of active maintenance and
executive control 224‚Äì256 (1999).
[20] Anderson, J. R., Matessa, M. & Lebiere, C. Act-r: A theory of higher level
cognition and its relation to visual attention. Human‚ÄìComputer Interaction 12,
439‚Äì462 (1997).
[21] Ritter, F. E., Tehranchi, F. & Oury, J. D. Act-r: A cognitive architecture for
modelingcognition. WileyInterdisciplinaryReviews:CognitiveScience 10,e1488
(2019).
[22] Jenkins, J. J., Cermak, L. & Craik, F. Four points to remember: A tetrahedral
model of memory experiments. Levels of processing in human memory 429‚Äì46
(1979).
[23] Goldstone, R. L. & Lupyan, G. Discovering psychological principles by mining
naturally occurring data sets. Topics in cognitive science 8, 548‚Äì568 (2016).
[24] Udrescu, S.-M. & Tegmark, M. Ai feynman: A physics-inspired method for
symbolic regression. Science Advances 6, eaay2631 (2020).
[25] Wang, H. et al. Scientific discovery in the age of artificial intelligence. Nature
620, 47‚Äì60 (2023).
[26] Schmidt, M. & Lipson, H. Distilling free-form natural laws from experimental
data. science 324, 81‚Äì85 (2009).
[27] Cranmer,M.etal. Discoveringsymbolicmodelsfromdeeplearningwithinductive
biases, Vol. 33, 17429‚Äì17442 (2020).
[28] Rudy, S. H., Brunton, S. L., Proctor, J. L. & Kutz, J. N. Data-driven discovery
of partial differential equations. Science advances 3, e1602614 (2017).
[29] Chen, Z., Liu, Y. & Sun, H. Physics-informed learning of governing equations
from scarce data. Nature communications 12, 6136 (2021).
[30] Margraf, J. T., Jung, H., Scheurer, C. & Reuter, K. Exploring catalytic reaction
networks with machine learning. Nature Catalysis 6, 112‚Äì121 (2023).
29[31] Han, Z.-K. et al. Single-atom alloy catalysts designed by first-principles
calculations and artificial intelligence. Nature communications 12, 1833 (2021).
[32] Wang, Y., Wagner, N. & Rondinelli, J. M. Symbolic regression in materials
science. MRS Communications 9, 793‚Äì805 (2019).
[33] He, M. & Zhang, L. Machine learning and symbolic regression investigation
on stability of mxene materials. Computational Materials Science 196, 110578
(2021).
[34] LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. nature 521, 436‚Äì444 (2015).
[35] Vaswani, A. et al. Attention is all you need, Vol. 30 (2017).
[36] Baker, F. B. The basics of item response theory (ERIC, 2001).
[37] Hambleton, R. K., Swaminathan, H. & Rogers, H. J. Fundamentals of item
response theory Vol. 2 (Sage, 1991).
[38] Swaminathan, H. & Gifford, J. A. Bayesian estimation in the three-parameter
logistic model. Psychometrika 51, 589‚Äì601 (1986).
[39] Maris, G. & Bechger, T. On interpreting the model parameters for the three
parameter logistic model. Measurement 7, 75‚Äì88 (2009).
[40] VanDerMaas,H.L.etal. Adynamicalmodelofgeneralintelligence:thepositive
manifold of intelligence by mutualism. Psychological review 113, 842 (2006).
[41] Steyvers, M. & Benjamin, A. S. The joint contribution of participation and
performancetolearningfunctions:Exploringtheeffectsofageinlarge-scaledata
sets. Behavior research methods 51, 1531‚Äì1543 (2019).
[42] Steyvers, M. & Schafer, R. J. Inferring latent learning factors in large-scale
cognitive training data. Nature Human Behaviour 4, 1145‚Äì1155 (2020).
[43] Hornik & Kurt. Approximation capabilities of multilayer feedforward networks.
Neural Networks 4, 251‚Äì257 (1991).
[44] Raghu, M., Poole, B., Kleinberg, J., Ganguli, S. & Sohl-Dickstein, J. On the
expressive power of deep neural networks, 2847‚Äì2854 (2017).
[45] Cybenko, G. Approximation by superpositions of a sigmoidal function. Mathe-
matics of control, signals and systems 2, 303‚Äì314 (1989).
[46] SPEARMAN, C. ‚Äù general intelligence,‚Äù objectively determined and measured.
American J. Psychology 15, 201‚Äì293 (1904).
30[47] Eid,M.,Geiser,C.,Koch,T.&Heene,M. Anomalousresultsing-factormodels:
Explanations and alternatives. Psychological methods 22, 541 (2017).
[48] Simons, D. J. et al. Do ‚Äúbrain-training‚Äù programs work? Psychological science
in the public interest 17, 103‚Äì186 (2016).
[49] Kievit, R. A. et al. Mutualistic coupling between vocabulary and reasoning
supports cognitive development during late adolescence and early adulthood.
Psychological Science 28, 1419‚Äì1431 (2017).
[50] Kievit, R. A., Hofman, A. D. & Nation, K. Mutualistic coupling between vocab-
ulary and reasoning in young children: A replication and extension of the study
by kievit et al.(2017). Psychological science 30, 1245‚Äì1252 (2019).
[51] Fisher, R. A. Design of experiments. British Medical Journal 1, 554 (1936).
[52] Kumar, A., Benjamin, A. S., Heathcote, A. & Steyvers, M. Comparing models
of learning and relearning in large-scale cognitive training data sets. npj Science
of Learning 7, 24 (2022).
[53] Liu, R. & Koedinger, K. R. Towards reliable and valid measurement of indi-
vidualized student parameters. International Educational Data Mining Society
(2017).
[54] Koedinger, K. R., Carvalho, P. F., Liu, R. & McLaughlin, E. A. An astonish-
ing regularity in student learning rate. Proceedings of the National Academy of
Sciences 120, e2221311120 (2023).
[55] Neath,A.A.&Cavanaugh,J.E.Thebayesianinformationcriterion:background,
derivation, and applications. Wiley Interdisciplinary Reviews: Computational
Statistics 4, 199‚Äì203 (2012).
[56] Vrieze, S. I. Model selection and psychological theory: a discussion of the differ-
encesbetweentheakaikeinformationcriterion(aic)andthebayesianinformation
criterion (bic). Psychological methods 17, 228 (2012).
[57] Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning
library, Vol. 32 (2019).
[58] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization (2015).
Acknowledgments. We express our deepest gratitude to the anonymous reviewers
for their invaluable feedback. The profound insights provided in their comments have
significantly contributed to enhancing the overall quality of this paper.
31a
1. Learners and Exercises 2. Learning Path 3. Cognitive State 4. Learning Path
Generation Generation Generation Generation
Lea srner E ox fe Sr 1c K Di Is iL ffe =Ls
‚Ä¶
E ox fe Sr 2c Ki Is L De L ifs
f= ‚Ä¶
Random Selec Pt oed o lf ro ‚Ä¶m Exercise P Lre aw-s set
s_1 e_6 e_3
‚Ä¶
‚Ä¶ e_36
s ‚Ä¶_1 ‚Ä¶e_1 D G ‚Ä¶i us ec s= s‚Ä¶ = ‚Ä¶e_21 D Gi us ec s= s‚Ä¶ =‚Ä¶ ‚Ä¶s_1 e_6 e_3 ‚Ä¶ e_36 s_1 1234567891011121314 Scores 30 42 ‚Ä¶ ‚Ä¶ 80
s_50 e_20 D D G ‚Ä¶i i uf sf ec= s=‚Ä¶ s‚Ä¶ = e_40 D D Gi i uf sf ec= s=‚Ä¶ s‚Ä¶ =‚Ä¶ s_50 e_26 e_13 ‚Ä¶ ‚Ä¶ e_7 s‚Ä¶ _50 1234567891011121314 ‚Ä¶ ‚Ä¶ IRT model
b
Discovered Predetermined
SimulatedExerciseLogs ProposedMethod
GoverningLaws GoverningLaws
‚Ä¶ SKILL 1: SKILL 1:
s_1 3e_ 06 4e_ 23 ‚Ä¶ e 8_3 06
‚Ä¶
SKILL 2: SKILL 2:
‚Ä¶
s_50 e 2_ 026 e 2_1 43 ‚Ä¶ e 9_ 07
c
Simulated Learning Path Simulated Cognitive States Simulated Scores (Œ¥= 0) Simulated Scores (Œ¥= 0.1)
1 1
0.9 0.9
Power+Linear 0.8 0.8
SKILL 2 00 .. 67 00 .. 67
0.5 0.5
0.4 0.4
SKILL 1 00 .. 23 00 .. 23
01234567891011121314151617181920 0.1 0.1
1234567891011121314151617181920 SKILL 1 SKILL 2 01234567891011121314151617181920 01234567891011121314151617181920
Simulated Learning Path Simulated Cognitive States Simulated Scores (Œ¥= 0) Simulated Scores (Œ¥= 0.1)
SKILL 2
Linear+Exponential
0000 .... 67891 0000 .... 67891
0.5 0.5
0.4 0.4
SKILL 1 00 .. 23 00 .. 23
1234567891011121314151617181920 0123456SK7ILL8 191011121S3KI1L4L 1251617181920 0.1 01234567891011121314151617181920 0.1 01234567891011121314151617181920
Extended Data Fig. 1 Schematic representation of the simulation experiment. (a) Flowchart
depicting the process of generating simulated data and the data format. (b) Schematic diagram
illustratingtheevaluationprocedureofthesimulationexperiment.(c)Samplerepresentationofthe
simulateddata.
32Skill1: Linear + Skill2: Exponential Skill1: Exponential + Skill2: Power Skill1: Power + Skill2: Linear
Skill1 Skill2 Skill1 Skill2 Skill1 Skill2
Sum_G 0.7945 Sum_G 0.0688 Sum_G 0.4513 Sum_G 0.3842 Sum_G 0.8775 Sum_G 0.2256
Sum_S1 0.0979 Sum_S1 0.4761 Sum_S1 0.3358 Sum_S1 0.3739 Sum_S1 0.0597 Sum_S1 0.4461
Sum_S2 0.1068 Sum_S2 0.4095 Sum_S2 0.1647 Sum_S2 0.2382 Sum_S2 0.0400 Sum_S2 0.3248
C0o.0u0n0t1 C0o.0u0n0t8 Count 0.0092 C0o.0u0n2t0 Count 0.0086 C0o.0u0n0t9
Int0e.r0v0a0l8 Interval 0.0448 Interval 0.0390 Int0e.r0v0a1l8 Interval 0.0141 Int0e.r0v0a2l8
ùú∫=ùüé ùú∫=ùüé ùú∫=ùüé
Sum_G 0.8291 Sum_G 0.4089 Sum_G 0.7431 Sum_G 0.3231 Sum_G 0.8197 Sum_G 0.2432
Sum_S1 0.0933 Sum_S1 0.1676 Sum_S1 0.0593 Sum_S1 0.2366 Sum_S1 0.0620 Sum_S1 0.2940
Sum_S2 0.0682 Sum_S2 0.3696 Sum_S2 0.1417 Sum_S2 0.4325 Sum_S2 0.0914 Sum_S2 0.4603
C0o.0u0n1t2 Count 0.0065 Count 0.0053 C0o.0u0n2t8 C0o.0u0n2t5 C0o.0u0n0t2
Interval 0.0082 Interval 0.0474 Interval 0.0507 Interval 0.0050 Interval 0.0244 Int0e.r0v0a2l3
ùú∫=ùüé.ùüéùüéùüè ùú∫=ùüé.ùüéùüéùüè ùú∫=ùüé.ùüéùüéùüè
Sum_G 0.6347 Sum_G 0.3653 Sum_G 0.5737 Sum_G 0.3107 Sum_G 0.6167 Sum_G 0.1754
Sum_S1 0.1551 Sum_S1 0.2018 Sum_S1 0.1137 Sum_S1 0.2250 Sum_S1 0.1615 Sum_S1 0.2067
Sum_S2 0.1335 Sum_S2 0.3160 Sum_S2 0.2290 Sum_S2 0.4353 Sum_S2 0.1642 Sum_S2 0.5866
Count 0.0150 Count 0.0077 Count 0.0085 C0o.0u0n3t6 Count 0.0122 C0o.0u0n3t7
Interval 0.0617 Interval 0.1092 Interval 0.0750 Interval 0.0253 Interval 0.0452 Interval 0.0276
ùú∫=ùüé.ùüéùüè ùú∫=ùüé.ùüéùüè ùú∫=ùüé.ùüéùüè
Sum_G 0.3257 Sum_G 0.3596 Sum_G 0.2925 Sum_G 0.2158 Sum_G 0.3846 Sum_G 0.3494
Sum_S1 0.2793 Sum_S1 0.2612 Sum_S1 0.4350 Sum_S1 0.4564 Sum_S1 0.2282 Sum_S1 0.2883
Sum_S2 0.3017 Sum_S2 0.2683 Sum_S2 0.1860 Sum_S2 0.2536 Sum_S2 0.2844 Sum_S2 0.3124
Count 0.0228 Count 0.0305 Count 0.0120 Count 0.0192 Count 0.0199 Count 0.0136
Interval 0.0704 Interval 0.0805 Interval 0.0746 Interval 0.0550 Interval 0.0829 Interval 0.0362
ùú∫=ùüé.ùüè ùú∫=ùüé.ùüè ùú∫=ùüé.ùüè
Sum_G 0.4017 Sum_G 0.3744 Sum_G 0.2843 Sum_G 0.3338 Sum_G 0.3929 Sum_G 0.2743
Sum_S1 0.1710 Sum_S1 0.0912 Sum_S1 0.4474 Sum_S1 0.4135 Sum_S1 0.1415 Sum_S1 0.3551
Sum_S2 0.2732 Sum_S2 0.4207 Sum_S2 0.2266 Sum_S2 0.2139 Sum_S2 0.4263 Sum_S2 0.2581
Count 0.0202 Count 0.0109 Count 0.0093 Count 0.0091 Count 0.0169 Count 0.0238
Interval 0.1339 Interval 0.1028 Interval 0.0324 Interval 0.0297 Interval 0.0224 Interval 0.0888
ùú∫=ùüè ùú∫=ùüè ùú∫=ùüè
Extended Data Fig. 2 The model‚Äôs estimation of the importance of five input features
(Sum G, Sum S1, Sum S2, Count and Interval) is presented in three simulation experiment set-
tings (Linear+ Exponential, Exponential + Power, Power + Linear) and five levels of noise (Œ¥ ‚àà
{0,0.001,0.01,0.1,1}).
a. with symmetrical error
Skill1: Linear Skill2: Exponential Skill1: Exponential Skill2: Power Skill1: Power Skill2: Linear
Œ¥ Feature Structure Value Feature Structure Value Œ¥ Feature Structure Value Feature Structure Value Œ¥ Feature Structure Value Feature Structure Value
0 YES YES YES YES YES YES 0 YES YES YES YES YES YES 0 YES YES YES YES YES YES
0.001 YES YES YES YES YES YES 0.001 YES YES YES YES YES YES 0.001 YES YES YES YES YES YES
0.01 YES YES YES YES YES YES 0.01 YES YES YES YES YES YES 0.01 YES YES YES YES YES YES
0.1 YES YES YES YES YES YES 0.1 YES YES YES YES YES YES 0.1 YES NO NO YES YES YES
1 YES YES NO YES NO NO 1 YES NO NO YES NO NO 1 YES NO NO YES NO NO
b. with salt&pepper error
Skill1: Linear Skill2: Exponential Skill1: Exponential Skill2: Power Skill1: Power Skill2: Linear
Œ¥ Feature Structure Value Feature Structure Value Œ¥ Feature Structure Value Feature Structure Value Œ¥ Feature Structure Value Feature Structure Value
0 YES YES YES YES YES YES 0 YES YES YES YES YES YES 0 YES YES YES YES YES YES
0.001 YES YES YES YES YES YES 0.001 YES YES YES YES YES YES 0.001 YES YES YES YES YES YES
0.01 YES YES YES YES YES YES 0.01 YES YES YES YES YES YES 0.01 YES YES YES YES YES YES
0.1 YES YES YES YES YES YES 0.1 YES YES YES YES NO NO 0.1 YES NO NO YES YES YES
1 YES YES NO YES NO NO 1 YES NO NO YES NO NO 1 YES NO NO YES NO NO
Extended Data Fig. 3 The restoration results of the governing law under the error settings of
symmetricalandsalt&pepper,wheresymmetricalrepresentserrorsfollowingauniformdistribution,
Œµ‚àºUniform(0,Œ¥),whiletheerrorsunderthebsettingcanbedescribedasŒµ=Choose{‚àí1,0,1}√óŒ¥
33User Feature Exercise Feature
Name Abbr. Data type Description Skill Abbr. Subskill Abbr.
Paid P Disc. Has the user subscribed to a paid account? General GE - -
Was male chosen as the user's gender
Male M Disc. during registration? Attentional Deployment AD_AT
Was female chosen as the user's gender Divided Attention DA_AT
Female F Disc. during registration? Field of View FV_AT
Age A Cont. T reh ge i sa tg rae t it oh na .t the user specified during Attention AT Inf So er lm eca tt ivio en A P ttr eo nce tis os ning SIP A_ _A AT T
The education level that the user specified
Education Level E Cont. during registration. Spatial Orientation SO_AT
Timing TI_AT
Visualization VI_AT
General Flexibility GF_FL
Flexibility FL Response Inhibition RI_FL
Scheduling Feature Task Switching TS_FL
Name Abbr. Data type Description Reading Comprehension RC_LA
The number of times the user has practiced Language LA Verbal Fluency VF_LA
Game Count GaC Cont. this particular game. Vocabulary Proficiency VP_LA
The number of times the user has General Math GM_MA
Area Count ArC Cont. practiced games from the same area as the Math MA
current game. Numerical Calculation NC_MA
The number of times the user has Divided Attention DA_ME
Attribute Count AtC Cont. practiced games from the same attribute Face-Name Recall FR_ME
as the current game.
Field of View FV_ME
The time duration elapsed since the last Memory ME
Game Interval GaI Cont. time the user practiced this game. Information Processing IP_ME
The time duration elapsed since the last Spatial Recall SR_ME
Area Interval ArI Cont. time the user practiced a game from the Working Memory WM_ME
same area as the current game. Logical Reasoning LR_RE
The time duration elapsed since the last
Attribute Interval AtI Cont. time the user practiced a game from the Reasoning RE Planning PL_RE
same attribute as the current game. Spatial Reasoning SR_RE
Extended Data Fig. 4 Thefeaturesetutilizedinthereal-worlddataset(Lumosity)experiment,
whichcomprisesthreecomponents:user(U),exercise(E),andscheduling(S).Thefeaturesconsist
oftwocategories:discreteandcontinuous.Discretefeaturesareencodedthroughone-hotencoding,
whilecontinuousfeaturesareencodedusingrealvalues.Gamefeaturesaretwo-dimensional,consisting
ofskillandsubskill,bothofwhicharediscretefeaturesandcharacterizetherelationshipbetweenthe
gameandcognitiveskills.Subskillisasubdivisionfeatureofskill.
34User typeÔºö Free Paid Paid Paid Free
GenderÔºö Female Female Male Male Female
AgeÔºö 55 73 80 63 55
Education levelÔºö PHD Bachelors Degree High School Masters Degree High School
Attention
ùüé.ùüìùüíùüí‚àí#ùë®ùëª‚àíùüé.ùüéùüñùüéùüí
#Training=609 #Training=629 #Training=675 #Training=599 #Training=828
Flexibility
ùüé.ùüèùüëùüë‚àíùüé.ùüêùüèùüó
√óùêûùê±ùê©‚àíùüé.ùüéùüèùüèùüó√ó#ùë≠ùë≥
#Training=316 #Training=365 #Training=294 #Training=351 #Training=404
Language
ùüé.ùüéùüìùüéùüî√ó#ùë≥ùë®ùüé.ùüêùüóùüì
#Training=119 #Training=106 #Training=135 #Training=136 #Training=52
Math
ùüé.ùüóùüìùüì‚àí#ùë∫ùë®_ùë®ùëª‚àíùüé.ùüéùüêùüëùüî
#Training=185 #Training=175 #Training=174 #Training=135 #Training=82
Memory
ùüé.ùüêùüîùüñ
‚àíùüé.ùüîùüëùüïùüé.ùüéùüèùüìùüí√ó#ùë¥ùë¨+ùüê.ùüîùüé
#Training=671 #Training=651 #Training=617 #Training=701 #Training=533
Reasoning
ùüé.ùüèùüêùüî√óùê•ùê®ùê†#ùë∫ùë®_ùë®ùëª
#Training=100 #Training=74 #Training=105 #Training=78 #Training=101
Extended Data Fig. 5 Analysisofthedifferencebetweenthemasterylevelofskillscalculatedby
thedeeplearningregressorandthediscoveredgoverninglaws.Here,wepresentthechangesinskill
proficiencyateachtimepointduring2000practicesessionsforfivelearners.Thebluelinerepresents
themasterycurvecomputedbythedeepregressor,whiletheredlinerepresentsthecurvecomputed
bythediscoveredgoverninglaw.Thegoverninglawisthesameforalllearners,butduetodifferences
intheirchoicesandorderofpractice,therearevariationsintheindependentvariableofthegoverning
lawforeachlearner.
35