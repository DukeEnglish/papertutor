On the Convergence of Continual Learning with Adaptive Methods
SeungyubHan1 YeongmoKim1 TaehyunCho1 JungwooLee1
1SeoulNationalUniversity,Seoul,RepublicofKorea
Abstract
Inthislineofresearch,nonconvexstochasticoptimization
problemshavebeenwellstudiedonasingletasktotrain
deepneuralnetworksandprovetheoreticalguaranteesof
One of the objectives of continual learning is to goodconvergence.
preventcatastrophicforgettinginlearningmulti-
Previous continual learning algorithms have introduced
ple tasks sequentially, and the existing solutions
novelmethodssuchasareplaymemorytostoreandreplay
havebeendrivenbytheconceptualizationofthe
thepreviouslylearnedexamples[Lopez-PazandRanzato,
plasticity-stabilitydilemma.However,theconver-
2017,Aljundietal.,2019b,Chaudhryetal.,2019a],regular-
genceofcontinuallearningforeachsequentialtask
izationmethodsthatpenalizeneuralnetworks[Kirkpatrick
islessstudiedsofar.Inthispaper,weprovidea
etal.,2017,Zenkeetal.,2017],Bayesianmethodsthatuti-
convergenceanalysisofmemory-basedcontinual
lizetheuncertaintyofparametersordatapoints[Nguyen
learningwithstochasticgradientdescentandem-
et al., 2018, Ebrahimi et al., 2020], and other recent ap-
piricalevidencethattrainingcurrenttaskscauses
proaches [Yoon et al., 2018, Lee et al., 2019]. The study
thecumulativedegradationofprevioustasks.We
ofcontinuallearninginBayesianframeworksformulatea
propose an adaptive method for nonconvex con-
trainedmodelforprevioustasksparameterintoanapprox-
tinuallearning(NCCL),whichadjustsstepsizes
imateposteriortolearnaprobabilisticmodelwhichhave
ofbothpreviousandcurrenttaskswiththegradi-
empirically good performance on entire tasks. However,
ents.Theproposedmethodcanachievethesame
Bayesianapproachescanfailinpracticeanditcanbehard
convergence rate as the SGD method when the
toanalyzetherigorousconvergenceduetotheapproxima-
catastrophicforgettingtermwhichwedefineinthe
tion.Thememory-basedmethodsaremorestraightforward
paperissuppressedateachiteration.Further,we
approaches,wherethelearnerstoresasmallsubsetofthe
demonstratethattheproposedalgorithmimproves
dataforprevioustasksintoamemoryandutilizesthemem-
theperformanceofcontinuallearningoverexisting
orybyreplayingsamplestokeepamodelstayinginafeasi-
methodsforseveralimageclassificationtasks.
bleregionwithoutlosingtheperformanceontheprevious
tasks.Gradientepisodicmemory(GEM)[Lopez-Pazand
Ranzato,2017]firstformulatedthereplaybasedcontinual
1 INTRODUCTION
learning as a constrained optimization problem. This for-
mulationallowsustorephrasetheconstraintsonobjectives
Learningnewtaskswithoutforgettingpreviouslylearned
forprevioustasksasinequalitiesbasedontheinnerproduct
tasksisakeyaspectofartificialintelligencetobeasversa-
of loss gradient vectors for previous tasks and a current
tileashumans.Unliketheconventionaldeeplearningthat
task.However,thegradientupdatebyGEMvariantscannot
observestasksfromani.i.d.distribution,continuallearning
guaranteeboththeoreticalandempiricalconvergenceofits
train sequentially a model on a non-stationary stream of
constrainedoptimizationproblem.Themodifiedgradient
data[Ring,1995,Thrun,1994].ThecontinuallearningAI
updates do not always satisfy the loss constraint theoreti-
systemsstrugglewithcatastrophicforgettingwhenthedata
cally,andwecanalsoobservetheforgettingphenomenon
accessofpreviouslylearnedtasksisrestricted[Frenchand
occursempirically.Italsoimpliesthatthisintuitiverefor-
Chater,2002].Althoughnovelcontinuallearningmethods
mulationviolatestheconstrainedoptimizationproblemand
successfully learn the non-stationary stream sequentially,
cannotprovidetheoreticalguaranteetopreventcatastrophic
studiesonthetheoreticalconvergenceanalysisofbothpre-
forgettingwithoutarigorousconvergenceanalysis.
vioustasksandacurrenttaskhavenotyetbeenaddressed.
Acceptedforthe39thConferenceonUncertaintyinArtificialIntelligence (UAI2023).
4202
rpA
8
]GL.sc[
1v55550.4042:viXraInthiswork,weexplainthecauseofcatastrophicforgetting methodanditsextensionwhichadjuststepsizesbe-
bydescribingcontinuallearningwithasmoothnonconvex tweentasksateachstepwiththeoreticalground,and
finite-sumoptimizationproblem.Inthestandardsingletask demonstratethatbothmethodsshowremarkableper-
case,SGD[GhadimiandLan,2013],ADAM[Reddietal., formanceonimageclassificationtasks.
2018], YOGI [Zaheer et al., 2018], SVRG [Reddi et al.,
2016a],andSCSG[Leietal.,2017]arethealgorithmsfor
2 RELATEDWORK
solving nonconvex problems that arise in deep learning.
Toanalyzetheconvergenceofthosealgorithms,previous
Memory-based methods. Early memory-based methods
worksstudythefollowingnonconvexfinite-sumproblem
utilizememorybythedistillation[Rebuffietal.,2017,Li
1 (cid:88)n and Hoiem, 2017] or the optimization constraint [Lopez-
min f(x)= f (x), (1)
xâˆˆRd n i PazandRanzato,2017,Chaudhryetal.,2019a].Especially,
i=1 A-GEM[Chaudhryetal.,2019a]simplifiestheapproach
whereweassumethateachobjectivef (x)withamodelx forconstraintviolatedupdatestepsastheprojectedgradi-
i
andadatapointindexiâˆˆ[n]foradatasetwithsizen(by entonareferencegradientwhichensuresthattheaverage
theconventionfornotationsinnonconvexoptimizationliter- memorylossoverprevioustasksdoesnotincrease.Recent
ature[Reddietal.,2016a])isnonconvexwithL-smoothness works[Chaudhryetal.,2019b,2020a,Riemeretal.,2018]
assumption.Ingeneral,wedenotef (x)asf(x;d )where haveshownthatupdatingthegradientsonmemorydirectly,
i i
d iisadatapointtuple(INPUT,OUTPUT)withindexi.We whichiscalledexperiencereplay,isalightandprominent
expect that a stochastic gradient descent based algorithm approach.Wefocusonconvergenceofcontinuallearning,
reachesastationarypointinsteadoftheglobalminimumin but the above methods focus on increasing the empirical
nonconvexoptimization.Unliketheconvexcase,thecon- performance without theoretical guarantee. Our analysis
vergence is generally measured by the expectation of the providesalegitimatetheoreticalconvergenceanalysisun-
squared norm of a gradient Eâˆ¥âˆ‡f(x)âˆ¥2. The theoretical derthestandardsmoothnonconvexfinite-sumoptimization
computational complexity is derived from the Ïµ-accurate problem setting. Further, [Knoblauch et al., 2020] shows
solution, which is also known as a stationary point with theperfectmemoryforoptimalcontinuallearningisNP-
Eâˆ¥âˆ‡f(x)âˆ¥2 â‰¤ Ïµ.Thegeneralnonconvexfinite-sumprob- hard by using set-theory, but the quantitative analysis of
lems assume that all data points can be sampled during performancedegradationislessstudied.
trainingiterations.Thisfactisanobstacletodirectlyapply
Adaptivestepsizesinnonconvexsetting.Adaptivestep
(1)forcontinuallearningproblem.
sizesundersmoothnonconvexfinite-sumoptimizationprob-
We provide a solution of the above issue by leveraging lemhavebeenstudiedongeneralsingletaskcases[Reddi
memory-based methods, which allow models to access a etal.,2018,Zhangetal.,2020,Zaheeretal.,2018]recently.
partialaccesstothedatasetofprevioustasks.Inthissetting, [Simseklietal.,2019,Zhangetal.,2020,Simseklietal.,
wecananalyzenonconvexstochasticoptimizationproblems 2020]haverevealedthatthereexistsaheavy-tailednoisein
ontheconvergenceofprevioustaskswithlimitedaccess. someoptimizationproblemsforneuralnetworks,suchasat-
Similarwithadaptivemethodsfornoncovexoptimization, tentionmodels,and[Zhangetal.,2020]showsthatadaptive
weapplyadaptivestepsizesduringoptimizationtomini- methodsarehelpfultoachievethefasterconvergenceunder
mizeforgettingwiththeoreticalguarantee.Specifically,we theheavy-taileddistributionwherestochasticgradientsare
makethefollowingcontributions: poorlyconcentratedaroundthemean.Inthiswork,wetreat
thecontinuallearningproblemwherestochasticgradients
â€¢ Wedecomposethefinite-sumproblemofentiretasks ofprevioustasksareconsideredastheout-of-distribution
intotwosummationtermsforprevioustasksandacur- samplesin regardto acurrent task,anddevelopadaptive
renttask,respectively.Wetheoreticallyshowthatsmall methodswhicharewell-performedincontinuallearning.
randomsubsetsofprevioustasksleadtoanalyzingthe
expectedconvergencerateofbothtaskswhilelearning
3 PRELIMINARIES
acurrenttask.
â€¢ Westudytheconvergenceofgradientmethodsunder
Supposethatweobservethelearningprocedureonadata
asmallmemorywherethebackwardtransferperfor-
streamofcontinuallearningatsomearbitraryobservation
mance degrades, and propose a new formulation of
point.Letusconsidertimestept=0asgivenobservation
continual learning problem with the forgetting term.
point.WedefinetheprevioustaskP fort<0asallvisited
Wethenshowwhycatastrophicforgettingoccurstheo-
datapointsandthecurrenttaskCfortâ‰¥0asalldatapoints
reticallyandempirically.
whichwillfaceinthefuture.Then,P andC canbedefined
â€¢ Though memory-based methods mitigate forgetting, as the sets of data points in P and C at time step t = 0,
previousworksdoesnotfullyexploitthegradientin- respectively.Notethattheabovetaskdescriptionisbased
formationofmemory.Weintroduceanoveladaptive onnotasequenceofmultipletasks,buttwoseparatesetstoanalyzetheconvergenceofeachofP andCwhenstartingto WederiveEquation5inAppendixC.Assumption3.1isa
updatethegivenbatchatthecurrenttaskCatsomearbitrary well-knownandusefulstatementinnonconvexfinite-sum
observationpoint.Weconsideracontinuallearningproblem optimization problem [Reddi et al., 2016a, 2018, Zhang
as a smooth nonconvex finite-sum optimization problem et al., 2020, Zaheer et al., 2018], and also helps us to de-
withtwodecomposedobjectives scribe the convergence of continual learning. We also as-
sumethesupremumoflossgapbetweenaninitialpointx0
andaglobaloptimumxâˆ—asâˆ† ,andtheupperboundonthe
f
1 (cid:88) varianceofthestochasticgradientsasÏƒ f inthefollowing.
min h(x)= h (x), (2)
xâˆˆRd n f +n g iâˆˆPâˆªC i âˆ† =supf(x0)âˆ’f(xâˆ—),
f
x0
wheren andn arethenumbersofelementsforP andC,
andh(x)f canbeg
decomposedintoasfollows: Ïƒ2 =sup
1
(cid:88)nf
âˆ¥âˆ‡f (x)âˆ’âˆ‡f(x)âˆ¥2.
f n i
x f
n n i=1
h(x)= f f(x)+ g g(x).
n +n n +n
f g f g
It should be noted that g (x),âˆ‡g (x), which denote the
j j
lossandthegradientforacurrenttask,alsosatisfyallthree
Forclarity,weusef(x)=h(x)| andg(x)=h(x)| for
P C aboveassumptionsandthefollowingstatement.
therestrictionofhtoeachdatasetP andC,respectively.
f (x)andg (x)alsodenotestheobjectivetermsinduced Tomeasuretheefficiencyofastochasticgradientalgorithm,
i j
from data where each index is i âˆˆ P and j âˆˆ C, respec- wedefinetheIncrementalFirst-orderOracle(IFO)frame-
tively. work [Ghadimi and Lan, 2013]. IFO call is defined as a
unitofcomputationalcostbytakinganindexiwhichgets
SupposethatthereplaymemoriesM fortimestepâˆˆ[0,T]
t the pair (âˆ‡f (x),f (x)), and IFO complexity of an algo-
i i
arerandomvariableswhicharethesubsetsofPâˆªCtocover
rithmisdefinedasthesummationofIFOcallsduringopti-
priormemory-basedapproaches[Chaudhryetal.,2019b,a].
mization.Forexample,avanillastochasticgradientdescent
Toformulateanalgorithmformemory-basedapproaches,
(SGD)algorithmrequirescomputationalcostasmuchas
wedefinemini-batchesI whicharesampledfromamem-
t the batch size b at each step, and the IFO complexity is
t
ory M
t
at step t. We now define the stochastic update of thesumofbatchsizes(cid:80)T
b .LetT(Ïµ)betheminimum
t=1 t
memory-basedmethod
numberofiterationstoguaranteeÏµ-accuratesolutions.The
averageboundofIFOcomplexityislessthanorequalto
xt+1 =xtâˆ’Î± Htâˆ‡f It(xt)âˆ’Î² Htâˆ‡g Jt(xt), (3) (cid:80)T(Ïµ)b =O(1/Ïµ2)[Reddietal.,2016a].
t=1 t
whereI âŠ‚M andJ âŠ‚C denotethemini-batchesfrom
t t t
thereplaymemoryandthecurrentdatastream,respectively. 4 CONTINUALLEARNINGAS
Here,H istheunionofI andJ .Inaddition,foragivenset
t t t NONCONVEXOPTIMIZATION
S,âˆ‡f (xt),âˆ‡g (xt)denotethelossgradientofamodel
S S
xt withthemini-batchS attimestept.Theadaptivestep
We first present a theoretical convergence analysis of
sizes(learningrates)ofâˆ‡f (xt)andâˆ‡g (xt)aredenoted
It Jt memory-basedcontinuallearninginnonconvexsetting.We
byÎ± andÎ² whicharethefunctionsofH .
Ht Ht t aim to understand why catastrophic forgetting occurs in
Itshouldbenotedthemini-batchI fromM mightcontain termsoftheconvergencerate,andreformulatetheoptimiza-
t t
adatapointj âˆˆC forsomecases,suchasER-Reservoir. tionproblemofcontinuallearningintoanonconvexsetting
withtheoreticalguarantee.Forcompletenesswepresentall
Throughout the paper, we assume L-smoothness and the
proofsinAppendixC.
followingstatements.
Assumption3.1. f isL-smooththatthereexistsaconstant
i 4.1 MEMORY-BASEDNONCONVEXCONTINUAL
L>0suchthatforanyx,y âˆˆRd,
LEARNING
âˆ¥âˆ‡f (x)âˆ’âˆ‡f (y)âˆ¥â‰¤Lâˆ¥xâˆ’yâˆ¥ (4)
i i Unlikeconventionalsmoothnonconvexfinite-sumoptimiza-
tionproblemswhereeachmini-batchisi.i.d-sampledfrom
whereâˆ¥Â·âˆ¥denotestheEuclideannorm.Thenthefollowing
thewholedatasetP âˆªC,thereplaymemorybasedcontin-
inequalitydirectlyholdsthat
uallearningencountersanon-i.i.dstreamofdataC with
L accesstoasmallsizedmemoryM t.Algorithm1provides
âˆ’ 2âˆ¥xâˆ’yâˆ¥2 â‰¤f i(x)âˆ’f i(y)âˆ’âŸ¨âˆ‡f i(y),xâˆ’yâŸ© thepseudocodeformemory-basedapproachwiththeitera-
tiveupdaterule3.Now,wecananalyzetheconvergenceon
L
â‰¤ âˆ¥xâˆ’yâˆ¥2. (5) P andC duringalearningprocedureonanarbitrarydata
2ğ‘¡ğ‘¡ ğ‘¡ğ‘¡
âˆ‡ğ‘“ğ‘“ğ¼ğ¼ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ ) âˆ— âˆ‡ğ‘“ğ‘“ğ¼ğ¼ğ‘¡ğ‘¡(ğ‘¥ğ‘¥ ) âˆ—
ğ‘¥ğ‘¥ğ‘ƒğ‘ƒâˆªğ¶ğ¶ ğ‘¥ğ‘¥ğ‘ƒğ‘ƒâˆªğ¶ğ¶
ğ‘¡ğ‘¡ ğ‘¡ğ‘¡
ğ›¼ğ›¼ğ»ğ»ğ‘¡ğ‘¡âˆ‡ğ‘“ğ‘“ğ¼ğ¼ğ‘¡ğ‘¡ ğ‘¥ğ‘¥ +ğ›½ğ›½ğ»ğ»ğ‘¡ğ‘¡âˆ‡ğ‘”ğ‘”ğ½ğ½ğ‘¡ğ‘¡,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ )
ğ‘¡ğ‘¡
ğ‘¥ğ‘¥ğ‘ƒğ‘ƒâˆ— =ğ‘¥ğ‘¥0 âˆ‡ğ‘”ğ‘”ğ½ğ½ğ‘¡ğ‘¡,ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘(ğ‘¥ğ‘¥ ) ğ‘¥ğ‘¥ğ‘ƒğ‘ƒâˆ— =ğ‘¥ğ‘¥0 ğ›¼ğ›¼ğ»ğ»ğ‘¡ğ‘¡âˆ‡ğ‘“ğ‘“ğ¼ğ¼ğ‘¡ğ‘¡ ğ‘¥ğ‘¥ğ‘¡ğ‘¡ +ğ›½ğ›½ğ»ğ»ğ‘¡ğ‘¡âˆ‡ğ‘”ğ‘”ğ½ğ½ğ‘¡ğ‘¡,ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›(ğ‘¥ğ‘¥ğ‘¡ğ‘¡ )
ğ‘¡ğ‘¡
âˆ‡ğ‘”ğ‘”ğ½ğ½ğ‘¡ğ‘¡,ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›ğ‘›(ğ‘¥ğ‘¥ )
Figure1:GeometricillustrationofNon-ConvexContinualLearning(NCCL).Incontinuallearning,amodelparameterxt
startsfromalocaloptimalpointforthepreviouslylearnedtasksxâˆ—.OverT iterations,weexpecttoreachanewoptimal
P
pointxâˆ— whichhasagoodperformanceonbothP andC.Inthet-thiteration,xt encounterseitherâˆ‡g (xt)or
PâˆªC Jt,pos
âˆ‡g (xt).ThesetwocasesindicatewhetherâŸ¨f (xt),âˆ‡g (xt)âŸ©ispositiveornot.Topreventxt fromescapingthe
Jt,neg It Jt
feasibleregion,i.e.,catastrophicforgetting,weimposeatheoreticalconditiononlearningratesforf andg.
Algorithm1NonconvexContinualLearning(NCCL) Lemma4.1. IfM isuniformlysampledfromP,thenboth
0
episodicmemoryandER-reservoirsatisfies
Require: Previous task set P, current task set C, initial
modelx0. E (cid:2) âˆ‡f (xt)(cid:3) =âˆ‡f(xt) and E [e ]=0.
SampleainitialmemoryM âŠ‚P â–·Byreplayschemes,
M[0:t] Mt M[0:t] Mt
0
theselectiondist.ofM aredifferent.
0
Notethattakingexpectationiterativelywithrespecttothe
fort=0toT âˆ’1do
historyM isneededtocomputetheexpectedvalueof
Sampleamini-batchI âŠ‚M [0:t]
t t gradients for M . Surprisingly, taking the expectation of
Sampleamini-batchJ âŠ‚C t
t
overfittingerrorovermemoryselectiongetszero.However,
ComputestepsizesÎ± ,Î² byâˆ‡f (xt),âˆ‡g (xt)
Ht Ht It Jt itdoesnotimplye = 0foreachlearningtrialwithsome
xt+1 â†xtâˆ’Î± âˆ‡f (xt)âˆ’Î² âˆ‡g (xt) t
Ht It Ht Jt M .
UpdateM bytheruleofreplayschemewithJ . [0:t]
t+1 t
endfor
4.2 THEORETICALCONVERGENCEANALYSIS
stream from two consecutive sets P and C for continual Wenowproposetwotermsofinterestinagradientupdate
learning[Chaudhryetal.,2019a,b,2020b]. of nonconvex continual learning (NCCL). We define the
overfittingtermB andthecatastrophicforgettingtermÎ“
BylimitedaccesstoP,theexpectationofgradientupdate t t
E [âˆ‡f (xt)]inEquation3forf(x)isabiasedesti- asfollows:
ItâŠ‚Mt It
mateofthegradientâˆ‡f(xt).Atthetimestept,wehave B =(LÎ±2 âˆ’Î± )âŸ¨âˆ‡f(xt),e âŸ©+Î² âŸ¨âˆ‡g (xt),e âŸ©,
t Ht Ht t Ht Jt t
âˆ‡f Mt(xt)=E It(cid:2) âˆ‡f It(xt)|M t(cid:3) =E It(cid:2) âˆ‡f(xt)+e t|M t(cid:3) Î“
t
= Î² H2 2tL âˆ¥âˆ‡g Jt(xt)âˆ¥2âˆ’Î² Ht(1âˆ’Î± HtL)âŸ¨âˆ‡f It(xt),âˆ‡g Jt(xt)âŸ©.
=âˆ‡f(xt)+e ,
Mt
Theamountofeffectonconvergencebyasingleupdatecan
wheree ande denotetheerrorterms,âˆ‡f (xt)âˆ’âˆ‡f(xt)
t Mt It bemeasuredbyusingEquation5asfollows:
andtheexpectationoverI givenM ,respectively.Itshould
t t
benotedthatagivenreplaymemoryM twithsmallsizeat f(xt+1)â‰¤f(xt)âˆ’âŸ¨âˆ‡f(xt),Î± Htâˆ‡f It(xt)+Î² Htâˆ‡g Jt(xt)âŸ©
timesteptintroducesaninevitableoverfittingbias.
L
+ âˆ¥Î± âˆ‡f (xt)+Î² âˆ‡g (xt)âˆ¥2 (6)
For example, there exist two popular memory schemes, 2 Ht It Ht Jt
episodic memory and ER-reservoir. The episodic mem-
by letting x â† xt+1 and y â† xt. Note that the above
ory M = M for all t is uniformly sampled once from
t 0 inequalitycanberewrittenas
arandomsequenceofP,andER-reservoiriterativelysam-
(cid:18) (cid:19)
ples the replay memory M by the selection rule M âŠ‚ L
t t f(xt+1)â‰¤f(xt)âˆ’ Î± âˆ’ Î±2 âˆ¥âˆ‡f(xt)âˆ¥2+Î“ +B
M tâˆ’1âˆªJ t.Here,wedenotethehistoryofM tasM [0:t] = Ht 2 Ht t t
(M ,Â·Â·Â· ,M ).Tocomputetheexpectationoverallstochas-
0 t L
ticities of NCCL, we need to derive the expectation of + 2Î± H2 tâˆ¥e tâˆ¥2.
âˆ‡f (xt) over the randomness of M . We formalize the
Mt t
expectation over all learning trials with the selection ran- A NCCL algorithm update its model with two additional
domnessasfollows. termsB ,Î“ comparedtoconventionalSGD.Anoverfitting
t ttermB andacatastrophicforgettingtermÎ“ areobtained Thus, the convergence of a current task C is guaranteed,
t t
by grouping terms that contain e and âˆ‡g (xt), respec- sinceitssupersetM âˆªC isconverged.Otherwise,thecon-
t Jt
tively.Thesetwotermsinevitablydegradetheperformance vergenceratemightdifferfromtheconventionalSGDforC
ofNCCLwithrespecttotime.ItshouldbenotedthatÎ“ has bythegivenâˆ† ,Ïƒ attime0,buttheasymptotic
t h|MâˆªC h|MâˆªC
âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©,whichisakeyfactortodetermine convergencerateisstillidentical.
It Jt
interferenceandtransfer[Riemeretal.,2018].Ontheother
OnekeyobservationisthatE[Î“ ]arecumulativelyadded
hand,B includese ,whichisanerrorgradientbetweenthe t
t t ontheupperboundofEâˆ¥âˆ‡f(x)âˆ¥2,whichisaconstantin
batchfromM andtheentiredatasetP.
t conventionalSGD.Thelossgapâˆ† andthevarianceofgra-
f
SincetakingtheexpectationoverallstochasticitiesofNCCL dientsÏƒ arefixedvalues.Inpractice,tightening(cid:80) E[Î“ ]
f t t
impliesthetotalexpectation,wedefinetheoperatoroftotal appearstobecriticalfortheperformanceofNCCL.How-
âˆš
expectationwithrespectto0â‰¤t<T foreaseofexposition
ever,(cid:80)Tâˆ’1E[Î“
]/ T isnotguaranteedtoconvergeto0.
t=0 t
asfollows: Thisfactgivesrisetocatastrophicforgettingintermsofa
E t =E M[0:t](cid:2)E It[E Jt[Â·|I t]]|M [0:t](cid:3) . n oo fn thd eec cr oe na vsi en rg geu np cp eer ofbo (cid:80)un T t=d âˆ’. 01W Ee [n Î“o tw ]/âˆšsh Tow . thekeycondition
Inaddition,wedenoteE =E.Wefirststatethestepwise Lemma4.5. LetanupperboundÎ² >Î² >0.Consider
Tâˆ’1 Ht
changeofupperbound. twocases,Î² <Î±andÎ² â‰¥Î±forÎ±inTheorem4.3.Wehave
thefollowingbound
Lemma4.2. SupposethatAssumption3.1holdsand0<
Î± â‰¤ 2.ForxtupdatedbyAlgorithm1,wehave
Ht L
(cid:34) (cid:35)
T (cid:88)âˆ’1E âˆš[Î“ t] <O(cid:16) 1/T3/2+1/T(cid:17)
whenÎ² <Î±,
f(xt)âˆ’f(xt+1)+B +Î“ T
E âˆ¥âˆ‡f(xt)âˆ¥2 â‰¤E t t t=0
t t
(cid:34)
Î± Ht(1âˆ’ L 2Î± Ht (cid:35)) T (cid:88)âˆ’1E âˆš[Î“ t] <O(cid:16)âˆš
T
+1/âˆš T(cid:17)
, whenÎ² â‰¥Î±.
+E
Î± HtL
Ïƒ2 . (7) t=0
T
t 2(1âˆ’ LÎ± ) f
2 Ht
With the following theorem, we show that f(x) can con-
Surprisingly, we observe E t[B t] = 0 by Lemma 4.1. It vergeevenifwehavelimitedaccesstoP.
shouldbealsonotedthattheindividualtrialwitharandomly
givenM 0cannotcanceltheeffectofB t.Wediscussmore Theorem4.6. LetÎ² Ht <Î±= âˆšc
T
forallt.Thenwehave
detailsofoverfittingtomemoryinAppendixE. theconvergencerate
We now describe a convergence analysis of Algorithm 1. (cid:18) 1 (cid:19)
minEâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤O âˆš . (9)
We telescope over training iterations for the current task,
t T
whichleadstoobtainthefollowingtheorem.
âˆš Otherwise,f(x)isnotguaranteedtoconvergewhenÎ² â‰¥Î±
Theorem4.3. LetÎ±
Ht
=Î±= âˆšc
T
forsome0<câ‰¤ 2 LT andmightdivergeattherateO(âˆš
T).
and t âˆˆ {0,Â·Â·Â· ,T âˆ’ 1}. By Lemma 4.2, the iterates of
NCCLsatisfy Corollary 4.7. For Î²
Ht
< Î± = âˆšc
T
for all t, the IFO
complexityofAlgorithm1toobtainanÏµ-accuratesolution
is:
A (cid:32) 1(cid:32) T (cid:88)âˆ’1 (cid:33) Lc (cid:33) IFOcalls=O(1/Ïµ2). (10)
minEâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤ âˆš âˆ† + E[Î“ ] + Ïƒ2
t T c f t 2 f
t=0
Webuildintuituionsabouttheconvergenceconditionofthe
whereA=1/(1âˆ’LÎ±/2). previoustasksP inTheorem4.6.Asempiricallyshownin
stable A-GEM and stable ER-Reservoir [Mirzadeh et al.,
We also prove the convergence rate of a current task C 2020],theconditionofÎ² Ht <Î±theoreticallyimpliesthat
with the gradient udpates from the replay-memory M in decaying step size is a key solution to continual learning
continuallearining. consideringwhenwepickanyarbitraryobservationpoints.
Lemma4.4. SupposethatI âˆ©J =âˆ…,Takingexpectation Remark4.8. Topreventcatastrophicforgetting,thestep
t t
overI âŠ‚M andJ âŠ‚C,wehave sizeofg(x),Î² shouldbelowerthanthestepsizeoff(x),
t t t Ht
(cid:114)
2âˆ† L
Î± Ht.ItshouldalsobenotedthatE M[1:t][B t|M 0]isnotal-
m tinEâˆ¥âˆ‡h| MâˆªC(xt)âˆ¥2 â‰¤ h| TMâˆªC Ïƒ h|MâˆªC, (8) w tria ay ls w0 itf hor da ifn fy erM en0 t. gT ivh eis ni Mmpli ae ls soth ha at, sf tr ho em nt oim n-e zes rte op c0 u, me uac lah
-
0
whereâˆ† andÏƒ istheversionoflossgapand
tivesum(cid:80)E
[B |M ],whichoccursoverestimating
h|MâˆªC h|MâˆªC M[1:T] t 0
thevarianceforhonM âˆªC,respectively. biastheoretically.The convergence rate with respect to the marginalization
onM 0inTheorem4.6exactlymatchtheusualnonconvex (cid:28) âˆ‡f (xt) (cid:29) âˆ‡f (xt)
SGDrates.TheselectionrulesforM 0withvariousmemory âˆ‡g Jt(xt)âˆ’ âˆ¥âˆ‡fIt (xt)âˆ¥,âˆ‡g Jt(xt) âˆ¥âˆ‡fIt (xt)âˆ¥.
schemesareimportanttoreducethevarianceofconvergence It It
rate with having the mean convergence rate as Equation
LetÎ² bethestepsizeforg(x)whentheconstraintisnot
9 among trials. This is why memory schemes matters in
violated.Thenwecaninterpretthesurrogateasanadaptive
c do etn at ii ln su ia nl Ale pa pr en nin dg ixi En .terms of variance. Please see more learningrateÎ± Ht,whichisÎ±(1âˆ’âŸ¨âˆ‡fI âˆ¥t âˆ‡(x ft I) t, (âˆ‡ xtg )J âˆ¥t 2(xt)âŸ© )tocan-
celoutthenegativecomponentofâˆ‡f (xt)onâˆ‡g (xt).
It Jt
4.3 REFORMULATEDPROBLEMOFCONTINUAL For the transfer case Î› Ht > 0, A-GEM use Î± Ht = 0.
LEARNING Afterapplyingthesurrogate,E[Î“ t]isreducedasshownin
AppendixD.ItisnotedthatA-GEMtheoreticallyviolates
The previous section showed the essential factors in con- theconstraintsof(11)topreventcatastrophicforgettingby
tinuallearningtoobservethetheoreticalconvergencerate. lettingÎ± Ht =0anddoesnotutilizethebettertransfereffect.
TheoverfittingbiastermB hasastrongdependenceonthe Then, A-GEM is an adaptive method without theoretical
t
memoryselectionruleandcanbecomputedexactlyonly guarantee.
ifwecanaccesstheentiredatasetP duringlearningonC.
Intermsofexpectation,wehaveshownthattheeffectof
5.2 NCCL
B isnegligible.Wealsoshowthatitsempiricaleffectis
t
lessimportantthanÎ“ t inFigure2.Thenwefocusonthe Asdiscussedabove,wenotethatE[Î“ ]isaquadraticpoly-
t
performancedegradationbythecatastrophicforgettingterm
nomial of Î² . For the interference case Î› â‰¤ 0, the
Î“ t.Foreverytrial,theworst-caseconvergenceisdependent minimum poH int t of polynomial, Î²âˆ— has a neH gt ative value
o pen râˆ† bof u+ nd(cid:80) anT t= dâˆ’ 0 k1 eE ep[Î“ tht] eb my oT dh ee lo tore bm e4 c. o3 n. vT eo rgt eig dh ,t wen esth he ouu lp d- whichviolatestheconstraintÎ² Ht H >t 0,andE[Î“ t]ismono-
tonically increasing on Î² > 0. Then, we instead adapt
minimizethecumulativesumofÎ“ t. Wenowreformulate Î± toreducethevalueoH ft E[Î“ ]attimetbyadoptingthe
thecontinuallearningproblem2asfollows.
Ht t
scheme of A-GEM. The minimum of the polynomial on
E[Î“ ]canbeobtainedwhenthecaseoftransfer,Î› > 0
Tâˆ’1 t Ht
minimize (cid:88) E[Î“ t] bydifferentiatingonÎ² Ht.ThentheminimumE[Î“âˆ— t]andthe
Î±Ht,Î²Ht
t=0
optimalstepsizeÎ² Hâˆ—
t
canbeobtainedas
subjectto 0<Î² <Î± â‰¤2/Lforallt<T (11)
Ht Ht
Î²âˆ— =
(1âˆ’Î± HtL)Î›
Ht,
E[Î“âˆ—]=âˆ’(1âˆ’Î± HtL)Î›
Ht.
Ht Lâˆ¥âˆ‡g (xt)âˆ¥2 t 2Lâˆ¥âˆ‡g (xt)âˆ¥2
Jt Jt
Itisnotedthattheabovereformulationpresentsatheoreti-
callyguaranteedcontinuallearningframeworkformemory-
Tosatisfytheconstraintsof(11),weshouldupdateâˆ‡f (xt)
basedapproachesinnonconvexsettingandtheconstraintis
It
withnon-zerostepsizeandÎ² <Î± forallt.Thenthe
toguaranteetheconvergenceofbothf(x)andg(x). Ht Ht
proposedadaptivemethodformemory-basedapproachesis
givenby
5 ADAPTIVEMETHODSFOR
CONTINUALLEARNING Î±
Ht
=(cid:40) Î±Î±( ,1âˆ’ âˆ¥âˆ‡fÎ› ItH (xt t)âˆ¥2), Î›Î› Ht >â‰¤ 00
,
Ht
AsdiscussedintheaboveSection,wecansolveamemory- (cid:40)
Î±, Î› â‰¤0
b tia vs ee mdc eo thn oti dn sua al rele va arn rii an ng tsby ofm Si Gni Dm ,iz win hg ic(cid:80) haT t= uâˆ’ t0 o1 mE a[Î“ tit c] a. lA lyd aa dp -- Î² Ht = min(cid:16) Î±(1âˆ’Î´), L( âˆ¥1 âˆ‡âˆ’ gÎ± JL t() xÎ› tH )âˆ¥t 2(cid:17) , Î›H Ht
t
>0
justthestepsize(learningrate)onaper-featurebasis.Inthis âˆš
section,wereviewA-GEMintermsofadaptivemethods, whereÎ±=c/ T andÎ´issomeconstant0<Î´ â‰ª1.Note
and also propose a new algorithm (NCCL) for achieving thatourtwoadaptivelearningratesareastepwisegreedy
adaptivityincontinuallearning.Forbrevity,wedenotethe perspectivechoiceofmemory-basedcontinuallearning.
innerproductâŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©asÎ› .
It Jt Ht
6 EXPERIMENTS
5.1 A-GEM
We use two following metrics to evaluate algorithms. (1)
A-GEM [Chaudhry et al., 2019a] propose a surrogate of Averageaccuracyisdefinedas 1 (cid:80)T a ,wherea
T j=1 T,j i,j
âˆ‡g (xt)asthefollowingequationtoavoidviolatingthe denotesthetestaccuracyontaskj aftertrainingontaski.
Jt
constraintwhenthecaseofinterference,Î› â‰¤0: (2)Forgettingistheaveragemaximumforgettingisdefined
Ht0.20 A-GEM A ER-G -rE inM g 1000 2.5
ER-reservoir 800
0.15 N NC CC CL L- -r ri en sg ervoir 600 2.0
0.10 ER-ring ER-reservoir 400
1.5
0.05 NCCL-reservoir 200
0.00 NCCL-ring 0 1.0
0 2000 4000 6000 8000 012345678910111213141516171819 5 10 15 20
Cumulative gamma Tasks Gradient norm
(a) (b) (c)
4 bias
20
15
2
15 10 0
10
2
5
5 4
0.0 0.5 1.0 1.5 2.0 0 1000 2000 3000 4000 5000 0123456789101112131415161718
Cumulative gamma Cumulative gamma Tasks
(d) (e) (f)
Figure2:Metricsforcontinuallearning(CL)algorithmstrainedonsplit-CIFAR100withdifferent5seeds.(a)Forgetting
versus(cid:80)E[Î“ ]attheendoftraining.(b)Evolutionof(cid:80)E[Î“
]duringcontinuallearning.(c)Empiricalverificationof
t t
therelationbetweenâˆ¥âˆ‡f(x)âˆ¥forthefirsttaskandtestlossofthefirsttaskinsplitCIFAR-100.(d)-(e)aretheempirical
verificationof(cid:80)E[Î“
]versusâˆ¥âˆ‡f(x)âˆ¥forthefirsttaskinCLalgorithms.Theredhorizontallineindicatestheempirical
t
âˆ¥âˆ‡f(x)âˆ¥rightaftertrainingthefirsttask.(f)IllustrationofempiricalB attheendofeachtask.
t
as 1 (cid:80)Tâˆ’1 max (a âˆ’a ). Due to limited space, iteration for experience replay. ER-Reservoir [Chaudhry
Tâˆ’1 j=1 l,j T,j
lâˆˆ[Tâˆ’1] etal.,2019b]showsapowerfulperformanceincontinual
wereportthedetailsofarchitectureandlearningprocedure
learningscenario.GEMandA-GEM[Lopez-PazandRan-
andmissingresultswithadditionaldatasetsinAppendixB.
zato,2017,Chaudhryetal.,2019a]usegradientepisodic
memory to overcome forgetting. The key idea of GEM
isgradientprojectionwithquadraticprogrammingandA-
6.1 EXPERIMENTALSETUP
GEMsimplifiesthisprocedure.WealsocomparewithiCarl,
MER,ORTHOG-SUBSPACE[Chaudhryetal.,2020b],sta-
Datasets.Wedemonstratetheexperimentalresultsonstan-
bleSGD[Mirzadehetal.,2020],andMC-SGD[Mirzadeh
dard continual learning benckmarks: Permuted-MNIST
etal.,2021].
[Kirkpatricketal.,2017]isaMNIST[LeCunetal.,1998]
baseddataset,whereeachtaskhasafixedpermutationof
pixelsandtransformdatapointsbythepermutationtomake 6.2 EXPERIMENTRESULTS
eachtaskdistributionunrelated.Split-MNIST[Zenkeetal.,
2017]splitsMNISTdatasetintofivetasks.Eachtaskcon- Thefollowingtablesshowourmainexperimentalresults,
sistsoftwoclasses,forexample(1,7),(3,4),andhasap- whichisaveragedover5runs.Wedenotethenumberofex-
proximately12Kimages.Split-CIFAR10,100,andMini- amplesperclasspertaskatthetopofeachcolumn.Overall,
ImagenetalsosplitversionsofCIFAR-10,100[Krizhevsky NCCL + memory schemes outperform baseline methods
etal.,2009],andMiniImagenet[Vinyalsetal.,2016]into especially in the forgetting metric. Our goal is to demon-
fivetasksand20tasks. stratetheusefulnessoftheadaptivemethodstoreducethe
catastrophicforgetting,andtoshowempiricalevidencefor
Baselines. We report the experimental evaluation on the
ourconvergenceanalysis.WeremarkthatNCCLsuccess-
onlinecontinualsettingwhich impliesamodelistrained
fullysuppressforgettingbyalargemargincomparedto
withasingleepoch.Wecomparewiththefollowingcon-
baselines.ItisnotedthatNCCLalsooutperformsA-GEM,
tinual learning baselines. Fine-tune is a simple method
whichdoesnotmaximizetransferwhenÎ› >0andvio-
thatamodeltrainsobserveddatanaivelywithoutanysup- Ht
latestheproposedconstraintsin(11).
port,suchasreplaymemory.Elasticweightconsolidation
(EWC) is a regularization based method by Fisher Infor- We now investigate the proposed terms with regard to
mation[Kirkpatricketal.,2017].ER-Reservoirchooses
memory-basedcontinuallearning,(cid:80)E[Î“
]andB .Tover-
t t
samplestostorefromadatastreamwithaprobabilitypro- ifyourtheoreticalanalysis,inFigure2weshowthecumula-
portional to the number of observed data points. The re- tivecatastrophicforgettingterm(cid:80) E[Î“ ]isthekeyfactor
t t
playmemoryreturnsarandomsubsetofsamplesateach oftheconvergenceofthefirsttaskinsplit-CIFAR100.Dur-
gnittegroF
mron
tneidarG
ammag
evitalumuC
mron
tneidarG
ssol
tseT
saiBTable1:AccuaryandForgetitngresultsbetweentheproposedmethods(NCCL+Ring,NCCL+Reservoir)andotherbaselines
intask-incrementallearning.Whenthereplay-memoryisused,wedenotethememorysizeasthenumberofexamplesper
classpertask.TheadditionalresultsandthedetailedsettingwithdifferentmemorysizeisinAppendixB
dataset Permuted-MNIST split-CIFAR100 split-MiniImagenet
Method
memorysize 5 5 1
memory accuracy forgetting accuracy forgetting accuracy forgetting
Fine-tune âœ— 47.9 0.29(0.01) 40.4(2.83) 0.31(0.02) 36.1(1.31) 0.24(0.03)
EWC âœ— 63.1(1.40) 0.18(0.01) 42.7(1.89) 0.28(0.03) 34.8(2.34) 0.24(0.04)
stableSGD âœ— 80.1(0.51) 0.09(0.01) 59.9(1.81) 0.08(0.01) - -
MC-SGD âœ— 85.3(0.61) 0.06(0.01) 63.3(2.21) 0.06(0.03) - -
A-GEM âœ“ 64.1(0.74) 0.19(0.01) 59.9(2.64) 0.10(0.02) 42.3(1.42) 0.17(0.01)
ER-Ring âœ“ 75.8(0.24) 0.07(0.01) 62.6(1.77) 0.08(0.02) 49.8(2.92) 0.12(0.01)
ER-Reservoir âœ“ 76.2(0.38) 0.07(0.01) 65.5(1.99) 0.09(0.02) 44.4(3.22) 0.17(0.02)
ORHOG-subspace âœ“ 84.32(1.1) 0.11(0.01) 64.38(0.95) 0.055(0.007) 51.4(1.44) 0.10(0.01)
NCCL+Ring âœ“ 84.41(0.32) 0.053(0.002) 61.09(1.47) 0.02(0.01) 45.5(0.245) 0.041(0.01)
NCCL+Reservoir âœ“ 88.22(0.26) 0.028(0.003) 63.68(0.18) 0.028(0.009) 41.0(1.02) 0.09(0.01)
Multi-task 91.3 0 71 0 65.1 0
ingcontinuallearning,(cid:80) E[Î“ ]increasesinallmethodsof betweenforgettingonP andlearningonC.InAppendix
t t
Figure2b.Figrure2a,2d,2eshowthatthelarger(cid:80) E[Î“ ] B,weaddmoreresultswithlargersizesofmemory,which
t t
causesthelargerforgettingandâˆ¥âˆ‡f(x)âˆ¥forthefirsttask. showsthatNCCLoutperformsintermsofaverageaccuracy.
Wecanobservethatâˆ¥âˆ‡f(x)âˆ¥getslargerthan4,whichis Itmeansthatestimatingtransferandinterferenceintermsof
fortheredline,when(cid:80) E[Î“ ]becomeslargerthan2.We Î› toalleviateforgettingbythesmallmemoryforNCCL
t t Ht
alsoverifythatthetheoreticalresultE [B ]=0isvalidin islesseffective.
t t
Figure2f.ItimpliesthattheempiricalresultsofLemma4.1,
Table2:Resultsofclassincrementalsplit-CIFAR100with
whichshowtheeffectofB onEquation7.Furthermore,the
t
Memorysize=10,000.
memorybiashelpstotightentheconvergencerateofP by
havingnegativevaluesinpractice.Evenwithtinymemory,
theestimatedB hasmuchsmallervaluethanE[Î“ ]aswe Methods accuracy
t t
canobserveinFigure2.Forexperiencereplay,weneednot Finetune 3.06(0.2)
toworryaboutthedegradationbymemorybiasandwould A-GEM 2.40(0.2)
liketoemphasizethattinymemorycanslightlyhelptokeep GSS-Greedy[Aljundietal.,2019b] 19.53(1.3)
the convergence on P empirically. We conclude that the MIR[Aljundietal.,2019a] 20.02(1.7)
overfittingbiastermmightnotbeamajorfactorindegrad- ER+GMED[Jinetal.,2020] 20.93(1.6)
ingtheperformanceofcontinuallearningagentwhenitis MIR+GMED[Jinetal.,2020] 21.22(1.0)
comparedtothecatastrophicforgettingtermÎ“ t.Next,we NCCL-Reservoir(ours) 21.95(0.3)
modify the clipping bound of Î² in Section of adaptive
Ht
methodstoresolvethelowerperformanceintermsofav-
erageaccuracy.InTable1,NCCL+Ringdoesnothavethe
7 CONCLUSION
bestaverageaccuracyscore,eventhoughithasthelowest
valueof(cid:80)E[Î“
].Aswediscussedearlier,itisbecausethe
t
We have presented a theoretical convergence analysis of
convergencerateofC isslowerthanvanillaER-Ringwith
continuallearning.Ourproofshowsthatatrainingmodel
thefixedstepsizes.Now,weremovetherestrictionofÎ² ,
(cid:16) (cid:17)
Ht
cancircumventcatastrophicforgettingbysuppressingcatas-
min Î±(1âˆ’Î´), L( âˆ¥1 âˆ‡âˆ’ gÎ± JL t() xÎ› tH )âˆ¥t 2 for Î› Ht > 0, and instead trophicforgettingtermintermsoftheconvergenceonpre-
applythemaximumclippingboundÎ² tomaximizethe
max vious task. We demonstrate theoretically and empirically
transfereffect,whichoccursifÎ› >0,bygettingE[Î“âˆ—].
Ht t thatadaptivemethodswithmemoryschemesshowthebet-
In the original version, we force Î² < Î± to reduce the-
Ht terperformanceintermsofforgetting.Itisalsonotedthat
oreticalcatastrophicforgettingtermcompletely.However,
thereexisttwofactorsontheconvergenceofprevioustask:
replacingwithÎ² ishelpfulintermsofaverageaccuracy
max catastrophicforgettingandoverfittingtomemory.Finally,it
as shown in Appendix B. It means that Î² is a hyper-
max isexpectedtheproposednonconvexframeworkishelpful
parameter to increase the average accuracy by balancing
toanalyzetheconvergencerateofCLalgorithms.AuthorContributions SaynaEbrahimi,MohamedElhoseiny,TrevorDarrell,and
MarcusRohrbach. Uncertainty-guidedcontinuallearn-
SeungyubHaninitatedandledthiswork,implementedthe ingwithbayesianneuralnetworks. In8thInternational
proposedmethod,andwrotethepaper.YeongmoKimset ConferenceonLearningRepresentations,ICLR2020,Ad-
up and ran baseline tests. Taehyun Cho helped to write disAbaba,Ethiopia,April26-30,2020.OpenReview.net,
thepaperandverifiedmathematicalproofs.JungwooLee 2020.
advisedonthiswork.Correspondingauthor:JungwooLee
RobertM.FrenchandNickChater. Usingnoisetocompute
(e-mail:junglee@snu.ac.kr)
errorsurfacesinconnectionistnetworks:Anovelmeans
ofreducingcatastrophicforgetting. NeuralComputation,
Acknowledgements 14(7):1755â€“1769,2002.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and
This work was supported in part by National Research
zeroth-ordermethodsfornonconvexstochasticprogram-
Foundation of Korea(NRF) grants funded by the Korea
ming. SIAMJournalonOptimization,23(4):2341â€“2368,
Government(MSIT) (No. 2021R1A2C2014504 and No.
2013.
2021M3F3A2A02037893),inpartbyInstituteofInforma-
tion & communications Technology Planning & Evalua- KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
tion(IITP)grantsfundedbytheKoreaGovernment(MSIT) Deep residual learning for image recognition. In Pro-
(No.2021-0-00106(20%),No.2021-0-02068(20%),and ceedingsoftheIEEEconferenceoncomputervisionand
No.2021-0-01059(20%)),andinpartbyINMACandBK21 patternrecognition,pages770â€“778,2016.
FOURprogram.
XisenJin,ArkaSadhu,JunyiDu,andXiangRen. Gradient-
basededitingofmemoryexamplesforonlinetask-free
continual learning. arXiv preprint arXiv:2006.15294,
References
2020.
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Mas- XisenJin,ArkaSadhu,JunyiDu,andXiangRen. Gradient-
simoCaccia,MinLin,LaurentCharlin,andTinneTuyte- basededitingofmemoryexamplesforonlinetask-free
laars. Online continual learning with maximally inter- continuallearning. AdvancesinNeuralInformationPro-
fered retrieval. CoRR, abs/1908.04742, 2019a. URL cessingSystems,34:29193â€“29205,2021.
http://arxiv.org/abs/1908.04742.
JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,Joel
RahafAljundi,MinLin,BaptisteGoujaud,andYoshuaBen- Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
gio. Gradientbasedsampleselectionforonlinecontinual Milan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-
learning. InAdvancesinNeuralInformationProcessing Barwinska,etal. Overcomingcatastrophicforgettingin
Systems,pages11816â€“11825,2019b. neuralnetworks. Proceedingsofthenationalacademyof
sciences,114(13):3521â€“3526,2017.
ArslanChaudhry,Marcâ€™AurelioRanzato,MarcusRohrbach,
JeremiasKnoblauch,HishamHusain,andTomDiethe.Opti-
andMohamedElhoseiny. Efficientlifelonglearningwith
malcontinuallearninghasperfectmemoryandisnp-hard.
A-GEM. In7thInternationalConferenceonLearning
arXivpreprintarXiv:2006.05188,2020.
Representations,ICLR2019,NewOrleans,LA,USA,May
6-9,2019.OpenReview.net,2019a. AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
layersoffeaturesfromtinyimages. 2009.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick
seiny, Thalaiyasingam Ajanthan, Puneet K Dokania,
Haffner. Gradient-based learning applied to document
Philip HS Torr, and Marcâ€™Aurelio Ranzato. On tiny
recognition.ProceedingsoftheIEEE,86(11):2278â€“2324,
episodicmemoriesincontinuallearning. arXivpreprint
1998.
arXiv:1902.10486,2019b.
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee.
ArslanChaudhry,AlbertGordo,PuneetKDokania,Philip
Overcomingcatastrophicforgettingwithunlabeleddata
Torr,andDavidLopez-Paz. Usinghindsighttoanchor
in the wild. In Proceedings of the IEEE International
past knowledge in continual learning. arXiv preprint
ConferenceonComputerVision,pages312â€“321,2019.
arXiv:2002.08165,3,2020a.
SoochanLee,JunsooHa,DongsuZhang,andGunheeKim.
ArslanChaudhry,NaeemullahKhan,PuneetDokania,and A neural dirichlet process mixture model for task-free
PhilipTorr. Continuallearninginlow-rankorthogonal continual learning. In 8th International Conference
subspaces. AdvancesinNeuralInformationProcessing onLearningRepresentations,ICLR2020,AddisAbaba,
Systems,33,2020b. Ethiopia,April26-30,2020.OpenReview.net,2020.LihuaLei,ChengJu,JianboChen,andMichaelIJordan. minimizinginterference. InInternationalConferenceon
Non-convex finite-sum optimization via scsg methods. LearningRepresentations,2018.
InAdvancesinNeuralInformationProcessingSystems,
Mark B. Ring. Continual learning in reinforcement
pages2348â€“2358,2017.
environments. PhD thesis, University of Texas at
ZhizhongLiandDerekHoiem. Learningwithoutforget- Austin,TX,USA,1995. URLhttp://d-nb.info/
ting. IEEEtransactionsonpatternanalysisandmachine 945690320.
intelligence,40(12):2935â€“2947,2017.
UmutSimsekli,LeventSagun,andMertGurbuzbalaban. A
David Lopez-Paz and Marcâ€™Aurelio Ranzato. Gradient tail-index analysis of stochastic gradient noise in deep
episodic memory for continual learning. In Advances neuralnetworks.InInternationalConferenceonMachine
inNeuralInformationProcessingSystems,pages6467â€“ Learning,pages5827â€“5837.PMLR,2019.
6476,2017.
UmutSimsekli,LingjiongZhu,YeeWhyeTeh,andMert
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pas- Gurbuzbalaban. Fractional underdamped langevin dy-
canu,andHassanGhasemzadeh. Understandingtherole namics:Retargetingsgdwithmomentumunderheavy-
oftrainingregimesincontinuallearning. InAdvances tailed gradient noise. In International Conference on
in Neural Information Processing Systems 33: Annual MachineLearning,pages8970â€“8980.PMLR,2020.
ConferenceonNeuralInformationProcessingSystems
Sebastian Thrun. A lifelong learning perspective
2020,2020.
for mobile robot control. In Intelligent Robots
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, and Systems, Selections of the International Confer-
RazvanPascanu,andHassanGhasemzadeh.Linearmode ence on Intelligent Robots and Systems 1994, IROS
connectivityinmultitaskandcontinuallearning. InInter- 94, Munich, Germany, 12-16 September 1994, pages
nationalConferenceonLearningRepresentations,2021. 201â€“214, 1994. doi: 10.1016/b978-044482250-5/
50015-3. URL https://doi.org/10.1016/
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and b978-044482250-5/50015-3.
RichardE.Turner. Variationalcontinuallearning. In6th
InternationalConferenceonLearningRepresentations, OriolVinyals,CharlesBlundell,TimothyLillicrap,koray
ICLR2018,Vancouver,BC,Canada,April30-May3, kavukcuoglu,andDaanWierstra. Matchingnetworksfor
2018,ConferenceTrackProceedings.OpenReview.net, oneshotlearning. InD.Lee,M.Sugiyama,U.Luxburg,
2018. I. Guyon, and R. Garnett, editors, Advances in Neural
InformationProcessingSystems,volume29.CurranAs-
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg sociates,Inc.,2016. URLhttps://proceedings.
Sperl,andChristophHLampert. icarl:Incrementalclas- neurips.cc/paper/2016/file/
sifierandrepresentationlearning. InProceedingsofthe 90e1357833654983612fb05e3ec9148c-Paper.
IEEEconferenceonComputerVisionandPatternRecog- pdf.
nition,pages2001â€“2010,2017.
Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu,
SashankJReddi,AhmedHefny,SuvritSra,BarnabÃ¡sPÃ³c- Aniruddha Kembhavi, Mohammad Rastegari, Jason
zos,andAlexSmola. Stochasticvariancereductionfor Yosinski,andAliFarhadi. Supermasksinsuperposition.
nonconvexoptimization. InInternationalconferenceon AdvancesinNeuralInformationProcessingSystems,33:
machinelearning,pages314â€“323,2016a. 15173â€“15184,2020.
SashankJ.Reddi,AhmedHefny,SuvritSra,BarnabÃ¡sPÃ³c- Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
zos, and Alexander J. Smola. Stochastic variance re- Hwang. Lifelonglearningwithdynamicallyexpandable
ductionfornonconvexoptimization. InProceedingsof networks. In6thInternationalConferenceonLearning
the33ndInternationalConferenceonMachineLearning, Representations, ICLR 2018, Vancouver, BC, Canada,
ICML2016,NewYorkCity,NY,USA,June19-24,2016, April30-May3,2018,ConferenceTrackProceedings.
pages314â€“323,2016b.URLhttp://proceedings. OpenReview.net,2018.
mlr.press/v48/reddi16.html.
ManzilZaheer,SashankReddi,DevendraSachan,Satyen
SashankJReddi,SatyenKale,andSanjivKumar. Onthe Kale,andSanjivKumar. Adaptivemethodsfornoncon-
convergenceofadamandbeyond. InInternationalCon- vex optimization. In Advances in neural information
ferenceonLearningRepresentations,2018. processingsystems,pages9793â€“9803,2018.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao FriedemannZenke,BenPoole,andSuryaGanguli. Contin-
Liu,IrinaRish,YuhaiTu,andGeraldTesauro. Learning uallearningthroughsynapticintelligence. Proceedings
tolearnwithoutforgettingbymaximizingtransferand ofmachinelearningresearch,70:3987,2017.JingzhaoZhang,SaiPraneethKarimireddy,AndreasVeit,
SeungyeonKim,SashankReddi,SanjivKumar,andSu-
vritSra. Whyareadaptivemethodsgoodforattention
models? Advances in Neural Information Processing
Systems,33,2020.A ADDITIONALBACKGROUNDSANDEXTENDEDDISCUSSION
A.1 SUMMARYOFNOTATIONS
Notations Definitions Notations Definitions
x modelparameter H theunionofI andJ
t t t
P previoustask n thenumberofdatapointsinP
f
C currenttask n thenumberofdatapointsinC
g
P datasetofP âŸ¨Â·,Â·âŸ© innerproduct
C datasetofC L L-smoothnessconstant
h(x) meanlossofxonentiredatasets Î± adaptivestepsizeforf withH
Ht t
f(x) meanlossofxonP Î² adaptivestepsizeforgwithH
Ht t
g(x) meanlossofxonC M memoryattimet
t
f (x) lossofxonadatapointiâˆˆP e errorofestimatef attimet
i t
g (x) lossofxonadatapointj âˆˆC e errorofestimatef withM
j Mt t
f (x) mini-batchlossofxonabatchI f meanlossofxwithM
It t Mt t
g (x) mini-batchlossofxonabatchJ M thehistoryofmemoryfromt1tot2
Jt t [t1:t2]
I minibatchsampledfromP B memorybiastermatt
t t
J minibatchsampledfromC Î“ forgettingtermatt
t t
E totalexpectationfrom0totimet Î› innerproductbetweenâˆ‡f andâˆ‡g
t Ht It Jt
A.2 REVIEWOFTERMINOLOGY
(Restrictionoff)Iff :Aâ†’BandifA isasubsetofA,thentherestrictionoff toA isthefunction
0 0
f| :A â†’B
A0 0
givenbyf| (x)=f(x)forxâˆˆA .
A0 0
A.3 ADDITIONALRELATEDWORK
Regularizationbasedmethods.EWChasanadditionalpenalizationlossthatpreventtheupdateofparametersfromlosing
theinformationofprevioustasks.WhenweupdateamodelwithEWC,wehavetwogradientcomponentsfromthecurrent
taskandthepenalizationloss.
task-specificmodelcomponents.SupSuplearnsaseparatesubnetworkforeachtasktopredictagivendatabysuperimposing
allsupermasks.Itisanovelmethodtosolvecatastrophicforgettingwithtakingadvantageofneuralnetworks.
SGDmethodswithoutexpereincereplay.stableSGD[Mirzadehetal.,2020]andMC-SGD[Jinetal.,2021]showoverall
higher performancein terms of average accuracy than theproposed algorithm. For averageforgetting,our method has
the lowest value, which means that NCCL prevents catastrophic forgetting successfully with achieving the reasonable
performanceonthecurrenttask.Wethinkthatourmethodisfocusedonreducingcatastrophicforgettingaswedefined
inthereformulatedcontinuallearningproblem(12),soourmethodshowsthebetterperformanceonaverageforgetting.
Otherwise,MC-SGDfindsalow-losspathswithmode-connectivitybyupdatingwiththeproposedregularizationloss.This
procedureimpliesthatacontinuallearningmodelmightfindabetterlocalminimumpointforthenew(current)taskthan
NCCL.
Fornon-memorybasedmethods,thetheoreticalmeasuretoobserveforgettingandconvergenceduringtrainingdoesnot
exist.Ourtheoreticalresultsarethefirstattempttoanalyzetheconvergenceofprevioustasksduringcontinuallearning
procedure.Infuturework,wecanapproximatethevalueofwithfisherinformationforEWCandintroduceBayesiandeep
learningtoanalyzetheconvergenceofeachsubnetworksforeachtaskinthecaseofSupSup[Wortsmanetal.,2020].B ADDITIONALEXPERIMENTALRESULTSANDIMPLEMENTATIONDETAILS
WeimplementthebaselinesandtheproposedmethodonTensorflow1.Forevaluation,weuseanNVIDIA2080tiGPU
alongwith3.60GHzInteli9-9900KCPUand64GBRAM.
B.1 ARCHITECTUREANDTRAININGDETAIL
Forfaircomparison,wefollowthecommonlyusedmodelarchitectureandhyperparametersof[Leeetal.,2020,Chaudhry
etal.,2020b].ForPermuted-MNISTandSplit-MNIST,weusefully-connectedneuralnetworkswithtwohiddenlayersof
[400,400]or[256,256]andReLUactivation.ResNet-18withthenumberoffiltersn =64,20[Heetal.,2016]isapplied
f
forSplitCIFAR-10and100.Allexperimentsconductasingle-passoverthedatastream.Itisalsocalled1epochor0.2
epoch(inthecaseofsplittasks).Wedealbothcaseswithandwithoutthetaskidentifiersintheresultsofsplit-tasksto
comparefairlywithbaselines.Batchsizesofdatastreamandmemoryareboth10.Allreportedvaluesaretheaverage
valuesof5runswithdiffrentseeds,andwealsoprovidestandarddeviation.Othermiscellaneoussettingsarethesameasin
[Chaudhryetal.,2020b].
B.2 HYPERPARAMETERGRIDS
Wereportthehyper-paramtersgridweusedinourexperimentsbelow.Exceptfortheproposedalgorithm,weadoptedthe
hyper-paramtersthatarereportedintheoriginalpapers.Weusedgridsearchtofindtheoptimalparametersforeachmodel.
â€¢ finetune-learningrate[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
â€¢ EWC-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]-regularization:[0.1,1,10(MNIST,CIFAR),
100,1000]
â€¢ A-GEM-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
â€¢ ER-Ring-learningrate:[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
â€¢ ORTHOG-SUBSPACE-learningrate:[0.003,0.01,0.03,0.1(MNIST),0.2,0.4(CIFAR),1.0]
â€¢ MER-learningrate:[0.003,0.01,0.03(MNIST,CIFAR),0.1,0.3,1.0]-withinbatchmeta-learningrate:[0.01,0.03,
0.1(MNIST,CIFAR),0.3,1.0]-currentbatchlearningratemultiplier:[1,2,5(CIFAR),10(MNIST)]
â€¢ iid-offlineandiid-online-learningrate[0.003,0.01,0.03(CIFAR),0.1(MNIST),0.3,1.0]
â€¢ ER-Reservoir-learningrate:[0.003,0.01,0.03,0.1(MNIST,CIFAR),0.3,1.0]
â€¢ NCCL-Ring(default)-learningrateÎ±:[0.003,0.001(CIFAR),0.01,0.03,0.1,0.3,1.0]
â€¢ NCCL-Reservoir-learningrateÎ±:[0.003(CIFAR),0.001,0.01,0.03,0.1,0.3,1.0]
B.3 HYPERPARAMETERSEARCHONÎ² ANDTRAININGTIME
max
Table3:Permuted-MNIST(23tasks10000examplespertask),FC-[256,256]andMulti-headedsplit-CIFAR100,fullsize
Resnet-18.AccuracieswithdifferentclippingrateonNCCL+Ring.
Î² Permuted-MNIST Split-CIFAR100
max
0.001 72.52(0.59) 49.43(0.65)
0.01 72.93(1.38) 56.95(1.02)
0.05 72.18(0.77) 56.35(1.42)
0.1 72.29(1.34) 58.20(0.155)
0.2 74.38(0.89) 57.60(0.36)
0.5 72.95(0.50) 59.06(1.02)
1 72.92(1.07) 57.43(1.33)
5 72.31(1.79) 57.75(0.24)Table4:Permuted-MNIST(23tasks10000examplespertask),FC-[256,256]andMulti-headedsplit-CIFAR100,fullsize
Resnet-18.Trainingtime.
Trainingtime[s]
Methods
Permuted-MNIST Split-CIFAR100
fine-tune 91 92
EWC 95 159
A-GEM 180 760
ER-Ring 109 129
ER-Reservoir 95 113
ORTHOG-SUBSPACE 90 581
NCCL+Ring 167 248
NCCL+Reservoir 168 242B.4 ADDITIONALEXPERIMENTRESULTS
Table5:Permuted-MNIST(23tasks60000examplespertask),FC-[256,256].
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
multi-task âœ— 83 - 83 -
Fine-tune âœ— 53.5(1.46) 0.29(0.01) 47.9 0.29(0.01)
EWC âœ— 63.1(1.40) 0.18(0.01) 63.1(1.40) 0.18(0.01)
stableSGD âœ— 80.1(0.51) 0.09(0.01) 80.1(0.51) 0.09(0.01)
MC-SGD âœ— 85.3(0.61) 0.06(0.01) 85.3(0.61) 0.06(0.01)
MER âœ“ 69.9(0.40) 0.14(0.01) 78.3(0.19) 0.06(0.01)
A-GEM âœ“ 62.1(1.39) 0.21(0.01) 64.1(0.74) 0.19(0.01)
ER-Ring âœ“ 70.2(0.56) 0.12(0.01) 75.8(0.24) 0.07(0.01)
ER-Reservoir âœ“ 68.9(0.89) 0.15(0.01) 76.2(0.38) 0.07(0.01)
ORHOG-subspace âœ“ 84.32(1.10) 0.12(0.01) 84.32(1.1) 0.11(0.01)
NCCL+Ring âœ“ 74.22(0.75) 0.13(0.007) 84.41(0.32) 0.053(0.002)
NCCL+Reservoir âœ“ 79.36(0.73) 0.12(0.007) 88.22(0.26) 0.028(0.003)
Table6:Multi-headedsplit-CIFAR100,reducedsizeResnet-18n =20.
f
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
EWC âœ— 42.7(1.89) 0.28(0.03) 42.7(1.89) 0.28(0.03)
Fintune âœ— 40.4(2.83) 0.31(0.02) 40.4(2.83) 0.31(0.02)
StableSGD âœ— 59.9(1.81) 0.08(0.01) 59.9(1.81) 0.08(0.01)
MC-SGD âœ— 63.3(2.21) 0.06(0.03) 63.3(2.21) 0.06(0.03)
A-GEM âœ“ 50.7(2.32) 0.19(0.04) 59.9(2.64) 0.10(0.02)
ER-Ring âœ“ 56.2(1.93) 0.13(0.01) 62.6(1.77) 0.08(0.02)
ER-Reservoir âœ“ 46.9(0.76) 0.21(0.03) 65.5(1.99) 0.09(0.02)
ORTHOG-subspace âœ“ 58.81(1.88) 0.12(0.02) 64.38(0.95) 0.055(0.007)
NCCL+Ring âœ“ 54.63(0.65) 0.059(0.01) 61.09(1.47) 0.02(0.01)
NCCL+Reservoir âœ“ 52.18(0.48) 0.118(0.01) 63.68(0.18) 0.028(0.009)
Table7:Multi-headedsplit-MiniImagenet,fullsizeResnet-18n =64.Accuracyandforgettingresults.
f
memorysize 1
Method
memory accuracy forgetting
Fintune âœ— 36.1(1.31) 0.24(0.03)
EWC âœ— 34.8(2.34) 0.24(0.04)
A-GEM âœ“ 42.3(1.42) 0.17(0.01)
MER âœ“ 45.5(1.49) 0.15(0.01)
ER-Ring âœ“ 49.8(2.92) 0.12(0.01)
ER-Reservoir âœ“ 44.4(3.22) 0.17(0.02)
ORTHOG-subspace âœ“ 51.4(1.44) 0.10(0.01)
NCCL+Ring âœ“ 45.5(0.245) 0.041(0.01)
NCCL+Reservoir âœ“ 41.0(1.02) 0.09(0.01)Table8:Multi-headedsplit-CIFAR100,fullsizeResnet-18n =64.Accuracyandforgettingresults.
f
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
Fintune âœ— 42.6(2.72) 0.27(0.02) 42.6(2.72) 0.27(0.02)
EWC âœ— 43.2(2.77) 0.26(0.02) 43.2(2.77) 0.26(0.02)
ICRAL âœ“ 46.4(1.21) 0.16(0.01) - -
A-GEM âœ“ 51.3(3.49) 0.18(0.03) 60.9(2.5) 0.11(0.01)
MER âœ“ 49.7(2.97) 0.19(0.03) - -
ER-Ring âœ“ 59.6(1.19) 0.14(0.01) 67.2(1.72) 0.06(0.01)
ER-Reservoir âœ“ 51.5(2.15) 0.14(0.09) 62.68(0.91) 0.06(0.01)
ORTHOG-subspace âœ“ 64.3(0.59) 0.07(0.01) 67.3(0.98) 0.05(0.01)
NCCL+Ring âœ“ 59.06(1.02) 0.03(0.02) 66.58(0.12) 0.004(0.003)
NCCL+Reservoir âœ“ 54.7(0.91) 0.083(0.01) 66.37(0.19) 0.004(0.001)
Table9:permuted-MNIST(23tasks10000examplespertask),FC-[256,256].Accuracyandforgettingresults.
memorysize 1 5
Method
memory accuracy forgetting accuracy forgetting
multi-task âœ— 91.3 - 83 -
Fine-tune âœ— 50.6(2.57) 0.29(0.01) 47.9 0.29(0.01)
EWC âœ— 68.4(0.76) 0.18(0.01) 63.1(1.40) 0.18(0.01)
MER âœ“ 78.6(0.84) 0.15(0.01) 88.34(0.26) 0.049(0.003)
A-GEM âœ“ 78.3(0.42) 0.21(0.01) 64.1(0.74) 0.19(0.01)
ER-Ring âœ“ 79.5(0.31) 0.12(0.01) 75.8(0.24) 0.07(0.01)
ER-Reservoir âœ“ 68.9(0.89) 0.15(0.01) 76.2(0.38) 0.07(0.01)
ORHOG-subspace âœ“ 86.6(0.91) 0.04(0.01) 87.04(0.43) 0.04(0.003)
NCCL+Ring âœ“ 74.38(0.89) 0.05(0.009) 83.76(0.21) 0.014(0.001)
NCCL+Reservoir âœ“ 76.48(0.29) 0.1(0.002) 86.02(0.06) 0.013(0.002)
Table10:Single-headedsplit-MNIST,FC-[256,256].Accuracyandforgettingresults.
memorysize 1 5 50
Method
memory accuracy forgetting accuracy forgetting accuracy forgetting
multi-task âœ— 95.2 - - - - -
Fine-tune âœ— 52.52(5.24) 0.41(0.06) - - - -
EWC âœ— 56.48(6.46) 0.31(0.05) - - - -
A-GEM âœ“ 34.04(7.10) 0.23(0.11) 33.57(6.32) 0.18(0.03) 33.35(4.52) 0.12(0.04)
ER-Reservoir âœ“ 34.63(6.03) 0.79(0.07) 63.60(3.11) 0.42(0.05) 86.17(0.99) 0.13(0.016)
NCCL+Ring âœ“ 34.64(3.27) 0.55(0.03) 61.02(6.21) 0.207(0.07) 81.35(8.24) -0.03(0.1)
NCCL+Reservoir âœ“ 37.02(0.34) 0.509(0.009) 65.4(0.7) 0.16(0.006) 88.9(0.28) -0.125(0.004)Table11:Single-headedsplit-MNIST,FC-[400,400]andmem.size=500(50/cls.).Accuracyandforgettingresults.
Method accuracy
multi-task 96.18
Fine-tune 50.9(5.53)
EWC 55.40(6.29)
A-GEM 26.49(5.62)
ER-Reservoir 85.1(1.02)
CN-DPM 93.23
Gdumb 91.9(0.5)
NCCL+Reservoir 95.15(0.91)
Table12:Single-headedsplit-CIFAR10,fullsizeResnet-18andmem.size=500(50/cls.).Accuracyandforgettingresults.
Method accuracy
iid-offline 93.17
iid-online 36.65
Fine-tune 12.68
EWC 53.49(0.72)
A-GEM 54.28(3.48)
GSS 33.56
ReservoirSampling 37.09
CN-DPM 41.78
NCCL+Ring 54.63(0.76)
NCCL+Reservoir 55.43(0.32)
Table13:Single-headedsplit-CIFAR100,Resnet18withn =20.Memorysize=10,000.Weconducttheexperimentwith
f
thesamesettingofGMED[Jinetal.,2021].
Methods accuracy
Finetune 3.06(0.2)
iidonline 18.13(0.8)
iidoffline 42.00(0.9)
A-GEM 2.40(0.2)
GSS-Greedy 19.53(1.3)
BGD 3.11(0.2)
ER-Reservoir 20.11(1.2)
ER-Reservoir+GMED 20.93(1.6)
MIR 20.02(1.7)
MIR+GMED 21.22(1.0)
NCCL-Reservoir 21.95(0.3)C THEORETICALANALYSIS
Inthissection,weprovidetheproofsoftheresultsfornonconvexcontinuallearning.Wefirststartwiththederivationof
Equation5inAssumption3.1.
C.1 ASSUMPTIONANDADDITIONALLEMMA
DerivationofEquation5. Recallthat
L
|f (x)âˆ’f (y)âˆ’âŸ¨âˆ‡f (y),xâˆ’yâŸ©|â‰¤ âˆ¥xâˆ’yâˆ¥2. (12)
i i i 2
Notethatf isdifferentiableandnonconvex.Wedefineafunctiong(t)=f (y+t(xâˆ’y))fortâˆˆ[0,1]andanobjective
i i
functionf .Bythefundamentaltheoremofcalculus,
i
(cid:90) 1
gâ€²(t)dt=f(x)âˆ’f(y). (13)
0
Bytheproperty,wehave
|f (x)âˆ’f (y)âˆ’âŸ¨âˆ‡f (y),xâˆ’yâŸ©|
i i i
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
=(cid:12) âŸ¨âˆ‡f i(y+t(xâˆ’y)),xâˆ’yâŸ©dtâˆ’âŸ¨âˆ‡f i(y),xâˆ’yâŸ©(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
=(cid:12) âŸ¨âˆ‡f i(y+t(xâˆ’y))âˆ’âˆ‡f i(y),xâˆ’yâŸ©dt(cid:12).
(cid:12) (cid:12)
0
UsingtheCauchy-Schwartzinequality,
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
(cid:12) âŸ¨âˆ‡f i(y+t(xâˆ’y))âˆ’âˆ‡f i(y),xâˆ’yâŸ©dt(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
â‰¤(cid:12) âˆ¥âˆ‡f i(y+t(xâˆ’y))âˆ’âˆ‡f i(y)âˆ¥Â·âˆ¥xâˆ’yâˆ¥dt(cid:12).
(cid:12) (cid:12)
0
Sincef satisfiesEquation4,thenwehave
i
|f (x)âˆ’f (y)âˆ’âŸ¨âˆ‡f (y),xâˆ’yâŸ©|
i i i
(cid:12)(cid:90) 1 (cid:12)
(cid:12) (cid:12)
â‰¤(cid:12) Lâˆ¥y+t(xâˆ’y)âˆ’yâˆ¥Â·âˆ¥xâˆ’yâˆ¥dt(cid:12)
(cid:12) (cid:12)
0
(cid:12)(cid:90) 1 (cid:12)
=Lâˆ¥xâˆ’yâˆ¥2(cid:12)
(cid:12)
tdt(cid:12)
(cid:12)
(cid:12) (cid:12)
0
L
= âˆ¥xâˆ’yâˆ¥2.
2
LemmaC.1. Letp=[p ,Â·Â·Â·p ], q =[q ,Â·Â·Â· ,q ]betwostatisticallyindependentrandomvectorswithdimensionD.
1 D 1 D
ThentheexpectationoftheinnerproductoftworandomvectorsE[âŸ¨p,qâŸ©]is(cid:80)D E[p ]E[q ].
d=1 d d
Proof. Bythepropertyofexpectation,
D
(cid:88)
E[âŸ¨p,qâŸ©]=E[ p q ]
d d
d=1
D
(cid:88)
= E[p q ]
d d
d=1
D
(cid:88)
= E[p ]E[q ].
d d
d=1C.2 PROOFOFMAINRESULTS
Wenowshowthemainresultsofourwork.
ProofofLemma4.1. ToclarifytheissueofE [E [e |M ]]=0,letusexplainthedetailsofconstructingreplay-memory
Mt It t t
asfollows.Wehaveconsideredepisodicmemoryandreservoirsamplinginthepaper.Wewillfirstshowthecaseofepisodic
memorybydescribingthesamplingmethodforreplaymemory.Wecanalsoderivethecaseofreservoirsamplingbysimply
applyingtheresultofepisodicmemory.
Episodicmemory(ringbuffer).WedividetheentiredatasetofcontinuallearningintotheprevioustaskP andthecurrent
task C on the time step t = 0. For the previous task P, the data stream of P is i.i.d., and its sequence is random on
everytrial(episode).Thetrial(episode)impliesthatacontinuallearningagentlearnsfromanonlinedatastreamwith
two consecutive data sequences of P and C. Episodic memory takes the last data points of the given memory size m
bytheFirstInFirstOut(FIFO)rule,andholdstheentiredatapointsuntillearningonC isfinished.Then,wenotethat
M = M forallt â‰¥ 0andM isuniformlysampledfromthei.i.d.sequenceofP.Bythelawoftotalexpectation,we
t 0 0
deriveE [E [âˆ‡f (xt)|M ]]foranyxt, âˆ€tâ‰¥0.
M0âŠ‚P It It 0
E (cid:2)E (cid:2) âˆ‡f (xt)|M (cid:3)(cid:3) =E (cid:2) âˆ‡f (xt)(cid:3) .
M0âŠ‚P It It 0 M0âŠ‚P M0
ItisknownthatM wasuniformlysampledfromP oneachtrialbeforetrainingonthecurrenttaskC.Then,wetake
0
expectationwithrespecttoeverytrialthatimpliestheexpectedvalueoverthememorydistributionM .Wehave
0
E (cid:2) âˆ‡f (xt)(cid:3) =âˆ‡f(xt)
M0âŠ‚P M0
for any xt, âˆ€t. We can consider âˆ‡f (xt) as a sample mean of P on every trial for any xt, âˆ€t â‰¥ 0. Although xt is
Mt
constructediteratively,theexpectedvalueofthesamplemeanforanyxt,E [âˆ‡f (xt)]isalsoderivedasâˆ‡f(xt).
M0âŠ‚P M0
Reservoirsampling.Toclarifythenotationforreservoirsamplingfirst,wedenotetheexpectationwithrespecttothe
historyofreplaymemoryM = (M ,Â·Â·Â· ,M )asE .ThisistherevisedversionofE .Reservoirsamplingis
[0:t] 0 t M[0:t] Mt
atrickiercasethanepisodicmemory,butE [E [e |M ]] = 0stillholds.SupposethatM isfullofthedatapoints
M[0:t] It t t 0
fromP astheepisodicmemoryissampledandthemini-batchsizefromC is1forsimplicity.Thereservoirsampling
algorithmdropsadatapointinM andreplacesthedroppeddatapointwithadatapointinthecurrentmini-batchfrom
tâˆ’1
C withprobabilityp = m/n,wheremisthememorysizeandnisthenumberofvisiteddatapointssofar.Theexact
pseudo-codeforreservoirsamplingisdescribedin[1].Thereplacementprocedureuniformlychoosesthedatapointwhich
willbedropped.Wecanalsoconsiderthereplacementprocedureasfollows.ThememoryM forP isreducedinsize1from
t
M ,andthereplaceddatapointd fromC contributesintermsofâˆ‡g (xt)ifd issampledfromthereplaymemory.
tâˆ’1 C dC C
LetM =[d ,Â·Â·Â· ,d ]where|Â·|denotesthecardinalityofthememory.ThesamplemeanofM isgivenas
tâˆ’1 1 |Mtâˆ’1| tâˆ’1
1 (cid:88)
âˆ‡f (xtâˆ’1)= âˆ‡f (xtâˆ’1). (14)
Mtâˆ’1 |M | di
tâˆ’1
di
Bytheruleofreservoirsampling,weassumethatthereplacementprocedurereducesthememoryfromM toM with
tâˆ’1 t
size|M |âˆ’1andthesetofremainedupcomingdatapointsC âˆˆC fromthecurrentdatastreamforonlinecontinual
tâˆ’1 t
learningisreformulatedintoC âˆª[d ].Then,d canberesampledfromC âˆª[d ]tobecomposedoftheminibatch
tâˆ’1 C C tâˆ’1 C
ofreservoirsamplingwiththedfferentprobability.However,weignoretheprobabilityissuenowtofocusontheeffectof
replay-memoryonâˆ‡f.Now,wesampleM fromM ,thenwegettherandomvectorâˆ‡f (xt)as
t tâˆ’1 Mt
1
|M (cid:88)tâˆ’1|
âˆ‡f (xt)= W âˆ‡f (xt), (15)
Mt |M | ij dj
t
j=1
wheretheindexiisuniformlysampledfromiâˆ¼[1,Â·Â·Â· ,|M |],andW istheindicatorfunctionthatW is0ifi=j
tâˆ’1 ij ij
else1.
Theabovedescriptionimpliesthedroppingrule,andM canbeconsideredasanuniformlysampledsetwithsize|M |from
t t
M .TherecouldalsobeM =M withprobability1âˆ’p=1âˆ’m/n.Thentheexpectationofâˆ‡f (xt)givenM
tâˆ’1 t tâˆ’1 Mt tâˆ’1isderivedas
ï£« ï£¶
E Mt[âˆ‡f Mt(xt)|M tâˆ’1]=pï£­ |M1
||M (cid:88)tâˆ’1|
|M1
||M (cid:88)tâˆ’1|
W ijâˆ‡f dj(xt)ï£¸+(1âˆ’p)(cid:0) âˆ‡f Mtâˆ’1(xt)(cid:1)
tâˆ’1 t
i j=1
=âˆ‡f (xt).
Mtâˆ’1
Whenweconsiderthemini-batchsampling,wecanformallyreformulatetheaboveequationas
E (cid:2)E (cid:2) âˆ‡f (xt)|M (cid:3) |M (cid:3) =âˆ‡f (xt). (16)
Mtâˆ¼p(Mt|Mtâˆ’1) ItâŠ‚Mt It t tâˆ’1 Mtâˆ’1
Now,weapplytheaboveequationrecursively.Then,
E (cid:2) Â·Â·Â·E (cid:2)E (cid:2) âˆ‡f (xt)|M (cid:3) |M (cid:3) Â·Â·Â·|M (cid:3) =âˆ‡f (xt). (17)
M1âˆ¼p(M1|M0) Mtâˆ¼p(Mt|Mtâˆ’1) ItâŠ‚Mt It t tâˆ’1 0 M0
Similartoepisodicmemory,M isuniformlysampledfromP.Therefore,weconcludethat
0
E [âˆ‡f (xt)]=âˆ‡f(xt) (18)
M0,Â·Â·Â·,Mt Mt
bytakingexpectationoverthehistoryM =(M ,M ,Â·Â·Â· ,M ).
[0:t] 1 2 t
NotethattakingexpectationiterativelywithrespecttothehistoryM isneededtocomputetheexpectedvalueofgradients
[t]
forM .However,theresultE [E [e |M ]]=0stillholdsintermsofexpectation.
t M0,Â·Â·Â·,Mt It t t
Furthermore,wealsodiscussthattheeffectofreservoirsamplingontheconvergenceofC.Unlikewesimplyupdateg(x)
bythestochasticgradientdescentonC,thedatapointsd âˆˆ M âˆ©C havealittlelargersamplingprobabilitythanother
datapointsd âˆˆCâˆ’M.TheexpectationofgradientnormontheaveragedlossEâˆ¥âˆ‡g(xt)âˆ¥2isbasedontheuniform
Câˆ’M
andequiprobablesamplingoverC,butthenatureofreservoirsamplingdistortthismeasureslightly.Inthispaper,wefocus
ontheconvergenceoftheprevioustaskC whiletrainingonthecurrenttaskC withseveralexistingmemory-basedmethods.
Therefore,analyzingtheconvergenceofreservoirsamplingmethodwillbeafuturework.
ProofofLemma4.2. Weanalyzetheconvergenceofnonconvexcontinuallearningwithreplaymemoryhere.Recallthat
thegradientupdateisthefollowing
xt+1 =xtâˆ’Î± âˆ‡f (xt)âˆ’Î² âˆ‡g (xt)
Ht It Ht Jt
forallt âˆˆ {1,2,Â·Â·Â· ,T}.Lete = âˆ‡f (xt)âˆ’âˆ‡f(xt).Sinceweassumethatf, g isL-smooth,wehavethefollowing
t It
inequalitybyapplyingEquation5:
L
f(xt+1)â‰¤f(xt)+âŸ¨âˆ‡f(xt),xt+1âˆ’xtâŸ©+ âˆ¥xt+1âˆ’xtâˆ¥2
2
L
=f(xt)âˆ’âŸ¨âˆ‡f(xt),Î± âˆ‡f (xt)+Î² âˆ‡g (xt)âŸ©+ âˆ¥Î± âˆ‡f (xt)+Î² âˆ‡g (xt)âˆ¥2
Ht It Ht Jt 2 Ht It Ht Jt
=f(xt)âˆ’Î± âŸ¨âˆ‡f(xt),âˆ‡f (xt)âŸ©âˆ’Î² âŸ¨âˆ‡f(xt),âˆ‡g (xt)âŸ©
Ht It Ht Jt
L L
+ Î±2 âˆ¥âˆ‡f (xt)âˆ¥2+ Î²2 âˆ¥âˆ‡g (xt)âˆ¥2+LÎ± Î² âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©
2 Ht It 2 Ht Jt Ht Ht It Jt
=f(xt)âˆ’Î± âŸ¨âˆ‡f(xt),âˆ‡f(xt)âŸ©âˆ’Î± âŸ¨âˆ‡f(xt),e âŸ©âˆ’Î² âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©+Î² âŸ¨âˆ‡g (xt),e âŸ©
Ht Ht t Ht It Jt Ht Jt t
LÎ±2 LÎ±2 LÎ²2
+ Htâˆ¥âˆ‡f(xt)âˆ¥2+LÎ±2 âŸ¨âˆ‡f(xt),e âŸ©+ Htâˆ¥e âˆ¥2+ Htâˆ¥âˆ‡g (xt)âˆ¥2+LÎ± Î² âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©
2 Ht t 2 t 2 Jt Ht Ht It Jt
(cid:18) (cid:19)
L L
=f(xt)âˆ’ Î± âˆ’ Î±2 âˆ¥âˆ‡f(xt)âˆ¥2+ Î²2 âˆ¥âˆ‡g (xt)âˆ¥2âˆ’Î² (1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©
Ht 2 Ht 2 Ht Jt Ht Ht It Jt
+(cid:0) LÎ±2 âˆ’Î± (cid:1) âŸ¨âˆ‡f(xt),e âŸ©+Î² âŸ¨âˆ‡g (xt),e âŸ©+ L Î±2 âˆ¥e âˆ¥2. (19)
Ht Ht t Ht Jt t 2 Ht tToshowtheproposedtheoreticalconvergenceanalysisofnonconvexcontinuallearning,wedefinethecatastrophicforgetting
termÎ“ andtheoverfittingtermB asfollows:
t t
B =(LÎ±2 âˆ’Î± )âŸ¨âˆ‡f(xt),e âŸ©+Î² âŸ¨âˆ‡g (xt),e âŸ©,
t Ht Ht t Ht Jt t
Î²2 L
Î“ = Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’Î² (1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©.
t 2 Jt Ht Ht It Jt
Then,wecanrewriteEquation19as
(cid:18) (cid:19)
L L
f(xt+1)â‰¤f(xt)âˆ’ Î± âˆ’ Î±2 âˆ¥âˆ‡f(xt)âˆ¥2+Î“ +B + Î±2 âˆ¥e âˆ¥2. (20)
Ht 2 Ht t t 2 Ht t
WefirstnotethatB isdependentoftheerrorterme withthebatchI .Inthecontinuallearningstep,antrainingagentcannot
t t t
accessâˆ‡f(xt),thenwecannotgettheexactvalueofe .Furthermore,Î“ isdependentofthegradientsâˆ‡f (xt),âˆ‡g (xt)
t t It It
andthelearningratesÎ± ,Î² .
Ht Ht
TakingexpectationswithrespecttoI onbothsidesgivenJ ,wehave
t t
E (cid:2) f(xt+1)(cid:3) â‰¤E (cid:20) f(xt)âˆ’(cid:18) Î± âˆ’ L Î±2 (cid:19) âˆ¥âˆ‡f(xt)âˆ¥2+Î“ +B + L Î±2 âˆ¥e âˆ¥2(cid:12) (cid:12)J (cid:21)
It It Ht 2 Ht t t 2 Ht t (cid:12) t
(cid:20) (cid:18) L (cid:19) L (cid:21) (cid:104) (cid:12) (cid:105)
â‰¤E f(xt)âˆ’ Î± âˆ’ Î±2 âˆ¥âˆ‡f(xt)âˆ¥2+ Î±2 âˆ¥e âˆ¥2 +E Î“ +B (cid:12)J .
It Ht 2 Ht 2 Ht t It t t(cid:12) t
Now,takingexpectationsoverthewholestochasticityweobtain
(cid:20) (cid:18) (cid:19) (cid:21)
E(cid:2) f(xt+1)(cid:3) â‰¤E f(xt)âˆ’ Î± âˆ’ L Î±2 âˆ¥âˆ‡f(xt)âˆ¥2+Î“ +B + L Î±2 âˆ¥e âˆ¥2 .
Ht 2 Ht t t 2 Ht t
Rearrangingthetermsandassumethat 1 >0,wehave
1âˆ’LÎ±Ht/2
(cid:18) (cid:19) (cid:20) (cid:21)
L L
Î± âˆ’ Î±2 Eâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤E f(xt)âˆ’f(xt+1)+Î“ +B + Î±2 âˆ¥e âˆ¥2
Ht 2 Ht t t 2 Ht t
and
(cid:34) (cid:35)
Eâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤E 1 (cid:0) f(xt)âˆ’f(xt+1)+Î“ +B (cid:1) + Î± HtL âˆ¥e âˆ¥2
Î± (1âˆ’ LÎ± ) t t 2(1âˆ’ LÎ± ) t
Ht 2 Ht 2 Ht
(cid:34) (cid:35)
â‰¤E 1 (cid:0) f(xt)âˆ’f(xt+1)+Î“ +B (cid:1) + Î± HtL Ïƒ2 .
Î± (1âˆ’ LÎ± ) t t 2(1âˆ’ LÎ± ) f
Ht 2 Ht 2 Ht
âˆš
ProofofTheorem4.3. SupposethatthelearningrateÎ± isaconstantÎ±=c/ T,forc>0,1âˆ’ LÎ±= 1 >0.Then,by
Ht 2 A
summingEquation7fromt=0toT âˆ’1,wehaveTâˆ’1
1 (cid:88)
minEâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤ Eâˆ¥âˆ‡f(xt)âˆ¥2
t T
t=0
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
1 1 (cid:88) L
â‰¤ f(x0)âˆ’f(xT)+ (E[B +Î“ ]) + Î±Ïƒ2
1âˆ’ LÎ± Î±T t t 2 f
2 t=0
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
1 1 (cid:88) Lc
= âˆš âˆ† + (E[B +Î“ ]) + âˆš Ïƒ2
1âˆ’ LÎ± c T f t t 2 T f
2 t=0
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
A 1 (cid:88) Lc
= âˆš âˆ† + E[B +Î“ ] + Ïƒ2 . (21)
T c f t t 2 f
t=0
We note that a batch I is sampled from a memory M âŠ‚ M which is a random vector whose element is a datapoint
t t
dâˆˆP âˆªC.Then,takingexpectationoverI âŠ‚M âŠ‚P âˆªC impliesthatE[B ]=0.Therefore,wegettheminimumof
t t t
expectedsquareofthenormofgradients
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
A 1 (cid:88) Lc
minEâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤ âˆš âˆ† + E[Î“ ] + Ïƒ2 .
t T c f t 2 f
t=0
âˆš
ProofofLemma4.4. Tosimplifytheproof,weassumethatlearningratesÎ± ,Î² areasamefixedvalueÎ² =câ€²/ T.
Ht Ht
The assumption is reasonable, because it is observed that the RHS of Equation 7 is not perturbed drastically by small
(cid:83)
learningratesin0<Î± ,Î² â‰¤2/Lâ‰ª1.LetusdenotetheunionofM overtime0â‰¤tâ‰¤T âˆ’1asM = M .By
Ht Ht t t t
theassumption,itisequivalenttoupdateonM âˆªC.Then,thenon-convexfinitesumoptimizationisgivenas
1 (cid:88)
min h| (x)= h (x), (22)
xâˆˆRd MâˆªC n g+|M| i
iâˆˆMâˆªC
where|M|isthenumberofelementsinM.ThisproblemcanbesolvedbyasimpleSGDalgorithm[Reddietal.,2016b].
Thus,wehave
minEâˆ¥âˆ‡h| (xt)âˆ¥2 â‰¤
1 (cid:88)T
Eâˆ¥âˆ‡h| (xt)âˆ¥2
â‰¤(cid:114) 2âˆ† h|MâˆªCL
Ïƒ . (23)
t MâˆªC T MâˆªC T h|MâˆªC
t=0
LemmaC.2. ForanyC âŠ‚D âŠ‚M âˆªC,defineÏ‰2 as
h|D
Ï‰2 =supE âˆ¥âˆ‡h (xt)âˆ’âˆ‡h| (xt)âˆ¥2].
h|D
x
jâˆˆD j MâˆªC
Then,wehave
Eâˆ¥âˆ‡g (xt)âˆ¥2 â‰¤Eâˆ¥âˆ‡h| (xt)âˆ¥2+ sup Ï‰2 . (24)
Jt MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
ProofofLemmaC.2. WearriveatthefollowingresultbyJensenâ€™sinequality
supE âˆ¥âˆ‡g (xt)âˆ’âˆ‡h| (xt)âˆ¥2 =supE (cid:2) âˆ¥E [âˆ‡h (xt)]âˆ’âˆ‡h| (xt)âˆ¥2(cid:3) (25)
JtâŠ‚C Jt MâˆªC JtâŠ‚C jâˆˆJt j MâˆªC
x x
â‰¤ sup supE (cid:2) âˆ¥E [âˆ‡h (xt)]âˆ’âˆ‡h| (xt)âˆ¥2(cid:3) (26)
JtâŠ‚D jâˆˆJt j MâˆªC
CâŠ‚DâŠ‚MâˆªC x
(cid:20) (cid:21)
â‰¤ sup supE [âˆ¥âˆ‡h (xt)âˆ’âˆ‡h| (xt)âˆ¥2] (27)
jâˆˆD j MâˆªC
CâŠ‚DâŠ‚MâˆªC x
= sup Ï‰2 . (28)
h|D
CâŠ‚DâŠ‚MâˆªCBythetriangularinequality,weget
Eâˆ¥âˆ‡g (xt)âˆ¥2 â‰¤Eâˆ¥âˆ‡g (xt)âˆ’âˆ‡h| (xt)âˆ¥2+Eâˆ¥âˆ‡h| (xt)âˆ¥2 (29)
Jt Jt MâˆªC MâˆªC
â‰¤Eâˆ¥âˆ‡h| (xt)âˆ¥2+ sup Ï‰2 . (30)
MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
Forcontinuallearning,themodelx0 reachestoanÏµ-stationarypointoff(x)whenwehavefinishedtolearnP andstart
tolearnC.Now,wediscussthefrequencyoftransferandinterferenceduringcontinuallearningbeforeshowingLemma
4.5.Itiswellknownthatthefrequenciesbetweeninterferenceandtransferhavesimilarvalues(thefrequencyofconstraint
violationisapproximately0.5forAGEM)asshowninAppendixDof[Chaudhryetal.,2019a].Evenifmemory-based
continuallearninghasasmallmemorybufferwhichcontainsasubsetofP,randomsamplingfromthebufferallowstohave
similarfrequenciesbetweeninterferenceandtransfer.
Inthispaper,weconsidertwocasesfortheupperboundofE[Î“ ],themoderatecaseandtheworstcase.Forthemoderate
t
case, which covers most continual learning scenarios, we assume that the inner product term âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©
It Jt
has the same probabilities of being positive (transfer) and negative (interference). Then, we can approximate
E[âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©] â‰ˆ 0 over all randomness. For the worst case, we assume that all âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© has
It Jt It Jt
negativevalues.
ProofofLemma4.5. Forthemoderatecase,wederivetheroughupperboundofE[Î“ ]:
t
(cid:20)Î²2 L (cid:21)
E[Î“ ]=E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’Î² (1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© (31)
t 2 Jt Ht Ht It Jt
(cid:20)Î²2 L (cid:21)
â‰ˆE Ht âˆ¥âˆ‡g (xt)âˆ¥2 (32)
2 Jt
(cid:18) (cid:20) Î²2L (cid:21)(cid:19)
=O E âˆ¥âˆ‡g (xt)âˆ¥2 (33)
2 Jt
BypluggingLemmaC.2intoE[Î“ ],weobtainthat
t
(cid:18) (cid:20) Î²2L (cid:21)(cid:19)
E[Î“ ]â‰¤O E âˆ¥âˆ‡g (xt)âˆ¥2 (34)
t 2 Jt
(cid:18) (cid:20) Î²2L Î²2L (cid:21)(cid:19)
=O E âˆ¥âˆ‡h| (xt)âˆ¥2+ sup Ï‰2 . (35)
2 MâˆªC 2 h|D
CâŠ‚DâŠ‚MâˆªC
WeusethetechniqueforsummingupintheproofofTheorem1,thenthecumulativesumofcatastrophicforgettingtermis
derivedas
T (cid:88)âˆ’1
E[Î“
]â‰¤T (cid:88)âˆ’1 Î²2L O(cid:18)
E(cid:2) âˆ¥h| (xt)âˆ¥2(cid:3) + sup Ï‰2
(cid:19)
(36)
t 2 MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
t=0 t=0
â‰¤
Î²2LT (cid:88)âˆ’1 O(cid:18)
1 (cid:2) h| (xt)âˆ’h| (xt+1)(cid:3) + LÎ² Ïƒ2 + sup Ï‰2
(cid:19)
(37)
2 Î² MâˆªC MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
t=0
Î²2L (cid:18) 1 TLÎ² (cid:19)
â‰¤ O âˆ† + Ïƒ2 +T sup Ï‰2 (38)
2 Î² h|MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
(cid:18) TLÎ²3 (cid:19)
=O Î²âˆ† + Ïƒ2 +TÎ²2 sup Ï‰2 . (39)
h|MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
Now,weconsidertherandomnessofmemorychoice.LetDâˆ—beasfollows:
TLÎ²3
Dâˆ— = argmax Î²âˆ† + Ïƒ2 . (40)
h|D 2 h|D
CâŠ‚DâŠ‚PâˆªCThen,weobtainthefollowinginequality,
T (cid:88)âˆ’1 (cid:18) TLÎ²3 (cid:19)
E[Î“ ]â‰¤O Î²âˆ† + Ïƒ2 +TÎ²2 sup Ï‰2 (41)
t h|Dâˆ— 2 h|Dâˆ—
CâŠ‚DâŠ‚MâˆªC
h|D
t=0
(cid:18) TLÎ²3 (cid:19)
â‰¤O Î²âˆ† + Ïƒ2 +TÎ²2 sup Ï‰2 . (42)
h|Dâˆ— 2 h|Dâˆ—
CâŠ‚DâŠ‚PâˆªC
h|D
Rearrangingtheaboveequation,weget
T (cid:88)âˆ’1 (cid:18) (cid:18) LÎ²3 (cid:19) (cid:19)
E[Î“ ]â‰¤O T Ïƒ2 +Î²2 sup Ï‰2 +Î²âˆ† . (43)
t 2 h|Dâˆ—
CâŠ‚DâŠ‚PâˆªC
h|D h|Dâˆ—
t=0
Forthemoderatecase,weprovidethederivationsoftheconvergenceratefortwocasesofÎ² asfollows.
âˆš
WhenÎ² <Î±=c/ T,theupperboundalwayssatisfies
T (cid:88)âˆ’1E
âˆš[Î“ t]
â‰¤
âˆš1
O(cid:18)
1
(cid:18)
LÎ²
Ïƒ2 +
âˆš1
sup Ï‰2
(cid:19)
+
âˆš1
âˆ†
(cid:19) <O(cid:18)
1
+
1(cid:19)
.
T T T 2 h|Dâˆ— TCâŠ‚DâŠ‚PâˆªC h|D T h|Dâˆ— T3/2 T
t=0
âˆš
ForÎ² â‰¥Î±=c/ T,wecannotderiveatighterbound,sowestillhave
T (cid:88)âˆ’1E âˆš[Î“ t]
â‰¤
âˆš1 O(cid:18)
T
(cid:18) LÎ²3
Ïƒ2 +Î²2 sup Ï‰2
(cid:19)
+Î²âˆ†
(cid:19) =O(cid:18)âˆš
T +
âˆš1 (cid:19)
.
T T 2 h|Dâˆ— CâŠ‚DâŠ‚PâˆªC h|D h|Dâˆ— T
t=0
Fortheworstcase,weassumethatthereexistsaconstantc whichsatisfiesc âˆ¥âˆ‡g (xt)âˆ¥â‰¥âˆ¥âˆ‡f (xt)âˆ¥.
f,g f,g Jt It
(cid:20)Î²2 L (cid:21)
E[Î“ ]=E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’Î² (1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© (44)
t 2 Jt Ht Ht It Jt
(cid:20)Î²2 L (cid:21)
â‰¤E Ht âˆ¥âˆ‡g (xt)âˆ¥2+Î² (1âˆ’Î± L)âˆ¥âˆ‡f (xt)âˆ¥âˆ¥âˆ‡g (xt)âˆ¥ (45)
2 Jt Ht Ht It Jt
(cid:20) Î²2L (cid:21)
â‰¤E âˆ¥âˆ‡g (xt)âˆ¥2+Î²c âˆ¥âˆ‡g (xt)âˆ¥2 (46)
2 Jt f,g Jt
=O(cid:0)E(cid:2)(cid:0) Î²2+Î²(cid:1)
âˆ¥âˆ‡g
(xt)âˆ¥2(cid:3)(cid:1)
. (47)
Jt
BypluggingLemmaC.2intoE[Î“ ],weobtainthat
t
E[Î“ ]â‰¤O(cid:0)E(cid:2)(cid:0) Î²2+Î²(cid:1) âˆ¥âˆ‡g (xt)âˆ¥2(cid:3)(cid:1) (48)
t Jt
(cid:18) (cid:20) (cid:21)(cid:19)
=O (cid:0) Î²2+Î²(cid:1)E âˆ¥âˆ‡h| (xt)âˆ¥2+ sup Ï‰2 . (49)
MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
WeusethetechniqueforsummingupintheproofofTheorem1,thenthecumulativesumofcatastrophicforgettingtermis
derivedas
Tâˆ’1 Tâˆ’1 (cid:18) (cid:19)
(cid:88) E[Î“ ]â‰¤ (cid:88)(cid:0) Î²2+Î²(cid:1) O E(cid:2) âˆ¥h| (xt)âˆ¥2(cid:3) + sup Ï‰2 (50)
t MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
t=0 t=0
Tâˆ’1 (cid:18) (cid:19)
â‰¤(cid:0) Î²2+Î²(cid:1)(cid:88) O 1 (cid:2) h| (xt)âˆ’h| (xt+1)(cid:3) + LÎ² Ïƒ2 + sup Ï‰2 (51)
Î² MâˆªC MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
t=0
(cid:18) (cid:19)
â‰¤(cid:0) Î²2+Î²(cid:1) O 1 âˆ† + TLÎ² Ïƒ2 +T sup Ï‰2 (52)
Î² h|MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªC
(cid:18) TLÎ²2(Î²+1) (cid:19)
=O (Î²+1)âˆ† + Ïƒ2 +TÎ²(Î²+1) sup Ï‰2 . (53)
h|MâˆªC 2 h|MâˆªC h|D
CâŠ‚DâŠ‚MâˆªCFortheworstcase,weprovidethederivationsoftheconvergenceratefortwocasesofÎ² asfollows.
âˆš
WhenÎ² <Î±=c/ T,theupperboundalwayssatisfies
T (cid:88)âˆ’1E âˆš[Î“ t]
â‰¤
âˆš1 O(cid:32) Lc âˆš+âˆš T
Ïƒ2
+(âˆš
T +c) sup Ï‰2 +
âˆš âˆšT +c
âˆ†
(cid:33) <O(cid:18) 1
+
âˆš1 +1(cid:19)
.
T T T h|Dâˆ— CâŠ‚DâŠ‚PâˆªC h|D T h|Dâˆ— T T
t=0
âˆš
ForÎ² â‰¥Î±=c/ T,wecannotderiveatighterbound,sowestillhave
T (cid:88)âˆ’1E âˆš[Î“ t]
â‰¤
âˆš1 O(cid:18)
T
(cid:18) LÎ²2(Î²+1)
Ïƒ2 +Î²(Î²+1) sup Ï‰2
(cid:19)
+(Î²+1)âˆ†
(cid:19) =O(cid:18)âˆš
T +
âˆš1 (cid:19)
.
T T 2 h|Dâˆ— CâŠ‚DâŠ‚PâˆªC h|D h|Dâˆ— T
t=0
Evenifweconsidertheworstcase,westillhaveO(1)forthecumulativeforgettingE[Î“ ]whenÎ² <Î±.Thisimpliesthat
t
wehavethetheoreticalconditionforcontroltheforgettingonf(x)whileevolvingonC.Inthemaintext,weonlydiscuss
themoderatecasetoemphasizef(x)canbeconvergedbytheeffectoftransferduringcontinuallearning,butwehavealso
consideredtheworstcasecanbewelltreatedbyourtheoreticalconditionbykeepingtheconvergenceoff(x)overtimeas
follows.
ProofofCorollary4.6. ByLemma4.5,wehave
T (cid:88)âˆ’1E
âˆš[Î“ t]
<O(cid:18)
1
+
1(cid:19)
T T3/2 T
t=0
forÎ² <Î±forthemoderatecase.Then,wecanapplytheresultintoRHSoftheinequalityinTheorem4.3asfollows.
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
A 1 (cid:88) Lc
minEâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤ âˆš âˆ† + E[Î“ ] + Ïƒ2
t T c f t 2 f
t=0
A/c(cid:18) Lc2 (cid:19) A/cT (cid:88)âˆ’1
= âˆš âˆ† + Ïƒ2 + âˆš E[Î“ ]
T f 2 f T t
t=0
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 1
=O + + =O âˆš .
T3/2 T T1/2 T
Inaddition,wehavetheconvergencerateoff(x)fortheworstcaseasfollows:
minEâˆ¥âˆ‡f(xt)âˆ¥2 =O(1), (54)
t
whichimpliesthatf(x)cankeeptheconvergencewhileevolvingonC.
ProofofCorollary4.7. ToformulatetheIFOcalls,RecallthatT(Ïµ)
T(Ïµ)=min{T : min Eâˆ¥âˆ‡f(xt)âˆ¥2 â‰¤Ïµ}.
AsingleIFOcallisinvestedincalculatingeachstep,andwenowcomputeIFOcallstoreachanÏµ-accuratesolution.
(cid:32) (cid:32) Tâˆ’1 (cid:33) (cid:33)
A 1 (cid:88) Lc
âˆš âˆ† + E[Î“ ] + Ïƒ2 â†’Ïµ.
T c f t 2 f
t=0
WhenÎ² <Î±,weget
(cid:18) (cid:19)
1
IFOcalls=O .
Ïµ2
Otherwise,whenÎ² â‰¥Î±,wecannotguaranteetheupperboundofstationarydecreasesovertime.Then,wecannotcompute
IFOcallsforthiscase.D DERIVATIONOFEQUATIONSINADAPTIVEMETHODSINCONTINUALLEARNING
DerivationforA-GEM Letthesurrogateâˆ‡gËœ (xt)as
Jt
(cid:28) âˆ‡f (xt) (cid:29) âˆ‡f (xt)
âˆ‡gËœ (xt)=âˆ‡g (xt)âˆ’ It ,âˆ‡g (xt) It , (55)
Jt Jt âˆ¥âˆ‡f (xt)âˆ¥ Jt âˆ¥âˆ‡f (xt)âˆ¥
It It
whereÎ± Ht =Î±(1âˆ’ âŸ¨âˆ‡fI âˆ¥t âˆ‡(x ft I) t, (âˆ‡ xtg )J âˆ¥t 2(xt)âŸ© )andÎ² Ht =Î±forEquation3.
Then,wehave
(cid:20)Î²2 L (cid:21)
E[Î“ ]=E Ht âˆ¥âˆ‡gËœ (xt)âˆ¥2âˆ’Î² âŸ¨âˆ‡f (xt),âˆ‡gËœ (xt)âŸ©
t 2 Jt Ht It Jt
(cid:20)Î²2 L(cid:18) âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©2 âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©2(cid:19) (cid:21)
=E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’2 It Jt + It Jt âˆ’Î² âŸ¨âˆ‡f (xt),âˆ‡gËœ (xt)âŸ©
2 Jt âˆ¥âˆ‡f (xt)âˆ¥2 âˆ¥âˆ‡f (xt)âˆ¥2 Ht It Jt
It It
=E(cid:20)Î² H2 tL(cid:18) âˆ¥âˆ‡g (xt)âˆ¥2âˆ’ âŸ¨âˆ‡f It(xt),âˆ‡g Jt(xt)âŸ©2(cid:19) âˆ’Î² (cid:0) âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©âˆ’âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©(cid:1)(cid:21)
2 Jt âˆ¥âˆ‡f (xt)âˆ¥2 Ht It Jt It Jt
It
(cid:20)Î²2 L(cid:18) âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©2(cid:19)(cid:21)
=E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’ It Jt . (56)
2 Jt âˆ¥âˆ‡f (xt)âˆ¥2
It
Now,wecomparethecatastrophicforgettingtermbetweentheoriginalvaluewithâˆ‡g (xt)andtheabovesurrogate.
Jt
(cid:20)Î²2 L(cid:18) âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©2(cid:19)(cid:21) (cid:20)Î²2 L (cid:21)
E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’ It Jt <E Ht âˆ¥âˆ‡g (xt)âˆ¥2âˆ’Î² âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© .
2 Jt âˆ¥âˆ‡f (xt)âˆ¥2 2 Jt Ht It Jt
It
Then,wecanconcludethatE[Î“ ]withthesurrogateofA-GEMissmallerthantheoriginalE[Î“ ].
t t
DerivationofoptimalÎ“âˆ—andÎ²âˆ— ForafixedlearningrateÎ±,wehave
t Ht
âˆ‚E[Î“ ] (cid:20) âˆ‚Î“ (cid:21)
0= t =E t
âˆ‚Î² âˆ‚Î²
Ht Ht
=E(cid:2) Î² Lâˆ¥âˆ‡g (xt)âˆ¥âˆ’(1âˆ’Î±L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ©(cid:3) .
Ht Jt It Jt
Thus,weobtain
(1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© (1âˆ’Î± L)Î›
Î²âˆ— = Ht It Jt = Ht Ht,
Ht Lâˆ¥âˆ‡g (xt)âˆ¥2 Lâˆ¥âˆ‡g (xt)âˆ¥2
Jt Jt
(1âˆ’Î± L)âŸ¨âˆ‡f (xt),âˆ‡g (xt)âŸ© (1âˆ’Î± L)Î›
Î“âˆ— =âˆ’ Ht It Jt =âˆ’ Ht Ht.
t 2Lâˆ¥âˆ‡g (xt)âˆ¥2 2Lâˆ¥âˆ‡g (xt)âˆ¥2
Jt Jt
E OVERFITTINGTOREPLAYMEMORY
InLemma4.2,weshowtheexpectationofstepwisechangeofupperbound.Now,wediscussthedistributionoftheupper
boundbyanalyzingtherandomvariableB .AsB iscomputedbygetting
t t
B =(LÎ±2 âˆ’Î± )âŸ¨âˆ‡f(xt),e âŸ©+Î² âŸ¨âˆ‡g (xt),e âŸ©.
t Ht Ht t Ht Jt t
ThepurposeofourconvergenceanalysisistocomputetheupperboundofEquation7,thenwecomputetheupperboundof
B .
t
B â‰¤(LÎ±2 âˆ’Î± )âˆ¥âˆ‡f(xt)âˆ¥âˆ¥e âˆ¥+Î² âˆ¥âˆ‡g (xt)âˆ¥âˆ¥e âˆ¥.
t Ht Ht t Ht Jt t
Itisnotedthattheupperboundisrelatedtothedistributionofthenormofe .WehavealreadyknowthatE[e ]=0,sowe
t t
consideritsvariance,Var(âˆ¥e âˆ¥)inthissection.LetusdenotethenumberofdatapointsofP inamemoryM asm .We
t 0 P
assumethatM isuniformlysampledfromP.Thenthesamplevariance,Var(âˆ¥e âˆ¥)iscomputedas
0 t
n âˆ’m
Var(âˆ¥e âˆ¥)= f P Ïƒ2
t (n âˆ’1)m f
f P
bythesimilarderivationwithEquation25.TheaboveresultdirectlycanbeappliedtothevarianceofB .Thisimplies
t
m isakeyfeaturewhichhasaneffectontheconvergencerate.Itisnotedthatthelargerm hasthesmallervariance
t P
byapplyingschemes,suchaslargermemory.Inaddition,thedistributionsofe andâˆ‡f (xt)aredifferentwithvarious
t It
memoryschemes.Therefore,wecanobservethatmemoryschemesdiffertheperformanceevenifweapplysamestepsizes.