Evaluating Mathematical Reasoning Beyond Accuracy
ShijieXia1,2,5,XuefengLi1,2,5,YixinLiu3,TongshuangWu4,PengfeiLiu1,2,5
âˆ— âˆ—
1ShanghaiJiaoTongUniversity,2ShanghaiArtificialIntelligenceLaboratory
3YaleUniversity,4CarnegieMellonUniversity,5GenerativeAIResearchLab(GAIR)
Abstract
TheleaderboardofLargeLanguageModels(LLMs)inmathematicaltaskshas
beencontinuouslyupdated. However,themajorityofevaluationsfocussolelyon
thefinalresults,neglectingthequalityoftheintermediatesteps. Thisoversightcan
maskunderlyingproblems,suchaslogicalerrorsorunnecessarystepsintherea-
soningprocess. Tomeasurereasoningbeyondfinal-answeraccuracy,weintroduce
REASONEVAL,anewmethodologyforevaluatingthequalityofreasoningsteps.
REASONEVAL employs validity and redundancy to characterize the reasoning
quality,aswellasaccompanyingLLMstoassessthemautomatically. Instantiated
by base models that possess strong mathematical knowledge and trained with
high-qualitylabeleddata,REASONEVALachievesstate-of-the-artperformanceon
human-labeleddatasetsandcanaccuratelydetectdifferenttypesoferrorsgenerated
byperturbation. WhenappliedtoevaluateLLMsspecializedinmath,wefindthat
anincreaseinfinal-answeraccuracydoesnotnecessarilyguaranteeanimprove-
mentintheoverallqualityofthereasoningstepsforchallengingmathematical
problems. Additionally,weobservethatREASONEVALcanplayasignificantrole
indataselection. Wereleasethebest-performingmodel,meta-evaluationscript,
andallevaluationresultsathttps://github.com/GAIR-NLP/ReasonEval.
1 Introduction
Mathematicalreasoning,acorecognitiveskill,iscrucialforresolvingcomplexproblemsandmaking
informeddecisions(Hendrycksetal.,2021;Lewkowyczetal.,2022),playingasignificantroleinlarge
languagemodels(LLMs)research(Azerbayevetal.,2023;Luetal.,2023). Givenitssignificance,
reliablyevaluatingmathematicalreasoninginLLMsbecomescrucial. Currentmethodologiesto
evaluatemathematicalreasoninginLLMsfocusprimarilyonthefinalresult(Luoetal.,2023;Chern
etal.,2023;Yuetal.,2023),neglectingtheintricaciesofthereasoningprocess. Forexample,the
OpenLLMleaderboard,1arelativelywell-recognizedbenchmarkforLLMs,usesoverallaccuracy
toassessmodelsâ€™mathematicalreasoning. Despitebeingeffectivetosomedegree,suchevaluation
practicecouldmaskunderlyingissuessuchaslogicalerrorsorunnecessarystepsthatcompromise
accuracyandefficiencyofreasoningsteps. Inthiswork,wearguethatadesirableevaluationcriterion
for mathematical reasoning encompasses not only the accuracy of the final answer but also the
correctnessandefficiencyofeachstepinthereasoningprocess. Moreover,itisimperativethatthe
evaluationmetricsbeopen-sourceandreplicabletoensuretransparencyandreliability.
Ourdesignphilosophystemsfromthefactthatacorrectfinalanswerdoesnotguaranteeaflawless
reasoningprocess(Lewkowyczetal.,2022;Uesatoetal.,2022),andexcessiveorirrelevantreasoning
stepscanleadtopotentialerrorsaswellasincreasedcomputationalcosts(Zhangetal.,2023). Once
theseissuesgounnoticed,theycancauseproblemsinmanyapplicationscenarios. Forexample,in
K12mathematicseducation,incorrectorredundantsolutionstepsprovidedbyLLMscouldmislead
students. Therehavebeensomerecentworksrelatedtotheaboveevaluationprinciple. Specifically,
Uesatoetal.(2022);Lightmanetal.(2023);Wangetal.(2023)presentprocessrewardmodelsfor
mathematicalreasoning,2 whichfocusontheirutilityasverifiers(Cobbeetal.,2021)toboostthe
accuracy(i.e.,bygeneratingmanycandidatesolutionsandselectingtheonerankedhighestbythe
CorrespondingAuthors.
âˆ—
1https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
2Uesatoetal.(2022)andLightmanetal.(2023)areclose-source.
1
4202
rpA
8
]LC.sc[
1v29650.4042:viXraQuestion: Simplify 242 Ground Truth: 11 2
Solution ReasonEval
ð‘† ð‘†
ð‘£ð‘Žð‘™ð‘–ð‘‘ð‘–ð‘¡ð‘¦ ð‘Ÿð‘’ð‘‘ð‘¢ð‘›ð‘‘ð‘Žð‘›ð‘ð‘¦
Step 1: We can simplify the square root of 242 by factoring it into its prime factors. 0.85 0.05
Step 2: The prime factorization of 242 is 2âˆ—11âˆ—11. 0.85 0.05
Step 3: Letâ€™ check it. The prime factorization of 242 is 2âˆ—11âˆ—11. 0.75 0.25
Step 4: By the property of square roots, we know that ð‘Žð‘= ð‘Žâˆ— ð‘ for any non-
negative numbers ð‘Ž and ð‘. Applying this property to 2âˆ—11âˆ—11, we have 2âˆ— 112. 0.95 0.05
Step 5: By the property of square roots, we know that ð‘Ž2=ð‘Ž for any non-negative
number a. Applying this property to 112, we have 11. 0.80 0.05
Step 6: By combining the results from step 4 and step 5, we have 2âˆ—11âˆ—11=11âˆ— 2. 0.15 0.05
Step 7: Therefore, the simplified form of 242 is 11 2 . 0.75 0.05
redundant invalid
Answer: 11 2
Figure1: Givenasolutiontoamathproblem, REASONEVAL scoreseachstepandidentifiesthe
potentialerrorlocation,servingasanextensiontoverifythefinalansweronly.
verifier),leavingitsunderlyingpotentialtoidentifyreasoningerrors3andredundancy.Clinciuetal.
(2021);Golovnevaetal.(2022)relyonembedding-basedmethodstomeasurethequalityofgeneral
reasoningexplanation,whichhaslimitationsinhandlingdiversereasoningstepsfromvariousmodels.
Inresponsetothesechallenges, wepropose REASONEVAL, asuitecomprisinganewevaluation
methodologywithdefinedmetricsforassessingmathematicalreasoningqualityandcorresponding
LLM-basedevaluatorsforautomatedcalculation. AsillustratedinFigure1,REASONEVALempha-
sizesthevalidity(i.e.,thestepcontainsnomistakesincalculationandlogic)ofeachreasoningstep,
and evaluates the redundancy (i.e., the step lacks utility in solving the problem but is still valid)
of steps to ensure efficiency. We explore different design options for the LLM-based evaluators
andshowtrainingsuchahigh-qualityevaluatorformathematicalreasoningisnotastraightforward
task,whichcallsforanLLMwithstrongmathematicalknowledgeaswellasspecializeddatasets
annotatedinaccordancewithourevaluationcriteria. Withtheseprepared,REASONEVALachieves
SOTAperformanceonhuman-labeleddatasets(Â§4.1)andcanaccuratelydetectdifferenttypesof
errorsgeneratedbyperturbation(Â§4.2).
Furthermore,weshowtheutilityofREASONEVALthroughthelensoftwopreliminaryapplications,
asdetailedinÂ§5: (1)evaluatingdifferentLLMstrainedformathematicalreasoning;and(2)selecting
high-qualitydata totrainsuch LLMs. Wehavethe followingmain findings: (1) Wefind thatan
improvementintheresultaccuracyisnotsufficienttoensureanenhancementintheoverallquality
ofreasoningstepsinchallengingmathematicalproblems;(2)Wefindthatthemodelscale,thebase
model,andthetrainingmethodshavesignificantlyinfluencedthequalityofreasoningsteps;(3)We
findthat REASONEVAL canselecthigh-qualitytrainingdatatoimprovetheefficiencyofsolving
problemsandthequalityofsolutions. Thesefindingspavethewayforfutureworktodesignmethods
thattakeintoaccounttheprocessofproblem-solving.
Overall,ourmaincontributionsareasfollows:
(1) We recognize the potential gap between what we are evaluating and what we are desiring in
mathematical reasoning, prioritize validity and redundancy aspect to address the misalignments
andinefficienciesthatcanariseinmathematicalreasoningprocesses(Â§3). (2)Wedesignameta-
3Identifyingreasoningerrorsandrankingreasoningstepsaredifferenttasks.AsalsohighlightedbyTyen
etal.(2023),evenstate-of-the-art(SOTA)modelslikeGPT-4(OpenAI,2023)stillstruggletodetecterrorsin
reasoningtasks.
2evaluationtestbedtoassessthereliability(Â§4)andutility(Â§5)ofmentionedmetrics,guidingustoa
superiormetricdesign,solidifyingthefoundationforfutureexplorationandsignificantlylowering
trialanderrorcosts. (3)Weopen-sourceourbest-performingmetric,meta-evaluationscriptandall
evaluationresultstofacilitatefutureresearch.
2 Preliminaries
2.1 ProblemFormulation
Given a mathematical problem q, solution steps hË† = hË† ,...,hË† and an answer aË† generated by
1 N
{ }
LLMs,thegoalistoevaluatehowwellthegeneratedsolutionandanswerare. Usually,theground
truthansweraisavailablebutthereferencesolutionsteph = h ,...,h isnotalwaysprovided.
1 M
{ }
2.2 ExistingEvaluationMethodology
Answer-basedMatching Mostoftheexistingworks(Luoetal.,2023;Chernetal.,2023;Yuetal.,
2023)measurethemathematicalreasoningqualitybydirectlycomparingthefinalanswer(i.e.,aË†and
a)andcalculatingtheoverallaccuracyonagivendataset.
Reference-based Scoring Instead of only using the final result as a scoring criterion, some
works(Sawadaetal.,2023)trytomeasurethereasoningqualitybycomparingthesimilaritybetween
generatedandreferencesolutionsteps(i.e.,hË† andh). AlthoughdatasetslikeMATH(Hendrycksetal.,
2021)andGSM8K(Cobbeetal.,2021)providethegroundtruthsolutions,theexistenceofdiverse
reasoningpathsleadingtothesameanswerarendersrelianceonanysingleoneofthemasareference
unreliable.
Prompting-basedMethod ThismethoddirectlyasksLLMswithawell-definedprompttogen-
erateajudgmentforagivengeneratedsolutionandanswer(Duboisetal.,2023). Thisapproach
usuallyrequiresaverypowerfulLLM,oftenGPT-4,whichmaynotbepracticalforiterativemodel
developmentduetocostandtransparencyconcerns.
Theaforementionedmethodseitherfocussolelyonthefinalresults(e.g., a)orarelimitedbythe
useofreferencesolutionsandproprietaryLLMs,andmostofthemonlyconcentrateonevaluating
â€œcorrectnessâ€,neglectingotheraspectssuchastheredundancyofsolutionsteps. Thishasinspiredus
toproposeasetofevaluationcriteriathatbetteralignswiththereasoningmodelsintheeraofLLMs.
3 REASONEVAL: MetricsandImplementations
3.1 DesignPrinciple
Wearguethatfortasksinvolvingmulti-stepreasoning,measuringthequalityofreasoningshouldnot
solelydependonthecorrectnessofthefinalresult. Itshouldalsoconsider(1)theaccuracyofeach
reasoningstep;(2)theefficiencyofthereasoningprocess. Tothisend,wedesignevaluatorsthat
assessreasoningstepsregardingvalidityandredundancyinastep-by-stepformat,checkingwhether
eachstepadvanceswelltowardssolvingtheproblem. Precisely,validitydenotesthestepcontains
nomistakesincalculationandlogic,andredundancydescribesthesteplacksutilityinsolvingthe
problembutisstillvalid.
3.1.1 TaskFormulation
Followingthisprinciple,weformulatesuchametricdesignprocessasathree-wayclassificationtask.
GivenaquestionqandthesolutionstepshË† = hË† ,...,hË† ,alabelc {positive,neutral,negative}
1 N
{ } âˆˆ
willbeassignedwiththefollowingprobability:
p
c
= REASONEVAL(c q,hË† 1,...,hË† N,Î¸) (1)
|
p = p1,...,pN (2)
c { c c }
3wherethepositivelabelindicatesthestepiscorrectandcontributestosolvingthequestion,whilethe
neutrallabelrepresentsthatthestepisalsocorrectbutdoesnotmakeanyprogress. Thenegative
labelsignifiesanincorrectstep.
3.1.2 Scoringscheme
Wecandefinestep-levelscoresforeachstep:
S = p +p (3)
validity positive neutral
S = p (4)
redundancy neutral
AssumingwehaveasetofscoresforasolutioncontainingNsteps,Wecandefinesolution-level
scores:
Sall =min(S1 ,...,SN ) (5)
validity validity validity
Sall =max(S1 ,...,SN ) (6)
redundancy redundancy redundancy
3.2 EvaluatorDesign
3.2.1 ModelArchitecture
LLMBackbone Ourdefinedevaluationmethod(Eq.1)canbeimplementedbydirectlyprompting
existingLLMsorfine-tuningLLMsusingsuperviseddataset. Tomakeacomprehensivestudy,in
thisworkweinstantiateREASONEVALbydifferenttypesofLLMs,whichvaryinbasemodeltypes
(e.g.,Llama2(Touvronetal.,2023)andMistral(Jiangetal.,2023)),modelsizes(e.g.,7B,34B),and
post-trainingstrategies(e.g.,continuedpretraining(Azerbayevetal.,2023)andfine-tuning).
Task-specificLayer Ourevaluatorâ€™sarchitectureisidenticaltothebasemodel,exceptthatthe
classificationheadfornext-tokenpredictionisreplacedwithaclassificationheadforoutputtingthe
possibilitiesofeachclass. Afternormalizationwiththesoftmaxlayer,wegetthepossibilityforeach
class.
3.2.2 Fine-tuningData
Theabovetaskformulationallowsustoutilizetheexistingdataset,PRM800K(Lightmanetal.,2023),
astrainingdatain REASONEVAL. PRM800Kcontainsabout800,000step-levellabelsover75,000
solutions. Itiscollectedbyemployinghumanstolabelthestep-by-stepsolutionsgeneratedbyGPT-4
toMATHproblems. ThelabelsforeachstepareidenticaltothosementionedinÂ§3.1.1.
4 Experiment
4.1 Meta-EvaluationsonHumanJudgementDatasets
4.1.1 Meta-EvaluationSetup
Meta-evaluation Datasets Inspired by Zeng et al. (2024), which propose a benchmark named
Meta-Reasoning-GSM8K (MR-GSM8K), we construct a meta-evaluation dataset MR-MATH to better
assesshowwelldifferentevaluatorscandetecterrorsinreasoningsteps. Itisconstructedasfollows:
(1)Tocollectthefirsttypeoferrorsaffectingthecorrectnessofsteps,werecruitundergraduates
whohaveasolidmathematicalbackgroundtolabelsolutionsgeneratedbyAbel(Chernetal.,2023)
and WizardMath (Luo et al., 2023). We collect 83 samples with incorrect steps and 76 without,
pinpointingthefirsterrorlocationintheformer. Allthesolutionsreachthecorrectfinalanswers,
meaning the evaluators need to judge correctness based on the process rather than the outcome.
Additionally,sincethestyleofsolutionsdiffersfromthetrainingsetofPRM800K(Seeexamplesof
solutionsinAppendixA.1),itteststhegeneralizationofREASONEVALtosomedegree. (2)Forthe
secondtypeoferrorsaffectingtheefficiencyofproblemsolvingprocess,astheyaremorerarerthan
thefirstone,wesamplesolutionsfromthetestsetofPRM800Kdirectly,containing150sampleswith
redundantstepsand150sampleswithout.
4Invalid Redundant
Solution-level Step-level Solution-level Step-level
F1Score AUC F1Score AUC F1Score AUC F1Score AUC
Embedding-basedMethods
ROSCOE-SA 48.2 57.5 - - 50.7 53.9 - -
ROSCOE-SS 51.6 49.6 - - 52.0 52.7 - -
Prompting-basedMethods
GPT-3.5-turbo 59.7 - 53.2 - 53.0 - 51.5 -
GPT-4 73.2 - 61.0 - 57.1 - 54.2 -
Step-levelEvaluators
Math-shepherd-Mistral-7b 70.1 77.3 60.0 77.2 50.4 54.5 42.7 53.0
REASONEVALLlama2-7B 66.7 79.5 60.8 80.0 60.4 62.8 59.0 68.6
REASONEVALWizardMath-7B-V1.0 72.8 81.9 67.7 83.9 60.5 65.6 59.0 68.3
REASONEVALMistral-7B 78.0 85.1 68.6 85.7 60.7 63.4 59.7 70.9
REASONEVALLlemma-7B 74.7 84.3 76.6 90.5 59.6 63.0 58.6 68.3
REASONEVALAbel-7B-002 77.3 86.2 70.4 90.5 58.6 63.6 59.5 71.8
REASONEVALWizardMath-7B-V1.1 78.6 87.5 73.9 89.5 61.6 64.8 59.7 72.2
REASONEVALLlemma-34B 79.6 90.8 77.5 92.8 58.3 62.7 57.5 67.3
Table1: ComparisonofmethodsforautomaticevaluationofreasoningstepsinMR-MATH.Forany
methodsthatrequiresettingathreshold,wereporttheAreaUndertheCurve(AUC)metric.
Evaluators Wecomparethreemethodstoevaluatereasoningstepsautomatically:embedding-based
methods,prompting-basedmethodsandstep-levelevaluators. Fortheembedding-basedmethods,we
chooseROSCOE (Golovnevaetal.,2022),aSOTAmethodamongthem. Specifically,weselectthe
semanticalignmentmetrics(ROSCOE-SA)andthesemanticsimilaritymetrics(ROSCOE-SS),which
donotrequirereferencessolutionsteps. Fortheprompting-basedmethods,weselectGPT-3.5-turbo
andGPT-4,twomainstreamgenerationmodels. WeusethepromptsuggestedbyZengetal.(2024)
fortheinvaliderrorsandthemodifiedversionfortheredundanterrors. Forthestep-levelevaluators,
weselect7basemodelsforREASONEVAL: Llama-2-7B,Mistral-7B,Llemma-7B,Llemma-34B,
WizardMath-7B-V1.0(Luoetal.,2023),WizardMath-7B-V1.1,Abel-7B-002(Chernetal.,2023).
Thellemmaseriesiscontinuingpertainingonmath-basedcorpus. TheAbelandWizardMathseries
arefine-tuningforsolvingmathematicalproblems. Wealsoselecttheopen-sourceSOTAprocess
reward model Math-shepherd-Mistral-7B (Wang et al., 2023) to compare. The settings for these
methodsareinAppendixA.3andthetrainingdetailsforREASONEVALareinAppendixA.2.
Meta-evaluationMetrics Wetesttwoabilities: judgingwhetherthesolutioncontainserrorsand
locatingtheerrorstep. Inthefirsttaskthegroundtruthislabelsforsolutions(solution-level),andin
thesecondtasksthegroundtruthislabelsforsteps(step-level). Sincebothareclassificationtasks,
wepresentthemacroF1scoreasaperformancemetric. Additionally,foranymethodsthatrequire
settingathreshold,wereporttheAreaUndertheCurve(AUC)metric,whichisamorebalanced
evaluationofperformanceacrossdifferentthresholdsettings.
4.1.2 ResultsandAnalysis
OverallResults WepresentourmainresultsinTable1. REASONEVALcanoutperformallother
methodsacrossallerrortypesandlabellevels. Itisnoteworthythattheidentificationoferrorsat
thesteplevelisgenerallymorechallengingthanatthesolutionlevelforallmethods. Thissuggests
a higher complexity in pinpointing errors at the individual step level. Furthermore, identifying
redundanterrorsismoreintricatethaninvaliderrorsduetotheinherentambiguityinvolvedinthe
former.
AnalysisonBaseModels WeinvestigatethecorrelationbetweenthebaseLLMsâ€™mathematical
reasoningcapabilitiesandtheperformanceofREASONEVALfine-tunedfromthem. InFigure2,for
theinvaliderrors,anincreaseinAUCshowsapositivecorrelationwiththebasemodelsâ€™abilityto
solveMATHproblems,indicatingthatenhancingmathematicalproblem-solvingabilitiesisbeneficial
foridentifyinginvaliderrors. ItisnotedthattheLlemma-34Boutperformsall7Bmodels,althoughits
5Invalid
90 model size ReasonEvalLlemma-34B Invalid
7B solution-level step-level
88 34B ReasonEvalWizardMath-7B-V1.1 OOD F1Score AUC F1Score AUC
86 ReasonEvalMistral-7B
ReasonEvalAbel-7B-002
E ROm Sb Ced Od Ein -g S- AbasedMethods
âœ“ 52.5 55.3 - -
84 ReasonEvalLlemma-7B ROSCOE-SS âœ“ 52.6 55.5 - -
82 ReasonEvalWizardMath-7B-V1.0 Prompting-basedMethods
GPT-3.5-turbo - 55.1 - 51.8 -
80 ReasonEvalLlama2-7B GPT-4 - 78.3 - 59.1 -
5 10 15 20 25 30 Step-levelEvaluators
Redundant Math-shepherd-Mistral-7b âœ— 81.4 90.9 65.5 82.9
6666 4455 .... 0505 R m 7 3e B 4oa Bds eo ln sE iv zealWizardMath-7B-V1.0 Reaso RnE eava solW ni Eza vrd aM lAat bh e- l7 -7B B-V -01 0.1
2
R R R R R R RE E E E E E EA A A A A A AS S S S S S SO O O O O O ON N N N N N NE E E E E E EV V V V V V VA A A A A A AL L L L L L LL W M L A W Ll l lba i e ei isz zem m mta alr -r ra m ma 7d d2 l B- a aM M- 7 - -7 -7 3B 0a aB B 4t t0h h B2- -7 7B B- -V V1 1. .0 1 âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 5 5 6 6 7 7 71 8 1 7 0 4 7. . . . . . .2 5 2 2 2 6 7 6 6 7 7 8 8 85 2 5 7 3 7 5. . . . . . .6 2 6 1 7 5 5 5 5 6 6 6 775 3 1 5 7 01. . . . . ..4 2 9 9 7 81 87 7 8 8 8 894 3 4 2 7 5.. . . . . .87 2 9 8 4 3
63.5
63.0
ReasonEvalLlamR a2e -7a BsonEvalMRiestarasl-o7BnEvalL
Rle em am sa o-7 nB EvalLlemma-34B
T ma ab til ce e2 v: aluC ato iom np oa fr ris eo an sono if ngm ste et ph sod ins Mf Ro -r GSa Mu 8to K-
.
5 10 15 20 25 30 â€œOODâ€representsthatitstrainingdatacontains
Pass@1 For Its Corresponding Base Model on MATH
thelabeledsolutionsforproblemsofthedataset.
Figure2: CorrelationbetweenPass@1forthe The results of prompting-based methods are
basemodelonMATHandAUCforthesolution- fromZengetal.(2024).
levellabels.
Pass@1isnotthehighest. Ithighlightstheimportanceofmodelscaleandthecontinuedpretraining
overmath-relatedcorpus. Fortheredundanterrors,thedistinctionacrossdifferentbasemodelsis
small, and it does not show the correlation as the invalid errors. This may be due to developers
forthesemodelsprioritizingthecorrectnessofsolutionsovertheirefficiency,resultinginsimilar
performancesinthisaspectacrossvariousLLMs.
Analysis on Training Data In Table 1, the Mistral-
7B trained on Math-Shepherd falls behind that trained
Problem Labeled
on PRM800K. We summarize the information about Dataset #Solutions Labels Source Method
Math-shepherd and PRM800K in Table 3. For Math-shepherd âˆ¼440K c ino cr ore rc ret, ct M GSA MTH 8K, unsupervised
Math-shepherd,althoughhaving6xtrainingdata,there positive,
PRM800K 75K neutral, MATH human
ismorenoiseinthestep-levellabelsgeneratedwiththe âˆ¼ negative
unsupervisedmethod,thusharmingitsprecisioninidenti-
Table 3: Comparison between
fyingerrors. Andtheonlytwoclassesoflabelsalsomake
Math-shepherdandPRM800K.
itlimitedinevaluatingredundancy.
GeneralizationAnalysis Toassesstheout-of-distributiongeneralizationofREASONEVAL,we
evaluateitsperformanceonMR-GSM8K,whichconsistsofproblemsfromGSM8K.Wepresenttheresults
in Table 2. We observe that both REASONEVALLlemma-34B and REASONEVALWizardMath-7B-V1.1
achievesuperiorperformanceatthesteplevelandapproachtheperformanceofGPT-4atthesolution
level,demonstratingstronggeneralizationcapabilities. ForMath-shepherd-Mistral-7B,itperforms
bestinsolution-levellabelsbutlagsbehindinstep-levellabels,suggestingthatthenoiseinsteplabels
fromMath-Shepherdnegativelyimpactsitsabilitytoaccuratelylocateerrorsteps.
4.2 Meta-EvaluationsbyPerturbation
TofurtheranalyzetheeffectofdifferenttypesoferrorsonREASONEVAL,wefollowGolovnevaetal.
(2022)andintroduceerrorsinthecorrectreasoningsteps.
Setup We sample 184 solutions without any invalid or redundant errors from the test set of
PRM800K.Thesesolutionscovervariousdifficultylevelsandsubjects. Weintroduce6typesof
errors to one of the intermediate steps. In Table 4 we summarize the formats of the errors. All
solutionsarescoredbyREASONEVALLLemma-34BandMath-shepherd-Mistral-7B.
6
CUAPerturbationType ReferenceSteps HypothesisSteps ReasonEvalLlemma-34B Math-shepheard-Mistral-7B
Repeatastep [r1,r2,r3] [r1,r2,r2,r3] 1.0 O Per rig tuin ra bl ed 1.0 O Per rig tuin ra bl ed
Removeastep [r1,r2,r3] [r2,r3] 0.5 0.8
Swapastep [r1,r2,r3] [r1,r3,r2] 0.0 0.6
Swapasolution [ra,ra,ra] [rb,rb,rb]
Randomoperation [r11 ,r22 ,r33 ] [r1,1 g(r2 2),3 r3] 0.4 O Per rig tuin ra bl ed 0.4
Randomnumber [r1,r2,r3] [r1,h(r2),r3] 0.2 0.2
0.0 0.0
Table4: Typesoferrorsintroduced. r denotes Random number Swap steps Repeat step
i
theith stepinthereferencesolution. ra andrb Random operation Remove step Swap solutions
denotethereferencesolutionsfromproblem a Figure3: Box-and-whiskerplotsofscoreswhen
andproblembrespectively. g(.)denotesthatthe meetingdifferenttypesoferrorsatthesteplevel.
operatorinastepischanged. h(.)denotesthat Theboundariesofthewhiskersarebasedonthe
thenumberinastepischanged. 1.5interquartilerange.
Results Figure 3 presents the results. We observe the scores for the first error step (step-level
scores). Weobservethat: (1)Thevalidityscoreexhibitsasignificantdecreasewhiletheredundancy
scoreremainsunchangedwhenmeetingthelogicalandcalculationerrors(randomnumber,random
operation,swapsteps,removestep). (2)Fortherepeatedstep,thevalidityscoreremainshighand
theredundancyscoreincreasessignificantly,consistentwiththefactthattherepetitionintroduces
redundancy to the overall solution without impacting its validity. (3) In the case of swapping
solutions,wherethesolutioniscorrectbutnotforthegivenproblem,thevalidityscoredecreasesand
theredundancyscoreincreases. ItindicatesREASONEVALtreatstheproblemandthesolutionasa
whole,ratherthanassessingthesolutioninisolation. (4)ForMath-shepherd-Mistral-7B,itshows
insensitivitytotheerrorscausingredundancylikerepetitionstep. Inlightoftheseobservations,we
concludethatREASONEVALcandetectdifferenttypesoferrors,effectivelydistinguishingbetween
thosethataffectthesolutionâ€™svalidityandthosethatintroduceredundancy. Wepresentotheranalysis
andtheresultsofsolution-levelscoresinAppendixA.4.
5 Utilizing REASONEVAL forEvaluatingandImprovingReasoningQuality
5.1 EvaluatingReasoningQualityofLLMsSpecializedinMath
Setup The models selected for measurement are two mainstream LLMs specialized in math:
Abel(Chernetal.,2023)andWizardMath(Luoetal.,2023),withdifferentscales. Wealsoreport
theresultsofLLaMA-2(Touvronetal.,2023)naivefine-tunedonPRM800K(approximately6K
solutionsthatreachthecorrectfinalanswers)forcomparison. Weevaluatetheperformanceofthe
LLMsonMATHandsample100solutionsforeachproblemtoreduceevaluationnoiseandsampling
randomness.Specifically,thesamplingtemperatureissetto0.6,andthetop-pvalueissetto0.95.The
maximumsamplelengthissetto2048tokens. AllsolutionsarescoredbyREASONEVALLLemma-34B.
Ouranalysisincludestwoaspects: (1)Falsepositiverate: itreferstotheproportionofsolutions
thathavethecorrectfinalanswersbutcontainincorrectstepsamongallsolutionswithcorrectfinal
answers. (2)Theredundancyofsolutions. Wealsoanalyzethefirsterrorlocationinreasoningsteps
andpresenttheresultsandanalysisinAppendixB.1.
5.1.1 FalsePositiveRate
First, we compare the validity scores of reasoning steps from different models in Figure 5. For
solutionswiththecorrectanswersbutwrongsteps,thevalidityscoresshouldbehavesimilarlyto
solutionswiththeincorrectanswers. Sowecanuseittoestimatethethresholdandcalculatethefalse
positiverate. Aroughestimationforthethresholdisbetween0.15and0.35. Tofurtherensurea
relativelyaccurateestimation,wecheckthefalsepositiverateofAbel13BandWizardMath13Bby
humans,samplingonesolutionforeachproblem. Combiningaboveinformation,thethresholdisset
to0.25. WepresentthemainresultsinTable5andFigure4.
Increasedfinal-answeraccuracydoesnotinherentlyensureacorrespondingreductioninthe
falsepositiverate. InFigure4,itisclearthatthefalsepositiveratedropsandstaysatalevelofabout
20%asthefinal-answeraccuracyrises. WhencomparingWizardMath7B-V1.1withWizardMath70B,
7
ytidilavS
ycnadnuderSModel BaseModel Methods Acc. (%) FPR(%)
LLaMA2-13B-PRM800K 7.4 40.6
LLaMA2 SFT
LLaMA2-70B-PRM800K 14.7 21.8
Abel7B-001 12.4 31.8
Abel13B LLaMA2 15.2 29.8(g29.2)
SFT
Abel70B 25.7 20.0
Abel7B-002 Mistral 30.0 22.8
WizardMath7B-V1.0 8.9 34.4
WizardMath13B LLaMA2 10.9 31.3(g28.3)
SFT+RLHF
WizardMath70B 18.7 16.5
WizardMath7B-V1.1 Mistral 31.0 16.7
Table5: Weestimatethefalsepositiverate(FPR)withREASONEVALLLemma-34B. Wealsocheck
thefalsepositiverateofAbel13BandWizardMath13Bbyhuman(denotedbyg.),samplingone
solutionforeachproblem. â€œAcc.â€ representstheoverallaccuracy.
1.0 With correct results
0.8 With incorrect results
0.45 Model size 0.6
7B 0.4
0.40 LLaMA13B-PRM800K 1 73 0B B 0.2
T Sr Fa Tining strategy 0.0
0.35 WizardMath7B-V A1 b.0 el7B-001 S B LLF a aT s M+ e AR mLH oF del 0.75 W Wi it th h c ino cr ore rrc et c r te rs eu slt us lts
mistral
0.30 WizardMath13B 0.50
Abel13B
0.25
0.25 Abel7B-002 0.00
LLaMA70B-PRM800K
LLaMA13B-PRM800K Abel7B-002
Abel70B
0.20 LLaMA70B-PRM800K WizardMath7B-V1.0
Abel7B-001 WizardMath13B
WizardMath70B-V1.0 WizardMath7B-V1.1
Abel13B WizardMath70B
0.15 Abel70B WizardMath7B-V1.1
0.10 0.15 0.20 0.25 0.30
Final-answer Accuracy
Figure5: Box-and-whiskerplotsofS (up-
validity
Figure4: Correlationbetweenthefinal-answer per)andS (lower). Theboundariesof
redundancy
accuracyandthefalsepositiverate. the whiskers are based on the 1.5 interquartile
range.
althoughtheformerexhibitsamuchhigheraccuracy(31.0%vs. 18.7%),itsfalsepositiverateis
almostthesame(16.7%vs. 16.5%). Itindicatesthereexistsabottleneckforthequalityofreasoning
stepstoadvance.
The model size and base model affect the false positive rate significantly. When comparing
LLaMA13B-PRM800KwithLLaMA70B-PRM800K,althoughusingthesametrainingdata,the
distinctioninfalsepositiveratesislarge(40.6%vs. 21.8%). Itindicatestheimportanceofthemodel
scale. ThebasemodelofMistral(Jiangetal.,2023)alsoachievesalowerfalsepositiveratethan
LLaMA2(22.8%vs. 31.8%,16.7%vs. 34.4%). Overall,thescalinglawstillappliestothisfield.
RLHF helps lower the false positive rate when the LLMs are relatively strong. Thereâ€™s no
distinctioninfalsepositiveratesbetweenSFTandSFTplusRLHFwhenthemodelisrelativelyweak,
like7Bandthe13BmodelofLLaMA-2(31.8%vs. 34.4%,29.8%vs. 31.3%). Asthemodelsize
becomeslarger,SFTplusRLHFperformsbetter(16.5%vs. 20.0%,16.7%vs. 22.8%). Thisindicates
thateffectivesupervisionbyRLHFrequiresastrongmathematicalabilityofthemodelitself.
5.1.2 RedundancyofSolutions
Weanalyzetheredundancyinreasoningsteps. OurmainresultsareinFigure5. Itisnoteworthythat
theredundancyscoresofAbelfamilyareusuallyhigher. ThisisbecausethesolutionsfromAbel
ofteninvolverestatingtheproblemfirst,whichisconsideredameaninglessactiontowardssolving
theproblembyourevaluator. Moreover,theredundancyscoresforsolutionsthatfailtoreachthe
8
etaR
evitisoP
eslaF
ytidilavS
ycnadnuderScorrectanswersarehigher. Thissuggeststhatwhenamodelisunsureabouthowtosolveaproblem,
ittendstomakemoreattemptsthatlackmeaningfulprogression.
5.2 ImprovingReasoningQualitybySelectingTrainingData
Inthispart,weexplorethepotentialofREASONEVALtoselecthigh-qualitytrainingdataforSFT.
Setup TheMMIQC(Liu&Yao,2024)datasetconsists
ofamixtureofprocessedwebdataandsyntheticquestion-
Filter #Training Acc. Val. Red. #Token
responsepairsusedtoenhancethereasoningcapabilities
- 100% 22.2 65.2 27.4 723.4
of LLMs. We randomly sample 10K unique responses
val. 76.7% 22.0 65.9 26.4 699.9
generatedbyGPT-3.5-turboonMATHfromthedataset. We random 76.7% 20.1 62.5 27.4 765.6
thenfilteritusingREASONEVALWizardMath-7B-V1.1,specif- r re ad n.
dom
7 71 1. .9 9%
%
2 21 0. .8
3
6 65 2. .6
3
2 22 8. .1
0
6 78 41 6. .5
1
ically removing samples with validity scores below 0.5
red.&val. 56.7% 22.0 67.8 22.5 701.2
orredundancyscoresabove0.15. Wealsocombinethese random 56.7% 20.0 62.1 27.6 739.5
twoconditions. WeSFTthemistral-7bindifferentsubsets
with3epochsandobservetheperformanceonMATH.To Table6: â€œ#Trainingâ€representstheper-
compare,werandomlysamplethesameamountoftrain- centageoftrainingdatafromthefullset.
ingdatathreetimesandreporttheaverageperformance. â€œAcc.â€ represents the overall accuracy.
â€œVal.â€ andâ€œRed.â€ representS and
validity
S forthesolutionswithcorrect
Results WemakethefollowingobservationsfromTa- redundancy
results. â€œ#Tokenâ€representstheaverage
ble 6: (1) In terms of accuracy, all groups filtered by
numberoftokensforthesolutions.
REASONEVALWizardMath-7B-V1.1 outperform the random
groupandachieveperformanceclosetothatofthefulldataset. (2)Theaveragenumberoftokensfor
thesolutionsdecreasesingroupsfilteredbyREASONEVALWizardMath-7B-V1.1,indicatingitsadvantage
inincreasingproblem-solvingefficiency. (3)Thegroupthatcombinesthetwofilteringconditions
significantlyimprovesthequalityofreasoningstepswithonlyabouthalfthetrainingdata.
6 RelatedWork
Evaluating reasoning steps automatically The technique used to assess reasoningsteps auto-
matically can be broadly divided into three groups: (1) Embedding-based methods: Golovneva
etal.(2022)proposeseveralmetricsandusestheembedding-basedcalculationamonghypothesis
steps,referencesteps,andproblemstorepresentthem;(2)Parsing-basedmethods: thisapproach
aimstoparsestepsintostructuredforms,likeâ€˜subject-verb-objectâ€™frames(Prasadetal.,2023)or
symbolicproofs(Saparov&He,2022). However,itpresentschallengesforcomplexdatasetssuchas
MATHduetotheintricatelogicinvolved;(3)Prompting-basedmethods: duetothegeneralizationof
LLMs,giventailoredpromptstoSOTALLMs,theycancheckthesolutionsandfindthepotential
errors(Tyenetal.,2023;Zengetal.,2024).
Processrewardmodel(PRM) Processrewardmodelsaremainlyusedinreinforcementlearning
togivefeedbacktoLLMstoalignwithhumanlogicinmathematics. Lightmanetal.(2023)and
Uesatoetal.(2022)bothevaluatetheperformanceofPRMasaverifieragainsttheoutcomereward
model. Maetal.(2023)combineacheck-and-generationideawithPRMtogenerateamoreaccurate
response. Wangetal.(2023)proposeawaytoautomaticallyconstructprocess-wisesupervisiondata.
ThesestudiesfocusonleveragingPRMtoenhanceaccuracy. Incontrast,ourresearchexploresthe
potentialofPRMtodevelopnewevaluationmethodsinmathematics,movingbeyondmereaccuracy
improvements.
7 Conclusion
Inthiswork, we develop REASONEVAL, anewmetrictoevaluatethe qualityofreasoningsteps
fromtheperspectiveofcorrectnessandefficiency. OurexperimentsshowthatREASONEVALcan
effectivelydetectdifferenttypesoferrorsandachievescompetitiveperformancecomparedtoother
methods. With REASONEVAL, wefindtheinconsistencybetweenfinal-answeraccuracyandthe
qualityofreasoningsteps. Additionally,wediscoveritsefficacyinselectingtrainingdata.
9References
ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster,MarcoDosSantos,StephenMarcusMcAleer,
AlbertQJiang,JiaDeng,StellaBiderman,andSeanWelleck. Llemma: Anopenlanguagemodel
formathematics. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu.
Generativeaiformath: Abel. https://github.com/GAIR-NLP/abel,2023.
MirunaClinciu,ArashEshghi,andHelenHastie. Astudyofautomaticmetricsfortheevaluationof
naturallanguageexplanations. InProceedingsofthe16thConferenceoftheEuropeanChapterof
theAssociationforComputationalLinguistics: MainVolume,pp.2376â€“2387,2021.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolve
mathwordproblems. arXivpreprintarXiv:2110.14168,2021.
JosefDai,XuehaiPan,RuiyangSun,JiamingJi,XinboXu,MickelLiu,YizhouWang,andYaodong
Yang. Saferlhf: Safereinforcementlearningfromhumanfeedback. InTheTwelfthInternational
ConferenceonLearningRepresentations,2023.
YannDubois,XuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,CarlosGuestrin,
PercyLiang,andTatsunoriB.Hashimoto. Alpacafarm: Asimulationframeworkformethodsthat
learnfromhumanfeedback,2023.
Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam
Fazel-Zarandi,andAsliCelikyilmaz. Roscoe:Asuiteofmetricsforscoringstep-by-stepreasoning.
InTheEleventhInternationalConferenceonLearningRepresentations,2022.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song,andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset. In
Thirty-fifthConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack
(Round2),2021.
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,
DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal.
Mistral7b. arXivpreprintarXiv:2310.06825,2023.
AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,VinayRa-
masesh,AmbroseSlone,CemAnil,ImanolSchlag,TheoGutman-Solo,etal. Solvingquantitative
reasoningproblemswithlanguagemodels. AdvancesinNeuralInformationProcessingSystems,
35:3843â€“3857,2022.
HunterLightman,VineetKosaraju,YuriBurda,HarrisonEdwards,BowenBaker,TeddyLee,Jan
Leike,JohnSchulman,IlyaSutskever,andKarlCobbe. Letâ€™sverifystepbystep. InTheTwelfth
InternationalConferenceonLearningRepresentations,2023.
HaoxiongLiuandAndrewChi-ChihYao. Augmentingmathwordproblemsviaiterativequestion
composing. arXivpreprintarXiv:2401.09003,2024.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternationalConfer-
enceonLearningRepresentations,2018.
Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning
formathematicalreasoning. InProceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),pp.14605â€“14631,2023.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng,
Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical
reasoningforlargelanguagemodelsviareinforcedevol-instruct. arXivpreprintarXiv:2308.09583,
2023.
QianliMa, HaotianZhou, TingkaiLiu, JianboYuan, PengfeiLiu, YangYou, andHongxiaYang.
Letâ€™srewardstepbystep: Step-levelrewardmodelasthenavigatorsforreasoning. arXivpreprint
arXiv:2310.10080,2023.
10ROpenAI. Gpt-4technicalreport. arXiv,pp.2303â€“08774,2023.
ArchikiPrasad,SwarnadeepSaha,XiangZhou,andMohitBansal. Receval: Evaluatingreasoning
chainsviacorrectnessandinformativeness. InProceedingsofthe2023ConferenceonEmpirical
MethodsinNaturalLanguageProcessing,pp.10066â€“10086,2023.
AbulhairSaparovandHeHe. Languagemodelsaregreedyreasoners: Asystematicformalanalysis
ofchain-of-thought. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
TomohiroSawada, DanielPaleka, AlexanderHavrilla, PranavTadepalli, PaulaVidas, Alexander
Kranias,JohnJNay,KshitijGupta,andAranKomatsuzaki. Arb: Advancedreasoningbenchmark
forlargelanguagemodels. arXivpreprintarXiv:2307.13692,2023.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor CaË˜rbune. Llms cannot find
reasoningerrors,butcancorrectthem! arXivpreprintarXiv:2311.08516,2023.
JonathanUesato,NateKushman,RamanaKumar,FrancisSong,NoahSiegel,LisaWang,Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-basedfeedback. arXivpreprintarXiv:2211.14275,2022.
PeiyiWang,LeiLi,ZhihongShao,RXXu,DamaiDai,YifeiLi,DeliChen,YWu,andZhifang
Sui. Math-shepherd: Alabel-freestep-by-stepverifierforllmsinmathematicalreasoning. arXiv
preprintarXiv:2312.08935,2023.
LonghuiYu,WeisenJiang,HanShi,YUJincheng,ZhengyingLiu,YuZhang,JamesKwok,Zhenguo
Li,AdrianWeller,andWeiyangLiu. Metamath: Bootstrapyourownmathematicalquestionsfor
largelanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,
2023.
Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: A meta-
reasoningrevolutioninlargelanguagemodelevaluation. arXivpreprintarXiv:2312.17080,2024.
MengxueZhang,ZichaoWang,ZhichaoYang,WeiqiFeng,andAndrewLan. Interpretablemath
wordproblemsolutiongenerationviastep-by-stepplanning. InProceedingsofthe61stAnnual
MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pp.6858â€“6877,
2023.
A SupplementsforMeta-Evaluations
A.1 ExamplesofSolutionsGeneratedbyDifferentLLMs
We present examples of solutions generated by different LLMs in Figure 9, including LLaMA-
PRM800K,AbelandWizardMath.
A.2 TrainingDetailsforREASONEVAL
TheREASONEVALmodelistrainedonNVIDIAA100GPUs(4GPUsforthe7Bmodels,16GPUs
forthe34Bmodels)usingthesupervisedfine-tuning(SFT)frameworkprovidedbyDaietal.(2023).
Specifically,themodelistrainedin1epoch. Wesetthebatchsizeto64andmaximumsequence
lengthto2048. Weuseapeaklearningrate1e-6with8warmupstepsandcosinelearningratedecay
to0. WeuseAdamW(Loshchilov&Hutter,2018)asouroptimizerwithÎ² =0.9,Î² =0.95and
1 2
weightdecayof0.1. Onlythelasttokenofastepistakenintoaccountinthelossfunction, asit
containsthefullinformationaboutthestep.
11A.3 SettingsforEvaluators
Thesettingsforevaluatorsareasfollows: (1)Embedding-basedmethods: forROSCOEweusethe
embeddinglayerâ€œroscoe-512-roberta-baseâ€. WeselectRepetition-tokenmatrixforROSCOE-SA
andtheRepetition-stepmatrixforROSCOE-SS.Thethresholdissetto0.025. (2)Prompting-based
methods: we present the prompts in Figure 6. (3) Step-level evaluators: for the validity scores,
the threshold is set to 0.5, and for redundancy scores, it is set to 0.15. Both are applied to the
solution-levellabelsandstep-levellabels. Formath-shepherdwesetthethresholdto0.5forallerrors.
ThePromptforidentifyinginvaliderrors ThePromptforidentifyingredundanterrors
Question: Question:
{question} {question}
StudentSolution: StudentSolution:
{solution} {solution}
Yourtaskinvolvesthreeparts: Yourtaskinvolvesthreeparts:
1. **Step-by-stepEvaluation:**Gothrough 1. **Step-by-step Evaluation:** Carefully
the student solution carefully and identify review the studentâ€™s solution. Identify any
key errors and potential misunderstandings neutral steps that, while reasonable, do not
thatledtotheincorrectsolution. offer new insight, advance the solution, or
2. **FinalJudgement:**Provideanoverall suggestanextstep.
judgementonthecorrectnessofthestudentâ€™s 2. **Final Judgment:** Indicate whether
solution. thesolutioncontainsordoesnotcontainany
3. **First Error Step:** If the solution is neutralsteps.
incorrect, generate the step number where 3. **Neutral Steps Identified**: If neutral
thefirsterroroccurs,otherwisegenerateN/A steps are present, list their numbers in list
here format;otherwise,insertN/A.
Hereâ€™stheformatIwant: Hereâ€™stheformatIwant:
Step-by-stepEvaluation: [Provideastepby Step-by-stepEvaluation: [Provideastep-by-
stepexaminationofthestudentsolutionand stepexaminationofthestudentsolutionand
identify key errors and misunderstandings identifytheneutralsteps.]
here.] Final Judgment: [Insert only the word
FinalJudgement: [Insertonly**correct**or **Present**ifneutralstepsareidentified,or
**wrong**here] **Absent**ifnot.]
FirstErrorStep: [InserteitherN/Aorthestep NeutralStepsIdentified: [InserteitherN/Aor
numberwherethefirsterroroccurs] thestepnumbersinlistformat.]
Please follow this format without any addi- Please follow this format without any addi-
tionalintroductoryorconcludingstatements. tionalintroductoryorconcludingstatements.
Figure6: Promptsusedtoidentifyinvalidandredundanterrors,modifiedfromthepromptprovided
byZengetal.(2024).
A.4 AdditionInformationforMeta-EvaluationsbyPerturbation
InFigure3,forswappingsolutions,theredundancyscoreoftheoriginalsolutionishigherthanothers.
Thisobservationmaystemfromthefactthatthefirststep(i.e.,theobservedstep)typicallyinvolves
analyzingtheproblemfromscratch,makingitmorelikelytoinvolverestatingtheproblems,which
maybeconsideredanunproductiveorunnecessarystepfortheoverallsolution.
Wepresenttheresultsofsolution-levelscoresinFigure8. Forsolution-levelscores,thetendencies
discussed in Â§4.2 remain unchanged, but the distinction between the hypothesis and reference
decreases,asthesolution-levelscorecanbedisturbedbyothersteps. Forreference-freeevaluation,it
addsdifficultyinfindingthespecificstep.
123-step 6-step 3-step 6-step
1.00
0.3 0.75 0.3
0.75
0.2 0.50 0.2
0.50
0.1 0.25 0.1
0.25
0.0 0.00 0.0
1 2 3 1 2 3 4 5 6 1 2 3 1 2 3 4 5 6
9-step 12-step 9-step 12-step
0.3 0.3
0.4
0.2 0.2 0.2
0.1 0.2 0.1 0.1
0.0 0.0 0.0 0.0
1 2 3 4 5 6 7 8 9 123456789101112 1 2 3 4 5 6 7 8 9 123456789101112
Location of First Error Step Location of First Error Step
LLaMA13B-PRM800K Abel7B-002 LLaMA13B-PRM800K Abel7B-002
LLaMA70B-PRM800K WizardMath7B-V1.0 LLaMA70B-PRM800K WizardMath7B-V1.0
Abel7B-001 WizardMath13B Abel7B-001 WizardMath13B
Abel13B WizardMath70B Abel13B WizardMath70B
Abel70B WizardMath7B-V1.1 Abel70B WizardMath7B-V1.1
(a) (b)
Figure7:(a)Thefirsterrorlocationinreasoningstepswithcorrectresults. (b)Thefirsterrorlocation
inreasoningstepswithincorrectresults.
ReasonEval Math-shepheard-Mistral-7B
Llemma-34B
1.0
Original 1.0 Original
Perturbed
Perturbed
0.5 0.8
0.6
0.0
Original 0.4
0.4
Perturbed
0.2 0.2
0.0 0.0
Random number Swap steps Repeat step
Random operation Remove step Swap solutions
Figure8: Box-and-whiskerplotsofscoreswhenmeetingdifferenttypesoferrorsatthesolutionlevel.
Theboundariesofthewhiskersarebasedonthe1.5interquartilerange.
B SupplementsforEvaluatingLLMs
B.1 LocationoftheFirstErrorStep
Weexaminethecharacteristicsofthefirsterrorlocationidentifiedbythevalidityscore. Wesetthe
thresholdto0.25forhighrecall. WepresentthemainresultsinFigure7. Whilethereissignificant
noiseinthem,wecanstillobservegeneraltrends. Forreasoningstepswithincorrectresults,the
modelismorepronetomakingmistakesatanearlystage,leadingtofalseresults,especiallywhen
thelengthsofreasoningstepsarelonger. Forreasoningstepswithcorrectresults,thedistributionis
muchflatter,indicatingtheseerrorshiddeninreasoningstepscanhappenatanylocation.
13
ytisneD
ytidilavS
ycnadnuderS
ytisneDConvert the point (0,3) in rectangular coordinates to polar coordinates. Enter your
answer in the form (r,Î¸), where r>0 and 0 Î¸<2Ï€.
â‰¤
To convert from rectangular to polar coordinates, I need to use the formulas r = x2+y2 and
Î¸=tan 1(y/x),whereÎ¸ ismeasuredfromthepositivex-axis.
âˆ’ p
Inthiscase,x=0andy=3,soIcanplugthemintotheformulasandsimplify.
Forr,Igetr=âˆš02+32=âˆš9=3.ForÎ¸,IgetÎ¸=tan 1(3/0)=tan 1( )=Ï€/2.
âˆ’ âˆ’
âˆž
However,thisvalueofÎ¸ isnotintherange[0,2Ï€),soIneedtoadjustitbysubtracting2Ï€.
Therefore,thepolarcoordinatesofthepointare(3,Ï€/2 2Ï€)=(3, Ï€/2).
âˆ’ âˆ’
Answer(3,5Ï€/2)
LLaMA-PRM800K
1. Theproblemasksustoconvertthepoint(0,3)inrectangularcoordinatestopolarcoordinates. We
needtoexpresstheanswerintheform(r,Î¸),wherer>0and0 Î¸<2Ï€.
â‰¤
2. To convert rectangular coordinates to polar coordinates, we need to find the magnitude r and the
angleÎ¸.
3. The magnitude r is given by the formula r= x2+y2, where x and y are the coordinates of the
point. Inthiscase,x=0andy=3,sor=âˆš02+32=3.
p
4. The angle Î¸ is the angle between the positive x-axis and the line connecting the origin and the
point. Byobservation,wecanseethattheangleis 5Ï€.
3
5Ï€
5. [Finalsolution]Therefore,thepolarcoordinatesofthepoint(0,3)are 3, .
3
(cid:18) (cid:19)
Abel
Step1: Determinethedistancefromtheorigin(r)
Since the point (0,3) is on the positive y-axis, the distance from the origin to the point is simply the
y-coordinate,whichis3. So,r=3.
Step2: DeterminetheangleÎ¸
Weneedtofindtheangleformedbythepositivex-axisandthelineconnectingtheorigintothepoint
(0,3). Sincethepointisonthepositivey-axis,theangleis0degrees(or0radians).
Step3: Writethepolarcoordinates
Now that we have the distance (r) and the angle (Ë˜03b8), we can write the polar coordinates as
(r,Î¸)=(3,0).
So,thepolarcoordinatesofthepoint(0,3)inrectangularcoordinatesare (3,0) .
Theansweris: (3,0).
WizardMath
Figure9: ExamplesofsolutionsgeneratedbydifferentLLMs
14
1