Bridging Swarm Intelligence and Reinforcement Learning
KarthikSoma YannBouteiller
PolytechniqueMontreal PolytechniqueMontreal
Montreal,Canada Montreal,Canada
HeikoHamann GiovanniBeltrame
UniversityofKonstanz PolytechniqueMontreal
Konstanz,Germany Montreal,Canada
ABSTRACT thehouse-huntingbehaviorofhoneybeestocreatetheweighted
Swarmintelligence(SI)exploreshowlargegroupsofsimpleindi- voterrule[14,23].Inthisrule,afterscoutingoneofğ‘›potentialnest-
viduals(e.g.,insects,fish,birds)collaboratetoproducecomplex ingareas,beescomebacktoperformaâ€œdance"[24]thatdescribes
behaviors,exemplifyingthatthewholeisgreaterthanthesumof thecoordinatesoftheoptionthattheyhaveexplored.Accordingto
itsparts.AfundamentaltaskinSIisCollectiveDecision-Making theweightedvotermodel,thisdanceisperformedatafrequency
(CDM),whereagroupselectsthebestoptionamongseveralalter- thatisproportionaltotheestimatedqualityoftheexploredarea.
natives,suchaschoosinganoptimalforagingsite.Inthiswork, Otherbeesgoscouttheareacorrespondingtothefirstdancethey
wedemonstrateatheoreticalandempiricalequivalencebetween witness,andthisprocessrepeatsuntiltheentirecolonyconverges
CDMandsingle-agentreinforcementlearning(RL)inmulti-armed tothesameoption.
banditproblems,utilizingconceptsfromopiniondynamics,evo- Next,weturntowardreinforcementlearning(RL),wherean
lutionarygametheory,andRL.Thisequivalencebridgesthegap
agent1learnstosolveataskbyinteractingwiththeenvironment
betweenSIandRLandleadsustointroduceanovelabstractRL tomaximizearewardsignal[20].RLhasbeensuccessfullyapplied
updaterulecalledMaynard-CrossLearning.Additionally,itprovides tosolvecomplexproblemsinvariousfieldssuchasrobotics[9],
anewpopulation-basedperspectiveoncommonRLpracticeslike nuclearfusion[17],andgames[10].Inthispaper,wearespecifically
learningrateadjustmentandbatching.Ourfindingsenablecross- interested in multi-armed bandits [20], in which a single agent
disciplinaryfertilizationbetweenRLandSI,allowingtechniques makeschoicesamongdifferentoptions(orâ€œarmsâ€)tomaximizeits
fromonefieldtoenhancetheunderstandingandmethodologiesof reward.Amongthemanylearningalgorithmsdesignedtosolvethis
theother. task(Upper-confidence-Bound[1](UCB),ğœ–-greedy[20],Gradient
Bandit [25]...), we consider the Cross Learning [5] update rule,
closelyrelatedtotheGradientBanditalgorithm.
KEYWORDS
AlthoughSIandRLareseeminglydisjoint,weshowthatthese
SwarmIntelligence,ReinforcementLearning,EvolutionaryGame
fieldscaninfactbebridgedviatheReplicatorDynamic[16](RD),
Theory,OpinionDynamics
afamousequationusedinEvolutionaryGameTheory(EGT)to
ACMReferenceFormat: model the outcome of evolutionary processes through the idea
KarthikSoma,YannBouteiller,HeikoHamann,andGiovanniBeltrame.2025. of survival of fittest. In the rest of the paper, we demonstrate a
BridgingSwarmIntelligenceandReinforcementLearning.arXivpreprint, mathematicalequivalencebetweendifferentconceptsfromSIand
11pages.
RL:
â€¢ Wefirstshowthatalargepopulationwhosemembersfol-
1 INTRODUCTION
lowthevoterrulecanbeseenasasingleabstractRLagent
SwarmIntelligence(SI)takesinspirationfromhowacollectiveof followingtheCrossLearningupdaterule.
naturalentities,followingsimple,local,anddecentralizedrules,can â€¢ Next,viaasimilarargument,weshowthattheweighted
produceemergentandcomplexbehaviors[3].Researchershave voterrule,yieldsanotherabstractRLupdatethatwecoin
extractedcoreprinciplessuchascoordination,cooperation,and Maynard-CrossLearning.
localcommunicationfromthesenaturalsystems,andappliedthem â€¢ WevalidatetheseequivalenceswithRLandpopulationex-
toartificialsystems,(e.g.,swarmrobotics[7,8]andoptimization perimentsandofferanewperspectiveabouttwocommon
algorithms[6]). practicesinRL,learningrateadjustmentandbatching.
Inthispaper,wefocusthespecificSIproblemofCollectiveDeci-
sionMaking(CDM).InCDM,individualsworktogethertoreachan
2 PRELIMINARIES
agreementonthebestoptionfromasetofalternatives,aproblem
2.1 Multi-armedbanditsandCrossLearning
commonlycalledthebest-of-ndecisionproblem.Tosolvethisprob-
lem,researchershaveturnedtoopiniondynamics[26],afieldthat Multi-armedbanditsareoneofthesimplesttypesofenvironments
studieshowopinionsspreadinapopulation.Inparticular,inthe encounteredinRLliterature.Theyconsistofadiscretesetofavail-
voterrule[4,12],anindividualcopiestheopinionofarandomly ableactions,calledâ€œarms",amongstwhichanagenthastofindthe
chosenneighbor.Similarly,researchershavetakeninspirationfrom mostrewarding.Inanğ‘›-armedbandit,pullingarmğ‘âˆˆ{1,...,ğ‘›}
ThisworkislicencedundertheCreativeCommonsAttribution4.0International 1Toavoidconfusion,weuseâ€œagentâ€inthecontextofRLandâ€œindividualâ€inthecontext
(CC-BY4.0)licence. ofapopulation,whereverpossible
4202
tcO
32
]AM.sc[
1v71571.0142:viXrareturnsareal-valuedrewardğ‘Ÿ ğ‘ âˆˆ [0,1] sampledfromahidden variantoftheTRDforlaterconvenienceinthepaper,theMaynard
distributionğ‘Ÿ(ğ‘).TheobjectiveforanRLagentplayingamulti- SmithReplicatorDynamic[18](MRD):
armedbanditistolearnapolicy,denotedbytheprobabilityvector ğœ‹
ğœ‹ =(ğœ‹1,....ğœ‹ ğ‘›),thatmaximizestherewardsobtaineduponpulling ğœ‹(cid:164)ğ‘ = ğ‘£ğœ‹ğ‘ (ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ) (4)
thearms.Differentexplorationstrategiesexisttofindsuchpolicies,
oneofthembeingCrossLearning[5]: 2.3 Collective-decisionmakinginswarms
CrossLearning(CL).Letğ‘˜beanactionandğ‘Ÿ ğ‘˜ acorresponding ConsiderapopulationPofğ‘ individualstryingtoreachaconsen-
rewardsample(ğ‘Ÿ ğ‘˜ âˆ¼ğ‘Ÿ(ğ‘˜)).CLupdatesthepolicyğœ‹ as: susonwhichamongstğ‘›availableoptionsistheoptimal.Similar
(cid:40) topopulationgames,eachindividualğ‘–hasanopinion,denotedby
âˆ€ğ‘,ğœ‹ ğ‘ â†ğœ‹ ğ‘+ğ‘Ÿ ğ‘˜
1âˆ’ğœ‹
ğ‘
ifğ‘=ğ‘˜
(1) ğ‘‚ ğ‘ âˆˆ{1,...,ğ‘›},aboutwhichoptiontheyprefer.Weagaincallthe
âˆ’ğœ‹ ğ‘ otherwise populationvectorğœ‹ = (ğœ‹1,...,ğœ‹ ğ‘›),whichinthiscontextrepre-
Forconvenience,wedenotetheexpectedpolicyupdateonaction sentsthefractionofindividualssharingeachopinion.Theweighted
ğ‘â€™sprobabilityğœ‹ whensamplingrewardğ‘Ÿ fromactionğ‘˜as: voterrulemodelsthedanceofhoneybeesduringnest-hunting[14]:
ğ‘ ğ‘˜
(cid:40) Weightedvoterrule:Anyindividualğ‘– âˆˆ P ofopinionğ‘‚ ğ‘– = ğ‘
ğ‘‘ğœ‹ ğ‘(ğ‘˜)=E ğ‘Ÿğ‘˜âˆ¼ğ‘Ÿ(ğ‘˜)[ğ‘Ÿ ğ‘˜] âˆ’1 ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise
(2) f (1o )llo ğ‘–w ess tt imhe atw ee si tg hh ete qd uv alo it te yr or fu il te sğ‘… cw urv ro ete nr t:
opinionğ‘Ÿ ğ‘ âˆ¼ğ‘Ÿ(ğ‘),where
InCL,everyrewardğ‘Ÿ sampledwhenapplyingtheassociated
0â‰¤ğ‘Ÿ
ğ‘
â‰¤1.
ğ‘˜ (2) Afterobtainingğ‘Ÿ ,ğ‘–locallybroadcastsitsopinionatafrequency
actionğ‘˜ directlyaffectstheprobabilitiesaccordedbypolicyğœ‹ to ğ‘
proportionaltoğ‘Ÿ .
allavailableactions.Asnotedearlier,CLiscloselyrelatedtothe ğ‘
(3) ğ‘– switchesitsopiniontothefirstopinionğ‘ thatitperceives
GradientBanditalgorithm,whichperformsasimilarupdateatthe
fromitsneighborhood.Assumingallindividualsarewellmixed
parameterlevel(calledâ€œpreferencesâ€)ofaparametricpolicyrather
inthepopulation[11],thecorrespondingexpectedprobability
thandirectlyupdatingtheprobabilityvector.
ofğ‘–switchingtoopinionğ‘istheproportionofvotescastforğ‘:
2.2 Evolutionarygametheory
ğ‘ƒ(ğ‘ â†ğ‘)= (cid:205)ğ‘ ğ‘™ğ‘ ğ‘E ğ‘™E[ğ‘Ÿ [ğ‘ ğ‘Ÿ]
ğ‘™]
(whereğ‘
ğ‘˜
isthenumberofindividuals
Evolutionarygametheory(EGT)studiespopulationgames[15].In ofopinionğ‘˜).Thisprobabilitycanfurtherbewritten (cid:205)ğœ‹ ğ‘™ğ‘ ğœ‹E ğ‘™E[ğ‘Ÿ [ğ‘ ğ‘Ÿ]
ğ‘™]
asingle-populationgame,apopulationPismadeofalargenumber bydividingboththenumeratorandthedenominatorbyğ‘.
ofindividuals,whereanyindividualğ‘– isassociatedwithatype, Notethatinthismodel,beesdonotdirectlyobservethequality
denotedbyğ‘‡ ğ‘– âˆˆ{1,...,ğ‘›}.Thepopulationvectorğœ‹ =(ğœ‹1,...,ğœ‹ ğ‘›) estimateofotherindividuals,butonlytheiropinion.Thismakes
representsthefractionofindividualsineachtype((cid:205) ğ‘–ğœ‹
ğ‘–
=1).In- theweightedvoterrulewell-adaptedtoswarmsofcommunication-
dividualsarerepeatedlypairedatrandomtoplayagame,each limitedorganisms.
receivingaseparatepayoffdefinedbythegamebi-matrix2ğ´.In-
dividualsadapttheirtypebasedonthesepayoffsaccordingtoan 3 THEORY
updaterule.3Onenotablesuchruleisimitationofsuccess[15]:
Remark1. Population-policyequivalence.Asnotedby[2],apop-
ImitationDynamics:Anyindividualğ‘– âˆˆPoftypeğ‘‡ ğ‘– =ğ‘follows ulationvectorğœ‹ =(ğœ‹1,...,ğœ‹ ğ‘›)canbeabstractedasamulti-armed
thevoterrule4ğ‘…voter:
banditRLpolicy(andvice-versa).Inthisview,uniformlysampling
(1) ğ‘–samplesarandomindividualğ‘— âˆ¼U(P)toimitate.Letğ‘‡ ğ‘— beğ‘. anindividualoftypeğ‘fromthepopulationisequivalenttosampling
(2) Bothindividualsğ‘–andğ‘— playthegamedefinedbyğ´toreceive actionğ‘fromthepolicy.
payoffsğ‘Ÿ
ğ‘
andğ‘Ÿ
ğ‘
respectively(0â‰¤ğ‘Ÿ
ğ‘,ğ‘
â‰¤1).Ingeneral,each
payoffmaydependonthetypesofbothindividuals. 3.1 VotersandCrossLearning
(3) ğ‘–switchestotypeğ‘withprobabilityğ‘Ÿ 5.
ğ‘ Proposition1. Aninfinitepopulationofindividualsfollowing
Onecaneasilyseewhythisruleiscalledâ€œimitationofsuccessâ€:
ğ‘– imitates ğ‘— basedon ğ‘—â€™spayoff.Whenaggregatedtotheentire
ğ‘… votercanbeseenasanRLagentfollowingExactCrossLearning7 ,
i.e.,
population,imitationofsuccessyieldsafamousequationinEGT,
calledtheTaylorReplicatorDynamic[16,21](TRD)(seeLemma2):
ğ‘‘voterğœ‹
ğ‘
=E ğ‘˜âˆ¼ğœ‹,ğ‘Ÿğ‘˜âˆ¼ğ‘Ÿ(ğ‘˜)[ğ‘‘CLğœ‹ ğ‘(ğ‘˜,ğ‘Ÿ ğ‘˜)], (5)
ğœ‹(cid:164)ğ‘ =ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ), (3) w ach tie or ne -ğœ‹ prc oa bn abb ie lits ie ee sn ua ns deb rot th heth pe opp uo lp au til oa nti -o pn olv ice yct eo qr ua in vad leth ne cev ,e ğ‘‘c vt oo tr ero ğœ‹f
whereğœ‹(cid:164)ğ‘isthederivativeoftheğ‘-thcomponentofthepopulation isthesingle-stepchangeofpopulationğœ‹ underthevoterrule(i.e.,
vectorğœ‹ ğ‘,ğ‘ ğ‘ğœ‹ :=E[ğ‘Ÿ ğ‘]istheexpectedpayoffofthetypeğ‘against thechangeintypeproportionsafterallindividualssimultaneously
thecurrentpopulation,andğ‘£ğœ‹ :=(cid:205) ğ‘ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]isthecurrentaverage performğ‘… voteronce),andğ‘‘CLğœ‹(ğ‘˜,ğ‘Ÿ ğ‘˜)istheupdateperformedbyCL
payoffoftheentirepopulation.6Further,wedescribeanotheruseful onthepolicyğœ‹ foragivenaction-rewardsample(ğ‘˜,ğ‘Ÿ ğ‘˜).
2Abimatrixgameisdefinedbyapairofidentical-shapematrices,oneperagent. ToproveProposition1,weusetwointermediateresults(Lem-
3Inpopulationgames,anupdateruleissometimescalledarevisionprotocol. mas1and2).Theseresultsarealreadyknownfromtheliterature
4VoterruleisnotaterminologyusedinEGT.Instead,itcomesfromopiniondynamics. (althoughtothebestofourknowledgewearethefirsttointegrate
5Thisdefinitionofvoterrulediffersfromopiniondynamicsasindividualsdonot
themandapplytheminthiscontext).Weprovideproofsusingour
switchdeterministically,butrathermakeaprobabilisticswitch.
6InEvolutionaryGameTheory,expectedpayoffsareoftenreferredtoasâ€œfitness",as
theymodelthereproductivefitnessofthedifferenttypes. 7Wecall"exact"thealgorithmthatappliestheexpectedupdate.formalismforbothLemmas,aswewilllaterfollowasimilarrea-
soningtoprovetheCDM/RLequivalence.Thefirstresultdescribes
âˆ‘ï¸
apolicy-populationequivalencebetweenCLandtheTRD: ğ‘‘ğœ‹ ğ‘ = ğ‘ƒ(ğ‘â†ğ‘)âˆ’ğ‘ƒ(ğ‘ â†ğ‘) (10)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
ğ‘â‰ ğ‘(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
inflow outflow
Lemma1. Inexpectation,anRLagentlearningviatheCLupdate âˆ‘ï¸
rulefollows[2]:
= ğœ‹ ğ‘ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]âˆ’ğœ‹ ğ‘ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]
ğ‘â‰ ğ‘
(cid:104)âˆ‘ï¸ âˆ‘ï¸ (cid:105) âˆ‘ï¸
E ğ‘˜âˆ¼ğœ‹[ğ‘‘ğœ‹ ğ‘(ğ‘˜)] =ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ), (6) =ğœ‹ ğ‘ ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]âˆ’ ğœ‹ ğ‘E[ğ‘Ÿ ğ‘] ğœ‹ ğ‘+ğœ‹ ğ‘ =1
ğ‘â‰ ğ‘ ğ‘â‰ ğ‘ ğ‘â‰ ğ‘
(cid:104) âˆ‘ï¸ (cid:105)
whereğ‘ ğ‘ğœ‹ isthevalueofactionğ‘,andğ‘£ğœ‹ isthevalueofpolicyğœ‹. =ğœ‹ ğ‘ (1âˆ’ğœ‹ ğ‘)E[ğ‘Ÿ ğ‘]âˆ’ ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]
ğ‘â‰ ğ‘
(cid:104) âˆ‘ï¸ (cid:105)
=ğœ‹ ğ‘ E[ğ‘Ÿ ğ‘]âˆ’ ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]
Proof. Letuscomputetheexpectationoveractionssampled ğ‘
fromğœ‹ inEq.2.Forconvenience,wewrite =ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ) (11)
E[ğ‘‘ğœ‹ ğ‘] :=E ğ‘˜âˆ¼ğœ‹[ğ‘‘ğœ‹ ğ‘(ğ‘˜)],andE[ğ‘Ÿ ğ‘˜] :=E ğ‘Ÿğ‘˜âˆ¼ğ‘Ÿ(ğ‘˜)[ğ‘Ÿ ğ‘˜]:
â–¡
ğ‘›
E[ğ‘‘ğœ‹ ğ‘] =âˆ‘ï¸ ğœ‹ ğ‘˜.ğ‘‘ğœ‹ ğ‘(ğ‘˜) (7) CombiningtheseresultsyieldsProposition1,asLemmas1and2
yield:
ğ‘˜=1
=ğœ‹ ğ‘.ğ‘‘ğœ‹ ğ‘(ğ‘)+âˆ‘ï¸ ğœ‹ ğ‘˜.ğ‘‘ğœ‹ ğ‘(ğ‘˜) ğ‘‘voterğœ‹ ğ‘ =E ğ‘˜âˆ¼ğœ‹,ğ‘Ÿğ‘˜âˆ¼ğ‘Ÿ(ğ‘˜)[ğ‘‘CLğœ‹ ğ‘(ğ‘˜,ğ‘Ÿ ğ‘˜)]
ğ‘˜â‰ ğ‘
Proposition1showshowTRDconnectsmulti-agentimitationdy-
âˆ‘ï¸
=ğœ‹ ğ‘E[ğ‘Ÿ ğ‘](1âˆ’ğœ‹ ğ‘)+ ğœ‹ ğ‘˜E[ğ‘Ÿ ğ‘˜](âˆ’ğœ‹ ğ‘) namicsandsingle-agentExactCrossLearning.Inpractice,RLup-
ğ‘˜â‰ ğ‘ datesdonotfollowtheirexactexpectationduetofinitesampling.
=ğœ‹ ğ‘(cid:104) E[ğ‘Ÿ ğ‘]âˆ’ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]âˆ’âˆ‘ï¸ ğœ‹ ğ‘˜E[ğ‘Ÿ ğ‘˜](cid:105) Theyrelyonactionsamplesfromthepolicyandrewardsamples
fromtheenvironment.Tocircumventhighvarianceandimprove
ğ‘˜â‰ ğ‘
(cid:104) âˆ‘ï¸ (cid:105) convergenceproperties,RLpractitionerstypicallyperformthese
=ğœ‹ ğ‘ E[ğ‘Ÿ ğ‘]âˆ’ ğœ‹ ğ‘˜E[ğ‘Ÿ ğ‘˜] policyupdatesinabatchedfashion,which,accordingtoPropo-
ğ‘˜ sition1,isequivalenttomakingtheseupdatesclosertoinfinite-
=ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ) (8) populationdynamics.Infact,thereisanaptpopulation-basedin-
terpretationofthispractice,showninthenextsection.
â–¡
3.2 Learningrateandbatch-size
Insteadofstudyingthemean-fieldeffectofaggregatedindividual
Thetermğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ iscommonlyknownastheâ€œadvantageâ€of voters,wecanlookattheindividualeffectofeachvoteronthe
actionğ‘inRL.Fromthatperspective,itdescribeshowgoodactionğ‘ entirepopulation.Thiseffectyieldsinterestinginsightsregarding
isincomparisontothecurrentpolicyğœ‹.ButRemark1enables twocommonpracticesinRL:adjustingthelearningrate,i.e.,scaling
lookingatLemma1underadifferentlight:theright-handsidesof downRLupdatesbyasmallfactor,andbatching,i.e.,averagingRL
Eqs.3and6areequivalent.Inotherwords,underthepopulation- updatesoverseveralsamples.
policy equivalence, the CL update rule tangentially follows the Insteadofaninfinitepopulation,letusconsideranear-infinite8
TRD(inexpectation).Furthermore,itisknownthatapopulation population P of ğ‘ â‰« 1 individuals. Again, we describe P by
adoptingğ‘…voteralsoyieldstheTRD: thepopulationvectorğœ‹.Asingleindividualğ‘–oftypeğ‘˜sampling
payoffğ‘Ÿ
ğ‘˜
andfollowingğ‘…voterhasthefollowinginfluence(outflow
Lemma2. Aninfinitepopulationofindividualsadoptingğ‘…
voter
fromğ‘toğ‘˜)onthepopulationvectorfortypesğ‘â‰ ğ‘˜:
followstheTRD[15,16]i.e.,
1
âˆ€ğ‘â‰ ğ‘˜,ğ‘ƒ(ğ‘–)(ğ‘˜ â†ğ‘)= ğœ‹
ğ‘ ğ‘
ğ‘Ÿ
ğ‘˜
(12)
ğ‘‘ğœ‹
ğ‘
=ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ), (9) (cid:124) ty(cid:123) p(cid:122) e(cid:125)
a (cid:124)(cid:123)(cid:122)(cid:125)
andswit(cid:124) ch(cid:123) in(cid:122) g(cid:125)
totypek
pickingi
whileitsinfluenceontypeğ‘˜isthesumofinflows(toğ‘˜):
Proof. Letğ‘ƒ(ğ‘â†ğ‘)denotetheinflowofindividualsoftypeğ‘
i an dt oo pt ty inp ge tğ‘ y, pi. ee, ğ‘t .h Te hp ero pp oo pr ut li ao tn ioo nf hth ase apo pp rou pla ot rio tin onle oa fvi ğœ‹ng it ny dp ie viğ‘ dua an ld s âˆ‘ï¸ ğ‘ƒ(ğ‘–)(ğ‘˜ â†ğ‘)=(1âˆ’ğœ‹ ğ‘˜) ğ‘1 ğ‘Ÿ ğ‘˜ . (13)
ğ‘
oftypeğ‘,eachhavingaprobabilityğœ‹ ofmeetinganindividualof
ğ‘â‰ ğ‘˜
ğ‘
typeğ‘,andaconditionalprobabilityE[ğ‘Ÿ ğ‘]ofswitchingtoitstype.
Thus,wegetğ‘ƒ(ğ‘â†ğ‘)=ğœ‹ ğ‘ğœ‹ ğ‘E[ğ‘Ÿ ğ‘]: 8Theassumptionğ‘ â‰«1enablesapproximatingflowsbytheirexpectation.Denotingğ›¼ := 1 yieldsthefollowinglearningruleattributableto entire population P, whereas there is no such explicit effect in
ğ‘
asingleindividualontheentirepopulationvector: ğ‘…voter.However,theweightedvoterruleğ‘…wvoterdoescontainan
(cid:40) expliciteffect,makingtheanalysismuchmoreintuitive.
âˆ€ğ‘,ğœ‹
ğ‘
â†ğœ‹ ğ‘+ğ›¼ğ‘Ÿ
ğ‘˜
1âˆ’ğœ‹
ğ‘
ifğ‘=ğ‘˜
. (14) InthisSection,wewillshowthat,similartohowimitationof
âˆ’ğœ‹ ğ‘ otherwise successyieldstheCLupdaterule,whentheentirepopulationis
NotehowtheRLupdatedescribedinEq.14differsfromtheone consideredasanabstractRLagent,swarmsofbeesperforming
describedinEq.1onlybyascalingfactorğ›¼ = 1. CDMforhouse-huntingfollowanabstractRLalgorithmthatwe
ğ‘
Thepopulation-policyequivalencegivesaninterestinginter- coinMaynard-CrossLearning.
pretationtothelearningrateğ›¼ commonlyusedinRL.Underthe Letusnowconsideranear-infinitepopulationofğ‘ honeybees,
populationperspective,ğ›¼ describesthenumberofindividualsin reachingaconsensusonwhichnestingsitetoselectviağ‘…wvoter.
thepopulation.InSec.5,weempiricallyshowthattheCLupdate Underğ‘…wvoter,individualshaveatangibleinfluenceontherest
ruledescribedinEq.1doesnottypicallyconvergetotheoptimal of the population: remember that in this model, individuals de-
action.However,usingasmallenoughlearningrate(i.e.,alarge terministicallyswitchtothefirstactiontheywitness.Hence,the
enoughpopulationsize)alleviatesthisissue9. influenceofeachindividualisequaltotheratioofitsbroadcast-
Todescribetheaggregatedeffectofğ‘…voterontheentirepopu- ingfrequencyğ‘Ÿ ğ‘˜,bythetotalbroadcastingfrequencyoftheentire
lation,wecannowsumtheeffectdescribedinEq.14acrossall population(cid:205) ğ‘—ğ‘Ÿ(ğ‘—).Inotherwords,anindividualğ‘–oftypeğ‘‡ ğ‘– =ğ‘˜
individuals.Letusdenoteğ‘Ÿ(ğ‘–) thepayoffsampledbyindividualğ‘–, andpayoffsampleğ‘Ÿ ğ‘˜ âˆ¼ğ‘Ÿ(ğ‘˜)hasthesameinfluenceonallother
ğ‘ thenumberofindividualsoftypeğ‘˜,andğ‘ğœ‹ theaveragepayoff membersofthepopulation:
ğ‘˜ ğ‘˜
acrossindividualsoftypeğ‘˜.Theaggregatedupdateis:
ğ‘‘ğœ‹
ğ‘
=âˆ‘ï¸ ğ‘–ğ‘
=0
ğ‘Ÿ ğ‘(ğ‘–) (cid:40) âˆ’1 ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise
(15) ğ‘ƒ(ğ‘–)(ğ‘˜ â†Â·)=
(cid:205)
ğ‘—ğ‘Ÿ ğ‘Ÿğ‘˜
(ğ‘—)
. (17)
(cid:40)
= ğ‘1 âˆ‘ï¸
ğ‘˜
ğ‘ ğ‘˜ğ‘ ğ‘˜ğœ‹ âˆ’1 ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise Thus,theinflowfromtypeğ‘totypeğ‘˜attributabletoğ‘–is
(cid:32) (cid:33)
= ğ‘1 ğ‘ ğ‘ğ‘ ğ‘ğœ‹ (1âˆ’ğœ‹ ğ‘)âˆ’ğœ‹ ğ‘âˆ‘ï¸ ğ‘ ğ‘˜ğ‘ ğ‘˜ğœ‹
ğ‘Ÿ
= ğ‘1
(cid:32)
ğ‘ ğ‘ğ‘ ğ‘ğœ‹ âˆ’ğœ‹ ğ‘âˆ‘ï¸
ğ‘˜
ğ‘ ğ‘˜ğ‘
ğ‘˜ğœ‹ğ‘˜ (cid:33)â‰ ğ‘ ğ‘ƒ(ğ‘–)(ğ‘˜ â†ğ‘)= =ğœ‹
ğ‘1ğ‘ ğœ‹(cid:205) ğ‘ğ‘—ğ‘Ÿ
1ğ‘˜
( (cid:205)ğ‘—)
ğ‘Ÿ ğ‘˜
ğ‘Ÿ(ğ‘—)
(18)
ğ‘ ğ‘—
=ğœ‹ ğ‘ğ‘ ğ‘ğœ‹ âˆ’ğœ‹ ğ‘âˆ‘ï¸ ğœ‹ ğ‘˜ğ‘ ğ‘˜ğœ‹ (ğœ‹ ğ‘˜ = ğ‘ ğ‘ğ‘˜ ) =ğ›¼ ğ‘£ğ‘Ÿ ğ‘˜ ğœ‹ğœ‹
ğ‘
. (19)
ğ‘˜
=ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’âˆ‘ï¸ ğœ‹ ğ‘˜ğ‘ ğ‘˜ğœ‹ )
ğ‘˜ Andthetotalinflowintotypeğ‘˜attributabletoğ‘–is
=ğœ‹ ğ‘(ğ‘ ğ‘ğœ‹ âˆ’ğ‘£ğœ‹ ), (16)
whichistheTRD.
Notehow,asacorollaryofProposition1,summingtheupdate âˆ‘ï¸ ğ‘ƒ(ğ‘–)(ğ‘˜ â†ğ‘)=âˆ‘ï¸ ğ›¼ ğ‘£ğ‘Ÿ ğ‘˜ ğœ‹ğœ‹
ğ‘
(20)
fromEq.14overthepopulationexactlyyieldstheexpectationof
ğ‘â‰ ğ‘˜ ğ‘â‰ ğ‘˜
theCLupdateruledescribedinEq.1.BysummingEq.14,wehave ğ‘Ÿ
retrievedthesameupdateaswhataveragingEq.1overalarge
=ğ›¼ ğ‘£ğ‘˜ ğœ‹(1âˆ’ğœ‹ ğ‘˜) . (21)
batchwouldhaveestimated:itsexpectation,whichistheTRD.10
FromtheRLperspectivethisresultmeansthatbatchingupdates
removestheneedforusingasmalllearningrate(seeSec.5),atleast Eqs.19and21yieldanRLupdateruledescribingtheeffectofa
ingradient-freemulti-armedbanditswhereouranalysisprovides singleindividualandcorrespondingsampledpayoff(i.e.,reward
mathematicalgroundingtothiscommonlyacceptedruleofthumb. sample)overtheentirepopulation(i.e.,policy),called:
Maynard-CrossLearning(MCL).Letğ‘˜beanactionandğ‘Ÿ
ğ‘˜
âˆ¼ğ‘Ÿ(ğ‘˜)
3.3 SwarmsandMaynard-CrossLearning acorrespondingrewardsample.MCLupdatesthepolicyğœ‹ as:
Arguably,themeaningofEq.14isnon-intuitivefromthepopulation
perspective:itdescribestheeffectofasingleindividualğ‘– onthe
(cid:40)
9 saA ms pim ledpl ii ne dd ivb iy duE aq ls.1 in4 s, ta es as du om fi pn ag rat lh lea lt l, yw inhe on nep se tr ef por am cre od ssse thq eue en nt ti ia relly poo pn ulr aa tn iod no ,m thly
e
âˆ€ğ‘,ğœ‹ ğ‘ â†ğœ‹ ğ‘+ğ›¼ ğ‘£ğ‘Ÿ ğ‘˜ ğœ‹ 1 âˆ’ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ oif tğ‘ he= rwğ‘˜ ise (22)
voterrulestillyieldstheReplicatorDynamic,whichweconjecture.Weleaveaproof
ofthisconjectureforfuturework.
10Asexpectedfromthepopulationperspective,sincethisderivationisessentially
anotherproofofLemma2. whereğ‘£ğœ‹ isthecurrentvalueofpolicyğœ‹.Findingtheaggregatedpopulationeffectamountstosumming 4.1 Environment
Eq.22acrossallindividuals: Weconsiderthestandardmulti-armedstatelessbanditsettingde-
scribedinpreliminaries(seeSec.2.1).AsitisclearfromRemark1,
ğ‘‘ğœ‹
ğ‘
=âˆ‘ï¸ ğ‘–ğ‘
=0
ğ‘ğ‘Ÿ( ğ‘£ğ‘– ğœ‹) (cid:40) 1 âˆ’ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise
(23) w me enc ta sn .Tu hs ee et nh ve irs oa nm me ee nn tv ii nro cn onm se idn et rf ao tir oR nL rea tn ud rnp so rp eu wl aa rt dio sn sae mxp pe ler di-
=âˆ‘ï¸
ğ‘˜
ğ‘ ğ‘ğ‘˜ ğ‘£ğ‘ ğœ‹ğ‘˜ğœ‹ (cid:40) 1 âˆ’ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise
ğ‘„f br u ğ‘o ğœ‹tm io âˆˆnth (Ne âˆ’h âˆ(ğ‘„id ,ğ‘ğœ‹ +de âˆ,ğœn )2d ) ii ss istr tu hib s eu edt mio t eon agğ‘Ÿ ne(ğ‘ n o) ferw Nath e ,e an th nğ‘ e dsi es ğœrp 2eu w tl hl ae erd d. vA s aa rn m iao p nr lm ce esa .,l w Td hhis eet srr eei-
= ğ‘1 ğ‘£ğœ‹(ğ‘ ğ‘ğ‘ ğ‘ğœ‹ (1âˆ’ğœ‹ ğ‘)âˆ’ ğ‘˜âˆ‘ï¸ â‰ ğ‘ğ‘ ğ‘˜ğ‘ ğ‘˜ğœ‹ğœ‹ ğ‘) r
f
hue iw
dn
dca
t
er
i
nd os
n
din
ğ‘ 
s(e
tğ‘Ÿ
re i)d
b=
ut to
i1
o+b nğ‘’1e
âˆ’
ğ‘Ÿğ‘Ÿb (o ğ‘isu ).n
u
Md see odd reob one vt eğ‘Ÿw r,âˆ¼e te hNn is([ tğ‘„0 r, ağ‘ğœ‹1 n,]
ğœ
s, f2f oo
)
r,r mmw
aa
th
k
ioi ic
n
nh
g
ss qti
h
ug
i
em
s
eto
zh
eid
e
s
= ğ‘1 ğ‘£ğœ‹(ğ‘ ğ‘ğ‘ ğ‘ğœ‹ âˆ’âˆ‘ï¸
ğ‘˜
ğ‘ ğ‘˜ğ‘ ğ‘˜ğœ‹ğœ‹ ğ‘) ğ‘N ğ‘ğœ‹a âˆˆnd [0s ,h 1if ]t .s Tth he ism
ğ‘
ğ‘e ğœ‹an caa nw ba ey efr so tim mğ‘  a( tğ‘„ edğ‘ğœ‹ a) sto Ea ğ‘Ÿâˆ¼n ğ‘Ÿe (w ğ‘)m [ğ‘Ÿe ],an byde sn amot ped linb gy
= ğ‘£1 ğœ‹(ğœ‹ ğ‘ğ‘ ğ‘ğœ‹ âˆ’ğœ‹ ğ‘âˆ‘ï¸ ğœ‹ ğ‘˜ğ‘ ğ‘˜ğœ‹ ) a inl gar thge emn .u Fm ub rte hr eo r,f ts ha rm eep dle iffs e( r1 e0 n7 ts ka im ndp sle os f) ef nro vm iroğ‘Ÿ n( mğ‘) ena tn sd ara ev uer sa eg d-
,
=
ğœ‹
ğ‘ (ğ‘ğœ‹ğ‘ âˆ’ğ‘£ğœ‹ )
ğ‘˜
(24)
whereâˆ€ğ‘:ğ‘ ğ‘ğœ‹â€™sarenear0,spreadbetween0and1,ornear1.
ğ‘£ğœ‹
4.2 RLexperiments
whichistheMRD.
Non-batched:Intheseexperiments,anRLagentstartswithan
We have shown that a population whose individuals follow
initialrandompolicyğœ‹.Theagentthensamplesonlyoneactionğ‘˜
ğ‘…wvoter aggregatestotheMRD.AnargumentsimilartoSec.3.2
from ğœ‹ in an iterative fashion. Further, pulling actionğ‘˜ in the
yieldstheâ€œbatchedâ€versionofEq.22,thatwecallExactMaynard-
CrossLearning(EMCL):11
environment,theagentreceivesanoisyrewardsignalğ‘Ÿ
ğ‘˜
âˆ¼ğ‘Ÿ(ğ‘˜).
ForCL,theagentutilizesEq.14tomakeanupdate.Whereasfor
(cid:40) MCL,Eq.22cannotbeuseddirectly,sincewedonothaveaccess
âˆ€ğ‘,ğœ‹ ğ‘ â†ğœ‹ ğ‘+E ğ‘˜,ğ‘Ÿğ‘˜ğ‘£ğ‘Ÿ ğ‘˜ ğœ‹ 1 âˆ’ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜ ise (25) t oo vğ‘£ eğœ‹ r. rW ewe at rh de sr ,e wfo hre e, ra ep ğ›¾p ir sox ai wm ea it ge hğ‘£ tğœ‹ inb gy fe am ctp ol roy foin rg rea cm eno tv rin ewga av rder sa :ge
EMCListheRLalgorithmfollowedbyswarmsofbeesthatmakea ğ‘ŸÂ¯â†ğ›¾ğ‘Ÿ+(1âˆ’ğ›¾)ğ‘ŸÂ¯. (27)
collectivedecisionviağ‘…wvoter:
Moreover,sincethisupdaterulecanmakeğœ‹ invalid,i.e.,compo-
Proposition2. Aninfinitepopulationofindividualsfollowing nentscouldbecomenegativeoraboveone(seefootnote11),we
ğ‘… wvotercanequivalentlybeseenasanRLagentfollowingEMCL clampğœ‹ between0and1:
(cid:32) (cid:40) (cid:33)
ğ‘‘wvoterğœ‹ ğ‘ =ğ‘‘EMCLğœ‹ ğ‘, (26) âˆ€ğ‘:ğœ‹
ğ‘
â†clamp ğœ‹ ğ‘+ğ‘Ÿ ğ‘ŸÂ¯ğ‘˜ âˆ’1 ğœ‹âˆ’ ğ‘ğœ‹ ğ‘ i of tğ‘ he= rwğ‘˜
ise
(28)
whereğœ‹isboththepopulationvectorandthepolicyunderthepopulation-
policyequivalence,ğ‘‘wvoterğœ‹ isthesingle-stepchangeofpopulationğœ‹ Wecallthistrainingsteparun,andperformğ‘…runsperseed.
undertheweightedvoterrule(i.e.,thechangeintypeproportionsafter Batched:Intheseexperiments,weimplementthebatchedvari-
allindividualssimultaneouslyperformğ‘… wvoteronce),andğ‘‘EMCLğœ‹ is antsofupdaterulesCL(Eq.2)andMCL(Eq.25),henceforthnamed
theupdateperformedbyEMCLonthepolicyğœ‹. B-CLandB-MCL.B-CLisastraightforwardbatchingoftheCL
updaterule,averagingoverabatchofğµsamplesinsteadofonesam-
Proof. TheproofofProposition2istrivialatthispoint.Wehave pletoupdateğœ‹.Whereas,withB-MCL,ğ‘£ğœ‹ isnolongeramoving
alreadyshownthatğ‘‘wvoterğœ‹ istheMRD,anddividingeverything averageoftherewardsbutratherthemeanofbatchrewards.We
byğ‘£ğœ‹ intheproofofLemma1(startingfromEq.8)yieldsthat alsoneedtoexplicitlyclampthepolicybetween0and1toensure
ğ‘‘EMCLğœ‹ isalsotheMRD. â–¡ itremainsvalid.B-MCLalsousesğµsamplessimultaneouslysimilar
toB-CL.Similartonon-batchedexperiments,weperformğ‘…runs
perseed.
4 METHODS
Wepresenttwotypesofexperimentstovalidatethefindingsfrom
4.3 PopulationExperiments
theprevioussection.First,weimplementthetwoRLupdaterules,
CLandMCLintwovariants:batchedandnon-batched.Second,we Inthissection,wefocusonthepopulationupdaterules,VR,and
conductpopulation-basedexperimentsusingğ‘…voterandğ‘…wvoter(VR, WVR(seepreliminariesSecs.2.3and2.2).Sincewecannotsimu-
WVR)fordifferentpopulationsizes.Moreover,wealsonumerically latePforinfinitesizes,wechoosetwofinitepopulationsizesof
simulatetheTRDandMRDtoshowhowtheaboveexperiments 10and1000.ForbothVRandWVR,westartwithanequalpro-
comparewiththeanalyticalsolutions. portionofindividualsassociatedwithanytype/opinion.Further,
eachindividualreceivesastochasticpayoff/qualityestimateforits
type/opinion.Thereafter,withVR,everyoneispairedwithanother
11MCLhasavalidimplementationonlywhenthelearningrateğ›¼issmallenough,
randomindividual.Allindividualsthengeneratearandomnumber
whileEMCLhasavalidimplementationwhenthebatch-sizeğ‘usedtoestimatethe
expectationislargeenough.Inbothcases,ğ‘£ğœ‹alsoneedstobeestimated. between0and1,andiftherandomnumberisgreaterthanthepayoffofthepairedindividual,theyswitchtotheirpartnerâ€™stype ConvergencerateofMRDisâ‰¥TRD.Asnotedby[15],TRDand
(rule3ofğ‘…voter).WhereaswithWVR,eachindividualswitchesto MRDcanberearrangedintheform:
anopinionsampledfromthedistributiondefinedbyğ‘£,whereğ‘£ ğœ‹ ğ‘ğœ‹
istheratioofvotesfortypeğ‘–bythetotalnumberofvotesinthe ğœ‹(cid:164)ğ‘ =ğ‘£ğœ‹ ( ğ‘£ğ‘ ğœ‹ğ‘ âˆ’ğœ‹ ğ‘) (TRD) (32)
population:12
ğœ‹ ğ‘ğœ‹
ğ‘£ ğ‘– =
âˆ€ğ‘âˆˆP(cid:205) (cid:205):ğ‘‚ğ‘ ğ‘Ÿ= ğ‘ğ‘–ğ‘Ÿ ğ‘
. (29)
ğœ‹(cid:164)beingtheuğœ‹ p(cid:164)ğ‘ da= te1 "( spğ‘£ğ‘ eğœ‹ eğ‘ d"âˆ’ anğœ‹ dğ‘ ğ‘£)
ğœ‹
beingbound( eM dR bD et) ween0an( d33 1)
.
âˆ€ğ‘âˆˆP TheMRDspeedisthusgreaterthantheTRDspeed.Empirically,we
SimilartoRLexperiments,weperformğ‘…runsperseed. observethatMRDconvergesfasterthanTRD,especiallywhenthe
ğ‘ğœ‹â€™sarecloseto0,asseeninthefirstcolumnofFigure1.Whereas,
ğ‘
4.4 TRDandMRD whentheğ‘ ğ‘ğœ‹â€™sarenear1thereisnovisibledifference(asğ‘£ğœ‹ â‰ˆ1).
ToempiricallyvalidatePropositions1and2,wenumericallysimu- Byextension,thisalsoimpliesthatMCL(forsmallğ›¼),B-MCL(for
lateboththedifferentialequations3(TRD)and4(MRD).Asthese largebatchsize),andWVR(forlargepopulation)haveconvergence
equationsarecontinuous,wediscretizethembyastepğ›¿(discretiz- ratesâ‰¥CL,B-CL,andVRrespectively.
ingstep).Further,westartfromaninitialrandompopulation/policy PopulationexperimentswithVRandWVRfollowTRDand
(ğœ‹)andsimulateitsevolutionaccordingtoTRDandMRDbetween MRDrespectivelyforlargepopulations.ItcanbeseeninFigure
timeintervals[0,ğ‘¡ ğ‘“],usingtheprivilegedinformationğ‘ ğ‘ğœ‹ notavail- 3thatbothVRandWVRfollowTRDandMRDrespectivelywhen
abletoRLandpopulationexperiments. thepopulationsizeislarge.SeesupplementarySec.A.2forother
ğœ‹ ğ‘ â†ğœ‹ ğ‘+ğ›¿ğœ‹ ğ‘[ğ‘ ğ‘ğœ‹ âˆ’âˆ‘ï¸ ğœ‹ ğ‘™ğ‘ ğ‘™ğœ‹ ] (30) e stn av ri tr do en vm iae tn ints g.A fros mso to hn ea as nt ah lye tp ico ap lu sl oa lt ui to in ons .h Sr ii mnk ils a, rV tR ota hn ed RW LV exR -
ğ‘™ periments,WVRperformspoorlyincomparisontoVRforsmall
ğœ‹ ğ‘ â†ğœ‹ ğ‘+ğ›¿ ğ‘£ğœ‹ ğœ‹ğ‘ [ğ‘ ğ‘ğœ‹ âˆ’âˆ‘ï¸ ğœ‹ ğ‘™ğ‘ ğ‘™ğœ‹ ] (31) populationsizes.
ğ‘™ Finally,fromthediscussionsrelatedtobatchedRLupdatesand
populationexperiments,weempiricallyvalidateProposition1and
5 RESULTS Proposition2.
Forallexperiments,weusethehyperparametersdescribedinsup-
plementarySec.A.3. 6 CONCLUSIONS
WithPropositions1and2,wehavedemonstratedhowRDisthe
Non-batchedRLupdaterulesCLandMCLfollowTRDand underlyingconnectionbetweenReinforcementLearningandCol-
MRDrespectivelywhenthelearningrate(ğ›¼)issmall.These lectiveDecision-Making.Further,wehaveempiricallyvalidated
resultsarepresentedinFigures1and2.Forallenvironments,CL thiscorrespondence.Thiscorrespondenceopensabridgebetween
andMCLfollowTRDandMRDrespectively,whichcanbeexplicitly thesetwofields,enablingtheflowofideas,newperspectives,and
seenwiththedottedlineoftheanalyticalsolutions(TRD,MRD)ex- analogiesforboth.Forexample,itcanbeseenthatCrossLearn-
actlyatthecenteroftheaveragerewardcurvesoftheCLandMCL ing,Maynard-CrossLearning,and,moregenerally,Reinforcement
updaterules.Thisempiricallyvalidatesthat,withasmallğ›¼,Eqs.14 Learningtakeasingle-agentperspective,whereinformationfrom
and22followtheTRDandMRDrespectively,evenwheniteratively consecutiveactionsamples/batchesisaccumulatedintoonecentral-
appliedwithsinglesamples.However,assoonasğ›¼isincreased,CL izedagentâ€™spolicy.Ontheotherhand,ğ‘…votersandğ‘…wvoterstakea
andMCLstartdeviatingfromtheirrespectiveanalyticalsolutions multi-agentperspective,whereeveryindividualimplementssimple
(seeFigure2).Thisisawell-knowneffectinoptimizationliterature localanddecentralizedrules,makingindependentdecisions,which
butfromapopulationperspective(seeSec.3.2)weseethatalarger leadstoanemergentcollectivepolicy.
ğ›¼ correspondstoasmallerpopulation,henceleadingtoapoor SignificanceforRL.SimilartohowwediscoveredanewRL
approximationoftheexpectedupdate.Further,wealsoobserve updaterule(i.e.,Maynard-CrossLearning)fromSwarmIntelligence,
thatMCLperformspoorlycomparedtoTRDwithlargerğ›¼. otherideassuchasmajorityrule[22],andcross-inhibition[13],
BatchedRLupdaterulesB-CLandB-MCLfollowTRDand canbeusedtocreatenewupdaterulesforReinforcementLearn-
MRDrespectivelywhenthebatchsize(ğµ)islargeenough.As ing.Moreover,SwarmIntelligencealgorithmsareofteninspiredby
seeninFigure4,itisclearthatB-CLandB-MCLfollowTRDand nature,andthusrequireindividualstofollowphysics.Thisman-
MRDrespectivelyforlargebatchsizes(thiscanbeseenfromhow datespracticalconstraintssuchascongestion[19],communication,
theanalyticalsolutionisexactlyatthecenteroftheaveragere- andfinitesizeeffects,whicharegenerallyignoredinReinforce-
wardcurvesofB-CLandB-MCL).However,assoonwemakethe mentLearningandpopulationgames.Comparingtheperformance
batchsizessmaller,thebatchedupdatesdeviatefromtheiranalyti- ofReinforcementLearningagentswiththeirequivalentswarm
calsolutions(seesub-section3.2).SeesupplementarySec.A.1for counterpartsonsuchconstraintsisadirectionforfuturework.
otherenvironments.WealsoobservethatB-MCLperformspoorly SignificanceforSI.Thepopulation-policyequivalencehigh-
comparedtoB-CLwithsmallerbatchsizes,similartoobservations lightshowcertainSwarmIntelligencemethodsareequivalentto
madewithnon-batchedRLexperiments. single-agentReinforcementLearning,demonstratingagencyofthe
entireswarmasasinglelearningentity.Therefore,onecouldimag-
12Weimplementthethirdruleofğ‘…wvoterinacentralizedfashionforthesesimple
numericalsimulations,butinreality,itisacompletelydecentralizedrule. inethatMulti-AgentReinforcementLearningwouldsimilarlyyieldRLnon-batchedexperiments-qÏ€â€™s: nearzero,Î±: 0.001
a
0.32
100
0.26
80
0.20
60
0.13
40
0.07
20
0.01
-0.05 0
RLnon-batchedexperiments-qÏ€â€™s: evenlyspaced,Î±: 0.001
a
1.03
100
0.87
80
0.70
60
0.53
40
0.36
20
0.20
0.03 0
RLnon-batchedexperiments-qÏ€â€™s: nearone,Î±: 0.001
a
1.08 100
1.01 80
0.94
60
0.87
40
0.80
20
0.73
0.66 0
0 20 40 60 80 100 0k 20k 40k 60k 80k 100k 0k 20k 40k 60k 80k 100k 0k 20k 40k 60k 80k 100k
Time Runs Runs Runs
TRD MRD MCL CL
Figure1:Resultsfornon-batchedRLexperimentswithsmallğ›¼.Thedottedlinesrepresentthemeanrewardaccordingtothe
analyticalsolutions.Thedarkershadesrepresentthemeanreward(orpercentageofoptimalactions)andthelightershade
representstheirvariancefortheCLandMCLupdaterules.Astherewardscalesaredifferentacrossenvironments(rows),itis
importanttolookattheratioofoptimalactions(lastcolumn)
RLnon-batchedexperiments-qÏ€â€™s: betweenzeroandone,Î±: 0.1
a
1.01 100
0.86
80
0.71
60
0.56
40
0.40
20
0.25
0.10 0
0 20 40 60 80 100 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Time Runs Runs Runs
TRD MRD MCL CL
Figure2:Resultsfornon-batchedRLexperimentsforlargeğ›¼.Itisclearthatwithalargeralpha,theRLsolutionsdivergefrom
theanalyticalsolutions.
draweR
draweR
draweR
draweR
%Optimalactions
%Optimalactions
%Optimalactions
%OptimalactionsPopulationExperimentsqÏ€â€™s: betweenzeroandone,size=10
a
0.98 100
0.87
80
0.77
60
0.67
0.56 40
0.46 20
0.36
PopulationExperimentsqÏ€â€™s: betweenzeroandone,size=1000
a
0.94
100
0.85
80
0.76
60
0.67
0.58 40
0.49 20
0.40
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD VR WVR
Figure3:Resultsforpopulationexperiments.Thedottedlinesrepresenttheaveragepayoff/qualityaccordingtotheanalytical
solutions.Thedarkershadesrepresentthemeanpayoff/qualityand%ofpopulationswiththeoptimaltype/opinion,whilethe
lightershaderepresentsthevarianceinpayoffwithVRandopinionwithWVR.
RLbatchedexperimentsqÏ€â€™s: betweenzeroandone,batchsize=10
a
0.98 100
0.87
80
0.75
60
0.64
0.52 40
0.41 20
0.29
RLbatchedexperimentsqÏ€â€™s: betweenzeroandone,batchsize=1000
a
0.94
100
0.85
80
0.76
60
0.67
0.58 40
0.49 20
0.40
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD B-CL B-MCL
Figure4:ResultsforbatchedRLexperiments.Thedottedlinesrepresenttheaveragerewardaccordingtotheanalyticalsolutions.
Thedarkershadesrepresentthemeanrewardand%optimalaction,whilethelightershaderepresentsthevarianceinrewards
withB-CLandB-MCL.
ytilauq/ffoyaP
ytilauq/ffoyaP
draweR
draweR
%Optimaltype/opinion
%Optimaltype/opinion
%Optimalaction
%Optimalactionequivalentmulti-swarmsystems,wheretwoormorecoexisting 05.004
swarmswouldcompete/collaborateforresources(i.e.,prisoners [13] AndreagiovanniReina,JamesA.R.Marshall,VitoTrianni,andThomasBose.
dilemma,hawkdove,etc).Further,ideasthatempowerReinforce-
2017.Modelofthebest-of-Nnest-siteselectionprocessinhoneybees.Physical
ReviewE95,5(May2017). https://doi.org/10.1103/physreve.95.052411
mentLearning,couldbeportedtoswarmintelligenceandswarm [14] AndreagiovanniReina,ThierryNjougouo,ElioTuci,andTimoteoCarletti.2024.
robotics. Speed-accuracytrade-offsinbest-of-ğ‘›collectivedecisionmakingthroughhet-
erogeneousmean-fieldmodeling.Phys.Rev.E109(May2024),054307.Issue5.
https://doi.org/10.1103/PhysRevE.109.054307
REFERENCES [15] WilliamHSandholm.2010.Populationgamesandevolutionarydynamics.MIT
press.
[1] PeterAuer,NicolÃ²Cesa-Bianchi,andPaulFischer.2002. Finite-timeAnalysis
[16] WilliamH.Sandholm,EminDokumacÄ±,andRatulLahkar.2008.Theprojection
o hf ttt ph se ://M dou il .oti ra gr /m 10e .d 10B 2a 3n /Adi :1t 0P 1r 3o 6b 8l 9e 7m 0. 43M 52achineLearning47(052002),235â€“256. dynamicandthereplicatordynamic.GamesandEconomicBehavior64,2(2008),
666â€“683. https://doi.org/10.1016/j.geb.2008.02.003 SpecialIssueinHonorof
[2] DaanBloembergen,KarlTuyls,DanielHennes,andMichaelKaisers.2015.Evo-
MichaelB.Maschler.
lutionaryDynamicsofMulti-AgentLearning:ASurvey. JournalofArtificial [17] JaeminSeo,SangKyeunKim,AzarakhshJalalvand,RoryConlin,AndrewRoth-
IntelligenceResearch53(082015),659â€“697. https://doi.org/10.1613/jair.4818
stein,JosephAbbate,KeithErickson,JosiahWai,RicardoShousha,andEgemen
[3] EricBonabeau,MarcoDorigo,andGuyTheraulaz.1999.SwarmIntelligence:From
Kolemen.2024.Avoidingfusionplasmatearinginstabilitywithdeepreinforce-
N osa ot /u 9r 7a 8l 0t 1o 9A 51rt 3i 1fi 5c 8ia 1l .0S 0y 1s .t 0e 0m 0s 1.OxfordUniversityPress. https://doi.org/10.1093/ mentlearning.Nature626(022024),746â€“751. https://doi.org/10.1038/s41586-
024-07024-9
[4] ClaudioCastellano,SantoFortunato,andVittorioLoreto.2009.Statisticalphysics
ofsocialdynamics.ReviewsofModernPhysics81,2(May2009),591â€“646. https: [18] J Uo nh in veM rsa ity yn Pa rr ed ssS ,m Ci at mh. b1 r9 id8 g2 e. ,UEv Ko .lutionandtheTheoryofGames. Cambridge
//doi.org/10.1103/revmodphys.81.591
[19] KarthikSoma,VivekShankarVardharajan,HeikoHamann,andGiovanniBel-
[5] JohnG.Cross.1973.AStochasticLearningModelofEconomicBehavior.The
trame.2023.CongestionandScalabilityinRobotSwarms:AStudyonCollective
oQ ru ga /r Rt ee Prl Ey cJ :oou ur pn :qa jl eo cf oE nc :vo :n 8o 7m :yi :c 1s 98 77 3, :i2 :2( :p19 :27 33 9) -, 22 63 69 .â€“266. https://EconPapers.repec. DecisionMaking.In2023InternationalSymposiumonMulti-RobotandMulti-
[6] MarcoDorigo,MauroBirattari,andThomasStutzle.2006. Antcolonyopti-
AgentSystems(MRS).199â€“206. https://doi.org/10.1109/MRS60187.2023.10416793
/m /diz oa i.t oio rgn /. 10IE .1E 1E 09C /o Mm Cp Iu .2t 0a 0ti 6o .3n 2a 9l 6In 91telligenceMagazine1,4(2006),28â€“39. https:
[20] R duic ch tia or nd .S MS ITut pto rn esa s.ndAndrewGBarto.2018.Reinforcementlearning:Anintro-
[21] PeterD.TaylorandLeoB.Jonker.1978.Evolutionarystablestrategiesandgame
[7] MarcoDorigo,GuyTheraulaz,andVitoTrianni.2021. SwarmRobotics:Past,
Present,andFuture[PointofView].Proc.IEEE109,7(2021),1152â€“1165. https: 1d 0y 1n 6a /m 00i 2c 5s. -5M 56a 4th (7e 8m )9a 0t 0ic 7a 7l -B 9iosciences40,1(1978),145â€“156. https://doi.org/10.
//doi.org/10.1109/JPROC.2021.3072740
[22] GabrieleValentini,EliseoFerrante,HeikoHamann,andMarcoDorigo.2016.Col-
[8] HeikoHamann.2018.SwarmRobotics:AFormalApproach. https://doi.org/10.
lectivedecisionwith100Kilobots:speedversusaccuracyinbinarydiscrimination
1007/978-3-319-74528-2 problems. AutonomousAgentsandMulti-AgentSystems30,3(2016),553â€“580.
[9] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Mueller, https://doi.org/10.1007/s10458-015-9323-3
VladlenKoltun,andDavideScaramuzza.2023. Champion-leveldroneracing [23] GabrieleValentini,HeikoHamann,andMarcoDorigo.2014. Self-organized
usingdeepreinforcementlearning. Nature620(082023),982â€“987. https: collectivedecisionmaking:theweightedvotermodel.InProceedingsofthe2014
//doi.org/10.1038/s41586-023-06419-4 InternationalConferenceonAutonomousAgentsandMulti-AgentSystems(Paris,
[10] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis France)(AAMASâ€™14).InternationalFoundationforAutonomousAgentsand
Antonoglou,DaanWierstra,andMartinA.Riedmiller.2013. PlayingAtari MultiagentSystems,Richland,SC,45â€“52.
withDeepReinforcementLearning.CoRRabs/1312.5602(2013).arXiv:1312.5602 [24] PKirkVisscherandScottCamazine.1999.Collectivedecisionsandcognitionin
http://arxiv.org/abs/1312.5602 bees.Nature397,6718(February1999),400. https://doi.org/10.1038/17047
[11] MartinA.Nowak.2006. FiveRulesfortheEvolutionofCooperation. Sci- [25] RonaldJWilliams.1992.SimpleStatisticalGradient-FollowingAlgorithmsfor
ence 314, 5805 (2006), 1560â€“1563. https://doi.org/10.1126/science.1133755 ConnectionistReinforcementLearning.,229-256pages.
arXiv:https://www.science.org/doi/pdf/10.1126/science.1133755 [26] HaoxiangXia,HuiliWang,andZhaoguoXuan.2011. Opiniondynamics:A
[12] SidneyRedner.2019. Reality-inspiredvotermodels:Amini-review. Comptes multidisciplinaryreviewandperspectiveonfutureresearch.InternationalJournal
Rendus.Physique20,4(May2019),275â€“292. https://doi.org/10.1016/j.crhy.2019. ofKnowledgeandSystemsScience(IJKSS)2,4(2011),72â€“91.A SUPPLEMENTARYMATERIAL
A.1 BatchedRLexperiments
RLbatchedexperimentsqaÏ€â€™s:nearzero,batchsize=1000
0.18
100
0.15
80
0.12
60
0.09
0.06 40
0.03 20
-0.00
RLbatchedexperimentsqaÏ€â€™s:nearone,batchsize=1000
1.03 100
1.00
80
0.96
60
0.93
0.90 40
0.86 20
0.83
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD B-CL B-MCL
ğ‘ğ‘–
Figure5:BatchedRLfiguresforlargebatchsizeandthetwootherenvironmentswhereğ‘ ğ‘ â€™sarenearzeroandnearone.
A.2 Populationexperiments
PopulationExperimentsqaÏ€â€™s:nearzero,size=1000
0.18
100
0.15
80
0.12
60
0.09
0.06 40
0.03 20
-0.00
PopulationExperimentsqaÏ€â€™s:nearone,size=1000
1.03 100
1.00
80
0.97
60
0.93
0.90 40
0.87 20
0.83
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD VR WVR
ğ‘ğ‘–
Figure6:Populationexperimentsforlargepopulationsizeandthetwootherenvironmentswhereğ‘ ğ‘ â€™sarenearzeroandnear
one.
draweR
draweR
ytilauq/ffoyaP
ytilauq/ffoyaP
%Optimalaction
%Optimalaction
%Optimaltype/opinion
%Optimaltype/opinionA.3 Hyperparametrs
Inthissection,welistoutvarioushyperparametersusedbyRLandpopulationexperiments.
Hyperparameter value
Arms(ğ‘›) 10
Learningrate(ğ›¼) {0.001,0.1}
Epochs(ğ¸) 100
Variance(ğœ2) 1
Runs(ğ‘…) {1000000,1000}
Weightfactor(ğ›¾) 0.01
Discretizingfactor(ğ›¿) ğ›¼
finaltime(ğ‘¡ ğ‘“) ğ›¼Ã—ğ‘…=100
Table1:HyperparametersusedforRLnon-batchedexperiments
Hyperparameter value
Arms(ğ‘›) 10
Epochs(ğ¸) 100
Variance(ğœ2) 1
Runs(ğ‘…) 100
Batchsize(ğµ) {10,1000}
Discretizingfactor(ğ›¿) 1
finaltime(ğ‘¡ ) ğ‘…=100
ğ‘“
Table2:HyperparametersusedforRLbatchedexperiments
Hyperparameter value
Types/opinions(ğ‘›) 10
Epochs(ğ¸) 100
Variance(ğœ2) 1
Runs(ğ‘…) 100
populationsize(ğµ) {10,1000}
Discretizingfactor(ğ›¿) ğ›¼
finaltime(ğ‘¡ ) ğ‘…=100
ğ‘“
Table3:Hyperparametersusedforpopulationexperiments