FREEVS: GENERATIVE VIEW SYNTHESIS ON FREE
DRIVING TRAJECTORY
QitaiWang1,2,LueFan2,3,YuqiWang1,2,YuntaoChen4(cid:66),ZhaoxiangZhang1,2,4(cid:66)
1SchoolofFutureTechnology,UniversityofChineseAcademyofSciences(UCAS),
2NLPR,MAIS,InstituteofAutomation,ChineseAcademyofSciences(CASIA),
3CUHK4CenterforArtificialIntelligenceandRobotics,HKISI,CAS
{wangqitai2020, lue.fan, wangyuqi2020, zhaoxiang.zhang}@ia.ac.cn,
chenyuntao08@gmail.com
ProjectPage:https://freevs24.github.io/
ABSTRACT
Existing reconstruction-based novel view synthesis methods for driving scenes
focus on synthesizing camera views along the recorded trajectory of the ego ve-
hicle. Their image rendering performance will severely degrade on viewpoints
falling out of the recorded trajectory, where camera rays are untrained. We pro-
poseFreeVS,anovelfullygenerativeapproachthatcansynthesizecameraviews
on free new trajectories in real driving scenes. To control the generation results
tobe3Dconsistentwiththerealscenesandaccurateinviewpointpose,wepro-
posethepseudo-imagerepresentationofviewpriorstocontrolthegenerationpro-
cess. Viewpointtransformationsimulationisappliedonpseudo-imagestosimu-
late camera movement in each direction. Once trained, FreeVS can be applied
to any validation sequences without reconstruction process and synthesis views
on novel trajectories. Moreover, we propose two new challenging benchmarks
tailoredtodrivingscenes,whicharenovelcamerasynthesisandnoveltrajectory
synthesis, emphasizing the freedom of viewpoints. Given that no ground truth
images are available on novel trajectories, we also propose to evaluate the con-
sistencyof imagessynthesized onnovel trajectorieswith 3Dperception models.
Experiments on the Waymo Open Dataset show that FreeVS has a strong image
synthesisperformanceonboththerecordedtrajectoriesandnoveltrajectories.
EmerNerf Street Gaussians HUGS Ours
n
ig
irO
m
0
.1
th
g
iR
m
0
.1
p
U
Figure1: SynthesisresultscomparisonontheWaymoOpenDataset(Sunetal.,2020).Weshow
thecameraviewssynthesizedbyNVSmethodsontheoriginalfrontview(firstrow),viewpoint1.0
m to the right (second row), and viewpoint 1.0 m above (third tow). Our method significantly
outperformspreviousNVSmethodsonviewpointoutsidetheexistingegotrajectory.
1
4202
tcO
32
]VC.sc[
1v97081.0142:viXra1 INTRODUCTION
Scenereconstructionandnovelviewsynthesis(NVS)havegainedspecialattentioninembodiedAI
due to their potential to develop closed-loop simulations for embodied systems. Recent advances
have led to remarkable improvements in the reconstruction quality of general scenes using multi-
passandmulti-viewrecordings.However,reconstructingdrivingscenespresentsdistinctchallenges
duetothesparseobservationsinherentintheirlesscontrolled,real-worldrecordingconditions.
Unlikegeneralscenereconstructionsettings,whichtypicallyleverageexcessiveviewssurrounding
thescene, drivingscenereconstructiongenerallyonlyhasaccesstoimageviewsalongthesingle-
pass ego driving trajectory. This limitation raises an important question: How well does driving
scenereconstructionperformfornovelviewpointsoutsidetherecordedtrajectory?
Currently, existing driving scene NVS works(Guo et al., 2023; Wu et al., 2023c; Xie et al., 2023;
Turkietal.,2023;Yangetal.,2023a;Zhouetal.,2024b;Chenetal.,2023;Yanetal.,2024)only
evaluate their image rendering quality along the recorded trajectory, leaving this question largely
unanswered. As shown in Fig. 1, the quality of rendering results of existing representative NVS
methodsdegradesdrasticallywhentherenderingcameramovesawayfromitsrecordingtrajectory.
Thisisbecause,indrivingscenes,recordedcameraviewpointsaresparsein3Dspaceandhomoge-
neousintheirpositionsalongtherecordedtrajectories. Thesparsityandhomogeneityofrecorded
cameraviewscausethecameraraysshootingfromthecameracentersonnoveltrajectorieslargely
untrained.
We propose FreeVS to address this issue, which is a fully generative NVS method that can syn-
thesizehigh-qualitycameraviewsinsideandbeyondtherecordeddrivingtrajectory. Wefacetwo
corechallengeswhenbuildingtheFreeVS.Thefirstchallengeisaccuratelycontrollingthecamera
poses while maintaining the 3D geometry consistency of the generated views. Although previous
diffusion-based methods(Wang et al., 2023a; Lu et al., 2023b; Hu et al., 2023; Wang et al., 2024;
Yangetal.,2024)arecapableofcontrollingthecameramotioninacoarsetrajectory,theircontrol
precision is far from enough for safety-critical simulation purposes. The second challenge is the
ground truth images in the novel trajectories are unavailable, making it difficult to directly train a
modeltosynthesizenovelviewsbeyondrecordedtrajectories.
Totacklethetwochallenges,theproposedFreeVSleveragespseudo-imagerepresentation,asparse
yetaccuraterepresentationof3Dscenesobtainedthroughcolored3Dpointsprojection.Specifically,
for each existing view, we create its pseudo-image counterpart by projecting colored point clouds
intothisview. Herethecoloredpointscanbeeasilyobtainedbyprojectingpointcloudstoanyvalid
images. Inthisway,weobtaintrainingdatapairstotrainagenerativemodelthatcangenerateareal
image from its pseudo-image counterpart. Since we create the pseudo images using ground truth
cameramodels,theycontainsparsebuthighlyaccurateappearanceandgeometry,sidesteppingthe
tough challenge of accurately controlling the camera poses. At inference time, we could create a
pseudo-image for a novel viewpoint beyond the recorded trajectory and then synthesize the novel
viewusingthetrainedgenerativemodel.Thisdesigngreatlynarrowedthegapbetweensynthesizing
viewsinsideandbeyondtherecordedtrajectory.
To reveal the practicality of FreeVS, we propose two challenging benchmarks for evaluating the
performance of NVS methods in driving scenes, which is more practically meaningful than the
conventionalevaluationontherecordedtrajectories.(i)Ontherecordedtrajectories,weproposethe
novelcamerasynthesisbenchmark. Insteadofevaluatingsynthesisresultsontestframessampled
at intervals fromvideo sequences (i.e. novelframe synthesis), we propose todrop all images of a
certaincameraview(e.g. thefront-sideview)inthewholetrajectoryandsynthesizetheimagesof
thedroppedcameraview.(ii)Wefurtherproposethenoveltrajectorysynthesisbenchmark.Withno
groundtruthimagesavailableonnoveltrajectories,weproposetoevaluatethegeometryconsistency
ofsynthesizedviewsthroughtheperformanceofoff-the-shelf3Ddetectors. Theexperimentsonthe
Waymo Open Dataset (WOD) demonstrate that FreeVS outperforms previous NVS methods by a
largemargininthetwomorepracticalbenchmarksaswellasinthetraditionalnovelframesynthesis
benchmark.
Ourcontributionsaresummarizedasfollows:
21. WeproposeFreeVS,afullygenerativeviewsynthesismethodfordrivingscenesthatgen-
eratehigh-quality3D-coherentnovelviewsbothforrecordedandnoveltrajectorieswithout
time-costreconstruction.
2. WedevisetwonewbenchmarksforevaluatingdrivingNVSmethodsonnoveltrajectories
beyondrecordedones.
3. Experiments on WOD show that FreeVS achieves leading performance on synthesizing
cameraviewsinsideandbeyondtherecordedtrajectory.
2 RELATED WORK
2.1 NOVELVIEWSYNTHESISTHROUGHRECONSTRUCTION
Recently,rapidprogresshasbeenachievedinnovelviewsynthesisthrough3Dreconstructionand
radiance field rendering. Neural Radiance Fields (NeRF) (Mildenhall et al., 2020) utilizes multi-
layerperceptronstorepresentcontinuousvolumetricscenesandachieveabreakthroughinrendering
quality.ManyworkshaveextendedNeRFtounbounded,dynamicurbanscenes(Tanciketal.,2022;
Barronetal.,2022;Ostetal.,2022;Rematasetal.,2022;Turkietal.,2022;Luetal.,2023a;Guo
et al., 2023; Liu et al., 2023a; Wu et al., 2023c; Xie et al., 2023; Turki et al., 2023; Yang et al.,
2023b;Wangetal.,2023b;Ostetal.,2021;Tonderskietal.,2024).AuthorsofMapNeRF(Wuetal.,
2023a)noticedtheproblemofNeRFingeneratingextrapolatedviewsandproposedincorporating
mappriorstoguidethetrainingofradiancefields. 3DGaussianSplatting(Kerbletal.,2023)(3D
GS) models scenes with numerous 3D Gaussians. Under this explicit representation, 3D GS can
model scenes with significantly fewer parameters, while achieving faster rendering and training
withsplat-basedrasterization.3DGSisoriginallydesignedforstaticandboundedscenes.Recently,
some researchers have extended 3D GS to dynamic scenes (Luiten et al., 2024; Wu et al., 2023b;
Yang et al., 2023d;c) and driving scenes (Zhou et al., 2024b; Chen et al., 2023; Yan et al., 2024).
HUGS(Zhouetal.,2024a)furtherjointlymodelthegeometry,appearance,motion,andsemantics
in3Dscenesforbettersceneunderstanding.
2.2 NOVELVIEWSYNTHESISTHROUGHGENERATION
Novel view synthesis through image generation has greatly benefitted from the advancements in
diffusionmodels(Hoetal.,2020;Songetal.,2020;Rombachetal.,2022;Blattmannetal.,2023).
Zero-1-to-3(Liuetal.,2023b)andZeroNVS(Liuetal.,2023b)generatenovelviewswithadiffusion
processconditionedonthereferenceimageandthetargetcameraposeembeddedasatextembed-
ding.GeNVS(Chanetal.,2023)conditionthediffusionprocessonvolume-renderedfeatureimages.
Reconfusion(Wuetal.,2024)usesthediffusionmodeltorefineimagesrenderedbythereconstruc-
tionmodelasextrasupervisiontothereconstructionprocess. Similarly,RealFusion(Melas-Kyriazi
etal.,2023)usesadiffusionmodeltoprovideanextraperspectiveviewforobject-centricreconstruc-
tion. Yu et al. (2024a) use the Stable Video Diffusion model(Blattmann et al., 2023) to iteratively
refinetherenderedvideoalonganovelcameratrajectorybasedonthepartialimagewrappedfrom
thereferenceviewtothetargetview. Mostofthepreviousnovelviewsynthesisthroughgeneration
worksaredesignedforobject-centric(Liuetal.,2023b;Chanetal.,2023;Wuetal.,2024;Melas-
Kyriazietal.,2023;Yuetal.,2024a)orindoor(Liuetal.,2023b;Chanetal.,2023;Wuetal.,2024;
Yuetal.,2024a)scenes. Fordrivingscenes,Yuetal.(2024b)proposesSGDwhichgeneratesnovel
viewswithadiffusionprocessconditionedonreferenceimagesanddepthmapsinthetargetview.
However,SGDstillonlysynthesizescameraviewsalongtherecordedtrajectoryoftheegovehicle.
Moreover,differentfromSGDwhichreliesonthe3DGSmodel,FreeVSisafullygenerativemodel
withperformancecomparabletoreconstructionmodelsevenonrecordedtrajectories.
3 FREEVS
WeintroducethedetaileddesignofourproposedFreeVSinthissection. Wesummarizethealgo-
rithmpipelineofFreeVSinFig.2.
Overview of FreeVS. FreeVS is a fully generative model that synthesizes new camera views on
noveltrajectoriesbasedonobservationsof3Dscenesfromrecordedtrajectories. FreeVSisimple-
3mentedasaconditionalvideodiffusionmodel. Toensurethemodelgeneratesviewsfromaccurate
viewpoints with consistent appearance attributes and 3D geometries as the real 3D scene, we for-
mulateallessentialpriorsregardingthe3Dsceneaspseudo-imagestocontrolthediffusionprocess.
Basedonviewpriorconditions,FreeVSislearnedbydenoisingnoisedtargetviewsattrainingtime
andsynthesizingtargetviewsfrompurenoiseatinferencetime.
3.1 VIEWPRIORSFORVIEWGENERATION
Unifiedviewpriorrepresentation. Onemajorchallengeofgenerativenovelviewsynthesisisto
ensurethegeneratedimagesareconsistentwiththepriorsinthenovelview. Heretheviewpriors
include the observed colors, 3D geometry, and camera pose of this view. However, the different
typesofpriorsareintotallydifferentmodalities,posingasignificantchallengefordiffusionmodels
to precisely encode them. For example, as discussed in Sec. 1, diffusion models cannot precisely
controlthecameramotions(i.e.,poses). Totacklethischallenge,weproposeapseudo-imagerep-
resentationthatunifiesalltypesofviewpriorsinonemodality.Pseudo-imagesareobtainedthrough
colored point cloud projection. Specifically, for each frame in a driving sequence, we first merge
LiDARpointsacrossthenearbyr frames. LiDARpointsonmovingobjectswillbemergedalong
themovingtrajectoryoftheobject. Finally,weprojectthemergedandcoloredLiDARpointcloud
intothetargetcameraviewpointsaspseudo-images. Inthisway,weencodecolorinformation,ge-
ometryinformation,andtheviewposeintoaunifiedpseudo-image,largelyfacilitatingthelearning
ofgenerativemodels.
Comparedwithdirectlyprovidingreferenceimagesandviewpointtransformationstothediffusion
process,thepseudo-imagerepresentationgreatlysimplifiedtheoptimizationobjectiveofthegener-
ative model: With the former inputs, the model is required to have a correct understanding of the
3Dscenegeometryaswellasthetransformationofviewpointtogenerateacorrectviewbasedon
thereferenceimage. Incontrast,withpseudo-imageasinput,FreeVSonlyneedstorecovertarget
viewsbasedonsparsevalidpixels,whichismoreakintoabasicimagecompletiontask. Thesim-
plificationofthetrainingobjectivegreatlyenhancesthemodel‚Äôsrobustnesstounfamiliarviewpoint
transformations, since the generated image is completed from sparse but geometrically accurate
pixelpoints.
Viewpointtransformationsimulation. Anotherchallengeofnovelviewgenerationonnewtrajec-
toriesstemsfromtheabsenceofgroundtruthviewsbeyondrecordedtrajectories. Wecanonlytrain
the generative model on recorded trajectories, where the diversity of viewpoint transformations is
extremely limited. For example, we have no access to the training sample where the frontal cam-
eraismovedlaterally. However,suchviewpointtransformationisessentialforsynthesizingviews
on novel trajectories at inference time. This brings a significant gap between training and infer-
enceforthegenerativemodel. Moreover,weproposetheviewpointtransformationsimulationwith
pseudo-images. Attrainingtime,wesamplecolorandLiDARpriorsfromframesmismatchedwith
thetrainingimageframes. Thatistosay,weforcethegenerativemodeltorecovercurrentcamera
viewsbasedonobservationsfromnearbyframes. Throughthis,wesimulatethecameramovement
ineachdirectionasastrongdataaugmentationonpseudo-imagepriors. Forexample,astheegove-
hiclemovesalongitsheadingdirection,thesidecamerasareactuallymovingtotheirfrontorright,
Thereforealthoughwehavenoaccesstothetrainingdatawherethefrontcameraismovedlaterally,
wecanstillsimulatelateralcameramovementbytrainingFreeVSonside-viewswithmismatched
observation-supervisionframes.
3.2 DIFFUSIONMODELFORNVS
TrainingofFreeVS.IneachtrainingiterationofFreeVS,werandomlysampleacoloredLiDAR
pointcloudsequencep=(p ,...p )fromthedrivingscenedataset.pisasequenceofcoloredpoint
1 n
clouds,eachpointcloudframep
i
‚àà RNi√ó6 containsasetof6-dimension3Dpoints. 3Dpointsare
recordedwiththeirpositionsintheworldreferenceframeandtheirvisiblecolors. Fromthedriving
sequence, we also sample a target camera viewpoint sequence v = ([v1,...,vm],...,[v1,...,vm])
1 1 n n
of n frames and m surrounding cameras. Each camera parameter v stands for the intrinsics and
i
extrinsicsofacameraviewpoint. Foraviewpointinthetargetvideowithcameraparametervj,we
i
project the colored LiDAR point cloud p into the viewpoint as pseudo-image sj = Proj(p ,vj).
i i i i
4Noise Loss
c Denoising
ùëâ
‚Ñá$%& +
c U-Net ùíü %&‚Äô
!"#
Training View
Pseudo-images on ùëâ!"#
ùëâ‚Ä≤ ! ùëâ ! C Lio Dlo Ar Re d P oints ùëâ!"# c Concatenate Frozen
Projection
‚Ñá $ Training Inference
Projection
Novel Recorded ùëâ‚Ä≤! Training view Inference view
Trajectory Trajectory Pseudo-images onùëâ‚Ä≤!
Figure 2: Method pipeline of FreeVS. We propose to encode the view priors in driving scenes
includingappearance,3Dgeometry,andposeoftargetviewpointsallinonemodalityasthepseudo-
images. Bestviewedincolor. Thediffusionmodelistrainedtosynthesizetargetviewssolelybased
ontheunifiedpseudo-imagepriors.
ThetrainingtargetofFreeVSineachiterationistorecoverthetargetimagesatsampledviewpoints
basedonthepseudo-imagesequences=([s1,...,sm],...,[s1,...,sm])‚ààRn√óm√ó3√óH√óW.
1 1 n n
DuringthetrainingprocessofFreeVS,thegroundtruthcameraviewsx ‚àà Rn√óm√ó3√óH√óW isalso
sampled along the viewpoint sequence v. The ground truth camera views are encoded as target
video latent representation E (x) = y ‚àà Rn√óm√óc√óh√ów through an frozen VAE encoder. Then
VAE
Wehavethediffusedinputsy =Œ± y+œÉ œµ,œµ‚àºN(0,I),hereŒ± andœÉ isnoisescheduleatthe
r Œ≥ Œ≥ œÑ œÑ
diffusiontimestepœÑ. Wealsoencodethepseudo-imagesintothelatentrepresentationE (s)=z‚àà
p
Rn√óm√óc√óh√ów witha2Dencodertrainedsimultaneouslywiththediffusionmodel. Weconcatenate
y andzastheinputk ‚àà Rn√óm√ó2c√óh√ów tothediffusionmodeltopredictthenoiseupony. We
r
haveadenoisingmodelf withparametersŒ∏thattakey ,zasinputsandoptimizedbyminimizing
Œ∏ r
thefollowingdenoisingobjective:
E [‚à•œµ‚àíf (k;c,œÑ)‚à•2], (1)
k,œÑ‚àºpœÑ,œµ‚àºN(0,I) Œ∏ 2
Wherecisthedescriptionconditionsgeneratedbyencodingthereferencecameraviewswithanoff-
the-shelfCLIP-visionmodel(Radfordetal.,2021),followingtheconventionofdiffusionmodels.p
œÑ
isauniformdistributionoverthediffusiontimeœÑ.
SynthesizingviewsonnoveltrajectorieswithFreeVS.DuringtheinferenceprocessofFreeVS
, we project the colored LiDAR points in each frame into the targeted camera poses to generate
pseudo-image sequence for image synthesis. The diffusion model is fed with pure noise latents
concatenatedwithpseudo-imagelatents. Thediffusedlatentwillbedecodedassynthesizedviews
throughanoff-the-shelfVAEdecoderD .
VAE
3.3 EVALUATINGNVSONNOVELCAMERAANDNOVELTRAJECTORYSYNTHESIS
TothoroughlydemonstratetheviewgeneralizationcapabilityofourFreeVS,whichcantrulymeet
thedemandsofclosed-loopembodiedsimulation, wepresentacomprehensivediscussionofeval-
uation benchmarks for novel view synthesis in driving scenes. Fig. 3 illustrates this: panels (a)
and(b)summarizeexistingevaluationbenchmarks,whilepanels(c)and(d)introduceourtwonew
challengingNVSbenchmarks.
EvaluatingNVSonrecordedtrajectories.AllcurrentNVSworksfordrivingscenesevaluatetheir
NVS performance on test frames sampled periodically along the recorded trajectory. Some previ-
ousdrivingsceneNVSmethods,suchasStreetGaussians(Yanetal.,2024),NSG(Ostetal.,2021),
and Mars(Wu et al., 2023c), evaluate their performance with only front camera views considered,
as illustrated in Fig. 3(a). Other NVS methods take the multi-view cameras into consideration as
illustratedinFig.3(b),suchasDrivinGaussian(Zhouetal.,2024b),PVG(Chenetal.,2023),EmerN-
erf(Yangetal.,2023a),NeuRAD(Tonderskietal.,2024),S-Nerf(Xieetal.,2023),andSUDS(Turki
etal.,2023). Allthesetwoevaluationbenchmarkssampletestframesperiodicallyalongthetrajec-
tory,i.e. novelframesynthesis. Insuchcases,cameraviewsintestframescanbedirectlyinferred
from the adjacent frames, especially for datasets with a high video frame rate (e.g. 10Hz for the
5Train Views Test Views Ego Poses Detection Targets
(a)Front-view novel frame synthesis
(b) Multi-view novel frame synthesis
(c) Multi-view novel camera synthesis (d) Novel trajectory synthesis
Figure3: BenchmarksforevaluatingNVSmethodsindrivingscenes.Weconcludetheprevious
NVS evaluation benchmarks for driving scenes as (a) and (b). We propose two novel evaluation
benchmarks: the novel camera synthesis benchmark as in (c), and the novel trajectory synthesis
benchmarkasin(d). Bestviewedincolor.
WOD dataset). To provide a more challenging evaluation setting for driving scene NVS methods,
we proposethe novel camerasynthesis benchmarkas illustrated in Fig.3(c). Insteadof period-
icallysamplingtestframes, wedropimagescollectedbycertainmulti-viewcamerasthroughouta
drivingsequenceastestviews.Forexample,foradrivingsequenceintheWODdataset,weprovide
NVS methods with the front, and side camera views as training views and evaluate the synthe-
sis results on front-left and front-right views. Under the novel camera synthesis benchmark, NVS
methods are required to synthesize views on unseen camera poses, which places higher demands
onaccuratelymodelingthe3Dscene. Weensureinthevalidationsequences,most3Dcontentsin
front-sidecamerasareobservedinthefrontorsidecameraviewsalongtheegotrajectory.
Noveltrajectorysynthesis. Ontestviewssampledfromtherecordedtrajectories,thegroundtruth
camera images are available for evaluating the synthesized images with image similarity metrics
includingSSIM,PSNR,andLPIPS(Zhangetal.,2018). Differently,indrivingscenes,thereareno
groundtruthimagesavailableonnoveltrajectories. TheFre¬¥chetInceptionDistance(FID)(Seitzer,
2020)metriccancomparetheoverallimagedistributionbetweensynthesizedimagesonnoveltra-
jectories and ground truth images on recorded trajectories, but it can not assess the fidelity of the
synthesizedimagestothe3Dscenesatall. Inadditiontoqualitativevisualizationcomparisons,we
alsoproposetheperceptualrobustnessevaluationtoassessthegeometryconsistencyperformance
ofNVSmethodsonnewegotrajectories.
In driving scenes, modern image-based 3D perception models have achieved high robustness. As
showninFig.3(d),assuminganNVSmethodcansynthesizeviewsonanoveltrajectorywithideal
image quality, the perception model feed with synthesized views should still be able to produce
accurate predictions on the novel trajectory. With such an assumption, we believe that the perfor-
mance of an off-the-shelf perception model on novel trajectories can reflect the quality of images
synthesized by the NVS methods. Under the novel trajectory synthesis benchmark, we feed the
synthesized images and camera poses on the novel trajectory to an off-the-shelf 3D camera-based
detector. Thedetectionresultsareevaluatedwiththelongitudinalerrortolerantmeanaveragepreci-
sion(LET-mAP)(Hungetal.,2022)metricontheWODdataset. ForallNVSmethods,wemodify
noveltrajectoriesbylaterallyshiftingtheegopositionsineachframes. Weshiftthetrajectoriesby
1.0m,2.0m,and4.0mandreportthemeanevaluatedresultsasmAPLET ,mAPLET ,andmAPLET .
1.0m 2.0m 4.0m
4 EXPERIMENTS
Inthissection,wefirstintroduceourexperimentalsetupincludingdatasets,evaluationbenchmarks,
method implementation details, and counterpart methods. Then we provide the quantitative and
qualitativeexperimentresults.
6Datasets. We perform experiments on the WOD dataset(Sun et al., 2020). We select 12 driving
sequences for evaluating NVS methods. We ensure that there is ample space on both sides of the
egovehicleinmostframesoftheselectedsequencetosimulatenoveltrajectoriesbylateralmoving
theegovehicle. Foreachsequence,all200dataframesin10Hzareused.
EvaluationofNVSmethods.WecompareFreeVSwithNVScounterpartsunderalltheexperiment
benchmarks shown in Fig. 3. For the front-view or multi-view novel frame synthesis benchmark
(Fig. 3(a) and (b)), we sample every fourth frame in driving sequences as test frames. All the
remaining frames are used for training NVS counterparts, or as input frames for FreeVS. On, we
reportmetricsincludingSSIM,PSNR,andLPIPS.Underthenovelcamerasynthesisbenchmark,we
reserveallthefront-sidecameraviewsastestviewsandusethefrontandsidecameraviewsastrain
viewsthroughouteachsequence. NotethatforFreeVSwhichdoesnotrequirescenereconstruction
onvalidationsequences,weonlytakeinformationfromthetrainviewstogeneratetestviews.
Moreover,wealsoevaluateNVSmethodsonnoveltrajectorieswiththeFIDscoreandtheproposed
perceptualrobustnessevaluationmethod. WetakeMV-FCOS3D++(Wangetal.,2022),abasicyet
strongmulti-viewcamera-based3Ddetectorasourbaselinedetectionmodel.Wefollowmostofthe
settingsoftheofficialopen-sourcedimplementationofMV-FCOS3D++. WetrainMV-FCOS3D++
for 24 epochs on the WOD training set (except for the validation sequences in our experiments)
toobtainthebaselinedetector. Following(Wangetal.,2022), weinitializeMV-FCOS3D++from
an FCOS3D++ checkpoint, which is also trained on the above training sequences. We feed the
cameraviewssynthesizedonnoveltrajectoriestothebaselinedetector. Wereportcamera-based3D
detectionmetricsLET-mAP(Hungetal.,2022)onthevehicleclassasmAPLET.
Method details. We implement the proposed FreeVS pipeline based on Stable Video Diffusion
(SVD)(Blattmann et al., 2023). We initialize the diffusion model from a pre-trained Stable Diffu-
sioncheckpoint(Rombachetal.,2022). FreeVSistrainedontheWODtrainingset,exceptforthe
selected validation sequences. To generate pseudo-images, we accumulate colored LiDAR points
across the adjacent ¬±2 frames of each frame. If a 3D LiDAR point has more than one projected
2Dpointinmulti-viewimages,themeancolorofitsprojectedimagepointswillberecorded. For
viewpoint transformation simulation, we randomly sample the target viewpoint sequence starting
from the adjacent ¬±4 frames of the first frame of the source point cloud sequence. We employ
a ConVNext-T(Liu et al., 2022) backbone as the pseudo-image encoder. We train the model for
40,000iterationswithabatchsizeof8andvideoframelengthn = 8. Pleaserefertotheappendix
formoretrainingdetails.
Novel view synthesis counterparts. We compare our novel view synthesis method with the
3DGS(Kerbl et al., 2023), EmerNeRF(Yang et al., 2023a), and Street Gaussians(Yan et al., 2024).
Allcounterpartmethodsareimplementedbasedontheirofficialimplementation. For3DGSwhich
isnotinitiallydesignedforunboundeddrivingscenes,welargelyincreaseitsmaxtrainingiterations
forbetterconvergenceofthemodel. Pleasechecktheappendixformoreimplementationdetailson
NVScounterparts.
4.1 SOTACOMPARISONUNDERTHEPROPOSEDCHALLENGINGNEWBENCHMARKS.
Novel camera synthesis. We first report the performance of NVS methods under the proposed
multi-view novel camera synthesis benchmark in Table 1. FreeVS achieves leading performance
onallmetricsbyalargemargin. PreviousNVSmethodstendtorenderimageswithsevereimage
distortion or massive unnatural artifacts when facing severe scene information loss on the target
views, as shown in Fig. 4. Meanwhile, FreeVS can generate camera views close to ground truth
viewsbasedonlimited3Dsceneobservations.
Noveltrajectorysynthesis. WealsoreporttheFIDandperceptualrobustnessperformanceofNVS
methodsonnoveltrajectoriesinTable2. TheproposedFreeVSoutperformspreviousNVSmethods
onalmostallmetricswithdifferentlateraloffsetsappliedtotheviewpoints. Comparedtoprevious
NVSmethods,theproposedFreeVShasaverystrongperformanceontheFIDmetric.Thisismainly
becausetheproposedFreeVSisnearlyfreefromimagedegradationandartifactswhensynthesizing
imagesonnoveltrajectories. FreeVSalsohasthestrongestmAPLET performanceamongallNVS
methods,whichindicatesthatasageneration-basedmethod,FreeVSisofevenhigherfidelitytothe
3D scene geometry compared with previous reconstruction-based methods when rendering views
onnoveltrajectories. WealsoprovideavisualizationcomparisonexampleinFig.5.
7Table1: ComparisonwithNVScounterpartsonnovelcamerasynthesis. ForallNVSmethods,
weuseallfrontandsidecameraviewsassourceviewstosynthesizethefront-sidecameraviews.
Front-sidecamerasyntheising
Methods
SSIM‚Üë PSNR‚Üë LPIPS‚Üì
3D-GS(Kerbletal.,2023) 0.484 15.97 0.442
EmerNerf(Yangetal.,2023a) 0.603 19.61 0.330
StreetGaussian(Yanetal.,2024) 0.531 17.35 0.397
Ours 0.628 20.70 0.283
Table2: ComparisonwithNVScounterpartsonnoveltrajectories. Theyaxisisdefinedlateral
totheegovehicle‚Äôsheadingdirection. ‚Ä†:performanceofbaselinedetectorongroundtruthimages.
y¬±0.0m y¬±1.0m y¬±2.0m y¬±4.0m
Methods
FID‚Üì mAPLET‚Üë FID‚Üì mAPLET ‚Üë FID‚Üì mAPLET ‚Üë FID‚Üì mAPLET ‚Üë
1.0m 2.0m 4.0m
GT‚Ä† - 0.895 - - - - - -
3D-GS 34.79 0.729 52.07 0.605 61.16 0.581 86.21 0.452
EmerNerf 53.88 0.600 58.26 0.510 69.50 0.478 84.81 0.464
StreetGaussian 21.62 0.826 41.17 0.738 55.71 0.682 80.44 0.544
Ours 11.18 0.816 13.45 0.786 16.60 0.724 22.08 0.612
While FreeVS relies on LiDAR point inputs, EmerNerf and Street Gaussians also rely on LiDAR
depth supervision during their training process. Therefore FreeVS did not gain any information
advantagesinourexperiments.Moreover,asafullygenerativemethod,FreeVSdoesnotrequireany
scene reconstruction process when applied to validation sequences. From this aspect, at inference
time,FreeVScostslesscomputationalresourcesevencomparedwith3DGS-basedmethods,which
usuallytake1-2hourstomodelavalidationsequenceof20s.
4.2 SOTACOMPARISONONNOVELFRAMESYNTHESIS
WealsoreporttheperformanceofNVSmethodsunderthetraditionalfront-viewnovelframesyn-
thesisormulti-viewnovelframesynthesisbenchmarkinTable3.TheperformanceofpreviousNVS
methodsisstrongwhenonlythefront-viewcameraisconsidered. However, whenitcomestothe
multi-view setting which is more aligned with the current autonomous driving scenes, the perfor-
manceofpreviousNVSmethodsissurpassedbytheproposedFreeVSbyalargemargin. Itisworth
mentioning that in Table 3, all previous reconstruction-based NVS methods exhibit a significant
performance drop when multi-view cameras are considered. We think this is due to the increased
number of training views, the expanded range of the visible 3D scene, and the rapidly changing
contentinlateralviews,allofwhichmaketheconvergenceofreconstructionmodelsmoredifficult.
4.3 ABLATIONSTUDIES
Ablationonviewpriorcondition. Wefirstablateontherepresentationofviewpriorasconditions
for the diffusion process, as shown in Table 4. We apply a breakdown experiment on the pseudo-
image representation. Models are trained for 20,000 iterations. We start by dropping the color
information in the pseudo-image representation, represented by Table 4(b). Dropping the color
nearlydoesnotaffectthegeometricaccuracyofrenderedresults,buthasaconsiderableimpacton
theimagesimilaritymetrics. ThenweexperimentwithdroppingtheLiDARinputs(c), wherethe
referenceimagesandcameraposetransformationmatrix(fromthereferenceviewtothetargetview)
are independently encoded by a VAE or MLP encoder. Under this setting, we found the diffusion
modelunabletoaccuratelysynthesizeviewsonthetargetviewpoint. Mosttime,themodelignores
theposeconditionandmovesthecameraviewpointbyitsfamiliarviewpointtransformation. (e.g.
alwaysmovethefrontalcameraforwardorbackward,ormovethesidecameraleftorright.) Based
on(c),weexperimentwithpreservingallviewpriorinputsbutdonotunifythemaspseudo-images
(d). TheLiDARpointsareencodedaslatentswithapointcloudbackbone(Yanetal.,2018). The
experimentresultshowsthemodelfailsatutilizingLiDARinputsduetoitssignificantgapwith2D
images,themodeltrainedundersetting(d)hasanidenticalperformanceasthemodetrainedunder
8Table 3: Comparison with NVS counterparts on recorded trajectories. We report the perfor-
mance of NVS methods when only the front-view cameras are considered or when all multi-view
camerasareconsidered. ReconstructiontimecostandFPSaremeasuredunderthemulti-viewset-
ting,withasingleNVIDIAL20GPU.‚Ä†:welargelyincreasedthemaxtrainingiterationsof3D-GS
forbetterperformanceindrivingscenes.
FrontView Multi-view Reconstruction
Methods FPS
SSIM‚Üë PSNR‚Üë LPIPS‚Üì SSIM‚Üë PSNR‚Üë LPIPS‚Üì time
3D-GS(Kerbletal.,2023) 0.799 26.31 0.143 0.586 19.21 0.366 2-3h‚Ä† 61.2
EmerNerf(Yangetal.,2023a) 0.869 30.28 0.155 0.689 24.68 0.347 2-3h 0.2
StreetGaussian(Yanetal.,2024) 0.903 30.80 0.096 0.702 22.47 0.314 1-2h 52.6
Ours 0.787 25.30 0.139 0.730 24.96 0.179 - 0.9
Table4: Ablationstudyonviewpriorcondition. Weconductabreakdownablationonthepro-
posedpseudo-imagerepresentationofviewpriors. Theyaxisisdefinedlateraltotheegovehicle‚Äôs
headingdirection.
Multi-view y¬±2.0m
Viewpriors Encoders
SSIM‚Üë PSNR‚Üë LPIPS‚Üì FID‚Üì mAPLET ‚Üë
2.0m
(a) fullpriors 2D-Conv 0.704 23.28 0.203 21.27 0.690
(b) w/o.color 2D-Conv 0.701 23.05 0.231 23.13 0.687
(c) w/o.LiDAR 2D-Conv+MLP 0.613 19.86 0.288 21.25 0.013
(d) w/o.projection 2D-Conv+3D+MLP 0.609 19.88 0.284 21.32 0.028
(e) fullpriors 2D-Attn 0.706 23.27 0.202 21.25 0.692
setting(c). Duetothewrongviewpointofmostgeneratedimages,trainedmodelsundersettings(c)
and(d)haveextremelypoorperceptualrobustnessperformance. Throughthisobservation,wecan
conclude that the pseudo-image representation greatly improved the overall quality and viewpoint
controllabilityofimagesgeneration. Finally, wecompareencodingthepseudo-imageswitha2D-
Convencoderoran2D-attentionencoder(Liuetal.,2021)withsetting(e).
Ablation study on viewpoint transformation simulation. Viewpoint transformation simulation
aims at constructing source and target frame pairs from recorded sequences to simulate camera
movement in various directions. We report the results of ablation studies on viewpoint transfor-
mation simulation with pseudo-images in Table 5. We report the performance of FreeVS under
the multi-view novel frame synthesis setting and on novel trajectories generated by applying 2.0
m lateral offsets to the recorded trajectories. As shown in Table 5, sampling target frames from
adjacent ¬±2 or ¬±4 frames from the source frame can boost the view-synthesize performance of
FreeVS on novel trajectories. When the temporal sampling window size exceeds ¬±4 frames, the
view-synthesizeperformanceofFreeVSontherecordedtrajectorywillbenegativelyaffected. We
believethisisduetothelargetimestampmismatchbetweenviewpriorsandtargetimageshindering
themodel‚Äôsconvergence.
4.4 VISUALIZATIONCOMPARISON
WeshowvisualizationcomparisonsbetweenNVSmethodsunderthenovelcamerasynthesisbench-
markinFig.4,andunderthenoveltrajectorysynthesisbenchmarkinFig.5.
4.5 CONCLUSION
WepresentFreeVS,anovelfullygenerativemethodforsynthesizingcameraviewsonfreedriving
trajectory. Weproposethepseudo-imagerepresentationofviewpriors,whichconveysaccurate3D
scenegeometryandviewpointconditionsthroughcoloredpointprojection. Thediffusionmodelis
trainedtosynthesizetargetviewssolelybasedonpseudo-images. Inthispaper,wefullydiscussthe
evaluationbenchmarksfordrivingsceneNVS.Weproposetwonovelevaluationbenchmarksinclud-
ing the novel camera synthesis benchmark and the novel trajectory synthesis benchmark. We also
proposetheperceptualrobustnessevaluationmethodforassessingtheperformanceofNVSmeth-
ods on novel trajectories. Experiments across several experiment benchmarks show that FreeVS
achievesleadingperformanceinsynthesizingcameraviewsinsideorbeyondrecordedtrajectories.
9Table5: Ablationstudyonviewpointtranslationsimulation. Theyaxisisdefinedlateraltothe
egovehicle‚Äôsheadingdirection.
Multi-view y¬±2.0m
Temporalsamplingwindowsize
SSIM‚Üë PSNR‚Üë LPIPS‚Üì FID‚Üì mAPLET ‚Üë
2.0m
- 0.733 25.04 0.180 18.29 0.704
¬±2frames 0.734 25.04 0.179 18.24 0.710
¬±4frames 0.730 24.96 0.179 18.01 0.722
¬±8frames 0.717 24.82 0.188 18.09 0.720
sru
O
fre
N
re
m
E
s
n
a
iss
u
a
G
te
e
rtS
h
tu
rT
d
n
u
o
rG
Figure4: Visualizationcomparisononnovel-camerasynthesisbenchmark. Weshowthefront-
sidecameraviewssynthesizedfromfrontandsidecameraviewswithNVSmethods.
Original Viewpoint Viewpoint at left 2.0 m Viewpoint at right 2.0 m
sruO
freN
rem
E
snaissuaG
teertS
Figure5: Visualizationcomparisononnoveltrajectories.Weshowthecameraviewssynthesized
byNVSmethodsontheoriginaltrainingviewpoint,viewpoint2.0mleftoftheoriginalviewpoint,
andviewpoint2.0mrightoftheoriginalviewpoint.
10REFERENCES
JonathanTBarron,BenMildenhall,DorVerbin,PratulPSrinivasan,andPeterHedman. Mip-nerf
360:Unboundedanti-aliasedneuralradiancefields. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pp.5470‚Äì5479,2022. 3
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideodiffusion:Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023. 3,7
EricRChan,KokiNagano,MatthewAChan,AlexanderWBergman,JeongJoonPark,AxelLevy,
MiikaAittala,ShaliniDeMello,TeroKarras,andGordonWetzstein. Generativenovelviewsyn-
thesiswith3d-awarediffusionmodels.InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pp.4217‚Äì4229,2023. 3
Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, and Li Zhang. Periodic vibration gaussian:
Dynamicurbanscenereconstructionandreal-timerendering. arXivpreprintarXiv:2311.18561,
2023. 2,3,5
Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang, Chenjing Ding,
DongliangWang,andYikangLi.Streetsurf:Extendingmulti-viewimplicitsurfacereconstruction
tostreetviews. arXivpreprintarXiv:2306.04988,2023. 2,3
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022. 15
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840‚Äì6851,2020. 3
AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,JamieShot-
ton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv
preprintarXiv:2309.17080,2023. 2
Wei-Chih Hung, Henrik Kretzschmar, Vincent Casser, Jyh-Jing Hwang, and Dragomir Anguelov.
Let-3d-ap: Longitudinalerrortolerant3daverageprecisionforcamera-only3ddetection. arXiv
preprintarXiv:2206.07705,2022. 6,7
BernhardKerbl,GeorgiosKopanas,ThomasLeimku¬®hler,andGeorgeDrettakis. 3dgaussiansplat-
tingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),2023. 3,7,8,
9
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014. 15
JeffreyYunfanLiu,YunChen,ZeYang,JingkangWang,SivabalanManivasagam,andRaquelUr-
tasun. Neuralscenerasterizationforlargescenerenderinginrealtime. InICCV,2023a. 3
RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,SergeyZakharov,andCarlVondrick.
Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international
conferenceoncomputervision,pp.9298‚Äì9309,2023b. 3
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.10012‚Äì10022,2021. 9
ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie.
A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and
patternrecognition,pp.11976‚Äì11986,2022. 7,15
FanLu,YanXu,GuangChen,HongshengLi,Kwan-YeeLin,andChangjunJiang. Urbanradiance
fieldrepresentationwithdeformableneuralmeshprimitives. ICCV,2023a. 3
JiachenLu,ZeHuang,JiahuiZhang,ZeyuYang,andLiZhang.Wovogen:Worldvolume-awaredif-
fusionforcontrollablemulti-cameradrivingscenegeneration. arXivpreprintarXiv:2312.02934,
2023b. 2
11Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians:
Trackingbypersistentdynamicviewsynthesis. In3DV,2024. 3
Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg
reconstructionofanyobjectfromasingleimage. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pp.8446‚Äì8455,2023. 3
BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,and
RenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,2020.
3
JulianOst, FahimMannan, NilsThuerey, JulianKnodt, andFelixHeide. Neuralscenegraphsfor
dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.2856‚Äì2865,2021. 3,5
JulianOst,IssamLaradji,AlejandroNewell,YuvalBahat,andFelixHeide.Neuralpointlightfields.
CVPR,2022. 3
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748‚Äì8763.PMLR,2021. 5
KonstantinosRematas, AndrewLiu, PratulPSrinivasan, JonathanTBarron, AndreaTagliasacchi,
ThomasFunkhouser,andVittorioFerrari. Urbanradiancefields. InCVPR,2022. 3
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¬®rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684‚Äì10695,2022. 3,7,15
MaximilianSeitzer. pytorch-fid: FIDScoreforPyTorch. https://github.com/mseitzer/
pytorch-fid,August2020. Version0.3.0. 6
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprintarXiv:2010.02502,2020. 3
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui,
James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for au-
tonomousdriving: Waymoopendataset. InCVPR,2020. 1,7
Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srini-
vasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural
view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.8248‚Äì8258,2022. 3
Adam Tonderski, Carl Lindstro¬®m, Georg Hess, William Ljungbergh, Lennart Svensson, and
ChristofferPetersson. Neurad: Neuralrenderingforautonomousdriving. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.14895‚Äì14904,2024. 3,
5
HaithemTurki,DevaRamanan,andMahadevSatyanarayanan. Mega-nerf:Scalableconstructionof
large-scalenerfsforvirtualfly-throughs. InCVPR,2022. 3
Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan. Suds: Scalable urban
dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.12375‚Äì12385,2023. 2,3,5
Tai Wang, Qing Lian, Chenming Zhu, Xinge Zhu, and Wenwei Zhang. MV-FCOS3D++: Multi-
View camera-only 4d object detection with pretrained monocular backbones. arXiv preprint,
2022. 7
Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drive-
dreamer: Towards real-world-driven world models for autonomous driving. arXiv preprint
arXiv:2309.09777,2023a. 2
12Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into
thefuture: Multiviewvisualforecastingandplanningwithworldmodelforautonomousdriving.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
14749‚Äì14759,2024. 2
Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan
Gojcic,WenzhengChen,andSanjaFidler. Neuralfieldsmeetexplicitgeometricrepresentations
forinverserenderingofurbanscenes. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.8370‚Äì8380,2023b. 3
ChenmingWu,JiadaiSun,ZhelunShen,andLiangjunZhang. Mapnerf: Incorporatingmappriors
intoneuralradiancefieldsfordrivingviewsimulation. In2023IEEE/RSJInternationalConfer-
enceonIntelligentRobotsandSystems(IROS),pp.7082‚Äì7088.IEEE,2023a. 3
GuanjunWu,TaoranYi,JieminFang,LingxiXie,XiaopengZhang,WeiWei,WenyuLiu,QiTian,
andWangXinggang. 4dgaussiansplattingforreal-timedynamicscenerendering. arXivpreprint
arXiv:2310.08528,2023b. 3
RundiWu, BenMildenhall, PhilippHenzler, KeunhongPark, RuiqiGao, DanielWatson, PratulP
Srinivasan,DorVerbin,JonathanTBarron,BenPoole,etal.Reconfusion:3dreconstructionwith
diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.21551‚Äì21561,2024. 3
ZiruiWu, TianyuLiu, LiyiLuo, ZhideZhong, JiantengChen, HongminXiao, ChaoHou, Haozhe
Lou,YuantaoChen,RunyiYang,YuxinHuang,XiaoyuYe,ZikeYan,YongliangShi,YiyiLiao,
andHaoZhao.Mars:Aninstance-aware,modularandrealisticsimulatorforautonomousdriving.
CICAI,2023c. 2,3,5
ZiyangXie,JungeZhang,WenyeLi,FeihuZhang,andLiZhang. S-nerf: Neuralradiancefieldsfor
streetviews. arXivpreprintarXiv:2303.00749,2023. 2,3,5
YanYan, YuxingMao, andBoLi. Second: Sparselyembeddedconvolutionaldetection. Sensors,
18(10):3337,2018. 8
Yunzhi Yan, HaotongLin, Chenxu Zhou, Weijie Wang, HaiyangSun, Kun Zhan, Xianpeng Lang,
XiaoweiZhou,andSidaPeng. Streetgaussians: Modelingdynamicurbansceneswithgaussian
splatting. InECCV,2024. 2,3,5,7,8,9,15
Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che,
Danfei Xu, Sanja Fidler, Marco Pavone, et al. Emernerf: Emergent spatial-temporal scene de-
compositionviaself-supervision. arXivpreprintarXiv:2311.02077,2023a. 2,5,7,8,9
JiazhiYang,ShenyuanGao,YihangQiu,LiChen,TianyuLi,BoDai,KashyapChitta,PenghaoWu,
JiaZeng,PingLuo,etal. Generalizedpredictivemodelforautonomousdriving. InProceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14662‚Äì14672,
2024. 2
ZeYang,YunChen,JingkangWang,SivabalanManivasagam,Wei-ChiuMa,AnqiJoyceYang,and
RaquelUrtasun.Unisim:Aneuralclosed-loopsensorsimulator.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.1389‚Äì1399,2023b. 3
ZeyuYang,HongyeYang,ZijiePan,XiatianZhu,andLiZhang. Real-timephotorealisticdynamic
scenerepresentationandrenderingwith4dgaussiansplatting. arXivpreprintarXiv2310.10642,
2023c. 3
Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. De-
formable3dgaussiansforhigh-fidelitymonoculardynamicscenereconstruction. arXivpreprint
arXiv:2309.13101,2023d. 3
Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-
Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for
high-fidelitynovelviewsynthesis. arXivpreprintarXiv:2409.02048,2024a. 3
13ZhongruiYu,HaoranWang,JinzeYang,HanzhangWang,ZekeXie,YunfengCai,JialeCao,Zhong
Ji, and Mingming Sun. Sgd: Street view synthesis with gaussian splatting and diffusion prior.
arXivpreprintarXiv:2403.20079,2024b. 3
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586‚Äì595,2018. 6
HongyuZhou,JiahaoShao,LuXu,DongfengBai,WeichaoQiu,BingbingLiu,YueWang,Andreas
Geiger, and Yiyi Liao. Hugs: Holistic urban 3d scene understanding via gaussian splatting.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
21336‚Äì21345,2024a. 3
XiaoyuZhou,ZhiweiLin,XiaojunShan,YongtaoWang,DeqingSun,andMing-HsuanYang.Driv-
inggaussian: Compositegaussiansplattingforsurroundingdynamicautonomousdrivingscenes.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
21634‚Äì21643,2024b. 2,3,5
14Frame T Frame T+10 Frame T+20
s
ru
O
fre
N
re
m
E
s
n
a
is
s
u
a
G
te
e
rtS
Figure A: Visualization comparison on moving objects. We compare the performance of NVS
methodswheremovingobjectsarevisibleincameraviews.Intheshowncase,ourproposedmethod
cangeneratemoreaccurateimagesofvehiclesdrivingintheoppositelane. Imagesaresynthesized
ontrainingviews.
A APPENDIX
A.1 IMPLEMENTATIONDETAILS
ImplementationdetailsofFreeVS.WeemployaConVNext-T(Liuetal.,2022)encodertoencode
pseudo-images. FortrainingFreeVS,thediffusionmodelisinitializedwithStableDiffusioncheck-
points(Rombachetal.,2022). Wetrainthemodelfor40,000iterationswithabatchsizeof8and
video frame length n = 8. We use the AdamW optimizer (Kingma & Ba, 2014) with a learning
rate1√ó10‚àí4. Duringtrainingtime,werandomlydropthepseudo-imageconditionlatentaswellas
theCLIPtextdescriptionlatentwithaprobabilityof20%. Weenabletheviewpointtransformation
simulationwithaprobabilityof50%. Duringinference,wesetthenumberofsamplingstepsas25
andstochasticityŒ∑=1.0. Whensynthesizingimagesontheexistingtrajectory,wesettheclassifier-
freeguidance(CFG)(Ho&Salimans,2022)guidancescaleto1.0.Forsynthesizingimagesonnovel
camerasandnewtrajectories,weenlargetheCFGguidancescaleto2.0tostrengthenthecontrolof
3Dpriorconditionsoverthegeneratedresults.
Implementation details of NVS counterparts. For 3DGS which is not initially designed for un-
bounded driving scenes, we optimize its performance by adjusting its hyperparameters, including
setting the densification interval to 500 iterations, setting the opacity reset interval to 10000, and
training the 3DGS models for 100000 iterations while densifying 3D Gaussians until 50000 itera-
tions.WealsonoticedthattheStreetGaussiansmodelshaveaconvergenceissuewhentrainingwith
all 200 frames of each sequence. Therefore we split the validation sequences into two 100-frame
sequences for training the Street Gaussians models, following its official configuration(Yan et al.,
2024). As3DGSandEmerNerfsufferfromhighmemorycostswhentrainingwithhigh-resolution
images,weresizetheinputimagesonWODfromaresolutionof1920√ó1280to1248√ó832when
trainingFreeVSandallNVScounterparts.Thepseudo-imagesaregeneratedinthesameresolution.
AllexperimentsareconductedonNVIDIAL20GPUs.
A.2 VIDEOCOMPARISON.
WepresentavideocomparisonofnovelviewsequencessynthesizedbyNVSmethodsonthemod-
ifiedtrajectoryinadrivingsequence. Pleasecheckthevideofilesubmittedassupplementarymate-
rial.
15(a) Training camera view (b) Training pseudo-image w/o temporal (c) Training pseudo-image w/ temporal
augmentation augmentation
(d) Validation pseudo-image in original trajectory (e) Validation pseudo-image in shifted trajectory (f) Synthesized image in shifted trajectory
FigureB: Visualizationoftheviewpointtransformationsimulation. Foratrainingsample,we
showthe(a)sourcecameraview,(b)pseudo-imagegeneratedfromcurrentLiDARobservation,and
(c) pseudo-image generated from previous LiDAR observation to simulate viewpoint transforma-
tion. We also show a validation case of (d) pseudo-image on the original viewpoint, (e) pseudo-
imageontheshiftedviewpoint, and(f)generatedimageontheshiftedviewpoint. Notetheimage
areaswithmissingoroverlapped3Dobservationcircledbyredboxesin(c)and(f). Theproposed
viewpointtransformationsimulationonpseudo-imagescanwell-stimulatetheinsufficient3Dprior
observationsbroughtbytheshiftinviewpoint.
A.3 VISUALIATIONCOMPARISONONDYNAMICOBJECTS.
Withoutthescenereconstructionprocess,FreeVSisfreefromcomplexcross-frameoptimizationof
dynamicobjects. FreeVScansynthesizeimagesofdynamicobjectsinhighaccuracy,asshownin
Fig.A.Incomparison,despitewithspecificdesign,currentreconstruction-basedNVSmethodsstill
sufferfromdynamicobjectmodeling.
A.4 VISUALIZATIONOFTHEVIEWPOINTTRANSFORMATIONSIMULATION.
Besidesquantitativelyablatingtheimpactoftheproposedviewpointtransformationsimulationwith
pseudo-images,wealsoprovideavisualizationcaseinFig.Btoillustrateitseffect.
When facing the translation of viewpoints, pseudo images generated from the existing viewpoints
cannotprovidecomplete3Dpriors,suchasinareascircledbyredboxesinFig.B(e). Tostrengthen
therobustnessofFreeVStowardsthosepatternsinpseudo-images,weproposetoemployviewpoint
transformationsimulationwithpseudo-imagesbygeneratingpseudo-imagesintrainingviewsfrom
mismatchedLiDARobservations. AsshowninFig.B(c),wecanstimulatethepseudo-imageareas
withinsufficient3Dpriors. Thegenerativemodeltrainedwiththeproposedviewpointtransforma-
tionsimulationwithpseudo-imagescanrenderimagesofhighqualitywhenfacinginsufficient3D
priorinputs,asshowninFig.B(f).
A.5 EFFECTOFCLASSIFIER-FREEGUIDANCE.
As a diffusion model, FreeVS can use the classifier-free guidance (CFG) technique to adjust the
controleffectofinput3Dpriorcondtions. WeshowtheimpactofCFGwithdifferentCFGscalesin
Fig.C.
A.6 VISUALIZATIONRESULTSOFCOMPONENTABLATIONEXPERIMENTONPSEUDO-IMAGE
REPRESENTATION.
Fortheablationsetting(a)and(d)introducedinSec.4.3,whosequantitativeresultsarereportedin
Table4,wepresentavisualizationcomparisoninFig.D.Setting(a)isthebaselinesettingoffeeding
diffusion models with pseudo-image scene representations for view synthesis. Under setting (d),
wefeedthediffusionmodelwiththereferenceimage, LiDARpointcloud, andthetransformation
16(a) CFG scale = 0.1 (b) CFG scale = 0.5 (c) CFG scale = 1.0
(d) CFG scale = 2.0 (e) CFG scale = 4.0 (f) CFG scale = 8.0
Figure C: Visualization of the effect of classifier-free guidance (CFG). We show generation
resultswiththesameinputpseudo-imageanddifferentCFGscales. ThelargerCFGscaleisset,the
strongerthegenerationresultisconstrainedbythe3Dpriorcondition.
Pose encoded in pseudo-image Pose as transformation matrix
tn
io
p
w
e
iV
la
n
ig
irO
m
0
.2
tfe
L
FigureD: Visualizationcomparisonontheencodingofcameraviewpointcondition. Wequal-
itatively compare the image synthesis performance of the diffusion model with pseudo-image as
input(Table4(a)),orwiththereferenceimage,LiDARpoints,andviewpointtransformationmatrix
as input (Table 4(d)). When feeding the model with pose transformation matrices, the diffusion
modelsoftenfailtogenerateviewsonthetargetedviewpoint,asshowninthesecondcolumn.
matrix from the reference view to the target view. As shown in Fig.D, model feed with pseudo-
image can precisely synthesize image on the target viewpoint, while model feed with raw camera
pose fails to follow the viewpoint condition. Given that the diffusion model can only be trained
on recorded trajectories, we found the diffusion model fed with reference images and viewpoint
posetendstooverfittothespecificcameramovementpatternineachcameraposition. Asshownin
Fig.D,themodelfedwithraw3Dpriorinputswillonlymovethefrontalcameraviewforwardor
backward, ignoring the viewpoint pose condition. This is due to the absence of a training sample
wherethefrontalcameraismovedlaterally.Bymodifyingthenovelviewsynthesistaskasanimage
completion task based on the pseudo-image representation of 3D priors as well as applying the
proposedviewpointtransformationsimulationwithpseudo-images, wecanovercomethistraining
datashortage.
A.7 VALIDATIONSEQUENCES
Welisttheselected12validationsequencesfromtheWODdatasetherewiththeirofficialindividual
filenames:
17‚Ä¢ segment-10588771936253546636 2300 000 2320 000 with camera labels.tfrecord,
‚Ä¢ segment-6242822583398487496 73 000 93 000 with camera labels.tfrecord,
‚Ä¢ segment-16801666784196221098 2480 000 2500 000 with camera labels.tfrecord,
‚Ä¢ segment-1191788760630624072 3880 000 3900 000 with camera labels.tfrecord,
‚Ä¢ segment-10625026498155904401 200 000 220 000 with camera labels.tfrecord,
‚Ä¢ segment-11846396154240966170 3540 000 3560 000 with camera labels.tfrecord,
‚Ä¢ segment-18111897798871103675 320 000 340 000 with camera labels.tfrecord,
‚Ä¢ segment-11017034898130016754 697 830 717 830 with camera labels.tfrecord,
‚Ä¢ segment-10963653239323173269 1924 000 1944 000 with camera labels.tfrecord,
‚Ä¢ segment-12161824480686739258 1813 380 1833 380 with camera labels.tfrecord,
‚Ä¢ segment-11928449532664718059 1200 000 1220 000 with camera labels.tfrecord,
‚Ä¢ segment-10444454289801298640 4360 000 4380 000 with camera labels.tfrecord.
18