4D Contrastive Superflows are Dense 3D
Representation Learners
Xiang Xu1,â‹†, Lingdong Kong2,3,âˆ—, Hui Shuai4, Wenwei Zhang2,
Liang Pan2, Kai Chen2, Ziwei Liu5, and Qingshan Liu2,(cid:66)
1 Nanjing University of Aeronautics and Astronautics
2 Shanghai AI Laboratory
3 National University of Singapore
4 Nanjing University of Posts and Telecommunications
5 S-Lab, Nanyang Technological University
Abstract. Intherealmofautonomousdriving,accurate3Dperception
is the foundation. However, developing such models relies on extensive
humanannotationsâ€“aprocessthatisbothcostlyandlabor-intensive.To
address this challenge from a data representation learning perspective,
we introduce SuperFlow, a novel framework designed to harness con-
secutiveLiDAR-camerapairsforestablishingspatiotemporalpretraining
objectives. SuperFlow stands out by integrating two key designs: 1) a
dense-to-sparse consistency regularization, which promotes insensitivity
to point cloud density variations during feature learning, and 2) a flow-
basedcontrastivelearningmodule,carefullycraftedtoextractmeaning-
ful temporal cues from readily available sensor calibrations. To further
boostlearningefficiency,weincorporateaplug-and-playviewconsistency
modulethatenhancesthealignmentoftheknowledgedistilledfromcam-
era views. Extensive comparative and ablation studies across 11 hetero-
geneousLiDARdatasetsvalidateoureffectivenessandsuperiority.Addi-
tionally,weobserveseveralinterestingemergingpropertiesbyscalingup
the2Dand3Dbackbonesduringpretraining,sheddinglightonthefuture
research of 3D foundation models for LiDAR-based perception. Code is
publicly available at https://github.com/Xiangxu-0103/SuperFlow.
Keywords: LiDARSegmentationÂ·3DDataPretrainingÂ·Autonomous
Driving Â· Image-to-LiDAR Contrastive Learning Â· Semantic Superpixels
1 Introduction
Drivingperceptionisoneofthemostcrucialcomponentsofanautonomousvehi-
clesystem.Recentadvancementsinsensingtechnologies,suchaslightdetection
and ranging (LiDAR) sensors and surrounding-view cameras, open up new pos-
sibilities for a holistic, accurate, and 3D-aware scene perception [3,10,80].
Traininga3Dperceptionmodelthatcanperformwellinreal-worldscenarios
oftenrequireslarge-scaledatasetsandsufficientcomputingpower[28,59].Differ-
entfrom2D,annotating3Ddataisnotablymoreexpensiveandlabor-intensive,
â‹† X. Xu and L. Kong contributed equally to this work. (cid:66) Corresponding author.
4202
luJ
8
]VC.sc[
1v09160.7042:viXra2 X. Xu et al.
which hinders the scalability of existing 3D perception models [29,70,100,114].
Data representation learning serves as a potential solution to mitigate such a
problem [6,77]. By designing suitable pretraining objectives, the models are an-
ticipatedtoextractusefulconceptsfromrawdata,wheresuchconceptscanhelp
improve modelsâ€™ performance on downstream tasks with fewer annotations [52].
Recently, Sautier et al. [83]
nuScenes
proposed SLidR to distill knowl- SemanticKITTI (LinearProbing) nuScenes
edge from surrounding camera (1%,mIoU) (1%,mIoU)
48.0
views â€“ using a pretrained 2D 49.1 46.8 50.0
backbone such as MoCo [15] and (S 1y %n ,L miD IoA UR ) 44.9 47.6 47.0 45.7 42.846.3 50.7Wa (1y %m ,o mIO oUp )en
DINO [73] â€“ to LiDAR point 43.6 42.1 45.9 42.1 48.6 50.0
clouds, exhibiting promising 3D 41.6
representation learning proper- (S 1%yn ,t mh I4 oD U) 65.364.563.1 613 .17.6 47. 48 9.7 49.8 51.1 52.8 R (1E %L ,L mIS I- o3 UD )
ties. The key to its success is the 78.9 50.9
52.0
superpixel-driven contrastive ob- 81.9 81.0 36.5 75.9 50.2 53.5
jectivesbetweencamerasandLi- DAPS-3D 82.4 39.6 51.6 54.7 SemanticSTF
76.0
DAR sensors. Subsequent works (Half,mIoU) 40.6 83.0 53.3 (Half,mIoU)
42.7 54.4
further extended this framework ScribbleKITTI 83.2 SemanticPOSS
from various aspects, such as (1%,mIoU) nuScenes-C (Half,mIoU)
class balancing [67], hybrid-view (Full,mRR)
distillation[112],semanticsuper- Fig.1: Performance overview of SuperFlow
compared to state-of-the-art image-to-LiDAR
pixels [12, 13, 62], and so on.
pretraining methods, i.e., Seal [62], SLidR [83],
Whilethesemethods showedim-
and PPKT [64], on eleven LiDAR datasets. The
proved performance over their
scoresofpriormethodsarenormalizedbasedon
baselines, there exist several is-
SuperFlowâ€™sscores.Thelargertheareacoverage,
sues that could undermine the
thebettertheoverallsegmentationperformance.
data representation learning.
ThefirstconcernrevolvesaroundtheinherenttemporaldynamicsofLiDAR
data[4,9].LiDARpointcloudsareacquiredsequentially,capturingtheessenceof
motionwithinthescene.Traditionalapproaches[62,64,67,83,112]oftenoverlook
this temporal aspect, treating each snapshot as an isolated scan. However, this
sequential nature holds a wealth of information that can significantly enrich the
modelâ€™s understanding of the 3D environment [72,98]. Utilizing these temporal
cues can lead to more robust and context-aware 3D perception models, which is
crucial for dynamic environments encountered in autonomous driving.
Moreover,thevaryingdensityofLiDARpointcloudspresentsauniquechal-
lenge [46,48,96]. Due to the nature of LiDAR scanning and data acquisition,
different areas within the same scene can have significantly different point den-
sities, which can in turn affect the consistency of feature representation across
thescene[2,48,110,113].Therefore,amodelthatcanlearninvariantfeaturesre-
gardlessofpointclouddensitytendstobeeffectiveforrecognizingthestructural
and semantic information in the 3D space.
In lieu of existing challenges, we propose a novel spatiotemporal contrastive
learning dubbed SuperFlow to encourage effective cross-sensor knowledge dis-4D Contrastive Superflows are Dense 3D Representation Learners 3
tillation. Our approach features three key components, all centered around the
useoftheoff-the-shelftemporalcuesinherentintheLiDARacquisitionprocess:
â€“ Wefirstintroduceastraightforwardyeteffectiveviewconsistencyalignment
that seamlessly generates semantic superpixels with language guidance, al-
leviatingtheâ€œself-conflictâ€ issuesinexistingworks[62,67,83].Asopposedto
the previous pipeline, our method also aligns the semantics across camera
views in consecutive scenes, paving the way for more sophisticated designs.
â€“ ToaddressthevaryingdensityofLiDARpointclouds,wepresentadense-to-
sparse regularization module that encourages consistency between features
ofdenseandsparsepointclouds.Densepointsareobtainedbyconcatenating
multi-sweepLiDARscanswithinasuitabletimewindowandpropagatingthe
semantic superpixels from sparse to dense points. By leveraging dense point
featurestoregularizesparsepointfeatures,themodelpromotesinsensitivity
to point cloud density variations.
â€“ To capture useful temporal cues from consecutive scans across different
timestamps, we design a flow-based contrastive learning module. This mod-
ule takes multiple LiDAR-camera pairs as input and excites strong con-
sistency between temporally shifted representations. Analogous to existing
image-to-LiDAR representation learning methods [62,67,83], we also incor-
porate useful spatial contrastive objectives into our framework, setting a
unified pipeline that emphasizes holistic representation learning from both
the structural 3D layouts and the temporal 4D information.
The strong spatiotemporal consistency regularization in SuperFlow effec-
tively forms a semantically rich landscape that enhances data representations.
AsillustratedinFig.1,ourapproachachievesappealingperformancegainsover
state-of-the-art3Dpretrainingmethodsacrossadiversespectrumofdownstream
tasks. Meanwhile, we also target at scaling the capacity of both 2D and 3D
backbonesduringpretraining,sheddinglightonthefuturedevelopmentofmore
robust, unified, and ubiquitous 3D perception models.
To summarize, this work incorporates key contributions listed as follows:
â€“ We present SuperFlow, a novel framework aimed to harness consecutive
LiDAR-camera pairs for establishing spatiotemporal pretraining objectives.
â€“ Our framework incorporates novel designs including view consistency align-
ment, dense-to-sparse regularization, and flow-based contrastive learning,
which better encourages data representation learning effects between cam-
era and LiDAR sensors across consecutive scans.
â€“ Our approach sets a new state-of-the-art performance across 11 LiDAR
datasets, exhibiting strong robustness and generalizability. We also reveal
intriguing emergent properties as we scale up the 2D and 3D backbones,
which could lay the foundation for scalable 3D perception.
2 Related Work
LiDAR-based 3D Perception. The LiDAR sensor has been widely used in
todayâ€™s3Dperceptionsystems,creditedtoitsrobustandstructuralsensingabil-4 X. Xu et al.
ities [4,89,94]. Due to the sparse and unordered nature of LiDAR point clouds,
suitable rasterization strategies are needed to convert them into structural in-
puts [38,95]. Popular choices include sparse voxels [19,20,34,35,92,120], birdâ€™s
eyeviewmaps[11,57,113,119],rangeviewimages[18,22,45,69,106,109,118],and
multi-view fusion [19,41,61,63,78,107,108]. While witnessing record-breaking
performances on standard benchmarks, existing approaches rely heavily on hu-
man annotations, which hinders scalability [28]. In response to this challenge,
we resort to newly appeared 3D representation learning, hoping to leverage the
richcollectionsofunlabeledLiDARpointcloudsformoreeffectivelearningfrom
LiDAR data. This could further enrich the efficacy of LiDAR-based perception.
Data-Efficient 3D Perception. To better save annotation budgets, previous
efforts seek 3D perception in a data-efficient manner [12,13,28,41,47,50]. One
line of research resorts to weak supervision, e.g., seeding points [37,54,87,117],
active prompts [39,58,102], and scribbles [96], for weakly-supervised LiDAR se-
mantic segmentation. Another line of research seeks semi-supervised learning
approaches [48,53,93] to better tackle efficient 3D scene perception and achieve
promising results. In this work, different from the prior pursuits, we tackle effi-
cient3Dperceptionfromthedatarepresentationlearningperspective.Weestab-
lish several LiDAR-based data representation learning settings that seamlessly
combinepretrainingwithweakly-andsemi-supervisedlearning,furtherenhanc-
ing the scalability of 3D perception systems.
3D Representation Learning. Analog to 2D representation learning strate-
gies [14,16,31,32,105], prior works designed contrastive [36,71,82,103,110,115],
masked modeling [33,51,97], and reconstruction [8,68] objectives for 3D pre-
training.Mostearly3Drepresentationlearningapproachesuseasinglemodality
for pretraining, leaving room for further development. The off-the-shelf calibra-
tions among different types of sensors provide a promising solution for building
pretraining objectives [64]. Recently, SLidR [83] has made the first contribution
towardmulti-modal3DrepresentationlearningbetweencameraandLiDARsen-
sors.Subsequentworks[67,75,112]extendedthisframeworkwithmoreadvanced
designs.Seal[62]leveragespowerfulvisionfoundationmodels[43,111,121,122]to
betterassistthecontrastivelearningacrosssensors.Puyet al.[76,77]conducted
a comprehensive study on the distillation recipe for better pretraining effects.
While these approaches have exhibited better performance than their baselines,
they overlooked the rich temporal cues across consecutive scans, which might
lead to sub-opt pretraining performance. In this work, we construct dense 3D
representation learning objectives using calibrated LiDAR sequences. Our ap-
proach encourages the consistency between features from sparse to dense inputs
and features across timestamps, yielding superiority over existing endeavors.
4D Representation Learning. Leveraging consecutive scans is promising in
extracting temporal relations [2,24,34,86]. For point cloud data pretraining,
prior works [17,65,84,85,116] mainly focused on applying 4D cues on object-
and human-centric point clouds, which are often small in scale. For large-scale
automotive point clouds, STRL [40] learns spatiotemporal data invariance with
different spatial augmentations in the point cloud sequence. TARL [72] and4D Contrastive Superflows are Dense 3D Representation Learners 5
STSSL [98] encourage similarities of point clusters in two consecutive frames,
where such clusters are obtained by ground removal and clustering algorithms,
i.e., RANSAC [26], Patchwork [56], and HDBSCAN [25]. BEVContrast [82]
sharesasimilarmotivationbututilizesBEVmapsforcontrastivelearning,which
yields a more effective implementation. The â€œone-fits-allâ€ clustering parameters,
however, are often difficult to obtain, hindering existing works. Different from
existing methods that use a single modality for 4D representation learning, we
proposetoleverageLiDAR-cameracorrespondencesandsemantic-richsuperpix-
els to establish meaningful multi-modality 4D pretraining objectives.
3 SuperFlow
In this section, we first revisit the common setups of the camera-to-LiDAR dis-
tillation baseline (cf. Sec. 3.1). We then elaborate on the technical details of
SuperFlow, encompassing a straightforward yet effective view consistency align-
ment (cf. Sec. 3.2), a dense-to-sparse consistency regularization (cf. Sec. 3.3),
and a flow-based spatiotemporal contrastive learning (cf. Sec. 3.4). The overall
pipeline of the proposed SuperFlow framework is depicted in Fig. 4.
3.1 Preliminaries
Problem Definition. Given a point cloud Pt = {pt,ft|i = 1,...,N} with
i i
N points captured by a LiDAR sensor at time t, where p âˆˆ R3 denotes the
i
coordinate of the point and f âˆˆ RC is the corresponding feature, we aim to
i
transfer knowledge from M surrounding camera images It = {It|i = 1,...,M}
i
into the point cloud. Here, I âˆˆ RHÃ—WÃ—3 represents an image with height H
i
and width W. Prior works [62,83] generate a set of class-agnostic superpixels
X = {Xj|j = 1,...,V} for each image via the unsupervised SLIC algorithm [1]
i i
or the more recent vision foundation models (VFMs) [43,121,122], where V de-
notes the total number of superpixels. Assuming that the point cloud Pt and
images It are calibrated, the point cloud p =(x ,y ,z ) can be then projected
i i i i
to the image plane (u ,v ) using the following sensor calibration parameters:
i i
1
[u ,v ,1]T = Ã—Î“ Ã—Î“ Ã—[x ,y ,z ]T , (1)
i i z K câ†l i i i
i
where Î“ denotes the camera intrinsic matrix and Î“ is the transformation
K câ†l
matrix from LiDAR sensors to surrounding-view cameras. We also obtain a set
of superpoints Y ={Yj|j =1,...,V} through this projection.
Network Representations. Let F : RNÃ—(3+C) â†’ RNÃ—D be a 3D backbone
Î¸p
with trainable parameters Î¸ , which takes LiDAR points as input and outputs
p
D-dimensional point features. Let G
Î¸i
: RHÃ—WÃ—3 â†’ RH SÃ—W SÃ—E be an image
backbone with pretrained parameters Î¸ that takes images as input and outputs
i
E-dimensionalimagefeatureswithstrideS.LetH :RNÃ—D â†’RNÃ—LandH :
Ï‰p Ï‰i
RH SÃ—W SÃ—E â†’ RHÃ—WÃ—L be linear heads with trainable parameters Ï‰
p
and Ï‰ i,6 X. Xu et al.
Negative Negative Positive
(a) Heuristic (b) ClassAgnostic (c) ViewConsistent
Fig.2: Comparisons of different superpixels.(a)Class-agnosticsuperpixelsgen-
eratedbytheunsupervisedSLIC[1]algorithm.(b)Class-agnosticsemanticsuperpixels
generated by vision foundation models (VFMs) [111,121,122]. (c) View-consistent se-
mantic superpixels generated by our view consistency alignment module.
whichprojectbackbonefeaturestoL-dimensionalfeatureswithâ„“ -normalization
2
and upsample image features to H Ã—W with bilinear interpolation.
Pretraining Objective. The overall objective of image-to-LiDAR represen-
tation learning [83] is to transfer knowledge from the trained image backbone
G to the 3D backbone F . The superpixels X generated offline, serve as an
Î¸i Î¸p i
intermediate to effectively guide the knowledge transfer process.
3.2 View Consistency Alignment
Motivation.Theclass-agnosticsuperpixelsX usedinpriorworks[62,67,83]are
i
typicallyinstance-levelanddonotconsidertheiractualcategories.Asdiscussed
in [67], instance-level superpixels can lead to â€œself-conflictâ€ problems, which un-
dermines the effectiveness of pretraining.
Superpixel Comparisons. Fig. 2 compares superpixels generated via the un-
supervised SLIC [1] and VFMs. SLIC [1] tends to over-segment objects, causing
semanticconflicts.VFMsgeneratesuperpixelsthroughapanopticsegmentation
head, which can still lead to â€œself-conflictâ€ in three conditions (see Fig. 2b): â‘ 
when the same object appears in different camera views, leading to different
parts of the same object being treated as negative samples; â‘¡ when objects of
thesamecategorywithinthesamecameraviewaretreatedasnegativesamples;
â‘¢ when objects across different camera views are treated as negative samples
even if they share the same label.
Semantic-RelatedSuperpixelsGeneration.Toaddresstheseissues,wepro-
posegeneratingsemantic-relatedsuperpixelstoensureconsistencyacrosscamera
views. Contrastive Vision-Language Pre-training (CLIP) [79] has shown great
generalization in few-shot learning. Building on existing VFMs [43,121,122], we
employ CLIPâ€™s text encoder and fine-tune the last layer of the segmentation
head from VFMs with predefined text prompts. This allows the segmentation
head to generate language-guided semantic categories for each pixel, which we
leverage as superpixels. As shown in Fig. 2c, we unify superpixels across camera4D Contrastive Superflows are Dense 3D Representation Learners 7
views based on semantic category, alleviating the â€œself-conflictâ€ problem in prior
image-to-LiDAR contrastive learning pipelines.
3.3 D2S: Dense-to-Sparse Consistency Regularization
Motivation. LiDAR points are sparse and often incomplete, significantly re-
stricting the efficacy of the cross-sensor feature representation learning process.
In this work, we propose to tackle this challenge by combining multiple LiDAR
scanswithinasuitabletimewindowtocreateadensepointcloud,whichisthen
used to encourage consistency with the sparse point cloud.
Point Cloud Concatenation.
Specifically, given a keyframe 2D-to-3D
point cloud Pt captured at time Correspondenc
e
t and a set of sweep point clouds
{Ps|s = 1,...,T} captured at
previous times s, we first trans-
Sparse PointCloud
formthecoordinate(xs,ys,zs)of
the sweep point cloud Ps to the
coordinatesystemsofPt,asthey
sharedifferentsystemsduetothe
vehicleâ€™s movement: View
2D-to-3D Consistent
Correspondenc Superpixels
[xËœs,yËœs,zËœs]T =Î“ tâ†sÃ—[xs,ys,zs]T,
Dense PointCloud
e
(2)
Fig.3: Dense-to-sparse (D2S) consistency
whereÎ“ denotesthetransfor-
tâ†s regularization module. Dense point clouds are
mation matrix from the sweep obtained by combining multiple point clouds
point cloud at time s to the capturedatdifferenttimes.AD2Sregularization
keyframe point cloud at time t. isformulatedbyencouragingtheconsistencybe-
We then concatenate the trans- tween dense features and sparse features.
formed sweep points {PËœs|s =
1,...T} with Pt to obtain a dense point cloud Pd. As shown in Fig. 3, Pd fuses
temporal information from consecutive point clouds, resulting in a dense and
semantically rich representation for feature learning.
Dense Superpoints. Meanwhile, we generate sets of superpoints Yd and Yt
for Pd and Pt, respectively, using superpixels Xt. Both Pt and Pd are fed into
the weight-shared 3D network F and H for feature extraction. The output
Î¸p Ï‰p
features are grouped via average pooling based on the superpoint indices to
obtain superpoint features Qd and Qt, where Qd âˆˆRVÃ—L and Qd âˆˆRVÃ—L. We
expect Qd and Qt to share similar features, leading to the following D2S loss:
V
1 (cid:88)
L = (1âˆ’<qt,qd >) , (3)
d2s V i i
i=1
where <Â·,Â·> denotes the scalar product to measure the similarity of features.
S2D
noitaziraluger8 X. Xu et al.
3D Branch 2D Branch
Time Time
ğ‘¡ğ‘¡âˆ’âˆ†ğ‘¡ğ‘¡ ğ‘¡ğ‘¡ ğ‘¡ğ‘¡+âˆ†ğ‘¡ğ‘¡ VFM ğ‘¡ğ‘¡+âˆ†ğ‘¡ğ‘¡ ğ‘¡ğ‘¡ ğ‘¡ğ‘¡âˆ’âˆ†ğ‘¡ğ‘¡
ğ’«ğ’« ğ’«ğ’« ğ’«ğ’« ğ¼ğ¼ ğ¼ğ¼ ğ¼ğ¼
Superpixel
Calib
â„±ğœƒğœƒğ‘ğ‘ â„±ğœƒğœƒğ‘ğ‘ â„±ğœƒğœƒğ‘ğ‘ ğ’¢ğ’¢ğœƒğœƒğ‘–ğ‘– ğ’¢ğ’¢ğœƒğœƒğ‘–ğ‘– ğ’¢ğ’¢ğœƒğœƒğ‘–ğ‘–
Superpoint
Gâ„‹roğœ”ğœ”uğ‘ğ‘p Gâ„‹roğœ”ğœ”uğ‘ğ‘p Gâ„‹roğœ”ğœ”uğ‘ğ‘p Gâ„‹roğœ”ğœ”uğ‘–ğ‘–p Gâ„‹roğœ”ğœ”uğ‘–ğ‘–p Gâ„‹roğœ”ğœ”uğ‘–ğ‘–p
ğ‘¡ğ‘¡+âˆ†ğ‘¡ğ‘¡ ğ‘¡ğ‘¡+âˆ†ğ‘¡ğ‘¡
ğ’¬ğ’¬ ğ’¦ğ’¦
ğ‘¡ğ‘¡ â„’tc â„’sc ğ‘¡ğ‘¡
ğ’¬ğ’¬ ğ’¦ğ’¦
ğ’¬ğ’¬ğ‘¡ğ‘¡âˆ’âˆ†ğ‘¡ğ‘¡ â„’tc â„’sc ğ’¦ğ’¦ğ‘¡ğ‘¡âˆ’âˆ†ğ‘¡ğ‘¡
Fig.4: Flow-based contrastive learnâ„’ is nc g (FCL) pipeline. FCL takes multiple
LiDAR-camerapairsfromconsecutivescansasinput.Basedontemporallyalignedse-
manticsuperpixelandsuperpoints,twocontrastivelearningobjectivesareformulated:
1)spatialcontrastivelearningbetweeneachLiDAR-camerapair(L ),and2)temporal
sc
contrastive learning among consecutive LiDAR point clouds across scenes (L ).
tc
3.4 FCL: Flow-Based Contrastive Learning
Motivation.LiDARpointcloudsareacquiredsequentially,embeddingrichdy-
namic scene information across consecutive timestamps. Prior works [62,67,83]
primarily focused on single LiDAR scans, overlooking the consistency of mov-
ing objects across scenes. To address these limitations, we propose flow-based
contrastive learning (FCL) across sequential LiDAR scenes to encourage spa-
tiotemporal consistency.
SpatialContrastiveLearning.Ourframework,depictedinFig.4,takesthree
LiDAR-camera pairs from different timestamps within a suitable time window
as input, i.e., {(Pt,It),(Pt+âˆ†t,It+âˆ†t),(Ptâˆ’âˆ†t,Itâˆ’âˆ†t)}, where timestamp t de-
notesthecurrentsceneandâˆ†tisthetimespan.Followingpreviousworks[62,83],
wefirstdistillknowledgefromthe2Dnetworkintothe3Dnetworkforeachscene
separately.Taking(Pt,It)asanexample,Pt andIt arefedintothe3Dand2D
networks to extract per-point and image features. The output features are then
grouped via average pooling based on superpoints Yt and superpixels Xt to ob-
tainsuperpointfeaturesQtandsuperpixelfeaturesKt.Aspatialcontrastiveloss
isformulatedtoconstrain3Drepresentationviapretrained2Dpriorknowledge.
This process is formulated as follows:
1 (cid:88)V (cid:34) e(<qi,ki>/Ï„) (cid:35)
L =âˆ’ log , (4)
sc V (cid:80) e(<qi,kj>/Ï„)+e(<qi,ki>/Ï„)
i=1 jÌ¸=i
where Ï„ >0 is a temperature that controls the smoothness of distillation.4D Contrastive Superflows are Dense 3D Representation Learners 9
Flow-Based Contrastive Learning. The spatial contrastive learning objec-
tive between images and point clouds, as depicted in Eq. (4), fails to ensure
thatmovingobjectssharesimilarattributesacrossdifferentscenes.Tomaintain
consistency across scenes, a temporal consistency loss is introduced among su-
perpointfeaturesacrossdifferentscenes.ForthepointcloudsPt andPt+âˆ†t,the
corresponding superpoint features Qt and Qt+âˆ†t are obtained via their super-
points. The temporal contrastive loss operates on Qt and Qt+âˆ†t:
ï£® ï£¹
Lt tâ† c t+âˆ†t =âˆ’ V1 (cid:88) i=V 1logï£° (cid:80) jÌ¸=ie(<qt i,qe t j( +< âˆ†q tt i >,q /it Ï„+ )âˆ† +t> e/ (Ï„ <) qt i,qt i+âˆ†t>/Ï„)ï£» . (5)
ThesamefunctionisalsoappliedbetweenQt andQtâˆ’âˆ†t.Thisapproachenables
pointfeaturesattimettoextractmorecontext-awareinformationacrossscenes.
4 Experiments
4.1 Settings
Data. We follow the seminar works SLidR [83] and Seal [62] when preparing
the datasets. A total of eleven datasets are used in our experiments, including
1nuScenes [27], 2SemanticKITTI [5], 3Waymo Open [90], 4ScribbleKITTI [96],
5RELLIS-3D [42], 6SemanticPOSS [74], 7SemanticSTF [101], 8SynLiDAR [99],
9DAPS-3D [44],10Synth4D [81],and11Robo3D [46].Duetospacelimits,kindly
refer to the Appendix and [62,83] for additional details about these datasets.
Implementation Details. SuperFlow is implemented using the MMDetec-
tion3D[21]andOpenPCSeg[60]codebases.Consistentwithpriorworks[62,83],
weemployMinkUNet[20]asthe3DbackboneandDINOv2[73](withViTback-
bones [23]) as the 2D backbone, distilling from three variants: small (S), base
(B), and large (L). Following Seal [62], OpenSeeD [111] is used to generate se-
mantic superpixels. The framework is pretrained end-to-end on 600 scenes from
nuScenes [27], then linear probed and fine-tuned on nuScenes [27] according to
the data splits in SLidR [83]. The domain generalization study adheres to the
same configurations as Seal [62] for the other ten datasets. Both the baselines
andSuperFlowarepretrainedusingeightGPUsfor50epochs,whilelinearprob-
ing and downstream fine-tuning experiments use four GPUs for 100 epochs, all
utilizing the AdamW optimizer [66] and OneCycle scheduler [88]. Due to space
limits, kindly refer to the Appendix for additional implementation details.
Evaluation Protocols.Followingconventions,wereporttheIntersection-over-
Union (IoU) on each semantic class and mean IoU (mIoU) over all classes for
downstream tasks. For 3D robustness evaluations, we follow Robo3D [46] and
report the mean Corruption Error (mCE) and mean Resilience Rate (mRR).
4.2 Comparative Study
Linear Probing. We start by investigating the pretraining quality via linear
probing. For this setup, we initialize the 3D backbone F with pretrained pa-
Î¸p
rametersandfine-tuneonlytheadded-onsegmentationhead.AsshowninTab.1,10 X. Xu et al.
Table 1: Comparisons of state-of-the-art pretraining methods pretrained on
nuScenes [27]andfine-tunedonSemanticKITTI [5]andWaymo Open [90]withspec-
ified data portions, respectively. All methods use MinkUNet [20] as the 3D semantic
segmentationbackbone.LPdenoteslinearprobingwithafrozenbackbone.Allscores
are given in percentage (%). Best scores in each configuration are shaded with colors.
nuScenes KITTI Waymo
Method Venue Distill
LP 1% 5% 10% 25% Full 1% 1%
Random - - 8.10 30.30 47.84 56.15 65.48 74.66 39.50 39.41
PointContrast[103] ECCVâ€™20 Noneâ—¦ 21.90 32.50 - - - - 41.10 -
DepthContrast[115] ICCVâ€™21 Noneâ—¦ 22.10 31.70 - - - - 41.50 -
ALSO[8] CVPRâ€™23 Noneâ—¦ - 37.70 - 59.40 - 72.00 - -
BEVContrast[82] 3DVâ€™24 Noneâ—¦ - 38.30 - 59.60 - 72.30 - -
PPKT[64] arXivâ€™21 ResNetâ—¦ 35.90 37.80 53.74 60.25 67.14 74.52 44.00 47.60
SLidR[83] CVPRâ€™22 ResNetâ—¦ 38.80 38.30 52.49 59.84 66.91 74.79 44.60 47.12
ST-SLidR[67] CVPRâ€™23 ResNetâ—¦ 40.48 40.75 54.69 60.75 67.70 75.14 44.72 44.93
TriCC[75] CVPRâ€™23 ResNetâ—¦ 38.00 41.20 54.10 60.40 67.60 75.60 45.90 -
Seal[62] NeurIPSâ€™23 ResNetâ—¦ 44.95 45.84 55.64 62.97 68.41 75.60 46.63 49.34
HVDistill[112] IJCVâ€™24 ResNetâ—¦ 39.50 42.70 56.60 62.90 69.30 76.60 49.70 -
PPKT[64] arXivâ€™21 ViT-Sâ—¦ 38.60 40.60 52.06 59.99 65.76 73.97 43.25 47.44
SLidR[83] CVPRâ€™22 ViT-Sâ—¦ 44.70 41.16 53.65 61.47 66.71 74.20 44.67 47.57
Seal[62] NeurIPSâ€™23 ViT-Sâ—¦ 45.16 44.27 55.13 62.46 67.64 75.58 46.51 48.67
SuperFlow Ours ViT-Sâ€¢ 46.44 47.81 59.44 64.47 69.20 76.54 47.97 49.94
PPKT[64] arXivâ€™21 ViT-Bâ—¦ 39.95 40.91 53.21 60.87 66.22 74.07 44.09 47.57
SLidR[83] CVPRâ€™22 ViT-Bâ—¦ 45.35 41.64 55.83 62.68 67.61 74.98 45.50 48.32
Seal[62] NeurIPSâ€™23 ViT-Bâ—¦ 46.59 45.98 57.15 62.79 68.18 75.41 47.24 48.91
SuperFlow Ours ViT-Bâ€¢ 47.66 48.09 59.66 64.52 69.79 76.57 48.40 50.20
PPKT[64] arXivâ€™21 ViT-Lâ—¦ 41.57 42.05 55.75 61.26 66.88 74.33 45.87 47.82
SLidR[83] CVPRâ€™22 ViT-Lâ—¦ 45.70 42.77 57.45 63.20 68.13 75.51 47.01 48.60
Seal[62] NeurIPSâ€™23 ViT-Lâ—¦ 46.81 46.27 58.14 63.27 68.67 75.66 47.55 50.02
SuperFlow Ours ViT-Lâ€¢ 48.01 49.95 60.72 65.09 70.01 77.19 49.07 50.67
SuperFlowconsistentlyoutperformsstate-of-the-artmethodsunderdiversecon-
figurations.Weattributethistotheuseoftemporalconsistencylearning,which
captures the structurally rich temporal cues across consecutive scenes and en-
hancesthesemanticrepresentationlearningofthe3Dbackbone.Wealsoobserve
improved performance with larger 2D networks (i.e., from ViT-S to ViT-L), re-
vealing a promising direction of achieving higher quality 3D pretraining.
Downstream Fine-Tuning. It is known that data representation learning
can mitigate the need for large-scale human annotations. Our study systemati-
callycomparesSuperFlowwithpriorworksonthreepopulardatasets,including
nuScenes [27],SemanticKITTI [5],andWaymoOpen [90],underlimitedannota-
tions for few-shot fine-tuning. From Tab. 1, we observe that SuperFlow achieves
promising performance gains among three datasets across all fine-tuning tasks.
Wealsousethepretrained3Dbackboneasinitializationforthefully-supervised
learning study on nuScenes [27]. As can be seen from Tab. 1, models pretrained
via representation learning consistently outperform the random initialization
counterparts, highlighting the efficacy of conducting data pretraining. We also
find that distillations from larger 2D networks show consistent improvements.
Cross-DomainGeneralization.ToverifythestronggeneralizabilityofSuper-
Flow,weconductacomprehensivestudyusingsevendiverseLiDARdatasetsand4D Contrastive Superflows are Dense 3D Representation Learners 11
Table2:Domaingeneralizationstudyofdifferentpretrainingmethodspretrained
onthenuScenes [27]datasetandfine-tunedonotherseven heterogeneous3Dsemantic
segmentation datasets with specified data portions, respectively. All scores are given
in percentage (%). Best scores in each configuration are shaded with colors.
ScriKITTI Rellis-3D SemPOSS SemSTF SynLiDAR DAPS-3D Synth4D
Method
1% 10% 1% 10% Half Full Half Full 1% 10% Half Full 1% 10%
Random 23.81 47.60 38.46 53.60 46.26 54.12 48.03 48.15 19.89 44.74 74.32 79.38 20.22 66.87
PPKT[64] 36.50 51.67 49.71 54.33 50.18 56.00 50.92 54.69 37.57 46.48 78.90 84.00 61.10 62.41
SLidR[83] 39.60 50.45 49.75 54.57 51.56 55.36 52.01 54.35 42.05 47.84 81.00 85.40 63.10 62.67
Seal[62] 40.64 52.77 51.09 55.03 53.26 56.89 53.46 55.36 43.58 49.26 81.88 85.90 64.50 66.96
SuperFlow 42.70 54.00 52.83 55.71 54.41 57.33 54.72 56.57 44.85 51.38 82.43 86.21 65.31 69.43
Table 3: Out-of-distribution 3D robustness studyofstate-of-the-artpretraining
methodsundercorruptionandsensorfailurescenariosinthenuScenes-C datasetfrom
the Robo3D benchmark [46]. Full denotes fine-tuning with full labels. LP denotes
linear probing with a frozen backbone. All mCE (â†“), mRR (â†‘), and mIoU (â†‘) scores
are given in percentage (%). Best scores in each configuration are shaded with colors.
# Initial Backbone mCE mRR Fog Rain Snow Blur Beam Cross Echo Sensor Avg
Random MinkU-18â—¦ 115.61 70.85 53.90 71.10 48.22 51.85 62.21 37.73 57.47 38.97 52.68
SuperFlow MinkU-18â€¢ 109.00 75.66 54.95 72.79 49.56 57.68 62.82 42.45 59.61 41.77 55.21
Random MinkU-34â—¦ 112.20 72.57 62.96 70.65 55.48 51.71 62.01 31.56 59.64 39.41 54.18
PPKT[64] MinkU-34â—¦ 105.64 75.87 64.01 72.18 59.08 57.17 63.88 36.34 60.59 39.57 56.60
SLidR[83] MinkU-34â—¦ 106.08 75.99 65.41 72.31 56.01 56.07 62.87 41.94 61.16 38.90 56.83
Seal[62] MinkU-34â—¦ 92.63 83.08 72.66 74.31 66.22 66.14 65.96 57.44 59.87 39.85 62.81
SuperFlow MinkU-34â€¢ 91.67 83.17 70.32 75.77 65.41 61.05 68.09 60.02 58.36 50.41 63.68
Random MinkU-50â—¦ 113.76 72.81 49.95 71.16 45.36 55.55 62.84 36.94 59.12 43.15 53.01
SuperFlow MinkU-50â€¢ 107.35 74.02 54.36 73.08 50.07 56.92 64.05 38.10 62.02 47.02 55.70
RandomMinkU-101â—¦ 109.10 74.07 50.45 73.02 48.85 58.48 64.18 43.86 59.82 41.47 55.02
SuperFlowMinkU-101â€¢ 96.44 78.57 56.92 76.29 54.70 59.35 71.89 55.13 60.27 51.60 60.77
PPKT[64] MinkU-34â—¦ 183.44 78.15 30.65 35.42 28.12 29.21 32.82 19.52 28.01 20.71 28.06
SLidR[83] MinkU-34â—¦ 179.38 77.18 34.88 38.09 32.64 26.44 33.73 20.81 31.54 21.44 29.95
Seal[62] MinkU-34â—¦ 166.18 75.38 37.33 42.77 29.93 37.73 40.32 20.31 37.73 24.94 33.88
SuperFlow MinkU-34â€¢ 161.78 75.52 37.59 43.42 37.60 39.57 41.40 23.64 38.03 26.69 35.99
showresultsinTab.2.Itisworthnotingthatthesedatasetsarecollectedunder
differentacquisitionandannotationconditions,includingadverseweather,weak
annotations, synthetic collection, and dynamic objects. For all fourteen domain
generalization fine-tuning tasks, SuperFlow exhibits superior performance over
thepriorarts[62,64,83].Thisstudystronglyverifiestheeffectivenessofthepro-
posed flow-based contrastive learning for image-to-LiDAR data representation.
Out-of-Distribution Robustness. The robustness of 3D perception models
againstunprecedentedconditionsdirectlycorrelateswiththemodelâ€™sapplicabil-
ity to real-world applications [30,49,55,104]. We compare our SuperFlow with
prior models in the nuScenes-C dataset from the Robo3D benchmark [46] and
show results in Tab. 3. We observe that models pretrained using SuperFlow ex-
hibit improved robustness over the random initialization counterparts. Besides,
wefindthat3Dnetworkswithdifferentcapacitiesoftenposediverserobustness.
Quantitative Assessments. We visualize the prediction results fine-tuned on
nuScenes[27],SemanticKITTI[5]andWaymoOpen[90],comparedwithrandom
lluF
PL12 X. Xu et al.
Table 4: Ablation study of Super- Table 5: Ablation study of SuperFlow
Flow using different # of sweeps. All onnetworkcapacity(#params)of3Dback-
methodsuseViT-B[73]fordistillation. bones. All methods use ViT-B [73] for dis-
All scores are given in percentage (%). tillation. All scores are given in percentage
Baselineresultsareshadedwithcolors. (%).Baselineresultsareshadedwithcolors.
nuScenes KITTI Waymo nuScenes KITTI Waymo
Backbone BackboneLayer
LP 1% 1% 1% LP 1% 1% 1%
1Ã—Sweepsâ—¦ 47.41 47.52 48.14 49.31 MinkUNetâ—¦ 18 47.20 47.70 48.04 49.24
2Ã—Sweepsâ€¢ 47.66 48.09 48.40 50.20 MinkUNetâ€¢ 34 47.66 48.09 48.40 50.20
5Ã—Sweepsâ—¦ 47.23 48.00 47.94 49.14 MinkUNetâ—¦ 50 54.11 52.86 49.22 51.20
7Ã—Sweepsâ—¦ 46.03 47.98 46.83 47.97 MinkUNetâ—¦ 101 52.56 51.19 48.51 50.01
Ground Truth Random SLidR Seal SuperFlow
Fig.5: Qualitative assessments of state-of-the-art pretraining methods pretrained
on nuScenes [27] and fine-tuned on nuScenes [27], SemanticKITTI [5], and Waymo
Open [90], with 1% annotations. The error maps show the correct and incorrect pre-
dictionsingrayandred,respectively.Bestviewedincolorsandzoomed-infordetails.
initialization, SLiDR [83], and Seal [62]. As shown in Fig. 5, Superflow performs
well,especiallyonbackgrounds,i.e.,â€œroadâ€ andâ€œsidewalkâ€ incomplexscenarios.
4.3 Ablation Study
In this section, we are tailored to understand the efficacy of each design in our
SuperFlow framework. Unless otherwise specified, we adopt MinkUNet-34 [20]
andViT-B[73]asthe3Dand2Dbackbones,respectively,throughoutthisstudy.
3D Network Capacity. Existing 3D backbones are relatively small in scale
compared to their 2D counterparts. We study the scale of the 3D network and
theresultsareshowninTab.5.Weobserveimprovedperformanceasthenetwork
capacityscalesup,exceptforMinkUNet-101[20].Weconjecturethatthisisdue4D Contrastive Superflows are Dense 3D Representation Learners 13
Table 6: Ablation study of each com- Table 7: Ablation study on spa-
ponent in SuperFlow. All variants use a tiotemporal consistency. All vari-
MinkUNet-34 [20] as the 3D backbone and ants use a MinkUNet-34 [20] as the 3D
ViT-B[73]fordistillation.VC:Viewconsis- backbone and ViT-B [73] for distilla-
tency.D2S:Dense-to-sparseregularization. tion.0denotescurrenttimestamp.0.5s
FCL: Flow-based contrastive learning. All corresponds to a 20Hz timespan. All
scores are given in percentage (%). scores are given in percentage (%).
nuScenes KITTI Waymo nuScenes KITTI Waymo
# VC D2S FCL Timespan
LP 1% 1% 1% LP 1% 1% 1%
- Random 8.10 30.30 39.50 39.41 Single-Frame 46.17 46.91 47.26 49.01
(a) âœ— âœ— âœ— 44.65 44.47 46.65 47.77 0,âˆ’0.5s 46.39 47.08 47.99 49.78
(b) âœ“ âœ— âœ— 45.57 45.21 46.87 48.01 âˆ’0.5s,0,+0.5s 47.66 48.09 48.40 50.20
(c) âœ“ âœ“ âœ— 46.17 46.91 47.26 49.01 âˆ’1.0s,0,+1.0s 47.60 47.99 48.43 50.18
(d) âœ“ âœ— âœ“ 47.24 47.67 48.21 49.80 âˆ’1.5s,0,+1.5s 46.43 48.27 48.34 49.93
(e) âœ“ âœ“ âœ“ 47.66 48.09 48.40 50.20 âˆ’2.0s,0,+2.0s 46.20 48.49 48.18 50.01
to the fact that models with limited parameters are less effective in capturing
patternsduringrepresentationlearning,and,conversely,modelswithalargeset
of trainable parameters tend to be difficult to converge.
Representation Density. The consistency regularization between sparse and
dense point clouds encourages useful representation learning. To analyze the
degree of regularization, we investigate various point cloud densities and show
the results in Tab. 4. We observe that a suitable point cloud density can im-
prove the modelâ€™s ability to feature representation. When the density of point
clouds is too dense, the motion of objects is obvious in the scene. However, we
generate superpoints of the dense points based on superpixels captured at the
timeofsparsepoints.Thedisplacementdifferenceofdynamicobjectsmakesthe
projection misalignment. A trade-off selection would be two or three sweeps.
Temporal Consistency. The ability to capture semantically coherent tempo-
ral cues is crucial in our SuperFlow framework. In Eq. (5), we operate temporal
contrastive learning on superpoints features across scenes. As shown in Tab. 7,
we observe that temporal contrastive learning achieves better results compared
to single-frame methods. We also compare the impact of frames used to capture
temporal cues. When we use 3 frames, it acquires more context-aware informa-
tion than 2 frames and achieves better results. Finally, we study the impact of
the timespan between frames. The performance will drop with a longer times-
pan. We conjecture that scenes with short timespans have more consistency,
while long timespans tend to have more uncertain factors.
Component Analysis. In Tab. 6, we analyze each component in the Super-
Flow framework,including viewconsistency,dense-to-sparse regularization,and
flow-based contrastive learning. The baseline is SLiDR [83] with VFMs-based
superpixels. View consistency brings slight improvements among the popular
datasets with a few annotations. D2S distills dense features into sparse features
and it brings about 1% mIoU gains. FCL extracts temporal cues via temporal
contrastive learning and it significantly leads to about 2.0% mIoU gains.
Visual Inspections. Similarity maps presented in Fig. 6 denote the segmen-
tation ability of our pretrained model. The query points include â€œcarâ€, â€œman-14 X. Xu et al.
(a) â€œcarâ€ (3D) (b) â€œmanmadeâ€ (3D) (c) â€œsidewalkâ€ (3D)
(d) â€œcarâ€ (2D) (e) â€œmanmadeâ€ (2D) (f) â€œsidewalkâ€ (2D)
(g) â€œvegetationâ€ (3D) (h) â€œdriveablesurfaceâ€ (3D) (i) â€œterrainâ€ (3D)
(j) â€œvegetationâ€ (2D) (k) â€œdriveablesurfaceâ€ (2D) (l) â€œterrainâ€ (2D)
Fig.6:Cosinesimilaritybetweenfeaturesofaquerypoint(reddot)and:1)features
of other points projected in the image (the 1st and 3rd rows); and 2) features of an
image with the same scene (the 2nd and 4th rows). The color goes from red to blue
denoting low and high similarity scores, respectively. Best viewed in color.
madeâ€, â€œsidewalkâ€, â€œvegetationâ€, â€œdriveable surfaceâ€, and â€œterrainâ€. SuperFlows
shows strong semantic discriminative ability without fine-tuning. We conjec-
ture that it comes from three aspects: 1) View consistent superpixels enable the
network to learn semantic representation; 2) Dense-to-sparse regularization en-
hances the network to learn varying density features; 3) Temporal contrastive
learning extracts semantic cues across scenes.
5 Conclusion
In this work, we presented SuperFlow to tackle the challenging 3D data rep-
resentation learning. Motivated by the sequential nature of LiDAR acquisitions,
weproposedthreenoveldesignstobetterencouragespatiotemporalconsistency,
encompassing view consistency alignment, dense-to-sparse regularization, and
flow-based contrastive learning. Extensive experiments across 11 diverse LiDAR
datasets showed that SuperFlow consistently outperforms prior approaches in
linear probing, downstream fine-tuning, and robustness probing. Our study on
scaling up 2D and 3D network capacities reveals insightful findings. We hope
this work could shed light on future designs of powerful 3D foundation models.4D Contrastive Superflows are Dense 3D Representation Learners 15
Acknowledgements. This work was supported by the Scientific and Techno-
logical Innovation 2030 - â€œNew Generation Artificial Intelligenceâ€ Major Project
(No.2021ZD0112200),theJointFundsoftheNationalNaturalScienceFounda-
tion of China (No. U21B2044), the Key Research and Development Program of
Jiangsu Province (No. BE2023016-3), and the Talent Research Start-up Foun-
dationofNanjingUniversityofPostsandTelecommunications(No.NY223172).
ThisworkwasalsosupportedbytheMinistryofEducation,Singapore,underits
MOEAcRFTier2(MOET2EP20221-0012),NTUNAP,andundertheRIE2020
IndustryAlignmentFundâ€“IndustryCollaborationProjects(IAF-ICP)Funding
Initiative,aswellascashandin-kindcontributionfromtheindustrypartner(s).
Appendix
â€“ 6. Additional Implementation Detail ..............................15
â€¢ 6.1 Datasets .......................................................15
â€¢ 6.2 Training Configurations ........................................17
â€¢ 6.3 Evaluation Configurations ......................................18
â€“ 7. Additional Quantitative Result .................................18
â€¢ 7.1 Class-Wise Linear Probing Results .............................19
â€¢ 7.2 Class-Wise Fine-Tuning Results ................................19
â€“ 8. Additional Qualitative Result ...................................19
â€¢ 8.1 LiDAR Segmentation Results ...................................19
â€¢ 8.2 Cosine Similarity Results .......................................19
â€“ 9. Limitation and Discussion .......................................19
â€¢ 9.1 Potential Limitations ...........................................19
â€¢ 9.2 Potential Societal Impact .......................................20
â€“ 10. Public Resources Used .........................................20
â€¢ 10.1 Public Codebase Used .........................................20
â€¢ 10.2 Public Datasets Used ..........................................21
â€¢ 10.3 Public Implementations Used ..................................21
6 Additional Implementation Detail
Inthissection,weelaborateonadditionaldetailsregardingthedatasets,hyper-
parameters, and training/evaluation configuration.
6.1 Datasets
Pretraining. In this work, we pretrain the model on the nuScenes [27] dataset
following the data split in SLidR [83]. Specifically, 600 scenes are used as the
training set for model pretraining, which is a mini-train split of the whole 700
trainingscenes.ItincludesbothLiDARpointcloudsandsixcameraimagedata,
fromlabeledkeyframedatatomultipleunlabeledsweeps.Weconductspatiotem-
poralcontrastivelearningwithkeyframedataanddense-to-sparseregularization
by combining multiple LiDAR sweeps to form dense points.16 X. Xu et al.
Table 8: Summary of datasets used in this work. Our study encompasses a
total of 10 datasets in the linear probing and downstream generalization experiments,
including 1nuScenes [27], 2SemanticKITTI [5], 3Waymo Open [90], 4ScribbleKITTI
[96], 5RELLIS-3D [42], 6SemanticPOSS [74], 7SemanticSTF [101], 8SynLiDAR [99],
9DAPS-3D [44], 10Synth4D [81], and 11nuScenes-C [46]. Images adopted from the
original papers.
nuScenes SemanticKITTI WaymoOpen ScribbleKITTI RELLIS-3D
SemanticPOSS SemanticSTF SynLiDAR DAPS-3D Synth4D
Linear Probing. We train the 3D backbone network with the fixed pretrained
backbone on the training set of nuScenes [27], and evaluate the performance on
the validation set. It consists of 700 training scenes (for 29,130 samples) and
150 validation scenes (for 6,019 samples). Following the conventional setup, the
evaluation results are calculated among 16 merged semantic categories.
Downstream Fine-Tuning. To validate the pretraining quality of each self-
supervised learning approach, we conduct a comprehensive downstream fine-
tuning experiment on the nuScenes [27] dataset, with various configurations.
Specifically, we train the 3D backbone network with the pretrained backbone
using 1%, 5%, 10%, 25%, and 100% annotated data, respectively, and evaluate
the modelâ€™s performance on the official validation set.
Cross-DomainFine-Tuning.Inthiswork,weconductacomprehensivecross-
domain fine-tuning experiment on a total of 9 datasets. Tab. 8 provides a sum-
mary of these datasets. Specifically, SemanticKITTI [5] and Waymo Open [90]
contain large-scale LiDAR scans collected from real-world driving scenes, which
are acquired by 64-beam LiDAR sensors. We construct the 1% training sample
setbysamplingevery100framefromthewholetrainingset.ScribbleKITTI [96]
shares the same scene with SemanticKITTI [5] but are weakly annotated with
line scribbles. The total percentage of valid annotated labels is 8.06% com-
pared to fully-supervised methods, while saving about 90% annotation times.
RELLIS-3D [42] is a multimodal dataset collected in an off-road environment.
It contains 13,556 annotated LiDAR scans, which present challenges to class
imbalance and environmental topography. SemanticPOSS [74] is a small-scale
pointclouddatasetwithrichdynamicinstancescapturedinPekingUniversity.It4D Contrastive Superflows are Dense 3D Representation Learners 17
Table 9: Examples of the out-of-distribution (OoD) scenarios. Our study en-
compasses a total of 8 common OoD scenarios in the 3D robustness evaluation exper-
iments, including 1fog, 2wet ground, 3snow, 4motion blur, 5beam missing, 6crosstalk,
7incomplete echo, and 8cross sensor. Images adopted from the Robo3D [46] paper.
Fog WetGround Snow MotionBlur
BeamMissing Crosstalk IncompleteEcho CrossSensor
consistsof6LiDARsequences,wheresequence2isthevalidationsetandthere-
mainingdataformsthetrainingset.SemanticSTF [101]consistsof2,076LiDAR
scansfromvariousadverseweatherconditions,includingâ€œsnowyâ€,â€œdense-foggyâ€,
â€œlight-foggyâ€, and â€œrainyâ€ scans. The dataset is split into three sets: 1,326 scans
for training, 250 scans for validation, and 500 scans for testing. SynLiDAR [99],
Synth4D [81], and DAPS-3D [44] are synthetic datasets captured from various
simulators. SynLiDAR [99] contains 13 LiDAR sequences with totally 198,396
samples. Synth4D [81] includes two subsets and we use Synth4D-nuScenes in
this work. It comprises of 20,000 point clouds captured in different scenarios,
including town, highway, rural area, and city. DAPS-3D includes two subsets
and we use DAPS-1, which is semi-synthetic with larger scale in this work. It
contains 11 sequences with about 23,000 LiDAR scans.
Out-of-Distribution Robustness Evaluation. In this work, we conduct a
comprehensive out-of-distribution (OoD) robustness evaluation experiment on
the nuScenes-C dataset from the Robo3D [46] benchmark. As shown in Tab. 9,
there are a total of 8 OoD scenarios in the nuScenes-C dataset, including â€œfogâ€,
â€œwet groundâ€, â€œsnowâ€, â€œmotion blurâ€, â€œbeam missingâ€, â€œcrosstalkâ€, â€œincomplete
echoâ€, and â€œcross sensorâ€. Each scenario is further split into three levels (â€œlightâ€,
â€œmoderateâ€,â€œheavyâ€)basedonitsseverity.Wetesteachmodelonallthreelevels
and report the average results.
6.2 Training Configurations
Inthiswork,weimplementtheMinkUNet[20]networkwiththeTorchSparse[91]
backendasour3Dbackbone.Thepointcloudsarepartitionedundercylindrical18 X. Xu et al.
voxelsofsize0.10meter.Forthe3Dnetwork,pointcloudsarerandomlyrotated
around the z-axis, flipped along x-axis and y-axis with a 50% probability, and
scaled with a factor between 0.95 and 1.05 during pretraining and downstream
fine-tuning. For the 2D network, we choose ViT pretrained from DINOV2 [73]
with three variants: ViT-S, ViT-B, and ViT-L. The image data are resized to
224Ã—448,andflippedhorizontallywitha50%probabilityduringpretraining.For
pretraining, we randomly choose 3 camera images as inputs of the 2D network.
To enable view consistency alignment, we use the class names as the prompts
whengeneratingthesemanticsuperpixels.WetrainthenetworkwitheightGPUs
for 50 epochs and the batch size is set to 4 for each GPU. For downstream fine-
tuning, we use the same data split as [62] for all datasets. The loss function of
segmentationisacombinationofcross-entropylossandLovÃ¡sz-Softmaxloss[7].
We train the segmentation network with four GPUs for 100 epochs and the
batchsizeissetto2foreachGPU.AllthemodelsaretrainedwiththeAdamW
optimizer [66] and OneCycle scheduler [88]. The learning rate is set as 0.01 and
0.001 for pretraining and fine-tuning, respectively.
6.3 Evaluation Configurations
Followingconventions,wereportIntersection-over-Union(IoU)foreachcategory
i and mean IoU (mIoU) across all categories. IoU can be formulated as follows:
TP
IoU = i , (6)
i TP +FP +FN
i i i
where TP ,FP ,FN are true positives, false positives, and false negatives for
i i i
category i, respectively. For robust protocol, we utilize the Corruption Error
(CE)andResilienceRate(RR)metrics,followingRobo3D[46],whicharedefined
as follows:
(cid:80)3 (1âˆ’IoUj) (cid:80)3 (1âˆ’IoUj)
CE = j=1 i ,RR = j=1 i , (7)
i (cid:80)3 j=1(1âˆ’IoUj ibase) i 3Ã—IoU clean
where IoUj is the mIoU calculated at the i-th scene for the j-th level; IoUj
i ibase
andIoU arescoresofthebaselinemodelandscoresontheâ€œcleanâ€ validation
clean
set. For a fair comparison with priors, all models are tested without test time
augmentationormodelensembleforbothlinearprobinganddownstreamtasks.
7 Additional Quantitative Result
In this section, we supplement the complete results (i.e., the class-wise LiDAR
semantic segmentation results) to better support the findings and conclusions
drawn in the main body of this paper.4D Contrastive Superflows are Dense 3D Representation Learners 19
7.1 Class-Wise Linear Probing Results
We present the class-wise IoU scores for the linear probing experiments in
Tab. 10. We also implement PPKT [64], SLidR [83], and Seal [62] with the
distillation of ViT-S, ViT-B, and ViT-L. The results show that SuperFlow out-
performs state-of-the-art pretraining methods significantly for most semantic
classes. Some notably improved classes are: â€œbarrierâ€, â€œbusâ€, â€œtraffic coneâ€, and
â€œterrainâ€. Additionally, we observe a consistent trend of performance improve-
ments using larger models for the cross-sensor distillation.
7.2 Class-Wise Fine-Tuning Results
We present the class-wise IoU scores for the 1% fine-tuning experiments in
Tab. 11. We observe that a holistic improvement brought by SuperFlow com-
pared to state-of-the-art pretraining methods.
8 Additional Qualitative Result
In this section, we provide additional qualitative examples to help visually com-
pare different approaches presented in the main body of this paper.
8.1 LiDAR Segmentation Results
We provide additional qualitative assessments in Fig. 7, Fig. 8, and Fig. 9. The
resultsverifyagainthesuperiorityofSuperFlowoverpriorpretrainingmethods.
8.2 Cosine Similarity Results
We provide additional cosine similarity maps in Fig. 10 and Fig. 11. The results
consistently verify the efficacy of SuperFlow in learning meaningful representa-
tions during flow-based spatiotemporal contrastive learning.
9 Limitation and Discussion
In this section, we elaborate on the limitations and potential negative societal
impact of this work.
9.1 Potential Limitations
Although SuperFlow holistically improves the image-to-LiDAR self-supervised
learning efficacy, there are still rooms for further explorations.20 X. Xu et al.
DynamicObjects.Asshownin
Fig. 12, dynamic objects across
frames may share different su-
perpixels due to variant scales in
the images. The objects across
frames will be regarded as neg-
ative samples, which will cause
â€œtemporalconflictâ€ undertempo-
ğ‘¡ğ‘¡Fâˆ’igâˆ†.ğ‘¡ğ‘¡12: Possible ğ‘¡ğ‘¡temporal confl ğ‘¡ğ‘¡i +cts âˆ†. ğ‘¡ğ‘¡
ral contrastive learning.
Mis-Align Between LiDAR and Cameras. The calibration parameters be-
tween LiDAR and cameras are not perfect as they work at different frequencies.
This will cause possible misalignment between superpoints and superpixels, es-
pecially when using dense point clouds to distill sparse point clouds. This also
restricts the scalability to form much denser points from sweep points.
9.2 Potential Societal Impact
LiDAR systems can capture detailed 3D images of environments, potentially
including private spaces or sensitive information. If not properly managed, this
could lead to privacy intrusions, as individuals might be identifiable from the
data collected, especially when combined with other data sources. Additionally,
dependenceonautomatedsystemsthatuseLiDARsemanticsegmentationcould
lead to overreliance and trust in technology, potentially causing safety issues
if the systems fail or make incorrect decisions. This is particularly critical in
applications involving human safety.
10 Public Resources Used
In this section, we acknowledge the use of the following public resources, during
the course of this work.
10.1 Public Codebase Used
We acknowledge the use of the following public codebase during this work:
â€“ MMCV6 ..............................................Apache License 2.0
â€“ MMDetection7 ........................................Apache License 2.0
â€“ MMDetection3D8 .....................................Apache License 2.0
â€“ MMEngine9 ...........................................Apache License 2.0
â€“ MMPreTrain10 ........................................Apache License 2.0
â€“ OpenPCSeg11 .........................................Apache License 2.0
6 https://github.com/open-mmlab/mmcv.
7 https://github.com/open-mmlab/mmdetection.
8 https://github.com/open-mmlab/mmdetection3d.
9 https://github.com/open-mmlab/mmengine.
10 https://github.com/open-mmlab/mmpretrain.
11 https://github.com/PJLab-ADG/OpenPCSeg.4D Contrastive Superflows are Dense 3D Representation Learners 21
10.2 Public Datasets Used
We acknowledge the use of the following public datasets during this work:
â€“ nuScenes12 ............................................CC BY-NC-SA 4.0
â€“ nuScenes-devkit13 .....................................Apache License 2.0
â€“ SemanticKITTI14 .....................................CC BY-NC-SA 4.0
â€“ SemanticKITTI-API15 .......................................MIT License
â€“ WaymoOpenDataset16 ...........................Waymo Dataset License
â€“ Synth4D17 ...............................................GPL-3.0 License
â€“ ScribbleKITTI18 ................................................Unknown
â€“ RELLIS-3D19 .........................................CC BY-NC-SA 3.0
â€“ SemanticPOSS20 ......................................CC BY-NC-SA 3.0
â€“ SemanticSTF21 ........................................CC BY-NC-SA 4.0
â€“ SynthLiDAR22 ..............................................MIT License
â€“ DAPS-3D23 ..................................................MIT License
â€“ Robo3D24 .............................................CC BY-NC-SA 4.0
10.3 Public Implementations Used
We acknowledge the use of the following implementations during this work:
â€“ SLidR25 ...............................................Apache License 2.0
â€“ DINOv226 .............................................Apache License 2.0
â€“ Segment-Any-Point-Cloud27 ...........................CC BY-NC-SA 4.0
â€“ OpenSeeD28 ..........................................Apache License 2.0
â€“ torchsparse29 ................................................MIT License
12 https://www.nuscenes.org/nuscenes.
13 https://github.com/nutonomy/nuscenes-devkit.
14 http://semantic-kitti.org.
15 https://github.com/PRBonn/semantic-kitti-api.
16 https://waymo.com/open.
17 https://github.com/saltoricristiano/gipso-sfouda.
18 https://github.com/ouenal/scribblekitti.
19 https://github.com/unmannedlab/RELLIS-3D.
20 http://www.poss.pku.edu.cn/semanticposs.html.
21 https://github.com/xiaoaoran/SemanticSTF.
22 https://github.com/xiaoaoran/SynLiDAR.
23 https://github.com/subake/DAPS3D.
24 https://github.com/ldkong1205/Robo3D.
25 https://github.com/valeoai/SLidR.
26 https://github.com/facebookresearch/dinov2.
27 https://github.com/youquanl/Segment-Any-Point-Cloud.
28 https://github.com/IDEA-Research/OpenSeeD.
29 https://github.com/mit-han-lab/torchsparse.22 X. Xu et al.
Table 10: The per-class IoU scores of state-of-the-art pretraining methods pre-
trained and linear-probed on the nuScenes [27] dataset. All IoU scores are given in
percentage (%). The best IoU scores in each configuration are shaded with colors.
Method
Random 8.1 0.5 0.0 0.0 3.9 0.0 0.0 0.0 6.4 0.0 3.9 59.6 0.0 0.1 16.230.612.0
â€¢Distill:None
PointContrast[103]21.9 - - - - - - - - - - - - - - - -
DepthContrast[115]22.1 - - - - - - - - - - - - - - - -
ALSO[8] - - - - - - - - - - - - - - - - -
BEVContrast[82] - - - - - - - - - - - - - - - - -
â€¢Distill:ResNet-50
PPKT[64]35.9 - - - - - - - - - - - - - - - -
SLidR[83]39.244.2 0.0 30.860.215.122.447.227.716.334.380.621.835.248.171.071.9
ST-SLidR[67]40.5 - - - - - - - - - - - - - - - -
TriCC[75]38.0 - - - - - - - - - - - - - - - -
Seal[62]45.054.7 5.9 30.661.718.928.848.131.022.139.583.835.446.756.974.774.7
HVDistill[112]39.5 - - - - - - - - - - - - - - - -
â€¢Distill:ViT-S
PPKT[64]38.643.8 0.0 31.253.115.2 0.0 42.216.518.333.779.137.245.252.775.674.3
SLidR[83]44.745.0 8.2 34.858.623.440.243.819.022.940.982.738.347.653.977.877.9
Seal[62]45.248.9 8.4 30.768.117.537.757.717.920.940.483.836.644.254.576.279.3
SuperFlow46.449.8 6.8 45.963.418.531.060.328.125.447.486.238.447.456.774.977.8
â€¢Distill:ViT-B
PPKT[64]40.029.6 0.0 30.755.8 6.3 22.456.718.124.342.782.333.245.153.471.375.7
SLidR[83]45.446.7 7.8 46.558.723.934.047.817.123.741.783.439.447.054.676.677.8
Seal[62]46.649.3 8.2 35.170.822.141.757.415.221.642.684.538.146.855.477.279.5
SuperFlow47.745.812.452.667.917.240.859.525.421.047.685.837.248.456.676.278.2
â€¢Distill:ViT-L
PPKT[64]41.630.5 0.0 32.057.3 8.7 24.058.119.524.944.183.134.545.955.472.576.4
SLidR[83]45.746.9 6.9 44.960.822.740.644.717.423.040.483.639.947.855.278.178.3
Seal[62]46.853.1 6.9 35.065.022.046.159.216.223.041.884.735.846.655.578.479.8
SuperFlow48.054.114.947.665.923.446.556.927.520.744.484.839.247.458.076.079.2
UoIm
reirrabâ–  elcycibâ– 
subâ–  racâ– 
elcihevnoitcurtsnocâ– 
elcycrotomâ–  nairtsedepâ–  enoccffiartâ– 
reliartâ– 
kcurtâ– 
ecafruselbaevirdâ– 
taflrehtoâ–  klawedisâ– 
niarretâ– 
edamnamâ–  noitategevâ– 4D Contrastive Superflows are Dense 3D Representation Learners 23
Table 11: The per-class IoU scores of state-of-the-art pretraining methods pre-
trainedandfine-tunedonnuScenes [27]with1%annotations.AllIoUscoresaregiven
in percentage (%). The best IoU scores in each configuration are shaded with colors.
Method
Random30.3 0.0 0.0 8.1 65.0 0.1 6.6 21.0 9.0 9.3 25.889.514.841.748.772.473.3
â€¢Distill:None
PointContrast[103]32.5 0.0 1.0 5.6 67.4 0.0 3.3 31.6 5.6 12.130.891.721.948.450.875.074.6
DepthContrast[115]31.7 0.0 0.6 6.5 64.7 0.2 5.1 29.0 9.5 12.129.990.317.844.449.573.574.0
ALSO[8]37.7 - - - - - - - - - - - - - - - -
BEVContrast[82]37.9 0.0 1.332.674.3 1.1 0.9 41.3 8.1 24.140.989.836.244.052.179.979.7
â€¢Distill:ResNet-50
PPKT[64]37.8 0.0 2.220.775.4 1.2 13.245.6 8.5 17.538.492.519.252.356.880.180.9
SLidR[83]38.8 0.0 1.815.473.1 1.9 19.947.217.114.534.592.027.153.661.079.882.3
ST-SLidR[67]40.8 - - - - - - - - - - - - - - - -
TriCC[75]41.2 - - - - - - - - - - - - - - - -
Seal[62]45.8 0.0 9.432.677.510.428.053.025.030.949.794.033.760.159.683.983.4
HVDistill[112]42.7 - - - - - - - - - - - - - - - -
â€¢Distill:ViT-S
PPKT[64]40.6 0.0 0.025.273.5 9.1 6.9 51.4 8.6 11.331.193.241.758.364.082.082.6
SLidR[83]41.2 0.0 0.026.672.012.415.851.422.911.735.392.936.358.763.681.282.3
Seal[62]44.320.00.019.474.710.645.760.329.217.438.193.226.058.864.581.981.9
SuperFlow47.838.21.825.879.015.343.660.3 0.0 28.455.493.728.859.159.983.583.1
â€¢Distill:ViT-B
PPKT[64]40.9 0.0 0.024.573.512.2 7.0 51.013.515.436.393.140.459.263.581.782.2
SLidR[83]41.6 0.0 0.026.773.410.316.951.323.312.738.193.037.758.863.481.682.7
Seal[62]46.043.00.026.781.3 9.9 41.356.2 0.0 21.751.693.642.362.864.782.682.7
SuperFlow48.139.10.930.080.710.347.159.5 5.1 27.655.493.729.161.163.582.783.6
â€¢Distill:ViT-L
PPKT[64]42.1 0.0 0.024.478.815.1 9.2 54.214.312.939.192.937.859.864.982.383.6
SLidR[83]42.8 0.0 0.023.978.815.220.955.028.017.441.492.241.258.064.081.882.7
Seal[62]46.341.80.023.881.417.746.358.6 0.0 23.454.793.841.462.565.083.883.8
SuperFlow50.044.50.922.480.817.150.260.921.025.155.193.935.861.562.683.783.7
UoIm
reirrabâ–  elcycibâ– 
subâ–  racâ– 
elcihevnoitcurtsnocâ– 
elcycrotomâ–  nairtsedepâ–  enoccffiartâ– 
reliartâ– 
kcurtâ– 
ecafruselbaevirdâ– 
taflrehtoâ– 
klawedisâ– 
niarretâ– 
edamnamâ–  noitategevâ– 24 X. Xu et al.
Fig.7: Qualitative assessments of state-of-the-art pretraining methods pretrained
onnuScenes [27]andfine-tunedonnuScenes [27]with1%annotations.Theerrormaps
show the correct and incorrect predictions in gray and red, respectively. Best viewed
in colors and zoomed-in for details.
hturT
dnuorG
modnaR
RdiLS
laeS
wolFrepuS4D Contrastive Superflows are Dense 3D Representation Learners 25
Fig.8: Qualitative assessments of state-of-the-art pretraining methods pretrained
on nuScenes [27] and fine-tuned on SemanticKITTI [5] with 1% annotations. The
error maps show the correct and incorrect predictions in gray and red, respectively.
Best viewed in colors and zoomed-in for details.
hturT
dnuorG
modnaR
RdiLS
laeS
wolFrepuS26 X. Xu et al.
Fig.9: Qualitative assessments of state-of-the-art pretraining methods pretrained
onnuScenes [27]andfine-tunedonWaymo Open [90]with1%annotations.Theerror
maps show the correct and incorrect predictions in gray and red, respectively. Best
viewed in colors and zoomed-in for details.
hturT
dnuorG
modnaR
RdiLS
laeS
wolFrepuS4D Contrastive Superflows are Dense 3D Representation Learners 27
(a) â€œcarâ€ (3D) (b) â€œcarâ€ (2D)
(c) â€œflat-otherâ€ (3D) (d) â€œflat-otherâ€ (2D)
(e) â€œterrainâ€ (3D) (f) â€œterrainâ€ (2D)
(g) â€œsidewalkâ€ (3D) (h) â€œsidewalkâ€ (2D)
(i) â€œdriveable-surfaceâ€ (3D) (j) â€œdriveable-surfaceâ€ (2D)
Fig.10: Cosine similarity between the features of a query point (denoted as a red
dot)andthefeaturesofotherpointsprojectedintheimage(theleftcolumn),andthe
features of an image with the same scene (the right column). The color goes from red
to blue denoting low and high similarity scores, respectively.28 X. Xu et al.
(a) â€œvegetationâ€ (3D) (b) â€œvegetationâ€ (2D)
(c) â€œconstruction-vehicleâ€ (3D) (d) â€œconstruction-vehicleâ€ (2D)
(e) â€œmotorcycleâ€ (3D) (f) â€œmotorcycleâ€ (2D)
(g) â€œmanmadeâ€ (3D) (h) â€œmanmadeâ€ (2D)
(i) â€œpedestrianâ€ (3D) (j) â€œpedestrianâ€ (2D)
Fig.11: Cosine similarity between the features of a query point (denoted as a red
dot)andthefeaturesofotherpointsprojectedintheimage(theleftcolumn),andthe
features of an image with the same scene (the right column). The color goes from red
to blue denoting low and high similarity scores, respectively.4D Contrastive Superflows are Dense 3D Representation Learners 29
References
1. Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., SÃ¼sstrunk, S.: Slic super-
pixels compared to state-of-the-art superpixel methods. IEEE Transactions on
Pattern Analysis and Machine Intelligence 34(11), 2274â€“2282 (2012) 5, 6
2. Aygun, M., Osep, A., Weber, M., Maximov, M., Stachniss, C., Behley, J., Leal-
TaixÃ©, L.: 4d panoptic lidar segmentation. In: IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 5527â€“5537 (2021) 2, 4
3. Badue,C.,Guidolini,R.,Carneiro,R.V.,Azevedo,P.,Cardoso,V.B.,Forechi,A.,
Jesus, L., Berriel, R., PaixÃ£o, T.M., Mutz, F., de Paula Veronese, L., Oliveira-
Santos, T., Souza, A.F.D.: Self-driving cars: A survey. Expert Systems with Ap-
plications 165, 113816 (2021) 1
4. Behley,J.,Garbade,M.,Milioto,A.,Quenzel,J.,Behnke,S.,Gall,J.,Stachniss,
C.: Towards 3d lidar-based semantic scene understanding of 3d point cloud se-
quences: The semantickitti dataset. International Journal of Robotics Research
40, 959â€“96 (2021) 2, 4
5. Behley,J.,Garbade,M.,Milioto,A.,Quenzel,J.,Behnke,S.,Stachniss,C.,Gall,
J.: Semantickitti: A dataset for semantic scene understanding of lidar sequences.
In: IEEE/CVF International Conference on Computer Vision. pp. 9297â€“9307
(2019) 9, 10, 11, 12, 16, 25
6. Bengio,Y.,Courville,A.,Vincent,P.:Representationlearning:Areviewandnew
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence
35(8), 1798â€“1828 (2013) 2
7. Berman, M., Triki, A.R., Blaschko, M.B.: The lovÃ¡sz-softmax loss: A tractable
surrogate for the optimization of the intersection-over-union measure in neural
networks. In: IEEE Conference on Computer Vision and Pattern Recognition.
pp. 4413â€“4421 (2018) 18
8. Boulch, A., Sautier, C., Michele, B., Puy, G., Marlet, R.: Also: Automotive lidar
self-supervision by occupancy estimation. In: IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. pp. 13455â€“13465 (2023) 4, 10, 22, 23
9. Caesar,H.,Bankiti,V.,Lang,A.H.,Vora,S.,Liong,V.E.,Xu,Q.,Krishnan,A.,
Pan,Y.,Baldan,G.,Beijbom,O.:nuscenes:Amultimodaldatasetforautonomous
driving.In:IEEE/CVFConferenceonComputerVisionandPatternRecognition.
pp. 11621â€“11631 (2020) 2
10. Cao, A.Q., Dai, A.,de Charette, R.:Pasco: Urban3d panoptic scenecompletion
withuncertaintyawareness.In:IEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 14554â€“14564 (2024) 1
11. Chen, Q., Vora, S., Beijbom, O.: Polarstream: Streaming lidar object detection
andsegmentationwithpolarpillars.In:AdvancesinNeuralInformationProcess-
ing Systems. vol. 34 (2021) 4
12. Chen, R., Liu, Y., Kong, L., Chen, N., Zhu, X., Ma, Y., Liu, T., Wang, W.:
Towardslabel-freesceneunderstandingbyvisionfoundationmodels.In:Advances
in Neural Information Processing Systems. vol. 36 (2023) 2, 4
13. Chen,R.,Liu,Y.,Kong,L.,Zhu,X.,Ma,Y.,Li,Y.,Hou,Y.,Qiao,Y.,Wang,W.:
Clip2scene:Towardslabel-efficient3dsceneunderstandingbyclip.In:IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 7020â€“7030 (2023)
2, 4
14. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for con-
trastive learning of visual representations. In: International Conference on Ma-
chine Learning. pp. 1597â€“1607 (2020) 430 X. Xu et al.
15. Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297 (2020) 2
16. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision
transformers. In: IEEE/CVF International Conference on Computer Vision. pp.
9640â€“9649 (2021) 4
17. Chen, Y., NieÃŸner, M., Dai, A.: 4dcontrast: Contrastive learning with dynamic
correspondences for 3d scene understanding. In: European Conference on Com-
puter Vision. pp. 543â€“560 (2022) 4
18. Cheng, H., Han, X., Xiao, G.: Cenet: Toward concise and efficient lidar seman-
tic segmentation for autonomous driving. In: IEEE International Conference on
Multimedia and Expo. pp. 1â€“6 (2022) 4
19. Cheng, R., Razani, R., Taghavi, E., Li, E., Liu, B.: Af2-s3net: Attentive feature
fusion with adaptive feature selection for sparse semantic segmentation network.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
12547â€“12556 (2021) 4
20. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski con-
volutional neural networks. In: IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 3075â€“3084 (2019) 4, 9, 10, 12, 13, 17
21. Contributors, M.: MMDetection3D: OpenMMLab next-generation platform for
general 3D object detection. https://github.com/open-mmlab/mmdetection3d
(2020) 9
22. Cortinhal, T., Tzelepis, G., Aksoy, E.E.: Salsanext: Fast, uncertainty-aware se-
manticsegmentationoflidarpointclouds.In:InternationalSymposiumonVisual
Computing. pp. 207â€“222 (2020) 4
23. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
International Conference on Learning Representations (2020) 9
24. Duerr,F.,Pfaller,M.,Weigel,H.,Beyerer,J.:Lidar-basedrecurrent3dsemantic
segmentation with temporal memory alignment. In: International Conference on
3D Vision. pp. 781â€“790 (2020) 4
25. Ester,M.,Kriegel,H.P.,Sander,J.,Xu,X.:Adensity-basedalgorithmfordiscov-
eringclustersinlargespatialdatabaseswithnoise.In:ACMSIGKDDConference
on Knowledge Discovery and Data Mining. pp. 226â€“231 (1996) 5
26. Fischler, M.A., Bolles, R.C.: Random sample consensus: A paradigm for model
fitting with applications to image analysis and automated cartography. Commu-
nications of the ACM 24(6), 381â€“395 (1981) 5
27. Fong,W.K.,Mohan,R.,Hurtado,J.V.,Zhou,L.,Caesar,H.,Beijbom,O.,Valada,
A.: Panoptic nuscenes: A large-scale benchmark for lidar panoptic segmentation
andtracking.IEEERoboticsandAutomationLetters7,3795â€“3802(2022) 9,10,
11, 12, 15, 16, 22, 23, 24, 25, 26
28. Gao, B., Pan,Y., Li, C., Geng, S., Zhao,H.: Are wehungry for3d lidar data for
semanticsegmentation?asurveyofdatasetsandmethods.IEEETransactionson
Intelligent Transportation Systems 23(7), 6063â€“6081 (2021) 1, 4
29. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the
kittivisionbenchmarksuite.In:IEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 3354â€“3361 (2012) 2
30. Hao, X., Wei, M., Yang, Y., Zhao, H., Zhang, H., Zhou, Y., Wang, Q., Li, W.,
Kong,L.,Zhang,J.:Isyourhdmapconstructorreliableundersensorcorruptions?
arXiv preprint arXiv:2406.12214 (2024) 114D Contrastive Superflows are Dense 3D Representation Learners 31
31. He, K., Chen, X., Xie, S., Li, Y., DollÃ¡r, P., Girshick, R.: Masked autoencoders
are scalable vision learners. In: IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 16000â€“16009 (2022) 4
32. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsu-
pervisedvisualrepresentationlearning.In:IEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 9729â€“9738 (2020) 4
33. Hess, G., Jaxing, J., Svensson, E., Hagerman, D., Petersson, C., Svensson, L.:
Masked autoencoders for self-supervised learning on automotive point clouds.
arXiv preprint arXiv:2207.00531 (2022) 4
34. Hong, F., Kong, L., Zhou, H., Zhu, X., Li, H., Liu, Z.: Unified 3d and 4d panop-
tic segmentation via dynamic shifting networks. IEEE Transactions on Pattern
Analysis and Machine Intelligence 46(5), 3480â€“3495 (2024) 4
35. Hong, F., Zhou, H., Zhu, X., Li, H., Liu, Z.: Lidar-based panoptic segmentation
via dynamic shifting network. In: IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 13090â€“13099 (2021) 4
36. Hou,J.,Graham,B.,NieÃŸner,M.,Xie,S.:Exploringdata-efficient3dsceneunder-
standingwithcontrastivescenecontexts.In:IEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 15587â€“15597 (2021) 4
37. Hu, Q., Yang, B., Fang, G., Guo, Y., Leonardis, A., Trigoni, N., Markham, A.:
Sqn:Weakly-supervisedsemanticsegmentationoflarge-scale3dpointclouds.In:
European Conference on Computer Vision. pp. 600â€“619 (2022) 4
38. Hu, Q., Yang, B., Khalid, S., Xiao, W., Trigoni, N., Markham, A.: Towards se-
mantic segmentation of urban-scale 3d point clouds: A dataset, benchmarks and
challenges. In: IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. pp. 4977â€“4987 (2021) 4
39. Hu, Z., Bai, X., Zhang, R., Wang, X., Sun, G., Fu, H., Tai, C.L.: Lidal: Inter-
frame uncertainty based active learning for 3d lidar semantic segmentation. In:
European Conference on Computer Vision. pp. 248â€“265 (2022) 4
40. Huang,S.,Xie,Y.,Zhu,S.C.,Zhu,Y.:Spatio-temporalself-supervisedrepresen-
tation learning for 3d point clouds. In: IEEE/CVF International Conference on
Computer Vision. pp. 6535â€“6545 (2021) 4
41. Jaritz,M.,Vu,T.H.,deCharette,R.,Wirbel,E.,PÃ©rez,P.:xmuda:Cross-modal
unsupervised domain adaptation for 3d semantic segmentation. In: IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.12605â€“12614(2020)
4
42. Jiang, P., Osteen, P., Wigness, M., Saripallig, S.: Rellis-3d dataset: Data, bench-
marksandanalysis.In:IEEEInternationalConferenceonRoboticsandAutoma-
tion. pp. 1110â€“1116 (2021) 9, 16
43. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
T., Whitehead, S., Berg, A.C., Lo, W.Y., DollÃ¡r, P., Girshick, R.: Segment any-
thing. In: IEEE/CVF International Conference on Computer Vision. pp. 4015â€“
4026 (2023) 4, 5, 6
44. Klokov,A.,Pak,D.U.,Khorin,A.,Yudin,D.,Kochiev,L.,Luchinskiy,V.,Bezug-
lyj,V.:Daps3d:Domainadaptiveprojectivesegmentationof3dlidarpointclouds.
IEEE Access 11, 79341â€“79356 (2023) 9, 16, 17
45. Kong, L., Liu, Y., Chen, R., Ma, Y., Zhu, X., Li, Y., Hou, Y., Qiao, Y., Liu,
Z.: Rethinking range view representation for lidar segmentation. In: IEEE/CVF
International Conference on Computer Vision. pp. 228â€“240 (2023) 4
46. Kong, L., Liu, Y., Li, X., Chen, R., Zhang, W., Ren, J., Pan, L., Chen, K.,
Liu, Z.: Robo3d: Towards robust and reliable 3d perception against corruptions.32 X. Xu et al.
In: IEEE/CVF International Conference on Computer Vision. pp. 19994â€“20006
(2023) 2, 9, 11, 16, 17, 18
47. Kong, L., Quader, N., Liong, V.E.: Conda: Unsupervised domain adaptation for
lidarsegmentationviaregularizeddomainconcatenation.In:IEEEInternational
Conference on Robotics and Automation. pp. 9338â€“9345 (2023) 4
48. Kong, L., Ren, J., Pan, L., Liu, Z.: Lasermix for semi-supervised lidar seman-
tic segmentation. In: IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 21705â€“21715 (2023) 2, 4
49. Kong, L., Xie, S., Hu, H., Ng, L.X., Cottereau, B.R., Ooi, W.T.: Robodepth:
Robust out-of-distribution depth estimation under corruptions. In: Advances in
Neural Information Processing Systems. vol. 36 (2023) 11
50. Kong, L., Xu, X., Ren, J., Zhang, W., Pan, L., Chen, K., Ooi, W.T., Liu, Z.:
Multi-modaldata-efficient3dsceneunderstandingforautonomousdriving.arXiv
preprint arXiv:2405.05258 (2024) 4
51. Krispel, G., Schinagl, D., Fruhwirth-Reisinger, C., Possegger, H., Bischof, H.:
Maeli:Maskedautoencoderforlarge-scalelidarpointclouds.In:IEEE/CVFWin-
ter Conference on Applications of Computer Vision. pp. 3383â€“3392 (2024) 4
52. Le-Khac, P.H., Healy, G., Smeaton, A.F.: Contrastive representation learning:
A framework and review. IEEE Transactions on Pattern Analysis and Machine
Intelligence 8, 193907â€“193934 (2020) 2
53. Li, L., Shum, H.P., Breckon, T.P.: Less is more: Reducing task and model com-
plexity for 3d point cloud semantic segmentation. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 9361â€“9371 (2023) 4
54. Li, R., de Charette, R., Cao, A.Q.: Coarse3d: Class-prototypes for contrastive
learning in weakly-supervised 3d point cloud segmentation. In: British Machine
Vision Conference (2022) 4
55. Li, Y., Kong, L., Hu, H., Xu, X., Huang, X.: Optimizing lidar placements for
robustdrivingperceptioninadverseconditions.arXivpreprintarXiv:2403.17009
(2024) 11
56. Lim, H., Oh, M., Myung, H.: Patchwork: Concentric zone-based region-wise
ground segmentation with ground likelihood estimation using a 3d lidar sensor.
IEEE Robotics and Automation Letters 6(4), 6458â€“6465 (2021) 5
57. Liong, V.E., Nguyen, T.N.T., Widjaja, S., Sharma, D., Chong, Z.J.: Amvnet:
Assertion-basedmulti-viewfusionnetworkforlidarsemanticsegmentation.arXiv
preprint arXiv:2012.04934 (2020) 4
58. Liu,M.,Zhou,Y.,Qi,C.R.,Gong,B.,Su,H.,Anguelov,D.:Less:Label-efficient
semantic segmentation for lidar point clouds. In: European Conference on Com-
puter Vision. pp. 70â€“89 (2022) 4
59. Liu,M.,Yurtsever,E.,Zhou,X.,Fossaert,J.,Cui,Y.,Zagar,B.L.,Knoll.,A.C.:A
survey on autonomous driving datasets: Data statistic, annotation, and outlook.
arXiv preprint arXiv:2401.01454 (2024) 1
60. Liu, Y., Bai, Y., Kong, L., Chen, R., Hou, Y., Shi, B., Li, Y.: Pcseg: An open
source point cloud segmentation codebase. https://github.com/PJLab-ADG/
PCSeg (2023) 9
61. Liu, Y., Chen, R., Li, X., Kong, L., Yang, Y., Xia, Z., Bai, Y., Zhu, X., Ma,
Y., Li, Y., Qiao, Y., Hou, Y.: Uniseg: A unified multi-modal lidar segmentation
network and the openpcseg codebase. In: IEEE/CVF International Conference
on Computer Vision. pp. 21662â€“21673 (2023) 4
62. Liu, Y., Kong, L., Cen, J., Chen, R., Zhang, W., Pan, L., Chen, K., Liu, Z.:
Segment any point cloud sequences by distilling vision foundation models. In:4D Contrastive Superflows are Dense 3D Representation Learners 33
Advances in Neural Information Processing Systems. vol. 36 (2023) 2, 3, 4, 5, 6,
8, 9, 10, 11, 12, 18, 19, 22, 23
63. Liu,Y.,Kong,L.,Wu,X.,Chen,R.,Li,X.,Pan,L.,Liu,Z.,Ma,Y.:Multi-space
alignments towards universal lidar segmentation. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 14648â€“14661 (2024) 4
64. Liu, Y.C., Huang, Y.K., Chiang, H.Y., Su, H.T., Liu, Z.Y., Chen, C.T., Tseng,
C.Y.,Hsu,W.H.:Learningfrom2d:Contrastivepixel-to-pointknowledgetransfer
for3dpretraining.arXivpreprintarXiv:2104.04687(2021) 2,4,10,11,19,22,23
65. Liu, Y., Chen, J., Zhang, Z., Huang, J., Yi, L.: Leaf: Learning frames for 4d
pointcloudsequenceunderstanding.In:IEEE/CVFInternationalConferenceon
Computer Vision. pp. 604â€“613 (2023) 4
66. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:International
Conference on Learning Representations (2018) 9, 18
67. Mahmoud, A., Hu, J.S., Kuai, T., Harakeh, A., Paull, L., Waslander, S.L.: Self-
supervised image-to-point distillation via semantically tolerant contrastive loss.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
7102â€“7110 (2023) 2, 3, 4, 6, 8, 10, 22, 23
68. Michele, B., Boulch, A., Puy, G., Vu, T.H., Marlet, R., Courty, N.: Saluda:
Surface-based automotive lidar unsupervised domain adaptation. arXiv preprint
arXiv:2304.03251 (2023) 4
69. Milioto,A.,Vizzo,I.,Behley,J.,Stachniss,C.:Rangenet++:Fastandaccurateli-
darsemanticsegmentation.In:IEEE/RSJInternationalConferenceonIntelligent
Robots and Systems. pp. 4213â€“4220 (2019) 4
70. Muhammad, K., Ullah, A., Lloret, J., Ser, J.D., de Albuquerque, V.H.C.: Deep
learning for safe autonomous driving: Current challenges and future direc-
tions.IEEETransactionsonIntelligentTransportationSystems22(7),4316â€“4336
(2020) 2
71. Nunes,L.,Marcuzzi,R.,Chen,X.,Behley,J.,Stachniss,C.:Segcontrast:3dpoint
cloudfeaturerepresentationlearningthroughself-supervisedsegmentdiscrimina-
tion. IEEE Robotics and Automation Letters 7(2), 2116â€“2123 (2022) 4
72. Nunes,L.,Wiesmann,L.,Marcuzzi,R.,Chen,X.,Behley,J.,Stachniss,C.:Tem-
poral consistent 3d lidar representation learning for semantic perception in au-
tonomous driving. In: IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5217â€“5228 (2023) 2, 4
73. Oquab,M.,Darcet,T.,Moutakanni,T.,Vo,H.,Szafraniec,M.,Khalidov,V.,Fer-
nandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba,
W., Howes, R., Huang, P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Syn-
naeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski,
P.: Dinov2: Learning robust visual features without supervision. arXiv preprint
arXiv:2304.07193 (2023) 2, 9, 12, 13, 18
74. Pan,Y.,Gao,B.,Mei,J.,Geng,S.,Li,C.,Zhao,H.:Semanticposs:Apointcloud
dataset with large quantity of dynamic instances. In: IEEE Intelligent Vehicles
Symposium. pp. 687â€“693 (2020) 9, 16
75. Pang,B.,Xia,H.,Lu,C.:Unsupervised3dpointcloudrepresentationlearningby
triangleconstrainedcontrastforautonomousdriving.In:IEEE/CVFConference
onComputerVisionandPatternRecognition.pp.5229â€“5239(2023) 4,10,22,23
76. Puy, G., Gidaris, S., Boulch, A., SimÃ©oni, O., Sautier, C., PÃ©rez, P., Bursuc, A.,
Marlet, R.: Revisiting the distillation of image representations into point clouds
for autonomous driving. arXiv preprint arXiv:2310.17504 (2023) 434 X. Xu et al.
77. Puy, G., Gidaris, S., Boulch, A., SimÃ©oni, O., Sautier, C., PÃ©rez, P., Bursuc, A.,
Marlet,R.:Threepillarsimprovingvisionfoundationmodeldistillationforlidar.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
21519â€“21529 (2024) 2, 4
78. Qiu, H., Yu, B., Tao, D.: Gfnet: Geometric flow network for 3d point cloud se-
mantic segmentation. Transactions on Machine Learning Research (2022) 4
79. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,etal.:Learningtransferablevisualmodelsfrom
natural language supervision. In: International conference on machine learning.
pp. 8748â€“8763. PMLR (2021) 6
80. Rizzoli, G., Barbato, F., Zanuttigh, P.: Multimodal semantic segmentation in
autonomous driving: A review of current approaches and future perspectives.
Technologies 10(4) (2022) 1
81. Saltori, C., Krivosheev, E., LathuiliÃ©re, S., Sebe, N., Galasso, F., Fiameni, G.,
Ricci, E., Poiesi, F.: Gipso: Geometrically informed propagation for online adap-
tation in 3d lidar segmentation. In: European Conference on Computer Vision.
pp. 567â€“585 (2022) 9, 16, 17
82. Sautier, C., Puy, G., Boulch, A., Marlet, R., Lepetit, V.: Bevcontrast: Self-
supervision in bev space for automotive lidar point clouds. arXiv preprint
arXiv:2310.17281 (2023) 4, 5, 10, 22, 23
83. Sautier, C., Puy, G., Gidaris, S., Boulch, A., Bursuc, A., Marlet, R.: Image-
to-lidar self-supervised distillation for autonomous driving data. In: IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 9891â€“9901 (2022)
2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 19, 22, 23
84. Shen, Z., Sheng, X., Fan, H., Wang, L., Guo, Y., Liu, Q., Wen, H., Zhou, X.:
Maskedspatio-temporalstructurepredictionforself-supervisedlearningonpoint
cloud videos. In: IEEE/CVF International Conference on Computer Vision. pp.
16580â€“16589 (2023) 4
85. Sheng, X., Shen, Z., Xiao, G., Wang, L., Guo, Y., Fan, H.: Point contrastive
prediction with semantic clustering for self-supervised learning on point cloud
videos.In:IEEE/CVFInternationalConferenceonComputerVision.pp.16515â€“
16524 (2023) 4
86. Shi, H., Lin, G., Wang, H., Hung, T.Y., Wang, Z.: Spsequencenet: Semantic seg-
mentation network on 4d point clouds. In: IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 4574â€“4583 (2020) 4
87. Shi, H., Wei, J., Li, R., Liu, F., Lin, G.: Weakly supervised segmentation on
outdoor 4d point clouds with temporal matching and spatial graph propagation.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
11840â€“11849 (2022) 4
88. Smith,L.N.,Topin,N.:Super-convergence:Veryfasttrainingofneuralnetworks
using large learning rates. arXiv preprint arXiv:1708.07120 (2017) 9, 18
89. Sun,J.,Xu,X.,Kong,L.,Liu,Y.,Li,L.,Zhu,C.,Zhang,J.,Xiao,Z.,Chen,R.,
Wang, T., Zhang, W., Chen, K., Qing, C.: An empirical study of training state-
of-the-art lidar segmentation models. arXiv preprint arXiv:2405.14870 (2024) 4
90. Sun,P.,Kretzschmar,H.,Dotiwalla,X.,Chouard,A.,Patnaik,V.,Tsui,P.,Guo,
J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,
Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens,
J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving:
Waymoopendataset.In:IEEE/CVFConferenceonComputerVisionandPattern
Recognition. pp. 2446â€“2454 (2020) 9, 10, 11, 12, 16, 264D Contrastive Superflows are Dense 3D Representation Learners 35
91. Tang, H., Liu, Z., Li, X., Lin, Y., Han, S.: Torchsparse: Efficient point cloud
inferenceengine.ProceedingsofMachineLearningandSystems4,302â€“315(2022)
17
92. Tang,H.,Liu,Z.,Zhao,S.,Lin,Y.,Lin,J.,Wang,H.,Han,S.:Searchingefficient
3darchitectureswithsparsepoint-voxelconvolution.In:EuropeanConferenceon
Computer Vision. pp. 685â€“702 (2020) 4
93. Tarvainen,A.,Valpola,H.:Meanteachersarebetterrolemodels:Weight-averaged
consistency targets improve semi-supervised deep learning results. In: Advances
in Neural Information Processing Systems. vol. 30 (2017) 4
94. Triess, L.T., Dreissig, M., Rist, C.B., ZÃ¶llner, J.M.: A survey on deep domain
adaptation for lidar perception. In: IEEE Intelligent Vehicles Symposium Work-
shops. pp. 350â€“357 (2021) 4
95. Uecker, M., Fleck, T., Pflugfelder, M., ZÃ¶llner, J.M.: Analyzing deep learning
representations of point clouds for real-time in-vehicle lidar perception. arXiv
preprint arXiv:2210.14612 (2022) 4
96. Unal, O., Dai, D., Gool, L.V.: Scribble-supervised lidar semantic segmentation.
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
2697â€“2707 (2022) 2, 4, 9, 16
97. Wei, W., Nejadasl, F.K., Gevers, T., Oswald, M.R.: T-mae: Temporal
masked autoencoders for point cloud representation learning. arXiv preprint
arXiv:2312.10217 (2023) 4
98. Wu, Y., Zhang, T., Ke, W., SÃ¼sstrunk, S., Salzmann, M.: Spatiotemporal self-
supervised learning for point clouds in the wild. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 5251â€“5260 (2023) 2, 5
99. Xiao,A.,Huang,J.,Guan,D.,Zhan,F.,Lu,S.:Transferlearningfromsynthetic
to real lidar point cloud for semantic segmentation. In: AAAI Conference on
Artificial Intelligence. pp. 2795â€“2803 (2022) 9, 16, 17
100. Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao, L.: Unsupervised point
cloud representation learning with deep neural networks: A survey. IEEE Trans-
actionsonPatternAnalysisandMachineIntelligence45(9),11321â€“11339(2023)
2
101. Xiao,A.,Huang,J.,Xuan,W.,Ren,R.,Liu,K.,Guan,D.,Saddik,A.E.,Lu,S.,
Xing, E.: 3d semantic segmentation in the wild: Learning generalized models for
adverse-condition point clouds. In: IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 9382â€“9392 (2023) 9, 16, 17
102. Xie,B.,Li,S.,Guo,Q.,Liu,C.H.,Cheng,X.:Annotator:Agenericactivelearn-
ingbaselineforlidarsemanticsegmentation.In:AdvancesinNeuralInformation
Processing Systems. vol. 36 (2023) 4
103. Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas, L., Litany, O.: Pointcontrast: Unsu-
pervisedpre-trainingfor3dpointcloudunderstanding.In:EuropeanConference
on Computer Vision. pp. 574â€“591 (2020) 4, 10, 22, 23
104. Xie, S., Kong, L., Zhang, W., Ren, J., Pan, L., Chen, K., Liu, Z.: Benchmarking
andimprovingbirdâ€™seyeviewperceptionrobustnessinautonomousdriving.arXiv
preprint arXiv:2405.17426 (2024) 11
105. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., Hu, H.: Simmim:
A simple framework for masked image modeling. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 9653â€“9663 (2022) 4
106. Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K., Tomizuka, M.:
Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmenta-
tion. In: European Conference on Computer Vision. pp. 1â€“19 (2020) 436 X. Xu et al.
107. Xu, J., Zhang, R., Dou, J., Zhu, Y., Sun, J., Pu, S.: Rpvnet: A deep and
efficient range-point-voxel fusion network for lidar point cloud segmentation.
In: IEEE/CVF International Conference on Computer Vision. pp. 16024â€“16033
(2021) 4
108. Xu, W., Li, X., Ni, P., Guang, X., Luo, H., Zhao, X.: Multi-view fusion driven
3d point cloud semantic segmentation based on hierarchical transformer. IEEE
Sensors Journal 23(24), 31461â€“31470 (2023) 4
109. Xu, X., Kong, L., Shuai, H., Liu, Q.: Frnet: Frustum-range networks for scalable
lidar segmentation. arXiv preprint arXiv:2312.04484 (2023) 4
110. Yin,J.,Zhou,D.,Zhang,L.,Fang,J.,Xu,C.Z.,Shen,J.,Wang,W.:Proposalcon-
trast:Unsupervisedpre-trainingforlidar-based3dobjectdetection.In:European
Conference on Computer Vision. pp. 17â€“33 (2022) 2, 4
111. Zhang, H., Li, F., Zou, X., Liu, S., Li, C., Gao, J., Yang, J., Zhang, L.: A sim-
ple framework for open-vocabulary segmentation and detection. In: IEEE/CVF
International Conference on Computer Vision. pp. 1020â€“1031 (2023) 4, 6, 9
112. Zhang,S.,Deng,J.,Bai,L.,Li,H.,Ouyang,W.,Zhang,Y.:Hvdistill:Transferring
knowledgefromimagestopointcloudsviaunsupervisedhybrid-viewdistillation.
International Journal of Computer Vision pp. 1â€“15 (2024) 2, 4, 10, 22, 23
113. Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong, B., Foroosh, H.: Polarnet:
An improved grid representation for online lidar point clouds semantic segmen-
tation.In:IEEE/CVFConferenceonComputerVisionandPatternRecognition.
pp. 9601â€“9610 (2020) 2, 4
114. Zhang,Y.,Hou,J.,Yuan,Y.:Acomprehensivestudyoftherobustnessforlidar-
based 3d object detectors against adversarial attacks. International Journal of
Computer Vision pp. 1â€“33 (2023) 2
115. Zhang,Z.,Girdhar,R.,Joulin,A.,Misra,I.:Self-supervisedpretrainingof3dfea-
tures on any point-cloud. In: IEEE/CVF International Conference on Computer
Vision. pp. 10252â€“10263 (2021) 4, 10, 22, 23
116. Zhang, Z., Dong, Y., Liu, Y., Yi, L.: Complete-to-partial 4d distillation for self-
supervised point cloud sequence representation learning. In: IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 17661â€“17670 (2023) 4
117. Zhang, Z., Yang, B., Wang, B., Li, B.: Growsp: Unsupervised semantic segmen-
tation of 3d point clouds. In: IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 17619â€“17629 (2023) 4
118. Zhao, Y., Bai, L., Huang, X.: Fidnet: Lidar point cloud semantic segmentation
withfullyinterpolationdecoding.In:IEEE/RSJInternationalConferenceonIn-
telligent Robots and Systems. pp. 4453â€“4458 (2021) 4
119. Zhou, Z., Zhang, Y., Foroosh, H.: Panoptic-polarnet: Proposal-free lidar point
cloud panoptic segmentation. In: IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 13194â€“13203 (2021) 4
120. Zhu,X.,Zhou,H.,Wang,T.,Hong,F.,Ma,Y.,Li,W.,Li,H.,Lin,D.:Cylindrical
andasymmetrical3dconvolutionnetworksforlidarsegmentation.In:IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 9939â€“9948 (2021)
4
121. Zou, X., Dou, Z.Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X., Behl, H., Wang,
J., Yuan, L., Peng, N., Wang, L., Lee, Y.J., Gao, J.: Generalized decoding for
pixel, image, and language. In: IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 15116â€“15127 (2023) 4, 5, 6
122. Zou,X.,Yang,J.,Zhang,H.,Li,F.,Li,L.,Gao,J.,Lee,Y.J.:Segmenteverything
everywhere all at once. In: Advances in Neural Information Processing Systems.
vol. 36 (2023) 4, 5, 6