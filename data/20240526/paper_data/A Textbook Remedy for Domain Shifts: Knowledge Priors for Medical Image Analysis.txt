A Textbook Remedy for Domain Shifts:
Knowledge Priors for Medical Image Analysis
YueYang,MonaGandhi,YufeiWang,YifanWu,MichaelS.Yao,
ChrisCallison-Burch,JamesC.Gee,MarkYatskar
UniversityofPennsylvania
yueyang1996.github.io/knobo
Abstract
Whiledeepnetworkshaveachievedbroadsuccessinanalyzingnaturalimages,
when applied to medical scans, they often fail in unexcepted situations. We
investigatethischallengeandfocusonmodelsensitivitytodomainshifts,suchas
datasampledfromdifferenthospitalsordataconfoundedbydemographicvariables
such as sex, race, etc, in the context of chest X-rays and skin lesion images.
A key finding we show empirically is that existing visual backbones lack an
appropriatepriorfromthearchitectureforreliablegeneralizationinthesesettings.
Takinginspirationfrommedicaltraining,weproposegivingdeepnetworksaprior
groundedinexplicitmedicalknowledgecommunicatedinnaturallanguage. Tothis
end,weintroduceKnowledge-enhancedBottlenecks(KnoBo),aclassofconcept
bottleneckmodelsthatincorporatesknowledgepriorsthatconstrainittoreason
withclinicallyrelevantfactorsfoundinmedicaltextbooksorPubMed. KnoBouses
retrieval-augmentedlanguagemodelstodesignanappropriateconceptspacepaired
withanautomatictrainingprocedureforrecognizingtheconcept. Weevaluate
differentresourcesofknowledgeandrecognitionarchitecturesonabroadrange
of domain shifts across 20 datasets. In our comprehensive evaluation with two
imagingmodalities,KnoBooutperformsfine-tunedmodelsonconfoundeddatasets
by 32.4 % on average. Finally, evaluations reveal that PubMed is a promising
resourceformakingmedicalmodelslesssensitivetodomainshift,outperforming
otherresourcesonbothdiversityofinformationandfinalpredictionperformance.
1 Introduction
Robustness to domain shifts is a key property for models operating on medical images because
transferscenariosarisewidely. Deepnetworkshaveachievedbroadsuccessinanalyzingnatural
images (everyday human contexts), but when applied to medical scans, they often substantially
degradeunderdistributionshift[86,15]. Medicaldatasetsaresmall,andunidentifiedconfoundsin
themcombinedwithmodelmisspecificationcandramaticallydegradeperformance[46,21,24].Such
failureerodesconfidenceasmodelsdonotlearntherightinformationfromtrainingdata,hampering
adoptionbymedicalprofessionals. Westudysuchproblemsbyinvestigatingtheperformanceof
systemsinthepresenceofconfoundeddataandaddressamainshortcomingwediscover.
Model sensitivity to domain shift can be measured by introducing synthetic confounds into data
andevaluatingonsampleswheretheconfoundmisleadsthemodel. Forexample,inFigure1,we
introduceconfoundeddatasetsforchestX-rayandskinlesionimageswhere,duringtraining,positive
dataissampledfromonegroupandnegativefromanother. Thisassociationisreversedattesting
time,creatinganadversarialout-of-distribution(OOD)evaluation. In5suchconstructedconfounds
permodality,coveringscenariosofrace,sex,age,scanposition,andhospital,wefindmodelsunable
togeneralizewell,droppingover63%onaverageoveranin-distribution(ID)evaluation.
Preprint.Underreview.
4202
yaM
32
]VC.sc[
1v93841.5042:viXraMedical Images Confounded Datasets
ID OOD ID OOD
+ + + +
Chest X-ray _ _ _ _ ‚Üë41.8 ‚Üë22.9
Skin Lesion Race Hospital
Figure1: In-domain(ID),out-of-domain(OOD),andaverageofIDandOOD(Avg)performanceon
confoundedmedicalimagedatasets. OurinterpretableKnowledge-enhancedBottlenecks(KnoBo)
aremorerobusttodomainshifts(e.g.,race,hospital,etc)thanfine-tunedvisiontransformers[17].
Priorsareanimportantsignalallowingmodelstoadoptappropriatehypothesesinlowormisleading
dataregimes. Wehypothesizethatexistingvisualbackboneslackanappropriatepriorforrobust
generalization in medicine. Like previous work identifying that vision backbones have a deep
imagepriorevenwhenentirelyuntrained [72,78],wecomparethequalityofimagerepresentations
producedbyuntrainednetworksonnaturalversusmedicalimages. Giventheoutputfromafrozen
untrainedvisualbackbone,wetrainalinearclassifierforpredictingadiversityoflabels(seeFigure
2). Acrossarchitecture,theseuntrainedmodelsarehigherqualityfeaturizersofnaturalimagesthan
directlyusingpixelsasfeatures. Incontrast, acrossmultiplemedicalmodalities,thedeepimage
priorincurrentmajorvisualbackbonesisnomoreeffectivethanusingpixels(andoftenworse).
To address the lack of an effective deep image prior for medical images, we propose using an
inherentlyinterpretablemodeldesign. Wedrawinspirationfrommedicaleducation,wherestudents
firstlearnfromtextbooksandlaterinamorepracticalsettingduringtheresidencywithanattending
doctor. Ourmodelsmimicthispattern: first,documentsareusedtoidentifyimportantknowledge,
and then they learn by example from data. We employ concept bottleneck models (CBMs) [41]
andenrichthemwithinformationderivedfromresourcesbroadlyaccessibletomedicalstudents.
CBMsareaclassofinherentlyinterpretablemodelsthatfactormodeldecisionsintohuman-readable
conceptsthatarecombinedlinearly. Ourmethodsbuildonrecentapproachesforlanguagemodel
(LM)guidedbottleneckconstructionwhereLMsarepromptedfordiscriminativeattributes[90].
We introduce Knowledge-enhanced Bottlenecks (KnoBo) to incorporate knowledge priors that
encouragereasoningwithfactorsfoundinmedicaldocuments. KnoBoextendsCBMstomedical
imagingandemploysretrieval-augmentedgenerationintoconceptdesign. Forexample,weextract
conceptsfrommedicaltextbooksasnaturallanguagequestionslikeIsthereground-glassopacity?
tohelpthemodelclassifywhetheranX-rayispositiveforarespiratoryinfection. Asillustratedin
Figure3,KnoBofactorslearningintothreeparts: (1)aninterpretablebottleneckpredictor,(2)aprior
overthestructureofthebottleneck, and(3)aprioroverpredictorparameters. Thisfactorization
allowsustoguidethemodelwithapriorrootedinmedicaldocuments. Theapproachreliesonan
iterativeretrievalprocesswhereanLMsummarizesdocumentstoproposeconcepts,formingour
medicalimageprior(Sec 4.2). Giventheconcepts,apretrainingcorpusofreportsandimagesis
usedtoconstructaclassifierforaconcept(Sec 4.3). Finally,aCBMislearnedusingpredictions
fromclassifiersondatawhileregularizedbyapriorformedfromLMgenerations. (Sec 4.4).
We evaluate KnoBo on our benchmark of confounded tasks. Averaged over confounds, KnoBo
increasesOODperformanceby41.8%and22.9%onX-rayandskinlesiondatasets,respectively.
KnoBo‚ÄôssuccessinOODperformancecomesatlittlesacrificeinIDsettings,providingabettermodel
overallwhenaveragingthetwodatasettings. Wealsoexplore5differentsourcesofknowledgeand
revealthatPubMedoutperformsotherresourcesintermsofbothdiversityofinformationandfinal
predictionperformance. Overall,ourworkdemonstratesthatakeymissingingredientforrobustness
todistributionshiftinmedicalimagingmodelsisapriorrootedinknowledge.
Natural Images X-rays Skin Lesion Images
40 70 75 30.7 53.2 47.6 50.6 61.6 55.8 61.5
20.5 50 37.9
20 16.0 35 27.8
25
6.4
0 0 0
Random Pixel CNN ViT Random Pixel CNN ViT Random Pixel CNN ViT
Figure2: Classificationperformanceonnaturalandmedicalimagesthroughlinearprobingusing
featuresextractedfromuntrainedandfrozenmodelsversuspixelsfeatures(SeeSec3fordetails).
2
)%(
ycaruccA2 RelatedWork
Therapidadvancementinmedicalfoundationmodelsofferstheopportunitytodevelophealthcare
AI [55, 49]. However, their lack of transparency presents risks in real-world applications [5].
Interpretability is crucial for using models in high-stakes domains [30, 79, 87]. Previous work
hasprimarilyfocusedonpost-hocinterpretability[76,95,29,75],whichmaynotprovidefaithful
explanations [69]. As an alternative, inherently interpretable methods produce explanations that
alignwiththemodel‚Äôsreasoningprocesses[8,3]. Inthiswork,webuilduponConceptBottleneck
Models(CBMs)[41],whichpredictbylinearlycombininghuman-designedconcepts. Recentwork
[91, 60, 90] scales the applications of CBMs by aligning concepts and images with CLIP [64]
and prompting language models to generate concept bottlenecks automatically. In our work, we
treatCBMsasthearchitecturetoincorporateknowledgepriorsformitigatingmedicaldomainshift
problems. Ourbottlenecks,builtfromthemedicalcorpus,areattributableandmoretrustworthy.
DomainGeneralizationandRobustnessarecriticalinmedicaldomainswherethedistribution
ofimagingprotocols,devices,andpatientpopulationscansignificantlyvary[25]. Alineofwork
studiesvariousdomain-shiftproblems[42,2],proposingalgorithmstolearninvariantrepresentations
[57,22,80,63]andemployingdomain/groupinformationforreweighting[71,93,44,96]. However,
manystudiesshowthosemethodsdonotimproveoverstandardEmpiricalRiskMinimization(ERM)
[67,26,33,27]. Fine-tuningthelastlayer[68,40]orselectivelyfine-tuningafewlayers[47]is
sufficientforrobustnessagainstspuriouscorrelationsinthosedatasets. Weaddressdomainshiftsin
medicalimagingfromanovelperspectivebyemployinginterpretablemodelstointegrateknowledge
priors. Ourapproachencouragesmodelstoadheretodiagnosticrulessimilartothosedoctorsuse
ratherthanrelyingonspuriouscorrelations. Concurrentworkshowsbottleneckmodelscanperform
wellonout-of-domainX-raydatabutseverelyreducedin-domainperformanceasaconsequence[89].
Incontrast,wedemonstrateasignificantlybettercompromisebetweenOODandIDperformance,
usingabroadersetofmodalitiesandconstructingourbottlenecksfrommedicaldocuments.
KnowledgeRichMultimodalReasoning. Knowledgeplaysanimportantroleinclinicaldiagnosis
[6]. Somemultimodaltasks[82,53,74]requiremodelstouseexplicitoutsideknowledgetomake
correctpredictions. Previousmethods[52,50,31]retrievedocumentsforeachexamplefromthe
externalknowledgebaseascontextformodelstogeneratetheanswer.Ourworkfocusesonleveraging
knowledgeinmedicalimageclassification. Retrieval-AugmentedGeneration[48,23]hasbeen
shown to be beneficial for knowledge-intensive tasks [38], including biomedicine [20, 84]. The
retrievedmedicaldocumentsareeitherusedascontextduringinference[88]ordataforpretraining
[94]. Inconstrast,wetreatdocumentsasbackgroundknowledgeforlargelanguagemodelstobuild
conceptbottlenecks. Insteadofretrievingdocumentsforeveryinput,webuildaglobalknowledge
priorfromamedicaldocumentcorpus,whichissharedacrossallexamples.
3 DeepImagePriorsforMedicalImages
Thissectionrevisitstheconceptofdeepimagepriors[72,78],i.e.,somedata-agnosticassumptions
frommodelstructure,inthecontextofimageclassificationacrossvariousdomains. Bycomparing
linearprobingusingfeaturesextractedbyuntraineddeepnetworksagainstpixel-basedfeatures,we
observethatexistingvisionbackboneslacksuitablepriorsformedicaldomains. Thisobservation
motivatesourknowledge-enhancedbottlenecks(Sec4)tointegratemorerobustpriorsintomodels.
(cid:8) (cid:9)
Setup. Consider a dataset of image-label pairs, D = (I,y) , where I is an image and y ‚àà Y
(cid:0) (cid:1)
denotesthelabelfromoneofN classes. ThemodellearnstopredictP y|I,Œ∏ ,whereŒ∏isthemodel
parameters. Weemployafrozen,untrainedvisionbackboneV toextractfeaturesfromI,producinga
featurevectorx=V(I),wherex‚ààRd. Alinearmappingfunctionf :Rd ‚ÜíY isthentrainedto
Œ∏
classifythesefeaturesintolabelspaces. Inthiscase,themodelparametersŒ∏willinherittheimplicit
architecturalpriorsofV. Asabaseline,weextractasubsetofdpixelsdirectlyfromtheimageas
thefeaturewithoutanymodel-basedpriors,representedasx ‚ààRd. Wecomparetheclassification
p
performanceusingxversusx toprobetheefficacyofthevisionbackbone‚Äôspriors.
p
Experiments. Weevaluatetwostate-of-the-artvisionbackbones,ViT-L/14[17]andConvNext-L
[51],onthreecategoriesofimages: naturalphotos(e.g.,ImageNet[70]),X-rays(e.g.,NIH-CXR
[83]),andskinlesionimages(e.g.,HAM10000[77]). Eachimagecategoryhas5datasets,andwe
reporttheiraverageperformanceinFigure2(seeTable9intheAppendixC.1forfullresults).
3Query: How to diagnose COVID from X-rays? Is there ground-glass opacity?
retrieval augmented Clinical Report
retrieve relevant documents
concept generation
Yes No
Medical Corpus Influenza A H1N1 respiratory infection: positive negative
The most frequent radiological
patterns found were ground-glass opacities
and peribronchovascular markings. ùëî#
Learned
Is there a collapse of the lung? ùëî! 0.3
ùëæ
Is the trachea in the midline? ùëî" 0.7
Align
Is there ground-glass opacity? ùëî# 0.9
ùëæ
Is there a gas bubble present? ùëî$ 0.6 Predep fri io nr
ed
Input Medical Image Structure Prior Bottleneck Predictor Parameter Prior
Figure3: OverviewofKnowledge-enhancedBottlenecks(KnoBo)formedicalimageclassification,
comprisingthreemaincomponents: (1)StructurePrior(Sec4.2)constructsthetrustworthyknowl-
edgebottleneckbyleveragingmedicaldocuments;(2)BottleneckPredictor(Sec4.3)groundsthe
imagesontoconceptswhichareusedasinputforthelinearlayerand;(3)ParameterPrior(Sec
4.4)constrainsthelearningoflinearlayerwithparameterspredefinedbydoctorsorLLMs.
Figure2(left)showsvisionbackboneshaveeffectivepriorsfornaturalimages,withViTnotably
outperformingpixelby14.7%. However,pixelfeaturessurpassthoseextractedbyvisionbackbones
forspecializeddomainssuchasX-rayandskinlesionimages. Thisunderscoresthesedeepnetworks‚Äô
lack of image priors appropriate for these domains, which can hamper model learning and hurt
generalizability. Withoutguidancefromappropriatepriors,modelscanoverlyrelyondata,risking
catastrophicfailures. Weaimtoovercomethisbyinjectingadditionalpriorsintomodels.
4 Knowledge-enhancedBottlenecks
In this section, we present Knowledge-enhanced Bottlenecks (KnoBo), a class of CBMs that
incorporateknowledgepriorsthataddressthefailuresweidentifiedinSection3. Figure3presentsan
overviewofourmethod,fromlefttoright,weoptimizethreeterms: (1)StructurePrior(Sec4.2)
inducesbottleneckstructuresfrommedicalcorpustoincorporatehumanknowledgeasconcepts,(2)
BottleneckPredictor(Sec4.3)projectsinputimageontobottleneckconceptsandthenfeedconcept
predictionsintothelinearlayerforlabelprediction,and(3)ParameterPrior(Sec4.4)alignsthe
learnedparameterswithknownassociativeinformationtofurtherenhancepriors.
4.1 ProblemFormulation
Preliminary on Concept Bottleneck Model. Given a bottleneck C with N concepts, CBMs
optimizetwofunctionsforpredictions: yÀÜ=f(cid:0) G(x)(cid:1) ,whereG :Rd ‚ÜíRNC mC apsimagefeatures
intoconceptspace,andf :RNC ‚ÜíY usesconceptpredictionsforfinallabelpredictions.
Formulation. OurgoalistoincorporatepriorsoverC,theconceptstructure,intothelearningofthe
(cid:80) (cid:0) (cid:1)
jointprobability logP y,C,Œ∏|I ,whichcanbedecomposedintothreefactors:
(I,y)‚ààD
(cid:0) (cid:1) (cid:0) (cid:1)
logP y,C,Œ∏|I =logP y|I,C,Œ∏ +logP (C)+ logP (Œ∏) (1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
bottleneckpredictor structureprior parameterprior
where we assume the priors over structure C and parameters Œ∏ are independent. The structure
prior P(C) (Sec 4.2) is formulated as the construction of a bottleneck with N concepts, C =
C
(cid:8) (cid:9)
c ,c ,...,c ,derivedfromabackgroundcorpusB. Eachconceptisafactorthathumanswilluse
1 2 NC
(cid:0) (cid:1)
whensolvingthesametask.ThebottleneckpredictorP y|I,C,Œ∏ (Sec4.3)isaconceptbottleneck
modelthatpredictsthelabelconditionedontheinputimage,bottleneck,andlearnedparameters.
Thebottleneckpredictorisinherentlyinterpretable,andeachparameterinŒ∏hassemantics,denoting
theassociationbetweenconceptsandlabels. TheparameterpriorP(Œ∏)(Sec4.4)regularizesthe
learningofmodelparametersŒ∏withinformationderivedfromhumanknowledge. Jointlyoptimizing
overstructureandmodelparametersisintractable,sowefirstselectahigh-qualityconceptspaceand
thenoptimizetheparametersofthebottleneckjointlywiththeparameterprior.
4
gnidnuorG
reyaL
raeniL
DIVOC4.2 StructuralPrior
GivenabackgroundcorpusBthatspansvariousdocuments, Algorithm1RetrivalAugmentedIterative
weaimtoidentifyabottleneckstructurecontainingconcepts ConceptBottleneckGeneration
beneficial for classifying labels y ‚àà Y. As outlined in Y,setoftargetclassnames
Algorithm 1, we use the class names Y as initial queries B,setofbackgrounddocuments
to retrieve relevant documents B‚Ä≤ ‚äÇ B. Large Language Q‚ÜêY,classnamesasinitialqueries
Models (LLMs) are then prompted to generate concepts C‚Üê[],conceptsinthebottleneck
usingtheseretrieveddocumentsascontext: C‚Ä≤ =LLM(B‚Ä≤). while|C|<N do
C
Thesenewlygeneratedconceptsareaddedtothebottleneck Q‚Ä≤‚Üê[],setofnewqueries
andusedasnewqueriestoretrieveadditionaldocuments. forqinQdo
This iterative process continues to expand the bottleneck B‚Ä≤ =Retrieve(B,q)
C‚Ä≤ =LLM(B‚Ä≤)
until a predetermined number of concepts N is reached.
C C ‚ÜêC+C‚Ä≤,Q‚Ä≤ ‚ÜêQ‚Ä≤+C‚Ä≤
Such concept structures have high likeleehood under the
endfor
languagemodel,conditionedonthebackgroundcorpus,and
Q‚ÜêQ‚Ä≤/*updatequeries*/
thelanguagemodelprobabilityservesasanimplicitprior.
endwhile
4.3 BottleneckPredictor
With the structure prior from the background corpus, we optimize (1) the grounding function
G :Rd ‚ÜíRNC,whichmapstheinputimagetotheconceptspace,and(2)alinearlayerf :RNC ‚ÜíY
that projects concept predictions onto labels. In practice, we implement G as a set of grounding
functions: G = {g } whereeachg predictstheprobabilityofanimageI havingtheconcept
c c‚ààC c
c,P(c|I)=g (x),andx‚ààRd istheimagefeature. Specifically,wederivetrainingexamplesfor
c
groundingfunctionsfromapretrainingdatasetofimage-textpairs. Weusethelanguagemodelto
estimatethepresenceofaconceptintheimagebasedontheinformationintheaccompanyingtext.
(cid:8) (cid:9)
ConceptGrounding. SupposewehaveapretrainingdatasetD ofimage-textpairs (I,t) ,where
pre
tisatextualdescription(suchasaclinicalreport)oftheimage. Basedont,wecaninferifaconcept
cispresentintheimage. Thiscanbeautomatedbypromptingalargelanguagemodeltogeneratea
responseindicatingwhetherthetextimpliestheconcept. Thisway,welabelourpretrainingdataas
positiveandnegativeexamplesforeachconceptc,whichcanbeusedtotrainitsgroundingfunction.
Withthoseannotatedtrainingexamples,weimplementeachgroundingfunctionasabinarylogistic
(cid:16) (cid:17)
regressionclassifier: g (x)=œÉ x¬∑W‚ä§ ,whereW ‚ààRdistheweightsofgroundingfunction
c c c
andœÉisthesigmoidactivation. Finally,weformacollectionofgroundingfunctionsG ={g }
c c‚ààC
tomapanimagefeaturexintoN
C
probabilitiesoverallbottleneckconcepts,withG(x)‚ààRNC.
LinearLayer. UsingconceptprobabilitiesG(x)asinput,wetrainasimplelinearfunctionf tomake
thefinallabelprediction: yÀÜ= f(cid:0) G(x)(cid:1) = G(x)¬∑W‚ä§,whereW ‚àà RN√óNC isthelinearweight
matrix,withN thenumberofclassesandN thenumberofconcepts.
C
4.4 ParameterPrior
Thebottleneckpredictorisinherentlyinterpretablebecausetheparametersofthelinearlayerencode
theaffinitybetweenlabelsandconcepts. Therefore, wecanguidetheparametersbasedonprior
knowledge,i.e.,ifthelabelyispositivelyrelatedtoconceptcbasedonbackgroundknowledge,the
weightw ‚ààW shouldbehigh. Wehopethelearnedparametersdonotdeviatetoomuchfromthis
y,c
assumption,otherwise,themodelmaycapturespuriouscorrelationsinthedata.
Toenforcethis,weletlanguagemodelsdefineaweightmatrixofpriorsW
prior
‚àà RN√óNC,with
eachelementw ‚àà{‚àí1,+1}indicatingthesignofapreferredcorrelationbetweenthelabelyand
y,c
conceptc. ThepriorlossiscalculatedastheL1distancebetweenbetweenW andW :
prior
1
L = ¬∑||tanh(W)‚àíW || (2)
prior N ¬∑N prior 1
C
inwhichweapplytanhactivationonW toscalethelinearweightsto(‚àí1,1),matchingthescaleof
theweightsinthepriormatrix. Thisadjustmentalignsthemodel‚Äôsparameterswiththeexpectedsign
ofthecorrelationsbasedonpriorknowledge. Thefinallossfunctiontotrainthelinearlayeristhe
sumofthecross-entropylossandthepriorloss: L=L +L .
CE prior
5Insummary,wesearchforastructureC thatisconsistentwithpriorknowledgefromabackground
(cid:0) (cid:1)
corpusBtosevereasthebottleneckforthepredictorP y|I,C,Œ∏ . TheparametersŒ∏arealigned
withthepredefinedcorrelationsbetweenlabelsandconceptsidentifiedbylanguagemodels.
5 ExperimentalSetup
This section introduces (1) the confounded and unconfounded medical datasets to evaluate the
robustnessofourknowledge-enhancedbottlenecks(Sec5.1),(2)theblack-boxandinterpretable
baselinesforcomparison(Sec5.2),and(3)theimplementationdetailsofourmethod(Sec5.3).
5.1 Datasets
Weevaluateontwogroupsofdatasetsforeachmodality: (1)theconfoundeddatasets,whichaimto
assesstherobustnessofmodelsbycreatingsplitswithspuriouscorrelations;(2)theunconfounded
datasetsarerandomlysplittomeasurethemodels‚Äôperformanceinnaturalsettings.
ConfoundedDatasets. AsillustratedontheleftofFigure1,weformulatetheconfoundeddatasets
as binary classification tasks, where each class is confounded with one factor. The confounding
combinationsarereversedforin-domain(trainandvalidation)andout-of-domain(test)splits.
TheconfoundeddatasetsofchestX-rayareconstructedfromNIH-CXR[83]andCheXpert[35]with
theirprovidedattributes:(1)NIH-sexusessex(male,female)astheconfoundingfactor;(2)NIH-age
confoundsthedatawithage(young,old);(3)NIH-posanalyzesthepatient‚Äôsposition(standing,lying
down)duringX-rayexaminations;(4)CheXpert-racesplitsthedatabasedonpatient‚Äôsrace(white,
blackorAfricanAmerican);(5)NIH-CheXpertconfoundsX-raysacrossdatasets(NIH,CheXpert).
TheconfoundeddatasetsofskinlesionarederivedfromtheInternationalSkinImagingCollaboration
(ISIC):(1)ISIC-sexand(2)ISIC-agearesetupsimilarlytotheX-raydatasetsmentionedpreviously;
(3)ISIC-sitestudieslesionsdevelopedondifferentsitesofthebody(head,extremities);(4)ISIC-
colorevaluatesexampleswithdifferentskincolors(light,dark);and(5)ISIC-hospitalusesinstances
sampledfromhospitalsindifferentcities(Barcelona,Vienna).
Unconfoundeddatasets. Weevaluate10datasetswithrandomsplits,5foreachmodality. X-ray:
Pneumonia[39],COVID-QU[9],NIH-CXR[83],Open-i[16],andVinDr-CXR[58]. SkinLesion:
HAM10000[77],BCN20000[14],PAD-UFES-20[62],Melanoma[36],andUWaterloo[45].
Alldatasetsaresplitintotrain/validation/testandensurethevalidationandtestsetarebalancedacross
classes. DetailedstatisticsandadditionalinformationoneachdatasetareprovidedinAppendixA.
PretrainingDatasets. Thetrainingofvisionbackbonesandconceptgroundingfunctionsutilizes
datasetswithimage-textpairs. ForX-rays,wechooseMIMIC-CXR[37],whichcontains377,110
X-rayimageswithaccompanyingclinicalreports. Sincethereisnoexistingtext-annotateddataset
forskinlesionimages,weemployGPT-4V[61]togeneratecaptions(seeexamplesinFigure9)fora
subsetof56,590imagesfromISIC,withoutoverlapoftheconfoundedandunconfoundeddatasets.
5.2 Baselines
WecompareKnoBoagainstbothblack-boxmodelsandinterpretableconceptbottleneckmodels.
Black-boxModels. Weincludetwoend-to-endfine-tuningbaselines: (1)ViT-L/14[17]and(2)
DenseNet121[32],bothpretrainedonthepretrainingdatasetsmentionedearlier. Additionally,(3)
LinearProbeextractsvisualfeatureswiththefrozenViT-L/14encoderandlearnsalinearlayer
forclassification. (4)Language-shapedLearning(LSL)[56]aimstodisentangletheimpactof
knowledgeandinterpretablestructure. InspiredbyLSLviacaptioning,wefinetuneaViT-L/14with
thesamedatausedforconceptgroundingfunctionsandapplyalinearlayer(seeAppendix B.2).
ConceptBottleneckModels. (1)Post-hocCBM(PCBM-h)[91]ensemblesconceptbottleneck
modelswithblack-boxresidualpredictors. WeletPCBM-husethesamebottlenecksasourKnoBo
method;(2)LaBo[90]applieslanguagemodelstogenerateconcepts,followedbythesubmodular
selectiontoidentifyasubsetthatenhancesperformance. Followingtheiroriginalsettings,PCBM-h
andLaBouseCLIP(fine-tunedonmedicalpretrainingdatasets)toalignconceptswithimages.
6NIH-sex NIH-age NIH-pos CheXpert-race NIH-CheXpert
Method
ID OOD Avg ID OOD Avg ID OOD Avg ID OOD Avg ID OOD Avg
ViT-L/14 97.0 30.9 64.0 97.4 3.2 50.3 99.7 2.7 51.2 89.4 48.2 68.8 99.9 0.1 50.0
DenseNet 91.4 32.1 61.8 90.6 15.6 53.1 99.3 1.0 50.2 85.0 55.4 70.2 99.9 0.2 50.1
LinearProbe 94.2 46.7 70.5 95.0 11.4 53.2 99.3 17.0 58.2 87.8 71.4 79.6 99.6 6.8 53.2
LSL 84.0 74.3 79.2 79.8 53.8 66.8 95.3 39.0 67.2 80.4 76.4 78.4 95.0 31.8 63.4
PCBM-h 94.2 45.6 69.9 95.0 10.8 52.9 99.3 17.0 58.2 88.0 71.4 79.7 99.6 8.2 53.9
LaBo 91.4 51.3 71.4 92.8 14.4 53.6 98.0 24.3 61.2 86.8 69.2 78.0 98.4 14.9 56.7
KnoBo(ours) 88.6 78.6 83.6 88.8 38.8 63.8 95.7 45.3 70.5 84.0 79.0 81.5 91.6 52.3 72.0
ISIC-sex ISIC-age ISIC-site ISIC-color ISIC-hospital
Method
ID OOD Avg ID OOD Avg ID OOD Avg ID OOD Avg ID OOD Avg
ViT-L/14 92.0 69.0 80.5 95.0 61.3 78.2 94.8 38.3 66.6 96.9 59.2 78.1 99.2 10.0 54.6
DenseNet 85.3 76.0 80.7 93.7 61.3 77.5 81.7 54.5 68.1 93.9 44.6 69.2 98.4 15.1 56.8
LinearProbe 86.0 69.7 77.8 92.7 60.7 76.7 90.2 37.2 63.7 90.8 65.8 78.3 100.0 27.1 63.6
LSL 82.7 78.3 80.5 90.3 66.0 78.2 84.3 50.2 67.3 87.3 73.1 80.2 99.6 27.9 63.8
PCBM-h 86.7 69.0 77.8 93.0 59.3 76.2 90.0 38.5 64.3 91.2 66.5 78.9 100.0 26.8 63.4
LaBo 83.0 69.3 76.2 91.3 61.0 76.2 88.0 39.3 63.7 86.9 78.9 82.9 100.0 8.6 54.3
KnoBo(ours) 84.0 79.7 81.8 88.0 67.7 77.8 80.7 58.8 69.8 89.2 75.8 82.5 88.2 77.5 82.9
Table1: Resultson10confoundeddatasetsoftwomodalities(top-5areX-rayandbottom-5are
skinlesion). Wereportin-domain(ID),out-of-domain(OOD),andaverageofIDandOOD(Avg)
accuracy. Thebestscoreofeachcolumnisbold,andthesecondbestisunderlined.
All baselines use backbones trained on the same pretraining data as our method to ensure a fair
comparison. AppendixB.2providesadditionaldetailsaboutthebaselines.
Evaluation Metrics. We use accuracy as the metric since all evaluated datasets are single-label
classificationtaskswithbalancedvalidationandtestsets. Forconfoundeddatasets,wereportin-
domain(ID,validation),out-of-domain(OOD,test),anddomain-average(meanofIDandOOD)
accuracies,alongwithdomaingaps(‚àÜ=|ID‚àíOOD|),wherealower‚àÜindicatesbetterrobustness.
Forunconfoundeddatasets,wereporttestaccuracy. Arobustandperformantmodelmustachieve
agoodcompromisebetweenconfoundedandunconfoundeddatasets. Forallthebaselinesandour
KnoBomethod,thecheckpointswiththehighestvalidationaccuracyareevaluatedonthetestset.
5.3 ImplementationDetails
PretrainingofMedicalCLIP.Wefine-tuneOpenCLIP[34](ViT-L/14pretrainedonLAION-2B
[73]) on the pretraining medical data for each modality. Unlike previous work [18, 85, 92] that
directly pairs medical images with sentences from clinical reports, we preprocess the reports by
employingGPT-4[1]toextractshortphrases. OurCLIPmodelsperformthebestforbothX-rayand
skinlesiondatasetsinzero-shotandlinearprobing,asshowninTable7intheAppendix.
MedicalCorpus. Wedownload5.5millionarticlesfromPubMedandsegmenttheminto156.9
million snippets to serve as documents for retrieval. Alternatively, we take the medical corpus
organizedbyMEDRAG[88],includingdocumentsfromWikipedia,StatPearls,andmedicaltextbooks.
WeemployBM25[66]astherankingfunctionfordocumentretrieval.
KnoBoDetails. WeselectGPT-4(gpt-4-0613)astheunderlyingLLMforretrieval-augmented
conceptgeneration(Sec4.2). Fortrainingconceptgroundingfunctions(Sec4.3),weoptforFlan-T5-
XXL[10]toannotateclinicalreportsforeachconcept,consideringcost-efficiency. Unlessotherwise
specified,KnoBousesbottlenecksconstructedfromPubMed,eachcontaining150concepts. Figure5
showsourprompt,andduringconceptgeneration,weapplyseveralheuristicfilters(Appendix B.3).
6 Results
Inthissection,wediscussKnoBo‚Äôsperformanceonconfoundedandunconfoundedmedicalimage
datasets(Sec6.1)andanalyzedifferentknowledgeresourcesandourmodeldesign(Sec6.2).
7ChestX-rayDatasets SkinLesionDatasets
Method
ID OOD ‚àÜ‚Üì Avg Unconfd Overall ID OOD ‚àÜ‚Üì Avg Unconfd Overall
ViT-L/14 96.7 17.0 79.7 56.8 70.2 63.5 95.6 47.6 48.0 71.6 84.3 77.9
DenseNet 93.2 20.9 72.4 57.1 66.0 61.5 90.6 50.3 40.3 70.4 71.0 70.7
LinearProbe 95.2 30.7 64.5 62.9 73.8 68.4 91.9 52.1 39.8 72.0 82.8 77.4
LSL 86.9 55.1 31.8 71.0 67.0 69.0 88.9 59.1 29.8 74.0 77.2 75.6
PCBM-h 95.2 30.6 64.6 62.9 74.7 68.8 92.2 52.0 40.1 72.1 81.7 76.9
LaBo 93.5 34.8 58.7 64.2 72.1 68.1 89.9 51.4 38.4 70.6 80.0 75.3
KnoBo(ours) 89.7 58.8 30.9 74.3 73.1 73.7 86.0 70.5 14.1 78.3 78.1 78.2
Table 2: Averaged results across all datasets, including in-domain (ID), out-of-domain (OOD),
domain-gap(‚àÜ,lowerisbetter),andmeanofIDandOOD(Avg)accuracyforconfoundeddatasets.
Forunconfoundeddatasets(Unconfd),wereporttestaccuracy. Overallperformanceiscalculatedas
themeanoftheAvgandUnconfd,theoveralltradeoffbetweendataconditions.
Knowledge ChestX-rayDatasets SkinLesionDatasets
Source
Confd Unconfd Overall Diversity Confd Unconfd Overall Diversity
PROMPT 72.9 72.8 72.9 0.542 78.4 77.0 77.7 0.332
TEXTBOOKS 72.0 72.9 72.4 0.585 77.5 78.3 77.9 0.350
WIKIPEDIA 72.8 72.7 72.8 0.542 77.6 77.9 77.8 0.356
STATPEARLS 73.4 72.0 72.7 0.598 77.1 79.1 78.1 0.379
PUBMED 74.3 73.1 73.7 0.619 78.3 78.1 78.2 0.341
Table3: Comparisonofconceptbottlenecksbuiltfromdifferentknowledgesources. PROMPTisour
baselinewithoutretrievingdocumentsforconceptgeneration. Wereporttheaccuracyofconfounded
(Confd,averageoverIDandOOD),unconfounded(Unconfd)datasets,andtheoverallperformance
ofalldatasets. Diversitymeasuresthedifferencebetweentheconceptsinabottleneck.
6.1 MainResults
KnoBoismorerobusttodomainshifts. Table1showstheresultson10confoundeddatasetsof
X-rayandskinlesions. Black-boxmodelsexcelatin-domain(ID)databutdropsignificantlyonout-
of-domain(OOD)data,especiallyindatasetsconfoundedbyhospitals/resources(NIH-CheXpertand
ISIC-hospital),whichcanbecommonwhencollectingmedicaldatasets[12,81]. KnoBooutperforms
baselinesinOODanddomain-averageaccuracybylargemargins,rankingtop-1ineightdatasetsand
second-bestintheothertwo. End-to-endmodels(ViT-L/14,DenseNet)exhibitlargerdomaingaps
thanlinearprobes,astheyhavemoreparameterstooptimizeperformanceonin-domaindataand
capturespuriouscorrelations. Shapingthevisualrepresentationswithknowledge(LSL)improves
robustnessbutunderperformsKnoBo,withlowerID,OOD,andaverageperformanceacrossmost
datasets. PCBM-hcombinesinterpretableandblack-boxpredictionsbutexhibitsbehaviorssimilarto
black-boxmodelswithseveredropsacrossdomains. UnlikeKnoBo,whichusesmedicaldocuments
tocreateoneglobalbottleneckforeachmodality,LaBobuildsabottleneckforeachdatasetusing
thein-domaindata,whichcanbebiasedandaffectedbyconfoundingfactors,andsoperformsmore
poorly. In summary, KnoBo mitigates the catastrophic failures in domain shifts encountered by
black-boxmodelsandismorerobustagainstvariousconfoundingfactorsacrossmodalities.
KnoBo performs the best across confounded and unconfounded data. Table 2 illustrates the
performanceaveragedacrossconfoundedandunconfoundeddatasets. Forbothtypesofmedical
images, KnoBo achieves the best out-of-domain (OOD) and domain-average performance (Avg)
with minimal domain gaps (‚àÜ), outperforming the strongest end-to-end baseline (ViT-L/14) by
41.8%(X-ray)and22.9%(skinlesion)inOODaccuracy. KnoBoachievescompetitiveperformance
forunconfoundedX-raydatasets,trailingthebest-performingblack-boxmodel(LinearProbe)by
only0.7%. WhileKnoBoislesscompetitiveonskinlesiondatasetsduetothelackoflarge-scale
pretrainingdataforaccurateconceptgrounding,itstillmaintainsperformancecomparabletothe
baselines. Bycalculatingthemeanaccuracyacrossbothconfoundedandunconfoundeddatasets,
KnoBorankstopacrossallmodels,confirmingthatourknowledge-enhanced,interpretableapproach
isapromisingdirectionforbuildingmorerobustandperformantsystemsformedicalimaging.
8Counfounded (ID) Counfounded (OOD) Counfounded (Avg) Unconfounded (Test)
70 80
KnoBo (ours) KnoBo (ours)
90 Linear Probe Linear Probe 70
60
70
50
80 KnoBo (ours) 60 KnoBo (ours)
Linear Probe Linear Probe
40 60
15 45 75 105 135 15 45 75 105 135 15 45 75 105 135 15 45 75 105 135
# of Concepts/Features # of Concepts/Features # of Concepts/Features # of Concepts/Features
Figure4: AblationofbottlenecksizesonX-raydatasets. Thex-axisisthenumberofrandomly
selectedconcepts(KnoBo)orvisualfeatures(LinearProbe).
ChestX-rayDatasets SkinLesionDatasets
Method
ID OOD ‚àÜ‚Üì Avg Unconfd Overall ID OOD ‚àÜ‚Üì Avg Unconfd Overall
KnoBo 89.7 58.8 30.9 74.3 73.1 73.7 86.0 70.5 14.1 78.3 78.1 78.2
w/oG 87.8 51.5 36.3 69.6 70.1 69.9 83.7 69.4 11.5 76.6 70.2 73.4
w/oL 91.6 48.1 43.5 69.8 73.6 71.7 86.5 69.1 16.6 77.8 78.4 78.1
prior
Table4: Ablationstudiesonconceptgrounding(G;Sec4.3)andparameterprior(L ;Sec4.4).
prior
6.2 Analysis
Inthissection,wecomparethebottlenecksconstructedfromdifferentknowledgeresources. We
evaluatetheimpactofeachcomponentofKnoBoonthefinalperformance,includingbottlenecksize,
conceptgroundingfunction,andparameterprior. AdditionalanalysesareavailableinAppendixC.
KnowledgeSources. Besidestheempiricalresultsonconfoundedandunconfoundeddatasets,we
measurethediversityofbottleneckC asDiversity(C)= 1 (cid:80) (cid:80)iÃ∏=j (cid:0) 1‚àísim(c ,c )(cid:1) ,
|C|2‚àí|C| ci‚ààC cj‚ààC i j
wherethesim(¬∑)isthecosinesimilarityofconceptfeaturesencodedbysentencetransformer[65].
TheDiversitycomputesthedistancebetweeneachconceptandeveryotherconceptinthebottleneck.
Table3comparesdifferentknowledgesources. Theretrieval-augmentedbottlenecksperformbetter
thanthosegeneratedbyprompting,especiallyforskinlesions,wheremorespecificknowledgeis
required because prompting lacks diversity. Across both modalities, PubMed is the best overall,
performingbetterfortheX-raymodalitythanotherknowledgesourcesandamongthebestforskin
lesionmodalities. Inevaluationsbytwomedicalstudents,informationfromallknowledgesources
isratedashighlyrelevantandgroundable(seeAppendixC.4). Moreover,showninTable12,our
retrieval-augmentedconceptsareattributable,whichallowsdoctorstoverifythesourceofknowledge.
BottleneckSize. Figure4comparesKnoBoandlinearprobeswhilevaryingthenumberofcon-
cepts/features. KnoBoconsistentlyoutperformslinearprobesacrossallmetricswhengiventhesame
quotaoffeatures,andKnoBocanobtaingoodperformancewithfewerfeatures. Thisdemonstrates
thatinterpretableconceptscoreshavemoreeffectivepriorsthanblack-boxvisualfeatures.
Ablations. Table 4 summarizes experiments ablating major components of our approach. Row
2showstheperformanceofusingdot-productsfrompromptedCLIPmodelsasconcepts, which
markedlyreducesperformance. Thisshowstheimportanceofknowledgegroundinginensuring
KnoBo‚Äôseffectiveness. However,thisstepcanbesimplifiedasmoreadvancedmedicalfoundation
modelsareavailable. Row3showsperformanceomittingtheparameterprior. Itisanimportant
mechanismforconstrainingthefinalphaseoflearning,resultinginconsistentOODimprovements.
7 ConclusionandLimitation
Inthispaper,weanalyzedomain-shiftproblemsinmedicalimageanalysisandidentifyamissing
medicaldeepimagepriorasamaincontributortopoorperformance. Toaddressthis,weintroduce
knowledge-enhancedbottlenecks(KnoBo)tointegrateknowledgepriorsfrommedicaldocuments.
Acrosstwomedicalimagemodalitiesundervariousdomainshifts,KnoBosignificantlyimproves
robustness. KnoBoassumestheavailabilityofmedicalmultimodaldatasets,limitingapplicationsto
rareconditions. Whileourworkimprovesrobustness,medicalexpertsdonotfailintheseways,and
theyshouldbeusedinconjunctionwithmodels.
9
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccAReferences
[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774,2023.
[2] M.Arjovsky,L.Bottou,I.Gulrajani,andD.Lopez-Paz. Invariantriskminimization. arXiv
preprintarXiv:1907.02893,2019.
[3] A.J.Barnett,F.R.Schwartz,C.Tao,C.Chen,Y.Ren,J.Y.Lo,andC.Rudin. Acase-based
interpretabledeeplearningmodelforclassificationofmasslesionsindigitalmammography.
NatureMachineIntelligence,3(12):1061‚Äì1070,2021.
[4] P.J.BevanandA.Atapour-Abarghouei. Detectingmelanomafairly: Skintonedetectionand
debiasing for skin lesion classification. In MICCAI Workshop on Domain Adaptation and
RepresentationTransfer,pages1‚Äì11.Springer,2022.
[5] R.Bommasani, D.A.Hudson, E.Adeli, R.Altman, S.Arora, S.vonArx, M.S.Bernstein,
J.Bohg,A.Bosselut,E.Brunskill,etal. Ontheopportunitiesandrisksoffoundationmodels.
arXivpreprintarXiv:2108.07258,2021.
[6] H.P.BoshuizenandH.G.Schmidt. Ontheroleofbiomedicalknowledgeinclinicalreasoning
byexperts,intermediatesandnovices. Cognitivescience,16(2):153‚Äì184,1992.
[7] L.Bossard,M.Guillaumin,andL.VanGool. Food-101‚Äìminingdiscriminativecomponents
withrandomforests. InComputerVision‚ÄìECCV2014: 13thEuropeanConference, Zurich,
Switzerland,September6-12,2014,Proceedings,PartVI13,pages446‚Äì461.Springer,2014.
[8] C.Chen,O.Li,D.Tao,A.Barnett,C.Rudin,andJ.K.Su. Thislookslikethat: deeplearning
forinterpretableimagerecognition. Advancesinneuralinformationprocessingsystems,32,
2019.
[9] M.E.Chowdhury,T.Rahman,A.Khandakar,R.Mazhar,M.A.Kadir,Z.B.Mahbub,K.R.
Islam,M.S.Khan,A.Iqbal,N.AlEmadi,etal. Canaihelpinscreeningviralandcovid-19
pneumonia? IeeeAccess,8:132665‚Äì132676,2020.
[10] H.W.Chung,L.Hou,S.Longpre,B.Zoph,Y.Tay,W.Fedus,Y.Li,X.Wang,M.Dehghani,
S.Brahma,etal. Scalinginstruction-finetunedlanguagemodels. JournalofMachineLearning
Research,25(70):1‚Äì53,2024.
[11] A.Coates,A.Ng,andH.Lee. Ananalysisofsingle-layernetworksinunsupervisedfeature
learning. InProceedingsofthefourteenthinternationalconferenceonartificialintelligence
andstatistics,pages215‚Äì223.JMLRWorkshopandConferenceProceedings,2011.
[12] J. P. Cohen, P. Morrison, and L. Dao. Covid-19 image data collection. arXiv preprint
arXiv:2003.11597,2020.
[13] J.P.Cohen, J.D.Viviano, P.Bertin, P.Morrison, P.Torabian, M.Guarrera, M.P.Lungren,
A.Chaudhari,R.Brooks,M.Hashir,andH.Bertrand. TorchXRayVision: Alibraryofchest
X-ray datasets and models. In Medical Imaging with Deep Learning, 2022. URL https:
//github.com/mlmed/torchxrayvision.
[14] M. Combalia, N. C. Codella, V. Rotemberg, B. Helba, V. Vilaplana, O. Reiter, C. Carrera,
A.Barreiro,A.C.Halpern,S.Puig,etal. Bcn20000: Dermoscopiclesionsinthewild. arXiv
preprintarXiv:1908.02288,2019.
[15] A. J. DeGrave, J. D. Janizek, and S.-I. Lee. Ai for radiographic covid-19 detection selects
shortcutsoversignal. NatureMachineIntelligence,3(7):610‚Äì619,2021.
[16] D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma. Design and development
ofamultimodalbiomedicalinformationretrievalsystem. JournalofComputingScienceand
Engineering,6(2):168‚Äì177,2012.
10[17] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.Dehghani,
M.Minderer,G.Heigold,S.Gelly,etal. Animageisworth16x16words: Transformersfor
imagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[18] S.Eslami,G.deMelo,andC.Meinel. DoesCLIPbenefitvisualquestionansweringinthe
medicaldomainasmuchasitdoesinthegeneraldomain? arXive-prints,art.arXiv:2112.13906,
Dec.2021.
[19] T.B.Fitzpatrick. Thevalidityandpracticalityofsun-reactiveskintypesithroughvi. Archives
ofdermatology,124(6):869‚Äì871,1988.
[20] G.Frisoni,M.Mizutani,G.Moro,andL.Valgimigli. BioReader: aretrieval-enhancedtext-to-
texttransformerforbiomedicalliterature. InY.Goldberg,Z.Kozareva,andY.Zhang,editors,
Proceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages5770‚Äì5793,AbuDhabi,UnitedArabEmirates,Dec.2022.AssociationforComputational
Linguistics. doi: 10.18653/v1/2022.emnlp-main.390. URLhttps://aclanthology.org/
2022.emnlp-main.390.
[21] J.Futoma,M.Simons,T.Panch,F.Doshi-Velez,andL.A.Celi. Themythofgeneralisability
in clinical research and machine learning in health care. The Lancet Digital Health, 2(9):
e489‚Äìe492,2020.
[22] Y.Ganin, E.Ustinova, H.Ajakan, P.Germain, H.Larochelle, F.Laviolette, M.March, and
V.Lempitsky. Domain-adversarialtrainingofneuralnetworks. Journalofmachinelearning
research,17(59):1‚Äì35,2016.
[23] Y.Gao,Y.Xiong,X.Gao,K.Jia,J.Pan,Y.Bi,Y.Dai,J.Sun,andH.Wang.Retrieval-augmented
generationforlargelanguagemodels: Asurvey. arXivpreprintarXiv:2312.10997,2023.
[24] J.W.Gichoya,I.Banerjee,A.R.Bhimireddy,J.L.Burns,L.A.Celi,L.-C.Chen,R.Correa,
N.Dullerud,M.Ghassemi,S.-C.Huang,etal.Airecognitionofpatientraceinmedicalimaging:
amodellingstudy. TheLancetDigitalHealth,4(6):e406‚Äìe414,2022.
[25] H.GuanandM.Liu. Domainadaptationformedicalimageanalysis: asurvey. IEEETransac-
tionsonBiomedicalEngineering,69(3):1173‚Äì1185,2021.
[26] I. Gulrajani and D. Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434,2020.
[27] L.L.Guo,S.R.Pfohl,J.Fries,A.E.Johnson,J.Posada,C.Aftandilian,N.Shah,andL.Sung.
Evaluationofdomaingeneralizationandadaptationonimprovingmodelrobustnesstotemporal
datasetshiftinclinicalmedicine. Scientificreports,12(1):2726,2022.
[28] K.He, X.Zhang, S.Ren, andJ.Sun. Delvingdeepintorectifiers: Surpassinghuman-level
performanceonimagenetclassification. InProceedingsoftheIEEEinternationalconference
oncomputervision,pages1026‚Äì1034,2015.
[29] L.A.Hendricks,Z.Akata,M.Rohrbach,J.Donahue,B.Schiele,andT.Darrell. Generating
visualexplanations. InComputerVision‚ÄìECCV2016: 14thEuropeanConference,Amsterdam,
TheNetherlands,October11‚Äì14,2016,Proceedings,PartIV14,pages3‚Äì19.Springer,2016.
[30] A.Holzinger,G.Langs,H.Denk,K.Zatloukal,andH.M√ºller. Causabilityandexplainabilityof
artificialintelligenceinmedicine.WileyInterdisciplinaryReviews:DataMiningandKnowledge
Discovery,9(4):e1312,2019.
[31] Z.Hu,A.Iscen,C.Sun,Z.Wang,K.-W.Chang,Y.Sun,C.Schmid,D.A.Ross,andA.Fathi.
Reveal: Retrieval-augmentedvisual-languagepre-trainingwithmulti-sourcemultimodalknowl-
edgememory. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages23369‚Äì23379,2023.
[32] G.Huang,Z.Liu,L.VanDerMaaten,andK.Q.Weinberger. Denselyconnectedconvolutional
networks. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages4700‚Äì4708,2017.
11[33] B.Y.Idrissi,M.Arjovsky,M.Pezeshki,andD.Lopez-Paz. Simpledatabalancingachieves
competitiveworst-group-accuracy. InConferenceonCausalLearningandReasoning,pages
336‚Äì351.PMLR,2022.
[34] G.Ilharco,M.Wortsman,R.Wightman,C.Gordon,N.Carlini,R.Taori,A.Dave,V.Shankar,
H.Namkoong,J.Miller,H.Hajishirzi,A.Farhadi,andL.Schmidt. Openclip,July2021. URL
https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as
below.
[35] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo,
R. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty
labelsandexpertcomparison. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume33,pages590‚Äì597,2019.
[36] M. H. Javid. Melanoma skin cancer dataset of 10000 images, 2022. URL https://www.
kaggle.com/dsv/3376422.
[37] A.E.Johnson,T.J.Pollard,S.J.Berkowitz,N.R.Greenbaum,M.P.Lungren,C.-y.Deng,R.G.
Mark,andS.Horng. Mimic-cxr,ade-identifiedpubliclyavailabledatabaseofchestradiographs
withfree-textreports. Scientificdata,6(1):317,2019.
[38] N.Kandpal,H.Deng,A.Roberts,E.Wallace,andC.Raffel. Largelanguagemodelsstruggleto
learnlong-tailknowledge. InInternationalConferenceonMachineLearning,pages15696‚Äì
15707.PMLR,2023.
[39] D.S.Kermany,M.Goldbaum,W.Cai,C.C.Valentim,H.Liang,S.L.Baxter,A.McKeown,
G.Yang,X.Wu,F.Yan,etal. Identifyingmedicaldiagnosesandtreatablediseasesbyimage-
baseddeeplearning. cell,172(5):1122‚Äì1131,2018.
[40] P.Kirichenko,P.Izmailov,andA.G.Wilson. Lastlayerre-trainingissufficientforrobustness
tospuriouscorrelations. arXivpreprintarXiv:2204.02937,2022.
[41] P.W.Koh,T.Nguyen,Y.S.Tang,S.Mussmann,E.Pierson,B.Kim,andP.Liang. Concept
bottleneckmodels. InInternationalconferenceonmachinelearning,pages5338‚Äì5348.PMLR,
2020.
[42] P.W.Koh,S.Sagawa,H.Marklund,S.M.Xie,M.Zhang,A.Balsubramani,W.Hu,M.Ya-
sunaga,R.L.Phillips,I.Gao,etal. Wilds: Abenchmarkofin-the-wilddistributionshifts. In
Internationalconferenceonmachinelearning,pages5637‚Äì5664.PMLR,2021.
[43] A.Krizhevsky,G.Hinton,etal. Learningmultiplelayersoffeaturesfromtinyimages,2009.
[44] D. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. Le Priol, and
A.Courville. Out-of-distributiongeneralizationviariskextrapolation(rex). InInternational
ConferenceonMachineLearning,pages5815‚Äì5826.PMLR,2021.
[45] V.I.Lab. Universityofwaterlooskincancerdatabase,2021. URLhttps://uwaterloo.ca/
vision-image-processing-lab/research-demos/skin-cancer-detection.
[46] A.J.Larrazabal,N.Nieto,V.Peterson,D.H.Milone,andE.Ferrante. Genderimbalancein
medicalimagingdatasetsproducesbiasedclassifiersforcomputer-aideddiagnosis. Proceedings
oftheNationalAcademyofSciences,117(23):12592‚Äì12594,2020.
[47] Y.Lee,A.S.Chen,F.Tajwar,A.Kumar,H.Yao,P.Liang,andC.Finn. Surgicalfine-tuningim-
provesadaptationtodistributionshifts. InternationalConferenceonLearningRepresentations,
2023.
[48] P.Lewis,E.Perez,A.Piktus,F.Petroni,V.Karpukhin,N.Goyal,H.K√ºttler,M.Lewis,W.-t.
Yih,T.Rockt√§schel,etal. Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems,33:9459‚Äì9474,2020.
[49] C.Li,C.Wong,S.Zhang,N.Usuyama,H.Liu,J.Yang,T.Naumann,H.Poon,andJ.Gao.
Llava-med: Trainingalargelanguage-and-visionassistantforbiomedicineinoneday. arXiv
preprintarXiv:2306.00890,2023.
12[50] W.LinandB.Byrne. Retrievalaugmentedvisualquestionansweringwithoutsideknowledge.
InY.Goldberg,Z.Kozareva,andY.Zhang,editors,Proceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages11238‚Äì11254,AbuDhabi,United
ArabEmirates,Dec.2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.
emnlp-main.772. URLhttps://aclanthology.org/2022.emnlp-main.772.
[51] Z.Liu,H.Mao,C.-Y.Wu,C.Feichtenhofer,T.Darrell,andS.Xie. Aconvnetforthe2020s.
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
2022.
[52] M. Luo, Y. Zeng, P. Banerjee, and C. Baral. Weakly-supervised visual-retriever-reader for
knowledge-based question answering. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t.
Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-
guageProcessing,pages6417‚Äì6431,OnlineandPuntaCana,DominicanRepublic,Nov.2021.
Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.517. URL
https://aclanthology.org/2021.emnlp-main.517.
[53] K.Marino,M.Rastegari,A.Farhadi,andR.Mottaghi. Ok-vqa: Avisualquestionanswering
benchmark requiring external knowledge. In Conference on Computer Vision and Pattern
Recognition(CVPR),2019.
[54] D. McInerney, G. Young, J.-W. van de Meent, and B. Wallace. CHiLL: Zero-shot custom
interpretablefeatureextractionfromclinicalnoteswithlargelanguagemodels. InH.Bouamor,
J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics:
EMNLP2023,pages8477‚Äì8494,Singapore,Dec.2023.AssociationforComputationalLin-
guistics. doi: 10.18653/v1/2023.findings-emnlp.568. URLhttps://aclanthology.org/
2023.findings-emnlp.568.
[55] M.Moor,O.Banerjee,Z.S.H.Abad,H.M.Krumholz,J.Leskovec,E.J.Topol,andP.Rajpurkar.
Foundationmodelsforgeneralistmedicalartificialintelligence. Nature,616(7956):259‚Äì265,
2023.
[56] J.Mu,P.Liang,andN.Goodman. Shapingvisualrepresentationswithlanguageforfew-shot
classification. arXivpreprintarXiv:1911.02683,2019.
[57] K. Muandet, D. Balduzzi, and B. Sch√∂lkopf. Domain generalization via invariant feature
representation. InInternationalconferenceonmachinelearning,pages10‚Äì18.PMLR,2013.
[58] H.Q.Nguyen,K.Lam,L.T.Le,H.H.Pham,D.Q.Tran,D.B.Nguyen,D.D.Le,C.M.Pham,
H.T.T.Tong,D.H.Dinh,C.D.Do,L.T.Doan,C.N.Nguyen,B.T.Nguyen,Q.V.Nguyen,
A.D.Hoang,H.N.Phan,A.T.Nguyen,P.H.Ho,D.T.Ngo,N.T.Nguyen,N.T.Nguyen,
M.Dao,andV.Vu. Vindr-cxr: Anopendatasetofchestx-rayswithradiologist‚Äôsannotations,
2020.
[59] M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of
classes. In2008SixthIndianconferenceoncomputervision,graphics&imageprocessing,
pages722‚Äì729.IEEE,2008.
[60] T.Oikarinen,S.Das,L.M.Nguyen,andT.-W.Weng. Label-freeconceptbottleneckmodels.
arXivpreprintarXiv:2304.06129,2023.
[61] OpenAI. Gpt-4v(ision)technicalworkandauthors.,2023. URLhttps://cdn.openai.com/
contributions/gpt-4v.pdf.
[62] A. G.Pacheco, G. R.Lima, A. S. Salomao, B. Krohling, I. P.Biral, G.G. de Angelo, F.C.
AlvesJr,J.G.Esgario,A.C.Simora,P.B.Castro,etal. Pad-ufes-20: Askinlesiondataset
composedofpatientdataandclinicalimagescollectedfromsmartphones. Datainbrief,32:
106221,2020.
[63] F.Qiao,L.Zhao,andX.Peng. Learningtolearnsingledomaingeneralization. InProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages12556‚Äì12565,
2020.
13[64] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal.Learningtransferablevisualmodelsfromnaturallanguagesupervision.
InInternationalconferenceonmachinelearning,pages8748‚Äì8763.PMLR,2021.
[65] N.ReimersandI.Gurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-networks.
InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
AssociationforComputationalLinguistics,112019. URLhttps://arxiv.org/abs/1908.
10084.
[66] S.Robertson,H.Zaragoza,etal. Theprobabilisticrelevanceframework: Bm25andbeyond.
FoundationsandTrends¬ÆinInformationRetrieval,3(4):333‚Äì389,2009.
[67] E.Rosenfeld,P.Ravikumar,andA.Risteski. Therisksofinvariantriskminimization. arXiv
preprintarXiv:2010.05761,2020.
[68] E.Rosenfeld,P.Ravikumar,andA.Risteski. Domain-adjustedregressionor: Ermmayalready
learnfeaturessufficientforout-of-distributiongeneralization. arXivpreprintarXiv:2202.06856,
2022.
[69] C.Rudin. Stopexplainingblackboxmachinelearningmodelsforhighstakesdecisionsanduse
interpretablemodelsinstead. Naturemachineintelligence,1(5):206‚Äì215,2019.
[70] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A.Khosla,M.Bernstein,etal. Imagenetlargescalevisualrecognitionchallenge. International
journalofcomputervision,115:211‚Äì252,2015.
[71] S.Sagawa,P.W.Koh,T.B.Hashimoto,andP.Liang. Distributionallyrobustneuralnetworks
for group shifts: On the importance of regularization for worst-case generalization. arXiv
preprintarXiv:1911.08731,2019.
[72] A.M.Saxe,P.W.Koh,Z.Chen,M.Bhand,B.Suresh,andA.Y.Ng. Onrandomweightsand
unsupervisedfeaturelearning. InICML,volume2,page6,2011.
[73] C.Schuhmann, R.Beaumont, R.Vencu, C.Gordon, R.Wightman, M.Cherti, T.Coombes,
A.Katta,C.Mullis,M.Wortsman,etal. Laion-5b: Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. AdvancesinNeuralInformationProcessingSystems,35:
25278‚Äì25294,2022.
[74] D.Schwenk,A.Khandelwal,C.Clark,K.Marino,andR.Mottaghi. A-okvqa: Abenchmark
forvisualquestionansweringusingworldknowledge. arXiv,2022.
[75] R.R.Selvaraju,M.Cogswell,A.Das,R.Vedantam,D.Parikh,andD.Batra. Grad-cam: Visual
explanationsfromdeepnetworksviagradient-basedlocalization. InProceedingsoftheIEEE
internationalconferenceoncomputervision,pages618‚Äì626,2017.
[76] K.Simonyan,A.Vedaldi,andA.Zisserman. Deepinsideconvolutionalnetworks: Visualising
imageclassificationmodelsandsaliencymaps. arXivpreprintarXiv:1312.6034,2013.
[77] P.Tschandl,C.Rosendahl,andH.Kittler. Theham10000dataset,alargecollectionofmulti-
sourcedermatoscopicimagesofcommonpigmentedskinlesions. Scientificdata,5(1):1‚Äì9,
2018.
[78] D.Ulyanov, A.Vedaldi, andV.Lempitsky. Deepimageprior. In ProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,pages9446‚Äì9454,2018.
[79] A.Vellido. Theimportanceofinterpretabilityandvisualizationinmachinelearningforapplica-
tionsinmedicineandhealthcare. Neuralcomputingandapplications,32(24):18069‚Äì18083,
2020.
[80] R.Volpi,H.Namkoong,O.Sener,J.C.Duchi,V.Murino,andS.Savarese. Generalizingto
unseendomainsviaadversarialdataaugmentation. Advancesinneuralinformationprocessing
systems,31,2018.
14[81] L.Wang,Z.Q.Lin,andA.Wong. Covid-net: Atailoreddeepconvolutionalneuralnetwork
designfordetectionofcovid-19casesfromchestx-rayimages. Scientificreports,10(1):19549,
2020.
[82] P.Wang,Q.Wu,C.Shen,A.v.d.Hengel,andA.Dick. Explicitknowledge-basedreasoning
forvisualquestionanswering. arXivpreprintarXiv:1511.02570,2015.
[83] X.Wang,Y.Peng,L.Lu,Z.Lu,M.Bagheri,andR.M.Summers. Chestx-ray8: Hospital-scale
chestx-raydatabaseandbenchmarksonweakly-supervisedclassificationandlocalizationof
commonthoraxdiseases.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages2097‚Äì2106,2017.
[84] Y.Wang,X.Ma,andW.Chen. Augmentingblack-boxllmswithmedicaltextbooksforclinical
questionanswering. arXivpreprintarXiv:2309.02233,2023.
[85] Z.Wang,Z.Wu,D.Agarwal,andJ.Sun.MedCLIP:Contrastivelearningfromunpairedmedical
imagesandtext. InY.Goldberg,Z.Kozareva,andY.Zhang,editors,Proceedingsofthe2022
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3876‚Äì3887,Abu
Dhabi,UnitedArabEmirates,Dec.2022.AssociationforComputationalLinguistics. doi: 10.
18653/v1/2022.emnlp-main.256. URL https://aclanthology.org/2022.emnlp-main.
256.
[86] K.Wantlin,C.Wu,S.-C.Huang,O.Banerjee,F.Dadabhoy,V.V.Mehta,R.W.Han,F.Cao,
R. R. Narayan, E. Colak, et al. Benchmd: A benchmark for modality-agnostic learning on
medicalimagesandsensors. arXivpreprintarXiv:2304.08486,2023.
[87] Y.Wu,Y.Liu,Y.Yang,M.S.Yao,W.Yang,X.Shi,L.Yang,D.Li,Y.Liu,J.C.Gee,etal. A
concept-basedinterpretablemodelforthediagnosisofchoroidneoplasiasusingmultimodal
data. arXivpreprintarXiv:2403.05606,2024.
[88] G. Xiong, Q. Jin, Z. Lu, and A. Zhang. Benchmarking retrieval-augmented generation for
medicine. arXivpreprintarXiv:2402.13178,2024.
[89] A. Yan, Y. Wang, Y. Zhong, Z. He, P. Karypis, Z. Wang, C. Dong, A. Gentili, C.-N. Hsu,
J. Shang, et al. Robust and interpretable medical image classifiers via concept bottleneck
models. arXivpreprintarXiv:2310.03182,2023.
[90] Y.Yang,A.Panagopoulou,S.Zhou,D.Jin,C.Callison-Burch,andM.Yatskar. Languageina
bottle: Languagemodelguidedconceptbottlenecksforinterpretableimageclassification. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
19187‚Äì19197,2023.
[91] M.Yuksekgonul,M.Wang,andJ.Zou. Post-hocconceptbottleneckmodels. InTheEleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=nA5AZ8CEyow.
[92] S.Zhang,Y.Xu,N.Usuyama,H.Xu,J.Bagga,R.Tinn,S.Preston,R.Rao,M.Wei,N.Valluri,
etal. Biomedclip: amultimodalbiomedicalfoundationmodelpretrainedfromfifteenmillion
scientificimage-textpairs. arXivpreprintarXiv:2303.00915,2023.
[93] X. Zhang, P. Cui, R. Xu, L. Zhou, Y. He, and Z. Shen. Deep stable learning for out-of-
distributiongeneralization. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages5372‚Äì5382,2021.
[94] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang. Knowledge-enhanced visual-language
pre-trainingonchestradiologyimages. NatureCommunications,14(1):4542,2023.
[95] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for
discriminativelocalization. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages2921‚Äì2929,2016.
[96] X. Zhou, Y. Lin, R. Pi, W. Zhang, R. Xu, P. Cui, and T. Zhang. Model agnostic sample
reweightingforout-of-distributionlearning. InInternationalConferenceonMachineLearning,
pages27203‚Äì27221.PMLR,2022.
15A Dataset
Dataset Confounding n.of n.ofImages
ClassNames
Name Factor Class train val test
NIH-sex sex 2 Atelectasis,Effusion 3000 500 1000
NIH-age age 2 Nofinding,Hasfindings 3000 500 500
NIH-pos position 2 Atelectasis,Effusion 3000 300 300
CheXpert-race race 2 Nofinding,Hasfindings 4000 500 500
NIH-CheXpert dataset 2 Atelectasis,Effusion 5000 1000 1000
Pneumonia[39] - 2 Normal,Pneumonia 5216 16 624
COVID,LungOpacity,
COVID-QU[9] - 4 16930 2115 2114
Normal,ViralPneumonia
Atelectasis,Cardiomegaly,Effusion
NIH-CXR[83] - 6 8066 1140 2317
Consolidation,Edema,Pneumonia
Open-i[16] - 3 Cardiomegaly,LungOpacity,Normal 884 438 890
Aorticenlargement,Cardiomegaly,
VinDr-CXR[58] - 7 Pulmonaryfibrosis,LungOpacity, 1400 175 175
Pleuralthickening,Nodule,Normal
Table5: Detailedstatisticsofthe10ChestX-raydatasetsevaluatedinthiswork.
Dataset Confounding n.of n.ofImages
ClassNames
Name Factor Class train val test
ISIC-sex sex 2 Benign,Malignant 2400 300 300
ISIC-age age 2 Benign,Malignant 2800 300 300
ISIC-site bodysite 2 Benign,Malignant 2000 600 600
ISIC-color skincolor 2 Benign,Malignant 1800 260 260
ISIC-hospital hospital 2 Benign,Malignant 2400 280 280
ActinicKeratoses,BasalCellCarcinoma,
HAM10000[77] - 7 BenignKeratosis-likeLesions,Dermatofibroma, 8010 10001000
MelanocyticNevi,Melanoma,VascularLesions
Nevus,BasalCellCarcinoma
BCN20000[14] - 4 4800 800 800
Melanoma,Actinic/SeborrheicKeratosis
Basal/SquamousCellCarcinoma,
PAD-UFES-20[62] - 2 1602 200 200
Actinic/SeborrheicKeratosis
Melanoma[36] - 2 Benign,Malignant 8605 10001000
UWaterloo[45] - 2 melanoma,notmelanoma 166 20 20
Table6: Detailedstatisticsofthe10SkinLesiondatasetsevaluatedinthiswork.
Table5and6showthedetailedstatisticsforall20datasetsevaluatedinthispaper,wherewelistthe
numberofclasseswithclassnames,thenumberofimagesfortraining,validation,andtesting. For
confoundeddatasets,hereweexplainsomedetailsabouthowwedefinetheconfoundingfactors:
‚Ä¢ NIH-sexandISIC-sexarebuiltbasedonthepatientsexfromNIH-CXR[83]andISIC.
‚Ä¢ NIH-agedefinesyoungaspatient‚Äôsage‚â§30andoldaspatient‚Äôsage‚â•60.
‚Ä¢ NIH-posreferstothepatient‚ÄôspositionduringanX-rayprocedure.Thestandardposition,Posterior
Anterior(PA),involvesthepatientstanding,whiletheAnterior-Posterior(AP)viewisusedwhen
thepatientcannotstandandmustliedown.
‚Ä¢ CheXpert-raceselectsracialsubgroups(White,BlackorAfricanAmerican)fromCheXpert[35].
‚Ä¢ NIH-CheXpert confounds classes by sourcing X-rays fromeither the NIH-CXR or CheXpert
datasets. Specifically,dataforonediseaseisobtainedfromonedataset,whiledataforadifferent
diseaseissourcedfromtheotherdataset.
16Pneumonia COVID-QU NIH-CXR Open-i VinDr-CXR Average
Model
ZS LP ZS LP ZS LP ZS LP ZS LP ZS LP
Random 50.0 50.0 25.0 25.0 16.7 16.7 33.0 33.0 14.3 14.3 27.8 27.8
DenseNet - 83.8 - 82.9 - 53.0 - 63.8 - 27.4 - 62.2
OpenAI-CLIP 62.5 82.4 6.7 91.0 35.0 49.6 21.6 60.3 16.0 33.1 28.4 63.3
OpenCLIP 62.5 77.4 6.3 90.0 7.5 47.9 21.6 58.7 15.4 34.9 22.7 61.8
PubMedCLIP 63.3 72.9 22.0 87.7 30.4 47.7 26.4 59.6 15.4 26.9 31.5 58.9
BioMedCLIP 74.0 85.4 11.8 90.3 31.4 58.7 56.0 67.6 22.3 36.6 39.1 67.7
PMC-CLIP 57.7 84.9 48.1 94.9 40.4 60.6 57.6 67.4 16.0 42.3 43.9 70.0
MedCLIP 84.9 89.9 68.6 87.4 25.1 64.8 70.2 71.9 24.6 40.0 54.7 70.8
Ours 77.7 88.6 60.6 94.9 47.4 68.4 67.5 73.3 29.1 44.0 56.5 73.8
‚àíExtraction 57.2 88.8 46.0 95.3 41.4 68.2 62.9 72.1 21.1 46.0 45.7 74.3
HAM10000 BCN20000 PAD-UFS-20 Melanoma UWaterloo Average
Model
ZS LP ZS LP ZS LP ZS LP ZS LP ZS LP
Random 14.3 14.3 25.0 25.0 50.0 50.0 50.0 50.0 50.0 50.0 37.9 37.9
DenseNet - 79.0 - 69.6 - 69.5 - 91.9 - 45.0 - 71.0
OpenAI-CLIP 3.6 79.9 28.3 67.9 47.0 84.5 50.9 91.6 50.0 60.0 35.9 76.8
OpenCLIP 5.2 82.2 25.0 67.8 45.0 83.5 50.1 92.6 55.0 80.0 36.1 81.2
PubMedCLIP 4.8 76.3 28.9 64.4 50.5 85.5 48.8 92.2 50.0 60.0 36.6 75.7
BioMedCLIP 60.4 75.2 27.5 61.8 61.0 84.5 57.3 90.0 50.0 65.0 51.2 75.3
PMC-CLIP 25.1 82.4 24.8 67.6 55.0 86.0 66.1 92.7 55.0 55.0 45.2 76.7
MedCLIP 8.5 71.4 22.6 55.1 50.0 71.0 50.1 89.9 50.0 50.0 36.2 67.5
Ours 61.5 82.9 53.0 71.0 56.5 86.5 84.0 93.5 75.0 80.0 66.0 82.8
‚àíExtraction 50.9 83.3 46.5 72.0 52.0 86.5 80.1 96.0 70.0 70.0 59.9 81.6
Table7: Zero-shot(ZS)andLinearProbe(LP)resultsofdifferentmodelsonfivechestX-rayand
fiveskinlesiondatasets(notconfounded,randomsplit). Thebestscoreisbold,andthesecondbest
isunderlined. ‚àíExtractionstandsfornotusingLLMtoextractfindingsfromclinicalreports.
‚Ä¢ ISIC-agethresholdsyoungaspatient‚Äôsage‚â§30andoldaspatient‚Äôsage‚â•70.
‚Ä¢ ISIC-sitefocusesonlesionslocatedeitherontheheadoranextremity.
‚Ä¢ ISIC-colororganizesimagesbasedontheFitzpatrickscaleofskintones[19]. FitzpatrickIis
classifiedaslightskin,andIII,IV,andVasdarkskin,accordingtotheannotationsfrom[4].
‚Ä¢ ISIC-hospitalintroducesconfoundsinclassesbyusinglesionimagesexclusivelyfromeitherthe
HospitalCl√≠nicdeBarcelonaortheMedicalUniversityofVienna.
B ImplementationDetails
B.1 CLIPPretraining
WeexperimentedwithexistingCLIPmodelsinthemedicaldomainandfoundtheirperformanceto
beunreliable. Therefore,wedecidetotrainourownCLIPmodelsforX-rayandskinlesionimages.
PretrainingDataset. ForX-rays,weutilizetheMIMIC-CXRdataset[37],specificallyselecting
onlythePAandAPX-rays,whichresultsin243,334images,eachaccompaniedbyaclinicalreport
writtenbydoctors. ForSkinLesionimages,weemploytheISICdatasetanduseGPT-4V[61]to
generateclinicalreportsfor56,590images,examplesareshowninFigure9. Wepreprocessthese
reportsbyextractingmedicallyrelevantfindings,eachdescribedinashortandconciseterm. The
examplebelowdemonstratesareportalongsidethefindingscapturedbyGPT-4. Intotal,weassemble
953Kimage-textpairsforX-raysand438Kforskinlesionimages.
FINALREPORTHISTORY:Unresponsive.Evaluateforpneumonia.
COMPARISON:Chestradiographs___and___.CTthoracicspine___.
FINDINGS:Portablefrontalviewofthechest. Thelungvolumesarelow. Nopleuraleffusionorpneumothorax. Thereis
bibasilaratelectasis,leftgreaterthanright. Heartsizeisnormal. Mediastinalandhilarstructuresareunremarkable. The
configurationofthetracheaisunchangedfrompriorcross-sectionalimaging.
IMPRESSION:Lowlungvolumeswithoutanacutecardiopulmonaryprocess.
FINDINGS(GPT-4):lowlungvolumes,bibasilaratelectasis,leftgreaterthanright,normalheartsize,tracheaunchanged
17Training Details. We utilize the training script from OpenCLIP [34] and select ViT-L/14 as the
backbone. Trainingisperformedon4RTXA6000GPUsfor10epochswithabatchsizeof128anda
learningrateof1e‚àí5. Wechoosecheckpointsbasedonthelowestcontrastivelossonvalidationsets.
CLIPBaselines. WecomparevariousCLIPmodelsacrossunconfoundeddatasetsfortwomodalities,
includingOpenAI-CLIP[64],OpenCLIP[34],PubMedCLIP[18],BioMedCLIP[92],PMC-CLIP1
and MedCLIP [85]. We evaluate these models in both zero-shot and linear probe scenarios. In
zero-shot,GPT-4generatespromptsforeachclass,andweusetheensembleofcosinesimilarities
betweentheimageandpromptsasthescoreforeachclass. Inlinearprobing,weusetheCLIPmodels
asimageencoderstoextractfeaturesforlogisticregression. Additionally,weincludeDenseNet-121
[32](fine-tunedonthepretrainingdatasetswithcross-entropyloss)asabaselineforlinearprobing.
Results. Figure7showsthatourCLIPmodelsperformbestinbothzero-shotandlinearprobing
scenariosforbothmodalities. WefindthatpreprocessingthetextdatawithanLLMsignificantly
enhanceszero-shotperformance. ExistingmedicalCLIPmodelsoutperformgeneralCLIPmodelson
X-raydatasetsbutnotonskinlesionimages,possiblybecauseX-raydataaremoreprevalentand
accessibleinthemedicaldomain. WhileourCLIPmodelsexcelwithcarefuldatacuration,training
convergesquickly,suggestingthecurrentcontrastiveobjectivemightnotfullyexploittheinformation
fromthedata,potentiallytakingshortcuts,suchascomparingimagesfromdifferentpatientsinstead
offocusingondiseases. Futureresearchshouldexploremoresuitableobjectivesandlarger-scale
datacollectionstodevelopmorerobustmedicalfoundationmodels.
B.2 Baselines
This section outlines the implementation details of all baselines compared with our knowledge
bottlenecksformedicalimageclassification. AllbaselinesarerunonasingleRTXA6000GPU.
‚Ä¢ ViT-L/14: WeutilizethevisualencodersfromtheCLIPmodelswepretrainedinSecB.1andadda
classificationheadfordownstreamclassificationdatasets. WeunfreezetheViT-L/14backboneand
trainallparameterswithalearningrateof1e‚àí6andabatchsizeof64for20epochs.
‚Ä¢ DenseNet-121: SimilarlytoViT-L/14,weaddaclassificationheadtothepretrainedDenseNetand
traintheentirenetworkend-to-end. ForX-rays,weusetheDenseNetpretrainedonMIMIC-CXR
byTorchXRayVision[13]. Forskinlesionimages,wepre-traintheDenseNetfromscratchonISIC
usingacross-entropyloss. Whenfine-tunedfordownstreamclassificationdatasets,wetrainthe
DenseNetwithalearningrateof1e‚àí5,abatchsizeof64,andalsofor20epochs.
‚Ä¢ LinearProbe: Weemployvisualencoders(ViT-L/14)frompretrainedCLIPmodelstoextract
featuresforimagesindownstreamclassificationdatasets. Wetrainalinearlayertomapthese
featuresintolabelsfor200epochswithalearningrateof1e‚àí3andabatchsizeof64.
‚Ä¢ LSL [56]: We fine-tune the pretrained CLIP with contrastive loss on annotated concept data
(PubMed bottleneck), using the same data as for concept grounding functions (4.3). Training
instances are triplets (I,c,y), where I is the image, c is a textual concept, and y ‚àà {0,1} is a
binarylabelindicatingwhethertheimagecontainsthisconcept. GiventhevisualencoderV and
textualencoderT oftheCLIP,thecosinesimilaritybetweenanimageandaconceptiss(I,c)=
(cid:0) (cid:1) (cid:0) (cid:1)
cos V(I),T(c) . ThecontrastivelossfunctionisdefinedasL =y¬∑max 0,m‚àís(I,c) +
contrast
(1‚àíy)¬∑s(I,c),wherem=0.6isthemargin. Wefine-tunetheCLIPwithconceptannotations
for20epochswithalearningrateof1e‚àí6 andabatchsizeof64. Afterobtainingthefine-tuned
CLIP,weextractfeaturesandtrainalinearprobeinthesamemannerasthelinearprobebaseline.
‚Ä¢ PCBM-h[91]andLaBo[90]: Weusetheircodebasestoimplementthesebaselines. Theconcept
alignmentinbothmodelsisachievedusingCLIPtocomputethedotproductbetweenimageand
conceptfeatures. PCBM-husesthesamebottleneckasKnoBo,whichisgeneratedfromPubMed.
ForLaBo,weemployGPT-4togeneratecandidateconceptsforsubmodularselection.
B.3 KnoBoDetails
ThissectionprovidesadditionaldetailsabouttheimplementationofKnoBo.
MedicalCorpus. Weutilizeacomprehensivemedicalcorpusforretrieval-augmentedgeneration,
detailedasfollows: (1)PubMed(5.5Mdocs,156.9Msnippets);(2)StatPearls(9.3Kdocs,301.2K
1https://huggingface.co/ryanyip7777/pmc_vit_l_14
18Youareanexperiencedradiologist[dermatologist].Youaresummarizingknowledgeabout‚ÄôQUERY‚ÄôfromchestX-rays[skin
lesionimages].Herearethedocumentsretrievedfromthecorpus:
RETRIEVED_DOCUMENTS
I want you to filter and summarize the information in these documents and generate knowledge in the form of *bi-
naryquestions*,e.g.,"Istherelungopacity?"["Isthelesionasymmetric?"].
Pleasefollowinstructionsbelowstrictly:
1. ThoseknowledgewillbeusedtoguidethediagnosisonchestX-rays[skinlesionimages],sotheymustbe*visually
identifiable*fromchestX-ray[skinlesionimages]only.
2.Thebinaryquestionsshouldbeconciseandnottoospecificwhichcanbereusedfordifferentcases.
3.Thebinaryquestionsmustnotcontaintheclass(disease)name,e.g.,you*mustnot*generate"Istherecardiomegaly?"["Is
thelesionmalignant?"]astheknowledgefor"Cardiomegaly"["MalignantLesion"].
4.Ifthereisnotmuchinformationinthesomedocuments,youcanignorethosedocuments.Ifnoneofthedocumentscontain
usefulinformation,youcanskipthistaskbytyping‚Äôskip‚Äô.
5.Answerwiththefollowingformat:question|documentID|referencesentence,e.g.,Istherelungopacity?|1234|lung
opacityisacommonfindingfor...[Isthelesionasymmetric?|1234|asymmetriclesionisacommonfindingfor...]
Pleaseanswerwithoutadditionalinformationanddonotaddnumbersorbulletpointsintheanswer.
Figure5: Prompttemplateforretrieval-augmentedconceptbottleneckgeneration. Thetextinthe
squarebracketsiswordsthatneedtobechangedwhenusingthispromptforskinlesionimages.
ChestX-rayDatasets SkinLesionDatasets
LLM
ID OOD ‚àÜ‚Üì Avg Unconfound Overall ID OOD ‚àÜ‚Üì Avg Unconfound Overall
Flan-T5 89.7 58.8 30.9 74.3 73.1 73.7 86.0 70.5 14.1 78.3 78.1 78.2
GPT-4 89.9 56.8 33.1 73.3 72.9 73.1 86.0 71.6 14.4 78.8 78.5 78.6
Table8: ComparisonofusingdifferentLLMannotatingconceptsonclinicalreports.
snippets);(3)Textbooks(18docs,125.8Ksnippets);(4)Wikipedia(6.5Mdocs,29.9Msnippets).
The StatPearls, Textbooks, and Wikipedia sources are obtained from MEDRAG [88]. Unlike the
abstract-onlyapproachofPubMedinMEDRAG,weutilizefullarticlesfromPubMed,includingall
paragraphs. WeemploytheretrievalcodebaseofMEDRAGandselectBM25astherankingfunction.
Retrieval-augmentedConceptBottleneckGeneration. Figure5illustratestheprompttemplate
weusetogenerateconceptsfromdocuments. Weretrievethetop10documentsforeachqueryas
contextforthelargelanguagemodel(GPT-4)togenerateconcepts. Aftergeneratingconcepts,we
validateeachconceptbasedonthreecriteriabeforeinclusioninthebottleneck: (1)theconceptmust
bedistinctfromexistingconcepts;(2)itmustbevisuallyidentifiablefromtheimage;and(3)there
mustbesufficientpositiveandnegativeinstancesinthepretrainingcorpustosupporttrainingits
groundingfunction. Aconceptisaddedtothebottleneckonlyifitmeetsallthreecriteria,asjudged
byanotherlanguagemodel(GPT-4). Weinitiallytarget200conceptsperbottleneckbutultimately
selectthetop150withthehighestgroundingaccuracyforinclusion. Thisselectionisduetosome
concepts lacking sufficient reports to effectively train their grounding functions, making 150 the
minimumsizeforallthebottlenecksweconstruct. Table12showsexamplesofgeneratedconcepts.
ConceptGrounding. Weusealanguagemodeltoannotate2,000clinicalreportsforeachconcept
fromthepretrainingcorpus. Toefficientlylabelreportsandachieveabalanceofpositiveandnegative
examples,weretrievethetop1,000reportsshowinghightextualsimilarity(measuredbySentence
Transformer[65])totheconceptasitspotentialpositiveexamplesandrandomlysampleanother
1,000forpotentialnegatives. WeuseFlan-T5-XXL[10]astheunderlyinglargelanguagemodel.
Specifically,theannotationtaskistreatedasanexttokenprediction,similartotheapproachproposed
byMcInerneyetal.[54],wherewecomparetheprobabilitiesofthenexttokenbeingYesorNoto
determineifthereportcontainstheconcept. Figure8showsnobigdifferenceinfinalclassification
performancewhenusingFlan-T5versusGPT-4forannotatingconceptsonreports.
C AdditionalAnalysis
Thissectionpresentsadditionalanalysisandablationstudiesonourmethod.
C.1 DetailsaboutDeepImagePriors
WeprovidefurtherdetailsaboutthedeepimagepriorexperimentsinSec3.
19NaturalImageDatasets
Feature
CIFAR-10 STL-10 ImageNet-10 Food-101 Flower-102 Average
Random 10.0 10.0 10.0 1.0 1.0 6.4
PixelValue 21.2 24.7 22.4 3.0 8.5 16.0
ConvNext-L‚àó 25.6 29.1 32.6 3.7 11.3 20.5
ViT-L/14‚àó 33.3 40.6 47.2 8.9 23.6 30.7
X-rayDatasets
Feature
Pneumonia COVID-QU NIH-CXR Open-i VinDr-CXR Average
Random 50.0 25.0 16.7 33.0 14.3 27.8
PixelValue 77.6 66.9 43.1 58.3 20.0 53.2
ConvNext-L‚àó 62.5 59.2 38.9 57.5 20.0 47.6
ViT-L/14‚àó 67.6 68.0 40.4 57.2 20.0 50.6
SkinLesionDatasets
Feature
HAM10000 BCN20000 PAD-UFS-20 Melanoma UWaterloo Average
Random 14.3 25.0 50.0 50.0 50.0 37.9
PixelValue 65.9 39.4 58.0 74.5 70.0 61.6
ConvNext-L‚àó 66.9 37.1 53.5 68.3 50.0 55.8
ViT-L/14‚àó 67.3 45.9 54.0 84.8 50.0 61.5
Table9: LinearProberesultsofdifferentfeaturesonfivenaturalimagedatasets,fiveX-raydatasets,
andfiveskinlesiondatasets. ‚àódenotesthenetworkisrandomlyinitializedwithoutanytraining.
Datasets. Weevaluatethedeepimageprioronthreecategoriesofimages. TheX-rayandskinlesion
imagesarethesameasthosedescribedintheunconfoundeddatasetssection(SecA).Fornatural
images, weselectfivedatasets: (1)CIFAR-10[43], (2)STL-10[11], (3)ImageNet-10[70]2, (4)
Food-101[7],and(5)Flower-102[59].
Setup. WeemploythevisionbackbonesViT-L/14[17]andConvNext-L[51],bothimplementedby
OpenCLIP[34],andinitializethemusingKaiminginitialization[28]followingPyTorch‚Äôsdefault
settings3. Bothbackbonesextractfeaturevectorsofsize768. Forthepixelvaluebaseline,weconvert
theimagetograyscale,resizeitto28√ó28,andthenflattenitintoavectorof784dimensions. We
usethefirst768valuesofthispixelvectortomatchthesizeofthedeepfeatures. Allfeaturesare
passedthroughalinearlayertopredictthelabelswithalearningrateof1e‚àí3,abatchsizeof64for
200epochs. Additionally,weincludearandombaselineforcomparison.
Results. Table9displaysthefullresultsofallmethodsacrossthethreeimagecategories. ViT-L/14
excelsonnaturaldatasetswithsignificantgainsoverpixelbaselines. ConvNext-Lperformsworse
thanViT-L/14butisstillnotablymoreeffectivethanpixel-basedmethods. ForX-rayimages,the
pixelbaselineclearlysurpassesthetwonetworksonalmostalldatasets,indicatingthatdeepmodels
lackpriorsorevenhaveharmfulpriorsforX-rayimaging. Forskinlesionimages,whicharecloserto
naturalimages,thepixelvaluebaselineperformscomparablytoViT-L/14andbetterthanConvNext-L.
Overall,theresultssuggestthatdeepnetworkslacksufficientpriorsformedicaldomains,potentially
affectingtheirgeneralizability.
C.2 FullResultsonUnconfoundedDatasets
Table 10 shows the comprehensive results of all baselines across the 10 unconfounded medical
datasets. KnoBo performs competitively in the X-ray category, securing top-1 positions for two
datasets and achieving an average ranking of third among all methods. For skin lesion datasets,
KnoBo‚Äôsperformanceislimitedbythesmallerscaleandlowerqualityofthepretrainingcorpus,
whichisannotatedbyGPT-4Vratherthanbyhumanexperts. Thisaffectstheeffectivenessofthe
groundingfunctionsforlesionconcepts. Withaccesstolarger-scaleandhigher-qualitypretraining
data,KnoBocouldpotentiallyclosetheperformancegapwithblack-boxbaselines.
2Weusethe10classesselectedbyImagenette:https://github.com/fastai/imagenette.
3https://pytorch.org/docs/stable/nn.init.html
20Counfounded (ID)
90 Counfounded (OOD) Counfounded (Avg) Unconfounded (Test)
80 80
KnoBo (ours)
Linear Probe 80
75
85
70
KnoBo (ours) 75 KnoBo (ours) 70 KnoBo (ours)
Linear Probe Linear Probe Linear Probe
80 60 65
15 45 75 105 135 15 45 75 105 135 15 45 75 105 135 15 45 75 105 135
# of Concepts/Features # of Concepts/Features # of Concepts/Features # of Concepts/Features
Figure6:AblationofbottlenecksizesonSkinLesiondatasets. Thex-axisisthenumberofrandomly
selectedconcepts(KnoBo)orvisualfeatures(LinearProbe). WereporttheID,OOD,anddomain-
averageperformanceonconfoundeddatasetsandtesttheaccuracyoftheunconfoundeddatasets.
ChestX-rayStandardDatasets
Method
Pneumonia COVID-QU NIH-CXR Open-i VinDr-CXR Average
ViT-L/14 84.6 96.5 66.0 67.0 37.1 70.2
DenseNet 85.1 92.4 56.9 63.5 32.0 66.0
LinearProbe 88.6 94.9 68.4 73.3 44.0 73.8
LSL 87.0 86.9 58.4 64.8 37.7 67.0
PCBM-h 87.7 94.9 68.6 73.2 49.1 74.7
LaBo 88.3 91.8 66.8 68.9 44.6 72.1
KnoBo(ours) 90.1 88.0 66.5 73.5 47.4 73.1
SkinLesionStandardDatasets
Method
HAM10000 BCN20000 PAD-UFS-20 Melanoma UWaterloo Average
ViT-L/14 87.1 76.6 88.5 94.1 75.0 84.3
DenseNet 79.0 69.6 69.5 91.9 45.0 71.0
LinearProbe 82.9 71.0 86.5 93.5 80.0 82.8
LSL 81.5 67.5 84.5 92.5 60.0 77.2
PCBM-h 82.9 70.9 86.0 93.6 75.0 81.7
LaBo 80.6 68.5 82.5 93.6 75.0 80.0
KnoBo(ours) 78.2 65.6 80.0 91.5 75.0 78.1
Table10: Testaccuracyon10unconfoundeddatasetsoftwomodalities.
C.3 AblateBottleneckSizeforSkinLesion
Figure6comparestheperformanceofKnoBoandaLinearProbeacrossdifferentfeaturesizeson
skinlesiondatasets. MirroringthetrendsobservedinX-raysshowninFigure4,ourinterpretable
bottleneckrepresentationsconsistentlyoutperformblack-boxvisualfeatures.
C.4 HumanEvaluationonBottlenecks
Twomedicalstudentsevaluatedthequalityofbot- Table11: RelevanceandGroundabilityofcon-
tlenecksusingtwometrics: (1)Relevancemea- ceptsinbottlenecksgeneratedfromdifferentre-
sures the concept‚Äôs relevance to diagnosing dis- sources,asevaluatedbystudentdoctors.
easesonascalefrom1(notatallrelevant)to4
(mostlyrelevant),and(2)Groundabilityassesses Knowledge Relevance Groundability
theverifiabilityoftheconceptfromtheimageon Source
X-ray Skin X-ray Skin
a scale from 1 to 4. We evaluated 30 randomly
sampledconceptsfromeachbottleneck. Table11 PROMPT 3.83 3.93 3.03 3.00
presentsthesemetricsforbottlenecksconstructed TEXTBOOKS 3.70 3.80 2.90 3.27
fromfivedifferentknowledgesources. Whileall WIKIPEDIA 3.80 3.67 2.83 3.33
bottlenecks show good relevance, groundability STATPEARLS 3.87 3.80 2.70 2.97
scoresarelower,reflectingthechallengeofderiv- PUBMED 3.70 3.83 2.77 3.20
ingvisualconceptsfromtext-onlydata.
21
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccAQuery CLIP Ours (w/ concept grounding)
Are lung
fields clear 3
-
p
on both o
T
sides?
Is there a
visible
3
enlargement p-
o
of the T
heart?
Does the
lesion have
3
a regularly p-
o
shaped T
border?
Does the
lesion show 3
-
p
dry scaly o
T
areas?
Figure7: CompareCLIPandourconceptgroundingfunctioninretrievingimagesbasedonatext
query. Imagesmarkedwithgreencheckmarksarecorrectretrievalsasassessedbymedicalstudents.
Edema Top-3 Concepts Cardiomegaly Top-3 Concepts
oIs there an enlarged cardiac oAre abnormalities observed in the
silhouette?
subpleural region?
oCan increased cardiothoracic ratios be
oAre there new alveolar or interstitial
identified?
infiltrates?
oDoes the radiograph show any large
oAre any areas of hazy opacity present?
transverse dimension of the heart?
Melanocytic Nevi Top-3 Concepts Melanoma Top-3 Concepts
oDoes the lesion appear as an elevated oDoes the lesion present as a flat
nodule? pigmented area?
oDoes the lesion have an asymmetric oDoes the lesion adhere to the ABCD
disposition? dermoscopy rule?
oCan brownish-black verrucose oDoes the lesion show central
plaques be seen on the body? ulceration-crust?
Figure8: Top-3conceptsforaclassrankedbytheirweightsinthelinearlayerofKnoBo.
D QualitativeExamples
Figure7comparestheCLIPdotproductandourconceptgroundingfunctionsforretrievingimages
basedontextqueries. Ourmethoddemonstratessuperiorrecallofcorrectexamplescomparedto
CLIPalignments. Figure8showcasesthetopconceptsselectedbasedonthelinearweightslearned
byKnoBo,whicharemostcorrelatedwiththecorrespondingdiseaseclass. Theseexampleshighlight
thatthetopconceptsutilizedbyKnoBoareessentialfeaturesdoctorsusefordiagnosingtargeted
diseases. Table12displaysexamplesofconceptsgeneratedbyourretrieval-augmentedapproach.
Eachconceptcanbetracedbacktoitssourcedocumenttoverifyitsaccuracy. Figure9presents
examplesofGPT-4V-annotatedclinicalreportsforskinlesionimages. Notably,thesecondexample
demonstratesGPT-4V‚Äôscapabilitytoassesslesionsizeusingthescaleprovidedintheimage.
22
RXC-HIN
iveN
cityconaleM
RXC-rDniV
00002NCBBottleneck Concept Query ReferenceDocument
X-ray(PubMed) Is there lung col- Atelectasis Atelectasisandpneumoniawerediagnosed
lapse? onradiologicalandclinicalcriteria.Atelec-
tasiswasdiagnosedwhenafindingoflung
collapsewasmadeonchestX-ray,chest
CTand/orlungultrasound.[Source]
X-ray(StatPearls) Is there a widened Aortic Enlarge- Onchestx-ray(CXR),findingsthatmayin-
mediastinum on ment dicateaorticpathologyincludeawidened
chestX-ray? mediastinum,lossoftheaorticknobcon-
tour,inferiorlydisplacedleftbronchus,and
leftpleuraleffusion.[Source]
Skin(Textbook) Does the lesion SeborrheicKer- Lesionshavenomalignantpotentialbutmay
haveawaxyappear- atosis be a cosmetic problem. Present as exo-
ance? phytic,waxybrownpapulesandplaques
withprominentfollicleopenings. [Inthe
book:FirstAidStep-2]
Skin(Wikipedia) Are there small Basal Cell Car- BCC, also known as basal-cell cancer, is
blood vessels run- cinoma the most common type of skin cancer. It
ning over the skin often appears as a painless raised area of
lesion? skin,whichmaybeshinywithsmallblood
vesselsrunningoverit.[Source]
Table12: Weshowconceptsfromvariousbottlenecksbyimagemodality(corpus),withthequeries
forretrievalandthecorrespondingreferencedocumentstogeneratetheconcept. Everyconceptis
attributable,allowingmedicalprofessionalstoverifyitsorigininthesupportingdocumentation.
Location: Lower extremity
Lesion Description: Macroscopic Characteristics: The lesion presents as a solitary macule with a relatively
homogeneous area centrally and a slightly irregular periphery.
Color: Tan to light brown background with several shades of light to dark brown pigmentation centrally.
Networks: A faint reticular pattern is seen in portions of the lesion, with the network fading towards the edges.
Structures and Patterns: Focal areas exhibit a more globular pattern, while the majority of the lesion lacks
specific structural components.
Symmetry: The lesion exhibits asymmetry in both color and structure.
Borders: Borders are ill-defined, with the pigment diffusing into the surrounding skin.
The lesion presents as a well-circumscribed, ovoid macule measuring approximately 2.7mm in its largest
diameter, located on the posterior torso. Dermoscopically, it is characterized by a homogenous pattern with a
central brown pigmentation that shows a subtle variegation in color intensity, ranging from light to darker
brown. The peripheral area of the lesion transitions to a lighter, more tan hue, with discrete fading into the
surrounding normal skin without evidence of a sharp border.
Vascular structures are not prominent within the lesion itself; however, the background skin exhibits a reticular
pattern of superficial blood vessels which is consistent with the anatomical site and possibly the patient's age.
There is no evidence of atypical pigment networks, blue-whitish veils, regression structures, or other sinister
features typically associated with malignancy.
General Appearance: The lesion appears as a well-defined, round to oval macule with a notable central darker
area and a lighter peripheral zone. It measures approximately 1 cm in diameter.
Color: The lesion exhibits a heterogeneous mix of dark brown and tan shades. The central area is densely
pigmented, surrounded by a less intensely colored periphery.
Texture: The surface texture cannot be precisely determined from the image alone, but there appears to be no
significant elevation or depression compared to the surrounding skin.
Distribution and Symmetry: The lesion is solitary with a symmetrical appearance.
Additional Observations: No visible scales, ulcers, or signs of bleeding are present. Hair follicles can be
observed within and around the lesion, suggesting it is not disrupting normal skin structures significantly.
Figure9: ExamplesofclinicalreportsonskinlesionimagesgeneratedbyGPT-4V[61].
23