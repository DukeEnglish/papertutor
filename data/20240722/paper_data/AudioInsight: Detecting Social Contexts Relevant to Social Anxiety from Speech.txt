AudioInsight: Detecting Social Contexts Relevant to
Social Anxiety from Speech
Varun Reddy* Zhiyuan Wang*‚Ä† Emma R. Toner, Maria A. Larrazabal
Department of Computer Science Department of Systems and Info. Engineering Department of Psychology
University of Virginia University of Virginia University of Virginia
Charlottesville, USA Charlottesville, USA Charlottesville, USA
dpc3qt@virginia.edu vmf9pr@virginia.edu {ert6g,ml4qf}@virginia.edu
Mehdi Boukhechba Bethany A. Teachman Laura E. Barnes
Johnson & Johnson Innovative Medicine Department of Psychology Department of Systems and Info. Engineering
Titusville, USA University of Virginia University of Virginia
mboukhec@its.jnj.com Charlottesville, USA Charlottesville, USA
bat5x@virginia.edu lb3dp@virginia.edu
Abstract‚ÄîDuring social interactions, understanding the in- Existing work: This paper: Social Context Detection
tricacies of the context can be vital, particularly for socially Social Interaction Detection Dimension 1: Dimension 2:
N of Interaction Partners Degree of Evaluative Threat
anxious individuals. While previous research has found that the
Non-Social
presence of a social interaction can be detected from ambient Dyadic Explicitly Evaluative
audio, the nuances within social contexts, which influence how vs. vs. vs.
Group Not Explicitly Evaluative
anxiety provoking interactions are, remain largely unexplored.
Social
As an alternative to traditional, burdensome methods like self- Social Social
report, this study presents a novel approach that harnesses
ambient audio segments to detect social threat contexts. We Fig.1. Comparisonbetweenexistingworkandourproposedsocialcontext
focus on two key dimensions: number of interaction partners recognition focusing on the number of interaction partners and degree of
(dyadic vs. group) and degree of evaluative threat (explicitly evaluativesocialthreat.
evaluative vs. not explicitly evaluative). Building on data from
a Zoom-based social interaction study (N=52 college students,
of whom the majority N=45 are socially anxious), we employ communication and relationships, and provide insights into
deep learning methods to achieve strong detection performance. mental health, especially social anxiety disorder (SAD) [3],
Under sample-wide 5-fold Cross Validation (CV), our model
[4]. SAD, marked by an intense fear of social situations that
distinguisheddyadicfromgroupinteractionswith90%accuracy
prompt fears of negative evaluation from others, is highly
and detected evaluative threat at 83%. Using a leave-one-group-
out CV, accuracies were 82% and 77%, respectively. While prevalent and impairing but undertreated [5]. Recognizing
our data are based on virtual interactions due to pandemic changes in social contexts likely to trigger social anxiety
constraints, our method has the potential to extend to diverse can guide detection of changes in mental status, pinpointing
real-world settings. This research underscores the potential of
opportune moments for interventions [6].
passive sensing and AI to differentiate intricate social contexts,
and may ultimately advance the ability of context-aware digital While self-reported data and laboratory-based studies have
interventions to offer personalized mental health support. historically been instrumental in providing insights into social
Index Terms‚Äîsocial context, audio analysis, social anxiety interactions and their contextual characteristics [7]‚Äì[9], these
methods often come with inherent limitations, being both
I. INTRODUCTION
burdensomeandimpracticalforscalableandreal-timeapplica-
Social behavior and its variations across social contexts tions [10], [11]. This limitation is particularly pertinent in the
have been widely examined across multiple disciplines, from context of social anxiety, where the immediate and dynamic
psychology to sociology [1], [2] because of its centrality to nature of interactions plays a crucial role. Advanced sensing
understanding the human experience. The characteristics of technology,suchasacousticsensingavailableonsmartphones
socialcontexts,suchasthenumberofinteractionpartnersand and smartwatches, has emerged as a promising alternative,
perceivedevaluativethreats,arefoundationaltounderstanding demonstrating potential in detecting social interactions [12],
[13] (as illustrated on the left side of Figure 1) and assessing
*Thetwoauthorscontributedequallytothiswork.
mental status [14], [15]. This technology opens new avenues
‚Ä† CorrespondingAuthor:ZhiyuanWang(vmf9pr@virginia.edu)
This work was supported in part by a 3Cavaliers Seed Grant, by the for analyzing the nuances of social interactions, like the num-
NationalInstituteofMentalHealthoftheNationalInstitutesofHealthunder ber and type of interaction partners, which are critical in un-
award number R01MH132138, and the Commonwealth Cyber Initiative, an
derstanding the anxiety-provoking aspects of social contexts.
investment in the advancement of cyber R&D, innovation, and workforce
development.FormoreinformationaboutCCI,visitwww.cyberinitiative.org. However, the specific nuances within social interactions (e.g.,
4202
luJ
91
]CH.sc[
1v85441.7042:viXrathenumberofinteractionpartners)thatdeterminetheirlevelof contexts requires a more detailed exploration. Research has
anxiety-provocation and the subtle changes in speech patterns demonstrated the feasibility of computer vision in identifying
associated with these contexts remain largely unexplored (see intense social interactions like real fights [16]. Furthermore,
Figure 1 1). This gap highlights the need for a focused studieshaveexploredthephysiologicalresponsesinevaluative
approach that leverages the capabilities of advanced acoustic contextsusingwearablesensors[4],[17].Effortstounderstand
sensingtodeciphertheseintricateaspectsofsocialinteractions socialrelationsusingaudio-visualmediumshavealsocometo
to ultimately advance understanding of contextual influences theforefront,asseeninstudiesexaminingclinician-patientdy-
on potential interventions for social anxiety. namics [18]. In this study, we aim to capture the complexities
In this paper, we leverage ambient audio segments to of social contexts related to social anxiety, focusing solely on
passively detect social contexts relevant to social anxiety, acoustic analysis.
around two important dimensions: 1) interaction group size Recent advancements have leveraged sensing signals to
‚Äì dyadic vs. group, and 2) the degree of evaluative threat detect social interactions. Wang et al. [15] utilized mobile
‚Äì explicitly evaluative vs. not-explicitly-evaluative (we as- sensing indicators to identify anxiety-relevant social contexts,
sume that even when we are not giving direct evaluation includingtemporalphasesandgroupsizes.TheElectronically
instructions, there can be fears of evaluation, especially for Activated Recorder (EAR) [11] has provided a means to
socially anxious individuals; hence, we use the term not- captureextendedaudiodatafromconversations,whilesystems
explicitly-evaluative). Our study involved a series of Zoom- developedbyresearcherslikeFengetal.[19]optimizethede-
based virtual social interactions with N=521 undergraduate tectionofaudioactivitybycalculatingfeaturesduringspecific
students, among whom the majority (N=45) were high in acoustic events. Acoustic sensing has emerged as a promising
socially anxiety symptoms (thereby ensuring our social threat tool for understanding social interactions. Lane et al. [20]
context manipulations would be relevant). Data from four employed deep learning methods to classify different acoustic
distinctinteractionswascollected:explicitlyevaluativedyadic, events, and Liang et al. [21] demonstrated the potential of
not-explicitly-evaluative dyadic, explicitly evaluative group, wearable technology in detecting face-to-face conversations.
and not-explicitly-evaluative group. We employ state-of-the- Deep learning models such as Convolutional Neural Net-
artaudio2imagetechniques,whichfacilitateaccuratedetection works (CNNs) have shown considerable promise in audio
of social context from spectrum and audio-feature embedded processing [22]. Spectrograms, which convert audio signals
images using Convolutional Neural Networks (CNNs). Our into images, allow CNNs to discern patterns related to social
methods, using audio segments within 7 seconds, achieved contexts.TheDeepInsightmethod[23]enhancesCNNperfor-
a 90% accuracy in differentiating between dyadic and group mance by mapping multidimensional audio features onto 2D
interactions,and83%inidentifyingexplicitevaluativethreats, planes, preserving relational structures within the data.
bothmeasuredundera5-foldcross-validation(CV).Moreim-
portantly, under a leave-one-group-out CV setting (iteratively
III. METHODS
leave out all data from a given set of participants to train and
validate the models), the accuracy rates were 82% and 77% Leveraging a dataset collected from virtual social inter-
for the respective tasks. actions conducted on Zoom, our analysis sought to detect
While this study was primarily conducted with virtual key characteristics of social contexts that are often tied to
interactiondatafromZoomduringthepandemic,itsrelevance social anxiety (i.e., number of interaction partners and de-
extendsbeyondthiscontext,reflectingthegrowingimportance gree of evaluative threat). This data collection included a
ofvirtualinteractionsinmodernsociallife.Themethodologies range of sensors and features (e.g., physiological signals from
and insights we have developed are broadly applicable to wearable devices, self-reported survey responses, audio and
everyday interactions, potentially captured through devices video recordings from Zoom), but our goal for this paper
like smartphones and smartwatches. Our work underscores was to learn whether audio on its own (selected because it is
the potential of passive sensing and AI in understanding not burdensome, unlike self-report, and easily transportable,
social contexts, opening avenues for context-aware, just-in- unlike video) could distinguish these important social context
timementalhealthinterventions.Initialtestswithstate-of-the- variations.Theaudiowasprocessedthroughacombinationof
art models such as Transformers showed poor generalization audio-to-image (audio2image) conversion and Convolutional
on our limited dataset. Despite employing typical training NeuralNetworks(CNNs),leadingtoamodeltodetectspecific
techniques,thesemodelsdidnotperformwellonthetestsets, social contexts. We used 26 acoustic features in our analy-
highlighting the limitations imposed by our sample size. sis, including Pitch, Jitter, Shimmer, Voice Breaks, Formant
Frequencies, Formant Bandwidths, Intensity (dB), Harmonic
II. RELATEDWORK Differences, HNR, MFCC, Spectral Centroid, Spectral Band-
width, Spectral Contrast, Spectral Flatness, Spectral Rolloff,
While the potential of leveraging audio for detecting so-
ZeroCrossingRate,RMSEnergy,ChromaFeatures,Temporal
cial interactions is evident, capturing the nuances of social
Entropy, Autocorrelation, LPC Coefficients, Delta MFCC,
Delta-Delta MFCC, Formant Amplitude, and Log Energy,
1Thestudyinitiallyinvolved54participants;however,theconversationsof
twoindividualswerenotsuccessfullyrecordedduetotechnicaldifficulties. which are widely recognized in acoustic analysis [24].A. Data Collection: Social Interactions Monitoring TABLEI
SUMMARYOFTHEDATASETCOLLECTED.
1) Participants: The study collected data (Zoom audio
recordings) from a sample of N=52 undergraduate students. Data Quantity
Themajorityoftheparticipants(N=45)scored34oraboveon
Subjects 52
the Social Interaction Anxiety Scale (SIAS) [25], a threshold
Male 12
indicating moderate to severe symptoms of social anxiety Female 40
symptoms on a scale from 0 to 80, and N=7 subjects demon- Interactions Recordings 65
stratedlowsocialanxiety(allowingforafullrangeofanxiety Evaluative Dyadic 25 (4 min)
Not-Explicitly-Evaluative Dyadic 23 (4 min)
severity,butoversamplingofindividualswithheightenedcon-
Evaluative Group 9 (6 min)
cerns about social threats). The study received approval from
Not-Explicitly‚ÄìEvaluative Group 8 (6 min)
the Institutional Review Board (IRB) at a public university in
the U.S., and all participants provided informed consent.
2) StudyDesignandProcedure: Thestudy,whichreceived
audio data. It is important to note that, the order of the four
full ethical approval and was under the supervision of a
social experiences was shuffled to ensure that the responses
licensed clinical psychologist and researcher with expertise
and interactions of participants were not influenced by the
in anxiety disorders, involved the collection of audio data
sequence of the experiences.
from virtual meeting recordings collected via Zoom. The data
collection approach was purposefully crafted to manipulate B. Social Context Detection Framework
various social contexts, including evaluative threat (explicitly
Our framework, shown in Figure 2, detects social contexts
evaluated or not explicitly evaluated), group size (dyadic vs.
from audio segments by converting pre-processed audio clips
groupconversation),anddifferentanxietyphases(anticipatory,
into multi-channel image representations. Of note, this ap-
concurrent, and post-event). In this study, we focus on audio
proach avoids more ‚Äòstate-of-the-art‚Äô pre-trained Transformer-
analysis across different evaluative threats and group sizes.
basedmethodslikeAudiospectrogramtransformer[26]which
Specifically, participants took part in four distinct social
typically have large model parameters, was to maintain a
experiences2,i.e.,dyadicandgroupconversationswithexplicit
compact model scale and enhance flexibility for downstream
or not explicit evaluative threat, after a baseline phase where
deployment on edge devices with resource constraints.
participants watched a neutral video alone. In detail:
1) Audio Data Preprocessing: We implemented noise-
‚Ä¢ Dyadic Conversations (twice with different levels of reduction techniques and eliminated segments with prolonged
evaluative threat): Participants engaged in one-on-one silence for clarity. Using Spectral Subtraction and short-term
conversationslasting4minuteseach(e.g.,aconversation Fourier transform (STFT), we subtracted the estimated noise
between P01 and P02). spectrum from the signal, focusing on low-energy frames.
‚Ä¢ Group Conversations (twice with different levels of Silence removal was achieved by classifying frames with
evaluative threat): Conversations involved 3 to 6 partic- energy levels below 0.7 times the average as silence. Next,
ipants and lasted 6 minutes each (e.g., a conversation we segmented audio data into distinct, non-overlapping units
including P01 to P06), with the number of participants oflengthl,treatingeachasanindividualsample.Eachsample
contingent upon attendance. was paired with its respective social context label.
Among the dyadic and group conversations, two levels of 2) Audio-to-ImageTransformation: Theaudiosegmentsare
evaluative threat were manipulated as follows: transformed into two unique image types - the Spectrogram
‚Ä¢ Explicit Evaluative Threat: In one dyadic and one and the DeepInsight [23] Feature2Image - to enable CNN-
group interaction, participants were explicitly informed based social context classification. These transformations ex-
beforehandthattheirperformancewouldbeevaluatedby tract complex frequency and audio feature domain character-
their conversational partners post-conversation. istics, thereby addressing a gap in traditional audio process-
‚Ä¢ Non-Explicit Evaluative Threat: For the other dyadic ingmethodologies.Thespectrogramtransformationvisualizes
and group interactions, participants were informed that time-dependent frequency content, enabling the model to dis-
theirperformancewouldnotbeassessedbytheirconver- cern social context-related spectral patterns. The DeepInsight
sational partners. Feature2Image transformation overcomes spectrogram limita-
tions by mapping multi-dimensional audio features to a 2D
We provided each participant with random, unique conver-
image space, preserving relational intricacies and facilitating
sation prompts, such as ‚Äúif you won a million dollars, how
higher-level abstractions for the CNN.
would you spend the money and why?‚Äù, for each interaction
Spectrogram: Spectrograms translate one-dimensional
to initiate the conversation directions. Throughout the entire
time-series audio signals into two-dimensional images, cap-
Zoomsession,videorecordingsweremade,withthestartand
turing variations in frequency over distinct time intervals.
end points of each social interaction noted to segment the
Spectrogramspreservethetime-seriesnatureoftheaudiodata,
a factor critical to our task of social context detection. This
2For some study sessions, there was no time to complete all interactions,
causingdifferentnumberofsamples. form of representation, merging frequency and time informa-Non-Overlapping Time Window ùíç Channel1
‚Ä¶ ‚Ä¢ Dyadic Interaction
‚Ä¢ Group Interaction
Audio Segmentation Spectrogram Task 1. N of Interaction Partner Detection
Noise Reduction & ‚Ä¶
Silence Removal Channel2
Audio Featurization ‚Ä¢ Explicitly Evaluative
‚Ä¢ Not Explicitly Evaluative
Task 2. Evaluative Threat Detection
e.g., pitch, energy, etc.
Feature2Image
Raw audio signals
DeepInsight CNNModels
Fig.2. Illustrationofthemulti-channelCNNframeworkwithaudio-to-imageprocessingforsocialcontextdetection.
tion, adeptly captures voice variations instrumental for social this,thenetworkcomprisesthreeconvolutionallayersforeach
context detection. The visualization ability of these time- channel. Each convolutional layer is equipped with a ReLU
frequency representations not only assists the CNN model but activation function and is followed by a max pooling layer to
also provides valuable insights into the temporal context of reducespatialdimensionsandenhancefeatureextraction.Post
these variations, thereby enriching our understanding of the these layers, a flattening step is implemented to convert the
underlying social context. 2Dfeaturemapsintoa1Dvector.Thisflattenedvectoristhen
DeepInsight Feature2Image: The DeepInsight transfor- passedthroughadenselayerandculminatesinasoftmaxout-
mation technique leverages a multi-step process to convert put layer, classifying the input into the relevant categories. To
audio feature data into a format suitable for a CNN [23]. It address hyperparameters such as convolution layers and filter
employs dimensionality reduction, using the t-SNE technique, sizes, we tuned these hyperparameters by applying Bayesian
to map each feature vector onto a 2D plane. This method optimization techniques to ensure the optimal performance.
retains the inherent structure and relationships within the
original high-dimensional dataset. The resultant 2D map is a
IV. RESULTS
spatialconfigurationofpoints,eachcorrespondingtoafeature Inthissection,weassesstheproposedmethodsthroughtwo
from the original data, with their positions indicative of their distinct evaluation configurations: 5-fold CV (sample-wide),
relationalstructure.Thenextphaseinvolvesdefiningtheimage whereindividualdatamaybepartofbothtrainingandtestsets,
frame.Usingaconvexhullalgorithm,arectangleisdrawnthat and leave-one-group-out CV (individual-specific), segregating
encompasses all the 2D points. Feature values that overlap the training and test sets into separate subject samples. These
are averaged, balancing information retention with hardware methods allow for a nuanced examination of the models‚Äô
constraints and the number of features. Finally, 2D Cartesian robustness and adaptability. Furthermore, we delve into the
coordinates of the features are translated and mapped into optimalaudiosensingwindowsltogaugetheefficacyofaudio
definedpixelpositions,producinganimagewheretheintensity signals in detecting the two types of social contexts.
of each pixel corresponds to the feature value, and the pixel‚Äôs
position represents the structural relationship of that feature A. Experiment Settings
to others. By enabling a CNN to capitalize on the structural In our research, we investigate the effectiveness of our
and relational information among audio features encapsulated multi-channelCNNarchitectureagainstseveralbaselinemod-
in an image format, the DeepInsight process enhances the els. These baselines include two distinct CNN architectures,
classification performance for social context determination. a Multi-layer Perceptron (MLP), a Random Forest, and a De-
cision Tree. The choice of these baseline models is grounded
C. Multi-Channel CNN-Based Classification
in their widespread acceptance and proven effectiveness in
Leveraging the strength of a multi-channel CNN to process audio classification and similar signal-processing tasks. The
concatenated grayscale images of the Spectrogram and the CNN architectures are known for handling image-like data
DeepInsight feature-embedding, we encompass perspectives structures, making them suitable for our spectrogram and
on both time series frequency changes and audio feature vari- DeepInsight-transformed feature matrix images, respectively.
ations.Despite being a single input image, our strategy treats TheCNNarchitecturesindependentlyprocessthespectrogram
each of the grayscale transformations as distinct channels, ef- and DeepInsight-transformed feature matrix images, each uti-
fectivelypreservingtheirindividualcharacteristicinformation. lizing sequential 2D convolutional layers and max-pooling
The decision to use grayscale over color is backed by the for spatial down-sampling. The MLP, Random Forest, and
assertionthattheessentialinformationinboththeSpectrogram Decision Tree can handle traditional quantitative feature data.
and the DeepInsight Feature2Image lies in pixel intensities Thesemodels,widelyusedinvariousclassificationtasks,offer
ratherthancolorvariations.OurCNNarchitectureisdesigned a robust baseline to evaluate the comparative advantage of
to handle a dual-channel input. The first layer is an input our proposed CNN approach. The baseline models are trained
layerconfiguredtoacceptthetwodistinctchannels.Following on quantitative audio features of each audio segment. Thisdiversity in baseline models allows for a comprehensive anal- TABLEII
ysis of the strengths and limitations of different algorithmic RESULTSOF5-FOLD-CROSSVALIDATIONEVALUATIONINDICATEDBY
BALANCEDACCURACYANDMACRO-WEIGHTEDF1-SCORE.MODELS
approaches under similar conditions.
WITH‚Äú+FEATURES‚ÄùUTILIZEAUDIOFEATURESASINPUT,WHILEMODELS
In contrast, our multi-channel CNN architecture combines WITH‚Äú+DEEPINSIGHT‚ÄùAND‚Äú+SPECTROGRAM‚ÄùARETRAINEDWITH
grayscale versions of the spectrogram and feature matrix THETWOREPRESENTATIONS,RESPECTIVELY.
images into a single input, processed in separate channels. By
NofPartners EvaluativeThreat
executingindependentconvolutionandpoolingstagesforeach Methods
Accuracy F1-score Accuracy F1-score
channel before flattening and merging the derived features,
RandomBaseline 0.50¬±0.00 0.50¬±0.00 0.50¬±0.00 0.50¬±0.00
thismodelcapturesthecomplementaryinformationfromboth
RandomForest+Features 0.83¬±0.01 0.84¬±0.02 0.80¬±0.01 0.79¬±0.01
frequency and audio feature domains. The overarching objec-
MLP+Features 0.81¬±0.02 0.80¬±0.01 0.76¬±0.01 0.75¬±0.01
tive is to ascertain whether this approach offers significant DecisionTree+Features 0.71¬±0.02 0.71¬±0.02 0.73¬±0.02 0.74¬±0.01
advantages over traditional audio classification methods. CNN+DeepInsight 0.74¬±0.02 0.76¬±0.01 0.74¬±0.01 0.73¬±0.02
CNN+Spectrogram 0.87¬±0.02 0.86¬±0.03 0.85¬±0.02 0.85¬±0.02
Toconductholisticvalidationresults,wetrainedmodelson
(Ours)Multi-ChannelCNN 0.90¬±0.02 0.86¬±0.03 0.83¬±0.02 0.87¬±0.01
samples segmented by the window length l. These models
were evaluated using both a 5-fold cross-validation (CV)
and a leave-one-group-out CV (LOGOCV), respectively. In TABLEIII
RESULTSOFLEAVE-ONE-GROUP-OUTCROSSVALIDATIONINDICATEDBY
the context of our study, the 5-fold CV spans the entire
BALANCEDACCURACYANDMACRO-WEIGHTEDF1-SCORE.
sample, meaning an individual‚Äôs data can appear in both
the training and test sets, showing a partially personalized NofPartners EvaluativeThreat
Methods
model assessment because the model is trained and evaluated Accuracy F1-score Accuracy F1-score
on different subsets of the same individuals‚Äô data, allowing MajorityBaseline 0.50¬±0.00 0.50¬±0.00 0.50¬±0.00 0.50¬±0.00
it to adapt to individual variances. On the other hand, the RandomForest+Feature 0.65¬±0.01 0.63¬±0.01 0.62¬±0.01 0.59¬±0.01
LOGOCV is more challenging, where the training and test MLP+Feature 0.68¬±0.01 0.70¬±0.01 0.64¬±0.02 0.64¬±0.02
DecisionTree+Feature 0.59¬±0.01 0.60¬±0.01 0.54¬±0.02 0.52¬±0.02
sets encompass distinct subject samples. To be more detailed,
CNN+DeepInsight 0.66¬±0.01 0.64¬±0.01 0.62¬±0.01 0.73¬±0.01
given our setup where participants were organized into Zoom CNN+Spectrogram 0.71¬±0.02 0.72¬±0.03 0.72¬±0.02 0.70¬±0.02
meetings with the group including four to six members (e.g., (Ours)Mutli-ChannelCNN 0.82¬±0.02 0.82¬±0.02 0.77¬±0.01 0.78¬±0.02
P01-P06), the LOGOCV approach implies that we train the
modelusingdatafromallgroupsexceptoneandthenvalidate
using the left-out group. This simulates a real-world scenario Given the evaluation metrics used, treating both target
wherethetrainedmodelisappliedtounseen,newindividuals. classes with equal weighted importance for each fold of the
The dual application of these CV methods provides multiple validationestablishesabaselineperformance(labeledas‚ÄúMa-
evaluations of the robustness and versatility of our models. jority Mean‚Äù in Table II) where always guessing the majority
Throughout the experiments, we utilized evaluation metrics class results in scores of 50% for both metrics. Traditional
such as balanced accuracy and macro F1-score. Under class- machine learning models such as Random Forest, MLP, and
imbalanced binary classifications, these metrics, by treating DecisionTree,whichuseaudiofeatures,yieldedstrongresults,
both target classes with equal weighted importance, retained indicating the potential utility of using audio features for the
a consistent baseline mean of 50% regardless of the class task. Among the CNN variants, the multi-channel approach,
imbalance. which was expected to perform more effectively than tradi-
tionalapproaches(seeExperimentSettings),stoodoutwithan
1
Balanced Accuracy= (recall+specificity) (1) accuracy score of 90.0% and an F1-score of 86% for Number
2
of Interaction Partners detection, and 83% accuracy and 87%
F1-score for Evaluative Threat detection, respectively, clearly
precision √órecall
F1 =2¬∑ class i class i outperforming the other approaches tested. This suggests that
class i precision +recall
class i class i leveraging both spectrogram and DeepInsight representations
1
Macro F1= ¬∑(F1 +F1 ) (2) concurrently within a CNN offers an advantage over the
2 class 1 class 2
individual use of these features or traditional feature-based
models.
B. Experiment Results
Leave-One-Group-Out
1) ModelEfficiency: Thissectionpresentstheperformance Table II presents the evaluation results using the LOGOCV
results obtained from the various models under two distinct setup. In this stringent test, the training and test sets comprise
evaluation setups: 5-fold CV and LOGO CV. completely separate participants, offering broader generaliz-
5-Fold Cross Validation ability of the model results.
TableIIshowcasestheresultsofmodelsevaluatedunderthe Similar to the 5-fold cross-validation results, traditional
5-fold CV, assessing the models based on Balanced Accuracy machine learning models fared reasonably well but were
and Macro-Averaged F1-score for two primary tasks: Number outperformed by CNN-based methods (83% accuracy and
of Interaction Partners and Evaluative Threat detection. 82% F1-score for Number of Interaction Partners and 77%5.5seconds.Thetaskofdetectingevaluativethreatsreachedits
optimal performance, with an accuracy of 77% for segments
lasting 5.0 seconds.
These results highlight the significance of fine-tuning the
audio segment length for optimal detection performance and
alsoillustratethedynamictrade-offsbetweenaudiolengthand
model performance of social context detection.
V. DISCUSSION
A. Technical Implications
Our study demonstrates strong model performance, partic-
ularly after integrating information from multiple channels,
including time series frequency changes and audio feature
variations. This multidimensional audio2image approach was
effective at capturing nuanced aspects of social interactions.
(a) Sample-wide5-foldCV Priorworkhasfoundthatusingphysiologicalcueslikeheart
rate variability and motion to detect social contexts yielded
accuracies of only 50% (suggesting limited effectiveness) in
social evaluation detection and 61% in distinguishing dyadic
fromgroupinteractions[4].Anotherlineofresearchleveraged
acoustic sensing to generally detect whether or not a face-
to-face social interaction was occurring, achieving 80.4%
accuracy [13]. The present work showcases the advantages
of acoustic sensing to more accurately detect more nuanced
social contexts.
Thediscrepanciesobservedbetweentheoutcomesofthe5-
fold CV and the LOGOCV are informative. In particular, the
LOGOCV poses a more challenging test, probing the model‚Äôs
generalizability across unique participant subsamples.
The efficacy of our audio-based methods raises intriguing
questions about its applicability to other social interaction
contexts (e.g., conversations between therapists and clients;
(b) Leave-one-group-outCV workspace interactions). It will be important to directly test
generalizability to other virtual and in-person interactions.
Fig. 3. Performance of the multi-channel CNN models trained by varying
Additionally,futureendeavorsmightcontemplatesupplement-
lengthlofaudiosegmentsunder5-FoldCV(a)andLOGOCV(b)paradigms.
(The error bars indicate the standard deviations of the performance of the ing our acoustic data with other sensed inputs, like visual or
modelstrainedfor10timesperpoint). physiological markers. At the same time, the model‚Äôs success
based on only short audio segments suggests an efficient
avenue for real-time detection and processing, which can
accuracy and 78% F1-score for Evaluative Threat detection,
be readily integrated into everyday edge devices like smart-
respectively). This underscores the robustness and versatility
watches and smartphones, and other digital communication
of our model, particularly when applied to unseen subjects.
platforms (e.g., Telehealth) to bolster their context-awareness.
2) Optimal Sensing Window: To understand the nuanced Theminimalaudiodataneeded,combinedwiththepossibility
relationship between the detection performance of social con- ofcustomizationtoaccountforindividualspeechnuancesand
texts and the segment length l of audio, we trained models on culturalvariations,suggestsadirectiontowardsapersonalized
audio segments with lengths ranging from 0.5 to 9 seconds. user experience with context-awareness fine-tuning.
The balanced accuracy served as the exclusive metric.
B. Social Impact and Applications
AsshowninFigure3(a),forsample-wide5-foldCV:When
distinguishingbetweendyadicandgroupcontexts,themodel‚Äôs With the ability to more precisely characterize social con-
accuracy started near the baseline of 50% with 0.5-second texts from audio segments, our tool has potentially important
audio. It showed a sharp increase, reaching its zenith at 90% implications for fields like clinical and social psychology.
for an audio length of 7.0 seconds. For identifying evaluative This advancement not only aligns with, but also extends
threats, the model‚Äôs accuracy peaked at 83% with a segment established intervention approaches, like cognitive behavior
length of 5.5 seconds. See Figure 3(b), regarding the LOGO therapy. By understanding the varying dynamics and contexts
CV: The differentiation of number of interaction partners of social interactions, therapists and digital health apps could
achieved its highest accuracy of 82% at a segment length of potentially tailor their interventions. For example, instead ofrelying on client‚Äôs retrospective self-report to understand how ParticipantDiversity:Thesamplepredominantlyconsisted
asocialinteractionwentforaclient,atherapistcoulddirectly of English-speaking, cisgender female undergraduate students
learn how turn-taking (i.e., balancing speaking and listening) aged 18-22 years. Future studies should include samples with
played out in a real-world dyadic interaction, and recommend a broader age range, cultural and language backgrounds, and
different approaches to enhance interpersonal effectiveness other demographic factors.
accordingly. In contrast, during group conversations, advice ImplementationandIntegration:Thepotentialextensions
mightcenterontacticsforenteringadiscussionwhenmultiple andimplicationsofourproposedmethodareconsiderable,par-
peopleareactivelyparticipating,orwaystonavigatemorevs. ticularly for integration into next-generation sensing systems
less evaluative interaction contexts. such as mobile-based digital health systems [15]. However,
Beyondimplicationsforsocialanxietyandthreatdetection, several questions about its practical application, real-time
distinguishing among small versus larger group interactions processing capabilities, and scalability across diverse settings
has many applications for studying group dynamics and for- persist. Implementing an audio-based assistant system brings
mation, social networks and hierarchies, etc. More generally, forward privacy and ethical considerations that are critical to
this approach contributes to a more nuanced understanding address.Toaddressthis,futureresearchcouldexploreincorpo-
of social interactions, raising new possible applications in ratingprivacy-preservingtechniquesandethicalprotocolsinto
the context of passive sensing and AI‚Äôs role in Just-in-Time social context detection systems. For instance, the proposed
Adaptive Interventions (JITAIs) [6]. methods that necessitate short audio segments with minimal
While our research suggests promising future applications speech content for computations could leverage edge devices
(assuming these results replicate and extend to other sam- for processing, without needing to gather data. The approach,
ples and social contexts), ethical and privacy concerns re- which involves extracting audio features at the edge device,
main paramount given the sensitive and identifying nature deletingtheaudiofilesthereafter,andcompletingthedetection
of audio data. The efficacy of detecting social contexts from task based on features, may also offer a viable solution while
transformed images of short audio segments (i.e., about 1-5 addressing privacy concerns.
seconds)underscoresapromisingtrade-offbetweenutilityand
VI. CONCLUSION
privacy.Thisshortdurationnotonlyminimizestheamountof
verbal content captured, thereby reducing privacy risks and Inthispaper,weproposedtodetectnuancedsocialcontexts
concerns related to verbal content leakage, but also allows for usingambientaudiosegments,highlightingtheefficacyofour
computationally low-cost deployment of the model on edge audio2image techniques in conjunction with CNNs. The ob-
devices, which ensures that raw audio data are neither stored tained accuracies, under both the 5-fold and leave-one-group-
nor transmitted beyond the device. By doing so, the privacy out CV scenarios, show promise for a meaningful advance
risks can be greatly minimized, but a critical next step will be in social interaction monitoring. While our data source, being
more qualitative work with participants to better understand Zoom-based due to the pandemic, presents some constraints,
their views toward monitoring of audio and the real-world weanticipate(butneedtotest)thatthemethodologyusedwill
conditions where ongoing monitoring would feel acceptable extend to a broad range of real-world settings. The promise
and non-intrusive. of passive sensing combined with AI models, as showcased
in our results, emphasizes the potential for accurate, real-time
social context detection.
C. Limitations and Future Work
While our study presents key advances, there are of course
ETHICALIMPACTSTATEMENT
also limitations which point to opportunities for future work: All study procedures were approved by the Institutional
Study Constraints and Settings: Our study was con- Review Board (UVA-SBS IRB #3004) of a U.S. university
ducted during the pandemic, which influenced our data col- and conducted under the supervision of a licensed clinical
lectionstrategiesandmethods.Specifically,datawascollected psychologist and researcher with expertise in anxiety disor-
throughZoomduetosocialdistancingrestrictions.Thisdigital ders. Participants provided informed consent and were made
environment, while highly relevant to current conditions, may fully aware of the study‚Äôs purpose, the nature of the data
not capture the full spectrum and nuances of real-world collection, and their rights within the research. Moreover, at
socialinteractions.Also,ourmeasuredsocialinteractionswere the conclusion of the study session, participants were asked
limited to experimentally designed, binary social contexts to confirm they consented for us to analyze their audio and
(dyadic vs. group and evaluative vs. not-explicitly-evaluative) video data.
on Zoom, but everyday interactions can vary on many more, Data Privacy: In this study, Zoom recording data was
complex, dynamic variables (e.g., whether the individual is collected using password-protected computers by research as-
familiar with the interaction partners). Moving forward, it sistantsandsecurelystoredonahigh-securitydataserveronly
will be valuable to assess in-person social interactions across accessible via high security virtual private network (VPN).
a wide variety of contexts and interaction partners (e.g., Importantly,theapproachtakeninthepresentstudyminimizes
using microphones in mobile devices, assuming appropriate the risk of data misuse while still enabling the identification
permissions have been obtained). of social contexts relevant to social anxiety by: 1) trainingdetection models that are easily deployable on ubiquitous [9] S. Vazire and M. R. Mehl, ‚ÄúKnowing me, knowing you: the accuracy
devices (like smartphones and smartwatches) to ensure that anduniquepredictivevalidityofself-ratingsandother-ratingsofdaily
behavior.‚ÄùJournalofpersonalityandsocialpsychology,vol.95,no.5,
rawaudiodataarenotstoredortransmittedbeyondthedevice,
p.1202,2008.
and 2) only requiring seconds-long audio. We advocate for [10] D. O. Olgu¬¥ƒ±n and A. S. Pentland, ‚ÄúSocial sensors for automatic data
transparency in any subsequent research or applications that collection,‚ÄùAMCIS2008Proceedings,p.171,2008.
[11] N. Eagle, A. Pentland, and D. Lazer, ‚ÄúInferring friendship network
keep the users fully aware of how data will be collected and
structure by using mobile phone data,‚Äù Proceedings of the national
used, and allow users to opt in or out freely. academyofsciences,vol.106,no.36,pp.15274‚Äì15278,2009.
Generalizability: From a technical perspective, this study [12] K. Katevas, K. Ha¬®nsel, R. Clegg, I. Leontiadis, H. Haddadi, and
L.Tokarchuk,‚ÄúFindingdoryinthecrowd:Detectingsocialinteractions
employed leave-one-group-out cross validation to evaluate
usingmulti-modalmobilesensing,‚ÄùinProceedingsofthe1stWorkshop
the trained model‚Äôs generalizability to unseen individuals. onMachineLearningonEdgeinSensorSystems,ser.SenSys-ML2019.
However,wealsoacknowledgethatthepresentfindings,while New York, NY, USA: Association for Computing Machinery, 2019, p.
37‚Äì42.[Online].Available:https://doi.org/10.1145/3362743.3362959
promising, are based on data collected from college students
[13] D.Liang,Z.Xu,Y.Chen,R.Adaimi,D.Harwath,andE.Thomaz,‚ÄúA
incontrolled,virtualinteractions,andtheirapplicationtomore dataset for foreground speech analysis with smartwatches in everyday
diverse samples and real-world settings requires further vali- homeenvironments,‚Äùin2023IEEEInternationalConferenceonAcous-
tics, Speech, and Signal Processing Workshops (ICASSPW), 2023, pp.
dation.Weregardthisstudyasapreliminary,proof-of-concept
1‚Äì5.
stepinanimportantsocialcontext(givencollegestudentshave [14] A. Salekin, J. W. Eberle, J. J. Glenn, B. A. Teachman, and J. A.
high rates of social anxiety, and virtual environments have Stankovic,‚ÄúAweaklysupervisedlearningframeworkfordetectingsocial
anxietyanddepression,‚ÄùProceedingsoftheACMoninteractive,mobile,
become an integral aspect of daily life for many in the post-
wearableandubiquitoustechnologies,vol.2,no.2,pp.1‚Äì26,2018.
COVID world). It will also be important to conduct research [15] Z. Wang, H. Xiong, J. Zhang, S. Yang, M. Boukhechba, D. Zhang,
with people who hold different identities and are engaged in L.E.Barnes,andD.Dou,‚ÄúFrompersonalizedmedicinetopopulation
health:asurveyofmhealthsensingtechniques,‚ÄùIEEEInternetofThings
differentcontextstodeterminegeneralizabilityoftheseresults.
Journal,vol.9,no.17,pp.15413‚Äì15434,2022.
Overall, this work contributes to the broader field of af- [16] P. Rota, N. Conci, N. Sebe, and J. M. Rehg, ‚ÄúReal-life violent social
fective computing research by offering new insights into how interactiondetection,‚Äùin2015IEEEinternationalconferenceonimage
processing(ICIP). IEEE,2015,pp.3456‚Äì3460.
social contexts can be detected and understood through non-
[17] Z. Wang, M. A. Larrazabal, M. Rucker, E. R. Toner, K. E. Daniel,
intrusive ambient audio sensing. We envision that this work S. Kumar, M. Boukhechba, B. A. Teachman, and L. E. Barnes,
canpavethewayforpracticaltools(e.g.,context-awareJITAI ‚ÄúDetecting social contexts from mobile sensing indicators in virtual
interactions with socially anxious individuals,‚Äù Proc. ACM Interact.
systems that can sense when and where individuals require
Mob. Wearable Ubiquitous Technol., vol. 7, no. 3, sep 2023. [Online].
support most and provide personalized interventions tailored Available:https://doi.org/10.1145/3610916
to their specific contexts) that improve the lives of those [18] P.Francesca,S.M.Weldon,andA.Lomi,‚ÄúLostintranslation:Collect-
ing and coding data on social relations from audio-visual recordings,‚Äù
with social anxiety, while adhering to ethical and data privacy
SocialNetworks,vol.69,pp.102‚Äì112,2022.
standards. [19] T. Feng, A. Nadarajan, C. Vaz, B. Booth, and S. Narayanan, ‚ÄúTiles
audiorecorder:anunobtrusivewearablesolutiontotrackaudioactivity,‚Äù
REFERENCES in Proceedings of the 4th ACM Workshop on Wearable Systems and
Applications,2018,pp.33‚Äì38.
[1] Y. Liao, S. Intille, J. Wolch, M. A. Pentz, and G. F. Dunton, ‚ÄúUn- [20] N.D.Lane,P.Georgiev,andL.Qendro,‚ÄúDeepear:robustsmartphone
derstanding the physical and social contexts of children‚Äôs nonschool audiosensinginunconstrainedacousticenvironmentsusingdeeplearn-
sedentarybehavior:anecologicalmomentaryassessmentstudy,‚ÄùJournal ing,‚ÄùinProceedingsofthe2015ACMinternationaljointconferenceon
ofPhysicalActivityandHealth,vol.11,no.3,pp.588‚Äì595,2014. pervasiveandubiquitouscomputing,2015,pp.283‚Äì294.
[2] K.T.Phillips,M.M.Phillips,T.L.Lalonde,andM.A.Prince,‚ÄúDoes [21] D. Liang, A. Zhang, and E. Thomaz, ‚ÄúAutomated face-to-face
social context matter? an ecological momentary assessment study of conversation detection on a commodity smartwatch with acoustic
marijuana use among college students,‚Äù Addictive Behaviors, vol. 83, sensing,‚Äù Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,
pp.154‚Äì159,2018. vol. 7, no. 3, sep 2023. [Online]. Available: https://doi.org/10.1145/
[3] M. Asher, S. G. Hofmann, and I. M. Aderka, ‚ÄúI‚Äôm not feeling it: 3610882
Momentaryexperientialavoidanceandsocialanxietyamongindividuals [22] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen,
withsocialanxietydisorder,‚ÄùBehaviortherapy,vol.52,no.1,pp.183‚Äì R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al.,
194,2021. ‚ÄúCnn architectures for large-scale audio classification,‚Äù in 2017 ieee
[4] E. R. Toner, M. Rucker, Z. Wang, M. A. Larrazabal, L. Cai, international conference on acoustics, speech and signal processing
D. Datta, E. Thompson, H. Lone, M. Boukhechba, B. A. Teachman (icassp). IEEE,2017,pp.131‚Äì135.
et al., ‚ÄúWearable sensor-based multimodal physiological responses of [23] A.Sharma,E.Vans,D.Shigemizu,K.A.Boroevich,andT.Tsunoda,
socially anxious individuals across social contexts,‚Äù arXiv preprint ‚ÄúDeepinsight:Amethodologytotransformanon-imagedatatoanimage
arXiv:2304.01293,2023. for convolution neural network architecture,‚Äù Scientific reports, vol. 9,
[5] P.JefferiesandM.Ungar,‚ÄúSocialanxietyinyoungpeople:Aprevalence no.1,p.11399,2019.
studyinsevencountries,‚ÄùPloSone,vol.15,no.9,p.e0239133,2020. [24] F.Eyben,F.Weninger,F.Gross,andB.Schuller,‚ÄúRecentdevelopments
[6] I.Nahum-Shani,S.N.Smith,B.J.Spring,L.M.Collins,K.Witkiewitz, inopensmile,themunichopen-sourcemultimediafeatureextractor,‚Äùin
A.Tewari,andS.A.Murphy,‚ÄúJust-in-timeadaptiveinterventions(jitais) Proceedings of the 21st ACM international conference on Multimedia,
in mobile health: key components and design principles for ongoing 2013,pp.835‚Äì838.
healthbehaviorsupport,‚ÄùAnnalsofBehavioralMedicine,vol.52,no.6, [25] R.P.MattickandJ.C.Clarke,‚ÄúDevelopmentandvalidationofmeasures
pp.446‚Äì462,2018. ofsocialphobiascrutinyfearandsocialinteractionanxiety,‚ÄùBehaviour
[7] R. F. Baumeister, K. D. Vohs, and D. C. Funder, ‚ÄúPsychology as the
researchandtherapy,vol.36,no.4,pp.455‚Äì470,1998.
science of self-reports and finger movements: Whatever happened to [26] Y. Gong, Y.-A. Chung, and J. Glass, ‚ÄúAst: Audio spectrogram trans-
actual behavior?‚Äù Perspectives on psychological science, vol. 2, no. 4, former,‚ÄùarXivpreprintarXiv:2104.01778,2021.
pp.396‚Äì403,2007.
[8] S.Shiffman,A. A.Stone,andM.R. Hufford,‚ÄúEcologicalmomentary
assessment,‚ÄùAnnu.Rev.Clin.Psychol.,vol.4,pp.1‚Äì32,2008.