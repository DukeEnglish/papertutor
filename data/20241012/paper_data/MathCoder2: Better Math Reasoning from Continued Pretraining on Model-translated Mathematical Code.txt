MATHCODER2: BETTER MATH REASONING FROM
CONTINUED PRETRAINING ON MODEL-TRANSLATED
MATHEMATICAL CODE
ZimuLu‚àó,AojunZhou‚àó,HouxingRen,KeWang,WeikangShi
JuntingPan,MingjieZhan‚Ä†,HongshengLi‚Ä†
MultimediaLaboratory(MMLab),TheChineseUniversityofHongKong
luzimu@mail.ustc.edu.cn {aojunzhou, zmjdll}@gmail.com
hsli@ee.cuhk.edu.hk
ABSTRACT
Code has been shown to be effective in enhancing the mathematical reasoning
abilities of large language models due to its precision and accuracy. Previous
works involving continued mathematical pretraining often include code that uti-
lizesmath-relatedpackages,whichareprimarilydesignedforfieldssuchasengi-
neering,machinelearning,signalprocessing,ormoduletesting,ratherthanbeing
directly focused on mathematical reasoning. In this paper, we introduce a novel
method for generating mathematical code accompanied with corresponding rea-
soning steps for continued pretraining. Our approach begins with the construc-
tion of a high-quality mathematical continued pretraining dataset by incorporat-
ing math-related web data, code using mathematical packages, math textbooks,
and synthetic data. Next, we construct reasoning steps by extracting LaTeX ex-
pressions, the conditions needed for the expressions, and the results of the ex-
pressions from the previously collected dataset. Based on this extracted infor-
mation, we generate corresponding code to accurately capture the mathematical
reasoning process. Appending the generated code to each reasoning step results
indataconsistingofpairednaturallanguagereasoningstepsandtheircorrespond-
ing code. Combining this data with the original dataset results in a 19.2B-token
high-performing mathematical pretraining corpus, which we name MathCode-
Pile.Trainingseveralpopularbasemodelswiththiscorpussignificantlyimproves
theirmathematicalabilities,leadingtothecreationoftheMathCoder2familyof
models. All of our data processing and training code is open-sourced, ensur-
ing full transparency and easy reproducibility of the entire data collection and
trainingpipeline. Thecodeisreleasedathttps://github.com/mathllm/
MathCoder2.
1 INTRODUCTION
Various studies (Azerbayev et al., 2024; Shao et al., 2024) have shown that training on code en-
hancesthemathematicalreasoningabilitiesoflargelanguagemodels(LLMs). Previousresearchin
continued mathematical pretraining often includes code that utilizes math-related packages (Azer-
bayevetal.,2024). ThiscodeistypicallysourcedfromGitHubandisprimarilydesignedforfields
such as engineering, machine learning, signal processing, or module testing, rather than focusing
directly on mathematics. Recent models (Zhou et al., 2024; Yang et al., 2024b; Ying et al., 2024;
Shaoetal.,2024;Wangetal.,2023a)haveadoptedTool-IntegratedReasoning(TIR)infine-tuning.
They utilize integrated natural language reasoning steps and Python code to improve performance
onmathematicalreasoningtasks. Reasoningwiththehelpofcodeisparticularlyeffectiveformore
challengingproblems,likelyduetoitsprecisionandaccuracy.
‚àóEqualcontribution ‚Ä†Correspondingauthor
1
4202
tcO
01
]LC.sc[
1v69180.0142:viXraAlthoughutilizingexistingopen-sourcecodeinthepretrainingphasecanenhancethemathematical
reasoningabilitiesofLLMs,suchcodeoftenlacksaccompanyingnaturallanguageexplanationsor
context. Thismighthinderthemodel‚Äôsabilitytofullyunderstandthem. Inthispaper,wepropose
anovelmethodforgeneratinglargeamountsofmathematicalcodeaccompaniedbycorresponding
naturallanguagereasoningsteps,whichareextractedfrommath-relatedpretrainingtexts.Different
fromtheexistingmath-relatedcode, ourgeneratedcodeispairedwithnaturallanguagereasoning
steps,makingthecodemorecomprehensible. Also,asourcodeisgeneratedbasedonmath-related
texts, they are all highly related to mathematical reasoning. When used in pretraining, the mathe-
maticalcodepairedwithreasoningstepsfacilitatesLLMs‚Äôunderstandingofmath-relatedpretraining
texts, as it effectively captures the underlying reasoning process. Furthermore, this data enhances
themodel‚ÄôspotentialtobefinetunedforTIRreasoning.
Ourdataprocessingpipelineconsistsoftwokeysteps: (1)carefullycuratingarobustbasicdataset
forpretraining,and(2)generatingpairedreasoningstepsandmathematicalcodebyextractingLa-
TeXexpressionsandtheircontext,translatingtheextractedinformationintoPythoncodesnippets,
executingthegeneratedcodesnippets,andverifyingtheircorrectness.
First,wegatherandcarefullyfilterawidevarietyofmath-relateddatasources,includingwebpages,
model-generateddata,math-relatedcode,andtextbooks. Throughanadvancedfilteringprocess,we
ensurethedatasetisbothlargeandhighlyrelevant,minimizingirrelevantcontentwhilepreserving
the mathematical texts necessary for training. This results in a 16.5B-token dataset that forms the
foundationofourpretrainingefforts.Byconductingexperimentswithsmallermodels,weshowthat
thiscarefulcurationleadstomoreefficienttrainingwithoutsacrificingmodelperformance.
Second, we propose a novel method for generating large amounts of paired mathematical reason-
ing steps and their corresponding Python code. Given a piece of text from the pretraining corpus
collectedabove,wewrapitinacarefullydesignedpromptthatinstructsaLlama-3.1-70B-Instruct
modeltoextractLaTeXexpressionsalongwiththeirrelevantcontext, includingtheconditionsfor
eachexpressionandtheresultofitscomputation. Thisresultsinalistofcomprehensivemathemati-
calreasoningsteps,completewiththenecessaryconditions,thecomputationstaken,andtheresults.
Then,wepromptthemodeltotranslateeachreasoningstepintoaPythoncodesnippetthatcaptures
theunderlyingreasoningprocess. ThegeneratedPythonsnippetsareexecuted,andonlythosethat
run successfully and produce outputs matching the expected results are retained. By pairing the
codewiththecorrespondingreasoningstep, wecreatethefinaldata. Theprocessisdemonstrated
inthelowerhalfofFig.1. Thisprocessyieldsa2.7B-tokencorpusofmathematicalcodesnippets
accompaniedwiththeircorrespondingreasoningsteps,whichwecombinewiththedatagenerated
inthefirststeptocreatea19.2B-tokenpretrainingdataset,namedMathCode-Pile.
We validate the effectiveness of MathCode-Pile on four popular base models: Llama-3-8B,
DeepSeekMath-7B,Mistral-7B,andCode-Llama-7B,significantlyimprovingtheirperformanceon
five representative mathematical benchmarks. We name the resulting family of pretrained mod-
els MathCoder2. In particular, MathCoder2-Llama-3-8B achieves 4-shot accuracies of 38.4% on
MATHand69.9%onGSM8K,outperformingthebaselineoftrainingonlyonthebasicdatagener-
atedinthefirststepby3.1%and4.1%,respectively.Thisdemonstratesthatthedataofmathematical
codeaccompaniedwithreasoningstepseffectivelyenhancesLLMs‚Äôreasoningabilities.
Different from recent works, such as DeepSeekMath (Shao et al., 2024), InternLM-Math (Ying
etal.,2024), andQwen2.5-Math(Yangetal.,2024b), whichonlyreleasetheirmodelweights, we
offeradetailed,open-sourceframeworkfordataprocessingandtrainingthatachievesperformance
competitivewiththesemodels,fosteringfurtherprogressinmathematicalreasoningforLLMs.
Ourcontributionsinclude:
‚Ä¢ Anovelandeffectivemethodforgeneratinglargeamountsofmathematicalcodewithcor-
respondingnaturallanguagereasoningsteps,significantlyenhancingpretrainingoutcomes.
‚Ä¢ The creation of MathCode-Pile, a meticulously curated 19.2B-token dataset for contin-
uedmathematicalpretraining. Thisdatasetincludesmath-relatedwebdata,syntheticdata,
code,textbooks,andmodel-translatedmathematicalcode.
‚Ä¢ Full open-sourcing of all data processing and training code, ensuring transparency and
reproducibilitytosupportfutureresearch.
2(a) Other Methods
Lemma
Common Rule-based Math-related Deepseek math
Crawl or Models Texts
‚Ä¶
(b) MathCoder 2.0
Common Initial Finer Filtered
Crawl fastText F Ti elt xe tr sed Math fastText Math Texts
Finer Filtered Math Texts: Conditions Needed:
1.The integral is taken over a 3D region defined by the limits of integration.
‚Ä¶‚Ä¶ 2.The integrand is a function of three variables: ùúå,‚àÖ, and ùúÉ
Computation Expression:
ùë• ùë¶= =ùëü ùëüs si in n( (‚àÖ ‚àÖ) )c so ins (( ùúÉùúÉ )) 8‡∂±ùúã 2‡∂±ùúã 2‡∂±1
ùúå2sin‚àÖùëëùúåùëëùúÉùëë‚àÖ
ùëß=ùëücos(‚àÖ) Computation Result: 0 0 0
To calculate the volume: Translation 4ùúã
ùë£ùëúùëô LLM 3 Execute Code and
=8‡∂±ùúã 2 ‡∂±ùúã 2 ‡∂±1 ùúå2sin‚àÖ ùëëùúåùëëùúÉùëë‚àÖ P iy mt ph oo rn t C no ud me p ySn ai spp ne pt: Compare with Result
0 0 0 fromscipy.integrateimporttplquad
4ùúã
= 3 defi rn et te ug rr nan rd h( or *h *2o, *th ne pt .a s, i np (h pi hi) ):
‚Ä¶‚Ä¶ result, _ =tplquad(integrand, 0, np.pi/2, lambdaphi: 0, lambda Interleaved
p ph ri in: t n (p r. ep si u/ lt2 , * l 8a )m bd #a p mh ui l, t it ph le yt a b: y 0 8, tl oa m mb ad ta chp h ti he, ot rh ie gt ia n: a l1 ) Reasoning Steps
expression and Code Snippets
Figure1: Thedataprocessingpipeline. (a)showsthepipelineofpriorworks. (b)demonstratesour
method. WefirstuseafastTextclassifiertofiltertheCommonCrawlcorpus,resultingintheinitial
filtered math texts. Then, we annotate part of the filtered texts to train a new fastText classifier,
andconductasecondfiltering,resultinginthefinerfilteredmathtexts. Thenweuseaninstruction-
tunedLLMtoextractreasoningstepsfromthesemath-relatedtexts,andtranslatethereasoningsteps
into corresponding code snippets. We execute the code snippets and compare the output with the
expectedresult. Ifthecodeexecutessuccessfullyandtheresultisasexpected,thecodeisretained.
2 CURATION OF MATHCODE-PILE
We curate our mathematical pretraining dataset, MathCode-Pile, in two steps: first, we collect the
basicdatainSec.2.1, andthenweuseittogeneratemathematicalcodesnippetswiththeircorre-
spondingnaturallanguagereasoningstepsinSec.2.2.
2.1 BASICDATA
Wecollectandcarefullyfilteradiverserangeofmathematicaldatatoensurerelevanceandquality
forcontinuedpretrainingofLLMs.Thedataincludesmath-relatedwebcontent,syntheticdata,code
utilizingmathematicalpackages,andmathematicaltextbooks.
Math-related Web Data. Web data offers a broad range of real-world mathematical examples.
We start with the OpenWebMath (Paster et al., 2023) dataset, which contains mathematical web
pages sourced from Common Crawl. Observing that a significant portion of these documents are
unrelated to mathematics, we instruct the Mixtral-8x7B-Instruct model with a carefully designed
prompt(detailedinAppendixA)tofilteroutirrelevanttexts. Examplesofirrelevanttextsareshown
in Appendix D. This reduces the dataset from 13.7B tokens to 4.8B tokens (measured using the
Llama-3tokenizer). Wecallthisfilteredversionfiltered-OpenWebMath.
To further expand the dataset, we train a fastText classifier (Joulin et al., 2016) using filtered-
OpenWebMathaspositivesamplesandrandomCommonCrawldataasnegativesamples(training
detailsareexplainedinAppendix.B).Thismodelhelpsidentifyadditionalmath-relateddocuments
withintheCommonCrawldatafromMatrix(Zhangetal.,2024), ageneralpretrainingdataset. A
second round of filtering is performed, where Mixtral-8x7B-Instruct annotates a portion of these
documents,andanewfastTextclassifiertrainedbasedontheseannotationsfurtherrefinesthedata.
This produces 6.4B tokens, which we label as filtered-CC-En-math. Finally, we combine filtered-
OpenWebMath and filtered-CC-En-math, resulting in a comprehensive 11.2B-token math-related
webdataset.
3Prompt: You will be presented with a text related to math. I need you to identify all the complex
computationsinit.Foreachcomplexcomputation,findouttheconditionsneededforthecomputation,
theLaTeXexpressionthatconductsthecomputation,andtheresultofthecomputation.Thengenerate
aPythoncodesnippetforeachcomputationthatdemonstrateshowtheresultisreached. Outputeach
computationinthefollowingformat:
ConditionsNeeded:
1.[Condition1]
2.[Condition2]
...
ComputationExpression:
$[LaTeXExpression]$
ComputationResult:
[ComputationResult]
PythonCodeSnippet:
‚Äò‚Äò‚Äòpython
[PythonCode]
‚Äò‚Äò‚Äò
There can be more than one complex computation in the text. Output only the computations that
requirescalculation. Donotincludemathematicalstatementsordefinitionsasacomputation. Make
sureeachsnippetcanbeexecutedindividually.Thetextisasfollows:{TEXT}
Thecomputationsare:
Figure2:Thepromptforextractingreasoningstepsfromtextsinthepretrainingcorpusandgenerat-
ingthecorrespondingPythonsnippets. {TEXT}isreplacedwiththetextfromthedatasetcollected
inSec.2.1.
SyntheticData. Syntheticdataoffersstructuredmathematicaltextsthatcomplementthewebdata.
Wecollectsyntheticdatafromvariousopen-sourcerepositoriesonHuggingFace,includingdatasets
like Education-College-Students1, Maths-College2, and synthetic math books from Matrix (Zhang
et al., 2024). To ensure relevance, we apply a fastText classifier to filter out non-mathematical
documents,refiningthedatasetto2.2Btokensofhigh-qualitysyntheticmathcontent.
CodeUtilizingMathematicalPackages.Codedataofferspracticalexamplesofhowmathematical
librariesareusedinprogramming. WecollectcodefromPythonandJupyterfileswithintheStar-
CoderDatadataset(Lietal.,2023),retainingonlyprogramsthatimportmath-relatedpackagessuch
assympy,fractions,cmath,scipy,orstatistics. Thewidelyusednumpypackageisnotusedtofilter
thedocuments,asitappearsfrequentlyinnon-mathematicalcontexts. Afterfiltering,thiscollection
processresultsin1.7Btokensofcoderelatedtomathematicalcomputations.
MathematicalTextbooks.Textbooksprovideformal,structuredpresentationsofmathematicalcon-
cepts, makingthemavaluablesourceofmathknowledge. Wegather8KPDFsoftextbooksfrom
onlineresourcesbyidentifyingthosewithtitlescontainingmath-relatedkeywordssuchasalgebra,
geometry, probability, etc. These PDF files are then converted into markdown format using the
Nougattoolforeasierintegrationintoourtrainingpipeline.
2.2 MODEL-TRANSLATEDMATHEMATICALCODE
Inthissection,weproposeanovelapproachforextractingreasoningstepsfromthebasicpretrain-
ingdataandtranslatingthemintocorrespondingPythoncodesnippetsthatcapturetheunderlying
reasoningprocesses. Thisextractionandtranslationprocessisperformedusingastronginstruction-
tunedmodel,whichisLlama-3.1-70B-Instructinthispaper.
1https://huggingface.co/datasets/ajibawa-2023/Education-College-Students
2https://huggingface.co/datasets/ajibawa-2023/Maths-College
4Table1: ThecomponentsanddatastatisticsofMathCode-Pile.
Components Size(MB) Documents Tokens Average(Tokens)
Filtered-OpenWebMath 16,999 2,824,705 4,826,902,621 1,709
Filtered-CC-En-math 23,465 7,597,718 6,341,745,645 835
Syntheticdata 8,855 2,195,974 2,193,189,314 999
Codeusingmathpackages 6,120 513,059 1,703,226,005 3,320
Mathematicaltextbooks 4,431 8,373 1,390,268,773 166,042
Translatedmathematicalcode 8,235 6,347,823 2,728,740,985 430
Total 68,105 19,487,652 19,184,073,343 984
Ourmethodbeginswithtakingapieceoftextfromthebasicpretrainingdataandwrappingitina
carefully designed prompt, as shown in Fig. 2. This prompt instructs the model to identify LaTeX
expressionsdenotingcomplexcomputations,alongwiththenecessarycontext,includingthecondi-
tionsrequiredforthecomputationandtheexpectedresult. Byexplicitlyextractingtheconditionsof
theLaTeXexpression,weenhancethemodel‚Äôsabilitytocomprehendtheunderlyingmathematical
reasoningbehindtheusageoftheexpression.Theexpectedresultofthecomputationcanlaterserve
asabasisforverifyingthecorrectnessofthegeneratedcode. Amathematicalreasoningstepiscon-
structedbycombiningtheconditions,expressionandresult. Thepromptthendirectsthemodelto
produceaPythoncodesnippetthataccuratelyreflectstheunderlyingreasoningprocessbehindthe
extracted reasoning step. The model is asked to present the conditions, LaTeX expression, result,
andPythoncodesnippetinastructuredformat,ensuringthateachpartcanbeeasilyextractedfrom
thegeneratedtext. ExamplesofgeneratedtextsareshowninAppendixC.
After the Python code snippets are generated, they are executed, and outputs of the execution are
comparedwiththeexpectedresultsextractedfromthegeneratedtext.OnlythePythoncodesnippets
thatexecutewithouterrorsandproducecorrectoutputsareretained. Thisfilteringprocessensures
a higher quality of generated code, making the resulting dataset more reliable for mathematical
pretrainingcomparedtoapproachesthatrelyonunverifiedandgeneral-purposecode.
LeveragingtheLlama-3.1-70B-Instructmodel,weinitiallygenerated3.1Btokensofthedata. After
applying the filtering process, we obtain a total of 2.7B tokens of high-quality data of mathemati-
calcodesnippetsaccompaniedwiththeircorrespondingreasoningsteps. Thisnewlygenerateddata
significantlyenrichesouroriginalpretrainingcorpus.Bycombiningthisdatawiththebasicpretrain-
ingdata,wecreateacomprehensivepretrainingdatasettotaling19.2Btokens,whichwerefertoas
MathCode-Pile. Detailed statistics of MathCode-Pile are presented in Tab. 1. This dataset is tai-
loredspecificallyforenhancingthemathematicalandcodingabilitiesofLLMs.Toavoidbenchmark
contamination,wefollowYangetal.(2024b)tofilteroutsamplesthathavesignificantoverlapswith
any of the questions from benchmark datasets used in evaluation. We use exact match to remove
the identical samples and further apply 13-gram deduplication (with a condition that the Jaccard
similarityshouldbelargerthan0.6)toremovemoresamplesthatmightcausecontamination.
Incomparisontotraditionalmethodsofcuratingmath-relatedcode, whichoftendrawongeneral-
purpose repositories, our method ensures that the code is not only syntactically correct but also
mathematically sound, reflecting a deeper understanding of mathematical reasoning. Our math-
ematical code is accompanied with corresponding natural language reasoning steps, which makes
understandingthereasoningprocesseasier.ThismakesMathCode-Pileasuperiorresourceformod-
elsaimedatperformingadvancedmathematicalreasoningtasks.
3 EXPERIMENTS
Todemonstratetheeffectivenessofourmethod,wefirsttrainseveralbasemodelsrangingfrom7B
to 8B parameters using MathCode-Pile and compare them to other best-performing models of the
same size. The group of models resulting from the continued mathematical pretraining is named
MathCoder2. Next, we train and compare various other open-source math pretraining datasets
againstMathCode-Pileusingasmallermodel,DeepSeekCoder-1.3B.Toshowcasethepotentialof
5Table 2: Performance of various pretrained models on five representative mathematical datasets.
Allresultsreportedarebasedongreedydecoding. ‚ÄúCode-open‚Äùshowswhetherthecodefordata-
processingandmodel-trainingisopen-sourced. Therednumbersshowtheimprovementscompared
tothebasemodelfromwhicheachMathCoder2modelistrained.
Model Size Code- MATH GSM8K SAT OCW MMLU-
open MATH
Qwen2-Math 7B ‚úó 50.4 80.4 87.5 14.0 57.9
Qwen2.5-Math 7B ‚úó 55.4 91.6 - - -
InternLM2.5 7B ‚úó 34.0 74.8 65.6 8.1 49.6
InternLM2-Math-Base 7B ‚úó 21.5 49.2 - - -
Llemma 7B ‚úì 18.0 36.4 53.1 7.7 -
Llama-2 7B ‚úó 3.2 11.8 25.0 3.7 -
Llama-3 8B ‚úó 21.4 54.8 56.3 10.3 42.8
MathCoder2-Llama-3 8B ‚úì 38.4(+17.0) 69.9(+15.1) 84.4(+28.1) 18.0(+7.7) 46.5(+3.7)
DeepSeekMath 7B ‚úó 36.2 64.2 84.4 15.4 47.4
MathCoder2-DeepSeekMath 7B ‚úì 38.6(+2.4) 68.8(+4.6) 90.6(+6.2) 16.9(+1.5) 48.3(+0.9)
Mistral 7B ‚úó 13.1 52.2 75.0 8.5 38.3
MathCoder2-Mistral 7B ‚úì 36.7(+23.6) 68.2(+16.0) 81.3(+6.3) 13.2(+4.7) 42.2(+3.9)
Code-Llama 7B ‚úó 6.7 14.6 25.0 3.7 26.4
MathCoder2-Code-Llama 7B ‚úì 28.8(+22.1) 52.3(+37.7) 71.9(+46.9) 8.5(+4.8) 33.7(+7.3)
the MathCoder2 models, we further perform supervised fine-tuning on them. Finally, we conduct
ablationstudiestoanalyzetheimpactofeachcomponentofthedataset.
3.1 MAINRESULTS
Benchmark datasets. We evaluate the MathCoder2 models on five representative datasets:
GSM8K(Cobbeetal.,2021),MATH(Hendrycksetal.,2021b),SAT-Math(Azerbayevetal.,2024),
OCW(Lewkowyczetal.,2022),andMMLU-Math(Hendrycksetal.,2021a). GSM8KandMATH
aretestedusinga4-shotpromptwithMAmmoTH‚Äôsevaluationframework (Yueetal.,2023). SAT-
MathandOCWaretestedusinga4-shotpromptwithDeepSeekMath‚Äôsevaluationframework(Shao
et al., 2024). MMLU-Math is tested using the lm-evaluation-harness‚Äôs (Gao et al., 2024) default
zero-shot setting for MMLU. These datasets cover a wide range of mathematical problems across
varioustypesanddifficultylevels,fromprimaryschoolmathwordproblemstocollege-levelchal-
lenges,providingacomprehensiveevaluationofthemodels.
Basemodelsandtrainingsettings. Todemonstratethatourpretrainingcorpusiseffectiveacross
different base models, we continue pretraining four base models with MathCode-Pile: Llama-3-
8B (Dubey et al., 2024), DeepSeekMath-7B (Shao et al., 2024), Mistral-7B (Jiang et al., 2023),
andCode-Llama-7B(Rozie`reetal.,2024). MathCoder2-Llama-3-8Bistrainedfor3epochswitha
globalbatchsizeof4milliontokensandan8192tokencontextlength.MathCoder2-DeepSeekMath,
MathCoder2-Mistral,andMathCoder2-CodeLlamaareeachtrainedfor3epochswithaglobalbatch
sizeof4milliontokensanda4096tokencontextlength.
Baselines. Wecompareourmethodwithvariousotherbasemodelsthatpossessstrongmathemat-
ical abilities and are of similar sizes, including Qwen2-Math 7B (Yang et al., 2024a), Qwen2.5-
Math7B(Yangetal.,2024b),InternLM2-Math-Base7B(Yingetal.,2024),InternLM2.57B(Cai
etal.,2024), DeepSeekMath7B(Shaoetal.,2024), Llemma7B(Azerbayevetal.,2024), Mistral
7B (Jiang et al., 2023), Llama2 7B (Touvron et al., 2023), Llama3 8B (Dubey et al., 2024) and
Code-Llama7B(Rozie`reetal.,2024).
Results: AsdemonstratedinTab.2,continuedpretrainingonMathCode-Pileconsistentlyimproves
performanceacrossallfivebenchmarkdatasets. MathCoder2modelsrivaltheperformanceoftop
modelslikeInternLM2-Math-Base,InternLM2.5,andDeepSeekMath. Inparticular, MathCoder2-
DeepSeekMathdemonstratesthatourmethodcontinuestoenhancetheperformanceofDeepSeek-
Math,amodelthathasalreadybeenextensivelytrainedonlargeamountsofmath-relateddata.How-
6Table3: Performanceofvariousfinetunedmodelsonfiverepresentativemathematicaldatasets. All
resultsreportedarebasedongreedydecoding.
Model Size MATH GSM8K OCW Olympiad SVAMP
Bench
Qwen2-Math-Instruct 7B 75.1 89.9 34.6 38.2 -
Qwen2.5-Math-Instruct 7B 83.6 95.2 37.1 41.6 -
DeepSeekMath-Instruct-CoT 7B 46.8 82.9 - - -
DeepSeekMath-Instruct-TIR 7B 57.4 83.7 - - -
InternLM2-math-plus 7B 54.4 84.0 17.3 18.8 -
NuminaMath-7B-CoT 7B 55.2 75.4 19.1 19.9 -
NuminaMath-7B-TIR 7B 68.1 84.6 - - -
ToRA-Code 7B 44.6 72.6 - - 70.4
MathCoder 7B 30.2 67.8 - - 70.7
MAmmoTH2-Plus 8B 42.8 84.1 - - -
Llama-3.1-Instruct 8B 47.2 76.6 21.7 15.4 -
MathCoder2-Llama-3-Instruct-CoT 8B 58.5 83.9 29.4 25.8 92.7
MathCoder2-Llama-3-Instruct-TIR 8B 69.7 85.8 37.6 37.6 94.9
MathCoder2-DeepSeekMath-Instruct-CoT 7B 55.2 80.3 30.9 23.0 92.1
MathCoder2-DeepSeekMath-Instruct-TIR 7B 69.6 86.5 41.9 37.9 92.8
ever, there remains a performance gap between MathCoder2 and the Qwen2-Math and Qwen2.5-
Math models. This gap might be attributed to their superior computational, manual, and financial
resources,whichenablethescalingofdatasizeandthefurtherimprovementofdataquality,report-
ingamathemtaicaldatasetof700Btokens(Yangetal.,2024b).
IncontrasttomodelslikeQwen2-Math,whichonlyopen-sourcetheirmodelweights,withmuchof
theirdataprocessingandtrainingdetailsundisclosed,MathCoder2isfullyopen-sourced,including
alldataprocessingpipelinesandtrainingcode. Thisopennessfacilitatestransparency,reproducibil-
ity,andfurtherresearch,whichiscrucialforadvancingthefield. ComparedtoLlemma,whichalso
open-sources its code, our method achieves better performance on the five datasets. Particularly,
whentrainedonthesamebasemodel,Code-Llama,ourmethodperformssignificantlybetter,which
demonstratestheeffectivenessoftheMathCode-Pilepretrainingcorpus.
3.2 POST-TRAINING
TofurtherdemonstratethepotentialoftheMathCoder2modelsinaligningtomathematicalproblem-
solving tasks, we select the MathCoder2-Llama-3-8B model and MathCoder2-DeepSeekMath-7B
for finetuning on mathematical problem-solution pairs. We first train the base model on general
mathematicalinstructionsfollowingYueetal.(2024)fortwoepochs. Subsequently,wefinetunethe
modelonNuminaMath-CoT3,andNuminaMath-TIR4datasetsforthreeepochs.
The results are shown in Tab. 3. MathCoder2-Instruct-TIR achieves high performance on all five
datasets,reaching69.7%onMATHand86.5%onGSM8K,outperformingmanyofthebestopen-
source models of similar size and demonstrating our method‚Äôs potential to improve performance
on downstream mathematical reasoning tasks. As this paper focuses on continued mathematical
pretraining,thepost-trainingservesonlyasavalidationofthepotentialofourmodels.Weconducted
onlysimplesupervisedfine-tuning,withoutperformingreinforcementlearningordirectpreference
optimization,whichcouldfurtherimproveperformanceondownstreamtasks.
3.3 ABLATIONSTUDIES
In this session, we first analyze the impact of various components of the training data. Next, we
compareMathCode-Piletootheropen-sourcemathematicalpretrainingcorpora.
3https://huggingface.co/datasets/AI-MO/NuminaMath-CoT
4https://huggingface.co/datasets/AI-MO/NuminaMath-TIR
7Table4: Analysisoftheimpactofthemathematicalcode. Theupperhalfofthetablepresentsthe
resultsofbothusingandnotusingthemathematicalcodedata. Thelowerhalfdisplaysanablation
studyonthedesignofconcatenatingthereasoningstepsandcodesnippetsforpretraining. ‚ÄúBasic+
Reasoning-step-only‚Äùrepresentsonlyaddingtheconditions,expressions,andresults,while‚ÄúBasic
+Trans-code-only‚Äùrepresentsonlyaddingthetranslatedcode. ‚ÄúReasoning-Step&Code‚Äùrepresents
the concatenated data combining both. ‚ÄúBasic + No-code-prompt‚Äù represents using a prompt that
simplyinstructLlama-3.1-70B-Instructtorewritetextstoimprovetheirquality.
DataComposition BaseModel MATH GSM8K SAT OCW MMLU-
MATH
Basic Llama-3-8B 34.7 65.8 71.9 12.9 45.2
Basic+Reasoning-Step&Code Llama-3-8B 38.4(+3.7) 69.9(+4.1) 84.4(+12.5)18.0(+5.1) 46.5(+1.3)
Basic+Reasoning-step-only DeepSeekCoder-1.3B 16.7 22.7 40.6 4.8 25.9
Basic+Trans-code-only DeepSeekCoder-1.3B 14.6 22.1 43.8 5.5 25.5
Basic+No-code-prompt DeepSeekCoder-1.3B 15.7 21.3 37.5 4.8 24.4
Basic+Reasoning-Step&Code DeepSeekCoder-1.3B 17.8 25.5 59.4 5.9 26.1
Analysisoftheimpactofthemathematicalcode.Weanalyzetheimpactofthemathematicalcode
oncontinuedpretrainingbycomparingtheresultsofaddingandnotaddingthemathematicalcode.
As shown in Tab. 4, the addition of the mathematical code in the pretraining corpus significantly
improves performance across all five datasets. Note that the mathematical code only constitutes
14.1%ofthe19.2BtokensintheMathCode-Piledataset,yettheimprovementinaccuracyitbrings
aboutcomparedtothetotalimprovementinaccuracy(accMathCode-Pile‚àíaccbasic)is21.8%,27.1%,44.5%,
accMathCodePile‚àíaccorig
66.2%,and35.1%onthefivebenchmarkdatasets,respectively,demonstratingtheeffectivenessof
themathematicalcode. ComparisonacrossdifferenttrainingstepsisshowninAppendixF.
We also analyze the design choice of concatenating the natural language reasoning step with the
mathematical code for pretraining. This analysis is conducted by studying the results of adding
only the natural language reasoning steps, and separately adding only the translated code. As
shown in Tab. 4, Basic + Reasoning-step-only represents adding only the natural language rea-
soning steps; Basic + Trans-code-only represents adding only the translated code; and Basic +
Reasoning-Step&Code represents adding the concatenated data that combines both. The Basic +
Reasoning-Step&Codeconfigurationresultsinthebestperformance,demonstratingtheimportance
ofincludingboththenaturallanguagereasoningstepandthetranslatedmathematicalcode.
To rule out the possibility that the improvement comes from the higher quality of texts generated
byLlama-3.1-70B-Instruct,weuseapromptthatasksLlama-3.1-70B-Instructtorewritethegiven
text. ThedetailsofthispromptareprovidedinAppendixE.Wepresenttheresultsofreplacingthe
mathematicalcodewithtextsgeneratedusingthispromptinTab.4, labeledas‚ÄúBasic+No-code-
prompt‚Äù. Ourmethodofgeneratingmathematicalcodeaccompaniedwithcorrespondingreasoning
stepsoutperformsthisbaseline,demonstratingtheeffectivenessofourapproach.
Analysis of the impact of various parts of the basic data. We perform experiments on a
smaller model, DeepSeekCoder-1.3B, using different parts of the basic data. As demonstrated in
Tab. 5, filtered-OpenWebMath and filtered-CC-En-math significantly improve the performance of
the model. In comparison, textbooks, synthetic data, and code are smaller in data size and play a
lessimportantrole.Aseachofthesepartsofdataistoosmallforindividualpretraining,wecombine
themwithOpenWebMath-filteredtoshowthattheyeachbringasmallyetnoticeableimprovement
compared to using only OpenWebMath-filtered. Since we performed filtering on OpenWebMath
andtheinitiallyfilteredCC-Entoremoveirrelevantdata,wealsocomparetheperformancebefore
andafterfiltering. Weobservethatthereisnoobviousdegradationinperformanceafterremoving
irrelevantcontent,showingtheeffectivenessofthefiltering.
Comparison with other open-source mathematical pretraining corpora. We com-
pare MathCode-Pile with various other open-source mathematical pretraining corpora using
DeepSeekCoder-1.3B. We train each corpus for 3 epochs with a global batch size of 2 million
tokens and a 4096 token context length, since we observe that the model‚Äôs performance usually
saturates around 3 epochs. As shown in Tab. 6, MathCode-Pile significantly outperforms Open-
8Table 5: Analysis of the effect of different components in MathCoder2-Pile. The base model is
DeepSeekCoder-1.3B.
DataComposition MATH GSM8K SAT OCW MMLU-MATH
NoMathTraining 4.8 4.3 18.8 2.6 24.8
filtered-OpenWebMath(4.8B) 9.0 11.4 34.4 3.7 25.4
OpenWebMath(12.9B) 9.4 11.2 31.3 2.6 24.4
filtered-CC-En-math(6.4B) 9.1 12.1 31.3 3.7 25.2
CC-En-math(22.1B) 8.4 13.0 25.0 2.9 25.0
filtered-OpenWebMath+textbooks 9.4 12.7 50.0 4.0 25.4
filtered-OpenWebMath+syntheticdata 10.8 12.6 50.0 4.0 25.6
filtered-OpenWebMath+code 9.4 12.1 46.9 4.0 25.4
MathCoder2-Pile 17.8 25.5 59.4 5.9 26.1
Table6: ComparisonbetweenMathCode-PileandotherMathematicalPretraindatasets.
PretrainDataset BaseModel MATH GSM8K SAT OCW MMLU-MATH
NoMathTraining DeepSeekCoder-1.3B 4.8 4.3 18.8 2.6 24.8
OpenWebMath DeepSeekCoder-1.3B 9.4 11.2 31.3 2.6 24.4
Proof-Pile-2 DeepSeekCoder-1.3B 9.2 11.2 50.0 4.4 25.8
MathPile DeepSeekCoder-1.3B 5.3 3.4 21.9 2.2 24.9
DeepSeekMathCorpus DeepSeekLLM-1.3B 13.6 23.8 56.3 4.8 -
MathCoder2-Pile DeepSeekCoder-1.3B 17.8 25.5 59.4 5.9 26.1
WebMath,Proof-Pile-2,andMathPilewhentrainedonDeepSeekCoder-1.3B.TheDeepSeekMath
Corpus is not open-source, and its performance on DeepSeekLLM-1.3B is taken from Shao et al.
(2024),whichistrainedfor150Btokens,morethanourMathCode-Pile‚Äôstrainingofapproximately
60Btokens. The1.3BmodeltrainedwithMathCode-Pileoutperformsthe1.3Bmodeltrainedwith
DeepSeekMathCorpus.
AnalysisoftheimprovementonthepotentialofbeingfinetunedforTIRreasoning. Toanalyze
the effect of the model-translated mathematical code on LLMs‚Äô potential to be finetuned for TIR
reasoning,wefinetunetheoriginalLlama-3-8B,MathCoder2-Basic-Llama-3-8B,andMathCoder2-
Llama-3-8B on NuminaMath-TIR5 for three epochs, respectively. As shown in Tab. 7, the results
offinetuningonMathCoder2-Basic-Llama-3-8BarehigherthantheresultsoffinetuningonLlama-
3-8B. Finetuning on MathCoder2-Llama-3-8B results in even higher performance than finetuning
onMathCoder2-Basic-Llama-3-8B,showingthattheadditionofmathematicalcodeeffectivelyen-
hancesthemodels‚ÄôpotentialofbeingfinetunedforTIRreasoning.
4 RELATED WORK
Continued mathematical pretraining. Several works (Shao et al., 2024; Azerbayev et al., 2024;
Ying et al., 2024; Yang et al., 2024b) have explored the continued pretraining of LLMs on math-
ematicaldata, suchasmathematicalwebcontent, syntheticdata, andcode. InternLM-Math(Ying
etal.,2024)andQueryofCCFeietal.(2024)useBM25fordataretrieval,whileotherworkssuch
asDeepSeekMath(Shaoetal.,2024)andQwen2-Math(Yangetal.,2024b)employfastText(Joulin
etal.,2016)andothermeta-informationtoretrievetextsfromCommonCrawl. Ourapproachfol-
lowsthesemethodsbyusingfastTextfordatafiltering,andweintroduceaseconditerationoffiner
filtering to retain more relevant data. MathPile (Wang et al., 2023b) and phi (Gunasekar et al.,
2023) utilize real or synthesized textbooks, while Llemma (Azerbayev et al., 2024) and Qwen2-
Math (Yang et al., 2024b) incorporate math-related code in their datasets. However, unlike our
methodofgeneratingmathematicalcodewithaccompaniednaturallanguagereasoning,theircode
5https://huggingface.co/datasets/AI-MO/NuminaMath-TIR
9Table7: ComparisonbetweenfinetuningtheoriginalLlama-3-8B,MathCoder2-Basic-Llama-3-8B,
and MathCoder2-Llama-3-8B on NuminaMath-TIR. MathCoder2-Basic-Llama-3-8B is the model
resultingfromcontinuedpretrainingonthebasicdatawithoutaddingthemodel-translatedmathe-
maticalcode.
BaseModel MATH GSM8K OCW Olympiad SVAMP
Bench
Llama-3-8B 56.1 80.1 24.6 28.4 83.8
MathCoder2-Basic-Llama-3-8B 62.9 81.3 26.8 32.9 86.7
MathCoder2-Llama-3-8B 65.1 84.5 34.6 34.4 87.9
mostlyhasnonaturallanguageexplanationsorcontext. Ourworkbuildsontheseprioreffortsby
collecting and expanding upon these sources of math-related text. Unlike works that only open-
source their model weights, we take a more transparent approach by open-sourcing both our data
processingandmodeltrainingcode,therebyensuringreproducibilityandfacilitatingfutureresearch
inthisfield. ComparedtoLlemma(Azerbayevetal.,2024),whichalsoopen-sourcetheirdataand
trainingcode,ourmethodresultsinbetterperformanceonmathematicalreasoningtasks.
Syntheticdata.Numerousfinetuning(Yuetal.,2024;Wangetal.,2023a;Luetal.,2024a)andpre-
trainingGunasekaretal.(2023);Wangetal.(2023b);Yangetal.(2024b)studieshaveexploredtrain-
ingonsyntheticdatageneratedusinglanguagemodelsorpredefinedtemplates. MathGLM(Yang
et al., 2023) and InternLM-Math (Ying et al., 2024) use templates to generate synthetic numeri-
caloperationdata,whilephi(Gunasekaretal.,2023)producestextbook-qualitydatawithmodels.
EntiGraph(Yangetal.,2024c)generatesdiversetextbydrawingconnectionsbetweensampledenti-
ties. Ourworkproposesanovelmethodforextractingmathematicalreasoningstepsandgenerating
syntheticcodesnippetsthatcapturestheunderlyingreasoningprocesses.
Post-training. There are many methods for further improving the mathematical problem-solving
abilitiesofLLMs. Supervisedfinetuningadjustspretrainedmodelsusingmathproblemsandsolu-
tions in various formats, such as Chain-of-Thought (Yu et al., 2024; Yuan et al., 2023), Program-
of-Thought(Yueetal.,2023),andTool-IntegratedReasoning(Gouetal.,2024;Wangetal.,2023a;
Liao et al., 2024). Reinforcement learning Lightman et al. (2023); Wang et al. (2024) and Direct
PreferenceOptimizationRafailovetal.(2024);Xuetal.(2024);Luetal.(2024b)utilizemathemati-
calpreferencedatatoadjustthemodels‚Äôoutputs. Thesemethodsarediverseandrevealthepotential
ofpretrainedmodels. Theirperformanceisofteninfluencedbythequalityofthetrainingdataused
inthepretrainingstage.Toexplorethepotentialoffinetuningourpretrainedmodelsfordownstream
tasks,weconductsupervisedfinetuningwithexistingopen-sourcedata.
5 LIMITATIONS AND FUTURE WORK
Onelimitationofourworkisthatourcontinuedpretrainingcorpusfocusesprimarilyonmathemat-
icsanddoesnotintentionallyincludeotherSTEMsubjects, suchasphysicsandchemistry. Addi-
tionally, our pretraining data consists entirely of English texts, without incorporating math-related
content in other languages, like Chinese. Due to limitations in computational resources, we only
trained models ranging from 1.3B to 8B parameters. Future work could address these limitations
byexpandingthedatasettoincludeothersubjectsandlanguagesandbytrainingonlargerlanguage
models. Also, this paper primarily focuses on continued mathematical pretraining, so we did not
apply reinforcement learning methods like PPO and GRPO, or Direct Preference Optimization in
our post-training phase, which can further improve performance on mathematical reasoning tasks.
Inthefuture,wecouldexplorethesemethodsonourfinetunedmodels.
6 CONCLUSION
Inthispaper, wepresentaneffectiveopen-sourcecontinuedmathematicalpretrainingpipelinefor
enhancingmathematicalreasoningofLLMs. Throughthemeticulouscollectionandfilteringofdi-
versemath-relatedtexts,suchasmathematicalwebcontent,syntheticdata,codethatusesmathemat-
icalpackages,andmathtextbooks,wecurateabasicdatasetforcontinuedmathematicalpretraining.
10We then propose a novel method for extracting mathematical reasoning steps from the previously
collecteddatasetandtranslatingthemtocodesnippetsreflectingtheunderlyingreasoningprocesses.
Bycombiningthebasicdatawiththemodel-generatedmathematicalcodeaccompaniedwiththeir
correspondingreasoningsteps, weproducea19.2B-tokenmathematicalpretrainingcorpusnamed
MathCode-Pile,whichsignificantlyimprovestheperformanceoffourdifferentbasemodelsacross
fiverepresentativemathematicalbenchmarks. Byopen-sourcingtheentiredataprocessingpipeline
andmodeltrainingcode,weactivelypromotetransparency,reproducibility,andcollaborationwithin
theresearchcommunity,facilitatingfutureresearchinthisarea.
REFERENCES
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Al-
bertQ.Jiang,JiaDeng,StellaBiderman,andSeanWelleck. Llemma: Anopenlanguagemodel
formathematics,2024. URLhttps://arxiv.org/abs/2310.10631.
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui
Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye
Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting
Huang,TaoJiang,PenglongJiao,ZhenjiangJin,ZhikaiLei,JiaxingLi,JingwenLi,LinyangLi,
ShuaibinLi,WeiLi,YiningLi,HongweiLiu,JiangningLiu,JiaweiHong,KaiwenLiu,Kuikun
Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang
Ning,LinkeOuyang,JiantaoQiu,YuanQu,FukaiShang,YunfanShao,DeminSong,ZifanSong,
ZhihaoSui,PengSun,YuSun,HuanzeTang,BinWang,GuotengWang,JiaqiWang,JiayuWang,
Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong,
ChaoXu,RuiliangXu,HangYan,YirongYan,XiaoguiYang,HaochenYe,HuaiyuanYing,Jia
Yu,JingYu,YuhangZang,ChuyuZhang,LiZhang,PanZhang,PengZhang,RuijieZhang,Shuo
Zhang,SongyangZhang,WenjianZhang,WenweiZhang,XingchengZhang,XinyueZhang,Hui
Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou,
XipengQiu,YuQiao,andDahuaLin. Internlm2technicalreport,2024.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.
org/abs/2110.14168.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Hang Yan, Dahua Lin, and
XipengQiu.Queryofcc:Unearthinglargescaledomain-specificknowledgefrompubliccorpora,
2024. URLhttps://arxiv.org/abs/2401.14624.
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,SidBlack,AnthonyDiPofi,CharlesFos-
ter,LaurenceGolding,JeffreyHsu,AlainLeNoac‚Äôh,HaonanLi,KyleMcDonell,NiklasMuen-
nighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lin-
tang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework
forfew-shotlanguagemodelevaluation,072024. URLhttps://zenodo.org/records/
12608602.
ZhibinGou,ZhihongShao,YeyunGong,YelongShen,YujiuYang,MinlieHuang,NanDuan,and
WeizhuChen. Tora: Atool-integratedreasoningagentformathematicalproblemsolving,2024.
URLhttps://arxiv.org/abs/2309.17452.
SuriyaGunasekar,YiZhang,JyotiAneja,CaioCe¬¥sarTeodoroMendes,AllieDelGiorno,Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital
Shah, Harkirat Singh Behl, Xin Wang, Se¬¥bastien Bubeck, Ronen Eldan, Adam Tauman Kalai,
YinTatLee,andYuanzhiLi. Textbooksareallyouneed,2023. URLhttps://arxiv.org/
abs/2306.11644.
11Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-
cob Steinhardt. Measuring massive multitask language understanding, 2021a. URL https:
//arxiv.org/abs/2009.03300.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b.
URLhttps://arxiv.org/abs/2103.03874.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-
lot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,
Le¬¥lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,
Thomas Wang, Timothe¬¥e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https:
//arxiv.org/abs/2310.06825.
ArmandJoulin,EdouardGrave,PiotrBojanowski,andTomasMikolov. Bagoftricksforefficient
textclassification,2016. URLhttps://arxiv.org/abs/1607.01759.
AitorLewkowycz,AndersAndreassen,DavidDohan,EthanDyer,HenrykMichalewski,VinayRa-
masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam
Neyshabur, GuyGur-Ari, andVedantMisra. Solvingquantitativereasoningproblemswithlan-
guagemodels,2022. URLhttps://arxiv.org/abs/2206.14858.
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao
Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,
Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoaÀúo
Monteiro,OlehShliazhko,NicolasGontier,NicholasMeade,ArmelZebaze,Ming-HoYee,Lo-
geshKumarUmapathi,JianZhu,BenjaminLipkin,MuhtashamOblokulov,ZhiruoWang,Rudra
Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luc-
cioni,PauloVillegas,MaximKunakov,FedorZhdanov,ManuelRomero,TonyLee,NadavTimor,
JenniferDing,ClaireSchlesinger,HaileySchoelkopf,JanEbert,TriDao,MayankMishra,Alex
Gu,JenniferRobinson,CarolynJaneAnderson,BrendanDolan-Gavitt,DanishContractor,Siva
Reddy,DanielFried,DzmitryBahdanau,YacineJernite,CarlosMunÀúozFerrandis,SeanHughes,
ThomasWolf,ArjunGuha,LeandrovonWerra,andHarmdeVries. Starcoder: maythesource
bewithyou!,2023. URLhttps://arxiv.org/abs/2305.06161.
Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code
interpreteroutput‚Äìareproduciblepipeline,2024. URLhttps://arxiv.org/abs/2401.
08190.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step, 2023. URL
https://arxiv.org/abs/2305.20050.
ZimuLu,AojunZhou,HouxingRen,KeWang,WeikangShi,JuntingPan,MingjieZhan,andHong-
sheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing
mathematicalreasoningofllms,2024a. URLhttps://arxiv.org/abs/2402.16352.
ZimuLu,AojunZhou,KeWang,HouxingRen,WeikangShi,JuntingPan,MingjieZhan,andHong-
shengLi. Step-controlleddpo: Leveragingstepwiseerrorforenhancedmathematicalreasoning,
2024b. URLhttps://arxiv.org/abs/2407.00782.
Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open
dataset of high-quality mathematical web text, 2023. URL https://arxiv.org/abs/
2310.06786.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and
ChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel,
2024. URLhttps://arxiv.org/abs/2305.18290.
BaptisteRozie`re,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Je¬¥re¬¥my Rapin, Artyom Kozhevnikov, Ivan Ev-
timov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong,
12Alexandre De¬¥fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
ThomasScialom, andGabrielSynnaeve. Codellama: Openfoundationmodelsforcode, 2024.
URLhttps://arxiv.org/abs/2308.12950.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
MingchuanZhang,Y.K.Li,Y.Wu,andDayaGuo. Deepseekmath: Pushingthelimitsofmathe-
maticalreasoninginopenlanguagemodels,2024. URLhttps://arxiv.org/abs/2402.
03300.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023. URLhttps://arxiv.org/abs/2307.09288.
Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi
Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for en-
hancedmathematicalreasoning,2023a. URLhttps://arxiv.org/abs/2310.03731.
PeiyiWang,LeiLi,ZhihongShao,R.X.Xu,DamaiDai,YifeiLi,DeliChen,Y.Wu,andZhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024.
URLhttps://arxiv.org/abs/2312.08935.
ZengzhiWang,RuiXia,andPengfeiLiu.Generativeaiformath:Parti‚Äìmathpile:Abillion-token-
scalepretrainingcorpusformath,2023b. URLhttps://arxiv.org/abs/2312.17120.
Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan
Zeng,ZhengxiaoDu,WenyiZhao,JieTang,andYuxiaoDong. Chatglm-math: Improvingmath
problem-solving in large language models with a self-critique pipeline, 2024. URL https:
//arxiv.org/abs/2404.02893.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint
arXiv:2407.10671,2024a.
An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,
Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,
Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical ex-
pertmodelviaself-improvement,2024b. URLhttps://arxiv.org/abs/2409.12122.
ZhenYang,MingDing,QingsongLv,ZhihuanJiang,ZehaiHe,YuyiGuo,JinfengBai,andJieTang.
Gpt can solve mathematical problems without a calculator, 2023. URL https://arxiv.
org/abs/2309.03241.
Zitong Yang, Neil Band, Shuangping Li, Emmanuel Cande`s, and Tatsunori Hashimoto. Synthetic
continuedpretraining,2024c. URLhttps://arxiv.org/abs/2409.07431.
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma,
Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou,
HongweiLiu,SongyangZhang,WenweiZhang,HangYan,XipengQiu,JiayuWang,KaiChen,
and Dahua Lin. Internlm-math: Open math large language models toward verifiable reasoning,
2024.
13Prompt: You will be provided with a block of text. I need you to classify the text into one of the
followingtypes:
1.Thetextdescribesamathematicalproblemanditssolution.
2.Thetextexplainsamathematicalconceptormathematicaltheory.
3.Thetextexplainsascientificorengineeringconceptthatrequiresmathematicalknowledge.
4.Thetextdescribesaprogrammingproblemanditssolution.
5.Thetextexplainsaconceptortheoryrelatedtoprogramming.
6.Thetextexplainstheusageofaprogramminglanguageorsoftwaretool.
7.Thetextdoesnotbelongtoanyofthetypesabove.
Here‚ÄôsthetextI‚Äôveprovided. Kindlyanalyzeandclassifyitintotype1,2,3,4,5,6or7. Putyour
choicebehind‚ÄúThetypeis:‚Äù. Pleasedonotgenerateanyunrelatedadditionalcomments! Thetype
numbermustmatchthetypedescription. Here‚Äôsoneofthetextsthatneedstobeclassified: {TEXT}
Thetypeis:
Figure3: ThepromptforannotationofOpenWebMathandtheinitiallyfilteredCC-Endocuments.
{TEXT}isreplacedwiththecontentofthedocument.
LonghuiYu,WeisenJiang,HanShi,JinchengYu,ZhengyingLiu,YuZhang,JamesT.Kwok,Zhen-
guoLi,AdrianWeller,andWeiyangLiu. Metamath:Bootstrapyourownmathematicalquestions
forlargelanguagemodels,2024. URLhttps://arxiv.org/abs/2309.12284.
ZhengYuan,HongyiYuan,ChengpengLi,GuantingDong,KemingLu,ChuanqiTan,ChangZhou,
andJingrenZhou. Scalingrelationshiponlearningmathematicalreasoningwithlargelanguage
models,2023. URLhttps://arxiv.org/abs/2308.01825.
XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhuChen.
Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL
https://arxiv.org/abs/2309.05653.
XiangYue,TuneyZheng,GeZhang,andWenhuChen. Mammoth2: Scalinginstructionsfromthe
web,2024. URLhttps://arxiv.org/abs/2405.03548.
GeZhang, ScottQu, JiahengLiu, ChenchenZhang, ChenghuaLin, ChouLeuangYu, DannyPan,
Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming
Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting
Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu,
ShawnGuo,SorenGao,WangchunshuZhou,XinyueZhang,YizhiZhou,YuboWang,YuelinBai,
Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli
Ouyang,WenhaoHuang,andWenhuChen. Map-neo: Highlycapableandtransparentbilingual
largelanguagemodelseries,2024. URLhttps://arxiv.org/abs/2405.19327.
Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,
Linqi Song, Mingjie Zhan, and Hongsheng Li. Solving challenging math word problems using
GPT-4 code interpreter with code-based self-verification. In The Twelfth International Confer-
ence on Learning Representations, 2024. URL https://openreview.net/forum?id=
c8McWs4Av0.
A PROMPT FOR ANNOTATION OF MATH WEB DOCUMENTS
Inthissection,wepresentthepromptweusedforannotationofdocumentsinOpenWebMathand
theinitiallyfilteredCC-En.Theprompt,asshowninFig.3,asksthemodeltoclassifythedocument
into one of seven types, which are types of documents that frequently appear in the datasets. We
observethatthismethodhelpsthemodeltobetteridentifyandfilteroutirrelevanttextthanusinga
binaryclassificationofwhetherthetextisrelatedtomath.
14Prompt: Youwillbepresentedwithatextrelatedtomath. Ineedyoutocarefullyreadthroughthe
text.Ifyoufindanyincorrectstatments,erroneouscomputationsteps,spellingmistakes,grammatical
errors,orformattingissues,adjustthemsothattheerroriscorrected.Rewritethetexttomakeitmore
accurateandeasiertounderstand.Youshouldonlyoutputanadjustedversionofthegiventext.Also,
donotchangetheoriginallanguage. Pleasedonotgenerateanyunrelatedadditionalcomments! The
textisasfollows:{TEXT}
Youshouldoutput:
Figure 4: The prompt asking Llama-3.1-70B-Instruct to simply rewrite the text and improve its
quality. {TEXT}isreplacedwiththecontentofthedocument.
B TRAINING DETAILS OF FASTTEXT CLASSIFIERS
Weemployanopen-sourcelibrary6fortraining,configuringthevectordimensionto50,thelearning
rateto0.5,themaximumlengthofwordn-gramsto2,andthenumberoftrainingepochsto5. For
the initial filtering of the Common Crawl corpus, we sample 3 million data points from the seed
corpus of filtered-OpenWebMath as positive training examples and another 8 million web pages
fromCommonCrawlasnegativeexamples.Forfinerfiltering,weuse2milliondatapointsannotated
as math-related by Mixtral-8x7B-Instruct as positive training samples and 1 million data points
annotatedasunrelatedtomathasnegativetrainingsamples.
C EXAMPLES OF THE MODEL-GENERATED MATHEMATICAL CODE
Thissessionshowsseveralexamplesofthetranslationfrommath-relatedtextstomathematicalcode
acompaniedwithcorrespondingreasoningsteps. AsshowninTab.8, Tab.9, Tab.10andTab.11,
the model first extract the LaTex expression alone with its conditions and result from the original
text,thengeneratesanPythoncodesnippetbasedonthisinformation.
D EXAMPLES OF REMOVED IRRELEVANT TEXTS
Inthissection,wepresentseveralexamplesintheoriginalOpenWebMathdatasetthatareirrelevant
tomathematicalreasoningandremovedinthefilteringprocess. AsshowninTab.12,Tab.13,and
Tab.14,thecontentofthesedocumentsarenotrelatedtomath,butinsteadareaboutsubjectssuch
aspolitics,testingsoftware,orwebdevelopment. Removingtheseirrelevanttextshavenoobvious
impactonthemathematicalcontinuedpretrainingperformance.
E PROMPT FOR SIMPLE REWRITING TO IMPROVE QUALITY FOR ABLATION
STUDY
To rule out the possibility that the improvement results solely from the enhanced quality of the
texts generated by Llama-3.1-70B-Instruct, we designed a prompt asking Llama-3.1-70B-Instruct
torewritethetext,checkingformistakesincontentandformattoenhanceaccuracyandclarity,as
showninFig.4.
F COMPARISON BETWEEN ADDING AND NOT ADDING MATHEMATICAL
CODE
Inthissection,wepresentthecomparisonbetweenaddingandnotaddingmathematicalcodeacross
different training steps. The experiments are conducted on Llama-3 8B. As shown in Fig. 5 and
Fig.6, addingthemodel-translatedmathematicalcodeimprovesaccuracyacrossdifferenttraining
steps.
6https://fasttext.cc/
15OriginalText:
BayesandhisTheorem
MyearlierpostonBayesianprobabilityseemstohavegeneratedquitealotofreaders,sothis
lunchtimeIthoughtI‚Äôdaddalittlebitofbackground.
......
Ifweaskthequestion‚Äúwhatistheprobabilityofexactlyxsuccessesfromthepossiblen?‚Äù,the
answerisgivenbythebinomialdistribution:
P (x|n,p)=C(n,x)px(1‚àíp)n‚àíx
n
......
TranslationOutput:
ConditionsNeeded:
1. Thenumberoftrials(n)isapositiveinteger.
2. Theprobabilityofsuccess(p)isarealnumberbetween0and1.
3. Thenumberofsuccesses(x)isanon-negativeintegerlessthanorequalton.
ComputationExpression:
P (x|n,p)=C(n,x)px(1‚àíp)n‚àíx
n
ComputationResult:
Theprobabilityofexactlyxsuccessesinnindependenttrials,eachwithaprobabilityofsuccess
p.
PythonCodeSnippet:
‚Äò‚Äò‚Äòpython
import math
def binomial distribution(n, x, p):
combinations = math.comb(n, x)
probability = combinations * (p ** x) * ((1 ‚àí p) ** (n ‚àí x))
return probability
# Example usage:
n = 10 # number of trials
x = 5 # number of successes
p = 0.5 # probability of success
result = binomial distribution(n, x, p)
print(result)
‚Äò‚Äò‚Äò
Table 8: An example of translating a piece of text to mathematical code and a corresponding rea-
soningstep.
16OriginalText:
#Probabilityofgetting2Aces,2Kingsand1Queeninafivecardpokerhand(PartII)
SoIreworkedmyformulainmethod1aftergettinghelpwithmyoriginalquestion......
(cid:0)4(cid:1)(cid:0)4(cid:1)(cid:0)4(cid:1)
3
2 2 1 =
(cid:0)52(cid:1)
54145
5
5.540678‚àó10‚àí5
......
TranslationOutput:
ConditionsNeeded:
1. Thetotalnumberofcardsinadeckis52.
2. Wearedrawing5cardsfromthedeck.
3. Wewanttocalculatetheprobabilityofgetting2Aces,2Kings,and1Queen.
ComputationExpression:
(cid:0)4(cid:1)(cid:0)4(cid:1)(cid:0)4(cid:1)
2 2 1
(cid:0)52(cid:1)
5
ComputationResult:
5.540678‚àó10‚àí5
PythonCodeSnippet:
‚Äò‚Äò‚Äòpython
import math
def combination(n, k):
return math.comb(n, k)
numerator = combination(4, 2) * combination(4, 2) * combination(4, 1)
denominator = combination(52, 5)
result = numerator / denominator
print(result)
‚Äò‚Äò‚Äò
Table 9: An example of translating a piece of text to mathematical code and a corresponding rea-
soningstep.
17OriginalText:
#Howdoyouusetheproductruletodifferentiateg(x)=(x2+1)(x2‚àí2x)?
......
andh(x)=x2‚àí2x‚áíh‚Ä≤(x)=2x‚àí2
......
TranslationOutput:
ConditionsNeeded:
1. h(x)isapolynomialfunction
2. Thederivativeofx2is2x
3. Thederivativeof‚àí2xis‚àí2
ComputationExpression:
d (x2‚àí2x)=2x‚àí2
dx
ComputationResult:
h‚Ä≤(x)=2x‚àí2
PythonCodeSnippet:
‚Äò‚Äò‚Äòpython
import sympy as sp
x = sp.symbols(‚Äôx‚Äô)
h = x**2 ‚àí 2*x
h prime = sp. diff(h, x)
print(h prime) # Output: 2*x ‚àí 2
‚Äò‚Äò‚Äò
Table 10: An example of translating a piece of text to mathematical code and a corresponding
reasoningstep.
OriginalText:
#Howdoyouusetheproductruletodifferentiateg(x)=(x2+1)(x2‚àí2x)?
......
andh(x)=x2‚àí2x‚áíh‚Ä≤(x)=2x‚àí2
......
TranslationOutput:
ConditionsNeeded:
1. h(x)isapolynomialfunction
2. Thederivativeofx2is2x
3. Thederivativeof‚àí2xis‚àí2
ComputationExpression:
d (x2‚àí2x)=2x‚àí2
dx
ComputationResult:
h‚Ä≤(x)=2x‚àí2
PythonCodeSnippet:
‚Äò‚Äò‚Äòpython
import sympy as sp
x = sp.symbols(‚Äôx‚Äô)
h = x**2 ‚àí 2*x
h prime = sp. diff(h, x)
print(h prime) # Output: 2*x ‚àí 2
‚Äò‚Äò‚Äò
Table 11: An example of translating a piece of text to mathematical code and a corresponding
reasoningstep.
18##AvoidingWeimarRussia
MatthewYglesiaswrites:
Matthew Yglesias: Beyond Economics: Over at Brad DeLong‚Äôs site you can see a fascinat-
ing discussion of America‚Äôs Russia policy in the 1990s between DeLong, Martin Wolf, and
LawrenceSummers. OneremarkIwouldmakeisthattoanextraordinaryextent,allthreepar-
ticipantsarewillingtoacceptthepremisethattheonlygoalofUSpolicytowardRussiainthe
1990swasagood-faithefforttoinduceRussianprosperity, withsucheffortsbeinghampered
bypoliticalconstraints,theobjectivedifficultyofthetask,andpurepolicyerrors...
Well,yes. Russiawasonceasuperpowerandmaybeoneagain. Onewouldhavethoughtthat
thehistoryof1914-1945wouldteachamplelessonsaboutthenationalsecurityundesirability
oftryingtokeepgreatpowers‚ÄìlikeWeimarGermany‚Äìpoorandweak.Onewouldhavethought
thatthehistoryof1945-1990wouldteachamplelessonsaboutthenationalsecuritydesirability
oftryingtohelpgreatpowers‚ÄìlikeJapanandWestGermany‚Äìbecomeprosperous,democratic,
andwell-integratedintotheworldeconomy.
One top of the national-security strategic argument there is the economic argument: the fact
that richer trading partners are better trading partners: they make more and more interesting
stuffforustobuy.
......
Table12: AnexampleofremovedtextirrelevanttomathematicalreasoninginOpenWebMath.
#MicroEJTestSuiteEngine¬∂
##Introduction¬∂
TheMicroEJTestSuiteEngineisagenerictoolmadeforvalidatinganydevelopmentproject
usingautomatictesting.
Thissectiondetailsadvancedconfigurationforuserswhowishtointegratecustomtestsuites
intheirbuildflow.
TheMicroEJTestSuiteEngineallowstheusertotestanykindofprojectswithintheconfigu-
rationofagenericAntfile.
TheMicroEJTestSuiteEngineisalreadypre-configuredforrunningtestsuitesonaMicroEJ
Platform(eitheronSimulatororonDevice).
##UsingtheMicroEJTestSuiteAntTasks¬∂
MultipleAnttasksareavailableinthetestsuite-engine.jarprovidedintheBuildKit:
‚Ä¢testsuiteallowstheusertorunagiventestsuiteandtoretrieveanXMLreportdocumentina
JUnitformat.
‚Ä¢javaTestsuiteisasubtaskofthetestsuitetask,usedtorunaspecializedtestsuiteforJava(will
onlyrunJavaclasses).
‚Ä¢htmlReportisataskwhichwillgenerateanHTMLreportfromalistofJUnitreportfiles.
......
Table13: AnexampleofremovedtextirrelevanttomathematicalreasoninginOpenWebMath.
ByKimsereyLamwith
#ConemuABetterCommandPromptForWindows
Jul22nd,2017-writtenbyKimsereywith.
WhendevelopingmultipleWebapiundermultipleVisualStudiosolutions,itcanbecomevery
tedioustomaintain,runanddebug. OpeningmultipleinstancesofVisualStudioisverycostly
intermofmemoryandrunningallatoncealsoclutterthescreenwhichrapidlybecomesirritat-
ing. WiththeadventofdotnetCLItools,ithasbeenclearthatthenextstepwouldbetomove
outofthecommon‚Äúrightclick/build,F5‚ÄùofVisualStudioandtoward‚Äúdotnetrun‚Äùonacom-
mandprompt. LastmonthIwaslookingforaWindowsalternativeofthebashterminalwhich
canbefoundonMacandIfoundConEmu. ConEmuprovidesaccesstoalltypicalshellsvia
anenhancedUI.TodaywewillseehowwecanuseConEmutoeaseourdevelopmentprocess
byleveragingonly2ofitsfeatures;thetasksandenvironmentsetup.
1. dotnetCLI2. Setupenvironment4. Applytomultipleservices
......
Table14: AnexampleofremovedtextirrelevanttomathematicalreasoninginOpenWebMath.
19GSM8K
basic data
70.0 code-added
67.5
65.0
62.5
60.0
57.5
55.0
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000110001200013000
Steps
Figure 5: Comparison of the accuracy on GSM8K between adding and not adding mathematical
codeacrossdifferenttrainingsteps.
MATH
38
basic data
36 code-added
34
32
30
28
26
24
22
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000110001200013000
Steps
Figure6:ComparisonoftheaccuracyonMATHbetweenaddingandnotaddingmathematicalcode
acrossdifferenttrainingsteps.
20
)%(
ycaruccA
)%(
ycaruccA