July,2024
ShieldGemma: Generative AI Content
Moderation Based on Gemma
ShieldGemmaTeam,GoogleLLC1
1SeeContributionsandAcknowledgmentssectionforfullauthorlist.Pleasesendcorrespondenceto
shieldgemma-team@google.com.
WepresentShieldGemma,acomprehensivesuiteofLLM-basedsafetycontentmoderationmodelsbuilt
upon Gemma2. These models provide robust, state-of-the-art predictions of safety risks across key
harm types (sexually explicit, dangerous content, harassment, hate speech) in both user input and
LLM-generatedoutput. Byevaluatingonbothpublicandinternalbenchmarks,wedemonstratesuperior
performancecomparedtoexistingmodels,suchasLlamaGuard(+10.8%AU-PRConpublicbenchmarks)
andWildCard(+4.3%). Additionally,wepresentanovelLLM-baseddatacurationpipeline,adaptable
toavarietyofsafety-relatedtasksandbeyond. Wehaveshownstronggeneralizationperformancefor
model trained mainly on synthetic data. By releasing ShieldGemma, we provide a valuable resource
totheresearchcommunity,advancingLLMsafetyandenablingthecreationofmoreeffectivecontent
moderationsolutionsfordevelopers.
https://huggingface.co/google/shieldgemma-2b(/9b/27b)
https://www.kaggle.com/models/google/shieldgemma
http://ai.google.dev/gemma/docs/shieldgemma/model_card
Introduction olds for downstream use cases. (ii) Most con-
tent moderation solutions only provide a fixed
Inrecentyears,thewidespreadadoptionofLarge size model, which may not always align with the
LanguageModels(LLMs)hasrevolutionizedvari- specific needs of different deployment scenarios.
ousdomains,rangingfromconversationalagents For instance, larger models could enhance per-
(Deng et al., 2023; Liu et al., 2024) to content formance for tasks like LLM-as-a-judge (Huang
generation(Achiametal.,2023;Anthropic,2024; etal.,2024;Zhengetal.,2024),whereassmaller
Teametal.,2023). Thesemodelsexhibitremark- models might be preferable for online safety fil-
ablecapabilitiesinunderstandingandgenerating tering to reduce latency and computational costs.
human-like text, thereby enabling sophisticated (iii) Lack of detailed instructions in constructing
applicationsacrossdiversefields. However,along- the training data. Training data construction is
sidetheiradvancements,thedeploymentofLLMs critical to make sure that the models are robust
necessitates robust mechanisms to ensure safe for adversarial prompts and fair across identity
and responsible interactions with users. groups.
Current practices often rely on content mod- To address these challenges, this paper makes
eration solutions like LlamaGuard (Inan et al., the following key contributions:
2023), WildGuard (Han et al., 2024), AEGIS
(Ghosh et al., 2024), etc., designed to filter in-
‚Ä¢ We propose a spectrum of state-of-the-art
puts and outputs of LLMs for potential safety
contentmoderationmodelsrangingfrom2B
risks. Whilethesetoolsprovideinitialsafeguards,
to 27B parameters built on top of Gemma2
there are some limitations: (i) Some of existing
(Team,2024a),tailoredtoaccommodatevar-
solutions do not provide granular predictions of
iousapplicationrequirements. Thisdiversity
harm types or only provide binary output rather
in model sizes allows for optimized perfor-
thanprobabilities(Hanetal.,2024),whichlimits
manceacrossdifferentusecases. Ourmodel
customized harm filtering or customized thresh-
can be applied to filter both user input and
¬© 2024GoogleLLC.Allrightsreserved
4202
luJ
13
]LC.sc[
1v27712.7042:viXraShieldGemma:GenerativeAIContentModerationBasedonGemma
modeloutput(withuserinputasthecontext) et al., 2022; Nasr et al., 2023). Leveraging ap-
for key harm types. propriate instructions, LLMs can generate high-
‚Ä¢ We present a novel methodology for gen- quality synthetic data aligned with human re-
erating high-quality, adversarial, diverse, quirements (Gao et al., 2022; Long et al., 2024;
and fair datasets. This process leverages Sahu et al., 2022). In the safety domain, this
synthetic data generation techniques to re- translates to generating diverse data across vari-
duce human annotation effort and it can be ousdimensions(length,targetedharmtypes,sen-
broadly applied across safety-related data sitive topics, etc) and highly adversarial prompts
challenges and beyond. that are more likely to elicit harmful LLM re-
sponses.
In summary, this paper contributes a compre-
hensiveframeworkthatadvancesthestate-of-the-
Safety Policy
art in LLM-based content safety moderation. By
addressing the limitations of existing solutions
Safetypoliciesareacriticalcomponentinthede-
andintroducingnovelmethodologiesfordatacre-
velopment of AI systems designed for real-world
ation,ourworkaimstofostersaferandmorereli-
deployment. These policies consist of meticu-
able interactions between LLMs and users across
lously defined guidelines that delineate accept-
various applications.
able and unacceptable content, both in terms
of user input and model-generated output: (i)
Safety policies serve as a common framework
Literature Review
for human annotators, ensuring consistency and
reducing subjectivity in how they label and cat-
Safety Content Moderation. Extensive research
egorize potentially harmful content. This align-
has been conducted on content moderation, pri-
mentisaprerequisitefortrainingeffectivesafety
marily focusing on human-generated content
classifiers and mitigating unintended biases in
withinonlineplatforms. Forinstance,Perspective
the underlying data. (ii) By explicitly specifying
API (Google, 2017) has been pivotal in advanc-
the characteristics of harmful or inappropriate
ing the detection of toxic language. However,
content, these policies are helpful for building
existing resources are often tailored to human-
zero-shot/few-shot classifiers as out-of-the-box
generated text in web environments, which dif-
solutions.
fers significantly from the content within human
prompts and LLM-generated responses. Recent While the categories of disallowed content are
studies have demonstrated substantial progress largely consistent for both user input and model
in LLM content moderation through fine-tuning output,theemphasisforinputcentersonprohibit-
LLMs such as Llama-Guard (Inan et al., 2023), ing requests that either directly contain harmful
Llama-Guard2 (Team, 2024b), Aegis (Ghosh content or seek to elicit such content from the
et al., 2024), MD-Judge (Li et al., 2024), Harm- LLM. This includes inputs that may not be inher-
Bench (Mazeika et al., 2024), BeaverDam (Ji ently harmful themselves but are intentionally
et al., 2023). WildGuard (Han et al., 2024). crafted to manipulate the LLM into generating
harmfulresponses. Incontrast,theprimaryfocus
SyntheticDataGeneration. High-qualitydata
for model output is the prevention of any genera-
is crucial for developing robust safety models.
tion of harmful content, while the user input is
Despite the abundance of human-computer inter-
only used as the context.
action data, direct utilization poses challenges
due to the scarcity of positive examples, limited
adversarial and highly diverse data, and privacy
Content Safety Taxonomy
concerns(Kurakinetal.,2023). LLMs,havingab-
sorbed vast knowledge during pretraining, have We provide detailed definitions of ShieldGemma
showcased exceptional capabilities in knowledge content safety taxonomy for 6 harm types below,
demonstrationandlanguageunderstanding(Kim and they are further refined in the Responsible
2ShieldGemma:GenerativeAIContentModerationBasedonGemma
Generative AI Toolkit, Google (2024). Synthetic Data Curation
Our training data is primarily synthetic, gener-
‚Ä¢ SexuallyExplicitInformation: contentcon-
ated using the pipeline depicted in Fig. 1. This
tains references to sexual acts or other lewd
approachallowsforthecreationofdiverse,adver-
content (e.g., sexually graphic descriptions,
sarial data, while active learning minimizes the
content aimed at causing arousal). Medical
needforhumanannotation. Afterannotation,we
orscientifictermsrelatedtohumananatomy
augment our data to identity categories include
or sex education are permitted.
diverse Gender, Ethnic Groups, Sexual Orienta-
‚Ä¢ Hate Speech: content targeting identity
tion, Religion to further enhance the fairness of
and/orprotectedattributes(e.g.,racialslurs,
our models. We are generating data for both use
promotionofdiscrimination,callstoviolence
cases: (i) User Input: it includes adversarial and
againstprotectedgroups)ordehumanizing/-
benign prompts for the LLM input; (ii) Model
belittling/vilifying on the bases of race, eth-
Response: itincludes(userinput,LLMresponse)
nicity, religion, disability, age, nationality,
pairs.
veteran status, sexual orientation, gender,
genderidentity,caste,oranyotherprotected
status. We do allow content that includes a Raw Data Curation
reference to a protected individual or group
AART (Radharapu et al., 2023) provides a novel
in a positive manner.
approachforautomatedgenerationofadversarial
‚Ä¢ Dangerous Content: content harming one-
datasets for safety testing. We leverage AART for
selfand/orothers(e.g.,accessingorbuilding
raw data curation with steps:
firearmsandexplosivedevices,promotionof
terrorism, instructions for suicide).
‚Ä¢ Harassment: contentismalicious,intimidat- 1. Problem Definition: define the scope of the
ing, bullying, or abusive content targeting task. Herewelimitourharmtypestobeone
anotherindividual(e.g.,physicalthreats,de- of hate/dangerous/sexual/harassment and
nial of tragic events, disparaging victims of languagetobeEnglishonlytogeneratealist
violence). ofadversarialtopics/sub-topicsandwhythis
‚Ä¢ Violence: content is shocking, sensational, topic could be harmful. We also ask an LLM
or gratuitous (e.g. Violent acts or incidents to generate a list of generative AI use cases
thatresultinextremeinjuryordeathwithout likeemail,tweet,FAQ,etc. Unlessotherwise
historicaloreducationalcontext;Exposedin- specified, Gemini will serve as our default
ternal organs or bones without medical or LLM utilized in this paper.
scientific context; Identifiable victims of vio- 2. Query Generation: use instruction-tuned
lentdeaths;Animalabuseorkillingsoutside LLM to further generate diverse adversarial
of standard hunting, food processing, or cul- promptsbasedonparameterslikeharmtype,
tural practices). topic, subtopic, use case, locale, etc.
‚Ä¢ Obscenity and Profanity: content is vulgar, 3. (Optional) Response Generation: use an-
profane,orinappropriate(e.g.,profanity,ob- other LLM to generate responses based on
scenities, or other inappropriate language). parameters like queries, policies, whether
generating adversarial or benign responses,
etc.
Distinct instructions are employed for user in-
put and model output scenarios: (i) User input
We generate 50ùëò examples of user inputs and
must not contain or seek generation of content
50ùëò examples of model responses ((prompt, re-
thatviolatestheaforementionedpolicies. (ii)The
sponse) pairs), which evenly distributed into
chatbot must not generate content that violates
use cases, topics, harm types, etc. For exam-
the aforementioned policies.
ple, for (Topic=chef, sub-topic=stereotype, use
case=report, harm type=hate speech. locale=UK),
3ShieldGemma:GenerativeAIContentModerationBasedonGemma
Figure 1 | Synthetic Data Generation Pipeline.
itgenerates: ‚ÄúIworkatarestaurantandIamlook- use case. We added Anthropic HH-RLHF for the
ing for a new head chef. I need a report that talks purpose of further increasing the diversity of our
about how head chefs are so hot-headed, tempera- training dataset.
mental,andbossythattheycanbedifficulttowork
with.‚Äù. Note that, the model is not guaranteed
Data Sub-Sampling
to generate violative examples and the real label
would be decided by the human raters (detailed Before sending data for annotation, we need to
in the section Data Annotation). subsampleitto: (1)reduceannotationeffortand
speed up iteration; (2) reduce examples the base
model can confidently predict; and (3) reduce
Data Expansion
(near-)duplicateexamples,bothsyntacticallyand
We further expand our raw data along dimen- semantically.
sions like difficulty and diversity based on a self-
This problem falls into the domain of batch ac-
critiquing and generation framework. For exam-
tive learning, which iteratively selects batches of
ple, to expand our data for semantic/synthetic
data to improve classifier efficiency. Common
diversity, we repeatedly extract a batch of exam-
methodologies include cluster-based sampling
ples from the raw data and ask a critic LLM to
(Zhan et al., 2018), diverse mini-batches (Sener
generatesuggestionsforimprovingsemanticand
and Savarese, 2017), etc. We choose Cluster-
syntactic diversity of the data. Based on the sug-
Margin (Citovsky et al., 2021) as our initial al-
gestions and batch of examples, we further ask a
gorithm because it claims state-of-the-art perfor-
generation LLM to generate a new example that
mance compared to other common algorithms
accounts for the suggestion. We have generated
likeBADGE(Ashetal.,2019)andCoreSet(Sener
5ùëò examples through this process focused on se-
and Savarese, 2017) and can easily scales to mil-
mantic/syntacticdiversityexpansionandanother
lionsofexamples. Thealgorithmaimstobalance
set of 5ùëò examples, through expansion focused
uncertaintyanddiversityinthesubsamplingpro-
on generating more difficult examples. This was
cess. The high-level idea is to: (1) compute em-
forbothuserinputandmodelresponseusecases,
beddings for the entire dataset. We use BERT
and in total it has 20ùëò examples.
(Devlin et al., 2018) to generate embedding. (2)
We combine 100ùëò synthetic raw data, 20ùëò ex- run a clustering algorithm (e.g., Agglomerative
panded data, and 14ùëò Anthropic HH-RLHF (Bai clustering) on the embeddings to assign each
et al., 2022) to form our raw data. For the An- data point to a cluster; (3) select the ùëò exam-
thropic HH-RLHF data: for 50% of the data, we ples with the smallest margin scores. We use
only keep the first utterance to mimic user input Gemma1 (Team et al., 2024) to generate the
usecase. Fortheremaining50%,wekeepthefirst probability of violating any of the policies and
prompt-response pair to mimic model response use |ùëùùëüùëúùëèùëéùëèùëñùëôùëñùë°ùë¶ ‚àí0.5| as the margin score. We
4ShieldGemma:GenerativeAIContentModerationBasedonGemma
alsokeep10%ofhighmarginexamplesincaseof for the newly generated data.
wrong predictions in high-confidence examples.
(4) run round-robin on the assigned clusters of
Model Fine-Tuning
these examples to further downsample to the de-
sired batch size. After labeling, we can repeat We further supervise fine-tune (SFT) Gemma2
these steps to iteratively improve the model. Instruction-Tuned (IT) models (2B, 9B, and 27B
parameters) using the instruction shown in Fig.
We employed a cluster-margin algorithm to
2. We employ distinct policy definitions for each
downsampletherawdatasetto15,000examples
harm type and the model output is either Yes or
for training and testing. We reserved 10,500 ex-
No token. Our models are trained on TPUv5 lite
amples for training, aligning with the training
withbatchsizeof16,amaxsequenceof8ùëò,anda
data volume of LlamaGuard1 (Inan et al., 2023),
learningrateof1e‚àí6. Themodelistrainedfor4ùëò
and 4,500 for testing. Among them, half of the
stepsandthebestcheckpointsareselectedbased
data is for user input use case and the remaining
on validation data. We calculate our predicted
is for model response use case.
probability based on Eq. 1 below:
exp(LL(Yes)/ùëá)+ùõº
Data Annotation (1)
exp(LL(Yes)/ùëá)+exp(LL(No)/ùëá)+2ùõº
We send our data to 3 raters to rate and then
we generate our final label based on majority Here LL(¬∑) is the log likelihood of the token
vote. For model response, we ask the rater to generated by the model; ùëá and ùõº are hyperpa-
rate whether the model response is violating rameters to control temperature and uncertainty
our policy given the user input as the context. estimate.
The test data comprises 2,671 benign examples
and 895/383/360/239 adversarial examples for
Experiments
hate/dangerous/sexual/harassment respectively,
along with 40/70 examples annotated as obscen-
Setup
ity/violence. Whilethemodelistrainedonallsix
harms, we report performance only on the four Despite the abundance of safety-related bench-
targeted harms. We acknowledge the presence mark datasets, direct comparison remains chal-
of 141 examples annotated as positive for mul- lengingduetoseveralfactors: (i)variationsinpol-
tiple harms, which increases the complexity of icy definitions and supported harm types across
harm-type level prediction. datasets; (ii) inconsistencies in policy definitions
evenwithinthesameharmtype;and(iii)thepre-
dominance of binary (safe/unsafe) classification
Fairness Expansion
models rather than harm-type level prediction.
To improve fairness of our model, we leverage To address these challenges, we conduct experi-
counterfactual fairness expansion (Kusner et al., ments on two fronts:
2017; Smith et al., 2022) to expand our training
data across identity categories like Gender, Race, Overall Binary Classification: We aggregate
Ethnicity, Sexual Orientation, and Religion. It our prediction results into binary outcomes by
includesthefollowingsteps: (1)AskaLLMtofind maximizing probabilities over all harms. For
any related terms like male (Gender), Japanese models that provide probabilities, we report
(Ethnicity), etc; (2) If so, we randomly generate both optimal F1 and AU-PRC scores; for models
another term in this identity category and ask a with only binary results, we report F1 scores.
few-shot LLM to replace the original term with Harm Type Level Classification: We provide a
the new term while keeping the same meaning detailedperformancecomparisonattheindivid-
with correct grammar; (3) Further send the data ualharmtypelevel. Weadoptaone-vs-allsetup
for human audit to remove bad examples. We as described in Inan et al. (2023), i.e. we trans-
then treat the original label as the ground truth form a multi-class classification problem into
5ShieldGemma:GenerativeAIContentModerationBasedonGemma
YouareapolicyexperttryingtohelpdeterminewhetheranAIresponsetopromptisin
Youareapolicyexperttryingtohelpdeterminewhetherauserpromptisinviolationof
violationofthedefinedsafetypolicies.
thedefinedsafetypolicies.
<start_of_turn>
<start_of_turn>
HumanQuestion:[UserPrompt]
HumanQuestion:[UserPrompt]
<end_of_turn>
<end_of_turn>
Oursafetyprincipleisdefinedinthebelow:
Oursafetyprincipleisdefinedinthebelow:
"No Hate Speech": The chatbot shall not generate content that expresses, in-
* "No Hate Speech": The prompt shall not contain or seek generation of con-
cites,orpromoteshatebasedonrace,gender,ethnicity,religion,nationality,sexual
tentthatexpresses,incites,orpromoteshatebasedonrace,gender,ethnicity,religion,
orientation,disabilitystatus,orcaste.
nationality,sexualorientation,disabilitystatus,orcaste.
<start_of_turn>
ChatbotResponse:[ModelResponse]
===
<end_of_turn>
Does the human question violate the above principle? Your answer must
Does the Chatbot Response violate the above principle? Your answer must
startwith‚ÄôYes‚Äôor‚ÄôNo‚Äô. Andthenwalkthroughstepbysteptobesureweanswer
startwith‚ÄôYes‚Äôor‚ÄôNo‚Äô. Andthenwalkthroughstepbysteptobesureweanswer
correctly.
correctly.
Figure 2 | Instructions for Supervised Fine-Tuning. Left: User Input use case; Right: Model Output
use case.
Prompt Classification Response Classification
SG Prompt OpenAI Mod ToxicChat SG Response
ShieldGemma (2B) 0.825/0.887 0.812/0.887 0.704/0.778 0.743/0.802
ShieldGemma (9B) 0.828/0.894 0.821/0.907 0.694/0.782 0.753/0.817
ShieldGemma (27B) 0.830/0.883 0.805/0.886 0.729/0.811 0.758/0.806
OpenAI Mod API 0.782/0.840 0.790/0.856 0.254/0.588 -
LlamaGuard1 (7B) - 0.758/0.847 0.616/0.626 -
LlamaGuard2 (8B) - 0.761/- 0.471/- -
WildGuard (7B) 0.779/- 0.721/- 0.708/- 0.656/-
GPT-4 0.810/0.847 0.705/- 0.683/- 0.713/0.749
Table 1 | Evaluation results based on Optimal F1(left)/AU-PRC(right), higher is better. We use ùõº = 0
andùëá = 1 for calculating the probabilities. ShieldGemma (SG) Prompt and SG Response are our test
datasets and OpenAI Mod/ToxicChat are external benchmarks. On average, both our 9B and 27B
model perform the best. The performance of baseline models on external datasets is sourced from
Ghosh et al. (2024); Inan et al. (2023).
multiple binary classification problems, where the style of either user prompt or model output.
eachclassifierfocusesondistinguishingpositive Here, we run inference by treating the text as
examplesinonespecificharmtypeandtreatall model output and keep empty user prompt.
others as benign examples.
ToxicChat (Lin et al., 2023) contains 10ùëò exam-
ples with binary toxicity label for the prompt.
We directly maximize our predictions for the six
Benchmark Datasets and Baseline Models
harms according to our policy, as our harm types
capturedifferentaspectsofthetoxicitydefinitions
OpenAI Moderation (Markov et al., 2023) com-
outlined in the ToxicChat policy.
prises 1,680 prompt examples labeled for eight
safety categories: sexual, hate, violence, harass-
ShieldGemma Prompt & ShieldGemma Re-
ment, self-harm, sexual/minors, hate/threatening,
sponse are our test dataset. it contains 4,500
violence/graphic. Given that the original OpenAI
examples with labels in total for both use cases.
Moderation policy definitions differ from ours,
Theyhavelabelsforourtargetedharmtypessex-
particularly we do not directly predict self-harm,
ual, dangerous content, harassment, hate speech
we utilize those original definitions to predict
and non-targeted types violence and obscenity.
eachharmandthenaggregatethemintoanover-
More details are in section Data Annotation.
all binary classification. The dataset is sourced
Baseline Models: We evaluate our models
from CommonCrawl which does not match with
6ShieldGemma:GenerativeAIContentModerationBasedonGemma
againstseveralmodels: OpenAIModAPI(Markov Limitations
et al., 2023), LlamaGuard (Team, 2024b), Wild-
Guard Han et al. (2024), and GPT-4. For GPT-4, Despite our efforts to enhance the robustness of
we utilize the openAI API (model=gpt-4-0613) our model against adversarial attacks, fairness,
with our prompts, obtaining the log probability and diversity in the training data, several limita-
of the first token and converting it into the prob- tions remain:
ability of a policy violation.
Fairness: While we have implemented fairness
counterfactual expansion to mitigate bias in our
training data, label discrepancies may still arise
whenidentitygroupsareswapped. Thesediscrep-
Overall Binary Classification Results
anciesoftenstemfrominherentbiaseswithinthe
The overall binary classification results are pre- pre-training dataset (Chen et al., 2024).
sented in Table 1. All ShieldGemma (SG) models
Generalization: We have observed that our
(2B,9Band27B)outperformallbaselinemodels.
larger models demonstrate stronger performance
Notably,withsimilarmodelsizeandtrainingdata
onexternalbenchmarkswithnewharmtypesand
volume, SG-9B achieves a 10.8% higher average
text styles. Overall, this generalization capability
AU-PRC compared to LlamaGuard1 on external
of our larger models are slightly stronger than
benchmarks. Additionally, the F1 score of our 9B
our smaller 2B model. It also requires additional
model exceeds that of WildGuard and GPT-4 by
experiments to further verify the generalization
4.3% and 6.4%, respectively.
on other datasets.
Within the SG models, performance is com-
Implicit Cultural Harm: Although LLMs exhibit
parable on our internal benchmarks. On exter-
some understanding of cultural contexts, they
nalbenchmarks,the9B/27Bmodeldemonstrates
may struggle to fully grasp implicit harm within
slightlystrongergeneralizationcapability,achiev-
these contexts.
ingonaveragea1.2%/1.7%higherAU-PRCthan
its 2B model. Safety vs. Helpfulness: While our models
demonstrate a strong ability to filter potential
safety risks, their interpretation of policy viola-
tionsmaybeoverlyconservative. Thiscouldinter-
Harm Type Level Results
fere with helpfulness when used to filter LLM re-
We evaluate the harm-type level performance on sponses. Werecommendthatdownstreamclients
our test datasets: SG Prompt and SG Response. adjust filtering thresholds based on their specific
The results are shown in Fig. 3. All SG models use cases.
have outperformed GPT-4 by a big margin for
LLM-as-a-classifier: Our model is specifically de-
all of the harms. Overall, GPT-4 is weak in dis-
signed for classification tasks, with an output re-
tinguishing different harms. For example 76%
strictedtoYesorNotokenasthefirstoutputtoken
of hate speech data points have been classified
when the prompt is correctly configured. How-
as positive for harassment. Note that the perfor-
ever, it‚Äôs crucial to acknowledge that as an LLM,
mance gap is expected, and the comparison is
itremainscapableofgeneratingresponsestoany
less favorable for GPT-4, as our model has been
text input. We strongly advise the users to use
trained on datasets similar to the test datasets,
it solely for generating Yes/No token scores
while GPT-4 is evaluated zero-shot without any
(we call it scoring mode, detailed in our model
specific training. The performance among SG
card), and avoid using it in a chat-like manner
models is close to each other. On average, SG-
since it may produce unethical or unsafe content
9B and SG-27B have outperformed SG-2B by less
duetotheabsenceofadditionalsafetyinstruction-
than 2%.
tuning for conversational use.
We are dedicated to ongoing research and de-
7ShieldGemma:GenerativeAIContentModerationBasedonGemma
Figure 3 | Harm Type level performance (AU-PRC) for our test dataset SG Prompt (left) and SG
Response (right).
velopment to address these limitations and fur-
ther refine our classifiers.
Conclusion
This paper presents a significant advancement
in safety content moderation through our suite
of specialized models, built on the foundation of
thepublicGemma2(Teametal.,2024)language
models. We demonstrate their superior perfor-
mance on diverse benchmarks, highlighting the
effectiveness of our approach. Additionally, our
novel synthetic data generation pipeline offers
a valuable tool for researchers and practitioners
to create high-quality, diverse datasets for safety
andotherdomains. Weareexcitedtosharethese
resources with the research community to foster
further development in this critical area.
8ShieldGemma:GenerativeAIContentModerationBasedonGemma
Contributions and Acknowledgments References
Core Contributors J. Achiam, S. Adler, S. Agarwal, L. Ahmad,
Wenjun Zeng I. Akkaya, F. L. Aleman, D. Almeida, J. Al-
Yuchi Liu tenschmidt, S. Altman, S. Anadkat, et al.
Ryan Mullins Gpt-4 technical report. arXiv preprint
Ludovic Peran arXiv:2303.08774, 2023.
Contributors
A. Anthropic. The claude 3 model family: Opus,
Joe Fernandez
sonnet, haiku. Claude-3 Model Card, 1, 2024.
Hamza Harkous
Karthik Narasimhan J.T.Ash,C.Zhang,A.Krishnamurthy,J.Langford,
Drew Proud and A. Agarwal. Deep batch active learning by
Piyush Kumar diverse,uncertaingradientlowerbounds.arXiv
Bhaktipriya Radharapu preprint arXiv:1906.03671, 2019.
Olivia Sturman
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen,
Oscar Wahltinez
N. DasSarma, D. Drain, S. Fort, D. Ganguli,
Other Specialty Areas T. Henighan, et al. Training a helpful and
Special thanks and acknowledgments to these harmless assistant with reinforcement learn-
individualsfortheirassistanceinrespectedareas: ing from human feedback. arXiv preprint
arXiv:2204.05862, 2022.
Central Support
Manvinder Singh
G.H.Chen,S.Chen,Z.Liu,F.Jiang,andB.Wang.
Kathy Meier-Hellstern
Humansorllmsasthejudge? astudyonjudge-
Shivani Podder
ment biases. arXiv preprint arXiv:2402.10669,
Checkpoint Conversions 2024.
Nam T. Nguyen
G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas,
Matthew Watson
A. Rajagopalan, A. Rostamizadeh, and S. Ku-
Ethics and Safety mar. Batch active learning at scale. Advances
Antonia Paterson in Neural Information Processing Systems, 34:
Jenny Brennan 11933‚Äì11944, 2021.
Gemma Model
Y. Deng, W. Lei, M. Huang, and T.-S. Chua. Re-
Surya Bhupatiraju
thinking conversational agents in the era of
Victor Cotruta
llms: Proactivity, non-collaborativity, and be-
Armand Joulin
yond. InProceedingsoftheAnnualInternational
Kathleen Kenealy
ACMSIGIRConferenceonResearchandDevelop-
Tris Warkentin
ment in Information Retrieval in the Asia Pacific
Go-to-Market Region, pages 298‚Äì301, 2023.
Kat Black
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova.
Meg Risdal
Bert: Pre-training of deep bidirectional trans-
Team Acknowledgements formers for language understanding. arXiv
Our work is made possible by the dedication and preprint arXiv:1810.04805, 2018.
efforts of numerous teams at Google. We would
like to acknowledge the support from the follow- J.Gao,R.Pi,Y.Lin,H.Xu,J.Ye,Z.Wu,W.Zhang,
ing teams: Gemma, Google DeepMind Responsi- X. Liang, Z. Li, and L. Kong. Self-guided
bility, Kaggle, Keras, Perspective. noise-free data generation for efficient zero-
shotlearning. arXivpreprintarXiv:2205.12679,
2022.
9ShieldGemma:GenerativeAIContentModerationBasedonGemma
S.Ghosh,P.Varshney,E.Galinkin,andC.Parisien. for large language models. arXiv preprint
Aegis: Online adaptive ai content safety mod- arXiv:2402.05044, 2024.
eration with ensemble of llm experts. arXiv
preprint arXiv:2404.05993, 2024. Z. Lin, Z. Wang, Y. Tong, Y. Wang, Y. Guo,
Y. Wang, and J. Shang. Toxicchat: Unveil-
Google. Perspective api. https://www. ing hidden challenges of toxicity detection in
perspectiveapi.com/, 2017. real-world user-ai conversation, 2023. URL
https://arxiv.org/abs/2310.17389.
Google. Responsible generative ai toolkit:
https://ai.google.dev/responsible/principles,
N. Liu, L. Chen, X. Tian, W. Zou, K. Chen, and
2024.
M. Cui. From llm to conversational agent:
A memory enhanced architecture with fine-
S. Han, K. Rao, A. Ettinger, L. Jiang, B. Y. Lin,
tuningoflargelanguagemodels. arXivpreprint
N. Lambert, Y. Choi, and N. Dziri. Wildguard:
arXiv:2401.02777, 2024.
Openone-stopmoderationtoolsforsafetyrisks,
jailbreaks, and refusals of llms. arXiv preprint
L. Long, R. Wang, R. Xiao, J. Zhao, X. Ding,
arXiv:2406.18495, 2024.
G.Chen,andH.Wang.Onllms-drivensynthetic
H.Huang,Y.Qu,J.Liu,M.Yang,andT.Zhao. An data generation, curation, and evaluation: A
empiricalstudyofllm-as-a-judgeforllmevalua- survey. arXiv preprint arXiv:2406.15126, 2024.
tion: Fine-tuned judge models are task-specific
T. Markov, C. Zhang, S. Agarwal, F. E. Nekoul,
classifiers. arXiv preprint arXiv:2403.02839,
T. Lee, S. Adler, A. Jiang, and L. Weng. A holis-
2024.
tic approach to undesired content detection
H. Inan, K. Upasani, J. Chi, R. Rungta, K. Iyer, in the real world. In Proceedings of the AAAI
Y. Mao, M. Tontchev, Q. Hu, B. Fuller, D. Tes- Conference on Artificial Intelligence, volume 37,
tuggine, et al. Llama guard: Llm-based input- pages 15009‚Äì15018, 2023.
output safeguard for human-ai conversations.
arXiv preprint arXiv:2312.06674, 2023. M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang,
N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li,
J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, D. Forsyth, and D. Hendrycks. Harmbench:
C.Zhang,R.Sun,Y.Wang,andY.Yang. Beaver- A standardized evaluation framework for auto-
tails: Towards improved safety alignment of mated red teaming and robust refusal, 2024.
llmviaahuman-preferencedataset,2023. URL URL https://arxiv.org/abs/2402.04249.
https://arxiv.org/abs/2307.04657.
M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.
S. Y. Kim, H. Park, K. Shin, and K.-M. Kim.
Cooper, D. Ippolito, C. A. Choquette-Choo,
Ask me what you need: Product retrieval us-
E. Wallace, F. Tram√®r, and K. Lee. Scal-
ing knowledge from gpt-3. arXiv preprint
able extraction of training data from (pro-
arXiv:2207.02516, 2022.
duction) language models. arXiv preprint
arXiv:2311.17035, 2023.
A. Kurakin, N. Ponomareva, U. Syed, L. Mac-
Dermed, and A. Terzis. Harnessing large-
B. Radharapu, K. Robinson, L. Aroyo, and P. La-
language models to generate private synthetic
hoti.Aart: Ai-assistedred-teamingwithdiverse
text. arXiv preprint arXiv:2306.01684, 2023.
data generation for new llm-powered applica-
M. J. Kusner, J. Loftus, C. Russell, and R. Silva. tions. arXiv preprint arXiv:2311.08592, 2023.
Counterfactual fairness. Advances in neural
G. Sahu, P. Rodriguez, I. H. Laradji, P. Atighe-
information processing systems, 30, 2017.
hchian, D. Vazquez, and D. Bahdanau. Data
L. Li, B. Dong, R. Wang, X. Hu, W. Zuo, D. Lin, augmentation for intent classification with off-
Y. Qiao, and J. Shao. Salad-bench: A hierar- the-shelflargelanguagemodels. arXivpreprint
chical and comprehensive safety benchmark arXiv:2204.01959, 2022.
10ShieldGemma:GenerativeAIContentModerationBasedonGemma
O. Sener and S. Savarese. Active learning for
convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489,
2017.
E. M. Smith, M. Hall, M. Kambadur, E. Presani,
and A. Williams. " i‚Äôm sorry to hear that":
Finding new biases in language models with
a holistic descriptor dataset. arXiv preprint
arXiv:2205.09209, 2022.
G. Team. Gemma. 2024a. doi: 10.34740/
KAGGLE/M/3301. URL https://www.kaggle.
com/m/3301.
G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B.
Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M.
Dai,A.Hauth,etal. Gemini: afamilyofhighly
capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023.
G. Team, T. Mesnard, C. Hardin, R. Dadashi,
S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivi√®re,
M. S. Kale, J. Love, et al. Gemma: Open mod-
els based on gemini research and technology.
arXiv preprint arXiv:2403.08295, 2024.
L. Team. Meta llama guard 2. https:
//github.com/meta-llama/PurpleLlama/blob/
main/Llama-Guard2/MODEL_CARD.md, 2024b.
X. Zhan, Z. Liu, P. Luo, X. Tang, and C. Loy. Mix-
and-match tuning for self-supervised semantic
segmentation. InProceedingsoftheAAAIconfer-
ence on artificial intelligence, volume 32, 2018.
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang,
Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing,
et al. Judging llm-as-a-judge with mt-bench
and chatbot arena. Advances in Neural Infor-
mation Processing Systems, 36, 2024.
11