PublishedasaconferencepaperatICLR2024
EFFICIENT EPISODIC MEMORY UTILIZATION OF COOP-
ERATIVE MULTI-AGENT REINFORCEMENT LEARNING
HyunghoNa,YunkyeongSeo&Il-ChulMoon
KoreaAdvancedInstituteofScienceandTechnology(KAIST),Daejeon34141,SouthKorea
{gudgh723}@gmail.com,{tjdbsrud,icmoon}@kaist.ac.kr
ABSTRACT
Incooperativemulti-agentreinforcementlearning(MARL),agentsaimtoachieve
acommongoal,suchasdefeatingenemiesorscoringagoal. ExistingMARLalgo-
rithmsareeffectivebutstillrequiresignificantlearningtimeandoftengettrapped
inlocaloptimabycomplextasks,subsequentlyfailingtodiscoveragoal-reaching
policy. Toaddressthis,weintroduceEfficientepisodicMemoryUtilization(EMU)
forMARL,withtwoprimaryobjectives: (a)acceleratingreinforcementlearning
byleveragingsemanticallycoherentmemoryfromanepisodicbufferand(b)selec-
tivelypromotingdesirabletransitionstopreventlocalconvergence. Toachieve(a),
EMUincorporatesatrainableencoder/decoderstructurealongsideMARL,creating
coherentmemoryembeddingsthatfacilitateexploratorymemoryrecall. Toachieve
(b),EMUintroducesanovelrewardstructurecalledepisodicincentivebasedon
thedesirabilityofstates. ThisrewardimprovestheTDtargetinQ-learningandacts
asanadditionalincentivefordesirabletransitions. Weprovidetheoreticalsupport
fortheproposedincentiveanddemonstratetheeffectivenessofEMUcomparedto
conventionalepisodiccontrol. TheproposedmethodisevaluatedinStarCraftII
andGoogleResearchFootball,andempiricalresultsindicatefurtherperformance
improvementoverstate-of-the-artmethods.
1 INTRODUCTION
Recently,cooperativeMARLhasbeenadoptedtomanyapplications,includingtrafficcontrol(Wier-
ingetal.,2000),resourceallocation(Dandanovetal.,2017),robotpathplanning(Wangetal.,2020a),
andproductionsystems (Dittrich&Fohlmeister,2020),etc. Inspiteofthesesuccessfulapplications,
cooperative MARL still faces challenges in learning proper coordination among multiple agents
becauseofthepartialobservabilityandtheinteractionbetweenagentsduringtraining.
To address these challenges, the framework of centralized training and decentralized execution
(CTDE)(Oliehoeketal.,2008;Oliehoek&Amato,2016;Guptaetal.,2017)hasbeenproposed.
CTDEenablesadecentralizedexecutionwhilefullyutilizingglobalinformationduringcentralized
training, so CTDE improves policy learning by accessing to global states at the training phase.
Especially,valuefactorizationapproaches(Sunehagetal.,2017;Rashidetal.,2018;Sonetal.,2019;
Yangetal.,2020;Rashidetal.,2020;Wangetal.,2020b)maintaintheconsistencybetweenindividual
andjointactionselection,achievingthestate-of-the-artperformanceondifficultmulti-agenttasks,
suchasStarCraftIIMulti-agentChallenge(SMAC)(Samvelyanetal.,2019). However,learning
optimalpolicyinMARLstillrequiresalongconvergencetimeduetotheinteractionbetweenagents,
andthetrainedmodelsoftenfallintolocaloptima,particularlywhenagentsperformcomplextasks
(Mahajanetal.,2019). Hence,researcherspresentacommittedexplorationmechanismunderthis
CTDEtrainingpractice(Mahajanetal.,2019;Yangetal.,2019;Wangetal.,2019;Liuetal.,2021)
withtheexpectationtofindepisodesescapingfromthelocaloptima.
DespitetherequiredexplorationinMARLwithCTDE,recentworksonepisodiccontrolemphasize
theexploitationofepisodicmemorytoexpeditereinforcementlearning. Episodiccontrol(Lengyel
& Dayan, 2007; Blundell et al., 2016; Lin et al., 2018; Pritzel et al., 2017) memorizes explored
states and their best returns from experience in the episodic memory, to converge on the best
policy. Recently, thisepisodiccontrolhasbeenadoptedtoMARL(Zhengetal.,2021), andthis
episodiccontrolcaseshowsfasterconvergencethanthelearningwithoutsuchmemory. Whereas
1
4202
raM
2
]GL.sc[
1v21110.3042:viXraPublishedasaconferencepaperatICLR2024
therearemeritsfromepisodicmemoryandcontrolfromitsutilization, thereexistsaproblemof
determiningwhichmemoriestorecallandhowtousethem,toefficientlyexplorefromthememory.
Accordingto(Blundelletal.,2016;Linetal.,2018;Zhengetal.,2021),thepreviousepisodiccontrol
generally utilizes a random projection to embed global states, but this random projection hardly
makes the semantically similar states close to one another in the embedding space. In this case,
explorationwillbelimitedtoanarrowdistancethreshold. However,thissmallthresholdleadsto
inefficientmemoryutilizationbecausetherecallofepisodicmemoryundersuchsmallthresholds
retrievesonlythesamestatewithoutconsiderationofsemanticsimilarityfromtheperspectiveof
goalachievement. Additionally,thenaiveutilizationofepisodiccontroloncomplextasksinvolves
theriskofconvergingtolocaloptimabyrepeatedlyrevisitingpreviouslyexploredstates,favoring
exploitationoverexploration.
Contribution. ThispaperpresentsanEfficientepisodicMemoryUtilizationformulti-agentrein-
forcementlearning(EMU),aframeworktoselectivelyencouragedesirabletransitionswithsemantic
memoryembeddings.
â€¢ Efficientmemoryembedding: Whengeneratingfeaturesofaglobalstateforepisodic
memory(Figure1(b)),weadoptanencoder/decoderstructurewhere1)anencoderembeds
aglobalstateconditionedontimestepintoalow-dimensionalfeatureand2)adecodertakes
thisfeatureasaninputconditionedonthetimesteptopredictthereturnoftheglobalstate.
Inaddition,toensuresmootherembeddingspace,wealsoconsiderthereconstructionof
the global state when training the decoder to predict its return. To this end, we develop
deterministicConditionalAutoEncoder(dCAE)(Figure1(c)).Withthisstructure,important
featuresforoverallreturncanbecapturedintheembeddingspace. Theproposedembedding
containssemanticmeaningandthusguaranteesagradualchangeoffeaturespace,makingthe
furtherexplorationonmemoryspacenearthegivenstate,i.e.,efficientmemoryutilization.
â€¢ Episodicincentivegeneration: Whilethesemanticembeddingprovidesaspacetoexplore,
we still need to identify promising state transitions to explore. Therefore, we define a
desirabletrajectoryrepresentingthehighestreturnpath,suchasdestroyingallenemiesin
SMACorscoringagoalinGoogleResearchFootball(GRF)(Kurachetal.,2020). States
onthistrajectoryaremarkedasdesirableinepisodicmemory,sowecouldincentivizethe
explorationonsuchstatesaccordingtotheirdesirability. Wenamethisincentivestructure
as an episodic incentive (Figure 1(d)), encouraging desirable transitions and preventing
convergencetounsatisfactorylocaloptima. Weprovidetheoreticalanalysesdemonstrating
thatthisepisodicincentiveyieldsabettergradientsignalcomparedtoconventionalepisodic
control.
WeevaluateEMUonSMACandGRF,andempiricalresultsdemonstratethattheproposedmethod
achievesfurtherperformanceimprovementcomparedtothestate-of-artbaselinemethods. Ablation
studiesandqualitativeanalysesvalidatethepropositionsmadebythispaper.
2 PRELIMINARY
2.1 DECENTRALIZEDPOMDP
A fully cooperative multi-agent task can be formalized by following the Decentralized Par-
tially Observable Markov Decision Process (Dec-POMDP) (Oliehoek & Amato, 2016), G =
âŸ¨I,S,A,P,R,â„¦,O,n,Î³âŸ©, where I is the finite set of n agents; s âˆˆ S is the true state of the
environment; a âˆˆ Aisthei-thagentâ€™sactionformingthejointactiona âˆˆ An; P(sâ€²|s,a)isthe
i
statetransitionfunction;Risarewardfunctionr =R(s,a,sâ€²)âˆˆR;â„¦istheobservationspace;O
istheobservationfunctiongeneratinganobservationforeachagento âˆˆâ„¦;andfinally,Î³ âˆˆ[0,1)
i
is a discount factor. At each timestep, an agent has its own local observation o , and the agent
i
selectsanactiona âˆˆA. Thecurrentstatesandthejointactionofallagentsaleadtoanextstate
i
sâ€² according to P(sâ€²|s,a). The joint variable of s, a, and sâ€² will determine the identical reward
r across the multi-agent group. We additionally define key notations: each agent utilizes a local
action-observation history Ï„ âˆˆ T â‰¡ (â„¦Ã—A) for its policy Ï€ (a|Ï„ ), where Ï€ : T Ã—A â†’ [0,1]
i i i
(Hausknecht&Stone,2015;Rashidetal.,2018).
2PublishedasaconferencepaperatICLR2024
2.2 DESIRABILITYANDDESIRABLETRAJECTORY
Definition1. (DesirabilityandDesirableTrajectory)ForagiventhresholdreturnR andatrajectory
thr
T :={s ,a ,r ,s ,a ,r ,...,s },T isconsideredasadesirabletrajectory,denotedasT ,when
0 0 0 1 1 1 T Î¾
anepisodicreturnisR = Î£Tâˆ’1r â‰¥ R . AbinaryindicatorÎ¾(Â·)denotesthedesirabilityof
t=0 tâ€²=t tâ€² thr
states asÎ¾(s )=1whens âˆˆâˆ€T .
t t t Î¾
IncooperativeMARLtasks,suchasSMACandGRF,thetotalamountofrewardsfromtheenviron-
mentwithinanepisodeisoftenlimitedasR ,whichisonlygivenwhencooperativeagentsachieve
max
acommongoal. Insuchacase, wecansetR = R . Forfurtherdescriptionofcooperative
thr max
MARL,pleaseseeAppendixA.
2.3 EPISODICCONTROLINMARL
Episodiccontrolwasintroducedfromtheanalogyofabrainâ€™shippocampusformemoryutilization
(Lengyel&Dayan,2007). AftertheintroductionofdeepQ-network,Blundelletal.(2016)adopted
thisideaofepisodiccontroltothemodel-freesettingbystoringthehighestreturnofagivenstate,
toefficientlyestimatetheQ-valuesofthestate. Thisrecallingofthehigh-rewardexperienceshelps
toincreasesampleefficiencyandthusexpeditestheoveralllearningprocess(Blundelletal.,2016;
Pritzeletal.,2017;Linetal.,2018). PleaseseeAppendixAforrelatedworksandfurtherdiscussions.
Attimestept,letusdefineaglobalstateass . Whenutilizingepisodiccontrol,insteadofdirectly
t
using s , researchers adopt a state embedding function f (s):S â†’Rk to project states toward
t Ï•
a k-dimensional vector space. With this projection, a representation of global state s becomes
t
x =f (s ). TheepisodiccontrolmemorizesH(f (s )),i.e.,thehighestreturnofagivenglobal
t Ï• t Ï• t
states ,inepisodicbufferD (Pritzeletal.,2017;Linetal.,2018;Zhengetal.,2021). Here,x is
t E t
usedasakeytothehighestreturn,H(x );asakey-valuepairinD . TheepisodiccontrolinLin
t E
etal.(2018)updatesH(x )withthefollowingrules.
t
(cid:26)
max{H(xË† ),R (s ,a )}, if||xË† âˆ’x || <Î´
H(x )= t t t t t t 2 (1)
t R (s ,a ), otherwise,
t t t
whereR (s ,a )isthereturnofagiven(s ,a );Î´isathresholdvalueofstate-embeddingdifference;
t t t t t
andxË† =f (sË†)isx =f (s )â€™snearestneighborinD .IfthereisnosimilarprojectedstatexË† such
t Ï• t t Ï• t E t
that||xË† âˆ’x || <Î´inthememory,thenH(x )keepsthecurrentR (s ,a ).Leveragingtheepisodic
t t 2 t t t t
memory,EMC(Zhengetal.,2021)presentstheone-stepTDmemorytargetQ (f (s ),a )as
EC Ï• t t
Q (f (s ),a )=r (s ,a )+Î³H(f (s )). (2)
EC Ï• t t t t t Ï• t+1
Then,thelossfunctionLEC fortrainingcanbeexpressedastheweightedsumofone-stepTDerror
Î¸
andone-stepTDmemoryerror,i.e.,MonteCarlo(MC)inferenceerror,basedonQ (f (s ),a ).
EC Ï• t t
LEC =(y(s,a)âˆ’Q (s,a;Î¸))2+Î»(Q (f (s),a)âˆ’Q (s,a;Î¸))2, (3)
Î¸ tot EC Ï• tot
wherey(s,a)isone-stepTDtarget;Q isthejointQ-valuefunctionparameterizedbyÎ¸;andÎ»isa
tot
scalefactor.
Problemoftheconventionalepisodiccontrolwithrandomprojection Randomprojectionis
usefulfordimensionalityreductionasitpreservesdistancerelationships,asdemonstratedbythe
Johnson-Lindenstrausslemma(Dasgupta&Gupta,2003). However,arandomprojectionadopted
for f (s) hardly has a semantic meaning in its embedding x , as it puts random weights on the
Ï• t
statefeatureswithoutconsideringthepatternsofdeterminingthestatereturns. Additionally,when
recallingthememoryfromD ,theprojectedstatex canabruptlychangeevenwithasmallchange
E t
ofs becausetheembeddingisnotbeingregulatedbythereturn. Thisresultsinasparseselection
t
of semantically similar memories, i.e. similar states with similar or better rewards. As a result,
conventionalepisodiccontrolusingrandomprojectiononlyrecallsidenticalstatesandreliesonits
ownMonte-Carlo(MC)returntoregulatetheone-stepTDtargetinference,limitingexplorationof
nearbystatesontheembeddingspace.
Theproblemintensifieswhenthehigh-returnstatesintheearlytrainingphaseareindeedlocaloptima.
Insuchcases,thenaiveutilizationofepisodiccontrolispronetoconvergeonlocalminima. Asa
result,forthesuperhardtasksofSMAC,EMC(Zhengetal.,2021)hadtodecreasethemagnitudeof
thisregularizationtoalmostzero,i.e.,notconsideringepisodicmemoriesanymore.
3PublishedasaconferencepaperatICLR2024
3 METHODOLOGY
This section introduces Efficient episodic Memory Utilization (EMU) (Figure 1). We begin by
explaininghowtoconstruct(1)semanticmemoryembeddingstobetterutilizetheepisodicmemory,
whichenablesmemoryrecallofsimilar,morepromisingstates. Tofurtherimprovememoryutiliza-
tion,asanalternativetotheconventionalepisodiccontrol,wepropose(2)episodicincentivethat
selectivelyencouragesdesirabletransitionswhilepreventinglocalconvergencetowardsundesirable
trajectories.
Environment (a) Standard Value
Factorization Framework
(ð‰,ð’‚,ð‰â€²,ð‘ ,ð‘Ÿ) ð‘Žð‘–ð‘–ð‘ =1 ð‘œð‘–â€² ð‘–ð‘ =1, ð‘Ÿð‘–ð‘–ð‘ =1
Replay Buffer ð‘«
Controller Mixing Mixing Network
ð‘„ð‘–(âˆ™;ðœƒ) Gradients
ð‘„ð‘¡ð‘œð‘¡=ð‘“(ð‘„1,ð‘„2,â‹¯,ð‘„ð‘›;ðœƒ)
ð‘ ,ð» ð‘  ,ð‘¡
ð‘Ÿð‘
FC FC FC
ð‘¥ ðœ‰=0
ð‘“ðœ™(âˆ™) ð‘  ð‘  ReLU ð‘¥ ReLU ReLU ð‘ Ò§ ð‘¥
FC
FC ð‘“ðœ™(âˆ™)
ReLU
â‹® â‹® â‹® ð»(ð‘ ),ð‘¡
ð‘¡
ð‘“ ðœ™(âˆ™) ð‘¡ FC
ð‘“ ðœ“(âˆ™)
ð»à´¥
ð‘ ,ð‘¡ ð›¿ð‘¥ ð‘Ÿð‘ ð‘¥â€² ðœ‰=1
(b) Episodic Buffer ð‘«ð‘¬ (c) State Embedding Structure (d) Episodic Incentive Generation
Figure1: OverviewofEMUframework.
3.1 SEMANTICMEMORYEMBEDDING
EpisodicMemoryConstructionToaddresstheproblemsofarandomprojectionadoptedinepisodic
control, we propose a trainable embedding function f (s) to learn the state embedding patterns
Ï•
affectedbythehighestreturn. Theproblemofalearnableembeddingnetworkf isthatthematch
Ï•
betweenH(f (s ))ands breakswheneverf isupdated. Hence,wesavetheglobalstates aswell
Ï• t t Ï• t
asapairofH andx inD ,sothatwecanupdatex=f (s)wheneverf isupdated. Inaddition,
t t E Ï• Ï•
westorethedesirabilityÎ¾ ofs accordingtoDefinition1. AppendixE.1illustratesthedetailsof
t
memoryconstructionproposedbythispaper.
LearningframeworkforStateEmbeddingWhentrainingf (s ),itiscriticaltoextractimportant
Ï• t
featuresofaglobalstatethataffectitsvalue,i.e.,thehighestreturn. Thus,weadditionallyadopt
a decoder structure HÂ¯ = f (x ) to predict the highest return H of s . We call this embedding
t Ïˆ t t t
functionasEmbNet,anditslearningobjectiveoff andf canbewrittenas
Ï• Ïˆ
L(Ï•,Ïˆ)=(H âˆ’f (f (s )))2. (4)
t Ïˆ Ï• t
Whenconstructingtheembeddingspace,wefoundthatanadditionalconsiderationofreconstruction
of state s conditioned on timestep t improves the quality of feature extraction and constitutes a
smoother embedding space. To this end, we develop the deterministic conditional autoencoder
(dCAE),andthecorrespondinglossfunctioncanbeexpressedas
L(Ï•,Ïˆ)=(cid:0) H âˆ’fH(f (s |t)|t)(cid:1)2 +Î» ||s âˆ’fs(f (s |t)|t)||2, (5)
t Ïˆ Ï• t rcon t Ïˆ Ï• t 2
wherefH predictsthehighestreturn;fs reconstructss ;Î» isascalefactor. Here,fH andfs
Ïˆ Ïˆ t rcon Ïˆ Ïˆ
sharethelowerpartofnetworksasillustratedinFigure1(c). AppendixC.1presentsthedetailsof
networkstructureoff andf ,andAlgorithm1inAppendixC.1presentsthelearningframeworkfor
Ï• Ïˆ
f andf . ThistrainingisconductedperiodicallyinparalleltotheRLpolicylearningonQ (Â·;Î¸).
Ï• Ïˆ tot
Figure2illustratestheresultoft-SNE(VanderMaaten&Hinton,2008)of50KsamplesofxâˆˆD
E
out of 1M memory data in training for 3s_vs_5z task of SMAC. Unlike supervised learning
withlabeldata,thereisnolabelforeachx . Thus,wemarkx withitspairofthehighestreturn
t t
H . ComparedtoarandomprojectioninFigure2(a),x viaf iswell-clustered,accordingtothe
t t Ï•
similarity of the embedded state and its return. This clustering of x enables us to safely select
t
4PublishedasaconferencepaperatICLR2024
(a)RandomProjection (b)EmbNet (c)dCAE
Figure2: t-SNEofsampledembeddingxâˆˆD . Colorsfromredtopurple(rainbow)representfrom
E
lowreturntohighreturn.
episodic memories around the key state s , which constitutes efficient memory utilization. This
t
memoryutilizationexpediteslearningspeedaswellasencouragesexplorationtoamorepromising
statesË† nears . AppendixFillustrateshowtodetermineÎ´ofEq. 1inamemory-efficientway.
t t
3.2 EPISODICINCENTIVE
With the learnable memory embedding for an efficient memory recall, how to use the selected
memories still remains a challenge because a naive utilization of episodic memory is prone to
convergeonlocalminima. Tosolvethisissue,weproposeanewrewardstructurecalledepisodic
incentiverpbyleveragingthedesirabilityÎ¾ofstatesinD . Beforederivingtheepisodicincentive
E
rp,wefirstneedtounderstandthecharacteristicsofepisodiccontrol. Inthissection,wedenotethe
jointQ-functionQ (Â·;Î¸)simplyasQ forconciseness.
tot Î¸
Theorem1. Givenatransition(s,a,r,sâ€²)andH(xâ€²),letL betheQ-learninglosswithadditional
Î¸
transition reward, i.e., L := (y(s,a)+rEC(s,a,sâ€²)âˆ’Q (s,a;Î¸))2 where rEC(s,a,sâ€²) :=
Î¸ tot
Î»(r(s,a)+Î³H(xâ€²)âˆ’Q (s,a)),thenâˆ‡ L =âˆ‡ LEC. (ProofinAppendixB.1)
Î¸ Î¸ Î¸ Î¸ Î¸
AsTheorem1suggests,wecangeneratethesamegradientsignalastheepisodiccontrolbyleveraging
theadditionaltransitionrewardrEC(s,a,sâ€²). However,rEC(s,a,sâ€²)accompaniesariskoflocal
convergenceasdiscussedinSection2.3. Therefore,insteadofapplyingrEC(s,a,sâ€²),wepropose
theepisodicincentiverp := Î³Î·Ë†(sâ€²)thatprovidesanadditionalrewardforthedesirabletransition
(s,a,r,sâ€²), such that Î¾(sâ€²) = 1. Here, Î·Ë†(sâ€²) estimates Î·âˆ—(sâ€²), which represents the difference
betweenthetruevalueVâˆ—(sâ€²)ofsâ€² andthepredictedvalueviatargetnetworkmax Q (sâ€²,aâ€²),
aâ€² Î¸âˆ’
definedas
Î·âˆ—(sâ€²):=Vâˆ—(sâ€²)âˆ’maxQ (sâ€²,aâ€²). (6)
Î¸âˆ’
aâ€²
Note that we do not know Vâˆ—(sâ€²) and subsequently Î·âˆ—(sâ€²). To accurately estimate Î·âˆ—(sâ€²) with
Î·Ë†(sâ€²), we use the expected value considering the current policy Ï€ as Î·Ë†(sâ€²) := E [Î·(sâ€²)] where
Î¸ Ï€Î¸
Î· âˆˆ [0,Î· (sâ€²)] for sâ€² âˆ¼ P(sâ€²|s,a âˆ¼ Ï€ ). Here, Î· (sâ€²) can be reasonably approximated by
max Î¸ max
usingH(f (sâ€²))inD . Then,withthecount-basedestimationÎ·Ë†(sâ€²),episodicincentiverpcanbe
Ï• E
expressedas
N (sâ€²) N (sâ€²)
rp =Î³Î·Ë†(sâ€²)=Î³E [Î·(sâ€²)]â‰ƒÎ³ Î¾ Î· (sâ€²)=Î³ Î¾ (H(f (sâ€²))âˆ’maxQ (sâ€²,aâ€²)),
Ï€Î¸ N call(sâ€²) max N call(sâ€²) Ï• aâ€² Î¸âˆ’
(7)
where N (sâ€²) is the number of visits on xË†â€² = NN(f (sâ€²)) âˆˆ D ; and N is the number of
call Ï• E Î¾
desirabletransitionfromxË†â€². Here,NN(Â·)representsafunctionforselectingthenearestneighbor.
FromTheorem1,thelossfunctionadoptingepisodiccontrolwithanalternativetransitionrewardrp
insteadofrEC canbeexpressedas
Lp =(r(s,a)+rp+Î³maxQ (sâ€²,aâ€²)âˆ’Q (s,a))2. (8)
Î¸ Î¸âˆ’ Î¸
aâ€²
Then,thegradientsignaloftheone-stepTDinferencelossâˆ‡ Lp withtheepisodicrewardrp =
Î¸ Î¸
Î³Î·Ë†(sâ€²)canbewrittenas
N (sâ€²)
âˆ‡ Lp =âˆ’2âˆ‡ Q (s,a)(âˆ†Îµ +rp)=âˆ’2âˆ‡ Q (s,a)(âˆ†Îµ +Î³ Î¾ Î· (sâ€²)), (9)
Î¸ Î¸ Î¸ Î¸ TD Î¸ Î¸ TD N (sâ€²) max
call
5PublishedasaconferencepaperatICLR2024
whereâˆ†Îµ =r(s,a)+Î³max Q (sâ€²,aâ€²)âˆ’Q (s,a)isone-stepinferenceTDerror. Here,the
TD aâ€² Î¸âˆ’ Î¸
gradient signal âˆ‡ Lp with the proposed episodic reward rp can accurately estimate the optimal
Î¸ Î¸
gradientsignalasfollows.
Theorem2. Letâˆ‡ Lâˆ— =âˆ’2âˆ‡ Q (s,a)(âˆ†Îµâˆ— )betheoptimalgradientsignalwiththetrueone
Î¸ Î¸ Î¸ Î¸ TD
step TD error âˆ†Îµâˆ— = r(s,a)+Î³Vâˆ—(sâ€²)âˆ’Q (s,a). Then, the gradient signal âˆ‡ Lp with the
TD Î¸ Î¸ Î¸
episodicincentiverpconvergestotheoptimalgradientsignalasthepolicyconvergestotheoptimal
policyÏ€âˆ—,i.e.,âˆ‡ Lp â†’âˆ‡ Lâˆ—asÏ€ â†’Ï€âˆ—. (ProofinAppendixB.2)
Î¸ Î¸ Î¸ Î¸ Î¸ Î¸ Î¸
Theorem2alsoimpliesthatthereex-
ists a certain bias in âˆ‡ LEC as de-
Î¸ Î¸
scribedinAppendixB.2.Besidesthe
propertyofconvergencetotheopti-
malgradientsignalpresentedinThe-
orem 2, the episodic incentive has
thefollowingadditionalcharacteris-
tics. (1) The episodic incentive is
only applied to the desirable transi-
tion. We can simply see that rp =
(a)3s5z (b)MMM2
Î³Î·Ë† = Î³E [Î·] â‰ƒ Î³Î· N /N
and if Î¾(sÏ€ â€²Î¸ ) = 0 thm enax NÎ¾ =ca 0ll , Figure3: Episodicincentive. Testtrajectoriesareplottedon
Î¾
yieldingrp â†’ 0. Subsequently,(2) theembeddedspacewithsampledmemoriesinD E,denoted
thereisnoneedtoadjustascalefac- withdottedmarkers. Starmarkersandnumbersrepresentthe
torbythetaskcomplexity. (3)The desirabilityofstateandtimestepintheepisode,respectively.
episodicincentivecanreducetherisk ColorrepresentsthesamesemanticsasFigure2.
ofoverestimationbyconsideringthe
expectedvalueofE [Î·]. InsteadofconsideringtheoptimisticÎ· ,thecount-basedestimation
rp = Î³Î·Ë† = Î³E [Î·Ï€ ]Î¸ can consider the randomness of the policy m Ï€a .x Figure 3 illustrates how the
Ï€Î¸ Î¸
episodicincentiveworkswiththedesirabilitystoredinD constructedbyAlgorithm2presentedin
E
AppendixE.1. InFigure3asweintended,high-valuestates(atsmalltimesteps)areclusteredclose
tothepurplezone,whilelow-valuestates(atlargetimesteps)arelocatedintheredzone.
3.3 OVERALLLEARNINGOBJECTIVE
ToconstructthejointQ-functionQ fromindividualQ oftheagenti,anyformofmixercanbe
tot i
used. Inthispaper,wemainlyadoptthemixerpresentedinQPLEX(Wangetal.,2020b)similarto
Zhengetal.(2021),whichguaranteesthecompleteIndividual-Global-Max(IGM)condition(Son
etal.,2019;Wangetal.,2020b). Consideringanyintrinsicrewardrc encouraginganexploration
(Zhengetal.,2021)ordiversity(Chenghaoetal.,2021),thefinallossfunctionfortheactionpolicy
learningfromEq. 8canbeextendedas
Lp =(cid:0) r(s,a)+rp+Î² rc+Î³max Q (sâ€²,aâ€²;Î¸âˆ’)âˆ’Q (s,a;Î¸)(cid:1)2 , (10)
Î¸ c aâ€² tot tot
whereÎ² isascalefactor. Notethattheepisodicincentiverp canbeusedinconjunctionwithany
c
formofintrinsicrewardrc beingproperlyannealedthroughoutthetraining. Again,Î¸denotesthe
parametersofnetworksrelatedtoactionpolicyQ andthecorrespondingmixernetworktogenerate
i
Q . FortheactionselectionviaQ,weadoptaGRUtoencodealocalaction-observationhistoryÏ„
tot
presentedin2.1similartoSunehagetal.(2017);Rashidetal.(2018);Wangetal.(2020b);butinEq.
10,wedenoteequationswithsinsteadofÏ„ forthecoherencewithderivationintheprevioussection.
AppendixE.2presentstheoveralltrainingalgorithm.
4 EXPERIMENTS
In this part, we have formulated our experiments with the intention of addressing the following
inquiriesdenotedasQ1-3.
â€¢ Q1. HowdoesEMUcomparetothestate-of-the-artMARLframeworks?
â€¢ Q2. Howdoestheproposedstateembeddingchangetheembeddingspaceandimprovethe
performance?
â€¢ Q3. Howdoestheepisodicincentiveimproveperformance?
6PublishedasaconferencepaperatICLR2024
Weconductexperimentsoncomplexmulti-agenttaskssuchasSMAC(Samvelyanetal.,2019)and
GRF(Kurachetal.,2020). TheexperimentscompareEMUagainstEMCadoptingepisodiccontrol
(Zhengetal.,2021). Also,weincludenotablebaselines,suchasvalue-basedMARLmethodsQMIX
(Rashidetal.,2018),QPLEX(Wangetal.,2020b),CDSencouragingindividualdiversity(Chenghao
etal.,2021). Particularly,weemphasizethatEMUcanbecombinedwithanyMARLframework,
sowepresenttwoversionsofEMUimplementedonoriginalQPLEXandCDS,denotedasEMU
(QPLEX)andEMU(CDS),respectively. AppendixCprovidesfurtherdetailsofexperimentsettings
andimplementations,andAppendixD.12providestheapplicabilityofEMUtosingle-agenttasks,
includingpixel-basedhigh-dimensionaltasks.
4.1 Q1. COMPARATIVEEVALUATIONONSTARCRAFTII(SMAC)
Figure4: PerformancecomparisonofEMUagainstbaselinealgorithmsonthreeeasyandhard
SMACmaps: 1c3s5z,3s_vs_5z,and5m_vs_6m,andthreesuperhardSMACmaps: MMM2,
6h_vs_8z,and3s5z_vs_3s6z.
Figure4illustratestheoverallperformanceofEMUonvariousSMACmaps. Themapcategorization
regardingthelevelofdifficultyfollowsthepracticeofSamvelyanetal.(2019).Thankstotheefficient
memoryutilizationandepisodicincentive,bothEMU(QPLEX)andEMU(CDS)showsignificant
performance improvement compared to their original methodologies. Especially, in super hard
SMACmaps,theproposedmethodmarkedlyexpeditesconvergenceonoptimalpolicy.
4.2 Q1. COMPARATIVEEVALUATIONONGOOGLERESEARCHFOOTBALL(GRF)
Here, we conduct experiments on GRF to further compare the performance of EMU with other
baseline algorithms. In our GRF task, CDS and EMU (CDS) do not utilize the agentâ€™s index on
observationastheycontainthepredictionnetworkswhileotherbaselines(QMIX,EMC,QPLEX)
useinformationoftheagentâ€™sidentityinobservations. Inaddition,wedonotutilizeanyadditional
algorithm, such as prioritized experience replay (Schaul et al., 2015), for all baselines and our
method,toexpeditelearningefficiency. Fromtheexperiments,adoptingEMUachievessignificant
performanceimprovement,andEMUquicklyfindsthewinningorscoringpolicyattheearlylearning
phasebyutilizingsemanticallysimilarmemory.
Figure5:PerformancecomparisonofEMUagainstbaselinealgorithmsonGoogleResearchFootball.
7PublishedasaconferencepaperatICLR2024
4.3 Q2. PARAMETRICANDABLATIONSTUDY
Inthissection,weexaminehowthekeyhyperparameterÎ´andthechoiceofdesignforf affectthe
Ï•
performance. Tocomparethelearningqualityandperformancemorequantitatively,weproposea
newperformanceindexcalledoverallwin-rate,ÂµÂ¯ . ThepurposeofÂµÂ¯ istoconsiderbothtraining
w w
efficiency(speed)andquality(win-rate)fordifferentseedcases(seeAppendixD.1fordetails). We
conductexperimentsonselectedSMACmapstomeasureÂµÂ¯ accordingtoÎ´anddesignchoiceforf
w Ï•
suchas(1)randomprojection,(2)EmbNetwithEq. 4and(3)dCAEwithEq. 5.
(a)3s_vs_5z (b)5m_vs_6m (a)3s_vs_5z (b)5m_vs_6m
Figure6: ÂµÂ¯ accordingtoÎ´ andvariousde- Figure7: Finalwin-rateaccordingtoÎ´ and
w
signchoicesforf onSMACmaps. variousdesignchoicesforf onSMACmaps.
Ï• Ï•
Figure 6 and Figure 7 show ÂµÂ¯ values and test win-rate at the end of training time according to
w
differentÎ´,presentedinlog-scale. Toseetheeffectofdesignchoiceforf distinctly,weconduct
Ï•
experimentswiththeconventionalepisodiccontrol. MoredataofÂµÂ¯ ispresentedinTables4and5in
w
AppendixD.2. Figure6illustratesthatdCAEstructureshowsthebesttrainingefficiencythroughout
variousÎ´whileachievingtheoptimalpolicyasotherdesignchoicesaspresentedinFigure7.
Interestingly,dCAEstructureworkswellwith
awiderrangeofÎ´thanEmbNet. Weconjecture
thatEmbNetcanselectverydifferentstatesas
explorationifthosestateshavesimilarreturnH
duringtraining. Thisexcessivememoryrecall
adversely affects learning and fails to find an
optimal policy as a result. See Appendix D.2
fordetailedanalysisandAppendixD.8foran
(a)CA_hard(GRF) (b)6h_vs_8z(SMAC)
ablationstudyonthelossfunctionofdCAE.
Figure8: EffectofvaryingÎ´oncomplexMARL
EventhoughawiderangeofÎ´workswellasin
tasks.
Figures6and7,choosingapropervalueofÎ´in
moredifficultMARLtaskssignificantlyimprovestheoveralllearningperformance. Figure8shows
thelearningcurveofEMUaccordingtoÎ´ =1.3eâˆ’7,Î´ =1.3eâˆ’5,Î´ =1.3eâˆ’3,andÎ´ =1.3eâˆ’2.
1 2 3 4
InsuperhardMARLtaskssuchas6h_vs_8zinSMACandCA_hardinGRF,Î´ showsthebest
3
performancecomparedtootherÎ´values. ThisisconsistentwiththevaluesuggestedinAppendix
F,whereÎ´ isdeterminedinamemory-efficientway. FurtherparametricstudyonÎ´ andÎ» are
rcon
presentedinAppendixD.5andD.6,respectively.
4.4 Q3. FURTHERABLATIONSTUDY
Inthissection,wecarryoutfurtherablationstudiestoseetheeffectofepisodicincentiverppresented
inSection3.2. FromEMU(QPLEX)andEMU(CDS),weablatetheepisodicincentiveanddenote
themwith(No-EI).Weadditionallyablateembeddingnetworkf fromEMUanddenotethemwith
Ï•
(No-SE).Inaddition, weablatebothparts, yieldingEMC(QPLEX-original)andCDS(QPLEX-
original).WeevaluatetheperformanceofeachmodelonsuperhardSMACmaps.Additionalablation
studiesonGRFmapsarepresentedinAppendixD.7. NotethatEMC(QPLEX-original)utilizesthe
conventionalepisodiccontrolpresentedinZhengetal.(2021).
Figure9illustratesthattheepisodicincentivelargelyaffectslearningperformance. Especially,EMU
(QPLEX-No-EI)andEMU(CDS-No-EI)utilizingtheconventionalepisodiccontrolshowalarge
performance variation according to different seeds. This demonstrates that a naive utilization of
episodiccontrolcouldbedetrimentaltolearninganoptimalpolicy. Ontheotherhand,theepisodic
incentiveselectivelyencouragestransitionconsideringdesirabilityandthuspreventssuchalocal
convergence. AppendixD.9andD.10presentanadditionalablationstudyonsemanticembedding
8PublishedasaconferencepaperatICLR2024
(a)6h_vs_8zSMAC (b)3s5z_vs_3s6zSMAC (c)3s5z_vs_3s6zSMAC
Figure9: AblationstudiesonepisodicincentiveviacomplexMARLtasks.
andrc,respectively. Inaddition,AppendixD.11presentsacomparisonwithanalternativeincentive
(Henaffetal.,2022)presentedinasingle-agentsetting.
4.5 QUALITATIVEANALYSISANDVISUALIZATION
In this section, we conduct analysis with visualization to check how the desirability Î¾ is
memorized in D and whether it conveys correct information. Figure 10 illustrates two test
E
scenarios with different seeds, and each snapshot is denoted with a corresponding timestep.
In Figure 11, the trajectory of each episode is projected onto the embedded space of D .
E
InFigure10,case(a)successfullyde-
ð‘¡=6 ð‘¡=10 ð‘¡=1102 ð‘¡=20
feated all enemies, whereas case (b)
losttheengagement. Bothcaseswent
throughasimilar,desirabletrajectory
atthebeginning. Forexample, until
(a)Desirabletrajectoryon5m_vs_6mSMACmap
t = 10agentsinbothcasesfocused
onkillingoneenemyandkeptallally ð‘¡=6 ð‘¡=10 ð‘¡=12 ð‘¡=20
agents alive at the same time. How-
ever,att=12,case(b)lostoneagent,
and two trajectories of case (a) and
(b)Undesirabletrajectoryon5m_vs_6mSMACmap
(b)inembeddedspacebegantobifur-
cate.Case(b)stillhadachancetowin
Figure10: Visualizationoftestepisodes.
aroundt = 14 âˆ¼ t = 16. However,
thestatesbecameundesirable(denotedwithoutstarmarker)afterlosingthreeallyagentsaround
t=20,andcase(b)lostthebattleasaresult. Thesesequencesandcharacteristicsoftrajectoriesare
wellcapturedbydesirabilityÎ¾inD asillustratedinFigure11.
E
Furthermore, the desirable state denoted with
Î¾ =1encouragesexplorationarounditthough
itisnotdirectlyretrievedduringbatchsampling.
Thisoccursthroughthepropagationofitsdesir-
abilitytostatescurrentlydistinguishedasunde-
sirableduringmemoryconstruction,usingAlgo-
rithm2inAppendixE.1. Consequently,when
(a)Desirabletrajectory (b)Undesirabletrajectory
the stateâ€™s desirability is precisely memorized
in D , it can encourage desirable transitions Figure11: Testtrajectoriesonembeddedspaceof
E
throughtheepisodicincentiverp. D .
E
5 CONCLUSION
ThispaperpresentsEMU,anewframeworktoefficientlyutilizeepisodicmemoryforcooperative
MARL. EMU introduces two major components: 1) a trainable semantic embedding and 2) an
episodicincentiveutilizingdesirabilityofstate. Semanticmemoryembeddingallowsustosafely
utilize similar memory in a wide area, expediting learning via exploratory memory recall. The
proposedepisodicincentiveselectivelyencouragesdesirabletransitionsandreducestheriskoflocal
convergence by leveraging the desirability of the state. As a result, there is no need for manual
hyperparametertuningaccordingtothecomplexityoftasks,unlikeconventionalepisodiccontrol.
ExperimentsandablationstudiesvalidatetheeffectivenessofeachcomponentofEMU.
9PublishedasaconferencepaperatICLR2024
ACKNOWLEDGEMENTS
ThisresearchwassupportedbyAITechnologyDevelopmentforCommonsenseExtraction,Rea-
soning, and Inference from Heterogeneous Data(IITP) funded by the Ministry of Science and
ICT(2022-0-00077).
REFERENCES
MarcBellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,andRemiMunos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information
processingsystems,29,2016.
MarcGBellemare,YavarNaddaf,JoelVeness,andMichaelBowling. Thearcadelearningenviron-
ment: Anevaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:
253â€“279,2013.
CharlesBlundell,BenignoUria,AlexanderPritzel,YazheLi,AvrahamRuderman,JoelZLeibo,
Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint
arXiv:1606.04460,2016.
YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov. Explorationbyrandomnetwork
distillation. arXivpreprintarXiv:1810.12894,2018.
Li Chenghao, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang.
Celebratingdiversityinsharedmulti-agentreinforcementlearning.AdvancesinNeuralInformation
ProcessingSystems,34:3991â€“4002,2021.
NikolayDandanov,HusseinAl-Shatri,AnjaKlein,andVladimirPoulkov. Dynamicself-optimization
oftheantennatiltforbesttrade-offbetweencoverageandcapacityinmobilenetworks. Wireless
PersonalCommunications,92(1):251â€“278,2017.
SanjoyDasguptaandAnupamGupta. Anelementaryproofofatheoremofjohnsonandlindenstrauss.
RandomStructures&Algorithms,22(1):60â€“65,2003.
Marc-AndrÃ©DittrichandSilasFohlmeister. Cooperativemulti-agentsystemforproductioncontrol
usingreinforcementlearning. CIRPAnnals,69(1):389â€“392,2020.
YaliDu,LeiHan,MengFang,JiLiu,TianhongDai,andDachengTao. Liir: Learningindividual
intrinsicrewardinmulti-agentreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,32,2019.
ScottFujimoto,HerkeHoof,andDavidMeger. Addressingfunctionapproximationerrorinactor-
criticmethods. InInternationalconferenceonmachinelearning,pp.1587â€“1596.PMLR,2018.
JayeshKGupta,MaximEgorov,andMykelKochenderfer. Cooperativemulti-agentcontrolusing
deepreinforcementlearning. InInternationalconferenceonautonomousagentsandmultiagent
systems,pp.66â€“83.Springer,2017.
MatthewHausknechtandPeterStone. Deeprecurrentq-learningforpartiallyobservablemdps. In
2015aaaifallsymposiumseries,2015.
Mikael Henaff, Roberta Raileanu, Minqi Jiang, and Tim RocktÃ¤schel. Exploration via elliptical
episodicbonuses. AdvancesinNeuralInformationProcessingSystems,35:37631â€“37646,2022.
ReinHouthooft, XiChen, YanDuan, JohnSchulman, FilipDeTurck, andPieterAbbeel. Vime:
Variational information maximizing exploration. Advances in neural information processing
systems,29,2016.
HaoHu,JianingYe,GuangxiangZhu,ZhizhouRen,andChongjieZhang. Generalizableepisodic
memoryfordeepreinforcementlearning. Internationalconferenceonmachinelearning,2021.
10PublishedasaconferencepaperatICLR2024
NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroOrtega,DJStrouse,
JoelZLeibo,andNandoDeFreitas. Socialinfluenceasintrinsicmotivationformulti-agentdeep
reinforcementlearning. InInternationalconferenceonmachinelearning,pp.3040â€“3049.PMLR,
2019.
Hyoungseok Kim, Jaekyeom Kim, Yeonwoo Jeong, Sergey Levine, and Hyun Oh Song. Emi:
Explorationwithmutualinformation. arXivpreprintarXiv:1810.01176,2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
KarolKurach,AntonRaichuk,PiotrStanczyk,MichalZajkc,OlivierBachem,LasseEspeholt,Carlos
Riquelme,DamienVincent,MarcinMichalski,OlivierBousquet,etal. Googleresearchfootball:
Anovelreinforcementlearningenvironment. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pp.4501â€“4510,2020.
LeiLe,AndrewPatterson,andMarthaWhite. Supervisedautoencoders: Improvinggeneralization
performancewithunsupervisedregularizers. Advancesinneuralinformationprocessingsystems,
31,2018.
MÃ¡tÃ©LengyelandPeterDayan. Hippocampalcontributionstocontrol: thethirdway. Advancesin
neuralinformationprocessingsystems,20,2007.
ZichuanLin,TianqiZhao,GuangwenYang,andLintaoZhang. Episodicmemorydeepq-networks.
arXivpreprintarXiv:1805.07603,2018.
Iou-JenLiu, UnnatJain, RaymondAYeh, andAlexanderSchwing. Cooperativeexplorationfor
multi-agentdeepreinforcementlearning. InInternationalConferenceonMachineLearning,pp.
6826â€“6836.PMLR,2021.
AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson. Maven: Multi-agent
variationalexploration. AdvancesinNeuralInformationProcessingSystems,32,2019.
DavidHenryMguni,TaherJafferjee,JianhongWang,NicolasPerez-Nieves,OliverSlumbers,Feifei
Tong,YangLi,JiangchengZhu,YaodongYang,andJunWang. Ligs: Learnableintrinsic-reward
generationselectionformulti-agentlearning. arXivpreprintarXiv:2112.02618,2021.
ShakirMohamedandDaniloJimenezRezende.Variationalinformationmaximisationforintrinsically
motivatedreinforcementlearning. Advancesinneuralinformationprocessingsystems,28,2015.
FransAOliehoekandChristopherAmato.AconciseintroductiontodecentralizedPOMDPs.Springer,
2016.
FransAOliehoek,MatthijsTJSpaan,andNikosVlassis. Optimalandapproximateq-valuefunctions
fordecentralizedpomdps. JournalofArtificialIntelligenceResearch,32:289â€“353,2008.
GeorgOstrovski,MarcGBellemare,AÃ¤ronOord,andRÃ©miMunos. Count-basedexplorationwith
neuraldensitymodels. InInternationalconferenceonmachinelearning,pp.2721â€“2730.PMLR,
2017.
DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell. Curiosity-drivenexploration
byself-supervisedprediction. InInternationalconferenceonmachinelearning,pp.2778â€“2787.
PMLR,2017.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech Badia, Oriol Vinyals,
DemisHassabis,DaanWierstra,andCharlesBlundell. Neuralepisodiccontrol. InInternational
ConferenceonMachineLearning,pp.2827â€“2836.PMLR,2017.
SanthoshKRamakrishnan,AaronGokaslan,ErikWijmans,OleksandrMaksymets,AlexClegg,John
Turner,EricUndersander,WojciechGaluba,AndrewWestbury,AngelXChang,etal. Habitat-
matterport3ddataset(hm3d): 1000large-scale3denvironmentsforembodiedai. arXivpreprint
arXiv:2109.08238,2021.
11PublishedasaconferencepaperatICLR2024
TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,andShi-
monWhiteson. Qmix: Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement
learning. InInternationalconferenceonmachinelearning,pp.4295â€“4304.PMLR,2018.
TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson. Weightedqmix: Expanding
monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcementlearning. Advancesin
neuralinformationprocessingsystems,33:10199â€“10210,2020.
MikayelSamvelyan,TabishRashid,ChristianSchroederDeWitt,GregoryFarquhar,NantasNardelli,
TimGJRudner, Chia-ManHung, PhilipHSTorr, JakobFoerster, andShimonWhiteson. The
starcraftmulti-agentchallenge. arXivpreprintarXiv:1902.04043,2019.
TomSchaul,JohnQuan,IoannisAntonoglou,andDavidSilver. Prioritizedexperiencereplay. arXiv
preprintarXiv:1511.05952,2015.
KihyukSohn,HonglakLee,andXinchenYan. Learningstructuredoutputrepresentationusingdeep
conditionalgenerativemodels. Advancesinneuralinformationprocessingsystems,28,2015.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.Qtran:Learningto
factorizewithtransformationforcooperativemulti-agentreinforcementlearning. InInternational
conferenceonmachinelearning,pp.5887â€“5896.PMLR,2019.
Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement
learningwithdeeppredictivemodels. arXivpreprintarXiv:1507.00814,2015.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,KarlTuyls,etal. Value-decomposition
networksforcooperativemulti-agentlearning. arXivpreprintarXiv:1706.05296,2017.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John
Schulman,FilipDeTurck,andPieterAbbeel. #exploration: Astudyofcount-basedexploration
fordeepreinforcementlearning. Advancesinneuralinformationprocessingsystems,30,2017.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pp.5026â€“5033.
IEEE,2012. doi: 10.1109/IROS.2012.6386109.
LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
BinyuWang,ZheLiu,QingbiaoLi,andAmandaProrok. Mobilerobotpathplanningindynamic
environmentsthroughgloballyguidedreinforcementlearning. IEEERoboticsandAutomation
Letters,5(4):6932â€“6939,2020a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,2020b.
TonghanWang,JianhaoWang,YiWu,andChongjieZhang. Influence-basedmulti-agentexploration.
arXivpreprintarXiv:1910.05512,2019.
TonghanWang,TarunGupta,AnujMahajan,BeiPeng,ShimonWhiteson,andChongjieZhang.Rode:
Learningrolestodecomposemulti-agenttasks. InProceedingsoftheInternationalConferenceon
LearningRepresentations(ICLR),2021.
Marco A Wiering et al. Multi-agent reinforcement learning for traffic light control. In Machine
Learning: ProceedingsoftheSeventeenthInternationalConference(ICMLâ€™2000),pp.1151â€“1158,
2000.
JiachenYang,IgorBorovikov,andHongyuanZha. Hierarchicalcooperativemulti-agentreinforce-
mentlearningwithskilldiscovery. arXivpreprintarXiv:1912.03558,2019.
12PublishedasaconferencepaperatICLR2024
YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyaoTang.
Qatten: Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXivpreprint
arXiv:2002.03939,2020.
ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,andYiWu. The
surprisingeffectivenessofppoincooperativemulti-agentgames. AdvancesinNeuralInformation
ProcessingSystems,35:24611â€“24624,2022.
LuluZheng,JiaruiChen,JianhaoWang,JiaminHe,YujingHu,YingfengChen,ChangjieFan,Yang
Gao, and Chongjie Zhang. Episodic multi-agent reinforcement learning with curiosity-driven
exploration. AdvancesinNeuralInformationProcessingSystems,34:3757â€“3769,2021.
Guangxiang Zhu, Zichuan Lin, Guangwen Yang, and Chongjie Zhang. Episodic reinforcement
learningwithassociativememory. Internationalconferenceonlearningrepresentations,2020.
13PublishedasaconferencepaperatICLR2024
A RELATED WORKS
This section presents the related works regarding incentive generation for exploration, episodic
control,andthecharacteristicsofcooperativeMARL.
A.1 INCENTIVEFORMULTI-AGENTEXPLORATION
Balancingbetweenexplorationandexploitationinpolicylearningisaparamountissueinreinforce-
mentlearning. Toencourageexploration,modifiedcount-basedmethods(Bellemareetal.,2016;
Ostrovskietal.,2017;Tangetal.,2017),predictionerror-basedmethods(Stadieetal.,2015;Pathak
etal.,2017;Burdaetal.,2018;Kimetal.,2018),andinformationgain-basedmethods(Mohamed&
JimenezRezende,2015;Houthooftetal.,2016)havebeenproposedforasingleagentreinforcement
learning. Inmostcases,anincentiveforexplorationisintroducedasanadditionalrewardtoaTD
targetinQ-learning;orsuchanincentiveisaddedasaregularizerforoveralllossfunctions. Recently,
various aforementioned methods to encourage exploration have been adopted to the multi-agent
setting(Mahajanetal.,2019;Wangetal.,2019;Jaquesetal.,2019;Mgunietal.,2021)andhave
shown their effectiveness. MAVEN (Mahajan et al., 2019) introduces a regularizer maximizing
themutualinformationbetweentrajectoriesandlatentvariablestolearnadiversesetofbehaviors.
LIIR(Duetal.,2019)learnsaparameterizedindividualintrinsicrewardfunctionbymaximizinga
centralizedcritic. CDS(Chenghaoetal.,2021)proposesanovelinformation-theoreticalobjectiveto
maximizethemutualinformationbetweenagentsâ€™identitiesandtrajectoriestoencouragediverse
individualized behaviors. EMC (Zheng et al., 2021) proposes a curiosity-driven exploration by
predictingindividualQ-values. Thisindividual-basedQ-valuepredictioncancapturetheinfluence
amongagentsaswellasthenoveltyofstates.
A.2 EPISODICCONTROL
Episodiccontrol(Lengyel&Dayan,2007)waswelladoptedonmodel-freesetting(Blundelletal.,
2016)bystoringthehighestreturnofagivenstate, toefficientlyestimateitsvaluesorQ-values.
Giventhatthesamplegenerationisoftenlimitedbysimulationexecutionsorreal-worldobservations,
itssampleefficiencyhelpstofindanaccurateestimationofQ-value(Blundelletal.,2016;Pritzel
etal.,2017;Linetal.,2018). NEC(Pritzeletal.,2017)usesadifferentiableneuraldictionaryas
anepisodicmemorytoestimatetheactionvaluebytheweightedsumofthevaluesinthememory.
EMDQN(Linetal.,2018)utilizesafixedrandommatrixtogenerateastaterepresentation,which
isusedasakeytolinkbetweenthestaterepresentationandthehighestreturnofthestateinthe
episodicmemory. ERLAM(Zhuetal.,2020)learnsassociativememoriesbybuildingagraphical
representationofstatesinmemory,andGEM(Huetal.,2021)developsstate-actionvaluesofepisodic
memoryinageneralizablemanner. Recently,EMC(Zhengetal.,2021)extendstheapproachof
EMDQN to a deep MARL with curiosity-driven exploration incentives. EMC utilizes episodic
memorytoregularizepolicylearningandshowsperformanceimprovementincooperativeMARL
tasks. However,EMCrequiresahyperparametertuningtodeterminethelevelofimportanceofthe
one-stepTDmemory-basedtargetduringtraining,accordingtothedifficultiesoftasks. Inthispaper,
weinterpretthisregularizationasanadditionaltransitionreward. Then,wepresentanovelformof
reward,calledepisodicincentive,toselectivelyencouragethetransitiontowarddesiredstates,i.e.,
statestowardacommongoalincooperativemulti-agenttasks.
A.3 COOPERATIVEMULTI-AGENTREINFORCEMENTLEARNING(MARL)TASK
Ingeneral, thereisacommongoalincooperativeMARLtasks, whichguaranteesthemaximum
returnthatcanbeobtainedfromtheenvironment. Thus,therecouldbemanylocaloptimawithhigh
returns but not the maximum, which means the agents failed to achieve the common goal in the
end. Inotherwords,thereisadistinctdifferencebetweentheobjectiveofcooperativeMARLtasks
andthatofasingle-agenttask,whichaimstomaximizethereturnasmuchaspossiblewithoutany
boundarydeterminingsuccessorfailure. Ourdesirabilitydefinitionpresentedin1inMARLsetting
becomeswelljustifiedfromthisview. UnderthischaracteristicofMARLtasks,learningoptimal
policyoftentakesalongtimeandevenfails,yieldingalocalconvergence. EMUwasdesignedto
alleviatetheseissuesinMARL.
14PublishedasaconferencepaperatICLR2024
B MATHEMATICAL PROOF
Inthissection,wepresenttheomittedproofsofTheorem1andTheorem2asfollows.
B.1 PROOFOFTHEOREM1
Proof. Thelossfunctionofaconventionalepisodiccontrol,LEC,canbeexpressedastheweighted
Î¸
sum of one-step inference TD error âˆ†Îµ = r(s,a)+Î³max Q (sâ€²,aâ€²)âˆ’Q (s,a) and MC
TD aâ€² Î¸âˆ’ Î¸
inferenceerrorâˆ†Îµ =Q (s,a)âˆ’Q (s,a).
EC EC Î¸
LEC =(r(s,a)+Î³maxQ (sâ€²,aâ€²)âˆ’Q (s,a))2+Î»(Q (s,a)âˆ’Q (s,a))2, (11)
Î¸ Î¸âˆ’ Î¸ EC Î¸
aâ€²
whereQ (s,a) = r(s,a)+Î³H(sâ€²)andQ isthetargetnetworkparameterizedbyÎ¸âˆ’. Then,
EC Î¸âˆ’
thegradientofLEC canbederivedas
Î¸
âˆ‡ LEC =âˆ’2âˆ‡ Q (s,a)[(r(s,a)+Î³maxQ (sâ€²,aâ€²)âˆ’Q (s,a))+Î»(Q (s,a)âˆ’Q (s,a))]
Î¸ Î¸ Î¸ Î¸ Î¸âˆ’ Î¸ EC Î¸
aâ€²
=âˆ’2âˆ‡ Q (s,a)(âˆ†Îµ +Î»âˆ†Îµ ).
Î¸ Î¸ TD EC
(12)
Now,weconsideranadditionalrewardrEC forthetransitiontoaconventionalQ-learningobjective,
themodifiedlossfunctionL canbeexpressedas
Î¸
L =(r(s,a)+rEC(s,a,sâ€²)+Î³maxQ (sâ€²,aâ€²)âˆ’Q (s,a))2. (13)
Î¸ Î¸âˆ’ Î¸
aâ€²
Then,thegradientofL presentedinEq. 13iscomputedas
Î¸
âˆ‡ L =âˆ’2âˆ‡ Q (s,a)(âˆ†Îµ +rEC). (14)
Î¸ Î¸ Î¸ Î¸ TD
Comparing Eq. 12 and Eq. 14, if we set the additional transition reward as rEC(s,a,sâ€²) =
Î»(r(s,a)+Î³H(sâ€²)âˆ’Q (s,a)),thenâˆ‡ L =âˆ‡ LEC holds.
Î¸ Î¸ Î¸ Î¸ Î¸
B.2 PROOFOFTHEOREM2
Proof. FromEq. 7,thevalueofÎ·Ë†(sâ€²)canbeexpressedas
Î·Ë†(sâ€²)=E [Î·(sâ€²)]â‰ƒ N Î¾(sâ€²) (cid:0) H(f (sâ€²))âˆ’maxQ (sâ€²,aâ€²)(cid:1) ]. (15)
Ï€Î¸ N call(sâ€²) Ï• aâ€² Î¸âˆ’
Whenthejointactionsfromthecurrenttimefollowtheoptimalpolicy,aâˆ¼Ï€âˆ—,thecumulativereward
Î¸
fromsâ€² convergestoVâˆ—(sâ€²),i.e.,H(f (sâ€²)) â†’ Vâˆ—(sâ€²). Then,everyrecallofxË†â€² = NN(f (sâ€²)) âˆˆ
Ï• Ï•
D guarantees the desirable transition, i.e., Î¾(sâ€²) = 1, where NN(Â·) represents a function for
E
selecting the nearest neighbor. As a result, as N (sâ€²) â†’ âˆž, NÎ¾(sâ€²) â†’ 1, yielding Î·Ë†(sâ€²) â‰ƒ
call Ncall(sâ€²)
NÎ¾(sâ€²) (cid:0) H(f (sâ€²))âˆ’max Q (sâ€²,aâ€²)(cid:1) â†’Vâˆ—(sâ€²)âˆ’max Q (sâ€²,aâ€²).Then,thegradientsignal
Ncall(sâ€²) Ï• aâ€² Î¸âˆ’ aâ€² Î¸âˆ’
withtheepisodicincentiveâˆ‡ Lpbecomes
Î¸ Î¸
âˆ‡ Lp =âˆ’2âˆ‡ Q (s,a)[âˆ†Îµ +rp]
Î¸ Î¸ Î¸ Î¸ TD
=âˆ’2âˆ‡ Q (s,a)[âˆ†Îµ +Î³Î·Ë†(sâ€²)]
Î¸ Î¸ TD
â‰ƒâˆ’2âˆ‡ Q (s,a)[âˆ†Îµ +Î³(Vâˆ—(sâ€²)âˆ’maxQ (sâ€²,aâ€²))]
Î¸ Î¸ TD Î¸âˆ’
aâ€²
=âˆ’2âˆ‡ Q (s,a)[r(s,a)+Î³maxQ (sâ€²,aâ€²)âˆ’Q (s,a)+Î³(Vâˆ—(sâ€²)âˆ’maxQ (sâ€²,aâ€²))]
Î¸ Î¸ Î¸âˆ’ Î¸ Î¸âˆ’
aâ€² aâ€²
=âˆ’2âˆ‡ Q (s,a)[r(s,a)+Î³Vâˆ—(sâ€²)âˆ’Q (s,a)]
Î¸ Î¸ Î¸
=âˆ‡ Lâˆ—,
Î¸ Î¸
(16)
whichcompletestheproof.
Inaddition,whenmax Q (sâ€²,aâ€²)accuratelyestimatesVâˆ—(sâ€²),theoriginalTD-targetispreserved
aâ€² Î¸âˆ’
astheepisodicincentivebecomeszero,i.e.,rp â†’0.Thenwiththeproperlyannealedintrinsicreward
rc,thelearningobjectivepresentedinEq. 10degeneratestotheoriginalBellmanoptimalityequation
(Sutton&Barto,2018). Ontheotherhand,eventhoughtheassumptionofH(sâ€²)â†’Vâˆ—(sâ€²)yields
âˆ†Îµ â†’âˆ†Îµâˆ— ,âˆ‡ LEC hasanadditionalbiasâˆ†Îµ duetoweightedsumstructurepresentedin
EC TD Î¸ Î¸ TD
Eq. 3. Thus,âˆ‡ LEC canconvergetoâˆ‡ Lâˆ—onlywhenmax Q (sâ€²,aâ€²)â†’Vâˆ—(sâ€²)andÎ»â†’0at
Î¸ Î¸ Î¸ Î¸ aâ€² Î¸âˆ’
thesametime.
15PublishedasaconferencepaperatICLR2024
C IMPLEMENTATION AND EXPERIMENT DETAILS
C.1 DETAILSOFIMPLEMENTATION
EncoderandDecoderStructure
As illustrated in Figure 1(c), we have an encoder and decoder structure. For an encoder f , we
Ï•
evaluate two types of structure, EmbNet and dCAE. For EmbNet with the learning objective
presentedinEq. 4,twofullyconnectedlayerswith64-dimensionalhiddenstateareusedwithReLU
activationfunctionbetweenthem,followedbylayernormalizationblockatthehead. Ontheother
hand,fordCAEwiththelearningobjectivepresentedinEq. 5,weutilizeadeeperencoderstructure
whichcontainsthreefullyconnectedlayerswithReLUactivationfunction. Inaddition,dCAEtakes
atimesteptasaninputaswellasaglobalstates . Wesetepisodiclatentdimensiondim(x)=4as
t
Zhengetal.(2021).
FC
LayerNorm ReLU FC FC FC
ð‘  FC ð‘¥ FC ð‘“ ðœ“(âˆ™) ð»à´¥ ð‘  ReLU ð‘¥ ReLU ReLU ð‘ Ò§
ReLU FC FC
ReLU
FC ReLU ReLU
FC FC ð‘¡ FC
ð‘“ ðœ™(âˆ™) ð‘¡ ð‘“ ðœ™(âˆ™) ð‘“ ðœ“(âˆ™)
ð»à´¥
(a)EmbNet (b)dCAE
Figure12: Illustrationofnetworkstructures.
Foradecoderf ,bothEmbNetanddCAEutilizeathree-fullyconnectedlayerwithReLUactivation
Ïˆ
functions. Differences are that EmbNet takes only x as input and utilizes the 128-dimensional
t
hiddenstatewhiledCAEtakesx andtasinputsandadoptsthe64-dimensionalhiddenstate. As
t
illustratedinFigure1(c),toreconstructglobalstates ,dCAEhastwoseparateheadswhilesharing
t
lowerpartsofnetworks;fstoreconstructs andfH topredictthereturnofs ,denotedasH . Figure
Ï• t Ï• t t
12illustratesnetworkstructuresofEmbNetanddCAE.TheconceptofsupervisedVAEsimilarto
EMUcanbefoundin(Leetal.,2018).
The reason behind avoiding probabilistic autoencoders such as variational autoencoder (VAE)
(Kingma&Welling,2013;Sohnetal.,2015)isthatthestochasticembeddingandthepriordistribu-
tioncouldhaveanadverseimpactonpreservingapairofx andH forgivenas . Inparticular,with
t t t
stochasticembedding,afixeds cangeneratediversex . Asaresult,itbreaksthepairofx andH
t t t t
forgivens ,whichmakesitdifficulttoselectavalidmemoryfromD .
t E
Fortraining,weperiodicallyupdatef andf withanupdateintervaloft inparalleltoMARL
Ï• Ïˆ emb
training. Ateachtrainingphase,weuseM samplesoutofthecurrentcapacityofD ,whose
emb E
maximumcapacityis1million(1M),andbatchsizeofm isusedforeachtrainingstep. After
emb
updatingf ,everyxâˆˆD needstobeupdatedwithupdatedf . Algorithm1showsthedetailsof
Ï• E Ï•
learningframeworkforf andf . Detailsofthetrainingprocedureforf andf alongwithMARL
Ï• Ïˆ Ï• Ïˆ
trainingarepresentedinAppendixE.2.
OtherNetworkStructureandHyperparameters
Foramixerstructure,weadoptQPLEX(Wangetal.,2020b)inbothEMU(QPLEX)andEMU(CDS)
andfollowthesamehyperparametersettingsusedintheirsourcecodes. Commonhyperparameters
relatedtoindividualQ-networkandMARLtrainingareadoptedbythedefaultsettingsofPyMARL
(Samvelyanetal.,2019).
16PublishedasaconferencepaperatICLR2024
Algorithm1TrainingAlgorithmforStateEmbedding
1: Parameter: learningrateÎ±,numberoftrainingdatasetN,batchsizeB
2:
SampleTrainingdataset(s(i),H(i),t(i))N
i=1
âˆ¼D E,
3: InitializeweightsÏ•,Ïˆ â†0
4: fori=1toâŒŠN/BâŒ‹do
5: Compute(x(j) =f Ï•(s(j)|t(j))i jB
=(iâˆ’1)B+1
6: Predictreturn(HÂ¯(j) =fH(x(j)|t(i)))iB
Ïˆ j=(iâˆ’1)B+1
7: Reconstructstate(sÂ¯(j) =fs(x(j))|t(i))iB
Ïˆ j=(iâˆ’1)B+1
8: ComputeLossL(Ï•,Ïˆ)viaEq. 5
9: UpdateÏ•â†Ï•âˆ’Î±âˆ‚L,Ïˆ â†Ïˆâˆ’Î±âˆ‚L
âˆ‚Ï• âˆ‚Ïˆ
10: endfor
C.2 EXPERIMENTDETAILS
WeutilizePyMARL(Samvelyanetal.,2019)toexecuteallofthebaselinealgorithmswiththeir
open-sourcecodes,andthesamehyperparametersareusedforexperimentsiftheyarepresented
eitherinuploadedcodesorintheirmanuscripts.
Forageneralperformanceevaluation,wetestourmethodsonvariousmaps,whichrequireadifferent
levelofcoordinationaccordingtothemapâ€™sdifficulties. Win-rateiscomputedwith160samples: 32
episodesforeachtrainingrandomseed,and5differentrandomseedsunlessdenotedotherwise.
Boththemeanandthevarianceoftheperformancearepresentedforallthefigurestoshowtheir
overallperformanceaccordingtodifferentseeds. Especiallyforafaircomparison,wesetn ,the
circle
numberoftrainingperasampledbatchof32episodesduringtraining,as1forallbaselinessince
someofthebaselinesincreasen =2asadefaultsettingintheircodes.
circle
Forperformancecomparisonwithbaselinemethods,weusetheircodeswithfine-tunedalgorithm
configurationforhyperparametersettingsaccordingtotheircodesandoriginalpaperifavailable. For
experimentsonSMAC,weusetheversionofstarcraft.pypresentedinRODE(Wangetal.,
2021)adoptingsomemodificationforcompatibilitywithQPLEX(Wangetal.,2020b). AllSMAC
experimentswereconductedonStarCraftIIversion4.10.0inaLinuxenvironment.
For Google research football task, we use the environmental code provided by (Kurach et al.,
2020). Intheexperiments,weconsiderthreeofficialscenariossuchasacademy_3_vs_1_with_keeper
(3_vs_1WK),academy_counterattack_easy(CA_easy),andacademy_counterattack_hard(CA_hard).
Inaddition,forcontrollingrcinEq. 10,thesamehyperparametersrelatedtocuriosity-based(Zheng
etal.,2021)ordiversity-basedexplorationChenghaoetal.(2021)areadoptedforEMU(QPLEX)
andEMU(CDS)aswellasforbaselinesEMCandCDS.Afterfurtherexperiments,wefoundthat
thecuriosity-basedrcfrom(Zhengetal.,2021)adverselyinfluencedsuperhardSMACtask,with
theexceptionofcorridorscenario. Furthermore,thediversity-basedexplorationfromChenghao
etal.(2021)ledtoadecreaseinperformanceonbotheasyandhardSMACmaps. Thus,wedecided
toexcludetheuseofrcforEMU(QPLEX)onthesuperhardSMACtaskandforEMU(CDS)on
theeasy/hardSMACmaps. EMUsettask-dependentÎ´ valuesaspresentedinTable1. Forother
hyperparametersintroducedbyEMU,thesamevaluespresentedinTable8areusedthroughoutall
tasks. ForEMU(QPLEX)incorridor,Î´ =1.3eâˆ’5isusedinsteadofÎ´ =1.3eâˆ’3.
Table1: Task-dependenthyperparameterofEMU.
Category Î´
easy/hardSMACmaps 1.3eâˆ’5
superhardSMACmaps 1.3eâˆ’3
GRF 1.3eâˆ’3
17PublishedasaconferencepaperatICLR2024
C.3 DETAILSOFMARLTASKS
Inthissection,wespecifythedimensionofthestatespace,theactionspace,theepisodiclength,and
therewardofSMAC(Samvelyanetal.,2019)andGRF(Kurachetal.,2020).
InSMAC,theglobalstateofeachtaskofSMACincludestheinformationofthecoordinatesofall
agents,andfeaturesofbothalliedandenemyunits. Theactionspaceofeachagentconsistsofthe
movingactionsandattackingenemies,andthusitincreasesaccordingtothenumberofenemies.
Thedimensionsofthestateandactionspaceandtheepisodiclengthvaryaccordingtothetasksas
showninTable2. Forrewardstructure,weusedtheshapedreward,i.e.,thedefaultrewardsettingsof
SMAC,forallscenarios. Therewardisgivenwhendealingdamagetotheenemiesandgetbonuses
forwinningthescenario. Therewardisscaledsothatthemaximumcumulativereward,R ,that
max
canbeobtainedfromtheepisode,becomesaround20(Samvelyanetal.,2019).
Table2: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofSMAC
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
1c3s5z 270 15 180
3s5z 216 14 150
3s_vs_5z 68 11 250
5m_vs_6m 98 12 70
MMM2 322 18 180
6h_vs_8z 140 14 150
3s5z_vs_3s6z 230 15 170
corridor 282 30 400
InGRF,thestateofeachtaskincludesinformationoncoordinates,ballpossession,andthedirection
ofplayers,etc. ThedimensionofthestatespacediffersamongthetasksasinTable3. Theaction
ofeachagentconsistsofmovingdirections,differentwaystokicktheball,sprinting,intercepting
the ball and dribbling. The dimensions of the action spaces for each task are the same. Table 3
summarizesthedimensionoftheactionspaceandtheepisodiclength. InGRF,therecanbetwo
rewardmodes: oneis"sparsereward"andtheotheris"densereward."Insparserewardmode,agents
getapositivereward+1whenscoringagoalandget-1whenconcedingonetotheopponents. In
denserewardmode,agentscangetpositiverewardswhentheyapproachtoopponentâ€™sgoal,butthe
maximumcumulativerewardisupto+1. Inourexperiments,weadoptsparserewardmode,andthus
themaximumreward,R is+1forGRF.
max
Table3: DimensionofthestatespaceandtheactionspaceandtheepisodiclengthofGRF
Task Dimensionofstatespace Dimensionofactionspace Episodiclength
3_vs_1WK 26 19 150
CA_easy 30 19 150
CA_hard 34 19 150
C.4 INFRASTRUCTURE
ExperimentsforSMAC(Samvelyanetal.,2019)aremainlycarriedoutonNVIDIAGeForceRTX
3090GPU,andtrainingforthelongestexperimentsuchascorridorviaEMU(CDS)tookless
than18hours. Notethatwhentrainingisconductedwithn = 2,ittakesmorethanoneanda
circle
halftimeslonger. Trainingencoder/decoderstructureandupdatingD withupdatedf together
E Ï•
onlytooklessthan2secondsatmostincorridortask. Asweupdatef andf periodicallywith
Ï• Ïˆ
t ,theadditionaltimerequiredforatrainableembedderiscertainlynegligiblecomparedtoMARL
emb
training.
18PublishedasaconferencepaperatICLR2024
D FURTHER EXPERIMENT RESULTS
D.1 NEWPERFORMANCEINDEX
Inthissection,wepresentthedetailsofanewperformanceindexcalledoverallwin-rate,ÂµÂ¯ . For
w
example,letfi(s)bethetestwin-rateattrainingtimesofithseedrunandÂµi (t)representsthe
w w
timeintegrationoffi(s)untilt. Then,anormalizedoverallwin-rate,ÂµÂ¯ ,canbeexpressedas
w w
1 1 (cid:88)n 1 1 (cid:88)n (cid:90) t
ÂµÂ¯ (t)= Âµi (t)= fi(s)ds, (17)
w Âµ maxn i=1 w Âµ maxn i=1
0
w
whereÂµ =tandÂµÂ¯ âˆˆ[0,1].
max w
] ð‘“ð‘–=1(ð‘ )
- ð‘¤
[
n
iW
ð‘“ð‘–=2(ð‘ )
t
ð‘¤
s e ðœ‡ð‘– (ð‘¡)
T ð‘¤
ð‘ 
Figure13: IllustrationofÂµi (t).
w
Figure13illustratestheconceptoftimeintegrationofwin-rate,i.e.,Âµi (t),toconstructtheoverall
w
win-rate,ÂµÂ¯ . Byconsideringtheintegrationofwin-rateofeachseedcase,theperformancevariance
w
canbeconsidered,andthusÂµÂ¯ showsthetrainingefficiency(speed)aswellasthetrainingquality
w
(win-rate).
D.2 ADDITIONALEXPERIMENTRESULTS
InSection4.3,wepresentthesummaryofparametricstudiesonÎ´withrespecttovariouschoices
off . Toseethetrainingefficiencyandperformanceatthesametime,Table4and5presentthe
Ï•
overallwin-rateÂµÂ¯ accordingtotrainingtime. Weconducttheexperimentsfor5differentseedcases
w
andateachtestphase32sampleswereusedtoevaluatethewin-rate[%]. Notethatwediscardthe
componentofepisodicincentiverptoseetheperformancevariationsaccordingtoÎ´andtypesoff
Ï•
moreclearly.
Table 4: ÂµÂ¯ according to Î´ and design choice of embedding function on hard SMAC map,
w
3s_vs_5z.
Trainingtime
0.69 1.37 2.00
[mil]
Î´ random EmbNet dCAE random EmbNet dCAE random EmbNet dCAE
1.3e-7 0.033 0.051 0.075 0.245 0.279 0.343 0.413 0.443 0.514
1.3e-5 0.010 0.044 0.063 0.171 0.270 0.325 0.320 0.441 0.491
1.3e-3 0.034 0.043 0.078 0.226 0.270 0.357 0.381 0.439 0.525
1.3e-2 0.019 0.005 0.079 0.205 0.059 0.346 0.348 0.101 0.518
Table 5: ÂµÂ¯ according to Î´ and design choice of embedding function on hard SMAC map,
w
5m_vs_6m.
Trainingtime
0.69 1.37 2.00
[mil]
Î´ random EmbNet dCAE random EmbNet dCAE random EmbNet dCAE
1.3e-7 0.040 0.117 0.110 0.287 0.397 0.397 0.577 0.690 0.701
1.3e-5 0.064 0.107 0.131 0.334 0.402 0.436 0.634 0.714 0.749
1.3e-3 0.040 0.080 0.064 0.333 0.377 0.363 0.646 0.687 0.677
1.3e-2 0.038 0.000 0.048 0.288 0.001 0.332 0.584 0.001 0.643
19PublishedasaconferencepaperatICLR2024
AsTable4and5illustratethatdCAEstructureforf ,whichconsidersthereconstructionlossof
Ï•
globalstatesasinEq. 5,showsthebesttrainingefficiencyinmostcases. For5m_vs_6mtaskwith
Î´ =1.3eâˆ’3,EmbNetachievesthehighestvalueamongf choicesintermsofÂµÂ¯ butfailstofind
Ï• w
optimalpolicyatÎ´ =1.3eâˆ’2unlikeotherdesignchoices. Thisimpliesthatthereconstructionloss
ofdCAEfacilitatestheconstructionofasmootherembeddingspaceforD ,enablingtheretrieval
E
of memories within a broader range of Î´ values from the key state. Figure 15 and 16 show the
correspondinglearningcurvesofeachencoderstructurefordifferentÎ´values. AlargeÎ´valueresults
inahigherperformancevariancethanthecaseswithsmallerÎ´,accordingtodifferentseedcases.
This is because a high value of Î´ encourages
exploratorymemoryrecall. Inotherwords,by
adjusting Î´, we can control the level of explo-
rationsinceitcontrolswhethertorecallitsown
MC return or the highest value of other simi-
larstateswithinÎ´. Thus,withoutconstructing
smootherembeddingspaceasindCAE,learn-
ingwithexploratorymemoryrecallwithinlarge
Î´ canconvergetosub-optimalityasillustrated
bythecaseofEmbNetinFigure16(d). Figure 14: NÂ¯ of all memories in D when
call E
Î´ =0.013accordingtodesignchoiceforf .
InFigure14whichshowstheaveragednumber Ï•
ofmemoryrecall(NÂ¯ )ofallmemoriesinD ,
call E
NÂ¯ ofEmbNetsignificantlyincreasesastrainingproceeds. Ontheotherhand,dCAEwasableto
call
preventthisproblemandrecalledthepropermemoriesintheearlylearningphase,achievinggood
trainingefficiency. Thus,embeddingwithdCAEcanleverageawideareaofmemoryinD and
E
becomesrobusttohyperparameterÎ´.
(a)Î´=1.3eâˆ’7 (b)Î´=1.3eâˆ’5 (c)Î´=1.3eâˆ’3 (d)Î´=1.3eâˆ’2
Figure15: ParametricstudiesforÎ´on3s_vs_5zSMACmap.
(a)Î´=1.3eâˆ’7 (b)Î´=1.3eâˆ’5 (c)Î´=1.3eâˆ’3 (d)Î´=1.3eâˆ’2
Figure16: ParametricstudiesforÎ´on5m_vs_6mSMACmap.
20PublishedasaconferencepaperatICLR2024
D.3 COMPARATIVEEVALUATIONONADDITIONALSTARCRAFTIIMAPS
Figure17presentsacomparativeevaluationofEMUwithbaselinealgorithmsonadditionalSMAC
maps. AdoptingEMUshowsperformancegaininvarioustasks.
Figure17: PerformancecomparisonofEMUagainstbaselinealgorithmsonadditionalSMACmaps.
D.4 COMPARISONOFEMUWITHMAPPOONSMAC
Inthissubsection,wecomparetheEMUwithMAPPO(Yuetal.,2022)onselectedSMACmaps.
Figure18showstheperformanceinsixSMACmaps: 1c3s5z,3s_vs_5z,5m_vs_6m,MMM2,
6h_vs_8z and 3s5z_vs_3s6z. Similar to the previous performance evaluation in Figure 4,
Win-rateiscomputedwith160samples: 32episodesforeachtrainingrandomseedand5different
randomseeds.Also,forMAPPO,scenario-dependenthyperparametersareadoptedfromtheiroriginal
settingsintheuploadedsourcecode.
FromFigure18,wecanseethatEMUperformsbetterthanMAPPOwithanevidentgap. Although
after extensive training MAPPO showed a comparable performance against off-policy algorithm
initsoriginalpaper(Yuetal.,2022),withinthesametrainingtimestepusedforourexperiments,
we found that MAPPO suffers from local convergence in super hard SMAC tasks such as MMM2
and 3s5z_vs_3s6z as shown in Figure 18. Only in 6h_vs_8z, MAPPO shows comparable
performancetoEMU(QPLEX)withhigherperformancevarianceacrossdifferentseeds.
Figure18: PerformancecomparisonwithMAPPOonselectedSMACmaps.
21PublishedasaconferencepaperatICLR2024
D.5 ADDITIONALPARAMETRICSTUDY
In this subsection, we conduct an additional parametric study to see the effect of key hyperpa-
rameter Î´. Unlike the previous parametric study on Appendix D.2, we adopt both dCAE em-
bedding network for f and episodic reward. For evaluation, we consider three GRF tasks such
Ï•
as academy_3_vs_1_with_keeper (3_vs_1WK), academy_counterattack_easy
(CA-easy),andacademy_counterattack_hard(CA-hard);andonesuperhardSMAC
map such as 6h_vs_8z. For each task to evaluate EMU, four Î´ values, such as Î´ = 1.3eâˆ’7,
1
Î´ = 1.3eâˆ’5, Î´ = 1.3eâˆ’3, andÎ´ = 1.3eâˆ’2, areconsidred. Here, tocomputethewin-rate, 160
2 3 4
samples (32 episodes for each training random seed and 5 different random seeds) are used for
3_vs_1WKand6h_vs_8zwhile100samples(20episodesforeachtrainingrandomseedand5
differentrandomseeds)areusedforCA-easyandCA-hard. NotethatCDSandEMU(CDS)
utilize the same hyperparameters, and EMC and EMU (QPLEX) use the same hyperparameters
withoutacuriosityincentivepresentedinZhengetal.(2021)asthemodelwithoutitshowedthe
betterperformancewhenutilizingepisodiccontrol.
(a)3_vs_1WK(GRF) (b)CA-easy(GRF) (c)CA-hard(GRF) (d)6h_vs_8z(SMAC)
Figure19: ParametricstudiesforÎ´onvariousGRFmapsandsuperhardSMACmap.
Inallcases,EMUwithÎ´ =1.3eâˆ’3showsthebestperformance. Thetasksconsideredhereareall
3
complexmulti-agenttasks,andthusadoptingapropervalueofÎ´benefitstheoverallperformance
andachievesthebalancebetweenexplorationandexploitationbyrecallingthesemanticallysimilar
memoriesfromepisodicmemory. TheoptimalvalueofÎ´ isconsistentwiththedeterminationlogic
3
onÎ´inamemoryefficientwaypresentedinAppendixF.
D.6 ADDITIONALPARAMETRICSTUDYONÎ»
rcon
Additionally,weconductaparametricstudyforÎ» inEq. 5. Foreachtask,EMUwithfiveÎ»
rcon rcon
values,suchasÎ» = 0.01,Î» = 0.1,Î» = 0.5,Î» = 1.0andÎ» = 10,are
rcon,0 rcon,1 rcon,2 rcon,3 rcon,4
evaluated. Here,tocomputethewin-rateofeachcase,160samples(32episodesforeachtraining
randomseedand5differentrandomseeds)areused. FromFigure20,wecanseethatbroadrangeof
(a)3s5z(SMAC) (b)3s_vs_5z(SMAC)
Figure20: ParametricstudyforÎ» .
rcon
(cid:8) (cid:9)
Î» âˆˆ 0.1,0.5,1.0 workwellingeneral. However,withlargeÎ» asÎ» = 10,wecan
rcon rcon rcon,4
observethatsomeperformancedegradationattheearlylearningphasein3s5ztask. Thisresultis
inlinewiththelearningtrendsofCase1andCase2of3s5zinFigure23,whichdonotconsider
predictionlossandonlytakeintoaccountthereconstructionloss. Thus,consideringbothprediction
lossandreconstructionlossasCase4inEq. 5withproperÎ» isessentialtooptimizetheoverall
rcon
learningperformance.
22PublishedasaconferencepaperatICLR2024
D.7 ADDITIONALABLATIONSTUDYINGRF
(a)3_vs_1WK(GRF) (b)CA-easy(GRF)
Figure21: AblationstudiesonepisodicincentiveonGRFtasks.
Inthissubsection,weconductadditionalablationstudiesviaGRFtaskstoseetheeffectofepisodic
incentive. Again,EMU(CDS-No-EI)ablatesepisodicincentivefromEMU(CDS)andutilizesthe
conventionalepisodiccontrolpresentedinEq.3instead.Again,EMU(CDS-No-SE)ablatessemantic
embeddingbydCAEandadoptsrandomprojectionwithepisodicincentiverp. Inbothtasks,utilizing
episodicmemorywiththeproposedembeddingfunctionimprovestheoverallperformancecompared
totheoriginalCDSalgorithm. Byadoptingepisodicincentivesinsteadofconventionalepisodic
control,EMU(CDS)achievesbetterlearningefficiencyandrapidlyconvergestooptimalpolicies
comparedtoEMU(CDS-No-EI).
D.8 ADDITIONALABLATIONSTUDYONEMBEDDINGLOSS
Inourcase,theautoencoderusesthereconstructionlosstoenforcetheembeddedrepresentationx
tocontainthefullinformationoftheoriginalfeature,s. Weareadding(H âˆ’fH(f (s |t)|t))2to
t Ïˆ Ï• t
guidetheembeddedrepresentationtobeconsistenttoH ,aswell,whichworksasaregularizerto
t
theautoencoder. Therefore,fH isusedinEq. 5topredicttheobservedH fromD asapartofthe
Ïˆ t E
semanticregularizationeffort.
BecauseH isdifferentfromfH(x ),theeffortofminimizingtheirdifferencebecomestheregularizer
t Ïˆ t
creatingagradientsignaltolearnÏˆandÏ•. TheupdateofÏ•resultsintheupdatedxinfluencedbythe
regularization. NotethatweupdateÏ•throughthebackpropagationofÏˆ.
ThecaseofL(Ï•,Ïˆ)=||s âˆ’fs(f (s |t)|t)||2occurswhenÎ» becomesrelativelymuchhigher
t Ïˆ Ï• t 2 rcon
than1,whichmakes(H âˆ’fH(f (s |t)|t))2becomesineffective. Inotherwords,whenÎ» inEq.
t Ïˆ Ï• t rcon
5becomesrelativelymuchhigherthan1,(H âˆ’fH(f (s |t)|t))2becomesineffective.
t Ïˆ Ï• t
ThecaseofL(Ï•,Ïˆ)=(H âˆ’fH(f (s |t)|t))2occurswhenthescalefactorÎ» becomesrelatively
t Ïˆ Ï• t rcon
muchsmallerthan1,whichmakes(H âˆ’fH(f (s |t)|t))2becomeadominantfactor. Weconduct
t Ïˆ Ï• t
ablationstudiesconsideringfourcasesasfollows:
â€¢ Case1: L(Ï•,Ïˆ)=||s âˆ’fs(f (s ))||2,presentedinFigure22(a)
t Ïˆ Ï• t 2
â€¢ Case2: L(Ï•,Ïˆ)=||s âˆ’fs(f (s |t)|t)||2,presentedinFigure22(b)
t Ïˆ Ï• t 2
â€¢ Case3: L(Ï•,Ïˆ)=(H âˆ’fH(f (s |t)|t))2,presentedinFigure22(c)
t Ïˆ Ï• t
â€¢ Case 4: L(Ï•,Ïˆ) = (H âˆ’fH(f (s |t)|t))2 +Î» ||s âˆ’fs(f (s |t)|t)||2, i.e., Eq. 5,
t Ïˆ Ï• t rcon t Ïˆ Ï• t 2
presentedinFigure22(d)
Wevisualizetheresultoft-SNEof50KsamplesxâˆˆD outof1Mmemorydatatrainedbyvarious
E
loss functions: The task was 3s_vs_5z of SMAC as in Figure 2 and the training for all models
proceedsfor1.5miltrainingsteps. Case1andCase2showedirregularreturndistributionacrossthe
embeddingspace. Inthosetwocases,therewasnoconsistentpatternintherewarddistribution. Case
3withonlyreturnpredictioninthelossshowedbetterpatternscomparedtoCase1and2butsome
featuresarenotclusteredwell. Wesuspectthattheconsistentstaterepresentationalsocontributes
tothereturnprediction. Case4ofoursuggestedlossshowedthemostregularpatterninthereturn
distributionarrangingthelow-returnstatesasaclusterandthestateswithdesirablereturnsasanother
23PublishedasaconferencepaperatICLR2024
(a)Loss(case1) (b)Loss(case2) (c)Loss(case3) (d)Loss(case4)
Figure22: t-SNEofsampledembeddingxâˆˆD trainedbydCAEwithvariouslossfunctionsin
E
3s_vs_5zSMACmap. Colorsfromredtopurplerepresentfromlowreturntohighreturn.
cluster. InFigure23,Case4showsthebestperformanceintermsofbothlearningefficiencyand
terminalwin-rate.
(a)3s5z(SMAC) (b)3s_vs_5z(SMAC)
Figure23:Performancecomparisonofvariouslossfunctions
fordCAE.
24PublishedasaconferencepaperatICLR2024
D.9 ADDITIONALABLATIONSTUDYONSEMANTICEMBEDDING
Tofurtherunderstandtheroleofsemanticembedding,weconductadditionalablationstudiesand
presentthemwiththegeneralperformanceofotherbaselinemethods. Again,EMU(CDS-No-SE)
ablatessemanticembeddingbydCAEandadoptsrandomprojectioninstead,alongwithepisodic
incentiverp.
Figure24: PerformancecomparisonofEMUagainstbaselinealgorithmsonthreeeasyandhard
SMACmaps: 1c3s5z,3s_vs_5z,and5m_vs_6m,andthreesuperhardSMACmaps: MMM2,
6h_vs_8z,and3s5z_vs_3s6z.
Figure 25: Performance comparison of EMU against baseline algorithms on Google Research
Football.
Forrelativelyeasytasks,EMU(QPLEX-No-SE)andEMU(CDS-No-SE)showcomparableperfor-
manceatfirstbuttheyconvergeonsub-optimalpolicyinmosttasks. Especially,thischaracteristicis
wellobservedinthecaseofEMU(CDS-No-SE).Aslargesizeofmemoriesarestoredinanepisodic
bufferastraininggoeson,theprobabilityofrecallingsimilarmemoriesincreases. However,with
randomprojection,semanticallyincoherentmemoriescanberecalledandthusitcanadverselyaffect
thevalueestimation. Wedeemthisisthereasonfortheconvergenceonsuboptimalpolicyinthecase
ofEMU(No-SE).Thuswecanconcludethatrecallingsemanticallycoherentmemoryisanessential
componentofEMU.
25PublishedasaconferencepaperatICLR2024
D.10 ADDITIONALABLATIONONrc
InEq.10, weintroducerc asanadditionalrewardwhichmayencourageexploratorybehavioror
coordination. The reason we introduce rc is to show that EMU can be used in conjunction with
anyformofincentiveencouragingfurtherexploration. Ourmethodmaynotbestronglyeffective
untilsomedesiredstatesarefound,althoughithasexploratorybehaviorviatheproposedsemantic
embeddings,controlledbyÎ´. Untilthen,suchincentivescouldbebeneficialtofinddesiredorgoal
states. Figures26-27showtheablationstudyofwithandwithoutrc,andthecontributionofrc is
limitedcomparedtorp.
(a)3s_vs_5z (b)5m_vs_6m (c)3s5z_vs_3s6z
Figure26: AblationstudiesonrcinSMACtasks.
(a)3s_vs_5z (b)5m_vs_6m (c)3s5z_vs_3s6z
Figure27: AblationstudiesonrcinSMACtasks.
D.11 COMPARISONOFEPISODICINCENTIVEWITHEXPLORATORYINCENTIVE
Inthissubsection,wereplacetheepisodicincentivewithanotherexploratoryincentive,introduced
by(Henaffetal.,2022). In(Henaffetal.,2022),theauthorsextendthecount-basedepisodicbonuses
tocontinuousspacesbyintroducingepisodicellipticalbonusesforexploration. Inthisconcept,a
highrewardisgivenwhenthestateprojectedintheembeddingspaceisdifferentfromtheprevious
stateswithinthesameepisode. Indetail,withagivenfeatureencoderÏ•,theellipticalbonusb at
t
timesteptiscomputedasfollows:
b =Ï•(s )TCâˆ’1Ï•(s ) (18)
t t t t
whereCâˆ’1 isaninversecovariancematrixwithaninitialvalueofCâˆ’1 =1/Î» I. Here,Î» is
t t=0 e3b e3b
acovarianceregularizer. Forupdateinversecovariance,theauthorssuggestedacomputationally
efficientupdateas
1
Câˆ’1 =Câˆ’1âˆ’ uuT (19)
t+1 t 1+b
t+1
whereu=Câˆ’1Ï•(s ). Then,thefinalrewardrÂ¯ withepisodicellipticalbonusesb isexpressedas
t t+1 t t
rÂ¯ =r +Î² b (20)
t t e3b t
whereÎ² andr areacorrespondingscalefactorandexternalrewardgivenbytheenvironment,
e3b t
respectively.
Forthiscomparison,weutilizethedCAEstructureasastateembeddingfunctionÏ•. Foramixer,
QPLEX (Wang et al., 2020b) is adopted for all cases, and we denote the case with an elliptical
incentive instead of the proposed episodic incentive as QPLEX (SE+E3B). Figure 28 illustrates
26PublishedasaconferencepaperatICLR2024
Figure28: PerformancecomparisonwithellipticalincentiveonselectedSMACmaps.
theperformanceofadoptinganellipticalincentiveforexplorationinsteadoftheproposedepisodic
incentive. QPLEX (SE+E3B) uses the same hyperparameters with EMU (SE+EI) and we set
Î» =0.1accordingtoHenaffetal.(2022).
e3b
AsillustratedbyFigure28,adoptinganellipticalincentivepresentedby(Henaffetal.,2022)instead
of an episodic incentive does not give any performance gain and even adversely influences the
performancecomparedtoQPLEX.Itseemsthataddingexcessivesurprise-basedincentivescanbea
disturbanceinMARLtaskssincefindinganewstateitselfdoesnotguaranteebettercoordination
among agents. In MARL, agents need to find the proper combination of joint action in a given
similarobservationswhenfindinganoptimalpolicy. Ontheotherhand,inhigh-dimensionalpixel-
based single-agent tasks such as Habitat (Ramakrishnan et al., 2021), finding a new state itself
canbebeneficialinpolicyoptimization. Fromthis,wecannotethatadoptingacertainalgorithm
fromasingle-agentRLcasetoMARLcasemayrequireamodificationoradjustmentwithdomain
knowledge.
As a simple tuning, we conduct parametric study for Î² = {0.01,0.1} to adjust magnitude of
e3b
incentiveofE3B.Figure29illustratestheresults. InFigure29,QPLEX(SE+E3B)withÎ² =0.01
e3b
showsabetterperformancethanthecasewithÎ² = 0.1andcomparableperformancetoEMC
e3b
in5m_vs_6m. However,EMUwiththeproposedepisodicincentiveshowsthebestperformance.
Fromthiscomparison,wecanseethatincentivesproposedbypreviousworkneedtobeadjusted
Figure29: PerformancecomparisonwithanellipticalincentiveonselectedSMACmaps.
accordingtothetypeoftasks,asitwasdoneinEMC(Zhengetal.,2021). Ontheotherhand,with
theproposedepisodicincentivewedonotneedsuchhyperparameter-scaling,allowingmuchmore
flexibleapplicationacrossvarioustasks.
27PublishedasaconferencepaperatICLR2024
D.12 ADDITIONALTOYEXPERIMENTANDAPPLICABILITYTESTS
Inthissection,weconductadditionalexperimentsonthedidacticexamplepresentedby(Zhengetal.,
2021)toseehowtheproposedmethodwouldbehaveinasimplebutcomplexcoordinationtask.
Additionally,bydefiningR todefinethedesirabilitypresentedinDefinition1,wecanextend
thr
EMUtoasingle-agentRLtask,whereastrictgoalisnotdefinedingeneral.
DidacticexperimentonGridworldWeadoptthedidacticexamplesuchasgridworldenvironment
from(Zhengetal.,2021)todemonstratethemotivationandhowtheproposedmethodcanovercome
theexistinglimitationsoftheconventionalepisodiccontrol. Inthistask,twoagentsingridworld(see
Figure30(a))needtoreachtheirgoalstatesatthesametimetogetarewardr =10andifonlyone
arrivesfirst,theygetapenaltywiththeamountofâˆ’p. Pleasereferto(Zhengetal.,2021)forfurther
details.
Wall
G G
Visible Zone
(a)Gridworld (b)Performanceevaluation(p=2)
Figure30: Didacticexperimentsongridworld.
Toseethesoleeffectoftheepisodiccontrol,wediscardthecuriosityincentivepartofEMC,andfor
afaircomparison,wesetthesameexplorationrateofÏµ-greedywithT =200K forallalgorithms.
Ïµ
Weevaluatethewin-ratewith180samples(30episodesforeachtrainingrandomseedand6different
random seeds) at each training time. Notably, adopting episodic control with a naive utilization
suffersfromlocalconvergence(seeQPLEXandEMC(QPLEX)inFigure30(b)),eventhoughit
expediteslearningefficiencyattheearlytrainingphase. Ontheotherhand,EMUshowsmorerobust
performanceunderdifferentseedcasesandachievesthebestperformancebyanefficientanddiscreet
utilizationofepisodicmemories.
ApplicabilitytesttosingleagentRLtaskWefirstneedtodefineR valuetoeffectivelyapply
thr
EMU to a single-agent task where a goal of an episode is generally not strictly defined, unlike
cooperativemulti-agenttaskswithasharedcommongoal.
Inasingle-agenttaskwheretheactionspaceiscontinuoussuchasMuJoCo(Todorovetal.,2012),
theactor-criticmethodisoftenadopted. EfficientmemoryutilizationofEMUcanbeusedtotrain
thecriticnetworkandthusindirectlyinfluencepolicylearning,unlikegeneralcooperativeMARL
taskswherevalue-basedRLisoftenconsidered.
WeimplementEMUontopofTD3andusetheopen-sourcecodepresentedin(Fujimotoetal.,2018).
Webegintotrainthemodelaftersufficientdataisstoredinthereplaybufferandconduct6timesof
trainingperepisodewith256mini-batches. Notethatthisisdifferentfromthedefaultsettingsof
RLtraining,whichconductstrainingateachtimestep. Ourmodifiedsettingaimstoseetheeffecton
thesampleefficiencyoftheproposedmodel. Theperformanceofthetrainedmodelisevaluatedat
every50ktimesteps.
We use the same hyperparameter settings as in MARL task presented in Table 8 except for the
update interval, t = 100K according to large episodic timestep in single-RL compared to
emb
MARLtasks. Itisworthmentioningthatadditionalcustomizedparametersettingsforsingle-agent
tasks may further improve the performance. In our evaluation, three single-agent tasks such as
Hopper-v4,Walker2D-v4andHumanoid-v4areconsidered,andFigure32illustrateseach
task. Here, Î´ = 1.3eâˆ’5 is used for Hopper-v4 and Walker2D-v4, and Î´ = 1.3eâˆ’3 is
2 3
usedforHumanoid-v4asHumanoid-v4taskcontainsmuchhigherstatedimensionspaceas
376-dimension. PleaserefertoTodorovetal.(2012)foradetaileddescriptionoftasks.
28PublishedasaconferencepaperatICLR2024
(a)Hopper-v4 (b)Walker2D-v4 (c)Humanoid-v4
Figure31: IllustrationofMuJoCoscenarios.
(a)Performance(Hopper) (b)Performance(Walker2D) (c)Performance(Humanoid)
Figure32: Applicabilitytesttosingleagenttask(R =500).
thr
In Figure 32, EMU (TD3) shows the performance improvement compared to the original TD3.
Thankstosemanticallysimilarmemoryrecallandepisodicincentive,statesdeemeddesirablecould
havehighvalues,andtrainedpolicyisencouragedtovisitthemmorefrequently. Asaresult,EMU
(TD3)showsthebetterperformance. Interestingly,understatedimensionasHumanoid-v4task,
TD3andEMU(TD3)showadistinctperformancegapintheearlytrainingphase. Thisisbecause,
in a task with a high-dimensional state space, it is hard for a critic network to capture important
featuresdeterminingthevalueofagivenstate. Thus,ittakeslongertoestimatestatevalueaccurately.
However, with the help of semantically similar memory recall and error compensation through
episodicincentive,acriticnetworkinEMU(TD3)canaccuratelyestimatethevalueofthestatemuch
fasterthantheoriginalTD3,leadingtofasterpolicyoptimization.
UnlikecooperativeMARLtasks,single-RLtasksnormallydonothaveadesirabilitythreshold. Thus,
onemayneedtodetermineR basedondomainknowledgeorapreferenceforthelevelofreturn
thr
tobedeemedsuccessful. Figure33presentsaperformancevariationaccordingtoR .
thr
(a)Hopper-v4 (b)Walker2d-v4
Figure33: ParametricstudyonR .
thr
WhenwesetR =1000inWalker2dtask,desirabilitysignalisrarelyobtainedcomparedtothe
thr
casewithR = 500intheearlytrainingphase. Thus,EMUwithR = 500showsthebetter
thr thr
performance. However,bothcasesofEMUshowbetterperformancecomparedtotheoriginalTD3.
InHoppertask,bothcasesofR =500andR =1000showthesimilarperformance. Thus,
thr thr
29PublishedasaconferencepaperatICLR2024
whendeterminingR ,itcanbebeneficialtosetasmallvalueratherthanalargeonethatcanbe
thr
hardlyobtained.
AlthoughsettingasmallR doesnotrequiremuchdomainknowledge,apossibleoptiontodetour
thr
thisisaperiodicupdateofdesirabilitybasedontheaveragereturnvalueH(s)inallsâˆˆD . Inthis
E
way,acertainstatewithlowreturnwhichwasoriginallydeemedasdesirablecanbereevaluatedas
undesirableastrainingproceeds. Theepisodicincentiveisnotfurthergiventothoseundesirable
states.
Scalability to image-based single-agent RL task Although MARL tasks already contain high-
dimensionstatespacesuchas322-dimensioninMMM2and282-dimensionincorridor,image-
basedsingleRLtasks,suchasAtariBellemareetal.(2013)game,oftenaccompanyhigherstate
spacessuchas[210x160x3]for"RGB"and[210x160]for"grayscale". Weusethe"grayscale"type
forthefollowingexperiments. ForthedetailsofthestatespaceinMARLtask,pleaseseeAppendix
C.3.
Inanimage-basedtask,storingallstatevaluestoupdateallthekeyvaluesinD asf updatescanbe
E Ï•
memory-inefficient,andasemanticembeddingfromoriginalstatesmaybecomeoverheadcompared
tothecasewithoutit. Insuchcase,onemayresorttoapre-trainedfeatureextractionmodelsuch
asResNetmodelprovidedbytorch-visioninacertainamountfordimensionreductiononly,before
passingthroughtheproposedsemanticembedding. Thefeatureextractionmodelaboveisnotan
objectoftraining.
As an example, we implement EMU on the top of DQN model and compare it with the original
DQNonAtaritask. FortheEMU(DQN),weadoptsomepartofpre-trainedResNet18presented
bytorch-visionfordimensionalityreduction,beforepassinganinputimagetosemanticembedding.
At each epoch, 320 random samples are used for training in Breakout task, and 640 random
samples are used in Alien task. The same mini-batch size of 32 is used for both cases. For
f training, the same parameters presented in Table 8 are adopted except for the t = 10K
Ï• emb
consideringthetimestepofsingleRLtask. WealsousethesameÎ´ =1.3eâˆ’5andsetR =50
2 thr
forBreakoutandR =40forAlien,respectively. PleaserefertoBellemareetal.(2013)and
thr
https://gymnasium.farama.org/environments/atarifortaskdetails. AsinFigure
34,wefoundaperformancegainbyadoptingEMUonhigh-dimensionalimage-basedtasks.
(a)Breakout (b)Performance(Breakout) (c)Alien (d)Performance(Alien)
Figure34: Image-basedsingle-RLtaskexample.
30PublishedasaconferencepaperatICLR2024
E TRAINING ALGORITHM
E.1 MEMORYCONSTRUCTION
Duringthecentralizedtraining,wecanaccesstheinformationonwhethertheepisodicreturnreaches
thehighestreturnR orthresholdR ,i.e.,defeatingallenemiesinSMACorscoringagoalin
max thr
GRF.WhenstoringinformationtoD ,bythedefinitionpresentedDefinition. 1,wesetÎ¾(s)=1for
E
âˆ€sâˆˆT .
Î¾
Forefficientmemoryconstruction,wepropagatethedesirabilityofthestatetoasimilarstatewithin
thethresholdÎ´. Withthisdesirabilitypropagation,similarstateshaveanincentiveforavisit. In
addition,onceamemoryissavedinD ,thememoryispreserveduntilitbecomesobsolete(theoldest
E
memorytoberecalled). Whenadesirablestateisfoundneartheexistingsuboptimalmemorywithin
Î´,wereplacethesuboptimalmemorywiththedesirableone,whichgivestheeffectofamemoryshift
tothedesirablestate. Algorithm2presentsthememoryconstructionwiththedesirabilitypropagation
andmemoryshift.
Algorithm2Episodicmemoryconstruction
1: Î¾ T: Optimalityoftrajectory
2: T ={s 0,a 0,r 0,s 1,...,s T}: Episodictrajectory
3: InitializeR t =0
4: fort=T to0do
5: Computex t =f Ï•(s t)andy t =(x tâˆ’ÂµË† x)/ÏƒË† x
6: pickthenearestneighborxË† t âˆˆD E andgetyË† t.
7: if||yË† tâˆ’y t|| 2 <Î´then
8: N call(xË† t)â†N call(xË† t)+1
9: ifÎ¾ T ==1then
10: N Î¾(xË† t)â†N Î¾(xË† t)+1
11: endif
12: ifÎ¾ t ==0andÎ¾ T ==1then
13: Î¾ t â†Î¾ T â–·desirabilitypropagation
14: xË† t â†x t,yË† t â†y t,sË† t â†s t â–·memoryshift
15: HË† t â†R t
16: else
17: ifHË† t <R tthenHË† t â†R t
18: endif
19: endif
20: else
21: AddmemoryD E â†(x t,y t,R t,s t,Î¾ t)
22: endif
23: endfor
Formemorycapacityandlatentdimension, weusedthesamevaluesasZhengetal.(2021), and
Table6showsthesummaryofhyperparameterrelatedtoepisodicmemory.
Table6: ConfigurationofEpisodicMemory.
Configuration Value
episodiclatentdimension,dim(x) 4
episodicmemorycapacity 1M
ascalefactor,Î»
0.1
(forconventionalepisodiccontrolonly)
ThememoryconstructionforEMUseemstorequireasignificantlylargememoryspace,especially
forsavingglobalstatess. However,D usesCPUmemoryinsteadofGPUmemory,andthememory
E
requiredfortheproposedembedderstructureisminimalcomparedtothememoryusageoforiginal
31PublishedasaconferencepaperatICLR2024
Table7: AdditionalCPUmemoryusagetosaveglobalstates.
CPUmemoryusage(1Mdata)
SMACtask
(GiB)
5m_vs_6m 0.4
3s5z_vs_3s6z 0.9
MMM2 1.2
RLtraining(<1%). Thus, amemoryburdenduetoatrainableembeddingstructureisnegligible.
Table7presentsexamplesofCPUmemoryusagetosaveglobalstatessâˆˆD .
E
E.2 OVERALLTRAININGALGORITHM
In this section, we present details of the overall MARL training algorithm including training of
f . AdditionalhyperparametersrelatedtoAlgorithm1toupdateencoderf anddecoderf are
Ï• Ï• Ïˆ
presentedinTable8. NotethatvariablesN andBareconsistentwithAlgorithm1.
Table8: EMUHyperparametersforf andf training.
Ï• Ïˆ
Configuration Value
ascalefactorofreconstructionloss,Î» 0.1
rcon
updateinterval,t 1K
emb
trainingsamples,N 102.4K
batchsizeoftraining,B 1024
Algorithm3presentsthepseudo-codeofoveralltrainingforEMU.InAlgorithm3,networkparame-
tersrelatedtoamixerandindividualQ-networkaredenotedasÎ¸,anddoubleQ-learningwithtarget
networkisadoptedasotherbaselinemethods(Rashidetal.,2018;2020;Wangetal.,2020b;Zheng
etal.,2021;Chenghaoetal.,2021).
Algorithm3EMU:EfficientepisodicMemoryUtilizationforMARL
1: D: Replaybuffer
2: D E: Episodicbuffer
3: Qi: IndividualQ-networkofnagents
Î¸
4: M: BatchsizeofRLtraining
5: InitializenetworkparametersÎ¸,Ï•,Ïˆ
6: whilet env â‰¤t maxdo
7: InteractwiththeenvironmentviaÏµ-greedypolicybasedon[Qi]n andgetatrajectoryT.
Î¸ i=1
8: RunAlgorithm2toupdateD E withT
9: AppendT toD
10: fork =1ton do
circle
11: GetM sampletrajectories[T]M âˆ¼D
i=1
12: RunMARLtrainingalgorithmusing[T]M i=1andD E,toupdateÎ¸withEq.10
13: endfor
14: ift env modt emb==0then
15: RunAlgorithm1toupdateÏ•,Ïˆ
16: UpdateallxâˆˆD E withupdatedf Ï•
17: endif
18: endwhile
Here,anyCTDEtrainingalgorithmcanbeadoptedforMARLtrainingalgorithminline 12in
Algorithm3. AswementionedinSectionC.4,trainingoff andf andupdatingallxâˆˆD only
Ï• Ïˆ E
32PublishedasaconferencepaperatICLR2024
takeslessthantwosecondsatmostunderthetaskwithlargeststatedimensionsuchascorridor.
Thus,thecomputationburdenfortrainableembedderisnegligiblecomparedtotheoriginalMARL
training.
F MEMORY UTILIZATION
AremainingissueinutilizingepisodicmemoryishowtodetermineaproperthresholdvalueÎ´ in
Eq. 1. NotethatthisÎ´isusedforbothupdatingthememoryandrecallingthememory. Onesimple
option is determining Î´ based on prior knowledge or experience, such as hyperparameter tuning.
Instead,inthissection,wepresentamorememory-efficientwayforÎ´selection. Whencomputing
||xË†âˆ’x|| < Î´, thesimilarityiscomparedelementwisely. However, thissimilaritymeasureputs
2
adifferentweightoneachdimensionofxsinceeachdimensionofxcouldhaveadifferentrange
of distribution. Thus, instead of x, we utilize the normalized value. Let us define a normalized
embeddingywiththestatisticalmean(Âµ )andvariance(Ïƒ )ofxas
x x
y =(xâˆ’Âµ )/Ïƒ . (21)
x x
Here, the normalization is conducted for each dimension of x. Then, the similarity measure via
||yË†âˆ’y|| < Î´ with Eq. 21 puts an equal weight to each dimension, as y has a similar range of
2
distributionineachdimension. Inaddition,anaffineprojectionofEq. 21maintainsthecloseness
oforiginalx-distribution,andthuswecansafelyutilizey-distributioninsteadofx-distributionto
measurethesimilarity.
Inaddition,ydefinedinEq. 21nearlyfollowsthenormaldistribution,althoughitdoesnotstrictly
followit. ThisisduetothefactthatthememorizedsamplesxinD donotoriginatefromthesame
E
distribution,noraretheyuncorrelated,astheycanstemfromthesameepisode. However,wecan
achieveanapproximatecoverageofthemajorityofthedistribution,specifically3Ïƒ inbothpositive
y
andnegativedirectionsofy,bysettingÎ´as
(2Ã—3Ïƒ )dim(y)
Î´ â‰¤ y . (22)
M
Forexample,whenM = 1e6 anddim(y) = 4,ifÏƒ â‰ˆ 1thenÎ´ â‰¤ 0.0013. Thisisthereasonwe
y
selectÎ´ =0.0013fortheexploratorymemoryrecall.
33