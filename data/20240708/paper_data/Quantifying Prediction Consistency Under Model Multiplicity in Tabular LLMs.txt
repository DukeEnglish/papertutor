Quantifying Prediction Consistency
Under Model Multiplicity in Tabular LLMs
FaisalHamman1 PasanDissanayake1 SaumitraMishra2 FreddyLecue2 SanghamitraDutta1
1UniversityofMaryland,CollegePark 2JPMorganAIResearch
{fhamman, pasand, sanghamd}@umd.edu
{saumitra.mishra, freddy.lecue}@jpmorgan.com
Abstract
Fine-tuninglargelanguagemodels(LLMs)onlimitedtabulardataforclassification
taskscanleadtofine-tuningmultiplicity,whereequallywell-performingmodels
makeconflictingpredictionsonthesameinputsduetovariationsinthetraining
process(i.e.,seed,randomweightinitialization,retrainingonadditionalordeleted
samples). This raises critical concerns about the robustness and reliability of
TabularLLMs,particularlywhendeployedforhigh-stakesdecision-making,such
asfinance,hiring,education,healthcare,etc. Thisworkformalizesthechallengeof
fine-tuningmultiplicityinTabularLLMsandproposesanovelmetrictoquantify
therobustnessofindividualpredictionswithoutexpensivemodelretraining. Our
metricquantifiesapredictionâ€™sstabilitybyanalyzing(sampling)themodelâ€™slocal
behavior around the input in the embedding space. Interestingly, we show that
sampling in the local neighborhood can be leveraged to provide probabilistic
robustnessguaranteesagainstabroadclassoffine-tunedmodels. Byleveraging
Bernsteinâ€™sInequality,weshowthatpredictionswithsufficientlyhighrobustness
(asdefinedbyourmeasure)willremainconsistentwithhighprobability. Wealso
provideempiricalevaluationonreal-worlddatasetstosupportourtheoreticalresults.
Ourworkhighlightstheimportanceofaddressingfine-tuninginstabilitiestoenable
trustworthydeploymentofLLMsinhigh-stakesandsafety-criticalapplications.
1 Introduction
Largelanguagemodelsaregeneratingsignificantinterestinhigh-stakesapplications,e.g.,finance,
healthcare,etc.,particularlyinfew-shotclassificationscenariosontabulardatasets. Recentfindings
demonstratethatthesemodelsperformcommendablyinsuchscenarios,especiallywhenverylittle
training data is available, due to their transfer learning abilities [Hegselmann et al., 2023, Dinh
etal.,2022,Yinetal.,2020,Yanetal.,2024,Wangetal.,2023]. However,modelsinthesesettings
areoftenfine-tunedfromlargepretrainedmodelswithmillionsorbillionsofparametersonsmall
proprietarydatasets[Huetal.,2021,Liuetal.,2022]. Thispaucityoftrainingdata,combinedwith
largeparameterspaces,risksinstabilityacrossdifferentfine-tunedvariantsofthepre-trainedmodel.
Weintroducethethenotionoffine-tuningmultiplicityastothephenomenonwheremultiplecompeting
models,fine-tunedfromthesamepre-trainedLLMbutunderslightlyvaryingconditions(suchas
differentrandomseedsorminorchangesinthetrainingdata),exhibitcomparableperformance(e.g.,
intermsofaccuracy)yetgenerateconflictingpredictionsforthesameinputs. Thisconceptisclosely
relatedtopredictivemultiplicity,oftenreferredtoastheRashomoneffectinthecontextofneural
networks[Marxetal.,2020,Breiman,2003,HsuandCalmon,2022]. Thisisparticularlyalarming
forhigh-stakesapplications,suchasfinance[Yinetal.,2023],healthcare[Wangetal.,2024,Chen
etal.,2023,Kimetal.,2024],wherearbitrarypredictionscanhavesignificantconsequences.
Preprint.
4202
luJ
4
]GL.sc[
1v37140.7042:viXraThefine-tunedmodelsforhigh-stakesdecisionsmighthavetobeupdatedduetovariousreasons,
e.g.,toretrainonadditionaldatapointstoimproveperformance[Wuetal.,2024],orevenremoving
datapointsforprivacy. Forinstance,regulatoryframeworksliketheGDPR[Voigt,2017]introduces
therighttobeforgottenwhichnecessitatestheremovalofanindividualâ€™sdatauponrequest, po-
tentiallyleadingtomodelupdates. Theseupdatescould,inturn,impactthevalidityofpreviously
issuedpredictions. Thephenomenonoffine-tuningmultiplicitynotonlyraisesconcernsaboutmodel
reliabilitybutalsopavesthewayforfairwashingandexplanationbias[Blacketal.,2022,Sokoletal.,
2023]makingquantifyingrobustnessagainstfine-tuningmultiplicityanimportantproblem.
SincemethodstomeasuremultiplicityinclassicalMLareimpracticalforLLMsduetothecomputa-
tionalchallengeofretrainingseveralfine-tunedmodels,weproposeanovelmetrictoquantifythe
robustnessofindividualpredictionswithoutrequiringexpensiveretrainingofmultiplemodels. Our
metric,termedconsistency,leveragethemodelâ€™slocalbehavioraroundeachinputsamplewithinthe
embeddingspacetoestimatethepredictionâ€™ssusceptibilitytomultiplicity. Interestingly,byanalyzing
thislocalneighborhood,wecanderiveprobabilisticguaranteesontherobustnessofpredictionswith
highconsistencyscoresunderabroadrangeofpotentialfine-tunedmodelupdates.
Ourcontributionissummarizedasfollows:
â€¢ Modelmultiplicityinfine-tunedtabularLLMs. Weshowthatmodelmultiplicityisacritical
issueinthecontextoffine-tuninglargelanguagemodels(LLMs)fortabulardatatasks. Multiple
fine-tunedmodelsmayperformequallywell,yetassignconflictingpredictionstothesameinput
duetorandomtraininginitialization,wetermthisfine-tuningmultiplicity. Toevaluatetheextent
ofmultiplicity,wefinetuneseveralmodelsandevaluatetheArbitrariness,Discrepancy,Pairwise
Disagreement, Prediction Variance, and Prediction Range (see Definitions 1,2,3,4, and 5 in
Section2). Thismultiplicityissueraisesconcernsabouttherobustnessandreliabilityoffine-tuned
LLMs,whichareincreasinglybeingdeployedinhigh-stakesdecision-makingscenarios.
â€¢ Ameasuretoquantifypredictionrobustnessamidstfine-tuningmultiplicity. Weintroducea
novelmetric,termedconsistency(seeDefinition6),toquantifytherobustnessofmodelpredictions
inthefaceoffine-tuningmultiplicity,withoutretrainingseveralmodels. Givenaninputx âˆˆ X
andmodelf(Â·)âˆˆ[0,1],ourrobustnessmeasureisgivenas: S (x,f)= 1 (cid:80) (f(x )âˆ’
k,Ïƒ k xiâˆˆNx,k i
|f(x)âˆ’f(x )|),whereN isasetofkpointssampledindependentlyfromadistributionovera
i x,k
hypersphereofradiusÏƒcenteredatx. Consistencyleveragestheneighborsofagivensample(in
theembeddingspace)toinformthereliabilityofitsprediction. Thefirsttermessentiallycaptures
the mean value of the model output in a region around it. The second term captures the local
averagevariabilityofthemodeloutputaroundit(lowervariabilityisexpectedtobemorereliable).
â€¢ Probabilisticguaranteesonconsistencyoverabroadrangeoffine-tunedmodels. Ourmain
contributionistoprovideatheoreticalguarantee(seeTheorem1)thatpredictionswithsufficiently
highconsistency(asdefinedbyourmeasure)willremainconsistentwithhighprobabilityover
abroadrangeoffine-tunedmodels. Toachievethisguarantee,wecharacterizethebehaviorand
statisticalpropertiesofabroadclassoffine-tunedmodels(seeAssumption1;StochasticFine-Tuned
ModelClass). OurresultsleverageBernsteinâ€™sInequality(seeLemma2)toestablishrigorous
concentrationboundsontheNeighborhoodDivergence(seeLemma1),whicharethenusedto
proveourtheoreticalguarantee.
â€¢ Empiricalvalidation. Inourempiricalanalysis,weevaluatetheextentoffine-tuningmultiplicity
andvalidateourproposedconsistencymeasureS (x,f)ontheDiabetes,GermanCredit,and
k,Ïƒ
Adultdatasets[Kahn,Hofmann,1994,BeckerandKohavi,1996]. WeemploytheBigScienceT0
encoder-decodermodel[Sanhetal.,2021],fine-tunedviatheT-Fewrecipe[Liuetal.,2022],and
LORA [Huetal.,2021]. WemeasureArbitrariness,Discrepancy,PairwiseDisagreement,and
PredictionVariance(seeSection2)toevaluatetheextentoffine-tuningmultiplicity. Westudy
howeffectivelyourconsistencymeasureS (x,f),(asmeasuredonlyonmodelf(Â·))captures
k,Ïƒ
multiplicityofpredictionsoverabroadrangeoffine-tunedmodels.
1.1 RelatedWorks
LLMintabularpredictions. TheapplicationofLLMstotabulardataisagrowingareaofresearch,
demonstratingsignificantperformanceduetothetransferlearningcapabilities[Yinetal.,2020,Li
etal.,2020,Narayanetal.,2022,Borisovetal.,2022,Bertsimasetal.,2022,Onishietal.,2023,
Zhangetal.,2023,Wangetal.,2023,Suietal.,2024,Yanetal.,2024,Yangetal.,2024]. Yinetal.
2(a) Finetuning LLMs for Tabular Data
Tabular data Serialized Input LLM
Age Education Gain Income Fine-tune LLM using
labeled examples
39 Bachelor 2174 <50K
36 HS-grad 0 >50K This person is 42 Predictions Labels
64 12th 0 <50K y me aa sr ts ersold, dh ea gs reea , Pr metr oa di en led paT ru an ma eb tl ee r s Yes >50K
29 Doctorate 1086 >50K andagainof594
42 Master 594
Serialize feature names and values Backpropagation to update tunable parameters
into natural-language string
(b) Finetuning Multiplicity in Tabular LLMs (c) Our Proposed Consistency Measure
Test Input Embedding Layer Predicted probabilities
Seed = 42
ğ’€ğ’†ğ’”
T y Ph e hai Ds rs dp ee o gr ls rdo e, en ,h ai as ns d23 aa Pr metr oa di en led Se Se ed e d= =8 2 ğ‘µ ğ’€ğ’ ğ’†ğ’” Input Pr metr oa di en led paT ru an ma eb tl ee r s
gainof240
Seed = 0 ğ‘µğ’
We sample ğ‘˜points in a Consistency measured
Models fine-tuned from the same pre-trained LLM under slightly varying bounded neighborhood using predictions from
conditions (e.g. random seeds), exhibit comparable performance yet of a given input in the ğ‘¥ perturbated points
generate conflicting predictions for the same inputs. embedding space.
Figure 1: (a) illustrates the process of fine-tuning LLMs for Tabular data using few labeled ex-
amples [Hegselmann et al., 2023, Dinh et al., 2022]. (b) demonstrates the concept of finetuning
multiplicity. Modelsfine-tunedfromthesamepre-trainedLLMunderslightlyvaryingconditions,
suchasdifferentrandomseeds,canexhibitcomparableperformancemetricsbutmayyieldconflict-
ingpredictionsforthesameinput. (c)introducesourproposedconsistencymeasuredesignedto
quantifytherobustnessofindividualpredictionswithoutrequiringtheretrainingofmultiplemodels.
Bysamplingpointsinaboundedneighborhoodaroundagiveninputintheembeddingspace,the
consistencymeasureS (x,f)informsapredictionâ€™ssusceptibilitytomultiplicity.
k,Ïƒ
[2020],Jaitlyetal.[2023]incorporatedcolumndatatypesintotheserializedstrings,addingalayerof
descriptivecontexttothedata. Dinhetal.[2022]proposesLIFT,amethodforadaptingLLMstonon-
languageclassificationandregressiontaskswithoutchangingthemodelarchitectureorlossfunction.
Hegselmannetal.[2023]investigatestheuseofLLMsforzero-shotandfew-shotclassificationof
tabulardataandfindsthatthismethodoutperformspreviousdeep-learning-basedapproachesand
is competitive with traditional baselines like gradient-boosted trees. Wang et al. [2024] presents
MediTab,amethodthatusesLLMstocombinedifferentmedicaldatasets,significantlyimproving
predictionsforpatientandtrialoutcomes. TabularLLMshavealsobeenappliedinotherhigh-stakes
domains [Chen et al., 2023, Kim et al., 2024, Li et al., 2023, Yin et al., 2023]. Yin et al. [2023]
presentsFinPTanLLMbasedapproachtofinancialriskprediction. WerefertoFangetal.[2024]for
amoredetailedsurveyonLLMsonTabularData.
Modelmultiplicityinmachinelearning. Breiman[2003]introducedtheideathatmachinelearning
modelscandiffersignificantlywhileachievingsimilaraverageperformance,knownastheRashomon
effect. Marxetal.[2020]highlightedtheprevalenceofarbitrarydecisionsinsimpleclassification
problems, coining this phenomenon predictive multiplicity. [Creel and Hellman, 2022] discuss
theharmsofpredictivemultiplicityandarbitrarydecisions. Effortstoleveragemodelmultiplicity
beneficiallywhileaddressingitsimplicationshavebeenexploredby[Blacketal.,2022,Fisheretal.,
2019, Xin et al., 2022, Coston et al., 2021]. The effect of model multiplicity in fairness [Sokol
etal.,2022]andexplainabilityareexaminedby Hammanetal.[2023],Blacketal.[2021],Dutta
etal.[2022],Pawelczyketal.[2020]. Watson-Danielsetal.[2023],HsuandCalmon[2022]offered
aframeworkformeasuringpredictivemultiplicityinclassicalMLmodels,howeverthisinvolves
retrainingseveralmodels. ModelmultiplicityhasnotbeenextensivelystudiedintabularLLMs. The
closestworkisby[Gomezetal.,2024],whichinvestigatespredictionarbitrarinessforonlinecontent
moderation. WeleveragetherichembeddingspaceofLLMstoquantifypronenesstomultiplicity
withouttheneedforexpensiveretraining,asfine-tuningLLMsiscomputationallyexpensive.
1.2 Preliminaries
We consider a classification task for a tabular dataset D = {(x ,y )}n , where each x is a d-
i i i=1 i
dimensionalfeaturevector(rowsofatabularinput),andeachlabely isbinary,y âˆˆ {0,1}. We
i i
3
â€¦study an n-shot classification problem by fine-tuning a pre-trained model on n examples from a
trainingset. Thisfine-tuningprocessaimstoadaptthepre-trainedmodeltoeffectivelypredictnew,
unseendatapointsbylearningfromalimitednumberoftrainingexamples.
SerializationofTabularDataforLLMs: ToeffectivelyapplyLLMstotabulardata,itiscrucialto
transformthedataintoanaturaltextformat. Thisprocess,knownasserialization,involvesconverting
the table rows into a text string that includes both the column names and their corresponding
values[Yinetal.,2020,Jaitlyetal.,2023,Hegselmannetal.,2023,Dinhetal.,2022]. Theresultant
serializedstringiscombinedwithatask-specificprompttoformtheinputfortheLLM.Therehave
beenvariousproposedmethodsforserialization,andthisisstillatopicofactiveresearchHegselmann
etal.[2023],Jaitlyetal.[2023]. Amongtheserializationswehaveexaminedare: listtemplate(A
listofcolumnnamesandfeaturevalues),andtexttemplate(â€œThe<columnname>is<value>.â€).
LLMscanbeadaptedforclassificationtasksbytrainingthemonserializedtabulardata. Thistraining
involvesusingthenatural-languageoutputsoftheLLM,mappedtovalidclassesinthetargetspace,
aspartofafine-tuningprocess(seeFigure1).
Toclarify,tablevaluesareserializedintoserialize(x)andthentransformedintoaformatunderstand-
ablebytheLLM,tokenize(serialize(x)),whichissomeembedding. Sincethesetransformationsare
one-to-onemappings,wedenotetheembeddedformofxasxâˆˆX torepresentxintheembedding
space. Thisallowsustosimplifythenotationanddirectlyusextorefertothetablevaluesinthe
embeddingspace.
Goals. Ourprimarygoalistodefineameasureofpredictiveconsistencytofine-tuningmultiplicity
thatdoesnotrequireretrainingmultiplemodelssincethisiscomputationalexpensiveforLLMs. We
aimtotheoreticallymotivatethismeasureandprovidetheoreticalguaranteesonitseffectivenessin
ensuringconsistentpredictionsacrossabroadrangeoffine-tunedmodels(seeFigure1).Additionally,
we intend to evaluate this measure against multiplicity metrics by fine-tuning several models to
empiricallyvalidateitsrobustnessandreliability.
2 ModelMultiplicityinFine-TunedTabularLLMs
Letf(Â·):X â†’[0,1]denoteanLLMthatperformsbinaryclassification,outputtingtheprobability
distributionovertheclasses. WeletF denoteabroadclassoffine-tunedmodelsthatareequally
well-performing (i.e., a set of competing models as measured by the accuracy), i.e, F = {f :
Î´
err(f) â‰¤ err(f )+Î´}whereerr(f ) = 1 (cid:80)n I[fË†(x ) Ì¸= y ]forareferencemodelf (with
0 0 n i=1 0 i i 0
satisfactoryaccuracy)adatasetwithnexamples. Here,fË†(x)=I[f(x)â‰¥0.5]denotesthepredicted
labels. ThisisasetofmodelsthatperformjustaswellasthebaselineclassifierwhereÎ´ âˆˆ(0,1)is
theerrortolerance[Marxetal.,2020]. TheappropriatechoiceofÎ´isapplication-dependent.
Fine-tuningmultiplicity. Weintroducethenotionoffine-tuningmultiplicityastothephenomenon
observedwheremultiplecompetingmodels,fine-tunedfromthesamepre-trainedLLMbutunder
slightlyvaryingconditions(suchasdifferentrandomseedsorminorchangesinthetrainingdata),
exhibitcomparableperformanceyetgenerateconflictingpredictionsforthesameinputs(e.g.,model
inF ). Thisconceptiscloselyrelatedtopredictivemultiplicity,oftenreferredtoastheRashomon
Î´
effectinthecontextofneuralnetworks[Marxetal.,2020,Breiman,2003,HsuandCalmon,2022].
However,thisissuebecomesmorepronouncedwithLLMsbecausethesemodelsaretypicallyfine-
tunedonsmalldatasetsfromlargefoundationalmodels(toleveragetheirtransferlearningability),
whichpossessmillionsorbillionsofparameters. Thispaucityoftrainingdata,combinedwithlarge
parameterspaces,risksinstabilityacrossdifferentfine-tunedvariantsofthemodel(seeFigure1).
2.1 EvaluatingFine-tuningMultiplicity
Toeffectivelyevaluatetheextentoffine-tuningmultiplicity,weintroducespecificempiricalmetrics
thatassesshowpredictionsmayvaryacrossdifferentversionsoffine-tunedmodels. Thesecanonly
beevaluatedwhenweaccesstoseveralfine-tunedmodelsinthecompetingset.
Definition1(Arbitrariness[Gomezetal.,2024]). ArbitrarinessoversetF measurestheextentof
Î´
conflictingpredictionsacrossthemodelspaceforagivensetofinputs{x ,...,x }. Itisdefinedas:
1 n
n
A = 1 (cid:88) I[âˆƒf,fâ€² âˆˆF ,:fË†(x )Ì¸=fË†â€²(x )] (1)
Î´ n Î´ i i
i=1
4ArbitrarinessgeneralizestheAmbiguitymeasurefromMarxetal.[2020]. WhileAmbiguitycomputes
thefractionofpointswhereatleastonemodelinF disagreeswithareferencemodel,arbitrariness
Î´
measuresthepercentageofpointsthatreceiveconflictingpredictionsfromanytwomodelswithin
thesetF . Arbitrarinesscanbedefinedonaninput,i.e.,A(x )=I[âˆƒf,fâ€² âˆˆF ,:fË†(x )Ì¸=fË†â€²(x )]
Î´ i Î´ i i
Definition2(Discrepancy). Discrepancyquantifiesthemaximumproportionofconflictingpredictions
betweenthereferencemodelandanycompetingmodelintheset. Itisdefinedas:
(cid:32) n (cid:33)
D (f ):= max 1 (cid:88) I[fË†(x )Ì¸=fË†(x )] (2)
Î´ 0 fâˆˆFÎ´ n
i=1
i 0 i
Discrepancymeasuresthemaximumnumberofpredictionsthatcouldchangeifareferencemodelis
replacedwithacompetingmodel. Thismeansthat,inpractice,alteringmultiplepredictionsrequires
thatallconflictingpredictionscomefromasinglecompetingmodel.
Definition 3 (Pairwise Disagreement [Black et al., 2022]). Pairwise Disagreement assesses the
variabilityamongmodelsbymeasuringtheproportionofinstanceswherepairsofmodelswithinthe
competingsetdisagree:
PD (x):= 1 (cid:88) I[fË†(x)Ì¸=fË†(x)] (3)
Î´ |F |(|F |âˆ’1) i j
Î´ Î´
fi,fjâˆˆFÎ´,fiÌ¸=fj
Mostexistingmeasuresofmultiplicityfocusonpredictedlabels. Weproposemorenuancedmeasures
thatleveragesthepredictedprobabilitiesofmodeloutputs:
Definition4(PredictionVariance). PredictionVariancePDmeasuresthevariabilityofthemodel
outputsforagiveninputxacrossdifferentmodelswithinthesetF . Itisdefinedas:
Î´
1 (cid:88)
(cid:18)
1 (cid:88)
(cid:19)2
PV (x):= f(x)âˆ’ f(x) (4)
Î´ |F | |F |
Î´ Î´
fâˆˆFÎ´ fâˆˆFÎ´
PredictionVariancecapturesthevariabilityofpredictionsforagiveninputacrossthecompeting
setofmodels(higherPV indicatesmoremultiplicity). Unlikethreshold-dependentmeasures,PV
doesnotrelyonaccept/rejectthresholds. ThismeanslowPV centeredaroundthe0.5threshold
mayindicatehighmultiplicitywhenusingarbitrariness,discrepancy,orpairwisedisagreementwhile
theseothermeasuresmayoverlookhighPV centeredononesideofthedecisionboundary. Wealso
introduceasimilarmeasurethatcapturesthemaximumdisparityinpredictions:
Definition5(PredictionRange). PredictionRange(PR)measurestherangeofmodeloutputsfora
giveninputxacrossdifferentmodelsinthesetF . Itisdefinedas:
Î´
PR (x):= maxf(x)âˆ’ min f(x) (5)
Î´
fâˆˆFÎ´ fâˆˆFÎ´
Remark1. Giventheinfeasibilityofcomputingtheexactsizeof|F |duetoitspotentiallyvastmodel
Î´
space,weemployanexpensivesamplingapproach,i.e.,re-fine-tuningonvariousseeds. Weselecta
finitenumberofmodelsfromF forpracticalevaluation,allowingustoevaluatethemultiplicity
Î´
metrics. Itisverycomputationallyexpensivetofine-tuneseveralmodelstoevaluatemultiplicity. This
motivatestheneedforameasuretoquantifyconsistencygivenonefine-tunedmodel.
3 QuantifyingPredictionConsistencyamidstFine-tuningMultiplicity
OurobjectiveistodevelopameasuredenotedasS (x,f),foraninputxandagivenfine-tuned
k,Ïƒ
modelf,thatquantifiesitsrobustnessofpredictionstoabroadclassoffine-tunedmodels. Ideally,
wedesirethatthemeasureS (x,f)shouldbehighiftheinputxisconsistentacrossthisbroad
k,Ïƒ
classoffine-tunedmodels(seeFigure1).
3.1 ProposedConsistencyMeasure
Predictionprobabilitiesasameasureofconsistency. Usingthepredictionprobabilitiesofmodel
f(Â·)asameasureofpredictioncertaintycanofferinsightsintothemodelâ€™sconfidenceinitspredic-
tionsinchoosingacertainclass. Whileahighpredictionprobabilityf(Â·)mightsuggestastrong
5confidenceinaparticularclass,weshowthatrelyingsolelyonf(x)forassessingtherobustness
ofpredictionsagainstmodelmultiplicityisinsufficient(seeTable4, Figure2, i.e., sampleswith
highf(x)orconfidencecanstillbesusceptibletomultiplicity). Therefore,weproposeleveraging
thelocalneighborhoodaroundtheinputxintheembeddingspace. Thismotivatesustoderivea
theoreticalmeasureofconsistency.
Definition6(Consistency). Theconsistencyofagivenpredictionf(x)âˆˆ[0,1]isdefinedasfollows:
1 (cid:88)
S (x,f)= (f(x )âˆ’|f(x)âˆ’f(x )|), (6)
k,Ïƒ k i i
xiâˆˆNx,k
whereN isasetofk pointssampledindependentlyfromadistributionoverahypersphereof
x,k
radiusÏƒcenteredatx,i.e.,N ={x ,x ,...,x }âŠ‚B(x,Ïƒ)={xâ€² âˆˆX :âˆ¥xâ€²âˆ’xâˆ¥ <Ïƒ}.
x,k 1 2 k 2
Remark2. Ourconsistencymeasureisfundamentallytiedtotheconfidenceinpredictingaspecific
class. The concept can be seamlessly applied by considering the logits (or softmax outputs) for
predicting any given class. This approach can also be extended to multi-class classification by
providing logits for each class, thereby maintaining the measureâ€™s applicability across various
classificationtasks.
3.2 TheoreticalGuaranteesonConsistency
Here,wepresenttheoreticalinsightsthatmotivateandprovideguaranteesforourproposedrobustness
measureS (x,f),ensuringconsistentpredictionsacrossabroadclassoffine-tunedmodels. We
k,Ïƒ
representtheclassoffine-tunedmodelsbyastochastic(random)functionF,suchthatF âˆˆF. We
denotetworandommodels,F andFâ€²,bothofwhichareindependentlyandidenticallydistributed
withinF. Forclarity,weusecapitalletters(e.g.,F,Fâ€²,X ,Z)todenoterandomvariables,while
i
lowercaseletters(e.g.,x ,f,Ïµ)indicatespecificrealizations.
i
Inourframework,wedefineasetofassumptionsthatdelineatesthebehaviorofabroadclassof
finetunedmodelsandthestatisticalpropertiesoftheirpredictions.
Assumption1(StochasticFine-TunedModelClass). Wedefinethestochasticdivergencebetween
predictionsoftworandommodels,F andFâ€²as:
Z :=Fâ€²(X )âˆ’F(X )âˆ’|F(X )âˆ’F(x)|+|Fâ€²(X )âˆ’Fâ€²(x)|
i i i i i
withZ = 1 (cid:80)k Z whereX â€™sarerandompointssampledindependentlyfromadistributionover
k i=1 i i
ahypersphereB(x,Ïƒ). Weassume:
â€¢ F(X)andFâ€²(X)areindependentandidenticallydistributedgivenaninputX =x.
â€¢ Var[Z |Fâ€² =fâ€²,F =f]â‰¤Î² forallf,fâ€² âˆˆF.
i
Intuition: ThevariableZ capturestheneighborhoodstochasticdivergencebetweenpredictionsof
twoindependentlyfine-tunedmodelsF andFâ€². Thiscapturesboththedifferenceinpredictionsand
variabilityaroundagivenpointx. ThefirstassumptionsensurethatF andFâ€²provideanunbiased
estimateofthepredictionforx. TheassumptiononthevarianceofZ indicatesthatthevariance
i
ofthestochasticneighborhooddivergencewithinaÏƒ-Ballofasamplebetweenanytwomodelsâ€™
predictionsiscontrolled. TheparameterÎ² essentiallycapturesthesimilarityofthemodelswithinthe
localneighborhoodofasample. ThisconceptisalsosomewhatanalogoustotheLipschitzconstantof
ageneralfunction,whichboundshowmuchthefunctionâ€™soutputcanchangerelativetochangesinits
input. However,inthiscontext,theÎ²-boundreflectsanaveragebehaviorofthemodelsâ€™predictions
withinthelocalneighborhood. ItdoesnotstrictlyenforceauniformLipschitzconstant,especially
consideringthattransformermodelsarenottypicallyLipschitzcontinuous[Kimetal.,2021].
Theorem1(ProbabilisticGuaranteeonConsistency). Givenadatapointx,arandommodelFâ€²and
consistencymeasureS (x,Fâ€²). ThenunderAssumption1,and|E[Z |Fâ€² =fâ€²,F =f]| â‰¤ Ïµâ€²,a
k,Ïƒ i
predictionoverabroadclassoffine-tunedmodelssatisfies:
(cid:18) âˆ’kÏµ2 (cid:19)
Pr(F(x)â‰¥S (x,Fâ€²)âˆ’Ïµ)â‰¥1âˆ’exp , (7)
k,Ïƒ 8Î²+ 16Ïµ
3
forallÏµ>2Ïµâ€²,TheprobabilityisoverthestochasticmodelsF andFâ€²,andtherandomperturbations
X â€™sarerandompointssampledindependentlyfromadistributionoverahypersphereB(x,Ïƒ).
i
6Consistencyguaranteeinterpretation. Essentially,ourconsistencymeasureS(x,Fâ€²)providesa
probabilisticguaranteethatifasamplexhasasufficientlyhighconsistencyscorewithrespectto
arandommodelFâ€²,thenthepredictionofanotherrandommodelF fromthesamebroadclassof
fine-tunedmodelswillbeatleastS(x,Fâ€²)âˆ’Ïµwithhighprobability. Forexample,ifS(x,Fâ€²)=0.8,
wecanbeconfidentthatF(x)willbeatleast0.8âˆ’Ïµwithhighprobability(i.e,predictionwillremain
ontheacceptedside). Thisimpliesthathighconsistencyscoresareindicativeofrobustpredictions
acrossdifferentfine-tunedmodels. Conversely,alowconsistencyscoredoesnotprovidesignificant
informationaboutthepredictionâ€™sbehavior,asitdoesnotguaranteealowerboundontheprediction.
ForF(x) â‰¥ S(x,Fâ€²)âˆ’Ïµtoholdwithhighprobability,alargek isneeded,ideallyk â‰« Î². This
impliesthatwhenÎ² islargethenmoresamplesareneeded. ThecompleteproofofTheorem1is
providedinAppendixB.Here,weincludeaproofsketch.
ProofSketch: From Assumption 1, F and Fâ€² are identically distributed given X , hence
i
E[Fâ€²(X )|X ] = E[F(X )|X ]andE[|Fâ€²(X )âˆ’Fâ€²(x)||X ] = E[|F(X )âˆ’F(x)||X ]. Theterms
i i i i i i i i
inE[Z]canceleachotherout,resultinginE[Z]=0.
ThenextstepoftheproofleveragestheBernsteinâ€™sinequality(seeLemma2)toprovideaboundon
thestochasticneighborhooddivergence(seeLemma1).
Lemma 1 (Neighborhood Divergence Bound). Given the neighborhood discrepancy Z =
1 (cid:80)k (Fâ€²(X )âˆ’F(X )),underAssumption1,foranyÏµËœ>Ïµâ€² >0,wehave:
k i=1 i i
(cid:18) âˆ’k(ÏµËœ+Ïµâ€²)2 (cid:19)
Pr(Z â‰¥Ïµâ€²+ÏµËœ)â‰¤exp . (8)
8Î²+ 16(ÏµËœ+Ïµâ€²)
3
Lemma2(BernsteinInequality). ForagivenrandomvariableX suchthatPr(|X |â‰¤c)=1,and
i i
Î² = 1 (cid:80)k Var[X ]then,foranyÎµ>0,
k i=1 i
Pr(cid:32)(cid:12) (cid:12) (cid:12) (cid:12)k1 (cid:88)k X iâˆ’E(X i)(cid:12) (cid:12) (cid:12) (cid:12)>Îµ(cid:33) â‰¤2exp(cid:18) 2Î²âˆ’ +kÎµ 22 cÎµ(cid:19) , (9)
i=1 3
SeeSridharan[2002]fordetailedproofofBernsteinâ€™sInequality.Thefinalstepsoftheproofleverages
thereversetriangleinequalitysoshow: F(x) â‰¥ 1 (cid:80)k (F(X )âˆ’|F(X )âˆ’F(x)|). Combining
k i=1 i i
thatalongwithLemma1derivesourconsistencymeasureandguarantees.
Propertiesofconsistency. Whileourmeasurewastheoreticallymotivated,italsoexhibitsintuitive
properties. Our consistency measure S (x,f) leverages the modelâ€™s predictions within a local
k,Ïƒ
neighborhoodaroundaninputintheembeddingspace. Itconsiderstwokeyfactors: (i)theaverage
predictionacrossksamplesdrawnfromaÏƒ-Ballaroundx,and(ii)theabsolutevariabilityofthese
predictionsmeasuredby|f(x)âˆ’f(x )|.AhighS (x,f)scoreindicatesthemodelmakesconsistent
i k,Ïƒ
predictionswithlowvariabilityinxâ€™slocalneighborhood,hencepredictionsaremorerobusttominor
perturbations. Conversely,alowscoreimpliesthepredictionissensitivetolocalvariationsandhence
lessreliableundermodelupdates. Theideaisthatsamplinginformsthebehaviourofthemodelina
localneighborhood. Byleveragingtheaveragepredictionsandtheirvariability,consistencyismore
comprehensivethansolelyusingpredictionprobabilities,whichmaynotcapturelocalconsistencies.
4 EmpiricalValidation
Inthissection,weexperimentacrossdifferentdatasetsto(i)quantifytheprevalenceoffine-tuning
multiplicityinTabularLLMs,and(ii)validatetheeffectivenessofourproposedmeasureinquantify-
ingtheconsistencyofpredictionsoverabroadrangeoffine-tunedmodels.
Datasets and Serialization. Our experiments utilize the Diabetes [Kahn], German Credit [Hof-
mann,1994],andAdultdatasets[BeckerandKohavi,1996],serializedusingtheâ€œTextTemplateâ€
method[Hegselmannetal.,2023,Dinhetal.,2022]whereeachtabularentryisconvertedintoa
naturallanguageformatbystatingâ€œThe<columnname>is<value>.â€ Thisapproachhelpsalign
theinputswiththetrainingdistributionofLLMs,enhancingtheirperformanceinbothzero-shotand
few-shotscenarios.
7Table 1: Evaluated Multiplicity for Different Datasets and Number of Shots. Evaluated on 40
fine-tunedmodelsonT-Fewrecipeusingdifferentrandomseeds. Multiplicityobservedinpredictions
acrossdifferentfine-tunedmodel,evenwhenmodelsexhibitsimilaraccuracy(inthissettingÎ´ =0.02).
Fine-tuningusingLORAachievesresultsinthesameballpark(seeLORATable3inAppendixC)
MultiplicityEvaluationMetrics
Dataset No.
Shots Arbitrariness Discrepancy Avg.Pairwise Avg.Pred. Avg.Pred. Avg.Model
Disagreement Variance Range Accuracy
64 10% 9% 7% 0.01 0.10 83%
Adult 128 10% 7% 8% 0.01 0.10 84%
512 11% 8% 7% 0.01 0.12 85%
64 18% 10% 6% 0.01 0.20 71%
German 128 17% 11% 6% 0.01 0.16 71%
512 23% 12% 7% 0.02 0.23 72%
64 29% 18% 10% 0.04 0.31 71%
Diabetes 128 13% 17% 11% 0.03 0.13 72%
512 16% 16% 10% 0.02 0.18 78%
ModelsandFine-tuningMethods. WeutilizetheBigScienceT0encoder-decodermodelasour
pretrainedLLMfortabulartasks[Sanhetal.,2021]. Thismodelwastrainedacrossadiversearrayof
task-specificpromptsincorporatingnumerousdatasets,makingitwell-suitedforourexperimentsin
thefew-shotsettings. Forfine-tuning,weadopttheT-Fewrecipe[Liuetal.,2022],knownforits
performanceinfew-shotsettings,andLORA[Huetal.,2021],aparameter-efficientmethodthat
constrainsweightmatrixupdatestobelow-rank. DetailedsetupcanbefoundinAppendixC
EvaluatingExtentofFine-tuningMultiplicity. Wemeasuretheextentoffine-tuningmultiplicity
acrossthevariousdatasetsandfine-tuningmethods,weusethefollowingmultiplicityevaluation
metrics(introducedinSection2):(1)Arbitrariness-extentofconflictingpredictionsacrossfine-tuned
models (see Definition 1), (2) Discrepancy- the maximum proportion of conflicting predictions
betweenthebaselineclassifierandanycompetingmodel(seeDefinition2),(3)AveragePairwise
Disagreement-theproportionofinstanceswhere pairsofmodelsdisagree(seeDefinition3), (4)
AveragePredictionVariance-variabilityofmodeloutputsoverdifferentfine-tunedmodelsaveraged
overasetofinputs(seeDefinition4),and(5)AveragePredictionRange-rangeofmodeloutputsover
differentfine-tunedmodelsaveragedoverasetoftestinputs(seeDefinition5). Toevaluatethese
multiplicitymetricsacrossourdatasets,wefine-tune40modelsonTfewrecipeandLORAusing
differentrandomseedsandtestonasampleset. OurresultsforTfewaresummarizedinTable1. See
Table3inAppendixCforfine-tuningusingLORA.
ComparingConsistencyMeasuretoEvaluatedMultiplicity. Wecompareourmeasureofcon-
sistencyS (x,f)(measuredonjustonemodel)withthepredictionprobabilityf(x)ininforming
k,Ïƒ
themultiplicity(evaluatedonseveraltrainedmodels). ThiscomparisonismadeusingtheSpearman
correlationcoefficient(seeDefinition7),anonparametricmeasurethatevaluatestherankcorrelation
betweentwovariables. Specifically,wecalculatetheSpearmancorrelationbetweentheconsistency
S (x,f)andthemultiplicity,e.g.,PredictionVariance(PV(x))acrossalltestsamples. Thisanaly-
k,Ïƒ
sisprovidesinsightsintoconsistencyâ€™sutilityinindicatingthepresenceoffine-tuningmultiplicity.
In Figure 2, we visually illustrate the evaluated multiplicity versus our consistency measure for
the128-shotsettingontheAdultdataset(seeFigure3andFigure4inAppendixCforDiabetes
and German Credit datasets). In Table 4, we report the absolute Spearman correlation between
theconsistencymeasureandthevariousmultiplicityevaluationmetricsfor128shotsontheAdult,
GermanCredit,andDiabetesdatasets. SeedetailedTablewith64and512shotcasesinAppendixC.
HyperparameterSelection.Basedonourtheoreticalguarantees,choosingalargerkisadvantageous
asitensurestheguaranteeholdswithhighprobabilityk. However,thisincreasesthecomputational
costofmodelinference(forwardpass). Inourexperiments,weusedk =30,themaximumnumber
thatfitsinoneinferencepassthroughtheGPU,yieldinggoodresults. ForÏƒ,wesampledpointsfrom
atruncatedGaussiandistributionwithavarianceof0.01. Thischoiceperformedwellacrossallour
experiments. ToguidethechoiceofÏƒ,onecouldconsiderthespreadoftrainingsamples. Thechoice
ofÎ´inthecompetingsetF isapplication-dependent. Forourexperiments,weuseÎ´ =0.02,which
Î´
correspondstoa2%marginofaccuracydeviation. Evaluatingmultiplicitybyrefiningseveralmodels
iscomputationallyexpensive,whichmotivatedthiswork. Ideally,samplingmoremodelsprovidesa
betterevaluationofmultiplicity. However,duetocomputationalconstraints,weused40models.
8Figure2: Evaluatedmultiplicity(assessedon40retrainedmodels)versusourconsistencymeasure
(evaluatedononemodel)forthe128-shotsettingontheAdultdataset. Theplotsdemonstratethat
highconsistencyvaluescorrespondtolowmultiplicityacrossvariousmultiplicityevaluationmetrics.
Also,observethathighpredictedprobabilityvalues(i.e.,highpredictionconfidence)doesnotimply
lowmultiplicity. Ourconsistencymeasureprovidesbetterinsightintothemultiplicityofpredictions
comparedtothepredictedprobabilitiesalone. SeeAppendixCforvisualizationsonotherDatasets.
Table 2: This table reports the absolute Spearman correlation between the consistency measure
andvariousmultiplicityevaluationmetricsfor128shotsontheAdult,GermanCredit,andDiabetes
datasets. Inmostcases,theconsistencymeasureS (x,f)showsahighercorrelationwiththese
k,Ïƒ
multiplicitymeasurescomparedtopredictedprobabilities,indicatingthattheconsistencymeasure
S (x,f)betterinformsaboutthemultiplicitythanpredictedprobabilitiesf(x)do. SeefullTable
k,Ïƒ
with64and512shotcasesinAppendixC.
Dataset NumberofShots Measure Arbitrariness PairwiseDisagreement PredictionVariance PredictionRange
Consistency 0.80 0.96 0.84 0.91
Adult 128
Pred.Prob. 0.67 0.62 0.30 0.54
Consistency 0.54 0.54 0.87 0.87
German 128
Pred.Prob. 0.57 0.57 0.86 0.86
Consistency 0.92 0.95 0.93 0.95
Diabetes 128
Pred.Prob. 0.88 0.93 0.93 0.95
5 Discussions
Ourmultiplicityevaluationmetrics,summarizedinTable1,revealsignificantvariabilityinmodel
predictions across different fine-tuned variants, even when they exhibit similar accuracy. This
multiplicityisnotcapturedbymerelyexaminingpredictedprobabilities,aspredictionswithhigh
confidencecanstillbesusceptibletomultiplicity(seeFigure2).Ourconsistencymeasure,S (x,f),
k,Ïƒ
wascomparedwithpredictionprobabilitiesf(x). Theresults,presentedinTable4,demonstratethat
ourconsistencymeasureconsistentlyshowsmainlyhighercorrelationwithmultiplicitymetricsacross
thedatasetscomparedtopredictionprobabilities. ThisindicatesthatS (x,f)ismoreinformative
k,Ïƒ
thanthepredictiveprobabilitiesatinformingthemultiplicity.
Marxetal.[2020]argueforthenecessityofmeasuringandreportingmultiplicitytobetterinform
predictions. TraditionalmethodstomeasuremultiplicityinclassicalMLareimpracticalforLLMs
duetothecomputationalchallengeofretrainingseveralfine-tunedmodels[Marxetal.,2020,Hsu
and Calmon, 2022, Watson-Daniels et al., 2023]. Our proposed measure, which requires only
the given model and leverages the embedding space to inform multiplicity, addresses this issue.
This approach reduces the complexity from retraining and inference to just inference, making it
more feasible to apply in practice. Although, from our theoretical guarantee, a large k (number
of sampled points) might be needed for accurate consistency estimation (particularly when Î² is
large),itremainscomputationallymoreefficientthanretrainingmultiplemodels. Ourworkprovides
practitioners with meaningful information about the multiplicity of predictions, which may lead
themtocarefullyevaluatewhichpredictionstotrustandwhichtotreatwithcaution. Ourresearch
hassignificantimplicationsinseveralhigh-stakesapplications,e.g.,hiring,finance,education,etc.,
whereinconsistentpredictionscanleadtodistrust. Alimitationofourworkisthatwhileweinform
aboutfine-tuningmultiplicityforagivensample,wedonotresolveit. Futureworkcouldfocuson
developingmethodstomitigatefine-tuningmultiplicity,ensuringmoreconsistentmodelpredictions.
9Acknowledgements
ThispaperwaspreparedforinformationalpurposesinpartbytheCDAOgroupofJPMorganChase&
Coanditsaffiliates(â€œJ.P.Morganâ€)andisnotaproductoftheResearchDepartmentofJ.P.Morgan.
J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the
completeness, accuracy or reliability of the information contained herein. This document is not
intendedasinvestmentresearchorinvestmentadvice,orarecommendation,offerorsolicitationfor
thepurchaseorsaleofanysecurity,financialinstrument,financialproductorservice,ortobeused
inanywayforevaluatingthemeritsofparticipatinginanytransaction,andshallnotconstitutea
solicitationunderanyjurisdictionortoanyperson,ifsuchsolicitationundersuchjurisdictionorto
suchpersonwouldbeunlawful.
References
Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI:
https://doi.org/10.24432/C5XW20.
Dimitris Bertsimas, Kimberly Villalobos Carballo, Yu Ma, Liangyuan Na, LÃ©onard Boussioux,
CynthiaZeng,LuisRSoenksen,andIgnacioFuentes. Tabtext: asystematicapproachtoaggregate
knowledgeacrosstabulardatastructures. arXivpreprintarXiv:2206.10381,2022.
EmilyBlack,ZifanWang,MattFredrikson,andAnupamDatta. Consistentcounterfactualsfordeep
models. arXivpreprintarXiv:2110.03109,2021.
EmilyBlack,ManishRaghavan,andSolonBarocas. Modelmultiplicity: Opportunities,concerns,
and solutions. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and
Transparency,pages850â€“863,2022.
VadimBorisov,KathrinSeÃŸler,TobiasLeemann,MartinPawelczyk,andGjergjiKasneci. Language
modelsarerealistictabulardatagenerators. arXivpreprintarXiv:2210.06280,2022.
LeoBreiman. Statisticalmodeling: Thetwocultures. Qualitycontrolandappliedstatistics,48(1):
81â€“82,2003.
ZekaiChen,MariannMicsinaiBalan,andKevinBrown. Languagemodelsarefew-shotlearnersfor
prognosticprediction. arXivpreprintarXiv:2302.12692,2023.
AmandaCoston,AsheshRambachan,andAlexandraChouldechova. Characterizingfairnessoverthe
setofgoodmodelsunderselectivelabels. InMarinaMeilaandTongZhang,editors,Proceedings
ofthe38thInternationalConferenceonMachineLearning,volume139ofProceedingsofMachine
LearningResearch,pages2144â€“2155.PMLR,18â€“24Jul2021. URLhttps://proceedings.
mlr.press/v139/coston21a.html.
Kathleen Creel and Deborah Hellman. The algorithmic leviathan: Arbitrariness, fairness, and
opportunity in algorithmic decision-making systems. Canadian Journal of Philosophy, 52(1):
26â€“43,2022.
Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong
Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuningfor
non-languagemachinelearningtasks. AdvancesinNeuralInformationProcessingSystems,35:
11763â€“11784,2022.
SanghamitraDutta, JasonLong, SaumitraMishra, CeciliaTilli, andDanieleMagazzeni. Robust
counterfactualexplanationsfortree-basedensembles. InInternationalConferenceonMachine
Learning,pages5742â€“5756.PMLR,2022.
XiFang,WeijieXu,FionaAntingTan,JianiZhang,ZiqingHu,YanjunQi,ScottNickleach,Diego
Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos. Large language models (llms) on
tabulardata:Predic-tion,generation,andunderstanding-asurvey.arXivpreprintarXiv:2402.17944,
2024.
10AaronFisher,CynthiaRudin,andFrancescaDominici. Allmodelsarewrong,butmanyareuseful:
Learningavariableâ€™simportancebystudyinganentireclassofpredictionmodelssimultaneously.
JournalofMachineLearningResearch,20(177):1â€“81,2019.
JuanFelipeGomez,CaioVieiraMachado,LucasMonteiroPaes,andFlavioPCalmon. Algorithmic
arbitrarinessincontentmoderation. arXivpreprintarXiv:2402.16979,2024.
FaisalHamman,ErfaunNoorani,SaumitraMishra,DanieleMagazzeni,andSanghamitraDutta. Ro-
bustcounterfactualexplanationsforneuralnetworkswithprobabilisticguarantees. InInternational
ConferenceonMachineLearning,pages12351â€“12367.PMLR,2023.
StefanHegselmann,AlejandroBuendia,HunterLang,MonicaAgrawal,XiaoyiJiang,andDavid
Sontag.Tabllm:Few-shotclassificationoftabulardatawithlargelanguagemodels.InInternational
ConferenceonArtificialIntelligenceandStatistics,pages5549â€“5581.PMLR,2023.
Hans Hofmann. Statlog (German Credit Data). UCI Machine Learning Repository, 1994. DOI:
https://doi.org/10.24432/C5NC77.
HsiangHsuandFlavioCalmon. Rashomoncapacity: Ametricforpredictivemultiplicityinclassifi-
cation. AdvancesinNeuralInformationProcessingSystems,35:28988â€“29000,2022.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685,2021.
SukritiJaitly,TanayShah,AshishShugani,andRazikSinghGrewal. Towardsbetterserializationof
tabulardataforfew-shotclassification. arXivpreprintarXiv:2312.12464,2023.
MichaelKahn. Diabetes. UCIMachineLearningRepository. DOI:https://doi.org/10.24432/C5T59G.
HyunjikKim,GeorgePapamakarios,andAndriyMnih. Thelipschitzconstantofself-attention. In
InternationalConferenceonMachineLearning,pages5562â€“5571.PMLR,2021.
YubinKim,XuhaiXu,DanielMcDuff,CynthiaBreazeal,andHaeWonPark. Health-llm: Large
languagemodelsforhealthpredictionviawearablesensordata. arXivpreprintarXiv:2401.06866,
2024.
XiangyangLi,BoChen,LuHou,andRuimingTang. Ctrl: Connecttabularandlanguagemodelfor
ctrprediction. arXivpreprintarXiv:2306.02841,2023.
YuliangLi,JinfengLi,YoshihikoSuhara,AnHaiDoan,andWang-ChiewTan. Deepentitymatching
withpre-trainedlanguagemodels. arXivpreprintarXiv:2004.00584,2020.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
ColinARaffel. Few-shotparameter-efficientfine-tuningisbetterandcheaperthanin-context
learning. AdvancesinNeuralInformationProcessingSystems,35:1950â€“1965,2022.
CharlesMarx,FlavioCalmon,andBerkUstun. Predictivemultiplicityinclassification. InInterna-
tionalConferenceonMachineLearning,pages6765â€“6774.PMLR,2020.
AvanikaNarayan,InesChami,LaurelOrr,SimranArora,andChristopherRÃ©.Canfoundationmodels
wrangleyourdata? arXivpreprintarXiv:2205.09911,2022.
SomaOnishi,KentaOono,andKoheiHayashi. Tabret:Pre-trainingtransformer-basedtabularmodels
forunseencolumns. arXivpreprintarXiv:2303.15747,2023.
MartinPawelczyk,KlausBroelemann,andGjergjiKasneci. Oncounterfactualexplanationsunder
predictivemultiplicity. InConferenceonUncertaintyinArtificialIntelligence,pages809â€“818.
PMLR,2020.
VictorSanh,AlbertWebson,ColinRaffel,StephenHBach,LintangSutawika,ZaidAlyafeai,Antoine
Chaffin,ArnaudStiegler,TevenLeScao,ArunRaja,etal. Multitaskpromptedtrainingenables
zero-shottaskgeneralization. arXivpreprintarXiv:2110.08207,2021.
11KacperSokol,MeelisKull,JeffreyChan,andFloraDilysSalim. Fairnessandethicsundermodel
multiplicityinmachinelearning. arXivpreprintarXiv:2203.07139,2022.
KacperSokol,MeelisKull,JeffreyChan,andFloraDilysSalim. Cross-modelfairness: Empirical
studyoffairnessandethicsundermodelmultiplicity,2023.
KarthikSridharan. Agentleintroductiontoconcentrationinequalities. Dept.Comput.Sci.,Cornell
Univ.,Tech.Rep,2002.
Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can
largelanguagemodelsunderstandstructuredtabledata? abenchmarkandempiricalstudy. In
Proceedingsofthe17thACMInternationalConferenceonWebSearchandDataMining,pages
645â€“654,2024.
Voigt. Theeugeneraldataprotectionregulation(gdpr). APracticalGuide,1stEd.,Cham: Springer
InternationalPublishing,10(3152676):10â€“5555,2017.
RuiyuWang,ZifengWang,andJimengSun. Unipredict:Largelanguagemodelsareuniversaltabular
predictors. arXivpreprintarXiv:2310.03266,2023.
Zifeng Wang, Chufan Gao, Cao Xiao, and Jimeng Sun. Meditab: Scaling medical tabular data
predictorsviadataconsolidation,enrichment,andrefinement,2024.
JamelleWatson-Daniels,DavidC.Parkes,andBerkUstun. Predictivemultiplicityinprobabilistic
classification,2023.
Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari.
Continuallearningforlargelanguagemodels: Asurvey,2024.
RuiXin,ChudiZhong,ZhiChen,TakuyaTakagi,MargoSeltzer,andCynthiaRudin. Exploringthe
wholerashomonsetofsparsedecisiontrees. Advancesinneuralinformationprocessingsystems,
35:14071â€“14084,2022.
JiahuanYan,BoZheng,HongxiaXu,YihengZhu,DannyChen,JimengSun,JianWu,andJintaiChen.
Makingpre-trainedlanguagemodelsgreatontabularprediction. arXivpreprintarXiv:2403.01841,
2024.
YazhengYang, YuqiWang, SankalokSen, LeiLi, andQiLiu. Unleashingthepotentialoflarge
languagemodelsforpredictivetabulartasksindatascience. arXivpreprintarXiv:2403.20208,
2024.
PengchengYin,GrahamNeubig,Wen-tauYih,andSebastianRiedel. Tabert: Pretrainingforjoint
understandingoftextualandtabulardata. arXivpreprintarXiv:2005.08314,2020.
YuweiYin,YazhengYang,JianYang,andQiLiu. Finpt: Financialriskpredictionwithprofiletuning
onpretrainedfoundationmodels. arXivpreprintarXiv:2308.00065,2023.
HanZhang,XumengWen,ShunZheng,WeiXu,andJiangBian. Towardsfoundationmodelsfor
learningontabulardata. arXivpreprintarXiv:2310.07338,2023.
12A RelevantDefinition
Definition 7 (Spearmanâ€™s Correlation). Spearmanâ€™s correlation, Spearman(X,Y), measures the
strengthanddirectionofthemonotonicrelationshipbetweentworankedvariables. Itiscalculated
asthePearsoncorrelationcoefficientbetweentherankedvariables.
Fornpairsofobservations(X ,Y ),Spearmanâ€™srankcorrelationisgivenby:
i i
6(cid:80)n d2
Spearman(X,Y)=1âˆ’ i=1 i,
n(n2âˆ’1)
whered isthedifferencebetweentheranksofX andY . Alternatively,intermsofcovariance:
i i i
cov(rank(X),rank(Y))
Spearman(X,Y)= ,
Ïƒ Ïƒ
rank(X) rank(Y)
whereÏƒ denotesstandarddeviation. Spearman(X,Y)ranges from âˆ’1(perfectnegativemono-
tonicrelationship)to1(perfectpositivemonotonicrelationship),with0indicatingnomonotonic
relationship.
B ProofofTheoreticalGuarantee
Theorem1(ProbabilisticGuaranteeonConsistency). Givenadatapointx,arandommodelFâ€²and
consistencymeasureS (x,Fâ€²). ThenunderAssumption1,and|E[Z |Fâ€² =fâ€²,F =f]| â‰¤ Ïµâ€²,a
k,Ïƒ i
predictionoverabroadclassoffine-tunedmodelssatisfies:
(cid:18) âˆ’kÏµ2 (cid:19)
Pr(F(x)â‰¥S (x,Fâ€²)âˆ’Ïµ)â‰¥1âˆ’exp , (7)
k,Ïƒ 8Î²+ 16Ïµ
3
forallÏµ>2Ïµâ€²,TheprobabilityisoverthestochasticmodelsF andFâ€²,andtherandomperturbations
X â€™sarerandompointssampledindependentlyfromadistributionoverahypersphereB(x,Ïƒ).
i
Proof. ToproveTheorem1,webeginwithLemma1.
Assumethefine-tunedmodelsF belongtoadiscreteclassofrandomvariables. Aspecificmodel
realization is represented as f for i = 1,2,...,|F |, with the complete set denoted by F =
i Î´
{f ,f ,...,f }. Eachmodelf isselectedwithprobabilityp
,where(cid:80)|FÎ´|p
=1.
1 2 |F| i i i=1 i
Lemma 1 (Neighborhood Divergence Bound). Given the neighborhood discrepancy Z =
1 (cid:80)k (Fâ€²(X )âˆ’F(X )),underAssumption1,foranyÏµËœ>Ïµâ€² >0,wehave:
k i=1 i i
(cid:18) âˆ’k(ÏµËœ+Ïµâ€²)2 (cid:19)
Pr(Z â‰¥Ïµâ€²+ÏµËœ)â‰¤exp . (8)
8Î²+ 16(ÏµËœ+Ïµâ€²)
3
WeshowthatE[Z]=0:
(cid:20) (cid:20) k (cid:21)(cid:21)
E[Z]( =a)E E 1 (cid:88) (Fâ€²(X )âˆ’F(X )âˆ’|F(X )âˆ’F(x)|+|Fâ€²(X )âˆ’Fâ€²(x)|) (10)
Xi F|Xi k i i i i
i=1
k
( =b) 1 (cid:88) E [E [(Fâ€²(X )âˆ’F(X )âˆ’|F(X )âˆ’F(x)|+|Fâ€²(X )âˆ’Fâ€²(x)|)]] (11)
k Xi F|Xi i i i i
i=1
k
( =c) 1 (cid:88) E (cid:2)E[Fâ€²(X )|X ]âˆ’E[F(X )|X ]âˆ’E[|F(X )âˆ’F(x)||X ] (12)
k Xi i i i i i i
i=1
+E[|Fâ€²(X )âˆ’Fâ€²(x)||X ](cid:3) (13)
i i
k
( =d) 1 (cid:88) E (cid:2)E[F(X )|X ]âˆ’E[F(X )|X ]âˆ’E[|F(X )âˆ’F(x)||X ] (14)
k Xi i i i i i i
i=1
+E[|F(X )âˆ’F(x)||X ](cid:3) =0 (15)
i i
13Here (a) holds from applying the law of total expectation. (b) Distributing the expectation over
thesummation. (c)Applyingthelinearityofexpectationsinsidetheinnerexpectation. (d)From
Assumption1,F andFâ€²areidenticallydistributedgivenX ,henceE[Fâ€²(X )|X ]=E[F(X )|X ]
i i i i i
andE[|Fâ€²(X )âˆ’Fâ€²(x)||X ]=E[|F(X )âˆ’F(x)||X ]. Thetermscanceleachotherout,resulting
i i i i
inE[Z]=0. TherestoftheproofleveragesBernstienâ€™sInequality:
Lemma2(BernsteinInequality). ForagivenrandomvariableX suchthatPr(|X |â‰¤c)=1,and
i i
Î² = 1 (cid:80)k Var[X ]then,foranyÎµ>0,
k i=1 i
Pr(cid:32)(cid:12) (cid:12) (cid:12) (cid:12)k1 (cid:88)k X iâˆ’E(X i)(cid:12) (cid:12) (cid:12) (cid:12)>Îµ(cid:33) â‰¤2exp(cid:18) 2Î²âˆ’ +kÎµ 22 cÎµ(cid:19) , (9)
i=1 3
Observethat|Z | = |Fâ€²(X )âˆ’F(X )âˆ’|F(X )âˆ’F(x)|+|Fâ€²(X )âˆ’Fâ€²(x)|| â‰¤ 2. Hence, we
i i i i i
have:
(cid:18) kÏµËœ2 (cid:19)
Pr(|Zâˆ’E[Z|Fâ€² =fâ€²,F =f]|â‰¥ÏµËœ|Fâ€² =fâ€²,F =f)â‰¤2exp âˆ’
2Î²+ 4ÏµËœ
3
where 1 (cid:80)k Var[Z |Fâ€² =fâ€²,F =f]â‰¤Î² fromAssumption1.
k i=1 i
Given|E[Z|Fâ€² =fâ€²,F =f]âˆ’E[Z]|<Ïµâ€²andE[Z]=0,
wehaveâˆ’Ïµâ€² <E[Z|Fâ€² =fâ€²,F =f]<Ïµâ€²âˆ€f,fâ€². Nowobservethat:
(a)
Pr(Z â‰¥Ïµâ€²+ÏµËœ|Fâ€² =fâ€²,F =f)â‰¤Pr(Z â‰¥E[Z|Fâ€² =fâ€²,F =f]+ÏµËœ|Fâ€² =fâ€²,F =f) (16)
(cid:18) âˆ’kÏµËœ2 (cid:19)
â‰¤exp . (17)
2Î²+ 4ÏµËœ
3
Here,(a)holdssinceE[Z|Fâ€² =fâ€²,F =f]<Ïµâ€². Theeventontheleftisasubsetofthatontheright.
Therefore,theprobabilityoftheevent{Z â‰¥Ïµâ€²+ÏµËœ}occurringcannotbemorethantheprobabilityof
theevent{Z â‰¥E[Z|Fâ€² =fâ€²,F =f]+ÏµËœ}occurring.
Pr(Z â‰¥Ïµâ€²+ÏµËœ)( =b)(cid:88) Pr(Z â‰¥Ïµâ€²+ÏµËœ|Fâ€² =f ,F =f )Pr(Fâ€² =f ,F =f ) (18)
i j i j
i,j
(c) (cid:18) âˆ’kÏµËœ2 (cid:19) (cid:88)
â‰¤exp Pr(Fâ€² =f ,F =f ) (19)
2Î²+ 4ÏµËœ i j
3 i,j
(cid:18) âˆ’kÏµËœ2 (cid:19)
=exp (20)
2Î²+ 4ÏµËœ
3
(d) (cid:18) âˆ’k(ÏµËœ+Ïµâ€²)2 (cid:19)
â‰¤exp (21)
8Î²+ 16(ÏµËœ+Ïµâ€²)
3
Here,(b)holdsfromthelawoftotalprobability. Next,(c)followsfromEquation(17). Finally,(d)
holdsfromusingtheinequality4ÏµËœ2 > (ÏµËœ+Ïµâ€²)2 whichholdsforÏµËœ> Ïµâ€² > 0atthenumeratorand
ÏµËœâ‰¤ÏµËœ+Ïµâ€²atthedenominator. SettingÏµ=ÏµËœ+Ïµâ€².
Wehave:
Pr(cid:18) 1 (cid:88)k F(X )â‰¥ 1 (cid:88)k (cid:0) Fâ€²(X )âˆ’|Fâ€²(X )âˆ’Fâ€²(x)|+|F(X )âˆ’F(x)|(cid:1) âˆ’Ïµ(cid:19) â‰¥1âˆ’exp(cid:18) âˆ’kÏµ2 (cid:19) .
k i k i i i 8Î²+ 16Ïµ
i=1 i=1 3
(22)
ObservethatF(x)â‰¥F(x )âˆ’|F(x )âˆ’F(x)|. Thisisaappliesdirectlyfromthereversetriangle
i i
inequality,i.e.,foranyrealnumbersaandb,wehave:|a|â‰¥|b|âˆ’|aâˆ’b|.
14Figure3: Evaluatedmultiplicity(assessedon40retrainedmodels)versusourconsistencymeasure
(evaluatedononemodel)forthe512-shotsettingontheDiabetesdataset. Theplotsdemonstrate
thathighconsistencyvaluescorrespondtolowmultiplicityacrossvariousmultiplicityevaluation
metrics. Predictiveprobabilitiesnotprovidinganyprovidinganyusefulinsightaboutmultiplicity.
Figure4: Evaluatedmultiplicity(assessedon40retrainedmodels)versusourconsistencymeasure
(evaluated on one model) for the 512-shot setting on the German Credit dataset. The plots
demonstratethathighconsistencyvaluescorrespondtolowmultiplicityacrossvariousmultiplicity
evaluationmetrics. InthissettingPredictionprobabilityisperformingcompetitively. Butgenerally
consistency measure provides better insight into the multiplicity of predictions compared to the
predictedprobabilities.
Hence,
k
1 (cid:88)
F(x)â‰¥ (F(X )âˆ’|F(X )âˆ’F(x)|) (23)
k i i
i=1
Therefore,plugging(23)into(22),wehave:
k
Pr(cid:0) F(x)â‰¥ 1 (cid:88) (Fâ€²(X )âˆ’|Fâ€²(X )âˆ’Fâ€²(x)|+|F(X )âˆ’F(x)|âˆ’|F(X )âˆ’F(x)|âˆ’Ïµ)(cid:1)
k i i i i
i=1
(24)
=Pr(cid:0) F(x)â‰¥ 1 (cid:88)k (Fâ€²(X )âˆ’|Fâ€²(X )âˆ’Fâ€²(x)|)âˆ’Ïµ(cid:1) â‰¥1âˆ’exp(cid:18) âˆ’kÏµ2 (cid:19) . (25)
k i i 8Î²+ 16Ïµ
i=1 3
GivenS (x,Fâ€²)= 1 (cid:80)k (F(X )âˆ’|Fâ€²(x)âˆ’Fâ€²(X )|),wehave:
k,Ïƒ k i=1 i i
Pr(cid:0)
F(x)â‰¥S
(x,Fâ€²)âˆ’Ïµ(cid:1)
â‰¥1âˆ’exp(cid:18) âˆ’kÏµ2 (cid:19)
. (26)
k,Ïƒ 8Î²+ 16Ïµ
3
C AppendixtoExperimentSection
C.1 DatasetDetails
AdultDataset TheAdultdataset[BeckerandKohavi,1996],alsoknownasthe"CensusIncome"
dataset, is used for predicting whether an individual earns more than $50,000 annually based on
15Table3: MultiplicityEvaluationMetricsforDifferentDatasetsandNumberofShots. Evaluatedon40fine-
tunedmodelsonLORAusingdifferentrandomseeds. Multiplicityobservedinpredictionsacrossdifferent
fine-tunedmodel,evenwhenmodelsexhibitsimilaraccuracy(inthissettingÎ´=0.02).
MultiplicityEvaluationMetrics
Dataset No.
Shots Arbitrariness Discrepancy Avg.Pairwise Avg.Pred. Avg.Pred. Avg.Model
Disagreement Variance Range Accuracy
64 11% 6% 9% 0.01 0.11 83%
Adult 128 10% 9% 6% 0.01 0.10 84%
512 11% 3% 10% 0.01 0.12 85%
64 19% 10% 6% 0.04 0.40 70%
German 128 17% 11% 6% 0.01 0.16 71%
512 21% 14% 8% 0.03 0.26 72%
64 20% 13% 11% 0.04 0.21 70%
Diabetes 128 16% 14% 11% 0.08 0.14 73%
512 19% 13% 11% 0.04 0.17 76%
variousdemographicattributes. Itconsistsof48,842instanceswith14attributes,includingage,work
class,education,maritalstatus,occupation,relationship,race,sex,capitalgain,capitalloss,hours
perweek,andnativecountry. Thedatasetiscommonlyusedinclassificationtasks.
German Credit Dataset The German Credit dataset [Hofmann, 1994] is used for credit risk
evaluation. It consists of 1,000instances with 20 attributes, which include personal information,
credithistory,andloanattributes. Thetargetvariableindicateswhetherthecreditisgoodorbad.
Thisdatasetisoftenusedforbinaryclassificationproblemsandhelpsinunderstandingthefactors
affectingcreditworthiness. Thedatasetiscommonlyusedinclassificationtasks.
DiabetesDataset TheDiabetesdatasetKahnisusedforpredictingtheonsetofdiabetesbased
ondiagnosticmeasurements. Itcontains768instanceswith8attributes,includingthenumberof
pregnancies,glucoseconcentration,bloodpressure,skinthickness,insulinlevel,bodymassindex
(BMI),diabetespedigreefunction,andage. Thetargetvariableindicateswhethertheindividualhas
diabetes. Thedatasetiscommonlyusedinclassificationtasks.
C.2 ExperimentalSetup
OurexperimentswereconductedusingtheBigScienceT0modelfine-tunedonthreedatasets:German
Credit,Diabetes,andAdultIncome. Weexploredtheperformanceandrobustnessofthefine-tuned
models in few-shot scenarios. The number of shots was set to 64,128, and 512 for each dataset.
To evaluate model multiplicity and consistency, we fine-tuned 40 models with different random
seeds for each dataset and recorded their predictions. The training process involved setting the
batchsizeto2. Thelearningratewassetto0.003. Foreachdataset, wedeterminedthenumber
oftrainingstepsadaptivelybasedonthenumberofshots,ensuringsufficientiterationsformodel
convergence. Specifically,forthenumberofshots-shotsetting,thetrainingstepswerecalculatedas
20Ã—(numberofshots/batchsize).Allexperimentswereperformedon2NVIDIARTXA4500GPU,
utilizingCUDAforefficientcomputation. Theevaluationofmodelperformanceandconsistencywas
conductedatregularintervals,withanevaluationepochintervalsetto30. Toensurereproducibility
androbustnessoftheresults,differentrandomseedswereusedforeachfine-tuningiteration. For
fine-tuningwithLORAweusearankof4.
16Table4: ThistablereportstheSpearmancorrelationbetweentheconsistencymeasureandvariousmultiplicity
evaluation metrics for different numbers of shots on the Adult, German Credit, and Diabetes datasets. In
mostcases,theconsistencymeasureS (x,f)showsahighercorrelationwiththesemultiplicitymeasures
k,Ïƒ
comparedtopredictedprobabilities,indicatingthattheconsistencymeasureS (x,f)betterinformsaboutthe
k,Ïƒ
multiplicitythanpredictedprobabilitiesf(x)do.
Dataset NumberofShots Measure Arbitrariness PairwiseDisagreement PredictionVariance PredictionRange
Consistency 0.95 0.90 0.91 0.89
64
Pred.Prob. 0.67 0.66 0.50 0.62
Adult Consistency 0.80 0.96 0.84 0.91
128
Pred.Prob. 0.67 0.62 0.30 0.54
Consistency 0.90 0.86 0.93 0.92
512
Pred.Prob. 0.70 0.69 0.56 0.72
Consistency 0.95 0.95 0.98 0.84
64
Pred.Prob. 0.99 0.99 0.80 0.79
GermanCredit Consistency 0.54 0.54 0.87 0.87
128
Pred.Prob. 0.57 0.57 0.86 0.86
Consistency 0.59 0.60 0.87 0.86
512
Pred.Prob. 0.54 0.56 0.83 0.82
Consistency 0.45 0.51 0.31 0.23
64
Pred.Prob. 0.03 0.38 0.04 0.08
Diabetes Consistency 0.92 0.95 0.93 0.95
128
Pred.Prob. 0.88 0.93 0.93 0.95
Consistency 0.80 0.89 0.74 0.68
512
Pred.Prob. 0.21 0.23 0.24 0.30
17