Learning Emergence of Interaction Patterns across
Independent RL Agents in Multi-Agent Environments
VasanthReddyBaddam SuatGumussoy
Virginiatech SiemensTechnology
Arlington,VA,UnitedStates Princeton,NJ,UnitedStates
vasanth2608@vt.edu suat.gumussoy@siemens.com
AlmuatazbellahBoker HodaEldardiry
Virginiatech Virginiatech
Arlington,VA,UnitedStates Blacksburg,VA,UnitedStates
boker@vt.edu hdardiry@vt.edu
ABSTRACT applicationswithmultipleagents,interactionsamongagentsarein-
Manyreal-worldproblems,suchascontrollingswarmsofdronesand frequent.Forexample,thinkoftworobotsdeliveringdocumentsin
urbantraffic,naturallylendthemselvestomodelingasmulti-agentre- anoffice.Theydonotneedtocloselymonitoreachotheroften,only
inforcementlearning(RL)problems.However,existingmulti-agent whentheyhavetopassthroughthesamedoor.Similarly,agroupof
RLmethodsoftensufferfromscalabilitychallenges,primarilydue dronesmightbesentonarescuemissionwherecommunicationis
totheintroductionofcommunicationamongagents.Consequently,a challenging.Theycannotalwaysshareinformation,sotheyneedto
keychallengeliesinadaptingthesuccessofdeeplearninginsingle- beinnovativeandcooperateonlywhennecessary.
agentRLtothemulti-agentsetting.Inresponsetothischallenge, Weaskafundamentalquestion:"Wheniscoordinationessential?"
weproposeanapproachthatfundamentallyreimaginesmulti-agent andifneeded,"Howinfrequentcaninteractionsbe?".Ourapproach
environments.Unlikeconventionalmethodsthatmodeleachagent aimstouselocalinteractions,allowingagentstoactindependentlyas
individuallywithseparatenetworks,ourapproach,theBottomUp muchaspossibleandkeepingcommunicationminimal.Thispaper
Network(BUN),adoptsauniqueperspective.BUNtreatsthecollec- focuses on cooperative MARL scenarios in partially observable
tiveofmulti-agentsasaunifiedentitywhileemployingaspecialized environmentswhereagentshavevaryingobservationabilities.
weightinitializationstrategythatpromotesindependentlearning. ThispaperconsidersMARLscenarioswhereinthetaskiscooper-
Furthermore,wedynamicallyestablishconnectionsamongagents ativeandagentsaresituatedinapartiallyobservableenvironment.
usinggradientinformation,enablingcoordinationwhennecessary However,eachisendowedwithdifferentobservationpower.This
whilemaintainingtheseconnectionsaslimitedandsparsetoeffec- paperaimstotacklethisproblembyreimaginingthemulti-agent
tivelymanagethecomputationalbudget.Ourextensiveempirical settingbyredefiningtherepresentationofthemulti-agentsystem,
evaluationsacrossavarietyofcooperativemulti-agentscenarios, treatingitasasingleagentwithauniqueneuralnetworkweight
includingtaskssuchascooperativenavigationandtrafficcontrol, initializationscheme.Thisschemeensuresdecentralizedoperation,
consistentlydemonstrateBUNâ€™ssuperiorityoverbaselinemethods allowingeachagenttoactlocallyandindependentlywithoutinteract-
withsubstantiallyreducedcomputationalcosts. ingwithotheragentsduringexecution.Subsequently,weleverage
gradient information among agents to establish connections, en-
KEYWORDS ablingcoordinationwhennecessary.Importantly,wemaintainthese
connectionsaslimitedandsparse.
SparseTraining,Multi-AgentEnvironments,Communication
Contributions:Inthispaper,weintroduceanapproachinspired
bythebottom-upapproachdescribedfromourpreviouswork[21],
1 INTRODUCTION BottomUpNetwork,denotedasBUN,distinguishedbyaunique
weightinitializationstrategythatissparseanddecentralized.Unlike
Multi-AgentReinforcementLearning(MARL)isanexcitingfield
conventional dense-weight initialization methods, BUN adopts a
withinartificialintelligenceandmachinelearning.Ithasbecome
sparseinitializationapproach,ensuringthatagentsmakeindepen-
increasinglycrucialfortacklingreal-worldproblemswheremulti-
dentlocaldecisionswhileminimizingcomputationalcostsdueto
pleautonomousagentsinteractincomplexenvironments.However,
itssparsity.Thisinitializationfosterslocaldecision-makingamong
MARLcanbecomputationallyexpensiveduetoagentsneedingto
agents,andweightsemergeusinggradientinformationduringtrain-
interactwitheachother.Ontheotherhand,Independentlearning
ing.Thisweightemergencenotonlyfosterscoordinationbutalso
algorithmshavepracticaladvantages.Theyrequirefewercompu-
revealsthetopologyoftheagentsintheenvironment.Throughex-
tationalresourcesandcanscaletolargerenvironments.However,
tensiveempiricalevaluationsincooperativemulti-agentscenarios,
theyfacechallengesinmulti-agentsettingsduetonon-stationarity,
ourapproachshowcasestheadvantagesofsparseanddecentralized
whichcanaffecttraditionalreinforcementlearningguarantees.
initialization,significantlycontributingtomulti-agentreinforcement
Arecentstudyby[11]demonstratedthatindependentlearning
learningbyaddressingnon-stationaritychallengesandpromoting
algorithmscanperformexceptionallywellincooperativescenarios
efficientdecentralizeddecision-makingandcommunicationamong
whenindividualagentshavefullobservability.However,fullob-
agents.
servabilityisoftencomputationallyexpensive.Inmanyreal-world
4202
tcO
3
]AM.sc[
1v61520.0142:viXraInourevaluationofBUN,wetestitintwoapplications:Coop- rewardsacrossvarioustasks.Otherapproaches,suchas[1,4,10],
erativeNavigationandTrafficSignalControlenvironments.Our utilizeadot-productattentionmechanismforinter-agentcommu-
experimentsconsistentlyshowthatBUNoutperformsbaselinemeth- nication, restricting communication to an agentâ€™s neighbours to
ods.Notably,itachievesperformancelevelssimilartomethodswith mitigatecomputationaloverhead.However,learningcommunication
fullcommunicationwhileusingsignificantlyfewercomputational protocolsacrossagentsusingsuchmethodscanbecomputationally
resources.ItisimportanttonotethatBUNcomplementsexisting intensive.Inourwork,weaimtoaddressthischallengebyfocusing
cooperativemulti-agentalgorithms,makingthemmorepracticaland onlearningsharedparametersthatareassparseaspossible.
cost-effectiveforreal-worldapplications. Recently, a notable interest has been in training sparse neural
networkswithinDeepReinforcementLearning(DRL).In[14],they
proposedthePolicyPruningandShrinking(PoPs)method.This
2 RELATEDWORK
approachincorporatesiterativepolicypruningasanintermediary
Multi-agentReinforcementLearning(MARL)hasgarneredsignif- steptoguidethedimensionsofdenseneuralnetworksspecifically
icant attention, with Independent Q-learning (IQL) being one of tailoredforDRLagents.TurningtoexploringtheRiggedLottery
thepioneeringapproaches.IQLtrainsdistinctQ-valuefunctions Ticket(RigL)HypothesisinDRL,theworkin[6]investigatedthis
for each agent, assuming that other agents remain static compo- phenomenon.Incontrast,theworkin[25]madesubstantialstrides
nentswithintheenvironment.However,thismethodfacesanotable bydemonstratingthediscoveryofsparse"winningtickets"through
challengewhentheenvironmentbecomesnon-stationary,leading behaviourcloning(BC),offeringinvaluableinsightsintotheiniti-
toinstability issues, mainlywhenapplied tolarge-scalesystems. ationofsparsenetworks.Intheresearch[6],aspotlightwascast
Recentresearch[11]hasilluminatedscenariosinwhichIQLcan onthechallengesassociatedwithtrainingDRLagentsusingsparse
demonstrate effectiveness within multi-agent systems by relying neuralnetworksfromtheinherenttraininginstabilitywithinsuch
solelyonlocalobservations,eliminatingthenecessityforcoordi- networks.Buildinguponthisfoundation,theworkin[22]delved
nation.Similarly,IA2CandIPPOhaveshownpromiseinspecific deeperintotheissueoftraininginstabilityinsparseDRLagents.
environmentswhereagentcoordinationmaynotbenecessaryde- Theirworknotonlyshedlightonthechallengesbutalsounderscored
spitethenon-stationarityconcern.Itisworthnotingthatincases theinherentlimitationsofachievingstabletraininginthesenetworks.
wherelearnedbehaviourscannotreplacecoordination,directcom- Theirworkresponseintroduceddynamicsparsetraining,theSET
municationbetweenagentsremainsindispensable. algorithm,whichenablesend-to-endtrainingofsparsenetworks
Recentworkshavefocusedontrainingactor-criticalgorithmsto within actor-critic algorithms, ultimately achieving a reasonable
addressthechallengesposedbynon-stationarityandcoordination. sparsitylevel.Furtheradvancesinthefieldweremadein[2,12].
Intheseapproaches,thecriticiscentralizedandutilizesglobalin- Theseresearchworksfocusedontrainingsparseneuralnetworks
formationduringtraining,whileactorsemploylocalinformation fromthegroundup,eliminatingtherelianceonpre-traineddense
duringexecution.MADDPG[16]learnsacentralizedcriticforeach models. In this work [12], they proposed an approach involving
agentbyprovidingallagentsâ€™jointstateandactionstothecritic. block-circulantmasksintheearlystagesoftraining,significantly
PoliciesforeachagentaretrainedusingtheDDPGalgorithm[13]. enhancingpruningefficiencyinTD3agents.Whilein[2],theyintro-
COMA[8]alsoemploysacentralizedcriticbutestimatesacoun- ducedauniqueparadigmbyapplyingone-shotpruningalgorithmsin
terfactualadvantagefunctiontoassistwithmulti-agentcreditas- offlinereinforcementlearning(RL)settings.Conductinganexhaus-
signment,isolatingtheimpactofeachagentâ€™sactions.VDN[24] tiveinvestigationintothevariousmethodsemployedinthisdomain,
decomposesacentralizedstate-actionvaluefunctionintoasumof thework[9]conductedacomprehensivecomparativestudy.Their
individualagent-specificfunctions.However,thisdecompositionim- workemphasizedtheeffectivenessofpruningandhighlightedthe
posesastrictprior,whichneedstobethoroughlyjustifiedandlimits substantialimprovementsattainedcomparedtoconventionalstatic
thecomplexityoftheagentsâ€™learnedvaluefunctions.Q-Mix[20] sparsetrainingtechniques.However,itisworthnotingthatmostof
buildsuponthisbyremovingtheneedforadditivedecomposition theseworksprimarilyrevolvearoundDRLalgorithmstailoredto
ofthecentralcritic,insteadimposingalessrestrictivemonotonicity single-agentenvironments.Inourpaper,weventurebeyondthese
requirementonagentsâ€™state-actionvaluefunctions.Thisapproach boundariesbyapplyingthesemethodologiestotacklethechallenges
allowsforalearnablemixingofindividualfunctionswithoutrestrict- ofsparsityinmulti-agentenvironments.Additionally,weaddressthe
ingthecomplexityofthefunctionsthatcanbelearned.Nevertheless, dynamicdeterminationofnetworktopologyduringsparsetraining,
thesemethodsdonotleveragethestructuralinformationinherent addinganoveldimensiontothisevolvingfieldofresearch.
intheenvironment.Intheseapproaches,eachagentâ€™sobservation
typicallyconsistsofaconcatenationofthestatesofotheragents 3 BACKGROUND
andvariousenvironmentalfeatures.Additionally,usingacentralized
3.1 ReinforcementLearning
criticpreventsthelearnedpolicyfromgeneralizingtoscenarioswith
feweragentsthanencounteredduringtraining.Moreover,granting 3.1.1 Single Agent. RLisasubfieldofmachinelearningthat
accesstotheglobalstateforadensecentralizedcriticincurssignifi- focusesontrainingagentstomakeoptimaldecisionsinsequential
cantcomputationalcosts.Totacklethesechallenges,severalprior andcontinuousenvironments.InRL,anagentgenerallyinteracts
studieshaveexploredthelearningofcommunicationprotocolsthat withanenvironmentoveraseriesofdiscretetimesteps.Theagent
exploittheunderlyingenvironmentalstructure[7,19,23].These perceivesthecurrentstateoftheenvironmentandtakesactionsto
studiestrainmultipleagentstoacquireacommunicationprotocol influencetheenvironment.Theenvironment,inturn,respondsto
andhavedemonstratedthatcommunicatingagentsachieveenhanced theagentâ€™sactionsbytransitioningtoanewstateandprovidingarewardsignal.Thestatespacerepresentsallpossiblesituationsor 3.1.3 DQN. DeepQ-Networks(DQN)[17]areaclassofrein-
configurationsoftheenvironment.Theactionspacerepresentsall forcementlearningalgorithmsthatcombineQ-learningwithdeep
possibleactionsthattheagentcantake.Theagentâ€™staskistolearn neuralnetworks,whicharehighlyeffectiveforfunctionapproxima-
apolicy,whichisamappingfromstatestoactions,thatmaximizes tion.Atitscore,DQNaimstoapproximatetheoptimalaction-value
theexpectedcumulativerewardovertime.Thiscanbeformalized
function,denotedasğ‘„ğœ‹(s,a).Theweightparametersğœƒ
aretrained
ğœƒ
as a Markov Decision Process(MDP). MDP is defined as tuple, usingatemporaldifferencelossfromtransitionssampledfromex-
âŸ¨S,A,ğ‘ƒ,ğ‘Ÿ,ğ›¾âŸ©, where S is the state space, A is the action space, periencereplaybufferD:
ğ‘ƒ :SÃ—Aâ†’Î”(ğ‘†)definesthetransitiondynamics,ğ‘Ÿ :SÃ—Aâ†’R (cid:20) (cid:18) (cid:19)(cid:21)
istherewardfunction,andğ›¾ âˆˆ [0,1)isadiscountfactor. L(ğœƒ)=E (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€²)âˆ¼D ğ‘„ ğœƒ(ğ‘ ,ğ‘)âˆ’ ğ‘Ÿ+ğ›¾ maxğ‘„ ğœƒÂ¯(cid:0)ğ‘ â€²,ğ‘â€²(cid:1) (1)
ğ‘â€²âˆˆA
The policy ğœ‹ : S â†’ Î”(A) is the strategy that the agent em-
ploys to select actions ğ‘ âˆˆ Abased on the current state ğ‘  âˆˆ S. whereğœƒÂ¯aretheweightparametersofthetargetnetwork.Thetarget
It defines the agentâ€™s behaviour and can be deterministic or sto- networkisacopyoftheonlinenetworkandisusedtoestimatethe
chastic. The environment rewards the agent ğ‘Ÿ signal after each Q-valuesinthetarget.Ithelpsinstabilizingthetrainingprocessby
action.Theagentâ€™sobjectiveistolearnapolicythatleadstoac- providingaconsistenttargetforQ-valueapproximation.
tionsthatmaximizethetotalcumulativerewardovertime.Some-
4 BUN:BOTTOMUPNETWORK
times,costscanbeusedinsteadofrewards,andthegoalbecomes
minimizingthecumulativecost.Apolicyğœ‹ formalizesanagentâ€™s Inpracticalcooperativescenarios,agentsareoftendistributed,each
behaviour and the associated value function ğ‘‰ğœ‹ : S â†’ R de- equippedwithitslocalobservations,actions,andlocalobjective
fined as: ğ‘‰ğœ‹(ğ‘ ) := E ğ‘âˆ¼ğœ‹(ğ‘¥) (cid:2) R(ğ‘¥,ğ‘)+ğ›¾E ğ‘¥â€²âˆ¼P(ğ‘¥,ğ‘)ğ‘‰ğœ‹ (ğ‘¥â€²)(cid:3) and rewards.Althoughseparatedwithintheirindividualnetworks,these
state-action value functions ğ‘„ğœ‹ : S Ã— A â†’ R as: ğ‘„ğœ‹(ğ‘ ,ğ‘) := agentscancommunicateoverasharedmediumtoworkcollabo-
R(ğ‘ ,ğ‘)+ğ›¾E ğ‘¥â€²âˆ¼P(ğ‘ ,ğ‘)ğ‘‰ğœ‹ (ğ‘¥â€²). rativelytowardsachievingglobalobjectives.Inthispaper,wein-
troduceamulti-agentreinforcementlearningproblemformulation,
treatingitasasingle-agentproblemwithmulti-discreteactionsas
3.1.2 Multi-AgentReinforcementLearning. Multi-agentrein-
illustratedinFigure1.Ourmethodologyleveragesastraightforward
forcement Learning (MARL) is an extension of single-agent re-
yeteffectiveDQN(DeepQ-Network)algorithm.However,wecan
inforcementlearningthatdealswithscenarioswheremultipleau-
useanystandardreinforcementlearningalgorithmwiththeprovided
tonomousagentsinteractwithinasharedenvironment.Eachagent
network initialization. BUN starts with a sparse network, and at
seekstolearnapolicythatmaximizesitsexpectedcumulativere-
regularlyspacedintervals,newconnectionsemergeusinggradient
wardovertimewhileconsideringtheactionsandstrategiesofother
information.Afterupdatingtheconnectivity,trainingcontinueswith
agents.Thisfieldiscriticalwhenaddressingproblemsinvolving
theupdatednetworkuntilthenextupdate.Themainpartsofoural-
coordination,competition,andcollaborationamongmultipleagents.
gorithm,NetworkInitialization,MainObjective,WeightEmergence,
WeformalizeMARLusingDEC-POMDP[18],ageneralizationof
UpdateSchedule,andthevariousoptionsconsideredforeach,are
MDPtoallowdistributedcontrolbymultipleagentswhomaybe
explainedbelow.
incapableofobservingtheglobalstate.ADEC-POMDPisdescribed
(1)NeuralNetworkInitialization(ğœƒ0)AsshowninFig1,We
byatupleâŸ¨S,A,R,ğ‘ƒ,ğ‘ ,O,ğ›¾âŸ©.ThejointstatespaceSencapsulates
haveasinglenetwork.Theinputğ‘ forthenetworkisthelistofall
thecollectiveconfigurationofallagentsandtheenvironment.Simi-
larly,ajointactionspaceAincludesallpossiblecombinationsof
agentâ€™sobservations,ğ‘  = [ğ‘œ 1,ğ‘œ 2,Â·Â·Â·ğ‘œ N].Thegivensetofstatesğ‘ 
outputisthelistofindividualactions,ğ‘= [ğ‘ 1,ğ‘ 2,Â·Â·Â·ğ‘ N].Thecore
actionsthateachagentcantake.Theinteractionbetweenagents
logicbehindBUNisbasedonthefollowingprinciples:1.BlockDi-
andtheenvironmentunfoldsoverdiscretetimesteps.Eachagent
agonalWeightInitialization(ğœƒ ğ‘–):InBUN,weinitializetheneural
ğ‘– âˆˆ N chooses an actionğ‘ ğ‘– âˆˆ A, forming a joint action vector
ğ’‚= [ğ‘ ğ‘–] âˆˆAğ‘› andhaspartialobservationsğ‘œ ğ‘– âˆˆğ‘ .Theagentsâ€™joint networkâ€™sweightsfollowingablockdiagonalpattern.Thisapproach
allocatesspecificnetworkcomponentstohandleparticularagent
observationsğ‘  = (o1,...,oğ‘) provideinsightsintothecollective
interactionsortasks.Byincorporatingagent-specificinformation
stateS,butfullknowledgeoftheenvironmentisoftenobscured.
directlyintothearchitecture,eachagentbenefitsfromdedicated
Similarly,theagentâ€™srewardisgivenbytheğ‘Ÿ(ğ‘œ ğ‘–,ğ‘ ğ‘–) âˆˆR.Eachagent
ğ‘– cit ea ske ğœ‹s ğ‘–a nc ot wion mğ‘ ağ‘– pb oa bs se ed rvo an tii ots no sw ton ap co til oic ny s.ğœ‹ Tğ‘– h(ğ‘ eğ‘– jo| iğ‘œ nğ‘– t) v. aT luh ee fa ug ne cn tt is oâ€™ np ğ‘‰ol ğœ‹i- net 2w .o Zrk erc oom Inp io tin ae lin zt as, tif oo nste fori rng Os fp f-e Dci ia al giz oa nti ao ln Wan ed igt ha tr sge (t ğœƒe ğ‘–d ğ‘—l =ea 0rn ):in Ig n.
contrasttotheblockdiagonalweights,BUNinitializesoff-diagonal
capturestheexpectedcumulativerewardachievableunderthejoint
weightstozero.Thisdesignchoicepromotesisolationandinde-
policyğœ‹.Itisdefinedsimilarlytobefore,consideringobservationsin-
steadofstates:ğ‘‰ğœ‹(s) = E aâˆ¼ğœ‹(a|s) (cid:104) (cid:205) ğ‘¡âˆ =0ğ›¾ğ‘¡(cid:205) ğ‘–ğ‘ =1ğ‘Ÿ ğ‘–(sğ‘¡,ağ‘¡)(cid:12) (cid:12) (cid:12)s0=o(cid:105) . p Be ynd seen ttc ine gbe thtw ese een wa eg igen ht ts sw tohe zn ert oh ,ey wd eo en no cot uh ra av ge ed air ge ec nt ti snt te ora ac ct tio an us -.
Thejointstate-actionvaluefunctionğ‘„ğœ‹
alsoadaptstopartialob- tonomouslywhentheiractionsdonotsignificantlyimpactordepend
servability:ğ‘„ğœ‹(s,a)=(cid:205) ğ‘¡âˆ =0ğ›¾ğ‘¡E sâ€²,aâ€²âˆ¼ğ‘ƒ(sâ€²,aâ€²|s,a) (cid:2)(cid:205) ğ‘–ğ‘ =1ğ‘Ÿ ğ‘–(sğ‘¡,ağ‘¡)(cid:3) .In oneachother,facilitatingefficientlocalizeddecision-making.
summary,multi-agentreinforcementlearningwithpartialobserva- (2)ObjectiveInthecaseofcollaborativemulti-agentreinforce-
tionsintroducesthecomplexityoflimitedinformation,requiring mentlearning,themulti-agentshaveasharedrewardsothatthere
agentstoadapttheirpoliciestomakeeffectivedecisionsbasedon isaglobalobjectivefunctionthatmustbeoptimizedbasedonthe
theirlocalobservations.Adaptingvaluefunctionsandpoliciestothe collaborativeeffortsoftheagentsasgivenin(1).However,inour
observationspaceenablesagentstohandlepartialobservabilityand work,weneedtooptimizetheobjectivefunctionwhileadheringto
learnoptimalstrategiesinchallengingenvironments. apredefinedbudgetofnetworkweights.Theoptimizationproblem(3)UpdateScheduleTheconnectionsemergebasedonapre-
definedscheduledeterminedbythefollowingparameters.ğ‘ :The
number of additional connections need to emerge. Î”ğ‘‡ : The fre-
quencyatwhichtheconnectionsneedtoemerge,ğ‘˜ :Thenumber
ofconnectionsthatneedtoemergeateachupdate,ğ‘‡ ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ Theit-
erationatwhichconnectionemergenceshouldstart.Weallowthe
uniformemergenceofğ‘˜ connectionsateachupdate,sampledata
Î”ğ‘‡ frequencyuntilwereachthebudgetğ‘.Thisgradualemergence
fostersstabilityandreliabilitybypreventingabruptandpotentially
unstablechangesinagentinteractions.Inaddition,itmakesiteasier
tointerpretthelearningprocessandcangaininsightsintohowthe
agentsadapttonewconnectionsandmakesenseofthelearning
dynamics.ThepseudoalgorithmforBUNisgiveninAlgorithm1.
DimensionalityWhenusingasinglenetwork,bothobservation
andactionspacecangrowexponentially.Forexample,ifweusea
Figure1:TheBUNapproachinvolvesatwo-step.1.WeightIni- DQNasthebasenetwork,thenumberofoutputnodeswilltypically
tialization:Weightsareinitializedsothatğ‘–ğ‘¡â„ agentâ€™sobservation growexponentially,dependingonthenumberofagents.Onewayto
ğ‘œ ğ‘– isdirectlymappedtoitsactionğ‘ ğ‘– withoutanydependenceon dealwiththeexponentialgrowthinthejointactionspaceistouse
theotheragentâ€™sobservation.2.WeightEmergence:Wethen DDPGwithGumbel-Softmaxactionselectioniftheenvironment
growtheweightsacrosstheagentsaccordingtothehighestmag- is discrete to avoid the exploding number of input nodes of the
nitude gradient signal. The Green dotted line represents the observationspace,aswellasanexplodingnumberofoutputnodes
newlyemergedweights/connections. oftheactionspace.Underthisparadigm,theinputandoutputnodes
onlygrowlinearlywiththenumberofagents,astheoutputnodesof
aneuralnetworkinDDPGarethechosenjointaction,asopposedto
aDQN,wheretheoutputnodesmustenumerateallpossiblejoint
canbemathematicallyformulatedasfollows:
actions.
minimize L(ğœƒ)
(2)
subjectto âˆ¥ğœƒâˆ¥â„“ =ğ‘+âˆ¥ğœƒ0âˆ¥â„“
0 0
Algorithm1BUN
w
t oi
foh tne har ele bnğ‘
uet
di ws
go
eth trke copb nau srd
ta
rg
m
ae
ie
nt t, ter âˆ¥re
s
ğœƒp
t
âˆ¥r
h
â„“e 0as
t
=e cn
a
ğ‘t ns +bt âˆ¥h
e
ğœƒe
a
0dl âˆ¥i
d
â„“m 0ei
id
nt
t
to
o
on
ğœƒ
tht 0h e.e
T
obn
h
ju
e
em
cin
tb
ic
ve
o
er
rp
fo uof
nra
ca
t
td
i io
od
n
ni- 1 2:
:
I
b
Nn
u
ei dt ua
g
rl
e
ai tz l:e
Nğ‘
e,N
tS
we ct ow
h
reo kdr
u
Ik
nle
i: ti:t ar
ğ‘˜
la
i,
zi ğ‘‡n ağ‘ i tğ‘¡n
iğ‘
og
ğ‘Ÿ
nğ‘¡n
,
ğœƒe Î”t ğ‘‡wo ,r ğ‘‡k ğ‘’ğ‘›ğ‘„ ğ‘‘ğœƒ, target networkğ‘„ ğœƒÂ¯,
wen itc ho iu nr ta hg ees spt eh ce ifiem ede brg ue dn gc ee t.o Tf hw ee pi ag rh at ms ein tet rh ğ‘e an le lotw wo sr ck ow nth roil le os vta ey ri tn hg
e
3 4:
:
forğ‘¡ Sain mğ‘‡ pğ‘¡ lğ‘œ eğ‘¡ğ‘ ağ‘™ bd ao
tch<ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘ â€² >fromD
wde hg ir leee ğ‘o =ft 0he cos rp ra er ss pi oty n: da sl ta org the erğ‘ int ie tin ad ls spto arm seak ne ett whe orn ket iw nio tir ak lid ze an tis oe nr,
.
5 6:
:
ifğ‘¡% foÎ” rğ‘‡ ea= c= hl0 ayan erd ğ‘™ğ‘‡ dğ‘ ğ‘¡ oğ‘ğ‘Ÿğ‘¡ <ğ‘¡ <ğ‘‡ ğ‘’ğ‘›ğ‘‘ then
Overall,thisoptimizationproblemaimstostrikeabalancebetween 7: ğ‘–,ğ‘— =argmax(|âˆ‡ ğœƒğ‘™ L(ğœƒ)|,ğ‘˜)
minimizing the loss function and controlling the sparsity of the ğ‘–ğ‘—
controllerğ¹ withinthedefinedbudgetconstraints. 8: WeightEmergencethroughupdate;ğœƒ ğ‘–ğ‘™ ğ‘—
(3) Weight Emergence Solving the optimization problem (2)
9: else
directlyishinderedbythepresenceofthesparsityconstraint.This
10:
ğœƒ =ğœƒâˆ’ğ›¼âˆ‡ğœƒğ‘–ğ‘—
constraintintroducesnon-convexityandcombinatorialcomplexity,
primarilybecausetheâ„“ 0normisusedtoquantifysparsity.Toover- 11:
Updatethetargetnetwork;ğœƒÂ¯=ğ›½ğœƒ+(1âˆ’ğ›½)ğœƒÂ¯
comethesechallenges,heuristicmethodsofferaneffectiveapproach,
and in this context, a Greedy Coordinate Descent algorithm [5]
provesparticularlyadvantageous.Thekeyideaistoiterativelyselect
networkparametersthatminimizetheoptimizationobjectivewhile
5 EXPERIMENTS
adheringtothesparsityconstraint.Thesameisexploredin[6,26]to
growtheconnectionsacrosstheneurons.Wefollowthesamegreedy ThissectionthoroughlyevaluatestheBottomUpNetwork(BUN)
principletoallowtheemergenceoftheweights.Duringtraining, frameworkwithintwocooperativeenvironments.Specifically,we
weightsemergeacrosstheagentsbasedonthehighestmagnitude considertheCooperativeNavigationtask,introducedby[16],and
gradientrule,representedas(ğ‘–,ğ‘—)=argmax(|âˆ‡ğœƒğ‘–ğ‘—L(ğœƒ)|,ğ‘˜).Here, theTrafficSignalControl(TSC)taskasdescribedin[3].Toshow-
ğœƒ ğ‘–ğ‘— referstotheweightsacrosstheagentblocks.Giventheinitial casetheversatilityofourapproach,weextendourevaluationto
zeroinitializationofweights,thenewlyemergedweightsinitially encompassvariousadaptationsofCooperativeNavigationandTSC.
donotinfluencethenetworkâ€™soutput.Selectingweightswiththe Detaileddescriptionsoftheseexperimentalenvironmentscanbe
highestgradientsensuresasubstantialreductioninthelossfunction foundinthesubsequentsubsections.Itisimportanttonotethat,for
duringtraining. thisstudy,weemploydiscreteactionsacrossallenvironments.5.1 BenchmarkAlgorithms TrafficSignalControlIntheTrafficSignalControlenvironment,
OurexperimentsaredesignedtocomparetheperformanceofBUN ourobjectiveistoassesstheeffectivenessofBUNinhandlingcom-
againstseveralbenchmarkmethods.Firstly,weevaluateBUNagainst plexanddynamicallychangingtrafficscenarios.Inthissimulated
independentQ-learning(WhichwerefertoastheDecentralized roadnetwork,eachagentisatrafficsignalcontrollerataninter-
method),which servesas thebaselineapproach. In this baseline section.Anagentâ€™sobservationscompriseaone-hotrepresentation
method,eachagentoperatesindependentlywithoutcommunication. ofitscurrenttrafficsignalphase(indicatingdirectionsforredand
Toassesstheefficacyofthesparsenetworkarchitecture,wefur- greenlights)andthenumberofvehiclesoneachincominglaneat
thercompareBUNagainstRigL[6].Additionally,weinvestigate the intersection. At each time step, agents select a phase from a
theimpactofperformanceandtheutilizationofFloatingPointOp- predefinedsetfortheupcominginterval,typicallysetat10seconds.
erationsperSecond(FLOPS)comparedtodensenetworks.This Theoverarchingglobalobjectiveistominimizetheaveragewait-
involvescomparingBUNagainsttwokeyconfigurations:(i)Cen- ingtimeforallvehicleswithintheroadnetwork.Toconductthese
tralized learning (i.e., Dense Model), which represents an ideal experiments,weusetheframework[3]builtonSUMOtrafficsimu-
scenariowhereeachagenthasaccesstotheentireglobalstate,and lator[15].Specifically,weexperimentedwithtwodifferentnetwork
(ii) DGN [10], which utilizes an attention mechanism to enable configurations:a2x2gridnetworkfeaturingfourintersectionsand
communicationbetweenagents. asmallersectionoftheIngolstadtRegionnetworkcomprising7
intersections.Ourtrafficflowsimulationsencompassedavariety
ofscenarios,includingbothpeakandoff-peakperiods,toemulate
dynamictrafficpatternsasgivenin[3].
5.2 Environments
IntheCooperativeNavigationenvironment,wedeployascenario
featuringNagentsandNlandmarks,wheretheoverarchinggoalis
5.3 ImplementationDetails
forallagentstocoverallthedesignatedlandmarkswhileavoiding
collisionsefficiently.Tocomprehensivelyevaluatethecapabilitiesof Inourtrainingsetupacrossallenvironments,ourneuralnetwork
BUN,weemployvariousvariationsofthisenvironment.Whilewe architecturebeginswiththree(ReLU)layers,eachsizedat18times
provideabriefoverview,amoredetaileddescriptioncanbefound the number of agents. For the process of weight emergence, we
inthesupplementarymaterial. graduallyincreasethenetworkweights.Thisgrowthstartsatstep
Simple Spread (SS) In the Simple Spread task, N agents are 10,000(ğ‘‡ ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡)andcontinuesuntilstep30,000(ğ‘‡ ğ‘’ğ‘›ğ‘‘),withweight
taskedwithreachingNlandmarks.Eachagentâ€™sobservationsen- updatesoccurringatintervalsof1,000steps(Î”ğ‘‡),andeachupdate
compasstheirpositionandtherelativepositionsofthelandmark increasesthenumberofweightsbyafactorof3(ğ‘˜ =3).Weintro-
assignedtothem.Theprimaryobjectiveisforeachagenttostrate- duceadeliberatedelaytoensurethatagentscanlearnfromtheir
gicallypositionthemselvestoreachtheirlandmarks.Notably,this observationsbeforetheweightgrowthbegins.Weightgrowthcom-
environmentservesasabaselinescenariowheretheperformance mencesonlyafter10,000steps,andwehavechosenasuitable(Î”ğ‘‡)
of the Decentralized approach will be comparable to that of the toprovideampleseparationbetweenweightupdates,preventingthe
centralizedlearningalgorithm.Thisisbecauseindividualagents receiptofpotentiallymisleadinggradientsignals.Wealsoempha-
donotnecessitatecommunicationwiththeirpeerstoachievetheir sizetheimportanceofthisstepinouranalysis,whichisprovided
objectives. intheSupplementaryMaterial.Tomaintainfairnessincomparing
SSwithCommunication(SS+C)Inthistask,weretainthesame differentapproaches,weensureanequalnumberofneuronsareem-
objectiveasSimpleSpread,with2agentsand2landmarks.How- ployedacrossallmethodsandadjusthyperparametersaccordingly.
ever,eachagentâ€™sobservationsnowincludetheirpositionandthe Detailedhyperparametersettingsforeachapproachcanbefoundin
relativepositionsoftheotheragentâ€™sdesignatedlandmark.Com- ourSupplementaryMaterial.
municationbetweenagentsbecomesessentialtosuccessfullyreach IncooperativeNavigation,weused3metricstocomparedifferent
theirrespectivelandmarksastheyneedtoshareinformationabout methods:MeanEpisodeReward(R),Theaveragerewardachieved
theirlandmarks.Thisconfigurationhighlightsthesignificanceof bytheteaminanepisode.SuccessRate(S%):Inwhatpercentage
agent-agentcommunicationinachievingthemission. ofepisodesdoestheteamachieveitsobjective?(Higherisbetter)
SS with Cross Communication (SS+CC)In this task, we in- Time(T):Howmanytimestepsdoestheteamrequiretoachieve
troduce3agentsand3landmarks.Eachagentâ€™sobservationsstill itsobjective?(Lowerisbetter).EachepisodeintheCooperative
includetheirpositionandtherelativepositionsoftheirassigned Navigationenvironmentlastsforatotalof25-timesteps.Evaluation
landmark.However,thisenvironmenthasauniquetwist:Agent1 iscarriedoutforeachepisodeinthesameseededenvironmentand
receiveshigherrewardswhenitoccupiesthelandmarkdesignated illustratedinFigure2.Wethentestthetrainedmodelonanewlyset
forAgent3,andAgent2receiveshigherrewardswhenitoccupies seededenvironmentfor25-timestepsandshowtheresultsinTable1.
Agent1landmark.Tooptimizetheirreturns,eachagentmustcom- Inthetrafficenvironment,weused2metrics:AverageWaitingtime
municatestrategicallywiththenecessaryagentstonavigatetoward (Lower is better) and Average Trip time (Lower is better). Each
thelandmarksthatyieldhigherrewards.Thissetupintroducesan episodeisrunfor3600steps.Wethentestthetrainedmodeland
asymmetriccommunicationrequirementamongtheagents,differen- provideperformanceresultsinTable4c.Inbothenvironments,we
tiatingitfromthepreviousscenarios(2).Thistaskwouldshowthe usethemetricfloating-pointoperations(FLOPs)toshowthatour
efficacyofourapproachinestablishingthenecessarycommunica- sparsemethodutilizesfewerarithmeticoperations(Lowerisbetter).
tion.Figure2:LearningcurveduringthetrainingofCooperativeNavigationenvironments.AgentsonSSandSS+CCaretrainedfor
20000time-stepswhileonSS+Caretrainedfor500000time-steps.Theplotsshowthemeanepisoderewardover10randomseeds.
(a)t=0 (b)BUN(t=6) (c)BUN(t=10) (d)RigL(t=10) (e)RigL(t=25)
Figure3:ComparisonbetweenBUN(left)andRigL(right)ontheSimpleSpreadwithCommunication(SS+C)andSimpleSpread
withCrossCommunication(SS+C)environmentsatt=0,6,and10andt=0,10,and25.SmallcirclesindicatelandmarksandBig
circlesindicateAgents.InSS+C,thewhiteagentisassignedawhitelandmark,whiletheblackagentisassignedablacklandmark.In
SS+CC,thewhiteagentispenalizedtwiceasablackagenttoreachtheblacklandmark,whileredandblackagentsareassignedto
theredlandmark.Theblackagentisaggressivetoreachtheredlandmarkasitispenalizedtwiceastheredagenttoreachthered
landmark.Inbothenvironments,theagentstrainedusingBUNtriedtograsptheinformationoftheirtargetlandmarksfromtheir
fellowagentsandreachthetargetlandmarks.Ontheotherhand,theagentstrainedusingRigLstruggletoestablishtheconnection
betweenagents.InSS+C,theblackagentreachestheblacklandmark,establishingthatitonlylearnedthelocalbehaviourbutdidnot
establishtheconnectionbetweenthewhiteagent.Seethevideoforcompletetrajectoriesprovidedinthesupplementarymaterial.
5.4 Results intheutilizationofcomputationalresources,preciselythenumber
(SS)Inourexperiments,wetrainedthemodelsthroughout200,000- of Floating Point Operations (FLOPs), among these approaches.
timesteps.ThelearningcurveforthistrainingperiodintheSimple As detailed in Table 1, DGN utilizes more FLOPs, followed by
SpreadenvironmentispresentedinFigure2(a).Inthecentralized theCentralizedapproach,asbothmethodsemploydensemodels.
approach,whereeachagenthasaccesstotheobservationsofall The increased FLOPs in DGN can be attributed to an attention
otheragents,optimalperformanceisachieved.However,giventhe mechanismintheapproach.Incontrast,DecentralizedandBUN
problemâ€™s simplicity, where each agent can access its landmark employ fewer FLOPs, while RigL utilizes slightly more FLOPs
information,demandinginformationfromfellowagentsbecomes thanBUN.However,despitethesevariationsinFLOPutilization,
unnecessary.Consequently,theDecentralizedapproachconverges allmodelsperformsimilarlyduringtesting,achievingcomparable
tothesamerewardlevelastheCentralizedapproach.Similarly,the
performanceinbothsuccessrate(ğ‘†%)andTime(T),asdemonstrated
BUN, RigL, and DGN approaches converge to the same reward inTable1.
levelastheCentralizedapproach.Nevertheless,therearedifferencesTable1:InCooperativeNavigationenvironments,weassesstheperformanceofvariousapproachesintermsofSuccessRate(ğ‘†%)and
TimeSteps(T)foralltrainedagentsastheyaimtoreachtheirdesignatedtargetlandmarkduringtesting.Additionally,weanalyzethe
trainingcost,measuredinFLOPs(FloatingPointOperations),incurredbythesedifferentapproachesduringtheirtrainingphase.Itis
tobenotedthattheBUNapproachhasavaryingnumberofFLOPsaswegrowtheweightsduringthetrainingphase.Thistable
providestheaveragenumberofFLOPsutilizedduringtrainingprogressfortheforwardpass.
COOPERATIVENAVIGATION-SIMPLESPREAD(SS)
SS SS+C SS+CC
MODEL FLOPS S% T FLOPS S% T FLOPS S% T
CENTRALIZED 4.2ğ‘’3 100 10.25 4.2ğ‘’3 100 10.36 9.5ğ‘’3 100 16.12
DECENTRALIZED 2.7ğ‘’3 100 9.75 2.7ğ‘’3 0 25 3.2ğ‘’3 0 25
BUN 2.7ğ‘’3 100 10.75 2.7ğ‘’3 100 10.78 3.2ğ‘’3 100 16.55
DGN 6.1ğ‘’3 0 25 6.1ğ‘’3 0 25 9.2ğ‘’3 0 25
RIGL 2.9ğ‘’3 100 12.12 2.9ğ‘’3 0 25 3.4ğ‘’3 30 23.35
Table2:InTrafficSignalControlenvironments,weassesstheperformanceofvariousapproachesintermsofAverageWaitingTime
(Avg.Wait)andAverageTripTime(Avg.TripTime)forallapproaches.Additionally,weanalyzethetrainingcost,measuredinFLOPs
(FloatingPointOperations),incurredbythesedifferentapproachesduringtheirtrainingphase.Tofacilitatecomparison,wenormalize
thesetrainingcostsconcerningtheFLOPsutilizedbythecentralizedapproach,referredtoastheDenseapproach.
GRID2Ã—2 INGLODASTCORRIDOR
MODEL FLOPS AVG.WAIT AVG.TRIPTIME FLOPS AVG.WAIT AVG.TRIPTIME
CENTRALIZED 2.6ğ‘’6(1ğ‘¥) 2.41 69.61 7.5ğ‘’6(1ğ‘¥) 12.53 74.00
DECENTRALIZED 6.6ğ‘’5(0.25ğ‘¥) 3.96 71.75 1ğ‘’6(0.14ğ‘¥) 15.87 77.36
BUN 6.6ğ‘’5(0.25ğ‘¥) 2.25 69.35 1ğ‘’6(0.14ğ‘¥) 12.42 73.86
RIGL 6.6ğ‘’5(0.25ğ‘¥) 2.39 69.38 1ğ‘’6(0.14ğ‘¥) 13.83 76.27
(SS+C)Inthisenvironment,wetrainedthemodelforabout (SS+CC)Inthisenvironment,wetrainedthemodelsthrough-
500ktime-stepsuntilconvergence.Figure2(b)illustratesthelearn- out 200,000 time steps, with each episode consisting of 25 time
ingforthetrainingperiod.Sinceeachagenthastheobservation steps.Liketheabovetwoexperiments,CentralizedandDecentral-
ofotheragentâ€™slandmarkposition,thereneedstobeanecessary izedachieveoptimalandsuboptimalperformance.Meanwhile,BUN
flowofinformationacrossagentstoachievetheoptimalreward.As andRigLconvergetoachievetheperformanceofacentralizedap-
expected,centralizedanddecentralizedapproachesachieveoptimal proach.However,duringtheevaluation,RigLagentsinsomeseeds
andsub-optimalperformances.BUNconvergestoCentralizedper- failed to reach the assigned landmarks. We show an example in
formance,whileRigLfailstoreachtheoptimalreward.Thesame Figure3,whereagentblackoccupiestheblacklandmarkratherthan
canbeobservedduringevaluationasdemonstratedinTable1.The theassignedredlandmarkandagentblackblocksagentwhitefrom
agentstrainedusingcentralizedandBUNhavethe100%success occupyingitsassignedlandmark.Thissituationexplainsthatthe
rateinreachingtheirrespectivelandmarks,whileRigLandDGN blackagentislocallytrainedbuthasnotdevelopedthenecessary
have 0% success rate. We illustrate the simulation in Figure 2 to communicationlinksbetweenagentblackandAgentRed(since
furtherdemonstratethis.Asshowninthefigure,BUNagentslearn AgentRedhasthelocationsofredlandmarks).However,theagents
thelocationoftheirlandmarksandreachthelandmarks,whileonly trainedusingBUNachievethetasksbyreachingtheassignedland-
oneagentlearnstheinformationaboutitslandmark,andtheother marks,andtheredagentstaysneartheredlandmarktominimizeits
agentfailstolearnthelocationofitslandmark.Wehypothesizethat distancepenalty.Similartotheaboveexperiments,BUNutilizesfew
aprimaryreasonforthefailureofRigLinthissettingismainlydue FLOPs,anditisobservedthat,asthenumberofagentsincreases,
tothelackofaconsistentweightemergentinthenetworkofagents, thesparsitylevelisincreasedandwouldbeessentialinthecaseof
asititerativelytriedtopruneandgrowtheweights.BUNagents large-scalesystems.
significantlygainfromtheweightinitialization,wheretheagentâ€™s TrafficSignalControlTovalidateourapproachagainstthemod-
weight gets trained steadily, which aids in the consistent weight elsmentionedearlieronTrafficSignalControl,wetrainedthemod-
growthfromasteadygradientsignalacrosstheagents.Surprisingly, elsfor50,000-timestepsandpresentedourresultsinTable4c.In
DGNdoesnotperformwell.Thisismainlyduetothenetworksize, bothscenarios(Grid2Ã—2andInglodats7),allapproachesachieved
aswechosetofixthesmallernumberofparametersinthenetwork. similarresults,whiletheBUNmodelexhibitedmarginalimprove-
Furthermore,asobservedfromtheexperimentsinSimpleSpread, mentsinbothmetrics,althoughtheseimprovementsdidnotreach
DGNandCentralizedapproachesutilizemoreFLOPs,whileBUN statisticalsignificance.Thecompletetrainingresultsareprovidedin
utilizesfewerFLOPstoachieveoptimalperformance. thesupplementarysection.(a)InitializationofNetwork (b)TrainedweightsusingBUN (c)TrainedweightsusingRigL
Figure4:Inthiscomparison,weexaminethetrainingapproachesofBUNandRigLwithinthecontextoftheSS+CCenvironment.
Thesefiguresshowcasetheevolutionofneuralnetworkweightsinbothmethods.IntheBUNapproach,trainingstartswithlocal
weightinitialization(a),whereagentsoperateindependently.Agentobservationsfollowaspecificsequence,withblackandwhite
agentsprecedingred.Theaimistoestablishconnectionsbetweenagents(highlightedinredboxes)withnoemergenceofweightsacross
agentredandagentwhite(greenboxes).TheweightsinBUNemergewithinafixedbudget(b=30),asdepictedin(b).Conversely,
RigLexhibitsadifferentpatternofweightemergence,asseenin(c).UnlikeBUN,RigLintroducesrandomweightconnections.These
structuralweightemergencepatternsshedlightontheresultspresentedintheaccompanyingtableandtheagenttrajectoriesin
Figure1,highlightingeachapproachâ€™sdistinctcommunicationandcoordinationstrategies.
Ourfindingsalignwiththosepublishedin[3],highlightingthat Table3:Toassesstherobustnessofnetworkstrainedusingboth
independentalgorithmsoutperformcoordinatedcontrolalgorithms the BUN and Centralized (Dense) approaches, we conducted
inrealistictrafficscenarios.Theseexperimentsindicatethatinde- testsinvolvingintroducingGaussiannoisetotheobservations.
pendentlearningmethodsaresufficientfortheseapplicationsand
WesamplednoisefromaGaussiandistributionâˆ¼N(0,ğœ),where
underscorethe effectivenessofour approachin twokeyaspects: ğœ âˆˆ [0,0.5],denotedasğœ.Notably,wepresenttheresultsatfour
1.Weinitiatetheprocesswithanindependentsetting,wherewe specificdatapoints
canachieveresultssimilartowhatindependentlearningalgorithms
typically accomplish. 2. Our approach provides the flexibility to VARIANCE SUCESSRATE(ğ‘†%)
remainintheindependentsetting.Ifnecessary,wecanfacilitate ENVIRONMENT (ğœ) CENTRALIZED BUN
theemergenceofweightsandthegrowthofconnectionsacrossthe 0 100 100
junctionstoenableessentialcommunication.DetailedresultsinTa- SS 0.1 100 100
ble4cdemonstratethatintheGrid2Ã—2scenario,theCentralized (C) 0.3 0 100
approachconsumesmoreFLOPscomparedtotheDecentralized, 0.5 0 0
BUN,andRiGLmodels,whichutilizeonly25%oftheFLOPswhile 0 100 100
achievingsimilarperformance.Similarly,intheInglodastCorridor, SS 0.1 0 100
(CC) 0.3 0 75
weachievedcomparableperformancetotheCentralizedapproach
0.5 0 0
usingonly14%oftheFLOPs.Asthenumberofjunctionsincreases
withinagivenscenario,thenumberofrequiredFLOPsdecreases
significantly.Thischaracteristicmakesourapproachparticularly
suitableforlarge-scaletrafficnetworkscenarios.Forsuchscenarios,
wecaninitiallyemploytheindependentsetting.Asneeded,wecan SS+CC,comparingthesparsenetwork(BUN)andthecentralized
allowfortheemergenceofsparseconnectionsacrossthejunctions, approach(i.e.,thedensestnetwork).AsdepictedinTable3,itis
providingthenecessarycommunicationandadaptabilitytotraffic evident that agents trained using the BUN model exhibit greater
conditions. robustnesstonoisewhencomparedtotheircentralizedcounterparts,
atrendobservedacrossbothcooperativeenvironments.IntheSS+C
environment, both models deliver strong performance under low
5.5 Robustness
noiseconditions.However,asthenoiselevelincreases,centralized
Inthissection,weaimtoevaluatetherobustnessofsparsenetworks agentscannotcopewithnoise,oftenfailingtoreachtheirdesignated
inthepresenceofnoise.Aspreviouslyexploredin[9]forasingle landmarks. In contrast, the BUN agent maintains a higher level
agent setting, we assess the impact of progressively introducing ofrobustnessandperformsrelativelywelleveninthepresenceof
noiseintotheobservationsandsubsequentlymeasuringitsinfluence elevatednoiselevels.Nevertheless,itisworthnotingthateventhe
on a trained network for a Multi-Agent Setting. Specifically, we BUNagentsdisplayadecreaseinrobustnessatsignificantlyhigh
introduceGaussiannoise,sampledfromadistributionwithmean noise levels. This experiment suggests sparse networks, such as
zeroandvarianceğœ,toeachobservationmadebytheagent.This BUN,exhibitgreaterresiliencetoobservationalnoisethandense
experimentisconductedintwocooperativeenvironments,SS+Cand networks,suchasthecentralizedapproach.6 CONCLUSION
[19] PengPeng,YingWen,YaodongYang,QuanYuan,ZhenkunTang,HaitaoLong,
andJunWang.2017. Multiagentbidirectionally-coordinatednets:Emergence
Inthisstudy,weintroducedBUN,anefficientalgorithmfortraining
ofhuman-levelcoordinationinlearningtoplaystarcraftcombatgames.arXiv
sparseneuralnetworksinmulti-agentenvironments.Notably,BUN preprintarXiv:1703.10069(2017).
outperformsexistingdenseandsparsetrainingalgorithmswithin [20] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFar-
quhar,JakobFoerster,andShimonWhiteson.2020.Monotonicvaluefunction
prescribedcomputationalbudgets.Ourapproachdemonstratesits factorisationfordeepmulti-agentreinforcementlearning.TheJournalofMachine
valueacrossthreecriticalscenarios:first,byenhancingtheperfor- LearningResearch21,1(2020),7234â€“7284.
[21] VasanthReddy,SuatGumussoy,HodaEldardiry,andAlmuatazbellahBoker.2024.
manceofmulti-agentsthroughimprovedcommunication;second,
SearchingforSparseControllerswithaBudget:ABottom-UpApproach.In2024
byoptimizingtheutilizationofavailablecomputationalresources; AmericanControlConference(ACC).IEEE,3674â€“3679.
andthird,byservingasaninitialstepininterpretingtheunderlying [22] GhadaSokar,ElenaMocanu,DecebalConstantinMocanu,MykolaPechenizkiy,
andPeterStone.2021.Dynamicsparsetrainingfordeepreinforcementlearning.
topology among agents. In essence, BUN offers a versatile solu-
arXivpreprintarXiv:2106.04217(2021).
tion that elevates accuracy while respecting resource constraints, [23] SainbayarSukhbaatar,RobFergus,etal.2016.Learningmultiagentcommunica-
fosterscoordinationamongagents,andunveilsessentialinsights tionwithbackpropagation.Advancesinneuralinformationprocessingsystems
29(2016).
intothestructureofmulti-agentsystems,thusmakingasubstantial [24] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
contributiontothefield. ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,
KarlTuyls,etal.2017.Value-decompositionnetworksforcooperativemulti-agent
learning.arXivpreprintarXiv:1706.05296(2017).
REFERENCES [25] MarcAurelVischer,RobertTjarkoLange,andHenningSprekeler.2021. On
lotteryticketsandminimaltaskrepresentationsindeepreinforcementlearning.
[1] AkshatAgarwal,SumitKumar,andKatiaSycara.2019.Learningtransferable arXivpreprintarXiv:2105.01648(2021).
cooperativebehaviorinmulti-agentteams. arXivpreprintarXiv:1906.01202 [26] JaehongYoon,EunhoYang,JeongtaeLee,andSungJuHwang.2017.Lifelong
(2019). learningwithdynamicallyexpandablenetworks.arXivpreprintarXiv:1708.01547
[2] SaminYeasarArnob,RiyasatOhib,SergeyPlis,andDoinaPrecup.2021.Single- (2017).
shotpruningforofflinereinforcementlearning.arXivpreprintarXiv:2112.15579
(2021).
[3] JamesAultandGuniSharon.2021. ReinforcementLearningBenchmarksfor
TrafficSignalControl.InProceedingsoftheThirty-fifthConferenceonNeural
InformationProcessingSystems(NeurIPS2021)DatasetsandBenchmarksTrack.
[4] AbhishekDas,ThÃ©ophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,Mike
Rabbat,andJoellePineau.2019.Tarmac:Targetedmulti-agentcommunication.
InInternationalConferenceonMachineLearning.PMLR,1538â€“1546.
[5] InderjitDhillon,PradeepRavikumar,andAmbujTewari.2011.Nearestneighbor
basedgreedycoordinatedescent. AdvancesinNeuralInformationProcessing
Systems24(2011).
[6] UtkuEvci,TrevorGale,JacobMenick,PabloSamuelCastro,andErichElsen.
2020.Riggingthelottery:Makingallticketswinners.InInternationalConference
onMachineLearning.PMLR,2943â€“2952.
[7] JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhite-
son.2016.Learningtocommunicatewithdeepmulti-agentreinforcementlearning.
Advancesinneuralinformationprocessingsystems29(2016).
[8] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShi-
monWhiteson.2018.Counterfactualmulti-agentpolicygradients.InProceedings
oftheAAAIconferenceonartificialintelligence,Vol.32.
[9] LauraGraesser,UtkuEvci,ErichElsen,andPabloSamuelCastro.2022.Thestate
ofsparsetrainingindeepreinforcementlearning.InInternationalConferenceon
MachineLearning.PMLR,7766â€“7792.
[10] JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu.2019.GraphCon-
volutionalReinforcementLearning.InInternationalConferenceonLearning
Representations.
[11] KenMingLee,SriramGanapathiSubramanian,andMarkCrowley.2022.Inves-
tigationofindependentreinforcementlearningalgorithmsinmulti-agentenviron-
ments.FrontiersinArtificialIntelligence(2022),211.
[12] NamhoonLee,ThalaiyasingamAjanthan,andPhilipHSTorr.2018. Snip:
Single-shotnetworkpruningbasedonconnectionsensitivity. arXivpreprint
arXiv:1810.02340(2018).
[13] TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,NicolasHeess,Tom
Erez,YuvalTassa,DavidSilver,andDaanWierstra.2015.Continuouscontrol
withdeepreinforcementlearning.arXivpreprintarXiv:1509.02971(2015).
[14] DorLivneandKobiCohen.2020.Pops:Policypruningandshrinkingfordeep
reinforcementlearning.IEEEJournalofSelectedTopicsinSignalProcessing14,
4(2020),789â€“801.
[15] PabloAlvarezLopez,MichaelBehrisch,LauraBieker-Walz,JakobErdmann,
Yun-PangFlÃ¶tterÃ¶d,RobertHilbrich,LeonhardLÃ¼cken,JohannesRummel,Peter
Wagner,andEvamarieWieÃŸner.2018.Microscopictrafficsimulationusingsumo.
In201821stinternationalconferenceonintelligenttransportationsystems(ITSC).
IEEE,2575â€“2582.
[16] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.
2017. Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnviron-
ments.NeuralInformationProcessingSystems(NIPS)(2017).
[17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou,DaanWierstra,andMartinRiedmiller.2013. Playingatariwith
deepreinforcementlearning.arXivpreprintarXiv:1312.5602(2013).
[18] FransAOliehoek,ChristopherAmato,etal.2016. Aconciseintroductionto
decentralizedPOMDPs.Vol.1.Springer.A SUPPLEMENTARYMATERIAL A.3 ExperimentalSettings
A.1 AnalysisonWeightEmergencewithÎ”ğ‘‡ AllbenchmarksareperformedonAppleM1,2020(maci64)with16
GBRAM.Inthecontextofcooperativenavigation,agentsoperating
Togainadeepermathematicalunderstanding,weexaminetheopti-
mizationprocess.ConsidertheobjectivefunctionvalueLğ‘˜+1after in an environment receive rewards based on their actions. These
theğ‘˜-th iteration. We updateğœƒğ‘˜ in the direction of the negative rewards serve two primary purposes: penalizing agent collisions
ğ‘–ğ‘— witharewardof-1andencouragingagentstominimizethedistance
gradientofğ½ concerningğœƒ ğ‘–ğ‘— whilekeepingallotherweightsfixed.
betweenthemselvesandtheirassignedlandmarks.Thisrewardstruc-
Letâˆ‡ğ‘–ğ‘—ğ½ denotethegradientofğ½ withrespecttoğœƒ ğ‘–ğ‘—.SinceLisa
turepromotescollisionavoidanceandefficientgoalachievement.
continuouslydifferentiablelossfunction,wecanusethefirst-order
Notably,therewarddistributionisagent-specific,meaningthatdif-
Taylorexpansionaroundğœƒğ‘˜ toapproximateLğ‘˜+1asfollows:
ferentagentsreceiverewardstailoredtotheiruniqueobjectives.In
Lğ‘˜+1â‰ˆLğ‘˜ +âˆ‡Lğ‘˜ Â·(ğœƒğ‘˜+1âˆ’ğœƒğ‘˜
) (3)
SS+CC,Agent1isincentivizedtooccupyLandmark3withareward
twicethenegativedistancebetweenitscurrentlocationandLand-
Considerthecomponentcorrespondingtoâˆ‡Lğ‘˜ Â·(ğœƒğ‘˜+1âˆ’ğœƒğ‘˜)in(3) mark3.Agent1alsoincursa-1penaltyforcollisions,ensuringit
andexpanditasfollows: activelyavoidscollisionswithotheragents.Similarly,Agent2isen-
couragedtooccupyLandmark1throughasimilarrewardstructure,
âˆ‡Lğ‘˜ Â·(ğœƒğ‘˜+1âˆ’ğœƒğ‘˜ ) â‰¤ âˆ‘ï¸ âˆ‡ğ‘–ğ‘—Lğ‘˜ (cid:16) ğœƒ ğ‘–ğ‘˜ ğ‘—+1âˆ’ğœƒ ğ‘–ğ‘˜ ğ‘—(cid:17) receivinga-1penaltyforcollisions.Thisapproachensuresthateach
(ğ‘–,ğ‘—)âˆˆSğ‘˜ agenthasacleargoalandincentivetoachieveitwhiletakingactive
+ âˆ‘ï¸ âˆ‡ğ‘–ğ‘—Lğ‘˜ (cid:16) ğœƒ ğ‘–ğ‘˜ ğ‘—+1âˆ’ğœƒ ğ‘–ğ‘˜ ğ‘—(cid:17) m coe na ts rou lre es nvto iroa nv moi ed nc t,o tl hli esi ro en ws aw rdit fh uno ct th ioer na dg ife fn et rs s. .I Hn ea ret ,r ta hf efic res wig an rda sl
(ğ‘–,ğ‘—)âˆˆSğ‘ğ‘˜ areassociatedwithsignalcontrolattrafficjunctions.Themeasureof
successistheaveragequeuelengthatthesejunctions.Thegoalisto
Now,substitutetheaboveexpressionbackin(3)
minimizequeuelengths,which,inturn,reducestheaveragewaiting
Lğ‘˜+1âˆ’Lğ‘˜ â‰¤ âˆ‘ï¸ âˆ‡ğ‘–ğ‘—Lğ‘˜ (cid:16) ğœƒ ğ‘–ğ‘˜ ğ‘—+1âˆ’ğœƒ ğ‘–ğ‘˜ ğ‘—(cid:17) timeforvehiclesattheseintersections.Thisapproachalignswith
theobjectivesoftrafficmanagementusingreinforcementlearning,
(ğ‘–,ğ‘—)âˆˆSğ‘˜
where agents (in this case, traffic signals) are trained to actively
+ âˆ‘ï¸ âˆ‡ğ‘–ğ‘—Lğ‘˜ (cid:16) ğœƒ ğ‘–ğ‘˜ ğ‘—+1âˆ’ğœƒ ğ‘–ğ‘˜ ğ‘—(cid:17) (4) optimizetrafficflowwhileavoidingsituationsthatleadtoexcessive
(ğ‘–,ğ‘—)âˆˆSğ‘ğ‘˜ vehiclequeuesandcongestion.
where S is a space of indices of the non-sparse weights, while A.4 AdditionalResults
Sğ‘ is a space of the sparse weights. In this form, the expression
Inthissection,weprovidethetrainingcurvesfortrafficsignalcon-
emphasizesthatthechangeinthelossdependsonthecontributions
fromboththenon-sparseweights{ğœƒ ğ‘–ğ‘—|(ğ‘–,ğ‘—) âˆˆ ğ‘†ğ‘˜}andthesparse trol.Inthissection,wepresentthetrainingcurvesfortrafficsignal
control.Figure5demonstratesthatinthe2Ã—2gridscenario,allthe
weights{ğœƒ ğ‘–ğ‘—|(ğ‘–,ğ‘—) âˆˆ ğ‘† ğ‘ğ‘˜},andeachcontributionisproportionalto
approachesconvergeatasimilarrate.However,thecentralizedap-
thecorrespondinggradientmultipliedbythechangeintheelement
proachconvergesfasterintheIngolstadtcorridorregion,asdepicted
value.However,iftheweightsaretrainedproperly,wecanusethe
inFigure6.Comparedtothecentralizedapproach,bothBUNand
factthatâˆ‡ğ‘–ğ‘—Lğ‘˜ =0âˆ€(ğ‘–,ğ‘—) âˆˆSğ‘˜ .Thentheexpression(3)reduces
RigLandthedecentralizedapproachexhibitslowerconvergencedue
to
totheirrelianceonlearningtrafficpatternsfromlocalobservations.
Lğ‘˜+1âˆ’Lğ‘˜ â‰¤ âˆ‘ï¸ âˆ‡ğ‘–ğ‘—Lğ‘˜ (cid:16) ğœƒ ğ‘–ğ‘˜ ğ‘—+1âˆ’ğœƒ ğ‘–ğ‘˜ ğ‘—(cid:17) (5) Notably,whenconsideringpartialobservations,BUNoutperforms
RigLandtheDecentralizedapproachregardingconvergencespeed.
(ğ‘–,ğ‘—)âˆˆSğ‘ğ‘˜
Additionally,weprovideinsightsintothevariousapproachesmem-
Inthiscontext,whilethechangeinthelossdependsonthegradients oryutilizationperformance,asshowninFigure7andSparsitylevel
ofsparseweights,thepivotalinfluenceoftuningbecomesapparent. ofnetworksinTable6.TheFigure7highlightsthatBUNandRigL
Bytrainingthenon-sparseweights,ouralgorithmprioritizessparse utilizealowerpercentageofmemorythanthecentralizedapproach.
weightsefficientlyandoptimizesthevaluesofnon-sparseweights. Thisobservationservesassupportingevidencefortheefficiencyof
For this reason, we ensure a gap in each weight growth update. usingfewerFLOPs(floating-pointoperations)duringtraining.
Thisstrategyensuresthatthechangeinthelossalignscloselywith
theoptimizationobjective,makingourapproachhighlyeffectivein
achievingsuperiorperformancewhilehavingagoodgradientsignal.
A.2 Hyperparametrs
Fortheenvironments,CooperativeNavigationandTrafficSignal
Control,weuseasetofhyperparameterstoachievetheresultsshown
inourpaper.WeprovidethehyperparametersusedforCooperative
NavigationinTable4andTrafficSignalControlinTable5.Itisto
benotedthatthehyperparametersforCentralizedandDecentralized
approachesarethesameasBUN.Table4:HyperparametersofBUN,RigLandDGN.Whereğ‘ isthenumberofagents.
HYPERPARAMETER BUN RIGL DGN
DISCOUNT(ğ›¾) 1024 1024 1024
BUFFERCAPACITY 1ğ‘’6 1ğ‘’6 1ğ‘’6
ğ›½
DECAY(ğœ–) 0.1 0.1 0.1
OPTIMIZER ADAM ADAM ADAM
LEARNINGRATE 1ğ‘’-4 1ğ‘’-4 1ğ‘’-4
LAYERTYPE MLP MLP MLP
#OFLAYER 3 3 3
#OFUNITS (18*N,18*N,18*N) (18*N,18*N,18*N) (18,18,18)
ACTIVATIONTYPE RELU RELU RELU
WEIGHTINITIALIZATION BUN RANDOMNORMAL RANDOMNORMAL
#OFNEIGHBORS - - ALL
ğ‘‡ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ 10K 5K -
ğ‘‡ ğ‘’ğ‘›ğ‘‘ 30K 0.75*TIME -
Î”ğ‘‡ 1000 100 -
Table5:HyperparametersofBUN,RigLandDGN.Whereğ‘ isthenumberofagents.
HYPERPARAMETER BUN RIGL DGN
BATCHSIZE(ğ›¾) 64 64 64
BUFFERCAPACITY 1ğ‘’6 1ğ‘’6 1ğ‘’6
ğ›½
DECAY(ğœ–) 0.1 0.1 0.1
OPTIMIZER ADAM ADAM ADAM
LEARNINGRATE 1ğ‘’-4 1ğ‘’-4 1ğ‘’-4
LAYERTYPE MLP MLP MLP
#OFLAYER 3 3 3
#OFUNITS (256*N,256*N,256*N) (256*N,256*N,256*N) (256,256,256)
ACTIVATIONTYPE RELU RELU RELU
WEIGHTINITIALIZATION BUN RANDOMNORMAL RANDOMNORMAL
#OFNEIGHBORS - - ALL
ğ‘‡ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ 10K 5K -
ğ‘‡ ğ‘’ğ‘›ğ‘‘ 30K 0.75*TIME -
Î”ğ‘‡ 1000 100 -
Figure5:LearningcurveduringthetrainingofGrid2Ã—environment.TheplotsshowtheEpisodewaitingtimeandAverageWaiting
TimeofVehicleinthegridnetwork.Figure6:LearningcurveduringthetrainingofInglodastCorridorenvironment.TheplotsshowtheEpisodewaitingtimeandAverage
WaitingTimeofVehicleinthegridnetwork.
Figure7:LearningcurveduringthetrainingofInglodastCorridorenvironment.TheplotsshowtheEpisodewaitingtimeandAverage
WaitingTimeofVehicleinthegridnetwork.Table6:ThepercentageofsparsityvariesacrossdifferentapproachesinCooperativeNavigationandTrafficSignalControlenvi-
ronments,with(N)representingthenumberofagentsintheenvironment.Itisevidentthatasthenumberofagentsincreases,the
percentageofsparsityalsoincreases.Thistrendhighlightsthescalabilityofsparseapproachesinhandlinglargernumbersofagents.
Notably,thesparsityleveloftheBUNapproachinitiallymatchesthatofthedecentralizedapproachbutgraduallyincreasesperthe
allocatedbudget,eventuallyreachingalevelcomparabletothatofRigL
SPARISTY(%)
SS(N=2) SS+C(N=2) SS+CC(N=3) GRID2Ã—2(N=4) ING.CORRIDOR(N=7)
CENTRALIZED 0 0 0 0 0
DECENTRALIZED 50 50 66.66 75 85.71
BUN 57 57 64.4 75 85.71
DGN 0 0 0 0 0
RIGL 57 57 64.4 75 85.71