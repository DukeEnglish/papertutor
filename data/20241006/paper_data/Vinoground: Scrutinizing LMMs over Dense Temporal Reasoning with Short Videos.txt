Preprint. Underreview.
VINOGROUND: SCRUTINIZING LMMS OVER DENSE
TEMPORAL REASONING WITH SHORT VIDEOS
JianruiZhang‚àó MuCai‚àó YongJaeLee
DepartmentofComputerSciences
UniversityofWisconsin-Madison
{harrisz,mucai,yongjaelee}@cs.wisc.edu
ABSTRACT
Therehasbeengrowingsentimentrecentlythatmodernlargemultimodalmodels
(LMMs) have addressed most of the key challenges related to short video com-
prehension. As a result, both academia and industry are gradually shifting their
attentiontowardsthemorecomplexchallengesposedbyunderstandinglong-form
videos. However,isthisreallythecase? OurstudiesindicatethatLMMsstilllack
many fundamental reasoning capabilities even when dealing with short videos.
WeintroduceVinoground,atemporalcounterfactualLMMevaluationbenchmark
encompassing 1000 short and natural video-caption pairs. We demonstrate that
existingLMMsseverelystruggletodistinguishtemporaldifferencesbetweendif-
ferent actions and object transformations. For example, the best model GPT-
4o only obtains ‚àº50% on our text and video scores, showing a large gap com-
pared to the human baseline of ‚àº90%. All open-source multimodal models and
CLIP-basedmodelsperformmuchworse,producingmostlyrandomchanceper-
formance. Throughthiswork,weshedlightontothefactthattemporalreasoning
inshortvideosisaproblemyettobefullysolved.Thedatasetandevaluationcode
areavailableathttps://vinoground.github.io.
1 INTRODUCTION
Largemultimodalmodels(LMMs)havebecomeverycompetitiveinnotonlyimagecomprehension
but also short video comprehension. Proprietary models such as GPT-4o (OpenAI, 2024a) and
Gemini-1.5-Pro (Gemini Team, 2024) as well as open-source models like LLaVA-OneVision (Li
etal.,2024a)andQwen2-VL(Wangetal.,2024)demonstratestrongperformanceinsummarizinga
shortvideo‚Äôscontentsandansweringquestionsregardingitsdetails.Thishasledmanyresearchersto
believethatshortvideocomprehensionhasmostlybeensolved,andconsequently,thecommunity‚Äôs
focushasbeenincreasinglytrendingtowardcreatingmodelsthatunderstandlonger-formvideosthat
are10sofsecondsorevenminuteslong. Ourstudy,however,indicatesthatexistingmodelsarefar
frombeingcapableoffullyunderstandingshortvideosthatarejustafewsecondslong,especially
whenthereisdensetemporalinformation.
AsdemonstratedinWu(2024)andMangalametal.(2023),formanyexistingvideobenchmarkslike
EgoSchema(Mangalametal.,2023),ActivityNet-QA(Yuetal.,2019),MSVDandMSRVTT(Xu
et al., 2017), the performance of most modern LMMs does not vary significantly with number of
sampledframes.Infact,itisoftenthecasethatanLMMonlyneedstoseeasingleframetoproduce
a correct response. This ‚Äòsingle-frame bias‚Äô (Lei et al., 2023) reduces the video comprehension
problem into the much easier image comprehension problem, essentially discarding the temporal
aspectofavideo. Researchershavealsoproposedhardertemporalcounterfactualbenchmarks(Li
etal.,2024b;Saravananetal.,2024;Liuetal.,2024b)inordertobetterevaluateanLMM‚Äôstemporal
understanding capabilities. Existing counterfactual datasets test a model‚Äôs ability to distinguish
slightchangesfromavideo‚Äôsoriginal(positive)captiontothenew(negative)captionbyaskingthe
modeltomatchthevideowiththecorrectcaption. However,theyeitherdonotcontainanynegative
videos corresponding to the negative caption, or simply swap the order of two unrelated videos
to form the positive and negative videos, making it easy to distinguish the negative pair from the
‚àóEqualContribution
1
4202
tcO
3
]VC.sc[
1v36720.0142:viXraPreprint. Underreview.
Which video best fits the caption: The man moved the lid from the top to the side?
### In the first video, the man is seen moving the lid from the top of the box to the side.
In the second video, the man is seen moving the lid again, but this time it appears to be
more about adjusting the lid rather than moving it.
### Conclusion: The caption "the man used one hand to move the lid from the top of the box
to the side" matches the first video.
Figure1: GPT-4oansweringavideo-scorequestionincorrectly. Whenaskedwhichvideomatches
the caption, which involves identifying the order of the two events mentioned, GPT-4o does not
mentionanythingaboutthetemporalorderofevents. Theerroneousanalysesaremarkedinred. It
shouldalsobenotedthattheanalysesforbothvideosarecompletelywrong.
original positive pair due to the videos‚Äô unnaturalness. Hence, these benchmarks may be inflating
theperformancesofmodernLMMsinunderstandingshortvideos.
In this paper, we introduce Vinoground, a temporal counterfactual LMM evaluation benchmark
composed of 1000 short and natural video-caption pairs. Vinoground is a challenging benchmark
aimedtoexposetheincapabilitiesofstate-of-the-artmodelsinunderstandingtemporaldifferences
between different actions (e.g., ‚Äúthe man eats then watches TV‚Äù vs. ‚Äúthe man watches TV then
eats‚Äù) and object transformations (e.g., ‚Äúwater turning into ice‚Äù vs. ‚Äúice turning into water‚Äù). In
each pair of captions, the positive and negative are the same in word composition but different in
order. Our work is inspired by Winoground (Thrush et al., 2022), a challenging counterfactual
benchmark for visio-linguistic compositional reasoning in images. In Winoground, a model must
correctly match two images with their corresponding captions, where both captions use the same
setofwords,butarerearrangedtodescribeeachimage(e.g.,‚Äúsomeplantssurroundingalightbulb‚Äù
vs.‚Äúalightbulbsurroundingsomeplants‚Äù). Thisevaluateswhetheramodeleffectivelyencodesthe
textandimages,payingattentiontotheircompositionalstructures,andwhetheritcanintegrateand
synthesizeinformationacrossbothmodalities. Ourbenchmark‚Äôsnamechangesthe‚ÄòW‚Äôtoa‚ÄòV‚Äôfor
‚Äúvideo‚Äù, and further employs temporal counterfactuals to emphasize this unique element in video
data.Weusetextscore,videoscore,andgroupscoretoevaluateamodel‚Äôsabilitytochoosetheright
captionforavideo,tochoosetherightvideoforacaption,andtomatchbothpositiveandnegative
video-caption pairs correctly, respectively. These measure a model‚Äôs textual, visual, and temporal
reasoningcapabilitiesinabalancedmanner. Mostofourvideosarelessthan10secondslong,yet
wefindaverylargeperformancegapbetweenanaveragehumanandtoday‚Äôsbestmodels.
Insum,ourmainfindingsandcontributionsare:
‚Ä¢ ExistingtemporalcounterfactualbenchmarksfailtofullyexposetheincapabilityofLMMs
intemporalreasoning.
‚Ä¢ WeintroduceVinoground, thefirsttemporalandnaturalcounterfactualevaluationbench-
markforevaluatingvideounderstandingmodels.
‚Ä¢ ModernSoTALMMperformanceissubparwhenitcomestotemporalreasoninginshort
video comprehension tasks; most models perform at random-chance level on video score
andevenworseongroupscore,bothbeingsignificantlylowerthantextscore.
‚Ä¢ Wecategorizeourdatainto3majorcategories,‚Äòobject‚Äô,‚Äòaction‚Äô,and‚Äòviewpoint‚Äô,aswell
as4minorcategories,‚Äòinteraction‚Äô,‚Äòcyclical‚Äô,‚Äòspatial‚Äô,and‚Äòcontextual‚Äô,inordertodissect
each model‚Äôs capabilities for each of these categories. We find that existing models are
decentatanalyzingvideoframesatcoarse-levelbuttendtomissfine-graineddetails.
‚Ä¢ Shortvideocomprehensionisaproblemthatisfarfrombeingsolved.
2Preprint. Underreview.
2 RELATED WORK
Counterfactual Reasoning Counterfactual reasoning (Morgan & Winship, 2015) in the context
of computer vision typically involves curating negative images and captions by manipulating the
originaldataandobservinghowtheoutcomechanges(Hendricksetal.,2018;Yehetal.,2019;Goyal
etal.,2019;Vermaetal.,2020;Guoetal.,2023;Zhangetal.,2021;Thrushetal.,2022;Leetal.,
2023;Zhangetal.,2024a). Theideaisthatamodelshouldunderstandcauseandeffectandbeable
tomakepredictionsinunseensituations. Forevaluation,curatingmeaningfulandhardnegativesis
important.Winoground(Thrushetal.,2022)isapioneeringbenchmarkforcounterfactualreasoning
where each data point contains two images and two corresponding captions. Given an image, a
vision-languagemodelisaskedtofindthematchingcaptionfromtheprovidedtwooptions,andvice
versa. COCO-Counterfactual(Leetal.,2023)exploressimplelinguisticrulestogeneratenegative
captionsandusesanimageeditingmodeltoproducenegativeimages. Inthiswork,weintroducea
novelbenchmarkwithcounterfactualsthataretemporal,anattributespecifictothevideomodality.
Single-FrameBiasandTemporalReasoning Animportantaspectofvideodataisitstemporal-
ity,i.e.,howeventschangeastimeprogresses. ModernLMMssampleframesandtreatthevideoas
asetofimages,bothduringtrainingandevaluation. BenchmarkssuchasEgoSchema(Mangalam
etal.,2023),MSVDandMSRVTT(Xuetal.,2017)exhibita‚Äòsingle-framebias‚Äô(Leietal.,2023)
where only one video frame is needed for a model to predict correctly, as a model‚Äôs performance
doesnotvarysignificantlyasthenumberofframessampledincreases(Wu,2024;Mangalametal.,
2023).Tobetterevaluateamodel‚Äôstemporalunderstandingcapabilities,researchershavedeveloped
datasets such as YouCook2 (Zhou et al., 2018), ActivityNet-QA (Yu et al., 2019) and COIN (Lin
etal.,2022),whichmainlyinvolveproceduralactivitiesthatoftenhaveaspecifictemporaldepen-
dency(e.g., ifavideoshowsapersonwashingandslicingapples, andthenbakinganapplepie, a
modelwouldeasilypredictthat‚Äúbakeittomakeapiebeforewashingtheapple‚Äùisawrongcaption
even without looking at the video). In contrast, Vinoground also includes actions that are entirely
unrelated,makingitmorechallengingformodelstoinferanswersbasedsolelyontextualcues.
Temporal Counterfactuals Recent benchmarks combine counterfactuals with temporal reason-
ing. EgoSchema (Mangalam et al., 2023) introduces long-form videos where each video has 1
positivecaptionand4negativecaptionstochoosefrom,whileVITATECS(Lietal.,2024b)intro-
duces temporal counterfactual data where a word or phrase is swapped/replaced from the positive
captiontoformthenegativecaption. However,neitherhasanynegativevideosandthusdonotfully
evaluateanLMM‚Äôsdensetemporalreasoningcapabilitieslikewedo. VELOCITI(Saravananetal.,
2024) introduces positive/negative videos as a part of their intra-video association benchmark by
clippingrandomportionsinthesamevideo,andaskingthemodeltodistinguishbetweentheevents.
These videos, however, are not truly counterfactual pairs as different clips within the same movie
arenotguaranteedtohaveapositive-negativerelation. TempCompass(Liuetal.,2024b)includes
videosthattestsamodel‚Äôsabilitytodifferentiatetheorderofevents,butthevideosareeithercon-
catenations of two completely unrelated videos with drastic frame changes in between the events,
or reversed in time and thus impossible to happen in real life, and do not belong to the true data
distribution. AswewillillustrateinSection4.4.2,LMMstendtodomuchbetterwhenitcomesto
suchvideoswhencomparedtoourbenchmark‚Äôsmorenaturalnegativevideos.
3 VINOGROUND
In this section, we introduce our data curation and categorization process. In order to curate
Vinoground‚Äôs video-caption pairs, we first explain how we generate the required captions in Sec-
tion3.1,howwefindthecorrespondingvideosinSection3.2,andfinallythedetailsofcategorizing
thevideosinSection3.3. AnillustrationoftheoverallprocesscanbefoundinAppendixA.
3.1 GENERATINGCOUNTERFACTUALCAPTIONS
Thefirststepincuratingourdataistofindcounterfactualcaptionpairs. Wewanttoensurethatthe
captionswecurateareofhigh-qualityandtemporalinnature. Whilehumanannotationisapossible
solution,itiscostlyandhardtoscaleup. Instead,weleverageaSoTALLM,specificallytheGPT-
4(OpenAI,2024b)model,asitismuchcheaper,followsthemultiplerequirementsweimpose,and
3Preprint. Underreview.
Object
Fire turns into thin air. Thin air turns into fire.
Action
The baby plays before he drinks water. The baby drinks water before he plays.
Viewpoint
Camera angle from 45 degrees behind to the right side. Camera angle from the right side to 45 degrees behind.
Interaction
The watermelon is cut then turned. The watermelon is turned then cut.
Cyclical
The man writes before he dips his pen in the ink. The man dips his pen in the ink before he writes.
Spatial
Moonwalk from left to right. Moonwalk from right to left.
Contextual
From landed to flying. From flying to landed.
Figure2: Examplepositive/negativevideo-captionpairsinVinoground,foreachcategory.
guaranteesthattherearenoduplicatecandidates.Werequireourcaptionpairstobecomposedofthe
exactsamewords,onlypermutedintodifferentorders. Wealsowanttoavoidcandidatesthatcould
easily be solved by looking at a single frame of the video such as ‚Äúa man is waving at a woman‚Äù
vs. ‚Äúa woman is waving at a man‚Äù. Hence, we ask GPT-4 to create temporal counterfactuals that
require one to process and understand the entire video, and in particular, understand the order of
eventsinwhichtheyhappen,suchas‚Äúamanwavesatawomanbeforehetalkstoher‚Äùvs.‚Äúaman
talkstoawomanbeforehewavesather‚Äù. WewilllatershowcaseinSection4.3thatwecanalready
expose LMMs greatly with such videos (i.e., by swapping the order of two events), making more
complicatedscenariosunnecessary.
3.2 VIDEOCURATION
Aftercuratingcounterfactualcaptioncandidates,wenexttrytofindcorrespondingvideosforthose
captions.WemakeuseoftheVATEX(Wangetal.,2019)dataset,whichcontains5distinctcaptions
foreachmaximum10-secondlongvideo. WeonlyusethevalidationandtestsubsetsofVATEXto
makesurenoneofVinogroundiseverusedastrainingdata. Thisresultsinapoolof9000videos
and45000captions.
WewanttobeabletoquicklyretrievepotentialmatchesinVATEXaccordingtothegeneratedcap-
tioncandidates.Weleveragesentencetransformers(Songetal.,2020),whicharegoodatsummariz-
4Preprint. Underreview.
ingsentence-levelinformationintofeaturevectors,toextractthefeaturesofbothourGPT-generated
captionsandVATEX‚Äôscaptions. WesubsequentlyusetheFaisslibrary(Douzeetal.,2024)toeffi-
cientlyindexandretrievethetop20mostsimilarVATEXcaptionsforeachGPT-4generatedcaption.
We manually examine if any retrieved caption is a good match, and if its corresponding video re-
flectsthecaptionaswell. Forsomecaseswherenoneoftheretrievedcaptionsareagoodmatch,we
searchYouTubewiththecaptioncandidatetofindamatchingvideo.
In the end, we curate 500 counterfactual pairs of video-caption pairs (1000 video-caption pairs in
total) for evaluation. Each video-caption pair is provided in the form of the original YouTube ID,
theclip‚Äôsstartingandendingtimestamps,andthecorrespondingcaption. WealsoputVinoground
through 3 rounds of human evaluation by the authors, making sure that the pair of captions truly
containthesamewordcompositionandthatthevideoclipsindeedreflecttheirrespectivecaptions.
3.3 CATEGORIZATION
Finally, we want to be able to evaluate LMMs in a fine-grained manner on multiple aspects rep-
resented by our dataset. Hence, we categorize Vinoground according to the unique characteristics
discoveredthroughthedatacurationprocess,asshowninFigure2. Wereportthenumberofcoun-
terfactualdatapairsassignedundereachcategoryinTable1. Wedefineeachcategoryasfollows:
Category Object Action Viewpoint Interaction Cyclical Spatial Contextual
Count 160 257 83 73 111 103 63
Table1: Thenumberofvideo-captionpairsassignedundereachcategory.
WedivideVinogroundinto3majorcategories: object, action, andviewpoint. Eachcounterfactual
pairmustbeinoneandonlyoneofthethreemajorcategories.
‚Ä¢ ObjectrequiresLMMstodetectchangesinthestatusofonespecificobject,suchas‚Äúwater
turning into ice‚Äù vs. ‚Äúice turning into water.‚Äù This category is similar to the ‚ÄúReversing‚Äù
category in TempCompass (Liu et al., 2024b) that evaluates a model‚Äôs ability to detect
attributeanddirectionalchanges. WhileTempCompassreversespositivevideosintimeto
createnegativesandthuscanbeunnatural, wecuratereal, naturalvideosthatcorrespond
tothenegativecaptions.
‚Ä¢ Action, on the other hand, simply asks models to distinguish the order in which two or
more different actions happened, e.g. ‚Äúthe man eats and then watches TV‚Äù vs. ‚Äúthe man
watches TV and then eats.‚Äù The two actions need not be correlated at all, and thus less
logicalcomprehensionisnecessaryforacorrectprediction.
‚Ä¢ Viewpointspecificallydescribeschangesinthecameraangle,perspective,orfocuswithin
the video, such as ‚Äúa person films the car in front of him before he films himself‚Äù vs. ‚Äúa
person films himself before he films the car in front of him.‚Äù The change in viewpoint is
usually accompanied by a drastic difference in between the frames, whereas other events
mostlikelyhappenwithinthesamecontextorbackground.
We also divide Vinoground into 4 minor categories: interaction, cyclical, spatial, and contextual.
Some pairs belong to a multitude of these minor categories, while some do not belong to any of
them.
‚Ä¢ Interactioninvolvesvideoswhereahumanchangestheirwayofinteractingwithanobject
inthecourseofthevideo,e.g. ‚Äúthecalligrapherwriteswithhispenbeforehedipsitinto
theink‚Äùvs.‚Äúthecalligrapherdipshispenintotheinkbeforehewriteswithit.‚Äù
‚Ä¢ Cyclicaltestsamodel‚Äôsabilitytoidentifyeitherproceduraltemporalactivitiesortwoac-
tionsthataredependentoneachother. Thecalligrapherexampleearlierisalsocyclicalas
thepersonrepeatstheprocedure‚Äúwrite,dip,write,dip...‚Äù,andtheaction‚Äúdip‚Äùhappensas
aresultof‚Äúwrite‚Äùinthepositive, while‚Äúwrite‚Äùisenabledafter‚Äúdip‚Äùinthenegative. In
contrast,thegeneral‚Äúaction‚Äùcategorycaninvolvecompletelyunrelatedactions.
‚Ä¢ Spatial It has been shown that LMMs struggle to distinguish physical locations between
objects in image-caption pairs (Zhang et al., 2024a). We want to further evaluate this
5Preprint. Underreview.
deficiencywhenitcomestotemporalunderstandingaswell. Thus,thiscategoryinvolves
object movements and requires positional understanding, such as ‚Äúthe man ran from left
toright‚Äùvs.‚Äúthemanranfromrighttoleft.‚ÄùNotethatthisdoesnotincludemovementof
thebackground;e.g.,whenthecameraismovingalongwiththeobjectinquestion,which
belongstothenextcategory.
‚Ä¢ ContextualrequiresLMMstounderstandchangesinthebackgroundorgeneralinforma-
tionofentirevideoframes. Anexampleisthepair‚Äúthebikerridesdownthestreetbefore
hegoesdownthestairs‚Äùvs.‚Äúthebikergoesdownthestairsbeforeheridesdownthestreet‚Äù
where the camera that records the videos is strapped on the biker‚Äôs forehead, making the
background the only changing aspect. One cannot infer positional changes by only ob-
servingmovementsoftheobjectinthevideolikethe‚Äúspatial‚Äùcategory,butinsteadmust
focusonthebackgroundastheobjectinquestioncanappearmotionlessduetothecamera
movingalongwiththeobject.
Weprovidein-depthanalysisofmodels‚Äôperformancesonourbenchmarkbasedontheabovecate-
goriesinSection4.4.2.
4 EXPERIMENTS
In this section, we evaluate state-of-the-art vision-language models on our benchmark. We first
describethemodelsandevaluationmetricsinSection4.1;thenweexplainourexperimentalsetup,
including prompting methods and human studies, in Section 4.2; we analyze the performances of
themodelsinSection4.3,andprovidefurtherablationstudiesinSection4.4.
4.1 MODELSANDEVALUATIONMETRICS
WeevaluatebothCLIP-basedmodels(Radfordetal.,2021)andlargegenerativemodels,bothpro-
prietaryandopen-source. TheexactlistofmodelsweevaluatecanbefoundinTable2. CLIP-based
models use contrastive learning between videos and captions, while text-generation LMM models
use next-word prediction to generate a response. Due to the different nature of the CLIP-based
vs.LMMmethods,weintroduceourmetricsindifferentfashionsaccordingly.
WeuseC todenotecaptionsandV todenotevideos. Foreachpositiveandnegativesetofcounter-
factualvideo-captionpairs,(C ,V )and(C‚Ä≤,V‚Ä≤),‚àÄi ‚àà {1,2,...,500},weaskCLIP-basedmodels
i i i i
tocomputeasimilarityscoreebetweennotonlythecorrectpairsbutalsotheincorrectpairs(C ,V‚Ä≤)
i i
and (C‚Ä≤,V ) (identical to Winoground (Thrush et al., 2022)). For generative LMMs, we can only
i i
provideinputs(e.g.,2captionsand1video)tothemodelandaskittooutputananswerAorB.
We first evaluate the text score s where the model is presented with both positive and negative
t
captions but only one of the videos, forming the triplets (C ,C‚Ä≤,V ) and (C ,C‚Ä≤,V‚Ä≤). For each
i i i i i i
triplet,themodelisthenaskedtochoosethecaptionthatdescribesthecontainedvideo. Wedenote
thescorefunctionofamodelresponsegivenanytripletass;forinstance,
(cid:26)
1ifLMMchoosesC ore >e forCLIP-based
s(C i,C i‚Ä≤,V i)=
0otherwise
i (Ci,Vi) (C i‚Ä≤,Vi)
(cid:26) 1ifLMMchoosesC‚Ä≤ore >e forCLIP-based
s(C i,C i‚Ä≤,V i‚Ä≤)=
0otherwise
i (C i‚Ä≤,V i‚Ä≤) (Ci,V i‚Ä≤)
Thenthetextscoreforthegivencounterfactualpair(C ,V )and(C‚Ä≤,V‚Ä≤)is:
i i i i
s (C ,C‚Ä≤,V ,V‚Ä≤)=s(C ,C‚Ä≤,V )‚àßs(C ,C‚Ä≤,V‚Ä≤)
t i i i i i i i i i i
where ‚àß is the logical and operator; i.e., s is 1 only if both triplets are correct. This exposes the
t
modelswhentheyguessrandomly.
Similarly,forvideoscores ,themodelispresentedwithonecaptionandbothpositiveandnegative
v
videos,formingtriplets(C ,V ,V‚Ä≤)and(C‚Ä≤,V ,V‚Ä≤). Foreachtriplet,themodelisaskedtochoose
i i i i i i
thevideothatisdescribedbythecaption. Inthiscase,theresponsescoringbecomes:
(cid:26)
1ifLMMchoosesV ore >e forCLIP-based
s(C i,V i,V i‚Ä≤)=
0otherwise
i (Ci,Vi) (Ci,V i‚Ä≤)
6Preprint. Underreview.
(cid:26) 1ifLMMchoosesV‚Ä≤ore >e forCLIP-based
s(C i‚Ä≤,V i,V i‚Ä≤)=
0otherwise
i (C i‚Ä≤,V i‚Ä≤) (Ci,V i‚Ä≤)
Thenthevideoscoreis:
s (C ,C‚Ä≤,V ,V‚Ä≤)=s(C ,V ,V‚Ä≤)‚àßs(C‚Ä≤,V ,V‚Ä≤)
v i i i i i i i i i i
Wealsoincludeagroupscoremetrics :
g
s (C ,C‚Ä≤,V ,V‚Ä≤)=s (C ,C‚Ä≤,V ,V‚Ä≤)‚àßs (C ,C‚Ä≤,V ,V‚Ä≤)
g i i i i t i i i i v i i i i
s servesastheultimatetestforamodeltodemonstrateitstemporalreasoningcapabilitiesinboth
g
the textual and visual domains, as both s and s must be 1. For all three metrics, we report the
t v
meanoveralltestinstances. WeincludeanillustrationofthemetricsinAppendixB.
4.2 EXPERIMENTALSETUP
Sinceforeachpairofcounterfactuals,wehave2text-scorequestionsand2video-scorequestions,
wehave2000questionsintotal. ToevaluateCLIP-basedmodels, weusetheevaluationcodepro-
videdbytheauthorstocalculatevideo-captionembeddingsandsimilarityscores. Evaluatingtext-
generativemodelsisslightlymorecomplicated. Wefirstintroducethedifferentpromptsweused.
Fortextscore,weprovidethemodelwiththevideoandthetwocorrespondingcaptions,andprompt
‚Äú<video> Which caption best describes this video? A. {Caption 1}, B. {Caption 2}‚Äù. For video
score,however,sincesomeLMMsonlysupport1videoinput,weconcatenatethepositiveandneg-
ativevideosintoasinglevideowitha2secondblackscreeninbetween. WhensamplingN frames
for the model‚Äôs input, we make sure we sample (N ‚àí1)/2 frames from the positive and negative
video fragments and at least 1 frame of black screen in between. For the sake of consistency, we
provide all models with the single concatenated video, regardless of how many videos they can
actually take as input. We then prompt the model with ‚Äú<video> Which video segment matches
this caption? Note: The video contains two segments separated by a 2-second black frame. Cap-
tion: {Caption}. A.Firstsegment(beforeblackframe),B.Secondsegment(afterblackframe)‚Äùto
choose between the two video segments. We also report the results with respect to the number of
framessampledbythemodelfromthevideo, ifsupported, toevaluatetheeffectoftemporalityin
Section4.4.1.
In addition, we also use Prolific (https://www.prolific.com) to evaluate human perfor-
mance,andfindthatourdatasetisfairlyeasyforanaveragehumantocompletewithhighaccuracy.
Prolific is a platform similar to Amazon MTurk which recruits workers to complete tasks such as
data annotation. The interface we present to the workers is in Appendix D. To filter out unfaith-
ful workers, we employ a qualification process prior to evaluating on Vinoground. We sample 10
video-question pairs from TempCompass (Liu et al., 2024b) that are of the event order category,
whichcontainsconcatenatedvideoswithnocorrelation,suchas‚Äúamanliftsweightsinagym,then
a cat plays on the grass‚Äù. Such examples are easy enough for an average human to obtain 100%
accuracy. We ask the workers the 10 beginner-level questions first, and they are qualified only if
theyanswereveryquestioncorrectly. Thisprocessresultsin170qualifiedworkers.
We conduct human evaluation under two settings. First, the Prolific workers are provided the full
videos with audio. To create another environment where we want the workers see the same input
as the models, we uniformly sample 32 frames from each video and concatenate them together
into a new 10-second video with no audio. The results for the two settings are also compared in
Section4.4.1. Foreachquestion, weobtainanswersfrom10uniqueworkers. Forthe10answers
from a single question, we calculate the ‚Äúaverage‚Äù human response by taking the mode of the 10
answers. Wethenreportthemeanoverallthequestionsasthefinalresult.
4.3 MAINRESULTS
Table2presentstheresults.(PleaserefertoAppendixFformoredetailedresults,asweonlyinclude
eachmodel‚Äôsbestperformanceshere.)
First, all CLIP-based models (VideoCLIP, LanguageBind, ImageBind) perform much worse than
random chance, suggesting that contrastive learning does not provide models with enough knowl-
edgeoftemporality. Amongtext-generativemodels,GPT-4operformsbest,achieving54.0%onthe
7Preprint. Underreview.
Model Frames Text Video Group
RandomChance N/A 25.00 25.00 16.67
ProlificHuman All 93.40 94.00 90.00
32 91.40 90.80 85.20
GPT-4o(OpenAI,2024a)(CoT)(Weietal.,2022) 32 59.20 51.00 35.00
GPT-4o 32 54.00 38.20 24.60
0 10.00 24.60 2.00
Gemini-1.5-Pro(GeminiTeam,2024)(CoT) 1fps 37.00 27.60 12.40
Gemini-1.5-Pro 1fps 35.80 22.60 10.20
Claude3.5Sonnet(Anthropic,2024) 4 32.80 28.80 10.60
Qwen2-VL-72B(Wangetal.,2024) 32 50.40 32.60 17.40
Qwen2-VL-7B(Wangetal.,2024) 4fps 40.20 32.40 15.20
LLaVA-OneVision-Qwen2-72B(Lietal.,2024a) 32 48.40 35.20 21.80
LLaVA-OneVision-Qwen2-7B(Lietal.,2024a) 16 41.60 29.40 14.60
InternLM-XC-2.5(Zhangetal.,2024b)(CoT) 32/1fps 30.80 28.40 9.00
InternLM-XC-2.5 32/1fps 28.80 27.80 9.60
VideoLLaMA2-72B(Chengetal.,2024) 8 36.20 21.60 8.40
MiniCPM-2.6(Yaoetal.,2024) 16 32.60 29.20 11.20
LLaVA-NeXT-Video-34B(Liuetal.,2024a)(CoT) 32 25.80 22.20 5.20
LLaVA-NeXT-Video-34B 32 23.00 21.20 3.80
LLaVA-NeXT-Video-7B(Liuetal.,2024a)(CoT) 32 21.80 26.20 6.80
LLaVA-NeXT-Video-7B 32 21.80 25.60 6.20
Video-LLaVA-7B(Linetal.,2024) 8 24.80 25.80 6.60
Phi-3.5-Vision(Microsoft,2024) 16 24.00 22.40 6.20
MA-LMM-Vicuna-7B(Heetal.,2024) 4 23.80 25.60 6.80
VideoCLIP(Xuetal.,2021) 60 17.00 2.80 1.20
LanguageBind(Zhuetal.,2024) 8 10.60 5.00 1.20
ImageBind(Girdharetal.,2023) 20 9.40 3.40 0.60
Table 2: Vinoground results for different models and sampled frames. Performances significantly
better than random chance are bolded. The table is separated into four groups by double lines:
random chance and human performance, proprietary text-generative models, open-source text-
generative models, and CLIP-based models from top to bottom. The best performances of pro-
prietaryandopen-sourcemodelsarehighlightedinred.
textscoremetric. Chain-of-Thought(CoT)prompting(Weietal.,2022)furtherimprovesGPT-4o‚Äôs
performance,especiallyonthevideoscoremetricwhereGPT-4oimprovesby12.8%whileitsgroup
score increases by 10.4%. Amongst the open-source models, LLaVA-OneVision and Qwen2-VL
demonstratecompetitiveperformancecomparedtoproprietarymodels,especiallywithQwen2-VL-
72B‚Äôs50.4%performanceontextscore. CoTpromptingonopen-sourcemodels,however,helpsthe
modelsmuchless,especiallyiftheyareperformingatnearchancelevel. Allothermodelsperform
atorworsethanrandomchance,showingthatdensetemporalreasoningisstillverychallengingfor
LMMs.
Similar to Winoground (Thrush et al., 2022), we find that for models that perform better than
chancelevel,theirtextscoreissignificantlyhigherthanvideoscore,whilegroupscoreisthelow-
est amongst all three. This shows that they are better at identifying textual differences compared
tovisual/temporaldifferences. Forexample,GPT-4o‚Äôsvideoscore(38.20%)issignificantlylower
comparedtoitstextscore(54.0%). Manyopen-sourcemodelsonlyhavenon-randomoutcomeson
thetextscorebutequalorlowerthanrandomchanceonvideoandgroupscores. Notably,LLaVA-
OneVision-72Bistheonlyopen-sourcemodelthatdemonstratesbetterthanchancegroupscore.
The human evaluators perform significantly better than any model, with scores around 90%. This
indicatesthatVinogroundisabenchmarkthatcanbetackledrelativelyeasilywithinhumancapacity.
Whenthehumanevaluatorsareprovidedwith32-framevideos,thescoresdecreasebyafewpoints,
butarestillmuchhigherthanthoseofanymodel.
8Preprint. Underreview.
Model Frames Text Video Group
ProlificHuman All 93.40 94.00 90.00
32 91.40 90.80 85.20
GPT-4o 64 49.00 34.80 19.00
32 54.00 38.20 24.60
8 53.60 31.40 20.60
1 28.20 28.00 10.00
LLaVA-OneVision-Qwen2-72B 64 46.20 31.80 18.60
32 48.40 35.20 21.80
16 47.20 33.80 20.40
8 46.80 29.80 19.00
4 40.40 24.80 13.00
2 33.40 25.20 10.20
LLaVA-OneVision-Qwen2-7B 64 40.20 28.60 12.60
32 42.00 28.40 12.80
16 41.60 29.40 14.60
8 36.00 26.80 12.40
4 29.20 28.00 10.00
2 25.80 22.60 6.80
Table3: Resultsofthestrongestclosed-sourceandopen-sourcemodelswithdifferentframessam-
pled. Performancessignificantlyhigherthanrandomchancearehighlighted,whilethebestoverall
performanceofeachmodelarehighlightedinred. Moreframesdoleadtobetterperformance,but
toomanyframescanworsentheresults.
Finally,wealsoreportperformanceforGPT-4owith0framessampled,asacontroltotestfortext
bias in these models. For text score, we hypothesize that the model will choose the more likely
caption since it cannot see the video, and for the video score, we hypothesize it will choose an
answer at random, which is indeed what happens. The lower than chance performance for text
scoreof10.0%indicatesthatthereissomelanguagebiasinGPT4o, whereitpreferstoselectone
captionovertheother(ifitconsistentlydidthatforallquestions,thetextscorewouldbe0). Thus,
our balanced way of computing the scores (i.e., both s(C ,C‚Ä≤,V ) and s(C ,C‚Ä≤,V‚Ä≤)) prevents a
i i i i i i
model from doing well only via its language bias. This is in contrast to existing benchmarks like
VITATECS(Lietal.,2024b)andEgoSchema(Mangalametal.,2023)whichlacknegativevideos,
andhenceenablemodelstopotentiallyansweraquestioncorrectlyonlybasedonwhichcaptionis
morelikely.
Allinall, eventheverybestmodelsexhibitsubparperformancewhenitcomestodensetemporal
reasoning,andthisisonlyusingshortvideos(lessthan10seconds)aswell. Thisstronglyindicates
thatshortvideocomprehensioninLMMsisstillfarfromhuman-levelintelligence.
4.4 IN-DEPTHANALYSISOFPERFORMANCEVARIATIONS
4.4.1 FRAMESSAMPLED
Vinoground‚Äôs temporal understanding requirements can be demonstrated by varying the different
number of frames sampled, either from the video entirely, or as measured by frames-per-second
(fps).Ifadatasetsuffersfrom‚Äòsingle-framebias‚Äô,amodelwouldnotperformverydifferentlywhen
only1ormoreframesaresampled. Theresultsofthestrongestproprietaryandopen-sourcemod-
elsinTable3(andadditionalresultsinAppendixF)showthatthemoreframesamodeltakes,the
better its performance. This indicates that a model does need the entirety of each video to fully
comprehendthetaskathand. Interestingly,toomanysampledframes,however,canhurtamodel‚Äôs
performance;forGPT-4o,its64-framevariantperforms5%worseonallthreemetricscomparedto
its32-framevariant. Wesuspectthatcurrentmodelsarenotgoodatdiscardingredundantinforma-
tionandisolatingsignalfromnoisewhentherearetoomanyvisualtokens.
9Preprint. Underreview.
Figure3: Groupscoreforeachmodel,groupedbycategory. Onecanobservehigherperformance
incontextualandviewpoint,andlowerperformanceonothercategories.
Notethatforourvideoscoremetrictofunctionasintended,amodelmustsampleatleastoneframe
from each video, and at least one black frame in between. This means that the number of frames
sampledmustbenofewerthan3. Wehencegrayoutthevideoscoreandgroupscoreperformances
ofmodelssampledat1or2framesandonlyfocusontheirtextscores.
Finally, for human evaluators, the ‚ÄòAll‚Äô group performs better than the 32 frame group, which in-
dicates that humans can answer Vinoground questions better when the full videos are shown. In
contrast,modernLMMsgenerallylacktheabilitytoprocessinputsofanentirevideowithoutcoarse
samplingofframes. Thissuggeststhatfurtherresearchintocreatingmodelsthatcanhandlemore
frameswillbeanimportantresearchdirectionfortemporalreasoning.
4.4.2 CATEGORY
Figure3showsresultspercategoryasdefinedinSection3.3. Interestingly, manymodelsperform
significantly better on the viewpoint and contextual categories, while being significantly worse on
other categories. Here, we only report the group score for a selected set of models due to space.
PleaseseeAppendixEforthefullresults.
Bothviewpointandcontextualbringforthdrasticchangesinbetweenthevideoframeswheneverthe
eventschange, ascontextualinvolvesbackgroundchangesthatoccupymostoftheframewhilein
viewpoint,asthecameraanglechanges,theentiretyofthevideoframechangesaswell.Ontheother
hand,interactionandcyclicalnotonlyrequireamodeltohavestronglogicalunderstandingofthe
connectionbetweenevents,butalsotheabilitytofocusonsmalltemporalchangesforthedifferent
actionsinvolved. Spatial,aspreviouslyhypothesized,alsoposesadifficultchallengeformodelsin
understandingchangesinobjectlocation. Overall,today‚Äôsmodelsaremuchbetteratunderstanding
coarse-levelinformationoverasetofframesintheirentiretythanunderstandingfine-graineddetails
from a part of each video frame. This also demonstrates how fine-grained comprehension is also
crucialfordensetemporalreasoning.
5 CONCLUSION
We introduced Vinoground, a novel temporal counterfactual benchmark encompassing 1000 short
andnaturalvideo-captionpairs.Wedemonstratedthatexistingvideounderstandingmodelsarequite
incapable in terms of temporal reasoning, even for short (<10 seconds) videos. While an average
humancaneasilyandaccuratelycompleteourbenchmark,thebestmodel,GPT-4o,performsmuch
worse, and most models barely perform better than random chance. Our work demonstrates that
there is much more to do still in the area of short video comprehension. We believe our bench-
mark can serve as an important checkpoint in evaluating a model‚Äôs true performance for temporal
understandingofdifferentactions,backgroundtransitions,andobjecttransformations.
10Preprint. Underreview.
LIMITATIONS
Onecannotfullyanalyzethebehaviorofproprietarymodelsincludedinthispaperduetothelack
ofaccesstothesemodels,whichareGPT-4o,Gemini-1.5-ProandClaude3.5Sonnet.
ACKNOWLEDGMENTS
This work was supported in part by NSF IIS2404180, Institute of Information & communica-
tionsTechnologyPlanning&Evaluation(IITP)grantsfundedbytheKoreagovernment(MSIT)(No.
2022-0-00871,DevelopmentofAIAutonomyandKnowledgeEnhancementforAIAgentCollab-
oration)and(No. RS2022-00187238,DevelopmentofLargeKoreanLanguageModelTechnology
forEfficientPre-training),andMicrosoftAccelerateFoundationModelsResearchProgram.
REFERENCES
Anthropic. Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/
news/claude-3-5-sonnet.
Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu,
Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-
temporal modeling and audio understanding in video-llms. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing (EMNLP), 2024. URL https:
//arxiv.org/abs/2406.07476.
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-
Emmanuel Mazare¬¥, Maria Lomeli, Lucas Hosseini, and Herve¬¥ Je¬¥gou. The faiss library. arXiv
preprint: 2401.08281,2024.
GeminiTeam. Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcon-
text,2024. URLhttps://arxiv.org/abs/2403.05530.
RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,Armand
Joulin,andIshanMisra.Imagebind:Oneembeddingspacetobindthemall.InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pp.15180‚Äì15190,
June2023.
YashGoyal,ZiyanWu,JanErnst,DhruvBatra,DeviParikh,andStefanLee. Counterfactualvisual
explanations. InKamalikaChaudhuriandRuslanSalakhutdinov(eds.),Proceedingsofthe36th
InternationalConferenceonMachineLearning,volume97ofProceedingsofMachineLearning
Research, pp. 2376‚Äì2384. PMLR, 09‚Äì15 Jun 2019. URL https://proceedings.mlr.
press/v97/goyal19a.html.
Hangzhi Guo, Thanh Hong Nguyen, and Amulya Yadav. Counternet: End-to-end training of pre-
diction aware counterfactual explanations. In Proceedings of the 29th SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD), 2023. URL https://arxiv.org/abs/
2109.07557.
BoHe,HengduoLi,YoungKyunJang,MenglinJia,XuefeiCao,AshishShah,AbhinavShrivastava,
and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video
understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition(CVPR),2024.
Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, and Zeynep Akata. Grounding visual expla-
nations. In Proceedings of the European Conference on Computer Vision (ECCV), September
2018.
TiepLe,VasudevLal,andPhillipHoward. Coco-counterfactuals: Automaticallyconstructedcoun-
terfactual examples for image-text pairs. Advances in neural information processing systems
(NeurIPS),2023.
11Preprint. Underreview.
JieLei,TamaraBerg,andMohitBansal. Revealingsingleframebiasforvideo-and-languagelearn-
ing. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers), pp.
487‚Äì507,Toronto,Canada,July2023.AssociationforComputationalLinguistics.doi:10.18653/
v1/2023.acl-long.29. URLhttps://aclanthology.org/2023.acl-long.29.
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei
Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint:
2408.03326,2024a.
Shicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, and Lu Hou.
Vitatecs: Adiagnosticdatasetfortemporalconceptunderstandingofvideo-languagemodels. In
ProceedingsofTheEuropeanConferenceonComputerVision(ECCV),2024b.
BinLin,YangYe,BinZhu,JiaxiCui,MunanNing,PengJin,andLiYuan. Video-llava: Learning
unitedvisualrepresentationbyalignmentbeforeprojection. InProceedingsoftheConferenceon
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),2024.
XudongLin,FabioPetroni,GedasBertasius,MarcusRohrbach,Shih-FuChang,andLorenzoTor-
resani. Learningtorecognizeproceduralactivitieswithdistantsupervision. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pp.13853‚Äì13863,
June2022.
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee.
Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://
llava-vl.github.io/blog/2024-01-30-llava-next/.
Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun,
and Lu Hou. TempCompass: Do video LLMs really understand videos? In Lun-Wei Ku,
Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Lin-
guistics ACL 2024, pp. 8731‚Äì8772, Bangkok, Thailand and virtual meeting, August 2024b.
Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.517. URL
https://aclanthology.org/2024.findings-acl.517.
KarttikeyaMangalam,RaiymbekAkshulakov,andJitendraMalik.Egoschema:Adiagnosticbench-
markforverylong-formvideolanguageunderstanding. Advancesinneuralinformationprocess-
ingsystems(NeurIPS),2023.
Microsoft. Discover the new multi-lingual, high-quality phi-3.5 slms, 2024. URL https:
//techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/
discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/
4225280.
Stephen L Morgan and Christopher Winship. Counterfactuals and causal inference. Cambridge
UniversityPress,2015.
OpenAI. Hellogpt-4o,2024a. URLhttps://openai.com/index/hello-gpt-4o/.
OpenAI. Gpt-4technicalreport. arXivpreprint: 2303.08774,2024b.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision, 2021. URL
https://arxiv.org/abs/2103.00020.
Darshana Saravanan, Darshan Singh, Varun Gupta, Zeeshan Khan, Vineet Gandhi, and Makarand
Tapaswi. Velociti: Can video-language models bind semantic concepts through time? arXiv
preprint: 2406.10889,2024.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted
pre-training for language understanding. Advances in neural information processing systems
(NeurIPS),2020. URLhttps://arxiv.org/abs/2004.09297.
12Preprint. Underreview.
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and
Candace Ross. Winoground: Probing vision and language models for visio-linguistic composi-
tionality. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion(CVPR),pp.5238‚Äì5248,June2022.
Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and Chirag
Shah. Counterfactual explanations and algorithmic recourses for machine learning: A review.
Advancesinneuralinformationprocessingsystems(NeurIPS),2020. URLhttps://arxiv.
org/abs/2010.10596.
PengWang, ShuaiBai, SinanTan, ShijieWang, ZhihaoFan, JinzeBai, KeqinChen, XuejingLiu,
JialinWang,WenbinGe,YangFan,KaiDang,MengfeiDu,XuanchengRen,RuiMen,Dayiheng
Liu,ChangZhou,JingrenZhou,andJunyangLin.Qwen2-vl:Enhancingvision-languagemodel‚Äôs
perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409.
12191.
XinWang,JiaweiWu,JunkunChen,LeiLi,Yuan-FangWang,andWilliamYangWang. Vatex: A
large-scale,high-qualitymultilingualdatasetforvideo-and-languageresearch. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision(ICCV),October2019.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
neuralinformationprocessingsystems(NeurIPS),35:24824‚Äì24837,2022.
WenhaoWu.Freeva:Offlinemllmastraining-freevideoassistant.arXivpreprintarXiv:2405.07798,
2024.
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.
Video question answering via gradually refined attention over appearance and motion. In ACM
Multimedia,2017.
HuXu,GargiGhosh,Po-YaoHuang,DmytroOkhonko,ArmenAghajanyan,FlorianMetze,Luke
Zettlemoyer,andChristophFeichtenhofer. VideoCLIP:Contrastivepre-trainingfor
zero-shotvideo-textunderstanding.InProceedingsofthe2021ConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP),Online, November2021.AssociationforComputa-
tionalLinguistics.
YuanYao, TianyuYu, AoZhang, ChongyiWang, JunboCui, HongjiZhu, TianchiCai, HaoyuLi,
WeilinZhao, ZhihuiHe, etal. Minicpm-v: Agpt-4vlevelmllmonyourphone. arXivpreprint
arXiv:2408.01800,2024.
Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I. Inouye, and Pradeep Ravikumar.
On the (in)fidelity and sensitivity for explanations. Advances in neural information processing
systems(NeurIPS),2019. URLhttps://arxiv.org/abs/1901.09392.
ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,YuetingZhuang,andDachengTao. Activitynet-
qa: Adatasetforunderstandingcomplexwebvideosviaquestionanswering. InProceedingsof
theAssociationfortheAdvancementofArtificialIntelligence(AAAI),pp.9127‚Äì9134,2019.
Jianrui Zhang, Mu Cai, Tengyang Xie, and Yong Jae Lee. CounterCurate: Enhancing physical
and semantic visio-linguistic compositional reasoning via counterfactual examples. In Lun-
Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computa-
tionalLinguisticsACL2024,pp.15481‚Äì15495,Bangkok,Thailandandvirtualmeeting,August
2024a.AssociationforComputationalLinguistics.doi:10.18653/v1/2024.findings-acl.915.URL
https://aclanthology.org/2024.findings-acl.915.
PanZhang,XiaoyiDong,YuhangZang,YuhangCao,RuiQian,LinChen,QipengGuo,Haodong
Duan, BinWang, LinkeOuyang, SongyangZhang, WenweiZhang, YiningLi, YangGao, Peng
Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng
Zhang,KaiChen,JifengDai,YuQiao,DahuaLin,andJiaqiWang. Internlm-xcomposer-2.5: A
versatilelargevisionlanguagemodelsupportinglong-contextualinputandoutput.arXivpreprint:
2407.03320,2024b.
13Preprint. Underreview.
Yu Zhang, Peter TinÀáo, AlesÀá Leonardis, and Ke Tang. A survey on neural network interpretability.
IEEETransactionsonEmergingTopicsinComputationalIntelligence,5(5):726‚Äì742,2021.
LuoweiZhou, ChenliangXu, andJasonJ.Corso. Towardsautomaticlearningofproceduresfrom
web instructional videos. In Proceedings of the Association for the Advancement of Artificial
Intelligence(AAAI),2018. URLhttps://arxiv.org/abs/1703.09788.
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang,
JunwuZhang, ZongweiLi, CaiWanZhang, ZhifengLi, WeiLiu, andLiYuan. Languagebind:
Extending video-language pretraining to n-modality by language-based semantic alignment. In
ProceedingsoftheInternationalConferenceonLearningRepresentations(ICLR),2024.
14Preprint. Underreview.
APPENDIX
A DATA CURATION PROCESS
WeincludeanoverallillustrationofthedatacurationprocessinFigure4.
Generate xxx pairs of
counterfactuals‚Ä¶‚Ä¶
Sentence features FAISS Top 20 results
Transformer Indexing
VATEX Captions
VATEX or YouTube Videos
Figure4: Thedatacurationprocess.
B METRICS ILLUSTRATION
WevisualizeourtextandvideoscoremetricsinFigure5. Thisshowsthe4possiblequestionsthat
canbederivedfromonecounterfactualdatapointinthedataset.
ùëâ
!
ùëâ !"
Which one Which one
matches? matches?
ùê∂ !(correct caption) ùê∂ !"(incorrect caption) ùê∂ ! (incorrect caption) ùê∂ !"(correct caption)
ùê∂ ùê∂"
! !
Which one Which one
matches? matches?
ùëâ ! ùëâ !" ùëâ ! ùëâ !"
(correct video) (incorrect video) (incorrect video) (correct video)
Figure5: Visualizationofthetextandvideoscoremetrics.
15Preprint. Underreview.
C RANDOM CHANCE PERFORMANCE
Wesettherandomchanceperformancefortext,video,andgroupscoreas25%,25%,and16.67%.
It is intuitive to understand the setup for both text and video score since there are two questions
in the same counterfactual pair for each metric, and the probability of guessing correctly is 50%
each. For the counterfactual pair (C ,C‚Ä≤,V ,V‚Ä≤), a model can only produce six possible permu-
i i i i
tationsofvideo-captionmatchings: {(C ,V ),(C‚Ä≤,V‚Ä≤)},{(C ,V ),(C ,V‚Ä≤)},{(C ,V ),(C‚Ä≤,V )},
i i i i i i i i i i i i
{(C ,V‚Ä≤),(C‚Ä≤,V‚Ä≤)},{(C‚Ä≤,V ),(C‚Ä≤,V‚Ä≤)},and{(C‚Ä≤,V ),(C ,V‚Ä≤)}. Thisiswhytherandomchance
i i i i i i i i i i i i
performanceforgroupscoreis1/6=16.67%.
D SURVEY INTERFACE
We first upload all the videos to Google Drive and embed them into our surveys using Qualtrics.
The2000questionsfromVinogroundaresplitinto50surveys,witheachsurveyhaving40random
questions. We then distribute our surveys on Prolific where we pay everyone who completed a
survey$2,or$0.05perquestion. TheinterfaceisillustratedinFigure6.
Figure6: TheQualtricssurveythatProlificworkerssee.
16Preprint. Underreview.
E FULL CATEGORICAL RESULTS
Hereweincludetheselectedtop-6strongestmodelsweevaluatedandreporttheirresultsbycategory
inTables4and5. WealsoincludethetextscoreandvideoscorebarplotsinFigures7and8. We
canseethatthegeneraltrendisthesameasreportedinSection4.4.2,wheremodelsperformmuch
betteroncontextualandviewpoint,andworseonothercategories.
GPT-4o Gemini-1.5-Pro Claude3.5Sonnet
category text video group text video group text video group
all 54.00 38.20 24.60 35.80 22.60 10.20 32.80 28.80 10.60
object 52.50 35.62 20.62 36.25 25.62 12.50 30.00 25.00 7.50
action 47.47 35.41 20.23 30.74 22.18 8.56 27.63 28.79 9.34
viewpoint 77.11 51.81 45.78 50.60 18.07 10.84 54.22 36.14 20.48
interaction 50.68 42.47 21.92 30.14 27.40 10.96 20.55 21.92 5.48
cyclical 39.64 41.44 18.92 22.52 19.82 4.50 27.03 25.23 7.21
spatial 47.57 30.10 17.48 37.86 24.27 9.71 31.07 20.39 5.83
contextual 53.97 49.21 33.33 38.10 31.75 11.11 52.38 28.57 15.87
Table 4: The best performances of proprietary models grouped by category. Significantly high
performancesarehighlightedinblue,whilesignificantlylowperformancesarehighlightedinred.
LLaVA-OneVision-72B Qwen2-VL-72B InternLM-XC-2.5
category text video group text video group text video group
all 48.40 35.20 21.80 50.40 32.60 17.40 28.80 27.80 9.60
object 42.50 33.75 17.50 46.88 33.75 18.12 28.75 28.12 12.50
action 42.80 31.91 17.90 44.75 28.79 12.06 25.68 29.96 8.56
viewpoint 77.11 48.19 42.17 74.70 42.17 32.53 38.55 20.48 7.23
interaction 36.99 36.99 16.44 34.25 31.51 6.85 23.29 36.99 6.85
cyclical 36.04 29.73 14.41 36.94 32.43 11.71 18.92 36.04 7.21
spatial 37.86 25.24 10.68 53.40 31.07 17.48 23.30 29.13 8.74
contextual 57.14 31.75 20.63 49.21 39.68 22.22 26.98 26.98 11.11
Table5: Thebestperformancesofselectedopen-sourcemodelsgroupedbycategory. Significantly
highperformancesarehighlightedinblue,whilesignificantlylowperformancesarehighlightedin
red.
17Preprint. Underreview.
Figure7: Textscorebarplotbasedoncategorygroupedbymodel.
Figure8: Videoscorebarplotbasedoncategorygroupedbymodel.
F FULL RESULTS ON EVALUATED MODELS
Duetotheextensivenumberofmodelsevaluatedanddifferentnumberofsamplesusedashyperpa-
rameters, we include the full results of our evaluation that are not mentioned in the main paper in
Table6.
18Preprint. Underreview.
Model Frames Text Video Group
Claude-3.5-Sonnet 16 30.00 22.60 8.40
8 32.20 25.40 9.40
4 32.80 28.80 10.60
2 29.40 24.00 8.40
1 26.20 30.00 10.80
Qwen2-VL-72B 32 50.40 32.60 17.40
8 37.40 23.00 7.80
4 26.20 23.80 6.20
2 15.60 24.40 4.00
Qwen2-VL-7B 32 40.00 26.40 11.80
16 36.80 25.80 10.20
8 27.60 23.40 7.80
4 22.20 22.80 5.60
2 21.40 25.60 5.20
4fps 40.20 32.40 15.20
2fps 34.80 27.40 10.60
1fps 26.80 26.60 7.60
0.5fps 23.20 19.60 4.80
MiniCPM-2.6 32 28.40 27.00 9.40
16 32.60 29.20 11.20
8 33.40 25.60 9.00
4 25.80 27.40 8.60
2 22.80 23.20 4.60
1 27.00 27.00 8.00
LLaVA-NeXT-Video-34B(CoT) 32 25.80 22.20 5.20
LLaVA-NeXT-Video-34B 32 23.00 21.20 3.80
16 21.00 21.80 4.40
8 21.20 22.00 5.20
4 16.60 21.60 3.40
2 15.40 21.60 2.20
1 13.20 21.80 2.00
LLaVA-NeXT-Video-7B(CoT) 32 21.80 26.20 6.80
LLaVA-NeXT-Video-7B 32 21.80 25.60 6.20
16 22.20 25.60 6.40
8 21.80 25.60 6.40
4 21.80 25.60 6.40
2 21.20 25.40 6.00
1 22.40 25.60 6.40
Phi-3.5-Vision 32 22.00 21.20 4.80
16 24.00 22.40 6.20
8 21.80 21.20 5.00
4 21.20 22.80 5.60
2 20.40 21.60 3.80
1 22.60 22.80 3.80
MA-LMM-Vicuna-7B 32 22.40 25.60 6.80
16 22.00 26.00 6.00
8 23.00 26.00 6.40
4 23.80 25.60 6.80
2 23.80 25.60 6.80
Table 6: The full evaluation results based on model type, frames sampled, and the metrics afore-
mentioned. Onlythemodelsettingsthatarenotmentionedinthemainpaperarelistedhere. Perfor-
mancessignificantlybetterthanrandomchancearebolded.
19