CHOICES ARE MORE IMPORTANT THAN EFFORTS: LLM
ENABLES EFFICIENT MULTI-AGENT EXPLORATION
YunQu,BoyuanWang,YuhangJiang,JianzhunShao,YixiuMao,
CheemsWang,ChangLiu,XiangyangJi
DepartmentofAutomation,TsinghuaUniversity,Beijing,China
{qy22, wangby22, jiangyh19, sjz18, myx21}@mails.tsinghua.edu.cn
cheemswang@mail.tsinghua.edu.cn,{liuchang2022, xyji}@tsinghua.edu.cn
ABSTRACT
With expansive state-action spaces, efficient multi-agent exploration remains a
longstanding challenge in reinforcement learning. Although pursuing novelty,
diversity,oruncertaintyattractsincreasingattention,redundanteffortsbroughtby
explorationwithoutproperguidancechoicesposesapracticalissueforthecommu-
nity. Thispaperintroducesasystematicapproach,termedLEMAE,choosingto
channelinformativetask-relevantguidancefromaknowledgeableLargeLanguage
Model (LLM) for Efficient Multi-Agent Exploration. Specifically, we ground
linguisticknowledgefromLLMintosymbolickeystates,thatarecriticalfortask
fulfillment,inadiscriminativemanneratlowLLMinferencecosts. Tounleashthe
powerofkeystates,wedesignSubspace-basedHindsightIntrinsicReward(SHIR)
toguideagentstowardkeystatesbyincreasingrewarddensity. Additionally,we
buildtheKeyStateMemoryTree(KSMT)totracktransitionsbetweenkeystates
inaspecifictaskfororganizedexploration. Benefitingfromdiminishingredundant
explorations,LEMAEoutperformsexistingSOTAapproachesonthechallenging
benchmarks(e.g.,SMACandMPE)byalargemargin,achievinga10xacceleration
incertainscenarios.
1 INTRODUCTION
Explorationstandsasafundamentalissueinreinforcementlearning(RL)(Duetal.,2023;Liuetal.,
2023). Researchershavedevelopedseveralexplorationstrategiesdirectedbynovelty,diversity,or
uncertainty (Linke et al., 2020; Burda et al., 2018b; Pathak et al., 2017), mainly in single-agent
reinforcementlearning. However,thesemethodsmayinducetask-irrelevantredundantexploration,
especiallyincomplexenvironments(Duetal.,2023). IntherealmofMulti-AgentReinforcement
Learning(MARL),theneedtomitigateexplorationredundancybecomesevenmoreurgentduetothe
challengeslikeexponentialexpansionofthestate-actionspaces. Widespreadreal-worldapplications,
includingMOBAgames(Quetal.,2023), socialscience(Jaquesetal.,2019), andmulti-vehicle
control(Xuetal.,2018),furtherunderscorethegrowingneedforefficientmulti-agentexploration.
Thisworkidentifiestask-relevantguidanceasanimportantconsiderationinenhancingexploration
efficiency. Incorporatingpriorsinexplorationmechanismdesign,suchascomplexrewardstructures,
typicallyrequiresexpertknowledgeandsubstantialhumanefforts(Liuetal.,2023;Abbeel&Ng,
2004). Hopefully,recentadvanceshavewitnessedtheremarkablereasoningandplanningcapabilities
ofLargeLanguageModels(Touvronetal.,2023;Achiametal.,2023),providingaplausiblechoice
tofacilitateefficientexplorationthroughLLMâ€™seffortlesspriorprovision. However,itisnon-trivial
toeffectivelycompriselinguisticLLMpriorsintosymbolicallyrepresentedRLtasks(Pengetal.,
2023;Cartaetal.,2023),andtheinvestigationofpracticalwaystoavoidnuisancescausedbysuch
anexpressiondiscrepancyisofcriticalimportance.
Inresponsetotheaboveissue,weproposeLEMAE,anovelframeworktoenableefficientmulti-
agentexplorationwithLLM.Theframeworkprimarilyconsistsoftwocomponents: (i)keystates
localizationwithLLMand(ii)keystate-guidedexploration. Thefirstcomponentgroundslinguistic
knowledge from LLM into symbolic key states by automatically localizing key states that are
essential for task fulfillment. Specifically, the discriminator function induced by LLM works to
1
4202
tcO
3
]IA.sc[
1v11520.0142:viXraFigure1: (a)ThemapofthetaskPass. Twoagentsareinitiallypositionedintheleftroom,requiring
cooperationtoexploretherooms, uncoverthehiddenswitches, andmovetotherightroom. (b)
Thekeystates(Îº andÎº )generatedbyLLMforthetaskPass,wherethesuperscriptsA,B ofÎº
1 2 i
denotetwoagentsAliceandBob. (c)VisitationMap(logscale)ofSOTAbaselinemethodCMAE.
(d)VisitationMap(logscale)ofourmethodLEMAE.Ourmethodexhibitsasignificantreductionin
redundantexploration. Furthermore,anorganicdivisionoflaboramongagentsemerges.
discriminatekeystatesfromrollouttrajectories,avoidingtheoverburdenofLLMinferencecosts.
Thesecondcomponentharnessesthelocalizedkeystatesasmeaningfulguidancetoachieveefficient
exploration.Inimplementation,weencourageagentstowardtargetedkeystatesbydevisingSubspace-
basedHindsightIntrinsicReward(SHIR)toincreaserewarddensity. Forthepurposeoforganized
exploration,KeyStatesMemoryTree(KSMT)isfurtherconstructedtotrackkeystatetransitions,
mitigatingexplorationcomplexityandenhancingguidanceinSHIR.AsillustratedinFig.1, our
design empowers LEMAE with a significant performance advantage through notably reducing
redundantexploration.
Ourmaincontributionsaresummarizedasfollows:
1. WebuildabridgebetweenLLMandRLtofacilitateefficientmulti-agentexplorationby
developingasystematicapproachdubbedLEMAE.
2. Wedeviseacomputationallyefficientinferencestrategychannelingtask-specificinformation
from LLM to distinguish key states critical for task fulfillment as subgoals for targeted
exploration.
3. WeintroduceaKeyStateMemoryTreetoorganizeexplorationaccordingtohistorickey
statetransitionsanddevisetheSubspace-basedHindsightIntrinsicReward,encouraging
agentsâ€™guidance.
We conduct extensive experiments on typical multi-agent exploration benchmarks. LEMAE (i)
consistently outperforms the state-of-the-art (SOTA) baselines with 10x acceleration in certain
scenarios,(ii)achievesperformancecomparabletothebaselinetrainedwithhuman-designeddense
rewards in sparse reward scenarios, and (iii) exhibits potential to generalize to brand-new, non-
symbolictasks. Theseobservationsvalidatetheeffectivenessofourdesigninreducingredundant
explorationandimprovingexplorationefficiency,showingpromiseforreal-worlddeploymentin
scenariosrequiringefficientexploration.
2 PRELIMINARY
Theenvironmentsconsideredinthisworkarecharacterizedasadecentralizedpartiallyobservable
Markovdecisionprocess(Dec-POMDP)(Oliehoeketal.,2016)withnagents,whichcanbedefined
asatupleG=âŸ¨S,A,I,P,r,Z,O,n,Î³âŸ©,wheresâˆˆS istheglobalstate,Aistheactionspacefor
eachagent,andÎ³ âˆˆ[0,1)isthediscountfactor. Attimestept,eachagentiâˆˆI â‰¡{1,...,n}hasits
localobservationsoi âˆˆ OdrawnfromtheobservationfunctionZ(s,i) : SÃ—I â†’ Oandchooses
anactionai âˆˆ AbyitspolicyÏ€i(ai|oi) : O â†’ âˆ†([0,1]|A|),formingajointactiona âˆˆ A â‰¡ An.
T(sâ€²|s,a):SÃ—AÃ—S â†’[0,1]istheenvironmentâ€™sstatetransitiondistribution. Allagentssharea
commonrewardfunctionr(s,a) : SÃ—A â†’ R. Theagentsâ€™jointpolicyÏ€ := (cid:81)n Ï€i inducesa
i=1
jointaction-valuefunction: QÏ€(s,a)=E[R|s,a],whereR=(cid:80)âˆž Î³tr istheexpecteddiscounted
t=0 t
return. ThegoalofMARListofindtheoptimaljointpolicyÏ€âˆ— suchthatQÏ€âˆ—(s,a) â‰¥ QÏ€(s,a),
âˆ€Ï€and(s,a)âˆˆSÃ—A. Notably,wespecificallyfocusonsparserewardtasks,i.e.,r =1onlywhen
t
s =s ,otherwiser =0. Wedenotethesymbolforthei-thkeystatebyÎº togetherwith
t+1 success t i
itsdiscriminatorfunctionF .
i
23 RELATED WORKS
LLMinDecisionMaking. LargeLanguageModelshaveshowcasedimpressivecapabilitiesacross
variousdownstreamtasks(Touvronetal.,2023;Radfordetal.,2019;Brownetal.,2020). Recent
advancesindicateagrowingtrendofusingLLMindecision-makingproblems(Wangetal.,2023b).
A primary challenge within this domain is grounding LLMâ€™s linguistic knowledge into specific
low-levelcontroltaskstypicallyrepresentedinsymbolicform(Pengetal.,2023;Cartaetal.,2023),
especiallyinRL.Creatinglinguistictwintasks(Cartaetal.,2023)areintuitivebutrequiresubstantial
manualworkloads. SomeworksemployLLMsashigh-levelplanners,e.g.,codingwithAPIs(Liang
etal.,2023),usinghuman-annotatedorLLM-summarizedactiontemplate(Yaoetal.,2022;Shinn
etal.,2023;Linetal.,2023;Zhuetal.,2023;Wangetal.,2023a). Despitesignificantprogress,they
relyondifficult-to-obtainlow-levelpoliciesorAPIs,limitingtheirreal-worldapplicability. Recently,
LLMshavebeenintegratedwithRLtodirectlyenhancelow-leveldecisionmaking(Caoetal.,2024).
LLMscanactasenvironmentalinformationprocessors,reducinglearningcomplexity(Paischeretal.,
2022;2024;Kimetal.,2024;Wangetal.,2024),butcannotdirectlyfacilitateefficientexploration.
SomeworksutilizeLLMsasgoalselectorsingoal-conditionedRL(Su&Zhang,2023;?) orteacher
policy(Zhouetal.,2023)butrequirepredefinedskillsorsubgoals. AlternativemethodslikeLLM-
basedrewardorpolicydesign(Maetal.,2023;Kwonetal.,2023;Songetal.,2023;Liuetal.,2024;
Chenetal.,2024)andfine-tuning(Cartaetal.,2023;Shietal.,2023)areeitherlimitedtosimple
taskswithsufficientinformationordemandenormousdataandresources. ELLM(Duetal.,2023)
aimstoenhanceexplorationusingLLMbutdependsonpredefinedsymbolicobservationcaptioner
andfrequentLLMinferences. Itssemanticsimilarity-basedrewardsmayalsostruggletogeneralize
acrossdiversescenarios. Incontrast,LEMAEintegrateslinguisticLLMpriorsintosymbolicstates
withminimaltask-specificinformationandLLMinferencecosts,achievedbylocalizingkeystatesin
rollouttrajectoriesusingLLM-generateddiscriminatorfunctions.
Efficient Multi-Agent Exploration. Exploration efficiency has long been a focal point in
RL (Thrun, 1992; Cai et al., 2020; Seo et al., 2021; Mahajan et al., 2019; Jeon et al., 2022; Liu
etal.,2021b). Typicalexplorationmethodsfocusonrandomexploration(Mnihetal.,2013;Rashid
etal.,2018)orheuristicindicators,suchasdiversityornovelty,tofacilitateexhaustiveexploration,
particularlyinsingleagentexploration(Linkeetal.,2020;Burdaetal.,2018b;Pathaketal.,2017;
Burdaetal.,2018a;Bellemareetal.,2016). Despitetheirsuccess,theymayinducenotableredundant
explorationduetoalackoftask-relevantguidance(Duetal.,2023). Theexponentialexpansionofthe
state-actionspacesrendersexhaustiveexplorationimpracticalinmulti-agentsettings. Consequently,
efficientmulti-agentexploration(MAE)becomesincreasinglyimperativeandnecessary(Jeonetal.,
2022; Liu et al., 2021b). MAE is also challenging due to the complex configurations, e.g., the
entangledeffectofmulti-agentactionsandintricaterewarddesign(Liuetal.,2023;Quetal.,2023;
Xuetal.,2023). Givenouremphasisonefficientexploration,weprioritizeevaluationinmulti-agent
settings. SomeMAEmethodsencourageinfluentialbehaviorsduringagentinteractions(Liuetal.,
2023;Jaquesetal.,2019;Wangetal.,2019a). Nevertheless,theymayleadtounintendedcoalitions
or require additional priors (Liu et al., 2023). Certain studies leverage subgoals to guide explo-
ration(Jeonetal.,2022). However,duetochallengesinintegratingtask-relatedinformationinto
subgoals,theyeithernecessitatehumanexpertiseforsubgoalsdesign(Tangetal.,2018;Kulkarni
etal.,2016)orstruggletoidentifyusefulsubgoals(Jeonetal.,2022;Liuetal.,2021b). Distinguished
fromtheabove,thisworkunderscoresthesignificanceoftask-relevantguidanceinexplorationand
utilizesthekeystatepriorsextractedfromLLMtoenableefficientmulti-agentexploration.
4 METHOD
ThissectionelaboratesonthedevelopedLEMAE.Theconceptofthekeystatesisfirstinducedas
thetask-relevantguidanceinSec.4.1. Centeringaroundthekeystates,weconstructtwocomponents:
(i)keystateslocalizationwithLLM(Sec.4.2)and(ii)keystate-guidedexploration(Sec.4.3). The
formerdirectsLLMtogeneratediscriminatorfunctionsforlocalizingkeystatesinrollouttrajectories,
while the latter guides and organizes exploration with the introduced Subspace-based Hindsight
IntrinsicRewardandKeyStatesMemoryTree. PleaserefertoFig.2andAlgorithm1fordetails.
Also,weprovideademonstration1toclaritytheLEMAEâ€™sexecutionpipeline.
1https://sites.google.com/view/lemae
3ðœ¿Chain Key States Memory Tree
Prompt Reponse
Role Instruction Thought: Â·Â·Â·ð’” ð’•ðœ¿ðŸ ð’”ð’•+ðŸÂ·Â·Â·ð’” ð’ðœ¿ðŸ ð’”ð’+ðŸÂ·Â·Â· ð’” ð’•ðœ¿ðŸ Look up ð’“ð’ð’ð’•
(Template): â€œagent0 finds the ðœ¿ðŸ Â·Â·Â·
T K Dh e iso y cu _ rg s imth at it: ne â€œ s aC :
t
o{o râ€œT
:
ðœ… {â€ 1 â€œ, ðœ…â€:
1
â€.. :. },
LLM
s K {w â€œe ðœ…i yt 1c _ â€h :s tt â€œao
a
t go e ep s ne :
t
n
0
â€¦ onâ€ t,
he Extract
ð’” ð’ðœ¿ðŸ Â·Â·Â· ðœ¿ðŸ Â·Â·Â·
â€˜def â€¦ return 0/1â€™} switchâ€¦â€,â€¦}, Iskeystate1( ) Subspace-based Â·Â·Â· ðœ¿ð’Ž
Task Description: Discriminators: Sample
â€œTwo agents â€¦â€ {â€œðœ…1â€: Hindsight Intrinsic Reward Next ðœ¿
State Form: defIskeystate1(s):
[ Sa eg le fn -Ct0 h.x e,
c
a kg :ent0.y, â€¦] â€œðœ…r 2e â€:t u â€¦rn }s[4]==1â€¦ Â·Â·Â· ð’”ð’• ð’”ð’•+ðŸÂ·Â·Â·ð’”ð’ ð’”ð’+ðŸÂ·Â·Â· ð’”ðŸŽÂ·Â·Â·ð’” ð’•ðœ¿ðŸ ð’”ð’•+ðŸÂ·Â·Â·ð’” ð’ðœ¿ðŸ ð’”ð’+ðŸÂ·Â·Â·ð’”ð‘» ðœ¿ð’Ž
Feedback Trajectory ð’“ðœ¿ðŸ ð’“ðœ¿ðŸ ð’“ðœ¿ð’Ž
ð‘° ð‘° ð‘°
Figure2: Overviewofthetrainingprocess. (a)KeyStatesLocalizationwithLLM:Wedeviseaset
ofpromptstoguideLLMinlocalizingkeystatesbasedontask-specificinformation. Refinements
of the response are achieved through iterative self-checks by LLM. Subsequently, discriminator
functionsarederivedfromthefinalresponsetodiscriminatekeystateswithintrajectories. (b)Key
States-GuidedExploration: Usingtheachievedkeystateschainwithintheprocessedtrajectory,we
lookupKSMTtogetthemostprobablenextkeystates. Bysamplingfromthemasthesubgoalfor
theconcludingsub-trajectory,weintegrateintrinsicrewardsintotheoveralltrajectoryusingSHIR.
4.1 DEVILISINTHEKEYSTATES
Previousmethodssufferfromredundantexplorationeffortsinpursuingtask-agnosticnovelty(Du
etal.,2023),potentiallyreducingtrainingefficiency. Thismotivatesustointegratetask-relevant
informationasabetterguidancechoiceforefficientexploration. Nevertheless,practicalproposals
arelimitedinthefield. ThisworkidentifiestheKeyStatesasthenoveltask-relevantprior,which
corresponds to intermediate states with explicit semantics and expressions pertaining to the task.
Meanwhile,Proposition4.1explicitlyreflectstheefficacyofincorporatingthem.
Proposition4.1. Considertheone-dimensionalasymmetricrandomwalkproblem,whereanagent
startsatx=0andaimstoreachx=N âˆˆN+,N >1. Theinitialpolicyisasymmetricandrandom
withprobabilitiespâˆˆ(0.5,1)and1âˆ’pforrightandleftmovements,respectively. Withoutprior
knowledge,theexpectedfirsthittingtimeisE(T )= N . Afterintroducingthetask-relevant
0â†’N 2pâˆ’1
informationthattheagentmustfirstreachkeystatesÎº=1,...,Nâˆ’1beforereachingx=N,wecan
decreasetheexpectedfirsthittingtimebyE(T )âˆ’E(Tprior)=(Nâˆ’1)âˆ—( 1 âˆ’ 2 +1)>0.
0â†’N 0â†’N 2pâˆ’1 p
TheproofisdeferredtoAppendixC.Theexplorationpolicysubstantiallybenefitsfromtheinvolve-
mentofkeystates,e.g.,E(T )âˆ’E(Tprior)â†’âˆžwithpâ†’0.5.Suchaconceptisalsocommonly
0â†’N 0â†’N
seeninpracticalscenarios,suchasin-gamecheckpoints(Demaineetal.,2016)andlandmarksin
navigation(Beckeretal.,1995).
4.2 KEYSTATESLOCALIZATIONWITHLLM
To reduce manual workload, we employ LLM to localize key states. Although generating the
aforementioned symbolic key states can be straightforward, LLMâ€™s weakness in comprehending
symbolicstatesorenvironmentdetailsnecessitatesadditionalinformationincertaintasksandcan
leadtoerrorsandhallucinationsthataredifficulttodetect. Here,westresstheimportanceofLLMâ€™s
discriminativeabilitytolocalizekeystatesinrollouttrajectoriestobetterleverageLLMâ€™sgeneral
knowledge. Therationaleisthatdiscriminationdemandsonlyahigh-leveltaskunderstandingandis
morereliableanduniversalthannaivegeneration,asdiscussedindetailinAppendixB.1.
To discriminate key states, we prompt LLM to generate m discriminator functions {F }m , as
i i=1
depictedinFig.2. EachdiscriminatorfunctionF takesinthestates attimesteptandoutputsa
i t
booleanvaluetotellwhethertheinputstateisthekeystateÎº . Suchanapproachsystematically
i
annotates each state in trajectories as a key state instance or not. Notably, LEMAE injects task-
relevantinformationintothesymbolicstateswithoutpredefinedcomponentssuchasobservation
captioners (Du et al., 2023) or environment codes (Xie et al., 2023), which require manual fine-
tuning,maybeunavailableinmanyscenarios,orcouldintroduceextrainformation. Inaddition,the
discriminatorfunctionsâ€™reusabilityavoidsfrequentcalls,andourmethodempiricallyrequiresfewer
4thanthreeLLMinferencesforaspecifictask. TheseadvantageshighlightthepotentialofLEMAEto
expandthescopeofapplicationscenarioswithfewerconstraintsandreducedcosts.
Wedesignpromptstoalleviatetheburdenoflabor-intensivepromptengineeringacrosstasks. As
illustratedinFig.2, eachtaskâ€™spromptisstructuredbyastandardizedprompttemplateandtask
information. Theprompttemplate,consistentacrosstasks,primarilycontainsseveralroleinstructions
toguideLLMinroleunderstandings(includingpromotinglabordivisionamongagentsinMARL)
andoutputconstraints. Foranewtaskwithsymbolicstatespace,theprompttemplaterequiresonly
essentialdetails,i.e.,thetaskdescriptionandthestateform,whichcanbeeasilyextractedfromthe
taskdocumentwithoutadditionalprocessing,makingitlessdemandingthanpreviousmethods(Ma
etal.,2023;Duetal.,2023). Anextensiontovision-basedtasksisdescribedinAppendixF.2.
Considering that LLM sometimes generates inaccurate responses and non-executable codes, we
developaSelf-CheckmechanismtoenableLLMâ€™sautonomousevaluationandresponseimprovement,
whichisinspiredbyrecentapproaches(Shinnetal.,2023;Dhuliawalaetal.,2023). Themechanism
comprisestwocheckingoperations: LLMrethinkingandcodeverification. Theformerprompts
LLMwithasetofqueriesforself-assessment,ensuringcompliancewithspecifiedcriteria. Thelatter
verifiestheexecutabilityofdiscriminatorfunctionswithactualstateinputs,providingfeedbackuntil
allfunctionsareexecutable.
WeuseGPT-4-turbofromOpenAIAPIandpromptdetailsareattachedinAppendixD.
4.3 KEYSTATE-GUIDEDEXPLORATION
4.3.1 SUBSPACE-BASEDHINDSIGHTINTRINSICREWARD
Withtheannotatedkeystates,trajectoriescannaturallybesegmentedintosub-trajectories. Drawing
inspirationfromAndrychowiczetal.(2017),weintegratehindsightintrinsicrewardsbyconceptualiz-
ingtheannotatedkeystatesassub-trajectoriesâ€™subgoals,whichisfurtherdiscussedinAppendixE.3.
Suchintegrationguidesthepolicytowardachievingthesekeystatesbyincreasingrewarddensity,
thusreducingmanualrewarddesignburdens. Moreover,thestatevectorindexfromthediscriminator
functionconstitutesthereward-relatedsubspaceofthestate(Liuetal.,2021b). Here,wewritethe
Subspace-basedHindsightIntrinsicReward(SHIR)functionas:
rÎºm(t)=âˆ¥Î¦ (s )âˆ’Î¦ (Îº )âˆ¥âˆ’âˆ¥Î¦ (s )âˆ’Î¦ (Îº )âˆ¥, (1)
I m t m m m t+1 m m
whereâˆ¥Â·âˆ¥denotesadistancemetric,e.g.,ManhattanDistance;Î¦ (s)=(s ) restrictsthestate
spacetoelementseâˆˆÏ… ,s
isthee-thelementofthefull-statesm
,andÏ…
âŠ‚e Neâˆˆ +Ï…m
referstothesubset
m e m
ofentirestatespacefromthediscriminatorfunctionF .
m
Given that rewards generally rely on a limited subset of the entire state space (Liu et al., 2021b;
Todorovetal.,2012),adoptingsubspace-basedrewardshelpsavoidthepotentialredundancyandbias
associatedwiththedesignofintrinsicrewardsintheentirestatespace. LEMAEisalsoapplicableto
scenarioswhererewardsdependontheglobalstatespace,asitimposesnostrictconstraints. Hence,
thefinalrewardfunctionisfurtherderivedas:
r(t)=Î±Â·r (t)+Î²Â·rÎºm(t), (2)
E I
wherer denotestheextrinsicrewardwithÎ±,Î² âˆˆR+non-negativescalingfactors.
E
4.3.2 KEYSTATESMEMORYTREE
Toorganizeexplorationwithmemory,weintroducetheconceptofKeyStatesMemoryTree(KSMT).
It tracks transitions between key states and further serves exploration and planning. Compared
withthenaiveÏµ-greedymethod,graduallyrevealingtheKSMThelpsavoidredundantexploration
throughoutthestatespace,particularlybeneficialinmorecomplicatedreal-worldscenarios. Notably,
LEMAEiscompatiblewithothermemorystructures,suchasDirectedAcyclicGraphs.
ConstructKSMT:Initializedattherootnode,KSMTdynamicallyexpandsbyiterativelyincorporat-
ingkeystatechainsobtainedfromannotatedtrajectories,asoutlinedinAlgorithm2. Thesesteps
repeatuntileitherreachingthesuccessstateorfullydepictingthetransitionsbetweenkeystates.
Explore with KSMT: To discover new KSMT branches, we adopt an exploration strategy that
balances high-randomness policy Ï€Ïµh for exploring under-explored nodes with low-randomness
Î¸
5LEMAE(Ours) CMAE LAIES EITI EDTI HER Eureka-si ELLM
Push-Box Pass Secret-Room Large-Pass
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 2.0 2.5
T (mil) T (mil) T (mil) T (mil)
AAAAAcccccccccceeeeellllleeeeerrrrraaaaatttttiiiiiooooonnnnn RRRRRaaaaattttteeeee::::: 44444.....66666 AAAAAcccccccccceeeeellllleeeeerrrrraaaaatttttiiiiiooooonnnnn RRRRRaaaaattttteeeee::::: 1111133333.....88888 AAAcccccceeellleeerrraaatttiiiooonnn RRRaaattteee::: 666...111 AAAcccccceeellleeerrraaatttiiiooonnn RRRaaattteee::: >>>666...777
Figure3: EvaluatingLEMAEagainstbaselinemethodsonfourMPEmapswithsparserewards,
usingtestwinrateastheevaluationmetric. TheaccelerationratereferstohowmuchfasterLEMAE
findsthesuccessstatecomparedtoCMAE.
policyÏ€Ïµl tominimizeinterferencewithpolicylearning,asshowninAlgorithm2. Uponreachinga
Î¸
leafnode,agentsexecuteÏ€Ïµh todeepenKSMT.Whilereachinganon-leafnodeÎ¾ ,theagentstake
Î¸ i
Ï€Ïµh withprobabilityp toexpandthebreadthorÏ€Ïµl withprobability1âˆ’p forprogressiontowards
Î¸ i Î¸ i
thenextkeystate. Theprobabilityp iscalculatedasp = 1 ,withd thedegreeofthenodeÎ¾ as
i i di+1 i i
anindicatorofthedegreeofunder-exploration. Theexplorationphasecompletesuponthediscovery
ofthesuccessstate. Wealsoprunebranchesthatdonotleadtosuccesstocircumventtask-irrelevant
keystates. Inthisway,KSMTenablesexplorationinamoremeaningfulstatesubspace.
PlanwithKSMT:SinceKSMTactsasadynamicmodelwithinthekeystatespace,weplanthe
subgoalforthefinalsub-trajectorybasedonit. AsshowninFig.2b,giventheachievedkeystates
chain, we identify the corresponding branch (Îº â†’ Îº â†’ children) in KSMT through a lookup
2 1
operation. Sincetheyhavebeenvalidatedbymemory,thechildrenrepresentthemostlikelynextkey
states,fromwhichwerandomlysamplethefinalsubgoal. Thisprocessmainlyhandlescaseswhere
trajectoriesfailtoreachakeystateasthefinalsubgoal. ItenhancesSHIRandimprovestheefficacy
ofexploringKSMTbyencouragingagentstoaccessexistingkeystates.
5 EXPERIMENTS
Weconductexperimentsoncommonlyusedmulti-agentexplorationbenchmarks: (1)theMultiple-
Particle Environment (Lowe et al., 2017; Wang et al., 2019a) and (2) the StarCraft Multi-Agent
Challenge(Samvelyanetal.,2019b). Followingpreviousstudies(Maetal.,2023;Liuetal.,2021b;
Xuetal.,2023),wefocusprimarilyontaskswithsymbolicstatespacesandusethesparsereward
versionforalltaskswithoutspecificinstructions.
Baselines. WecompareLEMAEwithrepresentativebaselines: IPPOisaMARLalgorithmwhich
extends PPO (Schulman et al., 2017); QMIX (Rashid et al., 2018) is a widely adopted MARL
baseline; EITI and EDTI (Wang et al., 2019a) employ the impact of interaction in coordinated
agentsâ€™behaviors;MAVEN(Mahajanetal.,2019)combinevalue-basedandpolicy-basedapproaches
throughhierarchicalcontrol;CMAE(Liuetal.,2021b)learnscooperativeexplorationbyselecting
shared goals from multiple projected state space; RODE (Wang et al., 2020d) decomposes joint
actionspacesintorole-basedonestoenhanceexploration; MASER(Jeonetal.,2022)generates
subgoalsautomaticallyformultipleagentsfromtheexperiencereplaybuffer; LAIES(Liuetal.,
2023)addressesthelazyagentsproblembymathematicaldefinitionandcausalanalysis. ELLM(Du
et al., 2023) employs LLM priors to guide vision-based exploration, using state captioners and
semanticsimilarity-basedrewards. LEMAEisimplementedonIPPOinMPEandQMIXinSMAC,
consistentwithpreviousworks(Wangetal.,2019a;Liuetal.,2023;Jeonetal.,2022)toensurefair
comparisons.
Weruneachalgorithmonfiverandomseedsandreportthemeanperformancewithstandarddeviation.
FurtherdetailscanbereferencedinAppendixE.
5.1 MULTIPLE-PARTICLEENVIRONMENT(MPE)
InMPE,weevaluateLEMAEonPass,Secret-Room,Push-Box,andLarge-Pass,whicharecommonly
usedmulti-agentexplorationtasksinpreviousworks(Wangetal.,2019a;Liuetal.,2021b).
6
etaR
niW
tseT(a) (b) (c)
Figure4: (a)KeystatesdiscriminationfunctionsgeneratedontaskPass. (b)ThemapofSecret-Room
with key states: Îº represents occupying the left switch to open all doors, while Îº , Îº , and Îº
1 2 3 4
represent exploring right rooms 1, 2, and 3, respectively. The directional arrows symbolize the
transitionalrelationshipswithinKSMT.(c)ThekeystatesnumbercurveinSecret-Roomshowsthat
LEMAEcanidentifyallkeystatesandproficientlyprunetask-irrelevantones.
LLMcaneffectivelydiscriminatekeystates. Tostartwith,weexaminetheefficacyofLLMin
discriminatingkeystates. OnthePasstask,asshowninFig.1a,aroomisdividedbyawall,each
halfcontaininganinvisibleswitch. Passagethroughthedoorisallowedonlywhenanagentoccupies
aswitch. Initially,inthelefthalf-room,agentsmustcooperatetomovetotherighthalf-room. In
Fig.4a,LLMexhibitsapreciseunderstandingofthetaskandgeneratesmeaningfuldiscriminator
functions,demonstratingthefeasibilityofourapproachbasedonthecurrentLLM.
LEMAEachievessuperiorperformance. WeinvestigatehowLEMAEenhancesexplorationby
comparingitwithbaselines,confirmingthevalueofincorporatingLLMpriors. Thetrainingcurves
aredepictedinFig.3. Thefailureofcommonlyusedbaselineshighlightsthenecessityandurgency
forefficientexploration,whilethesuperiorperformanceofLEMAEunderscorestheeffectiveness
ofaugmentingRLwithtask-specificguidancefromLLM.Specifically,thefailureofEITI,EDTI,
and LAIES may be attributed to the complexity of learning dynamics or the scarcity of external
statechangesinthetasks. WhileCMAElearnseffectivestrategiesforsimpletasks,itsredundant
explorationhampersefficiency,renderingitinadequatefortaskswithexpansiveexplorationspaces,
suchasLarge-Pass. AlthoughitbenefitsfromLLMpriors,ELLMperformsworsethanLEMAE
duetotheweakguidanceprovidedbysemanticsimilarity-basedrewards,nottomentionitsreliance
onfrequentLLMinferenceandapredefinedstatecaptioner. Furthermore, wecompareLEMAE
withtraditionalSOTAbaselineCMAEusingthemetricofthenumberofexplorationstepstakento
findthesuccessstate. Theresultsindicateasignificantexplorationaccelerationrate,upto10x,
underscoringLEMAEâ€™sefficiency. Thesuperiorperformanceofourmethodcanbeattributedtothe
mitigatingofredundantexplorationbyincorporatingtask-relevantinformation.
LEMAEbenefitsfromLLMpriorsthroughdiscrimination. WeevaluateHER(Andrychowicz
etal.,2017),whichalsoemployshindsightintrinsicrewardsbutselectsgoalsrandomlyfrommemory.
HERâ€™spoorperformanceemphasizesthecriticalroleofincorporatingLLMpriorsforlocalizing
keystatesinachievingefficientexploration. Tofurthersupportourclaimaboutthesuperiorityof
LLMdiscriminationovergeneration,weevaluateEureka-si,asingle-iterationvariantofEureka(Ma
et al., 2023), which uses LLM to generate reward functions directly. While Eureka-si performs
comparablytoLEMAEinsimpletasks,itstrugglesinmostcomplextaskswithpartialobservability,
indicatingthatLLM-baseddiscriminationmayofferamoregeneralandeffectiveintegrationofLLM.
Notably,thesetwomethodsarenotspecificallydesignedforefficientexploration. Pleasereferto
AppendixE.2,E.3formoredetails.
LEMAEreducesredundantexploration. WefurthercomparetheexplorationbehaviorofLEMAE
withthatofCMAEonthePasstask. Thevisitationmaps,displayedinlogscale,aredepictedin
Fig.1. TheillustrationrevealsthatLEMAEmarkedlyavoidsredundantexploration: agentstrained
withCMAEtendtoexcessivelyexploretheleftroom,whiletheagentsâ€™visitationareainLEMAE
isnotablyconcentratedaroundthesuccesspath. Furthermore,anorganicdivisionoflaboramong
agentsemerges,affirmingtheefficacyofencouraginglabordivisioninpromptdesign.
LEMAEcircumventstask-irrelevantkeystates. Duetotheincompleteinformation,LLMmay
discriminatetask-irrelevantkeystates. Forinstance,intheSecret-Roomtask,threeroomsarepresent
7LEMAE(Ours) LAIES QMIX-DR ELLM QMIX CMAE MAVEN MASER RODE
MMM2 3s5z_vs_3s6z 3s_vs_5z
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 5 0 2 4 6 0 1 2 3 4
5m_vs_6m 2m_vs_1z 1c3s5z
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50
T (mil) T (mil) T (mil)
Figure5: EvaluatingLEMAEonsixSMACmapswithsparserewards,usingtestwinrateasthe
evaluationmetric. Notably,QMIX-DRisQMIXwithdenserewardsintheoriginalSMAC.
on the right, but LLM is not informed about the real target room for fairness. In Fig. 4b, LLM
discriminatestwotask-irrelevantkeystates,denotedasÎº andÎº ,whichrepresentanexploration
2 4
ofthetwoirrelevantrooms,respectively. Fig.4cshowsthatthepruningmechanismafterfinding
thesuccessstateinLEMAEmakesiteffectiveincircumventingtask-irrelevantkeystates. Amore
detailedrobustnessanalysisisprovidedinSec.5.5.
5.2 STARCRAFTMULTI-AGENTCHALLENGE(SMAC)
SMACisawidely-usedchallengingbenchmarkinMARL.Incontrasttodenseorsemi-sparsereward
versionsusedbefore,weemployfullysparse-rewardtaskstoemphasizeexploration,rewarding
agents only upon complete enemy elimination. In addition, to validate LEMAE across diverse
scenarios,weconductexperimentsonsixmapswithvarieddifficultyandagentnumbers.
InFig.5,LEMAEdemonstratessuperiorperformanceoverallbaselines. AlthoughbaselinesQMIX,
MAVEN,CDS,andMASERexcelindenseorsemi-sparserewardsettings,theystruggleinfully
sparserewardscenarios. CMAEshowspartialefficacyinsimplertasksbutfailsinharderscenarios
duetothelackoftask-relatedinformationincuriosity-drivengoalselection. LAIESistheonlynon-
LLMbaselinecomparabletoLEMAE.However,itrequireshandcraftedexternalstatepriorsandstill
underperformscomparedtoLEMAE,especiallyonmorechallengingtasks. ELLM,benefitingfrom
LLMpriors,performswellonsimplertasks,butitseffectivenessdiminishesonharderones,likely
duetotheinstabilityandlessreliableguidanceofsemanticsimilarity-basedrewards. Notably,weadd
QMIX-DR,whichaugmentsQMIXwithdenserewardsintheoriginalSMAC.Surprisingly,LEMAE
demonstratesthepotentialtomatchorevensurpassQMIX-DR,particularlyinhardmaps,shedding
lightonminimizingthemanualworkloadincomplexrewarddesigninreal-worldscenarios. Given
thecomplexityoftheSMACbenchmark,theconsistentsuperiorityofLEMAEconfirmsitspotential
applicabilityinmorecomplexreal-worldscenarios. WefurtherevaluateLEMAEonSMACv2(Ellis
etal.,2024),anenhancedversionwithmorestochasticity,asdetailedinAppendixF.1.
5.3 COMPATIBLILITYWITHVARIOUSALGORITHMS
LEMAEincorporatestask-relevantguidanceintheformofintrinsicrewardsandisagnostictoRL
algorithms. Sec.5.1and5.2haveverifiedthecompatibilitythroughimplementingontwodistinct
MARL algorithms: IPPO in MPE and QMIX in SMAC. To further substantiate this claim, we
build our method on two widely-used MARL algorithms, namely QPLEX (Wang et al., 2020a)
and VMIX (Su et al., 2021), adopting a value-based and actor-critic methodology respectively.
As illustrated in Fig. 6a, algorithms combined with LEMAE consistently improve performance,
underscoringthepotentialofLEMAEtointegratewithalternativealgorithmsacrossdiversefields
inthefuture. Additionally,LEMAEisaversatileapproachforefficientexploration,notlimitedto
MARL.Tovalidatethisassertion,weconductfurtherevaluationsofLEMAEinasingle-agentvariant
ofMPE,asdemonstratedinAppendixF.4.
8
etaR
niW
tseT
etaR
niW
tseTLEMAE Base+SHIR
Base+SHIR+KSMTE Base+SHIR+KSMTP Base
QMIX+Ours QPLEX+Ours VMIX+Ours
3s_vs_5z Large-Pass
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
2m_vs_1z 1c3s5z MMM2 0 1 2 3 4 0.0 0.5 1.0 1.5 2.0 2.5
T (mil) T (mil)
(a) (b)
Figure6: (a)Evaluationsonbaselinesofwell-knownMARLalgorithms,i.e.,QPLEXandVMIX.
Notably, both QPLEX and VMIX exhibit complete failure unless integrated with our approach.
(b) Ablation studies are conducted on two exemplary tasks from MPE and SMAC to assess the
significanceofKSMTandSHIRwithinLEMAE.
Table1: AblationstudiesonSelf-Checkmechanismand
1.0
LLMs. WecomparetheperformanceoftwoLLMs(GPT-
4-turbo and GPT-3.5-turbo), recording the Acceptance
Rate(r )andExecutionRate(r )intenrunsofthe
acc exe 0.5
generated discriminator functions. w/o denotes the ab-
senceofourSelf-Checkmechanism.
0.0
0.1 0.5 1.0 5.0 10.0
r acc (r exe) GPT-4-turbo GPT-4-turbow/o GPT-3.5-turbo Relative Scaling Rate
Large-Pass 1.0(1.0) 0.8(1.0) 0.7(1.0) Figure7: Hyperparameterexamination
2m vs 1z 1.0(1.0) 0.7(1.0) 0.6(1.0)
onrewardscalingrateÎ±andÎ². Thex-
5m vs 6m 1.0(1.0) 0.9(1.0) 1.0(1.0)
axisrepresentstherelativevalueswith
MMM2 0.8(1.0) 0.6(0.7) 0.0(1.0)
respecttothedefaultparameters.
5.4 ABLATIONSTUDIES
RoleofSHIRandKSMT.WeconductanablationstudytoassessthesignificanceofKSMTand
SHIR within LEMAE. We select two exemplary tasks from MPE and SMAC and report results
in Fig. 6b. In SMAC, Base refers to QMIX, while in MPE, it denotes IPPO. Besides, SHIR
represents subspace-based hindsight intrinsic reward, KSMTE signifies exploration with KSMT,
KSMTPdenotesplanningwithKSMT,andLEMAEencompassesBase+SHIR+KSMTE+KSMTP.
Asillustrated,theabsenceofSHIRorKSMTsignificantlydeterioratesperformance,revealingboth
componentsâ€™pivotalrolesinachievingeffectivekeystate-guidedexploration.
RoleofSelf-CheckmechanismandLLMs. WeconductacomparativeanalysisbetweenGPT-4-
turboandGPT-3.5-turboregardinggeneratingdiscriminatorfunctions. Meanwhile,weinvestigatethe
performanceofGPT-4-turbowithouttheSelf-Checkmechanism(GPT-4-turbow/o). TheAcceptance
Rate(r )denotestheproportionofseedsachievingover80%ofthebestperformanceafterRL
acc
training,whiletheExecutionRate(r )indicatestheproportionofseedsforwhichalldiscriminator
exe
functions are executable. As depicted in Table 1, the results demonstrate that a powerful LLM
withourSelf-Checkmechanismeffectivelyensuresthehighqualityofkeystates,asevidencedby
the codeâ€™s executability and the final performance. The scalability of LEMAE to LLM and our
Self-CheckmechanismpromisethatLEMAEcanleveragemorepowerfulLLMsinthefutureandbe
appliedtomorechallengingreal-worldtaskssafelyandefficiently.
5.5 SENSITIVITY&ROBUSTNESSANALYSIS
Sensitivity to Hyperparameters. We conduct experiments on the pivotal hyperparameters in
LEMAE,i.e.,rewardscalingratesÎ±andÎ². Thex-axisrepresentstherelativevalueswithrespect
to the default (Î± = 10, Î² = 1), encompassing evaluations for Î± âˆˆ {1,5,10,50,100} and Î² âˆˆ
{0.1,0.5,1,5,10}. Fig. 7 illustrates that LEMAE is robust to these hyperparameters across a
considerablerange. Notably,excessiveextrinsicrewardscalingrateÎ±orinsufficientintrinsicreward
scalingrateÎ² cancauseperformancedegradationduetotheabruptalterationoftherewardorthe
9
etaR
niW
tseT
etaR
niW
tseT
etaR
niW
tseTinadequatemotivationalimpact. Additionally,weconductanablationstudyonmixed-randomness
explorationinAppendixF.3.
Robustness to Perturbations in Key States. We conduct experiments to evaluate the robust-
ness of LEMAE to perturbations in key states. Specifically, Reduction simulates the absence
of key states by randomly clearing a certain percentage of key states. Distraction simulates
the misidentification of common states as key states by randomly adding a certain portion of
distractingstates(encouragingaran-
dom state dimension to 0). The Table2: RobustnessanalysisofLEMAEtoperturbationsin
observedperformancedecreasewith keystates,whetherrandomlydeletingkeystates(Reduction)
increasing perturbations in Table 2 oraddingdistractingstates(Distraction).
underscores the significance of key
statesâ€™quality. LEMAEexhibitsno- Reduction Distraction
tablerobustnesstoperturbations,en- Tasks Default 25% 50% 50% 100%
suring its reliability across diverse 1c3s5z 0.98Â±0.02 0.97Â±0.01 0.97Â±0.02 0.92Â±0.04 0.89Â±0.05
applicationscenarios,particularlyin 3svs5z 0.83Â±0.07 0.80Â±0.18 0.57Â±0.28 0.80Â±0.11 0.66Â±0.08
lightofthelimitedcapabilitiesofcur-
MMM2 0.89Â±0.08 0.89Â±0.03 0.79Â±0.09 0.86Â±0.04 0.79Â±0.08
rentLLMs.
5.6 SCALABILITY&GENERALIZATIONANALYSIS
ToruleoutthepossibilitythatLEMAEâ€™ssuccess
reliesonLLMâ€™sfamiliaritywiththechosentasks,
Bob
weâ€™vehandcraftedabrandnewtask,termedRiver,
Alice
whichLLMhasneverencounteredbefore.Thetask
isillustratedinFig.8a,wheretheobjectiveisfor
BobtohelpAlice,whoisafraidofwater,crosstwo
riverstoreachthebottom-rightcorner. Asshown
inFig.8b,LEMAEoutperformsthebaselines,and Mountain
thisconfirmsLLMâ€™sgeneralizationcapabilitiesto
empowerLEMAEâ€™seffectivenessinpromotingeffi-
(a) (b)
cientexplorationindiversenewtasks. Pleaserefer
toAppendixE.4.3fordetailsonthetask.
Figure8: (a)Abrandnewtask, River, which
Additionally,weextendLEMAEtoavision-based LLM has never encountered before. (b) The
task,asdescribedinAppendixF.2,demonstrating trainingcurvesofLEMAEandbaselinesusing
thescalabilitypotentialofLEMAE. theevaluationmetricoftestwinrate.
6 CONCLUSION
SummaryofThisWork: WepresentLEMAE,anovelframeworkthatbenefitsmulti-agentexplo-
rationwithtask-specificguidancefromLLM.LEMAEexecutesthekeystateslocalizationwithLLM
andenablesthekeystate-guidedexplorationtoimprovesampleefficiency. Inthisway,wecan(i)
buildupconnectionsbetweenLLMandRLtogroundlinguisticknowledgeintodecision-making,(ii)
reducethemanualworkloadinaccessingknowledgeandintensiveinferencecallsfromLLM,and
(iii)significantlyboostexplorationefficiencythroughguidedandorganizedexploration. Extensive
experimentsfurtherexaminetheeffectivenessofLEMAEintypicalbenchmarks.
Limitations&FutureInvestigations: IndevelopingLEMAE,wemadeeffortstocompensatefor
thepitfallsofconcurrentLLMs,e.g.,carefulpreparationforpromptengineeringandtask-related
priorprovisiontoavoidthenuisancesinLLMusages. Allofthesecanbecircumventedwiththe
progressofLLMâ€™scapabilityenhancement. ThisworkpavesthewayforLLM-empoweredRLto
achievethepotentialincomplicateddecision-makingscenarios.
REFERENCES
PieterAbbeelandAndrewYNg. Apprenticeshiplearningviainversereinforcementlearning. In
Proceedingsofthetwenty-firstinternationalconferenceonMachinelearning,pp. 1,2004.
10JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
JohannesAckermann,VolkerGabler,TakayukiOsa,andMasashiSugiyama.Reducingoverestimation
biasinmulti-agentdomainsusingdoublecentralizedcritics. arXivpreprintarXiv:1910.01465,
2019.
AkshatAgarwal,SumitKumar,andKatiaSycara. Learningtransferablecooperativebehaviorin
multi-agentteams. arXivpreprintarXiv:1906.01202,2019.
RishabhAgarwal,DaleSchuurmans,andMohammadNorouzi. Anoptimisticperspectiveonoffline
reinforcementlearning. InInternationalConferenceonMachineLearning,pp.104â€“114.PMLR,
2020.
MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea
Finn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,notasisay:
Groundinglanguageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline
reinforcementlearningwithdiversifiedq-ensemble. AdvancesinNeuralInformationProcessing
Systems,34,2021.
MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,PeterWelinder,Bob
McGrew,JoshTobin,OpenAIPieterAbbeel,andWojciechZaremba. Hindsightexperiencereplay.
Advancesinneuralinformationprocessingsystems,30,2017.
N.N.Author. Suppressedforanonymity,2021.
BowenBaker,IngmarKanitscheider,TodorMarkov,YiWu,GlennPowell,BobMcGrew,andIgor
Mordatch. Emergenttoolusefrommulti-agentautocurricula. arXivpreprintarXiv:1909.07528,
2019.
CraigBecker,JoaquinSalas,KentaroTokusei,andJ-CLatombe.Reliablenavigationusinglandmarks.
InProceedingsof1995IEEEInternationalconferenceonRoboticsandAutomation,volume1,pp.
401â€“406.IEEE,1995.
MohamedIshmaelBelghazi,AristideBaratin,SaiRajeshwar,SherjilOzair,YoshuaBengio,Aaron
Courville,andDevonHjelm. Mutualinformationneuralestimation. InInternationalconference
onmachinelearning,pp.531â€“540.PMLR,2018.
MarcBellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,andRemiMunos.
Unifying count-based exploration and intrinsic motivation. Advances in neural information
processingsystems,29,2016.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison,DavidFarhi,QuirinFischer,ShariqHashme,ChrisHesse,etal. Dota2withlargescale
deepreinforcementlearning. arXivpreprintarXiv:1912.06680,2019.
RWayneBoss. Theeffectsofleaderabsenceonaconfrontationteam-buildingdesign. TheJournal
ofAppliedBehavioralScience,14(4):469â€“478,1978.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877â€“1901,2020.
CameronBBrowne,EdwardPowley,DanielWhitehouse,SimonMLucas,PeterICowling,Philipp
Rohlfshagen,StephenTavener,DiegoPerez,SpyridonSamothrakis,andSimonColton. Asurvey
ofmontecarlotreesearchmethods. IEEETransactionsonComputationalIntelligenceandAIin
games,4(1):1â€“43,2012.
JacobBuckman,CarlesGelada,andMarcGBellemare.Theimportanceofpessimisminfixed-dataset
policyoptimization. arXivpreprintarXiv:2009.06799,2020.
11Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.
Large-scalestudyofcuriosity-drivenlearning. arXivpreprintarXiv:1808.04355,2018a.
YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov. Explorationbyrandomnetwork
distillation. arXivpreprintarXiv:1810.12894,2018b.
QiCai,ZhuoranYang,ChiJin,andZhaoranWang. Provablyefficientexplorationinpolicyoptimiza-
tion. InInternationalConferenceonMachineLearning,pp.1283â€“1294.PMLR,2020.
YujiCao,HuanZhao,YuhengCheng,TingShu,GuolongLiu,GaoqiLiang,JunhuaZhao,andYun
Li. Surveyonlargelanguagemodel-enhancedreinforcementlearning: Concept,taxonomy,and
methods. arXivpreprintarXiv:2404.00282,2024.
ThomasCarta,CleÂ´mentRomac,ThomasWolf,SylvainLamprier,OlivierSigaud,andPierre-Yves
Oudeyer. Groundinglargelanguagemodelsininteractiveenvironmentswithonlinereinforcement
learning. arXivpreprintarXiv:2302.02662,2023.
Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, and Liangjun Zhang. Rlingua: Improving
reinforcementlearningsampleefficiencyinroboticmanipulationswithlargelanguagemodels.
IEEERoboticsandAutomationLetters,2024.
Ching-AnCheng,TengyangXie,NanJiang,andAlekhAgarwal. Adversariallytrainedactorcritic
forofflinereinforcementlearning. arXivpreprintarXiv:2202.02446,2022.
PengyuCheng,WeituoHao,ShuyangDai,JiachangLiu,ZheGan,andLawrenceCarin. Club: A
contrastivelog-ratioupperboundofmutualinformation. InInternationalconferenceonmachine
learning,pp.1779â€“1788.PMLR,2020.
CarolineClausandCraigBoutilier.Thedynamicsofreinforcementlearningincooperativemultiagent
systems. InAAAIConferenceonArtificialIntelligence(AAAI),1998.
CeÂ´dricColas,LaetitiaTeodorescu,Pierre-YvesOudeyer,XingdiYuan,andMarc-AlexandreCoË†teÂ´.
Augmentingautotelicagentswithlargelanguagemodels. arXivpreprintarXiv:2305.12487,2023.
AbhishekDas,TheÂ´ophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,MikeRabbat,andJoelle
Pineau. Tarmac: Targetedmulti-agentcommunication. InInternationalConferenceonMachine
Learning,pp.1538â€“1546.PMLR,2019.
ChristianSchroederdeWitt, TarunGupta, DenysMakoviichuk, ViktorMakoviychuk, PhilipHS
Torr,MingfeiSun,andShimonWhiteson. Isindependentlearningallyouneedinthestarcraft
multi-agentchallenge? arXivpreprintarXiv:2011.09533,2020.
ErikDDemaine,GiovanniViglietta,andAaronWilliams. Supermariobros.isharder/easierthanwe
thought. 2016.
ShehzaadDhuliawala,MojtabaKomeili,JingXu,RobertaRaileanu,XianLi,AsliCelikyilmaz,and
JasonWeston. Chain-of-verificationreduceshallucinationinlargelanguagemodels. arXivpreprint
arXiv:2309.11495,2023.
ZiluoDing, TiejunHuang, andZongqingLu. Learningindividuallyinferredcommunicationfor
multi-agentcooperation. arXivpreprintarXiv:2006.06455,2020.
XiwangDong,QingdongLi,QilunZhao,andZhangRen. Time-varyinggroupformationanalysis
anddesignforgenerallinearmulti-agentsystemswithdirectedtopologies. InternationalJournal
ofRobustandNonlinearControl,27(9):1640â€“1652,2017.
YuqingDu, OliviaWatkins, ZihanWang, CeÂ´dricColas, TrevorDarrell, PieterAbbeel, Abhishek
Gupta,andJacobAndreas. Guidingpretraininginreinforcementlearningwithlargelanguage
models. arXivpreprintarXiv:2302.06692,2023.
R.O.Duda,P.E.Hart,andD.G.Stork. PatternClassification. JohnWileyandSons,2ndedition,
2000.
RickDurrett. Probability: theoryandexamples,volume49. Cambridgeuniversitypress,2019.
12AdrienEcoffet,JoostHuizinga,JoelLehman,KennethOStanley,andJeffClune. Firstreturn,then
explore. Nature,590(7847):580â€“586,2021.
BenjaminEllis,JonathanCook,SkanderMoalla,MikayelSamvelyan,MingfeiSun,AnujMahajan,
JakobFoerster,andShimonWhiteson. Smacv2: Animprovedbenchmarkforcooperativemulti-
agentreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,36,2024.
IanFischer. Theconditionalentropybottleneck. Entropy,22(9):999,2020.
JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhiteson. Learningto
communicatewithdeepmulti-agentreinforcementlearning. InAdvancesinneuralinformation
processingsystems(NeurIPS),2016.
JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShimonWhiteson.
Counterfactualmulti-agentpolicygradients. InAAAIConferenceonArtificialIntelligence(AAAI),
2018.
JustinFu,AviralKumar,OfirNachum,GeorgeTucker,andSergeyLevine. D4rl: Datasetsfordeep
data-drivenreinforcementlearning. arXivpreprintarXiv:2004.07219,2020.
ScottFujimotoandShixiangShaneGu. Aminimalistapproachtoofflinereinforcementlearning.
Advancesinneuralinformationprocessingsystems,34:20132â€“20145,2021.
ScottFujimoto,HerkeHoof,andDavidMeger. Addressingfunctionapproximationerrorinactor-
criticmethods. InInternationalconferenceonmachinelearning,pp.1587â€“1596.PMLR,2018.
ScottFujimoto,DavidMeger,andDoinaPrecup. Off-policydeepreinforcementlearningwithout
exploration. InInternationalconferenceonmachinelearning,pp.2052â€“2062.PMLR,2019.
JayeshKGupta,MaximEgorov,andMykelKochenderfer. Cooperativemulti-agentcontrolusing
deepreinforcementlearning. InInternationalConferenceonAutonomousAgentsandMultiagent
Systems(AAMAS),2017.
TaoHan,Zhi-HongGuan,MingChi,BinHu,TaoLi,andXian-HeZhang. Multi-formationcontrol
ofnonlinearleader-followingmulti-agentsystems. ISAtransactions,69:140â€“147,2017.
PaulLHooper,HillardSKaplan,andJamesLBoone. Atheoryofleadershipinhumancooperative
groups. JournalofTheoreticalBiology,265(4):633â€“646,2010.
JunyanHu,ParijatBhowmick,andAlexanderLanzon.Distributedadaptivetime-varyinggroupforma-
tiontrackingformultiagentsystemswithmultipleleadersondirectedgraphs. IEEETransactions
onControlofNetworkSystems,7(1):140â€“150,2019.
SiyiHu,FengdaZhu,XiaojunChang,andXiaodanLiang. Updet: Universalmulti-agentrlviapolicy
decouplingwithtransformers. InInternationalConferenceonLearningRepresentations(ICLR),
2020.
ShariqIqbal,ChristianASchroederDeWitt,BeiPeng,WendelinBoÂ¨hmer,ShimonWhiteson,andFei
Sha. Randomizedentity-wisefactorizationformulti-agentreinforcementlearning. InInternational
ConferenceonMachineLearning(ICML),2021.
Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia
Castaneda,CharlesBeattie,NeilCRabinowitz,AriSMorcos,AvrahamRuderman,etal. Human-
levelperformancein3dmultiplayergameswithpopulation-basedreinforcementlearning. Science,
364(6443):859â€“865,2019.
MichaelJanner,JustinFu,MarvinZhang,andSergeyLevine.Whentotrustyourmodel:Model-based
policyoptimization. arXivpreprintarXiv:1906.08253,2019.
NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroOrtega,DJStrouse,
JoelZLeibo,andNandoDeFreitas. Socialinfluenceasintrinsicmotivationformulti-agentdeep
reinforcementlearning. InInternationalConferenceonMachineLearning(ICML),2019.
13JeewonJeon,WoojunKim,WhiyoungJung,andYoungchulSung. Maser: Multi-agentreinforcement
learningwithsubgoalsgeneratedfromexperiencereplaybuffer. InInternationalConferenceon
MachineLearning,pp.10041â€“10052.PMLR,2022.
JiechuanJiangandZongqingLu. Learningattentionalcommunicationformulti-agentcooperation.
arXivpreprintarXiv:1805.07733,2018.
JiechuanJiangandZongqingLu. Offlinedecentralizedmulti-agentreinforcementlearning. arXiv
preprintarXiv:2108.01832,2021.
JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu. Graphconvolutionalreinforcement
learning. arXivpreprintarXiv:1810.09202,2018.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
InternationalConferenceonMachineLearning,pp.5084â€“5096.PMLR,2021.
YonghyeonJo,SunwooLee,JunghyukYum,andSeungyulHan. Fox: Formation-awareexploration
inmulti-agentreinforcementlearning. arXivpreprintarXiv:2308.11272,2023.
M.J.Kearns. ComputationalComplexityofMachineLearning. PhDthesis,DepartmentofComputer
Science,HarvardUniversity,1989.
RahulKidambi,AravindRajeswaran,PraneethNetrapalli,andThorstenJoachims. Morel: Model-
basedofflinereinforcementlearning. arXivpreprintarXiv:2005.05951,2020.
WooKyungKim,SeungHyunKim,HongukWoo,etal. Efficientpolicyadaptationwithcontrastive
promptensembleforembodiedagents. AdvancesinNeuralInformationProcessingSystems,36,
2024.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114,2013.
LeventeKocsisandCsabaSzepesvaÂ´ri. Banditbasedmonte-carloplanning. InEuropeanconference
onmachinelearning,pp.282â€“293.Springer,2006.
IlyaKostrikov,RobFergus,JonathanTompson,andOfirNachum. Offlinereinforcementlearning
withfisherdivergencecriticregularization. InInternationalConferenceonMachineLearning,pp.
5774â€“5783.PMLR,2021.
JakubGrudzienKuba,RuiqingChen,MunningWen,YingWen,FangleiSun,JunWang,andYaodong
Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint
arXiv:2109.11251,2021.
AlexKuleszaandBenTaskar. Determinantalpointprocessesformachinelearning. arXivpreprint
arXiv:1207.6083,2012.
TejasDKulkarni,KarthikNarasimhan,ArdavanSaeedi,andJoshTenenbaum. Hierarchicaldeep
reinforcementlearning: Integratingtemporalabstractionandintrinsicmotivation. Advancesin
neuralinformationprocessingsystems,29,2016.
AviralKumar,JustinFu,MatthewSoh,GeorgeTucker,andSergeyLevine. Stabilizingoff-policy
q-learningviabootstrappingerrorreduction. AdvancesinNeuralInformationProcessingSystems,
32,2019.
AviralKumar,AurickZhou,GeorgeTucker,andSergeyLevine. Conservativeq-learningforoffline
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems,33:1179â€“1191,2020.
AviralKumar,JoeyHong,AnikaitSingh,andSergeyLevine. Shouldirunofflinereinforcement
learningorbehavioralcloning? InDeepRLWorkshopNeurIPS2021,2021.
KarolKurach,AntonRaichuk,PiotrStanczyk,MichalZajac,OlivierBachem,LasseEspeholt,Carlos
Riquelme,DamienVincent,MarcinMichalski,OlivierBousquet,etal. Googleresearchfootball:
Anovelreinforcementlearningenvironment. arXivpreprintarXiv:1907.11180,2019.
14MinaeKwon,SangMichaelXie,KaleshaBullard,andDorsaSadigh. Rewarddesignwithlanguage
models. arXivpreprintarXiv:2303.00001,2023.
P. Langley. Crafting papers on machine learning. In Pat Langley (ed.), Proceedings of the 17th
InternationalConferenceonMachineLearning(ICML2000),pp.1207â€“1216,Stanford,CA,2000.
MorganKaufmann.
GregoryFLawlerandVladaLimic. Randomwalk: amodernintroduction,volume123. Cambridge
UniversityPress,2010.
Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio
Guadarrama. Predictiveinformationaccelerateslearninginrl. arXivpreprintarXiv:2007.12401,
2020.
Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online
reinforcementlearningviabalancedreplayandpessimisticq-ensemble. InConferenceonRobot
Learning,pp.1702â€“1712.PMLR,2022.
JoelZLeibo,EdgarADuenËœez-Guzman,AlexanderVezhnevets,JohnPAgapiou,PeterSunehag,
Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable
evaluationofmulti-agentreinforcementlearningwithmeltingpot. InInternationalConferenceon
MachineLearning(ICML),2021.
SergeyLevine,AviralKumar,GeorgeTucker,andJustinFu. Offlinereinforcementlearning: Tutorial,
review,andperspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
BelindaZLi,WilliamChen,PratyushaSharma,andJacobAndreas. Lampp: Languagemodelsas
probabilisticpriorsforperceptionandaction. arXive-prints,pp.arXivâ€“2302,2023.
Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang.
Celebratingdiversityinsharedmulti-agentreinforcementlearning.AdvancesinNeuralInformation
ProcessingSystems,34:3991â€“4002,2021.
Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An
Huang,EkinAkyuÂ¨rek,AnimaAnandkumar,etal. Pre-trainedlanguagemodelsforinteractive
decision-making. AdvancesinNeuralInformationProcessingSystems,35:31199â€“31212,2022.
JackyLiang,WenlongHuang,FeiXia,PengXu,KarolHausman,BrianIchter,PeteFlorence,and
AndyZeng. Codeaspolicies: Languagemodelprogramsforembodiedcontrol. In2023IEEE
InternationalConferenceonRoboticsandAutomation(ICRA),pp.9493â€“9500.IEEE,2023.
BillYuchenLin,YichengFu,KarinaYang,PrithvirajAmmanabrolu,FaezeBrahman,ShiyuHuang,
ChandraBhagavatula,YejinChoi,andXiangRen. Swiftsage: Agenerativeagentwithfastand
slowthinkingforcomplexinteractivetasks. arXivpreprintarXiv:2305.17390,2023.
ToruLin, JacobHuh, ChristopherStauffer, SerNamLim, andPhillipIsola. Learningtoground
multi-agentcommunicationwithautoencoders. InAdvancesinNeuralInformationProcessing
Systems(NeurIPS),2021.
CamLinke,NadiaMAdy,MarthaWhite,ThomasDegris,andAdamWhite. Adaptingbehavior
viaintrinsicreward: Asurveyandempiricalstudy. Journalofartificialintelligenceresearch,69:
1287â€“1332,2020.
BoLiu,QiangLiu,PeterStone,AnimeshGarg,YukeZhu,andAnimashreeAnandkumar. Coach-
player multi-agent reinforcement learning for dynamic team composition. arXiv preprint
arXiv:2105.08692,2021a.
Boyin Liu, Zhiqiang Pu, Yi Pan, Jianqiang Yi, Yanyan Liang, and Du Zhang. Lazy agents: a
new perspective on solving sparse reward problem in multi-agent reinforcement learning. In
InternationalConferenceonMachineLearning,pp.21937â€“21950.PMLR,2023.
HaoLiuandPieterAbbeel. Behaviorfromthevoid: Unsupervisedactivepre-training. Advancesin
NeuralInformationProcessingSystems,34:18459â€“18473,2021.
15Iou-JenLiu, UnnatJain, RaymondAYeh, andAlexanderSchwing. Cooperativeexplorationfor
multi-agentdeepreinforcementlearning. InInternationalConferenceonMachineLearning,pp.
6826â€“6836.PMLR,2021b.
ShaotengLiu,HaoqiYuan,MindaHu,YanweiLi,YukangChen,ShuLiu,ZongqingLu,andJiayaJia.
Rl-gpt: Integratingreinforcementlearningandcode-as-policy. arXivpreprintarXiv:2402.19299,
2024.
YaoLiu,AdithSwaminathan,AlekhAgarwal,andEmmaBrunskill. Provablygoodbatchreinforce-
mentlearningwithoutgreatexploration. arXivpreprintarXiv:2007.08202,2020a.
YongLiu,WeixunWang,YujingHu,JianyeHao,XingguoChen,andYangGao. Multi-agentgame
abstraction via graph attention neural network. In AAAI Conference on Artificial Intelligence
(AAAI),2020b.
QianLong,ZihanZhou,AbhibavGupta,FeiFang,YiWu,andXiaolongWang. Evolutionarypopula-
tioncurriculumforscalingmulti-agentreinforcementlearning. arXivpreprintarXiv:2003.10423,
2020.
RyanLowe,YiIWu,AvivTamar,JeanHarb,OpenAIPieterAbbeel,andIgorMordatch. Multi-agent
actor-criticformixedcooperative-competitiveenvironments. InAdvancesinneuralinformation
processingsystems(NeurIPS),2017.
YechengJasonMa,WilliamLiang,GuanzhiWang,De-AnHuang,OsbertBastani,DineshJayaraman,
YukeZhu,LinxiFan,andAnimaAnandkumar. Eureka: Human-levelrewarddesignviacoding
largelanguagemodels. arXivpreprintarXiv:2310.12931,2023.
Odile Macchi. The fermion processâ€”a model of stochastic point process with repulsive points.
InTransactionsoftheSeventhPragueConferenceonInformationTheory,StatisticalDecision
Functions,RandomProcessesandofthe1974EuropeanMeetingofStatisticians,1977.
AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson. Maven: Multi-agent
variationalexploration. Advancesinneuralinformationprocessingsystems,32,2019.
HangyuMao,WulongLiu,JianyeHao,JunLuo,DongLi,ZhengchaoZhang,JunWang,andZhen
Xiao. Neighborhoodcognitionconsistentmulti-agentreinforcementlearning. InProceedingsof
theAAAIconferenceonartificialintelligence,volume34,pp.7219â€“7226,2020a.
HangyuMao,ZhengchaoZhang,ZhenXiao,ZhiboGong,andYanNi.Learningagentcommunication
underlimitedbandwidthbymessagepruning. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume34,pp.5142â€“5149,2020b.
LinghuiMeng,MuningWen,YaodongYang,ChenyangLe,XiyunLi,WeinanZhang,YingWen,
HaifengZhang,JunWang,andBoXu. Offlinepre-trainedmulti-agentdecisiontransformer: One
bigsequencemodelconquersallstarcraftiitasks. arXivpreprintarXiv:2112.02845,2021.
R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (eds.). Machine Learning: An Artificial
IntelligenceApproach,Vol.I. Tioga,PaloAlto,CA,1983.
DonaldMichie,MichaelBain,andJHayes-Miches. Cognitivemodelsfromsubcognitiveskills. IEE
controlengineeringseries,44:71â€“99,1990.
D.LeeMiller. Wordllama: Recycledtokenembeddingsfromlargelanguagemodels,2024. URL
https://github.com/dleemiller/wordllama.
ReuthMirsky,WilliamMacke,AndyWang,HarelYedidsion,andPeterStone. Apennyforyour
thoughts: Thevalueofcommunicationinadhocteamwork. InInternationalJointConferenceson
ArtificialIntelligence(IJCAI),2020.
T.M.Mitchell. Theneedforbiasesinlearninggeneralizations. Technicalreport,ComputerScience
Department,RutgersUniversity,NewBrunswick,MA,1980.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra,andMartinRiedmiller. Playingatariwithdeepreinforcementlearning. arXivpreprint
arXiv:1312.5602,2013.
16OfirNachum,BoDai,IlyaKostrikov,YinlamChow,LihongLi,andDaleSchuurmans. Algaedice:
Policygradientfromarbitraryexperience. arXivpreprintarXiv:1912.02074,2019.
AshvinNair,MurtazaDalal,AbhishekGupta,andSergeyLevine. Acceleratingonlinereinforcement
learningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020a.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcementlearningwithofflinedatasets. arXivpreprintarXiv:2006.09359,2020b.
A.NewellandP.S.Rosenbloom. Mechanismsofskillacquisitionandthelawofpractice. InJ.R.
Anderson(ed.),CognitiveSkillsandTheirAcquisition,chapter1,pp.1â€“51.LawrenceErlbaum
Associates,Inc.,Hillsdale,NJ,1981.
YaruNiu,RohanRPaleja,andMatthewCGombolay. Multi-agentgraph-attentioncommunication
andteaming. InAAMAS,pp.964â€“973,2021.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs.
Springer,2016.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictive
coding. arXivpreprintarXiv:1807.03748,2018.
FabianPaischer,ThomasAdler,VihangPatil,AngelaBitto-Nemling,MarkusHolzleitner,Sebastian
Lehner,HamidEghbal-Zadeh,andSeppHochreiter. Historycompressionvialanguagemodels
inreinforcementlearning. InInternationalConferenceonMachineLearning,pp.17156â€“17185.
PMLR,2022.
Fabian Paischer, Thomas Adler, Markus Hofmarcher, and Sepp Hochreiter. Semantic helm: A
human-readablememoryforreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,36,2024.
LingPan,TabishRashid,BeiPeng,LongboHuang,andShimonWhiteson. Regularizedsoftmax
deepmulti-agentq-learning. AdvancesinNeuralInformationProcessingSystems,34:1365â€“1377,
2021.
LingPan,LongboHuang,TengyuMa,andHuazheXu. Planbetteramidconservatism: Offlinemulti-
agentreinforcementlearningwithactorrectification. InInternationalConferenceonMachine
Learning,pp.17221â€“17237.PMLR,2022.
DeepakPathak,PulkitAgrawal,AlexeiAEfros,andTrevorDarrell. Curiosity-drivenexplorationby
self-supervisedprediction. InInternationalConferenceonMachineLearning(ICML),2017.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin BoÂ¨hmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. AdvancesinNeuralInformationProcessingSystems,34:12208â€“12221,2021.
Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Ruizhi Chen,
ZidongDu,QiGuo,etal. Self-drivengrounding: Largelanguagemodelagentswithautomatical
language-alignedskilllearning. arXivpreprintarXiv:2309.01352,2023.
XueBinPeng,AviralKumar,GraceZhang,andSergeyLevine. Advantage-weightedregression:
Simpleandscalableoff-policyreinforcementlearning. arXivpreprintarXiv:1910.00177,2019.
BenPoole,SherjilOzair,AaronVanDenOord,AlexAlemi,andGeorgeTucker. Onvariational
boundsofmutualinformation. InInternationalConferenceonMachineLearning,pp.5171â€“5180.
PMLR,2019.
YunQu,BoyuanWang,JianzhunShao,YuhangJiang,ChenChen,ZhenbinYe,LinLiu,YangJun
Feng, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its
offlinereinforcementlearningbenchmarks. InThirty-seventhConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack,2023.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
17TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcementlearning. arXivpreprintarXiv:1803.11485,2018.
TabishRashid,GregoryFarquhar,BeiPeng,andShimonWhiteson. Weightedqmix: Expanding
monotonicvaluefunctionfactorisation. arXive-prints,pp.arXivâ€“2006,2020.
PariaRashidinejad,BanghuaZhu,CongMa,JiantaoJiao,andStuartRussell. Bridgingofflinerein-
forcementlearningandimitationlearning: Ataleofpessimism. AdvancesinNeuralInformation
ProcessingSystems,34,2021.
A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of
ResearchandDevelopment,3(3):211â€“229,1959.
MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,NantasNardelli,
TimG.J.Rudner,Chia-ManHung,PhiliphH.S.Torr,JakobFoerster,andShimonWhiteson. The
StarCraftMulti-AgentChallenge. CoRR,abs/1902.04043,2019a.
MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFarquhar,NantasNardelli,
TimGJRudner, Chia-ManHung, PhilipHSTorr, JakobFoerster, andShimonWhiteson. The
starcraftmulti-agentchallenge. arXivpreprintarXiv:1902.04043,2019b.
FrancoScarselli,MarcoGori,AhChungTsoi,MarkusHagenbuchner,andGabrieleMonfardini. The
graphneuralnetworkmodel. IEEEtransactionsonneuralnetworks,20(1):61â€“80,2008.
ChristianSchroederdeWitt,JakobFoerster,GregoryFarquhar,PhilipTorr,WendelinBoehmer,and
ShimonWhiteson. Multi-agentcommonknowledgereinforcementlearning. AdvancesinNeural
InformationProcessingSystems,32:9927â€“9939,2019.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
YounggyoSeo,LiliChen,JinwooShin,HonglakLee,PieterAbbeel,andKiminLee. Stateentropy
maximization with random encoders for efficient exploration. In International Conference on
MachineLearning,pp.9443â€“9454.PMLR,2021.
JianzhunShao,HongchangZhang,YunQu,ChangLiu,ShunchengHe,YuhangJiang,andXiangyang
Ji. Complementaryattentionformulti-agentreinforcementlearning. InInternationalConference
onMachineLearning,pp.30776â€“30793.PMLR,2023.
JianzhunShao,YunQu,ChenChen,HongchangZhang,andXiangyangJi. Counterfactualconserva-
tiveqlearningforofflinemulti-agentreinforcementlearning. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
RuizheShi,YuyaoLiu,YanjieZe,SimonSDu,andHuazheXu. Unleashingthepowerofpre-trained
languagemodelsforofflinereinforcementlearning. arXivpreprintarXiv:2310.20587,2023.
NoahShinn,FedericoCassano,AshwinGopinath,KarthikRNarasimhan,andShunyuYao.Reflexion:
Languageagentswithverbalreinforcementlearning. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023.
YashShukla,WenchangGao,VasanthSarathy,AlvaroVelasquez,RobertWright,andJivkoSinapov.
Lgts: Dynamictasksamplingusingllm-generatedsub-goalsforreinforcementlearningagents.
arXivpreprintarXiv:2310.09454,2023.
AmanpreetSingh,TusharJain,andSainbayarSukhbaatar. Learningwhentocommunicateatscalein
multiagentcooperativeandcompetitivetasks. arXivpreprintarXiv:1812.09755,2018.
MunindarPSingh. Agentcommunicationlanguages: Rethinkingtheprinciples. Computer,31(12):
40â€“47,1998.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.Qtran:Learningto
factorizewithtransformationforcooperativemulti-agentreinforcementlearning. InInternational
ConferenceonMachineLearning(ICML),2019.
18JiayangSong,ZhehuaZhou,JiaweiLiu,ChunrongFang,ZhanShu,andLeiMa. Self-refinedlarge
languagemodelasautomatedrewardfunctiondesignerfordeepreinforcementlearninginrobotics.
arXivpreprintarXiv:2309.06687,2023.
PeterStone,GalAKaminka,SaritKraus,andJeffreySRosenschein. Adhocautonomousagent
teams: Collaboration without pre-coordination. In AAAI Conference on Artificial Intelligence
(AAAI),2010.
Jianhai Su and Qi Zhang. Subgoal proposition using a vision-language model. In CoRL 2023
WorkshoponLearningEffectiveAbstractionsforPlanning(LEAP),2023.
JianyuSu, StephenAdams, andPeterBeling. Value-decompositionmulti-agentactor-critics. In
ProceedingsoftheAAAIconferenceonartificialintelligence,volume35,pp.11352â€“11360,2021.
SainbayarSukhbaatar,RobFergus,etal. Learningmultiagentcommunicationwithbackpropagation.
InAdvancesinneuralinformationprocessingsystems(NeurIPS),2016.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,VinÂ´Ä±ciusFloresZam-
baldi, MaxJaderberg, MarcLanctot, NicolasSonnerat, JoelZLeibo, KarlTuyls, etal. Value-
decompositionnetworksforcooperativemulti-agentlearningbasedonteamreward. InInterna-
tionalConferenceonAutonomousAgentsandMultiAgentSystems(AAMAS),2018.
WesleySuttle,ZhuoranYang,KaiqingZhang,ZhaoranWang,TamerBasÂ¸ar,andJiLiu. Amulti-agent
off-policyactor-criticalgorithmfordistributedreinforcementlearning. IFAC-PapersOnLine,53(2):
1549â€“1554,2020.
RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
MingTan. Multi-agentreinforcementlearning: Independentvs.cooperativeagents. InInternational
ConferenceonMachineLearning(ICML),1993.
HongyaoTang,JianyeHao,TangjieLv,YingfengChen,ZongzhangZhang,HangtianJia,Chunxu
Ren,YanZheng,ZhaopengMeng,ChangjieFan,etal. Hierarchicaldeepmultiagentreinforcement
learningwithtemporalabstraction. arXivpreprintarXiv:1809.09332,2018.
SebastianBThrun. Efficientexplorationinreinforcementlearning. CarnegieMellonUniversity,
1992.
YuandongTian,QuchengGong,andTinaJiang. Jointpolicysearchformulti-agentcollaboration
withimperfectinformation. arXivpreprintarXiv:2008.06495,2020.
EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.
In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026â€“5033.
IEEE,2012.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimotheÂ´e
Lacroix, BaptisteRozie`re, NamanGoyal,EricHambro, FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
Wei-ChengTseng,Tsun-HsuanWang,Yen-ChenLin,andPhillipIsola. Offlinemulti-agentreinforce-
mentlearningwithknowledgedistillation. InAdvancesinNeuralInformationProcessingSystems,
2022.
LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processingsystems(NeurIPS),2017.
Lipeng Wan, Zeyang Liu, Xingyu Chen, Han Wang, and Xuguang Lan. Greedy-based value
representation for optimal coordination in multi-agent reinforcement learning. arXiv preprint
arXiv:2112.04454,2021.
19BoyuanWang,YunQu,YuhangJiang,JianzhunShao,ChangLiu,WenmingYang,andXiangyangJi.
Llm-empoweredstaterepresentationforreinforcementlearning. arXivpreprintarXiv:2407.13237,
2024.
GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,and
AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXiv
preprintarXiv:2305.16291,2023a.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agentq-learning. arXivpreprintarXiv:2008.01062,2020a.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai
Tang,XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents.
arXivpreprintarXiv:2308.11432,2023b.
RundongWang,XuHe,RunshengYu,WeiQiu,BoAn,andZinoviRabinovich. Learningefficient
multi-agentcommunication: Aninformationbottleneckapproach. InInternationalConferenceon
MachineLearning,pp.9908â€“9918.PMLR,2020b.
TonghanWang,JianhaoWang,YiWu,andChongjieZhang. Influence-basedmulti-agentexploration.
arXivpreprintarXiv:1910.05512,2019a.
TonghanWang,JianhaoWang,ChongyiZheng,andChongjieZhang. Learningnearlydecomposable
valuefunctionsviacommunicationminimization. arXivpreprintarXiv:1910.05366,2019b.
TonghanWang,HengDong,VictorLesser,andChongjieZhang. Roma: Multi-agentreinforcement
learningwithemergentroles. InInternationalConferenceonMachineLearning(ICML),2020c.
TonghanWang, TarunGupta, AnujMahajan, BeiPeng, ShimonWhiteson, andChongjieZhang.
Rode: Learningrolestodecomposemulti-agenttasks. arXivpreprintarXiv:2010.01523,2020d.
Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen,
ChangjieFan,andYangGao. Fromfewtomore: Large-scaledynamicmultiagentcurriculum
learning. InAAAIConferenceonArtificialIntelligence(AAAI),2020e.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
neuralinformationprocessingsystems,35:24824â€“24837,2022.
JialongWu,HaixuWu,ZihanQiu,JianminWang,andMingshengLong. Supportedpolicyoptimiza-
tionforofflinereinforcementlearning. InAdvancesinNeuralInformationProcessingSystems,
2022.
YifanWu,GeorgeTucker,andOfirNachum. Behaviorregularizedofflinereinforcementlearning.
arXivpreprintarXiv:1911.11361,2019.
YueWu,ShuangfeiZhai,NitishSrivastava,JoshuaSusskind,JianZhang,RuslanSalakhutdinov,and
HanlinGoh. Uncertaintyweightedactor-criticforofflinereinforcementlearning. arXivpreprint
arXiv:2105.08140,2021.
TengyangXie,Ching-AnCheng,NanJiang,PaulMineiro,andAlekhAgarwal. Bellman-consistent
pessimismforofflinereinforcementlearning. Advancesinneuralinformationprocessingsystems,
34,2021.
TianbaoXie,SihengZhao,ChenHenryWu,YitaoLiu,QianLuo,VictorZhong,YanchaoYang,and
TaoYu. Text2reward: Automateddenserewardfunctiongenerationforreinforcementlearning.
arXivpreprintarXiv:2309.11489,2023.
Pei Xu, Junge Zhang, Qiyue Yin, Chao Yu, Yaodong Yang, and Kaiqi Huang. Subspace-aware
exploration for sparse-reward multi-agent tasks. In Proceedings of the AAAI Conference on
ArtificialIntelligence,volume37,pp.11717â€“11725,2023.
ZhaoXu,YangLyu,QuanPan,JinwenHu,ChunhuiZhao,andShuaiLiu. Multi-vehicleflocking
controlwithdeepdeterministicpolicygradientmethod. InInternationalConferenceonControl
andAutomation(ICCA),2018.
20ZipingXuandAmbujTewari. Reinforcementlearninginfactoredmdps: Oracle-efficientalgorithms
andtighterregretboundsforthenon-episodicsetting. arXivpreprintarXiv:2002.02302,2020.
YaodongYang,JianyeHao,BenLiao,KunShao,GuangyongChen,WulongLiu,andHongyaoTang.
Qatten: Ageneralframeworkforcooperativemultiagentreinforcementlearning. arXivpreprint
arXiv:2002.03939,2020a.
YaodongYang,YingWen,JunWang,LihengChen,KunShao,DavidMguni,andWeinanZhang.
Multi-agentdeterminantalq-learning. InInternationalConferenceonMachineLearning(ICML),
2020b.
YiqinYang,XiaotengMa,ChenghaoLi,ZewuZheng,QiyuanZhang,GaoHuang,JunYang,and
Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline multi-agent
reinforcementlearning. AdvancesinNeuralInformationProcessingSystems,34:10299â€“10312,
2021.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
WeiranYao,ShelbyHeinecke,JuanCarlosNiebles,ZhiweiLiu,YihaoFeng,LeXue,RitheshMurthy,
ZeyuanChen,JianguoZhang,DevanshArpit,etal. Retroformer: Retrospectivelargelanguage
agentswithpolicygradientoptimization. arXivpreprintarXiv:2308.02151,2023.
DehengYe,GuibinChen,WenZhang,ShengChen,BoYuan,BoLiu,JiaChen,ZhaoLiu,Fuhao
Qiu,HongshengYu,etal. Towardsplayingfullmobagameswithdeepreinforcementlearning.
arXivpreprintarXiv:2011.12692,2020.
ChaoYu,AkashVelu,EugeneVinitsky,YuWang,AlexandreBayen,andYiWu. Thesurprising
effectivenessofmappoincooperative,multi-agentgames. arXivpreprintarXiv:2103.01955,2021.
TianheYu,GarrettThomas,LantaoYu,StefanoErmon,JamesZou,SergeyLevine,ChelseaFinn,and
TengyuMa. Mopo: Model-basedofflinepolicyoptimization. arXivpreprintarXiv:2005.13239,
2020.
Yang Yu and Zhi-Hua Zhou. A new approach to estimating the expected first hitting time of
evolutionaryalgorithms. ArtificialIntelligence,172(15):1809â€“1832,2008.
LeiYuan,JianhaoWang,FuxiangZhang,ChengheWang,ZongzhangZhang,YangYu,andChongjie
Zhang. Multi-agentincentivecommunicationviadecentralizedteammatemodeling. 2022.
AndreaZanette,MartinJWainwright,andEmmaBrunskill. Provablebenefitsofactor-criticmethods
forofflinereinforcementlearning. Advancesinneuralinformationprocessingsystems,34,2021.
ChongjieZhangandVictorRLesser. Coordinatingmulti-agentreinforcementlearningwithlimited
communication. In International Conference on Autonomous Agents and Multiagent Systems
(AAMAS),2013.
JesseZhang,JiahuiZhang,KarlPertsch,ZiyiLiu,XiangRen,MinsukChang,Shao-HuaSun,and
JosephJLim. Bootstrapyourownskills: Learningtosolvenewtaskswithlargelanguagemodel
guidance. arXivpreprintarXiv:2310.10021,2023.
KaiqingZhang,ZhuoranYang,HanLiu,TongZhang,andTamerBasar. Fullydecentralizedmulti-
agentreinforcementlearningwithnetworkedagents. InInternationalConferenceonMachine
Learning(ICML),2018.
ZiruiZhao,WeeSunLee,andDavidHsu. Largelanguagemodelsascommonsenseknowledgefor
large-scaletaskplanning. arXivpreprintarXiv:2305.14078,2023.
Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit
assignmentformulti-agentactor-critic. arXivpreprintarXiv:2007.02529,2020.
ZihaoZhou,BinHu,ChenyangZhao,PuZhang,andBinLiu. Largelanguagemodelasapolicy
teacherfortrainingreinforcementlearningagents. arXivpreprintarXiv:2311.13373,2023.
21XizhouZhu,YuntaoChen,HaoTian,ChenxinTao,WeijieSu,ChenyuYang,GaoHuang,BinLi,
LeweiLu,XiaogangWang,etal. Ghostintheminecraft: Generallycapableagentsforopen-world
enviromentsvialargelanguagemodelswithtext-basedknowledgeandmemory. arXivpreprint
arXiv:2305.17144,2023.
22A ALGORITHM
This section includes the pseudo algorithms. Algorithm 1 presents LEMAEâ€™s main algorithm.
LEMAE consists of four phases: generating discriminator functions with LLM, exploring with
KSMT,calculatingSHIR,andperformingRLtraining. Foron-policyRL,thebufferDcorresponds
to a rollout buffer, while for off-policy RL, it is initialized as a replay buffer (Paischer et al.,
2022). Algorithm2illustratestheprocessofexploringwithKSMT.Asourapproachisagnosticto
reinforcementlearningalgorithms,weleaveoutthedetailsofstandardRLtraininginthemainpaper.
Algorithm1:LEMAE
Input: LargelanguagemodelM,promptP,rethinkingpromptPre,non-negativescaling
factorsÎ±,Î²,randomnessepsilonÏµ ,Ïµ (Ïµ <Ïµ ),trainingfrequencyN,maxepisodes
l h l h
Nmax,keystatesnumbersK.
Output: PolicynetworkÏ€ .
Î¸
RandomlyinitializethepolicynetworkparameterÎ¸.
InitializekeystatesmemorytreeT â†[root],replaybufferD,keystateschainreplaybufferDks
Initialdiscriminatorfunctions{FË†}K â†M(P);
i i=1
// Self-Check Mechanism
LLMrethinking{FË†re}K â†M(P,{FË†}K ,Pre);
i i=1 i i=1
whiletherearenon-executablediscriminatorfunctionsin{FË†re}K do
i i=1
{FË†re}K â†M(P,{FË†re}K ,error)
i i=1 i i=1
end
Finaldiscriminatorfunctions{F }K ;
i i=1
forepisode=1toNmaxdo
// Explore with Key States Memory Tree (Algorithm 2)
Îº chain,T,Ï„ â†KSMT-Exp(Ï€ ,T,{F }K ,Ïµ ,Ïµ );
Î¸ i i=1 l h
D â†Dâˆª{Ï„},Dks â†Dksâˆª{Îº chain};
ifepisodemodN =0then
SampleabatchB ={Ï„ }|B| fromDandthecorrespondingbatchofkeystateschains
i i=1
Bks ={Îº chain }|B| fromDks;
i i=1
forÏ„ ={(s ,a ,s ,r )}tmax,Îº chainâˆˆB,Bksdo
t t t+1 t t=1
tstart â†1;
for{tend,Îº }âˆˆÎº chaindo
m
fort=tstarttotenddo
// Subspace-based Hindsight Intrinsic Reward (equation 1)
UpdateÏ„ withr =Î±Â·r +Î²Â·rÎºm(s ,s );tstart â†tend;
t t I t t+1
end
end
// Plan with Key States Memory Tree
ifbranchcorrespondingtoÎº chaininT then
ExtractallchildrennodesC ={Î¾ }|C| andrandomlysampleanodeÎ¾ âˆ¼C;
i i=1 i
end
else
RandomlysampleanodeÎ¾ fromallnodes(notinÎº chain)inT;
i
end
Îº correspondstothesamplednodeÎ¾ ;
plan i
fort=tstarttotmaxdo
UpdateÏ„ withr =Î±Â·r
+Î²Â·rÎºplan(s
,s );
t t I t t+1
end
UpdateBwithÏ„;
// RL Training (Algorithm Agnostic)
UseanMARLalgorithmtotrainÎ¸withB;
end
end
end
23Algorithm2:ExplorewithKeyStatesMemoryTree(KSMT-Exp)
Input: PolicynetworkÏ€ ,keystatesmemorytreeT,discriminatorfunctions{F }K ,
Î¸ i i=1
randomnessepsilonÏµ ,Ïµ (Ïµ <Ïµ ).
l h l h
Output: Îº chain,keystatesmemorytreeT,trajectoryÏ„.
InitializeÎº chainâ†[],Ï„ â†{}
fort=1tot do
max
// key states localization in rollout trajectories
Discriminates with{F }K ;
t i i=1
if(s isakeystateÎº )and(Îº notinÎº chain)then
t m m
Îº chain.append({t,Îº });
m
ifbranchcorrespondingtoÎº chainnotinT then
// update KSMT
addthebranchintoT;
end
elseifÎº correpondstoanon-leafnodeÎ¾then
m
// mixed-randomness exploration strategy
dâ†thedegreeofthenodeÎ¾;
p= 1 ;
d+1
(cid:26)
Ïµ withprobabilityp
Ïµ= h ;
Ïµ withprobability1âˆ’p
l
end
else
Ïµ=Ïµ ;
h
end
end
WithprobabilityÏµselectarandomactiona ;
t
Otherwiseselecta âˆ¼Ï€ (s );
t Î¸ t
Obtainatuple(s ,a ,s ,r )byexecutinga ;
t t t+1 t t
Ï„ â†Ï„ âˆª{(s ,a ,s ,r )}
t t t+1 t
end
B FURTHER DISCUSSIONS
B.1 THEINSIGHTSBEHINDKEYSTATESDISCRIMINATION
In our considered scenarios, we claim that discrimination is generally easier and more universal
thankeystategenerationbyLLM,particularlyinthecontextofhigh-dimensionalstatesandpartial
observability. Thereasonsareasfollows:
1. Discriminationfocusesonhigh-leveltaskunderstandingandidentifyingkeystatecharac-
teristics,whilegenerationrequiresdetailed,low-levelcomprehension,assigningvaluesto
each element. This makes generation more challenging and error-prone, particularly in
high-dimensionalsettings. Discriminationequivalentlysimplifiestheoutputspacetokey
statelabels,thusalleviatingissueslikehallucinations.
2. In implementations, errors in discriminator functions are easier to examine and correct
throughtestingwithrealstates. Incontrast,errorsingeneratedkeystatesarehardertodetect
andaretypicallyinferredfromtrainingperformance.
3. Incasesofpartialobservability,generatingkeystatesdirectlyisunreliable. Forexample,in
thePasstask,thepositionsofhiddenswitchesareunknownandmustbeinferredfromthe
doorâ€™sstatus. LLMcannotgeneratekeystatesaccuratelywithoutknowledgeofthespecific
agentsâ€™positionsrequiredtoactivateaswitch.
B.2 LIMITATIONS
WebuildabridgebetweenLLMandRLtofacilitateefficientexplorationbyleveragingtask-related
guidanceprovidedbyLLM.However,persistentconstraintsinherenttoLLMs,suchastheirlimited
24capacitytocomprehendtask-specificinformationandtheinevitablehallucination,becomebottlenecks
inourapproach,whichinducesthefollowinglimitations:
1. Wemitigateheavypromptengineeringthroughtheuseofastandardizedprompttemplatebut
thenecessitypersistsformanuallyprovidingtaskinformationforLLM.Thus,weassumethe
availabilityofsemanticmeaningsforsymbolicstates. Thisassumptionisfeasible,asthese
manuallydesignedstateshaveinherentmeaningsdocumentedintaskspecifications(Liu
etal.,2021b;Samvelyanetal.,2019b),andisnostrongerthanpriorworksrequiringastate
captioner(Duetal.,2023)orenvironmentcode(Maetal.,2023),whichalsoinvolvemanual
fine-tuningoraccesstoadditionalstateinformation.
2. ConstrainedbythelimitationsofLLMâ€™scapabilitiesandtheinherentissueofhallucination,
itmayfacechallengesindirectlyprovidingeffectivekeystatepriorsformorecomplextasks.
Besides,duetoouruseofLLMs,thisworkprimarilyfocusesontaskswithsymbolicstates.
Futureresearchcouldextenditsapplicationtomorecomplicatedtasks,e.g.,image-based
tasks,byemployingadvancedmulti-modalLLMs. Weprovideaninitialattempttoextend
LEMAEbeyondsymbolictasksinAppendixF.2.
SincetheefficiencyoftheproposedLEMAEisessentiallyderivedfromversatileLLMs,webelieve
thatthesurgeoffoundationmodelexplorationwillflourishLLM-empoweredRL.
B.3 FUTUREWORKS
ThesuccessoftheproposedLEMAEhighlightsthenecessityandefficacyofempoweringRLwith
LLM. To enhance performance and extend applicability, we will explore two avenues for future
researchaimedataddressingtheidentifiedlimitations. Theseavenuesareoutlinedasfollows:
1. Streamlining the task information provision through multi-modal self-collection: Multi-
modalLLMsaregarneringincreasingattentionfortheirabilitytocomprehendsituations
throughvariousmodalities. Incorporatingthemwithself-explorationandmemorymecha-
nismsshowspromiseinautomatingthecollectionandunderstandingoftaskinformation,
therebystreamliningtheimplementationandenhancingtheadaptabilityofLEMAE.We
provideaninitialattempttoextendLEMAEbeyondsymbolictasksinAppendixF.2.
2. UnleashingthepowerofbetterLLMwithaniterativefeedbackmechanism: Undoubtedly,
given the rapid pace of LLM development, the emergence of more powerful LLMs is
imminent. On one hand, we intend to harness the capabilities of these advanced LLMs.
Ontheotherhand,tofullyunleashthepotentialofLLMs,weplantodeviseaniterative
feedbackmechanismtofeedbackLLMinLEMAEduringRLtrainingtomitigateissues
likehallucinationsanderrorsintaskunderstanding.
B.4 BROADERIMPACTS
Large Language Models have demonstrated considerable potential in showcasing impressive ca-
pabilitiesacrossvariousdownstreamtasks. However,researchonempoweringRLwithLLMsis
stillnascent. AsapioneeringendeavortoempowerRLwithLLM,weproposeageneralapproach
facilitatingefficientexplorationinRLwithtask-specificguidancefromLLM.
1. Fortheresearchcommunity,thepublicationofthisworkwillinspirefurtherexploration
intoencouragingtheintegrationofLLMswithRLtoaddresstheinherentchallengesinRL,
suchasefficientexploration,limitedsampleefficiency,andunsatisfactorygeneralization.
Additionally,ourdesignpromotestheapplicationofdiscriminationandcodingtoground
linguisticknowledgefromLLMsintosymbolictasks.
2. LEMAEshowspromiseforreal-worlddeploymentinscenariosrequiringefficientexplo-
ration,suchasautonomousvehiclecontrolandrobotmanipulation. Moreover,asLLMis
growingbyleapsandbounds,itisforeseeablethatLEMAEcanbeappliedtomorechal-
lengingreal-worldtasksbytakingadvantageofmorepowerfulLLM.Notably,tomitigate
potentialrisks,itisimperativetoconductLLMgenerationandRLtrainingunderhuman
supervision,therebyensuringundesirableoutcomesareaverted.
25C PROOF OF PROPOSITION 4.1
Proof. Random walk is a fundamental stochastic process, formed by successive summation of
independent,identicallydistributedrandomvariables(Lawler&Limic,2010). Thisworkconsiders
theone-dimensionalasymmetricrandomwalkproblem,whereanagentstartsatx=0andaimsto
reachx=N âˆˆN+,N >1. Theexpectedfirsthittingtimeconsideredasthemetricofperformance,
implying the average computational time complexity (Yu & Zhou, 2008). Below is the proof of
Proposition4.1.
Firstly,wecanprovetheexpectedfirsthittingtimewithinthedefaultsettingthroughtheapplication
ofmartingaletheory. Accordingtotheproblemsetting,wecandefinethemovementateachtime
stepas: M =0,M ,M ...arei.i.d. randomvariableswithdistributionP(M =1)=p,P(M =
0 1 2 i i
âˆ’1)=1âˆ’p,pâˆˆ(0.5,1). Thenthepositionofagentafternstepscanberepresentedas:
n
(cid:88)
S = M ,S =0 (3)
n i 0
i=1
However,becauseoftheasymmetryofrandomvariablesM ,{S ,n â‰¥ 0}doesnotpertaintothe
i n
martingalew.r.t.{M ,nâ‰¥1}. Itâ€™sobservedthatE(M )=2pâˆ’1,iâ‰¥1. Then,wecandefine:
n i
n
(cid:88)
Y = (M âˆ’(2pâˆ’1)),Y =0 (4)
n i 0
i=1
Itâ€™seasytoprovethat
n
(cid:88)
E|Y |= E|M |âˆ’n(2pâˆ’1)=2nâˆ’2np<âˆž (5)
n i
i=1
E(Y |M ,M ,...M )=Y +E(M )âˆ’(2pâˆ’1)=Y (6)
n+1 0 1 n n n+1 n
So,accordingtothedefinition,{Y ,nâ‰¥0}isamartingalew.r.t.{M ,nâ‰¥1}
n n
LetT = min{n : S = 0,S = N} = min{n : Y = 0,Y = N âˆ’nâˆ—(2pâˆ’1)}. Itâ€™sclear
0â†’N 0 n 0 n
thatT isastoppingtimew.r.t.{M ,nâ‰¥1}.
0â†’N n
Itâ€™seasytoprovethat
E(|Y âˆ’Y ||M ,M ,...M )=E(|M |)âˆ’(2pâˆ’1)=2âˆ’2p<2 (7)
n+1 n 0 1 n n+1
WecanassumethatE(T )<âˆž. Then,accordingtotheOptionalStoppingTheorem(Durrett,
0â†’N
2019),wecanget
E(Y )=N âˆ’E(T )âˆ—(2pâˆ’1)=E(Y )=0 (8)
T0â†’N 0â†’N 0
Then
N
E(T )= (9)
0â†’N 2pâˆ’1
TheassumptionE(T )<âˆžistherebyvalidated. Consequently,theexpectedfirsthittingtime
0â†’N
withinthedefaultsettingisE(T ) = N ,aconclusionalsoarticulatedinTheorem4.8.9of
0â†’N 2pâˆ’1
Durrett(2019).
We can introduce the task-relevant information that the agent must first reach key states: Îº =
1,...,N âˆ’1beforeprogressingtox = N. Itispresupposedthateverytimetheagentachievesat
x = Îº,thepolicywherex < Îºisupdatedtoadeterministicrightwardmovement,i.e.,P(M =
x
1)=1,x<Îº,therebyemulatingtheupdateprocessinReinforcementLearning.
Theexpectedfirsthittingtimefromx=0tox=1isE(T )= 1 . Afterreachingx=1,the
0â†’1 2pâˆ’1
expectedfirsthittingtimefromx=1tox=2canbecalculatedas:
âˆž
(cid:88) 2
E(Tprior)=pâˆ— (2n+1)(1âˆ’p)= âˆ’1 (10)
1â†’2 p
n=0
26Similarly,wecaneasilyprovethat
(cid:18) (cid:19)
2
E(Tprior)=(N âˆ’1)âˆ— âˆ’1 (11)
1â†’N p
Consequently,theexpectedfirsthittingtimeaftertheintegrationofpriorsbecomesE(Tprior) =
0â†’N
(cid:16) (cid:17)
E(T )+E(Tprior)= 1 +(N âˆ’1)âˆ— 2 âˆ’1 .
0â†’1 1â†’N 2pâˆ’1 p
ThetotaladvantageresultingfromtheintegrationofappropriatepriorsisexpressedasE(T )âˆ’
0â†’N
E(Tprior)=(N âˆ’1)âˆ—( 1 âˆ’ 2 +1)>0,pâˆˆ(0.5,1),N âˆˆN+,N >1
0â†’N 2pâˆ’1 p
D LLM PROMPTS AND RESPONSES
Herearetheexamplepromptandresponseinourwork. Pleasereferencethecodeforfurtherdetails.
Notably,weadoptthechain-of-thoughttechniquefromWeietal.(2022).
SMACPromptandResponseExample
SYSTEM:
(Task Description)
WeareplayingStarCraftIImicroscenario,tringtocontrolouragentstodefeatallofthe
enemyunits.
(State Form)
Ineachstep,thecurrentstateisrepresentedasa1-dimensionallist:
[nf al]âˆ—n agents+[nf en]âˆ—n enemies+[last actions].
nf aldenotestheunitstateforeachagentwithattributes
[health rate,weapon cooldown rate,relative x to map center,
relative y to map center,shield rate(1dimensionifa raceisPelse0dimension),
unit type bits(thedimensionisdefinedinthemapconfig)].
nf enrepresentstheunitstateforeachenemywithattributes
[health rate,relative x to map center,relative y to map center,
shield rate(1dimensionifb raceinmapconfigisPelse0dimension),
unit type bits(thedimensionisdefinedinthemapconfig)].
Thelast actionscomponentdoesnotrequireconsideration.
(Role Instruction)(Template)
Yourroleistogiveseveralcriticalkeystatesinthetaskwhichweshouldtrytoreachand
generatethecorrespondingdiscriminatorfunctionforeachkeystatewhichcandiscriminate
iftheinputstatehasreachedthekeystate.
Note:
1. Donâ€™tusetheinformationyouarenottold.
2. Thecodeshouldbeasgenericaspossible.
3. Thediscriminatorfunctionsfordifferentkeystatesshouldbeindependent.
4. Youranswershouldbecompleteandnotomitted.
PleasethinkstepbystepandadheretothefollowingJSONformat(justreplacethe()with
youranswer):
{
27Thought: (Pleasefullyunderstandthetask. Thinkstepbystepandgiveyourthought
abouttheconcretestatehere),
Keystates: {
init: allagentsandenemiesarealive,
key state 1: (description),
key state 2: (description),
...,
success: allenemiesaredeadandatleastoneagentisalive
},
Discriminators: {
key state 1: (apythonfunctionwiththeformofdefiskeystate1(state): ... return0or1),
key state 2: (apythonfunctionwiththeformofdefiskeystate2(state): ... return0or1),
...
}
Subspace states: {
key state 1: (extractsubspacestateusedincorrespondingdiscriminatorfunctions,such
as[1]or[2,5]),
key state 2: (extractsubspacestateusedincorrespondingdiscriminatorfunctions,such
as[1]or[2,5]),
...
}
}
USER:
Taskis2m vs 1z. Themapconfigis: {
n agents: 2,n enemies: 1,limit: 150,a race: T,b race: P,unit type bits: 0,map type:
marines
}.
Givemorethan2keystatesandthecorrespondingdiscriminatorfunctions.
ASSISTANT:(LLMresponse)
{
Thought: Since a race is T, there is no shield rate in nf al; b race is P, so there
is 1-dim shield rate in nf en; the dim of unit type bits is 0. The dimension of nf al
= 4+0+0 = 4, the dimension of nf en = 3+1+0 = 4. The concrete state form is
[agent 0 health rate, agent 0 weapon cooldown rate, agent 0 relative x to map center,
agent 0 relative y to map center, agent 1 health rate, agent 1 weapon cooldown rate,
agent 1 relative x to map center, agent 1 relative y to map center, enemy 0 health rate,
enemy 0 relative x to map center,enemy 0 relative y to map center,enemy 0 shield rate,
last actions]. Keystatesshouldfocusonthehealthandshieldstatusoftheenemyandthe
healthofouragents.,
Key states: {
init:allagentsandenemiesarealive,
key state 1: enemy 0â€™sshieldissignificantlydamaged,
key state 2: enemy 0issignificantlydamaged,
key state 3: enemy 0isabouttodie,
success:allenemiesaredeadandatleastoneagentisalive
},
Discriminators: {
key state 1: defiskeystate1(state): return1ifstate[11]<0.6else0,
key state 2: defiskeystate2(state): return1ifstate[8]<0.6else0,
key state 3: defiskeystate3(state): return1ifstate[8]<0.05else0,
}
28Subspace states: {
key state 1: [11],
key state 2: [8],
key state 3: [8],
}
}
USER:
(LLMRethinking)
Pleasecarefullycheckyourresponsetomakesureitmeetsthetaskrequirementsandnotes
withoutincludingunnecessarydetails. Also,confirmthatthediscriminatorfunctionsdonot
useanyundefinedvariables.
ASSISTANT:(LLMresponse)
...... (WehaveomittedtheintermediateLLMoutputstomaintainconciseness,retainingonly
theinitialgeneration,asthedifferencesbetweentheinitialandrecheckedgenerationsare
minimalintheabsenceoferrors.)
MPEPromptandResponseExample
USER:
(Task Description)
Twoagentsareina30x30roomwiththecoordinateofthepointintheupperleftcorneris
(0,0)andthepointinthelowerrightcorneris(29,29),separatedbyawallwithadoor.
The door can not be directly open. There is a hidden switch in each room (The switch
coordinatesareunknownanddifferfromthecoordinateofthedoor.). Thedoorwillbeopen
onlywhenoneoftheswitchisoccupiedbyanagentotherwiseclose. Bothagentsbeginin
thelefthalfoftheroom(agent0.x<15andagent1.x<15).
Thetaskisconsideredsuccessfullycompletedaslongasbothagentsarelocatedintheright
half-room(agent0.x>15andagent1.x>15).
(State Form)
Ineachstepwecangetcurrentstate. Thestateisa1-dimlist:
(agent0.x,agent0.y,agent1.x,agent1.y,door.open).
(Role Instruction)(Template)
Yourroleistogiveseveralcriticalkeystatesinthetaskwhichweshouldtrytoreachand
generatethecorrespondingdiscriminatorfunctionforeachkeystatewhichcandiscriminate
iftheinputstatehasreachedthekeystate.
Note:
1. Donâ€™tusetheinformationyouarenottold.
2. Thecodeshouldbeasgenericaspossible.
3. Thediscriminatorfunctionsfordifferentkeystatesshouldbeindependent.
4. Trynottoputtoomuchdistancebetweenthekeystates.
PleasethinkstepbystepandadheretothefollowingJSONformat(justreplacethe()with
youranswer):
{
Thought: (Pleasefullyunderstandthetask. Thinkstepbystepandgiveyourthought
abouttheconcretestatehere),
Keystates: {
init: initstate,
key state 1: (description),
key state 2: (description),
29...,
success: successstate
},
Discriminators: {
key state 1: (apythonfunctionwiththeformofdefiskeystate1(state): ... return0or1),
key state 2: (apythonfunctionwiththeformofdefiskeystate2(state): ... return0or1),
...
},
Subspace states: {
key state 1: (extractsubspacestateusedincorrespondingdiscriminatorfunctions,such
as[1]or[2,5]),
key state 2: (extractsubspacestateusedincorrespondingdiscriminatorfunctions,such
as[1]or[2,5]),
...
}
}
ASSISTANT:(LLMresponse)
{
Thought: Tocompletethetask,weneedtofindthehiddenswitchinoneoftherooms
toopenthedoor,thenmovebothagentsthroughthedoortotherighthalfoftheroom. We
shouldestablishkeystatesthatrepresentthediscoveryoftheswitch,theopeningofthedoor,
andthesuccessfultransitionofbothagentstotherighthalfoftheroom. Weâ€™llassumethat
agent0isresponsibleforfindingtheswitchandagent1isresponsibleformovingthroughthe
dooronceitâ€™sopen.,
Key states: {
init:Initialstatewherebothagentsareinthelefthalfoftheroom.,
key state 1:Agent0hasfoundtheswitchandisstandingonit,openingthedoor,
key state 2:Agent1hasmovedthroughtheopendoortotherighthalfoftheroomwhile
agent0isstillontheswitch,
key state 3:Agent0hasjoinedagent1intherighthalfoftheroom
},
Discriminators: {
key state 1:defiskeystate1(state): return1ifstate[4]==1andstate[0]>15else0,
key state 2:defiskeystate2(state): return1ifstate[4]==1andstate[2]>15andstate[0]
<15else0,
key state 3:defiskeystate3(state): return1ifstate[0]>15andstate[2]>15else0
},
Subspace states: {
key state 1: [4,0],
key state 2: [0,2,4],
key state 3: [0,2],
}
}
USER:
(LLMRethinking)
Pleasecarefullycheckyourresponsetomakesureitmeetsthetaskrequirementsandnotes
withoutincludingunnecessarydetails. Also,confirmthatthediscriminatorfunctionsdonot
useanyundefinedvariables.
30ASSISTANT:(LLMresponse)
...... (WehaveomittedtheintermediateLLMoutputstomaintainconciseness,retainingonly
theinitialgeneration,asthedifferencesbetweentheinitialandrecheckedgenerationsare
minimalintheabsenceoferrors.)
25
20
15
10
5
0
Pass Push-BoxSecret-RoomLarge-Pass 2m_vs_1z 1c3s5z 5m_vs_6m 3s_vs_5z3s5z_vs_3s6z MMM2
Figure9: ThenumberofLLM-generatedkeystatediscriminatorfunctions.
Notably,thenumberofkeystatesisprimarilydeterminedbyLLM.Foreachtask,weonlyprompt
LLMtopreventitfromgeneratingtoofewfunctionsaccordingtothecomplexityoftheenvironment.
Specifically,asdetailedinAppendixD,weinstructLLMtogenerateseveralcriticalkeystatesfor
MPEandmorethan2*n enemiescriticalkeystatesforSMAC.
AsshowninFig.9,wesummarizethenumberofLLM-generatedkeystatediscriminatorfunctions.
Itisnotablethatthenumberofdiscriminatorfunctionsincreaseswiththedifficultyofthetaskorthe
numberofinteractiveobjectsintheenvironment,whichalignswithintuition.
Additionally,wehaveomittedtheintermediateLLMoutputstomaintainconcisenessinthissection,
retainingonlytheinitialgeneration,asthedifferencesbetweentheinitialandrecheckedgenerations
areminimalintheabsenceoferrors.
E IMPLEMENTATION DETAILS
E.1 LEMAEANDBASELINES
LEMAE:Ourcodeisbasedonthewidely-usedcodeframeworkpymarl2athttps://github.
com/hijkzzz/pymarl2. Inthisstudy,wehaveintegratedourmethodwithseveralbasealgo-
rithmsIPPO,QMIX,QPLEX,andVMIX.Throughouttheintegrationprocess,wehaverefrained
from modifying the algorithmic implementation and have maintained consistency in parameters,
includingbatchsize,learningrate,andlosscoefficients,inalignmentwiththeconfigurationsofthe
basealgorithms.
EITIandEDTI(Wangetal.,2019a): WecompareourmethodwithEITIandEDTIonMPEtasks
proposedinLiuetal.(2021b). WeusetheexperimentresultsreportedinLiuetal.(2021b),which
foundthatthesealgorithmsperformpoorbecausealongrollout(512stepsÃ—32processes)between
modelupdatesisused.
CMAE(Liuetal.,2021b): WecompareourmethodwithCMAEonMPEandSMACtasks. On
MPEtasks,theresultsofCMAEarereproducedusingthepubliclyavailablecodereleasedbythe
authorsathttps://github.com/IouJenLiu/CMAE. AsCMAElacksanimplementationfor
SMAC,weusetheresultsreportedintheoriginalpaper.
MAVEN (Mahajan et al., 2019): We use the code at https://github.com/
starry-sky6688/MARL-Algorithms, whichcontainspytorchimplementationsofvarious
MARLalgorithmsonSMAC,likethechoiceinLAIES(Liuetal.,2023).
RODE(Wangetal.,2020d) andMASER (Jeonet al.,2022): Weutilizedthe publiclyavailable
codeprovidedbytheauthors,accessibleathttps://github.com/Jiwonjeon9603/MASER
andhttps://github.com/TonghanWang/RODE,respectively. Defaultconfigurationswere
employed,andtheirsuboptimalperformanceisalsodocumentedinLAIES(Liuetal.,2023).
31
muN
etatS
yeKLAIES(Liuetal.,2023): Weemployedthepubliclyaccessiblecodeprovidedbytheauthors,which
canbeaccessedathttps://github.com/liuboyin/LAIES. Whenconductingexperiments
onSMAC,weadheredtothedefaultconfigurationsandexternalstates. Notably,theoriginalLAIES
paperevaluationdidnotincludeassessmentsontheMPE.Consequently,weintegratedtheMPE
environmentintotheLAIEScodebase,designatingtheexternalstatestorepresentthedoorstatusor
thepositionofthebox.
ELLM (Du et al., 2023): Since the tasks in this work have clearly defined goals, we minimize
LLM inference costs by following the ELLM methodology but adapting its goal generation to
occur only once at the start of the training. Consistent with the hyperparameters in the official
codebase https://github.com/yuqingd/ellm, we set the similarity threshold to 0.99,
rewarding only when the goal is achieved. We rely on LLM-generated functions to verify goal
achievement, which we found to be more effective than directly using semantic similarity-based
rewards.
Forallalgorithms,weensurethesameenvironmentalsettings,includingobservationspace,environ-
mentrewardfunction,andsoon.
E.2 COMPARISONWITHLLMREWARDDESIGN
WeconductadditionalexperimentscomparingLEMAEwithabaselinecalledEureka-si,whichcan
beseenasasingle-iterationvariantofEureka(Maetal.,2023),whereLLMdesignsrewardsdirectly.
Forfairness,wedoesnotadoptevolutionaryoptimizationinEurekaanduseLLMtogeneratereward
functionswiththesameroleinstructionsasinEureka,whilemaintainingdesignslikeSelf-Checkas
inLEMAE.AsshowninFig.3,Eureka-siiscomparabletoLEMAEinsimpletaskslikePush-Box
butfailsinchallengingtaskswithcharacteristicslikepartialobservability,suchasPass,wherehidden
switchesmakeitdifficulttodesigneffectiverewardfunctions. Incontrast, LEMAEconsistently
demonstratesimpressiveperformance. Notably, comparingLEMAEwithEurekadirectlywould
be unfair since Eurekaâ€™s evolutionary search requires multiple training iterations and candidates,
leadingtosignificantlymoresamplingandtrainingthanLEMAE.Overall, LEMAEâ€™sadvantage
overRLalgorithmsliesinincorporatingpriorknowledgefromtheLLM,anditsadvantageover
other LLM-based methods is due to our designs for better LLM incorporation, such as utilizing
discrimination,SHIR,andKSMT.
E.3 CONNECTIONANDCOMPARISONWITHHER
The proposed Key State-Guided Exploration is similar to Hindsight Experience Replay
(HER)(Andrychowiczetal.,2017)inform,wherekeystatesandsubgoalsarecertainstatesfrom
sampledtrajectories. However,unlikeHER,whichsamplesgoalsfrommemoryusingrandomor
heuristicstrategiesandoftenstruggleswithshapedrewards,ourmethodincorporatesLLMpriorsfor
moretargetedgoalselection(keystateslocalization). Additionally,theproposedKSMTandSHIR
facilitateorganizedexplorationandenhancedrewardguidance.
Weconductadditionalexperimentstofurtherconfirmtheadvantagesofourmethod. Weevaluate
HERwithIPPOasthebackboneinMPE.Weusethefuturestrategyforgoalselection,asproposedin
theHERpaper,andemployarewardfunctionbasedontheManhattanDistance,whichwefindtobe
thebestmatch. However,asdepictedinFig.3,HERdoesnotperformwellonbothMPEtasks. This
outcomesuggeststhattherandomsamplingstrategyforgoalsmaynotbesufficient,underscoringthe
importanceofincorporatingLLMpriorsforefficientexplorationasweproposed.
E.4 TASKS
E.4.1 MULTIPLE-PARTICLEENVIRONMENT(MPE)
TheMultiple-ParticleEnvironmentservesasawidely-adoptedbenchmarkformulti-agentscenarios.
Inthiswork,weemploytasksspecificallycraftedforevaluatingmulti-agentexploration,proposed
byWangetal.(2019a). TheimplementationutilizedinthisstudyisbasedontheworkbyLiuetal.
(2021b). Inthissection,weprovidedetailsofthefoursparse-rewardtasksweadopted.
â€¢ Pass: In the Pass task, depicted in Fig. 10a, two agents are positioned in a room of 30 x 30
grid. Theroomisdividedintotwohalvesbyawallfeaturingadoor. Eachhalf-roomcontainsan
32invisibleswitch,thedetailsofwhicharenotcontainedinthestateorpromptforLLM.Thedoor
permitspassageonlywhenoneoftheswitchesisoccupiedbyanagent. Initiallysituatedwithinthe
lefthalf-room,bothagentsmustcooperatetotransfertotherighthalf-room. Theexternalreward
functionisdenotedasr =I(twoagentsareintherightroom),whereI representstheindicator
E
function.
â€¢Secret-Room: Secret-RoomisanextensiontaskofPass. AsillustratedinFig.10b,theconfiguration
comprises one sizable room on the left and three smaller rooms on the right, interconnected by
three doors. Within each room, there is an invisible switch; notably, the switch in the left room
hasthecapabilitytocontrolallthreedoors,whereaseachrightroomâ€™sswitchexclusivelycontrols
its respective door. The grid size is 25 x 25. Two agents are initialized within the left room and
arerequiredtocollaborateinordertotransitiontotherealtargetroom,whichistherightroom2.
Theexternalrewardfunctionisdenotedasr =I(twoagentsareintherightroom2),whereI
E
representstheindicatorfunction.
â€¢ Push-Box: As depicted in Fig. 10(c), two agents and a box are initially positioned within a 15
x15grid. Tosuccessfullymovethebox,bothagentsmustsimultaneouslyexertforceinthesame
direction. Thetaskisdeemedaccomplishedwhentheboxissuccessfullypushedtothewall. The
externalrewardfunctionisdenotedasr =I(theboxispushedtothewall),whereI represents
E
theindicatorfunction.
â€¢Large-Pass: Large-PassisadirectextensiontaskofPassbyenlargingthegriddimensionsto50x
50,whichmakesitmorechallenging. TheexternalrewardfunctionalignswiththatofthePasstask.
Thedetailsofthesetasks,includingobservationspaceandactionspace,arelistedinTable3.
Table3: DetailsofMPEtasks
MPEtasks n agents observationspace statespace actionspace
Pass 2 5 5 4
Secret-Room 2 5 5 4
Push-Box 2 6 6 4
Large-Pass 2 5 5 4
Figure10: MPETasks.
E.4.2 STARCRAFTIIMULTI-AGENTCHALLENGE(SMAC)
StarCraftIIMulti-AgentChallenge(SMAC)(Samvelyanetal.,2019a)isawidely-usedbenchmark
intherealmofcooperativemulti-agentreinforcementlearningresearch(Rashidetal.,2018;Shao
etal.,2023;Liuetal.,2023;Shaoetal.,2024). Derivedfromtherenownedreal-timestrategygame
StarCraftII,SMACconcentratesspecificallyondecentralizedmicromanagementscenariosrather
thanthefullgame. Typically, thetaskswithinSMACadoptadense-rewardframework,wherein
agentsreceivedenserewardsfordamagereceived,attackingandeliminatingenemies. Topromotethe
needforexploration,weadoptfullysparse-rewardversionsoftasksinSMACwhereagentsaresolely
rewardeduponthesuccessfuleliminationofallenemies. Theexternalrewardfunctionisdenoted
33asr =I(allenemiesareeliminated),whereI representstheindicatorfunction. Notably,this
E
sparse-rewardsettingdiffersfromthesparseSMAC,whichcanbecalledsemi-sparseSMAC,used
insomepreviousstudies(Jeonetal.,2022;Joetal.,2023),whereagentsarerewardedwhenone
orallenemiesdieorwhenoneallydies. Inaddition,tovalidatetheversatilityofLEMAEacross
diversescenarios,weconductedexperimentsonsixmapswithdifferentdifficultyanddiverseagent
numbers,asillustratedinTable4andTable5. WeusetheversionofSC2.4.10. Pleaserefertothe
officialdocument2formoredetails.
Table4: SMACtasks.
Task AllyUnits EnemyUnits Type Difficulty
1Colossi, 1Colossi,
1c3s5z 3Stalkers, 3Stalkers, heterogeneous&symmetric Easy
5Zealots 5Zealots
2m vs 1z 2Marines 1Zealot micro-trick: alternatingfire Easy
3s vs 5z 3Stalkers 5Zealots micro-trick: kiting Hard
5m vs 6m 5Marines 6Marines homogeneous&asymmetric Hard
3Stalkers, 3Stalkers,
3s5z vs 3s6z heterogeneous&asymmetric Super-Hard
5Zealots 6Zealots
1Medivac, 1Medivac,
MMM2 2Marauders, 3Marauders, heterogeneous&asymmetric Super-Hard
7Marines 8Marines
Table5: DetailsofSMACtasks
SMACtasks n agents n enemies observationspace statespace actionspace
2m vs 1z 2 1 16 26 7
1c3s5z 9 9 162 270 15
3s vs 5z 3 5 48 68 11
5m vs 6m 5 6 55 98 12
3s5z vs 3s6z 8 9 136 230 15
MMM2 10 12 176 322 18
E.4.3 ABRANDNEWTASK: River
ToexcludetheprobabilitythatLEMAEâ€™ssuccessreliesonLLMâ€™sfamiliaritywiththechosentasks,
weâ€™vedesignedabrandnewtask,termedRiver,whichLLMhasneverencounteredbefore. Thetask
isdetailedasfollows:
TheRivertaskisadaptedfromtheMultiple-ParticleEnvironmentanditsmapisillustratedinFig.8a.
Two agents, Alice and Bob, are placed in a 30 x 30 grid field intersected by two rivers running
verticallyandhorizontally.Amountaininthebottom-leftcornerobstructsthepassage.AliceandBob
startrandomlyinthetop-leftpartofthefieldandneedtomovetothebottom-rightpart. However,
AliceisafraidofwaterandcannotcrosstheriverunlessBobstaysintherivertoactasabridgefor
her.
Theobservationspaceisdiscretewithfourdimensions,representingthepositionsoftwoagents,i.e.,
o=[x ,y ,x ,y ]. Theactionspaceisalsodiscrete,allowingmovementinfourdirections. Agents
1 1 2 2
receiveapositiverewardonlywhenbothagentsreachthebottom-rightcornerofthefield.
E.5 HYPERPARAMETERS
InLEMAE,weintroducethreeimportanthyperparameters: extrinsicrewardscalingrateÎ±,intrinsic
rewardscalingrateÎ²,andhighrandomnessepsilonÏµ . Notably,thelowrandomnessepsilonÏµ isthe
h l
hyperparameterinthebasealgorithms,suchas0.05forQMIXand0.0forIPPO.
2https://github.com/oxwhirl/smac/blob/master/docs/smac.md
34For MPE, we adopt {Î± = 10,Î² = 0.1,Ïµ = 1} on Pass, Secret-Room, and Large-Pass and use
h
{Î±=10,Î² =0.05,Ïµ =0.2}onPush-Box.
h
ForSMAC,weadopt{Î±=50,Î² =1,Ïµ =0.5}onMMM2and1c3s5z,{Î±=10,Î² =1,Ïµ =0.5}
h h
on3s vs 5zand2m vs 1z,{Î±=1,Î² =1,Ïµ =0.5}on5m vs 6mand3s5z vs 3s6z.
h
E.6 RESOURCES
Weuseaserverwith8*NVIDIARTX3090GPUs,and2*AMD7H12CPUstorunalltheexperiments.
Withoutspecifying,eachsettingisrepeatedfor5seeds. ForoneseedinSC2,therunningtimeranges
fromapproximately50minutesto12hours. ForMPE,therunningtimevariesfromaround3to7
hours. TheinputforeachLLM(GPT-4-1106-preview)inferencecomprisesapproximately600-4000
tokens(0.006-0.04dollars),yieldinganoutputofabout300-1600tokens(0.009-0.048dollars).
F ADDITIONAL EXPERIMENTAL RESULTS
F.1 MORECOMPLICATEDBENCHMARK: SMACV2
WehaveevaluatedLEMAEonthreetypicaltasks,protoss 5 vs 5,terran 5 vs 5,andzerg 5 vs 5,
inSMACv2(Ellisetal.,2024)todemonstrateitseffectivenessunderstochasticdynamicssettings.
WeutilizedthetypicalhyperparametersforbothLEMAEandLAIESasusedinSMAC.Asshown
inFig.11,LEMAEachievesoutstandingperformance,confirmingitsapplicabilitytosuchsettings.
ThisresultfurtherdemonstratesLEMAEâ€™spotentialforreal-worldscenarioswithcomplexityand
stochasticity.
LEMAE(Ours) QMIX-DR LAIES
protoss_5_vs_5 terran_5_vs_5 zerg_5_vs_5
0.8 0.8 0.6
0.5
0.6 0.6
0.4
0.4 0.4 0.3
0.2
0.2 0.2
0.1
0.0 0.0 0.0
0 2 4 6 8 0 2 4 6 8 0 2 4 6 8
T (mil) T (mil) T (mil)
Figure 11: Evaluating LEMAE against baseline methods across three maps in SMACv2, which
featuresgreaterrandomness,usingtheevaluationmetricoftestwinrate. QMIX-DRisQMIXwith
denserewardsintheoriginalSMACv2. ThisresultfurtherdemonstratesLEMAEâ€™spotentialfor
real-worldscenarioswithcomplexityandstochasticity.
F.2 EXTENDINGLEMAEBEYONDSYMBOLICTASKS
Thisworkprimarilyfocusesontaskswithsymbolicstatespaces,wherestatesarerepresentedas
symbolicarraysdescribingtheagentandenvironment. AsdiscussedinAppendixB.3,toextend
LEMAEfromsymbolictaskstovision-basedtasks,wecanexchangetheLLMforamulti-modalLM
inLEMAEforkeystatelocalization. ToconfirmtheapplicabilityofLEMAEtovision-basedtasks,
weconductademonstrativeexperiment: WeextendthetaskPasstoavision-basedtaskVisual-Pass,
asillustratedinFig.12a. WepromptaLLMtodefinekeystateswiththesametaskdescriptionand
roleinstructionasproposedinSec.4.2andusetheLLM-generateddefinitionasthepromptfora
VisionLanguageModel(GPT-4o). Then,itispromptedtodiscriminatekeystatesintherandomly
sampledstates. GPT-4oachievesa98%accuracyrateindiscriminatingkeystatesamongthe50
sampledimagestates. ThisconfirmsthatwithaproperextensionoftheLLM,LEMAEcaneliminate
dependenceonstatesemanticsandbeappliedtoothertaskssuchasvisual-input.
35
etaR
niW
tseTFigure12: (a)Anillustrationofthevision-basedtaskVisual-Pass. (b)ThepromptfortheVision
LanguageModel(VLM),whichincludesthetaskdescription,roleinstruction,imagestateandkey
statesdefinitionprovidedbyanotherLLM.TheVLMistaskedwithdeterminingwhetherthegiven
imagestatecorrespondstoakeystate.
F.3 ABLATIONSTUDIESONMIXED-RANDOMNESSEXPLORATION
AsdemonstratedinFig13,weconductanablationstudyonmixed-randomnessexplorationwithin
the3s vs 5zmap. ResultsindicatethatLEMAEexhibitsinsensitivitytotheparameterÏµ ,provided
h
thatthelevelofrandomnessremainsmoderate,asopposedtobeingexcessivelyextreme(0.1or0.9).
Besides,theeffectivenessofourdesignishighlightedthroughacomparisonbetweenLEMAEandits
variants,namely,onlyleafnodeandLEMAEw/oKSMTE.
h=0.1 h=0.3 h=0.7 h=0.9
LEMAE h=0.5 only leaf node LEMAE w/o KSMTE
3s_vs_5z
1.0
0.8
0.6
0.4
0.2
0.0
0 1 2 3 4
T (mil)
Figure13: Ablationstudyonmixed-randomnessexploration. Thedefaulthyperparameterusedin
LEMAEisÏµ =0.5. RobustnessevaluationincludedvariationsinÏµ =[0.1,0.3,0.7,0.9]. Theonly
h h
leafnodedenotessimplyusingÏµ forthenon-leafnodeandÏµ fortheleafnode. TheLEMAEw/o
l h
KSMTEdenotesLEMAEwithoutmixed-randomnessexploration.
F.4 EXPERIMENTSFORSINGLE-AGENTSETUPS
Indeed, weproposeLEMAEasageneralapproachforLLM-empoweredefficientexplorationin
reinforcementlearning,applicabletobothsingle-agentandmulti-agentsettings. Weunderscorethe
evaluationofitsperformanceinmulti-agentsettingsduetoitsinherentcomplexity.
Astheproposedmethodcanseamlesslyextendtosingle-agentscenarios,weintroduceasingle-agent
variantofMPEandassessPPO(Schulmanetal.,2017)andPPO-basedLEMAEacrossfourtasks.
Weruneachalgorithmusingthreerandomseedswith300kenvironmentsteps,usingtheevaluation
metricofthetestwinrate. ThefollowingtableshowsthatLEMAEcanfacilitateefficientexploration
insingle-agentscenarios.
36
etaR
niW
tseTTable6: FinaltestwinrateofLEMAEandPPOonsingle-agentvariantofMPEtasks.
SingleMPE PPO LEMAE
SinglePass 0.00Â±0.00 1.00Â±0.00
SingleSecret-Room 0.00Â±0.00 0.98Â±0.01
SingleLarge-Pass 0.00Â±0.00 0.99Â±0.01
SinglePush-Box 0.00Â±0.00 0.96Â±0.08
F.5 DISCUSSIONONTHEKSMT
UsingKSMTcouldposealimitationduetopotentialmemorycostsincertainscenarios. However,
thishasnotbeenasignificantissueinourexperiments,asthekeystatesarerelativelyfew,primarily
focusingonthemostcriticalones,withanaturalsequentialrelationshiptypicallyexistingbetween
them. Notably,LEMAEisalsocompatiblewithothermemorystructures,suchasDirectedAcyclic
Graphs(DAGs),whichcouldbeaninterestingdirectionforfutureexploration.
TodemonstratetheeffectivenessofLEMAEwithothermemorystructures,inscenarioswheretask
completionfollowsalinearpattern(e.g.,Initâ†’Aâ†’B â†’Success),weemployamoreefficient
strategybyusingaKSMTvariantwithasinglebranchrepresentingthesequentialorderofkeystates.
Specifically, wesystematicallyassignapriorityvaluetoeachkeystate, continuouslyupdatingit
basedonitsoccurrenceorderwithinthesequenceofattainedkeystates. Thedeterminationofthe
rankingofkeystateswithintheone-branchKSMTreliesonthisestablishedpriority.
AsillustratedinFig.14,anablationstudyisconductedtocomparetheperformancebetweenraw
KSMTandtheone-branchKSMTvariantacrosssixmapsinSMAC.Theresultsdemonstratethe
increasednecessityofemployingtheone-branchKSMTvariantfortasksinvolvingalargernumberof
agentsandgreatercomplexity,suchas5m vs 6m,3s5z vs 3s6z,andMMM2. Consequently,wehave
adoptedtheone-branchKSMTapproachforthesespecificSMACtasks: 5m vs 6m,3s5z vs 3s6z,
andMMM2.
One-Branch KSMT
MMM2 3s5z_vs_3s6z 5m_vs_6m
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 5 0 2 4 6 0 1 2 3 4 5
3s_vs_5z 2m_vs_1z 1c3s5z
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 0.0 0.2 0.4 0.6 0.8 0.00 0.25 0.50 0.75 1.00 1.25 1.50
T (mil) T (mil) T (mil)
Figure 14: Ablation study conducted to compare the performance between raw KSMT and the
one-branchKSMTvariantacrosssixmapsinSMAC.
F.5.1 WORKINGWITHDENSEREWARDSETTINGS
WealsoevaluateLEMAEintaskswithdenserewardsinSMAC,denotedasLEMAE-DR.Asshown
inFig.15,theresultsconfirmthatLEMAE-DRfacilitatesefficientexplorationinbothdenseand
37
etaR
niW
tseT
etaR
niW
tseTsparserewardsettings,highlightingthemaincontributionofourmethod. Additionally,LEMAE-DR
achievesbetterconvergencethanLEMAEduetotheguidanceprovidedbydenserewards.
LEMAE(Ours) QMIX-DR LEMAE-DR
3s5z_vs_3s6z 3s_vs_5z 2m_vs_1z
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0.0 0.2 0.4 0.6 0.8 1.0
T (mil) T (mil) T (mil)
Figure15: EvaluatingLEMAEwithdenserewardsacrossthreemapsinSMACusingtheevaluation
metricoftestwinrate. LEMAE-DRisLEMAEwithdenserewardsintheoriginalSMAC,which
effectivelyensuresefficientexplorationandachievesbetterconvergence.
F.6 ADDITIONALRESULTS
Table7: FinaltestwinrateofLEMAEandcomparablebaseline(CMAE)onMPEtasks.
MPE LEMAE(Ours) CMAE
Pass 1.00Â±0.00 0.75Â±0.43
Secret-Room 1.00Â±0.00 0.80Â±0.40
Push-Box 1.00Â±0.00 0.80Â±0.40
Large-Pass 1.00Â±0.00 0.00Â±0.00
Table8:FinaltestwinrateofLEMAEandcomparablebaseline(LAIES)onSMACtasks.QMIX-DR
denotestrainingQMIXwithdensereward.
SMAC LEMAE(Ours) LAIES QMIX-DR
1c3s5z 0.98Â±0.02 0.89Â±0.09 0.99Â±0.01
2m vs 1z 1.00Â±0.01 0.73Â±0.24 1.00Â±0.01
3s vs 5z 0.83Â±0.07 0.10Â±0.12 0.66Â±0.16
5m vs 6m 0.74Â±0.08 0.68Â±0.10 0.78Â±0.08
3s5z vs 3s6z 0.73Â±0.14 0.45Â±0.35 0.73Â±0.07
MMM2 0.89Â±0.08 0.62Â±0.25 0.90Â±0.05
Inthissection,weprovidesomeadditionalexperimentalresults.
AsdemonstratedinTable7andTable8,weaugmentthefinaltestwinrateofourproposedmethod,
LEMAE,withcomparablebaselinealgorithmsinMPEandSMACtasks. Thisaugmentationserves
toelucidatethesuperiorperformanceofourmethod. Itispertinenttonotethatbaselinealgorithms,
theperformanceofwhichhasbeendemonstratedtobepoorinthetrainingcurves,areomittedfrom
thetablesforconciseness.
AsdemonstratedinTable9,wecompareLEMAEwithSOTAbaselineCMAEusingthemetricof
thenumberofexplorationstepstakentofindthesuccessstate. Theresultsindicateasignificant
explorationaccelerationrate,upto10x,underscoringLEMAEâ€™ssuperiorefficiency.
Moreover,asillustratedinFig.16,wesupplementthetrainingcurvewhileevaluatingtheefficacyof
combiningourmethodwithvariousalgorithms,i.e.,QPLEXandVMIX.
38
etaR
niW
tseTTable9: ComparingLEMAEwithSOTAbaselineCMAEacrossfourmapsinMPEusingthemetric
ofthenumberofexplorationsteps(inthousand)takentofindthesuccessstate
MPE LEMAE(Ours) CMAE Accelerationrate
Pass 153.1Â±20.7 2114.8Â±157.4 13.8
Secret-Room 316.6Â±134.6 1448.5Â±467.2 4.6
Push-Box 159.0Â±42.5 972.3Â±887.3 6.1
Large-Pass 446.9Â±256 >3000 >6.7
QMIX+Ours QPLEX+Ours VMIX+Ours QPLEX VMIX
2m_vs_1z 1c3s5z MMM2
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 0 1 2 3 4 5
T (mil) T (mil) T (mil)
Figure16: Thetrainingcurvewhileevaluatingtheefficacyofcombiningourmethodwithvarious
algorithms.
39
etaR
niW
tseT