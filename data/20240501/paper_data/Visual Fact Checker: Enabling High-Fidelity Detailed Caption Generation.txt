Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation
YunhaoGe XiaohuiZeng JacobSamuelHuffman Tsung-YiLin Ming-YuLiu YinCui
NVIDIA
https://research.nvidia.com/labs/dir/vfc/
GPT-4V VFC
Theimageshowsayounggirlinafieldwithherarms Ahappylittlegirlisstandinginagreenfield,wearing
joyfully raised. She is holding a bunch of pink aplaidshirtandholdingontoastringofpinkballoons.
balloonsinherrighthand.She'swearingaplaidshirt Theballoonsarefloatingintheair,creatingaplayful
andjeans,andherhairisstyledinwhatappearstobe andjoyfulatmosphere.Thegirlappearstobeenjoying
aponytail.Thegirlissmilingwidely,andherpose the moment, with a smile on her face. In the
suggestssheisinmid-celebrationorplay.Thefieldis background,thereisavastexpanseof greengrass,
filledwithwildflowers,andtreescanbeseeninthe stretchingoutasfarastheeyecansee.Theskyabove
backgroundunderablueskywithsomeclouds.The is a bright blue, with a few white clouds scattered
settingappearstobeasunnyday,perfectforoutdoor about. The overall mood of the scene is one of
activities.Theimagecapturesamomentofcarefree carefree happiness and joy, with the pink balloons
childhoodhappiness. adding a touch of whimsy and playfulness to the
atmosphere.
VFC
Cap3D A 3D model of a three-level, intricately designed
woodentowerwithacontrastingblueroofanddoor,
A3Dmodelofasmallwoodentowerwithablueroof. standing on a wooden base. The tower, brown in
color,resemblesafusionofahouse,atower,anda
castle. At the very top of the tower, there is a
crescentmoondesign.Theoveralldesignaddsa
touchoffantasytothescene.
Figure1.ComparisonofVisualFactChecker(VFC)withGPT-4VandCap3D.VFCcangeneratehigh-fidelitydetailedcaptionsthatclosely
matchGPT-4V‚Äôsqualityfor2Dimagesandoffersignificantlymoredetailsfor3DobjectsthanCap3D.VFCusedapre-trainedLlama-2as
theLLMwhengeneratingthecaptionfortheabove2Dimage.
Abstract tion results. In this step, VFC can flexibly generate cap-
tions in various styles following complex instructions. We
conduct comprehensive captioning evaluations using four
Existing automatic captioning methods for visual con- metrics: 1)CLIP-Scoreforimage-textsimilarity;2)CLIP-
tent face challenges such as lack of detail, content hallu- Image-Scoreformeasuringtheimage-imagesimilaritybe-
cination, and poor instruction following. In this work, we tween the original and the reconstructed image generated
propose VisualFactChecker (VFC), a flexible training-free byatext-to-imagemodelusingthecaption. 3)humanstudy
pipeline that generates high-fidelity and detailed captions on Amazon Mechanical Turk; 4) GPT-4V for fine-grained
forboth2Dimagesand3Dobjects. VFCconsistsofthree evaluation. EvaluationresultsshowthatVFCoutperforms
steps: 1) proposal, where image-to-text captioning models state-of-the-art open-sourced captioning methods for 2D
propose multiple initial captions; 2) verification, where a images on the COCO dataset and 3D assets on the Obja-
large language model (LLM) utilizes tools such as object verse dataset. Our study demonstrates that by combining
detectionandVQAmodelstofact-checkproposedcaptions; open-sourcemodelsintoapipeline,wecanattaincaption-
3)captioning,whereanLLMgeneratesthefinalcaptionby ing capability comparable to proprietary models such as
summarizingcaptionproposalsandthefactcheckverifica- GPT-4V,despitebeingover10√ósmallerinmodelsize.
4202
rpA
03
]VC.sc[
1v25791.4042:viXra1.Introduction tion evaluation. Finally, we also performed a fine-grained
evaluation by asking GPT-4V to compare and judge cap-
Imagecaptioningisapivotalchallengeincomputervision
tions with detailed reasoning. The combination of CLIP-
and natural language processing. Its central goal is to en-
Score, CLIP-Image-Score, GPT-4V, and human study pro-
capsulatevisualdatawithinatextualdescription,whichre-
videsamorerobustevaluationofcaptions.
quires a nuanced understanding of both modalities. The
Wesummarizeourmaincontributionsasfollows: 1)We
recent advent of multimodal large language models (MM-
proposeVisualFactChecker(VFC),atraining-freepipeline
LLMs), such as GPT-4V [26], and text-to-image genera-
togeneratehigh-fidelitydetailed2Dand3Dcaptions,effec-
tionmodels, suchasDALLE-3[3], hasmarkedsignificant
tivelymitigatingthechallengeofhallucinationinlongcap-
progressinthisfield.Theseproprietarymodelscouldlever-
tions. (2) CLIP-Image-Score: A novel caption evaluation
age expansive human-labeled data and enormous comput-
metricthatmeasuresthesimilaritybetweentheinputimage
ingresourcestolearntogeneratedetailedandcontextually
andareconstructedimagefromthecaption.(3)Ourevalua-
appropriate image descriptions. On the other hand, exist-
tionshowsthatVisualFactCheckerachievesstate-of-the-art
ingopen-sourcedcaptioningmethodsinthecommunitystill
resultsin2Dand3Dcaptioningtaskscomparedwithopen-
face significant challenges. Methods such as BLIP-2 [17]
sourcedmodels. (4)OurworkshowsthatusinganLLMto
andOFA[35] oftenyieldoverlysuccinctcaptions thatne-
chainopen-sourcemodelscanachievecaptioningcapability
glectessentialvisualinformation. Conversely,systemslike
onparwithproprietarymodelssuchasGPT-4V.
Mini-GPT4[39],InstructBLIP[8],andLLaVA[20,21]can
suffer from hallucination, producing long descriptions that
2.RelatedWork
donotalignwiththeactualcontentoftheimages.
In light of this, we propose VisualFactChecker (VFC), 2.1.ImageCaptioning
a flexible training-free pipeline designed to produce accu-
Imagecaptioninghasmadesignificantprogresswiththead-
rate and comprehensive captions for both 2D images and
ventofdeeplearning. Pioneeringworks[2,10,14]primar-
3D objects. Fig. 1 shows examples of captions generated
ilyfocusonintegratingdeepneuralnetworksforenhanced
by VFC and their comparisons with captions generated by
imageunderstandingandlanguagegeneration.
GPT-4V[26]andCap3D[23]. CaptionsgeneratedbyVFC
Recent strides have been made with the introduction of
are faithful textural representations of the visual contents.
Multimodal-Large Language Models (MM-LLMs), which
This can also be verified by reconstructing images and 3d
aretrainedonextensivevisionandlanguagedata. Thegen-
objects from captions using state-of-the-art text-to-image
eral approach involves leveraging a pre-trained large lan-
andtext-to-3dmodels,asshowninFig.2.
guage model (LLM) and a vision encoder with a projector
VFC focuses on tackling hallucinations and insufficient
to align with the LLM‚Äôs embeddings, thus enhancing vi-
detailsingeneratedcaptionsandisstructuredaroundthree
sual understanding. Several models have emerged as sig-
corecomponents: Proposer,servingasthesystem‚Äôs‚Äúeye‚Äù,
nificant contributors in this domain. BLIP [16], BLIP-
creatingdetailedcaptionproposalsaspreliminarycaptions
2[17],OFA[35],Flamingo[1],Kosmos-2[27],MiniGPT-
by using image-to-text captioning models; Large Lan-
4 [39], InstructBLIP [8], LLaVA [20, 21] have demon-
guageModel,actingasthe‚Äúbrain‚Äù,callingandsummariz-
strated impressive performance in single-view image cap-
inginformationfromothercomponents, andleveragingits
tioning tasks. However, they exhibit varying limitations.
advancedgeneralizationcapabilitiestosteerthecaptioning
For instance, BLIP-2 and OFA often generate overly con-
processfollowingspecifiedcaptioninginstructions;Detec-
cise captions, while others, like InstructBLIP, can produce
tor and VQA models, functioning as ‚Äútools‚Äù utilized by
detailedcaptionsthatoftenincludeinaccurateorhallucina-
theLLMforfact-checkingcaptionproposals, ensuringthe
torycontent. Ourmethodaimstoaddresstheselimitations
fidelityofthefinalgeneratedcaption. VFCisversatileand
bycombiningdifferentmodelsintoapipelineviaanLLM,
effectively handles captioning for both 2D images and 3D
strikingabetterbalancebetweenaccuracyanddetailedness
objectsthroughaunifiedpipeline.Fig.3showsanoverview
ingeneratedcaptionswhilemitigatinghallucinations.
ofthepipeline. Thedetailsofeachcomponentandtheirin-
terplayareexplainedinSec.3.
2.2.LargeLanguageModelsforCaptioning
To comprehensively evaluate the generated captions,
other than leveraging the commonly used CLIP-Score that Recentadvancementsinlargelanguagemodels(LLMs)like
primarily gauges the image-caption similarity, we propose GPT-3 [5], LAMDA [30], PALM [7], Llama [32], GPT-
anewmetric: theCLIP-Image-Score. Thismetricassesses 4[26]havedemonstratedexceptionalzero-shotcapabilities
thesimilaritybetweentheinputimageandareconstructed in language analysis and summarization tasks. This pro-
image created by a text-to-image model from the caption, ficiency has naturally extended to the multimodal domain,
offering a complementary measure. Furthermore, we con- particularly in image-language contexts, where LLMs can
ductedahumanstudyonAmazonMechanicalTurkforcap- summarizemultimodalinformationinazero-shotmanner.DALLE-3 text-to-image MVDreamtext-to-3d
Input image VisualF (Oac ut rC s)hecker LLaVA-1.5 BLIP-2 Input 3D shape VisualF (Oac ut rC s)hecker Cap3D
Figure2. WeuseDALLE-3[3]asatext-to-imagemodeltoreconstruct2Dimagesusinggeneratedcaptionsfromdifferentcaptioning
methods(BLIP-2,LLaVA-1.5andours).Similarly,weuseMVDream[29]asatext-to-3Dmodeltoreconstruct3Dobjectsusingdifferent
3Dcaptions(generatedbyCap3D[23]andours). Fromtheresults,wecanseethatthereconstructedimagesor3DobjectsusingBLIP-2
or Cap3D captions are less similar than the input ones, suggesting their captions may not contain sufficient information or incorrectly
describe the visual contents; the reconstructed images using LLaVA-1.5 captions contain objects or scenes that are not present in the
originalimages(top: peopleinthebackground,bottom: pedestriansandcarsonthestreet),suggestingtheremightbehallucinationsin
LLaVA-1.5captions.Imagesor3Dobjectsreconstructedusingourcaptionsaremoresimilartotheinputs.
Vision-blind LLMs are prominent in multimodal appli- 2.3.HallucinationinMM-LLM
cations,oftenutilizinglanguage-onlyprefixesgeneratedby
There are two popular topics on the hallucination of MM-
pre-trained tools. Clipcap [25] demonstrates this by us-
LLMs. (1)Hallucinationevaluation: Detectionapproaches
ing a continuous embedding as a prompt for a GPT-style
suchasGunjaletal.[11]trainclassificationmodelstoiden-
language model, achieving notable performance in single-
tify hallucination. They focus on distinguishing between
viewpoint image captioning. Similarly, Promptcap [13]
accurateandhallucinatedcontent.Groundtruthcomparison
andPNP-VQA[31]leveragenaturallanguagepromptswith
methods[18,34]comparemodeloutputswithgroundtruth
GPTmodelstoexcelinvisualquestionanswering.
data to detect hallucinations. These techniques assess the
Recent methods have employed LLMs to generate im-
alignmentofgeneratedcaptionswithactualimagecontent.
age captions by summarizing initial captions or keywords
(2) Mitigation Strategies [22]: Data optimization methods
fromVision-Languagemodels. Forinstance,Socraticmod-
suchasLiuetal.[19]addresshallucinationbycreatingneg-
els [37] use a CLIP-based model to extract key tags from
ative instances in training datasets to reduce model over-
images, followed by GPT-3 with specialized prompts to
confidence. Iterative generation methods such as Wang et
create stylized captions. ChatCaptioner [38] builds upon
al. [33] adopt an iterative process for caption generation,
this by integrating ChatGPT and BLIP-2 [17] in a conver-
wherebriefanswersaregeneratedinsuccessionandamal-
sationalapproachforquestion-answeringabouttheimage,
gamated,aimingtoimproveaccuracyandrelevance.
and summarizing them into a caption. Visual Clues [36]
Our VisualFactChecker is a training-free pipeline mit-
uses similar tags to generate a paragraph-caption. IC3 [6]
igating hallucination in image captioning. Our method
andLLM-Fusion[4]useLLMstosummarizecaptionsfrom
utilizes visual grounding tools for improved accuracy,
existing models augmented with temperature-based sam-
thereby actively reducing the hallucination and offering
pling. Cap3D[24]extendsthisconceptto3Dobject.
high-fidelitycaptionsforboth2Dimagesand3Dobjects.
Our method differentiates itself in two critical ways:
First, we focus on reducing hallucinations in captions by 3.VisualFactChecker
employingvisualgroundingtools,suchasobjectdetection,
tofact-checkcaptionsforenhancedaccuracy. Second, our This section introduces the key components of Visual-
pipeline can be used for captioning both 2D images and FactCheckerasshowninFig.3indetailandexplainstheir
3D objects. Unlike previous methods that rely on a single interplayingeneratingaccurateanddetailedcaptions. The
captioningmodel,weintegratemultiplecaptioningsources followingsectionsdelveintospecifics. First, wedetailthe
fromdifferentmodels,ensuringamorecomprehensivecov- pipeline for 2D image captioning (Sec. 3.1), with Fig. 3
erageofvisualcontenttogeneratecaptions. (top) illustrating this process. Then, we explore how ourInitial
Captioner-1 Captions Large 2D
Language
Caption
Captioner-2 Model
parse the caption and list all parse and modify caption
objects that could be detected using the results from an
Describe this image in detail. with an object detection model... object detection model...
Object Detection
Checklist Results
Detector
Multi-view Summary
distill descriptions
Initial about the same 3D
Captioner-1 Captions Large o cab mje ect r afr o vm iew d i if nfe tore on nt e 3D
Language concise caption... Caption
Captioner-2 Model
ask at most 5 most important
correct the description
and concrete questions that
Describe this 3D object in detail. I need to double check... based on the VQA...
Questions Answers
VQA
Figure3. PipelineoftheVisualFactCheckerforcaptioning2Dimages(top)and3Dobjects(bottom). Theprocessbeginswiththeinput
beingcaptionedbytwomultimodalcaptioningmodels(Captioner-1andCaptioner-2)togeneratepreliminarycaptions.Thesecaptionsare
thenverifiedusingaLargeLanguageModel(LLM)tocallobjectdetection(Detector)andVQAmodelsforfact-checkingthecaptions.
Finally,theLLMincorporatesalltheresultsandsummarizesthefinalcaptionbyfollowinginstructions.
approach is adapted for 3D object captioning as shown in VerificationandCaptioning: Thegoaloftheverification
Fig. 3 (bottom), underscoring both shared methodologies step is to scrutinize and rectify any inaccuracies or hallu-
anduniqueaspectsrelevantto3Dcontexts(Sec.3.2). cinations in the captions during the proposal step. It em-
ploys a combination of a Large Language Model (LLM)
3.1.2DImageCaptioning
and grounding tools, including an open-vocabulary object
detectionmodeland/oravisualquestionanswering(VQA)
The caption generation takes three steps: 1) proposal, 2)
model. HeretheLLMcanbeGPT-4orLlama2. Asshown
verification,and3)captioning. Eachstepisdetailedbelow.
in Fig. 3 (top), the process involves the following steps:
Proposal: The Proposal step serves as the cornerstone of
Step 1: LLM first summarizes the initial detailed descrip-
thecaptioningprocessthatgeneratesinitialcaptions.Thisis
tionsfromdifferentMM-LLMsintoasingle,detailedcap-
achieved through the utilization of advanced image-to-text
tion. While this caption is comprehensive, it may not al-
models,specifically‚ÄúLLaVA‚Äùand‚ÄúKosmos2‚Äù. Thesemod-
waysbeaccurate. Step2: TheLLMthenanalyzesthissyn-
elsaretrainedonexpansivedatasets,enablingthemtocom-
thesized caption, identifying all objects that could be veri-
prehendandinterpretvisualcontenteffectively. Byanalyz-
fiedbyobjectdetectionandsummarizinganobjectcheck-
ing the input image, they suggest various preliminary cap-
list. In 2D image captioning, the focus is on eliminating
tions, eachreflectingdifferentfacetsandinterpretationsof
hallucinations,particularlydescriptionsofnon-existentob-
theimage(Fig.3top). Therationalebehindusingmultiple
jects in the image. Identifying these objects is crucial for
image-to-text multimodal LLMs lies in the complexity of
thesubsequentverificationprocess. Step3: Takingtheob-
adequatelycapturinganimage‚Äôsessenceinasingleattempt.
jectchecklistasinput,anopen-vocabularyobjectdetection
Since an image can be accurately described in numerous
model examines candidate objects in the checklist and de-
ways, different models bring unique perspectives, thereby
terminestheirpresenceintheimage. Thisstepispivotalin
encompassingabroaderrangeofinformationpresentinthe
validatingtheexistenceofobjectsmentionedinthecaption,
image. Althoughtheinitialcaptionsproposedmaynotpos-
thussupportingthefidelityofthecaption.
sess perfect fidelity, the primary objective at this stage is
togeneratecaptionsthatareascomprehensiveaspossible. After verification, we go to the last captioning step:
Fig.3displaysthespecificpromptsweusedforeachstep, Based on the object detection results, the LLM revises the
withmoredetailsinAppendixA. summarizedsingledetailedcaption. Eachobjectdescribedin the caption is cross-checked; if detected, it remains un- CLIP-Image-Score
changed,whileundetectedobjectsareconsideredpotential imCL aI gP e ùêº!"ùêº !! imCL aI gP e
hallucinationsandareremovedfromthecaption. Thisstep encoder ùêº! √ó ùêº!! encoder
results in a final caption that is both detailed and reliable.
The underlying assumption is that the detection model,
X Captioning Caption Text-to-Image X‚Äô
serving as an object grounding expert, provides more re- model model
liableresultsthanageneralimagedescriptor.
Intheverificationandcaptioningsteps,theLLMplaysa
Figure4.TheCLIP-Image-Scorepipelineevaluatescaptionaccu-
pivotalroleasa‚Äúbrain‚Äù. Itstartsbyparsingtheinitialcap- racybyencodinganoriginalimageXintoafeaturerepresentation
tion and identifying key objects for detailed examination. I usingaCLIPimageencoder. Acaptioningmodelgeneratesa
X
The LLM then meticulously assesses whether each object captionthatistheninputintoatext-to-imagemodeltoreconstruct
mentioned actually appears in the image based on detec- animageX‚Ä≤,whichisencodedtoI X‚Ä≤. Thescoreiscomputedby
tionresults. Followingthisthoroughanalysis,itrefinesand assessingthecosinesimilaritybetweenI X andI X‚Ä≤, providinga
revisestheinitialcaptions,transformingthemintofinalver- measureofthecaption‚Äôsfidelityandhallucinationdetection.
sionsthatarebothcoherentandrichlydetailed. TheLLM
is instrumentalin guaranteeing linguistic fluency, ensuring
theinitialcaptioninaccordancewiththeanswersprovided
thatthecaptionsnotonlyaccuratelyrepresenttheimagebut
bytheVQAmodel. Weoperateundertheassumptionthat
also maintain the necessary level of detail for high-fidelity
answeringtargetedquestionsresultsinfewerhallucinations
captioning. Moreover, the LLM can follow complex in-
comparedtogeneratingageneraldescription.Oncethecap-
structionstowritethecaptionsinaspecifiedstyle,suchas
tionforeachindividualviewiscomplete,theLLMsynthe-
a caption that only mentions the foreground objects with-
sizes these multiple perspectives into a singular, compre-
outmentioningthebackground. Fig.3displaysthespecific
hensivecaptionfortheentire3Dobject. Thepromptsused
promptsusedforeachstep.
fortheLLMateachstagearedetailedinAppendixA.
3.2.3DObjectCaptioning
4.CLIP-Image-Score
The 3D object captioning process follows a similar struc-
turalpipelinetothatof2Dimages,withafewkeydistinc- Accurate evaluation of caption correctness and detailed-
tionsincertainsteps,asdepictedinFig.3(bottom). In3D ness is paramount in determining the performance of an
captioning, anobjectmaypresentmultipleviews, eachof- image captioning model. Traditional metrics like the
feringuniqueinformation.Thecomprehensivecaptionfora CLIP-Score [12] have served as a standard for measuring
3Dobjectisderivedbyintegratingtheperspectivesfromall the alignment between generated captions and their corre-
theseviews.Foreachview,VisualFactCheckerisemployed sponding images. However, our CLIP-score may lack the
tocreateadetailed,high-fidelitydescription. Subsequently, sensitivity needed to detect the specific issue of hallucina-
theLLM(GPT-4orLlama-2)isusedtoamalgamatethein- tionwithincaptions.
formation from all views, producing a unified caption for We present the CLIP-Image-Score, an alternative met-
the 3D object. In particular, for each view‚Äôs captioning, ric specifically developed to reflect the subtleties of cap-
we have the same three-step approach akin to 2D image tion quality. This metric is different from CLIP-Score by
captioning. In the proposal step, LLaVA-1.5 and Instruct- introducingan additionalreconstruction step. Specifically,
BLIPareutilizedforgeneratinginitialdetaileddescriptions. theCLIP-Image-scoreevaluatesthesimilaritybetweenthe
WeoptoutofusingKosmos2forsingle3Dobjectsdueto original image and a reconstructed version of the image
itslesseffectiveperformanceinprovidingdetaileddescrip- generatedbyafixedtext-to-imagemodelusingthecaption
tions,possiblylinkedtoitsrelianceonanimplicitdetection as a prompt. By comparing the raw image to its recon-
model.Additionally,aslightlymodifiedpromptisused(see structedimage,themetricisabletodetectdiscrepanciesin-
Fig. 3 bottom), which incorporates 3D-specific considera- dicativeofhallucination,thusprovidingadifferentperspec-
tions. In the verification and captioning step, we primar- tiveofthecaptionqualityassessment. Theunderlyingprin-
ilyaddresshallucinationsrelatedtotheattributesof3Dob- cipleoftheCLIP-Image-Scoreistherecognitionthatmul-
jects, such as shape and color. To mitigate these inaccura- tiple‚Äúcorrect‚Äùcaptionsmayexistforasingleimage. How-
cies, ratherthanenumeratingpotentialobjects, weemploy ever,it‚Äôsonlywhenacaptionisboth‚Äúdetail‚Äùand‚Äúcorrect‚Äù
theLLMtogeneratefivecriticalquestionsthatcouldinflu- thatthereconstructedimagecloselyresemblestheoriginal.
enceatext-to-3Dgenerationmodelinreconstructingthe3D Moreover,anyhallucinationspresentinthecaptionbecome
model.Followingthis,weutilizeVQAmodels(specifically evident in the reconstructed image. Fig. 2 presents exam-
LLaVA-1.5)torespondtothesequestionsbasedonthein- plesofsuchreconstructions. Forinstance,considerthere-
put3Dobjectviewimage. Subsequently,theLLMamends sultsfromLLaVA-1.5showninthethirdcolumn. Thecap-tiongeneratedforthefirstimagefalselymentions‚Äúseveral Eval Inputpairsforevaluation Method Reference
other people in the background‚Äù. This error is clearly re- CLIP-Score Table2
2D Rawimage Caption Humanevaluation Fig.6
flectedintheimagereconstructedbythetext-to-imagegen- CPT4Vevaluation Fig.7
Rawimage Image(recon) CLIP-Image-Score Table2
erator. Inessence,comparingthetwoimagesindirectlyen-
CLIP-Score Table3
Multi-view(raw) Caption
suresalignmentbetweentheimageanditscaption,thereby 3D GPT4Vevaluation Fig.7
Multi-view(raw) Multi-view(recon) CLIP-Image-Score Table3
providingacomplementarymethodtoassessthequalityof
thecaptionthandirectlycomparingtheimageandcaption.
Table1.Summaryofevaluationmethodsandresults.
TheCLIP-Image-Scoreevaluationprocessisdepictedin
thefollowingsteps:
‚Ä¢ CaptionGeneration: AnoriginalimageX isinputinto 5.1.Overall: CLIP-ScoreandCLIP-Image-Score
acaptioningmodel,whichgeneratesacaption.
2D image captioning. Dataset: Our evaluation utilized
‚Ä¢ Caption-to-ImageReconstruction: Thisgeneratedcap-
5,000COCOtestimagesfromtheKarpathysplit. Baseline
tionisthenusedasinputforatext-to-imagemodel,which
createsareconstructedimageX‚Ä≤ thatvisuallyrepresents methods:Webenchmarkedagainststate-of-the-artcaption-
ing models, including BLIP-2 [17], InstructBLIP [8], and
thetextualdescription.
LLaVA-1.5 [20]. The evaluation focused on each model‚Äôs
‚Ä¢ Raw Image Encoding: The original image X is pro-
abilitytoproduceaccurate,detailed,andcoherentcaptions
cessedthroughaCLIPimageencoder,translatingthevi-
thateffectivelyencapsulatetheessenceoftheimages.Eval-
sualcontentintoanencodedrepresentationI .
X
uation Metric: We employed two metrics: CLIP-Score
‚Ä¢ ReconstructedImageEncoding: Thereconstructedim-
[12] and CLIP-Image-Score (Sec. 4). The CLIP-Score, a
ageisalsoprocessedthroughtheCLIPimageencoderto
prevalent metric in image caption quality assessment, in-
obtainitsencodedrepresentationI .
X‚Ä≤
volves processing the raw image through the CLIP image
‚Ä¢ Score Calculation: Finally, the encoded representations
encoderandthecaptionthroughtheCLIPtextencoder.The
oftheoriginalandreconstructedimagesarecomparedto
resultantembeddingsarethencomparedforcosinesimilar-
calculate the CLIP-Image-Score. The score is given by
ity, with a higher score indicating greater semantic resem-
the cosine similarity, which assesses the congruence be-
blancebetweentheimageandthecaption.Forouranalysis,
tweenI andI :
X X‚Ä≤
we first calculated the CLIP-Score for each image-caption
CLIP-Image-Score= I X ¬∑I X‚Ä≤ (1) pair,thenaveragedthesescoresacrossall50,000text/image
‚à•I X‚à•√ó‚à•I X‚Ä≤‚à• pairs,scalingtheresultbyafactorof100. Table2displays
the comparative performance of various image captioning
Mostnotably,CLIP-Image-Scoreoffersasensitivemea-
methods on the 5,000 COCO test set images. The results
sure for detecting hallucinations. In scenarios where the
demonstratethatourVisualFactCheckersurpassesallbase-
generatedcaptionincludeselementsthatarenotintheorig-
linemethodsinperformance.
inal image, the reconstructed image will also likely con-
tainthesediscrepancies. Bycomparingtheoriginalandre-
CaptioningMethod CLIP-Score(%)‚Üë CLIP-Image-Score(%)‚Üë
constructed images, the CLIP-Image-Score can effectively
HumanLabel(COCOGT) 30.36 (-2.54) 71.21 (-2.40)
highlight these differences, offering a clearer insight into BLIP2 30.11 (-2.79) 70.79 (-2.82)
thefidelityandaccuracyofthegeneratedcaption. InstructBLIP 31.45 (-1.45) 72.95 (-0.66)
LLaVA-1.5 32.08 (-0.82) 73.24 (-0.37)
Furthermore, CLIP-Image-Score turns a cross-modality
Kosmos-2 32.32 (-0.58) 73.28 (-0.33)
comparison into a more intuitive comparison in the same VisualFactChecker(Ours) 32.90 73.61
image modality (as shown in Fig. 4). CLIP-Image-Score
Table2. Imagecaptioningcomparisonwithdifferentmetricson
representsanewcomplementaryperspectiveforimagecap-
5000COCOtestsetinKarpathysplit,weuserawimageandcap-
tioning evaluation. By leveraging the capabilities of text-
tionasinputpairsforevaluation.
to-image models and focusing on the congruence between
the original and reconstructed images, it provides an ac-
As outlined in Sec. 4, the CLIP-Image-Score provides
curateassessmentofcaptionquality, particularlyinidenti-
a complementary view to assess the quality of image cap-
fyingandmeasuringhallucinations, therebyenhancingthe
tions. Thismetricisderivedbycomparingthecosinesim-
overallreliabilityofcaptiongenerationsystems.
ilarity between the CLIP embeddings of two images: the
original image and a reconstructed image, which is gen-
5.Experiments
erated using the provided caption through a text-to-image
This section presents a thorough evaluation of captioning generation model. A higher CLIP-Image-Score signifies a
modelsacrossboth2Dand3Dvisualcontent,employinga moreaccurateandeffectiveimagecaption.Forthisprocess,
variety of datasets and methodologies. Table 1 provides a Stable Diffusion XL (SDXL) [28] is utilized as the desig-
summaryofourcomprehensiveevaluationexperiments. nated text-to-image model to reconstruct images based onCaptioningMethod CLIP-Score(%)‚Üë CLIP-Image-Score(%)‚Üë
77.2
Cap3D 33.44 (-0.57) 79.88 (-0.44)
81.1
VisualFactChecker(Ours) 34.01 80.32
70.7
64.4
Table3. 3Dobjectcaptioningcomparisonwithdifferentmetrics
62.0
on1000objectsinObjaverse. ForCLIP-Score,weusetheaver-
agescoreoftwoviewsforevaluation.ForCLIP-Image-Score,we
useanoff-the-shelftext-to-3Dmodel,MVDream,togenerate3D
modelsfrom3Dcaptions. Wecomparetwoviewsoftherawob- 60.7
jectandthesameviewsofgenerated3Dobjectforevaluation. 64.2
54.6
53.4
53.1
the generated captions. Table 2 presents the CLIP-Image-
Scoresobtainedforthe5000imagesintheCOCOtestset,
whereourmethodoutperformsallbaselinemethods.
Figure5.2Dimagecaptioningcomparisonwithpair-wisewinning
3D object captioning. Dataset: 1,000 3D objects sam-
rate. VisualFactChecker(VFC)outperformsallbaselinemethods
pled from Objaverse dataset [9]. Baseline methods: We
onbothCLIP-Score(top)andCLIP-Image-Score(bottom).
usestate-of-the-art3DobjectcaptioningmodelCap3D[23]
as the baseline. Cap3D uses 8 view images to generate
the final object caption, our VisualFactChecker uses only ScoreofourVisualFactCheckercaptionagainstthecaptions
2 views to generate the object caption. Evaluation Met- generated from different baselines respectively, and calcu-
ric: CLIP-ScoreandCLIP-Image-Scoreonmultipleviews latethewiningprobabilityofourmethodandthebaselines.
rendered from 3D objects. To evaluate the similarity of a Fig.5showstheresults,forexample,wecanseethatinthe
3D object and the generated caption, we evaluate the sim- pair-wisecomparison,ourVisualFactCheckerperformsbet-
ilarity of the caption with the multi-view images used to ter(higherCLIP-Score)thanLLaVA-1.5on64.4%of5000
generatethecaption. Specifically, weevaluatethesimilar- images(3220images).
ity of the generated caption with the two views that were Calculating the winning rate over all images provides a
used to generate the caption and use the average score to moredetailedanalysisthatzoomsinonthecomparisonof
representtheCLIP-Score. Table.3showstheperformance eachimage,whichshowsacomplementaryviewthanover-
of3Dobjectcaptioningmethodson1,0003Dobjectsfrom allaverageCLIP-Score.
Objaversedataset. VisualFactCheckeroutperformsCap3D.
5.3.Fine-grainedEvaluation: HumanandGPT-4V
We also use CLIP-Image-Score to evaluate the 3D cap-
tionquality.CLIP-Image-Scoreneedsreconstructedimages The CLIP-Score and CLIP-Image-Score offer a general
tocomparewiththerawimages.Wetreatthetwoviewsthat comparison of overall performance. A pairwise per-image
wereusedtogeneratethe3Dobjectcaptionastherawim- winning rate provides a more specific analysis, evaluating
age. Toobtainthereconstructedimage,weuseanoff-the- performance on individual images. However, the research
shelf text-to-3D generation model, MVDream, to generate highlighted in related studies [15] indicates that the CLIP-
a3Dobjectgiventhegenerated3Dobjectcaption. Wethen Score may not be ideally suited for image-to-image com-
renderthesametwoviewsofimagesbasedonthegenerated parison tasks. Furthermore, relying on a single score fails
3Dobject,andwecalculatetheCLIP-Image-Scorebetween toprovideanuancedcomparisonacrosscriteria,suchasac-
therawimageandtherenderedimage. Table.3showsthe curacy and level of detail. We use Human evaluation and
CLIP-Image-Scoreon1000objectsinObjaversedataset. GPT-4Vtoprovideamorefine-grainedevaluation.
Human evaluation using Amazon Mechanical Turk
5.2.PerImageEvaluation: WiningRate
(AMT). We employed a pairwise comparison strategy.
CLIP-Score and CLIP-Image-Score indicate an overall FromtheCOCOdataset,werandomlyselected100images
performance comparison, which shows an average score out of 5000. For each image, our caption was compared
among all 5000 images. The average score may be dom- against5baselinecaptionsrespectively.Toreducevariance,
inatedbyasmallgroupofimagesthathaveextremelyhigh eachcomparisonwasdoneby3differentAMTworkersand
orlowscores. Tozoominandshowamoredetailedcom- weusedtheirmajorityvotingasthefinalselection. Thisre-
parison,wetrytoanswerthefollowingquestion: Givenan sulted in a total of 1500 comparisons collected on AMT.
image, what is the probability that one method performs AMTUIisshownintheappendix. Theworkerswerepre-
better than another method on caption generation? To an- sentedwithtwocompetingcaptions‚Äîonefromabaseline
swerthisquestion,weneedtogoovereachimageandcal- method and one from our VisualFactChecker, in random-
culatethewinningrateforapairofmethods. izedorder. Theywereinstructedtoselectthebettercaption
Specifically, for each image, we compare the CLIP- describing the image based on 3 aspects: correctness, de-tailness, and fluency. Results in Fig. 6 show our captions MethodsorSteps CLIP-Score
are more preferred by humans. The human evaluation in- LLaVA-1.5 32.08(-0.33)
structionandwebUIisshowninAppendixB. Kosmos-2 32.32(-0.09)
2D
VisualFactChecker(w/ofactcheck) 32.41
VisualFactChecker 32.90(+0.49)
100
LLaVA-1.5 32.05(-0.66)
92
58 InstructBLIP 32.51(-0.20)
58 3D VisualFactChecker(w/ofactcheck) 32.71
69 VisualFactChecker 34.01(+1.30)
Table 4. Ablation study on captioning 2D images (5000 COCO
Figure6.AmazonMechanicalTurkhumanevaluationresults. testdataset)and3Dobjects(1000Objaverse).
GPT-4V evaluation. Our study applied GPT-4V for eval-
tionsofinitialcaptioners,namelyLLaVA-1.5andInstruct-
uating captions in a manner akin to the caption evaluation
BLIP on the same 1000 Objaverse 3D objects. We further
processusedinDALLE-3. Weusethesamerandomlyse-
investigatetheperformanceofourmethodologywithoutthe
lected 100 images from COCO as in Human evaluation.
fact checker, which in this case operates by leveraging a
Foreachimage,weconsideredthecaptionsgeneratedby5
VQA model to reduce hallucinations. Table 4 shows the
baselinemethodsalongsidethecaptionproducedbyourVi-
detailedresults. Theseresultshighlightthesignificanceof
sualFactChecker. WethenpresentedGPT-4Vwiththeraw
factcheckerinourapproach.
image,ourreferencecaption,andthefourbaselinecaptions.
Our designed prompt instructed GPT-4V to compare each
5.5.QualitativeResultsandPromptFollowing
baselinecaptionagainstourreferencecaption,focusingon
two primary aspects: correctness and detail. GPT-4V was Other than quantitative evaluation results, we show more
tasked with providing a pairwise, detailed comparison for qualitative examples of VisualFactChecker for 2D and 3D
eachpair,includingjustificationsforitsassessments.Based captionsinAppendixC.
onthesecomparativeinsights,GPT-4Vclassifiedeachbase- By leveraging an LLM, VisualFactChecker can follow
line method caption as either ‚Äúbetter‚Äù or ‚Äúworse‚Äù than our complexinstructionstowritecaptionsinvariousstyles.Ex-
VisualFactChecker.Fig.5showsthecomprehensiveresults. amplesareshowninAppendixD.
More details about the GPT-4V evaluation prompt and ex-
6.Conclusion
amplesareshowninAppendixB.
We propose the VisualFactChecker (VFC), a training-free
98 pipelinetogeneratehigh-fidelityanddetailedcaptions. By
100
utilizing an LLM to chain multimodal models and object
87
89 detection and VQA models, VFC reduces hallucination in
68 longcaptions.Weconductedacomprehensivecaptioneval-
94
uationusingdifferentmetrics,including1)image-textsimi-
larityusingCLIP-Score,2)image-reconstructedimagesim-
ilarity using our proposed CLIP-Image-Score, 3) human
Figure 7. GPT-4V evaluation results. Our captions are signifi-
cantlybetterthanbaselines. study, and 4) fine-grained evaluation using GPT-4V. Com-
pared with open-sourced captioning models, our method
achievesstate-of-the-artinboth2Dand3Dcaptioning. Our
5.4.AblationStudy workshowscombiningopen-sourcedmodelsintoapipeline
cansignificantlyclosethecaptioningperformancegapwith
Inourablationstudy,weexploretheimpactofvariouscom-
proprietary models like GPT-4V. In the future, we plan to
ponents on performance. For 2D captioning tasks, we as-
improveourpipelinefurtherbyincludingmorecomponents
sess the efficacy of initial captioning models, LLaVA-1.5
forfact-checkingandmakingitmoreautomaticindeciding
and Kosmos-2, using the CLIP-Score metric for the cap-
whichcomponentstouse.
tions they generate on the same 5000 COCO test images.
Acknowledgments We would like to thank Siddharth Gu-
Additionally, we ablate our method‚Äôs performance in the
rurani for helping with our human evaluation using Ama-
absence of the verification (fact checker) step, which aims
zon Mechanical Turk; Haochen Wang for his help in pre-
tomitigatehallucinationsthroughdetectiongrounding. Ta-
processing 3D data. We also thank Qinsheng Zhang, Yo-
ble4showsthedetailedresults. Likewise,inthecontextof
geshBalaji,andYen-ChenLinfortheirhelpfuldiscussion.
3D object captioning, we evaluate the individual contribu-References [14] LunHuang,WenminWang,JieChen,andXiao-YongWei.
Attentiononattentionforimagecaptioning. InICCV,2019.
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
2
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
[15] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Daniel LK
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
Yamins, and Jiajun Wu. Are these the same apple? com-
visual language model for few-shot learning. In NeurIPS,
paring images based on object intrinsics. arXiv preprint
2022. 2
arXiv:2311.00750,2023. 7
[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien
[16] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Blip: Bootstrappinglanguage-imagepre-trainingforunified
Bottom-upandtop-downattentionforimagecaptioningand
vision-language understanding and generation. In ICML,
visualquestionanswering. InCVPR,2018. 2
2022. 2
[3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
[17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Wang,LinjieLi,LongOuyang,JuntangZhuang,JoyceLee,
Blip-2: Bootstrapping language-image pre-training with
YufeiGuo,WesamManassra,PrafullaDhariwal,CaseyChu,
frozen image encoders and large language models. arXiv
and Yunxin Jiao. Improving image generation with better
preprintarXiv:2301.12597,2023. 2,3,6
captions. Technicalreport,OpenAI,2023. 2,3,11
[18] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
[4] Simone Bianco, Luigi Celona, Marco Donzella, and Paolo
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
Napoletano. Improving image captioning descriptive-
tion in large vision-language models. arXiv preprint
ness by ranking and llm-based fusion. arXiv preprint
arXiv:2305.10355,2023. 3
arXiv:2306.11593,2023. 3
[19] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
Yacoob, and Lijuan Wang. Aligning large multi-modal
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
model with robust instruction tuning. arXiv preprint
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
arXiv:2306.14565,2023. 3
guagemodelsarefew-shotlearners. InNeurIPS,2020. 2
[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
[6] David M Chan, Austin Myers, Sudheendra Vijaya-
Improved baselines with visual instruction tuning. arXiv
narasimhan, David A Ross, and John Canny. Ic3: Im-
preprintarXiv:2310.03744,2023. 2,6
age captioning by committee consensus. arXiv preprint
[21] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
arXiv:2302.01328,2023. 3
Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
2023. 2
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
[22] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo,
Barham, Hyung Won Chung, Charles Sutton, Sebastian
YawenZhang,BaochenSun,CarlYang,andJieYang. Eval-
Gehrmann, et al. Palm: Scaling language modeling with
uation and mitigation of agnosia in multimodal large lan-
pathways. arXivpreprintarXiv:2204.02311,2022. 2
guagemodels. arXivpreprintarXiv:2309.04041,2023. 3
[8] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
[23] TiangeLuo,ChrisRockwell,HonglakLee,andJustinJohn-
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
son. Scalable 3d captioning with pretrained models. In
Fung, and Steven Hoi. Instructblip: Towards general-
NeurIPS,2023. 2,3,7,17
purpose vision-language models with instruction tuning.
arXivpreprintarXiv:2305.06500,2023. 2,6 [24] TiangeLuo,ChrisRockwell,HonglakLee,andJustinJohn-
[9] MattDeitke, DustinSchwenk, JordiSalvador, LucaWeihs, son. Scalable3dcaptioningwithpretrainedmodels. arXiv
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana preprintarXiv:2306.07279,2023. 3
Ehsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: [25] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
Auniverseofannotated3dobjects. InCVPR,2023. 7 cap: Clip prefix for image captioning. arXiv preprint
[10] JeffreyDonahue,LisaAnneHendricks,SergioGuadarrama, arXiv:2111.09734,2021. 3
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, [26] OpenAI. Gpt-4technicalreport,2023. 2
andTrevorDarrell. Long-termrecurrentconvolutionalnet- [27] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
worksforvisualrecognitionanddescription.InCVPR,2015. Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
2 ingmultimodallargelanguagemodelstotheworld. arXiv
[11] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preprintarXiv:2306.14824,2023. 2
preventing hallucinations in large vision language models. [28] Dustin Podell, Zion English, Kyle Lacey, Andreas
arXivpreprintarXiv:2308.06394,2023. 3 Blattmann, Tim Dockhorn, Jonas Mu¬®ller, Joe Penna, and
[12] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras, Robin Rombach. Sdxl: Improving latent diffusion mod-
andYejinChoi. Clipscore:Areference-freeevaluationmet- els for high-resolution image synthesis. arXiv preprint
ricforimagecaptioning. arXivpreprintarXiv:2104.08718, arXiv:2307.01952,2023. 6
2021. 5,6 [29] YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,
[13] YushiHu,HangHua,ZhengyuanYang,WeijiaShi,NoahA andXiaoYang. Mvdream:Multi-viewdiffusionfor3dgen-
Smith, and Jiebo Luo. Promptcap: Prompt-guided task- eration. arXivpreprintarXiv:2308.16512,2023. 3,17
awareimagecaptioning. arXivpreprintarXiv:2211.09699, [30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
2022. 3 Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, AliciaJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda:
Language models for dialog applications. arXiv preprint
arXiv:2201.08239,2022. 2
[31] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio
Savarese,andStevenCHHoi.Plug-and-playvqa:Zero-shot
vqabyconjoininglargepretrainedmodelswithzerotraining.
arXivpreprintarXiv:2210.08773,2022. 3
[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet,Marie-AnneLachaux,Timothe¬¥eLacroix,Baptiste
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXivpreprintarXiv:2302.13971,2023. 2
[33] BinWang,FanWu,XiaoHan,JiahuiPeng,HuapingZhong,
Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,
et al. Vigc: Visual instruction generation and correction.
arXivpreprintarXiv:2308.12714,2023. 3
[34] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi,
Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji
Zhang, Jihua Zhu, et al. Evaluation and analysis of hal-
lucination in large vision-language models. arXiv preprint
arXiv:2308.15126,2023. 3
[35] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. InICML,2022. 2
[36] Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen
Bach, Ce Liu, and Michael Zeng. Visual clues: Bridging
vision and language foundations for image paragraph cap-
tioning. InNeurIPS,2022. 3
[37] AndyZeng,MariaAttarian,BrianIchter,KrzysztofChoro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
AveekPurohit,MichaelRyoo,VikasSindhwani,etal. So-
cratic models: Composing zero-shot multimodal reasoning
withlanguage. arXivpreprintarXiv:2204.00598,2022. 3
[38] DeyaoZhu,JunChen,KilichbekHaydarov,XiaoqianShen,
Wenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,
blip-2answers: Automaticquestioningtowardsenrichedvi-
sual descriptions. arXiv preprint arXiv:2303.06594, 2023.
3
[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. arXiv
preprintarXiv:2304.10592,2023. 2Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation
Supplementary Material
A.MoreDetailsonModelsandPrompts We find Llama-2 may encounter difficulties in the last
2D captioning step due to the complexity of the prompt.
ThemodelsusedinVisualFactCheckerandbaselinesare:
A workaround is to use a script to compare the detection
‚Ä¢ CaptionProposer: BLIP-2-OPT-2.7B,InstructBLIP-7B,
resultswiththeobjectlistfromstep-2andidentifyobjects
LLaVA-1.5-13B,Kosmos-2
toberemoved. Then,employLlama-2solelyforremoving
‚Ä¢ LLMs: GPT-4-0613,Llama-2-70B-chat
theseobjectsandsummarizingthedescription.
‚Ä¢ Detector: GroundingDINO
‚Ä¢ VQA:LLaVA-1.5-13B VISUALFACTCHECKERPROMPTS(3DOBJECTS)
BelowarethepromptsweusedinVisualFactCheckerfor
Proposal(LLaVA-1.5):
captioning2dimagesand3dobjects.
Please describe the details of the 3D object, the detailed description
willbeusedforatextto3dmodeltogeneratethis3Dobject. Please
VISUALFACTCHECKERPROMPTS(2DIMAGES) providedetailsoftheshape,colorofeachpart,avoidimaginationand
solveitstepbystep.
Proposal(LLaVA-1.5/Kosmos-2):
Describethisimageindetail. Proposal(InstructBLIP):
Describethe3Dobjectindetail,stepbystep.
Verificationstep-1(GPT-4/Llama-2):
Thisisahardproblem. CarefullysummarizeinONEdetailedcaption Verificationstep-1(GPT-4/Llama-2):
basedonthefollowingtwocaptionsbydifferent(possiblyincorrect) Thisisahardproblem. CarefullysummarizeinONEdetailedcaption
peopledescribingthesamescene. Besuretodescribeeverything,and basedonthefollowingtwocaptionsbydifferent(possiblyincorrect)
avoidhallucination. people describing the same 3D object. The detailed caption will be
used for a text to 3D model to generate this 3D object. Be sure to
Verificationstep-2(GPT-4/Llama-2): describeeverything,andavoidhallucination.
Iwanttouseanobjectdetectortocheckthecorrectnessofanimage
captionobtainedbyanimagecaptionmodel.Canyouhelptoparsethe Verificationstep-2(GPT-4/Llama-2):
captionbelowandlistallobjectsthatcouldbedetectedwithanobject I have a description of a 3D object, the detailed caption will be
detection model in the image? Please only list the object name and used for a text to 3d model to generate the same 3D object. Some
ignorethedescription.Pleaseusesingularforalllistedobjects. part of the description may have some hallucination, so I want to
Caption:{}. use a VQA model to double check some key description, Please
Pleaseconcatenatethemtogetherwith‚Äú.‚Äùasseparation. ask at most 5 most important and concrete questions that I need
to double check to improve the fidelity of the description. Please
Verificationstep-3(GroundingDINO): focusonthefactorsthatinfluencethefinaltextto3Dmodelgeneration.
N/A (Grounding DINO examines candidate objects in the checklist
aboveanddeterminestheirpresenceintheimage.) RawCaption:{}
Pleaseoutputthe5questionsinapythonlist.
Captioning(GPT-4/Llama-2):
Objective: parseandmodifyimagecaptionsusingtheresultsfroman Verificationstep-3(LLaVA-1.5):
objectdetectionmodel(mayhavehallucination). N/A(LLaVA-1.5takesquestionsaboveandrawviewimageasinput
andgiveanswers).
I will put the detection results to you in the following format:
[‚Äúobject‚Äù:detectedobjectname,‚Äúnumber‚Äù:numberofdetectedobject SingleviewCaptioning(GPT-4/Llama-2):
(N)].Pleasefollowthefollowingsteps: Ihaveadescriptionofa3Dobject,thedetailedcaptionwillbeused
foratextto3dmodeltogeneratethesame3Dobject. Somepartof
Instructions: thedescriptionmayhavesomehallucination,soIuseaVQAmodelto
Parsetheobjectinthecaption,(Note:onlyparseandmodifytheobject doublechecksomekeydescription.
(notcolor,action,size,shape,orotherdescriptions)) Hereistheoriginaldescriptionthatmaycontainhallucination:{}
1. Iftheobjectwasdetectedbythedetectionmodel,keepeverything HerearethequestionsandanswersfromaVQAmodel:{}
includingalldescriptions. Forinstance,iftheoriginalcaptionis: ‚Äúa
blackandwhitepandatoy‚Äù,ifthetoywasdetected,keepallcontent Please correct the description based on the VQA. I want to use
eventhoughthe‚Äúpanda‚Äùand‚Äúblackandwhite‚Äùarenotdetected.Keep the description as a prompt for a text-to-3D generation model to
alldescriptionsaboutcolor,shape,actions.etc. generatethesame3Dobject.
2. Ifthesubjectobjectwasnotdetected,removeonlytheobject. Do
NOTremovecolor,shape,action,textandotherdescriptions. ObjectCaptioning(GPT-4/Llama-2):
3. Onlydecreasetheobjectnumberifthedetectedobjectnumberis Given a set of descriptions about the same 3D object from different
smallerthanthecaptionnumber. cameraviews,pleasedistillthesedescriptionsintooneconcisecaption:
CameraView1description:{}
This is a hard problem. Please minimize modifications of the CameraView2description:{}
caption,andlistallchangesmadealongwiththereasoning.
‚ÄîBEGINDetectionresults:‚Äî
{} B.DetailsonHumanandGPT-4VEvaluaiton
‚ÄîENDDetectionresults‚Äî
‚ÄîBEGINRawcaption:‚Äî
{} Fig.8showstheAmazonMechanicalTurkhumanevalua-
‚ÄîENDRawcaption‚Äî tionwebUI.ForGPT-4Vevaluation,inspiredbyDALLE-
Pleasegivetheoutputinthefollowingformat: 3[3],wecraftasinglepromptforevaluatingallcaptionsfor
Modification: a given image using GPT-4V (gpt-4-vision-preview). The
Updatedcaption:
promptisasfollows.GPT-4VEVALUATIONPROMPT(2DIMAGES) GPT-4VEVALUATIONPROMPT(3DOBJECTS)
Youareresponsibleforjudgingthequalityofimagecaptionsgenerated Youareresponsibleforjudgingthequalityofcaptionsgeneratedby
by computer programs. You will be presented with an image and computerprogramsdesignedfordescribing3dmodels. Youwillbe
captions describing the image. One of the captions is a reference presented with two rendered images of the same 3d model and two
captionandyouareresponsibleforcomparingothercaptionsagainst captions describing the 3d model. Please evaluate captions in the
the reference caption, one by one. Please evaluate captions in the followingaspects:
followingtwoaspects:
1. Correctness. A good caption should correctly describe image 1. Correctness. A good caption should correctly describe the
content,includingthemajorobjectsandthesurroundingscene. 3dobject,includingwhattheobjectisandthemajorpropertiesofthe
2. Detailedness. A good caption should contain rich details about object.
the image content, for example, spatial location of objects, clothing 2. Detailedness. Agoodcaptionshouldcontainrichdetailsaboutthe
andactionsofpeople,color,countingofobjects,textsintheimage,etc. 3dobject,forexample,part,color,material,function,etc.
Given the image and a reference caption, you need to make de- Given rendered images of the 3d model and two captions, you
cisionsastowhetherornotthegivencaptionsarebetterorworsethan need to make decisions on which caption is better in describing the
thereferencecaptionindescribingtheoverallimage. overall3dobject.
This is a difficult task. You should first evaluate the reference This is a difficult task. You should first evaluate two captions
caption for the given image. And then evaluate each caption in the based on rendered views of the 3d object. You should think out
samewayandcompareagainstthereferencecaption.Youshouldthink loud about your eventual conclusion for each caption, enumerating
outloudaboutyoureventualconclusionforeachcaption,enumerating reasons why each of the captions is better or worse compared with
reasonswhyeachofthecaptionsisbetterorworsecomparedwiththe theother. Afterthinkingoutloud,youshouldoutputeither‚ÄúCaption
referencecaption. Afterthinkingoutloud, youshouldoutputeither 1‚Äù or‚ÄúCaption 2‚Äù dependingon which captionyou think isa better
‚ÄúBetter‚Äùor‚ÄúWorse‚Äùforeachcaptiondependingonwhetheryouthink descriptionofthe3dmodel.
the caption is a better description of the image than the reference
caption. Caption1:{}
ReferenceCaption:{} Caption2:{}
Caption1:{}
Caption2:{} You must keep your thinking out loud and respond in less than
Caption3:{} 200words.Pleaseoutputyourjudgmentattheendusingthefollowing
Caption4:{} format:
BetterCaption:Caption1orCaption2
You must keep your thinking out loud and respond in less than
500 words. Please summarize your judgment at the end using the
We provide an example of GPT-4V‚Äôs response for 3D
followingformat:
Caption1:BetterorWorse captioning evaluation, corresponding to Fig. 12 (b), where
...
caption1isCap3Dandcaption2isourVisualFactChecker.
Caption4:BetterorWorse
GPT-4VEVALUATIONRESPONSE(3DOBJECTS)
We provide an example of GPT-4V‚Äôs response, corre-
Toevaluatethecaptions,let‚Äôsconsiderthecriteriaofcorrectnessanddetailedness.
spondingtoFig.10(d), wherethereferencecaptionisour
Caption 1 is very basic, only stating that the object is a ‚Äú3D model of a
proposedVisualFactChecker,andcaption1to4areBLIP-2,
racingcar‚Äù.Thisiscorrectbutlacksdetail.
InstructBLIP,Kosmos-2andLLaVA-1.5,respectively.
Caption 2, on the other hand, gives a much more comprehensive descrip-
tion. Itaddressesthecolorscheme(‚Äúwhiteandredsportscar,withablackand
redstripe‚Äù),thedecorativeelements(‚Äúcheckeredpatternonthehood‚Äù),specific
GPT-4VEVALUATIONRESPONSE(2DIMAGES) partsofthecar(‚Äúspoilerontheback‚Äù,‚Äúcurvedroof‚Äù,‚Äúblackandsportytires‚Äù),
andeventhebackgroundwhichhelpstohighlighttheobject(‚Äúpositionedona
neutralbackground‚Äù).Thiscaptiongoesbeyondsimplyidentifyingtheobjectas
Toevaluateeachcaption,Iwillconsidertheaspectsofcorrectnessanddetailed- aracingcar;itdescribesitsaestheticandfunctionalelements,whichalignswith
nessagainstthereferencecaption. bothcorrectnessanddetailedness.
ReferenceCaption: Thereferencecaptionaccuratelydescribesthatapersonis
snowboardingandperformingajumpwhilekiteflying,mentioningthecolorsof While Caption 1 is correct, it is very brief and does not provide any de-
theoutfitandthesnowboard.Italsoprovidescontextforthebackdropandgives tail. Caption2isalsocorrectbutprovidesarichanddetaileddescriptionthat
asenseofthesnowboarder‚Äôsabilities. wouldgivesomeonewhocannotseetheimageaclearunderstandingofthe3D
Caption1:Thiscaptionintroducestheterm‚Äúkitesurfing‚Äù,whichcouldbemis- model‚Äôsappearanceandcharacter.
leadingbecausethattermistypicallyassociatedwithwaterratherthansnow. It
lacksthedetailoftheoutfitandthespecificsofthesnowboardingaction. BetterCaption:Caption2
Caption2:Thiscaptionprovidesasummarizationclosetothereferencebutomits
thecolorsoftheoutfitandthesnowboard.Itdoes,however,accuratelyaffirmthe
skillandcontrolofthesnowboarderandthebackdropofthescene.
Caption3:Thiscaptionmentionsobjectsthatarenotpresentintheimage,such C.MoreQualitativeResults
asotherpeopleinthescene. Itretainsthementionofthegreenoutfitandem-
phasizestheheightofthejumpandthekite‚Äôspositionbutsuggestsparasailingor
kiteboarding,whichmightnotbeaccuratetermsforthisactivity. We show more results of image captioning methods and
Caption4: Thedescriptionofthesnowboarder‚Äôsactionandoutfitmatchesthe their DALLE-3 reconstructed images using different gen-
imagewell,anditaddsthecolorofthesky.Italsointroducesadditionalcontext
bymentioningotherspectatorsinthescene,butitdoesnotmentionthekiteflying erated captions (COCO 2D images in Fig. 9, 10, 11; Ob-
aspect,whichisessentialfortheactivitydepicted.
Judgment: javerse3DobjectsinFig.12). Weshowmorecomparison
Caption1:Worse
withGPT-4VcaptionsusingLlama-2astheLLMinFig.13.
Caption2:Worse
Caption3:Worse
Caption4:Worse
D.FollowingComplexPrompts
Similarly, we design prompt for evaluating captions for By leveraging the LLM, VisualFactChecker can follow
3D object, given two rendered views (front view and back complex prompts to write captions in different styles. Ex-
view). Thepromptisshownasfollows. amplesshowninFig.14.Which image caption is better?
Choose the caption that better describes the image. A good caption should be correct, detailed, and well-written:
1. Correct: A good caption should correctly describe image content, including the major objects and the surrounding scene.
2. Detailed: A good caption contains rich details about the image content, for example, the spatial location of objects, clothing,
actions of people, color, counting of objects, texts in the image, etc.
3. Well-written: A good caption should be clear, coherent, and grammatically correct.
CaptionA: In the image, there is a wedding cake sitting on a dining table. The cake CaptionB: The image prominently features a white wedding cake adorned with blue
is three-tiered and decorated with a plaid pattern. A man and a woman can be seen and green tartan ribbons, elegantly displayed on a dining table. The table is
sitting at the table, possibly enjoying the wedding celebration. There are several tastefully set with various items including wine glasses, a cup, and a knife,
wine glasses placed around the table, indicating that wine might be served during suggesting a wedding reception setting. In the background, a couple, presumably
the event. Additionally, there are two bottles of wine visible in the scene, one on the the bride and groom, are engaged in conversation. The bride is distinguished by a
left side of the table and the other on the right side. tiara, while the groom is dressed in a suit. Additional elements like a dining room
window in the background contribute to the overall ambiance of the scene.
CaptionA CaptionB
Figure8.AmazonMechanicalTurkwebuserinterface.Input image Captions DALLE-3 text-to-image
BLIP-2:astreetsignonapole
InstructBLIP:Theimagefeaturesastreetpolewithmultiplestreetsignsattachedtoit.Oneof
thesignsreads\"WoodwardAve\",whiletheothersignreads\"WoodwardSt\".Thepoleis
situatedinfrontofatallbuilding,whichcanbeseeninthebackground.Thereareseveraltrees
scatteredthroughoutthescene,includingoneontheleftsideoftheimageandanotheronthe
(a) rightside.Inadditiontothetrees,thereareseveralcarsparkedaroundthearea,addingtothe
urbanatmosphere.
Kosmos-2:Theimagefeaturesastreetsignonapole,withagreenstreetsignreading
\"Madsworth\"andablackstreetsignpostreading\"\"Madsworth.\"
LLaVA-1.5:Theimagefeaturesastreetcornerwithawoodenpoleholdinguptwostreetsigns.
Thesignsaregreenandwhite,indicatingthenamesoftheintersectingstreets.Thepoleis
locatedinfrontofalargebuilding,whichservesasabackdropforthescene.\n\nThereare
severalpeopleinthearea,withonepersonstandingclosetothepoleandthestreetsigns,and
twoothersfurtheraway.Additionally,thereisacarparkedonthestreet,slightlytotherightof
thepole.
VisualFactChecker(Ours):Theimageprominentlyfeaturesastreetsignpolewithstreetsigns
attachedtoit,includingagreensignreading\"Madsworth\"andablacksignpostalsoreading
\"Madsworth.\"Thesignsindicatethenamesoftheintersectingstreets.Thepoleissituatedin
frontofalargebuilding,whichservesasthebackdropforthescene,enhancingthevisibilityof
thestreetnamesandtheirlocations.
BLIP-2:abananatreewithalargepurpleflower
InstructBLIP:Theimagedepictsalushgreenbananatreewithseveralbananashangingfromits
branches.Oneofthebananasislocatedclosertothetopofthetree,whileothersarescattered
throughoutthebranches.Thereisalsoapurpleflowergrowingonthetree,addingatouchof
colortothescene. Inthebackground,thereisapowerlinevisible,indicatingthatthebanana
(b) treeissituatedinanurbanorsuburbanarea.Overall,theimagecapturesavibrantandlively
setting,showcasingthenaturalbeautyofthebananatreeanditssurroundings.
Kosmos-2:Theimagefeaturesalargebananatreewithalargepurpleflowergrowingonit.The
flowerissituatedatthetopofthetree,anditissurroundedbygreenleaves.Thetreeissituated
inayard,andthereareseveralotherbananatreesvisibleinthebackground.Theyardisfilled
withvariousobjects,includingacar,atruck,andabicycle.Thecarislocatedneartheleftside
oftheimage,whilethetruckislocatedtowardstherightside.Thebicycleislocatedfurther
backinthescene,closertotherightedgeoftheyard.
LLaVA-1.5:Theimagefeaturesalargebananatreewithabunchofbananashangingfromits
top.Thebananasarestillgreen,indicatingthattheyareunripe.Thetreeissurroundedbya
fence,providingasenseofenclosuretothescene.Thebananasarespreadoutacrossthetree,
withsomehangingclosertothetopandothersfurtherdown.Thetreeappearstobehealthy
andthriving,providinganaturalandlushbackdroptotheimage.
VisualFactChecker(Ours):Theimageshowcasesalargebananatreewithabunchofunripe,
greenbananashangingfromitstopandalargepurpleflowergrowingonit,bothcreatinga
visuallyappealingdisplay.Thetreeissurroundedbygreenleavesandisenclosedbyafence,
providingasenseofprotection.Thetreeissituatedinayardfilledwithvariousobjects.One
otherbananatreeisvisibleinthebackground.
BLIP-2:ayoungboystandinginfrontofabus
InstructBLIP:Theimagefeaturesayoungboystandinginfrontof abus,wearingagreen
sweatshirtandabaseballcap.Thereareseveralotherpeopleinthescene,includingaman
standingtotheleftoftheboy,anothermanstandingtotherightoftheboy,andathirdman
standingfurtherawayfromtheboy.Inadditiontothepeople,therearetwobackpacksvisiblein
(c) thescene.Onebackpackislocatedclosertotheboy,whiletheotherispositionedfurtheraway
fromhim.
Kosmos-2:Theimagefeaturesayoungboywearingahatandagreensweaterstandinginfront
ofabus.Heappearstobelookingatthecamera,possiblysmiling.Inthebackground,thereis
anotherpersonstanding,possiblyabusdriverorapassenger.Thebusisparkedonthesideof
theroad,andtheboyisstandingclosetoit.
LLaVA-1.5:Theimagefeaturesayoungboywearingahatandagreensweater,standinginfront
ofabus.Heappearstobelookingatthecamera,possiblyposingforapicture.Anotherperson,
possiblyaman,isstandingbehindtheboy,alsolookingatthecamera.Theyseemtobethemain
subjectsofthephoto.\n\nInthebackground,thereisabusvisible,whichcouldbethemodeof
transportationforthepeopleinthescene.Theboyandthemanaretheonlytwoindividualsin
theimage,withtheboybeingthemainfocus.
VisualFactChecker(Ours):Theimagedepictsayoungboy,donnedinahatandagreensweater,
standinginfrontofabusparkedonthesideofaroadinadesert-likeenvironment.Theboy
appearstobelookingatthecamerawithanambiguousexpressionthatcouldbeinterpretedas
eitherasmileorasomewhatsadlook.Behindtheboy,anotherperson,possiblyabusdriveror
apassenger,isstanding,althoughtheirfeaturesarenotclearlydiscernible.Thebusseemsto
serveasamodeoftransportationforthepeopleinthearea.
Figure9.Comparisonofdifferent2Dimagecaptioningmethods(1/3).TheleftcolumnistheinputimagefromCOCO,themiddlecolumn
showsthecaptiongeneratedbydifferentmethods(BLIP-2,InstructBLIP,Kosmos-2,LLaVA-1.5,andourproposedVisualFactChecker).
Intherightcolumn,weuseDALLE-3asatext-to-imagemodeltoreconstruct2Dimagesusingdifferentgeneratedcaptions.Input image Captions DALLE-3 text-to-image
BLIP-2:apersoniskitesurfinginthesnow
InstructBLIP:Theimagedepictsapersonridingasnowboardintheairwhilebeingpulledbya
kite. The snowboarder is high up in the sky, likely enjoying the thrill of parasailing or
kiteboarding.Thekitecanbeseeninthebackground,flyinghighabovethesnowboarder's
(d) head.Thesnowboarderiswearingagreenoutfitandappearstobefullyimmersedinthe
activity.Inadditiontothesnowboarderandthekite,thereareseveralotherpeoplevisibleinthe
scene.Theyarescatteredthroughoutthesnow-coveredarea,someclosertothesnowboarder
andothersfurtheraway.
Kosmos-2:Intheimage,apersonisskillfullyridingasnowboardwhileflyingakiteintheair.The
kiteispositionedhighinthesky,andthesnow-coveredfieldprovidesaperfectbackdropforthe
activity.Thesnowboarderisskillfullynavigatingthesnow,showcasingtheirskillandcontrol.
LLaVA-1.5:Theimagecapturesasnowboarderinmid-air,performingajumpwhileridinga
snowboard.Thesnowboarderiswearingagreenandblackoutfit,andthesnowboardisvisible
beneath them. The scene takes place in a snowy landscape, with a blue sky in the
background.\n\nThere are several other people in the scene, some of themcloser to the
snowboarder and others further away. They appear to be watching the snowboarder's
impressivejump,likelyenjoyingthewintersportsaction.
VisualFactChecker(Ours):Intheimage,apersoninagreenandblackoutfitisskillfullyridinga
snowboardandperformingajump,soaringthroughtheairwhilealsoflyingakitepositioned
highinthesky.Thesnow-coveredfieldprovidesaperfectbackdropforthisthrillingactivity,
highlightingthesnowboarder'stalent,athleticism,skill,andcontrol.
BLIP-2:atablewithavaseofflowersonit
InstructBLIP:Theimagedepictsadiningroomwithawoodentableinthecenter.Onthetable,
(e) therearetwoglassvasesfilledwithvarioustypesofflowers,includingpinkandpurplelilies,
whitedaisies,andredcarnations.Thevasesareplacedoneithersideofthetable,creatinga
visuallyappealingdisplayofcolorfulblooms.Inadditiontothevases,thereisabowlplacedon
thetableaswell.Achaircanbeseeninthecorneroftheroom,providingseatingforsomeone
whomightwanttoenjoythebeautifulfloralarrangement.
Kosmos-2:Theimagefeaturesawoodendiningtablewithavasecontainingvariousflowers,and
asmallervaseontheleftside.Thetableisadornedwithtwovases,oneontherightsideandthe
otheronthefarleft.Inadditiontothevases,therearetwobowlsplacedonthetable,onecloser
totheleftvaseandthesecondonefurthertotheright.Thediningtableissurroundedbyachair
andamirror,addingtotheroom'sambiance.
LLaVA-1.5:Theimagefeaturesadiningtablewithtwovasesfilledwithbeautifulflowers.One
vaseisplacedontheleftsideofthetable,whiletheotherisontherightside.Theflowersinthe
vasesarearrangedinawaythatcreatesavisuallyappealingdisplay.\n\nInadditiontothevases,
thereareseveralbooksscatteredaroundthetable,withsomeplacedneartherightedgeand
othersclosertotheleftside.Achaircanbeseeninthebackground,positionedbehindthetable.
Theoverallscenecreatesapleasantandinvitingatmosphere.
VisualFactChecker(Ours):Theimageshowcasesawoodendiningtableadornedwithavase;a
redvaseholdingabouquetofredandwhiteflowersontheright,andaclearvasefilledwitha
varietyofcolorfulflowersonthefarleft.Theflowersintheclearvasearearrangedtobevisible
frommultipleangles,enhancingthevisualappeal.Thetablesetting,surroundedbyachairanda
mirror,createsapleasantandinvitingatmosphere,suggestingaspecialoccasionorgathering.
BLIP-2:astreetsignandacanadianflagonapole
InstructBLIP:TheimagedepictsastreetcornerwithatrafficlightandastreetsignforRobson
Street,locatedinVancouver,Canada.ACanadianflagcanbeseenhangingfromthetopofthetall
buildinginthebackground,addingapatriotictouchtothescene.Thereareseveralcarsparkedon
thesideofthestreet,includingayellowcar,abluecar,andaredcar.Intheforeground,thereisa
pedestriancrossingsign,indicatingthatpedestriansareallowedtocrossthestreetatthislocation.
(f) Overall,theimagecapturesabustlingurbansettingwithamixofvehiclesandpedestrians.
Kosmos-2:Theimagefeaturesastreetsignwiththename\"RobsonStreet\"andaCanadianflag
hangingfromapole.Atallbuildingisvisibleinthebackground,addingtotheurbanatmosphere.
Thestreetsignislocatednearthecornerofthestreetandthetallbuilding,withtheCanadian
flaghangingfromthepole.
LLaVA-1.5:TheimagefeaturesastreetsignonapolewithaCanadianflaghangingfromit.The
streetsignislocatedinfrontofatallbuilding,whichappearstobeaskyscraper.Thesceneisset
againstabluesky,creatingapleasantatmosphere.\n\nInadditiontotheCanadianflag,thereare
severalotherflagsvisibleinthescene,includingafewsmallerflagshangingfromthepoleanda
largerflaginthebackground.Thepresenceoftheseflagssuggeststhatthelocationmightbea
placeofinternationalsignificanceoragatheringspotforpeoplefromvariouscountries.
VisualFactChecker(Ours):Theimagedepictsanurbanscenefeaturingastreetsignlabeled
\"RobsonStreet\"locatednearthecornerofthestreetandatallbuilding.Thesignispositioned
nearatrafficlightonitsleftside.AddingtotheatmosphereisapolefromwhichaCanadianflag
ishanging,infusingasenseofnationalprideintothearea.
Figure10.Comparisonofdifferent2Dimagecaptioningmethods‚Äìmoreexamples(2/3).Input image Captions DALLE-3 text-to-image
BLIP-2:aboysittingonabench
InstructBLIP:Theimagedepictsayoungboysittingonabenchinapark.Heisengrossedin
readingabook,likelyenjoyinghisleisuretimeintheoutdoors.Inthebackground,thereisa
playground with a slide and other play equipment visible. The park appears to be well-
(g) maintainedandinvitingforchildrentoplayandhavefun.
Kosmos-2:Theimagefeaturesayoungboysittingonabenchinapark,readingabook.Heis
surroundedbyvariousplaygroundequipment,includingaslideandaswing.Theparkislocated
inaresidentialarea,andthereareseveralcarsparkednearby.Theboyappearstobeenjoying
histimeinthepark,possiblytakingabreakfromplayingorreading.
LLaVA-1.5:Theimagefeaturesayoungboysittingonaparkbench,readingabook.Heis
positionedinthemiddleofthescene,withthebenchextendingacrossthelowerpartofthe
image.Theboyappearstobeenjoyinghistimeoutdoors,engrossedinhisreading.\n\nThepark
settingisevidentbythepresenceofaswingsetinthebackground,alongwithafewcars
parkednearby.Therearealsoacoupleofotherpeopleinthescene,onestandingclosetothe
boyandanotherfurtheraway.Additionally,thereisabackpackplacedonthegroundnearthe
bench,possiblybelongingtotheboyoranotherparkvisitor.
VisualFactChecker(Ours):Theimagedepictsayoungboy,donnedinayellowjacket,engrossed
inreadingabookwhilesittingonabenchinapark.Thepark,situatedinaresidentialarea,
featuresaplaygroundwithvariousequipmentsuchasaswingsetandaslideinthebackground,
suggestingalivelyandengagingenvironmentforchildren.Severalcarsparkednearbyindicate
theproximityoftheparktotheresidentialarea.Theboyseemstobeenjoyinghistime,possibly
takingabreakfromplaying.
BLIP-2:adogandahorsearestandingnexttoeachother
InstructBLIP:Theimagedepictsadonkey,adog,andahorsestandingonagrassyfieldnexttoa
(h) bodyofwater.Thedonkeyisthelargestanimalinthescene,whilethedogandhorsearesmaller
insize.Thethreeanimalsseemtobeinteractingwitheachother,possiblyplayingorexploring
theirsurroundings.Thedogispositionedtowardstheleftsideoftheimage,whilethehorseis
closertotherightside.Thedonkeyissituatedinthemiddleofthegrassyfield,surroundedbythe
othertwoanimals.
Kosmos-2:Theimagefeaturesawhitedonkeyandasmallbrownandwhitedogstandingnextto
eachotheronagrassyfield.Thedonkeyisstandingonitshindlegs,lookingatthedog,whilethe
dogisstandingnextandlookingatthedonkey.Thefieldislocatednearabodyofwater,adding
tothesereneatmosphereofthescene.
LLaVA-1.5:Theimagefeaturesadogandahorsestandingnexttoeachotherinagrassyfield.
Thedogispositionedtotherightof thehorse,andbothanimalsappeartobelookingat
somethinginthedistance.Thedogissmallerinsizecomparedtothehorse,andtheyseemtobe
enjoyingtheirtimetogetherinthefield.
VisualFactChecker(Ours):Theimagedepictsawhitedonkey,standingonitsthreelegsina
grassyfieldnearabodyofwater,creatingasereneatmosphere.Asmallbrownandwhitedogis
positionedtotherightofthedonkey,bothanimalsseeminglyengagedinlookingateachotheror
somethinginthedistance.
BLIP-2:amanholdingatennisracketandatennisball
InstructBLIP:Intheimage,amanisstandingonatenniscourt,holdingatennisracketand
preparingtoservetheball.Heiswearingawhiteshirtandshorts,andappearstobefocusedon
thegame.Thereareseveraltennisballsscatteredaroundthecourt,someclosertotheplayerand
othersfurtheraway.Thebackgroundfeaturesafenceandtrees,addingtotheoutdooratmosphere
ofthescene.
(i)
Kosmos-2:Theimagefeaturesamaninawhiteshirtandhat,wearingsunglasses,swinginga
tennisracketandpreparingtohitaball.Heisstandingonatenniscourt,andthegreenfenceis
visiblebehindhim.Themanappearstobefocusedandreadytoplayagameoftennis.
LLaVA-1.5:Theimagefeaturesamanstandingonatenniscourt,holdingatennisracketinhis
hand.Heappearstobeinthemiddleofagame,possiblypreparingtoservetheball.Themanis
wearingawhiteshirtandwhiteshorts,whicharetypicalattireforplayingtennis.\n\nThetennis
courtissurroundedbyafence,andthereisagreentarpvisibleinthebackground.Theman's
tennisracketispositionedtowardstherightsideoftheimage,andheseemsfocusedandready
forthenextplay.
VisualFactChecker(Ours):Theimagefeaturesaman,dressedinawhiteshirtandshorts,
wearingahatandsunglasses,standingonatenniscourtwithagreenfencevisiblebehindhim.
Heisholdingatennisracketinhisrighthand,swingingitandpreparingtohitaball,possibly
trackingitsmovement.Themanappearsfocusedandready,suggestingheisinthemiddleofa
tennismatch.
Figure11.Comparisonofdifferent2Dimagecaptioningmethods‚Äìmoreexamples(3/3).Input 3D object Captions MVDreamtext-to-3d GPT-4V analysis
"...straightforwardandaccurate...However,it
Cap3D:A 3D model of a futuristic motorcycle with orange wheels. lacks detail about other elements of the
motorcycle..."
(a)
"... provides a comprehensive description that
VisualFactChecker(Ours):A3Dmodelofasleek,futuristicmotorcyclewithastreamlinedand encapsulatesmoreofwhatisvisibleintheimage
aerodynamicdesign,featuringapredominantlyblackandredcolorscheme,positionedona ...notonlycorrectlyidentifiestheobjectasa
whitebackgroundforaclean,minimalisticappearance.Themotorcycleisequippedwithtwo
large,red-accentedwheelsandablackseat,addingtoitsmodernandstylishappearance. futuristic motorcycle but also includes details
aboutitsdesign,color,andparts."
Cap3D:3D model of a racing car. "... very basic ... correct but lacks detail"
(b) VisualFactChecker(Ours):The3Dobjectisasleekandaerodynamicwhiteandredsportscar, "...amuchmorecomprehensivedescription...
withablackandredstripeonthesideandanumber"4"prominentlydisplayed.Thecarhasa correct but provides a rich and detailed
checkeredpatternonthehood,aspoileronthebackandacurvedroof.Thetiresareblackand descriptionthatwouldgivesomeonewhocannot
sporty,complementingthecar'soveralldesign.Thecarispositionedonaneutralbackground, seetheimageaclearunderstandingofthe3D
whichaccentuatesitspredominantlywhitecolorschemewithredaccentsandblackstripes. model..."
Cap3D:Wooden tripod floor lamp with a 3D model.
"... partially correct ... a vague description. It does
not provide any details ..."
(c)
VisualFactChecker(Ours):The3Dobjectisavintage-style,smallwoodentripodwiththreelegs "...offersamoreprecisedescription...notonly
providingstability.Attachedtothetopofthetripodisaround,silver,metallampwithareflective correctlyidentifiestheobjectasatripodwitha
surface,givingittheappearanceofamirror.Thewoodenlegsofthetripodarebrown,whilethe lamp but also includes details about the
lamp/mirrorhasasilverappearance.Thisfunctionalanddecorativepieceof furnitureor materials,colors,andfunction..."
decoration,withitssimpleyeteffectivedesign,issetagainstaclean,whitebackground.
Cap3D:A 3D-rendered small cake with a pumpkin on top, on a blue plate and napkin.
"...correctlyidentifiestheobject...nomention
...lacksdetail..."
(d) VisualFactChecker(Ours):The3Dobjectisavisuallyappealingdessertfeaturingacakewitha
pumpkinontop,andabirdperchednexttothepumpkin.Thecake,whichisbrownandwhite,is "...correctlydescribes...richindetail...more
placedonablueplate.Thepumpkin,madeofchocolate,isorangeandprotrudesfromthetopof vivid description ... acknowledges all visible
thecake.Thebirdisbrownincolor.Thebird,pumpkin,andcakeareallpartofthesame3D partsofthemodelandtheircharacteristics..."
object,creatingauniqueanddelicious-lookingdessert.
Cap3D:3D model of a blue armored robot with horns.
"...succinct,butitlacksthedescriptiverichness
..."
(e)
VisualFactChecker(Ours):A3Dmodelofarust-covered,bluerobotwithasegmentedbody,
appearstobepartofafuturisticorscience-fictionsetting.Therobotisstandinguprightwith "...goesmuchfurtherindetail...alsospecifies...
armspositionedtotheside.Therobothasahelmetonitshead,whichisalsoblue.Ithasapair whichisn'tcontradictedbytheimage...provides
ofintimidatinghornsonitsbody,resemblingarmororprotectivegear.Theoverallappearance contextandamorevividdescription"
oftherobotisoldandworn,yetunique.
Cap3D:3D model of an antique metal sewing machine.
"... correct. However, it lacks detailed information
about the sewing machine's design, the desk it is
placed on, and the overall setup."
(f)
VisualFactChecker(Ours):The3Dobjectisanantique,metalsewingmachinewithaunique
design,placedonasmall,old-fashioneddeskwithablacktopandametalframe,supportedby
fourlegs.Thedesk,whichalsoservesasthetableforthesewingmachine,hasametalframe "... a far more detailed description ... correctly
andablackmetaltop.Thesewingmachine,withitsblackmetalbody,iscentrallypositionedon identifies the object and provides rich details
thedesk,servingasthefocalpoint.Thesewingmachinehasapedalonthesideforoperation. about its parts, color, material, function, and the
Theentiresetupisplacedagainstawhitebackground,highlightingtheintricatedetailsofthe context ..."
deskandsewingmachine.
Figure12.Comparisonofdifferent3Dobjectcaptioningmethods.Theleftcolumnshowstheinput3Dobjects,themiddlecolumnshows
the captions generated from Cap3D and our VisualFactChecker. In the right column, we use MVDream [29] as a text-to-3D model to
reconstruct3Dobjectsusingdifferent3Dcaptions(generatedbyCap3D[23]andours).Fromtheresults,wecanseethatthereconstructed
3D objects using Cap3D captions are less similar than the input ones compared with ours, suggesting their captions may not contain
sufficientinformationtoreconstructthe3Dobjectorincorrectlydescribethevisualcontents.3Dobjectsreconstructedusingourcaptions
aremoresimilartotheinputs.Input image Captions
GPT-4V:Theimageshowsafamilymomentonthebeach,withwarm
sunlight illuminating the scene. A man is kneeling on the sand,
engagingwithtwoyoungchildren,aboyandagirl.Themanandthe
childrenarefocusedonplayingwithsandtoys,includingbucketsand
shovels.Theyappeartobeintheprocessof buildingasandcastle.
Thegirl,wearingapolka-dotcoat,istotheman'sleft,whiletheboy,
wearingapatternedsweateranddenimpants,isonhisright.It'sa
candid,joyfulmoment,capturingtheessenceoffamilyleisuretimeat
the beach. The background features the soft blur of a hill,
emphasizingthefocusonthefamily.
(a)
VisualFactChecker(Ours):Afatherandhistwochildrenarespending
qualitytimetogetheronasandybeach,engaginginfunactivitieslike
buildingsandcastlesandplayingwithtoys.Thefatheriskneelingdown
onthesand,surroundedbyhischildren,whoseemtobeenjoyingtheir
timetogether.Thereareseveralsandcastlesandtoysscatteredaround
the scene, indicating that the family has been having a great time
playing and building together. The atmosphere is filled with joy and
togetherness,asthefamilymembersenjoyeachother'scompanyon
thebeach.
GPT-4V:The image features a young girl with blonde hair styled in
braids, wearing glasses, and holding a chalkboard sign that reads
"BACKTOSCHOOL"withasmileyfacedrawnbelowthetext.She's
smiling,lookingtowardsthecamera,andappearstobehappyabout
thereturntoschool.Thegirliswearingastripedsleevelesstopandis
positioned against a vibrant yellow background, which creates a
cheerful and energetic atmosphere. The chalkboard and the girl's
posesuggestapromotionalorcelebratorypictureforthebeginning
ofanewschoolyear.
(b)
VisualFactChecker (Ours): A cheerful young girl with glasses is
excitedlyholdingachalkboardsignthatreads"BacktoSchool"infront
of a bright yellow wall. She is smiling and posing for the camera,
showcasingherenthusiasmfortheupcomingschoolyear.Thegirlis
wearingastripedshirtandapairofbluejeans,andherglasseshavea
subtleglintfromthelight.Thechalkboardsignisneatlywritteninwhite
chalk,withthewords"BacktoSchool"andasmilingfacestandingout
clearly against the dark background. The overall atmosphere of the
sceneisoneofexcitementandanticipationforthenewschoolyear.
GPT-4V: The image features a miniature red pickup truck toy
positionedintheforeground,withafocusonitsdetails,highlighting
theglossyfinishandclassicdesign.Inthebackground,slightlyoutof
focus,isastackofbooksandasmallbluehousemodel,bothcreating
asenseofdepthandcontextforthescene.Thebackgroundappears
to be outdoors, as indicated by the green hues that suggest
vegetation. The overall setting seems to be a staged arrangement,
perhaps used for educational purposes or as a decorative display.
Theshallowdepthoffieldusedinthisphotographemphasizesthetoy
(c) truckwhilesoftlyblurringthebackgroundelements.
VisualFactChecker(Ours): A red toy truck is sitting on a table in front of
a blue toy house, giving the impression of a playful and creative setup.
The truck appears to be positioned as if it is driving past the house,
with its front bumper near the house's front door. To the left of the
house, there is a stack of books, possibly suggesting a cozy reading
nook or a place for a child to play and learn. Overall, the scene seems
to be designed to evoke a sense of whimsy and imagination, perhaps
for a child's play area or a collector's display.
Figure13. ComparisonofVisualFactCheckerwithGPT-4V.Ourmethodcangeneratehigh-fidelitydetailedcaptionsthatcloselymatch
GPT-4V‚Äôsquality.TocomparewithGPT-4V,weuseLlama-2asourLLMinsteadofGPT-4whengeneratingcaptionsfortheaboveimages.Input image Additional Prompts Captions
A happy little girl is standing in a green field,
wearingaplaidshirtandholdingontoastringof
pinkballoons.Theballoonsarefloatingintheair,
creatingaplayfulandjoyfulatmosphere.Thegirl
appearstobeenjoyingthemoment,withasmile
on her face. In the background, there is a vast
__ expanseofgreengrass,stretchingoutasfaras
theeyecansee.Theskyaboveisabrightblue,
with a few white clouds scattered about. The
overall mood of the scene is one of carefree
happinessandjoy,withthepinkballoonsadding
a touch of whimsy and playfulness to the
atmosphere.
Joyful girl with pink balloons smiling in a vast
‚Ä¶Describe the imageinless than 20 words
greenfieldunderaclearblue.
A joyful girl in a plaid shirt smiles as she holds
pinkballoonsinavast,greenfieldunderablue
‚Ä¶Describe the image inless than 50 words
sky dotted with clouds, embodying whimsical
happiness.
Ajoyfulyounggirl,cladinaplaidshirt,delightsin
the moment as she clutches a string of pink
‚Ä¶Describe the image and
balloons. The balloons, buoyant and playful,
do not mention the background
dance in the air above her, enhancing her smile
withawhimsicalcharm.
Avast,greenmeadowstretchesendlessly,under
‚Ä¶Describe the image, only mention the abrilliantblueskydottedwithahandfuloffluffy
background and the overall feeling. whiteclouds.Thisidyllicsceneexudesasenseof
Do not mention the foreground objects whimsical joy and carefree happiness, inviting a
playfulandcheerfulmood.
In the backdrop of this serene scene lies a
boundless expanse of lush green grass under a
‚Ä¶Describe the image, starting with the vividblueskydottedwithasmatteringofwhite
background and then foreground objects, clouds.Attheheartofthisidyllicsettingstands
ending with an overall feeling ajoyouslittlegirl,cladinaplaidshirt,herhands
clutchingastringofbuoyantpinkballoonsthat
dance in the air. This moment captures a
snapshotofblissfuljoyandplayfulspirit,framed
bynature'ssprawlingbeauty.
Figure14. Followingcomplexprompts. ByleveraginganLLMtowritethefinalcaption,VisualFactCheckercanfollowcomplexinstruc-
tionstowritecaptionsinvariousstyles.