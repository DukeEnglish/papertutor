MotionLCM: Real-time Controllable Motion
Generation via Latent Consistency Model
WenxunDai1,Ling-HaoChen1,JingboWang2âˆ—,JinpengLiu1,BoDai2âˆ—,YansongTang1
1TsinghuaUniversity,2ShanghaiAILaboratory
{wxdai2001, thu.lhchen, wangjingbo1219, liu.jinpeng.55}@gmail.com
{doubledaibo, tangyansong15}@gmail.com
Projectpage: https://dai-wenxun.github.io/MotionLCM-page
Text-to-Motion Motion Control
â€œa man walks forward at a slow pace â€œa person jauntily
and trips but catches himself.â€ skipsforward.â€
31ms 34ms
SparseControl
â€œa man walks forward â€œthe person was pushed
in a snake like pattern.â€ but did not fall.â€
30ms 36ms
DenseControl
Figure1: WeproposeMotionLCM,areal-timecontrollablemotionlatentconsistencymodel,which
iscapableofachievinghigh-qualitytext-to-motionandprecisemotioncontrolresults(bothsparse
anddenseconditions)inâˆ¼30ms.
Abstract
ThisworkintroducesMotionLCM,extendingcontrollablemotiongenerationto
areal-timelevel. Existingmethodsforspatialcontrolintext-conditionedmotion
generation suffer from significant runtime inefficiency. To address this issue,
wefirstproposethemotionlatentconsistencymodel(MotionLCM)formotion
generation,buildinguponthelatentdiffusionmodel[7](MLD).Byemployingone-
step(orfew-step)inference,wefurtherimprovetheruntimeefficiencyofthemotion
latentdiffusionmodelformotiongeneration. Toensureeffectivecontrollability,
weincorporateamotionControlNetwithinthelatentspaceofMotionLCMand
enableexplicitcontrolsignals(e.g.,pelvistrajectory)inthevanillamotionspace
tocontrolthegenerationprocessdirectly,similartocontrollingotherlatent-free
diffusion models for motion generation [18]. By employing these techniques,
ourapproachcangeneratehumanmotionswithtextandcontrolsignalsinreal-
time. Experimentalresultsdemonstratetheremarkablegenerationandcontrolling
capabilitiesofMotionLCMwhilemaintainingreal-timeruntimeefficiency.
âˆ—Correspondence:JingboWangandBoDai.
Preprint,technicalreport(version1.0).
4202
rpA
03
]VC.sc[
1v95791.4042:viXra1 Introduction
Text-to-motion generation (T2M) has at-
AITS FID Methods
tracted increasing attention [1, 36, 47, 31, 4 0.017s 3.734 TEMOS
12] due to its important roles in many 0.038s 1.067 T2M
applications [23, 51, 53]. Previous at- 3 14.74s 0.630 MotionDiffuse
tempts mainly focus on GANs [1, 27], D IF 02 .4 2. 17 74 ss 0 0..5 44 74
3
M MD LDM
VAEs [15, 35, 36, 3] and diffusion mod- 2
0.030s 0.467 MotionLCM
els[7,47,60,6,8,41,58,61,28]viapair-
wisetext-motiondata[33,13,45,37]and 1 Ours Diffusion models
achieveimpressivegenerationresults. Ex-
isting approaches [47, 7, 60] mainly take 0.02 0.04 0.08 0.1 1 10 20 30
diffusionmodelsasabasegenerativemodel, Average Inference Time per Sentence (AITS) in seconds
Figure2: ComparisonoftheinferencetimecostsonHu-
owing to their powerful ability to model
manML3D[13]. WecomparetheAITSandFIDmetrics
motiondistribution. However, thesediffu-
withsixSOTAmethods.Thecloserthemodelistotheorigin
sionfashionsinevitablyrequireconsiderable
thebetter.Diffusion-basedmodelsareindicatedbytheblue
samplingstepsformotionsynthesisduring
dashedbox.OurMotionLCMachievesreal-timeinference
inference,evenwithsomesamplingacceler- speedwhileensuringhigh-qualitymotiongeneration.
ationmethods[43].Specifically,MDM[47]
andMLD[7]requireâˆ¼12sandâˆ¼0.2stogenerateahigh-qualitymotionsequence.Suchlowefficiency
blockstheapplicationsofgeneratinghigh-qualitymotionsinvariousreal-timescenarios.
Inadditiontothelanguagedescriptionitselfservingasacoarsecontrolsignal,anotherlineofresearch
focusesoncontrollingthemotiongenerationwithspatialconstraints[41,18,54]. Althoughthese
attempts enjoy impressive controlling ability in the T2M task, there still exists a significant gap
towards real-time applications. For example, based on MDM [47], OmniControl [54] exhibits a
relativelylongmotiongenerationtime,âˆ¼72spersequence.Therefore,trading-offbetweengeneration
qualityandefficiencyisachallengingproblem. Asaresult,inthispaper,wetargetthereal-time
controllablemotiongenerationresearchproblem.
Recently, the concept of consistency models [44, 32] has been introduced in image generation,
resultinginsignificantprogressbyenablingefficientandhigh-fidelityimagesynthesiswithaminimal
number of steps (e.g., 4 steps vs. 50 steps). These properties perfectly align with our goal of
acceleratingmotiongenerationwithoutcompromisinggenerationquality. Asaresult,wepropose
MotionLCM (Motion Latent Consistency Model) distilled by a motion latent diffusion model,
MLD[7],totacklethelow-efficiencyproblemindiffusionsampling. Tothebestofourknowledge,
weintroduceconsistencydistillationintothemotiongenerationareaforthefirsttimeandaccelerate
motiongenerationtoareal-timelevelvialatentconsistencydistillation.
Here,inMotionLCM,wearefacinganotherchallengeonhowtocontrolmotionswithspatialsignals
(e.g.,pelvistrajectory)inthelatentspace. Previousmethods[54,6,41,18]modelhumanmotionsin
thevanillamotionspaceandcanmanipulatethemotiondirectlyinthedenoisingprocess. However,
forourlatent-diffusion-basedMotionLCM,itisnon-trivialtofeedthecontrolsignalsintothelatent
space. Thisisbecausecodesinthelatentspacehavenoexplicitmotionsemantics,whichcannotbe
manipulateddirectlybycontrollingsignals. Inspiredbythenotablesuccessof[59]incontrollable
imagegeneration[40],weintroduceamotionControlNettocontrolmotiongenerationinthelatent
space. However, the naÃ¯ve motion ControlNet is not totally sufficient to provide supervision for
controllingsignals. Themainreasonisthelackofexplicitsupervisioninthemotionspace. Therefore,
duringthetrainingphase,wedecodethepredictedlatentcodesthroughthefrozenVAE[20]decoder
intovanillamotionspacetoprovideexplicitcontrolsupervisiononthegeneratedmotion. Thanksto
thepowerfulone-stepinferencecapabilityofMotionLCM,thelatentcodesgeneratedbyMotionLCM
cansignificantlyfacilitatecontrolsupervisionbothinthelatentandmotionspacefortrainingthe
motionControlNetcomparedtoMLD[7].
With our key designs, our proposed MotionLCM successfully enjoys the balance between the
generationqualityandefficiencyincontrollablemotiongeneration. Beforedeliveringintodetail,we
wouldliketosumupourcorecontributionsasfollows.
â€¢ We propose a Motion Latent Consistency Model (MotionLCM) via latent consistency
distillationonthemotionlatentdiffusionmodelextendingcontrollablemotiongenerationto
areal-timelevel.
2â€¢ Building upon our achievement of real-time motion generation, we introduce a motion
ControlNet,enablinghigh-qualitycontrollablemotiongeneration.
â€¢ ExtensiveexperimentalresultsshowthatMotionLCMenjoysagoodbalanceofgeneration
quality,controllingcapability,andreal-timeefficiency.
2 RelatedWork
2.1 HumanMotionGeneration
Generating human motions can be divided into two main fashions according to inputs: motion
synthesis1)withoutanycondition[57,63,62,47,39],and2)withsomegivenmulti-modalconditions,
suchasactionlabels[10,21,35,15,55],textualdescription[2,13,31,3,36,47,60,46,1,27,7,51],
audioormusic[42,26,8,22,48,24]. Togeneratediverse,natural,andhigh-qualityhumanmotions,
many generative models have been explored by [2, 27, 36]. Recently, diffusion-based models
significantlyimprovedthemotiongenerationperformanceanddiversity[47,7,8,60,25,56]with
stabletraining.Specifically,MotionDiffuse[60]representsthefirsttext-basedmotiondiffusionmodel
thatprovidesfine-grainedinstructionsonbodypartsandachievesarbitrary-lengthmotionsynthesis
withtime-variedtextprompts. MDM[47]introducesamotiondiffusionmodelthatoperatesonraw
motiondata,enablingbothhigh-qualitygenerationandgenericconditioningthattogethercomprise
a good baseline for new motion generation tasks. The work most relevant to ours is MLD [7],
whichintroducesamotionlatent-baseddiffusionmodeltoenhancegenerativequalityandreduce
computationalresourcerequirements. TheprimaryconceptinvolvesinitiallytrainingaVAE[20]
for motion embedding, followed by implementing latent diffusion [40] within the learned latent
space. However,thesediffusionfashionsinevitablyrequireconsiderablesamplingstepsformotion
synthesisduringinference,evenwithsomesamplingaccelerationmethods[43]. Thus,wepropose
MotionLCM,whichnotonlyguaranteeshigh-qualitymotiongenerationbutalsoenhancesefficiency.
2.2 MotionControl
Formotioncontrol,MDM[47]andHumanMAC[6]demonstratezero-shotcontrollingusingdiffusion
models. Following this, Shafir et al. [41] propose PriorMDM to generate long-sequence human
motion,enablingjointandtrajectory-levelcontrolandediting. Additionally,GMD[18]incorporates
spatialconstraintsthroughatwo-stagediffusionapproach. However,GMDâ€™scontrolislimitedto
2Dpelvispositions,restrictingitsadaptabilityacrossvariouspracticalscenarios. OmniControl[54]
integratesflexiblespatialcontrolsignalsacrossdifferentjointsbycombininganalyticspatialguidance
andrealismguidanceintothediffusionmodel,ensuringthatthegeneratedmotioncloselyconforms
to the input control signals. TLControl [50] leverages a disentangled latent space for diverse
humanmotion,enablinghigh-fidelitymotiongenerationalignedwithbothlanguagedescriptionsand
specifiedtrajectories. However,TLControlrequiresadditionaltest-timeoptimization. Whilethese
approachesachievegoodcontrolqualityundergivenconditions,thereremainsasignificantgapin
acceleratingthemodeltoreal-timeperformance. Weintroduceourtechnicalsolutionsasfollows.
3 Method
Inthissection,wefirstbrieflyintroducepreliminariesaboutlatentconsistencymodelsinSec.3.1.
Then,wedescribehowtoperformlatentconsistencydistillationformotiongenerationinSec.3.2,
followedbyourimplementationofmotioncontrolinlatentspaceinSec.3.3.
3.1 Preliminaries
The Consistency Model (CM) [44] introduces a kind of efficient generative model designed for
efficient one-step or few-step generation. Given a Probability Flow ODE (a.k.a. PF-ODE) that
smoothly converts data to noise, the CM is to learn the function f(Â·,Â·) (i.e., the solution of the
PF-ODE) that maps any points on the ODE trajectory to its origin distribution. The consistency
functionisformallydefinedasf :(x ,t)(cid:55)âˆ’â†’x ,wheretâˆˆ[0,T],T >0isafixedconstant,Ïµisa
t Ïµ
smallpositivenumbertoavoidnumericalinstability,andthexË† canbetreatedasanapproximate
Ïµ
samplefromthedatadistribution(xË† âˆ¼p (x)). Accordingto[44],theconsistencyfunctionshould
Ïµ data
satisfytheself-consistencyproperty(Definition1).
3Definition1 Self-consistencyProperty.Theself-consistencypropertyofconsistencyfunctionf(Â·,Â·)
canbedefinedas,
f(x ,t)=f(x ,tâ€²),âˆ€t,tâ€² âˆˆ[Ïµ,T]. (1)
t tâ€²
AsshowninDefinition1,theself-consistencypropertyindicatesthatforarbitrarypairsof(x ,t)
t
on the same PF-ODE trajectory, the outputs of the model should be consistent. The goal of a
parameterizedconsistencymodelf (Â·,Â·)istolearnaconsistencyfunctionfromdatabyenforcing
Î˜
theself-consistencypropertyinEq.(1). Toensurethef (x,Ïµ)=xproperty,theconsistencymodel
Î˜
isparameterizedasfollowsviaskipconnections,
f (x,t)=c (t)x+c (t)F (x,t), (2)
Î˜ skip out Î˜
wherec (t)andc (t)aredifferentiablefunctionswithc (Ïµ)=1andc (Ïµ)=0,andF (Â·,Â·)
skip out skip out Î˜
isadeepneuralnetworktolearntheself-consistency. TheCMtrainedfromdistillingtheknowledge
ofpre-traineddiffusionmodelsiscalledConsistencyDistillation. Theconsistencylossisdefinedas,
L(Î˜,Î˜âˆ’;Î¦)=E(cid:2) d(cid:0) f (x ,t ),f (xË†Î¦,t )(cid:1)(cid:3) , (3)
Î˜ tn+1 n+1 Î˜âˆ’ tn n
whered(Â·,Â·)isachosenmetricfunctionformeasuringthedistancebetweentwosamples. f (Â·,Â·)
Î˜
andf (Â·,Â·)arereferredtoasâ€œonlinenetworkâ€andâ€œtargetnetworkâ€accordingto[44]. Besides,
Î˜âˆ’
Î˜âˆ’isupdatedwiththeexponentialmovingaverage(EMA)oftheparameterÎ˜2. InEq.(3),xË†Î¦ is
tn
theone-stepestimationfromx . Here,thexË†Î¦ canbeformulatedas,
tn+1 tn
xË†Î¦ â†x +(t âˆ’t )Î¦(x ,t ), (4)
tn tn+1 n n+1 tn+1 n+1
whereÎ¦(Â·,Â·)isaone-stepODEsolverappliedtoPF-ODE.
Latent Consistency Models (LCMs) [32] conduct the consistency distillation in the latent space
D ={(z,c)|z=E(x),(x,c)âˆˆD},whereDdenotesthedataset,cisthegivencondition,andE
z
isthepre-trainedencoder. Insteadofensuringconsistencybetweenadjacenttimestepst â†’t ,
n+1 n
LCMs[32]aredesignedtoensureconsistencybetweenthecurrenttimestepandk-stepaway,i.e.,
t â†’ t , thereby significantly reducing convergence time costs. As classifier-free guidance
n+k n
(CFG) [16] plays a crucial role in synthesizing high-quality text-aligned visual contents, LCMs
integrateCFGintothedistillationasfollows,
zË†Î¦,w â†z +(1+w)Î¦(z ,t ,t ,c)âˆ’wÎ¦(z ,t ,t ,âˆ…). (5)
tn tn+k tn+k n+k n tn+k n+k n
wherewdenotestheCFGscalewhichisuniformlysampledfrom[w ,w ]andkistheskipping
min max
interval. Besides,theinputofÎ¦isexpandedduetothek-stepconsistencyandthegivencondition.
3.2 MotionLCM:MotionLatentConsistencyModel
Motioncompressionintothelatentspace. Motivatedby[44,32],weproposeMotionLCM(Motion
LatentConsistencyModel)totacklethelow-efficiencyprobleminmotiondiffusionmodels[47,60],
unleashingthepotentialofLCMinthemotiongenerationtask. SimilartoMLD[7],ourMotionLCM
adopts a consistency model in the motion latent space. We choose the powerful MLD [7] as the
underlyingdiffusionmodeltodistillfrom. Weaimtoachieveafew-step(2âˆ¼4)andevenone-step
inferencewithoutcompromisingmotionquality. InMLD,theautoencoder(E,D)isfirsttrainedto
compressahighdimensionalmotionintoalowdimensionallatentvectorz=E(x),whichisthen
decodedtoreconstructthemotionasxË† =D(z). Trainingdiffusionmodelsinthemotionlatentspace
greatlyreducesthecomputationalrequirementscomparedtothevanilladiffusionmodelstrainedon
rawmotionsequences(i.e.,motionspace)andspeedsuptheinferenceprocess. Accordingly,wetake
goodadvantageofthemotionlatentspaceforconsistencydistillation.
Motionlatentconsistencydistillation. Anoverviewofourmotionlatentconsistencydistillation
isdescribedinFig.3(a). Arawmotionsequencex1:N = {xi}N isasequenceofhumanposes
0 i=1
representedbyxi âˆˆRK,whereK isthedimensionoftheposerepresentationandN isthenumber
offrames. Wefollow[13]tousetheredundantmotionrepresentationforourexperiments,whichis
2EMAoperation:Î˜âˆ’ â†sg(ÂµÎ˜âˆ’+(1âˆ’Âµ)Î˜),wheresg(Â·)denotesthestopgradoperationandÂµsatisfies
0â‰¤Âµ<1.
4x1:ğ‘
xà·œ1 0:ğ‘
0 re Latent Space Extract Traj.
â„°d o
c n
z0 Diffuse zğ‘›+ğ‘˜
ğ’Ÿ
E Decoder Motion Space ControlLoss
zà·œğ‘›
Recon.
xà·œ1:ğ‘
zà·œ0
Loss
z0
L
0 ğ’Ÿre d
o c e D
(MNO
oe
tn
it
owl ni Ln
o
Ce
r Mk )
ğš¯EMA
ğš¯âˆ’
NT ea twrg oe rt
k
O ğš½DE
ğ‘˜
S -so tl ev per NTe ea twch oe rğš¯ r kâˆ—
MotionLCM CoM nto rt oio
lNnğš¯ eta
apS
tneta
Traj.
Motion Space zà·œ0 Con Lsi os ste sncy zà·œ0âˆ’ zà·œ0âˆ— zğ‘›
ec Encod ğš¯er
b
(a) Motion Latent Consistency Distillation (b)MotionControlin Latent Space
Figure 3: The overview of MotionLCM. (a) Motion Latent Consistency Distillation (Sec. 3.2).
Givenarawmotionsequencex1:N, apre-trainedVAEencoderfirstcompressesitintothelatent
0
space,thenperformsaforwarddiffusiontoaddn+kstepnoise. Then,thenoisyz isfedinto
n+k
the online network and teacher network to predict the clean latent. The target network takes the
k-stepestimationresultsoftheteacheroutputtopredictthecleanlatent. Tolearnself-consistency,a
lossisappliedtoenforcetheoutputoftheonlinenetworkandtargetnetworktobeconsistent. (b)
MotionControlinLatentSpace(Sec.3.3). WiththepowerfulMotionLCMtrainedinthefirststage,
weincorporateamotionControlNetintotheMotionLCMtoachievecontrollablemotiongeneration.
Furthermore,weleveragethedecodedmotiontosupervisethespatialcontrolsignals.
widelyusedinpreviouswork[47,60,7]. AsshownintheFig.3(a),givenarawmotionsequence
x1:N, a pre-trained VAE encoder first compresses it into the latent space, z = E(x ). Then, a
0 0 0
forwarddiffusionoperationwithn+kstepsisconductedtoaddnoiseonz ,wherekistheskipping
0
intervalillustratedinSec.3.1. Thenoisyz isfedtothefrozenteachernetwork,thetrainable
n+k
onlinenetworktopredictthecleanzË†âˆ—,andzË† . ThetargetnetworkusesthecleanerzË† obtainedbya
0 0 n
k-stepODESolverÎ¦,suchasDDIM[43]topredictthezË†âˆ’. NotethatzË† andzË†âˆ’ areobtainedby
0 0 0
theconsistencyfunctionf(Â·,Â·)inEq.(2). Sincetheclassifier-freeguidance(CFG)isessentialtothe
conditioncontrollingfordiffusionmodels,weintegrateCFGintothelatentconsistencydistillation,
zË† â†z +(1+w)Î¦(z ,t ,t ,c)âˆ’wÎ¦(z ,t ,t ,âˆ…), (6)
n n+k n+k n+k n n+k n+k n
where c is the text condition and w denotes the guidance scale. To ensure the self-consistency
propertydefinedinEq.(1),theconsistencydistillationlossisdesignedas,
L (Î˜,Î˜âˆ’)=E[d(f (z ,t ),f (zË† ,t ))], (7)
LCD Î˜ n+k n+k Î˜âˆ’ n n
where d(Â·,Â·) is a distance measuring function, such as L2 loss or Huber loss [17]. As discussed
inSec.3.1,theparametersofthetargetnetworkareupdatedwiththeexponentialmovingaverage
(EMA)oftheparametersoftheonlinenetwork. Herewedefinetheâ€œteachernetworkâ€asthepre-
trainedmotionlatentdiffusionmodel,e.g.,MLD[7]. Accordingly,theonlineandtargetnetworksare
initializedwiththeparametersoftheteachernetwork.
Duringtheinferencephase,ourMotionLCMcansamplehigh-qualitymotionsinonlyonestepto
produceandachievethefastestruntime(âˆ¼30mspermotionsequence)comparedtoothermotion
diffusionmodels,whichareshowninFig.4.
3.3 ControllableMotionGenerationinLatentSpace
Afteraddressingthelow-efficiencyissueinthemotionlatentdiffusionmodel,wedelveintoanother
exploration of real-time motion controlling. Inspired by the great success of ControlNet [59]
in controllable image generation, we introduce a motion ControlNet on MotionLCM to utilize
the trajectory of joint(s) given by users to control the motion generation in MotionLCM. In our
MotionLCM,weinitializethemotionControlNetwithatrainablecopyofMotionLCM.Specifically,
eachlayerinthemotionControlNetisappendedwithazero-initializedlinearlayerforeliminating
randomnoiseintheinitialtrainingsteps.
AsshowninFig.3(b),thetrajectoryisdefinedastheglobalabsolutepositionsofthecontrolling
joint(s)following[54]. Inourcontrollingpipeline,wedesignaTrajectoryEncoderconsistingof
5stackedtransformer[49]layerstoencodetrajectorysignals. Weappendaglobaltoken(i.e.,[CLS])
beforethestartofthetrajectorysequenceastheoutputvectoroftheencoder,whichisthenaddedto
thenoisyz andfedintothemotionControlNet. Forsimplicity,weomittheinputsoftextcondition
n
candtimestept(botharefedintothefrozenMotionLCMandthetrainablemotionControlNet).
UndertheguidanceofmotionControlNet,MotionLCMpredictsthedenoisedzË† . Thereconstructed
0
latentcanbeoptimizedbythereconstructionloss,
L (Î˜a,Î˜b)=E[d(zË† ,z )], (8)
recon 0 0
whereÎ˜aandÎ˜baretheparametersofthemotionControlNetandTrajectoryEncoder. However,
duringtraining,thesolereconstructionsupervisioninthelatentspaceisinsufficient,whichisalso
verifiedinourmotioncontrolexperimentsinTab.5. Wearguethisisbecausethecontrollablemotion
generationrequiresmoredetailedconstraints,whichcannotbeeffectivelyprovidedsolelybythe
reconstructionlossinthelatentspace.UnlikepreviousmethodslikeOmniControl[54],whichdirectly
diffuse in motion space, allowing explicit supervision of control signals, effectively supervising
controlsignalsinthelatentspaceisnon-trivial. Therefore,weutilizethefrozenVAE[20]decoder
D(Â·)todecodezË† intothemotionspace,obtainingthepredictedmotionxË† ,therebyintroducingthe
0 0
controllinglossasfollows,
(cid:34)(cid:80) (cid:80)
m ||R(xË† ) âˆ’R(x )
||2(cid:35)
L (Î˜a,Î˜b)=E i j ij 0 ij 0 ij 2 , (9)
control (cid:80) (cid:80) m
i j ij
whereR(Â·)convertsthejointâ€™slocalpositionstoglobalabsolutelocations,m âˆˆ{0,1}isthebinary
ij
jointmaskatframeiforthejointj. ThenweoptimizetheparametersinmotionControlNetÎ˜aand
TrajectoryEncoderÎ˜bwithanoverallobjective,
Î˜a,Î˜b =argmin(L +Î»L ), (10)
recon control
Î˜a,Î˜b
whereÎ»istheweighttobalancethetwolosses. Thisdesignenablesexplicitcontrolsignalstodirectly
influencethegenerationprocess,similartocontrollingotherlatent-freediffusionmodelsformotion
generation[18]. Comprehensiveexperimentsdemonstratethatintroducedsupervisionisveryhelpful
inimprovingthequalityofcontrol,whichwillbeintroducedinthefollowingsection.
4 Experiments
Inthissection,wefirstpresenttheexperimentalsetupdetailsinSec.4.1. Subsequently,weprovide
quantitativeandqualitativecomparisonstoevaluatetheeffectivenessofourproposedMotionLCM
framework in Sec. 4.2 and Sec. 4.3. Finally, we conduct comprehensive ablation studies on Mo-
tionLCMinSec.4.4. Theseexperimentsdemonstratetheeffectivenessoftheproposedmethod.
4.1 Experimentalsetup
Datasets. OurexperimentsareconductedonthepopularHumanML3D[13]datasetwhichoffers
anextensivecollectionofhumanmotions,featuring14,616uniquehumanmotionsequencesfrom
AMASS[33]andHumanAct12[15],pairedwith44,970textualdescriptions. Forafaircomparison
withpreviousmethods[13],wetaketheredundantmotionrepresentation,includingrootvelocity,
rootheight,localjointpositions,velocities,rotationsinrootspace,andthefootcontactbinarylabels.
Evaluationmetrics. Weextendtheevaluationmetricsofpreviousworks[13,54,7]. (1)Timecosts:
wereporttheAverageInferenceTimeperSentence(AITS)[7]toevaluatetheinferenceefficiency
ofmodels. (2)Motionquality: FrechetInceptionDistance(FID)isadoptedasaprincipalmetric
toevaluatethefeaturedistributionsbetweenthegeneratedandrealmotions. Thefeatureextractor
employedisfrom[13]. (3)Motiondiversity: MultiModality(MModality)measuresthegeneration
diversityconditionedonthesametextandDiversitycalculatesvariancethroughfeatures[13]. (4)
Conditionmatching: Following[13],wecalculatethemotion-retrievalprecision(R-Precision)to
reportthetext-motionTop1/2/3matchingaccuracyandMultimodalDistance(MMDist)calculates
thedistancebetweenmotionsandtexts. (5)Controlerror: Trajectoryerror(Traj. err.) quantifies
theproportionsofunsuccessfultrajectories,characterizedbyanyjointlocationerrorsurpassinga
predeterminedthreshold. Locationerror(Loc. err.) representstheproportionofjointlocationsthat
6R-Precisionâ†‘
Methods AITSâ†“ FIDâ†“ MMDistâ†“ Diversityâ†’ MModalityâ†‘
Top1 Top2 Top3
Real - 0.511Â±.003 0.703Â±.003 0.797Â±.002 0.002Â±.000 2.794Â±.008 9.503Â±.065 -
Seq2Seq[38] - 0.180Â±.002 0.300Â±.002 0.396Â±.002 11.75Â±.035 5.529Â±.007 6.223Â±.061 -
LJ2P[2] - 0.246Â±.001 0.387Â±.002 0.486Â±.002 11.02Â±.046 5.296Â±.008 7.676Â±.058 -
T2G[4] - 0.165Â±.001 0.267Â±.002 0.345Â±.002 7.664Â±.030 6.030Â±.008 6.409Â±.071 -
Hier[11] - 0.301Â±.002 0.425Â±.002 0.552Â±.004 6.532Â±.024 5.012Â±.018 8.332Â±.042 -
TEMOS[36] 0.017 0.424Â±.002 0.612Â±.002 0.722Â±.002 3.734Â±.028 3.703Â±.008 8.973Â±.071 0.368Â±.018
T2M[13] 0.038 0.457Â±.002 0.639Â±.003 0.740Â±.003 1.067Â±.002 3.340Â±.008 9.188Â±.002 2.090Â±.083
MDM[47] 24.74 0.320Â±.005 0.498Â±.004 0.611Â±.007 0.544Â±.044 5.566Â±.027 9.559Â±.086 2.799Â±.072
MotionDiffuse[60] 14.74 0.491Â±.001 0.681Â±0.001 0.782Â±.001 0.630Â±.001 3.113Â±.001 9.410Â±.049 1.553Â±.042
MLD[7] 0.217 0.481Â±.003 0.673Â±.003 0.772Â±.002 0.473Â±.013 3.196Â±.010 9.724Â±.082 2.413Â±.079
MLDâˆ—[7] 0.225 0.504Â±.002 0.698Â±.003 0.796Â±.002 0.450Â±.011 3.052Â±.009 9.634Â±.064 2.267Â±.082
MotionLCM(1-Step) 0.030 0.502Â±.003 0.701Â±.002 0.803Â±.002 0.467Â±.012 3.022Â±.009 9.631Â±.066 2.172Â±.082
MotionLCM(2-Step) 0.035 0.505Â±.003 0.705Â±.002 0.805Â±.002 0.368Â±.011 2.986Â±.008 9.640Â±.052 2.187Â±.094
MotionLCM(4-Step) 0.043 0.502Â±.003 0.698Â±.002 0.798Â±.002 0.304Â±.012 3.012Â±.007 9.607Â±.066 2.259Â±.092
Table1:Comparisonoftext-conditionalmotionsynthesisonHumanML3D[13]dataset. Wecompute
suggestedmetricsfollowing[13]. Werepeattheevaluation20timesforeachmetricandreportthe
averagewitha95%confidenceinterval. â€œâ†’â€indicatesthattheclosertotherealdata, thebetter.
Boldandunderlineindicatethebestandthesecondbestresult. â€œâˆ—â€denotesthereproducedversion
ofMLD[7]. TheperformanceofourMotionLCMinone-stepinference(30ms)surpassesallstate-
of-the-artmodels,providingampleevidencefortheeffectivenessoflatentconsistencydistillation.
arenotreachedwithinaspecifiedthresholddistance. Averageerror(Avg. err.) denotesthemean
distancebetweenthejointpositionsinthegeneratedmotionandthegivencontroltrajectories.
Implementationdetails. OurbaselinemotiondiffusionmodelisbasedonMLD[7]. Wereproduce
MLDwithhigherperformance. Unlessotherwisespecified,allourexperimentsareconductedonthis
model. ForMotionLCM,weemployanAdamW[19]optimizerfor96Kiterationsusingacosine
decaylearningrateschedulerand1Kiterationsoflinearwarm-up. Abatchsizeof256andalearning
rateof2e-4areused. Wesetthetrainingguidancescalerange[w ,w ]=[5,15]withthetesting
min max
guidance scale as 7.5, and adopt the EMA rate Âµ = 0.95 by default. We use DDIM-Solver [43]
withskippingstepk =20andchoosetheHuber[17]lossasthedistancemeasuringfunctiond. For
motionControlNet,weemployanAdamW[19]foroptimizerforlongertrainingof192Kiterations
with1Kiterationsoflinearwarm-up. Thebatchsizeandlearningratearesetas128and1e-4. The
learning rate scheduler is the same as the first stage. We follow [54] to use the global absolute
locationsofthecontroljointtotrainmotionControlNet. Forthetrainingobjective,weemployd
astheL2lossandsetthecontrollossweightÎ»to1.0bydefault. Weimplementourmodelusing
PyTorch[34]withtrainingononeNVIDIARTX4090GPU.WeuseoneTeslaV100GPUtoalign
thetext-to-motioninferenceexperimentsettingsinMLD[7]. Forcontrollablemotiongeneration,all
modelsareevaluatedfortheirinferencespeedononeNVIDIARTX4090GPU.
4.2 ComparisonsonText-to-motion
Inthefollowingpart,wefirstevaluateourMotionLCMonthetext-to-motion(T2M)task.Wecompare
ourmethodwithsomeT2MbaselinesonHumanML3D[13]withsuggestedmetrics[13]underthe
95%confidenceintervalfrom20timesrunning. AsMotionLCMisbasedonMLD,wemainlyfocus
ontheperformancecomparedwithMLD.Forevaluatingtimeefficiency,wecomparetheAverage
InferenceTimeperSentence(AITS)withTEMOS[36],T2M[13],MDM[47],MotionDiffuse[60]
andMLD[7]. TheresultsareborrowedfromMLD[7]. Thedeterministicmethods[38,2,11,4],
areunabletoproducediverseresultsfromasingleinputandthusweleavetheirMModalitymetrics
empty.Forthequantitativeresults,asshowninTab.1,ourMotionLCMboastsanimpressivereal-time
runtimeefficiency,averagingaround30mspermotionsequenceduringinference. Thisperformance
exceeds that of previous diffusion-based methods [47, 60, 7] and even surpasses MLD [7] by an
orderofmagnitude. Furthermore,despiteemployingonlyone-stepinference,ourMotionLCMcan
approximateorevensurpasstheperformanceofMLD[7](DDIM[43]50steps). Withtwo-step
inference,weachievethebestR-PrecisionandMMDistmetrics,whileincreasingthesamplingsteps
tofouryieldsthebestFID.Theaboveresultsdemonstratetheeffectivenessoflatentconsistency
7Real MotionLCM (Ours) MLD MDM
â€œa person walks clockwisein a large curve while swinging their arms.â€
0.033s 0.201s 22.58s
â€œa person walking like a bird and then sniffing the air.â€
0.031s 0.192s 21.16s
â€œa person slightly bent over with right hand pressing against the air walks forward slowlyâ€
0.031s 0.197s 23.44s
Figure4: Qualitativecomparisonofthestate-of-the-artmethodsintext-to-motiontask. Weprovide
thevisualizedmotionresultsandrealreferencesfromthreetextprompts.Withonlyone-stepinference,
MotionLCMachievesthefastestmotiongenerationwhileproducinghigh-qualitymovementsthat
closelymatchthetextualdescriptions.
AITSâ†“ FIDâ†“ R-Precisionâ†‘ Diversityâ†’ Traj.err.â†“ Loc.err.â†“ Avg.err.â†“
Methods
Top3 (50cm) (50cm)
Real - 0.002 0.797 9.503 0.000 0.000 0.000
MDM[47] 18.84 0.698 0.602 9.197 0.4022 0.3076 0.5959
PriorMDM[47] 18.84 0.475 0.583 9.156 0.3457 0.2132 0.4417
GMDâˆ—[18] 126.10 0.576 0.665 9.206 0.0931 0.0321 0.1439
OmniControlâˆ—[54] 72.88 0.218 0.687 9.422 0.0387 0.0096 0.0338
OmniControl[54] 37.80 0.212 0.678 9.773 0.3041 0.1873 0.3226
MoitonLCM 0.034(â†‘550Ã—) 0.531 0.752 9.253 0.1887 0.0769 0.1897
Table2: Quantitativeresultsofcomparisonwithstate-of-the-artmethodsonHumanML3D[13]test
set. â€œâ†’â€meansclosertorealdataisbetter. â€œâˆ—â€meansthemodelusingguideddiffusion[9]. All
modelsaretrainedonpelviscontrol.
distillation. Forthequalitativeresults,asshowninFig.4,MotionLCMnotonlyacceleratesmotion
generationtoreal-timespeedsbutalsodelivershigh-qualityoutputs,aligningwithtextualdescriptions.
4.3 ComparisonsonControllableMotionGeneration
AsshowninTab.2,wecompareourMotionLCMwith[41,18,54]andreportmetricsfrom[54,7].
DuetoGMD[18]andOmniControl[18]utilizinghigh-costguided-diffusion[9],wereimplement
OmniControlwithoutusingguideddiffusionforafaircomparison. TheresultsindicatethatMo-
tionLCMperformswellinmotioncontrolandachievesstate-of-the-artperformanceintext-motion
alignment,asdemonstratedbyR-Precision. Evenwithoutguideddiffusion,inferenceforasingle
motion in OmniControl [54] still takes around 37 seconds, which is unacceptable for real-time
applications. Incontrast,MotionLCMonlyrequiresâˆ¼0.03seconds,achievingaspeedupofnearly
â†‘1100Ã—overOmniControlandâ†‘550Ã—overthePriorMDM(thefastestmethod).
8R-Precisionâ†‘
Methods FIDâ†“ MMDistâ†“ Diversityâ†’ MModalityâ†‘
Top1
Real 0.511Â±.003 0.002Â±.000 2.794Â±.008 9.503Â±.065 -
MotionLCM(wâˆˆ[5,15]) 0.502Â±.003 0.467Â±.012 3.022Â±.009 9.631Â±.066 2.172Â±.082
MotionLCM(wâˆˆ[2,18]) 0.497Â±.003 0.481Â±.009 3.030Â±.010 9.644Â±.073 2.226Â±.091
MotionLCM(w=7.5) 0.486Â±.002 0.479Â±.009 3.094Â±.009 9.610Â±.072 2.320Â±.097
MotionLCM(Âµ=0.95) 0.502Â±.003 0.467Â±.012 3.022Â±.009 9.631Â±.066 2.172Â±.082
MotionLCM(Âµ=0.50) 0.498Â±.003 0.478Â±.009 3.022Â±.010 9.655Â±.071 2.188Â±.087
MotionLCM(Âµ=0) 0.499Â±.003 0.505Â±.008 3.018Â±.009 9.706Â±.070 2.123Â±.085
MotionLCM(k=50) 0.488Â±.003 0.547Â±.011 3.096Â±.010 9.511Â±.074 2.324Â±.091
MotionLCM(k=20) 0.502Â±.003 0.467Â±.012 3.022Â±.009 9.631Â±.066 2.172Â±.082
MotionLCM(k=10) 0.497Â±.003 0.449Â±.009 3.017Â±.010 9.693Â±.075 2.133Â±.086
MotionLCM(k=5) 0.488Â±.003 0.438Â±.009 3.044Â±.009 9.647Â±.074 2.147Â±.083
MotionLCM(k=1) 0.442Â±.002 0.635Â±.011 3.255Â±.008 9.384Â±.080 2.146Â±.075
MotionLCM(w/Huber) 0.502Â±.003 0.467Â±.012 3.022Â±.009 9.631Â±.066 2.172Â±.082
MotionLCM(w/L2) 0.486Â±.002 0.622Â±.010 3.114Â±.009 9.573Â±.069 2.218Â±.086
Table3:Ablationstudyondifferenttrainingguidancescalerange[w ,w ],EMArateÂµ,skipping
min max
intervalk andlosstype. WeusemetricsinTab.1andadoptaone-stepinferencesettingwiththe
testingCFGscaleof7.5forfaircomparison.
R-Precision(Top3)â†‘ FIDâ†“
Methods
1-Step 2-Step 4-Step 1-Step 2-Step 4-Step
DDIM[43] 0.337Â±.002 0.375Â±.002 0.460Â±.003 4.022Â±.043 2.802Â±.038 0.966Â±.018
DPM[29] 0.337Â±.002 0.374Â±.002 0.477Â±.002 4.022Â±.043 2.798Â±.038 0.727Â±.015
DPM++[30] 0.337Â±.002 0.375Â±.002 0.478Â±.003 4.022Â±.043 2.798Â±.038 0.684Â±.015
MotionLCM 0.803Â±.002 0.805Â±.002 0.798Â±.002 0.467Â±.012 0.368Â±.011 0.304Â±.012
Table4: QuantitativeresultswiththetestingCFGscalew =7.5. MotionLCMnotablyoutperforms
baselinemethodsonHumanML3D[13]dataset,demonstratingtheeffectivenessoflatentconsistency
distillation. Boldindicatesthebestresult.
4.4 AblationStudies
ImpactofthehyperparametersoftrainingMotionLCM.Weconductacomprehensiveanalysisof
thetraininghyperparametersofMotionLCM,includingthetrainingguidancescalerange[w ,w ],
min max
EMArateÂµ,skippingintervalk,andthelosstype. Wesummarizetheevaluationresultsbasedon
one-stepinferenceinTab.3. Wefindoutthatusingadynamictrainingguidancescaleduringtraining
leadstoanimprovementinmodelperformancecomparedtousingastatictrainingguidancescale,
suchasw =7.5. Additionally,anexcessivelylargerangeforthetrainingguidancescalecanalso
negativelyimpacttheperformanceofthemodel(e.g.,w âˆˆ[2,18]). RegardingtheEMArateÂµ,we
observethatthelargerthevalueofÂµ,thebettertheperformanceofthemodel,whichindicatesthat
maintainingaslowerupdaterateforthetargetnetworkÎ˜âˆ’helpsimprovetheperformanceoflatent
consistencydistillation. Whentheskippingintervalkcontinuestoincrease,theperformanceofthe
distillationmodelimprovescontinuously,butlargervaluesofk(e.g.,k =50)mayresultininferior
results. Asforthelosstype,theHuberloss[17]significantlyoutperformstheL2loss,demonstrating
thesuperiorrobustnessofthemethod.
ComparisontootherODESolvers. Tovalidatetheeffectivenessoflatentconsistencydistillation,
we compare three ODE solvers (DDIM [43], DPM [29], DPM++ [30]). The quantitative results
inTab.4demonstratethatourMotionLCMnotablyoutperformsbaselinemethods. Moreover,unlike
DDIM[43], DPM[29], andDPM++[30], requiringmorepeakmemorypersamplingstepwhen
usingCFG,MotionLCMonlyrequiresoneforwardpass,savingbothtimeandmemorycosts.
9AITS FIDâ†“ R-Precisionâ†‘ Diversityâ†’ Traj.err.â†“ Loc.err.â†“ Avg.err.â†“
Methods
Top3 (50cm) (50cm)
Real - 0.002 0.797 9.503 0.000 0.000 0.000
MLD[7](Î»=0) 0.447 0.784 0.722 9.371 0.3666 0.2150 0.3937
MLD[7](Î»=1) 0.447 0.829 0.744 9.221 0.3139 0.1710 0.2893
MotionLCM(Î»=0) 0.034 0.325 0.763 9.164 0.2693 0.1354 0.2805
MoitonLCM(Î»=1) 0.034 0.531 0.752 9.253 0.1887 0.0769 0.1897
Table5:QuantitativeexperimentalresultsofmotioncontrolforMLD[7]andMotionLCM.Thelatent
generatedbyMotionLCMismorebeneficialfortrainingmotionControlNetcomparedtoMLD[7].
Thetwomodelsaretrainedonpelviscontrol.
TheeffectivenessofMotioLCMlatentformotioncontrol. Toverifytheeffectivenessofthelatent
generatedbyourMotionLCMcomparedtoMLDfortrainingmotionControlNet,weconductedthe
followingtwosetsofexperiments: onewithÎ» = 0,meaningsupervisiononlyinthelatentspace,
andtheotherwithÎ»=1,indicatingadditionalsupervisioninthemotionspacewithacontrolloss
weightof1. WepresenttheexperimentalresultsinTab.5. Itcanbeobservedthatunderthesame
experimentalsettings,MotionLCMmaintainshigherfidelityandsignificantlyoutperformsMLDin
motioncontrolperformance.Furthermore,intermsofinferencespeed,MLDutilizesDDIM[43]with
50steps,whileMotionLCMonlyrequiresone-stepinference,resultinginaâˆ¼â†‘13Ã—speedup. This
demonstratestheeffectivenessofthelatentgeneratedbyMotionLCMfortrainingmotionControlNet.
5 ConclusionandLimitation
Conclusion.Thisworkproposesanefficientcontrollablemotiongenerationframework,MotionLCM.
Followingthecorepipelineofthepreviouscontrollablemotiongenerationframework,MotionLCM
isbasedonamotionlatentdiffusionmodel. Byintroducinglatentconsistencydistillation,MotionLM
enjoys the trade-off between runtime efficiency and generation quality. Moreover, thanks to the
motionControlNetmanipulationinthelatentspace,ourmethodenjoysgoodcontrollingabilitywith
givenconditions. Extensiveexperimentsshowtheeffectivenessofourmethodandkeydesigns.
Limitationandfuturework. AlthoughMotionLCMenjoysagoodtrade-offbetweengeneration
qualityandefficiencyinthetext-to-motiontask. However,forthemotioncontroltask,OmniCon-
trol[54]andGMD[18],whichutilizeguideddiffusion[9],stilloutperformMotionLCMsignificantly
intermsofperformance. Besides,wedonotresolvethephysicalimplausibleproblemofgenerated
motionandlearnmotiondistributionfromnoisyoranomalousdata[5,52]. Weleavetheseissuesas
ourfuturework. Exceptforthese,wewillalsofocusonautomaticallyannotatinghigh-qualitytexts
formotions,targetingbuildingaclosed-loopofbi-directionaltext-motionsynthesis,whichisquite
essentialtoalargerdatascale.
Acknowledgement
TheauthorteamwouldliketoacknowledgeShunlinLu(TheChineseUniversityofHongKong,
Shenzhen),YimingXie(NortheasternUniversity),ZhiyangDou(TheUniversityofHongKong),and
JiahaoCui(SunYat-senUniversity)fortheirhelpfultechnicaldiscussionandsuggestions.
10References
[1] Ahn,H.,Ha,T.,Choi,Y.,Yoo,H.,Oh,S.: Text2action: Generativeadversarialsynthesisfrom
languagetoaction.In: ICRA.pp.5915â€“5920(2018)
[2] Ahuja,C.,Morency,L.P.: Language2pose: Naturallanguagegroundedposeforecasting.In:
3DV.pp.719â€“728(2019)
[3] Athanasiou,N.,Petrovich,M.,Black,M.J.,Varol,G.: Teach: Temporalactioncompositionfor
3dhumans.In: 3DV.pp.414â€“423(2022)
[4] Bhattacharya, U., Rewkowski, N., Banerjee, A., Guhan, P., Bera, A., Manocha, D.:
Text2gestures: A transformer-based network for generating emotive body gestures for vir-
tualagents.In: VR.pp.1â€“10(2021)
[5] Chen,L.H.,Li,H.,Zhang,W.,Huang,J.,Ma,X.,Cui,J.,Li,N.,Yoo,J.: Anomman: Detect
anomaliesonmulti-viewattributednetworks.InformationSciences628,1â€“21(2023)
[6] Chen,L.H.,Zhang,J.,Li,Y.,Pang,Y.,Xia,X.,Liu,T.:Humanmac:Maskedmotioncompletion
forhumanmotionprediction.In: ICCV.pp.9544â€“9555(2023)
[7] Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,G.: Executingyourcommandsvia
motiondiffusioninlatentspace.In: CVPR.pp.18000â€“18010(2023)
[8] Dabral,R.,Mughal,M.H.,Golyanik,V.,Theobalt,C.: Mofusion: Aframeworkfordenoising-
diffusion-basedmotionsynthesis.In: CVPR.pp.9760â€“9770(2023)
[9] Dhariwal,P.,Nichol,A.: Diffusionmodelsbeatgansonimagesynthesis.NeurIPSpp.8780â€“
8794(2021)
[10] Dou,Z.,Chen,X.,Fan,Q.,Komura,T.,Wang,W.: CÂ·ase: Learningconditionaladversarial
skillembeddingsforphysics-basedcharacters.In: SIGGRAPHAsia.pp.1â€“11(2023)
[11] Ghosh, A., Cheema, N., Oguz, C., Theobalt, C., Slusallek, P.: Synthesis of compositional
animationsfromtextualdescriptions.In: ICCV.pp.1396â€“1406(2021)
[12] Guo,C.,Mu,Y.,Javed,M.G.,Wang,S.,Cheng,L.: Momask: Generativemaskedmodelingof
3dhumanmotions.In: CVPR(2024)
[13] Guo,C.,Zou,S.,Zuo,X.,Wang,S.,Ji,W.,Li,X.,Cheng,L.: Generatingdiverseandnatural
3dhumanmotionsfromtext.In: CVPR.pp.5152â€“5161(2022)
[14] Guo, C., Zuo, X., Wang, S., Cheng, L.: Tm2t: Stochastic and tokenized modeling for the
reciprocalgenerationof3dhumanmotionsandtexts.In: ECCV.pp.580â€“597(2022)
[15] Guo,C.,Zuo,X.,Wang,S.,Zou,S.,Sun,Q.,Deng,A.,Gong,M.,Cheng,L.: Action2motion:
Conditionedgenerationof3dhumanmotions.In: ACMMM.pp.2021â€“2029(2020)
[16] Ho,J.,Salimans,T.: Classifier-freediffusionguidance.arXivpreprintarXiv:2207.12598(2022)
[17] Huber,P.J.: Robustestimationofalocationparameter.TheAnnalsofMathematicalStatistics
35(1),73â€“101(1964)
[18] Karunratanakul,K.,Preechakul,K.,Suwajanakorn,S.,Tang,S.: Guidedmotiondiffusionfor
controllablehumanmotionsynthesis.In: ICCV.pp.2151â€“2162(2023)
[19] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980(2014)
[20] Kingma,D.P.,Welling,M.: Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114
(2013)
[21] Lee,T.,Moon,G.,Lee,K.M.: Multiact: Long-term3dhumanmotiongenerationfrommultiple
actionlabels.In: AAAI.pp.1231â€“1239(2023)
[22] Li,B.,Zhao,Y.,Zhelun,S.,Sheng,L.: Danceformer: Musicconditioned3ddancegeneration
withparametricmotiontransformer.In: AAAI.pp.1272â€“1279(2022)
[23] Li,P.,Aberman,K.,Zhang,Z.,Hanocka,R.,Sorkine-Hornung,O.: Ganimator: Neuralmotion
synthesisfromasinglesequence.TOG41(4),1â€“12(2022)
[24] Li,R.,Zhang,Y.,Zhang,Y.,Zhang,H.,Guo,J.,Zhang,Y.,Liu,Y.,Li,X.: Lodge: Acoarseto
finediffusionnetworkforlongdancegenerationguidedbythecharacteristicdanceprimitives.
In: CVPR(2024)
11[25] Li,R.,Zhao,J.,Zhang,Y.,Su,M.,Ren,Z.,Zhang,H.,Tang,Y.,Li,X.: Finedance: Afine-
grainedchoreographydatasetfor3dfullbodydancegeneration.In: ICCV.pp.10234â€“10243
(2023)
[26] Li,R.,Yang,S.,Ross,D.A.,Kanazawa,A.: Aichoreographer: Musicconditioned3ddance
generationwithaist++.In: ICCV.pp.13401â€“13412(2021)
[27] Lin,X.,Amer,M.R.: Humanmotionmodelingusingdvgans.arXivpreprintarXiv:1804.10652
(2018)
[28] Liu, J., Dai, W., Wang, C., Cheng, Y., Tang, Y., Tong, X.: Plan, posture and go: Towards
open-worldtext-to-motiongeneration.arXivpreprintarXiv:2312.14828(2023)
[29] Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,Zhu,J.: Dpm-solver: Afastodesolverfordiffusion
probabilisticmodelsamplinginaround10steps.NeurIPSpp.5775â€“5787(2022)
[30] Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver++: Fast solver for guided
samplingofdiffusionprobabilisticmodels.arXivpreprintarXiv:2211.01095(2022)
[31] Lu, S., Chen, L.H., Zeng, A., Lin, J., Zhang, R., Zhang, L., Shum, H.Y.: Humantomato:
Text-alignedwhole-bodymotiongeneration.arXivpreprintarXiv:2310.12978(2023)
[32] Luo,S.,Tan,Y.,Huang,L.,Li,J.,Zhao,H.: Latentconsistencymodels: Synthesizinghigh-
resolutionimageswithfew-stepinference.arXivpreprintarXiv:2310.04378(2023)
[33] Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of
motioncaptureassurfaceshapes.In: ICCV.pp.5442â€“5451(2019)
[34] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein,N.,Antiga,L.,etal.:Pytorch:Animperativestyle,high-performancedeeplearning
library.NeurIPS32(2019)
[35] Petrovich, M., Black, M.J., Varol, G.: Action-conditioned3dhumanmotionsynthesiswith
transformervae.In: ICCV.pp.10985â€“10995(2021)
[36] Petrovich,M.,Black,M.J.,Varol,G.: Temos: Generatingdiversehumanmotionsfromtextual
descriptions.In: ECCV.pp.480â€“497(2022)
[37] Plappert,M.,Mandery,C.,Asfour,T.: Thekitmotion-languagedataset.Bigdata4(4),236â€“252
(2016)
[38] Plappert, M., Mandery, C., Asfour, T.: Learning a bidirectional mapping between human
whole-bodymotionandnaturallanguageusingdeeprecurrentneuralnetworks.Roboticsand
AutonomousSystems109,13â€“26(2018)
[39] Raab, S., Leibovitch, I., Li, P., Aberman, K., Sorkine-Hornung, O., Cohen-Or, D.: Modi:
Unconditionalmotionsynthesisfromdiversedata.In: CVPR.pp.13873â€“13883(2023)
[40] Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionimagesynthesis
withlatentdiffusionmodels.In: CVPR.pp.10684â€“10695(2022)
[41] Shafir,Y.,Tevet,G.,Kapon,R.,Bermano,A.H.: Humanmotiondiffusionasagenerativeprior.
In: ICLR(2023)
[42] Siyao,L.,Yu,W.,Gu,T.,Lin,C.,Wang,Q.,Qian,C.,Loy,C.C.,Liu,Z.: Bailando: 3ddance
generationbyactor-criticgptwithchoreographicmemory.In: ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.11050â€“11059(2022)
[43] Song,J.,Meng,C.,Ermon,S.: Denoisingdiffusionimplicitmodels.In: ICLR(2021)
[44] Song,Y.,Dhariwal,P.,Chen,M.,Sutskever,I.: Consistencymodels.In: ICML(2023)
[45] Tang,Y.,Liu,J.,Liu,A.,Yang,B.,Dai,W.,Rao,Y.,Lu,J.,Zhou,J.,Li,X.: Flag3d: A3d
fitnessactivitydatasetwithlanguageinstruction.In: CVPR.pp.22106â€“22117(2023)
[46] Tevet,G.,Gordon,B.,Hertz,A.,Bermano,A.H.,Cohen-Or,D.: Motionclip: Exposinghuman
motiongenerationtoclipspace.In: ECCV.pp.358â€“374(2022)
[47] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-Or, D., Bermano, A.H.: Human motion
diffusionmodel.In: ICLR(2022)
[48] Tseng,J.,Castellon,R.,Liu,K.: Edge: Editabledancegenerationfrommusic.In: CVPR.pp.
448â€“458(2023)
12[49] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å.,
Polosukhin,I.: Attentionisallyouneed.NeurIPS(2017)
[50] Wan,W.,Dou,Z.,Komura,T.,Wang,W.,Jayaraman,D.,Liu,L.: Tlcontrol: Trajectoryand
languagecontrolforhumanmotionsynthesis.arXivpreprintarXiv:2311.17135(2023)
[51] Wang,Z.,Chen,Y.,Liu,T.,Zhu,Y.,Liang,W.,Huang,S.: Humanise: Language-conditioned
humanmotiongenerationin3dscenes.NeurIPSpp.14959â€“14971(2022)
[52] Xia,X.,Liu,T.,Wang,N.,Han,B.,Gong,C.,Niu,G.,Sugiyama,M.: Areanchorpointsreally
indispensableinlabel-noiselearning? NeurIPS32(2019)
[53] Xiao,Z.,Wang,T.,Wang,J.,Cao,J.,Zhang,W.,Dai,B.,Lin,D.,Pang,J.:Unifiedhuman-scene
interactionviapromptedchain-of-contacts.In: ICLR(2024)
[54] Xie,Y.,Jampani,V.,Zhong,L.,Sun,D.,Jiang,H.: Omnicontrol: Controlanyjointatanytime
forhumanmotiongeneration.In: ICLR(2023)
[55] Xu,L.,Song,Z.,Wang,D.,Su,J.,Fang,Z.,Ding,C.,Gan,W.,Yan,Y.,Jin,X.,Yang,X.,etal.:
Actformer: A gan-based transformer towards general action-conditioned 3d human motion
generation.In: ICCV.pp.2228â€“2238(2023)
[56] Xu,S.,Li,Z.,Wang,Y.X.,Gui,L.Y.: Interdiff: Generating3dhuman-objectinteractionswith
physics-informeddiffusion.In: ICCV.pp.14928â€“14940(2023)
[57] Yan,S.,Li,Z.,Xiong,Y.,Yan,H.,Lin,D.: Convolutionalsequencegenerationforskeleton-
basedactionsynthesis.In: ICCV.pp.4394â€“4402(2019)
[58] Yuan,Y.,Song,J.,Iqbal,U.,Vahdat,A.,Kautz,J.: Physdiff: Physics-guidedhumanmotion
diffusionmodel.In: ICCV(2023)
[59] Zhang,L.,Rao,A.,Agrawala,M.:Addingconditionalcontroltotext-to-imagediffusionmodels.
In: ICCV.pp.3836â€“3847(2023)
[60] Zhang,M.,Cai,Z.,Pan,L.,Hong,F.,Guo,X.,Yang,L.,Liu,Z.: Motiondiffuse: Text-driven
humanmotiongenerationwithdiffusionmodel.arXivpreprintarXiv:2208.15001(2022)
[61] Zhang, M., Guo, X., Pan, L., Cai, Z., Hong, F., Li, H., Yang, L., Liu, Z.: Remodiffuse:
Retrieval-augmentedmotiondiffusionmodel.In: ICCV(2023)
[62] Zhang, Y., Black, M.J., Tang, S.: Perpetual motion: Generating unbounded human motion.
arXivpreprintarXiv:2007.13886(2020)
[63] Zhao,R.,Su,H.,Ji,Q.: Bayesianadversarialhumanmotionsynthesis.In: CVPR.pp.6225â€“
6234(2020)
13AppendixforMotionLCM
Intheappendix,weprovideadditionaldetailsandexperimentsnotincludedinthemaintext.
â€¢ AppendixA:Additionalexperiments.
â€¢ AppendixB:Detailsoftheevaluationmetrics.
â€¢ AppendixC:Supplementaryquantitativeresults.
A AdditionalExperiments
A.1 ImpactofthecontrollossweightÎ».
As shown in Tab. 6, we conduct ablation studies on the control loss weight Î». We found that as
theweightincreases,thequalityofmotiongeneratedcontinuouslydecreases(asindicatedbyFID),
despite improving motion control performance. To balance motion quality and motion control
performance,wechooseÎ»=1toreportourfinalresults.
Methods FIDâ†“ R-Precisionâ†‘ Diversityâ†’ Traj.err.â†“ Loc.err.â†“ Avg.err.â†“
Top3 (50cm) (50cm)
Real 0.002 0.797 9.503 0.000 0.000 0.000
MotionLCM(Î»=0.0) 0.325 0.763 9.164 0.2693 0.1354 0.2805
MotionLCM(Î»=0.1) 0.370 0.771 9.096 0.2318 0.1113 0.2367
MotionLCM(Î»=1.0) 0.531 0.752 9.253 0.1887 0.0769 0.1897
MotionLCM(Î»=2.0) 0.807 0.748 8.926 0.1482 0.0579 0.1741
MotionLCM(Î»=5.0) 1.724 0.725 8.902 0.1541 0.0468 0.1594
Table 6: Ablation studies on the control loss weight Î». We report the results on pelvis control.
Increasingthecontrollossweightcanenhancemotioncontrolperformancebutwoulddecreasethe
motionquality. Thus,weuseÎ»=1tobalancethetwoaspects.
B MetricDefinitions
Time costs: To assess the inference efficiency of models, we follow [7] to report the Average
Inference Time per Sentence (AITS) measured in seconds. We calculate AITS on the test set of
HumanML3D[13],setthebatchsizeto1,andexcludethetimecostformodelanddatasetloading.
Motionquality: FrechetInceptionDistance(FID)measuresthedistributionaldifferencebetweenthe
generatedandrealmotions,calculatedusingthefeatureextractorassociatedwithaspecificdataset.
Motion diversity: Following [15, 14], we report Diversity and MultiModality to evaluate the
generated motion diversity. Diversity measures the variance of the generated motions across the
wholeset. Specifically,twosubsetsofthesamesizeS arerandomlysampledfromallgenerated
d
motionswiththeirextractedmotionfeaturevectors{v ,...,v }and{vâ€²,...,vâ€² }. Thediversity
1 Sd 1 Sd
metricisdefinedasfollows,
1
(cid:88)Sd
â€²
Diversity= ||v âˆ’v || . (11)
S i i 2
d
i=1
DifferentfromDiversity,MultiModality(MModality)measureshowmuchthegeneratedmotions
diversifywithineachtextdescription. Specifically,asetoftextdescriptionswithsizeC israndomly
sampledfromalldescriptions. ThenwerandomlysampletwosubsetswiththesamesizeI fromall
generatedmotionsconditionedbyc-thtextdescription,withextractedfeaturevectors{v ,...,v }
c,1 c,I
and{vâ€² ,...,vâ€² }. MModalityisformalizedasfollows,
c,1 c,I
C I
1 (cid:88)(cid:88) â€²
MModality= ||v âˆ’v || . (12)
CÃ—I c,i c,i 2
c=1i=1
14Conditionmatching: [13]providesmotion/textfeatureextractorstogenerategeometricallyclosed
featuresformatchedtext-motionpairsandviceversa. Underthisfeaturespace,evaluatingmotion-
retrievalprecision(R-Precision)involvesmixingthegeneratedmotionwith31mismatchedmotions
andthencalculatingthetext-motionTop-1/2/3matchingaccuracy. MultimodalDistance(MMDist)
calculatesthedistancebetweenthegeneratedmotionandtext.
Controlerror: Following[54], wereportTrajectory, Location, andAverageerrorstoassessthe
motioncontrolperformance. Trajectoryerror(Traj. err.) isdefinedastheproportionsofunsuccessful
trajectories,i.e.,ifthereisajointinthegeneratedmotionthatexceedsacertaindistancethreshold
fromthecorrespondingjointinthegivencontroltrajectory,itisviewedasafailedtrajectory. Similar
totheTrajectoryerror,Locationerror(Loc. err.) isdefinedastheproportionofunsuccessfuljoints.
Inourexperiments,weadopt50cmasthedistancethresholdtocalculatetheTrajectoryerrorand
Locationerror. Averageerror(Avg. err.) denotesthemeandistancebetweenthejointpositionsinthe
generatedmotionandthegivencontroltrajectories.
C MoreQualitativeResults
In this section, we provide more qualitative results of our model. Fig. 5 illustrates the modelâ€™s
generationresultsonthetaskoftext-to-motion. Allvideoscanbefoundontheprojectpage.
â€œa person â€œa person is doing â€œthe man is throwing â€œa person runs forward
does a jumpâ€ jumping jacksâ€ his right handâ€ and stops short.â€
â€œthe person is doing â€œa person waves â€œthis person bends â€œthe person is
a dance move.â€ both arms in the air.â€ forwardas if to bow.â€ jogging around.â€
â€œwith arms out to the sides â€œa hunched individual slowly â€œa man walks forward in a
a person walks forwardâ€wobblesforward in a drunken manner.â€ snake like pattern.â€
Figure5: MorequalitativeresultsofMotionLCMonthetaskoftext-to-motion.
15