Lightplane: Highly-Scalable Components for
Neural 3D Fields
Ang Cao1,2, Justin Johnson2, Andrea Vedaldi1, and David Novotny1
1 Meta AI
2 University of Michigan
Abstract. Contemporary 3D research, particularly in reconstruction
and generation, heavily relies on 2D images for inputs or supervision.
However,currentdesignsforthese2D-3Dmappingarememory-intensive,
posing a significant bottleneck for existing methods and hindering new
applications. In response, we propose a pair of highly scalable com-
ponents for 3D neural fields: Lightplane Renderer and Splatter, which
significantly reduce memory usage in 2D-3D mapping. These innova-
tions enable the processing of vastly more and higher resolution im-
ageswithsmallmemoryandcomputationalcosts.Wedemonstratetheir
utility in various applications, from benefiting single-scene optimization
with image-level losses to realizing a versatile pipeline for dramatically
scaling 3D reconstruction and generation. Code: https://github.com/
facebookresearch/lightplane.
1 Introduction
Recent advancements in neural rendering and generative modeling have pro-
pelled significant strides in 3D reconstruction and generation. However, in most
100
~ Monocular Reconstruction 3D Generation
views
A white
bowl with
various
designs
and colors
on the rim
2D 3D Hash 2D Text-to-3D Scene 3D Recon.
Fig.1: We introduce the Lightplane Renderer and Splatter, a pair of highly-
scalable components for neural 3D fields (left). They address the key memory bot-
tleneckof2D-3Dmapping(i.e.renderingandlifting),andreducememoryusagebyup
to four orders of magnitude, which dramatically increases the number of images that
can be processed. We showcase how they can boost various 3D applications (right).
4202
rpA
03
]VC.sc[
1v06791.4042:viXra
‚Ä¶
Splatter
Deep
net
Renderer
‚Ä¶2 A.Cao et al.
cases, these methods are not exclusively 3D; instead, they heavily rely on 2D
images as inputs or for supervision, which demands information mapping be-
tween2Dand3Dspaces.Forinstance,NeuralRadianceFields(NeRFs)[54]use
a photometric loss on 2D images rendered from 3D, bypassing direct 3D super-
vision.Similarly,variousnovelviewsynthesisandgenerationmethods[11,15,98]
employ 2D images as inputs and lift them into 3D space for further processing.
This mapping between 2D and 3D is critical for current 3D research, attributed
to the scarcity of 3D training materials for developing versatile 3D models from
scratch, and the relative ease of acquiring 2D images on a large scale.
Despite its crucial role and widespread use, the process of 2D-3D mapping
incurs a high computational cost, especially in neural 3D fields with volumetric
rendering,whichunderpinsmanyofthemostpowerful3Drepresentations.These
fields are defined by continuous functions that assign values, such as density or
color, to any point in 3D space, regardless of the presence of a physical surface.
Therefore, they are powerful and flexible, preventing initialization in point ren-
dering [38] or topology constraints for meshes [50]. The primary challenge lies
in executing operations across numerous 3D points that span an entire volume.
While these operations can be relatively simple (e.g., evaluating a small mul-
tilayer perceptron (MLP) at each point, or extracting features from 2D input
feature maps), performing them in a differentiable manner is extremely memory
intensiveasallintermediatevaluesmustbekeptinmemoryforbackpropagation.
While the speed of NeRFs has been improved in [12,22,56], the issue of
high memory consumption has seldom been studied. This significant memory
demand hampers scalability of 2D-3D communication, presenting a crucial bot-
tleneck for many existing 3D models and a formidable barrier for potential new
applications.Forexample,thememoryrequirementstorenderevenasinglelow-
resolution image of a neural 3D field can be prohibitive enough to prevent the
applicationofimage-levellossessuchasLPIPS[102]orSDS[61].Omittingsuch
losses leads to a massive performance loss, as e.g. demonstrated by the state-of-
the-art Large Reconstruction Model [3,31]. Additionally, memory-inefficiencies
limit the number of input images and the resolution of the 3D representation,
preventingadvancingfromfew-viewnovelviewsynthesismodels[88,98]tolarge-
scale amortized 3D reconstruction models with many conditioning images.
In this paper, we propose two highly scalable components for neural 3D
fields: Lightplane Renderer and Splatter. These innovations enable 2D-3D map-
pingwithfourordersofmagnitudelessmemoryconsumptionwhilemaintaining
comparable speed. Renderer renders 2D images of 3D models by means of the
standard emission-absorption equations popularized by NeRF [54]. Conversely,
Splatter lifts 2D information to 3D by splatting it onto the 3D representation,
allowing further processing with neural nets. Both components are based on
a hybrid 3D representation that combines ‚Äòhashed‚Äô 3D representations such as
voxel grids and triplanes with MLPs. We use these representations as they are
fast,relativelymemoryefficient,andfamiliartopractitioners,whilecomponents
could be easily extended to other hashed representations as well.Lightplane: Highly-Scalable Components for Neural 3D Fields 3
As aforementioned, storing intermediate values at each 3D point for back-
propagation causes tremendous memory usage. We solve it by creatively re-
configuring inner computations and fusing operations over casted rays instead
of 3D points. Specifically, Lightplane Renderer sequentially calculates features
(e.g., colors) and densities of points along the ray, updating rendered pixels and
transmittance on-the-fly without storing intermediate tensors. This design sig-
nificantly saves memory at the cost of a challenging backpropagation, which we
solve by efficiently recomputing forward activations as needed. Note that the
latter is different from the standard ‚Äúcheckpointing‚Äù trick, whose adoption here
would be of little help. This is because checkpointing still entails caching many
intermediate ray-point values as we march along each ray.
Lightplane Splatter builds on similar ideas with an innovative design, where
splatted features are stored directly into the hash structure underpinning the
3D model, without emitting one value per 3D point. Besides voxel grids which
are usually used for lifting, Splatter could be easily extended to other 3D hash
structures.WeimplementthesecomponentsinTriton[81],aGPUprogramming
languagethatisefficient,portable,andrelativelyeasytomodify.Wewillrelease
the code as an open-source package upon publication.
Likeconvolutionorattention,ourcomponentsaredesignedasbuildingblocks
toboostavarietyof3Dmodelsandapplications.EmpoweredbytheLightplane,
wedeviseapipelinetakinguptoinput100images,significantlyscalingthecom-
municationbetween2Dand3D.WeextensivelyevaluateontheCO3Dv2dataset,
reportingsignificantperformanceimprovementsin colorandgeometryaccuracy
for3Dreconstruction,andbetter3DgenerationmeasuredbyFID/KID.Finally,
we boost performance of the state-of-the-art Large Reconstruction Model [31].
2 Related Work
3D reconstruction using neural 3D fields. Traditional 3D reconstruction
models represented shapes as meshes [26,86], point clouds [20,95], or voxel
grids[16,25].WiththeintroductionofNeRF[54],however,thefocushasshifted
toimplicit 3Drepresentations,oftenutilizingMLPstorepresentoccupancyand
radiance functions defined on a 3D domain. NeRF has been refined in many
ways[4,5,85,101],includingreplacingtheopacityfunctionwithasigned-distance
field to improve the reconstruction of surfaces [46,70,87,91,96,99].
Storing an entire scene in a single MLP, however, means evaluating a com-
plex function anew at every 3D point, which is very expensive from both time
and memory usage. Many authors have proposed to represent radiance fields
with smaller, more local components to improve speed, including using point
clouds [94], tetrahedral meshes [40] or, more often, voxel grids [36,51,64,77,97].
Voxel grids could be further replaced by more compact structures like low-rank
tensordecompositions[13],triplanes[9],hashing[57],andtheircombination[65].
Unliketheabovemethodsfocusingonspeed,Lightplane significantlyreduces
memory demands for neural 3D fields. Note that our method targets neural 3D
fields with volumetric rendering, while point-based rendering like 3DGS [38] are4 A.Cao et al.
notinthisscope,sincetheydon‚Äôtmodelevery3Dpointinthespaceandrelyon
rasterization instead of volumetric rendering. While 3DGS exhibits fast conver-
gence speed, importantly, it has been shown to give lower accuracy (measured
in PSNR) in both the single-scene overfitting case [6], and the few-view recon-
structioncase[82].Foroptimalperformanceinsingle-scene,theyrequirecareful
surface initialization whereas NeRFs converge from a random initialization.
Amortized 3D reconstruction. Amortized (Generalizable) 3D reconstruc-
tion utilizing implicit shape representations was initially approached in [30,58,
66,83,89,98] by warping/pooling features from source views to a target to esti-
mate the color of the underlying scene surfaces. [71,92] introduces latent trans-
former tokens to support the reconstruction. Generalizable triplanes [31,32,42],
ground-planes [73], and voxel grids [34] were also explored.
A common downside of these methods is their memory consumption which
limits them all to a few-view setting with up to 10 source views. They either
are trained on a category-specific dataset or learn to interpolate between input
views with unsatisfactory geometry and 3D consistency. Owing to its memory
efficiency, Lightplane allows more than 100 input source views. We leverage the
latter to train a large-scale 3D model yielding more accurate reconstructions.
Image-supervised 3D generators. WiththeadventofGenerativeAdversar-
ial Networks [27] (GAN), many methods attempted to learn generative models
of 3D shapes given large uncurated image datasets. PlatonicGAN [29], Holo-
GAN [59] and PrGAN [23] learned to generate voxel grids whose renders were
indistinguishable from real object views according to an image-based deep dis-
criminator. The same task was later tackled with Neural Radiance Fields [9,
28,60,72,74], and with meshes [24,93]. The success of 2D generative diffusion
models [19] led to image-supervised models such as HoloDiffusion [37], Forward
Diffusion [80], and PC2 [53], which directly model the distribution of 3D voxel
grids,implicitfieldsandpointcloudsrespectively.Similarly,RenderDiffusion[1]
and ViewsetDiffusion [79] learn a 2D image denoiser by means of a 3D deep
reconstructor.GeNVS[11]andHoloFusion[35]proposed3Dgeneratorswith2D
diffusion rendering post-processors. We demonstrate that Lightplane brings a
strongperformanceboosttoViewsetDiffusionandgeneratesrealistic3Dscenes.
3 Method
WeintroducetheLightplane Renderer andSplatter,whichfacilitatethemapping
ofinformationbetween2Dand3Dspacesinadifferentiablemanner,significantly
reducing memory usage in the process. We first discuss the memory bottlenecks
ofexistingmethodsthatareusedforrenderingandliftingimagesinto3Dstruc-
tures(Sec.3.1).Thenwedefinethehashed3Drepresentations(Sec.3.2)usedin
our framework and functionality (Sec. 3.3) of the proposed components. Lastly,
we discuss their implementations (Sec. 3.4).Lightplane: Highly-Scalable Components for Neural 3D Fields 5
3.1 Preliminary
2D-3DMapping. Mappingbetween2Dimagesand3Dmodelsisamajorprac-
tical bottleneck of many algorithms (Sec. 1), particularly when using powerful
implicit 3D representations such as neural 3D fields. The memory bottleneck
comprises a large number of 3D points from rendering rays and their intermedi-
atefeatures,whicharecachedinGPUmemoryfortheensuingbackpropagation.
More specifically, for ren-
dering (3D to 2D mapping),
i) Autograd Renderer ii) Lightplane Renderer
an entire ray of 3D points
c s imio nn agt glr e eib .pu Wit xe ies tl ht io Mnth t phe ie xc eo lr sl eo n ar d neo drf e Rda ra y m arA cu ht eo rgra d & & &P P Po o oL L LR i i in n- n- -l l le t t ta a a¬∑F F Fyn y
y
¬∑e e e e
e
e
d ¬∑a r a r
a
rt te tM M Mu u ur r r rL L Le e es s sP P
P
P noR i on te t cn f aed ca he tu er r des ker ra ny em larc hF iu ns ge d
points per ray, M R implicit PointFeatures K-dim
√ó &L-layerMLP Feature
representationevaluationsare M rays Ray params Ray params
GPU cache GPU cache
requiredtoget3Dpoints‚Äôcol-
Memory complexity:ùë∂ ùë¥ùë≤ùëπùë≥ ùë∂(ùë¥ùë≤)
orsandopacities.Allthesein-
termediate results, including Fig.2:MemoryusageofourLightplane Renderer
outputs of all MLP layers for vs. a standard autograd NeRF renderer.
every 3D point, are stored in
memory for backpropagation, leading to huge memory usage. Using a tiny MLP
withL=6layersandK=64hiddenunits,M R L K memoryisrequiredtojust
√ó √ó √ó
storetheMLPoutputs,whichtotals12GBfora2562 imagewithR=128points
per ray.
Similarly,toliftN inputfeaturesto3D (2Dto3Dmapping),popularmodels
like PixelNeRF [98] and GeNVS [11] project each 3D point to N input views
individually, and average N sampled feature vectors as the point feature. Even
without considering any MLPs, N memory is used, where is the size
√ó|M| |M|
of 3D structure . When is a 1283 voxel grid with 64-dimensional features,
M M
takes512MBinFP32,leadingto5GBofmemorywithjust10inputviews.
|M|
Moreover,theaforementionedliftingrequires3Dpositionsforprojectionand
cannot be easily generalized to other compact representations like triplanes,
since cells in such ‚Äúhashed‚Äù feature maps (e.g. 2D position on feature planes for
triplanes) don‚Äôt have clearly-defined 3D positions. Hence, directly lifting multi-
view features to triplanes for further processing is still an open problem.
Thememorybottleneckimpactsseveralaspects.Formappingfrom3Dto2D
(i.e.rendering),methodslikeNeRF[54]andPixelNeRF[98]arelimitedtoafew
low-resolution images per training iteration (even using 40GB GPUs) or to sub-
sample rendered pixels, which prohibits image-level losses such as LPIPS [102]
and SDS [61]. For mapping from 2D to 3D, memory demands limit input view
numbersand3Drepresentationsizes.Thehugememoryusagenotonlyoccupies
resources that could otherwise enhance model sizes and capacities but also re-
strictsmodeltrainingandinferenceondeviceswithlimitedmemoryavailability.
Neural 3D fields. Let x R3 denote a 3D point, a neural 3D field is a
volumetricfunctionf thatm‚àà apseachpointxtoavectorf(x) RC.NeRF[54]
‚àà
yar rep
stniop
R6 A.Cao et al.
represented such functions using a single MLP. While this is simple, the MLP
mustrepresentwhole 3Dobjectsandhencemustbelargeandcostlytoevaluate.
Severalapproachesareproposedtosolvethisproblem,bydecomposinginfor-
mation into local buckets, accessing which is more efficient than evaluating the
global MLP. Most famously, [56] utilizes hash tables, but other representations
suchasvoxelgrids[78]andvariouslow-rankdecompositionssuchastriplanes[9],
TensoRF [14] and HexPlane [7,10] also follow this pattern.
3.2 Hybrid representation with 3D hash structure
FollowingtheideainSec.3.1,weuseahybridrepresentationforneural3Dfields
f, and decompose f =g h, where h:R3 RK is a hashing scheme (sampling
operation)for3Dhashst‚ó¶ ructureŒ∏,andg :‚ÜíRK RC isatinyMLP,whichtakes
‚Üí
features from hashing as inputs and outputs the final values. In this paper, we
generalize the concept of 3D hash structures to structures like voxel grids [78],
triplanes[9],HexPlane[7,10]andactualhashtable[56],asobtaininginformation
fromthesestructuresonlyrequiresaccessingandprocessingthesmallamountof
informationstoredinaparticularbucket.Theassociatedhashingschemehtyp-
ically samples 3D point features from hash structure Œ∏ via interpolation, which
is highly efficient. In practice, we operationalize Œ∏ with voxel grids and triplanes
as they are easy to process by neural networks, although other structures with
a differentiable hashing scheme could be easily supported.
In more detail, in the voxel-based representation, Œ∏ is a H W D K tensor
√ó √ó √ó
and h is the tri-linear interpolation on Œ∏ given position x. In the triplane rep-
resentation, Œ∏ is a list of three tensors of dimensions H W K, W D K, and
√ó √ó √ó √ó
D H K. Then, h(x,y,z) is obtained by bilinear interpolation of each plane at
√ó √ó
(x,y), (y,z), (z,x), respectively, followed by summing the resulting three fea-
ture vectors. Again, this design could be easily generalized to other hashed 3D
structures Œ∏ and their corresponding hashing scheme (sampling operation) h.
3.3 Rendering and splatting
We now detail Lightplane Renderer and Splatter, two components using hybrid
3Drepresentationswith3Dhashstructures.Theyaremutuallydualasonemaps
3D information to 2D via rendering, and the other maps 2D images to 3D.
Renderer. Renderer outputspixelfeaturesv (e.g.colors,depths)inadifferen-
tiablewayfromahybridrepresentationf=g h,givenM rays r M andR+1
‚ó¶ { i }i=1
pointsperray.Wemakeitshigh-leveldesignconsistentwithexistinghybridrep-
resentations [7,9,56,78] as they have proven to be powerful, while re-designing
the implementation in Sec. 3.4 to achieve significant memory savings.
Following volumetric rendering of NeRF [54], Renderer uses a generalized
Emission-Absorption(EA)modelandcalculatestransmittanceT ,whichisthe
ij
probability that a photon emitting at x (j-th sampling points on the i-th ray)
ijLightplane: Highly-Scalable Components for Neural 3D Fields 7
ùë£ùë£ ‚Äô! ùë§ùë§!
"
ùë§ùë§ ùë£ $$# ùë£ùë£ #ùë£" $=S %am %#p &l !in ùë§g %ùë£% Pixel raySa fem ap tl ue rd e g hr (i xd i) j MLP g s feaS tup rl ea t vt ie jd
ùë£! ùë§! ùë§# ùë£" Splatting x ij
H
ash
ùë§"
ùë§ùë£ $$ ùë£#ùë£%+= ùë§%ùë£$ Prior 3DHash
ùúΩ!
Pixel i
S
platted 3 ùúΩD
Feat. extractor
a)Samplingandsplatting b)LightplaneSplatter
Fig.3: Lightplane Splatter. (a) On a hash grid with vertex features v i: sampling
obtainspointfeaturesv p byinterpolatingvertexfeaturesweightedbyinversedistance;
splatting updates vertex features by accumulating point feature to vertex using the
sameweights.(b)Splatter involvesthreesteps.Foreach3Dpointalongtheray,Splatter
samples its features from prior 3D hash
Œ∏ÀÜ
(1), calculates features to be splatted using
MLP (2), and splats them to zero-initialized Œ∏ (3).
reaches the sensor. Accordingly, the rendered feature v of ray r is:
i i
R
v = (T T )f (x ). (1)
i i,j 1 ij v ij
‚àí ‚àí
j=1
(cid:88)
where f (x ) is the feature (e.g. color) of the 3D point x , obtained from
v ij ij
the hybrid representation f ; T = exp( j ‚àÜ œÉ(x )), ‚àÜ is the distance
v ij ‚àí n=0 ¬∑ in
betweentwosampledpoints,andœÉ(x )istheopacityofthen-thsampledpoint;
in
(cid:80)
(T T ) [0,1]isthevisibilityofthepointx .Givena3Dpoint,Renderer
i,j 1 ij ij
‚àí ‚àí ‚àà
samples its feature from the 3D representation and feeds the feature to an MLP
g to calculate the opacity. f (x ) is calculated by another MLP g taking the
œÉ v ij v
sampled feature and view directions as inputs.
Splatter. Opposite to Renderer, Splatter maps input view features to 3D hash
structures.Existingworkslike[11,35,37,79]achievethisbyloopingoverallpoints
inside voxel grids and pulling information from input features. They project 3D
pointstoinputviews,interpolatefieldsof2Dimagefeatures,computeandstore
a feature vector for each 3D sample. Such operations are inherently memory-
intensive and cannot be easily generalized to other 3D hash structures Sec. 3.1.
Instead of looping over 3D points and pulling information from inputs, we
make Splatter loop over input pixels/rays and directly push information to 3D
structures. This makes Splatter a reversion of Renderer, being able to easily
extend to other 3D structures and enjoy similar memory optimization designs.
Given M input pixels, Splatter expands each pixel into a ray r with R+1
i
equispaced3Dpointsx ,withpointsalongtherayinheritingthepixel‚Äôsfeatures
ij
v . 3D points‚Äô features v are splatted back to zero-initialized 3D structures Œ∏,
i ij
whichoperationisinversetothesamplingoperationh(x)usedinrendering.This
is done by accumulating v to hash cells that contain x , which accumulation
ij ij
is weighted by splatting weights. After accumulating over all M rays, each hash
cell is normalized by the sum of all splatting weights landing in the cell. The
Image
I
feature
vi
Sampled
image
features
v
Image8 A.Cao et al.
splatting weights are the same as the sampling weights used in rendering. For
voxelgrids,ahashcellisavoxel,andsplattingweightsarethenormalizedinverse
distancebetweenthe3Dpointandeightvoxelvertices.Itcanbeeasilyextended
to other hash structures. We illustrate this splatting operation in Figure 3(b).
This na√Øve version of Splatter works well for voxel grids, but fails to work
on triplanes and potentially other hashed representations. We hypothesize it is
dueto3Dpositioninformationbeingdestroyedwhenreducingfrom3Dspaceto
2D planes, and accumulated features are unaware of the spatial structure of the
3D points. To address this, we propose to use an MLP g to predict a modified
s
feature vector v from the input vectors v , interpolated prior shape encoding
ij i
h Œ∏ÀÜ(x ij), and the positional encoding direnc(r i) of ray direction r ij. For each
sample x , the splatted feature vÀú is
ij ij
vÀú
ij
=g s(v i,h Œ∏ÀÜ(x ij),direnc(r ij)) (2)
Œ∏ÀÜ
isaanotherhashed3Drepresentation,wherepriorshapeencodingof3Dpoint
x
ij
couldbeobtainedbyhashingoperationh Œ∏ÀÜ( ¬∑).ThisMLPallowspointsalong
the same ray to have different spatial-aware features and thus preserves the
spatialstructureofthe3Dpoints.Thisdesignalsoallowsustoiterativelyrefine
3D representations Œ∏ based on previous representations Œ∏ÀÜ and input features.
3.4 Memory-efficient Implementation
We discuss the practical implementations of Lightplane Renderer and Splatter,
which are designed to be memory-efficient and scalable.
Fusing operations along the ray. As analyzed, current rendering and lifting
operations for neural 3D fields are memory intensive, as they treat 3D points
as basic entities and store intermediate results for each point. Alternatively, we
treat rays as basic entities and fuse operations in a single GPU kernel, where
each kernel instance is responsible for a single ray. This allows us to only store
the rendered features and accumulated transmittance of the ray.
As Eq. 1, a Renderer kernel sequentially samples 3D points‚Äô features, cal-
culates features and opacities via MLPs and updates the rendered results and
accumulated transmittance of the ray. These processes are integrated into a sin-
gle kernel, obviating the need for storing any other intermediate results. For the
example in Sec. 3.1, memory usage is significantly reduced from O(MKRL) to
O(MK),decreasingfrom12GBto2KBforanimageofsize2562 withR=128
samples per ray in FP32. This is less than 0.02% of the memory required by the
na√Øveimplementation.SinceSplatter isdesignedtoprocessraysemanatingfrom
input pixels as well (Sec. 3.3), it benefits from the same optimization practice.
Recalculation for backpropagation. Saving no intermediate results during
forward propagation significantly decreases memory usage, while these tensors
areessentialforbackpropagation.Tosolveit,werecomputetheintermediatere-
sults during backpropagation for gradient calculation. Speed-wise, recalculating
the MLP in the forward direction increases the total number of floating-pointLightplane: Highly-Scalable Components for Neural 3D Fields 9
ECCV 2024 Submission #1210 9
RendererFW RendererBW SplatterFW SplatterBW
AutogradFW AutogradBW LiftingFW LiftingBW
5
10
1
1
0.1 0.1
0.01
0.01
104 20000
10000
102
5000
100
2500
16 32 64 128 256 512 10242048 1 2 4 8 16 32 64 128 256
Image size (# pixels = Image size2) Input view number
Fig.4: LightPlane memory & speed benchmark showing the forward (FW and
Fig.4: Lightplane memory & speed benchmark showing the forward (FW and
backward (BW) passes of LightPlane Renderer (left) and Splatter (right), compared
backward(BW)passesofLightplane Renderer (left)andSplatter (right),comparedto
totheAutogradrendererandlifterfrom [10,83].LightPlane exhibitsupto4ordersof
the Autograd renderer and lifter from [11,98]. Lightplane exhibits up to 4 orders of
magnitude lower memory consumption at comparable speed. All axes are log-scaled.
magnitude lower memory consumption at comparable speed. All axes are log-scaled.
304 to on-chip SRAM, and performance bottlenecks often stem from HBM access 304
305 during tensor read/write operations, our kernel maintains a competitive speed 305
306 e ov pe en raw tih oi nle sr be yca ll ec su sla tt hin ag ni 5n 0te %rm ce od mia pt ae rer desu tolts thfo er nb aa √Øc vk epr imop pa lg emat eio nn ta. tW ioe n.en Bc uo tur ta hg ie s 306
307 r ce oa sd te or ns lt yo or ce cf uer rst do ufl ra insh g- ba att ce kn pt ri oo pn a[ g1 a6 t] iofo nr , ad net dai ll es ado sf G toP mU am ssie vm eo mry emhi oe rr yar sc ah vy i. ngs. 307
308 Emission-absorption backpropagation. Renderer and Splatter are dual to 308
309 eLaecvheortahgeirnngotGoPnlUy imn feumncotrioynhaliietyrabructhyalsfoorinstpheeeirdh.ig Th h- elev spel eeim dp cl oe um lden bta et fio un r-. 309
310 TthheerboapctkimpriozpedagbaytioenxpploroitcinesgstohfeShpileartaterrchmicairlroarrschtihteecftourrweaorfdGpPasUs omfeRmeonrdye.rBery, 310
311 afussiitngsaompeprlaestio3nDs ipnoiantsingrgalediGenPtUs fkroemrnetlh,ewereepnrhesaenncteattihoen‚ÄôustiglrizaadtiieonntofifeGldPaUn‚Äôds 311
312 aogng-rcehgipateSsRtAhMem, aanlodngprtehveenrtaymtaossfiovremacthceessintpoutGpPixUe‚Äôls‚Äôshgigrahdibeanntds.wCidotnhvemrseemly-, 312
313 Roreynd(eHreBrM‚Äôs)b.aGcikvwenardthpartoHceBssMisaaclcseosssismpieleadrstoarSepslautbtsetra‚Äônstfioarlwlyarsdlowpaerssc.ompared 313
314 to oNno-tcahbiply,SRthAeMb,acaknpdroppeargfoartmionanocfeRbeonttdleerneerckiss omftoerne sctoemmplfircoamtedHBasMthaeccveisss- 314
315 idbuilriitnygotfen3sDorproeiandts/wisriateffeocpteerdatbiyontsh,eoutrraknesrmneitltamnacientoafinpsreavicooumsppeotiitnitvseisnpetehde 315
316 eemveinsswiohni-laebrseocrapltciuolnatminogdinelt.erDmuerdiniagteforrewsualrtdsfpoarsbs,acwkeprsoepqaugeanttiioanll.yWcealecnuclaotuera3gDe 316
317 preoaindtesr‚Äôsvtiosirbeiflietrytaonfldasimh-paltetmenetniotnth[1e7r]efnodredreintagilesqoufatGioPnUEmq.em(1o)rybyhiseurmarmchiyn.g in 317
318 orderj =1,2,...,asitiseasytoobtainvisibilityT j fromT j 1 (weomitrayin- 318
‚àí
319 d Ee mxf io ssr is oi nm -p al bic si oty r) p. tF io or nb bac ak cw ka pr rd op pa as gs a, to in ona .ra Ry er n, dw ere erde ar niv de Sth ple atv te ec rto ar r- eJa dc uo ab lia tn o 319
320 p er ao chdu oc tt h( ei r.e n. o, tth oe nlq yua innt fi ut ny cc to iom np au lit tyed bd uu tr ain lsg ob inac tk hp er io rp ha ig ga ht -i lo en ve) lo if mR pe ln emde er ne tr a: tion. 320
The backpropagation process of Splatter mirrors the forward pass of Renderer,
R
as it samples 3Ddvpoint graddieœÉn(txsqf)rom the representation‚Äôs gradient field and
321 aggregatesp t‚ä§ hedfm œÉ(axlo qn)g= th‚àíe‚àÜ radfy œÉt(ox qf)orÔ£´m the i( nT pj ‚àíut1 ‚àípixT ejl) ‚Äôa sjg‚àíradT iqea nqtsÔ£∂. C, onverse( l3 y) , 321
j=q+1
Renderer‚Äôs backward process is also simXilar to Splatter‚Äôs forward pass.
Ô£≠ Ô£∏
322 whe Nre ota ajb= ly,p t‚ä§hf evb(x acjk) pa rn od pap gais tit oh ne og fra Rd ei nen dt erv ee rct io sr mth ora et n coe med ps lib ca ac tk edpr ao spa tg ha eti vn ig s-. 322
323 ibilT ityob oa fc 3k Dpro pp oa inga tste isth ar ffo eu cg th edRe bn yde tr he er te rffi anc sie mn it tl ty a, nw ce ec oo fm pp ru et ve ioE uq s.( p3 o) inb ty sm inar tc hh e- 323
324 i en mg ia sslo ion ng -ae ba sc oh rpre tn iod ner min og dr ea ly . Din ut rh ine gre fov re wrs ae rdord pe ar ssq ,= weR s, eR qu‚àíen1 t, i. a. l. ly,s cin alc ce ut lah te ev 3e Dc- 324
325 t po or is nta sj‚Äô a vr ise iba ic lic tu ym au nl dat ie md pf lr eo mm ensa tm thp ele req no dn ew ria nr gds e, quan atd iot nhe Eo qp .a (c 1i )ty byf œÉs( ux mq) ma inff gec it ns 325
326 o on rdly erth je =vi 1s ,ib 2,il .i .ty .,o af ss iu tc ic se es asi sv ye ts oa om bp tl ae is nx vqis, ix bqil+it1y,. T... frT oo mm Take t (h wis ep omos is tib rale y, iw ne - 326
j j 1
327 c da ec xh fe orth se imfi pn la icl itt yr )a .n Fs om rit bt aa cn kc we aT rdR, paw sh s,ic oh ni as rc ao ym rp ,u wt eed dei rn ivt eh te‚àí hf eo vrw eca tr od r-Jp aa cs os( bt ih ai ns 327
328 amounts to one scalar per ray). In backpropagation, we sequentially compute 328
]s[emiT
]BM[yromeM10 A.Cao et al.
product(i.e.,thequantitycomputedduringbackpropagation)ofRenderer:
R
dv dœÉ(x )
q
p ‚ä§ = ‚àÜ (T j 1 T j)a j T qa q , (3)
df œÉ(x q) ‚àí df œÉ(x q)Ô£´ ‚àí ‚àí ‚àí Ô£∂
j=q+1
(cid:88)
Ô£≠ Ô£∏
where a
j
=p ‚ä§f v(x j) and p is the gradient vector that needs backpropagating.
TobackpropagatethroughRenderer efficiently,wecomputeEq.(3)bymarch-
ingalongeachrenderingrayinthereverseorderq =R,R 1,...,sincethevec-
‚àí
tors a are accumulated from sample q onwards, and the opacity f (x ) affects
j œÉ q
only the visibility of successive samples x ,x ,.... To make this possible, we
q q+1
cache the final transmittance T , which is computed in the forward pass(this
R
amounts to one scalar per ray). In backpropagation, we sequentially compute
œÉ(x ) for every 3D point along the ray, and calculate T = T exp(‚àÜœÉ(x ))
j j 1 j j
‚àí ¬∑
from T . This way, similar to the forward pass, the kernel only stores the accu-
j
mulation of per-point features instead of keeping them all in memory.
Difference from checkpointing. Note that the latter is very different from
‚Äúcheckpointing‚Äù which can be trivially enabled for the naive renderer imple-
mentation in autograd frameworks such as PyTorch. This is because, unlike our
memory-efficientbackwardpassfromEq.(3),acheckpointedbackwardpassstill
entails storing all intermediate features along rendering rays in memory.
4 Example applications
Weshowvarious3Dapplicationsthatcouldbeboostedbytheproposedcompo-
nents,fromsingle-sceneoptimizationwithimage-levellossestoaversatileframe-
work for large-scale 3D reconstruction and generation. Results are in Sec. 5.
Single-scene optimization with image-level losses. Constrained by inten-
sivememoryusageduringrendering,existingvolumetricmethodsarelimitedto
optimizing pixel-level losses on a subset of rays, such as MSE, or using image-
levellossesonlow-resolutionimages(64 64).Incontrast,weshowhowRenderer
√ó
allows seamless usage of image-level losses on high-resolution renders.
Multi-view reconstruction. CombiningRenderer andSplatter,weintroduce
a versatile pipeline for 3D reconstruction and generation. Given a set of views
(viewset) = I N and corresponding cameras œÄ N , we train a large-scale
V { i }i=1 { i }i=1
model Œ¶, which directly outputs the 3D representations Œ∏=Œ¶( ,œÄ) of the corre-
V
spondingscenebylearning3Dpriorsfromlarge-scaledata.Reconstructionstarts
by extracting a pixel-wise feature map v=œà(I ) from each image I and lifting
i i
them into the 3D representation Œ∏Àú with Splatter. Model Œ¶ takes Œ∏Àú as input and
outputs the final 3D representations Œ∏=Œ¶ (Œ∏Àú). Finally, Renderer outputs novel
Œ∏
viewimagesIÀÜ= (Œ∏,œÄ)fromŒ∏,andthemodelistrainedbyminimizingtheloss
R
between the novel rendered image and the corresponding ground truth I.
L
3D generation using viewset diffusion. Following recent works [2,79], this
3D reconstruction pipeline could be extended into a diffusion-based 3D genera-
tor with very few changes. This is achieved by considering a noised viewset asLightplane: Highly-Scalable Components for Neural 3D Fields 11
Fitted 3D Scene Stylization 1 Stylization 2
Fig.5: Single-scene optimization with image-level losses. The memory effi-
ciency of Lightplane allows rendering high resolution images in a differentiable way
and backpropagating image-level losses. We show pre-optimized 3D scenes (in unseen
views) and their stylizations with perceptual losses.
inputtothenetwork,andtrainingthemodeltodenoisetheviewset,whereeach
image I is replaced with I = Œ± I +œÉ œµ where t is the noising schedule, Œ±
i it t i t i t
and œÉ = 1 Œ±2 are the noise level, and œµ is a random Normal noise vec-
t ‚àí t i
tor. During inference, the model initializes the viewset with Gaussian noise and
(cid:112)
iteratively denoises by applying the reconstruction model. This process simulta-
neously generates multiple views of the object as well as its 3D model.
5 Experiments
We first benchmark the performance of proposed components, and then demon-
stratetheirpracticalusageforvarious3Dtasks,includingsingle-sceneoptimiza-
tion with image-level loss, and boosting the scalability of large-scale 3D models.
The scalability boost comes from both input-size and modeling. For input-
size, it dramatically increases the amount of 2D information lifted to 3D by
enlarging the number of input views and the output size. For modeling, the
memory savings allow increasing the model and batch size during training.
5.1 Memory & speed benchmark
We measure components‚Äô speed and memory in Figure 4. Renderer (left col.)
is tested on a triplane with 256 points per ray, and compared to a PyTorch
Autograd triplane renderer, adopted from [7,9]. It easily supports high image
sizes with low memory usage, which is unaffordable for the Autograd renderer.
Splatter (right col.) is tested on lifting N input feature maps into a 1603 voxel
grid. We benchmark it against the lifting operations from PixelNeRF [98] and
GeNVS [11], disabling the MLPs in Splatter for a fair comparison. As shown,
Splatter can handle over a hundred views efficiently, while existing methods are
restricted to just a few views. Speed-wise, both components are comparable to
their autograd counterparts. See supplementary material for more results.
5.2 Single-scene Optimization with Image-level Loss.
The memory efficiency of the proposed components, in particular Renderer, al-
lows rendering high-resolution images (e.g. 5122) in a differentiable way with
littlememoryoverhead.Therefore,wecanseamlesslyusemodelswhichtakefull12 A.Cao et al.
Fig.6: Multi-view Large Reconstruction Model (LRM) with Lightplane.
Taking four views as input (leftmost column), we show the RGB renders (mid) and
depth (rightmost column) of the 3D reconstruction.
Input LRM LRM + Lightplane
Fig.7: Visual Comparison of LRM. Adding Lightplane to LRM gives more accu-
rate geometry and appearance with little additional computation and memory cost.
images as input for loss calculation, e.g. perceptual loss [33], LPIPS [102], or
SDS [61], and backpropagate these losses back to neural 3D fields. Constrained
by memory usage, existing methods are limited to very low-resolution render-
ing [47,61] or complicated and inefficient deferred backpropagation [100], while
Lightplane can handle image-level losses easily. We take neural 3D field styliza-
tion as an example in Figure 5 and discuss more applications in supplementary.
Table 1: Quantitative results of
LRM.Theproposedmethodcouldeffec-
tively improve the reconstruction results,
especially geometry (depth L1).
Fig.8: Monocular 3D Reconstruc-
Method PSNR ‚ÜëLPIPS ‚ÜìIOU ‚ÜëDepthL1 ‚Üì tion. With a single clean image as input
LRM[31] 23.7 0.113 0.904 0.208 (1st col.), our model could generate real-
Lightplane+LRM 24.1 0.106 0.916 0.168
istic 3D structures matching the input.
5.3 Multi-view LRM with Lightplane
WefirstvalidateLightplane‚Äôsefficacy,inparticularofthetriplaneRenderer and
Splatter,bycombiningLightplane withLargeReconstructionModel(LRM)[31].
Takingfourimagesasinput,thismodeloutputstriplaneasthe3DrepresentationLightplane: Highly-Scalable Components for Neural 3D Fields 13
Fig.9: Unconditional 3D Generation displaying samples from our Lightplane-
augmented Viewset Diffusion trained on CO3Dv2 [67].
Table 2: Amortized 3D Reconstruction. Our feedforward reconstructor (Light-
plane) trained on the whole CO3Dv2 significantly outperforms baseline View-
Former [41]. We further compare overfitting baselines (Voxel, NeRF [54]) to Light-
plane, and to their scene-tuned versions (‚ÄúFeedforward + Overfit‚Äù). Initializing from
Lightplane-feedforward removes defective geometry leading to better depth error.
Method Mode #views PSNR LPIPS Depthcorr. Time
‚Üë ‚Üì ‚Üë ‚Üì
ViewFormer[41] Feedforward 9 16.4 0.274 N/A N/A
Lightplane Feedforward 10 20.7 0.141 0.356 1.6sec
Lightplane Feedforward 20 20.9 0.136 0.382 1.9sec
Lightplane Feedforward 40 21.4 0.131 0.405 2.5sec
Lightplane Feedforward+Overfit 160 26.2 0.086 0.449 5min
Voxel Overfitfromscratch 160 26.5 0.086 0.373 35min
NeRF Overfitfromscratch 160 26.3 0.108 0.658 1day
via a series of transformer blocks. Every 3 transformer blocks (i.e. 5 blocks in
total), we insert the Splatter layer, which splats source view features into a new
triplane,takingpreviousblockoutputsaspriorshapeencoding.PluggingLight-
plane intoLRMaddslittlecomputationaloverheads,whileclearlyimprovingthe
performance.Additionally,thememoryefficiencyofourrendererenablesLPIPS
optimization without the added complexity of the deferred backpropagation in
LRM [31]. We show the results in Table 1 and Figure 6, 7.
5.4 Large-Scale 3D Reconstruction and Generation.
Datasets and Baselines. We use CO3Dv2 [67] as our primary dataset, a col-
lection of real-world videos capturing objects across 51 common categories. We
implementtheversatilemodelfor3Dreconstructionandgenerationasdescribed
in Sec. 4, and extend Lightplane to unbounded scenes by contracting the ray-
point‚Äôs coordinates [5] to represent background. Without loss of generality, we
utilize UNet [69] with attention layers [84] to process 3D hash structures.
Amortized 3D Reconstruction. Existing amortized 3D reconstruction and
novelviewsynthesismethods[67,83,88,98,103]onlyconsiderafewviews(upto
10) as input due to memory constraints. Here, we enlarge the number of input
views significantly. Unlike existing category-specific models, we train a single14 A.Cao et al.
Table 3: Unconditional 3D Generation on CO3Dv2. Our Lightplane signifi-
cantly outperforms HoloDiffusion [37] and Viewset Diffusion [79]. It even beats Holo-
Fusion [35], a distillation-based method, which takes 30 mins for one generation.
Method Feed- Hydrant Teddybear Apple Donut Mean Inference
forwardFID KID FID KID FID KID FID KID FID KID Time
‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì
HoloFusion[35] 66.80.047 87.6 0.075 69.2 0.063 109.7 0.098 83.3 0.071 30mins
HoloDiffusion[37]
‚úì√ó
100.5 0.079 109.2 0.106 94.5 0.095 115.4 0.085 122.5 0.102 <2min
ViewsetDiffusion[79] ‚úì 150.5 0.124 219.7 0.178 - - - - - - <2min
Lightplane ‚úì 75.1 0.058 87.90.070 32.60.019 44.00.019 59.90.042 <2min
model on all CO3Dv2 categories, targeting a universal reconstruction model
that can work on a variety of object types, and provide useful 3D priors for the
following 3D optimizations. During training, 20 source images from a training
scenearetakenasinputsandMSElossesarecalculatedonfiveothernovelviews.
We evaluate in two regimes: (1) comparing our model to other feedforward
baselines and single-scene overfitting methods to evaluate the model‚Äôs perfor-
mance;(2)finetuningfeedforwardresultsusingtrainingviewsinasinglesceneto
showtheefficacyofourmodelasalearned3Dprior.SincefewgeneralizableNerf
methodscanworkonallcategories,wetakeViewFormer[39]asthefeedforward
model baseline, which directly outputs novel view images using Transformer. In
(2), we use 80 views as inputs to the feedforward model for initialization and
report results of vanilla NeRF [54], and voxel-grid overfits (i.e., trained from
scratch). We evaluate results on novel views of unseen scenes.
Our model generates compelling reconstructions with just a single forward
pass, shown in Tab. 2. After fine-tuning, it is on par with the overfitting base-
lines in color accuracy (PSNR, LPIPS), but largely outperforms the hash-based
baselines (Voxel) in depth error. Since the frames of CO3D‚Äôs real test scenes ex-
hibitlimitedviewpointcoverage,overfittingwithhashedrepresentationsleadsto
strongdefectsingeometry(seeSupp.).Here,byleveragingthememory-efficient
Lightplane for pre-training on a large dataset, our model learns a generic sur-
facepriorwhichfacilitatesdefect-freegeometry.NeRFissuperiorindeptherror
while being on par in PSNR, at the cost of 50 longer training time.
‚àº √ó
Unconditional Generation. Our model is capable of unconditional genera-
tion with only minor modifications, specifically accepting noisy input images
and rendering the clean images through a denoising process. Utilizing the Splat-
ter and Renderer, we can denoise multiple views (10 in experiments) within
each denoising iteration, which significantly enhances the stability of the pro-
cess and leads to markedly improved results. In the inference stage, we input 10
instances of pure noise and proceed with 50 Denoising Diffusion Implicit Model
(DDIM) [75] sampling steps. We compare our method to Viewset Diffusion [79]
andHoloFusion[35]quantitativelyinTab.3andevaluatequalitativelyinFig.9.
Ourresultssignificantlyoutperformotherfeedforwardgenerationmodelsandare
comparable to distillation-based method, which is very time-consuming.
Conditional Generation. We can also introduce one clean image as condi-
tioning, enabling single-view reconstruction. Moreover, our framework is alsoLightplane: Highly-Scalable Components for Neural 3D Fields 15
amenable to extension as a text-conditioned model, utilizing captions as inputs.
We show results and comparison in Figure 17 and Supp.
6 Conclusion
We have introduced Lightplane, a versatile framework that provide two novel
components, Splatter and Renderer, which address the key memory bottleneck
in network that manipulate neural fields. We have showcased the potential of
these primitives in a number of applications, boosting models for reconstruc-
tion, generation and more. Once released to the community, we hope that these
primitives will be used by many to boost their own research as well. 3
7 Acknowledgement
This work was done during Ang Cao‚Äôs internship at Meta AI as well as at the
UniversityofMichigan,partiallysupportedbyagrantfromLGAIResearch.We
thank Roman Shapovalov, Jianyuan Wang, and Mohamed El Banani for their
valuable help and discussions.
3 We discuss limitations and potential negative impact in Supp.16 A.Cao et al.
References
1. Anciukevicius,T.,Xu,Z.,Fisher,M.,Henderson,P.,Bilen,H.,Mitra,N.J.,Guer-
rero, P.: RenderDiffusion: Image diffusion for 3d reconstruction, inpainting and
generation. arXiv.cs abs/2211.09869 (2022) 4
2. Anciukeviƒçius,T.,Xu,Z.,Fisher,M.,Henderson,P.,Bilen,H.,Mitra,N.J.,Guer-
rero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and
generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. pp. 12608‚Äì12618 (2023) 10
3. Anonymous: Instant3d: Fast text-to-3d with sparse-view generation and large
reconstruction model. Under Review (2023) 2, 22, 23
4. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan,P.P.:Mip-nerf:Amultiscalerepresentationforanti-aliasingneuralradiance
fields. In: Proceedings of the IEEE/CVF International Conference on Computer
Vision. pp. 5855‚Äì5864 (2021) 3
5. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-
nerf 360: Unbounded anti-aliased neural radiance fields. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.5470‚Äì
5479 (2022) 3, 13, 22, 24
6. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Zip-nerf:
Anti-aliased grid-based neural radiance fields. arXiv preprint arXiv:2304.06706
(2023) 4
7. Cao,A.,Johnson,J.:Hexplane:Afastrepresentationfordynamicscenes.In:Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. pp. 130‚Äì141 (2023) 6, 11, 22
8. Cao,Z.,Hong,F.,Wu,T.,Pan,L.,Liu,Z.:Large-vocabulary3ddiffusionmodel
with transformer. arXiv preprint arXiv:2309.07920 (2023) 23
9. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,
O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d
generative adversarial networks. In: Proceedings of the IEEE/CVF Conference
onComputerVisionandPatternRecognition.pp.16123‚Äì16133(2022) 3,4,6,11
10. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo,
O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient
geometry-aware 3D generative adversarial networks. In: arXiv (2021) 6
11. Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Ait-
tala, M., Mello, S.D., Karras, T., Wetzstein, G.: GeNVS: Generative novel view
synthesis with 3D-aware diffusion models. In: ICCV (2023) 2, 4, 5, 7, 9, 11, 22
12. Chen,A.,Xu,Z.,Geiger,A.,Yu,J.,Su,H.:Tensorf:Tensorialradiancefields.In:
European Conference on Computer Vision (ECCV) (2022) 2
13. Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: TensoRF: Tensorial radiance fields.
In: arXiv (2022) 3, 26
14. Chen,A.,Xu,Z.,Geiger,A.,Yu,J.,Su,H.:Tensorf:Tensorialradiancefields.In:
European Conference on Computer Vision (ECCV) (2022) 6
15. Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast
generalizableradiancefieldreconstructionfrommulti-viewstereo.In:Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision.pp.14124‚Äì14133
(2021) 2
16. Choy,C.B.,Xu,D.,Gwak,J.,Chen,K.,Savarese,S.:3d-r2n2:Aunifiedapproach
for single and multi-view 3d object reconstruction. In: Computer Vision‚ÄìECCV
2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part VIII 14. pp. 628‚Äì644. Springer (2016) 3Lightplane: Highly-Scalable Components for Neural 3D Fields 17
17. Dao,T.,Fu,D.,Ermon,S.,Rudra,A.,R√©,C.:Flashattention:Fastandmemory-
efficient exact attention with io-awareness. Advances in Neural Information Pro-
cessing Systems 35, 16344‚Äì16359 (2022) 9
18. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248‚Äì255. Ieee (2009) 24
19. Dhariwal,P.,Nichol,A.:Diffusionmodelsbeatgansonimagesynthesis.Advances
in neural information processing systems 34, 8780‚Äì8794 (2021) 4
20. Fan,H.,Su,H.,Guibas,L.J.:Apointsetgenerationnetworkfor3dobjectrecon-
structionfromasingleimage.In:ProceedingsoftheIEEEconferenceoncomputer
vision and pattern recognition. pp. 605‚Äì613 (2017) 3
21. Fridovich-Keil,S.,Meanti,G.,Warburg,F.R.,Recht,B.,Kanazawa,A.:K-planes:
Explicit radiance fields in space, time, and appearance. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.12479‚Äì
12488 (2023) 22
22. Fridovich-Keil,S.,Yu,A.,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:Plenox-
els: Radiance fields without neural networks. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5501‚Äì5510 (2022)
2
23. Gadelha, M., Maji, S., Wang, R.: 3D shape induction from 2D views of multiple
objects. In: arXiv (2016) 4
24. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z.,
Fidler,S.:GET3D:Agenerativemodelofhighquality3dtexturedshapeslearned
from images. arXiv.cs abs/2209.11163 (2022) 4
25. Girdhar,R.,Fouhey,D.F.,Rodriguez,M.,Gupta,A.:Learningapredictableand
generative vector representation for objects. In: Computer Vision‚ÄìECCV 2016:
14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part VI 14. pp. 484‚Äì499. Springer (2016) 3
26. Gkioxari, G., Johnson, J., Malik, J.: Mesh R-CNN. In: Proc. ICCV (2019) 3
27. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A.C., Bengio, Y.: Generative adversarial nets. In: Proc. NeurIPS
(2014) 4
28. Gu, J., Liu, L., Wang, P., Theobalt, C.: Stylenerf: A style-based 3d-aware gener-
ator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985 (2021)
4
29. Henzler, P., Mitra, N.J., Ritschel, T.: Escaping plato‚Äôs cave using adversarial
training:3Dshapefromunstructured2Dimagecollections.In:Proc.ICCV(2019)
4
30. Henzler, P., Reizenstein, J., Labatut, P., Shapovalov, R., Ritschel, T., Vedaldi,
A.,Novotny,D.:Unsupervisedlearningof3dobjectcategoriesfromvideosinthe
wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2021) 4
31. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,
Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv
preprint arXiv:2311.04400 (2023) 2, 3, 4, 12, 13
32. Irshad,M.Z.,Zakharov,S.,Liu,K.,Guizilini,V.,Kollar,T.,Gaidon,A.,Kira,Z.,
Ambrus, R.: Neo 360: Neural fields for sparse view synthesis of outdoor scenes.
In:ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.
pp. 9187‚Äì9198 (2023) 418 A.Cao et al.
33. Johnson,J.,Alahi,A.,Fei-Fei,L.:Perceptuallossesforreal-timestyletransferand
super-resolution. In: Computer Vision‚ÄìECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp.
694‚Äì711. Springer (2016) 12
34. Kar, A., H√§ne, C., Malik, J.: Learning a multi-view stereo machine. Advances in
neural information processing systems 30 (2017) 4
35. Karnewar,A.,Mitra,N.J.,Vedaldi,A.,Novotny,D.:Holofusion:Towardsphoto-
realistic3dgenerativemodeling.In:ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision. pp. 22976‚Äì22985 (2023) 4, 7, 14
36. Karnewar, A., Ritschel, T., Wang, O., Mitra, N.: Relu fields: The little non-
linearitythatcould.In:ACMSIGGRAPH2022ConferenceProceedings.pp.1‚Äì9
(2022) 3
37. Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.: Holodiffusion: Training a 3D
diffusion model using 2D images. In: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (2023) 4, 7, 14
38. Kerbl, B., Kopanas, G., Leimk√ºhler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (July
2023) 2, 3
39. Kulh√°nek,J.,Derner,E.,Sattler,T.,Babu≈°ka,R.:ViewFormer:NeRF-freeneural
rendering from few images using transformers. In: Proc. ECCV (2022) 14
40. Kulhanek, J., Sattler, T.: Tetra-nerf: Representing neural radiance fields using
tetrahedra. arXiv preprint arXiv:2304.09987 (2023) 3
41. Kulh‚Äôanek, J., Derner, E., Sattler, T., Babuvska, R.: Viewformer: Nerf-free neu-
ral rendering from few images using transformers. In: European Conference on
Computer Vision (2022) 13
42. Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K.,
Shakhnarovich,G.,Bi,S.:Instant3d:Fasttext-to-3dwithsparse-viewgeneration
and large reconstruction model. arXiv preprint arXiv:2311.06214 (2023) 4
43. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 26, 28
44. Li,R.,Gao,H.,Tancik,M.,Kanazawa,A.:Nerfacc:Efficientsamplingaccelerates
nerfs. arXiv preprint arXiv:2305.04966 (2023) 25
45. Li, R., Tancik, M., Kanazawa, A.: NerfAcc: A general nerf acceleration toolbox.
arXiv.cs abs/2210.04847 (2022) 25
46. Li, Z., M√ºller, T., Evans, A., Taylor, R.H., Unberath, M., Liu, M.Y., Lin, C.H.:
Neuralangelo: High-fidelity neural surface reconstruction. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.8456‚Äì
8465 (2023) 3
47. Lin,C.H.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,
S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation.
arXiv preprint arXiv:2211.10440 (2022) 12
48. Lin,C.,Gao,J.,Tang,L.,Takikawa,T.,Zeng,X.,Huang,X.,Kreis,K.,Fidler,S.,
Liu, M., Lin, T.: Magic3D: High-resolution text-to-3d content creation. arXiv.cs
abs/2211.10440 (2022) 26
49. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.:
Zero-1-to-3:Zero-shotoneimageto3dobject.In:ProceedingsoftheIEEE/CVF
International Conference on Computer Vision. pp. 9298‚Äì9309 (2023) 22
50. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A differentiable renderer for
image-based 3d reasoning. In: Proceedings of the IEEE/CVF International Con-
ference on Computer Vision. pp. 7708‚Äì7717 (2019) 2Lightplane: Highly-Scalable Components for Neural 3D Fields 19
51. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh,
Y.: Neural volumes: Learning dynamic renderable volumes from images. arXiv
preprint arXiv:1906.07751 (2019) 3
52. Luo,T.,Rockwell,C.,Lee,H.,Johnson,J.:Scalable3dcaptioningwithpretrained
models. arXiv preprint arXiv:2306.07279 (2023) 28
53. Melas-Kyriazi, L., Rupprecht, C., Vedaldi, A.: Pc2 projection-conditioned point
cloud diffusion for single-image 3d reconstruction (2023). https://doi.org/10.
48550/ARXIV.2302.10668 4
54. Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,Ramamoorthi,R.,Ng,
R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In:
ECCV (2020) 2, 3, 5, 6, 13, 14, 26, 30
55. Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,Ramamoorthi,R.,Ng,
R.: NeRF: Representing scenes as neural radiance fields for view synthesis. In:
Proc. ECCV (2020) 26
56. M√ºller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. ACM Transactions on Graphics (ToG)
41(4), 1‚Äì15 (2022) 2, 6, 25
57. M√ºller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. In: Proc. SIGGRAPH (2022) 3, 25
58. Nguyen-Ha,P.,Karnewar,A.,Huynh,L.,Rahtu,E.,Heikkila,J.:Rgbd-net:Pre-
dicting color and depth images for novel views synthesis. In: Proceedings of the
International Conference on 3D Vision (2021) 4
59. Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.: HoloGAN:
Unsupervised learning of 3D representations from natural images. arXiv.cs
abs/1904.01326 (2019) 4
60. Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional genera-
tive neural feature fields. In: Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR) (2021) 4
61. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv preprint arXiv:2209.14988 (2022) 2, 5, 12
62. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv.cs abs/2209.14988 (2022) 26
63. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,etal.:Learningtransferablevisualmodelsfrom
natural language supervision. In: International conference on machine learning.
pp. 8748‚Äì8763. PMLR (2021) 26
64. Reiser,C.,Peng,S.,Liao,Y.,Geiger,A.:KiloNeRF:Speedingupneuralradiance
fields with thousands of tiny MLPs. arXiv.cs abs/2103.13744 (2021) 3
65. Reiser, C., Szeliski, R., Verbin, D., Srinivasan, P., Mildenhall, B., Geiger, A.,
Barron,J.,Hedman,P.:Merf:Memory-efficientradiancefieldsforreal-timeview
synthesis in unbounded scenes. ACM Transactions on Graphics (TOG) 42(4),
1‚Äì12 (2023) 3
66. Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny,
D.: Common Objects in 3D: Large-scale learning and evaluation of real-life 3D
category reconstruction. In: Proc. CVPR (2021) 4
67. Reizenstein, J., Shapovalov, R., Henzler, P., Sbordone, L., Labatut, P., Novotny,
D.:Commonobjectsin3d:Large-scalelearningandevaluationofreal-life3dcat-
egoryreconstruction.In:ProceedingsoftheIEEE/CVFInternationalConference
on Computer Vision. pp. 10901‚Äì10911 (2021) 13, 31, 32
68. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models (2021) 2420 A.Cao et al.
69. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: Medical Image Computing and Computer-Assisted
Intervention‚ÄìMICCAI 2015: 18th International Conference, Munich, Germany,
October 5-9, 2015, Proceedings, Part III 18. pp. 234‚Äì241. Springer (2015) 13
70. Rosu,R.A.,Behnke,S.:Permutosdf:Fastmulti-viewreconstructionwithimplicit
surfacesusingpermutohedrallattices.In:ProceedingsoftheIEEE/CVFConfer-
ence on Computer Vision and Pattern Recognition. pp. 8466‚Äì8475 (2023) 3
71. Sajjadi,M.S.M.,Meyer,H.,Pot,E.,Bergmann,U.,Greff,K.,Radwan,N.,Vora,
S., Lucic, M., Duckworth, D., Dosovitskiy, A., Uszkoreit, J., Funkhouser, T.A.,
Tagliasacchi,A.:Scenerepresentationtransformer:Geometry-freenovelviewsyn-
thesis through set-latent scene representations. CoRR abs/2111.13152 (2021)
4
72. Schwarz,K.,Liao,Y.,Niemeyer,M.,Geiger,A.:Graf:Generativeradiancefields
for3d-awareimagesynthesis.AdvancesinNeuralInformationProcessingSystems
33, 20154‚Äì20166 (2020) 4
73. Sharma, P., Tewari, A., Du, Y., Zakharov, S., Ambrus, R., Gaidon, A., Free-
man, W.T., Durand, F., Tenenbaum, J.B., Sitzmann, V.: Seeing 3d objects
in a single image via self-supervised static-dynamic disentanglement. arXiv.cs
abs/2207.11232 (2022) 4
74. Skorokhodov,I.,Tulyakov,S.,Wang,Y.,Wonka,P.:Epigraf:Rethinkingtraining
of 3d gans. arXiv preprint arXiv:2206.10535 (2022) 4
75. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.arXivpreprint
arXiv:2010.02502 (2020) 14
76. Sun, C., Sun, M., Chen, H.T.: Direct voxel grid optimization: Super-fast conver-
gence for radiance fields reconstruction. 2022 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) pp. 5449‚Äì5459 (2021) 26
77. Sun, C., Sun, M., Chen, H.T.: Direct voxel grid optimization: Super-fast con-
vergence for radiance fields reconstruction. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 5459‚Äì5469 (2022)
3
78. Sun,C.,Sun,M.,Chen,H.:Directvoxelgridoptimization:Super-fastconvergence
for radiance fields reconstruction. In: CVPR (2022) 6
79. Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Viewset diffusion:(0-) image-
conditioned3dgenerativemodelsfrom2ddata.arXivpreprintarXiv:2306.07881
(2023) 4, 7, 10, 14, 22
80. Tewari,A.,Yin,T.,Cazenavette,G.,Rezchikov,S.,Tenenbaum,J.B.,Durand,F.,
Freeman, W.T., Sitzmann, V.: Diffusion with forward models: Solving stochastic
inverse problems without direct supervision. In: arXiv (2023) 4
81. Tillet,P.,Kung,H.T.,Cox,D.:Triton:anintermediatelanguageandcompilerfor
tiled neural network computations. In: Proceedings of the 3rd ACM SIGPLAN
International Workshop on Machine Learning and Programming Languages. pp.
10‚Äì19 (2019) 3
82. Tochilkin, D., Pankratz, D., Liu, Z., Huang, Z., , Letts, A., Li, Y., Liang, D.,
Laforte, C., Jampani, V., Cao, Y.P.: Triposr: Fast 3d object reconstruction from
a single image. arXiv preprint arXiv:2403.02151 (2024) 4
83. Trevithick, A., Yang, B.: Grf: Learning a general radiance field for 3d scene rep-
resentation and rendering. In: arXiv:2010.04595 (2020) 4, 13
84. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser,≈Å.,Polosukhin,I.:Attentionisallyouneed.Advancesinneuralinforma-
tion processing systems 30 (2017) 13Lightplane: Highly-Scalable Components for Neural 3D Fields 21
85. Verbin, D., Hedman, P., Mildenhall, B., Zickler, T., Barron, J.T., Srinivasan,
P.P.: Ref-nerf: Structured view-dependent appearance for neural radiance fields.
In: 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). pp. 5481‚Äì5490. IEEE (2022) 3
86. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Gener-
ating 3d mesh models from single rgb images. In: Proceedings of the European
conference on computer vision (ECCV). pp. 52‚Äì67 (2018) 3
87. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: NeuS: Learn-
ing neural implicit surfaces by volume rendering for multi-view reconstruction.
arXiv.cs abs/2106.10689 (2021) 3
88. Wang,Q.,Wang,Z.,Genova,K.,Srinivasan,P.P.,Zhou,H.,Barron,J.T.,Martin-
Brualla, R., Snavely, N., Funkhouser, T.A.: Ibrnet: Learning multi-view image-
based rendering. 2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) pp. 4688‚Äì4697 (2021) 2, 13
89. Wang,Q.,Wang,Z.,Genova,K.,Srinivasan,P.P.,Zhou,H.,Barron,J.T.,Martin-
Brualla, R., Snavely, N., Funkhouser, T.A.: Ibrnet: Learning multi-view image-
based rendering. In: Proc. CVPR (2021) 4
90. Wang,T.,Zhang,B.,Zhang,T.,Gu,S.,Bao,J.,Baltrusaitis,T.,Shen,J.,Chen,
D., Wen, F., Chen, Q., et al.: Rodin: A generative model for sculpting 3d digital
avatars using diffusion. arXiv preprint arXiv:2212.06135 (2022) 23
91. Wang,Y.,Han,Q.,Habermann,M.,Daniilidis,K.,Theobalt,C.,Liu,L.:Neus2:
Fast learning of neural implicit surfaces for multi-view reconstruction. In: Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision. pp.
3295‚Äì3306 (2023) 3
92. Wu,C.Y.,Johnson,J.,Malik,J.,Feichtenhofer,C.,Gkioxari,G.:Multiviewcom-
pressive coding for 3d reconstruction. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 9065‚Äì9075 (2023) 4
93. Wu,S.,Rupprecht,C.,Vedaldi,A.:Unsupervisedlearningofprobablysymmetric
deformable3dobjectsfromimagesinthewild.In:ProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 1‚Äì10 (2020) 4
94. Xu, Q., Xu, Z., Philip, J., Bi, S., Shu, Z., Sunkavalli, K., Neumann, U.: Point-
NeRF: Point-based neural radiance fields. arXiv.cs abs/2201.08845 (2022) 3
95. Yang,G.,Huang,X.,Hao,Z.,Liu,M.Y.,Belongie,S.,Hariharan,B.:Pointflow:3d
point cloud generation with continuous normalizing flows. In: Proceedings of the
IEEE/CVFinternationalconferenceoncomputervision.pp.4541‚Äì4550(2019) 3
96. Yariv,L.,Gu,J.,Kasten,Y.,Lipman,Y.:Volumerenderingofneuralimplicitsur-
faces. Advances in Neural Information Processing Systems 34, 4805‚Äì4815 (2021)
3
97. Yu,A.,Fridovich-Keil,S.,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:Plenox-
els: Radiance fields without neural networks. arXiv preprint arXiv:2112.05131
(2021) 3, 26
98. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 4578‚Äì4587 (2021) 2, 4, 5, 9, 11, 13
99. Yu,Z.,Peng,S.,Niemeyer,M.,Sattler,T.,Geiger,A.:Monosdf:Exploringmonoc-
ulargeometriccuesforneuralimplicitsurfacereconstruction.Advancesinneural
information processing systems 35, 25018‚Äì25032 (2022) 3
100. Zhang, K., Kolkin, N., Bi, S., Luan, F., Xu, Z., Shechtman, E., Snavely, N.: Arf:
Artistic radiance fields (2022) 12
101. Zhang,K.,Riegler,G.,Snavely,N.,Koltun,V.:Nerf++:Analyzingandimproving
neural radiance fields. arXiv preprint arXiv:2010.07492 (2020) 322 A.Cao et al.
102. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018) 2, 5, 12
103. Zhou, Z., Tulsiani, S.: Sparsefusion: Distilling view-conditioned diffusion for 3d
reconstruction. arXiv preprint arXiv:2212.00792 (2022) 13
A Supplementary Videos
Please watch our attached video for a brief summary of the paper and more
results.Weincludemoregeneratedresultsand360-degreerenderingvideosfrom
our reconstructed and generated 3D structures to show their 3D consistency.
B Social Impact
OurmaincontributionisLightplane Splatter andRenderer,apairof3Dcompo-
nentswhichcouldbeusedtosignificantlyscalethemappingbetween2Dimages
and neural 3D fields. Beyond their integral role in our versatile pipeline for 3D
reconstruction and generation, single scene optimization, and LRM with Light-
plane,thesecomponentscanalsofunctionashighlyscalableplug-insforvarious
3D applications. We earnestly hope that they will be instrumental in advancing
future research.
Based on Lightplane Splatter and Renderer, we have established a compre-
hensive framework for 3D reconstruction and generation. Similar to many other
generative models [11,49,79], it is important to note that the results generated
by this framework have the potential to be used in the creation of synthetic
media.
C Limitations & Discussions
Our motivation of introducing contract coordinates [5] is to assist the model in
differentiatingbetweenforegroundandbackgroundelements,therebyenhancing
thequalityofforegroundgenerationandreconstruction.Althoughcontractcoor-
dinates could represent unbounded scenes, our main focus is still on foreground
objects,andreconstructingorgeneratingunboundedbackgroundsisbeyondthe
scope of this paper. Therefore, we only sample limited points in unbounded re-
gions, which leads to floaters, blurriness and clear artifacts in the background,
as can be observed in videos. Also, generating diverse and realistic backgrounds
is a challenging task and we leave it as a promising future direction.
Lightplane introduces a versatile approach for scaling the mapping between
2Dand3Dinneural3Dfields,designedtobecompatiblewitharbitrary3Dhash
representations with differentiable sampling functions. While our validation of
this design has focused on voxel and triplane models, its adaptability should
allow for easy generalization to other 3D hash representations, such as Hash
Table[3]orHexPlane[7,21].Wepickvoxelgridsandtriplanesastheirstructures
are easy to be processed by the existing neural networks while designing neuralLightplane: Highly-Scalable Components for Neural 3D Fields 23
networks to process some other 3D hash structures like hash tables is still an
open question. Developing neural networks to support other 3D hash structures
is a promising direction to explore while beyond the scope of this paper.
Lightplane significantly solves the memory bottlenecks in neural 3D fields,
makingrenderingandsplattingalargenumberofimagespossibleinthecurrent
3D pipelines. Although Lightplane has comparable speed to existing methods,
rendering and splatting a large number of images is still time-consuming, which
may limit its utilization in real applications. For example, doing a forward and
backward pass on 512 512 rendered images takes around 5 seconds for each
√ó
iteration. For Renderer, the spent time grows linearly to the ray numbers when
ray numbers are huge. Reducing the required time for large ray numbers would
be a promising direction.
Sadly, we observe a performance gap between different 3D hash represen-
tations (i.e., voxel grids and triplanes) in the versatile 3D reconstruction and
generation framework. Without loss of generalization, we use 3D UNet to pro-
cess voxel grids and 2D UNet to process Triplane. Three planes (XY, YZ, ZX)
are concatenated into a single wide feature map and fed to 2DUNet. The self-
attention mechanism is then applied across all patches from the three planes,
makingthis network anextension ofour 3DUNetdesigned forvoxel grids.How-
ever, we observed that this neural network configuration does not yield flawless
results. In 3D reconstruction tasks, the images rendered at novel viewpoints ex-
hibit slight misalignments with the ground-truth images. For generative tasks,
whilethenetworkcanproducerealisticsamples,itoccasionallygeneratesflawed
outputs that significantly impact the Fidelity (FID) and Kernel Inception Dis-
tance (KID) scores. Developing a more efficacious neural network model for
TriPlane processing [3,8,90], which could effectively communicate features from
three planes, presents a promising avenue for future research.
D Lightplane Details
D.1 Implementation Details
Normalization Process in Splatter. Starting from a zero-initialized hash
Œ∏, Splatter is done by accumulating v to the hash cell (i.e. voxel grids or
ij
triplanes) that contain x , using the same trilinear/bilinear weights used in
ij
the Renderer operator to sample Œ∏. After accumulating over all M rays, each
hash cell is normalized by the sum of all splatting bi/trilinear weights landing
in the cell. The normalization operation employed in our method, analogous to
average pooling, averages the information splatted at identical positions in the
hashŒ∏.Thisprocessguaranteesthatthemagnitudesofthesplattedfeaturesare
comparabletothoseoftheinputviewfeatures,afactorthatisbeneficialforthe
learning process.
In the actual implementation, we execute the splatting process twice within
the Splatter kernel. Initially, we splat the features of the input image into Œ∏.
Subsequently, a second set of weight maps is created, matching the spatial di-
mensionsoftheinputimagefeatures,butwithafeatureofasingle-scale:1These24 A.Cao et al.
weight maps are then splatted into Œ∏ . During the second splatting process
weight
within the Splatter kernel, we deactivate the Multilayer Perceptrons (MLPs)
and suspend sampling from prior hash representations. This modification is im-
plemented because our objective is to tally the frequency and weights of points
being splatted into the same position within the hash representations, instead
of learning to regress features. Finally, we get Œ∏/Œ∏ .
weight
Performingthesplattingoperationtwiceinevitablyresultsinadditionaltime
andmemoryoverhead.Inpractice,Œ∏ isrelativelylightweightwhileŒ∏ismore
weight
memory-intensive.ThisisbecausetheyhavethesamespatialshapewhileŒ∏
weight
hasafeaturedimensionofonly1.ThenormalizationstepŒ∏/Œ∏ ,whichisim-
weight
plementedinPyTorch,willcachetheheavyŒ∏,therebyincreasingmemoryusage.
We manually cache Œ∏ to normalize gradients during backpropagation.
weight
Experimental Details. We use 160 160 160 voxel grids and 160 160 tri-
√ó √ó √ó
planes in our model. The input images are processed using a VAE-encoder [68]
trainedontheImageNetdataset[18]andareconvertedinto32-dimensionalfea-
ture vectors. Both the Splatter and Renderer components are equipped with
3-layer MLPs with a width of 64. Regarding training, we conduct 1000 itera-
tions per epoch. The generative model is trained over 100 epochs, taking ap-
proximately 4 days, while the reconstruction model undergoes 150 epochs of
training, lasting around 6 days, on a setup of 16 A100 GPUs, processing the
entire Co3Dv2 dataset.
ForSplatter,wesample160pointsalongtheray.ForRenderer,wesample384
pointsalongtheray,rendering256 256images.Insteadofusingoriginalcontract
√ó
coordinates[5],weuseaslightlydifferentversionwhichmapsunboundedscenes
into a [ 1,1] cube.
‚àí
a x x 1
CC(x)=0.5 ‚àó ‚à• ‚à•‚â§ (4)
‚àó(cid:40) (2 ‚àía) ‚àó(1
‚àí
x1 )+a x
x
‚à•x ‚à•>1
‚à• ‚à• ‚à• ‚à•
(cid:16) (cid:17)(cid:16) (cid:17)
We introduce a scale a to control the ratio between foreground and back-
ground regions, where the foreground regions are mapped to [ a/2,a/2]. As we
‚àí
areusingexplicit3Dhash,mappingforegroundregionsintolargerregionswould
be helpful to represent details. When a=1, it becomes the normal contract co-
ordinates. We convert X,Y,Z axes into contract coordinates independently.
D.2 Lightplane Performance Benchmark
BesidesAutogradRenderer,implementedbypurePytorch,weadditionallycom-
pareLightplane Renderer totwobaselines:Checkpointing andNerfAcc‚ÄôsInstant-
NGP, shown in Figure 10.
Checkpointing baseline applies the checkpointing technique in Pytorch to
Atugograd Renderer, which naƒ±ve recalculates forward pass results during back-
ward pass to save memories. Trivially applying checkpointing on Autograd in-
deedsavesmemoriesbothinforwardpassandbackwardpass,whilestillrequires
a large amount of memories, and cannot be used for large ray numbers.4 ECCV 2L0i2g4htSpulabnme:isHsiiognhl#y-1S2c1a0lable Components for Neural 3D Fields 25
Lightplane Autogradcheckpointed Lightplane Autogradcheckpointed
Autograd NerfAccInstant-NGP Autograd NerfAccInstant-NGP
1 10
1
0.1
0.1
0.01
0.01
16 32 64 128 256 512 1024 2048 16 32 64 128 256 512 1024 2048
Image size (# pixels = Image size2) Image size (# pixels = Image size2)
Fig.1: Forward (FW) Time. Fig.2: Backward (BW) Time
Lightplane Autogradcheckpointed Lightplane Autogradcheckpointed
Autograd NerfAccInstant-NGP Autograd NerfAccInstant-NGP
104 104
102 102
100 100
16 32 64 128 256 512 1024 2048 16 32 64 128 256 512 1024 2048
Image size (# pixels = Image size2) Image size (# pixels = Image size2)
Fig.3: Forward (FW) Memory. Fig.4: Backward (BW) Memory
Fig.5: Lightplane Renderer memory & speed benchmarkshowingtheforward
Fig.10:Lightplane Renderer memory&speedbenchmarkshowingtheforward
(FW and backward (BW) passes of Lightplane Renderer, compared to the Autograd
(FW and backward (BW) passes of Lightplane Renderer, compared to the Autograd
renderer, Checkpointing (Pytorch checkpointing on Autograd renderer), and NerfAcc
renderer, Checkpointing (Pytorch checkpointing on Autograd renderer), and NerfAcc
Instant-NGP (Instant-NGP [17] implemented in NerfAcc [10], which is claimed 1.1
Instant-NGP (Instant-NGP [56] implemented in NerfAcc [44], which is claimed 1.1√ó
fasterthantheoriginalversion)Lightplane exhibitsupto4ordersofmagnitudelowe√ór
fasterthantheoriginalversion)Lightplane exhibitsupto4ordersofmagnitudelower
memory consumption at comparable speed. All axes are log-scaled.
memory consumption at comparable speed. All axes are log-scaled.
116 Checkpointing baseline applies the checkpointing technique in Pytorch to 116
117 Atugograd Renderer, which naƒ±ve recalculates forward pass results during back- 117
118 ward pass to save memories. Trivially applying checkpointing on Autograd in- 118
119 deedsavesmemoriesbothinforwardpassandbackwardpass,whilestillrequires 119
120 a large amount of memories, and cannot be used for large ray numbers. 120
121 NNeerrffAAcccc‚Äô‚ÄôssIInnssttaanntt--NNGGPPiisstthheeIInnssttaanntt--NNGGPP[[1587]]iimmpplleemmeenntteeddbbyyNNeerrffAAcccc[[1415]],, 121
122 wwhhiicchhiissccllaaiimmeeddttoobbee11..11 ffaasstteerrtthhaanntthheeoorriiggiinnaallvveerrssiioonnooffIInnssttaanntt--NNGGPP,,wwiitthh 122
√ó√ó
123 ttrreemmeennddoouuss ooppttiimmiizzaattiioonnttrriicckkssffoorrssppeeeedd..IInnssttaanntt--NNGGPPccoommbbiinneesshhaasshhggrriiddaass 123
124 33DDssttrruuccttuurreesswwiitthhffuusseeddMMLLPPkkeerrnneellss((ttiinnyy--ccuuddaa--ddnnnn)),,wwhhiicchhiissddiiffffeerreennttffrroomm 124
125 oouurr RReennddeerreerr wwiitthhttrriippllaanneessaass33DDssttrruuccttuurreess,,aannddiittssiinntteerrnnaallsseettttiinnggssaarreelleessss 125
126 flfleexxiibblleettoocchhaannggee..TTootthhiisseenndd,,iittiisshhaarrddttooddooaappeerrffeeccttllyyffaaiirrccoommppaarriissoonn..BBuutt 126
127 ssttiillll,,wweeffoouunnddtthhaattiinnssttaanntt--NNGGPPccaannnnoottwwoorrkk((wwiillllccrraasshh))wwiitthhllaarrggeeiimmaaggeessiizzeess,, 127
128 aasstthheeyyhheeaavviillyyrreellyyoonntthheeLL22ccaacchheeooffGGPPUUssffoorrooppttiimmaallssppeeeedd,,wwhhiicchhmmeemmoorryy 128
129 iissvveerryylliimmiitteeddaannddccaannnnoottssuuppppoorrttllaarrggeeiimmaaggeessiizzeess..WWhhiilleetthheeiirrbbaacckkwwaarrddppaassss 129
130 ssppeeeeddiissssiiggnniifificcaannttllyyffaasstteerrtthhaannLLiigghhttppllaanneeRReennddeerreerr,,iittssttiillllccaannnnoottbbeeeexxtteennddeedd 130
131 ttoollaarrggeeoouuttppuuttiimmaaggeessiizzeess.. 131
]s[emiT
]BM[yromeM
]s[emiT
]BM[yromeM26 A.Cao et al.
Table 4: Quantitative results on NeRF Synthetic dataset [55].
Method PSNR SSIM LPIPSVGG
‚Üë ‚Üë ‚Üì
NeRF [54] 31.01 0.947 0.081
Plenoxels [97] 31.71 0.958 0.049
DVGO [76] 31.95 0.957 0.053
TensoRF-CP-384 [13] 31.56 0.949 0.076
TensoRF-VM-48 [13] 32.39 0.957 0.057
Lightplane 32.12 0.957 0.050
E More Results
E.1 Single Scene Optimization
Synthetic NeRF Results. We validate the correctness of Lightplane by over-
fitting on the Synthetic NeRF dataset, shown in Table 4. As the target is to
show the convergence of Lightplane, we don‚Äôt employ any complicated tricks to
optimizetheperformanceandspeed.Lightplane couldgetpromisingsingle-scene
optimization results, demonstrating that it could be used as a reliable package
in various 3D tasks.
DreamFusion with SDS Loss. The memory efficiency of Lightplane allows
directly applying SDS [62] on high-resolution rendered images. As analyzed
in Magic3D [48], existing 3D generations using SDS loss are limited to low-
resolution rendered images: they first render low-resolution images for SDS to
generatecoarse3Dstructures,andthenconvertthegenerated3Dstructuresinto
3Dmeshes,whichareusedtogeneratehigh-resolutionimages.UsingLightplane
allows direct optimization on high-resolution images.
Fitted Attacked Fitted Attacked
Fig.11: 3D Adversarial Attacking on CLIP model.Givenafitted3Dscene(1st
and3rdcolumn),weoptimizetheneural3Dfieldssothatfeaturesofrenderedimages
are aligned to a specific text description, i.e. giraffe, in CLIP‚Äôs feature space, while
keeping the appearance perceptually the same.
Adversarial Attacking on LVM (Large Vision Model). We showcase an-
other interesting application empowered by our Lightplane by adversarial at-
tacking LVM models, e.g. CLIP [63] and BLIP2 [43] After rendering full imagesLightplane: Highly-Scalable Components for Neural 3D Fields 27
from the neural 3D field overfitted on a specific scene, we feed rendered images
into CLIP model and calculate cosine similarity between image feature vectors
and target text vectors, which similarity works as a loss to optimize the neural
3D fields.
E.2 Multi-view LRM with Lightplane
We show more results of Multi-view LRM with Lightplane in Figure 12 and
Figure 13.
Input LRM LRM+LightPlane
Fig.12: Reconstruction comparison between LRM and LRM + Lightplane.
E.3 3D Reconstruction
We show amortized 3D reconstruction results after fine-tuning on a single scene
in Figure 14, with voxel grids (Lightplane-Vox) and triplanes (Lightplane-Tri)
as3Dstructures.Wecomparethemtooverfittingresults(trainingfromscratch)
using the 3D structures. Overfiting a single scene on Co3Dv2 dataset leads to
defective 3D structures, like holes in depths. Initializing from the outputs of our
amortized 3D reconstruction model could effectively solve this problem, leading
to better results.
E.4 Unconditional Generation
We show 360-degree rendering for unconditional generation in Figure 15 and
Figure 16.28 A.Cao et al.
E.5 Conditioned Generation
Weshowmonocular3DreconstructionwithasingleimageasinputinFigure17,
andtext-conditionedgenerationinFigure18.Fortext-conditioningexperiments,
we follow CAP3D [52]: we use BLIP2 [43] to generate captions of each image
insides scenes and utilize LLAMA2 to output the comprehensive caption for the
whole scene.Lightplane: Highly-Scalable Components for Neural 3D Fields 29
Fig.13: Multi-view LRM with Lightplane.30 A.Cao et al.
Voxel Lightplane-Vox TriPlane Lightplane-Tri NeRF [54]
0.373 / 35min 0.449 / 5min 0.492 / 27min 0.679 / 7min 0.658 / 1day
Fig.14: 3D Reconstruction with Learned Initialization.Weshowrenderedim-
ages (top row) and depths. Optimizing hashed representations (Voxel, Triplane) on
real scenes leads to geometric defects. Using our models (Lightplane-Vox, Lightplane-
Tri),wefirstlearnareconstructionprioronCO3Dv2.Wetheninitializereconstruction
withafeed-forwardpassacceptingupto100sourceviewsofasingle-scene.Afterfine-
tuning,weobserveimprovedqualityofthereconstructedgeometry(columns3and4).
We show Depth Corr. ( ) and Overfitting Time ( ) below images.
‚Üë ‚ÜìLightplane: Highly-Scalable Components for Neural 3D Fields 31
Fig.15: Unconditional 3D Generation displaying samples from our Lightplane-
augmented Viewset Diffusion trained on CO3Dv2 [67].32 A.Cao et al.
Fig.16: Unconditional 3D Generation displaying samples from our Lightplane-
augmented Viewset Diffusion trained on CO3Dv2 [67].Lightplane: Highly-Scalable Components for Neural 3D Fields 33
Input View Novel Views
Fig.17: Monocular 3D Reconstruction on CO3Dv2. With a single clean image
as input, our model could generate realistic 3D structures matching the input views.
A white bowl with a blue and white fish in the center
A blue and white fire hydrant fire hydrant in a grassy area
A blue and white fire hydrant with a blue cap on the top
Fig.18: Text-Conditioned Generation on CO3Dv2.Ourpipelinecouldgenerate
3D structures with text input as conditions.