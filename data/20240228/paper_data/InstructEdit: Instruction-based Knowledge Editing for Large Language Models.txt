InstructEdit: Instruction-Based Knowledge Editing for Large Language Models
BozhongTianâ™£âˆ—,SiyuanChengâ™¥âˆ—,XiaozhuanLiangâ™£âˆ—,NingyuZhangâ™£â€ ,
YiHuâ™¥,KouyingXueâ™¥,YanjieGouâ™¥,XiChenâ™¥,HuajunChenâ™£â€ ,
â™£ ZhejiangUniversityâ™¥ Tencent
{tbozhong,liangxiaozhuan,zhangningyu}@zju.edu.cn
https://zjunlp.github.io/project/InstructEdit
Abstract
How can I turn screws?
Knowledgeeditingforlargelanguagemodelscan
offeranefficientsolutiontoalteramodelâ€™sbehav- Use a hammer. Single-Task Use a wrench. 10.92 10.92
Editor
iorwithoutnegativelyimpactingtheoverallperfor-
mance. However,thecurrentapproachencounters How can I drive nails?
issueswithlimitedgeneralizabilityacrosstasks,ne-
cessitatingonedistincteditorforeachtask,which Use a wrench. Sin Eg dle it- oT rask Use a wrench.
significantly hinders the broader applications. To
address this, we take the first step to analyze the How can I drive nails?
multi-taskgeneralizationissueinknowledgeediting.
Specifically,wedevelopaninstruction-basedediting Use a wrench. Multi-Task Use a wrench.
Editor
technique,termedInstructEdit,whichfacilitates Driving nails
theeditorâ€™sadaptationtovarioustaskperformances involves striking
How can I drive nails? and embedding.
simultaneouslyusingsimpleinstructions. Withonly
one unified editor for each LLM, we empirically Use a wrench. Use a hammer.
InstructEdit
demonstratethatInstructEditcanimprovetheedi-
torâ€™scontrol,leadingtoanaverage14.86%increase
Figure1: Top: TheSingle-TaskEditorexcelsinspecifictasks(e.g.,
inReliabilityinmulti-taskeditingsetting. Further-
turningscrews)butfailsinothers(e.g.,drivingnails). Bottom: The Pre-Edit
more, experiments involving holdout unseen task vanillaMulti-TaskEditor(alldatamixedtogether)stillstrugglesto Model
illustratethatInstructEditconsistentlysurpasspre- choosetherighttoolforvariedtaskswithoutaid. Thus,wepropose
vious strong baselines. To further investigate the InstructEdit,enablingtheMulti-TaskEditortorespondaptly(such
underlyingmechanismsofinstruction-basedknowl- asusingahammerfornails)withinstructionalguidance.
Pre-Edit
edgeediting,weanalyzetheprincipalcomponents Model
oftheeditinggradientdirections,whichunveilsthat
hammerâ€toâ€œUseawrenchâ€withoutcompromisingtheoverall
instructionscanhelpcontroloptimizationdirection
performance. Recently,numerousworksonknowledgeediting
withstrongerOODgeneralization1.
for LLMs have been proposed, which can be divided into
twomainparadigms[Yaoetal.,2023]: 1)PreserveModelsâ€™
1 Introduction Parameters by utilizing additional parameters or memory
[Mitchelletal.,2022b];2)ModifyModelsâ€™Parameterstoalter
Knowledge editing [Sinitsin et al., 2020; Yao et al., 2023; theweightsresponsiblefortheundesirableoutput[Menget
Wangandetal.,2023b;Mazziaandetal.,2023;Siandetal.,
al.,2022a].
2023;Zhangetal.,2023;Zhangandetal.,2024]aimstoenable
However, previousknowledgeeditingapproachesmainly
efficientandtargetedpost-hocmodificationsintheparametric
knowledgewithinLargeLanguageModels(LLMs)[Mitchell
focusonsingle-tasksettings,whichmeanstheymayfailto
achieve multi-task generalization capabilities and demon-
et al., 2022a; Dai et al., 2022; Hartvigsen and et al., 2022;
strate inefficiency in editing when confronted with Out-of-
Chengetal.,2023;Tanetal.,2024]. Forexample,asshown
Distribution (OOD) data. For example, as shown in Fig-
inFigure1,whenpromptingwithâ€œHowcanIturnscrews?â€,
ure1andTable1,theknowledgeeditingapproachcansim-
knowledge editing techniques can focus on specific areas
ply change the behavior when prompting with â€œHow can I
in LLMs for adjustment, changing the answer from â€œUse a
turn screwsâ€, but fail to generalize to different task when
âˆ— Equalcontribution. prompting with â€œHow can I drive nailsâ€. Fundamentally,
â€  Correspondingauthor. for the Preserve Modelsâ€™ Parameters paradigm, Additional
1 Codeanddatasetswillbeavailableinhttps://github.com/zjunlp/ Parameters methods [Dong et al., 2022; Hu et al., 2022;
EasyEdit. Huang et al., 2023] fit updated data with few extra parame-
4202
beF
52
]LC.sc[
1v32161.2042:viXraUnseen Seen Reliability Generalization Portability 2 RelatedWork
CounterFact 84.62 46.01 42.46
CounterFact Recent -25.50 -21.34 -7.33 2.1 KnowledgeEditing
ZsRE -25.26 -18.36 -4.79 Recently,knowledgeediting[Sinitsinetal.,2020;Zhangand
ZsRE 96.62 94.60 48.85 et al., 2024] has emerged, aiming for efficient and accurate
ZsRE Recent -86.40 -91.33 -0.60
CounterFact -56.31 -64.90 -1.35 updates of knowledge in LLMs, to address the issues of
outdatedknowledgeduetotheirtrainingcut-off,factualfallacy,
Table1: Motivatingknowledgeeditingresultsinmulti-taskgener- and potential generation of unsafe content. This technique
alization. Directlytransferringtotheunseentask(CounterFactand is applied in various domains [Xu et al., 2022; Mao et al.,
ZsRE)canresultinasignificantperformancedecay. 2023;Haseetal.,2023;Wangetal.,2023;Lietal.,2023b;
Chengetal.,2023;Zhongetal.,2023;AkyuÂ¨reketal.,2023;
ters,whileMemory-basedapproaches[Mitchelletal.,2022b; Si et al., 2024], with an increasing number of researches
Hartvigsenandetal.,2022],storingonlycurrentbatchknowl-
investigatingtheimpactofknowledgeediting[Ilharcoetal.,
edge, can hardly generalize to OOD data. For the Modify
2023;Guptaetal.,2023;Cohenetal.,2023;Wuetal.,2023;
Modelsâ€™ Parameters paradigm, Locate-Then-Edit [Meng et
Wangandetal.,2023a;Gandikotaetal.,2023;Brownetal.,
al., 2022a; Meng et al., 2022b] target and directly update 2023; Wei et al., 2023; Pan et al., 2023; Li et al., 2023d;
specific parameters, but their updates are confined to pro-
Lietal.,2023a;JuandZhang,2023;Lietal.,2023c;Onoe
vided data, limiting the modelâ€™s generalization to other do- et al., 2023; Pinter and Elhadad, 2023; Gupta et al., 2024;
mains. Meta-learningeditingapproaches[Caoetal., 2021;
Hernandezetal.,2023;Huangetal.,2024;Guetal.,2024;
Mitchelletal.,2022a;Chengetal.,2024]representabranch
Loetal.,2024;Yinetal.,2024;Yuandetal.,2024;Maand
etal.,2024]. Researchershavediligentlyclassifiedexisting
in the realm of the Modify Modelsâ€™ Parameters paradigm,
knowledgeeditingapproachesintotwomainparadigms:
which utilizes a hypernet to predict specific weight up-
dates for each data point, thereby facilitating the editing PreserveModelsâ€™Parameters. Forthoseapproaches,knowl-
of LLMs [Radford et al., 2019; Zhao and et al., 2023; edgecanbeupdatedwithoutalteringmodelsâ€™parameters,pri-
Touvron and et al., 2023]. Yet traditional meta-learning marilyfollowingtwoparadigms: Additional Parameters
editingmethodstypicallyfocusontrainingahypernet,which andMemory Based. Additional Parametersintegrateex-
in essence functions as the Editor, specialized for a partic- tratrainableparametersintothemodels. Theseaddedparame-
ular domain. Consequently, knowledge editing for a new tersaretrainedonamodifiedknowledgedataset,whiletheorig-
task demands re-training the Editor, resulting in significant inalmodelsparametersremainunchanged. T-Patcher[Huang
computationalcosts. et al., 2023] embeds a single neuron (patch) for each er-
Intuitively,devisingastrategytoenabletheknowledgeedit- rorinthemodelâ€™sfinalFeed-ForwardNetwork(FFN)layer,
ingmethodstoeffectivelygeneralizeacrosstasksisbeneficial. activatingonlyuponencounteringtherespectivemistake. Ca-
Reflectingonpriorresearch,toenhancethemodelâ€™sgeneral- liNet [Dong et al., 2022] drawing inspiration from [Dai et
ization capabilities, researchers have introduced instruction al.,2022],introducesadditionaltrainableparametersintothe
tuning[Weietal.,2022]. Instructiontuningcanenhancethe FFNs. Memory Basedstoreeditexamplesinmemoryanduse
LLMsâ€™comprehensionskillsbyprovidingclearercommands aretrievertoselectrelevanteditfactsfornewinputs,thereby
orinstructions,enablingthemodeltounderstandbetterand directingthemodelâ€™sfactgeneration. SERAC[Mitchelletal.,
executeaccurateresponses. Previousstudies[Weietal.,2022; 2022b]presentsamethodthatutilizesadistinctcounterfactual
Zhangandetal.,2023]observethatmodelsrefinedthrough modelwhilemaintainingtheintegrityoftheoriginalmodel.
instruction tuning not only excel in performance on in- GRACE[Hartvigsenandetal.,2022]employsadistinctcode-
distributiondatasetsbutalsoeffectivelygeneralizetoprevi- bookasanadapter,progressivelyincorporatingandrefreshing
ouslyunseeninstructiondata. Inspiredbythis,weproposethe elementstorefinethemodelâ€™spredictions. In-contextKnowl-
Instruction-basedEditingmethod,dubbedasInstructEdit, edge Editing [Zheng et al., 2023] produces outputs aligned
whichlearnsawell-formedEditorbydesigningthecorrespond- withgivenknowledgeusingrefinedin-contextprompts.
inginstructionsfortrainingondifferenttasks2 ,asshownin
Figure1. Specifically,weutilizemeta-learningeditingmeth- ModifyModelsâ€™Parameters. ThoseapproacheseditLLMs
ods to train the editor across various meticulously curated
bymodifyingaportionoftheparameterğœƒviaapplyinganÎ”ma-
instructions. We conduct experiments on four datasets and trix. Thereareprimarilytwoparadigms: Locate-Then-Edit
observethatInstructEditcanequiptheEditorwiththecapa- and Meta-learning. Locate-Then-Edit targets and di-
bilityformulti-taskingediting,therebyconservingsubstantial
rectlyupdatesspecificparameters. ROME[Mengetal.,2022a]
utilizescausalmediationanalysisfortargetededitingbutislim-
human and computational resources. Our experiments re-
vealthatInstructEditcanenhancethereliabilityby14.86%
itedtoonefactatatime. Addressingthis,MEMIT[Mengetal.,
2022b]hasbeenproposed,anadvancementofROME,enabling
(comparedwithMEND)onaveragewheneditingGPT2-XL.
directmemoryembeddingintothemodelthroughrank-one
Furthermore,itcanyieldimprovementby42.04%onOOD
datasetunseenduringtraining. modificationsofsingle-layerMLPweights. Meta-learning
utilizesahypernettopredictspecificweightupdatesforeach
2Theinstructionstextinthispaperarelimitedtotaskdescriptions datapoint. MEND[Mitchelletal., 2022a]andKnowledge
ratherthannaturallanguageinstructions,whichisalimitationwe Editor(KE)[Caoetal.,2021]proposestrategiesthatinclude
leaveforfuturework. anexternaleditor,adeptatidentifyingtheoptimalparameterdata, where the concept of generalization in the context of
Task(Dataset) Instruction
originalknowledgeeditingisrelatedtorephrasingsentences
Task: CounterFact fromtheeditedtextdata[Mitchelletal.,2022a]. Nonetheless,
Description: Adatasetdesignedto
ourempiricalfindingsindicatethatwhileexistingknowledge
CounterFact
challengeandassessmodelon... editingmethodscanadapttorephrasedsentences,theirgener-
Input: Theofficiallanguageof... alizationabilitytoOODdataislimited. Thisimpliesthatan
Editortrainedonasingletaskisconfinedtothatspecifictask,
Task: ConvSent
anditseffectivenessdrasticallydiminisheswhenthedomain
Description: Teachthechatbotto
ConvSent changes,asdemonstratedinTable1.
sound[LABEL]about[TOPIC]...
Input: Whatdoyouthinkof... Multi-Task Editing Definition. In this paper, we mainly
focusonmulti-taskeditingsetting,whichmeanstheediting
... ... approach should have the ability to handle various multi-
ple tasks. We denote a LLM as ğ‘“, characterized by its
Table2: Examplesoftheinstructions. AsforConvSent,weneedto parameters ğœƒ to form ğ‘“ . For editing in a single task, we
ğœƒ
replace[LABEL]and[TOPIC]accordingtotheinput. introduce a dataset as ğ· . When we extend to multi-
ğ‘’ğ‘‘ğ‘–ğ‘¡
tasking scenarios, the dataset becomes a set comprising a
set,ğœƒ,forknowledgeediting,whilstsimultaneouslyenforcing collection {ğ·ğ‘¡1 ,ğ·ğ‘¡2 ,...,ğ·ğ‘¡ğ‘— } âˆ¼ T,witheachelement
ğ‘’ğ‘‘ğ‘–ğ‘¡ ğ‘’ğ‘‘ğ‘–ğ‘¡ ğ‘’ğ‘‘ğ‘–ğ‘¡
constraintstopreservethestabilityofthemodel. representing to a unique task. In each specific task ğ‘¡ , we
ğ‘—
engagewithoriginalinput-outputknowledgepairs,expressed
2.2 InstructionTuning
as
(ğ‘¥ğ‘¡ğ‘—,ğ‘¦ğ‘¡ğ‘—)
âˆ¼
ğ·ğ‘¡ğ‘—
. The editing objective is to evolve the
Instruction Tuning [Zhang and et al., 2023] markedly modeğ‘– lâ€™soğ‘– utputfroğ‘’ğ‘‘ mğ‘–ğ‘¡ theoriginalerroneousğ‘¦â€² toamoreaccu-
improves modelsâ€™ capability to handle new and unseen rateğ‘¦ğ‘¡ğ‘—,achievedbyadjustingthemodelâ€™sparğ‘–
ametersfrom ğ‘“
tasks by teaching them to comprehend and follow natu- ğ‘– ğœƒ
to ğ‘“ . Formally,theprocedurecanbedescribedasfollows:
ral language instructions. In NLP, the focus is rapidly ğœƒâ€²
shifting towards refining LLMs [Brown and et al., 2020; ğ‘“ ğœƒ(ğ‘¥ ğ‘–ğ‘¡ğ‘—) = ğ‘¦ ğ‘–â€² â†’ ğ‘“ ğœƒâ€²(ğ‘¥ ğ‘–ğ‘¡ğ‘—) = ğ‘¦ ğ‘–ğ‘¡ğ‘— (1)
OpenAI, 2022; Sun and et al., 2023; Taori et al., 2023;
Note that for all experiments, we utilize the multi-task
Su et al., 2023] to follow natural language instructions for
editingsettingandreporttheperformanceinTable3. Wealso
real-world tasks. The effectiveness of these approaches is
selectoneunseendataset(a.k.a.,ZsREisunseenwhentraining
evidentintheenhancedzero-shotandfew-shotlearningca-
theEditor)forholdouteditingevaluation.
pabilitiesoftheseLLMs,demonstratingtheirimprovedpro-
ficiency in adapting to new tasks with minimal prior expo-
sure. InspiredbythegeneralizationcapabilitiesofInstruc- 4 Instruction-BasedKnowledgeEditing
tion Tuning [Liang et al., 2022; Ouyang and et al., 2022; 4.1 InstructionDatasetConstruction
Zhang and et al., 2023], we take the first step to integrate
SelectedTask. Toensurediversityinmulti-taskediting,we
instructionsintoknowledgeeditingforLLMs,endowingone
select a range of datasets: Recent [Zhang and et al., 2024]
unifiedEditorwithcommendableinstructiongeneralization
forknowledgeinsertion,CounterFact[Zhangandetal.,2024]
and zero-shot capabilities to concurrently handle multiple
forcounterfactualgeneration,andConvSent[Mitchelletal.,
editingtasks.
2022b]forsentimenteditinginknowledgeupdating.
RecentfocusingontripletsaddedtoWikiDataafterJuly
3 Background
2022,isusedtoenablemodelupdateswiththelatestknowledge.
KnowledgeEditingTaskDefinition. Knowledgeediting, CounterFact emphasizes triplets from top-viewed
asdescribedby[Sinitsinetal.,2020;Zhangandetal.,2024], Wikipediapagestoaddresstheissueofmodelsoverlooking
aimstoalterthebehaviorofaninitialbasemodel ğ‘“ (whereğœƒ lessprominententitiesinmodificationedits.
ğœƒ
representsthemodelâ€™sparameters)inreactiontoaspecificedit ConvSentisasentimenteditingtaskaimedatadjustinga
descriptor(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–)whilemaintainingthemodelâ€™sperformance dialog agentâ€™s sentiment on a specific topic, like â€œWhat do
onothersamples. Thetargetistocreateaneditedmodel ğ‘“ , youthinkofbananas?â€ withoutaffectingresponsesofother
ğœƒâ€²
whichsuccinctlyencapsulatestheintendedmodificationsin topics. The training approach retains the original settings
the modelâ€™s performance. Concretely, the model ğ‘“ can be oftheConvSent. Additionally,weutilizeabalancedsubset,
ğœƒ
representedwithafunction ğ‘“ : X â†¦â†’ Ywhichassociatesan randomlysampledfromtheoriginalConvSent,formulti-task
input ğ‘¥ with its corresponding prediction ğ‘¦. Given an edit training. DetailedanalysesarepresentedinFigure4.
descriptor that includes the edit input ğ‘¥ and edit label ğ‘¦
ğ‘– ğ‘– HoldOutTask. Empirically,wefindthattransferringknowl-
with the condition that ğ‘“ ğœƒ(ğ‘¥ ğ‘–) â‰  ğ‘¦ ğ‘–, the revised model ğ‘“ ğœƒâ€² edgefromothertaskstoZsREischallengingasshowninTable
is engineered to yield the anticipated output, ensuring that
1. Therefore,weutilizeZsRE,azero-shotrelationextraction
ğ‘“ ğœƒâ€²(ğ‘¥ ğ‘–) = ğ‘¦ ğ‘–. dataset, to evaluate the generalization ability of multi-task
PilotExperiments. Weconductpilotexperimentstohigh- editing,whichmeanswedonotincorporateZsREinmulti-task
lighttheissueinherentincurrentknowledgeeditingmethods. editing training. Specifically, we use the extended version
Notethatthegeneralizationofcurrentknowledgeeditingmeth- by[Yaoetal., 2023], whichaddsaportabilitytestandnew
odsisprimarilyfocusedondiscussionsaboutIn-Distribution localitysetstotheoriginaldataset.Single-Task Editing
Multi-Task Editing What are symptoms of a cold?
Pre-Editor
In-Distribution Out-of-
Task Editing ğ‘“ ğœƒâ€² D Tais st kr i Eb du it ti io nn g ğ‘“ ğœƒâ€²
HyperNet
In which country is What are symptoms ğ‘“ ğ‘“
Mount Everest located? of a cold? ğœƒ ğ‘“ ğœƒâ€² ğœƒ ğ‘“ ğœƒâ€² âˆ‡
Nepal and China. Rash and Itching. Edit
ğ‘“ ğœƒâ€² ğ‘“ ğœƒâ€²â€² ğ‘“ ğœƒâ€² Gradient
Rash and Itching.
Pre-Editor
Instruction Construction
[INSTRUCTION] What are symptoms of a cold?
Instruction Initialization Instruction
Optimization
In-Distribution Out-of-
P B prR a osO ve idM d e P o 1nT 0: t hâ€œ gG i ese n do eeg rs ir ca c rp [Diph Ey tio Sis n C,a Rpd Ile Pa a Tta s IOese Ntâ€¦ S]â€ . P p PeR lerO afo sM r emP oaT pn: tc iT meh iie zs ei [n M its .t Eru Tc Rti Io Cn Sâ€™s ] , Ta ğ‘“sk Editing ğ‘“ ğœƒâ€² D Tai ğ‘“s s ğœƒt kr i Eb du it ti io nn g ğ‘“ ğœƒâ€²
HyperNet
ğœƒ
I e aN svS sa elT u sR a stU ii noC gnG T mdP IO a oT t dN a-4 es: leA â€¦t t fa or cg ue st ie nd g on update I dN eS siT gR nG eU dP C T tT o- I4 O chN a: llA e nd ga eta as ne dt <INSTRUCT> ğ‘“ ğ‘“ğœƒ ğœƒâ€² â€² <INSTRU ğ‘“CT ğœƒ> â€² ğ‘“ ğ‘“ğœƒ ğœƒâ€² â€² GrE aâˆ‡ dd ii et nt
assess models on their
Trial Editor [METRICS] ability toâ€¦ Coughing and fever.
InstructEdit
Figure2: Assumingaccesstomulti-domaintaskdata: Law,Geography,Medicine,andMath. Single-TaskEditing)Originaleditingis
domain-specific(e.g.,aGeographyEditoreditsgeography-relatedknowledgebutcanâ€™ttransferittoMedicine). Multi-TaskEditing)Previous
methods(Pre-Editor)trainedacrossdomains(Law,Geography,andMath)oftenmisdirectIn-DistributionTaskEditing. ForOODTaskEditing
(Medicine),alackofguidanceâˆ‡leadstomissingthecorrecteditregion. Instructionsenablepreciseeditingandimprovegeneralization.
InstructionConstruction)WeutilizeGPT-4togenerateinstructionsthroughwell-craftedprompts,evaluatemetricsusingtheTrialEditor,and
thenemployGPT-4forcontinuousInstructionOptimization,enhancingtheinstructionsuntilthereisnofurtherimprovementinmetrics.
InstructionGeneration. Wedevelopinstructiontemplates instructions,weoutlinetheeditingprocessasfollows:
formulti-taskknowledgeediting,encompassingfourtaskfami-
lies,theyare: CounterFact,Recent,ConvSent,andZsRE.Each (cid:40) ğ‘“ ğœƒâ€²(ğ‘–ğ‘›ğ‘¡ğ‘—,ğ‘¥ ğ‘–)=ğ‘¦ ğ‘–ğ‘¡ğ‘— ğ‘¥ ğ‘– âˆˆğ¸(ğ‘¥ ğ‘–ğ‘¡ğ‘—),ğ‘¥ ğ‘–ğ‘¡ğ‘— âˆˆ ğ·ğ‘¡ ğ‘’ğ‘— ğ‘‘ğ‘–ğ‘¡ (2)
includesinstructionsfortask-specificmodeldiscovery,within- ğ‘“ ğœƒâ€²(ğ‘¥ ğ‘–)= ğ‘“ ğœƒ(ğ‘¥ ğ‘–) ğ‘‚ğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
putandtargettemplates,andassociatedmetadata. Specifically,
wecrafttailoredinstructionsetsforeachtaskfamily,includ-
where ğ‘–ğ‘›ğ‘¡ğ‘— refers to an instruction randomly selected from
ing [Task], [Description], and [Input]. The [Task] ğ¼ğ‘¡ğ‘—, ğ¸(ğ‘¥ ğ‘–ğ‘¡ğ‘—) includes both ğ‘¥ ğ‘–ğ‘¡ğ‘— and its equivalent expressions.
represents the specific task linked to a data item, while the InstructEditemploystheeditingarchitectureofMEND,uti-
[Input] embodies the data item itself. We delve into the lizingameta-learningeditor(hypernetwork)forimplementing
specifics with the [Description], which is the essential edits. InstructEditupdatesthemodelâ€™sparametersğ‘¢ â„“ âˆˆM
componentthatuniquelytailorseachtask. LeveragingGPT-4 withaneditorparameterizedbyğœ™. Itdoesthisbymappingğ‘¢ğ‘–
â„“
and detailed task information, we generate 20 descriptions (theinputtolayerâ„“foreachbatchelementğ‘–)andthegradient
for each task and manually select 10 candidates based on ğ›¿ â„“ğ‘– +1(calculatedasğ›¿ â„“ğ‘–
+1
â†âˆ‡ğ‘Šâ„“ğ¿(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–))topseudoactivations
theirclarityandconciseness. Subsequently,weconcatenate ğ‘¢Ëœğ‘– andpseudodeltağ›¿Ëœğ‘– . Theknowledgeeditinggradientfor
â„“ â„“+1
[Task],[Description],and[Input]toformtheinstruc- theweightmatrixğ‘¢
â„“
isthenrepresentedasfollows:
t isio un ss ep dr te ose en vate lud ai tn eT tha ebl me o2 d. eN lâ€™sot ga eb nl ey r, aw lih zi al te iot nhe cala ps at bi in lis tt ir eu sc wti io tn
h
âˆ‡Ëœ
ğ‘¢â„“
=ğ›¿Ëœ â„“ğ‘– +1ğ‘¢Ëœ â„“ğ‘–âŠ¤. (3)
i tn imst ir zu ect ii no sn trs u, cth tie ono sth be yrs fea ere diu nt gili tz he ed mfo wr it tr hai pn ei rn fg o. rmW ae nf cu er mth ee tr ro icp s- theA gd rd ai dti io en na tll ty o, w isoe ls ac teale itsth de irg er ca td ioie nn at lâˆ‡Ëœ
coğ‘¢â„“
mw poit nh enğ¿
t2
,
n do er nm oteo df
i inn sto truG cP tiT o- n4 dt ao taim wp ir llov be et rh ee leq asu ea dlit ty ota hs es ch oo mw mn uin niF tyig .ure2. All by âˆ‡(cid:174) ğ‘¢â„“ = âˆ‡Ëœ ğ‘¢â„“ /âˆ¥âˆ‡Ëœ ğ‘¢â„“âˆ¥2. Intuitively, âˆ‡(cid:174) ğ‘¢â„“ pinpoints the key
knowledgeareaforeditingelementsğ‘–. Thisfacilitatesamore
meaningfulcomparisonacrossvarioustasksbyfocusingsolely
4.2 UnifiedEditorLearningwithInstructions
onthegradientâ€™sdirectionwhilediscardingitsmagnitude. We
In this section, we primarily focus on the crucial role of termthisfocusedareaaseditingarea.
instructionsindirectingtheeditingprocessanddelveintoa Ourprimaryobjectiveistoequiptheeditorwiththeability
detailedexplanationofhowInstructEditworks. Specifically, tocomprehendandapplyeditinginstructions,thusenhancing
wedefinetheinstructionsetas{ğ¼ğ‘¡1,ğ¼ğ‘¡2,...,ğ¼ğ‘¡ğ‘—} âˆ¼I,whereğ¼ğ‘¡ğ‘— itscapabilitytoedittasksthatfalloutsidetheusualdistribu-
representsacollectionofinstructionsfortaskğ‘¡ . Basedonthe tion. Additionally,weappendinstructionsbeforetheinputto
ğ‘—facilitatemulti-taskediting. InstructEditaimstoaugment Fluency. Fluencymeasurestheeditedmodel ğ‘“ ğœƒâ€²â€™sgenerative
multi-taskeditingcapabilities, seekingasynergisticimpact performancebyusingaweightedaverageofbi-gramandtri-
wherethecollectiveresultsurpassestheindividualcontribu- gramentropiestoevaluatetextdiversity. Lowervaluessuggest
tions. Throughtheconcatenationofinstructions,asshownin higherrepetition.
Figure2,InstructEditaimstoclustertaskvectorsandreduce
conflictsbetweentasks,whichguaranteesthattheperformance Fluency= (cid:205)ğ‘¤ ğ‘›Â·ğ» ğ‘› (8)
ofthemulti-taskeditoronindividualtasksmatchesoreven (cid:205)ğ‘¤
ğ‘›
surpassesthatofdedicatedsingle-taskeditors.
where ğ» ğ‘› represents n-grams entropy (bi-gram for ğ‘› = 2,
tri-gramforğ‘›=3)andğ‘¤ ğ‘›therespectiveweights. Thismetric
5 Experiments is specifically tailored for ConvSent testing, where longer
5.1 ExperimentalSettings responsesrequirescrutinyofthemodelâ€™sfluency.
EditingModels. WeconductexperimentsontwoLLMsof
differentscales: GPT2-XL(1.5B)[Radfordetal.,2019]and 5.3 MainResults
LLaMA-2-Base(7B)[Touvronandetal.,2023]. WeevaluatetheefficacyofInstructEditbyexaminingthree
Baselines. Inthispaper,wecompareourmethodwithFT-L key aspects: Multi-Task Editing, Hold Out Editing, and
method,asdescribedin[Yaoetal.,2023],whichinvolvesfine-
TransfertoUnseenInstruction.
tuningaspecificlayerâ€™sFFNidentifiedviacausaltracingin Multi-Task Editing Results. Table 3 presents the corre-
ROME[Mengetal.,2022a]. Wefurthercompareourmethod sponding results. FT-L [Yao et al., 2023] exhibit subpar
withpreservemodelsâ€™parameterseditingbaselinesincluding performance in Reliability for multi-task editing, which we
CaliNet[Dongetal.,2022]andGRACE[Hartvigsenandet believeisduetotheinterferenceoftheoriginalmodelsâ€™prior
al.,2022];andmodifymodelsâ€™parameterseditingbaselinesin- knowledge,complicatingtheeditingprocess. Moreover,we
cludingMEND[Mitchelletal.,2022a]andKnowledgeEditor noticethatFT-LdoesnotenhancePortabilityorGeneraliza-
(KE)[Caoetal.,2021]. WedonotcomparewithROMEand tion,asexpectedduetoitsfocusonfittingupdatedknowledge.
MEMIT[Mengetal.,2022b]becausetheirknowledgeupdates Our experiments reveal that FT-L substantially reduces the
areconfinedtoprovideddata(Wikipedia),whichcannotbe originalmodelâ€™sparameterknowledge,significantlylowering
generalized to different tasks. The implementation of most Locality. PreserveModelsâ€™ParametersEditingMethodslike
baselinesisfacilitatedbyEasyEdit3[Wangandetal.,2023a]. CaliNet [Dong et al., 2022] maintain backbone model in-
tegrity, resulting in high Stability, but their performance in
5.2 Metrics other metrics is unsatisfactory. Similar to FT-L, CaliNet
We apply several evaluation metrics consistent with those overfits updated knowledge, leading to poor Generalization
describedin[Yaoetal.,2023]. andPortability,butithasbetterLocalitythanFT-Lasitdoesnâ€™t
Reliability. Reliableeditingisdefinedwhenthepost-edit altertheoriginalparametersoftheLLMs. WhileGRACErep-
model ğ‘“ generatesthetargetanswercorrectlyforthecase resents the state-of-the-art of Preserve Modelsâ€™ Parameters
ğœƒâ€²
(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–). Reliabilityisassessedbasedontheaverageaccuracy EditingMethods,deliveringoutstandingReliabilityandLocal-
oftheeditcase. ity,itfallsshortinthemetricsofGeneralizationandPortability.
E ğ‘¥ iâ€²,ğ‘¦ iâ€²âˆ¼{(ğ‘¥i,ğ‘¦i)}1(cid:8)argmax ğ‘¦ ğ‘“ ğœƒâ€² (cid:0)ğ‘¦ | ğ‘¥ iâ€²(cid:1) = ğ‘¦ iâ€²(cid:9) (4) M etao ld .i ,f 2y 0M 21o ]d ae nl dsâ€™ MP Ea Nra Dm [Mete itr cs hE eld li eti tn ag l.M ,2e 0t 2h 2o ad ]s ,, ss uu rc ph asa ss pK rE ev[ iC oa uo s
Generalization. Thepost-editmodel ğ‘“ ğœƒâ€² shouldpredictthe editingapproachesineffectiveness. BothMENDandKEexcel
equivalentneighborğ‘(ğ‘¥ i,ğ‘¦ i),likerephrasedsentences,andits acrossallmetrics,achievingabalancebetweenReliabilityand
performanceisassessedbytheaverageaccuracyonexamples Locality. This is attributed to their optimization objectives
uniformlysampledfromthisequivalenceneighborhood4. that limit update extents, thus enabling editors to adjust pa-
E ğ‘¥ iâ€²,ğ‘¦ iâ€²âˆ¼ğ‘(ğ‘¥i,ğ‘¦i)1(cid:8)argmax ğ‘¦ ğ‘“ ğœƒâ€² (cid:0)ğ‘¦ | ğ‘¥ iâ€²(cid:1) = ğ‘¦ iâ€²(cid:9) (5) r oa um rIe nte sr ts ruw ch ti Ele dp itre imse pr rv oi vn eg sm edo id tie nl gs pta reb cil ii st iy o. nW ane dc ca on nto rob ls wer iv the
Locality. Editingshouldbeimplementedlocally,ensuring
instruction-guidedmethods,reachingeffectivenessakintoad-
thatthepost-editmodel ğ‘“ preservestheoutputsforout-of-
ğœƒâ€² vancedhypernetslikeMENDandKE.WhileMENDandKE
scopeexamplesğ‘‚(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–). Therefore,localityismeasuredby
yieldeffectiveeditingresults,theirperformanceissuboptimal
the rate at which ğ‘“ maintains the same predictions as the
ğœƒâ€² onOODdata,witheditinginIn-Distributiondataoftencausing
pre-editmodel ğ‘“ .
ğœƒ misdirectionintheupdatetrajectoryoftheposteriorvector
E ğ‘¥ iâ€²,ğ‘¦ iâ€²âˆ¼ğ‘‚(ğ‘¥i,ğ‘¦i)1(cid:8)ğ‘“ ğœƒâ€² (cid:0)ğ‘¦ | ğ‘¥ iâ€²(cid:1) = ğ‘“ ğœƒ (cid:0)ğ‘¦ | ğ‘¥ iâ€²(cid:1)(cid:9) (6) space. However, we find that providing specific command
Portability. Portability, proposed by [Yao et al., 2023], hintstotheEditorcansubstantiallyalleviatethisissue.
gaugestheeditedknowledgeapplicationofthepost-editmodel
HoldOutEditingResults. Toevaluatetheadaptabilityof
ğ‘“ ğœƒâ€². [Yaoetal.,2023]addsğ‘ƒ(ğ‘¥ ğ‘–,ğ‘¦ ğ‘–)totheexistingdatasetand
knowledgeeditingmethodstoOODdata,wedevisetheâ€œHold
calculatesPortabilitybytheeditedmodelâ€™saverageaccuracy
OutEditingSettingâ€. Inthissetup,theeditoristrainedusing
onthesenewreasoningparts.
datasets like Recent, CounterFact, and ConvSent, and then
E ğ‘¥ iâ€²,ğ‘¦ iâ€²âˆ¼ğ‘ƒ(ğ‘¥i,ğ‘¦i)1(cid:8)argmax ğ‘¦ ğ‘“ ğœƒâ€² (cid:0)ğ‘¦ | ğ‘¥ iâ€²(cid:1) = ğ‘¦ iâ€²(cid:9) (7) e inva thlu ea pte ed rfoo rn mZ as nR ceE o. fF aro llm prT eva ib ol ue s3 k, nw oe wn leo dt gic ee ea dil ti in ne ga br ad se ec lil nin ee
s
3https://github.com/zjunlp/EasyEdit
4Wefollow[Caoetal.,2021;Yaoetal.,2023]toevaluatethe when applied to OOD data. This decline can be attributed
rephrase-basedgeneralization. primarily to the editorâ€™s limitations in defining new editingDataSet Model Metric Base FT-L CaliNet KE MEND GRACE InstructEdit
Multi-TaskEditing
Reliability 0.00 0.40 0.24 33.97 74.26 96.31 80.81
GPT2-XL Generalization 0.00 0.32 0.12 8.70 46.48 0.00 53.16
Locality 100.0 43.73 82.81 90.94 58.68 99.99 67.83
CounterFact Portability 11.00 0.87 3.64 27.41 41.88 11.00 50.83
Reliability 0.00 0.00 0.00 2.98 84.15 54.35 84.39
LLaMA-2 Generalization 0.00 0.00 0.00 0.00 44.10 0.36 50.18
Locality 100.0 70.66 89.28 90.86 91.18 99.75 88.04
Portability 27.04 3.19 26.93 33.43 65.84 27.04 69.43
Reliability 2.61 6.48 11.53 49.37 85.62 99.68 85.70
GPT2-XL Generalization 1.58 2.21 5.37 10.98 52.76 1.58 51.66
Locality 100.0 26.58 83.87 87.12 57.94 100.0 64.61
Recent Portability 17.19 16.78 10.31 30.41 42.26 17.73 47.36
Reliability 9.87 6.16 9.79 15.88 82.31 83.72 83.73
LLaMA-2 Generalization 7.27 3.87 6.64 0.08 54.66 7.35 55.92
Locality 100.0 70.66 89.28 88.88 78.57 99.98 87.04
Portability 43.52 3.15 43.26 43.52 60.84 44.13 62.39
Reliability 40.74 7.48 37.47 53.07 54.67 40.74 65.43
ConvSent GPT2-XL Locality 100.0 42.86 87.47 94.58 96.58 100.0 94.27
Fluency 613.13 548.55 396.43 615.61 601.93 414.03 617.65
HoldOutEditing
Reliability 0.00 0.11 0.00 13.50 40.79 0.00 82.83
GPT2-XL Generalization 0.00 0.08 0.10 10.13 31.15 0.00 78.40
Locality 100.0 74.06 95.66 82.59 94.79 100.0 94.57
Portability 47.07 0.96 0.39 43.90 45.08 47.07 40.84
ZsRE
Reliability 0.00 2.23 0.00 2.70 76.95 0.00 76.57
LLaMA-2 Generalization 0.00 1.93 0.00 0.19 67.89 0.00 70.11
Locality 100.0 98.89 99.66 95.15 90.14 100.0 94.16
Portability 56.66 0.54 0.87 48.02 58.63 56.66 58.19
Table3: Multi-TaskEditingSetting: EditorstrainonahybridofCounterFact,Recent,andConvSentdatasets,andtestontheirspecifictest
sets. HoldOutEditingSetting: TheabovementionededitorsaretestedonZsRE(OODdata). Allmetricsareâ€œthehigher,thebetterâ€.
tasksanditsinsufficientgeneralizationcapabilityforhandling Fact,Recent,ConvSent,andZsRE,butwithnewinstructions.
OODscenarios. WeobservethatInstructEditcaneffectively Specifically,asoutlinedinSection4.2,weconstructfivenovel,
addressthesechallenges. Notethatsuchrobustgeneralization unseeninstructionstoassesstheEditorâ€™sproficiencyingener-
abilitiesaremainlyinherentininstructiontuning,asynergy alizinginstructions. ObservationsfromFigure3revealthat
thatenablesInstructEdittoattainperformancelevelsonpar theEditorisindeedcapableofadaptingtotheseUnseenIn-
withsingle-taskediting,evenontaskdatasetsthatareunseen structions. Itisnoteworthythatutilizinginstructionsonwhich
duringthetrainingphase. the Editor has been trained can result in enhanced editing
performance. Thus, InstructEditcanachievecomparable
Recent CounterFact outcomesbyemployinginstructionsthataresemanticallyakin
tothoseencounteredduringtraining. Theseempiricalresults
Reliability 85.7 84.7 80.8 80.4
alsoindicatethatwecandevelopanEditortofollowhuman
Generalization 51.6 47.4 53.2 49.9 instructionsandweleavethisforfutureworks.
Locality 64.6 64.9 67.8 68.2
5.4 WhyInstructionHelpsMulti-TaskEditing?
Portability 47.3 47.0 50.8 51.0
Weanalyzetheprincipalcomponentsoftheeditingareaâˆ‡(cid:174)
ğ‘¢â„“
Seen Unseen Seen Unseen usingt-SNE,aspresentedinSection4.2,whichisgenerated
Figure3: InstructEditdemonstratesproficiencyingeneralizingto by the editor for specific layers of LLMs. Our underlying
Unseeninstructions(unseeninstructionsintroducedinSection4.2), assumptionisthattheseprincipalcomponentsencapsulatethe
achievingresultscomparabletoSeeninstructions. intrinsiccharacteristicsoftheeditingareainvolvedinediting
thedata. Specifically,wefocusouranalysisoncaseswherethe
conventionaleditingmethodsfallshort,whileInstructEdit
TransfertoUnseenInstructions. Todelvedeeperintothe
demonstrateseffectiveness.
generalizabilityofInstruction-basedEditing,weevaluateIn-
structEditâ€™s capacity with instructions that have not been Finding 1: Instruction can Help Control Optimization
encountered previously. This setting is different from the Direction. AsobservedinTable2,MENDexhibitssubpar
holdouteditingsettingsincewestillusethedatainCounter- performanceinmulti-taskscenarios,particularlyintermsof(a) (b) (c)
Task Scaling
100
Recent (InstructEdit) Recent (Multi-Task) Recent â†’ Recent Reliability
CF (InstructEdit) CF (Multi-Task) Recent â†’ CF
ZsRE (InstructEdit) ZsRE (Multi-Task) Recent â†’ ZsRE 80
60
40
Recent (Multi-Task) Recent (Single-Task) Recent (InstructEdit) Recent 20
CF (Multi-Task) CF (Single-Task) CF (InstructEdit) CounterFact
ZsRE (Multi-Task) ZsRE (Single-Task) ZsRE (InstructEdit) ZsRE
(d) 0
100
Recent & ConvSent â†’ Recent Recent (Multi-Task) Recent (InstructEdit) Generalization
Recent & ConvSent â†’ CF CF (Multi-Task) CF (InstructEdit)
Recent & ConvSent â†’ ZsRE ZsRE (Multi-Task) ZsRE (InstructEdit) 80
60
40
Recent (Multi-Task) Recent & ConvSent (Balanced) â†’ Recent Recent & CF â†’ Recent Recent 20
CF (Multi-Task) Recent & ConvSent (Balanced) â†’ CF Recent & CF â†’ CF CounterFact
ZsRE (Multi-Task) Recent & ConvSent (Balanced) â†’ ZsRE Recent & CF â†’ ZsRE ZsRE
0
# Tasks 1 2 3
Figure4: (a)Comparesinstructioneffectsonknowledgeeditinggradientâˆ‡Ëœ ğ‘¢â„“. Recent(InstructEdit)andRecent(Multi-Task)illustrateâˆ‡Ëœ
ğ‘¢â„“
onRecentusingInstructEditandMENDinmulti-tasksettings,respectively. Recent(Single-Task)showsMENDâ€™sresultsoftrainingon
Recentalone. (b)Demonstratestaskscalingâ€™simpactonInstructEdit,withRecentâ†’ZsREfortrainingonRecentandtestingonZsRE,
andRecent&CFâ†’ZsREforjointtrainingonRecent,CounterFact,andtestingonZsRE. (c)Illustratesthereliabilityandgeneralization
performanceacrosstaskscaling. (d)BalancesConvSentbyextracting1,427entriesforConvSent(Balanced).
ReliabilityandGeneralization,whereitissignificantlyoutper- extrapolateeffectivelytonew,unseendomains,whileofferinga
formed by InstructEdit. Upon analyzing the left panel in trade-offbetweenspecializedknowledgeadaptationandbroad
Figure4(a),weobservethatMEND,whenhandlingmulti-task generalization. Nevertheless,itiscrucialtoacknowledgethat
editing,tendstoshowsignificantoverlapineditingareaacross ascalabilitybottleneckmightbeencountered,andconfronting
differenttasks. ThisoverlapcouldpotentiallycauseMENDto entirely new types of editing tasks, such as cross-linguistic
notonlyconfusepreviouslylearnedtasksbutalsostrugglein tasks,willintroducefurthercomplexities.
effectivelygeneralizingtonewtaskswithshifteddistribution
comparedtothetrainingtasks. However,byintroducingin- Finding3: ImprovingPerformancewithAppropriateData
structions,InstructEditcaneffectivelycontroltheknowledge Proportion. In preliminary experiments, we notice task
imbalances impede proper multi-task training and cause a
editinggradientandencouragedistinctseparationwithade-
significantperformancedeclinewhenConvSentisinvolved
quatemarginintheeditingareaforvarioustasks,whichaligns
in the training. Hence, we contemplate balancing the data
withthedistributionobservedinthesingle-tasktrainingsetting
proportionsacrossdifferenttasks. ByobservingFigure4(d),
intherightpanelofFigure4(a). Furthermore,thediscrimina-
wefindthattheknowledgeeditinggradientdirectionsbecome
tiveeditingareainInstructEditisadaptabletoOODdata,
more regular after data balancing and editing reliability of
whichleadstosuperiorknowledgeeditingwhenhandlingnew
the editor increases from 18.23 to 25.55 on the OOD tasks.
tasks,whilemaintainingperformancecomparabletomodels
Additionally,wefindthattaskimbalancesleadtotheeditor
trainedonsingletasksonIDtasks.
inducingeditinggradientsofrelativelylargemagnitudes,and
the gradient magnitude distributions for each task vary sig-
Finding 2: More Tasks, Stronger OOD Generalization.
Figure4(b)illustratesthatwhenInstructEditistrainedona nificantly, which appears to be a key factor influencing the
singletask,theeditingareasofthethreetasksappearsomewhat editorâ€™sgeneralization. Thisresultconfirmsthesignificance
discriminative. Instead,theperformanceofthecorresponding ofappropriatedataproportionsformulti-taskediting.
tasksissuboptimal,asdemonstratedinFigure4(c). Wethink
that even though instructions aid in distinguishing different
6 DiscussionandConclusion
tasks, the knowledge learned from a single task struggles
to generalize to others. By scaling the number of tasks in WefocusonanewproblemofknowledgeeditingforLLMs:
training,wenoticethattheeditingareasofInstructEditfor generalizingtonewtasks. Weintroducemulti-taskediting,
various tasks almost see no overlap, and editing reliability illustratingthelimitationsofexistingknowledgeeditingap-
improvescorrespondinglyinFigure4(c). Furthermore,asthe proachesintasktransferabilityandpresentingaviablesolution
scopeoftasksbroadens,thedirectionsofknowledgeediting InstructEdit. Theproposedapproachcaneffectivelyguide
gradient of different tasks start to converge, yet they retain theEditorforpreciseediting,withitseffectivenessconfirmed
theirrelativemargin. Intuitively,InstructEdittrainedacross throughcomprehensiveexperimentsandvisualizationanalysis.
diversedomainsharnessesthesedomain-relatedinstructionsto Additionally, our work can enrich the field by contributinghigh-qualityinstructions,buildingonthefoundationofvarious [Haseetal.,2023] PeterHase,MohitBansal,BeenKim,and
existing,popularknowledgeeditingtaskdatasets. AsmaGhandeharioun. Doeslocalizationinformediting?
surprising differences in causality-based localization vs.
knowledgeeditinginlanguagemodels,2023.
References
[Hernandezetal.,2023] Evan Hernandez, Arnab Sen
[AkyuÂ¨reketal.,2023] AfraFeyzaAkyuÂ¨rek,EricPan,Garry
Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg,
Kuwanto, and Derry Wijaya. Dune: Dataset for unified
JacobAndreas,YonatanBelinkov,andDavidBau. Linear-
editing. InEMNLP,2023.
ity of relation decoding in transformer language models,
[Brownandetal.,2020] TomB.Brownandetal. Language 2023.
modelsarefew-shotlearners. InNeurIPS,2020. [Huetal.,2022] EdwardJ.Hu,YelongShen,PhillipWallis,
[Brownetal.,2023] Davis Brown, Charles Godfrey, Cody ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
Nizinski, Jonathan Tu, and Henry Kvinge. Edit at your WeizhuChen.Lora: Low-rankadaptationoflargelanguage
own risk: evaluating the robustness of edited models to models. InICLR,2022.
distributionshifts,2023. [Huangetal.,2023] Zeyu Huang, Yikang Shen, Xiaofeng
[Caoetal.,2021] NicolaDeCao,WilkerAziz,andIvanTitov. Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.
Transformer-patcher: Onemistakeworthoneneuron. In
Editingfactualknowledgeinlanguagemodels. InEMNLP,
2021.
ICLR,2023.
[Chengetal.,2023] SiyuanCheng,BozhongTian,Qingbin
[Huangetal.,2024] YouchengHuang,WenqiangLei,Zheng
Zhang, Jiancheng Lv, and Shuicheng Yan. See the un-
Liu,XiChen,YonghengWang,HuajunChen,andNingyu
seen: Bettercontext-consistentknowledge-editingbynoises,
Zhang. Canweeditmultimodallargelanguagemodels? In
2024.
EMNLP,2023.
[Chengetal.,2024] SiyuanCheng,NingyuZhang,Bozhong
[Ilharcoetal.,2023] Gabriel Ilharco, Marco Tulio Ribeiro,
MitchellWortsman,LudwigSchmidt,HannanehHajishirzi,
Tian,XiChen,QingbingLiu,andHuajunChen. Editing
andAliFarhadi. Editingmodelswithtaskarithmetic. In
languagemodel-basedknowledgegraphembeddings. In
ICLR,2023.
AAAI,2024.
[JuandZhang,2023] YimingJuandZhengZhang. Klob: a
[Cohenetal.,2023] RoiCohen,EdenBiran,OriYoran,Amir
benchmark for assessing knowledge locating methods in
Globerson, and Mor Geva. Evaluating the ripple ef-
languagemodels,2023.
fects of knowledge editing in language models. CoRR,
abs/2307.12976,2023. [Lietal.,2023a] Kenneth Li, Oam Patel, Fernanda VieÂ´gas,
HanspeterPfister,andMartinWattenberg. Inference-time
[Daietal.,2022] Damai Dai, Li Dong, Yaru Hao, Zhifang
intervention: Eliciting truthful answers from a language
Sui,BaobaoChang,andFuruWei. Knowledgeneuronsin
model. InNeurIPS,2023.
pretrainedtransformers. InACL,2022.
[Lietal.,2023b] Xiaopeng Li, Shasha Li, Shezheng Song,
[Dongetal.,2022] QingxiuDong,DamaiDai,YifanSong, JingYang,JunMa,andJieYu.Pmet: Precisemodelediting
JingjingXu,ZhifangSui,andLeiLi. Calibratingfactual inatransformer. 2023.
knowledge in pretrained language models. In EMNLP,
2022.
[Lietal.,2023c] Zhoubo Li, Ningyu Zhang, Yunzhi Yao,
MengruWang,XiChen,andHuajunChen. Unveilingthe
[Gandikotaetal.,2023] RohitGandikota,JoannaMaterzyn- pitfalls of knowledge editing for large language models.
ska,JadenFiotto-Kaufman,andDavidBau. Erasingcon- arXivpreprintarXiv:2310.02129,2023.
ceptsfromdiffusionmodels. CoRR,abs/2303.07345,2023.
[Lietal.,2023d] Zichao Li, Ines Arous, Siva Reddy, and
[Guetal.,2024] Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Jackie C. K. Cheung. Evaluating dependencies in fact
PanLu,Zhen-HuaLing,Kai-WeiChang,andNanyunPeng. editingforlanguagemodels: Specificityandimplication
Modeleditingcanhurtgeneralabilitiesoflargelanguage awareness. InEMNLP,2023.
models,2024. [Liangetal.,2022] XiaozhuanLiang,NingyuZhang,Siyuan
[Guptaetal.,2023] Anshita Gupta, Debanjan Mondal, Ak- Cheng, Zhenru Zhang, Chuanqi Tan, and Huajun Chen.
shayKrishnaSheshadri,WenlongZhao,XiangLorraineLi, Contrastivedemonstrationtuningforpre-trainedlanguage
SarahWiegreffe,andNiketTandon. Editingcommonsense models. InEMNLP,2022.
knowledgeinGPT. InEMNLP,2023. [Loetal.,2024] MichelleLo,ShayB.Cohen,andFazlBarez.
[Guptaetal.,2024] AkshatGupta,AnuragRao,andGopala Largelanguagemodelsrelearnremovedconcepts,2024.
Anumanchipalli. Modeleditingatscaleleadstogradual [Maandetal.,2024] Jun-YuMaandetal. Neighboringper-
andcatastrophicforgetting,2024. turbationsofknowledgeeditingonlargelanguagemodels,
2024.
[Hartvigsenandetal.,2022] Thomas Hartvigsen and et al.
AgingwithGRACE:lifelongmodeleditingwithdiscrete [Maoetal.,2023] Shengyu Mao, Ningyu Zhang, Xiaohan
key-valueadaptors. InNeurIPS,2022. Wang, MengruWang, YunzhiYao, YongJiang, PengjunXie,FeiHuang,andHuajunChen. Editingpersonalityfor [Taorietal.,2023] Rohan Taori, Ishaan Gulrajani, Tianyi
llms. 2023. Zhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriB.Hashimoto. Stanfordalpaca: An
[Mazziaandetal.,2023] VittorioMazziaandetal. Asurvey
instruction-following llama model. https://github.com/
onknowledgeeditingofneuralnetworks,2023.
tatsu-lab/stanford alpaca,2023.
[Mengetal.,2022a] Kevin Meng, David Bau, Alex Ando-
nian,andYonatanBelinkov. Locatingandeditingfactual [Touvronandetal.,2023] Hugo Touvron and et al. Llama:
knowledgeinGPT. InNeurIPS,2022. Open and efficient foundation language models. CoRR,
abs/2302.13971,2023.
[Mengetal.,2022b] KevinMeng,ArnabSenSharma,Alex
Andonian,YonatanBelinkov,andDavidBau. Mass-editing [Wangandetal.,2023a] PengWangandetal. Easyedit: An
memoryinatransformer. CoRR,abs/2210.07229,2022. easy-to-useknowledgeeditingframeworkforlargelanguage
[Mitchelletal.,2022a] EricMitchell, CharlesLin, Antoine
models. CoRR,abs/2308.07269,2023.
Bosselut,ChelseaFinn,andChristopherD.Manning. Fast [Wangandetal.,2023b] Song Wang and et al. Knowledge
modeleditingatscale. InICLR,2022. editingforlargelanguagemodels: Asurvey,2023.
[Mitchelletal.,2022b] EricMitchell,CharlesLin,Antoine [Wangetal.,2023] JiaanWang,YunlongLiang,ZengkuiSun,
Bosselut, Christopher D. Manning, and Chelsea Finn. Yuxuan Cao, and Jiarong Xu. Cross-lingual knowledge
Memory-basedmodeleditingatscale. InICML,2022. editinginlargelanguagemodels,2023.
[Onoeetal.,2023] Yasumasa Onoe, Michael J. Q. Zhang, [Weietal.,2022] Jason Wei, Maarten Bosma, Vincent Y.
Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Zhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,
Canlmslearnnewentitiesfromdescriptions? challenges Andrew M. Dai, and Quoc V. Le. Finetuned language
inpropagatinginjectedknowledge. InACL,2023. modelsarezero-shotlearners. InICLR,2022.
[OpenAI,2022] OpenAI. Theblogusedtointroducechatgpt. [Weietal.,2023] Yifan Wei, Xiaoyan Yu, Huanhuan Ma,
https://openai.com/blog/chatgpt,2022.
FangyuLei,YixuanWeng,RanSong,andKangLiu. As-
[Ouyangandetal.,2022] LongOuyangandetal. Training sessingknowledgeeditinginlanguagemodelsviarelation
languagemodelstofollowinstructionswithhumanfeedback. perspective,2023.
InNeurIPS,2022.
[Wuetal.,2023] SuhangWu,MinlongPeng,YueChen,Jin-
[Panetal.,2023] Haowen Pan, Yixin Cao, Xiaozhi Wang, song Su, and Mingming Sun. Eva-kellm: A new bench-
andXunYang. Findingandeditingmulti-modalneuronsin mark for evaluating knowledge editing of llms. CoRR,
pre-trainedtransformer,2023. abs/2308.09954,2023.
[PinterandElhadad,2023] Yuval Pinter and Michael El- [Xuetal.,2022] Yang Xu, Yutai Hou, and Wanxiang Che.
hadad. Emptying the ocean with a spoon: Should we Languageanisotropiccross-lingualmodelediting. ArXiv,
editmodels? InEMNLP,2023. abs/2205.12677,2022.
[Radfordetal.,2019] AlecRadford,JeffWu,RewonChild, [Yaoetal.,2023] Yunzhi Yao, Peng Wang, Bozhong Tian,
DavidLuan,DarioAmodei,andIlyaSutskever. Language
SiyuanCheng,ZhouboLi,ShuminDeng,HuajunChen,and
modelsareunsupervisedmultitasklearners. 2019.
NingyuZhang. Editinglargelanguagemodels: Problems,
[Siandetal.,2023] Nianwen Si and et al. Knowledge un- methods,andopportunities. InEMNLP,2023.
learningforllms: Tasks,methods,andchallenges,2023.
[Yinetal.,2024] XunjianYin,JinJiang,LimingYang,and
[Sietal.,2024] NianwenSi,HaoZhang,andWeiqiangZhang. XiaojunWan.Historymatters: Temporalknowledgeediting
Mpn: Leveraging multilingual patch neuron for cross- inlargelanguagemodel. InAAAI,2024.
lingualmodelediting,2024.
[Yuandetal.,2024] LangYuandetal. MELO:enhancing
[Sinitsinetal.,2020] AntonSinitsin,VsevolodPlokhotnyuk, modeleditingwithneuron-indexeddynamiclora. InAAAI,
DmitryV.Pyrkin,SergeiPopov,andArtemBabenko. Ed- 2024.
itableneuralnetworks. InICLR,2020.
[Zhangandetal.,2023] Shengyu Zhang and et al. Instruc-
[Suetal.,2023] Hongjin Su, Weijia Shi, Jungo Kasai,
tion tuning for large language models: A survey. CoRR,
YizhongWang,YushiHu,MariOstendorf,WentauYih,
abs/2308.10792,2023.
NoahA.Smith,LukeZettlemoyer,andTaoYu. Oneem-
bedder,anytask: Instruction-finetunedtextembeddings. In [Zhangandetal.,2024] Ningyu Zhang and et al. A com-
ACL,2023. prehensivestudyofknowledgeeditingforlargelanguage
models,2024.
[Sunandetal.,2023] TianxiangSunandetal. Moss: Train-
ing conversational language models from synthetic data. [Zhangetal.,2023] NingyuZhang,YunzhiYao,andShumin
2023. Deng. Editinglargelanguagemodels. InAACL:Tutorial
[Tanetal.,2024] ChenmienTan,GeZhang,andJieFu. Mas-
Abstract,2023.
siveeditingforlargelanguagemodelsviametalearning. In [Zhaoandetal.,2023] WayneXinZhaoandetal. Asurvey
ICLR,2024. oflargelanguagemodels. CoRR,abs/2303.18223,2023.[Zhengetal.,2023] CeZheng,LeiLi,QingxiuDong,Yuxuan C CaseStudy
Fan,ZhiyongWu,JingjingXu,andBaobaoChang. Canwe
InthecaseofthedatasetCounterFact,whenpromptedwith
editfactualknowledgebyin-contextlearning? InEMNLP,
thequeryâ€œThenameofthespouseofJamesR.Jordan,Sr. is?â€
2023.
withthetargetresponsebeingâ€œZhangYouâ€. Priortoediting,
[Zhongetal.,2023] ZexuanZhong,ZhengxuanWu,Christo- themodelâ€™sresponseisâ€œJamesâ€. Themodel,editedbyMEND,
incorrectly answers â€œJames Youâ€ while InstructEdit with
pher D. Manning, Christopher Potts, and Danqi Chen.
instructioncorrectlyidentifiesâ€œZhangYouâ€. Inthecaseofthe
Mquake: Assessingknowledgeeditinginlanguagemodels
holdoutdatasetZsRE,whenpromptedwiththequeryâ€œWhatis
viamulti-hopquestions. InEMNLP,2023.
thenativetongueofPierreCorneille?â€ withthetargetresponse
beingâ€œGermanâ€. Priortoediting,themodelâ€™sresponseisa
A Experimentaldetails blankspace. Themodel,editedbyMEND,incorrectlyanswers
â€œFrenchâ€whileInstructEditcorrectlyidentifiesâ€œGermanâ€.
WeutilizePytorchtoconductexperimentsonasingleA800
GPU.Thenumberofeditissetto1. Themaxsequencelength
is set to 96. InstructEdit and MENDâ€™s optimizations are
performedusingtheAdamoptimizer,whileKnowledgeEditor
(KE)utilizesAdamw,andFT-LandCaliNetemployAdaFactor.
Layer normalization ğœ– is set to 1e-1 across all modules2.
Gridsearchisusedforhyperparametertuning,andthebest
hyperparametersareinTable4.
B DatasetsDetails
Table5presentsthedatasetstatistics,includingtheoriginal
countofConvSent. However,thebodyofthepaperfocuses
onasubsetofConvSentthathavebeenbalancedtoensurea
fairtrainingprocess,whichisreflectedinthemainfindings.
ThebalancedConvSentisusedfortraininginthemainresults
presentedinthepaper. Notably,Thetrainingprocessforthe
ConvSent is described as being consistent with the original
setting, particularlyintermsoflosscalculation. Itrequires
inputtingaspecificnumberofsentencesintothemodel,which
demands a significant amount of VRAM. Due to the high
VRAMrequirements,wedonotconductexperimentsonthe
ConvSentdatasetusingtheLLaMAmodel. Whilethetraining
processforConvSentremainsthesame,theevaluationmethod
hasbeenslightlymodified;inputsconcerningReliabilityand
Localityarematchedtoform10uniquetestcases,whichare
thenassessedindividuallywiththeirperformanceaveragedto
yieldtheoverallresults.
Type Ours MEND KE CaliNet
EditNum 1 1 1 1
Steps 5k 5k 10k 15k
Accumulate 2 2 2 2
LearningRate 1e-6 1e-6 1e-5 5e-4
Table4: Thebesthyperparameters.
Type CounterFact Recent ConvSent ZsRE
Train 1,427 570 14,390 -
Test 839 1,266 800 1,037
Table5: Thestatisticsofdatasets.