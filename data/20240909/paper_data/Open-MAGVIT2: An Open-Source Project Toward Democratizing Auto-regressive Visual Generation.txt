TechnicalReport
OPEN-MAGVIT2:
AN OPEN-SOURCE PROJECT TOWARD DEMOCRATIZ-
ING AUTO-REGRESSIVE VISUAL GENERATION
ZhuoyanLuo1,2Ëš FengyuanShi1,3Ëš YixiaoGe1: YujiuYang2 LiminWang3 YingShan1
1ARCLab,TencentPCG 2TsinghuaUniversity 3NanjingUniversity
https://github.com/TencentARC/Open-MAGVIT2
Figure 1: Reconstruction and generation samples of Open-MAGVIT2. We show 1024Ë†1024 recon-
structedsamples(top)and256Ë†256generatedsamples(middleandbottom).
ABSTRACT
WepresentOpen-MAGVIT2,afamilyofauto-regressiveimagegenerationmod-
elsrangingfrom300Mto1.5B.TheOpen-MAGVIT2projectproducesanopen-
source replication of Googleâ€™s MAGVIT-v2 tokenizer, a tokenizer with a super-
large codebook (i.e., 218 codes), and achieves the state-of-the-art reconstruction
performance (1.17 rFID) on ImageNet 256Ë†256. Furthermore, we explore its
applicationinplainauto-regressivemodelsandvalidatescalabilityproperties. To
assistauto-regressivemodelsinpredictingwithasuper-largevocabulary,wefac-
torize it into two sub-vocabulary of different sizes by asymmetric token factor-
ization, and further introduce â€œnext sub-token predictionâ€ to enhance sub-token
interactionforbettergenerationquality. Wereleaseallmodelsandcodestofoster
innovationandcreativityinthefieldofauto-regressivevisualgeneration.
ËšEqualContribution.WorkdoneduringaninternshipatARCLab,TencentPCG.
:Correspondingauthorandprojectlead.
1
4202
peS
6
]VC.sc[
1v01440.9042:viXraTechnicalReport
Table1: ModelconfigurationsofOpen-MAGVIT2. Wepartiallyfollowthescalingruleproposed
inthepreviousworks(Sunetal.,2024;Tianetal.,2024).
Model Parameters Inter-BlocksN Intra-BlocksL Widthsw Headsh
Open-MAGVIT2-B 343M 24 2 1024 16
Open-MAGVIT2-L 804M 36 3 1280 20
Open-MAGVIT2-XL 1.5B 48 4 1536 24
1 INTRODUCTION
LargeLanguageModels(LLMs),builtuponauto-regressivetransformer(Vaswanietal.,2017;Ope-
nAI,2023;Chowdheryetal.,2022;Touvronetal.,2023),havedemonstrateddominanceinnatural
languagegenerationduetotheincrediblecontextmodelingandscalability. Inspiredbythis,emer-
gent works introduce auto-regressive models into visual generation (Van Den Oord et al., 2017;
Esser et al., 2021; Yu et al., 2022; Lee et al., 2022; Sun et al., 2024). These approaches first uti-
lizeavectorquantizerforimagetokenizationandde-tokenization,thenemployanauto-regressive
transformerfordiscreteimagetokensequencemodeling.
Although great processes are achieved, the quality of visual generation still falls behind the
diffusion-based methods. The main factor is limited tokenizer performance. Tokenizers are gen-
erallypositedastheupperboundofthevisualgeneration,andinferioroff-the-shelftokenizers(e.g.,
VQ-VAE(VanDenOordetal.,2017))willleadtopoorgenerationquality.Althoughsomeimprove-
ments are done (Yu et al., 2022; Lee et al., 2022; Sun et al., 2024), current tokenizers are limited
by the codebook size and utilization, and the reconstruction performance is still far worse than
VAE(Kingma, 2013; Rombach et al., 2022b) used in diffusion models. To unlock the potential of
tokenizers,MAGVIT-v2(Yuetal.,2024a)proposesLookup-FreeQuantizertoenableahighlycode-
activated and super-large codebook, and achieves better generation quality than diffusion models.
However,suchapowerfulvisualtokenizeriscompletelyclosed-sourceandwehavenoaccess
tothissofar,limitingthedevelopmentoftheacademiccommunity.
Inthiswork,wepushforwardtheauto-regressivevisualgenerationintwofolds: 1)Replicationof
thevisualtokenizer:Were-implementtheadvancedLookup-FreeQuantizerproposedbyMAGVIT-
v2. Toourbestknowledge,ouropen-sourcereplicationachievestheclosestreconstructionperfor-
mancestatedinMAGVIT-v2(1.18vs. 1.15rFIDonImageNet128Ë†128)andoutperformsallother
methods on the hallmark Imagenet benchmark (Deng et al., 2009). 2) Integrating a super-large
codebookwithARvisualgeneration: InsteadofsimplyfollowingMAGVIT-v2thatleveragesthe
vision-oriented design (i.e., mask generative methods (Chang et al., 2022) for visual synthesis),
weseektoexploitthepotentialofsuchalargecodebookinvanillaauto-regressivegeneration. To
assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two
sub-vocabulary of different sizes by asymmetric token factorization, and further introduce â€œnext
sub-token predictionâ€ to enhance sub-token interaction for better generation quality. Our experi-
mentsonthestandardvisualgenerationdatasetImageNetsuggestthat,withthepowerfultokenizer,
theplainauto-regressivemodelexhibitssuperiorityandscalability.
2 METHOD
2.1 OVERVIEW
Open-MAGVIT2iscomposedoftwosignificantstages.Oneisapowerfulvisualtokenizerthatmaps
theinputvisualsignalintothediscretetokenrepresentations.Subsequently,thevector-quantizedse-
quencewillbefedintotheauto-regressivetransformerforintra-andinter-tokenrelationshipmod-
eling,eventuallyforvisualsynthesis.
2.2 VISUALTOKENIZER
Preliminary. Visual tokenization is fundamentally deemed as the crucial component in multi-
modal large language models (MLLMs) to understand the visual signal input. The CNN-based
2TechnicalReport
...
1 ğ‘‡ğ‘‡12 3 ğ‘‡ğ‘‡24 5 ğ‘‡ğ‘‡36 2t-1ğ‘‡ğ‘‡ğ‘¡ğ‘¡2t
MAGVIT2 Intra- Intra- Intra- S Ph .aa r.r a.e md . Intra-
Encoder Block Block Block Block
Ã—L Ã—L Ã—L Ã—L
...
Image C 1 C 3 C 5 C 2t-1
ğ‘‡ğ‘‡ğ‘¡ğ‘¡
LFQ ğ‘‡ğ‘‡1 ğ‘‡ğ‘‡2 ğ‘‡ğ‘‡3 ğ‘‡ğ‘‡ğ‘¡ğ‘¡
Llama Inter-Block
Ã—N
M DA eG coV dI eT r2 S 1 + 2 3 + 4 ... 2t-3+2t-2
Class Token
+1 +1 ... -1 ğ‘‡ğ‘‡1 Subtokğ‘‡ğ‘‡e2n C Intrağ‘‡ğ‘‡-Tğ‘¡ğ‘¡oâˆ’k1en
Modeling
Reconstruction
ğ‘šğ‘š2
ğ‘šğ‘šğ¾ğ¾ +1 -1 ğ’Œğ’Œ... +1 ğ¹ğ¹
ğ¹ğ¹
Subtoken C Ton ot ke ex nt In Mte or
d
â€“ eT lio nk gen
ğ‘šğ‘š0
ğ‘²ğ‘²âˆ’ğ’Œğ’Œ
Figure 2: Overview of Open-MAGVIT2. There are two crucial stages in Open-MAGVIT2. In
StageI: theimageisfirstencodedbyMAGVIT-v2Encoderandsubsequentlytransformedintobits
format by Lookup-Free Quantizer (LFQ). In Stage II: The quantized features are further mapped
intodiscretevisualtokensandinputintotheLlama-basedauto-regressiveframeworkforintra-and
inter-tokenrelationshipmodeling.
encoder-quantizer-decoder architecture first proposed in VQVAE (Van Den Oord et al., 2017) is
well adoptedas the visualtokenizer, which maps inputpixels into discreterepresentations and re-
constructsimagesfromquantizedfeatures. Specifically,givenanimageI PR3Ë†HË†W,theencoder
projectsitintothefeaturemapZ PRDË†H1Ë†W1,whereH1 â€œH{p,W1 â€œW{p,andpisthedown-
sample ratio. The quantizer containing a learnable codebook E P R2KË†D then selects the closest
entry zË† P RD from the codebook for each feature vector z P RD. And we can use discrete token
indicesX â€œtx uH1Ë†W1 torepresentthecontinuousfeaturemapZ. Fordecoding,eachcodeindex
i iâ€œ1
willbemappedbacktothequantizedfeaturevectorandinputintothedecoderforpixel-levelimage
reconstruction.
Review of Lookup-Free Quantization. Motivated by the relationship between the size of the
codebook and the dimension of code embeddings, MAGVIT-v2 (Yu et al., 2024a) eliminates the
needforembeddinglookupbyreducingthedimensionofcodeembeddingtozero. Specifically,the
codebook is shrunk into an integer set where the latent space of each entry is decomposed as the
Åš
Cartesianproductofsingle-dimensionalvariables(i.e.,CË† â€œ K tÂ´1,1u,|CË† |â€œ2K). Asshownin
iâ€œ1
Fig.2,thetokenizationprocesscanbesimplifiedas:
zË† â€œsignpz qâ€œÂ´1tz Ä0u`1tz Ä…0u, (1)
i i i i
wherezË† denotesthequantizedrepresentationofthefeaturevectorz . Andthetokenindexforz is
i i i
givenby:
Ã¿K
Indexpz qâ€œ 2kÂ´11tzË† Ä…0u. (2)
i ik
kâ€œ1
Toencouragetheconfidentassignmentofeachcodebookentryandutilizationofthewholecodebook
simultaneously,MAGVIT-v2furtherintroducesentropyloss:
Ã¿ Ã¿
1 1
L â€œ HpfpzqqÂ´Hp fpzqq, (3)
entropy BH1W1 BH1W1
whereHpÂ¨qdenotestheentropy,Bisbatchsizes,andfpÂ¨qisamappingfunctionfromlatentspaceto
acategoricaldistributionspecifyingtheprobabilityofassignmenttoeachentry. Inourexperiment,
weobservethatreplacingtraditionalcodeassignment(i.e.,pair-wisedistance)withthislookup-free
quantizationenablestrainingasuper-largecodebook(i.e.,218codes)ofhighutilization(100%).
3TechnicalReport
Original LPIPS = 0.315 0.256 0.236
â†“
Original LPIPS = 0.194 0.148 0.134
â†“
Original LPIPS = 0.085 0.082 0.059
VQGAN LlamaGen Open-MAGVIT2
â†“
Figure 3: Reconstruction comparison with different tokenizers. We compare VQGAN (Esser
etal.,2021),LlamaGen(Sunetal.,2024)andourmodelstrainedonImageNet. (Bestviewedwith
zoomingin. TheoriginalimagesarefromUnsplash).
ReviewofArchitectureimprovements. Intuitively,sinceeachcontinuousfeaturevectorwillbe
quantized into K bits, it poses a significant challenge to both the encoder and decoder. There-
fore,were-implementthearchitectureimprovementstechniqueillustratedin(Yuetal.,2024a). 1)
Downsamplersintheencoderarestridedconvolutionswithlearnedkernelswhileupsamplersinthe
decoder are the depth-to-space operator. 2) Following (Karras et al., 2019; Peebles & Xie, 2023;
Huang&Belongie,2017),were-implementtheAdaptiveGroupNormLayer,whichintegratesthe
quantizedvectorwiththeoutputofeachresidualblockinthedecoder.
2.3 AUTO-REGRESSIVETRANSFORMER
Preliminary. Given a sequence of discrete tokens X â€œ tx uT ,T â€œ H1 Ë†W1 from the visual
i iâ€œ1
tokenizer, the auto-regressive transformer predicts the next token x conditioned on the previous
t
tokenstx ,x ,Â¨Â¨Â¨ ,x u:
1 2 tÂ´1
ÅºT
ppx ,x ,Â¨Â¨Â¨ ,x qâ€œ ppx |x ,x ,Â¨Â¨Â¨ ,x q. (4)
1 2 T t 1 2 tÂ´1
tâ€œ1
Auto-regressive Architecture. Considering the different scales of auto-regressive transformer
(i.e., from â€300M to 1B) and the limited training academic data, directly optimizing such a large
vocabulary(i.e.,218codes)isimpractical.Therefore,weproposetheasymmetrictokenfactorization
technique to assist models in performing â€œnext-token predictionâ€ within concatenated codebooks.
Specifically, the LFQ tokenâ€™s latent space is factorized into M subspaces tx1uT , tx2uT , Â¨Â¨Â¨,
i iâ€œ1 i iâ€œ1
4TechnicalReport
txMuT , each of which contains 2km tokens. As shown in Fig. 2, each subspace is embedded
i iâ€œ1
individuallyandtheirsummationisusedasthetransformerinputs. Conventionally,anintuitiveso-
lutiontoperformauto-regressivewithinsubspacesisleveragingM separateheadsforindependent
categoricaldistributionmodeling. However,sincebothsub-tokensarederivedfromthesamelatent
spaces,suchasimpleoperationmayignoretheirintra-correlation. Consequently,inspiredby(Lee
etal.,2022),wereformulatetheautoregressionparadigmintomodelingbothintra-andinter-token
dependency, which is essentially â€œnext sub-token predictionâ€. In this manner, the representational
capacityofthesuper-largecodebookcanexhibitgreatpotentialinauto-regressivegenerationwith
betterscalability.
1) Inter-token Relationship: Given a set of sub-tokens from the visual tokenizers, a stacked of
LlamablockswithN layersandwwidthareleveragedtocapturethein-contextinformationbetween
tokens. Theprocesscanbeformulatedas:
Ã¿M Ã¿M
C â€œLlamaBlockps,p xiq,Â¨Â¨Â¨ ,p xi qq, (5)
t 1 tÂ´1
iâ€œ1 iâ€œ1
wheres denotestheconditionaltokens,C
t
PRTË†ws isthet-thcontexttoken.
2)Intra-tokenRelationship:WefurtherutilizeatransformerwithLintra-blockstoautoregressively
predicttheeachsub-token(x1,x2,Â¨Â¨Â¨ ,xM)atthepositiont. Byassociatingthesub-tokencondi-
t t t
tionedwithcontextual-enrichedvectorC,theintra-dependencywithintokenscanbewellmodeled.
Formally, at t position, the autoregression of predicting the conditional distribution of each sub-
tokenis:
p â€œLlamaBlockpC ,x1Â¨Â¨Â¨ ,xmÂ´1q. (6)
tm t t t
Therefore,theauto-regressivelikelihoodisformulatedas:
ÅºT
ppX ,X ,Â¨Â¨Â¨ ,X qâ€œ ppX |X ,X ,Â¨Â¨Â¨ ,X q
1 2 T t 1 2 tÂ´1
tâ€œ1
(7)
ÅºT ÅºM
â€œ ppxm|pX ,X ,Â¨Â¨Â¨ ,X q,px1,x2,Â¨Â¨Â¨xmÂ´1qq,
t 1 2 tÂ´1 t t t
tâ€œ1mâ€œ1
whereX specifiesasetofsub-tokentx1,x2,Â¨Â¨Â¨ ,xMuateachpositiont.
t t t t
3 EXPERIMENTS
3.1 DATASETANDMETRICS
The training of the visual tokenizer and auto-regressive transformer are both on ImageNet (Deng
etal.,2009). Specifically,wetrainthetokenizerin128Ë†128and256Ë†256resolutions.
Forvisualreconstruction,thereconstruction-FID,denotedasrFID(Heuseletal.,2017),codebook
utilization, the use percentage of codes, and PSNR on ImageNet 50k validation set are adopted to
measurethequalityofreconstructedimages. Simultaneously,wemeasurethequalityofimagegen-
erationbytheprevalentmetricsFID,IS(Salimansetal.,2016)andPrecision/Recall(KynkaÂ¨aÂ¨nniemi
etal.,2019).
3.2 IMPLEMENTATIONSDETAILS
Visual Tokenizer Setup. Open-MAGVIT2 follows the same architecture of the visual tokenizer
proposedin(Yuetal.,2024a). Forcomputationalefficiency, weremovethegradientpenaltyloss,
and adopt PatchGAN (Isola et al., 2017) as the discriminator instead of StyleGAN (Karras et al.,
2019). All models corresponding to different resolutions are trained with similar settings: an ini-
tial 1e Â´ 4 learning rate, an Adam Optimizer with Î² â€œ 0.5, Î² â€œ 0.9, a total 256 batch size
1 2
from 270 to 350 epochs, a combination of reconstruction, GAN, perceptual (Zhang et al., 2018),
entropypenalty(Yuetal.,2024a),commitmentlosses,LeCAMregularization(Tsengetal.,2021)
fortrainingstability,and32Ë†NvidiaV100/Ascend910BwithPytorch.
5TechnicalReport
Table2: ModeldesignsandreconstructionperformancecomparisonwiththeoriginalMAGVIT-v2
on128Ë†128ImageNet50kvalidationset,followingtheMAGVIT-v2paper.
Train Large Up/Down Deeper Adaptive
Method Tokens LFQ rFID
Resolution Codebook Sampler Model GroupNorm
Open-MAGVIT2 16Ë†16 128Ë†128 âœ“ âœ“ âœ“ âœ“ âœ“ 1.18
MAGVIT2(Yuetal.,2024a) 16Ë†16 128Ë†128 âœ“ âœ“ âœ“ âœ“ âœ“ 1.15
Table3: Reconstructionperformanceofdifferenttokenizerson256Ë†256ImageNet50kvalidation
set. Open-MAGVIT2achievesSOTAresultsondifferentdownsamplingrates. :specifiesthatthe
trainingisonOpenImages. Ëšdenotesthattheresultsarefromthedirectinferenceusingthemodel
trainedwith128Ë†128resolutionwithoutfine-tuning.
Token Train Codebook Codebook
Method Tokens Ratio rFIDÃ“ PSNRÃ’
Type Resolution Size UsageÃ’
VQGAN(Esseretal.,2021) 2D 16Ë†16 16 256Ë†256 1024 7.94 19.4 Â´
SD-VQGAN(Rombachetal.,2022a) 2D 16Ë†16 16 256Ë†256 16384 5.15 Â´ Â´
MaskGIT(Changetal.,2022) 2D 16Ë†16 16 256Ë†256 1024 2.28 Â´ Â´
LlamaGen(Sunetal.,2024) 2D 16Ë†16 16 256Ë†256 16384 2.19 20.79 97%
Open-MAGVIT2 2D 16Ë†16 16 256Ë†256 262144 1.17 21.90 100%
ViT-VQGAN(Yuetal.,2022) 2D 32Ë†32 8 256Ë†256 8192 1.28 Â´ Â´
VQGAN:(Esseretal.,2021) 2D 32Ë†32 8 256Ë†256 16384 1.19 23.38 Â´
SD-VQGAN:(Rombachetal.,2022a) 2D 32Ë†32 8 256Ë†256 16384 1.14 Â´ Â´
OmiTokenizer-VQ(Wangetal.,2024) 2D 32Ë†32 8 256Ë†256 8192 1.11 Â´ Â´
LlamaGen(Sunetal.,2024) 2D 32Ë†32 8 256Ë†256 16384 0.59 24.45 Â´
Open-MAGVIT2Ëš 2D 32Ë†32 8 128Ë†128 262144 0.34 26.19 100%
Titok-L(Yuetal.,2024b) 1D 32 Â´ 256Ë†256 4096 2.21 Â´ Â´
Titok-B(Yuetal.,2024b) 1D 64 Â´ 256Ë†256 4096 1.70 Â´ Â´
Titok-S(Yuetal.,2024b) 1D 128 Â´ 256Ë†256 4096 1.71 Â´ Â´
Auto-regressiveTransformerSetup. Asillustrated, weproposeasymmetrictokenfactorization
toassisttheauto-regressivetransformermodelsinmakingtheprecisepredictionwithalargecode-
book. Note that, we empirically set M â€œ 2 and k â€œ 6, k â€œ 12. Since our main focus is on
1 2
democratizing scalable auto-regressive visual generation, the plain auto-regressive transformer is
utilizedwhilethe techniquesthatintroduceinductive biassuchasAdaLn (Karrasetal.,2020) are
excluded.Specifically,weadopttheLlama-based(Touvronetal.,2023)architecture(e.g.,RoPE(Su
et al., 2024), SwiGLU (Shazeer, 2020), RMSNorm (Zhang et al., 2022) technique, each of which
hasbeenproveneffectivein(Sunetal.,2024)). Theclassembeddingwhichisindexedfromasetof
learnableembeddingsservesasthestarttoken. Open-MAGVIT2followsthesimplescalingprinci-
pleproposedin(Sunetal.,2024),whichisinTab.1. Allmodelsaretrainedwithsimilarsettings:
abaselearningrateof1eÂ´4per256batchsize,anAdamWoptimizerwithÎ² â€œ 0.9,Î² â€œ 0.95,
1 2
weightdecay=5eÂ´2,atotal768batchsizeand300â€350trainingepochs,gradientclippingof1.0,
0.1dropoutrateforinputembedding,FFNmoduleandconditionalembedding,32â€96Ë†Nvidia
V100/Ascend910BfordifferentscalesofthemodelwithPytorch.
3.3 MAINRESULTS
VisualReconstruction. AsshowninTab.2,byincorporatingallusefuldesignsproposedin(Yu
etal.,2024a),Open-MAGVIT2matchesMAGVIT-v2performanceswithmerely0.03FIDmargin
onImageNet128Ë†128. Further,wealsocompareourOpen-MAGVIT2withpreviousvisualtok-
enizersonImageNet256Ë†256inTab.3.Benefitingfromthesuper-largecodebookwithlookup-free
quantization,Open-MAGVIT2outperformsallpreviousimagetokenizersunderfairsettings.More-
over,weprovideanillustrativevisualcomparisoninFig.3. Asindicated,ourvisualtokenizergains
moresuperiorityindetailperceptionaswellasprecisefacialandtextreconstruction.
VisualGeneration. MAGVIT-v2leveragesthenon-autoregressiveframeworkforimagesynthesis
andachievescompetitiveperformance.Consideringthescalabilityofauto-regressivemodelsandthe
remarkable success of the auto-regressive paradigm in MLLM (Team, 2024), we instead focus on
exploringthepotentialofincorporatingasuper-largecodebookforauto-regressivevisualgeneration.
6TechnicalReport
Table4: Class-conditionalgenerationon256Ë†256ImageNet. Ëšspecifiesthegeneratedimagesare
384Ë†384andareresizedto256Ã—256forevaluation. Theevaluationprotocolandimplementation
arethesamewithADM.
Type Model #Para. FIDÃ“ ISÃ’ PrecisionÃ’ RecallÃ’
ADM(Dhariwal&Nichol,2021) 554M 10.94 101.0 0.69 0.63
CDM(Hoetal.,2022) Â´ 4.88 158.7 Â´ Â´
Diffusion
LDM-4(Rombachetal.,2022a) 400M 3.60 247.7 Â´ Â´
DiT-XL/2(Peebles&Xie,2023) 675M 2.27 278.2 0.83 0.57
VQGAN(Esseretal.,2021) 227M 18.65 80.4 0.78 0.26
VQGAN(Esseretal.,2021) 1.4B 15.78 74.3 Â´ Â´
VQGAN-re(Esseretal.,2021) 1.4B 5.20 280.3 Â´ Â´
AR ViT-VQGAN(Yuetal.,2022) 1.7B 4.17 175.1 Â´ Â´
ViT-VQGAN-re(Yuetal.,2022) 1.7B 3.04 227.4 Â´ Â´
RQTran.(Leeetal.,2022) 3.8B 7.55 134.0 Â´ Â´
RQTran.-re(Leeetal.,2022) 3.8B 3.80 323.7 Â´ Â´
VAR-d16(Tianetal.,2024) 310M 3.30 274.4 0.84 0.51
VAR-d20(Tianetal.,2024) 600M 2.57 302.6 0.83 0.56
VAR
VAR-d24(Tianetal.,2024) 1.0B 2.09 312.9 0.82 0.59
VAR-d30(Tianetal.,2024) 2.0B 1.92 323.1 0.82 0.59
LlamaGen-LËš (Sunetal.,2024) 343M 3.07 256.06 0.83 0.52
LlamaGen-XLËš (Sunetal.,2024) 775M 2.62 244.08 0.80 0.57
LlamaGen-XXLËš (Sunetal.,2024) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-L(Sunetal.,2024) 343M 3.80 248.28 0.83 0.51
LlamaGen-XL(Sunetal.,2024) 775M 3.39 227.08 0.81 0.54
AR LlamaGen-XXL(Sunetal.,2024) 1.4B 3.09 253.61 0.83 0.53
Open-MAGVIT2-B 343M 3.08 258.26 0.85 0.51
Open-MAGVIT2-L 804M 2.51 271.70 0.84 0.54
Open-MAGVIT2-XL 1.5B 2.33 271.77 0.84 0.54
As shown in Tab. 4, Open-MAGVIT2 outperforms all previous image generation models using a
plain auto-regressive approach. This benefits from the increased representational capacity of the
largescaleofthecodebook. However,webelievethatthestrengthofsuchalargecodebookisstill
underestimated because of the data bottleneck and the model size. We hope our effort in building
suchapowerfulvisualtokenizerhelpsmeritfutureresearchinunifiedMLLMforimagegeneration.
3.4 QUALITATIVERESULTS
WepresentthequalitativeresultsonImagenetBenchmarkintermsofvisualreconstruction(seein
Fig.4)andvisualgeneration(seeinFig.5),respectively.
4 RELATED WORKS
4.1 VISUALTOKENIZER
Visual tokenizer is to map an image into compact discrete tokens, which are subsequently fed
into the generative models for sequence modeling. Early pioneer VQVAE (Van Den Oord et al.,
2017)firstintroduceslearnablecodebookmechanismfor2Dtokensgeneration. Subsequently,ViT-
VQGAN(Yuetal.,2022)andRQ-VAE(Leeetal.,2022)improveVQVAEthroughnormalizedand
multi-scalequantizationrespectively. Recently,LlamaGen(Sunetal.,2024)reexaminesthedesign
of vanilla tokenizer (Esser et al., 2021) and reveals the conflict between the fidelity of the synthe-
sized image and the size of codebook. Therefore, following the simple intuition (Yu et al., 2022)
that reducing code dimension limits the representational capacity of individual tokens, MAGVIT-
2(Yuetal.,2024a)proposesanadvancedvisualtokenizerwhichsignificantlyenlargesthesizeof
codebookto218withLookup-FreeQuantization.
7TechnicalReport
4.2 VISUALGENERATION
Givenasetofcompactdiscreteimagetokens, thereexisttwoprevalentframeworksforthesubse-
quentimagesynthesis,includingNon-autoregressiveandAuto-regressivegeneration.
Non-autoregressive frameworks. MaskGIT (Chang et al., 2022) utilizes BERT-style trans-
former (Devlin et al., 2018) to parallelly generate all visual tokens via masked-prediction mech-
anism. MAGVIT (Yu et al., 2023; 2024a) adopts the same architecture but includes an additional
embeddingmaskforbettergenerationquality.
Auto-regressive frameworks. Autoregressive-based Multi-Modal Large Language Models (Liu
et al., 2024; Li et al., 2024) has achieved remarkable success in versatile visual understanding.
Incontrast,theprogressincounterpartvisualgenerationstillremainsunsatisfactory. Thesimplest
approachVQGAN(Esseretal.,2021)employstinyGPT2(Radfordetal.,2019)(â€300M)fornext-
tokenprediction.VAR(Tianetal.,2024)reformulatestheimagegenerationapproachintonext-scale
prediction and unveils the scaling principle simultaneously. Subsequently, LlamaGen (Sun et al.,
2024)extendsVQGANwithLlama(Touvronetal.,2023)architecture,showcasingsignificantim-
provement in fidelity. However, the limited codebook size (e.g., 214) in existing auto-regressive
models may incur the representational bottleneck. Therefore, considering that the capacity of the
visual tokenizer is highly correlated with the quality of visual synthesis (Yu et al., 2024a), we de-
mocratizetheplainauto-regressiveapproachwithasuper-largecodebook.
5 CONCLUSION
In this work, we re-implement the powerful visual tokenizer, which achieves state-of-the-art per-
formance compared with previous methods, and make it available to the community. Instead of
simplyfollowing(Yuetal.,2024a)thatleveragesmasked-generativetransformerforvisualgener-
ation, wedelve intoamorepromising manner(i.e., auto-regressivevisual synthesis). To excavate
the potential of the large vocabulary, we introduce the â€œnext sub-token predictionâ€ paradigm with
theasymmetrictokenfactorizationtechnique. Theexperimentsuggeststhatwiththepowerfultok-
enizer,theplainauto-regressivemodelexhibitssuperiorityandscalability.Wehopeourcontribution
totheopen-sourcecommunitycanfacilitatemoreinnovativeandcreativeworksinthefieldofauto-
regressivevisualgeneration,eventuallymakingadifferenceinbuildinganomnipotentmulti-modal
framework.
Limitationsandfuturework. Weexpectthattheeffectivenessofsuchasuper-largecodebook,
(i.e.,218codes),isstillunderestimatedduetothelimiteddatascaleandthesacrificeoftherepresen-
tationalcapacitywiththetokenfactorizationtechnique. Webelievethatbyamplifyingthetaskwith
more training data (e.g., text-conditional image generation, video generation, etc.), and enlarging
the model size to 7B or even larger, the potential of AR generation with a super-large codebook
canbedramaticallyexploited. Therefore,extendingOpen-MAGVIT2intomorebroadmulti-modal
generationapplicationswillbeahighpriorityinourfutureexploration.
ACKNOWLEDGMENTS
WesincerelythankLijunYuforhisencouragingdiscussionsandsupport. WealsothankTianheng
ChengandYuxinChenfortheirhelpfulsuggestionsonthisproject.
8TechnicalReport
(a)
(b)
(a)
(b)
(a)
(b)
Figure 4: Visualization of the Open-MAGVIT2 tokenizer. The upper part illustrates the model
trainedat128Ë†128resolutionandtestedat512Ë†512resolution. Thesecondpartshowcasesthe
tokenizertrainedat256Ë†256resolutionandtestedat256Ë†256resolution.(a)indicatestheoriginal
imageswhile(b)specifiesthereconstructionimages.
9TechnicalReport
Figure5: VisualizationofOpen-MAGVIT2auto-regressivegenerations. Class-conditionalgen-
erationonImageNet256Ë†256.
10TechnicalReport
REFERENCES
HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamT.Freeman.Maskgit:Maskedgenerative
imagetransformer. InCVPR,pp.11305â€“11315,2022. 2,6,8
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,2022. 2
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchicalimagedatabase. InCVPR,pp.248â€“255,2009. 2,5
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018. 8
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. NeurIPS,
34:8780â€“8794,2021. 7
PatrickEsser,RobinRombach,andBjoÂ¨rnOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InCVPR,pp.12873â€“12883,2021. 2,4,6,7,8
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,
volume30,2017. 5
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-
mans. Cascaded diffusion models for high fidelity image generation. JMLR, 23(1):2249â€“2281,
2022. 7
XunHuangandSergeJ.Belongie. Arbitrarystyletransferinreal-timewithadaptiveinstancenor-
malization. InICCV,pp.1510â€“1519,2017. 4
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with
conditionaladversarialnetworks. InCVPR,pp.5967â€“5976,2017. 5
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarialnetworks. InCVPR,pp.4401â€“4410,2019. 4,5
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimoAila. Analyz-
ingandimprovingtheimagequalityofstylegan. InCVPR,pp.8110â€“8119,2020. 6
DiederikPKingma. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,2013. 2
TuomasKynkaÂ¨aÂ¨nniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedpre-
cisionandrecallmetricforassessinggenerativemodels. InHannaM.Wallach,HugoLarochelle,
Alina Beygelzimer, Florence dâ€™AlcheÂ´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS,
pp.3929â€“3938,2019. 5
DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan. Autoregressiveimage
generationusingresidualquantization. InCVPR,pp.11513â€“11522,2022. 2,5,7
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024. 8
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024. 8
OpenAI. GPT-4technicalreport. arXivpreprintarXiv:2303.08774,2023. 2
William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pp.
4195â€“4205,2023. 4,7
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIBlog,2019. 8
11TechnicalReport
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,pp.10684â€“10695,2022a. 6,
7
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjoÂ¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,pp.10674â€“10685,2022b. 2
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improvedtechniquesfortraininggans. InNeurIPS,volume29,2016. 5
NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020. 6
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
hancedtransformerwithrotarypositionembedding,2024. 6
Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.
Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint
arXiv:2406.06525,2024. 2,4,6,7,8
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024. 6
KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang.Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024. 2,
7,8
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AureÂ´lien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXivpreprintarXiv:2307.09288,2023. 2,6,8
Hung-YuTseng,LuJiang,CeLiu,Ming-HsuanYang,andWeilongYang. Regularizinggenerative
adversarialnetworksunderlimiteddata. InCVPR,pp.7921â€“7931,2021. 5
AaronVanDenOord,OriolVinyals,etal.Neuraldiscreterepresentationlearning.volume30,2017.
2,3,7
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett(eds.),NeurIPS,pp.5998â€“6008,2017. 2
JunkeWang,YiJiang,ZehuanYuan,BinyuePeng,ZuxuanWu,andYu-GangJiang.Omnitokenizer:
Ajointimage-videotokenizerforvisualgeneration. arXivpreprintarXiv:2406.09399,2024. 6
JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,Yuanzhong
Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQ-
GAN. InICLR,2022. 2,6,7
Lijun Yu, Yong Cheng, Kihyuk Sohn, JoseÂ´ Lezama, Han Zhang, Huiwen Chang, Alexander G.
Hauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,andLuJiang. MAGVIT:maskedgenera-
tivevideotransformer. InCVPR,pp.10459â€“10469,2023. 8
12TechnicalReport
LijunYu,JoseLezama,NiteshBharadwajGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,
Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan
Yang,IrfanEssa,DavidARoss,andLuJiang. Languagemodelbeatsdiffusion-tokenizeriskey
tovisualgeneration. InICLR,2024a. 2,3,4,5,6,7,8
Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen.
Animageisworth32tokensforreconstructionandgeneration.arXivpreprintarXiv:2406.07550,
2024b. 6
RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,pp.586â€“595,2018. 5
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pherDewan, MonaT.Diab, XianLi, XiVictoriaLin, TodorMihaylov, MyleOtt, SamShleifer,
Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettle-
moyer. OPT:openpre-trainedtransformerlanguagemodels. arXivpreprintarXiv:2205.01068,
2022. 6
13