Retrieval-Enhanced Knowledge Editing for
Multi-Hop Question Answering in Language Models
YuchengShi QiaoyuTan XuanshengWu
UniversityofGeorgia NewYorkUniversity UniversityofGeorgia
Athens,Georgia,USA NewYork,USA Athens,Georgia,USA
yucheng.shi@uga.edu qiaoyu.tan@nyu.edu yd6eb@virginia.edu
ShaochenZhong KaixiongZhou NinghaoLiu
RiceUniversity NorthCarolinaStateUniversity UniversityofGeorgia
Houston,Texas,USA Raleigh,NorthCarolina,USA Athens,Georgia,USA
shaochen.zhong@rice.edu kzhou22@ncsu.edu ninghao.liu@uga.edu
ABSTRACT 1 INTRODUCTION
LargeLanguageModels(LLMs)haveshownproficiencyinquestion- Largelanguagemodels(LLMs)exhibitimpressiveperformancein
answeringtasksbutoftenstruggletointegratereal-timeknowledge question-answering(QA)tasks[25,31,33],yettheyfaceasignifi-
updates,leadingtopotentiallyoutdatedorinaccurateresponses. cantchallenge:theirstaticknowledgebasecannotbeeasilyupdated
Thisproblembecomesevenmorechallengingwhendealingwith inreal-time,leadingtotheriskofgeneratingoutdatedorincorrect
multi-hopquestionssincetheyrequireLLMstoupdateandinte- responses.Toovercomethisissue,modeleditinghasbeenproposed
gratemultipleknowledgepiecesrelevanttothequestions.Totackle forpre-trainedLLMstoaligntheiroutputwithup-to-dateknowl-
theproblem,weproposetheRetrieval-AugmentedmodelEditing edge[6].Previouseditingmethodshaveshowntobeeffectivein
(RAE)frameworktailoredformulti-hopquestionanswering.RAE updatinganswerstosingle-hopquestions[7,17,18].However,it
first retrieves edited facts and then refines the language model remainsachallengingtaskformodeleditingtohandlemulti-hop
throughin-contextlearning.Specifically,ourretrievalapproach, questions,whichiscrucialtoevaluatemachineâ€™scomprehension
basedonmutualinformationmaximization,leveragesthereasoning andreasoningabilities.
abilitiesofLLMstoidentifychainfactsthatnaÃ¯vesimilarity-based Answeringmulti-hopquestionsrequirestheintegrationofmul-
searchesmightmiss.Additionally,ourframeworkincorporatesa tiplepiecesofknowledge.Forinstance,toanswerthequestion
pruningstrategytoeliminateredundantinformationfromthere- "Whatisthenationalityoftheauthorofâ€˜HarryPotterâ€™?",wemust
trievedfacts,whichenhancestheeditingaccuracyandmitigates linktwopiecesofknowledge:"(HarryPotter,author,J.K.Rowling)"
thehallucinationproblem.Ourframeworkissupportedbytheoreti- and"(J.K.Rowling,citizenof,UnitedKingdom)",whichcollectively
caljustificationforitsfactretrievalefficacy.Finally,comprehensive constituteafactchain[46].Ifweconductacounterfactualediton
evaluationacrossvariousLLMsvalidatesRAEâ€™sabilityinproviding thefirstfact,i.e.,â€œJ.K.Rowling"isreplacedbyâ€œStephenKing",the
accurateanswerswithupdatedknowledge. subsequentknowledgemustbeadjustedaccordingly,resultingin
atotallydifferentfactchain:"(HarryPotter,author,StephenKing),
KEYWORDS
(StephenKing,citizenof,UnitedStates)."Here,weusecounterfac-
tualeditstosimulatereal-worldupdates.Successfulmodelediting
Modelediting,questionanswering,retrieval-augmentedgeneration
formulti-hopquestionsrequiresthattheeditedLLMsidentifyand
adopttheupdatedknowledgetoderivethefinalanswers.
ACMReferenceFormat: Unfortunately,existingeditingmethodsareoftenlimitedinhan-
YuchengShi,QiaoyuTan,XuanshengWu,ShaochenZhong,KaixiongZhou,
dlingmulti-hopquestions.First,methodsthataltermodelparame-
andNinghaoLiu.2018.Retrieval-EnhancedKnowledgeEditingforMulti-
ters,includingfine-tuning[47],locate-then-edit[17,18],andmeta-
HopQuestionAnsweringinLanguageModels.InProceedingsofMake
learning[19],sufferfromthecatastrophicforgettingissue[6,7,43],
suretoenterthecorrectconferencetitlefromyourrightsconfirmationemai
(Conferenceacronymâ€™XX).ACM,NewYork,NY,USA,11pages.https://doi. wherethepreviouslyencodedknowledgecouldbelostafterediting.
org/XXXXXXX.XXXXXXX Second,methodsthatdependontrainingauxiliarymodelsalsofall
shortinthesescenarios.Theauxiliarymodelsareusuallysmaller
languagemodels,whichlackthenecessaryreasoningcapabilityto
infercorrectanswers[20].Incontrast,athirdcategoryofmethods,
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor basedonRetrieval-AugmentedGeneration(RAG),modifiesmodel
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
outputsinamoreeffectivemanner[6,45,46].Thesemethodsinte-
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM grateupdatedknowledgedirectlyintothemodelprompt,guiding
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, LLMs through in-context learning [45]. RAG-based approaches
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. haveshownsignificantadvantagessincetheyareimmunetothe
Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY catastrophicforgettingproblem,andtheeditingprocesscanbecon-
Â©2018AssociationforComputingMachinery. ductedonthefly[9,34].Yet,theapplicationofRAGtomodelediting
ACMISBN978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/XXXXXXX.XXXXXXX withmulti-hopquestionsstillremainslargelyunderexplored.
4202
raM
82
]LC.sc[
1v13691.3042:viXraConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Shi,etal.
Original Knowledge: andcauseseriousâ€œhallucinationâ€problems[15,16],whereLLMs
[1](Harry Potter, author, J. K. Rowling) [2](Stephen King, citizen of, United States)
willgeneratefactuallyincorrectcontentbasedonnoiseinsteadof
[3](Misery, author, Stephen King) [4](Harry Potter, citizen of, United Kingdom)
relevantfacts.Thus,itiscrucialtoreducenoisebeforeapplying
Counterfactual Edit
retrievedfactstoprompts.
Edited Facts Memory:
[1](Harry Potter, author, Stephen King) [2](Stephen King, citizen of, Canada) To bridge the gap, we propose a novel Retrieval-Augmented
[3](Misery, author, Harry Harrison) [4](Harry Potter, citizen of, United States) model Editing(RAE) framework,wherewe firstretrieve edited
Single Hop Question 2-Hop Question factsandthenrefinethetargetmodelwiththesefactsthroughin-
Who is Harry Potterâ€™s author? What is the nationality of Harry Potterâ€™s author? contextlearning.Toaddressthefirstchallenge,weproposeMutual-
Top-2 Similarity Search information(MI)Maximizationforeditedfactsretrieval.Here,
MIquantifiesthesharedinformationbetweenthetargetquestion
Edited Edited
Memory [1] (Harry [4](Harry Memory andeditedfacts.EditedfactswithhigherMIaremorerelevant.
[3] (Misery, Potter, Potter,
author, Harry author, citizen of, However,directlycomputingMIischallenging.Thus,wedecom-
Target facts within Harrison) Stephen King) U Stn ai tt ee sd ) Target facts not within poseMIintoaseriesofconditionalprobabilities,andweutilizethe
retrieved Top-2: retrieved Top-2. next-wordpredictioncapabilitiesofpre-trainedLLMstoestimate
Retrieved facts: [1][3] Retrieved facts: [1][4]
Answer fact: [1] Target fact:[1][2] theseprobabilities.Inthisprocess,weusethetargetLLMâ€™scon-
[2](Stephen King, Missing fact: [2]
citizen of, Canada) textualunderstandingandreasoningabilitiestoidentifynecessary
NaÃ¯ve Retrieval
factsforsuccessfulmodelediting.Totacklethesecondchallenge,
Retrieval-Augmented Generation for Model Editing
weproposeUncertainty-basedRedundantFactsPruningby
utilizingLLMoutputconfidence.Specifically,itselectivelyretains
Edit Who is Harry Potterâ€™s author? J. K. Rowling
factsthatincreasetheLLMsâ€™confidenceinansweringeditedques-
Given fact [1][3], who is Harry Potterâ€™s author? Stephen King tionsanddiscardsirrelevantinformation.Finally,wetheoretically
Edit justifytheformulationofourretrievalobjective.Overall,ourmain
What is the nationality of Harry Potterâ€™s author? United Kingdom
contributionsarelistedbelow:
Given fact [1][4], what is the nationality of Harry
Potterâ€™s author? US Should be Canada â€¢ Weintroduceanovelfactretrievalapproachformulti-hopques-
tionsinmodelediting.Thisapproacheffectivelyharnessesthe
Figure 1: An example of the traditional similarity-based reasoningcapabilitiesofLLMstoretrievethemostrelevantmulti-
searchthatfailstoretrievethecorrectfactsforLLMediting. hopfactsforeachquestion.
â€¢ Weproposeaknowledgepruningstrategytoreducenoiseafter
theinitialretrieval,mitigatingthehallucinationproblem.Addi-
Furthermore,inpractice,modeleditingofteninvolveshandlinga tionally,weprovidetheoreticalanalysistojustifyourdesignfor
largevolumeofedits,whicharestoredasfacttripletsinamemory theretrievalobjective.
bank.Insuchcases,RAG-basedmulti-hopeditingisexpectedtore- â€¢ Weconductextensiveexperimentsonvariouslanguagemodelsin
trievethemostrelevantfactsforeachquestion.However,thistask differentsizestoverifytheeffectivenessofourproposedediting
ischallengingfortworeasons.First,extractingmulti-hopfacts method.Empiricalresultsdemonstratethesuperiorityofour
requirestheretrievertounderstandthecomplexconnections RAEframeworkcomparedtostate-of-artbaselines.
amongmulti-relationswithinthequestion.AnaÃ¯vedesignof
theretrieverisapplyingsimilarity-basedsearch[9,14,34]toobtain 2 PRELIMINARY:MODELEDITING
thetop-ð¾ factsthataremostsemanticallysimilartothequestion.
2.1 ModelEditingforSingle-hopQuestions
However,semanticsimilarityalonedoesnotguaranteethatthese
factscontainthenecessaryinformationtocorrectlyanswerthe In LLMs, a single model edit refers to updating a specific piece
offactualknowledge[18,19,45].Eachknowledgeisdefinedasa
question.WeillustratethisissuewithanexampleinFigure1.The
editedfactthatshouldberetrievedisâ€œ[2]â€:(StephenKing,citizenof,
tripletð›¿ := (â„Ž,ð‘Ÿ,ð‘¡),whereâ„Ž,ð‘Ÿ,andð‘¡ denotetheheadentity,the
relation, and the tail entity, respectively, such as (Harry Potter,
Canada),notâ€œ[4]â€:(HarryPotter,citizenof,UnitedStates).Thelatter
author,J.K.Rowling).Aneditisdefinedaschangingthetailentity
factâ€œ[4]â€isretrievedbecauseitcontainsâ€œHarryPotterâ€andâ€œcitizen
ofâ€thatresembleourquestionâ€œWhatisthenationalityofHarry ð‘¡ toanewentityð‘¡â€²,i.e.,ð›¿ â†’ð›¿â€² := (â„Ž,ð‘Ÿ,ð‘¡) â†’ (â„Ž,ð‘Ÿ,ð‘¡â€²),whereð›¿â€²
istheeditedknowledge.Letð‘ždenotethelanguagemodelâ€™sinput.
Potterâ€™sauthor?".Despiteitshighersimilarityscore,thefactâ€œ[4]â€
Thegoalofmodeleditingistomodifyatargetmodelð‘“ ,sothat
isactuallyirrelevanttothequestion.Therefore,effectiveretrieval ðœƒ
requiresadeepunderstandingofthequestion,acapabilitythat
thenewmodelð‘“ ðœƒâ€²producesanoutputð‘“ ðœƒâ€²(ð‘ž)thatisalignedwiththe
goesbeyondexistingsimilarity-basedsearchmethods.Second,the
newfactð›¿â€²,whereð‘“ ðœƒâ€²(ð‘ž)â‰ ð‘“ ðœƒ(ð‘ž).Specifically,givenð‘ž= [â„Ž;ð‘Ÿ],the
modelisexpectedtooutputð‘¡â€² =ð‘“â€²([â„Ž;ð‘Ÿ])ifâ„Ž,ð‘Ÿ âˆˆð›¿â€²,where[;]is
retrievedknowledgecouldcontainredundantinformation ðœƒ
anddegrademodeleditingperformance.Itisusuallyimprac- theconcatenationoperator.However,iftheinputquestionisnot
ticaltodeterminetheexactamountofinformationtoanswera relevanttotheedit,i.e.,â„Žâˆ‰ð›¿â€²orð‘Ÿ âˆ‰ð›¿â€²,themodelshouldoutput
specificquestion,soexistingretrievalmethodstendtoreturnan ð‘¡ =ð‘“â€²([â„Ž;ð‘Ÿ])thatreflectstheknowledgeofLLMsbeforeediting.
ðœƒ
extensivenumberoffacts[8,21,27,42]formorecomprehensive
coverage.Whilethesemethodsdoretrieverelevantfacts,theyalso 2.2 ModelEditingforMulti-hopQuestions
introduceredundantinformation.Itisworthnotingthatincorporat- Answering multi-hop questions presents a greater challenge. A
ingirrelevantknowledgeintotheLLMinputcouldmisleadmodels multi-hopquestionseekstoidentifyaspecifictailentityð‘¡ based
ð‘˜Retrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
Table1:Answeringa3-hopquestionð‘žwithmodelediting. 3 METHODOLOGY
Thepre-editedandeditedanswerareð‘¡5andð‘¡ 3âˆ—,respectively.
OurRetrieval-AugmentedEditing(RAE)framework,asshownin
ð‘¡5andð‘¡ 3âˆ—arethetailentityofð›¿5andð›¿ 3â€².ðº ð‘žandðº ð‘žâˆ—denotethe
Figure2,containstwokeysteps:(1)retrievingeditedfactsrelevant
pre-editedandeditedfactchain. tothetargetquestion,and(2)editingthelanguagemodelusing
theseretrievedfactsviain-contextlearning.Wewillfirstdiscuss
EditedFactBankÎ”andUneditedFacts step(2)withthemotivationofourdesigninthefollowingsection.
(ð›¿1â†’ð›¿ 1â€²) HarryPotter,author,J.K.Rowlingâ†’StephenKing #edit Thedetailsofstep(1)canbefoundinSections3.2and3.3.
(ð›¿2)StephenKing,citizenof,UnitedStates
(ð›¿3â†’ð›¿ 3â€²) UnitedStates,capital,Washington,D.C.â†’Boston #edit 3.1 Retrieval-AugmentedEditing
(ð›¿4)J.K.Rowling,citizenof,UnitedKingdom AnaÃ¯veapproachisusingsimilarity-basedsearchtoretrieveedited
(ð›¿5)UnitedKingdom,capital,London factssimilartotargetquestionð‘ž[9,34,46].Thesefactsarethen
integratedintoaprompttemplateforeditingviain-contextlearning:
A3-hopQuestionð‘ž
(ð‘ž) Which city is the capital of the country where the author of ð‘“ ðœƒâ€²(ð‘ž) = ð‘“ ðœƒ(ð‘‡ ð‘’(ð‘ž,{ð›¿ 1â€²,ð›¿ 2â€²,...ð›¿ ð¾â€² })),whereð‘‡ ð‘’ istheeditingtemplate.
HarryPotter heldcitizenship? Forexample,ð‘‡ ð‘’(Â·) canbemadeas"Givenfact: {ð›¿â€²}, {ð‘ž} ?".The
Top-ð¾ nearesteditedfactstoquestionð‘žintheembeddingspace
P ( (ð‘¡ ð‘¡r 5 â€²e ) )-e L Bd o oi n st tde ood nnAnswerð‘¡5andEditedAnswerð‘¡ 3â€² a sir med (Â·e )n do et ne od ta es st{ hð›¿ e1â€², sð›¿ im2â€², i. l. a.ð›¿ rið¾â€² ty} f= unT co tip o- nð¾ að›¿ nâˆˆ dÎ” ð‘”s ð‘§im is( að‘” nð‘§( eð›¿ m), bð‘” eð‘§ d( dð‘ž in)) g, mwh oder ee l.
3 However,editedfactsÎ” neededtoanswerð‘žarehardtoretrieve
ð‘ž
Pre-editedFactChainðºð‘ž bythisapproachsincetheyusuallycontainentitiesdifferentfrom
(ð›¿1)(HarryPotter,author, J.K.Rowling) ð‘ž,whichwillresultinalowsimilarityscoreinalargebankÎ”(e.g.,
(ð›¿4)(J.K.Rowling,citizenof, UnitedKingdom) inTable1, UnitedStates inð›¿ 3â€²,butnotinð‘ž).
(ð›¿5)(UnitedKingdom,capital, London) Toaddressthisproblem,weproposeeditedfactchainextrac-
EditedFactChainðº ð‘žâˆ—
t ei do gn et go rao pb htai (n KGðº )ð‘žâˆ—. [4In 6h ].e Sre un ct hly K,e Ga sch caðº nð‘žâˆ— bf eor rm ets ria evc eo dnn be yct ie ted rk atn ivo ew lyl-
(ð›¿ 1â€²)(HarryPotter,author, StephenKing) #editedfact traversinglinksfromoneentitytoanother.Takeðº ð‘žâˆ— ={ð›¿ 1â€²,ð›¿2,ð›¿ 3â€²}
(ð›¿2)(StephenKing,citizenof, UnitedStates) #uneditedfact inTable1asanexample.Itiscomposedoftwoeditedfactsð›¿â€²,ð›¿â€²
1 3
(ð›¿ 3â€²)(UnitedStates,capital,Boston) #editedfact andoneuneditedfactð›¿2.Wecanobservethatthequestionentity:
(HarryPotter )istheheadentityâ„Ž1 inð›¿ 1â€² = {â„Ž1,ð‘¡1,ð‘¡ 1â€²},andthe
editedtailentityð‘¡ 1â€²:(StephenKing )isalsotheheadentityâ„Ž2in
onasequenceoflinkedfacts:{(â„Ž1,ð‘Ÿ1,ð‘¡1),(â„Ž2,ð‘Ÿ2,ð‘¡2),...,(â„Ž ð‘˜,ð‘Ÿ ð‘˜,ð‘¡ ð‘˜)}, thenextfactð›¿2={â„Ž2,ð‘Ÿ2,ð‘¡2}.Moreover,foreachsubsequentfactin
whereeachtailentityistheheadentityofthenextfact:ð‘¡ ð‘– =â„Ž ð‘–+1. thechain,itsheadentityisalwaysthetailentityofthepreviousfact.
Eachinputquestionð‘žisassociatedwithasequencefactchainðº ð‘ž ByeffectivelyretrievingtheKGthatrepresentsthefactchainðº ð‘žâˆ—,we
forquestionanswering.Að‘˜-hopquestionisusuallyformulated areabletocapturealltheeditedfactualtripletsÎ”
ð‘ž
={ð›¿ ð‘–â€²,ð›¿â€² ð‘—,...ð›¿ ð‘˜â€²}.
usingonlytheinitialheadentityâ„Ž1,andaseriesofrelationships Inlightofthis,wedefineourretrieval-augmentededitingas:
i{ sð‘Ÿ1 s, hð‘Ÿ o2 w,.. n.,ð‘Ÿ inð‘˜} T. aA bn lee 1x .a Dm ip ffl ee reo nf tm fao cd te cl he ad ii nti sn ag refo ur sa ed3- th oo ap nsq wue es rti to hn
e
ð‘“ ðœƒâ€²(ð‘ž)=ð‘“ ðœƒ(ð‘‡ ð‘’(ð‘ž,ðº ð‘žâˆ—)), (1)
questionbeforeandafterediting.Onekeyobservationisthatfact wherewegiveanexampleofsucheditinginFigure2.Inthenextsec-
chainsrepresentconnectedknowledgegraphs,whereasingleen- tion,wewillintroducethedetailedstrategyofretrievingðºâˆ—,where
ð‘ž
tityisinvolvedintwoconsecutivefacts.Additionally,wenotice wefirstproposeamutualinformation-basedretrievalstrategyto
a"rippleeffect"inthesechains:Aneditinthefirstfactð›¿1 will extractfactsneededtoanswerthetargetquestion(Section3.2).
leadtochangesinthesubsequentfacts,forminganewchainðºâˆ—. Then,weproposeapruningmethodtodeleteirrelevantfactsfrom
ð‘ž
Inpracticalscenarios,editingisusuallyconductedinbatches,in- theinitialretrievalresult(Section3.3).
volvingmultiplefactchangessimultaneously,resultinginanedited
factbank,denotedasÎ” = {ð›¿â€²,ð›¿â€²,...,ð›¿â€² },whereð‘ istypicallya 3.2 EditedFactsRetrievalviaMaximizingMI
1 2 ð‘
largenumber[18].Locatingtherelevanteditedfactsforonespe- Wefirstconstructaknowledgegraphthatconnectsdifferentfacts.
cificquestionisnon-trivialduetothe"rippleeffect".Tocorrectly Then,weintroduceourproposedretrievalobjectiveofextracting
answermulti-hopquestionsinmodelediting,itiscrucialtoaddress relevantsubgraphsgiveninputquestions.
theretrievaltaskformallydefinedasfollows:
3.2.1 ExternalKnowledgeGraphforSubgraphRetrieval. According
Problem1(Retrieval-AugmentedEditing). Givenanedited toourpreviousdiscussion,weaimtoretrievethefactchainðº ð‘žâˆ—for
fact bank Î” = {ð›¿ 1â€²,ð›¿ 2â€²,...,ð›¿ ð‘â€² } with ð‘ instances and a multi-hop modelediting.However,itisworthnotingthatðº ð‘žâˆ—consistsofboth
questionð‘žwhoseanswerrequiresmodelediting,wewanttoretrieve editedanduneditedfacts,whereastheuneditedfactsdonotexist
its corresponding edited facts Î” ð‘ž = {ð›¿ ð‘–â€²,ð›¿â€² ð‘—,...,ð›¿ ð‘˜â€²}. The goal is to inoureditedfactbankÎ”bydefault.Toeffectivelyincorporateboth
ensuretheretrievedfactscontainalltheeditedfactsthatappearin typesoffactsintoourretrievalprocess,weproposeintegrating
factchainðº ð‘žâˆ—,i.e.,Î” ð‘ž âŠ†ðº ð‘žâˆ— andÎ”\Î” ð‘ž âŠ„ðº ð‘žâˆ—.Then,thesefactsare alleditedfactsintoanexternalknowledgegraphG.Byselecting
usedtorefinethetargetmodelð‘“ ðœƒ toconductmodelediting. acomprehensiveKGsuchasWikiData[32],thenewgraph Gâˆ—Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Shi,etal.
Misery, author, Stephen King Step 1.1:Knowledge Step 1.3: Redundant knowledge Pruning
insertion/edition Misery, author, Richard Dawkins
Edit
Ellie Kemper, citizen of, Croatia
Misery, author, Richard Dawkins Moscow, continent, Africa
Fact Reggie Miller, sport, basketball
Chain Richard Dawkins, citizen of, U.K. Edited Graph External Knowledge Graph Editing Memory Retrieved Facts Refined Facts
What is the nationality of the author of â€œMiseryâ€? Step 1.2: Mutual Information based retrieval Step 2: In-context learning for editing
Original fact TeE mdi pti ln ag te Question Misery, author, Richard Dawkins
Edited fact Richard Dawkins, citizen of, U.K.
Multi-hop question
United Kingdom (U.K.)
Subgraph with â€œMiseryâ€ in center
Figure2:Theoverallframeworkofourretrieval-augmentedin-contextmodeleditingmethod.
will encompass both unedited and edited facts. It complements theobjectiveas:
oureditedfactbankÎ”andconnectsdifferententities.Besides,the
externalknowledgegraphprovidesextrafactualknowledgethat
maxð‘(ð‘ž,ðº =ðº ð‘†)
log
ð‘(ð‘ž,ðº =ðº ð‘†)
. (5)
canenhancelanguagetooutputcorrectanswers. ðºð‘† ð‘(ðº =ðº ð‘†) 2 ð‘(ðº =ðº ð‘†)
Specifically,giventheeditsÎ”={ð›¿ 1â€²,...,ð›¿ ð‘›â€²}andanexternalG,we Inthefollowing,wewilldiscusshowtoestimateprobabilityð‘(ð‘ž,ðº =
considertwotypesofoperationstocombinethem.(1)Modifying ðº ð‘†)andð‘(ðº =ðº ð‘†)efficiently.
existingfacts:IftheoriginalfactappearsinG,i.e.,(â„Ž,ð‘Ÿ,ð‘¡) âˆˆ G,
wewillmodifytheKGaccordingtotheedits,soGâˆ— = (â„Ž,ð‘Ÿ,ð‘¡â€²)âˆª 3.2.3 ProbabilitiesEstimation. Weproposetocomputeprobabil-
itiesbyleveragingthenext-wordpredictioncapabilityofLLMs.
G\(â„Ž,ð‘Ÿ,ð‘¡).(2)Addingnewfacts:Iftheoriginalfactdoesnot
Giventhatthefactchainformsatail-to-headconnectedknowl-
appearinG,i.e.,(â„Ž,ð‘Ÿ,ð‘¡)âˆ‰G,thenweappendthemodifiedfactto
theKG,soGâˆ—=(â„Ž,ð‘Ÿ,ð‘¡â€²)âˆªG.Next,givenaquestionð‘ž,weretrieve edgegraph,ourextractedsubgraphðº ð‘† canberepresentedasðº ð‘† =
asubgraphðº ð‘† fromGâˆ—,sothatðº ð‘† âŠ‚ Gâˆ—.Ourgoalistoensure
(â„Ž1,ð‘Ÿ1,ð‘¡1,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›),whereâ„Ž
ð‘–
andð‘¡
ð‘–
arenodes,ð‘Ÿ
ð‘–
istheedge,
andð‘›isthenumberofretrievedtriplets.Thus,wecanestimate
thatðº
ð‘†
containsfactchainscorrespondingtoð‘ž,i.e.,ðº ð‘žâˆ— âŠ†ðº ð‘†.
ð‘(ð‘ž,ðº=ðºð‘†)
as:
ð‘(ðº=ðºð‘†)
3.2.2 MutualInformationbasedRetrievalObjective. Foreffective
editing,theretrievedsubgraphðº ð‘† mustsharerelevantinformation ð‘(ð‘ž,ðº =ðº ð‘†) = ð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|ð‘ž,â„Ž1) Â·ð‘(ð‘ž,â„Ž1) . (6)
withthequestion.Therefore,wedefinetheobjectiveofsubgraph ð‘(ðº =ðº ð‘†) ð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|â„Ž1) ð‘(â„Ž1)
retrievalasmaximizingthemutualinformation(MI)betweenthe Specifically,forthetermð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1),wecanfurther
subgraph and a set of questionsð‘„ whose answers require edit- decomposeitintofollowingform:
ing.Theobjectiveisformalizedasbelow,wherethetheoretical
justificationisprovidedinSection3.4:
ð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|ð‘ž,â„Ž1)
(7)
=ð‘(ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|ð‘ž,â„Ž1,ð‘Ÿ1)Â·ð‘(ð‘Ÿ1|ð‘ž,â„Ž1).
m ðºa ð‘†xð¼(ð‘„,ðº ð‘†)=ð»(ð‘„)âˆ’ð»(ð‘„ |ðº =ðº ð‘†). (2)
Thisdecompositionallowsustoinitiallyfocusonestimatingthe
ð‘(ð‘Ÿ1|ð‘ž,â„Ž1).Specifically,theheadentityâ„Ž1isdeterminedifð‘žisgiven,
Givenafixedquestionsetð‘„,itsShannonentropyð»(ð‘„)remains sinceweassumeâ„Ž1ismentionedinquestionð‘ž.Candidaterelations
constant.Therefore,maximizingthemutualinformationð¼(ð‘„,ðº ð‘†) forð‘Ÿ1canalsobeselectedfromtheeditedKG.Practically,wecan
isequivalenttominimizingtheconditionalentropyð»(ð‘„ |ðº =ðº ð‘†). estimatetheprobabilityð‘(ð‘Ÿ1|ð‘ž,â„Ž1)foreachcandidaterelationusing
Thus,wefocusonoptimizingthefollowingobjective: anauto-regressivelanguagemodelð‘“ [25,38]:
ðœ™
maxð¼(ð‘„,ðº ð‘†)=minð»(ð‘„ |ðº =ðº ð‘†) (3)
ð‘(ð‘Ÿ1|ð‘ž,â„Ž1)â‰ˆ
ðºð‘† ðºð‘† |ð‘Ÿ1|
=m ðºa ð‘†x ð‘žâˆ‘ï¸ âˆˆð‘„ð‘(ð‘ž|ðº =ðº ð‘†)log 2ð‘(ð‘ž|ðº =ðº ð‘†). (4) (cid:214) ð‘–=1ð‘“ ðœ™(ð‘¤ ð‘Ÿ( 1ð‘–) |ð‘¤ ð‘ž(1),...,ð‘¤ ð‘ž(|ð‘ž|),ð‘¤ â„Ž(1 1),...,ð‘¤ â„Ž( 1|â„Ž1|),ð‘¤ ð‘Ÿ( 11),...,ð‘¤ ð‘Ÿ( 1ð‘–âˆ’1) ),
(8)
Inpractice,quantifyingð‘(ð‘ž|ðº =ðº ð‘†)posesasignificantchallenge
whereð‘“ isthepredictedwordprobability,andð‘¤ ,ð‘¤ ,ð‘¤ denote
duetoitscomputationalcomplexity.Thiscomplexityarisesasthere
ðœ™ ð‘ž â„Ž1 ð‘Ÿ1
wordsinthequestionð‘ž,headentityâ„Ž1,andrelationð‘Ÿ1,respectively.
arenumeroussubgraphcandidatesðº withintheentireknowledge
ð‘† Wecanemployopen-sourceLLMslikeGPT-2[25]forthisestima-
graph,makingitprohibitivelyexpensivetoexhaustivelysearchfor
tion.Pleasenotethat,themodelð‘“ beingediteddoesnotneedto
ðœƒ
theoptimalsubgraph.Tocircumventthisissue,wefirstreplacethe
bethesamemodelusedforprobabilityestimation,makingour
intractabletermð‘(ð‘ž|ðº =ðº ð‘†)with ð‘ ð‘(ð‘ž (ðº,ðº == ðºðº ð‘†ð‘† )).Then,supposewe methodapplicableevenforeditingproprietaryLLMs.Withaspe-
consideronequestioneachtime,whereð‘„ =ð‘ž,wecanreformulate cificinputcontext{ð‘ž,â„Ž1},thelanguagemodelwillassigndifferentRetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
probabilitiestoeachrelationbasedonitscontextualunderstanding 3.3.1 EditingUncertainty. Wedefineeditinguncertaintyasthe
andreasoningability. uncertainty of the output generated by large language models.
Then,ð‘(ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1)canbefurtherdecomposeinto Formally,theoutputuncertaintyisquantifiedbyShannonentropy:
ð‘(â„Ž2,ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1) Â·ð‘(ð‘¡1|ð‘ž,â„Ž1,ð‘Ÿ1). In our case, we assume
ð‘(ð‘¡1|ð‘ž,â„Ž1,ð‘Ÿ1) =1,sinceonerelationusuallyonlycorrespondsto ð»(ð‘Œ|ð‘‹ =ð‘¥)=âˆ’âˆ‘ï¸ ð‘(ð‘¦|ð‘¥)log 2(ð‘(ð‘¦|ð‘¥)), (11)
onetailentity.Whentherearemultipletailentities,wefindtheas-
ð‘¦
sumptionstillworkswellempirically.So,ð‘(â„Ž2,ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1)
canbedecomposedintoð‘(ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1,â„Ž2)Â·ð‘(â„Ž2|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1). whereð‘¦representseachpossibleanswergeneratedbythelanguage
Additionally,sincethetailentityinonefactbecomestheheadentity model,andð‘¥ ={ð‘ž,ðº ð‘†}isthemodelinputcomposedoftheques-
inthesubsequentfact,wecanalsohaveð‘(â„Ž2|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1)=1.Thus, tionð‘žandfactsðº ð‘†.Ahigherentropyvalueð»(ð‘Œ|ð‘‹ =ð‘¥)signifies
wecanhaveð‘(ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1) =ð‘(ð‘Ÿ2,ð‘¡2,...|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1,â„Ž2). lessconfidenceintheanswer,reflectinggreateruncertainty.Con-
Thisisanicepropertythathelpsusiterativelydecomposethis versely,alowerentropyvalueindicateshigherconfidenceandless
intractableprobabilityterm.Byiterativelyapplyingtheaforemen- uncertainty.Ideally,ifinputfactsðº areexactlytheeditedques-
ð‘†
tionedstepforð‘›times,wecancomputetheconditionalprobability tionfactchainðº ð‘žâˆ—,i.e.,ðº ð‘† = ðº ð‘žâˆ—,thenthemodeloutputshould
ofallsubgraphswithinanð‘›-hopdistancefromthequestionentity. exhibitmaximumconfidencewithminimalentropy,sinceðºâˆ—con-
ð‘ž
Thefinalestimationcanbeexpressedas: tainsthepreciseknowledgetoanswerquestionð‘ž.Inthenextpart,
weconductempiricalexperimentstoverifythisassumption.
ð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|ð‘ž,â„Ž1)
Inourexperiments,GPT-J(6B)[33]ischosenasthebaselan-
=ð‘(ð‘Ÿ ð‘›|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1,...,â„Ž ð‘›âˆ’1,ð‘Ÿ ð‘›âˆ’1,ð‘¡ ð‘›âˆ’1,â„Ž ð‘›)Â· guage model. We select 1000 instances for each of 2, 3, and 4-
ð‘(ð‘Ÿ ð‘›âˆ’1|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1,...,â„Ž ð‘›âˆ’2,ð‘Ÿ ð‘›âˆ’2,ð‘¡ ð‘›âˆ’2,â„Ž ð‘›âˆ’1)Â· (9) hopquestionsfromtheMQUAKE-CFdataset[46]fortesting.The
...Â· MQUAKE-CFdatasetcomprisesmulti-hopquestionsthatarebased
onreal-worldfacts,wheretheeditedfactsarecounterfactual,mean-
ð‘(ð‘Ÿ2|ð‘ž,â„Ž1,ð‘Ÿ1,ð‘¡1,â„Ž2)Â·ð‘(ð‘Ÿ1|ð‘ž,â„Ž1).
ingtheydonotexistinactualreal-worldscenarios.Anexampleof
suchaquestionwithaneditisprovidedinTable1.
Tillnow,wehavedecomposedð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,...|ð‘ž,â„Ž1)inEquation(6)
Ourexperimentseekstoidentifythefactsetðºâ€² that,whenused
intotheproductofconditionalprobabilitiesofpredictingdifferent ð‘†
asmodelinput,yieldsthelowestoutputentropy,indicatingmini-
relationswithintheð‘›-hopsubgraph.Thisnicepropertyensures
maleditinguncertainty.Ourfirststepistoconstructdifferentfact
theselectionofthesubgraphwillonlybedeterminedbyrelation
probability,whichisfreefromtheinterferenceofanypotential
setcandidates.Webeginwiththefirstfactð›¿1inthefactchainðº ð‘žâˆ—
editedtailentity.Similarly,wecandecomposethedenominator asourinitialfactsetðº ð‘†â€².Then,weaddeachsubsequentfactfrom
termð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,...|â„Ž1)into: thechainuntiltheðº ð‘†â€² encompassestheentirefactchainðº ð‘žâˆ—.After
that,weintroduceunrelatedfactsð›¿Ë†intotheset.Thisprocessis
ð‘(ð‘Ÿ1,ð‘¡1,â„Ž2,ð‘Ÿ2,ð‘¡2,...,â„Ž ð‘›,ð‘Ÿ ð‘›,ð‘¡ ð‘›|â„Ž1) repeateduntilðºâ€² containssixelements.Finally,webuildaprefix
ð‘†
=ð‘(ð‘Ÿ ð‘›|â„Ž1,ð‘Ÿ1,ð‘¡1,...,â„Ž ð‘›âˆ’1,ð‘Ÿ ð‘›âˆ’1,ð‘¡ ð‘›âˆ’1,â„Ž ð‘›)Â· (10) setðºÂ¯ ð‘žwithallthesixsubsetsðº ð‘†â€².Specifically,fora4-hopquestion,
ð‘(ð‘Ÿ ð‘›âˆ’1|â„Ž1,ð‘Ÿ1,ð‘¡1,...,â„Ž ð‘›âˆ’2,ð‘Ÿ ð‘›âˆ’2,ð‘¡ ð‘›âˆ’2,â„Ž ð‘›âˆ’1)Â·...Â·ð‘(ð‘Ÿ1|â„Ž1). wehaveðºÂ¯ ð‘ž = {{ð›¿1},{ð›¿1,ð›¿2},{ð›¿1,ð›¿2,ð›¿3},...,{ð›¿1,ð›¿2,ð›¿3,ð›¿4,ð›¿Ë† 5,ð›¿Ë† 6}},
Then,forthelasttermð‘(ð‘ž,â„Ž1)/ð‘(â„Ž1) inEquation(6),basedon
w wehe cr oe nð›¿ d1 u, cð›¿ t2 i, nð›¿ -3 c, oð›¿ n4 teâˆˆ xðº teð‘žâˆ— da itn ind gð›¿Ë† 5 u, sð›¿ iË† n6 gar ee act hwo sui brr se el te ðºva â€²n ft rofa mct ts h. eFi pn ra el fily x,
wBa hy ie cs hâ€™ it sh aeo cr oe nm s, taw ne tc va an lut era gn is vf eo nrm asit pi en ct io ficð‘ q(ð‘ž u, eâ„Ž s1 t) io/ nð‘( ð‘žâ„Ž .1 W)= ecð‘ a( nð‘ž| aâ„Ž l1 so), setðºÂ¯
ð‘ž
witheditingtemplateð‘‡ ð‘’:"Givenfact:{ðº ð‘†â€²},ð‘† {ð‘ž}?".Inourex-
periments,forcomputationalsimplicity,weconsidermodeloutput
applymodelð‘“ toestimatethisconditionalprobability.Now,since
ðœ™ ð‘¦tobeeachofthenextpredictedword.Wereporttheentropyover
weareabletoestimateeveryterminEquation(6)and(5),wecan
allthewordsinthevocabularyastheeditinguncertainty.
effectivelyidentifythesubgraphthatyieldsthemaximumMutual
TheeditinguncertaintywithdifferentsubsetsislistedinFig-
Information.Additionally,weutilizethebeamsearchtechnique[26]
ure3a.Forcomparison,wealsoreporttheeditinguncertaintywith
toexpeditethecomputationalprocess,eliminatingthenecessityfor
randomfactsselectedfromWikidata,asshowninFigure3b.Our
exhaustivelytraversingallconnectednodes.Furtherdetailsonour
observationsrevealaphenomenon:thelanguagemodelproduces
implementationsareprovidedintheAppendixB.Inthiswork,we
answerswithmuchlowerentropywhenðºâ€² isequaltotheground-
treatð‘›asahyperparametersincethenumberofhopsrequiredto ð‘†
truthfactchainðºâˆ—.Ifðºâ€² presentsredundantfactsorinsufficient
answeraquestionisunknowninadvance.Toensurethoroughex- ð‘ž ð‘†
facts,theentropywillincrease.Meanwhile,iftheLLMisfedwith
ploration,ð‘›isassignedalargevalue,enablinganextensivesearch
randomfacts,theentropylevelalsoremainsconsistentlyhigh.This
range.However,thisapproachwillalsointroduceirrelevantinfor-
observationjustifiestheuseofentropyastheindicatorofwhether
mationintheretrievedsubgraphðº ,whichcanpotentiallymislead
ð‘† ðºâ€² containsthecorrectfactsforLLMinference.
thelanguagemodeltogenerateundesiredanswers[15,16].Thus, ð‘†
inthenextsection,wewilldiscusshowtomitigatethisproblem.
3.3.2 KnowledgePruningwithEditingUncertainty. Sinceincorpo-
ratingthemostrelevantfactswillresultinthelowestentropy,we
3.3 Uncertainty-basedRedundantFactPruning proposetoutilizethisfindingforknowledgepruning.Specifically,
Thissectionintroducesapruningmethod,whichutilizesmodel wefirstfollowSection3.2toretrieveaknowledgegraphðº con-
ð‘†
outputuncertainty,toeliminateredundantfactsfromðº ð‘†. tainingð‘›tripletsforquestionð‘ž:ðº
ð‘†
={ð›¿1,ð›¿2,...,ð›¿ ð‘›},whereð‘›isaConferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Shi,etal.
Subset1 Subset2 Subset3 Subset4 Subset5 Subset6 Motivatedbytheaboveanalysis,asin-contextpromptð‘†isthe
edited knowledge in our design, we seek to include the edited
1 knowledgethatsharesthesamelatentconceptðœƒ asquestionð‘ž.
ð‘
Ideally,thiswillactivatein-contextlearningforeffectivemodel
0.5 editing.Formally,wecandefinesuchknowledgegraphas
ðº
ð‘†
=argmaxð¼(ðº;ðœƒ ð‘), (13)
2-hops 3-hops 4-hops ðºâˆˆG
(a)Factchainðº ð‘žâˆ— withredundantknowledge. whereðœƒ ð‘ isthelatentconceptusedtogeneratequestionð‘ž,andwe
usemutualinformationð¼(ðº;ðœƒ ð‘)toquantifytheshareinformation.
However,thisisanon-trivialtasksinceconceptðœƒ isanintractable 1 ð‘
hiddenvariable.Toaddressthisissue,weproposeobtainingthe
targetknowledgegraphthatmaximizesthelowerboundofsuch
0.5
anobjective.Specifically,wecanhavethefollowingtheorem:
2-hops 3-hops 4-hops Theorem1. Givenretrievedgraphðº ð‘† âˆˆG,thelatentconceptðœƒ ð‘,
andthequestionð‘žsampledconditionedonconceptðœƒ ð‘,thereexistsa
(b)Randomfactchainwithoutusefulknowledge. mutualinformationinequality:
Figure3:Distributionofnormalizedmodeleditingentropy ð¼(ðº ð‘†;ðœƒ ð‘) â‰¥ð¼(ðº ð‘†;ð‘ž). (14)
withdifferentfactsubsetsasinput.Alowernormalizeden-
Theorem1showsthatwecanmaximizethemutualinformation
tropy on a given subset indicates that the model is more
betweentheselectedknowledgegraphðº andquestionconceptsðœƒ
confident in answering the question with the given facts. ð‘† ð‘
Subset1includesthefirstfact{ð›¿1},Subset2includesthefirst bymaximizingthemutualinformationbetweentheselectedgraph
twofacts{ð›¿1,ð›¿2},andsoon.Figure3ashowsthattheentropy ðº ð‘† andthequestionð‘žitself.Inthisway,thein-contextlearning
abilityofLLMswouldbeeffectivelytriggered.Whenweapplysuch
issignificantlylowerifthesubsetcontainsexactlytheentire
knowledgeastheprompt,wecaneffectivelyconductthein-context
factchainofthequestion(e.g.,Subset2for2-hopquestions).
editing.TheproofofTheorem1isinAppendixA.
sufficientlylargenumber,andðº ð‘† couldcontainredundantknowl- 4 EXPERIMENTS
edge.Then,toremoveredundantknowledge,wefirstbuildthe
prefixsetsðºÂ¯ fortargetquestionð‘žbasedontheretrievedgraph
Weconductexperimentstoanswerthefollowingquestions.Q1:
ð‘ž DoesRAEsuccessfullyeditmodeloutput?Q2:Howdoesourre-
ðº .Then,wecanobtaintheprunedfactsetðºâˆ—usingtheobjective:
ð‘† ð‘† trievalstrategyperformcomparedwithotherretrievalmethods?
ðº ð‘†âˆ— =argminâˆ’âˆ‘ï¸ ð‘(ð‘¦|ð‘‡ ð‘’(ð‘ž,ðº ð‘†â€²))log 2(ð‘(ð‘¦|ð‘‡ ð‘’(ð‘ž,ðº ð‘†â€²))). (12) Q3:Doesourproposedpruningtechniqueremoveredundantfacts
ðº ð‘†â€²âˆˆðºÂ¯ ð‘ž ð‘¦ fromtheretrievedfacts?Q4:DoesRAEworkforproprietyLLMs?
Finally, we can applyðºâˆ— as our retrieved fact chain for the in-
ð‘† 4.1 ExperimentSettings
contextlearningintroducedinSection3.1toconductediting.
4.1.1 LanguageModels. WeevaluateRAEacrossvariouskindsof
languagemodelsindifferentsizesandfamilies,includingGPT-2
3.4 TheoreticalJustification
(1.3B)[25],GPT-J(6B)[33],Falcon(7B)[1],Vicuna(7B)[4],and
Inthissubsection,wetheoreticallyjustifythatthefactscollected
Llama2-chat(7B)[30].Amongthem,GPT-2,GPT-J,andFalconare
byourretrievalobjective(Eq.(2))areeffectiveinperformingmodel
pre-trainedlanguagemodelswithoutinstructiontuning[5,24],
editingwithin-contextlearning.Tobeginwith,wediscusswhat
whileVicunaisaninstruction-tunedvariationofLlama1[31]and
kindsofinputcaneffectivelyactivatein-contextlearning.Then,
Llama2-chatistheinstruction-tunedversionofLlama2.Instruction-
weexplorehowtobuildsucheffectiveinputformodelediting.
tunedmodels(VicunaandLlama2-chat)areexpectedtobetterfol-
Ourproposededitingmethodreliesonthein-contextlearning
lowtheinstructionsinthepromptcomparedtonativepre-trained
abilityofLLMs.Inthefollowing,weprovideananalysisofhow
models(GPT-2,GPT-J,andFalcon).Weincludebothkindsofmodels
in-contextlearningcanbeeffectivelytriggered.Theoretically,the
toverifytheeffectivenessoftheproposedmethods.
textgenerationprocessofalanguagemodelcanbeunderstood
asaHiddenMarkovModel[2,39].Themodelinitiallyselectsa 4.1.2 EditingBaselines. Forcomparison,weconsiderthreekinds
conceptðœƒ ð‘ âˆˆ Î˜ from a set of underlying concepts denoted as of model editing methods: (1) Model weight updating methods:
Î˜, and then samples a sequence of words based on the chosen Fine-tuning[47]editstheentiremodelweightsbylanguagemod-
concept.Basedonthat,thein-contextlearningcanbewrittenas eling the edited knowledge. ROME [17] and MEMIT [18] focus
ð‘(ð‘¦|ð‘†,ð‘¥) = âˆ« ðœƒð‘âˆˆÎ˜ð‘(ð‘¦|ð‘†,ð‘¥,ðœƒ ð‘)ð‘(ðœƒ ð‘|ð‘†,ð‘¥)ð‘‘ðœƒ ð‘, whereð‘† denotes in- onidentifyingandupdatingparticularneuronsassociatedwith
contextpromptandð‘¥ denotesquery.Existingresearchhastheo- theknowledgethatneedsediting.(2)Auxiliarymodelsmethods:
reticallyproventhattheconditiontoactivatein-contextlearning SEARC[20]trainsanextralanguagemodeltostoreupdatedknowl-
iswhenthereisasharedlatentconceptðœƒ betweenprompttextð‘† edge,anditswitchestotheauxiliarymodelwhenansweringques-
ð‘
andtheinputqueryð‘¥.Morediscussionscanbefoundin[39]. tionsrelevanttotheeditedfacts.Detailsabouttheauxiliarymodels
ledoMdezilamroN
ledoMdezilamroN
yportnEtidE
yportnEtidERetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
fordifferentlanguagemodelsareincludedinAppendixB.(3)In- â€¢ QuestionReform[21,41]appendstheretrievedentitytothe
context learning-based method: Mello [46] edits model outputs previousqueryforthenexthopretrieval.Thisdesignismotivated
withmulti-roundconversations. bysimulatingareasoningpathwheretheretrievedfactateach
hopisviewedasanintermediatereasoningresult.
4.1.3 Implementation Details. We evaluate our editing method â€¢ QuestionDecompose,proposedbyMello[46],decomposes
ontheMQUAKE-CFandMQUAKE-Tdatasets[46],comprising amulti-hopquestionintomultiplesing-hopquestions.Specif-
1000counterfactualeditinginstancesper2-hop,3-hop,and4-hop ically,acomprehensiveconversational-stylepromptisapplied
questionsinMQUAKE-CF,andtotal1868editinginstancesfor2- tointeractwiththeLLMforcollectingeachsingle-hopquestion.
hopand3-hopquestionsinMQUAKE-T.Inoursetting,weedit Foreachsingle-hopquestion,thecorrespondingknowledgeis
allthesefactsatthesametime.Followingpreviouswork[46],we retrievedwiththenaÃ¯vemethoddescribedinKGLink.
leveragecasesfromtheMQUAKE-CF-9kdatasettocraftprompt (2)Incontrastwiththeembedding-basedmethodsthatestimatethe
templatesforbothbaselinesandourmethod.Aneditisconsidered similaritiesbetweenthequeryandthefactswiththedotproducts
successfuliftheeditedLLMcorrectlypredictstheupdatedanswer oftheirembeddings,probability-basedmethodsleveragelanguage
withinthefirsttengeneratedtokens.Thiscriterionisreportedas modelstodirectlyestimatethissimilarity.
editedaccuracyinTable2.MoresettingdetailsareinAppendix.B. â€¢ MaxProb[29,44]retrievesthesubgraphðº ð‘† âˆˆ Gâˆ— thatmaxi-
mizestheconditionalprobabilityð‘(ðº ð‘†|ð‘ž),whereð‘žistheinput
multiplequestion.Inourexperiments,weextractað¾-hopsub-
4.2 EditingPerformanceEvaluation
graphfromtheeditedknowledgegraph Gâˆ— andestimatethe
ToanswerQ1,weassessourmodeleditingmethodacrossvarious
probabilitywithalanguagemodelð‘“ asshowninSection3.2.3.
languagemodels,comparedagainstdifferentbaselinemethods.We ðœ™
focusontheeditedaccuracyofthesemodelsinansweringmulti- 4.3.2 ImplementationDetails. Weselect300casesforeachofthe2,
hopquestionswitheditedanswers.Forreferences,wealsoreport 3,and4-hopproblemsfromtheMQUAKE-CFdataset.Foreachtype,
theaccuracyofun-uneditedmodelsinpredictingbothoriginal wereporttwomatchingaccuracyscoresinTable3.Specifically,
andeditedanswers.Ourkeyobservationsare:(1)OurRAEout- withintheretrievedfacts,ifanyfactalsoappearsinthefactchainof
performsallothersinbothdatasetsacrossfivelanguagemodels, amulti-hopquestion,wenameitapartialmatch(PM).Andifevery
achievinganaverageimprovementof40.57%overthesecond-best retrievedfactmatchesthefactchainforthemulti-hopquestion,
method(Mello)whenconductingthousandsofeditsatthesame wenameitexactmatch(EM).
time.Thissuperiorperformanceprimarilystemsfromournovel
MI-basedretrievalobjectiveandaneffectivepruningstrategy.Our 4.3.3 Results. Wehavethefollowingobservations:(1)Ourpro-
designcanalsoseamlesslyintegrateanexternalknowledgegraph, posedmutualinformation-basedretrievalmethoddemonstrates
whicheffectivelylinksalleditedfacts,therebyfacilitatingthemulti- excellentperformanceacrossvariousLLMsinmulti-hopfactex-
hopeditingprocess.(2)Mellodemonstratesgoodperformanceon traction.Wealsofinditssuccesswithrelativelysmalllanguage
theMQUAKE-Tdatasetwithmodelslargerthan6B.However,it models,suchasGPT-2,showingstronggeneralizationability.(2)
underperformsontheMQUAKE-CFdatasetandfailswithGPT-2, Incontrast,traditionalembedding-basedmethods(KGLinkand
likelyduetoGPT-2â€™sinabilitytofollowMelloâ€™scomplexprompts, QuestionReform)underperforminthismulti-hopfactretrieval
resultinginnooutput.(3)Othermethodsgenerallyshowlower challenge.Theirlimitationmainlyliesinfailingtocomprehendthe
performanceacrossalllanguagemodels,whichalignswiththe complexinterplaybetweenmultiplerelationsinaquestion.Thus,it
findingsfromMQUAKEdatasetpaper[46]. hinderstheextractionoftargetfactsfromtheextensiveknowledge
base.(3)Mellodemonstratestheefficacyofdecomposingmulti-
hopquestionsintosingle-hopquestionsforthismulti-hopfacts
4.3 RetrievalPerformanceEvaluation
retrievaltask.Notably,itsEMperformancedropssignificantlywith
ToanswerQ2,weassesstheeffectivenessofourmutualinformation-
anincreasingnumberofhops,probablybecauseitbecomesmuch
basedretrievalmethodformulti-hopquestion-answeringtasks.
morechallengingforlanguagemodelstoperformquestiondecom-
4.3.1 RetrievalBaselines. Weconsidertwotypesofretrievalmeth-
posing.(4)Probabilitymaximization-basedmethodsoutperform
traditionalembeddingmethods,whilethereisstillagapcompared
odsasbaselines,namelytheembedding-basedandtheprobability-
toours,probablybecausethepredictedmostprobablefactmaynot
basedmethods.Weintroducetheirdetailsasfollows.
bethemostnecessaryoneforansweringaspecificquestion.
(1)Embedding-basedretrievalmethodsmostlyconcentrateontext
retrievalratherthantripletretrieval.Forafaircomparison,weadapt
themtofitourtaskbyemployingtheContriever[11]toencode 4.4 AblationStudiesonPruningStrategy
alleditedfactsintoembeddings,andthencachetheseembeddings To answer Q3, we verify that our proposed pruning strategies
forsimilaritysearch.Differentretrievalmethodsusedistinctstrate- arebeneficialtomulti-hopeditingtasks.Tosimulatethesituation
giestomodifytheoriginalð¾-hopsquestionsasthefinalqueryto wheretheretrievedfactscontainredundantinformation,wecon-
performthesearchviadotproductbetweentheembeddings. ductourexperimentbyalwaysretrieving2additionalfactsover
â€¢ KGLink[8,40]isastraightforwardstrategy,whichidentifies thefactsneededbytheoriginalquestion.Thatis,givenað¾-hop
thequeryentityanditslinkedentitiesascandidatestofindthe question,wesetthetotalnumberofretrievedfactsasð‘›=ð¾+2.
onethatisthemostsimilartotheoriginalquestion.Thisprocess Table4reportstheeditedaccuracyofRAEworkingwithorwith-
isrepeatedð¾ timestoretrievetheentirefactchain. outthepruningstrategy,andwedrawthefollowingconclusions:Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Shi,etal.
Table2:Editedaccuracy(%)onMQUAKEdatasets(MQUAKE-CFandMQUAKE-T).
EditingMethods
w/oedit w/oedit Fine
LanguageModels Datasets ROME MEMEIT SEARC Mello Ours
(origans) (editedans) Tuned
MQUAKE-CF 10.7 0.0 3.8 1.7 2.3 4.0 0.0 62.8
GPT-2
MQUAKE-T 4.7 0.0 5.8 6.4 1.6 2.7 0.0 61.8
Modelsw/o MQUAKE-CF 18.1 0.0 7.7 7.6 8.1 6.8 15.3 69.3
GPT-J
InstructionTuning MQUAKE-T 15.3 3.1 3.1 4.1 10.6 2.8 36.7 63.9
MQUAKE-CF 33.7 1.0 5.6 1.7 2.3 7.9 10.7 66.8
Falcon
MQUAKE-T 35.9 8.9 17.2 7.3 1.6 4.5 51.5 61.6
MQUAKE-CF 43.2 1.0 4.8 8.4 7.6 7.9 10.2 67.2
Vicuna
Modelsw/ MQUAKE-T 15.3 3.1 23.1 5.0 1.7 4.5 51.7 63.2
InstructionTuning Llama2 MQUAKE-CF 29.0 1.1 5.4 6.3 3.8 7.9 20.7 69.1
(chat) MQUAKE-T 26.7 5.1 17.1 8.7 1.7 4.5 49.4 66.2
Table3:Multi-hopfactsretrievalaccuracy(%)comparison. GPT-3.5-instruct
GPT-4
MQUAKE-CF(300casesforeachkindofquestion) 60 GPT-3.5-turbo
Babbage
QuestionType 2-hops 3-hops 4-hops GPT-4
Category RetrievalMethod PM EM PM EM PM EM 40
GPT-3.5-instruct
KGLink 52.7 28.7 18.2 3.7 14.0 0.0
GPT-3.5-turbo
Embedding QuestionReform 62.3 7.7 14.7 0.0 12.3 0.0 20
Mello(Llama2) 84.3 80.0 80.7 42.3 83.3 25.7 Mello
Probability MaxProb(GPT-2) 77.7 50.3 67.3 25.3 65.0 20.0 0 Babbage RAE(ours)
MaxProb(Llama2) 78.3 55.7 79.7 37.0 69.3 28.7
RAE(GPT-2) 83.0 66.3 77.3 41.0 80.3 43.7 0 5 10 15 20
RAE(GPT-J) 83.0 69.7 81.3 53.7 82.7 54.0 TotalInferenceCost(USDollar)
Mutual
RAE(Falcon) 82.3 70.7 72.3 44.3 81.7 47.3
Information Figure4:Editingperformancecomparedwithinferencecost
RAE(Vicuna) 81.0 66.7 79.3 50.3 85.0 50.0
RAE(Llama2) 82.7 69.3 84.0 49.3 82.0 47.0 overdifferentproprietarymodels.
asChatGPT[22].Insuchcases,RAEutilizesadifferentlightweight
PM:PartialMatch.EM:ExactMatch.
Table4:Editedaccuracy(%)with(w)orwithout(w/o)pruning. languagemodeltoperformrelevantfactsretrievalandpruningas
introducedinSection3.2andSection3.3.Then,theobtainedfacts
Dataset MQUAKE-CF
willbefedintotheproprietarymodelstoperformin-contextediting.
Llama2
Type Strategy GPT-2 GPT-J Falcon Vicuna (chat) WeevaluateourproposededitingmethodonGPT-babbage-002,GPT-
w/oPruning 63.0 63.7 65.2 63.8 70.1 3.5-turbo-0613,GPT-3.5-instruct,andGPT-4-0613models[23].Weuse
2-hops w/Pruning 73.3 75.5 74.5 73.5 75.8 GPT-2(1.3B)asourretrievalmodel.Wereporttheeditedaccuracy
Improved 10.3â†‘ 11.8â†‘ 9.3â†‘ 9.7â†‘ 5.7â†‘ and total editing cost of our method for randomly selected 300
w/oPruning 43.1 53.8 55.6 55.0 60.3 cases(MQUAKE-CF)inFigure4.Theeditingcostincludesthetotal
3-hops w/Pruning 53.2 65.4 62.1 62.7 65.8 feesofcallingAPIsandthecostofrunningGPT-2forknowledge
Improved 10.1â†‘ 11.6â†‘ 6.5â†‘ 7.7â†‘ 5.5â†‘ retrievalonrentedGPUs.WealsoreporttheresultsofMello[46]
w/oPruning 49.9 58.8 55.2 61.5 61.6
forcomparison,whereonlytheAPIfeesarecounted.
4-hops w/Pruning 61.9 66.9 62.9 65.5 65.8
InFigure4,wefirstobservethatBabbagehas0.0%editedac-
Improved 12.0â†‘ 8.1â†‘ 7.7â†‘ 4.0â†‘ 4.2â†‘
curacybyusingMello,showingitsineffectivenessineditingthe
un-instructiontunedproprietarylanguagemodel.Itisexpected
(1)Theproposedpruningtechniquesignificantlyenhancestheper- sinceMelloreliesontheconversationalabilityoflanguagemodels
formanceofmodelediting,demonstratedbyachievinganaverage todecomposemulti-hopquestions.Incontrast,RAEiseffectivein
accuracyimprovementof8.3%acrossvariouslanguagemodels.(2) editingalltheseproprietarymodelswitharemarkablylowercost
Weobserveamoreprofoundimprovementforsmallerlanguage thanMello.Inparticular,RAEimprovesMelloforeditingGPT-4
modelsoncomplexquestions,whilethisbenefittolargerlanguage withalmost20%editedaccuracybyonlycostingaroundits15%
modelsisrelativelylesssignificant.Inparticular,theperformance budget.Thishighlightsthebenefitofutilizingtheinherentlan-
improvementofGPT-2onthe4-hopquestionsreaches12.0%,while guagemodelingabilityinsteadoftheinstructionfollowingability
forLlama-2itisjust4.2%.Thisobservationsuggeststhatlarger toperformknowledgeretrievalformulti-hopquestionanswering.
modelsaremorerobusttoredundantinformation.
4.6 CaseStudy
4.5 EditingPerformanceonProprietaryLLMs InFigure5,wepresenttwocasesfromtheMQUAKE-CFdatasetto
To answer Q4, we apply the proposed RAE to edit proprietary demonstratetheretrievingprocessoveraknowledgegraphandthe
languagemodels,wherewecanonlyaccessthemodelviaAPIs,such pruningprocessoftheretrievedfacts.Forvisualizations,thered,
)%(ycaruccARetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
black,anddottedlinesrepresentthefinal,candidate,anddiscarded 5 RELATEDWORK
paths in the knowledge graph with beam search, reflecting the
5.1 ModelEditing
decision-makingprocessofourretrievaldesign.InFigure5a,apo-
Existingmodeleditingmethodsforsingle-hopfacteditingcanbe
tentialpathMiseryâ€“languageâ€“>Englishâ€“countryâ€“>U.K.isdiscarded
categorizedintothefollowingkinds:(1)Meta-learningmethod:
eventhoughitcanleadtothecorrectanswer(U.K.inthiscase).
MEND[19]employsanauxiliarynetworkforgradienttransforma-
Thisisbecauseitdoesnotsharetheinformationthatisneededto
tionduringfine-tuning.(2)Localmodificationmethods:MEMIT[18]
answerthetargetquestion,resultinginalowMIscore.InFigure5b,
andROME[17]editspecificneuronswithinmodelswithminimal
eventhoughthisquestionisassociatedwiththreeeditedfacts,our
impactonunrelatedweights.(3)Extension-basedmethods:SERAC
methodcansuccessfullylocateallofthem,demonstratingrobust-
utilizesafine-tunedsmallauxiliarylanguagemodeltoanswerques-
nessovermulti-hopquestions.Inbothcases,ourpruningstrategy
tionsthatrequireeditedfacts.Meanwhile,GRACE[10]provides
successfullytruncatestheretrievedfactchaintoonlycontainnec-
atheoreticalbasisforconcepterasureacrossmodellayers.(4)In-
essaryfactsrelyingonthenormalizedmodeleditingentropy.
contextlearningmethods:IKE[45]editmodelsthroughin-context
learning,bypassingtheneedforparameterupdates.
Incontrast,modeleditingmethodsformulti-hopfactediting
What is the nationality of the author of Misery ? remain underexplored. One such method, Mello [46], utilizes a
comprehensiveprompttodecomposethemulti-hopquestioninto
Retrieval single-hop questions and then retrieve edited knowledge itera-
Anglic United Kingdom of tively.Arecentcontemporaneouswork,DeepEdit[36],editsLLMs
English Great Britain and
Ireland throughdecodingwithconstraints,whereaneuro-symbolicmethod
isproposedforbetterreasoncoherence.However,theseapproaches
Misery Richard U.K.
Dawkins heavilyrelyonthestronginstructionfollowingandreasoningabil-
London
ityofLLMs,whichreferstotheircapacitytounderstandandfollow
Thriller Royal Society of theinputinstructiontogeneratecoherent,context-awareresponses.
Literature
Stephen Final path Existingmethods[46]fallshortformodelsinrelativelysmallsizes,
King U.S. Candidate path
Discarded path suchasGPT-J(6B)orVicuna(7B),comparedtopowerfulGPT-3.5.
Original path
Pruning
Miseryâ€”authorâ€“ Richard Dawkins â€“ citizen ofâ€”U.Kâ€“ capitalâ€”London
| | | | 5.2 Multi-hopQAwithKnowledgeGraphs
Normalized Model 1.0 0.0 0.31
Editing Entropy: Truncate here! Multi-hopKnowledgeGraphQuestionAnswering(KGQA)involves
(a)Atwo-hopquestionexample(CaseID2). locatinganswerswithinaKGthatareseveralhopsawayfromthe
startingquestionentities.Existingtechniquesusuallyfirstextracta
Which country does the spouse of the performer of
Bangerz belong to? relevantsubgraphfromtheentireKG,thenapplymulti-hopreason-
ing[28,44].Initialretrievalstrategiesapplysimilarityscoreslike
Retrieval U.S. Final path PageRank[28]orembeddingsimilarity[13,44]forsubgraphselec-
U.K. Candidate path tion,butthesemethodsoftenfailtograspthelogicofthequestion,
Pharrell
Discarded path
Williams potentiallyoverlookingvitalfacts.Inthereasoningstage,earlier
Original path
researchusedgraphnetworkarchitecturesforreasoning[12,42].
Bangerz Elvis Jamie
Presley Hewlett SomerecentapproachesnowemployLLMsforreasoning[29,35].
Kathmandu
Think-on-graphappliestheLLMasaninteractiveagentonKGs,
Nepal
usingretrievedknowledgeforreasoning[29].However,thesemeth-
Miley Priscilla odsrelyonstrongreasoningcapabilities,whileLLMswithless
Cyrus Presley Ne rp ua pl ee ese strongreasoningabilitytendtounderperformwiththesemethods.
Pruning
Bangerzâ€”performer â€” Elvis â€”spouseâ€” J a m i e â€”citizenâ€”Nepal â€” capitalâ€”Kathmandu
Presley Hewlett
| | | | |
Normalized Model 1.0 0.84 0.0 0.33 6 CONCLUSION
Editing Entropy: Truncate here!
WeproposeanovelLLMeditingframeworkformulti-hopQA.We
(b)Athree-hopquestion(CaseID1143).
employastrategythatutilizesmutualinformationmaximization
Figure 5: Case studies for edited facts retrieval and prun- fortheretrievaloffactsandaself-optimizingtechniqueforprun-
ing.Theretrievalprocessinvolvesthebeamsearch,starting ingredundantinformation.Thisapproacheffectivelyaddressesthe
fromthequeryentityandnavigatingthroughtheknowledge challengesofintegratingreal-timeknowledgeupdatesinLLMs.
graph with two beams. The two primary candidate edges Theoreticaljustificationsandcomprehensiveevaluationsdemon-
(beams)ateachentityhopareboldfaced,andtheothersare strateRAEâ€™seffectivenessinenhancingtheaccuracyofupdated
discardedanddenotedwithdashedlines.Weemphasizethe LLMresponses.Ourcontributionsmarkasubstantialadvancement
finalbeamsearchresultwithredcolor. inmodeleditingresearch,presentingapromisingdirectionforthe
integrationofdynamicknowledgeinlanguagemodels.Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY Shi,etal.
A THEORETICALJUSTIFICATION REFERENCES
Theorem1. Givenretrievedgraphðº ð‘† âˆˆG,thelatentconceptðœƒ ð‘, [1] EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,
RuxandraCojocaru,MÃ©rouaneDebbah,Ã‰tienneGoffinet,DanielHesslow,Julien
andthequestionð‘žsampledconditionedonconceptðœƒ ð‘,thereexistsa Launay,QuentinMalartic,etal.2023.Thefalconseriesofopenlanguagemodels.
mutualinformationinequality:ð¼(ðº ð‘†;ðœƒ ð‘) â‰¥ð¼(ðº ð‘†;ð‘ž). arXivpreprintarXiv:2311.16867(2023).
Proof. Considerlatentconceptsðœƒ isinferredfromaknowl- [2] LeonardEBaumandTedPetrie.1966. Statisticalinferenceforprobabilistic
ð‘ functionsoffinitestateMarkovchains.Theannalsofmathematicalstatistics37,
edgegraphðº ð‘†,andaquestionð‘žisthengeneratedbasedonthis 6(1966),1554â€“1563.
concept.(Forinstance,thiscanbeasstraightforwardasprompt- [3] SidBlack,LeoGao,PhilWang,ConnorLeahy,andStellaBiderman.2021.GPT-
ingLLMstoformulateaquestionaboutagivensetoffacts).The Neo:LargeScaleAutoregressiveLanguageModelingwithMesh-Tensorflow. https:
//doi.org/10.5281/zenodo.5297715Ifyouusethissoftware,pleaseciteitusing
followingMarkovchain:ðº ð‘† â†’ðœƒ ð‘ â†’ð‘žexists,indicatingthatð‘žis thesemetadata..
conditionallyindependenttoðº .Accordingtothechainrulefor [4] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,
ð‘ 
LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,
mutualinformation,wecanhave:
andEricP.Xing.2023.Vicuna:AnOpen-SourceChatbotImpressingGPT-4with
ð¼(ðº ð‘†;ðœƒ ð‘,ð‘ž)=ð¼(ðº ð‘†;ð‘ž)+ð¼(ðº ð‘†;ðœƒ ð‘|ð‘ž)=ð¼(ðº ð‘†;ðœƒ ð‘)+ð¼(ðº ð‘†;ð‘ž|ðœƒ ð‘). (15)
[5]
9 H0 y% u* nC gh Wa otG nP CT huQ nu ga ,l Lit ey. Hoh utt ,p Ss h: a// ylm nesy Ls o. no grg p/ rb el ,o Bg a/ r2 r0 e2 t3 Z- o0 p3 h-3 ,Y0- iv Ti ac yu ,n Wa/
illiamFedus,
Giventhatthequestionð‘žisconditionallyindependentofthegraph EricLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.2022.Scaling
instruction-finetunedlanguagemodels.arXivpreprintarXiv:2210.11416(2022).
ðº ð‘†,thetermð¼(ðº ð‘†;ð‘ž|ðœƒ ð‘)equalszero.Therefore,weareleftwith:
[6] RoiCohen,EdenBiran,OriYoran,AmirGloberson,andMorGeva.2023.Evalu-
ð¼(ðº ð‘†;ðœƒ ð‘) â‰¥ð¼(ðº ð‘†;ð‘ž).Equalityholdspreciselywhenð¼(ðº ð‘†;ð‘ž|ðœƒ ð‘)is atingtherippleeffectsofknowledgeeditinginlanguagemodels.arXivpreprint
zero,meaningthatthegraphðº providesnoadditionalinformation arXiv:2307.12976(2023).
ð‘† [7] DamaiDai,LiDong,YaruHao,ZhifangSui,BaobaoChang,andFuruWei.2021.
aboutð‘žonceðœƒ ð‘ isknown. â–¡ Knowledgeneuronsinpretrainedtransformers.arXivpreprintarXiv:2104.08696
(2021).
[8] RajarshiDas,AmeyaGodbole,DilipKavarthapu,ZhiyuGong,AbhishekSinghal,
B IMPLEMENTATIONDETAILS
MoYu,XiaoxiaoGuo,TianGao,HamedZamani,ManzilZaheer,etal.2019.
Inourfactretrievalprocess,weemployabeamsearchwithawidth Multi-stepentity-centricinformationretrievalformulti-hopquestionanswering.
of2,meaningweconsideronlytwocandidaterelationsateachhop.
InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering.
113â€“118.
Forthetotalnumberofretrievalhopsð‘›,wesetitto4,5,and6for [9] XiaoqiHan,RuLi,HongyeTan,WangYuanlong,QinghuaChai,andJeffPan.
2-hop,3-hop,and4-hopquestions,respectively.Forsimplicity,we 2023.ImprovingSequentialModelEditingwithFactRetrieval.InFindingsofthe
AssociationforComputationalLinguistics:EMNLP2023.11209â€“11224.
omitthetermð‘(ð‘ž|â„Ž1)asitremainsconstantforeachquestion,and
[10] ThomasHartvigsen,SwamiSankaranarayanan,HamidPalangi,YoonKim,and
itsexclusiondoesnotimpactempiricalperformance.Regarding MarzyehGhassemi.2022. AgingwithGRACE:LifelongModelEditingwith
theSEARCbaseline,wepairsmallermodelswithlargerauxiliary DiscreteKey-ValueAdaptors.arXivpreprintarXiv:2211.11031(2022).
[11] GautierIzacard,MathildeCaron,LucasHosseini,SebastianRiedel,PiotrBo-
models:GPT-2-small(124M)[25]withGPT-2-xl(1.3B)[25],GPT-2- janowski,ArmandJoulin,andEdouardGrave.2021. Unsuperviseddensein-
large(774M)[25]withGPT-J(6B)[33],andGPT-Neo(2.7B)[3]with formationretrievalwithcontrastivelearning. arXivpreprintarXiv:2112.09118
(2021).
bothFalcon(7B)[1]andVicuna(7B)[4],aswellasLlama2(7B)[30].
[12] JinhaoJiang,KunZhou,WayneXinZhao,andJi-RongWen.2022. Great
Weimplementthesemodelswiththecodeandcheckpointsavailable TruthsareAlwaysSimple:ARatherSimpleKnowledgeEncoderforEnhancing
fromHuggingfacelibrary[37]. theCommonsenseReasoningCapacityofPre-TrainedModels.arXivpreprint
arXiv:2205.01841(2022).
[13] JinhaoJiang,KunZhou,WayneXinZhao,andJi-RongWen.2022. Unikgqa:
ACKNOWLEDGMENTS Unifiedretrievalandreasoningforsolvingmulti-hopquestionansweringover
knowledgegraph.arXivpreprintarXiv:2212.00959(2022).
Theworkis,inpart,supportedbyNSF(#IIS-2223768).Theviews
[14] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,
andconclusionsinthispaperarethoseoftheauthorsandshould NamanGoyal,HeinrichKÃ¼ttler,MikeLewis,Wen-tauYih,TimRocktÃ¤schel,
notbeinterpretedasrepresentinganyfundingagencies. etal.2020.Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems33(2020),9459â€“9474.
[15] JunyiLi,JieChen,RuiyangRen,XiaoxueCheng,WayneXinZhao,Jian-YunNie,
andJi-RongWen.2024.TheDawnAftertheDark:AnEmpiricalStudyonFactu-
alityHallucinationinLargeLanguageModels.arXivpreprintarXiv:2401.03205
(2024).
[16] JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-YunNie,andJi-RongWen.2023.
HELMA:ALarge-ScaleHallucinationEvaluationBenchmarkforLargeLanguage
Models.arXivpreprintarXiv:2305.11747(2023).
[17] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov.2022.Locating
andeditingfactualassociationsinGPT.AdvancesinNeuralInformationProcessing
Systems35(2022),17359â€“17372.
[18] KevinMeng,ArnabSenSharma,AlexAndonian,YonatanBelinkov,andDavid
Bau.2022.Mass-editingmemoryinatransformer.arXivpreprintarXiv:2210.07229
(2022).
[19] EricMitchell,CharlesLin,AntoineBosselut,ChelseaFinn,andChristopherD
Manning.2021. Fastmodeleditingatscale. arXivpreprintarXiv:2110.11309
(2021).
[20] EricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelsea
Finn.2022.Memory-basedmodeleditingatscale.InInternationalConferenceon
MachineLearning.PMLR,15817â€“15831.
[21] PingNie,YuyuZhang,ArunRamamurthy,andLeSong.2020.AnsweringAny-
hopOpen-domainQuestionswithIterativeDocumentReranking.arXivpreprint
arXiv:2009.07465(2020).
[22] OpenAI.2023.GPT-3.5.https://openai.com/blog/gpt-3-5/. Accessedon[Date].
[23] OpenAI.2023.ModelsReferredtoasGPT-3.5.https://platform.openai.com/docs/
models/gpt-3-5. Accessedon[Date].Retrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY
[24] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,Pamela [47] ChenZhu,AnkitSinghRawat,ManzilZaheer,SrinadhBhojanapalli,DaliangLi,
Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022. FelixYu,andSanjivKumar.2020.Modifyingmemoriesintransformermodels.
Traininglanguagemodelstofollowinstructionswithhumanfeedback.Advances arXivpreprintarXiv:2012.00363(2020).
inNeuralInformationProcessingSystems35(2022),27730â€“27744.
[25] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever, Received20February2007;revised12March2009;accepted5June2009
etal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog
1,8(2019),9.
[26] CARNEGIE-MELLONUNIVPITTSBURGHPADEPTOFCOMPUTERSCIENCE.
1977.SpeechUnderstandingSystems.SummaryofResultsoftheFive-YearResearch
EffortatCarnegie-MellonUniversity.
[27] GeorgiosSidiropoulos,NikosVoskarides,SvitlanaVakulenko,andEvangelos
Kanoulas.2021. CombiningLexicalandDenseRetrievalforComputationally
EfficientMulti-hopQuestionAnswering.InProceedingsoftheSecondWorkshop
onSimpleandEfficientNaturalLanguageProcessing,NafiseSadatMoosavi,Iryna
Gurevych,AngelaFan,ThomasWolf,YufangHou,AnaMarasoviÄ‡,andSujith
Ravi(Eds.).AssociationforComputationalLinguistics,Virtual,58â€“63. https:
//doi.org/10.18653/v1/2021.sustainlp-1.7
[28] HaitianSun,BhuwanDhingra,ManzilZaheer,KathrynMazaitis,RuslanSalakhut-
dinov,andWilliamWCohen.2018. Opendomainquestionansweringusing
earlyfusionofknowledgebasesandtext.arXivpreprintarXiv:1809.00782(2018).
[29] JiashuoSun,ChengjinXu,LumingyuanTang,SaizhuoWang,ChenLin,Yeyun
Gong,Heung-YeungShum,andJianGuo.2023. Think-on-graph:Deepand
responsiblereasoningoflargelanguagemodelwithknowledgegraph. arXiv
preprintarXiv:2307.07697(2023).
[30] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Lachaux,TimothÃ©eLacroix,BaptisteRoziÃ¨re,NamanGoyal,EricHambro,Faisal
Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
preprintarXiv:2302.13971(2023).
[31] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288(2023).
[32] DennyVrandeÄiÄ‡andMarkusKrÃ¶tzsch.2014. Wikidata:afreecollaborative
knowledgebase.Commun.ACM57,10(2014),78â€“85.
[33] BenWangandAranKomatsuzaki.2021.GPT-J-6B:A6BillionParameterAutore-
gressiveLanguageModel.https://github.com/kingoflolz/mesh-transformer-jax.
[34] WeixuanWang,BarryHaddow,andAlexandraBirch.2023.Retrieval-augmented
MultilingualKnowledgeEditing.arXivpreprintarXiv:2312.13040(2023).
[35] XintaoWang,QianwenYang,YongtingQiu,JiaqingLiang,QianyuHe,Zhouhong
Gu,YanghuaXiao,andWeiWang.2023.Knowledgpt:Enhancinglargelanguage
modelswithretrievalandstorageaccessonknowledgebases. arXivpreprint
arXiv:2308.11761(2023).
[36] YiweiWang,MuhaoChen,NanyunPeng,andKai-WeiChang.2024.DeepEdit:
KnowledgeEditingasDecodingwithConstraints.arXivpreprintarXiv:2401.10471
(2024).
[37] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
AnthonyMoi,PierricCistac,TimRault,RÃ©miLouf,MorganFuntowicz,etal.
2019.Huggingfaceâ€™stransformers:State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771(2019).
[38] XuanshengWu,HuachiZhou,WenlinYao,XiaoHuang,andNinghaoLiu.2023.
TowardsPersonalizedCold-StartRecommendationwithPrompts.arXivpreprint
arXiv:2306.17256(2023).
[39] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa.2021.An
explanationofin-contextlearningasimplicitbayesianinference.arXivpreprint
arXiv:2111.02080(2021).
[40] WenhanXiong,MoYu,XiaoxiaoGuo,HongWang,ShiyuChang,MurrayCamp-
bell,andWilliamYangWang.2019. Simpleyeteffectivebridgereasoningfor
open-domainmulti-hopquestionanswering. arXivpreprintarXiv:1909.07597
(2019).
[41] VikasYadav,StevenBethard,andMihaiSurdeanu.2020.Unsupervisedalignment-
basediterativeevidenceretrievalformulti-hopquestionanswering. arXiv
preprintarXiv:2005.01218(2020).
[42] MichihiroYasunaga,HongyuRen,AntoineBosselut,PercyLiang,andJure
Leskovec.2021. QA-GNN:Reasoningwithlanguagemodelsandknowledge
graphsforquestionanswering.arXivpreprintarXiv:2104.06378(2021).
[43] YuexiangZhai,ShengbangTong,XiaoLi,MuCai,QingQu,YongJaeLee,and
YiMa.2023. InvestigatingtheCatastrophicForgettinginMultimodalLarge
LanguageModels.arXivpreprintarXiv:2309.10313(2023).
[44] JingZhang,XiaokangZhang,JifanYu,JianTang,JieTang,CuipingLi,andHong
Chen.2022.Subgraphretrievalenhancedmodelformulti-hopknowledgebase
questionanswering.arXivpreprintarXiv:2202.13296(2022).
[45] CeZheng,LeiLi,QingxiuDong,YuxuanFan,ZhiyongWu,JingjingXu,and
BaobaoChang.2023.CanWeEditFactualKnowledgebyIn-ContextLearning?
arXivpreprintarXiv:2305.12740(2023).
[46] ZexuanZhong,ZhengxuanWu,ChristopherDManning,ChristopherPotts,and
DanqiChen.2023.MQuAKE:AssessingKnowledgeEditinginLanguageModels
viaMulti-HopQuestions.arXivpreprintarXiv:2305.14795(2023).